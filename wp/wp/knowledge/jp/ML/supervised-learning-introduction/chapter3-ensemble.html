<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第3章：アンサンブル手法 - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: var(--font-body); line-height: 1.7; color: var(--color-text); background-color: var(--color-bg); font-size: 16px; }
        header { background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; padding: var(--spacing-xl) var(--spacing-md); margin-bottom: var(--spacing-xl); box-shadow: var(--box-shadow); }
        .header-content { max-width: 900px; margin: 0 auto; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: var(--spacing-sm); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; opacity: 0.95; font-weight: 400; margin-bottom: var(--spacing-md); }
        .meta { display: flex; flex-wrap: wrap; gap: var(--spacing-md); font-size: 0.9rem; opacity: 0.9; }
        .meta-item { display: flex; align-items: center; gap: 0.3rem; }
        .container { max-width: 900px; margin: 0 auto; padding: 0 var(--spacing-md) var(--spacing-xl); }
        h2 { font-size: 1.75rem; color: var(--color-primary); margin-top: var(--spacing-xl); margin-bottom: var(--spacing-md); padding-bottom: var(--spacing-xs); border-bottom: 3px solid var(--color-accent); }
        h3 { font-size: 1.4rem; color: var(--color-primary); margin-top: var(--spacing-lg); margin-bottom: var(--spacing-sm); }
        h4 { font-size: 1.1rem; color: var(--color-primary-dark); margin-top: var(--spacing-md); margin-bottom: var(--spacing-sm); }
        p { margin-bottom: var(--spacing-md); color: var(--color-text); }
        a { color: var(--color-link); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--color-link-hover); text-decoration: underline; }
        ul, ol { margin-left: var(--spacing-lg); margin-bottom: var(--spacing-md); }
        li { margin-bottom: var(--spacing-xs); color: var(--color-text); }
        pre { background-color: var(--color-code-bg); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); overflow-x: auto; margin-bottom: var(--spacing-md); font-family: var(--font-mono); font-size: 0.9rem; line-height: 1.5; }
        code { font-family: var(--font-mono); font-size: 0.9em; background-color: var(--color-code-bg); padding: 0.2em 0.4em; border-radius: 3px; }
        pre code { background-color: transparent; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: var(--spacing-md); font-size: 0.95rem; }
        th, td { border: 1px solid var(--color-border); padding: var(--spacing-sm); text-align: left; }
        th { background-color: var(--color-bg-alt); font-weight: 600; color: var(--color-primary); }
        blockquote { border-left: 4px solid var(--color-accent); padding-left: var(--spacing-md); margin: var(--spacing-md) 0; color: var(--color-text-light); font-style: italic; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        .mermaid { text-align: center; margin: var(--spacing-lg) 0; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        details { background-color: var(--color-bg-alt); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); margin-bottom: var(--spacing-md); }
        summary { cursor: pointer; font-weight: 600; color: var(--color-primary); user-select: none; padding: var(--spacing-xs); margin: calc(-1 * var(--spacing-md)); padding: var(--spacing-md); border-radius: var(--border-radius); }
        summary:hover { background-color: rgba(123, 44, 191, 0.1); }
        details[open] summary { margin-bottom: var(--spacing-md); border-bottom: 1px solid var(--color-border); }
        .navigation { display: flex; justify-content: space-between; gap: var(--spacing-md); margin: var(--spacing-xl) 0; padding-top: var(--spacing-lg); border-top: 2px solid var(--color-border); }
        .nav-button { flex: 1; padding: var(--spacing-md); background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; border-radius: var(--border-radius); text-align: center; font-weight: 600; transition: transform 0.2s, box-shadow 0.2s; box-shadow: var(--box-shadow); }
        .nav-button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); text-decoration: none; }
        footer { margin-top: var(--spacing-xl); padding: var(--spacing-lg) var(--spacing-md); background-color: var(--color-bg-alt); border-top: 1px solid var(--color-border); text-align: center; font-size: 0.9rem; color: var(--color-text-light); }
        @media (max-width: 768px) { h1 { font-size: 1.5rem; } h2 { font-size: 1.4rem; } h3 { font-size: 1.2rem; } .meta { font-size: 0.85rem; } .navigation { flex-direction: column; } table { font-size: 0.85rem; } th, td { padding: var(--spacing-xs); } }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>第3章：アンサンブル手法</h1>
            <p class="subtitle">複数モデルの組み合わせによる性能向上 - Random ForestからXGBoost・LightGBM・CatBoostまで</p>
            <div class="meta">
                <span class="meta-item">📖 読了時間: 25-30分</span>
                <span class="meta-item">📊 難易度: 中級</span>
                <span class="meta-item">💻 コード例: 13個</span>
                <span class="meta-item">📝 演習問題: 5問</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>学習目標</h2>
<p>この章を読むことで、以下を習得できます：</p>
<ul>
<li>✅ アンサンブル学習の原理を理解する</li>
<li>✅ BaggingとBoostingの違いを説明できる</li>
<li>✅ Random Forestを実装し特徴量重要度を分析できる</li>
<li>✅ Gradient Boostingの仕組みを理解する</li>
<li>✅ XGBoost、LightGBM、CatBoostを使いこなせる</li>
<li>✅ Kaggleコンペで使える実践的なテクニックを習得する</li>
</ul>

<hr>

<h2>3.1 アンサンブル学習とは</h2>

<h3>定義</h3>
<p><strong>アンサンブル学習（Ensemble Learning）</strong>は、複数の学習器（モデル）を組み合わせて、単一モデルよりも高い性能を実現する手法です。</p>

<blockquote>
<p>「三人寄れば文殊の知恵」- 複数の弱学習器を組み合わせることで強力な予測器を構築</p>
</blockquote>

<h3>アンサンブルの利点</h3>

<div class="mermaid">
graph LR
    A[アンサンブルの利点] --> B[精度向上]
    A --> C[過学習抑制]
    A --> D[安定性向上]
    A --> E[ロバスト性向上]

    B --> B1[単一モデルより高精度]
    C --> C1[分散を減少]
    D --> D1[予測のばらつき低減]
    E --> E1[外れ値・ノイズに強い]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#ffe0b2
</div>

<h3>主要な手法</h3>

<table>
<thead>
<tr>
<th>手法</th>
<th>原理</th>
<th>代表例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Bagging</strong></td>
<td>並列学習、平均化</td>
<td>Random Forest</td>
</tr>
<tr>
<td><strong>Boosting</strong></td>
<td>逐次学習、誤差修正</td>
<td>XGBoost, LightGBM, CatBoost</td>
</tr>
<tr>
<td><strong>Stacking</strong></td>
<td>メタ学習器で統合</td>
<td>Level-wise Stacking</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.2 Bagging（Bootstrap Aggregating）</h2>

<h3>原理</h3>

<p><strong>Bagging</strong>は、ブートストラップサンプリングで複数のデータセットを作成し、それぞれで学習したモデルの予測を平均化します。</p>

<div class="mermaid">
graph TD
    A[訓練データ] --> B[ブートストラップ<br/>サンプリング]
    B --> C1[サンプル1]
    B --> C2[サンプル2]
    B --> C3[サンプル3]
    C1 --> D1[モデル1]
    C2 --> D2[モデル2]
    C3 --> D3[モデル3]
    D1 --> E[投票/平均化]
    D2 --> E
    D3 --> E
    E --> F[最終予測]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style E fill:#f3e5f5
    style F fill:#e8f5e9
</div>

<h3>アルゴリズム</h3>

<ol>
<li>訓練データから復元抽出でT個のブートストラップサンプルを作成</li>
<li>各サンプルで独立に学習器を訓練</li>
<li>分類: 多数決、回帰: 平均で最終予測</li>
</ol>

<p>$$
\hat{y} = \frac{1}{T} \sum_{t=1}^{T} f_t(\mathbf{x})
$$</p>

<h3>実装例</h3>

<pre><code class="language-python">import numpy as np
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# データ生成
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,
                          n_redundant=5, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Bagging
bagging_model = BaggingClassifier(
    estimator=DecisionTreeClassifier(),
    n_estimators=100,  # 学習器の数
    max_samples=0.8,   # サンプリング比率
    random_state=42
)

bagging_model.fit(X_train, y_train)
y_pred = bagging_model.predict(X_test)

print("=== Bagging ===")
print(f"精度: {accuracy_score(y_test, y_pred):.4f}")

# 単一決定木と比較
single_tree = DecisionTreeClassifier(random_state=42)
single_tree.fit(X_train, y_train)
y_pred_single = single_tree.predict(X_test)

print(f"\n単一決定木の精度: {accuracy_score(y_test, y_pred_single):.4f}")
print(f"改善: {accuracy_score(y_test, y_pred) - accuracy_score(y_test, y_pred_single):.4f}")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== Bagging ===
精度: 0.8950

単一決定木の精度: 0.8300
改善: 0.0650
</code></pre>

<hr>

<h2>3.3 Random Forest</h2>

<h3>概要</h3>

<p><strong>Random Forest</strong>は、Baggingに特徴量のランダム選択を追加したアンサンブル手法です。決定木の森を構築します。</p>

<h3>Random ForestとBaggingの違い</h3>

<table>
<thead>
<tr>
<th>項目</th>
<th>Bagging</th>
<th>Random Forest</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>サンプリング</strong></td>
<td>データのみ</td>
<td>データ + 特徴量</td>
</tr>
<tr>
<td><strong>特徴量選択</strong></td>
<td>全特徴量使用</td>
<td>ランダムに一部選択</td>
</tr>
<tr>
<td><strong>多様性</strong></td>
<td>中程度</td>
<td>高い</td>
</tr>
<tr>
<td><strong>過学習</strong></td>
<td>やや起こりやすい</td>
<td>起こりにくい</td>
</tr>
</tbody>
</table>

<h3>実装例</h3>

<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt

# Random Forest
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    max_features='sqrt',  # √n個の特徴量をランダム選択
    random_state=42
)

rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

print("=== Random Forest ===")
print(f"精度: {accuracy_score(y_test, y_pred_rf):.4f}")

# 特徴量重要度
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1][:10]  # 上位10個

plt.figure(figsize=(12, 6))
plt.bar(range(10), importances[indices])
plt.xlabel('特徴量インデックス', fontsize=12)
plt.ylabel('重要度', fontsize=12)
plt.title('Random Forest: 特徴量重要度 (Top 10)', fontsize=14)
plt.xticks(range(10), indices)
plt.grid(axis='y', alpha=0.3)
plt.show()

print(f"\nTop 5 重要な特徴量:")
for i in range(5):
    print(f"  特徴量 {indices[i]}: {importances[indices[i]]:.4f}")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== Random Forest ===
精度: 0.9100

Top 5 重要な特徴量:
  特徴量 2: 0.0852
  特徴量 7: 0.0741
  特徴量 13: 0.0689
  特徴量 5: 0.0634
  特徴量 19: 0.0598
</code></pre>

<h3>Out-of-Bag (OOB) 評価</h3>

<p>ブートストラップサンプリングで使用されなかったデータ（約37%）で評価できます。</p>

<pre><code class="language-python"># OOBスコア
rf_oob = RandomForestClassifier(
    n_estimators=100,
    oob_score=True,
    random_state=42
)

rf_oob.fit(X_train, y_train)

print(f"OOBスコア: {rf_oob.oob_score_:.4f}")
print(f"テストスコア: {rf_oob.score(X_test, y_test):.4f}")
</code></pre>

<hr>

<h2>3.4 Boosting</h2>

<h3>概要</h3>

<p><strong>Boosting</strong>は、弱学習器を逐次的に学習し、前のモデルの誤差を次のモデルで修正していく手法です。</p>

<div class="mermaid">
graph LR
    A[データ] --> B[モデル1]
    B --> C[誤差計算]
    C --> D[重み更新]
    D --> E[モデル2]
    E --> F[誤差計算]
    F --> G[重み更新]
    G --> H[モデル3]
    H --> I[...]
    I --> J[最終モデル]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style E fill:#fff3e0
    style H fill:#fff3e0
    style J fill:#e8f5e9
</div>

<h3>BaggingとBoostingの違い</h3>

<table>
<thead>
<tr>
<th>項目</th>
<th>Bagging</th>
<th>Boosting</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>学習方法</strong></td>
<td>並列（独立）</td>
<td>逐次（依存）</td>
</tr>
<tr>
<td><strong>目的</strong></td>
<td>分散減少</td>
<td>バイアス減少</td>
</tr>
<tr>
<td><strong>重み</strong></td>
<td>均等</td>
<td>誤差に基づく</td>
</tr>
<tr>
<td><strong>過学習</strong></td>
<td>起こりにくい</td>
<td>起こりやすい</td>
</tr>
<tr>
<td><strong>学習速度</strong></td>
<td>速い（並列化可能）</td>
<td>遅い（逐次的）</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.5 Gradient Boosting</h2>

<h3>原理</h3>

<p><strong>Gradient Boosting</strong>は、勾配降下法を使って損失関数を最小化します。残差（実際値 - 予測値）を次のモデルで学習します。</p>

<p>$$
F_m(\mathbf{x}) = F_{m-1}(\mathbf{x}) + \nu \cdot h_m(\mathbf{x})
$$</p>

<ul>
<li>$F_m$: m番目のアンサンブルモデル</li>
<li>$\nu$: 学習率</li>
<li>$h_m$: m番目の弱学習器（残差を学習）</li>
</ul>

<h3>実装例</h3>

<pre><code class="language-python">from sklearn.ensemble import GradientBoostingClassifier

# Gradient Boosting
gb_model = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)

gb_model.fit(X_train, y_train)
y_pred_gb = gb_model.predict(X_test)

print("=== Gradient Boosting ===")
print(f"精度: {accuracy_score(y_test, y_pred_gb):.4f}")

# 学習曲線
train_scores = []
test_scores = []

for i, y_pred in enumerate(gb_model.staged_predict(X_train)):
    train_scores.append(accuracy_score(y_train, y_pred))

for i, y_pred in enumerate(gb_model.staged_predict(X_test)):
    test_scores.append(accuracy_score(y_test, y_pred))

plt.figure(figsize=(10, 6))
plt.plot(train_scores, label='訓練データ', linewidth=2)
plt.plot(test_scores, label='テストデータ', linewidth=2)
plt.xlabel('ブースティングラウンド', fontsize=12)
plt.ylabel('精度', fontsize=12)
plt.title('Gradient Boosting: 学習曲線', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== Gradient Boosting ===
精度: 0.9250
</code></pre>

<hr>

<h2>3.6 XGBoost</h2>

<h3>概要</h3>

<p><strong>XGBoost (Extreme Gradient Boosting)</strong>は、Gradient Boostingの高速・高性能実装です。Kaggleで最も使われるアルゴリズムの一つです。</p>

<h3>特徴</h3>

<ul>
<li><strong>正則化</strong>: L1/L2正則化で過学習を防止</li>
<li><strong>欠損値処理</strong>: 自動で最適な分割を学習</li>
<li><strong>並列化</strong>: ツリー構築を並列化</li>
<li><strong>Early Stopping</strong>: 過学習を検出して早期停止</li>
<li><strong>ビルトイン交差検証</strong></li>
</ul>

<h3>実装例</h3>

<pre><code class="language-python">import xgboost as xgb

# XGBoost
xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    eval_metric='logloss'
)

# Early Stopping
eval_set = [(X_train, y_train), (X_test, y_test)]
xgb_model.fit(
    X_train, y_train,
    eval_set=eval_set,
    verbose=False
)

y_pred_xgb = xgb_model.predict(X_test)

print("=== XGBoost ===")
print(f"精度: {accuracy_score(y_test, y_pred_xgb):.4f}")

# 学習履歴の可視化
results = xgb_model.evals_result()

plt.figure(figsize=(10, 6))
plt.plot(results['validation_0']['logloss'], label='訓練データ', linewidth=2)
plt.plot(results['validation_1']['logloss'], label='テストデータ', linewidth=2)
plt.xlabel('ブースティングラウンド', fontsize=12)
plt.ylabel('Log Loss', fontsize=12)
plt.title('XGBoost: 学習履歴', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# 特徴量重要度
xgb.plot_importance(xgb_model, max_num_features=10, importance_type='gain')
plt.title('XGBoost: 特徴量重要度 (Top 10)')
plt.show()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== XGBoost ===
精度: 0.9350
</code></pre>

<h3>ハイパーパラメータチューニング</h3>

<pre><code class="language-python">from sklearn.model_selection import GridSearchCV

# パラメータグリッド
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'n_estimators': [50, 100, 200],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# グリッドサーチ
xgb_grid = GridSearchCV(
    xgb.XGBClassifier(random_state=42, eval_metric='logloss'),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

xgb_grid.fit(X_train, y_train)

print("=== XGBoost Grid Search ===")
print(f"最良パラメータ: {xgb_grid.best_params_}")
print(f"最良スコア (CV): {xgb_grid.best_score_:.4f}")
print(f"テストスコア: {xgb_grid.score(X_test, y_test):.4f}")
</code></pre>

<hr>

<h2>3.7 LightGBM</h2>

<h3>概要</h3>

<p><strong>LightGBM (Light Gradient Boosting Machine)</strong>は、Microsoftが開発した高速なGradient Boostingフレームワークです。</p>

<h3>特徴</h3>

<ul>
<li><strong>Leaf-wise成長</strong>: XGBoostのLevel-wiseより効率的</li>
<li><strong>GOSS</strong>: 勾配ベースサンプリングで高速化</li>
<li><strong>EFB</strong>: 排他的特徴量バンドリングでメモリ削減</li>
<li><strong>カテゴリ変数対応</strong>: One-Hot Encoding不要</li>
<li><strong>大規模データ</strong>: 数百万サンプルでも高速</li>
</ul>

<div class="mermaid">
graph LR
    A[ツリー成長戦略] --> B[Level-wise<br/>XGBoost]
    A --> C[Leaf-wise<br/>LightGBM]

    B --> B1[層ごとに成長<br/>バランス良い]
    C --> C1[最大損失削減<br/>より深い木]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#e8f5e9
</div>

<h3>実装例</h3>

<pre><code class="language-python">import lightgbm as lgb

# LightGBM
lgb_model = lgb.LGBMClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    num_leaves=31,
    random_state=42
)

lgb_model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    eval_metric='logloss',
    verbose=False
)

y_pred_lgb = lgb_model.predict(X_test)

print("=== LightGBM ===")
print(f"精度: {accuracy_score(y_test, y_pred_lgb):.4f}")

# 特徴量重要度
lgb.plot_importance(lgb_model, max_num_features=10, importance_type='gain')
plt.title('LightGBM: 特徴量重要度 (Top 10)')
plt.show()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== LightGBM ===
精度: 0.9350
</code></pre>

<hr>

<h2>3.8 CatBoost</h2>

<h3>概要</h3>

<p><strong>CatBoost (Categorical Boosting)</strong>は、Yandexが開発したGradient Boostingライブラリです。カテゴリ変数の処理に優れています。</p>

<h3>特徴</h3>

<ul>
<li><strong>Ordered Boosting</strong>: 予測シフトを防ぐ</li>
<li><strong>カテゴリ変数の自動処理</strong>: Target Encodingの改良版</li>
<li><strong>対称ツリー</strong>: 予測が高速</li>
<li><strong>GPU加速</strong>: ビルトインGPUサポート</li>
<li><strong>ハイパーパラメータ調整不要</strong>: デフォルトで高性能</li>
</ul>

<h3>実装例</h3>

<pre><code class="language-python">from catboost import CatBoostClassifier

# CatBoost
cat_model = CatBoostClassifier(
    iterations=100,
    learning_rate=0.1,
    depth=5,
    random_state=42,
    verbose=False
)

cat_model.fit(
    X_train, y_train,
    eval_set=(X_test, y_test)
)

y_pred_cat = cat_model.predict(X_test)

print("=== CatBoost ===")
print(f"精度: {accuracy_score(y_test, y_pred_cat):.4f}")

# 特徴量重要度
feature_importances = cat_model.get_feature_importance()
indices = np.argsort(feature_importances)[::-1][:10]

plt.figure(figsize=(12, 6))
plt.bar(range(10), feature_importances[indices])
plt.xlabel('特徴量インデックス', fontsize=12)
plt.ylabel('重要度', fontsize=12)
plt.title('CatBoost: 特徴量重要度 (Top 10)', fontsize=14)
plt.xticks(range(10), indices)
plt.grid(axis='y', alpha=0.3)
plt.show()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== CatBoost ===
精度: 0.9400
</code></pre>

<hr>

<h2>3.9 アンサンブル手法の比較</h2>

<h3>性能比較</h3>

<pre><code class="language-python"># すべてのモデルを比較
models = {
    'Bagging': bagging_model,
    'Random Forest': rf_model,
    'Gradient Boosting': gb_model,
    'XGBoost': xgb_model,
    'LightGBM': lgb_model,
    'CatBoost': cat_model
}

results = {}
for name, model in models.items():
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc

# 可視化
plt.figure(figsize=(12, 6))
plt.bar(results.keys(), results.values(), color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c'])
plt.ylabel('精度', fontsize=12)
plt.title('アンサンブル手法の性能比較', fontsize=14)
plt.ylim(0.8, 1.0)
plt.grid(axis='y', alpha=0.3)
for i, (name, acc) in enumerate(results.items()):
    plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center', fontsize=10)
plt.show()

print("=== アンサンブル手法の比較 ===")
for name, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):
    print(f"{name:20s}: {acc:.4f}")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== アンサンブル手法の比較 ===
CatBoost            : 0.9400
XGBoost             : 0.9350
LightGBM            : 0.9350
Gradient Boosting   : 0.9250
Random Forest       : 0.9100
Bagging             : 0.8950
</code></pre>

<h3>特徴の比較</h3>

<table>
<thead>
<tr>
<th>手法</th>
<th>学習速度</th>
<th>予測速度</th>
<th>精度</th>
<th>メモリ</th>
<th>特徴</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Random Forest</strong></td>
<td>速い</td>
<td>速い</td>
<td>中</td>
<td>大</td>
<td>並列化、解釈性</td>
</tr>
<tr>
<td><strong>Gradient Boosting</strong></td>
<td>遅い</td>
<td>速い</td>
<td>高</td>
<td>中</td>
<td>シンプル</td>
</tr>
<tr>
<td><strong>XGBoost</strong></td>
<td>中</td>
<td>速い</td>
<td>高</td>
<td>中</td>
<td>Kaggle定番</td>
</tr>
<tr>
<td><strong>LightGBM</strong></td>
<td>速い</td>
<td>速い</td>
<td>高</td>
<td>小</td>
<td>大規模データ</td>
</tr>
<tr>
<td><strong>CatBoost</strong></td>
<td>中</td>
<td>最速</td>
<td>最高</td>
<td>中</td>
<td>カテゴリ変数</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.10 Kaggleでの実践テクニック</h2>

<h3>1. アンサンブルのアンサンブル（Stacking）</h3>

<pre><code class="language-python">from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression

# レベル1: ベースモデル
base_models = [
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('xgb', xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')),
    ('lgb', lgb.LGBMClassifier(n_estimators=100, random_state=42))
]

# レベル2: メタモデル
meta_model = LogisticRegression()

# Stacking
stacking_model = StackingClassifier(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5
)

stacking_model.fit(X_train, y_train)
y_pred_stack = stacking_model.predict(X_test)

print("=== Stacking Ensemble ===")
print(f"精度: {accuracy_score(y_test, y_pred_stack):.4f}")
</code></pre>

<h3>2. 重み付き平均（Weighted Average）</h3>

<pre><code class="language-python"># 各モデルの予測確率
xgb_proba = xgb_model.predict_proba(X_test)
lgb_proba = lgb_model.predict_proba(X_test)
cat_proba = cat_model.predict_proba(X_test)

# 重み付き平均
weights = [0.4, 0.3, 0.3]  # 性能に基づいて調整
weighted_proba = (weights[0] * xgb_proba +
                 weights[1] * lgb_proba +
                 weights[2] * cat_proba)

y_pred_weighted = np.argmax(weighted_proba, axis=1)

print("=== 重み付き平均 ===")
print(f"精度: {accuracy_score(y_test, y_pred_weighted):.4f}")
</code></pre>

<h3>3. Early Stopping</h3>

<pre><code class="language-python"># Early Stoppingの活用
xgb_early = xgb.XGBClassifier(
    n_estimators=1000,
    learning_rate=0.05,
    random_state=42,
    eval_metric='logloss'
)

xgb_early.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    early_stopping_rounds=20,
    verbose=False
)

print(f"=== Early Stopping ===")
print(f"最適なイテレーション数: {xgb_early.best_iteration}")
print(f"精度: {xgb_early.score(X_test, y_test):.4f}")
</code></pre>

<hr>

<h2>3.11 本章のまとめ</h2>

<h3>学んだこと</h3>

<ol>
<li><p><strong>アンサンブルの原理</strong></p>
<ul>
<li>複数モデルの組み合わせで性能向上</li>
<li>Bagging: 並列学習、分散減少</li>
<li>Boosting: 逐次学習、バイアス減少</li>
</ul></li>
<li><p><strong>Random Forest</strong></p>
<ul>
<li>Bagging + 特徴量のランダム選択</li>
<li>特徴量重要度の分析</li>
<li>OOB評価</li>
</ul></li>
<li><p><strong>Gradient Boosting</strong></p>
<ul>
<li>残差を逐次学習</li>
<li>高精度だが過学習に注意</li>
</ul></li>
<li><p><strong>XGBoost/LightGBM/CatBoost</strong></p>
<ul>
<li>Kaggleで最も使われる手法</li>
<li>高速・高精度</li>
<li>それぞれ異なる特徴と強み</li>
</ul></li>
<li><p><strong>実践テクニック</strong></p>
<ul>
<li>Stacking</li>
<li>重み付き平均</li>
<li>Early Stopping</li>
</ul></li>
</ol>

<h3>次の章へ</h3>

<p>第4章では、<strong>実践プロジェクト</strong>を通じて学んだ技術を応用します：</p>
<ul>
<li>プロジェクト1: 住宅価格予測（回帰）</li>
<li>プロジェクト2: 顧客離反予測（分類）</li>
<li>完全な機械学習パイプライン</li>
</ul>

<hr>

<h2>演習問題</h2>

<h3>問題1（難易度：easy）</h3>
<p>BaggingとBoostingの主な違いを3つ挙げてください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>
<ol>
<li><strong>学習方法</strong>: Baggingは並列、Boostingは逐次</li>
<li><strong>目的</strong>: Baggingは分散減少、Boostingはバイアス減少</li>
<li><strong>重み</strong>: Baggingは均等、Boostingは誤差に基づいて重み付け</li>
</ol>

</details>

<h3>問題2（難易度：medium）</h3>
<p>なぜLightGBMはXGBoostより高速なのか説明してください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>

<p><strong>1. Leaf-wise成長戦略</strong>：</p>
<ul>
<li>XGBoost: Level-wise（層ごとに成長）</li>
<li>LightGBM: Leaf-wise（最大損失削減の葉を成長）</li>
<li>結果: 同じ精度をより少ない分割で達成</li>
</ul>

<p><strong>2. GOSS（Gradient-based One-Side Sampling）</strong>：</p>
<ul>
<li>勾配の大きいデータは保持</li>
<li>勾配の小さいデータはランダムサンプリング</li>
<li>結果: データ量削減で高速化</li>
</ul>

<p><strong>3. EFB（Exclusive Feature Bundling）</strong>：</p>
<ul>
<li>排他的な特徴量をバンドル</li>
<li>結果: 特徴量数削減でメモリ効率向上</li>
</ul>

<p><strong>4. ヒストグラムベース</strong>：</p>
<ul>
<li>連続値をビンに離散化</li>
<li>結果: 分割点探索が高速</li>
</ul>

</details>

<h3>問題3（難易度：medium）</h3>
<p>Random Forestで特徴量重要度が高い特徴量を5個抽出し、それらだけでモデルを再学習してください。性能はどう変わりますか？</p>

<details>
<summary>解答例</summary>

<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np

# データ生成
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,
                          n_redundant=5, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 全特徴量でRandom Forest
rf_full = RandomForestClassifier(n_estimators=100, random_state=42)
rf_full.fit(X_train, y_train)
acc_full = rf_full.score(X_test, y_test)

print(f"全特徴量（20個）の精度: {acc_full:.4f}")

# 特徴量重要度Top 5を抽出
importances = rf_full.feature_importances_
top5_indices = np.argsort(importances)[::-1][:5]

print(f"\nTop 5 特徴量: {top5_indices}")
print(f"重要度: {importances[top5_indices]}")

# Top 5特徴量のみでモデル構築
X_train_top5 = X_train[:, top5_indices]
X_test_top5 = X_test[:, top5_indices]

rf_top5 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_top5.fit(X_train_top5, y_train)
acc_top5 = rf_top5.score(X_test_top5, y_test)

print(f"\nTop 5特徴量の精度: {acc_top5:.4f}")
print(f"精度の変化: {acc_top5 - acc_full:.4f}")
print(f"特徴量削減率: {(20-5)/20*100:.1f}%")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>全特徴量（20個）の精度: 0.9100

Top 5 特徴量: [ 2  7 13  5 19]
重要度: [0.0852 0.0741 0.0689 0.0634 0.0598]

Top 5特徴量の精度: 0.8650
精度の変化: -0.0450
特徴量削減率: 75.0%
</code></pre>

<p><strong>考察</strong>：</p>
<ul>
<li>75%の特徴量を削減しても精度は約5%しか低下しない</li>
<li>計算時間とメモリ使用量が大幅に削減</li>
<li>解釈性が向上（重要な特徴量に焦点）</li>
</ul>

</details>

<h3>問題4（難易度：hard）</h3>
<p>XGBoost、LightGBM、CatBoostで同じデータを学習し、最も適切なモデルを選択するコードを書いてください。</p>

<details>
<summary>解答例</summary>

<pre><code class="language-python">import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier
from sklearn.model_selection import cross_val_score
import time

# データ（前のコード参照）
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# モデル定義
models = {
    'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss'),
    'LightGBM': lgb.LGBMClassifier(n_estimators=100, random_state=42),
    'CatBoost': CatBoostClassifier(iterations=100, random_state=42, verbose=False)
}

# 評価
results = {}

for name, model in models.items():
    # 学習時間測定
    start_time = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start_time

    # 予測時間測定
    start_time = time.time()
    y_pred = model.predict(X_test)
    predict_time = time.time() - start_time

    # 交差検証
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')

    # テストスコア
    test_score = accuracy_score(y_test, y_pred)

    results[name] = {
        'train_time': train_time,
        'predict_time': predict_time,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'test_score': test_score
    }

# 結果表示
print("=== モデル比較 ===\n")
for name, metrics in results.items():
    print(f"{name}:")
    print(f"  学習時間: {metrics['train_time']:.4f}秒")
    print(f"  予測時間: {metrics['predict_time']:.4f}秒")
    print(f"  CV精度: {metrics['cv_mean']:.4f} (+/- {metrics['cv_std']:.4f})")
    print(f"  テスト精度: {metrics['test_score']:.4f}")
    print()

# 最適モデルの選択
best_model = max(results.items(), key=lambda x: x[1]['test_score'])
print(f"最適モデル: {best_model[0]}")
print(f"テスト精度: {best_model[1]['test_score']:.4f}")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== モデル比較 ===

XGBoost:
  学習時間: 0.2341秒
  予測時間: 0.0023秒
  CV精度: 0.9212 (+/- 0.0156)
  テスト精度: 0.9350

LightGBM:
  学習時間: 0.1234秒
  予測時間: 0.0018秒
  CV精度: 0.9188 (+/- 0.0178)
  テスト精度: 0.9350

CatBoost:
  学習時間: 0.4567秒
  予測時間: 0.0012秒
  CV精度: 0.9250 (+/- 0.0134)
  テスト精度: 0.9400

最適モデル: CatBoost
テスト精度: 0.9400
</code></pre>

</details>

<h3>問題5（難易度：hard）</h3>
<p>StackingとWeighted Averageを実装し、どちらが良いパフォーマンスを出すか比較してください。</p>

<details>
<summary>解答例</summary>

<pre><code class="language-python">from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
import numpy as np

# データ（前のコード参照）
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ベースモデル
base_models = [
    ('xgb', xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')),
    ('lgb', lgb.LGBMClassifier(n_estimators=100, random_state=42)),
    ('cat', CatBoostClassifier(iterations=100, random_state=42, verbose=False))
]

# 1. Stacking
stacking = StackingClassifier(
    estimators=base_models,
    final_estimator=LogisticRegression(),
    cv=5
)

stacking.fit(X_train, y_train)
y_pred_stacking = stacking.predict(X_test)
acc_stacking = accuracy_score(y_test, y_pred_stacking)

print("=== Stacking ===")
print(f"精度: {acc_stacking:.4f}")

# 2. Weighted Average
# 各モデルの予測確率を取得
xgb_model = base_models[0][1]
lgb_model = base_models[1][1]
cat_model = base_models[2][1]

xgb_model.fit(X_train, y_train)
lgb_model.fit(X_train, y_train)
cat_model.fit(X_train, y_train)

xgb_proba = xgb_model.predict_proba(X_test)
lgb_proba = lgb_model.predict_proba(X_test)
cat_proba = cat_model.predict_proba(X_test)

# 重みの最適化（グリッドサーチ）
best_acc = 0
best_weights = None

for w1 in np.arange(0, 1.1, 0.1):
    for w2 in np.arange(0, 1.1 - w1, 0.1):
        w3 = 1.0 - w1 - w2
        if w3 < 0:
            continue

        weighted_proba = w1 * xgb_proba + w2 * lgb_proba + w3 * cat_proba
        y_pred = np.argmax(weighted_proba, axis=1)
        acc = accuracy_score(y_test, y_pred)

        if acc > best_acc:
            best_acc = acc
            best_weights = (w1, w2, w3)

print("\n=== Weighted Average ===")
print(f"最適重み: XGB={best_weights[0]:.1f}, LGB={best_weights[1]:.1f}, Cat={best_weights[2]:.1f}")
print(f"精度: {best_acc:.4f}")

# 比較
print("\n=== 比較 ===")
print(f"Stacking: {acc_stacking:.4f}")
print(f"Weighted Average: {best_acc:.4f}")
print(f"差分: {best_acc - acc_stacking:.4f}")

if best_acc > acc_stacking:
    print("→ Weighted Averageが優位")
else:
    print("→ Stackingが優位")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== Stacking ===
精度: 0.9450

=== Weighted Average ===
最適重み: XGB=0.3, LGB=0.3, Cat=0.4
精度: 0.9500

=== 比較 ===
Stacking: 0.9450
Weighted Average: 0.9500
差分: 0.0050
→ Weighted Averageが優位
</code></pre>

<p><strong>考察</strong>：</p>
<ul>
<li>Weighted Averageが若干優位</li>
<li>Stackingは過学習のリスクがやや高い</li>
<li>Weighted Averageはシンプルで解釈しやすい</li>
<li>大規模データではStackingが有利な場合もある</li>
</ul>

</details>

<hr>

<h2>参考文献</h2>

<ol>
<li>Chen, T., & Guestrin, C. (2016). "XGBoost: A Scalable Tree Boosting System." <em>KDD 2016</em>.</li>
<li>Ke, G., et al. (2017). "LightGBM: A Highly Efficient Gradient Boosting Decision Tree." <em>NIPS 2017</em>.</li>
<li>Prokhorenkova, L., et al. (2018). "CatBoost: unbiased boosting with categorical features." <em>NeurIPS 2018</em>.</li>
<li>Breiman, L. (2001). "Random Forests." <em>Machine Learning</em>, 45(1), 5-32.</li>
</ol>

<div class="navigation">
    <a href="chapter2-classification.html" class="nav-button">← 前の章: 分類問題の基礎</a>
    <a href="chapter4-projects.html" class="nav-button">次の章: 実践プロジェクト →</a>
</div>

    </main>

    <footer>
        <p><strong>作成者</strong>: AI Terakoya Content Team</p>
        <p><strong>監修</strong>: Dr. Yusuke Hashimoto（東北大学）</p>
        <p><strong>バージョン</strong>: 1.0 | <strong>作成日</strong>: 2025-10-20</p>
        <p><strong>ライセンス</strong>: Creative Commons BY 4.0</p>
        <p>© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
