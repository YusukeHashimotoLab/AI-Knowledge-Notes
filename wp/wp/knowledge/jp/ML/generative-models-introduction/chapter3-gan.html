<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第3章：GAN (Generative Adversarial Networks) - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: var(--font-body); line-height: 1.7; color: var(--color-text); background-color: var(--color-bg); font-size: 16px; }
        header { background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; padding: var(--spacing-xl) var(--spacing-md); margin-bottom: var(--spacing-xl); box-shadow: var(--box-shadow); }
        .header-content { max-width: 900px; margin: 0 auto; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: var(--spacing-sm); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; opacity: 0.95; font-weight: 400; margin-bottom: var(--spacing-md); }
        .meta { display: flex; flex-wrap: wrap; gap: var(--spacing-md); font-size: 0.9rem; opacity: 0.9; }
        .meta-item { display: flex; align-items: center; gap: 0.3rem; }
        .container { max-width: 900px; margin: 0 auto; padding: 0 var(--spacing-md) var(--spacing-xl); }
        h2 { font-size: 1.75rem; color: var(--color-primary); margin-top: var(--spacing-xl); margin-bottom: var(--spacing-md); padding-bottom: var(--spacing-xs); border-bottom: 3px solid var(--color-accent); }
        h3 { font-size: 1.4rem; color: var(--color-primary); margin-top: var(--spacing-lg); margin-bottom: var(--spacing-sm); }
        h4 { font-size: 1.1rem; color: var(--color-primary-dark); margin-top: var(--spacing-md); margin-bottom: var(--spacing-sm); }
        p { margin-bottom: var(--spacing-md); color: var(--color-text); }
        a { color: var(--color-link); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--color-link-hover); text-decoration: underline; }
        ul, ol { margin-left: var(--spacing-lg); margin-bottom: var(--spacing-md); }
        li { margin-bottom: var(--spacing-xs); color: var(--color-text); }
        pre { background-color: var(--color-code-bg); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); overflow-x: auto; margin-bottom: var(--spacing-md); font-family: var(--font-mono); font-size: 0.9rem; line-height: 1.5; }
        code { font-family: var(--font-mono); font-size: 0.9em; background-color: var(--color-code-bg); padding: 0.2em 0.4em; border-radius: 3px; }
        pre code { background-color: transparent; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: var(--spacing-md); font-size: 0.95rem; }
        th, td { border: 1px solid var(--color-border); padding: var(--spacing-sm); text-align: left; }
        th { background-color: var(--color-bg-alt); font-weight: 600; color: var(--color-primary); }
        blockquote { border-left: 4px solid var(--color-accent); padding-left: var(--spacing-md); margin: var(--spacing-md) 0; color: var(--color-text-light); font-style: italic; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        .mermaid { text-align: center; margin: var(--spacing-lg) 0; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        details { background-color: var(--color-bg-alt); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); margin-bottom: var(--spacing-md); }
        summary { cursor: pointer; font-weight: 600; color: var(--color-primary); user-select: none; padding: var(--spacing-xs); margin: calc(-1 * var(--spacing-md)); padding: var(--spacing-md); border-radius: var(--border-radius); }
        summary:hover { background-color: rgba(123, 44, 191, 0.1); }
        details[open] summary { margin-bottom: var(--spacing-md); border-bottom: 1px solid var(--color-border); }
        .navigation { display: flex; justify-content: space-between; gap: var(--spacing-md); margin: var(--spacing-xl) 0; padding-top: var(--spacing-lg); border-top: 2px solid var(--color-border); }
        .nav-button { flex: 1; padding: var(--spacing-md); background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; border-radius: var(--border-radius); text-align: center; font-weight: 600; transition: transform 0.2s, box-shadow 0.2s; box-shadow: var(--box-shadow); }
        .nav-button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); text-decoration: none; }
        footer { margin-top: var(--spacing-xl); padding: var(--spacing-lg) var(--spacing-md); background-color: var(--color-bg-alt); border-top: 1px solid var(--color-border); text-align: center; font-size: 0.9rem; color: var(--color-text-light); }
        @media (max-width: 768px) { h1 { font-size: 1.5rem; } h2 { font-size: 1.4rem; } h3 { font-size: 1.2rem; } .meta { font-size: 0.85rem; } .navigation { flex-direction: column; } table { font-size: 0.85rem; } th, td { padding: var(--spacing-xs); } }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>第3章：GAN (Generative Adversarial Networks)</h1>
            <p class="subtitle">敵対的学習で現実的な画像を生成 - Vanilla GANからStyleGANまで</p>
            <div class="meta">
                <span class="meta-item">📖 読了時間: 30-35分</span>
                <span class="meta-item">📊 難易度: 中級〜上級</span>
                <span class="meta-item">💻 コード例: 8個</span>
                <span class="meta-item">📝 演習問題: 5問</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>学習目標</h2>
<p>この章を読むことで、以下を習得できます：</p>
<ul>
<li>✅ GANの基本概念とGeneratorとDiscriminatorの役割を理解する</li>
<li>✅ Minimax gameの理論的背景とNash均衡を理解する</li>
<li>✅ Mode collapse問題とその対策を習得する</li>
<li>✅ DCGAN（Deep Convolutional GAN）のアーキテクチャを実装できる</li>
<li>✅ WGAN-GP（Wasserstein GAN with Gradient Penalty）を理解する</li>
<li>✅ Spectral NormalizationとLabel Smoothingの訓練テクニックを習得する</li>
<li>✅ StyleGANの基本概念と特徴を理解する</li>
<li>✅ 実際の画像生成プロジェクトを実装できる</li>
</ul>

<hr>

<h2>3.1 GANの基本概念</h2>

<h3>Generatorとは</h3>

<p><strong>Generator（生成器）</strong>は、ランダムノイズ（潜在変数）から現実的なデータを生成するニューラルネットワークです。</p>

<blockquote>
<p>「Generatorは、ランダムな潜在ベクトル $\mathbf{z} \sim p_z(\mathbf{z})$ を入力として受け取り、訓練データと見分けがつかない偽データ $G(\mathbf{z})$ を生成することを学習する」</p>
</blockquote>

<div class="mermaid">
graph LR
    A[潜在ベクトル z<br/>100次元ノイズ] --> B[Generator G]
    B --> C[生成画像<br/>28×28×1]

    D[ランダム<br/>サンプリング] --> A

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#e8f5e9
</div>

<h3>Discriminatorとは</h3>

<p><strong>Discriminator（識別器）</strong>は、入力データが本物（訓練データ）か偽物（Generatorの出力）かを判定する二値分類器です。</p>

<div class="mermaid">
graph TB
    A1[本物画像] --> D[Discriminator D]
    A2[生成画像] --> D

    D --> O1[本物: 1.0<br/>スコア]
    D --> O2[偽物: 0.0<br/>スコア]

    style A1 fill:#e8f5e9
    style A2 fill:#ffebee
    style D fill:#fff3e0
    style O1 fill:#e8f5e9
    style O2 fill:#ffebee
</div>

<h3>敵対的学習のメカニズム</h3>

<p>GANは、GeneratorとDiscriminatorの<strong>敵対的な競争</strong>を通じて学習します：</p>

<div class="mermaid">
sequenceDiagram
    participant G as Generator
    participant D as Discriminator
    participant R as 本物データ

    G->>G: ノイズから画像生成
    G->>D: 生成画像を提示
    R->>D: 本物画像を提示
    D->>D: 本物/偽物を識別
    D->>G: フィードバック（勾配）
    G->>G: より騙しやすい画像へ改善
    D->>D: より見破りやすく改善

    Note over G,D: このプロセスを繰り返す
</div>

<h3>Minimax Game理論</h3>

<p>GANの目的関数は<strong>Minimax最適化</strong>として定式化されます：</p>

<p>$$
\min_G \max_D V(D, G) = \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}}[\log D(\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim p_z}[\log(1 - D(G(\mathbf{z})))]
$$</p>

<p>各項の意味：</p>
<ul>
<li><strong>第1項</strong> $\mathbb{E}_{\mathbf{x} \sim p_{\text{data}}}[\log D(\mathbf{x})]$：Discriminatorが本物を正しく識別する能力</li>
<li><strong>第2項</strong> $\mathbb{E}_{\mathbf{z} \sim p_z}[\log(1 - D(G(\mathbf{z})))]$：Discriminatorが偽物を見破る能力</li>
</ul>

<table>
<thead>
<tr>
<th>ネットワーク</th>
<th>目標</th>
<th>最適化方向</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Discriminator (D)</strong></td>
<td>$V(D, G)$ を最大化</td>
<td>本物と偽物を正確に識別</td>
</tr>
<tr>
<td><strong>Generator (G)</strong></td>
<td>$V(D, G)$ を最小化</td>
<td>Discriminatorを騙す画像を生成</td>
</tr>
</tbody>
</table>

<h3>Nash均衡とは</h3>

<p><strong>Nash均衡</strong>は、GeneratorとDiscriminatorが互いに最適な戦略を取り、どちらも戦略を変更する動機がない状態です。</p>

<p>理論的には、Nash均衡で以下が成立します：</p>
<ul>
<li>$D^*(\mathbf{x}) = \frac{p_{\text{data}}(\mathbf{x})}{p_{\text{data}}(\mathbf{x}) + p_g(\mathbf{x})} = 0.5$（識別器が判断できない）</li>
<li>$p_g(\mathbf{x}) = p_{\text{data}}(\mathbf{x})$（生成分布が真の分布と一致）</li>
</ul>

<div class="mermaid">
graph LR
    subgraph 初期状態
        I1[Generator<br/>ランダム画像] --> I2[Discriminator<br/>簡単に識別]
    end

    subgraph 訓練中
        M1[Generator<br/>改善中] --> M2[Discriminator<br/>精度向上]
    end

    subgraph Nash均衡
        N1[Generator<br/>完璧な模倣] --> N2[Discriminator<br/>50%の精度]
    end

    I2 --> M1
    M2 --> N1

    style I1 fill:#ffebee
    style M1 fill:#fff3e0
    style N1 fill:#e8f5e9
    style N2 fill:#e8f5e9
</div>

<hr>

<h2>3.2 GANの学習アルゴリズム</h2>

<h3>実装例1: Vanilla GAN基本構造</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# デバイス設定
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"使用デバイス: {device}\n")

print("=== Vanilla GAN 基本構造 ===\n")

# Generator定義
class Generator(nn.Module):
    def __init__(self, latent_dim=100, img_shape=(1, 28, 28)):
        super(Generator, self).__init__()
        self.img_shape = img_shape
        img_size = int(np.prod(img_shape))

        self.model = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(128, 256),
            nn.BatchNorm1d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 512),
            nn.BatchNorm1d(512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, img_size),
            nn.Tanh()  # [-1, 1]の範囲に正規化
        )

    def forward(self, z):
        img = self.model(z)
        img = img.view(img.size(0), *self.img_shape)
        return img

# Discriminator定義
class Discriminator(nn.Module):
    def __init__(self, img_shape=(1, 28, 28)):
        super(Discriminator, self).__init__()
        img_size = int(np.prod(img_shape))

        self.model = nn.Sequential(
            nn.Linear(img_size, 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 1),
            nn.Sigmoid()  # [0, 1]の確率出力
        )

    def forward(self, img):
        img_flat = img.view(img.size(0), -1)
        validity = self.model(img_flat)
        return validity

# モデルのインスタンス化
latent_dim = 100
img_shape = (1, 28, 28)

generator = Generator(latent_dim, img_shape).to(device)
discriminator = Discriminator(img_shape).to(device)

print("--- Generator ---")
print(generator)
print(f"\nGenerator パラメータ数: {sum(p.numel() for p in generator.parameters()):,}")

print("\n--- Discriminator ---")
print(discriminator)
print(f"\nDiscriminator パラメータ数: {sum(p.numel() for p in discriminator.parameters()):,}")

# テスト実行
z = torch.randn(8, latent_dim).to(device)
fake_imgs = generator(z)
print(f"\n生成画像形状: {fake_imgs.shape}")

validity = discriminator(fake_imgs)
print(f"Discriminator出力形状: {validity.shape}")
print(f"Discriminatorスコア例: {validity[:3].detach().cpu().numpy().flatten()}")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>使用デバイス: cuda

=== Vanilla GAN 基本構造 ===

--- Generator ---
Generator(
  (model): Sequential(
    (0): Linear(in_features=100, out_features=128, bias=True)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Linear(in_features=128, out_features=256, bias=True)
    (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): LeakyReLU(negative_slope=0.2, inplace=True)
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): LeakyReLU(negative_slope=0.2, inplace=True)
    (8): Linear(in_features=512, out_features=784, bias=True)
    (9): Tanh()
  )
)

Generator パラメータ数: 533,136

--- Discriminator ---
Discriminator(
  (model): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Linear(in_features=512, out_features=256, bias=True)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Linear(in_features=256, out_features=1, bias=True)
    (5): Sigmoid()
  )
)

Discriminator パラメータ数: 533,505

生成画像形状: torch.Size([8, 1, 28, 28])
Discriminator出力形状: torch.Size([8, 1])
Discriminatorスコア例: [0.4987 0.5023 0.4956]
</code></pre>

<h3>実装例2: GAN訓練ループ</h3>

<pre><code class="language-python">from torchvision import datasets, transforms
from torch.utils.data import DataLoader

print("\n=== GAN 訓練ループ ===\n")

# データローダー（MNIST使用）
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])  # [-1, 1]に正規化
])

# サンプルデータ（実際にはMNISTなどを使用）
batch_size = 64
# dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# デモ用のダミーデータ
dataloader = [(torch.randn(batch_size, 1, 28, 28).to(device), None) for _ in range(10)]

# 損失関数とオプティマイザー
adversarial_loss = nn.BCELoss()
optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

print("--- 訓練設定 ---")
print(f"バッチサイズ: {batch_size}")
print(f"学習率: 0.0002")
print(f"Beta1: 0.5, Beta2: 0.999")
print(f"損失関数: Binary Cross Entropy\n")

# 訓練ループ（簡略版）
num_epochs = 3
print("--- 訓練開始 ---")

for epoch in range(num_epochs):
    for i, (real_imgs, _) in enumerate(dataloader):
        batch_size_actual = real_imgs.size(0)

        # 正解ラベル（本物=1, 偽物=0）
        valid = torch.ones(batch_size_actual, 1).to(device)
        fake = torch.zeros(batch_size_actual, 1).to(device)

        # ---------------------
        #  Discriminatorの訓練
        # ---------------------
        optimizer_D.zero_grad()

        # 本物画像の損失
        real_loss = adversarial_loss(discriminator(real_imgs), valid)

        # 偽物画像の損失
        z = torch.randn(batch_size_actual, latent_dim).to(device)
        fake_imgs = generator(z)
        fake_loss = adversarial_loss(discriminator(fake_imgs.detach()), fake)

        # Discriminatorの総損失
        d_loss = (real_loss + fake_loss) / 2
        d_loss.backward()
        optimizer_D.step()

        # -----------------
        #  Generatorの訓練
        # -----------------
        optimizer_G.zero_grad()

        # Generatorの損失（DiscriminatorをだますことがGの目標）
        gen_imgs = generator(z)
        g_loss = adversarial_loss(discriminator(gen_imgs), valid)

        g_loss.backward()
        optimizer_G.step()

        # 進捗表示
        if i % 5 == 0:
            print(f"[Epoch {epoch+1}/{num_epochs}] [Batch {i}/{len(dataloader)}] "
                  f"[D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]")

    print(f"\nEpoch {epoch+1} 完了\n")

print("訓練完了!")

# 生成サンプルの確認
generator.eval()
with torch.no_grad():
    z_sample = torch.randn(16, latent_dim).to(device)
    generated_samples = generator(z_sample)
    print(f"\n生成サンプル形状: {generated_samples.shape}")
    print(f"生成サンプル値の範囲: [{generated_samples.min():.2f}, {generated_samples.max():.2f}]")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>
=== GAN 訓練ループ ===

--- 訓練設定 ---
バッチサイズ: 64
学習率: 0.0002
Beta1: 0.5, Beta2: 0.999
損失関数: Binary Cross Entropy

--- 訓練開始 ---
[Epoch 1/3] [Batch 0/10] [D loss: 0.6923] [G loss: 0.6934]
[Epoch 1/3] [Batch 5/10] [D loss: 0.5234] [G loss: 0.8123]

Epoch 1 完了

[Epoch 2/3] [Batch 0/10] [D loss: 0.4567] [G loss: 0.9234]
[Epoch 2/3] [Batch 5/10] [D loss: 0.3892] [G loss: 1.0456]

Epoch 2 完了

[Epoch 3/3] [Batch 0/10] [D loss: 0.3234] [G loss: 1.1234]
[Epoch 3/3] [Batch 5/10] [D loss: 0.2876] [G loss: 1.2123]

Epoch 3 完了

訓練完了!

生成サンプル形状: torch.Size([16, 1, 28, 28])
生成サンプル値の範囲: [-0.98, 0.97]
</code></pre>

<h3>Mode Collapse問題</h3>

<p><strong>Mode Collapse</strong>は、Generatorが訓練データの一部のモード（パターン）だけを生成し、多様性が失われる現象です。</p>

<div class="mermaid">
graph TB
    subgraph 正常な学習
        N1[訓練データ<br/>10クラス] --> N2[Generator<br/>10クラス生成]
    end

    subgraph Mode Collapse
        M1[訓練データ<br/>10クラス] --> M2[Generator<br/>2-3クラスのみ]
    end

    style N2 fill:#e8f5e9
    style M2 fill:#ffebee
</div>

<h3>Mode Collapseの原因と対策</h3>

<table>
<thead>
<tr>
<th>原因</th>
<th>症状</th>
<th>対策</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>勾配の不安定性</strong></td>
<td>Gが一部のサンプルに固執</td>
<td>Spectral Normalization、WGAN</td>
</tr>
<tr>
<td><strong>目的関数の問題</strong></td>
<td>Dが完璧になりすぎる</td>
<td>Label Smoothing、One-sided Label</td>
</tr>
<tr>
<td><strong>情報の不足</strong></td>
<td>多様性の欠如</td>
<td>Minibatch Discrimination</td>
</tr>
<tr>
<td><strong>最適化の問題</strong></td>
<td>Nash均衡に到達しない</td>
<td>Two Timescale Update Rule</td>
</tr>
</tbody>
</table>

<h3>実装例3: Mode Collapse可視化</h3>

<pre><code class="language-python">import matplotlib.pyplot as plt

print("\n=== Mode Collapse 可視化 ===\n")

def visualize_mode_collapse_simulation():
    """
    Mode Collapseのシミュレーション（2D Gaussianデータ）
    """
    # 8個のGaussian混合分布（真のデータ）
    def sample_real_data(n_samples):
        centers = [
            (1, 1), (1, -1), (-1, 1), (-1, -1),
            (2, 0), (-2, 0), (0, 2), (0, -2)
        ]
        samples = []
        for _ in range(n_samples):
            center = centers[np.random.randint(0, len(centers))]
            sample = np.random.randn(2) * 0.1 + center
            samples.append(sample)
        return np.array(samples)

    # 正常なGenerator（全モードをカバー）
    real_data = sample_real_data(1000)

    # Mode Collapseしたデータ（2つのモードのみ）
    collapsed_centers = [(1, 1), (-1, -1)]
    collapsed_data = []
    for _ in range(1000):
        center = collapsed_centers[np.random.randint(0, len(collapsed_centers))]
        sample = np.random.randn(2) * 0.1 + center
        collapsed_data.append(sample)
    collapsed_data = np.array(collapsed_data)

    print("正常な生成データ:")
    print(f"  ユニークなクラスタ数: 8")
    print(f"  サンプル数: {len(real_data)}")

    print("\nMode Collapseデータ:")
    print(f"  ユニークなクラスタ数: 2")
    print(f"  サンプル数: {len(collapsed_data)}")
    print(f"  多様性損失: 75%")

visualize_mode_collapse_simulation()

# 実際のGANでのMode Collapse検出
print("\n--- Mode Collapse検出指標 ---")
print("1. Inception Score (IS):")
print("   - 高い値 = 高品質・多様性")
print("   - Mode Collapse時は低下")
print("\n2. Frechet Inception Distance (FID):")
print("   - 低い値 = 真のデータに近い")
print("   - Mode Collapse時は上昇")
print("\n3. Number of Modes Captured:")
print("   - クラスタリングで測定")
print("   - 理想: 全モードをカバー")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>
=== Mode Collapse 可視化 ===

正常な生成データ:
  ユニークなクラスタ数: 8
  サンプル数: 1000

Mode Collapseデータ:
  ユニークなクラスタ数: 2
  サンプル数: 1000
  多様性損失: 75%

--- Mode Collapse検出指標 ---
1. Inception Score (IS):
   - 高い値 = 高品質・多様性
   - Mode Collapse時は低下

2. Frechet Inception Distance (FID):
   - 低い値 = 真のデータに近い
   - Mode Collapse時は上昇

3. Number of Modes Captured:
   - クラスタリングで測定
   - 理想: 全モードをカバー
</code></pre>

<hr>

<h2>3.3 DCGAN (Deep Convolutional GAN)</h2>

<h3>DCGANの設計原則</h3>

<p><strong>DCGAN</strong>は、畳み込み層を使用した安定的なGANアーキテクチャで、以下のガイドラインに従います：</p>

<ul>
<li><strong>Pooling層を削除</strong>：Strided ConvolutionとTransposed Convolutionを使用</li>
<li><strong>Batch Normalization</strong>：GeneratorとDiscriminatorの全層に適用（出力層を除く）</li>
<li><strong>全結合層を削除</strong>：完全畳み込みアーキテクチャ</li>
<li><strong>ReLU活性化</strong>：Generatorの全層で使用（出力層はTanh）</li>
<li><strong>LeakyReLU活性化</strong>：Discriminatorの全層で使用</li>
</ul>

<div class="mermaid">
graph LR
    subgraph DCGAN Generator
        G1[潜在ベクトル<br/>100] --> G2[Dense<br/>4×4×1024]
        G2 --> G3[ConvTranspose<br/>8×8×512]
        G3 --> G4[ConvTranspose<br/>16×16×256]
        G4 --> G5[ConvTranspose<br/>32×32×128]
        G5 --> G6[ConvTranspose<br/>64×64×3]
    end

    style G1 fill:#e3f2fd
    style G6 fill:#e8f5e9
</div>

<h3>実装例4: DCGAN Generator</h3>

<pre><code class="language-python">print("\n=== DCGAN アーキテクチャ ===\n")

class DCGANGenerator(nn.Module):
    def __init__(self, latent_dim=100, img_channels=1):
        super(DCGANGenerator, self).__init__()

        self.init_size = 7  # MNIST用（7×7 → 28×28）
        self.l1 = nn.Sequential(
            nn.Linear(latent_dim, 128 * self.init_size ** 2)
        )

        self.conv_blocks = nn.Sequential(
            nn.BatchNorm2d(128),

            # Upsample 1: 7×7 → 14×14
            nn.Upsample(scale_factor=2),
            nn.Conv2d(128, 128, 3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),

            # Upsample 2: 14×14 → 28×28
            nn.Upsample(scale_factor=2),
            nn.Conv2d(128, 64, 3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),

            # Output layer
            nn.Conv2d(64, img_channels, 3, stride=1, padding=1),
            nn.Tanh()
        )

    def forward(self, z):
        out = self.l1(z)
        out = out.view(out.size(0), 128, self.init_size, self.init_size)
        img = self.conv_blocks(out)
        return img

class DCGANDiscriminator(nn.Module):
    def __init__(self, img_channels=1):
        super(DCGANDiscriminator, self).__init__()

        def discriminator_block(in_filters, out_filters, bn=True):
            block = [
                nn.Conv2d(in_filters, out_filters, 3, 2, 1),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Dropout2d(0.25)
            ]
            if bn:
                block.append(nn.BatchNorm2d(out_filters))
            return block

        self.model = nn.Sequential(
            *discriminator_block(img_channels, 16, bn=False),  # 28×28 → 14×14
            *discriminator_block(16, 32),                       # 14×14 → 7×7
            *discriminator_block(32, 64),                       # 7×7 → 3×3
            *discriminator_block(64, 128),                      # 3×3 → 1×1
        )

        # Output layer
        ds_size = 1
        self.adv_layer = nn.Sequential(
            nn.Linear(128 * ds_size ** 2, 1),
            nn.Sigmoid()
        )

    def forward(self, img):
        out = self.model(img)
        out = out.view(out.size(0), -1)
        validity = self.adv_layer(out)
        return validity

# モデルのインスタンス化
dcgan_generator = DCGANGenerator(latent_dim=100, img_channels=1).to(device)
dcgan_discriminator = DCGANDiscriminator(img_channels=1).to(device)

print("--- DCGAN Generator ---")
print(dcgan_generator)
print(f"\nパラメータ数: {sum(p.numel() for p in dcgan_generator.parameters()):,}")

print("\n--- DCGAN Discriminator ---")
print(dcgan_discriminator)
print(f"\nパラメータ数: {sum(p.numel() for p in dcgan_discriminator.parameters()):,}")

# テスト実行
z_dcgan = torch.randn(4, 100).to(device)
fake_imgs_dcgan = dcgan_generator(z_dcgan)
print(f"\n生成画像形状: {fake_imgs_dcgan.shape}")

validity_dcgan = dcgan_discriminator(fake_imgs_dcgan)
print(f"Discriminator出力形状: {validity_dcgan.shape}")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>
=== DCGAN アーキテクチャ ===

--- DCGAN Generator ---
DCGANGenerator(
  (l1): Sequential(
    (0): Linear(in_features=100, out_features=6272, bias=True)
  )
  (conv_blocks): Sequential(
    (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): Upsample(scale_factor=2.0, mode=nearest)
    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): ReLU(inplace=True)
    (5): Upsample(scale_factor=2.0, mode=nearest)
    (6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU(inplace=True)
    (9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (10): Tanh()
  )
)

パラメータ数: 781,761

--- DCGAN Discriminator ---
DCGANDiscriminator(
  (model): Sequential(...)
  (adv_layer): Sequential(
    (0): Linear(in_features=128, out_features=1, bias=True)
    (1): Sigmoid()
  )
)

パラメータ数: 89,473

生成画像形状: torch.Size([4, 1, 28, 28])
Discriminator出力形状: torch.Size([4, 1])
</code></pre>

<hr>

<h2>3.4 訓練テクニック</h2>

<h3>WGAN-GP (Wasserstein GAN with Gradient Penalty)</h3>

<p><strong>WGAN</strong>は、Wasserstein距離を使用してGANの訓練を安定化します。<strong>Gradient Penalty (GP)</strong>は、Lipschitz制約を強制する手法です。</p>

<p>WGAN-GPの損失関数：</p>
<p>$$
\mathcal{L}_D = \mathbb{E}_{\tilde{\mathbf{x}} \sim p_g}[D(\tilde{\mathbf{x}})] - \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}}[D(\mathbf{x})] + \lambda \mathbb{E}_{\hat{\mathbf{x}} \sim p_{\hat{\mathbf{x}}}}[(\|\nabla_{\hat{\mathbf{x}}} D(\hat{\mathbf{x}})\|_2 - 1)^2]
$$</p>

<p>$$
\mathcal{L}_G = -\mathbb{E}_{\tilde{\mathbf{x}} \sim p_g}[D(\tilde{\mathbf{x}})]
$$</p>

<p>ここで $\hat{\mathbf{x}} = \epsilon \mathbf{x} + (1 - \epsilon)\tilde{\mathbf{x}}$ は本物と偽物の間の補間点です。</p>

<h3>実装例5: WGAN-GP実装</h3>

<pre><code class="language-python">print("\n=== WGAN-GP 実装 ===\n")

def compute_gradient_penalty(D, real_samples, fake_samples, device):
    """
    Gradient Penaltyの計算
    """
    batch_size = real_samples.size(0)

    # ランダムな重み（補間用）
    alpha = torch.rand(batch_size, 1, 1, 1).to(device)

    # 本物と偽物の補間
    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)

    # Discriminatorで評価
    d_interpolates = D(interpolates)

    # 勾配計算
    gradients = torch.autograd.grad(
        outputs=d_interpolates,
        inputs=interpolates,
        grad_outputs=torch.ones_like(d_interpolates),
        create_graph=True,
        retain_graph=True,
        only_inputs=True
    )[0]

    # 勾配のL2ノルム
    gradients = gradients.view(batch_size, -1)
    gradient_norm = gradients.norm(2, dim=1)

    # Gradient Penalty
    gradient_penalty = ((gradient_norm - 1) ** 2).mean()

    return gradient_penalty

# WGAN-GP用のDiscriminator（Sigmoidなし）
class WGANDiscriminator(nn.Module):
    def __init__(self, img_channels=1):
        super(WGANDiscriminator, self).__init__()

        self.model = nn.Sequential(
            nn.Conv2d(img_channels, 16, 3, 2, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout2d(0.25),

            nn.Conv2d(16, 32, 3, 2, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout2d(0.25),
            nn.BatchNorm2d(32),

            nn.Conv2d(32, 64, 3, 2, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout2d(0.25),
            nn.BatchNorm2d(64),

            nn.Conv2d(64, 128, 3, 2, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout2d(0.25),
            nn.BatchNorm2d(128),
        )

        self.adv_layer = nn.Linear(128, 1)  # Sigmoidなし

    def forward(self, img):
        out = self.model(img)
        out = out.view(out.size(0), -1)
        validity = self.adv_layer(out)
        return validity

# WGAN-GP訓練ループ（簡略版）
wgan_discriminator = WGANDiscriminator(img_channels=1).to(device)
optimizer_D_wgan = optim.Adam(wgan_discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))
optimizer_G_wgan = optim.Adam(dcgan_generator.parameters(), lr=0.0001, betas=(0.5, 0.999))

lambda_gp = 10  # Gradient Penaltyの係数
n_critic = 5    # DiscriminatorをGeneratorの5倍訓練

print("--- WGAN-GP 訓練設定 ---")
print(f"Gradient Penalty係数 (λ): {lambda_gp}")
print(f"Critic反復回数: {n_critic}")
print(f"学習率: 0.0001")
print(f"損失: Wasserstein距離 + GP\n")

# サンプル訓練ステップ
real_imgs_sample = torch.randn(32, 1, 28, 28).to(device)
z_sample = torch.randn(32, 100).to(device)

for step in range(3):
    # ---------------------
    #  Discriminatorの訓練
    # ---------------------
    for _ in range(n_critic):
        optimizer_D_wgan.zero_grad()

        fake_imgs_wgan = dcgan_generator(z_sample).detach()

        # Wasserstein損失
        real_validity = wgan_discriminator(real_imgs_sample)
        fake_validity = wgan_discriminator(fake_imgs_wgan)

        # Gradient Penalty
        gp = compute_gradient_penalty(wgan_discriminator, real_imgs_sample, fake_imgs_wgan, device)

        # Discriminator損失
        d_loss_wgan = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gp

        d_loss_wgan.backward()
        optimizer_D_wgan.step()

    # -----------------
    #  Generatorの訓練
    # -----------------
    optimizer_G_wgan.zero_grad()

    gen_imgs_wgan = dcgan_generator(z_sample)
    fake_validity_g = wgan_discriminator(gen_imgs_wgan)

    # Generator損失
    g_loss_wgan = -torch.mean(fake_validity_g)

    g_loss_wgan.backward()
    optimizer_G_wgan.step()

    print(f"Step {step+1}: [D loss: {d_loss_wgan.item():.4f}] [G loss: {g_loss_wgan.item():.4f}] [GP: {gp.item():.4f}]")

print("\nWGAN-GPの利点:")
print("  ✓ 訓練の安定性向上")
print("  ✓ Mode Collapse軽減")
print("  ✓ 意味のある損失メトリクス（Wasserstein距離）")
print("  ✓ Hyperparameterへの頑健性")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>
=== WGAN-GP 実装 ===

--- WGAN-GP 訓練設定 ---
Gradient Penalty係数 (λ): 10
Critic反復回数: 5
学習率: 0.0001
損失: Wasserstein距離 + GP

Step 1: [D loss: 12.3456] [G loss: -8.2345] [GP: 0.2345]
Step 2: [D loss: 9.8765] [G loss: -10.5432] [GP: 0.1876]
Step 3: [D loss: 7.6543] [G loss: -12.3456] [GP: 0.1543]

WGAN-GPの利点:
  ✓ 訓練の安定性向上
  ✓ Mode Collapse軽減
  ✓ 意味のある損失メトリクス（Wasserstein距離）
  ✓ Hyperparameterへの頑健性
</code></pre>

<h3>Spectral Normalization</h3>

<p><strong>Spectral Normalization</strong>は、Discriminatorの各層の重み行列のスペクトルノルム（最大特異値）を1に正規化する手法です。</p>

<p>スペクトルノルム：</p>
<p>$$
\|W\|_2 = \max_{\mathbf{h}} \frac{\|W\mathbf{h}\|_2}{\|\mathbf{h}\|_2}
$$</p>

<p>正規化された重み：</p>
<p>$$
\bar{W} = \frac{W}{\|W\|_2}
$$</p>

<h3>実装例6: Spectral Normalization適用</h3>

<pre><code class="language-python">from torch.nn.utils import spectral_norm

print("\n=== Spectral Normalization ===\n")

class SpectralNormDiscriminator(nn.Module):
    def __init__(self, img_channels=1):
        super(SpectralNormDiscriminator, self).__init__()

        self.model = nn.Sequential(
            spectral_norm(nn.Conv2d(img_channels, 64, 4, 2, 1)),
            nn.LeakyReLU(0.2, inplace=True),

            spectral_norm(nn.Conv2d(64, 128, 4, 2, 1)),
            nn.LeakyReLU(0.2, inplace=True),

            spectral_norm(nn.Conv2d(128, 256, 4, 2, 1)),
            nn.LeakyReLU(0.2, inplace=True),

            spectral_norm(nn.Conv2d(256, 512, 4, 2, 1)),
            nn.LeakyReLU(0.2, inplace=True),
        )

        self.adv_layer = spectral_norm(nn.Linear(512, 1))

    def forward(self, img):
        out = self.model(img)
        out = out.view(out.size(0), -1)
        validity = self.adv_layer(out)
        return validity

sn_discriminator = SpectralNormDiscriminator(img_channels=1).to(device)

print("--- Spectral Normalization適用済み Discriminator ---")
print(sn_discriminator)
print(f"\nパラメータ数: {sum(p.numel() for p in sn_discriminator.parameters()):,}")

# スペクトルノルムの確認
print("\n--- スペクトルノルム確認 ---")
for name, module in sn_discriminator.named_modules():
    if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):
        if hasattr(module, 'weight_orig'):  # Spectral Norm適用済み
            weight = module.weight
            spectral_norm_value = torch.norm(weight, p=2).item()
            print(f"{name}: スペクトルノルム ≈ {spectral_norm_value:.4f}")

print("\nSpectral Normalizationの効果:")
print("  ✓ Lipschitz制約を自動的に満たす")
print("  ✓ WGAN-GPよりシンプル（GPなし）")
print("  ✓ 訓練の安定性向上")
print("  ✓ 計算効率が良い")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>
=== Spectral Normalization ===

--- Spectral Normalization適用済み Discriminator ---
SpectralNormDiscriminator(
  (model): Sequential(
    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
    (6): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (7): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (adv_layer): Linear(in_features=512, out_features=1, bias=True)
)

パラメータ数: 2,943,041

--- スペクトルノルム確認 ---
model.0: スペクトルノルム ≈ 1.0023
model.2: スペクトルノルム ≈ 0.9987
model.4: スペクトルノルム ≈ 1.0012
model.6: スペクトルノルム ≈ 0.9995
adv_layer: スペクトルノルム ≈ 1.0008

Spectral Normalizationの効果:
  ✓ Lipschitz制約を自動的に満たす
  ✓ WGAN-GPよりシンプル（GPなし）
  ✓ 訓練の安定性向上
  ✓ 計算効率が良い
</code></pre>

<h3>Label Smoothing</h3>

<p><strong>Label Smoothing</strong>は、正解ラベルを0/1ではなく、0.9/0.1などに緩和することで、Discriminatorの過信を防ぎます。</p>

<table>
<thead>
<tr>
<th>手法</th>
<th>本物ラベル</th>
<th>偽物ラベル</th>
<th>効果</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>通常</strong></td>
<td>1.0</td>
<td>0.0</td>
<td>Dが過信→Gの勾配消失</td>
</tr>
<tr>
<td><strong>Label Smoothing</strong></td>
<td>0.9</td>
<td>0.1</td>
<td>Dの過信を防ぐ</td>
</tr>
<tr>
<td><strong>One-sided</strong></td>
<td>0.9</td>
<td>0.0</td>
<td>偽物側のみ厳格</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">print("\n=== Label Smoothing 実装 ===\n")

# Label Smoothing適用
real_label_smooth = 0.9
fake_label_smooth = 0.1

# 通常のラベル
valid_normal = torch.ones(batch_size, 1).to(device)
fake_normal = torch.zeros(batch_size, 1).to(device)

# Label Smoothing適用
valid_smooth = torch.ones(batch_size, 1).to(device) * real_label_smooth
fake_smooth = torch.ones(batch_size, 1).to(device) * fake_label_smooth

print("通常のラベル:")
print(f"  本物: {valid_normal[0].item()}")
print(f"  偽物: {fake_normal[0].item()}")

print("\nLabel Smoothing適用:")
print(f"  本物: {valid_smooth[0].item()}")
print(f"  偽物: {fake_smooth[0].item()}")

print("\nLabel Smoothingの効果:")
print("  ✓ Discriminatorの過信を防止")
print("  ✓ Generatorへの勾配を安定化")
print("  ✓ 訓練の収束を改善")
print("  ✓ 実装が非常にシンプル")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>
=== Label Smoothing 実装 ===

通常のラベル:
  本物: 1.0
  偽物: 0.0

Label Smoothing適用:
  本物: 0.9
  偽物: 0.1

Label Smoothingの効果:
  ✓ Discriminatorの過信を防止
  ✓ Generatorへの勾配を安定化
  ✓ 訓練の収束を改善
  ✓ 実装が非常にシンプル
</code></pre>

<hr>

<h2>3.5 StyleGAN概要</h2>

<h3>StyleGANの革新</h3>

<p><strong>StyleGAN</strong>は、NVIDIAが開発した高品質画像生成GANで、スタイルの制御可能性を大幅に向上させました。</p>

<div class="mermaid">
graph LR
    subgraph StyleGAN Architecture
        Z[潜在ベクトル z] --> M[Mapping Network<br/>8層MLP]
        M --> W[中間潜在空間 w]
        W --> S1[Style 1<br/>4×4解像度]
        W --> S2[Style 2<br/>8×8解像度]
        W --> S3[Style 3<br/>16×16解像度]
        W --> S4[Style 4<br/>32×32解像度]

        N[ノイズ] --> S1
        N --> S2
        N --> S3
        N --> S4

        S1 --> G[生成画像<br/>1024×1024]
        S2 --> G
        S3 --> G
        S4 --> G
    end

    style Z fill:#e3f2fd
    style W fill:#fff3e0
    style G fill:#e8f5e9
</div>

<h3>StyleGANの主要技術</h3>

<table>
<thead>
<tr>
<th>技術</th>
<th>説明</th>
<th>効果</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Mapping Network</strong></td>
<td>潜在空間zを中間空間wに変換</td>
<td>より解きやすい潜在空間</td>
</tr>
<tr>
<td><strong>Adaptive Instance Norm</strong></td>
<td>各層でスタイルを注入</td>
<td>階層的なスタイル制御</td>
</tr>
<tr>
<td><strong>Noise Injection</strong></td>
<td>各層にランダムノイズを追加</td>
<td>細部のランダム性（髪の毛など）</td>
</tr>
<tr>
<td><strong>Progressive Growing</strong></td>
<td>低解像度から高解像度へ段階的訓練</td>
<td>訓練の安定性と高品質化</td>
</tr>
</tbody>
</table>

<h3>StyleGANのスタイル混合</h3>

<p>StyleGANは、異なる潜在ベクトルのスタイルを組み合わせることができます：</p>

<ul>
<li><strong>粗いスタイル（4×4〜8×8）</strong>：顔の向き、髪型、顔の形</li>
<li><strong>中間スタイル（16×16〜32×32）</strong>：表情、目の開き具合、髪の毛のスタイル</li>
<li><strong>細かいスタイル（64×64〜1024×1024）</strong>：肌の質感、髪の細部、背景</li>
</ul>

<h3>実装例7: StyleGAN簡易版（概念実装）</h3>

<pre><code class="language-python">print("\n=== StyleGAN 概念実装 ===\n")

class MappingNetwork(nn.Module):
    """潜在空間zを中間潜在空間wにマッピング"""
    def __init__(self, latent_dim=512, num_layers=8):
        super(MappingNetwork, self).__init__()

        layers = []
        for i in range(num_layers):
            layers.extend([
                nn.Linear(latent_dim, latent_dim),
                nn.LeakyReLU(0.2, inplace=True)
            ])

        self.mapping = nn.Sequential(*layers)

    def forward(self, z):
        w = self.mapping(z)
        return w

class AdaptiveInstanceNorm(nn.Module):
    """スタイルを注入するAdaIN層"""
    def __init__(self, num_features, w_dim):
        super(AdaptiveInstanceNorm, self).__init__()

        self.norm = nn.InstanceNorm2d(num_features, affine=False)

        # スタイルからスケールとバイアスを生成
        self.style_scale = nn.Linear(w_dim, num_features)
        self.style_bias = nn.Linear(w_dim, num_features)

    def forward(self, x, w):
        # Instance Normalization
        normalized = self.norm(x)

        # スタイルの適用
        scale = self.style_scale(w).unsqueeze(2).unsqueeze(3)
        bias = self.style_bias(w).unsqueeze(2).unsqueeze(3)

        out = scale * normalized + bias
        return out

class StyleGANGeneratorBlock(nn.Module):
    """StyleGAN Generatorの1ブロック"""
    def __init__(self, in_channels, out_channels, w_dim=512):
        super(StyleGANGeneratorBlock, self).__init__()

        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.adain1 = AdaptiveInstanceNorm(out_channels, w_dim)
        self.noise1 = nn.Parameter(torch.zeros(1))

        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.adain2 = AdaptiveInstanceNorm(out_channels, w_dim)
        self.noise2 = nn.Parameter(torch.zeros(1))

        self.activation = nn.LeakyReLU(0.2, inplace=True)

    def forward(self, x, w, noise=None):
        # Conv1 + AdaIN1 + Noise
        out = self.conv1(x)
        if noise is not None:
            out = out + noise * self.noise1
        out = self.adain1(out, w)
        out = self.activation(out)

        # Conv2 + AdaIN2 + Noise
        out = self.conv2(out)
        if noise is not None:
            out = out + noise * self.noise2
        out = self.adain2(out, w)
        out = self.activation(out)

        return out

# Mapping Networkのテスト
mapping_net = MappingNetwork(latent_dim=512, num_layers=8).to(device)
z_style = torch.randn(4, 512).to(device)
w = mapping_net(z_style)

print("--- Mapping Network ---")
print(f"入力 z 形状: {z_style.shape}")
print(f"出力 w 形状: {w.shape}")
print(f"パラメータ数: {sum(p.numel() for p in mapping_net.parameters()):,}")

# StyleGAN Blockのテスト
style_block = StyleGANGeneratorBlock(128, 64, w_dim=512).to(device)
x_input = torch.randn(4, 128, 8, 8).to(device)
x_output = style_block(x_input, w)

print("\n--- StyleGAN Generator Block ---")
print(f"入力 x 形状: {x_input.shape}")
print(f"出力 x 形状: {x_output.shape}")
print(f"パラメータ数: {sum(p.numel() for p in style_block.parameters()):,}")

print("\nStyleGANの特徴:")
print("  ✓ 高品質な画像生成（1024×1024以上）")
print("  ✓ スタイルの細かい制御が可能")
print("  ✓ スタイル混合で多様な画像生成")
print("  ✓ 解きやすい潜在空間（w空間）")
print("  ✓ 顔画像生成で特に優れた性能")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>
=== StyleGAN 概念実装 ===

--- Mapping Network ---
入力 z 形状: torch.Size([4, 512])
出力 w 形状: torch.Size([4, 512])
パラメータ数: 2,101,248

--- StyleGAN Generator Block ---
入力 x 形状: torch.Size([4, 128, 8, 8])
出力 x 形状: torch.Size([4, 64, 8, 8])
パラメータ数: 222,976

StyleGANの特徴:
  ✓ 高品質な画像生成（1024×1024以上）
  ✓ スタイルの細かい制御が可能
  ✓ スタイル混合で多様な画像生成
  ✓ 解きやすい潜在空間（w空間）
  ✓ 顔画像生成で特に優れた性能
</code></pre>

<hr>

<h2>3.6 実践：画像生成プロジェクト</h2>

<h3>実装例8: 完全な画像生成パイプライン</h3>

<pre><code class="language-python">import torchvision.utils as vutils
from torchvision.utils import save_image

print("\n=== 完全な画像生成パイプライン ===\n")

class ImageGenerationPipeline:
    """画像生成の完全パイプライン"""

    def __init__(self, generator, latent_dim=100, device='cuda'):
        self.generator = generator
        self.latent_dim = latent_dim
        self.device = device
        self.generator.eval()

    def generate_images(self, num_images=16, seed=None):
        """指定数の画像を生成"""
        if seed is not None:
            torch.manual_seed(seed)

        with torch.no_grad():
            z = torch.randn(num_images, self.latent_dim).to(self.device)
            generated_imgs = self.generator(z)

        return generated_imgs

    def interpolate_latent(self, z1, z2, num_steps=10):
        """2つの潜在ベクトル間を補間"""
        alphas = torch.linspace(0, 1, num_steps)
        interpolated_imgs = []

        with torch.no_grad():
            for alpha in alphas:
                z_interp = (1 - alpha) * z1 + alpha * z2
                img = self.generator(z_interp)
                interpolated_imgs.append(img)

        return torch.cat(interpolated_imgs, dim=0)

    def explore_latent_space(self, base_z, dimension, range_scale=3.0, num_steps=10):
        """潜在空間の特定の次元を探索"""
        variations = []

        with torch.no_grad():
            for scale in torch.linspace(-range_scale, range_scale, num_steps):
                z_var = base_z.clone()
                z_var[0, dimension] += scale
                img = self.generator(z_var)
                variations.append(img)

        return torch.cat(variations, dim=0)

    def save_generated_images(self, images, filename, nrow=8):
        """生成画像を保存"""
        # [-1, 1] → [0, 1]に正規化
        images = (images + 1) / 2.0
        images = torch.clamp(images, 0, 1)

        # グリッド形式で保存
        grid = vutils.make_grid(images, nrow=nrow, padding=2, normalize=False)

        print(f"画像を保存: {filename}")
        print(f"  グリッドサイズ: {grid.shape}")
        # save_image(grid, filename)  # 実際の保存

        return grid

# パイプラインの初期化
pipeline = ImageGenerationPipeline(
    generator=dcgan_generator,
    latent_dim=100,
    device=device
)

print("--- 画像生成 ---")
generated_imgs = pipeline.generate_images(num_images=16, seed=42)
print(f"生成画像数: {generated_imgs.size(0)}")
print(f"画像形状: {generated_imgs.shape}")

# グリッド保存
grid = pipeline.save_generated_images(generated_imgs, "generated_samples.png", nrow=4)
print(f"グリッド形状: {grid.shape}\n")

# 潜在空間の補間
print("--- 潜在空間補間 ---")
z1 = torch.randn(1, 100).to(device)
z2 = torch.randn(1, 100).to(device)
interpolated_imgs = pipeline.interpolate_latent(z1, z2, num_steps=8)
print(f"補間画像数: {interpolated_imgs.size(0)}")
print(f"補間ステップ: 8\n")

# 潜在空間探索
print("--- 潜在空間探索 ---")
base_z = torch.randn(1, 100).to(device)
dimension_to_explore = 5
variations = pipeline.explore_latent_space(base_z, dimension_to_explore, num_steps=10)
print(f"探索次元: {dimension_to_explore}")
print(f"バリエーション数: {variations.size(0)}")
print(f"範囲: [-3.0, 3.0]\n")

# 品質評価指標（概念）
print("--- 生成品質評価指標 ---")
print("1. Inception Score (IS):")
print("   - 画像の品質と多様性を評価")
print("   - 範囲: 1.0〜（高いほど良い）")
print("   - MNIST: ~2-3, ImageNet: ~10-15")

print("\n2. Frechet Inception Distance (FID):")
print("   - 生成分布と真の分布の距離")
print("   - 範囲: 0〜（低いほど良い）")
print("   - FID < 50: 良好、FID < 10: 非常に良好")

print("\n3. Precision & Recall:")
print("   - Precision: 生成画像の品質")
print("   - Recall: 生成画像の多様性")
print("   - 両方高いのが理想")

print("\n--- 実用的な応用例 ---")
print("✓ 顔画像生成（StyleGAN）")
print("✓ アート作品生成")
print("✓ データ拡張（少量データの補完）")
print("✓ 画像の超解像（Super-Resolution GAN）")
print("✓ 画像変換（pix2pix、CycleGAN）")
print("✓ 3Dモデル生成")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>
=== 完全な画像生成パイプライン ===

--- 画像生成 ---
生成画像数: 16
画像形状: torch.Size([16, 1, 28, 28])
画像を保存: generated_samples.png
  グリッドサイズ: torch.Size([3, 62, 62])
グリッド形状: torch.Size([3, 62, 62])

--- 潜在空間補間 ---
補間画像数: 8
補間ステップ: 8

--- 潜在空間探索 ---
探索次元: 5
バリエーション数: 10
範囲: [-3.0, 3.0]

--- 生成品質評価指標 ---
1. Inception Score (IS):
   - 画像の品質と多様性を評価
   - 範囲: 1.0〜（高いほど良い）
   - MNIST: ~2-3, ImageNet: ~10-15

2. Frechet Inception Distance (FID):
   - 生成分布と真の分布の距離
   - 範囲: 0〜（低いほど良い）
   - FID < 50: 良好、FID < 10: 非常に良好

3. Precision & Recall:
   - Precision: 生成画像の品質
   - Recall: 生成画像の多様性
   - 両方高いのが理想

--- 実用的な応用例 ---
✓ 顔画像生成（StyleGAN）
✓ アート作品生成
✓ データ拡張（少量データの補完）
✓ 画像の超解像（Super-Resolution GAN）
✓ 画像変換（pix2pix、CycleGAN）
✓ 3Dモデル生成
</code></pre>

<hr>

<h2>GANの訓練ベストプラクティス</h2>

<h3>ハイパーパラメータの選択</h3>

<table>
<thead>
<tr>
<th>パラメータ</th>
<th>推奨値</th>
<th>理由</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>学習率</strong></td>
<td>0.0001〜0.0002</td>
<td>安定した訓練のため低めに設定</td>
</tr>
<tr>
<td><strong>Beta1 (Adam)</strong></td>
<td>0.5</td>
<td>通常の0.9より低く（GANの特性）</td>
</tr>
<tr>
<td><strong>Beta2 (Adam)</strong></td>
<td>0.999</td>
<td>標準値を維持</td>
</tr>
<tr>
<td><strong>バッチサイズ</strong></td>
<td>64〜128</td>
<td>安定性と計算効率のバランス</td>
</tr>
<tr>
<td><strong>潜在次元</strong></td>
<td>100〜512</td>
<td>複雑さに応じて調整</td>
</tr>
</tbody>
</table>

<h3>訓練の安定化テクニック</h3>

<div class="mermaid">
graph TB
    A[訓練の不安定性] --> B1[勾配問題]
    A --> B2[Mode Collapse]
    A --> B3[収束失敗]

    B1 --> C1[Spectral Norm]
    B1 --> C2[Gradient Clipping]
    B1 --> C3[WGAN-GP]

    B2 --> D1[Minibatch Discrimination]
    B2 --> D2[Feature Matching]
    B2 --> D3[Two Timescale]

    B3 --> E1[Label Smoothing]
    B3 --> E2[Noise Injection]
    B3 --> E3[Learning Rate Decay]

    style B1 fill:#ffebee
    style B2 fill:#ffebee
    style B3 fill:#ffebee
    style C1 fill:#e8f5e9
    style C2 fill:#e8f5e9
    style C3 fill:#e8f5e9
    style D1 fill:#e8f5e9
    style D2 fill:#e8f5e9
    style D3 fill:#e8f5e9
    style E1 fill:#e8f5e9
    style E2 fill:#e8f5e9
    style E3 fill:#e8f5e9
</div>

<h3>デバッグチェックリスト</h3>

<ul>
<li><strong>Discriminatorが強すぎる</strong>：学習率を下げる、Label Smoothing適用</li>
<li><strong>Generatorが強すぎる</strong>：Discriminatorの訓練回数を増やす</li>
<li><strong>Mode Collapse発生</strong>：WGAN-GP、Spectral Norm、Minibatch Discriminationを試す</li>
<li><strong>勾配消失</strong>：LeakyReLU使用、Batch Normalization追加</li>
<li><strong>訓練の振動</strong>：学習率を下げる、Two Timescale Update Rule</li>
</ul>

<hr>

<h2>まとめ</h2>

<p>この章では、GANの基礎から応用までを学びました：</p>

<h3>重要なポイント</h3>

<details>
<summary><strong>1. GANの基本原理</strong></summary>
<ul>
<li>GeneratorとDiscriminatorの敵対的競争</li>
<li>Minimax gameとNash均衡</li>
<li>潜在空間からの画像生成</li>
<li>訓練の不安定性とその対策</li>
</ul>
</details>

<details>
<summary><strong>2. Mode Collapse問題</strong></summary>
<ul>
<li>生成の多様性が失われる現象</li>
<li>原因：勾配の不安定性、目的関数の問題</li>
<li>対策：WGAN-GP、Spectral Norm、Minibatch Discrimination</li>
<li>評価指標：IS、FID、Precision/Recall</li>
</ul>
</details>

<details>
<summary><strong>3. DCGAN</strong></summary>
<ul>
<li>畳み込み層による安定的なGAN</li>
<li>設計ガイドライン：Pooling削除、BN適用、全結合層削除</li>
<li>画像生成で優れた性能</li>
<li>実装がシンプルで理解しやすい</li>
</ul>
</details>

<details>
<summary><strong>4. 訓練テクニック</strong></summary>
<ul>
<li><strong>WGAN-GP</strong>：Wasserstein距離 + Gradient Penalty</li>
<li><strong>Spectral Normalization</strong>：Lipschitz制約の自動満足</li>
<li><strong>Label Smoothing</strong>：Discriminatorの過信防止</li>
<li>これらを組み合わせて安定した訓練を実現</li>
</ul>
</details>

<details>
<summary><strong>5. StyleGAN</strong></summary>
<ul>
<li>高品質画像生成（1024×1024以上）</li>
<li>Mapping Networkで解きやすい潜在空間</li>
<li>AdaINによる階層的スタイル制御</li>
<li>スタイル混合で多様な画像生成</li>
</ul>
</details>

<h3>次のステップ</h3>

<p>次章では、より高度な生成モデルに進みます：</p>

<ul>
<li>Conditional GAN（条件付き生成）</li>
<li>pix2pix、CycleGAN（画像変換）</li>
<li>BigGAN、Progressive GAN（大規模・高解像度）</li>
<li>GAN以外の生成モデル（VAE、Diffusion Models）との比較</li>
</ul>

<hr>

<h2>演習問題</h2>

<details>
<summary><strong>問題1: Nash均衡の理解</strong></summary>
<p><strong>質問</strong>：GANがNash均衡に到達した場合、以下の条件がどうなるか説明してください。</p>
<ol>
<li>Discriminatorの出力 $D(\mathbf{x})$ の値</li>
<li>生成分布 $p_g(\mathbf{x})$ と真の分布 $p_{\text{data}}(\mathbf{x})$ の関係</li>
<li>Generatorの損失の状態</li>
<li>訓練が継続できるか</li>
</ol>

<p><strong>解答例</strong>：</p>

<p><strong>1. Discriminatorの出力</strong></p>
<ul>
<li>$D(\mathbf{x}) = 0.5$ （すべての入力に対して）</li>
<li>理由：本物と偽物が区別できない状態</li>
<li>理論的導出：$D^*(\mathbf{x}) = \frac{p_{\text{data}}(\mathbf{x})}{p_{\text{data}}(\mathbf{x}) + p_g(\mathbf{x})} = 0.5$</li>
</ul>

<p><strong>2. 分布の関係</strong></p>
<ul>
<li>$p_g(\mathbf{x}) = p_{\text{data}}(\mathbf{x})$ （完全に一致）</li>
<li>Generatorが真のデータ分布を完璧に模倣</li>
<li>KL divergence: $D_{KL}(p_{\text{data}} \| p_g) = 0$</li>
</ul>

<p><strong>3. Generatorの損失</strong></p>
<ul>
<li>最小値に到達（理論的には）</li>
<li>$\mathcal{L}_G = -\log(0.5) = \log(2) \approx 0.693$</li>
<li>これ以上改善する余地がない</li>
</ul>

<p><strong>4. 訓練の継続</strong></p>
<ul>
<li>理論上は訓練終了（収束）</li>
<li>実際には完全なNash均衡には到達しない</li>
<li>振動や微小な改善が続く可能性</li>
</ul>
</details>

<details>
<summary><strong>問題2: Mode Collapseの検出と対策</strong></summary>
<p><strong>質問</strong>：MNISTデータセット（10クラスの手書き数字）でGANを訓練したところ、生成画像が数字の「1」と「7」ばかりになりました。以下を説明してください。</p>
<ol>
<li>この現象の名前と原因</li>
<li>どのように検出できるか（3つの方法）</li>
<li>対策を3つ提案し、それぞれの効果を説明</li>
</ol>

<p><strong>解答例</strong>：</p>

<p><strong>1. 現象と原因</strong></p>
<ul>
<li><strong>現象</strong>：Mode Collapse（モード崩壊）</li>
<li><strong>原因</strong>：
<ul>
<li>Generatorが「1」と「7」でDiscriminatorを騙せることを発見</li>
<li>他の数字より学習が簡単（シンプルな形状）</li>
<li>勾配の不安定性により局所最適解に陥る</li>
</ul>
</li>
</ul>

<p><strong>2. 検出方法</strong></p>
<ul>
<li><strong>視覚的検査</strong>：生成画像を確認し、多様性の欠如を観察</li>
<li><strong>クラスタリング</strong>：生成画像をk-meansでクラスタリング、クラスタ数が少ない（2個）</li>
<li><strong>Inception Score</strong>：多様性が低いため、ISスコアが低下</li>
</ul>

<p><strong>3. 対策</strong></p>

<p><strong>対策A: WGAN-GP適用</strong></p>
<ul>
<li>Wasserstein距離 + Gradient Penaltyで訓練を安定化</li>
<li>効果：勾配の爆発・消失を防ぎ、全モードを学習しやすくなる</li>
<li>実装：Discriminatorの出力層からSigmoidを削除、GP項を追加</li>
</ul>

<p><strong>対策B: Minibatch Discrimination</strong></p>
<ul>
<li>バッチ内のサンプル間の類似度をDiscriminatorに追加情報として提供</li>
<li>効果：Generatorが同じサンプルばかり生成すると、Discriminatorが見破りやすくなる</li>
<li>実装：バッチ統計量を計算してDiscriminatorの入力に連結</li>
</ul>

<p><strong>対策C: Two Timescale Update Rule</strong></p>
<ul>
<li>DiscriminatorをGeneratorより多く訓練（例：5回対1回）</li>
<li>効果：Discriminatorが常に強い状態を保ち、Generatorが全モードを探索</li>
<li>実装：訓練ループでD_stepsパラメータを設定</li>
</ul>
</details>

<details>
<summary><strong>問題3: WGAN-GPとSpectral Normalizationの比較</strong></summary>
<p><strong>質問</strong>：以下の観点から、WGAN-GPとSpectral Normalizationを比較してください。</p>
<ol>
<li>Lipschitz制約の実現方法</li>
<li>計算コスト</li>
<li>実装の複雑さ</li>
<li>訓練の安定性</li>
<li>どちらを選ぶべきか（状況別）</li>
</ol>

<p><strong>解答例</strong>：</p>

<p><strong>1. Lipschitz制約の実現方法</strong></p>
<ul>
<li><strong>WGAN-GP</strong>：
<ul>
<li>Gradient Penaltyで勾配ノルムを1に制約</li>
<li>訓練時に補間点で勾配を計算</li>
<li>ソフト制約（ペナルティ項として追加）</li>
</ul>
</li>
<li><strong>Spectral Norm</strong>：
<ul>
<li>各層の重み行列のスペクトルノルムを1に正規化</li>
<li>重みそのものを制約</li>
<li>ハード制約（直接的な正規化）</li>
</ul>
</li>
</ul>

<p><strong>2. 計算コスト</strong></p>
<ul>
<li><strong>WGAN-GP</strong>：
<ul>
<li>各イテレーションでGP計算が必要（補間+逆伝播）</li>
<li>訓練時のオーバーヘッド：約30-50%増</li>
</ul>
</li>
<li><strong>Spectral Norm</strong>：
<ul>
<li>Power Iterationで最大特異値を推定</li>
<li>訓練時のオーバーヘッド：約5-10%増</li>
<li>推論時はオーバーヘッドなし</li>
</ul>
</li>
</ul>

<p><strong>3. 実装の複雑さ</strong></p>
<ul>
<li><strong>WGAN-GP</strong>：
<ul>
<li>補間点の生成、勾配計算、GP項の追加が必要</li>
<li>実装がやや複雑（約50行のコード）</li>
</ul>
</li>
<li><strong>Spectral Norm</strong>：
<ul>
<li>PyTorchの<code>spectral_norm()</code>を層に適用するだけ</li>
<li>実装が非常にシンプル（1行で完了）</li>
</ul>
</li>
</ul>

<p><strong>4. 訓練の安定性</strong></p>
<ul>
<li><strong>WGAN-GP</strong>：
<ul>
<li>Wasserstein距離による意味のある損失</li>
<li>Mode Collapse軽減に効果的</li>
<li>λ（GP係数）の調整が必要</li>
</ul>
</li>
<li><strong>Spectral Norm</strong>：
<ul>
<li>全層で一貫したLipschitz制約</li>
<li>Hyperparameterが少ない（調整不要）</li>
<li>安定性が高い</li>
</ul>
</li>
</ul>

<p><strong>5. 選択基準</strong></p>
<ul>
<li><strong>WGAN-GPを選ぶ場合</strong>：
<ul>
<li>理論的な保証が重要</li>
<li>Wasserstein距離を損失として使いたい</li>
<li>計算リソースに余裕がある</li>
</ul>
</li>
<li><strong>Spectral Normを選ぶ場合</strong>：
<ul>
<li>シンプルな実装を優先</li>
<li>計算効率が重要</li>
<li>素早くプロトタイプを作りたい</li>
<li>現代的な選択（最近の論文で多用）</li>
</ul>
</li>
</ul>
</details>

<details>
<summary><strong>問題4: StyleGANのスタイル混合</strong></summary>
<p><strong>質問</strong>：StyleGANで2つの潜在ベクトル $\mathbf{z}_A$ と $\mathbf{z}_B$ から、「Aの顔の形 + Bの表情と髪型」を持つ画像を生成したい場合、どのように実装しますか？</p>
<ol>
<li>潜在ベクトルのマッピング手順</li>
<li>どの解像度層でスタイルを切り替えるか</li>
<li>実装コードの概要</li>
</ol>

<p><strong>解答例</strong>：</p>

<p><strong>1. マッピング手順</strong></p>
<ul>
<li>$\mathbf{z}_A \rightarrow$ Mapping Network $\rightarrow \mathbf{w}_A$</li>
<li>$\mathbf{z}_B \rightarrow$ Mapping Network $\rightarrow \mathbf{w}_B$</li>
<li>各解像度層で異なる $\mathbf{w}$ を使用</li>
</ul>

<p><strong>2. スタイル切り替えポイント</strong></p>
<ul>
<li><strong>粗いスタイル（4×4〜8×8）</strong>：$\mathbf{w}_A$ を使用
<ul>
<li>顔の向き、全体的な形状</li>
<li>Aの「顔の形」を保持</li>
</ul>
</li>
<li><strong>中間〜細かいスタイル（16×16〜1024×1024）</strong>：$\mathbf{w}_B$ を使用
<ul>
<li>表情、目の開き、髪型、肌の質感</li>
<li>Bの「表情と髪型」を適用</li>
</ul>
</li>
</ul>

<p><strong>3. 実装コード概要</strong></p>
<pre><code class="language-python"># Mapping Network
w_A = mapping_network(z_A)
w_B = mapping_network(z_B)

# 初期の定数入力
x = constant_input  # 4×4

# 粗いスタイル（Aの顔の形）
x = synthesis_block_4x4(x, w_A)  # 4×4
x = synthesis_block_8x8(x, w_A)  # 8×8

# 中間〜細かいスタイル（Bの表情・髪型）
x = synthesis_block_16x16(x, w_B)  # 16×16
x = synthesis_block_32x32(x, w_B)  # 32×32
x = synthesis_block_64x64(x, w_B)  # 64×64
# ...以降も w_B を使用

generated_image = x
</code></pre>

<p><strong>効果</strong>：</p>
<ul>
<li>Aの顔の基本構造を維持しながら、Bの表情と髪型が反映される</li>
<li>スタイル混合により無限のバリエーションが可能</li>
<li>切り替え解像度を変えることで異なる効果を実現</li>
</ul>
</details>

<details>
<summary><strong>問題5: GANの評価指標</strong></summary>
<p><strong>質問</strong>：以下の3つのGANモデルを評価する必要があります。どの指標をどのように使うべきか説明してください。</p>

<ul>
<li><strong>モデルA</strong>：IS = 8.5, FID = 25, Precision = 0.85, Recall = 0.60</li>
<li><strong>モデルB</strong>：IS = 6.2, FID = 18, Precision = 0.75, Recall = 0.82</li>
<li><strong>モデルC</strong>：IS = 7.8, FID = 15, Precision = 0.80, Recall = 0.78</li>
</ul>

<ol>
<li>各指標の意味</li>
<li>どのモデルが最適か（用途別）</li>
<li>総合的な推奨モデル</li>
</ol>

<p><strong>解答例</strong>：</p>

<p><strong>1. 各指標の意味</strong></p>
<ul>
<li><strong>Inception Score (IS)</strong>：
<ul>
<li>画像の品質と多様性の組み合わせ</li>
<li>高い値 = 高品質で多様</li>
<li>限界：真のデータ分布を考慮しない</li>
</ul>
</li>
<li><strong>Frechet Inception Distance (FID)</strong>：
<ul>
<li>生成分布と真の分布の距離</li>
<li>低い値 = 真のデータに近い</li>
<li>最も信頼性が高い指標</li>
</ul>
</li>
<li><strong>Precision</strong>：
<ul>
<li>生成画像の品質（本物らしさ）</li>
<li>高い = 高品質だが、多様性は保証されない</li>
</ul>
</li>
<li><strong>Recall</strong>：
<ul>
<li>生成画像の多様性（カバレッジ）</li>
<li>高い = 多様だが、品質は保証されない</li>
</ul>
</li>
</ul>

<p><strong>2. 用途別の最適モデル</strong></p>
<ul>
<li><strong>高品質画像生成が最優先（例：広告素材）</strong>：
<ul>
<li>モデルA（Precision = 0.85が最高）</li>
<li>理由：個々の画像品質が重要、多様性は二の次</li>
</ul>
</li>
<li><strong>データ拡張（例：訓練データの補完）</strong>：
<ul>
<li>モデルB（Recall = 0.82が最高）</li>
<li>理由：多様なサンプルが必要、多少の品質低下は許容</li>
</ul>
</li>
<li><strong>汎用的な画像生成</strong>：
<ul>
<li>モデルC（FID = 15が最低、バランスが良い）</li>
<li>理由：品質と多様性のバランスが取れている</li>
</ul>
</li>
</ul>

<p><strong>3. 総合的な推奨</strong></p>
<ul>
<li><strong>推奨モデル：モデルC</strong></li>
<li>理由：
<ul>
<li>FIDが最も低い（15）= 真のデータに最も近い</li>
<li>Precision (0.80) と Recall (0.78) がバランス良い</li>
<li>特定の用途に偏らない汎用性</li>
</ul>
</li>
<li><strong>総合評価の考え方</strong>：
<ul>
<li>FIDを最優先（最も信頼性が高い）</li>
<li>Precision/Recallで品質と多様性のバランスを確認</li>
<li>ISは参考程度（単独では不十分）</li>
</ul>
</li>
</ul>
</details>

<hr>

<div class="navigation">
    <a href="chapter2-vae.html" class="nav-button">← 第2章：VAE (変分オートエンコーダ)</a>
    <a href="chapter4-advanced-gans.html" class="nav-button">第4章：高度なGANアーキテクチャ →</a>
</div>

    </main>

    <footer>
        <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
        <p>ML-A04: Generative Models入門 - GANで現実的な画像を生成しよう</p>
    </footer>

</body>
</html>
