<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Á¨¨5Á´†ÔºöÂ§ßË¶èÊ®°Ê©üÊ¢∞Â≠¶Áøí„Éë„Ç§„Éó„É©„Ç§„É≥ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .info-box {
            background-color: #e7f3ff;
            border-left: 4px solid #2196F3;
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .warning-box {
            background-color: #fff3e0;
            border-left: 4px solid #ff9800;
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .success-box {
            background-color: #e8f5e9;
            border-left: 4px solid #4caf50;
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .exercise-box {
            background-color: var(--color-bg-alt);
            border: 2px solid var(--color-accent);
            padding: var(--spacing-md);
            margin: var(--spacing-lg) 0;
            border-radius: var(--border-radius);
        }

        .exercise-box h3 {
            color: var(--color-accent);
            margin-top: 0;
        }

        .nav-links {
            display: flex;
            justify-content: space-between;
            margin-top: var(--spacing-xl);
            padding-top: var(--spacing-lg);
            border-top: 1px solid var(--color-border);
            flex-wrap: wrap;
            gap: var(--spacing-sm);
        }

        .nav-button {
            display: inline-block;
            padding: var(--spacing-sm) var(--spacing-md);
            background-color: var(--color-accent);
            color: white;
            border-radius: var(--border-radius);
            text-decoration: none;
            transition: background-color 0.2s;
        }

        .nav-button:hover {
            background-color: var(--color-accent-light);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            color: var(--color-text-light);
            font-size: 0.9rem;
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                flex-direction: column;
                gap: var(--spacing-xs);
            }

            .nav-links {
                flex-direction: column;
            }

            .nav-button {
                text-align: center;
                width: 100%;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Á¨¨5Á´†ÔºöÂ§ßË¶èÊ®°Ê©üÊ¢∞Â≠¶Áøí„Éë„Ç§„Éó„É©„Ç§„É≥</h1>
            <p class="subtitle">„Ç®„É≥„Éâ„ÉÑ„Éº„Ç®„É≥„Éâ„ÅÆÂàÜÊï£Ê©üÊ¢∞Â≠¶Áøí„Ç∑„Çπ„ÉÜ„É†ÊßãÁØâ</p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 35-40ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ‰∏äÁ¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 8ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 5Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Â≠¶ÁøíÁõÆÊ®ô</h2>
<p>„Åì„ÅÆÁ´†„ÇíË™≠„ÇÄ„Åì„Å®„Åß„ÄÅ‰ª•‰∏ã„ÇíÁøíÂæó„Åß„Åç„Åæ„ÅôÔºö</p>
<ul>
<li>‚úÖ Â§ßË¶èÊ®°Ê©üÊ¢∞Â≠¶Áøí„Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆË®≠Ë®àÂéüÂâá„ÇíÁêÜËß£„Åô„Çã</li>
<li>‚úÖ Spark„Å®PyTorch„ÇíÁµ±Âêà„Åó„ÅüÂàÜÊï£ETL„Éë„Ç§„Éó„É©„Ç§„É≥„ÇíÊßãÁØâ„Åß„Åç„Çã</li>
<li>‚úÖ ÂàÜÊï£Áí∞Â¢É„Åß„ÅÆ„É¢„Éá„É´Ë®ìÁ∑¥„Å®„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøÊúÄÈÅ©Âåñ„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>‚úÖ Â§ßË¶èÊ®°„Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊúÄÈÅ©ÂåñÊâãÊ≥ï„ÇíÈÅ©Áî®„Åß„Åç„Çã</li>
<li>‚úÖ „Ç®„É≥„Éâ„ÉÑ„Éº„Ç®„É≥„Éâ„ÅÆÊú¨Áï™Áí∞Â¢ÉÂØæÂøúML„Ç∑„Çπ„ÉÜ„É†„ÇíÊßãÁØâ„Åß„Åç„Çã</li>
<li>‚úÖ Áõ£Ë¶ñ„Éª„É°„É≥„ÉÜ„Éä„É≥„ÇπÊà¶Áï•„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
</ul>

<hr>

<h2>5.1 „Éë„Ç§„Éó„É©„Ç§„É≥Ë®≠Ë®àÂéüÂâá</h2>

<h3>„Çπ„Ç±„Éº„É©„Éñ„É´„Å™ML„Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆË¶Å‰ª∂</h3>

<p>Â§ßË¶èÊ®°Ê©üÊ¢∞Â≠¶Áøí„Éë„Ç§„Éó„É©„Ç§„É≥„ÇíË®≠Ë®à„Åô„ÇãÈöõ„ÅÆ‰∏ªË¶Å„Å™ËÄÉÊÖÆ‰∫ãÈ†ÖÔºö</p>

<table>
<thead>
<tr>
<th>Ë¶Å‰ª∂</th>
<th>Ë™¨Êòé</th>
<th>ÂÆüË£ÖÊäÄË°ì</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>„Éá„Éº„Çø„Çπ„Ç±„Éº„É©„Éì„É™„ÉÜ„Ç£</strong></td>
<td>TB„ÄúPB„Çπ„Ç±„Éº„É´„ÅÆ„Éá„Éº„ÇøÂá¶ÁêÜ</td>
<td>Spark„ÄÅDask„ÄÅÂàÜÊï£„Çπ„Éà„É¨„Éº„Ç∏</td>
</tr>
<tr>
<td><strong>Ë®àÁÆó„Çπ„Ç±„Éº„É©„Éì„É™„ÉÜ„Ç£</strong></td>
<td>„Éû„É´„ÉÅ„Éé„Éº„Éâ‰∏¶ÂàóÂá¶ÁêÜ</td>
<td>Ray„ÄÅHorovod„ÄÅKubernetes</td>
</tr>
<tr>
<td><strong>„Éï„Ç©„Éº„É´„Éà„Éà„É¨„É©„É≥„Çπ</strong></td>
<td>ÈöúÂÆ≥ÊôÇ„ÅÆËá™Âãï„É™„Ç´„Éê„É™„Éº</td>
<td>„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„ÄÅÂÜóÈï∑Âåñ</td>
</tr>
<tr>
<td><strong>ÂÜçÁèæÊÄß</strong></td>
<td>ÂÆüÈ®ì„Éª„É¢„Éá„É´„ÅÆÂÆåÂÖ®„Å™ÂÜçÁèæ</td>
<td>„Éê„Éº„Ç∏„Éß„É≥ÁÆ°ÁêÜ„ÄÅ„Ç∑„Éº„ÉâÂõ∫ÂÆö</td>
</tr>
<tr>
<td><strong>„É¢„Éã„Çø„É™„É≥„Ç∞</strong></td>
<td>„É™„Ç¢„É´„Çø„Ç§„É†ÊÄßËÉΩÁõ£Ë¶ñ</td>
<td>Prometheus„ÄÅGrafana„ÄÅMLflow</td>
</tr>
<tr>
<td><strong>„Ç≥„Çπ„ÉàÂäπÁéá</strong></td>
<td>„É™„ÇΩ„Éº„Çπ‰ΩøÁî®„ÅÆÊúÄÈÅ©Âåñ</td>
<td>„Ç™„Éº„Éà„Çπ„Ç±„Éº„É™„É≥„Ç∞„ÄÅ„Çπ„Éù„ÉÉ„Éà„Ç§„É≥„Çπ„Çø„É≥„Çπ</td>
</tr>
</tbody>
</table>

<h3>„Éë„Ç§„Éó„É©„Ç§„É≥„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„Éë„Çø„Éº„É≥</h3>

<pre class="mermaid">
graph TB
    subgraph "„Éá„Éº„ÇøÂ±§"
        A[Áîü„Éá„Éº„Çø<br/>HDFS/S3] --> B[„Éá„Éº„ÇøÊ§úË®º<br/>Great Expectations]
        B --> C[ÂàÜÊï£ETL<br/>Apache Spark]
    end

    subgraph "ÁâπÂæ¥ÈáèÂ±§"
        C --> D[ÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞<br/>Spark ML]
        D --> E[ÁâπÂæ¥Èáè„Çπ„Éà„Ç¢<br/>Feast/Tecton]
    end

    subgraph "Ë®ìÁ∑¥Â±§"
        E --> F[ÂàÜÊï£Ë®ìÁ∑¥<br/>Ray/Horovod]
        F --> G[„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøÊúÄÈÅ©Âåñ<br/>Ray Tune]
        G --> H[„É¢„Éá„É´„É¨„Ç∏„Çπ„Éà„É™<br/>MLflow]
    end

    subgraph "Êé®Ë´ñÂ±§"
        H --> I[„É¢„Éá„É´ÈÖç‰ø°<br/>Kubernetes]
        I --> J[‰∫àÊ∏¨„Çµ„Éº„Éì„Çπ<br/>TorchServe]
    end

    subgraph "Áõ£Ë¶ñÂ±§"
        J --> K[„É°„Éà„É™„ÇØ„ÇπÂèéÈõÜ<br/>Prometheus]
        K --> L[ÂèØË¶ñÂåñ<br/>Grafana]
        L --> M[„Ç¢„É©„Éº„Éà<br/>PagerDuty]
    end
</pre>

<h4>„Ç≥„Éº„Éâ‰æã1: „Éë„Ç§„Éó„É©„Ç§„É≥Ë®≠ÂÆö„ÇØ„É©„Çπ</h4>

<pre><code>"""
Â§ßË¶èÊ®°ML„Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆË®≠ÂÆöÁÆ°ÁêÜ
"""
from dataclasses import dataclass, field
from typing import Dict, List, Optional
import yaml
from pathlib import Path


@dataclass
class DataConfig:
    """„Éá„Éº„ÇøÂá¶ÁêÜË®≠ÂÆö"""
    source_path: str
    output_path: str
    num_partitions: int = 1000
    file_format: str = "parquet"
    compression: str = "snappy"
    validation_rules: Dict[str, any] = field(default_factory=dict)


@dataclass
class TrainingConfig:
    """Ë®ìÁ∑¥Ë®≠ÂÆö"""
    model_type: str
    num_workers: int = 4
    num_gpus_per_worker: int = 1
    batch_size: int = 256
    max_epochs: int = 100
    learning_rate: float = 0.001
    checkpoint_freq: int = 10
    early_stopping_patience: int = 5


@dataclass
class ResourceConfig:
    """„É™„ÇΩ„Éº„ÇπË®≠ÂÆö"""
    num_nodes: int = 4
    cpus_per_node: int = 16
    memory_per_node: str = "64GB"
    gpus_per_node: int = 4
    storage_type: str = "ssd"
    network_bandwidth: str = "10Gbps"


@dataclass
class MonitoringConfig:
    """Áõ£Ë¶ñË®≠ÂÆö"""
    metrics_interval: int = 60  # Áßí
    log_level: str = "INFO"
    alert_thresholds: Dict[str, float] = field(default_factory=dict)
    dashboard_url: Optional[str] = None


@dataclass
class PipelineConfig:
    """Áµ±Âêà„Éë„Ç§„Éó„É©„Ç§„É≥Ë®≠ÂÆö"""
    pipeline_name: str
    version: str
    data: DataConfig
    training: TrainingConfig
    resources: ResourceConfig
    monitoring: MonitoringConfig

    @classmethod
    def from_yaml(cls, config_path: Path) -> 'PipelineConfig':
        """YAML„Éï„Ç°„Ç§„É´„Åã„ÇâË®≠ÂÆö„ÇíË™≠„ÅøËæº„ÇÄ"""
        with open(config_path) as f:
            config_dict = yaml.safe_load(f)

        return cls(
            pipeline_name=config_dict['pipeline_name'],
            version=config_dict['version'],
            data=DataConfig(**config_dict['data']),
            training=TrainingConfig(**config_dict['training']),
            resources=ResourceConfig(**config_dict['resources']),
            monitoring=MonitoringConfig(**config_dict['monitoring'])
        )

    def to_yaml(self, output_path: Path):
        """Ë®≠ÂÆö„ÇíYAML„Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò"""
        config_dict = {
            'pipeline_name': self.pipeline_name,
            'version': self.version,
            'data': self.data.__dict__,
            'training': self.training.__dict__,
            'resources': self.resources.__dict__,
            'monitoring': self.monitoring.__dict__
        }

        with open(output_path, 'w') as f:
            yaml.dump(config_dict, f, default_flow_style=False)


# ‰ΩøÁî®‰æã
if __name__ == "__main__":
    # Ë®≠ÂÆö‰ΩúÊàê
    config = PipelineConfig(
        pipeline_name="customer_churn_prediction",
        version="v1.0.0",
        data=DataConfig(
            source_path="s3://my-bucket/raw-data/",
            output_path="s3://my-bucket/processed-data/",
            num_partitions=2000,
            validation_rules={
                "min_records": 1000000,
                "required_columns": ["customer_id", "features", "label"]
            }
        ),
        training=TrainingConfig(
            model_type="neural_network",
            num_workers=8,
            num_gpus_per_worker=2,
            batch_size=512,
            max_epochs=50
        ),
        resources=ResourceConfig(
            num_nodes=8,
            cpus_per_node=32,
            memory_per_node="128GB",
            gpus_per_node=4
        ),
        monitoring=MonitoringConfig(
            alert_thresholds={
                "accuracy": 0.85,
                "latency_p99": 100.0,  # „Éü„É™Áßí
                "error_rate": 0.01
            }
        )
    )

    # Ë®≠ÂÆö‰øùÂ≠ò
    config.to_yaml(Path("pipeline_config.yaml"))

    # Ë®≠ÂÆöË™≠„ÅøËæº„Åø
    loaded_config = PipelineConfig.from_yaml(Path("pipeline_config.yaml"))
    print(f"Pipeline: {loaded_config.pipeline_name}")
    print(f"Workers: {loaded_config.training.num_workers}")
    print(f"Partitions: {loaded_config.data.num_partitions}")
</code></pre>

<div class="info-box">
<strong>üí° Ë®≠Ë®à„ÅÆ„Éù„Ç§„É≥„ÉàÔºö</strong>
<ul>
<li>Ë®≠ÂÆö„ÇíÁí∞Â¢ÉÂ§âÊï∞„ÇÑ„Ç≥„Éº„Éâ„Å´Âüã„ÇÅËæº„Åæ„Åö„ÄÅYAML„ÅßÂ§ñÈÉ®Âåñ</li>
<li>„Éá„Éº„Çø„ÇØ„É©„Çπ„ÅßÂûãÂÆâÂÖ®ÊÄß„ÇíÁ¢∫‰øù</li>
<li>„Éê„Éº„Ç∏„Éß„É≥ÁÆ°ÁêÜ„ÅßË®≠ÂÆö„ÅÆÂ±•Ê≠¥„ÇíËøΩË∑°</li>
<li>Áí∞Â¢ÉÔºàÈñãÁô∫/Êú¨Áï™Ôºâ„Åî„Å®„Å´Ë®≠ÂÆö„Éï„Ç°„Ç§„É´„ÇíÂàÜÈõ¢</li>
</ul>
</div>

<h3>„Ç®„É©„Éº„Éè„É≥„Éâ„É™„É≥„Ç∞„Å®„É™„Éà„É©„Ç§Êà¶Áï•</h3>

<h4>„Ç≥„Éº„Éâ‰æã2: Â†ÖÁâ¢„Å™„Éë„Ç§„Éó„É©„Ç§„É≥ÂÆüË°å„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ</h4>

<pre><code>"""
„Éï„Ç©„Éº„É´„Éà„Éà„É¨„É©„É≥„Éà„Å™„Éë„Ç§„Éó„É©„Ç§„É≥ÂÆüË°å
"""
import time
import logging
from typing import Callable, Any, Optional, List
from functools import wraps
from dataclasses import dataclass
from enum import Enum


class TaskStatus(Enum):
    """„Çø„Çπ„ÇØÂÆüË°åÁä∂ÊÖã"""
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    RETRYING = "retrying"


@dataclass
class TaskResult:
    """„Çø„Çπ„ÇØÂÆüË°åÁµêÊûú"""
    status: TaskStatus
    result: Any = None
    error: Optional[Exception] = None
    execution_time: float = 0.0
    retry_count: int = 0


class RetryStrategy:
    """„É™„Éà„É©„Ç§Êà¶Áï•"""

    def __init__(
        self,
        max_retries: int = 3,
        initial_delay: float = 1.0,
        backoff_factor: float = 2.0,
        max_delay: float = 60.0,
        retryable_exceptions: List[type] = None
    ):
        self.max_retries = max_retries
        self.initial_delay = initial_delay
        self.backoff_factor = backoff_factor
        self.max_delay = max_delay
        self.retryable_exceptions = retryable_exceptions or [Exception]

    def get_delay(self, retry_count: int) -> float:
        """„É™„Éà„É©„Ç§ÂæÖÊ©üÊôÇÈñì„ÇíË®àÁÆóÔºàÊåáÊï∞„Éê„ÉÉ„ÇØ„Ç™„ÉïÔºâ"""
        delay = self.initial_delay * (self.backoff_factor ** retry_count)
        return min(delay, self.max_delay)

    def should_retry(self, exception: Exception) -> bool:
        """„É™„Éà„É©„Ç§„Åô„Åπ„Åç„ÅãÂà§ÂÆö"""
        return any(isinstance(exception, exc_type)
                  for exc_type in self.retryable_exceptions)


def with_retry(retry_strategy: RetryStrategy):
    """„É™„Éà„É©„Ç§Ê©üËÉΩ„ÇíËøΩÂä†„Åô„Çã„Éá„Ç≥„É¨„Éº„Çø"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs) -> TaskResult:
            retry_count = 0
            start_time = time.time()

            while retry_count <= retry_strategy.max_retries:
                try:
                    result = func(*args, **kwargs)
                    execution_time = time.time() - start_time

                    return TaskResult(
                        status=TaskStatus.SUCCESS,
                        result=result,
                        execution_time=execution_time,
                        retry_count=retry_count
                    )

                except Exception as e:
                    if (retry_count < retry_strategy.max_retries and
                        retry_strategy.should_retry(e)):

                        delay = retry_strategy.get_delay(retry_count)
                        logging.warning(
                            f"Task failed (attempt {retry_count + 1}/"
                            f"{retry_strategy.max_retries + 1}): {str(e)}"
                            f"\nRetrying in {delay:.1f} seconds..."
                        )

                        time.sleep(delay)
                        retry_count += 1
                    else:
                        execution_time = time.time() - start_time
                        return TaskResult(
                            status=TaskStatus.FAILED,
                            error=e,
                            execution_time=execution_time,
                            retry_count=retry_count
                        )

            return TaskResult(
                status=TaskStatus.FAILED,
                error=Exception("Max retries exceeded"),
                execution_time=time.time() - start_time,
                retry_count=retry_count
            )

        return wrapper
    return decorator


class PipelineExecutor:
    """„Éë„Ç§„Éó„É©„Ç§„É≥ÂÆüË°å„Ç®„É≥„Ç∏„É≥"""

    def __init__(self, checkpoint_dir: str = "./checkpoints"):
        self.checkpoint_dir = checkpoint_dir
        self.logger = logging.getLogger(__name__)

    def execute_stage(
        self,
        stage_name: str,
        task_func: Callable,
        retry_strategy: Optional[RetryStrategy] = None,
        checkpoint: bool = True
    ) -> TaskResult:
        """„Éë„Ç§„Éó„É©„Ç§„É≥„Çπ„ÉÜ„Éº„Ç∏„ÇíÂÆüË°å"""
        self.logger.info(f"Starting stage: {stage_name}")

        # „ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„Åå„ÅÇ„Çå„Å∞Âæ©ÂÖÉ
        if checkpoint and self._checkpoint_exists(stage_name):
            self.logger.info(f"Restoring from checkpoint: {stage_name}")
            return self._load_checkpoint(stage_name)

        # „É™„Éà„É©„Ç§Êà¶Áï•„ÇíÈÅ©Áî®
        if retry_strategy:
            task_func = with_retry(retry_strategy)(task_func)

        # „Çø„Çπ„ÇØÂÆüË°å
        result = task_func()

        # „ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà‰øùÂ≠ò
        if checkpoint and result.status == TaskStatus.SUCCESS:
            self._save_checkpoint(stage_name, result)

        self.logger.info(
            f"Stage {stage_name} completed: {result.status.value} "
            f"(time: {result.execution_time:.2f}s, "
            f"retries: {result.retry_count})"
        )

        return result

    def _checkpoint_exists(self, stage_name: str) -> bool:
        """„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„ÅÆÂ≠òÂú®Á¢∫Ë™ç"""
        from pathlib import Path
        checkpoint_path = Path(self.checkpoint_dir) / f"{stage_name}.ckpt"
        return checkpoint_path.exists()

    def _save_checkpoint(self, stage_name: str, result: TaskResult):
        """„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà‰øùÂ≠ò"""
        import pickle
        from pathlib import Path

        Path(self.checkpoint_dir).mkdir(exist_ok=True)
        checkpoint_path = Path(self.checkpoint_dir) / f"{stage_name}.ckpt"

        with open(checkpoint_path, 'wb') as f:
            pickle.dump(result, f)

    def _load_checkpoint(self, stage_name: str) -> TaskResult:
        """„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„ÉàË™≠„ÅøËæº„Åø"""
        import pickle
        from pathlib import Path

        checkpoint_path = Path(self.checkpoint_dir) / f"{stage_name}.ckpt"

        with open(checkpoint_path, 'rb') as f:
            return pickle.load(f)


# ‰ΩøÁî®‰æã
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # „É™„Éà„É©„Ç§Êà¶Áï•ÂÆöÁæ©
    retry_strategy = RetryStrategy(
        max_retries=3,
        initial_delay=2.0,
        backoff_factor=2.0,
        retryable_exceptions=[ConnectionError, TimeoutError]
    )

    # „Éë„Ç§„Éó„É©„Ç§„É≥ÂÆüË°å
    executor = PipelineExecutor(checkpoint_dir="./pipeline_checkpoints")

    # „Çπ„ÉÜ„Éº„Ç∏1: „Éá„Éº„ÇøË™≠„ÅøËæº„ÅøÔºàÂ§±Êïó„Åô„ÇãÂèØËÉΩÊÄß„ÅÇ„ÇäÔºâ
    def load_data():
        import random
        if random.random() < 0.3:  # 30%„ÅÆÁ¢∫Áéá„ÅßÂ§±Êïó
            raise ConnectionError("Failed to connect to data source")
        return {"data": list(range(1000))}

    result1 = executor.execute_stage(
        "data_loading",
        load_data,
        retry_strategy=retry_strategy
    )

    if result1.status == TaskStatus.SUCCESS:
        print(f"Data loaded: {len(result1.result['data'])} records")

        # „Çπ„ÉÜ„Éº„Ç∏2: „Éá„Éº„ÇøÂá¶ÁêÜ
        def process_data():
            data = result1.result['data']
            processed = [x * 2 for x in data]
            return {"processed_data": processed}

        result2 = executor.execute_stage(
            "data_processing",
            process_data,
            checkpoint=True
        )

        if result2.status == TaskStatus.SUCCESS:
            print(f"Processing completed successfully")
</code></pre>

<div class="warning-box">
<strong>‚ö†Ô∏è „É™„Éà„É©„Ç§Êà¶Áï•„ÅÆÊ≥®ÊÑèÁÇπÔºö</strong>
<ul>
<li>ÂÜ™Á≠âÊÄßÔºöÂêå„ÅòÊìç‰Ωú„ÇíË§áÊï∞ÂõûÂÆüË°å„Åó„Å¶„ÇÇÁµêÊûú„ÅåÂ§â„Çè„Çâ„Å™„ÅÑ„Çà„ÅÜ„Å´Ë®≠Ë®à</li>
<li>„Çø„Ç§„É†„Ç¢„Ç¶„ÉàÔºöÁÑ°Èôê„É´„Éº„Éó„ÇíÈò≤„Åê„Åü„ÇÅÊúÄÂ§ß„É™„Éà„É©„Ç§ÂõûÊï∞„ÇíË®≠ÂÆö</li>
<li>„Éê„ÉÉ„ÇØ„Ç™„ÉïÔºö„Çµ„Éº„Éì„ÇπÈÅéË≤†Ëç∑„ÇíÈò≤„Åê„Åü„ÇÅÂæÖÊ©üÊôÇÈñì„ÇíÊåáÊï∞ÁöÑ„Å´Â¢óÂä†</li>
<li>ÈÅ∏ÊäûÁöÑ„É™„Éà„É©„Ç§Ôºö‰∏ÄÊôÇÁöÑ„Å™„Ç®„É©„Éº„ÅÆ„Åø„É™„Éà„É©„Ç§„Åó„ÄÅÊ∞∏Á∂öÁöÑ„Å™„Ç®„É©„Éº„ÅØÂç≥Â∫ß„Å´Â§±Êïó</li>
</ul>
</div>

<hr>

<h2>5.2 „Éá„Éº„ÇøÂá¶ÁêÜ„Éë„Ç§„Éó„É©„Ç§„É≥</h2>

<h3>ÂàÜÊï£ETL„Éë„Ç§„Éó„É©„Ç§„É≥ÊßãÁØâ</h3>

<h4>„Ç≥„Éº„Éâ‰æã3: Spark„Éô„Éº„Çπ„ÅÆÂ§ßË¶èÊ®°„Éá„Éº„ÇøETL</h4>

<pre><code>"""
Apache Spark„Å´„Çà„ÇãÂ§ßË¶èÊ®°„Éá„Éº„ÇøETL„Éë„Ç§„Éó„É©„Ç§„É≥
"""
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType
from pyspark.ml.feature import VectorAssembler, StandardScaler
from typing import List, Dict
import logging


class DistributedETLPipeline:
    """ÂàÜÊï£ETL„Éë„Ç§„Éó„É©„Ç§„É≥"""

    def __init__(
        self,
        app_name: str = "MLDataPipeline",
        master: str = "spark://master:7077",
        num_partitions: int = 1000
    ):
        self.spark = SparkSession.builder \
            .appName(app_name) \
            .master(master) \
            .config("spark.sql.shuffle.partitions", num_partitions) \
            .config("spark.default.parallelism", num_partitions) \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .getOrCreate()

        self.logger = logging.getLogger(__name__)

    def extract(
        self,
        source_path: str,
        file_format: str = "parquet",
        schema: StructType = None
    ) -> DataFrame:
        """„Éá„Éº„ÇøÊäΩÂá∫"""
        self.logger.info(f"Extracting data from {source_path}")

        if schema:
            df = self.spark.read.schema(schema).format(file_format).load(source_path)
        else:
            df = self.spark.read.format(file_format).load(source_path)

        self.logger.info(f"Extracted {df.count()} records with {len(df.columns)} columns")
        return df

    def validate(self, df: DataFrame, validation_rules: Dict) -> DataFrame:
        """„Éá„Éº„ÇøÊ§úË®º"""
        self.logger.info("Validating data quality")

        # ÂøÖÈ†à„Ç´„É©„É†„ÉÅ„Çß„ÉÉ„ÇØ
        if "required_columns" in validation_rules:
            missing_cols = set(validation_rules["required_columns"]) - set(df.columns)
            if missing_cols:
                raise ValueError(f"Missing required columns: {missing_cols}")

        # NullÂÄ§„ÉÅ„Çß„ÉÉ„ÇØ
        if "non_null_columns" in validation_rules:
            for col in validation_rules["non_null_columns"]:
                null_count = df.filter(F.col(col).isNull()).count()
                if null_count > 0:
                    self.logger.warning(f"Column {col} has {null_count} null values")

        # ÂÄ§ÁØÑÂõ≤„ÉÅ„Çß„ÉÉ„ÇØ
        if "value_ranges" in validation_rules:
            for col, (min_val, max_val) in validation_rules["value_ranges"].items():
                df = df.filter(
                    (F.col(col) >= min_val) & (F.col(col) <= max_val)
                )

        # ÈáçË§á„ÉÅ„Çß„ÉÉ„ÇØ
        if "unique_columns" in validation_rules:
            unique_cols = validation_rules["unique_columns"]
            initial_count = df.count()
            df = df.dropDuplicates(unique_cols)
            duplicates_removed = initial_count - df.count()
            if duplicates_removed > 0:
                self.logger.warning(f"Removed {duplicates_removed} duplicate records")

        return df

    def transform(
        self,
        df: DataFrame,
        feature_columns: List[str],
        label_column: str = None,
        normalize: bool = True
    ) -> DataFrame:
        """„Éá„Éº„ÇøÂ§âÊèõ„Å®ÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞"""
        self.logger.info("Transforming data")

        # Ê¨†ÊêçÂÄ§Âá¶ÁêÜ
        df = self._handle_missing_values(df, feature_columns)

        # „Ç´„ÉÜ„Ç¥„É™Â§âÊï∞„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞
        df = self._encode_categorical_features(df, feature_columns)

        # ÁâπÂæ¥Èáè„Éô„ÇØ„Éà„É´‰ΩúÊàê
        assembler = VectorAssembler(
            inputCols=feature_columns,
            outputCol="features_raw"
        )
        df = assembler.transform(df)

        # Ê≠£Ë¶èÂåñ
        if normalize:
            scaler = StandardScaler(
                inputCol="features_raw",
                outputCol="features",
                withMean=True,
                withStd=True
            )
            scaler_model = scaler.fit(df)
            df = scaler_model.transform(df)
        else:
            df = df.withColumnRenamed("features_raw", "features")

        # ÂøÖË¶Å„Å™„Ç´„É©„É†„ÅÆ„ÅøÈÅ∏Êäû
        select_cols = ["features"]
        if label_column:
            select_cols.append(label_column)

        return df.select(select_cols)

    def _handle_missing_values(
        self,
        df: DataFrame,
        columns: List[str],
        strategy: str = "mean"
    ) -> DataFrame:
        """Ê¨†ÊêçÂÄ§Âá¶ÁêÜ"""
        for col in columns:
            if strategy == "mean":
                mean_val = df.select(F.mean(col)).first()[0]
                df = df.fillna({col: mean_val})
            elif strategy == "median":
                median_val = df.approxQuantile(col, [0.5], 0.01)[0]
                df = df.fillna({col: median_val})
            elif strategy == "drop":
                df = df.dropna(subset=[col])

        return df

    def _encode_categorical_features(
        self,
        df: DataFrame,
        feature_columns: List[str]
    ) -> DataFrame:
        """„Ç´„ÉÜ„Ç¥„É™Â§âÊï∞„Çí„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞"""
        from pyspark.ml.feature import StringIndexer, OneHotEncoder

        categorical_cols = [
            col for col in feature_columns
            if dict(df.dtypes)[col] == 'string'
        ]

        for col in categorical_cols:
            # StringIndexer
            indexer = StringIndexer(
                inputCol=col,
                outputCol=f"{col}_index",
                handleInvalid="keep"
            )
            df = indexer.fit(df).transform(df)

            # OneHotEncoder
            encoder = OneHotEncoder(
                inputCol=f"{col}_index",
                outputCol=f"{col}_encoded"
            )
            df = encoder.fit(df).transform(df)

        return df

    def load(
        self,
        df: DataFrame,
        output_path: str,
        file_format: str = "parquet",
        mode: str = "overwrite",
        partition_by: List[str] = None
    ):
        """„Éá„Éº„ÇøÊõ∏„ÅçËæº„Åø"""
        self.logger.info(f"Loading data to {output_path}")

        writer = df.write.mode(mode).format(file_format)

        if partition_by:
            writer = writer.partitionBy(partition_by)

        writer.save(output_path)
        self.logger.info(f"Data successfully loaded to {output_path}")

    def run_etl(
        self,
        source_path: str,
        output_path: str,
        feature_columns: List[str],
        label_column: str = None,
        validation_rules: Dict = None,
        partition_by: List[str] = None
    ):
        """ÂÆåÂÖ®„Å™ETL„Éë„Ç§„Éó„É©„Ç§„É≥ÂÆüË°å"""
        # Extract
        df = self.extract(source_path)

        # Validate
        if validation_rules:
            df = self.validate(df, validation_rules)

        # Transform
        df = self.transform(df, feature_columns, label_column)

        # Load
        self.load(df, output_path, partition_by=partition_by)

        return df


# ‰ΩøÁî®‰æã
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # ETL„Éë„Ç§„Éó„É©„Ç§„É≥ÂàùÊúüÂåñ
    pipeline = DistributedETLPipeline(
        app_name="CustomerChurnETL",
        num_partitions=2000
    )

    # Ê§úË®º„É´„Éº„É´ÂÆöÁæ©
    validation_rules = {
        "required_columns": ["customer_id", "age", "balance", "churn"],
        "non_null_columns": ["customer_id", "churn"],
        "value_ranges": {
            "age": (18, 100),
            "balance": (0, 1000000)
        },
        "unique_columns": ["customer_id"]
    }

    # ETLÂÆüË°å
    feature_columns = [
        "age", "balance", "num_products", "credit_score",
        "country", "gender", "is_active_member"
    ]

    pipeline.run_etl(
        source_path="s3://my-bucket/raw-data/customers/",
        output_path="s3://my-bucket/processed-data/customers/",
        feature_columns=feature_columns,
        label_column="churn",
        validation_rules=validation_rules,
        partition_by=["country"]
    )
</code></pre>

<div class="success-box">
<strong>‚úÖ Spark ETL„ÅÆ„Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„ÇπÔºö</strong>
<ul>
<li><strong>Adaptive Query Execution (AQE)</strong>: „ÇØ„Ç®„É™ÂÆüË°åË®àÁîª„ÇíÂãïÁöÑ„Å´ÊúÄÈÅ©Âåñ</li>
<li><strong>„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥ÊúÄÈÅ©Âåñ</strong>: „Éá„Éº„Çø„Çµ„Ç§„Ç∫„Å´Âøú„Åò„Å¶ÈÅ©Âàá„Å™„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥Êï∞„ÇíË®≠ÂÆö</li>
<li><strong>„Ç≠„É£„ÉÉ„Ç∑„É≥„Ç∞</strong>: Áπ∞„ÇäËøî„Åó‰ΩøÁî®„Åô„ÇãDataFrame„Çí„É°„É¢„É™„Å´„Ç≠„É£„ÉÉ„Ç∑„É•</li>
<li><strong>„Çπ„Ç≠„Éº„ÉûÊé®Ë´ñÂõûÈÅø</strong>: Â§ßË¶èÊ®°„Éá„Éº„Çø„Åß„ÅØÊòéÁ§∫ÁöÑ„Å™„Çπ„Ç≠„Éº„ÉûÂÆöÁæ©„Çí‰ΩøÁî®</li>
</ul>
</div>

<hr>

<h2>5.3 ÂàÜÊï£„É¢„Éá„É´Ë®ìÁ∑¥</h2>

<h3>„Éû„É´„ÉÅ„Éé„Éº„ÉâË®ìÁ∑¥„Å®„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøÊúÄÈÅ©Âåñ</h3>

<h4>„Ç≥„Éº„Éâ‰æã4: Ray Tune„Å´„Çà„ÇãÂàÜÊï£„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøÊúÄÈÅ©Âåñ</h4>

<pre><code>"""
Ray Tune„Çí‰Ωø„Å£„ÅüÂ§ßË¶èÊ®°„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøÊúÄÈÅ©Âåñ
"""
import ray
from ray import tune
from ray.tune import CLIReporter
from ray.tune.schedulers import ASHAScheduler
from ray.tune.search.optuna import OptunaSearch
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from typing import Dict


class NeuralNetwork(nn.Module):
    """„Ç∑„É≥„Éó„É´„Å™„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ"""

    def __init__(self, input_dim: int, hidden_dims: list, output_dim: int, dropout: float):
        super().__init__()

        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))
            prev_dim = hidden_dim

        layers.append(nn.Linear(prev_dim, output_dim))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)


def train_model(config: Dict, checkpoint_dir=None):
    """
    Ë®ìÁ∑¥Èñ¢Êï∞ÔºàRay Tune„Åã„ÇâÂëº„Å≥Âá∫„Åï„Çå„ÇãÔºâ

    Args:
        config: „Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøË®≠ÂÆö
        checkpoint_dir: „ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„Éá„Ç£„É¨„ÇØ„Éà„É™
    """
    # „Éá„Éº„ÇøÊ∫ñÂÇôÔºàÂÆüÈöõ„ÅØSpark„Åã„ÇâË™≠„ÅøËæº„ÇÄÔºâ
    np.random.seed(42)
    X_train = np.random.randn(10000, 50).astype(np.float32)
    y_train = np.random.randint(0, 2, 10000).astype(np.int64)
    X_val = np.random.randn(2000, 50).astype(np.float32)
    y_val = np.random.randint(0, 2, 2000).astype(np.int64)

    train_dataset = TensorDataset(
        torch.from_numpy(X_train),
        torch.from_numpy(y_train)
    )
    val_dataset = TensorDataset(
        torch.from_numpy(X_val),
        torch.from_numpy(y_val)
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=config["batch_size"],
        shuffle=True,
        num_workers=2
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config["batch_size"],
        num_workers=2
    )

    # „É¢„Éá„É´ÂàùÊúüÂåñ
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = NeuralNetwork(
        input_dim=50,
        hidden_dims=config["hidden_dims"],
        output_dim=2,
        dropout=config["dropout"]
    ).to(device)

    # „Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂„Å®ÊêçÂ§±Èñ¢Êï∞
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=config["lr"],
        weight_decay=config["weight_decay"]
    )
    criterion = nn.CrossEntropyLoss()

    # „ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„Åã„ÇâÂæ©ÂÖÉ
    if checkpoint_dir:
        checkpoint = torch.load(checkpoint_dir + "/checkpoint.pt")
        model.load_state_dict(checkpoint["model_state"])
        optimizer.load_state_dict(checkpoint["optimizer_state"])
        start_epoch = checkpoint["epoch"]
    else:
        start_epoch = 0

    # Ë®ìÁ∑¥„É´„Éº„Éó
    for epoch in range(start_epoch, config["max_epochs"]):
        # Ë®ìÁ∑¥„Éï„Çß„Éº„Ç∫
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()

        # Ê§úË®º„Éï„Çß„Éº„Ç∫
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)

                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()

        # „É°„Éà„É™„ÇØ„ÇπË®àÁÆó
        train_acc = train_correct / train_total
        val_acc = val_correct / val_total

        # „ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà‰øùÂ≠ò
        with tune.checkpoint_dir(step=epoch) as checkpoint_dir:
            torch.save({
                "epoch": epoch + 1,
                "model_state": model.state_dict(),
                "optimizer_state": optimizer.state_dict(),
            }, checkpoint_dir + "/checkpoint.pt")

        # Ray Tune„Å´„É°„Éà„É™„ÇØ„ÇπÂ†±Âëä
        tune.report(
            train_loss=train_loss / len(train_loader),
            train_accuracy=train_acc,
            val_loss=val_loss / len(val_loader),
            val_accuracy=val_acc
        )


def run_hyperparameter_optimization():
    """ÂàÜÊï£„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøÊúÄÈÅ©ÂåñÂÆüË°å"""

    # RayÂàùÊúüÂåñ
    ray.init(
        address="auto",  # Êó¢Â≠ò„ÅÆRay„ÇØ„É©„Çπ„Çø„Å´Êé•Á∂ö
        _redis_password="password"
    )

    # Êé¢Á¥¢Á©∫ÈñìÂÆöÁæ©
    config = {
        "lr": tune.loguniform(1e-5, 1e-2),
        "batch_size": tune.choice([128, 256, 512]),
        "hidden_dims": tune.choice([
            [128, 64],
            [256, 128],
            [512, 256, 128]
        ]),
        "dropout": tune.uniform(0.1, 0.5),
        "weight_decay": tune.loguniform(1e-6, 1e-3),
        "max_epochs": 50
    }

    # „Çπ„Ç±„Ç∏„É•„Éº„É©ÔºàÊó©ÊúüÂÅúÊ≠¢Ôºâ
    scheduler = ASHAScheduler(
        metric="val_accuracy",
        mode="max",
        max_t=50,
        grace_period=10,
        reduction_factor=2
    )

    # „Çµ„Éº„ÉÅ„Ç¢„É´„Ç¥„É™„Ç∫„É†ÔºàOptunaÔºâ
    search_alg = OptunaSearch(
        metric="val_accuracy",
        mode="max"
    )

    # „É¨„Éù„Éº„Çø„ÉºË®≠ÂÆö
    reporter = CLIReporter(
        metric_columns=["train_loss", "train_accuracy", "val_loss", "val_accuracy"],
        max_progress_rows=20
    )

    # „ÉÅ„É•„Éº„Éã„É≥„Ç∞ÂÆüË°å
    analysis = tune.run(
        train_model,
        config=config,
        num_samples=100,  # Ë©¶Ë°åÂõûÊï∞
        scheduler=scheduler,
        search_alg=search_alg,
        progress_reporter=reporter,
        resources_per_trial={
            "cpu": 4,
            "gpu": 1
        },
        checkpoint_at_end=True,
        checkpoint_freq=10,
        local_dir="./ray_results",
        name="neural_network_hpo"
    )

    # ÊúÄËâØ„ÅÆË®≠ÂÆö„ÇíÂèñÂæó
    best_config = analysis.get_best_config(metric="val_accuracy", mode="max")
    best_trial = analysis.get_best_trial(metric="val_accuracy", mode="max")

    print("\n" + "="*80)
    print("Best Hyperparameters:")
    print("="*80)
    for key, value in best_config.items():
        print(f"{key:20s}: {value}")

    print(f"\nBest Validation Accuracy: {best_trial.last_result['val_accuracy']:.4f}")

    # ÁµêÊûú„ÇíDataFrame„Å®„Åó„Å¶ÂèñÂæó
    df = analysis.dataframe()
    df.to_csv("hpo_results.csv", index=False)

    ray.shutdown()

    return best_config, analysis


# ‰ΩøÁî®‰æã
if __name__ == "__main__":
    best_config, analysis = run_hyperparameter_optimization()
</code></pre>

<h3>ÂàÜÊï£Ë®ìÁ∑¥Êà¶Áï•</h3>

<h4>„Ç≥„Éº„Éâ‰æã5: PyTorch Distributed Data Parallel (DDP)</h4>

<pre><code>"""
PyTorch DDP„Å´„Çà„Çã„Éû„É´„ÉÅ„Éé„Éº„ÉâÂàÜÊï£Ë®ìÁ∑¥
"""
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler
import os
from typing import Optional


def setup_distributed(rank: int, world_size: int, backend: str = "nccl"):
    """
    ÂàÜÊï£Áí∞Â¢É„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó

    Args:
        rank: ÁèæÂú®„ÅÆ„Éó„É≠„Çª„Çπ„ÅÆ„É©„É≥„ÇØ
        world_size: Á∑è„Éó„É≠„Çª„ÇπÊï∞
        backend: ÈÄö‰ø°„Éê„ÉÉ„ÇØ„Ç®„É≥„ÉâÔºànccl, gloo, mpiÔºâ
    """
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'

    # „Éó„É≠„Çª„Çπ„Ç∞„É´„Éº„ÉóÂàùÊúüÂåñ
    dist.init_process_group(backend, rank=rank, world_size=world_size)

    # GPU„Éá„Éê„Ç§„ÇπË®≠ÂÆö
    torch.cuda.set_device(rank)


def cleanup_distributed():
    """ÂàÜÊï£Áí∞Â¢É„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó"""
    dist.destroy_process_group()


class DistributedTrainer:
    """ÂàÜÊï£Ë®ìÁ∑¥„Éû„Éç„Éº„Ç∏„É£„Éº"""

    def __init__(
        self,
        model: nn.Module,
        train_dataset,
        val_dataset,
        rank: int,
        world_size: int,
        batch_size: int = 256,
        learning_rate: float = 0.001,
        checkpoint_dir: str = "./checkpoints"
    ):
        self.rank = rank
        self.world_size = world_size
        self.checkpoint_dir = checkpoint_dir

        # „Éá„Éê„Ç§„ÇπË®≠ÂÆö
        self.device = torch.device(f"cuda:{rank}")

        # „É¢„Éá„É´„ÇíDDP„Åß„É©„ÉÉ„Éó
        self.model = model.to(self.device)
        self.model = DDP(self.model, device_ids=[rank])

        # „Éá„Éº„Çø„É≠„Éº„ÉÄ„ÉºÔºàDistributedSampler„Çí‰ΩøÁî®Ôºâ
        train_sampler = DistributedSampler(
            train_dataset,
            num_replicas=world_size,
            rank=rank,
            shuffle=True
        )

        self.train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            sampler=train_sampler,
            num_workers=4,
            pin_memory=True
        )

        val_sampler = DistributedSampler(
            val_dataset,
            num_replicas=world_size,
            rank=rank,
            shuffle=False
        )

        self.val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            sampler=val_sampler,
            num_workers=4,
            pin_memory=True
        )

        # „Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂„Å®ÊêçÂ§±Èñ¢Êï∞
        self.optimizer = torch.optim.Adam(
            self.model.parameters(),
            lr=learning_rate
        )
        self.criterion = nn.CrossEntropyLoss()

    def train_epoch(self, epoch: int):
        """1„Ç®„Éù„ÉÉ„ÇØË®ìÁ∑¥"""
        self.model.train()
        self.train_loader.sampler.set_epoch(epoch)  # „Ç∑„É£„ÉÉ„Éï„É´„ÅÆ„Ç∑„Éº„ÉâË®≠ÂÆö

        total_loss = 0.0
        total_correct = 0
        total_samples = 0

        for batch_idx, (inputs, labels) in enumerate(self.train_loader):
            inputs, labels = inputs.to(self.device), labels.to(self.device)

            # Forward pass
            self.optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = self.criterion(outputs, labels)

            # Backward pass
            loss.backward()
            self.optimizer.step()

            # „É°„Éà„É™„ÇØ„ÇπË®àÁÆó
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total_correct += predicted.eq(labels).sum().item()
            total_samples += labels.size(0)

            if batch_idx % 100 == 0 and self.rank == 0:
                print(f"Epoch {epoch} [{batch_idx}/{len(self.train_loader)}] "
                      f"Loss: {loss.item():.4f}")

        # ÂÖ®„Éó„É≠„Çª„Çπ„Åß„É°„Éà„É™„ÇØ„Çπ„ÇíÈõÜÁ¥Ñ
        avg_loss = self._aggregate_metric(total_loss / len(self.train_loader))
        accuracy = self._aggregate_metric(total_correct / total_samples)

        return avg_loss, accuracy

    def validate(self):
        """Ê§úË®º"""
        self.model.eval()

        total_loss = 0.0
        total_correct = 0
        total_samples = 0

        with torch.no_grad():
            for inputs, labels in self.val_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)

                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total_correct += predicted.eq(labels).sum().item()
                total_samples += labels.size(0)

        # ÂÖ®„Éó„É≠„Çª„Çπ„Åß„É°„Éà„É™„ÇØ„Çπ„ÇíÈõÜÁ¥Ñ
        avg_loss = self._aggregate_metric(total_loss / len(self.val_loader))
        accuracy = self._aggregate_metric(total_correct / total_samples)

        return avg_loss, accuracy

    def _aggregate_metric(self, value: float) -> float:
        """ÂÖ®„Éó„É≠„Çª„Çπ„Åß„É°„Éà„É™„ÇØ„Çπ„ÇíÈõÜÁ¥Ñ"""
        tensor = torch.tensor(value).to(self.device)
        dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
        return (tensor / self.world_size).item()

    def save_checkpoint(self, epoch: int, val_accuracy: float):
        """„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà‰øùÂ≠òÔºà„É©„É≥„ÇØ0„ÅÆ„ÅøÔºâ"""
        if self.rank == 0:
            os.makedirs(self.checkpoint_dir, exist_ok=True)
            checkpoint_path = os.path.join(
                self.checkpoint_dir,
                f"checkpoint_epoch_{epoch}.pt"
            )

            torch.save({
                'epoch': epoch,
                'model_state_dict': self.model.module.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'val_accuracy': val_accuracy,
            }, checkpoint_path)

            print(f"Checkpoint saved: {checkpoint_path}")

    def load_checkpoint(self, checkpoint_path: str):
        """„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„ÉàË™≠„ÅøËæº„Åø"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        self.model.module.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        return checkpoint['epoch']

    def train(self, num_epochs: int, save_freq: int = 10):
        """ÂÆåÂÖ®„Å™Ë®ìÁ∑¥„É´„Éº„Éó"""
        for epoch in range(num_epochs):
            # Ë®ìÁ∑¥
            train_loss, train_acc = self.train_epoch(epoch)

            # Ê§úË®º
            val_loss, val_acc = self.validate()

            # „É≠„Ç∞Âá∫ÂäõÔºà„É©„É≥„ÇØ0„ÅÆ„ÅøÔºâ
            if self.rank == 0:
                print(f"\nEpoch {epoch + 1}/{num_epochs}")
                print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
                print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

            # „ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà‰øùÂ≠ò
            if (epoch + 1) % save_freq == 0:
                self.save_checkpoint(epoch + 1, val_acc)


def distributed_training_worker(
    rank: int,
    world_size: int,
    model_class,
    train_dataset,
    val_dataset
):
    """ÂàÜÊï£Ë®ìÁ∑¥„ÉØ„Éº„Ç´„ÉºÈñ¢Êï∞"""
    # ÂàÜÊï£Áí∞Â¢É„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó
    setup_distributed(rank, world_size)

    # „É¢„Éá„É´‰ΩúÊàê
    model = model_class(input_dim=50, hidden_dims=[256, 128], output_dim=2, dropout=0.3)

    # „Éà„É¨„Éº„Éä„ÉºÂàùÊúüÂåñ
    trainer = DistributedTrainer(
        model=model,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        rank=rank,
        world_size=world_size,
        batch_size=256,
        learning_rate=0.001
    )

    # Ë®ìÁ∑¥ÂÆüË°å
    trainer.train(num_epochs=50, save_freq=10)

    # „ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó
    cleanup_distributed()


# ‰ΩøÁî®‰æã
if __name__ == "__main__":
    import torch.multiprocessing as mp
    from torch.utils.data import TensorDataset
    import numpy as np

    # „ÉÄ„Éü„Éº„Éá„Éº„Çø‰ΩúÊàê
    np.random.seed(42)
    X_train = torch.from_numpy(np.random.randn(100000, 50).astype(np.float32))
    y_train = torch.from_numpy(np.random.randint(0, 2, 100000).astype(np.int64))
    X_val = torch.from_numpy(np.random.randn(20000, 50).astype(np.float32))
    y_val = torch.from_numpy(np.random.randint(0, 2, 20000).astype(np.int64))

    train_dataset = TensorDataset(X_train, y_train)
    val_dataset = TensorDataset(X_val, y_val)

    # ÂàÜÊï£Ë®ìÁ∑¥Ëµ∑ÂãïÔºà4 GPUsÔºâ
    world_size = 4
    mp.spawn(
        distributed_training_worker,
        args=(world_size, NeuralNetwork, train_dataset, val_dataset),
        nprocs=world_size,
        join=True
    )
</code></pre>

<div class="info-box">
<strong>üí° ÂàÜÊï£Ë®ìÁ∑¥„ÅÆÈÅ∏ÊäûÂü∫Ê∫ñÔºö</strong>
<ul>
<li><strong>Data Parallel (DP)</strong>: Âçò‰∏Ä„Éé„Éº„Éâ„ÄÅË§áÊï∞GPUÔºà„Ç∑„É≥„Éó„É´„Å†„ÅåÈÅÖ„ÅÑÔºâ</li>
<li><strong>Distributed Data Parallel (DDP)</strong>: „Éû„É´„ÉÅ„Éé„Éº„Éâ„ÄÅÂäπÁéáÁöÑ„Å™ÂãæÈÖçÂêåÊúü</li>
<li><strong>Fully Sharded Data Parallel (FSDP)</strong>: Ë∂ÖÂ§ßË¶èÊ®°„É¢„Éá„É´ÔºàGPT-3„ÇØ„É©„ÇπÔºâ</li>
<li><strong>Model Parallel</strong>: Âçò‰∏ÄGPU„Å´‰πó„Çâ„Å™„ÅÑÂ∑®Â§ß„É¢„Éá„É´</li>
</ul>
</div>

<hr>

<h2>5.4 „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊúÄÈÅ©Âåñ</h2>

<h3>„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„Å®„Éú„Éà„É´„Éç„ÉÉ„ÇØÁâπÂÆö</h3>

<h4>„Ç≥„Éº„Éâ‰æã6: ÂàÜÊï£„Ç∑„Çπ„ÉÜ„É†„ÅÆ„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞</h4>

<pre><code>"""
ÂàÜÊï£Ê©üÊ¢∞Â≠¶Áøí„Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆ„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„Å®ÊúÄÈÅ©Âåñ
"""
import time
import psutil
import torch
from contextlib import contextmanager
from typing import Dict, List
import json
from dataclasses import dataclass, asdict
import numpy as np


@dataclass
class ProfileMetrics:
    """„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„É°„Éà„É™„ÇØ„Çπ"""
    stage_name: str
    execution_time: float
    cpu_percent: float
    memory_mb: float
    gpu_memory_mb: float = 0.0
    io_read_mb: float = 0.0
    io_write_mb: float = 0.0
    network_sent_mb: float = 0.0
    network_recv_mb: float = 0.0


class PerformanceProfiler:
    """„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Éó„É≠„Éï„Ç°„Ç§„É©„Éº"""

    def __init__(self, enable_gpu: bool = True):
        self.enable_gpu = enable_gpu and torch.cuda.is_available()
        self.metrics: List[ProfileMetrics] = []
        self.process = psutil.Process()

    @contextmanager
    def profile(self, stage_name: str):
        """„Çπ„ÉÜ„Éº„Ç∏„ÅÆ„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞"""
        # ÈñãÂßãÊôÇ„ÅÆ„É°„Éà„É™„ÇØ„Çπ
        start_time = time.time()
        start_cpu = self.process.cpu_percent()
        start_memory = self.process.memory_info().rss / 1024 / 1024  # MB

        io_start = self.process.io_counters()
        net_start = psutil.net_io_counters()

        if self.enable_gpu:
            torch.cuda.reset_peak_memory_stats()
            start_gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024

        try:
            yield
        finally:
            # ÁµÇ‰∫ÜÊôÇ„ÅÆ„É°„Éà„É™„ÇØ„Çπ
            end_time = time.time()
            end_cpu = self.process.cpu_percent()
            end_memory = self.process.memory_info().rss / 1024 / 1024

            io_end = self.process.io_counters()
            net_end = psutil.net_io_counters()

            # GPU „É°„É¢„É™
            if self.enable_gpu:
                end_gpu_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
            else:
                end_gpu_memory = 0.0

            # „É°„Éà„É™„ÇØ„ÇπË®òÈå≤
            metrics = ProfileMetrics(
                stage_name=stage_name,
                execution_time=end_time - start_time,
                cpu_percent=(start_cpu + end_cpu) / 2,
                memory_mb=end_memory - start_memory,
                gpu_memory_mb=end_gpu_memory - start_gpu_memory if self.enable_gpu else 0.0,
                io_read_mb=(io_end.read_bytes - io_start.read_bytes) / 1024 / 1024,
                io_write_mb=(io_end.write_bytes - io_start.write_bytes) / 1024 / 1024,
                network_sent_mb=(net_end.bytes_sent - net_start.bytes_sent) / 1024 / 1024,
                network_recv_mb=(net_end.bytes_recv - net_start.bytes_recv) / 1024 / 1024
            )

            self.metrics.append(metrics)

    def print_summary(self):
        """„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞ÁµêÊûú„Çµ„Éû„É™„ÉºË°®Á§∫"""
        print("\n" + "="*100)
        print("Performance Profiling Summary")
        print("="*100)
        print(f"{'Stage':<30} {'Time (s)':<12} {'CPU %':<10} {'Mem (MB)':<12} "
              f"{'GPU (MB)':<12} {'I/O Read':<12} {'I/O Write':<12}")
        print("-"*100)

        total_time = 0.0
        for m in self.metrics:
            print(f"{m.stage_name:<30} {m.execution_time:<12.2f} {m.cpu_percent:<10.1f} "
                  f"{m.memory_mb:<12.1f} {m.gpu_memory_mb:<12.1f} "
                  f"{m.io_read_mb:<12.1f} {m.io_write_mb:<12.1f}")
            total_time += m.execution_time

        print("-"*100)
        print(f"{'Total':<30} {total_time:<12.2f}")
        print("="*100)

    def get_bottlenecks(self, top_k: int = 3) -> List[ProfileMetrics]:
        """„Éú„Éà„É´„Éç„ÉÉ„ÇØÁâπÂÆö"""
        sorted_metrics = sorted(
            self.metrics,
            key=lambda m: m.execution_time,
            reverse=True
        )
        return sorted_metrics[:top_k]

    def export_json(self, output_path: str):
        """ÁµêÊûú„ÇíJSON„Å´„Ç®„ÇØ„Çπ„Éù„Éº„Éà"""
        data = [asdict(m) for m in self.metrics]
        with open(output_path, 'w') as f:
            json.dump(data, f, indent=2)


class DataLoaderOptimizer:
    """DataLoaderÊúÄÈÅ©Âåñ„Éò„É´„Éë„Éº"""

    @staticmethod
    def benchmark_dataloader(
        dataset,
        batch_sizes: List[int],
        num_workers_list: List[int],
        num_iterations: int = 100
    ) -> Dict:
        """DataLoaderË®≠ÂÆö„ÅÆÊúÄÈÅ©Âåñ"""
        results = []

        for batch_size in batch_sizes:
            for num_workers in num_workers_list:
                loader = torch.utils.data.DataLoader(
                    dataset,
                    batch_size=batch_size,
                    num_workers=num_workers,
                    pin_memory=True
                )

                start_time = time.time()
                for i, batch in enumerate(loader):
                    if i >= num_iterations:
                        break
                    _ = batch  # „Éá„Éº„Çø„É≠„Éº„Éâ
                elapsed = time.time() - start_time

                throughput = (batch_size * num_iterations) / elapsed

                results.append({
                    'batch_size': batch_size,
                    'num_workers': num_workers,
                    'throughput': throughput,
                    'time_per_batch': elapsed / num_iterations
                })

        # ÊúÄÈÅ©Ë®≠ÂÆö„ÇíË¶ã„Å§„Åë„Çã
        best_config = max(results, key=lambda x: x['throughput'])

        print("\nDataLoader Optimization Results:")
        print(f"Best Configuration: batch_size={best_config['batch_size']}, "
              f"num_workers={best_config['num_workers']}")
        print(f"Throughput: {best_config['throughput']:.2f} samples/sec")

        return best_config


class GPUOptimizer:
    """GPUÊúÄÈÅ©Âåñ„Éò„É´„Éë„Éº"""

    @staticmethod
    def optimize_memory():
        """GPU „É°„É¢„É™ÊúÄÈÅ©Âåñ"""
        if torch.cuda.is_available():
            # Êú™‰ΩøÁî®„Ç≠„É£„ÉÉ„Ç∑„É•„ÇØ„É™„Ç¢
            torch.cuda.empty_cache()

            # „É°„É¢„É™Áµ±Ë®àË°®Á§∫
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3

            print(f"\nGPU Memory Status:")
            print(f"Allocated: {allocated:.2f} GB")
            print(f"Reserved: {reserved:.2f} GB")

    @staticmethod
    def enable_auto_mixed_precision():
        """Ëá™ÂãïÊ∑∑ÂêàÁ≤æÂ∫¶Ë®ìÁ∑¥„ÇíÊúâÂäπÂåñ"""
        return torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 7

    @staticmethod
    def benchmark_precision(model, sample_input, num_iterations: int = 100):
        """Á≤æÂ∫¶Âà•„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉ"""
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
        sample_input = sample_input.to(device)

        results = {}

        # FP32
        model.float()
        start = time.time()
        for _ in range(num_iterations):
            with torch.no_grad():
                _ = model(sample_input.float())
        torch.cuda.synchronize()
        results['fp32'] = time.time() - start

        # FP16 (if available)
        if GPUOptimizer.enable_auto_mixed_precision():
            model.half()
            start = time.time()
            for _ in range(num_iterations):
                with torch.no_grad():
                    _ = model(sample_input.half())
            torch.cuda.synchronize()
            results['fp16'] = time.time() - start

        print("\nPrecision Benchmark:")
        for precision, elapsed in results.items():
            print(f"{precision.upper()}: {elapsed:.4f}s "
                  f"({num_iterations/elapsed:.2f} iter/s)")

        return results


# ‰ΩøÁî®‰æã
if __name__ == "__main__":
    # „Éó„É≠„Éï„Ç°„Ç§„É©„ÉºÂàùÊúüÂåñ
    profiler = PerformanceProfiler(enable_gpu=True)

    # „Éá„Éº„ÇøÊ∫ñÂÇô„ÅÆ„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞
    with profiler.profile("Data Loading"):
        data = torch.randn(100000, 50)
        time.sleep(0.5)  # „Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥

    # ÂâçÂá¶ÁêÜ„ÅÆ„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞
    with profiler.profile("Preprocessing"):
        normalized_data = (data - data.mean()) / data.std()
        time.sleep(0.3)

    # „É¢„Éá„É´Ë®ìÁ∑¥„ÅÆ„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞
    with profiler.profile("Model Training"):
        model = torch.nn.Linear(50, 10)
        optimizer = torch.optim.Adam(model.parameters())

        for _ in range(100):
            outputs = model(normalized_data)
            loss = outputs.sum()
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

    # ÁµêÊûúË°®Á§∫
    profiler.print_summary()

    # „Éú„Éà„É´„Éç„ÉÉ„ÇØÁâπÂÆö
    bottlenecks = profiler.get_bottlenecks(top_k=2)
    print("\nTop Bottlenecks:")
    for i, m in enumerate(bottlenecks, 1):
        print(f"{i}. {m.stage_name}: {m.execution_time:.2f}s")

    # ÁµêÊûú„Ç®„ÇØ„Çπ„Éù„Éº„Éà
    profiler.export_json("profiling_results.json")

    # DataLoaderÊúÄÈÅ©Âåñ
    dataset = torch.utils.data.TensorDataset(data, torch.zeros(len(data)))
    DataLoaderOptimizer.benchmark_dataloader(
        dataset,
        batch_sizes=[128, 256, 512],
        num_workers_list=[2, 4, 8],
        num_iterations=50
    )

    # GPUÊúÄÈÅ©Âåñ
    GPUOptimizer.optimize_memory()
    if torch.cuda.is_available():
        sample_model = torch.nn.Sequential(
            torch.nn.Linear(50, 128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, 10)
        )
        sample_input = torch.randn(32, 50)
        GPUOptimizer.benchmark_precision(sample_model, sample_input)
</code></pre>

<h3>I/OÊúÄÈÅ©Âåñ„Å®„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂäπÁéáÂåñ</h3>

<div class="info-box">
<strong>üí° I/OÊúÄÈÅ©Âåñ„ÅÆ„Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„ÇπÔºö</strong>
<ul>
<li><strong>„Éá„Éº„Çø„Éï„Ç©„Éº„Éû„ÉÉ„Éà</strong>: ParquetÔºàÂàóÊåáÂêëÔºâ„ÅØË°åÊåáÂêë„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÔºàCSVÔºâ„Çà„ÇäÈ´òÈÄü</li>
<li><strong>ÂúßÁ∏Æ</strong>: SnappyÂúßÁ∏Æ„ÅßË™≠„ÅøËæº„ÅøÈÄüÂ∫¶„Å®„Çπ„Éà„É¨„Éº„Ç∏ÂäπÁéá„Çí„Éê„É©„É≥„Çπ</li>
<li><strong>„Éó„É™„Éï„Çß„ÉÉ„ÉÅ</strong>: DataLoader„ÅÆ`num_workers`„Åß„Éê„ÉÉ„ÇØ„Ç∞„É©„Ç¶„É≥„Éâ„É≠„Éº„Éâ</li>
<li><strong>„É°„É¢„É™„Éû„ÉÉ„Éî„É≥„Ç∞</strong>: Â§ßË¶èÊ®°„Éï„Ç°„Ç§„É´„ÅØ„É°„É¢„É™„Éû„ÉÉ„Éó„ÅßÂäπÁéáÁöÑ„Å´„Ç¢„ÇØ„Çª„Çπ</li>
<li><strong>„Ç∑„É£„Éº„Éá„Ç£„É≥„Ç∞</strong>: „Éá„Éº„Çø„ÇíË§áÊï∞„Éï„Ç°„Ç§„É´„Å´ÂàÜÂâ≤„Åó‰∏¶ÂàóË™≠„ÅøËæº„Åø</li>
</ul>
</div>

<hr>

<h2>5.5 „Ç®„É≥„Éâ„ÉÑ„Éº„Ç®„É≥„Éâ„ÅÆÂÆüË£Ö‰æã</h2>

<h3>ÂÆåÂÖ®„Å™Â§ßË¶èÊ®°ML„Éë„Ç§„Éó„É©„Ç§„É≥</h3>

<h4>„Ç≥„Éº„Éâ‰æã7: Spark + PyTorch Áµ±Âêà„Éë„Ç§„Éó„É©„Ç§„É≥</h4>

<pre><code>"""
Spark„Å®PyTorch„ÇíÁµ±Âêà„Åó„ÅüÂ§ßË¶èÊ®°ML„Éë„Ç§„Éó„É©„Ç§„É≥
"""
from pyspark.sql import SparkSession
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
from typing import List, Tuple
import pickle


class SparkDatasetConverter:
    """Spark DataFrame„ÇíPyTorch Dataset„Å´Â§âÊèõ"""

    @staticmethod
    def spark_to_pytorch(
        spark_df,
        feature_column: str = "features",
        label_column: str = "label",
        output_path: str = "/tmp/pytorch_data"
    ):
        """
        Spark DataFrame„ÇíPyTorch„ÅßË™≠„ÅøËæº„ÅøÂèØËÉΩ„Å™ÂΩ¢Âºè„Åß‰øùÂ≠ò

        Args:
            spark_df: Spark DataFrame
            feature_column: ÁâπÂæ¥Èáè„Ç´„É©„É†Âêç
            label_column: „É©„Éô„É´„Ç´„É©„É†Âêç
            output_path: Âá∫Âäõ„Éë„Çπ
        """
        # PandasÁµåÁî±„ÅßÂ§âÊèõÔºàÂ∞è„Äú‰∏≠Ë¶èÊ®°„Éá„Éº„ÇøÔºâ
        # Â§ßË¶èÊ®°„Éá„Éº„Çø„ÅÆÂ†¥Âêà„ÅØ„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„Åî„Å®„Å´Âá¶ÁêÜ

        def convert_partition(iterator):
            """ÂêÑ„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„ÇíÂ§âÊèõ"""
            data_list = []
            for row in iterator:
                features = row[feature_column].toArray()
                label = row[label_column]
                data_list.append((features, label))

            # „Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„Åî„Å®„Å´„Éï„Ç°„Ç§„É´‰øùÂ≠ò
            import random
            partition_id = random.randint(0, 10000)
            output_file = f"{output_path}/partition_{partition_id}.pkl"

            with open(output_file, 'wb') as f:
                pickle.dump(data_list, f)

            yield (output_file, len(data_list))

        # ÂêÑ„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„ÇíÂá¶ÁêÜ
        spark_df.rdd.mapPartitions(convert_partition).collect()


class DistributedDataset(Dataset):
    """ÂàÜÊï£‰øùÂ≠ò„Åï„Çå„Åü„Éá„Éº„Çø„ÇíË™≠„ÅøËæº„ÇÄDataset"""

    def __init__(self, data_dir: str):
        from pathlib import Path

        self.data_files = list(Path(data_dir).glob("partition_*.pkl"))

        # ÂÖ®„Éá„Éº„Çø„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÇíÊßãÁØâ
        self.file_indices = []
        for file_path in self.data_files:
            with open(file_path, 'rb') as f:
                data = pickle.load(f)
                self.file_indices.append((file_path, len(data)))

        self.total_samples = sum(count for _, count in self.file_indices)

        # „Ç≠„É£„ÉÉ„Ç∑„É•Ôºà„É°„É¢„É™„Å´‰ΩôË£ï„Åå„ÅÇ„Çå„Å∞Ôºâ
        self.cache = {}

    def __len__(self):
        return self.total_samples

    def __getitem__(self, idx):
        # „Å©„ÅÆ„Éï„Ç°„Ç§„É´„ÅÆ„Å©„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÅãË®àÁÆó
        file_idx = 0
        cumsum = 0

        for i, (_, count) in enumerate(self.file_indices):
            if idx < cumsum + count:
                file_idx = i
                local_idx = idx - cumsum
                break
            cumsum += count

        # „Éï„Ç°„Ç§„É´„Åã„Çâ„Éá„Éº„ÇøË™≠„ÅøËæº„Åø
        file_path = self.file_indices[file_idx][0]

        if file_path not in self.cache:
            with open(file_path, 'rb') as f:
                self.cache[file_path] = pickle.load(f)

        features, label = self.cache[file_path][local_idx]

        return torch.FloatTensor(features), torch.LongTensor([label])[0]


class EndToEndMLPipeline:
    """„Ç®„É≥„Éâ„ÉÑ„Éº„Ç®„É≥„ÉâML„Éë„Ç§„Éó„É©„Ç§„É≥"""

    def __init__(
        self,
        spark_master: str = "local[*]",
        app_name: str = "EndToEndML"
    ):
        # SparkÂàùÊúüÂåñ
        self.spark = SparkSession.builder \
            .appName(app_name) \
            .master(spark_master) \
            .config("spark.sql.adaptive.enabled", "true") \
            .getOrCreate()

        self.profiler = PerformanceProfiler()

    def run_pipeline(
        self,
        data_path: str,
        model_class,
        model_params: dict,
        training_config: dict,
        output_dir: str = "./pipeline_output"
    ):
        """ÂÆåÂÖ®„Å™„Éë„Ç§„Éó„É©„Ç§„É≥ÂÆüË°å"""

        # „Çπ„ÉÜ„ÉÉ„Éó1: „Éá„Éº„ÇøË™≠„ÅøËæº„ÅøÔºàSparkÔºâ
        with self.profiler.profile("1. Data Loading (Spark)"):
            raw_df = self.spark.read.parquet(data_path)
            print(f"Loaded {raw_df.count()} records")

        # „Çπ„ÉÜ„ÉÉ„Éó2: „Éá„Éº„ÇøÊ§úË®º
        with self.profiler.profile("2. Data Validation"):
            # Âü∫Êú¨Áµ±Ë®àÈáè
            raw_df.describe().show()

            # NullÂÄ§„ÉÅ„Çß„ÉÉ„ÇØ
            null_counts = raw_df.select([
                F.count(F.when(F.col(c).isNull(), c)).alias(c)
                for c in raw_df.columns
            ])
            null_counts.show()

        # „Çπ„ÉÜ„ÉÉ„Éó3: ÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞ÔºàSparkÔºâ
        with self.profiler.profile("3. Feature Engineering (Spark)"):
            from pyspark.ml.feature import VectorAssembler, StandardScaler

            feature_cols = [c for c in raw_df.columns if c != 'label']

            assembler = VectorAssembler(
                inputCols=feature_cols,
                outputCol="features_raw"
            )
            df_assembled = assembler.transform(raw_df)

            scaler = StandardScaler(
                inputCol="features_raw",
                outputCol="features",
                withMean=True,
                withStd=True
            )
            scaler_model = scaler.fit(df_assembled)
            df_scaled = scaler_model.transform(df_assembled)

            # Ë®ìÁ∑¥/„ÉÜ„Çπ„Éà„Éá„Éº„ÇøÂàÜÂâ≤
            train_df, test_df = df_scaled.randomSplit([0.8, 0.2], seed=42)

        # „Çπ„ÉÜ„ÉÉ„Éó4: PyTorchÁî®„Éá„Éº„ÇøÂ§âÊèõ
        with self.profiler.profile("4. Spark to PyTorch Conversion"):
            train_data_dir = f"{output_dir}/train_data"
            test_data_dir = f"{output_dir}/test_data"

            SparkDatasetConverter.spark_to_pytorch(
                train_df.select("features", "label"),
                output_path=train_data_dir
            )
            SparkDatasetConverter.spark_to_pytorch(
                test_df.select("features", "label"),
                output_path=test_data_dir
            )

        # „Çπ„ÉÜ„ÉÉ„Éó5: PyTorch Dataset/DataLoader‰ΩúÊàê
        with self.profiler.profile("5. PyTorch DataLoader Setup"):
            train_dataset = DistributedDataset(train_data_dir)
            test_dataset = DistributedDataset(test_data_dir)

            train_loader = DataLoader(
                train_dataset,
                batch_size=training_config['batch_size'],
                shuffle=True,
                num_workers=4,
                pin_memory=True
            )
            test_loader = DataLoader(
                test_dataset,
                batch_size=training_config['batch_size'],
                num_workers=4
            )

        # „Çπ„ÉÜ„ÉÉ„Éó6: „É¢„Éá„É´Ë®ìÁ∑¥
        with self.profiler.profile("6. Model Training"):
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            model = model_class(**model_params).to(device)

            optimizer = torch.optim.Adam(
                model.parameters(),
                lr=training_config['learning_rate']
            )
            criterion = nn.CrossEntropyLoss()

            # Ë®ìÁ∑¥„É´„Éº„Éó
            for epoch in range(training_config['num_epochs']):
                model.train()
                train_loss = 0.0

                for inputs, labels in train_loader:
                    inputs, labels = inputs.to(device), labels.to(device)

                    optimizer.zero_grad()
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    loss.backward()
                    optimizer.step()

                    train_loss += loss.item()

                if (epoch + 1) % 10 == 0:
                    print(f"Epoch {epoch+1}: Loss = {train_loss/len(train_loader):.4f}")

        # „Çπ„ÉÜ„ÉÉ„Éó7: „É¢„Éá„É´Ë©ï‰æ°
        with self.profiler.profile("7. Model Evaluation"):
            model.eval()
            correct = 0
            total = 0

            with torch.no_grad():
                for inputs, labels in test_loader:
                    inputs, labels = inputs.to(device), labels.to(device)
                    outputs = model(inputs)
                    _, predicted = outputs.max(1)
                    total += labels.size(0)
                    correct += predicted.eq(labels).sum().item()

            accuracy = correct / total
            print(f"\nTest Accuracy: {accuracy:.4f}")

        # „Çπ„ÉÜ„ÉÉ„Éó8: „É¢„Éá„É´‰øùÂ≠ò
        with self.profiler.profile("8. Model Saving"):
            model_path = f"{output_dir}/model.pt"
            torch.save({
                'model_state_dict': model.state_dict(),
                'model_params': model_params,
                'accuracy': accuracy
            }, model_path)
            print(f"Model saved to {model_path}")

        # „Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞ÁµêÊûú
        self.profiler.print_summary()
        self.profiler.export_json(f"{output_dir}/profiling.json")

        return model, accuracy


# ‰ΩøÁî®‰æã
if __name__ == "__main__":
    # „Éë„Ç§„Éó„É©„Ç§„É≥ÂÆüË°å
    pipeline = EndToEndMLPipeline(
        spark_master="spark://master:7077",
        app_name="CustomerChurnPrediction"
    )

    # „É¢„Éá„É´ÂÆöÁæ©
    class ChurnPredictor(nn.Module):
        def __init__(self, input_dim, hidden_dims, output_dim):
            super().__init__()
            layers = []
            prev_dim = input_dim
            for hidden_dim in hidden_dims:
                layers.extend([
                    nn.Linear(prev_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Dropout(0.3)
                ])
                prev_dim = hidden_dim
            layers.append(nn.Linear(prev_dim, output_dim))
            self.network = nn.Sequential(*layers)

        def forward(self, x):
            return self.network(x)

    # „Éë„Ç§„Éó„É©„Ç§„É≥ÂÆüË°å
    model, accuracy = pipeline.run_pipeline(
        data_path="s3://my-bucket/customer-data/",
        model_class=ChurnPredictor,
        model_params={
            'input_dim': 50,
            'hidden_dims': [256, 128, 64],
            'output_dim': 2
        },
        training_config={
            'batch_size': 512,
            'learning_rate': 0.001,
            'num_epochs': 50
        },
        output_dir="./churn_prediction_output"
    )
</code></pre>

<h3>Áõ£Ë¶ñ„Å®„É°„É≥„ÉÜ„Éä„É≥„Çπ</h3>

<h4>„Ç≥„Éº„Éâ‰æã8: „É™„Ç¢„É´„Çø„Ç§„É†Áõ£Ë¶ñ„Ç∑„Çπ„ÉÜ„É†</h4>

<pre><code>"""
ML„Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆÁõ£Ë¶ñ„Å®„Ç¢„É©„Éº„Éà„Ç∑„Çπ„ÉÜ„É†
"""
import time
import threading
from dataclasses import dataclass
from typing import Dict, List, Callable, Optional
from datetime import datetime
import json


@dataclass
class Metric:
    """„É°„Éà„É™„ÇØ„Çπ"""
    name: str
    value: float
    timestamp: datetime
    tags: Dict[str, str] = None


class MetricsCollector:
    """„É°„Éà„É™„ÇØ„ÇπÂèéÈõÜ"""

    def __init__(self):
        self.metrics: List[Metric] = []
        self.lock = threading.Lock()

    def record(self, name: str, value: float, tags: Dict[str, str] = None):
        """„É°„Éà„É™„ÇØ„ÇπË®òÈå≤"""
        with self.lock:
            metric = Metric(
                name=name,
                value=value,
                timestamp=datetime.now(),
                tags=tags or {}
            )
            self.metrics.append(metric)

    def get_latest(self, name: str, n: int = 1) -> List[Metric]:
        """ÊúÄÊñ∞„ÅÆ„É°„Éà„É™„ÇØ„ÇπÂèñÂæó"""
        with self.lock:
            filtered = [m for m in self.metrics if m.name == name]
            return sorted(filtered, key=lambda m: m.timestamp, reverse=True)[:n]

    def get_average(self, name: str, window_seconds: int = 60) -> Optional[float]:
        """ÊúüÈñìÂÜÖ„ÅÆÂπ≥ÂùáÂÄ§"""
        now = datetime.now()
        with self.lock:
            recent = [
                m for m in self.metrics
                if m.name == name and (now - m.timestamp).total_seconds() <= window_seconds
            ]
            if not recent:
                return None
            return sum(m.value for m in recent) / len(recent)


class AlertRule:
    """„Ç¢„É©„Éº„Éà„É´„Éº„É´"""

    def __init__(
        self,
        name: str,
        metric_name: str,
        threshold: float,
        comparison: str = "greater",  # greater, less, equal
        window_seconds: int = 60,
        callback: Callable = None
    ):
        self.name = name
        self.metric_name = metric_name
        self.threshold = threshold
        self.comparison = comparison
        self.window_seconds = window_seconds
        self.callback = callback or self.default_callback

    def check(self, collector: MetricsCollector) -> bool:
        """„É´„Éº„É´„ÉÅ„Çß„ÉÉ„ÇØ"""
        avg_value = collector.get_average(self.metric_name, self.window_seconds)

        if avg_value is None:
            return False

        if self.comparison == "greater":
            triggered = avg_value > self.threshold
        elif self.comparison == "less":
            triggered = avg_value < self.threshold
        else:  # equal
            triggered = abs(avg_value - self.threshold) < 0.0001

        if triggered:
            self.callback(self.name, self.metric_name, avg_value, self.threshold)

        return triggered

    def default_callback(self, rule_name, metric_name, value, threshold):
        """„Éá„Éï„Ç©„É´„Éà„ÅÆ„Ç¢„É©„Éº„Éà„Ç≥„Éº„É´„Éê„ÉÉ„ÇØ"""
        print(f"\n‚ö†Ô∏è  ALERT: {rule_name}")
        print(f"   Metric: {metric_name} = {value:.4f}")
        print(f"   Threshold: {threshold:.4f}")
        print(f"   Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


class MonitoringSystem:
    """Áµ±ÂêàÁõ£Ë¶ñ„Ç∑„Çπ„ÉÜ„É†"""

    def __init__(self, check_interval: int = 10):
        self.collector = MetricsCollector()
        self.alert_rules: List[AlertRule] = []
        self.check_interval = check_interval
        self.running = False
        self.monitor_thread = None

    def add_alert_rule(self, rule: AlertRule):
        """„Ç¢„É©„Éº„Éà„É´„Éº„É´ËøΩÂä†"""
        self.alert_rules.append(rule)

    def start(self):
        """Áõ£Ë¶ñÈñãÂßã"""
        self.running = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        print("Monitoring system started")

    def stop(self):
        """Áõ£Ë¶ñÂÅúÊ≠¢"""
        self.running = False
        if self.monitor_thread:
            self.monitor_thread.join()
        print("Monitoring system stopped")

    def _monitor_loop(self):
        """Áõ£Ë¶ñ„É´„Éº„Éó"""
        while self.running:
            for rule in self.alert_rules:
                rule.check(self.collector)
            time.sleep(self.check_interval)

    def export_metrics(self, output_path: str):
        """„É°„Éà„É™„ÇØ„Çπ„Ç®„ÇØ„Çπ„Éù„Éº„Éà"""
        with self.collector.lock:
            data = [
                {
                    'name': m.name,
                    'value': m.value,
                    'timestamp': m.timestamp.isoformat(),
                    'tags': m.tags
                }
                for m in self.collector.metrics
            ]

        with open(output_path, 'w') as f:
            json.dump(data, f, indent=2)


# ‰ΩøÁî®‰æã
if __name__ == "__main__":
    # Áõ£Ë¶ñ„Ç∑„Çπ„ÉÜ„É†ÂàùÊúüÂåñ
    monitor = MonitoringSystem(check_interval=5)

    # „Ç¢„É©„Éº„Éà„É´„Éº„É´Ë®≠ÂÆö
    monitor.add_alert_rule(AlertRule(
        name="High Training Loss",
        metric_name="train_loss",
        threshold=0.5,
        comparison="greater",
        window_seconds=30
    ))

    monitor.add_alert_rule(AlertRule(
        name="Low Validation Accuracy",
        metric_name="val_accuracy",
        threshold=0.80,
        comparison="less",
        window_seconds=60
    ))

    monitor.add_alert_rule(AlertRule(
        name="High GPU Memory Usage",
        metric_name="gpu_memory_percent",
        threshold=90.0,
        comparison="greater",
        window_seconds=30
    ))

    # Áõ£Ë¶ñÈñãÂßã
    monitor.start()

    # „Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥: „É°„Éà„É™„ÇØ„ÇπË®òÈå≤
    try:
        for i in range(50):
            # Ë®ìÁ∑¥„É°„Éà„É™„ÇØ„ÇπÔºàÂæê„ÄÖ„Å´ÊîπÂñÑÔºâ
            train_loss = 1.0 / (i + 1) + 0.1
            val_acc = min(0.95, 0.5 + i * 0.01)
            gpu_mem = 70 + (i % 5) * 5

            monitor.collector.record("train_loss", train_loss)
            monitor.collector.record("val_accuracy", val_acc)
            monitor.collector.record("gpu_memory_percent", gpu_mem)

            # Áï∞Â∏∏ÂÄ§„ÇíÊôÇ„ÄÖÊ≥®ÂÖ•
            if i == 20:
                monitor.collector.record("train_loss", 0.8)  # „Ç¢„É©„Éº„ÉàÁô∫ÁÅ´
            if i == 30:
                monitor.collector.record("val_accuracy", 0.75)  # „Ç¢„É©„Éº„ÉàÁô∫ÁÅ´
            if i == 40:
                monitor.collector.record("gpu_memory_percent", 95.0)  # „Ç¢„É©„Éº„ÉàÁô∫ÁÅ´

            time.sleep(1)

    except KeyboardInterrupt:
        pass
    finally:
        # Áõ£Ë¶ñÂÅúÊ≠¢„Å®„É°„Éà„É™„ÇØ„Çπ„Ç®„ÇØ„Çπ„Éù„Éº„Éà
        monitor.stop()
        monitor.export_metrics("monitoring_metrics.json")
        print("\nMetrics exported to monitoring_metrics.json")
</code></pre>

<div class="success-box">
<strong>‚úÖ Êú¨Áï™Áí∞Â¢É„ÅÆÁõ£Ë¶ñÈ†ÖÁõÆÔºö</strong>
<ul>
<li><strong>„É¢„Éá„É´ÊÄßËÉΩ</strong>: Á≤æÂ∫¶„ÄÅF1„Çπ„Ç≥„Ç¢„ÄÅAUC-ROC„ÄÅÊé®Ë´ñ„É¨„Ç§„ÉÜ„É≥„Ç∑</li>
<li><strong>„Éá„Éº„ÇøÂìÅË≥™</strong>: Ê¨†ÊêçÁéá„ÄÅÁï∞Â∏∏ÂÄ§Áéá„ÄÅ„Éá„Éº„Çø„Éâ„É™„Éï„ÉàÊ§úÂá∫</li>
<li><strong>„Ç∑„Çπ„ÉÜ„É†„É™„ÇΩ„Éº„Çπ</strong>: CPU/GPU‰ΩøÁî®Áéá„ÄÅ„É°„É¢„É™‰ΩøÁî®Èáè„ÄÅ„Éá„Ç£„Çπ„ÇØI/O</li>
<li><strong>ÂèØÁî®ÊÄß</strong>: „Ç¢„ÉÉ„Éó„Çø„Ç§„É†„ÄÅ„Ç®„É©„ÉºÁéá„ÄÅ„É™„ÇØ„Ç®„Çπ„Éà„Çπ„É´„Éº„Éó„ÉÉ„Éà</li>
<li><strong>„Ç≥„Çπ„Éà</strong>: „ÇØ„É©„Ç¶„Éâ„É™„ÇΩ„Éº„Çπ‰ΩøÁî®Èáè„ÄÅÊé®Ë´ñ„Ç≥„Çπ„Éà per request</li>
</ul>
</div>

<hr>

<h2>Á∑¥ÁøíÂïèÈ°å</h2>

<div class="exercise-box">
<h3>ÂïèÈ°å1: „Éë„Ç§„Éó„É©„Ç§„É≥Ë®≠Ë®à</h3>
<p><strong>ÂïèÈ°å:</strong> 1Êó•„ÅÇ„Åü„Çä10TB„ÅÆÊñ∞Ë¶è„Éá„Éº„Çø„ÅåÁîüÊàê„Åï„Çå„ÇãÊé®Ëñ¶„Ç∑„Çπ„ÉÜ„É†„ÇíË®≠Ë®à„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ‰ª•‰∏ã„ÅÆË¶Å‰ª∂„ÇíÊ∫Ä„Åü„Åô„Éë„Ç§„Éó„É©„Ç§„É≥„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÇíÊèêÊ°à„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö</p>
<ul>
<li>„Éá„Éº„ÇøÂèñ„ÇäËæº„Åø„Åã„ÇâÊé®Ëñ¶ÁîüÊàê„Åæ„Åß4ÊôÇÈñì‰ª•ÂÜÖ</li>
<li>99.9%„ÅÆÂèØÁî®ÊÄß</li>
<li>A/B„ÉÜ„Çπ„ÉàÊ©üËÉΩ</li>
<li>„É¢„Éá„É´„ÅÆËá™ÂãïÂÜçË®ìÁ∑¥</li>
</ul>

<p><strong>„Éí„É≥„Éà:</strong></p>
<ul>
<li>Â¢óÂàÜÂ≠¶ÁøíÔºàIncremental LearningÔºâ„ÇíÊ§úË®é</li>
<li>ÁâπÂæ¥Èáè„Çπ„Éà„Ç¢ÔºàFeature StoreÔºâ„Åß„Éá„Éº„ÇøÂÜçÂà©Áî®</li>
<li>„Éû„É´„ÉÅ„Ç¢„Éº„É†„Éê„É≥„Éá„Ç£„ÉÉ„Éà for A/B„ÉÜ„Çπ„Éà</li>
<li>„É¢„Éá„É´ÊÄßËÉΩ„ÅÆËá™ÂãïÁõ£Ë¶ñ„Å®„Éà„É™„Ç¨„Éº</li>
</ul>
</div>

<div class="exercise-box">
<h3>ÂïèÈ°å2: ÂàÜÊï£Ë®ìÁ∑¥„ÅÆÊúÄÈÅ©Âåñ</h3>
<p><strong>ÂïèÈ°å:</strong> 8„Éé„Éº„Éâ x 4 GPU (ÂêàË®à32 GPU) „ÅÆ„ÇØ„É©„Çπ„Çø„ÅßÁîªÂÉèÂàÜÈ°û„É¢„Éá„É´„ÇíË®ìÁ∑¥„Åó„Åæ„Åô„ÄÇ‰ª•‰∏ã„ÅÆÊúÄÈÅ©Âåñ„ÇíÂÆüË£Ö„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö</p>
<ul>
<li>ÂäπÁéáÁöÑ„Å™ÂãæÈÖçÂêåÊúüÊà¶Áï•</li>
<li>„Éá„Éº„Çø„É≠„Éº„ÉÄ„Éº„ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÉÅ„É•„Éº„Éã„É≥„Ç∞</li>
<li>Ê∑∑ÂêàÁ≤æÂ∫¶Ë®ìÁ∑¥„ÅÆÈÅ©Áî®</li>
<li>„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„ÉàÊà¶Áï•</li>
</ul>

<p><strong>ÊúüÂæÖ„Åï„Çå„ÇãÊîπÂñÑ:</strong></p>
<ul>
<li>Ë®ìÁ∑¥ÊôÇÈñì„ÇíÂçò‰∏ÄGPUÊØî„Åß25ÂÄç‰ª•‰∏ä„Å´Áü≠Á∏Æ</li>
<li>GPU‰ΩøÁî®Áéá„Çí90%‰ª•‰∏ä„Å´Á∂≠ÊåÅ</li>
<li>„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂ∏ØÂüüÂπÖ„ÅÆÂäπÁéáÁöÑÂà©Áî®</li>
</ul>
</div>

<div class="exercise-box">
<h3>ÂïèÈ°å3: „Ç≥„Çπ„ÉàÊúÄÈÅ©Âåñ</h3>
<p><strong>ÂïèÈ°å:</strong> ÊúàÈñì„ÅÆ„ÇØ„É©„Ç¶„Éâ„Ç≥„Çπ„Éà„Åå$50,000„Å´ÈÅî„Åó„Å¶„ÅÑ„ÇãML„Éë„Ç§„Éó„É©„Ç§„É≥„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ„Ç≥„Çπ„Éà„Çí30%ÂâäÊ∏õ„Åó„Å§„Å§„ÄÅ„É¢„Éá„É´ÊÄßËÉΩ„ÇíÁ∂≠ÊåÅ„Åô„ÇãÊà¶Áï•„ÇíÊèêÊ°à„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>

<p><strong>ËÄÉÊÖÆ„Åô„Åπ„ÅçË¶ÅÁ¥†:</strong></p>
<ul>
<li>„Çπ„Éù„ÉÉ„Éà„Ç§„É≥„Çπ„Çø„É≥„Çπ„ÅÆÊ¥ªÁî®</li>
<li>„Ç™„Éº„Éà„Çπ„Ç±„Éº„É™„É≥„Ç∞Ë®≠ÂÆö</li>
<li>„Çπ„Éà„É¨„Éº„Ç∏ÈöéÂ±§ÂåñÔºàHot/Cold dataÔºâ</li>
<li>Ë®àÁÆó„É™„ÇΩ„Éº„Çπ„ÅÆÊúÄÈÅ©Âåñ</li>
<li>‰∏çË¶Å„Å™„Éë„Ç§„Éó„É©„Ç§„É≥ÂÆüË°å„ÅÆÂâäÊ∏õ</li>
</ul>
</div>

<div class="exercise-box">
<h3>ÂïèÈ°å4: „Éá„Éº„Çø„Éâ„É™„Éï„ÉàÊ§úÂá∫</h3>
<p><strong>ÂïèÈ°å:</strong> Êú¨Áï™Áí∞Â¢É„Åß„Éá„Éº„Çø„Éâ„É™„Éï„Éà„ÇíËá™ÂãïÊ§úÂá∫„Åô„Çã„Ç∑„Çπ„ÉÜ„É†„ÇíÂÆüË£Ö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ‰ª•‰∏ã„ÇíÂê´„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑÔºö</p>
<ul>
<li>Áµ±Ë®àÁöÑÊ§úÂÆöÔºàKSÊ§úÂÆö„ÄÅ„Ç´„Ç§‰∫å‰πóÊ§úÂÆöÔºâ</li>
<li>ÂàÜÂ∏É„ÅÆÂèØË¶ñÂåñ</li>
<li>„Ç¢„É©„Éº„ÉàÈñæÂÄ§„ÅÆË®≠ÂÆö</li>
<li>Ëá™ÂãïÂÜçË®ìÁ∑¥„ÅÆ„Éà„É™„Ç¨„Éº</li>
</ul>

<p><strong>ÂÆüË£Ö‰æã:</strong></p>
<pre><code>class DataDriftDetector:
    def detect_drift(self, reference_data, current_data):
        # KSÊ§úÂÆöÂÆüË£Ö
        pass

    def visualize_distributions(self, feature_name):
        # ÂàÜÂ∏ÉÊØîËºÉ„Éó„É≠„ÉÉ„Éà
        pass

    def trigger_retraining(self, drift_score, threshold=0.05):
        # ÂÜçË®ìÁ∑¥„Éà„É™„Ç¨„Éº
        pass
</code></pre>
</div>

<div class="exercise-box">
<h3>ÂïèÈ°å5: „Ç®„É≥„Éâ„ÉÑ„Éº„Ç®„É≥„Éâ„Éë„Ç§„Éó„É©„Ç§„É≥ÂÆüË£Ö</h3>
<p><strong>ÂïèÈ°å:</strong> ‰ª•‰∏ã„ÅÆË¶Å‰ª∂„ÇíÊ∫Ä„Åü„ÅôÂÆåÂÖ®„Å™ML„Éë„Ç§„Éó„É©„Ç§„É≥„ÇíÂÆüË£Ö„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö</p>

<p><strong>„Éá„Éº„Çø:</strong> Kaggle„ÅÆ "Credit Card Fraud Detection" „Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºà284,807‰ª∂Ôºâ</p>

<p><strong>Ë¶Å‰ª∂:</strong></p>
<ul>
<li>Spark„Åß„Éá„Éº„ÇøÂâçÂá¶ÁêÜÔºàÊ¨†ÊêçÂÄ§Âá¶ÁêÜ„ÄÅÊ≠£Ë¶èÂåñ„ÄÅ‰∏çÂùáË°°„Éá„Éº„ÇøÂØæÂøúÔºâ</li>
<li>Ray Tune„Åß„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøÊúÄÈÅ©ÂåñÔºà100Ë©¶Ë°åÔºâ</li>
<li>ÂàÜÊï£Ë®ìÁ∑¥ÔºàË§áÊï∞GPUÂØæÂøúÔºâ</li>
<li>„É¢„Éá„É´Ë©ï‰æ°ÔºàPrecision, Recall, F1, AUC-ROCÔºâ</li>
<li>„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞</li>
<li>Áõ£Ë¶ñ„Ç∑„Çπ„ÉÜ„É†„ÅÆÁµ±Âêà</li>
</ul>

<p><strong>Ë©ï‰æ°Âü∫Ê∫ñ:</strong></p>
<ul>
<li>F1„Çπ„Ç≥„Ç¢ > 0.85</li>
<li>Ë®ìÁ∑¥ÊôÇÈñì < 30ÂàÜÔºà4 GPUÁí∞Â¢ÉÔºâ</li>
<li>ÂÆåÂÖ®„Å™ÂÜçÁèæÊÄßÔºà„Ç∑„Éº„ÉâÂõ∫ÂÆöÔºâ</li>
<li>„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„É¨„Éù„Éº„ÉàÁîüÊàê</li>
</ul>
</div>

<hr>

<h2>„Åæ„Å®„ÇÅ</h2>

<p>„Åì„ÅÆÁ´†„Åß„ÅØ„ÄÅÂ§ßË¶èÊ®°Ê©üÊ¢∞Â≠¶Áøí„Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆË®≠Ë®à„Å®ÂÆüË£Ö„Å´„Å§„ÅÑ„Å¶Â≠¶„Å≥„Åæ„Åó„ÅüÔºö</p>

<table>
<thead>
<tr>
<th>„Éà„Éî„ÉÉ„ÇØ</th>
<th>‰∏ªË¶Å„Å™Â≠¶ÁøíÂÜÖÂÆπ</th>
<th>ÂÆüË∑µ„Çπ„Ç≠„É´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>„Éë„Ç§„Éó„É©„Ç§„É≥Ë®≠Ë®à</strong></td>
<td>„Çπ„Ç±„Éº„É©„Éì„É™„ÉÜ„Ç£„ÄÅ„Éï„Ç©„Éº„É´„Éà„Éà„É¨„É©„É≥„Çπ„ÄÅÁõ£Ë¶ñ</td>
<td>Ë®≠ÂÆöÁÆ°ÁêÜ„ÄÅ„Ç®„É©„Éº„Éè„É≥„Éâ„É™„É≥„Ç∞„ÄÅ„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà</td>
</tr>
<tr>
<td><strong>„Éá„Éº„ÇøÂá¶ÁêÜ</strong></td>
<td>Spark ETL„ÄÅ„Éá„Éº„ÇøÊ§úË®º„ÄÅÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞</td>
<td>ÂàÜÊï£„Éá„Éº„ÇøÂ§âÊèõ„ÄÅÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØ„ÄÅÊúÄÈÅ©Âåñ</td>
</tr>
<tr>
<td><strong>ÂàÜÊï£Ë®ìÁ∑¥</strong></td>
<td>DDP„ÄÅRay Tune„ÄÅ„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøÊúÄÈÅ©Âåñ</td>
<td>„Éû„É´„ÉÅ„Éé„Éº„ÉâË®ìÁ∑¥„ÄÅÂäπÁéáÁöÑ„Å™HPO</td>
</tr>
<tr>
<td><strong>„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊúÄÈÅ©Âåñ</strong></td>
<td>„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„ÄÅI/OÊúÄÈÅ©Âåñ„ÄÅGPUÊ¥ªÁî®</td>
<td>„Éú„Éà„É´„Éç„ÉÉ„ÇØÁâπÂÆö„ÄÅÊ∑∑ÂêàÁ≤æÂ∫¶Ë®ìÁ∑¥</td>
</tr>
<tr>
<td><strong>Êú¨Áï™ÈÅãÁî®</strong></td>
<td>Áõ£Ë¶ñ„ÄÅ„Ç¢„É©„Éº„Éà„ÄÅ„Ç≥„Çπ„ÉàÊúÄÈÅ©Âåñ</td>
<td>„É°„Éà„É™„ÇØ„ÇπÂèéÈõÜ„ÄÅÁï∞Â∏∏Ê§úÁü•„ÄÅËá™ÂãïÂåñ</td>
</tr>
</tbody>
</table>

<h3>Ê¨°„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó</h3>

<ul>
<li><strong>Kubernetes„Åß„ÅÆ„Éá„Éó„É≠„Ç§</strong>: ML„Éë„Ç§„Éó„É©„Ç§„É≥„Çí„Ç≥„É≥„ÉÜ„ÉäÂåñ„Åó„ÄÅK8s„ÅßÈÅãÁî®</li>
<li><strong>„Çπ„Éà„É™„Éº„Éü„É≥„Ç∞Âá¶ÁêÜ</strong>: Kafka + Spark Streaming„Åß„É™„Ç¢„É´„Çø„Ç§„É†ML</li>
<li><strong>MLOpsÁµ±Âêà</strong>: MLflow„ÄÅKubeflow„ÄÅSageMaker„Å®„ÅÆÁµ±Âêà</li>
<li><strong>„É¢„Éá„É´„Çµ„Éº„Éì„É≥„Ç∞</strong>: TorchServe„ÄÅTensorFlow Serving„Åß„ÅÆÊé®Ë´ñÊúÄÈÅ©Âåñ</li>
<li><strong>AutoML</strong>: Ëá™ÂãïÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞„ÄÅ„Éã„É•„Éº„É©„É´„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£Êé¢Á¥¢</li>
</ul>

<div class="info-box">
<strong>üí° ÂÆüÂãô„Åß„ÅÆÂøúÁî®:</strong>
<p>Â§ßË¶èÊ®°ML„Éë„Ç§„Éó„É©„Ç§„É≥„ÅØ„ÄÅÊé®Ëñ¶„Ç∑„Çπ„ÉÜ„É†„ÄÅ‰∏çÊ≠£Ê§úÁü•„ÄÅÈúÄË¶Å‰∫àÊ∏¨„Å™„Å©„ÄÅ„Éì„Ç∏„Éç„Çπ„ÇØ„É™„ÉÜ„Ç£„Ç´„É´„Å™„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„Åß‰ΩøÁî®„Åï„Çå„Åæ„Åô„ÄÇÊú¨Á´†„ÅßÂ≠¶„Çì„Å†ÊäÄË°ì„ÅØ„ÄÅ„Éá„Éº„Çø„Çµ„Ç§„Ç®„É≥„ÉÜ„Ç£„Çπ„Éà„Åã„ÇâML„Ç®„É≥„Ç∏„Éã„Ç¢„Å∏„ÅÆ„Ç≠„É£„É™„Ç¢„Éë„Çπ„Å´„Åä„ÅÑ„Å¶ÈáçË¶Å„Å™„Çπ„Ç≠„É´„Åß„Åô„ÄÇ</p>
</div>

<hr>

<h2>ÂèÇËÄÉÊñáÁåÆ</h2>

<h3>Êõ∏Á±ç</h3>
<ul>
<li>Kleppmann, M. (2017). <em>Designing Data-Intensive Applications</em>. O'Reilly Media.</li>
<li>Ryza, S. et al. (2017). <em>Advanced Analytics with Spark</em> (2nd ed.). O'Reilly Media.</li>
<li>Huyen, C. (2022). <em>Designing Machine Learning Systems</em>. O'Reilly Media.</li>
<li>Gift, N. & Deza, A. (2021). <em>Practical MLOps</em>. O'Reilly Media.</li>
</ul>

<h3>Ë´ñÊñá</h3>
<ul>
<li>Zaharia, M. et al. (2016). "Apache Spark: A Unified Engine for Big Data Processing". <em>Communications of the ACM</em>, 59(11), 56-65.</li>
<li>Li, M. et al. (2014). "Scaling Distributed Machine Learning with the Parameter Server". <em>OSDI</em>, 14, 583-598.</li>
<li>Goyal, P. et al. (2017). "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour". <em>arXiv:1706.02677</em>.</li>
<li>Liaw, R. et al. (2018). "Tune: A Research Platform for Distributed Model Selection and Training". <em>arXiv:1807.05118</em>.</li>
</ul>

<h3>ÂÖ¨Âºè„Éâ„Ç≠„É•„É°„É≥„Éà</h3>
<ul>
<li><a href="https://spark.apache.org/docs/latest/">Apache Spark Documentation</a></li>
<li><a href="https://pytorch.org/tutorials/beginner/dist_overview.html">PyTorch Distributed Overview</a></li>
<li><a href="https://docs.ray.io/en/latest/tune/index.html">Ray Tune Documentation</a></li>
<li><a href="https://mlflow.org/docs/latest/index.html">MLflow Documentation</a></li>
</ul>

<h3>„Ç™„É≥„É©„Ç§„É≥„É™„ÇΩ„Éº„Çπ</h3>
<ul>
<li><a href="https://github.com/databricks/spark-deep-learning">Databricks Spark Deep Learning</a></li>
<li><a href="https://www.tensorflow.org/guide/distributed_training">TensorFlow Distributed Training Guide</a></li>
<li><a href="https://engineering.fb.com/2020/08/06/ml-applications/scaling-machine-learning-infrastructure/">Facebook: Scaling ML Infrastructure</a></li>
<li><a href="https://netflixtechblog.com/distributed-time-travel-for-feature-generation-389cccdd3907">Netflix: Distributed Feature Generation</a></li>
</ul>

<h3>„ÉÑ„Éº„É´„Éª„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ</h3>
<ul>
<li><strong>Apache Spark</strong>: <a href="https://spark.apache.org/">https://spark.apache.org/</a></li>
<li><strong>Ray</strong>: <a href="https://www.ray.io/">https://www.ray.io/</a></li>
<li><strong>Horovod</strong>: <a href="https://horovod.ai/">https://horovod.ai/</a></li>
<li><strong>Kubeflow</strong>: <a href="https://www.kubeflow.org/">https://www.kubeflow.org/</a></li>
<li><strong>Feast (Feature Store)</strong>: <a href="https://feast.dev/">https://feast.dev/</a></li>
</ul>

    </main>

    <div class="nav-links">
        <a href="chapter4-distributed-ml-frameworks.html" class="nav-button">‚Üê Á¨¨4Á´†: ÂàÜÊï£Ê©üÊ¢∞Â≠¶Áøí„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ</a>
        <a href="index.html" class="nav-button">ÁõÆÊ¨°„Å´Êàª„Çã</a>
    </div>

    <footer>
        <p>&copy; 2024 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
