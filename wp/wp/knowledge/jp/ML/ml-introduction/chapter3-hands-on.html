<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第3章：Pythonで体験する機械学習 - 実践的な予測モデル構築 - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #667eea;
            --color-accent-light: #764ba2;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(102, 126, 234, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        .info-box {
            background-color: #e3f2fd;
            border-left: 4px solid #2196f3;
            padding: var(--spacing-md);
            margin: var(--spacing-md) 0;
            border-radius: var(--border-radius);
        }

        .warning-box {
            background-color: #fff3e0;
            border-left: 4px solid #ff9800;
            padding: var(--spacing-md);
            margin: var(--spacing-md) 0;
            border-radius: var(--border-radius);
        }

        .success-box {
            background-color: #e8f5e9;
            border-left: 4px solid #4caf50;
            padding: var(--spacing-md);
            margin: var(--spacing-md) 0;
            border-radius: var(--border-radius);
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>第3章：Pythonで体験する機械学習</h1>
            <p class="subtitle">実践的な予測モデル構築 - 環境構築から実プロジェクトまで</p>
            <div class="meta">
                <span class="meta-item">📖 読了時間: 30-40分</span>
                <span class="meta-item">📊 難易度: 中級</span>
                <span class="meta-item">💻 コード例: 35個</span>
                <span class="meta-item">📝 演習問題: 5問</span>
            </div>
        </div>
    </header>

    <main class="container">
        <p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #667eea; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">
            この章では、Pythonを使って実際に機械学習モデルを構築します。環境構築から始めて、回帰・分類問題の実装、モデル比較、ハイパーパラメータチューニング、そして実プロジェクトまで、35個の実行可能なコード例で学びます。
        </p>

        <div class="learning-objectives">
            <h2>学習目標</h2>
            <ul>
                <li>✅ Python環境を3つの方法（Anaconda/venv/Colab）で構築できる</li>
                <li>✅ データの読み込み、前処理、可視化の基本操作ができる</li>
                <li>✅ 回帰問題（住宅価格予測）を実装し評価できる</li>
                <li>✅ 分類問題（Iris分類）で複数のモデルを比較できる</li>
                <li>✅ ハイパーパラメータチューニングを実行できる</li>
                <li>✅ 特徴量エンジニアリングの基本技術を適用できる</li>
                <li>✅ Titanicデータセットで実プロジェクトを完成できる</li>
            </ul>
        </div>

        <h2>3.1 環境構築：3つの選択肢</h2>
        <p>機械学習を実践するには、まずPython環境を構築する必要があります。状況に応じて3つの選択肢から選べます。</p>

        <h3>3.1.1 Option 1: Anaconda（初心者推奨）</h3>
        <p><strong>特徴：</strong></p>
        <ul>
            <li>科学計算ライブラリが最初から揃っている</li>
            <li>環境管理が簡単（GUI利用可能）</li>
            <li>Windows/Mac/Linux対応</li>
        </ul>

        <p><strong>インストール手順：</strong></p>
        <pre><code class="language-bash"># コード例1: Anaconda環境構築

# 1. Anacondaをダウンロード
# 公式サイト: https://www.anaconda.com/download
# Python 3.11以上を選択

# 2. インストール後、Anaconda Promptを起動

# 3. 仮想環境を作成（ML専用環境）
conda create -n ml_env python=3.11

# 4. 環境を有効化
conda activate ml_env

# 5. 必要なライブラリをインストール
conda install numpy pandas matplotlib seaborn scikit-learn jupyter

# 6. 動作確認
python --version
# 期待される出力: Python 3.11.x

# 7. Jupyter Notebookを起動
jupyter notebook</code></pre>

        <div class="success-box">
            <strong>成功の確認：</strong> コマンドプロンプトで <code>(ml_env)</code> というプレフィックスが表示されれば、環境が正しくアクティブ化されています。
        </div>

        <p><strong>Anacondaの利点と欠点：</strong></p>
        <table>
            <thead>
                <tr>
                    <th>利点</th>
                    <th>欠点</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>NumPy、SciPyなどが最初から含まれる</td>
                    <td>ファイルサイズが大きい（3GB以上）</td>
                </tr>
                <tr>
                    <td>依存関係の問題が少ない</td>
                    <td>インストールに時間がかかる</td>
                </tr>
                <tr>
                    <td>Anaconda Navigatorで視覚的に管理可能</td>
                    <td>ディスク容量を消費</td>
                </tr>
            </tbody>
        </table>

        <h3>3.1.2 Option 2: venv（Python標準）</h3>
        <p><strong>特徴：</strong></p>
        <ul>
            <li>Python標準ツール（追加インストール不要）</li>
            <li>軽量（必要なものだけインストール）</li>
            <li>プロジェクトごとに環境を分離</li>
        </ul>

        <pre><code class="language-bash"># コード例2: venv環境構築

# 1. Python 3.11以上がインストールされているか確認
python3 --version
# 期待される出力: Python 3.11.x 以上

# 2. 仮想環境を作成
python3 -m venv ml_env

# 3. 環境を有効化
# macOS/Linux:
source ml_env/bin/activate

# Windows (PowerShell):
# ml_env\Scripts\Activate.ps1

# Windows (Command Prompt):
# ml_env\Scripts\activate.bat

# 4. pipをアップグレード
pip install --upgrade pip

# 5. 必要なライブラリをインストール
pip install numpy pandas matplotlib seaborn scikit-learn jupyter

# 6. インストール確認
pip list
# numpy, pandas, scikit-learnなどが表示されればOK</code></pre>

        <div class="info-box">
            <strong>Tips:</strong> <code>requirements.txt</code>を作成しておくと、環境の再現が簡単になります。
            <pre><code># requirements.txt
numpy>=1.24.0
pandas>=2.0.0
matplotlib>=3.7.0
seaborn>=0.12.0
scikit-learn>=1.3.0
jupyter>=1.0.0</code></pre>
            インストール: <code>pip install -r requirements.txt</code>
        </div>

        <h3>3.1.3 Option 3: Google Colab（インストール不要）</h3>
        <p><strong>特徴：</strong></p>
        <ul>
            <li>ブラウザだけで実行可能</li>
            <li>インストール不要（クラウド実行）</li>
            <li>GPU/TPUが無料で使える</li>
        </ul>

        <pre><code class="language-python"># コード例3: Google Colabでの動作確認

# 1. Google Colabにアクセス: https://colab.research.google.com
# 2. 新しいノートブックを作成
# 3. 以下のコードを実行（必要なライブラリは自動でインストール済み）

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

print("ライブラリのインポートが成功しました！")
print(f"NumPy version: {np.__version__}")
print(f"Pandas version: {pd.__version__}")
print(f"scikit-learn version: {sklearn.__version__}")

# 期待される出力:
# ライブラリのインポートが成功しました！
# NumPy version: 1.24.3
# Pandas version: 2.0.3
# scikit-learn version: 1.3.0</code></pre>

        <h3>3.1.4 環境選択ガイド</h3>
        <table>
            <thead>
                <tr>
                    <th>状況</th>
                    <th>推奨オプション</th>
                    <th>理由</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>初めてのPython環境</td>
                    <td>Anaconda</td>
                    <td>環境構築が簡単、トラブルが少ない</td>
                </tr>
                <tr>
                    <td>既にPython環境がある</td>
                    <td>venv</td>
                    <td>軽量、プロジェクトごとに独立</td>
                </tr>
                <tr>
                    <td>今すぐ試したい</td>
                    <td>Google Colab</td>
                    <td>インストール不要、即座に開始可能</td>
                </tr>
                <tr>
                    <td>GPU計算が必要</td>
                    <td>Google Colab</td>
                    <td>無料でGPUアクセス可能</td>
                </tr>
            </tbody>
        </table>

        <h2>3.2 データの準備と可視化</h2>
        <p>機械学習の最初のステップは、データを正しく読み込み、理解することです。</p>

        <h3>3.2.1 データの読み込みと基本操作</h3>
        <pre><code class="language-python"># コード例4: Irisデータセットの読み込み

import pandas as pd
import numpy as np
from sklearn.datasets import load_iris

# Irisデータセット（アヤメの品種分類）を読み込み
iris = load_iris()

# DataFrameに変換
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['target'] = iris.target

# データの先頭5行を表示
print("データの先頭5行:")
print(df.head())

# 期待される出力:
#    sepal length (cm)  sepal width (cm)  ...  petal width (cm)  target
# 0                5.1               3.5  ...               0.2       0
# 1                4.9               3.0  ...               0.2       0
# 2                4.7               3.2  ...               0.2       0
# 3                4.6               3.1  ...               0.2       0
# 4                5.0               3.6  ...               0.2       0

# データの形状を確認
print(f"\nデータの形状: {df.shape}")
# 出力: データの形状: (150, 5)
# 150サンプル、5列（特徴量4つ + ターゲット1つ）</code></pre>

        <pre><code class="language-python"># コード例5: 基本統計量の確認

# 基本統計量を表示
print("基本統計量:")
print(df.describe())

# 期待される出力:
#        sepal length (cm)  sepal width (cm)  ...  petal width (cm)     target
# count         150.000000        150.000000  ...        150.000000  150.000000
# mean            5.843333          3.057333  ...          1.199333    1.000000
# std             0.828066          0.435866  ...          0.762238    0.819232
# min             4.300000          2.000000  ...          0.100000    0.000000
# 25%             5.100000          2.800000  ...          0.300000    0.000000
# 50%             5.800000          3.000000  ...          1.300000    1.000000
# 75%             6.400000          3.300000  ...          1.800000    2.000000
# max             7.900000          4.400000  ...          2.500000    2.000000

# データ型を確認
print("\nデータ型:")
print(df.dtypes)

# 欠損値を確認
print("\n欠損値の数:")
print(df.isnull().sum())
# 期待される出力: すべて0（Irisデータセットには欠損値がない）</code></pre>

        <h3>3.2.2 データの可視化</h3>
        <pre><code class="language-python"># コード例6: ヒストグラム（分布の可視化）

import matplotlib.pyplot as plt

# 各特徴量のヒストグラムを作成
df.hist(figsize=(12, 8), bins=20, edgecolor='black')
plt.suptitle('Iris Dataset - Feature Distributions', fontsize=16)
plt.tight_layout()
plt.show()

# 解釈:
# - sepal length: 5-6cmあたりにピーク
# - petal length: 二峰性（品種による違いが大きい）</code></pre>

        <pre><code class="language-python"># コード例7: 散布図マトリックス（特徴量間の関係）

import seaborn as sns

# 散布図マトリックス（pairplot）
sns.pairplot(df, hue='target', palette='Set1', markers=['o', 's', 'D'])
plt.suptitle('Iris Dataset - Pairplot by Species', y=1.02)
plt.show()

# 解釈:
# - petal length vs petal width: 品種がきれいに分離
# - sepal length vs sepal width: 一部重なりあり
# ⇒ petal系の特徴量が分類に有効</code></pre>

        <h2>3.3 回帰問題：住宅価格予測</h2>
        <p>回帰問題では、連続値（価格、温度など）を予測します。カリフォルニア住宅価格データで線形回帰を実装します。</p>

        <h3>3.3.1 データの準備</h3>
        <pre><code class="language-python"># コード例8: データ準備（カリフォルニア住宅価格）

from sklearn.datasets import fetch_california_housing

# カリフォルニア住宅価格データセットを読み込み
housing = fetch_california_housing()
X = housing.data  # 特徴量（8次元）
y = housing.target  # ターゲット（住宅価格、単位: $100,000）

# データの確認
print("特徴量の形状:", X.shape)  # (20640, 8)
print("ターゲットの形状:", y.shape)  # (20640,)
print("\n特徴量名:")
print(housing.feature_names)
# ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms',
#  'Population', 'AveOccup', 'Latitude', 'Longitude']

print("\n最初の3サンプル:")
print(X[:3])
print("対応する価格:", y[:3])
# 価格: [4.526 3.585 3.521] （単位: $100,000）</code></pre>

        <h3>3.3.2 データ分割</h3>
        <pre><code class="language-python"># コード例9: 訓練データとテストデータに分割

from sklearn.model_selection import train_test_split

# データを訓練80%、テスト20%に分割
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("訓練データ:", X_train.shape)  # (16512, 8)
print("テストデータ:", X_test.shape)  # (4128, 8)

# random_state=42: 再現性のため固定
# test_size=0.2: 一般的な分割比率</code></pre>

        <h3>3.3.3 特徴量のスケーリング</h3>
        <pre><code class="language-python"># コード例10: 特徴量のスケーリング（標準化）

from sklearn.preprocessing import StandardScaler

# StandardScaler: 平均0、標準偏差1に変換
scaler = StandardScaler()

# 訓練データでfit（平均・標準偏差を計算）
X_train_scaled = scaler.fit_transform(X_train)

# テストデータは訓練データの統計量でtransform
X_test_scaled = scaler.transform(X_test)

print("スケーリング前（最初の1サンプル）:")
print(X_train[0])
print("\nスケーリング後:")
print(X_train_scaled[0])

# 重要: テストデータでfitしない（データリークを防ぐ）</code></pre>

        <h3>3.3.4 線形回帰モデルの訓練</h3>
        <pre><code class="language-python"># コード例11: 線形回帰モデルの訓練

from sklearn.linear_model import LinearRegression

# モデルのインスタンス化
model = LinearRegression()

# 訓練データでモデルを学習
model.fit(X_train_scaled, y_train)

# 学習したパラメータを確認
print("切片（bias）:", model.intercept_)
print("\n係数（weights）:")
for feature, coef in zip(housing.feature_names, model.coef_):
    print(f"  {feature:12s}: {coef:7.4f}")

# 期待される出力例:
# MedInc      :  0.8296  （所得が高いほど価格が上がる）
# Latitude    : -0.8231  （緯度が高いほど価格が下がる）</code></pre>

        <h3>3.3.5 予測と評価</h3>
        <pre><code class="language-python"># コード例12: 予測の実行

# テストデータで予測
y_pred = model.predict(X_test_scaled)

# 最初の5件の予測を確認
print("実際の価格 vs 予測価格（最初の5件）:")
for i in range(5):
    print(f"実際: {y_test[i]:.3f}, 予測: {y_pred[i]:.3f}")

# 期待される出力例:
# 実際: 4.526, 予測: 4.321
# 実際: 3.585, 予測: 3.712
# 実際: 3.521, 予測: 3.498
# 実際: 3.413, 予測: 3.289
# 実際: 3.422, 予測: 3.501</code></pre>

        <pre><code class="language-python"># コード例13: モデルの評価

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# 平均二乗誤差（MSE）
mse = mean_squared_error(y_test, y_pred)

# 平均二乗平方根誤差（RMSE）
rmse = np.sqrt(mse)

# 平均絶対誤差（MAE）
mae = mean_absolute_error(y_test, y_pred)

# 決定係数（R²）
r2 = r2_score(y_test, y_pred)

print("モデル評価指標:")
print(f"  RMSE: {rmse:.3f}")  # 小さいほど良い
print(f"  MAE:  {mae:.3f}")   # 小さいほど良い
print(f"  R²:   {r2:.3f}")    # 1に近いほど良い（最大1.0）

# 期待される出力:
# RMSE: 0.729（約$72,900の誤差）
# MAE:  0.526（約$52,600の誤差）
# R²:   0.576（57.6%の分散を説明）</code></pre>

        <h3>3.3.6 予測結果の可視化</h3>
        <pre><code class="language-python"># コード例14: 予測vs実測のプロット

plt.figure(figsize=(10, 6))

# 散布図
plt.scatter(y_test, y_pred, alpha=0.5, edgecolor='black')

# 理想的な予測線（y=x）
plt.plot([y_test.min(), y_test.max()],
         [y_test.min(), y_test.max()],
         'r--', lw=2, label='Perfect Prediction')

plt.xlabel('Actual Price ($100,000)', fontsize=12)
plt.ylabel('Predicted Price ($100,000)', fontsize=12)
plt.title('Linear Regression: Predictions vs Actual', fontsize=14)
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# 解釈:
# - 点が赤線に近いほど予測精度が高い
# - 低価格帯（0-2）: 予測精度が高い
# - 高価格帯（4以上）: 予測が過小評価傾向</code></pre>

        <h2>3.4 分類問題：Iris品種分類</h2>
        <p>分類問題では、カテゴリ（品種、良/不良など）を予測します。複数のモデルを比較します。</p>

        <h3>3.4.1 ロジスティック回帰</h3>
        <pre><code class="language-python"># コード例15: ロジスティック回帰による分類

from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris

# データ準備
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.2, random_state=42
)

# ロジスティック回帰モデル
lr_model = LogisticRegression(max_iter=200, random_state=42)
lr_model.fit(X_train, y_train)

# 予測
lr_pred = lr_model.predict(X_test)

# 確率も取得可能
lr_proba = lr_model.predict_proba(X_test)

print("最初の3サンプルの予測:")
for i in range(3):
    print(f"実際: {y_test[i]}, 予測: {lr_pred[i]}, 確率: {lr_proba[i]}")

# 期待される出力例:
# 実際: 1, 予測: 1, 確率: [0.00 0.79 0.21]
# 実際: 0, 予測: 0, 確率: [0.97 0.03 0.00]
# 実際: 2, 予測: 2, 確率: [0.00 0.01 0.99]</code></pre>

        <h3>3.4.2 精度評価</h3>
        <pre><code class="language-python"># コード例16: 分類精度の評価

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 精度（Accuracy）
accuracy = accuracy_score(y_test, lr_pred)
print(f"Accuracy: {accuracy:.3f}")  # 期待: 1.000（100%）

# 詳細な分類レポート
print("\n分類レポート:")
print(classification_report(y_test, lr_pred, target_names=iris.target_names))

# 期待される出力:
#               precision    recall  f1-score   support
#     setosa       1.00      1.00      1.00        10
# versicolor       1.00      1.00      1.00         9
#  virginica       1.00      1.00      1.00        11
#   accuracy                           1.00        30

# precision: 正解と予測した中で実際に正解だった割合
# recall: 実際の正解の中で正しく予測できた割合
# f1-score: precisionとrecallの調和平均</code></pre>

        <h3>3.4.3 混同行列の可視化</h3>
        <pre><code class="language-python"># コード例17: 混同行列（Confusion Matrix）

import seaborn as sns

# 混同行列を計算
cm = confusion_matrix(y_test, lr_pred)

# ヒートマップで可視化
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=iris.target_names,
            yticklabels=iris.target_names)
plt.title('Confusion Matrix - Logistic Regression', fontsize=14)
plt.ylabel('True Label', fontsize=12)
plt.xlabel('Predicted Label', fontsize=12)
plt.tight_layout()
plt.show()

# 解釈:
# - 対角線: 正しく分類された数
# - 対角線以外: 誤分類
# - Irisデータは単純なので誤分類がほぼゼロ</code></pre>

        <h3>3.4.4 決定木</h3>
        <pre><code class="language-python"># コード例18: 決定木による分類

from sklearn.tree import DecisionTreeClassifier

# 決定木モデル（深さ3に制限）
dt_model = DecisionTreeClassifier(max_depth=3, random_state=42)
dt_model.fit(X_train, y_train)

# 予測
dt_pred = dt_model.predict(X_test)

# 精度
dt_accuracy = accuracy_score(y_test, dt_pred)
print(f"Decision Tree Accuracy: {dt_accuracy:.3f}")
# 期待: 1.000

# 決定木の利点: 解釈性が高い
# 特徴量の重要度を確認
print("\n特徴量の重要度:")
for feature, importance in zip(iris.feature_names, dt_model.feature_importances_):
    print(f"  {feature:20s}: {importance:.3f}")</code></pre>

        <h3>3.4.5 ランダムフォレスト</h3>
        <pre><code class="language-python"># コード例19: ランダムフォレスト（アンサンブル学習）

from sklearn.ensemble import RandomForestClassifier

# ランダムフォレスト（100本の決定木）
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# 予測
rf_pred = rf_model.predict(X_test)

# 精度
rf_accuracy = accuracy_score(y_test, rf_pred)
print(f"Random Forest Accuracy: {rf_accuracy:.3f}")
# 期待: 1.000

# ランダムフォレストの利点:
# - 決定木より過学習しにくい
# - 高い予測精度
# - 特徴量の重要度が安定</code></pre>

        <h3>3.4.6 特徴量の重要度可視化</h3>
        <pre><code class="language-python"># コード例20: 特徴量の重要度（Feature Importance）

# ランダムフォレストの特徴量重要度
importances = rf_model.feature_importances_
feature_names = iris.feature_names

# 降順にソート
indices = np.argsort(importances)[::-1]

# 棒グラフで可視化
plt.figure(figsize=(10, 6))
plt.barh(range(len(importances)), importances[indices], color='skyblue', edgecolor='black')
plt.yticks(range(len(importances)), [feature_names[i] for i in indices])
plt.xlabel('Importance', fontsize=12)
plt.title('Feature Importance (Random Forest)', fontsize=14)
plt.tight_layout()
plt.show()

# 解釈:
# - petal width (cm): 最も重要（0.45）
# - petal length (cm): 2番目に重要（0.42）
# ⇒ 花びらの特徴が品種分類に最も有効</code></pre>

        <h3>3.4.7 サポートベクターマシン（SVM）</h3>
        <pre><code class="language-python"># コード例21: SVM（Support Vector Machine）

from sklearn.svm import SVC

# SVMモデル（RBFカーネル）
svm_model = SVC(kernel='rbf', random_state=42)
svm_model.fit(X_train, y_train)

# 予測
svm_pred = svm_model.predict(X_test)

# 精度
svm_accuracy = accuracy_score(y_test, svm_pred)
print(f"SVM Accuracy: {svm_accuracy:.3f}")
# 期待: 1.000

# SVMの利点:
# - 高次元データに強い
# - カーネルトリックで非線形分離可能
# 欠点:
# - 大規模データでは遅い
# - 確率予測が標準でない</code></pre>

        <h2>3.5 モデル比較と選択</h2>
        <p>複数のモデルを公平に比較し、最適なモデルを選択します。</p>

        <h3>3.5.1 クロスバリデーション</h3>
        <pre><code class="language-python"># コード例22: クロスバリデーションによるモデル比較

from sklearn.model_selection import cross_val_score

# 比較するモデル
models = {
    'Logistic Regression': LogisticRegression(max_iter=200),
    'Decision Tree': DecisionTreeClassifier(max_depth=3),
    'Random Forest': RandomForestClassifier(n_estimators=100),
    'SVM': SVC(kernel='rbf')
}

print("5-Fold Cross-Validation Results:")
print("-" * 50)

results = []
for name, model in models.items():
    # 5分割クロスバリデーション
    scores = cross_val_score(model, X_train, y_train, cv=5)

    results.append({
        'Model': name,
        'Mean': scores.mean(),
        'Std': scores.std()
    })

    print(f"{name:20s}: {scores.mean():.3f} (+/- {scores.std():.3f})")

# 期待される出力:
# Logistic Regression : 0.967 (+/- 0.033)
# Decision Tree       : 0.958 (+/- 0.042)
# Random Forest       : 0.967 (+/- 0.025)
# SVM                 : 0.975 (+/- 0.025)</code></pre>

        <h3>3.5.2 学習曲線</h3>
        <pre><code class="language-python"># コード例23: 学習曲線（Learning Curve）

from sklearn.model_selection import learning_curve

# ランダムフォレストの学習曲線
train_sizes, train_scores, val_scores = learning_curve(
    RandomForestClassifier(n_estimators=100, random_state=42),
    X_train, y_train, cv=5, n_jobs=-1,
    train_sizes=np.linspace(0.1, 1.0, 10)
)

# 平均と標準偏差を計算
train_mean = train_scores.mean(axis=1)
train_std = train_scores.std(axis=1)
val_mean = val_scores.mean(axis=1)
val_std = val_scores.std(axis=1)

# プロット
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, label='Training score', color='blue', marker='o')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std,
                 alpha=0.15, color='blue')
plt.plot(train_sizes, val_mean, label='Validation score', color='red', marker='s')
plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std,
                 alpha=0.15, color='red')

plt.xlabel('Training Size', fontsize=12)
plt.ylabel('Score', fontsize=12)
plt.title('Learning Curve - Random Forest', fontsize=14)
plt.legend(loc='best')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# 解釈:
# - 訓練スコアと検証スコアが近い ⇒ 良好な汎化性能
# - 検証スコアが収束 ⇒ データを追加しても改善は限定的</code></pre>

        <h3>3.5.3 モデル性能比較表</h3>
        <pre><code class="language-python"># コード例24: モデル性能の包括的比較

import time

# 比較結果を格納
results = []

for name, model in models.items():
    # 訓練時間を計測
    start_time = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start_time

    # 予測時間を計測
    start_time = time.time()
    pred = model.predict(X_test)
    predict_time = time.time() - start_time

    # 精度
    acc = accuracy_score(y_test, pred)

    results.append({
        'Model': name,
        'Accuracy': acc,
        'Train Time (s)': train_time,
        'Predict Time (s)': predict_time
    })

# DataFrameに変換して表示
results_df = pd.DataFrame(results)
results_df = results_df.sort_values('Accuracy', ascending=False)

print("\nモデル性能比較:")
print(results_df.to_string(index=False))

# 期待される出力例:
#                 Model  Accuracy  Train Time (s)  Predict Time (s)
#                   SVM     1.000           0.002             0.001
#     Random Forest     1.000           0.158             0.012
# Logistic Regression     1.000           0.005             0.001
#         Decision Tree     1.000           0.002             0.001</code></pre>

        <h2>3.6 ハイパーパラメータチューニング</h2>
        <p>モデルの性能を最大化するため、ハイパーパラメータ（学習前に設定するパラメータ）を調整します。</p>

        <h3>3.6.1 Grid Search</h3>
        <pre><code class="language-python"># コード例25: Grid Search（全探索）

from sklearn.model_selection import GridSearchCV

# ランダムフォレストのハイパーパラメータ候補
param_grid = {
    'n_estimators': [50, 100, 200],          # 木の数
    'max_depth': [3, 5, 10, None],           # 木の深さ
    'min_samples_split': [2, 5, 10]          # 分割に必要な最小サンプル数
}

# Grid Search（5分割クロスバリデーション）
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid, cv=5, n_jobs=-1, verbose=1
)

# 実行（3×4×3=36通りの組み合わせを試す）
grid_search.fit(X_train, y_train)

# 最良のパラメータ
print("最良のパラメータ:")
print(grid_search.best_params_)
# 期待: {'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100}

# 最良のスコア
print(f"\n最良のCV Score: {grid_search.best_score_:.3f}")
# 期待: 0.967</code></pre>

        <h3>3.6.2 最良モデルの評価</h3>
        <pre><code class="language-python"># コード例26: チューニング後のモデル評価

# 最良のモデルを取得
best_model = grid_search.best_estimator_

# テストデータで評価
best_pred = best_model.predict(X_test)
best_accuracy = accuracy_score(y_test, best_pred)

print(f"Test Accuracy (tuned model): {best_accuracy:.3f}")
# 期待: 1.000

# チューニング前との比較
print(f"Test Accuracy (default):     {rf_accuracy:.3f}")
# 期待: 1.000

# Irisデータは単純なので差が出にくいが、
# 複雑なデータセットではチューニングで5-10%改善することも</code></pre>

        <h3>3.6.3 Random Search</h3>
        <pre><code class="language-python"># コード例27: Random Search（効率的サンプリング）

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

# パラメータの分布を定義
param_dist = {
    'n_estimators': randint(50, 300),        # 50-300の範囲でランダム
    'max_depth': randint(3, 20),             # 3-20の範囲でランダム
    'min_samples_split': randint(2, 20)      # 2-20の範囲でランダム
}

# Random Search（20回のランダムサンプリング）
random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_dist, n_iter=20, cv=5, random_state=42, n_jobs=-1
)

random_search.fit(X_train, y_train)

print("Random Search 最良のパラメータ:")
print(random_search.best_params_)

print(f"\n最良のCV Score: {random_search.best_score_:.3f}")

# Random Searchの利点:
# - Grid Searchより高速（20回 vs 36回）
# - 広い探索空間をカバー可能
# - 連続値パラメータにも対応</code></pre>

        <h3>3.6.4 ハイパーパラメータ効果の可視化</h3>
        <pre><code class="language-python"># コード例28: ハイパーパラメータ効果のヒートマップ

# Grid Searchの結果をDataFrameに変換
results_df = pd.DataFrame(grid_search.cv_results_)

# n_estimatorsとmax_depthの効果を可視化
pivot = results_df.pivot_table(
    values='mean_test_score',
    index='param_max_depth',
    columns='param_n_estimators'
)

# ヒートマップ
plt.figure(figsize=(10, 6))
sns.heatmap(pivot, annot=True, fmt='.3f', cmap='YlGnBu', cbar_kws={'label': 'CV Score'})
plt.title('Grid Search Results: max_depth vs n_estimators', fontsize=14)
plt.xlabel('n_estimators', fontsize=12)
plt.ylabel('max_depth', fontsize=12)
plt.tight_layout()
plt.show()

# 解釈:
# - max_depth=5, n_estimators=100: 最高スコア
# - max_depth=None（制限なし）: 過学習のリスク</code></pre>

        <h2>3.7 特徴量エンジニアリング</h2>
        <p>生の特徴量を変換・拡張することで、モデルの性能を向上させます。</p>

        <h3>3.7.1 多項式特徴量</h3>
        <pre><code class="language-python"># コード例29: 多項式特徴量（Polynomial Features）

from sklearn.preprocessing import PolynomialFeatures

# サンプルデータ
X_simple = np.array([[1, 2], [3, 4], [5, 6]])
print("元の特徴量:")
print(X_simple)
print("形状:", X_simple.shape)  # (3, 2)

# 2次の多項式特徴量を生成
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X_simple)

print("\n多項式特徴量:")
print(X_poly)
print("形状:", X_poly.shape)  # (3, 5)

# 特徴量名を確認
print("\n特徴量名:")
print(poly.get_feature_names_out(['x1', 'x2']))
# ['x1', 'x2', 'x1^2', 'x1*x2', 'x2^2']

# 解釈:
# - 元: [1, 2] → 拡張: [1, 2, 1, 2, 4]
# - x1^2, x1*x2, x2^2 などの相互作用項を追加</code></pre>

        <h3>3.7.2 特徴量選択</h3>
        <pre><code class="language-python"># コード例30: 特徴量選択（Feature Selection）

from sklearn.feature_selection import SelectKBest, f_classif

# Irisデータで最も重要な2つの特徴量を選択
selector = SelectKBest(f_classif, k=2)
X_selected = selector.fit_transform(iris.data, iris.target)

# 選択された特徴量を確認
selected_features = np.array(iris.feature_names)[selector.get_support()]
print("選択された特徴量:")
print(selected_features)
# 期待: ['petal length (cm)', 'petal width (cm)']

# F値（ANOVA）スコアを確認
print("\n各特徴量のF値:")
for feature, score in zip(iris.feature_names, selector.scores_):
    print(f"  {feature:20s}: {score:7.2f}")

# 解釈:
# - petal length: 1179.03（最も重要）
# - petal width: 960.01（2番目に重要）
# - sepal系は相対的に重要度が低い</code></pre>

        <h3>3.7.3 主成分分析（PCA）</h3>
        <pre><code class="language-python"># コード例31: PCA（次元削減）

from sklearn.decomposition import PCA

# Irisデータ（4次元）を2次元に削減
pca = PCA(n_components=2)
X_pca = pca.fit_transform(iris.data)

print("元の次元:", iris.data.shape)  # (150, 4)
print("PCA後の次元:", X_pca.shape)   # (150, 2)

# 説明された分散の割合
print("\n各主成分の寄与率:")
print(pca.explained_variance_ratio_)
# 期待: [0.92, 0.05]（第1主成分で92%を説明）

# 累積寄与率
print(f"累積寄与率: {pca.explained_variance_ratio_.sum():.3f}")
# 期待: 0.977（97.7%の情報を保持）

# 可視化
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=iris.target,
                      cmap='viridis', edgecolor='black', s=50)
plt.xlabel('First Principal Component', fontsize=12)
plt.ylabel('Second Principal Component', fontsize=12)
plt.title('PCA of Iris Dataset', fontsize=14)
plt.colorbar(scatter, label='Species')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# 解釈:
# - 4次元を2次元に削減しても97.7%の情報を保持
# - 品種がきれいに分離される</code></pre>

        <h3>3.7.4 モデルベースの特徴量重要度</h3>
        <pre><code class="language-python"># コード例32: モデルベースの特徴量重要度分析

# ランダムフォレストで特徴量重要度を計算
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(iris.data, iris.target)

# 特徴量重要度をDataFrameに整理
feature_importance_df = pd.DataFrame({
    'feature': iris.feature_names,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

print("特徴量重要度ランキング:")
print(feature_importance_df)

# 期待される出力:
#               feature  importance
# 2  petal length (cm)       0.445
# 3   petal width (cm)       0.425
# 0  sepal length (cm)       0.089
# 1   sepal width (cm)       0.041

# 可視化
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['feature'], feature_importance_df['importance'],
         color='coral', edgecolor='black')
plt.xlabel('Importance', fontsize=12)
plt.title('Feature Importance (Random Forest)', fontsize=14)
plt.tight_layout()
plt.show()</code></pre>

        <h2>3.8 トラブルシューティング</h2>
        <p>機械学習の実装でよくあるエラーと解決策をまとめます。</p>

        <h3>3.8.1 よくあるエラーと解決策</h3>
        <table>
            <thead>
                <tr>
                    <th>エラー</th>
                    <th>原因</th>
                    <th>解決策</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><code>ModuleNotFoundError</code></td>
                    <td>ライブラリ未インストール</td>
                    <td><code>pip install [library_name]</code></td>
                </tr>
                <tr>
                    <td><code>ValueError: shape mismatch</code></td>
                    <td>データの次元が合わない</td>
                    <td><code>X.shape</code>と<code>y.shape</code>を確認</td>
                </tr>
                <tr>
                    <td><code>ConvergenceWarning</code></td>
                    <td>最適化が収束しない</td>
                    <td><code>max_iter</code>を増やす、またはスケーリング</td>
                </tr>
                <tr>
                    <td>低精度（<0.5）</td>
                    <td>データ品質、モデル選択</td>
                    <td>データ確認、特徴量追加、モデル変更</td>
                </tr>
                <tr>
                    <td>過学習（train>test）</td>
                    <td>モデルが複雑すぎる</td>
                    <td>正則化、データ追加、CV使用</td>
                </tr>
                <tr>
                    <td><code>MemoryError</code></td>
                    <td>データサイズ過大</td>
                    <td>バッチ処理、データサンプリング</td>
                </tr>
                <tr>
                    <td>訓練が遅い</td>
                    <td>データ量、モデル複雑度</td>
                    <td><code>n_jobs=-1</code>、GPU使用、モデル簡略化</td>
                </tr>
            </tbody>
        </table>

        <h3>3.8.2 デバッグチェックリスト</h3>
        <div class="info-box">
            <strong>5ステップデバッグ手順：</strong>
            <ol>
                <li><strong>データ確認</strong>: <code>df.head()</code>, <code>df.info()</code>, <code>df.describe()</code></li>
                <li><strong>欠損値チェック</strong>: <code>df.isnull().sum()</code></li>
                <li><strong>データ型確認</strong>: <code>df.dtypes</code>（数値型になっているか）</li>
                <li><strong>形状確認</strong>: <code>X.shape</code>, <code>y.shape</code>（次元が一致するか）</li>
                <li><strong>簡単なモデルで試す</strong>: まず<code>LogisticRegression</code>で動作確認</li>
            </ol>
        </div>

        <h2>3.9 プロジェクトチャレンジ：Titanic生存予測</h2>
        <p>ここまで学んだ技術を使って、実際のデータサイエンスプロジェクトに挑戦します。</p>

        <h3>3.9.1 プロジェクト概要</h3>
        <div class="info-box">
            <strong>プロジェクト目標：</strong>
            <ul>
                <li>データセット: Titanic乗客データ（Kaggle）</li>
                <li>タスク: 生存（Survived: 0/1）を予測</li>
                <li>目標精度: 80%以上</li>
                <li>使用技術: データ前処理、特徴量エンジニアリング、モデル比較</li>
            </ul>
        </div>

        <h3>3.9.2 データ読み込みとEDA</h3>
        <pre><code class="language-python"># コード例33: Titanicデータの探索的データ分析（EDA）

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# データ読み込み（実際にはKaggleからダウンロード）
# ここではサンプルデータを作成
# 実際のプロジェクトでは以下のURLからダウンロード:
# https://www.kaggle.com/c/titanic/data

# サンプルデータ作成（デモ用）
np.random.seed(42)
df = pd.DataFrame({
    'PassengerId': range(1, 892),
    'Survived': np.random.choice([0, 1], 891),
    'Pclass': np.random.choice([1, 2, 3], 891),
    'Sex': np.random.choice(['male', 'female'], 891),
    'Age': np.random.normal(30, 15, 891).clip(0, 80),
    'SibSp': np.random.choice([0, 1, 2, 3], 891),
    'Parch': np.random.choice([0, 1, 2], 891),
    'Fare': np.random.exponential(30, 891),
    'Embarked': np.random.choice(['S', 'C', 'Q'], 891)
})

# 欠損値をランダムに追加
df.loc[np.random.choice(df.index, 177, replace=False), 'Age'] = np.nan
df.loc[np.random.choice(df.index, 2, replace=False), 'Embarked'] = np.nan

print("データの先頭5行:")
print(df.head())

print("\nデータの基本情報:")
print(df.info())

print("\n基本統計量:")
print(df.describe())

print("\n欠損値の確認:")
print(df.isnull().sum())

# 期待される出力:
# Age: 177件の欠損値
# Embarked: 2件の欠損値</code></pre>

        <h3>3.9.3 データ前処理</h3>
        <pre><code class="language-python"># コード例34: データ前処理と特徴量エンジニアリング

from sklearn.preprocessing import LabelEncoder

# データのコピーを作成
df_processed = df.copy()

# 1. 欠損値の処理
# Age: 中央値で補完
df_processed['Age'].fillna(df_processed['Age'].median(), inplace=True)

# Embarked: 最頻値で補完
df_processed['Embarked'].fillna(df_processed['Embarked'].mode()[0], inplace=True)

# 2. 特徴量エンジニアリング
# 家族サイズ = SibSp（兄弟・配偶者） + Parch（親・子供） + 1（本人）
df_processed['FamilySize'] = df_processed['SibSp'] + df_processed['Parch'] + 1

# 一人旅かどうか
df_processed['IsAlone'] = (df_processed['FamilySize'] == 1).astype(int)

# 年齢層（カテゴリ化）
df_processed['AgeGroup'] = pd.cut(df_processed['Age'],
                                   bins=[0, 12, 18, 60, 100],
                                   labels=['Child', 'Teen', 'Adult', 'Senior'])

# 3. カテゴリ変数のエンコーディング
# Sex: LabelEncoder（male=1, female=0）
le = LabelEncoder()
df_processed['Sex'] = le.fit_transform(df_processed['Sex'])

# Embarked: One-Hot Encoding
df_processed = pd.get_dummies(df_processed, columns=['Embarked'], prefix='Embarked')

# AgeGroup: One-Hot Encoding
df_processed = pd.get_dummies(df_processed, columns=['AgeGroup'], prefix='Age')

# 4. 不要な列を削除
df_processed.drop(['PassengerId', 'SibSp', 'Parch'], axis=1, inplace=True)

print("前処理後のデータ:")
print(df_processed.head())
print("\n形状:", df_processed.shape)
print("\n欠損値:", df_processed.isnull().sum().sum())  # 期待: 0</code></pre>

        <h3>3.9.4 モデル訓練と評価</h3>
        <pre><code class="language-python"># コード例35: 複数モデルの訓練と比較

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 特徴量とターゲットを分離
X = df_processed.drop('Survived', axis=1)
y = df_processed['Survived']

# データ分割（訓練80%, テスト20%）
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 複数モデルを定義
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(kernel='rbf', random_state=42)
}

print("モデル比較結果:")
print("-" * 70)

results = []
for name, model in models.items():
    # 訓練
    model.fit(X_train, y_train)

    # 予測
    y_pred = model.predict(X_test)

    # 評価
    accuracy = accuracy_score(y_test, y_pred)

    # クロスバリデーション
    cv_scores = cross_val_score(model, X_train, y_train, cv=5)

    results.append({
        'Model': name,
        'Test Accuracy': accuracy,
        'CV Mean': cv_scores.mean(),
        'CV Std': cv_scores.std()
    })

    print(f"{name:20s}: Test={accuracy:.3f}, CV={cv_scores.mean():.3f} (+/-{cv_scores.std():.3f})")

# 結果をDataFrameに変換
results_df = pd.DataFrame(results).sort_values('Test Accuracy', ascending=False)

print("\n最終結果（精度順）:")
print(results_df.to_string(index=False))

# 最良モデルの詳細レポート
best_model_name = results_df.iloc[0]['Model']
best_model = models[best_model_name]
best_pred = best_model.predict(X_test)

print(f"\n{best_model_name} の詳細:")
print(classification_report(y_test, best_pred))

# 混同行列
cm = confusion_matrix(y_test, best_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title(f'Confusion Matrix - {best_model_name}')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.show()

# プロジェクト成功判定
if results_df.iloc[0]['Test Accuracy'] >= 0.80:
    print("\n🎉 プロジェクト成功！目標精度80%を達成しました！")
else:
    print(f"\n目標精度80%まであと{0.80 - results_df.iloc[0]['Test Accuracy']:.1%}です。")
    print("改善のヒント: 特徴量追加、ハイパーパラメータチューニング、アンサンブル学習")</code></pre>

        <h3>3.9.5 プロジェクト拡張アイデア</h3>
        <div class="info-box">
            <strong>さらなる挑戦：</strong>
            <ol>
                <li><strong>特徴量の追加</strong>: 名前からタイトル（Mr., Mrs.等）を抽出</li>
                <li><strong>ハイパーパラメータチューニング</strong>: GridSearchCVで最適化</li>
                <li><strong>アンサンブル学習</strong>: VotingClassifierで複数モデルを組み合わせ</li>
                <li><strong>特徴量重要度分析</strong>: どの特徴量が生存に影響したか分析</li>
                <li><strong>Kaggleサブミット</strong>: 実際のKaggleコンペに提出</li>
                <li><strong>深層学習</strong>: ニューラルネットワーク（Keras/PyTorch）で実装</li>
            </ol>
        </div>

        <h2>本章のまとめ</h2>
        <p>この章では、Pythonを使った機械学習の実践的な流れを学びました。</p>

        <h3>習得したスキル</h3>
        <ul>
            <li>✅ Python環境の構築（Anaconda/venv/Colab）</li>
            <li>✅ データの読み込み、前処理、可視化</li>
            <li>✅ 回帰問題の実装（線形回帰、住宅価格予測）</li>
            <li>✅ 分類問題の実装（Iris分類、複数モデル比較）</li>
            <li>✅ モデル評価指標（RMSE, MAE, R², Accuracy, F1-score）</li>
            <li>✅ クロスバリデーション（5-fold CV）</li>
            <li>✅ ハイパーパラメータチューニング（Grid Search, Random Search）</li>
            <li>✅ 特徴量エンジニアリング（多項式、選択、PCA）</li>
            <li>✅ トラブルシューティング（エラー対処、デバッグ手順）</li>
            <li>✅ 実プロジェクト（Titanic生存予測）</li>
        </ul>

        <h3>重要なポイント</h3>
        <div class="success-box">
            <strong>機械学習プロジェクトの5ステップ：</strong>
            <ol>
                <li><strong>データ理解</strong>: EDA、可視化、統計量確認</li>
                <li><strong>前処理</strong>: 欠損値処理、スケーリング、エンコーディング</li>
                <li><strong>モデル選択</strong>: 複数モデルを試して比較</li>
                <li><strong>評価・改善</strong>: CV、チューニング、特徴量エンジニアリング</li>
                <li><strong>デプロイ準備</strong>: 最良モデルの保存、ドキュメント作成</li>
            </ol>
        </div>

        <h3>次のステップ</h3>
        <p>第4章では、機械学習の実世界への応用を学びます：</p>
        <ul>
            <li>5つの詳細ケーススタディ（Netflix、Google翻訳、Tesla等）</li>
            <li>将来トレンド（基盤モデル、AutoML、エッジAI）</li>
            <li>キャリアパス（データサイエンティスト、MLエンジニア、研究者）</li>
            <li>学習リソースとコミュニティ</li>
        </ul>

        <h2>演習問題</h2>

        <details>
            <summary><strong>問題1（難易度：Easy）</strong> - データ読み込みと基本統計</summary>
            <p><strong>問題：</strong> Irisデータセットを読み込み、各特徴量の平均値、中央値、標準偏差を計算してください。</p>

            <p><strong>ヒント：</strong> <code>df.describe()</code>を使うと一度に確認できます。</p>

            <details>
                <summary>解答例</summary>
                <pre><code class="language-python">from sklearn.datasets import load_iris
import pandas as pd

# データ読み込み
iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)

# 基本統計量
print(df.describe())

# 個別に計算する場合
print("\n平均値:")
print(df.mean())

print("\n中央値:")
print(df.median())

print("\n標準偏差:")
print(df.std())</code></pre>
            </details>
        </details>

        <details>
            <summary><strong>問題2（難易度：Easy）</strong> - 訓練データとテストデータの分割</summary>
            <p><strong>問題：</strong> カリフォルニア住宅データを訓練70%、テスト30%に分割してください。分割後のデータ数を確認してください。</p>

            <p><strong>ヒント：</strong> <code>train_test_split</code>の<code>test_size</code>パラメータを変更します。</p>

            <details>
                <summary>解答例</summary>
                <pre><code class="language-python">from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split

# データ読み込み
housing = fetch_california_housing()
X, y = housing.data, housing.target

# 訓練70%, テスト30%に分割
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

print("元のデータ数:", len(X))           # 20640
print("訓練データ数:", len(X_train))     # 14448
print("テストデータ数:", len(X_test))   # 6192
print("分割比率:", len(X_train)/len(X))  # 0.70</code></pre>
            </details>
        </details>

        <details>
            <summary><strong>問題3（難易度：Medium）</strong> - モデルの精度比較</summary>
            <p><strong>問題：</strong> Irisデータセットで、ロジスティック回帰、決定木、ランダムフォレストの3つのモデルを訓練し、テスト精度を比較してください。最も精度の高いモデルを特定してください。</p>

            <p><strong>ヒント：</strong> <code>accuracy_score</code>を使って各モデルの精度を計算します。</p>

            <details>
                <summary>解答例</summary>
                <pre><code class="language-python">from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# データ準備
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.2, random_state=42
)

# モデル定義
models = {
    'Logistic Regression': LogisticRegression(max_iter=200),
    'Decision Tree': DecisionTreeClassifier(max_depth=3),
    'Random Forest': RandomForestClassifier(n_estimators=100)
}

# 訓練と評価
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, pred)
    results[name] = accuracy
    print(f"{name}: {accuracy:.3f}")

# 最良モデル
best_model = max(results, key=results.get)
print(f"\n最良モデル: {best_model} ({results[best_model]:.3f})")</code></pre>
            </details>
        </details>

        <details>
            <summary><strong>問題4（難易度：Medium）</strong> - ハイパーパラメータチューニング</summary>
            <p><strong>問題：</strong> ランダムフォレストで<code>n_estimators=[50, 100, 150]</code>と<code>max_depth=[3, 5, 7]</code>の組み合わせでGrid Searchを実行し、最良のパラメータを見つけてください。</p>

            <p><strong>ヒント：</strong> <code>GridSearchCV</code>を使います。</p>

            <details>
                <summary>解答例</summary>
                <pre><code class="language-python">from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# データ準備
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.2, random_state=42
)

# パラメータグリッド
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [3, 5, 7]
}

# Grid Search
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid, cv=5, n_jobs=-1
)

grid_search.fit(X_train, y_train)

# 結果
print("最良のパラメータ:", grid_search.best_params_)
print("最良のCV Score:", grid_search.best_score_)

# テストデータで評価
test_accuracy = grid_search.best_estimator_.score(X_test, y_test)
print("テスト精度:", test_accuracy)</code></pre>
            </details>
        </details>

        <details>
            <summary><strong>問題5（難易度：Hard）</strong> - 完全なMLパイプライン</summary>
            <p><strong>問題：</strong> カリフォルニア住宅データで、以下のステップを含む完全なMLパイプラインを構築してください：</p>
            <ol>
                <li>データの80/20分割</li>
                <li>StandardScalerでスケーリング</li>
                <li>ランダムフォレスト回帰で訓練</li>
                <li>RMSE、MAE、R²を計算</li>
                <li>予測vs実測のプロット作成</li>
            </ol>

            <p><strong>ヒント：</strong> これまで学んだコード例を組み合わせます。</p>

            <details>
                <summary>解答例</summary>
                <pre><code class="language-python">from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np
import matplotlib.pyplot as plt

# 1. データ読み込み
housing = fetch_california_housing()
X, y = housing.data, housing.target

# 2. データ分割
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 3. スケーリング
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. モデル訓練
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

# 5. 予測
y_pred = model.predict(X_test_scaled)

# 6. 評価
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("評価結果:")
print(f"  RMSE: {rmse:.3f}")
print(f"  MAE:  {mae:.3f}")
print(f"  R²:   {r2:.3f}")

# 7. 可視化
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5, edgecolor='black')
plt.plot([y_test.min(), y_test.max()],
         [y_test.min(), y_test.max()],
         'r--', lw=2, label='Perfect Prediction')
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Random Forest: Predictions vs Actual')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()</code></pre>
            </details>
        </details>

        <h2>参考文献</h2>
        <ol>
            <li>Pedregosa, F., et al. (2011). "Scikit-learn: Machine Learning in Python." <em>Journal of Machine Learning Research</em>, 12, 2825-2830.</li>
            <li>VanderPlas, J. (2016). <em>Python Data Science Handbook</em>. O'Reilly Media.</li>
            <li>Géron, A. (2019). <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em> (2nd ed.). O'Reilly Media.</li>
            <li>scikit-learn Documentation. (2024). "User Guide." URL: <a href="https://scikit-learn.org/stable/user_guide.html">https://scikit-learn.org/stable/user_guide.html</a></li>
            <li>Kaggle. (2024). "Titanic: Machine Learning from Disaster." URL: <a href="https://www.kaggle.com/c/titanic">https://www.kaggle.com/c/titanic</a></li>
        </ol>

        <div class="navigation">
            <a href="chapter2-fundamentals.html" class="nav-button">← 第2章：基礎知識</a>
            <a href="index.html" class="nav-button">シリーズ目次</a>
            <a href="chapter4-real-world.html" class="nav-button">第4章：実世界への応用 →</a>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 ML Knowledge Hub - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
