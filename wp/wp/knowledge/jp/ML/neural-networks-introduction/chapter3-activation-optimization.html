<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第3章：活性化関数と最適化 - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>第3章：活性化関数と最適化</h1>
            <p class="subtitle">深層学習の性能を決定する重要要素</p>
            <div class="meta">
                <span class="meta-item">📖 読了時間: 25-30分</span>
                <span class="meta-item">📊 難易度: 中級</span>
                <span class="meta-item">💻 コード例: 12個</span>
                <span class="meta-item">📝 演習問題: 5問</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>学習目標</h2>
<p>この章を読むことで、以下を習得できます：</p>
<ul>
<li>✅ 様々な活性化関数（Sigmoid、ReLU、Leaky ReLU、ELU、Swish）の特徴を理解する</li>
<li>✅ 勾配消失問題（Vanishing Gradient Problem）とその対策を説明できる</li>
<li>✅ 高度な最適化アルゴリズム（Momentum、AdaGrad、RMSprop、Adam）を実装できる</li>
<li>✅ 学習率スケジューリングの重要性を理解する</li>
<li>✅ 重みの初期化戦略（Xavier、He初期化）を適用できる</li>
</ul>

<hr>

<h2>3.1 活性化関数（Activation Functions）</h2>

<h3>活性化関数の役割</h3>

<p><strong>活性化関数</strong>は、ニューラルネットワークに<strong>非線形性</strong>を導入します。活性化関数がなければ、何層重ねても線形変換にしかならず、複雑なパターンを学習できません。</p>

<blockquote>
<p>「活性化関数は、ニューラルネットワークの"表現力"を決定する重要な要素です。」</p>
</blockquote>

<div class="mermaid">
graph LR
    A[線形変換<br/>z = Wx + b] --> B[活性化関数<br/>a = f(z)]
    B --> C[非線形出力]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#e8f5e9
</div>

<hr>

<h3>3.1.1 Sigmoid関数</h3>

<p><strong>数式</strong>：</p>
<p>$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$</p>

<p><strong>導関数</strong>：</p>
<p>$$
\sigma'(x) = \sigma(x)(1 - \sigma(x))
$$</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    """Sigmoid関数"""
    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))

def sigmoid_derivative(x):
    """Sigmoidの導関数"""
    s = sigmoid(x)
    return s * (1 - s)

# 可視化
x = np.linspace(-10, 10, 200)
y = sigmoid(x)
dy = sigmoid_derivative(x)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(x, y, linewidth=2, label='σ(x)')
plt.xlabel('x', fontsize=12)
plt.ylabel('σ(x)', fontsize=12)
plt.title('Sigmoid関数', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(x, dy, linewidth=2, color='red', label="σ'(x)")
plt.xlabel('x', fontsize=12)
plt.ylabel("σ'(x)", fontsize=12)
plt.title('Sigmoidの導関数', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)
plt.legend()

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>特徴</strong>：</p>
<ul>
<li>✅ 出力範囲: (0, 1)</li>
<li>✅ 滑らかで微分可能</li>
<li>❌ <strong>勾配消失問題</strong>: $|x|$が大きいとき、導関数が0に近づく</li>
<li>❌ 出力が0中心でない（学習の収束が遅い）</li>
</ul>

<hr>

<h3>3.1.2 tanh関数（双曲線正接）</h3>

<p><strong>数式</strong>：</p>
<p>$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$</p>

<p><strong>導関数</strong>：</p>
<p>$$
\tanh'(x) = 1 - \tanh^2(x)
$$</p>

<pre><code class="language-python">def tanh(x):
    """tanh関数"""
    return np.tanh(x)

def tanh_derivative(x):
    """tanhの導関数"""
    return 1 - np.tanh(x) ** 2

# 可視化
y_tanh = tanh(x)
dy_tanh = tanh_derivative(x)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(x, y_tanh, linewidth=2, color='green')
plt.xlabel('x', fontsize=12)
plt.ylabel('tanh(x)', fontsize=12)
plt.title('tanh関数', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(x, dy_tanh, linewidth=2, color='red')
plt.xlabel('x', fontsize=12)
plt.ylabel("tanh'(x)", fontsize=12)
plt.title('tanhの導関数', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>特徴</strong>：</p>
<ul>
<li>✅ 出力範囲: (-1, 1)</li>
<li>✅ 0中心の出力（Sigmoidより優れている）</li>
<li>❌ 勾配消失問題は残る</li>
</ul>

<hr>

<h3>3.1.3 ReLU（Rectified Linear Unit）</h3>

<p><strong>数式</strong>：</p>
<p>$$
\text{ReLU}(x) = \max(0, x)
$$</p>

<p><strong>導関数</strong>：</p>
<p>$$
\text{ReLU}'(x) = \begin{cases}
1 & \text{if } x > 0 \\
0 & \text{if } x \leq 0
\end{cases}
$$</p>

<pre><code class="language-python">def relu(x):
    """ReLU関数"""
    return np.maximum(0, x)

def relu_derivative(x):
    """ReLUの導関数"""
    return (x > 0).astype(float)

# 可視化
y_relu = relu(x)
dy_relu = relu_derivative(x)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(x, y_relu, linewidth=2, color='purple')
plt.xlabel('x', fontsize=12)
plt.ylabel('ReLU(x)', fontsize=12)
plt.title('ReLU関数', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(x, dy_relu, linewidth=2, color='red')
plt.xlabel('x', fontsize=12)
plt.ylabel("ReLU'(x)", fontsize=12)
plt.title('ReLUの導関数', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>特徴</strong>：</p>
<ul>
<li>✅ 計算が非常に高速（max演算のみ）</li>
<li>✅ 勾配消失問題が大幅に軽減</li>
<li>✅ <strong>現在最も広く使われる活性化関数</strong></li>
<li>❌ <strong>Dying ReLU問題</strong>: 負の入力でニューロンが死ぬ（勾配0）</li>
</ul>

<hr>

<h3>3.1.4 Leaky ReLU</h3>

<p><strong>数式</strong>：</p>
<p>$$
\text{Leaky ReLU}(x) = \begin{cases}
x & \text{if } x > 0 \\
\alpha x & \text{if } x \leq 0
\end{cases}
$$</p>

<p>通常、$\alpha = 0.01$</p>

<pre><code class="language-python">def leaky_relu(x, alpha=0.01):
    """Leaky ReLU関数"""
    return np.where(x > 0, x, alpha * x)

def leaky_relu_derivative(x, alpha=0.01):
    """Leaky ReLUの導関数"""
    return np.where(x > 0, 1.0, alpha)

# 可視化
y_leaky = leaky_relu(x)
dy_leaky = leaky_relu_derivative(x)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(x, y_leaky, linewidth=2, color='orange')
plt.xlabel('x', fontsize=12)
plt.ylabel('Leaky ReLU(x)', fontsize=12)
plt.title('Leaky ReLU関数 (α=0.01)', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(x, dy_leaky, linewidth=2, color='red')
plt.xlabel('x', fontsize=12)
plt.ylabel("Leaky ReLU'(x)", fontsize=12)
plt.title('Leaky ReLUの導関数', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>特徴</strong>：</p>
<ul>
<li>✅ Dying ReLU問題を解決</li>
<li>✅ 負の入力でも小さな勾配が流れる</li>
</ul>

<hr>

<h3>3.1.5 ELU（Exponential Linear Unit）</h3>

<p><strong>数式</strong>：</p>
<p>$$
\text{ELU}(x) = \begin{cases}
x & \text{if } x > 0 \\
\alpha (e^x - 1) & \text{if } x \leq 0
\end{cases}
$$</p>

<pre><code class="language-python">def elu(x, alpha=1.0):
    """ELU関数"""
    return np.where(x > 0, x, alpha * (np.exp(x) - 1))

def elu_derivative(x, alpha=1.0):
    """ELUの導関数"""
    return np.where(x > 0, 1.0, alpha * np.exp(x))

# 可視化
y_elu = elu(x)
dy_elu = elu_derivative(x)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(x, y_elu, linewidth=2, color='brown')
plt.xlabel('x', fontsize=12)
plt.ylabel('ELU(x)', fontsize=12)
plt.title('ELU関数 (α=1.0)', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(x, dy_elu, linewidth=2, color='red')
plt.xlabel('x', fontsize=12)
plt.ylabel("ELU'(x)", fontsize=12)
plt.title('ELUの導関数', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>特徴</strong>：</p>
<ul>
<li>✅ 滑らかで微分可能</li>
<li>✅ 平均が0に近い出力</li>
<li>❌ 指数関数の計算コストが高い</li>
</ul>

<hr>

<h3>活性化関数の比較</h3>

<table>
<thead>
<tr>
<th>関数</th>
<th>出力範囲</th>
<th>勾配消失</th>
<th>計算速度</th>
<th>推奨用途</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Sigmoid</strong></td>
<td>(0, 1)</td>
<td>❌ あり</td>
<td>⚡ 遅い</td>
<td>出力層（二値分類）</td>
</tr>
<tr>
<td><strong>tanh</strong></td>
<td>(-1, 1)</td>
<td>❌ あり</td>
<td>⚡ 遅い</td>
<td>RNN（過去）</td>
</tr>
<tr>
<td><strong>ReLU</strong></td>
<td>[0, ∞)</td>
<td>✅ なし</td>
<td>⚡⚡⚡ 非常に速い</td>
<td>隠れ層（デフォルト）</td>
</tr>
<tr>
<td><strong>Leaky ReLU</strong></td>
<td>(-∞, ∞)</td>
<td>✅ なし</td>
<td>⚡⚡⚡ 非常に速い</td>
<td>ReLUで失敗時</td>
</tr>
<tr>
<td><strong>ELU</strong></td>
<td>[-α, ∞)</td>
<td>✅ なし</td>
<td>⚡⚡ 中程度</td>
<td>高精度が必要な場合</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.2 勾配消失問題（Vanishing Gradient Problem）</h2>

<h3>問題の本質</h3>

<p>深いネットワークでは、逆伝播時に勾配が<strong>指数関数的に小さくなる</strong>現象が発生します。</p>

<p>連鎖律により：</p>
<p>$$
\frac{\partial L}{\partial w^{(1)}} = \frac{\partial L}{\partial w^{(10)}} \cdot \frac{\partial w^{(10)}}{\partial w^{(9)}} \cdot \ldots \cdot \frac{\partial w^{(2)}}{\partial w^{(1)}}
$$</p>

<p>各層で $|\frac{\partial w^{(l)}}{\partial w^{(l-1)}}| < 1$ の場合、勾配が消失します。</p>

<pre><code class="language-python">def demonstrate_vanishing_gradient():
    """勾配消失問題のデモンストレーション"""

    # Sigmoidネットワーク（10層）
    def forward_sigmoid_deep(x, n_layers=10):
        a = x
        activations = [a]

        for _ in range(n_layers):
            z = a * 0.5  # 簡略化した重み
            a = sigmoid(z)
            activations.append(a)

        return activations

    # 勾配の計算
    x = np.array([1.0])
    activations = forward_sigmoid_deep(x, n_layers=10)

    # 各層の勾配を計算
    gradients = []
    grad = 1.0

    for i in range(len(activations) - 1, 0, -1):
        a = activations[i]
        grad = grad * (a * (1 - a)) * 0.5  # 連鎖律
        gradients.append(grad)

    gradients = gradients[::-1]

    # プロット
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(gradients) + 1), gradients, marker='o',
             linewidth=2, markersize=8, label='勾配の大きさ')
    plt.xlabel('層の深さ', fontsize=12)
    plt.ylabel('勾配', fontsize=12)
    plt.title('勾配消失問題の可視化（Sigmoid、10層）', fontsize=14, fontweight='bold')
    plt.yscale('log')
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.show()

    print("=== 各層の勾配 ===")
    for i, grad in enumerate(gradients, 1):
        print(f"第{i}層: {grad:.10f}")

demonstrate_vanishing_gradient()
</code></pre>

<h3>対策</h3>

<ol>
<li><strong>ReLUを使う</strong>: 勾配が1（$x > 0$の場合）</li>
<li><strong>Batch Normalization</strong>: 各層の入力を正規化</li>
<li><strong>Residual Connection</strong>: 勾配をショートカット</li>
<li><strong>適切な初期化</strong>: Xavier、He初期化</li>
</ol>

<hr>

<h2>3.3 最適化アルゴリズム（Optimization Algorithms）</h2>

<h3>3.3.1 SGD（Stochastic Gradient Descent）</h3>

<p><strong>更新式</strong>：</p>
<p>$$
w \leftarrow w - \eta \frac{\partial L}{\partial w}
$$</p>

<pre><code class="language-python">class SGD:
    """確率的勾配降下法"""

    def __init__(self, learning_rate=0.01):
        self.learning_rate = learning_rate

    def update(self, params, grads):
        """
        パラメータの更新

        Args:
            params: パラメータの辞書 {'W1': ..., 'b1': ...}
            grads: 勾配の辞書 {'W1': ..., 'b1': ...}
        """
        for key in params.keys():
            params[key] -= self.learning_rate * grads[key]
</code></pre>

<hr>

<h3>3.3.2 Momentum</h3>

<p><strong>更新式</strong>：</p>
<p>$$
\begin{align}
v &\leftarrow \beta v - \eta \frac{\partial L}{\partial w} \\
w &\leftarrow w + v
\end{align}
$$</p>

<p>通常、$\beta = 0.9$</p>

<pre><code class="language-python">class Momentum:
    """Momentum最適化"""

    def __init__(self, learning_rate=0.01, momentum=0.9):
        self.learning_rate = learning_rate
        self.momentum = momentum
        self.velocity = None

    def update(self, params, grads):
        if self.velocity is None:
            self.velocity = {}
            for key, val in params.items():
                self.velocity[key] = np.zeros_like(val)

        for key in params.keys():
            self.velocity[key] = self.momentum * self.velocity[key] - self.learning_rate * grads[key]
            params[key] += self.velocity[key]
</code></pre>

<p><strong>特徴</strong>：</p>
<ul>
<li>✅ 過去の勾配を考慮</li>
<li>✅ 振動を抑制し、収束を加速</li>
</ul>

<hr>

<h3>3.3.3 AdaGrad（Adaptive Gradient）</h3>

<p><strong>更新式</strong>：</p>
<p>$$
\begin{align}
h &\leftarrow h + \left(\frac{\partial L}{\partial w}\right)^2 \\
w &\leftarrow w - \frac{\eta}{\sqrt{h} + \epsilon} \frac{\partial L}{\partial w}
\end{align}
$$</p>

<pre><code class="language-python">class AdaGrad:
    """AdaGrad最適化"""

    def __init__(self, learning_rate=0.01):
        self.learning_rate = learning_rate
        self.h = None
        self.epsilon = 1e-8

    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)

        for key in params.keys():
            self.h[key] += grads[key] ** 2
            params[key] -= self.learning_rate * grads[key] / (np.sqrt(self.h[key]) + self.epsilon)
</code></pre>

<p><strong>特徴</strong>：</p>
<ul>
<li>✅ パラメータごとに学習率を調整</li>
<li>❌ 学習率が徐々に小さくなりすぎる</li>
</ul>

<hr>

<h3>3.3.4 RMSprop</h3>

<p><strong>更新式</strong>：</p>
<p>$$
\begin{align}
h &\leftarrow \beta h + (1 - \beta) \left(\frac{\partial L}{\partial w}\right)^2 \\
w &\leftarrow w - \frac{\eta}{\sqrt{h} + \epsilon} \frac{\partial L}{\partial w}
\end{align}
$$</p>

<pre><code class="language-python">class RMSprop:
    """RMSprop最適化"""

    def __init__(self, learning_rate=0.01, decay_rate=0.99):
        self.learning_rate = learning_rate
        self.decay_rate = decay_rate
        self.h = None
        self.epsilon = 1e-8

    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)

        for key in params.keys():
            self.h[key] = self.decay_rate * self.h[key] + (1 - self.decay_rate) * grads[key] ** 2
            params[key] -= self.learning_rate * grads[key] / (np.sqrt(self.h[key]) + self.epsilon)
</code></pre>

<p><strong>特徴</strong>：</p>
<ul>
<li>✅ AdaGradの改良版</li>
<li>✅ 指数移動平均で学習率の減衰を緩和</li>
</ul>

<hr>

<h3>3.3.5 Adam（Adaptive Moment Estimation）</h3>

<p><strong>更新式</strong>：</p>
<p>$$
\begin{align}
m &\leftarrow \beta_1 m + (1 - \beta_1) \frac{\partial L}{\partial w} \\
v &\leftarrow \beta_2 v + (1 - \beta_2) \left(\frac{\partial L}{\partial w}\right)^2 \\
\hat{m} &\leftarrow \frac{m}{1 - \beta_1^t} \\
\hat{v} &\leftarrow \frac{v}{1 - \beta_2^t} \\
w &\leftarrow w - \frac{\eta}{\sqrt{\hat{v}} + \epsilon} \hat{m}
\end{align}
$$</p>

<pre><code class="language-python">class Adam:
    """Adam最適化（最も推奨）"""

    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999):
        self.learning_rate = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.m = None
        self.v = None
        self.t = 0
        self.epsilon = 1e-8

    def update(self, params, grads):
        if self.m is None:
            self.m = {}
            self.v = {}
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)

        self.t += 1

        for key in params.keys():
            # 1次モーメント（平均）
            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]

            # 2次モーメント（分散）
            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[key] ** 2)

            # バイアス補正
            m_hat = self.m[key] / (1 - self.beta1 ** self.t)
            v_hat = self.v[key] / (1 - self.beta2 ** self.t)

            # パラメータ更新
            params[key] -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)
</code></pre>

<p><strong>特徴</strong>：</p>
<ul>
<li>✅ MomentumとRMSpropの長所を組み合わせ</li>
<li>✅ <strong>現在最も広く使われる最適化アルゴリズム</strong></li>
<li>✅ ハイパーパラメータのチューニングがほぼ不要</li>
</ul>

<hr>

<h3>最適化アルゴリズムの比較</h3>

<pre><code class="language-python">def compare_optimizers():
    """最適化アルゴリズムの比較"""

    # テスト関数: f(x, y) = x^2 + 10*y^2（楕円）
    def f(x, y):
        return x ** 2 + 10 * y ** 2

    def grad_f(x, y):
        return np.array([2*x, 20*y])

    # 初期値
    init_pos = (-7.0, 2.0)
    learning_rate = 0.1
    iterations = 30

    # 各最適化アルゴリズムで最適化
    optimizers = {
        'SGD': SGD(learning_rate=learning_rate),
        'Momentum': Momentum(learning_rate=learning_rate),
        'AdaGrad': AdaGrad(learning_rate=learning_rate),
        'RMSprop': RMSprop(learning_rate=learning_rate),
        'Adam': Adam(learning_rate=learning_rate)
    }

    trajectories = {}

    for name, optimizer in optimizers.items():
        pos = np.array(init_pos)
        params = {'pos': pos}
        trajectory = [pos.copy()]

        for _ in range(iterations):
            grads = {'pos': grad_f(pos[0], pos[1])}
            optimizer.update(params, grads)
            pos = params['pos']
            trajectory.append(pos.copy())

        trajectories[name] = np.array(trajectory)

    # プロット
    plt.figure(figsize=(12, 10))

    # 等高線の描画
    x = np.linspace(-8, 2, 100)
    y = np.linspace(-3, 3, 100)
    X, Y = np.meshgrid(x, y)
    Z = f(X, Y)

    plt.contour(X, Y, Z, levels=20, alpha=0.3)

    # 各最適化アルゴリズムの軌跡
    colors = ['blue', 'green', 'red', 'purple', 'orange']
    for (name, trajectory), color in zip(trajectories.items(), colors):
        plt.plot(trajectory[:, 0], trajectory[:, 1], marker='o',
                 label=name, color=color, linewidth=2, markersize=4)

    plt.plot(0, 0, 'r*', markersize=20, label='最適解')
    plt.xlabel('x', fontsize=12)
    plt.ylabel('y', fontsize=12)
    plt.title('最適化アルゴリズムの比較', fontsize=14, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.show()

compare_optimizers()
</code></pre>

<hr>

<h2>3.4 重みの初期化</h2>

<h3>なぜ初期化が重要か</h3>

<p>重みの初期値が不適切だと：</p>
<ul>
<li>❌ 勾配消失・勾配爆発</li>
<li>❌ 学習が進まない</li>
<li>❌ 局所最適解に陥る</li>
</ul>

<h3>3.4.1 Xavier初期化</h3>

<p><strong>数式</strong>（Sigmoid、tanh用）：</p>
<p>$$
W \sim \mathcal{N}\left(0, \sqrt{\frac{2}{n_{\text{in}} + n_{\text{out}}}}\right)
$$</p>

<pre><code class="language-python">def xavier_init(n_in, n_out):
    """Xavier初期化"""
    return np.random.randn(n_in, n_out) * np.sqrt(2.0 / (n_in + n_out))

# 例
W = xavier_init(100, 50)
print(f"Xavier初期化: 平均={W.mean():.4f}, 標準偏差={W.std():.4f}")
</code></pre>

<h3>3.4.2 He初期化</h3>

<p><strong>数式</strong>（ReLU用）：</p>
<p>$$
W \sim \mathcal{N}\left(0, \sqrt{\frac{2}{n_{\text{in}}}}\right)
$$</p>

<pre><code class="language-python">def he_init(n_in, n_out):
    """He初期化（ReLU用）"""
    return np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)

# 例
W = he_init(100, 50)
print(f"He初期化: 平均={W.mean():.4f}, 標準偏差={W.std():.4f}")
</code></pre>

<h3>初期化の比較</h3>

<table>
<thead>
<tr>
<th>初期化手法</th>
<th>数式</th>
<th>推奨活性化関数</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ゼロ初期化</strong></td>
<td>$W = 0$</td>
<td>❌ 使用不可</td>
</tr>
<tr>
<td><strong>ランダム初期化</strong></td>
<td>$W \sim \mathcal{N}(0, 0.01)$</td>
<td>基本的に非推奨</td>
</tr>
<tr>
<td><strong>Xavier初期化</strong></td>
<td>$\sqrt{2/(n_{in}+n_{out})}$</td>
<td>Sigmoid、tanh</td>
</tr>
<tr>
<td><strong>He初期化</strong></td>
<td>$\sqrt{2/n_{in}}$</td>
<td>ReLU、Leaky ReLU</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.5 本章のまとめ</h2>

<h3>学んだこと</h3>

<ol>
<li><p><strong>活性化関数</strong></p>
<ul>
<li>ReLU: 現在のデフォルト</li>
<li>Leaky ReLU: Dying ReLU対策</li>
<li>Sigmoid/tanh: 勾配消失問題あり</li>
</ul></li>
<li><p><strong>勾配消失問題</strong></p>
<ul>
<li>深いネットワークでの課題</li>
<li>ReLU、Batch Norm、適切な初期化で対策</li>
</ul></li>
<li><p><strong>最適化アルゴリズム</strong></p>
<ul>
<li>Adam: 最も推奨</li>
<li>Momentum: 収束を加速</li>
<li>SGD: 基本だが遅い</li>
</ul></li>
<li><p><strong>重みの初期化</strong></p>
<ul>
<li>ReLU → He初期化</li>
<li>Sigmoid/tanh → Xavier初期化</li>
</ul></li>
</ol>

<h3>推奨設定</h3>

<table>
<thead>
<tr>
<th>要素</th>
<th>推奨</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>活性化関数</strong></td>
<td>ReLU（隠れ層）</td>
</tr>
<tr>
<td><strong>最適化</strong></td>
<td>Adam</td>
</tr>
<tr>
<td><strong>初期化</strong></td>
<td>He初期化</td>
</tr>
<tr>
<td><strong>学習率</strong></td>
<td>0.001（Adam）</td>
</tr>
</tbody>
</table>

<div class="navigation">
    <a href="chapter2-mlp.html" class="nav-button">← 前の章: 多層パーセプトロン</a>
    <a href="chapter4-frameworks.html" class="nav-button">次の章: PyTorchとTensorFlow →</a>
</div>

    </main>

    <footer>
        <p><strong>作成者</strong>: AI Terakoya Content Team</p>
        <p><strong>監修</strong>: Dr. Yusuke Hashimoto（東北大学）</p>
        <p><strong>バージョン</strong>: 1.0 | <strong>作成日</strong>: 2025-10-20</p>
        <p><strong>ライセンス</strong>: Creative Commons BY 4.0</p>
        <p>© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
