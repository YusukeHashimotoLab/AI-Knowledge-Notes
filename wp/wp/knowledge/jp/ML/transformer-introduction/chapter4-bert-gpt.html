<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第4章：BERT・GPT - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: var(--font-body); line-height: 1.7; color: var(--color-text); background-color: var(--color-bg); font-size: 16px; }
        header { background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; padding: var(--spacing-xl) var(--spacing-md); margin-bottom: var(--spacing-xl); box-shadow: var(--box-shadow); }
        .header-content { max-width: 900px; margin: 0 auto; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: var(--spacing-sm); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; opacity: 0.95; font-weight: 400; margin-bottom: var(--spacing-md); }
        .meta { display: flex; flex-wrap: wrap; gap: var(--spacing-md); font-size: 0.9rem; opacity: 0.9; }
        .meta-item { display: flex; align-items: center; gap: 0.3rem; }
        .container { max-width: 900px; margin: 0 auto; padding: 0 var(--spacing-md) var(--spacing-xl); }
        h2 { font-size: 1.75rem; color: var(--color-primary); margin-top: var(--spacing-xl); margin-bottom: var(--spacing-md); padding-bottom: var(--spacing-xs); border-bottom: 3px solid var(--color-accent); }
        h3 { font-size: 1.4rem; color: var(--color-primary); margin-top: var(--spacing-lg); margin-bottom: var(--spacing-sm); }
        h4 { font-size: 1.1rem; color: var(--color-primary-dark); margin-top: var(--spacing-md); margin-bottom: var(--spacing-sm); }
        p { margin-bottom: var(--spacing-md); color: var(--color-text); }
        a { color: var(--color-link); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--color-link-hover); text-decoration: underline; }
        ul, ol { margin-left: var(--spacing-lg); margin-bottom: var(--spacing-md); }
        li { margin-bottom: var(--spacing-xs); color: var(--color-text); }
        pre { background-color: var(--color-code-bg); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); overflow-x: auto; margin-bottom: var(--spacing-md); font-family: var(--font-mono); font-size: 0.9rem; line-height: 1.5; }
        code { font-family: var(--font-mono); font-size: 0.9em; background-color: var(--color-code-bg); padding: 0.2em 0.4em; border-radius: 3px; }
        pre code { background-color: transparent; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: var(--spacing-md); font-size: 0.95rem; }
        th, td { border: 1px solid var(--color-border); padding: var(--spacing-sm); text-align: left; }
        th { background-color: var(--color-bg-alt); font-weight: 600; color: var(--color-primary); }
        blockquote { border-left: 4px solid var(--color-accent); padding-left: var(--spacing-md); margin: var(--spacing-md) 0; color: var(--color-text-light); font-style: italic; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        .mermaid { text-align: center; margin: var(--spacing-lg) 0; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        details { background-color: var(--color-bg-alt); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); margin-bottom: var(--spacing-md); }
        summary { cursor: pointer; font-weight: 600; color: var(--color-primary); user-select: none; padding: var(--spacing-xs); margin: calc(-1 * var(--spacing-md)); padding: var(--spacing-md); border-radius: var(--border-radius); }
        summary:hover { background-color: rgba(123, 44, 191, 0.1); }
        details[open] summary { margin-bottom: var(--spacing-md); border-bottom: 1px solid var(--color-border); }
        .navigation { display: flex; justify-content: space-between; gap: var(--spacing-md); margin: var(--spacing-xl) 0; padding-top: var(--spacing-lg); border-top: 2px solid var(--color-border); }
        .nav-button { flex: 1; padding: var(--spacing-md); background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; border-radius: var(--border-radius); text-align: center; font-weight: 600; transition: transform 0.2s, box-shadow 0.2s; box-shadow: var(--box-shadow); }
        .nav-button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); text-decoration: none; }
        footer { margin-top: var(--spacing-xl); padding: var(--spacing-lg) var(--spacing-md); background-color: var(--color-bg-alt); border-top: 1px solid var(--color-border); text-align: center; font-size: 0.9rem; color: var(--color-text-light); }
        .project-box { background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 50%); border-radius: var(--border-radius); padding: var(--spacing-lg); margin: var(--spacing-lg) 0; box-shadow: var(--box-shadow); }
        @media (max-width: 768px) { h1 { font-size: 1.5rem; } h2 { font-size: 1.4rem; } h3 { font-size: 1.2rem; } .meta { font-size: 0.85rem; } .navigation { flex-direction: column; } table { font-size: 0.85rem; } th, td { padding: var(--spacing-xs); } }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>第4章：BERT・GPT</h1>
            <p class="subtitle">事前学習モデルの双璧：双方向エンコーダと自己回帰生成モデルの理論と実践</p>
            <div class="meta">
                <span class="meta-item">📖 読了時間: 28分</span>
                <span class="meta-item">📊 難易度: 中級〜上級</span>
                <span class="meta-item">💻 コード例: 9個</span>
                <span class="meta-item">📝 演習問題: 6問</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>学習目標</h2>
<p>この章を読むことで、以下を習得できます：</p>
<ul>
<li>✅ BERTの双方向エンコーディングとMasked Language Modelingを理解できる</li>
<li>✅ GPTの自己回帰生成とCausal Maskingの仕組みを理解できる</li>
<li>✅ BERT・GPTの事前学習タスク（MLM、NSP、CLM）を実装できる</li>
<li>✅ Hugging Face Transformersライブラリで両モデルを使用できる</li>
<li>✅ Fine-tuningによるタスク特化型モデルの構築ができる</li>
<li>✅ BERTとGPTの使い分けと適用場面を判断できる</li>
<li>✅ 質問応答システムとテキスト生成の実践プロジェクトを完成できる</li>
</ul>

<hr>

<h2>4.1 BERTアーキテクチャ</h2>

<h3>4.1.1 BERTの革新性と設計思想</h3>

<p><strong>BERT</strong>（Bidirectional Encoder Representations from Transformers）は、2018年にGoogleが発表した事前学習モデルで、自然言語処理に革命をもたらしました。</p>

<table>
<thead>
<tr>
<th>特性</th>
<th>従来モデル（ELMo、GPT-1など）</th>
<th>BERT</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>方向性</strong></td>
<td>単方向（左→右）または浅い双方向</td>
<td>深い双方向（左右両方の文脈を利用）</td>
</tr>
<tr>
<td><strong>アーキテクチャ</strong></td>
<td>RNN、LSTM、浅いTransformer</td>
<td>Transformer Encoderのみ（12〜24層）</td>
</tr>
<tr>
<td><strong>事前学習</strong></td>
<td>言語モデリング（次単語予測）</td>
<td>Masked LM + Next Sentence Prediction</td>
</tr>
<tr>
<td><strong>用途</strong></td>
<td>主に生成タスク</td>
<td>分類、抽出、質問応答など理解タスク</td>
</tr>
<tr>
<td><strong>Fine-tuning</strong></td>
<td>複雑なタスク特化アーキテクチャ必要</td>
<td>シンプルな出力層追加のみ</td>
</tr>
</tbody>
</table>

<h3>4.1.2 BERTの双方向性の実現</h3>

<p>BERTの最大の特徴は、<strong>双方向のコンテキスト理解</strong>です。従来の言語モデルは左から右へ順次単語を予測していましたが、BERTは文全体を見渡して各単語を理解します。</p>

<div class="mermaid">
graph LR
    subgraph "従来の単方向モデル（GPT-1など）"
        A1[The] --> A2[cat]
        A2 --> A3[sat]
        A3 --> A4[on]
        A4 --> A5[mat]

        style A1 fill:#e74c3c,color:#fff
        style A2 fill:#e74c3c,color:#fff
        style A3 fill:#e74c3c,color:#fff
    end

    subgraph "BERTの双方向モデル"
        B1[The] <--> B2[cat]
        B2 <--> B3[sat]
        B3 <--> B4[on]
        B4 <--> B5[mat]

        style B2 fill:#27ae60,color:#fff
        style B3 fill:#27ae60,color:#fff
    end
</div>

<blockquote>
<p><strong>重要</strong>: BERTは文中の単語「cat」を理解する際、「The」（左文脈）と「sat on mat」（右文脈）の両方を同時に利用します。これにより、単語の意味を正確に捉えることができます。</p>
</blockquote>

<h3>4.1.3 BERTのアーキテクチャ構成</h3>

<p>BERTは複数のTransformer Encoderブロックを積み重ねた構造です：</p>

<table>
<thead>
<tr>
<th>モデル</th>
<th>層数（L）</th>
<th>隠れ層サイズ（H）</th>
<th>Attention Heads（A）</th>
<th>パラメータ数</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>BERT-Base</strong></td>
<td>12</td>
<td>768</td>
<td>12</td>
<td>110M</td>
</tr>
<tr>
<td><strong>BERT-Large</strong></td>
<td>24</td>
<td>1024</td>
<td>16</td>
<td>340M</td>
</tr>
</tbody>
</table>

<p>各Transformer Encoderブロックは、第2章で学んだMulti-Head AttentionとFeed-Forward Networkで構成されます：</p>

$$
\text{EncoderBlock}(x) = \text{LayerNorm}(x + \text{FFN}(\text{LayerNorm}(x + \text{MultiHeadAttn}(x))))
$$

<h3>4.1.4 入力表現：Token + Segment + Position Embeddings</h3>

<p>BERTの入力は3種類のEmbeddingの合計です：</p>

<ol>
<li><strong>Token Embeddings</strong>: 単語（サブワード）の埋め込み表現</li>
<li><strong>Segment Embeddings</strong>: 文A・文Bを区別（NSPタスク用）</li>
<li><strong>Position Embeddings</strong>: 位置情報（学習可能、GPTのSinusoidalとは異なる）</li>
</ol>

$$
\text{Input} = \text{TokenEmbed}(x) + \text{SegmentEmbed}(x) + \text{PositionEmbed}(x)
$$

<div class="mermaid">
graph TB
    subgraph "BERT入力構成"
        T1["[CLS] The cat sat [SEP] on mat [SEP]"]

        T2[Token Embeddings]
        T3[Segment Embeddings]
        T4[Position Embeddings]

        T5[Input to Transformer]

        T1 --> T2
        T1 --> T3
        T1 --> T4

        T2 --> T5
        T3 --> T5
        T4 --> T5

        style T5 fill:#7b2cbf,color:#fff
    end
</div>

<p><strong>特殊トークン</strong>：</p>
<ul>
<li><code>[CLS]</code>: 文全体の分類表現（Classification token）</li>
<li><code>[SEP]</code>: 文の区切り（Separator）</li>
<li><code>[MASK]</code>: Masked Language Modeling用のマスクトークン</li>
</ul>

<hr>

<h2>4.2 BERTの事前学習タスク</h2>

<h3>4.2.1 Masked Language Modeling (MLM)</h3>

<p>MLMは、入力の一部をマスクして、その単語を予測するタスクです。これにより双方向の文脈を学習します。</p>

<h4>MLMの手順</h4>

<ol>
<li>入力トークンの15%をランダムに選択</li>
<li>選択されたトークンに対して：
<ul>
<li>80%の確率で<code>[MASK]</code>トークンに置換</li>
<li>10%の確率でランダムな別の単語に置換</li>
<li>10%の確率で元の単語のまま保持</li>
</ul>
</li>
<li>マスクされた位置の元の単語を予測</li>
</ol>

<p><strong>例</strong>：</p>
<pre><code>入力: "The cat sat on the mat"
マスク後: "The [MASK] sat on the mat"
目標: "cat"を予測
</code></pre>

<h4>なぜ100%マスクしないのか？</h4>

<p>Fine-tuning時に<code>[MASK]</code>トークンは存在しません。訓練と本番のギャップを減らすため、一部をランダム単語や元の単語のままにします。</p>

<h3>4.2.2 Next Sentence Prediction (NSP)</h3>

<p>NSPは、2つの文が連続しているかを判定するタスクです。質問応答や自然言語推論で文間関係の理解が重要となります。</p>

<h4>NSPの構成</h4>

<ul>
<li><strong>IsNext</strong> (50%): 実際に連続する文ペア</li>
<li><strong>NotNext</strong> (50%): ランダムに選ばれた非連続文ペア</li>
</ul>

<p><strong>例</strong>：</p>
<pre><code>入力A: "The cat sat on the mat."
入力B (IsNext): "It was very comfortable."
入力B (NotNext): "Paris is the capital of France."

BERT入力: [CLS] The cat sat on the mat [SEP] It was very comfortable [SEP]
目標: IsNext = True
</code></pre>

<h3>4.2.3 PyTorchによるMLMの実装</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import random
import numpy as np

class MaskedLanguageModel:
    """BERT-style Masked Language Modeling実装"""

    def __init__(self, vocab_size, mask_prob=0.15):
        self.vocab_size = vocab_size
        self.mask_prob = mask_prob

        # 特殊トークンID
        self.MASK_TOKEN_ID = vocab_size - 3
        self.CLS_TOKEN_ID = vocab_size - 2
        self.SEP_TOKEN_ID = vocab_size - 1

    def create_masked_lm_data(self, input_ids):
        """
        MLM用のマスクデータ生成

        Args:
            input_ids: [batch_size, seq_len] 入力トークンID

        Returns:
            masked_input: マスク適用後の入力
            labels: 予測対象のラベル（マスク位置のみ有効、他は-100）
        """
        batch_size, seq_len = input_ids.shape

        # ラベル初期化（-100は損失計算で無視される）
        labels = torch.full_like(input_ids, -100)
        masked_input = input_ids.clone()

        for i in range(batch_size):
            # 特殊トークンを除外してマスク対象を選択
            special_tokens_mask = (input_ids[i] == self.CLS_TOKEN_ID) | \
                                 (input_ids[i] == self.SEP_TOKEN_ID)

            # マスク可能な位置
            candidate_indices = torch.where(~special_tokens_mask)[0]

            # 15%をマスク対象に選択
            num_to_mask = max(1, int(len(candidate_indices) * self.mask_prob))
            mask_indices = candidate_indices[torch.randperm(len(candidate_indices))[:num_to_mask]]

            for idx in mask_indices:
                labels[i, idx] = input_ids[i, idx]  # 元の単語を保存

                rand = random.random()
                if rand < 0.8:
                    # 80%: [MASK]トークンに置換
                    masked_input[i, idx] = self.MASK_TOKEN_ID
                elif rand < 0.9:
                    # 10%: ランダムな単語に置換
                    random_token = random.randint(0, self.vocab_size - 4)
                    masked_input[i, idx] = random_token
                # 10%: 元の単語のまま（else不要）

        return masked_input, labels


# デモンストレーション
print("=== Masked Language Modeling Demo ===\n")

# パラメータ設定
vocab_size = 1000
batch_size = 3
seq_len = 10

# ダミー入力生成
mlm = MaskedLanguageModel(vocab_size)
input_ids = torch.randint(0, vocab_size - 3, (batch_size, seq_len))

# [CLS]を先頭、[SEP]を末尾に追加
input_ids[:, 0] = mlm.CLS_TOKEN_ID
input_ids[:, -1] = mlm.SEP_TOKEN_ID

print("Original Input IDs (Batch 0):")
print(input_ids[0].numpy())

# MLMマスク適用
masked_input, labels = mlm.create_masked_lm_data(input_ids)

print("\nMasked Input IDs (Batch 0):")
print(masked_input[0].numpy())

print("\nLabels (Batch 0, -100は無視):")
print(labels[0].numpy())

# マスク位置を確認
mask_positions = torch.where(labels[0] != -100)[0]
print(f"\nMasked Positions: {mask_positions.numpy()}")
print(f"Number of masked tokens: {len(mask_positions)} / {seq_len-2} (excluding [CLS] and [SEP])")

for pos in mask_positions:
    original = input_ids[0, pos].item()
    masked = masked_input[0, pos].item()
    target = labels[0, pos].item()

    mask_type = "MASK" if masked == mlm.MASK_TOKEN_ID else \
                "RANDOM" if masked != original else \
                "UNCHANGED"

    print(f"  Position {pos}: Original={original}, Masked={masked} ({mask_type}), Target={target}")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== Masked Language Modeling Demo ===

Original Input IDs (Batch 0):
[998 453 721 892 156 334 667 289 445 999]

Masked Input IDs (Batch 0):
[998 997 721 542 156 997 667 289 445 999]

Labels (Batch 0, -100は無視):
[-100 453 -100 892 -100 334 -100 -100 -100 -100]

Masked Positions: [1 3 5]
Number of masked tokens: 3 / 8 (excluding [CLS] and [SEP])
  Position 1: Original=453, Masked=997 (MASK), Target=453
  Position 3: Original=892, Masked=542 (RANDOM), Target=892
  Position 5: Original=334, Masked=997 (MASK), Target=334
</code></pre>

<hr>

<h2>4.3 BERTの使用例</h2>

<h3>4.3.1 テキスト分類（Sentiment Analysis）</h3>

<p>BERTを使った感情分析の実装例です。<code>[CLS]</code>トークンの出力を分類に使用します。</p>

<pre><code class="language-python">from transformers import BertTokenizer, BertForSequenceClassification
import torch

# モデルとTokenizerの読み込み
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2  # 2クラス分類（Positive/Negative）
)

# 推論モードに設定
model.eval()

# サンプルテキスト
texts = [
    "I absolutely loved this movie! It was fantastic.",
    "This product is terrible and waste of money.",
    "The service was okay, nothing special."
]

print("=== BERT Sentiment Analysis Demo ===\n")

for text in texts:
    # トークナイズ
    inputs = tokenizer(
        text,
        return_tensors='pt',
        padding=True,
        truncation=True,
        max_length=128
    )

    # 推論
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=1)
        predicted_class = torch.argmax(probs, dim=1).item()

    sentiment = "Positive" if predicted_class == 1 else "Negative"
    confidence = probs[0, predicted_class].item()

    print(f"Text: {text}")
    print(f"Sentiment: {sentiment} (Confidence: {confidence:.4f})")
    print(f"Probabilities: Negative={probs[0, 0]:.4f}, Positive={probs[0, 1]:.4f}\n")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== BERT Sentiment Analysis Demo ===

Text: I absolutely loved this movie! It was fantastic.
Sentiment: Positive (Confidence: 0.8234)
Probabilities: Negative=0.1766, Positive=0.8234

Text: This product is terrible and waste of money.
Sentiment: Negative (Confidence: 0.9102)
Probabilities: Negative=0.9102, Positive=0.0898

Text: The service was okay, nothing special.
Sentiment: Negative (Confidence: 0.5621)
Probabilities: Negative=0.5621, Positive=0.4379
</code></pre>

<h3>4.3.2 固有表現認識（Named Entity Recognition）</h3>

<p>BERTをToken Classification（各トークンにラベルを付与）に使用する例です。</p>

<pre><code class="language-python">from transformers import BertTokenizerFast, BertForTokenClassification
import torch

print("\n=== BERT Named Entity Recognition Demo ===\n")

# NER用のモデル（事前学習済み）
model_name = 'dbmdz/bert-large-cased-finetuned-conll03-english'
tokenizer = BertTokenizerFast.from_pretrained(model_name)
model = BertForTokenClassification.from_pretrained(model_name)

model.eval()

# ラベルマッピング
label_list = [
    'O',       # Outside
    'B-MISC', 'I-MISC',  # Miscellaneous
    'B-PER', 'I-PER',    # Person
    'B-ORG', 'I-ORG',    # Organization
    'B-LOC', 'I-LOC'     # Location
]

# サンプルテキスト
text = "Apple Inc. was founded by Steve Jobs in Cupertino, California."

# トークナイズ（word_idsを取得するためis_split_into_words=Falseでも処理）
inputs = tokenizer(text, return_tensors='pt', truncation=True)

# 推論
with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.argmax(outputs.logits, dim=2)

# トークンとラベルを表示
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
predicted_labels = [label_list[pred] for pred in predictions[0].numpy()]

print(f"Text: {text}\n")
print("Token-Level Predictions:")
print(f"{'Token':<15} {'Label':<10}")
print("-" * 25)

for token, label in zip(tokens, predicted_labels):
    if token not in ['[CLS]', '[SEP]', '[PAD]']:
        print(f"{token:<15} {label:<10}")

# エンティティ抽出
print("\nExtracted Entities:")
current_entity = []
current_label = None

for token, label in zip(tokens, predicted_labels):
    if label.startswith('B-'):
        if current_entity:
            print(f"  {current_label}: {' '.join(current_entity)}")
        current_entity = [token]
        current_label = label[2:]
    elif label.startswith('I-') and current_label == label[2:]:
        current_entity.append(token)
    else:
        if current_entity:
            print(f"  {current_label}: {' '.join(current_entity)}")
        current_entity = []
        current_label = None

if current_entity:
    print(f"  {current_label}: {' '.join(current_entity)}")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== BERT Named Entity Recognition Demo ===

Text: Apple Inc. was founded by Steve Jobs in Cupertino, California.

Token-Level Predictions:
Token           Label
-------------------------
Apple           B-ORG
Inc             I-ORG
.               O
was             O
founded         O
by              O
Steve           B-PER
Jobs            I-PER
in              O
Cup             B-LOC
##ert           I-LOC
##ino           I-LOC
,               O
California      B-LOC
.               O

Extracted Entities:
  ORG: Apple Inc
  PER: Steve Jobs
  LOC: Cup ##ert ##ino
  LOC: California
</code></pre>

<h3>4.3.3 質問応答（Question Answering）</h3>

<p>BERTの代表的な応用例であるSQuAD形式の質問応答システムです。</p>

<pre><code class="language-python">from transformers import BertForQuestionAnswering, BertTokenizer
import torch

print("\n=== BERT Question Answering Demo ===\n")

# SQuADでFine-tunedされたBERTモデル
model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForQuestionAnswering.from_pretrained(model_name)

model.eval()

# コンテキストと質問
context = """
Transformers is a state-of-the-art natural language processing library developed by Hugging Face.
It provides thousands of pretrained models to perform tasks on texts such as classification,
information extraction, question answering, summarization, translation, and text generation.
The library supports PyTorch, TensorFlow, and JAX frameworks.
"""

questions = [
    "Who developed Transformers?",
    "What tasks can Transformers perform?",
    "Which frameworks does the library support?"
]

for question in questions:
    # トークナイズ
    inputs = tokenizer(
        question,
        context,
        return_tensors='pt',
        truncation=True,
        max_length=384
    )

    # 推論
    with torch.no_grad():
        outputs = model(**inputs)
        start_logits = outputs.start_logits
        end_logits = outputs.end_logits

    # 開始・終了位置の予測
    start_idx = torch.argmax(start_logits)
    end_idx = torch.argmax(end_logits)

    # 回答トークンの抽出
    answer_tokens = inputs['input_ids'][0][start_idx:end_idx+1]
    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)

    # 確信度スコア
    start_score = start_logits[0, start_idx].item()
    end_score = end_logits[0, end_idx].item()
    confidence = (start_score + end_score) / 2

    print(f"Question: {question}")
    print(f"Answer: {answer}")
    print(f"Confidence Score: {confidence:.4f}\n")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== BERT Question Answering Demo ===

Question: Who developed Transformers?
Answer: Hugging Face
Confidence Score: 8.2341

Question: What tasks can Transformers perform?
Answer: classification, information extraction, question answering, summarization, translation, and text generation
Confidence Score: 7.9823

Question: Which frameworks does the library support?
Answer: PyTorch, TensorFlow, and JAX
Confidence Score: 9.1247
</code></pre>

<hr>

<h2>4.4 GPTアーキテクチャ</h2>

<h3>4.4.1 GPTの設計思想：自己回帰言語モデル</h3>

<p><strong>GPT</strong>（Generative Pre-trained Transformer）は、OpenAIが開発した自己回帰型（autoregressive）言語モデルです。BERTとは対照的に、テキスト生成に特化しています。</p>

<table>
<thead>
<tr>
<th>特性</th>
<th>BERT</th>
<th>GPT</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>アーキテクチャ</strong></td>
<td>Transformer Encoder</td>
<td>Transformer Decoder（Cross-Attentionなし）</td>
</tr>
<tr>
<td><strong>方向性</strong></td>
<td>双方向（Bidirectional）</td>
<td>単方向（Unidirectional、左→右）</td>
</tr>
<tr>
<td><strong>事前学習</strong></td>
<td>MLM + NSP</td>
<td>Causal Language Modeling（次単語予測）</td>
</tr>
<tr>
<td><strong>Attention Mask</strong></td>
<td>なし（全トークンを参照）</td>
<td>Causal Mask（未来のトークンを隠す）</td>
</tr>
<tr>
<td><strong>主な用途</strong></td>
<td>分類、抽出、質問応答</td>
<td>テキスト生成、対話、要約</td>
</tr>
<tr>
<td><strong>推論方式</strong></td>
<td>並列処理（全トークン同時）</td>
<td>逐次生成（1トークンずつ）</td>
</tr>
</tbody>
</table>

<h3>4.4.2 Causal Masking：未来を見ないAttention</h3>

<p>GPTの核心は<strong>Causal Attention Mask</strong>です。各位置は自分より前のトークンのみを参照できます。</p>

<p><strong>Causal Mask行列</strong>（1=参照可能、0=参照不可）：</p>
$$
\text{CausalMask} = \begin{bmatrix}
1 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 \\
1 & 1 & 1 & 0 \\
1 & 1 & 1 & 1
\end{bmatrix}
$$

<p>Attention計算時に未来のトークンのスコアを$-\infty$にすることで、Softmax後に確率0になります：</p>

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + M\right) V
$$

<p>ここで $M$ は Causal Mask行列で、マスク位置は$-\infty$です。</p>

<h3>4.4.3 GPT-1/2/3の進化</h3>

<table>
<thead>
<tr>
<th>モデル</th>
<th>発表年</th>
<th>層数</th>
<th>隠れ層サイズ</th>
<th>パラメータ数</th>
<th>訓練データ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>GPT-1</strong></td>
<td>2018</td>
<td>12</td>
<td>768</td>
<td>117M</td>
<td>BooksCorpus (4.5GB)</td>
</tr>
<tr>
<td><strong>GPT-2</strong></td>
<td>2019</td>
<td>48</td>
<td>1600</td>
<td>1.5B</td>
<td>WebText (40GB)</td>
</tr>
<tr>
<td><strong>GPT-3</strong></td>
<td>2020</td>
<td>96</td>
<td>12288</td>
<td>175B</td>
<td>CommonCrawl (570GB)</td>
</tr>
<tr>
<td><strong>GPT-4</strong></td>
<td>2023</td>
<td>非公開</td>
<td>非公開</td>
<td>推定1.7T</td>
<td>非公開（マルチモーダル）</td>
</tr>
</tbody>
</table>

<p><strong>主な進化ポイント</strong>：</p>
<ul>
<li><strong>スケール拡大</strong>: パラメータ数の指数的増加</li>
<li><strong>Few-shot Learning</strong>: GPT-3以降、例を数個示すだけで新タスクに対応</li>
<li><strong>In-context Learning</strong>: Fine-tuningなしでプロンプトのみで学習</li>
<li><strong>Emergent Abilities</strong>: 規模拡大で突然現れる能力（推論、翻訳など）</li>
</ul>

<h3>4.4.4 Causal Attention実装</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

class CausalSelfAttention(nn.Module):
    """GPT-style Causal Self-Attention実装"""

    def __init__(self, embed_size, num_heads):
        super(CausalSelfAttention, self).__init__()
        assert embed_size % num_heads == 0

        self.embed_size = embed_size
        self.num_heads = num_heads
        self.head_dim = embed_size // num_heads

        # Q, K, Vの線形変換
        self.query = nn.Linear(embed_size, embed_size)
        self.key = nn.Linear(embed_size, embed_size)
        self.value = nn.Linear(embed_size, embed_size)

        # 出力層
        self.proj = nn.Linear(embed_size, embed_size)

    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, embed_size]

        Returns:
            output: [batch, seq_len, embed_size]
            attention_weights: [batch, num_heads, seq_len, seq_len]
        """
        batch_size, seq_len, _ = x.shape

        # Q, K, V計算
        Q = self.query(x)
        K = self.key(x)
        V = self.value(x)

        # Multi-head用に分割: [batch, num_heads, seq_len, head_dim]
        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        # Scaled Dot-Product Attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)

        # Causal Mask適用（上三角を-infにする）
        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
        scores = scores.masked_fill(causal_mask, float('-inf'))

        # Softmax
        attention_weights = F.softmax(scores, dim=-1)

        # Valueとの重み付き和
        out = torch.matmul(attention_weights, V)

        # Headsを結合
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_size)

        # 最終射影
        output = self.proj(out)

        return output, attention_weights


# デモンストレーション
print("=== Causal Self-Attention Demo ===\n")

batch_size = 1
seq_len = 8
embed_size = 64
num_heads = 4

# ダミー入力
x = torch.randn(batch_size, seq_len, embed_size)

# Causal Attention適用
causal_attn = CausalSelfAttention(embed_size, num_heads)
output, attn_weights = causal_attn(x)

print(f"Input shape: {x.shape}")
print(f"Output shape: {output.shape}")
print(f"Attention weights shape: {attn_weights.shape}")

# Causal Maskの可視化
sample_attn = attn_weights[0, 0].detach().numpy()  # 1st batch, 1st head

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# 左: Causal Attention重み
ax1 = axes[0]
sns.heatmap(sample_attn,
            cmap='YlOrRd',
            cbar_kws={'label': 'Attention Weight'},
            ax=ax1,
            annot=True,
            fmt='.3f',
            linewidths=0.5,
            xticklabels=[f't{i+1}' for i in range(seq_len)],
            yticklabels=[f't{i+1}' for i in range(seq_len)])

ax1.set_xlabel('Key Position', fontsize=12, fontweight='bold')
ax1.set_ylabel('Query Position', fontsize=12, fontweight='bold')
ax1.set_title('GPT Causal Attention Weights\n(下三角のみ有効)', fontsize=13, fontweight='bold')

# 右: Causal Mask構造
causal_mask_viz = np.tril(np.ones((seq_len, seq_len)))
ax2 = axes[1]
sns.heatmap(causal_mask_viz,
            cmap='RdYlGn',
            cbar_kws={'label': '1=参照可能, 0=マスク'},
            ax=ax2,
            annot=True,
            fmt='.0f',
            linewidths=0.5,
            xticklabels=[f't{i+1}' for i in range(seq_len)],
            yticklabels=[f't{i+1}' for i in range(seq_len)])

ax2.set_xlabel('Key Position', fontsize=12, fontweight='bold')
ax2.set_ylabel('Query Position', fontsize=12, fontweight='bold')
ax2.set_title('Causal Mask Structure\n(未来のトークンを隠す)', fontsize=13, fontweight='bold')

plt.tight_layout()
plt.show()

print("\n特徴:")
print("✓ 各位置は自分より前（左）のトークンのみを参照")
print("✓ 下三角行列の構造（上三角は0）")
print("✓ 未来の情報を使わないため、逐次生成が可能")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== Causal Self-Attention Demo ===

Input shape: torch.Size([1, 8, 64])
Output shape: torch.Size([1, 8, 64])
Attention weights shape: torch.Size([1, 4, 8, 8])

特徴:
✓ 各位置は自分より前（左）のトークンのみを参照
✓ 下三角行列の構造（上三角は0）
✓ 未来の情報を使わないため、逐次生成が可能
</code></pre>

<hr>

<h2>4.5 GPTによるテキスト生成</h2>

<h3>4.5.1 自己回帰生成の仕組み</h3>

<p>GPTは1トークンずつ逐次的に生成します：</p>

<ol>
<li>プロンプト（入力テキスト）をモデルに入力</li>
<li>次トークンの確率分布を予測</li>
<li>サンプリング戦略で次トークンを選択</li>
<li>選択したトークンを入力に追加</li>
<li>ステップ2〜4を繰り返し</li>
</ol>

<h3>4.5.2 サンプリング戦略</h3>

<table>
<thead>
<tr>
<th>戦略</th>
<th>説明</th>
<th>特徴</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Greedy Decoding</strong></td>
<td>最高確率のトークンを選択</td>
<td>決定的、繰り返しが多い</td>
</tr>
<tr>
<td><strong>Beam Search</strong></td>
<td>複数候補を保持して探索</td>
<td>品質高いが多様性低い</td>
</tr>
<tr>
<td><strong>Temperature Sampling</strong></td>
<td>温度パラメータで確率を調整</td>
<td>T→0で決定的、T→∞でランダム</td>
</tr>
<tr>
<td><strong>Top-k Sampling</strong></td>
<td>確率上位k個からサンプリング</td>
<td>多様性と品質のバランス</td>
</tr>
<tr>
<td><strong>Top-p (Nucleus)</strong></td>
<td>累積確率p以上からサンプリング</td>
<td>動的な語彙サイズ調整</td>
</tr>
</tbody>
</table>

<h3>4.5.3 GPT-2によるテキスト生成実装</h3>

<pre><code class="language-python">from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

print("=== GPT-2 Text Generation Demo ===\n")

# GPT-2モデル読み込み
model_name = 'gpt2'  # 124M parameters
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

model.eval()

# プロンプト
prompt = "Artificial intelligence is transforming the world by"

print(f"Prompt: {prompt}\n")
print("=" * 80)

# 異なるサンプリング戦略での生成
strategies = [
    {
        'name': 'Greedy Decoding',
        'params': {
            'do_sample': False,
            'max_length': 50
        }
    },
    {
        'name': 'Temperature Sampling (T=0.7)',
        'params': {
            'do_sample': True,
            'max_length': 50,
            'temperature': 0.7
        }
    },
    {
        'name': 'Top-k Sampling (k=50)',
        'params': {
            'do_sample': True,
            'max_length': 50,
            'top_k': 50,
            'temperature': 1.0
        }
    },
    {
        'name': 'Top-p Sampling (p=0.9)',
        'params': {
            'do_sample': True,
            'max_length': 50,
            'top_p': 0.9,
            'temperature': 1.0
        }
    }
]

for strategy in strategies:
    # トークナイズ
    inputs = tokenizer(prompt, return_tensors='pt')

    # 生成
    with torch.no_grad():
        outputs = model.generate(
            inputs['input_ids'],
            **strategy['params'],
            pad_token_id=tokenizer.eos_token_id
        )

    # デコード
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print(f"\n{strategy['name']}:")
    print(f"{generated_text}")
    print("-" * 80)
</code></pre>

<p><strong>出力例</strong>：</p>
<pre><code>=== GPT-2 Text Generation Demo ===

Prompt: Artificial intelligence is transforming the world by

================================================================================

Greedy Decoding:
Artificial intelligence is transforming the world by making it easier for people to do things that they would otherwise have to do manually. The most common example is the use of AI to automate tasks such as scheduling, scheduling appointments, and scheduling meetings.

--------------------------------------------------------------------------------

Temperature Sampling (T=0.7):
Artificial intelligence is transforming the world by enabling machines to learn from experience and make decisions without human intervention. From self-driving cars to medical diagnosis systems, AI technologies are revolutionizing industries and improving our daily lives.

--------------------------------------------------------------------------------

Top-k Sampling (k=50):
Artificial intelligence is transforming the world by creating new possibilities in healthcare, education, and entertainment. AI systems can now analyze vast amounts of data, recognize patterns, and provide insights that were previously impossible to obtain.

--------------------------------------------------------------------------------

Top-p Sampling (p=0.9):
Artificial intelligence is transforming the world by automating complex tasks, enhancing decision-making processes, and opening doors to innovations we never thought possible. As AI continues to evolve, its impact on society will only grow stronger.

--------------------------------------------------------------------------------
</code></pre>

<h3>4.5.4 カスタム生成関数の実装</h3>

<pre><code class="language-python">def generate_text_custom(model, tokenizer, prompt, max_length=50,
                        strategy='top_p', temperature=1.0, top_k=50, top_p=0.9):
    """
    カスタムテキスト生成関数

    Args:
        model: GPT-2モデル
        tokenizer: トークナイザー
        prompt: 入力プロンプト
        max_length: 最大生成長
        strategy: 'greedy', 'temperature', 'top_k', 'top_p'
        temperature: 温度パラメータ
        top_k: Top-kサンプリングのk
        top_p: Top-pサンプリングのp

    Returns:
        生成テキスト
    """
    # トークナイズ
    input_ids = tokenizer.encode(prompt, return_tensors='pt')

    # 生成ループ
    for _ in range(max_length):
        with torch.no_grad():
            outputs = model(input_ids)
            logits = outputs.logits

        # 最後のトークンのlogitsを取得
        next_token_logits = logits[0, -1, :]

        # Temperature適用
        if temperature != 1.0:
            next_token_logits = next_token_logits / temperature

        # サンプリング戦略
        if strategy == 'greedy':
            next_token_id = torch.argmax(next_token_logits).unsqueeze(0)

        elif strategy == 'temperature':
            probs = F.softmax(next_token_logits, dim=-1)
            next_token_id = torch.multinomial(probs, num_samples=1)

        elif strategy == 'top_k':
            # Top-kマスキング
            top_k_values, top_k_indices = torch.topk(next_token_logits, top_k)
            next_token_logits_filtered = torch.full_like(next_token_logits, float('-inf'))
            next_token_logits_filtered[top_k_indices] = top_k_values

            probs = F.softmax(next_token_logits_filtered, dim=-1)
            next_token_id = torch.multinomial(probs, num_samples=1)

        elif strategy == 'top_p':
            # Top-pマスキング
            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

            # 累積確率がpを超える位置を見つける
            sorted_indices_to_remove = cumulative_probs > top_p
            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
            sorted_indices_to_remove[0] = 0

            # マスク適用
            next_token_logits_filtered = next_token_logits.clone()
            next_token_logits_filtered[sorted_indices[sorted_indices_to_remove]] = float('-inf')

            probs = F.softmax(next_token_logits_filtered, dim=-1)
            next_token_id = torch.multinomial(probs, num_samples=1)

        # 入力に追加
        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=1)

        # EOSトークンで終了
        if next_token_id.item() == tokenizer.eos_token_id:
            break

    # デコード
    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)
    return generated_text


# カスタム生成関数のテスト
print("\n=== Custom Generation Function Test ===\n")

prompt = "The future of machine learning is"
print(f"Prompt: {prompt}\n")

for strategy in ['greedy', 'temperature', 'top_k', 'top_p']:
    generated = generate_text_custom(
        model, tokenizer, prompt,
        max_length=30,
        strategy=strategy,
        temperature=0.8,
        top_k=40,
        top_p=0.9
    )
    print(f"{strategy.upper()}: {generated}\n")
</code></pre>

<hr>

<h2>4.6 BERT vs GPT：比較と使い分け</h2>

<h3>4.6.1 アーキテクチャの比較</h3>

<div class="mermaid">
graph TB
    subgraph "BERT (Encoder-only)"
        B1[Input: 文全体] --> B2[Token + Segment + Position Embeddings]
        B2 --> B3[Transformer Encoder × 12]
        B3 --> B4[Bidirectional Attention]
        B4 --> B5["[CLS] for Classification<br/>All Tokens for Token-level"]

        style B4 fill:#27ae60,color:#fff
    end

    subgraph "GPT (Decoder-only)"
        G1[Input: プロンプト] --> G2[Token + Position Embeddings]
        G2 --> G3[Transformer Decoder × 12]
        G3 --> G4[Causal Attention]
        G4 --> G5[Next Token Prediction]
        G5 --> G6[Autoregressive Generation]

        style G4 fill:#e74c3c,color:#fff
    end
</div>

<h3>4.6.2 性能比較実験</h3>

<pre><code class="language-python">from transformers import BertModel, GPT2Model, BertTokenizer, GPT2Tokenizer
import torch
import time

print("=== BERT vs GPT Performance Comparison ===\n")

# モデル読み込み
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased')

gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
gpt2_model = GPT2Model.from_pretrained('gpt2')

bert_model.eval()
gpt2_model.eval()

# テストテキスト
text = "Natural language processing is a fascinating field of artificial intelligence."

# BERT処理
bert_inputs = bert_tokenizer(text, return_tensors='pt', padding=True, truncation=True)
start_time = time.time()
with torch.no_grad():
    bert_outputs = bert_model(**bert_inputs)
bert_time = time.time() - start_time

# GPT-2処理
gpt2_inputs = gpt2_tokenizer(text, return_tensors='pt')
start_time = time.time()
with torch.no_grad():
    gpt2_outputs = gpt2_model(**gpt2_inputs)
gpt2_time = time.time() - start_time

# 結果表示
print("Input Text:", text)
print(f"\nBERT:")
print(f"  Model: bert-base-uncased")
print(f"  Parameters: {sum(p.numel() for p in bert_model.parameters()):,}")
print(f"  Input shape: {bert_inputs['input_ids'].shape}")
print(f"  Output shape: {bert_outputs.last_hidden_state.shape}")
print(f"  Processing time: {bert_time*1000:.2f} ms")
print(f"  [CLS] embedding shape: {bert_outputs.pooler_output.shape}")

print(f"\nGPT-2:")
print(f"  Model: gpt2")
print(f"  Parameters: {sum(p.numel() for p in gpt2_model.parameters()):,}")
print(f"  Input shape: {gpt2_inputs['input_ids'].shape}")
print(f"  Output shape: {gpt2_outputs.last_hidden_state.shape}")
print(f"  Processing time: {gpt2_time*1000:.2f} ms")

# Attention可視化比較
print("\n" + "="*80)
print("Attention Pattern Comparison")
print("="*80)

# BERT: すべてのトークンを相互参照可能
print("\nBERT Attention Pattern:")
print("  ✓ Bidirectional - すべてのトークンがすべてのトークンを参照")
print("  ✓ 並列処理可能 - 全トークンを同時に処理")
print("  ✓ 用途: 分類、NER、QA、文エンコーディング")

# GPT: 左側のトークンのみ参照可能
print("\nGPT Attention Pattern:")
print("  ✓ Unidirectional - 各トークンは左側のトークンのみ参照")
print("  ✓ 逐次生成 - 1トークンずつ生成")
print("  ✓ 用途: テキスト生成、対話、補完、翻訳")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== BERT vs GPT Performance Comparison ===

Input Text: Natural language processing is a fascinating field of artificial intelligence.

BERT:
  Model: bert-base-uncased
  Parameters: 109,482,240
  Input shape: torch.Size([1, 14])
  Output shape: torch.Size([1, 14, 768])
  Processing time: 45.23 ms
  [CLS] embedding shape: torch.Size([1, 768])

GPT-2:
  Model: gpt2
  Parameters: 124,439,808
  Input shape: torch.Size([1, 14])
  Output shape: torch.Size([1, 14, 768])
  Processing time: 38.67 ms

================================================================================
Attention Pattern Comparison
================================================================================

BERT Attention Pattern:
  ✓ Bidirectional - すべてのトークンがすべてのトークンを参照
  ✓ 並列処理可能 - 全トークンを同時に処理
  ✓ 用途: 分類、NER、QA、文エンコーディング

GPT Attention Pattern:
  ✓ Unidirectional - 各トークンは左側のトークンのみ参照
  ✓ 逐次生成 - 1トークンずつ生成
  ✓ 用途: テキスト生成、対話、補完、翻訳
</code></pre>

<h3>4.6.3 使い分けガイド</h3>

<table>
<thead>
<tr>
<th>タスク</th>
<th>推奨モデル</th>
<th>理由</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>感情分析</strong></td>
<td>BERT</td>
<td>文全体の文脈理解が必要</td>
</tr>
<tr>
<td><strong>固有表現認識</strong></td>
<td>BERT</td>
<td>各トークンの分類、双方向文脈が有利</td>
</tr>
<tr>
<td><strong>質問応答</strong></td>
<td>BERT</td>
<td>文章中から回答箇所を特定</td>
</tr>
<tr>
<td><strong>文書分類</strong></td>
<td>BERT</td>
<td>[CLS]トークンで文全体をエンコード</td>
</tr>
<tr>
<td><strong>テキスト生成</strong></td>
<td>GPT</td>
<td>自己回帰生成に特化</td>
</tr>
<tr>
<td><strong>対話システム</strong></td>
<td>GPT</td>
<td>応答生成が主タスク</td>
</tr>
<tr>
<td><strong>要約</strong></td>
<td>GPT（or BART）</td>
<td>生成タスク、抽象的要約</td>
</tr>
<tr>
<td><strong>コード生成</strong></td>
<td>GPT（Codex）</td>
<td>逐次的なコード生成</td>
</tr>
<tr>
<td><strong>翻訳</strong></td>
<td>両方可能</td>
<td>BERT→Encoder、GPT→Decoder的に使用</td>
</tr>
</tbody>
</table>

<hr>

<h2>4.7 実践プロジェクト</h2>

<h3>4.7.1 プロジェクト1: BERTによる質問応答システム</h3>

<div class="project-box">
<h4>目標</h4>
<p>SQuAD形式の質問応答システムを構築し、コンテキストから正確な回答を抽出します。</p>

<h4>実装要件</h4>
<ul>
<li>Fine-tunedされたBERTモデルの使用</li>
<li>複数の質問に対する回答抽出</li>
<li>確信度スコアの計算と表示</li>
<li>回答の妥当性検証</li>
</ul>
</div>

<pre><code class="language-python">from transformers import BertForQuestionAnswering, BertTokenizer
import torch

class QuestionAnsweringSystem:
    """BERTベースの質問応答システム"""

    def __init__(self, model_name='bert-large-uncased-whole-word-masking-finetuned-squad'):
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.model = BertForQuestionAnswering.from_pretrained(model_name)
        self.model.eval()

    def answer_question(self, question, context, return_confidence=True):
        """
        質問に対する回答を抽出

        Args:
            question: 質問文
            context: コンテキスト（回答元の文章）
            return_confidence: 確信度を返すかどうか

        Returns:
            answer: 抽出された回答
            confidence: 確信度スコア（return_confidence=Trueの場合）
        """
        # トークナイズ
        inputs = self.tokenizer(
            question,
            context,
            return_tensors='pt',
            truncation=True,
            max_length=512,
            padding=True
        )

        # 推論
        with torch.no_grad():
            outputs = self.model(**inputs)

        # 開始・終了位置の予測
        start_logits = outputs.start_logits
        end_logits = outputs.end_logits

        start_idx = torch.argmax(start_logits)
        end_idx = torch.argmax(end_logits)

        # 回答抽出
        answer_tokens = inputs['input_ids'][0][start_idx:end_idx+1]
        answer = self.tokenizer.decode(answer_tokens, skip_special_tokens=True)

        if return_confidence:
            # 確信度計算
            start_score = torch.softmax(start_logits, dim=1)[0, start_idx].item()
            end_score = torch.softmax(end_logits, dim=1)[0, end_idx].item()
            confidence = (start_score + end_score) / 2

            return answer, confidence
        else:
            return answer

    def batch_answer(self, qa_pairs):
        """
        複数の質問に一括で回答

        Args:
            qa_pairs: [(question, context), ...] のリスト

        Returns:
            results: [(answer, confidence), ...] のリスト
        """
        results = []
        for question, context in qa_pairs:
            answer, confidence = self.answer_question(question, context)
            results.append((answer, confidence))
        return results


# システムのテスト
print("=== Question Answering System Demo ===\n")

qa_system = QuestionAnsweringSystem()

# テストケース
context = """
The Transformer architecture was introduced in the paper "Attention is All You Need"
by Vaswani et al. in 2017. It relies entirely on self-attention mechanisms to compute
representations of input and output sequences without using recurrent or convolutional layers.
The model achieved state-of-the-art results on machine translation tasks and has since become
the foundation for models like BERT and GPT. The architecture consists of an encoder and a decoder,
each composed of multiple identical layers. Each layer has two sub-layers: a multi-head self-attention
mechanism and a position-wise fully connected feed-forward network.
"""

questions = [
    "When was the Transformer introduced?",
    "Who introduced the Transformer?",
    "What does the Transformer rely on?",
    "What are the two main components of the Transformer?",
    "What models are based on the Transformer?"
]

print("Context:")
print(context)
print("\n" + "="*80 + "\n")

for i, question in enumerate(questions, 1):
    answer, confidence = qa_system.answer_question(question, context)

    print(f"Q{i}: {question}")
    print(f"A{i}: {answer}")
    print(f"Confidence: {confidence:.4f}")
    print()

# バッチ処理のデモ
print("="*80)
print("\nBatch Processing Demo:")
print("="*80 + "\n")

qa_pairs = [(q, context) for q in questions]
results = qa_system.batch_answer(qa_pairs)

for (question, _), (answer, conf) in zip(qa_pairs, results):
    print(f"Q: {question}")
    print(f"A: {answer} (Conf: {conf:.4f})\n")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== Question Answering System Demo ===

Context:
The Transformer architecture was introduced in the paper "Attention is All You Need"
by Vaswani et al. in 2017. It relies entirely on self-attention mechanisms to compute
representations of input and output sequences without using recurrent or convolutional layers.
The model achieved state-of-the-art results on machine translation tasks and has since become
the foundation for models like BERT and GPT. The architecture consists of an encoder and a decoder,
each composed of multiple identical layers. Each layer has two sub-layers: a multi-head self-attention
mechanism and a position-wise fully connected feed-forward network.

================================================================================

Q1: When was the Transformer introduced?
A1: 2017
Confidence: 0.9523

Q2: Who introduced the Transformer?
A2: Vaswani et al.
Confidence: 0.8876

Q3: What does the Transformer rely on?
A3: self-attention mechanisms
Confidence: 0.9234

Q4: What are the two main components of the Transformer?
A4: an encoder and a decoder
Confidence: 0.8912

Q5: What models are based on the Transformer?
A5: BERT and GPT
Confidence: 0.9101
</code></pre>

<h3>4.7.2 プロジェクト2: GPTによるテキスト生成アプリ</h3>

<div class="project-box">
<h4>目標</h4>
<p>カスタマイズ可能なテキスト生成システムを構築し、様々な生成戦略を試します。</p>

<h4>実装要件</h4>
<ul>
<li>複数のサンプリング戦略のサポート</li>
<li>生成パラメータの調整機能</li>
<li>プロンプトエンジニアリングの実践</li>
<li>生成品質の評価</li>
</ul>
</div>

<pre><code class="language-python">from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

class TextGenerator:
    """GPT-2ベーステキスト生成システム"""

    def __init__(self, model_name='gpt2-medium'):
        """
        Args:
            model_name: 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'
        """
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        self.model = GPT2LMHeadModel.from_pretrained(model_name)
        self.model.eval()

        # PADトークン設定
        self.tokenizer.pad_token = self.tokenizer.eos_token

    def generate(self, prompt, max_length=100, strategy='top_p',
                num_return_sequences=1, **kwargs):
        """
        テキスト生成

        Args:
            prompt: 入力プロンプト
            max_length: 最大生成長
            strategy: 'greedy', 'beam', 'temperature', 'top_k', 'top_p'
            num_return_sequences: 生成する候補数
            **kwargs: 戦略固有のパラメータ

        Returns:
            生成されたテキストのリスト
        """
        # トークナイズ
        inputs = self.tokenizer(prompt, return_tensors='pt')

        # 戦略に応じたパラメータ設定
        gen_params = {
            'max_length': max_length,
            'num_return_sequences': num_return_sequences,
            'pad_token_id': self.tokenizer.eos_token_id,
            'early_stopping': True
        }

        if strategy == 'greedy':
            gen_params['do_sample'] = False

        elif strategy == 'beam':
            gen_params['num_beams'] = kwargs.get('num_beams', 5)
            gen_params['do_sample'] = False

        elif strategy == 'temperature':
            gen_params['do_sample'] = True
            gen_params['temperature'] = kwargs.get('temperature', 0.7)

        elif strategy == 'top_k':
            gen_params['do_sample'] = True
            gen_params['top_k'] = kwargs.get('top_k', 50)
            gen_params['temperature'] = kwargs.get('temperature', 1.0)

        elif strategy == 'top_p':
            gen_params['do_sample'] = True
            gen_params['top_p'] = kwargs.get('top_p', 0.9)
            gen_params['temperature'] = kwargs.get('temperature', 1.0)

        # 生成
        with torch.no_grad():
            outputs = self.model.generate(inputs['input_ids'], **gen_params)

        # デコード
        generated_texts = [
            self.tokenizer.decode(output, skip_special_tokens=True)
            for output in outputs
        ]

        return generated_texts

    def interactive_generation(self):
        """対話的な生成セッション"""
        print("=== Interactive Text Generation ===")
        print("Type 'quit' to exit\n")

        while True:
            prompt = input("Prompt: ")
            if prompt.lower() == 'quit':
                break

            # 生成設定
            print("\nGeneration Settings:")
            strategy = input("Strategy (greedy/beam/temperature/top_k/top_p) [top_p]: ") or 'top_p'
            max_length = int(input("Max length [100]: ") or 100)

            # 生成
            outputs = self.generate(prompt, max_length=max_length, strategy=strategy)

            print("\n--- Generated Text ---")
            print(outputs[0])
            print("-" * 80 + "\n")


# システムのテスト
print("=== Text Generation System Demo ===\n")

generator = TextGenerator(model_name='gpt2')

# プロンプトテンプレート
prompts = [
    "In the future of artificial intelligence,",
    "The most important breakthrough in deep learning was",
    "Once upon a time in a distant galaxy,"
]

print("Comparing Different Generation Strategies:\n")
print("="*80 + "\n")

for prompt in prompts:
    print(f"Prompt: {prompt}\n")

    strategies = [
        ('greedy', {}),
        ('top_k', {'top_k': 50, 'temperature': 0.8}),
        ('top_p', {'top_p': 0.9, 'temperature': 0.8})
    ]

    for strategy, params in strategies:
        outputs = generator.generate(
            prompt,
            max_length=60,
            strategy=strategy,
            num_return_sequences=1,
            **params
        )

        print(f"{strategy.upper()}:")
        print(f"{outputs[0]}\n")

    print("="*80 + "\n")

# 複数候補生成のデモ
print("\nMultiple Candidates Generation:")
print("="*80 + "\n")

prompt = "The key to successful machine learning is"
outputs = generator.generate(
    prompt,
    max_length=50,
    strategy='top_p',
    num_return_sequences=3,
    top_p=0.9,
    temperature=0.9
)

for i, output in enumerate(outputs, 1):
    print(f"Candidate {i}:")
    print(output)
    print()
</code></pre>

<p><strong>出力例</strong>：</p>
<pre><code>=== Text Generation System Demo ===

Comparing Different Generation Strategies:

================================================================================

Prompt: In the future of artificial intelligence,

GREEDY:
In the future of artificial intelligence, we will be able to create a new kind of AI that can do things that we have never done before. We will be able to build systems that can learn from data and make decisions based on that data.

TOP_K:
In the future of artificial intelligence, machines will become increasingly capable of understanding human language, emotions, and intentions. This will revolutionize how we interact with technology and open new possibilities in healthcare, education, and entertainment.

TOP_P:
In the future of artificial intelligence, we can expect to see breakthroughs in areas such as natural language understanding, computer vision, and autonomous decision-making. These advances will transform industries and create opportunities we haven't yet imagined.

================================================================================
</code></pre>

<hr>

<h2>4.8 まとめと発展トピック</h2>

<h3>本章で学んだこと</h3>

<table>
<thead>
<tr>
<th>トピック</th>
<th>重要ポイント</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>BERT</strong></td>
<td>双方向エンコーダ、MLM+NSP、理解タスクに最適</td>
</tr>
<tr>
<td><strong>GPT</strong></td>
<td>自己回帰生成、Causal Masking、生成タスクに最適</td>
</tr>
<tr>
<td><strong>事前学習</strong></td>
<td>大規模データで学習、Fine-tuningで特化</td>
</tr>
<tr>
<td><strong>使い分け</strong></td>
<td>分類・抽出はBERT、生成はGPT</td>
</tr>
<tr>
<td><strong>実践手法</strong></td>
<td>Hugging Face、サンプリング戦略、QAシステム</td>
</tr>
</tbody>
</table>

<h3>発展トピック</h3>

<details>
<summary><strong>RoBERTa：BERTの改良版</strong></summary>
<p>FacebookによるBERTの改良版。NSPタスクを削除し、動的マスキング、より大規模な訓練データ、長い訓練時間を採用して性能を向上させました。</p>
</details>

<details>
<summary><strong>ALBERT：パラメータ効率化</strong></summary>
<p>パラメータ共有とFactor化により、BERTと同等の性能を少ないパラメータで実現。大規模モデルの訓練を効率化します。</p>
</details>

<details>
<summary><strong>GPT-3.5/4：InstructGPT・ChatGPT</strong></summary>
<p>Instruction TuningとRLHF（人間フィードバックからの強化学習）により、ユーザー指示に従う能力を大幅に向上。対話システムの主流に。</p>
</details>

<details>
<summary><strong>Prompt Engineering</strong></summary>
<p>モデルの性能を最大化するプロンプト設計技術。Few-shot examples、Chain-of-Thought prompting、Role promptingなど。</p>
</details>

<details>
<summary><strong>PEFT (Parameter-Efficient Fine-Tuning)</strong></summary>
<p>LoRA、Adapter、Prefix Tuningなど、全パラメータを更新せずに効率的にFine-tuningする手法。大規模モデル時代の必須技術。</p>
</details>

<h3>演習問題</h3>

<div class="project-box">
<h4>演習 4.1: BERT Fine-tuningによる感情分析</h4>
<p><strong>課題</strong>: IMDBレビューデータセットでBERTをFine-tuningし、感情分析モデルを構築してください。</p>
<p><strong>要件</strong>:</p>
<ul>
<li>データの前処理とトークナイゼーション</li>
<li>BERT-Baseモデルのロードと分類層の追加</li>
<li>訓練ループの実装</li>
<li>精度・F1スコアの評価</li>
</ul>
</div>

<div class="project-box">
<h4>演習 4.2: GPT-2による対話システム</h4>
<p><strong>課題</strong>: GPT-2を使った簡単な対話システムを実装してください。</p>
<p><strong>要件</strong>:</p>
<ul>
<li>ユーザー入力の受付とコンテキスト管理</li>
<li>応答生成（複数のサンプリング戦略）</li>
<li>会話履歴の保持と反映</li>
<li>対話の自然性評価</li>
</ul>
</div>

<div class="project-box">
<h4>演習 4.3: BERT vs GPT性能比較</h4>
<p><strong>課題</strong>: 同じタスクでBERTとGPTの性能を比較してください。</p>
<p><strong>タスク</strong>: 文書分類（ニュース記事カテゴリ分類）</p>
<p><strong>比較項目</strong>:</p>
<ul>
<li>精度、F1スコア</li>
<li>訓練時間</li>
<li>推論速度</li>
<li>メモリ使用量</li>
</ul>
</div>

<div class="project-box">
<h4>演習 4.4: Masked Language Modelingの実装</h4>
<p><strong>課題</strong>: 小規模データセットでMLMを実装し、BERTの事前学習を再現してください。</p>
<p><strong>実装内容</strong>:</p>
<ul>
<li>マスクデータ生成ロジック</li>
<li>MLM損失関数</li>
<li>訓練ループ</li>
<li>マスク予測精度の評価</li>
</ul>
</div>

<div class="project-box">
<h4>演習 4.5: 多言語BERT（mBERT）の活用</h4>
<p><strong>課題</strong>: 多言語BERTを使って、複数言語でのテキスト分類を実装してください。</p>
<p><strong>言語</strong>: 英語、日本語、中国語</p>
<p><strong>タスク</strong>: ニュース記事のトピック分類</p>
</div>

<div class="project-box">
<h4>演習 4.6: GPTによるコード生成</h4>
<p><strong>課題</strong>: GPT-2を使って、自然言語の指示からPythonコードを生成するシステムを構築してください。</p>
<p><strong>要件</strong>:</p>
<ul>
<li>プロンプトテンプレートの設計</li>
<li>コード生成と構文検証</li>
<li>生成品質の評価</li>
</ul>
</div>

<hr>

<h3>次章予告</h3>

<p>第5章では、<strong>Vision Transformer (ViT)</strong>を学びます。TransformerアーキテクチャをComputer Visionに適用し、画像を「トークン」として扱う革新的なアプローチを探ります。</p>

<blockquote>
<p><strong>次章のトピック</strong>:<br>
・Vision Transformerのアーキテクチャ<br>
・画像パッチのトークン化<br>
・Position Embeddingsの2D拡張<br>
・CNNとの性能比較<br>
・Pre-training戦略（ImageNet-21k）<br>
・実装：ViTによる画像分類<br>
・応用：Object Detection、Segmentation</p>
</blockquote>

        <div class="navigation">
            <a href="chapter3-advanced-topics.html" class="nav-button">← 第3章: 発展トピック</a>
            <a href="chapter5-vision-transformer.html" class="nav-button">第5章: Vision Transformer →</a>
        </div>

    </main>

    <footer>
        <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
        <p>第4章：BERT・GPT | Transformer入門シリーズ</p>
    </footer>

</body>
</html>
