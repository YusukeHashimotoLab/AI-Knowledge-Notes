<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Á¨¨2Á´†ÔºöTransformer„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Á¨¨2Á´†ÔºöTransformer„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£ÔºàTransformer ArchitectureÔºâ</h1>
            <p class="subtitle">Encoder-Decoder„ÅÆÂÆåÂÖ®ÁêÜËß£„Å®PyTorch„Å´„Çà„ÇãÂÆüË£Ö</p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 30-35ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ‰∏≠Á¥ö„Äú‰∏äÁ¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 12ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 5Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Â≠¶ÁøíÁõÆÊ®ô</h2>
<p>„Åì„ÅÆÁ´†„ÇíË™≠„ÇÄ„Åì„Å®„Åß„ÄÅ‰ª•‰∏ã„ÇíÁøíÂæó„Åß„Åç„Åæ„ÅôÔºö</p>
<ul>
<li>‚úÖ Transformer„ÅÆÂÖ®‰Ωì„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£ÔºàEncoder-DecoderÊßãÈÄ†Ôºâ„ÇíÁêÜËß£„Åô„Çã</li>
<li>‚úÖ Encoder„ÅÆÊßãÊàêË¶ÅÁ¥†ÔºàMulti-Head Attention„ÄÅFFN„ÄÅLayer Norm„ÄÅResidualÔºâ„ÇíË™¨Êòé„Åß„Åç„Çã</li>
<li>‚úÖ Decoder„ÅÆÁâπÂæ¥ÔºàMasked Self-Attention„ÄÅCross-AttentionÔºâ„ÇíÁêÜËß£„Åô„Çã</li>
<li>‚úÖ PyTorch„ÅßTransformer„ÇíÂÆåÂÖ®ÂÆüË£Ö„Åß„Åç„Çã</li>
<li>‚úÖ Ëá™Â∑±ÂõûÂ∏∞ÁîüÊàê„ÅÆ„É°„Ç´„Éã„Ç∫„É†„ÇíÁêÜËß£„Åô„Çã</li>
<li>‚úÖ ÂÆüË∑µÁöÑ„Å™Ê©üÊ¢∞ÁøªË®≥„Ç∑„Çπ„ÉÜ„É†„ÇíÊßãÁØâ„Åß„Åç„Çã</li>
</ul>

<hr>

<h2>2.1 TransformerÂÖ®‰ΩìÂÉè</h2>

<h3>„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÅÆÊ¶ÇË¶Å</h3>

<p><strong>Transformer</strong>„ÅØ„ÄÅ"Attention is All You Need" (Vaswani et al., 2017)„ÅßÊèêÊ°à„Åï„Çå„ÅüÈù©Êñ∞ÁöÑ„Å™„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„Åß„ÄÅRNN„ÉªCNN„Çí‰Ωø„Çè„Åö„ÄÅ<strong>AttentionÊ©üÊßã„ÅÆ„Åø</strong>„ÅßÁ≥ªÂàóÂ§âÊèõ„ÇíÂÆüÁèæ„Åó„Åæ„Åô„ÄÇ</p>

<div class="mermaid">
graph TB
    Input["ÂÖ•ÂäõÁ≥ªÂàó<br/>(Source)"] --> Encoder["Encoder<br/>(NÂ±§„Çπ„Çø„ÉÉ„ÇØ)"]
    Encoder --> Memory["„Ç®„É≥„Ç≥„Éº„ÉâÊ∏à„ÅøË°®Áèæ<br/>(Memory)"]
    Memory --> Decoder["Decoder<br/>(NÂ±§„Çπ„Çø„ÉÉ„ÇØ)"]
    Target["ÁõÆÊ®ôÁ≥ªÂàó<br/>(Target)"] --> Decoder
    Decoder --> Output["Âá∫ÂäõÁ≥ªÂàó<br/>(Prediction)"]

    style Encoder fill:#b3e5fc
    style Decoder fill:#ffab91
    style Memory fill:#fff9c4
</div>

<h3>‰∏ªË¶Å„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà</h3>

<table>
<thead>
<tr>
<th>„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà</th>
<th>ÂΩπÂâ≤</th>
<th>ÁâπÂæ¥</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Encoder</strong></td>
<td>ÂÖ•ÂäõÁ≥ªÂàó„ÇíÊñáËÑàË°®Áèæ„Å´Â§âÊèõ</td>
<td>6Â±§„Çπ„Çø„ÉÉ„ÇØ„ÄÅ‰∏¶ÂàóÂá¶ÁêÜÂèØËÉΩ</td>
</tr>
<tr>
<td><strong>Decoder</strong></td>
<td>„Ç®„É≥„Ç≥„Éº„ÉâË°®Áèæ„Åã„ÇâÂá∫ÂäõÁ≥ªÂàó„ÇíÁîüÊàê</td>
<td>6Â±§„Çπ„Çø„ÉÉ„ÇØ„ÄÅËá™Â∑±ÂõûÂ∏∞ÁîüÊàê</td>
</tr>
<tr>
<td><strong>Multi-Head Attention</strong></td>
<td>Ë§áÊï∞„ÅÆË¶≥ÁÇπ„Åã„Çâ‰æùÂ≠òÈñ¢‰øÇ„ÇíÊçâ„Åà„Çã</td>
<td>8„Éò„ÉÉ„Éâ‰∏¶ÂàóÂÆüË°å</td>
</tr>
<tr>
<td><strong>Feed-Forward Network</strong></td>
<td>ÂêÑ‰ΩçÁΩÆ„ÇíÁã¨Á´ã„Å´Â§âÊèõ</td>
<td>2Â±§MLPÔºàReLUÊ¥ªÊÄßÂåñÔºâ</td>
</tr>
<tr>
<td><strong>Positional Encoding</strong></td>
<td>‰ΩçÁΩÆÊÉÖÂ†±„ÇíÊ≥®ÂÖ•</td>
<td>Sin/CosÈñ¢Êï∞„Éô„Éº„Çπ</td>
</tr>
<tr>
<td><strong>Layer Normalization</strong></td>
<td>Â≠¶Áøí„ÇíÂÆâÂÆöÂåñ</td>
<td>ÂêÑ„Çµ„Éñ„É¨„Ç§„É§„ÉºÂæå„Å´ÈÅ©Áî®</td>
</tr>
<tr>
<td><strong>Residual Connection</strong></td>
<td>ÂãæÈÖçÊµÅ„ÇíÊîπÂñÑ</td>
<td>Skip connection</td>
</tr>
</tbody>
</table>

<h3>RNN„Å®„ÅÆÈÅï„ÅÑ</h3>

<table>
<thead>
<tr>
<th>È†ÖÁõÆ</th>
<th>RNN/LSTM</th>
<th>Transformer</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Âá¶ÁêÜÊñπÂºè</strong></td>
<td>ÈÄêÊ¨°ÁöÑÔºàsequentialÔºâ</td>
<td>‰∏¶ÂàóÁöÑÔºàparallelÔºâ</td>
</tr>
<tr>
<td><strong>Èï∑Êúü‰æùÂ≠ò</strong></td>
<td>Ë∑ùÈõ¢„ÅåÈõ¢„Çå„Çã„Å®Âº±„Åæ„Çã</td>
<td>Ë∑ùÈõ¢„Å´Èñ¢‰øÇ„Å™„ÅèÁõ¥Êé•Êé•Á∂ö</td>
</tr>
<tr>
<td><strong>Ë®àÁÆóË§áÈõëÂ∫¶</strong></td>
<td>$O(n)$ÊôÇÈñì</td>
<td>$O(1)$ÊôÇÈñìÔºà‰∏¶ÂàóÂåñÂèØËÉΩÔºâ</td>
</tr>
<tr>
<td><strong>„É°„É¢„É™</strong></td>
<td>Èö†„ÇåÁä∂ÊÖã„ÅßÂúßÁ∏Æ</td>
<td>ÂÖ®‰ΩçÁΩÆ„ÅÆÊÉÖÂ†±„Çí‰øùÊåÅ</td>
</tr>
<tr>
<td><strong>Ë®ìÁ∑¥ÈÄüÂ∫¶</strong></td>
<td>ÈÅÖ„ÅÑ</td>
<td>È´òÈÄüÔºàGPUÊ¥ªÁî®Ôºâ</td>
</tr>
</tbody>
</table>

<blockquote>
<p>„ÄåTransformer„ÅØ‰∏¶ÂàóÂá¶ÁêÜ„Å´„Çà„Çä„ÄÅRNN„Çà„Çä10ÂÄç‰ª•‰∏äÈ´òÈÄü„Å´Ë®ìÁ∑¥„Åß„Åç„Åæ„ÅôÔºÅ„Äç</p>
</blockquote>

<h3>Âü∫Êú¨ÊßãÈÄ†„ÅÆÂèØË¶ñÂåñ</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import math

# Transformer„ÅÆÂü∫Êú¨„Éë„É©„É°„Éº„Çø
print("=== Transformer„ÅÆÂü∫Êú¨Ë®≠ÂÆö ===")
d_model = 512        # „É¢„Éá„É´„ÅÆÊ¨°ÂÖÉÊï∞
nhead = 8            # Attention„Éò„ÉÉ„ÉâÊï∞
num_layers = 6       # Encoder/Decoder„ÅÆÂ±§Êï∞
d_ff = 2048          # Feed-Forward„ÅÆÈö†„ÇåÂ±§„Çµ„Ç§„Ç∫
dropout = 0.1        # DropoutÁéá
max_len = 5000       # ÊúÄÂ§ßÁ≥ªÂàóÈï∑

print(f"„É¢„Éá„É´Ê¨°ÂÖÉ: d_model = {d_model}")
print(f"Attention„Éò„ÉÉ„ÉâÊï∞: nhead = {nhead}")
print(f"ÂêÑ„Éò„ÉÉ„Éâ„ÅÆÊ¨°ÂÖÉ: d_k = d_v = {d_model // nhead}")
print(f"Encoder/DecoderÂ±§Êï∞: {num_layers}")
print(f"FFNÈö†„ÇåÂ±§„Çµ„Ç§„Ç∫: {d_ff}")
print(f"Á∑è„Éë„É©„É°„Éº„ÇøÊï∞ÔºàÊ¶ÇÁÆóÔºâ: {(num_layers * 2) * (4 * d_model**2 + 2 * d_model * d_ff):,}")

# ÂÖ•Âá∫Âäõ„Çµ„Ç§„Ç∫„ÅÆ‰æã
batch_size = 32
src_len = 20  # „ÇΩ„Éº„ÇπÁ≥ªÂàóÈï∑
tgt_len = 15  # „Çø„Éº„Ç≤„ÉÉ„ÉàÁ≥ªÂàóÈï∑

print(f"\n=== ÂÖ•Âá∫Âäõ‰æã ===")
print(f"ÂÖ•ÂäõÔºà„ÇΩ„Éº„ÇπÔºâ: ({batch_size}, {src_len}, {d_model})")
print(f"ÂÖ•ÂäõÔºà„Çø„Éº„Ç≤„ÉÉ„ÉàÔºâ: ({batch_size}, {tgt_len}, {d_model})")
print(f"Âá∫Âäõ: ({batch_size}, {tgt_len}, {d_model})")
</code></pre>

<hr>

<h2>2.2 EncoderÊßãÈÄ†</h2>

<h3>Encoder„ÅÆÂΩπÂâ≤</h3>

<p>Encoder„ÅØÂÖ•ÂäõÁ≥ªÂàó„Çí„ÄÅÂêÑ‰ΩçÁΩÆ„ÅÆÊñáËÑà„ÇíËÄÉÊÖÆ„Åó„ÅüÈ´òÊ¨°ÂÖÉË°®Áèæ„Å´Â§âÊèõ„Åó„Åæ„Åô„ÄÇNÂ±§ÔºàÈÄöÂ∏∏6Â±§Ôºâ„ÅÆEncoderLayer„Çí„Çπ„Çø„ÉÉ„ÇØ„Åó„Åæ„Åô„ÄÇ</p>

<div class="mermaid">
graph TB
    Input["ÂÖ•ÂäõÂüã„ÇÅËæº„Åø + ‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞"] --> E1["Encoder Layer 1"]
    E1 --> E2["Encoder Layer 2"]
    E2 --> E3["..."]
    E3 --> EN["Encoder Layer N"]
    EN --> Output["„Ç®„É≥„Ç≥„Éº„ÉâÊ∏à„ÅøË°®Áèæ"]

    style Input fill:#e1f5ff
    style Output fill:#b3e5fc
</div>

<h3>EncoderLayer„ÅÆÊßãÈÄ†</h3>

<p>ÂêÑEncoderLayer„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆ2„Å§„ÅÆ„Çµ„Éñ„É¨„Ç§„É§„Éº„ÅßÊßãÊàê„Åï„Çå„Åæ„ÅôÔºö</p>

<ol>
<li><strong>Multi-Head Self-Attention</strong>ÔºöÂÖ•ÂäõÁ≥ªÂàóÂÜÖ„ÅÆ‰æùÂ≠òÈñ¢‰øÇ„ÇíÊçâ„Åà„Çã</li>
<li><strong>Position-wise Feed-Forward Network</strong>ÔºöÂêÑ‰ΩçÁΩÆ„ÇíÁã¨Á´ã„Å´Â§âÊèõ</li>
</ol>

<p>ÂêÑ„Çµ„Éñ„É¨„Ç§„É§„Éº„Å´„ÅØ„ÄÅ<strong>Residual Connection</strong>„Å®<strong>Layer Normalization</strong>„ÅåÈÅ©Áî®„Åï„Çå„Åæ„Åô„ÄÇ</p>

<div class="mermaid">
graph TB
    X["ÂÖ•Âäõ x"] --> MHA["Multi-Head<br/>Self-Attention"]
    MHA --> Add1["Add & Norm"]
    X --> Add1
    Add1 --> FFN["Feed-Forward<br/>Network"]
    Add1 --> Add2["Add & Norm"]
    FFN --> Add2
    Add2 --> Y["Âá∫Âäõ y"]

    style MHA fill:#b3e5fc
    style FFN fill:#ffccbc
    style Add1 fill:#c5e1a5
    style Add2 fill:#c5e1a5
</div>

<h3>Multi-Head Attention„ÅÆÊï∞Âºè</h3>

<p>Multi-Head Attention„ÅØ„ÄÅË§áÊï∞„ÅÆÁï∞„Å™„ÇãË°®ÁèæÈÉ®ÂàÜÁ©∫Èñì„Åß‰∏¶Âàó„Å´Attention„ÇíË®àÁÆó„Åó„Åæ„ÅôÔºö</p>

$$
\begin{align}
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O \\
\text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\
\text{Attention}(Q, K, V) &= \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{align}
$$

<p>„Åì„Åì„ÅßÔºö</p>
<ul>
<li>$h$Ôºö„Éò„ÉÉ„ÉâÊï∞ÔºàÈÄöÂ∏∏8Ôºâ</li>
<li>$d_k = d_v = d_{\text{model}} / h$ÔºöÂêÑ„Éò„ÉÉ„Éâ„ÅÆÊ¨°ÂÖÉ</li>
<li>$W_i^Q, W_i^K, W_i^V, W^O$ÔºöÂ≠¶ÁøíÂèØËÉΩ„Å™Â∞ÑÂΩ±Ë°åÂàó</li>
</ul>

<h3>EncoderLayer„ÅÆÂÆüË£Ö</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadAttention(nn.Module):
    """Multi-Head AttentionÊ©üÊßã"""
    def __init__(self, d_model, nhead, dropout=0.1):
        super(MultiHeadAttention, self).__init__()
        assert d_model % nhead == 0, "d_model must be divisible by nhead"

        self.d_model = d_model
        self.nhead = nhead
        self.d_k = d_model // nhead

        # Q, K, V „ÅÆÁ∑öÂΩ¢Â§âÊèõ
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)

        # Âá∫Âäõ„ÅÆÁ∑öÂΩ¢Â§âÊèõ
        self.W_o = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(dropout)

    def split_heads(self, x):
        """(batch, seq_len, d_model) -> (batch, nhead, seq_len, d_k)"""
        batch_size, seq_len, d_model = x.size()
        return x.view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)

    def combine_heads(self, x):
        """(batch, nhead, seq_len, d_k) -> (batch, seq_len, d_model)"""
        batch_size, nhead, seq_len, d_k = x.size()
        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)

    def forward(self, query, key, value, mask=None):
        """
        query, key, value: (batch, seq_len, d_model)
        mask: (batch, 1, seq_len) or (batch, seq_len, seq_len)
        """
        # Á∑öÂΩ¢Â§âÊèõ
        Q = self.W_q(query)  # (batch, seq_len, d_model)
        K = self.W_k(key)
        V = self.W_v(value)

        # „Éò„ÉÉ„Éâ„Å´ÂàÜÂâ≤
        Q = self.split_heads(Q)  # (batch, nhead, seq_len, d_k)
        K = self.split_heads(K)
        V = self.split_heads(V)

        # Scaled Dot-Product Attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        # scores: (batch, nhead, seq_len, seq_len)

        # „Éû„Çπ„ÇØÈÅ©Áî®
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # Softmax
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Value„Å®„ÅÆÁ©ç
        attn_output = torch.matmul(attn_weights, V)
        # attn_output: (batch, nhead, seq_len, d_k)

        # „Éò„ÉÉ„Éâ„ÇíÁµêÂêà
        attn_output = self.combine_heads(attn_output)
        # attn_output: (batch, seq_len, d_model)

        # Âá∫ÂäõÁ∑öÂΩ¢Â§âÊèõ
        output = self.W_o(attn_output)

        return output, attn_weights


class PositionwiseFeedForward(nn.Module):
    """Position-wise Feed-Forward Network"""
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # x: (batch, seq_len, d_model)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x


class EncoderLayer(nn.Module):
    """Transformer Encoder Layer"""
    def __init__(self, d_model, nhead, d_ff, dropout=0.1):
        super(EncoderLayer, self).__init__()

        # Multi-Head Self-Attention
        self.self_attn = MultiHeadAttention(d_model, nhead, dropout)

        # Feed-Forward Network
        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)

        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        # Dropout
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        """
        x: (batch, seq_len, d_model)
        mask: (batch, 1, seq_len) - „Éë„Éá„Ç£„É≥„Ç∞„Éû„Çπ„ÇØ
        """
        # Self-Attention + Residual + Norm
        attn_output, attn_weights = self.self_attn(x, x, x, mask)
        x = x + self.dropout1(attn_output)  # Residual connection
        x = self.norm1(x)  # Layer normalization

        # Feed-Forward + Residual + Norm
        ffn_output = self.ffn(x)
        x = x + self.dropout2(ffn_output)  # Residual connection
        x = self.norm2(x)  # Layer normalization

        return x, attn_weights


# Âãï‰ΩúÁ¢∫Ë™ç
print("=== EncoderLayer„ÅÆÂãï‰ΩúÁ¢∫Ë™ç ===")
d_model = 512
nhead = 8
d_ff = 2048
batch_size = 32
seq_len = 20

encoder_layer = EncoderLayer(d_model, nhead, d_ff)
x = torch.randn(batch_size, seq_len, d_model)

output, attn_weights = encoder_layer(x)

print(f"ÂÖ•Âäõ: {x.shape}")
print(f"Âá∫Âäõ: {output.shape}")
print(f"AttentionÈáç„Åø: {attn_weights.shape}")
print("‚Üí ÂÖ•Âäõ„Å®Âá∫Âäõ„ÅÆ„Çµ„Ç§„Ç∫„ÅåÂêå„ÅòÔºàÊÆãÂ∑ÆÊé•Á∂ö„ÅÆ„Åü„ÇÅÔºâ")

# „Éë„É©„É°„Éº„ÇøÊï∞
total_params = sum(p.numel() for p in encoder_layer.parameters())
print(f"\nEncoderLayer „Éë„É©„É°„Éº„ÇøÊï∞: {total_params:,}")
</code></pre>

<h3>ÂÆåÂÖ®„Å™Encoder„ÅÆÂÆüË£Ö</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    """‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞ÔºàSin/CosÔºâ"""
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(dropout)

        # ‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞Ë°åÂàó„Çí‰ΩúÊàê
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                           -(math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        x: (batch, seq_len, d_model)
        """
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)


class TransformerEncoder(nn.Module):
    """ÂÆåÂÖ®„Å™Transformer Encoder"""
    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6,
                 d_ff=2048, dropout=0.1, max_len=5000):
        super(TransformerEncoder, self).__init__()

        self.d_model = d_model

        # ÂçòË™ûÂüã„ÇÅËæº„Åø
        self.embedding = nn.Embedding(vocab_size, d_model)

        # ‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞
        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)

        # EncoderLayer„Çí„Çπ„Çø„ÉÉ„ÇØ
        self.layers = nn.ModuleList([
            EncoderLayer(d_model, nhead, d_ff, dropout)
            for _ in range(num_layers)
        ])

        self.dropout = nn.Dropout(dropout)

    def forward(self, src, src_mask=None):
        """
        src: (batch, src_len) - „Éà„Éº„ÇØ„É≥ID
        src_mask: (batch, 1, src_len) - „Éë„Éá„Ç£„É≥„Ç∞„Éû„Çπ„ÇØ
        """
        # Âüã„ÇÅËæº„Åø + „Çπ„Ç±„Éº„É™„É≥„Ç∞
        x = self.embedding(src) * math.sqrt(self.d_model)

        # ‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞ËøΩÂä†
        x = self.pos_encoding(x)

        # ÂêÑEncoderLayer„ÇíÈÄöÈÅé
        attn_weights_list = []
        for layer in self.layers:
            x, attn_weights = layer(x, src_mask)
            attn_weights_list.append(attn_weights)

        return x, attn_weights_list


# Âãï‰ΩúÁ¢∫Ë™ç
print("\n=== ÂÆåÂÖ®„Å™Encoder„ÅÆÂãï‰ΩúÁ¢∫Ë™ç ===")
vocab_size = 10000
encoder = TransformerEncoder(vocab_size, d_model=512, nhead=8, num_layers=6)

# „ÉÄ„Éü„Éº„Éá„Éº„Çø
batch_size = 16
src_len = 25
src = torch.randint(0, vocab_size, (batch_size, src_len))

# „Éë„Éá„Ç£„É≥„Ç∞„Éû„Çπ„ÇØ‰ΩúÊàêÔºà‰æãÔºöÊúÄÂæå„ÅÆ5„Éà„Éº„ÇØ„É≥„Åå„Éë„Éá„Ç£„É≥„Ç∞Ôºâ
src_mask = torch.ones(batch_size, 1, src_len)
src_mask[:, :, -5:] = 0

# EncoderÂÆüË°å
encoder_output, attn_weights_list = encoder(src, src_mask)

print(f"ÂÖ•Âäõ„Éà„Éº„ÇØ„É≥: {src.shape}")
print(f"EncoderÂá∫Âäõ: {encoder_output.shape}")
print(f"AttentionÈáç„Åø„ÅÆÊï∞: {len(attn_weights_list)} (Â±§„Åî„Å®)")
print(f"ÂêÑAttentionÈáç„Åø: {attn_weights_list[0].shape}")

# „Éë„É©„É°„Éº„ÇøÊï∞
total_params = sum(p.numel() for p in encoder.parameters())
print(f"\nÁ∑è„Éë„É©„É°„Éº„ÇøÊï∞: {total_params:,}")
</code></pre>

<h3>Layer Normalization„Å®Residual Connection„ÅÆÈáçË¶ÅÊÄß</h3>

<pre><code class="language-python">import torch
import torch.nn as nn

# Layer Normalization„ÅÆÂäπÊûú
print("=== Layer Normalization„ÅÆÂäπÊûú ===")

x = torch.randn(32, 20, 512)  # (batch, seq_len, d_model)

# Layer NormalizationÂâç
print(f"Ê≠£Ë¶èÂåñÂâç - Âπ≥Âùá: {x.mean():.4f}, Ê®ôÊ∫ñÂÅèÂ∑Æ: {x.std():.4f}")

layer_norm = nn.LayerNorm(512)
x_normalized = layer_norm(x)

# Layer NormalizationÂæå
print(f"Ê≠£Ë¶èÂåñÂæå - Âπ≥Âùá: {x_normalized.mean():.4f}, Ê®ôÊ∫ñÂÅèÂ∑Æ: {x_normalized.std():.4f}")
print("‚Üí ÂêÑ„Çµ„É≥„Éó„É´„Éª‰ΩçÁΩÆ„ÅßÂπ≥Âùá0„ÄÅÊ®ôÊ∫ñÂÅèÂ∑Æ1„Å´Ê≠£Ë¶èÂåñ")

# Residual Connection„ÅÆÂäπÊûú
print("\n=== Residual Connection„ÅÆÂäπÊûú ===")

class WithoutResidual(nn.Module):
    def __init__(self, d_model, num_layers):
        super().__init__()
        self.layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(num_layers)])

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)  # ResidualÊé•Á∂ö„Å™„Åó
        return x

class WithResidual(nn.Module):
    def __init__(self, d_model, num_layers):
        super().__init__()
        self.layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(num_layers)])

    def forward(self, x):
        for layer in self.layers:
            x = x + layer(x)  # ResidualÊé•Á∂ö„ÅÇ„Çä
        return x

# ÂãæÈÖçÊµÅ„ÇíÊØîËºÉ
model_without = WithoutResidual(d_model=512, num_layers=10)
model_with = WithResidual(d_model=512, num_layers=10)

x = torch.randn(1, 512, requires_grad=True)

# Forward + Backward
out_without = model_without(x)
out_without.sum().backward()
grad_without = x.grad.norm().item()

x.grad = None
out_with = model_with(x)
out_with.sum().backward()
grad_with = x.grad.norm().item()

print(f"ResidualÊé•Á∂ö„Å™„Åó - ÂãæÈÖç„Éé„É´„É†: {grad_without:.6f}")
print(f"ResidualÊé•Á∂ö„ÅÇ„Çä - ÂãæÈÖç„Éé„É´„É†: {grad_with:.6f}")
print("‚Üí ResidualÊé•Á∂ö„Å´„Çà„ÇäÂãæÈÖç„ÅåÊ∂àÂ§±„Åõ„Åö„ÄÅÊ∑±„ÅÑÂ±§„Åß„ÇÇÂ≠¶ÁøíÂèØËÉΩ")
</code></pre>

<hr>

<h2>2.3 DecoderÊßãÈÄ†</h2>

<h3>Decoder„ÅÆÂΩπÂâ≤</h3>

<p>Decoder„ÅØ„ÄÅEncoder„ÅÆÂá∫ÂäõÔºà„É°„É¢„É™Ôºâ„Å®Êó¢„Å´ÁîüÊàê„Åó„Åü„Éà„Éº„ÇØ„É≥„Åã„Çâ„ÄÅÊ¨°„ÅÆ„Éà„Éº„ÇØ„É≥„Çí<strong>Ëá™Â∑±ÂõûÂ∏∞ÁöÑ</strong>„Å´ÁîüÊàê„Åó„Åæ„Åô„ÄÇ</p>

<div class="mermaid">
graph TB
    Target["„Çø„Éº„Ç≤„ÉÉ„ÉàÁ≥ªÂàó<br/>(„Ç∑„Éï„ÉàÊ∏à„Åø)"] --> D1["Decoder Layer 1"]
    Memory["EncoderÂá∫Âäõ<br/>(Memory)"] --> D1
    D1 --> D2["Decoder Layer 2"]
    Memory --> D2
    D2 --> D3["..."]
    Memory --> D3
    D3 --> DN["Decoder Layer N"]
    Memory --> DN
    DN --> Output["Âá∫Âäõ<br/>(Ê¨°„Éà„Éº„ÇØ„É≥‰∫àÊ∏¨)"]

    style Target fill:#e1f5ff
    style Memory fill:#fff9c4
    style Output fill:#ffab91
</div>

<h3>DecoderLayer„ÅÆÊßãÈÄ†</h3>

<p>ÂêÑDecoderLayer„ÅØ„ÄÅ<strong>3„Å§„ÅÆ„Çµ„Éñ„É¨„Ç§„É§„Éº</strong>„ÅßÊßãÊàê„Åï„Çå„Åæ„ÅôÔºö</p>

<ol>
<li><strong>Masked Multi-Head Self-Attention</strong>ÔºöÊú™Êù•„ÅÆ„Éà„Éº„ÇØ„É≥„ÇíË¶ã„Å™„ÅÑ„Çà„ÅÜ„Éû„Çπ„ÇØ</li>
<li><strong>Cross-Attention</strong>ÔºöEncoder„ÅÆÂá∫ÂäõÔºà„É°„É¢„É™Ôºâ„ÇíÂèÇÁÖß</li>
<li><strong>Position-wise Feed-Forward Network</strong>ÔºöÂêÑ‰ΩçÁΩÆ„ÇíÁã¨Á´ã„Å´Â§âÊèõ</li>
</ol>

<div class="mermaid">
graph TB
    X["ÂÖ•Âäõ x"] --> MMHA["Masked Multi-Head<br/>Self-Attention"]
    MMHA --> Add1["Add & Norm"]
    X --> Add1

    Add1 --> CA["Cross-Attention<br/>(EncoderÂá∫ÂäõÂèÇÁÖß)"]
    Memory["Encoder Memory"] --> CA
    CA --> Add2["Add & Norm"]
    Add1 --> Add2

    Add2 --> FFN["Feed-Forward<br/>Network"]
    FFN --> Add3["Add & Norm"]
    Add2 --> Add3
    Add3 --> Y["Âá∫Âäõ y"]

    style MMHA fill:#ffab91
    style CA fill:#ce93d8
    style FFN fill:#ffccbc
    style Memory fill:#fff9c4
</div>

<h3>Masked Self-Attention„ÅÆÈáçË¶ÅÊÄß</h3>

<p><strong>Causal MaskingÔºàÂõ†Êûú„Éû„Çπ„ÇØÔºâ</strong>„Å´„Çà„Çä„ÄÅ‰ΩçÁΩÆ$i$„ÅØ‰ΩçÁΩÆ$i$‰ª•Ââç„ÅÆ„Éà„Éº„ÇØ„É≥„ÅÆ„Åø„ÇíÂèÇÁÖß„Åß„Åç„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅË®ìÁ∑¥ÊôÇ„Åß„ÇÇÊé®Ë´ñÊôÇ„Å®Âêå„ÅòËá™Â∑±ÂõûÂ∏∞ÁöÑ„Å™Êù°‰ª∂„Çí‰øù„Å°„Åæ„Åô„ÄÇ</p>

$$
\text{Mask}_{ij} =
\begin{cases}
0 & \text{if } i < j \text{ (Êú™Êù•„ÅÆ„Éà„Éº„ÇØ„É≥)} \\
1 & \text{if } i \geq j \text{ (ÈÅéÂéª„ÅÆ„Éà„Éº„ÇØ„É≥)}
\end{cases}
$$

<h3>DecoderLayer„ÅÆÂÆüË£Ö</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class DecoderLayer(nn.Module):
    """Transformer Decoder Layer"""
    def __init__(self, d_model, nhead, d_ff, dropout=0.1):
        super(DecoderLayer, self).__init__()

        # 1. Masked Self-Attention
        self.self_attn = MultiHeadAttention(d_model, nhead, dropout)

        # 2. Cross-AttentionÔºàEncoder„ÅÆÂá∫Âäõ„ÇíÂèÇÁÖßÔºâ
        self.cross_attn = MultiHeadAttention(d_model, nhead, dropout)

        # 3. Feed-Forward Network
        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)

        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)

        # Dropout
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

    def forward(self, x, memory, tgt_mask=None, memory_mask=None):
        """
        x: (batch, tgt_len, d_model) - „Çø„Éº„Ç≤„ÉÉ„ÉàÁ≥ªÂàó
        memory: (batch, src_len, d_model) - Encoder„ÅÆÂá∫Âäõ
        tgt_mask: (batch, tgt_len, tgt_len) - Âõ†Êûú„Éû„Çπ„ÇØ
        memory_mask: (batch, 1, src_len) - „Éë„Éá„Ç£„É≥„Ç∞„Éû„Çπ„ÇØ
        """
        # 1. Masked Self-Attention + Residual + Norm
        self_attn_output, self_attn_weights = self.self_attn(x, x, x, tgt_mask)
        x = x + self.dropout1(self_attn_output)
        x = self.norm1(x)

        # 2. Cross-Attention + Residual + Norm
        # Query: Decoder„ÅÆÂá∫Âäõ, Key/Value: Encoder„ÅÆÂá∫Âäõ
        cross_attn_output, cross_attn_weights = self.cross_attn(x, memory, memory, memory_mask)
        x = x + self.dropout2(cross_attn_output)
        x = self.norm2(x)

        # 3. Feed-Forward + Residual + Norm
        ffn_output = self.ffn(x)
        x = x + self.dropout3(ffn_output)
        x = self.norm3(x)

        return x, self_attn_weights, cross_attn_weights


# Âãï‰ΩúÁ¢∫Ë™ç
print("=== DecoderLayer„ÅÆÂãï‰ΩúÁ¢∫Ë™ç ===")
d_model = 512
nhead = 8
d_ff = 2048
batch_size = 32
tgt_len = 15
src_len = 20

decoder_layer = DecoderLayer(d_model, nhead, d_ff)

# „ÉÄ„Éü„Éº„Éá„Éº„Çø
tgt = torch.randn(batch_size, tgt_len, d_model)
memory = torch.randn(batch_size, src_len, d_model)

# Âõ†Êûú„Éû„Çπ„ÇØ‰ΩúÊàê
def create_causal_mask(seq_len):
    """‰∏ã‰∏âËßíË°åÂàóÔºàÊú™Êù•„Çí„Éû„Çπ„ÇØÔºâ"""
    mask = torch.tril(torch.ones(seq_len, seq_len))
    return mask.unsqueeze(0)  # (1, seq_len, seq_len)

tgt_mask = create_causal_mask(tgt_len)

# DecoderÂÆüË°å
output, self_attn_weights, cross_attn_weights = decoder_layer(tgt, memory, tgt_mask)

print(f"„Çø„Éº„Ç≤„ÉÉ„ÉàÂÖ•Âäõ: {tgt.shape}")
print(f"Encoder„É°„É¢„É™: {memory.shape}")
print(f"DecoderÂá∫Âäõ: {output.shape}")
print(f"Self-AttentionÈáç„Åø: {self_attn_weights.shape}")
print(f"Cross-AttentionÈáç„Åø: {cross_attn_weights.shape}")
print("‚Üí Cross-Attention„ÅßEncoder„ÅÆÊÉÖÂ†±„ÇíÂèÇÁÖß")
</code></pre>

<h3>Âõ†Êûú„Éû„Çπ„ÇØ„ÅÆÂèØË¶ñÂåñ</h3>

<pre><code class="language-python">import torch
import matplotlib.pyplot as plt

# Âõ†Êûú„Éû„Çπ„ÇØ„ÅÆ‰ΩúÊàê„Å®ÂèØË¶ñÂåñ
def create_and_visualize_causal_mask(seq_len=10):
    """Âõ†Êûú„Éû„Çπ„ÇØ„Çí‰ΩúÊàê„Åó„Å¶ÂèØË¶ñÂåñ"""
    mask = torch.tril(torch.ones(seq_len, seq_len))

    print(f"=== Âõ†Êûú„Éû„Çπ„ÇØÔºàÁ≥ªÂàóÈï∑={seq_len}Ôºâ ===")
    print(mask.numpy())
    print("\n1 = ÂèÇÁÖßÂèØËÉΩÔºàÈÅéÂéª„ÉªÁèæÂú®Ôºâ")
    print("0 = ÂèÇÁÖß‰∏çÂèØÔºàÊú™Êù•Ôºâ")
    print("\n‰æã: ‰ΩçÁΩÆ3„ÅØ‰ΩçÁΩÆ0,1,2,3„ÅÆ„ÅøÂèÇÁÖßÂèØËÉΩÔºà‰ΩçÁΩÆ4‰ª•Èôç„ÅØË¶ã„Åà„Å™„ÅÑÔºâ")

    return mask

# „Éû„Çπ„ÇØ‰ΩúÊàê
causal_mask = create_and_visualize_causal_mask(seq_len=8)

# Attention„Çπ„Ç≥„Ç¢„Å∏„ÅÆÈÅ©Áî®‰æã
print("\n=== „Éû„Çπ„ÇØÈÅ©Áî®„ÅÆÂäπÊûú ===")
scores = torch.randn(8, 8)  # „É©„É≥„ÉÄ„É†„Å™Attention„Çπ„Ç≥„Ç¢

print("„Éû„Çπ„ÇØÂâç„ÅÆ„Çπ„Ç≥„Ç¢Ôºà‰∏ÄÈÉ®Ôºâ:")
print(scores[:4, :4].numpy())

# „Éû„Çπ„ÇØÈÅ©Áî®ÔºàÊú™Êù•„Çí-inf„Å´Ôºâ
masked_scores = scores.masked_fill(causal_mask == 0, float('-inf'))

print("\n„Éû„Çπ„ÇØÂæå„ÅÆ„Çπ„Ç≥„Ç¢Ôºà‰∏ÄÈÉ®Ôºâ:")
print(masked_scores[:4, :4].numpy())

# SoftmaxÈÅ©Áî®
attn_weights = F.softmax(masked_scores, dim=-1)

print("\nSoftmaxÂæå„ÅÆÈáç„ÅøÔºà‰∏ÄÈÉ®Ôºâ:")
print(attn_weights[:4, :4].numpy())
print("‚Üí Êú™Êù•„ÅÆ‰ΩçÁΩÆÔºà-infÔºâ„ÅÆÈáç„Åø„ÅØ0„Å´„Å™„Çã")
</code></pre>

<h3>ÂÆåÂÖ®„Å™Decoder„ÅÆÂÆüË£Ö</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import math

class TransformerDecoder(nn.Module):
    """ÂÆåÂÖ®„Å™Transformer Decoder"""
    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6,
                 d_ff=2048, dropout=0.1, max_len=5000):
        super(TransformerDecoder, self).__init__()

        self.d_model = d_model

        # ÂçòË™ûÂüã„ÇÅËæº„Åø
        self.embedding = nn.Embedding(vocab_size, d_model)

        # ‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞
        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)

        # DecoderLayer„Çí„Çπ„Çø„ÉÉ„ÇØ
        self.layers = nn.ModuleList([
            DecoderLayer(d_model, nhead, d_ff, dropout)
            for _ in range(num_layers)
        ])

        # Âá∫ÂäõÂ±§
        self.fc_out = nn.Linear(d_model, vocab_size)

        self.dropout = nn.Dropout(dropout)

    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):
        """
        tgt: (batch, tgt_len) - „Çø„Éº„Ç≤„ÉÉ„Éà„Éà„Éº„ÇØ„É≥ID
        memory: (batch, src_len, d_model) - Encoder„ÅÆÂá∫Âäõ
        tgt_mask: (batch, tgt_len, tgt_len) - Âõ†Êûú„Éû„Çπ„ÇØ
        memory_mask: (batch, 1, src_len) - „Éë„Éá„Ç£„É≥„Ç∞„Éû„Çπ„ÇØ
        """
        # Âüã„ÇÅËæº„Åø + „Çπ„Ç±„Éº„É™„É≥„Ç∞
        x = self.embedding(tgt) * math.sqrt(self.d_model)

        # ‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞ËøΩÂä†
        x = self.pos_encoding(x)

        # ÂêÑDecoderLayer„ÇíÈÄöÈÅé
        self_attn_weights_list = []
        cross_attn_weights_list = []

        for layer in self.layers:
            x, self_attn_weights, cross_attn_weights = layer(x, memory, tgt_mask, memory_mask)
            self_attn_weights_list.append(self_attn_weights)
            cross_attn_weights_list.append(cross_attn_weights)

        # Ë™ûÂΩô„Å∏„ÅÆÂ∞ÑÂΩ±
        logits = self.fc_out(x)  # (batch, tgt_len, vocab_size)

        return logits, self_attn_weights_list, cross_attn_weights_list


# Âãï‰ΩúÁ¢∫Ë™ç
print("\n=== ÂÆåÂÖ®„Å™Decoder„ÅÆÂãï‰ΩúÁ¢∫Ë™ç ===")
vocab_size = 10000
decoder = TransformerDecoder(vocab_size, d_model=512, nhead=8, num_layers=6)

# „ÉÄ„Éü„Éº„Éá„Éº„Çø
batch_size = 16
tgt_len = 20
src_len = 25

tgt = torch.randint(0, vocab_size, (batch_size, tgt_len))
memory = torch.randn(batch_size, src_len, 512)

# Âõ†Êûú„Éû„Çπ„ÇØ
tgt_mask = create_causal_mask(tgt_len)

# DecoderÂÆüË°å
logits, self_attn_weights, cross_attn_weights = decoder(tgt, memory, tgt_mask)

print(f"„Çø„Éº„Ç≤„ÉÉ„ÉàÂÖ•Âäõ: {tgt.shape}")
print(f"Encoder„É°„É¢„É™: {memory.shape}")
print(f"DecoderÂá∫ÂäõÔºà„É≠„Ç∏„ÉÉ„ÉàÔºâ: {logits.shape}")
print(f"‚Üí ÂêÑ‰ΩçÁΩÆ„ÅßË™ûÂΩôÂÖ®‰Ωì„Å´ÂØæ„Åô„ÇãÁ¢∫ÁéáÂàÜÂ∏É„ÇíÂá∫Âäõ")

# „Éë„É©„É°„Éº„ÇøÊï∞
total_params = sum(p.numel() for p in decoder.parameters())
print(f"\nÁ∑è„Éë„É©„É°„Éº„ÇøÊï∞: {total_params:,}")
</code></pre>

<hr>

<h2>2.4 ÂÆåÂÖ®„Å™Transformer„É¢„Éá„É´</h2>

<h3>Encoder„Å®Decoder„ÅÆÁµ±Âêà</h3>

<pre><code class="language-python">import torch
import torch.nn as nn

class Transformer(nn.Module):
    """ÂÆåÂÖ®„Å™Transformer„É¢„Éá„É´ÔºàEncoder-DecoderÔºâ"""
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, nhead=8,
                 num_encoder_layers=6, num_decoder_layers=6, d_ff=2048,
                 dropout=0.1, max_len=5000):
        super(Transformer, self).__init__()

        # Encoder
        self.encoder = TransformerEncoder(
            src_vocab_size, d_model, nhead, num_encoder_layers,
            d_ff, dropout, max_len
        )

        # Decoder
        self.decoder = TransformerDecoder(
            tgt_vocab_size, d_model, nhead, num_decoder_layers,
            d_ff, dropout, max_len
        )

        self.d_model = d_model

        # „Éë„É©„É°„Éº„ÇøÂàùÊúüÂåñ
        self._reset_parameters()

    def _reset_parameters(self):
        """XavierÂàùÊúüÂåñ"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        """
        src: (batch, src_len) - „ÇΩ„Éº„Çπ„Éà„Éº„ÇØ„É≥
        tgt: (batch, tgt_len) - „Çø„Éº„Ç≤„ÉÉ„Éà„Éà„Éº„ÇØ„É≥
        src_mask: (batch, 1, src_len) - „ÇΩ„Éº„Çπ„ÅÆ„Éë„Éá„Ç£„É≥„Ç∞„Éû„Çπ„ÇØ
        tgt_mask: (batch, tgt_len, tgt_len) - „Çø„Éº„Ç≤„ÉÉ„Éà„ÅÆÂõ†Êûú„Éû„Çπ„ÇØ
        """
        # Encoder„Åß„ÇΩ„Éº„Çπ„ÇíÂá¶ÁêÜ
        memory, _ = self.encoder(src, src_mask)

        # Decoder„Åß„Çø„Éº„Ç≤„ÉÉ„Éà„ÇíÁîüÊàê
        output, _, _ = self.decoder(tgt, memory, tgt_mask, src_mask)

        return output

    def encode(self, src, src_mask=None):
        """Encoder„ÅÆ„ÅøÂÆüË°åÔºàÊé®Ë´ñÊôÇ„Å´‰ΩøÁî®Ôºâ"""
        memory, _ = self.encoder(src, src_mask)
        return memory

    def decode(self, tgt, memory, tgt_mask=None, memory_mask=None):
        """Decoder„ÅÆ„ÅøÂÆüË°åÔºàÊé®Ë´ñÊôÇ„Å´‰ΩøÁî®Ôºâ"""
        output, _, _ = self.decoder(tgt, memory, tgt_mask, memory_mask)
        return output


# „É¢„Éá„É´‰ΩúÊàê
print("=== ÂÆåÂÖ®„Å™Transformer„É¢„Éá„É´ ===")
src_vocab_size = 10000
tgt_vocab_size = 8000

model = Transformer(
    src_vocab_size=src_vocab_size,
    tgt_vocab_size=tgt_vocab_size,
    d_model=512,
    nhead=8,
    num_encoder_layers=6,
    num_decoder_layers=6,
    d_ff=2048,
    dropout=0.1
)

# Âãï‰ΩúÁ¢∫Ë™ç
batch_size = 16
src_len = 25
tgt_len = 20

src = torch.randint(0, src_vocab_size, (batch_size, src_len))
tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_len))

# „Éû„Çπ„ÇØ‰ΩúÊàê
src_mask = torch.ones(batch_size, 1, src_len)
tgt_mask = create_causal_mask(tgt_len)

# Forward pass
output = model(src, tgt, src_mask, tgt_mask)

print(f"„ÇΩ„Éº„ÇπÂÖ•Âäõ: {src.shape}")
print(f"„Çø„Éº„Ç≤„ÉÉ„ÉàÂÖ•Âäõ: {tgt.shape}")
print(f"„É¢„Éá„É´Âá∫Âäõ: {output.shape}")
print(f"‚Üí Âá∫Âäõ„ÅØ (batch, tgt_len, tgt_vocab_size) „ÅÆÂΩ¢Áä∂")

# Á∑è„Éë„É©„É°„Éº„ÇøÊï∞
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"\nÁ∑è„Éë„É©„É°„Éº„ÇøÊï∞: {total_params:,}")
print(f"Â≠¶ÁøíÂèØËÉΩ„Éë„É©„É°„Éº„ÇøÊï∞: {trainable_params:,}")
</code></pre>

<h3>Ëá™Â∑±ÂõûÂ∏∞ÁîüÊàê„ÅÆÂÆüË£Ö</h3>

<pre><code class="language-python">import torch
import torch.nn.functional as F

def generate_greedy(model, src, src_mask, max_len, start_token, end_token):
    """
    Greedy Decoding„Å´„Çà„ÇãÁ≥ªÂàóÁîüÊàê

    Args:
        model: Transformer„É¢„Éá„É´
        src: (batch, src_len) - „ÇΩ„Éº„ÇπÁ≥ªÂàó
        src_mask: (batch, 1, src_len) - „ÇΩ„Éº„Çπ„Éû„Çπ„ÇØ
        max_len: ÊúÄÂ§ßÁîüÊàêÈï∑
        start_token: ÈñãÂßã„Éà„Éº„ÇØ„É≥ID
        end_token: ÁµÇ‰∫Ü„Éà„Éº„ÇØ„É≥ID

    Returns:
        generated: (batch, gen_len) - ÁîüÊàêÁ≥ªÂàó
    """
    model.eval()
    batch_size = src.size(0)
    device = src.device

    # Encoder„Åß‰∏ÄÂ∫¶„Å†„ÅëÂá¶ÁêÜ
    memory = model.encode(src, src_mask)

    # ÁîüÊàêÁ≥ªÂàó„ÅÆÂàùÊúüÂåñÔºàÈñãÂßã„Éà„Éº„ÇØ„É≥Ôºâ
    generated = torch.full((batch_size, 1), start_token, dtype=torch.long, device=device)

    # Ëá™Â∑±ÂõûÂ∏∞ÁöÑ„Å´ÁîüÊàê
    for _ in range(max_len - 1):
        # Âõ†Êûú„Éû„Çπ„ÇØ‰ΩúÊàê
        tgt_len = generated.size(1)
        tgt_mask = create_causal_mask(tgt_len).to(device)

        # DecoderÂÆüË°å
        output = model.decode(generated, memory, tgt_mask, src_mask)

        # ÊúÄÂæå„ÅÆ‰ΩçÁΩÆ„ÅÆ‰∫àÊ∏¨„ÇíÂèñÂæó
        next_token_logits = output[:, -1, :]  # (batch, vocab_size)

        # GreedyÈÅ∏ÊäûÔºàÊúÄ„ÇÇÁ¢∫Áéá„ÅåÈ´ò„ÅÑ„Éà„Éº„ÇØ„É≥Ôºâ
        next_token = next_token_logits.argmax(dim=-1, keepdim=True)  # (batch, 1)

        # ÁîüÊàêÁ≥ªÂàó„Å´ËøΩÂä†
        generated = torch.cat([generated, next_token], dim=1)

        # ÂÖ®„Çµ„É≥„Éó„É´„ÅåÁµÇ‰∫Ü„Éà„Éº„ÇØ„É≥„Å´Âà∞ÈÅî„Åó„Åü„ÇâÁµÇ‰∫Ü
        if (next_token == end_token).all():
            break

    return generated


def generate_beam_search(model, src, src_mask, max_len, start_token, end_token, beam_size=5):
    """
    Beam Search„Å´„Çà„ÇãÁ≥ªÂàóÁîüÊàê

    Args:
        model: Transformer„É¢„Éá„É´
        src: (1, src_len) - „ÇΩ„Éº„ÇπÁ≥ªÂàóÔºà„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫1Ôºâ
        src_mask: (1, 1, src_len)
        max_len: ÊúÄÂ§ßÁîüÊàêÈï∑
        start_token: ÈñãÂßã„Éà„Éº„ÇØ„É≥ID
        end_token: ÁµÇ‰∫Ü„Éà„Éº„ÇØ„É≥ID
        beam_size: „Éì„Éº„É†„Çµ„Ç§„Ç∫

    Returns:
        best_sequence: (1, gen_len) - ÊúÄËâØ„ÅÆÁîüÊàêÁ≥ªÂàó
    """
    model.eval()
    device = src.device

    # Encoder
    memory = model.encode(src, src_mask)  # (1, src_len, d_model)
    memory = memory.repeat(beam_size, 1, 1)  # (beam_size, src_len, d_model)

    # „Éì„Éº„É†ÂàùÊúüÂåñ
    beams = torch.full((beam_size, 1), start_token, dtype=torch.long, device=device)
    beam_scores = torch.zeros(beam_size, device=device)
    beam_scores[1:] = float('-inf')  # ÊúÄÂàù„ÅØ1„Å§„ÅÆ„Éì„Éº„É†„ÅÆ„ÅøÊúâÂäπ

    finished_beams = []

    for step in range(max_len - 1):
        tgt_len = beams.size(1)
        tgt_mask = create_causal_mask(tgt_len).to(device)

        # Decoder
        output = model.decode(beams, memory, tgt_mask, src_mask.repeat(beam_size, 1, 1))
        next_token_logits = output[:, -1, :]  # (beam_size, vocab_size)

        # LogÁ¢∫Áéá
        log_probs = F.log_softmax(next_token_logits, dim=-1)

        # „Éì„Éº„É†„Çπ„Ç≥„Ç¢Êõ¥Êñ∞
        vocab_size = log_probs.size(-1)
        scores = beam_scores.unsqueeze(1) + log_probs  # (beam_size, vocab_size)
        scores = scores.view(-1)  # (beam_size * vocab_size)

        # Top-kÈÅ∏Êäû
        top_scores, top_indices = scores.topk(beam_size, largest=True)

        # Êñ∞„Åó„ÅÑ„Éì„Éº„É†
        beam_indices = top_indices // vocab_size
        token_indices = top_indices % vocab_size

        new_beams = []
        new_scores = []

        for i, (beam_idx, token_idx, score) in enumerate(zip(beam_indices, token_indices, top_scores)):
            # „Éì„Éº„É†„ÇíÊã°Âºµ
            new_beam = torch.cat([beams[beam_idx], token_idx.unsqueeze(0)])

            # ÁµÇ‰∫Ü„Éà„Éº„ÇØ„É≥„Å´Âà∞ÈÅî„Åó„Åü„ÇâÂÆåÊàê„Éì„Éº„É†„Å´ËøΩÂä†
            if token_idx == end_token:
                finished_beams.append((new_beam, score.item()))
            else:
                new_beams.append(new_beam)
                new_scores.append(score)

        # ÂÆåÊàê„Éì„Éº„É†„ÅåÂçÅÂàÜ„ÅÇ„Çå„Å∞ÁµÇ‰∫Ü
        if len(finished_beams) >= beam_size:
            break

        # „Éì„Éº„É†„ÅåÊÆã„Å£„Å¶„ÅÑ„Å™„ÅÑÂ†¥Âêà„ÇÇÁµÇ‰∫Ü
        if len(new_beams) == 0:
            break

        # „Éì„Éº„É†„ÇíÊõ¥Êñ∞
        beams = torch.stack(new_beams)
        beam_scores = torch.tensor(new_scores, device=device)

    # ÊúÄËâØ„ÅÆ„Éì„Éº„É†„ÇíÈÅ∏Êäû
    if finished_beams:
        best_beam, best_score = max(finished_beams, key=lambda x: x[1])
    else:
        best_beam = beams[0]

    return best_beam.unsqueeze(0)


# Âãï‰ΩúÁ¢∫Ë™ç
print("\n=== Ëá™Â∑±ÂõûÂ∏∞ÁîüÊàê„ÅÆ„ÉÜ„Çπ„Éà ===")

# „ÉÄ„Éü„Éº„É¢„Éá„É´„Å®„Éá„Éº„Çø
src_vocab_size = 100
tgt_vocab_size = 100
model = Transformer(src_vocab_size, tgt_vocab_size, d_model=128, nhead=4,
                   num_encoder_layers=2, num_decoder_layers=2)

src = torch.randint(1, src_vocab_size, (1, 10))
src_mask = torch.ones(1, 1, 10)

start_token = 1
end_token = 2
max_len = 20

# Greedy Decoding
with torch.no_grad():
    generated_greedy = generate_greedy(model, src, src_mask, max_len, start_token, end_token)

print(f"„ÇΩ„Éº„ÇπÁ≥ªÂàó: {src.shape}")
print(f"GreedyÁîüÊàê: {generated_greedy.shape}")
print(f"ÁîüÊàêÁ≥ªÂàó: {generated_greedy[0].tolist()}")

# Beam Search
with torch.no_grad():
    generated_beam = generate_beam_search(model, src, src_mask, max_len, start_token, end_token, beam_size=5)

print(f"\nBeam SearchÁîüÊàê: {generated_beam.shape}")
print(f"ÁîüÊàêÁ≥ªÂàó: {generated_beam[0].tolist()}")
</code></pre>

<hr>

<h2>2.5 ÂÆüË∑µÔºöÊ©üÊ¢∞ÁøªË®≥„Ç∑„Çπ„ÉÜ„É†</h2>

<h3>„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÊ∫ñÂÇô</h3>

<pre><code class="language-python">import torch
from torch.utils.data import Dataset, DataLoader
from collections import Counter
import re

class TranslationDataset(Dataset):
    """Á∞°ÊòìÁöÑ„Å™ÁøªË®≥„Éá„Éº„Çø„Çª„ÉÉ„Éà"""
    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab):
        self.src_sentences = src_sentences
        self.tgt_sentences = tgt_sentences
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab

    def __len__(self):
        return len(self.src_sentences)

    def __getitem__(self, idx):
        src = self.src_sentences[idx]
        tgt = self.tgt_sentences[idx]

        # „Éà„Éº„ÇØ„É≥ID„Å´Â§âÊèõ
        src_ids = [self.src_vocab.get(w, self.src_vocab['<unk>']) for w in src.split()]
        tgt_ids = [self.tgt_vocab.get(w, self.tgt_vocab['<unk>']) for w in tgt.split()]

        return torch.tensor(src_ids), torch.tensor(tgt_ids)


def build_vocab(sentences, max_vocab_size=10000):
    """Ë™ûÂΩô„ÇíÊßãÁØâ"""
    words = []
    for sent in sentences:
        words.extend(sent.split())

    # È†ªÂ∫¶„Ç´„Ç¶„É≥„Éà
    word_counts = Counter(words)
    most_common = word_counts.most_common(max_vocab_size - 4)  # ÁâπÊÆä„Éà„Éº„ÇØ„É≥ÂàÜ„ÇíÈô§„Åè

    # Ë™ûÂΩôËæûÊõ∏‰ΩúÊàê
    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}
    for word, _ in most_common:
        vocab[word] = len(vocab)

    return vocab


# „ÉÄ„Éü„Éº„Éá„Éº„ÇøÔºàÂÆüÈöõ„ÅØMulti30k„ÇÑWMT„Å™„Å©„Çí‰ΩøÁî®Ôºâ
src_sentences = [
    "i love machine learning",
    "transformers are powerful",
    "attention is all you need",
    "deep learning is amazing",
    "natural language processing"
]

tgt_sentences = [
    "ÁßÅ „ÅØ Ê©üÊ¢∞ Â≠¶Áøí „Åå Â•Ω„Åç „Åß„Åô",
    "„Éà„É©„É≥„Çπ„Éï„Ç©„Éº„Éû„Éº „ÅØ Âº∑Âäõ „Åß„Åô",
    "„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥ „Åå ÂÖ®„Å¶ „Åß„Åô",
    "Ê∑±Â±§ Â≠¶Áøí „ÅØ Á¥†Êô¥„Çâ„Åó„ÅÑ „Åß„Åô",
    "Ëá™ÁÑ∂ Ë®ÄË™û Âá¶ÁêÜ"
]

# Ë™ûÂΩôÊßãÁØâ
src_vocab = build_vocab(src_sentences)
tgt_vocab = build_vocab(tgt_sentences)

print("=== ÁøªË®≥„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÊ∫ñÂÇô ===")
print(f"„ÇΩ„Éº„ÇπË™ûÂΩô„Çµ„Ç§„Ç∫: {len(src_vocab)}")
print(f"„Çø„Éº„Ç≤„ÉÉ„ÉàË™ûÂΩô„Çµ„Ç§„Ç∫: {len(tgt_vocab)}")
print(f"\n„ÇΩ„Éº„ÇπË™ûÂΩôÔºà‰∏ÄÈÉ®Ôºâ: {list(src_vocab.items())[:10]}")
print(f"„Çø„Éº„Ç≤„ÉÉ„ÉàË™ûÂΩôÔºà‰∏ÄÈÉ®Ôºâ: {list(tgt_vocab.items())[:10]}")

# „Éá„Éº„Çø„Çª„ÉÉ„Éà‰ΩúÊàê
dataset = TranslationDataset(src_sentences, tgt_sentences, src_vocab, tgt_vocab)

# „Çµ„É≥„Éó„É´Á¢∫Ë™ç
src_sample, tgt_sample = dataset[0]
print(f"\n„Çµ„É≥„Éó„É´ 0:")
print(f"„ÇΩ„Éº„Çπ: {src_sentences[0]}")
print(f"„ÇΩ„Éº„ÇπID: {src_sample.tolist()}")
print(f"„Çø„Éº„Ç≤„ÉÉ„Éà: {tgt_sentences[0]}")
print(f"„Çø„Éº„Ç≤„ÉÉ„ÉàID: {tgt_sample.tolist()}")
</code></pre>

<h3>Ë®ìÁ∑¥„É´„Éº„Éó„ÅÆÂÆüË£Ö</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn.utils.rnn import pad_sequence

def collate_fn(batch, src_vocab, tgt_vocab):
    """„Éê„ÉÉ„ÉÅ„ÅÆ„Ç≥„É¨„Éº„ÉàÈñ¢Êï∞"""
    src_batch, tgt_batch = zip(*batch)

    # „Éë„Éá„Ç£„É≥„Ç∞
    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=src_vocab['<pad>'])
    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_vocab['<pad>'])

    return src_padded, tgt_padded


def create_masks(src, tgt, src_pad_idx, tgt_pad_idx):
    """„Éû„Çπ„ÇØ„Çí‰ΩúÊàê"""
    # „ÇΩ„Éº„Çπ„ÅÆ„Éë„Éá„Ç£„É≥„Ç∞„Éû„Çπ„ÇØ
    src_mask = (src != src_pad_idx).unsqueeze(1)  # (batch, 1, src_len)

    # „Çø„Éº„Ç≤„ÉÉ„Éà„ÅÆÂõ†Êûú„Éû„Çπ„ÇØ + „Éë„Éá„Ç£„É≥„Ç∞„Éû„Çπ„ÇØ
    tgt_len = tgt.size(1)
    tgt_mask = create_causal_mask(tgt_len).to(tgt.device)  # (1, tgt_len, tgt_len)
    tgt_pad_mask = (tgt != tgt_pad_idx).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, tgt_len)
    tgt_mask = tgt_mask & tgt_pad_mask

    return src_mask, tgt_mask


def train_epoch(model, dataloader, optimizer, criterion, src_vocab, tgt_vocab, device):
    """1„Ç®„Éù„ÉÉ„ÇØ„ÅÆË®ìÁ∑¥"""
    model.train()
    total_loss = 0

    for src, tgt in dataloader:
        src, tgt = src.to(device), tgt.to(device)

        # „Çø„Éº„Ç≤„ÉÉ„Éà„ÇíÂÖ•Âäõ„Å®ÊïôÂ∏´„Éá„Éº„Çø„Å´ÂàÜÂâ≤
        tgt_input = tgt[:, :-1]  # <eos>„ÇíÈô§„Åè
        tgt_output = tgt[:, 1:]  # <sos>„ÇíÈô§„Åè

        # „Éû„Çπ„ÇØ‰ΩúÊàê
        src_mask, tgt_mask = create_masks(src, tgt_input,
                                         src_vocab['<pad>'], tgt_vocab['<pad>'])

        # Forward
        optimizer.zero_grad()
        output = model(src, tgt_input, src_mask, tgt_mask)

        # LossË®àÁÆóÔºà„Éë„Éá„Ç£„É≥„Ç∞„ÇíÁÑ°Ë¶ñÔºâ
        output = output.reshape(-1, output.size(-1))
        tgt_output = tgt_output.reshape(-1)

        loss = criterion(output, tgt_output)

        # Backward
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(dataloader)


# Ë®ìÁ∑¥Ë®≠ÂÆö
print("\n=== ÁøªË®≥„É¢„Éá„É´„ÅÆË®ìÁ∑¥ ===")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"„Éá„Éê„Ç§„Çπ: {device}")

# „É¢„Éá„É´‰ΩúÊàê
model = Transformer(
    src_vocab_size=len(src_vocab),
    tgt_vocab_size=len(tgt_vocab),
    d_model=256,
    nhead=8,
    num_encoder_layers=3,
    num_decoder_layers=3,
    d_ff=1024,
    dropout=0.1
).to(device)

# DataLoader
from functools import partial
loader = DataLoader(
    dataset,
    batch_size=2,
    shuffle=True,
    collate_fn=partial(collate_fn, src_vocab=src_vocab, tgt_vocab=tgt_vocab)
)

# ÊêçÂ§±Èñ¢Êï∞„Å®„Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂
criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab['<pad>'])
optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)

# Ë®ìÁ∑¥
num_epochs = 10
for epoch in range(num_epochs):
    loss = train_epoch(model, loader, optimizer, criterion, src_vocab, tgt_vocab, device)
    print(f"Epoch {epoch+1}/{num_epochs} - Loss: {loss:.4f}")

print("\nË®ìÁ∑¥ÂÆå‰∫ÜÔºÅ")
</code></pre>

<h3>ÁøªË®≥Êé®Ë´ñ</h3>

<pre><code class="language-python">import torch

def translate(model, src_sentence, src_vocab, tgt_vocab, device, max_len=50):
    """Êñá„ÇíÁøªË®≥"""
    model.eval()

    # „ÇΩ„Éº„ÇπÊñá„Çí„Éà„Éº„ÇØ„É≥ID„Å´Â§âÊèõ
    src_tokens = src_sentence.split()
    src_ids = [src_vocab.get(w, src_vocab['<unk>']) for w in src_tokens]
    src = torch.tensor(src_ids).unsqueeze(0).to(device)  # (1, src_len)

    # „ÇΩ„Éº„Çπ„Éû„Çπ„ÇØ
    src_mask = torch.ones(1, 1, src.size(1)).to(device)

    # Greedy Decoding„ÅßÁîüÊàê
    with torch.no_grad():
        generated = generate_greedy(
            model, src, src_mask, max_len,
            start_token=tgt_vocab['<sos>'],
            end_token=tgt_vocab['<eos>']
        )

    # „Éà„Éº„ÇØ„É≥ID„ÇíÂçòË™û„Å´Â§âÊèõ
    idx_to_word = {v: k for k, v in tgt_vocab.items()}
    translated = [idx_to_word.get(idx.item(), '<unk>') for idx in generated[0]]

    # <sos>„Å®<eos>„ÇíÈô§Âéª
    if translated[0] == '<sos>':
        translated = translated[1:]
    if '<eos>' in translated:
        eos_idx = translated.index('<eos>')
        translated = translated[:eos_idx]

    return ' '.join(translated)


# ÁøªË®≥„ÉÜ„Çπ„Éà
print("\n=== ÁøªË®≥„ÉÜ„Çπ„Éà ===")

test_sentences = [
    "i love machine learning",
    "transformers are powerful",
    "attention is all you need"
]

for src_sent in test_sentences:
    translated = translate(model, src_sent, src_vocab, tgt_vocab, device)
    print(f"„ÇΩ„Éº„Çπ: {src_sent}")
    print(f"ÁøªË®≥: {translated}")
    print()

print("‚Üí Â∞èË¶èÊ®°„Éá„Éº„Çø„ÅÆ„Åü„ÇÅÂÆåÁíß„Åß„ÅØ„Å™„ÅÑ„Åå„ÄÅÂü∫Êú¨ÁöÑ„Å™ÁøªË®≥Ê©üËÉΩ„ÇíÂÆüË£Ö")
</code></pre>

<hr>

<h2>2.6 Transformer„ÅÆÂ≠¶Áøí„ÉÜ„ÇØ„Éã„ÉÉ„ÇØ</h2>

<h3>Â≠¶ÁøíÁéá„ÅÆ„Ç¶„Ç©„Éº„É†„Ç¢„ÉÉ„Éó</h3>

<p>Transformer„ÅÆË®ìÁ∑¥„Åß„ÅØ„ÄÅ<strong>„Ç¶„Ç©„Éº„É†„Ç¢„ÉÉ„Éó„Çπ„Ç±„Ç∏„É•„Éº„É©</strong>„ÅåÈáçË¶Å„Åß„Åô„ÄÇÂàùÊúü„ÅØÂ≠¶ÁøíÁéá„ÇíÂ∞è„Åï„Åè‰øù„Å°„ÄÅÂæê„ÄÖ„Å´Â¢ó„ÇÑ„Åó„Å¶„Åã„ÇâÊ∏õË°∞„Åï„Åõ„Åæ„Åô„ÄÇ</p>

$$
\text{lr}(step) = d_{\text{model}}^{-0.5} \cdot \min(step^{-0.5}, step \cdot \text{warmup\_steps}^{-1.5})
$$

<pre><code class="language-python">import torch.optim as optim

class NoamOpt:
    """NoamÂ≠¶ÁøíÁéá„Çπ„Ç±„Ç∏„É•„Éº„É©ÔºàË´ñÊñáÂÆüË£ÖÔºâ"""
    def __init__(self, d_model, warmup_steps, optimizer):
        self.d_model = d_model
        self.warmup_steps = warmup_steps
        self.optimizer = optimizer
        self._step = 0
        self._rate = 0

    def step(self):
        """1„Çπ„ÉÜ„ÉÉ„ÉóÊõ¥Êñ∞"""
        self._step += 1
        rate = self.rate()
        for p in self.optimizer.param_groups:
            p['lr'] = rate
        self._rate = rate
        self.optimizer.step()

    def rate(self, step=None):
        """ÁèæÂú®„ÅÆÂ≠¶ÁøíÁéá„ÇíË®àÁÆó"""
        if step is None:
            step = self._step
        return (self.d_model ** (-0.5)) * min(step ** (-0.5),
                                               step * self.warmup_steps ** (-1.5))

# ‰ΩøÁî®‰æã
print("=== NoamÂ≠¶ÁøíÁéá„Çπ„Ç±„Ç∏„É•„Éº„É© ===")
d_model = 512
warmup_steps = 4000

optimizer = optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)
scheduler = NoamOpt(d_model, warmup_steps, optimizer)

# Â≠¶ÁøíÁéá„ÅÆÊé®Áßª„ÇíÂèØË¶ñÂåñ
steps = list(range(1, 20000))
lrs = [scheduler.rate(step) for step in steps]

print(f"ÂàùÊúüÂ≠¶ÁøíÁéáÔºàstep=1Ôºâ: {lrs[0]:.6f}")
print(f"„Éî„Éº„ÇØÂ≠¶ÁøíÁéáÔºàstep={warmup_steps}Ôºâ: {lrs[warmup_steps-1]:.6f}")
print(f"ÂæåÊúüÂ≠¶ÁøíÁéáÔºàstep=20000Ôºâ: {lrs[-1]:.6f}")
print("‚Üí „Ç¶„Ç©„Éº„É†„Ç¢„ÉÉ„Éó„ÅßÂæê„ÄÖ„Å´Â¢óÂä†„ÄÅ„Åù„ÅÆÂæåÊ∏õË°∞")
</code></pre>

<h3>Label Smoothing</h3>

<p><strong>Label Smoothing</strong>„ÅØ„ÄÅÊ≠£Ëß£„É©„Éô„É´„ÅÆÁ¢∫Áéá„Çí1„Åß„ÅØ„Å™„Åè0.9Á®ãÂ∫¶„Å´„Åó„ÄÅ‰ªñ„ÅÆ„ÇØ„É©„Çπ„Å´Â∞ë„ÅóÁ¢∫Áéá„ÇíÂàÜÊï£„Åï„Åõ„ÇãÊ≠£ÂâáÂåñÊâãÊ≥ï„Åß„Åô„ÄÇ</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class LabelSmoothingLoss(nn.Module):
    """Label Smoothing‰ªò„ÅçCross Entropy Loss"""
    def __init__(self, num_classes, smoothing=0.1, ignore_index=-100):
        super(LabelSmoothingLoss, self).__init__()
        self.num_classes = num_classes
        self.smoothing = smoothing
        self.ignore_index = ignore_index
        self.confidence = 1.0 - smoothing

    def forward(self, pred, target):
        """
        pred: (batch * seq_len, num_classes) - „É≠„Ç∏„ÉÉ„Éà
        target: (batch * seq_len) - Ê≠£Ëß£„É©„Éô„É´
        """
        # Log-softmax
        log_probs = F.log_softmax(pred, dim=-1)

        # Ê≠£Ëß£‰ΩçÁΩÆ„ÅÆÁ¢∫Áéá„ÇíÂèñÂæó
        nll_loss = -log_probs.gather(dim=-1, index=target.unsqueeze(1)).squeeze(1)

        # ÂÖ®„ÇØ„É©„Çπ„ÅÆÂπ≥ÂùálogÁ¢∫Áéá
        smooth_loss = -log_probs.mean(dim=-1)

        # ÁµÑ„ÅøÂêà„Çè„Åõ
        loss = self.confidence * nll_loss + self.smoothing * smooth_loss

        # ignore_index„Çí„Éû„Çπ„ÇØ
        if self.ignore_index >= 0:
            mask = (target != self.ignore_index).float()
            loss = (loss * mask).sum() / mask.sum()
        else:
            loss = loss.mean()

        return loss


# ÊØîËºÉ
print("\n=== Label Smoothing„ÅÆÂäπÊûú ===")

num_classes = 10
criterion_normal = nn.CrossEntropyLoss()
criterion_smooth = LabelSmoothingLoss(num_classes, smoothing=0.1)

# „ÉÄ„Éü„Éº„Éá„Éº„Çø
pred = torch.randn(32, num_classes)
target = torch.randint(0, num_classes, (32,))

loss_normal = criterion_normal(pred, target)
loss_smooth = criterion_smooth(pred, target)

print(f"ÈÄöÂ∏∏„ÅÆCross Entropy Loss: {loss_normal.item():.4f}")
print(f"Label Smoothing Loss: {loss_smooth.item():.4f}")
print("‚Üí Label Smoothing„ÅØÈÅé‰ø°„ÇíÈò≤„Åé„ÄÅÊ±éÂåñÊÄßËÉΩ„ÇíÂêë‰∏ä")
</code></pre>

<h3>Mixed Precision Training</h3>

<pre><code class="language-python">import torch
from torch.cuda.amp import autocast, GradScaler

# Mixed Precision TrainingÔºàGPUÂà©Áî®ÊôÇÔºâ
if torch.cuda.is_available():
    print("\n=== Mixed Precision Training„ÅÆ‰æã ===")

    device = torch.device('cuda')
    model = model.to(device)

    scaler = GradScaler()

    # Ë®ìÁ∑¥„É´„Éº„Éó„ÅÆ‰∏ÄÈÉ®
    for src, tgt in loader:
        src, tgt = src.to(device), tgt.to(device)
        tgt_input = tgt[:, :-1]
        tgt_output = tgt[:, 1:]

        optimizer.zero_grad()

        # Mixed Precision„ÅßË®àÁÆó
        with autocast():
            output = model(src, tgt_input)
            output = output.reshape(-1, output.size(-1))
            tgt_output = tgt_output.reshape(-1)
            loss = criterion(output, tgt_output)

        # „Çπ„Ç±„Éº„É´„Åï„Çå„ÅüÂãæÈÖç„ÅßÈÄÜ‰ºùÊí≠
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

    print("‚Üí FP16Ë®àÁÆó„ÅßÈ´òÈÄüÂåñÔºÜ„É°„É¢„É™ÂâäÊ∏õÔºàÊúÄÂ§ß2ÂÄçÈ´òÈÄüÔºâ")
else:
    print("\n=== Mixed Precision Training ===")
    print("GPU„ÅåÂà©Áî®„Åß„Åç„Å™„ÅÑ„Åü„ÇÅ„ÄÅ„Çπ„Ç≠„ÉÉ„Éó„Åó„Åæ„Åô")
</code></pre>

<hr>

<h2>ÊºîÁøíÂïèÈ°å</h2>

<details>
<summary><strong>ÊºîÁøí1ÔºöMulti-Head Attention„ÅÆ„Éò„ÉÉ„ÉâÊï∞„ÅÆÂΩ±Èüø</strong></summary>

<p>Áï∞„Å™„Çã„Éò„ÉÉ„ÉâÊï∞Ôºà1, 2, 4, 8, 16Ôºâ„Åß„É¢„Éá„É´„ÇíË®ìÁ∑¥„Åó„ÄÅÊÄßËÉΩ„Å®Ë®àÁÆó„Ç≥„Çπ„Éà„ÇíÊØîËºÉ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# TODO: „Éò„ÉÉ„ÉâÊï∞„ÇíÂ§â„Åà„Å¶Ë§áÊï∞„ÅÆ„É¢„Éá„É´„Çí‰ΩúÊàê
# TODO: Âêå„Åò„Éá„Éº„Çø„ÅßË®ìÁ∑¥„Åó„ÄÅÊÄßËÉΩ„ÉªË®ìÁ∑¥ÊôÇÈñì„Éª„Éë„É©„É°„Éº„ÇøÊï∞„ÇíÊØîËºÉ
# TODO: AttentionÂèØË¶ñÂåñ„Åß„Éò„ÉÉ„Éâ„Åî„Å®„ÅÆÂΩπÂâ≤ÂàÜÊãÖ„ÇíÂàÜÊûê
# „Éí„É≥„Éà: „Éò„ÉÉ„ÉâÊï∞„ÅåÂ§ö„ÅÑ„Åª„Å©ÊÄßËÉΩÂêë‰∏ä„Åô„Çã„Åå„ÄÅË®àÁÆó„Ç≥„Çπ„Éà„ÇÇÂ¢óÂä†
</code></pre>

</details>

<details>
<summary><strong>ÊºîÁøí2ÔºöPositional Encoding„ÅÆÂÆüÈ®ì</strong></summary>

<p>Sin/Cos‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞„Å®Â≠¶ÁøíÂèØËÉΩ„Å™‰ΩçÁΩÆÂüã„ÇÅËæº„Åø„ÇíÊØîËºÉ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# TODO: 2Á®ÆÈ°û„ÅÆ‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞„ÇíÂÆüË£Ö
# 1. Sin/CosÔºàÂõ∫ÂÆöÔºâ
# 2. nn.EmbeddingÔºàÂ≠¶ÁøíÂèØËÉΩÔºâ

# TODO: Âêå„Åò„Çø„Çπ„ÇØ„ÅßÊÄßËÉΩÊØîËºÉ
# TODO: Á≥ªÂàóÈï∑„ÅÆÊ±éÂåñÊÄßËÉΩ„ÇíË©ï‰æ°ÔºàË®ìÁ∑¥„Çà„ÇäÈï∑„ÅÑÁ≥ªÂàó„Åß„ÉÜ„Çπ„ÉàÔºâ
# ÊúüÂæÖ: Sin/Cos„ÅØ‰ªªÊÑèÈï∑„Å´Ê±éÂåñÂèØËÉΩ
</code></pre>

</details>

<details>
<summary><strong>ÊºîÁøí3ÔºöÂõ†Êûú„Éû„Çπ„ÇØ„ÅÆÂèØË¶ñÂåñ</strong></summary>

<p>Decoder„ÅÆÂõ†Êûú„Éû„Çπ„ÇØ„ÅåAttentionÈáç„Åø„Å´„Å©„ÅÜÂΩ±Èüø„Åô„Çã„ÅãÂèØË¶ñÂåñ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>

<pre><code class="language-python">import torch
import matplotlib.pyplot as plt

# TODO: DecoderLayer„ÅÆSelf-AttentionÈáç„Åø„ÇíÂèñÂæó
# TODO: „Éû„Çπ„ÇØÊúâ„Çä„ÉªÁÑ°„Åó„ÅßAttentionÈáç„Åø„ÇíÂèØË¶ñÂåñ
# TODO: „Éí„Éº„Éà„Éû„ÉÉ„Éó„ÅßÊú™Êù•„ÅÆ„Éà„Éº„ÇØ„É≥„ÅåË¶ã„Åà„Å™„ÅÑ„Åì„Å®„ÇíÁ¢∫Ë™ç
</code></pre>

</details>

<details>
<summary><strong>ÊºîÁøí4ÔºöBeam Search„ÅÆ„Éì„Éº„É†„Çµ„Ç§„Ç∫ÊúÄÈÅ©Âåñ</strong></summary>

<p>Áï∞„Å™„Çã„Éì„Éº„É†„Çµ„Ç§„Ç∫Ôºà1, 3, 5, 10, 20Ôºâ„ÅßÁøªË®≥ÂìÅË≥™„Å®ÈÄüÂ∫¶„ÇíÊØîËºÉ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>

<pre><code class="language-python">import torch
import time

# TODO: „Éì„Éº„É†„Çµ„Ç§„Ç∫„ÇíÂ§â„Åà„Å¶ÁøªË®≥„ÇíÂÆüË°å
# TODO: BLEU „Çπ„Ç≥„Ç¢„ÄÅÁîüÊàêÊôÇÈñì„ÇíË®àÊ∏¨
# TODO: „Éì„Éº„É†„Çµ„Ç§„Ç∫ vs ÂìÅË≥™„ÉªÈÄüÂ∫¶„ÅÆ„Ç∞„É©„Éï„Çí‰ΩúÊàê
# ÊúüÂæÖ: „Éì„Éº„É†„Çµ„Ç§„Ç∫5-10„ÅßÂìÅË≥™„Å®ÈÄüÂ∫¶„ÅÆ„Éê„É©„É≥„Çπ„ÅåËâØ„ÅÑ
</code></pre>

</details>

<details>
<summary><strong>ÊºîÁøí5ÔºöLayerÊï∞„ÅÆÂΩ±Èüø„ÇíË™øÊüª</strong></summary>

<p>Encoder/Decoder„ÅÆÂ±§Êï∞Ôºà1, 2, 4, 6, 12Ôºâ„ÇíÂ§â„Åà„Å¶ÊÄßËÉΩ„ÇíÊØîËºÉ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# TODO: Áï∞„Å™„ÇãÂ±§Êï∞„Åß„É¢„Éá„É´„Çí‰ΩúÊàê
# TODO: Ë®ìÁ∑¥Loss„ÄÅÊ§úË®ºLoss„ÄÅ„Éë„É©„É°„Éº„ÇøÊï∞„ÄÅË®ìÁ∑¥ÊôÇÈñì„ÇíË®òÈå≤
# TODO: Â±§Êï∞ vs ÊÄßËÉΩ„ÅÆ„Ç∞„É©„Éï„Çí‰ΩúÊàê
# ÂàÜÊûê: Ê∑±„Åô„Åé„Çã„Å®ÈÅéÂ≠¶Áøí„ÉªË®ìÁ∑¥ÊôÇÈñìÂ¢ó„ÄÅÊµÖ„Åô„Åé„Çã„Å®Ë°®ÁèæÂäõ‰∏çË∂≥
</code></pre>

</details>

<hr>

<h2>„Åæ„Å®„ÇÅ</h2>

<p>„Åì„ÅÆÁ´†„Åß„ÅØ„ÄÅTransformer„ÅÆÂÆåÂÖ®„Å™„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÇíÂ≠¶„Å≥„Åæ„Åó„Åü„ÄÇ</p>

<h3>ÈáçË¶Å„Éù„Ç§„É≥„Éà</h3>

<ul>
<li><strong>Transformer„ÅÆÊßãÈÄ†</strong>ÔºöEncoder-Decoder„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÄÅ6Â±§„Çπ„Çø„ÉÉ„ÇØ</li>
<li><strong>Encoder</strong>ÔºöMulti-Head Self-Attention + FFN„ÄÅ‰∏¶ÂàóÂá¶ÁêÜÂèØËÉΩ</li>
<li><strong>Decoder</strong>ÔºöMasked Self-Attention + Cross-Attention + FFN„ÄÅËá™Â∑±ÂõûÂ∏∞ÁîüÊàê</li>
<li><strong>Multi-Head Attention</strong>ÔºöË§áÊï∞„ÅÆË¶≥ÁÇπ„Åã„Çâ‰æùÂ≠òÈñ¢‰øÇ„ÇíÊçâ„Åà„Çã</li>
<li><strong>Positional Encoding</strong>ÔºöSin/CosÈñ¢Êï∞„Åß‰ΩçÁΩÆÊÉÖÂ†±„ÇíÊ≥®ÂÖ•</li>
<li><strong>Residual + Layer Norm</strong>ÔºöÊ∑±„ÅÑÂ±§„Åß„ÇÇÂ≠¶Áøí„ÇíÂÆâÂÆöÂåñ</li>
<li><strong>Âõ†Êûú„Éû„Çπ„ÇØ</strong>ÔºöÊú™Êù•„ÅÆ„Éà„Éº„ÇØ„É≥„ÇíË¶ã„Å™„ÅÑ„Çà„ÅÜÂà∂Âæ°</li>
<li><strong>Ëá™Â∑±ÂõûÂ∏∞ÁîüÊàê</strong>ÔºöGreedy Decoding„ÄÅBeam Search</li>
<li><strong>Â≠¶Áøí„ÉÜ„ÇØ„Éã„ÉÉ„ÇØ</strong>Ôºö„Ç¶„Ç©„Éº„É†„Ç¢„ÉÉ„Éó„ÄÅLabel Smoothing„ÄÅMixed Precision</li>
<li><strong>ÂÆüË∑µ</strong>ÔºöÊ©üÊ¢∞ÁøªË®≥„Ç∑„Çπ„ÉÜ„É†„ÅÆÂÆåÂÖ®ÂÆüË£Ö</li>
</ul>

<h3>Ê¨°„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó</h3>

<p>Ê¨°Á´†„Åß„ÅØ„ÄÅ<strong>Transformer„ÅÆÂ≠¶Áøí„Å®ÊúÄÈÅ©Âåñ</strong>„Å´„Å§„ÅÑ„Å¶Â≠¶„Å≥„Åæ„Åô„ÄÇÂäπÁéáÁöÑ„Å™Ë®ìÁ∑¥ÊâãÊ≥ï„ÄÅ„Éá„Éº„ÇøÊã°Âºµ„ÄÅË©ï‰æ°ÊåáÊ®ô„ÄÅ„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Å™„Å©„ÄÅÂÆüÁî®ÁöÑ„Å™„ÉÜ„ÇØ„Éã„ÉÉ„ÇØ„ÇíÁøíÂæó„Åó„Åæ„Åô„ÄÇ</p>

<div class="navigation">
    <a href="chapter1-attention.html" class="nav-button">‚Üê Á¨¨1Á´†ÔºöAttentionÊ©üÊßã</a>
    <a href="chapter3-training.html" class="nav-button">Á¨¨3Á´†ÔºöÂ≠¶Áøí„Å®ÊúÄÈÅ©Âåñ ‚Üí</a>
</div>

</main>

<footer>
    <p>&copy; 2024 AI Terakoya. All rights reserved.</p>
</footer>

</body>
</html>
