<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第1章：クラスタリングの基礎 - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>第1章：クラスタリングの基礎</h1>
            <p class="subtitle">教師なし学習の基本 - データからパターンを発見する</p>
            <div class="meta">
                <span class="meta-item">📖 読了時間: 20-25分</span>
                <span class="meta-item">📊 難易度: 初級</span>
                <span class="meta-item">💻 コード例: 12個</span>
                <span class="meta-item">📝 演習問題: 5問</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>学習目標</h2>
<p>この章を読むことで、以下を習得できます：</p>
<ul>
<li>✅ 教師なし学習とクラスタリングの概念を理解する</li>
<li>✅ K-meansアルゴリズムを実装できる</li>
<li>✅ 階層的クラスタリングとデンドログラムを使える</li>
<li>✅ DBSCANで密度ベースのクラスタリングができる</li>
<li>✅ シルエット係数などの評価指標を適用できる</li>
<li>✅ 実データでクラスタ分析を実行できる</li>
</ul>

<hr>

<h2>1.1 教師なし学習とクラスタリング</h2>

<h3>教師なし学習とは</h3>
<p><strong>教師なし学習（Unsupervised Learning）</strong>は、ラベルのないデータから構造やパターンを発見する機械学習のアプローチです。</p>

<blockquote>
<p>「正解ラベル $y$ がないデータ $X$ から、隠れた構造や関係性を見つけ出す」</p>
</blockquote>

<h3>教師あり学習 vs 教師なし学習</h3>

<table>
<thead>
<tr>
<th>特徴</th>
<th>教師あり学習</th>
<th>教師なし学習</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>データ</strong></td>
<td>ラベル付き $(X, y)$</td>
<td>ラベルなし $(X)$</td>
</tr>
<tr>
<td><strong>目的</strong></td>
<td>予測モデルの構築</td>
<td>パターン・構造の発見</td>
</tr>
<tr>
<td><strong>タスク例</strong></td>
<td>分類、回帰</td>
<td>クラスタリング、次元削減</td>
</tr>
<tr>
<td><strong>評価</strong></td>
<td>テストデータで評価</td>
<td>内的指標や可視化</td>
</tr>
</tbody>
</table>

<h3>クラスタリングの定義</h3>

<p><strong>クラスタリング（Clustering）</strong>は、類似したデータポイントをグループ（クラスタ）にまとめる手法です。</p>

<p>$$
C = \{C_1, C_2, \ldots, C_k\}
$$</p>

<ul>
<li>各クラスタ $C_i$ は類似度が高いデータの集合</li>
<li>異なるクラスタ間は類似度が低い</li>
</ul>

<h3>実世界の応用例</h3>

<div class="mermaid">
graph LR
    A[クラスタリングの応用] --> B[マーケティング: 顧客セグメンテーション]
    A --> C[生物学: 遺伝子グルーピング]
    A --> D[画像処理: 画像セグメンテーション]
    A --> E[文書分析: トピック抽出]
    A --> F[異常検知: 正常パターンの特定]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#fff3e0
    style D fill:#fff3e0
    style E fill:#fff3e0
    style F fill:#fff3e0
</div>

<hr>

<h2>1.2 K-meansクラスタリング</h2>

<h3>アルゴリズムの概要</h3>

<p><strong>K-means</strong>は最も広く使われるクラスタリング手法で、データを $k$ 個のクラスタに分割します。</p>

<div class="mermaid">
graph TD
    A[1. k個の中心点をランダム初期化] --> B[2. 各点を最近接の中心に割り当て]
    B --> C[3. 各クラスタの中心を再計算]
    C --> D{収束?}
    D -->|No| B
    D -->|Yes| E[クラスタリング完了]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#ffe0b2
    style E fill:#e8f5e9
</div>

<h3>数学的定義</h3>

<p><strong>目的関数</strong>（クラスタ内二乗和）：</p>

<p>$$
J = \sum_{i=1}^{k} \sum_{\mathbf{x} \in C_i} ||\mathbf{x} - \boldsymbol{\mu}_i||^2
$$</p>

<ul>
<li>$C_i$: クラスタ $i$</li>
<li>$\boldsymbol{\mu}_i$: クラスタ $i$ の中心（centroid）</li>
<li>$||\mathbf{x} - \boldsymbol{\mu}_i||$: ユークリッド距離</li>
</ul>

<p><strong>中心の更新式</strong>：</p>

<p>$$
\boldsymbol{\mu}_i = \frac{1}{|C_i|} \sum_{\mathbf{x} \in C_i} \mathbf{x}
$$</p>

<h3>実装例：スクラッチ実装</h3>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

class KMeans:
    """K-meansクラスタリングのスクラッチ実装"""

    def __init__(self, k=3, max_iters=100, random_state=42):
        self.k = k
        self.max_iters = max_iters
        self.random_state = random_state
        self.centroids = None
        self.labels = None

    def fit(self, X):
        """K-meansアルゴリズムを実行"""
        np.random.seed(self.random_state)
        n_samples, n_features = X.shape

        # 1. 中心点をランダムに初期化
        random_indices = np.random.choice(n_samples, self.k, replace=False)
        self.centroids = X[random_indices]

        for iteration in range(self.max_iters):
            # 2. 各点を最近接の中心に割り当て
            distances = self._compute_distances(X)
            new_labels = np.argmin(distances, axis=1)

            # 3. 中心を再計算
            new_centroids = np.array([
                X[new_labels == i].mean(axis=0)
                for i in range(self.k)
            ])

            # 収束判定
            if np.allclose(self.centroids, new_centroids):
                print(f"収束しました (iteration {iteration})")
                break

            self.centroids = new_centroids
            self.labels = new_labels

        return self

    def _compute_distances(self, X):
        """各点と各中心との距離を計算"""
        distances = np.zeros((X.shape[0], self.k))
        for i, centroid in enumerate(self.centroids):
            distances[:, i] = np.linalg.norm(X - centroid, axis=1)
        return distances

    def predict(self, X):
        """新しいデータのクラスタを予測"""
        distances = self._compute_distances(X)
        return np.argmin(distances, axis=1)

    def inertia(self, X):
        """クラスタ内二乗和を計算"""
        distances = self._compute_distances(X)
        min_distances = np.min(distances, axis=1)
        return np.sum(min_distances ** 2)

# データ生成
X, y_true = make_blobs(n_samples=300, centers=4,
                       cluster_std=0.6, random_state=42)

# K-meansの実行
kmeans = KMeans(k=4)
kmeans.fit(X)

print(f"最終的なクラスタ内二乗和: {kmeans.inertia(X):.2f}")

# 可視化
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', alpha=0.6)
plt.title('元のデータ（真のラベル）', fontsize=14)
plt.xlabel('特徴量 1')
plt.ylabel('特徴量 2')

plt.subplot(1, 2, 2)
plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels, cmap='viridis', alpha=0.6)
plt.scatter(kmeans.centroids[:, 0], kmeans.centroids[:, 1],
            c='red', marker='X', s=200, edgecolors='black', linewidths=2,
            label='中心点')
plt.title('K-meansの結果', fontsize=14)
plt.xlabel('特徴量 1')
plt.ylabel('特徴量 2')
plt.legend()

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>収束しました (iteration 6)
最終的なクラスタ内二乗和: 108.45
</code></pre>

<h3>scikit-learnによる実装</h3>

<pre><code class="language-python">from sklearn.cluster import KMeans

# K-meansモデル
kmeans_sklearn = KMeans(n_clusters=4, random_state=42, n_init=10)
kmeans_sklearn.fit(X)

print("scikit-learn K-means:")
print(f"クラスタ内二乗和 (inertia): {kmeans_sklearn.inertia_:.2f}")
print(f"反復回数: {kmeans_sklearn.n_iter_}")
print(f"\n中心点の座標:")
print(kmeans_sklearn.cluster_centers_)
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>scikit-learn K-means:
クラスタ内二乗和 (inertia): 108.45
反復回数: 7

中心点の座標:
[[ 1.83  -8.88]
 [ 2.81   2.85]
 [-9.49   7.27]
 [-3.51  -7.92]]
</code></pre>

<h3>エルボー法による最適なK値の選択</h3>

<p><strong>エルボー法（Elbow Method）</strong>は、異なる $k$ 値でのクラスタ内二乗和をプロットし、「肘」の位置を見つける手法です。</p>

<pre><code class="language-python">inertias = []
K_range = range(1, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)

# 可視化
plt.figure(figsize=(10, 6))
plt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)
plt.xlabel('クラスタ数 (k)', fontsize=12)
plt.ylabel('クラスタ内二乗和 (Inertia)', fontsize=12)
plt.title('エルボー法 - 最適なk値の選択', fontsize=14)
plt.grid(True, alpha=0.3)
plt.axvline(x=4, color='r', linestyle='--', alpha=0.7, label='推奨k=4')
plt.legend()
plt.show()
</code></pre>

<blockquote>
<p><strong>ヒント</strong>: 曲線が「肘」のように曲がる点が最適な $k$ 値です。この例では $k=4$ が適切です。</p>
</blockquote>

<h3>K-meansの長所と短所</h3>

<table>
<thead>
<tr>
<th>長所</th>
<th>短所</th>
</tr>
</thead>
<tbody>
<tr>
<td>シンプルで高速</td>
<td>$k$ を事前に指定する必要がある</td>
</tr>
<tr>
<td>大規模データに適用可能</td>
<td>初期値に依存する（局所最適解）</td>
</tr>
<tr>
<td>実装が容易</td>
<td>球状クラスタしか見つけられない</td>
</tr>
<tr>
<td>解釈しやすい</td>
<td>外れ値に敏感</td>
</tr>
</tbody>
</table>

<hr>

<h2>1.3 階層的クラスタリング</h2>

<h3>概要</h3>

<p><strong>階層的クラスタリング（Hierarchical Clustering）</strong>は、データを階層構造（樹形図）として表現します。</p>

<p>2つのアプローチ：</p>
<ul>
<li><strong>凝集型（Agglomerative）</strong>: ボトムアップ - 各点を個別クラスタとして開始し、統合</li>
<li><strong>分割型（Divisive）</strong>: トップダウン - 全データを1つのクラスタとして開始し、分割</li>
</ul>

<h3>凝集型アルゴリズム</h3>

<div class="mermaid">
graph TD
    A[各データ点を個別クラスタに] --> B[最も近いクラスタ対を見つける]
    B --> C[2つのクラスタを統合]
    C --> D{1つのクラスタ?}
    D -->|No| B
    D -->|Yes| E[デンドログラム完成]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#ffe0b2
    style E fill:#e8f5e9
</div>

<h3>連結法（Linkage Methods）</h3>

<p>クラスタ間の距離を計算する方法：</p>

<table>
<thead>
<tr>
<th>連結法</th>
<th>距離の定義</th>
<th>特徴</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>単連結</strong><br>(Single)</td>
<td>$\min(\text{dist}(a, b))$</td>
<td>最も近い点同士の距離。長い鎖状クラスタを作る</td>
</tr>
<tr>
<td><strong>完全連結</strong><br>(Complete)</td>
<td>$\max(\text{dist}(a, b))$</td>
<td>最も遠い点同士の距離。コンパクトなクラスタ</td>
</tr>
<tr>
<td><strong>平均連結</strong><br>(Average)</td>
<td>$\text{mean}(\text{dist}(a, b))$</td>
<td>全点対の平均距離。バランスが良い</td>
</tr>
<tr>
<td><strong>Ward法</strong><br>(Ward)</td>
<td>クラスタ内分散の増加量</td>
<td>分散を最小化。最もよく使われる</td>
</tr>
</tbody>
</table>

<h3>実装例</h3>

<pre><code class="language-python">from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering

# サンプルデータ生成
np.random.seed(42)
X_small = np.random.randn(20, 2)
X_small[:10] += [3, 3]  # 1つ目のクラスタ
X_small[10:] += [-3, -3]  # 2つ目のクラスタ

# 階層的クラスタリング（Ward法）
linkage_matrix = linkage(X_small, method='ward')

# デンドログラムの描画
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
dendrogram(linkage_matrix)
plt.xlabel('サンプルインデックス', fontsize=12)
plt.ylabel('距離', fontsize=12)
plt.title('デンドログラム（Ward法）', fontsize=14)
plt.grid(True, alpha=0.3, axis='y')

# クラスタリング結果の可視化
agg_clustering = AgglomerativeClustering(n_clusters=2, linkage='ward')
labels = agg_clustering.fit_predict(X_small)

plt.subplot(1, 2, 2)
plt.scatter(X_small[:, 0], X_small[:, 1], c=labels, cmap='viridis',
            s=100, alpha=0.6, edgecolors='black')
plt.xlabel('特徴量 1', fontsize=12)
plt.ylabel('特徴量 2', fontsize=12)
plt.title('階層的クラスタリング結果', fontsize=14)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3>異なる連結法の比較</h3>

<pre><code class="language-python"># 複雑な形状のデータ生成
from sklearn.datasets import make_moons
X_moons, _ = make_moons(n_samples=200, noise=0.05, random_state=42)

linkage_methods = ['single', 'complete', 'average', 'ward']

plt.figure(figsize=(14, 10))

for i, method in enumerate(linkage_methods):
    agg = AgglomerativeClustering(n_clusters=2, linkage=method)
    labels = agg.fit_predict(X_moons)

    plt.subplot(2, 2, i+1)
    plt.scatter(X_moons[:, 0], X_moons[:, 1], c=labels, cmap='viridis',
                alpha=0.6, edgecolors='black')
    plt.title(f'{method.capitalize()}連結法', fontsize=12)
    plt.xlabel('特徴量 1')
    plt.ylabel('特徴量 2')
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<blockquote>
<p><strong>注意</strong>: Ward法は球状クラスタに適していますが、単連結法は複雑な形状のクラスタも検出できます。</p>
</blockquote>

<hr>

<h2>1.4 DBSCAN（密度ベースクラスタリング）</h2>

<h3>アルゴリズムの概要</h3>

<p><strong>DBSCAN（Density-Based Spatial Clustering of Applications with Noise）</strong>は、密度が高い領域をクラスタとして識別します。</p>

<p><strong>主要な概念</strong>：</p>
<ul>
<li><strong>コア点（Core Point）</strong>: 半径 $\varepsilon$ 内に $\text{minPts}$ 個以上の点がある</li>
<li><strong>境界点（Border Point）</strong>: コア点の近傍にあるが、コア点ではない</li>
<li><strong>ノイズ点（Noise Point）</strong>: どのクラスタにも属さない</li>
</ul>

<h3>パラメータ</h3>

<table>
<thead>
<tr>
<th>パラメータ</th>
<th>説明</th>
<th>選択方法</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\varepsilon$ (eps)</td>
<td>近傍の半径</td>
<td>k距離グラフで決定</td>
</tr>
<tr>
<td>minPts</td>
<td>コア点となる最小点数</td>
<td>通常、次元数×2 または 4以上</td>
</tr>
</tbody>
</table>

<h3>アルゴリズムの流れ</h3>

<div class="mermaid">
graph TD
    A[未訪問点を選択] --> B{コア点?}
    B -->|Yes| C[新しいクラスタを作成]
    C --> D[近傍の全点を探索]
    D --> E[クラスタを拡張]
    E --> A
    B -->|No| F{境界点?}
    F -->|Yes| G[既存クラスタに追加]
    F -->|No| H[ノイズとしてマーク]
    G --> A
    H --> A
    A --> I{全点訪問?}
    I -->|No| A
    I -->|Yes| J[完了]

    style A fill:#e3f2fd
    style C fill:#e8f5e9
    style H fill:#ffebee
    style J fill:#f3e5f5
</div>

<h3>実装例</h3>

<pre><code class="language-python">from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons

# 複雑な形状のデータ生成
X_moons, _ = make_moons(n_samples=300, noise=0.05, random_state=42)

# DBSCANの適用
dbscan = DBSCAN(eps=0.3, min_samples=5)
labels = dbscan.fit_predict(X_moons)

# ノイズ点の数
n_noise = list(labels).count(-1)
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)

print(f"クラスタ数: {n_clusters}")
print(f"ノイズ点数: {n_noise}")

# 可視化
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(X_moons[:, 0], X_moons[:, 1], alpha=0.6)
plt.title('元のデータ', fontsize=14)
plt.xlabel('特徴量 1')
plt.ylabel('特徴量 2')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(X_moons[:, 0], X_moons[:, 1], c=labels, cmap='viridis',
            alpha=0.6, edgecolors='black')
plt.title(f'DBSCAN結果 ({n_clusters}クラスタ, {n_noise}ノイズ点)', fontsize=14)
plt.xlabel('特徴量 1')
plt.ylabel('特徴量 2')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>クラスタ数: 2
ノイズ点数: 4
</code></pre>

<h3>パラメータの影響</h3>

<pre><code class="language-python"># 異なるepsでの比較
eps_values = [0.15, 0.2, 0.3, 0.5]

plt.figure(figsize=(14, 10))

for i, eps in enumerate(eps_values):
    dbscan = DBSCAN(eps=eps, min_samples=5)
    labels = dbscan.fit_predict(X_moons)

    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    n_noise = list(labels).count(-1)

    plt.subplot(2, 2, i+1)
    plt.scatter(X_moons[:, 0], X_moons[:, 1], c=labels, cmap='viridis',
                alpha=0.6, edgecolors='black')
    plt.title(f'eps={eps} ({n_clusters}クラスタ, {n_noise}ノイズ)', fontsize=12)
    plt.xlabel('特徴量 1')
    plt.ylabel('特徴量 2')
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3>K-means vs DBSCAN</h3>

<pre><code class="language-python"># K-meansとDBSCANの比較
kmeans_moons = KMeans(n_clusters=2, random_state=42)
kmeans_labels = kmeans_moons.fit_predict(X_moons)

dbscan_moons = DBSCAN(eps=0.3, min_samples=5)
dbscan_labels = dbscan_moons.fit_predict(X_moons)

plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(X_moons[:, 0], X_moons[:, 1], c=kmeans_labels,
            cmap='viridis', alpha=0.6, edgecolors='black')
plt.scatter(kmeans_moons.cluster_centers_[:, 0],
            kmeans_moons.cluster_centers_[:, 1],
            c='red', marker='X', s=200, edgecolors='black', linewidths=2)
plt.title('K-means（球状クラスタを仮定）', fontsize=14)
plt.xlabel('特徴量 1')
plt.ylabel('特徴量 2')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(X_moons[:, 0], X_moons[:, 1], c=dbscan_labels,
            cmap='viridis', alpha=0.6, edgecolors='black')
plt.title('DBSCAN（任意形状を検出）', fontsize=14)
plt.xlabel('特徴量 1')
plt.ylabel('特徴量 2')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<blockquote>
<p><strong>重要</strong>: DBSCANは複雑な形状のクラスタを検出でき、ノイズも扱えます。K-meansは球状クラスタしか見つけられません。</p>
</blockquote>

<hr>

<h2>1.5 クラスタリングの評価指標</h2>

<h3>評価の難しさ</h3>

<p>教師なし学習では「正解」がないため、評価が困難です。</p>

<ul>
<li><strong>内的評価</strong>: クラスタの品質を内部データで評価</li>
<li><strong>外的評価</strong>: 真のラベルがある場合の比較</li>
</ul>

<h3>シルエット係数（Silhouette Score）</h3>

<p>各データ点のクラスタ適合度を測定：</p>

<p>$$
s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
$$</p>

<ul>
<li>$a(i)$: 点 $i$ と同じクラスタ内の他の点との平均距離</li>
<li>$b(i)$: 点 $i$ と最も近い別クラスタ内の点との平均距離</li>
<li>範囲: $[-1, 1]$ （1に近いほど良い）</li>
</ul>

<pre><code class="language-python">from sklearn.metrics import silhouette_score, silhouette_samples

# データ生成
X, y_true = make_blobs(n_samples=300, centers=4,
                       cluster_std=0.6, random_state=42)

# 異なるk値でのシルエット係数
silhouette_scores = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X)
    score = silhouette_score(X, labels)
    silhouette_scores.append(score)
    print(f"k={k}: シルエット係数 = {score:.3f}")

# 可視化
plt.figure(figsize=(10, 6))
plt.plot(K_range, silhouette_scores, 'bo-', linewidth=2, markersize=8)
plt.xlabel('クラスタ数 (k)', fontsize=12)
plt.ylabel('シルエット係数', fontsize=12)
plt.title('シルエット係数によるクラスタ数の評価', fontsize=14)
plt.grid(True, alpha=0.3)
plt.axvline(x=4, color='r', linestyle='--', alpha=0.7, label='最適k=4')
plt.legend()
plt.show()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>k=2: シルエット係数 = 0.617
k=3: シルエット係数 = 0.588
k=4: シルエット係数 = 0.651
k=5: シルエット係数 = 0.563
k=6: シルエット係数 = 0.542
k=7: シルエット係数 = 0.528
k=8: シルエット係数 = 0.515
k=9: シルエット係数 = 0.503
k=10: シルエット係数 = 0.491
</code></pre>

<h3>シルエットプロット</h3>

<pre><code class="language-python">from sklearn.metrics import silhouette_samples
import matplotlib.cm as cm

# k=4でのシルエットプロット
kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
labels = kmeans.fit_predict(X)
silhouette_vals = silhouette_samples(X, labels)

plt.figure(figsize=(10, 6))
y_lower = 10

for i in range(4):
    cluster_silhouette_vals = silhouette_vals[labels == i]
    cluster_silhouette_vals.sort()

    size_cluster_i = cluster_silhouette_vals.shape[0]
    y_upper = y_lower + size_cluster_i

    color = cm.viridis(float(i) / 4)
    plt.fill_betweenx(np.arange(y_lower, y_upper),
                      0, cluster_silhouette_vals,
                      facecolor=color, edgecolor=color, alpha=0.7)

    plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
    y_lower = y_upper + 10

plt.axvline(x=silhouette_score(X, labels), color='red', linestyle='--',
            label=f'平均: {silhouette_score(X, labels):.3f}')
plt.xlabel('シルエット係数', fontsize=12)
plt.ylabel('クラスタ', fontsize=12)
plt.title('シルエットプロット (k=4)', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

<h3>Davies-Bouldin指数</h3>

<p>クラスタの分離度を測定（低いほど良い）：</p>

<p>$$
DB = \frac{1}{k} \sum_{i=1}^{k} \max_{i \neq j} \left( \frac{\sigma_i + \sigma_j}{d(c_i, c_j)} \right)
$$</p>

<ul>
<li>$\sigma_i$: クラスタ $i$ の内部距離</li>
<li>$d(c_i, c_j)$: クラスタ中心間の距離</li>
</ul>

<pre><code class="language-python">from sklearn.metrics import davies_bouldin_score

db_scores = []

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X)
    score = davies_bouldin_score(X, labels)
    db_scores.append(score)
    print(f"k={k}: Davies-Bouldin指数 = {score:.3f}")

plt.figure(figsize=(10, 6))
plt.plot(K_range, db_scores, 'bo-', linewidth=2, markersize=8)
plt.xlabel('クラスタ数 (k)', fontsize=12)
plt.ylabel('Davies-Bouldin指数', fontsize=12)
plt.title('Davies-Bouldin指数（低いほど良い）', fontsize=14)
plt.grid(True, alpha=0.3)
plt.axvline(x=4, color='r', linestyle='--', alpha=0.7, label='最適k=4')
plt.legend()
plt.show()
</code></pre>

<h3>Calinski-Harabasz指数</h3>

<p>クラスタ間分散とクラスタ内分散の比（高いほど良い）：</p>

<p>$$
CH = \frac{\text{tr}(B_k)}{\text{tr}(W_k)} \times \frac{n - k}{k - 1}
$$</p>

<ul>
<li>$B_k$: クラスタ間分散行列</li>
<li>$W_k$: クラスタ内分散行列</li>
</ul>

<pre><code class="language-python">from sklearn.metrics import calinski_harabasz_score

ch_scores = []

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X)
    score = calinski_harabasz_score(X, labels)
    ch_scores.append(score)
    print(f"k={k}: Calinski-Harabasz指数 = {score:.2f}")

plt.figure(figsize=(10, 6))
plt.plot(K_range, ch_scores, 'bo-', linewidth=2, markersize=8)
plt.xlabel('クラスタ数 (k)', fontsize=12)
plt.ylabel('Calinski-Harabasz指数', fontsize=12)
plt.title('Calinski-Harabasz指数（高いほど良い）', fontsize=14)
plt.grid(True, alpha=0.3)
plt.axvline(x=4, color='r', linestyle='--', alpha=0.7, label='最適k=4')
plt.legend()
plt.show()
</code></pre>

<h3>評価指標のまとめ</h3>

<table>
<thead>
<tr>
<th>指標</th>
<th>範囲</th>
<th>最適値</th>
<th>特徴</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>シルエット係数</strong></td>
<td>[-1, 1]</td>
<td>高いほど良い</td>
<td>各点の適合度を測定</td>
</tr>
<tr>
<td><strong>Davies-Bouldin</strong></td>
<td>[0, ∞)</td>
<td>低いほど良い</td>
<td>クラスタの分離度</td>
</tr>
<tr>
<td><strong>Calinski-Harabasz</strong></td>
<td>[0, ∞)</td>
<td>高いほど良い</td>
<td>分散比に基づく</td>
</tr>
</tbody>
</table>

<hr>

<h2>1.6 実践例：顧客セグメンテーション</h2>

<h3>データ準備</h3>

<pre><code class="language-python"># 顧客データの生成（購入金額と頻度）
np.random.seed(42)

# 3つの顧客セグメント
segment1 = np.random.randn(100, 2) * [10, 2] + [30, 5]   # 高価格・低頻度
segment2 = np.random.randn(100, 2) * [5, 5] + [50, 20]   # 中価格・高頻度
segment3 = np.random.randn(100, 2) * [8, 3] + [80, 10]   # 高価格・中頻度

X_customers = np.vstack([segment1, segment2, segment3])

# カラム名
feature_names = ['平均購入金額（千円）', '月間購入回数']
</code></pre>

<h3>データの標準化</h3>

<pre><code class="language-python">from sklearn.preprocessing import StandardScaler

# 標準化（異なるスケールの特徴量を扱う際に重要）
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_customers)

print("元のデータの統計:")
print(f"平均購入金額: 平均={X_customers[:, 0].mean():.1f}, 標準偏差={X_customers[:, 0].std():.1f}")
print(f"月間購入回数: 平均={X_customers[:, 1].mean():.1f}, 標準偏差={X_customers[:, 1].std():.1f}")

print("\n標準化後のデータの統計:")
print(f"平均購入金額: 平均={X_scaled[:, 0].mean():.3f}, 標準偏差={X_scaled[:, 0].std():.3f}")
print(f"月間購入回数: 平均={X_scaled[:, 1].mean():.3f}, 標準偏差={X_scaled[:, 1].std():.3f}")
</code></pre>

<h3>最適なクラスタ数の決定</h3>

<pre><code class="language-python"># エルボー法とシルエット係数の両方を評価
inertias = []
silhouettes = []
K_range = range(2, 9)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)
    silhouettes.append(silhouette_score(X_scaled, kmeans.labels_))

# 可視化
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)
axes[0].set_xlabel('クラスタ数 (k)', fontsize=12)
axes[0].set_ylabel('Inertia', fontsize=12)
axes[0].set_title('エルボー法', fontsize=14)
axes[0].grid(True, alpha=0.3)
axes[0].axvline(x=3, color='r', linestyle='--', alpha=0.7)

axes[1].plot(K_range, silhouettes, 'go-', linewidth=2, markersize=8)
axes[1].set_xlabel('クラスタ数 (k)', fontsize=12)
axes[1].set_ylabel('シルエット係数', fontsize=12)
axes[1].set_title('シルエット係数', fontsize=14)
axes[1].grid(True, alpha=0.3)
axes[1].axvline(x=3, color='r', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()
</code></pre>

<h3>最終的なクラスタリングと解釈</h3>

<pre><code class="language-python"># k=3でクラスタリング
kmeans_final = KMeans(n_clusters=3, random_state=42, n_init=10)
labels = kmeans_final.fit_predict(X_scaled)

# 元のスケールで中心点を計算
centers_original = scaler.inverse_transform(kmeans_final.cluster_centers_)

# 各セグメントの特徴を分析
print("=== 顧客セグメント分析 ===\n")
for i in range(3):
    cluster_data = X_customers[labels == i]
    print(f"セグメント {i+1} (n={len(cluster_data)}人):")
    print(f"  平均購入金額: {cluster_data[:, 0].mean():.1f}千円")
    print(f"  月間購入回数: {cluster_data[:, 1].mean():.1f}回")
    print()

# 可視化
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(X_customers[:, 0], X_customers[:, 1], c=labels,
            cmap='viridis', alpha=0.6, edgecolors='black', s=50)
plt.scatter(centers_original[:, 0], centers_original[:, 1],
            c='red', marker='X', s=300, edgecolors='black', linewidths=2,
            label='セグメント中心')
plt.xlabel('平均購入金額（千円）', fontsize=12)
plt.ylabel('月間購入回数', fontsize=12)
plt.title('顧客セグメンテーション結果', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

# セグメントサイズの可視化
plt.subplot(1, 2, 2)
segment_sizes = [np.sum(labels == i) for i in range(3)]
segment_names = [f'セグメント {i+1}' for i in range(3)]
colors = plt.cm.viridis(np.linspace(0, 1, 3))
plt.pie(segment_sizes, labels=segment_names, autopct='%1.1f%%',
        colors=colors, startangle=90)
plt.title('セグメント分布', fontsize=14)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== 顧客セグメント分析 ===

セグメント 1 (n=100人):
  平均購入金額: 29.8千円
  月間購入回数: 5.0回

セグメント 2 (n=100人):
  平均購入金額: 50.2千円
  月間購入回数: 20.1回

セグメント 3 (n=100人):
  平均購入金額: 79.7千円
  月間購入回数: 10.2回
</code></pre>

<blockquote>
<p><strong>ビジネスへの示唆</strong>:</p>
<ul>
<li><strong>セグメント1</strong>: 低価格・低頻度 → 新規顧客または機会的購入者</li>
<li><strong>セグメント2</strong>: 中価格・高頻度 → ロイヤル顧客（最重要）</li>
<li><strong>セグメント3</strong>: 高価格・中頻度 → プレミアム顧客</li>
</ul>
</blockquote>

<hr>

<h2>1.7 本章のまとめ</h2>

<h3>学んだこと</h3>

<ol>
<li><p><strong>教師なし学習の概念</strong></p>
<ul>
<li>ラベルなしデータからパターンを発見</li>
<li>クラスタリング、次元削減、異常検知</li>
</ul></li>
<li><p><strong>K-meansクラスタリング</strong></p>
<ul>
<li>中心点ベースの分割手法</li>
<li>エルボー法で最適な $k$ を選択</li>
<li>高速だが球状クラスタが前提</li>
</ul></li>
<li><p><strong>階層的クラスタリング</strong></p>
<ul>
<li>デンドログラムで階層構造を可視化</li>
<li>連結法（単連結、完全連結、Ward法など）</li>
<li>クラスタ数を事後的に決定可能</li>
</ul></li>
<li><p><strong>DBSCAN</strong></p>
<ul>
<li>密度ベースのクラスタリング</li>
<li>任意形状のクラスタを検出</li>
<li>ノイズを自動的に識別</li>
</ul></li>
<li><p><strong>評価指標</strong></p>
<ul>
<li>シルエット係数: クラスタ適合度</li>
<li>Davies-Bouldin指数: 分離度</li>
<li>Calinski-Harabasz指数: 分散比</li>
</ul></li>
</ol>

<h3>アルゴリズムの選択指針</h3>

<table>
<thead>
<tr>
<th>状況</th>
<th>推奨手法</th>
<th>理由</th>
</tr>
</thead>
<tbody>
<tr>
<td>大規模データ</td>
<td>K-means</td>
<td>高速で効率的</td>
</tr>
<tr>
<td>球状クラスタ</td>
<td>K-means</td>
<td>最適な性能</td>
</tr>
<tr>
<td>任意形状</td>
<td>DBSCAN, 単連結法</td>
<td>複雑な形状に対応</td>
</tr>
<tr>
<td>ノイズあり</td>
<td>DBSCAN</td>
<td>ノイズを自動識別</td>
</tr>
<tr>
<td>階層構造</td>
<td>階層的クラスタリング</td>
<td>デンドログラム分析</td>
</tr>
<tr>
<td>クラスタ数不明</td>
<td>DBSCAN, 階層的</td>
<td>自動的に決定</td>
</tr>
</tbody>
</table>

<h3>次の章へ</h3>

<p>第2章では、<strong>次元削減手法</strong>を学びます：</p>
<ul>
<li>主成分分析（PCA）</li>
<li>t-SNE</li>
<li>UMAP</li>
<li>オートエンコーダ</li>
</ul>

<hr>

<h2>演習問題</h2>

<h3>問題1（難易度：easy）</h3>
<p>教師あり学習と教師なし学習の違いを3つ挙げてください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>
<ol>
<li><strong>データ</strong>: 教師あり学習はラベル付きデータ、教師なし学習はラベルなしデータ</li>
<li><strong>目的</strong>: 教師あり学習は予測、教師なし学習はパターン発見</li>
<li><strong>評価</strong>: 教師あり学習はテストデータで評価、教師なし学習は内的指標や可視化</li>
</ol>

</details>

<h3>問題2（難易度：medium）</h3>
<p>以下のデータでK-meansクラスタリング（k=2）を実装し、各クラスタの中心を求めてください。</p>

<pre><code class="language-python">X = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])
</code></pre>

<details>
<summary>解答例</summary>

<pre><code class="language-python">import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

X = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])

# K-meansクラスタリング
kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
labels = kmeans.fit_predict(X)

print("クラスタラベル:", labels)
print("\nクラスタ中心:")
print(kmeans.cluster_centers_)

# 可視化
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis',
            s=100, edgecolors='black')
plt.scatter(kmeans.cluster_centers_[:, 0],
            kmeans.cluster_centers_[:, 1],
            c='red', marker='X', s=300, edgecolors='black', linewidths=2)
plt.xlabel('特徴量 1')
plt.ylabel('特徴量 2')
plt.title('K-meansクラスタリング結果')
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>クラスタラベル: [0 0 1 1 0 1]

クラスタ中心:
[[1.17 1.47]
 [7.33 9.  ]]
</code></pre>

</details>

<h3>問題3（難易度：medium）</h3>
<p>シルエット係数が -0.5 の場合、そのデータ点についてどのようなことが言えますか？</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>
<p>シルエット係数が負の値（-0.5）ということは：</p>
<ul>
<li><strong>間違ったクラスタに割り当てられている可能性が高い</strong></li>
<li>$a(i) > b(i)$ であり、同じクラスタ内の点との距離が、別のクラスタとの距離よりも大きい</li>
<li>そのデータ点は別のクラスタに属すべき</li>
</ul>

<p><strong>シルエット係数の解釈</strong>：</p>
<ul>
<li>$s(i) \approx 1$: 適切に分類されている</li>
<li>$s(i) \approx 0$: クラスタ境界上にある</li>
<li>$s(i) < 0$: 間違ったクラスタに割り当てられている</li>
</ul>

</details>

<h3>問題4（難易度：hard）</h3>
<p>K-meansとDBSCANの長所と短所を比較し、どのような場合にそれぞれを使うべきか説明してください。</p>

<details>
<summary>解答例</summary>

<p><strong>K-means</strong>：</p>

<p>長所：</p>
<ul>
<li>高速で大規模データに適用可能</li>
<li>実装がシンプル</li>
<li>結果が解釈しやすい</li>
</ul>

<p>短所：</p>
<ul>
<li>クラスタ数 $k$ を事前に指定する必要がある</li>
<li>球状クラスタしか検出できない</li>
<li>外れ値に敏感</li>
<li>初期値に依存する</li>
</ul>

<p><strong>DBSCAN</strong>：</p>

<p>長所：</p>
<ul>
<li>任意形状のクラスタを検出可能</li>
<li>ノイズを自動的に識別</li>
<li>クラスタ数を自動決定</li>
<li>外れ値に頑健</li>
</ul>

<p>短所：</p>
<ul>
<li>パラメータ（eps, min_samples）の調整が難しい</li>
<li>密度が異なるクラスタの検出が困難</li>
<li>高次元データで性能が低下</li>
</ul>

<p><strong>使い分け</strong>：</p>

<table>
<thead>
<tr>
<th>状況</th>
<th>推奨手法</th>
</tr>
</thead>
<tbody>
<tr>
<td>球状クラスタ、クラスタ数が既知</td>
<td>K-means</td>
</tr>
<tr>
<td>複雑な形状、ノイズあり</td>
<td>DBSCAN</td>
</tr>
<tr>
<td>大規模データ、速度重視</td>
<td>K-means</td>
</tr>
<tr>
<td>クラスタ数が不明</td>
<td>DBSCAN</td>
</tr>
<tr>
<td>密度ベースの定義が自然</td>
<td>DBSCAN</td>
</tr>
</tbody>
</table>

</details>

<h3>問題5（難易度：hard）</h3>
<p>以下のコードを完成させ、階層的クラスタリングでデンドログラムを描画してください。</p>

<pre><code class="language-python">from scipy.cluster.hierarchy import dendrogram, linkage
import numpy as np
import matplotlib.pyplot as plt

# データ生成
np.random.seed(42)
X = np.random.randn(15, 2)

# ここに実装
</code></pre>

<details>
<summary>解答例</summary>

<pre><code class="language-python">from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering
import numpy as np
import matplotlib.pyplot as plt

# データ生成
np.random.seed(42)
X = np.random.randn(15, 2)

# 階層的クラスタリング（Ward法）
linkage_matrix = linkage(X, method='ward')

# デンドログラムの描画
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
dendrogram(linkage_matrix)
plt.xlabel('サンプルインデックス', fontsize=12)
plt.ylabel('距離', fontsize=12)
plt.title('デンドログラム（Ward法）', fontsize=14)
plt.grid(True, alpha=0.3, axis='y')

# k=3でクラスタリング
agg = AgglomerativeClustering(n_clusters=3, linkage='ward')
labels = agg.fit_predict(X)

plt.subplot(1, 2, 2)
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis',
            s=100, edgecolors='black', alpha=0.7)
for i, (x, y) in enumerate(X):
    plt.annotate(str(i), (x, y), fontsize=9, alpha=0.7)
plt.xlabel('特徴量 1', fontsize=12)
plt.ylabel('特徴量 2', fontsize=12)
plt.title('階層的クラスタリング結果 (k=3)', fontsize=14)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("クラスタラベル:", labels)
print("\n各クラスタのサイズ:")
for i in range(3):
    print(f"クラスタ {i}: {np.sum(labels == i)}個")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>クラスタラベル: [2 0 1 2 0 1 0 2 1 0 2 1 0 1 2]

各クラスタのサイズ:
クラスタ 0: 6個
クラスタ 1: 5個
クラスタ 2: 4個
</code></pre>

</details>

<hr>

<h2>参考文献</h2>

<ol>
<li>MacQueen, J. (1967). "Some methods for classification and analysis of multivariate observations". <em>Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</em>.</li>
<li>Ester, M., Kriegel, H. P., Sander, J., & Xu, X. (1996). "A density-based algorithm for discovering clusters in large spatial databases with noise". <em>KDD</em>.</li>
<li>Kaufman, L., & Rousseeuw, P. J. (1990). <em>Finding Groups in Data: An Introduction to Cluster Analysis</em>. Wiley.</li>
<li>Hastie, T., Tibshirani, R., & Friedman, J. (2009). <em>The Elements of Statistical Learning</em>. Springer.</li>
</ol>

<div class="navigation">
    <a href="index.html" class="nav-button">← シリーズ目次</a>
    <a href="chapter2-dimensionality-reduction.html" class="nav-button">次の章: 次元削減 →</a>
</div>

    </main>

    <footer>
        <p><strong>作成者</strong>: AI Terakoya Content Team</p>
        <p><strong>監修</strong>: Dr. Yusuke Hashimoto（東北大学）</p>
        <p><strong>バージョン</strong>: 1.0 | <strong>作成日</strong>: 2025-10-20</p>
        <p><strong>ライセンス</strong>: Creative Commons BY 4.0</p>
        <p>© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
