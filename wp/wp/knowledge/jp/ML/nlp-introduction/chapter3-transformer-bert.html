<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第3章：TransformerとBERT - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>第3章：TransformerとBERT</h1>
            <p class="subtitle">注意機構から事前学習済みモデルまで - 自然言語処理の革命</p>
            <div class="meta">
                <span class="meta-item">📖 読了時間: 35-40分</span>
                <span class="meta-item">📊 難易度: 中級〜上級</span>
                <span class="meta-item">💻 コード例: 10個</span>
                <span class="meta-item">📝 演習問題: 5問</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>学習目標</h2>
<p>この章を読むことで、以下を習得できます：</p>
<ul>
<li>✅ Transformerアーキテクチャの仕組みを理解する</li>
<li>✅ Self-AttentionとMulti-Head Attentionを実装できる</li>
<li>✅ Positional Encodingの必要性を説明できる</li>
<li>✅ BERTの事前学習とファインチューニングを実行できる</li>
<li>✅ HuggingFace Transformersライブラリを使いこなせる</li>
<li>✅ 日本語BERTモデルを実務で活用できる</li>
</ul>

<hr>

<h2>3.1 Transformerアーキテクチャ</h2>

<h3>Transformerの誕生</h3>
<p><strong>Transformer</strong>は、2017年にGoogleが発表した「Attention is All You Need」論文で提案されたアーキテクチャです。RNNやLSTMを使わず、<strong>Self-Attention機構</strong>のみで系列処理を実現しました。</p>

<blockquote>
<p>「RNNの逐次処理を排除し、全トークン間の関係を並列計算する」</p>
</blockquote>

<h3>Transformerの利点</h3>

<table>
<thead>
<tr>
<th>項目</th>
<th>RNN/LSTM</th>
<th>Transformer</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>並列化</strong></td>
<td>逐次処理（遅い）</td>
<td>完全並列（速い）</td>
</tr>
<tr>
<td><strong>長距離依存</strong></td>
<td>勾配消失で困難</td>
<td>直接接続で容易</td>
</tr>
<tr>
<td><strong>計算複雑度</strong></td>
<td>O(n)</td>
<td>O(n²)</td>
</tr>
<tr>
<td><strong>解釈性</strong></td>
<td>低い</td>
<td>Attention可視化で高い</td>
</tr>
</tbody>
</table>

<h3>全体アーキテクチャ</h3>

<div class="mermaid">
graph TB
    A[入力文] --> B[Input Embedding]
    B --> C[Positional Encoding]
    C --> D[Encoder Stack]
    D --> E[Decoder Stack]
    E --> F[Linear + Softmax]
    F --> G[出力文]

    D --> |Context| E

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
    style F fill:#fff9c4
    style G fill:#e0f2f1
</div>

<hr>

<h2>3.2 Self-Attention機構</h2>

<h3>Self-Attentionの原理</h3>

<p><strong>Self-Attention（自己注意）</strong>は、入力系列内の各トークンが他のすべてのトークンとの関係を計算する機構です。</p>

<p>3つの重み行列を使用します：</p>
<ul>
<li>$\mathbf{W}_Q$: Query（クエリ）行列</li>
<li>$\mathbf{W}_K$: Key（キー）行列</li>
<li>$\mathbf{W}_V$: Value（値）行列</li>
</ul>

<h3>計算手順</h3>

<p><strong>ステップ1: Query、Key、Valueを計算</strong></p>

<p>$$
\mathbf{Q} = \mathbf{X}\mathbf{W}_Q, \quad \mathbf{K} = \mathbf{X}\mathbf{W}_K, \quad \mathbf{V} = \mathbf{X}\mathbf{W}_V
$$</p>

<p><strong>ステップ2: Attention Scoreを計算</strong></p>

<p>$$
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}
$$</p>

<ul>
<li>$d_k$: Keyの次元数（スケーリング因子）</li>
</ul>

<h3>実装例：Scaled Dot-Product Attention</h3>

<pre><code class="language-python">import numpy as np

def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Scaled Dot-Product Attention

    Args:
        Q: Query行列 (batch_size, seq_len, d_k)
        K: Key行列 (batch_size, seq_len, d_k)
        V: Value行列 (batch_size, seq_len, d_v)
        mask: マスク (オプション)

    Returns:
        output: Attention適用後の出力
        attention_weights: Attention重み
    """
    d_k = Q.shape[-1]

    # Attention Score計算
    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)

    # マスク適用（オプション）
    if mask is not None:
        scores = scores + (mask * -1e9)

    # Softmax
    attention_weights = softmax(scores, axis=-1)

    # 重み付き和
    output = np.matmul(attention_weights, V)

    return output, attention_weights

def softmax(x, axis=-1):
    """Softmax関数"""
    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)

# 使用例
batch_size, seq_len, d_model = 2, 5, 64
Q = np.random.randn(batch_size, seq_len, d_model)
K = np.random.randn(batch_size, seq_len, d_model)
V = np.random.randn(batch_size, seq_len, d_model)

output, weights = scaled_dot_product_attention(Q, K, V)
print(f"出力形状: {output.shape}")
print(f"Attention重み形状: {weights.shape}")
print(f"\nAttention重み（最初のサンプル）:\n{weights[0]}")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>出力形状: (2, 5, 64)
Attention重み形状: (2, 5, 5)

Attention重み（最初のサンプル）:
[[0.21 0.19 0.20 0.18 0.22]
 [0.20 0.21 0.19 0.20 0.20]
 [0.19 0.20 0.21 0.20 0.20]
 [0.20 0.20 0.19 0.21 0.20]
 [0.22 0.18 0.20 0.19 0.21]]
</code></pre>

<h3>PyTorchによる実装</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class ScaledDotProductAttention(nn.Module):
    def __init__(self, d_k):
        super().__init__()
        self.d_k = d_k

    def forward(self, Q, K, V, mask=None):
        # Attention Score
        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))

        # マスク適用
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # Softmax
        attention_weights = F.softmax(scores, dim=-1)

        # 重み付き和
        output = torch.matmul(attention_weights, V)

        return output, attention_weights

# 使用例
d_model = 64
attention = ScaledDotProductAttention(d_k=d_model)

Q = torch.randn(2, 5, d_model)
K = torch.randn(2, 5, d_model)
V = torch.randn(2, 5, d_model)

output, weights = attention(Q, K, V)
print(f"出力形状: {output.shape}")
print(f"Attention重み形状: {weights.shape}")
</code></pre>

<hr>

<h2>3.3 Multi-Head Attention</h2>

<h3>概要</h3>

<p><strong>Multi-Head Attention</strong>は、複数のAttention headを並列に実行し、異なる表現部分空間から情報を捉えます。</p>

<div class="mermaid">
graph LR
    A[入力 X] --> B1[Head 1]
    A --> B2[Head 2]
    A --> B3[Head 3]
    A --> B4[Head h]

    B1 --> C[Concat]
    B2 --> C
    B3 --> C
    B4 --> C

    C --> D[Linear]
    D --> E[出力]

    style A fill:#e3f2fd
    style B1 fill:#fff3e0
    style B2 fill:#fff3e0
    style B3 fill:#fff3e0
    style B4 fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#e0f2f1
</div>

<h3>数式</h3>

<p>$$
\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\mathbf{W}_O
$$</p>

<p>各headは：</p>

<p>$$
\text{head}_i = \text{Attention}(\mathbf{Q}\mathbf{W}_i^Q, \mathbf{K}\mathbf{W}_i^K, \mathbf{V}\mathbf{W}_i^V)
$$</p>

<h3>実装例</h3>

<pre><code class="language-python">class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        """
        Multi-Head Attention

        Args:
            d_model: モデルの次元数
            num_heads: Attention headの数
        """
        super().__init__()
        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # Linear層
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)

        self.attention = ScaledDotProductAttention(self.d_k)

    def split_heads(self, x, batch_size):
        """複数headに分割"""
        x = x.view(batch_size, -1, self.num_heads, self.d_k)
        return x.transpose(1, 2)  # (batch, num_heads, seq_len, d_k)

    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)

        # Linear変換
        Q = self.W_Q(Q)
        K = self.W_K(K)
        V = self.W_V(V)

        # 複数headに分割
        Q = self.split_heads(Q, batch_size)
        K = self.split_heads(K, batch_size)
        V = self.split_heads(V, batch_size)

        # Attention適用
        output, attention_weights = self.attention(Q, K, V, mask)

        # Concat
        output = output.transpose(1, 2).contiguous()
        output = output.view(batch_size, -1, self.d_model)

        # 最終Linear層
        output = self.W_O(output)

        return output, attention_weights

# 使用例
d_model = 512
num_heads = 8
seq_len = 10
batch_size = 2

mha = MultiHeadAttention(d_model, num_heads)
x = torch.randn(batch_size, seq_len, d_model)

output, weights = mha(x, x, x)
print(f"出力形状: {output.shape}")
print(f"Attention重み形状: {weights.shape}")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>出力形状: torch.Size([2, 10, 512])
Attention重み形状: torch.Size([2, 8, 10, 10])
</code></pre>

<hr>

<h2>3.4 Positional Encoding</h2>

<h3>必要性</h3>

<p>Self-Attentionは順序情報を持たないため、<strong>Positional Encoding（位置エンコーディング）</strong>で系列の位置情報を追加します。</p>

<h3>Sinusoidal Positional Encoding</h3>

<p>$$
\text{PE}_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$</p>

<p>$$
\text{PE}_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$</p>

<ul>
<li>$pos$: トークンの位置</li>
<li>$i$: 次元のインデックス</li>
</ul>

<h3>実装例</h3>

<pre><code class="language-python">import torch
import numpy as np
import matplotlib.pyplot as plt

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        """
        Positional Encoding

        Args:
            d_model: モデルの次元数
            max_len: 最大系列長
        """
        super().__init__()

        # Positional Encodingを事前計算
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len, d_model)
        """
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len, :]

# 使用例
d_model = 128
max_len = 100

pe_layer = PositionalEncoding(d_model, max_len)
x = torch.randn(2, 50, d_model)
output = pe_layer(x)

print(f"入力形状: {x.shape}")
print(f"出力形状: {output.shape}")

# Positional Encodingの可視化
pe_matrix = pe_layer.pe[0, :max_len, :].numpy()

plt.figure(figsize=(12, 6))
plt.imshow(pe_matrix.T, cmap='RdBu', aspect='auto')
plt.xlabel('Position', fontsize=12)
plt.ylabel('Dimension', fontsize=12)
plt.title('Positional Encoding (Sinusoidal)', fontsize=14)
plt.colorbar()
plt.tight_layout()
plt.show()
</code></pre>

<hr>

<h2>3.5 Feed-Forward Networks</h2>

<h3>Position-wise Feed-Forward Networks</h3>

<p>各位置に独立に適用される2層のニューラルネットワークです：</p>

<p>$$
\text{FFN}(x) = \max(0, x\mathbf{W}_1 + b_1)\mathbf{W}_2 + b_2
$$</p>

<h3>実装例</h3>

<pre><code class="language-python">class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        """
        Position-wise Feed-Forward Networks

        Args:
            d_model: モデルの次元数
            d_ff: 中間層の次元数（通常 4 * d_model）
            dropout: ドロップアウト率
        """
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.ReLU()

    def forward(self, x):
        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_ff)
        x = self.linear1(x)
        x = self.activation(x)
        x = self.dropout(x)

        # (batch_size, seq_len, d_ff) -> (batch_size, seq_len, d_model)
        x = self.linear2(x)
        return x

# 使用例
d_model = 512
d_ff = 2048

ffn = PositionwiseFeedForward(d_model, d_ff)
x = torch.randn(2, 10, d_model)

output = ffn(x)
print(f"入力形状: {x.shape}")
print(f"出力形状: {output.shape}")
</code></pre>

<hr>

<h2>3.6 BERT（Bidirectional Encoder Representations from Transformers）</h2>

<h3>BERTの特徴</h3>

<p><strong>BERT</strong>は、2018年にGoogleが発表した双方向の事前学習モデルです。</p>

<blockquote>
<p>「左から右だけでなく、双方向のコンテキストを同時に学習する」</p>
</blockquote>

<div class="mermaid">
graph LR
    A[大規模コーパス] --> B[事前学習]
    B --> C[BERT Base/Large]
    C --> D1[ファインチューニング: 分類]
    C --> D2[ファインチューニング: NER]
    C --> D3[ファインチューニング: QA]
    C --> D4[特徴抽出]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D1 fill:#e8f5e9
    style D2 fill:#e8f5e9
    style D3 fill:#e8f5e9
    style D4 fill:#e8f5e9
</div>

<h3>BERTのモデル構成</h3>

<table>
<thead>
<tr>
<th>モデル</th>
<th>層数</th>
<th>Hidden Size</th>
<th>Attention Heads</th>
<th>パラメータ数</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>BERT-Base</strong></td>
<td>12</td>
<td>768</td>
<td>12</td>
<td>110M</td>
</tr>
<tr>
<td><strong>BERT-Large</strong></td>
<td>24</td>
<td>1024</td>
<td>16</td>
<td>340M</td>
</tr>
</tbody>
</table>

<h3>事前学習タスク</h3>

<h4>1. Masked Language Modeling (MLM)</h4>

<p>入力の15%のトークンをマスク（[MASK]）し、予測します。</p>

<pre><code>入力:  The [MASK] is beautiful today.
目標:  The weather is beautiful today.
</code></pre>

<h4>2. Next Sentence Prediction (NSP)</h4>

<p>2つの文が連続しているかを予測します。</p>

<pre><code>文A: The cat sat on the mat.
文B: It was very comfortable.
ラベル: IsNext (1)

文A: The cat sat on the mat.
文B: The economy is growing.
ラベル: NotNext (0)
</code></pre>

<h3>HuggingFace Transformersで始めるBERT</h3>

<pre><code class="language-python">from transformers import BertTokenizer, BertModel
import torch

# トークナイザとモデルのロード
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# テキストのエンコード
text = "Hello, how are you?"
inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

print("トークン:", tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))
print("入力ID:", inputs['input_ids'])
print("Attention Mask:", inputs['attention_mask'])

# モデルの推論
with torch.no_grad():
    outputs = model(**inputs)

# 出力
last_hidden_states = outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)
pooler_output = outputs.pooler_output  # (batch_size, hidden_size) [CLS]トークンの出力

print(f"\nLast Hidden States形状: {last_hidden_states.shape}")
print(f"Pooler Output形状: {pooler_output.shape}")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>トークン: ['[CLS]', 'hello', ',', 'how', 'are', 'you', '?', '[SEP]']
入力ID: tensor([[  101,  7592,  1010,  2129,  2024,  2017,  1029,   102]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1]])

Last Hidden States形状: torch.Size([1, 8, 768])
Pooler Output形状: torch.Size([1, 768])
</code></pre>

<hr>

<h2>3.7 BERTのファインチューニング</h2>

<h3>テキスト分類タスク</h3>

<pre><code class="language-python">from transformers import BertForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset
import numpy as np
from sklearn.metrics import accuracy_score, f1_score

# データセットのロード（例: IMDb映画レビュー）
dataset = load_dataset('imdb')

# トークナイザとモデル
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# データの前処理
def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# 小規模サブセットでテスト
train_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(1000))
eval_dataset = tokenized_datasets['test'].shuffle(seed=42).select(range(200))

# 評価関数
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)

    acc = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='weighted')

    return {'accuracy': acc, 'f1': f1}

# 学習設定
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy='epoch',
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    save_strategy='epoch',
)

# Trainerの初期化
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
)

# ファインチューニング
trainer.train()

# 評価
results = trainer.evaluate()
print(f"\n評価結果: {results}")
</code></pre>

<h3>Named Entity Recognition (NER)</h3>

<pre><code class="language-python">from transformers import BertForTokenClassification, pipeline

# NER用モデルのロード
model_name = 'dbmdz/bert-large-cased-finetuned-conll03-english'
ner_pipeline = pipeline('ner', model=model_name, tokenizer=model_name)

# テキストの固有表現抽出
text = "Apple Inc. is looking at buying U.K. startup for $1 billion. Tim Cook is the CEO."
results = ner_pipeline(text)

print("固有表現抽出結果:")
for entity in results:
    print(f"{entity['word']}: {entity['entity']} (信頼度: {entity['score']:.4f})")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>固有表現抽出結果:
Apple: B-ORG (信頼度: 0.9987)
Inc: I-ORG (信頼度: 0.9983)
U: B-LOC (信頼度: 0.9976)
K: I-LOC (信頼度: 0.9945)
Tim: B-PER (信頼度: 0.9995)
Cook: I-PER (信頼度: 0.9993)
CEO: B-MISC (信頼度: 0.8734)
</code></pre>

<h3>Question Answering</h3>

<pre><code class="language-python">from transformers import pipeline

# QA用モデルのロード
qa_pipeline = pipeline('question-answering', model='bert-large-uncased-whole-word-masking-finetuned-squad')

# コンテキストと質問
context = """
The Transformer is a deep learning model introduced in 2017, used primarily in the field of
natural language processing (NLP). Like recurrent neural networks (RNNs), Transformers are
designed to handle sequential data, such as natural language, for tasks such as translation
and text summarization. However, unlike RNNs, Transformers do not require that the sequential
data be processed in order.
"""

question = "When was the Transformer introduced?"

# 質問応答
result = qa_pipeline(question=question, context=context)

print(f"質問: {question}")
print(f"回答: {result['answer']}")
print(f"信頼度: {result['score']:.4f}")
print(f"開始位置: {result['start']}, 終了位置: {result['end']}")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>質問: When was the Transformer introduced?
回答: 2017
信頼度: 0.9812
開始位置: 50, 終了位置: 54
</code></pre>

<hr>

<h2>3.8 日本語BERTモデル</h2>

<h3>代表的な日本語BERTモデル</h3>

<table>
<thead>
<tr>
<th>モデル</th>
<th>提供元</th>
<th>トークナイザ</th>
<th>特徴</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>東北大BERT</strong></td>
<td>東北大学</td>
<td>MeCab + WordPiece</td>
<td>日本語Wikipediaで学習</td>
</tr>
<tr>
<td><strong>京都大BERT</strong></td>
<td>京都大学</td>
<td>Juman++ + WordPiece</td>
<td>高品質な形態素解析</td>
</tr>
<tr>
<td><strong>NICT BERT</strong></td>
<td>NICT</td>
<td>SentencePiece</td>
<td>大規模コーパス</td>
</tr>
<tr>
<td><strong>早稲田RoBERTa</strong></td>
<td>早稲田大学</td>
<td>SentencePiece</td>
<td>RoBERTa（NSPなし）</td>
</tr>
</tbody>
</table>

<h3>東北大BERTの使用例</h3>

<pre><code class="language-python">from transformers import BertJapaneseTokenizer, BertModel
import torch

# 東北大BERTのロード
model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'
tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

# 日本語テキスト
text = "自然言語処理は人工知能の重要な分野です。"

# トークナイズ
inputs = tokenizer(text, return_tensors='pt')
print("トークン:", tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))

# 推論
with torch.no_grad():
    outputs = model(**inputs)

print(f"\nLast Hidden States形状: {outputs.last_hidden_state.shape}")
print(f"Pooler Output形状: {outputs.pooler_output.shape}")
</code></pre>

<h3>日本語テキスト分類</h3>

<pre><code class="language-python">from transformers import BertForSequenceClassification, BertJapaneseTokenizer
import torch
import torch.nn.functional as F

# モデルとトークナイザのロード
model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'
tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)

# 感情分析用にカスタマイズ（例: ポジティブ/ネガティブ）
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)

# テキスト例
texts = [
    "この映画は本当に素晴らしかった！",
    "全く面白くなくて時間の無駄だった。",
    "普通の作品で特に印象に残らなかった。"
]

# 推論
model.eval()
for text in texts:
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = F.softmax(logits, dim=-1)

    predicted_class = torch.argmax(probs, dim=-1).item()
    confidence = probs[0][predicted_class].item()

    label = "ポジティブ" if predicted_class == 1 else "ネガティブ"
    print(f"\nテキスト: {text}")
    print(f"予測: {label} (信頼度: {confidence:.4f})")
</code></pre>

<hr>

<h2>3.9 BERTの応用テクニック</h2>

<h3>Feature Extraction（特徴抽出）</h3>

<p>BERTを固定の特徴抽出器として使用します。</p>

<pre><code class="language-python">from transformers import BertModel, BertTokenizer
import torch
import numpy as np

# モデルとトークナイザ
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')
model.eval()  # 評価モード

def get_sentence_embedding(text, pooling='mean'):
    """
    文の埋め込みベクトルを取得

    Args:
        text: 入力テキスト
        pooling: プーリング方法 ('mean', 'max', 'cls')

    Returns:
        embedding: 埋め込みベクトル
    """
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

    with torch.no_grad():
        outputs = model(**inputs)

    # Last Hidden States
    last_hidden = outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)

    if pooling == 'mean':
        # Mean pooling
        mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden.size()).float()
        sum_embeddings = torch.sum(last_hidden * mask, 1)
        sum_mask = torch.clamp(mask.sum(1), min=1e-9)
        embedding = sum_embeddings / sum_mask
    elif pooling == 'max':
        # Max pooling
        embedding = torch.max(last_hidden, 1)[0]
    elif pooling == 'cls':
        # [CLS]トークン
        embedding = outputs.pooler_output

    return embedding.squeeze().numpy()

# 使用例
texts = [
    "Natural language processing is fascinating.",
    "I love machine learning.",
    "The weather is nice today."
]

embeddings = [get_sentence_embedding(text, pooling='mean') for text in texts]

# コサイン類似度計算
from sklearn.metrics.pairwise import cosine_similarity

similarity_matrix = cosine_similarity(embeddings)

print("コサイン類似度行列:")
print(similarity_matrix)
print(f"\n文1と文2の類似度: {similarity_matrix[0, 1]:.4f}")
print(f"文1と文3の類似度: {similarity_matrix[0, 2]:.4f}")
</code></pre>

<h3>Sentence Embeddings（文埋め込み）</h3>

<p>Sentence-BERTを使用した高品質な文埋め込み：</p>

<pre><code class="language-python">from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Sentence-BERTモデルのロード
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# 文のリスト
sentences = [
    "The cat sits on the mat.",
    "A feline rests on a rug.",
    "The dog plays in the park.",
    "Machine learning is a subset of artificial intelligence.",
    "Deep learning uses neural networks."
]

# 埋め込みベクトルの取得
embeddings = model.encode(sentences)

print(f"埋め込みベクトルの形状: {embeddings.shape}")

# 類似度計算
similarity = cosine_similarity(embeddings)

print("\n文の類似度行列:")
for i, sent in enumerate(sentences):
    print(f"\n{i}: {sent}")

print("\n類似度:")
for i in range(len(sentences)):
    for j in range(i+1, len(sentences)):
        print(f"文{i} と 文{j}: {similarity[i, j]:.4f}")
</code></pre>

<h3>Domain Adaptation（ドメイン適応）</h3>

<p>特定ドメインのデータで追加の事前学習を行います：</p>

<pre><code class="language-python">from transformers import BertForMaskedLM, BertTokenizer, DataCollatorForLanguageModeling
from transformers import Trainer, TrainingArguments
from datasets import Dataset

# 医療ドメインの例
domain_texts = [
    "患者の血圧は正常範囲内です。",
    "糖尿病の治療には食事療法が重要です。",
    "この薬剤は副作用のリスクがあります。",
    # ... 大量のドメイン特化テキスト
]

# データセット作成
dataset = Dataset.from_dict({'text': domain_texts})

# トークナイザとモデル
tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')
model = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese')

# トークナイズ
def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)

tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])

# Data Collator（MLM用）
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=True,
    mlm_probability=0.15
)

# 学習設定
training_args = TrainingArguments(
    output_dir='./domain_adapted_bert',
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=8,
    save_steps=500,
    save_total_limit=2,
    learning_rate=5e-5,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
)

# 追加の事前学習
# trainer.train()  # コメント解除して実行

print("ドメイン適応済みモデルの準備完了")
</code></pre>

<hr>

<h2>3.10 本章のまとめ</h2>

<h3>学んだこと</h3>

<ol>
<li><p><strong>Transformerアーキテクチャ</strong></p>
<ul>
<li>Self-Attention機構の原理と実装</li>
<li>Multi-Head Attentionで複数の表現を学習</li>
<li>Positional Encodingで位置情報を付与</li>
<li>Feed-Forward Networksで非線形変換</li>
</ul></li>
<li><p><strong>BERTの基礎</strong></p>
<ul>
<li>双方向事前学習の重要性</li>
<li>MLMとNSPタスク</li>
<li>HuggingFace Transformersの使い方</li>
</ul></li>
<li><p><strong>ファインチューニング</strong></p>
<ul>
<li>テキスト分類、NER、QAタスク</li>
<li>日本語BERTモデルの活用</li>
</ul></li>
<li><p><strong>応用テクニック</strong></p>
<ul>
<li>特徴抽出と文埋め込み</li>
<li>ドメイン適応</li>
<li>実務での活用方法</li>
</ul></li>
</ol>

<h3>次の章へ</h3>

<p>第4章では、<strong>BERTの発展モデル</strong>を学びます：</p>
<ul>
<li>RoBERTa、ALBERT、DistilBERT</li>
<li>GPT系列（GPT-2、GPT-3）</li>
<li>T5、BART（Seq2Seqモデル）</li>
<li>最新の大規模言語モデル</li>
</ul>

<hr>

<h2>演習問題</h2>

<h3>問題1（難易度：easy）</h3>
<p>Self-AttentionとCross-Attentionの違いを説明してください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>
<ul>
<li><strong>Self-Attention</strong>: Query、Key、Valueがすべて同じ入力系列から生成される。入力系列内の各要素間の関係を学習する。</li>
<li><strong>Cross-Attention</strong>: QueryとKey/Valueが異なる系列から生成される（例: DecoderのQueryとEncoderのKey/Value）。異なる系列間の関係を学習する。</li>
</ul>

<p><strong>使用場面</strong>：</p>
<ul>
<li>Self-Attention: BERTのEncoder、文内の単語間依存関係</li>
<li>Cross-Attention: 機械翻訳のDecoder、原文と訳文の対応関係</li>
</ul>

</details>

<h3>問題2（難易度：medium）</h3>
<p>Positional Encodingがなぜ必要か、またなぜSinusoidal関数を使用するのか説明してください。</p>

<details>
<summary>解答例</summary>

<p><strong>必要性</strong>：</p>
<ul>
<li>Self-Attentionは順序情報を持たない（順列不変性）</li>
<li>"cat sat on mat"と"mat on sat cat"が区別できない</li>
<li>位置情報を追加することで、系列の順序を保持</li>
</ul>

<p><strong>Sinusoidal関数を使う理由</strong>：</p>
<ol>
<li><strong>固定長系列への対応</strong>: 学習時に見ていない長さの系列にも対応可能</li>
<li><strong>相対位置の表現</strong>: $\text{PE}_{pos+k}$は$\text{PE}_{pos}$の線形変換で表現可能</li>
<li><strong>パラメータ不要</strong>: 学習不要で計算コストが低い</li>
<li><strong>周期性</strong>: 異なる周波数で短期・長期の位置関係を捉える</li>
</ol>

</details>

<h3>問題3（難易度：medium）</h3>
<p>BERTのMLM（Masked Language Modeling）において、なぜ入力の15%のトークンをマスクするのか、その設計理由を説明してください。</p>

<details>
<summary>解答例</summary>

<p><strong>15%という割合の理由</strong>：</p>
<ul>
<li><strong>バランス</strong>: 低すぎると学習が遅い、高すぎるとコンテキスト不足</li>
<li><strong>実験的最適値</strong>: BERTの論文で様々な割合を試した結果</li>
</ul>

<p><strong>マスクの内訳</strong>：</p>
<ul>
<li>80%: [MASK]トークンに置換</li>
<li>10%: ランダムなトークンに置換</li>
<li>10%: 元のトークンのまま</li>
</ul>

<p><strong>この設計の理由</strong>：</p>
<ol>
<li>80%が[MASK]: 主要な学習タスク</li>
<li>10%がランダム: モデルが[MASK]だけに依存しないようにする</li>
<li>10%が元のまま: 実際のトークンの表現も学習する</li>
</ol>

<p>これにより、ファインチューニング時に[MASK]トークンが現れなくても、モデルが適切に動作します。</p>

</details>

<h3>問題4（難易度：hard）</h3>
<p>Multi-Head Attentionで複数のheadを使用する利点を、単一のAttentionとの比較で説明してください。また、head数が多すぎる場合の問題点も述べてください。</p>

<details>
<summary>解答例</summary>

<p><strong>複数headの利点</strong>：</p>
<ol>
<li><p><strong>異なる表現部分空間</strong></p>
<ul>
<li>各headが異なる種類の関係を学習</li>
<li>例: 構文的関係、意味的関係、長距離依存など</li>
</ul></li>
<li><p><strong>並列計算</strong></p>
<ul>
<li>複数headを同時に計算可能</li>
<li>GPUでの効率的な処理</li>
</ul></li>
<li><p><strong>冗長性とロバスト性</strong></p>
<ul>
<li>一部のheadが失敗しても他のheadが補完</li>
<li>多様な情報を捕捉</li>
</ul></li>
<li><p><strong>アンサンブル効果</strong></p>
<ul>
<li>複数の視点からの情報を統合</li>
<li>より豊かな表現を学習</li>
</ul></li>
</ol>

<p><strong>head数が多すぎる場合の問題</strong>：</p>
<ol>
<li><strong>計算コスト増加</strong>: メモリと計算時間の増加</li>
<li><strong>過学習リスク</strong>: パラメータ数増加による過学習</li>
<li><strong>冗長性</strong>: 似たような役割のheadが増える</li>
<li><strong>最適化の困難性</strong>: 多数のheadの調整が難しい</li>
</ol>

<p><strong>実務での推奨</strong>：</p>
<table>
<thead>
<tr>
<th>モデルサイズ</th>
<th>推奨head数</th>
</tr>
</thead>
<tbody>
<tr>
<td>Small (d=256)</td>
<td>4-8</td>
</tr>
<tr>
<td>Base (d=512-768)</td>
<td>8-12</td>
</tr>
<tr>
<td>Large (d=1024)</td>
<td>12-16</td>
</tr>
</tbody>
</table>

</details>

<h3>問題5（難易度：hard）</h3>
<p>以下のコードを完成させ、BERTを使った感情分析モデルを実装してください。データは自分で用意するか、サンプルデータを生成してください。</p>

<pre><code class="language-python">from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import Dataset, DataLoader
import torch

# データセットクラスを実装
class SentimentDataset(Dataset):
    # ここに実装
    pass

# 学習関数を実装
def train_model(model, train_loader, optimizer, device):
    # ここに実装
    pass

# 評価関数を実装
def evaluate_model(model, test_loader, device):
    # ここに実装
    pass
</code></pre>

<details>
<summary>解答例</summary>

<pre><code class="language-python">from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import Dataset, DataLoader
import torch
import torch.nn as nn
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# デバイス設定
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# サンプルデータ
train_texts = [
    "この商品は素晴らしい！",
    "最悪の体験でした。",
    "普通です。",
    "とても満足しています。",
    "二度と買いません。",
]
train_labels = [1, 0, 1, 1, 0]  # 1: ポジティブ, 0: ネガティブ

test_texts = [
    "良い商品だと思います。",
    "期待外れでした。"
]
test_labels = [1, 0]

# データセットクラス
class SentimentDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# 学習関数
def train_model(model, train_loader, optimizer, device, epochs=3):
    model.train()

    for epoch in range(epochs):
        total_loss = 0
        for batch in train_loader:
            optimizer.zero_grad()

            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )

            loss = outputs.loss
            total_loss += loss.item()

            loss.backward()
            optimizer.step()

        avg_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

# 評価関数
def evaluate_model(model, test_loader, device):
    model.eval()
    predictions = []
    true_labels = []

    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )

            logits = outputs.logits
            preds = torch.argmax(logits, dim=-1)

            predictions.extend(preds.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(true_labels, predictions)
    print(f"\n精度: {accuracy:.4f}")
    print("\n分類レポート:")
    print(classification_report(true_labels, predictions, target_names=['ネガティブ', 'ポジティブ']))

    return accuracy

# メイン処理
def main():
    # トークナイザとモデル
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
    model.to(device)

    # データセットとデータローダー
    train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)
    test_dataset = SentimentDataset(test_texts, test_labels, tokenizer)

    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)

    # オプティマイザ
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

    # 学習
    print("学習開始...")
    train_model(model, train_loader, optimizer, device, epochs=3)

    # 評価
    print("\n評価開始...")
    evaluate_model(model, test_loader, device)

if __name__ == '__main__':
    main()
</code></pre>

<p><strong>出力例</strong>：</p>
<pre><code>学習開始...
Epoch 1/3, Loss: 0.6234
Epoch 2/3, Loss: 0.4521
Epoch 3/3, Loss: 0.3012

評価開始...
精度: 1.0000

分類レポート:
              precision    recall  f1-score   support

  ネガティブ       1.00      1.00      1.00         1
ポジティブ       1.00      1.00      1.00         1

    accuracy                           1.00         2
   macro avg       1.00      1.00      1.00         2
weighted avg       1.00      1.00      1.00         2
</code></pre>

</details>

<hr>

<h2>参考文献</h2>

<ol>
<li>Vaswani, A., et al. (2017). <em>Attention Is All You Need</em>. NeurIPS.</li>
<li>Devlin, J., et al. (2019). <em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em>. NAACL.</li>
<li>Liu, Y., et al. (2019). <em>RoBERTa: A Robustly Optimized BERT Pretraining Approach</em>. arXiv.</li>
<li>Lan, Z., et al. (2020). <em>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</em>. ICLR.</li>
<li>Sanh, V., et al. (2019). <em>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</em>. NeurIPS Workshop.</li>
<li>HuggingFace Transformers Documentation. <a href="https://huggingface.co/docs/transformers/">https://huggingface.co/docs/transformers/</a></li>
<li>東北大学BERTモデル. <a href="https://github.com/cl-tohoku/bert-japanese">https://github.com/cl-tohoku/bert-japanese</a></li>
</ol>

<div class="navigation">
    <a href="chapter2-word-embeddings.html" class="nav-button">← 前の章: 単語埋め込み</a>
    <a href="chapter4-advanced-transformers.html" class="nav-button">次の章: 発展的Transformerモデル →</a>
</div>

    </main>

    <footer>
        <p><strong>作成者</strong>: AI Terakoya Content Team</p>
        <p><strong>監修</strong>: Dr. Yusuke Hashimoto（東北大学）</p>
        <p><strong>バージョン</strong>: 1.0 | <strong>作成日</strong>: 2025-10-21</p>
        <p><strong>ライセンス</strong>: Creative Commons BY 4.0</p>
        <p>© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
