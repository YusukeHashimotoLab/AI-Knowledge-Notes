<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第4章：音声合成（TTS） - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>第4章：音声合成（TTS）</h1>
            <p class="subtitle">テキストから自然な音声を生成する技術の理解</p>
            <div class="meta">
                <span class="meta-item">📖 読了時間: 30-35分</span>
                <span class="meta-item">📊 難易度: 中級</span>
                <span class="meta-item">💻 コード例: 7個</span>
                <span class="meta-item">📝 演習問題: 5問</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>学習目標</h2>
<p>この章を読むことで、以下を習得できます：</p>
<ul>
<li>✅ 音声合成（TTS）の基本概念とパイプラインを理解する</li>
<li>✅ Tacotron/Tacotron 2のアーキテクチャと動作原理を理解する</li>
<li>✅ FastSpeechによる非自己回帰的TTSの仕組みを学ぶ</li>
<li>✅ 主要なニューラルボコーダの特徴と使い分けを理解する</li>
<li>✅ 最新のTTS技術とその応用を把握する</li>
<li>✅ Pythonライブラリを使った音声合成を実装できる</li>
</ul>

<hr>

<h2>4.1 TTSの基礎</h2>

<h3>Text-to-Speech（TTS）とは</h3>
<p><strong>音声合成（Text-to-Speech, TTS）</strong>は、テキストを自然な音声に変換する技術です。近年のディープラーニングの発展により、人間に近い自然な音声が生成可能になりました。</p>

<blockquote>
<p>「TTSは、言語学的理解と音響モデリングの統合により、書かれた言葉を話し言葉に変換します。」</p>
</blockquote>

<h3>TTSパイプライン</h3>

<div class="mermaid">
graph LR
    A[テキスト入力] --> B[テキスト解析]
    B --> C[言語特徴抽出]
    C --> D[音響モデル]
    D --> E[Mel-スペクトログラム]
    E --> F[Vocoder]
    F --> G[音声波形]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#e8f5e9
    style F fill:#fce4ec
    style G fill:#c8e6c9
</div>

<h3>パイプラインの各段階</h3>

<table>
<thead>
<tr>
<th>段階</th>
<th>役割</th>
<th>出力</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>テキスト解析</strong></td>
<td>正規化、トークン化</td>
<td>正規化されたテキスト</td>
</tr>
<tr>
<td><strong>言語特徴抽出</strong></td>
<td>音素変換、韻律予測</td>
<td>言語特徴ベクトル</td>
</tr>
<tr>
<td><strong>音響モデル</strong></td>
<td>Mel-スペクトログラム生成</td>
<td>Mel-スペクトログラム</td>
</tr>
<tr>
<td><strong>Vocoder</strong></td>
<td>波形生成</td>
<td>音声波形</td>
</tr>
</tbody>
</table>

<h3>Vocoderの役割</h3>

<p><strong>Vocoder（ボコーダー）</strong>は、Mel-スペクトログラムから音声波形を生成するモジュールです。</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
import librosa
import librosa.display

# サンプル音声の読み込み
y, sr = librosa.load(librosa.example('trumpet'), sr=22050, duration=3)

# Mel-スペクトログラムの生成
mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80)
mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)

# 可視化
fig, axes = plt.subplots(2, 1, figsize=(14, 8))

# 波形
axes[0].plot(np.linspace(0, len(y)/sr, len(y)), y)
axes[0].set_xlabel('時間 (秒)')
axes[0].set_ylabel('振幅')
axes[0].set_title('音声波形', fontsize=14)
axes[0].grid(True, alpha=0.3)

# Mel-スペクトログラム
img = librosa.display.specshow(mel_spec_db, x_axis='time', y_axis='mel',
                               sr=sr, ax=axes[1], cmap='viridis')
axes[1].set_title('Mel-スペクトログラム', fontsize=14)
fig.colorbar(img, ax=axes[1], format='%+2.0f dB')

plt.tight_layout()
plt.show()

print("=== TTSにおけるVocoderの役割 ===")
print(f"入力: Mel-スペクトログラム {mel_spec.shape}")
print(f"出力: 音声波形 {y.shape}")
print(f"サンプリングレート: {sr} Hz")
</code></pre>

<h3>Prosody（韻律）と自然性</h3>

<p><strong>韻律（Prosody）</strong>は、音声の自然性を決定する重要な要素です：</p>

<ul>
<li><strong>ピッチ（Pitch）</strong>: 音の高さ、抑揚</li>
<li><strong>デュレーション（Duration）</strong>: 音素の長さ</li>
<li><strong>エネルギー（Energy）</strong>: 音の強さ</li>
<li><strong>リズム（Rhythm）</strong>: 発話のテンポ</li>
</ul>

<h3>評価指標</h3>

<h4>1. MOS（Mean Opinion Score）</h4>

<p><strong>MOS</strong>は、人間による主観評価の平均スコアです。</p>

<table>
<thead>
<tr>
<th>スコア</th>
<th>品質</th>
<th>説明</th>
</tr>
</thead>
<tbody>
<tr>
<td>5</td>
<td>優秀</td>
<td>自然な人間の音声と区別不可</td>
</tr>
<tr>
<td>4</td>
<td>良好</td>
<td>わずかな不自然さがある</td>
</tr>
<tr>
<td>3</td>
<td>普通</td>
<td>明らかに合成音声だが理解可能</td>
</tr>
<tr>
<td>2</td>
<td>劣る</td>
<td>聞き取りにくい部分がある</td>
</tr>
<tr>
<td>1</td>
<td>悪い</td>
<td>理解困難</td>
</tr>
</tbody>
</table>

<h4>2. Naturalness（自然性）</h4>

<p>音声の人間らしさを評価します：</p>
<ul>
<li>韻律の自然さ</li>
<li>音質の滑らかさ</li>
<li>感情表現の豊かさ</li>
</ul>

<hr>

<h2>4.2 Tacotron & Tacotron 2</h2>

<h3>Tacotronの概要</h3>

<p><strong>Tacotron</strong>は、Seq2Seqアーキテクチャを用いた初期のエンドツーエンドTTSモデルです（Googleが2017年に発表）。</p>

<h3>Seq2Seq for TTS</h3>

<div class="mermaid">
graph LR
    A[テキスト] --> B[Encoder]
    B --> C[Attention]
    C --> D[Decoder]
    D --> E[Mel-スペクトログラム]
    E --> F[Vocoder]
    F --> G[音声波形]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#e8f5e9
    style F fill:#fce4ec
    style G fill:#c8e6c9
</div>

<h3>Attention機構</h3>

<p><strong>Attention</strong>は、デコーダが各ステップでエンコーダのどの部分に注目するかを決定します。</p>

<p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$</p>

<ul>
<li>$Q$: Query（デコーダの隠れ状態）</li>
<li>$K$: Key（エンコーダの隠れ状態）</li>
<li>$V$: Value（エンコーダの隠れ状態）</li>
</ul>

<h3>Tacotron 2のアーキテクチャ</h3>

<p><strong>Tacotron 2</strong>は、Tacotronを改良し、より高品質な音声を生成します。</p>

<h4>主な改良点</h4>

<table>
<thead>
<tr>
<th>コンポーネント</th>
<th>Tacotron</th>
<th>Tacotron 2</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Encoder</strong></td>
<td>CBHG</td>
<td>Conv + Bi-LSTM</td>
</tr>
<tr>
<td><strong>Attention</strong></td>
<td>Basic</td>
<td>Location-sensitive</td>
</tr>
<tr>
<td><strong>Decoder</strong></td>
<td>GRU</td>
<td>LSTM + Prenet</td>
</tr>
<tr>
<td><strong>Vocoder</strong></td>
<td>Griffin-Lim</td>
<td>WaveNet</td>
</tr>
</tbody>
</table>

<h3>Mel-スペクトログラム予測</h3>

<p>Tacotron 2は、80次元のMel-スペクトログラムを予測します。</p>

<pre><code class="language-python">import torch
import numpy as np
import matplotlib.pyplot as plt

# Tacotron 2風のシンプルなMel予測のデモ
# 注: 実際のTacotron 2は非常に複雑です

class SimpleTacotronDecoder(torch.nn.Module):
    def __init__(self, hidden_size=256, n_mels=80):
        super().__init__()
        self.prenet = torch.nn.Sequential(
            torch.nn.Linear(n_mels, 256),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.5),
            torch.nn.Linear(256, 128)
        )
        self.lstm = torch.nn.LSTM(hidden_size + 128, hidden_size,
                                  num_layers=2, batch_first=True)
        self.projection = torch.nn.Linear(hidden_size, n_mels)

    def forward(self, encoder_outputs, prev_mel):
        # Prenet処理
        prenet_out = self.prenet(prev_mel)

        # LSTMデコーダ
        lstm_input = torch.cat([encoder_outputs, prenet_out], dim=-1)
        lstm_out, _ = self.lstm(lstm_input)

        # Mel-スペクトログラム予測
        mel_pred = self.projection(lstm_out)

        return mel_pred

# モデルのインスタンス化
model = SimpleTacotronDecoder()
print("=== Tacotron 2風デコーダ ===")
print(model)

# ダミーデータでテスト
batch_size, seq_len = 4, 10
encoder_out = torch.randn(batch_size, seq_len, 256)
prev_mel = torch.randn(batch_size, seq_len, 80)

# 予測
mel_output = model(encoder_out, prev_mel)
print(f"\n入力エンコーダ出力: {encoder_out.shape}")
print(f"入力Mel: {prev_mel.shape}")
print(f"出力Mel: {mel_output.shape}")
</code></pre>

<h3>Location-sensitive Attention</h3>

<p>Tacotron 2では、<strong>Location-sensitive Attention</strong>により、安定した単調なアライメントを実現します。</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class LocationSensitiveAttention(nn.Module):
    """Tacotron 2のLocation-sensitive Attention"""

    def __init__(self, attention_dim=128, n_location_filters=32,
                 location_kernel_size=31):
        super().__init__()
        self.W_query = nn.Linear(256, attention_dim, bias=False)
        self.W_keys = nn.Linear(256, attention_dim, bias=False)
        self.W_location = nn.Linear(n_location_filters, attention_dim, bias=False)
        self.location_conv = nn.Conv1d(2, n_location_filters,
                                       kernel_size=location_kernel_size,
                                       padding=(location_kernel_size - 1) // 2,
                                       bias=False)
        self.v = nn.Linear(attention_dim, 1, bias=False)

    def forward(self, query, keys, attention_weights_cat):
        """
        Args:
            query: デコーダの隠れ状態 (B, 1, 256)
            keys: エンコーダの出力 (B, T, 256)
            attention_weights_cat: 過去のattention weights (B, 2, T)
        """
        # Location features
        location_features = self.location_conv(attention_weights_cat)
        location_features = location_features.transpose(1, 2)

        # Attention計算
        query_proj = self.W_query(query)  # (B, 1, attention_dim)
        keys_proj = self.W_keys(keys)     # (B, T, attention_dim)
        location_proj = self.W_location(location_features)  # (B, T, attention_dim)

        # エネルギー計算
        energies = self.v(torch.tanh(query_proj + keys_proj + location_proj))
        energies = energies.squeeze(-1)  # (B, T)

        # Attention weights
        attention_weights = F.softmax(energies, dim=1)

        return attention_weights

# テスト
attention = LocationSensitiveAttention()
query = torch.randn(4, 1, 256)
keys = torch.randn(4, 100, 256)
prev_attention = torch.randn(4, 2, 100)

weights = attention(query, keys, prev_attention)
print("=== Location-sensitive Attention ===")
print(f"Attention weights shape: {weights.shape}")
print(f"Sum of weights: {weights.sum(dim=1)}")  # Should be ~1.0
</code></pre>

<h3>Tacotron 2の実装例（PyTorch）</h3>

<pre><code class="language-python"># Tacotron 2のエンコーダ部分の簡略版
class TacotronEncoder(nn.Module):
    def __init__(self, num_chars=150, embedding_dim=512, hidden_size=256):
        super().__init__()
        self.embedding = nn.Embedding(num_chars, embedding_dim)

        # Convolution layers
        self.convolutions = nn.ModuleList([
            nn.Sequential(
                nn.Conv1d(embedding_dim if i == 0 else hidden_size,
                         hidden_size, kernel_size=5, padding=2),
                nn.BatchNorm1d(hidden_size),
                nn.ReLU(),
                nn.Dropout(0.5)
            ) for i in range(3)
        ])

        # Bi-directional LSTM
        self.lstm = nn.LSTM(hidden_size, hidden_size // 2,
                           num_layers=1, batch_first=True,
                           bidirectional=True)

    def forward(self, text_inputs):
        # Embedding
        x = self.embedding(text_inputs).transpose(1, 2)  # (B, C, T)

        # Convolutions
        for conv in self.convolutions:
            x = conv(x)

        # LSTM
        x = x.transpose(1, 2)  # (B, T, C)
        outputs, _ = self.lstm(x)

        return outputs

# テスト
encoder = TacotronEncoder()
text_input = torch.randint(0, 150, (4, 50))  # Batch of 4, length 50
encoder_output = encoder(text_input)

print("=== Tacotron 2 Encoder ===")
print(f"入力テキスト: {text_input.shape}")
print(f"エンコーダ出力: {encoder_output.shape}")
</code></pre>

<hr>

<h2>4.3 FastSpeech</h2>

<h3>非自己回帰的TTSの動機</h3>

<p>Tacotron 2のような<strong>自己回帰的モデル</strong>の問題点：</p>

<ul>
<li>生成が逐次的で遅い</li>
<li>不安定なアライメント</li>
<li>語の繰り返しやスキップ</li>
</ul>

<p><strong>FastSpeech</strong>は、これらの問題を解決する<strong>非自己回帰的TTS</strong>です。</p>

<h3>FastSpeechのアーキテクチャ</h3>

<div class="mermaid">
graph LR
    A[テキスト] --> B[Encoder]
    B --> C[Duration Predictor]
    C --> D[Length Regulator]
    B --> D
    D --> E[Decoder]
    E --> F[Mel-スペクトログラム]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#e8f5e9
    style F fill:#c8e6c9
</div>

<h3>Duration Prediction（継続時間予測）</h3>

<p>FastSpeechの核心は、各音素の継続時間を明示的に予測することです。</p>

<p>$$
d_i = \text{DurationPredictor}(h_i)
$$</p>

<ul>
<li>$h_i$: 音素$i$の隠れ状態</li>
<li>$d_i$: 音素$i$の継続時間（フレーム数）</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn

class DurationPredictor(nn.Module):
    """FastSpeechの継続時間予測器"""

    def __init__(self, hidden_size=256, filter_size=256,
                 kernel_size=3, dropout=0.5):
        super().__init__()

        self.layers = nn.ModuleList([
            nn.Sequential(
                nn.Conv1d(hidden_size, filter_size, kernel_size,
                         padding=(kernel_size - 1) // 2),
                nn.ReLU(),
                nn.LayerNorm(filter_size),
                nn.Dropout(dropout)
            ) for _ in range(2)
        ])

        self.linear = nn.Linear(filter_size, 1)

    def forward(self, encoder_output):
        """
        Args:
            encoder_output: (B, T, hidden_size)
        Returns:
            duration: (B, T) - 各音素の継続時間
        """
        x = encoder_output.transpose(1, 2)  # (B, C, T)

        for layer in self.layers:
            x = layer(x)

        x = x.transpose(1, 2)  # (B, T, C)
        duration = self.linear(x).squeeze(-1)  # (B, T)

        return duration

# テスト
duration_predictor = DurationPredictor()
encoder_out = torch.randn(4, 50, 256)  # Batch=4, Seq=50
durations = duration_predictor(encoder_out)

print("=== Duration Predictor ===")
print(f"入力: {encoder_out.shape}")
print(f"予測継続時間: {durations.shape}")
print(f"サンプル継続時間: {durations[0, :10]}")
</code></pre>

<h3>Length Regulator</h3>

<p><strong>Length Regulator</strong>は、予測された継続時間に基づいて、音素レベルの隠れ状態をフレームレベルに拡張します。</p>

<pre><code class="language-python">class LengthRegulator(nn.Module):
    """FastSpeechのLength Regulator"""

    def __init__(self):
        super().__init__()

    def forward(self, x, durations):
        """
        Args:
            x: 音素レベルの隠れ状態 (B, T_phoneme, C)
            durations: 各音素の継続時間 (B, T_phoneme)
        Returns:
            expanded: フレームレベルの隠れ状態 (B, T_frame, C)
        """
        output = []
        for batch_idx in range(x.size(0)):
            expanded = []
            for phoneme_idx in range(x.size(1)):
                # 各音素を継続時間分だけ繰り返す
                duration = int(durations[batch_idx, phoneme_idx].item())
                expanded.append(x[batch_idx, phoneme_idx].unsqueeze(0).expand(duration, -1))

            if expanded:
                output.append(torch.cat(expanded, dim=0))

        # パディングして同じ長さにする
        max_len = max([seq.size(0) for seq in output])
        padded_output = []
        for seq in output:
            pad_len = max_len - seq.size(0)
            if pad_len > 0:
                padding = torch.zeros(pad_len, seq.size(1))
                seq = torch.cat([seq, padding], dim=0)
            padded_output.append(seq.unsqueeze(0))

        return torch.cat(padded_output, dim=0)

# テスト
length_regulator = LengthRegulator()
phoneme_hidden = torch.randn(2, 10, 256)  # 2 samples, 10 phonemes
durations = torch.tensor([[3, 2, 4, 1, 5, 2, 3, 2, 1, 4],
                          [2, 3, 2, 5, 1, 4, 2, 3, 2, 1]], dtype=torch.float32)

frame_hidden = length_regulator(phoneme_hidden, durations)
print("=== Length Regulator ===")
print(f"音素レベル: {phoneme_hidden.shape}")
print(f"予測継続時間: {durations}")
print(f"フレームレベル: {frame_hidden.shape}")
</code></pre>

<h3>FastSpeech 2の改良</h3>

<p><strong>FastSpeech 2</strong>は、より多くの韻律情報を予測します：</p>

<table>
<thead>
<tr>
<th>予測対象</th>
<th>FastSpeech</th>
<th>FastSpeech 2</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Duration</strong></td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td><strong>Pitch</strong></td>
<td>-</td>
<td>✓</td>
</tr>
<tr>
<td><strong>Energy</strong></td>
<td>-</td>
<td>✓</td>
</tr>
<tr>
<td><strong>学習ターゲット</strong></td>
<td>Teacher強制</td>
<td>Ground truth</td>
</tr>
</tbody>
</table>

<h3>Speed vs Quality</h3>

<p>FastSpeechの利点：</p>

<table>
<thead>
<tr>
<th>指標</th>
<th>Tacotron 2</th>
<th>FastSpeech</th>
<th>改善率</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>生成速度</strong></td>
<td>1x</td>
<td>38x</td>
<td>38倍高速</td>
</tr>
<tr>
<td><strong>MOS</strong></td>
<td>4.41</td>
<td>4.27</td>
<td>-3%</td>
</tr>
<tr>
<td><strong>ロバスト性</strong></td>
<td>低</td>
<td>高</td>
<td>エラー率ほぼ0%</td>
</tr>
<tr>
<td><strong>制御性</strong></td>
<td>低</td>
<td>高</td>
<td>速度調整可能</td>
</tr>
</tbody>
</table>

<blockquote>
<p><strong>重要</strong>: FastSpeechは、わずかな品質低下で大幅な高速化とロバスト性向上を実現します。</p>
</blockquote>

<hr>

<h2>4.4 Neural Vocoders</h2>

<h3>Vocoderの進化</h3>

<p>ニューラルボコーダは、Mel-スペクトログラムから高品質な音声波形を生成します。</p>

<h3>1. WaveNet</h3>

<p><strong>WaveNet</strong>は、自己回帰的な生成モデルで、非常に高品質な音声を生成します（DeepMind、2016年）。</p>

<h4>Dilated Causal Convolution</h4>

<p>WaveNetの核心は、<strong>Dilated Causal Convolution</strong>です。</p>

<p>$$
y_t = f\left(\sum_{i=0}^{k-1} w_i \cdot x_{t-d \cdot i}\right)
$$</p>

<ul>
<li>$d$: Dilation factor（拡張係数）</li>
<li>$k$: Kernel size</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn

class DilatedCausalConv1d(nn.Module):
    """WaveNetのDilated Causal Convolution"""

    def __init__(self, in_channels, out_channels, kernel_size, dilation):
        super().__init__()
        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size,
                             padding=(kernel_size - 1) * dilation,
                             dilation=dilation)

    def forward(self, x):
        # Causal: 未来の情報を使わない
        output = self.conv(x)
        # 右側のパディングを削除
        return output[:, :, :x.size(2)]

class WaveNetBlock(nn.Module):
    """WaveNetの残差ブロック"""

    def __init__(self, residual_channels, gate_channels, skip_channels,
                 kernel_size, dilation):
        super().__init__()

        self.dilated_conv = DilatedCausalConv1d(
            residual_channels, gate_channels, kernel_size, dilation
        )

        self.conv_1x1_skip = nn.Conv1d(gate_channels // 2, skip_channels, 1)
        self.conv_1x1_res = nn.Conv1d(gate_channels // 2, residual_channels, 1)

    def forward(self, x):
        # Dilated convolution
        h = self.dilated_conv(x)

        # Gated activation
        tanh_out, sigmoid_out = h.chunk(2, dim=1)
        h = torch.tanh(tanh_out) * torch.sigmoid(sigmoid_out)

        # Skip connection
        skip = self.conv_1x1_skip(h)

        # Residual connection
        residual = self.conv_1x1_res(h)
        return (x + residual) * 0.707, skip  # sqrt(0.5) for scaling

# テスト
block = WaveNetBlock(residual_channels=64, gate_channels=128,
                     skip_channels=64, kernel_size=3, dilation=2)
x = torch.randn(4, 64, 1000)  # (B, C, T)
residual, skip = block(x)

print("=== WaveNet Block ===")
print(f"入力: {x.shape}")
print(f"残差出力: {residual.shape}")
print(f"スキップ出力: {skip.shape}")
</code></pre>

<h3>2. WaveGlow</h3>

<p><strong>WaveGlow</strong>は、Flow-based生成モデルで、並列生成が可能です（NVIDIA、2018年）。</p>

<h4>特徴</h4>

<ul>
<li><strong>リアルタイム生成</strong>: WaveNetより高速</li>
<li><strong>並列化可能</strong>: 全サンプルを一度に生成</li>
<li><strong>可逆変換</strong>: 音声とlatent変数の双方向変換</li>
</ul>

<h3>3. HiFi-GAN</h3>

<p><strong>HiFi-GAN</strong>（High Fidelity GAN）は、GANベースの高速・高品質ボコーダです（2020年）。</p>

<h4>アーキテクチャ</h4>

<table>
<thead>
<tr>
<th>コンポーネント</th>
<th>説明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Generator</strong></td>
<td>Transposed Convolutionによるアップサンプリング</td>
</tr>
<tr>
<td><strong>Multi-Period Discriminator</strong></td>
<td>異なる周期パターンを識別</td>
</tr>
<tr>
<td><strong>Multi-Scale Discriminator</strong></td>
<td>異なる解像度で識別</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">import torch
import torch.nn as nn

class HiFiGANGenerator(nn.Module):
    """HiFi-GANのGenerator（簡略版）"""

    def __init__(self, mel_channels=80, upsample_rates=[8, 8, 2, 2]):
        super().__init__()

        self.num_upsamples = len(upsample_rates)

        # 初期Conv
        self.conv_pre = nn.Conv1d(mel_channels, 512, 7, padding=3)

        # Upsampling layers
        self.ups = nn.ModuleList()
        for i, u in enumerate(upsample_rates):
            self.ups.append(nn.ConvTranspose1d(
                512 // (2 ** i),
                512 // (2 ** (i + 1)),
                u * 2,
                stride=u,
                padding=u // 2
            ))

        # 最終Conv
        self.conv_post = nn.Conv1d(512 // (2 ** len(upsample_rates)),
                                   1, 7, padding=3)

    def forward(self, mel):
        """
        Args:
            mel: Mel-スペクトログラム (B, mel_channels, T)
        Returns:
            audio: 音声波形 (B, 1, T * prod(upsample_rates))
        """
        x = self.conv_pre(mel)

        for i in range(self.num_upsamples):
            x = torch.nn.functional.leaky_relu(x, 0.1)
            x = self.ups[i](x)

        x = torch.nn.functional.leaky_relu(x, 0.1)
        x = self.conv_post(x)
        x = torch.tanh(x)

        return x

# テスト
generator = HiFiGANGenerator()
mel_input = torch.randn(2, 80, 100)  # (B, mel_channels, T)
audio_output = generator(mel_input)

print("=== HiFi-GAN Generator ===")
print(f"入力Mel: {mel_input.shape}")
print(f"出力音声: {audio_output.shape}")
print(f"アップサンプリング率: {audio_output.size(2) / mel_input.size(2):.0f}x")
</code></pre>

<h3>Vocoderの比較</h3>

<table>
<thead>
<tr>
<th>Vocoder</th>
<th>生成方式</th>
<th>速度</th>
<th>品質（MOS）</th>
<th>特徴</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Griffin-Lim</strong></td>
<td>反復アルゴリズム</td>
<td>高速</td>
<td>3.0-3.5</td>
<td>シンプル、品質低</td>
</tr>
<tr>
<td><strong>WaveNet</strong></td>
<td>自己回帰</td>
<td>非常に遅い</td>
<td>4.5+</td>
<td>最高品質</td>
</tr>
<tr>
<td><strong>WaveGlow</strong></td>
<td>Flow-based</td>
<td>中速</td>
<td>4.2-4.3</td>
<td>並列生成可能</td>
</tr>
<tr>
<td><strong>HiFi-GAN</strong></td>
<td>GAN</td>
<td>非常に高速</td>
<td>4.3-4.5</td>
<td>高速・高品質</td>
</tr>
</tbody>
</table>

<blockquote>
<p><strong>推奨</strong>: 現在では、HiFi-GANが速度と品質のバランスが最良で広く使用されています。</p>
</blockquote>

<hr>

<h2>4.5 最新のTTS技術</h2>

<h3>1. VITS（End-to-End TTS）</h3>

<p><strong>VITS</strong>（Variational Inference with adversarial learning for end-to-end Text-to-Speech）は、音響モデルとボコーダを統合したエンドツーエンドモデルです（2021年）。</p>

<h4>VITSの特徴</h4>

<ul>
<li><strong>統合アーキテクチャ</strong>: 音響モデル + Vocoder</li>
<li><strong>VAE + GAN</strong>: Variational Autoencoderと敵対的学習の組み合わせ</li>
<li><strong>高速・高品質</strong>: リアルタイム生成可能</li>
<li><strong>多様性</strong>: 同じテキストから多様な音声を生成</li>
</ul>

<div class="mermaid">
graph LR
    A[テキスト] --> B[Text Encoder]
    B --> C[Posterior Encoder]
    B --> D[Prior Encoder]
    C --> E[Latent Variable z]
    D --> E
    E --> F[Decoder/Generator]
    F --> G[音声波形]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#f3e5f5
    style E fill:#e3f2fd
    style F fill:#e8f5e9
    style G fill:#c8e6c9
</div>

<h3>2. Voice Cloning（音声クローニング）</h3>

<p><strong>Voice Cloning</strong>は、少量の音声サンプルから特定の話者の声を再現する技術です。</p>

<h4>アプローチ</h4>

<table>
<thead>
<tr>
<th>手法</th>
<th>説明</th>
<th>必要データ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Speaker Adaptation</strong></td>
<td>既存モデルを少量データで微調整</td>
<td>数分～数十分</td>
</tr>
<tr>
<td><strong>Speaker Embedding</strong></td>
<td>話者埋め込みベクトルを学習</td>
<td>数秒～数分</td>
</tr>
<tr>
<td><strong>Zero-shot TTS</strong></td>
<td>未知話者の声を即座に模倣</td>
<td>数秒</td>
</tr>
</tbody>
</table>

<h3>3. Multi-speaker TTS</h3>

<p><strong>Multi-speaker TTS</strong>は、単一モデルで複数の話者の声を生成します。</p>

<h4>Speaker Embedding</h4>

<p>話者IDを埋め込みベクトルに変換し、モデルに条件付けします。</p>

<p>$$
e_{\text{speaker}} = \text{Embedding}(\text{speaker\_id})
$$</p>

<p>$$
h = f(x, e_{\text{speaker}})
$$</p>

<h3>4. Japanese TTS Systems</h3>

<p>日本語TTSシステムの特徴：</p>

<h4>日本語特有の課題</h4>

<ul>
<li><strong>アクセント</strong>: 高低アクセントの再現</li>
<li><strong>イントネーション</strong>: 文末の上昇・下降</li>
<li><strong>促音・長音</strong>: 特殊拍の扱い</li>
<li><strong>漢字読み</strong>: 文脈依存の読み分け</li>
</ul>

<h4>主要な日本語TTSライブラリ</h4>

<table>
<thead>
<tr>
<th>システム</th>
<th>特徴</th>
<th>ライセンス</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>OpenJTalk</strong></td>
<td>HMM-based、軽量</td>
<td>BSD</td>
</tr>
<tr>
<td><strong>VOICEVOX</strong></td>
<td>ディープラーニング、高品質</td>
<td>LGPL/Commercial</td>
</tr>
<tr>
<td><strong>ESPnet-TTS</strong></td>
<td>研究用、最新手法実装</td>
<td>Apache 2.0</td>
</tr>
</tbody>
</table>

<h3>実装例：gTTS（Google Text-to-Speech）</h3>

<pre><code class="language-python">from gtts import gTTS
import os
from IPython.display import Audio

# テキスト
text_en = "Hello, this is a demonstration of text-to-speech synthesis."
text_ja = "こんにちは、これは音声合成のデモンストレーションです。"

# 英語TTS
tts_en = gTTS(text=text_en, lang='en', slow=False)
tts_en.save("output_en.mp3")

# 日本語TTS
tts_ja = gTTS(text=text_ja, lang='ja', slow=False)
tts_ja.save("output_ja.mp3")

print("=== gTTS（Google Text-to-Speech）===")
print("英語音声を生成: output_en.mp3")
print("日本語音声を生成: output_ja.mp3")

# 音声の再生（Jupyter環境）
# display(Audio("output_en.mp3"))
# display(Audio("output_ja.mp3"))
</code></pre>

<hr>

<h2>4.6 本章のまとめ</h2>

<h3>学んだこと</h3>

<ol>
<li><p><strong>TTSの基礎</strong></p>
<ul>
<li>Text-to-Speechパイプライン: テキスト解析 → 音響モデル → Vocoder</li>
<li>韻律（Prosody）の重要性: ピッチ、デュレーション、エネルギー</li>
<li>評価指標: MOS、自然性</li>
</ul></li>

<li><p><strong>Tacotron & Tacotron 2</strong></p>
<ul>
<li>Seq2SeqアーキテクチャによるエンドツーエンドTTS</li>
<li>Attention機構によるテキストと音声のアライメント</li>
<li>Location-sensitive Attentionによる安定性向上</li>
</ul></li>

<li><p><strong>FastSpeech</strong></p>
<ul>
<li>非自己回帰的TTSによる高速化</li>
<li>Duration Predictorによる明示的な継続時間制御</li>
<li>FastSpeech 2: ピッチとエネルギーの追加予測</li>
</ul></li>

<li><p><strong>Neural Vocoders</strong></p>
<ul>
<li>WaveNet: 最高品質だが遅い</li>
<li>WaveGlow: 並列生成可能</li>
<li>HiFi-GAN: 高速・高品質のバランス</li>
</ul></li>

<li><p><strong>最新技術</strong></p>
<ul>
<li>VITS: エンドツーエンド統合モデル</li>
<li>Voice Cloning: 少量データからの音声再現</li>
<li>Multi-speaker TTS: 単一モデルで複数話者</li>
<li>日本語TTS: アクセント・イントネーションの課題</li>
</ul></li>
</ol>

<h3>TTS技術の選択ガイドライン</h3>

<table>
<thead>
<tr>
<th>目的</th>
<th>推奨モデル</th>
<th>理由</th>
</tr>
</thead>
<tbody>
<tr>
<td>最高品質</td>
<td>Tacotron 2 + WaveNet</td>
<td>MOS 4.5+</td>
</tr>
<tr>
<td>リアルタイム生成</td>
<td>FastSpeech 2 + HiFi-GAN</td>
<td>高速・高品質</td>
</tr>
<tr>
<td>エンドツーエンド</td>
<td>VITS</td>
<td>統合アーキテクチャ</td>
</tr>
<tr>
<td>音声クローニング</td>
<td>Speaker Embedding TTS</td>
<td>少量データで対応</td>
</tr>
<tr>
<td>研究・実験</td>
<td>ESPnet-TTS</td>
<td>最新手法実装</td>
</tr>
</tbody>
</table>

<h3>次の章へ</h3>

<p>第5章では、<strong>音声変換とボイスコンバージョン</strong>を学びます：</p>
<ul>
<li>Voice Conversionの基礎</li>
<li>スタイル転送</li>
<li>感情表現の制御</li>
<li>リアルタイム音声変換</li>
</ul>

<hr>

<h2>演習問題</h2>

<h3>問題1（難易度：easy）</h3>
<p>TTSパイプラインの主要な4つの段階を順番に説明し、各段階の役割を述べてください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>

<ol>
<li><p><strong>テキスト解析</strong></p>
<ul>
<li>役割: テキストの正規化、トークン化、数字や略語の展開</li>
<li>出力: 正規化されたテキスト</li>
</ul></li>

<li><p><strong>言語特徴抽出</strong></p>
<ul>
<li>役割: テキストを音素に変換、韻律情報の予測</li>
<li>出力: 音素列と韻律特徴</li>
</ul></li>

<li><p><strong>音響モデル</strong></p>
<ul>
<li>役割: 言語特徴からMel-スペクトログラムを生成</li>
<li>出力: Mel-スペクトログラム（音響特徴）</li>
</ul></li>

<li><p><strong>Vocoder</strong></p>
<ul>
<li>役割: Mel-スペクトログラムから音声波形を生成</li>
<li>出力: 最終的な音声波形</li>
</ul></li>
</ol>

</details>

<h3>問題2（難易度：medium）</h3>
<p>Tacotron 2とFastSpeechの主な違いを、生成方式、速度、ロバスト性の観点から比較してください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>

<table>
<thead>
<tr>
<th>観点</th>
<th>Tacotron 2</th>
<th>FastSpeech</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>生成方式</strong></td>
<td>自己回帰的（Autoregressive）<br>前のフレームを使って次を予測</td>
<td>非自己回帰的（Non-autoregressive）<br>全フレームを並列生成</td>
</tr>
<tr>
<td><strong>速度</strong></td>
<td>遅い（逐次生成）<br>基準: 1x</td>
<td>非常に高速（並列生成）<br>約38倍高速</td>
</tr>
<tr>
<td><strong>ロバスト性</strong></td>
<td>低い<br>- 単語の繰り返し<br>- スキップ<br>- 不安定なアライメント</td>
<td>高い<br>- ほぼエラーなし<br>- 安定したアライメント<br>- 予測可能な出力</td>
</tr>
<tr>
<td><strong>制御性</strong></td>
<td>低い<br>速度・韻律の明示的制御困難</td>
<td>高い<br>Duration調整で速度制御可能</td>
</tr>
<tr>
<td><strong>品質（MOS）</strong></td>
<td>4.41（高品質）</td>
<td>4.27（わずかに低い）</td>
</tr>
</tbody>
</table>

<p><strong>結論</strong>: FastSpeechは、わずかな品質低下（-3%）で、大幅な高速化（38倍）とロバスト性向上を実現。実用アプリケーションではFastSpeechが有利。</p>

</details>

<h3>問題3（難易度：medium）</h3>
<p>WaveNet、WaveGlow、HiFi-GANの3つのVocoderを、生成方式、速度、品質の観点から比較し、それぞれの使用場面を提案してください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>

<p><strong>比較表</strong>：</p>

<table>
<thead>
<tr>
<th>Vocoder</th>
<th>生成方式</th>
<th>速度</th>
<th>品質（MOS）</th>
<th>特徴</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>WaveNet</strong></td>
<td>自己回帰的<br>Dilated Causal Conv</td>
<td>非常に遅い</td>
<td>4.5+（最高）</td>
<td>- 最高品質<br>- リアルタイム不可</td>
</tr>
<tr>
<td><strong>WaveGlow</strong></td>
<td>Flow-based<br>可逆変換</td>
<td>中速</td>
<td>4.2-4.3</td>
<td>- 並列生成<br>- 安定した学習</td>
</tr>
<tr>
<td><strong>HiFi-GAN</strong></td>
<td>GAN<br>敵対的学習</td>
<td>非常に高速</td>
<td>4.3-4.5</td>
<td>- 高速・高品質<br>- 学習やや難</td>
</tr>
</tbody>
</table>

<p><strong>使用場面の提案</strong>：</p>

<ol>
<li><p><strong>WaveNet</strong></p>
<ul>
<li>使用場面: オフライン音声合成、高品質が最優先</li>
<li>例: スタジオ品質のオーディオブック制作</li>
</ul></li>

<li><p><strong>WaveGlow</strong></p>
<ul>
<li>使用場面: 研究目的、Flow-basedモデルの理解</li>
<li>例: 生成モデル研究、VAEとの組み合わせ</li>
</ul></li>

<li><p><strong>HiFi-GAN</strong></p>
<ul>
<li>使用場面: リアルタイムアプリ、実用システム</li>
<li>例: 音声アシスタント、ライブ配信のナレーション</li>
</ul></li>
</ol>

<p><strong>推奨</strong>: 現在では、速度と品質のバランスが優れたHiFi-GANが最も広く使用されています。</p>

</details>

<h3>問題4（難易度：hard）</h3>
<p>FastSpeechのDuration PredictorとLength Regulatorの役割を説明し、なぜこれらが非自己回帰的TTSに必要なのか述べてください。Pythonコードで簡単な実装例を示してください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>

<p><strong>役割の説明</strong>：</p>

<ol>
<li><p><strong>Duration Predictor（継続時間予測器）</strong></p>
<ul>
<li>役割: 各音素が何フレーム続くかを予測</li>
<li>入力: エンコーダの隠れ状態（音素レベル）</li>
<li>出力: 各音素の継続時間（フレーム数）</li>
</ul></li>

<li><p><strong>Length Regulator</strong></p>
<ul>
<li>役割: 音素レベルの表現をフレームレベルに拡張</li>
<li>入力: 音素の隠れ状態 + 予測された継続時間</li>
<li>出力: フレームレベルの隠れ状態</li>
</ul></li>
</ol>

<p><strong>必要性</strong>：</p>

<p>非自己回帰的TTSでは、全フレームを並列生成するため：</p>
<ul>
<li>事前に出力長を知る必要がある</li>
<li>テキスト（音素）と音声（フレーム）の長さが異なる</li>
<li>各音素の継続時間が異なる（例: 「あ」は3フレーム、「ん」は1フレーム）</li>
</ul>

<p>Duration Predictorで継続時間を予測し、Length Regulatorで音素表現を適切な長さに拡張することで、並列生成が可能になります。</p>

<p><strong>実装例</strong>：</p>

<pre><code class="language-python">import torch
import torch.nn as nn

class SimpleDurationPredictor(nn.Module):
    """継続時間予測器"""

    def __init__(self, hidden_size=256):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Conv1d(hidden_size, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.LayerNorm(256),
            nn.Dropout(0.5),
            nn.Conv1d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.LayerNorm(256),
            nn.Dropout(0.5)
        )
        self.output = nn.Linear(256, 1)

    def forward(self, x):
        # x: (B, T, hidden_size)
        x = x.transpose(1, 2)  # (B, hidden_size, T)
        x = self.layers(x)
        x = x.transpose(1, 2)  # (B, T, 256)
        duration = self.output(x).squeeze(-1)  # (B, T)
        return torch.relu(duration)  # 正の値のみ

class SimpleLengthRegulator(nn.Module):
    """Length Regulator"""

    def forward(self, x, durations):
        # x: (B, T_phoneme, hidden_size)
        # durations: (B, T_phoneme)

        output = []
        for batch_idx in range(x.size(0)):
            expanded = []
            for phoneme_idx in range(x.size(1)):
                dur = int(durations[batch_idx, phoneme_idx].item())
                if dur > 0:
                    # 音素の隠れ状態をdur回繰り返す
                    phoneme_hidden = x[batch_idx, phoneme_idx].unsqueeze(0)
                    expanded.append(phoneme_hidden.expand(dur, -1))

            if expanded:
                output.append(torch.cat(expanded, dim=0))

        # パディングして同じ長さに
        max_len = max(seq.size(0) for seq in output)
        padded = []
        for seq in output:
            if seq.size(0) < max_len:
                pad = torch.zeros(max_len - seq.size(0), seq.size(1))
                seq = torch.cat([seq, pad], dim=0)
            padded.append(seq.unsqueeze(0))

        return torch.cat(padded, dim=0)

# テスト
print("=== Duration Predictor & Length Regulator ===\n")

# ダミーデータ
batch_size, n_phonemes, hidden_size = 2, 5, 256
phoneme_hidden = torch.randn(batch_size, n_phonemes, hidden_size)

# Duration予測
duration_predictor = SimpleDurationPredictor(hidden_size)
predicted_durations = duration_predictor(phoneme_hidden)

print(f"音素の隠れ状態: {phoneme_hidden.shape}")
print(f"予測された継続時間: {predicted_durations.shape}")
print(f"サンプル継続時間: {predicted_durations[0]}")

# Length Regulation
length_regulator = SimpleLengthRegulator()
frame_hidden = length_regulator(phoneme_hidden, predicted_durations)

print(f"\nフレームレベルの隠れ状態: {frame_hidden.shape}")
print(f"拡張率: {frame_hidden.size(1) / phoneme_hidden.size(1):.2f}x")

# 具体例
print("\n=== 具体例 ===")
print("音素: ['k', 'o', 'n', 'n', 'i']")
print("継続時間: [3, 4, 1, 1, 2] フレーム")
print("→ 合計 11 フレームの音声生成")
</code></pre>

<p><strong>出力例</strong>：</p>
<pre><code>=== Duration Predictor & Length Regulator ===

音素の隠れ状態: torch.Size([2, 5, 256])
予測された継続時間: torch.Size([2, 5])
サンプル継続時間: tensor([2.3, 1.8, 3.1, 2.5, 1.2])

フレームレベルの隠れ状態: torch.Size([2, 11, 256])
拡張率: 2.20x

=== 具体例 ===
音素: ['k', 'o', 'n', 'n', 'i']
継続時間: [3, 4, 1, 1, 2] フレーム
→ 合計 11 フレームの音声生成
</code></pre>

</details>

<h3>問題5（難易度：hard）</h3>
<p>Multi-speaker TTSにおけるSpeaker Embeddingの役割を説明し、どのようにモデルに組み込まれるか述べてください。また、Voice Cloningとの関連性を論じてください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>

<p><strong>Speaker Embeddingの役割</strong>：</p>

<ol>
<li><p><strong>話者特徴の表現</strong></p>
<ul>
<li>各話者を低次元ベクトル（通常64-512次元）で表現</li>
<li>話者の声質、ピッチ、話し方の特徴をエンコード</li>
</ul></li>

<li><p><strong>条件付け</strong></p>
<ul>
<li>TTSモデルに話者情報を提供</li>
<li>同じテキストでも異なる話者の声を生成可能</li>
</ul></li>
</ol>

<p><strong>モデルへの組み込み方法</strong>：</p>

<p><strong>方法1: Embedding Lookupテーブル</strong></p>
<pre><code class="language-python">import torch
import torch.nn as nn

class MultiSpeakerTTS(nn.Module):
    def __init__(self, n_speakers=100, speaker_embed_dim=256,
                 text_embed_dim=512):
        super().__init__()

        # Speaker Embedding
        self.speaker_embedding = nn.Embedding(n_speakers, speaker_embed_dim)

        # Text Encoder
        self.text_encoder = nn.LSTM(text_embed_dim, 512,
                                    num_layers=2, batch_first=True)

        # Speaker-conditioned Decoder
        self.decoder = nn.LSTM(512 + speaker_embed_dim, 512,
                              num_layers=2, batch_first=True)

        self.mel_projection = nn.Linear(512, 80)  # 80-dim Mel

    def forward(self, text_features, speaker_ids):
        # Speaker Embedding取得
        speaker_emb = self.speaker_embedding(speaker_ids)  # (B, speaker_dim)

        # Text Encoding
        text_encoded, _ = self.text_encoder(text_features)  # (B, T, 512)

        # Speaker Embeddingを全タイムステップに拡張
        speaker_emb_expanded = speaker_emb.unsqueeze(1).expand(
            -1, text_encoded.size(1), -1
        )  # (B, T, speaker_dim)

        # Concatenate
        decoder_input = torch.cat([text_encoded, speaker_emb_expanded],
                                 dim=-1)  # (B, T, 512+speaker_dim)

        # Decode
        decoder_output, _ = self.decoder(decoder_input)

        # Mel prediction
        mel_output = self.mel_projection(decoder_output)

        return mel_output

# テスト
model = MultiSpeakerTTS(n_speakers=100)
text_features = torch.randn(4, 50, 512)  # Batch=4, Seq=50
speaker_ids = torch.tensor([0, 5, 10, 15])  # 異なる話者

mel_output = model(text_features, speaker_ids)
print("=== Multi-Speaker TTS ===")
print(f"入力テキスト: {text_features.shape}")
print(f"話者ID: {speaker_ids}")
print(f"出力Mel: {mel_output.shape}")
</code></pre>

<p><strong>方法2: Speaker Encoder（Voice Cloning用）</strong></p>
<pre><code class="language-python">class SpeakerEncoder(nn.Module):
    """音声から話者埋め込みを抽出"""

    def __init__(self, mel_dim=80, embed_dim=256):
        super().__init__()
        self.lstm = nn.LSTM(mel_dim, 256, num_layers=3,
                           batch_first=True)
        self.projection = nn.Linear(256, embed_dim)

    def forward(self, mel_spectrograms):
        # mel_spectrograms: (B, T, 80)
        _, (hidden, _) = self.lstm(mel_spectrograms)
        # 最終層の隠れ状態を使用
        speaker_emb = self.projection(hidden[-1])  # (B, embed_dim)
        # L2正規化
        speaker_emb = speaker_emb / torch.norm(speaker_emb, dim=1, keepdim=True)
        return speaker_emb

# Voice Cloningのワークフロー
speaker_encoder = SpeakerEncoder()

# 参照音声から話者埋め込みを抽出
reference_mel = torch.randn(1, 100, 80)  # 数秒の音声
speaker_emb = speaker_encoder(reference_mel)

print("\n=== Voice Cloning ===")
print(f"参照音声: {reference_mel.shape}")
print(f"抽出された話者埋め込み: {speaker_emb.shape}")
print("→ この埋め込みを使って、任意のテキストをこの話者の声で合成")
</code></pre>

<p><strong>Voice Cloningとの関連性</strong>：</p>

<table>
<thead>
<tr>
<th>観点</th>
<th>Multi-speaker TTS</th>
<th>Voice Cloning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>話者表現</strong></td>
<td>Embedding Lookup<br>（学習済み話者のみ）</td>
<td>Speaker Encoder<br>（未知話者も可能）</td>
</tr>
<tr>
<td><strong>必要データ</strong></td>
<td>各話者の大量データ</td>
<td>数秒～数分の音声</td>
</tr>
<tr>
<td><strong>柔軟性</strong></td>
<td>低（固定話者セット）</td>
<td>高（新規話者対応）</td>
</tr>
<tr>
<td><strong>品質</strong></td>
<td>高（各話者で最適化）</td>
<td>中～高（データ量依存）</td>
</tr>
</tbody>
</table>

<p><strong>統合アプローチ</strong>：</p>

<p>最新のシステムでは、両方を組み合わせて使用：</p>
<ol>
<li>Multi-speaker TTSを大規模データで事前学習</li>
<li>Speaker Encoderで新規話者の埋め込みを抽出</li>
<li>少量の追加データで微調整（Optional）</li>
</ol>

<p>これにより、学習済み話者の高品質と、未知話者への対応を両立できます。</p>

</details>

<hr>

<h2>参考文献</h2>

<ol>
<li>Wang, Y., et al. (2017). "Tacotron: Towards End-to-End Speech Synthesis." <em>Interspeech 2017</em>.</li>
<li>Shen, J., et al. (2018). "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions." <em>ICASSP 2018</em>.</li>
<li>Ren, Y., et al. (2019). "FastSpeech: Fast, Robust and Controllable Text to Speech." <em>NeurIPS 2019</em>.</li>
<li>van den Oord, A., et al. (2016). "WaveNet: A Generative Model for Raw Audio." <em>arXiv:1609.03499</em>.</li>
<li>Prenger, R., Valle, R., & Catanzaro, B. (2019). "WaveGlow: A Flow-based Generative Network for Speech Synthesis." <em>ICASSP 2019</em>.</li>
<li>Kong, J., Kim, J., & Bae, J. (2020). "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis." <em>NeurIPS 2020</em>.</li>
<li>Kim, J., Kong, J., & Son, J. (2021). "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech." <em>ICML 2021</em>.</li>
<li>Casanova, E., et al. (2022). "YourtTS: Towards Zero-Shot Multi-Speaker TTS." <em>arXiv:2112.02418</em>.</li>
</ol>

<div class="navigation">
    <a href="chapter3-speech-recognition.html" class="nav-button">← 前の章: 音声認識</a>
    <a href="index.html" class="nav-button">シリーズ目次</a>
</div>

    </main>

    <footer>
        <p><strong>作成者</strong>: AI Terakoya Content Team</p>
        <p><strong>監修</strong>: Dr. Yusuke Hashimoto（東北大学）</p>
        <p><strong>バージョン</strong>: 1.0 | <strong>作成日</strong>: 2025-10-21</p>
        <p><strong>ライセンス</strong>: Creative Commons BY 4.0</p>
        <p>© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
