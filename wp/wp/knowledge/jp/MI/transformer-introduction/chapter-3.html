<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Chapter</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 20-25ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ÂàùÁ¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 0ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 0Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Á¨¨3Á´†: ‰∫ãÂâçÂ≠¶Áøí„É¢„Éá„É´„Å®Ëª¢ÁßªÂ≠¶Áøí</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">MatBERT/ChemBERTa„Å™„Å©ÊùêÊñô„ÉªÂåñÂ≠¶ÁâπÂåñ„ÅÆ‰∫ãÂâçÂ≠¶Áøí„É¢„Éá„É´„ÅÆ‰Ωø„ÅÑ„Å©„Åì„Çç„ÇíÊï¥ÁêÜ„Åó„Åæ„Åô„ÄÇÂ∞ë„Éá„Éº„ÇøÂæÆË™øÊï¥„ÅÆÂãòÊâÄ„ÇíÂ≠¶„Å≥„Åæ„Åô„ÄÇ</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>üí° Ë£úË∂≥:</strong> ÂáçÁµê„ÉªÈÉ®ÂàÜÂáçÁµê„ÉªÂÖ®Â±§Â≠¶Áøí„ÇíÊØîËºÉ„Åó„ÄÅË®àÁÆóË≥áÊ∫ê„Å®Á≤æÂ∫¶„ÅÆÊúÄÈÅ©ÁÇπ„ÇíË¶ã„Å§„Åë„Åæ„Åô„ÄÇ</p>





<p><strong>Â≠¶ÁøíÊôÇÈñì</strong>: 25-30ÂàÜ | <strong>Èõ£ÊòìÂ∫¶</strong>: ‰∏≠Á¥ö„Äú‰∏äÁ¥ö</p>
<h2>üìã „Åì„ÅÆÁ´†„ÅßÂ≠¶„Å∂„Åì„Å®</h2>
<ul>
<li>‰∫ãÂâçÂ≠¶ÁøíÔºàPre-trainingÔºâ„ÅÆÈáçË¶ÅÊÄß„Å®ÂéüÁêÜ</li>
<li>MatBERT„ÄÅMolBERT„Å™„Å©ÊùêÊñôÁßëÂ≠¶Âêë„Åë‰∫ãÂâçÂ≠¶Áøí„É¢„Éá„É´</li>
<li>„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞ÔºàFine-tuningÔºâ„ÅÆÊà¶Áï•</li>
<li>Few-shotÂ≠¶Áøí„Å®„Éó„É≠„É≥„Éó„Éà„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞</li>
<li>„Éâ„É°„Ç§„É≥ÈÅ©ÂøúÔºàDomain AdaptationÔºâ</li>
</ul>
<hr />
<h2>3.1 ‰∫ãÂâçÂ≠¶Áøí„ÅÆÈáçË¶ÅÊÄß</h2>
<h3>„Å™„Åú‰∫ãÂâçÂ≠¶Áøí„ÅåÂøÖË¶Å„Åã</h3>
<p><strong>ÊùêÊñôÁßëÂ≠¶„ÅÆË™≤È°å</strong>:
- ‚ùå „É©„Éô„É´‰ªò„Åç„Éá„Éº„Çø„ÅåÂ∞ë„Å™„ÅÑÔºàÂÆüÈ®ì„Éá„Éº„Çø„ÅØÈ´ò„Ç≥„Çπ„ÉàÔºâ
- ‚ùå „Éâ„É°„Ç§„É≥Âõ∫Êúâ„ÅÆÁü•Ë≠ò„ÅåÂøÖË¶Å
- ‚ùå „Çº„É≠„Åã„ÇâÂ≠¶Áøí„Åô„Çã„Å®ÊôÇÈñì„Å®„Ç≥„Çπ„Éà„Åå„Åã„Åã„Çã</p>
<p><strong>‰∫ãÂâçÂ≠¶Áøí„ÅÆÂà©ÁÇπ</strong>:
- ‚úÖ Â§ßË¶èÊ®°„Å™<strong>„É©„Éô„É´„Å™„Åó„Éá„Éº„Çø</strong>„Åß‰∏ÄËà¨ÁöÑ„Å™Áü•Ë≠ò„ÇíÁç≤Âæó
- ‚úÖ Â∞ëÈáè„ÅÆ„É©„Éô„É´‰ªò„Åç„Éá„Éº„Çø„Åß<strong>È´òÁ≤æÂ∫¶</strong>„ÇíÂÆüÁèæ
- ‚úÖ ÈñãÁô∫ÊúüÈñì„ÅÆ<strong>Â§ßÂπÖÁü≠Á∏Æ</strong>ÔºàÊï∞ÈÄ±Èñì‚ÜíÊï∞ÊôÇÈñìÔºâ</p>
<div class="mermaid">
flowchart LR
    A[Â§ßË¶èÊ®°„É©„Éô„É´„Å™„Åó„Éá„Éº„Çø] --> B[‰∫ãÂâçÂ≠¶Áøí]
    B --> C[Ê±éÁî®Ë°®Áèæ„É¢„Éá„É´]
    C --> D[„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞]
    E[Â∞ëÈáè„É©„Éô„É´‰ªò„Åç„Éá„Éº„Çø] --> D
    D --> F[„Çø„Çπ„ÇØÁâπÂåñ„É¢„Éá„É´]

    style B fill:#e1f5ff
    style D fill:#ffe1e1
</div>

<h3>‰∫ãÂâçÂ≠¶Áøí„ÅÆ„Çø„Çπ„ÇØ</h3>
<p><strong>Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„Åß„ÅÆ‰æã</strong>:
- <strong>Masked Language Model (MLM)</strong>: ‰∏ÄÈÉ®„ÅÆÂçòË™û„Çí„Éû„Çπ„ÇØ„Åó„Å¶‰∫àÊ∏¨
- <strong>Next Sentence Prediction (NSP)</strong>: 2Êñá„ÅÆÈÄ£Á∂öÊÄß„Çí‰∫àÊ∏¨</p>
<p><strong>ÊùêÊñôÁßëÂ≠¶„Åß„ÅÆÂøúÁî®</strong>:
- <strong>Masked Atom Prediction</strong>: ‰∏ÄÈÉ®„ÅÆÂéüÂ≠ê„Çí„Éû„Çπ„ÇØ„Åó„Å¶‰∫àÊ∏¨
- <strong>Property Prediction</strong>: Ë§áÊï∞„ÅÆÊùêÊñôÁâπÊÄß„ÇíÂêåÊôÇ‰∫àÊ∏¨
- <strong>Contrastive Learning</strong>: È°û‰ººÊùêÊñô„ÇíËøë„Åè„ÄÅÁï∞„Å™„ÇãÊùêÊñô„ÇíÈÅ†„Åè„Å´ÈÖçÁΩÆ</p>
<hr />
<h2>3.2 MatBERT: Materials BERT</h2>
<h3>Ê¶ÇË¶Å</h3>
<p><strong>MatBERT</strong>„ÅØ„ÄÅÊùêÊñô„ÅÆÁµÑÊàêÂºè„ÇíBERT„ÅßÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ</p>
<p><strong>ÁâπÂæ¥</strong>:
- <strong>500kÊùêÊñô</strong>„ÅÆÁµÑÊàêÂºè„Åß‰∫ãÂâçÂ≠¶Áøí
- <strong>„Éû„Çπ„ÇØÂéüÂ≠ê‰∫àÊ∏¨</strong>„Çø„Çπ„ÇØ
- Ëª¢ÁßªÂ≠¶Áøí„ÅßÊßò„ÄÖ„Å™ÁâπÊÄß‰∫àÊ∏¨„Å´ÈÅ©Áî®ÂèØËÉΩ</p>
<h3>ÁµÑÊàêÂºè„ÅÆ„Éà„Éº„ÇØ„É≥Âåñ</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
from transformers import BertTokenizer, BertModel

class CompositionTokenizer:
    def __init__(self):
        # „Ç´„Çπ„Çø„É†Ë™ûÂΩôÔºàÂë®ÊúüË°®„ÅÆÂÖÉÁ¥†Ôºâ
        self.vocab = ['[PAD]', '[CLS]', '[SEP]', '[MASK]'] + [
            'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne',
            'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'Ar', 'K', 'Ca',
            # ... ÂÖ®ÂÖÉÁ¥†
        ]
        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}
        self.id_to_token = {i: token for i, token in enumerate(self.vocab)}

    def tokenize(self, composition):
        &quot;&quot;&quot;
        ÁµÑÊàêÂºè„Çí„Éà„Éº„ÇØ„É≥Âåñ

        Args:
            composition: 'Fe2O3' „ÅÆ„Çà„ÅÜ„Å™ÁµÑÊàêÂºè
        Returns:
            tokens: „Éà„Éº„ÇØ„É≥„ÅÆ„É™„Çπ„Éà
        &quot;&quot;&quot;
        import re
        # ÂÖÉÁ¥†„Å®Êï∞Â≠ó„ÇíÂàÜÂâ≤
        pattern = r'([A-Z][a-z]?)(\d*\.?\d*)'
        matches = re.findall(pattern, composition)

        tokens = ['[CLS]']
        for element, count in matches:
            if element in self.vocab:
                # ÂÖÉÁ¥†„ÇíËøΩÂä†
                tokens.append(element)
                # Êï∞„Åå1„Çà„ÇäÂ§ß„Åç„ÅÑÂ†¥Âêà„ÄÅ„Åù„ÅÆÂõûÊï∞„Å†„ÅëÁπ∞„ÇäËøî„ÅôÔºàÁ∞°Áï•ÂåñÔºâ
                if count and float(count) &gt; 1:
                    for _ in range(int(float(count)) - 1):
                        tokens.append(element)
        tokens.append('[SEP]')

        return tokens

    def encode(self, compositions, max_length=32):
        &quot;&quot;&quot;
        ÁµÑÊàêÂºè„ÇíID„Å´Â§âÊèõ

        Args:
            compositions: ÁµÑÊàêÂºè„ÅÆ„É™„Çπ„Éà
            max_length: ÊúÄÂ§ßÈï∑
        Returns:
            input_ids: (batch_size, max_length)
            attention_mask: (batch_size, max_length)
        &quot;&quot;&quot;
        batch_input_ids = []
        batch_attention_mask = []

        for comp in compositions:
            tokens = self.tokenize(comp)
            ids = [self.token_to_id.get(token, 0) for token in tokens]

            # „Éë„Éá„Ç£„É≥„Ç∞
            attention_mask = [1] * len(ids)
            while len(ids) &lt; max_length:
                ids.append(0)  # [PAD]
                attention_mask.append(0)

            # „Éà„É©„É≥„Ç±„Éº„Ç∑„Éß„É≥
            ids = ids[:max_length]
            attention_mask = attention_mask[:max_length]

            batch_input_ids.append(ids)
            batch_attention_mask.append(attention_mask)

        return torch.tensor(batch_input_ids), torch.tensor(batch_attention_mask)

# ‰ΩøÁî®‰æã
tokenizer = CompositionTokenizer()

compositions = [
    'Fe2O3',     # ÈÖ∏ÂåñÈâÑ
    'LiCoO2',    # „É™„ÉÅ„Ç¶„É†„Ç≥„Éê„É´„ÉàÈÖ∏ÂåñÁâ©ÔºàÈõªÊ±†ÊùêÊñôÔºâ
    'BaTiO3'     # „ÉÅ„Çø„É≥ÈÖ∏„Éê„É™„Ç¶„É†ÔºàË™òÈõª‰ΩìÔºâ
]

input_ids, attention_mask = tokenizer.encode(compositions)
print(f&quot;Input IDs shape: {input_ids.shape}&quot;)
print(f&quot;First composition tokens: {input_ids[0][:10]}&quot;)
</code></pre>
<h3>MatBERT„É¢„Éá„É´</h3>
<pre><code class="language-python">class MatBERT(nn.Module):
    def __init__(self, vocab_size, d_model=512, num_layers=6, num_heads=8):
        super(MatBERT, self).__init__()

        # Embedding
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(512, d_model)

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=num_heads,
            dim_feedforward=2048,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)

        self.d_model = d_model

    def forward(self, input_ids, attention_mask):
        &quot;&quot;&quot;
        Args:
            input_ids: (batch_size, seq_len)
            attention_mask: (batch_size, seq_len)
        Returns:
            embeddings: (batch_size, seq_len, d_model)
        &quot;&quot;&quot;
        batch_size, seq_len = input_ids.shape

        # Token embedding
        token_embeddings = self.embedding(input_ids)

        # Positional embedding
        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)
        position_embeddings = self.position_embedding(positions)

        # ÂêàË®à
        embeddings = token_embeddings + position_embeddings

        # Transformer
        # attention_mask„ÇíTransformerÁî®„Å´Â§âÊèõÔºà0‚Üí-inf, 1‚Üí0Ôºâ
        transformer_mask = (1 - attention_mask).bool()
        output = self.transformer_encoder(embeddings, src_key_padding_mask=transformer_mask)

        return output

# ‰ΩøÁî®‰æã
vocab_size = len(tokenizer.vocab)
model = MatBERT(vocab_size, d_model=512, num_layers=6, num_heads=8)

embeddings = model(input_ids, attention_mask)
print(f&quot;Embeddings shape: {embeddings.shape}&quot;)  # (3, 32, 512)
</code></pre>
<h3>‰∫ãÂâçÂ≠¶Áøí: Masked Atom Prediction</h3>
<pre><code class="language-python">def masked_atom_prediction_loss(model, input_ids, attention_mask, mask_prob=0.15):
    &quot;&quot;&quot;
    „Éû„Çπ„ÇØÂéüÂ≠ê‰∫àÊ∏¨„Å´„Çà„Çã‰∫ãÂâçÂ≠¶Áøí

    Args:
        model: MatBERT„É¢„Éá„É´
        input_ids: (batch_size, seq_len)
        attention_mask: (batch_size, seq_len)
        mask_prob: „Éû„Çπ„ÇØ„Åô„ÇãÁ¢∫Áéá
    Returns:
        loss: ÊêçÂ§±
    &quot;&quot;&quot;
    batch_size, seq_len = input_ids.shape

    # „É©„É≥„ÉÄ„É†„Å´„Éû„Çπ„ÇØ
    mask_token_id = tokenizer.token_to_id['[MASK]']
    mask = torch.rand(batch_size, seq_len) &lt; mask_prob
    mask = mask &amp; (attention_mask == 1)  # „Éë„Éá„Ç£„É≥„Ç∞ÈÉ®ÂàÜ„ÅØÈô§Â§ñ

    # ÂÖÉ„ÅÆ„Éà„Éº„ÇØ„É≥„Çí‰øùÂ≠ò
    original_input_ids = input_ids.clone()

    # „Éû„Çπ„ÇØ„ÇíÈÅ©Áî®
    input_ids[mask] = mask_token_id

    # Forward
    embeddings = model(input_ids, attention_mask)

    # ‰∫àÊ∏¨„Éò„ÉÉ„Éâ
    prediction_head = nn.Linear(model.d_model, vocab_size)
    logits = prediction_head(embeddings)

    # ÊêçÂ§±Ë®àÁÆóÔºà„Éû„Çπ„ÇØ„Åï„Çå„Åü‰ΩçÁΩÆ„ÅÆ„ÅøÔºâ
    criterion = nn.CrossEntropyLoss(ignore_index=-100)
    labels = original_input_ids.clone()
    labels[~mask] = -100  # „Éû„Çπ„ÇØ„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÈÉ®ÂàÜ„ÅØÁÑ°Ë¶ñ

    loss = criterion(logits.view(-1, vocab_size), labels.view(-1))

    return loss

# ‰∫ãÂâçÂ≠¶Áøí„É´„Éº„ÉóÔºàÁ∞°Áï•ÁâàÔºâ
def pretrain_matbert(model, dataloader, epochs=10):
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for input_ids, attention_mask in dataloader:
            loss = masked_atom_prediction_loss(model, input_ids, attention_mask)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        print(f&quot;Epoch {epoch+1}, Pretraining Loss: {avg_loss:.4f}&quot;)

    return model
</code></pre>
<hr />
<h2>3.3 „Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞Êà¶Áï•</h2>
<h3>„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Å®„ÅØ</h3>
<p><strong>ÂÆöÁæ©</strong>: ‰∫ãÂâçÂ≠¶Áøí„É¢„Éá„É´„ÇíÁâπÂÆö„Çø„Çπ„ÇØ„Å´ÈÅ©Âøú„Åï„Åõ„ÇãËøΩÂä†Â≠¶Áøí</p>
<p><strong>Êà¶Áï•</strong>:
1. <strong>Full Fine-tuning</strong>: „Åô„Åπ„Å¶„ÅÆ„Éë„É©„É°„Éº„Çø„ÇíÊõ¥Êñ∞
2. <strong>Feature Extraction</strong>: Âüã„ÇÅËæº„ÅøÂ±§„ÅÆ„Åø‰ΩøÁî®„ÄÅ‰∫àÊ∏¨„Éò„ÉÉ„Éâ„ÅÆ„ÅøÂ≠¶Áøí
3. <strong>Partial Fine-tuning</strong>: ‰∏ÄÈÉ®„ÅÆÂ±§„ÅÆ„ÅøÊõ¥Êñ∞</p>
<div class="mermaid">
flowchart TD
    A[‰∫ãÂâçÂ≠¶ÁøíÊ∏à„ÅøMatBERT] --> B{„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞Êà¶Áï•}
    B --> C[Full Fine-tuning]
    B --> D[Feature Extraction]
    B --> E[Partial Fine-tuning]

    C --> F[ÂÖ®Â±§„ÇíÊõ¥Êñ∞]
    D --> G[Âüã„ÇÅËæº„ÅøÂõ∫ÂÆö„ÄÅ‰∫àÊ∏¨„Éò„ÉÉ„Éâ„ÅÆ„ÅøÂ≠¶Áøí]
    E --> H[‰∏ä‰ΩçÂ±§„ÅÆ„ÅøÊõ¥Êñ∞]

    style C fill:#ffe1e1
    style D fill:#e1f5ff
    style E fill:#f5ffe1
</div>

<h3>ÂÆüË£Ö: „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó‰∫àÊ∏¨</h3>
<pre><code class="language-python">class MatBERTForBandgap(nn.Module):
    def __init__(self, matbert_model, d_model=512):
        super(MatBERTForBandgap, self).__init__()
        self.matbert = matbert_model

        # ‰∫àÊ∏¨„Éò„ÉÉ„Éâ
        self.bandgap_predictor = nn.Sequential(
            nn.Linear(d_model, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 1)
        )

    def forward(self, input_ids, attention_mask):
        # MatBERTÂüã„ÇÅËæº„Åø
        embeddings = self.matbert(input_ids, attention_mask)

        # [CLS]„Éà„Éº„ÇØ„É≥„ÅÆÂüã„ÇÅËæº„Åø„Çí‰ΩøÁî®
        cls_embedding = embeddings[:, 0, :]

        # „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó‰∫àÊ∏¨
        bandgap = self.bandgap_predictor(cls_embedding)
        return bandgap

# „Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞
def finetune_for_bandgap(pretrained_model, train_loader, val_loader, strategy='full'):
    &quot;&quot;&quot;
    „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó‰∫àÊ∏¨„Å∏„ÅÆ„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞

    Args:
        pretrained_model: ‰∫ãÂâçÂ≠¶ÁøíÊ∏à„ÅøMatBERT
        train_loader: Ë®ìÁ∑¥„Éá„Éº„Çø„É≠„Éº„ÉÄ„Éº
        val_loader: Ê§úË®º„Éá„Éº„Çø„É≠„Éº„ÉÄ„Éº
        strategy: 'full', 'feature', 'partial'
    &quot;&quot;&quot;
    model = MatBERTForBandgap(pretrained_model)

    # Êà¶Áï•„Å´Âøú„Åò„Å¶„Éë„É©„É°„Éº„Çø„ÅÆÂõ∫ÂÆö
    if strategy == 'feature':
        # MatBERT„ÇíÂõ∫ÂÆö
        for param in model.matbert.parameters():
            param.requires_grad = False
    elif strategy == 'partial':
        # ‰∏ã‰ΩçÂ±§„ÇíÂõ∫ÂÆö„ÄÅ‰∏ä‰ΩçÂ±§„ÅÆ„ÅøÊõ¥Êñ∞
        for i, layer in enumerate(model.matbert.transformer_encoder.layers):
            if i &lt; 3:  # ‰∏ã‰Ωç3Â±§„ÇíÂõ∫ÂÆö
                for param in layer.parameters():
                    param.requires_grad = False

    # ÊúÄÈÅ©Âåñ
    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)
    criterion = nn.MSELoss()

    # Ë®ìÁ∑¥„É´„Éº„Éó
    best_val_loss = float('inf')
    for epoch in range(20):
        model.train()
        train_loss = 0
        for input_ids, attention_mask, bandgaps in train_loader:
            predictions = model(input_ids, attention_mask)
            loss = criterion(predictions, bandgaps)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        # Ê§úË®º
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for input_ids, attention_mask, bandgaps in val_loader:
                predictions = model(input_ids, attention_mask)
                loss = criterion(predictions, bandgaps)
                val_loss += loss.item()

        train_loss /= len(train_loader)
        val_loss /= len(val_loader)

        print(f&quot;Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}&quot;)

        if val_loss &lt; best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), 'best_matbert_bandgap.pt')

    return model
</code></pre>
<hr />
<h2>3.4 Few-shotÂ≠¶Áøí</h2>
<h3>Ê¶ÇË¶Å</h3>
<p><strong>Few-shotÂ≠¶Áøí</strong>: Â∞ëÈáè„ÅÆ„Çµ„É≥„Éó„É´ÔºàÊï∞ÂÄã„ÄúÊï∞ÂçÅÂÄãÔºâ„ÅßÊñ∞„Åó„ÅÑ„Çø„Çπ„ÇØ„ÇíÂ≠¶Áøí</p>
<p><strong>ÊùêÊñôÁßëÂ≠¶„Åß„ÅÆÈáçË¶ÅÊÄß</strong>:
- Êñ∞Ë¶èÊùêÊñô„ÅÆ„Éá„Éº„Çø„ÅØÈùûÂ∏∏„Å´Â∞ë„Å™„ÅÑ
- ÂÆüÈ®ì„Éá„Éº„Çø„ÅÆÂèñÂæó„ÅØÈ´ò„Ç≥„Çπ„Éà
- ËøÖÈÄü„Å™„Éó„É≠„Éà„Çø„Ç§„Éî„É≥„Ç∞„ÅåÂøÖË¶Å</p>
<h3>Prototypical Networks</h3>
<pre><code class="language-python">class PrototypicalNetwork(nn.Module):
    def __init__(self, matbert_model, d_model=512):
        super(PrototypicalNetwork, self).__init__()
        self.encoder = matbert_model

    def forward(self, support_ids, support_mask, query_ids, query_mask, support_labels):
        &quot;&quot;&quot;
        Prototypical Networks„Å´„Çà„ÇãÂàÜÈ°û

        Args:
            support_ids: „Çµ„Éù„Éº„Éà„Çª„ÉÉ„ÉàÂÖ•Âäõ (n_support, seq_len)
            support_mask: „Çµ„Éù„Éº„Éà„Çª„ÉÉ„Éà„Éû„Çπ„ÇØ
            query_ids: „ÇØ„Ç®„É™ÂÖ•Âäõ (n_query, seq_len)
            query_mask: „ÇØ„Ç®„É™„Éû„Çπ„ÇØ
            support_labels: „Çµ„Éù„Éº„Éà„Çª„ÉÉ„Éà„É©„Éô„É´ (n_support,)
        Returns:
            predictions: „ÇØ„Ç®„É™„ÅÆ‰∫àÊ∏¨„É©„Éô„É´
        &quot;&quot;&quot;
        # „Çµ„Éù„Éº„Éà„Çª„ÉÉ„Éà„Å®„ÇØ„Ç®„É™„ÅÆÂüã„ÇÅËæº„Åø
        support_embeddings = self.encoder(support_ids, support_mask)[:, 0, :]  # [CLS]
        query_embeddings = self.encoder(query_ids, query_mask)[:, 0, :]

        # ÂêÑ„ÇØ„É©„Çπ„ÅÆ„Éó„É≠„Éà„Çø„Ç§„ÉóÔºàÂπ≥ÂùáÂüã„ÇÅËæº„ÅøÔºâ„ÇíË®àÁÆó
        unique_labels = torch.unique(support_labels)
        prototypes = []
        for label in unique_labels:
            mask = (support_labels == label)
            prototype = support_embeddings[mask].mean(dim=0)
            prototypes.append(prototype)

        prototypes = torch.stack(prototypes)  # (num_classes, d_model)

        # „ÇØ„Ç®„É™„Å®„Éó„É≠„Éà„Çø„Ç§„ÉóÈñì„ÅÆË∑ùÈõ¢
        distances = torch.cdist(query_embeddings, prototypes)  # (n_query, num_classes)

        # ÊúÄ„ÇÇËøë„ÅÑ„Éó„É≠„Éà„Çø„Ç§„Éó„ÅÆ„ÇØ„É©„Çπ„Çí‰∫àÊ∏¨
        predictions = torch.argmin(distances, dim=1)

        return predictions

# ‰ΩøÁî®‰æã: 3-way 5-shotÂàÜÈ°û
# 3„ÇØ„É©„Çπ„ÄÅÂêÑ„ÇØ„É©„Çπ5„Çµ„É≥„Éó„É´
n_classes = 3
n_support_per_class = 5
n_query = 10

support_ids = torch.randint(0, vocab_size, (n_classes * n_support_per_class, 32))
support_mask = torch.ones_like(support_ids)
support_labels = torch.arange(n_classes).repeat_interleave(n_support_per_class)

query_ids = torch.randint(0, vocab_size, (n_query, 32))
query_mask = torch.ones_like(query_ids)

proto_net = PrototypicalNetwork(model)
predictions = proto_net(support_ids, support_mask, query_ids, query_mask, support_labels)
print(f&quot;Predictions: {predictions}&quot;)
</code></pre>
<hr />
<h2>3.5 „Éó„É≠„É≥„Éó„Éà„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞</h2>
<h3>ÊùêÊñôÁßëÂ≠¶„Åß„ÅÆ„Éó„É≠„É≥„Éó„Éà</h3>
<p><strong>„Éó„É≠„É≥„Éó„Éà</strong>: „É¢„Éá„É´„Å´ËøΩÂä†ÊÉÖÂ†±„Çí‰∏é„Åà„Å¶ÊÄßËÉΩ„ÇíÂêë‰∏ä</p>
<p><strong>‰æã</strong>:</p>
<pre><code class="language-python"># ÈÄöÂ∏∏: 'Fe2O3'
# „Éó„É≠„É≥„Éó„Éà‰ªò„Åç: '[OXIDE] Fe2O3 [BANDGAP]'
</code></pre>
<h3>ÂÆüË£Ö</h3>
<pre><code class="language-python">class PromptedMatBERT(nn.Module):
    def __init__(self, matbert_model, d_model=512):
        super(PromptedMatBERT, self).__init__()
        self.matbert = matbert_model

        # „Çø„Çπ„ÇØÂà•„Éó„É≠„É≥„Éó„ÉàÂüã„ÇÅËæº„ÅøÔºàÂ≠¶ÁøíÂèØËÉΩÔºâ
        self.task_prompts = nn.Parameter(torch.randn(10, d_model))  # 10Á®ÆÈ°û„ÅÆ„Çø„Çπ„ÇØ

    def forward(self, input_ids, attention_mask, task_id=0):
        &quot;&quot;&quot;
        Args:
            input_ids: (batch_size, seq_len)
            attention_mask: (batch_size, seq_len)
            task_id: „Çø„Çπ„ÇØID (0-9)
        &quot;&quot;&quot;
        batch_size = input_ids.size(0)

        # ÈÄöÂ∏∏„ÅÆÂüã„ÇÅËæº„Åø
        embeddings = self.matbert(input_ids, attention_mask)

        # „Çø„Çπ„ÇØ„Éó„É≠„É≥„Éó„Éà„ÇíÂÖàÈ†≠„Å´ËøΩÂä†
        task_prompt = self.task_prompts[task_id].unsqueeze(0).expand(batch_size, -1, -1)
        embeddings = torch.cat([task_prompt, embeddings], dim=1)

        return embeddings

# ‰ΩøÁî®‰æã
prompted_model = PromptedMatBERT(model)

# „Çø„Çπ„ÇØ0: „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó‰∫àÊ∏¨
embeddings_task0 = prompted_model(input_ids, attention_mask, task_id=0)

# „Çø„Çπ„ÇØ1: ÂΩ¢Êàê„Ç®„Éç„É´„ÇÆ„Éº‰∫àÊ∏¨
embeddings_task1 = prompted_model(input_ids, attention_mask, task_id=1)

print(f&quot;Embeddings with prompt shape: {embeddings_task0.shape}&quot;)
</code></pre>
<hr />
<h2>3.6 „Éâ„É°„Ç§„É≥ÈÅ©Âøú</h2>
<h3>Ê¶ÇË¶Å</h3>
<p><strong>„Éâ„É°„Ç§„É≥ÈÅ©Âøú</strong>: „ÇΩ„Éº„Çπ„Éâ„É°„Ç§„É≥„ÅßË®ìÁ∑¥„Åó„Åü„É¢„Éá„É´„Çí„Çø„Éº„Ç≤„ÉÉ„Éà„Éâ„É°„Ç§„É≥„Å´ÈÅ©Âøú</p>
<p><strong>‰æã</strong>:
- „ÇΩ„Éº„Çπ: ÁÑ°Ê©üÊùêÊñô„Éá„Éº„Çø
- „Çø„Éº„Ç≤„ÉÉ„Éà: ÊúâÊ©üÂàÜÂ≠ê„Éá„Éº„Çø</p>
<h3>Adversarial Domain Adaptation</h3>
<pre><code class="language-python">class DomainClassifier(nn.Module):
    def __init__(self, d_model=512):
        super(DomainClassifier, self).__init__()
        self.classifier = nn.Sequential(
            nn.Linear(d_model, 256),
            nn.ReLU(),
            nn.Linear(256, 2)  # „ÇΩ„Éº„Çπ or „Çø„Éº„Ç≤„ÉÉ„Éà
        )

    def forward(self, embeddings):
        return self.classifier(embeddings)

class DomainAdaptiveMatBERT(nn.Module):
    def __init__(self, matbert_model):
        super(DomainAdaptiveMatBERT, self).__init__()
        self.matbert = matbert_model
        self.domain_classifier = DomainClassifier()
        self.task_predictor = nn.Linear(512, 1)  # ‰æã: „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó‰∫àÊ∏¨

    def forward(self, input_ids, attention_mask, alpha=1.0):
        &quot;&quot;&quot;
        Args:
            alpha: „Éâ„É°„Ç§„É≥ÈÅ©Âøú„ÅÆÂº∑„Åï
        &quot;&quot;&quot;
        embeddings = self.matbert(input_ids, attention_mask)[:, 0, :]

        # „Çø„Çπ„ÇØ‰∫àÊ∏¨
        task_output = self.task_predictor(embeddings)

        # „Éâ„É°„Ç§„É≥‰∫àÊ∏¨ÔºàÂãæÈÖçÂèçËª¢Â±§„Çí‰ΩøÁî®Ôºâ
        # „Åì„Åì„Åß„ÅØÁ∞°Áï•Âåñ„ÅÆ„Åü„ÇÅÁúÅÁï•
        domain_output = self.domain_classifier(embeddings)

        return task_output, domain_output

# Ë®ìÁ∑¥„É´„Éº„ÉóÔºàÁ∞°Áï•ÁâàÔºâ
def train_domain_adaptive(model, source_loader, target_loader, epochs=20):
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
    task_criterion = nn.MSELoss()
    domain_criterion = nn.CrossEntropyLoss()

    for epoch in range(epochs):
        for (source_ids, source_mask, source_labels), (target_ids, target_mask, _) in zip(source_loader, target_loader):
            # „ÇΩ„Éº„Çπ„Éâ„É°„Ç§„É≥
            source_task, source_domain = model(source_ids, source_mask)
            source_domain_labels = torch.zeros(source_ids.size(0), dtype=torch.long)  # „ÇΩ„Éº„Çπ = 0

            # „Çø„Éº„Ç≤„ÉÉ„Éà„Éâ„É°„Ç§„É≥
            target_task, target_domain = model(target_ids, target_mask)
            target_domain_labels = torch.ones(target_ids.size(0), dtype=torch.long)  # „Çø„Éº„Ç≤„ÉÉ„Éà = 1

            # ÊêçÂ§±
            task_loss = task_criterion(source_task, source_labels)
            domain_loss = domain_criterion(source_domain, source_domain_labels) + \
                          domain_criterion(target_domain, target_domain_labels)

            total_loss = task_loss + 0.1 * domain_loss

            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()

        print(f&quot;Epoch {epoch+1}, Task Loss: {task_loss.item():.4f}, Domain Loss: {domain_loss.item():.4f}&quot;)
</code></pre>
<hr />
<h2>3.7 „Åæ„Å®„ÇÅ</h2>
<h3>ÈáçË¶Å„Éù„Ç§„É≥„Éà</h3>
<ol>
<li><strong>‰∫ãÂâçÂ≠¶Áøí</strong>: Â§ßË¶èÊ®°„É©„Éô„É´„Å™„Åó„Éá„Éº„Çø„Åß‰∏ÄËà¨ÁöÑÁü•Ë≠ò„ÇíÁç≤Âæó</li>
<li><strong>„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞</strong>: Â∞ëÈáè„Éá„Éº„Çø„Åß„Çø„Çπ„ÇØÁâπÂåñ</li>
<li><strong>Few-shotÂ≠¶Áøí</strong>: Êï∞ÂÄã„ÅÆ„Çµ„É≥„Éó„É´„ÅßÊñ∞„Çø„Çπ„ÇØÂ≠¶Áøí</li>
<li><strong>„Éó„É≠„É≥„Éó„Éà„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞</strong>: „Çø„Çπ„ÇØÊÉÖÂ†±„ÇíÂüã„ÇÅËæº„Åø„ÅßË°®Áèæ</li>
<li><strong>„Éâ„É°„Ç§„É≥ÈÅ©Âøú</strong>: Áï∞„Å™„Çã„Éâ„É°„Ç§„É≥Èñì„ÅßÁü•Ë≠òËª¢Áßª</li>
</ol>
<h3>Ê¨°Á´†„Å∏„ÅÆÊ∫ñÂÇô</h3>
<p>Á¨¨4Á´†„Åß„ÅØ„ÄÅÊã°Êï£„É¢„Éá„É´„Å´„Çà„ÇãÂàÜÂ≠êÁîüÊàê„Å®ÊùêÊñôÈÄÜË®≠Ë®à„ÇíÂ≠¶„Å≥„Åæ„Åô„ÄÇ</p>
<hr />
<h2>üìù ÊºîÁøíÂïèÈ°å</h2>
<h3>ÂïèÈ°å1: Ê¶ÇÂøµÁêÜËß£</h3>
<p>„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÅÆ3„Å§„ÅÆÊà¶Áï•ÔºàFull„ÄÅFeature Extraction„ÄÅPartialÔºâ„Å´„Å§„ÅÑ„Å¶„ÄÅ„Åù„Çå„Åû„Çå„Å©„ÅÆ„Çà„ÅÜ„Å™Â†¥Âêà„Å´ÈÅ©„Åó„Å¶„ÅÑ„Çã„ÅãË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<details>
<summary>Ëß£Á≠î‰æã</summary>

1. **Full Fine-tuning**:
   - **ÈÅ©Áî®Â†¥Èù¢**: „Çø„Éº„Ç≤„ÉÉ„Éà„Éâ„É°„Ç§„É≥„ÅÆ„Éá„Éº„Çø„ÅåÊØîËºÉÁöÑÂ§ö„ÅÑÔºàÊï∞ÂçÉ„Çµ„É≥„Éó„É´‰ª•‰∏äÔºâ
   - **Âà©ÁÇπ**: ÊúÄÈ´òÁ≤æÂ∫¶„ÇíÈÅîÊàêÂèØËÉΩ
   - **Ê¨†ÁÇπ**: ÈÅéÂ≠¶Áøí„É™„Çπ„ÇØ„ÄÅË®àÁÆó„Ç≥„Çπ„ÉàÂ§ß

2. **Feature Extraction**:
   - **ÈÅ©Áî®Â†¥Èù¢**: „Éá„Éº„Çø„ÅåÈùûÂ∏∏„Å´Â∞ë„Å™„ÅÑÔºàÊï∞ÂçÅ„ÄúÊï∞Áôæ„Çµ„É≥„Éó„É´Ôºâ
   - **Âà©ÁÇπ**: ÈÅéÂ≠¶Áøí„ÇíÈò≤„Åé„ÇÑ„Åô„ÅÑ„ÄÅÈ´òÈÄü
   - **Ê¨†ÁÇπ**: „Éâ„É°„Ç§„É≥„ÅåÂ§ß„Åç„ÅèÁï∞„Å™„ÇãÂ†¥Âêà„ÅØÁ≤æÂ∫¶‰Ωé‰∏ã

3. **Partial Fine-tuning**:
   - **ÈÅ©Áî®Â†¥Èù¢**: ‰∏≠Á®ãÂ∫¶„ÅÆ„Éá„Éº„ÇøÈáè„ÄÅ„Éâ„É°„Ç§„É≥„ÅåÈ°û‰ºº
   - **Âà©ÁÇπ**: „Éê„É©„É≥„Çπ„ÅÆÂèñ„Çå„ÅüÊÄßËÉΩ„Å®„Ç≥„Çπ„Éà
   - **Ê¨†ÁÇπ**: „Å©„ÅÆÂ±§„ÇíÊõ¥Êñ∞„Åô„Çã„ÅãÈÅ∏Êäû„ÅåÈõ£„Åó„ÅÑ
</details>

<h3>ÂïèÈ°å2: ÂÆüË£Ö</h3>
<p>‰ª•‰∏ã„ÅÆ„Ç≥„Éº„Éâ„ÅÆÁ©∫Ê¨Ñ„ÇíÂüã„ÇÅ„Å¶„ÄÅ‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„Çí„É≠„Éº„Éâ„Åó„Å¶„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åô„ÇãÈñ¢Êï∞„ÇíÂÆåÊàê„Åï„Åõ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<pre><code class="language-python">def load_and_finetune(pretrained_path, train_loader, val_loader):
    # ‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„Çí„É≠„Éº„Éâ
    matbert = MatBERT(vocab_size=______, d_model=512)
    matbert.load_state_dict(torch.load(______))

    # „Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞Áî®„É¢„Éá„É´„ÇíÊßãÁØâ
    model = MatBERTForBandgap(______)

    # ÊúÄÈÅ©Âåñ
    optimizer = torch.optim.Adam(______.parameters(), lr=1e-5)
    criterion = nn.MSELoss()

    # Ë®ìÁ∑¥„É´„Éº„Éó
    for epoch in range(10):
        model.train()
        for input_ids, attention_mask, targets in train_loader:
            predictions = model(______, ______)
            loss = ______(predictions, targets)

            optimizer.zero_grad()
            ______.backward()
            optimizer.step()

    return model
</code></pre>
<details>
<summary>Ëß£Á≠î‰æã</summary>


<pre><code class="language-python">def load_and_finetune(pretrained_path, train_loader, val_loader):
    # ‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„Çí„É≠„Éº„Éâ
    matbert = MatBERT(vocab_size=len(tokenizer.vocab), d_model=512)
    matbert.load_state_dict(torch.load(pretrained_path))

    # „Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞Áî®„É¢„Éá„É´„ÇíÊßãÁØâ
    model = MatBERTForBandgap(matbert)

    # ÊúÄÈÅ©Âåñ
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
    criterion = nn.MSELoss()

    # Ë®ìÁ∑¥„É´„Éº„Éó
    for epoch in range(10):
        model.train()
        for input_ids, attention_mask, targets in train_loader:
            predictions = model(input_ids, attention_mask)
            loss = criterion(predictions, targets)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    return model
</code></pre>

</details>

<h3>ÂïèÈ°å3: ÂøúÁî®</h3>
<p>ÊùêÊñôÁßëÂ≠¶„Åß Few-shotÂ≠¶Áøí„ÅåÁâπ„Å´ÊúâÁî®„Å™3„Å§„ÅÆ„Ç∑„Éä„É™„Ç™„ÇíÊåô„Åí„ÄÅ„Åù„Çå„Åû„Çå„ÅÆÁêÜÁî±„ÇíË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<details>
<summary>Ëß£Á≠î‰æã</summary>

1. **Êñ∞Ë¶èÊùêÊñô„ÅÆËøÖÈÄüË©ï‰æ°**:
   - **„Ç∑„Éä„É™„Ç™**: Êñ∞„Åó„ÅÑ„ÇØ„É©„Çπ„ÅÆÊùêÊñôÔºà‰æã: Êñ∞Âûã„Éö„É≠„Éñ„Çπ„Ç´„Ç§„ÉàÔºâ
   - **ÁêÜÁî±**: ÂÆüÈ®ì„Éá„Éº„Çø„Åå„Åæ„Å†Â∞ë„Å™„Åè„ÄÅÊï∞„Çµ„É≥„Éó„É´„ÅßÁâπÊÄß‰∫àÊ∏¨„ÅåÂøÖË¶Å

2. **ÂÆüÈ®ìË®àÁîª„ÅÆÂäπÁéáÂåñ**:
   - **„Ç∑„Éä„É™„Ç™**: È´ò„Ç≥„Çπ„Éà„Å™ÂÆüÈ®ìÔºàÂçòÁµêÊô∂ÊàêÈï∑„ÄÅÈ´òÂúßÂêàÊàêÔºâ
   - **ÁêÜÁî±**: Â∞ëÊï∞„ÅÆÂÆüÈ®ìÁµêÊûú„Åã„ÇâÊ¨°„ÅÆÂÆüÈ®ìÊù°‰ª∂„ÇíÊèêÊ°à

3. **‰ºÅÊ•≠„ÅÆÁã¨Ëá™ÊùêÊñôÈñãÁô∫**:
   - **„Ç∑„Éä„É™„Ç™**: Á´∂Âêà„Å´ÂÖ¨Èñã„Åß„Åç„Å™„ÅÑÁã¨Ëá™ÊùêÊñô
   - **ÁêÜÁî±**: Á§æÂÜÖ„Éá„Éº„Çø„ÅÆ„Åø„ÅßÂ≠¶Áøí„ÄÅÂ§ñÈÉ®„Éá„Éº„Çø„ÅØ‰Ωø„Åà„Å™„ÅÑ
</details>

<hr />
<h2>üöÄ ÂÆüË£ÖÊºîÁøí: Transformer for Materials</h2>
<h3>ÊºîÁøí1: MatBERTÂÆüË£ÖÔºàBERT for MaterialsÔºâ</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
from transformers import BertConfig, BertModel

class MaterialsBERT(nn.Module):
    def __init__(self, vocab_size=120, d_model=768, num_layers=12, num_heads=12):
        &quot;&quot;&quot;
        Materials BERT implementation

        Args:
            vocab_size: ÂéüÂ≠êÁ®ÆÊï∞ + ÁâπÊÆä„Éà„Éº„ÇØ„É≥
            d_model: Èö†„ÇåÂ±§Ê¨°ÂÖÉ
            num_layers: Transformer„É¨„Ç§„É§„ÉºÊï∞
            num_heads: Attention„Éò„ÉÉ„ÉâÊï∞
        &quot;&quot;&quot;
        super().__init__()

        # BERT configuration
        config = BertConfig(
            vocab_size=vocab_size,
            hidden_size=d_model,
            num_hidden_layers=num_layers,
            num_attention_heads=num_heads,
            intermediate_size=d_model * 4,
            hidden_dropout_prob=0.1,
            attention_probs_dropout_prob=0.1,
            max_position_embeddings=512
        )

        self.bert = BertModel(config)

    def forward(self, input_ids, attention_mask=None, token_type_ids=None):
        &quot;&quot;&quot;
        Args:
            input_ids: (batch_size, seq_len) ÂéüÂ≠êÁï™Âè∑„Ç∑„Éº„Ç±„É≥„Çπ
            attention_mask: (batch_size, seq_len)
            token_type_ids: (batch_size, seq_len)
        Returns:
            outputs: BERT outputs with pooler_output
        &quot;&quot;&quot;
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids
        )

        return outputs

# ‰ΩøÁî®‰æã
mat_bert = MaterialsBERT(vocab_size=120, d_model=768)

# „ÉÄ„Éü„Éº„Éá„Éº„Çø: Fe2O3 (ÈÖ∏ÂåñÈâÑ)
# [CLS] Fe Fe O O O [SEP]
input_ids = torch.tensor([[101, 26, 26, 8, 8, 8, 102]])  # 101=[CLS], 102=[SEP]
attention_mask = torch.ones_like(input_ids)

outputs = mat_bert(input_ids, attention_mask)
print(f&quot;Last hidden state shape: {outputs.last_hidden_state.shape}&quot;)  # (1, 7, 768)
print(f&quot;Pooler output shape: {outputs.pooler_output.shape}&quot;)  # (1, 768)
</code></pre>
<h3>ÊºîÁøí2: MatGPTÂÆüË£ÖÔºàGPT for Materials GenerationÔºâ</h3>
<pre><code class="language-python">from transformers import GPT2Config, GPT2LMHeadModel

class MaterialsGPT(nn.Module):
    def __init__(self, vocab_size=120, d_model=768, num_layers=12, num_heads=12):
        &quot;&quot;&quot;
        Materials GPT for generative tasks

        Args:
            vocab_size: ÂéüÂ≠êÁ®ÆÊï∞ + ÁâπÊÆä„Éà„Éº„ÇØ„É≥
            d_model: Èö†„ÇåÂ±§Ê¨°ÂÖÉ
            num_layers: Transformer„É¨„Ç§„É§„ÉºÊï∞
            num_heads: Attention„Éò„ÉÉ„ÉâÊï∞
        &quot;&quot;&quot;
        super().__init__()

        config = GPT2Config(
            vocab_size=vocab_size,
            n_positions=512,
            n_embd=d_model,
            n_layer=num_layers,
            n_head=num_heads,
            resid_pdrop=0.1,
            embd_pdrop=0.1,
            attn_pdrop=0.1
        )

        self.gpt = GPT2LMHeadModel(config)

    def forward(self, input_ids, labels=None):
        &quot;&quot;&quot;
        Args:
            input_ids: (batch_size, seq_len)
            labels: (batch_size, seq_len) for training
        &quot;&quot;&quot;
        outputs = self.gpt(input_ids=input_ids, labels=labels)
        return outputs

    def generate_composition(self, start_tokens, max_length=50, temperature=1.0):
        &quot;&quot;&quot;
        ÁµÑÊàêÂºèÁîüÊàê

        Args:
            start_tokens: (1, start_len) ÈñãÂßã„Éà„Éº„ÇØ„É≥
            max_length: ÊúÄÂ§ßÁîüÊàêÈï∑
            temperature: „Çµ„É≥„Éó„É™„É≥„Ç∞Ê∏©Â∫¶
        &quot;&quot;&quot;
        self.eval()
        with torch.no_grad():
            for _ in range(max_length - start_tokens.size(1)):
                outputs = self.gpt(start_tokens)
                logits = outputs.logits[:, -1, :] / temperature

                probs = torch.softmax(logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)

                start_tokens = torch.cat([start_tokens, next_token], dim=1)

                # [SEP]„Éà„Éº„ÇØ„É≥„ÅßÂÅúÊ≠¢
                if next_token.item() == 102:
                    break

        return start_tokens

# ‰ΩøÁî®‰æã
mat_gpt = MaterialsGPT(vocab_size=120, d_model=768)

# ÁîüÊàê: [CLS] Fe ... (ÈÖ∏ÂåñÁâ©„ÇíÁîüÊàê)
start = torch.tensor([[101, 26]])  # [CLS] Fe
generated = mat_gpt.generate_composition(start, max_length=20)
print(f&quot;Generated sequence: {generated}&quot;)
</code></pre>
<h3>ÊºîÁøí3: MatT5ÂÆüË£ÖÔºàT5 for Materials Seq2SeqÔºâ</h3>
<pre><code class="language-python">from transformers import T5Config, T5ForConditionalGeneration

class MaterialsT5(nn.Module):
    def __init__(self, vocab_size=120, d_model=512, num_layers=6):
        &quot;&quot;&quot;
        Materials T5 for sequence-to-sequence tasks
        (e.g., composition ‚Üí properties description)

        Args:
            vocab_size: Ë™ûÂΩô„Çµ„Ç§„Ç∫
            d_model: „É¢„Éá„É´Ê¨°ÂÖÉ
            num_layers: „Ç®„É≥„Ç≥„Éº„ÉÄ„Éª„Éá„Ç≥„Éº„ÉÄÂ±§Êï∞
        &quot;&quot;&quot;
        super().__init__()

        config = T5Config(
            vocab_size=vocab_size,
            d_model=d_model,
            d_kv=64,
            d_ff=d_model * 4,
            num_layers=num_layers,
            num_decoder_layers=num_layers,
            num_heads=8,
            dropout_rate=0.1
        )

        self.t5 = T5ForConditionalGeneration(config)

    def forward(self, input_ids, attention_mask=None, labels=None):
        &quot;&quot;&quot;
        Args:
            input_ids: (batch_size, src_len) ÂÖ•ÂäõÁ≥ªÂàó
            labels: (batch_size, tgt_len) „Çø„Éº„Ç≤„ÉÉ„ÉàÁ≥ªÂàó
        &quot;&quot;&quot;
        outputs = self.t5(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        return outputs

    def predict_properties(self, composition_ids, max_length=50):
        &quot;&quot;&quot;
        ÁµÑÊàêÂºè„Åã„ÇâÁâπÊÄßË®òËø∞„ÇíÁîüÊàê

        Args:
            composition_ids: (batch_size, seq_len) ÁµÑÊàêÂºè
            max_length: ÊúÄÂ§ßÁîüÊàêÈï∑
        &quot;&quot;&quot;
        self.eval()
        with torch.no_grad():
            outputs = self.t5.generate(
                composition_ids,
                max_length=max_length,
                num_beams=4,
                early_stopping=True
            )
        return outputs

# ‰ΩøÁî®‰æã
mat_t5 = MaterialsT5(vocab_size=120, d_model=512)

# ÂÖ•Âäõ: Fe2O3 ‚Üí Âá∫Âäõ: &quot;semiconductor bandgap 2.0 eV&quot;
input_ids = torch.tensor([[26, 26, 8, 8, 8]])  # Fe Fe O O O
outputs = mat_t5.predict_properties(input_ids, max_length=20)
print(f&quot;Predicted properties: {outputs}&quot;)
</code></pre>
<hr />
<h2>üß™ SMILES/SELFIES „Éà„Éº„ÇØ„É≥Âåñ„ÅÆÂÆüË£Ö</h2>
<h3>SMILES Tokenizer</h3>
<pre><code class="language-python">import re
from typing import List, Dict

class SMILESTokenizer:
    &quot;&quot;&quot;
    SMILESÊñáÂ≠óÂàó„ÅÆÂÆåÂÖ®„Éà„Éº„ÇØ„É≥Âåñ

    ÂØæÂøú:
    - Ëä≥È¶ôÊóèÊÄß (c, n, o, s)
    - Á´ã‰ΩìÂåñÂ≠¶ (@, @@, /, \\)
    - ÂàÜÂ≤ê ((, ))
    - ÁµêÂêà (-, =, #, :)
    - Áí∞ (Êï∞Â≠ó)
    &quot;&quot;&quot;

    def __init__(self):
        # Ê≠£Ë¶èË°®Áèæ„Éë„Çø„Éº„É≥ÔºàÂÑ™ÂÖàÈ†Ü‰ΩçÈ†ÜÔºâ
        self.pattern = r'(\[[^\]]+\]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\(|\)|\.|=|#|-|\+|\\|\/|:|~|@|\?|&gt;|\*|\$|\%[0-9]{2}|[0-9])'

        # ÁâπÊÆä„Éà„Éº„ÇØ„É≥
        self.special_tokens = {
            '[PAD]': 0,
            '[CLS]': 1,
            '[SEP]': 2,
            '[MASK]': 3,
            '[UNK]': 4
        }

        # Ë™ûÂΩô„ÅÆÊßãÁØâ
        self.vocab = self._build_vocab()
        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}
        self.id_to_token = {i: token for token, i in self.token_to_id.items()}

    def _build_vocab(self) -&gt; List[str]:
        &quot;&quot;&quot;Ë™ûÂΩô„ÇíÊßãÁØâ&quot;&quot;&quot;
        vocab = list(self.special_tokens.keys())

        # ÂÖÉÁ¥†Ë®òÂè∑
        elements = ['C', 'N', 'O', 'S', 'P', 'F', 'Cl', 'Br', 'I',
                   'c', 'n', 'o', 's', 'p']  # Ëä≥È¶ôÊóè

        # Ë®òÂè∑
        symbols = ['(', ')', '[', ']', '=', '#', '-', '+', '\\', '/',
                  ':', '.', '@', '@@']

        # Êï∞Â≠ó
        numbers = [str(i) for i in range(10)]

        vocab.extend(elements + symbols + numbers)

        return vocab

    def tokenize(self, smiles: str) -&gt; List[str]:
        &quot;&quot;&quot;
        SMILESÊñáÂ≠óÂàó„Çí„Éà„Éº„ÇØ„É≥Âåñ

        Args:
            smiles: SMILESÊñáÂ≠óÂàó

        Returns:
            tokens: „Éà„Éº„ÇØ„É≥„ÅÆ„É™„Çπ„Éà

        Examples:
            &gt;&gt;&gt; tokenizer = SMILESTokenizer()
            &gt;&gt;&gt; tokenizer.tokenize(&quot;CC(C)Cc1ccc(cc1)C(C)C(=O)O&quot;)
            ['C', 'C', '(', 'C', ')', 'C', 'c', '1', 'c', 'c', 'c', '(', ...]
        &quot;&quot;&quot;
        tokens = re.findall(self.pattern, smiles)
        return ['[CLS]'] + tokens + ['[SEP]']

    def encode(self, smiles: str, max_length: int = 128) -&gt; Dict[str, torch.Tensor]:
        &quot;&quot;&quot;
        SMILESÊñáÂ≠óÂàó„ÇíID„Å´Â§âÊèõ

        Args:
            smiles: SMILESÊñáÂ≠óÂàó
            max_length: ÊúÄÂ§ßÈï∑

        Returns:
            encoding: input_ids, attention_mask
        &quot;&quot;&quot;
        tokens = self.tokenize(smiles)

        # „Éà„Éº„ÇØ„É≥„ÇíID„Å´Â§âÊèõ
        ids = [self.token_to_id.get(token, self.token_to_id['[UNK]'])
               for token in tokens]

        # „Éë„Éá„Ç£„É≥„Ç∞
        attention_mask = [1] * len(ids)
        while len(ids) &lt; max_length:
            ids.append(self.token_to_id['[PAD]'])
            attention_mask.append(0)

        # „Éà„É©„É≥„Ç±„Éº„Ç∑„Éß„É≥
        ids = ids[:max_length]
        attention_mask = attention_mask[:max_length]

        return {
            'input_ids': torch.tensor([ids]),
            'attention_mask': torch.tensor([attention_mask])
        }

    def decode(self, ids: List[int]) -&gt; str:
        &quot;&quot;&quot;ID„Åã„ÇâSMILESÊñáÂ≠óÂàó„Å´Âæ©ÂÖÉ&quot;&quot;&quot;
        tokens = [self.id_to_token.get(id, '[UNK]') for id in ids]
        # ÁâπÊÆä„Éà„Éº„ÇØ„É≥„ÇíÈô§Âéª
        tokens = [t for t in tokens if t not in self.special_tokens]
        return ''.join(tokens)

# ‰ΩøÁî®‰æã
tokenizer = SMILESTokenizer()

# „Ç§„Éñ„Éó„É≠„Éï„Çß„É≥
smiles = &quot;CC(C)Cc1ccc(cc1)C(C)C(=O)O&quot;
tokens = tokenizer.tokenize(smiles)
print(f&quot;Tokens: {tokens[:10]}...&quot;)

encoding = tokenizer.encode(smiles)
print(f&quot;Input IDs shape: {encoding['input_ids'].shape}&quot;)
print(f&quot;First 10 IDs: {encoding['input_ids'][0][:10]}&quot;)

# „Éá„Ç≥„Éº„Éâ
decoded = tokenizer.decode(encoding['input_ids'][0].tolist())
print(f&quot;Decoded: {decoded}&quot;)
</code></pre>
<h3>SELFIES Tokenizer</h3>
<pre><code class="language-python">try:
    import selfies as sf
except ImportError:
    print(&quot;Install selfies: pip install selfies&quot;)

class SELFIESTokenizer:
    &quot;&quot;&quot;
    SELFIES (SELF-referencIng Embedded Strings) Tokenizer

    Âà©ÁÇπ:
    - 100%ÊúâÂäπ„Å™ÂàÜÂ≠ê„ÇíÁîüÊàê
    - ÊñáÊ≥ïÁöÑ„Å´Ê≠£„Åó„ÅÑ
    - SMILES„Çà„ÇäÈ†ëÂÅ•
    &quot;&quot;&quot;

    def __init__(self):
        self.special_tokens = {
            '[PAD]': 0,
            '[CLS]': 1,
            '[SEP]': 2,
            '[MASK]': 3
        }

        # ‰∏ÄËà¨ÁöÑ„Å™SELFIES„Éà„Éº„ÇØ„É≥
        self.vocab = self._build_vocab()
        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}
        self.id_to_token = {i: token for token, i in self.token_to_id.items()}

    def _build_vocab(self) -&gt; List[str]:
        &quot;&quot;&quot;
        SELFIESË™ûÂΩô„ÇíÊßãÁØâ

        ‰∏ÄËà¨ÁöÑ„Å™„Éà„Éº„ÇØ„É≥:
        [C], [N], [O], [=C], [=N], [Ring1], [Branch1], etc.
        &quot;&quot;&quot;
        vocab = list(self.special_tokens.keys())

        # Âü∫Êú¨„Éà„Éº„ÇØ„É≥
        common_tokens = [
            '[C]', '[N]', '[O]', '[S]', '[P]', '[F]', '[Cl]', '[Br]', '[I]',
            '[=C]', '[=N]', '[=O]', '[#C]', '[#N]',
            '[Ring1]', '[Ring2]', '[Branch1]', '[Branch2]',
            '[O-1]', '[N+1]', '[nop]'
        ]

        vocab.extend(common_tokens)
        return vocab

    def smiles_to_selfies(self, smiles: str) -&gt; str:
        &quot;&quot;&quot;SMILES„ÇíSELFIES„Å´Â§âÊèõ&quot;&quot;&quot;
        try:
            selfies = sf.encoder(smiles)
            return selfies
        except Exception as e:
            print(f&quot;Encoding error: {e}&quot;)
            return &quot;&quot;

    def selfies_to_smiles(self, selfies: str) -&gt; str:
        &quot;&quot;&quot;SELFIES„ÇíSMILES„Å´Â§âÊèõ&quot;&quot;&quot;
        try:
            smiles = sf.decoder(selfies)
            return smiles
        except Exception as e:
            print(f&quot;Decoding error: {e}&quot;)
            return &quot;&quot;

    def tokenize(self, selfies: str) -&gt; List[str]:
        &quot;&quot;&quot;
        SELFIESÊñáÂ≠óÂàó„Çí„Éà„Éº„ÇØ„É≥Âåñ

        Args:
            selfies: SELFIESÊñáÂ≠óÂàó

        Returns:
            tokens: „Éà„Éº„ÇØ„É≥„ÅÆ„É™„Çπ„Éà

        Examples:
            &gt;&gt;&gt; tokenizer = SELFIESTokenizer()
            &gt;&gt;&gt; tokenizer.tokenize(&quot;[C][C][Branch1][C][C][C]&quot;)
            ['[CLS]', '[C]', '[C]', '[Branch1]', '[C]', '[C]', '[C]', '[SEP]']
        &quot;&quot;&quot;
        tokens = list(sf.split_selfies(selfies))
        return ['[CLS]'] + tokens + ['[SEP]']

    def encode(self, selfies: str, max_length: int = 128) -&gt; Dict[str, torch.Tensor]:
        &quot;&quot;&quot;SELFIESÊñáÂ≠óÂàó„ÇíID„Å´Â§âÊèõ&quot;&quot;&quot;
        tokens = self.tokenize(selfies)

        # „Éà„Éº„ÇØ„É≥„ÇíID„Å´Â§âÊèõÔºàÊú™Áü•„Éà„Éº„ÇØ„É≥„ÅØÂãïÁöÑ„Å´ËøΩÂä†Ôºâ
        ids = []
        for token in tokens:
            if token not in self.token_to_id:
                new_id = len(self.vocab)
                self.vocab.append(token)
                self.token_to_id[token] = new_id
                self.id_to_token[new_id] = token
            ids.append(self.token_to_id[token])

        # „Éë„Éá„Ç£„É≥„Ç∞
        attention_mask = [1] * len(ids)
        while len(ids) &lt; max_length:
            ids.append(self.token_to_id['[PAD]'])
            attention_mask.append(0)

        # „Éà„É©„É≥„Ç±„Éº„Ç∑„Éß„É≥
        ids = ids[:max_length]
        attention_mask = attention_mask[:max_length]

        return {
            'input_ids': torch.tensor([ids]),
            'attention_mask': torch.tensor([attention_mask])
        }

# ‰ΩøÁî®‰æã
if 'sf' in dir():
    tokenizer_selfies = SELFIESTokenizer()

    # SMILES„Åã„ÇâSELFIES„Å´Â§âÊèõ
    smiles = &quot;CC(C)Cc1ccc(cc1)C(C)C(=O)O&quot;
    selfies = tokenizer_selfies.smiles_to_selfies(smiles)
    print(f&quot;SELFIES: {selfies}&quot;)

    # „Éà„Éº„ÇØ„É≥Âåñ
    tokens = tokenizer_selfies.tokenize(selfies)
    print(f&quot;Tokens: {tokens[:10]}...&quot;)

    # „Ç®„É≥„Ç≥„Éº„Éâ
    encoding = tokenizer_selfies.encode(selfies)
    print(f&quot;Encoded shape: {encoding['input_ids'].shape}&quot;)
</code></pre>
<hr />
<h2>‚ö†Ô∏è ÂÆüË∑µÁöÑ„Å™ËêΩ„Å®„ÅóÁ©¥„Å®ÂØæÂá¶Ê≥ï</h2>
<h3>1. „Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÅÆÈÅéÂ≠¶Áøí</h3>
<p><strong>ÂïèÈ°å</strong>: Â∞ëÈáè„Éá„Éº„Çø„Åß„ÅÆË®ìÁ∑¥„ÅßÊ§úË®ºÊêçÂ§±„ÅåÁô∫Êï£</p>
<pre><code class="language-python"># ‚ùå ÂïèÈ°å: ÂÖ®„Éë„É©„É°„Éº„Çø„ÇíÂ§ß„Åç„Å™Â≠¶ÁøíÁéá„ÅßÊõ¥Êñ∞
def wrong_finetuning():
    model = MatBERTForBandgap(pretrained_matbert)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Â§ß„Åç„Åô„ÅéÔºÅ

    for epoch in range(100):  # „Ç®„Éù„ÉÉ„ÇØÊï∞„ÇÇÂ§ö„Åô„Åé
        for batch in train_loader:
            loss = compute_loss(batch)
            loss.backward()
            optimizer.step()

# ‚úÖ Ëß£Ê±∫Á≠ñ: Layer-wise learning rate decay + Early stopping
def correct_finetuning():
    model = MatBERTForBandgap(pretrained_matbert)

    # Layer-wise learning rate
    no_decay = ['bias', 'LayerNorm.weight']
    optimizer_grouped_parameters = [
        {
            'params': [p for n, p in model.matbert.named_parameters()
                      if not any(nd in n for nd in no_decay)],
            'weight_decay': 0.01,
            'lr': 2e-5  # ‰∫ãÂâçÂ≠¶ÁøíÈÉ®ÂàÜ„ÅØÂ∞è„Åï„Åè
        },
        {
            'params': [p for n, p in model.matbert.named_parameters()
                      if any(nd in n for nd in no_decay)],
            'weight_decay': 0.0,
            'lr': 2e-5
        },
        {
            'params': model.bandgap_predictor.parameters(),
            'lr': 1e-4  # ‰∫àÊ∏¨„Éò„ÉÉ„Éâ„ÅØÂ§ß„Åç„Åè
        }
    ]

    optimizer = torch.optim.AdamW(optimizer_grouped_parameters)

    # Early stopping
    best_val_loss = float('inf')
    patience = 5
    patience_counter = 0

    for epoch in range(100):
        train_loss = train_epoch(model, train_loader, optimizer)
        val_loss = validate(model, val_loader)

        if val_loss &lt; best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), 'best_model.pt')
            patience_counter = 0
        else:
            patience_counter += 1

        if patience_counter &gt;= patience:
            print(f&quot;Early stopping at epoch {epoch}&quot;)
            break

    # „Éô„Çπ„Éà„É¢„Éá„É´„ÇíÂæ©ÂÖÉ
    model.load_state_dict(torch.load('best_model.pt'))
    return model
</code></pre>
<h3>2. „Éâ„É°„Ç§„É≥„Ç∑„Éï„Éà„ÅÆÂïèÈ°å</h3>
<p><strong>ÂïèÈ°å</strong>: ÁÑ°Ê©üÊùêÊñô„Åß‰∫ãÂâçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„ÇíÊúâÊ©üÂàÜÂ≠ê„Å´ÈÅ©Áî®</p>
<pre><code class="language-python"># ‚ùå ÂïèÈ°å: „Éâ„É°„Ç§„É≥„ÅåÁï∞„Å™„Çã„ÅÆ„Å´Áõ¥Êé•ÈÅ©Áî®
def wrong_domain_adaptation():
    # ÁÑ°Ê©üÊùêÊñô„Åß‰∫ãÂâçÂ≠¶Áøí
    matbert = pretrained_on_inorganic_materials()

    # ÊúâÊ©üÂàÜÂ≠ê„Éá„Éº„Çø„ÅßÁõ¥Êé•„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞
    # ‚Üí ÊÄßËÉΩ„Åå‰Ωé„ÅÑÔºÅ
    finetune_on_organic_molecules(matbert)

# ‚úÖ Ëß£Ê±∫Á≠ñ: Intermediate task transfer
def correct_domain_adaptation():
    # Step 1: ÁÑ°Ê©üÊùêÊñô„Åß‰∫ãÂâçÂ≠¶Áøí
    matbert = pretrained_on_inorganic_materials()

    # Step 2: ‰∏≠Èñì„Çø„Çπ„ÇØÔºàÁÑ°Ê©ü„Å®ÊúâÊ©ü„ÅÆ‰∏≠ÈñìÔºâ„ÅßÁ∂ôÁ∂öÂ≠¶Áøí
    # ‰æã: ÈáëÂ±ûÊúâÊ©üÈ™®Ê†º (MOF) „Éá„Éº„Çø
    matbert = continual_pretrain_on_mof(matbert)

    # Step 3: ÊúâÊ©üÂàÜÂ≠ê„Éá„Éº„Çø„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞
    model = finetune_on_organic_molecules(matbert)

    return model

# „Åæ„Åü„ÅØ: Domain-adversarial training
class DomainAdversarialTraining:
    def train(self, source_data, target_data):
        for source_batch, target_batch in zip(source_data, target_data):
            # Source domain: „Çø„Çπ„ÇØÊêçÂ§±
            source_output = model(source_batch)
            task_loss = compute_task_loss(source_output, source_batch.labels)

            # Both domains: „Éâ„É°„Ç§„É≥ÂàÜÈ°ûÊêçÂ§±ÔºàÈÄÜËª¢ÂãæÈÖçÔºâ
            source_domain_pred = domain_classifier(source_output, reverse_gradient=True)
            target_domain_pred = domain_classifier(target_output, reverse_gradient=True)

            domain_loss = compute_domain_loss(source_domain_pred, target_domain_pred)

            total_loss = task_loss + 0.1 * domain_loss
            total_loss.backward()
            optimizer.step()
</code></pre>
<h3>3. Masked Language Modeling„ÅÆ„Éû„Çπ„ÇØÊà¶Áï•„Éü„Çπ</h3>
<p><strong>ÂïèÈ°å</strong>: „Éû„Çπ„ÇØ„Éë„Çø„Éº„É≥„ÅåÂÅè„Å£„Å¶„ÅÑ„Çã</p>
<pre><code class="language-python"># ‚ùå ÂïèÈ°å: „É©„É≥„ÉÄ„É†„Å´„Éû„Çπ„ÇØÔºàÂåñÂ≠¶ÁöÑ„Å´ÁÑ°ÊÑèÂë≥Ôºâ
def wrong_masking(composition_ids):
    mask_prob = 0.15
    mask = torch.rand(composition_ids.shape) &lt; mask_prob
    composition_ids[mask] = MASK_TOKEN_ID
    return composition_ids

# ‚úÖ Ëß£Ê±∫Á≠ñ: ÂåñÂ≠¶ÁöÑ„Å´ÊÑèÂë≥„ÅÆ„ÅÇ„Çã„Éû„Çπ„ÇØ
def chemically_aware_masking(composition_ids, element_groups):
    &quot;&quot;&quot;
    ÂÖÉÁ¥†„Ç∞„É´„Éº„Éó„ÇíËÄÉÊÖÆ„Åó„Åü„Éû„Çπ„ÇØ

    Args:
        composition_ids: (batch, seq_len)
        element_groups: {group_id: [element_ids]}
            ‰æã: {0: [26, 27, 28], 1: [8, 16]}  # ÈÅ∑ÁßªÈáëÂ±û„ÄÅ„Ç´„É´„Ç≥„Ç≤„É≥
    &quot;&quot;&quot;
    mask_prob = 0.15
    masked_ids = composition_ids.clone()

    for i in range(composition_ids.size(0)):
        # ÂåñÂ≠¶ÁöÑ„Ç∞„É´„Éº„ÉóÂçò‰Ωç„Åß„Éû„Çπ„ÇØ
        for group_id, element_ids in element_groups.items():
            group_positions = torch.isin(composition_ids[i], torch.tensor(element_ids))
            if group_positions.sum() &gt; 0:
                # „Ç∞„É´„Éº„ÉóÂÜÖ„ÅÆ‰∏ÄÈÉ®„Çí„Éû„Çπ„ÇØ
                mask_within_group = torch.rand(group_positions.sum()) &lt; mask_prob
                group_indices = torch.where(group_positions)[0]
                masked_positions = group_indices[mask_within_group]
                masked_ids[i, masked_positions] = MASK_TOKEN_ID

    return masked_ids

# ‰ΩøÁî®‰æã
element_groups = {
    0: [26, 27, 28, 29],  # Fe, Co, Ni, CuÔºàÈÅ∑ÁßªÈáëÂ±ûÔºâ
    1: [8, 16, 34],       # O, S, SeÔºà„Ç´„É´„Ç≥„Ç≤„É≥Ôºâ
    2: [3, 11, 19]        # Li, Na, KÔºà„Ç¢„É´„Ç´„É™ÈáëÂ±ûÔºâ
}

masked_composition = chemically_aware_masking(composition_ids, element_groups)
</code></pre>
<h3>4. Few-shotÂ≠¶Áøí„ÅÆ„Çµ„Éù„Éº„Éà„Çª„ÉÉ„ÉàÈÅ∏Êäû„Éü„Çπ</h3>
<p><strong>ÂïèÈ°å</strong>: „Çµ„Éù„Éº„Éà„Çª„ÉÉ„Éà„ÅåÂÅè„Å£„Å¶„ÅÑ„Çã</p>
<pre><code class="language-python"># ‚ùå ÂïèÈ°å: „É©„É≥„ÉÄ„É†„Å´„Çµ„Éù„Éº„Éà„Çª„ÉÉ„Éà„ÇíÈÅ∏Êäû
def wrong_support_selection(dataset, k=5):
    indices = torch.randperm(len(dataset))[:k]
    return dataset[indices]

# ‚úÖ Ëß£Ê±∫Á≠ñ: Â§öÊßòÊÄß„ÇíËÄÉÊÖÆ„Åó„Åü„Çµ„Éù„Éº„Éà„Çª„ÉÉ„ÉàÈÅ∏Êäû
def diverse_support_selection(dataset, embeddings, k=5):
    &quot;&quot;&quot;
    K-means„ÅßÂ§öÊßò„Å™„Çµ„É≥„Éó„É´„ÇíÈÅ∏Êäû

    Args:
        dataset: „Éá„Éº„Çø„Çª„ÉÉ„Éà
        embeddings: (N, d) „Çµ„É≥„Éó„É´„ÅÆÂüã„ÇÅËæº„Åø
        k: „Çµ„Éù„Éº„Éà„Çª„ÉÉ„Éà size
    &quot;&quot;&quot;
    from sklearn.cluster import KMeans

    # K-means„Åß„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(embeddings.numpy())

    # ÂêÑ„ÇØ„É©„Çπ„Çø„ÅÆ‰∏≠ÂøÉ„Å´ÊúÄ„ÇÇËøë„ÅÑ„Çµ„É≥„Éó„É´„ÇíÈÅ∏Êäû
    support_indices = []
    for i in range(k):
        cluster_indices = torch.where(torch.tensor(labels) == i)[0]
        cluster_embeddings = embeddings[cluster_indices]
        cluster_center = kmeans.cluster_centers_[i]

        # ‰∏≠ÂøÉ„Å´ÊúÄ„ÇÇËøë„ÅÑ„Çµ„É≥„Éó„É´
        distances = torch.norm(cluster_embeddings - torch.tensor(cluster_center), dim=1)
        closest_idx = cluster_indices[torch.argmin(distances)]
        support_indices.append(closest_idx.item())

    return dataset[support_indices]

# ‰ΩøÁî®‰æã
# „Éá„Éº„Çø„Çª„ÉÉ„ÉàÂüã„ÇÅËæº„Åø„Çí‰∫ãÂâçË®àÁÆó
embeddings = compute_embeddings(dataset, matbert)
support_set = diverse_support_selection(dataset, embeddings, k=10)
</code></pre>
<h3>5. „Éó„É≠„É≥„Éó„Éà„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞„ÅÆÊúÄÈÅ©Âåñ‰∏çË∂≥</h3>
<p><strong>ÂïèÈ°å</strong>: Âõ∫ÂÆö„Éó„É≠„É≥„Éó„Éà„ÅßÊÄßËÉΩ„Åå‰Ωé„ÅÑ</p>
<pre><code class="language-python"># ‚ùå ÂïèÈ°å: ÊâãÂãï„ÅßË®≠Ë®à„Åó„ÅüÂõ∫ÂÆö„Éó„É≠„É≥„Éó„Éà
class FixedPromptModel(nn.Module):
    def __init__(self, matbert):
        super().__init__()
        self.matbert = matbert
        # Âõ∫ÂÆö„Éó„É≠„É≥„Éó„Éà
        self.prompt = nn.Parameter(torch.randn(1, 10, 768), requires_grad=False)

# ‚úÖ Ëß£Ê±∫Á≠ñ: Â≠¶ÁøíÂèØËÉΩ„Å™„Éó„É≠„É≥„Éó„ÉàÔºàPrefix-TuningÔºâ
class LearnablePromptModel(nn.Module):
    def __init__(self, matbert, prompt_length=10, num_tasks=5):
        super().__init__()
        self.matbert = matbert
        self.prompt_length = prompt_length

        # „Çø„Çπ„ÇØÂà•„ÅÆÂ≠¶ÁøíÂèØËÉΩ„Å™„Éó„É≠„É≥„Éó„Éà
        self.task_prompts = nn.Parameter(torch.randn(num_tasks, prompt_length, 768))

        # MatBERT„ÅÆ„Éë„É©„É°„Éº„Çø„ÅØÂõ∫ÂÆö
        for param in self.matbert.parameters():
            param.requires_grad = False

    def forward(self, input_ids, task_id=0):
        batch_size = input_ids.size(0)

        # ÂÖ•ÂäõÂüã„ÇÅËæº„Åø
        input_embeddings = self.matbert.embeddings(input_ids)

        # „Çø„Çπ„ÇØÂõ∫Êúâ„Éó„É≠„É≥„Éó„Éà„ÇíËøΩÂä†
        prompt = self.task_prompts[task_id].unsqueeze(0).expand(batch_size, -1, -1)
        embeddings = torch.cat([prompt, input_embeddings], dim=1)

        # Transformer„Å´ÈÄö„Åô
        outputs = self.matbert.encoder(embeddings)

        return outputs

# Ë®ìÁ∑¥
model = LearnablePromptModel(pretrained_matbert, prompt_length=10, num_tasks=5)

# „Éó„É≠„É≥„Éó„Éà„ÅÆ„ÅøÊúÄÈÅ©ÂåñÔºà„Éë„É©„É°„Éº„ÇøÊï∞„ÇíÂ§ßÂπÖÂâäÊ∏õÔºâ
optimizer = torch.optim.Adam([model.task_prompts], lr=1e-3)
</code></pre>
<hr />
<h2>‚úÖ Á¨¨3Á´†ÂÆå‰∫Ü„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà</h2>
<h3>Ê¶ÇÂøµÁêÜËß£Ôºà10È†ÖÁõÆÔºâ</h3>
<ul>
<li>[ ] ‰∫ãÂâçÂ≠¶Áøí„ÅÆÈáçË¶ÅÊÄß„Å®Âà©ÁÇπ„ÇíË™¨Êòé„Åß„Åç„Çã</li>
<li>[ ] Masked Language Modeling„ÅÆÂéüÁêÜ„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] Full/Feature Extraction/Partial Fine-tuning„ÅÆÈÅï„ÅÑ„ÇíË™¨Êòé„Åß„Åç„Çã</li>
<li>[ ] Few-shotÂ≠¶Áøí„ÅÆÂéüÁêÜÔºàPrototypical NetworksÔºâ„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] „Éó„É≠„É≥„Éó„Éà„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞„ÅÆÊ¶ÇÂøµ„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] „Éâ„É°„Ç§„É≥ÈÅ©Âøú„ÅÆÂøÖË¶ÅÊÄß„ÇíË™¨Êòé„Åß„Åç„Çã</li>
<li>[ ] ‰∫ãÂâçÂ≠¶Áøí„Çø„Çπ„ÇØ„Å®‰∏ãÊµÅ„Çø„Çπ„ÇØ„ÅÆÈñ¢‰øÇ„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] Transfer Learning„ÅÆÂäπÊûú„ÇíÂÆöÈáèÁöÑ„Å´Ë©ï‰æ°„Åß„Åç„Çã</li>
<li>[ ] MatBERT„ÄÅMolBERT„Å™„Å©ÊùêÊñôÁâπÂåñ„É¢„Éá„É´„ÅÆÁâπÂæ¥„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] BERT/GPT/T5„ÅÆÈÅï„ÅÑ„Å®ÈÅ©Áî®Â†¥Èù¢„ÇíË™¨Êòé„Åß„Åç„Çã</li>
</ul>
<h3>ÂÆüË£Ö„Çπ„Ç≠„É´Ôºà15È†ÖÁõÆÔºâ</h3>
<ul>
<li>[ ] <code>MatBERT</code>„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] <code>MatGPT</code>„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] <code>MatT5</code>„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] SMILES„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] SELFIES„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] Masked Atom Prediction„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] Fine-tuningÊà¶Áï•ÔºàFull/Feature/PartialÔºâ„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] Prototypical Networks„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] Â≠¶ÁøíÂèØËÉΩ„Å™„Éó„É≠„É≥„Éó„Éà„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] Domain-adversarial training„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] Early stopping„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] Layer-wise learning rate„ÇíË®≠ÂÆö„Åß„Åç„Çã</li>
<li>[ ] ‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„Çí‰øùÂ≠ò„ÉªË™≠„ÅøËæº„Åø„Åß„Åç„Çã</li>
<li>[ ] Hugging Face Transformers„É©„Ç§„Éñ„É©„É™„ÇíÊ¥ªÁî®„Åß„Åç„Çã</li>
<li>[ ] „Ç´„Çπ„Çø„É†„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„ÇíTransformers„Å´Áµ±Âêà„Åß„Åç„Çã</li>
</ul>
<h3>„Éá„Éê„ÉÉ„Ç∞„Çπ„Ç≠„É´Ôºà5È†ÖÁõÆÔºâ</h3>
<ul>
<li>[ ] ÈÅéÂ≠¶Áøí„ÇíÊ§úÂá∫„Åó„ÄÅÊ≠£ÂâáÂåñ„ÅßÂØæÂá¶„Åß„Åç„Çã</li>
<li>[ ] „Éâ„É°„Ç§„É≥„Ç∑„Éï„Éà„ÇíÊ§úÂá∫„Åó„ÄÅÈÅ©ÂøúÊâãÊ≥ï„ÇíÈÅ©Áî®„Åß„Åç„Çã</li>
<li>[ ] „Éû„Çπ„ÇØÊà¶Áï•„ÅÆÂ¶•ÂΩìÊÄß„ÇíË©ï‰æ°„Åß„Åç„Çã</li>
<li>[ ] Few-shot„ÅÆ„Çµ„Éù„Éº„Éà„Çª„ÉÉ„ÉàÂìÅË≥™„ÇíË©ï‰æ°„Åß„Åç„Çã</li>
<li>[ ] „Éó„É≠„É≥„Éó„Éà„ÅÆÂäπÊûú„ÇíÂèØË¶ñÂåñ„ÉªÂàÜÊûê„Åß„Åç„Çã</li>
</ul>
<h3>ÂøúÁî®ÂäõÔºà5È†ÖÁõÆÔºâ</h3>
<ul>
<li>[ ] Êñ∞„Åó„ÅÑÊùêÊñôÁâπÊÄß‰∫àÊ∏¨„Çø„Çπ„ÇØ„Å´‰∫ãÂâçÂ≠¶Áøí„É¢„Éá„É´„ÇíÈÅ©Áî®„Åß„Åç„Çã</li>
<li>[ ] Ë§áÊï∞„ÅÆ‰∫ãÂâçÂ≠¶Áøí„Çø„Çπ„ÇØ„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„Å¶ÊÄßËÉΩÂêë‰∏ä„Åß„Åç„Çã</li>
<li>[ ] „Éâ„É°„Ç§„É≥ÈÅ©ÂøúÊà¶Áï•„ÇíË®≠Ë®à„Åß„Åç„Çã</li>
<li>[ ] Few-shotÂ≠¶Áøí„Çí„Éá„Éº„ÇøÊã°Âºµ„Å®ÁµÑ„ÅøÂêà„Çè„Åõ„Çâ„Çå„Çã</li>
<li>[ ] „Éó„É≠„É≥„Éó„Éà„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞„ÅßÊÄßËÉΩ„ÇíÊúÄÈÅ©Âåñ„Åß„Åç„Çã</li>
</ul>
<h3>„Éá„Éº„ÇøÂá¶ÁêÜÔºà5È†ÖÁõÆÔºâ</h3>
<ul>
<li>[ ] SMILES„Éá„Éº„Çø„ÇíÂâçÂá¶ÁêÜ„Åß„Åç„Çã</li>
<li>[ ] SELFIES„Å´Â§âÊèõ„Åß„Åç„Çã</li>
<li>[ ] „Éá„Éº„ÇøÊã°ÂºµÔºàSMILES enumerationÔºâ„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] „Éâ„É°„Ç§„É≥Âà•„Å´„Éá„Éº„Çø„ÇíÂàÜÂâ≤„Åß„Åç„Çã</li>
<li>[ ] Few-shotÁî®„ÅÆ„Ç®„Éî„ÇΩ„Éº„Éâ„ÇíÁîüÊàê„Åß„Åç„Çã</li>
</ul>
<h3>Ë©ï‰æ°„Çπ„Ç≠„É´Ôºà5È†ÖÁõÆÔºâ</h3>
<ul>
<li>[ ] ‰∫ãÂâçÂ≠¶Áøí„ÅÆÂäπÊûú„ÇíÂÆöÈáèË©ï‰æ°„Åß„Åç„ÇãÔºàvs from scratchÔºâ</li>
<li>[ ] Fine-tuningÊà¶Áï•„ÇíÊØîËºÉË©ï‰æ°„Åß„Åç„Çã</li>
<li>[ ] Few-shotÊÄßËÉΩ„ÇíÈÅ©Âàá„Å´Ë©ï‰æ°„Åß„Åç„ÇãÔºàN-way K-shotÔºâ</li>
<li>[ ] „Éâ„É°„Ç§„É≥ÈÅ©Âøú„ÅÆÂäπÊûú„ÇíÊ∏¨ÂÆö„Åß„Åç„Çã</li>
<li>[ ] „Éó„É≠„É≥„Éó„Éà„ÅÆÂΩ±Èüø„ÇíÂàÜÊûê„Åß„Åç„Çã</li>
</ul>
<h3>ÁêÜË´ñÁöÑËÉåÊôØÔºà5È†ÖÁõÆÔºâ</h3>
<ul>
<li>[ ] MatBERT/MolBERTË´ñÊñá„ÇíË™≠„Çì„Å†</li>
<li>[ ] BERTË´ñÊñáÔºàDevlin et al., 2019Ôºâ„ÇíË™≠„Çì„Å†</li>
<li>[ ] GPTË´ñÊñá„ÇíË™≠„Çì„Å†</li>
<li>[ ] Few-shotÂ≠¶Áøí„ÅÆË´ñÊñá„Çí1Êú¨‰ª•‰∏äË™≠„Çì„Å†</li>
<li>[ ] Transfer LearningÁêÜË´ñ„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
</ul>
<h3>ÂÆå‰∫ÜÂü∫Ê∫ñ</h3>
<ul>
<li><strong>ÊúÄ‰ΩéÂü∫Ê∫ñ</strong>: 40È†ÖÁõÆ‰ª•‰∏äÈÅîÊàêÔºà80%Ôºâ</li>
<li><strong>Êé®Â•®Âü∫Ê∫ñ</strong>: 45È†ÖÁõÆ‰ª•‰∏äÈÅîÊàêÔºà90%Ôºâ</li>
<li><strong>ÂÑ™ÁßÄÂü∫Ê∫ñ</strong>: 50È†ÖÁõÆÂÖ®„Å¶ÈÅîÊàêÔºà100%Ôºâ</li>
</ul>
<hr />
<p><strong>Ê¨°Á´†</strong>: <strong><a href="chapter-4.html">Á¨¨4Á´†: ÁîüÊàê„É¢„Éá„É´„Å®ÈÄÜË®≠Ë®à</a></strong></p>
<hr />
<p><strong>‰ΩúÊàêËÄÖ</strong>: Ê©ãÊú¨‰Ωë‰ªãÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ
<strong>ÊúÄÁµÇÊõ¥Êñ∞</strong>: 2025Âπ¥10Êúà19Êó•</p><div class="navigation">
    <a href="chapter-2.html" class="nav-button">‚Üê Ââç„ÅÆÁ´†</a>
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
    <a href="chapter-4.html" class="nav-button">Ê¨°„ÅÆÁ´† ‚Üí</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>Áõ£‰øÆ</strong>: Dr. Yusuke HashimotoÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-17</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
