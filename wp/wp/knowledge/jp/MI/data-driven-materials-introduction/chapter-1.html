<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Chapter</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 20-25åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 0å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 0å•</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 1: ãƒ‡ãƒ¼ã‚¿åé›†æˆ¦ç•¥ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°</h1>
<hr />
<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<p>âœ… ææ–™ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´ï¼ˆå°è¦æ¨¡ãƒ»ä¸å‡è¡¡ãƒ»ãƒã‚¤ã‚ºï¼‰ã¨èª²é¡Œã®ç†è§£
âœ… å®Ÿé¨“è¨ˆç”»æ³•ï¼ˆDOEï¼‰ã¨Latin Hypercube Samplingã®å®Ÿè·µ
âœ… æ¬ æå€¤å‡¦ç†ã®é©åˆ‡ãªæ‰‹æ³•é¸æŠï¼ˆSimple/KNN/MICEï¼‰
âœ… å¤–ã‚Œå€¤æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆIsolation Forestã€LOFã€DBSCANï¼‰ã®æ´»ç”¨
âœ… ç†±é›»ææ–™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ãŸå®Ÿè·µçš„ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°</p>
<hr />
<h2>1.1 ææ–™ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´</h2>
<p>ææ–™ç§‘å­¦ã«ãŠã‘ã‚‹ãƒ‡ãƒ¼ã‚¿ã«ã¯ã€ä¸€èˆ¬çš„ãªãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿ã¨ã¯ç•°ãªã‚‹ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ã€‚</p>
<h3>å°è¦æ¨¡ãƒ»ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã®å•é¡Œ</h3>
<p><strong>ç‰¹å¾´</strong>ï¼š
- <strong>ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„</strong>ï¼šå®Ÿé¨“ã«ã¯æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿æ•°ã¯æ•°åã€œæ•°åƒä»¶ç¨‹åº¦
- <strong>ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡</strong>ï¼šç‰¹å®šã®çµ„æˆã‚„æ¡ä»¶ã«åã£ãŸãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ
- <strong>æ¬¡å…ƒã®å‘ªã„</strong>ï¼šèª¬æ˜å¤‰æ•°ï¼ˆè¨˜è¿°å­ï¼‰ã®æ•°ã«å¯¾ã—ã¦ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„</p>
<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ææ–™ãƒ‡ãƒ¼ã‚¿ã®å…¸å‹çš„ã‚µã‚¤ã‚º
datasets_info = {
    'ææ–™ã‚¿ã‚¤ãƒ—': ['ç†±é›»ææ–™', 'ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—', 'è¶…ä¼å°ä½“',
                   'è§¦åª’', 'é›»æ± ææ–™'],
    'ã‚µãƒ³ãƒ—ãƒ«æ•°': [312, 1563, 89, 487, 253],
    'ç‰¹å¾´é‡æ•°': [45, 128, 67, 93, 112]
}

df_info = pd.DataFrame(datasets_info)
df_info['ã‚µãƒ³ãƒ—ãƒ«/ç‰¹å¾´é‡æ¯”'] = (
    df_info['ã‚µãƒ³ãƒ—ãƒ«æ•°'] / df_info['ç‰¹å¾´é‡æ•°']
)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# ã‚µãƒ³ãƒ—ãƒ«æ•° vs ç‰¹å¾´é‡æ•°
axes[0].scatter(df_info['ç‰¹å¾´é‡æ•°'], df_info['ã‚µãƒ³ãƒ—ãƒ«æ•°'],
                s=100, alpha=0.6, c='steelblue')
for idx, row in df_info.iterrows():
    axes[0].annotate(row['ææ–™ã‚¿ã‚¤ãƒ—'],
                     (row['ç‰¹å¾´é‡æ•°'], row['ã‚µãƒ³ãƒ—ãƒ«æ•°']),
                     fontsize=9, ha='right')
axes[0].plot([0, 150], [0, 150], 'r--',
             label='ã‚µãƒ³ãƒ—ãƒ«æ•°=ç‰¹å¾´é‡æ•°', alpha=0.5)
axes[0].set_xlabel('ç‰¹å¾´é‡æ•°', fontsize=12)
axes[0].set_ylabel('ã‚µãƒ³ãƒ—ãƒ«æ•°', fontsize=12)
axes[0].set_title('ææ–™ãƒ‡ãƒ¼ã‚¿ã®è¦æ¨¡', fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# ã‚µãƒ³ãƒ—ãƒ«/ç‰¹å¾´é‡æ¯”
axes[1].barh(df_info['ææ–™ã‚¿ã‚¤ãƒ—'],
             df_info['ã‚µãƒ³ãƒ—ãƒ«/ç‰¹å¾´é‡æ¯”'],
             color='coral', alpha=0.7)
axes[1].axvline(x=10, color='red', linestyle='--',
                label='æ¨å¥¨æœ€å°æ¯” (10:1)', linewidth=2)
axes[1].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«æ•° / ç‰¹å¾´é‡æ•°', fontsize=12)
axes[1].set_title('ãƒ‡ãƒ¼ã‚¿å……è¶³åº¦', fontsize=13, fontweight='bold')
axes[1].legend()
axes[1].grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;ææ–™ãƒ‡ãƒ¼ã‚¿ã®å…¸å‹çš„ç‰¹å¾´ï¼š&quot;)
print(f&quot;å¹³å‡ã‚µãƒ³ãƒ—ãƒ«æ•°: {df_info['ã‚µãƒ³ãƒ—ãƒ«æ•°'].mean():.0f}&quot;)
print(f&quot;å¹³å‡ç‰¹å¾´é‡æ•°: {df_info['ç‰¹å¾´é‡æ•°'].mean():.0f}&quot;)
print(f&quot;å¹³å‡ã‚µãƒ³ãƒ—ãƒ«/ç‰¹å¾´é‡æ¯”: {df_info['ã‚µãƒ³ãƒ—ãƒ«/ç‰¹å¾´é‡æ¯”'].mean():.2f}&quot;)
print(&quot;\nâš ï¸ å¤šãã®ææ–™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æ¨å¥¨æ¯” 10:1 ã‚’ä¸‹å›ã‚‹&quot;)
</code></pre>
<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>ææ–™ãƒ‡ãƒ¼ã‚¿ã®å…¸å‹çš„ç‰¹å¾´ï¼š
å¹³å‡ã‚µãƒ³ãƒ—ãƒ«æ•°: 541
å¹³å‡ç‰¹å¾´é‡æ•°: 89
å¹³å‡ã‚µãƒ³ãƒ—ãƒ«/ç‰¹å¾´é‡æ¯”: 7.36

âš ï¸ å¤šãã®ææ–™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æ¨å¥¨æ¯” 10:1 ã‚’ä¸‹å›ã‚‹
</code></pre>
<h3>ãƒã‚¤ã‚ºã¨å¤–ã‚Œå€¤</h3>
<p>ææ–™å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã«ã¯æ§˜ã€…ãªãƒã‚¤ã‚ºæºãŒã‚ã‚Šã¾ã™ï¼š</p>
<pre><code class="language-python"># ãƒã‚¤ã‚ºã®ç¨®é¡ã¨å½±éŸ¿ã‚’å¯è¦–åŒ–
np.random.seed(42)

# çœŸã®é–¢ä¿‚ï¼ˆãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ— vs æ ¼å­å®šæ•°ï¼‰
n_samples = 100
lattice_constant = np.linspace(3.5, 6.5, n_samples)
bandgap_true = 2.5 * np.exp(-0.3 * (lattice_constant - 4))

# å„ç¨®ãƒã‚¤ã‚ºã‚’è¿½åŠ 
measurement_noise = np.random.normal(0, 0.1, n_samples)
systematic_bias = 0.2  # æ¸¬å®šè£…ç½®ã®ç³»çµ±èª¤å·®
outliers_idx = np.random.choice(n_samples, 5, replace=False)

bandgap_measured = bandgap_true + measurement_noise + systematic_bias
bandgap_measured[outliers_idx] += np.random.uniform(0.5, 1.5, 5)

# å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(10, 6))

ax.plot(lattice_constant, bandgap_true, 'b-',
        linewidth=2, label='çœŸã®é–¢ä¿‚', alpha=0.7)
ax.scatter(lattice_constant, bandgap_measured,
           c='gray', s=50, alpha=0.5, label='æ¸¬å®šå€¤ï¼ˆãƒã‚¤ã‚ºå«ï¼‰')
ax.scatter(lattice_constant[outliers_idx],
           bandgap_measured[outliers_idx],
           c='red', s=100, marker='X',
           label='å¤–ã‚Œå€¤', zorder=10)

ax.set_xlabel('æ ¼å­å®šæ•° (Ã…)', fontsize=12)
ax.set_ylabel('ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ— (eV)', fontsize=12)
ax.set_title('ææ–™ãƒ‡ãƒ¼ã‚¿ã«ãŠã‘ã‚‹ãƒã‚¤ã‚ºã¨å¤–ã‚Œå€¤',
             fontsize=13, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

plt.tight_layout()
plt.show()

# ãƒã‚¤ã‚ºçµ±è¨ˆ
print(&quot;ãƒã‚¤ã‚ºåˆ†æï¼š&quot;)
print(f&quot;æ¸¬å®šãƒã‚¤ã‚ºæ¨™æº–åå·®: {measurement_noise.std():.3f} eV&quot;)
print(f&quot;ç³»çµ±èª¤å·®: {systematic_bias:.3f} eV&quot;)
print(f&quot;å¤–ã‚Œå€¤æ•°: {len(outliers_idx)} / {n_samples}&quot;)
print(f&quot;å¤–ã‚Œå€¤ã®å¹³å‡åå·®: &quot;
      f&quot;{(bandgap_measured[outliers_idx] - bandgap_true[outliers_idx]).mean():.3f} eV&quot;)
</code></pre>
<h3>ãƒ‡ãƒ¼ã‚¿ã®ä¿¡é ¼æ€§è©•ä¾¡</h3>
<p>ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’å®šé‡è©•ä¾¡ã™ã‚‹æŒ‡æ¨™ï¼š</p>
<pre><code class="language-python">def assess_data_quality(data, true_values=None):
    &quot;&quot;&quot;
    ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡

    Parameters:
    -----------
    data : array-like
        æ¸¬å®šãƒ‡ãƒ¼ã‚¿
    true_values : array-like, optional
        çœŸå€¤ï¼ˆæ—¢çŸ¥ã®å ´åˆï¼‰

    Returns:
    --------
    dict : å“è³ªæŒ‡æ¨™
    &quot;&quot;&quot;
    quality_metrics = {}

    # åŸºæœ¬çµ±è¨ˆ
    quality_metrics['mean'] = np.mean(data)
    quality_metrics['std'] = np.std(data)
    quality_metrics['cv'] = np.std(data) / np.mean(data)  # å¤‰å‹•ä¿‚æ•°

    # å¤–ã‚Œå€¤å‰²åˆï¼ˆIQRæ³•ï¼‰
    Q1, Q3 = np.percentile(data, [25, 75])
    IQR = Q3 - Q1
    outliers = (data &lt; Q1 - 1.5*IQR) | (data &gt; Q3 + 1.5*IQR)
    quality_metrics['outlier_ratio'] = outliers.sum() / len(data)

    # çœŸå€¤ã¨ã®æ¯”è¼ƒï¼ˆæ—¢çŸ¥ã®å ´åˆï¼‰
    if true_values is not None:
        quality_metrics['mae'] = np.mean(np.abs(data - true_values))
        quality_metrics['rmse'] = np.sqrt(
            np.mean((data - true_values)**2)
        )
        quality_metrics['r2'] = 1 - (
            np.sum((data - true_values)**2) /
            np.sum((true_values - np.mean(true_values))**2)
        )

    return quality_metrics

# è©•ä¾¡å®Ÿè¡Œ
quality = assess_data_quality(bandgap_measured, bandgap_true)

print(&quot;ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡ï¼š&quot;)
print(f&quot;å¹³å‡å€¤: {quality['mean']:.3f} eV&quot;)
print(f&quot;æ¨™æº–åå·®: {quality['std']:.3f} eV&quot;)
print(f&quot;å¤‰å‹•ä¿‚æ•°: {quality['cv']:.3f}&quot;)
print(f&quot;å¤–ã‚Œå€¤å‰²åˆ: {quality['outlier_ratio']:.1%}&quot;)
print(f&quot;\nçœŸå€¤ã¨ã®æ¯”è¼ƒï¼š&quot;)
print(f&quot;MAE: {quality['mae']:.3f} eV&quot;)
print(f&quot;RMSE: {quality['rmse']:.3f} eV&quot;)
print(f&quot;RÂ²: {quality['r2']:.3f}&quot;)
</code></pre>
<h3>ãƒ‡ãƒ¼ã‚¿ã®ç¨®é¡ï¼šå®Ÿé¨“ã€è¨ˆç®—ã€æ–‡çŒ®</h3>
<pre><code class="language-python"># ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ç‰¹å¾´
data_sources = pd.DataFrame({
    'ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹': ['å®Ÿé¨“', 'DFTè¨ˆç®—', 'æ–‡çŒ®', 'çµ±åˆ'],
    'ã‚µãƒ³ãƒ—ãƒ«æ•°': [150, 500, 300, 950],
    'ç²¾åº¦': [0.85, 0.95, 0.75, 0.80],
    'ã‚³ã‚¹ãƒˆï¼ˆç›¸å¯¾ï¼‰': [10, 3, 1, 4],
    'å–å¾—æ™‚é–“ï¼ˆæ—¥ï¼‰': [30, 7, 3, 15]
})

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# ã‚µãƒ³ãƒ—ãƒ«æ•°
axes[0,0].bar(data_sources['ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹'],
              data_sources['ã‚µãƒ³ãƒ—ãƒ«æ•°'],
              color=['#FF6B6B', '#4ECDC4', '#FFE66D', '#95E1D3'])
axes[0,0].set_ylabel('ã‚µãƒ³ãƒ—ãƒ«æ•°', fontsize=11)
axes[0,0].set_title('ãƒ‡ãƒ¼ã‚¿é‡', fontsize=12, fontweight='bold')
axes[0,0].grid(axis='y', alpha=0.3)

# ç²¾åº¦
axes[0,1].bar(data_sources['ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹'],
              data_sources['ç²¾åº¦'],
              color=['#FF6B6B', '#4ECDC4', '#FFE66D', '#95E1D3'])
axes[0,1].set_ylabel('ç²¾åº¦', fontsize=11)
axes[0,1].set_ylim(0, 1)
axes[0,1].set_title('ãƒ‡ãƒ¼ã‚¿ç²¾åº¦', fontsize=12, fontweight='bold')
axes[0,1].grid(axis='y', alpha=0.3)

# ã‚³ã‚¹ãƒˆ
axes[1,0].bar(data_sources['ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹'],
              data_sources['ã‚³ã‚¹ãƒˆï¼ˆç›¸å¯¾ï¼‰'],
              color=['#FF6B6B', '#4ECDC4', '#FFE66D', '#95E1D3'])
axes[1,0].set_ylabel('ç›¸å¯¾ã‚³ã‚¹ãƒˆ', fontsize=11)
axes[1,0].set_title('å–å¾—ã‚³ã‚¹ãƒˆ', fontsize=12, fontweight='bold')
axes[1,0].grid(axis='y', alpha=0.3)

# å–å¾—æ™‚é–“
axes[1,1].bar(data_sources['ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹'],
              data_sources['å–å¾—æ™‚é–“ï¼ˆæ—¥ï¼‰'],
              color=['#FF6B6B', '#4ECDC4', '#FFE66D', '#95E1D3'])
axes[1,1].set_ylabel('æ—¥æ•°', fontsize=11)
axes[1,1].set_title('å–å¾—æ™‚é–“', fontsize=12, fontweight='bold')
axes[1,1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;\nå„ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ç‰¹å¾´ï¼š&quot;)
print(data_sources.to_string(index=False))
</code></pre>
<hr />
<h2>1.2 ãƒ‡ãƒ¼ã‚¿åé›†æˆ¦ç•¥</h2>
<p>åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿åé›†ã®ãŸã‚ã®æˆ¦ç•¥çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å­¦ã³ã¾ã™ã€‚</p>
<h3>å®Ÿé¨“è¨ˆç”»æ³•ï¼ˆDOE: Design of Experimentsï¼‰</h3>
<p><strong>ç›®çš„</strong>ï¼šé™ã‚‰ã‚ŒãŸå®Ÿé¨“å›æ•°ã§æœ€å¤§ã®æƒ…å ±ã‚’å¾—ã‚‹</p>
<pre><code class="language-python">from scipy.stats import qmc

def full_factorial_design(factors, levels):
    &quot;&quot;&quot;
    å®Œå…¨è¦å› é…ç½®æ³•

    Parameters:
    -----------
    factors : list of str
        å› å­åãƒªã‚¹ãƒˆ
    levels : list of list
        å„å› å­ã®æ°´æº–ãƒªã‚¹ãƒˆ

    Returns:
    --------
    pd.DataFrame : å®Ÿé¨“è¨ˆç”»è¡¨
    &quot;&quot;&quot;
    import itertools

    # å…¨çµ„ã¿åˆã‚ã›ç”Ÿæˆ
    combinations = list(itertools.product(*levels))

    df = pd.DataFrame(combinations, columns=factors)
    return df

# ä¾‹ï¼šç†±é›»ææ–™ã®åˆæˆæ¡ä»¶æœ€é©åŒ–
factors = ['æ¸©åº¦(â„ƒ)', 'åœ§åŠ›(GPa)', 'æ™‚é–“(h)']
levels = [
    [600, 800, 1000],  # æ¸©åº¦
    [1, 3, 5],         # åœ§åŠ›
    [2, 6, 12]         # æ™‚é–“
]

design_full = full_factorial_design(factors, levels)
print(f&quot;å®Œå…¨è¦å› é…ç½®: {len(design_full)} å®Ÿé¨“&quot;)
print(&quot;\næœ€åˆã®10å®Ÿé¨“ï¼š&quot;)
print(design_full.head(10))

# éƒ¨åˆ†è¦å› é…ç½®ï¼ˆFractional Factorialï¼‰
def fractional_factorial_design(factors, levels, fraction=0.5):
    &quot;&quot;&quot;
    éƒ¨åˆ†è¦å› é…ç½®æ³•ï¼ˆå®Ÿé¨“æ•°å‰Šæ¸›ï¼‰
    &quot;&quot;&quot;
    full_design = full_factorial_design(factors, levels)
    n_experiments = int(len(full_design) * fraction)

    # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå®Ÿéš›ã«ã¯ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸé¸æŠæ³•ã‚’ä½¿ç”¨ï¼‰
    sampled_idx = np.random.choice(
        len(full_design), n_experiments, replace=False
    )
    return full_design.iloc[sampled_idx].reset_index(drop=True)

design_frac = fractional_factorial_design(factors, levels, fraction=0.33)
print(f&quot;\néƒ¨åˆ†è¦å› é…ç½®: {len(design_frac)} å®Ÿé¨“ &quot;
      f&quot;(å‰Šæ¸›ç‡: {(1-len(design_frac)/len(design_full)):.1%})&quot;)
print(design_frac.head(10))
</code></pre>
<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>å®Œå…¨è¦å› é…ç½®: 27 å®Ÿé¨“

æœ€åˆã®10å®Ÿé¨“ï¼š
   æ¸©åº¦(â„ƒ)  åœ§åŠ›(GPa)  æ™‚é–“(h)
0      600        1      2
1      600        1      6
2      600        1     12
3      600        3      2
...

éƒ¨åˆ†è¦å› é…ç½®: 9 å®Ÿé¨“ (å‰Šæ¸›ç‡: 66.7%)
</code></pre>
<h3>Latin Hypercube Sampling</h3>
<p><strong>åˆ©ç‚¹</strong>ï¼šå…¨æ¢ç´¢ç©ºé–“ã‚’åŠ¹ç‡çš„ã«ã‚«ãƒãƒ¼</p>
<pre><code class="language-python">def latin_hypercube_sampling(n_samples, bounds, seed=42):
    &quot;&quot;&quot;
    Latin Hypercube Sampling

    Parameters:
    -----------
    n_samples : int
        ã‚µãƒ³ãƒ—ãƒ«æ•°
    bounds : list of tuple
        å„å¤‰æ•°ã®ç¯„å›² [(min1, max1), (min2, max2), ...]
    seed : int
        ä¹±æ•°ã‚·ãƒ¼ãƒ‰

    Returns:
    --------
    np.ndarray : ã‚µãƒ³ãƒ—ãƒ«ç‚¹ (n_samples, n_dimensions)
    &quot;&quot;&quot;
    n_dim = len(bounds)
    sampler = qmc.LatinHypercube(d=n_dim, seed=seed)
    sample_unit = sampler.random(n=n_samples)

    # [0,1]åŒºé–“ã‹ã‚‰å®Ÿéš›ã®ç¯„å›²ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    sample = np.zeros_like(sample_unit)
    for i, (lower, upper) in enumerate(bounds):
        sample[:, i] = lower + sample_unit[:, i] * (upper - lower)

    return sample

# ç†±é›»ææ–™ã®çµ„æˆç©ºé–“ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
bounds = [
    (0, 1),    # å…ƒç´ Aã®å‰²åˆ
    (0, 1),    # å…ƒç´ Bã®å‰²åˆ
    (0, 1)     # ãƒ‰ãƒ¼ãƒ‘ãƒ³ãƒˆæ¿ƒåº¦
]

# LHS vs ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”è¼ƒ
n_samples = 50
lhs_samples = latin_hypercube_sampling(n_samples, bounds)

np.random.seed(42)
random_samples = np.random.uniform(0, 1, (n_samples, 3))

# å¯è¦–åŒ–ï¼ˆ2æ¬¡å…ƒæŠ•å½±ï¼‰
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# LHS
axes[0].scatter(lhs_samples[:, 0], lhs_samples[:, 1],
                c='steelblue', s=80, alpha=0.6, edgecolors='k')
axes[0].set_xlabel('å…ƒç´ Aå‰²åˆ', fontsize=12)
axes[0].set_ylabel('å…ƒç´ Bå‰²åˆ', fontsize=12)
axes[0].set_title('Latin Hypercube Sampling',
                  fontsize=13, fontweight='bold')
axes[0].grid(alpha=0.3)
axes[0].set_xlim(0, 1)
axes[0].set_ylim(0, 1)

# Random
axes[1].scatter(random_samples[:, 0], random_samples[:, 1],
                c='coral', s=80, alpha=0.6, edgecolors='k')
axes[1].set_xlabel('å…ƒç´ Aå‰²åˆ', fontsize=12)
axes[1].set_ylabel('å…ƒç´ Bå‰²åˆ', fontsize=12)
axes[1].set_title('ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°',
                  fontsize=13, fontweight='bold')
axes[1].grid(alpha=0.3)
axes[1].set_xlim(0, 1)
axes[1].set_ylim(0, 1)

plt.tight_layout()
plt.show()

print(&quot;LHS: æ¢ç´¢ç©ºé–“ã‚’å‡ä¸€ã«ã‚«ãƒãƒ¼&quot;)
print(&quot;Random: åã‚ŠãŒç”Ÿã˜ã‚„ã™ã„&quot;)
</code></pre>
<h3>Active Learningçµ±åˆ</h3>
<p><strong>æˆ¦ç•¥</strong>ï¼šä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„é ˜åŸŸã‚’å„ªå…ˆçš„ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</p>
<pre><code class="language-python">from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

def uncertainty_sampling(model, X_pool, n_samples=5):
    &quot;&quot;&quot;
    ä¸ç¢ºå®Ÿæ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆActive Learningï¼‰

    Parameters:
    -----------
    model : sklearn model
        äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ï¼ˆpredict ã‚’æŒã¤ï¼‰
    X_pool : array-like
        å€™è£œã‚µãƒ³ãƒ—ãƒ«é›†åˆ
    n_samples : int
        é¸æŠã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°

    Returns:
    --------
    indices : array
        é¸æŠã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    &quot;&quot;&quot;
    if hasattr(model, 'estimators_'):
        # Random Forestã®å ´åˆã€å„æœ¨ã®äºˆæ¸¬ã®ã°ã‚‰ã¤ãã‚’ä¸ç¢ºå®Ÿæ€§ã¨ã™ã‚‹
        predictions = np.array([
            tree.predict(X_pool)
            for tree in model.estimators_
        ])
        uncertainty = np.std(predictions, axis=0)
    else:
        # å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯ãƒ€ãƒŸãƒ¼ä¸ç¢ºå®Ÿæ€§
        uncertainty = np.random.random(len(X_pool))

    # ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„é †ã«ã‚µãƒ³ãƒ—ãƒ«é¸æŠ
    indices = np.argsort(uncertainty)[-n_samples:]
    return indices

# ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼šActive Learning vs Random Sampling
np.random.seed(42)

# çœŸã®é–¢æ•°ï¼ˆæœªçŸ¥ã¨ä»®å®šï¼‰
def true_function(X):
    &quot;&quot;&quot;ç†±é›»ç‰¹æ€§ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³&quot;&quot;&quot;
    return (
        2.5 * X[:, 0]**2 -
        1.5 * X[:, 1] +
        0.5 * X[:, 0] * X[:, 1] +
        np.random.normal(0, 0.1, len(X))
    )

# åˆæœŸãƒ‡ãƒ¼ã‚¿
X_init = latin_hypercube_sampling(20, [(0, 1), (0, 1)])
y_init = true_function(X_init)

# å€™è£œãƒ—ãƒ¼ãƒ«
X_pool = latin_hypercube_sampling(100, [(0, 1), (0, 1)])
y_pool = true_function(X_pool)

# Active Learning
X_train_al, y_train_al = X_init.copy(), y_init.copy()
model_al = RandomForestRegressor(n_estimators=10, random_state=42)

for iteration in range(5):
    model_al.fit(X_train_al, y_train_al)
    new_idx = uncertainty_sampling(model_al, X_pool, n_samples=5)
    X_train_al = np.vstack([X_train_al, X_pool[new_idx]])
    y_train_al = np.hstack([y_train_al, y_pool[new_idx]])

# Random Sampling
X_train_rs, y_train_rs = X_init.copy(), y_init.copy()
random_idx = np.random.choice(len(X_pool), 25, replace=False)
X_train_rs = np.vstack([X_train_rs, X_pool[random_idx]])
y_train_rs = np.hstack([y_train_rs, y_pool[random_idx]])

model_rs = RandomForestRegressor(n_estimators=10, random_state=42)
model_rs.fit(X_train_rs, y_train_rs)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
X_test = latin_hypercube_sampling(50, [(0, 1), (0, 1)])
y_test = true_function(X_test)

mae_al = np.mean(np.abs(model_al.predict(X_test) - y_test))
mae_rs = np.mean(np.abs(model_rs.predict(X_test) - y_test))

print(f&quot;Active Learning MAE: {mae_al:.4f}&quot;)
print(f&quot;Random Sampling MAE: {mae_rs:.4f}&quot;)
print(f&quot;æ”¹å–„ç‡: {(mae_rs - mae_al) / mae_rs * 100:.1f}%&quot;)
print(f&quot;\nã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X_train_al)} (ä¸¡æ–¹)&quot;)
</code></pre>
<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>Active Learning MAE: 0.1523
Random Sampling MAE: 0.2187
æ”¹å–„ç‡: 30.4%

ã‚µãƒ³ãƒ—ãƒ«æ•°: 45 (ä¸¡æ–¹)
</code></pre>
<h3>ãƒ‡ãƒ¼ã‚¿ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°æˆ¦ç•¥</h3>
<pre><code class="language-python">from sklearn.utils import resample

def balance_dataset(X, y, strategy='oversample', random_state=42):
    &quot;&quot;&quot;
    ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°

    Parameters:
    -----------
    X : array-like
        ç‰¹å¾´é‡
    y : array-like
        ãƒ©ãƒ™ãƒ«ï¼ˆã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ï¼‰
    strategy : str
        'oversample' or 'undersample'

    Returns:
    --------
    X_balanced, y_balanced : ãƒãƒ©ãƒ³ã‚¹å¾Œã®ãƒ‡ãƒ¼ã‚¿
    &quot;&quot;&quot;
    df = pd.DataFrame(X)
    df['target'] = y

    # å„ã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«æ•°
    class_counts = df['target'].value_counts()

    if strategy == 'oversample':
        # å¤šæ•°æ´¾ã‚¯ãƒ©ã‚¹ã«åˆã‚ã›ã¦ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        max_count = class_counts.max()

        dfs = []
        for class_label in class_counts.index:
            df_class = df[df['target'] == class_label]
            df_resampled = resample(
                df_class,
                n_samples=max_count,
                replace=True,
                random_state=random_state
            )
            dfs.append(df_resampled)

        df_balanced = pd.concat(dfs)

    elif strategy == 'undersample':
        # å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã«åˆã‚ã›ã¦ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        min_count = class_counts.min()

        dfs = []
        for class_label in class_counts.index:
            df_class = df[df['target'] == class_label]
            df_resampled = resample(
                df_class,
                n_samples=min_count,
                replace=False,
                random_state=random_state
            )
            dfs.append(df_resampled)

        df_balanced = pd.concat(dfs)

    X_balanced = df_balanced.drop('target', axis=1).values
    y_balanced = df_balanced['target'].values

    return X_balanced, y_balanced

# ä¾‹ï¼šä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
np.random.seed(42)
X_imb = np.random.randn(200, 5)
y_imb = np.array([0]*150 + [1]*30 + [2]*20)  # ä¸å‡è¡¡

print(&quot;å…ƒã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒï¼š&quot;)
print(pd.Series(y_imb).value_counts().sort_index())

# ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
X_over, y_over = balance_dataset(X_imb, y_imb, strategy='oversample')
print(&quot;\nã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œï¼š&quot;)
print(pd.Series(y_over).value_counts().sort_index())

# ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
X_under, y_under = balance_dataset(X_imb, y_imb, strategy='undersample')
print(&quot;\nã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œï¼š&quot;)
print(pd.Series(y_under).value_counts().sort_index())
</code></pre>
<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>å…ƒã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒï¼š
0    150
1     30
2     20

ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œï¼š
0    150
1    150
2    150

ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œï¼š
0    20
1    20
2    20
</code></pre>
<hr />
<h2>1.3 æ¬ æå€¤å‡¦ç†</h2>
<p>å®Ÿéš›ã®ææ–™ãƒ‡ãƒ¼ã‚¿ã§ã¯ã€æ¸¬å®šã®å¤±æ•—ã‚„è¨˜éŒ²æ¼ã‚Œã«ã‚ˆã‚Šæ¬ æå€¤ãŒç™ºç”Ÿã—ã¾ã™ã€‚</p>
<h3>æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†é¡</h3>
<pre><code class="language-python">def analyze_missing_pattern(df):
    &quot;&quot;&quot;
    æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ

    MCAR: Missing Completely At Randomï¼ˆå®Œå…¨ã«ãƒ©ãƒ³ãƒ€ãƒ ï¼‰
    MAR: Missing At Randomï¼ˆä»–ã®å¤‰æ•°ã«ä¾å­˜ï¼‰
    MNAR: Missing Not At Randomï¼ˆè‡ªèº«ã®å€¤ã«ä¾å­˜ï¼‰
    &quot;&quot;&quot;
    # æ¬ æå€¤ãƒãƒƒãƒ—
    missing_mask = df.isnull()

    # æ¬ æç‡
    missing_rate = missing_mask.mean()

    # æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³å¯è¦–åŒ–
    plt.figure(figsize=(12, 6))
    sns.heatmap(missing_mask, cmap='YlOrRd', cbar_kws={'label': 'æ¬ æ'})
    plt.title('æ¬ æå€¤ãƒ‘ã‚¿ãƒ¼ãƒ³', fontsize=13, fontweight='bold')
    plt.xlabel('ç‰¹å¾´é‡', fontsize=11)
    plt.ylabel('ã‚µãƒ³ãƒ—ãƒ«', fontsize=11)
    plt.tight_layout()
    plt.show()

    print(&quot;æ¬ æç‡ï¼š&quot;)
    print(missing_rate.sort_values(ascending=False))

    return missing_rate

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæ„å›³çš„ã«æ¬ æã‚’å°å…¥ï¼‰
np.random.seed(42)
df_sample = pd.DataFrame({
    'æ ¼å­å®šæ•°': np.random.uniform(3, 6, 100),
    'ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—': np.random.uniform(0, 3, 100),
    'é›»æ°—ä¼å°åº¦': np.random.uniform(1e3, 1e6, 100),
    'ç†±ä¼å°åº¦': np.random.uniform(1, 100, 100)
})

# MCAR: ãƒ©ãƒ³ãƒ€ãƒ ã«10%æ¬ æ
mcar_mask = np.random.random(100) &lt; 0.1
df_sample.loc[mcar_mask, 'æ ¼å­å®šæ•°'] = np.nan

# MAR: ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ãŒå¤§ãã„ã¨ç†±ä¼å°åº¦ãŒæ¬ æã—ã‚„ã™ã„
mar_mask = df_sample['ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—'] &gt; 2.0
mar_prob = np.random.random(sum(mar_mask))
df_sample.loc[mar_mask, 'ç†±ä¼å°åº¦'] = np.where(
    mar_prob &lt; 0.5, np.nan, df_sample.loc[mar_mask, 'ç†±ä¼å°åº¦']
)

print(&quot;æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æï¼š&quot;)
missing_stats = analyze_missing_pattern(df_sample)
</code></pre>
<h3>Simple Imputationï¼ˆå¹³å‡å€¤ã€ä¸­å¤®å€¤ï¼‰</h3>
<pre><code class="language-python">from sklearn.impute import SimpleImputer

def simple_imputation_comparison(df, strategy_list=['mean', 'median']):
    &quot;&quot;&quot;
    Simple Imputationã®æ¯”è¼ƒ
    &quot;&quot;&quot;
    results = {}

    for strategy in strategy_list:
        imputer = SimpleImputer(strategy=strategy)
        df_imputed = pd.DataFrame(
            imputer.fit_transform(df),
            columns=df.columns
        )
        results[strategy] = df_imputed

    return results

# å®Ÿè¡Œ
imputed_results = simple_imputation_comparison(
    df_sample,
    strategy_list=['mean', 'median']
)

# æ¯”è¼ƒå¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

for idx, col in enumerate(df_sample.columns):
    ax = axes[idx // 2, idx % 2]

    # å…ƒãƒ‡ãƒ¼ã‚¿
    ax.hist(df_sample[col].dropna(), bins=20,
            alpha=0.5, label='å…ƒãƒ‡ãƒ¼ã‚¿', color='gray')

    # è£œå®Œãƒ‡ãƒ¼ã‚¿
    ax.hist(imputed_results['mean'][col], bins=20,
            alpha=0.5, label='å¹³å‡å€¤è£œå®Œ', color='steelblue')
    ax.hist(imputed_results['median'][col], bins=20,
            alpha=0.5, label='ä¸­å¤®å€¤è£œå®Œ', color='coral')

    ax.set_xlabel(col, fontsize=11)
    ax.set_ylabel('é »åº¦', fontsize=11)
    ax.set_title(f'{col}ã®åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    ax.legend()
    ax.grid(alpha=0.3)

plt.tight_layout()
plt.show()

# çµ±è¨ˆé‡æ¯”è¼ƒ
print(&quot;\nå…ƒãƒ‡ãƒ¼ã‚¿ vs è£œå®Œãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆé‡ï¼š&quot;)
for col in df_sample.columns:
    print(f&quot;\n{col}:&quot;)
    print(f&quot;  å…ƒãƒ‡ãƒ¼ã‚¿å¹³å‡: {df_sample[col].mean():.3f}&quot;)
    print(f&quot;  å¹³å‡å€¤è£œå®Œ: {imputed_results['mean'][col].mean():.3f}&quot;)
    print(f&quot;  ä¸­å¤®å€¤è£œå®Œ: {imputed_results['median'][col].mean():.3f}&quot;)
</code></pre>
<h3>KNN Imputation</h3>
<pre><code class="language-python">from sklearn.impute import KNNImputer

def knn_imputation(df, n_neighbors=5):
    &quot;&quot;&quot;
    Kè¿‘å‚æ³•ã«ã‚ˆã‚‹æ¬ æå€¤è£œå®Œ
    &quot;&quot;&quot;
    imputer = KNNImputer(n_neighbors=n_neighbors)
    df_imputed = pd.DataFrame(
        imputer.fit_transform(df),
        columns=df.columns
    )
    return df_imputed

# å®Ÿè¡Œ
df_knn = knn_imputation(df_sample, n_neighbors=5)

# KNN vs Simpleæ¯”è¼ƒ
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# æ ¼å­å®šæ•°ï¼ˆMCARæ¬ æï¼‰
axes[0].scatter(range(100), df_sample['æ ¼å­å®šæ•°'],
                c='gray', s=30, alpha=0.5, label='å…ƒãƒ‡ãƒ¼ã‚¿')
axes[0].scatter(range(100), imputed_results['mean']['æ ¼å­å®šæ•°'],
                c='steelblue', s=20, alpha=0.7, label='å¹³å‡å€¤è£œå®Œ',
                marker='s')
axes[0].scatter(range(100), df_knn['æ ¼å­å®šæ•°'],
                c='coral', s=20, alpha=0.7, label='KNNè£œå®Œ',
                marker='^')
axes[0].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ID', fontsize=11)
axes[0].set_ylabel('æ ¼å­å®šæ•°', fontsize=11)
axes[0].set_title('æ ¼å­å®šæ•°ã®è£œå®Œæ¯”è¼ƒ', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# ç†±ä¼å°åº¦ï¼ˆMARæ¬ æï¼‰
axes[1].scatter(range(100), df_sample['ç†±ä¼å°åº¦'],
                c='gray', s=30, alpha=0.5, label='å…ƒãƒ‡ãƒ¼ã‚¿')
axes[1].scatter(range(100), imputed_results['mean']['ç†±ä¼å°åº¦'],
                c='steelblue', s=20, alpha=0.7, label='å¹³å‡å€¤è£œå®Œ',
                marker='s')
axes[1].scatter(range(100), df_knn['ç†±ä¼å°åº¦'],
                c='coral', s=20, alpha=0.7, label='KNNè£œå®Œ',
                marker='^')
axes[1].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ID', fontsize=11)
axes[1].set_ylabel('ç†±ä¼å°åº¦', fontsize=11)
axes[1].set_title('ç†±ä¼å°åº¦ã®è£œå®Œæ¯”è¼ƒ', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;KNNã¯è¿‘å‚ã‚µãƒ³ãƒ—ãƒ«ã®æƒ…å ±ã‚’æ´»ç”¨ã™ã‚‹ãŸã‚ã€&quot;)
print(&quot;ç›¸é–¢ã®ã‚ã‚‹å¤‰æ•°é–“ã®é–¢ä¿‚ã‚’ä¿ã¡ã‚„ã™ã„&quot;)
</code></pre>
<h3>MICE (Multiple Imputation by Chained Equations)</h3>
<pre><code class="language-python">from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

def mice_imputation(df, max_iter=10, random_state=42):
    &quot;&quot;&quot;
    MICEï¼ˆå¤šé‡ä»£å…¥æ³•ï¼‰

    å„å¤‰æ•°ã‚’ä»–ã®å¤‰æ•°ã§äºˆæ¸¬ã—ã€åå¾©çš„ã«è£œå®Œ
    &quot;&quot;&quot;
    imputer = IterativeImputer(
        max_iter=max_iter,
        random_state=random_state
    )
    df_imputed = pd.DataFrame(
        imputer.fit_transform(df),
        columns=df.columns
    )
    return df_imputed

# å®Ÿè¡Œ
df_mice = mice_imputation(df_sample, max_iter=10)

# æ‰‹æ³•ã®æ¯”è¼ƒ
methods = {
    'å¹³å‡å€¤': imputed_results['mean'],
    'KNN': df_knn,
    'MICE': df_mice
}

# è£œå®Œç²¾åº¦è©•ä¾¡ï¼ˆå…ƒã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã¨ã®æ¯”è¼ƒï¼‰
np.random.seed(42)
df_complete = pd.DataFrame({
    'æ ¼å­å®šæ•°': np.random.uniform(3, 6, 100),
    'ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—': np.random.uniform(0, 3, 100),
    'é›»æ°—ä¼å°åº¦': np.random.uniform(1e3, 1e6, 100),
    'ç†±ä¼å°åº¦': np.random.uniform(1, 100, 100)
})

# æ¬ æãƒã‚¹ã‚¯
missing_indices = df_sample.isnull()

# å„æ‰‹æ³•ã®MAEè¨ˆç®—
print(&quot;è£œå®Œç²¾åº¦æ¯”è¼ƒï¼ˆMAEï¼‰ï¼š&quot;)
for method_name, df_method in methods.items():
    mae_list = []
    for col in df_sample.columns:
        if missing_indices[col].any():
            mask = missing_indices[col]
            mae = np.mean(
                np.abs(
                    df_method.loc[mask, col] -
                    df_complete.loc[mask, col]
                )
            )
            mae_list.append(mae)

    print(f&quot;{method_name}: {np.mean(mae_list):.4f}&quot;)
</code></pre>
<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>è£œå®Œç²¾åº¦æ¯”è¼ƒï¼ˆMAEï¼‰ï¼š
å¹³å‡å€¤: 0.8523
KNN: 0.5127
MICE: 0.4856
</code></pre>
<hr />
<h2>1.4 å¤–ã‚Œå€¤æ¤œå‡ºã¨å‡¦ç†</h2>
<p>å¤–ã‚Œå€¤ã¯æ¸¬å®šã‚¨ãƒ©ãƒ¼ã®å¯èƒ½æ€§ã‚‚ã‚ã‚Œã°ã€æ–°è¦ææ–™ã®ç™ºè¦‹ã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ã‚‚ã‚ã‚Šã¾ã™ã€‚</p>
<h3>çµ±è¨ˆçš„æ‰‹æ³•ï¼ˆZ-score, IQRï¼‰</h3>
<pre><code class="language-python">def detect_outliers_zscore(data, threshold=3):
    &quot;&quot;&quot;
    Z-scoreã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º
    &quot;&quot;&quot;
    z_scores = np.abs((data - np.mean(data)) / np.std(data))
    return z_scores &gt; threshold

def detect_outliers_iqr(data, multiplier=1.5):
    &quot;&quot;&quot;
    IQRï¼ˆå››åˆ†ä½ç¯„å›²ï¼‰ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º
    &quot;&quot;&quot;
    Q1 = np.percentile(data, 25)
    Q3 = np.percentile(data, 75)
    IQR = Q3 - Q1

    lower_bound = Q1 - multiplier * IQR
    upper_bound = Q3 + multiplier * IQR

    return (data &lt; lower_bound) | (data &gt; upper_bound)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
np.random.seed(42)
data_normal = np.random.normal(50, 10, 100)
data_with_outliers = np.concatenate([
    data_normal,
    [10, 15, 95, 100]  # å¤–ã‚Œå€¤
])

# æ¤œå‡º
outliers_z = detect_outliers_zscore(data_with_outliers, threshold=3)
outliers_iqr = detect_outliers_iqr(data_with_outliers, multiplier=1.5)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Z-score
axes[0].scatter(range(len(data_with_outliers)), data_with_outliers,
                c=outliers_z, cmap='RdYlGn_r', s=60, alpha=0.7,
                edgecolors='k')
axes[0].axhline(y=np.mean(data_with_outliers) + 3*np.std(data_with_outliers),
                color='r', linestyle='--', label='Â±3Ïƒ')
axes[0].axhline(y=np.mean(data_with_outliers) - 3*np.std(data_with_outliers),
                color='r', linestyle='--')
axes[0].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ID', fontsize=11)
axes[0].set_ylabel('å€¤', fontsize=11)
axes[0].set_title('Z-scoreæ³•', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# IQR
axes[1].scatter(range(len(data_with_outliers)), data_with_outliers,
                c=outliers_iqr, cmap='RdYlGn_r', s=60, alpha=0.7,
                edgecolors='k')
Q1 = np.percentile(data_with_outliers, 25)
Q3 = np.percentile(data_with_outliers, 75)
IQR = Q3 - Q1
axes[1].axhline(y=Q3 + 1.5*IQR, color='r', linestyle='--', label='Q3+1.5Ã—IQR')
axes[1].axhline(y=Q1 - 1.5*IQR, color='r', linestyle='--', label='Q1-1.5Ã—IQR')
axes[1].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ID', fontsize=11)
axes[1].set_ylabel('å€¤', fontsize=11)
axes[1].set_title('IQRæ³•', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(f&quot;Z-scoreæ³•: {outliers_z.sum()} å€‹ã®å¤–ã‚Œå€¤æ¤œå‡º&quot;)
print(f&quot;IQRæ³•: {outliers_iqr.sum()} å€‹ã®å¤–ã‚Œå€¤æ¤œå‡º&quot;)
</code></pre>
<h3>Isolation Forest</h3>
<pre><code class="language-python">from sklearn.ensemble import IsolationForest

def detect_outliers_iforest(X, contamination=0.1, random_state=42):
    &quot;&quot;&quot;
    Isolation Forestã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º

    é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã«æœ‰åŠ¹
    &quot;&quot;&quot;
    clf = IsolationForest(
        contamination=contamination,
        random_state=random_state
    )
    predictions = clf.fit_predict(X)

    # -1: å¤–ã‚Œå€¤, 1: æ­£å¸¸å€¤
    return predictions == -1

# 2æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã§å¯è¦–åŒ–
np.random.seed(42)
X_normal = np.random.randn(200, 2) * [2, 3] + [50, 60]
X_outliers = np.random.uniform(40, 70, (20, 2))
X = np.vstack([X_normal, X_outliers])

outliers_if = detect_outliers_iforest(X, contamination=0.1)

# å¯è¦–åŒ–
plt.figure(figsize=(10, 8))
plt.scatter(X[~outliers_if, 0], X[~outliers_if, 1],
            c='steelblue', s=50, alpha=0.6, label='æ­£å¸¸å€¤')
plt.scatter(X[outliers_if, 0], X[outliers_if, 1],
            c='red', s=100, alpha=0.8, marker='X', label='å¤–ã‚Œå€¤')
plt.xlabel('ç‰¹å¾´é‡1', fontsize=12)
plt.ylabel('ç‰¹å¾´é‡2', fontsize=12)
plt.title('Isolation Forest ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º',
          fontsize=13, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(f&quot;æ¤œå‡ºã•ã‚ŒãŸå¤–ã‚Œå€¤: {outliers_if.sum()} / {len(X)}&quot;)
</code></pre>
<h3>Local Outlier Factor (LOF)</h3>
<pre><code class="language-python">from sklearn.neighbors import LocalOutlierFactor

def detect_outliers_lof(X, n_neighbors=20, contamination=0.1):
    &quot;&quot;&quot;
    Local Outlier Factorã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º

    å±€æ‰€çš„ãªå¯†åº¦ã«åŸºã¥ãæ¤œå‡º
    &quot;&quot;&quot;
    clf = LocalOutlierFactor(
        n_neighbors=n_neighbors,
        contamination=contamination
    )
    predictions = clf.fit_predict(X)

    return predictions == -1

# LOF vs Isolation Forestæ¯”è¼ƒ
outliers_lof = detect_outliers_lof(X, n_neighbors=20, contamination=0.1)

fig, axes = plt.subplots(1, 2, figsize=(16, 7))

# Isolation Forest
axes[0].scatter(X[~outliers_if, 0], X[~outliers_if, 1],
                c='steelblue', s=50, alpha=0.6, label='æ­£å¸¸å€¤')
axes[0].scatter(X[outliers_if, 0], X[outliers_if, 1],
                c='red', s=100, alpha=0.8, marker='X', label='å¤–ã‚Œå€¤')
axes[0].set_xlabel('ç‰¹å¾´é‡1', fontsize=11)
axes[0].set_ylabel('ç‰¹å¾´é‡2', fontsize=11)
axes[0].set_title('Isolation Forest', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# LOF
axes[1].scatter(X[~outliers_lof, 0], X[~outliers_lof, 1],
                c='steelblue', s=50, alpha=0.6, label='æ­£å¸¸å€¤')
axes[1].scatter(X[outliers_lof, 0], X[outliers_lof, 1],
                c='red', s=100, alpha=0.8, marker='X', label='å¤–ã‚Œå€¤')
axes[1].set_xlabel('ç‰¹å¾´é‡1', fontsize=11)
axes[1].set_ylabel('ç‰¹å¾´é‡2', fontsize=11)
axes[1].set_title('Local Outlier Factor', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(f&quot;Isolation Forest: {outliers_if.sum()} å€‹&quot;)
print(f&quot;LOF: {outliers_lof.sum()} å€‹&quot;)
</code></pre>
<h3>DBSCAN clustering</h3>
<pre><code class="language-python">from sklearn.cluster import DBSCAN

def detect_outliers_dbscan(X, eps=3, min_samples=5):
    &quot;&quot;&quot;
    DBSCANã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º

    ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã§ãƒ©ãƒ™ãƒ«-1ãŒå¤–ã‚Œå€¤
    &quot;&quot;&quot;
    clustering = DBSCAN(eps=eps, min_samples=min_samples)
    labels = clustering.fit_predict(X)

    return labels == -1

# å®Ÿè¡Œ
outliers_dbscan = detect_outliers_dbscan(X, eps=5, min_samples=10)

# å¯è¦–åŒ–
plt.figure(figsize=(10, 8))

clustering = DBSCAN(eps=5, min_samples=10)
labels = clustering.fit_predict(X)

unique_labels = set(labels)
colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))

for k, col in zip(unique_labels, colors):
    if k == -1:
        # å¤–ã‚Œå€¤
        class_member_mask = (labels == k)
        xy = X[class_member_mask]
        plt.scatter(xy[:, 0], xy[:, 1], c='red', s=100,
                    marker='X', label='å¤–ã‚Œå€¤', alpha=0.8)
    else:
        # ã‚¯ãƒ©ã‚¹ã‚¿
        class_member_mask = (labels == k)
        xy = X[class_member_mask]
        plt.scatter(xy[:, 0], xy[:, 1], c=[col], s=50,
                    alpha=0.6, label=f'ã‚¯ãƒ©ã‚¹ã‚¿ {k}')

plt.xlabel('ç‰¹å¾´é‡1', fontsize=12)
plt.ylabel('ç‰¹å¾´é‡2', fontsize=12)
plt.title('DBSCAN ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(f&quot;æ¤œå‡ºã•ã‚ŒãŸå¤–ã‚Œå€¤: {outliers_dbscan.sum()} / {len(X)}&quot;)
</code></pre>
<hr />
<h2>1.5 ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ï¼šç†±é›»ææ–™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</h2>
<p>å®Ÿéš›ã®ç†±é›»ææ–™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã®å…¨å·¥ç¨‹ã‚’å®Ÿè·µã—ã¾ã™ã€‚</p>
<pre><code class="language-python"># ç†±é›»ææ–™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
np.random.seed(42)

n_samples = 200

thermoelectric_data = pd.DataFrame({
    'çµ„æˆ_A': np.random.uniform(0.1, 0.9, n_samples),
    'çµ„æˆ_B': np.random.uniform(0.05, 0.3, n_samples),
    'ãƒ‰ãƒ¼ãƒ‘ãƒ³ãƒˆæ¿ƒåº¦': np.random.uniform(0.001, 0.05, n_samples),
    'åˆæˆæ¸©åº¦': np.random.uniform(600, 1200, n_samples),
    'æ ¼å­å®šæ•°': np.random.uniform(5.5, 6.5, n_samples),
    'ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—': np.random.uniform(0.1, 0.8, n_samples),
    'é›»æ°—ä¼å°åº¦': np.random.lognormal(10, 2, n_samples),
    'ã‚¼ãƒ¼ãƒ™ãƒƒã‚¯ä¿‚æ•°': np.random.normal(200, 50, n_samples),
    'ç†±ä¼å°åº¦': np.random.uniform(1, 10, n_samples),
    'ZTå€¤': np.random.uniform(0.1, 2.0, n_samples)
})

# å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ + DFTè¨ˆç®—ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ
thermoelectric_data['ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹'] = np.random.choice(
    ['å®Ÿé¨“', 'DFT'], n_samples, p=[0.6, 0.4]
)

# æ¬ æå€¤ã‚’20%å°å…¥
missing_mask_lattice = np.random.random(n_samples) &lt; 0.15
thermoelectric_data.loc[missing_mask_lattice, 'æ ¼å­å®šæ•°'] = np.nan

missing_mask_bandgap = np.random.random(n_samples) &lt; 0.12
thermoelectric_data.loc[missing_mask_bandgap, 'ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—'] = np.nan

missing_mask_thermal = np.random.random(n_samples) &lt; 0.18
thermoelectric_data.loc[missing_mask_thermal, 'ç†±ä¼å°åº¦'] = np.nan

# å¤–ã‚Œå€¤ã‚’å°å…¥
outlier_idx = np.random.choice(n_samples, 10, replace=False)
thermoelectric_data.loc[outlier_idx, 'ZTå€¤'] += np.random.uniform(2, 5, 10)

print(&quot;=== ç†±é›»ææ–™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ===&quot;)
print(f&quot;ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(thermoelectric_data)}&quot;)
print(f&quot;ç‰¹å¾´é‡æ•°: {thermoelectric_data.shape[1]}&quot;)
print(f&quot;\næ¬ æå€¤æ•°:&quot;)
print(thermoelectric_data.isnull().sum())
</code></pre>
<h3>Step 1: æ¬ æå€¤å‡¦ç†</h3>
<pre><code class="language-python"># æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³å¯è¦–åŒ–
plt.figure(figsize=(12, 6))
sns.heatmap(thermoelectric_data.isnull(),
            cmap='YlOrRd', cbar_kws={'label': 'æ¬ æ'})
plt.title('ç†±é›»ææ–™ãƒ‡ãƒ¼ã‚¿ã®æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³', fontsize=13, fontweight='bold')
plt.xlabel('ç‰¹å¾´é‡', fontsize=11)
plt.ylabel('ã‚µãƒ³ãƒ—ãƒ«', fontsize=11)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# MICEè£œå®Œ
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# æ•°å€¤åˆ—ã®ã¿æŠ½å‡º
numeric_cols = thermoelectric_data.select_dtypes(
    include=[np.number]
).columns

imputer = IterativeImputer(max_iter=10, random_state=42)
thermoelectric_imputed = thermoelectric_data.copy()
thermoelectric_imputed[numeric_cols] = imputer.fit_transform(
    thermoelectric_data[numeric_cols]
)

print(&quot;\næ¬ æå€¤è£œå®Œå®Œäº†&quot;)
print(thermoelectric_imputed.isnull().sum())
</code></pre>
<h3>Step 2: å¤–ã‚Œå€¤æ¤œå‡º</h3>
<pre><code class="language-python"># Isolation Forestã§å¤–ã‚Œå€¤æ¤œå‡º
X_features = thermoelectric_imputed[numeric_cols].values

clf = IsolationForest(contamination=0.05, random_state=42)
outlier_labels = clf.fit_predict(X_features)
outliers_mask = outlier_labels == -1

print(f&quot;\næ¤œå‡ºã•ã‚ŒãŸå¤–ã‚Œå€¤: {outliers_mask.sum()} / {len(thermoelectric_imputed)}&quot;)

# ZTå€¤ã®åˆ†å¸ƒã¨å¤–ã‚Œå€¤
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# ç®±ã²ã’å›³
axes[0].boxplot([
    thermoelectric_imputed.loc[~outliers_mask, 'ZTå€¤'],
    thermoelectric_imputed.loc[outliers_mask, 'ZTå€¤']
], labels=['æ­£å¸¸å€¤', 'å¤–ã‚Œå€¤'])
axes[0].set_ylabel('ZTå€¤', fontsize=12)
axes[0].set_title('ZTå€¤ã®åˆ†å¸ƒ', fontsize=12, fontweight='bold')
axes[0].grid(alpha=0.3)

# æ•£å¸ƒå›³ï¼ˆé›»æ°—ä¼å°åº¦ vs ZTå€¤ï¼‰
axes[1].scatter(
    thermoelectric_imputed.loc[~outliers_mask, 'é›»æ°—ä¼å°åº¦'],
    thermoelectric_imputed.loc[~outliers_mask, 'ZTå€¤'],
    c='steelblue', s=50, alpha=0.6, label='æ­£å¸¸å€¤'
)
axes[1].scatter(
    thermoelectric_imputed.loc[outliers_mask, 'é›»æ°—ä¼å°åº¦'],
    thermoelectric_imputed.loc[outliers_mask, 'ZTå€¤'],
    c='red', s=100, alpha=0.8, marker='X', label='å¤–ã‚Œå€¤'
)
axes[1].set_xlabel('é›»æ°—ä¼å°åº¦ (S/m)', fontsize=11)
axes[1].set_ylabel('ZTå€¤', fontsize=11)
axes[1].set_xscale('log')
axes[1].set_title('å¤–ã‚Œå€¤ã®å¯è¦–åŒ–', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>Step 3: ç‰©ç†çš„å¦¥å½“æ€§æ¤œè¨¼</h3>
<pre><code class="language-python">def validate_physical_constraints(df):
    &quot;&quot;&quot;
    ç‰©ç†çš„åˆ¶ç´„æ¡ä»¶ã®ãƒã‚§ãƒƒã‚¯
    &quot;&quot;&quot;
    violations = []

    # çµ„æˆã®åˆè¨ˆãŒ1å‰å¾Œ
    composition_sum = df['çµ„æˆ_A'] + df['çµ„æˆ_B']
    composition_violation = (composition_sum &lt; 0.9) | (composition_sum &gt; 1.1)
    if composition_violation.any():
        violations.append(
            f&quot;çµ„æˆåˆè¨ˆç•°å¸¸: {composition_violation.sum()} ã‚µãƒ³ãƒ—ãƒ«&quot;
        )

    # ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã¯æ­£
    bandgap_violation = df['ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—'] &lt; 0
    if bandgap_violation.any():
        violations.append(
            f&quot;è² ã®ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—: {bandgap_violation.sum()} ã‚µãƒ³ãƒ—ãƒ«&quot;
        )

    # ZTå€¤ã®ç†è«–ä¸Šé™ï¼ˆZT &gt; 4 ã¯éç¾å®Ÿçš„ï¼‰
    zt_violation = df['ZTå€¤'] &gt; 4
    if zt_violation.any():
        violations.append(
            f&quot;ZTå€¤ç•°å¸¸ï¼ˆ&gt;4ï¼‰: {zt_violation.sum()} ã‚µãƒ³ãƒ—ãƒ«&quot;
        )

    return violations

# æ¤œè¨¼
violations = validate_physical_constraints(thermoelectric_imputed)

print(&quot;\nç‰©ç†çš„å¦¥å½“æ€§æ¤œè¨¼ï¼š&quot;)
if violations:
    for v in violations:
        print(f&quot;âš ï¸ {v}&quot;)
else:
    print(&quot;âœ… å…¨ã¦ã®ã‚µãƒ³ãƒ—ãƒ«ãŒç‰©ç†çš„åˆ¶ç´„ã‚’æº€ãŸã™&quot;)

# å¤–ã‚Œå€¤é™¤å»
thermoelectric_cleaned = thermoelectric_imputed[~outliers_mask].copy()

print(f&quot;\nã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(thermoelectric_cleaned)}&quot;)
print(f&quot;é™¤å»ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«: {outliers_mask.sum()}&quot;)
</code></pre>
<h3>Step 4: ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å‰å¾Œã®æ¯”è¼ƒ</h3>
<pre><code class="language-python"># ãƒ‡ãƒ¼ã‚¿å“è³ªã®æ¯”è¼ƒ
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

features_to_compare = ['ZTå€¤', 'é›»æ°—ä¼å°åº¦', 'ã‚¼ãƒ¼ãƒ™ãƒƒã‚¯ä¿‚æ•°', 'ç†±ä¼å°åº¦']

for idx, feature in enumerate(features_to_compare):
    ax = axes[idx // 2, idx % 2]

    # å…ƒãƒ‡ãƒ¼ã‚¿
    ax.hist(thermoelectric_data[feature].dropna(), bins=30,
            alpha=0.5, label='å…ƒãƒ‡ãƒ¼ã‚¿', color='gray')

    # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œ
    ax.hist(thermoelectric_cleaned[feature], bins=30,
            alpha=0.7, label='ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œ', color='steelblue')

    ax.set_xlabel(feature, fontsize=11)
    ax.set_ylabel('é »åº¦', fontsize=11)
    ax.set_title(f'{feature}ã®åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    ax.legend()
    ax.grid(alpha=0.3)

plt.tight_layout()
plt.show()

# çµ±è¨ˆã‚µãƒãƒªãƒ¼
print(&quot;\n=== ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹æœ ===&quot;)
print(f&quot;å…ƒãƒ‡ãƒ¼ã‚¿: {len(thermoelectric_data)} ã‚µãƒ³ãƒ—ãƒ«, &quot;
      f&quot;{thermoelectric_data.isnull().sum().sum()} æ¬ æå€¤&quot;)
print(f&quot;ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œ: {len(thermoelectric_cleaned)} ã‚µãƒ³ãƒ—ãƒ«, &quot;
      f&quot;{thermoelectric_cleaned.isnull().sum().sum()} æ¬ æå€¤&quot;)
print(f&quot;\nZTå€¤çµ±è¨ˆ:&quot;)
print(f&quot;  å…ƒãƒ‡ãƒ¼ã‚¿: å¹³å‡ {thermoelectric_data['ZTå€¤'].mean():.3f}, &quot;
      f&quot;æ¨™æº–åå·® {thermoelectric_data['ZTå€¤'].std():.3f}&quot;)
print(f&quot;  ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œ: å¹³å‡ {thermoelectric_cleaned['ZTå€¤'].mean():.3f}, &quot;
      f&quot;æ¨™æº–åå·® {thermoelectric_cleaned['ZTå€¤'].std():.3f}&quot;)
</code></pre>
<hr />
<h2>1.6 ãƒ‡ãƒ¼ã‚¿ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¨å†ç¾æ€§</h2>
<h3>ä¸»è¦ææ–™ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</h3>
<p>ææ–™ãƒ‡ãƒ¼ã‚¿ã‚’åˆ©ç”¨ã™ã‚‹éš›ã¯ã€å„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’ç†è§£ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚</p>
<pre><code class="language-python"># ä¸»è¦ææ–™ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±
database_info = pd.DataFrame({
    'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹': [
        'Materials Project',
        'OQMD',
        'NOMAD',
        'AFLOW',
        'Citrination'
    ],
    'ãƒ©ã‚¤ã‚»ãƒ³ã‚¹': [
        'CC BY 4.0',
        'Academic Use',
        'CC BY 4.0',
        'AFLOWLIB Consortium',
        'Commercial/Academic'
    ],
    'ãƒ‡ãƒ¼ã‚¿æ•°': [
        '150,000+',
        '1,000,000+',
        '10,000,000+',
        '3,500,000+',
        '250,000+'
    ],
    'ä¸»è¦ãƒ‡ãƒ¼ã‚¿': [
        'DFTè¨ˆç®—ã€ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã€å½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼',
        'DFTè¨ˆç®—ã€å®‰å®šæ€§ã€ç›¸å›³',
        'è¨ˆç®—ãƒ»å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«',
        'ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—æ§‹é€ ã€ç‰©æ€§äºˆæ¸¬',
        'å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã€ãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶'
    ],
    'API': [
        'pymatgen',
        'qmpy',
        'NOMAD API',
        'AFLOW API',
        'citrination-client'
    ]
})

print(&quot;=== ææ–™ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¯”è¼ƒ ===&quot;)
print(database_info.to_string(index=False))

# ä½¿ç”¨ä¾‹
print(&quot;\nã€Materials Project APIä½¿ç”¨ä¾‹ã€‘&quot;)
print(&quot;```python&quot;)
print(&quot;from pymatgen.ext.matproj import MPRester&quot;)
print(&quot;# API key: https://materialsproject.org/api ã§å–å¾—&quot;)
print(&quot;with MPRester('YOUR_API_KEY') as mpr:&quot;)
print(&quot;    structure = mpr.get_structure_by_material_id('mp-149')&quot;)
print(&quot;    bandgap = mpr.get_bandstructure_by_material_id('mp-149')&quot;)
print(&quot;```&quot;)

print(&quot;\nã€æ³¨æ„äº‹é …ã€‘&quot;)
print(&quot;âœ… è«–æ–‡ãƒ»å•†ç”¨åˆ©ç”¨æ™‚ã¯ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’ç¢ºèª&quot;)
print(&quot;âœ… å¼•ç”¨ã‚’é©åˆ‡ã«è¡Œã†ï¼ˆå„DBã®å¼•ç”¨å½¢å¼ã«å¾“ã†ï¼‰&quot;)
print(&quot;âœ… APIã‚­ãƒ¼ã¯ç’°å¢ƒå¤‰æ•°ã§ç®¡ç†ï¼ˆ.envãƒ•ã‚¡ã‚¤ãƒ«ï¼‰&quot;)
print(&quot;âœ… ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒ»å–å¾—æ—¥ã‚’è¨˜éŒ²&quot;)
</code></pre>
<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ææ–™ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¯”è¼ƒ ===
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹         ãƒ©ã‚¤ã‚»ãƒ³ã‚¹              ãƒ‡ãƒ¼ã‚¿æ•°      ä¸»è¦ãƒ‡ãƒ¼ã‚¿                              API
Materials Project    CC BY 4.0               150,000+      DFTè¨ˆç®—ã€ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã€å½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼    pymatgen
OQMD                 Academic Use            1,000,000+    DFTè¨ˆç®—ã€å®‰å®šæ€§ã€ç›¸å›³                    qmpy
NOMAD                CC BY 4.0               10,000,000+   è¨ˆç®—ãƒ»å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«          NOMAD API
AFLOW                AFLOWLIB Consortium     3,500,000+    ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—æ§‹é€ ã€ç‰©æ€§äºˆæ¸¬                AFLOW API
Citrination          Commercial/Academic     250,000+      å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã€ãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶                  citrination-client
</code></pre>
<h3>ã‚³ãƒ¼ãƒ‰å†ç¾æ€§ã®ç¢ºä¿</h3>
<pre><code class="language-python"># ç’°å¢ƒä»•æ§˜ã®è¨˜éŒ²
import sys
import sklearn
import pandas as pd
import numpy as np

reproducibility_info = {
    'Python': sys.version,
    'NumPy': np.__version__,
    'Pandas': pd.__version__,
    'scikit-learn': sklearn.__version__,
    'Date': '2025-10-19'
}

print(&quot;=== å†ç¾æ€§æƒ…å ± ===&quot;)
for key, value in reproducibility_info.items():
    print(f&quot;{key}: {value}&quot;)

# requirements.txtç”Ÿæˆ
print(&quot;\nã€æ¨å¥¨ç’°å¢ƒã€‘&quot;)
requirements = &quot;&quot;&quot;
numpy==1.24.3
pandas==2.0.3
scikit-learn==1.3.0
matplotlib==3.7.2
seaborn==0.12.2
scipy==1.11.1
&quot;&quot;&quot;
print(requirements)

print(&quot;ã€ç’°å¢ƒæ§‹ç¯‰ã‚³ãƒãƒ³ãƒ‰ã€‘&quot;)
print(&quot;```bash&quot;)
print(&quot;# ä»®æƒ³ç’°å¢ƒä½œæˆ&quot;)
print(&quot;python -m venv venv&quot;)
print(&quot;source venv/bin/activate  # Linux/Mac&quot;)
print(&quot;# venv\\Scripts\\activate  # Windows&quot;)
print(&quot;&quot;)
print(&quot;# ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«&quot;)
print(&quot;pip install -r requirements.txt&quot;)
print(&quot;```&quot;)
</code></pre>
<h3>å®Ÿè·µçš„ãªè½ã¨ã—ç©´ï¼ˆPitfallsï¼‰</h3>
<pre><code class="language-python"># è½ã¨ã—ç©´1: ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ï¼ˆData Leakageï¼‰
print(&quot;=== å®Ÿè·µçš„ãªè½ã¨ã—ç©´ ===\n&quot;)

print(&quot;ã€è½ã¨ã—ç©´1: ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã€‘&quot;)
print(&quot;âŒ æ‚ªã„ä¾‹ï¼šå…¨ãƒ‡ãƒ¼ã‚¿ã§å‰å‡¦ç† â†’ Train/Teståˆ†å‰²&quot;)
print(&quot;```python&quot;)
print(&quot;X_scaled = StandardScaler().fit_transform(X)  # å…¨ãƒ‡ãƒ¼ã‚¿ã§fit&quot;)
print(&quot;X_train, X_test = train_test_split(X_scaled)&quot;)
print(&quot;# â†’ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ãŒè¨“ç·´æ™‚ã«æ¼ã‚Œã¦ã„ã‚‹ï¼&quot;)
print(&quot;```&quot;)

print(&quot;\nâœ… æ­£ã—ã„ä¾‹ï¼šTrain/Teståˆ†å‰² â†’ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§å‰å‡¦ç†&quot;)
print(&quot;```python&quot;)
print(&quot;X_train, X_test = train_test_split(X)&quot;)
print(&quot;scaler = StandardScaler().fit(X_train)  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§fit&quot;)
print(&quot;X_train_scaled = scaler.transform(X_train)&quot;)
print(&quot;X_test_scaled = scaler.transform(X_test)&quot;)
print(&quot;```&quot;)

print(&quot;\nã€è½ã¨ã—ç©´2: çµ„æˆãƒ™ãƒ¼ã‚¹åˆ†å‰²ã®å¿…è¦æ€§ã€‘&quot;)
print(&quot;âŒ æ‚ªã„ä¾‹ï¼šãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²&quot;)
print(&quot;- Liâ‚€.â‚‰CoOâ‚‚ï¼ˆè¨“ç·´ï¼‰ã¨Liâ‚.â‚€CoOâ‚‚ï¼ˆãƒ†ã‚¹ãƒˆï¼‰ã¯é¡ä¼¼&quot;)
print(&quot;- éåº¦ã«æ¥½è¦³çš„ãªæ€§èƒ½è©•ä¾¡&quot;)

print(&quot;\nâœ… æ­£ã—ã„ä¾‹ï¼šçµ„æˆã‚°ãƒ«ãƒ¼ãƒ—åˆ†å‰²&quot;)
print(&quot;```python&quot;)
print(&quot;from sklearn.model_selection import GroupKFold&quot;)
print(&quot;groups = [get_composition_family(formula) for formula in formulas]&quot;)
print(&quot;gkf = GroupKFold(n_splits=5)&quot;)
print(&quot;for train_idx, test_idx in gkf.split(X, y, groups):&quot;)
print(&quot;    # åŒã˜çµ„æˆç³»ã¯åŒã˜foldã«&quot;)
print(&quot;```&quot;)

print(&quot;\nã€è½ã¨ã—ç©´3: å¤–æŒ¿ã®é™ç•Œã€‘&quot;)
print(&quot;âš ï¸ æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¯è¨“ç·´ç¯„å›²å¤–ã®äºˆæ¸¬ãŒè‹¦æ‰‹&quot;)
print(&quot;ä¾‹ï¼šãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—0-3 eVã§è¨“ç·´ â†’ 5 eVã®äºˆæ¸¬ã¯ä¸æ­£ç¢º&quot;)

print(&quot;\nå¯¾ç­–:&quot;)
print(&quot;- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²ã‚’æ˜ç¤º&quot;)
print(&quot;- å¤–æŒ¿é ˜åŸŸã®ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–ï¼ˆãƒ™ã‚¤ã‚ºæ‰‹æ³•ï¼‰&quot;)
print(&quot;- Active Learningã§æ®µéšçš„ã«ç¯„å›²æ‹¡å¤§&quot;)

print(&quot;\nã€è½ã¨ã—ç©´4: ç‰¹å¾´é‡é–“ã®ç›¸é–¢ã€‘&quot;)
print(&quot;âš ï¸ é«˜ç›¸é–¢ç‰¹å¾´é‡ã¯å†—é•·ã§éå­¦ç¿’ã‚’æ‹›ã&quot;)
print(&quot;```python&quot;)
print(&quot;# ç›¸é–¢è¡Œåˆ—ã§ç¢ºèª&quot;)
print(&quot;correlation_matrix = X.corr()&quot;)
print(&quot;high_corr = (correlation_matrix.abs() &gt; 0.9) &amp; (correlation_matrix != 1.0)&quot;)
print(&quot;print(high_corr.sum())  # é«˜ç›¸é–¢ãƒšã‚¢æ•°&quot;)
print(&quot;&quot;)
print(&quot;# å¯¾ç­–ï¼šVIFï¼ˆåˆ†æ•£æ‹¡å¤§ä¿‚æ•°ï¼‰ã§å¤šé‡å…±ç·šæ€§æ¤œå‡º&quot;)
print(&quot;from statsmodels.stats.outliers_influence import variance_inflation_factor&quot;)
print(&quot;vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]&quot;)
print(&quot;```&quot;)
</code></pre>
<hr />
<h2>æ¼”ç¿’å•é¡Œ</h2>
<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦: easyï¼‰</h3>
<p>ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ã€Simple Imputationï¼ˆå¹³å‡å€¤ï¼‰ã¨KNN Imputationã‚’é©ç”¨ã—ã€è£œå®Œç²¾åº¦ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>
<pre><code class="language-python"># æ¼”ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿
np.random.seed(123)
exercise_data = pd.DataFrame({
    'feature1': np.random.normal(50, 10, 100),
    'feature2': np.random.normal(30, 5, 100),
    'feature3': np.random.normal(100, 20, 100)
})

# ãƒ©ãƒ³ãƒ€ãƒ ã«10%æ¬ æ
for col in exercise_data.columns:
    missing_idx = np.random.choice(100, 10, replace=False)
    exercise_data.loc[missing_idx, col] = np.nan
</code></pre>
<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

1. `SimpleImputer(strategy='mean')`ã‚’ä½¿ç”¨
2. `KNNImputer(n_neighbors=5)`ã‚’ä½¿ç”¨
3. å…ƒã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¦ã€è£œå®Œå€¤ã¨ã®å·®ï¼ˆMAEï¼‰ã‚’è¨ˆç®—

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python">from sklearn.impute import SimpleImputer, KNNImputer

# å…ƒã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¯”è¼ƒç”¨ï¼‰
np.random.seed(123)
true_data = pd.DataFrame({
    'feature1': np.random.normal(50, 10, 100),
    'feature2': np.random.normal(30, 5, 100),
    'feature3': np.random.normal(100, 20, 100)
})

# Simple Imputation
simple_imputer = SimpleImputer(strategy='mean')
data_simple = pd.DataFrame(
    simple_imputer.fit_transform(exercise_data),
    columns=exercise_data.columns
)

# KNN Imputation
knn_imputer = KNNImputer(n_neighbors=5)
data_knn = pd.DataFrame(
    knn_imputer.fit_transform(exercise_data),
    columns=exercise_data.columns
)

# ç²¾åº¦è©•ä¾¡
missing_mask = exercise_data.isnull()
mae_simple = []
mae_knn = []

for col in exercise_data.columns:
    mask = missing_mask[col]
    if mask.any():
        mae_s = np.mean(np.abs(data_simple.loc[mask, col] - true_data.loc[mask, col]))
        mae_k = np.mean(np.abs(data_knn.loc[mask, col] - true_data.loc[mask, col]))
        mae_simple.append(mae_s)
        mae_knn.append(mae_k)

print(f&quot;Simple Imputation MAE: {np.mean(mae_simple):.4f}&quot;)
print(f&quot;KNN Imputation MAE: {np.mean(mae_knn):.4f}&quot;)
</code></pre>


</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦: mediumï¼‰</h3>
<p>Latin Hypercube Samplingã‚’ç”¨ã„ã¦ã€3æ¬¡å…ƒã®çµ„æˆç©ºé–“ï¼ˆå…ƒç´ A, B, Cã®å‰²åˆï¼‰ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦ãã ã•ã„ã€‚åˆ¶ç´„æ¡ä»¶ã¨ã—ã¦ã€A + B + C = 1 ã‚’æº€ãŸã™ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚</p>
<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

1. 2æ¬¡å…ƒã§LHSã‚’å®Ÿè¡Œï¼ˆAã¨Bã®ã¿ï¼‰
2. C = 1 - A - B ã§è¨ˆç®—
3. 3æ¬¡å…ƒç©ºé–“ã§å¯è¦–åŒ–

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python">from scipy.stats import qmc
from mpl_toolkits.mplot3d import Axes3D

# 2æ¬¡å…ƒLHSï¼ˆA, Bï¼‰
sampler = qmc.LatinHypercube(d=2, seed=42)
samples_2d = sampler.random(n=50)

# A + B &lt;= 1 ã¨ãªã‚‹ã‚ˆã†ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
A = samples_2d[:, 0] * 0.9  # 0ã€œ0.9
B = (1 - A) * samples_2d[:, 1]  # æ®‹ã‚Šã®ç¯„å›²å†…
C = 1 - A - B

# 3æ¬¡å…ƒå¯è¦–åŒ–
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

ax.scatter(A, B, C, c='steelblue', s=100, alpha=0.6, edgecolors='k')
ax.set_xlabel('å…ƒç´ A', fontsize=12)
ax.set_ylabel('å…ƒç´ B', fontsize=12)
ax.set_zlabel('å…ƒç´ C', fontsize=12)
ax.set_title('çµ„æˆç©ºé–“ã®Latin Hypercube Sampling', fontsize=13, fontweight='bold')

plt.tight_layout()
plt.show()

# åˆ¶ç´„ç¢ºèª
print(f&quot;å…¨ã‚µãƒ³ãƒ—ãƒ«ã§ A+B+C=1: {np.allclose(A+B+C, 1)}&quot;)
</code></pre>


</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦: hardï¼‰</h3>
<p>Isolation Forestã¨LOFã‚’ç”¨ã„ã¦ã€å¤šæ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã®å¤–ã‚Œå€¤æ¤œå‡ºã‚’è¡Œã„ã€ã©ã¡ã‚‰ãŒã‚ˆã‚Šé©åˆ‡ã‹è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚è©•ä¾¡ã«ã¯ã€æ—¢çŸ¥ã®å¤–ã‚Œå€¤ãƒ©ãƒ™ãƒ«ã¨ã®ä¸€è‡´ç‡ï¼ˆPrecision, Recall, F1-scoreï¼‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚</p>
<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

1. æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ + æ„å›³çš„ãªå¤–ã‚Œå€¤ã‚’ç”Ÿæˆ
2. Isolation Forestã¨LOFã§æ¤œå‡º
3. `sklearn.metrics.classification_report`ã§è©•ä¾¡

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python">from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.metrics import classification_report, confusion_matrix

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
X_normal = np.random.randn(200, 5) * 2 + 10
X_outliers = np.random.uniform(0, 20, (20, 5))
X = np.vstack([X_normal, X_outliers])

# çœŸã®ãƒ©ãƒ™ãƒ«ï¼ˆ0: æ­£å¸¸, 1: å¤–ã‚Œå€¤ï¼‰
y_true = np.array([0]*200 + [1]*20)

# Isolation Forest
clf_if = IsolationForest(contamination=0.1, random_state=42)
y_pred_if = (clf_if.fit_predict(X) == -1).astype(int)

# LOF
clf_lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)
y_pred_lof = (clf_lof.fit_predict(X) == -1).astype(int)

# è©•ä¾¡
print(&quot;=== Isolation Forest ===&quot;)
print(classification_report(y_true, y_pred_if,
                           target_names=['æ­£å¸¸', 'å¤–ã‚Œå€¤']))

print(&quot;\n=== Local Outlier Factor ===&quot;)
print(classification_report(y_true, y_pred_lof,
                           target_names=['æ­£å¸¸', 'å¤–ã‚Œå€¤']))

# æ··åŒè¡Œåˆ—
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

cm_if = confusion_matrix(y_true, y_pred_if)
sns.heatmap(cm_if, annot=True, fmt='d', cmap='Blues', ax=axes[0])
axes[0].set_xlabel('äºˆæ¸¬ãƒ©ãƒ™ãƒ«', fontsize=11)
axes[0].set_ylabel('çœŸã®ãƒ©ãƒ™ãƒ«', fontsize=11)
axes[0].set_title('Isolation Forest', fontsize=12, fontweight='bold')

cm_lof = confusion_matrix(y_true, y_pred_lof)
sns.heatmap(cm_lof, annot=True, fmt='d', cmap='Oranges', ax=axes[1])
axes[1].set_xlabel('äºˆæ¸¬ãƒ©ãƒ™ãƒ«', fontsize=11)
axes[1].set_ylabel('çœŸã®ãƒ©ãƒ™ãƒ«', fontsize=11)
axes[1].set_title('LOF', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.show()
</code></pre>


</details>

<hr />
<h2>ã¾ã¨ã‚</h2>
<p>ã“ã®ç« ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿é§†å‹•ææ–™ç§‘å­¦ã«ãŠã‘ã‚‹<strong>ãƒ‡ãƒ¼ã‚¿åé›†æˆ¦ç•¥ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°</strong>ã‚’å­¦ã³ã¾ã—ãŸã€‚</p>
<p><strong>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</strong>ï¼š</p>
<ol>
<li><strong>ææ–™ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´</strong>ï¼šå°è¦æ¨¡ãƒ»ä¸å‡è¡¡ãƒ»ãƒã‚¤ã‚ºãŒå¤šã„ â†’ é©åˆ‡ãªå‰å‡¦ç†ãŒä¸å¯æ¬ </li>
<li><strong>å®Ÿé¨“è¨ˆç”»æ³•</strong>ï¼šDOEã€LHSã€Active Learningã§åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿åé›†</li>
<li><strong>æ¬ æå€¤å‡¦ç†</strong>ï¼šSimple &lt; KNN &lt; MICE ã®é †ã§ç²¾åº¦å‘ä¸Š</li>
<li><strong>å¤–ã‚Œå€¤æ¤œå‡º</strong>ï¼šçµ±è¨ˆçš„æ‰‹æ³•ã€Isolation Forestã€LOFã€DBSCANã‚’ä½¿ã„åˆ†ã‘</li>
<li><strong>ç‰©ç†çš„å¦¥å½“æ€§</strong>ï¼šæ©Ÿæ¢°çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã ã‘ã§ãªãã€ç‰©ç†çš„æ„å‘³ã‚’æ¤œè¨¼</li>
<li><strong>ãƒ‡ãƒ¼ã‚¿ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>ï¼šMaterials Projectã€OQMDã€NOMADãªã©ä¸»è¦DBã®åˆ©ç”¨è¦ç´„ã‚’ç¢ºèª</li>
<li><strong>å†ç¾æ€§ç¢ºä¿</strong>ï¼šç’°å¢ƒãƒãƒ¼ã‚¸ãƒ§ãƒ³è¨˜éŒ²ã€requirements.txtç®¡ç†</li>
<li><strong>å®Ÿè·µçš„è½ã¨ã—ç©´</strong>ï¼šãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã€çµ„æˆãƒ™ãƒ¼ã‚¹åˆ†å‰²ã€å¤–æŒ¿ã®é™ç•Œã€ç‰¹å¾´é‡ç›¸é–¢</li>
</ol>
<p><strong>æ¬¡ç« äºˆå‘Š</strong>ï¼š
Chapter 2ã§ã¯ã€ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰<strong>æœ‰åŠ¹ãªç‰¹å¾´é‡ã‚’è¨­è¨ˆ</strong>ã™ã‚‹æ‰‹æ³•ï¼ˆç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼‰ã‚’å­¦ã³ã¾ã™ã€‚matminerã‚’ç”¨ã„ãŸææ–™è¨˜è¿°å­ç”Ÿæˆã€æ¬¡å…ƒå‰Šæ¸›ã€ç‰¹å¾´é‡é¸æŠã‚’å®Ÿè·µã—ã¾ã™ã€‚</p>
<hr />
<h2>Chapter 1 ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ</h2>
<h3>ãƒ‡ãƒ¼ã‚¿åé›†</h3>
<ul>
<li>[ ] <strong>å®Ÿé¨“è¨ˆç”»æ³•ã®é¸æŠ</strong></li>
<li>[ ] å…¨æ¢ç´¢ï¼ˆFull Factorialï¼‰vs éƒ¨åˆ†æ¢ç´¢ï¼ˆFractional Factorialï¼‰ã‚’åˆ¤æ–­</li>
<li>[ ] Latin Hypercube Samplingã§æ¢ç´¢ç©ºé–“ã‚’å‡ä¸€ã«ã‚«ãƒãƒ¼</li>
<li>
<p>[ ] Active Learningã§ä¸ç¢ºå®Ÿæ€§ã®é«˜ã„é ˜åŸŸã‚’å„ªå…ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</p>
</li>
<li>
<p>[ ] <strong>ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ç¢ºèª</strong></p>
</li>
<li>[ ] å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã€DFTè¨ˆç®—ã€æ–‡çŒ®ãƒ‡ãƒ¼ã‚¿ã®æ··åœ¨ã‚’æŠŠæ¡</li>
<li>[ ] å„ã‚½ãƒ¼ã‚¹ã®ç²¾åº¦ã¨ä¿¡é ¼æ€§ã‚’è©•ä¾¡</li>
<li>
<p>[ ] ãƒ‡ãƒ¼ã‚¿å–å¾—æ—¥ã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’è¨˜éŒ²</p>
</li>
<li>
<p>[ ] <strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¨å¼•ç”¨</strong></p>
</li>
<li>[ ] Materials Projectã€OQMDã€NOMADã€AFLOWã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’ç¢ºèª</li>
<li>[ ] è«–æ–‡ãƒ»å•†ç”¨åˆ©ç”¨æ™‚ã®åˆ¶ç´„ã‚’ç†è§£</li>
<li>[ ] é©åˆ‡ãªå¼•ç”¨å½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’å¼•ç”¨</li>
</ul>
<h3>ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°</h3>
<ul>
<li>[ ] <strong>æ¬ æå€¤å‡¦ç†</strong></li>
<li>[ ] æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†é¡ï¼ˆMCARã€MARã€MNARï¼‰</li>
<li>[ ] Simple Imputationï¼ˆå¹³å‡å€¤ãƒ»ä¸­å¤®å€¤ï¼‰ã§åŸºæº–æ€§èƒ½ç¢ºèª</li>
<li>[ ] KNN Imputationã§ç›¸é–¢ã‚’è€ƒæ…®ã—ãŸè£œå®Œ</li>
<li>[ ] MICEã§è¤‡é›‘ãªä¾å­˜é–¢ä¿‚ã‚’æ‰ãˆã‚‹è£œå®Œ</li>
<li>
<p>[ ] è£œå®Œå‰å¾Œã®çµ±è¨ˆé‡å¤‰åŒ–ã‚’ç¢ºèª</p>
</li>
<li>
<p>[ ] <strong>å¤–ã‚Œå€¤æ¤œå‡º</strong></p>
</li>
<li>[ ] Z-scoreæ³•ã€IQRæ³•ã§å˜å¤‰é‡å¤–ã‚Œå€¤æ¤œå‡º</li>
<li>[ ] Isolation Forestã§å¤šå¤‰é‡å¤–ã‚Œå€¤æ¤œå‡º</li>
<li>[ ] LOFã§å±€æ‰€çš„å¯†åº¦ãƒ™ãƒ¼ã‚¹ã®æ¤œå‡º</li>
<li>[ ] DBSCANã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ™ãƒ¼ã‚¹ã®æ¤œå‡º</li>
<li>
<p>[ ] å¤–ã‚Œå€¤ãŒç‰©ç†çš„ã«å¦¥å½“ã‹æ¤œè¨¼ï¼ˆæ¸¬å®šã‚¨ãƒ©ãƒ¼ vs æ–°ç™ºè¦‹ï¼‰</p>
</li>
<li>
<p>[ ] <strong>ç‰©ç†çš„å¦¥å½“æ€§æ¤œè¨¼</strong></p>
</li>
<li>[ ] çµ„æˆã®åˆè¨ˆãŒ1å‰å¾Œï¼ˆè¨±å®¹ç¯„å›²Â±0.1ï¼‰</li>
<li>[ ] ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã€å½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼ãªã©ãŒæ­£ã®å€¤</li>
<li>[ ] ç‰©æ€§å€¤ãŒç†è«–çš„ä¸Šé™ã‚’è¶…ãˆã¦ã„ãªã„ã‹</li>
<li>[ ] æ—¢çŸ¥ã®ç‰©ç†æ³•å‰‡ï¼ˆã‚¢ãƒ¬ãƒ‹ã‚¦ã‚¹å‰‡ãªã©ï¼‰ã¨çŸ›ç›¾ã—ãªã„ã‹</li>
</ul>
<h3>å®Ÿè·µçš„è½ã¨ã—ç©´ã®å›é¿</h3>
<ul>
<li>[ ] <strong>ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢</strong></li>
<li>[ ] Train/Teståˆ†å‰²<strong>å¾Œ</strong>ã«å‰å‡¦ç†ï¼ˆStandardScalerã€Imputerãªã©ï¼‰</li>
<li>[ ] ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ™‚ã‚‚å„foldã§ç‹¬ç«‹ã«å‰å‡¦ç†</li>
<li>
<p>[ ] ç›®çš„å¤‰æ•°ã‚’ç”¨ã„ãŸç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’é¿ã‘ã‚‹</p>
</li>
<li>
<p>[ ] <strong>çµ„æˆãƒ™ãƒ¼ã‚¹åˆ†å‰²</strong></p>
</li>
<li>[ ] é¡ä¼¼çµ„æˆãŒåŒã˜foldã«å…¥ã‚‹ã‚ˆã†GroupKFoldä½¿ç”¨</li>
<li>[ ] ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²ã§éåº¦ã«æ¥½è¦³çš„ãªè©•ä¾¡ã‚’é¿ã‘ã‚‹</li>
<li>
<p>[ ] ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ææ–™ç³»ã®å¤šæ§˜æ€§ã‚’ç¢ºä¿</p>
</li>
<li>
<p>[ ] <strong>å¤–æŒ¿ã®é™ç•Œèªè­˜</strong></p>
</li>
<li>[ ] è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²ï¼ˆæœ€å°å€¤ãƒ»æœ€å¤§å€¤ï¼‰ã‚’æ˜ç¤º</li>
<li>[ ] å¤–æŒ¿é ˜åŸŸã®äºˆæ¸¬ã«ã¯ä¸ç¢ºå®Ÿæ€§ã‚’ä»˜ä¸</li>
<li>
<p>[ ] Active Learningã§æ®µéšçš„ã«ç¯„å›²æ‹¡å¤§</p>
</li>
<li>
<p>[ ] <strong>ç‰¹å¾´é‡é–“ã®ç›¸é–¢ç®¡ç†</strong></p>
</li>
<li>[ ] ç›¸é–¢è¡Œåˆ—ã§é«˜ç›¸é–¢ãƒšã‚¢ï¼ˆ|r| &gt; 0.9ï¼‰ã‚’ç‰¹å®š</li>
<li>[ ] VIFï¼ˆåˆ†æ•£æ‹¡å¤§ä¿‚æ•°ï¼‰ã§å¤šé‡å…±ç·šæ€§ã‚’æ¤œå‡º</li>
<li>[ ] å†—é•·ãªç‰¹å¾´é‡ã‚’é™¤å»ã¾ãŸã¯ä¸»æˆåˆ†åˆ†æã§é›†ç´„</li>
</ul>
<h3>å†ç¾æ€§ã®ç¢ºä¿</h3>
<ul>
<li>[ ] <strong>ç’°å¢ƒè¨˜éŒ²</strong></li>
<li>[ ] Pythonã€NumPyã€Pandasã€scikit-learnã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³è¨˜éŒ²</li>
<li>[ ] requirements.txtã¾ãŸã¯environment.ymlã‚’ä½œæˆ</li>
<li>
<p>[ ] ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’å›ºå®šï¼ˆ<code>random_state=42</code>ãªã©ï¼‰</p>
</li>
<li>
<p>[ ] <strong>ãƒ‡ãƒ¼ã‚¿ç®¡ç†</strong></p>
</li>
<li>[ ] å…ƒãƒ‡ãƒ¼ã‚¿ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†é›¢ä¿å­˜</li>
<li>[ ] ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†</li>
<li>
<p>[ ] ãƒ‡ãƒ¼ã‚¿ã®å–å¾—æ—¥ãƒ»ã‚½ãƒ¼ã‚¹ãƒ»å‡¦ç†å±¥æ­´ã‚’è¨˜éŒ²</p>
</li>
<li>
<p>[ ] <strong>ã‚³ãƒ¼ãƒ‰å“è³ª</strong></p>
</li>
<li>[ ] é–¢æ•°åŒ–ãƒ»ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã§å†åˆ©ç”¨æ€§å‘ä¸Š</li>
<li>[ ] Docstringã§å‡¦ç†å†…å®¹ã‚’æ–‡æ›¸åŒ–</li>
<li>[ ] å˜ä½“ãƒ†ã‚¹ãƒˆã§ä¸»è¦é–¢æ•°ã®å‹•ä½œç¢ºèª</li>
</ul>
<h3>ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡æŒ‡æ¨™</h3>
<ul>
<li>[ ] <strong>å®Œå…¨æ€§</strong></li>
<li>[ ] æ¬ æç‡ &lt; 20%ï¼ˆæ¨å¥¨ï¼‰</li>
<li>
<p>[ ] é‡è¦ç‰¹å¾´é‡ã®æ¬ æç‡ &lt; 10%</p>
</li>
<li>
<p>[ ] <strong>æ­£ç¢ºæ€§</strong></p>
</li>
<li>[ ] å¤–ã‚Œå€¤ç‡ &lt; 5%</li>
<li>
<p>[ ] ç‰©ç†åˆ¶ç´„é•åç‡ = 0%</p>
</li>
<li>
<p>[ ] <strong>ä»£è¡¨æ€§</strong></p>
</li>
<li>[ ] ã‚µãƒ³ãƒ—ãƒ«/ç‰¹å¾´é‡æ¯” &gt; 10:1ï¼ˆæ¨å¥¨ï¼‰</li>
<li>
<p>[ ] ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡æ¯” &lt; 10:1ï¼ˆåˆ†é¡å•é¡Œï¼‰</p>
</li>
<li>
<p>[ ] <strong>ä¿¡é ¼æ€§</strong></p>
</li>
<li>[ ] è¤‡æ•°ã‚½ãƒ¼ã‚¹é–“ã®ä¸€è‡´åº¦ &gt; 80%</li>
<li>[ ] æ¸¬å®šèª¤å·®ã®æ¨™æº–åå·®è¨˜éŒ²</li>
</ul>
<hr />
<h2>å‚è€ƒæ–‡çŒ®</h2>
<ol>
<li>
<p><strong>Little, R. J. &amp; Rubin, D. B.</strong> (2019). <em>Statistical Analysis with Missing Data</em> (3rd ed.). Wiley. <a href="https://doi.org/10.1002/9781119482260">DOI: 10.1002/9781119482260</a></p>
</li>
<li>
<p><strong>Liu, F. T., Ting, K. M., &amp; Zhou, Z. H.</strong> (2008). Isolation forest. In <em>2008 Eighth IEEE International Conference on Data Mining</em> (pp. 413-422). IEEE. <a href="https://doi.org/10.1109/ICDM.2008.17">DOI: 10.1109/ICDM.2008.17</a></p>
</li>
<li>
<p><strong>Breunig, M. M., Kriegel, H. P., Ng, R. T., &amp; Sander, J.</strong> (2000). LOF: identifying density-based local outliers. In <em>ACM SIGMOD Record</em> (Vol. 29, No. 2, pp. 93-104). <a href="https://doi.org/10.1145/335191.335388">DOI: 10.1145/335191.335388</a></p>
</li>
<li>
<p><strong>McKay, M. D., Beckman, R. J., &amp; Conover, W. J.</strong> (1979). A comparison of three methods for selecting values of input variables in the analysis of output from a computer code. <em>Technometrics</em>, 21(2), 239-245. <a href="https://doi.org/10.1080/00401706.1979.10489755">DOI: 10.1080/00401706.1979.10489755</a></p>
</li>
<li>
<p><strong>Settles, B.</strong> (2009). <em>Active Learning Literature Survey</em> (Computer Sciences Technical Report 1648). University of Wisconsin-Madison.</p>
</li>
</ol>
<hr />
<p><a href="index.html">â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a> | <a href="chapter-2.html">Chapter 2ã¸é€²ã‚€ â†’</a></p><div class="navigation">
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter-2.html" class="nav-button">æ¬¡ã®ç«  â†’</a>
</div>
    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimotoï¼ˆæ±åŒ—å¤§å­¦ï¼‰</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
