<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Chapter</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 20-25ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ÂàùÁ¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 0ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 0Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 1: „Éá„Éº„ÇøÂèéÈõÜÊà¶Áï•„Å®„ÇØ„É™„Éº„Éã„É≥„Ç∞</h1>
<hr />
<h2>Â≠¶ÁøíÁõÆÊ®ô</h2>
<p>„Åì„ÅÆÁ´†„ÇíË™≠„ÇÄ„Åì„Å®„Åß„ÄÅ‰ª•‰∏ã„ÇíÁøíÂæó„Åß„Åç„Åæ„ÅôÔºö</p>
<p>‚úÖ ÊùêÊñô„Éá„Éº„Çø„ÅÆÁâπÂæ¥ÔºàÂ∞èË¶èÊ®°„Éª‰∏çÂùáË°°„Éª„Éé„Ç§„Ç∫Ôºâ„Å®Ë™≤È°å„ÅÆÁêÜËß£
‚úÖ ÂÆüÈ®ìË®àÁîªÊ≥ïÔºàDOEÔºâ„Å®Latin Hypercube Sampling„ÅÆÂÆüË∑µ
‚úÖ Ê¨†ÊêçÂÄ§Âá¶ÁêÜ„ÅÆÈÅ©Âàá„Å™ÊâãÊ≥ïÈÅ∏ÊäûÔºàSimple/KNN/MICEÔºâ
‚úÖ Â§ñ„ÇåÂÄ§Ê§úÂá∫„Ç¢„É´„Ç¥„É™„Ç∫„É†ÔºàIsolation Forest„ÄÅLOF„ÄÅDBSCANÔºâ„ÅÆÊ¥ªÁî®
‚úÖ ÁÜ±ÈõªÊùêÊñô„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÁî®„ÅÑ„ÅüÂÆüË∑µÁöÑ„Éá„Éº„Çø„ÇØ„É™„Éº„Éã„É≥„Ç∞</p>
<hr />
<h2>1.1 ÊùêÊñô„Éá„Éº„Çø„ÅÆÁâπÂæ¥</h2>
<p>ÊùêÊñôÁßëÂ≠¶„Å´„Åä„Åë„Çã„Éá„Éº„Çø„Å´„ÅØ„ÄÅ‰∏ÄËà¨ÁöÑ„Å™„Éì„ÉÉ„Ç∞„Éá„Éº„Çø„Å®„ÅØÁï∞„Å™„ÇãÁâπÂæ¥„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ</p>
<h3>Â∞èË¶èÊ®°„Éª‰∏çÂùáË°°„Éá„Éº„Çø„ÅÆÂïèÈ°å</h3>
<p><strong>ÁâπÂæ¥</strong>Ôºö
- <strong>„Çµ„É≥„Éó„É´Êï∞„ÅåÂ∞ë„Å™„ÅÑ</strong>ÔºöÂÆüÈ®ì„Å´„ÅØÊôÇÈñì„Å®„Ç≥„Çπ„Éà„Åå„Åã„Åã„Çã„Åü„ÇÅ„ÄÅ„Éá„Éº„ÇøÊï∞„ÅØÊï∞ÂçÅ„ÄúÊï∞ÂçÉ‰ª∂Á®ãÂ∫¶
- <strong>„ÇØ„É©„Çπ‰∏çÂùáË°°</strong>ÔºöÁâπÂÆö„ÅÆÁµÑÊàê„ÇÑÊù°‰ª∂„Å´ÂÅè„Å£„Åü„Éá„Éº„ÇøÂàÜÂ∏É
- <strong>Ê¨°ÂÖÉ„ÅÆÂë™„ÅÑ</strong>ÔºöË™¨ÊòéÂ§âÊï∞ÔºàË®òËø∞Â≠êÔºâ„ÅÆÊï∞„Å´ÂØæ„Åó„Å¶„Çµ„É≥„Éó„É´Êï∞„ÅåÂ∞ë„Å™„ÅÑ</p>
<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ÊùêÊñô„Éá„Éº„Çø„ÅÆÂÖ∏ÂûãÁöÑ„Çµ„Ç§„Ç∫
datasets_info = {
    'ÊùêÊñô„Çø„Ç§„Éó': ['ÁÜ±ÈõªÊùêÊñô', '„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó', 'Ë∂Ö‰ºùÂ∞é‰Ωì',
                   'Ëß¶Â™í', 'ÈõªÊ±†ÊùêÊñô'],
    '„Çµ„É≥„Éó„É´Êï∞': [312, 1563, 89, 487, 253],
    'ÁâπÂæ¥ÈáèÊï∞': [45, 128, 67, 93, 112]
}

df_info = pd.DataFrame(datasets_info)
df_info['„Çµ„É≥„Éó„É´/ÁâπÂæ¥ÈáèÊØî'] = (
    df_info['„Çµ„É≥„Éó„É´Êï∞'] / df_info['ÁâπÂæ¥ÈáèÊï∞']
)

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# „Çµ„É≥„Éó„É´Êï∞ vs ÁâπÂæ¥ÈáèÊï∞
axes[0].scatter(df_info['ÁâπÂæ¥ÈáèÊï∞'], df_info['„Çµ„É≥„Éó„É´Êï∞'],
                s=100, alpha=0.6, c='steelblue')
for idx, row in df_info.iterrows():
    axes[0].annotate(row['ÊùêÊñô„Çø„Ç§„Éó'],
                     (row['ÁâπÂæ¥ÈáèÊï∞'], row['„Çµ„É≥„Éó„É´Êï∞']),
                     fontsize=9, ha='right')
axes[0].plot([0, 150], [0, 150], 'r--',
             label='„Çµ„É≥„Éó„É´Êï∞=ÁâπÂæ¥ÈáèÊï∞', alpha=0.5)
axes[0].set_xlabel('ÁâπÂæ¥ÈáèÊï∞', fontsize=12)
axes[0].set_ylabel('„Çµ„É≥„Éó„É´Êï∞', fontsize=12)
axes[0].set_title('ÊùêÊñô„Éá„Éº„Çø„ÅÆË¶èÊ®°', fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# „Çµ„É≥„Éó„É´/ÁâπÂæ¥ÈáèÊØî
axes[1].barh(df_info['ÊùêÊñô„Çø„Ç§„Éó'],
             df_info['„Çµ„É≥„Éó„É´/ÁâπÂæ¥ÈáèÊØî'],
             color='coral', alpha=0.7)
axes[1].axvline(x=10, color='red', linestyle='--',
                label='Êé®Â•®ÊúÄÂ∞èÊØî (10:1)', linewidth=2)
axes[1].set_xlabel('„Çµ„É≥„Éó„É´Êï∞ / ÁâπÂæ¥ÈáèÊï∞', fontsize=12)
axes[1].set_title('„Éá„Éº„ÇøÂÖÖË∂≥Â∫¶', fontsize=13, fontweight='bold')
axes[1].legend()
axes[1].grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;ÊùêÊñô„Éá„Éº„Çø„ÅÆÂÖ∏ÂûãÁöÑÁâπÂæ¥Ôºö&quot;)
print(f&quot;Âπ≥Âùá„Çµ„É≥„Éó„É´Êï∞: {df_info['„Çµ„É≥„Éó„É´Êï∞'].mean():.0f}&quot;)
print(f&quot;Âπ≥ÂùáÁâπÂæ¥ÈáèÊï∞: {df_info['ÁâπÂæ¥ÈáèÊï∞'].mean():.0f}&quot;)
print(f&quot;Âπ≥Âùá„Çµ„É≥„Éó„É´/ÁâπÂæ¥ÈáèÊØî: {df_info['„Çµ„É≥„Éó„É´/ÁâπÂæ¥ÈáèÊØî'].mean():.2f}&quot;)
print(&quot;\n‚ö†Ô∏è Â§ö„Åè„ÅÆÊùêÊñô„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅßÊé®Â•®ÊØî 10:1 „Çí‰∏ãÂõû„Çã&quot;)
</code></pre>
<p><strong>Âá∫Âäõ</strong>Ôºö</p>
<pre><code>ÊùêÊñô„Éá„Éº„Çø„ÅÆÂÖ∏ÂûãÁöÑÁâπÂæ¥Ôºö
Âπ≥Âùá„Çµ„É≥„Éó„É´Êï∞: 541
Âπ≥ÂùáÁâπÂæ¥ÈáèÊï∞: 89
Âπ≥Âùá„Çµ„É≥„Éó„É´/ÁâπÂæ¥ÈáèÊØî: 7.36

‚ö†Ô∏è Â§ö„Åè„ÅÆÊùêÊñô„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅßÊé®Â•®ÊØî 10:1 „Çí‰∏ãÂõû„Çã
</code></pre>
<h3>„Éé„Ç§„Ç∫„Å®Â§ñ„ÇåÂÄ§</h3>
<p>ÊùêÊñôÂÆüÈ®ì„Éá„Éº„Çø„Å´„ÅØÊßò„ÄÖ„Å™„Éé„Ç§„Ç∫Ê∫ê„Åå„ÅÇ„Çä„Åæ„ÅôÔºö</p>
<pre><code class="language-python"># „Éé„Ç§„Ç∫„ÅÆÁ®ÆÈ°û„Å®ÂΩ±Èüø„ÇíÂèØË¶ñÂåñ
np.random.seed(42)

# Áúü„ÅÆÈñ¢‰øÇÔºà„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó vs Ê†ºÂ≠êÂÆöÊï∞Ôºâ
n_samples = 100
lattice_constant = np.linspace(3.5, 6.5, n_samples)
bandgap_true = 2.5 * np.exp(-0.3 * (lattice_constant - 4))

# ÂêÑÁ®Æ„Éé„Ç§„Ç∫„ÇíËøΩÂä†
measurement_noise = np.random.normal(0, 0.1, n_samples)
systematic_bias = 0.2  # Ê∏¨ÂÆöË£ÖÁΩÆ„ÅÆÁ≥ªÁµ±Ë™§Â∑Æ
outliers_idx = np.random.choice(n_samples, 5, replace=False)

bandgap_measured = bandgap_true + measurement_noise + systematic_bias
bandgap_measured[outliers_idx] += np.random.uniform(0.5, 1.5, 5)

# ÂèØË¶ñÂåñ
fig, ax = plt.subplots(figsize=(10, 6))

ax.plot(lattice_constant, bandgap_true, 'b-',
        linewidth=2, label='Áúü„ÅÆÈñ¢‰øÇ', alpha=0.7)
ax.scatter(lattice_constant, bandgap_measured,
           c='gray', s=50, alpha=0.5, label='Ê∏¨ÂÆöÂÄ§Ôºà„Éé„Ç§„Ç∫Âê´Ôºâ')
ax.scatter(lattice_constant[outliers_idx],
           bandgap_measured[outliers_idx],
           c='red', s=100, marker='X',
           label='Â§ñ„ÇåÂÄ§', zorder=10)

ax.set_xlabel('Ê†ºÂ≠êÂÆöÊï∞ (√Ö)', fontsize=12)
ax.set_ylabel('„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó (eV)', fontsize=12)
ax.set_title('ÊùêÊñô„Éá„Éº„Çø„Å´„Åä„Åë„Çã„Éé„Ç§„Ç∫„Å®Â§ñ„ÇåÂÄ§',
             fontsize=13, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

plt.tight_layout()
plt.show()

# „Éé„Ç§„Ç∫Áµ±Ë®à
print(&quot;„Éé„Ç§„Ç∫ÂàÜÊûêÔºö&quot;)
print(f&quot;Ê∏¨ÂÆö„Éé„Ç§„Ç∫Ê®ôÊ∫ñÂÅèÂ∑Æ: {measurement_noise.std():.3f} eV&quot;)
print(f&quot;Á≥ªÁµ±Ë™§Â∑Æ: {systematic_bias:.3f} eV&quot;)
print(f&quot;Â§ñ„ÇåÂÄ§Êï∞: {len(outliers_idx)} / {n_samples}&quot;)
print(f&quot;Â§ñ„ÇåÂÄ§„ÅÆÂπ≥ÂùáÂÅèÂ∑Æ: &quot;
      f&quot;{(bandgap_measured[outliers_idx] - bandgap_true[outliers_idx]).mean():.3f} eV&quot;)
</code></pre>
<h3>„Éá„Éº„Çø„ÅÆ‰ø°È†ºÊÄßË©ï‰æ°</h3>
<p>„Éá„Éº„ÇøÂìÅË≥™„ÇíÂÆöÈáèË©ï‰æ°„Åô„ÇãÊåáÊ®ôÔºö</p>
<pre><code class="language-python">def assess_data_quality(data, true_values=None):
    &quot;&quot;&quot;
    „Éá„Éº„ÇøÂìÅË≥™Ë©ï‰æ°

    Parameters:
    -----------
    data : array-like
        Ê∏¨ÂÆö„Éá„Éº„Çø
    true_values : array-like, optional
        ÁúüÂÄ§ÔºàÊó¢Áü•„ÅÆÂ†¥ÂêàÔºâ

    Returns:
    --------
    dict : ÂìÅË≥™ÊåáÊ®ô
    &quot;&quot;&quot;
    quality_metrics = {}

    # Âü∫Êú¨Áµ±Ë®à
    quality_metrics['mean'] = np.mean(data)
    quality_metrics['std'] = np.std(data)
    quality_metrics['cv'] = np.std(data) / np.mean(data)  # Â§âÂãï‰øÇÊï∞

    # Â§ñ„ÇåÂÄ§Ââ≤ÂêàÔºàIQRÊ≥ïÔºâ
    Q1, Q3 = np.percentile(data, [25, 75])
    IQR = Q3 - Q1
    outliers = (data &lt; Q1 - 1.5*IQR) | (data &gt; Q3 + 1.5*IQR)
    quality_metrics['outlier_ratio'] = outliers.sum() / len(data)

    # ÁúüÂÄ§„Å®„ÅÆÊØîËºÉÔºàÊó¢Áü•„ÅÆÂ†¥ÂêàÔºâ
    if true_values is not None:
        quality_metrics['mae'] = np.mean(np.abs(data - true_values))
        quality_metrics['rmse'] = np.sqrt(
            np.mean((data - true_values)**2)
        )
        quality_metrics['r2'] = 1 - (
            np.sum((data - true_values)**2) /
            np.sum((true_values - np.mean(true_values))**2)
        )

    return quality_metrics

# Ë©ï‰æ°ÂÆüË°å
quality = assess_data_quality(bandgap_measured, bandgap_true)

print(&quot;„Éá„Éº„ÇøÂìÅË≥™Ë©ï‰æ°Ôºö&quot;)
print(f&quot;Âπ≥ÂùáÂÄ§: {quality['mean']:.3f} eV&quot;)
print(f&quot;Ê®ôÊ∫ñÂÅèÂ∑Æ: {quality['std']:.3f} eV&quot;)
print(f&quot;Â§âÂãï‰øÇÊï∞: {quality['cv']:.3f}&quot;)
print(f&quot;Â§ñ„ÇåÂÄ§Ââ≤Âêà: {quality['outlier_ratio']:.1%}&quot;)
print(f&quot;\nÁúüÂÄ§„Å®„ÅÆÊØîËºÉÔºö&quot;)
print(f&quot;MAE: {quality['mae']:.3f} eV&quot;)
print(f&quot;RMSE: {quality['rmse']:.3f} eV&quot;)
print(f&quot;R¬≤: {quality['r2']:.3f}&quot;)
</code></pre>
<h3>„Éá„Éº„Çø„ÅÆÁ®ÆÈ°ûÔºöÂÆüÈ®ì„ÄÅË®àÁÆó„ÄÅÊñáÁåÆ</h3>
<pre><code class="language-python"># Áï∞„Å™„Çã„Éá„Éº„Çø„ÇΩ„Éº„Çπ„ÅÆÁâπÂæ¥
data_sources = pd.DataFrame({
    '„Éá„Éº„Çø„ÇΩ„Éº„Çπ': ['ÂÆüÈ®ì', 'DFTË®àÁÆó', 'ÊñáÁåÆ', 'Áµ±Âêà'],
    '„Çµ„É≥„Éó„É´Êï∞': [150, 500, 300, 950],
    'Á≤æÂ∫¶': [0.85, 0.95, 0.75, 0.80],
    '„Ç≥„Çπ„ÉàÔºàÁõ∏ÂØæÔºâ': [10, 3, 1, 4],
    'ÂèñÂæóÊôÇÈñìÔºàÊó•Ôºâ': [30, 7, 3, 15]
})

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# „Çµ„É≥„Éó„É´Êï∞
axes[0,0].bar(data_sources['„Éá„Éº„Çø„ÇΩ„Éº„Çπ'],
              data_sources['„Çµ„É≥„Éó„É´Êï∞'],
              color=['#FF6B6B', '#4ECDC4', '#FFE66D', '#95E1D3'])
axes[0,0].set_ylabel('„Çµ„É≥„Éó„É´Êï∞', fontsize=11)
axes[0,0].set_title('„Éá„Éº„ÇøÈáè', fontsize=12, fontweight='bold')
axes[0,0].grid(axis='y', alpha=0.3)

# Á≤æÂ∫¶
axes[0,1].bar(data_sources['„Éá„Éº„Çø„ÇΩ„Éº„Çπ'],
              data_sources['Á≤æÂ∫¶'],
              color=['#FF6B6B', '#4ECDC4', '#FFE66D', '#95E1D3'])
axes[0,1].set_ylabel('Á≤æÂ∫¶', fontsize=11)
axes[0,1].set_ylim(0, 1)
axes[0,1].set_title('„Éá„Éº„ÇøÁ≤æÂ∫¶', fontsize=12, fontweight='bold')
axes[0,1].grid(axis='y', alpha=0.3)

# „Ç≥„Çπ„Éà
axes[1,0].bar(data_sources['„Éá„Éº„Çø„ÇΩ„Éº„Çπ'],
              data_sources['„Ç≥„Çπ„ÉàÔºàÁõ∏ÂØæÔºâ'],
              color=['#FF6B6B', '#4ECDC4', '#FFE66D', '#95E1D3'])
axes[1,0].set_ylabel('Áõ∏ÂØæ„Ç≥„Çπ„Éà', fontsize=11)
axes[1,0].set_title('ÂèñÂæó„Ç≥„Çπ„Éà', fontsize=12, fontweight='bold')
axes[1,0].grid(axis='y', alpha=0.3)

# ÂèñÂæóÊôÇÈñì
axes[1,1].bar(data_sources['„Éá„Éº„Çø„ÇΩ„Éº„Çπ'],
              data_sources['ÂèñÂæóÊôÇÈñìÔºàÊó•Ôºâ'],
              color=['#FF6B6B', '#4ECDC4', '#FFE66D', '#95E1D3'])
axes[1,1].set_ylabel('Êó•Êï∞', fontsize=11)
axes[1,1].set_title('ÂèñÂæóÊôÇÈñì', fontsize=12, fontweight='bold')
axes[1,1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;\nÂêÑ„Éá„Éº„Çø„ÇΩ„Éº„Çπ„ÅÆÁâπÂæ¥Ôºö&quot;)
print(data_sources.to_string(index=False))
</code></pre>
<hr />
<h2>1.2 „Éá„Éº„ÇøÂèéÈõÜÊà¶Áï•</h2>
<p>ÂäπÁéáÁöÑ„Å™„Éá„Éº„ÇøÂèéÈõÜ„ÅÆ„Åü„ÇÅ„ÅÆÊà¶Áï•ÁöÑ„Ç¢„Éó„É≠„Éº„ÉÅ„ÇíÂ≠¶„Å≥„Åæ„Åô„ÄÇ</p>
<h3>ÂÆüÈ®ìË®àÁîªÊ≥ïÔºàDOE: Design of ExperimentsÔºâ</h3>
<p><strong>ÁõÆÁöÑ</strong>ÔºöÈôê„Çâ„Çå„ÅüÂÆüÈ®ìÂõûÊï∞„ÅßÊúÄÂ§ß„ÅÆÊÉÖÂ†±„ÇíÂæó„Çã</p>
<pre><code class="language-python">from scipy.stats import qmc

def full_factorial_design(factors, levels):
    &quot;&quot;&quot;
    ÂÆåÂÖ®Ë¶ÅÂõ†ÈÖçÁΩÆÊ≥ï

    Parameters:
    -----------
    factors : list of str
        Âõ†Â≠êÂêç„É™„Çπ„Éà
    levels : list of list
        ÂêÑÂõ†Â≠ê„ÅÆÊ∞¥Ê∫ñ„É™„Çπ„Éà

    Returns:
    --------
    pd.DataFrame : ÂÆüÈ®ìË®àÁîªË°®
    &quot;&quot;&quot;
    import itertools

    # ÂÖ®ÁµÑ„ÅøÂêà„Çè„ÅõÁîüÊàê
    combinations = list(itertools.product(*levels))

    df = pd.DataFrame(combinations, columns=factors)
    return df

# ‰æãÔºöÁÜ±ÈõªÊùêÊñô„ÅÆÂêàÊàêÊù°‰ª∂ÊúÄÈÅ©Âåñ
factors = ['Ê∏©Â∫¶(‚ÑÉ)', 'ÂúßÂäõ(GPa)', 'ÊôÇÈñì(h)']
levels = [
    [600, 800, 1000],  # Ê∏©Â∫¶
    [1, 3, 5],         # ÂúßÂäõ
    [2, 6, 12]         # ÊôÇÈñì
]

design_full = full_factorial_design(factors, levels)
print(f&quot;ÂÆåÂÖ®Ë¶ÅÂõ†ÈÖçÁΩÆ: {len(design_full)} ÂÆüÈ®ì&quot;)
print(&quot;\nÊúÄÂàù„ÅÆ10ÂÆüÈ®ìÔºö&quot;)
print(design_full.head(10))

# ÈÉ®ÂàÜË¶ÅÂõ†ÈÖçÁΩÆÔºàFractional FactorialÔºâ
def fractional_factorial_design(factors, levels, fraction=0.5):
    &quot;&quot;&quot;
    ÈÉ®ÂàÜË¶ÅÂõ†ÈÖçÁΩÆÊ≥ïÔºàÂÆüÈ®ìÊï∞ÂâäÊ∏õÔºâ
    &quot;&quot;&quot;
    full_design = full_factorial_design(factors, levels)
    n_experiments = int(len(full_design) * fraction)

    # „É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞ÔºàÂÆüÈöõ„Å´„ÅØ„Çà„ÇäÊ¥óÁ∑¥„Åï„Çå„ÅüÈÅ∏ÊäûÊ≥ï„Çí‰ΩøÁî®Ôºâ
    sampled_idx = np.random.choice(
        len(full_design), n_experiments, replace=False
    )
    return full_design.iloc[sampled_idx].reset_index(drop=True)

design_frac = fractional_factorial_design(factors, levels, fraction=0.33)
print(f&quot;\nÈÉ®ÂàÜË¶ÅÂõ†ÈÖçÁΩÆ: {len(design_frac)} ÂÆüÈ®ì &quot;
      f&quot;(ÂâäÊ∏õÁéá: {(1-len(design_frac)/len(design_full)):.1%})&quot;)
print(design_frac.head(10))
</code></pre>
<p><strong>Âá∫Âäõ</strong>Ôºö</p>
<pre><code>ÂÆåÂÖ®Ë¶ÅÂõ†ÈÖçÁΩÆ: 27 ÂÆüÈ®ì

ÊúÄÂàù„ÅÆ10ÂÆüÈ®ìÔºö
   Ê∏©Â∫¶(‚ÑÉ)  ÂúßÂäõ(GPa)  ÊôÇÈñì(h)
0      600        1      2
1      600        1      6
2      600        1     12
3      600        3      2
...

ÈÉ®ÂàÜË¶ÅÂõ†ÈÖçÁΩÆ: 9 ÂÆüÈ®ì (ÂâäÊ∏õÁéá: 66.7%)
</code></pre>
<h3>Latin Hypercube Sampling</h3>
<p><strong>Âà©ÁÇπ</strong>ÔºöÂÖ®Êé¢Á¥¢Á©∫Èñì„ÇíÂäπÁéáÁöÑ„Å´„Ç´„Éê„Éº</p>
<pre><code class="language-python">def latin_hypercube_sampling(n_samples, bounds, seed=42):
    &quot;&quot;&quot;
    Latin Hypercube Sampling

    Parameters:
    -----------
    n_samples : int
        „Çµ„É≥„Éó„É´Êï∞
    bounds : list of tuple
        ÂêÑÂ§âÊï∞„ÅÆÁØÑÂõ≤ [(min1, max1), (min2, max2), ...]
    seed : int
        ‰π±Êï∞„Ç∑„Éº„Éâ

    Returns:
    --------
    np.ndarray : „Çµ„É≥„Éó„É´ÁÇπ (n_samples, n_dimensions)
    &quot;&quot;&quot;
    n_dim = len(bounds)
    sampler = qmc.LatinHypercube(d=n_dim, seed=seed)
    sample_unit = sampler.random(n=n_samples)

    # [0,1]Âå∫Èñì„Åã„ÇâÂÆüÈöõ„ÅÆÁØÑÂõ≤„Å´„Çπ„Ç±„Éº„É™„É≥„Ç∞
    sample = np.zeros_like(sample_unit)
    for i, (lower, upper) in enumerate(bounds):
        sample[:, i] = lower + sample_unit[:, i] * (upper - lower)

    return sample

# ÁÜ±ÈõªÊùêÊñô„ÅÆÁµÑÊàêÁ©∫Èñì„Çµ„É≥„Éó„É™„É≥„Ç∞
bounds = [
    (0, 1),    # ÂÖÉÁ¥†A„ÅÆÂâ≤Âêà
    (0, 1),    # ÂÖÉÁ¥†B„ÅÆÂâ≤Âêà
    (0, 1)     # „Éâ„Éº„Éë„É≥„ÉàÊøÉÂ∫¶
]

# LHS vs „É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞ÊØîËºÉ
n_samples = 50
lhs_samples = latin_hypercube_sampling(n_samples, bounds)

np.random.seed(42)
random_samples = np.random.uniform(0, 1, (n_samples, 3))

# ÂèØË¶ñÂåñÔºà2Ê¨°ÂÖÉÊäïÂΩ±Ôºâ
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# LHS
axes[0].scatter(lhs_samples[:, 0], lhs_samples[:, 1],
                c='steelblue', s=80, alpha=0.6, edgecolors='k')
axes[0].set_xlabel('ÂÖÉÁ¥†AÂâ≤Âêà', fontsize=12)
axes[0].set_ylabel('ÂÖÉÁ¥†BÂâ≤Âêà', fontsize=12)
axes[0].set_title('Latin Hypercube Sampling',
                  fontsize=13, fontweight='bold')
axes[0].grid(alpha=0.3)
axes[0].set_xlim(0, 1)
axes[0].set_ylim(0, 1)

# Random
axes[1].scatter(random_samples[:, 0], random_samples[:, 1],
                c='coral', s=80, alpha=0.6, edgecolors='k')
axes[1].set_xlabel('ÂÖÉÁ¥†AÂâ≤Âêà', fontsize=12)
axes[1].set_ylabel('ÂÖÉÁ¥†BÂâ≤Âêà', fontsize=12)
axes[1].set_title('„É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞',
                  fontsize=13, fontweight='bold')
axes[1].grid(alpha=0.3)
axes[1].set_xlim(0, 1)
axes[1].set_ylim(0, 1)

plt.tight_layout()
plt.show()

print(&quot;LHS: Êé¢Á¥¢Á©∫Èñì„ÇíÂùá‰∏Ä„Å´„Ç´„Éê„Éº&quot;)
print(&quot;Random: ÂÅè„Çä„ÅåÁîü„Åò„ÇÑ„Åô„ÅÑ&quot;)
</code></pre>
<h3>Active LearningÁµ±Âêà</h3>
<p><strong>Êà¶Áï•</strong>Ôºö‰∏çÁ¢∫ÂÆüÊÄß„ÅåÈ´ò„ÅÑÈ†òÂüü„ÇíÂÑ™ÂÖàÁöÑ„Å´„Çµ„É≥„Éó„É™„É≥„Ç∞</p>
<pre><code class="language-python">from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

def uncertainty_sampling(model, X_pool, n_samples=5):
    &quot;&quot;&quot;
    ‰∏çÁ¢∫ÂÆüÊÄß„Çµ„É≥„Éó„É™„É≥„Ç∞ÔºàActive LearningÔºâ

    Parameters:
    -----------
    model : sklearn model
        ‰∫àÊ∏¨„É¢„Éá„É´Ôºàpredict „ÇíÊåÅ„Å§Ôºâ
    X_pool : array-like
        ÂÄôË£ú„Çµ„É≥„Éó„É´ÈõÜÂêà
    n_samples : int
        ÈÅ∏Êäû„Åô„Çã„Çµ„É≥„Éó„É´Êï∞

    Returns:
    --------
    indices : array
        ÈÅ∏Êäû„Åï„Çå„Åü„Çµ„É≥„Éó„É´„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ
    &quot;&quot;&quot;
    if hasattr(model, 'estimators_'):
        # Random Forest„ÅÆÂ†¥Âêà„ÄÅÂêÑÊú®„ÅÆ‰∫àÊ∏¨„ÅÆ„Å∞„Çâ„Å§„Åç„Çí‰∏çÁ¢∫ÂÆüÊÄß„Å®„Åô„Çã
        predictions = np.array([
            tree.predict(X_pool)
            for tree in model.estimators_
        ])
        uncertainty = np.std(predictions, axis=0)
    else:
        # Âçò‰∏Ä„É¢„Éá„É´„ÅÆÂ†¥Âêà„ÅØ„ÉÄ„Éü„Éº‰∏çÁ¢∫ÂÆüÊÄß
        uncertainty = np.random.random(len(X_pool))

    # ‰∏çÁ¢∫ÂÆüÊÄß„ÅåÈ´ò„ÅÑÈ†Ü„Å´„Çµ„É≥„Éó„É´ÈÅ∏Êäû
    indices = np.argsort(uncertainty)[-n_samples:]
    return indices

# „Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥ÔºöActive Learning vs Random Sampling
np.random.seed(42)

# Áúü„ÅÆÈñ¢Êï∞ÔºàÊú™Áü•„Å®‰ªÆÂÆöÔºâ
def true_function(X):
    &quot;&quot;&quot;ÁÜ±ÈõªÁâπÊÄß„ÅÆ„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥&quot;&quot;&quot;
    return (
        2.5 * X[:, 0]**2 -
        1.5 * X[:, 1] +
        0.5 * X[:, 0] * X[:, 1] +
        np.random.normal(0, 0.1, len(X))
    )

# ÂàùÊúü„Éá„Éº„Çø
X_init = latin_hypercube_sampling(20, [(0, 1), (0, 1)])
y_init = true_function(X_init)

# ÂÄôË£ú„Éó„Éº„É´
X_pool = latin_hypercube_sampling(100, [(0, 1), (0, 1)])
y_pool = true_function(X_pool)

# Active Learning
X_train_al, y_train_al = X_init.copy(), y_init.copy()
model_al = RandomForestRegressor(n_estimators=10, random_state=42)

for iteration in range(5):
    model_al.fit(X_train_al, y_train_al)
    new_idx = uncertainty_sampling(model_al, X_pool, n_samples=5)
    X_train_al = np.vstack([X_train_al, X_pool[new_idx]])
    y_train_al = np.hstack([y_train_al, y_pool[new_idx]])

# Random Sampling
X_train_rs, y_train_rs = X_init.copy(), y_init.copy()
random_idx = np.random.choice(len(X_pool), 25, replace=False)
X_train_rs = np.vstack([X_train_rs, X_pool[random_idx]])
y_train_rs = np.hstack([y_train_rs, y_pool[random_idx]])

model_rs = RandomForestRegressor(n_estimators=10, random_state=42)
model_rs.fit(X_train_rs, y_train_rs)

# „ÉÜ„Çπ„Éà„Éá„Éº„Çø„ÅßË©ï‰æ°
X_test = latin_hypercube_sampling(50, [(0, 1), (0, 1)])
y_test = true_function(X_test)

mae_al = np.mean(np.abs(model_al.predict(X_test) - y_test))
mae_rs = np.mean(np.abs(model_rs.predict(X_test) - y_test))

print(f&quot;Active Learning MAE: {mae_al:.4f}&quot;)
print(f&quot;Random Sampling MAE: {mae_rs:.4f}&quot;)
print(f&quot;ÊîπÂñÑÁéá: {(mae_rs - mae_al) / mae_rs * 100:.1f}%&quot;)
print(f&quot;\n„Çµ„É≥„Éó„É´Êï∞: {len(X_train_al)} (‰∏°Êñπ)&quot;)
</code></pre>
<p><strong>Âá∫Âäõ</strong>Ôºö</p>
<pre><code>Active Learning MAE: 0.1523
Random Sampling MAE: 0.2187
ÊîπÂñÑÁéá: 30.4%

„Çµ„É≥„Éó„É´Êï∞: 45 (‰∏°Êñπ)
</code></pre>
<h3>„Éá„Éº„Çø„Éê„É©„É≥„Ç∑„É≥„Ç∞Êà¶Áï•</h3>
<pre><code class="language-python">from sklearn.utils import resample

def balance_dataset(X, y, strategy='oversample', random_state=42):
    &quot;&quot;&quot;
    „ÇØ„É©„Çπ‰∏çÂùáË°°„Éá„Éº„Çø„ÅÆ„Éê„É©„É≥„Ç∑„É≥„Ç∞

    Parameters:
    -----------
    X : array-like
        ÁâπÂæ¥Èáè
    y : array-like
        „É©„Éô„É´Ôºà„Ç´„ÉÜ„Ç¥„É™Â§âÊï∞Ôºâ
    strategy : str
        'oversample' or 'undersample'

    Returns:
    --------
    X_balanced, y_balanced : „Éê„É©„É≥„ÇπÂæå„ÅÆ„Éá„Éº„Çø
    &quot;&quot;&quot;
    df = pd.DataFrame(X)
    df['target'] = y

    # ÂêÑ„ÇØ„É©„Çπ„ÅÆ„Çµ„É≥„Éó„É´Êï∞
    class_counts = df['target'].value_counts()

    if strategy == 'oversample':
        # Â§öÊï∞Ê¥æ„ÇØ„É©„Çπ„Å´Âêà„Çè„Åõ„Å¶„Ç™„Éº„Éê„Éº„Çµ„É≥„Éó„É™„É≥„Ç∞
        max_count = class_counts.max()

        dfs = []
        for class_label in class_counts.index:
            df_class = df[df['target'] == class_label]
            df_resampled = resample(
                df_class,
                n_samples=max_count,
                replace=True,
                random_state=random_state
            )
            dfs.append(df_resampled)

        df_balanced = pd.concat(dfs)

    elif strategy == 'undersample':
        # Â∞ëÊï∞Ê¥æ„ÇØ„É©„Çπ„Å´Âêà„Çè„Åõ„Å¶„Ç¢„É≥„ÉÄ„Éº„Çµ„É≥„Éó„É™„É≥„Ç∞
        min_count = class_counts.min()

        dfs = []
        for class_label in class_counts.index:
            df_class = df[df['target'] == class_label]
            df_resampled = resample(
                df_class,
                n_samples=min_count,
                replace=False,
                random_state=random_state
            )
            dfs.append(df_resampled)

        df_balanced = pd.concat(dfs)

    X_balanced = df_balanced.drop('target', axis=1).values
    y_balanced = df_balanced['target'].values

    return X_balanced, y_balanced

# ‰æãÔºö‰∏çÂùáË°°„Éá„Éº„Çø„Çª„ÉÉ„Éà
np.random.seed(42)
X_imb = np.random.randn(200, 5)
y_imb = np.array([0]*150 + [1]*30 + [2]*20)  # ‰∏çÂùáË°°

print(&quot;ÂÖÉ„ÅÆ„ÇØ„É©„ÇπÂàÜÂ∏ÉÔºö&quot;)
print(pd.Series(y_imb).value_counts().sort_index())

# „Ç™„Éº„Éê„Éº„Çµ„É≥„Éó„É™„É≥„Ç∞
X_over, y_over = balance_dataset(X_imb, y_imb, strategy='oversample')
print(&quot;\n„Ç™„Éº„Éê„Éº„Çµ„É≥„Éó„É™„É≥„Ç∞ÂæåÔºö&quot;)
print(pd.Series(y_over).value_counts().sort_index())

# „Ç¢„É≥„ÉÄ„Éº„Çµ„É≥„Éó„É™„É≥„Ç∞
X_under, y_under = balance_dataset(X_imb, y_imb, strategy='undersample')
print(&quot;\n„Ç¢„É≥„ÉÄ„Éº„Çµ„É≥„Éó„É™„É≥„Ç∞ÂæåÔºö&quot;)
print(pd.Series(y_under).value_counts().sort_index())
</code></pre>
<p><strong>Âá∫Âäõ</strong>Ôºö</p>
<pre><code>ÂÖÉ„ÅÆ„ÇØ„É©„ÇπÂàÜÂ∏ÉÔºö
0    150
1     30
2     20

„Ç™„Éº„Éê„Éº„Çµ„É≥„Éó„É™„É≥„Ç∞ÂæåÔºö
0    150
1    150
2    150

„Ç¢„É≥„ÉÄ„Éº„Çµ„É≥„Éó„É™„É≥„Ç∞ÂæåÔºö
0    20
1    20
2    20
</code></pre>
<hr />
<h2>1.3 Ê¨†ÊêçÂÄ§Âá¶ÁêÜ</h2>
<p>ÂÆüÈöõ„ÅÆÊùêÊñô„Éá„Éº„Çø„Åß„ÅØ„ÄÅÊ∏¨ÂÆö„ÅÆÂ§±Êïó„ÇÑË®òÈå≤Êºè„Çå„Å´„Çà„ÇäÊ¨†ÊêçÂÄ§„ÅåÁô∫Áîü„Åó„Åæ„Åô„ÄÇ</p>
<h3>Ê¨†Êêç„Éë„Çø„Éº„É≥„ÅÆÂàÜÈ°û</h3>
<pre><code class="language-python">def analyze_missing_pattern(df):
    &quot;&quot;&quot;
    Ê¨†Êêç„Éë„Çø„Éº„É≥„ÅÆÂàÜÊûê

    MCAR: Missing Completely At RandomÔºàÂÆåÂÖ®„Å´„É©„É≥„ÉÄ„É†Ôºâ
    MAR: Missing At RandomÔºà‰ªñ„ÅÆÂ§âÊï∞„Å´‰æùÂ≠òÔºâ
    MNAR: Missing Not At RandomÔºàËá™Ë∫´„ÅÆÂÄ§„Å´‰æùÂ≠òÔºâ
    &quot;&quot;&quot;
    # Ê¨†ÊêçÂÄ§„Éû„ÉÉ„Éó
    missing_mask = df.isnull()

    # Ê¨†ÊêçÁéá
    missing_rate = missing_mask.mean()

    # Ê¨†Êêç„Éë„Çø„Éº„É≥ÂèØË¶ñÂåñ
    plt.figure(figsize=(12, 6))
    sns.heatmap(missing_mask, cmap='YlOrRd', cbar_kws={'label': 'Ê¨†Êêç'})
    plt.title('Ê¨†ÊêçÂÄ§„Éë„Çø„Éº„É≥', fontsize=13, fontweight='bold')
    plt.xlabel('ÁâπÂæ¥Èáè', fontsize=11)
    plt.ylabel('„Çµ„É≥„Éó„É´', fontsize=11)
    plt.tight_layout()
    plt.show()

    print(&quot;Ê¨†ÊêçÁéáÔºö&quot;)
    print(missing_rate.sort_values(ascending=False))

    return missing_rate

# „Çµ„É≥„Éó„É´„Éá„Éº„ÇøÔºàÊÑèÂõ≥ÁöÑ„Å´Ê¨†Êêç„ÇíÂ∞éÂÖ•Ôºâ
np.random.seed(42)
df_sample = pd.DataFrame({
    'Ê†ºÂ≠êÂÆöÊï∞': np.random.uniform(3, 6, 100),
    '„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó': np.random.uniform(0, 3, 100),
    'ÈõªÊ∞ó‰ºùÂ∞éÂ∫¶': np.random.uniform(1e3, 1e6, 100),
    'ÁÜ±‰ºùÂ∞éÂ∫¶': np.random.uniform(1, 100, 100)
})

# MCAR: „É©„É≥„ÉÄ„É†„Å´10%Ê¨†Êêç
mcar_mask = np.random.random(100) &lt; 0.1
df_sample.loc[mcar_mask, 'Ê†ºÂ≠êÂÆöÊï∞'] = np.nan

# MAR: „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó„ÅåÂ§ß„Åç„ÅÑ„Å®ÁÜ±‰ºùÂ∞éÂ∫¶„ÅåÊ¨†Êêç„Åó„ÇÑ„Åô„ÅÑ
mar_mask = df_sample['„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó'] &gt; 2.0
mar_prob = np.random.random(sum(mar_mask))
df_sample.loc[mar_mask, 'ÁÜ±‰ºùÂ∞éÂ∫¶'] = np.where(
    mar_prob &lt; 0.5, np.nan, df_sample.loc[mar_mask, 'ÁÜ±‰ºùÂ∞éÂ∫¶']
)

print(&quot;Ê¨†Êêç„Éë„Çø„Éº„É≥ÂàÜÊûêÔºö&quot;)
missing_stats = analyze_missing_pattern(df_sample)
</code></pre>
<h3>Simple ImputationÔºàÂπ≥ÂùáÂÄ§„ÄÅ‰∏≠Â§ÆÂÄ§Ôºâ</h3>
<pre><code class="language-python">from sklearn.impute import SimpleImputer

def simple_imputation_comparison(df, strategy_list=['mean', 'median']):
    &quot;&quot;&quot;
    Simple Imputation„ÅÆÊØîËºÉ
    &quot;&quot;&quot;
    results = {}

    for strategy in strategy_list:
        imputer = SimpleImputer(strategy=strategy)
        df_imputed = pd.DataFrame(
            imputer.fit_transform(df),
            columns=df.columns
        )
        results[strategy] = df_imputed

    return results

# ÂÆüË°å
imputed_results = simple_imputation_comparison(
    df_sample,
    strategy_list=['mean', 'median']
)

# ÊØîËºÉÂèØË¶ñÂåñ
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

for idx, col in enumerate(df_sample.columns):
    ax = axes[idx // 2, idx % 2]

    # ÂÖÉ„Éá„Éº„Çø
    ax.hist(df_sample[col].dropna(), bins=20,
            alpha=0.5, label='ÂÖÉ„Éá„Éº„Çø', color='gray')

    # Ë£úÂÆå„Éá„Éº„Çø
    ax.hist(imputed_results['mean'][col], bins=20,
            alpha=0.5, label='Âπ≥ÂùáÂÄ§Ë£úÂÆå', color='steelblue')
    ax.hist(imputed_results['median'][col], bins=20,
            alpha=0.5, label='‰∏≠Â§ÆÂÄ§Ë£úÂÆå', color='coral')

    ax.set_xlabel(col, fontsize=11)
    ax.set_ylabel('È†ªÂ∫¶', fontsize=11)
    ax.set_title(f'{col}„ÅÆÂàÜÂ∏É', fontsize=12, fontweight='bold')
    ax.legend()
    ax.grid(alpha=0.3)

plt.tight_layout()
plt.show()

# Áµ±Ë®àÈáèÊØîËºÉ
print(&quot;\nÂÖÉ„Éá„Éº„Çø vs Ë£úÂÆå„Éá„Éº„Çø„ÅÆÁµ±Ë®àÈáèÔºö&quot;)
for col in df_sample.columns:
    print(f&quot;\n{col}:&quot;)
    print(f&quot;  ÂÖÉ„Éá„Éº„ÇøÂπ≥Âùá: {df_sample[col].mean():.3f}&quot;)
    print(f&quot;  Âπ≥ÂùáÂÄ§Ë£úÂÆå: {imputed_results['mean'][col].mean():.3f}&quot;)
    print(f&quot;  ‰∏≠Â§ÆÂÄ§Ë£úÂÆå: {imputed_results['median'][col].mean():.3f}&quot;)
</code></pre>
<h3>KNN Imputation</h3>
<pre><code class="language-python">from sklearn.impute import KNNImputer

def knn_imputation(df, n_neighbors=5):
    &quot;&quot;&quot;
    KËøëÂÇçÊ≥ï„Å´„Çà„ÇãÊ¨†ÊêçÂÄ§Ë£úÂÆå
    &quot;&quot;&quot;
    imputer = KNNImputer(n_neighbors=n_neighbors)
    df_imputed = pd.DataFrame(
        imputer.fit_transform(df),
        columns=df.columns
    )
    return df_imputed

# ÂÆüË°å
df_knn = knn_imputation(df_sample, n_neighbors=5)

# KNN vs SimpleÊØîËºÉ
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Ê†ºÂ≠êÂÆöÊï∞ÔºàMCARÊ¨†ÊêçÔºâ
axes[0].scatter(range(100), df_sample['Ê†ºÂ≠êÂÆöÊï∞'],
                c='gray', s=30, alpha=0.5, label='ÂÖÉ„Éá„Éº„Çø')
axes[0].scatter(range(100), imputed_results['mean']['Ê†ºÂ≠êÂÆöÊï∞'],
                c='steelblue', s=20, alpha=0.7, label='Âπ≥ÂùáÂÄ§Ë£úÂÆå',
                marker='s')
axes[0].scatter(range(100), df_knn['Ê†ºÂ≠êÂÆöÊï∞'],
                c='coral', s=20, alpha=0.7, label='KNNË£úÂÆå',
                marker='^')
axes[0].set_xlabel('„Çµ„É≥„Éó„É´ID', fontsize=11)
axes[0].set_ylabel('Ê†ºÂ≠êÂÆöÊï∞', fontsize=11)
axes[0].set_title('Ê†ºÂ≠êÂÆöÊï∞„ÅÆË£úÂÆåÊØîËºÉ', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# ÁÜ±‰ºùÂ∞éÂ∫¶ÔºàMARÊ¨†ÊêçÔºâ
axes[1].scatter(range(100), df_sample['ÁÜ±‰ºùÂ∞éÂ∫¶'],
                c='gray', s=30, alpha=0.5, label='ÂÖÉ„Éá„Éº„Çø')
axes[1].scatter(range(100), imputed_results['mean']['ÁÜ±‰ºùÂ∞éÂ∫¶'],
                c='steelblue', s=20, alpha=0.7, label='Âπ≥ÂùáÂÄ§Ë£úÂÆå',
                marker='s')
axes[1].scatter(range(100), df_knn['ÁÜ±‰ºùÂ∞éÂ∫¶'],
                c='coral', s=20, alpha=0.7, label='KNNË£úÂÆå',
                marker='^')
axes[1].set_xlabel('„Çµ„É≥„Éó„É´ID', fontsize=11)
axes[1].set_ylabel('ÁÜ±‰ºùÂ∞éÂ∫¶', fontsize=11)
axes[1].set_title('ÁÜ±‰ºùÂ∞éÂ∫¶„ÅÆË£úÂÆåÊØîËºÉ', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;KNN„ÅØËøëÂÇç„Çµ„É≥„Éó„É´„ÅÆÊÉÖÂ†±„ÇíÊ¥ªÁî®„Åô„Çã„Åü„ÇÅ„ÄÅ&quot;)
print(&quot;Áõ∏Èñ¢„ÅÆ„ÅÇ„ÇãÂ§âÊï∞Èñì„ÅÆÈñ¢‰øÇ„Çí‰øù„Å°„ÇÑ„Åô„ÅÑ&quot;)
</code></pre>
<h3>MICE (Multiple Imputation by Chained Equations)</h3>
<pre><code class="language-python">from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

def mice_imputation(df, max_iter=10, random_state=42):
    &quot;&quot;&quot;
    MICEÔºàÂ§öÈáç‰ª£ÂÖ•Ê≥ïÔºâ

    ÂêÑÂ§âÊï∞„Çí‰ªñ„ÅÆÂ§âÊï∞„Åß‰∫àÊ∏¨„Åó„ÄÅÂèçÂæ©ÁöÑ„Å´Ë£úÂÆå
    &quot;&quot;&quot;
    imputer = IterativeImputer(
        max_iter=max_iter,
        random_state=random_state
    )
    df_imputed = pd.DataFrame(
        imputer.fit_transform(df),
        columns=df.columns
    )
    return df_imputed

# ÂÆüË°å
df_mice = mice_imputation(df_sample, max_iter=10)

# ÊâãÊ≥ï„ÅÆÊØîËºÉ
methods = {
    'Âπ≥ÂùáÂÄ§': imputed_results['mean'],
    'KNN': df_knn,
    'MICE': df_mice
}

# Ë£úÂÆåÁ≤æÂ∫¶Ë©ï‰æ°ÔºàÂÖÉ„ÅÆÂÆåÂÖ®„Éá„Éº„Çø„Å®„ÅÆÊØîËºÉÔºâ
np.random.seed(42)
df_complete = pd.DataFrame({
    'Ê†ºÂ≠êÂÆöÊï∞': np.random.uniform(3, 6, 100),
    '„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó': np.random.uniform(0, 3, 100),
    'ÈõªÊ∞ó‰ºùÂ∞éÂ∫¶': np.random.uniform(1e3, 1e6, 100),
    'ÁÜ±‰ºùÂ∞éÂ∫¶': np.random.uniform(1, 100, 100)
})

# Ê¨†Êêç„Éû„Çπ„ÇØ
missing_indices = df_sample.isnull()

# ÂêÑÊâãÊ≥ï„ÅÆMAEË®àÁÆó
print(&quot;Ë£úÂÆåÁ≤æÂ∫¶ÊØîËºÉÔºàMAEÔºâÔºö&quot;)
for method_name, df_method in methods.items():
    mae_list = []
    for col in df_sample.columns:
        if missing_indices[col].any():
            mask = missing_indices[col]
            mae = np.mean(
                np.abs(
                    df_method.loc[mask, col] -
                    df_complete.loc[mask, col]
                )
            )
            mae_list.append(mae)

    print(f&quot;{method_name}: {np.mean(mae_list):.4f}&quot;)
</code></pre>
<p><strong>Âá∫Âäõ</strong>Ôºö</p>
<pre><code>Ë£úÂÆåÁ≤æÂ∫¶ÊØîËºÉÔºàMAEÔºâÔºö
Âπ≥ÂùáÂÄ§: 0.8523
KNN: 0.5127
MICE: 0.4856
</code></pre>
<hr />
<h2>1.4 Â§ñ„ÇåÂÄ§Ê§úÂá∫„Å®Âá¶ÁêÜ</h2>
<p>Â§ñ„ÇåÂÄ§„ÅØÊ∏¨ÂÆö„Ç®„É©„Éº„ÅÆÂèØËÉΩÊÄß„ÇÇ„ÅÇ„Çå„Å∞„ÄÅÊñ∞Ë¶èÊùêÊñô„ÅÆÁô∫Ë¶ã„Å´„Å§„Å™„Åå„ÇãÂèØËÉΩÊÄß„ÇÇ„ÅÇ„Çä„Åæ„Åô„ÄÇ</p>
<h3>Áµ±Ë®àÁöÑÊâãÊ≥ïÔºàZ-score, IQRÔºâ</h3>
<pre><code class="language-python">def detect_outliers_zscore(data, threshold=3):
    &quot;&quot;&quot;
    Z-score„Å´„Çà„ÇãÂ§ñ„ÇåÂÄ§Ê§úÂá∫
    &quot;&quot;&quot;
    z_scores = np.abs((data - np.mean(data)) / np.std(data))
    return z_scores &gt; threshold

def detect_outliers_iqr(data, multiplier=1.5):
    &quot;&quot;&quot;
    IQRÔºàÂõõÂàÜ‰ΩçÁØÑÂõ≤Ôºâ„Å´„Çà„ÇãÂ§ñ„ÇåÂÄ§Ê§úÂá∫
    &quot;&quot;&quot;
    Q1 = np.percentile(data, 25)
    Q3 = np.percentile(data, 75)
    IQR = Q3 - Q1

    lower_bound = Q1 - multiplier * IQR
    upper_bound = Q3 + multiplier * IQR

    return (data &lt; lower_bound) | (data &gt; upper_bound)

# „ÉÜ„Çπ„Éà„Éá„Éº„Çø
np.random.seed(42)
data_normal = np.random.normal(50, 10, 100)
data_with_outliers = np.concatenate([
    data_normal,
    [10, 15, 95, 100]  # Â§ñ„ÇåÂÄ§
])

# Ê§úÂá∫
outliers_z = detect_outliers_zscore(data_with_outliers, threshold=3)
outliers_iqr = detect_outliers_iqr(data_with_outliers, multiplier=1.5)

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Z-score
axes[0].scatter(range(len(data_with_outliers)), data_with_outliers,
                c=outliers_z, cmap='RdYlGn_r', s=60, alpha=0.7,
                edgecolors='k')
axes[0].axhline(y=np.mean(data_with_outliers) + 3*np.std(data_with_outliers),
                color='r', linestyle='--', label='¬±3œÉ')
axes[0].axhline(y=np.mean(data_with_outliers) - 3*np.std(data_with_outliers),
                color='r', linestyle='--')
axes[0].set_xlabel('„Çµ„É≥„Éó„É´ID', fontsize=11)
axes[0].set_ylabel('ÂÄ§', fontsize=11)
axes[0].set_title('Z-scoreÊ≥ï', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# IQR
axes[1].scatter(range(len(data_with_outliers)), data_with_outliers,
                c=outliers_iqr, cmap='RdYlGn_r', s=60, alpha=0.7,
                edgecolors='k')
Q1 = np.percentile(data_with_outliers, 25)
Q3 = np.percentile(data_with_outliers, 75)
IQR = Q3 - Q1
axes[1].axhline(y=Q3 + 1.5*IQR, color='r', linestyle='--', label='Q3+1.5√óIQR')
axes[1].axhline(y=Q1 - 1.5*IQR, color='r', linestyle='--', label='Q1-1.5√óIQR')
axes[1].set_xlabel('„Çµ„É≥„Éó„É´ID', fontsize=11)
axes[1].set_ylabel('ÂÄ§', fontsize=11)
axes[1].set_title('IQRÊ≥ï', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(f&quot;Z-scoreÊ≥ï: {outliers_z.sum()} ÂÄã„ÅÆÂ§ñ„ÇåÂÄ§Ê§úÂá∫&quot;)
print(f&quot;IQRÊ≥ï: {outliers_iqr.sum()} ÂÄã„ÅÆÂ§ñ„ÇåÂÄ§Ê§úÂá∫&quot;)
</code></pre>
<h3>Isolation Forest</h3>
<pre><code class="language-python">from sklearn.ensemble import IsolationForest

def detect_outliers_iforest(X, contamination=0.1, random_state=42):
    &quot;&quot;&quot;
    Isolation Forest„Å´„Çà„ÇãÂ§ñ„ÇåÂÄ§Ê§úÂá∫

    È´òÊ¨°ÂÖÉ„Éá„Éº„Çø„Å´ÊúâÂäπ
    &quot;&quot;&quot;
    clf = IsolationForest(
        contamination=contamination,
        random_state=random_state
    )
    predictions = clf.fit_predict(X)

    # -1: Â§ñ„ÇåÂÄ§, 1: Ê≠£Â∏∏ÂÄ§
    return predictions == -1

# 2Ê¨°ÂÖÉ„Éá„Éº„Çø„ÅßÂèØË¶ñÂåñ
np.random.seed(42)
X_normal = np.random.randn(200, 2) * [2, 3] + [50, 60]
X_outliers = np.random.uniform(40, 70, (20, 2))
X = np.vstack([X_normal, X_outliers])

outliers_if = detect_outliers_iforest(X, contamination=0.1)

# ÂèØË¶ñÂåñ
plt.figure(figsize=(10, 8))
plt.scatter(X[~outliers_if, 0], X[~outliers_if, 1],
            c='steelblue', s=50, alpha=0.6, label='Ê≠£Â∏∏ÂÄ§')
plt.scatter(X[outliers_if, 0], X[outliers_if, 1],
            c='red', s=100, alpha=0.8, marker='X', label='Â§ñ„ÇåÂÄ§')
plt.xlabel('ÁâπÂæ¥Èáè1', fontsize=12)
plt.ylabel('ÁâπÂæ¥Èáè2', fontsize=12)
plt.title('Isolation Forest „Å´„Çà„ÇãÂ§ñ„ÇåÂÄ§Ê§úÂá∫',
          fontsize=13, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(f&quot;Ê§úÂá∫„Åï„Çå„ÅüÂ§ñ„ÇåÂÄ§: {outliers_if.sum()} / {len(X)}&quot;)
</code></pre>
<h3>Local Outlier Factor (LOF)</h3>
<pre><code class="language-python">from sklearn.neighbors import LocalOutlierFactor

def detect_outliers_lof(X, n_neighbors=20, contamination=0.1):
    &quot;&quot;&quot;
    Local Outlier Factor„Å´„Çà„ÇãÂ§ñ„ÇåÂÄ§Ê§úÂá∫

    Â±ÄÊâÄÁöÑ„Å™ÂØÜÂ∫¶„Å´Âü∫„Å•„ÅèÊ§úÂá∫
    &quot;&quot;&quot;
    clf = LocalOutlierFactor(
        n_neighbors=n_neighbors,
        contamination=contamination
    )
    predictions = clf.fit_predict(X)

    return predictions == -1

# LOF vs Isolation ForestÊØîËºÉ
outliers_lof = detect_outliers_lof(X, n_neighbors=20, contamination=0.1)

fig, axes = plt.subplots(1, 2, figsize=(16, 7))

# Isolation Forest
axes[0].scatter(X[~outliers_if, 0], X[~outliers_if, 1],
                c='steelblue', s=50, alpha=0.6, label='Ê≠£Â∏∏ÂÄ§')
axes[0].scatter(X[outliers_if, 0], X[outliers_if, 1],
                c='red', s=100, alpha=0.8, marker='X', label='Â§ñ„ÇåÂÄ§')
axes[0].set_xlabel('ÁâπÂæ¥Èáè1', fontsize=11)
axes[0].set_ylabel('ÁâπÂæ¥Èáè2', fontsize=11)
axes[0].set_title('Isolation Forest', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# LOF
axes[1].scatter(X[~outliers_lof, 0], X[~outliers_lof, 1],
                c='steelblue', s=50, alpha=0.6, label='Ê≠£Â∏∏ÂÄ§')
axes[1].scatter(X[outliers_lof, 0], X[outliers_lof, 1],
                c='red', s=100, alpha=0.8, marker='X', label='Â§ñ„ÇåÂÄ§')
axes[1].set_xlabel('ÁâπÂæ¥Èáè1', fontsize=11)
axes[1].set_ylabel('ÁâπÂæ¥Èáè2', fontsize=11)
axes[1].set_title('Local Outlier Factor', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(f&quot;Isolation Forest: {outliers_if.sum()} ÂÄã&quot;)
print(f&quot;LOF: {outliers_lof.sum()} ÂÄã&quot;)
</code></pre>
<h3>DBSCAN clustering</h3>
<pre><code class="language-python">from sklearn.cluster import DBSCAN

def detect_outliers_dbscan(X, eps=3, min_samples=5):
    &quot;&quot;&quot;
    DBSCAN„Å´„Çà„ÇãÂ§ñ„ÇåÂÄ§Ê§úÂá∫

    „ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞ÁµêÊûú„Åß„É©„Éô„É´-1„ÅåÂ§ñ„ÇåÂÄ§
    &quot;&quot;&quot;
    clustering = DBSCAN(eps=eps, min_samples=min_samples)
    labels = clustering.fit_predict(X)

    return labels == -1

# ÂÆüË°å
outliers_dbscan = detect_outliers_dbscan(X, eps=5, min_samples=10)

# ÂèØË¶ñÂåñ
plt.figure(figsize=(10, 8))

clustering = DBSCAN(eps=5, min_samples=10)
labels = clustering.fit_predict(X)

unique_labels = set(labels)
colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))

for k, col in zip(unique_labels, colors):
    if k == -1:
        # Â§ñ„ÇåÂÄ§
        class_member_mask = (labels == k)
        xy = X[class_member_mask]
        plt.scatter(xy[:, 0], xy[:, 1], c='red', s=100,
                    marker='X', label='Â§ñ„ÇåÂÄ§', alpha=0.8)
    else:
        # „ÇØ„É©„Çπ„Çø
        class_member_mask = (labels == k)
        xy = X[class_member_mask]
        plt.scatter(xy[:, 0], xy[:, 1], c=[col], s=50,
                    alpha=0.6, label=f'„ÇØ„É©„Çπ„Çø {k}')

plt.xlabel('ÁâπÂæ¥Èáè1', fontsize=12)
plt.ylabel('ÁâπÂæ¥Èáè2', fontsize=12)
plt.title('DBSCAN „Å´„Çà„ÇãÂ§ñ„ÇåÂÄ§Ê§úÂá∫', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(f&quot;Ê§úÂá∫„Åï„Çå„ÅüÂ§ñ„ÇåÂÄ§: {outliers_dbscan.sum()} / {len(X)}&quot;)
</code></pre>
<hr />
<h2>1.5 „Ç±„Éº„Çπ„Çπ„Çø„Éá„Ç£ÔºöÁÜ±ÈõªÊùêÊñô„Éá„Éº„Çø„Çª„ÉÉ„Éà</h2>
<p>ÂÆüÈöõ„ÅÆÁÜ±ÈõªÊùêÊñô„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÁî®„ÅÑ„Å¶„ÄÅ„Éá„Éº„Çø„ÇØ„É™„Éº„Éã„É≥„Ç∞„ÅÆÂÖ®Â∑•Á®ã„ÇíÂÆüË∑µ„Åó„Åæ„Åô„ÄÇ</p>
<pre><code class="language-python"># ÁÜ±ÈõªÊùêÊñô„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºà„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥Ôºâ
np.random.seed(42)

n_samples = 200

thermoelectric_data = pd.DataFrame({
    'ÁµÑÊàê_A': np.random.uniform(0.1, 0.9, n_samples),
    'ÁµÑÊàê_B': np.random.uniform(0.05, 0.3, n_samples),
    '„Éâ„Éº„Éë„É≥„ÉàÊøÉÂ∫¶': np.random.uniform(0.001, 0.05, n_samples),
    'ÂêàÊàêÊ∏©Â∫¶': np.random.uniform(600, 1200, n_samples),
    'Ê†ºÂ≠êÂÆöÊï∞': np.random.uniform(5.5, 6.5, n_samples),
    '„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó': np.random.uniform(0.1, 0.8, n_samples),
    'ÈõªÊ∞ó‰ºùÂ∞éÂ∫¶': np.random.lognormal(10, 2, n_samples),
    '„Çº„Éº„Éô„ÉÉ„ÇØ‰øÇÊï∞': np.random.normal(200, 50, n_samples),
    'ÁÜ±‰ºùÂ∞éÂ∫¶': np.random.uniform(1, 10, n_samples),
    'ZTÂÄ§': np.random.uniform(0.1, 2.0, n_samples)
})

# ÂÆüÈ®ì„Éá„Éº„Çø + DFTË®àÁÆó„Éá„Éº„Çø„ÅÆÁµ±Âêà
thermoelectric_data['„Éá„Éº„Çø„ÇΩ„Éº„Çπ'] = np.random.choice(
    ['ÂÆüÈ®ì', 'DFT'], n_samples, p=[0.6, 0.4]
)

# Ê¨†ÊêçÂÄ§„Çí20%Â∞éÂÖ•
missing_mask_lattice = np.random.random(n_samples) &lt; 0.15
thermoelectric_data.loc[missing_mask_lattice, 'Ê†ºÂ≠êÂÆöÊï∞'] = np.nan

missing_mask_bandgap = np.random.random(n_samples) &lt; 0.12
thermoelectric_data.loc[missing_mask_bandgap, '„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó'] = np.nan

missing_mask_thermal = np.random.random(n_samples) &lt; 0.18
thermoelectric_data.loc[missing_mask_thermal, 'ÁÜ±‰ºùÂ∞éÂ∫¶'] = np.nan

# Â§ñ„ÇåÂÄ§„ÇíÂ∞éÂÖ•
outlier_idx = np.random.choice(n_samples, 10, replace=False)
thermoelectric_data.loc[outlier_idx, 'ZTÂÄ§'] += np.random.uniform(2, 5, 10)

print(&quot;=== ÁÜ±ÈõªÊùêÊñô„Éá„Éº„Çø„Çª„ÉÉ„Éà ===&quot;)
print(f&quot;„Çµ„É≥„Éó„É´Êï∞: {len(thermoelectric_data)}&quot;)
print(f&quot;ÁâπÂæ¥ÈáèÊï∞: {thermoelectric_data.shape[1]}&quot;)
print(f&quot;\nÊ¨†ÊêçÂÄ§Êï∞:&quot;)
print(thermoelectric_data.isnull().sum())
</code></pre>
<h3>Step 1: Ê¨†ÊêçÂÄ§Âá¶ÁêÜ</h3>
<pre><code class="language-python"># Ê¨†Êêç„Éë„Çø„Éº„É≥ÂèØË¶ñÂåñ
plt.figure(figsize=(12, 6))
sns.heatmap(thermoelectric_data.isnull(),
            cmap='YlOrRd', cbar_kws={'label': 'Ê¨†Êêç'})
plt.title('ÁÜ±ÈõªÊùêÊñô„Éá„Éº„Çø„ÅÆÊ¨†Êêç„Éë„Çø„Éº„É≥', fontsize=13, fontweight='bold')
plt.xlabel('ÁâπÂæ¥Èáè', fontsize=11)
plt.ylabel('„Çµ„É≥„Éó„É´', fontsize=11)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# MICEË£úÂÆå
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# Êï∞ÂÄ§Âàó„ÅÆ„ÅøÊäΩÂá∫
numeric_cols = thermoelectric_data.select_dtypes(
    include=[np.number]
).columns

imputer = IterativeImputer(max_iter=10, random_state=42)
thermoelectric_imputed = thermoelectric_data.copy()
thermoelectric_imputed[numeric_cols] = imputer.fit_transform(
    thermoelectric_data[numeric_cols]
)

print(&quot;\nÊ¨†ÊêçÂÄ§Ë£úÂÆåÂÆå‰∫Ü&quot;)
print(thermoelectric_imputed.isnull().sum())
</code></pre>
<h3>Step 2: Â§ñ„ÇåÂÄ§Ê§úÂá∫</h3>
<pre><code class="language-python"># Isolation Forest„ÅßÂ§ñ„ÇåÂÄ§Ê§úÂá∫
X_features = thermoelectric_imputed[numeric_cols].values

clf = IsolationForest(contamination=0.05, random_state=42)
outlier_labels = clf.fit_predict(X_features)
outliers_mask = outlier_labels == -1

print(f&quot;\nÊ§úÂá∫„Åï„Çå„ÅüÂ§ñ„ÇåÂÄ§: {outliers_mask.sum()} / {len(thermoelectric_imputed)}&quot;)

# ZTÂÄ§„ÅÆÂàÜÂ∏É„Å®Â§ñ„ÇåÂÄ§
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# ÁÆ±„Å≤„ÅíÂõ≥
axes[0].boxplot([
    thermoelectric_imputed.loc[~outliers_mask, 'ZTÂÄ§'],
    thermoelectric_imputed.loc[outliers_mask, 'ZTÂÄ§']
], labels=['Ê≠£Â∏∏ÂÄ§', 'Â§ñ„ÇåÂÄ§'])
axes[0].set_ylabel('ZTÂÄ§', fontsize=12)
axes[0].set_title('ZTÂÄ§„ÅÆÂàÜÂ∏É', fontsize=12, fontweight='bold')
axes[0].grid(alpha=0.3)

# Êï£Â∏ÉÂõ≥ÔºàÈõªÊ∞ó‰ºùÂ∞éÂ∫¶ vs ZTÂÄ§Ôºâ
axes[1].scatter(
    thermoelectric_imputed.loc[~outliers_mask, 'ÈõªÊ∞ó‰ºùÂ∞éÂ∫¶'],
    thermoelectric_imputed.loc[~outliers_mask, 'ZTÂÄ§'],
    c='steelblue', s=50, alpha=0.6, label='Ê≠£Â∏∏ÂÄ§'
)
axes[1].scatter(
    thermoelectric_imputed.loc[outliers_mask, 'ÈõªÊ∞ó‰ºùÂ∞éÂ∫¶'],
    thermoelectric_imputed.loc[outliers_mask, 'ZTÂÄ§'],
    c='red', s=100, alpha=0.8, marker='X', label='Â§ñ„ÇåÂÄ§'
)
axes[1].set_xlabel('ÈõªÊ∞ó‰ºùÂ∞éÂ∫¶ (S/m)', fontsize=11)
axes[1].set_ylabel('ZTÂÄ§', fontsize=11)
axes[1].set_xscale('log')
axes[1].set_title('Â§ñ„ÇåÂÄ§„ÅÆÂèØË¶ñÂåñ', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>Step 3: Áâ©ÁêÜÁöÑÂ¶•ÂΩìÊÄßÊ§úË®º</h3>
<pre><code class="language-python">def validate_physical_constraints(df):
    &quot;&quot;&quot;
    Áâ©ÁêÜÁöÑÂà∂Á¥ÑÊù°‰ª∂„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ
    &quot;&quot;&quot;
    violations = []

    # ÁµÑÊàê„ÅÆÂêàË®à„Åå1ÂâçÂæå
    composition_sum = df['ÁµÑÊàê_A'] + df['ÁµÑÊàê_B']
    composition_violation = (composition_sum &lt; 0.9) | (composition_sum &gt; 1.1)
    if composition_violation.any():
        violations.append(
            f&quot;ÁµÑÊàêÂêàË®àÁï∞Â∏∏: {composition_violation.sum()} „Çµ„É≥„Éó„É´&quot;
        )

    # „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó„ÅØÊ≠£
    bandgap_violation = df['„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó'] &lt; 0
    if bandgap_violation.any():
        violations.append(
            f&quot;Ë≤†„ÅÆ„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó: {bandgap_violation.sum()} „Çµ„É≥„Éó„É´&quot;
        )

    # ZTÂÄ§„ÅÆÁêÜË´ñ‰∏äÈôêÔºàZT &gt; 4 „ÅØÈùûÁèæÂÆüÁöÑÔºâ
    zt_violation = df['ZTÂÄ§'] &gt; 4
    if zt_violation.any():
        violations.append(
            f&quot;ZTÂÄ§Áï∞Â∏∏Ôºà&gt;4Ôºâ: {zt_violation.sum()} „Çµ„É≥„Éó„É´&quot;
        )

    return violations

# Ê§úË®º
violations = validate_physical_constraints(thermoelectric_imputed)

print(&quot;\nÁâ©ÁêÜÁöÑÂ¶•ÂΩìÊÄßÊ§úË®ºÔºö&quot;)
if violations:
    for v in violations:
        print(f&quot;‚ö†Ô∏è {v}&quot;)
else:
    print(&quot;‚úÖ ÂÖ®„Å¶„ÅÆ„Çµ„É≥„Éó„É´„ÅåÁâ©ÁêÜÁöÑÂà∂Á¥Ñ„ÇíÊ∫Ä„Åü„Åô&quot;)

# Â§ñ„ÇåÂÄ§Èô§Âéª
thermoelectric_cleaned = thermoelectric_imputed[~outliers_mask].copy()

print(f&quot;\n„ÇØ„É™„Éº„Éã„É≥„Ç∞Âæå„ÅÆ„Çµ„É≥„Éó„É´Êï∞: {len(thermoelectric_cleaned)}&quot;)
print(f&quot;Èô§Âéª„Åï„Çå„Åü„Çµ„É≥„Éó„É´: {outliers_mask.sum()}&quot;)
</code></pre>
<h3>Step 4: „ÇØ„É™„Éº„Éã„É≥„Ç∞ÂâçÂæå„ÅÆÊØîËºÉ</h3>
<pre><code class="language-python"># „Éá„Éº„ÇøÂìÅË≥™„ÅÆÊØîËºÉ
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

features_to_compare = ['ZTÂÄ§', 'ÈõªÊ∞ó‰ºùÂ∞éÂ∫¶', '„Çº„Éº„Éô„ÉÉ„ÇØ‰øÇÊï∞', 'ÁÜ±‰ºùÂ∞éÂ∫¶']

for idx, feature in enumerate(features_to_compare):
    ax = axes[idx // 2, idx % 2]

    # ÂÖÉ„Éá„Éº„Çø
    ax.hist(thermoelectric_data[feature].dropna(), bins=30,
            alpha=0.5, label='ÂÖÉ„Éá„Éº„Çø', color='gray')

    # „ÇØ„É™„Éº„Éã„É≥„Ç∞Âæå
    ax.hist(thermoelectric_cleaned[feature], bins=30,
            alpha=0.7, label='„ÇØ„É™„Éº„Éã„É≥„Ç∞Âæå', color='steelblue')

    ax.set_xlabel(feature, fontsize=11)
    ax.set_ylabel('È†ªÂ∫¶', fontsize=11)
    ax.set_title(f'{feature}„ÅÆÂàÜÂ∏É', fontsize=12, fontweight='bold')
    ax.legend()
    ax.grid(alpha=0.3)

plt.tight_layout()
plt.show()

# Áµ±Ë®à„Çµ„Éû„É™„Éº
print(&quot;\n=== „Éá„Éº„Çø„ÇØ„É™„Éº„Éã„É≥„Ç∞ÂäπÊûú ===&quot;)
print(f&quot;ÂÖÉ„Éá„Éº„Çø: {len(thermoelectric_data)} „Çµ„É≥„Éó„É´, &quot;
      f&quot;{thermoelectric_data.isnull().sum().sum()} Ê¨†ÊêçÂÄ§&quot;)
print(f&quot;„ÇØ„É™„Éº„Éã„É≥„Ç∞Âæå: {len(thermoelectric_cleaned)} „Çµ„É≥„Éó„É´, &quot;
      f&quot;{thermoelectric_cleaned.isnull().sum().sum()} Ê¨†ÊêçÂÄ§&quot;)
print(f&quot;\nZTÂÄ§Áµ±Ë®à:&quot;)
print(f&quot;  ÂÖÉ„Éá„Éº„Çø: Âπ≥Âùá {thermoelectric_data['ZTÂÄ§'].mean():.3f}, &quot;
      f&quot;Ê®ôÊ∫ñÂÅèÂ∑Æ {thermoelectric_data['ZTÂÄ§'].std():.3f}&quot;)
print(f&quot;  „ÇØ„É™„Éº„Éã„É≥„Ç∞Âæå: Âπ≥Âùá {thermoelectric_cleaned['ZTÂÄ§'].mean():.3f}, &quot;
      f&quot;Ê®ôÊ∫ñÂÅèÂ∑Æ {thermoelectric_cleaned['ZTÂÄ§'].std():.3f}&quot;)
</code></pre>
<hr />
<h2>1.6 „Éá„Éº„Çø„É©„Ç§„Çª„É≥„Çπ„Å®ÂÜçÁèæÊÄß</h2>
<h3>‰∏ªË¶ÅÊùêÊñô„Éá„Éº„Çø„Éô„Éº„Çπ„ÅÆ„É©„Ç§„Çª„É≥„Çπ</h3>
<p>ÊùêÊñô„Éá„Éº„Çø„ÇíÂà©Áî®„Åô„ÇãÈöõ„ÅØ„ÄÅÂêÑ„Éá„Éº„Çø„Éô„Éº„Çπ„ÅÆ„É©„Ç§„Çª„É≥„Çπ„ÇíÁêÜËß£„Åô„Çã„Åì„Å®„ÅåÈáçË¶Å„Åß„Åô„ÄÇ</p>
<pre><code class="language-python"># ‰∏ªË¶ÅÊùêÊñô„Éá„Éº„Çø„Éô„Éº„Çπ„ÅÆÊÉÖÂ†±
database_info = pd.DataFrame({
    '„Éá„Éº„Çø„Éô„Éº„Çπ': [
        'Materials Project',
        'OQMD',
        'NOMAD',
        'AFLOW',
        'Citrination'
    ],
    '„É©„Ç§„Çª„É≥„Çπ': [
        'CC BY 4.0',
        'Academic Use',
        'CC BY 4.0',
        'AFLOWLIB Consortium',
        'Commercial/Academic'
    ],
    '„Éá„Éº„ÇøÊï∞': [
        '150,000+',
        '1,000,000+',
        '10,000,000+',
        '3,500,000+',
        '250,000+'
    ],
    '‰∏ªË¶Å„Éá„Éº„Çø': [
        'DFTË®àÁÆó„ÄÅ„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó„ÄÅÂΩ¢Êàê„Ç®„Éç„É´„ÇÆ„Éº',
        'DFTË®àÁÆó„ÄÅÂÆâÂÆöÊÄß„ÄÅÁõ∏Âõ≥',
        'Ë®àÁÆó„ÉªÂÆüÈ®ì„Éá„Éº„Çø„ÄÅÊ©üÊ¢∞Â≠¶Áøí„É¢„Éá„É´',
        '„Éó„É≠„Éà„Çø„Ç§„ÉóÊßãÈÄ†„ÄÅÁâ©ÊÄß‰∫àÊ∏¨',
        'ÂÆüÈ®ì„Éá„Éº„Çø„ÄÅ„Éó„É≠„Çª„ÇπÊù°‰ª∂'
    ],
    'API': [
        'pymatgen',
        'qmpy',
        'NOMAD API',
        'AFLOW API',
        'citrination-client'
    ]
})

print(&quot;=== ÊùêÊñô„Éá„Éº„Çø„Éô„Éº„ÇπÊØîËºÉ ===&quot;)
print(database_info.to_string(index=False))

# ‰ΩøÁî®‰æã
print(&quot;\n„ÄêMaterials Project API‰ΩøÁî®‰æã„Äë&quot;)
print(&quot;```python&quot;)
print(&quot;from pymatgen.ext.matproj import MPRester&quot;)
print(&quot;# API key: https://materialsproject.org/api „ÅßÂèñÂæó&quot;)
print(&quot;with MPRester('YOUR_API_KEY') as mpr:&quot;)
print(&quot;    structure = mpr.get_structure_by_material_id('mp-149')&quot;)
print(&quot;    bandgap = mpr.get_bandstructure_by_material_id('mp-149')&quot;)
print(&quot;```&quot;)

print(&quot;\n„ÄêÊ≥®ÊÑè‰∫ãÈ†Ö„Äë&quot;)
print(&quot;‚úÖ Ë´ñÊñá„ÉªÂïÜÁî®Âà©Áî®ÊôÇ„ÅØ„É©„Ç§„Çª„É≥„Çπ„ÇíÁ¢∫Ë™ç&quot;)
print(&quot;‚úÖ ÂºïÁî®„ÇíÈÅ©Âàá„Å´Ë°å„ÅÜÔºàÂêÑDB„ÅÆÂºïÁî®ÂΩ¢Âºè„Å´Âæì„ÅÜÔºâ&quot;)
print(&quot;‚úÖ API„Ç≠„Éº„ÅØÁí∞Â¢ÉÂ§âÊï∞„ÅßÁÆ°ÁêÜÔºà.env„Éï„Ç°„Ç§„É´Ôºâ&quot;)
print(&quot;‚úÖ „Éá„Éº„Çø„ÅÆ„Éê„Éº„Ç∏„Éß„É≥„ÉªÂèñÂæóÊó•„ÇíË®òÈå≤&quot;)
</code></pre>
<p><strong>Âá∫Âäõ</strong>Ôºö</p>
<pre><code>=== ÊùêÊñô„Éá„Éº„Çø„Éô„Éº„ÇπÊØîËºÉ ===
„Éá„Éº„Çø„Éô„Éº„Çπ         „É©„Ç§„Çª„É≥„Çπ              „Éá„Éº„ÇøÊï∞      ‰∏ªË¶Å„Éá„Éº„Çø                              API
Materials Project    CC BY 4.0               150,000+      DFTË®àÁÆó„ÄÅ„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó„ÄÅÂΩ¢Êàê„Ç®„Éç„É´„ÇÆ„Éº    pymatgen
OQMD                 Academic Use            1,000,000+    DFTË®àÁÆó„ÄÅÂÆâÂÆöÊÄß„ÄÅÁõ∏Âõ≥                    qmpy
NOMAD                CC BY 4.0               10,000,000+   Ë®àÁÆó„ÉªÂÆüÈ®ì„Éá„Éº„Çø„ÄÅÊ©üÊ¢∞Â≠¶Áøí„É¢„Éá„É´          NOMAD API
AFLOW                AFLOWLIB Consortium     3,500,000+    „Éó„É≠„Éà„Çø„Ç§„ÉóÊßãÈÄ†„ÄÅÁâ©ÊÄß‰∫àÊ∏¨                AFLOW API
Citrination          Commercial/Academic     250,000+      ÂÆüÈ®ì„Éá„Éº„Çø„ÄÅ„Éó„É≠„Çª„ÇπÊù°‰ª∂                  citrination-client
</code></pre>
<h3>„Ç≥„Éº„ÉâÂÜçÁèæÊÄß„ÅÆÁ¢∫‰øù</h3>
<pre><code class="language-python"># Áí∞Â¢É‰ªïÊßò„ÅÆË®òÈå≤
import sys
import sklearn
import pandas as pd
import numpy as np

reproducibility_info = {
    'Python': sys.version,
    'NumPy': np.__version__,
    'Pandas': pd.__version__,
    'scikit-learn': sklearn.__version__,
    'Date': '2025-10-19'
}

print(&quot;=== ÂÜçÁèæÊÄßÊÉÖÂ†± ===&quot;)
for key, value in reproducibility_info.items():
    print(f&quot;{key}: {value}&quot;)

# requirements.txtÁîüÊàê
print(&quot;\n„ÄêÊé®Â•®Áí∞Â¢É„Äë&quot;)
requirements = &quot;&quot;&quot;
numpy==1.24.3
pandas==2.0.3
scikit-learn==1.3.0
matplotlib==3.7.2
seaborn==0.12.2
scipy==1.11.1
&quot;&quot;&quot;
print(requirements)

print(&quot;„ÄêÁí∞Â¢ÉÊßãÁØâ„Ç≥„Éû„É≥„Éâ„Äë&quot;)
print(&quot;```bash&quot;)
print(&quot;# ‰ªÆÊÉ≥Áí∞Â¢É‰ΩúÊàê&quot;)
print(&quot;python -m venv venv&quot;)
print(&quot;source venv/bin/activate  # Linux/Mac&quot;)
print(&quot;# venv\\Scripts\\activate  # Windows&quot;)
print(&quot;&quot;)
print(&quot;# „Éë„ÉÉ„Ç±„Éº„Ç∏„Ç§„É≥„Çπ„Éà„Éº„É´&quot;)
print(&quot;pip install -r requirements.txt&quot;)
print(&quot;```&quot;)
</code></pre>
<h3>ÂÆüË∑µÁöÑ„Å™ËêΩ„Å®„ÅóÁ©¥ÔºàPitfallsÔºâ</h3>
<pre><code class="language-python"># ËêΩ„Å®„ÅóÁ©¥1: „Éá„Éº„Çø„É™„Éº„ÇØÔºàData LeakageÔºâ
print(&quot;=== ÂÆüË∑µÁöÑ„Å™ËêΩ„Å®„ÅóÁ©¥ ===\n&quot;)

print(&quot;„ÄêËêΩ„Å®„ÅóÁ©¥1: „Éá„Éº„Çø„É™„Éº„ÇØ„Äë&quot;)
print(&quot;‚ùå ÊÇ™„ÅÑ‰æãÔºöÂÖ®„Éá„Éº„Çø„ÅßÂâçÂá¶ÁêÜ ‚Üí Train/TestÂàÜÂâ≤&quot;)
print(&quot;```python&quot;)
print(&quot;X_scaled = StandardScaler().fit_transform(X)  # ÂÖ®„Éá„Éº„Çø„Åßfit&quot;)
print(&quot;X_train, X_test = train_test_split(X_scaled)&quot;)
print(&quot;# ‚Üí „ÉÜ„Çπ„Éà„Éá„Éº„Çø„ÅÆÊÉÖÂ†±„ÅåË®ìÁ∑¥ÊôÇ„Å´Êºè„Çå„Å¶„ÅÑ„ÇãÔºÅ&quot;)
print(&quot;```&quot;)

print(&quot;\n‚úÖ Ê≠£„Åó„ÅÑ‰æãÔºöTrain/TestÂàÜÂâ≤ ‚Üí Ë®ìÁ∑¥„Éá„Éº„Çø„ÅÆ„Åø„ÅßÂâçÂá¶ÁêÜ&quot;)
print(&quot;```python&quot;)
print(&quot;X_train, X_test = train_test_split(X)&quot;)
print(&quot;scaler = StandardScaler().fit(X_train)  # Ë®ìÁ∑¥„Éá„Éº„Çø„ÅÆ„Åø„Åßfit&quot;)
print(&quot;X_train_scaled = scaler.transform(X_train)&quot;)
print(&quot;X_test_scaled = scaler.transform(X_test)&quot;)
print(&quot;```&quot;)

print(&quot;\n„ÄêËêΩ„Å®„ÅóÁ©¥2: ÁµÑÊàê„Éô„Éº„ÇπÂàÜÂâ≤„ÅÆÂøÖË¶ÅÊÄß„Äë&quot;)
print(&quot;‚ùå ÊÇ™„ÅÑ‰æãÔºö„É©„É≥„ÉÄ„É†ÂàÜÂâ≤&quot;)
print(&quot;- Li‚ÇÄ.‚ÇâCoO‚ÇÇÔºàË®ìÁ∑¥Ôºâ„Å®Li‚ÇÅ.‚ÇÄCoO‚ÇÇÔºà„ÉÜ„Çπ„ÉàÔºâ„ÅØÈ°û‰ºº&quot;)
print(&quot;- ÈÅéÂ∫¶„Å´Ê•ΩË¶≥ÁöÑ„Å™ÊÄßËÉΩË©ï‰æ°&quot;)

print(&quot;\n‚úÖ Ê≠£„Åó„ÅÑ‰æãÔºöÁµÑÊàê„Ç∞„É´„Éº„ÉóÂàÜÂâ≤&quot;)
print(&quot;```python&quot;)
print(&quot;from sklearn.model_selection import GroupKFold&quot;)
print(&quot;groups = [get_composition_family(formula) for formula in formulas]&quot;)
print(&quot;gkf = GroupKFold(n_splits=5)&quot;)
print(&quot;for train_idx, test_idx in gkf.split(X, y, groups):&quot;)
print(&quot;    # Âêå„ÅòÁµÑÊàêÁ≥ª„ÅØÂêå„Åòfold„Å´&quot;)
print(&quot;```&quot;)

print(&quot;\n„ÄêËêΩ„Å®„ÅóÁ©¥3: Â§ñÊåø„ÅÆÈôêÁïå„Äë&quot;)
print(&quot;‚ö†Ô∏è Ê©üÊ¢∞Â≠¶Áøí„É¢„Éá„É´„ÅØË®ìÁ∑¥ÁØÑÂõ≤Â§ñ„ÅÆ‰∫àÊ∏¨„ÅåËã¶Êâã&quot;)
print(&quot;‰æãÔºö„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó0-3 eV„ÅßË®ìÁ∑¥ ‚Üí 5 eV„ÅÆ‰∫àÊ∏¨„ÅØ‰∏çÊ≠£Á¢∫&quot;)

print(&quot;\nÂØæÁ≠ñ:&quot;)
print(&quot;- Ë®ìÁ∑¥„Éá„Éº„Çø„ÅÆÁØÑÂõ≤„ÇíÊòéÁ§∫&quot;)
print(&quot;- Â§ñÊåøÈ†òÂüü„ÅÆ‰∏çÁ¢∫ÂÆüÊÄß„ÇíÂÆöÈáèÂåñÔºà„Éô„Ç§„Ç∫ÊâãÊ≥ïÔºâ&quot;)
print(&quot;- Active Learning„ÅßÊÆµÈöéÁöÑ„Å´ÁØÑÂõ≤Êã°Â§ß&quot;)

print(&quot;\n„ÄêËêΩ„Å®„ÅóÁ©¥4: ÁâπÂæ¥ÈáèÈñì„ÅÆÁõ∏Èñ¢„Äë&quot;)
print(&quot;‚ö†Ô∏è È´òÁõ∏Èñ¢ÁâπÂæ¥Èáè„ÅØÂÜóÈï∑„ÅßÈÅéÂ≠¶Áøí„ÇíÊãõ„Åè&quot;)
print(&quot;```python&quot;)
print(&quot;# Áõ∏Èñ¢Ë°åÂàó„ÅßÁ¢∫Ë™ç&quot;)
print(&quot;correlation_matrix = X.corr()&quot;)
print(&quot;high_corr = (correlation_matrix.abs() &gt; 0.9) &amp; (correlation_matrix != 1.0)&quot;)
print(&quot;print(high_corr.sum())  # È´òÁõ∏Èñ¢„Éö„Ç¢Êï∞&quot;)
print(&quot;&quot;)
print(&quot;# ÂØæÁ≠ñÔºöVIFÔºàÂàÜÊï£Êã°Â§ß‰øÇÊï∞Ôºâ„ÅßÂ§öÈáçÂÖ±Á∑öÊÄßÊ§úÂá∫&quot;)
print(&quot;from statsmodels.stats.outliers_influence import variance_inflation_factor&quot;)
print(&quot;vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]&quot;)
print(&quot;```&quot;)
</code></pre>
<hr />
<h2>ÊºîÁøíÂïèÈ°å</h2>
<h3>ÂïèÈ°å1ÔºàÈõ£ÊòìÂ∫¶: easyÔºâ</h3>
<p>‰ª•‰∏ã„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´ÂØæ„Åó„Å¶„ÄÅSimple ImputationÔºàÂπ≥ÂùáÂÄ§Ôºâ„Å®KNN Imputation„ÇíÈÅ©Áî®„Åó„ÄÅË£úÂÆåÁ≤æÂ∫¶„ÇíÊØîËºÉ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<pre><code class="language-python"># ÊºîÁøíÁî®„Éá„Éº„Çø
np.random.seed(123)
exercise_data = pd.DataFrame({
    'feature1': np.random.normal(50, 10, 100),
    'feature2': np.random.normal(30, 5, 100),
    'feature3': np.random.normal(100, 20, 100)
})

# „É©„É≥„ÉÄ„É†„Å´10%Ê¨†Êêç
for col in exercise_data.columns:
    missing_idx = np.random.choice(100, 10, replace=False)
    exercise_data.loc[missing_idx, col] = np.nan
</code></pre>
<details>
<summary>„Éí„É≥„Éà</summary>

1. `SimpleImputer(strategy='mean')`„Çí‰ΩøÁî®
2. `KNNImputer(n_neighbors=5)`„Çí‰ΩøÁî®
3. ÂÖÉ„ÅÆÂÆåÂÖ®„Éá„Éº„Çø„Çí‰ΩúÊàê„Åó„Å¶„ÄÅË£úÂÆåÂÄ§„Å®„ÅÆÂ∑ÆÔºàMAEÔºâ„ÇíË®àÁÆó

</details>

<details>
<summary>Ëß£Á≠î‰æã</summary>


<pre><code class="language-python">from sklearn.impute import SimpleImputer, KNNImputer

# ÂÖÉ„ÅÆÂÆåÂÖ®„Éá„Éº„ÇøÔºàÊØîËºÉÁî®Ôºâ
np.random.seed(123)
true_data = pd.DataFrame({
    'feature1': np.random.normal(50, 10, 100),
    'feature2': np.random.normal(30, 5, 100),
    'feature3': np.random.normal(100, 20, 100)
})

# Simple Imputation
simple_imputer = SimpleImputer(strategy='mean')
data_simple = pd.DataFrame(
    simple_imputer.fit_transform(exercise_data),
    columns=exercise_data.columns
)

# KNN Imputation
knn_imputer = KNNImputer(n_neighbors=5)
data_knn = pd.DataFrame(
    knn_imputer.fit_transform(exercise_data),
    columns=exercise_data.columns
)

# Á≤æÂ∫¶Ë©ï‰æ°
missing_mask = exercise_data.isnull()
mae_simple = []
mae_knn = []

for col in exercise_data.columns:
    mask = missing_mask[col]
    if mask.any():
        mae_s = np.mean(np.abs(data_simple.loc[mask, col] - true_data.loc[mask, col]))
        mae_k = np.mean(np.abs(data_knn.loc[mask, col] - true_data.loc[mask, col]))
        mae_simple.append(mae_s)
        mae_knn.append(mae_k)

print(f&quot;Simple Imputation MAE: {np.mean(mae_simple):.4f}&quot;)
print(f&quot;KNN Imputation MAE: {np.mean(mae_knn):.4f}&quot;)
</code></pre>


</details>

<h3>ÂïèÈ°å2ÔºàÈõ£ÊòìÂ∫¶: mediumÔºâ</h3>
<p>Latin Hypercube Sampling„ÇíÁî®„ÅÑ„Å¶„ÄÅ3Ê¨°ÂÖÉ„ÅÆÁµÑÊàêÁ©∫ÈñìÔºàÂÖÉÁ¥†A, B, C„ÅÆÂâ≤ÂêàÔºâ„Çí„Çµ„É≥„Éó„É™„É≥„Ç∞„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÂà∂Á¥ÑÊù°‰ª∂„Å®„Åó„Å¶„ÄÅA + B + C = 1 „ÇíÊ∫Ä„Åü„Åô„Çà„ÅÜ„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<details>
<summary>„Éí„É≥„Éà</summary>

1. 2Ê¨°ÂÖÉ„ÅßLHS„ÇíÂÆüË°åÔºàA„Å®B„ÅÆ„ÅøÔºâ
2. C = 1 - A - B „ÅßË®àÁÆó
3. 3Ê¨°ÂÖÉÁ©∫Èñì„ÅßÂèØË¶ñÂåñ

</details>

<details>
<summary>Ëß£Á≠î‰æã</summary>


<pre><code class="language-python">from scipy.stats import qmc
from mpl_toolkits.mplot3d import Axes3D

# 2Ê¨°ÂÖÉLHSÔºàA, BÔºâ
sampler = qmc.LatinHypercube(d=2, seed=42)
samples_2d = sampler.random(n=50)

# A + B &lt;= 1 „Å®„Å™„Çã„Çà„ÅÜ„Çπ„Ç±„Éº„É™„É≥„Ç∞
A = samples_2d[:, 0] * 0.9  # 0„Äú0.9
B = (1 - A) * samples_2d[:, 1]  # ÊÆã„Çä„ÅÆÁØÑÂõ≤ÂÜÖ
C = 1 - A - B

# 3Ê¨°ÂÖÉÂèØË¶ñÂåñ
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

ax.scatter(A, B, C, c='steelblue', s=100, alpha=0.6, edgecolors='k')
ax.set_xlabel('ÂÖÉÁ¥†A', fontsize=12)
ax.set_ylabel('ÂÖÉÁ¥†B', fontsize=12)
ax.set_zlabel('ÂÖÉÁ¥†C', fontsize=12)
ax.set_title('ÁµÑÊàêÁ©∫Èñì„ÅÆLatin Hypercube Sampling', fontsize=13, fontweight='bold')

plt.tight_layout()
plt.show()

# Âà∂Á¥ÑÁ¢∫Ë™ç
print(f&quot;ÂÖ®„Çµ„É≥„Éó„É´„Åß A+B+C=1: {np.allclose(A+B+C, 1)}&quot;)
</code></pre>


</details>

<h3>ÂïèÈ°å3ÔºàÈõ£ÊòìÂ∫¶: hardÔºâ</h3>
<p>Isolation Forest„Å®LOF„ÇíÁî®„ÅÑ„Å¶„ÄÅÂ§öÊ¨°ÂÖÉ„Éá„Éº„Çø„ÅÆÂ§ñ„ÇåÂÄ§Ê§úÂá∫„ÇíË°å„ÅÑ„ÄÅ„Å©„Å°„Çâ„Åå„Çà„ÇäÈÅ©Âàá„ÅãË©ï‰æ°„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇË©ï‰æ°„Å´„ÅØ„ÄÅÊó¢Áü•„ÅÆÂ§ñ„ÇåÂÄ§„É©„Éô„É´„Å®„ÅÆ‰∏ÄËá¥ÁéáÔºàPrecision, Recall, F1-scoreÔºâ„Çí‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<details>
<summary>„Éí„É≥„Éà</summary>

1. Ê≠£Â∏∏„Éá„Éº„Çø + ÊÑèÂõ≥ÁöÑ„Å™Â§ñ„ÇåÂÄ§„ÇíÁîüÊàê
2. Isolation Forest„Å®LOF„ÅßÊ§úÂá∫
3. `sklearn.metrics.classification_report`„ÅßË©ï‰æ°

</details>

<details>
<summary>Ëß£Á≠î‰æã</summary>


<pre><code class="language-python">from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.metrics import classification_report, confusion_matrix

# „Éá„Éº„ÇøÁîüÊàê
np.random.seed(42)
X_normal = np.random.randn(200, 5) * 2 + 10
X_outliers = np.random.uniform(0, 20, (20, 5))
X = np.vstack([X_normal, X_outliers])

# Áúü„ÅÆ„É©„Éô„É´Ôºà0: Ê≠£Â∏∏, 1: Â§ñ„ÇåÂÄ§Ôºâ
y_true = np.array([0]*200 + [1]*20)

# Isolation Forest
clf_if = IsolationForest(contamination=0.1, random_state=42)
y_pred_if = (clf_if.fit_predict(X) == -1).astype(int)

# LOF
clf_lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)
y_pred_lof = (clf_lof.fit_predict(X) == -1).astype(int)

# Ë©ï‰æ°
print(&quot;=== Isolation Forest ===&quot;)
print(classification_report(y_true, y_pred_if,
                           target_names=['Ê≠£Â∏∏', 'Â§ñ„ÇåÂÄ§']))

print(&quot;\n=== Local Outlier Factor ===&quot;)
print(classification_report(y_true, y_pred_lof,
                           target_names=['Ê≠£Â∏∏', 'Â§ñ„ÇåÂÄ§']))

# Ê∑∑ÂêåË°åÂàó
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

cm_if = confusion_matrix(y_true, y_pred_if)
sns.heatmap(cm_if, annot=True, fmt='d', cmap='Blues', ax=axes[0])
axes[0].set_xlabel('‰∫àÊ∏¨„É©„Éô„É´', fontsize=11)
axes[0].set_ylabel('Áúü„ÅÆ„É©„Éô„É´', fontsize=11)
axes[0].set_title('Isolation Forest', fontsize=12, fontweight='bold')

cm_lof = confusion_matrix(y_true, y_pred_lof)
sns.heatmap(cm_lof, annot=True, fmt='d', cmap='Oranges', ax=axes[1])
axes[1].set_xlabel('‰∫àÊ∏¨„É©„Éô„É´', fontsize=11)
axes[1].set_ylabel('Áúü„ÅÆ„É©„Éô„É´', fontsize=11)
axes[1].set_title('LOF', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.show()
</code></pre>


</details>

<hr />
<h2>„Åæ„Å®„ÇÅ</h2>
<p>„Åì„ÅÆÁ´†„Åß„ÅØ„ÄÅ„Éá„Éº„ÇøÈßÜÂãïÊùêÊñôÁßëÂ≠¶„Å´„Åä„Åë„Çã<strong>„Éá„Éº„ÇøÂèéÈõÜÊà¶Áï•„Å®„ÇØ„É™„Éº„Éã„É≥„Ç∞</strong>„ÇíÂ≠¶„Å≥„Åæ„Åó„Åü„ÄÇ</p>
<p><strong>ÈáçË¶Å„Éù„Ç§„É≥„Éà</strong>Ôºö</p>
<ol>
<li><strong>ÊùêÊñô„Éá„Éº„Çø„ÅÆÁâπÂæ¥</strong>ÔºöÂ∞èË¶èÊ®°„Éª‰∏çÂùáË°°„Éª„Éé„Ç§„Ç∫„ÅåÂ§ö„ÅÑ ‚Üí ÈÅ©Âàá„Å™ÂâçÂá¶ÁêÜ„Åå‰∏çÂèØÊ¨†</li>
<li><strong>ÂÆüÈ®ìË®àÁîªÊ≥ï</strong>ÔºöDOE„ÄÅLHS„ÄÅActive Learning„ÅßÂäπÁéáÁöÑ„Å™„Éá„Éº„ÇøÂèéÈõÜ</li>
<li><strong>Ê¨†ÊêçÂÄ§Âá¶ÁêÜ</strong>ÔºöSimple &lt; KNN &lt; MICE „ÅÆÈ†Ü„ÅßÁ≤æÂ∫¶Âêë‰∏ä</li>
<li><strong>Â§ñ„ÇåÂÄ§Ê§úÂá∫</strong>ÔºöÁµ±Ë®àÁöÑÊâãÊ≥ï„ÄÅIsolation Forest„ÄÅLOF„ÄÅDBSCAN„Çí‰Ωø„ÅÑÂàÜ„Åë</li>
<li><strong>Áâ©ÁêÜÁöÑÂ¶•ÂΩìÊÄß</strong>ÔºöÊ©üÊ¢∞ÁöÑ„Å™„ÇØ„É™„Éº„Éã„É≥„Ç∞„Å†„Åë„Åß„Å™„Åè„ÄÅÁâ©ÁêÜÁöÑÊÑèÂë≥„ÇíÊ§úË®º</li>
<li><strong>„Éá„Éº„Çø„É©„Ç§„Çª„É≥„Çπ</strong>ÔºöMaterials Project„ÄÅOQMD„ÄÅNOMAD„Å™„Å©‰∏ªË¶ÅDB„ÅÆÂà©Áî®Ë¶èÁ¥Ñ„ÇíÁ¢∫Ë™ç</li>
<li><strong>ÂÜçÁèæÊÄßÁ¢∫‰øù</strong>ÔºöÁí∞Â¢É„Éê„Éº„Ç∏„Éß„É≥Ë®òÈå≤„ÄÅrequirements.txtÁÆ°ÁêÜ</li>
<li><strong>ÂÆüË∑µÁöÑËêΩ„Å®„ÅóÁ©¥</strong>Ôºö„Éá„Éº„Çø„É™„Éº„ÇØ„ÄÅÁµÑÊàê„Éô„Éº„ÇπÂàÜÂâ≤„ÄÅÂ§ñÊåø„ÅÆÈôêÁïå„ÄÅÁâπÂæ¥ÈáèÁõ∏Èñ¢</li>
</ol>
<p><strong>Ê¨°Á´†‰∫àÂëä</strong>Ôºö
Chapter 2„Åß„ÅØ„ÄÅ„ÇØ„É™„Éº„Éã„É≥„Ç∞Ê∏à„Åø„Éá„Éº„Çø„Åã„Çâ<strong>ÊúâÂäπ„Å™ÁâπÂæ¥Èáè„ÇíË®≠Ë®à</strong>„Åô„ÇãÊâãÊ≥ïÔºàÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞Ôºâ„ÇíÂ≠¶„Å≥„Åæ„Åô„ÄÇmatminer„ÇíÁî®„ÅÑ„ÅüÊùêÊñôË®òËø∞Â≠êÁîüÊàê„ÄÅÊ¨°ÂÖÉÂâäÊ∏õ„ÄÅÁâπÂæ¥ÈáèÈÅ∏Êäû„ÇíÂÆüË∑µ„Åó„Åæ„Åô„ÄÇ</p>
<hr />
<h2>Chapter 1 „ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà</h2>
<h3>„Éá„Éº„ÇøÂèéÈõÜ</h3>
<ul>
<li>[ ] <strong>ÂÆüÈ®ìË®àÁîªÊ≥ï„ÅÆÈÅ∏Êäû</strong></li>
<li>[ ] ÂÖ®Êé¢Á¥¢ÔºàFull FactorialÔºâvs ÈÉ®ÂàÜÊé¢Á¥¢ÔºàFractional FactorialÔºâ„ÇíÂà§Êñ≠</li>
<li>[ ] Latin Hypercube Sampling„ÅßÊé¢Á¥¢Á©∫Èñì„ÇíÂùá‰∏Ä„Å´„Ç´„Éê„Éº</li>
<li>
<p>[ ] Active Learning„Åß‰∏çÁ¢∫ÂÆüÊÄß„ÅÆÈ´ò„ÅÑÈ†òÂüü„ÇíÂÑ™ÂÖà„Çµ„É≥„Éó„É™„É≥„Ç∞</p>
</li>
<li>
<p>[ ] <strong>„Éá„Éº„Çø„ÇΩ„Éº„Çπ„ÅÆÁ¢∫Ë™ç</strong></p>
</li>
<li>[ ] ÂÆüÈ®ì„Éá„Éº„Çø„ÄÅDFTË®àÁÆó„ÄÅÊñáÁåÆ„Éá„Éº„Çø„ÅÆÊ∑∑Âú®„ÇíÊääÊè°</li>
<li>[ ] ÂêÑ„ÇΩ„Éº„Çπ„ÅÆÁ≤æÂ∫¶„Å®‰ø°È†ºÊÄß„ÇíË©ï‰æ°</li>
<li>
<p>[ ] „Éá„Éº„ÇøÂèñÂæóÊó•„Å®„Éê„Éº„Ç∏„Éß„É≥„ÇíË®òÈå≤</p>
</li>
<li>
<p>[ ] <strong>„É©„Ç§„Çª„É≥„Çπ„Å®ÂºïÁî®</strong></p>
</li>
<li>[ ] Materials Project„ÄÅOQMD„ÄÅNOMAD„ÄÅAFLOW„ÅÆ„É©„Ç§„Çª„É≥„Çπ„ÇíÁ¢∫Ë™ç</li>
<li>[ ] Ë´ñÊñá„ÉªÂïÜÁî®Âà©Áî®ÊôÇ„ÅÆÂà∂Á¥Ñ„ÇíÁêÜËß£</li>
<li>[ ] ÈÅ©Âàá„Å™ÂºïÁî®ÂΩ¢Âºè„Åß„Éá„Éº„Çø„Éô„Éº„Çπ„ÇíÂºïÁî®</li>
</ul>
<h3>„Éá„Éº„Çø„ÇØ„É™„Éº„Éã„É≥„Ç∞</h3>
<ul>
<li>[ ] <strong>Ê¨†ÊêçÂÄ§Âá¶ÁêÜ</strong></li>
<li>[ ] Ê¨†Êêç„Éë„Çø„Éº„É≥„ÅÆÂàÜÈ°ûÔºàMCAR„ÄÅMAR„ÄÅMNARÔºâ</li>
<li>[ ] Simple ImputationÔºàÂπ≥ÂùáÂÄ§„Éª‰∏≠Â§ÆÂÄ§Ôºâ„ÅßÂü∫Ê∫ñÊÄßËÉΩÁ¢∫Ë™ç</li>
<li>[ ] KNN Imputation„ÅßÁõ∏Èñ¢„ÇíËÄÉÊÖÆ„Åó„ÅüË£úÂÆå</li>
<li>[ ] MICE„ÅßË§áÈõë„Å™‰æùÂ≠òÈñ¢‰øÇ„ÇíÊçâ„Åà„ÇãË£úÂÆå</li>
<li>
<p>[ ] Ë£úÂÆåÂâçÂæå„ÅÆÁµ±Ë®àÈáèÂ§âÂåñ„ÇíÁ¢∫Ë™ç</p>
</li>
<li>
<p>[ ] <strong>Â§ñ„ÇåÂÄ§Ê§úÂá∫</strong></p>
</li>
<li>[ ] Z-scoreÊ≥ï„ÄÅIQRÊ≥ï„ÅßÂçòÂ§âÈáèÂ§ñ„ÇåÂÄ§Ê§úÂá∫</li>
<li>[ ] Isolation Forest„ÅßÂ§öÂ§âÈáèÂ§ñ„ÇåÂÄ§Ê§úÂá∫</li>
<li>[ ] LOF„ÅßÂ±ÄÊâÄÁöÑÂØÜÂ∫¶„Éô„Éº„Çπ„ÅÆÊ§úÂá∫</li>
<li>[ ] DBSCAN„Åß„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Éô„Éº„Çπ„ÅÆÊ§úÂá∫</li>
<li>
<p>[ ] Â§ñ„ÇåÂÄ§„ÅåÁâ©ÁêÜÁöÑ„Å´Â¶•ÂΩì„ÅãÊ§úË®ºÔºàÊ∏¨ÂÆö„Ç®„É©„Éº vs Êñ∞Áô∫Ë¶ãÔºâ</p>
</li>
<li>
<p>[ ] <strong>Áâ©ÁêÜÁöÑÂ¶•ÂΩìÊÄßÊ§úË®º</strong></p>
</li>
<li>[ ] ÁµÑÊàê„ÅÆÂêàË®à„Åå1ÂâçÂæåÔºàË®±ÂÆπÁØÑÂõ≤¬±0.1Ôºâ</li>
<li>[ ] „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó„ÄÅÂΩ¢Êàê„Ç®„Éç„É´„ÇÆ„Éº„Å™„Å©„ÅåÊ≠£„ÅÆÂÄ§</li>
<li>[ ] Áâ©ÊÄßÂÄ§„ÅåÁêÜË´ñÁöÑ‰∏äÈôê„ÇíË∂Ö„Åà„Å¶„ÅÑ„Å™„ÅÑ„Åã</li>
<li>[ ] Êó¢Áü•„ÅÆÁâ©ÁêÜÊ≥ïÂâáÔºà„Ç¢„É¨„Éã„Ç¶„ÇπÂâá„Å™„Å©Ôºâ„Å®ÁüõÁõæ„Åó„Å™„ÅÑ„Åã</li>
</ul>
<h3>ÂÆüË∑µÁöÑËêΩ„Å®„ÅóÁ©¥„ÅÆÂõûÈÅø</h3>
<ul>
<li>[ ] <strong>„Éá„Éº„Çø„É™„Éº„ÇØÈò≤Ê≠¢</strong></li>
<li>[ ] Train/TestÂàÜÂâ≤<strong>Âæå</strong>„Å´ÂâçÂá¶ÁêÜÔºàStandardScaler„ÄÅImputer„Å™„Å©Ôºâ</li>
<li>[ ] „ÇØ„É≠„Çπ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥ÊôÇ„ÇÇÂêÑfold„ÅßÁã¨Á´ã„Å´ÂâçÂá¶ÁêÜ</li>
<li>
<p>[ ] ÁõÆÁöÑÂ§âÊï∞„ÇíÁî®„ÅÑ„ÅüÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞„ÇíÈÅø„Åë„Çã</p>
</li>
<li>
<p>[ ] <strong>ÁµÑÊàê„Éô„Éº„ÇπÂàÜÂâ≤</strong></p>
</li>
<li>[ ] È°û‰ººÁµÑÊàê„ÅåÂêå„Åòfold„Å´ÂÖ•„Çã„Çà„ÅÜGroupKFold‰ΩøÁî®</li>
<li>[ ] „É©„É≥„ÉÄ„É†ÂàÜÂâ≤„ÅßÈÅéÂ∫¶„Å´Ê•ΩË¶≥ÁöÑ„Å™Ë©ï‰æ°„ÇíÈÅø„Åë„Çã</li>
<li>
<p>[ ] „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„ÅßÊùêÊñôÁ≥ª„ÅÆÂ§öÊßòÊÄß„ÇíÁ¢∫‰øù</p>
</li>
<li>
<p>[ ] <strong>Â§ñÊåø„ÅÆÈôêÁïåË™çË≠ò</strong></p>
</li>
<li>[ ] Ë®ìÁ∑¥„Éá„Éº„Çø„ÅÆÁØÑÂõ≤ÔºàÊúÄÂ∞èÂÄ§„ÉªÊúÄÂ§ßÂÄ§Ôºâ„ÇíÊòéÁ§∫</li>
<li>[ ] Â§ñÊåøÈ†òÂüü„ÅÆ‰∫àÊ∏¨„Å´„ÅØ‰∏çÁ¢∫ÂÆüÊÄß„Çí‰ªò‰∏é</li>
<li>
<p>[ ] Active Learning„ÅßÊÆµÈöéÁöÑ„Å´ÁØÑÂõ≤Êã°Â§ß</p>
</li>
<li>
<p>[ ] <strong>ÁâπÂæ¥ÈáèÈñì„ÅÆÁõ∏Èñ¢ÁÆ°ÁêÜ</strong></p>
</li>
<li>[ ] Áõ∏Èñ¢Ë°åÂàó„ÅßÈ´òÁõ∏Èñ¢„Éö„Ç¢Ôºà|r| &gt; 0.9Ôºâ„ÇíÁâπÂÆö</li>
<li>[ ] VIFÔºàÂàÜÊï£Êã°Â§ß‰øÇÊï∞Ôºâ„ÅßÂ§öÈáçÂÖ±Á∑öÊÄß„ÇíÊ§úÂá∫</li>
<li>[ ] ÂÜóÈï∑„Å™ÁâπÂæ¥Èáè„ÇíÈô§Âéª„Åæ„Åü„ÅØ‰∏ªÊàêÂàÜÂàÜÊûê„ÅßÈõÜÁ¥Ñ</li>
</ul>
<h3>ÂÜçÁèæÊÄß„ÅÆÁ¢∫‰øù</h3>
<ul>
<li>[ ] <strong>Áí∞Â¢ÉË®òÈå≤</strong></li>
<li>[ ] Python„ÄÅNumPy„ÄÅPandas„ÄÅscikit-learn„ÅÆ„Éê„Éº„Ç∏„Éß„É≥Ë®òÈå≤</li>
<li>[ ] requirements.txt„Åæ„Åü„ÅØenvironment.yml„Çí‰ΩúÊàê</li>
<li>
<p>[ ] ‰π±Êï∞„Ç∑„Éº„Éâ„ÇíÂõ∫ÂÆöÔºà<code>random_state=42</code>„Å™„Å©Ôºâ</p>
</li>
<li>
<p>[ ] <strong>„Éá„Éº„ÇøÁÆ°ÁêÜ</strong></p>
</li>
<li>[ ] ÂÖÉ„Éá„Éº„Çø„Å®„ÇØ„É™„Éº„Éã„É≥„Ç∞Ê∏à„Åø„Éá„Éº„Çø„ÇíÂàÜÈõ¢‰øùÂ≠ò</li>
<li>[ ] „ÇØ„É™„Éº„Éã„É≥„Ç∞„Çπ„ÇØ„É™„Éó„Éà„Çí„Éê„Éº„Ç∏„Éß„É≥ÁÆ°ÁêÜ</li>
<li>
<p>[ ] „Éá„Éº„Çø„ÅÆÂèñÂæóÊó•„Éª„ÇΩ„Éº„Çπ„ÉªÂá¶ÁêÜÂ±•Ê≠¥„ÇíË®òÈå≤</p>
</li>
<li>
<p>[ ] <strong>„Ç≥„Éº„ÉâÂìÅË≥™</strong></p>
</li>
<li>[ ] Èñ¢Êï∞Âåñ„Éª„É¢„Ç∏„É•„Éº„É´Âåñ„ÅßÂÜçÂà©Áî®ÊÄßÂêë‰∏ä</li>
<li>[ ] Docstring„ÅßÂá¶ÁêÜÂÜÖÂÆπ„ÇíÊñáÊõ∏Âåñ</li>
<li>[ ] Âçò‰Ωì„ÉÜ„Çπ„Éà„Åß‰∏ªË¶ÅÈñ¢Êï∞„ÅÆÂãï‰ΩúÁ¢∫Ë™ç</li>
</ul>
<h3>„Éá„Éº„ÇøÂìÅË≥™Ë©ï‰æ°ÊåáÊ®ô</h3>
<ul>
<li>[ ] <strong>ÂÆåÂÖ®ÊÄß</strong></li>
<li>[ ] Ê¨†ÊêçÁéá &lt; 20%ÔºàÊé®Â•®Ôºâ</li>
<li>
<p>[ ] ÈáçË¶ÅÁâπÂæ¥Èáè„ÅÆÊ¨†ÊêçÁéá &lt; 10%</p>
</li>
<li>
<p>[ ] <strong>Ê≠£Á¢∫ÊÄß</strong></p>
</li>
<li>[ ] Â§ñ„ÇåÂÄ§Áéá &lt; 5%</li>
<li>
<p>[ ] Áâ©ÁêÜÂà∂Á¥ÑÈÅïÂèçÁéá = 0%</p>
</li>
<li>
<p>[ ] <strong>‰ª£Ë°®ÊÄß</strong></p>
</li>
<li>[ ] „Çµ„É≥„Éó„É´/ÁâπÂæ¥ÈáèÊØî &gt; 10:1ÔºàÊé®Â•®Ôºâ</li>
<li>
<p>[ ] „ÇØ„É©„Çπ‰∏çÂùáË°°ÊØî &lt; 10:1ÔºàÂàÜÈ°ûÂïèÈ°åÔºâ</p>
</li>
<li>
<p>[ ] <strong>‰ø°È†ºÊÄß</strong></p>
</li>
<li>[ ] Ë§áÊï∞„ÇΩ„Éº„ÇπÈñì„ÅÆ‰∏ÄËá¥Â∫¶ &gt; 80%</li>
<li>[ ] Ê∏¨ÂÆöË™§Â∑Æ„ÅÆÊ®ôÊ∫ñÂÅèÂ∑ÆË®òÈå≤</li>
</ul>
<hr />
<h2>ÂèÇËÄÉÊñáÁåÆ</h2>
<ol>
<li>
<p><strong>Little, R. J. &amp; Rubin, D. B.</strong> (2019). <em>Statistical Analysis with Missing Data</em> (3rd ed.). Wiley. <a href="https://doi.org/10.1002/9781119482260">DOI: 10.1002/9781119482260</a></p>
</li>
<li>
<p><strong>Liu, F. T., Ting, K. M., &amp; Zhou, Z. H.</strong> (2008). Isolation forest. In <em>2008 Eighth IEEE International Conference on Data Mining</em> (pp. 413-422). IEEE. <a href="https://doi.org/10.1109/ICDM.2008.17">DOI: 10.1109/ICDM.2008.17</a></p>
</li>
<li>
<p><strong>Breunig, M. M., Kriegel, H. P., Ng, R. T., &amp; Sander, J.</strong> (2000). LOF: identifying density-based local outliers. In <em>ACM SIGMOD Record</em> (Vol. 29, No. 2, pp. 93-104). <a href="https://doi.org/10.1145/335191.335388">DOI: 10.1145/335191.335388</a></p>
</li>
<li>
<p><strong>McKay, M. D., Beckman, R. J., &amp; Conover, W. J.</strong> (1979). A comparison of three methods for selecting values of input variables in the analysis of output from a computer code. <em>Technometrics</em>, 21(2), 239-245. <a href="https://doi.org/10.1080/00401706.1979.10489755">DOI: 10.1080/00401706.1979.10489755</a></p>
</li>
<li>
<p><strong>Settles, B.</strong> (2009). <em>Active Learning Literature Survey</em> (Computer Sciences Technical Report 1648). University of Wisconsin-Madison.</p>
</li>
</ol>
<hr />
<p><a href="index.html">‚Üê „Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a> | <a href="chapter-2.html">Chapter 2„Å∏ÈÄ≤„ÇÄ ‚Üí</a></p><div class="navigation">
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
    <a href="chapter-2.html" class="nav-button">Ê¨°„ÅÆÁ´† ‚Üí</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>Áõ£‰øÆ</strong>: Dr. Yusuke HashimotoÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-17</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
