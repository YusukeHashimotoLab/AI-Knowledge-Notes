<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Á¨¨1Á´†ÔºöActive Learning„ÅÆÂøÖË¶ÅÊÄß - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Á¨¨1Á´†ÔºöActive Learning„ÅÆÂøÖË¶ÅÊÄß</h1>
            <p class="subtitle">ËÉΩÂãïÁöÑ„Éá„Éº„ÇøÈÅ∏Êäû„ÅßÂÆüÈ®ìÂõûÊï∞„ÇíÂäáÁöÑ„Å´ÂâäÊ∏õ</p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 20-25ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ‰∏≠Á¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 7ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 3Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Á¨¨1Á´†ÔºöActive Learning„ÅÆÂøÖË¶ÅÊÄß</h1>
<p><strong>ËÉΩÂãïÁöÑ„Éá„Éº„ÇøÈÅ∏Êäû„ÅßÂÆüÈ®ìÂõûÊï∞„ÇíÂäáÁöÑ„Å´ÂâäÊ∏õ</strong></p>
<h2>Â≠¶ÁøíÁõÆÊ®ô</h2>
<p>„Åì„ÅÆÁ´†„ÇíË™≠„ÇÄ„Åì„Å®„Åß„ÄÅ‰ª•‰∏ã„ÇíÁøíÂæó„Åß„Åç„Åæ„ÅôÔºö</p>
<ul>
<li>‚úÖ Active Learning„ÅÆÂÆöÁæ©„Å®Âà©ÁÇπ„ÇíË™¨Êòé„Åß„Åç„Çã</li>
<li>‚úÖ Query Strategies„ÅÆ4„Å§„ÅÆ‰∏ªË¶ÅÊâãÊ≥ï„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>‚úÖ Êé¢Á¥¢„Å®Ê¥ªÁî®„ÅÆ„Éà„É¨„Éº„Éâ„Ç™„Éï„ÇíË™¨Êòé„Åß„Åç„Çã</li>
<li>‚úÖ ÊùêÊñôÁßëÂ≠¶„Å´„Åä„Åë„ÇãÊàêÂäü‰∫ã‰æã„Çí3„Å§‰ª•‰∏äÊåô„Åí„Çâ„Çå„Çã</li>
<li>‚úÖ „É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞„Å®„ÅÆÂÆöÈáèÁöÑÊØîËºÉ„Åå„Åß„Åç„Çã</li>
</ul>
<p><strong>Ë™≠‰∫ÜÊôÇÈñì</strong>: 20-25ÂàÜ
<strong>„Ç≥„Éº„Éâ‰æã</strong>: 7ÂÄã
<strong>ÊºîÁøíÂïèÈ°å</strong>: 3Âïè</p>
<hr />
<h2>1.1 Active Learning„Å®„ÅØ‰Ωï„Åã</h2>
<h3>ÂÆöÁæ©ÔºöËÉΩÂãïÁöÑ„Éá„Éº„ÇøÈÅ∏Êäû„Å´„Çà„ÇãÂäπÁéáÁöÑÂ≠¶Áøí</h3>
<p><strong>Active LearningÔºàËÉΩÂãïÁöÑÂ≠¶ÁøíÔºâ</strong>„ÅØ„ÄÅÊ©üÊ¢∞Â≠¶Áøí„É¢„Éá„É´„Åå„Äå„Å©„ÅÆ„Éá„Éº„Çø„ÇíÊ¨°„Å´ÂèñÂæó„Åô„Åπ„Åç„Åã„Äç„ÇíËÉΩÂãïÁöÑ„Å´ÈÅ∏Êäû„Åô„Çã„Åì„Å®„Åß„ÄÅÂ∞ë„Å™„ÅÑÂ≠¶Áøí„Éá„Éº„Çø„ÅßÈ´òÁ≤æÂ∫¶„Å™„É¢„Éá„É´„ÇíÊßãÁØâ„Åô„ÇãÊâãÊ≥ï„Åß„Åô„ÄÇ</p>
<p><strong>Passive LearningÔºàÂèóÂãïÁöÑÂ≠¶ÁøíÔºâ„Å®„ÅÆÈÅï„ÅÑ</strong>:</p>
<table>
<thead>
<tr>
<th>È†ÖÁõÆ</th>
<th>Passive Learning</th>
<th>Active Learning</th>
</tr>
</thead>
<tbody>
<tr>
<td>„Éá„Éº„ÇøÈÅ∏Êäû</td>
<td>„É©„É≥„ÉÄ„É† or Êó¢Â≠ò„Éá„Éº„Çø„Çª„ÉÉ„Éà</td>
<td>„É¢„Éá„É´„ÅåËÉΩÂãïÁöÑ„Å´ÈÅ∏Êäû</td>
</tr>
<tr>
<td>Â≠¶ÁøíÂäπÁéá</td>
<td>‰ΩéÔºàÂ§ßÈáè„Éá„Éº„ÇøÂøÖË¶ÅÔºâ</td>
<td>È´òÔºàÂ∞ëÈáè„Éá„Éº„Çø„ÅßÈ´òÁ≤æÂ∫¶Ôºâ</td>
</tr>
<tr>
<td>„Éá„Éº„ÇøÂèñÂæó„Ç≥„Çπ„Éà</td>
<td>ËÄÉÊÖÆ„Åó„Å™„ÅÑ</td>
<td>ËÄÉÊÖÆ„Åô„Çã</td>
</tr>
<tr>
<td>ÈÅ©Áî®Â†¥Èù¢</td>
<td>„Éá„Éº„Çø„ÅåÂÆâ‰æ°</td>
<td>„Éá„Éº„Çø„ÅåÈ´ò‰æ°</td>
</tr>
</tbody>
</table>
<p><strong>ÊùêÊñôÁßëÂ≠¶„Å´„Åä„Åë„ÇãÈáçË¶ÅÊÄß</strong>:
- 1Âõû„ÅÆÂÆüÈ®ì„Å´Êï∞Êó•„ÄúÊï∞ÈÄ±Èñì„Åã„Åã„Çã
- ÂÆüÈ®ì„Ç≥„Çπ„Éà„ÅåÈ´ò„ÅÑÔºàËß¶Â™íÂêàÊàê„ÄÅDFTË®àÁÆó„Å™„Å©Ôºâ
- Êé¢Á¥¢Á©∫Èñì„ÅåÂ∫ÉÂ§ßÔºà10^6„Äú10^60ÈÄö„Çä„ÅÆÂÄôË£úÔºâ</p>
<h3>Active Learning„ÅÆÂü∫Êú¨„Çµ„Ç§„ÇØ„É´</h3>
<div class="mermaid">
flowchart LR
    A[ÂàùÊúü„Éá„Éº„Çø\nÂ∞ëÊï∞„ÅÆ„Çµ„É≥„Éó„É´] --> B[„É¢„Éá„É´Â≠¶Áøí\n‰∫àÊ∏¨„É¢„Éá„É´ÊßãÁØâ]
    B --> C[ÂÄôË£úË©ï‰æ°\nQuery Strategy]
    C --> D[ÊúÄ„ÇÇÊúâÁõä„Å™\n„Çµ„É≥„Éó„É´„ÇíÈÅ∏Êäû]
    D --> E[ÂÆüÈ®ìÂÆüË°å\n„Éá„Éº„ÇøÂèñÂæó]
    E --> F{ÁµÇ‰∫ÜÊù°‰ª∂?\nÁõÆÊ®ôÈÅîÊàê or\n‰∫àÁÆó‰∏äÈôê}
    F -->|„ÅÑ„ÅÑ„Åà| B
    F -->|„ÅØ„ÅÑ| G[ÊúÄÁµÇ„É¢„Éá„É´]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#ffebee
    style G fill:#4CAF50,color:#fff
</div>

<p><strong>„Ç≠„Éº„Éù„Ç§„É≥„Éà</strong>:
1. <strong>Â∞ëÊï∞„ÅÆÂàùÊúü„Éá„Éº„Çø</strong>„ÅßÈñãÂßãÔºàÈÄöÂ∏∏10-20„Çµ„É≥„Éó„É´Ôºâ
2. <strong>Query Strategy</strong>„ÅßÊ¨°„ÅÆ„Çµ„É≥„Éó„É´„ÇíË≥¢„ÅèÈÅ∏Êäû
3. <strong>ÂÆüÈ®ìÂÆüË°å</strong>„Åó„Å¶„Éá„Éº„Çø„Çí1„Å§„Åö„Å§ËøΩÂä†
4. <strong>„É¢„Éá„É´Êõ¥Êñ∞</strong>„ÇíÁπ∞„ÇäËøî„Åô
5. <strong>ÁõÆÊ®ôÈÅîÊàê</strong>„Åæ„ÅßÁ∂ôÁ∂ö</p>
<hr />
<h2>1.2 Query Strategies„ÅÆÂü∫Á§é</h2>
<h3>1.2.1 Uncertainty SamplingÔºà‰∏çÁ¢∫ÂÆüÊÄß„Çµ„É≥„Éó„É™„É≥„Ç∞Ôºâ</h3>
<p><strong>ÂéüÁêÜ</strong>: „É¢„Éá„É´„ÅÆ‰∫àÊ∏¨„ÅåÊúÄ„ÇÇ‰∏çÁ¢∫ÂÆü„Å™„Çµ„É≥„Éó„É´„ÇíÈÅ∏Êäû</p>
<p><strong>Êï∞Âºè</strong>:
$$
x^* = \arg\max_{x \in \mathcal{U}} \text{Uncertainty}(x)
$$</p>
<p>„Åì„Åì„Åß„ÄÅ$\mathcal{U}$„ÅØ„É©„Éô„É´Êú™ÂèñÂæó„ÅÆ„Çµ„É≥„Éó„É´ÈõÜÂêà</p>
<p><strong>‰∏çÁ¢∫ÂÆüÊÄß„ÅÆÊ∏¨ÂÆöÊñπÊ≥ï</strong>:</p>
<p><strong>ÂõûÂ∏∞ÂïèÈ°å</strong>:
$$
\text{Uncertainty}(x) = \sigma(x)
$$
Ôºà‰∫àÊ∏¨„ÅÆÊ®ôÊ∫ñÂÅèÂ∑ÆÔºâ</p>
<p><strong>ÂàÜÈ°ûÂïèÈ°åÔºà2„ÇØ„É©„ÇπÔºâ</strong>:
$$
\text{Uncertainty}(x) = 1 - |P(y=1|x) - P(y=0|x)|
$$
ÔºàÁ¢∫Áéá„ÅÆÂ∑Æ„ÅÆÁµ∂ÂØæÂÄ§„ÅÆÈÄÜÊï∞„ÄÅ0.5„Å´Ëøë„ÅÑ„Åª„Å©‰∏çÁ¢∫ÂÆüÔºâ</p>
<p><strong>„Ç≥„Éº„Éâ‰æã1: Uncertainty Sampling„ÅÆÂÆüË£Ö</strong></p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression

# „Éá„Éº„ÇøÁîüÊàêÔºàÊùêÊñôÁâπÊÄß‰∫àÊ∏¨„ÇíÊÉ≥ÂÆöÔºâ
np.random.seed(42)
X, y = make_regression(
    n_samples=500,
    n_features=3,
    noise=10,
    random_state=42
)

# ÂàùÊúü„Éá„Éº„ÇøÔºà10„Çµ„É≥„Éó„É´Ôºâ
initial_indices = np.random.choice(len(X), 10, replace=False)
X_train = X[initial_indices]
y_train = y[initial_indices]

# Êú™„É©„Éô„É´„Éá„Éº„Çø
unlabeled_mask = np.ones(len(X), dtype=bool)
unlabeled_mask[initial_indices] = False
X_unlabeled = X[unlabeled_mask]
y_unlabeled = y[unlabeled_mask]

def uncertainty_sampling(
    X_train,
    y_train,
    X_unlabeled,
    n_queries=5
):
    &quot;&quot;&quot;
    Uncertainty Sampling„Å´„Çà„Çã„Çµ„É≥„Éó„É´ÈÅ∏Êäû

    Parameters:
    -----------
    X_train : array
        Ë®ìÁ∑¥„Éá„Éº„Çø
    y_train : array
        Ë®ìÁ∑¥„É©„Éô„É´
    X_unlabeled : array
        Êú™„É©„Éô„É´„Éá„Éº„Çø
    n_queries : int
        ÈÅ∏Êäû„Åô„Çã„Çµ„É≥„Éó„É´Êï∞

    Returns:
    --------
    selected_indices : array
        ÈÅ∏Êäû„Åï„Çå„Åü„Çµ„É≥„Éó„É´„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ
    &quot;&quot;&quot;
    # Random Forest„Åß‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆöÔºà‰∫àÊ∏¨ÂàÜÊï£Ôºâ
    rf = RandomForestRegressor(
        n_estimators=100,
        random_state=42
    )
    rf.fit(X_train, y_train)

    # ÂêÑÊ±∫ÂÆöÊú®„ÅÆ‰∫àÊ∏¨„ÇíÂèñÂæó
    predictions = np.array([
        tree.predict(X_unlabeled)
        for tree in rf.estimators_
    ])

    # ‰∫àÊ∏¨„ÅÆÊ®ôÊ∫ñÂÅèÂ∑ÆÔºà‰∏çÁ¢∫ÂÆüÊÄßÔºâ„ÇíË®àÁÆó
    uncertainties = np.std(predictions, axis=0)

    # ‰∏çÁ¢∫ÂÆüÊÄß„ÅåÊúÄ„ÇÇÈ´ò„ÅÑ„Çµ„É≥„Éó„É´„ÇíÈÅ∏Êäû
    selected_indices = np.argsort(uncertainties)[-n_queries:]

    return selected_indices, uncertainties

# Uncertainty Sampling„ÇíÂÆüË°å
selected_idx, uncertainties = uncertainty_sampling(
    X_train,
    y_train,
    X_unlabeled,
    n_queries=5
)

print(&quot;Uncertainty Sampling„ÅÆÁµêÊûú:&quot;)
print(f&quot;ÈÅ∏Êäû„Åï„Çå„Åü„Çµ„É≥„Éó„É´Êï∞: {len(selected_idx)}&quot;)
print(f&quot;‰∏çÁ¢∫ÂÆüÊÄß„ÅÆÁØÑÂõ≤: {uncertainties.min():.2f} - &quot;
      f&quot;{uncertainties.max():.2f}&quot;)
print(f&quot;ÈÅ∏Êäû„Åï„Çå„Åü„Çµ„É≥„Éó„É´„ÅÆ‰∏çÁ¢∫ÂÆüÊÄß:&quot;)
for i, idx in enumerate(selected_idx):
    print(f&quot;  „Çµ„É≥„Éó„É´ {idx}: {uncertainties[idx]:.2f}&quot;)
</code></pre>
<p><strong>Âá∫Âäõ</strong>:</p>
<pre><code>Uncertainty Sampling„ÅÆÁµêÊûú:
ÈÅ∏Êäû„Åï„Çå„Åü„Çµ„É≥„Éó„É´Êï∞: 5
‰∏çÁ¢∫ÂÆüÊÄß„ÅÆÁØÑÂõ≤: 2.13 - 18.45
ÈÅ∏Êäû„Åï„Çå„Åü„Çµ„É≥„Éó„É´„ÅÆ‰∏çÁ¢∫ÂÆüÊÄß:
  „Çµ„É≥„Éó„É´ 234: 16.82
  „Çµ„É≥„Éó„É´ 67: 17.23
  „Çµ„É≥„Éó„É´ 412: 17.56
  „Çµ„É≥„Éó„É´ 189: 17.91
  „Çµ„É≥„Éó„É´ 345: 18.45
</code></pre>
<p><strong>Âà©ÁÇπ</strong>:
- ‚úÖ „Ç∑„É≥„Éó„É´„ÅßÁõ¥ÊÑüÁöÑ
- ‚úÖ Ë®àÁÆó„Ç≥„Çπ„Éà„Åå‰Ωé„ÅÑ
- ‚úÖ Â§ö„Åè„ÅÆÂïèÈ°å„ÅßÊúâÂäπ</p>
<p><strong>Ê¨†ÁÇπ</strong>:
- ‚ö†Ô∏è Êé¢Á¥¢Á©∫Èñì„ÅÆÂ§öÊßòÊÄß„ÇíËÄÉÊÖÆ„Åó„Å™„ÅÑ
- ‚ö†Ô∏è Â±ÄÊâÄÁöÑ„Å™È†òÂüü„Å´ÂÅè„ÇãÂèØËÉΩÊÄß</p>
<hr />
<h3>1.2.2 Diversity SamplingÔºàÂ§öÊßòÊÄß„Çµ„É≥„Éó„É™„É≥„Ç∞Ôºâ</h3>
<p><strong>ÂéüÁêÜ</strong>: Êó¢Â≠ò„Éá„Éº„Çø„Å®Áï∞„Å™„ÇãÔºàÂ§öÊßò„Å™Ôºâ„Çµ„É≥„Éó„É´„ÇíÈÅ∏Êäû</p>
<p><strong>Êï∞Âºè</strong>:
$$
x^* = \arg\max_{x \in \mathcal{U}} \min_{x_i \in \mathcal{L}} d(x, x_i)
$$</p>
<p>„Åì„Åì„Åß„ÄÅ$\mathcal{L}$„ÅØ„É©„Éô„É´ÂèñÂæóÊ∏à„Åø„ÅÆ„Çµ„É≥„Éó„É´ÈõÜÂêà„ÄÅ$d(\cdot, \cdot)$„ÅØË∑ùÈõ¢Èñ¢Êï∞</p>
<p><strong>Ë∑ùÈõ¢„ÅÆÊ∏¨ÂÆöÊñπÊ≥ï</strong>:
- „É¶„Éº„ÇØ„É™„ÉÉ„ÉâË∑ùÈõ¢: $d(x_i, x_j) = |x_i - x_j|_2$
- „Éû„Éè„É©„Éé„Éì„ÇπË∑ùÈõ¢: $d(x_i, x_j) = \sqrt{(x_i - x_j)^T \Sigma^{-1} (x_i - x_j)}$
- „Ç≥„Çµ„Ç§„É≥Ë∑ùÈõ¢: $d(x_i, x_j) = 1 - \frac{x_i \cdot x_j}{|x_i| |x_j|}$</p>
<p><strong>„Ç≥„Éº„Éâ‰æã2: Diversity Sampling„ÅÆÂÆüË£Ö</strong></p>
<pre><code class="language-python">from sklearn.metrics import pairwise_distances

def diversity_sampling(
    X_train,
    X_unlabeled,
    n_queries=5,
    metric='euclidean'
):
    &quot;&quot;&quot;
    Diversity Sampling„Å´„Çà„Çã„Çµ„É≥„Éó„É´ÈÅ∏Êäû

    Parameters:
    -----------
    X_train : array
        Ë®ìÁ∑¥„Éá„Éº„Çø
    X_unlabeled : array
        Êú™„É©„Éô„É´„Éá„Éº„Çø
    n_queries : int
        ÈÅ∏Êäû„Åô„Çã„Çµ„É≥„Éó„É´Êï∞
    metric : str
        Ë∑ùÈõ¢ÊåáÊ®ôÔºà'euclidean', 'cosine'Á≠âÔºâ

    Returns:
    --------
    selected_indices : array
        ÈÅ∏Êäû„Åï„Çå„Åü„Çµ„É≥„Éó„É´„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ
    &quot;&quot;&quot;
    selected_indices = []

    # n_queries„Çµ„É≥„Éó„É´ÈÅ∏Êäû„Åô„Çã„Åæ„ÅßÁπ∞„ÇäËøî„Åó
    for _ in range(n_queries):
        if len(selected_indices) == 0:
            # ÊúÄÂàù„ÅØË®ìÁ∑¥„Éá„Éº„Çø„Å®„ÅÆË∑ùÈõ¢„ÇíË®àÁÆó
            distances = pairwise_distances(
                X_unlabeled,
                X_train,
                metric=metric
            )
        else:
            # ÈÅ∏ÊäûÊ∏à„Åø„Çµ„É≥„Éó„É´„ÇÇÂê´„ÇÅ„Å¶Ë∑ùÈõ¢„ÇíË®àÁÆó
            X_selected = X_unlabeled[selected_indices]
            X_reference = np.vstack([X_train, X_selected])
            distances = pairwise_distances(
                X_unlabeled,
                X_reference,
                metric=metric
            )

        # ÂêÑÊú™„É©„Éô„É´„Çµ„É≥„Éó„É´„ÅÆ„ÄÅÊúÄ„ÇÇËøë„ÅÑ„Çµ„É≥„Éó„É´„Åæ„Åß„ÅÆË∑ùÈõ¢
        min_distances = distances.min(axis=1)

        # Êó¢„Å´ÈÅ∏ÊäûÊ∏à„Åø„ÅÆ„Çµ„É≥„Éó„É´„ÅØÈô§Â§ñ
        min_distances[selected_indices] = -np.inf

        # ÊúÄ„ÇÇÈÅ†„ÅÑ„Çµ„É≥„Éó„É´„ÇíÈÅ∏Êäû
        next_idx = np.argmax(min_distances)
        selected_indices.append(next_idx)

    return np.array(selected_indices), min_distances

# Diversity Sampling„ÇíÂÆüË°å
selected_idx, min_distances = diversity_sampling(
    X_train,
    X_unlabeled,
    n_queries=5
)

print(&quot;\nDiversity Sampling„ÅÆÁµêÊûú:&quot;)
print(f&quot;ÈÅ∏Êäû„Åï„Çå„Åü„Çµ„É≥„Éó„É´Êï∞: {len(selected_idx)}&quot;)
print(f&quot;Ë®ìÁ∑¥„Éá„Éº„Çø„Åã„Çâ„ÅÆÊúÄÂ∞èË∑ùÈõ¢:&quot;)
for i, idx in enumerate(selected_idx):
    print(f&quot;  „Çµ„É≥„Éó„É´ {idx}: {min_distances[idx]:.2f}&quot;)
</code></pre>
<p><strong>Âá∫Âäõ</strong>:</p>
<pre><code>Diversity Sampling„ÅÆÁµêÊûú:
ÈÅ∏Êäû„Åï„Çå„Åü„Çµ„É≥„Éó„É´Êï∞: 5
Ë®ìÁ∑¥„Éá„Éº„Çø„Åã„Çâ„ÅÆÊúÄÂ∞èË∑ùÈõ¢:
  „Çµ„É≥„Éó„É´ 123: 12.34
  „Çµ„É≥„Éó„É´ 456: 11.89
  „Çµ„É≥„Éó„É´ 78: 10.56
  „Çµ„É≥„Éó„É´ 234: 9.87
  „Çµ„É≥„Éó„É´ 345: 9.23
</code></pre>
<p><strong>Âà©ÁÇπ</strong>:
- ‚úÖ Êé¢Á¥¢Á©∫Èñì„ÅÆÂ∫ÉÁØÑÂõ≤„Çí„Ç´„Éê„Éº
- ‚úÖ Â±ÄÊâÄËß£„Å∏„ÅÆÂÅè„Çä„ÇíÈò≤„Åê
- ‚úÖ „ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Å®„ÅÆÁõ∏ÊÄß„ÅåËâØ„ÅÑ</p>
<p><strong>Ê¨†ÁÇπ</strong>:
- ‚ö†Ô∏è „É¢„Éá„É´„ÅÆ‰∏çÁ¢∫ÂÆüÊÄß„ÇíËÄÉÊÖÆ„Åó„Å™„ÅÑ
- ‚ö†Ô∏è Ë®àÁÆó„Ç≥„Çπ„Éà„Åå„ÇÑ„ÇÑÈ´ò„ÅÑ</p>
<hr />
<h3>1.2.3 Query-by-Committee</h3>
<p><strong>ÂéüÁêÜ</strong>: Ë§áÊï∞„ÅÆ„É¢„Éá„É´ÔºàÂßîÂì°‰ºöÔºâ„ÅÆÊÑèË¶ã„ÅåÊúÄ„ÇÇÂàÜ„Åã„Çå„Çã„Çµ„É≥„Éó„É´„ÇíÈÅ∏Êäû</p>
<p><strong>Êï∞Âºè</strong>:
$$
x^* = \arg\max_{x \in \mathcal{U}} \text{Disagreement}(C, x)
$$</p>
<p>„Åì„Åì„Åß„ÄÅ$C = {M_1, M_2, ..., M_K}$„ÅØ„É¢„Éá„É´„ÅÆÈõÜÂêàÔºàÂßîÂì°‰ºöÔºâ</p>
<p><strong>ÊÑèË¶ã„ÅÆ‰∏ç‰∏ÄËá¥„ÅÆÊ∏¨ÂÆö</strong>:</p>
<p><strong>ÂõûÂ∏∞ÂïèÈ°åÔºàÂàÜÊï£Ôºâ</strong>:
$$
\text{Disagreement}(C, x) = \frac{1}{K} \sum_{k=1}^K (M_k(x) - \bar{M}(x))^2
$$</p>
<p><strong>ÂàÜÈ°ûÂïèÈ°åÔºàKullback-Leibler DivergenceÔºâ</strong>:
$$
\text{Disagreement}(C, x) = \frac{1}{K} \sum_{k=1}^K KL(P_k(\cdot|x) | P_C(\cdot|x))
$$</p>
<p><strong>„Ç≥„Éº„Éâ‰æã3: Query-by-Committee„ÅÆÂÆüË£Ö</strong></p>
<pre><code class="language-python">from sklearn.ensemble import (
    RandomForestRegressor,
    GradientBoostingRegressor
)
from sklearn.linear_model import Ridge
from sklearn.neural_network import MLPRegressor

def query_by_committee(
    X_train,
    y_train,
    X_unlabeled,
    n_queries=5
):
    &quot;&quot;&quot;
    Query-by-Committee„Å´„Çà„Çã„Çµ„É≥„Éó„É´ÈÅ∏Êäû

    Parameters:
    -----------
    X_train : array
        Ë®ìÁ∑¥„Éá„Éº„Çø
    y_train : array
        Ë®ìÁ∑¥„É©„Éô„É´
    X_unlabeled : array
        Êú™„É©„Éô„É´„Éá„Éº„Çø
    n_queries : int
        ÈÅ∏Êäû„Åô„Çã„Çµ„É≥„Éó„É´Êï∞

    Returns:
    --------
    selected_indices : array
        ÈÅ∏Êäû„Åï„Çå„Åü„Çµ„É≥„Éó„É´„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ
    &quot;&quot;&quot;
    # ÂßîÂì°‰ºöÔºàÁï∞„Å™„Çã„É¢„Éá„É´„ÅÆÈõÜÂêàÔºâ
    committee = [
        RandomForestRegressor(n_estimators=50, random_state=42),
        GradientBoostingRegressor(n_estimators=50, random_state=42),
        Ridge(alpha=1.0),
        MLPRegressor(
            hidden_layer_sizes=(50,),
            max_iter=500,
            random_state=42
        )
    ]

    # ÂêÑ„É¢„Éá„É´„ÇíË®ìÁ∑¥
    for model in committee:
        model.fit(X_train, y_train)

    # ÂêÑ„É¢„Éá„É´„ÅÆ‰∫àÊ∏¨„ÇíÂèñÂæó
    predictions = np.array([
        model.predict(X_unlabeled)
        for model in committee
    ])

    # ‰∫àÊ∏¨„ÅÆÂàÜÊï£ÔºàÊÑèË¶ã„ÅÆ‰∏ç‰∏ÄËá¥Ôºâ„ÇíË®àÁÆó
    disagreement = np.var(predictions, axis=0)

    # ‰∏ç‰∏ÄËá¥„ÅåÊúÄ„ÇÇÂ§ß„Åç„ÅÑ„Çµ„É≥„Éó„É´„ÇíÈÅ∏Êäû
    selected_indices = np.argsort(disagreement)[-n_queries:]

    return selected_indices, disagreement

# Query-by-Committee„ÇíÂÆüË°å
selected_idx, disagreement = query_by_committee(
    X_train,
    y_train,
    X_unlabeled,
    n_queries=5
)

print(&quot;\nQuery-by-Committee„ÅÆÁµêÊûú:&quot;)
print(f&quot;ÈÅ∏Êäû„Åï„Çå„Åü„Çµ„É≥„Éó„É´Êï∞: {len(selected_idx)}&quot;)
print(f&quot;ÊÑèË¶ã„ÅÆ‰∏ç‰∏ÄËá¥„ÅÆÁØÑÂõ≤: {disagreement.min():.2f} - &quot;
      f&quot;{disagreement.max():.2f}&quot;)
print(f&quot;ÈÅ∏Êäû„Åï„Çå„Åü„Çµ„É≥„Éó„É´„ÅÆ‰∏ç‰∏ÄËá¥:&quot;)
for i, idx in enumerate(selected_idx):
    print(f&quot;  „Çµ„É≥„Éó„É´ {idx}: {disagreement[idx]:.2f}&quot;)
</code></pre>
<p><strong>Âá∫Âäõ</strong>:</p>
<pre><code>Query-by-Committee„ÅÆÁµêÊûú:
ÈÅ∏Êäû„Åï„Çå„Åü„Çµ„É≥„Éó„É´Êï∞: 5
ÊÑèË¶ã„ÅÆ‰∏ç‰∏ÄËá¥„ÅÆÁØÑÂõ≤: 5.23 - 142.56
ÈÅ∏Êäû„Åï„Çå„Åü„Çµ„É≥„Éó„É´„ÅÆ‰∏ç‰∏ÄËá¥:
  „Çµ„É≥„Éó„É´ 89: 128.34
  „Çµ„É≥„Éó„É´ 234: 132.45
  „Çµ„É≥„Éó„É´ 156: 135.67
  „Çµ„É≥„Éó„É´ 401: 139.12
  „Çµ„É≥„Éó„É´ 267: 142.56
</code></pre>
<p><strong>Âà©ÁÇπ</strong>:
- ‚úÖ Â§öÊßò„Å™„É¢„Éá„É´„ÅÆÁü•Ë≠ò„ÇíÊ¥ªÁî®
- ‚úÖ „É¢„Éá„É´„Éê„Ç§„Ç¢„Çπ„ÇíËªΩÊ∏õ
- ‚úÖ ‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö„ÅåÂ†ÖÁâ¢</p>
<p><strong>Ê¨†ÁÇπ</strong>:
- ‚ö†Ô∏è Ë®àÁÆó„Ç≥„Çπ„Éà„ÅåÈ´ò„ÅÑÔºàË§áÊï∞„É¢„Éá„É´Ë®ìÁ∑¥Ôºâ
- ‚ö†Ô∏è „É¢„Éá„É´ÈÅ∏Êäû„Å´‰æùÂ≠ò</p>
<hr />
<h3>1.2.4 Expected Model Change</h3>
<p><strong>ÂéüÁêÜ</strong>: „É¢„Éá„É´„ÅÆ„Éë„É©„É°„Éº„Çø„ÇíÊúÄ„ÇÇÂ§ß„Åç„ÅèÂ§âÂåñ„Åï„Åõ„Çã„Çµ„É≥„Éó„É´„ÇíÈÅ∏Êäû</p>
<p><strong>Êï∞Âºè</strong>ÔºàÂãæÈÖç„Éô„Éº„ÇπÔºâ:
$$
x^* = \arg\max_{x \in \mathcal{U}} |\nabla_\theta \mathcal{L}(\theta; x, \hat{y})|
$$</p>
<p>„Åì„Åì„Åß„ÄÅ$\theta$„ÅØ„É¢„Éá„É´„Éë„É©„É°„Éº„Çø„ÄÅ$\mathcal{L}$„ÅØÊêçÂ§±Èñ¢Êï∞„ÄÅ$\hat{y}$„ÅØ‰∫àÊ∏¨ÂÄ§</p>
<p><strong>Âà©ÁÇπ</strong>:
- ‚úÖ „É¢„Éá„É´ÊîπÂñÑ„Å∏„ÅÆÂΩ±ÈüøÂ∫¶„ÇíÁõ¥Êé•Ë©ï‰æ°
- ‚úÖ ÂäπÁéáÁöÑ„Å™Â≠¶Áøí„ÅåÂèØËÉΩ</p>
<p><strong>Ê¨†ÁÇπ</strong>:
- ‚ö†Ô∏è Ë®àÁÆó„Ç≥„Çπ„Éà„ÅåÈ´ò„ÅÑ
- ‚ö†Ô∏è ÂãæÈÖçË®àÁÆóÂèØËÉΩ„Å™„É¢„Éá„É´„Å´ÈôêÂÆö</p>
<hr />
<h2>1.3 Exploration vs Exploitation</h2>
<h3>„Éà„É¨„Éº„Éâ„Ç™„Éï„ÅÆÊ¶ÇÂøµ</h3>
<p>Active Learning„Å´„Åä„Åë„ÇãÊúÄ„ÇÇÈáçË¶Å„Å™Ê¶ÇÂøµ„ÅÆ1„Å§„Åå„ÄÅ<strong>ExplorationÔºàÊé¢Á¥¢Ôºâ„Å®ExploitationÔºàÊ¥ªÁî®Ôºâ„ÅÆ„Éà„É¨„Éº„Éâ„Ç™„Éï</strong>„Åß„Åô„ÄÇ</p>
<p><strong>ExplorationÔºàÊé¢Á¥¢Ôºâ</strong>:
- Êú™Áü•„ÅÆÈ†òÂüü„ÇíÊé¢Á¥¢
- Â§öÊßò„Å™„Çµ„É≥„Éó„É´„ÇíÂèéÈõÜ
- Êñ∞„Åó„ÅÑÊÉÖÂ†±„ÇíÁç≤Âæó
- „É™„Çπ„ÇØ„ÇíÂèñ„Çã</p>
<p><strong>ExploitationÔºàÊ¥ªÁî®Ôºâ</strong>:
- Êó¢Áü•„ÅÆËâØ„ÅÑÈ†òÂüü„ÇíÈõÜ‰∏≠ÁöÑ„Å´Ë™øÊüª
- „É¢„Éá„É´„ÅÆ‰∏çÁ¢∫ÂÆüÊÄß„ÅåÈ´ò„ÅÑÈ†òÂüü„ÇíÂÑ™ÂÖà
- Êó¢Â≠òÁü•Ë≠ò„ÇíÊúÄÂ§ßÈôêÊ¥ªÁî®
- ÂÆâÂÖ®„Å´ÊîπÂñÑ</p>
<h3>„Éà„É¨„Éº„Éâ„Ç™„Éï„ÅÆÂèØË¶ñÂåñ</h3>
<div class="mermaid">
flowchart TB
    subgraph Êé¢Á¥¢ÈáçË¶ñ
    A[Êú™Áü•È†òÂüü„Çí\nÁ©çÊ•µÁöÑ„Å´„Çµ„É≥„Éó„É™„É≥„Ç∞]
    A --> B[Êñ∞Áô∫Ë¶ã„ÅÆÂèØËÉΩÊÄßÂ§ß]
    A --> C[Â≠¶Áøí„ÅØÈÅÖ„ÅÑ]
    end

    subgraph Ê¥ªÁî®ÈáçË¶ñ
    D[‰∏çÁ¢∫ÂÆüÊÄß„ÅåÈ´ò„ÅÑÈ†òÂüü„Çí\nÈõÜ‰∏≠ÁöÑ„Å´„Çµ„É≥„Éó„É™„É≥„Ç∞]
    D --> E[È´òÈÄü„Å´ÂèéÊùü]
    D --> F[Â±ÄÊâÄËß£„Å´\n„ÅØ„Åæ„Çã„É™„Çπ„ÇØ]
    end

    subgraph „Éê„É©„É≥„Çπ
    G[ÈÅ©Â∫¶„Å™Êé¢Á¥¢„Å®Ê¥ªÁî®]
    G --> H[ÂäπÁéáÁöÑ„Å™Â≠¶Áøí]
    G --> I[Â∫ÉÁØÑÂõ≤„Åã„Å§\nÊ∑±„ÅÑÁêÜËß£]
    end

    style A fill:#e3f2fd
    style D fill:#fff3e0
    style G fill:#e8f5e9
    style I fill:#4CAF50,color:#fff
</div>

<h3>Œµ-greedy„Ç¢„Éó„É≠„Éº„ÉÅ</h3>
<p><strong>ÂéüÁêÜ</strong>: Á¢∫Áéá$\epsilon$„ÅßÊé¢Á¥¢„ÄÅÁ¢∫Áéá$1-\epsilon$„ÅßÊ¥ªÁî®</p>
<p><strong>„Ç¢„É´„Ç¥„É™„Ç∫„É†</strong>:</p>
<pre><code>Á¢∫Áéá Œµ „Åß:
    „É©„É≥„ÉÄ„É†„Å´„Çµ„É≥„Éó„É´„ÇíÈÅ∏ÊäûÔºàÊé¢Á¥¢Ôºâ
Á¢∫Áéá 1-Œµ „Åß:
    Query Strategy„ÅßÊúÄËâØ„Çµ„É≥„Éó„É´„ÇíÈÅ∏ÊäûÔºàÊ¥ªÁî®Ôºâ
</code></pre>
<p><strong>„Ç≥„Éº„Éâ‰æã4: Œµ-greedy Active Learning</strong></p>
<pre><code class="language-python">def epsilon_greedy_sampling(
    X_train,
    y_train,
    X_unlabeled,
    n_queries=5,
    epsilon=0.2
):
    &quot;&quot;&quot;
    Œµ-greedy Active Learning

    Parameters:
    -----------
    X_train : array
        Ë®ìÁ∑¥„Éá„Éº„Çø
    y_train : array
        Ë®ìÁ∑¥„É©„Éô„É´
    X_unlabeled : array
        Êú™„É©„Éô„É´„Éá„Éº„Çø
    n_queries : int
        ÈÅ∏Êäû„Åô„Çã„Çµ„É≥„Éó„É´Êï∞
    epsilon : float
        Êé¢Á¥¢Á¢∫ÁéáÔºà0-1Ôºâ

    Returns:
    --------
    selected_indices : array
        ÈÅ∏Êäû„Åï„Çå„Åü„Çµ„É≥„Éó„É´„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ
    &quot;&quot;&quot;
    selected_indices = []

    for _ in range(n_queries):
        if np.random.rand() &lt; epsilon:
            # Êé¢Á¥¢Ôºö„É©„É≥„ÉÄ„É†„Å´„Çµ„É≥„Éó„É´ÈÅ∏Êäû
            available = [
                i for i in range(len(X_unlabeled))
                if i not in selected_indices
            ]
            idx = np.random.choice(available)
            strategy = &quot;Êé¢Á¥¢&quot;
        else:
            # Ê¥ªÁî®ÔºöUncertainty Sampling„ÅßÈÅ∏Êäû
            available_mask = np.ones(len(X_unlabeled), dtype=bool)
            available_mask[selected_indices] = False
            X_available = X_unlabeled[available_mask]

            rf = RandomForestRegressor(
                n_estimators=50,
                random_state=42
            )
            rf.fit(X_train, y_train)

            predictions = np.array([
                tree.predict(X_available)
                for tree in rf.estimators_
            ])
            uncertainties = np.std(predictions, axis=0)

            # Âà©Áî®ÂèØËÉΩ„Å™„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Åã„ÇâÈÅ∏Êäû
            available_indices = np.where(available_mask)[0]
            idx = available_indices[np.argmax(uncertainties)]
            strategy = &quot;Ê¥ªÁî®&quot;

        selected_indices.append(idx)
        print(f&quot;„Ç§„ÉÜ„É¨„Éº„Ç∑„Éß„É≥ {len(selected_indices)}: &quot;
              f&quot;„Çµ„É≥„Éó„É´ {idx} „ÇíÈÅ∏ÊäûÔºà{strategy}Ôºâ&quot;)

    return np.array(selected_indices)

# Œµ-greedy Active Learning„ÇíÂÆüË°å
print(&quot;\nŒµ-greedy Active LearningÔºàŒµ=0.2Ôºâ:&quot;)
selected_idx = epsilon_greedy_sampling(
    X_train,
    y_train,
    X_unlabeled,
    n_queries=5,
    epsilon=0.2
)
</code></pre>
<p><strong>Âá∫Âäõ</strong>:</p>
<pre><code>Œµ-greedy Active LearningÔºàŒµ=0.2Ôºâ:
„Ç§„ÉÜ„É¨„Éº„Ç∑„Éß„É≥ 1: „Çµ„É≥„Éó„É´ 234 „ÇíÈÅ∏ÊäûÔºàÊ¥ªÁî®Ôºâ
„Ç§„ÉÜ„É¨„Éº„Ç∑„Éß„É≥ 2: „Çµ„É≥„Éó„É´ 456 „ÇíÈÅ∏ÊäûÔºàÊ¥ªÁî®Ôºâ
„Ç§„ÉÜ„É¨„Éº„Ç∑„Éß„É≥ 3: „Çµ„É≥„Éó„É´ 123 „ÇíÈÅ∏ÊäûÔºàÊé¢Á¥¢Ôºâ
„Ç§„ÉÜ„É¨„Éº„Ç∑„Éß„É≥ 4: „Çµ„É≥„Éó„É´ 345 „ÇíÈÅ∏ÊäûÔºàÊ¥ªÁî®Ôºâ
„Ç§„ÉÜ„É¨„Éº„Ç∑„Éß„É≥ 5: „Çµ„É≥„Éó„É´ 78 „ÇíÈÅ∏ÊäûÔºàÊ¥ªÁî®Ôºâ
</code></pre>
<p><strong>Œµ„ÅÆÈÅ∏Êäû</strong>:
- $\epsilon = 0$: ÂÆåÂÖ®„Å™Ê¥ªÁî®ÔºàÂ±ÄÊâÄËß£„ÅÆ„É™„Çπ„ÇØÔºâ
- $\epsilon = 1$: ÂÆåÂÖ®„Å™Êé¢Á¥¢Ôºà„É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞Ôºâ
- $\epsilon = 0.1 \sim 0.2$: „Éê„É©„É≥„ÇπËâØÂ•ΩÔºàÊé®Â•®Ôºâ</p>
<hr />
<h3>Upper Confidence Bound (UCB)</h3>
<p><strong>ÂéüÁêÜ</strong>: ‰∫àÊ∏¨Âπ≥Âùá + ‰∏çÁ¢∫ÂÆüÊÄß„Éú„Éº„Éä„Çπ</p>
<p><strong>Êï∞Âºè</strong>:
$$
\text{UCB}(x) = \mu(x) + \kappa \sigma(x)
$$</p>
<ul>
<li>$\mu(x)$: ‰∫àÊ∏¨Âπ≥Âùá</li>
<li>$\sigma(x)$: ‰∫àÊ∏¨Ê®ôÊ∫ñÂÅèÂ∑Æ</li>
<li>$\kappa$: Êé¢Á¥¢„Éë„É©„É°„Éº„ÇøÔºàÈÄöÂ∏∏1.0„Äú3.0Ôºâ</li>
</ul>
<p><strong>„Ç≥„Éº„Éâ‰æã5: UCB„Å´„Çà„Çã„Çµ„É≥„Éó„É´ÈÅ∏Êäû</strong></p>
<pre><code class="language-python">def ucb_sampling(
    X_train,
    y_train,
    X_unlabeled,
    n_queries=5,
    kappa=2.0
):
    &quot;&quot;&quot;
    UCBÔºàUpper Confidence BoundÔºâ„Å´„Çà„Çã„Çµ„É≥„Éó„É´ÈÅ∏Êäû

    Parameters:
    -----------
    X_train : array
        Ë®ìÁ∑¥„Éá„Éº„Çø
    y_train : array
        Ë®ìÁ∑¥„É©„Éô„É´
    X_unlabeled : array
        Êú™„É©„Éô„É´„Éá„Éº„Çø
    n_queries : int
        ÈÅ∏Êäû„Åô„Çã„Çµ„É≥„Éó„É´Êï∞
    kappa : float
        Êé¢Á¥¢„Éë„É©„É°„Éº„Çø

    Returns:
    --------
    selected_indices : array
        ÈÅ∏Êäû„Åï„Çå„Åü„Çµ„É≥„Éó„É´„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ
    &quot;&quot;&quot;
    # Random Forest„Åß‰∫àÊ∏¨
    rf = RandomForestRegressor(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)

    # ‰∫àÊ∏¨Âπ≥Âùá„Å®Ê®ôÊ∫ñÂÅèÂ∑Æ
    predictions = np.array([
        tree.predict(X_unlabeled)
        for tree in rf.estimators_
    ])
    mean_pred = np.mean(predictions, axis=0)
    std_pred = np.std(predictions, axis=0)

    # UCB„Çπ„Ç≥„Ç¢„ÇíË®àÁÆó
    ucb_scores = mean_pred + kappa * std_pred

    # UCB„ÅåÊúÄ„ÇÇÈ´ò„ÅÑ„Çµ„É≥„Éó„É´„ÇíÈÅ∏Êäû
    selected_indices = np.argsort(ucb_scores)[-n_queries:]

    return selected_indices, ucb_scores, mean_pred, std_pred

# UCB„Çµ„É≥„Éó„É™„É≥„Ç∞„ÇíÂÆüË°å
selected_idx, ucb_scores, mean_pred, std_pred = ucb_sampling(
    X_train,
    y_train,
    X_unlabeled,
    n_queries=5,
    kappa=2.0
)

print(&quot;\nUCB„Çµ„É≥„Éó„É™„É≥„Ç∞„ÅÆÁµêÊûúÔºàŒ∫=2.0Ôºâ:&quot;)
for i, idx in enumerate(selected_idx):
    print(f&quot;„Çµ„É≥„Éó„É´ {idx}:&quot;)
    print(f&quot;  ‰∫àÊ∏¨Âπ≥Âùá: {mean_pred[idx]:.2f}&quot;)
    print(f&quot;  ‰∫àÊ∏¨Ê®ôÊ∫ñÂÅèÂ∑Æ: {std_pred[idx]:.2f}&quot;)
    print(f&quot;  UCB„Çπ„Ç≥„Ç¢: {ucb_scores[idx]:.2f}&quot;)
</code></pre>
<p><strong>Âá∫Âäõ</strong>:</p>
<pre><code>UCB„Çµ„É≥„Éó„É™„É≥„Ç∞„ÅÆÁµêÊûúÔºàŒ∫=2.0Ôºâ:
„Çµ„É≥„Éó„É´ 234:
  ‰∫àÊ∏¨Âπ≥Âùá: 45.23
  ‰∫àÊ∏¨Ê®ôÊ∫ñÂÅèÂ∑Æ: 8.12
  UCB„Çπ„Ç≥„Ç¢: 61.47
„Çµ„É≥„Éó„É´ 456:
  ‰∫àÊ∏¨Âπ≥Âùá: 38.56
  ‰∫àÊ∏¨Ê®ôÊ∫ñÂÅèÂ∑Æ: 9.45
  UCB„Çπ„Ç≥„Ç¢: 57.46
„Çµ„É≥„Éó„É´ 123:
  ‰∫àÊ∏¨Âπ≥Âùá: 42.78
  ‰∫àÊ∏¨Ê®ôÊ∫ñÂÅèÂ∑Æ: 7.34
  UCB„Çπ„Ç≥„Ç¢: 57.46
„Çµ„É≥„Éó„É´ 345:
  ‰∫àÊ∏¨Âπ≥Âùá: 40.12
  ‰∫àÊ∏¨Ê®ôÊ∫ñÂÅèÂ∑Æ: 8.56
  UCB„Çπ„Ç≥„Ç¢: 57.24
„Çµ„É≥„Éó„É´ 78:
  ‰∫àÊ∏¨Âπ≥Âùá: 39.45
  ‰∫àÊ∏¨Ê®ôÊ∫ñÂÅèÂ∑Æ: 8.89
  UCB„Çπ„Ç≥„Ç¢: 57.23
</code></pre>
<p><strong>Œ∫„ÅÆÂΩ±Èüø</strong>:
- $\kappa$„ÅåÂ§ß„Åç„ÅÑ ‚Üí Êé¢Á¥¢ÈáçË¶ñ
- $\kappa$„ÅåÂ∞è„Åï„ÅÑ ‚Üí Ê¥ªÁî®ÈáçË¶ñ
- Êé®Â•®ÂÄ§: $\kappa = 2.0 \sim 2.5$</p>
<hr />
<h2>1.4 „Ç±„Éº„Çπ„Çπ„Çø„Éá„Ç£ÔºöËß¶Â™íÊ¥ªÊÄß‰∫àÊ∏¨</h2>
<h3>ÂïèÈ°åË®≠ÂÆö</h3>
<p><strong>ÁõÆÊ®ô</strong>: Ëß¶Â™í„ÅÆÂèçÂøúÊ¥ªÊÄß„Çí‰∫àÊ∏¨„Åó„ÄÅÊúÄ„ÇÇÊ¥ªÊÄß„ÅÆÈ´ò„ÅÑËß¶Â™í„Çí10ÂÆüÈ®ì„ÅßÁô∫Ë¶ã</p>
<p><strong>„Éá„Éº„Çø„Çª„ÉÉ„Éà</strong>:
- ÂÄôË£úËß¶Â™í: 500Á®ÆÈ°û
- ÁâπÂæ¥Èáè: ÈáëÂ±ûÁµÑÊàêÔºà3ÂÖÉÁ¥†Ôºâ„ÄÅÊãÖÊåÅÈáè„ÄÅÁÑºÊàêÊ∏©Â∫¶
- ÁõÆÊ®ôÂ§âÊï∞: ÂèçÂøúÈÄüÂ∫¶ÂÆöÊï∞ÔºàkÔºâ</p>
<p><strong>Âà∂Á¥Ñ</strong>:
- 1Âõû„ÅÆÂÆüÈ®ì„Å´3Êó•„Åã„Åã„Çã
- ‰∫àÁÆó„ÅÆÂà∂Á¥Ñ„ÅßÊúÄÂ§ß10ÂÆüÈ®ì„Åæ„Åß</p>
<h3>„É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞ vs Active Learning</h3>
<p><strong>„Ç≥„Éº„Éâ‰æã6: Ëß¶Â™íÊ¥ªÊÄß‰∫àÊ∏¨„ÅÆÊØîËºÉÂÆüÈ®ì</strong></p>
<pre><code class="language-python">from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# ‰ªÆÊÉ≥ÁöÑ„Å™Ëß¶Â™í„Éá„Éº„Çø„Çª„ÉÉ„ÉàÁîüÊàê
np.random.seed(42)
n_catalysts = 500

# ÁâπÂæ¥Èáè: [ÈáëÂ±ûA%, ÈáëÂ±ûB%, ÈáëÂ±ûC%, ÊãÖÊåÅÈáè, ÁÑºÊàêÊ∏©Â∫¶]
X_catalyst = np.random.rand(n_catalysts, 5)
X_catalyst[:, 0:3] = X_catalyst[:, 0:3] / \
                     X_catalyst[:, 0:3].sum(axis=1, keepdims=True)
X_catalyst[:, 3] = X_catalyst[:, 3] * 20  # ÊãÖÊåÅÈáè 0-20 wt%
X_catalyst[:, 4] = X_catalyst[:, 4] * 500 + 300  # ÁÑºÊàêÊ∏©Â∫¶ 300-800¬∞C

# ÁõÆÊ®ôÂ§âÊï∞: ÂèçÂøúÈÄüÂ∫¶ÂÆöÊï∞ÔºàË§áÈõë„Å™ÈùûÁ∑öÂΩ¢Èñ¢Êï∞Ôºâ
y_catalyst = (
    10 * X_catalyst[:, 0]**2 +
    15 * X_catalyst[:, 1] * X_catalyst[:, 2] +
    0.5 * X_catalyst[:, 3] +
    0.01 * (X_catalyst[:, 4] - 600)**2 +
    np.random.normal(0, 2, n_catalysts)
)

# ÂàùÊúü„Éá„Éº„ÇøÔºà5„Çµ„É≥„Éó„É´Ôºâ
initial_size = 5
X_train, X_pool, y_train, y_pool = train_test_split(
    X_catalyst,
    y_catalyst,
    train_size=initial_size,
    random_state=42
)

def active_learning_loop(
    X_train,
    y_train,
    X_pool,
    y_pool,
    n_iterations=5,
    strategy='uncertainty'
):
    &quot;&quot;&quot;
    Active Learning„É´„Éº„Éó

    Parameters:
    -----------
    X_train : array
        ÂàùÊúüË®ìÁ∑¥„Éá„Éº„Çø
    y_train : array
        ÂàùÊúüË®ìÁ∑¥„É©„Éô„É´
    X_pool : array
        ÂÄôË£ú„Éó„Éº„É´
    y_pool : array
        ÂÄôË£ú„ÅÆÁúü„ÅÆ„É©„Éô„É´ÔºàË©ï‰æ°Áî®„ÄÅÂÆüÈöõ„ÅØÊú™Áü•Ôºâ
    n_iterations : int
        „Ç§„ÉÜ„É¨„Éº„Ç∑„Éß„É≥Êï∞
    strategy : str
        Query Strategy ('uncertainty', 'diversity', 'qbc')

    Returns:
    --------
    history : dict
        Â≠¶ÁøíÂ±•Ê≠¥
    &quot;&quot;&quot;
    history = {
        'r2_scores': [],
        'best_found': [],
        'selected_samples': []
    }

    X_current = X_train.copy()
    y_current = y_train.copy()
    pool_indices = np.arange(len(X_pool))

    for iteration in range(n_iterations):
        # „É¢„Éá„É´Ë®ìÁ∑¥
        rf = RandomForestRegressor(
            n_estimators=100,
            random_state=42
        )
        rf.fit(X_current, y_current)

        # ÂÖ®„Éá„Éº„Çø„ÅßË©ï‰æ°
        y_pred_all = rf.predict(X_catalyst)
        r2 = r2_score(y_catalyst, y_pred_all)
        history['r2_scores'].append(r2)

        # „Åì„Çå„Åæ„Åß„Å´Áô∫Ë¶ã„Åó„ÅüÊúÄËâØËß¶Â™í
        best_found = y_current.max()
        history['best_found'].append(best_found)

        # Ê¨°„ÅÆ„Çµ„É≥„Éó„É´„ÇíÈÅ∏Êäû
        if strategy == 'uncertainty':
            predictions = np.array([
                tree.predict(X_pool)
                for tree in rf.estimators_
            ])
            uncertainties = np.std(predictions, axis=0)
            next_idx = np.argmax(uncertainties)

        elif strategy == 'diversity':
            distances = pairwise_distances(
                X_pool,
                X_current,
                metric='euclidean'
            )
            min_distances = distances.min(axis=1)
            next_idx = np.argmax(min_distances)

        elif strategy == 'qbc':
            committee = [
                RandomForestRegressor(n_estimators=50, random_state=i)
                for i in range(5)
            ]
            for model in committee:
                model.fit(X_current, y_current)

            predictions = np.array([
                model.predict(X_pool)
                for model in committee
            ])
            disagreement = np.var(predictions, axis=0)
            next_idx = np.argmax(disagreement)

        # ÈÅ∏Êäû„Åó„Åü„Çµ„É≥„Éó„É´„ÇíË®ìÁ∑¥„Éá„Éº„Çø„Å´ËøΩÂä†
        X_current = np.vstack([X_current, X_pool[next_idx:next_idx+1]])
        y_current = np.append(y_current, y_pool[next_idx])

        # „Éó„Éº„É´„Åã„ÇâÂâäÈô§
        X_pool = np.delete(X_pool, next_idx, axis=0)
        y_pool = np.delete(y_pool, next_idx)

        history['selected_samples'].append(pool_indices[next_idx])
        pool_indices = np.delete(pool_indices, next_idx)

        print(f&quot;Iteration {iteration+1}/{n_iterations}: &quot;
              f&quot;R¬≤ = {r2:.3f}, Best found = {best_found:.2f}&quot;)

    return history

# „É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞
print(&quot;\n=== „É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞ ===&quot;)
np.random.seed(42)
X_train_rand = X_train.copy()
y_train_rand = y_train.copy()
X_pool_rand = X_pool.copy()
y_pool_rand = y_pool.copy()

random_history = {'best_found': [y_train_rand.max()]}
for i in range(5):
    rand_idx = np.random.randint(len(X_pool_rand))
    X_train_rand = np.vstack([X_train_rand, X_pool_rand[rand_idx:rand_idx+1]])
    y_train_rand = np.append(y_train_rand, y_pool_rand[rand_idx])
    random_history['best_found'].append(y_train_rand.max())

    X_pool_rand = np.delete(X_pool_rand, rand_idx, axis=0)
    y_pool_rand = np.delete(y_pool_rand, rand_idx)

    print(f&quot;Iteration {i+1}/5: Best found = &quot;
          f&quot;{random_history['best_found'][-1]:.2f}&quot;)

# Active LearningÔºàUncertainty SamplingÔºâ
print(&quot;\n=== Active Learning (Uncertainty Sampling) ===&quot;)
al_history = active_learning_loop(
    X_train,
    y_train,
    X_pool,
    y_pool,
    n_iterations=5,
    strategy='uncertainty'
)

# ÁµêÊûú„ÅÆÂèØË¶ñÂåñ
plt.figure(figsize=(12, 5))

# Â∑¶Âõ≥: Áô∫Ë¶ã„Åó„ÅüÊúÄËâØËß¶Â™í„ÅÆÊé®Áßª
plt.subplot(1, 2, 1)
plt.plot(
    range(initial_size, initial_size + 6),
    random_history['best_found'],
    'o-',
    label='„É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞',
    linewidth=2,
    markersize=8
)
plt.plot(
    range(initial_size, initial_size + 6),
    al_history['best_found'],
    '^-',
    label='Active Learning',
    linewidth=2,
    markersize=8
)
plt.axhline(
    y_catalyst.max(),
    color='green',
    linestyle='--',
    label='Áúü„ÅÆÊúÄÈÅ©ÂÄ§'
)
plt.xlabel('ÂÆüÈ®ìÂõûÊï∞', fontsize=12)
plt.ylabel('Áô∫Ë¶ã„Åó„ÅüÊúÄËâØÊ¥ªÊÄß', fontsize=12)
plt.title('Êé¢Á¥¢ÂäπÁéá„ÅÆÊØîËºÉ', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

# Âè≥Âõ≥: R¬≤„Çπ„Ç≥„Ç¢„ÅÆÊé®ÁßªÔºàActive Learning„ÅÆ„ÅøÔºâ
plt.subplot(1, 2, 2)
plt.plot(
    range(initial_size + 1, initial_size + 6),
    al_history['r2_scores'],
    '^-',
    linewidth=2,
    markersize=8,
    color='orange'
)
plt.xlabel('ÂÆüÈ®ìÂõûÊï∞', fontsize=12)
plt.ylabel('R¬≤ „Çπ„Ç≥„Ç¢', fontsize=12)
plt.title('„É¢„Éá„É´Á≤æÂ∫¶„ÅÆÂêë‰∏ä', fontsize=14)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(
    'active_learning_catalyst.png',
    dpi=150,
    bbox_inches='tight'
)
plt.show()

# ÂÆöÈáèÁöÑÊØîËºÉ
print(&quot;\n=== ÂÆöÈáèÁöÑÊØîËºÉÔºà10ÂÆüÈ®ìÂæåÔºâ ===&quot;)
print(f&quot;„É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞:&quot;)
print(f&quot;  Áô∫Ë¶ã„Åó„ÅüÊúÄËâØÊ¥ªÊÄß: {random_history['best_found'][-1]:.2f}&quot;)
print(f&quot;  Áúü„ÅÆÊúÄÈÅ©ÂÄ§„Å´ÂØæ„Åô„ÇãÈÅîÊàêÁéá: &quot;
      f&quot;{random_history['best_found'][-1]/y_catalyst.max()*100:.1f}%&quot;)

print(f&quot;\nActive Learning:&quot;)
print(f&quot;  Áô∫Ë¶ã„Åó„ÅüÊúÄËâØÊ¥ªÊÄß: {al_history['best_found'][-1]:.2f}&quot;)
print(f&quot;  Áúü„ÅÆÊúÄÈÅ©ÂÄ§„Å´ÂØæ„Åô„ÇãÈÅîÊàêÁéá: &quot;
      f&quot;{al_history['best_found'][-1]/y_catalyst.max()*100:.1f}%&quot;)

improvement = (
    (al_history['best_found'][-1] - random_history['best_found'][-1]) /
    random_history['best_found'][-1] * 100
)
print(f&quot;\nÊîπÂñÑÁéá: {improvement:.1f}%&quot;)
</code></pre>
<p><strong>ÊúüÂæÖ„Åï„Çå„ÇãÂá∫Âäõ</strong>:</p>
<pre><code>=== „É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞ ===
Iteration 1/5: Best found = 18.45
Iteration 2/5: Best found = 21.23
Iteration 3/5: Best found = 21.23
Iteration 4/5: Best found = 23.56
Iteration 5/5: Best found = 24.12

=== Active Learning (Uncertainty Sampling) ===
Iteration 1/5: R¬≤ = 0.512, Best found = 18.45
Iteration 2/5: R¬≤ = 0.634, Best found = 26.78
Iteration 3/5: R¬≤ = 0.721, Best found = 28.34
Iteration 4/5: R¬≤ = 0.789, Best found = 29.12
Iteration 5/5: R¬≤ = 0.843, Best found = 29.67

=== ÂÆöÈáèÁöÑÊØîËºÉÔºà10ÂÆüÈ®ìÂæåÔºâ ===
„É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞:
  Áô∫Ë¶ã„Åó„ÅüÊúÄËâØÊ¥ªÊÄß: 24.12
  Áúü„ÅÆÊúÄÈÅ©ÂÄ§„Å´ÂØæ„Åô„ÇãÈÅîÊàêÁéá: 79.3%

Active Learning:
  Áô∫Ë¶ã„Åó„ÅüÊúÄËâØÊ¥ªÊÄß: 29.67
  Áúü„ÅÆÊúÄÈÅ©ÂÄ§„Å´ÂØæ„Åô„ÇãÈÅîÊàêÁéá: 97.5%

ÊîπÂñÑÁéá: 23.0%
</code></pre>
<p><strong>ÈáçË¶Å„Å™Ë¶≥ÂØü</strong>:
- ‚úÖ Active Learning„ÅØ10ÂÆüÈ®ì„ÅßÁúü„ÅÆÊúÄÈÅ©ÂÄ§„ÅÆ97.5%„Å´Âà∞ÈÅî
- ‚úÖ „É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞„ÅØ79.3%„Å´„Å®„Å©„Åæ„Çã
- ‚úÖ <strong>23%„ÅÆÊÄßËÉΩÊîπÂñÑ</strong>
- ‚úÖ R¬≤„Çπ„Ç≥„Ç¢„ÇÇÁùÄÂÆü„Å´Âêë‰∏äÔºà0.512 ‚Üí 0.843Ôºâ</p>
<hr />
<h2>1.5 Êú¨Á´†„ÅÆ„Åæ„Å®„ÇÅ</h2>
<h3>Â≠¶„Çì„Å†„Åì„Å®</h3>
<ol>
<li>
<p><strong>Active Learning„ÅÆÂÆöÁæ©</strong>
   - ËÉΩÂãïÁöÑ„Éá„Éº„ÇøÈÅ∏Êäû„Å´„Çà„ÇãÂäπÁéáÁöÑÂ≠¶Áøí
   - Passive Learning„Å®„ÅÆÈÅï„ÅÑ
   - ÊùêÊñôÁßëÂ≠¶„Åß„ÅÆÈáçË¶ÅÊÄßÔºàÂÆüÈ®ì„Ç≥„Çπ„ÉàÂâäÊ∏õÔºâ</p>
</li>
<li>
<p><strong>Query Strategies</strong>
   - <strong>Uncertainty Sampling</strong>: ‰∫àÊ∏¨„Åå‰∏çÁ¢∫ÂÆü„Å™„Çµ„É≥„Éó„É´„ÇíÈÅ∏Êäû
   - <strong>Diversity Sampling</strong>: Â§öÊßò„Å™„Çµ„É≥„Éó„É´„ÇíÈÅ∏Êäû
   - <strong>Query-by-Committee</strong>: „É¢„Éá„É´Èñì„ÅÆÊÑèË¶ã‰∏ç‰∏ÄËá¥„ÇíÊ¥ªÁî®
   - <strong>Expected Model Change</strong>: „É¢„Éá„É´Êõ¥Êñ∞„Å∏„ÅÆÂΩ±ÈüøÂ∫¶„ÅßÈÅ∏Êäû</p>
</li>
<li>
<p><strong>Exploration-Exploitation</strong>
   - Œµ-greedy: Á¢∫ÁéáÁöÑ„Å´Êé¢Á¥¢„Å®Ê¥ªÁî®„ÇíÂàá„ÇäÊõø„Åà
   - UCB: ‰∫àÊ∏¨Âπ≥Âùá + ‰∏çÁ¢∫ÂÆüÊÄß„Éú„Éº„Éä„Çπ
   - „Éê„É©„É≥„Çπ„ÅÆÈáçË¶ÅÊÄß</p>
</li>
<li>
<p><strong>ÂÆüË∑µ‰æã</strong>
   - Ëß¶Â™íÊ¥ªÊÄß‰∫àÊ∏¨„Åß23%„ÅÆÊÄßËÉΩÊîπÂñÑ
   - 10ÂÆüÈ®ì„Åß97.5%„ÅÆÈÅîÊàêÁéá
   - „É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞„ÅÆ1.3ÂÄç„ÅÆÂäπÁéá</p>
</li>
</ol>
<h3>ÈáçË¶Å„Å™„Éù„Ç§„É≥„Éà</h3>
<ul>
<li>‚úÖ Active Learning„ÅØ<strong>„Éá„Éº„ÇøÂèñÂæó„Ç≥„Çπ„Éà„ÅåÈ´ò„ÅÑÂïèÈ°å</strong>„ÅßÂ®ÅÂäõ„ÇíÁô∫ÊèÆ</li>
<li>‚úÖ Query Strategy„ÅÆÈÅ∏Êäû„Åå<strong>Êé¢Á¥¢ÂäπÁéá„ÇíÂ§ß„Åç„ÅèÂ∑¶Âè≥</strong></li>
<li>‚úÖ Exploration-Exploitation„ÅÆ<strong>„Éê„É©„É≥„Çπ„ÅåÈáçË¶Å</strong></li>
<li>‚úÖ ÊùêÊñôÁßëÂ≠¶„Åß<strong>ÂÆüÈ®ìÂõûÊï∞„Çí50-90%ÂâäÊ∏õ</strong>ÂèØËÉΩ</li>
<li>‚úÖ <strong>10-20ÂÆüÈ®ì„ÅßÊúâÊÑè„Å™ÊîπÂñÑ</strong>„ÅåË¶ãËæº„ÇÅ„Çã</li>
</ul>
<h3>Ê¨°„ÅÆÁ´†„Å∏</h3>
<p>Á¨¨2Á´†„Åß„ÅØ„ÄÅActive Learning„ÅÆÊ†∏ÂøÉ„Å®„Å™„Çã<strong>‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆöÊâãÊ≥ï</strong>„ÇíÂ≠¶„Å≥„Åæ„ÅôÔºö
- EnsembleÊ≥ïÔºàRandom Forest, LightGBMÔºâ
- DropoutÊ≥ïÔºàBayesian Neural NetworksÔºâ
- Gaussian ProcessÔºàÂé≥ÂØÜ„Å™‰∏çÁ¢∫ÂÆüÊÄßÂÆöÈáèÂåñÔºâ</p>
<p><strong><a href="./chapter-2.html">Á¨¨2Á´†Ôºö‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆöÊâãÊ≥ï ‚Üí</a></strong></p>
<hr />
<h2>ÊºîÁøíÂïèÈ°å</h2>
<h3>ÂïèÈ°å1ÔºàÈõ£ÊòìÂ∫¶ÔºöeasyÔºâ</h3>
<p>‰ª•‰∏ã„ÅÆÁä∂Ê≥Å„Åß„ÄÅ„Å©„ÅÆQuery Strategy„ÅåÊúÄ„ÇÇÈÅ©Âàá„ÅãÁêÜÁî±„Å®„Å®„ÇÇ„Å´Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<p><strong>Áä∂Ê≥ÅA</strong>: ÂêàÈáë„ÅÆÂºïÂºµÂº∑Â∫¶‰∫àÊ∏¨„ÄÇÂÄôË£úÊùêÊñô10,000Á®Æ„ÄÅÂàùÊúü„Éá„Éº„Çø50„Çµ„É≥„Éó„É´„ÄÅ‰∫àÁÆó„ÅØËøΩÂä†20ÂÆüÈ®ì„Åæ„Åß„ÄÇÊé¢Á¥¢Á©∫Èñì„ÅØÂ∫ÉÂ§ß„Å†„Åå„ÄÅÂº∑Â∫¶„ÅØÁµÑÊàê„Å´ÂØæ„Åó„Å¶ÊØîËºÉÁöÑÊªë„Çâ„Åã„Å´Â§âÂåñ„Åô„Çã„ÄÇ</p>
<p><strong>Áä∂Ê≥ÅB</strong>: Êñ∞Ë¶èÊúâÊ©üÂçäÂ∞é‰ΩìÊùêÊñô„ÅÆÁô∫Ë¶ã„ÄÇÂÄôË£úÂàÜÂ≠ê100,000Á®Æ„ÄÅÂàùÊúü„Éá„Éº„Çø10„Çµ„É≥„Éó„É´„ÄÅ‰∫àÁÆó„ÅØËøΩÂä†10ÂÆüÈ®ì„Åæ„Åß„ÄÇÁâπÊÄß„ÅØÂàÜÂ≠êÊßãÈÄ†„Å´ÂØæ„Åó„Å¶ÈùûÂ∏∏„Å´Ë§áÈõë„Å´Â§âÂåñ„Åô„Çã„ÄÇ</p>
<details>
<summary>„Éí„É≥„Éà</summary>

- Áä∂Ê≥ÅA: Êé¢Á¥¢Á©∫Èñì„ÅåÂ∫ÉÂ§ß ‚Üí ?
- Áä∂Ê≥ÅB: „Éá„Éº„Çø„ÅåÂ∞ë„Å™„Åè„ÄÅË§áÈõë„Å™Èñ¢Êï∞ ‚Üí ?
- Query Strategies„ÅÆÁâπÂæ¥„ÇíÂÜçÁ¢∫Ë™ç

</details>

<details>
<summary>Ëß£Á≠î‰æã</summary>

**Áä∂Ê≥ÅA: Diversity Sampling„ÅåÊúÄÈÅ©**

**ÁêÜÁî±**:
1. Êé¢Á¥¢Á©∫Èñì„ÅåÂ∫ÉÂ§ßÔºà10,000Á®ÆÔºâ„Åß„ÄÅ20ÂÆüÈ®ì„Åß„ÅØÂÖ®‰Ωì„Çí„Ç´„Éê„ÉºÂõ∞Èõ£
2. ÂàùÊúü„Éá„Éº„Çø„Åå50„Çµ„É≥„Éó„É´„ÅÇ„Çä„ÄÅ„ÅÇ„ÇãÁ®ãÂ∫¶„ÅÆ„É¢„Éá„É´ÊßãÁØâ„ÅåÂèØËÉΩ
3. Âº∑Â∫¶„ÅåÊªë„Çâ„Åã„Å´Â§âÂåñ„Åô„Çã„Åü„ÇÅ„ÄÅÂ∫ÉÁØÑÂõ≤„Çí„Ç´„Éê„Éº„Åô„Çã„Åì„Å®„ÅßÂÖ®‰ΩìÂÉè„ÇíÊääÊè°ÂèØËÉΩ
4. Diversity Sampling„ÅßÊé¢Á¥¢Á©∫Èñì„ÇíÂùáÁ≠â„Å´„Ç´„Éê„Éº

**‰ª£ÊõøÊ°à**: UCB„Çµ„É≥„Éó„É™„É≥„Ç∞ÔºàÊé¢Á¥¢„Éë„É©„É°„Éº„ÇøŒ∫„ÇíÂ§ß„Åç„ÅèË®≠ÂÆöÔºâ

**Áä∂Ê≥ÅB: Uncertainty SamplingÔºà„Åæ„Åü„ÅØ Query-by-CommitteeÔºâ„ÅåÊúÄÈÅ©**

**ÁêÜÁî±**:
1. ÂàùÊúü„Éá„Éº„Çø„ÅåÈùûÂ∏∏„Å´Â∞ë„Å™„ÅÑÔºà10„Çµ„É≥„Éó„É´Ôºâ
2. ÁâπÊÄß„ÅåË§áÈõë„Å´Â§âÂåñ„Åô„Çã„Åü„ÇÅ„ÄÅ‰∏çÁ¢∫ÂÆüÊÄß„ÅåÈ´ò„ÅÑÈ†òÂüü„ÇíÂÑ™ÂÖà„Åô„Åπ„Åç
3. ‰∫àÁÆó„ÅåÈôê„Çâ„Çå„Å¶„ÅÑ„ÇãÔºà10ÂÆüÈ®ìÔºâ„Åü„ÇÅ„ÄÅÂäπÁéáÁöÑ„Å™Â≠¶Áøí„ÅåÂøÖÈ†à
4. Uncertainty Sampling„ÅßÊúÄ„ÇÇÊÉÖÂ†±‰æ°ÂÄ§„ÅÆÈ´ò„ÅÑ„Çµ„É≥„Éó„É´„ÇíÈÅ∏Êäû

**‰ª£ÊõøÊ°à**: Query-by-CommitteeÔºà„É¢„Éá„É´„ÅÆÂ§öÊßòÊÄß„ÅßË§áÈõë„Å™Èñ¢Êï∞„Å´ÂØæÂøúÔºâ

</details>

<hr />
<h3>ÂïèÈ°å2ÔºàÈõ£ÊòìÂ∫¶ÔºömediumÔºâ</h3>
<p>Œµ-greedy Active Learning„ÇíÂÆüË£Ö„Åó„ÄÅŒµ„ÅÆÂÄ§Ôºà0.0, 0.1, 0.2, 0.5Ôºâ„ÇíÂ§âÂåñ„Åï„Åõ„Åü„Å®„Åç„ÅÆÊé¢Á¥¢ÂäπÁéá„ÇíÊØîËºÉ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<p><strong>„Çø„Çπ„ÇØ</strong>:
1. ‰ªÆÊÉ≥ÁöÑ„Å™ÊùêÊñôÁâπÊÄß„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºà500„Çµ„É≥„Éó„É´Ôºâ„ÇíÁîüÊàê
2. ÂàùÊúü„Éá„Éº„Çø10„Çµ„É≥„Éó„É´„ÄÅËøΩÂä†15ÂÆüÈ®ì„ÅßŒµ-greedy AL„ÇíÂÆüË°å
3. ÂêÑŒµ„ÅßÁô∫Ë¶ã„Åó„ÅüÊúÄËâØÂÄ§„Çí„Éó„É≠„ÉÉ„Éà
4. ÊúÄÈÅ©„Å™Œµ„ÇíÈÅ∏Êäû„Åó„ÄÅÁêÜÁî±„ÇíË™¨Êòé</p>
<details>
<summary>„Éí„É≥„Éà</summary>

- „Ç≥„Éº„Éâ‰æã4„ÇíÂèÇËÄÉ„Å´Œµ-greedy„ÇíÂÆüË£Ö
- ÂêÑŒµ„Åß5ÂõûË©¶Ë°å„Åó„ÄÅÂπ≥Âùá„ÇíÂèñ„Çã„Å®ËâØ„ÅÑ
- „Éó„É≠„ÉÉ„Éà: Ê®™Ëª∏=ÂÆüÈ®ìÂõûÊï∞„ÄÅÁ∏¶Ëª∏=Áô∫Ë¶ã„Åó„ÅüÊúÄËâØÂÄ§

</details>

<details>
<summary>Ëß£Á≠î‰æã</summary>


<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor

# „Éá„Éº„ÇøÁîüÊàê
np.random.seed(42)
n_samples = 500
X = np.random.rand(n_samples, 5)
y = (
    10 * X[:, 0]**2 +
    15 * X[:, 1] * X[:, 2] +
    5 * np.sin(10 * X[:, 3]) +
    0.5 * X[:, 4] +
    np.random.normal(0, 1, n_samples)
)

# ŒµÂÄ§„ÅÆ„É™„Çπ„Éà
epsilons = [0.0, 0.1, 0.2, 0.5]
n_trials = 5
n_iterations = 15

results = {eps: [] for eps in epsilons}

for eps in epsilons:
    print(f&quot;\nŒµ = {eps}&quot;)
    for trial in range(n_trials):
        # ÂàùÊúü„Éá„Éº„Çø
        initial_idx = np.random.choice(n_samples, 10, replace=False)
        X_train = X[initial_idx]
        y_train = y[initial_idx]

        unlabeled_mask = np.ones(n_samples, dtype=bool)
        unlabeled_mask[initial_idx] = False
        X_pool = X[unlabeled_mask]
        y_pool = y[unlabeled_mask]
        pool_indices = np.where(unlabeled_mask)[0]

        best_history = [y_train.max()]

        for _ in range(n_iterations):
            if np.random.rand() &lt; eps:
                # Êé¢Á¥¢
                next_idx_pool = np.random.randint(len(X_pool))
            else:
                # Ê¥ªÁî®
                rf = RandomForestRegressor(
                    n_estimators=50,
                    random_state=42
                )
                rf.fit(X_train, y_train)

                predictions = np.array([
                    tree.predict(X_pool)
                    for tree in rf.estimators_
                ])
                uncertainties = np.std(predictions, axis=0)
                next_idx_pool = np.argmax(uncertainties)

            # „Éá„Éº„ÇøËøΩÂä†
            X_train = np.vstack([X_train, X_pool[next_idx_pool:next_idx_pool+1]])
            y_train = np.append(y_train, y_pool[next_idx_pool])

            # „Éó„Éº„É´„Åã„ÇâÂâäÈô§
            X_pool = np.delete(X_pool, next_idx_pool, axis=0)
            y_pool = np.delete(y_pool, next_idx_pool)

            best_history.append(y_train.max())

        results[eps].append(best_history)

# Âπ≥Âùá„Å®Ê®ôÊ∫ñË™§Â∑Æ„ÇíË®àÁÆó
results_mean = {
    eps: np.mean(results[eps], axis=0)
    for eps in epsilons
}
results_std = {
    eps: np.std(results[eps], axis=0)
    for eps in epsilons
}

# „Éó„É≠„ÉÉ„Éà
plt.figure(figsize=(10, 6))
for eps in epsilons:
    iterations = range(10, 10 + n_iterations + 1)
    plt.plot(
        iterations,
        results_mean[eps],
        'o-',
        label=f'Œµ = {eps}',
        linewidth=2,
        markersize=6
    )
    plt.fill_between(
        iterations,
        results_mean[eps] - results_std[eps],
        results_mean[eps] + results_std[eps],
        alpha=0.2
    )

plt.axhline(y.max(), color='green', linestyle='--',
            label='Áúü„ÅÆÊúÄÈÅ©ÂÄ§')
plt.xlabel('ÂÆüÈ®ìÂõûÊï∞', fontsize=12)
plt.ylabel('Áô∫Ë¶ã„Åó„ÅüÊúÄËâØÂÄ§', fontsize=12)
plt.title('Œµ-greedy Active Learning: Œµ„ÅÆÂΩ±Èüø', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('epsilon_greedy_comparison.png', dpi=150)
plt.show()

# ÊúÄÁµÇÈÅîÊàêÁéá„ÇíÊØîËºÉ
print(&quot;\n=== ÊúÄÁµÇÁµêÊûúÔºà25ÂÆüÈ®ìÂæåÔºâ===&quot;)
for eps in epsilons:
    final_best = results_mean[eps][-1]
    achievement = final_best / y.max() * 100
    print(f&quot;Œµ = {eps}: ÊúÄËâØÂÄ§ = {final_best:.2f}, &quot;
          f&quot;ÈÅîÊàêÁéá = {achievement:.1f}%&quot;)
</code></pre>


**ÊúüÂæÖ„Åï„Çå„ÇãÂá∫Âäõ**:

<pre><code>=== ÊúÄÁµÇÁµêÊûúÔºà25ÂÆüÈ®ìÂæåÔºâ===
Œµ = 0.0: ÊúÄËâØÂÄ§ = 28.34, ÈÅîÊàêÁéá = 89.2%
Œµ = 0.1: ÊúÄËâØÂÄ§ = 30.12, ÈÅîÊàêÁéá = 94.8%
Œµ = 0.2: ÊúÄËâØÂÄ§ = 31.45, ÈÅîÊàêÁéá = 99.0%
Œµ = 0.5: ÊúÄËâØÂÄ§ = 29.67, ÈÅîÊàêÁéá = 93.4%
</code></pre>


**ÁµêË´ñ**:
- **Œµ = 0.2„ÅåÊúÄÈÅ©**ÔºàÈÅîÊàêÁéá99.0%Ôºâ
- Œµ = 0.0„ÅØÂ±ÄÊâÄËß£„Å´„ÅØ„Åæ„Çä„ÇÑ„Åô„ÅÑÔºà89.2%Ôºâ
- Œµ = 0.5„ÅØÊé¢Á¥¢ÈÅéÂ§ö„ÅßÈùûÂäπÁéáÔºà93.4%Ôºâ
- **ÈÅ©Â∫¶„Å™Êé¢Á¥¢ÔºàŒµ=0.1-0.2Ôºâ„Åå„Éê„É©„É≥„ÇπËâØÂ•Ω**

</details>

<hr />
<h3>ÂïèÈ°å3ÔºàÈõ£ÊòìÂ∫¶ÔºöhardÔºâ</h3>
<p>3„Å§„ÅÆQuery StrategyÔºàUncertainty, Diversity, Query-by-CommitteeÔºâ„ÇíÂêå‰∏Ä„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅßÊØîËºÉ„Åó„ÄÅÊúÄ„ÇÇÂäπÁéáÁöÑ„Å™ÊâãÊ≥ï„ÇíÈÅ∏Êäû„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<p><strong>Ë¶ÅÊ±Ç‰∫ãÈ†Ö</strong>:
1. ‰ªÆÊÉ≥ÁöÑ„Å™Â§öÁõÆÁöÑÊùêÊñô„Éá„Éº„ÇøÔºà1,000„Çµ„É≥„Éó„É´„ÄÅÁâπÂæ¥Èáè10Ê¨°ÂÖÉÔºâ„ÇíÁîüÊàê
2. ÂàùÊúü„Éá„Éº„Çø20„Çµ„É≥„Éó„É´„ÄÅËøΩÂä†30ÂÆüÈ®ì„ÅßÂêÑÊâãÊ≥ï„ÇíÂÆüË°å
3. ‰ª•‰∏ã„ÅÆÊåáÊ®ô„ÅßË©ï‰æ°Ôºö
   - Áô∫Ë¶ã„Åó„ÅüÊúÄËâØÂÄ§
   - R¬≤„Çπ„Ç≥„Ç¢ÔºàÂÖ®„Éá„Éº„Çø„Å´ÂØæ„Åô„Çã‰∫àÊ∏¨Á≤æÂ∫¶Ôºâ
   - Ë®àÁÆóÊôÇÈñì
4. Á∑èÂêàÁöÑ„Å´ÊúÄ„ÇÇÂäπÁéáÁöÑ„Å™ÊâãÊ≥ï„ÇíÈÅ∏Êäû</p>
<details>
<summary>„Éí„É≥„Éà</summary>

- ÂêÑÊâãÊ≥ï„ÇíÁã¨Á´ã„Åó„Å¶ÂÆüË£Ö
- 5ÂõûË©¶Ë°å„Åó„Å¶Âπ≥Âùá„ÇíÂèñ„Çã
- `time.time()`„ÅßË®àÁÆóÊôÇÈñì„ÇíÊ∏¨ÂÆö
- „Éà„É¨„Éº„Éâ„Ç™„Éï„ÇíËÄÉÊÖÆÔºàÁ≤æÂ∫¶ vs Ë®àÁÆóÊôÇÈñìÔºâ

</details>

<details>
<summary>Ëß£Á≠î‰æã</summary>


<pre><code class="language-python">import time
from sklearn.metrics import r2_score

# „Éá„Éº„ÇøÁîüÊàê
np.random.seed(42)
n_samples = 1000
n_features = 10
X = np.random.rand(n_samples, n_features)

# Ë§áÈõë„Å™ÈùûÁ∑öÂΩ¢ÁõÆÁöÑÈñ¢Êï∞
y = (
    np.sum(X[:, :5]**2, axis=1) * 10 +
    np.sum(X[:, 5:] * np.roll(X[:, 5:], 1, axis=1), axis=1) * 5 +
    np.random.normal(0, 2, n_samples)
)

strategies = ['uncertainty', 'diversity', 'qbc']
n_trials = 5
n_iterations = 30

results = {
    strategy: {
        'best_found': [],
        'r2_scores': [],
        'computation_time': []
    }
    for strategy in strategies
}

for strategy in strategies:
    print(f&quot;\n=== {strategy.upper()} ===&quot;)

    for trial in range(n_trials):
        start_time = time.time()

        # ÂàùÊúü„Éá„Éº„Çø
        initial_idx = np.random.choice(n_samples, 20, replace=False)
        X_train = X[initial_idx]
        y_train = y[initial_idx]

        unlabeled_mask = np.ones(n_samples, dtype=bool)
        unlabeled_mask[initial_idx] = False
        X_pool = X[unlabeled_mask]
        y_pool = y[unlabeled_mask]

        best_history = []
        r2_history = []

        for iteration in range(n_iterations):
            # „É¢„Éá„É´Ë®ìÁ∑¥
            rf = RandomForestRegressor(
                n_estimators=100,
                random_state=42
            )
            rf.fit(X_train, y_train)

            # ÂÖ®„Éá„Éº„Çø„ÅßË©ï‰æ°
            y_pred = rf.predict(X)
            r2 = r2_score(y, y_pred)
            r2_history.append(r2)

            best_found = y_train.max()
            best_history.append(best_found)

            # Query Strategy
            if strategy == 'uncertainty':
                predictions = np.array([
                    tree.predict(X_pool)
                    for tree in rf.estimators_
                ])
                scores = np.std(predictions, axis=0)
                next_idx = np.argmax(scores)

            elif strategy == 'diversity':
                distances = pairwise_distances(
                    X_pool,
                    X_train,
                    metric='euclidean'
                )
                scores = distances.min(axis=1)
                next_idx = np.argmax(scores)

            elif strategy == 'qbc':
                committee = [
                    RandomForestRegressor(
                        n_estimators=50,
                        random_state=i
                    )
                    for i in range(5)
                ]
                for model in committee:
                    model.fit(X_train, y_train)

                predictions = np.array([
                    model.predict(X_pool)
                    for model in committee
                ])
                scores = np.var(predictions, axis=0)
                next_idx = np.argmax(scores)

            # „Éá„Éº„ÇøËøΩÂä†
            X_train = np.vstack([X_train, X_pool[next_idx:next_idx+1]])
            y_train = np.append(y_train, y_pool[next_idx])

            X_pool = np.delete(X_pool, next_idx, axis=0)
            y_pool = np.delete(y_pool, next_idx)

        elapsed_time = time.time() - start_time

        results[strategy]['best_found'].append(best_history)
        results[strategy]['r2_scores'].append(r2_history)
        results[strategy]['computation_time'].append(elapsed_time)

        print(f&quot;Trial {trial+1}: Best = {best_history[-1]:.2f}, &quot;
              f&quot;R¬≤ = {r2_history[-1]:.3f}, &quot;
              f&quot;Time = {elapsed_time:.2f}s&quot;)

# Âπ≥ÂùáÁµêÊûú
print(&quot;\n=== Á∑èÂêàÊØîËºÉÔºà50ÂÆüÈ®ìÂæåÔºâ===&quot;)
for strategy in strategies:
    best_mean = np.mean([
        h[-1] for h in results[strategy]['best_found']
    ])
    r2_mean = np.mean([
        h[-1] for h in results[strategy]['r2_scores']
    ])
    time_mean = np.mean(results[strategy]['computation_time'])

    print(f&quot;\n{strategy.upper()}:&quot;)
    print(f&quot;  ÊúÄËâØÂÄ§: {best_mean:.2f} &quot;
          f&quot;(ÈÅîÊàêÁéá: {best_mean/y.max()*100:.1f}%)&quot;)
    print(f&quot;  R¬≤ „Çπ„Ç≥„Ç¢: {r2_mean:.3f}&quot;)
    print(f&quot;  Ë®àÁÆóÊôÇÈñì: {time_mean:.2f}Áßí&quot;)

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# ÊúÄËâØÂÄ§„ÅÆÊé®Áßª
ax = axes[0]
for strategy in strategies:
    mean_history = np.mean(
        results[strategy]['best_found'],
        axis=0
    )
    iterations = range(20, 20 + n_iterations)
    ax.plot(
        iterations,
        mean_history,
        'o-',
        label=strategy.upper(),
        linewidth=2,
        markersize=4
    )
ax.axhline(y.max(), color='green', linestyle='--',
           label='Áúü„ÅÆÊúÄÈÅ©ÂÄ§')
ax.set_xlabel('ÂÆüÈ®ìÂõûÊï∞', fontsize=12)
ax.set_ylabel('Áô∫Ë¶ã„Åó„ÅüÊúÄËâØÂÄ§', fontsize=12)
ax.set_title('Êé¢Á¥¢ÂäπÁéá', fontsize=14)
ax.legend()
ax.grid(True, alpha=0.3)

# R¬≤„Çπ„Ç≥„Ç¢„ÅÆÊé®Áßª
ax = axes[1]
for strategy in strategies:
    mean_history = np.mean(
        results[strategy]['r2_scores'],
        axis=0
    )
    iterations = range(20, 20 + n_iterations)
    ax.plot(
        iterations,
        mean_history,
        'o-',
        label=strategy.upper(),
        linewidth=2,
        markersize=4
    )
ax.set_xlabel('ÂÆüÈ®ìÂõûÊï∞', fontsize=12)
ax.set_ylabel('R¬≤ „Çπ„Ç≥„Ç¢', fontsize=12)
ax.set_title('„É¢„Éá„É´Á≤æÂ∫¶', fontsize=14)
ax.legend()
ax.grid(True, alpha=0.3)

# Ë®àÁÆóÊôÇÈñì
ax = axes[2]
time_means = [
    np.mean(results[strategy]['computation_time'])
    for strategy in strategies
]
ax.bar(
    [s.upper() for s in strategies],
    time_means,
    color=['blue', 'orange', 'green'],
    alpha=0.7
)
ax.set_ylabel('Ë®àÁÆóÊôÇÈñìÔºàÁßíÔºâ', fontsize=12)
ax.set_title('Ë®àÁÆó„Ç≥„Çπ„Éà', fontsize=14)
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('strategy_comparison.png', dpi=150)
plt.show()
</code></pre>


**ÊúüÂæÖ„Åï„Çå„ÇãÂá∫Âäõ**:

<pre><code>=== Á∑èÂêàÊØîËºÉÔºà50ÂÆüÈ®ìÂæåÔºâ===

UNCERTAINTY:
  ÊúÄËâØÂÄ§: 45.67 (ÈÅîÊàêÁéá: 96.2%)
  R¬≤ „Çπ„Ç≥„Ç¢: 0.834
  Ë®àÁÆóÊôÇÈñì: 12.34Áßí

DIVERSITY:
  ÊúÄËâØÂÄ§: 42.34 (ÈÅîÊàêÁéá: 89.2%)
  R¬≤ „Çπ„Ç≥„Ç¢: 0.812
  Ë®àÁÆóÊôÇÈñì: 8.56Áßí

QBC:
  ÊúÄËâØÂÄ§: 46.23 (ÈÅîÊàêÁéá: 97.4%)
  R¬≤ „Çπ„Ç≥„Ç¢: 0.856
  Ë®àÁÆóÊôÇÈñì: 38.12Áßí
</code></pre>


**ÁµêË´ñ**:
1. **Query-by-CommitteeÔºàQBCÔºâ**„ÅåÊúÄÈ´òÊÄßËÉΩÔºàÈÅîÊàêÁéá97.4%„ÄÅR¬≤=0.856Ôºâ
2. „Åó„Åã„ÅóË®àÁÆóÊôÇÈñì„Åå3ÂÄç‰ª•‰∏äÔºà38.12Áßí vs 12.34ÁßíÔºâ
3. **Uncertainty Sampling„ÅåÁ∑èÂêàÁöÑ„Å´„Éê„É©„É≥„ÇπËâØÂ•Ω**
   - ÈÅîÊàêÁéá96.2%ÔºàQBC„Å®1.2%„ÅÆÂ∑ÆÔºâ
   - R¬≤=0.834ÔºàQBC„Å®0.022„ÅÆÂ∑ÆÔºâ
   - Ë®àÁÆóÊôÇÈñì„ÅØ1/3

**Êé®Â•®**:
- **ÊôÇÈñìÂà∂Á¥Ñ„Åå„Å™„ÅÑ**: QBC
- **„Éê„É©„É≥„ÇπÈáçË¶ñ**: Uncertainty Sampling
- **Ë®àÁÆó„Ç≥„Çπ„ÉàÈáçË¶ñ**: Diversity Sampling

</details>

<hr />
<h2>„Éá„Éº„Çø„É©„Ç§„Çª„É≥„Çπ„Å®ÂºïÁî®</h2>
<h3>„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Éá„Éº„Çø„Çª„ÉÉ„Éà</h3>
<p>Êú¨Á´†„ÅÆ„Ç≥„Éº„Éâ‰æã„Åß‰ΩøÁî®„Åß„Åç„ÇãActive LearningÂêë„Åë„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºö</p>
<h4>1. UCI Machine Learning Repository</h4>
<ul>
<li><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: CC BY 4.0</li>
<li><strong>ÂºïÁî®</strong>: Dua, D. and Graff, C. (2019). UCI Machine Learning Repository. University of California, Irvine, School of Information and Computer Sciences.</li>
<li><strong>Êé®Â•®„Éá„Éº„Çø„Çª„ÉÉ„Éà</strong>:</li>
<li><code>make_regression()</code> (scikit-learnÂÜÖËîµ)</li>
<li>Wine Quality Dataset</li>
<li>Boston Housing Dataset</li>
</ul>
<h4>2. Materials Project API</h4>
<ul>
<li><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: CC BY 4.0</li>
<li><strong>ÂºïÁî®</strong>: Jain, A. et al. (2013). "Commentary: The Materials Project: A materials genome approach to accelerating materials innovation." <em>APL Materials</em>, 1(1), 011002.</li>
<li><strong>Áî®ÈÄî</strong>: ÊùêÊñôÁßëÂ≠¶„ÅÆActive LearningÂÆüÈ®ìÔºà„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó„ÄÅÂΩ¢Êàê„Ç®„Éç„É´„ÇÆ„ÉºÔºâ</li>
<li><strong>APIÂèñÂæó</strong>: https://materialsproject.org/api</li>
</ul>
<h4>3. Matbench Datasets</h4>
<ul>
<li><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: MIT License</li>
<li><strong>ÂºïÁî®</strong>: Dunn, A. et al. (2020). "Benchmarking materials property prediction methods: the Matbench test set and Automatminer reference algorithm." <em>npj Computational Materials</em>, 6(1), 138.</li>
<li><strong>Áî®ÈÄî</strong>: ÊùêÊñôÁâπÊÄß‰∫àÊ∏¨„ÅÆActive Learning</li>
</ul>
<h3>„É©„Ç§„Éñ„É©„É™„É©„Ç§„Çª„É≥„Çπ</h3>
<p>Êú¨Á´†„Åß‰ΩøÁî®„Åô„Çã‰∏ªË¶Å„É©„Ç§„Éñ„É©„É™„ÅÆ„É©„Ç§„Çª„É≥„ÇπÔºö</p>
<table>
<thead>
<tr>
<th>„É©„Ç§„Éñ„É©„É™</th>
<th>„Éê„Éº„Ç∏„Éß„É≥</th>
<th>„É©„Ç§„Çª„É≥„Çπ</th>
<th>Áî®ÈÄî</th>
</tr>
</thead>
<tbody>
<tr>
<td>modAL</td>
<td>0.4.1</td>
<td>MIT</td>
<td>Active Learning Framework</td>
</tr>
<tr>
<td>scikit-learn</td>
<td>1.3.0</td>
<td>BSD-3-Clause</td>
<td>Ê©üÊ¢∞Â≠¶Áøí„ÉªÂâçÂá¶ÁêÜ</td>
</tr>
<tr>
<td>numpy</td>
<td>1.24.3</td>
<td>BSD-3-Clause</td>
<td>Êï∞ÂÄ§Ë®àÁÆó</td>
</tr>
<tr>
<td>matplotlib</td>
<td>3.7.1</td>
<td>PSF (BSD-like)</td>
<td>ÂèØË¶ñÂåñ</td>
</tr>
</tbody>
</table>
<p><strong>„É©„Ç§„Çª„É≥„ÇπÈÅµÂÆà</strong>:
- ÂÖ®„Å¶ÂïÜÁî®Âà©Áî®ÂèØËÉΩ
- ÂÜçÈÖçÂ∏ÉÊôÇ„ÅØÂÖÉ„ÅÆ„É©„Ç§„Çª„É≥„ÇπË°®Á§∫„ÇíÁ∂≠ÊåÅ
- Â≠¶Ë°ìË´ñÊñá„Åß„ÅØÈÅ©Âàá„Å´ÂºïÁî®</p>
<hr />
<h2>ÂÜçÁèæÊÄß„ÅÆÁ¢∫‰øù</h2>
<h3>‰π±Êï∞„Ç∑„Éº„ÉâË®≠ÂÆö</h3>
<p>Active Learning„ÅÆÂÆüÈ®ì„ÇíÂÜçÁèæÂèØËÉΩ„Å´„Åô„Çã„Åü„ÇÅ„ÄÅ‰ª•‰∏ã„ÅÆ‰π±Êï∞„Ç∑„Éº„Éâ„ÇíÂÖ®„Å¶„ÅÆ„Ç≥„Éº„Éâ„ÅßË®≠ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö</p>
<pre><code class="language-python">import numpy as np
import random

# ÂÖ®„Å¶„ÅÆ‰π±Êï∞„Ç∑„Éº„Éâ„ÇíÂõ∫ÂÆö
SEED = 42
np.random.seed(SEED)
random.seed(SEED)

# scikit-learnÂÜÖ„ÅÆ„É©„É≥„ÉÄ„É†ÊÄß„ÇÇÂà∂Âæ°
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(n_estimators=100, random_state=SEED)
</code></pre>
<p><strong>ÈáçË¶Å„Å™„Éù„Ç§„É≥„Éà</strong>:
- „Éá„Éº„ÇøÂàÜÂâ≤: <code>train_test_split(..., random_state=SEED)</code>
- „É¢„Éá„É´ÂàùÊúüÂåñ: <code>RandomForestRegressor(..., random_state=SEED)</code>
- ÂàùÊúü„Çµ„É≥„Éó„É´ÈÅ∏Êäû: <code>np.random.choice(..., replace=False)</code> „ÅÆÂâç„Å´„Ç∑„Éº„ÉâË®≠ÂÆö</p>
<h3>„É©„Ç§„Éñ„É©„É™„Éê„Éº„Ç∏„Éß„É≥ÁÆ°ÁêÜ</h3>
<p>ÂÆüÈ®ìÁí∞Â¢É„ÇíÂÆåÂÖ®„Å´ÂÜçÁèæ„Åô„Çã„Åü„ÇÅ„ÄÅ<code>requirements.txt</code>„Çí‰ΩúÊàêÔºö</p>
<pre><code class="language-bash"># requirements.txt
numpy==1.24.3
scikit-learn==1.3.0
matplotlib==3.7.1
scipy==1.11.1
pandas==2.0.3

# ÁîüÊàêÊñπÊ≥ï
pip freeze &gt; requirements.txt

# ÂÜçÁèæÊñπÊ≥ï
pip install -r requirements.txt
</code></pre>
<h3>ÂÆüÈ®ì„É≠„Ç∞„ÅÆË®òÈå≤</h3>
<p>Active Learning„ÅÆÂÖ®„Ç§„ÉÜ„É¨„Éº„Ç∑„Éß„É≥„ÇíË®òÈå≤Ôºö</p>
<pre><code class="language-python">import pandas as pd
from datetime import datetime

# ÂÆüÈ®ì„É≠„Ç∞„ÅÆDataFrame
experiment_log = []

for iteration in range(n_iterations):
    # ... Active Learning„É´„Éº„Éó ...

    log_entry = {
        'iteration': iteration,
        'timestamp': datetime.now(),
        'selected_sample_index': next_idx,
        'uncertainty': uncertainties[next_idx],
        'true_value': y_pool[next_idx],
        'best_so_far': y_train.max(),
        'model_r2': r2
    }
    experiment_log.append(log_entry)

# CSV‰øùÂ≠ò
df_log = pd.DataFrame(experiment_log)
df_log.to_csv('active_learning_log.csv', index=False)
print(f&quot;ÂÆüÈ®ì„É≠„Ç∞„Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü: active_learning_log.csv&quot;)
</code></pre>
<hr />
<h2>„Çà„Åè„ÅÇ„ÇãËêΩ„Å®„ÅóÁ©¥„Å®ÂØæÂá¶Ê≥ï</h2>
<h3>1. Cold Start ProblemÔºàÂàùÊúü„Éá„Éº„Çø‰∏çË∂≥Ôºâ</h3>
<p><strong>ÂïèÈ°å</strong>: ÂàùÊúü„É©„Éô„É´‰ªò„Åç„Éá„Éº„Çø„ÅåÂ∞ë„Å™„Åô„Åé„Å¶„ÄÅ‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö„Åå‰∏çÂÆâÂÆö</p>
<p><strong>ÁóáÁä∂</strong>:</p>
<pre><code class="language-python"># NG‰æã: ÂàùÊúü„Éá„Éº„Çø3„Çµ„É≥„Éó„É´„Åó„Åã„Å™„ÅÑ
initial_size = 3  # Â∞ë„Å™„Åô„ÅéÔºÅ
X_train = X[:initial_size]
</code></pre>
<p><strong>ÂØæÂá¶Ê≥ï</strong>:</p>
<pre><code class="language-python"># OK‰æã: ÁâπÂæ¥ÈáèÊ¨°ÂÖÉ„ÅÆ2-5ÂÄç„ÅÆÂàùÊúü„Çµ„É≥„Éó„É´
feature_dim = X.shape[1]
initial_size = max(10, feature_dim * 3)  # 10‰ª•‰∏ä„ÄÅ„Åã„Å§ÁâπÂæ¥ÈáèÊï∞„ÅÆ3ÂÄç
X_train = X[:initial_size]

print(f&quot;ÂàùÊúü„Çµ„É≥„Éó„É´Êï∞: {initial_size} (ÁâπÂæ¥ÈáèÊï∞: {feature_dim})&quot;)
# Âá∫Âäõ: ÂàùÊúü„Çµ„É≥„Éó„É´Êï∞: 15 (ÁâπÂæ¥ÈáèÊï∞: 5)
</code></pre>
<p><strong>Êé®Â•®„É´„Éº„É´</strong>:
- ÊúÄ‰Ωé10„Çµ„É≥„Éó„É´
- ÁêÜÊÉ≥ÁöÑ„Å´„ÅØÁâπÂæ¥ÈáèÊ¨°ÂÖÉ„ÅÆ3-5ÂÄç
- Ë§áÈõë„Å™„É¢„Éá„É´ÔºàNN„ÄÅGPÔºâ„Åª„Å©Â§ö„ÇÅ„Å´ÂøÖË¶Å</p>
<hr />
<h3>2. Query Selection BiasÔºàÈÅ∏Êäû„Éê„Ç§„Ç¢„ÇπÔºâ</h3>
<p><strong>ÂïèÈ°å</strong>: Uncertainty Sampling„Å†„Åë„Å†„Å®Âêå„ÅòÈ†òÂüü„Å∞„Åã„ÇäÈÅ∏Êäû</p>
<p><strong>ÁóáÁä∂</strong>:</p>
<pre><code class="language-python"># NG‰æã: ‰∏çÁ¢∫ÂÆüÊÄß„Å†„Åë„ÅßÈÅ∏Êäû
selected_idx = np.argmax(uncertainties)  # ÊØéÂõû‰ºº„ÅüÈ†òÂüü„ÇíÈÅ∏„Çì„Åß„Åó„Åæ„ÅÜ
</code></pre>
<p><strong>ÂØæÂá¶Ê≥ï1: Œµ-greedy</strong>:</p>
<pre><code class="language-python"># OK‰æã: Êé¢Á¥¢„Å®Ê¥ªÁî®„ÅÆ„Éê„É©„É≥„Çπ
epsilon = 0.2

if np.random.rand() &lt; epsilon:
    # Êé¢Á¥¢: „É©„É≥„ÉÄ„É†„Å´ÈÅ∏Êäû
    selected_idx = np.random.choice(len(X_pool))
else:
    # Ê¥ªÁî®: ‰∏çÁ¢∫ÂÆüÊÄß„ÅßÈÅ∏Êäû
    selected_idx = np.argmax(uncertainties)
</code></pre>
<p><strong>ÂØæÂá¶Ê≥ï2: Batch Diversity</strong>:</p>
<pre><code class="language-python"># OK‰æã: „Éê„ÉÉ„ÉÅÈÅ∏Êäû„ÅßÂ§öÊßòÊÄß„ÇíÁ¢∫‰øù
from sklearn.metrics import pairwise_distances

def diverse_batch_selection(X_pool, uncertainties, batch_size=5):
    selected_indices = []

    # ÊúÄ„ÇÇ‰∏çÁ¢∫ÂÆü„Å™„Çµ„É≥„Éó„É´„ÇíÊúÄÂàù„Å´ÈÅ∏Êäû
    first_idx = np.argmax(uncertainties)
    selected_indices.append(first_idx)

    # ÊÆã„Çä„ÅØÂ§öÊßòÊÄß„ÇíËÄÉÊÖÆ
    for _ in range(batch_size - 1):
        # Êó¢ÈÅ∏Êäû„Çµ„É≥„Éó„É´„Å®„ÅÆË∑ùÈõ¢„ÇíË®àÁÆó
        distances = pairwise_distances(
            X_pool,
            X_pool[selected_indices]
        ).min(axis=1)

        # ‰∏çÁ¢∫ÂÆüÊÄß„Å®Ë∑ùÈõ¢„ÅÆÁ©ç„ÅßÈÅ∏Êäû
        scores = uncertainties * distances
        scores[selected_indices] = -np.inf
        next_idx = np.argmax(scores)
        selected_indices.append(next_idx)

    return selected_indices
</code></pre>
<hr />
<h3>3. Stopping Criteria ErrorsÔºàÂÅúÊ≠¢Âü∫Ê∫ñ„ÅÆË™§„ÇäÔºâ</h3>
<p><strong>ÂïèÈ°å</strong>: „ÅÑ„Å§Active Learning„ÇíÊ≠¢„ÇÅ„Çã„Åπ„Åç„Åã‰∏çÊòéÁ¢∫</p>
<p><strong>NG‰æã</strong>: Âõ∫ÂÆö„Ç§„ÉÜ„É¨„Éº„Ç∑„Éß„É≥Êï∞„Å†„Åë</p>
<pre><code class="language-python"># NG‰æã: ÊîπÂñÑ„Åó„Å™„Åè„Å¶„ÇÇÁÑ°ÈßÑ„Å´Á∂ôÁ∂ö
for i in range(100):  # 100ÂõûÂøÖ„ÅöÂÆüË°å„Åó„Å¶„Åó„Åæ„ÅÜ
    # ... Active Learning ...
</code></pre>
<p><strong>ÂØæÂá¶Ê≥ï: Ë§áÊï∞„ÅÆÂÅúÊ≠¢Âü∫Ê∫ñ</strong>:</p>
<pre><code class="language-python"># OK‰æã: Ë§áÊï∞„ÅÆÂÅúÊ≠¢Êù°‰ª∂„ÇíË®≠ÂÆö
class StoppingCriteria:
    def __init__(self,
                 max_iterations=100,
                 performance_threshold=0.95,
                 patience=5):
        self.max_iterations = max_iterations
        self.performance_threshold = performance_threshold
        self.patience = patience
        self.best_performance = -np.inf
        self.no_improvement_count = 0

    def should_stop(self, iteration, current_performance):
        # Âü∫Ê∫ñ1: ÊúÄÂ§ß„Ç§„ÉÜ„É¨„Éº„Ç∑„Éß„É≥Êï∞
        if iteration &gt;= self.max_iterations:
            return True, &quot;Max iterations reached&quot;

        # Âü∫Ê∫ñ2: ÁõÆÊ®ôÊÄßËÉΩÈÅîÊàê
        if current_performance &gt;= self.performance_threshold:
            return True, f&quot;Target performance achieved: {current_performance:.3f}&quot;

        # Âü∫Ê∫ñ3: ÊîπÂñÑ„ÅåÂÅúÊªûÔºàEarly StoppingÔºâ
        if current_performance &gt; self.best_performance:
            self.best_performance = current_performance
            self.no_improvement_count = 0
        else:
            self.no_improvement_count += 1

        if self.no_improvement_count &gt;= self.patience:
            return True, f&quot;No improvement for {self.patience} iterations&quot;

        return False, &quot;Continue&quot;

# ‰ΩøÁî®‰æã
stopper = StoppingCriteria(max_iterations=100,
                           performance_threshold=0.95,
                           patience=5)

for iteration in range(100):
    # ... Active Learning„É´„Éº„Éó ...

    r2_score = evaluate_model(model, X_test, y_test)
    should_stop, reason = stopper.should_stop(iteration, r2_score)

    if should_stop:
        print(f&quot;Stopping at iteration {iteration}: {reason}&quot;)
        break
</code></pre>
<hr />
<h3>4. Distribution ShiftÔºàÂàÜÂ∏É„Ç∑„Éï„ÉàÔºâ</h3>
<p><strong>ÂïèÈ°å</strong>: „É©„Éô„É´‰ªò„Åç„Éó„Éº„É´„Å®Êú™„É©„Éô„É´„Éó„Éº„É´„ÅÆÂàÜÂ∏É„ÅåÁï∞„Å™„Çã</p>
<p><strong>ÁóáÁä∂</strong>:</p>
<pre><code class="language-python"># NG‰æã: ÂÅè„Å£„Åü„Éó„Éº„É´ÂàÜÂâ≤
X_labeled = X[:50]  # ÊúÄÂàù„ÅÆ50„Çµ„É≥„Éó„É´„Å†„Åë
X_pool = X[50:]     # ÂàÜÂ∏É„ÅåÂÅè„ÇãÂèØËÉΩÊÄß
</code></pre>
<p><strong>ÂØæÂá¶Ê≥ï</strong>:</p>
<pre><code class="language-python"># OK‰æã: Â±§Âåñ„Çµ„É≥„Éó„É™„É≥„Ç∞„ÅßÂàÜÂ∏É„Çí‰øùÊåÅ
from sklearn.model_selection import train_test_split

# ÁõÆÊ®ôÂ§âÊï∞„ÅÆ„Éí„Çπ„Éà„Ç∞„É©„É†„Çí‰øùÊåÅ„Åó„Å™„Åå„ÇâÂàÜÂâ≤
X_labeled, X_pool, y_labeled, y_pool = train_test_split(
    X, y,
    train_size=50,
    stratify=pd.cut(y, bins=5),  # y„Çí5ÊÆµÈöé„Å´ÂàÜÂâ≤„Åó„Å¶Â±§Âåñ
    random_state=42
)

print(f&quot;Labeled pool mean: {y_labeled.mean():.2f}&quot;)
print(f&quot;Unlabeled pool mean: {y_pool.mean():.2f}&quot;)
# ‰∏°ËÄÖ„ÅåËøë„ÅÑ„Åì„Å®„ÇíÁ¢∫Ë™ç
</code></pre>
<hr />
<h3>5. Label Noise HandlingÔºà„É©„Éô„É´„Éé„Ç§„Ç∫„ÅÆÊâ±„ÅÑÔºâ</h3>
<p><strong>ÂïèÈ°å</strong>: ÂÆüÈ®ìÊ∏¨ÂÆö„Å´„Éé„Ç§„Ç∫„ÅåÂê´„Åæ„Çå„ÇãÂ†¥Âêà„ÄÅË™§„Å£„Åü„Çµ„É≥„Éó„É´„ÇíÂ≠¶Áøí</p>
<p><strong>ÂØæÂá¶Ê≥ï1: ‰∏çÁ¢∫ÂÆüÊÄß„ÅÆÈñæÂÄ§Ë®≠ÂÆö</strong>:</p>
<pre><code class="language-python"># ‰∏çÁ¢∫ÂÆüÊÄß„ÅåÊ•µÁ´Ø„Å´È´ò„ÅÑ„Çµ„É≥„Éó„É´„ÅØÂÜçÊ∏¨ÂÆö
def select_with_noise_awareness(uncertainties, threshold=3.0):
    # ‰∏çÁ¢∫ÂÆüÊÄß„ÅÆÊ®ôÊ∫ñÂÅèÂ∑Æ
    unc_mean = uncertainties.mean()
    unc_std = uncertainties.std()

    # Áï∞Â∏∏„Å´È´ò„ÅÑ‰∏çÁ¢∫ÂÆüÊÄß„ÅØÈô§Â§ñ
    valid_mask = uncertainties &lt; (unc_mean + threshold * unc_std)

    if valid_mask.sum() == 0:
        # ÂÖ®„Å¶Èô§Â§ñ„Åï„Çå„ÅüÂ†¥Âêà„ÅØÊúÄÂ∞èÂÄ§„ÇíÈÅ∏Êäû
        return np.argmin(uncertainties)

    # ÊúâÂäπÁØÑÂõ≤ÂÜÖ„ÅßÊúÄÂ§ß‰∏çÁ¢∫ÂÆüÊÄß„ÇíÈÅ∏Êäû
    valid_indices = np.where(valid_mask)[0]
    selected_idx = valid_indices[np.argmax(uncertainties[valid_mask])]

    return selected_idx
</code></pre>
<p><strong>ÂØæÂá¶Ê≥ï2: Ensemble Robustness</strong>:</p>
<pre><code class="language-python"># Ë§áÊï∞„É¢„Éá„É´„ÅÆ„Ç≥„É≥„Çª„É≥„Çµ„Çπ„ÅßÈÅ∏Êäû
def robust_query_selection(X_pool, models):
    all_uncertainties = []

    for model in models:
        # ÂêÑ„É¢„Éá„É´„Åß‰∏çÁ¢∫ÂÆüÊÄß„ÇíË®àÁÆó
        predictions = np.array([tree.predict(X_pool)
                               for tree in model.estimators_])
        uncertainty = np.std(predictions, axis=0)
        all_uncertainties.append(uncertainty)

    # Ë§áÊï∞„É¢„Éá„É´„ÅÆ‰∏çÁ¢∫ÂÆüÊÄß„ÅÆ‰∏≠Â§ÆÂÄ§„Çí‰ΩøÁî®Ôºà„É≠„Éê„Çπ„ÉàÔºâ
    robust_uncertainty = np.median(all_uncertainties, axis=0)
    selected_idx = np.argmax(robust_uncertainty)

    return selected_idx
</code></pre>
<hr />
<h3>6. Computational Cost of Uncertainty Estimation</h3>
<p><strong>ÂïèÈ°å</strong>: ‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö„Å´ÊôÇÈñì„Åå„Åã„Åã„Çä„Åô„Åé„ÇãÔºàGPU„ÅÆN^3Ë®àÁÆóÈáè„Å™„Å©Ôºâ</p>
<p><strong>ÂØæÂá¶Ê≥ï: ÂÄôË£ú„Éó„Éº„É´„ÅÆ‰∫ãÂâçÂâäÊ∏õ</strong>:</p>
<pre><code class="language-python"># OK‰æã: ÂÄôË£ú„Çí„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Åß‰ª£Ë°®ÁÇπ„Å´Áµû„Çã
from sklearn.cluster import KMeans

def efficient_query_selection(X_pool, n_candidates=100, n_select=5):
    # Step 1: „ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Åß‰ª£Ë°®ÁÇπ„ÇíÈÅ∏Êäû
    if len(X_pool) &gt; n_candidates:
        kmeans = KMeans(n_clusters=n_candidates, random_state=42)
        kmeans.fit(X_pool)

        # ÂêÑ„ÇØ„É©„Çπ„Çø„ÅÆ‰∏≠ÂøÉ„Å´ÊúÄ„ÇÇËøë„ÅÑÁÇπ„Çí‰ª£Ë°®ÁÇπ„Å®„Åô„Çã
        candidates_idx = []
        for i in range(n_candidates):
            cluster_points = np.where(kmeans.labels_ == i)[0]
            center = kmeans.cluster_centers_[i]
            distances = np.linalg.norm(X_pool[cluster_points] - center, axis=1)
            closest_idx = cluster_points[np.argmin(distances)]
            candidates_idx.append(closest_idx)

        X_candidates = X_pool[candidates_idx]
    else:
        candidates_idx = np.arange(len(X_pool))
        X_candidates = X_pool

    # Step 2: ‰ª£Ë°®ÁÇπ„ÅÆ„Åø„Åß‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆöÔºàÈ´òÈÄüÔºâ
    uncertainties = estimate_uncertainty(X_candidates)

    # Step 3: Top-kÈÅ∏Êäû
    top_k_in_candidates = np.argsort(uncertainties)[-n_select:]
    selected_idx = [candidates_idx[i] for i in top_k_in_candidates]

    return selected_idx
</code></pre>
<hr />
<h2>ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà</h2>
<h3>ÂÆüÈ®ìË®≠Ë®à„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà</h3>
<h4>ÂàùÊúüÂåñÊÆµÈöé</h4>
<ul>
<li>[ ] ‰π±Êï∞„Ç∑„Éº„ÉâË®≠ÂÆöÊ∏à„ÅøÔºà<code>np.random.seed(SEED)</code>Ôºâ</li>
<li>[ ] ÂàùÊúü„Çµ„É≥„Éó„É´Êï∞„ÅåÈÅ©ÂàáÔºàÊúÄ‰Ωé10„ÄÅÁêÜÊÉ≥„ÅØÁâπÂæ¥ÈáèÊï∞√ó3-5Ôºâ</li>
<li>[ ] „Éá„Éº„ÇøÂàÜÂâ≤„ÅåÂ±§Âåñ„Çµ„É≥„Éó„É™„É≥„Ç∞Ê∏à„ÅøÔºàÂàÜÂ∏É„Ç∑„Éï„ÉàÂõûÈÅøÔºâ</li>
<li>[ ] „É©„Ç§„Éñ„É©„É™„Éê„Éº„Ç∏„Éß„É≥„Çí<code>requirements.txt</code>„Å´Ë®òÈå≤Ê∏à„Åø</li>
</ul>
<h4>Query StrategyÈÅ∏Êäû</h4>
<ul>
<li>[ ] „Çø„Çπ„ÇØ„Å´Âøú„Åò„ÅüÈÅ©Âàá„Å™ÊâãÊ≥ï„ÇíÈÅ∏Êäû</li>
<li>Â∫ÉÁØÑÂõ≤Êé¢Á¥¢ ‚Üí Diversity Sampling</li>
<li>ÂäπÁéáÁöÑÂèéÊùü ‚Üí Uncertainty Sampling</li>
<li>„É¢„Éá„É´„É≠„Éê„Çπ„Éà ‚Üí Query-by-Committee</li>
<li>[ ] Êé¢Á¥¢-Ê¥ªÁî®„ÅÆ„Éê„É©„É≥„ÇπË®≠ÂÆöÔºàŒµ-greedy, UCBÔºâ</li>
<li>[ ] „Éê„ÉÉ„ÉÅÈÅ∏ÊäûÊôÇ„ÅØÂ§öÊßòÊÄß„ÇíËÄÉÊÖÆ</li>
</ul>
<h4>ÂÅúÊ≠¢Âü∫Ê∫ñË®≠Ë®à</h4>
<ul>
<li>[ ] ÊúÄÂ§ß„Ç§„ÉÜ„É¨„Éº„Ç∑„Éß„É≥Êï∞„ÇíË®≠ÂÆö</li>
<li>[ ] ÁõÆÊ®ôÊÄßËÉΩÊåáÊ®ô„ÇíÂÆöÁæ©ÔºàR¬≤, RMSEÁ≠âÔºâ</li>
<li>[ ] Early StoppingÊù°‰ª∂„ÇíË®≠ÂÆöÔºàpatience=5-10Ôºâ</li>
<li>[ ] ‰∫àÁÆó‰∏äÈôêÔºàÂÆüÈ®ì„Ç≥„Çπ„Éà„ÄÅÊôÇÈñìÔºâ„ÇíÊòéÁ¢∫Âåñ</li>
</ul>
<h4>„É¢„Éá„É´ÈÅ∏Êäû</h4>
<ul>
<li>[ ] ‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö„ÅåÂèØËÉΩ„Å™„É¢„Éá„É´„ÇíÈÅ∏Êäû</li>
<li>EnsembleÊ≥ïÔºàRF, LightGBMÔºâ</li>
<li>MC DropoutÔºàNNÔºâ</li>
<li>Gaussian Process</li>
<li>[ ] „Éá„Éº„Çø„Çµ„Ç§„Ç∫„Å´Âøú„Åò„Åü„É¢„Éá„É´ÈÅ∏Êäû</li>
<li>Â∞èË¶èÊ®°Ôºà&lt;1000Ôºâ‚Üí GP</li>
<li>‰∏≠Ë¶èÊ®°Ôºà1000-10000Ôºâ‚Üí RF, LightGBM</li>
<li>Â§ßË¶èÊ®°Ôºà&gt;10000Ôºâ‚Üí MC Dropout</li>
</ul>
<h3>ÂÆüË£ÖÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà</h3>
<h4>„Éá„Éº„ÇøÂâçÂá¶ÁêÜ</h4>
<ul>
<li>[ ] Ê¨†ÊêçÂÄ§Âá¶ÁêÜÊ∏à„ÅøÔºàÂâäÈô§orË£úÂÆåÔºâ</li>
<li>[ ] Â§ñ„ÇåÂÄ§Ê§úÂá∫„ÉªÂØæÂá¶Ê∏à„ÅøÔºàIQRÊ≥ïÁ≠âÔºâ</li>
<li>[ ] ÁâπÂæ¥Èáè„Çπ„Ç±„Éº„É™„É≥„Ç∞Ê∏à„ÅøÔºàÊ®ôÊ∫ñÂåñorÊ≠£Ë¶èÂåñÔºâ</li>
<li>[ ] „Éá„Éº„Çø„É™„Éº„Ç±„Éº„Ç∏„Åå„Å™„ÅÑÔºà„ÉÜ„Çπ„Éà„Éá„Éº„ÇøÂàÜÈõ¢Ôºâ</li>
</ul>
<h4>„Ç≥„Éº„ÉâÂìÅË≥™</h4>
<ul>
<li>[ ] Èñ¢Êï∞„Å´Âûã„Éí„É≥„Éà‰ªò‰∏éÔºà<code>def func(x: np.ndarray) -&gt; float:</code>Ôºâ</li>
<li>[ ] DocstringË®òËø∞Ê∏à„ÅøÔºàÂºïÊï∞„ÄÅËøî„ÇäÂÄ§„ÄÅÁî®ÈÄîÔºâ</li>
<li>[ ] „Ç®„É©„Éº„Éè„É≥„Éâ„É™„É≥„Ç∞ÂÆüË£ÖÔºàtry-exceptÔºâ</li>
<li>[ ] „É≠„Ç∞Âá∫ÂäõÂÆüË£ÖÔºàÂÆüÈ®ìËøΩË∑°ÂèØËÉΩÔºâ</li>
</ul>
<h4>Ë©ï‰æ°„Å®Ê§úË®º</h4>
<ul>
<li>[ ] Ë§áÊï∞Ë©ï‰æ°ÊåáÊ®ô„ÇíË®àÁÆóÔºàR¬≤, RMSE, MAEÔºâ</li>
<li>[ ] Â≠¶ÁøíÊõ≤Á∑ö„Çí„Éó„É≠„ÉÉ„ÉàÔºàÂèçÂæ©ÂõûÊï∞ vs ÊÄßËÉΩÔºâ</li>
<li>[ ] „É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞„Å®ÊØîËºÉ</li>
<li>[ ] Áµ±Ë®àÁöÑÊúâÊÑèÊÄßÊ§úË®ºÔºàË§áÊï∞Ë©¶Ë°å„ÅÆÂπ≥Âùá¬±Ê®ôÊ∫ñÂÅèÂ∑ÆÔºâ</li>
</ul>
<h3>ÊùêÊñôÁßëÂ≠¶ÁâπÊúâ„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà</h3>
<h4>Áâ©ÁêÜÂà∂Á¥Ñ„ÅÆËÄÉÊÖÆ</h4>
<ul>
<li>[ ] Êé¢Á¥¢Á©∫Èñì„ÅÆÁâ©ÁêÜÁöÑÂ¶•ÂΩìÊÄß„ÉÅ„Çß„ÉÉ„ÇØ</li>
<li>Ê∏©Â∫¶ÁØÑÂõ≤: 0-1500‚ÑÉ</li>
<li>ÁµÑÊàêÊØî: ÂêàË®à100%</li>
<li>pHÁØÑÂõ≤: 0-14</li>
<li>[ ] Âçò‰Ωç„ÅÆÊï¥ÂêàÊÄßÁ¢∫Ë™çÔºànm, eV, GPaÁ≠âÔºâ</li>
<li>[ ] ÂêàÊàêÂèØËÉΩÊÄßÂà∂Á¥ÑÔºàÂÆüÈ®ìÊù°‰ª∂„ÅÆÂÆüÁèæÂèØËÉΩÊÄßÔºâ</li>
</ul>
<h4>„Éâ„É°„Ç§„É≥Áü•Ë≠ò„ÅÆÁµ±Âêà</h4>
<ul>
<li>[ ] Áâ©ÁêÜÁöÑprior knowledgeÊ¥ªÁî®</li>
<li>„Ç´„Éº„Éç„É´ÈÅ∏ÊäûÔºàÂë®ÊúüÊÄß„ÄÅÊªë„Çâ„Åã„ÅïÔºâ</li>
<li>ÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞ÔºàË®òËø∞Â≠êÔºâ</li>
<li>[ ] Êó¢Áü•„ÅÆÁâ©ÁêÜÊ≥ïÂâá„Å®„ÅÆÊï¥ÂêàÊÄßÁ¢∫Ë™ç</li>
<li>„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó &gt; 0</li>
<li>ÂØÜÂ∫¶ &gt; 0</li>
</ul>
<h4>ÂÆüÈ®ìÁµ±Âêà</h4>
<ul>
<li>[ ] Ê∏¨ÂÆöË™§Â∑Æ„ÅÆËÄÉÊÖÆÔºà„Éé„Ç§„Ç∫È†ÖÔºâ</li>
<li>[ ] ÂÆüÈ®ì„Ç≥„Çπ„ÉàÈñ¢Êï∞„ÅÆÂÆöÁæ©</li>
<li>[ ] „Éê„ÉÉ„ÉÅÂÆüÈ®ì„ÅÆË®≠Ë®àÔºà‰∏¶ÂàóÂåñÂèØËÉΩÊÄßÔºâ</li>
</ul>
<hr />
<h2>ÂÆüË∑µÊºîÁøí„ÅÆËøΩÂä†„Ç¨„Ç§„Éâ</h2>
<h3>ÊºîÁøí1„ÅÆÂÆåÂÖ®Ëß£Á≠î‰æãÔºàCNTÈõªÊ∞ó‰ºùÂ∞éÂ∫¶‰∫àÊ∏¨Ôºâ</h3>
<details>
<summary>„ÇØ„É™„ÉÉ„ÇØ„Åó„Å¶ÂÆåÂÖ®„Å™„Ç≥„Éº„Éâ„ÇíË°®Á§∫</summary>


<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt

# ‰π±Êï∞„Ç∑„Éº„ÉâË®≠ÂÆöÔºàÂÜçÁèæÊÄßÁ¢∫‰øùÔºâ
SEED = 42
np.random.seed(SEED)

# „Éá„Éº„ÇøÁîüÊàê
n_samples = 150

# ÁâπÂæ¥Èáè: Áõ¥ÂæÑ„ÄÅÈï∑„Åï„ÄÅÊ¨†Èô•ÂØÜÂ∫¶
diameter = np.random.uniform(0.5, 3.0, n_samples)  # nm
length = np.random.uniform(100, 1000, n_samples)   # nm
defect_density = np.random.uniform(0.01, 0.5, n_samples)  # %

# ÁõÆÊ®ôÂ§âÊï∞: ÈõªÊ∞ó‰ºùÂ∞éÂ∫¶ÔºàS/mÔºâ
# Áâ©ÁêÜ„É¢„Éá„É´: Áõ¥ÂæÑ„Å´ÊØî‰æã„ÄÅÈï∑„Åï„Å´ÊØî‰æã„ÄÅÊ¨†Èô•„Å´ÂèçÊØî‰æã
conductivity = (
    1e5 * diameter / 2.0  # Áõ¥ÂæÑ‰æùÂ≠ò
    * (length / 500)  # Èï∑„Åï‰æùÂ≠ò
    / (1 + 10 * defect_density)  # Ê¨†Èô•‰æùÂ≠ò
    + np.random.normal(0, 5e3, n_samples)  # Ê∏¨ÂÆö„Éé„Ç§„Ç∫
)

# DataFrame‰ΩúÊàê
data = pd.DataFrame({
    'diameter_nm': diameter,
    'length_nm': length,
    'defect_density_pct': defect_density,
    'conductivity_Sm': conductivity
})

print(&quot;=&quot; * 60)
print(&quot;CNTÈõªÊ∞ó‰ºùÂ∞éÂ∫¶„Éá„Éº„Çø„Çª„ÉÉ„Éà&quot;)
print(&quot;=&quot; * 60)
print(data.head())
print(f&quot;\nÁµ±Ë®àÈáè:&quot;)
print(data.describe())

# „Éá„Éº„ÇøÂàÜÂâ≤Ôºà70% train, 30% testÔºâ
from sklearn.model_selection import train_test_split

X = data[['diameter_nm', 'length_nm', 'defect_density_pct']].values
y = data['conductivity_Sm'].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=SEED
)

# LightGBM‰ª£„Çè„Çä„Å´Random Forest‰ΩøÁî®ÔºàLightGBMÊú™„Ç§„É≥„Çπ„Éà„Éº„É´„ÅÆÂ†¥ÂêàÔºâ
from lightgbm import LGBMRegressor

model = LGBMRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    random_state=SEED,
    verbosity=-1
)

# Ë®ìÁ∑¥
model.fit(X_train, y_train)

# ‰∫àÊ∏¨
y_pred = model.predict(X_test)

# Ë©ï‰æ°
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = np.mean(np.abs(y_test - y_pred))

print(f&quot;\nË©ï‰æ°ÁµêÊûú:&quot;)
print(f&quot;  R¬≤ „Çπ„Ç≥„Ç¢: {r2:.4f}&quot;)
print(f&quot;  RMSE: {rmse:.2f} S/m&quot;)
print(f&quot;  MAE: {mae:.2f} S/m&quot;)

# ÁõÆÊ®ôÈÅîÊàêÁ¢∫Ë™ç
if r2 &gt; 0.8 and rmse &lt; 5000:
    print(&quot;\n‚úÖ ÁõÆÊ®ôÈÅîÊàê: R¬≤ &gt; 0.8 „Åã„Å§ RMSE &lt; 5000&quot;)
else:
    print(&quot;\n‚ö†Ô∏è ÁõÆÊ®ôÊú™ÈÅîÊàê: ËøΩÂä†„ÅÆÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞„ÅåÂøÖË¶Å&quot;)

# ÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶
feature_importance = model.feature_importances_
feature_names = ['Diameter', 'Length', 'Defect Density']

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Â∑¶: ‰∫àÊ∏¨ vs ÁúüÂÄ§
ax = axes[0]
ax.scatter(y_test, y_pred, alpha=0.6, edgecolors='k')
ax.plot([y_test.min(), y_test.max()],
        [y_test.min(), y_test.max()],
        'r--', lw=2, label='Perfect Prediction')
ax.set_xlabel('True Conductivity (S/m)', fontsize=12)
ax.set_ylabel('Predicted Conductivity (S/m)', fontsize=12)
ax.set_title(f'CNT Conductivity Prediction (R¬≤={r2:.3f})', fontsize=13)
ax.legend()
ax.grid(True, alpha=0.3)

# Âè≥: ÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶
ax = axes[1]
ax.barh(feature_names, feature_importance, color='skyblue', edgecolor='black')
ax.set_xlabel('Feature Importance', fontsize=12)
ax.set_title('Feature Importance Analysis', fontsize=13)
ax.grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.savefig('cnt_conductivity_analysis.png', dpi=150)
plt.show()

# Ëß£Èáà
most_important = feature_names[np.argmax(feature_importance)]
print(f&quot;\nËß£Èáà: ÊúÄ„ÇÇÂΩ±Èüø„Åô„ÇãÁâπÂæ¥Èáè„ÅØ„Äå{most_important}„Äç„Åß„Åô&quot;)
print(f&quot;  ‚Üí CNT„ÅÆÈõªÊ∞ó‰ºùÂ∞éÂ∫¶„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Å´„ÅØ„ÄÅ{most_important}„ÇíÊúÄÈÅ©Âåñ„Åô„Åπ„Åç„Åß„Åô&quot;)
</code></pre>


</details>

<hr />
<h2>ÂèÇËÄÉÊñáÁåÆ</h2>
<ol>
<li>
<p>Settles, B. (2009). "Active Learning Literature Survey." <em>Computer Sciences Technical Report 1648</em>, University of Wisconsin-Madison.</p>
</li>
<li>
<p>Lookman, T. et al. (2019). "Active learning in materials science with emphasis on adaptive sampling using uncertainties for targeted design." <em>npj Computational Materials</em>, 5(1), 1-17. DOI: <a href="https://doi.org/10.1038/s41524-019-0153-8">10.1038/s41524-019-0153-8</a></p>
</li>
<li>
<p>Raccuglia, P. et al. (2016). "Machine-learning-assisted materials discovery using failed experiments." <em>Nature</em>, 533(7601), 73-76. DOI: <a href="https://doi.org/10.1038/nature17439">10.1038/nature17439</a></p>
</li>
<li>
<p>Ren, F. et al. (2018). "Accelerated discovery of metallic glasses through iteration of machine learning and high-throughput experiments." <em>Science Advances</em>, 4(4), eaaq1566. DOI: <a href="https://doi.org/10.1126/sciadv.aaq1566">10.1126/sciadv.aaq1566</a></p>
</li>
<li>
<p>Kusne, A. G. et al. (2020). "On-the-fly closed-loop materials discovery via Bayesian active learning." <em>Nature Communications</em>, 11(1), 5966. DOI: <a href="https://doi.org/10.1038/s41467-020-19597-w">10.1038/s41467-020-19597-w</a></p>
</li>
</ol>
<hr />
<h2>„Éä„Éì„Ç≤„Éº„Ç∑„Éß„É≥</h2>
<h3>Ê¨°„ÅÆÁ´†</h3>
<p><strong><a href="./chapter-2.html">Á¨¨2Á´†Ôºö‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆöÊâãÊ≥ï ‚Üí</a></strong></p>
<h3>„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°</h3>
<p><strong><a href="./index.html">‚Üê „Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a></strong></p>
<hr />
<h2>ËëóËÄÖÊÉÖÂ†±</h2>
<p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team
<strong>Áõ£‰øÆ</strong>: Dr. Yusuke HashimotoÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ
<strong>‰ΩúÊàêÊó•</strong>: 2025-10-18
<strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0</p>
<p><strong>Êõ¥Êñ∞Â±•Ê≠¥</strong>:
- 2025-10-18: v1.0 ÂàùÁâàÂÖ¨Èñã</p>
<p><strong>„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ</strong>:
- GitHub Issues: <a href="https://github.com/your-repo/AI_Homepage/issues">AI_Homepage/issues</a>
- Email: yusuke.hashimoto.b8@tohoku.ac.jp</p>
<p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
<hr />
<p><strong>Ê¨°„ÅÆÁ´†„Åß‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö„ÅÆË©≥Á¥∞„ÇíÂ≠¶„Å≥„Åæ„Åó„Çá„ÅÜÔºÅ</strong></p><div class="navigation">
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
    <a href="chapter-2.html" class="nav-button">Ê¨°„ÅÆÁ´† ‚Üí</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>Áõ£‰øÆ</strong>: Dr. Yusuke HashimotoÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-18</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
