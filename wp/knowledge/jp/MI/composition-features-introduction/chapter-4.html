<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¨ã®çµ±åˆ - çµ„æˆãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ã§GNNä¸¦ã¿ã®ç²¾åº¦ã‚’å®Ÿç¾">
    <title>ç¬¬4ç« ï¼šæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¨ã®çµ±åˆ - çµ„æˆãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡å…¥é–€ã‚·ãƒªãƒ¼ã‚º</title>

    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #f093fb;
            --accent-color: #f5576c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #f5576c;
            --link-hover: #d64556;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        /* Header with MS Gradient */
        header {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header .container {
            padding: 0 1.5rem;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        .subtitle {
            font-size: 1.2rem;
            margin-bottom: 1rem;
            opacity: 0.95;
        }

        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }

        .meta span {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
        }

        /* Typography */
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            color: var(--primary-color);
        }

        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.6rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1rem;
        }

        /* Links */
        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--link-hover);
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-bottom: 1rem;
            padding-left: 2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Code Blocks */
        pre {
            background: var(--code-bg);
            border-left: 4px solid var(--accent-color);
            padding: 1rem;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border-radius: 4px;
            font-size: 0.9rem;
        }

        code {
            font-family: 'Courier New', Courier, monospace;
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-size: 0.9em;
        }

        pre code {
            background: none;
            padding: 0;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
            overflow-x: auto;
            display: block;
        }

        thead {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            color: white;
        }

        th, td {
            padding: 0.75rem;
            text-align: left;
            border-bottom: 1px solid var(--border-color);
        }

        tbody tr:hover {
            background: #f9f9f9;
        }

        /* Info Boxes */
        .info-box {
            background: #e3f2fd;
            border-left: 4px solid #2196F3;
            padding: 1rem;
            margin-bottom: 1.5rem;
            border-radius: 4px;
        }

        .warning-box {
            background: #fff3e0;
            border-left: 4px solid #ff9800;
            padding: 1rem;
            margin-bottom: 1.5rem;
            border-radius: 4px;
        }

        .tip-box {
            background: #f1f8e9;
            border-left: 4px solid #8bc34a;
            padding: 1rem;
            margin-bottom: 1.5rem;
            border-radius: 4px;
        }

        /* Exercise Boxes */
        .exercise {
            background: #fce4ec;
            border-left: 4px solid #e91e63;
            padding: 1rem;
            margin-bottom: 1.5rem;
            border-radius: 4px;
        }

        .exercise h4 {
            margin-top: 0;
            color: #c2185b;
        }

        /* Details/Summary */
        details {
            background: #f5f5f5;
            border: 1px solid var(--border-color);
            padding: 1rem;
            margin-bottom: 1rem;
            border-radius: 4px;
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--primary-color);
            user-select: none;
        }

        summary:hover {
            color: var(--accent-color);
        }

        /* Mermaid Diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
        }

        /* Navigation */
        .navigation {
            display: flex;
            justify-content: space-between;
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 2px solid var(--border-color);
            flex-wrap: wrap;
            gap: 1rem;
        }

        .nav-button {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            color: white;
            padding: 0.75rem 1.5rem;
            border-radius: 5px;
            text-decoration: none;
            transition: transform 0.2s, box-shadow 0.2s;
            display: inline-block;
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(245, 87, 108, 0.3);
            text-decoration: none;
        }

        /* Learning Objectives */
        .learning-objectives {
            background: linear-gradient(135deg, rgba(240, 147, 251, 0.1) 0%, rgba(245, 87, 108, 0.1) 100%);
            border: 2px solid var(--secondary-color);
            padding: 1.5rem;
            margin-bottom: 2rem;
            border-radius: 8px;
        }

        .learning-objectives h3 {
            margin-top: 0;
            color: var(--accent-color);
        }

        .learning-objectives ul {
            margin-bottom: 0;
        }

        /* Colab Badge */
        .colab-badge {
            display: inline-block;
            margin: 0.5rem 0;
        }

        /* Figure */
        figure {
            margin: 2rem 0;
            text-align: center;
        }

        figcaption {
            margin-top: 0.5rem;
            font-style: italic;
            color: #666;
        }

        /* Responsive */
        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .navigation {
                flex-direction: column;
            }

            .nav-button {
                width: 100%;
                text-align: center;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: 0.5rem;
            }
        }

        /* Benchmark Table Styling */
        .benchmark-table {
            font-size: 0.9rem;
        }

        .benchmark-table th {
            font-size: 0.85rem;
        }

        .benchmark-table td {
            font-size: 0.85rem;
        }

        .best-score {
            font-weight: bold;
            color: #2e7d32;
        }

        /* Hyperparameter Table */
        .hyperparameter-table {
            font-size: 0.9rem;
        }

        .hyperparameter-table code {
            font-size: 0.8rem;
        }
    </style>

    <!-- MathJax Configuration -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Mermaid for Diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- Prism.js for Syntax Highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</head>
<body>
    <header>
        <div class="container">
            <h1>ç¬¬4ç« ï¼šæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¨ã®çµ±åˆ</h1>
            <p class="subtitle">çµ„æˆãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ã§GNNä¸¦ã¿ã®ç²¾åº¦ã‚’å®Ÿç¾</p>
            <div class="meta">
                <span>ğŸ“š çµ„æˆãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡å…¥é–€ã‚·ãƒªãƒ¼ã‚º</span>
                <span>â±ï¸ èª­äº†æ™‚é–“: ç´„30åˆ†</span>
                <span>ğŸ¯ é›£æ˜“åº¦: ä¸­ç´š</span>
            </div>
        </div>
    </header>

    <div class="container">
        <!-- Learning Objectives -->
        <div class="learning-objectives">
            <h3>ğŸ“– ã“ã®ç« ã§å­¦ã¶ã“ã¨</h3>
            <ul>
                <li><strong>åŸºæœ¬ç†è§£</strong>: scikit-learnãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ãƒ¢ãƒ‡ãƒ«é¸æŠåŸºæº–ã€çµ„æˆãƒ™ãƒ¼ã‚¹ vs æ§‹é€ ãƒ™ãƒ¼ã‚¹æ€§èƒ½æ¯”è¼ƒ</li>
                <li><strong>å®Ÿè·µã‚¹ã‚­ãƒ«</strong>: RandomForest/GradientBoosting/MLPå®Ÿè£…ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿GridSearchã€å­¦ç¿’æ›²ç·šåˆ†æ</li>
                <li><strong>å¿œç”¨åŠ›</strong>: SHAPè§£é‡ˆã€è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã€Matbenchãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡</li>
            </ul>
        </div>

        <!-- Introduction -->
        <section id="introduction">
            <h2>å°å…¥</h2>
            <p>
                å‰ç« ã¾ã§ã«ã€çµ„æˆãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ã®ç”Ÿæˆæ–¹æ³•ï¼ˆMagpieã€ElementPropertyã€ã‚«ã‚¹ã‚¿ãƒ Featurizerï¼‰ã‚’å­¦ã³ã¾ã—ãŸã€‚æœ¬ç« ã§ã¯ã€ã“ã‚Œã‚‰ã®ç‰¹å¾´é‡ã‚’æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¨çµ±åˆã—ã€ææ–™ç‰¹æ€§ã‚’é«˜ç²¾åº¦ã«äºˆæ¸¬ã™ã‚‹å®Ÿè·µçš„ãªæ‰‹æ³•ã‚’ç¿’å¾—ã—ã¾ã™ã€‚
            </p>
            <p>
                ç‰¹ã«é‡è¦ãªã®ã¯ã€<strong>çµ„æˆæƒ…å ±ã®ã¿</strong>ã§Graph Neural Networksï¼ˆGNNï¼‰ã«åŒ¹æ•µã™ã‚‹äºˆæ¸¬ç²¾åº¦ã‚’å®Ÿç¾ã§ãã‚‹å¯èƒ½æ€§ã§ã™ã€‚Matbench v0.1ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã¯ã€çµ„æˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆElemNetç­‰ï¼‰ãŒå½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼äºˆæ¸¬ã§RÂ² 0.92ã‚’é”æˆã—ã€çµæ™¶æ§‹é€ ã‚’å¿…è¦ã¨ã™ã‚‹GNNãƒ¢ãƒ‡ãƒ«ï¼ˆRÂ² 0.95-0.97ï¼‰ã¨ã®å·®ã¯ã‚ãšã‹3-5%ã«ç•™ã¾ã‚Šã¾ã™ã€‚
            </p>
            <p>
                æœ¬ç« ã§ã¯ã€scikit-learnãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ã€è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒï¼ˆRandomForestã€GradientBoostingã€MLPï¼‰ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã€SHAPè§£é‡ˆã‚’é€šã˜ã¦ã€å®Ÿå‹™ã§å³åº§ã«æ´»ç”¨ã§ãã‚‹æ©Ÿæ¢°å­¦ç¿’ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ç¿’å¾—ã—ã¾ã™ã€‚
            </p>
        </section>

        <!-- Section 4.1: scikit-learn Pipeline Integration -->
        <section id="sklearn-pipeline">
            <h2>4.1 scikit-learnãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ</h2>

            <h3>4.1.1 ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åŸºæœ¬æ¦‚å¿µ</h3>
            <p>
                scikit-learnã®<code>Pipeline</code>ã¯ã€ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ»ç‰¹å¾´é‡ç”Ÿæˆãƒ»ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’ä¸€ã¤ã®å‡¦ç†ãƒ•ãƒ­ãƒ¼ã¨ã—ã¦çµ±åˆã™ã‚‹å¼·åŠ›ãªãƒ„ãƒ¼ãƒ«ã§ã™ã€‚matminerã®Featurizerã‚‚ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«çµ±åˆå¯èƒ½ã§ã€ä»¥ä¸‹ã®ã‚ˆã†ãªåˆ©ç‚¹ãŒã‚ã‚Šã¾ã™ï¼š
            </p>
            <ul>
                <li><strong>å†ç¾æ€§ã®ç¢ºä¿</strong>: å…¨å‡¦ç†ã‚’ä¸€ã¤ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ç®¡ç†ã—ã€åŒã˜å‰å‡¦ç†ã‚’è¨“ç·´/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«é©ç”¨</li>
                <li><strong>ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®é˜²æ­¢</strong>: ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ™‚ã«æ­£ã—ããƒ‡ãƒ¼ã‚¿åˆ†å‰²</li>
                <li><strong>ãƒ‡ãƒ—ãƒ­ã‚¤ã®å®¹æ˜“æ€§</strong>: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’joblibã§ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿å¯èƒ½</li>
            </ul>

            <div class="mermaid">
                graph LR
                A[åŒ–å­¦çµ„æˆ<br/>Fe2O3] --> B[Featurizer<br/>MagpieData]
                B --> C[ç‰¹å¾´é‡<br/>145æ¬¡å…ƒ]
                C --> D[StandardScaler<br/>æ¨™æº–åŒ–]
                D --> E[ML Model<br/>RandomForest]
                E --> F[äºˆæ¸¬å€¤<br/>å½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼]
                style A fill:#e3f2fd
                style C fill:#fff3e0
                style F fill:#f1f8e9
            </div>

            <h3>4.1.2 åŸºæœ¬çš„ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰</h3>
            <p>
                matminerã®Featurizerã‚’scikit-learnãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«çµ±åˆã™ã‚‹åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¤ºã—ã¾ã™ã€‚
            </p>

            <div class="colab-badge">
                <a href="https://colab.research.google.com/github/hackingmaterials/matminer_examples/blob/main/matminer_examples/machine_learning-nb/sklearn_pipeline_example.ipynb" target="_blank">
                    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
                </a>
            </div>

            <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹1: scikit-learn Pipeline with Magpie Featurizer

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from matminer.featurizers.composition import ElementProperty
from matminer.featurizers.conversions import StrToComposition
import joblib

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆå½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼äºˆæ¸¬ï¼‰
data = pd.DataFrame({
    'composition': ['Fe2O3', 'Al2O3', 'TiO2', 'SiO2', 'MgO',
                    'CaO', 'Na2O', 'K2O', 'ZnO', 'CuO'],
    'formation_energy': [-8.3, -16.6, -9.7, -9.1, -6.1,
                         -6.3, -4.2, -3.6, -3.6, -1.6]  # eV/atom
})

# 1. åŒ–å­¦çµ„æˆã‚’æ–‡å­—åˆ—ã‹ã‚‰Compositionã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¸å¤‰æ›
str_to_comp = StrToComposition(target_col_id='composition_obj')
data = str_to_comp.featurize_dataframe(data, 'composition')

# 2. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰
# Pipelineæ§‹æ–‡: [('åå‰', ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ), ...]ã®ã‚¿ãƒ—ãƒ«ãƒªã‚¹ãƒˆ
pipeline = Pipeline([
    ('scaler', StandardScaler()),           # ç‰¹å¾´é‡ã®æ¨™æº–åŒ–
    ('model', RandomForestRegressor(       # ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°
        n_estimators=100,
        max_depth=10,
        random_state=42
    ))
])

# 3. ç‰¹å¾´é‡ç”Ÿæˆï¼ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¤–ã§å®Ÿè¡Œï¼‰
featurizer = ElementProperty.from_preset('magpie')
data = featurizer.featurize_dataframe(data, 'composition_obj')

# 4. ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
feature_cols = featurizer.feature_labels()
X = data[feature_cols]
y = data['formation_energy']

# 5. è¨“ç·´/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 6. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è¨“ç·´
pipeline.fit(X_train, y_train)

# 7. æ€§èƒ½è©•ä¾¡
train_score = pipeline.score(X_train, y_train)
test_score = pipeline.score(X_test, y_test)

print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿RÂ²ã‚¹ã‚³ã‚¢: {train_score:.4f}")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿RÂ²ã‚¹ã‚³ã‚¢: {test_score:.4f}")

# 8. ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
cv_scores = cross_val_score(
    pipeline, X_train, y_train,
    cv=5, scoring='r2'
)
print(f"\n5-Fold CVå¹³å‡RÂ²: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

# 9. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä¿å­˜
joblib.dump(pipeline, 'formation_energy_model.pkl')
print("\nãƒ¢ãƒ‡ãƒ«ã‚’formation_energy_model.pklã«ä¿å­˜ã—ã¾ã—ãŸ")

# 10. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®èª­ã¿è¾¼ã¿ã¨ä½¿ç”¨
loaded_pipeline = joblib.load('formation_energy_model.pkl')
predictions = loaded_pipeline.predict(X_test)
print(f"\näºˆæ¸¬ä¾‹ï¼ˆæœ€åˆã®3ã‚µãƒ³ãƒ—ãƒ«ï¼‰:")
for i in range(min(3, len(predictions))):
    print(f"  å®Ÿæ¸¬å€¤: {y_test.iloc[i]:.2f} eV/atom, "
          f"äºˆæ¸¬å€¤: {predictions[i]:.2f} eV/atom")
</code></pre>

            <div class="info-box">
                <h4>ğŸ’¡ make_pipeline vs Pipeline</h4>
                <p>
                    <code>make_pipeline</code>ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã«è‡ªå‹•ã§åå‰ãŒä»˜ä¸ã•ã‚Œã€ã‚ˆã‚Šç°¡æ½”ã«è¨˜è¿°ã§ãã¾ã™ï¼š
                </p>
                <pre><code class="language-python">from sklearn.pipeline import make_pipeline

# è‡ªå‹•å‘½åç‰ˆï¼ˆåå‰ã¯ 'standardscaler', 'randomforestregressor' ç­‰ï¼‰
pipeline = make_pipeline(
    StandardScaler(),
    RandomForestRegressor(n_estimators=100, random_state=42)
)

# æ‰‹å‹•å‘½åç‰ˆï¼ˆä»»æ„ã®åå‰ã‚’æŒ‡å®šå¯èƒ½ï¼‰
pipeline = Pipeline([
    ('scaling', StandardScaler()),
    ('rf_model', RandomForestRegressor(n_estimators=100, random_state=42))
])
</code></pre>
                <p>
                    <strong>ä½¿ã„åˆ†ã‘åŸºæº–</strong>: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã§ç‰¹å®šã‚¹ãƒ†ãƒƒãƒ—ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹å ´åˆã¯<code>Pipeline</code>ï¼ˆåå‰ã‚’æ˜ç¤ºï¼‰ã€ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«ã¯<code>make_pipeline</code>ãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚
                </p>
            </div>

            <h3>4.1.3 ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¸ã®Featurizerçµ±åˆ</h3>
            <p>
                matminerã®Featurizerã‚’scikit-learnãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ç›´æ¥çµ±åˆã™ã‚‹ã«ã¯ã€<code>DFMLAdaptor</code>ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™ï¼ˆmatminer 0.7.4ä»¥é™ï¼‰ã€‚
            </p>

            <pre><code class="language-python"># Featurizerã‚’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«çµ±åˆã™ã‚‹æ–¹æ³•

from matminer.featurizers.base import MultipleFeaturizer
from matminer.featurizers.composition import (
    ElementProperty, Stoichiometry, ValenceOrbital
)
from matminer.utils.pipeline import DFMLAdaptor

# è¤‡æ•°ã®Featurizerã‚’çµ±åˆ
multi_featurizer = MultipleFeaturizer([
    ElementProperty.from_preset('magpie'),
    Stoichiometry(),
    ValenceOrbital()
])

# DFMLAdaptorã§scikit-learnäº’æ›ã«ã™ã‚‹
featurizer_step = DFMLAdaptor(
    multi_featurizer,
    input_cols=['composition_obj'],
    ignore_cols=['composition', 'formation_energy']  # ç‰¹å¾´é‡ç”Ÿæˆå¯¾è±¡å¤–ã®åˆ—
)

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰
full_pipeline = Pipeline([
    ('featurize', featurizer_step),
    ('scale', StandardScaler()),
    ('model', RandomForestRegressor(n_estimators=100, random_state=42))
])

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å…¨ä½“ã‚’å…¥åŠ›ã¨ã—ã¦è¨“ç·´å¯èƒ½
full_pipeline.fit(data, data['formation_energy'])
</code></pre>

            <div class="warning-box">
                <h4>âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã«æ³¨æ„</h4>
                <p>
                    ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ™‚ã€FeaturizerãŒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®çµ±è¨ˆé‡ï¼ˆä¾‹: å¹³å‡å€¤ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãŒç™ºç”Ÿã—ã¾ã™ã€‚ã“ã‚Œã‚’é˜²ãã«ã¯ï¼š
                </p>
                <ul>
                    <li><strong>æ–¹æ³•1</strong>: Featurizeré©ç”¨å¾Œã«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ï¼ˆæ¨å¥¨ï¼‰</li>
                    <li><strong>æ–¹æ³•2</strong>: ã‚«ã‚¹ã‚¿ãƒ Transformerã§fitæ™‚ã®ã¿çµ±è¨ˆé‡è¨ˆç®—</li>
                </ul>
                <p>
                    æœ¬ã‚·ãƒªãƒ¼ã‚ºã§ã¯ç°¡æ½”æ€§ã®ãŸã‚ã€æ–¹æ³•1ï¼ˆFeaturizeré©ç”¨å¾Œã«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ï¼‰ã‚’æ¡ç”¨ã—ã¾ã™ã€‚
                </p>
            </div>
        </section>

        <!-- Section 4.2: Model Selection and Hyperparameter Optimization -->
        <section id="model-selection">
            <h2>4.2 ãƒ¢ãƒ‡ãƒ«é¸æŠã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</h2>

            <h3>4.2.1 ä¸»è¦ãªæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«</h3>
            <p>
                çµ„æˆãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ã‚’ç”¨ã„ãŸææ–™ç‰¹æ€§äºˆæ¸¬ã§ã¯ã€ä»¥ä¸‹ã®3ã¤ã®ãƒ¢ãƒ‡ãƒ«ãŒåºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ï¼š
            </p>

            <table class="hyperparameter-table">
                <thead>
                    <tr>
                        <th>ãƒ¢ãƒ‡ãƒ«</th>
                        <th>ç‰¹å¾´</th>
                        <th>ä¸»è¦ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</th>
                        <th>æ¨å¥¨ç¯„å›²</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Random Forest</strong></td>
                        <td>æ±ºå®šæœ¨ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã€éå­¦ç¿’ã«å¼·ã„ã€ç‰¹å¾´é‡é‡è¦åº¦å–å¾—å¯èƒ½</td>
                        <td>
                            <code>n_estimators</code><br>
                            <code>max_depth</code><br>
                            <code>min_samples_split</code>
                        </td>
                        <td>
                            100-500<br>
                            10-50<br>
                            2-10
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Gradient Boosting</strong><br>(XGBoost/LightGBM)</td>
                        <td>é€æ¬¡çš„ã«èª¤å·®ã‚’ä¿®æ­£ã€é«˜ç²¾åº¦ã€éå­¦ç¿’ãƒªã‚¹ã‚¯ã‚ã‚Š</td>
                        <td>
                            <code>learning_rate</code><br>
                            <code>n_estimators</code><br>
                            <code>max_depth</code>
                        </td>
                        <td>
                            0.01-0.3<br>
                            100-1000<br>
                            3-10
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Neural Network</strong><br>(MLP)</td>
                        <td>éç·šå½¢é–¢ä¿‚ã®å­¦ç¿’ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§æœ‰åŠ¹ã€å­¦ç¿’æ™‚é–“é•·ã„</td>
                        <td>
                            <code>hidden_layer_sizes</code><br>
                            <code>activation</code><br>
                            <code>alpha</code>
                        </td>
                        <td>
                            (100,), (100, 50)<br>
                            'relu', 'tanh'<br>
                            0.0001-0.01
                        </td>
                    </tr>
                </tbody>
            </table>

            <h3>4.2.2 Random Forestå›å¸°</h3>
            <p>
                Random Forestã¯ã€è¤‡æ•°ã®æ±ºå®šæœ¨ã®äºˆæ¸¬ã‚’å¹³å‡åŒ–ã™ã‚‹ã“ã¨ã§ã€é«˜ã„äºˆæ¸¬ç²¾åº¦ã¨éå­¦ç¿’è€æ€§ã‚’å®Ÿç¾ã—ã¾ã™ã€‚ææ–™ç§‘å­¦ã§ã¯ç‰¹å¾´é‡é‡è¦åº¦åˆ†æã«ã‚‚é »ç¹ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
            </p>

            <div class="colab-badge">
                <a href="https://colab.research.google.com/github/hackingmaterials/matminer_examples/blob/main/matminer_examples/machine_learning-nb/random_forest_example.ipynb" target="_blank">
                    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
                </a>
            </div>

            <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹2: Random Forestå›å¸°ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–å«ã‚€ï¼‰

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

# å‰ç« ã§ç”Ÿæˆã—ãŸç‰¹å¾´é‡ã‚’ä½¿ç”¨ï¼ˆX_train, X_test, y_train, y_testï¼‰

# 1. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
rf_baseline = RandomForestRegressor(random_state=42)
rf_baseline.fit(X_train, y_train)
baseline_score = rf_baseline.score(X_test, y_test)
print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ« RÂ² ã‚¹ã‚³ã‚¢: {baseline_score:.4f}")

# 2. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf_model = RandomForestRegressor(random_state=42)
grid_search = GridSearchCV(
    rf_model,
    param_grid,
    cv=5,                    # 5-fold ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    scoring='r2',            # RÂ²ã‚¹ã‚³ã‚¢ã§è©•ä¾¡
    n_jobs=-1,               # å…¨CPUã‚³ã‚¢ã‚’ä½¿ç”¨
    verbose=1
)

# GridSearchå®Ÿè¡Œ
print("\nGridSearchCVå®Ÿè¡Œä¸­...")
grid_search.fit(X_train, y_train)

# æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
print(f"\næœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {grid_search.best_params_}")
print(f"CVæœ€è‰¯ã‚¹ã‚³ã‚¢: {grid_search.best_score_:.4f}")

# 3. æœ€é©ãƒ¢ãƒ‡ãƒ«ã§ã®è©•ä¾¡
best_rf = grid_search.best_estimator_
y_pred_train = best_rf.predict(X_train)
y_pred_test = best_rf.predict(X_test)

# è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
def evaluate_model(y_true, y_pred, dataset_name):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)

    print(f"\n{dataset_name} è©•ä¾¡çµæœ:")
    print(f"  MAE:  {mae:.4f}")
    print(f"  RMSE: {rmse:.4f}")
    print(f"  RÂ²:   {r2:.4f}")

    return mae, rmse, r2

train_metrics = evaluate_model(y_train, y_pred_train, "è¨“ç·´ãƒ‡ãƒ¼ã‚¿")
test_metrics = evaluate_model(y_test, y_pred_test, "ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿")

# 4. ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–
feature_importance = best_rf.feature_importances_
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

print("\nä¸Šä½10ç‰¹å¾´é‡:")
print(importance_df.head(10).to_string(index=False))

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.barh(importance_df.head(10)['feature'],
         importance_df.head(10)['importance'])
plt.xlabel('é‡è¦åº¦')
plt.title('Random Forest ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆä¸Šä½10ï¼‰')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.savefig('rf_feature_importance.png', dpi=150)
print("\nç‰¹å¾´é‡é‡è¦åº¦ã‚°ãƒ©ãƒ•ã‚’ rf_feature_importance.png ã«ä¿å­˜ã—ã¾ã—ãŸ")

# 5. äºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤ãƒ—ãƒ­ãƒƒãƒˆ
plt.figure(figsize=(8, 8))
plt.scatter(y_test, y_pred_test, alpha=0.6, edgecolors='k', linewidth=0.5)
plt.plot([y_test.min(), y_test.max()],
         [y_test.min(), y_test.max()],
         'r--', lw=2, label='ç†æƒ³çš„ãªäºˆæ¸¬')
plt.xlabel('å®Ÿæ¸¬å€¤ (eV/atom)')
plt.ylabel('äºˆæ¸¬å€¤ (eV/atom)')
plt.title(f'Random Forestäºˆæ¸¬æ€§èƒ½\nRÂ² = {test_metrics[2]:.4f}')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('rf_prediction_plot.png', dpi=150)
print("äºˆæ¸¬ãƒ—ãƒ­ãƒƒãƒˆã‚’ rf_prediction_plot.png ã«ä¿å­˜ã—ã¾ã—ãŸ")
</code></pre>

            <h3>4.2.3 Gradient Boosting (XGBoost)</h3>
            <p>
                Gradient Boostingã¯ã€å¼±å­¦ç¿’å™¨ï¼ˆæµ…ã„æ±ºå®šæœ¨ï¼‰ã‚’é€æ¬¡çš„ã«è¿½åŠ ã—ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã§å‰ã®ãƒ¢ãƒ‡ãƒ«ã®èª¤å·®ã‚’ä¿®æ­£ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚XGBoostã¯é«˜é€ŸåŒ–ã¨æ­£å‰‡åŒ–ã‚’å®Ÿè£…ã—ãŸå®Ÿè£…ã§ã€Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã§é »ç¹ã«å„ªå‹ã—ã¦ã„ã¾ã™ã€‚
            </p>

            <div class="colab-badge">
                <a href="https://colab.research.google.com/github/hackingmaterials/matminer_examples/blob/main/matminer_examples/machine_learning-nb/xgboost_example.ipynb" target="_blank">
                    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
                </a>
            </div>

            <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹3: Gradient Boostingå®Ÿè£…ï¼ˆXGBoostï¼‰

# XGBoostã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆGoogle Colabã§ã¯ä¸è¦ï¼‰
# !pip install xgboost

import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

# 1. XGBoostç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚ï¼‰
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# 2. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«
params_baseline = {
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse',
    'eta': 0.1,                     # å­¦ç¿’ç‡
    'max_depth': 6,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'seed': 42
}

# è¨“ç·´ï¼ˆearly stoppingä½¿ç”¨ï¼‰
evals = [(dtrain, 'train'), (dtest, 'test')]
bst_baseline = xgb.train(
    params_baseline,
    dtrain,
    num_boost_round=1000,
    evals=evals,
    early_stopping_rounds=50,
    verbose_eval=False
)

print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æœ€è‰¯ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: {bst_baseline.best_iteration}")
print(f"ãƒ†ã‚¹ãƒˆRMSE: {bst_baseline.best_score:.4f}")

# 3. RandomizedSearchCVã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
# XGBoostRegressorã‚’ä½¿ç”¨ï¼ˆscikit-learnäº’æ›ï¼‰
xgb_model = xgb.XGBRegressor(
    objective='reg:squarederror',
    eval_metric='rmse',
    random_state=42
)

param_distributions = {
    'n_estimators': randint(100, 1000),
    'learning_rate': uniform(0.01, 0.3),
    'max_depth': randint(3, 10),
    'min_child_weight': randint(1, 10),
    'subsample': uniform(0.6, 0.4),          # 0.6-1.0
    'colsample_bytree': uniform(0.6, 0.4),   # 0.6-1.0
    'gamma': uniform(0, 0.5)
}

random_search = RandomizedSearchCV(
    xgb_model,
    param_distributions,
    n_iter=50,                # 50å›ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    cv=5,
    scoring='r2',
    n_jobs=-1,
    verbose=1,
    random_state=42
)

print("\nRandomizedSearchCVå®Ÿè¡Œä¸­...")
random_search.fit(X_train, y_train)

print(f"\næœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {random_search.best_params_}")
print(f"CVæœ€è‰¯RÂ²ã‚¹ã‚³ã‚¢: {random_search.best_score_:.4f}")

# 4. æœ€é©ãƒ¢ãƒ‡ãƒ«ã§ã®è©•ä¾¡
best_xgb = random_search.best_estimator_
y_pred_test_xgb = best_xgb.predict(X_test)
xgb_metrics = evaluate_model(y_test, y_pred_test_xgb, "XGBoost ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿")

# 5. ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆGainåŸºæº–ï¼‰
xgb_importance = best_xgb.get_booster().get_score(importance_type='gain')
importance_df_xgb = pd.DataFrame({
    'feature': list(xgb_importance.keys()),
    'importance': list(xgb_importance.values())
}).sort_values('importance', ascending=False)

print("\nXGBoost ä¸Šä½10ç‰¹å¾´é‡ (Gain):")
print(importance_df_xgb.head(10).to_string(index=False))

# 6. å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–
evals_result = best_xgb.evals_result()
plt.figure(figsize=(10, 6))
plt.plot(evals_result['validation_0']['rmse'], label='è¨“ç·´RMSE')
plt.xlabel('ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³')
plt.ylabel('RMSE')
plt.title('XGBoostå­¦ç¿’æ›²ç·š')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('xgb_learning_curve.png', dpi=150)
print("\nå­¦ç¿’æ›²ç·šã‚’ xgb_learning_curve.png ã«ä¿å­˜ã—ã¾ã—ãŸ")
</code></pre>

            <div class="tip-box">
                <h4>ğŸ’¡ GridSearchCV vs RandomizedSearchCV</h4>
                <p>
                    <strong>GridSearchCV</strong>: å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›ã‚’ç¶²ç¾…çš„ã«æ¢ç´¢ï¼ˆè¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ï¼‰
                </p>
                <p>
                    <strong>RandomizedSearchCV</strong>: ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§åŠ¹ç‡çš„ã«æ¢ç´¢ï¼ˆæ¨å¥¨ï¼‰
                </p>
                <p>
                    <strong>ä½¿ã„åˆ†ã‘</strong>: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ãŒåºƒã„å ´åˆï¼ˆXGBoostç­‰ï¼‰ã¯RandomizedSearchCVã€ç‹­ã„å ´åˆï¼ˆRandom Forestç­‰ï¼‰ã¯GridSearchCVãŒé©åˆ‡ã§ã™ã€‚
                </p>
            </div>

            <h3>4.2.4 Neural Network (MLP Regressor)</h3>
            <p>
                å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ï¼ˆMLPï¼‰ã¯ã€è¤‡æ•°ã®éš ã‚Œå±¤ã‚’æŒã¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚éç·šå½¢é–¢ä¿‚ã®å­¦ç¿’ã«å„ªã‚Œã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§é«˜ã„æ€§èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚
            </p>

            <div class="colab-badge">
                <a href="https://colab.research.google.com/github/hackingmaterials/matminer_examples/blob/main/matminer_examples/machine_learning-nb/neural_network_example.ipynb" target="_blank">
                    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
                </a>
            </div>

            <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹4: Neural Networkï¼ˆMLP Regressorï¼‰

from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import validation_curve

# 1. ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ–ï¼ˆMLPã§ã¯å¿…é ˆï¼‰
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 2. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³MLP
mlp_baseline = MLPRegressor(
    hidden_layer_sizes=(100,),      # éš ã‚Œå±¤1å±¤ã€100ãƒ¦ãƒ‹ãƒƒãƒˆ
    activation='relu',
    solver='adam',
    alpha=0.0001,                   # L2æ­£å‰‡åŒ–ä¿‚æ•°
    batch_size='auto',
    learning_rate_init=0.001,
    max_iter=500,
    random_state=42,
    early_stopping=True,            # æ¤œè¨¼æå¤±ãŒæ”¹å–„ã—ãªã„å ´åˆåœæ­¢
    validation_fraction=0.1,
    verbose=False
)

mlp_baseline.fit(X_train_scaled, y_train)
print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³MLP è¨“ç·´å®Œäº†ï¼ˆ{mlp_baseline.n_iter_}ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰")
print(f"ãƒ†ã‚¹ãƒˆRÂ²ã‚¹ã‚³ã‚¢: {mlp_baseline.score(X_test_scaled, y_test):.4f}")

# 3. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ
param_grid_mlp = {
    'hidden_layer_sizes': [(50,), (100,), (100, 50), (150, 100, 50)],
    'activation': ['relu', 'tanh'],
    'alpha': [0.0001, 0.001, 0.01],
    'learning_rate_init': [0.001, 0.01]
}

mlp_model = MLPRegressor(
    solver='adam',
    max_iter=1000,
    early_stopping=True,
    random_state=42,
    verbose=False
)

grid_search_mlp = GridSearchCV(
    mlp_model,
    param_grid_mlp,
    cv=3,                           # MLPã¯æ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚3-fold
    scoring='r2',
    n_jobs=-1,
    verbose=1
)

print("\nMLP GridSearchCVå®Ÿè¡Œä¸­...")
grid_search_mlp.fit(X_train_scaled, y_train)

print(f"\næœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {grid_search_mlp.best_params_}")
print(f"CVæœ€è‰¯RÂ²ã‚¹ã‚³ã‚¢: {grid_search_mlp.best_score_:.4f}")

# 4. æœ€é©ãƒ¢ãƒ‡ãƒ«ã§ã®è©•ä¾¡
best_mlp = grid_search_mlp.best_estimator_
y_pred_test_mlp = best_mlp.predict(X_test_scaled)
mlp_metrics = evaluate_model(y_test, y_pred_test_mlp, "MLP ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿")

# 5. æå¤±æ›²ç·šã®å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.plot(best_mlp.loss_curve_, label='è¨“ç·´æå¤±')
if hasattr(best_mlp, 'validation_scores_'):
    plt.plot(best_mlp.validation_scores_, label='æ¤œè¨¼ã‚¹ã‚³ã‚¢')
plt.xlabel('ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³')
plt.ylabel('æå¤± / ã‚¹ã‚³ã‚¢')
plt.title('MLPå­¦ç¿’æ›²ç·š')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('mlp_loss_curve.png', dpi=150)
print("\næå¤±æ›²ç·šã‚’ mlp_loss_curve.png ã«ä¿å­˜ã—ã¾ã—ãŸ")

# 6. éš ã‚Œå±¤ã‚µã‚¤ã‚ºã®å½±éŸ¿ã‚’èª¿æŸ»ï¼ˆValidation Curveï¼‰
train_scores, valid_scores = validation_curve(
    MLPRegressor(activation='relu', alpha=0.001, random_state=42, max_iter=500),
    X_train_scaled, y_train,
    param_name='hidden_layer_sizes',
    param_range=[(50,), (100,), (150,), (100, 50), (150, 100)],
    cv=3,
    scoring='r2',
    n_jobs=-1
)

plt.figure(figsize=(10, 6))
plt.plot(['(50,)', '(100,)', '(150,)', '(100,50)', '(150,100)'],
         train_scores.mean(axis=1), 'o-', label='è¨“ç·´ã‚¹ã‚³ã‚¢')
plt.plot(['(50,)', '(100,)', '(150,)', '(100,50)', '(150,100)'],
         valid_scores.mean(axis=1), 's-', label='æ¤œè¨¼ã‚¹ã‚³ã‚¢')
plt.xlabel('éš ã‚Œå±¤ã‚µã‚¤ã‚º')
plt.ylabel('RÂ²ã‚¹ã‚³ã‚¢')
plt.title('MLPã®éš ã‚Œå±¤ã‚µã‚¤ã‚ºã¨ãƒ¢ãƒ‡ãƒ«æ€§èƒ½')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('mlp_validation_curve.png', dpi=150)
print("Validation Curveã‚’ mlp_validation_curve.png ã«ä¿å­˜ã—ã¾ã—ãŸ")
</code></pre>

            <h3>4.2.5 å­¦ç¿’æ›²ç·šåˆ†æ</h3>
            <p>
                å­¦ç¿’æ›²ç·šï¼ˆLearning Curveï¼‰ã¯ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿é‡ã¨äºˆæ¸¬æ€§èƒ½ã®é–¢ä¿‚ã‚’å¯è¦–åŒ–ã—ã€éå­¦ç¿’ãƒ»æœªå­¦ç¿’ã®è¨ºæ–­ã«ä½¿ç”¨ã—ã¾ã™ã€‚
            </p>

            <div class="mermaid">
                graph TD
                A[å­¦ç¿’æ›²ç·šåˆ†æ] --> B{è¨“ç·´ã‚¹ã‚³ã‚¢ vs æ¤œè¨¼ã‚¹ã‚³ã‚¢}
                B -->|è¨“ç·´â†‘æ¤œè¨¼â†‘| C[è‰¯å¥½ãªæ±åŒ–æ€§èƒ½]
                B -->|è¨“ç·´â†‘æ¤œè¨¼â†“| D[éå­¦ç¿’<br/>ãƒ¢ãƒ‡ãƒ«ç°¡ç´ åŒ–]
                B -->|è¨“ç·´â†“æ¤œè¨¼â†“| E[æœªå­¦ç¿’<br/>ãƒ¢ãƒ‡ãƒ«è¤‡é›‘åŒ–]
                B -->|ä¸¡æ–¹åæŸ| F[ãƒ‡ãƒ¼ã‚¿è¿½åŠ ã§æ”¹å–„æœŸå¾…]
                style C fill:#c8e6c9
                style D fill:#ffccbc
                style E fill:#ffe0b2
                style F fill:#b3e5fc
            </mermaid>

            <div class="colab-badge">
                <a href="https://colab.research.google.com/github/hackingmaterials/matminer_examples/blob/main/matminer_examples/machine_learning-nb/learning_curve_example.ipynb" target="_blank">
                    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
                </a>
            </div>

            <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹5: å­¦ç¿’æ›²ç·šåˆ†æï¼ˆlearning_curveé–¢æ•°ï¼‰

from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
import numpy as np

def plot_learning_curve(estimator, X, y, title, cv=5, n_jobs=-1,
                        train_sizes=np.linspace(0.1, 1.0, 10)):
    """
    å­¦ç¿’æ›²ç·šã‚’æç”»ã™ã‚‹é–¢æ•°

    Parameters:
    -----------
    estimator : scikit-learn estimator
        è©•ä¾¡å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«
    X : array-like, shape (n_samples, n_features)
        ç‰¹å¾´é‡
    y : array-like, shape (n_samples,)
        ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
    title : str
        ã‚°ãƒ©ãƒ•ã‚¿ã‚¤ãƒˆãƒ«
    cv : int
        ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®foldæ•°
    train_sizes : array-like
        è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”ç‡
    """
    plt.figure(figsize=(10, 6))

    # learning_curveé–¢æ•°ã§è¨“ç·´/æ¤œè¨¼ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    train_sizes_abs, train_scores, valid_scores = learning_curve(
        estimator, X, y,
        train_sizes=train_sizes,
        cv=cv,
        scoring='r2',
        n_jobs=n_jobs,
        verbose=0
    )

    # å¹³å‡ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—
    train_scores_mean = train_scores.mean(axis=1)
    train_scores_std = train_scores.std(axis=1)
    valid_scores_mean = valid_scores.mean(axis=1)
    valid_scores_std = valid_scores.std(axis=1)

    # ãƒ—ãƒ­ãƒƒãƒˆ
    plt.fill_between(train_sizes_abs,
                     train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std,
                     alpha=0.1, color='blue')
    plt.fill_between(train_sizes_abs,
                     valid_scores_mean - valid_scores_std,
                     valid_scores_mean + valid_scores_std,
                     alpha=0.1, color='red')

    plt.plot(train_sizes_abs, train_scores_mean, 'o-', color='blue',
             label='è¨“ç·´ã‚¹ã‚³ã‚¢')
    plt.plot(train_sizes_abs, valid_scores_mean, 's-', color='red',
             label='ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢')

    plt.xlabel('è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°')
    plt.ylabel('RÂ²ã‚¹ã‚³ã‚¢')
    plt.title(title)
    plt.legend(loc='best')
    plt.grid(alpha=0.3)
    plt.tight_layout()

    return plt

# è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’æ›²ç·šã‚’æ¯”è¼ƒ
models = {
    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=20,
                                           random_state=42),
    'XGBoost': xgb.XGBRegressor(n_estimators=200, max_depth=6,
                                learning_rate=0.1, random_state=42),
    'MLP': MLPRegressor(hidden_layer_sizes=(100, 50), alpha=0.001,
                        max_iter=500, random_state=42)
}

for model_name, model in models.items():
    # MLPã®å ´åˆã¯æ¨™æº–åŒ–ãŒå¿…è¦
    if model_name == 'MLP':
        X_current = X_train_scaled
        y_current = y_train
    else:
        X_current = X_train
        y_current = y_train

    plot_learning_curve(
        model, X_current, y_current,
        title=f'{model_name} å­¦ç¿’æ›²ç·š',
        cv=5
    )
    plt.savefig(f'learning_curve_{model_name.replace(" ", "_").lower()}.png',
                dpi=150)
    print(f"{model_name} ã®å­¦ç¿’æ›²ç·šã‚’ä¿å­˜ã—ã¾ã—ãŸ")

# è¨ºæ–­ã‚¬ã‚¤ãƒ‰
print("\nã€å­¦ç¿’æ›²ç·šã®è¨ºæ–­ã‚¬ã‚¤ãƒ‰ã€‘")
print("1. è¨“ç·´ã‚¹ã‚³ã‚¢é«˜ãƒ»æ¤œè¨¼ã‚¹ã‚³ã‚¢ä½ â†’ éå­¦ç¿’ï¼ˆãƒ¢ãƒ‡ãƒ«ç°¡ç´ åŒ–ã€æ­£å‰‡åŒ–å¼·åŒ–ï¼‰")
print("2. è¨“ç·´ã‚¹ã‚³ã‚¢ä½ãƒ»æ¤œè¨¼ã‚¹ã‚³ã‚¢ä½ â†’ æœªå­¦ç¿’ï¼ˆãƒ¢ãƒ‡ãƒ«è¤‡é›‘åŒ–ã€ç‰¹å¾´é‡è¿½åŠ ï¼‰")
print("3. ä¸¡ã‚¹ã‚³ã‚¢é«˜ä½ã§åæŸ â†’ è‰¯å¥½ï¼ˆã“ã‚Œä»¥ä¸Šãƒ‡ãƒ¼ã‚¿è¿½åŠ ã—ã¦ã‚‚æ”¹å–„å°ï¼‰")
print("4. ä¸¡ã‚¹ã‚³ã‚¢ä¸Šæ˜‡å‚¾å‘ â†’ ãƒ‡ãƒ¼ã‚¿è¿½åŠ ã§æ”¹å–„æœŸå¾…")
</code></pre>
        </section>

        <!-- Section 4.3: Composition vs GNN Performance -->
        <section id="composition-vs-gnn">
            <h2>4.3 çµ„æˆãƒ™ãƒ¼ã‚¹ vs GNNæ€§èƒ½æ¯”è¼ƒ</h2>

            <h3>4.3.1 Matbench v0.1ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯</h3>
            <p>
                <a href="https://matbench.materialsproject.org/" target="_blank">Matbench v0.1</a>ã¯ã€ææ–™ç‰¹æ€§äºˆæ¸¬ã®æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã€13å€‹ã®ã‚¿ã‚¹ã‚¯ï¼ˆå½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼ã€ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã€å¼¾æ€§ç‡ç­‰ï¼‰ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚å„ã‚¿ã‚¹ã‚¯ã¯æ•°åƒï½æ•°ä¸‡ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’å«ã¿ã€å…¬å¹³ãªæ€§èƒ½æ¯”è¼ƒãŒå¯èƒ½ã§ã™ã€‚
            </p>

            <div class="info-box">
                <h4>ğŸ“Š Matbench v0.1ã®ç‰¹å¾´</h4>
                <ul>
                    <li><strong>13ã‚¿ã‚¹ã‚¯</strong>: å½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼ã€ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã€èª˜é›»ç‡ã€ä½“ç©å¼¾æ€§ç‡ã€å‰ªæ–­å¼¾æ€§ç‡ç­‰</li>
                    <li><strong>çµ±ä¸€ãƒ‡ãƒ¼ã‚¿åˆ†å‰²</strong>: 5-fold CVã€äº‹å‰å®šç¾©ã•ã‚ŒãŸè¨“ç·´/ãƒ†ã‚¹ãƒˆåˆ†å‰²</li>
                    <li><strong>å¤šæ§˜ãªå…¥åŠ›</strong>: çµ„æˆã®ã¿ï¼ˆ9ã‚¿ã‚¹ã‚¯ï¼‰ã€æ§‹é€ å¿…é ˆï¼ˆ4ã‚¿ã‚¹ã‚¯ï¼‰</li>
                    <li><strong>å…¬é–‹ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰</strong>: æœ€æ–°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ€§èƒ½ã‚’æ¯”è¼ƒå¯èƒ½</li>
                </ul>
            </div>

            <h3>4.3.2 çµ„æˆãƒ™ãƒ¼ã‚¹ vs GNNæ€§èƒ½è©³ç´°</h3>
            <p>
                ä»¥ä¸‹ã®è¡¨ã¯ã€Matbenchãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã®çµ„æˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆMagpieã€ElemNetï¼‰ã¨GNNãƒ¢ãƒ‡ãƒ«ï¼ˆCGCNNã€MEGNetã€ALIGNNï¼‰ã®æ€§èƒ½æ¯”è¼ƒã§ã™ã€‚
            </p>

            <table class="benchmark-table">
                <thead>
                    <tr>
                        <th>ã‚¿ã‚¹ã‚¯</th>
                        <th>ã‚µãƒ³ãƒ—ãƒ«æ•°</th>
                        <th>è©•ä¾¡æŒ‡æ¨™</th>
                        <th>Magpie + RF</th>
                        <th>ElemNet</th>
                        <th>CGCNN</th>
                        <th>MEGNet</th>
                        <th>ALIGNN</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>å½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼</strong><br>(matbench_mp_e_form)</td>
                        <td>132,752</td>
                        <td>MAE (eV/atom)</td>
                        <td>0.082</td>
                        <td>0.065</td>
                        <td class="best-score">0.052</td>
                        <td>0.059</td>
                        <td class="best-score">0.047</td>
                    </tr>
                    <tr>
                        <td><strong>ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—</strong><br>(matbench_mp_gap)</td>
                        <td>106,113</td>
                        <td>MAE (eV)</td>
                        <td>0.42</td>
                        <td>0.35</td>
                        <td class="best-score">0.27</td>
                        <td>0.30</td>
                        <td class="best-score">0.25</td>
                    </tr>
                    <tr>
                        <td><strong>ä½“ç©å¼¾æ€§ç‡</strong><br>(matbench_mp_bulk_modulus)</td>
                        <td>10,987</td>
                        <td>MAE (log10(GPa))</td>
                        <td>0.082</td>
                        <td>0.075</td>
                        <td class="best-score">0.063</td>
                        <td>0.068</td>
                        <td class="best-score">0.059</td>
                    </tr>
                    <tr>
                        <td><strong>å‰ªæ–­å¼¾æ€§ç‡</strong><br>(matbench_mp_shear_modulus)</td>
                        <td>10,987</td>
                        <td>MAE (log10(GPa))</td>
                        <td>0.092</td>
                        <td>0.084</td>
                        <td class="best-score">0.071</td>
                        <td>0.076</td>
                        <td class="best-score">0.067</td>
                    </tr>
                    <tr>
                        <td><strong>èª˜é›»ç‡</strong><br>(matbench_dielectric)</td>
                        <td>4,764</td>
                        <td>MAE (log10)</td>
                        <td>0.29</td>
                        <td>0.26</td>
                        <td class="best-score">0.21</td>
                        <td>0.23</td>
                        <td class="best-score">0.19</td>
                    </tr>
                    <tr>
                        <td><strong>ãƒšãƒ­ãƒ–ã‚¹ã‚«ã‚¤ãƒˆå½¢æˆ</strong><br>(matbench_perovskites)</td>
                        <td>18,928</td>
                        <td>ROCAUC</td>
                        <td>0.91</td>
                        <td class="best-score">0.94</td>
                        <td>0.93</td>
                        <td>0.92</td>
                        <td class="best-score">0.95</td>
                    </tr>
                    <tr>
                        <td><strong>ãƒ•ã‚©ãƒãƒ³å‘¨æ³¢æ•°</strong><br>(matbench_phonons)</td>
                        <td>1,265</td>
                        <td>MAE (cmâ»Â¹)</td>
                        <td>89.5</td>
                        <td>â€”</td>
                        <td class="best-score">62.3</td>
                        <td>71.2</td>
                        <td class="best-score">58.7</td>
                    </tr>
                </tbody>
            </table>

            <div class="info-box">
                <h4>ğŸ” æ€§èƒ½å·®ã®è¦å› åˆ†æ</h4>
                <p>
                    <strong>çµ„æˆãƒ™ãƒ¼ã‚¹ãŒæœ‰åˆ©ãªå ´åˆ</strong>:
                </p>
                <ul>
                    <li><strong>ãƒšãƒ­ãƒ–ã‚¹ã‚«ã‚¤ãƒˆå½¢æˆ</strong>: åŒ–å­¦çµ„æˆã®è¦å‰‡æ€§ãŒæ”¯é…çš„ï¼ˆElemNet: ROCAUC 0.94ï¼‰</li>
                    <li><strong>å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</strong>: GNNã¯å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚’è¦æ±‚ã€çµ„æˆãƒ™ãƒ¼ã‚¹ã¯æ•°ç™¾ã‚µãƒ³ãƒ—ãƒ«ã§å­¦ç¿’å¯èƒ½</li>
                </ul>
                <p>
                    <strong>GNNãŒæœ‰åˆ©ãªå ´åˆ</strong>:
                </p>
                <ul>
                    <li><strong>å½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼</strong>: çµæ™¶æ§‹é€ ï¼ˆé…ä½æ•°ã€çµåˆè·é›¢ï¼‰ãŒé‡è¦ï¼ˆALIGNN: MAE 0.047 vs ElemNet: 0.065ï¼‰</li>
                    <li><strong>æ©Ÿæ¢°çš„æ€§è³ª</strong>: å¼¾æ€§ç‡ã¯çµæ™¶å¯¾ç§°æ€§ã«å¼·ãä¾å­˜</li>
                    <li><strong>ãƒ•ã‚©ãƒãƒ³å‘¨æ³¢æ•°</strong>: åŸå­é…ç½®ãŒç›´æ¥å½±éŸ¿</li>
                </ul>
            </div>

            <h3>4.3.3 ä½¿ã„åˆ†ã‘åŸºæº–</h3>
            <p>
                çµ„æˆãƒ™ãƒ¼ã‚¹ã¨GNNãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®é¸æŠåŸºæº–ã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ï¼š
            </p>

            <table>
                <thead>
                    <tr>
                        <th>åŸºæº–</th>
                        <th>çµ„æˆãƒ™ãƒ¼ã‚¹æ¨å¥¨</th>
                        <th>GNNæ¨å¥¨</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º</strong></td>
                        <td>å°è¦æ¨¡ï¼ˆ&lt;1,000ã‚µãƒ³ãƒ—ãƒ«ï¼‰</td>
                        <td>å¤§è¦æ¨¡ï¼ˆ&gt;10,000ã‚µãƒ³ãƒ—ãƒ«ï¼‰</td>
                    </tr>
                    <tr>
                        <td><strong>çµæ™¶æ§‹é€ </strong></td>
                        <td>æœªçŸ¥ãƒ»æœªæ±ºå®š</td>
                        <td>æ—¢çŸ¥ãƒ»é«˜ç²¾åº¦</td>
                    </tr>
                    <tr>
                        <td><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆ</strong></td>
                        <td>CPUã§ç§’å˜ä½</td>
                        <td>GPUã§åˆ†ï½æ™‚é–“å˜ä½</td>
                    </tr>
                    <tr>
                        <td><strong>è§£é‡ˆå¯èƒ½æ€§</strong></td>
                        <td>ç‰¹å¾´é‡é‡è¦åº¦ãŒç›´æ„Ÿçš„</td>
                        <td>ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹æ€§å¼·ã„</td>
                    </tr>
                    <tr>
                        <td><strong>äºˆæ¸¬ç²¾åº¦</strong></td>
                        <td>GNNã®85-95%</td>
                        <td>æœ€é«˜ç²¾åº¦</td>
                    </tr>
                    <tr>
                        <td><strong>æ¢ç´¢ãƒ•ã‚§ãƒ¼ã‚º</strong></td>
                        <td>åˆæœŸã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°</td>
                        <td>æœ€çµ‚å€™è£œã®ç²¾å¯†è©•ä¾¡</td>
                    </tr>
                </tbody>
            </table>

            <div class="tip-box">
                <h4>ğŸ’¡ å®Ÿå‹™ã§ã®æ¨å¥¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</h4>
                <ol>
                    <li><strong>Phase 1 - åˆæœŸã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°</strong>: çµ„æˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§10ä¸‡å€™è£œã‚’1,000å€™è£œã«çµã‚‹ï¼ˆCPUã§æ•°åˆ†ï¼‰</li>
                    <li><strong>Phase 2 - æ§‹é€ æœ€é©åŒ–</strong>: 1,000å€™è£œã®çµæ™¶æ§‹é€ ã‚’DFTç·©å’Œè¨ˆç®—ï¼ˆæ•°æ—¥ï¼‰</li>
                    <li><strong>Phase 3 - ç²¾å¯†äºˆæ¸¬</strong>: GNNã§100å€™è£œã‚’é¸å®šï¼ˆGPUã§æ•°æ™‚é–“ï¼‰</li>
                    <li><strong>Phase 4 - å®Ÿé¨“æ¤œè¨¼</strong>: æœ€çµ‚10å€™è£œã‚’åˆæˆãƒ»æ¸¬å®š</li>
                </ol>
                <p>
                    ã“ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚Šã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’æŠ‘ãˆã¤ã¤é«˜ç²¾åº¦ãªææ–™æ¢ç´¢ãŒå®Ÿç¾ã§ãã¾ã™ã€‚
                </p>
            </div>

            <h3>4.3.4 Matbenchãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¾‹</h3>
            <p>
                Matbenchãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿéš›ã«å®Ÿè¡Œã—ã€çµ„æˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
            </p>

            <div class="colab-badge">
                <a href="https://colab.research.google.com/github/hackingmaterials/matminer_examples/blob/main/matminer_examples/machine_learning-nb/matbench_example.ipynb" target="_blank">
                    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
                </a>
            </div>

            <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹6: Matbenchãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡

# Matbenchã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# !pip install matbench

from matbench.bench import MatbenchBenchmark
from matminer.featurizers.composition import ElementProperty
from matminer.featurizers.conversions import StrToComposition
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import pandas as pd

# 1. Matbenchãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®åˆæœŸåŒ–
mb = MatbenchBenchmark(
    autoload=False,
    subset=[
        'matbench_mp_e_form',        # å½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼
        'matbench_mp_gap',           # ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—
        'matbench_perovskites'       # ãƒšãƒ­ãƒ–ã‚¹ã‚«ã‚¤ãƒˆå½¢æˆ
    ]
)

# 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰
for task in mb.tasks:
    task.load()
    print(f"\n{task.dataset_name}:")
    print(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(task.df)}")
    print(f"  ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {task.metadata['target']}")
    print(f"  ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—: {task.metadata['task_type']}")

# 3. çµ„æˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨è©•ä¾¡
def evaluate_composition_model(task):
    """
    çµ„æˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆMagpie + RandomForestï¼‰ã§Matbenchã‚¿ã‚¹ã‚¯ã‚’è©•ä¾¡
    """
    # åŒ–å­¦çµ„æˆã®å¤‰æ›
    str_to_comp = StrToComposition(target_col_id='composition_obj')
    task.df = str_to_comp.featurize_dataframe(task.df, 'composition')

    # Magpieç‰¹å¾´é‡ã®ç”Ÿæˆ
    featurizer = ElementProperty.from_preset('magpie')
    task.df = featurizer.featurize_dataframe(task.df, 'composition_obj')

    feature_cols = featurizer.feature_labels()

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰
    if task.metadata['task_type'] == 'regression':
        model = RandomForestRegressor(
            n_estimators=300,
            max_depth=30,
            min_samples_split=5,
            random_state=42,
            n_jobs=-1
        )
    else:  # classification
        from sklearn.ensemble import RandomForestClassifier
        model = RandomForestClassifier(
            n_estimators=300,
            max_depth=30,
            random_state=42,
            n_jobs=-1
        )

    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('model', model)
    ])

    # Matbenchæ¨™æº–ã®5-fold CVã§è©•ä¾¡
    for fold_idx, fold in enumerate(task.folds):
        # è¨“ç·´/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å–å¾—
        train_inputs, train_outputs = task.get_train_and_val_data(fold)
        X_train = train_inputs[feature_cols]
        y_train = train_outputs

        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        pipeline.fit(X_train, y_train)

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
        test_inputs, test_outputs = task.get_test_data(fold,
                                                        include_target=True)
        X_test = test_inputs[feature_cols]
        predictions = pipeline.predict(X_test)

        # äºˆæ¸¬çµæœã‚’è¨˜éŒ²
        task.record(fold, predictions)

        print(f"  Fold {fold_idx + 1} å®Œäº†")

    # å…¨ä½“ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
    scores = task.scores
    print(f"\n{task.dataset_name} è©•ä¾¡çµæœ:")
    for metric, values in scores.items():
        print(f"  {metric}: {values['mean']:.4f} Â± {values['std']:.4f}")

    return scores

# 4. å…¨ã‚¿ã‚¹ã‚¯ã§è©•ä¾¡å®Ÿè¡Œ
results = {}
for task in mb.tasks:
    print(f"\n{'='*60}")
    print(f"è©•ä¾¡ä¸­: {task.dataset_name}")
    print('='*60)
    scores = evaluate_composition_model(task)
    results[task.dataset_name] = scores

# 5. çµæœã®ã‚µãƒãƒªãƒ¼è¡¨ç¤º
print("\n" + "="*60)
print("å…¨ã‚¿ã‚¹ã‚¯è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼")
print("="*60)
for dataset_name, scores in results.items():
    print(f"\n{dataset_name}:")
    for metric, values in scores.items():
        print(f"  {metric}: {values['mean']:.4f} Â± {values['std']:.4f}")

# 6. çµæœã‚’Matbenchå…¬å¼ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ä¿å­˜
mb.to_file('matbench_composition_results.json')
print("\nçµæœã‚’ matbench_composition_results.json ã«ä¿å­˜ã—ã¾ã—ãŸ")
print("å…¬å¼ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ã¸ã®æå‡ºãŒå¯èƒ½ã§ã™: https://matbench.materialsproject.org/")
</code></pre>
        </section>

        <!-- Section 4.4: Model Interpretability with SHAP -->
        <section id="model-interpretability">
            <h2>4.4 ãƒ¢ãƒ‡ãƒ«è§£é‡ˆå¯èƒ½æ€§ï¼ˆSHAPï¼‰</h2>

            <h3>4.4.1 SHAPã®åŸºç¤ç†è«–</h3>
            <p>
                SHAPï¼ˆSHapley Additive exPlanationsï¼‰ã¯ã€ã‚²ãƒ¼ãƒ ç†è«–ã®Shapleyå€¤ã‚’æ©Ÿæ¢°å­¦ç¿’ã«å¿œç”¨ã—ãŸè§£é‡ˆæ‰‹æ³•ã§ã™ã€‚å„ç‰¹å¾´é‡ãŒäºˆæ¸¬å€¤ã«ä¸ãˆã‚‹è²¢çŒ®åº¦ã‚’å®šé‡åŒ–ã—ã€ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«ã®æ„æ€æ±ºå®šã‚’èª¬æ˜å¯èƒ½ã«ã—ã¾ã™ã€‚
            </p>

            <div class="info-box">
                <h4>ğŸ“ SHAPå€¤ã®å®šç¾©</h4>
                <p>
                    ç‰¹å¾´é‡ $i$ ã®SHAPå€¤ $\phi_i$ ã¯ã€ä»¥ä¸‹ã®Shapleyå€¤ã¨ã—ã¦å®šç¾©ã•ã‚Œã¾ã™ï¼š
                </p>
                <p>
                    $$
                    \phi_i = \sum_{S \subseteq F \setminus \{i\}} \frac{|S|!(|F|-|S|-1)!}{|F|!} [f(S \cup \{i\}) - f(S)]
                    $$
                </p>
                <p>
                    ã“ã“ã§ã€$F$ ã¯å…¨ç‰¹å¾´é‡ã®é›†åˆã€$S$ ã¯ç‰¹å¾´é‡ã®éƒ¨åˆ†é›†åˆã€$f(S)$ ã¯ç‰¹å¾´é‡é›†åˆ $S$ ã‚’ä½¿ç”¨ã—ãŸäºˆæ¸¬å€¤ã§ã™ã€‚
                </p>
                <p>
                    <strong>ç›´æ„Ÿçš„è§£é‡ˆ</strong>: ç‰¹å¾´é‡ $i$ ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§äºˆæ¸¬å€¤ãŒã©ã‚Œã ã‘å¤‰åŒ–ã™ã‚‹ã‹ã‚’ã€å…¨ã¦ã®å¯èƒ½ãªç‰¹å¾´é‡çµ„ã¿åˆã‚ã›ã§å¹³å‡åŒ–ã—ãŸå€¤ã€‚
                </p>
            </div>

            <h3>4.4.2 ç‰¹å¾´é‡é‡è¦åº¦ vs SHAP</h3>
            <p>
                Random Forestã®ç‰¹å¾´é‡é‡è¦åº¦ã¨SHAPã®é•ã„ã‚’ç†è§£ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ï¼š
            </p>

            <table>
                <thead>
                    <tr>
                        <th>æ¯”è¼ƒé …ç›®</th>
                        <th>ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆFeature Importanceï¼‰</th>
                        <th>SHAPå€¤</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>å®šç¾©</strong></td>
                        <td>ä¸ç´”åº¦ï¼ˆGiniï¼‰æ¸›å°‘ã®å¹³å‡</td>
                        <td>äºˆæ¸¬å€¤ã¸ã®è²¢çŒ®åº¦ï¼ˆShapleyå€¤ï¼‰</td>
                    </tr>
                    <tr>
                        <td><strong>è§£é‡ˆ</strong></td>
                        <td>ã‚°ãƒ­ãƒ¼ãƒãƒ«ï¼ˆå…¨ä½“çš„ãªé‡è¦åº¦ï¼‰</td>
                        <td>ãƒ­ãƒ¼ã‚«ãƒ«ï¼ˆå€‹åˆ¥ã‚µãƒ³ãƒ—ãƒ«ã®è²¢çŒ®åº¦ï¼‰</td>
                    </tr>
                    <tr>
                        <td><strong>æ–¹å‘æ€§</strong></td>
                        <td>ãªã—ï¼ˆé‡è¦åº¦ã®ã¿ï¼‰</td>
                        <td>ã‚ã‚Šï¼ˆæ­£/è² ã®è²¢çŒ®ï¼‰</td>
                    </tr>
                    <tr>
                        <td><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆ</strong></td>
                        <td>ä½ï¼ˆè¨“ç·´æ™‚ã«è¨ˆç®—ï¼‰</td>
                        <td>é«˜ï¼ˆäºˆæ¸¬ã”ã¨ã«è¨ˆç®—ï¼‰</td>
                    </tr>
                    <tr>
                        <td><strong>ç†è«–ä¿è¨¼</strong></td>
                        <td>ãªã—ï¼ˆãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ï¼‰</td>
                        <td>ã‚ã‚Šï¼ˆå…¬ç†çš„åŸºç¤ï¼‰</td>
                    </tr>
                </tbody>
            </table>

            <h3>4.4.3 SHAPå®Ÿè£…ã¨å¯è¦–åŒ–</h3>
            <p>
                SHAPãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã€Random Forestãƒ¢ãƒ‡ãƒ«ã‚’SHAPè§£æã—ã¾ã™ã€‚
            </p>

            <div class="colab-badge">
                <a href="https://colab.research.google.com/github/hackingmaterials/matminer_examples/blob/main/matminer_examples/machine_learning-nb/shap_example.ipynb" target="_blank">
                    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
                </a>
            </div>

            <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹7: SHAPè§£é‡ˆï¼ˆsummary_plotã€dependence_plotï¼‰

# SHAPã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# !pip install shap

import shap
import matplotlib.pyplot as plt
import numpy as np

# è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆå‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§è¨“ç·´ã—ãŸbest_rfï¼‰
# X_test, y_test ã‚‚å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ç¶™ç¶š

# 1. SHAP Explainerã®ä½œæˆï¼ˆTree Explainer for Random Forestï¼‰
explainer = shap.TreeExplainer(best_rf)

# 2. SHAPå€¤ã®è¨ˆç®—ï¼ˆå…¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰
print("SHAPå€¤è¨ˆç®—ä¸­...")
shap_values = explainer.shap_values(X_test)
print(f"SHAPå€¤ã®shape: {shap_values.shape}")  # (n_samples, n_features)

# 3. Summary Plotï¼ˆå…¨ä½“çš„ãªç‰¹å¾´é‡é‡è¦åº¦ã¨åˆ†å¸ƒï¼‰
plt.figure(figsize=(10, 8))
shap.summary_plot(shap_values, X_test, feature_names=feature_cols,
                  show=False, max_display=20)
plt.tight_layout()
plt.savefig('shap_summary_plot.png', dpi=150, bbox_inches='tight')
print("\nSummary Plotã‚’ shap_summary_plot.png ã«ä¿å­˜ã—ã¾ã—ãŸ")
plt.close()

# 4. Summary Plotï¼ˆæ£’ã‚°ãƒ©ãƒ•ç‰ˆï¼šå¹³å‡çµ¶å¯¾SHAPå€¤ï¼‰
plt.figure(figsize=(10, 8))
shap.summary_plot(shap_values, X_test, feature_names=feature_cols,
                  plot_type='bar', show=False, max_display=20)
plt.tight_layout()
plt.savefig('shap_summary_bar.png', dpi=150, bbox_inches='tight')
print("Summary Plotï¼ˆæ£’ã‚°ãƒ©ãƒ•ï¼‰ã‚’ shap_summary_bar.png ã«ä¿å­˜ã—ã¾ã—ãŸ")
plt.close()

# 5. Dependence Plotï¼ˆç‰¹å®šç‰¹å¾´é‡ã®è©³ç´°åˆ†æï¼‰
# æœ€ã‚‚é‡è¦ãªç‰¹å¾´é‡ã‚’å–å¾—
shap_importance = np.abs(shap_values).mean(axis=0)
top_feature_idx = np.argmax(shap_importance)
top_feature_name = feature_cols[top_feature_idx]

plt.figure(figsize=(10, 6))
shap.dependence_plot(
    top_feature_idx,
    shap_values,
    X_test,
    feature_names=feature_cols,
    show=False
)
plt.tight_layout()
plt.savefig(f'shap_dependence_{top_feature_name}.png',
            dpi=150, bbox_inches='tight')
print(f"\nDependence Plotï¼ˆ{top_feature_name}ï¼‰ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
plt.close()

# 6. Force Plotï¼ˆå€‹åˆ¥ã‚µãƒ³ãƒ—ãƒ«ã®äºˆæ¸¬èª¬æ˜ï¼‰
# JavaScriptãƒ™ãƒ¼ã‚¹ã®ãŸã‚ã€Jupyterç’°å¢ƒæ¨å¥¨
shap.initjs()

# æœ€åˆã®ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ã®äºˆæ¸¬ã‚’èª¬æ˜
sample_idx = 0
shap_values_sample = shap_values[sample_idx, :]
base_value = explainer.expected_value
prediction = best_rf.predict(X_test.iloc[[sample_idx]])[0]

print(f"\nã‚µãƒ³ãƒ—ãƒ«#{sample_idx}ã®äºˆæ¸¬èª¬æ˜:")
print(f"  åŸºæº–å€¤ï¼ˆå¹³å‡äºˆæ¸¬ï¼‰: {base_value:.4f}")
print(f"  äºˆæ¸¬å€¤: {prediction:.4f}")
print(f"  å®Ÿæ¸¬å€¤: {y_test.iloc[sample_idx]:.4f}")

# Force Plotã®è¡¨ç¤ºï¼ˆJupyterç’°å¢ƒï¼‰
# shap.force_plot(base_value, shap_values_sample, X_test.iloc[sample_idx, :],
#                 feature_names=feature_cols)

# 7. ç‰¹å¾´é‡åˆ¥ã®å¹³å‡çµ¶å¯¾SHAPå€¤ãƒ©ãƒ³ã‚­ãƒ³ã‚°
shap_df = pd.DataFrame({
    'feature': feature_cols,
    'mean_abs_shap': np.abs(shap_values).mean(axis=0)
}).sort_values('mean_abs_shap', ascending=False)

print("\nä¸Šä½20ç‰¹å¾´é‡ï¼ˆå¹³å‡çµ¶å¯¾SHAPå€¤ï¼‰:")
print(shap_df.head(20).to_string(index=False))

# 8. éƒ¨åˆ†ä¾å­˜ãƒ—ãƒ­ãƒƒãƒˆï¼ˆPartial Dependence Plotï¼‰ã¨ã®æ¯”è¼ƒ
from sklearn.inspection import partial_dependence, PartialDependenceDisplay

# ä¸Šä½3ç‰¹å¾´é‡ã®éƒ¨åˆ†ä¾å­˜ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆ
top_3_features = shap_df.head(3)['feature'].tolist()
top_3_indices = [feature_cols.index(f) for f in top_3_features]

fig, axes = plt.subplots(1, 3, figsize=(15, 4))
for i, (feature_idx, feature_name) in enumerate(zip(top_3_indices, top_3_features)):
    pd_result = partial_dependence(
        best_rf, X_test, [feature_idx], grid_resolution=50
    )
    axes[i].plot(pd_result['grid_values'][0], pd_result['average'][0])
    axes[i].set_xlabel(feature_name)
    axes[i].set_ylabel('éƒ¨åˆ†ä¾å­˜')
    axes[i].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('partial_dependence_plots.png', dpi=150)
print("\néƒ¨åˆ†ä¾å­˜ãƒ—ãƒ­ãƒƒãƒˆã‚’ partial_dependence_plots.png ã«ä¿å­˜ã—ã¾ã—ãŸ")

# 9. SHAPå€¤ã®çµ±è¨ˆçš„ã‚µãƒãƒªãƒ¼
print("\nSHAPå€¤ã®çµ±è¨ˆ:")
print(f"  å…¨ç‰¹å¾´é‡ã®å¹³å‡çµ¶å¯¾SHAPå€¤: {np.abs(shap_values).mean():.4f}")
print(f"  æœ€å¤§SHAPå€¤: {shap_values.max():.4f}")
print(f"  æœ€å°SHAPå€¤: {shap_values.min():.4f}")

# æ­£ã®è²¢çŒ®ã¨è² ã®è²¢çŒ®ã‚’åˆ†é›¢
positive_shap = shap_values[shap_values > 0].sum()
negative_shap = shap_values[shap_values < 0].sum()
print(f"  æ­£ã®è²¢çŒ®åˆè¨ˆ: {positive_shap:.4f}")
print(f"  è² ã®è²¢çŒ®åˆè¨ˆ: {negative_shap:.4f}")
</code></pre>

            <h3>4.4.4 SHAPè§£æã®å®Ÿè·µçš„è§£é‡ˆ</h3>
            <p>
                SHAPã®å„å¯è¦–åŒ–æ‰‹æ³•ã®è§£é‡ˆæ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ï¼š
            </p>

            <div class="mermaid">
                graph TB
                A[SHAPå¯è¦–åŒ–æ‰‹æ³•] --> B[Summary Plot]
                A --> C[Dependence Plot]
                A --> D[Force Plot]
                A --> E[Waterfall Plot]

                B --> B1[å…¨ç‰¹å¾´é‡ã®é‡è¦åº¦<br/>è‰²=ç‰¹å¾´é‡å€¤]
                C --> C1[ç‰¹å¾´é‡å€¤ã¨SHAPå€¤ã®é–¢ä¿‚<br/>ç›¸äº’ä½œç”¨ã®å¯è¦–åŒ–]
                D --> D2[å€‹åˆ¥äºˆæ¸¬ã®è¦å› åˆ†è§£<br/>åŸºæº–å€¤ã‹ã‚‰ã®å¤‰åŒ–]
                E --> E1[äºˆæ¸¬ã®ç©ã¿ä¸Šã’ã‚°ãƒ©ãƒ•<br/>ç´¯ç©çš„ãªè²¢çŒ®]

                style B fill:#e3f2fd
                style C fill:#fff3e0
                style D fill:#f1f8e9
                style E fill:#fce4ec
            </mermaid>

            <div class="tip-box">
                <h4>ğŸ’¡ SHAPè§£æã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h4>
                <ol>
                    <li><strong>Summary Plot</strong>: å…¨ä½“å‚¾å‘ã®æŠŠæ¡ã«æœ€é©ã€‚èµ¤ï¼ˆé«˜å€¤ï¼‰ãŒäºˆæ¸¬ã‚’ä¸Šã’ã‚‹ã‹ä¸‹ã’ã‚‹ã‹ã‚’ç¢ºèª</li>
                    <li><strong>Dependence Plot</strong>: éç·šå½¢é–¢ä¿‚ã®ç™ºè¦‹ã€‚é–¾å€¤åŠ¹æœã‚„é£½å’ŒåŠ¹æœã‚’ç‰¹å®š</li>
                    <li><strong>Force Plot</strong>: å€‹åˆ¥äºˆæ¸¬ã®èª¬æ˜ã€‚ç•°å¸¸å€¤ã‚„äºˆæ¸¬å¤–ã‚Œã®åŸå› åˆ†æ</li>
                    <li><strong>éƒ¨åˆ†ä¾å­˜ãƒ—ãƒ­ãƒƒãƒˆã¨ã®ä½µç”¨</strong>: SHAPã¯å±€æ‰€çš„ã€PDPã¯å¤§åŸŸçš„ãªç‰¹å¾´é‡åŠ¹æœã‚’å¯è¦–åŒ–</li>
                </ol>
                <p>
                    <strong>ææ–™ç§‘å­¦ã§ã®å¿œç”¨ä¾‹</strong>: å½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼äºˆæ¸¬ã§ã€Œé›»æ°—é™°æ€§åº¦ã®å·®ã€ãŒé«˜SHAPå€¤ã‚’ç¤ºã™å ´åˆã€ã‚¤ã‚ªãƒ³çµåˆæ€§ãŒå®‰å®šæ€§ã«å¯„ä¸ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¾ã™ã€‚
                </p>
            </div>
        </section>

        <!-- Section: Exercises -->
        <section id="exercises">
            <h2>æ¼”ç¿’å•é¡Œ</h2>

            <!-- Easy Exercises -->
            <div class="exercise">
                <h4>æ¼”ç¿’1ï¼ˆEasyï¼‰: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰</h4>
                <p>
                    Magpieç‰¹å¾´é‡ã€StandardScalerã€RandomForestRegressorã‚’å«ã‚€scikit-learnãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ã€ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ï¼ˆ5-foldã€RÂ²ï¼‰ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚
                </p>
                <details>
                    <summary>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</summary>
                    <pre><code class="language-python">from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from matminer.featurizers.composition import ElementProperty

# ç‰¹å¾´é‡ç”Ÿæˆï¼ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¤–ï¼‰
featurizer = ElementProperty.from_preset('magpie')
data = featurizer.featurize_dataframe(data, 'composition_obj')

X = data[featurizer.feature_labels()]
y = data['target']

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', RandomForestRegressor(n_estimators=100, random_state=42))
])

# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='r2')
print(f"5-Fold CV RÂ²: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
</code></pre>
                </details>
            </div>

            <div class="exercise">
                <h4>æ¼”ç¿’2ï¼ˆEasyï¼‰: åŸºæœ¬ãƒ¢ãƒ‡ãƒ«è¨“ç·´</h4>
                <p>
                    XGBoostRegressorã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§è¨“ç·´ã—ã€è¨“ç·´/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®MAEã€RMSEã€RÂ²ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚
                </p>
                <details>
                    <summary>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</summary>
                    <pre><code class="language-python">import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
xgb_model = xgb.XGBRegressor(random_state=42)
xgb_model.fit(X_train, y_train)

# äºˆæ¸¬
y_pred_train = xgb_model.predict(X_train)
y_pred_test = xgb_model.predict(X_test)

# è©•ä¾¡æŒ‡æ¨™
print("è¨“ç·´ãƒ‡ãƒ¼ã‚¿:")
print(f"  MAE:  {mean_absolute_error(y_train, y_pred_train):.4f}")
print(f"  RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.4f}")
print(f"  RÂ²:   {r2_score(y_train, y_pred_train):.4f}")

print("\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿:")
print(f"  MAE:  {mean_absolute_error(y_test, y_pred_test):.4f}")
print(f"  RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.4f}")
print(f"  RÂ²:   {r2_score(y_test, y_pred_test):.4f}")
</code></pre>
                </details>
            </div>

            <div class="exercise">
                <h4>æ¼”ç¿’3ï¼ˆEasyï¼‰: ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè£…</h4>
                <p>
                    MLPRegressorã«å¯¾ã—ã¦3-fold ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã€å„foldã®RÂ²ã‚¹ã‚³ã‚¢ã¨å¹³å‡ã‚’è¡¨ç¤ºã—ã¦ãã ã•ã„ã€‚
                </p>
                <details>
                    <summary>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</summary>
                    <pre><code class="language-python">from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler

# ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# MLPãƒ¢ãƒ‡ãƒ«
mlp = MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=42)

# 3-fold CV
cv_results = cross_validate(
    mlp, X_scaled, y,
    cv=3,
    scoring='r2',
    return_train_score=True
)

print("å„foldã®ã‚¹ã‚³ã‚¢:")
for i, (train_score, test_score) in enumerate(zip(
    cv_results['train_score'], cv_results['test_score']
)):
    print(f"  Fold {i+1}: è¨“ç·´RÂ²={train_score:.4f}, ãƒ†ã‚¹ãƒˆRÂ²={test_score:.4f}")

print(f"\nå¹³å‡ãƒ†ã‚¹ãƒˆRÂ²: {cv_results['test_score'].mean():.4f} "
      f"Â± {cv_results['test_score'].std():.4f}")
</code></pre>
                </details>
            </div>

            <!-- Medium Exercises -->
            <div class="exercise">
                <h4>æ¼”ç¿’4ï¼ˆMediumï¼‰: GridSearchæœ€é©åŒ–</h4>
                <p>
                    Random Forestã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆn_estimatorsã€max_depthã€min_samples_splitï¼‰ã‚’GridSearchCVã§æœ€é©åŒ–ã—ã€æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨CVæœ€è‰¯ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤ºã—ã¦ãã ã•ã„ã€‚
                </p>
                <details>
                    <summary>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</summary>
                    <pre><code class="language-python">from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

rf_model = RandomForestRegressor(random_state=42)
grid_search = GridSearchCV(
    rf_model, param_grid, cv=5, scoring='r2', n_jobs=-1, verbose=1
)

grid_search.fit(X_train, y_train)

print(f"æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {grid_search.best_params_}")
print(f"CVæœ€è‰¯RÂ²ã‚¹ã‚³ã‚¢: {grid_search.best_score_:.4f}")

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
test_score = grid_search.best_estimator_.score(X_test, y_test)
print(f"ãƒ†ã‚¹ãƒˆRÂ²ã‚¹ã‚³ã‚¢: {test_score:.4f}")
</code></pre>
                </details>
            </div>

            <div class="exercise">
                <h4>æ¼”ç¿’5ï¼ˆMediumï¼‰: å­¦ç¿’æ›²ç·šè§£é‡ˆ</h4>
                <p>
                    å­¦ç¿’æ›²ç·šã‚’æç”»ã—ã€éå­¦ç¿’ãƒ»æœªå­¦ç¿’ã®åˆ¤å®šã‚’è¡Œã£ã¦ãã ã•ã„ã€‚è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’10%, 30%, 50%, 70%, 100%ã§å¤‰åŒ–ã•ã›ã¾ã™ã€‚
                </p>
                <details>
                    <summary>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</summary>
                    <pre><code class="language-python">from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt

train_sizes, train_scores, valid_scores = learning_curve(
    RandomForestRegressor(n_estimators=100, random_state=42),
    X, y,
    train_sizes=[0.1, 0.3, 0.5, 0.7, 1.0],
    cv=5,
    scoring='r2',
    n_jobs=-1
)

plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_scores.mean(axis=1), 'o-', label='è¨“ç·´ã‚¹ã‚³ã‚¢')
plt.plot(train_sizes, valid_scores.mean(axis=1), 's-', label='æ¤œè¨¼ã‚¹ã‚³ã‚¢')
plt.xlabel('è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°')
plt.ylabel('RÂ²ã‚¹ã‚³ã‚¢')
plt.title('å­¦ç¿’æ›²ç·š')
plt.legend()
plt.grid(alpha=0.3)

# è¨ºæ–­
final_train_score = train_scores.mean(axis=1)[-1]
final_valid_score = valid_scores.mean(axis=1)[-1]
gap = final_train_score - final_valid_score

if gap > 0.1:
    diagnosis = "éå­¦ç¿’ã®å¯èƒ½æ€§ï¼ˆè¨“ç·´ã‚¹ã‚³ã‚¢ >> æ¤œè¨¼ã‚¹ã‚³ã‚¢ï¼‰"
elif final_valid_score < 0.7:
    diagnosis = "æœªå­¦ç¿’ã®å¯èƒ½æ€§ï¼ˆä¸¡ã‚¹ã‚³ã‚¢ã¨ã‚‚ä½ã„ï¼‰"
else:
    diagnosis = "è‰¯å¥½ãªæ±åŒ–æ€§èƒ½"

plt.text(0.5, 0.1, f"è¨ºæ–­: {diagnosis}",
         transform=plt.gca().transAxes, fontsize=12,
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
plt.tight_layout()
plt.show()
</code></pre>
                </details>
            </div>

            <div class="exercise">
                <h4>æ¼”ç¿’6ï¼ˆMediumï¼‰: SHAPå€¤è¨ˆç®—ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°</h4>
                <p>
                    è¨“ç·´æ¸ˆã¿Random Forestãƒ¢ãƒ‡ãƒ«ã®SHAPå€¤ã‚’è¨ˆç®—ã—ã€å¹³å‡çµ¶å¯¾SHAPå€¤ã®ä¸Šä½10ç‰¹å¾´é‡ã‚’ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ç¤ºã—ã¦ãã ã•ã„ã€‚
                </p>
                <details>
                    <summary>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</summary>
                    <pre><code class="language-python">import shap
import numpy as np

# TreeExplainerã®ä½œæˆ
explainer = shap.TreeExplainer(best_rf)

# SHAPå€¤è¨ˆç®—
shap_values = explainer.shap_values(X_test)

# å¹³å‡çµ¶å¯¾SHAPå€¤ã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°
shap_importance = np.abs(shap_values).mean(axis=0)
feature_names = X_test.columns.tolist()

shap_df = pd.DataFrame({
    'feature': feature_names,
    'mean_abs_shap': shap_importance
}).sort_values('mean_abs_shap', ascending=False)

print("ä¸Šä½10ç‰¹å¾´é‡ï¼ˆå¹³å‡çµ¶å¯¾SHAPå€¤ï¼‰:")
print(shap_df.head(10).to_string(index=False))

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.barh(shap_df.head(10)['feature'], shap_df.head(10)['mean_abs_shap'])
plt.xlabel('å¹³å‡çµ¶å¯¾SHAPå€¤')
plt.title('ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆSHAPï¼‰')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()
</code></pre>
                </details>
            </div>

            <div class="exercise">
                <h4>æ¼”ç¿’7ï¼ˆMediumï¼‰: ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ</h4>
                <p>
                    Random Forestã€XGBoostã€MLPã®3ãƒ¢ãƒ‡ãƒ«ã‚’åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¨“ç·´ã—ã€MAEã€RMSEã€RÂ²ã‚’æ¯”è¼ƒè¡¨ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
                </p>
                <details>
                    <summary>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</summary>
                    <pre><code class="language-python">from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor
import xgboost as xgb
from sklearn.preprocessing import StandardScaler

# ãƒ¢ãƒ‡ãƒ«å®šç¾©
models = {
    'Random Forest': RandomForestRegressor(n_estimators=200, max_depth=20,
                                           random_state=42),
    'XGBoost': xgb.XGBRegressor(n_estimators=200, max_depth=6,
                                learning_rate=0.1, random_state=42),
    'MLP': MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500,
                        random_state=42)
}

# è©•ä¾¡çµæœã‚’æ ¼ç´
results = []

for model_name, model in models.items():
    # MLPã®å ´åˆã¯æ¨™æº–åŒ–
    if model_name == 'MLP':
        scaler = StandardScaler()
        X_train_current = scaler.fit_transform(X_train)
        X_test_current = scaler.transform(X_test)
    else:
        X_train_current = X_train
        X_test_current = X_test

    # è¨“ç·´ã¨äºˆæ¸¬
    model.fit(X_train_current, y_train)
    y_pred = model.predict(X_test_current)

    # è©•ä¾¡æŒ‡æ¨™
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    results.append({
        'Model': model_name,
        'MAE': mae,
        'RMSE': rmse,
        'RÂ²': r2
    })

# çµæœè¡¨ç¤º
results_df = pd.DataFrame(results)
print("\nãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ:")
print(results_df.to_string(index=False))

# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’å¼·èª¿
best_model_idx = results_df['RÂ²'].idxmax()
print(f"\næœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {results_df.loc[best_model_idx, 'Model']}")
</code></pre>
                </details>
            </div>

            <!-- Hard Exercises -->
            <div class="exercise">
                <h4>æ¼”ç¿’8ï¼ˆHardï¼‰: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ï¼ˆStackingï¼‰</h4>
                <p>
                    Random Forestã€XGBoostã€MLPã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã—ã€LinearRegressionã‚’ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã¨ã™ã‚‹StackingRegressorã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šæ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã‹æ¤œè¨¼ã—ã¦ãã ã•ã„ã€‚
                </p>
                <details>
                    <summary>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</summary>
                    <pre><code class="language-python">from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import LinearRegression

# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
base_models = [
    ('rf', RandomForestRegressor(n_estimators=200, max_depth=20,
                                 random_state=42)),
    ('xgb', xgb.XGBRegressor(n_estimators=200, max_depth=6,
                             learning_rate=0.1, random_state=42)),
    ('mlp', Pipeline([
        ('scaler', StandardScaler()),
        ('model', MLPRegressor(hidden_layer_sizes=(100, 50),
                               max_iter=500, random_state=42))
    ]))
]

# ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«
meta_model = LinearRegression()

# StackingRegressorã®æ§‹ç¯‰
stacking_model = StackingRegressor(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5
)

# è¨“ç·´
print("Stackingãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
stacking_model.fit(X_train, y_train)

# è©•ä¾¡
y_pred_stacking = stacking_model.predict(X_test)
stacking_mae = mean_absolute_error(y_test, y_pred_stacking)
stacking_rmse = np.sqrt(mean_squared_error(y_test, y_pred_stacking))
stacking_r2 = r2_score(y_test, y_pred_stacking)

print("\nStacking ãƒ¢ãƒ‡ãƒ«æ€§èƒ½:")
print(f"  MAE:  {stacking_mae:.4f}")
print(f"  RMSE: {stacking_rmse:.4f}")
print(f"  RÂ²:   {stacking_r2:.4f}")

# å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒ
print("\næ€§èƒ½æ”¹å–„ç‡ï¼ˆvs Random Forestï¼‰:")
rf_r2 = 0.85  # å‰ã®æ¼”ç¿’çµæœã‚’ä»®å®š
improvement = (stacking_r2 - rf_r2) / rf_r2 * 100
print(f"  RÂ²æ”¹å–„: {improvement:.2f}%")
</code></pre>
                </details>
            </div>

            <div class="exercise">
                <h4>æ¼”ç¿’9ï¼ˆHardï¼‰: Matbenchãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å†ç¾</h4>
                <p>
                    Matbenchã®matbench_mp_e_formï¼ˆå½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰ã‚¿ã‚¹ã‚¯ã§ã€çµ„æˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€å…¬å¼ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§çµæœã‚’ä¿å­˜ã—ã¦ãã ã•ã„ã€‚
                </p>
                <details>
                    <summary>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</summary>
                    <pre><code class="language-python">from matbench.bench import MatbenchBenchmark

# Matbenchãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®åˆæœŸåŒ–
mb = MatbenchBenchmark(autoload=False, subset=['matbench_mp_e_form'])

for task in mb.tasks:
    task.load()

    # åŒ–å­¦çµ„æˆå¤‰æ›ã¨Magpieç‰¹å¾´é‡ç”Ÿæˆ
    str_to_comp = StrToComposition(target_col_id='composition_obj')
    task.df = str_to_comp.featurize_dataframe(task.df, 'composition')

    featurizer = ElementProperty.from_preset('magpie')
    task.df = featurizer.featurize_dataframe(task.df, 'composition_obj')

    feature_cols = featurizer.feature_labels()

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('model', RandomForestRegressor(n_estimators=300, max_depth=30,
                                        min_samples_split=5, random_state=42))
    ])

    # 5-fold CVã§è©•ä¾¡
    for fold_idx, fold in enumerate(task.folds):
        train_inputs, train_outputs = task.get_train_and_val_data(fold)
        X_train_fold = train_inputs[feature_cols]
        y_train_fold = train_outputs

        pipeline.fit(X_train_fold, y_train_fold)

        test_inputs, _ = task.get_test_data(fold, include_target=False)
        X_test_fold = test_inputs[feature_cols]
        predictions = pipeline.predict(X_test_fold)

        task.record(fold, predictions)
        print(f"Fold {fold_idx + 1} å®Œäº†")

    # ã‚¹ã‚³ã‚¢è¡¨ç¤º
    scores = task.scores
    for metric, values in scores.items():
        print(f"{metric}: {values['mean']:.4f} Â± {values['std']:.4f}")

# çµæœä¿å­˜
mb.to_file('matbench_eform_results.json')
print("\nçµæœã‚’ matbench_eform_results.json ã«ä¿å­˜ã—ã¾ã—ãŸ")
</code></pre>
                </details>
            </div>

            <div class="exercise">
                <h4>æ¼”ç¿’10ï¼ˆHardï¼‰: ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡æŒ‡æ¨™ï¼ˆMAPEï¼‰</h4>
                <p>
                    Mean Absolute Percentage Errorï¼ˆMAPEï¼‰ã‚’ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡æŒ‡æ¨™ã¨ã—ã¦å®Ÿè£…ã—ã€GridSearchCVã§ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚MAPEã¯ä»¥ä¸‹ã®å¼ã§å®šç¾©ã•ã‚Œã¾ã™ï¼š<br>
                    $$\text{MAPE} = \frac{100}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|$$
                </p>
                <details>
                    <summary>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</summary>
                    <pre><code class="language-python">from sklearn.metrics import make_scorer

def mean_absolute_percentage_error(y_true, y_pred):
    """
    MAPEï¼ˆMean Absolute Percentage Errorï¼‰ã‚’è¨ˆç®—

    æ³¨æ„: y_true ã«0ãŒå«ã¾ã‚Œã‚‹å ´åˆã¯ã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚å°ã•ã„å€¤ã‚’è¿½åŠ 
    """
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)

    # 0é™¤ç®—å›é¿
    epsilon = 1e-10
    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100

# scikit-learn scorerã«å¤‰æ›
mape_scorer = make_scorer(mean_absolute_percentage_error,
                          greater_is_better=False)  # MAPEã¯å°ã•ã„ã»ã©è‰¯ã„

# GridSearchCVã§ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡æŒ‡æ¨™ã‚’ä½¿ç”¨
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20]
}

grid_search = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid,
    cv=5,
    scoring=mape_scorer,  # ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡æŒ‡æ¨™
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

print(f"æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {grid_search.best_params_}")
print(f"CVæœ€è‰¯MAPEã‚¹ã‚³ã‚¢: {-grid_search.best_score_:.4f}%")  # è² ã‚’åè»¢

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
y_pred_test = grid_search.best_estimator_.predict(X_test)
test_mape = mean_absolute_percentage_error(y_test, y_pred_test)
print(f"ãƒ†ã‚¹ãƒˆMAPE: {test_mape:.4f}%")
</code></pre>
                </details>
            </div>
        </section>

        <!-- Learning Check -->
        <section id="learning-check">
            <h2>å­¦ç¿’ç›®æ¨™é”æˆåº¦ãƒã‚§ãƒƒã‚¯</h2>
            <div class="info-box">
                <h4>ä»¥ä¸‹ã®é …ç›®ã‚’è‡ªå·±è©•ä¾¡ã—ã¦ãã ã•ã„ï¼ˆ3æ®µéš: åˆç´š/ä¸­ç´š/ä¸Šç´šï¼‰</h4>

                <h4>åˆç´šãƒ¬ãƒ™ãƒ«ï¼ˆåŸºæœ¬ç†è§£ï¼‰</h4>
                <ul>
                    <li>scikit-learnãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰ã¨ä¿å­˜/èª­ã¿è¾¼ã¿ãŒã§ãã‚‹</li>
                    <li>Random Forestã€XGBoostã€MLPã®ç‰¹å¾´ã¨ä½¿ã„åˆ†ã‘ã‚’èª¬æ˜ã§ãã‚‹</li>
                    <li>çµ„æˆãƒ™ãƒ¼ã‚¹ã¨GNNãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½å·®ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
                    <li>ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®ç›®çš„ã¨å®Ÿè£…æ–¹æ³•ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
                </ul>

                <h4>ä¸­ç´šãƒ¬ãƒ™ãƒ«ï¼ˆå®Ÿè·µã‚¹ã‚­ãƒ«ï¼‰</h4>
                <ul>
                    <li>GridSearchCVã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã§ãã‚‹</li>
                    <li>å­¦ç¿’æ›²ç·šã‹ã‚‰éå­¦ç¿’/æœªå­¦ç¿’ã‚’è¨ºæ–­ã§ãã‚‹</li>
                    <li>SHAPå€¤ã‚’è¨ˆç®—ã—ã€ç‰¹å¾´é‡é‡è¦åº¦ã‚’è§£é‡ˆã§ãã‚‹</li>
                    <li>è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å®šé‡çš„ã«æ¯”è¼ƒã§ãã‚‹</li>
                </ul>

                <h4>ä¸Šç´šãƒ¬ãƒ™ãƒ«ï¼ˆå¿œç”¨åŠ›ï¼‰</h4>
                <ul>
                    <li>StackingRegressorã§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
                    <li>Matbenchãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§è©•ä¾¡ã—ã€çµæœã‚’å…¬å¼ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ä¿å­˜ã§ãã‚‹</li>
                    <li>ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡æŒ‡æ¨™ã‚’å®Ÿè£…ã—ã€GridSearchCVã«çµ±åˆã§ãã‚‹</li>
                    <li>SHAPã®Dependence Plotã¨Partial Dependence Plotã‚’ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹</li>
                </ul>
            </div>
        </section>

        <!-- Summary -->
        <section id="summary">
            <h2>ã¾ã¨ã‚</h2>
            <p>
                æœ¬ç« ã§ã¯ã€çµ„æˆãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ã‚’æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¨çµ±åˆã—ã€ææ–™ç‰¹æ€§ã‚’é«˜ç²¾åº¦ã«äºˆæ¸¬ã™ã‚‹å®Ÿè·µçš„æ‰‹æ³•ã‚’å­¦ã³ã¾ã—ãŸã€‚ä¸»è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æŒ¯ã‚Šè¿”ã‚Šã¾ã™ï¼š
            </p>

            <h3>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</h3>
            <ul>
                <li><strong>scikit-learnãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</strong>: ç‰¹å¾´é‡ç”Ÿæˆãƒ»å‰å‡¦ç†ãƒ»ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚’çµ±åˆã—ã€å†ç¾æ€§ã‚’ç¢ºä¿</li>
                <li><strong>ãƒ¢ãƒ‡ãƒ«é¸æŠ</strong>: Random Forestï¼ˆè§£é‡ˆæ€§é‡è¦–ï¼‰ã€XGBoostï¼ˆç²¾åº¦é‡è¦–ï¼‰ã€MLPï¼ˆå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰ã®ä½¿ã„åˆ†ã‘</li>
                <li><strong>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</strong>: GridSearchCVï¼ˆç¶²ç¾…çš„ï¼‰ã¨RandomizedSearchCVï¼ˆåŠ¹ç‡çš„ï¼‰ã®é¸æŠ</li>
                <li><strong>å­¦ç¿’æ›²ç·šåˆ†æ</strong>: éå­¦ç¿’/æœªå­¦ç¿’ã®è¨ºæ–­ã¨ãƒ‡ãƒ¼ã‚¿è¿½åŠ åŠ¹æœã®äºˆæ¸¬</li>
                <li><strong>çµ„æˆ vs GNN</strong>: Matbenchã§çµ„æˆãƒ™ãƒ¼ã‚¹ã¯GNNã®85-95%ã®ç²¾åº¦ã‚’å®Ÿç¾ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã¯1/100ä»¥ä¸‹</li>
                <li><strong>SHAPè§£é‡ˆ</strong>: äºˆæ¸¬ã®è¦å› ã‚’å®šé‡åŒ–ã—ã€ç‰©ç†çš„æ´å¯Ÿã‚’ç²å¾—</li>
            </ul>

            <h3>å®Ÿå‹™ã§ã®æ´»ç”¨æŒ‡é‡</h3>
            <div class="tip-box">
                <h4>æ¨å¥¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</h4>
                <ol>
                    <li><strong>åˆæœŸæ¢ç´¢</strong>: çµ„æˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§10âµå€™è£œã‚’10Â³å€™è£œã«çµã‚‹ï¼ˆæ•°åˆ†ï¼‰</li>
                    <li><strong>æ§‹é€ æœ€é©åŒ–</strong>: é¸å®šå€™è£œã®DFTæ§‹é€ ç·©å’Œï¼ˆæ•°æ—¥ï¼‰</li>
                    <li><strong>ç²¾å¯†äºˆæ¸¬</strong>: GNNã§æœ€çµ‚å€™è£œã‚’é¸å®šï¼ˆæ•°æ™‚é–“ï¼‰</li>
                    <li><strong>å®Ÿé¨“æ¤œè¨¼</strong>: ä¸Šä½10-20å€™è£œã‚’åˆæˆãƒ»æ¸¬å®š</li>
                </ol>
                <p>
                    ã“ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚Šã€<strong>è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’90%å‰Šæ¸›</strong>ã—ã¤ã¤ã€é«˜ç²¾åº¦ãªææ–™æ¢ç´¢ãŒå®Ÿç¾ã§ãã¾ã™ã€‚
                </p>
            </div>

            <h3>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>
            <p>
                ç¬¬5ç« ã§ã¯ã€å®Ÿéš›ã®ææ–™æ¢ç´¢ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«çµ„æˆãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ã‚’é©ç”¨ã—ã€æ–°è¦ææ–™ã®ç™ºè¦‹ã‹ã‚‰å®Ÿé¨“æ¤œè¨¼ã¾ã§ã®ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å­¦ã³ã¾ã™ã€‚Materials Projectãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ã®çµ±åˆã€èƒ½å‹•å­¦ç¿’ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–ã€ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–ãªã©ã€ã‚ˆã‚Šå®Ÿè·µçš„ãªå†…å®¹ã«é€²ã¿ã¾ã™ã€‚
            </p>
        </section>

        <!-- References -->
        <section id="references">
            <h2>å‚è€ƒæ–‡çŒ®</h2>
            <ol>
                <li>
                    Ward, L., Agrawal, A., Choudhary, A., & Wolverton, C. (2016). "A general-purpose machine learning framework for predicting properties of inorganic materials." <em>npj Computational Materials</em>, 2, 16028, pp. 4-6.
                    <a href="https://doi.org/10.1038/npjcompumats.2016.28" target="_blank">https://doi.org/10.1038/npjcompumats.2016.28</a>
                    <br><small>ï¼ˆMagpieç‰¹å¾´é‡ã®æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€Random Forestã¨ã®çµ±åˆæ‰‹æ³•ã‚’è©³è¿°ï¼‰</small>
                </li>
                <li>
                    Jha, D., Ward, L., Paul, A., Liao, W., Choudhary, A., Wolverton, C., & Agrawal, A. (2018). "ElemNet: Deep Learning the Chemistry of Materials From Only Elemental Composition." <em>Scientific Reports</em>, 8, 17593, pp. 6-9.
                    <a href="https://doi.org/10.1038/s41598-018-35934-y" target="_blank">https://doi.org/10.1038/s41598-018-35934-y</a>
                    <br><small>ï¼ˆçµ„æˆã®ã¿ã‹ã‚‰é«˜ç²¾åº¦äºˆæ¸¬ã‚’å®Ÿç¾ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€Matbenchãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ€§èƒ½ï¼‰</small>
                </li>
                <li>
                    Dunn, A., Wang, Q., Ganose, A., Dopp, D., & Jain, A. (2020). "Benchmarking materials property prediction methods: the Matbench test set and Automatminer reference algorithm." <em>npj Computational Materials</em>, 6, 138, pp. 1-10.
                    <a href="https://doi.org/10.1038/s41524-020-00406-3" target="_blank">https://doi.org/10.1038/s41524-020-00406-3</a>
                    <br><small>ï¼ˆMatbench v0.1ã®è¨­è¨ˆæ€æƒ³ã€13ã‚¿ã‚¹ã‚¯ã®è©³ç´°ã€æ¨™æº–è©•ä¾¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«ï¼‰</small>
                </li>
                <li>
                    scikit-learn Documentation: Pipeline and GridSearchCV.
                    <a href="https://scikit-learn.org/stable/" target="_blank">https://scikit-learn.org/stable/</a>
                    <br><small>ï¼ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã€å­¦ç¿’æ›²ç·šã®å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰</small>
                </li>
                <li>
                    XGBoost Documentation.
                    <a href="https://xgboost.readthedocs.io/" target="_blank">https://xgboost.readthedocs.io/</a>
                    <br><small>ï¼ˆXGBoostã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€æ—©æœŸåœæ­¢ã€ç‰¹å¾´é‡é‡è¦åº¦ã®è©³ç´°ï¼‰</small>
                </li>
                <li>
                    Lundberg, S.M., & Lee, S.I. (2017). "A Unified Approach to Interpreting Model Predictions." <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, pp. 4765-4774.
                    <a href="https://arxiv.org/abs/1705.07874" target="_blank">https://arxiv.org/abs/1705.07874</a>
                    <br><small>ï¼ˆSHAPåŸè«–æ–‡ã€Shapleyå€¤ã®ç†è«–çš„åŸºç¤ã¨TreeExplainerã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰</small>
                </li>
                <li>
                    matminer Tutorials: Machine Learning Integration.
                    <a href="https://hackingmaterials.lbl.gov/matminer/" target="_blank">https://hackingmaterials.lbl.gov/matminer/</a>
                    <br><small>ï¼ˆmatminerã¨scikit-learnçµ±åˆã®å®Ÿè·µä¾‹ã€DFMLAdaptorã®ä½¿ç”¨æ³•ã€ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ï¼‰</small>
                </li>
            </ol>
        </section>

        <!-- Navigation -->
        <nav class="navigation">
            <a href="chapter-3.html" class="nav-button">â† ç¬¬3ç« : å…ƒç´ ç‰¹æ€§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨Featurizer</a>
            <a href="index.html" class="nav-button">ç›®æ¬¡ã¸</a>
            <a href="chapter-5.html" class="nav-button">ç¬¬5ç« : å®Ÿè·µçš„ææ–™æ¢ç´¢ â†’</a>
        </nav>
    </div>

    <footer style="text-align: center; padding: 2rem 0; margin-top: 3rem; border-top: 1px solid var(--border-color);">
        <p>&copy; 2025 çµ„æˆãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡å…¥é–€ã‚·ãƒªãƒ¼ã‚º | AI Terakoya Knowledge Base</p>
    </footer>
</body>
</html>
