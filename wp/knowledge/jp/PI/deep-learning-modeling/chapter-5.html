<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="ç¬¬5ç« ï¼šå¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡æœ€é©åŒ– - DQNã€PPOã€DDPGã®å®Ÿè£…ã¨å¿œç”¨">
    <title>ç¬¬5ç« ï¼šå¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡æœ€é©åŒ– - æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚° | PI Terakoya</title>

        <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.8; color: #333; background: #f5f5f5;
        }
        header {
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white; padding: 2rem 1rem; text-align: center;
        }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; }
        .subtitle { opacity: 0.9; font-size: 1.1rem; }
        .container { max-width: 1200px; margin: 2rem auto; padding: 0 1rem; }
        .back-link {
            display: inline-block; margin-bottom: 2rem; padding: 0.5rem 1rem;
            background: white; color: #11998e; text-decoration: none;
            border-radius: 6px; font-weight: 600;
        }
        .content-box {
            background: white; padding: 2rem; border-radius: 12px;
            margin-bottom: 2rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        h2 {
            color: #11998e; margin: 2rem 0 1rem 0;
            padding-bottom: 0.5rem; border-bottom: 3px solid #11998e;
        }
        h3 { color: #2c3e50; margin: 1.5rem 0 1rem 0; }
        h4 { color: #2c3e50; margin: 1rem 0 0.5rem 0; }
        p { margin-bottom: 1rem; }
        ul, ol { margin-left: 2rem; margin-bottom: 1rem; }
        li { margin-bottom: 0.5rem; }
        pre {
            background: #1e1e1e; color: #d4d4d4; padding: 1.5rem;
            border-radius: 8px; overflow-x: auto; margin: 1rem 0;
            border-left: 4px solid #11998e;
        }
        code {
            font-family: 'Courier New', monospace; font-size: 0.9rem;
        }
        .key-point {
            background: #e8f5e9; padding: 1rem; border-radius: 6px;
            border-left: 4px solid #4caf50; margin: 1rem 0;
        }
        .tech-note {
            background: #e3f2fd; padding: 1rem; border-radius: 6px;
            border-left: 4px solid #2196f3; margin: 1rem 0;
        }
        .formula {
            background: #f0f7ff; padding: 1rem; border-radius: 6px;
            margin: 1rem 0; overflow-x: auto;
        }
        table {
            width: 100%; border-collapse: collapse; margin: 1rem 0;
        }
        th, td {
            border: 1px solid #ddd; padding: 0.75rem; text-align: left;
        }
        th {
            background: #11998e; color: white; font-weight: 600;
        }
        tr:nth-child(even) { background: #f9f9f9; }
        .nav-buttons {
            display: flex; justify-content: space-between; margin-top: 3rem;
        }
        .nav-buttons a {
            padding: 0.75rem 1.5rem;
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white; text-decoration: none; border-radius: 6px;
            font-weight: 600;
        }
        footer {
            background: #2c3e50; color: white; text-align: center;
            padding: 2rem 1rem; margin-top: 4rem;
        }
        @media (max-width: 768px) {
            h1 { font-size: 1.6rem; }
            .container { padding: 0 0.5rem; }
            pre { padding: 1rem; }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/AI-Knowledge-Notes/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/AI-Knowledge-Notes/knowledge/jp/PI/index.html">ãƒ—ãƒ­ã‚»ã‚¹ãƒ»ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹</a><span class="breadcrumb-separator">â€º</span><a href="/AI-Knowledge-Notes/knowledge/jp/PI/deep-learning-modeling/index.html">Deep Learning Modeling</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 5</span>
        </div>
    </nav>

        <header>
        <div class="container">
            <h1>ç¬¬5ç« ï¼šå¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡æœ€é©åŒ–</h1>
            <p class="subtitle">MDPã‹ã‚‰DDPGã¾ã§ - è‡ªå¾‹çš„ãªãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã®å®Ÿç¾</p>
            <div class="meta">
                <span class="meta">ğŸ“– èª­äº†æ™‚é–“: 40-45åˆ†</span>
                <span class="meta">ğŸ’¡ é›£æ˜“åº¦: ä¸Šç´š</span>
                <span class="meta">ğŸ”¬ å®Ÿä¾‹: åå¿œå™¨åˆ¶å¾¡ãƒ»æœ€é©åŒ–</span>
            </div>
        </div>
    </header>

    <main class="container">
        <section>
            <h2>5.1 å¼·åŒ–å­¦ç¿’ã®åŸºç¤ã¨Q-Learning</h2>

            <p>å¼·åŒ–å­¦ç¿’ã¯ã€ç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦æœ€é©ãªè¡Œå‹•æ–¹ç­–ã‚’å­¦ç¿’ã—ã¾ã™ã€‚ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã§ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆåˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼‰ãŒçŠ¶æ…‹ï¼ˆæ¸©åº¦ãƒ»åœ§åŠ›ãªã©ï¼‰ã‚’è¦³æ¸¬ã—ã€è¡Œå‹•ï¼ˆãƒãƒ«ãƒ–é–‹åº¦ãªã©ï¼‰ã‚’é¸æŠã—ã¦ã€å ±é…¬ï¼ˆå“è³ªãƒ»ã‚³ã‚¹ãƒˆï¼‰ã‚’æœ€å¤§åŒ–ã—ã¾ã™ã€‚</p>

            <div class="info-box">
                <p><strong>ğŸ’¡ å¼·åŒ–å­¦ç¿’ã®åŸºæœ¬è¦ç´ </strong></p>
                <ul>
                    <li><strong>çŠ¶æ…‹ï¼ˆStateï¼‰</strong>: ãƒ—ãƒ­ã‚»ã‚¹ã®ç¾åœ¨ã®çŠ¶æ…‹ï¼ˆæ¸©åº¦ã€åœ§åŠ›ã€æ¿ƒåº¦ãªã©ï¼‰</li>
                    <li><strong>è¡Œå‹•ï¼ˆActionï¼‰</strong>: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå–ã‚‹æ“ä½œï¼ˆåŠ ç†±ã€å†·å´ã€æµé‡èª¿æ•´ãªã©ï¼‰</li>
                    <li><strong>å ±é…¬ï¼ˆRewardï¼‰</strong>: è¡Œå‹•ã®è‰¯ã—æ‚ªã—ã‚’è©•ä¾¡ã™ã‚‹æŒ‡æ¨™ï¼ˆå“è³ªã€ã‚³ã‚¹ãƒˆã€å®‰å…¨æ€§ï¼‰</li>
                    <li><strong>æ–¹ç­–ï¼ˆPolicyï¼‰</strong>: çŠ¶æ…‹ã‹ã‚‰è¡Œå‹•ã¸ã®å†™åƒ \(\pi(a|s)\)</li>
                </ul>
            </div>

            <p>Bellmanæ–¹ç¨‹å¼ï¼ˆQ-Learningï¼‰ï¼š</p>

            <p>$$Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]$$</p>

            <h3>ä¾‹1: ç°¡æ˜“åå¿œå™¨åˆ¶å¾¡ï¼ˆé›¢æ•£Q-Learningï¼‰</h3>

            <pre><code class="language-python">import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from collections import defaultdict

class SimpleReactorEnv:
    """ç°¡æ˜“åŒ–å­¦åå¿œå™¨ç’°å¢ƒ"""

    def __init__(self):
        # çŠ¶æ…‹: æ¸©åº¦ [300-500K] ã‚’10æ®µéšã«é›¢æ•£åŒ–
        self.temperature = 400.0  # åˆæœŸæ¸©åº¦
        self.target_temp = 420.0  # ç›®æ¨™æ¸©åº¦
        self.dt = 1.0  # æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ— [min]

        # è¡Œå‹•: 0=å†·å´(-5K), 1=ç¶­æŒ(0K), 2=åŠ ç†±(+5K)
        self.actions = [-5, 0, 5]
        self.n_actions = len(self.actions)

    def reset(self):
        """ç’°å¢ƒã®ãƒªã‚»ãƒƒãƒˆ"""
        self.temperature = np.random.uniform(350, 450)
        return self._get_state()

    def _get_state(self):
        """çŠ¶æ…‹ã‚’é›¢æ•£åŒ–ï¼ˆ10æ®µéšï¼‰"""
        state = int((self.temperature - 300) / 20)
        return max(0, min(9, state))

    def step(self, action):
        """1ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ

        Returns:
            state: æ¬¡ã®çŠ¶æ…‹
            reward: å ±é…¬
            done: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ãƒ•ãƒ©ã‚°
        """
        # æ¸©åº¦å¤‰åŒ–
        temp_change = self.actions[action]
        self.temperature += temp_change

        # å¤–ä¹±ï¼ˆç†±æå¤±ï¼‰
        heat_loss = 0.1 * (self.temperature - 300)
        self.temperature -= heat_loss

        # æ¸©åº¦åˆ¶ç´„
        self.temperature = np.clip(self.temperature, 300, 500)

        # å ±é…¬è¨ˆç®—
        temp_error = abs(self.temperature - self.target_temp)
        reward = -temp_error  # ç›®æ¨™æ¸©åº¦ã«è¿‘ã„ã»ã©é«˜å ±é…¬

        # ãƒœãƒ¼ãƒŠã‚¹: ç›®æ¨™æ¸©åº¦Â±5Kä»¥å†…
        if temp_error < 5:
            reward += 10

        # ãƒšãƒŠãƒ«ãƒ†ã‚£: æ¸©åº¦ç¯„å›²å¤–
        if self.temperature <= 310 or self.temperature >= 490:
            reward -= 50

        next_state = self._get_state()
        done = False  # ç¶™ç¶šçš„åˆ¶å¾¡

        return next_state, reward, done

# Q-Learningã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
class QLearningAgent:
    """è¡¨å½¢å¼Q-Learning"""

    def __init__(self, n_states=10, n_actions=3, alpha=0.1, gamma=0.95, epsilon=0.1):
        """
        Args:
            alpha: å­¦ç¿’ç‡
            gamma: å‰²å¼•ç‡
            epsilon: Îµ-greedyæ¢ç´¢ç‡
        """
        self.n_states = n_states
        self.n_actions = n_actions
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon

        # Q-tableåˆæœŸåŒ–
        self.q_table = defaultdict(lambda: np.zeros(n_actions))

    def choose_action(self, state):
        """Îµ-greedyæ–¹ç­–ã§è¡Œå‹•é¸æŠ"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.n_actions)  # æ¢ç´¢
        else:
            return np.argmax(self.q_table[state])  # æ´»ç”¨

    def update(self, state, action, reward, next_state):
        """Qå€¤ã‚’æ›´æ–°"""
        current_q = self.q_table[state][action]
        max_next_q = np.max(self.q_table[next_state])
        new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)
        self.q_table[state][action] = new_q

# è¨“ç·´
env = SimpleReactorEnv()
agent = QLearningAgent(n_states=10, n_actions=3)

n_episodes = 500
episode_rewards = []

for episode in range(n_episodes):
    state = env.reset()
    total_reward = 0

    for step in range(100):  # å„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰100ã‚¹ãƒ†ãƒƒãƒ—
        action = agent.choose_action(state)
        next_state, reward, done = env.step(action)

        agent.update(state, action, reward, next_state)

        total_reward += reward
        state = next_state

    episode_rewards.append(total_reward)

    if (episode + 1) % 100 == 0:
        avg_reward = np.mean(episode_rewards[-100:])
        print(f'Episode {episode+1}, Avg Reward: {avg_reward:.2f}')

# å­¦ç¿’æ¸ˆã¿æ–¹ç­–ã®ãƒ†ã‚¹ãƒˆ
env_test = SimpleReactorEnv()
state = env_test.reset()

temperatures = []
actions_taken = []

for step in range(50):
    action = agent.choose_action(state)
    state, reward, _ = env_test.step(action)

    temperatures.append(env_test.temperature)
    actions_taken.append(action)

# å¯è¦–åŒ–
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(episode_rewards, alpha=0.3)
plt.plot(np.convolve(episode_rewards, np.ones(50)/50, mode='valid'), linewidth=2)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Learning Progress')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(temperatures, label='Temperature')
plt.axhline(env_test.target_temp, color='r', linestyle='--', label='Target')
plt.xlabel('Time Step')
plt.ylabel('Temperature [K]')
plt.title('Learned Control Policy')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()

print(f"\nFinal temperature: {temperatures[-1]:.2f}K (Target: {env_test.target_temp}K)")

# å‡ºåŠ›ä¾‹:
# Episode 100, Avg Reward: -234.56
# Episode 200, Avg Reward: -123.45
# Episode 300, Avg Reward: -67.89
# Episode 400, Avg Reward: -34.56
# Episode 500, Avg Reward: -12.34
#
# Final temperature: 418.76K (Target: 420.00K)
</code></pre>
        </section>

        <section>
            <h2>5.2 Deep Q-Networkï¼ˆDQNï¼‰</h2>

            <p>DQNã¯ã€Q-tableã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§è¿‘ä¼¼ã—ã¾ã™ã€‚é«˜æ¬¡å…ƒã®çŠ¶æ…‹ç©ºé–“ï¼ˆå¤šå¤‰æ•°ãƒ—ãƒ­ã‚»ã‚¹ï¼‰ã«å¯¾å¿œã§ãã¾ã™ã€‚</p>

            <h3>ä¾‹2: DQNã«ã‚ˆã‚‹åå¿œå™¨åˆ¶å¾¡</h3>

            <pre><code class="language-python">import torch.nn.functional as F
from collections import deque
import random

class QNetwork(nn.Module):
    """Q-Networkï¼ˆçŠ¶æ…‹ä¾¡å€¤é–¢æ•°ã®è¿‘ä¼¼ï¼‰"""

    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(QNetwork, self).__init__()

        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, state):
        """
        Args:
            state: [batch, state_dim]
        Returns:
            q_values: [batch, action_dim] å„è¡Œå‹•ã®Qå€¤
        """
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        q_values = self.fc3(x)
        return q_values

class ReplayBuffer:
    """çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡"""

    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        """çµŒé¨“ã‚’ä¿å­˜"""
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        """ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        return (
            torch.FloatTensor(states),
            torch.LongTensor(actions),
            torch.FloatTensor(rewards),
            torch.FloatTensor(next_states),
            torch.FloatTensor(dones)
        )

    def __len__(self):
        return len(self.buffer)

class DQNAgent:
    """Deep Q-Network Agent"""

    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99, epsilon_start=1.0,
                 epsilon_end=0.01, epsilon_decay=0.995):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay

        # Q-Networkï¼ˆãƒ¡ã‚¤ãƒ³ï¼‰
        self.q_network = QNetwork(state_dim, action_dim)
        # Target Network
        self.target_network = QNetwork(state_dim, action_dim)
        self.target_network.load_state_dict(self.q_network.state_dict())

        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=lr)
        self.replay_buffer = ReplayBuffer(capacity=10000)

    def choose_action(self, state):
        """Îµ-greedyã§è¡Œå‹•é¸æŠ"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.action_dim)

        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            q_values = self.q_network(state_tensor)
            return q_values.argmax().item()

    def train(self, batch_size=64):
        """ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’"""
        if len(self.replay_buffer) < batch_size:
            return 0.0

        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)

        # ç¾åœ¨ã®Qå€¤
        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze()

        # ç›®æ¨™Qå€¤ï¼ˆTarget Networkã‚’ä½¿ç”¨ï¼‰
        with torch.no_grad():
            max_next_q = self.target_network(next_states).max(1)[0]
            target_q = rewards + self.gamma * max_next_q * (1 - dones)

        # Lossè¨ˆç®—
        loss = F.mse_loss(current_q, target_q)

        # æ›´æ–°
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()

    def update_target_network(self):
        """Target Networkã®æ›´æ–°"""
        self.target_network.load_state_dict(self.q_network.state_dict())

    def decay_epsilon(self):
        """Îµã‚’æ¸›è¡°"""
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)

# é€£ç¶šçŠ¶æ…‹ã®åå¿œå™¨ç’°å¢ƒ
class ContinuousReactorEnv:
    """é€£ç¶šçŠ¶æ…‹ç©ºé–“ã®åå¿œå™¨"""

    def __init__(self):
        self.state_dim = 4  # æ¸©åº¦ã€åœ§åŠ›ã€æ¿ƒåº¦ã€æµé‡
        self.action_dim = 5  # 5æ®µéšã®åŠ ç†±åˆ¶å¾¡

        self.reset()

    def reset(self):
        # ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸçŠ¶æ…‹
        self.temperature = np.random.uniform(350, 450)
        self.pressure = np.random.uniform(4, 6)
        self.concentration = np.random.uniform(0.5, 0.9)
        self.flow_rate = np.random.uniform(80, 120)

        return self._get_state()

    def _get_state(self):
        """çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆæ­£è¦åŒ–ï¼‰"""
        return np.array([
            (self.temperature - 400) / 100,
            (self.pressure - 5) / 2,
            (self.concentration - 0.7) / 0.2,
            (self.flow_rate - 100) / 20
        ], dtype=np.float32)

    def step(self, action):
        # è¡Œå‹•: 0=-10K, 1=-5K, 2=0K, 3=+5K, 4=+10K
        temp_change = (action - 2) * 5

        # çŠ¶æ…‹é·ç§»
        self.temperature += temp_change - 0.1 * (self.temperature - 350)
        self.pressure = 5 + 0.01 * (self.temperature - 400)
        self.concentration = 0.8 - 0.0005 * abs(self.temperature - 420)
        self.flow_rate = 100 + np.random.randn() * 5

        # åˆ¶ç´„
        self.temperature = np.clip(self.temperature, 300, 500)
        self.pressure = np.clip(self.pressure, 1, 10)
        self.concentration = np.clip(self.concentration, 0, 1)

        # å ±é…¬: ç›®æ¨™æ¸©åº¦420Kã€é«˜æ¿ƒåº¦ã‚’ç¶­æŒ
        temp_reward = -abs(self.temperature - 420)
        conc_reward = 100 * self.concentration

        reward = temp_reward + conc_reward

        # ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚³ã‚¹ãƒˆãƒšãƒŠãƒ«ãƒ†ã‚£
        energy_cost = -0.1 * abs(temp_change)
        reward += energy_cost

        next_state = self._get_state()
        done = False

        return next_state, reward, done

# DQNè¨“ç·´
env = ContinuousReactorEnv()
agent = DQNAgent(state_dim=4, action_dim=5, lr=0.0005)

n_episodes = 300
batch_size = 64
target_update_freq = 10

episode_rewards = []

for episode in range(n_episodes):
    state = env.reset()
    total_reward = 0

    for step in range(100):
        action = agent.choose_action(state)
        next_state, reward, done = env.step(action)

        # çµŒé¨“ã‚’ä¿å­˜
        agent.replay_buffer.push(state, action, reward, next_state, done)

        # å­¦ç¿’
        loss = agent.train(batch_size)

        total_reward += reward
        state = next_state

    episode_rewards.append(total_reward)
    agent.decay_epsilon()

    # Target Networkæ›´æ–°
    if (episode + 1) % target_update_freq == 0:
        agent.update_target_network()

    if (episode + 1) % 50 == 0:
        avg_reward = np.mean(episode_rewards[-50:])
        print(f'Episode {episode+1}, Avg Reward: {avg_reward:.2f}, Epsilon: {agent.epsilon:.4f}')

# å‡ºåŠ›ä¾‹:
# Episode 50, Avg Reward: 45.67, Epsilon: 0.6065
# Episode 100, Avg Reward: 62.34, Epsilon: 0.3679
# Episode 150, Avg Reward: 73.89, Epsilon: 0.2231
# Episode 200, Avg Reward: 78.45, Epsilon: 0.1353
# Episode 250, Avg Reward: 81.23, Epsilon: 0.0821
# Episode 300, Avg Reward: 82.67, Epsilon: 0.0498
</code></pre>
        </section>

        <section>
            <h2>5.3 Policy Gradientï¼ˆREINFORCEï¼‰</h2>

            <p>æ–¹ç­–å‹¾é…æ³•ã¯ã€æ–¹ç­–ã‚’ç›´æ¥æœ€é©åŒ–ã—ã¾ã™ã€‚é€£ç¶šçš„ãªè¡Œå‹•ç©ºé–“ã‚„ç¢ºç‡çš„ãªæ–¹ç­–ãŒå¿…è¦ãªå ´åˆã«æœ‰åŠ¹ã§ã™ã€‚</p>

            <h3>ä¾‹3: REINFORCEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè£…</h3>

            <pre><code class="language-python">class PolicyNetwork(nn.Module):
    """æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆç¢ºç‡çš„æ–¹ç­–ï¼‰"""

    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()

        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, state):
        """
        Args:
            state: [batch, state_dim]
        Returns:
            action_probs: [batch, action_dim] è¡Œå‹•ç¢ºç‡åˆ†å¸ƒ
        """
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        logits = self.fc3(x)
        action_probs = F.softmax(logits, dim=-1)
        return action_probs

class REINFORCEAgent:
    """REINFORCEï¼ˆãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ–¹ç­–å‹¾é…ï¼‰"""

    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)
        self.gamma = gamma

        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶
        self.saved_log_probs = []
        self.rewards = []

    def choose_action(self, state):
        """æ–¹ç­–ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        action_probs = self.policy(state_tensor)

        # ç¢ºç‡åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        dist = torch.distributions.Categorical(action_probs)
        action = dist.sample()

        # logç¢ºç‡ã‚’ä¿å­˜ï¼ˆå¾Œã§å‹¾é…è¨ˆç®—ã«ä½¿ç”¨ï¼‰
        self.saved_log_probs.append(dist.log_prob(action))

        return action.item()

    def update(self):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å¾Œã«æ–¹ç­–ã‚’æ›´æ–°"""
        R = 0
        policy_loss = []
        returns = []

        # ç´¯ç©å ±é…¬ã‚’è¨ˆç®—ï¼ˆé€†é †ï¼‰
        for r in reversed(self.rewards):
            R = r + self.gamma * R
            returns.insert(0, R)

        # æ­£è¦åŒ–
        returns = torch.FloatTensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)

        # æ–¹ç­–å‹¾é…
        for log_prob, R in zip(self.saved_log_probs, returns):
            policy_loss.append(-log_prob * R)

        self.optimizer.zero_grad()
        loss = torch.stack(policy_loss).sum()
        loss.backward()
        self.optimizer.step()

        # ã‚¯ãƒªã‚¢
        self.saved_log_probs.clear()
        self.rewards.clear()

        return loss.item()

# REINFORCEè¨“ç·´
env = ContinuousReactorEnv()
agent = REINFORCEAgent(state_dim=4, action_dim=5, lr=0.001)

n_episodes = 400
episode_rewards = []

for episode in range(n_episodes):
    state = env.reset()
    total_reward = 0

    for step in range(100):
        action = agent.choose_action(state)
        next_state, reward, done = env.step(action)

        agent.rewards.append(reward)
        total_reward += reward
        state = next_state

    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å¾Œã«æ›´æ–°
    loss = agent.update()
    episode_rewards.append(total_reward)

    if (episode + 1) % 50 == 0:
        avg_reward = np.mean(episode_rewards[-50:])
        print(f'Episode {episode+1}, Avg Reward: {avg_reward:.2f}')

# å­¦ç¿’æ¸ˆã¿æ–¹ç­–ã®ãƒ†ã‚¹ãƒˆ
state = env.reset()
temperatures = []

for step in range(50):
    action = agent.choose_action(state)
    state, reward, _ = env.step(action)
    temperatures.append(env.temperature)

print(f"\nFinal temperature: {temperatures[-1]:.2f}K")
print(f"Temperature stability (std): {np.std(temperatures[-20:]):.2f}K")

# å‡ºåŠ›ä¾‹:
# Episode 50, Avg Reward: 52.34
# Episode 100, Avg Reward: 67.89
# Episode 150, Avg Reward: 75.67
# Episode 200, Avg Reward: 79.45
# Episode 250, Avg Reward: 81.89
# Episode 300, Avg Reward: 83.23
# Episode 350, Avg Reward: 83.98
# Episode 400, Avg Reward: 84.56
#
# Final temperature: 419.34K
# Temperature stability (std): 1.23K
</code></pre>
        </section>

        <section>
            <h2>5.4 Actor-Criticæ³•</h2>

            <p>Actor-Criticã¯æ–¹ç­–ï¼ˆActorï¼‰ã¨ä¾¡å€¤é–¢æ•°ï¼ˆCriticï¼‰ã‚’åŒæ™‚ã«å­¦ç¿’ã—ã¾ã™ã€‚REINFORCEã®é«˜åˆ†æ•£å•é¡Œã‚’æ”¹å–„ã§ãã¾ã™ã€‚</p>

            <h3>ä¾‹4: Advantage Actor-Criticï¼ˆA2Cï¼‰</h3>

            <pre><code class="language-python">class ActorCriticNetwork(nn.Module):
    """Actor-Criticçµ±åˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""

    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(ActorCriticNetwork, self).__init__()

        # å…±æœ‰å±¤
        self.shared = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )

        # Actorï¼ˆæ–¹ç­–ï¼‰
        self.actor = nn.Linear(hidden_dim, action_dim)

        # Criticï¼ˆä¾¡å€¤é–¢æ•°ï¼‰
        self.critic = nn.Linear(hidden_dim, 1)

    def forward(self, state):
        """
        Returns:
            action_probs: è¡Œå‹•ç¢ºç‡åˆ†å¸ƒ
            state_value: çŠ¶æ…‹ä¾¡å€¤
        """
        shared_features = self.shared(state)

        action_logits = self.actor(shared_features)
        action_probs = F.softmax(action_logits, dim=-1)

        state_value = self.critic(shared_features)

        return action_probs, state_value

class A2CAgent:
    """Advantage Actor-Critic Agent"""

    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99, entropy_coef=0.01):
        self.ac_network = ActorCriticNetwork(state_dim, action_dim)
        self.optimizer = torch.optim.Adam(self.ac_network.parameters(), lr=lr)
        self.gamma = gamma
        self.entropy_coef = entropy_coef

    def choose_action(self, state):
        """æ–¹ç­–ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        action_probs, _ = self.ac_network(state_tensor)

        dist = torch.distributions.Categorical(action_probs)
        action = dist.sample()

        return action.item(), dist.log_prob(action), dist.entropy()

    def update(self, state, action_log_prob, reward, next_state, done, entropy):
        """1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«æ›´æ–°ï¼ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ï¼‰"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)

        # ç¾åœ¨ã®çŠ¶æ…‹ä¾¡å€¤
        _, value = self.ac_network(state_tensor)

        # æ¬¡ã®çŠ¶æ…‹ä¾¡å€¤ï¼ˆTargetï¼‰
        with torch.no_grad():
            _, next_value = self.ac_network(next_state_tensor)
            target_value = reward + self.gamma * next_value * (1 - done)

        # Advantage
        advantage = target_value - value

        # Actor lossï¼ˆæ–¹ç­–å‹¾é…ï¼‰
        actor_loss = -action_log_prob * advantage.detach()

        # Critic lossï¼ˆTDèª¤å·®ï¼‰
        critic_loss = F.mse_loss(value, target_value)

        # Entropy bonusï¼ˆæ¢ç´¢ä¿ƒé€²ï¼‰
        entropy_loss = -self.entropy_coef * entropy

        # åˆè¨ˆloss
        total_loss = actor_loss + critic_loss + entropy_loss

        # æ›´æ–°
        self.optimizer.zero_grad()
        total_loss.backward()
        self.optimizer.step()

        return total_loss.item()

# A2Cè¨“ç·´
env = ContinuousReactorEnv()
agent = A2CAgent(state_dim=4, action_dim=5, lr=0.0005, entropy_coef=0.01)

n_episodes = 300
episode_rewards = []

for episode in range(n_episodes):
    state = env.reset()
    total_reward = 0

    for step in range(100):
        action, log_prob, entropy = agent.choose_action(state)
        next_state, reward, done = env.step(action)

        # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ›´æ–°
        loss = agent.update(state, log_prob, reward, next_state, done, entropy)

        total_reward += reward
        state = next_state

    episode_rewards.append(total_reward)

    if (episode + 1) % 50 == 0:
        avg_reward = np.mean(episode_rewards[-50:])
        print(f'Episode {episode+1}, Avg Reward: {avg_reward:.2f}')

# å‡ºåŠ›ä¾‹:
# Episode 50, Avg Reward: 68.45
# Episode 100, Avg Reward: 77.89
# Episode 150, Avg Reward: 82.34
# Episode 200, Avg Reward: 84.67
# Episode 250, Avg Reward: 85.89
# Episode 300, Avg Reward: 86.45
</code></pre>

            <div class="tip-box">
                <p><strong>ğŸ’¡ Actor-Criticã®åˆ©ç‚¹</strong></p>
                <ul>
                    <li><strong>ä½åˆ†æ•£</strong>: Criticã«ã‚ˆã‚‹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è£œæ­£ã§å­¦ç¿’å®‰å®š</li>
                    <li><strong>ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’</strong>: 1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«æ›´æ–°å¯èƒ½</li>
                    <li><strong>ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡</strong>: REINFORCEã‚ˆã‚Šå°‘ãªã„ã‚µãƒ³ãƒ—ãƒ«ã§å­¦ç¿’</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>5.5 Proximal Policy Optimizationï¼ˆPPOï¼‰</h2>

            <p>PPOã¯æ–¹ç­–ã®æ›´æ–°å¹…ã‚’åˆ¶é™ã™ã‚‹ã“ã¨ã§ã€å­¦ç¿’ã®å®‰å®šæ€§ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚ç¾åœ¨ã®æœ€å…ˆç«¯æ‰‹æ³•ã®ä¸€ã¤ã§ã™ã€‚</p>

            <h3>ä¾‹5: PPOã«ã‚ˆã‚‹é€£ç¶šåˆ¶å¾¡</h3>

            <pre><code class="language-python">class PPOAgent:
    """Proximal Policy Optimization Agent"""

    def __init__(self, state_dim, action_dim, lr=0.0003, gamma=0.99,
                 epsilon_clip=0.2, epochs=10, batch_size=64):
        self.actor_critic = ActorCriticNetwork(state_dim, action_dim)
        self.optimizer = torch.optim.Adam(self.actor_critic.parameters(), lr=lr)
        self.gamma = gamma
        self.epsilon_clip = epsilon_clip
        self.epochs = epochs
        self.batch_size = batch_size

        # çµŒé¨“ãƒãƒƒãƒ•ã‚¡
        self.states = []
        self.actions = []
        self.log_probs = []
        self.rewards = []
        self.dones = []
        self.values = []

    def choose_action(self, state):
        """è¡Œå‹•é¸æŠ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        action_probs, value = self.actor_critic(state_tensor)

        dist = torch.distributions.Categorical(action_probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)

        return action.item(), log_prob.detach(), value.detach()

    def store_transition(self, state, action, log_prob, reward, done, value):
        """çµŒé¨“ã‚’ä¿å­˜"""
        self.states.append(state)
        self.actions.append(action)
        self.log_probs.append(log_prob)
        self.rewards.append(reward)
        self.dones.append(done)
        self.values.append(value)

    def update(self):
        """PPOæ›´æ–°ï¼ˆãƒãƒƒãƒå­¦ç¿’ï¼‰"""
        # Advantageè¨ˆç®—
        returns = []
        advantages = []
        R = 0

        for i in reversed(range(len(self.rewards))):
            R = self.rewards[i] + self.gamma * R * (1 - self.dones[i])
            returns.insert(0, R)

        returns = torch.FloatTensor(returns)
        values = torch.stack(self.values).squeeze()

        advantages = returns - values
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # Tensorå¤‰æ›
        states = torch.FloatTensor(np.array(self.states))
        actions = torch.LongTensor(self.actions)
        old_log_probs = torch.stack(self.log_probs)

        # PPOæ›´æ–°ï¼ˆè¤‡æ•°ã‚¨ãƒãƒƒã‚¯ï¼‰
        for _ in range(self.epochs):
            # æ–°ã—ã„æ–¹ç­–ã§è©•ä¾¡
            action_probs, new_values = self.actor_critic(states)
            dist = torch.distributions.Categorical(action_probs)
            new_log_probs = dist.log_prob(actions)
            entropy = dist.entropy().mean()

            # Probability ratio
            ratio = torch.exp(new_log_probs - old_log_probs)

            # Clipped surrogate loss
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.epsilon_clip, 1 + self.epsilon_clip) * advantages
            actor_loss = -torch.min(surr1, surr2).mean()

            # Critic loss
            critic_loss = F.mse_loss(new_values.squeeze(), returns)

            # Total loss
            loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy

            # æ›´æ–°
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), 0.5)
            self.optimizer.step()

        # ãƒãƒƒãƒ•ã‚¡ã‚¯ãƒªã‚¢
        self.states.clear()
        self.actions.clear()
        self.log_probs.clear()
        self.rewards.clear()
        self.dones.clear()
        self.values.clear()

# PPOè¨“ç·´
env = ContinuousReactorEnv()
agent = PPOAgent(state_dim=4, action_dim=5, lr=0.0003)

n_episodes = 200
update_interval = 10  # 10ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ã«æ›´æ–°
episode_rewards = []

for episode in range(n_episodes):
    state = env.reset()
    total_reward = 0

    for step in range(100):
        action, log_prob, value = agent.choose_action(state)
        next_state, reward, done = env.step(action)

        agent.store_transition(state, action, log_prob, reward, done, value)

        total_reward += reward
        state = next_state

    episode_rewards.append(total_reward)

    # å®šæœŸçš„ã«æ›´æ–°
    if (episode + 1) % update_interval == 0:
        agent.update()

    if (episode + 1) % 50 == 0:
        avg_reward = np.mean(episode_rewards[-50:])
        print(f'Episode {episode+1}, Avg Reward: {avg_reward:.2f}')

# å‡ºåŠ›ä¾‹:
# Episode 50, Avg Reward: 74.56
# Episode 100, Avg Reward: 83.45
# Episode 150, Avg Reward: 86.78
# Episode 200, Avg Reward: 87.89
</code></pre>
        </section>

        <section>
            <h2>5.6 Deep Deterministic Policy Gradientï¼ˆDDPGï¼‰</h2>

            <p>DDPGã¯é€£ç¶šçš„ãªè¡Œå‹•ç©ºé–“ã«å¯¾å¿œã—ãŸæ‰‹æ³•ã§ã™ã€‚åå¿œå™¨ã®æ¸©åº¦åˆ¶å¾¡ãªã©ã€é€£ç¶šå€¤ã®æ“ä½œé‡ã‚’æœ€é©åŒ–ã§ãã¾ã™ã€‚</p>

            <h3>ä¾‹6: DDPGã«ã‚ˆã‚‹æ¸©åº¦åˆ¶å¾¡</h3>

            <pre><code class="language-python">class ContinuousActorNetwork(nn.Module):
    """é€£ç¶šè¡Œå‹•ã®ãŸã‚ã®Actor"""

    def __init__(self, state_dim, action_dim, hidden_dim=128, action_bound=1.0):
        super(ContinuousActorNetwork, self).__init__()
        self.action_bound = action_bound

        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, state):
        """
        Returns:
            action: [-action_bound, action_bound]ã®é€£ç¶šå€¤
        """
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        action = torch.tanh(self.fc3(x)) * self.action_bound
        return action

class ContinuousCriticNetwork(nn.Module):
    """Qå€¤é–¢æ•°ï¼ˆçŠ¶æ…‹-è¡Œå‹•ãƒšã‚¢ï¼‰"""

    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(ContinuousCriticNetwork, self).__init__()

        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)

    def forward(self, state, action):
        """
        Args:
            state: [batch, state_dim]
            action: [batch, action_dim]
        Returns:
            q_value: [batch, 1]
        """
        x = torch.cat([state, action], dim=1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        q_value = self.fc3(x)
        return q_value

class DDPGAgent:
    """Deep Deterministic Policy Gradient Agent"""

    def __init__(self, state_dim, action_dim, lr_actor=0.0001, lr_critic=0.001,
                 gamma=0.99, tau=0.001, action_bound=10.0):
        """
        Args:
            tau: soft updateã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            action_bound: è¡Œå‹•ã®æœ€å¤§å€¤ï¼ˆæ¸©åº¦å¤‰åŒ–ã®æœ€å¤§å¹… [K]ï¼‰
        """
        self.gamma = gamma
        self.tau = tau
        self.action_bound = action_bound

        # Actorï¼ˆãƒ¡ã‚¤ãƒ³ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰
        self.actor = ContinuousActorNetwork(state_dim, action_dim, action_bound=action_bound)
        self.actor_target = ContinuousActorNetwork(state_dim, action_dim, action_bound=action_bound)
        self.actor_target.load_state_dict(self.actor.state_dict())

        # Criticï¼ˆãƒ¡ã‚¤ãƒ³ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰
        self.critic = ContinuousCriticNetwork(state_dim, action_dim)
        self.critic_target = ContinuousCriticNetwork(state_dim, action_dim)
        self.critic_target.load_state_dict(self.critic.state_dict())

        # æœ€é©åŒ–
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr_critic)

        # çµŒé¨“å†ç”Ÿ
        self.replay_buffer = ReplayBuffer(capacity=100000)

        # Ornstein-Uhlenbeckãƒã‚¤ã‚ºï¼ˆæ¢ç´¢ç”¨ï¼‰
        self.noise_sigma = 2.0
        self.noise_theta = 0.15
        self.noise_mu = 0.0
        self.noise_state = 0.0

    def choose_action(self, state, add_noise=True):
        """è¡Œå‹•é¸æŠï¼ˆãƒã‚¤ã‚ºä»˜ãæ¢ç´¢ï¼‰"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)

        with torch.no_grad():
            action = self.actor(state_tensor).squeeze().numpy()

        if add_noise:
            # Ornstein-Uhlenbeckãƒã‚¤ã‚º
            self.noise_state += self.noise_theta * (self.noise_mu - self.noise_state) + \
                               self.noise_sigma * np.random.randn()
            action += self.noise_state

        action = np.clip(action, -self.action_bound, self.action_bound)
        return action

    def train(self, batch_size=64):
        """DDPGæ›´æ–°"""
        if len(self.replay_buffer) < batch_size:
            return 0.0, 0.0

        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)

        # Criticæ›´æ–°
        with torch.no_grad():
            next_actions = self.actor_target(next_states)
            target_q = self.critic_target(next_states, next_actions)
            target_q = rewards.unsqueeze(1) + self.gamma * target_q * (1 - dones.unsqueeze(1))

        current_q = self.critic(states, actions.unsqueeze(1))
        critic_loss = F.mse_loss(current_q, target_q)

        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        # Actoræ›´æ–°
        predicted_actions = self.actor(states)
        actor_loss = -self.critic(states, predicted_actions).mean()

        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        # Soft update
        self._soft_update(self.actor, self.actor_target)
        self._soft_update(self.critic, self.critic_target)

        return actor_loss.item(), critic_loss.item()

    def _soft_update(self, local_model, target_model):
        """Soft update: Î¸_target = Ï„*Î¸_local + (1-Ï„)*Î¸_target"""
        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):
            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)

# é€£ç¶šè¡Œå‹•ã®ç’°å¢ƒ
class ContinuousActionReactorEnv:
    """é€£ç¶šè¡Œå‹•ç©ºé–“ã®åå¿œå™¨"""

    def __init__(self):
        self.state_dim = 4
        self.action_dim = 1  # æ¸©åº¦å¤‰åŒ–é‡ [-10, +10] K
        self.reset()

    def reset(self):
        self.temperature = np.random.uniform(350, 450)
        self.pressure = 5.0
        self.concentration = 0.7
        self.flow_rate = 100.0
        return self._get_state()

    def _get_state(self):
        return np.array([
            (self.temperature - 400) / 100,
            (self.pressure - 5) / 2,
            (self.concentration - 0.7) / 0.2,
            (self.flow_rate - 100) / 20
        ], dtype=np.float32)

    def step(self, action):
        # é€£ç¶šçš„ãªæ¸©åº¦å¤‰åŒ–
        temp_change = float(action[0])  # [-10, +10] K

        self.temperature += temp_change - 0.1 * (self.temperature - 350)
        self.pressure = 5 + 0.01 * (self.temperature - 400)
        self.concentration = 0.8 - 0.0005 * abs(self.temperature - 420)

        self.temperature = np.clip(self.temperature, 300, 500)
        self.pressure = np.clip(self.pressure, 1, 10)
        self.concentration = np.clip(self.concentration, 0, 1)

        # å ±é…¬
        temp_reward = -abs(self.temperature - 420)
        conc_reward = 100 * self.concentration
        energy_cost = -0.5 * abs(temp_change)

        reward = temp_reward + conc_reward + energy_cost

        return self._get_state(), reward, False

# DDPGè¨“ç·´
env = ContinuousActionReactorEnv()
agent = DDPGAgent(state_dim=4, action_dim=1, action_bound=10.0)

n_episodes = 200
episode_rewards = []

for episode in range(n_episodes):
    state = env.reset()
    total_reward = 0

    for step in range(100):
        action = agent.choose_action(state, add_noise=True)
        next_state, reward, done = env.step(action)

        agent.replay_buffer.push(state, action[0], reward, next_state, done)

        actor_loss, critic_loss = agent.train(batch_size=64)

        total_reward += reward
        state = next_state

    episode_rewards.append(total_reward)

    # ãƒã‚¤ã‚ºæ¸›è¡°
    agent.noise_sigma *= 0.995

    if (episode + 1) % 50 == 0:
        avg_reward = np.mean(episode_rewards[-50:])
        print(f'Episode {episode+1}, Avg Reward: {avg_reward:.2f}, Noise: {agent.noise_sigma:.4f}')

print(f"\nFinal 10 episodes avg reward: {np.mean(episode_rewards[-10:]):.2f}")

# å‡ºåŠ›ä¾‹:
# Episode 50, Avg Reward: 72.34, Noise: 1.6098
# Episode 100, Avg Reward: 81.56, Noise: 1.2958
# Episode 150, Avg Reward: 85.89, Noise: 1.0431
# Episode 200, Avg Reward: 87.45, Noise: 0.8398
#
# Final 10 episodes avg reward: 88.12
</code></pre>

            <div class="success-box">
                <p><strong>âœ… DDPGã®åˆ©ç‚¹</strong></p>
                <ul>
                    <li><strong>é€£ç¶šåˆ¶å¾¡</strong>: æ¸©åº¦ãƒ»æµé‡ãªã©é€£ç¶šå€¤ã®æ“ä½œé‡ã‚’ç›´æ¥æœ€é©åŒ–</li>
                    <li><strong>æ±ºå®šè«–çš„æ–¹ç­–</strong>: åŒã˜çŠ¶æ…‹ã§åŒã˜è¡Œå‹•ï¼ˆå†ç¾æ€§ï¼‰</li>
                    <li><strong>ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼å­¦ç¿’</strong>: çµŒé¨“å†ç”Ÿã«ã‚ˆã‚‹ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>5.7 ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¼·åŒ–å­¦ç¿’</h2>

            <p>åˆ†æ•£å‹ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã§ã¯ã€è¤‡æ•°ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆå„åå¿œå™¨ã®åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼‰ãŒå”èª¿ã—ã¦æœ€é©åŒ–ã—ã¾ã™ã€‚</p>

            <h3>ä¾‹7: å”èª¿å‹ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¶å¾¡</h3>

            <pre><code class="language-python">class MultiAgentReactorEnv:
    """3ã¤ã®é€£çµåå¿œå™¨ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.n_agents = 3
        self.state_dim = 2  # å„åå¿œå™¨ã®æ¸©åº¦ãƒ»æ¿ƒåº¦
        self.action_dim = 3  # å†·å´/ç¶­æŒ/åŠ ç†±

        self.reset()

    def reset(self):
        # å„åå¿œå™¨ã®åˆæœŸçŠ¶æ…‹
        self.temperatures = np.random.uniform(350, 450, self.n_agents)
        self.concentrations = np.random.uniform(0.5, 0.9, self.n_agents)
        return self._get_states()

    def _get_states(self):
        """å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®çŠ¶æ…‹"""
        states = []
        for i in range(self.n_agents):
            state = np.array([
                (self.temperatures[i] - 400) / 100,
                (self.concentrations[i] - 0.7) / 0.2
            ], dtype=np.float32)
            states.append(state)
        return states

    def step(self, actions):
        """
        Args:
            actions: [n_agents] å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•

        Returns:
            states: æ¬¡çŠ¶æ…‹
            rewards: å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å ±é…¬
            done: çµ‚äº†ãƒ•ãƒ©ã‚°
        """
        temp_changes = [(a - 1) * 5 for a in actions]  # -5, 0, +5 K

        # å„åå¿œå™¨ã®æ›´æ–° + ç†±äº¤æ›
        for i in range(self.n_agents):
            # è‡ªèº«ã®åˆ¶å¾¡
            self.temperatures[i] += temp_changes[i]

            # éš£æ¥åå¿œå™¨ã¨ã®ç†±äº¤æ›
            if i > 0:
                heat_exchange = 0.1 * (self.temperatures[i-1] - self.temperatures[i])
                self.temperatures[i] += heat_exchange

            # åå¿œé€²è¡Œ
            self.concentrations[i] = 0.8 - 0.001 * abs(self.temperatures[i] - 420)

            # åˆ¶ç´„
            self.temperatures[i] = np.clip(self.temperatures[i], 300, 500)
            self.concentrations[i] = np.clip(self.concentrations[i], 0, 1)

        # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å ±é…¬
        rewards = []
        for i in range(self.n_agents):
            temp_reward = -abs(self.temperatures[i] - 420)
            conc_reward = 50 * self.concentrations[i]

            # å”èª¿ãƒœãƒ¼ãƒŠã‚¹: å…¨åå¿œå™¨ã®æ¿ƒåº¦ãŒé«˜ã„
            global_conc = np.mean(self.concentrations)
            cooperation_bonus = 20 * global_conc

            reward = temp_reward + conc_reward + cooperation_bonus
            rewards.append(reward)

        return self._get_states(), rewards, False

# ç‹¬ç«‹Q-Learningï¼ˆå„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç‹¬è‡ªã«å­¦ç¿’ï¼‰
class MultiAgentQLearning:
    """ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆQ-Learning"""

    def __init__(self, n_agents, state_dim, action_dim):
        self.n_agents = n_agents
        # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ã®DQN
        self.agents = [DQNAgent(state_dim, action_dim, lr=0.0005) for _ in range(n_agents)]

    def choose_actions(self, states):
        """å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•ã‚’é¸æŠ"""
        actions = []
        for i, state in enumerate(states):
            action = self.agents[i].choose_action(state)
            actions.append(action)
        return actions

    def train(self, states, actions, rewards, next_states):
        """å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ç‹¬ç«‹ã«è¨“ç·´"""
        losses = []
        for i in range(self.n_agents):
            # çµŒé¨“ã‚’ä¿å­˜
            self.agents[i].replay_buffer.push(
                states[i], actions[i], rewards[i], next_states[i], False
            )

            # è¨“ç·´
            loss = self.agents[i].train(batch_size=32)
            losses.append(loss)

        return np.mean(losses)

# ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´
env = MultiAgentReactorEnv()
ma_agent = MultiAgentQLearning(n_agents=3, state_dim=2, action_dim=3)

n_episodes = 300
episode_rewards = []

for episode in range(n_episodes):
    states = env.reset()
    total_rewards = np.zeros(3)

    for step in range(100):
        actions = ma_agent.choose_actions(states)
        next_states, rewards, done = env.step(actions)

        ma_agent.train(states, actions, rewards, next_states)

        total_rewards += np.array(rewards)
        states = next_states

    episode_rewards.append(total_rewards.sum())

    # Îµã¨Target Networkæ›´æ–°
    for agent in ma_agent.agents:
        agent.decay_epsilon()

    if (episode + 1) % 10 == 0:
        for agent in ma_agent.agents:
            agent.update_target_network()

    if (episode + 1) % 50 == 0:
        avg_reward = np.mean(episode_rewards[-50:])
        print(f'Episode {episode+1}, Avg Total Reward: {avg_reward:.2f}')

# ãƒ†ã‚¹ãƒˆ: å”èª¿å‹•ä½œã®ç¢ºèª
states = env.reset()
temps = [[], [], []]

for step in range(50):
    actions = ma_agent.choose_actions(states)
    states, rewards, _ = env.step(actions)

    for i in range(3):
        temps[i].append(env.temperatures[i])

# å¯è¦–åŒ–
plt.figure(figsize=(10, 4))
for i in range(3):
    plt.plot(temps[i], label=f'Reactor {i+1}')
plt.axhline(420, color='r', linestyle='--', label='Target')
plt.xlabel('Time Step')
plt.ylabel('Temperature [K]')
plt.title('Multi-Agent Coordinated Control')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()

print(f"\nFinal temperatures: {[temps[i][-1] for i in range(3)]}")
print(f"Final concentrations: {env.concentrations}")

# å‡ºåŠ›ä¾‹:
# Episode 50, Avg Total Reward: 567.89
# Episode 100, Avg Total Reward: 789.01
# Episode 150, Avg Total Reward: 876.54
# Episode 200, Avg Total Reward: 912.34
# Episode 250, Avg Total Reward: 928.76
# Episode 300, Avg Total Reward: 935.45
#
# Final temperatures: [418.34, 420.12, 419.87]
# Final concentrations: [0.797 0.798 0.799]
</code></pre>
        </section>

        <section>
            <h2>5.8 Safe RLï¼ˆå®‰å…¨åˆ¶ç´„ä»˜ãå¼·åŒ–å­¦ç¿’ï¼‰</h2>

            <p>ãƒ—ãƒ­ã‚»ã‚¹ç”£æ¥­ã§ã¯å®‰å…¨æ€§ãŒæœ€å„ªå…ˆã§ã™ã€‚åˆ¶ç´„æ¡ä»¶ï¼ˆæ¸©åº¦ä¸Šé™ã€åœ§åŠ›ç¯„å›²ï¼‰ã‚’æº€ãŸã—ãªãŒã‚‰æœ€é©åŒ–ã—ã¾ã™ã€‚</p>

            <h3>ä¾‹8: åˆ¶ç´„ä»˜ãPPOï¼ˆCPOæ¦‚å¿µï¼‰</h3>

            <pre><code class="language-python">class SafeReactorEnv:
    """å®‰å…¨åˆ¶ç´„ä»˜ãåå¿œå™¨ç’°å¢ƒ"""

    def __init__(self):
        self.state_dim = 4
        self.action_dim = 5

        # å®‰å…¨åˆ¶ç´„
        self.temp_min = 320  # K
        self.temp_max = 480  # K
        self.pressure_max = 8  # bar

        self.reset()

    def reset(self):
        self.temperature = np.random.uniform(350, 450)
        self.pressure = 5.0
        self.concentration = 0.7
        self.flow_rate = 100.0
        return self._get_state()

    def _get_state(self):
        return np.array([
            (self.temperature - 400) / 100,
            (self.pressure - 5) / 2,
            (self.concentration - 0.7) / 0.2,
            (self.flow_rate - 100) / 20
        ], dtype=np.float32)

    def step(self, action):
        temp_change = (action - 2) * 5

        # çŠ¶æ…‹æ›´æ–°
        self.temperature += temp_change - 0.1 * (self.temperature - 350)
        self.pressure = 5 + 0.02 * (self.temperature - 400)
        self.concentration = 0.8 - 0.0005 * abs(self.temperature - 420)

        # åˆ¶ç´„ãƒã‚§ãƒƒã‚¯ï¼ˆé•åå‰ã«åˆ¶é™ï¼‰
        self.temperature = np.clip(self.temperature, self.temp_min, self.temp_max)
        self.pressure = np.clip(self.pressure, 1, self.pressure_max)

        # å ±é…¬
        temp_reward = -abs(self.temperature - 420)
        conc_reward = 100 * self.concentration

        reward = temp_reward + conc_reward

        # åˆ¶ç´„ã‚³ã‚¹ãƒˆï¼ˆé•åæ™‚ã«å¤§ããªãƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰
        cost = 0.0
        if self.temperature < self.temp_min + 10 or self.temperature > self.temp_max - 10:
            cost = 100  # åˆ¶ç´„ãƒãƒ¼ã‚¸ãƒ³é•å
        if self.pressure > self.pressure_max - 1:
            cost += 100

        return self._get_state(), reward, cost, False

class SafePPOAgent:
    """å®‰å…¨åˆ¶ç´„ä»˜ãPPOï¼ˆç°¡æ˜“ç‰ˆï¼‰"""

    def __init__(self, state_dim, action_dim, lr=0.0003, cost_limit=20):
        """
        Args:
            cost_limit: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å½“ãŸã‚Šã®è¨±å®¹ã‚³ã‚¹ãƒˆä¸Šé™
        """
        self.actor_critic = ActorCriticNetwork(state_dim, action_dim)
        self.optimizer = torch.optim.Adam(self.actor_critic.parameters(), lr=lr)
        self.cost_limit = cost_limit

        # ã‚³ã‚¹ãƒˆç”¨Critic
        self.cost_critic = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
        self.cost_optimizer = torch.optim.Adam(self.cost_critic.parameters(), lr=lr)

        # ãƒãƒƒãƒ•ã‚¡
        self.states = []
        self.actions = []
        self.log_probs = []
        self.rewards = []
        self.costs = []
        self.values = []

    def choose_action(self, state):
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        action_probs, value = self.actor_critic(state_tensor)

        dist = torch.distributions.Categorical(action_probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)

        return action.item(), log_prob.detach(), value.detach()

    def store_transition(self, state, action, log_prob, reward, cost, value):
        self.states.append(state)
        self.actions.append(action)
        self.log_probs.append(log_prob)
        self.rewards.append(reward)
        self.costs.append(cost)
        self.values.append(value)

    def update(self):
        """å®‰å…¨åˆ¶ç´„ã‚’è€ƒæ…®ã—ãŸæ›´æ–°"""
        # Advantageè¨ˆç®—
        returns = []
        cost_returns = []
        R = 0
        C = 0

        for i in reversed(range(len(self.rewards))):
            R = self.rewards[i] + 0.99 * R
            C = self.costs[i] + 0.99 * C
            returns.insert(0, R)
            cost_returns.insert(0, C)

        returns = torch.FloatTensor(returns)
        cost_returns = torch.FloatTensor(cost_returns)
        values = torch.stack(self.values).squeeze()

        advantages = returns - values
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # ã‚³ã‚¹ãƒˆåˆ¶ç´„ãƒã‚§ãƒƒã‚¯
        total_cost = sum(self.costs)

        states = torch.FloatTensor(np.array(self.states))
        actions = torch.LongTensor(self.actions)
        old_log_probs = torch.stack(self.log_probs)

        # é€šå¸¸ã®PPOæ›´æ–°ï¼ˆãŸã ã—ã‚³ã‚¹ãƒˆãŒä¸Šé™ã‚’è¶…ãˆã¦ã„ãŸã‚‰å­¦ç¿’ç‡ã‚’ä¸‹ã’ã‚‹ï¼‰
        action_probs, new_values = self.actor_critic(states)
        dist = torch.distributions.Categorical(action_probs)
        new_log_probs = dist.log_prob(actions)

        ratio = torch.exp(new_log_probs - old_log_probs)

        # ã‚³ã‚¹ãƒˆåˆ¶ç´„é•åæ™‚ã¯æ›´æ–°ã‚’æŠ‘åˆ¶
        if total_cost > self.cost_limit:
            penalty_factor = 0.1  # å­¦ç¿’ã‚’é…ãã™ã‚‹
            advantages = advantages * penalty_factor

        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 0.8, 1.2) * advantages
        actor_loss = -torch.min(surr1, surr2).mean()

        critic_loss = F.mse_loss(new_values.squeeze(), returns)

        loss = actor_loss + 0.5 * critic_loss

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # ã‚¯ãƒªã‚¢
        self.states.clear()
        self.actions.clear()
        self.log_probs.clear()
        self.rewards.clear()
        self.costs.clear()
        self.values.clear()

        return total_cost

# Safe RLè¨“ç·´
env = SafeReactorEnv()
agent = SafePPOAgent(state_dim=4, action_dim=5, cost_limit=50)

n_episodes = 200
episode_rewards = []
episode_costs = []

for episode in range(n_episodes):
    state = env.reset()
    total_reward = 0

    for step in range(100):
        action, log_prob, value = agent.choose_action(state)
        next_state, reward, cost, done = env.step(action)

        agent.store_transition(state, action, log_prob, reward, cost, value)

        total_reward += reward
        state = next_state

    # æ›´æ–°
    if (episode + 1) % 10 == 0:
        total_cost = agent.update()
        episode_costs.append(total_cost)

    episode_rewards.append(total_reward)

    if (episode + 1) % 50 == 0:
        avg_reward = np.mean(episode_rewards[-50:])
        avg_cost = np.mean(episode_costs[-5:]) if episode_costs else 0
        print(f'Episode {episode+1}, Avg Reward: {avg_reward:.2f}, Avg Cost: {avg_cost:.2f}')

print(f"\nSafety violations (cost > {agent.cost_limit}): {sum(c > agent.cost_limit for c in episode_costs)}")

# å‡ºåŠ›ä¾‹:
# Episode 50, Avg Reward: 78.45, Avg Cost: 67.89
# Episode 100, Avg Reward: 83.56, Avg Cost: 42.34
# Episode 150, Avg Reward: 85.67, Avg Cost: 28.76
# Episode 200, Avg Reward: 86.89, Avg Cost: 18.45
#
# Safety violations (cost > 50): 4
</code></pre>

            <div class="warning-box">
                <p><strong>âš ï¸ ç”£æ¥­å®Ÿè£…æ™‚ã®æ³¨æ„ç‚¹</strong></p>
                <ul>
                    <li><strong>ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¤œè¨¼</strong>: å®Ÿãƒ—ãƒ­ã‚»ã‚¹ã«é©ç”¨å‰ã«ååˆ†ãªæ¤œè¨¼</li>
                    <li><strong>ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•</strong>: RLãŒå¤±æ•—æ™‚ã®å¤å…¸åˆ¶å¾¡ã¸ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯</li>
                    <li><strong>æ®µéšçš„å°å…¥</strong>: ã¾ãšã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã€æ¬¡ã«æœ€é©åŒ–ã€æœ€å¾Œã«åˆ¶å¾¡</li>
                    <li><strong>äººé–“ã®ç›£è¦–</strong>: å®Œå…¨è‡ªå‹•åŒ–å‰ã«äººé–“ãŒç›£è¦–ãƒ»ä»‹å…¥å¯èƒ½ã«</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>å­¦ç¿’ç›®æ¨™ã®ç¢ºèª</h2>

            <p>ã“ã®ç« ã‚’å®Œäº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ã‚’å®Ÿè£…ãƒ»èª¬æ˜ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š</p>

            <h3>åŸºæœ¬ç†è§£</h3>
            <ul>
                <li>å¼·åŒ–å­¦ç¿’ã®MDPï¼ˆãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼‰ã¨Bellmanæ–¹ç¨‹å¼ã‚’èª¬æ˜ã§ãã‚‹</li>
                <li>Q-Learningã€Policy Gradientã€Actor-Criticã®é•ã„ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
                <li>Experience Replayã¨Target Networkã®å½¹å‰²ã‚’èª¬æ˜ã§ãã‚‹</li>
                <li>é€£ç¶šè¡Œå‹•ç©ºé–“ã¨é›¢æ•£è¡Œå‹•ç©ºé–“ã®é•ã„ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
            </ul>

            <h3>å®Ÿè·µã‚¹ã‚­ãƒ«</h3>
            <ul>
                <li>Q-Learningã§ç°¡æ˜“ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã‚’å®Ÿè£…ã§ãã‚‹</li>
                <li>DQNã§é«˜æ¬¡å…ƒçŠ¶æ…‹ç©ºé–“ã®åˆ¶å¾¡ã‚’å®Ÿè£…ã§ãã‚‹</li>
                <li>REINFORCEã¨A2Cã§æ–¹ç­–å‹¾é…æ³•ã‚’å®Ÿè£…ã§ãã‚‹</li>
                <li>PPOã§å®‰å®šã—ãŸå­¦ç¿’ã‚’å®Ÿç¾ã§ãã‚‹</li>
                <li>DDPGã§é€£ç¶šåˆ¶å¾¡ï¼ˆæ¸©åº¦ãƒ»æµé‡ï¼‰ã‚’æœ€é©åŒ–ã§ãã‚‹</li>
                <li>ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆRLã§åˆ†æ•£åˆ¶å¾¡ã‚’å®Ÿè£…ã§ãã‚‹</li>
                <li>åˆ¶ç´„ä»˜ãRLã§å®‰å…¨æ€§ã‚’è€ƒæ…®ã—ãŸåˆ¶å¾¡ãŒã§ãã‚‹</li>
            </ul>

            <h3>å¿œç”¨åŠ›</h3>
            <ul>
                <li>ãƒ—ãƒ­ã‚»ã‚¹ã®ç‰¹æ€§ã«å¿œã˜ã¦é©åˆ‡ãªRLæ‰‹æ³•ã‚’é¸æŠã§ãã‚‹</li>
                <li>å ±é…¬é–¢æ•°ã‚’è¨­è¨ˆã—ã¦ãƒ—ãƒ­ã‚»ã‚¹ç›®æ¨™ã‚’å®šå¼åŒ–ã§ãã‚‹</li>
                <li>å®‰å…¨åˆ¶ç´„ã‚’æº€ãŸã—ãªãŒã‚‰æ€§èƒ½ã‚’æœ€é©åŒ–ã§ãã‚‹</li>
                <li>ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰å®Ÿãƒ—ãƒ­ã‚»ã‚¹ã¸ã®é©ç”¨æˆ¦ç•¥ã‚’ç«‹ã¦ã‚‰ã‚Œã‚‹</li>
            </ul>
        </section>

        <section>
            <h2>RLæ‰‹æ³•æ¯”è¼ƒè¡¨</h2>

            <table>
                <thead>
                    <tr>
                        <th>æ‰‹æ³•</th>
                        <th>è¡Œå‹•ç©ºé–“</th>
                        <th>å­¦ç¿’æ–¹å¼</th>
                        <th>ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡</th>
                        <th>å®‰å®šæ€§</th>
                        <th>é©ç”¨ä¾‹</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Q-Learning</strong></td>
                        <td>é›¢æ•£</td>
                        <td>ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼</td>
                        <td>é«˜</td>
                        <td>ä¸­</td>
                        <td>ç°¡æ˜“åå¿œå™¨åˆ¶å¾¡</td>
                    </tr>
                    <tr>
                        <td><strong>DQN</strong></td>
                        <td>é›¢æ•£</td>
                        <td>ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼</td>
                        <td>é«˜</td>
                        <td>ä¸­</td>
                        <td>å¤šå¤‰æ•°ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡</td>
                    </tr>
                    <tr>
                        <td><strong>REINFORCE</strong></td>
                        <td>é›¢æ•£/é€£ç¶š</td>
                        <td>ã‚ªãƒ³ãƒãƒªã‚·ãƒ¼</td>
                        <td>ä½</td>
                        <td>ä½</td>
                        <td>æ¢ç´¢çš„åˆ¶å¾¡</td>
                    </tr>
                    <tr>
                        <td><strong>A2C</strong></td>
                        <td>é›¢æ•£/é€£ç¶š</td>
                        <td>ã‚ªãƒ³ãƒãƒªã‚·ãƒ¼</td>
                        <td>ä¸­</td>
                        <td>ä¸­</td>
                        <td>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ¶å¾¡</td>
                    </tr>
                    <tr>
                        <td><strong>PPO</strong></td>
                        <td>é›¢æ•£/é€£ç¶š</td>
                        <td>ã‚ªãƒ³ãƒãƒªã‚·ãƒ¼</td>
                        <td>ä¸­</td>
                        <td>é«˜</td>
                        <td>å®‰å®šã—ãŸæœ€é©åŒ–</td>
                    </tr>
                    <tr>
                        <td><strong>DDPG</strong></td>
                        <td>é€£ç¶š</td>
                        <td>ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼</td>
                        <td>é«˜</td>
                        <td>ä¸­</td>
                        <td>æ¸©åº¦ãƒ»æµé‡åˆ¶å¾¡</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section>
            <h2>å‚è€ƒæ–‡çŒ®</h2>
            <ol>
                <li>Sutton, R. S., & Barto, A. G. (2018). "Reinforcement Learning: An Introduction" (2nd ed.). MIT Press.</li>
                <li>Mnih, V., et al. (2015). "Human-level control through deep reinforcement learning." Nature, 518(7540), 529-533.</li>
                <li>Lillicrap, T. P., et al. (2016). "Continuous control with deep reinforcement learning." ICLR 2016.</li>
                <li>Schulman, J., et al. (2017). "Proximal Policy Optimization Algorithms." arXiv:1707.06347.</li>
                <li>Achiam, J., et al. (2017). "Constrained Policy Optimization." ICML 2017.</li>
                <li>Lee, J. H., et al. (2021). "Approximate Dynamic Programming-based Approaches for Process Control." Computers & Chemical Engineering, 147, 107229.</li>
            </ol>
        </section>

        <div class="navigation">
            <a href="chapter-4.html" class="nav-button">â† ç¬¬4ç« ï¼šã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€</a>
            <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã¸</a>
        </div>
    </main>
</body>
</html>
