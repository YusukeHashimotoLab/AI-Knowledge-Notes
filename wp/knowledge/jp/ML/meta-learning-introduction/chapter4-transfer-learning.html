<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬4ç« ï¼šè»¢ç§»å­¦ç¿’ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: var(--font-body); line-height: 1.7; color: var(--color-text); background-color: var(--color-bg); font-size: 16px; }
        header { background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; padding: var(--spacing-xl) var(--spacing-md); margin-bottom: var(--spacing-xl); box-shadow: var(--box-shadow); }
        .header-content { max-width: 900px; margin: 0 auto; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: var(--spacing-sm); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; opacity: 0.95; font-weight: 400; margin-bottom: var(--spacing-md); }
        .meta { display: flex; flex-wrap: wrap; gap: var(--spacing-md); font-size: 0.9rem; opacity: 0.9; }
        .meta-item { display: flex; align-items: center; gap: 0.3rem; }
        .container { max-width: 900px; margin: 0 auto; padding: 0 var(--spacing-md) var(--spacing-xl); }
        h2 { font-size: 1.75rem; color: var(--color-primary); margin-top: var(--spacing-xl); margin-bottom: var(--spacing-md); padding-bottom: var(--spacing-xs); border-bottom: 3px solid var(--color-accent); }
        h3 { font-size: 1.4rem; color: var(--color-primary); margin-top: var(--spacing-lg); margin-bottom: var(--spacing-sm); }
        h4 { font-size: 1.1rem; color: var(--color-primary-dark); margin-top: var(--spacing-md); margin-bottom: var(--spacing-sm); }
        p { margin-bottom: var(--spacing-md); color: var(--color-text); }
        a { color: var(--color-link); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--color-link-hover); text-decoration: underline; }
        ul, ol { margin-left: var(--spacing-lg); margin-bottom: var(--spacing-md); }
        li { margin-bottom: var(--spacing-xs); color: var(--color-text); }
        pre { background-color: var(--color-code-bg); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); overflow-x: auto; margin-bottom: var(--spacing-md); font-family: var(--font-mono); font-size: 0.9rem; line-height: 1.5; }
        code { font-family: var(--font-mono); font-size: 0.9em; background-color: var(--color-code-bg); padding: 0.2em 0.4em; border-radius: 3px; }
        pre code { background-color: transparent; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: var(--spacing-md); font-size: 0.95rem; }
        th, td { border: 1px solid var(--color-border); padding: var(--spacing-sm); text-align: left; }
        th { background-color: var(--color-bg-alt); font-weight: 600; color: var(--color-primary); }
        blockquote { border-left: 4px solid var(--color-accent); padding-left: var(--spacing-md); margin: var(--spacing-md) 0; color: var(--color-text-light); font-style: italic; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        .mermaid { text-align: center; margin: var(--spacing-lg) 0; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        details { background-color: var(--color-bg-alt); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); margin-bottom: var(--spacing-md); }
        summary { cursor: pointer; font-weight: 600; color: var(--color-primary); user-select: none; padding: var(--spacing-xs); margin: calc(-1 * var(--spacing-md)); padding: var(--spacing-md); border-radius: var(--border-radius); }
        summary:hover { background-color: rgba(123, 44, 191, 0.1); }
        details[open] summary { margin-bottom: var(--spacing-md); border-bottom: 1px solid var(--color-border); }
        .navigation { display: flex; justify-content: space-between; gap: var(--spacing-md); margin: var(--spacing-xl) 0; padding-top: var(--spacing-lg); border-top: 2px solid var(--color-border); }
        .nav-button { flex: 1; padding: var(--spacing-md); background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; border-radius: var(--border-radius); text-align: center; font-weight: 600; transition: transform 0.2s, box-shadow 0.2s; box-shadow: var(--box-shadow); }
        .nav-button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); text-decoration: none; }
        footer { margin-top: var(--spacing-xl); padding: var(--spacing-lg) var(--spacing-md); background-color: var(--color-bg-alt); border-top: 1px solid var(--color-border); text-align: center; font-size: 0.9rem; color: var(--color-text-light); }
        .project-box { background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 50%); border-radius: var(--border-radius); padding: var(--spacing-lg); margin: var(--spacing-lg) 0; box-shadow: var(--box-shadow); }
        @media (max-width: 768px) { h1 { font-size: 1.5rem; } h2 { font-size: 1.4rem; } h3 { font-size: 1.2rem; } .meta { font-size: 0.85rem; } .navigation { flex-direction: column; } table { font-size: 0.85rem; } th, td { padding: var(--spacing-xs); } }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬4ç« ï¼šè»¢ç§»å­¦ç¿’</h1>
            <p class="subtitle">äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ãŸåŠ¹ç‡çš„ãªå­¦ç¿’ã¨ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œæŠ€è¡“</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… è»¢ç§»å­¦ç¿’ã®åŸºç¤æ¦‚å¿µã¨äº‹å‰å­¦ç¿’ã®åŠ¹æœã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã‚’å®Ÿè£…ã—ã€æœ€é©åŒ–ã§ãã‚‹</li>
<li>âœ… Domain Adaptationã«ã‚ˆã‚‹åˆ†å¸ƒã‚·ãƒ•ãƒˆå•é¡Œã‚’è§£æ±ºã§ãã‚‹</li>
<li>âœ… çŸ¥è­˜è’¸ç•™ã«ã‚ˆã‚‹è»½é‡åŒ–ã¨ãƒ¡ã‚¿å­¦ç¿’ã®çµ„ã¿åˆã‚ã›ãŒã§ãã‚‹</li>
<li>âœ… å®Ÿè·µçš„ãªè»¢ç§»å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹ç¯‰ã§ãã‚‹</li>
</ul>

<h2>1. è»¢ç§»å­¦ç¿’ã®åŸºç¤</h2>

<h3>1.1 è»¢ç§»å­¦ç¿’ã¨ã¯</h3>
<p>è»¢ç§»å­¦ç¿’ã¯ã€ã‚ã‚‹ã‚¿ã‚¹ã‚¯ï¼ˆã‚½ãƒ¼ã‚¹ã‚¿ã‚¹ã‚¯ï¼‰ã§å­¦ç¿’ã—ãŸçŸ¥è­˜ã‚’åˆ¥ã®ã‚¿ã‚¹ã‚¯ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¿ã‚¹ã‚¯ï¼‰ã«æ´»ç”¨ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’ã®æ‰‹æ³•ã§ã™ã€‚ãƒ¡ã‚¿å­¦ç¿’ãŒã€Œå­¦ç¿’ã®å­¦ç¿’ã€ã‚’ç›®æŒ‡ã™ã®ã«å¯¾ã—ã€è»¢ç§»å­¦ç¿’ã¯ã€ŒçŸ¥è­˜ã®å†åˆ©ç”¨ã€ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[Source Domain<br/>ImageNet] -->|äº‹å‰å­¦ç¿’| B[Pre-trained Model]
    B -->|è»¢ç§»| C[Target Domain<br/>Medical Images]
    C -->|ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°| D[Specialized Model]
    style A fill:#e3f2fd
    style D fill:#c8e6c9
</div>

<h3>1.2 è»¢ç§»å­¦ç¿’ã®ç¨®é¡</h3>

<table>
<thead>
<tr>
<th>è»¢ç§»ã‚¿ã‚¤ãƒ—</th>
<th>èª¬æ˜</th>
<th>å…·ä½“ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ‰ãƒ¡ã‚¤ãƒ³è»¢ç§»</strong></td>
<td>ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒé–“ã§ã®è»¢ç§»</td>
<td>è‡ªç„¶ç”»åƒ â†’ åŒ»ç™‚ç”»åƒ</td>
</tr>
<tr>
<td><strong>ã‚¿ã‚¹ã‚¯è»¢ç§»</strong></td>
<td>ç•°ãªã‚‹ã‚¿ã‚¹ã‚¯é–“ã§ã®è»¢ç§»</td>
<td>åˆ†é¡ â†’ ç‰©ä½“æ¤œå‡º</td>
</tr>
<tr>
<td><strong>ãƒ¢ãƒ‡ãƒ«è»¢ç§»</strong></td>
<td>ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®å†åˆ©ç”¨</td>
<td>ResNet â†’ ã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</td>
</tr>
</tbody>
</table>

<h3>1.3 è»¢ç§»å¯èƒ½æ€§ã®è©•ä¾¡</h3>
<p>è»¢ç§»å­¦ç¿’ã®æˆåŠŸã¯ã€ã‚½ãƒ¼ã‚¹ã‚¿ã‚¹ã‚¯ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¿ã‚¹ã‚¯ã®é–¢é€£æ€§ã«ä¾å­˜ã—ã¾ã™ï¼š</p>

<pre><code class="language-python">import torch
import torch.nn as nn
from torchvision import models
from scipy.stats import spearmanr

def compute_transferability_score(source_features, target_features):
    """
    è»¢ç§»å¯èƒ½æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—

    Args:
        source_features: ã‚½ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ç‰¹å¾´é‡
        target_features: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³ã®ç‰¹å¾´é‡

    Returns:
        transferability_score: è»¢ç§»å¯èƒ½æ€§ã‚¹ã‚³ã‚¢
    """
    # ç‰¹å¾´é‡ã®ç›¸é–¢ä¿‚æ•°ã‚’è¨ˆç®—
    correlation, _ = spearmanr(
        source_features.flatten(),
        target_features.flatten()
    )

    # ç‰¹å¾´é‡åˆ†å¸ƒã®è·é›¢ï¼ˆMMDï¼‰ã‚’è¨ˆç®—
    def compute_mmd(x, y):
        xx = torch.mm(x, x.t())
        yy = torch.mm(y, y.t())
        xy = torch.mm(x, y.t())
        return xx.mean() + yy.mean() - 2 * xy.mean()

    mmd_distance = compute_mmd(
        torch.tensor(source_features),
        torch.tensor(target_features)
    )

    # ç·åˆã‚¹ã‚³ã‚¢ï¼ˆé«˜ã„ã»ã©è»¢ç§»ã«é©ã—ã¦ã„ã‚‹ï¼‰
    transferability_score = correlation - 0.1 * mmd_distance.item()

    return transferability_score

# ä½¿ç”¨ä¾‹
source_feats = torch.randn(100, 512)
target_feats = torch.randn(100, 512)
score = compute_transferability_score(source_feats, target_feats)
print(f"Transferability Score: {score:.4f}")
</code></pre>

<h2>2. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥</h2>

<h3>2.1 å…¨å±¤ vs éƒ¨åˆ†å±¤ã®æ›´æ–°</h3>
<p>äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ã©ã®å±¤ã‚’æ›´æ–°ã™ã‚‹ã‹ã¯ã€ãƒ‡ãƒ¼ã‚¿é‡ã¨ã‚¿ã‚¹ã‚¯ã®é¡ä¼¼æ€§ã«ã‚ˆã£ã¦æ±ºå®šã—ã¾ã™ï¼š</p>

<pre><code class="language-python">import torch.nn as nn
from torchvision import models

class TransferLearningModel(nn.Module):
    def __init__(self, num_classes, freeze_strategy='partial'):
        """
        è»¢ç§»å­¦ç¿’ãƒ¢ãƒ‡ãƒ«

        Args:
            num_classes: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¿ã‚¹ã‚¯ã®ã‚¯ãƒ©ã‚¹æ•°
            freeze_strategy: 'all', 'partial', 'none'
        """
        super().__init__()
        # ResNet50äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        self.backbone = models.resnet50(pretrained=True)

        # å‡çµæˆ¦ç•¥ã®é©ç”¨
        if freeze_strategy == 'all':
            # å…¨å±¤ã‚’å‡çµï¼ˆåˆ†é¡å±¤ä»¥å¤–ï¼‰
            for param in self.backbone.parameters():
                param.requires_grad = False

        elif freeze_strategy == 'partial':
            # åˆæœŸå±¤ã®ã¿å‡çµï¼ˆç‰¹å¾´æŠ½å‡ºå™¨ã¨ã—ã¦ä½¿ç”¨ï¼‰
            for name, param in self.backbone.named_parameters():
                if 'layer4' not in name and 'fc' not in name:
                    param.requires_grad = False

        # åˆ†é¡å±¤ã‚’ç½®ãæ›ãˆ
        num_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(num_features, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        return self.backbone(x)

# æˆ¦ç•¥åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ä½œæˆ
model_frozen = TransferLearningModel(num_classes=10, freeze_strategy='all')
model_partial = TransferLearningModel(num_classes=10, freeze_strategy='partial')
model_full = TransferLearningModel(num_classes=10, freeze_strategy='none')

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®ç¢ºèª
for name, model in [('Frozen', model_frozen), ('Partial', model_partial), ('Full', model_full)]:
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    print(f"{name}: {trainable:,} / {total:,} parameters trainable")
</code></pre>

<h3>2.2 Discriminative Fine-Tuning</h3>
<p>å±¤ã”ã¨ã«ç•°ãªã‚‹å­¦ç¿’ç‡ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚ŠåŠ¹æœçš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¯èƒ½ã§ã™ï¼š</p>

<pre><code class="language-python">def get_discriminative_params(model, base_lr=1e-4, multiplier=2.6):
    """
    å±¤ã”ã¨ã«ç•°ãªã‚‹å­¦ç¿’ç‡ã‚’è¨­å®š

    Args:
        model: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«
        base_lr: æœ€çµ‚å±¤ã®å­¦ç¿’ç‡
        multiplier: å±¤é–“ã®å­¦ç¿’ç‡å€ç‡

    Returns:
        param_groups: å±¤åˆ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒ«ãƒ¼ãƒ—
    """
    param_groups = []

    # ResNetã®å±¤ã‚°ãƒ«ãƒ¼ãƒ—
    layer_groups = [
        ('layer1', model.backbone.layer1),
        ('layer2', model.backbone.layer2),
        ('layer3', model.backbone.layer3),
        ('layer4', model.backbone.layer4),
        ('fc', model.backbone.fc)
    ]

    # å±¤ã”ã¨ã«å­¦ç¿’ç‡ã‚’è¨­å®šï¼ˆæ·±ã„å±¤ã»ã©é«˜ã„å­¦ç¿’ç‡ï¼‰
    for i, (name, layer) in enumerate(layer_groups):
        lr = base_lr / (multiplier ** (len(layer_groups) - i - 1))
        param_groups.append({
            'params': layer.parameters(),
            'lr': lr,
            'name': name
        })
        print(f"{name}: lr = {lr:.2e}")

    return param_groups

# Discriminative Fine-Tuningã®é©ç”¨
model = TransferLearningModel(num_classes=10, freeze_strategy='none')
param_groups = get_discriminative_params(model, base_lr=1e-3)
optimizer = torch.optim.Adam(param_groups)

# å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
    optimizer, T_0=10, T_mult=2
)
</code></pre>

<h2>3. Domain Adaptation</h2>

<h3>3.1 Domain Shiftã®å•é¡Œ</h3>
<p>ã‚½ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³ã®åˆ†å¸ƒãŒç•°ãªã‚‹å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ãŒä½ä¸‹ã—ã¾ã™ã€‚Domain Adaptationã¯ã€ã“ã®åˆ†å¸ƒã‚·ãƒ•ãƒˆã‚’ç·©å’Œã™ã‚‹æŠ€è¡“ã§ã™ã€‚</p>

<div class="mermaid">
graph TB
    A[Source Domain<br/>P_s(X,Y)] -->|Training Data| B[Model]
    C[Target Domain<br/>P_t(X,Y)] -.->|No Labels| B
    B -->|Adapt| D[Domain-Invariant<br/>Features]
    D -->|Predict| E[Target Predictions]
    style A fill:#e3f2fd
    style C fill:#fff3e0
    style D fill:#c8e6c9
</div>

<h3>3.2 Domain Adversarial Neural Networks (DANN)</h3>
<p>DANNã¯ã€ç‰¹å¾´æŠ½å‡ºå™¨ãŒãƒ‰ãƒ¡ã‚¤ãƒ³ã«ä¸å¤‰ãªè¡¨ç¾ã‚’å­¦ç¿’ã™ã‚‹ã‚ˆã†ã€æ•µå¯¾çš„è¨“ç·´ã‚’ä½¿ç”¨ã—ã¾ã™ï¼š</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class GradientReversalLayer(torch.autograd.Function):
    """å‹¾é…åè»¢å±¤ï¼ˆDANNç”¨ï¼‰"""
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        return -ctx.alpha * grad_output, None

class DANN(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()

        # ç‰¹å¾´æŠ½å‡ºå™¨ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³ä¸å¤‰ï¼‰
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(3, 64, 5), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 5), nn.ReLU(), nn.MaxPool2d(2),
            nn.Flatten(),
            nn.Linear(128 * 5 * 5, 1024), nn.ReLU(),
            nn.Dropout(0.5)
        )

        # ã‚¯ãƒ©ã‚¹åˆ†é¡å™¨
        self.class_classifier = nn.Sequential(
            nn.Linear(1024, 256), nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )

        # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†é¡å™¨
        self.domain_classifier = nn.Sequential(
            nn.Linear(1024, 256), nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, 2)  # Source vs Target
        )

    def forward(self, x, alpha=1.0):
        features = self.feature_extractor(x)

        # ã‚¯ãƒ©ã‚¹äºˆæ¸¬
        class_output = self.class_classifier(features)

        # ãƒ‰ãƒ¡ã‚¤ãƒ³äºˆæ¸¬ï¼ˆå‹¾é…åè»¢ï¼‰
        reversed_features = GradientReversalLayer.apply(features, alpha)
        domain_output = self.domain_classifier(reversed_features)

        return class_output, domain_output

# DANNã®è¨“ç·´
def train_dann(model, source_loader, target_loader, epochs=50):
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    for epoch in range(epochs):
        model.train()
        # ã‚¢ãƒ«ãƒ•ã‚¡å€¤ã‚’å¾ã€…ã«å¢—åŠ ï¼ˆå‹¾é…åè»¢ã®å¼·åº¦ï¼‰
        alpha = 2 / (1 + np.exp(-10 * epoch / epochs)) - 1

        for (source_data, source_labels), (target_data, _) in zip(source_loader, target_loader):
            # ã‚½ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ã®æå¤±
            source_class, source_domain = model(source_data, alpha)
            class_loss = F.cross_entropy(source_class, source_labels)
            source_domain_loss = F.cross_entropy(
                source_domain,
                torch.zeros(len(source_data), dtype=torch.long)
            )

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³ã®æå¤±
            _, target_domain = model(target_data, alpha)
            target_domain_loss = F.cross_entropy(
                target_domain,
                torch.ones(len(target_data), dtype=torch.long)
            )

            # ç·åˆæå¤±
            loss = class_loss + source_domain_loss + target_domain_loss

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}: Loss = {loss.item():.4f}, Alpha = {alpha:.4f}")

# ä½¿ç”¨ä¾‹
model = DANN(num_classes=10)
print("DANN model created with gradient reversal layer")
</code></pre>

<h3>3.3 Maximum Mean Discrepancy (MMD)</h3>
<p>MMDã¯ã€ã‚½ãƒ¼ã‚¹ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†å¸ƒé–“ã®è·é›¢ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã§ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œã‚’å®Ÿç¾ã—ã¾ã™ï¼š</p>

<pre><code class="language-python">def compute_mmd_loss(source_features, target_features, kernel='rbf'):
    """
    Maximum Mean Discrepancyæå¤±ã®è¨ˆç®—

    Args:
        source_features: ã‚½ãƒ¼ã‚¹ç‰¹å¾´é‡ (N_s, D)
        target_features: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹å¾´é‡ (N_t, D)
        kernel: ã‚«ãƒ¼ãƒãƒ«ã‚¿ã‚¤ãƒ— ('rbf', 'linear')

    Returns:
        mmd_loss: MMDæå¤±
    """
    def gaussian_kernel(x, y, sigma=1.0):
        x_size = x.size(0)
        y_size = y.size(0)
        dim = x.size(1)

        x = x.unsqueeze(1)  # (N_s, 1, D)
        y = y.unsqueeze(0)  # (1, N_t, D)

        diff = x - y  # (N_s, N_t, D)
        dist_sq = torch.sum(diff ** 2, dim=2)  # (N_s, N_t)

        return torch.exp(-dist_sq / (2 * sigma ** 2))

    if kernel == 'rbf':
        # RBFã‚«ãƒ¼ãƒãƒ«
        xx = gaussian_kernel(source_features, source_features).mean()
        yy = gaussian_kernel(target_features, target_features).mean()
        xy = gaussian_kernel(source_features, target_features).mean()
    else:
        # ç·šå½¢ã‚«ãƒ¼ãƒãƒ«
        xx = torch.mm(source_features, source_features.t()).mean()
        yy = torch.mm(target_features, target_features.t()).mean()
        xy = torch.mm(source_features, target_features.t()).mean()

    mmd_loss = xx + yy - 2 * xy
    return mmd_loss

class MMDNet(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.feature_extractor = nn.Sequential(
            nn.Linear(784, 512), nn.ReLU(), nn.Dropout(0.5),
            nn.Linear(512, 256), nn.ReLU(), nn.Dropout(0.5)
        )
        self.classifier = nn.Linear(256, num_classes)

    def forward(self, x):
        features = self.feature_extractor(x)
        output = self.classifier(features)
        return output, features

# MMDã‚’ä½¿ã£ãŸè¨“ç·´
def train_with_mmd(model, source_loader, target_loader, lambda_mmd=0.1):
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    for (source_data, source_labels), (target_data, _) in zip(source_loader, target_loader):
        # ç‰¹å¾´é‡ã¨äºˆæ¸¬ã‚’å–å¾—
        source_pred, source_feat = model(source_data)
        _, target_feat = model(target_data)

        # åˆ†é¡æå¤±
        class_loss = F.cross_entropy(source_pred, source_labels)

        # MMDæå¤±
        mmd_loss = compute_mmd_loss(source_feat, target_feat)

        # ç·åˆæå¤±
        total_loss = class_loss + lambda_mmd * mmd_loss

        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()

    return class_loss.item(), mmd_loss.item()
</code></pre>

<h2>4. çŸ¥è­˜è’¸ç•™ (Knowledge Distillation)</h2>

<h3>4.1 Teacher-Studentå­¦ç¿’ã®åŸºç¤</h3>
<p>çŸ¥è­˜è’¸ç•™ã¯ã€å¤§ããªTeacherãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã‚’å°ã•ãªStudentãƒ¢ãƒ‡ãƒ«ã«è»¢ç§»ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚ãƒ¡ã‚¿å­¦ç¿’ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€åŠ¹ç‡çš„ãªå°‘æ•°ã‚·ãƒ§ãƒƒãƒˆå­¦ç¿’ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[Large Teacher Model<br/>High Accuracy] -->|Soft Targets| B[Knowledge<br/>Distillation]
    C[Training Data] --> B
    B -->|Transfer| D[Small Student Model<br/>Fast Inference]
    style A fill:#e3f2fd
    style D fill:#c8e6c9
</div>

<h3>4.2 æ¸©åº¦ä»˜ãè’¸ç•™ã®å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class DistillationLoss(nn.Module):
    def __init__(self, temperature=3.0, alpha=0.7):
        """
        çŸ¥è­˜è’¸ç•™ã®æå¤±é–¢æ•°

        Args:
            temperature: ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            alpha: ãƒãƒ¼ãƒ‰æå¤±ã¨ã‚½ãƒ•ãƒˆæå¤±ã®ãƒãƒ©ãƒ³ã‚¹
        """
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.ce_loss = nn.CrossEntropyLoss()

    def forward(self, student_logits, teacher_logits, targets):
        # ã‚½ãƒ•ãƒˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆæå¤±ï¼ˆçŸ¥è­˜è’¸ç•™ï¼‰
        soft_targets = F.softmax(teacher_logits / self.temperature, dim=1)
        soft_student = F.log_softmax(student_logits / self.temperature, dim=1)
        distillation_loss = F.kl_div(
            soft_student, soft_targets, reduction='batchmean'
        ) * (self.temperature ** 2)

        # ãƒãƒ¼ãƒ‰ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæå¤±ï¼ˆé€šå¸¸ã®åˆ†é¡ï¼‰
        student_loss = self.ce_loss(student_logits, targets)

        # ç·åˆæå¤±
        total_loss = (
            self.alpha * distillation_loss +
            (1 - self.alpha) * student_loss
        )

        return total_loss, distillation_loss, student_loss

# Teacherã¨Studentãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
class TeacherModel(nn.Module):
    """å¤§ããªTeacherãƒ¢ãƒ‡ãƒ«"""
    def __init__(self, num_classes=10):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(),
            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),
            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Flatten(),
            nn.Linear(256 * 8 * 8, 512), nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        return self.features(x)

class StudentModel(nn.Module):
    """è»½é‡ãªStudentãƒ¢ãƒ‡ãƒ«"""
    def __init__(self, num_classes=10):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Flatten(),
            nn.Linear(64 * 8 * 8, 128), nn.ReLU(),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        return self.features(x)

# çŸ¥è­˜è’¸ç•™ã®è¨“ç·´
def train_distillation(teacher, student, train_loader, epochs=50):
    # Teacherã¯è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰
    teacher.eval()
    student.train()

    criterion = DistillationLoss(temperature=3.0, alpha=0.7)
    optimizer = torch.optim.Adam(student.parameters(), lr=1e-3)

    for epoch in range(epochs):
        total_loss = 0
        for images, labels in train_loader:
            # Teacheräºˆæ¸¬ï¼ˆå‹¾é…ãªã—ï¼‰
            with torch.no_grad():
                teacher_logits = teacher(images)

            # Studentäºˆæ¸¬
            student_logits = student(images)

            # è’¸ç•™æå¤±
            loss, dist_loss, student_loss = criterion(
                student_logits, teacher_logits, labels
            )

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f}")

# ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®æ¯”è¼ƒ
teacher = TeacherModel()
student = StudentModel()
teacher_params = sum(p.numel() for p in teacher.parameters())
student_params = sum(p.numel() for p in student.parameters())
print(f"Teacher: {teacher_params:,} parameters")
print(f"Student: {student_params:,} parameters")
print(f"Compression ratio: {teacher_params/student_params:.2f}x")
</code></pre>

<h3>4.3 ãƒ¡ã‚¿å­¦ç¿’ã¨ã®çµ„ã¿åˆã‚ã›</h3>

<pre><code class="language-python">class MetaDistillation(nn.Module):
    def __init__(self, teacher_model, student_model, inner_lr=0.01):
        """
        ãƒ¡ã‚¿å­¦ç¿’ã¨çŸ¥è­˜è’¸ç•™ã®çµ„ã¿åˆã‚ã›

        Args:
            teacher_model: å¤§ããªTeacherãƒ¢ãƒ‡ãƒ«
            student_model: è»½é‡ãªStudentãƒ¢ãƒ‡ãƒ«
            inner_lr: å†…å´ãƒ«ãƒ¼ãƒ—ã®å­¦ç¿’ç‡
        """
        super().__init__()
        self.teacher = teacher_model
        self.student = student_model
        self.inner_lr = inner_lr
        self.temperature = 3.0

    def inner_loop(self, support_x, support_y, steps=5):
        """
        å†…å´ãƒ«ãƒ¼ãƒ—ï¼šStudentãƒ¢ãƒ‡ãƒ«ã®ã‚¿ã‚¹ã‚¯é©å¿œ
        """
        # Studentãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ
        adapted_params = [p.clone() for p in self.student.parameters()]

        for _ in range(steps):
            # Teacheräºˆæ¸¬
            with torch.no_grad():
                teacher_logits = self.teacher(support_x)

            # Studentäºˆæ¸¬
            student_logits = self.student(support_x)

            # è’¸ç•™æå¤±
            soft_targets = F.softmax(teacher_logits / self.temperature, dim=1)
            soft_student = F.log_softmax(student_logits / self.temperature, dim=1)
            loss = F.kl_div(soft_student, soft_targets, reduction='batchmean')

            # å‹¾é…è¨ˆç®—ã¨æ›´æ–°
            grads = torch.autograd.grad(loss, self.student.parameters())
            adapted_params = [
                p - self.inner_lr * g
                for p, g in zip(adapted_params, grads)
            ]

        return adapted_params

    def forward(self, support_x, support_y, query_x, query_y):
        """
        ãƒ¡ã‚¿è’¸ç•™ã®é †ä¼æ’­
        """
        # å†…å´ãƒ«ãƒ¼ãƒ—ã§é©å¿œ
        adapted_params = self.inner_loop(support_x, support_y)

        # é©å¿œã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆã‚’è©•ä¾¡
        query_logits = self.student(query_x)
        loss = F.cross_entropy(query_logits, query_y)

        return loss

# ä½¿ç”¨ä¾‹
teacher = TeacherModel(num_classes=5)
student = StudentModel(num_classes=5)
meta_distill = MetaDistillation(teacher, student, inner_lr=0.01)
print("Meta-Distillation model initialized")
</code></pre>

<h2>5. å®Ÿè·µï¼šè»¢ç§»å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ</h2>

<div class="project-box">
<h3>ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼šImageNetã‹ã‚‰åŒ»ç™‚ç”»åƒåˆ†é¡ã¸ã®è»¢ç§»å­¦ç¿’</h3>
<p><strong>ç›®æ¨™ï¼š</strong>ImageNetã§äº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã€å°‘æ•°ã®åŒ»ç™‚ç”»åƒãƒ‡ãƒ¼ã‚¿ã§é«˜ç²¾åº¦ãªè¨ºæ–­ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚</p>
</div>

<h3>5.1 å®Œå…¨ãªè»¢ç§»å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
from torchvision import models, transforms
from torch.utils.data import DataLoader, Dataset
import numpy as np

class MedicalImageDataset(Dataset):
    """åŒ»ç™‚ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]

        if self.transform:
            image = self.transform(image)

        return image, label

class TransferLearningPipeline:
    def __init__(self, num_classes, device='cuda'):
        self.device = device
        self.num_classes = num_classes

        # ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆDomain Adaptationç”¨ï¼‰
        self.train_transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(15),
            transforms.ColorJitter(brightness=0.2, contrast=0.2),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])

        self.val_transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])

        # äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        self.model = self._create_model()

    def _create_model(self):
        """äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º"""
        # ResNet50ï¼ˆImageNetäº‹å‰å­¦ç¿’æ¸ˆã¿ï¼‰
        model = models.resnet50(pretrained=True)

        # åˆæœŸå±¤ã‚’å‡çµ
        for name, param in model.named_parameters():
            if 'layer4' not in name and 'fc' not in name:
                param.requires_grad = False

        # åˆ†é¡å±¤ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
        num_features = model.fc.in_features
        model.fc = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(num_features, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.Dropout(0.3),
            nn.Linear(512, self.num_classes)
        )

        return model.to(self.device)

    def train(self, train_data, val_data, epochs=50, use_distillation=False):
        """
        è¨“ç·´å®Ÿè¡Œ

        Args:
            train_data: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ (images, labels)
            val_data: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ (images, labels)
            epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            use_distillation: çŸ¥è­˜è’¸ç•™ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        """
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        train_dataset = MedicalImageDataset(*train_data, self.train_transform)
        val_dataset = MedicalImageDataset(*val_data, self.val_transform)

        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

        # Discriminative Learning Rates
        optimizer = torch.optim.Adam([
            {'params': self.model.layer4.parameters(), 'lr': 1e-3},
            {'params': self.model.fc.parameters(), 'lr': 1e-2}
        ])

        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, T_0=10, T_mult=2
        )

        criterion = nn.CrossEntropyLoss()

        best_val_acc = 0.0
        history = {'train_loss': [], 'val_loss': [], 'val_acc': []}

        for epoch in range(epochs):
            # è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚º
            self.model.train()
            train_loss = 0.0

            for images, labels in train_loader:
                images, labels = images.to(self.device), labels.to(self.device)

                optimizer.zero_grad()
                outputs = self.model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

                train_loss += loss.item()

            # æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º
            val_loss, val_acc = self._validate(val_loader, criterion)

            # å±¥æ­´ä¿å­˜
            history['train_loss'].append(train_loss / len(train_loader))
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)

            # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                torch.save(self.model.state_dict(), 'best_model.pth')

            scheduler.step()

            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{epochs}")
                print(f"  Train Loss: {history['train_loss'][-1]:.4f}")
                print(f"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

        return history

    def _validate(self, val_loader, criterion):
        """æ¤œè¨¼"""
        self.model.eval()
        val_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(self.device), labels.to(self.device)
                outputs = self.model(images)
                loss = criterion(outputs, labels)

                val_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()

        val_loss /= len(val_loader)
        val_acc = correct / total

        return val_loss, val_acc

    def evaluate_transferability(self, source_data, target_data):
        """è»¢ç§»å¯èƒ½æ€§ã®è©•ä¾¡"""
        self.model.eval()

        def extract_features(data):
            features = []
            with torch.no_grad():
                for images, _ in DataLoader(data, batch_size=32):
                    images = images.to(self.device)
                    # æœ€çµ‚å±¤ã®å‰ã®ç‰¹å¾´é‡ã‚’æŠ½å‡º
                    feat = self.model.layer4(
                        self.model.layer3(
                            self.model.layer2(
                                self.model.layer1(
                                    self.model.conv1(images)
                                )
                            )
                        )
                    )
                    features.append(feat.cpu().flatten(1))
            return torch.cat(features, dim=0)

        source_feats = extract_features(source_data)
        target_feats = extract_features(target_data)

        # MMDã‚¹ã‚³ã‚¢è¨ˆç®—
        score = compute_transferability_score(source_feats, target_feats)
        print(f"Transferability Score: {score:.4f}")

        return score

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã¯åŒ»ç™‚ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
    num_samples = 1000
    train_images = np.random.randint(0, 255, (num_samples, 224, 224, 3), dtype=np.uint8)
    train_labels = np.random.randint(0, 3, num_samples)

    val_images = np.random.randint(0, 255, (200, 224, 224, 3), dtype=np.uint8)
    val_labels = np.random.randint(0, 3, 200)

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    pipeline = TransferLearningPipeline(num_classes=3, device='cpu')

    print("Starting transfer learning training...")
    history = pipeline.train(
        train_data=(train_images, train_labels),
        val_data=(val_images, val_labels),
        epochs=20
    )

    print("\nTraining completed!")
    print(f"Best validation accuracy: {max(history['val_acc']):.4f}")
</code></pre>

<h2>ã¾ã¨ã‚</h2>

<p>ã“ã®ç« ã§ã¯ã€è»¢ç§»å­¦ç¿’ã®åŒ…æ‹¬çš„ãªæŠ€è¡“ã‚’å­¦ã³ã¾ã—ãŸï¼š</p>

<ul>
<li><strong>è»¢ç§»å­¦ç¿’ã®åŸºç¤ï¼š</strong>äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æ´»ç”¨ã«ã‚ˆã‚Šã€å°‘æ•°ãƒ‡ãƒ¼ã‚¿ã§ã‚‚é«˜ç²¾åº¦ãªãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ãŒå¯èƒ½</li>
<li><strong>ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ï¼š</strong>å±¤åˆ¥ã®å­¦ç¿’ç‡èª¿æ•´ã«ã‚ˆã‚ŠåŠ¹ç‡çš„ãªçŸ¥è­˜è»¢ç§»ã‚’å®Ÿç¾</li>
<li><strong>Domain Adaptationï¼š</strong>DANNã‚„MMDã«ã‚ˆã‚Šãƒ‰ãƒ¡ã‚¤ãƒ³ã‚·ãƒ•ãƒˆå•é¡Œã‚’è§£æ±º</li>
<li><strong>çŸ¥è­˜è’¸ç•™ï¼š</strong>å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã‚’è»½é‡ãƒ¢ãƒ‡ãƒ«ã«è»¢ç§»ã—ã€æ¨è«–åŠ¹ç‡ã‚’å‘ä¸Š</li>
<li><strong>å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼š</strong>ImageNetã‹ã‚‰åŒ»ç™‚ç”»åƒã¸ã®è»¢ç§»ã«ã‚ˆã‚Šå®Ÿç”¨çš„ãªå¿œç”¨ã‚’å®Ÿç¾</li>
</ul>

<blockquote>
<strong>é‡è¦ãƒã‚¤ãƒ³ãƒˆï¼š</strong>è»¢ç§»å­¦ç¿’ã¯ã€ãƒ¡ã‚¿å­¦ç¿’ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ã•ã‚‰ã«å¼·åŠ›ãªå°‘æ•°ã‚·ãƒ§ãƒƒãƒˆå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚äº‹å‰å­¦ç¿’ã§ç²å¾—ã—ãŸä¸€èˆ¬çš„ãªç‰¹å¾´è¡¨ç¾ã¨ã€ãƒ¡ã‚¿å­¦ç¿’ã«ã‚ˆã‚‹é«˜é€Ÿé©å¿œèƒ½åŠ›ã‚’çµ±åˆã™ã‚‹ã“ã¨ãŒã€å®Ÿä¸–ç•Œã§ã®æˆåŠŸã®éµã¨ãªã‚Šã¾ã™ã€‚
</blockquote>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<details>
<summary><strong>æ¼”ç¿’1ï¼šè»¢ç§»å¯èƒ½æ€§ã®åˆ†æ</strong></summary>
<p>ç•°ãªã‚‹ã‚½ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆImageNetã€Places365ã€COCOï¼‰ã‹ã‚‰ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆåŒ»ç™‚ç”»åƒï¼‰ã¸ã®è»¢ç§»å¯èƒ½æ€§ã‚’è©•ä¾¡ã—ã€æœ€é©ãªäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚MMDã‚¹ã‚³ã‚¢ã¨å®Ÿéš›ã®æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>
</details>

<details>
<summary><strong>æ¼”ç¿’2ï¼šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã®æ¯”è¼ƒ</strong></summary>
<p>ä»¥ä¸‹ã®3ã¤ã®æˆ¦ç•¥ã‚’æ¯”è¼ƒå®Ÿè£…ã—ã¦ãã ã•ã„ï¼š<br>
1) å…¨å±¤å‡çµï¼ˆåˆ†é¡å±¤ã®ã¿è¨“ç·´ï¼‰<br>
2) éƒ¨åˆ†å‡çµï¼ˆLayer4ã¨åˆ†é¡å±¤ã®ã¿è¨“ç·´ï¼‰<br>
3) Discriminative Fine-Tuning<br>
ãƒ‡ãƒ¼ã‚¿é‡ã‚’å¤‰åŒ–ã•ã›ã€å„æˆ¦ç•¥ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚</p>
</details>

<details>
<summary><strong>æ¼”ç¿’3ï¼šDANNã®å®Ÿè£…ã¨è©•ä¾¡</strong></summary>
<p>ã‚½ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆMNISTï¼‰ã‹ã‚‰ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆMNIST-Mï¼‰ã¸ã®Domain Adaptationã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚å‹¾é…åè»¢å±¤ã®ã‚¢ãƒ«ãƒ•ã‚¡å€¤ã‚’å¤‰åŒ–ã•ã›ã€ãƒ‰ãƒ¡ã‚¤ãƒ³ä¸å¤‰æ€§ã¨åˆ†é¡æ€§èƒ½ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚</p>
</details>

<details>
<summary><strong>æ¼”ç¿’4ï¼šçŸ¥è­˜è’¸ç•™ã®æœ€é©åŒ–</strong></summary>
<p>æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆT=1, 3, 5, 10ï¼‰ã¨ã‚¢ãƒ«ãƒ•ã‚¡å€¤ï¼ˆÎ±=0.3, 0.5, 0.7, 0.9ï¼‰ã‚’å¤‰åŒ–ã•ã›ã¦ã€Studentãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¸ã®å½±éŸ¿ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¦‹ã¤ã‘ã¦ãã ã•ã„ã€‚</p>
</details>

<details>
<summary><strong>æ¼”ç¿’5ï¼šãƒ¡ã‚¿è’¸ç•™ã®å®Ÿè£…</strong></summary>
<p>MAMLã¨çŸ¥è­˜è’¸ç•™ã‚’çµ„ã¿åˆã‚ã›ãŸãƒ¡ã‚¿è’¸ç•™ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚é€šå¸¸ã®MAMLã€é€šå¸¸ã®è’¸ç•™ã€ãƒ¡ã‚¿è’¸ç•™ã®3ã¤ã®æ‰‹æ³•ã‚’æ¯”è¼ƒã—ã€å°‘æ•°ã‚·ãƒ§ãƒƒãƒˆå­¦ç¿’ã«ãŠã‘ã‚‹åŠ¹æœã‚’æ¤œè¨¼ã—ã¦ãã ã•ã„ã€‚</p>
</details>

<h2>å‚è€ƒæ–‡çŒ®</h2>
<ul>
<li>Pan, S. J., & Yang, Q. (2009). "A Survey on Transfer Learning". IEEE Transactions on Knowledge and Data Engineering.</li>
<li>Ganin, Y., et al. (2016). "Domain-Adversarial Training of Neural Networks". JMLR.</li>
<li>Hinton, G., Vinyals, O., & Dean, J. (2015). "Distilling the Knowledge in a Neural Network". NIPS Deep Learning Workshop.</li>
<li>Yosinski, J., et al. (2014). "How transferable are features in deep neural networks?". NIPS.</li>
<li>Howard, J., & Ruder, S. (2018). "Universal Language Model Fine-tuning for Text Classification". ACL.</li>
</ul>

<div class="navigation">
    <a href="chapter3-maml.html" class="nav-button">â† ç¬¬3ç« ï¼šMAML</a>
    <a href="index.html" class="nav-button">ç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter5-meta-applications.html" class="nav-button">ç¬¬5ç« ï¼šå®Ÿè·µå¿œç”¨ â†’</a>
</div>

</main>

<footer>
    <p>&copy; 2024 AI Terakoya. All rights reserved.</p>
</footer>

</body>
</html>
