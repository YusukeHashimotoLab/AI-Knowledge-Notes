<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="音声処理・音声認識入門シリーズ - 音響特徴量から最新の音声AIまでの実践完全ガイド">
    <title>音声処理・音声認識入門シリーズ v1.0 - AI Terakoya</title>

    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header .container {
            padding: 0 1.5rem;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }

        .meta span {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
        }

        /* Typography */
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            color: var(--primary-color);
        }

        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.6rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1.2rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--link-hover);
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1.2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Code blocks */
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border: 1px solid var(--border-color);
        }

        pre code {
            background: none;
            padding: 0;
            font-size: 0.9rem;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
            overflow-x: auto;
            display: block;
        }

        thead {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        tbody {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        th, td {
            padding: 0.8rem;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        th {
            background: var(--primary-color);
            color: white;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background: #f9f9f9;
        }

        /* Blockquotes */
        blockquote {
            border-left: 4px solid var(--secondary-color);
            padding-left: 1.5rem;
            margin: 1.5rem 0;
            font-style: italic;
            color: #666;
        }

        /* Images */
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1rem 0;
        }

        /* Mermaid diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
            background: white;
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        /* Details/Summary (for exercises) */
        details {
            margin: 1rem 0;
            padding: 1rem;
            background: #f8f9fa;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--primary-color);
            padding: 0.5rem;
        }

        summary:hover {
            color: var(--secondary-color);
        }

        /* Footer */
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }

        /* Navigation buttons */
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }

        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            h1 {
                font-size: 1.6rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.85rem;
            }

            table {
                font-size: 0.9rem;
            }
        }
    </style>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        // Mermaid.js Converter - Converts markdown-style mermaid code blocks to renderable divs
        document.addEventListener('DOMContentLoaded', function() {
            // Find all code blocks with class="language-mermaid"
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                // Create a new div with mermaid class
                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                // Replace the pre element with the new div
                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            // Re-initialize mermaid after conversion
            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({ startOnLoad: true, theme: 'default' });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</head>
<body>
    <header>
        <div class="container">
            <h1>🎙️ 音声処理・音声認識入門シリーズ v1.0</h1>
            <p style="font-size: 1.1rem; margin-top: 0.5rem; opacity: 0.95;">音響特徴量から最新の音声AIまで</p>
            <div class="meta">
                <span>📖 総学習時間: 5-6時間</span>
                <span>📊 レベル: 中級</span>
            </div>
        </div>
    </header>

    <main class="container">
        <p><strong>音声信号処理の基礎から、深層学習を用いた音声認識、音声合成、音声分類まで、音声データを扱うための実践的な知識とスキルを習得します</strong></p>

        <h2 id="overview">シリーズ概要</h2>
        <p>このシリーズは、音声処理と音声認識の理論と実装を基礎から段階的に学べる全5章構成の実践的教育コンテンツです。</p>

        <p><strong>音声処理・音声認識</strong>は、音声アシスタント（Siri、Alexa、Google Assistant）、自動字幕生成、音声翻訳、コールセンター自動化、音声検索など、現代社会のあらゆる場面で活用されている重要な技術です。デジタル音声の基礎からMFCC・メルスペクトログラムなどの音響特徴量、伝統的なHMM-GMMモデル、最新の深層学習ベースの音声認識（Whisper、Wav2Vec 2.0）、音声合成（TTS、Tacotron、VITS）、さらに話者認識・感情認識・音声強調などの応用技術まで、音声AIの全体像を体系的に理解できます。Google、Meta、OpenAIが開発した最新モデルの原理と実装を学び、実際の音声データを使った実践的なスキルを身につけます。librosa、torchaudio、Transformersなどの主要ライブラリを使った実装方法を提供します。</p>

        <p><strong>特徴:</strong></p>
        <ul>
            <li>✅ <strong>理論から実践まで</strong>: 音響学の基礎から最新の深層学習モデルまで体系的に学習</li>
            <li>✅ <strong>実装重視</strong>: 50個以上の実行可能なPython/librosa/PyTorchコード例</li>
            <li>✅ <strong>実務指向</strong>: 実際の音声データを使った実践的なプロジェクト</li>
            <li>✅ <strong>最新技術準拠</strong>: Whisper、Wav2Vec 2.0、VITS、Transformersを使った実装</li>
            <li>✅ <strong>実用的応用</strong>: 音声認識・音声合成・話者認識・感情認識の実践</li>
        </ul>

        <p><strong>総学習時間</strong>: 5-6時間（コード実行と演習を含む）</p>

        <h2 id="learning">学習の進め方</h2>

        <h3>推奨学習順序</h3>

        <div class="mermaid">
graph TD
    A[第1章: 音声信号処理の基礎] --> B[第2章: 伝統的音声認識]
    B --> C[第3章: 深層学習による音声認識]
    C --> D[第4章: 音声合成]
    D --> E[第5章: 音声の応用]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
</div>

        <p><strong>初学者の方（音声処理をまったく知らない）:</strong><br>
        - 第1章 → 第2章 → 第3章 → 第4章 → 第5章（全章推奨）<br>
        - 所要時間: 5-6時間</p>

        <p><strong>中級者の方（機械学習の経験あり）:</strong><br>
        - 第1章 → 第3章 → 第4章 → 第5章<br>
        - 所要時間: 4-5時間</p>

        <p><strong>特定トピックの強化:</strong><br>
        - 音声信号処理・MFCC: 第1章（集中学習）<br>
        - HMM・GMM: 第2章（集中学習）<br>
        - 深層学習音声認識: 第3章（集中学習）<br>
        - 音声合成・TTS: 第4章（集中学習）<br>
        - 話者認識・感情認識: 第5章（集中学習）<br>
        - 所要時間: 60-80分/章</p>

        <h2 id="chapters">各章の詳細</h2>

        <h3><a href="./chapter1-signal-processing-basics.html">第1章：音声信号処理の基礎</a></h3>
        <p><strong>難易度</strong>: 中級<br>
        <strong>読了時間</strong>: 60-70分<br>
        <strong>コード例</strong>: 12個</p>

        <h4>学習内容</h4>
        <ol>
            <li><strong>デジタル音声の基礎</strong> - サンプリング、量子化、ナイキスト定理</li>
            <li><strong>音響特徴量</strong> - MFCC、メルスペクトログラム、ピッチ、フォルマント</li>
            <li><strong>スペクトル分析</strong> - フーリエ変換、STFT、スペクトログラム</li>
            <li><strong>librosaの使い方</strong> - 音声読み込み、特徴量抽出、可視化</li>
            <li><strong>音声の前処理</strong> - ノイズ除去、正規化、VAD（音声区間検出）</li>
        </ol>

        <h4>学習目標</h4>
        <ul>
            <li>✅ デジタル音声の基本原理を理解する</li>
            <li>✅ 音響特徴量（MFCC、メルスペクトログラム）を説明できる</li>
            <li>✅ スペクトル分析の手法を理解する</li>
            <li>✅ librosaで音声データを処理できる</li>
            <li>✅ 音声の前処理技術を実装できる</li>
        </ul>

        <p><strong><a href="./chapter1-signal-processing-basics.html">第1章を読む →</a></strong></p>

        <hr>

        <h3><a href="./chapter2-traditional-asr.html">第2章：伝統的音声認識</a></h3>
        <p><strong>難易度</strong>: 中級<br>
        <strong>読了時間</strong>: 60-70分<br>
        <strong>コード例</strong>: 8個</p>

        <h4>学習内容</h4>
        <ol>
            <li><strong>音声認識の基礎</strong> - 音響モデル、言語モデル、デコーディング</li>
            <li><strong>HMM（隠れマルコフモデル）</strong> - 状態遷移、観測確率、Viterbiアルゴリズム</li>
            <li><strong>GMM（混合ガウスモデル）</strong> - 音響モデリング、EMアルゴリズム</li>
            <li><strong>言語モデル</strong> - N-gram、統計的言語モデル、スムージング</li>
            <li><strong>評価指標</strong> - WER（単語誤り率）、CER（文字誤り率）</li>
        </ol>

        <h4>学習目標</h4>
        <ul>
            <li>✅ 音声認識の基本アーキテクチャを理解する</li>
            <li>✅ HMMの原理とViterbiアルゴリズムを説明できる</li>
            <li>✅ GMMによる音響モデリングを理解する</li>
            <li>✅ N-gram言語モデルを実装できる</li>
            <li>✅ WER・CERで性能を評価できる</li>
        </ul>

        <p><strong><a href="./chapter2-traditional-asr.html">第2章を読む →</a></strong></p>

        <hr>

        <h3><a href="./chapter3-deep-learning-asr.html">第3章：深層学習による音声認識</a></h3>
        <p><strong>難易度</strong>: 中級〜上級<br>
        <strong>読了時間</strong>: 80-90分<br>
        <strong>コード例</strong>: 10個</p>

        <h4>学習内容</h4>
        <ol>
            <li><strong>エンドツーエンド音声認識</strong> - CTC（Connectionist Temporal Classification）</li>
            <li><strong>RNN-Transducer</strong> - ストリーミング音声認識、オンライン認識</li>
            <li><strong>Transformer音声認識</strong> - Self-Attention、Positional Encoding</li>
            <li><strong>Whisper</strong> - OpenAIの多言語音声認識モデル、ゼロショット学習</li>
            <li><strong>Wav2Vec 2.0</strong> - 自己教師あり学習、音声表現学習</li>
        </ol>

        <h4>学習目標</h4>
        <ul>
            <li>✅ CTC損失関数の原理を理解する</li>
            <li>✅ RNN-Transducerでストリーミング認識を実装できる</li>
            <li>✅ Transformerの音声認識への応用を理解する</li>
            <li>✅ Whisperで多言語音声認識を実装できる</li>
            <li>✅ Wav2Vec 2.0で音声表現を学習できる</li>
        </ul>

        <p><strong><a href="./chapter3-deep-learning-asr.html">第3章を読む →</a></strong></p>

        <hr>

        <h3><a href="./chapter4-speech-synthesis.html">第4章：音声合成</a></h3>
        <p><strong>難易度</strong>: 中級〜上級<br>
        <strong>読了時間</strong>: 70-80分<br>
        <strong>コード例</strong>: 10個</p>

        <h4>学習内容</h4>
        <ol>
            <li><strong>TTS（Text-to-Speech）の基礎</strong> - 音韻変換、韻律生成、音声合成</li>
            <li><strong>Tacotron 2</strong> - Seq2Seqモデル、Attention機構、メルスペクトログラム生成</li>
            <li><strong>FastSpeech</strong> - 非自己回帰モデル、並列生成、高速合成</li>
            <li><strong>VITS</strong> - エンドツーエンドTTS、変分推論、ニューラルボコーダー</li>
            <li><strong>ボコーダー</strong> - WaveNet、WaveGlow、HiFi-GAN</li>
        </ol>

        <h4>学習目標</h4>
        <ul>
            <li>✅ TTSの基本アーキテクチャを理解する</li>
            <li>✅ Tacotron 2でメルスペクトログラムを生成できる</li>
            <li>✅ FastSpeechで高速音声合成を実装できる</li>
            <li>✅ VITSでエンドツーエンドTTSを実装できる</li>
            <li>✅ ニューラルボコーダーで音声波形を生成できる</li>
        </ul>

        <p><strong><a href="./chapter4-speech-synthesis.html">第4章を読む →</a></strong></p>

        <hr>

        <h3><a href="./chapter5-speech-applications.html">第5章：音声の応用</a></h3>
        <p><strong>難易度</strong>: 中級〜上級<br>
        <strong>読了時間</strong>: 70-80分<br>
        <strong>コード例</strong>: 12個</p>

        <h4>学習内容</h4>
        <ol>
            <li><strong>話者認識</strong> - 話者識別、話者照合、x-vector、d-vector</li>
            <li><strong>感情認識</strong> - 音響特徴量、韻律特徴、深層学習モデル</li>
            <li><strong>音声強調</strong> - ノイズ除去、ビームフォーミング、マスキング手法</li>
            <li><strong>音楽情報処理</strong> - テンポ検出、ビート追跡、ジャンル分類</li>
            <li><strong>音声活動検出（VAD）</strong> - WebRTC VAD、深層学習ベースVAD</li>
        </ol>

        <h4>学習目標</h4>
        <ul>
            <li>✅ 話者認識の手法を理解し実装できる</li>
            <li>✅ 音声から感情を認識できる</li>
            <li>✅ 音声強調技術を実装できる</li>
            <li>✅ 音楽情報処理の基礎を理解する</li>
            <li>✅ VADで音声区間を検出できる</li>
        </ul>

        <p><strong><a href="./chapter5-speech-applications.html">第5章を読む →</a></strong></p>

        <hr>

        <h2 id="outcomes">全体の学習成果</h2>

        <p>このシリーズを完了すると、以下のスキルと知識を習得できます：</p>

        <h3>知識レベル（Understanding）</h3>
        <ul>
            <li>✅ デジタル音声とMFCCなどの音響特徴量を説明できる</li>
            <li>✅ HMM-GMMとCTCの違いを理解している</li>
            <li>✅ 深層学習音声認識の最新動向を説明できる</li>
            <li>✅ TTSと音声合成の原理を理解している</li>
            <li>✅ 話者認識・感情認識の手法を説明できる</li>
        </ul>

        <h3>実践スキル（Doing）</h3>
        <ul>
            <li>✅ librosaで音声データを処理できる</li>
            <li>✅ MFCC・メルスペクトログラムを抽出できる</li>
            <li>✅ Whisperで音声認識を実装できる</li>
            <li>✅ VITSで音声合成を実装できる</li>
            <li>✅ 話者認識・感情認識モデルを構築できる</li>
        </ul>

        <h3>応用力（Applying）</h3>
        <ul>
            <li>✅ プロジェクトに適した音声認識手法を選択できる</li>
            <li>✅ 音声データの前処理パイプラインを設計できる</li>
            <li>✅ カスタム音声認識システムを構築できる</li>
            <li>✅ 音声合成アプリケーションを開発できる</li>
            <li>✅ 音声AIシステムを評価・改善できる</li>
        </ul>

        <hr>

        <h2 id="prerequisites">前提知識</h2>

        <p>このシリーズを効果的に学習するために、以下の知識があることが望ましいです：</p>

        <h3>必須（Must Have）</h3>
        <ul>
            <li>✅ <strong>Python基礎</strong>: 変数、関数、クラス、NumPy、pandas</li>
            <li>✅ <strong>機械学習の基礎</strong>: 学習・評価・損失関数の概念</li>
            <li>✅ <strong>数学基礎</strong>: 線形代数、確率・統計、微積分</li>
            <li>✅ <strong>信号処理の基礎</strong>: フーリエ変換の概念（推奨）</li>
            <li>✅ <strong>深層学習の基礎</strong>: CNN、RNN、Transformerの基本（第3章以降）</li>
        </ul>

        <h3>推奨（Nice to Have）</h3>
        <ul>
            <li>💡 <strong>PyTorch基礎</strong>: テンソル操作、モデル構築、学習ループ</li>
            <li>💡 <strong>Transformers経験</strong>: Hugging Face Transformersライブラリ</li>
            <li>💡 <strong>音響学の知識</strong>: 音波、周波数、デシベル</li>
            <li>💡 <strong>自然言語処理</strong>: トークン化、言語モデル（音声認識のため）</li>
            <li>💡 <strong>時系列データ処理</strong>: RNN、LSTM、Seq2Seq</li>
        </ul>

        <p><strong>推奨される前の学習</strong>:</p>
        <ul>
            <li>📚 <a href="../machine-learning-basics/">機械学習入門シリーズ</a> - ML基礎知識</li>
            <li>📚 <a href="../deep-learning-basics/">深層学習入門シリーズ</a> - CNN、RNN、Transformer</li>
            <li>📚 <a href="../pytorch-introduction/">PyTorch実践入門</a> - PyTorchの使い方</li>
            <li>📚 <a href="../signal-processing-basics/">信号処理入門</a> - フーリエ変換、スペクトル分析</li>
        </ul>

        <hr>

        <h2 id="tech">使用技術とツール</h2>

        <h3>主要ライブラリ</h3>
        <ul>
            <li><strong>librosa 0.10+</strong> - 音声信号処理、特徴量抽出</li>
            <li><strong>PyTorch 2.0+</strong> - 深層学習フレームワーク</li>
            <li><strong>torchaudio 2.0+</strong> - PyTorch音声処理ライブラリ</li>
            <li><strong>Transformers 4.30+</strong> - Hugging Face、Whisper、Wav2Vec 2.0</li>
            <li><strong>SpeechBrain 0.5+</strong> - 音声処理ツールキット</li>
            <li><strong>Kaldi</strong> - 伝統的音声認識ツールキット（参考）</li>
            <li><strong>ESPnet</strong> - エンドツーエンド音声処理ツールキット</li>
        </ul>

        <h3>開発環境</h3>
        <ul>
            <li><strong>Python 3.8+</strong> - プログラミング言語</li>
            <li><strong>Jupyter Notebook / Google Colab</strong> - 対話的開発環境</li>
            <li><strong>NumPy 1.23+</strong> - 数値計算</li>
            <li><strong>SciPy 1.10+</strong> - 科学技術計算</li>
            <li><strong>matplotlib / seaborn</strong> - 可視化</li>
        </ul>

        <h3>データセット（推奨）</h3>
        <ul>
            <li><strong>LibriSpeech</strong> - 英語音声認識ベンチマーク</li>
            <li><strong>Common Voice</strong> - 多言語音声データセット</li>
            <li><strong>LJSpeech</strong> - 英語音声合成データセット</li>
            <li><strong>VCTK</strong> - 多話者音声データセット</li>
            <li><strong>RAVDESS</strong> - 感情音声データセット</li>
        </ul>

        <hr>

        <h2 id="start">さあ、始めましょう！</h2>
        <p>準備はできましたか？ 第1章から始めて、音声処理と音声認識の技術を習得しましょう！</p>

        <p><strong><a href="./chapter1-signal-processing-basics.html">第1章: 音声信号処理の基礎 →</a></strong></p>

        <hr>

        <h2 id="next">次のステップ</h2>

        <p>このシリーズを完了した後、以下のトピックへ進むことをお勧めします：</p>

        <h3>深掘り学習</h3>
        <ul>
            <li>📚 <strong>音声対話システム</strong>: 音声アシスタント、対話管理、NLU統合</li>
            <li>📚 <strong>多言語音声処理</strong>: 言語横断転移学習、低リソース言語対応</li>
            <li>📚 <strong>リアルタイム音声処理</strong>: ストリーミング処理、低レイテンシー最適化</li>
            <li>📚 <strong>音声生成モデル</strong>: 音声変換、ボイスクローニング、歌声合成</li>
        </ul>

        <h3>関連シリーズ</h3>
        <ul>
            <li>🎯 <a href="../nlp-introduction/">自然言語処理入門</a> - テキスト処理、言語モデル</li>
            <li>🎯 <a href="../computer-vision-introduction/">コンピュータビジョン入門</a> - マルチモーダルAI</li>
            <li>🎯 <a href="../transformer-architecture/">Transformer完全ガイド</a> - Attention機構</li>
        </ul>

        <h3>実践プロジェクト</h3>
        <ul>
            <li>🚀 音声アシスタント - ウェイクワード検出、音声認識、音声応答</li>
            <li>🚀 自動字幕生成システム - 動画音声認識、タイムスタンプ付き字幕</li>
            <li>🚀 多言語音声翻訳アプリ - 音声認識→機械翻訳→音声合成</li>
            <li>🚀 感情認識コールセンターAI - 顧客感情分析、品質モニタリング</li>
        </ul>

        <hr>

        <p><strong>更新履歴</strong></p>
        <ul>
            <li><strong>2025-10-21</strong>: v1.0 初版公開</li>
        </ul>

        <hr>

        <p><strong>あなたの音声AIの旅はここから始まります！</strong></p>

    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
