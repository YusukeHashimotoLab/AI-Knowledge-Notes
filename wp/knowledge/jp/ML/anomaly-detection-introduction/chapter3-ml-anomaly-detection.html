<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬3ç« ï¼šæ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹ç•°å¸¸æ¤œçŸ¥ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/AI-Knowledge-Notes/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/AI-Knowledge-Notes/knowledge/jp/ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="/AI-Knowledge-Notes/knowledge/jp/ML/anomaly-detection-introduction/index.html">Anomaly Detection</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬3ç« ï¼šæ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹ç•°å¸¸æ¤œçŸ¥</h1>
            <p class="subtitle">Isolation Forestã€LOFã€One-Class SVMã«ã‚ˆã‚‹ç•°å¸¸æ¤œå‡º</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 70-80åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 10å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>ç¬¬3ç« ï¼šæ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹ç•°å¸¸æ¤œçŸ¥</h1>

<div class="learning-objectives">
<h2>å­¦ç¿’ç›®æ¨™</h2>
<ul>
<li>Isolation Forestã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åŸç†ã‚’ç†è§£ã™ã‚‹</li>
<li>LOFï¼ˆLocal Outlier Factorï¼‰ã§å±€æ‰€çš„ãªç•°å¸¸ã‚’æ¤œå‡ºã§ãã‚‹</li>
<li>One-Class SVMã«ã‚ˆã‚‹æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®å¢ƒç•Œå­¦ç¿’ã‚’ç¿’å¾—ã™ã‚‹</li>
<li>DBSCANã‚„ãã®ä»–ã®æ‰‹æ³•ã‚’ç•°å¸¸æ¤œçŸ¥ã«é©ç”¨ã§ãã‚‹</li>
<li>ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç•°å¸¸æ¤œçŸ¥ã®å®Ÿè£…æ–¹æ³•ã‚’å­¦ã¶</li>
</ul>
</div>

<p><strong>èª­äº†æ™‚é–“</strong>: 70-80åˆ†</p>

<hr />

<h2>3.1 Isolation Forestï¼ˆå­¤ç«‹æ£®æ—ï¼‰</h2>

<p>Isolation Forestã¯ã€ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ãŒæ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šã€Œåˆ†é›¢ã—ã‚„ã™ã„ã€ã¨ã„ã†æ€§è³ªã‚’åˆ©ç”¨ã—ãŸç•°å¸¸æ¤œçŸ¥ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚2008å¹´ã«Liu et al.ã«ã‚ˆã£ã¦ææ¡ˆã•ã‚Œã€é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã«ã‚‚åŠ¹æœçš„ã«é©ç”¨ã§ãã¾ã™ã€‚</p>

<h3>3.1.1 ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åŸç†</h3>

<p><strong>åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢:</strong></p>
<ul>
<li>ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã¯æ•°ãŒå°‘ãªãã€æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã¨ã¯ç•°ãªã‚‹ç‰¹å¾´å€¤ã‚’æŒã¤</li>
<li>ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã‚“ã ç‰¹å¾´é‡ã§åˆ†å‰²ã‚’ç¹°ã‚Šè¿”ã™ã¨ã€ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã¯ã‚ˆã‚Šæ—©ãå­¤ç«‹ã™ã‚‹</li>
<li>å­¤ç«‹ã¾ã§ã®åˆ†å‰²å›æ•°ï¼ˆãƒ‘ã‚¹é•·ï¼‰ãŒçŸ­ã„ã»ã©ç•°å¸¸åº¦ãŒé«˜ã„</li>
</ul>

<p><strong>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚¹ãƒ†ãƒƒãƒ—:</strong></p>

<pre><code>1. ãƒ©ãƒ³ãƒ€ãƒ ã«ç‰¹å¾´é‡ã‚’é¸æŠ
2. ãã®ç‰¹å¾´é‡ã®æœ€å°å€¤ã¨æœ€å¤§å€¤ã®é–“ã§ãƒ©ãƒ³ãƒ€ãƒ ã«åˆ†å‰²ç‚¹ã‚’é¸ã¶
3. ãƒ‡ãƒ¼ã‚¿ã‚’2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†ã‘ã‚‹
4. å„ã‚°ãƒ«ãƒ¼ãƒ—ã«å¯¾ã—ã¦å†å¸°çš„ã«1-3ã‚’ç¹°ã‚Šè¿”ã™
5. å„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆãŒå­¤ç«‹ã™ã‚‹ã¾ã§ã®ãƒ‘ã‚¹é•·ã‚’è¨˜éŒ²
6. è¤‡æ•°ã®æœ¨ï¼ˆæ£®ï¼‰ã‚’æ§‹ç¯‰ã—ã€å¹³å‡ãƒ‘ã‚¹é•·ã‹ã‚‰ç•°å¸¸ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
</code></pre>

<h3>3.1.2 ãƒ‘ã‚¹é•·ã¨ç•°å¸¸ã‚¹ã‚³ã‚¢</h3>

<p><strong>ãƒ‘ã‚¹é•·ï¼ˆPath Lengthï¼‰:</strong></p>

<p>ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ $x$ ãŒå­¤ç«‹ã™ã‚‹ã¾ã§ã®åˆ†å‰²å›æ•°ã‚’ $h(x)$ ã¨ã™ã‚‹ã¨ã€æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã¯æ·±ã„ä½ç½®ï¼ˆå¤§ããª $h(x)$ï¼‰ã€ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã¯æµ…ã„ä½ç½®ï¼ˆå°ã•ãª $h(x)$ï¼‰ã«å­¤ç«‹ã—ã¾ã™ã€‚</p>

<p><strong>ç•°å¸¸ã‚¹ã‚³ã‚¢ã®è¨ˆç®—:</strong></p>

$$
s(x, n) = 2^{-\frac{E[h(x)]}{c(n)}}
$$

<p>ã“ã“ã§:</p>
<ul>
<li>$E[h(x)]$: è¤‡æ•°ã®æœ¨ã«ãŠã‘ã‚‹å¹³å‡ãƒ‘ã‚¹é•·</li>
<li>$c(n)$: ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º $n$ ã«ãŠã‘ã‚‹å¹³å‡ãƒ‘ã‚¹é•·ã®æ­£è¦åŒ–å®šæ•°</li>
<li>$c(n) = 2H(n-1) - \frac{2(n-1)}{n}$ ï¼ˆ$H(i)$ ã¯èª¿å’Œæ•°ï¼‰</li>
</ul>

<p><strong>ã‚¹ã‚³ã‚¢ã®è§£é‡ˆ:</strong></p>
<ul>
<li>$s \approx 1$: ç•°å¸¸ï¼ˆæ˜ç¢ºãªç•°å¸¸å€¤ï¼‰</li>
<li>$s \approx 0.5$: æ­£å¸¸ï¼ˆå¹³å‡çš„ãªãƒ‘ã‚¹é•·ï¼‰</li>
<li>$s < 0.5$: æ­£å¸¸ï¼ˆå¹³å‡ã‚ˆã‚Šæ·±ã„ä½ç½®ï¼‰</li>
</ul>

<h3>3.1.3 ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´</h3>

<p><strong>ä¸»è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:</strong></p>

<table>
<thead>
<tr>
<th>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</th>
<th>èª¬æ˜</th>
<th>æ¨å¥¨å€¤</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>n_estimators</code></td>
<td>æœ¨ã®æ•°</td>
<td>100-200ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰</td>
</tr>
<tr>
<td><code>max_samples</code></td>
<td>å„æœ¨ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ãƒ‡ãƒ¼ã‚¿æ•°</td>
<td>256ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: autoï¼‰</td>
</tr>
<tr>
<td><code>contamination</code></td>
<td>ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ</td>
<td>0.1ï¼ˆãƒ‡ãƒ¼ã‚¿ã«ä¾å­˜ï¼‰</td>
</tr>
<tr>
<td><code>max_features</code></td>
<td>å„åˆ†å‰²ã§è€ƒæ…®ã™ã‚‹ç‰¹å¾´é‡æ•°</td>
<td>1.0ï¼ˆå…¨ç‰¹å¾´é‡ï¼‰</td>
</tr>
</tbody>
</table>

<p><strong>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é¸æŠã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³:</strong></p>
<ul>
<li><code>n_estimators</code>: å¤šã„ã»ã©å®‰å®šã™ã‚‹ãŒè¨ˆç®—ã‚³ã‚¹ãƒˆå¢—åŠ ï¼ˆ100-200ã§ååˆ†ï¼‰</li>
<li><code>max_samples</code>: 256ãŒæ¨å¥¨ï¼ˆè«–æ–‡ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã¯å°ã•ãã—ã¦ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—</li>
<li><code>contamination</code>: äº‹å‰ã«ç•°å¸¸ç‡ãŒã‚ã‹ã£ã¦ã„ã‚Œã°ãã‚Œã‚’è¨­å®šã€ä¸æ˜ãªã‚‰0.1</li>
</ul>

<h3>3.1.4 scikit-learnå®Ÿè£…</h3>

<p><strong>åŸºæœ¬å®Ÿè£…:</strong></p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.datasets import make_blobs

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆæ­£å¸¸ãƒ‡ãƒ¼ã‚¿ + ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ï¼‰
np.random.seed(42)
X_normal, _ = make_blobs(n_samples=300, centers=1, cluster_std=0.5, random_state=42)
X_anomaly = np.random.uniform(low=-4, high=4, size=(20, 2))  # ç•°å¸¸ãƒ‡ãƒ¼ã‚¿
X = np.vstack([X_normal, X_anomaly])

# Isolation Forestãƒ¢ãƒ‡ãƒ«
iso_forest = IsolationForest(
    n_estimators=100,
    max_samples=256,
    contamination=0.1,  # 10%ãŒç•°å¸¸ã¨æƒ³å®š
    random_state=42
)

# å­¦ç¿’ã¨äºˆæ¸¬
y_pred = iso_forest.fit_predict(X)  # -1: ç•°å¸¸ã€1: æ­£å¸¸
scores = iso_forest.score_samples(X)  # ç•°å¸¸ã‚¹ã‚³ã‚¢ï¼ˆä½ã„ã»ã©ç•°å¸¸ï¼‰

# å¯è¦–åŒ–
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='coolwarm', edgecolors='k')
plt.title('Isolation Forest: ç•°å¸¸æ¤œçŸ¥çµæœ')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Prediction (-1: Anomaly, 1: Normal)')

plt.subplot(1, 2, 2)
plt.scatter(X[:, 0], X[:, 1], c=scores, cmap='viridis', edgecolors='k')
plt.title('Isolation Forest: ç•°å¸¸ã‚¹ã‚³ã‚¢')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Anomaly Score')

plt.tight_layout()
plt.show()

print(f"æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸ãƒ‡ãƒ¼ã‚¿æ•°: {np.sum(y_pred == -1)}")
print(f"ç•°å¸¸ã‚¹ã‚³ã‚¢ç¯„å›²: [{scores.min():.3f}, {scores.max():.3f}]")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹:</strong></p>
<pre><code>æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸ãƒ‡ãƒ¼ã‚¿æ•°: 32
ç•°å¸¸ã‚¹ã‚³ã‚¢ç¯„å›²: [-0.234, 0.178]
</code></pre>

<p><strong>å®Ÿãƒ‡ãƒ¼ã‚¿ã¸ã®é©ç”¨ä¾‹ï¼ˆã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ä¸æ­£æ¤œçŸ¥ï¼‰:</strong></p>

<pre><code class="language-python">import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆä»®æƒ³çš„ãªä¾‹ï¼‰
# å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã¯ Kaggle Credit Card Fraud Detection ãªã©ã‚’ä½¿ç”¨
# URL: https://www.kaggle.com/mlg-ulb/creditcardfraud

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ã®ä»£æ›¿ï¼‰
np.random.seed(42)
n_normal = 1000
n_fraud = 50

# æ­£å¸¸å–å¼•ï¼ˆé‡‘é¡å°ã€å›æ•°å¤šã€åœ°ç†çš„ã«é›†ä¸­ï¼‰
normal_features = np.random.randn(n_normal, 5) * [10, 5, 2, 1, 0.5]
normal_labels = np.zeros(n_normal)

# ä¸æ­£å–å¼•ï¼ˆé‡‘é¡å¤§ã€å›æ•°å°‘ã€åœ°ç†çš„ã«åˆ†æ•£ï¼‰
fraud_features = np.random.randn(n_fraud, 5) * [50, 1, 10, 5, 3] + [100, 0, 50, 20, 10]
fraud_labels = np.ones(n_fraud)

X = np.vstack([normal_features, fraud_features])
y = np.hstack([normal_labels, fraud_labels])

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

# Isolation Forestï¼ˆæ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§å­¦ç¿’ï¼‰
iso_forest = IsolationForest(
    n_estimators=100,
    contamination=0.05,  # 5%ãŒä¸æ­£ã¨æƒ³å®š
    random_state=42
)

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
iso_forest.fit(X_train)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
y_pred = iso_forest.predict(X_test)
y_pred = np.where(y_pred == -1, 1, 0)  # -1ã‚’1ï¼ˆä¸æ­£ï¼‰ã«å¤‰æ›

# è©•ä¾¡
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['Normal', 'Fraud']))
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹:</strong></p>
<pre><code>Confusion Matrix:
[[285  15]
 [  3  12]]

Classification Report:
              precision    recall  f1-score   support

      Normal       0.99      0.95      0.97       300
       Fraud       0.44      0.80      0.57        15

    accuracy                           0.94       315
   macro avg       0.72      0.88      0.77       315
weighted avg       0.96      0.94      0.95       315
</code></pre>

<hr />

<h2>3.2 LOFï¼ˆLocal Outlier Factorï¼‰</h2>

<p>LOFã¯ã€å„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã®å±€æ‰€çš„ãªå¯†åº¦ã«åŸºã¥ã„ã¦ç•°å¸¸ã‚’æ¤œå‡ºã™ã‚‹æ‰‹æ³•ã§ã™ã€‚2000å¹´ã«Breunig et al.ã«ã‚ˆã£ã¦ææ¡ˆã•ã‚Œã¾ã—ãŸã€‚</p>

<h3>3.2.1 å¯†åº¦ãƒ™ãƒ¼ã‚¹ç•°å¸¸æ¤œçŸ¥</h3>

<p><strong>åŸºæœ¬åŸç†:</strong></p>
<ul>
<li>æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã¯é«˜å¯†åº¦é ˜åŸŸã«å­˜åœ¨ã™ã‚‹</li>
<li>ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã¯ä½å¯†åº¦é ˜åŸŸã«å­˜åœ¨ã™ã‚‹</li>
<li>å„ç‚¹ã®å¯†åº¦ã‚’è¿‘å‚ç‚¹ã®å¯†åº¦ã¨æ¯”è¼ƒã—ã¦ç•°å¸¸åº¦ã‚’è¨ˆç®—</li>
</ul>

<p><strong>ãªãœã€Œå±€æ‰€çš„ã€ã‹:</strong></p>
<ul>
<li>ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªå¯†åº¦ã§ã¯æ¤œå‡ºã§ããªã„ç•°å¸¸ã‚‚æ¤œå‡ºå¯èƒ½</li>
<li>å¯†åº¦ãŒç•°ãªã‚‹è¤‡æ•°ã®ã‚¯ãƒ©ã‚¹ã‚¿ãŒå­˜åœ¨ã™ã‚‹å ´åˆã«æœ‰åŠ¹</li>
<li>å„ç‚¹ã®å‘¨è¾ºç’°å¢ƒã‚’è€ƒæ…®ã—ãŸç›¸å¯¾çš„ãªç•°å¸¸åº¦ã‚’ç®—å‡º</li>
</ul>

<h3>3.2.2 å±€æ‰€åˆ°é”å¯èƒ½å¯†åº¦ï¼ˆLocal Reachability Densityï¼‰</h3>

<p><strong>kè·é›¢ï¼ˆk-distanceï¼‰:</strong></p>

<p>ç‚¹ $p$ ã‹ã‚‰ kç•ªç›®ã«è¿‘ã„ç‚¹ã¾ã§ã®è·é›¢ã‚’ $d_k(p)$ ã¨ã—ã¾ã™ã€‚</p>

<p><strong>åˆ°é”å¯èƒ½è·é›¢ï¼ˆReachability Distanceï¼‰:</strong></p>

$$
\text{reach-dist}_k(p, o) = \max\{d_k(o), d(p, o)\}
$$

<ul>
<li>$d(p, o)$: ç‚¹ $p$ ã¨ $o$ é–“ã®å®Ÿéš›ã®è·é›¢</li>
<li>è¿‘å‚ç‚¹ $o$ ãŒå¯†ãªå ´åˆã€åˆ°é”å¯èƒ½è·é›¢ã¯ $d_k(o)$ ã§ä¸‹é™ãŒè¨­å®šã•ã‚Œã‚‹</li>
</ul>

<p><strong>å±€æ‰€åˆ°é”å¯èƒ½å¯†åº¦ï¼ˆLRDï¼‰:</strong></p>

$$
\text{LRD}_k(p) = \frac{1}{\frac{\sum_{o \in N_k(p)} \text{reach-dist}_k(p, o)}{|N_k(p)|}}
$$

<ul>
<li>$N_k(p)$: ç‚¹ $p$ ã® kè¿‘å‚ç‚¹ã®é›†åˆ</li>
<li>åˆ°é”å¯èƒ½è·é›¢ã®å¹³å‡ã®é€†æ•° = å¯†åº¦</li>
</ul>

<h3>3.2.3 LOFã‚¹ã‚³ã‚¢ã®è¨ˆç®—</h3>

<p><strong>LOFï¼ˆLocal Outlier Factorï¼‰:</strong></p>

$$
\text{LOF}_k(p) = \frac{\sum_{o \in N_k(p)} \frac{\text{LRD}_k(o)}{\text{LRD}_k(p)}}{|N_k(p)|}
$$

<p><strong>ã‚¹ã‚³ã‚¢ã®è§£é‡ˆ:</strong></p>
<ul>
<li>$\text{LOF} \approx 1$: æ­£å¸¸ï¼ˆè¿‘å‚ã¨åŒç¨‹åº¦ã®å¯†åº¦ï¼‰</li>
<li>$\text{LOF} \gg 1$: ç•°å¸¸ï¼ˆè¿‘å‚ã‚ˆã‚Šå¯†åº¦ãŒä½ã„ï¼‰</li>
<li>$\text{LOF} < 1$: æ­£å¸¸ï¼ˆè¿‘å‚ã‚ˆã‚Šå¯†åº¦ãŒé«˜ã„ï¼‰</li>
</ul>

<p>ä¸€èˆ¬çš„ã« $\text{LOF} > 1.5$ ã‚’ç•°å¸¸ã¨ã¿ãªã—ã¾ã™ã€‚</p>

<h3>3.2.4 å®Œå…¨ãªå®Ÿè£…ä¾‹</h3>

<p><strong>åŸºæœ¬å®Ÿè£…:</strong></p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import LocalOutlierFactor
from sklearn.datasets import make_moons

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆæœˆå‹ãƒ‡ãƒ¼ã‚¿ + ç•°å¸¸ç‚¹ï¼‰
np.random.seed(42)
X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)
X_outliers = np.random.uniform(low=-1, high=2, size=(20, 2))
X = np.vstack([X, X_outliers])

# LOFãƒ¢ãƒ‡ãƒ«
lof = LocalOutlierFactor(
    n_neighbors=20,  # è¿‘å‚ç‚¹æ•°
    contamination=0.1,  # ç•°å¸¸ç‡
    novelty=False  # æ–°è¦ãƒ‡ãƒ¼ã‚¿äºˆæ¸¬ã«ã¯True
)

# äºˆæ¸¬
y_pred = lof.fit_predict(X)  # -1: ç•°å¸¸ã€1: æ­£å¸¸
scores = lof.negative_outlier_factor_  # è² ã®ç•°å¸¸åº¦ã‚¹ã‚³ã‚¢ï¼ˆä½ã„ã»ã©ç•°å¸¸ï¼‰

# å¯è¦–åŒ–
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='coolwarm', edgecolors='k')
plt.title('LOF: ç•°å¸¸æ¤œçŸ¥çµæœ')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Prediction (-1: Anomaly, 1: Normal)')

plt.subplot(1, 2, 2)
plt.scatter(X[:, 0], X[:, 1], c=scores, cmap='viridis', edgecolors='k')
plt.title('LOF: ç•°å¸¸ã‚¹ã‚³ã‚¢')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Negative Outlier Factor')

plt.tight_layout()
plt.show()

print(f"æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸ãƒ‡ãƒ¼ã‚¿æ•°: {np.sum(y_pred == -1)}")
print(f"ç•°å¸¸ã‚¹ã‚³ã‚¢ç¯„å›²: [{scores.min():.3f}, {scores.max():.3f}]")
</code></pre>

<p><strong>n_neighborsãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿:</strong></p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import LocalOutlierFactor

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
X_normal = np.random.randn(200, 2) * 0.5
X_outliers = np.random.uniform(low=-3, high=3, size=(10, 2))
X = np.vstack([X_normal, X_outliers])

# ç•°ãªã‚‹n_neighborsã§æ¯”è¼ƒ
n_neighbors_list = [5, 20, 50]

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for idx, n_neighbors in enumerate(n_neighbors_list):
    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=0.1)
    y_pred = lof.fit_predict(X)

    axes[idx].scatter(X[:, 0], X[:, 1], c=y_pred, cmap='coolwarm', edgecolors='k')
    axes[idx].set_title(f'LOF (n_neighbors={n_neighbors})')
    axes[idx].set_xlabel('Feature 1')
    axes[idx].set_ylabel('Feature 2')

    anomaly_count = np.sum(y_pred == -1)
    axes[idx].text(0.05, 0.95, f'Anomalies: {anomaly_count}',
                   transform=axes[idx].transAxes, verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸æ¤œçŸ¥ï¼ˆnovelty=Trueï¼‰:</strong></p>

<pre><code class="language-python">from sklearn.neighbors import LocalOutlierFactor
from sklearn.model_selection import train_test_split

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
np.random.seed(42)
X_train = np.random.randn(500, 2) * 0.5  # æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®ã¿
X_test_normal = np.random.randn(100, 2) * 0.5
X_test_outliers = np.random.uniform(low=-3, high=3, size=(10, 2))
X_test = np.vstack([X_test_normal, X_test_outliers])

# LOFï¼ˆnovelty=True: æ–°è¦ãƒ‡ãƒ¼ã‚¿äºˆæ¸¬ãƒ¢ãƒ¼ãƒ‰ï¼‰
lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1, novelty=True)
lof.fit(X_train)  # æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§å­¦ç¿’

# æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬
y_pred = lof.predict(X_test)
scores = lof.score_samples(X_test)

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.scatter(X_train[:, 0], X_train[:, 1], alpha=0.3, label='Training Data', color='blue')
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap='coolwarm',
            edgecolors='k', s=100, label='Test Data')
plt.title('LOF: æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸æ¤œçŸ¥ï¼ˆnovelty=Trueï¼‰')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.colorbar(label='Prediction (-1: Anomaly, 1: Normal)')
plt.show()

print(f"æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸ãƒ‡ãƒ¼ã‚¿æ•°: {np.sum(y_pred == -1)}/{len(y_pred)}")
</code></pre>

<hr />

<h2>3.3 One-Class SVM</h2>

<p>One-Class SVMã¯ã€æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®å¢ƒç•Œã‚’å­¦ç¿’ã—ã€ãã®å¢ƒç•Œã®å¤–å´ã«ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ç•°å¸¸ã¨ã—ã¦æ¤œå‡ºã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<h3>3.3.1 æœ€å¤§ãƒãƒ¼ã‚¸ãƒ³è¶…å¹³é¢</h3>

<p><strong>åŸºæœ¬åŸç†:</strong></p>
<ul>
<li>æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã‚’åŸç‚¹ã‹ã‚‰æœ€ã‚‚ã‚ˆãåˆ†é›¢ã™ã‚‹è¶…å¹³é¢ã‚’è¦‹ã¤ã‘ã‚‹</li>
<li>è¶…å¹³é¢ã¨ãƒ‡ãƒ¼ã‚¿ç‚¹ã®é–“ã®ãƒãƒ¼ã‚¸ãƒ³ã‚’æœ€å¤§åŒ–</li>
<li>ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã§éç·šå½¢ãªå¢ƒç•Œã‚’å­¦ç¿’</li>
</ul>

<p><strong>æ•°å¼å®šç¾©:</strong></p>

<p>æ±ºå®šé–¢æ•°:</p>

$$
f(x) = \text{sign}(w \cdot \phi(x) - \rho)
$$

<ul>
<li>$w$: æ³•ç·šãƒ™ã‚¯ãƒˆãƒ«</li>
<li>$\phi(x)$: ã‚«ãƒ¼ãƒãƒ«å¤‰æ›å¾Œã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«</li>
<li>$\rho$: ãƒã‚¤ã‚¢ã‚¹é …</li>
</ul>

<p>æœ€é©åŒ–å•é¡Œ:</p>

$$
\min_{w, \rho, \xi} \frac{1}{2} \|w\|^2 + \frac{1}{\nu n} \sum_{i=1}^{n} \xi_i - \rho
$$

<p>åˆ¶ç´„:</p>

$$
w \cdot \phi(x_i) \geq \rho - \xi_i, \quad \xi_i \geq 0
$$

<h3>3.3.2 ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯</h3>

<p><strong>ç·šå½¢ã‚«ãƒ¼ãƒãƒ«:</strong></p>

$$
K(x, x') = x \cdot x'
$$

<ul>
<li>é«˜é€Ÿã€è§£é‡ˆã—ã‚„ã™ã„</li>
<li>ç·šå½¢åˆ†é›¢å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã«é©ç”¨</li>
</ul>

<p><strong>RBFï¼ˆã‚¬ã‚¦ã‚¹ï¼‰ã‚«ãƒ¼ãƒãƒ«:</strong></p>

$$
K(x, x') = \exp\left(-\gamma \|x - x'\|^2\right)
$$

<ul>
<li>éç·šå½¢å¢ƒç•Œã‚’å­¦ç¿’å¯èƒ½</li>
<li>æœ€ã‚‚ã‚ˆãä½¿ã‚ã‚Œã‚‹ã‚«ãƒ¼ãƒãƒ«</li>
<li>$\gamma$: ã‚«ãƒ¼ãƒãƒ«å¹…ï¼ˆå¤§ãã„ã»ã©è¤‡é›‘ãªå¢ƒç•Œï¼‰</li>
</ul>

<p><strong>å¤šé …å¼ã‚«ãƒ¼ãƒãƒ«:</strong></p>

$$
K(x, x') = (\gamma x \cdot x' + r)^d
$$

<ul>
<li>æ¬¡æ•° $d$ ã®å¤šé …å¼å¢ƒç•Œ</li>
<li>RBFã‚ˆã‚Šåˆ¶ç´„ãŒå¼·ã„</li>
</ul>

<h3>3.3.3 nuãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</h3>

<p><strong>nuã®æ„å‘³:</strong></p>

<p>$\nu \in (0, 1]$ ã¯ä»¥ä¸‹ã®2ã¤ã®é‡ã®ä¸Šé™ã¨ä¸‹é™ã‚’åˆ¶å¾¡ã—ã¾ã™:</p>
<ul>
<li>è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ãŠã‘ã‚‹ç•°å¸¸å€¤ã®å‰²åˆã®<strong>ä¸Šé™</strong></li>
<li>ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ã®å‰²åˆã®<strong>ä¸‹é™</strong></li>
</ul>

<p><strong>æ¨å¥¨å€¤:</strong></p>
<ul>
<li>$\nu = 0.1$: 10%ãŒç•°å¸¸ã¨æƒ³å®š</li>
<li>$\nu = 0.05$: 5%ãŒç•°å¸¸ã¨æƒ³å®š</li>
<li>$\nu = 0.01$: 1%ãŒç•°å¸¸ã¨æƒ³å®š</li>
</ul>

<p><strong>æ³¨æ„ç‚¹:</strong></p>
<ul>
<li>$\nu$ ã‚’å°ã•ãã—ã™ãã‚‹ã¨ã€ã»ã¨ã‚“ã©ç•°å¸¸ãŒæ¤œå‡ºã•ã‚Œãªã„</li>
<li>$\nu$ ã‚’å¤§ããã—ã™ãã‚‹ã¨ã€æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã‚‚ç•°å¸¸ã¨åˆ¤å®šã•ã‚Œã‚‹</li>
<li>ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã‚„äº‹å‰ã®ç•°å¸¸ç‡ã‹ã‚‰è¨­å®š</li>
</ul>

<h3>3.3.4 scikit-learnå®Ÿè£…</h3>

<p><strong>åŸºæœ¬å®Ÿè£…:</strong></p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import OneClassSVM

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
X_train = np.random.randn(200, 2) * 0.5  # æ­£å¸¸ãƒ‡ãƒ¼ã‚¿
X_test_normal = np.random.randn(50, 2) * 0.5
X_test_outliers = np.random.uniform(low=-3, high=3, size=(10, 2))
X_test = np.vstack([X_test_normal, X_test_outliers])

# One-Class SVM
oc_svm = OneClassSVM(
    kernel='rbf',  # RBFã‚«ãƒ¼ãƒãƒ«
    gamma='auto',  # gamma = 1 / n_features
    nu=0.1  # 10%ãŒç•°å¸¸ã¨æƒ³å®š
)

# å­¦ç¿’
oc_svm.fit(X_train)

# äºˆæ¸¬
y_pred_train = oc_svm.predict(X_train)
y_pred_test = oc_svm.predict(X_test)
scores_test = oc_svm.decision_function(X_test)

# æ±ºå®šå¢ƒç•Œã®å¯è¦–åŒ–
xx, yy = np.meshgrid(np.linspace(-3, 3, 500), np.linspace(-3, 3, 500))
Z = oc_svm.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.Blues_r)
plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')
plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=20, edgecolors='k', label='Training')
plt.title('One-Class SVM: æ±ºå®šå¢ƒç•Œ')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()

plt.subplot(1, 2, 2)
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_test, cmap='coolwarm',
            edgecolors='k', s=100)
plt.title('One-Class SVM: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿äºˆæ¸¬')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Prediction (-1: Anomaly, 1: Normal)')

plt.tight_layout()
plt.show()

print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸æ•°: {np.sum(y_pred_train == -1)}/{len(y_pred_train)}")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸æ•°: {np.sum(y_pred_test == -1)}/{len(y_pred_test)}")
</code></pre>

<p><strong>gammaãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿:</strong></p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import OneClassSVM

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
X_train = np.random.randn(200, 2) * 0.5

# ç•°ãªã‚‹gammaã§æ¯”è¼ƒ
gamma_list = [0.01, 0.1, 1.0]

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for idx, gamma in enumerate(gamma_list):
    oc_svm = OneClassSVM(kernel='rbf', gamma=gamma, nu=0.1)
    oc_svm.fit(X_train)

    # æ±ºå®šå¢ƒç•Œ
    xx, yy = np.meshgrid(np.linspace(-3, 3, 300), np.linspace(-3, 3, 300))
    Z = oc_svm.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    axes[idx].contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.Blues_r)
    axes[idx].contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')
    axes[idx].scatter(X_train[:, 0], X_train[:, 1], c='white', s=20, edgecolors='k')
    axes[idx].set_title(f'One-Class SVM (gamma={gamma})')
    axes[idx].set_xlabel('Feature 1')
    axes[idx].set_ylabel('Feature 2')

plt.tight_layout()
plt.show()
</code></pre>

<hr />

<h2>3.4 ãã®ä»–ã®æ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•</h2>

<h3>3.4.1 DBSCANï¼ˆå¯†åº¦ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼‰</h3>

<p><strong>åŸç†:</strong></p>
<ul>
<li>å¯†åº¦ã®é«˜ã„é ˜åŸŸã‚’ã‚¯ãƒ©ã‚¹ã‚¿ã¨ã—ã¦æ¤œå‡º</li>
<li>ã©ã®ã‚¯ãƒ©ã‚¹ã‚¿ã«ã‚‚å±ã•ãªã„ç‚¹ã‚’ãƒã‚¤ã‚ºï¼ˆç•°å¸¸ï¼‰ã¨ã¿ãªã™</li>
<li>ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’äº‹å‰ã«æŒ‡å®šã™ã‚‹å¿…è¦ãŒãªã„</li>
</ul>

<p><strong>ä¸»è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:</strong></p>
<ul>
<li><code>eps</code>: è¿‘å‚ã®åŠå¾„ï¼ˆè·é›¢ã®é–¾å€¤ï¼‰</li>
<li><code>min_samples</code>: ã‚³ã‚¢ç‚¹ã«ãªã‚‹ãŸã‚ã®æœ€å°è¿‘å‚ç‚¹æ•°</li>
</ul>

<p><strong>å®Ÿè£…ä¾‹:</strong></p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
X_cluster1 = np.random.randn(100, 2) * 0.3 + [0, 0]
X_cluster2 = np.random.randn(100, 2) * 0.3 + [3, 3]
X_outliers = np.random.uniform(low=-2, high=5, size=(20, 2))
X = np.vstack([X_cluster1, X_cluster2, X_outliers])

# DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X)

# ãƒ©ãƒ™ãƒ«-1ãŒãƒã‚¤ã‚ºï¼ˆç•°å¸¸ï¼‰
outliers = labels == -1

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.scatter(X[~outliers, 0], X[~outliers, 1], c=labels[~outliers],
            cmap='viridis', edgecolors='k', label='Clusters')
plt.scatter(X[outliers, 0], X[outliers, 1], c='red', marker='x',
            s=100, label='Outliers (Anomalies)')
plt.title('DBSCAN: å¯†åº¦ãƒ™ãƒ¼ã‚¹ç•°å¸¸æ¤œçŸ¥')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.show()

print(f"æ¤œå‡ºã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(set(labels)) - (1 if -1 in labels else 0)}")
print(f"ç•°å¸¸ãƒ‡ãƒ¼ã‚¿æ•°: {np.sum(outliers)}")
</code></pre>

<h3>3.4.2 Elliptic Envelopeï¼ˆæ¥•å††ã‚¨ãƒ³ãƒ™ãƒ­ãƒ¼ãƒ—ï¼‰</h3>

<p><strong>åŸç†:</strong></p>
<ul>
<li>æ­£è¦åˆ†å¸ƒã‚’ä»®å®šã—ã€ãƒ‡ãƒ¼ã‚¿ã®ä¸­å¿ƒã¨å…±åˆ†æ•£ã‚’æ¨å®š</li>
<li>ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã§ç•°å¸¸ã‚’æ¤œå‡º</li>
<li>ãƒ­ãƒã‚¹ãƒˆæ¨å®šï¼ˆMinimum Covariance Determinantï¼‰ã§å¤–ã‚Œå€¤ã®å½±éŸ¿ã‚’æŠ‘ãˆã‚‹</li>
</ul>

<p><strong>å®Ÿè£…ä¾‹:</strong></p>

<pre><code class="language-python">from sklearn.covariance import EllipticEnvelope
import numpy as np
import matplotlib.pyplot as plt

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
X_normal = np.random.randn(200, 2)
X_outliers = np.random.uniform(low=-5, high=5, size=(10, 2))
X = np.vstack([X_normal, X_outliers])

# Elliptic Envelope
elliptic = EllipticEnvelope(contamination=0.1, random_state=42)
y_pred = elliptic.fit_predict(X)

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='coolwarm', edgecolors='k')
plt.title('Elliptic Envelope: ç•°å¸¸æ¤œçŸ¥')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Prediction (-1: Anomaly, 1: Normal)')
plt.show()

print(f"æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸ãƒ‡ãƒ¼ã‚¿æ•°: {np.sum(y_pred == -1)}")
</code></pre>

<h3>3.4.3 Robust Covarianceï¼ˆãƒ­ãƒã‚¹ãƒˆå…±åˆ†æ•£æ¨å®šï¼‰</h3>

<p><strong>Minimum Covariance Determinantï¼ˆMCDï¼‰:</strong></p>
<ul>
<li>å…±åˆ†æ•£è¡Œåˆ—ã®è¡Œåˆ—å¼ã‚’æœ€å°åŒ–ã™ã‚‹ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’æ¢ç´¢</li>
<li>å¤–ã‚Œå€¤ã«å¯¾ã—ã¦ãƒ­ãƒã‚¹ãƒˆãªæ¨å®š</li>
<li>ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã®è¨ˆç®—ã«ä½¿ç”¨</li>
</ul>

<pre><code class="language-python">from sklearn.covariance import MinCovDet
import numpy as np

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
X = np.random.randn(100, 2)
X[:5] = X[:5] + 5  # å¤–ã‚Œå€¤è¿½åŠ 

# MCDæ¨å®š
mcd = MinCovDet(random_state=42)
mcd.fit(X)

# ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã®è¨ˆç®—
distances = mcd.mahalanobis(X)

# ç•°å¸¸åˆ¤å®šï¼ˆã‚«ã‚¤äºŒä¹—åˆ†å¸ƒã®95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼‰
from scipy import stats
threshold = stats.chi2.ppf(0.95, df=2)
outliers = distances > threshold

print(f"ç•°å¸¸ãƒ‡ãƒ¼ã‚¿æ•°: {np.sum(outliers)}")
print(f"è·é›¢ã®é–¾å€¤: {threshold:.2f}")
</code></pre>

<h3>3.4.4 PyODãƒ©ã‚¤ãƒ–ãƒ©ãƒª</h3>

<p><strong>PyODï¼ˆPython Outlier Detectionï¼‰</strong>ã¯ã€ç•°å¸¸æ¤œçŸ¥å°‚é–€ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§40ä»¥ä¸Šã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æä¾›ã—ã¾ã™ã€‚</p>

<p><strong>ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:</strong></p>

<pre><code class="language-bash">pip install pyod
</code></pre>

<p><strong>ä½¿ç”¨ä¾‹:</strong></p>

<pre><code class="language-python">from pyod.models.knn import KNN
from pyod.models.iforest import IForest
from pyod.models.lof import LOF
from pyod.utils.data import generate_data
from pyod.utils.utility import standardizer
import numpy as np

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
X_train, X_test, y_train, y_test = generate_data(
    n_train=200, n_test=100, n_features=2,
    contamination=0.1, random_state=42
)

# ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
X_train = standardizer(X_train)
X_test = standardizer(X_test)

# è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ
models = {
    'KNN': KNN(contamination=0.1),
    'IForest': IForest(contamination=0.1, random_state=42),
    'LOF': LOF(contamination=0.1)
}

for name, model in models.items():
    model.fit(X_train)
    y_pred = model.predict(X_test)
    scores = model.decision_function(X_test)

    # è©•ä¾¡ï¼ˆä»®æƒ³çš„ãªæ­£è§£ãƒ©ãƒ™ãƒ«ã§ï¼‰
    from sklearn.metrics import roc_auc_score
    auc = roc_auc_score(y_test, scores)

    print(f"{name}:")
    print(f"  AUC-ROC: {auc:.3f}")
    print(f"  æ¤œå‡ºç•°å¸¸æ•°: {np.sum(y_pred == 1)}")
    print()
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹:</strong></p>
<pre><code>KNN:
  AUC-ROC: 0.892
  æ¤œå‡ºç•°å¸¸æ•°: 10

IForest:
  AUC-ROC: 0.915
  æ¤œå‡ºç•°å¸¸æ•°: 10

LOF:
  AUC-ROC: 0.903
  æ¤œå‡ºç•°å¸¸æ•°: 10
</code></pre>

<hr />

<h2>3.5 ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç•°å¸¸æ¤œçŸ¥</h2>

<p>è¤‡æ•°ã®ç•°å¸¸æ¤œçŸ¥ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šé«˜ç²¾åº¦ã§å®‰å®šã—ãŸç•°å¸¸æ¤œçŸ¥ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚</p>

<h3>3.5.1 Feature Bagging</h3>

<p><strong>åŸç†:</strong></p>
<ul>
<li>ç‰¹å¾´é‡ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ</li>
<li>å„ã‚µãƒ–ã‚»ãƒƒãƒˆã§ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´</li>
<li>è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’é›†ç´„</li>
</ul>

<p><strong>å®Ÿè£…ä¾‹:</strong></p>

<pre><code class="language-python">from pyod.models.feature_bagging import FeatureBagging
from pyod.models.lof import LOF
from pyod.utils.data import generate_data
from sklearn.metrics import roc_auc_score

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
X_train, X_test, y_train, y_test = generate_data(
    n_train=200, n_test=100, n_features=10,
    contamination=0.1, random_state=42
)

# Feature Baggingï¼ˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: LOFï¼‰
fb = FeatureBagging(
    base_estimator=LOF(),
    n_estimators=10,  # ãƒ¢ãƒ‡ãƒ«æ•°
    contamination=0.1,
    random_state=42
)

# å­¦ç¿’ã¨äºˆæ¸¬
fb.fit(X_train)
y_pred = fb.predict(X_test)
scores = fb.decision_function(X_test)

# è©•ä¾¡
auc = roc_auc_score(y_test, scores)
print(f"Feature Bagging AUC-ROC: {auc:.3f}")
print(f"æ¤œå‡ºç•°å¸¸æ•°: {np.sum(y_pred == 1)}")
</code></pre>

<h3>3.5.2 ãƒ¢ãƒ‡ãƒ«å¹³å‡åŒ–ï¼ˆModel Averagingï¼‰</h3>

<p><strong>åŸç†:</strong></p>
<ul>
<li>è¤‡æ•°ã®ç•°ãªã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è¨“ç·´</li>
<li>å„ãƒ¢ãƒ‡ãƒ«ã®ç•°å¸¸ã‚¹ã‚³ã‚¢ã‚’å¹³å‡åŒ–</li>
<li>å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šé ‘å¥ãªäºˆæ¸¬</li>
</ul>

<p><strong>å®Ÿè£…ä¾‹:</strong></p>

<pre><code class="language-python">from pyod.models.combination import average, maximization
from pyod.models.knn import KNN
from pyod.models.iforest import IForest
from pyod.models.lof import LOF
from pyod.utils.data import generate_data
import numpy as np

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
X_train, X_test, y_train, y_test = generate_data(
    n_train=200, n_test=100, n_features=5,
    contamination=0.1, random_state=42
)

# è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
models = [
    KNN(contamination=0.1),
    IForest(contamination=0.1, random_state=42),
    LOF(contamination=0.1)
]

# å„ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
scores_list = []
for model in models:
    model.fit(X_train)
    scores = model.decision_function(X_test)
    scores_list.append(scores)

scores_array = np.array(scores_list)

# ã‚¹ã‚³ã‚¢é›†ç´„ï¼ˆå¹³å‡ï¼‰
scores_avg = average(scores_array)

# ã‚¹ã‚³ã‚¢é›†ç´„ï¼ˆæœ€å¤§å€¤ï¼‰
scores_max = maximization(scores_array)

# è©•ä¾¡
from sklearn.metrics import roc_auc_score
auc_avg = roc_auc_score(y_test, scores_avg)
auc_max = roc_auc_score(y_test, scores_max)

print(f"Average Combination AUC-ROC: {auc_avg:.3f}")
print(f"Maximum Combination AUC-ROC: {auc_max:.3f}")
</code></pre>

<h3>3.5.3 Isolation-Based Ensemble</h3>

<p><strong>LSCPï¼ˆLocally Selective Combination in Parallelï¼‰:</strong></p>
<ul>
<li>å„ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã—ã¦å±€æ‰€çš„ã«æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ</li>
<li>è¿‘å‚ã§ã®æ€§èƒ½ã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚’é‡ã¿ä»˜ã‘</li>
<li>ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªå¹³å‡åŒ–ã‚ˆã‚Šé«˜ç²¾åº¦</li>
</ul>

<pre><code class="language-python">from pyod.models.lscp import LSCP
from pyod.models.knn import KNN
from pyod.models.iforest import IForest
from pyod.models.lof import LOF
from pyod.utils.data import generate_data

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
X_train, X_test, y_train, y_test = generate_data(
    n_train=200, n_test=100, n_features=5,
    contamination=0.1, random_state=42
)

# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ
detector_list = [
    KNN(),
    IForest(random_state=42),
    LOF()
]

# LSCP
lscp = LSCP(detector_list, contamination=0.1, random_state=42)
lscp.fit(X_train)

# äºˆæ¸¬
y_pred = lscp.predict(X_test)
scores = lscp.decision_function(X_test)

# è©•ä¾¡
from sklearn.metrics import roc_auc_score
auc = roc_auc_score(y_test, scores)
print(f"LSCP AUC-ROC: {auc:.3f}")
print(f"æ¤œå‡ºç•°å¸¸æ•°: {np.sum(y_pred == 1)}")
</code></pre>

<h3>3.5.4 å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¾‹</h3>

<p><strong>ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç† â†’ è¤‡æ•°ãƒ¢ãƒ‡ãƒ«è¨“ç·´ â†’ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« â†’ è©•ä¾¡:</strong></p>

<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from pyod.models.knn import KNN
from pyod.models.iforest import IForest
from pyod.models.lof import LOF
from pyod.models.ocsvm import OCSVM
from pyod.models.combination import average
from sklearn.metrics import classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ã®ä»£æ›¿ï¼‰
np.random.seed(42)
n_samples = 1000
n_features = 10
contamination = 0.05

# æ­£å¸¸ãƒ‡ãƒ¼ã‚¿
X_normal = np.random.randn(int(n_samples * (1 - contamination)), n_features)
# ç•°å¸¸ãƒ‡ãƒ¼ã‚¿
X_anomaly = np.random.uniform(low=-5, high=5, size=(int(n_samples * contamination), n_features))
X = np.vstack([X_normal, X_anomaly])
y = np.hstack([np.zeros(len(X_normal)), np.ones(len(X_anomaly))])

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

# ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
models = {
    'KNN': KNN(contamination=contamination),
    'IForest': IForest(contamination=contamination, random_state=42),
    'LOF': LOF(contamination=contamination),
    'OCSVM': OCSVM(contamination=contamination)
}

scores_dict = {}
predictions_dict = {}

for name, model in models.items():
    model.fit(X_train_scaled)
    scores = model.decision_function(X_test_scaled)
    y_pred = model.predict(X_test_scaled)

    scores_dict[name] = scores
    predictions_dict[name] = y_pred

    auc = roc_auc_score(y_test, scores)
    print(f"{name} AUC-ROC: {auc:.3f}")

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆå¹³å‡ï¼‰
scores_list = [scores_dict[name] for name in models.keys()]
scores_ensemble = average(np.array(scores_list))
auc_ensemble = roc_auc_score(y_test, scores_ensemble)
print(f"\nEnsemble AUC-ROC: {auc_ensemble:.3f}")

# ROCæ›²ç·šã®å¯è¦–åŒ–
plt.figure(figsize=(10, 6))

for name, scores in scores_dict.items():
    fpr, tpr, _ = roc_curve(y_test, scores)
    auc_val = roc_auc_score(y_test, scores)
    plt.plot(fpr, tpr, label=f'{name} (AUC={auc_val:.3f})')

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®ROC
fpr_ens, tpr_ens, _ = roc_curve(y_test, scores_ensemble)
plt.plot(fpr_ens, tpr_ens, 'k--', linewidth=2,
         label=f'Ensemble (AUC={auc_ensemble:.3f})')

plt.plot([0, 1], [0, 1], 'r--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«')
plt.legend()
plt.grid(alpha=0.3)
plt.show()
</code></pre>

<hr />

<h2>3.6 ã¾ã¨ã‚</h2>

<h3>æœ¬ç« ã§å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li>
<p><strong>Isolation Forest:</strong></p>
<ul>
<li>ãƒ©ãƒ³ãƒ€ãƒ åˆ†é›¢ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥</li>
<li>ãƒ‘ã‚¹é•·ã‹ã‚‰ç•°å¸¸ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—</li>
<li>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆn_estimators, max_samples, contaminationï¼‰</li>
<li>é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã«åŠ¹æœçš„</li>
</ul>
</li>
<li>
<p><strong>LOFï¼ˆLocal Outlier Factorï¼‰:</strong></p>
<ul>
<li>å±€æ‰€å¯†åº¦ã«åŸºã¥ãç•°å¸¸æ¤œçŸ¥</li>
<li>åˆ°é”å¯èƒ½è·é›¢ã¨å±€æ‰€åˆ°é”å¯èƒ½å¯†åº¦</li>
<li>LOFã‚¹ã‚³ã‚¢ã®è¨ˆç®—ã¨è§£é‡ˆ</li>
<li>å¯†åº¦ãŒç•°ãªã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ã«å¯¾å¿œ</li>
</ul>
</li>
<li>
<p><strong>One-Class SVM:</strong></p>
<ul>
<li>æœ€å¤§ãƒãƒ¼ã‚¸ãƒ³è¶…å¹³é¢ã«ã‚ˆã‚‹å¢ƒç•Œå­¦ç¿’</li>
<li>ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ï¼ˆRBF, ç·šå½¢, å¤šé …å¼ï¼‰</li>
<li>nuãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ç•°å¸¸ç‡åˆ¶å¾¡</li>
<li>éç·šå½¢å¢ƒç•Œã®å­¦ç¿’</li>
</ul>
</li>
<li>
<p><strong>ãã®ä»–ã®æ‰‹æ³•:</strong></p>
<ul>
<li>DBSCANï¼ˆå¯†åº¦ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼‰</li>
<li>Elliptic Envelopeï¼ˆæ¥•å††ã‚¨ãƒ³ãƒ™ãƒ­ãƒ¼ãƒ—ï¼‰</li>
<li>Robust Covarianceï¼ˆãƒ­ãƒã‚¹ãƒˆå…±åˆ†æ•£æ¨å®šï¼‰</li>
<li>PyODãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆ40ä»¥ä¸Šã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰</li>
</ul>
</li>
<li>
<p><strong>ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç•°å¸¸æ¤œçŸ¥:</strong></p>
<ul>
<li>Feature Baggingï¼ˆç‰¹å¾´é‡ã‚µãƒ–ã‚»ãƒƒãƒˆï¼‰</li>
<li>Model Averagingï¼ˆã‚¹ã‚³ã‚¢å¹³å‡åŒ–ï¼‰</li>
<li>Isolation-Based Ensembleï¼ˆLSCPï¼‰</li>
<li>è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Š</li>
</ul>
</li>
</ol>

<h3>æ‰‹æ³•ã®ä½¿ã„åˆ†ã‘</h3>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>é©ç”¨å ´é¢</th>
<th>é•·æ‰€</th>
<th>çŸ­æ‰€</th>
</tr>
</thead>
<tbody>
<tr>
<td>Isolation Forest</td>
<td>é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿</td>
<td>é«˜é€Ÿã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«</td>
<td>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãŒå¿…è¦</td>
</tr>
<tr>
<td>LOF</td>
<td>å¯†åº¦ãŒç•°ãªã‚‹ã‚¯ãƒ©ã‚¹ã‚¿</td>
<td>å±€æ‰€çš„ãªç•°å¸¸ã‚’æ¤œå‡º</td>
<td>è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„</td>
</tr>
<tr>
<td>One-Class SVM</td>
<td>éç·šå½¢å¢ƒç•Œã€ç†è«–çš„ä¿è¨¼</td>
<td>é ‘å¥ã€ç†è«–çš„åŸºç›¤</td>
<td>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§é…ã„</td>
</tr>
<tr>
<td>DBSCAN</td>
<td>ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° + ç•°å¸¸æ¤œçŸ¥</td>
<td>ã‚¯ãƒ©ã‚¹ã‚¿æ•°ä¸è¦</td>
<td>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«æ•æ„Ÿ</td>
</tr>
<tr>
<td>ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«</td>
<td>é«˜ç²¾åº¦ãŒå¿…è¦ãªå ´é¢</td>
<td>é ‘å¥ã€é«˜ç²¾åº¦</td>
<td>è¨ˆç®—ã‚³ã‚¹ãƒˆå¢—åŠ </td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>

<p>ç¬¬4ç« ã§ã¯ã€æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥ã‚’å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>Autoencoderï¼ˆå†æ§‹æˆèª¤å·®ãƒ™ãƒ¼ã‚¹ï¼‰</li>
<li>VAEï¼ˆVariational Autoencoderï¼‰</li>
<li>GANï¼ˆGenerative Adversarial Networkï¼‰</li>
<li>LSTM Autoencoderï¼ˆæ™‚ç³»åˆ—ç•°å¸¸æ¤œçŸ¥ï¼‰</li>
<li>Transformerï¼ˆAttentionæ©Ÿæ§‹ï¼‰</li>
</ul>

<hr />

<h2>æ¼”ç¿’å•é¡Œ</h2>

<details>
<summary><strong>å•1:</strong> Isolation Forestã§ã€ç•°å¸¸ã‚¹ã‚³ã‚¢ $s(x, n) = 0.8$ ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã¯ç•°å¸¸ã¨åˆ¤å®šã™ã¹ãã‹ï¼Ÿç†ç”±ã¨ã¨ã‚‚ã«ç­”ãˆã‚ˆã€‚</summary>

<p><strong>è§£ç­”:</strong></p>
<p>ã¯ã„ã€ç•°å¸¸ã¨åˆ¤å®šã™ã¹ãã§ã™ã€‚</p>

<p><strong>ç†ç”±:</strong></p>
<ul>
<li>ç•°å¸¸ã‚¹ã‚³ã‚¢ $s \approx 1$ ã¯æ˜ç¢ºãªç•°å¸¸ã‚’ç¤ºã™</li>
<li>$s \approx 0.5$ ã¯æ­£å¸¸ï¼ˆå¹³å‡çš„ãªãƒ‘ã‚¹é•·ï¼‰</li>
<li>$s = 0.8$ ã¯1ã«è¿‘ãã€é€šå¸¸ã‚ˆã‚Šæ—©ãå­¤ç«‹ã—ã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹</li>
<li>ä¸€èˆ¬çš„ã« $s > 0.6$ ã‚’ç•°å¸¸ã¨ã¿ãªã™é–¾å€¤ã¨ã—ã¦ä½¿ç”¨</li>
</ul>
</details>

<details>
<summary><strong>å•2:</strong> LOFã‚¹ã‚³ã‚¢ãŒ $\text{LOF}_k(p) = 2.5$ ã®å ´åˆã€ã“ã®ç‚¹ã¯ç•°å¸¸ã‹ï¼Ÿã¾ãŸã€ã“ã®ã‚¹ã‚³ã‚¢ãŒæ„å‘³ã™ã‚‹ã“ã¨ã‚’èª¬æ˜ã›ã‚ˆã€‚</summary>

<p><strong>è§£ç­”:</strong></p>
<p>ã¯ã„ã€ç•°å¸¸ã§ã™ã€‚</p>

<p><strong>æ„å‘³:</strong></p>
<ul>
<li>$\text{LOF} \approx 1$ ã¯è¿‘å‚ã¨åŒç¨‹åº¦ã®å¯†åº¦ï¼ˆæ­£å¸¸ï¼‰</li>
<li>$\text{LOF} > 1$ ã¯è¿‘å‚ã‚ˆã‚Šå¯†åº¦ãŒä½ã„ï¼ˆç•°å¸¸ã®å¯èƒ½æ€§ï¼‰</li>
<li>$\text{LOF} = 2.5$ ã¯ã€ã“ã®ç‚¹ã®å¯†åº¦ãŒè¿‘å‚ã®å¹³å‡å¯†åº¦ã®ç´„1/2.5ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™</li>
<li>ä¸€èˆ¬çš„ã« $\text{LOF} > 1.5$ ã‚’ç•°å¸¸ã¨ã¿ãªã™ãŸã‚ã€2.5ã¯æ˜ç¢ºãªç•°å¸¸</li>
</ul>
</details>

<details>
<summary><strong>å•3:</strong> One-Class SVMã®nuãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’0.05ã«è¨­å®šã—ãŸå ´åˆã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ä½•%ãŒç•°å¸¸ã¨åˆ¤å®šã•ã‚Œã‚‹ã‹ï¼Ÿã¾ãŸã€nuã‚’å¤§ããã—ãŸå ´åˆã®å½±éŸ¿ã‚’èª¬æ˜ã›ã‚ˆã€‚</summary>

<p><strong>è§£ç­”:</strong></p>
<p>è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æœ€å¤§5%ãŒç•°å¸¸ã¨åˆ¤å®šã•ã‚Œã¾ã™ã€‚</p>

<p><strong>nuã‚’å¤§ããã—ãŸå ´åˆã®å½±éŸ¿:</strong></p>
<ul>
<li>$\nu = 0.1$: æœ€å¤§10%ãŒç•°å¸¸ã¨åˆ¤å®šã•ã‚Œã‚‹</li>
<li>$\nu = 0.2$: æœ€å¤§20%ãŒç•°å¸¸ã¨åˆ¤å®šã•ã‚Œã‚‹</li>
<li>nuã‚’å¤§ããã™ã‚‹ã¨ã€ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ãŒç•°å¸¸ã¨åˆ¤å®šã•ã‚Œã‚‹</li>
<li>æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã‚‚ç•°å¸¸ã¨èª¤åˆ¤å®šã•ã‚Œã‚‹ãƒªã‚¹ã‚¯ãŒå¢—åŠ ï¼ˆå½é™½æ€§å¢—åŠ ï¼‰</li>
<li>ç•°å¸¸æ¤œçŸ¥ã®æ„Ÿåº¦ãŒé«˜ããªã‚‹ãŒã€ç²¾åº¦ã¯ä½ä¸‹ã™ã‚‹å¯èƒ½æ€§</li>
</ul>
</details>

<details>
<summary><strong>å•4:</strong> DBSCANã§ç•°å¸¸æ¤œçŸ¥ã‚’è¡Œã†éš›ã€epsã¨min_samplesãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã©ã®ã‚ˆã†ã«é¸æŠã™ã¹ãã‹ï¼Ÿå…·ä½“çš„ãªé¸æŠæ–¹æ³•ã‚’3ã¤è¿°ã¹ã‚ˆã€‚</summary>

<p><strong>è§£ç­”:</strong></p>

<ol>
<li>
<p><strong>Kè·é›¢ã‚°ãƒ©ãƒ•æ³•:</strong></p>
<ul>
<li>å„ç‚¹ã®kç•ªç›®ã®æœ€è¿‘å‚è·é›¢ã‚’è¨ˆç®—ï¼ˆkã¯min_samplesã®å€™è£œï¼‰</li>
<li>è·é›¢ã‚’é™é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ãƒ—ãƒ­ãƒƒãƒˆ</li>
<li>æ€¥æ¿€ã«å¢—åŠ ã™ã‚‹ç‚¹ï¼ˆã‚¨ãƒ«ãƒœãƒ¼ç‚¹ï¼‰ã‚’epsã¨ã—ã¦é¸æŠ</li>
</ul>
</li>
<li>
<p><strong>ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«åŸºã¥ãé¸æŠ:</strong></p>
<ul>
<li>ãƒ‡ãƒ¼ã‚¿ã®æ€§è³ªã‹ã‚‰é©åˆ‡ãªè¿‘å‚ã‚µã‚¤ã‚ºã‚’æ¨å®š</li>
<li>ä¾‹: 2æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ãªã‚‰ min_samples=4ã€é«˜æ¬¡å…ƒãªã‚‰ min_samples=2Ã—æ¬¡å…ƒæ•°</li>
<li>epsã¯ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒ«ã«å¿œã˜ã¦èª¿æ•´</li>
</ul>
</li>
<li>
<p><strong>ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ:</strong></p>
<ul>
<li>è¤‡æ•°ã®(eps, min_samples)ã®çµ„ã¿åˆã‚ã›ã‚’è©¦ã™</li>
<li>ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ã‚„ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã§è©•ä¾¡</li>
<li>æœ€é©ãªçµ„ã¿åˆã‚ã›ã‚’é¸æŠ</li>
</ul>
</li>
</ol>
</details>

<details>
<summary><strong>å•5:</strong> ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç•°å¸¸æ¤œçŸ¥ã§ã€Feature Baggingã¨ãƒ¢ãƒ‡ãƒ«å¹³å‡åŒ–ã®é•ã„ã‚’èª¬æ˜ã—ã€ãã‚Œãã‚ŒãŒã©ã®ã‚ˆã†ãªçŠ¶æ³ã§æœ‰åŠ¹ã‹è«–ã˜ã‚ˆï¼ˆ300å­—ä»¥å†…ï¼‰ã€‚</summary>

<p><strong>è§£ç­”ä¾‹:</strong></p>

<p>Feature Baggingã¯ç‰¹å¾´é‡ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã§è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã«ãŠã‘ã‚‹ç‰¹å¾´é‡é–“ã®ç›¸é–¢ã‚„å†—é•·æ€§ã«å¯¾å‡¦ã—ã¾ã™ã€‚ç‰¹å¾´é‡ãŒå¤šãç›¸é–¢ãŒå¼·ã„å ´åˆã«æœ‰åŠ¹ã§ã™ã€‚ä¸€æ–¹ã€ãƒ¢ãƒ‡ãƒ«å¹³å‡åŒ–ã¯ç•°ãªã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆKNNã€Isolation Forestã€LOFãªã©ï¼‰ã®äºˆæ¸¬ã‚’é›†ç´„ã—ã€å„æ‰‹æ³•ã®é•·æ‰€ã‚’æ´»ã‹ã—ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã®æ€§è³ªãŒä¸æ˜ç¢ºã§ã€ã©ã®æ‰‹æ³•ãŒæœ€é©ã‹äº‹å‰ã«ã‚ã‹ã‚‰ãªã„å ´åˆã«æœ‰åŠ¹ã§ã™ã€‚Feature Baggingã¯åŒä¸€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å¤šæ§˜æ€§ã‚’é«˜ã‚ã€ãƒ¢ãƒ‡ãƒ«å¹³å‡åŒ–ã¯ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å¤šæ§˜æ€§ã‚’æ´»ç”¨ã™ã‚‹ç‚¹ãŒä¸»ãªé•ã„ã§ã™ã€‚å®Ÿå‹™ã§ã¯ä¸¡æ–¹ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šé ‘å¥ãªç•°å¸¸æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚</p>
</details>

<hr />

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Liu, F. T., Ting, K. M., &amp; Zhou, Z. H. (2008). "Isolation Forest." <em>IEEE International Conference on Data Mining (ICDM)</em>.</li>
<li>Breunig, M. M., Kriegel, H. P., Ng, R. T., &amp; Sander, J. (2000). "LOF: Identifying Density-Based Local Outliers." <em>ACM SIGMOD International Conference on Management of Data</em>.</li>
<li>SchÃ¶lkopf, B., Platt, J. C., Shawe-Taylor, J., Smola, A. J., &amp; Williamson, R. C. (2001). "Estimating the Support of a High-Dimensional Distribution." <em>Neural Computation</em>, 13(7), 1443-1471.</li>
<li>Ester, M., Kriegel, H. P., Sander, J., &amp; Xu, X. (1996). "A Density-Based Algorithm for Discovering Clusters." <em>KDD</em>, 96(34), 226-231.</li>
<li>Zhao, Y., Nasrullah, Z., &amp; Li, Z. (2019). "PyOD: A Python Toolbox for Scalable Outlier Detection." <em>Journal of Machine Learning Research</em>, 20(96), 1-7.</li>
</ol>

<hr />

<p><strong>æ¬¡ç« </strong>: <a href="chapter4-deep-learning-anomaly.html">ç¬¬4ç« ï¼šæ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥</a></p>

<p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: ã“ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯CC BY 4.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§æä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚</p>

<div class="navigation">
    <a href="chapter2-statistical-methods.html" class="nav-button">â† å‰ã®ç« </a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter4-deep-learning-anomaly.html" class="nav-button">æ¬¡ã®ç«  â†’</a>
</div>
    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
