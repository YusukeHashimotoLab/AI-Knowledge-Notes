<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬3ç« ï¼šã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³• - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: var(--font-body); line-height: 1.7; color: var(--color-text); background-color: var(--color-bg); font-size: 16px; }
        header { background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; padding: var(--spacing-xl) var(--spacing-md); margin-bottom: var(--spacing-xl); box-shadow: var(--box-shadow); }
        .header-content { max-width: 900px; margin: 0 auto; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: var(--spacing-sm); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; opacity: 0.95; font-weight: 400; margin-bottom: var(--spacing-md); }
        .meta { display: flex; flex-wrap: wrap; gap: var(--spacing-md); font-size: 0.9rem; opacity: 0.9; }
        .meta-item { display: flex; align-items: center; gap: 0.3rem; }
        .container { max-width: 900px; margin: 0 auto; padding: 0 var(--spacing-md) var(--spacing-xl); }
        h2 { font-size: 1.75rem; color: var(--color-primary); margin-top: var(--spacing-xl); margin-bottom: var(--spacing-md); padding-bottom: var(--spacing-xs); border-bottom: 3px solid var(--color-accent); }
        h3 { font-size: 1.4rem; color: var(--color-primary); margin-top: var(--spacing-lg); margin-bottom: var(--spacing-sm); }
        h4 { font-size: 1.1rem; color: var(--color-primary-dark); margin-top: var(--spacing-md); margin-bottom: var(--spacing-sm); }
        p { margin-bottom: var(--spacing-md); color: var(--color-text); }
        a { color: var(--color-link); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--color-link-hover); text-decoration: underline; }
        ul, ol { margin-left: var(--spacing-lg); margin-bottom: var(--spacing-md); }
        li { margin-bottom: var(--spacing-xs); color: var(--color-text); }
        pre { background-color: var(--color-code-bg); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); overflow-x: auto; margin-bottom: var(--spacing-md); font-family: var(--font-mono); font-size: 0.9rem; line-height: 1.5; }
        code { font-family: var(--font-mono); font-size: 0.9em; background-color: var(--color-code-bg); padding: 0.2em 0.4em; border-radius: 3px; }
        pre code { background-color: transparent; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: var(--spacing-md); font-size: 0.95rem; }
        th, td { border: 1px solid var(--color-border); padding: var(--spacing-sm); text-align: left; }
        th { background-color: var(--color-bg-alt); font-weight: 600; color: var(--color-primary); }
        blockquote { border-left: 4px solid var(--color-accent); padding-left: var(--spacing-md); margin: var(--spacing-md) 0; color: var(--color-text-light); font-style: italic; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        .mermaid { text-align: center; margin: var(--spacing-lg) 0; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        details { background-color: var(--color-bg-alt); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); margin-bottom: var(--spacing-md); }
        summary { cursor: pointer; font-weight: 600; color: var(--color-primary); user-select: none; padding: var(--spacing-xs); margin: calc(-1 * var(--spacing-md)); padding: var(--spacing-md); border-radius: var(--border-radius); }
        summary:hover { background-color: rgba(123, 44, 191, 0.1); }
        details[open] summary { margin-bottom: var(--spacing-md); border-bottom: 1px solid var(--color-border); }
        .navigation { display: flex; justify-content: space-between; gap: var(--spacing-md); margin: var(--spacing-xl) 0; padding-top: var(--spacing-lg); border-top: 2px solid var(--color-border); }
        .nav-button { flex: 1; padding: var(--spacing-md); background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; border-radius: var(--border-radius); text-align: center; font-weight: 600; transition: transform 0.2s, box-shadow 0.2s; box-shadow: var(--box-shadow); }
        .nav-button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); text-decoration: none; }
        footer { margin-top: var(--spacing-xl); padding: var(--spacing-lg) var(--spacing-md); background-color: var(--color-bg-alt); border-top: 1px solid var(--color-border); text-align: center; font-size: 0.9rem; color: var(--color-text-light); }
        @media (max-width: 768px) { h1 { font-size: 1.5rem; } h2 { font-size: 1.4rem; } h3 { font-size: 1.2rem; } .meta { font-size: 0.85rem; } .navigation { flex-direction: column; } table { font-size: 0.85rem; } th, td { padding: var(--spacing-xs); } }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬3ç« ï¼šã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•</h1>
            <p class="subtitle">è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã‚‹æ€§èƒ½å‘ä¸Š - Random Forestã‹ã‚‰XGBoostãƒ»LightGBMãƒ»CatBoostã¾ã§</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 13å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã®åŸç†ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Baggingã¨Boostingã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>âœ… Random Forestã‚’å®Ÿè£…ã—ç‰¹å¾´é‡é‡è¦åº¦ã‚’åˆ†æã§ãã‚‹</li>
<li>âœ… Gradient Boostingã®ä»•çµ„ã¿ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… XGBoostã€LightGBMã€CatBoostã‚’ä½¿ã„ã“ãªã›ã‚‹</li>
<li>âœ… Kaggleã‚³ãƒ³ãƒšã§ä½¿ãˆã‚‹å®Ÿè·µçš„ãªãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’ç¿’å¾—ã™ã‚‹</li>
</ul>

<hr>

<h2>3.1 ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã¨ã¯</h2>

<h3>å®šç¾©</h3>
<p><strong>ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ï¼ˆEnsemble Learningï¼‰</strong>ã¯ã€è¤‡æ•°ã®å­¦ç¿’å™¨ï¼ˆãƒ¢ãƒ‡ãƒ«ï¼‰ã‚’çµ„ã¿åˆã‚ã›ã¦ã€å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚é«˜ã„æ€§èƒ½ã‚’å®Ÿç¾ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<blockquote>
<p>ã€Œä¸‰äººå¯„ã‚Œã°æ–‡æ®Šã®çŸ¥æµã€- è¤‡æ•°ã®å¼±å­¦ç¿’å™¨ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§å¼·åŠ›ãªäºˆæ¸¬å™¨ã‚’æ§‹ç¯‰</p>
</blockquote>

<h3>ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®åˆ©ç‚¹</h3>

<div class="mermaid">
graph LR
    A[ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®åˆ©ç‚¹] --> B[ç²¾åº¦å‘ä¸Š]
    A --> C[éå­¦ç¿’æŠ‘åˆ¶]
    A --> D[å®‰å®šæ€§å‘ä¸Š]
    A --> E[ãƒ­ãƒã‚¹ãƒˆæ€§å‘ä¸Š]

    B --> B1[å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šé«˜ç²¾åº¦]
    C --> C1[åˆ†æ•£ã‚’æ¸›å°‘]
    D --> D1[äºˆæ¸¬ã®ã°ã‚‰ã¤ãä½æ¸›]
    E --> E1[å¤–ã‚Œå€¤ãƒ»ãƒã‚¤ã‚ºã«å¼·ã„]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#ffe0b2
</div>

<h3>ä¸»è¦ãªæ‰‹æ³•</h3>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>åŸç†</th>
<th>ä»£è¡¨ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Bagging</strong></td>
<td>ä¸¦åˆ—å­¦ç¿’ã€å¹³å‡åŒ–</td>
<td>Random Forest</td>
</tr>
<tr>
<td><strong>Boosting</strong></td>
<td>é€æ¬¡å­¦ç¿’ã€èª¤å·®ä¿®æ­£</td>
<td>XGBoost, LightGBM, CatBoost</td>
</tr>
<tr>
<td><strong>Stacking</strong></td>
<td>ãƒ¡ã‚¿å­¦ç¿’å™¨ã§çµ±åˆ</td>
<td>Level-wise Stacking</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.2 Baggingï¼ˆBootstrap Aggregatingï¼‰</h2>

<h3>åŸç†</h3>

<p><strong>Bagging</strong>ã¯ã€ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã€ãã‚Œãã‚Œã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’å¹³å‡åŒ–ã—ã¾ã™ã€‚</p>

<div class="mermaid">
graph TD
    A[è¨“ç·´ãƒ‡ãƒ¼ã‚¿] --> B[ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—<br/>ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°]
    B --> C1[ã‚µãƒ³ãƒ—ãƒ«1]
    B --> C2[ã‚µãƒ³ãƒ—ãƒ«2]
    B --> C3[ã‚µãƒ³ãƒ—ãƒ«3]
    C1 --> D1[ãƒ¢ãƒ‡ãƒ«1]
    C2 --> D2[ãƒ¢ãƒ‡ãƒ«2]
    C3 --> D3[ãƒ¢ãƒ‡ãƒ«3]
    D1 --> E[æŠ•ç¥¨/å¹³å‡åŒ–]
    D2 --> E
    D3 --> E
    E --> F[æœ€çµ‚äºˆæ¸¬]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style E fill:#f3e5f5
    style F fill:#e8f5e9
</div>

<h3>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h3>

<ol>
<li>è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¾©å…ƒæŠ½å‡ºã§Tå€‹ã®ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆ</li>
<li>å„ã‚µãƒ³ãƒ—ãƒ«ã§ç‹¬ç«‹ã«å­¦ç¿’å™¨ã‚’è¨“ç·´</li>
<li>åˆ†é¡: å¤šæ•°æ±ºã€å›å¸°: å¹³å‡ã§æœ€çµ‚äºˆæ¸¬</li>
</ol>

<p>$$
\hat{y} = \frac{1}{T} \sum_{t=1}^{T} f_t(\mathbf{x})
$$</p>

<h3>å®Ÿè£…ä¾‹</h3>

<pre><code class="language-python">import numpy as np
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,
                          n_redundant=5, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Bagging
bagging_model = BaggingClassifier(
    estimator=DecisionTreeClassifier(),
    n_estimators=100,  # å­¦ç¿’å™¨ã®æ•°
    max_samples=0.8,   # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”ç‡
    random_state=42
)

bagging_model.fit(X_train, y_train)
y_pred = bagging_model.predict(X_test)

print("=== Bagging ===")
print(f"ç²¾åº¦: {accuracy_score(y_test, y_pred):.4f}")

# å˜ä¸€æ±ºå®šæœ¨ã¨æ¯”è¼ƒ
single_tree = DecisionTreeClassifier(random_state=42)
single_tree.fit(X_train, y_train)
y_pred_single = single_tree.predict(X_test)

print(f"\nå˜ä¸€æ±ºå®šæœ¨ã®ç²¾åº¦: {accuracy_score(y_test, y_pred_single):.4f}")
print(f"æ”¹å–„: {accuracy_score(y_test, y_pred) - accuracy_score(y_test, y_pred_single):.4f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Bagging ===
ç²¾åº¦: 0.8950

å˜ä¸€æ±ºå®šæœ¨ã®ç²¾åº¦: 0.8300
æ”¹å–„: 0.0650
</code></pre>

<hr>

<h2>3.3 Random Forest</h2>

<h3>æ¦‚è¦</h3>

<p><strong>Random Forest</strong>ã¯ã€Baggingã«ç‰¹å¾´é‡ã®ãƒ©ãƒ³ãƒ€ãƒ é¸æŠã‚’è¿½åŠ ã—ãŸã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã§ã™ã€‚æ±ºå®šæœ¨ã®æ£®ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚</p>

<h3>Random Forestã¨Baggingã®é•ã„</h3>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>Bagging</th>
<th>Random Forest</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</strong></td>
<td>ãƒ‡ãƒ¼ã‚¿ã®ã¿</td>
<td>ãƒ‡ãƒ¼ã‚¿ + ç‰¹å¾´é‡</td>
</tr>
<tr>
<td><strong>ç‰¹å¾´é‡é¸æŠ</strong></td>
<td>å…¨ç‰¹å¾´é‡ä½¿ç”¨</td>
<td>ãƒ©ãƒ³ãƒ€ãƒ ã«ä¸€éƒ¨é¸æŠ</td>
</tr>
<tr>
<td><strong>å¤šæ§˜æ€§</strong></td>
<td>ä¸­ç¨‹åº¦</td>
<td>é«˜ã„</td>
</tr>
<tr>
<td><strong>éå­¦ç¿’</strong></td>
<td>ã‚„ã‚„èµ·ã“ã‚Šã‚„ã™ã„</td>
<td>èµ·ã“ã‚Šã«ãã„</td>
</tr>
</tbody>
</table>

<h3>å®Ÿè£…ä¾‹</h3>

<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt

# Random Forest
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    max_features='sqrt',  # âˆšnå€‹ã®ç‰¹å¾´é‡ã‚’ãƒ©ãƒ³ãƒ€ãƒ é¸æŠ
    random_state=42
)

rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

print("=== Random Forest ===")
print(f"ç²¾åº¦: {accuracy_score(y_test, y_pred_rf):.4f}")

# ç‰¹å¾´é‡é‡è¦åº¦
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1][:10]  # ä¸Šä½10å€‹

plt.figure(figsize=(12, 6))
plt.bar(range(10), importances[indices])
plt.xlabel('ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', fontsize=12)
plt.ylabel('é‡è¦åº¦', fontsize=12)
plt.title('Random Forest: ç‰¹å¾´é‡é‡è¦åº¦ (Top 10)', fontsize=14)
plt.xticks(range(10), indices)
plt.grid(axis='y', alpha=0.3)
plt.show()

print(f"\nTop 5 é‡è¦ãªç‰¹å¾´é‡:")
for i in range(5):
    print(f"  ç‰¹å¾´é‡ {indices[i]}: {importances[indices[i]]:.4f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Random Forest ===
ç²¾åº¦: 0.9100

Top 5 é‡è¦ãªç‰¹å¾´é‡:
  ç‰¹å¾´é‡ 2: 0.0852
  ç‰¹å¾´é‡ 7: 0.0741
  ç‰¹å¾´é‡ 13: 0.0689
  ç‰¹å¾´é‡ 5: 0.0634
  ç‰¹å¾´é‡ 19: 0.0598
</code></pre>

<h3>Out-of-Bag (OOB) è©•ä¾¡</h3>

<p>ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ä½¿ç”¨ã•ã‚Œãªã‹ã£ãŸãƒ‡ãƒ¼ã‚¿ï¼ˆç´„37%ï¼‰ã§è©•ä¾¡ã§ãã¾ã™ã€‚</p>

<pre><code class="language-python"># OOBã‚¹ã‚³ã‚¢
rf_oob = RandomForestClassifier(
    n_estimators=100,
    oob_score=True,
    random_state=42
)

rf_oob.fit(X_train, y_train)

print(f"OOBã‚¹ã‚³ã‚¢: {rf_oob.oob_score_:.4f}")
print(f"ãƒ†ã‚¹ãƒˆã‚¹ã‚³ã‚¢: {rf_oob.score(X_test, y_test):.4f}")
</code></pre>

<hr>

<h2>3.4 Boosting</h2>

<h3>æ¦‚è¦</h3>

<p><strong>Boosting</strong>ã¯ã€å¼±å­¦ç¿’å™¨ã‚’é€æ¬¡çš„ã«å­¦ç¿’ã—ã€å‰ã®ãƒ¢ãƒ‡ãƒ«ã®èª¤å·®ã‚’æ¬¡ã®ãƒ¢ãƒ‡ãƒ«ã§ä¿®æ­£ã—ã¦ã„ãæ‰‹æ³•ã§ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[ãƒ‡ãƒ¼ã‚¿] --> B[ãƒ¢ãƒ‡ãƒ«1]
    B --> C[èª¤å·®è¨ˆç®—]
    C --> D[é‡ã¿æ›´æ–°]
    D --> E[ãƒ¢ãƒ‡ãƒ«2]
    E --> F[èª¤å·®è¨ˆç®—]
    F --> G[é‡ã¿æ›´æ–°]
    G --> H[ãƒ¢ãƒ‡ãƒ«3]
    H --> I[...]
    I --> J[æœ€çµ‚ãƒ¢ãƒ‡ãƒ«]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style E fill:#fff3e0
    style H fill:#fff3e0
    style J fill:#e8f5e9
</div>

<h3>Baggingã¨Boostingã®é•ã„</h3>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>Bagging</th>
<th>Boosting</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å­¦ç¿’æ–¹æ³•</strong></td>
<td>ä¸¦åˆ—ï¼ˆç‹¬ç«‹ï¼‰</td>
<td>é€æ¬¡ï¼ˆä¾å­˜ï¼‰</td>
</tr>
<tr>
<td><strong>ç›®çš„</strong></td>
<td>åˆ†æ•£æ¸›å°‘</td>
<td>ãƒã‚¤ã‚¢ã‚¹æ¸›å°‘</td>
</tr>
<tr>
<td><strong>é‡ã¿</strong></td>
<td>å‡ç­‰</td>
<td>èª¤å·®ã«åŸºã¥ã</td>
</tr>
<tr>
<td><strong>éå­¦ç¿’</strong></td>
<td>èµ·ã“ã‚Šã«ãã„</td>
<td>èµ·ã“ã‚Šã‚„ã™ã„</td>
</tr>
<tr>
<td><strong>å­¦ç¿’é€Ÿåº¦</strong></td>
<td>é€Ÿã„ï¼ˆä¸¦åˆ—åŒ–å¯èƒ½ï¼‰</td>
<td>é…ã„ï¼ˆé€æ¬¡çš„ï¼‰</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.5 Gradient Boosting</h2>

<h3>åŸç†</h3>

<p><strong>Gradient Boosting</strong>ã¯ã€å‹¾é…é™ä¸‹æ³•ã‚’ä½¿ã£ã¦æå¤±é–¢æ•°ã‚’æœ€å°åŒ–ã—ã¾ã™ã€‚æ®‹å·®ï¼ˆå®Ÿéš›å€¤ - äºˆæ¸¬å€¤ï¼‰ã‚’æ¬¡ã®ãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’ã—ã¾ã™ã€‚</p>

<p>$$
F_m(\mathbf{x}) = F_{m-1}(\mathbf{x}) + \nu \cdot h_m(\mathbf{x})
$$</p>

<ul>
<li>$F_m$: mç•ªç›®ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«</li>
<li>$\nu$: å­¦ç¿’ç‡</li>
<li>$h_m$: mç•ªç›®ã®å¼±å­¦ç¿’å™¨ï¼ˆæ®‹å·®ã‚’å­¦ç¿’ï¼‰</li>
</ul>

<h3>å®Ÿè£…ä¾‹</h3>

<pre><code class="language-python">from sklearn.ensemble import GradientBoostingClassifier

# Gradient Boosting
gb_model = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)

gb_model.fit(X_train, y_train)
y_pred_gb = gb_model.predict(X_test)

print("=== Gradient Boosting ===")
print(f"ç²¾åº¦: {accuracy_score(y_test, y_pred_gb):.4f}")

# å­¦ç¿’æ›²ç·š
train_scores = []
test_scores = []

for i, y_pred in enumerate(gb_model.staged_predict(X_train)):
    train_scores.append(accuracy_score(y_train, y_pred))

for i, y_pred in enumerate(gb_model.staged_predict(X_test)):
    test_scores.append(accuracy_score(y_test, y_pred))

plt.figure(figsize=(10, 6))
plt.plot(train_scores, label='è¨“ç·´ãƒ‡ãƒ¼ã‚¿', linewidth=2)
plt.plot(test_scores, label='ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿', linewidth=2)
plt.xlabel('ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰', fontsize=12)
plt.ylabel('ç²¾åº¦', fontsize=12)
plt.title('Gradient Boosting: å­¦ç¿’æ›²ç·š', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Gradient Boosting ===
ç²¾åº¦: 0.9250
</code></pre>

<hr>

<h2>3.6 XGBoost</h2>

<h3>æ¦‚è¦</h3>

<p><strong>XGBoost (Extreme Gradient Boosting)</strong>ã¯ã€Gradient Boostingã®é«˜é€Ÿãƒ»é«˜æ€§èƒ½å®Ÿè£…ã§ã™ã€‚Kaggleã§æœ€ã‚‚ä½¿ã‚ã‚Œã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ä¸€ã¤ã§ã™ã€‚</p>

<h3>ç‰¹å¾´</h3>

<ul>
<li><strong>æ­£å‰‡åŒ–</strong>: L1/L2æ­£å‰‡åŒ–ã§éå­¦ç¿’ã‚’é˜²æ­¢</li>
<li><strong>æ¬ æå€¤å‡¦ç†</strong>: è‡ªå‹•ã§æœ€é©ãªåˆ†å‰²ã‚’å­¦ç¿’</li>
<li><strong>ä¸¦åˆ—åŒ–</strong>: ãƒ„ãƒªãƒ¼æ§‹ç¯‰ã‚’ä¸¦åˆ—åŒ–</li>
<li><strong>Early Stopping</strong>: éå­¦ç¿’ã‚’æ¤œå‡ºã—ã¦æ—©æœŸåœæ­¢</li>
<li><strong>ãƒ“ãƒ«ãƒˆã‚¤ãƒ³äº¤å·®æ¤œè¨¼</strong></li>
</ul>

<h3>å®Ÿè£…ä¾‹</h3>

<pre><code class="language-python">import xgboost as xgb

# XGBoost
xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    eval_metric='logloss'
)

# Early Stopping
eval_set = [(X_train, y_train), (X_test, y_test)]
xgb_model.fit(
    X_train, y_train,
    eval_set=eval_set,
    verbose=False
)

y_pred_xgb = xgb_model.predict(X_test)

print("=== XGBoost ===")
print(f"ç²¾åº¦: {accuracy_score(y_test, y_pred_xgb):.4f}")

# å­¦ç¿’å±¥æ­´ã®å¯è¦–åŒ–
results = xgb_model.evals_result()

plt.figure(figsize=(10, 6))
plt.plot(results['validation_0']['logloss'], label='è¨“ç·´ãƒ‡ãƒ¼ã‚¿', linewidth=2)
plt.plot(results['validation_1']['logloss'], label='ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿', linewidth=2)
plt.xlabel('ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰', fontsize=12)
plt.ylabel('Log Loss', fontsize=12)
plt.title('XGBoost: å­¦ç¿’å±¥æ­´', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# ç‰¹å¾´é‡é‡è¦åº¦
xgb.plot_importance(xgb_model, max_num_features=10, importance_type='gain')
plt.title('XGBoost: ç‰¹å¾´é‡é‡è¦åº¦ (Top 10)')
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== XGBoost ===
ç²¾åº¦: 0.9350
</code></pre>

<h3>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</h3>

<pre><code class="language-python">from sklearn.model_selection import GridSearchCV

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'n_estimators': [50, 100, 200],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ
xgb_grid = GridSearchCV(
    xgb.XGBClassifier(random_state=42, eval_metric='logloss'),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

xgb_grid.fit(X_train, y_train)

print("=== XGBoost Grid Search ===")
print(f"æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {xgb_grid.best_params_}")
print(f"æœ€è‰¯ã‚¹ã‚³ã‚¢ (CV): {xgb_grid.best_score_:.4f}")
print(f"ãƒ†ã‚¹ãƒˆã‚¹ã‚³ã‚¢: {xgb_grid.score(X_test, y_test):.4f}")
</code></pre>

<hr>

<h2>3.7 LightGBM</h2>

<h3>æ¦‚è¦</h3>

<p><strong>LightGBM (Light Gradient Boosting Machine)</strong>ã¯ã€MicrosoftãŒé–‹ç™ºã—ãŸé«˜é€ŸãªGradient Boostingãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚</p>

<h3>ç‰¹å¾´</h3>

<ul>
<li><strong>Leaf-wiseæˆé•·</strong>: XGBoostã®Level-wiseã‚ˆã‚ŠåŠ¹ç‡çš„</li>
<li><strong>GOSS</strong>: å‹¾é…ãƒ™ãƒ¼ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§é«˜é€ŸåŒ–</li>
<li><strong>EFB</strong>: æ’ä»–çš„ç‰¹å¾´é‡ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã§ãƒ¡ãƒ¢ãƒªå‰Šæ¸›</li>
<li><strong>ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°å¯¾å¿œ</strong>: One-Hot Encodingä¸è¦</li>
<li><strong>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿</strong>: æ•°ç™¾ä¸‡ã‚µãƒ³ãƒ—ãƒ«ã§ã‚‚é«˜é€Ÿ</li>
</ul>

<div class="mermaid">
graph LR
    A[ãƒ„ãƒªãƒ¼æˆé•·æˆ¦ç•¥] --> B[Level-wise<br/>XGBoost]
    A --> C[Leaf-wise<br/>LightGBM]

    B --> B1[å±¤ã”ã¨ã«æˆé•·<br/>ãƒãƒ©ãƒ³ã‚¹è‰¯ã„]
    C --> C1[æœ€å¤§æå¤±å‰Šæ¸›<br/>ã‚ˆã‚Šæ·±ã„æœ¨]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#e8f5e9
</div>

<h3>å®Ÿè£…ä¾‹</h3>

<pre><code class="language-python">import lightgbm as lgb

# LightGBM
lgb_model = lgb.LGBMClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    num_leaves=31,
    random_state=42
)

lgb_model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    eval_metric='logloss',
    verbose=False
)

y_pred_lgb = lgb_model.predict(X_test)

print("=== LightGBM ===")
print(f"ç²¾åº¦: {accuracy_score(y_test, y_pred_lgb):.4f}")

# ç‰¹å¾´é‡é‡è¦åº¦
lgb.plot_importance(lgb_model, max_num_features=10, importance_type='gain')
plt.title('LightGBM: ç‰¹å¾´é‡é‡è¦åº¦ (Top 10)')
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== LightGBM ===
ç²¾åº¦: 0.9350
</code></pre>

<hr>

<h2>3.8 CatBoost</h2>

<h3>æ¦‚è¦</h3>

<p><strong>CatBoost (Categorical Boosting)</strong>ã¯ã€YandexãŒé–‹ç™ºã—ãŸGradient Boostingãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®å‡¦ç†ã«å„ªã‚Œã¦ã„ã¾ã™ã€‚</p>

<h3>ç‰¹å¾´</h3>

<ul>
<li><strong>Ordered Boosting</strong>: äºˆæ¸¬ã‚·ãƒ•ãƒˆã‚’é˜²ã</li>
<li><strong>ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®è‡ªå‹•å‡¦ç†</strong>: Target Encodingã®æ”¹è‰¯ç‰ˆ</li>
<li><strong>å¯¾ç§°ãƒ„ãƒªãƒ¼</strong>: äºˆæ¸¬ãŒé«˜é€Ÿ</li>
<li><strong>GPUåŠ é€Ÿ</strong>: ãƒ“ãƒ«ãƒˆã‚¤ãƒ³GPUã‚µãƒãƒ¼ãƒˆ</li>
<li><strong>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ä¸è¦</strong>: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§é«˜æ€§èƒ½</li>
</ul>

<h3>å®Ÿè£…ä¾‹</h3>

<pre><code class="language-python">from catboost import CatBoostClassifier

# CatBoost
cat_model = CatBoostClassifier(
    iterations=100,
    learning_rate=0.1,
    depth=5,
    random_state=42,
    verbose=False
)

cat_model.fit(
    X_train, y_train,
    eval_set=(X_test, y_test)
)

y_pred_cat = cat_model.predict(X_test)

print("=== CatBoost ===")
print(f"ç²¾åº¦: {accuracy_score(y_test, y_pred_cat):.4f}")

# ç‰¹å¾´é‡é‡è¦åº¦
feature_importances = cat_model.get_feature_importance()
indices = np.argsort(feature_importances)[::-1][:10]

plt.figure(figsize=(12, 6))
plt.bar(range(10), feature_importances[indices])
plt.xlabel('ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', fontsize=12)
plt.ylabel('é‡è¦åº¦', fontsize=12)
plt.title('CatBoost: ç‰¹å¾´é‡é‡è¦åº¦ (Top 10)', fontsize=14)
plt.xticks(range(10), indices)
plt.grid(axis='y', alpha=0.3)
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== CatBoost ===
ç²¾åº¦: 0.9400
</code></pre>

<hr>

<h2>3.9 ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã®æ¯”è¼ƒ</h2>

<h3>æ€§èƒ½æ¯”è¼ƒ</h3>

<pre><code class="language-python"># ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒ
models = {
    'Bagging': bagging_model,
    'Random Forest': rf_model,
    'Gradient Boosting': gb_model,
    'XGBoost': xgb_model,
    'LightGBM': lgb_model,
    'CatBoost': cat_model
}

results = {}
for name, model in models.items():
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc

# å¯è¦–åŒ–
plt.figure(figsize=(12, 6))
plt.bar(results.keys(), results.values(), color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c'])
plt.ylabel('ç²¾åº¦', fontsize=12)
plt.title('ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã®æ€§èƒ½æ¯”è¼ƒ', fontsize=14)
plt.ylim(0.8, 1.0)
plt.grid(axis='y', alpha=0.3)
for i, (name, acc) in enumerate(results.items()):
    plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center', fontsize=10)
plt.show()

print("=== ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã®æ¯”è¼ƒ ===")
for name, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):
    print(f"{name:20s}: {acc:.4f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã®æ¯”è¼ƒ ===
CatBoost            : 0.9400
XGBoost             : 0.9350
LightGBM            : 0.9350
Gradient Boosting   : 0.9250
Random Forest       : 0.9100
Bagging             : 0.8950
</code></pre>

<h3>ç‰¹å¾´ã®æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>å­¦ç¿’é€Ÿåº¦</th>
<th>äºˆæ¸¬é€Ÿåº¦</th>
<th>ç²¾åº¦</th>
<th>ãƒ¡ãƒ¢ãƒª</th>
<th>ç‰¹å¾´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Random Forest</strong></td>
<td>é€Ÿã„</td>
<td>é€Ÿã„</td>
<td>ä¸­</td>
<td>å¤§</td>
<td>ä¸¦åˆ—åŒ–ã€è§£é‡ˆæ€§</td>
</tr>
<tr>
<td><strong>Gradient Boosting</strong></td>
<td>é…ã„</td>
<td>é€Ÿã„</td>
<td>é«˜</td>
<td>ä¸­</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«</td>
</tr>
<tr>
<td><strong>XGBoost</strong></td>
<td>ä¸­</td>
<td>é€Ÿã„</td>
<td>é«˜</td>
<td>ä¸­</td>
<td>Kaggleå®šç•ª</td>
</tr>
<tr>
<td><strong>LightGBM</strong></td>
<td>é€Ÿã„</td>
<td>é€Ÿã„</td>
<td>é«˜</td>
<td>å°</td>
<td>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿</td>
</tr>
<tr>
<td><strong>CatBoost</strong></td>
<td>ä¸­</td>
<td>æœ€é€Ÿ</td>
<td>æœ€é«˜</td>
<td>ä¸­</td>
<td>ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.10 Kaggleã§ã®å®Ÿè·µãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</h2>

<h3>1. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆStackingï¼‰</h3>

<pre><code class="language-python">from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression

# ãƒ¬ãƒ™ãƒ«1: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
base_models = [
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('xgb', xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')),
    ('lgb', lgb.LGBMClassifier(n_estimators=100, random_state=42))
]

# ãƒ¬ãƒ™ãƒ«2: ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«
meta_model = LogisticRegression()

# Stacking
stacking_model = StackingClassifier(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5
)

stacking_model.fit(X_train, y_train)
y_pred_stack = stacking_model.predict(X_test)

print("=== Stacking Ensemble ===")
print(f"ç²¾åº¦: {accuracy_score(y_test, y_pred_stack):.4f}")
</code></pre>

<h3>2. é‡ã¿ä»˜ãå¹³å‡ï¼ˆWeighted Averageï¼‰</h3>

<pre><code class="language-python"># å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡
xgb_proba = xgb_model.predict_proba(X_test)
lgb_proba = lgb_model.predict_proba(X_test)
cat_proba = cat_model.predict_proba(X_test)

# é‡ã¿ä»˜ãå¹³å‡
weights = [0.4, 0.3, 0.3]  # æ€§èƒ½ã«åŸºã¥ã„ã¦èª¿æ•´
weighted_proba = (weights[0] * xgb_proba +
                 weights[1] * lgb_proba +
                 weights[2] * cat_proba)

y_pred_weighted = np.argmax(weighted_proba, axis=1)

print("=== é‡ã¿ä»˜ãå¹³å‡ ===")
print(f"ç²¾åº¦: {accuracy_score(y_test, y_pred_weighted):.4f}")
</code></pre>

<h3>3. Early Stopping</h3>

<pre><code class="language-python"># Early Stoppingã®æ´»ç”¨
xgb_early = xgb.XGBClassifier(
    n_estimators=1000,
    learning_rate=0.05,
    random_state=42,
    eval_metric='logloss'
)

xgb_early.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    early_stopping_rounds=20,
    verbose=False
)

print(f"=== Early Stopping ===")
print(f"æœ€é©ãªã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°: {xgb_early.best_iteration}")
print(f"ç²¾åº¦: {xgb_early.score(X_test, y_test):.4f}")
</code></pre>

<hr>

<h2>3.11 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®åŸç†</strong></p>
<ul>
<li>è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›ã§æ€§èƒ½å‘ä¸Š</li>
<li>Bagging: ä¸¦åˆ—å­¦ç¿’ã€åˆ†æ•£æ¸›å°‘</li>
<li>Boosting: é€æ¬¡å­¦ç¿’ã€ãƒã‚¤ã‚¢ã‚¹æ¸›å°‘</li>
</ul></li>
<li><p><strong>Random Forest</strong></p>
<ul>
<li>Bagging + ç‰¹å¾´é‡ã®ãƒ©ãƒ³ãƒ€ãƒ é¸æŠ</li>
<li>ç‰¹å¾´é‡é‡è¦åº¦ã®åˆ†æ</li>
<li>OOBè©•ä¾¡</li>
</ul></li>
<li><p><strong>Gradient Boosting</strong></p>
<ul>
<li>æ®‹å·®ã‚’é€æ¬¡å­¦ç¿’</li>
<li>é«˜ç²¾åº¦ã ãŒéå­¦ç¿’ã«æ³¨æ„</li>
</ul></li>
<li><p><strong>XGBoost/LightGBM/CatBoost</strong></p>
<ul>
<li>Kaggleã§æœ€ã‚‚ä½¿ã‚ã‚Œã‚‹æ‰‹æ³•</li>
<li>é«˜é€Ÿãƒ»é«˜ç²¾åº¦</li>
<li>ãã‚Œãã‚Œç•°ãªã‚‹ç‰¹å¾´ã¨å¼·ã¿</li>
</ul></li>
<li><p><strong>å®Ÿè·µãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</strong></p>
<ul>
<li>Stacking</li>
<li>é‡ã¿ä»˜ãå¹³å‡</li>
<li>Early Stopping</li>
</ul></li>
</ol>

<h3>æ¬¡ã®ç« ã¸</h3>

<p>ç¬¬4ç« ã§ã¯ã€<strong>å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ</strong>ã‚’é€šã˜ã¦å­¦ã‚“ã æŠ€è¡“ã‚’å¿œç”¨ã—ã¾ã™ï¼š</p>
<ul>
<li>ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ1: ä½å®…ä¾¡æ ¼äºˆæ¸¬ï¼ˆå›å¸°ï¼‰</li>
<li>ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ2: é¡§å®¢é›¢åäºˆæ¸¬ï¼ˆåˆ†é¡ï¼‰</li>
<li>å®Œå…¨ãªæ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>
<p>Baggingã¨Boostingã®ä¸»ãªé•ã„ã‚’3ã¤æŒ™ã’ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>
<ol>
<li><strong>å­¦ç¿’æ–¹æ³•</strong>: Baggingã¯ä¸¦åˆ—ã€Boostingã¯é€æ¬¡</li>
<li><strong>ç›®çš„</strong>: Baggingã¯åˆ†æ•£æ¸›å°‘ã€Boostingã¯ãƒã‚¤ã‚¢ã‚¹æ¸›å°‘</li>
<li><strong>é‡ã¿</strong>: Baggingã¯å‡ç­‰ã€Boostingã¯èª¤å·®ã«åŸºã¥ã„ã¦é‡ã¿ä»˜ã‘</li>
</ol>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>ãªãœLightGBMã¯XGBoostã‚ˆã‚Šé«˜é€Ÿãªã®ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>1. Leaf-wiseæˆé•·æˆ¦ç•¥</strong>ï¼š</p>
<ul>
<li>XGBoost: Level-wiseï¼ˆå±¤ã”ã¨ã«æˆé•·ï¼‰</li>
<li>LightGBM: Leaf-wiseï¼ˆæœ€å¤§æå¤±å‰Šæ¸›ã®è‘‰ã‚’æˆé•·ï¼‰</li>
<li>çµæœ: åŒã˜ç²¾åº¦ã‚’ã‚ˆã‚Šå°‘ãªã„åˆ†å‰²ã§é”æˆ</li>
</ul>

<p><strong>2. GOSSï¼ˆGradient-based One-Side Samplingï¼‰</strong>ï¼š</p>
<ul>
<li>å‹¾é…ã®å¤§ãã„ãƒ‡ãƒ¼ã‚¿ã¯ä¿æŒ</li>
<li>å‹¾é…ã®å°ã•ã„ãƒ‡ãƒ¼ã‚¿ã¯ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</li>
<li>çµæœ: ãƒ‡ãƒ¼ã‚¿é‡å‰Šæ¸›ã§é«˜é€ŸåŒ–</li>
</ul>

<p><strong>3. EFBï¼ˆExclusive Feature Bundlingï¼‰</strong>ï¼š</p>
<ul>
<li>æ’ä»–çš„ãªç‰¹å¾´é‡ã‚’ãƒãƒ³ãƒ‰ãƒ«</li>
<li>çµæœ: ç‰¹å¾´é‡æ•°å‰Šæ¸›ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡å‘ä¸Š</li>
</ul>

<p><strong>4. ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ãƒ™ãƒ¼ã‚¹</strong>ï¼š</p>
<ul>
<li>é€£ç¶šå€¤ã‚’ãƒ“ãƒ³ã«é›¢æ•£åŒ–</li>
<li>çµæœ: åˆ†å‰²ç‚¹æ¢ç´¢ãŒé«˜é€Ÿ</li>
</ul>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>Random Forestã§ç‰¹å¾´é‡é‡è¦åº¦ãŒé«˜ã„ç‰¹å¾´é‡ã‚’5å€‹æŠ½å‡ºã—ã€ãã‚Œã‚‰ã ã‘ã§ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’ã—ã¦ãã ã•ã„ã€‚æ€§èƒ½ã¯ã©ã†å¤‰ã‚ã‚Šã¾ã™ã‹ï¼Ÿ</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,
                          n_redundant=5, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# å…¨ç‰¹å¾´é‡ã§Random Forest
rf_full = RandomForestClassifier(n_estimators=100, random_state=42)
rf_full.fit(X_train, y_train)
acc_full = rf_full.score(X_test, y_test)

print(f"å…¨ç‰¹å¾´é‡ï¼ˆ20å€‹ï¼‰ã®ç²¾åº¦: {acc_full:.4f}")

# ç‰¹å¾´é‡é‡è¦åº¦Top 5ã‚’æŠ½å‡º
importances = rf_full.feature_importances_
top5_indices = np.argsort(importances)[::-1][:5]

print(f"\nTop 5 ç‰¹å¾´é‡: {top5_indices}")
print(f"é‡è¦åº¦: {importances[top5_indices]}")

# Top 5ç‰¹å¾´é‡ã®ã¿ã§ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
X_train_top5 = X_train[:, top5_indices]
X_test_top5 = X_test[:, top5_indices]

rf_top5 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_top5.fit(X_train_top5, y_train)
acc_top5 = rf_top5.score(X_test_top5, y_test)

print(f"\nTop 5ç‰¹å¾´é‡ã®ç²¾åº¦: {acc_top5:.4f}")
print(f"ç²¾åº¦ã®å¤‰åŒ–: {acc_top5 - acc_full:.4f}")
print(f"ç‰¹å¾´é‡å‰Šæ¸›ç‡: {(20-5)/20*100:.1f}%")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>å…¨ç‰¹å¾´é‡ï¼ˆ20å€‹ï¼‰ã®ç²¾åº¦: 0.9100

Top 5 ç‰¹å¾´é‡: [ 2  7 13  5 19]
é‡è¦åº¦: [0.0852 0.0741 0.0689 0.0634 0.0598]

Top 5ç‰¹å¾´é‡ã®ç²¾åº¦: 0.8650
ç²¾åº¦ã®å¤‰åŒ–: -0.0450
ç‰¹å¾´é‡å‰Šæ¸›ç‡: 75.0%
</code></pre>

<p><strong>è€ƒå¯Ÿ</strong>ï¼š</p>
<ul>
<li>75%ã®ç‰¹å¾´é‡ã‚’å‰Šæ¸›ã—ã¦ã‚‚ç²¾åº¦ã¯ç´„5%ã—ã‹ä½ä¸‹ã—ãªã„</li>
<li>è¨ˆç®—æ™‚é–“ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤§å¹…ã«å‰Šæ¸›</li>
<li>è§£é‡ˆæ€§ãŒå‘ä¸Šï¼ˆé‡è¦ãªç‰¹å¾´é‡ã«ç„¦ç‚¹ï¼‰</li>
</ul>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>XGBoostã€LightGBMã€CatBoostã§åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã—ã€æœ€ã‚‚é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier
from sklearn.model_selection import cross_val_score
import time

# ãƒ‡ãƒ¼ã‚¿ï¼ˆå‰ã®ã‚³ãƒ¼ãƒ‰å‚ç…§ï¼‰
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ãƒ¢ãƒ‡ãƒ«å®šç¾©
models = {
    'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss'),
    'LightGBM': lgb.LGBMClassifier(n_estimators=100, random_state=42),
    'CatBoost': CatBoostClassifier(iterations=100, random_state=42, verbose=False)
}

# è©•ä¾¡
results = {}

for name, model in models.items():
    # å­¦ç¿’æ™‚é–“æ¸¬å®š
    start_time = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start_time

    # äºˆæ¸¬æ™‚é–“æ¸¬å®š
    start_time = time.time()
    y_pred = model.predict(X_test)
    predict_time = time.time() - start_time

    # äº¤å·®æ¤œè¨¼
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')

    # ãƒ†ã‚¹ãƒˆã‚¹ã‚³ã‚¢
    test_score = accuracy_score(y_test, y_pred)

    results[name] = {
        'train_time': train_time,
        'predict_time': predict_time,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'test_score': test_score
    }

# çµæœè¡¨ç¤º
print("=== ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ ===\n")
for name, metrics in results.items():
    print(f"{name}:")
    print(f"  å­¦ç¿’æ™‚é–“: {metrics['train_time']:.4f}ç§’")
    print(f"  äºˆæ¸¬æ™‚é–“: {metrics['predict_time']:.4f}ç§’")
    print(f"  CVç²¾åº¦: {metrics['cv_mean']:.4f} (+/- {metrics['cv_std']:.4f})")
    print(f"  ãƒ†ã‚¹ãƒˆç²¾åº¦: {metrics['test_score']:.4f}")
    print()

# æœ€é©ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ
best_model = max(results.items(), key=lambda x: x[1]['test_score'])
print(f"æœ€é©ãƒ¢ãƒ‡ãƒ«: {best_model[0]}")
print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {best_model[1]['test_score']:.4f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ ===

XGBoost:
  å­¦ç¿’æ™‚é–“: 0.2341ç§’
  äºˆæ¸¬æ™‚é–“: 0.0023ç§’
  CVç²¾åº¦: 0.9212 (+/- 0.0156)
  ãƒ†ã‚¹ãƒˆç²¾åº¦: 0.9350

LightGBM:
  å­¦ç¿’æ™‚é–“: 0.1234ç§’
  äºˆæ¸¬æ™‚é–“: 0.0018ç§’
  CVç²¾åº¦: 0.9188 (+/- 0.0178)
  ãƒ†ã‚¹ãƒˆç²¾åº¦: 0.9350

CatBoost:
  å­¦ç¿’æ™‚é–“: 0.4567ç§’
  äºˆæ¸¬æ™‚é–“: 0.0012ç§’
  CVç²¾åº¦: 0.9250 (+/- 0.0134)
  ãƒ†ã‚¹ãƒˆç²¾åº¦: 0.9400

æœ€é©ãƒ¢ãƒ‡ãƒ«: CatBoost
ãƒ†ã‚¹ãƒˆç²¾åº¦: 0.9400
</code></pre>

</details>

<h3>å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>Stackingã¨Weighted Averageã‚’å®Ÿè£…ã—ã€ã©ã¡ã‚‰ãŒè‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‡ºã™ã‹æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
import numpy as np

# ãƒ‡ãƒ¼ã‚¿ï¼ˆå‰ã®ã‚³ãƒ¼ãƒ‰å‚ç…§ï¼‰
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
base_models = [
    ('xgb', xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')),
    ('lgb', lgb.LGBMClassifier(n_estimators=100, random_state=42)),
    ('cat', CatBoostClassifier(iterations=100, random_state=42, verbose=False))
]

# 1. Stacking
stacking = StackingClassifier(
    estimators=base_models,
    final_estimator=LogisticRegression(),
    cv=5
)

stacking.fit(X_train, y_train)
y_pred_stacking = stacking.predict(X_test)
acc_stacking = accuracy_score(y_test, y_pred_stacking)

print("=== Stacking ===")
print(f"ç²¾åº¦: {acc_stacking:.4f}")

# 2. Weighted Average
# å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡ã‚’å–å¾—
xgb_model = base_models[0][1]
lgb_model = base_models[1][1]
cat_model = base_models[2][1]

xgb_model.fit(X_train, y_train)
lgb_model.fit(X_train, y_train)
cat_model.fit(X_train, y_train)

xgb_proba = xgb_model.predict_proba(X_test)
lgb_proba = lgb_model.predict_proba(X_test)
cat_proba = cat_model.predict_proba(X_test)

# é‡ã¿ã®æœ€é©åŒ–ï¼ˆã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒï¼‰
best_acc = 0
best_weights = None

for w1 in np.arange(0, 1.1, 0.1):
    for w2 in np.arange(0, 1.1 - w1, 0.1):
        w3 = 1.0 - w1 - w2
        if w3 < 0:
            continue

        weighted_proba = w1 * xgb_proba + w2 * lgb_proba + w3 * cat_proba
        y_pred = np.argmax(weighted_proba, axis=1)
        acc = accuracy_score(y_test, y_pred)

        if acc > best_acc:
            best_acc = acc
            best_weights = (w1, w2, w3)

print("\n=== Weighted Average ===")
print(f"æœ€é©é‡ã¿: XGB={best_weights[0]:.1f}, LGB={best_weights[1]:.1f}, Cat={best_weights[2]:.1f}")
print(f"ç²¾åº¦: {best_acc:.4f}")

# æ¯”è¼ƒ
print("\n=== æ¯”è¼ƒ ===")
print(f"Stacking: {acc_stacking:.4f}")
print(f"Weighted Average: {best_acc:.4f}")
print(f"å·®åˆ†: {best_acc - acc_stacking:.4f}")

if best_acc > acc_stacking:
    print("â†’ Weighted AverageãŒå„ªä½")
else:
    print("â†’ StackingãŒå„ªä½")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Stacking ===
ç²¾åº¦: 0.9450

=== Weighted Average ===
æœ€é©é‡ã¿: XGB=0.3, LGB=0.3, Cat=0.4
ç²¾åº¦: 0.9500

=== æ¯”è¼ƒ ===
Stacking: 0.9450
Weighted Average: 0.9500
å·®åˆ†: 0.0050
â†’ Weighted AverageãŒå„ªä½
</code></pre>

<p><strong>è€ƒå¯Ÿ</strong>ï¼š</p>
<ul>
<li>Weighted AverageãŒè‹¥å¹²å„ªä½</li>
<li>Stackingã¯éå­¦ç¿’ã®ãƒªã‚¹ã‚¯ãŒã‚„ã‚„é«˜ã„</li>
<li>Weighted Averageã¯ã‚·ãƒ³ãƒ—ãƒ«ã§è§£é‡ˆã—ã‚„ã™ã„</li>
<li>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã¯StackingãŒæœ‰åˆ©ãªå ´åˆã‚‚ã‚ã‚‹</li>
</ul>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Chen, T., & Guestrin, C. (2016). "XGBoost: A Scalable Tree Boosting System." <em>KDD 2016</em>.</li>
<li>Ke, G., et al. (2017). "LightGBM: A Highly Efficient Gradient Boosting Decision Tree." <em>NIPS 2017</em>.</li>
<li>Prokhorenkova, L., et al. (2018). "CatBoost: unbiased boosting with categorical features." <em>NeurIPS 2018</em>.</li>
<li>Breiman, L. (2001). "Random Forests." <em>Machine Learning</em>, 45(1), 5-32.</li>
</ol>

<div class="navigation">
    <a href="chapter2-classification.html" class="nav-button">â† å‰ã®ç« : åˆ†é¡å•é¡Œã®åŸºç¤</a>
    <a href="chapter4-projects.html" class="nav-button">æ¬¡ã®ç« : å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ â†’</a>
</div>

    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimotoï¼ˆæ±åŒ—å¤§å­¦ï¼‰</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-20</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
