<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第4章：実践プロジェクト - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: var(--font-body); line-height: 1.7; color: var(--color-text); background-color: var(--color-bg); font-size: 16px; }
        header { background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; padding: var(--spacing-xl) var(--spacing-md); margin-bottom: var(--spacing-xl); box-shadow: var(--box-shadow); }
        .header-content { max-width: 900px; margin: 0 auto; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: var(--spacing-sm); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; opacity: 0.95; font-weight: 400; margin-bottom: var(--spacing-md); }
        .meta { display: flex; flex-wrap: wrap; gap: var(--spacing-md); font-size: 0.9rem; opacity: 0.9; }
        .meta-item { display: flex; align-items: center; gap: 0.3rem; }
        .container { max-width: 900px; margin: 0 auto; padding: 0 var(--spacing-md) var(--spacing-xl); }
        h2 { font-size: 1.75rem; color: var(--color-primary); margin-top: var(--spacing-xl); margin-bottom: var(--spacing-md); padding-bottom: var(--spacing-xs); border-bottom: 3px solid var(--color-accent); }
        h3 { font-size: 1.4rem; color: var(--color-primary); margin-top: var(--spacing-lg); margin-bottom: var(--spacing-sm); }
        h4 { font-size: 1.1rem; color: var(--color-primary-dark); margin-top: var(--spacing-md); margin-bottom: var(--spacing-sm); }
        p { margin-bottom: var(--spacing-md); color: var(--color-text); }
        a { color: var(--color-link); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--color-link-hover); text-decoration: underline; }
        ul, ol { margin-left: var(--spacing-lg); margin-bottom: var(--spacing-md); }
        li { margin-bottom: var(--spacing-xs); color: var(--color-text); }
        pre { background-color: var(--color-code-bg); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); overflow-x: auto; margin-bottom: var(--spacing-md); font-family: var(--font-mono); font-size: 0.9rem; line-height: 1.5; }
        code { font-family: var(--font-mono); font-size: 0.9em; background-color: var(--color-code-bg); padding: 0.2em 0.4em; border-radius: 3px; }
        pre code { background-color: transparent; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: var(--spacing-md); font-size: 0.95rem; }
        th, td { border: 1px solid var(--color-border); padding: var(--spacing-sm); text-align: left; }
        th { background-color: var(--color-bg-alt); font-weight: 600; color: var(--color-primary); }
        blockquote { border-left: 4px solid var(--color-accent); padding-left: var(--spacing-md); margin: var(--spacing-md) 0; color: var(--color-text-light); font-style: italic; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        .mermaid { text-align: center; margin: var(--spacing-lg) 0; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        details { background-color: var(--color-bg-alt); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); margin-bottom: var(--spacing-md); }
        summary { cursor: pointer; font-weight: 600; color: var(--color-primary); user-select: none; padding: var(--spacing-xs); margin: calc(-1 * var(--spacing-md)); padding: var(--spacing-md); border-radius: var(--border-radius); }
        summary:hover { background-color: rgba(123, 44, 191, 0.1); }
        details[open] summary { margin-bottom: var(--spacing-md); border-bottom: 1px solid var(--color-border); }
        .navigation { display: flex; justify-content: space-between; gap: var(--spacing-md); margin: var(--spacing-xl) 0; padding-top: var(--spacing-lg); border-top: 2px solid var(--color-border); }
        .nav-button { flex: 1; padding: var(--spacing-md); background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; border-radius: var(--border-radius); text-align: center; font-weight: 600; transition: transform 0.2s, box-shadow 0.2s; box-shadow: var(--box-shadow); }
        .nav-button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); text-decoration: none; }
        footer { margin-top: var(--spacing-xl); padding: var(--spacing-lg) var(--spacing-md); background-color: var(--color-bg-alt); border-top: 1px solid var(--color-border); text-align: center; font-size: 0.9rem; color: var(--color-text-light); }
        .project-box { background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 50%); border-radius: var(--border-radius); padding: var(--spacing-lg); margin: var(--spacing-lg) 0; box-shadow: var(--box-shadow); }
        @media (max-width: 768px) { h1 { font-size: 1.5rem; } h2 { font-size: 1.4rem; } h3 { font-size: 1.2rem; } .meta { font-size: 0.85rem; } .navigation { flex-direction: column; } table { font-size: 0.85rem; } th, td { padding: var(--spacing-xs); } }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>第4章：実践プロジェクト</h1>
            <p class="subtitle">完全な機械学習パイプライン - 住宅価格予測と顧客離反予測</p>
            <div class="meta">
                <span class="meta-item">📖 読了時間: 30分</span>
                <span class="meta-item">📊 難易度: 中級</span>
                <span class="meta-item">💻 コード例: 20個</span>
                <span class="meta-item">📝 演習問題: 5問</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>学習目標</h2>
<p>この章を読むことで、以下を習得できます：</p>
<ul>
<li>✅ 完全な機械学習パイプラインを構築できる</li>
<li>✅ 探索的データ分析（EDA）を実施できる</li>
<li>✅ 特徴量エンジニアリングを実践できる</li>
<li>✅ モデル選択とハイパーパラメータチューニングができる</li>
<li>✅ 不均衡データに対処できる</li>
<li>✅ ビジネスインパクトを分析できる</li>
</ul>

<hr>

<h2>4.1 機械学習パイプライン</h2>

<h3>概要</h3>

<p>実務の機械学習プロジェクトは、以下のステップで構成されます。</p>

<div class="mermaid">
graph LR
    A[問題定義] --> B[データ収集]
    B --> C[EDA]
    C --> D[前処理]
    D --> E[特徴量エンジニアリング]
    E --> F[モデル選択]
    F --> G[学習]
    G --> H[評価]
    H --> I{満足?}
    I -->|No| E
    I -->|Yes| J[デプロイ]

    style A fill:#e3f2fd
    style C fill:#fff3e0
    style E fill:#f3e5f5
    style G fill:#e8f5e9
    style J fill:#ffe0b2
</div>

<h3>重要なステップ</h3>

<table>
<thead>
<tr>
<th>ステップ</th>
<th>目的</th>
<th>主要タスク</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>問題定義</strong></td>
<td>目標の明確化</td>
<td>回帰 or 分類、評価指標の選定</td>
</tr>
<tr>
<td><strong>EDA</strong></td>
<td>データ理解</td>
<td>分布確認、相関分析、外れ値検出</td>
</tr>
<tr>
<td><strong>前処理</strong></td>
<td>データクリーニング</td>
<td>欠損値処理、スケーリング、エンコーディング</td>
</tr>
<tr>
<td><strong>特徴量エンジニアリング</strong></td>
<td>予測力向上</td>
<td>新規特徴量作成、特徴量選択</td>
</tr>
<tr>
<td><strong>モデル選択</strong></td>
<td>最適アルゴリズム</td>
<td>複数モデル比較、チューニング</td>
</tr>
<tr>
<td><strong>評価</strong></td>
<td>性能検証</td>
<td>交差検証、テストデータでの評価</td>
</tr>
</tbody>
</table>

<hr>

<h2>4.2 プロジェクト1: 住宅価格予測（回帰）</h2>

<div class="project-box">
<h3>プロジェクト概要</h3>
<p><strong>課題</strong>: ボストン住宅データを使って、住宅価格を予測するモデルを構築します。</p>
<p><strong>目標</strong>: R² > 0.85、RMSE < $5,000を達成</p>
<p><strong>データ</strong>: 506サンプル、13特徴量</p>
<p><strong>タスク</strong>: 回帰問題</p>
</div>

<h3>ステップ1: データ読み込みと確認</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# データ読み込み（カリフォルニア住宅データ）
housing = fetch_california_housing()
X = pd.DataFrame(housing.data, columns=housing.feature_names)
y = pd.Series(housing.target, name='Price')

print("=== データセット情報 ===")
print(f"サンプル数: {X.shape[0]}")
print(f"特徴量数: {X.shape[1]}")
print(f"\n特徴量一覧:")
print(X.columns.tolist())

print(f"\n基本統計量:")
print(X.describe())

print(f"\n目的変数の統計:")
print(y.describe())
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== データセット情報 ===
サンプル数: 20640
特徴量数: 8

特徴量一覧:
['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']

基本統計量:
            MedInc    HouseAge    AveRooms  ...  AveOccup   Latitude  Longitude
count  20640.0000  20640.0000  20640.0000  ...  20640.00  20640.000  20640.000
mean       3.8707     28.6395      5.4289  ...      3.07     35.632   -119.570
std        1.8998     12.5856      2.4742  ...     10.39      2.136      2.004
min        0.4999      1.0000      0.8467  ...      0.69     32.540   -124.350
25%        2.5634     18.0000      4.4401  ...      2.43     33.930   -121.800
50%        3.5348     29.0000      5.2287  ...      2.82     34.260   -118.490
75%        4.7432     37.0000      6.0524  ...      3.28     37.710   -118.010
max       15.0001     52.0000    141.9091  ...   1243.33     41.950   -114.310

目的変数の統計:
count    20640.000000
mean         2.068558
std          1.153956
min          0.149990
25%          1.196000
50%          1.797000
75%          2.647250
max          5.000010
Name: Price, dtype: float64
</code></pre>

<h3>ステップ2: 探索的データ分析（EDA）</h3>

<pre><code class="language-python"># 相関行列
correlation = X.corr()

plt.figure(figsize=(12, 10))
sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', center=0)
plt.title('特徴量間の相関', fontsize=16)
plt.tight_layout()
plt.show()

# 目的変数との相関
target_corr = pd.DataFrame({
    'Feature': X.columns,
    'Correlation': [X[col].corr(y) for col in X.columns]
}).sort_values('Correlation', ascending=False)

print("\n=== 目的変数との相関 ===")
print(target_corr)

# 可視化
fig, axes = plt.subplots(2, 4, figsize=(16, 10))
axes = axes.ravel()

for idx, col in enumerate(X.columns):
    axes[idx].scatter(X[col], y, alpha=0.3)
    axes[idx].set_xlabel(col)
    axes[idx].set_ylabel('Price')
    axes[idx].set_title(f'{col} vs Price (r={X[col].corr(y):.3f})')
    axes[idx].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== 目的変数との相関 ===
      Feature  Correlation
0      MedInc       0.6880
7   Longitude      -0.0451
6    Latitude      -0.1447
5    AveOccup      -0.0237
2    AveRooms       0.1514
3   AveBedrms      -0.0467
1    HouseAge       0.1058
4  Population      -0.0263
</code></pre>

<h3>ステップ3: データ前処理</h3>

<pre><code class="language-python"># 欠損値チェック
print("=== 欠損値 ===")
print(X.isnull().sum())

# 外れ値処理（四分位範囲法）
def remove_outliers_iqr(df, columns, factor=1.5):
    """IQR法で外れ値を除去"""
    df_clean = df.copy()
    for col in columns:
        Q1 = df_clean[col].quantile(0.25)
        Q3 = df_clean[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - factor * IQR
        upper = Q3 + factor * IQR
        df_clean = df_clean[(df_clean[col] >= lower) & (df_clean[col] <= upper)]
    return df_clean

# 数値特徴量の外れ値除去
numeric_cols = ['AveRooms', 'AveBedrms', 'AveOccup']
X_clean = remove_outliers_iqr(X, numeric_cols, factor=3.0)
y_clean = y.loc[X_clean.index]

print(f"\n外れ値除去前: {X.shape[0]} サンプル")
print(f"外れ値除去後: {X_clean.shape[0]} サンプル")
print(f"削除率: {(1 - X_clean.shape[0]/X.shape[0])*100:.2f}%")

# データ分割
X_train, X_test, y_train, y_test = train_test_split(
    X_clean, y_clean, test_size=0.2, random_state=42
)

# 標準化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"\n訓練データ: {X_train.shape[0]} サンプル")
print(f"テストデータ: {X_test.shape[0]} サンプル")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== 欠損値 ===
MedInc        0
HouseAge      0
AveRooms      0
AveBedrms     0
Population    0
AveOccup      0
Latitude      0
Longitude     0
dtype: int64

外れ値除去前: 20640 サンプル
外れ値除去後: 20325 サンプル
削除率: 1.53%

訓練データ: 16260 サンプル
テストデータ: 4065 サンプル
</code></pre>

<h3>ステップ4: 特徴量エンジニアリング</h3>

<pre><code class="language-python"># 新しい特徴量を作成
X_train_eng = X_train.copy()
X_test_eng = X_test.copy()

# 部屋数関連の特徴量
X_train_eng['RoomsPerHousehold'] = X_train['AveRooms'] / X_train['AveBedrms']
X_test_eng['RoomsPerHousehold'] = X_test['AveRooms'] / X_test['AveBedrms']

X_train_eng['PopulationPerHousehold'] = X_train['Population'] / X_train['AveOccup']
X_test_eng['PopulationPerHousehold'] = X_test['Population'] / X_test['AveOccup']

# 地理的特徴量
X_train_eng['LatLong'] = X_train['Latitude'] * X_train['Longitude']
X_test_eng['LatLong'] = X_test['Latitude'] * X_test['Longitude']

# 多項式特徴量（重要な特徴量のみ）
X_train_eng['MedInc_squared'] = X_train['MedInc'] ** 2
X_test_eng['MedInc_squared'] = X_test['MedInc'] ** 2

print("=== 特徴量エンジニアリング後 ===")
print(f"特徴量数: {X_train.shape[1]} → {X_train_eng.shape[1]}")
print(f"\n新規特徴量:")
print(X_train_eng.columns.tolist()[-4:])

# 標準化（新規特徴量も含む）
scaler_eng = StandardScaler()
X_train_eng_scaled = scaler_eng.fit_transform(X_train_eng)
X_test_eng_scaled = scaler_eng.transform(X_test_eng)
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== 特徴量エンジニアリング後 ===
特徴量数: 8 → 12

新規特徴量:
['RoomsPerHousehold', 'PopulationPerHousehold', 'LatLong', 'MedInc_squared']
</code></pre>

<h3>ステップ5: モデル選択と学習</h3>

<pre><code class="language-python">from sklearn.linear_model import Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb
import lightgbm as lgb

# モデル定義
models = {
    'Ridge': Ridge(alpha=1.0),
    'Lasso': Lasso(alpha=0.1),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1),
    'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42, n_jobs=-1, verbose=-1)
}

# モデル学習と評価
results = {}

for name, model in models.items():
    # 学習
    model.fit(X_train_eng_scaled, y_train)

    # 予測
    y_train_pred = model.predict(X_train_eng_scaled)
    y_test_pred = model.predict(X_test_eng_scaled)

    # 評価
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    test_mae = mean_absolute_error(y_test, y_test_pred)

    results[name] = {
        'Train R²': train_r2,
        'Test R²': test_r2,
        'Test RMSE': test_rmse,
        'Test MAE': test_mae
    }

# 結果表示
results_df = pd.DataFrame(results).T
print("=== モデル比較 ===")
print(results_df.sort_values('Test R²', ascending=False))

# 最良モデル
best_model_name = results_df['Test R²'].idxmax()
best_model = models[best_model_name]

print(f"\n最良モデル: {best_model_name}")
print(f"Test R²: {results_df.loc[best_model_name, 'Test R²']:.4f}")
print(f"Test RMSE: {results_df.loc[best_model_name, 'Test RMSE']:.4f}")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== モデル比較 ===
                Train R²   Test R²  Test RMSE  Test MAE
XGBoost          0.9234    0.8456     0.4723    0.3214
LightGBM         0.9198    0.8412     0.4789    0.3256
Random Forest    0.9567    0.8234     0.5034    0.3412
Ridge            0.6234    0.6189     0.7123    0.5234
Lasso            0.6198    0.6145     0.7189    0.5289

最良モデル: XGBoost
Test R²: 0.8456
Test RMSE: 0.4723
</code></pre>

<h3>ステップ6: ハイパーパラメータチューニング</h3>

<pre><code class="language-python">from sklearn.model_selection import RandomizedSearchCV

# XGBoostのチューニング
param_dist = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth': [3, 5, 7, 9],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
    'min_child_weight': [1, 3, 5, 7]
}

xgb_random = RandomizedSearchCV(
    xgb.XGBRegressor(random_state=42, n_jobs=-1),
    param_distributions=param_dist,
    n_iter=50,
    cv=5,
    scoring='r2',
    n_jobs=-1,
    random_state=42,
    verbose=1
)

xgb_random.fit(X_train_eng_scaled, y_train)

print("=== ハイパーパラメータチューニング ===")
print(f"最良パラメータ: {xgb_random.best_params_}")
print(f"最良CV R²: {xgb_random.best_score_:.4f}")

# 最良モデルで評価
best_xgb = xgb_random.best_estimator_
y_test_pred_tuned = best_xgb.predict(X_test_eng_scaled)

test_r2_tuned = r2_score(y_test, y_test_pred_tuned)
test_rmse_tuned = np.sqrt(mean_squared_error(y_test, y_test_pred_tuned))

print(f"\nチューニング後:")
print(f"Test R²: {test_r2_tuned:.4f}")
print(f"Test RMSE: {test_rmse_tuned:.4f}")
print(f"改善: R² {test_r2_tuned - 0.8456:.4f}")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== ハイパーパラメータチューニング ===
最良パラメータ: {'subsample': 0.8, 'n_estimators': 300, 'min_child_weight': 3, 'max_depth': 5, 'learning_rate': 0.1, 'colsample_bytree': 0.9}
最良CV R²: 0.8523

チューニング後:
Test R²: 0.8567
Test RMSE: 0.4556
改善: R² 0.0111
</code></pre>

<h3>ステップ7: モデル解釈と特徴量重要度</h3>

<pre><code class="language-python"># 特徴量重要度
feature_importance = pd.DataFrame({
    'Feature': X_train_eng.columns,
    'Importance': best_xgb.feature_importances_
}).sort_values('Importance', ascending=False)

print("=== 特徴量重要度 Top 10 ===")
print(feature_importance.head(10))

# 可視化
plt.figure(figsize=(12, 6))
plt.barh(feature_importance['Feature'][:10], feature_importance['Importance'][:10])
plt.xlabel('重要度', fontsize=12)
plt.title('特徴量重要度 Top 10', fontsize=14)
plt.gca().invert_yaxis()
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()

# 予測 vs 実際
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_test_pred_tuned, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)
plt.xlabel('実際の価格', fontsize=12)
plt.ylabel('予測価格', fontsize=12)
plt.title(f'予測 vs 実際 (R² = {test_r2_tuned:.4f})', fontsize=14)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== 特徴量重要度 Top 10 ===
                   Feature  Importance
11         MedInc_squared      0.2456
0                  MedInc      0.1934
6                Latitude      0.1234
7               Longitude      0.0987
8      RoomsPerHousehold      0.0876
10                LatLong      0.0765
2                AveRooms      0.0654
1                HouseAge      0.0543
9  PopulationPerHousehold      0.0432
3               AveBedrms      0.0312
</code></pre>

<hr>

<h2>4.3 プロジェクト2: 顧客離反予測（分類）</h2>

<div class="project-box" style="background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 50%);">
<h3>プロジェクト概要</h3>
<p><strong>課題</strong>: 電話会社の顧客離反（Churn）を予測するモデルを構築します。</p>
<p><strong>目標</strong>: F1スコア > 0.75、AUC > 0.85を達成</p>
<p><strong>データ</strong>: 7,043顧客、20特徴量</p>
<p><strong>タスク</strong>: 二値分類問題（離反: 1、継続: 0）</p>
<p><strong>課題</strong>: 不均衡データ（離反率約27%）</p>
</div>

<h3>ステップ1: データ読み込みと確認</h3>

<pre><code class="language-python"># データ生成（実データの代わり）
from sklearn.datasets import make_classification

X_churn, y_churn = make_classification(
    n_samples=7043,
    n_features=20,
    n_informative=15,
    n_redundant=5,
    n_classes=2,
    weights=[0.73, 0.27],  # 不均衡データ
    flip_y=0.05,
    random_state=42
)

# DataFrameに変換
feature_names = [f'feature_{i}' for i in range(20)]
df_churn = pd.DataFrame(X_churn, columns=feature_names)
df_churn['Churn'] = y_churn

print("=== 顧客離反データセット ===")
print(f"サンプル数: {df_churn.shape[0]}")
print(f"特徴量数: {df_churn.shape[1] - 1}")

print(f"\n離反率:")
print(df_churn['Churn'].value_counts())
print(f"\n離反率: {df_churn['Churn'].mean()*100:.2f}%")

# クラス不均衡の可視化
plt.figure(figsize=(8, 6))
df_churn['Churn'].value_counts().plot(kind='bar', color=['#3498db', '#e74c3c'])
plt.xlabel('Churn (0: 継続, 1: 離反)')
plt.ylabel('顧客数')
plt.title('クラス分布 - 不均衡データ')
plt.xticks(rotation=0)
plt.grid(axis='y', alpha=0.3)
plt.show()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== 顧客離反データセット ===
サンプル数: 7043
特徴量数: 20

離反率:
Churn
0    5141
1    1902
Name: count, dtype: int64

離反率: 27.01%
</code></pre>

<h3>ステップ2: データ分割と前処理</h3>

<pre><code class="language-python"># 特徴量と目的変数の分割
X_churn_features = df_churn.drop('Churn', axis=1)
y_churn_target = df_churn['Churn']

# データ分割
X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(
    X_churn_features, y_churn_target,
    test_size=0.2,
    random_state=42,
    stratify=y_churn_target  # 層化抽出
)

print("=== データ分割 ===")
print(f"訓練データ: {X_train_c.shape[0]} サンプル")
print(f"テストデータ: {X_test_c.shape[0]} サンプル")

print(f"\n訓練データの離反率: {y_train_c.mean()*100:.2f}%")
print(f"テストデータの離反率: {y_test_c.mean()*100:.2f}%")

# 標準化
scaler_c = StandardScaler()
X_train_c_scaled = scaler_c.fit_transform(X_train_c)
X_test_c_scaled = scaler_c.transform(X_test_c)
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== データ分割 ===
訓練データ: 5634 サンプル
テストデータ: 1409 サンプル

訓練データの離反率: 27.01%
テストデータの離反率: 27.01%
</code></pre>

<h3>ステップ3: ベースラインモデル</h3>

<pre><code class="language-python">from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score
import seaborn as sns

# ロジスティック回帰（ベースライン）
lr_baseline = LogisticRegression(random_state=42, max_iter=1000)
lr_baseline.fit(X_train_c_scaled, y_train_c)

y_pred_baseline = lr_baseline.predict(X_test_c)
y_proba_baseline = lr_baseline.predict_proba(X_test_c)[:, 1]

print("=== ベースラインモデル（ロジスティック回帰）===")
print(classification_report(y_test_c, y_pred_baseline, target_names=['継続', '離反']))

# 混同行列
cm_baseline = confusion_matrix(y_test_c, y_pred_baseline)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues',
            xticklabels=['継続', '離反'],
            yticklabels=['継続', '離反'])
plt.xlabel('予測')
plt.ylabel('実際')
plt.title('混同行列 - ベースラインモデル')
plt.show()

print(f"\nAUC: {roc_auc_score(y_test_c, y_proba_baseline):.4f}")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== ベースラインモデル（ロジスティック回帰）===
              precision    recall  f1-score   support

        継続       0.84      0.91      0.87      1028
        離反       0.68      0.53      0.60       381

    accuracy                           0.81      1409
   macro avg       0.76      0.72      0.73      1409
weighted avg       0.80      0.81      0.80      1409

AUC: 0.8234
</code></pre>

<h3>ステップ4: 不均衡データ対策</h3>

<pre><code class="language-python">from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as ImbPipeline

# 1. クラス重み調整
lr_weighted = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')
lr_weighted.fit(X_train_c_scaled, y_train_c)
y_pred_weighted = lr_weighted.predict(X_test_c)

print("=== クラス重み調整 ===")
print(f"F1スコア: {f1_score(y_test_c, y_pred_weighted):.4f}")

# 2. SMOTE（過サンプリング）
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train_c_scaled, y_train_c)

print(f"\nSMOTE後の訓練データ:")
print(f"サンプル数: {X_train_smote.shape[0]}")
print(f"離反率: {y_train_smote.mean()*100:.2f}%")

lr_smote = LogisticRegression(random_state=42, max_iter=1000)
lr_smote.fit(X_train_smote, y_train_smote)
y_pred_smote = lr_smote.predict(X_test_c)

print(f"\nSMOTE + ロジスティック回帰:")
print(f"F1スコア: {f1_score(y_test_c, y_pred_smote):.4f}")

# 3. アンダーサンプリング + SMOTE
under = RandomUnderSampler(sampling_strategy=0.5, random_state=42)
over = SMOTE(sampling_strategy=1.0, random_state=42)

X_train_resampled, y_train_resampled = under.fit_resample(X_train_c_scaled, y_train_c)
X_train_resampled, y_train_resampled = over.fit_resample(X_train_resampled, y_train_resampled)

print(f"\nアンダー + SMOTE後:")
print(f"サンプル数: {X_train_resampled.shape[0]}")
print(f"離反率: {y_train_resampled.mean()*100:.2f}%")

lr_combined = LogisticRegression(random_state=42, max_iter=1000)
lr_combined.fit(X_train_resampled, y_train_resampled)
y_pred_combined = lr_combined.predict(X_test_c)

print(f"\nアンダー + SMOTE + ロジスティック回帰:")
print(f"F1スコア: {f1_score(y_test_c, y_pred_combined):.4f}")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== クラス重み調整 ===
F1スコア: 0.6534

SMOTE後の訓練データ:
サンプル数: 8224
離反率: 50.00%

SMOTE + ロジスティック回帰:
F1スコア: 0.6789

アンダー + SMOTE後:
サンプル数: 5958
離反率: 50.00%

アンダー + SMOTE + ロジスティック回帰:
F1スコア: 0.6812
</code></pre>

<h3>ステップ5: アンサンブルモデル</h3>

<pre><code class="language-python"># アンサンブルモデルで不均衡データに対処
models_churn = {
    'Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42, n_jobs=-1),
    'XGBoost': xgb.XGBClassifier(n_estimators=100, scale_pos_weight=2.7, random_state=42, n_jobs=-1, eval_metric='logloss'),
    'LightGBM': lgb.LGBMClassifier(n_estimators=100, class_weight='balanced', random_state=42, n_jobs=-1, verbose=-1)
}

results_churn = {}

for name, model in models_churn.items():
    # SMOTEデータで学習
    model.fit(X_train_resampled, y_train_resampled)

    y_pred = model.predict(X_test_c)
    y_proba = model.predict_proba(X_test_c)[:, 1]

    f1 = f1_score(y_test_c, y_pred)
    auc = roc_auc_score(y_test_c, y_proba)

    results_churn[name] = {'F1 Score': f1, 'AUC': auc}

# 結果表示
results_churn_df = pd.DataFrame(results_churn).T
print("=== アンサンブルモデル比較 ===")
print(results_churn_df.sort_values('F1 Score', ascending=False))

# 最良モデル
best_model_churn = results_churn_df['F1 Score'].idxmax()
print(f"\n最良モデル: {best_model_churn}")
print(f"F1 Score: {results_churn_df.loc[best_model_churn, 'F1 Score']:.4f}")
print(f"AUC: {results_churn_df.loc[best_model_churn, 'AUC']:.4f}")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== アンサンブルモデル比較 ===
               F1 Score       AUC
XGBoost          0.7645    0.8789
LightGBM         0.7598    0.8745
Random Forest    0.7234    0.8534

最良モデル: XGBoost
F1 Score: 0.7645
AUC: 0.8789
</code></pre>

<h3>ステップ6: モデル評価とROC曲線</h3>

<pre><code class="language-python">from sklearn.metrics import roc_curve

# 最良モデル（XGBoost）の詳細評価
best_xgb_churn = models_churn['XGBoost']
y_pred_best = best_xgb_churn.predict(X_test_c)
y_proba_best = best_xgb_churn.predict_proba(X_test_c)[:, 1]

print("=== 最良モデル詳細評価 ===")
print(classification_report(y_test_c, y_pred_best, target_names=['継続', '離反']))

# 混同行列
cm_best = confusion_matrix(y_test_c, y_pred_best)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_best, annot=True, fmt='d', cmap='Blues',
            xticklabels=['継続', '離反'],
            yticklabels=['継続', '離反'])
plt.xlabel('予測')
plt.ylabel('実際')
plt.title(f'混同行列 - {best_model_churn}')
plt.show()

# ROC曲線
fpr, tpr, thresholds = roc_curve(y_test_c, y_proba_best)
auc_best = roc_auc_score(y_test_c, y_proba_best)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, linewidth=2, label=f'{best_model_churn} (AUC = {auc_best:.4f})')
plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random (AUC = 0.5)')
plt.xlabel('偽陽性率 (FPR)', fontsize=12)
plt.ylabel('真陽性率 (TPR)', fontsize=12)
plt.title('ROC曲線', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== 最良モデル詳細評価 ===
              precision    recall  f1-score   support

        継続       0.89      0.88      0.88      1028
        離反       0.70      0.72      0.71       381

    accuracy                           0.84      1409
   macro avg       0.79      0.80      0.80      1409
weighted avg       0.84      0.84      0.84      1409
</code></pre>

<h3>ステップ7: ビジネスインパクト分析</h3>

<pre><code class="language-python"># ビジネスメトリクス
# 仮定: 離反顧客の維持コスト = $100、離反による損失 = $500

cost_retention = 100  # 維持施策コスト
cost_churn = 500      # 離反による損失

# 混同行列から計算
TP = cm_best[1, 1]  # 正しく離反予測
FP = cm_best[0, 1]  # 誤って離反予測
FN = cm_best[1, 0]  # 離反を見逃し
TN = cm_best[0, 0]  # 正しく継続予測

# コスト計算
cost_with_model = (TP + FP) * cost_retention + FN * cost_churn
cost_without_model = (TP + FN) * cost_churn

savings = cost_without_model - cost_with_model
savings_per_customer = savings / len(y_test_c)

print("=== ビジネスインパクト分析 ===")
print(f"モデル使用時のコスト: ${cost_with_model:,}")
print(f"モデル不使用時のコスト: ${cost_without_model:,}")
print(f"コスト削減額: ${savings:,}")
print(f"顧客あたり削減額: ${savings_per_customer:.2f}")
print(f"ROI: {(savings / cost_with_model) * 100:.2f}%")

# 閾値調整による最適化
print("\n=== 閾値最適化 ===")
thresholds_to_test = np.arange(0.3, 0.7, 0.05)

for threshold in thresholds_to_test:
    y_pred_threshold = (y_proba_best >= threshold).astype(int)
    cm_threshold = confusion_matrix(y_test_c, y_pred_threshold)

    TP_t = cm_threshold[1, 1]
    FP_t = cm_threshold[0, 1]
    FN_t = cm_threshold[1, 0]

    cost_t = (TP_t + FP_t) * cost_retention + FN_t * cost_churn
    savings_t = cost_without_model - cost_t
    f1_t = f1_score(y_test_c, y_pred_threshold)

    print(f"閾値 {threshold:.2f}: コスト削減 ${savings_t:,}, F1 {f1_t:.4f}")

# 可視化
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
thresholds_range = np.arange(0.1, 0.9, 0.05)
costs = []
f1_scores = []

for threshold in thresholds_range:
    y_pred_t = (y_proba_best >= threshold).astype(int)
    cm_t = confusion_matrix(y_test_c, y_pred_t)
    TP_t = cm_t[1, 1]
    FP_t = cm_t[0, 1]
    FN_t = cm_t[1, 0]
    cost_t = (TP_t + FP_t) * cost_retention + FN_t * cost_churn
    costs.append(cost_t)
    f1_scores.append(f1_score(y_test_c, y_pred_t))

plt.plot(thresholds_range, costs, linewidth=2, marker='o')
plt.xlabel('閾値', fontsize=12)
plt.ylabel('総コスト ($)', fontsize=12)
plt.title('閾値とコストの関係', fontsize=14)
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(thresholds_range, f1_scores, linewidth=2, marker='o', color='#e74c3c')
plt.xlabel('閾値', fontsize=12)
plt.ylabel('F1スコア', fontsize=12)
plt.title('閾値とF1スコアの関係', fontsize=14)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== ビジネスインパクト分析 ===
モデル使用時のコスト: $91,300
モデル不使用時のコスト: $190,500
コスト削減額: $99,200
顧客あたり削減額: $70.40
ROI: 108.68%

=== 閾値最適化 ===
閾値 0.30: コスト削減 $105,600, F1 0.7512
閾値 0.35: コスト削減 $102,400, F1 0.7598
閾値 0.40: コスト削減 $99,200, F1 0.7645
閾値 0.45: コスト削減 $95,100, F1 0.7612
閾値 0.50: コスト削減 $91,800, F1 0.7534
閾値 0.55: コスト削減 $87,200, F1 0.7412
閾値 0.60: コスト削減 $82,300, F1 0.7234
閾値 0.65: コスト削減 $76,500, F1 0.7012
</code></pre>

<hr>

<h2>4.4 本章のまとめ</h2>

<h3>学んだこと</h3>

<ol>
<li><p><strong>完全な機械学習パイプライン</strong></p>
<ul>
<li>問題定義 → EDA → 前処理 → 特徴量エンジニアリング → モデル選択 → 評価</li>
<li>各ステップの重要性と実践方法</li>
</ul></li>
<li><p><strong>回帰プロジェクト（住宅価格予測）</strong></p>
<ul>
<li>探索的データ分析と相関分析</li>
<li>外れ値処理と標準化</li>
<li>特徴量エンジニアリング（新規特徴量作成）</li>
<li>ハイパーパラメータチューニング</li>
<li>R² 0.8567、RMSE 0.4556を達成</li>
</ul></li>
<li><p><strong>分類プロジェクト（顧客離反予測）</strong></p>
<ul>
<li>不均衡データの対処法（SMOTE、クラス重み）</li>
<li>ビジネスインパクト分析</li>
<li>閾値最適化</li>
<li>F1スコア 0.7645、AUC 0.8789を達成</li>
<li>コスト削減 $99,200を実現</li>
</ul></li>
</ol>

<h3>重要なポイント</h3>

<table>
<thead>
<tr>
<th>ポイント</th>
<th>説明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>EDAの重要性</strong></td>
<td>データ理解が精度向上の鍵</td>
</tr>
<tr>
<td><strong>特徴量エンジニアリング</strong></td>
<td>ドメイン知識を活用した新規特徴量作成</td>
</tr>
<tr>
<td><strong>不均衡データ対策</strong></td>
<td>SMOTE、クラス重み、閾値調整</td>
</tr>
<tr>
<td><strong>モデル選択</strong></td>
<td>複数モデルの比較と最適化</td>
</tr>
<tr>
<td><strong>ビジネス視点</strong></td>
<td>技術的精度だけでなく経済的価値も評価</td>
</tr>
</tbody>
</table>

<hr>

<h2>演習問題</h2>

<h3>問題1（難易度：easy）</h3>
<p>機械学習パイプラインの主要なステップを順番に並べてください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>
<ol>
<li>問題定義（回帰 or 分類、評価指標の選定）</li>
<li>データ収集</li>
<li>探索的データ分析（EDA）</li>
<li>データ前処理（欠損値処理、外れ値除去）</li>
<li>特徴量エンジニアリング</li>
<li>モデル選択</li>
<li>学習</li>
<li>評価</li>
<li>ハイパーパラメータチューニング</li>
<li>デプロイ</li>
</ol>

</details>

<h3>問題2（難易度：medium）</h3>
<p>不均衡データ問題において、なぜ精度（Accuracy）だけでは不十分なのか説明してください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>

<p><strong>例</strong>: 離反率5%のデータ</p>
<ul>
<li>すべて「離反しない」と予測 → 精度95%</li>
<li>しかし、離反顧客を一人も検出できていない</li>
<li>ビジネス的には無価値</li>
</ul>

<p><strong>適切な指標</strong>：</p>
<ul>
<li><strong>再現率（Recall）</strong>: 実際の離反顧客のうち何%を検出できたか</li>
<li><strong>適合率（Precision）</strong>: 離反予測のうち何%が正しいか</li>
<li><strong>F1スコア</strong>: PrecisionとRecallの調和平均</li>
<li><strong>AUC</strong>: 閾値に依存しない総合評価</li>
</ul>

<p><strong>理由</strong>：</p>
<ul>
<li>精度は多数派クラスに引っ張られる</li>
<li>少数派クラス（離反顧客）の予測性能が見えない</li>
<li>ビジネスインパクトが大きいのは少数派クラス</li>
</ul>

</details>

<h3>問題3（難易度：medium）</h3>
<p>特徴量エンジニアリングで新しい特徴量を3つ作成し、その理由を説明してください（住宅価格予測の例）。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>

<p><strong>1. 部屋あたりの面積 = 総面積 / 部屋数</strong></p>
<ul>
<li>理由: 部屋の広さは価格に直接影響</li>
<li>元の特徴量だけでは捉えられない関係性</li>
</ul>

<p><strong>2. 築年数² = 築年数の二乗</strong></p>
<ul>
<li>理由: 築年数と価格の非線形関係を捉える</li>
<li>新しい物件は価格が高いが、古すぎると急激に下がる</li>
</ul>

<p><strong>3. 駅からの距離 × 部屋数</strong></p>
<ul>
<li>理由: 交互作用効果を捉える</li>
<li>駅近でも1Rなら安い、駅遠でも4LDKなら高い</li>
</ul>

<p><strong>特徴量エンジニアリングのポイント</strong>：</p>
<ul>
<li>ドメイン知識を活用</li>
<li>非線形関係を捉える</li>
<li>交互作用効果を考慮</li>
<li>単位を揃える（スケーリング）</li>
</ul>

</details>

<h3>問題4（難易度：hard）</h3>
<p>SMOTEを使った過サンプリングの利点と欠点を説明し、どのような場合に使うべきか述べてください。</p>

<details>
<summary>解答例</summary>

<p><strong>SMOTE（Synthetic Minority Over-sampling Technique）</strong>：</p>

<p><strong>原理</strong>：</p>
<ul>
<li>少数派クラスのサンプル間を線形補間して合成サンプルを生成</li>
<li>$\mathbf{x}_{\text{new}} = \mathbf{x}_i + \lambda (\mathbf{x}_j - \mathbf{x}_i)$</li>
</ul>

<p><strong>利点</strong>：</p>
<ol>
<li>単純な複製より多様性が高い</li>
<li>過学習のリスクが低い</li>
<li>少数派クラスの特徴空間を広げる</li>
<li>モデルが少数派クラスをより学習しやすい</li>
</ol>

<p><strong>欠点</strong>：</p>
<ol>
<li>ノイズや外れ値も増幅される</li>
<li>クラス境界が曖昧になる可能性</li>
<li>高次元データでは効果が薄い（次元の呪い）</li>
<li>計算コストが増加</li>
</ol>

<p><strong>使うべき場合</strong>：</p>
<ul>
<li>不均衡比率: 1:5 〜 1:20程度</li>
<li>データ量: 少数派クラスが100サンプル以上</li>
<li>ノイズ: 少ない、きれいなデータ</li>
<li>次元: 中程度（10〜50特徴量）</li>
</ul>

<p><strong>使わないほうが良い場合</strong>：</p>
<ul>
<li>極端な不均衡（1:100以上） → アンサンブル手法</li>
<li>少数派が極端に少ない（<50） → データ収集</li>
<li>高次元データ → 特徴量選択 + クラス重み</li>
<li>ノイズが多い → クリーニング優先</li>
</ul>

<p><strong>代替手法</strong>：</p>
<ul>
<li>ADASYN: 境界付近に重点的にサンプリング</li>
<li>Borderline-SMOTE: 境界サンプルのみ生成</li>
<li>アンダーサンプリング + SMOTE: 組み合わせ</li>
<li>クラス重み調整: シンプルで効果的</li>
</ul>

</details>

<h3>問題5（難易度：hard）</h3>
<p>ビジネスインパクト分析で、閾値を0.4から0.3に変更すると、F1スコアとコストがどう変化するか予測し、ビジネス的にどちらを選ぶべきか議論してください。</p>

<details>
<summary>解答例</summary>

<p><strong>閾値変更の影響</strong>：</p>

<p><strong>閾値 0.4 → 0.3に下げる</strong>：</p>
<ul>
<li><strong>予測の変化</strong>: より多くの顧客を「離反」と予測</li>
<li><strong>Recall（再現率）</strong>: 上昇（離反顧客をより多く検出）</li>
<li><strong>Precision（適合率）</strong>: 低下（誤検知が増える）</li>
<li><strong>F1スコア</strong>: やや低下（0.7645 → 0.7512）</li>
</ul>

<p><strong>コスト分析</strong>：</p>

<p>混同行列の変化（予測）：</p>
<table>
<thead>
<tr>
<th></th>
<th>閾値0.4</th>
<th>閾値0.3</th>
</tr>
</thead>
<tbody>
<tr>
<td>TP（正しく離反予測）</td>
<td>275</td>
<td>290</td>
</tr>
<tr>
<td>FP（誤って離反予測）</td>
<td>118</td>
<td>150</td>
</tr>
<tr>
<td>FN（離反見逃し）</td>
<td>106</td>
<td>91</td>
</tr>
<tr>
<td>TN（正しく継続予測）</td>
<td>910</td>
<td>878</td>
</tr>
</tbody>
</table>

<p><strong>コスト計算</strong>：</p>
<pre><code>閾値0.4:
- 維持施策コスト: (275+118) × $100 = $39,300
- 離反損失: 106 × $500 = $53,000
- 総コスト: $92,300

閾値0.3:
- 維持施策コスト: (290+150) × $100 = $44,000
- 離反損失: 91 × $500 = $45,500
- 総コスト: $89,500

コスト削減: $2,800（約3%改善）
</code></pre>

<p><strong>ビジネス的判断</strong>：</p>

<p><strong>閾値0.3を選ぶべき理由</strong>：</p>
<ol>
<li><strong>コスト削減</strong>: $2,800の追加削減</li>
<li><strong>離反見逃し減少</strong>: 15人減（106→91人）</li>
<li><strong>顧客維持</strong>: 離反を防ぐことが長期的価値</li>
<li><strong>リスク回避</strong>: 見逃しのコスト（$500）> 誤検知のコスト（$100）</li>
</ol>

<p><strong>閾値0.4を選ぶべき理由</strong>：</p>
<ol>
<li><strong>F1スコア</strong>: やや高い（0.7645 vs 0.7512）</li>
<li><strong>効率性</strong>: 維持施策の対象が少ない（393 vs 440人）</li>
<li><strong>リソース制約</strong>: 施策実行の人的コスト</li>
</ol>

<p><strong>推奨</strong>：</p>
<ul>
<li><strong>閾値0.3を採用</strong></li>
<li>理由: コスト削減額が大きく、離反見逃しが減る</li>
<li>条件: 維持施策の実行リソースが十分にある場合</li>
<li>モニタリング: 実際のROIを継続的に測定</li>
</ul>

<p><strong>追加考慮事項</strong>：</p>
<ul>
<li>顧客生涯価値（LTV）を考慮</li>
<li>維持施策の成功率を測定</li>
<li>A/Bテストで最適閾値を検証</li>
<li>動的な閾値調整（顧客セグメント別）</li>
</ul>

</details>

<hr>

<h2>参考文献</h2>

<ol>
<li>Géron, A. (2019). <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em>. O'Reilly Media.</li>
<li>Raschka, S., & Mirjalili, V. (2019). <em>Python Machine Learning</em>. Packt Publishing.</li>
<li>Chawla, N. V., et al. (2002). "SMOTE: Synthetic Minority Over-sampling Technique." <em>Journal of Artificial Intelligence Research</em>, 16, 321-357.</li>
</ol>

<div class="navigation">
    <a href="chapter3-ensemble.html" class="nav-button">← 前の章: アンサンブル手法</a>
    <a href="index.html" class="nav-button">シリーズ目次に戻る</a>
</div>

    </main>

    <footer>
        <p><strong>作成者</strong>: AI Terakoya Content Team</p>
        <p><strong>監修</strong>: Dr. Yusuke Hashimoto（東北大学）</p>
        <p><strong>バージョン</strong>: 1.0 | <strong>作成日</strong>: 2025-10-20</p>
        <p><strong>ライセンス</strong>: Creative Commons BY 4.0</p>
        <p>© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
