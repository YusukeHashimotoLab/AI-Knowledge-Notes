<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬2ç« ï¼šXGBoost - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬2ç« ï¼šXGBoost</h1>
            <p class="subtitle">å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã®æœ€é©åŒ–å®Ÿè£… - é«˜é€Ÿã§æ­£ç¢ºãªäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 9å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… XGBoostã®åŸç†ã¨æ­£å‰‡åŒ–æŠ€è¡“ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½¹å‰²ã¨èª¿æ•´æ–¹æ³•ã‚’ç¿’å¾—ã™ã‚‹</li>
<li>âœ… xgboostãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§åŠ¹ç‡çš„ãªè¨“ç·´ã‚’å®Ÿè¡Œã§ãã‚‹</li>
<li>âœ… ç‰¹å¾´é‡é‡è¦åº¦ã‚’è§£é‡ˆã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’æ”¹å–„ã§ãã‚‹</li>
<li>âœ… GPUåŠ é€Ÿã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã‚’å®Ÿè·µã§ãã‚‹</li>
</ul>

<hr>

<h2>2.1 XGBoostã®åŸç†</h2>

<h3>XGBoostã¨ã¯</h3>
<p><strong>XGBoostï¼ˆeXtreme Gradient Boostingï¼‰</strong>ã¯ã€å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°æ±ºå®šæœ¨ã®é«˜é€Ÿã§åŠ¹ç‡çš„ãªå®Ÿè£…ã§ã™ã€‚</p>

<blockquote>
<p>ã€ŒXGBoostã¯å¤šãã®Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã§å„ªå‹ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚ã€</p>
</blockquote>

<h3>ä¸»è¦ãªç‰¹å¾´</h3>

<table>
<thead>
<tr>
<th>ç‰¹å¾´</th>
<th>èª¬æ˜</th>
<th>åˆ©ç‚¹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ­£å‰‡åŒ–</strong></td>
<td>L1/L2æ­£å‰‡åŒ–ã«ã‚ˆã‚‹éå­¦ç¿’é˜²æ­¢</td>
<td>æ±åŒ–æ€§èƒ½å‘ä¸Š</td>
</tr>
<tr>
<td><strong>æœ¨ã®å‰ªå®š</strong></td>
<td>max_depthå¾Œã®æåˆˆã‚Š</td>
<td>åŠ¹ç‡çš„ãªå­¦ç¿’</td>
</tr>
<tr>
<td><strong>ä¸¦åˆ—å‡¦ç†</strong></td>
<td>åˆ—ï¼ˆç‰¹å¾´é‡ï¼‰å˜ä½ã®ä¸¦åˆ—åŒ–</td>
<td>é«˜é€Ÿãªè¨“ç·´</td>
</tr>
<tr>
<td><strong>æ¬ æå€¤å‡¦ç†</strong></td>
<td>è‡ªå‹•ã§æœ€é©ãªæ–¹å‘ã‚’å­¦ç¿’</td>
<td>å‰å‡¦ç†ä¸è¦</td>
</tr>
</tbody>
</table>

<h3>XGBoostã®ç›®çš„é–¢æ•°</h3>

<p>XGBoostã¯ä»¥ä¸‹ã®ç›®çš„é–¢æ•°ã‚’æœ€å°åŒ–ã—ã¾ã™ï¼š</p>

<p>$$
\mathcal{L} = \sum_{i=1}^{n} l(y_i, \hat{y}_i) + \sum_{k=1}^{K} \Omega(f_k)
$$</p>

<ul>
<li>$l$: æå¤±é–¢æ•°ï¼ˆäºŒä¹—èª¤å·®ã€å¯¾æ•°æå¤±ãªã©ï¼‰</li>
<li>$\Omega(f_k)$: æ­£å‰‡åŒ–é …ï¼ˆæœ¨ã®è¤‡é›‘ã•ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰</li>
</ul>

<p>æ­£å‰‡åŒ–é …ï¼š</p>

<p>$$
\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T} w_j^2
$$</p>

<ul>
<li>$T$: è‘‰ã®æ•°</li>
<li>$w_j$: è‘‰ã®é‡ã¿</li>
<li>$\gamma$: è‘‰æ•°ã®ãƒšãƒŠãƒ«ãƒ†ã‚£</li>
<li>$\lambda$: L2æ­£å‰‡åŒ–ä¿‚æ•°</li>
</ul>

<h3>åŸºæœ¬çš„ãªå®Ÿè£…</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
X, y = make_classification(
    n_samples=1000,
    n_features=20,
    n_informative=15,
    n_redundant=5,
    random_state=42
)

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# XGBoostãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=3,
    learning_rate=0.1,
    random_state=42
)

# è¨“ç·´
model.fit(X_train, y_train)

# äºˆæ¸¬
y_pred = model.predict(X_test)

# è©•ä¾¡
accuracy = accuracy_score(y_test, y_pred)
print("=== XGBoostã®åŸºæœ¬æ€§èƒ½ ===")
print(f"ç²¾åº¦: {accuracy:.3f}")
print(f"\nè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_test, y_pred))
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== XGBoostã®åŸºæœ¬æ€§èƒ½ ===
ç²¾åº¦: 0.935

è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ:
              precision    recall  f1-score   support

           0       0.94      0.93      0.93       102
           1       0.93      0.94      0.93        98

    accuracy                           0.93       200
   macro avg       0.93      0.93      0.93       200
weighted avg       0.93      0.93      0.93       200
</code></pre>

<hr>

<h2>2.2 ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</h2>

<h3>ä¸»è¦ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</h3>

<table>
<thead>
<tr>
<th>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</th>
<th>èª¬æ˜</th>
<th>æ¨å¥¨ç¯„å›²</th>
<th>ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>learning_rate (eta)</strong></td>
<td>å­¦ç¿’ç‡ã€å„æœ¨ã®å¯„ä¸ã‚’ç¸®å°</td>
<td>0.01 - 0.3</td>
<td>0.3</td>
</tr>
<tr>
<td><strong>max_depth</strong></td>
<td>æœ¨ã®æœ€å¤§æ·±ã•</td>
<td>3 - 10</td>
<td>6</td>
</tr>
<tr>
<td><strong>n_estimators</strong></td>
<td>æœ¨ã®æ•°</td>
<td>100 - 1000</td>
<td>100</td>
</tr>
<tr>
<td><strong>subsample</strong></td>
<td>å„æœ¨ã§ä½¿ç”¨ã™ã‚‹è¡Œã®å‰²åˆ</td>
<td>0.5 - 1.0</td>
<td>1.0</td>
</tr>
<tr>
<td><strong>colsample_bytree</strong></td>
<td>å„æœ¨ã§ä½¿ç”¨ã™ã‚‹åˆ—ã®å‰²åˆ</td>
<td>0.5 - 1.0</td>
<td>1.0</td>
</tr>
<tr>
<td><strong>gamma</strong></td>
<td>åˆ†å²ã®æœ€å°æå¤±æ¸›å°‘</td>
<td>0 - 5</td>
<td>0</td>
</tr>
<tr>
<td><strong>reg_alpha</strong></td>
<td>L1æ­£å‰‡åŒ–ï¼ˆé‡ã¿ã®çµ¶å¯¾å€¤ï¼‰</td>
<td>0 - 1</td>
<td>0</td>
</tr>
<tr>
<td><strong>reg_lambda</strong></td>
<td>L2æ­£å‰‡åŒ–ï¼ˆé‡ã¿ã®äºŒä¹—ï¼‰</td>
<td>0 - 1</td>
<td>1</td>
</tr>
</tbody>
</table>

<h3>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ã‚’å¯è¦–åŒ–</h3>

<pre><code class="language-python">import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X, y = make_classification(
    n_samples=1000,
    n_features=20,
    n_informative=15,
    random_state=42
)

# learning_rateã®å½±éŸ¿
learning_rates = [0.01, 0.05, 0.1, 0.2, 0.3]
lr_scores = []

for lr in learning_rates:
    model = xgb.XGBClassifier(
        learning_rate=lr,
        n_estimators=100,
        max_depth=3,
        random_state=42
    )
    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
    lr_scores.append(scores.mean())

# max_depthã®å½±éŸ¿
max_depths = [2, 3, 4, 5, 6, 8, 10]
depth_scores = []

for depth in max_depths:
    model = xgb.XGBClassifier(
        max_depth=depth,
        n_estimators=100,
        learning_rate=0.1,
        random_state=42
    )
    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
    depth_scores.append(scores.mean())

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].plot(learning_rates, lr_scores, marker='o', linewidth=2)
axes[0].set_xlabel('Learning Rate', fontsize=12)
axes[0].set_ylabel('Cross-Validation Accuracy', fontsize=12)
axes[0].set_title('Learning Rate vs Accuracy', fontsize=14)
axes[0].grid(True, alpha=0.3)

axes[1].plot(max_depths, depth_scores, marker='s', linewidth=2, color='orange')
axes[1].set_xlabel('Max Depth', fontsize=12)
axes[1].set_ylabel('Cross-Validation Accuracy', fontsize=12)
axes[1].set_title('Max Depth vs Accuracy', fontsize=14)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´çµæœ ===")
print(f"æœ€é©learning_rate: {learning_rates[np.argmax(lr_scores)]}")
print(f"æœ€é©max_depth: {max_depths[np.argmax(depth_scores)]}")
</code></pre>

<h3>subsampleã¨colsampleã®åŠ¹æœ</h3>

<pre><code class="language-python"># subsampleã¨colsample_bytreeã®çµ„ã¿åˆã‚ã›
subsample_values = [0.5, 0.7, 0.9, 1.0]
colsample_values = [0.5, 0.7, 0.9, 1.0]

results = []

for sub in subsample_values:
    for col in colsample_values:
        model = xgb.XGBClassifier(
            subsample=sub,
            colsample_bytree=col,
            n_estimators=100,
            max_depth=3,
            learning_rate=0.1,
            random_state=42
        )
        scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
        results.append({
            'subsample': sub,
            'colsample': col,
            'accuracy': scores.mean()
        })

# çµæœã®å¯è¦–åŒ–
df_results = pd.DataFrame(results)
pivot_table = df_results.pivot(
    index='subsample',
    columns='colsample',
    values='accuracy'
)

import seaborn as sns
plt.figure(figsize=(10, 8))
sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='YlGnBu',
            cbar_kws={'label': 'Accuracy'})
plt.title('Subsample vs Colsample_bytree', fontsize=14)
plt.xlabel('Colsample_bytree', fontsize=12)
plt.ylabel('Subsample', fontsize=12)
plt.tight_layout()
plt.show()

print("\n=== æœ€é©ãªçµ„ã¿åˆã‚ã› ===")
best_idx = df_results['accuracy'].idxmax()
print(f"subsample: {df_results.loc[best_idx, 'subsample']}")
print(f"colsample_bytree: {df_results.loc[best_idx, 'colsample']}")
print(f"ç²¾åº¦: {df_results.loc[best_idx, 'accuracy']:.3f}")
</code></pre>

<hr>

<h2>2.3 å®Ÿè£…ã¨è¨“ç·´</h2>

<h3>DMatrixå½¢å¼ã§ã®åŠ¹ç‡çš„ãªè¨“ç·´</h3>

<pre><code class="language-python">import xgboost as xgb
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
data = load_breast_cancer()
X, y = data.data, data.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# DMatrixå½¢å¼ã«å¤‰æ›ï¼ˆé«˜é€ŸåŒ–ï¼‰
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
params = {
    'objective': 'binary:logistic',
    'max_depth': 3,
    'learning_rate': 0.1,
    'eval_metric': 'logloss'
}

# è¨“ç·´ï¼ˆè©•ä¾¡ã‚»ãƒƒãƒˆä»˜ãï¼‰
evals = [(dtrain, 'train'), (dtest, 'test')]
num_rounds = 100

print("=== XGBoostè¨“ç·´ï¼ˆDMatrixå½¢å¼ï¼‰===")
model = xgb.train(
    params,
    dtrain,
    num_boost_round=num_rounds,
    evals=evals,
    early_stopping_rounds=10,
    verbose_eval=20
)

print(f"\næœ€é©ãªåå¾©å›æ•°: {model.best_iteration}")
print(f"æœ€è‰¯ã‚¹ã‚³ã‚¢: {model.best_score:.4f}")
</code></pre>

<h3>Early Stoppingã«ã‚ˆã‚‹éå­¦ç¿’é˜²æ­¢</h3>

<pre><code class="language-python">from sklearn.model_selection import train_test_split

# æ¤œè¨¼ã‚»ãƒƒãƒˆã‚’å«ã‚€åˆ†å‰²
X_train_full, X_test, y_train_full, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_full, y_train_full, test_size=0.2, random_state=42
)

# XGBoostãƒ¢ãƒ‡ãƒ«ï¼ˆsklearn APIï¼‰
model = xgb.XGBClassifier(
    n_estimators=1000,
    max_depth=3,
    learning_rate=0.1,
    random_state=42
)

# Early Stoppingä»˜ãè¨“ç·´
model.fit(
    X_train, y_train,
    eval_set=[(X_train, y_train), (X_val, y_val)],
    early_stopping_rounds=20,
    verbose=50
)

# ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®è©•ä¾¡
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print("\n=== Early Stoppingçµæœ ===")
print(f"ä½¿ç”¨ã•ã‚ŒãŸæœ¨ã®æ•°: {model.best_iteration}")
print(f"æ¤œè¨¼ç²¾åº¦: {accuracy:.3f}")

# å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–
results = model.evals_result()

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(results['validation_0']['logloss'], label='Train')
plt.plot(results['validation_1']['logloss'], label='Validation')
plt.axvline(x=model.best_iteration, color='r', linestyle='--',
            label=f'Best iteration: {model.best_iteration}')
plt.xlabel('Number of Trees')
plt.ylabel('Log Loss')
plt.title('Learning Curve with Early Stopping')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(results['validation_0']['logloss'][-100:], label='Train (last 100)')
plt.plot(results['validation_1']['logloss'][-100:], label='Validation (last 100)')
plt.xlabel('Number of Trees')
plt.ylabel('Log Loss')
plt.title('Learning Curve (Detail)')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3>Cross-Validationã«ã‚ˆã‚‹æ€§èƒ½è©•ä¾¡</h3>

<pre><code class="language-python">import xgboost as xgb

# DMatrixå½¢å¼ã§ã®ãƒ‡ãƒ¼ã‚¿
dtrain = xgb.DMatrix(X, label=y)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
params = {
    'objective': 'binary:logistic',
    'max_depth': 3,
    'learning_rate': 0.1,
    'eval_metric': 'logloss'
}

# Cross-Validation
cv_results = xgb.cv(
    params,
    dtrain,
    num_boost_round=200,
    nfold=5,
    metrics='logloss',
    early_stopping_rounds=20,
    seed=42,
    verbose_eval=50
)

print("\n=== Cross-Validationçµæœ ===")
print(f"æœ€é©ãªåå¾©å›æ•°: {len(cv_results)}")
print(f"è¨“ç·´ log loss: {cv_results['train-logloss-mean'].iloc[-1]:.4f}")
print(f"æ¤œè¨¼ log loss: {cv_results['test-logloss-mean'].iloc[-1]:.4f}")
print(f"æ¨™æº–åå·®: {cv_results['test-logloss-std'].iloc[-1]:.4f}")

# CVçµæœã®å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.plot(cv_results['train-logloss-mean'], label='Train')
plt.plot(cv_results['test-logloss-mean'], label='Test')
plt.fill_between(
    range(len(cv_results)),
    cv_results['test-logloss-mean'] - cv_results['test-logloss-std'],
    cv_results['test-logloss-mean'] + cv_results['test-logloss-std'],
    alpha=0.2
)
plt.xlabel('Number of Trees')
plt.ylabel('Log Loss')
plt.title('Cross-Validation Learning Curve')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<hr>

<h2>2.4 ç‰¹å¾´é‡é‡è¦åº¦</h2>

<h3>é‡è¦åº¦ã®ç¨®é¡</h3>

<table>
<thead>
<tr>
<th>ã‚¿ã‚¤ãƒ—</th>
<th>èª¬æ˜</th>
<th>è§£é‡ˆ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>gain</strong></td>
<td>ãã®ç‰¹å¾´é‡ã«ã‚ˆã‚‹æå¤±ã®å¹³å‡æ”¹å–„é‡</td>
<td>äºˆæ¸¬ç²¾åº¦ã¸ã®å¯„ä¸</td>
</tr>
<tr>
<td><strong>weight</strong></td>
<td>ç‰¹å¾´é‡ãŒåˆ†å²ã«ä½¿ç”¨ã•ã‚ŒãŸå›æ•°</td>
<td>ä½¿ç”¨é »åº¦</td>
</tr>
<tr>
<td><strong>cover</strong></td>
<td>åˆ†å²ã§ã‚«ãƒãƒ¼ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«æ•°</td>
<td>å½±éŸ¿ç¯„å›²</td>
</tr>
</tbody>
</table>

<h3>ç‰¹å¾´é‡é‡è¦åº¦ã®è¨ˆç®—ã¨å¯è¦–åŒ–</h3>

<pre><code class="language-python">import xgboost as xgb
from sklearn.datasets import load_breast_cancer
import matplotlib.pyplot as plt

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
data = load_breast_cancer()
X, y = data.data, data.target
feature_names = data.feature_names

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=3,
    learning_rate=0.1,
    random_state=42
)
model.fit(X, y)

# 3ã¤ã®é‡è¦åº¦ã‚¿ã‚¤ãƒ—ã‚’å–å¾—
importance_gain = model.get_booster().get_score(importance_type='gain')
importance_weight = model.get_booster().get_score(importance_type='weight')
importance_cover = model.get_booster().get_score(importance_type='cover')

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# Gain
xgb.plot_importance(model, importance_type='gain', max_num_features=10,
                    ax=axes[0], title='Feature Importance (Gain)')
axes[0].set_xlabel('Gain')

# Weight
xgb.plot_importance(model, importance_type='weight', max_num_features=10,
                    ax=axes[1], title='Feature Importance (Weight)')
axes[1].set_xlabel('Weight')

# Cover
xgb.plot_importance(model, importance_type='cover', max_num_features=10,
                    ax=axes[2], title='Feature Importance (Cover)')
axes[2].set_xlabel('Cover')

plt.tight_layout()
plt.show()

# æ•°å€¤ã§ç¢ºèª
print("=== Top 10ç‰¹å¾´é‡ï¼ˆGainåŸºæº–ï¼‰===")
importance_dict = model.get_booster().get_score(importance_type='gain')
sorted_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)
for i, (feature, score) in enumerate(sorted_features[:10], 1):
    feature_idx = int(feature.replace('f', ''))
    print(f"{i}. {feature_names[feature_idx]}: {score:.2f}")
</code></pre>

<h3>SHAPå€¤ã«ã‚ˆã‚‹è©³ç´°åˆ†æ</h3>

<pre><code class="language-python">import shap
import xgboost as xgb
import matplotlib.pyplot as plt

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = xgb.XGBClassifier(n_estimators=100, max_depth=3, random_state=42)
model.fit(X_train, y_train)

# SHAPå€¤ã®è¨ˆç®—
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# SHAP Summary Plot
plt.figure(figsize=(12, 8))
shap.summary_plot(shap_values, X_test, feature_names=feature_names,
                  show=False)
plt.title('SHAP Summary Plot', fontsize=14)
plt.tight_layout()
plt.show()

# SHAP Feature Importance
plt.figure(figsize=(10, 6))
shap.summary_plot(shap_values, X_test, feature_names=feature_names,
                  plot_type='bar', show=False)
plt.title('SHAP Feature Importance', fontsize=14)
plt.tight_layout()
plt.show()

print("=== SHAPå€¤ã«ã‚ˆã‚‹é‡è¦åº¦åˆ†æ ===")
print("å„ç‰¹å¾´é‡ã®å¹³å‡çµ¶å¯¾SHAPå€¤ï¼ˆTop 5ï¼‰:")
shap_importance = np.abs(shap_values).mean(axis=0)
top_indices = np.argsort(shap_importance)[-5:][::-1]
for idx in top_indices:
    print(f"{feature_names[idx]}: {shap_importance[idx]:.4f}")
</code></pre>

<hr>

<h2>2.5 å®Ÿè·µæœ€é©åŒ–</h2>

<h3>GPUåŠ é€Ÿ</h3>

<pre><code class="language-python">import xgboost as xgb
import time

# ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰
X, y = make_classification(
    n_samples=100000,
    n_features=50,
    n_informative=40,
    random_state=42
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# CPUè¨“ç·´
start_time = time.time()
model_cpu = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    tree_method='hist',  # CPUã§ã®é«˜é€ŸåŒ–
    random_state=42
)
model_cpu.fit(X_train, y_train)
cpu_time = time.time() - start_time

# GPUè¨“ç·´ï¼ˆGPUãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
try:
    start_time = time.time()
    model_gpu = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        tree_method='gpu_hist',  # GPUåŠ é€Ÿ
        random_state=42
    )
    model_gpu.fit(X_train, y_train)
    gpu_time = time.time() - start_time

    print("=== GPU vs CPUæ€§èƒ½æ¯”è¼ƒ ===")
    print(f"CPUè¨“ç·´æ™‚é–“: {cpu_time:.2f}ç§’")
    print(f"GPUè¨“ç·´æ™‚é–“: {gpu_time:.2f}ç§’")
    print(f"é€Ÿåº¦å‘ä¸Š: {cpu_time/gpu_time:.2f}x")
except Exception as e:
    print("=== CPUè¨“ç·´çµæœ ===")
    print(f"è¨“ç·´æ™‚é–“: {cpu_time:.2f}ç§’")
    print(f"GPUåˆ©ç”¨ä¸å¯: {str(e)}")

# æ€§èƒ½è©•ä¾¡
y_pred = model_cpu.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"\nãƒ†ã‚¹ãƒˆç²¾åº¦: {accuracy:.3f}")
</code></pre>

<h3>ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</h3>

<pre><code class="language-python">from sklearn.model_selection import GridSearchCV
import xgboost as xgb

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X, y = make_classification(
    n_samples=2000,
    n_features=20,
    n_informative=15,
    random_state=42
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'n_estimators': [50, 100, 200],
    'subsample': [0.7, 0.9, 1.0],
    'colsample_bytree': [0.7, 0.9, 1.0]
}

# ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ
model = xgb.XGBClassifier(random_state=42)

grid_search = GridSearchCV(
    model,
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=2
)

print("=== ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒå®Ÿè¡Œä¸­ ===")
grid_search.fit(X_train, y_train)

# æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
print("\n=== æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===")
print(grid_search.best_params_)
print(f"\næœ€è‰¯CVç²¾åº¦: {grid_search.best_score_:.3f}")

# ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®è©•ä¾¡
y_pred = grid_search.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)
print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_accuracy:.3f}")

# ä¸Šä½5ã¤ã®çµ„ã¿åˆã‚ã›
cv_results = pd.DataFrame(grid_search.cv_results_)
top_5 = cv_results.nlargest(5, 'mean_test_score')[
    ['params', 'mean_test_score', 'std_test_score']
]
print("\n=== Top 5ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã› ===")
for i, row in top_5.iterrows():
    print(f"\n{i+1}. ã‚¹ã‚³ã‚¢: {row['mean_test_score']:.4f} (Â±{row['std_test_score']:.4f})")
    print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {row['params']}")
</code></pre>

<h3>ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªæ¢ç´¢</h3>

<pre><code class="language-python">from sklearn.model_selection import cross_val_score
from scipy.stats import uniform, randint

# ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒï¼ˆãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ä»£æ›¿ï¼‰
from sklearn.model_selection import RandomizedSearchCV

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†å¸ƒ
param_distributions = {
    'max_depth': randint(3, 10),
    'learning_rate': uniform(0.01, 0.29),
    'n_estimators': randint(50, 300),
    'subsample': uniform(0.6, 0.4),
    'colsample_bytree': uniform(0.6, 0.4),
    'gamma': uniform(0, 5),
    'reg_alpha': uniform(0, 1),
    'reg_lambda': uniform(0, 1)
}

# ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒ
model = xgb.XGBClassifier(random_state=42)

random_search = RandomizedSearchCV(
    model,
    param_distributions,
    n_iter=50,  # è©¦è¡Œå›æ•°
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42,
    verbose=1
)

print("=== ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒå®Ÿè¡Œä¸­ ===")
random_search.fit(X_train, y_train)

# æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
print("\n=== æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒï¼‰===")
print(random_search.best_params_)
print(f"\næœ€è‰¯CVç²¾åº¦: {random_search.best_score_:.3f}")

# ãƒ†ã‚¹ãƒˆç²¾åº¦
y_pred = random_search.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)
print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_accuracy:.3f}")

# æ¢ç´¢éç¨‹ã®å¯è¦–åŒ–
cv_results = pd.DataFrame(random_search.cv_results_)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(range(len(cv_results)), cv_results['mean_test_score'],
            alpha=0.6, edgecolors='black')
plt.axhline(y=random_search.best_score_, color='r', linestyle='--',
            label=f'Best: {random_search.best_score_:.3f}')
plt.xlabel('Iteration')
plt.ylabel('CV Score')
plt.title('Random Search Progress')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
sorted_scores = sorted(cv_results['mean_test_score'])
plt.plot(sorted_scores, linewidth=2)
plt.xlabel('Rank')
plt.ylabel('CV Score')
plt.title('Sorted CV Scores')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<hr>

<h2>2.6 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>XGBoostã®åŸç†</strong></p>
<ul>
<li>å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã®æœ€é©åŒ–å®Ÿè£…</li>
<li>æ­£å‰‡åŒ–ã«ã‚ˆã‚‹éå­¦ç¿’é˜²æ­¢</li>
<li>åŠ¹ç‡çš„ãªæœ¨ã®å‰ªå®šã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </li>
</ul></li>

<li><p><strong>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</strong></p>
<ul>
<li>learning_rate: å­¦ç¿’ã®å®‰å®šæ€§ã¨é€Ÿåº¦</li>
<li>max_depth: ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•åˆ¶å¾¡</li>
<li>subsample/colsample: ãƒ©ãƒ³ãƒ€ãƒ æ€§å°å…¥</li>
<li>gamma, reg_alpha, reg_lambda: æ­£å‰‡åŒ–</li>
</ul></li>

<li><p><strong>å®Ÿè£…ã¨è¨“ç·´</strong></p>
<ul>
<li>DMatrixå½¢å¼ã§ã®åŠ¹ç‡çš„ãªå‡¦ç†</li>
<li>Early Stoppingã«ã‚ˆã‚‹è‡ªå‹•æœ€é©åŒ–</li>
<li>Cross-Validationã§ã®å …ç‰¢ãªè©•ä¾¡</li>
</ul></li>

<li><p><strong>ç‰¹å¾´é‡é‡è¦åº¦</strong></p>
<ul>
<li>gain, weight, coverã®3ç¨®é¡</li>
<li>SHAPå€¤ã«ã‚ˆã‚‹è©³ç´°ãªè§£é‡ˆ</li>
<li>ãƒ¢ãƒ‡ãƒ«æ”¹å–„ã¸ã®æ´»ç”¨</li>
</ul></li>

<li><p><strong>å®Ÿè·µæœ€é©åŒ–</strong></p>
<ul>
<li>GPUåŠ é€Ÿã«ã‚ˆã‚‹é«˜é€ŸåŒ–</li>
<li>ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã¨ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒ</li>
<li>åŠ¹ç‡çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢æˆ¦ç•¥</li>
</ul></li>
</ol>

<h3>XGBoostæ´»ç”¨ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>

<table>
<thead>
<tr>
<th>åŸå‰‡</th>
<th>èª¬æ˜</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å°ã•ã„learning_rate</strong></td>
<td>0.01-0.1ã§å®‰å®šã—ãŸå­¦ç¿’ã€n_estimatorsã‚’å¢—ã‚„ã™</td>
</tr>
<tr>
<td><strong>Early Stoppingæ´»ç”¨</strong></td>
<td>éå­¦ç¿’é˜²æ­¢ã¨è¨“ç·´æ™‚é–“çŸ­ç¸®</td>
</tr>
<tr>
<td><strong>subsampleå°å…¥</strong></td>
<td>0.7-0.9ã§ãƒ©ãƒ³ãƒ€ãƒ æ€§ã¨æ±åŒ–æ€§èƒ½å‘ä¸Š</td>
</tr>
<tr>
<td><strong>ç‰¹å¾´é‡é‡è¦åº¦ç¢ºèª</strong></td>
<td>ä¸è¦ãªç‰¹å¾´é‡å‰Šé™¤ã§ãƒ¢ãƒ‡ãƒ«ç°¡ç´ åŒ–</td>
</tr>
<tr>
<td><strong>CVè©•ä¾¡</strong></td>
<td>å …ç‰¢ãªæ€§èƒ½è©•ä¾¡ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é¸æŠ</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ç« ã¸</h3>

<p>ç¬¬3ç« ã§ã¯ã€<strong>LightGBM</strong>ã‚’å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>Leaf-wiseã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </li>
<li>ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ç›´æ¥å‡¦ç†</li>
<li>è¶…é«˜é€Ÿè¨“ç·´ã®å®Ÿç¾</li>
<li>XGBoostã¨ã®æ¯”è¼ƒ</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>
<p>XGBoostã®æ­£å‰‡åŒ–é … $\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T} w_j^2$ ã«ãŠã„ã¦ã€å„é …ã®æ„å‘³ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<ul>
<li><p><strong>$\gamma T$</strong>: è‘‰ã®æ•°ã«å¯¾ã™ã‚‹ãƒšãƒŠãƒ«ãƒ†ã‚£</p>
<ul>
<li>$\gamma$: è‘‰æ•°ã®ãƒšãƒŠãƒ«ãƒ†ã‚£ä¿‚æ•°</li>
<li>$T$: æœ¨ã®è‘‰ã®æ•°</li>
<li>åŠ¹æœ: è‘‰ãŒå¤šã„ã»ã©ãƒšãƒŠãƒ«ãƒ†ã‚£ãŒå¢—åŠ ã—ã€æœ¨ãŒè¤‡é›‘ã«ãªã‚Šã™ãã‚‹ã®ã‚’é˜²ã</li>
</ul></li>

<li><p><strong>$\frac{1}{2}\lambda \sum_{j=1}^{T} w_j^2$</strong>: è‘‰ã®é‡ã¿ã«å¯¾ã™ã‚‹L2æ­£å‰‡åŒ–</p>
<ul>
<li>$\lambda$: L2æ­£å‰‡åŒ–ä¿‚æ•°</li>
<li>$w_j$: å„è‘‰ã®é‡ã¿ï¼ˆäºˆæ¸¬å€¤ï¼‰</li>
<li>åŠ¹æœ: é‡ã¿ãŒå¤§ãããªã‚Šã™ãã‚‹ã®ã‚’é˜²ãã€æ»‘ã‚‰ã‹ãªäºˆæ¸¬ã‚’ä¿ƒé€²</li>
</ul></li>
</ul>

<p>ã“ã‚Œã‚‰ã®æ­£å‰‡åŒ–ã«ã‚ˆã‚Šã€XGBoostã¯éå­¦ç¿’ã‚’é˜²ãã€æ±åŒ–æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚</p>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦XGBoostãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€Early Stoppingã‚’ä½¿ç”¨ã—ã¦æœ€é©ãªæœ¨ã®æ•°ã‚’è¦‹ã¤ã‘ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split

data = load_diabetes()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import xgboost as xgb
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
data = load_diabetes()
X, y = data.data, data.target

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆè¨“ç·´ã€æ¤œè¨¼ã€ãƒ†ã‚¹ãƒˆï¼‰
X_train_full, X_test, y_train_full, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_full, y_train_full, test_size=0.2, random_state=42
)

# XGBoostãƒ¢ãƒ‡ãƒ«ï¼ˆå›å¸°ï¼‰
model = xgb.XGBRegressor(
    n_estimators=1000,
    max_depth=3,
    learning_rate=0.05,
    random_state=42
)

# Early Stoppingä»˜ãè¨“ç·´
model.fit(
    X_train, y_train,
    eval_set=[(X_train, y_train), (X_val, y_val)],
    early_stopping_rounds=50,
    verbose=100
)

# çµæœ
print("\n=== Early Stoppingçµæœ ===")
print(f"æœ€é©ãªæœ¨ã®æ•°: {model.best_iteration}")
print(f"è¨“ç·´æ™‚ã®RMSE: {model.best_score:.2f}")

# ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®è©•ä¾¡
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"\nãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆæ€§èƒ½:")
print(f"MSE: {mse:.2f}")
print(f"RMSE: {np.sqrt(mse):.2f}")
print(f"RÂ²: {r2:.3f}")

# å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–
import matplotlib.pyplot as plt

results = model.evals_result()
plt.figure(figsize=(10, 6))
plt.plot(results['validation_0']['rmse'], label='Train')
plt.plot(results['validation_1']['rmse'], label='Validation')
plt.axvline(x=model.best_iteration, color='r', linestyle='--',
            label=f'Best: {model.best_iteration}')
plt.xlabel('Number of Trees')
plt.ylabel('RMSE')
plt.title('Early Stopping - Learning Curve')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>learning_rateã¨n_estimatorsã®é–¢ä¿‚ã‚’èª¬æ˜ã—ã€æœ€é©ãªçµ„ã¿åˆã‚ã›ã‚’é¸ã¶æˆ¦ç•¥ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>learning_rateã¨n_estimatorsã®é–¢ä¿‚</strong>ï¼š</p>

<ul>
<li><strong>learning_rateï¼ˆå­¦ç¿’ç‡ï¼‰</strong>: å„æœ¨ã®å¯„ä¸ã‚’ç¸®å°ã™ã‚‹ä¿‚æ•°
<ul>
<li>å°ã•ã„å€¤ï¼ˆ0.01-0.05ï¼‰: å®‰å®šã—ãŸå­¦ç¿’ã€éå­¦ç¿’ã—ã«ãã„</li>
<li>å¤§ãã„å€¤ï¼ˆ0.2-0.3ï¼‰: é«˜é€Ÿãªå­¦ç¿’ã€éå­¦ç¿’ãƒªã‚¹ã‚¯</li>
</ul></li>

<li><strong>n_estimatorsï¼ˆæœ¨ã®æ•°ï¼‰</strong>: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã™ã‚‹æœ¨ã®ç·æ•°
<ul>
<li>å¤šã„ï¼ˆ500-1000ï¼‰: è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ã€è¨ˆç®—ã‚³ã‚¹ãƒˆå¤§</li>
<li>å°‘ãªã„ï¼ˆ50-100ï¼‰: é«˜é€Ÿã€å˜ç´”ãªãƒ¢ãƒ‡ãƒ«</li>
</ul></li>
</ul>

<p><strong>ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•</strong>ï¼š</p>
<ul>
<li>learning_rateã‚’å°ã•ãã™ã‚‹ã¨ã€åŒã˜æ€§èƒ½ã«åˆ°é”ã™ã‚‹ãŸã‚ã«n_estimatorsã‚’å¢—ã‚„ã™å¿…è¦ãŒã‚ã‚‹</li>
<li>ç›®å®‰: <code>learning_rate Ã— n_estimators â‰ˆ ä¸€å®š</code></li>
</ul>

<p><strong>æœ€é©åŒ–æˆ¦ç•¥</strong>ï¼š</p>

<ol>
<li><p><strong>åˆæœŸæ¢ç´¢</strong>ï¼ˆé€Ÿåº¦é‡è¦–ï¼‰</p>
<ul>
<li>learning_rate = 0.1, n_estimators = 100</li>
<li>ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½ã‚’ç¢ºèª</li>
</ul></li>

<li><p><strong>Early Stoppingæ´»ç”¨</strong></p>
<ul>
<li>learning_rate = 0.05, n_estimators = 1000ï¼ˆå¤§ãã‚ï¼‰</li>
<li>Early Stoppingã§æœ€é©ãªæœ¨ã®æ•°ã‚’è‡ªå‹•æ±ºå®š</li>
</ul></li>

<li><p><strong>ç²¾å¯†èª¿æ•´</strong>ï¼ˆç²¾åº¦é‡è¦–ï¼‰</p>
<ul>
<li>learning_rate = 0.01, n_estimators = 500-2000</li>
<li>æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã§æœ€é«˜ç²¾åº¦ã‚’è¿½æ±‚</li>
</ul></li>

<li><p><strong>æœ¬ç•ªç’°å¢ƒ</strong>ï¼ˆãƒãƒ©ãƒ³ã‚¹ï¼‰</p>
<ul>
<li>learning_rate = 0.03-0.05</li>
<li>Early Stoppingã§æ±ºå®šã•ã‚ŒãŸn_estimatorsä½¿ç”¨</li>
</ul></li>
</ol>

<p><strong>å®Ÿè£…ä¾‹</strong>ï¼š</p>
<pre><code class="language-python"># æˆ¦ç•¥1: é€Ÿåº¦é‡è¦–
model_fast = xgb.XGBClassifier(learning_rate=0.1, n_estimators=100)

# æˆ¦ç•¥2: Early Stoppingï¼ˆæ¨å¥¨ï¼‰
model_auto = xgb.XGBClassifier(learning_rate=0.05, n_estimators=1000)
model_auto.fit(X_train, y_train, eval_set=[(X_val, y_val)],
               early_stopping_rounds=50)

# æˆ¦ç•¥3: ç²¾åº¦é‡è¦–
model_accurate = xgb.XGBClassifier(learning_rate=0.01, n_estimators=1500)
</code></pre>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã‚’å®Ÿè¡Œã—ã€æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¦‹ã¤ã‘ã¦ãã ã•ã„ã€‚æ¢ç´¢ã™ã¹ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãã®ç¯„å›²ã‚‚ææ¡ˆã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">from sklearn.datasets import make_classification

X, y = make_classification(n_samples=5000, n_features=30, n_informative=20, random_state=42)
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import xgboost as xgb
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
X, y = make_classification(
    n_samples=5000,
    n_features=30,
    n_informative=20,
    random_state=42
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰ï¼ˆæ®µéšçš„æ¢ç´¢ï¼‰
# ãƒ•ã‚§ãƒ¼ã‚º1: ä¸»è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç²—æ¢ç´¢
param_grid_phase1 = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'n_estimators': [100, 200],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

model = xgb.XGBClassifier(random_state=42)

print("=== ãƒ•ã‚§ãƒ¼ã‚º1: ç²—æ¢ç´¢ ===")
grid_search_phase1 = GridSearchCV(
    model,
    param_grid_phase1,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search_phase1.fit(X_train, y_train)
print(f"\nãƒ•ã‚§ãƒ¼ã‚º1æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {grid_search_phase1.best_params_}")
print(f"ãƒ•ã‚§ãƒ¼ã‚º1æœ€è‰¯CVç²¾åº¦: {grid_search_phase1.best_score_:.4f}")

# ãƒ•ã‚§ãƒ¼ã‚º2: æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç²¾å¯†èª¿æ•´
best_params = grid_search_phase1.best_params_

param_grid_phase2 = {
    'max_depth': [best_params['max_depth']],
    'learning_rate': [best_params['learning_rate']],
    'n_estimators': [best_params['n_estimators']],
    'subsample': [best_params['subsample']],
    'colsample_bytree': [best_params['colsample_bytree']],
    'gamma': [0, 0.1, 0.5, 1],
    'reg_alpha': [0, 0.1, 0.5],
    'reg_lambda': [1, 1.5, 2]
}

print("\n=== ãƒ•ã‚§ãƒ¼ã‚º2: æ­£å‰‡åŒ–èª¿æ•´ ===")
grid_search_phase2 = GridSearchCV(
    model,
    param_grid_phase2,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search_phase2.fit(X_train, y_train)
print(f"\nãƒ•ã‚§ãƒ¼ã‚º2æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {grid_search_phase2.best_params_}")
print(f"ãƒ•ã‚§ãƒ¼ã‚º2æœ€è‰¯CVç²¾åº¦: {grid_search_phase2.best_score_:.4f}")

# ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®æœ€çµ‚è©•ä¾¡
final_model = grid_search_phase2.best_estimator_
y_pred = final_model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)

print("\n=== æœ€çµ‚ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ ===")
print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_accuracy:.4f}")
print(f"\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_test, y_pred))

# æ¢ç´¢çµæœã®åˆ†æ
cv_results = pd.DataFrame(grid_search_phase2.cv_results_)
top_10 = cv_results.nlargest(10, 'mean_test_score')[
    ['params', 'mean_test_score', 'std_test_score']
]

print("\n=== Top 10ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã› ===")
for i, row in top_10.iterrows():
    print(f"\n{i+1}. CVç²¾åº¦: {row['mean_test_score']:.4f} (Â±{row['std_test_score']:.4f})")
    print(f"   gamma: {row['params']['gamma']}")
    print(f"   reg_alpha: {row['params']['reg_alpha']}")
    print(f"   reg_lambda: {row['params']['reg_lambda']}")

# å¯è¦–åŒ–: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for ax, param in zip(axes, ['gamma', 'reg_alpha', 'reg_lambda']):
    param_values = cv_results['param_' + param].values
    scores = cv_results['mean_test_score'].values

    ax.scatter(param_values, scores, alpha=0.6, edgecolors='black')
    ax.set_xlabel(param, fontsize=12)
    ax.set_ylabel('CV Accuracy', fontsize=12)
    ax.set_title(f'Impact of {param}', fontsize=14)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>æ¨å¥¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²</strong>ï¼š</p>
<table>
<thead>
<tr>
<th>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</th>
<th>åˆæœŸæ¢ç´¢</th>
<th>ç²¾å¯†èª¿æ•´</th>
</tr>
</thead>
<tbody>
<tr>
<td>max_depth</td>
<td>3, 5, 7</td>
<td>Â±1ç¯„å›²</td>
</tr>
<tr>
<td>learning_rate</td>
<td>0.01, 0.1, 0.3</td>
<td>æœ€é©å€¤å‘¨è¾º</td>
</tr>
<tr>
<td>n_estimators</td>
<td>100, 200</td>
<td>Early Stoppingæ¨å¥¨</td>
</tr>
<tr>
<td>subsample</td>
<td>0.8, 1.0</td>
<td>0.7-1.0</td>
</tr>
<tr>
<td>colsample_bytree</td>
<td>0.8, 1.0</td>
<td>0.7-1.0</td>
</tr>
<tr>
<td>gamma</td>
<td>-</td>
<td>0, 0.1, 0.5, 1</td>
</tr>
<tr>
<td>reg_alpha</td>
<td>-</td>
<td>0, 0.1, 0.5</td>
</tr>
<tr>
<td>reg_lambda</td>
<td>-</td>
<td>1, 1.5, 2</td>
</tr>
</tbody>
</table>

</details>

<h3>å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>XGBoostã«ãŠã‘ã‚‹ç‰¹å¾´é‡é‡è¦åº¦ã®gain, weight, coverã®é•ã„ã‚’èª¬æ˜ã—ã€ãã‚Œãã‚Œã‚’ã©ã®ã‚ˆã†ãªå ´é¢ã§ä½¿ç”¨ã™ã¹ãã‹è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>3ã¤ã®é‡è¦åº¦æŒ‡æ¨™</strong>ï¼š</p>

<ol>
<li><p><strong>Gainï¼ˆåˆ©å¾—ï¼‰</strong></p>
<ul>
<li>å®šç¾©: ãã®ç‰¹å¾´é‡ã«ã‚ˆã‚‹æå¤±é–¢æ•°ã®å¹³å‡æ”¹å–„é‡</li>
<li>è¨ˆç®—: åˆ†å²æ™‚ã®æå¤±æ¸›å°‘ã®åˆè¨ˆã‚’ã€ãã®ç‰¹å¾´é‡ã®ä½¿ç”¨å›æ•°ã§å‰²ã£ãŸå€¤</li>
<li>æ„å‘³: äºˆæ¸¬ç²¾åº¦ã¸ã®ç›´æ¥çš„ãªå¯„ä¸åº¦</li>
<li>æ•°å¼: $\text{Gain}_f = \sum_{t \in \text{splits}(f)} \Delta L_t / |\text{splits}(f)|$</li>
</ul></li>

<li><p><strong>Weightï¼ˆé‡ã¿ï¼‰</strong></p>
<ul>
<li>å®šç¾©: ãã®ç‰¹å¾´é‡ãŒåˆ†å²ã«ä½¿ç”¨ã•ã‚ŒãŸå›æ•°</li>
<li>è¨ˆç®—: å…¨ã¦ã®æœ¨ã§ãã®ç‰¹å¾´é‡ãŒåˆ†å²ã«ä½¿ã‚ã‚ŒãŸç·å›æ•°</li>
<li>æ„å‘³: ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã«ãŠã‘ã‚‹ä½¿ç”¨é »åº¦</li>
<li>æ•°å¼: $\text{Weight}_f = |\text{splits}(f)|$</li>
</ul></li>

<li><p><strong>Coverï¼ˆã‚«ãƒãƒ¬ãƒƒã‚¸ï¼‰</strong></p>
<ul>
<li>å®šç¾©: åˆ†å²ã§ã‚«ãƒãƒ¼ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«æ•°ã®åˆè¨ˆ</li>
<li>è¨ˆç®—: ãã®ç‰¹å¾´é‡ã§åˆ†å²ã—ãŸéš›ã«å½±éŸ¿ã‚’å—ã‘ãŸã‚µãƒ³ãƒ—ãƒ«æ•°ã®åˆè¨ˆ</li>
<li>æ„å‘³: ç‰¹å¾´é‡ãŒå½±éŸ¿ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²</li>
<li>æ•°å¼: $\text{Cover}_f = \sum_{t \in \text{splits}(f)} n_t$ï¼ˆ$n_t$ã¯åˆ†å²ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼‰</li>
</ul></li>
</ol>

<p><strong>ä½¿ã„åˆ†ã‘ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³</strong>ï¼š</p>

<table>
<thead>
<tr>
<th>å ´é¢</th>
<th>æ¨å¥¨æŒ‡æ¨™</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td>ç‰¹å¾´é¸æŠ</td>
<td>Gain</td>
<td>ç²¾åº¦ã¸ã®å¯„ä¸ãŒç›´æ¥çš„ã«ã‚ã‹ã‚‹</td>
</tr>
<tr>
<td>ãƒ¢ãƒ‡ãƒ«è§£é‡ˆ</td>
<td>Gain</td>
<td>ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ã‚’èª¬æ˜ã—ã‚„ã™ã„</td>
</tr>
<tr>
<td>è¨ˆç®—åŠ¹ç‡åŒ–</td>
<td>Weight</td>
<td>é »ç¹ã«ä½¿ã†ç‰¹å¾´ã‚’å„ªå…ˆçš„ã«è¨ˆç®—</td>
</tr>
<tr>
<td>ãƒ‡ãƒ¼ã‚¿å½±éŸ¿ç¯„å›²</td>
<td>Cover</td>
<td>ã©ã‚Œã ã‘ã®ã‚µãƒ³ãƒ—ãƒ«ã«å½±éŸ¿ã™ã‚‹ã‹</td>
</tr>
<tr>
<td>ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿</td>
<td>Cover</td>
<td>å°‘æ•°ã‚¯ãƒ©ã‚¹ã¸ã®å½±éŸ¿ã‚’è©•ä¾¡</td>
</tr>
<tr>
<td>ä¸€èˆ¬çš„ãªç”¨é€”</td>
<td>Gain</td>
<td>æœ€ã‚‚è§£é‡ˆã—ã‚„ã™ãå®Ÿç”¨çš„</td>
</tr>
</tbody>
</table>

<p><strong>å®Ÿè£…ä¾‹</strong>ï¼š</p>
<pre><code class="language-python">import xgboost as xgb
from sklearn.datasets import load_breast_cancer
import matplotlib.pyplot as plt

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
data = load_breast_cancer()
X, y = data.data, data.target

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = xgb.XGBClassifier(n_estimators=100, max_depth=3, random_state=42)
model.fit(X, y)

# 3ã¤ã®é‡è¦åº¦ã‚’å–å¾—
importance_gain = model.get_booster().get_score(importance_type='gain')
importance_weight = model.get_booster().get_score(importance_type='weight')
importance_cover = model.get_booster().get_score(importance_type='cover')

# Top 5ã‚’æ¯”è¼ƒ
print("=== Top 5ç‰¹å¾´é‡ã®æ¯”è¼ƒ ===\n")

for imp_type, imp_dict in [('Gain', importance_gain),
                            ('Weight', importance_weight),
                            ('Cover', importance_cover)]:
    print(f"{imp_type}åŸºæº–:")
    sorted_features = sorted(imp_dict.items(), key=lambda x: x[1], reverse=True)
    for i, (feature, score) in enumerate(sorted_features[:5], 1):
        feature_idx = int(feature.replace('f', ''))
        print(f"  {i}. {data.feature_names[feature_idx]}: {score:.2f}")
    print()

# å¯è¦–åŒ–: 3ã¤ã®é‡è¦åº¦ã®ç›¸é–¢
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Gain vs Weight
common_features = set(importance_gain.keys()) & set(importance_weight.keys())
gain_vals = [importance_gain[f] for f in common_features]
weight_vals = [importance_weight[f] for f in common_features]

axes[0].scatter(weight_vals, gain_vals, alpha=0.6, edgecolors='black')
axes[0].set_xlabel('Weight')
axes[0].set_ylabel('Gain')
axes[0].set_title('Gain vs Weight')
axes[0].grid(True, alpha=0.3)

# Gain vs Cover
cover_vals = [importance_cover[f] for f in common_features]

axes[1].scatter(cover_vals, gain_vals, alpha=0.6, edgecolors='black', color='orange')
axes[1].set_xlabel('Cover')
axes[1].set_ylabel('Gain')
axes[1].set_title('Gain vs Cover')
axes[1].grid(True, alpha=0.3)

# Weight vs Cover
axes[2].scatter(weight_vals, cover_vals, alpha=0.6, edgecolors='black', color='green')
axes[2].set_xlabel('Weight')
axes[2].set_ylabel('Cover')
axes[2].set_title('Weight vs Cover')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>çµè«–</strong>ï¼š</p>
<ul>
<li>ä¸€èˆ¬çš„ã«ã¯<strong>Gain</strong>ã‚’ä½¿ç”¨ï¼ˆäºˆæ¸¬ç²¾åº¦ã¸ã®å¯„ä¸ãŒæ˜ç¢ºï¼‰</li>
<li>è¨ˆç®—åŠ¹ç‡ã‚„ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®åˆ†æã«ã¯<strong>Weight</strong>ã‚„<strong>Cover</strong>ã‚‚æœ‰ç”¨</li>
<li>è¤‡æ•°ã®æŒ‡æ¨™ã‚’çµ„ã¿åˆã‚ã›ã¦ç·åˆçš„ã«åˆ¤æ–­ã™ã‚‹ã“ã¨ãŒé‡è¦</li>
</ul>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>.</li>
<li>Chen, T., He, T., Benesty, M., et al. (2023). <em>XGBoost Documentation</em>. https://xgboost.readthedocs.io/</li>
<li>Hastie, T., Tibshirani, R., & Friedman, J. (2009). <em>The Elements of Statistical Learning</em> (2nd ed.). Springer.</li>
<li>Lundberg, S. M., & Lee, S. I. (2017). A Unified Approach to Interpreting Model Predictions. <em>Advances in Neural Information Processing Systems</em>.</li>
</ol>

<div class="navigation">
    <a href="chapter1-gradient-boosting.html" class="nav-button">â† å‰ã®ç« : å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°åŸºç¤</a>
    <a href="chapter3-lightgbm.html" class="nav-button">æ¬¡ã®ç« : LightGBM â†’</a>
</div>

    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimotoï¼ˆæ±åŒ—å¤§å­¦ï¼‰</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
