<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="強化学習入門シリーズ - Q学習からDQN・PPOまでの実装完全ガイド">
    <title>強化学習入門シリーズ v1.0 - AI Terakoya</title>

    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header .container {
            padding: 0 1.5rem;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }

        .meta span {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
        }

        /* Typography */
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            color: var(--primary-color);
        }

        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.6rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1.2rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--link-hover);
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1.2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Code blocks */
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border: 1px solid var(--border-color);
        }

        pre code {
            background: none;
            padding: 0;
            font-size: 0.9rem;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
            overflow-x: auto;
            display: block;
        }

        thead {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        tbody {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        th, td {
            padding: 0.8rem;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        th {
            background: var(--primary-color);
            color: white;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background: #f9f9f9;
        }

        /* Blockquotes */
        blockquote {
            border-left: 4px solid var(--secondary-color);
            padding-left: 1.5rem;
            margin: 1.5rem 0;
            font-style: italic;
            color: #666;
        }

        /* Images */
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1rem 0;
        }

        /* Mermaid diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
            background: white;
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        /* Details/Summary (for exercises) */
        details {
            margin: 1rem 0;
            padding: 1rem;
            background: #f8f9fa;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--primary-color);
            padding: 0.5rem;
        }

        summary:hover {
            color: var(--secondary-color);
        }

        /* Footer */
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }

        /* Navigation buttons */
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }

        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            h1 {
                font-size: 1.6rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.85rem;
            }

            table {
                font-size: 0.9rem;
            }
        }
    </style>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        // Mermaid.js Converter - Converts markdown-style mermaid code blocks to renderable divs
        document.addEventListener('DOMContentLoaded', function() {
            // Find all code blocks with class="language-mermaid"
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                // Create a new div with mermaid class
                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                // Replace the pre element with the new div
                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            // Re-initialize mermaid after conversion
            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({ startOnLoad: true, theme: 'default' });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</head>
<body>
    <header>
        <div class="container">
            <h1>🎮 強化学習入門シリーズ v1.0</h1>
            <p style="font-size: 1.1rem; margin-top: 0.5rem; opacity: 0.95;">Q学習からDQN・PPOまでの実装ガイド</p>
            <div class="meta">
                <span>📖 総学習時間: 120-150分</span>
                <span>📊 レベル: 上級</span>
            </div>
        </div>
    </header>

    <main class="container">
        <p><strong>試行錯誤を通じて最適な行動を学習する強化学習アルゴリズムを基礎から体系的にマスター</strong></p>

        <h2 id="overview">シリーズ概要</h2>
        <p>このシリーズは、強化学習（Reinforcement Learning, RL）の理論と実装を基礎から段階的に学べる全5章構成の実践的教育コンテンツです。</p>

        <p><strong>強化学習（Reinforcement Learning）</strong>は、エージェントが環境との相互作用を通じて試行錯誤しながら最適な行動方策を学習する機械学習の一分野です。Markov Decision Process（MDP）による問題の定式化、Bellman方程式による価値関数の計算、Q学習やSARSAといった古典的手法、Deep Q-Network（DQN）によるAtariゲームの攻略、Policy Gradient法による連続行動空間への対応、Proximal Policy Optimization（PPO）やSoft Actor-Critic（SAC）といった最新アルゴリズム、これらの技術はロボット制御・ゲームAI・自動運転・金融取引・リソース最適化など、幅広い分野で革新をもたらしています。DeepMind、OpenAI、Googleといった企業が実用化している意思決定の基盤技術を理解し、実装できるようになります。Tabular methodsからDeep RLまで、体系的な知識を提供します。</p>

        <p><strong>特徴:</strong></p>
        <ul>
            <li>✅ <strong>理論から実装まで</strong>: MDPの基礎から最新のPPO・SACまで体系的に学習</li>
            <li>✅ <strong>実装重視</strong>: 35個以上の実行可能なPyTorch/Gymnasium/Stable-Baselines3コード例</li>
            <li>✅ <strong>直感的理解</strong>: Cliff Walking、CartPole、Atariでの動作可視化で原理を理解</li>
            <li>✅ <strong>最新技術準拠</strong>: Gymnasium（OpenAI Gym後継）、Stable-Baselines3を使った実装</li>
            <li>✅ <strong>実用的応用</strong>: ゲームAI・ロボット制御・リソース最適化など実践的なタスクへの適用</li>
        </ul>

        <p><strong>総学習時間</strong>: 120-150分（コード実行と演習を含む）</p>

        <h2 id="learning">学習の進め方</h2>

        <h3>推奨学習順序</h3>

        <div class="mermaid">
graph TD
    A[第1章: 強化学習の基礎] --> B[第2章: Q学習とSARSA]
    B --> C[第3章: Deep Q-Network]
    C --> D[第4章: Policy Gradient法]
    D --> E[第5章: 高度なRL手法]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
</div>

        <p><strong>初学者の方（強化学習をまったく知らない）:</strong><br>
        - 第1章 → 第2章 → 第3章 → 第4章 → 第5章（全章推奨）<br>
        - 所要時間: 120-150分</p>

        <p><strong>中級者の方（MDPの経験あり）:</strong><br>
        - 第2章 → 第3章 → 第4章 → 第5章<br>
        - 所要時間: 90-110分</p>

        <p><strong>特定トピックの強化:</strong><br>
        - MDP・Bellman方程式: 第1章（集中学習）<br>
        - Tabular methods: 第2章（集中学習）<br>
        - Deep Q-Network: 第3章（集中学習）<br>
        - Policy Gradient: 第4章（集中学習）<br>
        - 所要時間: 25-30分/章</p>

        <h2 id="chapters">各章の詳細</h2>

        <h3><a href="./chapter1-rl-basics.html">第1章：強化学習の基礎</a></h3>
        <p><strong>難易度</strong>: 上級<br>
        <strong>読了時間</strong>: 25-30分<br>
        <strong>コード例</strong>: 7個</p>

        <h4>学習内容</h4>
        <ol>
            <li><strong>強化学習の基本概念</strong> - エージェント、環境、状態、行動、報酬</li>
            <li><strong>Markov Decision Process（MDP）</strong> - 状態遷移確率、報酬関数、割引率</li>
            <li><strong>Bellman方程式</strong> - 状態価値関数、行動価値関数、最適性</li>
            <li><strong>方策（Policy）</strong> - 決定論的方策、確率的方策、最適方策</li>
            <li><strong>Gymnasium入門</strong> - 環境の作成、状態・行動空間、ステップ実行</li>
        </ol>

        <h4>学習目標</h4>
        <ul>
            <li>✅ 強化学習の基本用語を理解する</li>
            <li>✅ MDPとして問題を定式化できる</li>
            <li>✅ Bellman方程式を説明できる</li>
            <li>✅ 価値関数と方策の関係を理解する</li>
            <li>✅ Gymnasiumで環境を操作できる</li>
        </ul>

        <p><strong><a href="./chapter1-rl-basics.html">第1章を読む →</a></strong></p>

        <hr>

        <h3><a href="./chapter2-q-learning-sarsa.html">第2章：Q学習とSARSA</a></h3>
        <p><strong>難易度</strong>: 上級<br>
        <strong>読了時間</strong>: 25-30分<br>
        <strong>コード例</strong>: 8個</p>

        <h4>学習内容</h4>
        <ol>
            <li><strong>Tabular methods</strong> - Q-table、状態-行動価値の表形式表現</li>
            <li><strong>Q学習（Q-Learning）</strong> - Off-policy TD制御、Q値の更新則</li>
            <li><strong>SARSA</strong> - On-policy TD制御、Q学習との違い</li>
            <li><strong>探索と活用のトレードオフ</strong> - ε-greedy、ε-decay、Boltzmann探索</li>
            <li><strong>Cliff Walking問題</strong> - グリッドワールドでのQ学習/SARSA実装</li>
        </ol>

        <h4>学習目標</h4>
        <ul>
            <li>✅ Q学習のアルゴリズムを理解する</li>
            <li>✅ SARSAとQ学習の違いを説明できる</li>
            <li>✅ ε-greedyによる探索戦略を実装できる</li>
            <li>✅ Q-tableを使った学習を実装できる</li>
            <li>✅ Cliff Walkingで両手法を比較できる</li>
        </ul>

        <p><strong><a href="./chapter2-q-learning-sarsa.html">第2章を読む →</a></strong></p>

        <hr>

        <h3><a href="./chapter3-dqn.html">第3章：Deep Q-Network（DQN）</a></h3>
        <p><strong>難易度</strong>: 上級<br>
        <strong>読了時間</strong>: 30-35分<br>
        <strong>コード例</strong>: 8個</p>

        <h4>学習内容</h4>
        <ol>
            <li><strong>関数近似</strong> - Q-tableの限界、ニューラルネットワークによる近似</li>
            <li><strong>DQNの仕組み</strong> - Q-networkの学習、損失関数、勾配降下法</li>
            <li><strong>Experience Replay</strong> - 経験の再利用、相関の低減、安定化</li>
            <li><strong>Target Network</strong> - 固定ターゲット、学習の安定性向上</li>
            <li><strong>Atariゲームへの応用</strong> - 画像入力、CNN、Pong/Breakout</li>
        </ol>

        <h4>学習目標</h4>
        <ul>
            <li>✅ DQNの構成要素を理解する</li>
            <li>✅ Experience Replayの役割を説明できる</li>
            <li>✅ Target Networkの必要性を理解する</li>
            <li>✅ PyTorchでDQNを実装できる</li>
            <li>✅ CartPole/Atariでエージェントを訓練できる</li>
        </ul>

        <p><strong><a href="./chapter3-dqn.html">第3章を読む →</a></strong></p>

        <hr>

        <h3><a href="./chapter4-policy-gradient.html">第4章：Policy Gradient法</a></h3>
        <p><strong>難易度</strong>: 上級<br>
        <strong>読了時間</strong>: 30-35分<br>
        <strong>コード例</strong>: 7個</p>

        <h4>学習内容</h4>
        <ol>
            <li><strong>REINFORCE</strong> - 方策勾配定理、モンテカルロ方策勾配</li>
            <li><strong>Actor-Critic</strong> - 俳優と批評家、バイアスと分散のトレードオフ</li>
            <li><strong>Advantage Actor-Critic（A2C）</strong> - Advantage関数、分散低減</li>
            <li><strong>Proximal Policy Optimization（PPO）</strong> - クリップ目的関数、安定した学習</li>
            <li><strong>連続行動空間</strong> - ガウス方策、ロボット制御への応用</li>
        </ol>

        <h4>学習目標</h4>
        <ul>
            <li>✅ 方策勾配定理を理解する</li>
            <li>✅ REINFORCEアルゴリズムを実装できる</li>
            <li>✅ Actor-Criticの仕組みを説明できる</li>
            <li>✅ PPOの目的関数を理解する</li>
            <li>✅ 連続行動空間に対応したエージェントを作成できる</li>
        </ul>

        <p><strong><a href="./chapter4-policy-gradient.html">第4章を読む →</a></strong></p>

        <hr>

        <h3><a href="./chapter5-advanced-rl.html">第5章：高度なRL手法</a></h3>
        <p><strong>難易度</strong>: 上級<br>
        <strong>読了時間</strong>: 25-30分<br>
        <strong>コード例</strong>: 5個</p>

        <h4>学習内容</h4>
        <ol>
            <li><strong>Asynchronous Advantage Actor-Critic（A3C）</strong> - 並列学習、スレッド間の同期</li>
            <li><strong>Soft Actor-Critic（SAC）</strong> - エントロピー正則化、最大エントロピーRL</li>
            <li><strong>Multi-agent RL</strong> - 複数エージェント、協調と競争</li>
            <li><strong>実世界への応用</strong> - ロボット制御、リソース最適化、自動運転</li>
            <li><strong>Stable-Baselines3</strong> - 実装済みアルゴリズムの活用、ハイパーパラメータ調整</li>
        </ol>

        <h4>学習目標</h4>
        <ul>
            <li>✅ A3Cの並列学習を理解する</li>
            <li>✅ SACのエントロピー正則化を説明できる</li>
            <li>✅ Multi-agent RLの課題を理解する</li>
            <li>✅ Stable-Baselines3でアルゴリズムを活用できる</li>
            <li>✅ 実世界の問題にRLを適用できる</li>
        </ul>

        <p><strong><a href="./chapter5-advanced-rl.html">第5章を読む →</a></strong></p>

        <hr>

        <h2 id="outcomes">全体の学習成果</h2>

        <p>このシリーズを完了すると、以下のスキルと知識を習得できます：</p>

        <h3>知識レベル（Understanding）</h3>
        <ul>
            <li>✅ MDPとBellman方程式の理論的基礎を説明できる</li>
            <li>✅ Q学習・SARSA・DQN・PPO・SACの仕組みを理解している</li>
            <li>✅ Value-based法とPolicy-based法の違いを説明できる</li>
            <li>✅ Experience ReplayとTarget Networkの役割を理解している</li>
            <li>✅ 各アルゴリズムの使い分けを説明できる</li>
        </ul>

        <h3>実践スキル（Doing）</h3>
        <ul>
            <li>✅ PyTorch/Gymnasiumで強化学習エージェントを実装できる</li>
            <li>✅ Q学習・DQN・PPOをスクラッチで実装できる</li>
            <li>✅ Stable-Baselines3で高度なアルゴリズムを活用できる</li>
            <li>✅ 探索戦略（ε-greedy、ε-decay）を実装できる</li>
            <li>✅ CartPole・Atariゲームでエージェントを訓練できる</li>
        </ul>

        <h3>応用力（Applying）</h3>
        <ul>
            <li>✅ タスクに応じて適切なRLアルゴリズムを選択できる</li>
            <li>✅ 連続・離散行動空間に対応したエージェントを設計できる</li>
            <li>✅ ハイパーパラメータを適切に調整できる</li>
            <li>✅ ロボット制御・ゲームAIに強化学習を応用できる</li>
        </ul>

        <hr>

        <h2 id="prerequisites">前提知識</h2>

        <p>このシリーズを効果的に学習するために、以下の知識があることが望ましいです：</p>

        <h3>必須（Must Have）</h3>
        <ul>
            <li>✅ <strong>Python基礎</strong>: 変数、関数、クラス、ループ、条件分岐</li>
            <li>✅ <strong>NumPy基礎</strong>: 配列操作、行列演算、乱数生成</li>
            <li>✅ <strong>深層学習の基礎</strong>: ニューラルネットワーク、誤差逆伝播、勾配降下法</li>
            <li>✅ <strong>PyTorch基礎</strong>: テンソル操作、nn.Module、オプティマイザ</li>
            <li>✅ <strong>確率・統計の基礎</strong>: 期待値、分散、確率分布</li>
            <li>✅ <strong>微分の基礎</strong>: 勾配、偏微分、連鎖律</li>
        </ul>

        <h3>推奨（Nice to Have）</h3>
        <ul>
            <li>💡 <strong>動的計画法</strong>: Value Iteration、Policy Iteration（理論理解のため）</li>
            <li>💡 <strong>CNN基礎</strong>: 畳み込み層、プーリング（Atari学習のため）</li>
            <li>💡 <strong>最適化アルゴリズム</strong>: Adam、RMSprop、学習率スケジューリング</li>
            <li>💡 <strong>線形代数</strong>: ベクトル、行列演算</li>
            <li>💡 <strong>GPU環境</strong>: CUDAの基本的な理解</li>
        </ul>

        <p><strong>推奨される前の学習</strong>:</p>
        <ul>
            <li>📚 <a href="../deep-learning-basics/">深層学習の基礎シリーズ</a> - ニューラルネットワークの基本</li>
            <li>📚 <a href="../pytorch-introduction/">PyTorch入門シリーズ</a> - PyTorchの基本操作</li>
            <li>📚 <a href="../probability-statistics-ml/">機械学習のための確率・統計</a> - 確率分布、期待値</li>
            <li>📚 <a href="../optimization-algorithms/">最適化アルゴリズム入門</a> - 勾配降下法、Adam（推奨）</li>
        </ul>

        <hr>

        <h2 id="tech">使用技術とツール</h2>

        <h3>主要ライブラリ</h3>
        <ul>
            <li><strong>PyTorch 2.0+</strong> - 深層学習フレームワーク</li>
            <li><strong>Gymnasium 0.29+</strong> - 強化学習環境（OpenAI Gym後継）</li>
            <li><strong>Stable-Baselines3 2.1+</strong> - 実装済みRLアルゴリズムライブラリ</li>
            <li><strong>NumPy 1.24+</strong> - 数値計算</li>
            <li><strong>Matplotlib 3.7+</strong> - 可視化</li>
            <li><strong>TensorBoard 2.14+</strong> - 学習過程の可視化</li>
            <li><strong>imageio 2.31+</strong> - ビデオ保存、GIF作成</li>
        </ul>

        <h3>開発環境</h3>
        <ul>
            <li><strong>Python 3.8+</strong> - プログラミング言語</li>
            <li><strong>Jupyter Notebook / Lab</strong> - 対話的開発環境</li>
            <li><strong>Google Colab</strong> - GPU環境（無料で利用可能）</li>
            <li><strong>CUDA 11.8+ / cuDNN</strong> - GPU高速化（推奨）</li>
        </ul>

        <h3>環境（Environments）</h3>
        <ul>
            <li><strong>FrozenLake</strong> - グリッドワールド（Tabular methods）</li>
            <li><strong>Cliff Walking</strong> - グリッドワールド（Q学習 vs SARSA）</li>
            <li><strong>CartPole-v1</strong> - 倒立振子（古典制御問題）</li>
            <li><strong>LunarLander-v2</strong> - 月面着陸（連続制御）</li>
            <li><strong>Atari: Pong, Breakout</strong> - ゲームAI（画像入力、DQN）</li>
            <li><strong>MuJoCo: Humanoid, Ant</strong> - ロボット制御（連続行動空間）</li>
        </ul>

        <hr>

        <h2 id="start">さあ、始めましょう！</h2>
        <p>準備はできましたか？ 第1章から始めて、強化学習の技術を習得しましょう！</p>

        <p><strong><a href="./chapter1-rl-basics.html">第1章: 強化学習の基礎 →</a></strong></p>

        <hr>

        <h2 id="next">次のステップ</h2>

        <p>このシリーズを完了した後、以下のトピックへ進むことをお勧めします：</p>

        <h3>深掘り学習</h3>
        <ul>
            <li>📚 <strong>Model-Based RL</strong>: 環境モデルの学習、計画ベースの手法</li>
            <li>📚 <strong>Meta-RL</strong>: 学習の学習、Few-shot RL</li>
            <li>📚 <strong>Offline RL</strong>: バッチデータからの学習、Behavioral Cloning</li>
            <li>📚 <strong>Hierarchical RL</strong>: オプション、階層的方策</li>
        </ul>

        <h3>関連シリーズ</h3>
        <ul>
            <li>🎯 <a href="../imitation-learning/">模倣学習入門</a> - Behavioral Cloning、Inverse RL</li>
            <li>🎯 <a href="../robot-control-rl/">ロボット制御とRL</a> - MuJoCo、実機制御</li>
            <li>🎯 <a href="../game-ai/">ゲームAI開発</a> - AlphaGo、モンテカルロ木探索</li>
        </ul>

        <h3>実践プロジェクト</h3>
        <ul>
            <li>🚀 AtariゲームマスターAI - DQN/PPOによるPong・Breakout攻略</li>
            <li>🚀 倒立振子制御 - CartPoleの安定化とロボット応用</li>
            <li>🚀 自律ドローン制御 - 連続行動空間での飛行制御</li>
            <li>🚀 トレーディングボット - 金融市場での意思決定最適化</li>
        </ul>

        <hr>

        <p><strong>更新履歴</strong></p>
        <ul>
            <li><strong>2025-10-21</strong>: v1.0 初版公開</li>
        </ul>

        <hr>

        <p><strong>あなたの強化学習の旅はここから始まります！</strong></p>

    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
