<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬5ç« ï¼šç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å¿œç”¨ (Applications of Generative Models) - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/AI-Knowledge-Notes/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/AI-Knowledge-Notes/knowledge/jp/ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="/AI-Knowledge-Notes/knowledge/jp/ML/generative-models-introduction/index.html">Generative Models</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 5</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬5ç« ï¼šç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å¿œç”¨ (Applications of Generative Models)</h1>
            <p class="subtitle">Text-to-Imageç”Ÿæˆã‹ã‚‰ã‚¢ãƒã‚¿ãƒ¼ä½œæˆã‚·ã‚¹ãƒ†ãƒ ã¾ã§ã®å®Ÿè·µçš„å¿œç”¨</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 7å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… Stable Diffusionã‚’ä½¿ã£ãŸText-to-Imageç”Ÿæˆã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®æŠ€è¡“ã‚’ç†è§£ã—ã€åŠ¹æœçš„ãªç”»åƒã‚’ç”Ÿæˆã§ãã‚‹</li>
<li>âœ… Image-to-Imageå¤‰æ›ï¼ˆStyle transferã€Super-resolutionï¼‰ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Conditional GANï¼ˆcGANï¼‰ã«ã‚ˆã‚‹æ¡ä»¶ä»˜ãç”Ÿæˆã®ä»•çµ„ã¿ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… Audioç”ŸæˆæŠ€è¡“ï¼ˆWaveGANï¼‰ã®åŸºç¤ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… å®Ÿç”¨çš„ãªã‚¢ãƒã‚¿ãƒ¼ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
<li>âœ… ç”ŸæˆAIã®å€«ç†çš„èª²é¡Œã¨è²¬ä»»ã‚ã‚‹åˆ©ç”¨æ–¹æ³•ã‚’ç†è§£ã§ãã‚‹</li>
</ul>

<hr>

<h2>5.1 Text-to-Imageç”Ÿæˆ</h2>

<h3>Stable Diffusionã®æ¦‚è¦</h3>

<p><strong>Stable Diffusion</strong>ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰é«˜å“è³ªãªç”»åƒã‚’ç”Ÿæˆã™ã‚‹æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚2022å¹´ã«Stability AIã«ã‚ˆã£ã¦å…¬é–‹ã•ã‚Œã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§åˆ©ç”¨å¯èƒ½ãªæœ€ã‚‚å¼·åŠ›ãªText-to-Imageç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ä¸€ã¤ã§ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ] --> B[CLIP Text Encoder]
    B --> C[Text Embedding<br/>77Ã—768]
    C --> D[U-Net Denoiser]
    E[ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚º<br/>Latent Space] --> D
    D --> F[Denoising Steps<br/>20-50å›]
    F --> G[VAE Decoder]
    G --> H[ç”Ÿæˆç”»åƒ<br/>512Ã—512 or 1024Ã—1024]

    style A fill:#e3f2fd
    style H fill:#c8e6c9
    style D fill:#fff9c4
</div>

<h4>Stable Diffusionã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ§‹æˆè¦ç´ </h4>

<table>
<thead>
<tr>
<th>ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</th>
<th>å½¹å‰²</th>
<th>æŠ€è¡“è©³ç´°</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Text Encoder</strong></td>
<td>ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›</td>
<td>CLIP ViT-L/14ï¼ˆOpenAIï¼‰</td>
</tr>
<tr>
<td><strong>VAE Encoder/Decoder</strong></td>
<td>ç”»åƒã¨Latentç©ºé–“ã®å¤‰æ›</td>
<td>åœ§ç¸®ç‡8Ã—ã€512Ã—512â†’64Ã—64</td>
</tr>
<tr>
<td><strong>U-Net Denoiser</strong></td>
<td>ãƒã‚¤ã‚ºé™¤å»ã¨ç”»åƒç”Ÿæˆ</td>
<td>Cross-attentionæ©Ÿæ§‹ã§ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ã‘</td>
</tr>
<tr>
<td><strong>Scheduler</strong></td>
<td>ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç®¡ç†</td>
<td>DDPM, DDIM, Euler, DPM-Solver++</td>
</tr>
</tbody>
</table>

<h3>Stable Diffusionã®å®Ÿè£…</h3>

<pre><code class="language-python">import torch
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
from PIL import Image
import matplotlib.pyplot as plt

class StableDiffusionGenerator:
    """
    Stable Diffusionã‚’ä½¿ã£ãŸText-to-Imageç”Ÿæˆã‚¯ãƒ©ã‚¹

    Features:
    - è¤‡æ•°ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚µãƒãƒ¼ãƒˆï¼ˆDDPM, DDIM, Euler, DPM-Solver++ï¼‰
    - Negative promptã‚µãƒãƒ¼ãƒˆ
    - CFGï¼ˆClassifier-Free Guidanceï¼‰åˆ¶å¾¡
    - ã‚·ãƒ¼ãƒ‰å›ºå®šã«ã‚ˆã‚‹å†ç¾æ€§
    """

    def __init__(self, model_id="stabilityai/stable-diffusion-2-1", device="cuda"):
        """
        Args:
            model_id: HuggingFaceã®ãƒ¢ãƒ‡ãƒ«ID
            device: ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹ï¼ˆcuda or cpuï¼‰
        """
        self.device = device if torch.cuda.is_available() else "cpu"

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–
        self.pipe = StableDiffusionPipeline.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            safety_checker=None  # å®Ÿé‹ç”¨ã§ã¯é©åˆ‡ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè£…
        )

        # ã‚ˆã‚Šé«˜é€ŸãªDPM-Solver++ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’ä½¿ç”¨
        self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(
            self.pipe.scheduler.config
        )

        self.pipe = self.pipe.to(self.device)

        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼ˆGPUä½¿ç”¨æ™‚ï¼‰
        if self.device == "cuda":
            self.pipe.enable_attention_slicing()
            self.pipe.enable_vae_slicing()

    def generate(
        self,
        prompt,
        negative_prompt="",
        num_inference_steps=25,
        guidance_scale=7.5,
        width=512,
        height=512,
        seed=None,
        num_images=1
    ):
        """
        ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆ

        Args:
            prompt: ç”Ÿæˆã—ãŸã„ç”»åƒã®èª¬æ˜æ–‡
            negative_prompt: é¿ã‘ãŸã„è¦ç´ ã®èª¬æ˜æ–‡
            num_inference_steps: ãƒã‚¤ã‚ºé™¤å»ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆ20-50æ¨å¥¨ï¼‰
            guidance_scale: CFGã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆ7-15æ¨å¥¨ã€é«˜ã„ã»ã©ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¿ å®Ÿï¼‰
            width, height: ç”Ÿæˆç”»åƒã®ã‚µã‚¤ã‚ºï¼ˆ8ã®å€æ•°ï¼‰
            seed: å†ç¾æ€§ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰å€¤
            num_images: ç”Ÿæˆã™ã‚‹ç”»åƒã®æšæ•°

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸç”»åƒã®ãƒªã‚¹ãƒˆ
        """
        # ã‚·ãƒ¼ãƒ‰è¨­å®š
        generator = None
        if seed is not None:
            generator = torch.Generator(device=self.device).manual_seed(seed)

        # ç”»åƒç”Ÿæˆ
        with torch.autocast(self.device):
            output = self.pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                width=width,
                height=height,
                generator=generator,
                num_images_per_prompt=num_images
            )

        return output.images

    def generate_grid(self, prompts, **kwargs):
        """
        è¤‡æ•°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ç”»åƒã‚°ãƒªãƒƒãƒ‰ã‚’ç”Ÿæˆ

        Args:
            prompts: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒªã‚¹ãƒˆ
            **kwargs: generateãƒ¡ã‚½ãƒƒãƒ‰ã¸ã®è¿½åŠ å¼•æ•°

        Returns:
            ã‚°ãƒªãƒƒãƒ‰ç”»åƒ
        """
        images = []
        for prompt in prompts:
            img = self.generate(prompt, num_images=1, **kwargs)[0]
            images.append(img)

        # ã‚°ãƒªãƒƒãƒ‰ä½œæˆ
        n = len(images)
        cols = int(n ** 0.5)
        rows = (n + cols - 1) // cols

        w, h = images[0].size
        grid = Image.new('RGB', (w * cols, h * rows))

        for idx, img in enumerate(images):
            grid.paste(img, ((idx % cols) * w, (idx // cols) * h))

        return grid

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã®åˆæœŸåŒ–
    sd = StableDiffusionGenerator()

    # åŸºæœ¬çš„ãªç”Ÿæˆ
    prompt = "A beautiful sunset over mountains, oil painting style, highly detailed"
    negative_prompt = "blurry, low quality, distorted"

    images = sd.generate(
        prompt=prompt,
        negative_prompt=negative_prompt,
        num_inference_steps=30,
        guidance_scale=7.5,
        seed=42,
        num_images=2
    )

    # ç”»åƒè¡¨ç¤º
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))
    for idx, img in enumerate(images):
        axes[idx].imshow(img)
        axes[idx].axis('off')
        axes[idx].set_title(f'Image {idx + 1}')
    plt.tight_layout()
    plt.show()

    # è¤‡æ•°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã‚°ãƒªãƒƒãƒ‰ç”Ÿæˆ
    prompts = [
        "A cat astronaut in space, digital art",
        "A futuristic city at night, cyberpunk style",
        "A magical forest with glowing mushrooms",
        "A steampunk robot playing violin"
    ]

    grid = sd.generate_grid(
        prompts,
        negative_prompt="ugly, blurry, low quality",
        num_inference_steps=25,
        guidance_scale=7.5,
        seed=42
    )

    plt.figure(figsize=(10, 10))
    plt.imshow(grid)
    plt.axis('off')
    plt.title('Generated Image Grid')
    plt.show()
</code></pre>

<h3>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æŠ€è¡“</h3>

<p><strong>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</strong>ã¯ã€æœ›ã‚€ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æœ€é©åŒ–ã™ã‚‹æŠ€è¡“ã§ã™ã€‚åŠ¹æœçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹æˆè¦ç´ ã‚’ç†è§£ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚</p>

<h4>åŠ¹æœçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹é€ </h4>

<p>$$
\text{Prompt} = \text{Subject} + \text{Style} + \text{Quality} + \text{Details} + \text{Modifiers}
$$</p>

<table>
<thead>
<tr>
<th>è¦ç´ </th>
<th>èª¬æ˜</th>
<th>ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Subject</strong></td>
<td>ãƒ¡ã‚¤ãƒ³ã®è¢«å†™ä½“</td>
<td>"a majestic lion", "a futuristic building"</td>
</tr>
<tr>
<td><strong>Style</strong></td>
<td>èŠ¸è¡“ã‚¹ã‚¿ã‚¤ãƒ«</td>
<td>"oil painting", "anime style", "photorealistic"</td>
</tr>
<tr>
<td><strong>Quality</strong></td>
<td>å“è³ªä¿®é£¾å­</td>
<td>"highly detailed", "8k resolution", "masterpiece"</td>
</tr>
<tr>
<td><strong>Details</strong></td>
<td>å…·ä½“çš„ãªè©³ç´°</td>
<td>"golden hour lighting", "dramatic shadows"</td>
</tr>
<tr>
<td><strong>Modifiers</strong></td>
<td>è¿½åŠ ã®èª¿æ•´</td>
<td>"trending on artstation", "by Greg Rutkowski"</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">class PromptEngineer:
    """
    åŠ¹æœçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹
    """

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    STYLE_KEYWORDS = {
        'photorealistic': 'photorealistic, photo, realistic, high quality photograph',
        'digital_art': 'digital art, digital painting, artstation',
        'oil_painting': 'oil painting, traditional art, canvas',
        'anime': 'anime style, manga, japanese animation',
        'cyberpunk': 'cyberpunk style, neon lights, futuristic',
        '3d_render': '3d render, octane render, unreal engine, blender'
    }

    QUALITY_KEYWORDS = [
        'highly detailed',
        '8k resolution',
        'masterpiece',
        'best quality',
        'sharp focus',
        'professional'
    ]

    NEGATIVE_KEYWORDS = [
        'blurry',
        'low quality',
        'bad anatomy',
        'distorted',
        'ugly',
        'duplicate',
        'watermark'
    ]

    @staticmethod
    def build_prompt(
        subject,
        style='photorealistic',
        quality_level='high',
        additional_details=None,
        artist=None
    ):
        """
        æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰

        Args:
            subject: ãƒ¡ã‚¤ãƒ³ã®è¢«å†™ä½“
            style: ã‚¹ã‚¿ã‚¤ãƒ«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            quality_level: å“è³ªãƒ¬ãƒ™ãƒ«ï¼ˆ'high', 'medium', 'low'ï¼‰
            additional_details: è¿½åŠ ã®è©³ç´°ï¼ˆãƒªã‚¹ãƒˆã¾ãŸã¯æ–‡å­—åˆ—ï¼‰
            artist: ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆåï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            æ§‹ç¯‰ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        components = [subject]

        # ã‚¹ã‚¿ã‚¤ãƒ«è¿½åŠ 
        if style in PromptEngineer.STYLE_KEYWORDS:
            components.append(PromptEngineer.STYLE_KEYWORDS[style])
        else:
            components.append(style)

        # å“è³ªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¿½åŠ 
        if quality_level == 'high':
            components.extend(PromptEngineer.QUALITY_KEYWORDS[:4])
        elif quality_level == 'medium':
            components.extend(PromptEngineer.QUALITY_KEYWORDS[:2])

        # è¿½åŠ è©³ç´°
        if additional_details:
            if isinstance(additional_details, list):
                components.extend(additional_details)
            else:
                components.append(additional_details)

        # ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆå
        if artist:
            components.append(f"by {artist}")

        return ", ".join(components)

    @staticmethod
    def build_negative_prompt(custom_negatives=None):
        """
        Negative promptã‚’æ§‹ç¯‰

        Args:
            custom_negatives: ã‚«ã‚¹ã‚¿ãƒ ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰

        Returns:
            Negative promptæ–‡å­—åˆ—
        """
        negatives = PromptEngineer.NEGATIVE_KEYWORDS.copy()
        if custom_negatives:
            negatives.extend(custom_negatives)
        return ", ".join(negatives)

    @staticmethod
    def optimize_for_faces(base_prompt):
        """
        é¡”ã®ç”Ÿæˆã«ç‰¹åŒ–ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–
        """
        face_keywords = [
            'detailed face',
            'perfect eyes',
            'symmetrical face',
            'professional portrait',
            'sharp facial features'
        ]
        return f"{base_prompt}, {', '.join(face_keywords)}"

    @staticmethod
    def optimize_for_landscapes(base_prompt):
        """
        é¢¨æ™¯ç”»ã«ç‰¹åŒ–ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–
        """
        landscape_keywords = [
            'wide angle',
            'epic vista',
            'atmospheric',
            'dramatic lighting',
            'depth of field'
        ]
        return f"{base_prompt}, {', '.join(landscape_keywords)}"

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    pe = PromptEngineer()

    # ãƒãƒ¼ãƒˆãƒ¬ãƒ¼ãƒˆç”Ÿæˆ
    portrait_prompt = pe.build_prompt(
        subject="a young woman with flowing red hair",
        style="digital_art",
        quality_level="high",
        additional_details=["golden hour lighting", "soft shadows"],
        artist="Ilya Kuvshinov"
    )
    portrait_prompt = pe.optimize_for_faces(portrait_prompt)

    # é¢¨æ™¯ç”»ç”Ÿæˆ
    landscape_prompt = pe.build_prompt(
        subject="a serene mountain lake surrounded by pine trees",
        style="oil_painting",
        quality_level="high",
        additional_details=["misty morning", "reflections on water"]
    )
    landscape_prompt = pe.optimize_for_landscapes(landscape_prompt)

    # Negative prompt
    negative = pe.build_negative_prompt(["deformed", "disfigured"])

    print("Portrait Prompt:")
    print(portrait_prompt)
    print("\nLandscape Prompt:")
    print(landscape_prompt)
    print("\nNegative Prompt:")
    print(negative)
</code></pre>

<blockquote>
<p><strong>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</strong>:</p>
<ul>
<li><strong>å…·ä½“çš„ã«è¨˜è¿°</strong>: ã€Œç¾ã—ã„é¢¨æ™¯ã€ã‚ˆã‚Šã€Œé›ªå±±ã«å›²ã¾ã‚ŒãŸæ¾„ã‚“ã æ¹–ã€å¤•æš®ã‚Œã®å…‰ã€</li>
<li><strong>å“è³ªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ´»ç”¨</strong>: "highly detailed", "8k", "masterpiece"ãªã©</li>
<li><strong>Negative promptæ´»ç”¨</strong>: é¿ã‘ãŸã„è¦ç´ ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š</li>
<li><strong>é‡ã¿ä»˜ã‘ä½¿ç”¨</strong>: (keyword:1.5)ã§å¼·èª¿ã€(keyword:0.8)ã§å¼±ã‚ã‚‹</li>
<li><strong>ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆå‚ç…§</strong>: ç‰¹å®šã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’å¾—ã‚‹ãŸã‚è‘—åã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆåã‚’ä½¿ç”¨</li>
</ul>
</blockquote>

<h3>CFGï¼ˆClassifier-Free Guidanceï¼‰ã®ç†è«–</h3>

<p>CFGã¯ã€æ¡ä»¶ä»˜ãç”Ÿæˆã«ãŠã„ã¦å“è³ªã‚’å‘ä¸Šã•ã›ã‚‹æŠ€è¡“ã§ã™ã€‚æ¡ä»¶ä»˜ããƒ¢ãƒ‡ãƒ«ã¨ç„¡æ¡ä»¶ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’çµ„ã¿åˆã‚ã›ã¾ã™ã€‚</p>

<p>$$
\epsilon_\theta(z_t, c, t) = \epsilon_\theta(z_t, \emptyset, t) + s \cdot (\epsilon_\theta(z_t, c, t) - \epsilon_\theta(z_t, \emptyset, t))
$$</p>

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$\epsilon_\theta(z_t, c, t)$: æ¡ä»¶$c$ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰ä»˜ããƒã‚¤ã‚ºäºˆæ¸¬</li>
<li>$\epsilon_\theta(z_t, \emptyset, t)$: ç„¡æ¡ä»¶ãƒã‚¤ã‚ºäºˆæ¸¬</li>
<li>$s$: ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆé€šå¸¸7ã€œ15ï¼‰</li>
<li>$z_t$: æ™‚åˆ»$t$ã®Latentå¤‰æ•°</li>
</ul>

<div class="mermaid">
graph TB
    A[Input: Noisy Latent z_t] --> B[Conditional Path<br/>with Text c]
    A --> C[Unconditional Path<br/>no text]

    B --> D[Îµ_cond]
    C --> E[Îµ_uncond]

    D --> F[Guidance Calculation]
    E --> F

    F --> G[Îµ_guided = Îµ_uncond + s Ã— Î”Îµ]
    G --> H[Denoising Step]

    I[Guidance Scale s] --> F

    style A fill:#e3f2fd
    style H fill:#c8e6c9
    style I fill:#fff9c4
</div>

<hr>

<h2>5.2 Image-to-Imageå¤‰æ›</h2>

<h3>Style Transferï¼ˆã‚¹ã‚¿ã‚¤ãƒ«è»¢é€ï¼‰</h3>

<p><strong>Style Transfer</strong>ã¯ã€ã‚ã‚‹ç”»åƒã®ã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆè‰²å½©ã€ç­†è§¦ã€è³ªæ„Ÿï¼‰ã‚’åˆ¥ã®ç”»åƒã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«é©ç”¨ã™ã‚‹æŠ€è¡“ã§ã™ã€‚Stable Diffusionã§ã¯ã€åˆæœŸãƒã‚¤ã‚ºã®ä»£ã‚ã‚Šã«æ—¢å­˜ç”»åƒã‚’ä½¿ç”¨ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">from diffusers import StableDiffusionImg2ImgPipeline
from PIL import Image
import torch

class StyleTransferGenerator:
    """
    Stable Diffusionã‚’ä½¿ã£ãŸImage-to-Imageå¤‰æ›ã‚¯ãƒ©ã‚¹

    Features:
    - Style transfer
    - Image variationç”Ÿæˆ
    - Strengthåˆ¶å¾¡ã«ã‚ˆã‚‹å¤‰æ›åº¦åˆã„èª¿æ•´
    """

    def __init__(self, model_id="stabilityai/stable-diffusion-2-1", device="cuda"):
        self.device = device if torch.cuda.is_available() else "cpu"

        self.pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            safety_checker=None
        )

        self.pipe = self.pipe.to(self.device)

        if self.device == "cuda":
            self.pipe.enable_attention_slicing()

    def transfer_style(
        self,
        input_image,
        style_prompt,
        strength=0.75,
        guidance_scale=7.5,
        num_inference_steps=50,
        seed=None
    ):
        """
        ç”»åƒã«ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é©ç”¨

        Args:
            input_image: å…¥åŠ›ç”»åƒï¼ˆPIL Imageã¾ãŸã¯ãƒ‘ã‚¹Stringï¼‰
            style_prompt: é©ç”¨ã—ãŸã„ã‚¹ã‚¿ã‚¤ãƒ«ã®èª¬æ˜
            strength: å¤‰æ›ã®å¼·åº¦ï¼ˆ0.0-1.0ã€é«˜ã„ã»ã©å¤§ããå¤‰åŒ–ï¼‰
            guidance_scale: CFGã‚¹ã‚±ãƒ¼ãƒ«
            num_inference_steps: ã‚¹ãƒ†ãƒƒãƒ—æ•°
            seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰

        Returns:
            ã‚¹ã‚¿ã‚¤ãƒ«å¤‰æ›å¾Œã®ç”»åƒ
        """
        # ç”»åƒèª­ã¿è¾¼ã¿
        if isinstance(input_image, str):
            input_image = Image.open(input_image).convert('RGB')

        # ãƒªã‚µã‚¤ã‚ºï¼ˆ8ã®å€æ•°ã«ï¼‰
        w, h = input_image.size
        w = (w // 8) * 8
        h = (h // 8) * 8
        input_image = input_image.resize((w, h))

        # ã‚·ãƒ¼ãƒ‰è¨­å®š
        generator = None
        if seed is not None:
            generator = torch.Generator(device=self.device).manual_seed(seed)

        # ã‚¹ã‚¿ã‚¤ãƒ«è»¢é€
        with torch.autocast(self.device):
            output = self.pipe(
                prompt=style_prompt,
                image=input_image,
                strength=strength,
                guidance_scale=guidance_scale,
                num_inference_steps=num_inference_steps,
                generator=generator
            )

        return output.images[0]

    def create_variations(
        self,
        input_image,
        prompt,
        num_variations=4,
        strength=0.5,
        **kwargs
    ):
        """
        å…¥åŠ›ç”»åƒã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ

        Args:
            input_image: å…¥åŠ›ç”»åƒ
            prompt: å¤‰æ›ã®æ–¹å‘æ€§ã‚’ç¤ºã™ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            num_variations: ç”Ÿæˆã™ã‚‹ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³æ•°
            strength: å¤‰æ›å¼·åº¦

        Returns:
            ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”»åƒã®ãƒªã‚¹ãƒˆ
        """
        variations = []
        for i in range(num_variations):
            seed = kwargs.get('seed', None)
            if seed is not None:
                seed = seed + i

            var_img = self.transfer_style(
                input_image,
                prompt,
                strength=strength,
                seed=seed,
                **{k: v for k, v in kwargs.items() if k != 'seed'}
            )
            variations.append(var_img)

        return variations

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    st = StyleTransferGenerator()

    # ã‚¹ã‚¿ã‚¤ãƒ«è»¢é€ã®ä¾‹
    input_image = "path/to/photo.jpg"

    style_prompts = [
        "oil painting in the style of Van Gogh, swirling brushstrokes",
        "anime style, Studio Ghibli aesthetic, vibrant colors",
        "cyberpunk style, neon lights, futuristic",
        "watercolor painting, soft colors, artistic"
    ]

    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()

    # å…ƒç”»åƒè¡¨ç¤º
    original = Image.open(input_image)
    axes[0].imshow(original)
    axes[0].set_title('Original')
    axes[0].axis('off')

    # å„ã‚¹ã‚¿ã‚¤ãƒ«ã§è»¢é€
    for idx, style_prompt in enumerate(style_prompts):
        styled_img = st.transfer_style(
            input_image,
            style_prompt,
            strength=0.75,
            num_inference_steps=50,
            seed=42
        )

        axes[idx + 1].imshow(styled_img)
        axes[idx + 1].set_title(style_prompt[:30] + '...')
        axes[idx + 1].axis('off')

    # æœ€å¾Œã®ã‚»ãƒ«ã¯éè¡¨ç¤º
    axes[-1].axis('off')

    plt.tight_layout()
    plt.show()
</code></pre>

<h3>Super-Resolutionï¼ˆè¶…è§£åƒï¼‰</h3>

<p><strong>Super-Resolution</strong>ã¯ã€ä½è§£åƒåº¦ç”»åƒã‹ã‚‰é«˜è§£åƒåº¦ç”»åƒã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã§ã™ã€‚æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒæœ€å…ˆç«¯ã®æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
from diffusers import StableDiffusionUpscalePipeline
from PIL import Image

class SuperResolutionModel:
    """
    Stable Diffusion Upscalerã‚’ä½¿ã£ãŸè¶…è§£åƒã‚¯ãƒ©ã‚¹

    Features:
    - 4å€ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    - ãƒã‚¤ã‚ºé™¤å»ã¨è©³ç´°è£œå®Œ
    - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚ˆã‚‹å“è³ªåˆ¶å¾¡
    """

    def __init__(self, model_id="stabilityai/stable-diffusion-x4-upscaler", device="cuda"):
        self.device = device if torch.cuda.is_available() else "cpu"

        self.pipe = StableDiffusionUpscalePipeline.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
        )

        self.pipe = self.pipe.to(self.device)

        if self.device == "cuda":
            self.pipe.enable_attention_slicing()
            self.pipe.enable_vae_slicing()

    def upscale(
        self,
        input_image,
        prompt="high quality, detailed",
        num_inference_steps=50,
        guidance_scale=7.5,
        noise_level=20,
        seed=None
    ):
        """
        ç”»åƒã‚’4å€ã«ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ«

        Args:
            input_image: ä½è§£åƒåº¦å…¥åŠ›ç”»åƒ
            prompt: å“è³ªå‘ä¸Šã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            num_inference_steps: ã‚¹ãƒ†ãƒƒãƒ—æ•°
            guidance_scale: CFGã‚¹ã‚±ãƒ¼ãƒ«
            noise_level: ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ï¼ˆ0-100ã€é«˜ã„ã»ã©ã‚ˆã‚Šå¤šãã®è©³ç´°ã‚’ç”Ÿæˆï¼‰
            seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰

        Returns:
            ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ«ã•ã‚ŒãŸç”»åƒ
        """
        # ç”»åƒèª­ã¿è¾¼ã¿
        if isinstance(input_image, str):
            input_image = Image.open(input_image).convert('RGB')

        # ã‚·ãƒ¼ãƒ‰è¨­å®š
        generator = None
        if seed is not None:
            generator = torch.Generator(device=self.device).manual_seed(seed)

        # ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        with torch.autocast(self.device):
            upscaled = self.pipe(
                prompt=prompt,
                image=input_image,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                noise_level=noise_level,
                generator=generator
            ).images[0]

        return upscaled

    def progressive_upscale(self, input_image, target_size, **kwargs):
        """
        æ®µéšçš„ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆéå¸¸ã«å¤§ããªã‚µã‚¤ã‚ºå‘ã‘ï¼‰

        Args:
            input_image: å…¥åŠ›ç”»åƒ
            target_size: ç›®æ¨™ã‚µã‚¤ã‚º (width, height)

        Returns:
            ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ«ã•ã‚ŒãŸç”»åƒ
        """
        if isinstance(input_image, str):
            input_image = Image.open(input_image).convert('RGB')

        current_img = input_image
        current_size = current_img.size

        while current_size[0] < target_size[0] or current_size[1] < target_size[1]:
            # 4å€ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ«
            current_img = self.upscale(current_img, **kwargs)
            current_size = current_img.size

            print(f"Upscaled to: {current_size}")

            # ç›®æ¨™ã‚µã‚¤ã‚ºã‚’è¶…ãˆãŸã‚‰çµ‚äº†
            if current_size[0] >= target_size[0] and current_size[1] >= target_size[1]:
                break

        # æœ€çµ‚çš„ã«ç›®æ¨™ã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚º
        if current_size != target_size:
            current_img = current_img.resize(target_size, Image.LANCZOS)

        return current_img

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    sr = SuperResolutionModel()

    # ä½è§£åƒåº¦ç”»åƒã‚’ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ«
    low_res_image = "path/to/low_res.jpg"

    # ç•°ãªã‚‹ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã§æ¯”è¼ƒ
    noise_levels = [10, 20, 40, 60]

    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()

    # å…ƒç”»åƒ
    original = Image.open(low_res_image)
    axes[0].imshow(original)
    axes[0].set_title(f'Original ({original.size[0]}x{original.size[1]})')
    axes[0].axis('off')

    # å„ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã§ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ«
    for idx, noise_level in enumerate(noise_levels):
        upscaled = sr.upscale(
            low_res_image,
            prompt="high quality, sharp, detailed, professional photograph",
            noise_level=noise_level,
            seed=42
        )

        axes[idx + 1].imshow(upscaled)
        axes[idx + 1].set_title(f'Noise Level {noise_level}\n({upscaled.size[0]}x{upscaled.size[1]})')
        axes[idx + 1].axis('off')

    axes[-1].axis('off')

    plt.tight_layout()
    plt.show()
</code></pre>

<hr>

<h2>5.3 æ¡ä»¶ä»˜ãç”Ÿæˆ (Conditional Generation)</h2>

<h3>Conditional GAN (cGAN)</h3>

<p><strong>Conditional GAN</strong>ã¯ã€ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã‚„å±æ€§æƒ…å ±ãªã©ã®æ¡ä»¶ã«åŸºã¥ã„ã¦ç”»åƒã‚’ç”Ÿæˆã™ã‚‹GANã®æ‹¡å¼µã§ã™ã€‚Generatorã¨Discriminatorã®ä¸¡æ–¹ãŒæ¡ä»¶æƒ…å ±ã‚’å—ã‘å–ã‚Šã¾ã™ã€‚</p>

<div class="mermaid">
graph TB
    A[Random Noise z] --> G[Generator G]
    B[Condition c<br/>Class Label] --> G
    G --> C[Fake Image xÌƒ]

    D[Real Image x] --> Disc[Discriminator D]
    C --> Disc
    B --> Disc

    Disc --> E[Real/Fake + Class]

    style A fill:#e3f2fd
    style B fill:#fff9c4
    style C fill:#ffccbc
    style D fill:#c8e6c9
    style E fill:#f8bbd0
</div>

<h4>cGANã®ç›®çš„é–¢æ•°</h4>

<p>$$
\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x|c)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z|c)|c))]
$$</p>

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$c$: æ¡ä»¶æƒ…å ±ï¼ˆã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒãªã©ï¼‰</li>
<li>$D(x|c)$: æ¡ä»¶$c$ã‚’è€ƒæ…®ã—ãŸè­˜åˆ¥</li>
<li>$G(z|c)$: æ¡ä»¶$c$ã«åŸºã¥ãç”Ÿæˆ</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class ConditionalGenerator(nn.Module):
    """
    Conditional GANã®Generator

    æ¡ä»¶æƒ…å ±ï¼ˆã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ï¼‰ã‚’åŸ‹ã‚è¾¼ã¿ã¨ã—ã¦çµåˆã—ã€
    æ¡ä»¶ä»˜ãç”»åƒç”Ÿæˆã‚’å®Ÿç¾
    """

    def __init__(self, latent_dim=100, num_classes=10, img_size=32, channels=3):
        super(ConditionalGenerator, self).__init__()

        self.latent_dim = latent_dim
        self.num_classes = num_classes
        self.img_size = img_size

        # ã‚¯ãƒ©ã‚¹åŸ‹ã‚è¾¼ã¿
        self.label_emb = nn.Embedding(num_classes, latent_dim)

        # Generatoræœ¬ä½“
        self.init_size = img_size // 4
        self.fc = nn.Linear(latent_dim * 2, 128 * self.init_size ** 2)

        self.conv_blocks = nn.Sequential(
            nn.BatchNorm2d(128),

            nn.Upsample(scale_factor=2),
            nn.Conv2d(128, 128, 3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Upsample(scale_factor=2),
            nn.Conv2d(128, 64, 3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(64, channels, 3, stride=1, padding=1),
            nn.Tanh()
        )

    def forward(self, noise, labels):
        """
        Args:
            noise: ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚º (batch_size, latent_dim)
            labels: ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ« (batch_size,)

        Returns:
            ç”Ÿæˆç”»åƒ (batch_size, channels, img_size, img_size)
        """
        # ã‚¯ãƒ©ã‚¹åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—
        label_embedding = self.label_emb(labels)  # (batch_size, latent_dim)

        # ãƒã‚¤ã‚ºã¨ãƒ©ãƒ™ãƒ«åŸ‹ã‚è¾¼ã¿ã‚’çµåˆ
        gen_input = torch.cat([noise, label_embedding], dim=1)  # (batch_size, latent_dim*2)

        # å…¨çµåˆå±¤
        out = self.fc(gen_input)
        out = out.view(out.size(0), 128, self.init_size, self.init_size)

        # ç•³ã¿è¾¼ã¿å±¤
        img = self.conv_blocks(out)

        return img


class ConditionalDiscriminator(nn.Module):
    """
    Conditional GANã®Discriminator

    ç”»åƒã¨ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€
    Real/Fakeã®è­˜åˆ¥ã‚’è¡Œã†
    """

    def __init__(self, num_classes=10, img_size=32, channels=3):
        super(ConditionalDiscriminator, self).__init__()

        self.num_classes = num_classes
        self.img_size = img_size

        # ã‚¯ãƒ©ã‚¹åŸ‹ã‚è¾¼ã¿ï¼ˆç”»åƒã‚µã‚¤ã‚ºã«å±•é–‹ï¼‰
        self.label_emb = nn.Embedding(num_classes, img_size * img_size)

        # Discriminatoræœ¬ä½“
        self.conv_blocks = nn.Sequential(
            nn.Conv2d(channels + 1, 64, 3, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout2d(0.25),

            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout2d(0.25),

            nn.Conv2d(128, 256, 3, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout2d(0.25),
        )

        # å‡ºåŠ›å±¤ã®ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
        ds_size = img_size // 2 ** 3
        self.adv_layer = nn.Sequential(
            nn.Linear(256 * ds_size ** 2, 1),
            nn.Sigmoid()
        )

    def forward(self, img, labels):
        """
        Args:
            img: å…¥åŠ›ç”»åƒ (batch_size, channels, img_size, img_size)
            labels: ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ« (batch_size,)

        Returns:
            è­˜åˆ¥ã‚¹ã‚³ã‚¢ (batch_size, 1)
        """
        # ã‚¯ãƒ©ã‚¹åŸ‹ã‚è¾¼ã¿ã‚’ç”»åƒã‚µã‚¤ã‚ºã«å¤‰å½¢
        label_embedding = self.label_emb(labels)  # (batch_size, img_size*img_size)
        label_embedding = label_embedding.view(-1, 1, self.img_size, self.img_size)

        # ç”»åƒã¨ãƒ©ãƒ™ãƒ«åŸ‹ã‚è¾¼ã¿ã‚’çµåˆ
        d_in = torch.cat([img, label_embedding], dim=1)  # (batch_size, channels+1, H, W)

        # ç•³ã¿è¾¼ã¿å±¤
        out = self.conv_blocks(d_in)
        out = out.view(out.size(0), -1)

        # è­˜åˆ¥
        validity = self.adv_layer(out)

        return validity


class ConditionalGANTrainer:
    """
    Conditional GANã®è¨“ç·´ã‚¯ãƒ©ã‚¹
    """

    def __init__(self, latent_dim=100, num_classes=10, img_size=32, channels=3, device='cuda'):
        self.device = device
        self.latent_dim = latent_dim
        self.num_classes = num_classes

        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        self.generator = ConditionalGenerator(
            latent_dim, num_classes, img_size, channels
        ).to(device)

        self.discriminator = ConditionalDiscriminator(
            num_classes, img_size, channels
        ).to(device)

        # æœ€é©åŒ–å™¨
        self.optimizer_G = torch.optim.Adam(self.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
        self.optimizer_D = torch.optim.Adam(self.discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

        # æå¤±é–¢æ•°
        self.criterion = nn.BCELoss()

    def train_step(self, real_imgs, labels):
        """
        1è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—

        Args:
            real_imgs: å®Ÿç”»åƒãƒãƒƒãƒ
            labels: å¯¾å¿œã™ã‚‹ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«

        Returns:
            d_loss, g_loss: Discriminatorã¨Generatorã®æå¤±
        """
        batch_size = real_imgs.size(0)

        # ãƒ©ãƒ™ãƒ«
        real_label = torch.ones(batch_size, 1, device=self.device)
        fake_label = torch.zeros(batch_size, 1, device=self.device)

        # ---------------------
        # Discriminatorã®è¨“ç·´
        # ---------------------
        self.optimizer_D.zero_grad()

        # Realç”»åƒã®è­˜åˆ¥
        real_validity = self.discriminator(real_imgs, labels)
        d_real_loss = self.criterion(real_validity, real_label)

        # Fakeç”»åƒã®ç”Ÿæˆã¨è­˜åˆ¥
        z = torch.randn(batch_size, self.latent_dim, device=self.device)
        gen_labels = torch.randint(0, self.num_classes, (batch_size,), device=self.device)
        fake_imgs = self.generator(z, gen_labels)
        fake_validity = self.discriminator(fake_imgs.detach(), gen_labels)
        d_fake_loss = self.criterion(fake_validity, fake_label)

        # Discriminatoræå¤±
        d_loss = (d_real_loss + d_fake_loss) / 2
        d_loss.backward()
        self.optimizer_D.step()

        # -----------------
        # Generatorã®è¨“ç·´
        # -----------------
        self.optimizer_G.zero_grad()

        # Generatorã®æå¤±ï¼ˆDiscriminatorã‚’é¨™ã™ï¼‰
        gen_validity = self.discriminator(fake_imgs, gen_labels)
        g_loss = self.criterion(gen_validity, real_label)

        g_loss.backward()
        self.optimizer_G.step()

        return d_loss.item(), g_loss.item()

    def generate_samples(self, num_samples=10, class_id=None):
        """
        ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã‚’ç”Ÿæˆ

        Args:
            num_samples: ç”Ÿæˆæ•°
            class_id: ç‰¹å®šã‚¯ãƒ©ã‚¹ã‚’ç”Ÿæˆï¼ˆNoneã®å ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ ï¼‰

        Returns:
            ç”Ÿæˆç”»åƒã¨ãƒ©ãƒ™ãƒ«
        """
        self.generator.eval()

        with torch.no_grad():
            z = torch.randn(num_samples, self.latent_dim, device=self.device)

            if class_id is not None:
                labels = torch.full((num_samples,), class_id, dtype=torch.long, device=self.device)
            else:
                labels = torch.randint(0, self.num_classes, (num_samples,), device=self.device)

            gen_imgs = self.generator(z, labels)

        self.generator.train()
        return gen_imgs, labels

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # MNISTé¢¨ã®è¨­å®š
    trainer = ConditionalGANTrainer(
        latent_dim=100,
        num_classes=10,
        img_size=28,
        channels=1,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )

    # å„ã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆ
    fig, axes = plt.subplots(2, 5, figsize=(12, 6))
    axes = axes.flatten()

    for class_id in range(10):
        gen_imgs, _ = trainer.generate_samples(num_samples=1, class_id=class_id)
        img = gen_imgs[0].cpu().squeeze().numpy()

        axes[class_id].imshow(img, cmap='gray')
        axes[class_id].set_title(f'Class {class_id}')
        axes[class_id].axis('off')

    plt.tight_layout()
    plt.show()
</code></pre>

<hr>

<h2>5.4 Audioç”Ÿæˆ</h2>

<h3>WaveGANæ¦‚è¦</h3>

<p><strong>WaveGAN</strong>ã¯ã€ç”Ÿã®éŸ³å£°æ³¢å½¢ã‚’ç›´æ¥ç”Ÿæˆã™ã‚‹GANã§ã™ã€‚ç”»åƒç”ŸæˆGANã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’1æ¬¡å…ƒç•³ã¿è¾¼ã¿ã«é©å¿œã•ã›ã¦ã„ã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[Random Noise<br/>100-dim] --> B[FC Layer<br/>16Ã—256]
    B --> C[Reshape<br/>256Ã—16]
    C --> D[Transposed Conv1DÃ—5<br/>Upsample]
    D --> E[Output<br/>16384 samples<br/>1 second @ 16kHz]

    F[Real Audio] --> G[Conv1DÃ—5<br/>Downsample]
    E --> G
    G --> H[FC Layer] --> I[Real/Fake]

    style A fill:#e3f2fd
    style E fill:#c8e6c9
    style I fill:#f8bbd0
</div>

<h4>WaveGANã®ç‰¹å¾´</h4>

<table>
<thead>
<tr>
<th>ç‰¹å¾´</th>
<th>ç”»åƒGAN</th>
<th>WaveGAN</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ç•³ã¿è¾¼ã¿</strong></td>
<td>2D Conv</td>
<td>1D Convï¼ˆæ™‚é–“è»¸ï¼‰</td>
</tr>
<tr>
<td><strong>ã‚µãƒ³ãƒ—ãƒ«é•·</strong></td>
<td>64Ã—64ãƒ”ã‚¯ã‚»ãƒ«</td>
<td>16384ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ1ç§’@16kHzï¼‰</td>
</tr>
<tr>
<td><strong>ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</strong></td>
<td>2å€ãšã¤</td>
<td>4å€ã€8å€ã€16å€ãªã©</td>
</tr>
<tr>
<td><strong>æ­£è¦åŒ–</strong></td>
<td>Batch Norm</td>
<td>Phase Shuffleï¼ˆä½ç›¸ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼‰</td>
</tr>
</tbody>
</table>

<blockquote>
<p><strong>Phase Shuffle</strong>: WaveGANã®é‡è¦ãªæŠ€è¡“ã§ã€è¨“ç·´ä¸­ã«ãƒ©ãƒ³ãƒ€ãƒ ã«ä½ç›¸ã‚’ã‚·ãƒ•ãƒˆã•ã›ã‚‹ã“ã¨ã§ã€DiscriminatorãŒç‰¹å®šã®ä½ç›¸ã«éå­¦ç¿’ã™ã‚‹ã®ã‚’é˜²ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã®å°‘ãªã„è‡ªç„¶ãªéŸ³å£°ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>5.5 å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: ã‚¢ãƒã‚¿ãƒ¼ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ </h2>

<h3>ã‚¢ãƒã‚¿ãƒ¼ç”Ÿæˆã®è¦ä»¶å®šç¾©</h3>

<p>å®Ÿç”¨çš„ãªã‚¢ãƒã‚¿ãƒ¼ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã«ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ãŒå¿…è¦ã§ã™ï¼š</p>

<ul>
<li><strong>å¤šæ§˜æ€§</strong>: ç•°ãªã‚‹é¡”ç«‹ã¡ã€é«ªå‹ã€æœè£…ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³</li>
<li><strong>ä¸€è²«æ€§</strong>: åŒä¸€äººç‰©ã®ç•°ãªã‚‹ãƒãƒ¼ã‚ºã‚„è¡¨æƒ…</li>
<li><strong>åˆ¶å¾¡æ€§</strong>: ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã®å±æ€§ï¼ˆé«ªè‰²ã€ç›®ã®è‰²ãªã©ï¼‰</li>
<li><strong>å“è³ª</strong>: é«˜è§£åƒåº¦ã§è‡ªç„¶ãªè¦‹ãŸç›®</li>
</ul>

<div class="mermaid">
graph TB
    A[User Input] --> B[Text Prompt Builder]
    A --> C[Attribute Selector<br/>Hair, Eyes, Style]

    B --> D[Stable Diffusion Pipeline]
    C --> D

    D --> E[Initial Generation<br/>512Ã—512]
    E --> F[Face Detection &<br/>Alignment]
    F --> G[Super Resolution<br/>2048Ã—2048]

    G --> H{Quality Check}
    H -->|Pass| I[Final Avatar]
    H -->|Fail| D

    style A fill:#e3f2fd
    style I fill:#c8e6c9
    style H fill:#fff9c4
</div>

<pre><code class="language-python">import torch
from diffusers import StableDiffusionPipeline, StableDiffusionUpscalePipeline
from PIL import Image, ImageDraw, ImageFont
import random

class AvatarGenerationSystem:
    """
    åŒ…æ‹¬çš„ãªã‚¢ãƒã‚¿ãƒ¼ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 

    Features:
    - ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªå±æ€§ï¼ˆé«ªã€ç›®ã€ã‚¹ã‚¿ã‚¤ãƒ«ãªã©ï¼‰
    - ä¸€è²«æ€§ã®ã‚ã‚‹è¤‡æ•°ãƒãƒ¼ã‚ºç”Ÿæˆ
    - è‡ªå‹•å“è³ªãƒã‚§ãƒƒã‚¯
    - è¶…è§£åƒã«ã‚ˆã‚‹é«˜å“è³ªåŒ–
    """

    # å±æ€§ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    HAIR_STYLES = ['long flowing', 'short', 'curly', 'straight', 'wavy', 'braided']
    HAIR_COLORS = ['blonde', 'brunette', 'black', 'red', 'white', 'blue', 'pink']
    EYE_COLORS = ['blue', 'green', 'brown', 'hazel', 'gray', 'amber']
    STYLES = ['anime', 'realistic', 'cartoon', 'semi-realistic', 'fantasy']
    EXPRESSIONS = ['smiling', 'serious', 'happy', 'calm', 'mysterious']
    BACKGROUNDS = ['simple background', 'gradient background', 'nature background', 'abstract background']

    def __init__(self, device="cuda"):
        self.device = device if torch.cuda.is_available() else "cpu"

        # Text-to-Imageãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        self.sd_pipe = StableDiffusionPipeline.from_pretrained(
            "stabilityai/stable-diffusion-2-1",
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            safety_checker=None
        ).to(self.device)

        # Upscalerãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        self.upscale_pipe = StableDiffusionUpscalePipeline.from_pretrained(
            "stabilityai/stable-diffusion-x4-upscaler",
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
        ).to(self.device)

        if self.device == "cuda":
            self.sd_pipe.enable_attention_slicing()
            self.upscale_pipe.enable_attention_slicing()

    def build_avatar_prompt(
        self,
        gender='female',
        hair_style=None,
        hair_color=None,
        eye_color=None,
        style=None,
        expression=None,
        background=None,
        additional_features=None
    ):
        """
        ã‚¢ãƒã‚¿ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰

        Args:
            gender: 'male' or 'female'
            hair_style, hair_color, eye_color: é«ªã¨ç›®ã®å±æ€§
            style: ã‚¢ãƒ¼ãƒˆã‚¹ã‚¿ã‚¤ãƒ«
            expression: è¡¨æƒ…
            background: èƒŒæ™¯
            additional_features: è¿½åŠ ã®ç‰¹å¾´ï¼ˆãƒªã‚¹ãƒˆï¼‰

        Returns:
            æ§‹ç¯‰ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        # ãƒ©ãƒ³ãƒ€ãƒ é¸æŠï¼ˆæŒ‡å®šãŒãªã„å ´åˆï¼‰
        hair_style = hair_style or random.choice(self.HAIR_STYLES)
        hair_color = hair_color or random.choice(self.HAIR_COLORS)
        eye_color = eye_color or random.choice(self.EYE_COLORS)
        style = style or random.choice(self.STYLES)
        expression = expression or random.choice(self.EXPRESSIONS)
        background = background or random.choice(self.BACKGROUNDS)

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        components = [
            f"portrait of a {gender}",
            f"{hair_color} {hair_style} hair",
            f"{eye_color} eyes",
            expression,
            background,
            f"{style} style",
            "highly detailed face",
            "professional digital art",
            "8k quality",
            "perfect anatomy",
            "beautiful lighting"
        ]

        # è¿½åŠ ç‰¹å¾´
        if additional_features:
            components.extend(additional_features)

        prompt = ", ".join(components)

        # Negative prompt
        negative_prompt = ", ".join([
            "blurry", "low quality", "distorted", "deformed",
            "bad anatomy", "disfigured", "ugly", "duplicate",
            "extra limbs", "mutation", "watermark"
        ])

        return prompt, negative_prompt

    def generate_avatar(
        self,
        prompt=None,
        negative_prompt=None,
        num_inference_steps=50,
        guidance_scale=7.5,
        seed=None,
        upscale=False,
        **prompt_kwargs
    ):
        """
        ã‚¢ãƒã‚¿ãƒ¼ã‚’ç”Ÿæˆ

        Args:
            prompt: ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆNoneã®å ´åˆã¯è‡ªå‹•æ§‹ç¯‰ï¼‰
            negative_prompt: Negative prompt
            num_inference_steps: ã‚¹ãƒ†ãƒƒãƒ—æ•°
            guidance_scale: CFGã‚¹ã‚±ãƒ¼ãƒ«
            seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
            upscale: è¶…è§£åƒã‚’é©ç”¨ã™ã‚‹ã‹
            **prompt_kwargs: build_avatar_promptã¸ã®å¼•æ•°

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸã‚¢ãƒã‚¿ãƒ¼ç”»åƒ
        """
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        if prompt is None:
            prompt, auto_negative = self.build_avatar_prompt(**prompt_kwargs)
            negative_prompt = negative_prompt or auto_negative

        # ã‚·ãƒ¼ãƒ‰è¨­å®š
        generator = None
        if seed is not None:
            generator = torch.Generator(device=self.device).manual_seed(seed)

        # åˆæœŸç”Ÿæˆ
        with torch.autocast(self.device):
            output = self.sd_pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                generator=generator,
                width=512,
                height=512
            )

        avatar = output.images[0]

        # è¶…è§£åƒï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if upscale:
            with torch.autocast(self.device):
                avatar = self.upscale_pipe(
                    prompt=prompt,
                    image=avatar,
                    num_inference_steps=50,
                    guidance_scale=7.5,
                    noise_level=20
                ).images[0]

        return avatar, prompt

    def generate_avatar_set(
        self,
        num_avatars=4,
        consistent_style=True,
        upscale=False,
        seed=None,
        **base_kwargs
    ):
        """
        ä¸€è²«æ€§ã®ã‚ã‚‹ã‚¢ãƒã‚¿ãƒ¼ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ

        Args:
            num_avatars: ç”Ÿæˆæ•°
            consistent_style: ã‚¹ã‚¿ã‚¤ãƒ«ã‚’çµ±ä¸€ã™ã‚‹ã‹
            upscale: è¶…è§£åƒã‚’é©ç”¨
            seed: ãƒ™ãƒ¼ã‚¹ã‚·ãƒ¼ãƒ‰
            **base_kwargs: å…±é€šã®å±æ€§

        Returns:
            ã‚¢ãƒã‚¿ãƒ¼ç”»åƒã®ãƒªã‚¹ãƒˆã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        avatars = []
        prompts = []

        # ä¸€è²«æ€§ã®ãŸã‚ã®å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        if consistent_style:
            style = base_kwargs.get('style') or random.choice(self.STYLES)
            base_kwargs['style'] = style

        for i in range(num_avatars):
            current_seed = seed + i if seed is not None else None

            # å„ã‚¢ãƒã‚¿ãƒ¼ç”Ÿæˆï¼ˆè¡¨æƒ…ã‚’å¤‰ãˆã‚‹ï¼‰
            avatar, prompt = self.generate_avatar(
                expression=random.choice(self.EXPRESSIONS),
                seed=current_seed,
                upscale=upscale,
                **base_kwargs
            )

            avatars.append(avatar)
            prompts.append(prompt)

        return avatars, prompts

    def create_avatar_sheet(self, avatars, prompts, grid_size=(2, 2)):
        """
        ã‚¢ãƒã‚¿ãƒ¼ã‚·ãƒ¼ãƒˆã‚’ä½œæˆï¼ˆè¤‡æ•°ã‚¢ãƒã‚¿ãƒ¼ã‚’ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤ºï¼‰

        Args:
            avatars: ã‚¢ãƒã‚¿ãƒ¼ç”»åƒã®ãƒªã‚¹ãƒˆ
            prompts: å¯¾å¿œã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒªã‚¹ãƒˆ
            grid_size: ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º (rows, cols)

        Returns:
            ã‚°ãƒªãƒƒãƒ‰ç”»åƒ
        """
        rows, cols = grid_size
        w, h = avatars[0].size

        # ãƒ†ã‚­ã‚¹ãƒˆç”¨ã®ã‚¹ãƒšãƒ¼ã‚¹ã‚’è¿½åŠ 
        text_height = 100
        grid = Image.new('RGB', (w * cols, h * rows + text_height * rows), 'white')
        draw = ImageDraw.Draw(grid)

        for idx, (avatar, prompt) in enumerate(zip(avatars[:rows*cols], prompts[:rows*cols])):
            row = idx // cols
            col = idx % cols

            # ã‚¢ãƒã‚¿ãƒ¼è²¼ã‚Šä»˜ã‘
            grid.paste(avatar, (col * w, row * (h + text_height)))

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚­ã‚¹ãƒˆï¼ˆç°¡ç•¥ç‰ˆï¼‰
            short_prompt = prompt[:60] + '...' if len(prompt) > 60 else prompt
            text_y = row * (h + text_height) + h + 10
            draw.text((col * w + 10, text_y), short_prompt, fill='black')

        return grid

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    avatar_system = AvatarGenerationSystem()

    # å˜ä¸€ã‚¢ãƒã‚¿ãƒ¼ç”Ÿæˆ
    avatar, prompt = avatar_system.generate_avatar(
        gender='female',
        hair_color='pink',
        eye_color='blue',
        style='anime',
        expression='smiling',
        seed=42,
        upscale=False
    )

    print(f"Generated avatar with prompt: {prompt}")
    avatar.show()

    # ã‚¢ãƒã‚¿ãƒ¼ã‚»ãƒƒãƒˆç”Ÿæˆï¼ˆä¸€è²«æ€§ã‚ã‚Šï¼‰
    avatars, prompts = avatar_system.generate_avatar_set(
        num_avatars=4,
        gender='male',
        style='realistic',
        hair_color='black',
        consistent_style=True,
        seed=100
    )

    # ã‚¢ãƒã‚¿ãƒ¼ã‚·ãƒ¼ãƒˆä½œæˆ
    sheet = avatar_system.create_avatar_sheet(avatars, prompts, grid_size=(2, 2))
    sheet.show()

    # ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
    random_avatars = []
    for i in range(6):
        avatar, _ = avatar_system.generate_avatar(seed=i*10)
        random_avatars.append(avatar)

    random_sheet = avatar_system.create_avatar_sheet(
        random_avatars,
        ['Random Avatar'] * 6,
        grid_size=(2, 3)
    )
    random_sheet.show()
</code></pre>

<h3>ã‚¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ä½œæˆã‚·ã‚¹ãƒ†ãƒ </h3>

<pre><code class="language-python">class ArtworkCreationSystem:
    """
    èŠ¸è¡“ä½œå“ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 

    Features:
    - æ§˜ã€…ãªã‚¢ãƒ¼ãƒˆã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆæ²¹çµµã€æ°´å½©ã€ãƒ‡ã‚¸ã‚¿ãƒ«ã‚¢ãƒ¼ãƒˆãªã©ï¼‰
    - æ§‹å›³åˆ¶å¾¡
    - è‰²å½©ãƒ‘ãƒ¬ãƒƒãƒˆæŒ‡å®š
    - ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã‚¹ã‚¿ã‚¤ãƒ«æ¨¡å€£
    """

    ART_STYLES = {
        'oil_painting': 'oil painting on canvas, thick brush strokes, impasto technique',
        'watercolor': 'watercolor painting, soft colors, transparent layers, paper texture',
        'digital_art': 'digital art, digital painting, trending on artstation, highly detailed',
        'impressionism': 'impressionist style, loose brushwork, emphasis on light, outdoor scene',
        'surrealism': 'surrealist art, dreamlike, bizarre imagery, subconscious inspiration',
        'abstract': 'abstract art, non-representational, geometric shapes, bold colors',
        'minimalist': 'minimalist art, simple composition, limited color palette, negative space',
        'cyberpunk': 'cyberpunk art, neon colors, futuristic, high tech low life aesthetic'
    }

    COMPOSITIONS = {
        'rule_of_thirds': 'rule of thirds composition, balanced',
        'symmetrical': 'symmetrical composition, centered, mirror-like',
        'diagonal': 'diagonal composition, dynamic, movement',
        'golden_ratio': 'golden ratio composition, harmonious proportions',
        'minimalist': 'minimalist composition, lots of negative space'
    }

    COLOR_PALETTES = {
        'warm': 'warm color palette, reds, oranges, yellows',
        'cool': 'cool color palette, blues, greens, purples',
        'monochromatic': 'monochromatic color scheme, shades of single color',
        'complementary': 'complementary colors, high contrast',
        'pastel': 'pastel colors, soft, muted tones',
        'vibrant': 'vibrant colors, saturated, bold'
    }

    def __init__(self, device="cuda"):
        self.device = device if torch.cuda.is_available() else "cpu"

        self.sd_pipe = StableDiffusionPipeline.from_pretrained(
            "stabilityai/stable-diffusion-2-1",
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            safety_checker=None
        ).to(self.device)

        if self.device == "cuda":
            self.sd_pipe.enable_attention_slicing()

    def create_artwork(
        self,
        subject,
        art_style='digital_art',
        composition='rule_of_thirds',
        color_palette='vibrant',
        artist_reference=None,
        mood=None,
        additional_details=None,
        num_inference_steps=50,
        guidance_scale=7.5,
        seed=None,
        size=(768, 768)
    ):
        """
        èŠ¸è¡“ä½œå“ã‚’ç”Ÿæˆ

        Args:
            subject: ä¸»é¡Œï¼ˆä¾‹: "a mountain landscape", "a cat"ï¼‰
            art_style: ã‚¢ãƒ¼ãƒˆã‚¹ã‚¿ã‚¤ãƒ«
            composition: æ§‹å›³
            color_palette: è‰²å½©ãƒ‘ãƒ¬ãƒƒãƒˆ
            artist_reference: å‚ç…§ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆå
            mood: é›°å›²æ°—ï¼ˆä¾‹: "melancholic", "joyful"ï¼‰
            additional_details: è¿½åŠ è©³ç´°
            num_inference_steps: ã‚¹ãƒ†ãƒƒãƒ—æ•°
            guidance_scale: CFGã‚¹ã‚±ãƒ¼ãƒ«
            seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
            size: ç”»åƒã‚µã‚¤ã‚º

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸã‚¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ã€ä½¿ç”¨ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        components = [subject]

        # ã‚¹ã‚¿ã‚¤ãƒ«
        if art_style in self.ART_STYLES:
            components.append(self.ART_STYLES[art_style])
        else:
            components.append(art_style)

        # æ§‹å›³
        if composition in self.COMPOSITIONS:
            components.append(self.COMPOSITIONS[composition])

        # è‰²å½©ãƒ‘ãƒ¬ãƒƒãƒˆ
        if color_palette in self.COLOR_PALETTES:
            components.append(self.COLOR_PALETTES[color_palette])

        # é›°å›²æ°—
        if mood:
            components.append(f"{mood} mood")

        # ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆå‚ç…§
        if artist_reference:
            components.append(f"in the style of {artist_reference}")

        # å“è³ªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        components.extend([
            "masterpiece",
            "highly detailed",
            "professional",
            "award winning"
        ])

        # è¿½åŠ è©³ç´°
        if additional_details:
            if isinstance(additional_details, list):
                components.extend(additional_details)
            else:
                components.append(additional_details)

        prompt = ", ".join(components)

        # Negative prompt
        negative_prompt = "low quality, blurry, distorted, ugly, bad art, amateur"

        # ç”Ÿæˆ
        generator = None
        if seed is not None:
            generator = torch.Generator(device=self.device).manual_seed(seed)

        with torch.autocast(self.device):
            output = self.sd_pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                generator=generator,
                width=size[0],
                height=size[1]
            )

        return output.images[0], prompt

    def create_series(
        self,
        base_subject,
        num_variations=4,
        vary_parameter='color_palette',
        **base_kwargs
    ):
        """
        ãƒ†ãƒ¼ãƒã«æ²¿ã£ãŸä½œå“ã‚·ãƒªãƒ¼ã‚ºã‚’ç”Ÿæˆ

        Args:
            base_subject: åŸºæœ¬ä¸»é¡Œ
            num_variations: ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³æ•°
            vary_parameter: å¤‰åŒ–ã•ã›ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            **base_kwargs: å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            ä½œå“ãƒªã‚¹ãƒˆã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒªã‚¹ãƒˆ
        """
        artworks = []
        prompts = []

        # å¤‰åŒ–ã•ã›ã‚‹å€¤ã®ãƒªã‚¹ãƒˆ
        if vary_parameter == 'color_palette':
            variations = list(self.COLOR_PALETTES.keys())
        elif vary_parameter == 'art_style':
            variations = list(self.ART_STYLES.keys())
        elif vary_parameter == 'composition':
            variations = list(self.COMPOSITIONS.keys())
        else:
            variations = [None] * num_variations

        for i, variation in enumerate(variations[:num_variations]):
            kwargs = base_kwargs.copy()
            if variation:
                kwargs[vary_parameter] = variation

            seed = base_kwargs.get('seed')
            if seed is not None:
                kwargs['seed'] = seed + i

            artwork, prompt = self.create_artwork(base_subject, **kwargs)
            artworks.append(artwork)
            prompts.append(prompt)

        return artworks, prompts

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    art_system = ArtworkCreationSystem()

    # å˜ä¸€ä½œå“ç”Ÿæˆ
    artwork, prompt = art_system.create_artwork(
        subject="a serene zen garden with cherry blossoms",
        art_style="watercolor",
        composition="rule_of_thirds",
        color_palette="pastel",
        mood="peaceful",
        seed=42
    )

    print(f"Artwork prompt: {prompt}")
    artwork.show()

    # ä½œå“ã‚·ãƒªãƒ¼ã‚ºï¼ˆè‰²å½©ãƒ‘ãƒ¬ãƒƒãƒˆå¤‰åŒ–ï¼‰
    artworks, prompts = art_system.create_series(
        base_subject="a mystical forest",
        num_variations=4,
        vary_parameter='color_palette',
        art_style='digital_art',
        composition='diagonal',
        seed=100
    )

    # ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤º
    fig, axes = plt.subplots(2, 2, figsize=(12, 12))
    axes = axes.flatten()

    for idx, (artwork, prompt) in enumerate(zip(artworks, prompts)):
        axes[idx].imshow(artwork)
        axes[idx].axis('off')
        # è‰²å½©ãƒ‘ãƒ¬ãƒƒãƒˆåã‚’æŠ½å‡º
        palette = prompt.split('color palette')[0].split(',')[-1].strip()
        axes[idx].set_title(palette.capitalize())

    plt.tight_layout()
    plt.show()
</code></pre>

<hr>

<h2>5.6 å€«ç†çš„è€ƒæ…®äº‹é …</h2>

<h3>ç”ŸæˆAIã®å€«ç†çš„èª²é¡Œ</h3>

<p>ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å¿œç”¨ã«ã¯é‡è¦ãªå€«ç†çš„èª²é¡ŒãŒä¼´ã„ã¾ã™ã€‚è²¬ä»»ã‚ã‚‹é–‹ç™ºã¨åˆ©ç”¨ã®ãŸã‚ã«ä»¥ä¸‹ã®ç‚¹ã‚’è€ƒæ…®ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚</p>

<table>
<thead>
<tr>
<th>èª²é¡Œ</th>
<th>èª¬æ˜</th>
<th>å¯¾ç­–ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ•ã‚§ã‚¤ã‚¯</strong></td>
<td>æœ¬ç‰©ã¨è¦‹åˆ†ã‘ãŒã¤ã‹ãªã„å½ç”»åƒãƒ»å‹•ç”»</td>
<td>é›»å­é€ã‹ã—ã€å‡ºæ‰€è¨¼æ˜ã€æ¤œå‡ºæŠ€è¡“</td>
</tr>
<tr>
<td><strong>è‘—ä½œæ¨©ä¾µå®³</strong></td>
<td>è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ¨©åˆ©ã€ç”Ÿæˆç‰©ã®å¸°å±</td>
<td>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ç¢ºèªã€é©åˆ‡ãªã‚¯ãƒ¬ã‚¸ãƒƒãƒˆè¡¨è¨˜</td>
</tr>
<tr>
<td><strong>ãƒã‚¤ã‚¢ã‚¹ã¨å…¬å¹³æ€§</strong></td>
<td>è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚¤ã‚¢ã‚¹ãŒç”Ÿæˆç‰©ã«åæ˜ </td>
<td>å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒã‚¤ã‚¢ã‚¹æ¤œå‡º</td>
</tr>
<tr>
<td><strong>æ‚ªç”¨ãƒªã‚¹ã‚¯</strong></td>
<td>æœ‰å®³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€è©æ¬ºã€ãƒãƒ©ã‚¹ãƒ¡ãƒ³ãƒˆ</td>
<td>ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã€åˆ©ç”¨è¦ç´„</td>
</tr>
<tr>
<td><strong>ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼</strong></td>
<td>è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å€‹äººæƒ…å ±æ¼æ´©</td>
<td>ãƒ‡ãƒ¼ã‚¿åŒ¿ååŒ–ã€å·®åˆ†ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼</td>
</tr>
</tbody>
</table>

<h4>è²¬ä»»ã‚ã‚‹åˆ©ç”¨ã®ãŸã‚ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h4>

<div class="mermaid">
graph TB
    A[ç”ŸæˆAIã®é–‹ç™ºãƒ»åˆ©ç”¨] --> B[é€æ˜æ€§]
    A --> C[èª¬æ˜è²¬ä»»]
    A --> D[å…¬å¹³æ€§]
    A --> E[ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·]
    A --> F[å®‰å…¨æ€§]

    B --> B1[ãƒ¢ãƒ‡ãƒ«ã®é™ç•Œã‚’æ˜ç¤º]
    B --> B2[ç”Ÿæˆç‰©ã§ã‚ã‚‹æ—¨ã‚’è¡¨ç¤º]

    C --> C1[åˆ©ç”¨è¦ç´„ã®æ•´å‚™]
    C --> C2[ç›£æŸ»å¯èƒ½æ€§ã®ç¢ºä¿]

    D --> D1[ãƒã‚¤ã‚¢ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿæ–½]
    D --> D2[å¤šæ§˜ãªè¡¨ç¾ã®ä¿è¨¼]

    E --> E1[ãƒ‡ãƒ¼ã‚¿ä¿è­·æªç½®]
    E --> E2[åŒæ„å–å¾—ãƒ—ãƒ­ã‚»ã‚¹]

    F --> F1[æœ‰å®³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼]
    F --> F2[èª¤ç”¨é˜²æ­¢æ©Ÿèƒ½]

    style A fill:#e3f2fd
    style B fill:#c8e6c9
    style C fill:#fff9c4
    style D fill:#ffccbc
    style E fill:#f8bbd0
    style F fill:#b2dfdb
</div>

<blockquote>
<p><strong>å®Ÿè£…æ™‚ã®æ¨å¥¨äº‹é …</strong>:</p>
<ol>
<li><strong>Safety Checkerå®Ÿè£…</strong>: æœ‰å®³ãƒ»ä¸é©åˆ‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ¤œå‡ºã¨é™¤å¤–</li>
<li><strong>é€ã‹ã—åŸ‹ã‚è¾¼ã¿</strong>: AIç”Ÿæˆç‰©ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ä¸å¯è¦–ãƒãƒ¼ã‚«ãƒ¼</li>
<li><strong>åˆ©ç”¨ãƒ­ã‚°è¨˜éŒ²</strong>: æ‚ªç”¨æ™‚ã®è¿½è·¡å¯èƒ½æ€§ç¢ºä¿</li>
<li><strong>ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•™è‚²</strong>: é©åˆ‡ãªåˆ©ç”¨æ–¹æ³•ã¨ãƒªã‚¹ã‚¯ã®å‘¨çŸ¥</li>
<li><strong>ç¶™ç¶šçš„ç›£è¦–</strong>: ãƒ¢ãƒ‡ãƒ«ã®æŒ¯ã‚‹èˆã„ã¨ãƒã‚¤ã‚¢ã‚¹ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°</li>
</ol>
</blockquote>

<h3>æ³•çš„ãƒ»è¦åˆ¶çš„å´é¢</h3>

<ul>
<li><strong>è‘—ä½œæ¨©æ³•</strong>: AIç”Ÿæˆç‰©ã®è‘—ä½œæ¨©å¸°å±ã¯è¤‡é›‘ã§ã€å„å›½ã§å¯¾å¿œãŒç•°ãªã‚‹</li>
<li><strong>è‚–åƒæ¨©ãƒ»ãƒ‘ãƒ–ãƒªã‚·ãƒ†ã‚£æ¨©</strong>: å®Ÿåœ¨äººç‰©ã«ä¼¼ãŸç”»åƒã®ç”Ÿæˆã¨åˆ©ç”¨ã«ã¯æ³¨æ„ãŒå¿…è¦</li>
<li><strong>EU AI Act</strong>: é«˜ãƒªã‚¹ã‚¯AIã‚·ã‚¹ãƒ†ãƒ ã¸ã®è¦åˆ¶ã€é€æ˜æ€§è¦ä»¶</li>
<li><strong>ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ãƒãƒªã‚·ãƒ¼</strong>: å„ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã®åˆ©ç”¨è¦ç´„éµå®ˆ</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<details>
<summary><strong>æ¼”ç¿’1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</strong></summary>

<p><strong>èª²é¡Œ</strong>: ä»¥ä¸‹ã®ã‚·ãƒŠãƒªã‚ªã«å¯¾ã—ã¦åŠ¹æœçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ï¼š</p>
<ol>
<li>ä¸­ä¸–ãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘ã®åŸã®é¢¨æ™¯ç”»ï¼ˆæ²¹çµµã‚¹ã‚¿ã‚¤ãƒ«ï¼‰</li>
<li>æœªæ¥éƒ½å¸‚ã®ãƒã‚ªãƒ³è¡—ï¼ˆã‚µã‚¤ãƒãƒ¼ãƒ‘ãƒ³ã‚¯ã‚¹ã‚¿ã‚¤ãƒ«ï¼‰</li>
<li>é™ã‹ãªæ—¥æœ¬åº­åœ’ï¼ˆæ°´å½©ç”»ã‚¹ã‚¿ã‚¤ãƒ«ï¼‰</li>
</ol>

<p><strong>è¦ä»¶</strong>:</p>
<ul>
<li>Subjectã€Styleã€Qualityã€Detailsã€Modifiersã‚’å«ã‚ã‚‹</li>
<li>é©åˆ‡ãªNegative promptã‚’è¨­è¨ˆ</li>
<li>CFGã‚¹ã‚±ãƒ¼ãƒ«ã¨ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’æ¨å¥¨</li>
</ul>

<p><strong>ãƒ’ãƒ³ãƒˆ</strong>: PromptEngineerã‚¯ãƒ©ã‚¹ã‚’å‚è€ƒã«ã€å„è¦ç´ ã‚’æ˜ç¢ºã«åˆ†é›¢ã—ã¦æ§‹ç¯‰ã—ã¾ã—ã‚‡ã†ã€‚</p>
</details>

<details>
<summary><strong>æ¼”ç¿’2: Style Transferå®Ÿè£…</strong></summary>

<p><strong>èª²é¡Œ</strong>: StyleTransferGeneratorã‚¯ãƒ©ã‚¹ã‚’æ‹¡å¼µã—ã€ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ï¼š</p>
<ol>
<li><strong>è¤‡æ•°ã‚¹ã‚¿ã‚¤ãƒ«æ¯”è¼ƒ</strong>: 1ã¤ã®ç”»åƒã«è¤‡æ•°ã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é©ç”¨ã—æ¯”è¼ƒè¡¨ç¤º</li>
<li><strong>å¼·åº¦ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³</strong>: strengthå€¤ã‚’æ®µéšçš„ã«å¤‰åŒ–ã•ã›ãŸçµæœã‚’è¡¨ç¤º</li>
<li><strong>ã‚¹ã‚¿ã‚¤ãƒ«åˆæˆ</strong>: 2ã¤ã®ã‚¹ã‚¿ã‚¤ãƒ«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµ„ã¿åˆã‚ã›</li>
</ol>

<p><strong>æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›</strong>: ã‚°ãƒªãƒƒãƒ‰ç”»åƒã§å„ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¯è¦–åŒ–</p>
</details>

<details>
<summary><strong>æ¼”ç¿’3: Conditional GANæ‹¡å¼µ</strong></summary>

<p><strong>èª²é¡Œ</strong>: ConditionalGANã‚¯ãƒ©ã‚¹ã‚’æ‹¡å¼µã—ã€è¤‡æ•°å±æ€§ã®æ¡ä»¶ä»˜ã‘ï¼ˆMulti-Label Conditional GANï¼‰ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>

<p><strong>ä»•æ§˜</strong>:</p>
<ul>
<li>2ã¤ä»¥ä¸Šã®å±æ€§ã‚’åŒæ™‚ã«æ¡ä»¶ä»˜ã‘ï¼ˆä¾‹: ã‚¯ãƒ©ã‚¹ + è‰²ï¼‰</li>
<li>å„å±æ€§ã«å¯¾ã™ã‚‹åŸ‹ã‚è¾¼ã¿å±¤ã‚’å®Ÿè£…</li>
<li>å±æ€§ã‚’çµ„ã¿åˆã‚ã›ã¦ç”Ÿæˆã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹</li>
</ul>

<p><strong>è©•ä¾¡åŸºæº–</strong>: æŒ‡å®šã—ãŸè¤‡æ•°å±æ€§ã‚’æŒã¤ç”»åƒãŒç”Ÿæˆã•ã‚Œã‚‹ã‹</p>
</details>

<details>
<summary><strong>æ¼”ç¿’4: ã‚¢ãƒã‚¿ãƒ¼ã‚·ã‚¹ãƒ†ãƒ æ”¹å–„</strong></summary>

<p><strong>èª²é¡Œ</strong>: AvatarGenerationSystemã«ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ï¼š</p>
<ol>
<li><strong>ã‚¢ãƒã‚¿ãƒ¼ç·¨é›†æ©Ÿèƒ½</strong>: ç”Ÿæˆå¾Œã«å±æ€§ã‚’éƒ¨åˆ†çš„ã«å¤‰æ›´</li>
<li><strong>ä¸€è²«æ€§ã‚¹ã‚³ã‚¢</strong>: è¤‡æ•°ç”Ÿæˆã•ã‚ŒãŸã‚¢ãƒã‚¿ãƒ¼ã®ä¸€è²«æ€§ã‚’è©•ä¾¡</li>
<li><strong>ãƒãƒƒãƒå‡¦ç†</strong>: å¤§é‡ã®ã‚¢ãƒã‚¿ãƒ¼ã‚’åŠ¹ç‡çš„ã«ç”Ÿæˆ</li>
<li><strong>ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚¿ã‚¤ãƒ«å­¦ç¿’</strong>: ãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›ç”»åƒã‹ã‚‰ã‚¹ã‚¿ã‚¤ãƒ«ã‚’å­¦ç¿’</li>
</ol>

<p><strong>å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ</strong>: Image-to-Imageå¤‰æ›ã‚’æ´»ç”¨ã—ã€æ—¢å­˜ã‚¢ãƒã‚¿ãƒ¼ã‚’åŸºã«ä¿®æ­£ã™ã‚‹æ–¹æ³•ã‚’æ¤œè¨ã—ã¾ã—ã‚‡ã†ã€‚</p>
</details>

<details>
<summary><strong>æ¼”ç¿’5: å€«ç†çš„ã‚»ãƒ¼ãƒ•ã‚¬ãƒ¼ãƒ‰å®Ÿè£…</strong></summary>

<p><strong>èª²é¡Œ</strong>: ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã«å€«ç†çš„ã‚»ãƒ¼ãƒ•ã‚¬ãƒ¼ãƒ‰ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š</p>

<p><strong>å®Ÿè£…é …ç›®</strong>:</p>
<ol>
<li><strong>ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼</strong>: ä¸é©åˆ‡ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ¤œå‡ºãƒ»æ‹’å¦</li>
<li><strong>é€ã‹ã—åŸ‹ã‚è¾¼ã¿</strong>: AIç”Ÿæˆã‚’ç¤ºã™ãƒãƒ¼ã‚«ãƒ¼ã‚’ç”»åƒã«è¿½åŠ </li>
<li><strong>ç”Ÿæˆãƒ­ã‚°</strong>: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ç”Ÿæˆç‰©ã‚’è¨˜éŒ²</li>
<li><strong>ãƒã‚¤ã‚¢ã‚¹æ¤œå‡º</strong>: ç‰¹å®šå±æ€§ã®éå‰°/éå°‘è¡¨ç¾ã‚’æ¤œå‡º</li>
</ol>

<p><strong>ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹</strong>:</p>
<ul>
<li>ä¸é©åˆ‡ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒé©åˆ‡ã«æ‹’å¦ã•ã‚Œã‚‹ã‹</li>
<li>é€ã‹ã—ãŒç”»åƒã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼ˆç›®è¦– or ãƒ—ãƒ­ã‚°ãƒ©ãƒ çš„æ¤œå‡ºï¼‰</li>
<li>å¤šæ§˜æ€§æŒ‡æ¨™ãŒä¸€å®šåŸºæº–ã‚’æº€ãŸã™ã‹</li>
</ul>
</details>

<hr>

<h2>ã¾ã¨ã‚</h2>

<p>ã“ã®ç« ã§ã¯ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å®Ÿè·µçš„å¿œç”¨ã«ã¤ã„ã¦å­¦ç¿’ã—ã¾ã—ãŸï¼š</p>

<ul>
<li><strong>Text-to-Imageç”Ÿæˆ</strong>: Stable Diffusionã‚’ä½¿ã£ãŸé«˜å“è³ªç”»åƒç”Ÿæˆã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æŠ€è¡“</li>
<li><strong>Image-to-Imageå¤‰æ›</strong>: Style transferã¨Super-resolutionã«ã‚ˆã‚‹ç”»åƒå¤‰æ›</li>
<li><strong>æ¡ä»¶ä»˜ãç”Ÿæˆ</strong>: Conditional GANã«ã‚ˆã‚‹åˆ¶å¾¡å¯èƒ½ãªç”Ÿæˆ</li>
<li><strong>Audioç”Ÿæˆ</strong>: WaveGANã«ã‚ˆã‚‹éŸ³å£°æ³¢å½¢ç”Ÿæˆã®åŸºç¤</li>
<li><strong>å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ</strong>: ã‚¢ãƒã‚¿ãƒ¼ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã¨ã‚¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ä½œæˆã®åŒ…æ‹¬çš„å®Ÿè£…</li>
<li><strong>å€«ç†çš„è€ƒæ…®</strong>: è²¬ä»»ã‚ã‚‹AIé–‹ç™ºã®ãŸã‚ã®èª²é¡Œã¨å¯¾ç­–</li>
</ul>

<p>ç”ŸæˆAIã¯å¼·åŠ›ãªæŠ€è¡“ã§ã™ãŒã€ãã®åˆ©ç”¨ã«ã¯è²¬ä»»ãŒä¼´ã„ã¾ã™ã€‚æŠ€è¡“çš„ã‚¹ã‚­ãƒ«ã¨å€«ç†çš„é…æ…®ã®ä¸¡æ–¹ã‚’æŒã£ã¦ã€ç¤¾ä¼šã«è²¢çŒ®ã™ã‚‹å¿œç”¨ã‚’é–‹ç™ºã—ã¦ã„ãã¾ã—ã‚‡ã†ã€‚</p>

<div class="navigation">
    <a href="chapter4-diffusion-models.html" class="nav-button">â† ç¬¬4ç« : æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«</a>
    <a href="../index.html" class="nav-button">ã‚³ãƒ¼ã‚¹ç›®æ¬¡ã«æˆ»ã‚‹</a>
</div>

</main>


    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
    <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
    <p>ML-A04: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«å…¥é–€ | ç¬¬5ç« ï¼šç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å¿œç”¨</p>
</footer>

</body>
</html>
