<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第1章：デプロイメントの基礎 - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>第1章：デプロイメントの基礎</h1>
            <p class="subtitle">機械学習モデルを本番環境で動かす技術</p>
            <div class="meta">
                <span class="meta-item">📖 読了時間: 25-30分</span>
                <span class="meta-item">📊 難易度: 初級〜中級</span>
                <span class="meta-item">💻 コード例: 9個</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>学習目標</h2>
<p>この章を読むことで、以下を習得できます：</p>
<ul>
<li>✅ 機械学習モデルのライフサイクル全体を理解する</li>
<li>✅ デプロイメント方式（バッチ、リアルタイム、エッジ）を使い分けられる</li>
<li>✅ FlaskとFastAPIで推論APIを構築できる</li>
<li>✅ モデルのシリアライゼーション形式を適切に選択できる</li>
<li>✅ 実践的な画像分類APIを実装できる</li>
</ul>

<hr>

<h2>1.1 デプロイメントの基礎</h2>

<h3>機械学習モデルのライフサイクル</h3>

<p>機械学習モデルは、開発から本番運用まで以下のステージを経ます。</p>

<div class="mermaid">
graph LR
    A[問題定義] --> B[データ収集]
    B --> C[特徴量エンジニアリング]
    C --> D[モデル開発]
    D --> E[モデル評価]
    E --> F[デプロイメント]
    F --> G[監視・運用]
    G --> H[再学習]
    H --> D

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#e8f5e9
    style F fill:#fce4ec
    style G fill:#ede7f6
    style H fill:#e0f2f1
</div>

<blockquote>
<p><strong>デプロイメント</strong>は、訓練済みモデルを実際のユーザーが利用できる環境に配置し、予測を提供するプロセスです。</p>
</blockquote>

<h3>デプロイメント方式の比較</h3>

<table>
<thead>
<tr>
<th>方式</th>
<th>特徴</th>
<th>レイテンシ</th>
<th>適用例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>バッチ推論</strong></td>
<td>定期的に大量データを一括処理</td>
<td>数分〜数時間</td>
<td>レコメンデーション、レポート生成</td>
</tr>
<tr>
<td><strong>リアルタイム推論</strong></td>
<td>リクエストごとに即座に応答</td>
<td>数十ms〜数秒</td>
<td>Web API、チャットボット</td>
</tr>
<tr>
<td><strong>エッジ推論</strong></td>
<td>デバイス上でローカル実行</td>
<td>数ms</td>
<td>スマホアプリ、IoTデバイス</td>
</tr>
</tbody>
</table>

<h3>REST APIの基本</h3>

<p>リアルタイム推論では、RESTful APIが最も一般的です。</p>

<pre><code class="language-python"># 基本的なAPIリクエストの流れ
"""
1. クライアント → サーバー: POST /predict
   {
       "features": [5.1, 3.5, 1.4, 0.2]
   }

2. サーバー処理:
   - データ検証
   - 前処理
   - モデル推論
   - 結果フォーマット

3. サーバー → クライアント: 200 OK
   {
       "prediction": "setosa",
       "confidence": 0.98
   }
"""
</code></pre>

<hr>

<h2>1.2 Flaskによる推論API</h2>

<h3>Flask基本セットアップ</h3>

<p>Flaskは軽量で学習コストが低いPythonウェブフレームワークです。</p>

<pre><code class="language-python"># app.py - 基本的なFlaskアプリケーション
from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/health', methods=['GET'])
def health_check():
    """ヘルスチェックエンドポイント"""
    return jsonify({
        'status': 'healthy',
        'version': '1.0.0'
    })

@app.route('/predict', methods=['POST'])
def predict():
    """予測エンドポイント"""
    data = request.get_json()

    # 簡易的な応答（後でモデル推論に置き換え）
    return jsonify({
        'prediction': 'sample',
        'input_received': data
    })

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)
</code></pre>

<p><strong>起動方法</strong>：</p>
<pre><code class="language-bash">python app.py
# → http://localhost:5000 でサーバー起動

# 別のターミナルでテスト
curl http://localhost:5000/health
</code></pre>

<h3>scikit-learnモデルのデプロイ</h3>

<pre><code class="language-python"># train_model.py - モデル訓練とシリアライゼーション
import joblib
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# データ準備
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.2, random_state=42
)

# モデル訓練
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# モデル保存
joblib.dump(model, 'iris_model.pkl')
print(f"モデル精度: {model.score(X_test, y_test):.3f}")
print("モデル保存完了: iris_model.pkl")
</code></pre>

<h3>POSTリクエストでの予測</h3>

<pre><code class="language-python"># flask_app.py - 完全な推論API
from flask import Flask, request, jsonify
import joblib
import numpy as np

app = Flask(__name__)

# モデル読み込み（起動時に1回だけ）
model = joblib.load('iris_model.pkl')
class_names = ['setosa', 'versicolor', 'virginica']

@app.route('/predict', methods=['POST'])
def predict():
    """
    Iris分類の予測

    リクエスト例:
    {
        "features": [5.1, 3.5, 1.4, 0.2]
    }
    """
    try:
        # リクエストデータ取得
        data = request.get_json()
        features = np.array(data['features']).reshape(1, -1)

        # 予測
        prediction = model.predict(features)[0]
        probabilities = model.predict_proba(features)[0]

        # 結果フォーマット
        return jsonify({
            'prediction': class_names[prediction],
            'class_id': int(prediction),
            'probabilities': {
                class_names[i]: float(prob)
                for i, prob in enumerate(probabilities)
            }
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 400

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)
</code></pre>

<p><strong>テスト</strong>：</p>
<pre><code class="language-bash">curl -X POST http://localhost:5000/predict \
  -H "Content-Type: application/json" \
  -d '{"features": [5.1, 3.5, 1.4, 0.2]}'

# 出力:
# {
#   "prediction": "setosa",
#   "class_id": 0,
#   "probabilities": {
#     "setosa": 0.98,
#     "versicolor": 0.02,
#     "virginica": 0.0
#   }
# }
</code></pre>

<h3>エラーハンドリング</h3>

<pre><code class="language-python"># error_handling.py - 堅牢なエラーハンドリング
from flask import Flask, request, jsonify
import joblib
import numpy as np

app = Flask(__name__)
model = joblib.load('iris_model.pkl')
class_names = ['setosa', 'versicolor', 'virginica']

def validate_input(data):
    """入力データの検証"""
    if 'features' not in data:
        raise ValueError("'features' キーが必要です")

    features = data['features']
    if not isinstance(features, list):
        raise ValueError("features はリストである必要があります")

    if len(features) != 4:
        raise ValueError(f"features は4要素必要です（受信: {len(features)}）")

    return np.array(features).reshape(1, -1)

@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.get_json()

        # 入力検証
        features = validate_input(data)

        # 予測
        prediction = model.predict(features)[0]
        probabilities = model.predict_proba(features)[0]

        return jsonify({
            'prediction': class_names[prediction],
            'class_id': int(prediction),
            'probabilities': {
                class_names[i]: float(prob)
                for i, prob in enumerate(probabilities)
            }
        }), 200

    except ValueError as e:
        return jsonify({'error': f'入力エラー: {str(e)}'}), 400

    except Exception as e:
        return jsonify({'error': f'サーバーエラー: {str(e)}'}), 500

@app.errorhandler(404)
def not_found(error):
    return jsonify({'error': 'エンドポイントが見つかりません'}), 404

if __name__ == '__main__':
    app.run(debug=False, host='0.0.0.0', port=5000)
</code></pre>

<hr>

<h2>1.3 FastAPIによる高速推論</h2>

<h3>FastAPIの利点</h3>

<p>FastAPIは、Flaskより高速で、自動ドキュメント生成や型検証を提供します。</p>

<table>
<thead>
<tr>
<th>特徴</th>
<th>Flask</th>
<th>FastAPI</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>速度</strong></td>
<td>中程度</td>
<td>高速（非同期対応）</td>
</tr>
<tr>
<td><strong>型検証</strong></td>
<td>手動</td>
<td>Pydanticで自動</td>
</tr>
<tr>
<td><strong>ドキュメント</strong></td>
<td>手動</td>
<td>自動生成（Swagger UI）</td>
</tr>
<tr>
<td><strong>学習コスト</strong></td>
<td>低い</td>
<td>やや高い</td>
</tr>
</tbody>
</table>

<h3>Pydanticモデル定義</h3>

<pre><code class="language-python"># models.py - Pydanticモデル定義
from pydantic import BaseModel, Field, validator
from typing import List

class IrisFeatures(BaseModel):
    """Iris特徴量の入力スキーマ"""
    features: List[float] = Field(
        ...,
        description="4つの特徴量 [sepal_length, sepal_width, petal_length, petal_width]",
        min_items=4,
        max_items=4
    )

    @validator('features')
    def check_positive(cls, v):
        """特徴量が正の値であることを検証"""
        if any(x < 0 for x in v):
            raise ValueError('すべての特徴量は正の値である必要があります')
        return v

class PredictionResponse(BaseModel):
    """予測結果のレスポンススキーマ"""
    prediction: str = Field(..., description="予測クラス名")
    class_id: int = Field(..., description="クラスID (0-2)")
    probabilities: dict = Field(..., description="各クラスの確率")

# 使用例
sample_input = IrisFeatures(features=[5.1, 3.5, 1.4, 0.2])
print(sample_input.json())
# → {"features": [5.1, 3.5, 1.4, 0.2]}
</code></pre>

<h3>PyTorchモデルのデプロイ</h3>

<pre><code class="language-python"># fastapi_pytorch.py - FastAPI + PyTorch推論API
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
import torch.nn as nn
from typing import List
import uvicorn

# Pydanticモデル
class InputData(BaseModel):
    features: List[float]

class PredictionOutput(BaseModel):
    prediction: int
    confidence: float

# PyTorchモデル定義
class SimpleNN(nn.Module):
    def __init__(self, input_size=4, hidden_size=16, num_classes=3):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# FastAPIアプリケーション
app = FastAPI(
    title="Iris Classification API",
    description="PyTorchモデルによるIris分類API",
    version="1.0.0"
)

# モデル読み込み（起動時）
model = SimpleNN()
model.load_state_dict(torch.load('iris_pytorch_model.pth'))
model.eval()

@app.post("/predict", response_model=PredictionOutput)
async def predict(data: InputData):
    """
    Iris分類の予測

    - **features**: 4つの特徴量のリスト
    """
    try:
        # テンソル変換
        features_tensor = torch.tensor([data.features], dtype=torch.float32)

        # 推論
        with torch.no_grad():
            outputs = model(features_tensor)
            probabilities = torch.softmax(outputs, dim=1)
            prediction = torch.argmax(probabilities, dim=1).item()
            confidence = probabilities[0][prediction].item()

        return PredictionOutput(
            prediction=prediction,
            confidence=confidence
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
</code></pre>

<h3>Swagger UI自動生成</h3>

<p>FastAPIを起動すると、自動的に対話的なAPIドキュメントが生成されます。</p>

<pre><code class="language-bash"># 起動
python fastapi_pytorch.py

# ブラウザで以下にアクセス:
# - Swagger UI: http://localhost:8000/docs
# - ReDoc: http://localhost:8000/redoc
# - OpenAPIスキーマ: http://localhost:8000/openapi.json
</code></pre>

<blockquote>
<p><strong>利点</strong>: ブラウザから直接APIをテストでき、開発効率が大幅に向上します。</p>
</blockquote>

<hr>

<h2>1.4 モデルのシリアライゼーション</h2>

<h3>pickle / joblib</h3>

<p>Pythonオブジェクトを保存する標準的な方法です。</p>

<pre><code class="language-python"># serialization_comparison.py
import pickle
import joblib
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# モデル訓練
iris = load_iris()
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(iris.data, iris.target)

# pickle保存
with open('model_pickle.pkl', 'wb') as f:
    pickle.dump(model, f)

# joblib保存（より効率的）
joblib.dump(model, 'model_joblib.pkl')

# 読み込み
model_pickle = pickle.load(open('model_pickle.pkl', 'rb'))
model_joblib = joblib.load('model_joblib.pkl')

# ファイルサイズ比較
import os
print(f"pickle: {os.path.getsize('model_pickle.pkl')} bytes")
print(f"joblib: {os.path.getsize('model_joblib.pkl')} bytes")
# → joblibの方が効率的（特に大きなnumpy配列を含む場合）
</code></pre>

<h3>ONNX形式</h3>

<p>ONNX（Open Neural Network Exchange）は、異なるフレームワーク間で互換性のある形式です。</p>

<pre><code class="language-python"># onnx_export.py - PyTorchモデルをONNXに変換
import torch
import torch.nn as nn
import onnxruntime as ort
import numpy as np

# PyTorchモデル定義
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(4, 16)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(16, 3)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# モデルのONNXエクスポート
model = SimpleNN()
model.eval()

dummy_input = torch.randn(1, 4)
torch.onnx.export(
    model,
    dummy_input,
    "iris_model.onnx",
    input_names=['features'],
    output_names=['logits'],
    dynamic_axes={
        'features': {0: 'batch_size'},
        'logits': {0: 'batch_size'}
    }
)

# ONNX Runtimeで推論
ort_session = ort.InferenceSession("iris_model.onnx")

def predict_onnx(features):
    ort_inputs = {'features': features.astype(np.float32)}
    ort_outputs = ort_session.run(None, ort_inputs)
    return ort_outputs[0]

# テスト
test_input = np.array([[5.1, 3.5, 1.4, 0.2]])
output = predict_onnx(test_input)
print(f"ONNX推論結果: {output}")
</code></pre>

<h3>TorchScript</h3>

<p>PyTorchモデルを本番環境用に最適化する方式です。</p>

<pre><code class="language-python"># torchscript_export.py
import torch
import torch.nn as nn

class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(4, 16)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(16, 3)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

model = SimpleNN()
model.eval()

# TorchScript変換（トレーシング）
example_input = torch.randn(1, 4)
traced_model = torch.jit.trace(model, example_input)

# 保存
traced_model.save("iris_torchscript.pt")

# 読み込みと推論
loaded_model = torch.jit.load("iris_torchscript.pt")
test_input = torch.tensor([[5.1, 3.5, 1.4, 0.2]])

with torch.no_grad():
    output = loaded_model(test_input)
    print(f"TorchScript推論結果: {output}")
</code></pre>

<h3>形式の比較と選択</h3>

<table>
<thead>
<tr>
<th>形式</th>
<th>対象</th>
<th>長所</th>
<th>短所</th>
<th>推奨用途</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>pickle/joblib</strong></td>
<td>scikit-learn</td>
<td>簡単、軽量</td>
<td>Python依存、セキュリティリスク</td>
<td>開発、プロトタイピング</td>
</tr>
<tr>
<td><strong>ONNX</strong></td>
<td>全般</td>
<td>フレームワーク非依存、高速</td>
<td>変換の手間</td>
<td>本番環境、マルチ言語</td>
</tr>
<tr>
<td><strong>TorchScript</strong></td>
<td>PyTorch</td>
<td>最適化、C++実行可能</td>
<td>PyTorch専用</td>
<td>本番環境（PyTorch）</td>
</tr>
</tbody>
</table>

<hr>

<h2>1.5 実践: 画像分類APIの構築</h2>

<h3>ResNet推論API</h3>

<pre><code class="language-python"># image_classification_api.py - ResNetによる画像分類API
from fastapi import FastAPI, File, UploadFile, HTTPException
from pydantic import BaseModel
import torch
import torchvision.transforms as transforms
from torchvision.models import resnet50, ResNet50_Weights
from PIL import Image
import io
import time

app = FastAPI(title="Image Classification API")

# ResNet50モデル読み込み（起動時）
print("モデル読み込み中...")
weights = ResNet50_Weights.DEFAULT
model = resnet50(weights=weights)
model.eval()

# 前処理パイプライン
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

# ImageNetクラス名
categories = weights.meta["categories"]

class PredictionResult(BaseModel):
    top_class: str
    confidence: float
    top_5: dict
    inference_time_ms: float

@app.post("/predict", response_model=PredictionResult)
async def predict_image(file: UploadFile = File(...)):
    """
    画像分類

    - **file**: 画像ファイル（JPEG, PNG等）
    """
    try:
        # 画像読み込み
        image_bytes = await file.read()
        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')

        # 前処理
        input_tensor = preprocess(image)
        input_batch = input_tensor.unsqueeze(0)

        # 推論
        start_time = time.time()
        with torch.no_grad():
            output = model(input_batch)
        inference_time = (time.time() - start_time) * 1000

        # 確率計算
        probabilities = torch.nn.functional.softmax(output[0], dim=0)

        # Top-5予測
        top5_prob, top5_idx = torch.topk(probabilities, 5)
        top5_results = {
            categories[idx]: float(prob)
            for idx, prob in zip(top5_idx, top5_prob)
        }

        return PredictionResult(
            top_class=categories[top5_idx[0]],
            confidence=float(top5_prob[0]),
            top_5=top5_results,
            inference_time_ms=inference_time
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
</code></pre>

<h3>Base64画像処理</h3>

<pre><code class="language-python"># base64_image_api.py - Base64エンコード画像の処理
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import base64
import io
from PIL import Image
import torch
import torchvision.transforms as transforms
from torchvision.models import resnet50, ResNet50_Weights

app = FastAPI()

# モデル設定
weights = ResNet50_Weights.DEFAULT
model = resnet50(weights=weights)
model.eval()
categories = weights.meta["categories"]

preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

class Base64ImageInput(BaseModel):
    image: str  # Base64エンコード文字列

class PredictionOutput(BaseModel):
    prediction: str
    confidence: float

@app.post("/predict", response_model=PredictionOutput)
async def predict_base64(data: Base64ImageInput):
    """
    Base64エンコード画像の分類

    リクエスト例:
    {
        "image": "data:image/jpeg;base64,/9j/4AAQSkZJRg..."
    }
    """
    try:
        # Base64デコード
        if ',' in data.image:
            image_data = data.image.split(',')[1]  # "data:image/jpeg;base64," を除去
        else:
            image_data = data.image

        image_bytes = base64.b64decode(image_data)
        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')

        # 推論
        input_tensor = preprocess(image).unsqueeze(0)
        with torch.no_grad():
            output = model(input_tensor)

        probabilities = torch.nn.functional.softmax(output[0], dim=0)
        top_prob, top_idx = torch.max(probabilities, dim=0)

        return PredictionOutput(
            prediction=categories[top_idx],
            confidence=float(top_prob)
        )

    except Exception as e:
        raise HTTPException(status_code=400, detail=f"画像処理エラー: {str(e)}")
</code></pre>

<h3>パフォーマンス測定</h3>

<pre><code class="language-python"># benchmark.py - APIパフォーマンス測定
import requests
import time
import numpy as np
from PIL import Image
import io

API_URL = "http://localhost:8000/predict"

def create_test_image():
    """テスト用ダミー画像作成"""
    img = Image.new('RGB', (224, 224), color='red')
    buf = io.BytesIO()
    img.save(buf, format='JPEG')
    buf.seek(0)
    return buf

def benchmark_api(num_requests=100):
    """API性能測定"""
    latencies = []

    print(f"{num_requests}回のリクエストを送信中...")

    for i in range(num_requests):
        image_file = create_test_image()
        files = {'file': ('test.jpg', image_file, 'image/jpeg')}

        start = time.time()
        response = requests.post(API_URL, files=files)
        latency = (time.time() - start) * 1000

        if response.status_code == 200:
            latencies.append(latency)
        else:
            print(f"エラー: {response.status_code}")

        if (i + 1) % 10 == 0:
            print(f"  進捗: {i + 1}/{num_requests}")

    # 統計
    latencies = np.array(latencies)
    print("\n=== パフォーマンス統計 ===")
    print(f"リクエスト数: {len(latencies)}")
    print(f"平均レイテンシ: {latencies.mean():.2f} ms")
    print(f"中央値: {np.median(latencies):.2f} ms")
    print(f"最小: {latencies.min():.2f} ms")
    print(f"最大: {latencies.max():.2f} ms")
    print(f"標準偏差: {latencies.std():.2f} ms")
    print(f"P95: {np.percentile(latencies, 95):.2f} ms")
    print(f"P99: {np.percentile(latencies, 99):.2f} ms")

if __name__ == "__main__":
    benchmark_api(num_requests=100)
</code></pre>

<p><strong>実行例</strong>：</p>
<pre><code>=== パフォーマンス統計 ===
リクエスト数: 100
平均レイテンシ: 125.34 ms
中央値: 120.12 ms
最小: 98.45 ms
最大: 210.67 ms
標準偏差: 18.92 ms
P95: 155.23 ms
P99: 180.45 ms
</code></pre>

<hr>

<h2>1.6 本章のまとめ</h2>

<h3>学んだこと</h3>

<ol>
<li><p><strong>デプロイメントの基礎</strong></p>
<ul>
<li>機械学習ライフサイクル全体の理解</li>
<li>バッチ、リアルタイム、エッジ推論の使い分け</li>
</ul></li>

<li><p><strong>Flaskによる推論API</strong></p>
<ul>
<li>軽量で学習コストが低い</li>
<li>エラーハンドリングの実装</li>
</ul></li>

<li><p><strong>FastAPIによる高速推論</strong></p>
<ul>
<li>Pydanticによる型検証</li>
<li>自動ドキュメント生成（Swagger UI）</li>
<li>非同期処理による高速化</li>
</ul></li>

<li><p><strong>モデルシリアライゼーション</strong></p>
<ul>
<li>pickle/joblib: 開発・プロトタイピング</li>
<li>ONNX: マルチフレームワーク対応</li>
<li>TorchScript: PyTorch最適化</li>
</ul></li>

<li><p><strong>実践的な画像分類API</strong></p>
<ul>
<li>ResNetによるリアルタイム推論</li>
<li>Base64画像処理</li>
<li>パフォーマンス測定と最適化</li>
</ul></li>
</ol>

<h3>次の章へ</h3>

<p>第2章では、<strong>Dockerコンテナ化とデプロイメント</strong>を学びます：</p>
<ul>
<li>Dockerの基礎とコンテナ化</li>
<li>マルチステージビルド</li>
<li>Docker Composeによる環境構築</li>
<li>クラウドへのデプロイ（AWS、GCP）</li>
</ul>

<hr>

<h2>参考文献</h2>

<ol>
<li>Géron, A. (2019). <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em> (2nd ed.). O'Reilly Media.</li>
<li>Huyen, C. (2022). <em>Designing Machine Learning Systems</em>. O'Reilly Media.</li>
<li>FastAPI公式ドキュメント: <a href="https://fastapi.tiangolo.com/">https://fastapi.tiangolo.com/</a></li>
<li>ONNX公式サイト: <a href="https://onnx.ai/">https://onnx.ai/</a></li>
</ol>

<div class="navigation">
    <a href="index.html" class="nav-button">← シリーズ目次</a>
    <a href="chapter2-docker-deployment.html" class="nav-button">次の章: Dockerコンテナ化 →</a>
</div>

    </main>

    <footer>
        <p><strong>作成者</strong>: AI Terakoya Content Team</p>
        <p><strong>監修</strong>: Dr. Yusuke Hashimoto（東北大学）</p>
        <p><strong>バージョン</strong>: 1.0 | <strong>作成日</strong>: 2025-10-23</p>
        <p><strong>ライセンス</strong>: Creative Commons BY 4.0</p>
        <p>© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
