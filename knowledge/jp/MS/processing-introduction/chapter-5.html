<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
<meta content="ç¬¬5ç« ï¼šPythonå®Ÿè·µï¼šãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿è§£æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ - MS Terakoya. å¤šæ§˜ãªãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿å½¢å¼ï¼ˆCSV, JSON, Excel, è£…ç½®å›ºæœ‰ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ãŒã§ãã‚‹" name="description"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬5ç« ï¼šPythonå®Ÿè·µï¼šãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿è§£æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ - MS Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #f093fb;
            --color-accent-light: #f5576c;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #f093fb;
            --color-link-hover: #d07be8;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #fce7f3 0%, #fbcfe8 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .breadcrumb {
            max-width: 900px;
            margin: 0 auto;
            padding: var(--spacing-sm) var(--spacing-md);
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        .breadcrumb a {
            color: var(--color-link);
            text-decoration: none;
        }

        .breadcrumb a:hover {
            text-decoration: underline;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <!-- Prism.js for syntax highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
    <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/MS/processing-introduction/chapter-5.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
        <div class="header-content">
            <h1>ç¬¬5ç« ï¼šPythonå®Ÿè·µï¼šãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿è§£æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</h1>
            <p class="subtitle">SPC, DOE, Machine Learning, Automated Reporting</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 35-45åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 7å€‹</span>
            </div>
        </div>
    </header>

    <div class="breadcrumb">
        <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a> &gt;
        <a href="../index.html">MS Dojo</a> &gt;
        <a href="index.html">ãƒ—ãƒ­ã‚»ã‚¹æŠ€è¡“å…¥é–€</a> &gt;
        ç¬¬5ç« 
    </div>

    <main class="container">
        <p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #fce7f3 0%, #fbcfe8 100%); border-left: 4px solid #f093fb; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">
            ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿è§£æã¯ã€ææ–™è£½é€ ã®å“è³ªç®¡ç†ã¨æœ€é©åŒ–ã®æ ¸å¿ƒã§ã™ã€‚ã“ã®ç« ã§ã¯ã€çµ±è¨ˆçš„ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ï¼ˆSPCï¼‰ã€å®Ÿé¨“è¨ˆç”»æ³•ï¼ˆDOEï¼‰ã€æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã€ç•°å¸¸æ¤œçŸ¥ã€è‡ªå‹•ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã¾ã§ã‚’çµ±åˆã—ãŸPythonãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè·µã—ã€å®Ÿå‹™ã§å³ä½¿ãˆã‚‹ã‚¹ã‚­ãƒ«ã‚’ç¿’å¾—ã—ã¾ã™ã€‚
        </p>

        <div class="learning-objectives">
            <h2>å­¦ç¿’ç›®æ¨™</h2>
            <p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
            <ul>
                <li>âœ… å¤šæ§˜ãªãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿å½¢å¼ï¼ˆCSV, JSON, Excel, è£…ç½®å›ºæœ‰ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ãŒã§ãã‚‹</li>
                <li>âœ… SPCï¼ˆStatistical Process Controlï¼‰ãƒãƒ£ãƒ¼ãƒˆï¼ˆX-bar, R-chart, Cp/Cpkï¼‰ã‚’ç”Ÿæˆãƒ»è§£é‡ˆã§ãã‚‹</li>
                <li>âœ… å®Ÿé¨“è¨ˆç”»æ³•ï¼ˆDOE: Design of Experimentsï¼‰ã‚’è¨­è¨ˆã—ã€å¿œç­”æ›²é¢æ³•ï¼ˆRSMï¼‰ã§æœ€é©åŒ–ã§ãã‚‹</li>
                <li>âœ… æ©Ÿæ¢°å­¦ç¿’ï¼ˆå›å¸°ãƒ»åˆ†é¡ï¼‰ã§ãƒ—ãƒ­ã‚»ã‚¹çµæœã‚’äºˆæ¸¬ã—ã€ç‰¹å¾´é‡é‡è¦åº¦ã‚’è©•ä¾¡ã§ãã‚‹</li>
                <li>âœ… ç•°å¸¸æ¤œçŸ¥ï¼ˆIsolation Forest, One-Class SVMï¼‰ã§ä¸è‰¯å“ã‚’æ—©æœŸç™ºè¦‹ã§ãã‚‹</li>
                <li>âœ… è‡ªå‹•ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆmatplotlib, seaborn, Jinja2ï¼‰ã§æ—¥æ¬¡/é€±æ¬¡å ±å‘Šã‚’åŠ¹ç‡åŒ–ã§ãã‚‹</li>
                <li>âœ… å®Œå…¨çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆãƒ‡ãƒ¼ã‚¿ â†’ è§£æ â†’ æœ€é©åŒ– â†’ ãƒ¬ãƒãƒ¼ãƒˆï¼‰ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
            </ul>
        </div>

        <h2>5.1 ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†</h2>

        <h3>5.1.1 å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿å½¢å¼ã¸ã®å¯¾å¿œ</h3>
        <p>å®Ÿéš›ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã¯ã€è£…ç½®ãƒ­ã‚°ï¼ˆCSV, TXTï¼‰ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆJSON, Excelï¼‰ã€ç‹¬è‡ªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆãƒã‚¤ãƒŠãƒªï¼‰ãªã©å¤šå²ã«ã‚ãŸã‚Šã¾ã™ã€‚</p>

        <p><strong>ä¸»è¦ãªãƒ‡ãƒ¼ã‚¿å½¢å¼</strong>ï¼š</p>
        <ul>
            <li><strong>CSV/TSV</strong>ï¼šæœ€ã‚‚ä¸€èˆ¬çš„ã€‚pandas.read_csv()ã§èª­ã¿è¾¼ã¿</li>
            <li><strong>Excel (.xlsx, .xls)</strong>ï¼špandas.read_excel()ã§èª­ã¿è¾¼ã¿</li>
            <li><strong>JSON</strong>ï¼špandas.read_json()ã¾ãŸã¯json.load()ã§èª­ã¿è¾¼ã¿</li>
            <li><strong>HDF5</strong>ï¼šå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã€‚pandas.read_hdf()ã§èª­ã¿è¾¼ã¿</li>
            <li><strong>SQL Database</strong>ï¼špandas.read_sql()ã§ç›´æ¥ã‚¯ã‚¨ãƒª</li>
        </ul>

        <h4>ã‚³ãƒ¼ãƒ‰ä¾‹5-1: å¤šå½¢å¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰</h4>
        <pre><code class="language-python">import pandas as pd
import numpy as np
import json
import glob
from pathlib import Path

class ProcessDataLoader:
    """
    ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆãƒ­ãƒ¼ãƒ€ãƒ¼

    è¤‡æ•°å½¢å¼ãƒ»è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒãƒå‡¦ç†ã‚’ã‚µãƒãƒ¼ãƒˆ
    """

    def __init__(self, data_dir='./process_data'):
        """
        Parameters
        ----------
        data_dir : str or Path
            ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        """
        self.data_dir = Path(data_dir)
        self.supported_formats = ['.csv', '.xlsx', '.json', '.txt']

    def load_single_file(self, filepath):
        """
        å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿

        Parameters
        ----------
        filepath : str or Path
            ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

        Returns
        -------
        df : pd.DataFrame
            èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        """
        filepath = Path(filepath)
        ext = filepath.suffix.lower()

        try:
            if ext == '.csv' or ext == '.txt':
                # CSV/TXTã®èª­ã¿è¾¼ã¿ï¼ˆåŒºåˆ‡ã‚Šæ–‡å­—è‡ªå‹•æ¤œå‡ºï¼‰
                df = pd.read_csv(filepath, sep=None, engine='python')
            elif ext == '.xlsx' or ext == '.xls':
                # Excelã®èª­ã¿è¾¼ã¿ï¼ˆæœ€åˆã®ã‚·ãƒ¼ãƒˆã®ã¿ï¼‰
                df = pd.read_excel(filepath)
            elif ext == '.json':
                # JSONã®èª­ã¿è¾¼ã¿
                df = pd.read_json(filepath)
            else:
                raise ValueError(f"Unsupported file format: {ext}")

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
            df['source_file'] = filepath.name
            df['load_timestamp'] = pd.Timestamp.now()

            print(f"Loaded: {filepath.name} ({len(df)} rows, {len(df.columns)} columns)")
            return df

        except Exception as e:
            print(f"Error loading {filepath}: {e}")
            return None

    def load_batch(self, pattern='*', file_extension='.csv'):
        """
        ãƒãƒƒãƒèª­ã¿è¾¼ã¿ï¼ˆè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆï¼‰

        Parameters
        ----------
        pattern : str
            ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰å¯ï¼‰
        file_extension : str
            ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ãƒ•ã‚£ãƒ«ã‚¿

        Returns
        -------
        df_combined : pd.DataFrame
            çµ±åˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        search_pattern = str(self.data_dir / f"{pattern}{file_extension}")
        files = glob.glob(search_pattern)

        if not files:
            print(f"No files found matching: {search_pattern}")
            return None

        print(f"Found {len(files)} files matching pattern '{pattern}{file_extension}'")

        dfs = []
        for filepath in sorted(files):
            df = self.load_single_file(filepath)
            if df is not None:
                dfs.append(df)

        if not dfs:
            print("No data loaded successfully")
            return None

        # çµ±åˆ
        df_combined = pd.concat(dfs, ignore_index=True)
        print(f"\nCombined data: {len(df_combined)} rows, {len(df_combined.columns)} columns")

        return df_combined

    def preprocess(self, df, dropna_thresh=0.5, drop_duplicates=True):
        """
        åŸºæœ¬çš„ãªå‰å‡¦ç†

        Parameters
        ----------
        df : pd.DataFrame
            å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        dropna_thresh : float
            æ¬ æå€¤ãŒã“ã®å‰²åˆä»¥ä¸Šã®åˆ—ã‚’å‰Šé™¤ï¼ˆ0-1ï¼‰
        drop_duplicates : bool
            é‡è¤‡è¡Œã‚’å‰Šé™¤ã™ã‚‹ã‹

        Returns
        -------
        df_clean : pd.DataFrame
            ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
        """
        df_clean = df.copy()

        # å…ƒã®ã‚µã‚¤ã‚º
        n_rows_orig, n_cols_orig = df_clean.shape

        # 1. æ¬ æå€¤ãŒå¤šã„åˆ—ã®å‰Šé™¤
        thresh = int(len(df_clean) * dropna_thresh)
        df_clean = df_clean.dropna(thresh=thresh, axis=1)

        # 2. å®Œå…¨ã«æ¬ æã—ã¦ã„ã‚‹è¡Œã®å‰Šé™¤
        df_clean = df_clean.dropna(how='all', axis=0)

        # 3. é‡è¤‡è¡Œã®å‰Šé™¤
        if drop_duplicates:
            df_clean = df_clean.drop_duplicates()

        # 4. ãƒ‡ãƒ¼ã‚¿å‹ã®è‡ªå‹•æ¨å®š
        df_clean = df_clean.infer_objects()

        # 5. æ•°å€¤ã‚«ãƒ©ãƒ ã®ç•°å¸¸å€¤æ¤œå‡ºï¼ˆç°¡æ˜“ç‰ˆï¼šÂ±5Ïƒï¼‰
        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            mean = df_clean[col].mean()
            std = df_clean[col].std()
            lower = mean - 5 * std
            upper = mean + 5 * std
            outliers = (df_clean[col] < lower) | (df_clean[col] > upper)
            if outliers.sum() > 0:
                print(f"  {col}: {outliers.sum()} outliers detected (outside Â±5Ïƒ)")
                # ç•°å¸¸å€¤ã‚’NaNã«ç½®æ›ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                # df_clean.loc[outliers, col] = np.nan

        n_rows_clean, n_cols_clean = df_clean.shape

        print(f"\nPreprocessing summary:")
        print(f"  Rows: {n_rows_orig} â†’ {n_rows_clean} ({n_rows_orig - n_rows_clean} removed)")
        print(f"  Columns: {n_cols_orig} â†’ {n_cols_clean} ({n_cols_orig - n_cols_clean} removed)")

        return df_clean

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆå®Ÿéš›ã«ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼‰
    import os
    os.makedirs('./process_data', exist_ok=True)

    # ã‚µãƒ³ãƒ—ãƒ«CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    for i in range(3):
        df_sample = pd.DataFrame({
            'timestamp': pd.date_range('2025-01-01', periods=100, freq='h'),
            'temperature': np.random.normal(400, 10, 100),
            'pressure': np.random.normal(0.5, 0.05, 100),
            'power': np.random.normal(300, 20, 100),
            'thickness': np.random.normal(100, 5, 100)
        })
        df_sample.to_csv(f'./process_data/run_{i+1}.csv', index=False)

    # ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½¿ç”¨
    loader = ProcessDataLoader(data_dir='./process_data')

    # ãƒãƒƒãƒèª­ã¿è¾¼ã¿
    df = loader.load_batch(pattern='run_*', file_extension='.csv')

    if df is not None:
        # å‰å‡¦ç†
        df_clean = loader.preprocess(df, dropna_thresh=0.5, drop_duplicates=True)

        # çµ±è¨ˆã‚µãƒãƒªãƒ¼
        print("\nData summary:")
        print(df_clean.describe())
</code></pre>

        <h2>5.2 çµ±è¨ˆçš„ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ï¼ˆSPC: Statistical Process Controlï¼‰</h2>

        <h3>5.2.1 SPCã®åŸºç¤</h3>
        <p>SPCã¯ã€ãƒ—ãƒ­ã‚»ã‚¹ã®å¤‰å‹•ã‚’çµ±è¨ˆçš„ã«ç›£è¦–ã—ã€ç•°å¸¸ã‚’æ—©æœŸç™ºè¦‹ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

        <p><strong>ä¸»è¦ãªç®¡ç†å›³ï¼ˆControl Chartsï¼‰</strong>ï¼š</p>
        <ul>
            <li><strong>X-bar ãƒãƒ£ãƒ¼ãƒˆ</strong>ï¼šã‚µãƒ³ãƒ—ãƒ«å¹³å‡ã®æ¨ç§»ã‚’ç›£è¦–</li>
            <li><strong>R ãƒãƒ£ãƒ¼ãƒˆ</strong>ï¼šã‚µãƒ³ãƒ—ãƒ«ç¯„å›²ï¼ˆRangeï¼‰ã®æ¨ç§»ã‚’ç›£è¦–</li>
            <li><strong>S ãƒãƒ£ãƒ¼ãƒˆ</strong>ï¼šã‚µãƒ³ãƒ—ãƒ«æ¨™æº–åå·®ã®æ¨ç§»ã‚’ç›£è¦–</li>
            <li><strong>I-MR ãƒãƒ£ãƒ¼ãƒˆ</strong>ï¼šå€‹åˆ¥å€¤ã¨ç§»å‹•ç¯„å›²ï¼ˆIndividual & Moving Rangeï¼‰</li>
        </ul>

        <p><strong>ç®¡ç†é™ç•Œï¼ˆControl Limitsï¼‰</strong>ï¼š</p>

        $$
        \text{UCL} = \bar{X} + 3\sigma, \quad \text{LCL} = \bar{X} - 3\sigma
        $$

        <ul>
            <li>UCL: Upper Control Limitï¼ˆä¸Šæ–¹ç®¡ç†é™ç•Œï¼‰</li>
            <li>LCL: Lower Control Limitï¼ˆä¸‹æ–¹ç®¡ç†é™ç•Œï¼‰</li>
            <li>$\bar{X}$: ãƒ—ãƒ­ã‚»ã‚¹å¹³å‡</li>
            <li>$\sigma$: ãƒ—ãƒ­ã‚»ã‚¹æ¨™æº–åå·®</li>
        </ul>

        <h3>5.2.2 ãƒ—ãƒ­ã‚»ã‚¹èƒ½åŠ›æŒ‡æ•°ï¼ˆCp/Cpkï¼‰</h3>
        <p>ãƒ—ãƒ­ã‚»ã‚¹ãŒè¦æ ¼ã‚’æº€ãŸã™èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹æŒ‡æ¨™ã§ã™ã€‚</p>

        <p><strong>Cpï¼ˆProcess Capabilityï¼‰</strong>ï¼š</p>

        $$
        C_p = \frac{\text{USL} - \text{LSL}}{6\sigma}
        $$

        <ul>
            <li>USL: Upper Specification Limitï¼ˆä¸Šé™è¦æ ¼ï¼‰</li>
            <li>LSL: Lower Specification Limitï¼ˆä¸‹é™è¦æ ¼ï¼‰</li>
        </ul>

        <p><strong>Cpkï¼ˆProcess Capability Indexï¼‰</strong>ï¼š</p>

        $$
        C_{pk} = \min\left(\frac{\text{USL} - \mu}{3\sigma}, \frac{\mu - \text{LSL}}{3\sigma}\right)
        $$

        <ul>
            <li>$\mu$: ãƒ—ãƒ­ã‚»ã‚¹å¹³å‡</li>
        </ul>

        <p><strong>è©•ä¾¡åŸºæº–</strong>ï¼š</p>
        <ul>
            <li>Cpk â‰¥ 1.33: å„ªç§€ï¼ˆä¸è‰¯ç‡ <64 ppmï¼‰</li>
            <li>Cpk â‰¥ 1.00: ååˆ†ï¼ˆä¸è‰¯ç‡ <2700 ppmï¼‰</li>
            <li>Cpk < 1.00: ä¸ååˆ†ï¼ˆãƒ—ãƒ­ã‚»ã‚¹æ”¹å–„å¿…è¦ï¼‰</li>
        </ul>

        <h4>ã‚³ãƒ¼ãƒ‰ä¾‹5-2: SPCãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆï¼ˆX-bar, R-chart, Cp/Cpkï¼‰</h4>
        <pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats

class SPCAnalyzer:
    """
    çµ±è¨ˆçš„ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ï¼ˆSPCï¼‰è§£æã‚¯ãƒ©ã‚¹
    """

    def __init__(self, data, sample_size=5):
        """
        Parameters
        ----------
        data : array-like
            ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆæ™‚ç³»åˆ—ï¼‰
        sample_size : int
            ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚ºï¼‰
        """
        self.data = np.array(data)
        self.sample_size = sample_size
        self.n_samples = len(data) // sample_size

        # ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†å‰²
        self.samples = self.data[:self.n_samples * sample_size].reshape(-1, sample_size)

    def calculate_xbar_r(self):
        """
        X-bar ãƒãƒ£ãƒ¼ãƒˆã¨Rãƒãƒ£ãƒ¼ãƒˆã®çµ±è¨ˆé‡ã‚’è¨ˆç®—

        Returns
        -------
        stats_dict : dict
            çµ±è¨ˆé‡ï¼ˆxbar, R, UCL, LCLï¼‰
        """
        # ã‚µãƒ³ãƒ—ãƒ«å¹³å‡ã¨ã‚µãƒ³ãƒ—ãƒ«ç¯„å›²
        xbar = np.mean(self.samples, axis=1)
        R = np.ptp(self.samples, axis=1)  # Range (max - min)

        # å…¨ä½“å¹³å‡ã¨å¹³å‡ç¯„å›²
        xbar_mean = np.mean(xbar)
        R_mean = np.mean(R)

        # ç®¡ç†å›³å®šæ•°ï¼ˆn=5ã®å ´åˆï¼‰
        # A2, D3, D4ã¯çµ±è¨ˆè¡¨ã‹ã‚‰å–å¾—ï¼ˆJIS Z 9020-2ï¼‰
        control_constants = {
            2: {'A2': 1.880, 'D3': 0, 'D4': 3.267},
            3: {'A2': 1.023, 'D3': 0, 'D4': 2.574},
            4: {'A2': 0.729, 'D3': 0, 'D4': 2.282},
            5: {'A2': 0.577, 'D3': 0, 'D4': 2.114},
            6: {'A2': 0.483, 'D3': 0, 'D4': 2.004},
            7: {'A2': 0.419, 'D3': 0.076, 'D4': 1.924},
            8: {'A2': 0.373, 'D3': 0.136, 'D4': 1.864},
            9: {'A2': 0.337, 'D3': 0.184, 'D4': 1.816},
            10: {'A2': 0.308, 'D3': 0.223, 'D4': 1.777}
        }

        if self.sample_size not in control_constants:
            raise ValueError(f"Sample size {self.sample_size} not supported (use 2-10)")

        consts = control_constants[self.sample_size]

        # X-barãƒãƒ£ãƒ¼ãƒˆã®ç®¡ç†é™ç•Œ
        xbar_UCL = xbar_mean + consts['A2'] * R_mean
        xbar_LCL = xbar_mean - consts['A2'] * R_mean

        # Rãƒãƒ£ãƒ¼ãƒˆã®ç®¡ç†é™ç•Œ
        R_UCL = consts['D4'] * R_mean
        R_LCL = consts['D3'] * R_mean

        return {
            'xbar': xbar,
            'xbar_mean': xbar_mean,
            'xbar_UCL': xbar_UCL,
            'xbar_LCL': xbar_LCL,
            'R': R,
            'R_mean': R_mean,
            'R_UCL': R_UCL,
            'R_LCL': R_LCL
        }

    def calculate_cp_cpk(self, USL, LSL):
        """
        ãƒ—ãƒ­ã‚»ã‚¹èƒ½åŠ›æŒ‡æ•°ï¼ˆCp, Cpkï¼‰ã‚’è¨ˆç®—

        Parameters
        ----------
        USL : float
            ä¸Šé™è¦æ ¼
        LSL : float
            ä¸‹é™è¦æ ¼

        Returns
        -------
        cp_cpk : dict
            {'Cp': float, 'Cpk': float, 'ppm': float}
        """
        mu = np.mean(self.data)
        sigma = np.std(self.data, ddof=1)  # æ¨™æœ¬æ¨™æº–åå·®

        # Cp
        Cp = (USL - LSL) / (6 * sigma)

        # Cpk
        Cpk_upper = (USL - mu) / (3 * sigma)
        Cpk_lower = (mu - LSL) / (3 * sigma)
        Cpk = min(Cpk_upper, Cpk_lower)

        # ä¸è‰¯ç‡ã®æ¨å®šï¼ˆppm: parts per millionï¼‰
        # æ­£è¦åˆ†å¸ƒã‚’ä»®å®š
        z_USL = (USL - mu) / sigma
        z_LSL = (LSL - mu) / sigma

        ppm_upper = (1 - stats.norm.cdf(z_USL)) * 1e6
        ppm_lower = stats.norm.cdf(z_LSL) * 1e6
        ppm_total = ppm_upper + ppm_lower

        return {
            'Cp': Cp,
            'Cpk': Cpk,
            'ppm': ppm_total,
            'sigma': sigma,
            'mu': mu
        }

    def plot_control_charts(self, USL=None, LSL=None):
        """
        ç®¡ç†å›³ã®ãƒ—ãƒ­ãƒƒãƒˆ

        Parameters
        ----------
        USL, LSL : float, optional
            è¦æ ¼é™ç•Œï¼ˆCp/Cpkè¨ˆç®—ç”¨ï¼‰
        """
        stats_dict = self.calculate_xbar_r()

        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 12))

        sample_indices = np.arange(1, len(stats_dict['xbar']) + 1)

        # X-barãƒãƒ£ãƒ¼ãƒˆ
        ax1.plot(sample_indices, stats_dict['xbar'], 'bo-', linewidth=2, markersize=6,
                label='Sample Mean')
        ax1.axhline(stats_dict['xbar_mean'], color='green', linestyle='-', linewidth=2,
                   label=f"Center Line: {stats_dict['xbar_mean']:.2f}")
        ax1.axhline(stats_dict['xbar_UCL'], color='red', linestyle='--', linewidth=2,
                   label=f"UCL: {stats_dict['xbar_UCL']:.2f}")
        ax1.axhline(stats_dict['xbar_LCL'], color='red', linestyle='--', linewidth=2,
                   label=f"LCL: {stats_dict['xbar_LCL']:.2f}")

        # ç®¡ç†é™ç•Œå¤–ã®ç‚¹ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
        out_of_control = (stats_dict['xbar'] > stats_dict['xbar_UCL']) | \
                         (stats_dict['xbar'] < stats_dict['xbar_LCL'])
        if out_of_control.any():
            ax1.scatter(sample_indices[out_of_control], stats_dict['xbar'][out_of_control],
                       color='red', s=150, marker='x', linewidths=3, zorder=5,
                       label='Out of Control')

        ax1.set_xlabel('Sample Number', fontsize=12)
        ax1.set_ylabel('Sample Mean', fontsize=12)
        ax1.set_title('X-bar Control Chart', fontsize=14, fontweight='bold')
        ax1.legend(fontsize=10)
        ax1.grid(alpha=0.3)

        # Rãƒãƒ£ãƒ¼ãƒˆ
        ax2.plot(sample_indices, stats_dict['R'], 'go-', linewidth=2, markersize=6,
                label='Sample Range')
        ax2.axhline(stats_dict['R_mean'], color='blue', linestyle='-', linewidth=2,
                   label=f"Center Line: {stats_dict['R_mean']:.2f}")
        ax2.axhline(stats_dict['R_UCL'], color='red', linestyle='--', linewidth=2,
                   label=f"UCL: {stats_dict['R_UCL']:.2f}")
        ax2.axhline(stats_dict['R_LCL'], color='red', linestyle='--', linewidth=2,
                   label=f"LCL: {stats_dict['R_LCL']:.2f}")

        ax2.set_xlabel('Sample Number', fontsize=12)
        ax2.set_ylabel('Sample Range', fontsize=12)
        ax2.set_title('R Control Chart', fontsize=14, fontweight='bold')
        ax2.legend(fontsize=10)
        ax2.grid(alpha=0.3)

        # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã¨ãƒ—ãƒ­ã‚»ã‚¹èƒ½åŠ›
        ax3.hist(self.data, bins=30, alpha=0.7, color='skyblue', edgecolor='black',
                density=True, label='Data Distribution')

        # æ­£è¦åˆ†å¸ƒãƒ•ã‚£ãƒƒãƒˆ
        mu = np.mean(self.data)
        sigma = np.std(self.data, ddof=1)
        x_range = np.linspace(self.data.min(), self.data.max(), 200)
        ax3.plot(x_range, stats.norm.pdf(x_range, mu, sigma), 'r-', linewidth=2,
                label=f'Normal Fit (Î¼={mu:.2f}, Ïƒ={sigma:.2f})')

        # è¦æ ¼é™ç•Œ
        if USL is not None and LSL is not None:
            ax3.axvline(USL, color='red', linestyle='--', linewidth=2, label=f'USL: {USL}')
            ax3.axvline(LSL, color='red', linestyle='--', linewidth=2, label=f'LSL: {LSL}')

            # Cp/Cpkè¨ˆç®—
            cp_cpk = self.calculate_cp_cpk(USL, LSL)
            textstr = f"Cp = {cp_cpk['Cp']:.2f}\nCpk = {cp_cpk['Cpk']:.2f}\nDefect Rate â‰ˆ {cp_cpk['ppm']:.1f} ppm"
            ax3.text(0.02, 0.98, textstr, transform=ax3.transAxes, fontsize=11,
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

        ax3.set_xlabel('Value', fontsize=12)
        ax3.set_ylabel('Density', fontsize=12)
        ax3.set_title('Process Distribution & Capability', fontsize=14, fontweight='bold')
        ax3.legend(fontsize=10)
        ax3.grid(alpha=0.3)

        plt.tight_layout()
        plt.show()

# å®Ÿè¡Œä¾‹
if __name__ == "__main__":
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
    np.random.seed(42)

    # æ­£å¸¸ãƒ—ãƒ­ã‚»ã‚¹
    data_normal = np.random.normal(100, 2, 100)

    # ç•°å¸¸ãŒæ··å…¥ï¼ˆ90-110ç•ªç›®ã§ã‚·ãƒ•ãƒˆï¼‰
    data_shift = np.random.normal(105, 2, 20)
    data = np.concatenate([data_normal[:90], data_shift, data_normal[90:]])

    # SPCè§£æ
    spc = SPCAnalyzer(data, sample_size=5)

    # ç®¡ç†å›³ãƒ—ãƒ­ãƒƒãƒˆï¼ˆè¦æ ¼: 95-105ï¼‰
    spc.plot_control_charts(USL=105, LSL=95)

    print("\nSPC Analysis Summary:")
    print(f"Total samples: {len(data)}")
    print(f"Subgroups: {spc.n_samples}")
    cp_cpk = spc.calculate_cp_cpk(USL=105, LSL=95)
    print(f"Cp = {cp_cpk['Cp']:.3f}")
    print(f"Cpk = {cp_cpk['Cpk']:.3f}")
    print(f"Estimated defect rate: {cp_cpk['ppm']:.1f} ppm")

    if cp_cpk['Cpk'] >= 1.33:
        print("Process capability: Excellent")
    elif cp_cpk['Cpk'] >= 1.00:
        print("Process capability: Adequate")
    else:
        print("Process capability: Poor - Improvement needed")
</code></pre>

        <h2>5.3 å®Ÿé¨“è¨ˆç”»æ³•ï¼ˆDOE: Design of Experimentsï¼‰</h2>

        <h3>5.3.1 DOEã®åŸºç¤</h3>
        <p>DOEã¯ã€å°‘ãªã„å®Ÿé¨“å›æ•°ã§å¤šãã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ã‚’åŠ¹ç‡çš„ã«èª¿æŸ»ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

        <p><strong>ä¸»è¦ãªå®Ÿé¨“è¨ˆç”»</strong>ï¼š</p>
        <ul>
            <li><strong>å…¨å› å­å®Ÿé¨“</strong>ï¼šå…¨çµ„ã¿åˆã‚ã›ã‚’å®Ÿé¨“ï¼ˆ2<sup>k</sup>å®Ÿé¨“ï¼‰</li>
            <li><strong>éƒ¨åˆ†å®Ÿæ–½è¨ˆç”»</strong>ï¼šä¸»åŠ¹æœã®ã¿è©•ä¾¡ï¼ˆ2<sup>k-p</sup>å®Ÿé¨“ï¼‰</li>
            <li><strong>ç›´äº¤è¡¨</strong>ï¼šã‚¿ã‚°ãƒãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆL8, L16, L27ãªã©ï¼‰</li>
            <li><strong>ä¸­å¿ƒè¤‡åˆè¨ˆç”»ï¼ˆCCDï¼‰</strong>ï¼šå¿œç­”æ›²é¢æ³•ï¼ˆRSMï¼‰ç”¨</li>
        </ul>

        <h3>5.3.2 å¿œç­”æ›²é¢æ³•ï¼ˆRSM: Response Surface Methodologyï¼‰</h3>
        <p>RSMã¯ã€å¿œç­”å¤‰æ•°ï¼ˆç›®çš„é–¢æ•°ï¼‰ã¨èª¬æ˜å¤‰æ•°ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã®é–¢ä¿‚ã‚’2æ¬¡å¤šé …å¼ã§ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¾ã™ã€‚</p>

        $$
        y = \beta_0 + \sum_{i=1}^{k} \beta_i x_i + \sum_{i=1}^{k} \beta_{ii} x_i^2 + \sum_{i<j} \beta_{ij} x_i x_j + \epsilon
        $$

        <ul>
            <li>$y$: å¿œç­”å¤‰æ•°ï¼ˆè†œåšã€å¿œåŠ›ã€å“è³ªã‚¹ã‚³ã‚¢ãªã©ï¼‰</li>
            <li>$x_i$: èª¬æ˜å¤‰æ•°ï¼ˆæ¸©åº¦ã€åœ§åŠ›ã€ãƒ‘ãƒ¯ãƒ¼ãªã©ï¼‰</li>
            <li>$\beta$: å›å¸°ä¿‚æ•°</li>
        </ul>

        <h4>ã‚³ãƒ¼ãƒ‰ä¾‹5-3: å®Ÿé¨“è¨ˆç”»æ³•ï¼ˆ2å› å­å…¨å› å­å®Ÿé¨“+RSMï¼‰</h4>
        <pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from itertools import product

def full_factorial_design(factors, levels):
    """
    å…¨å› å­å®Ÿé¨“ã®å®Ÿé¨“è¨ˆç”»ã‚’ç”Ÿæˆ

    Parameters
    ----------
    factors : dict
        {'factor_name': [level1, level2, ...]}
    levels : int
        å„å› å­ã®æ°´æº–æ•°ï¼ˆ2æ°´æº–ã€3æ°´æº–ãªã©ï¼‰

    Returns
    -------
    design : pd.DataFrame
        å®Ÿé¨“è¨ˆç”»è¡¨
    """
    factor_names = list(factors.keys())
    factor_values = [factors[name] for name in factor_names]

    # å…¨çµ„ã¿åˆã‚ã›ç”Ÿæˆ
    combinations = list(product(*factor_values))

    design = pd.DataFrame(combinations, columns=factor_names)

    return design

def response_surface_model(X, y, degree=2):
    """
    å¿œç­”æ›²é¢ãƒ¢ãƒ‡ãƒ«ï¼ˆå¤šé …å¼å›å¸°ï¼‰

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        èª¬æ˜å¤‰æ•°
    y : array-like, shape (n_samples,)
        å¿œç­”å¤‰æ•°
    degree : int
        å¤šé …å¼æ¬¡æ•°ï¼ˆé€šå¸¸2ï¼‰

    Returns
    -------
    model : sklearn model
        ãƒ•ã‚£ãƒƒãƒˆæ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
    poly : PolynomialFeatures
        å¤šé …å¼å¤‰æ›å™¨
    """
    # å¤šé …å¼ç‰¹å¾´é‡ç”Ÿæˆ
    poly = PolynomialFeatures(degree=degree, include_bias=True)
    X_poly = poly.fit_transform(X)

    # ç·šå½¢å›å¸°
    model = LinearRegression()
    model.fit(X_poly, y)

    print(f"RÂ² score: {model.score(X_poly, y):.3f}")

    return model, poly

# å®Ÿé¨“è¨ˆç”»ã®è¨­å®š
factors = {
    'Temperature': [300, 350, 400, 450, 500],  # [Â°C]
    'Pressure': [0.2, 0.35, 0.5, 0.65, 0.8]     # [Pa]
}

design = full_factorial_design(factors, levels=5)
print("Experimental Design (Full Factorial):")
print(design.head(10))
print(f"Total experiments: {len(design)}")

# å¿œç­”å¤‰æ•°ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå®Ÿéš›ã«ã¯å®Ÿé¨“ã§æ¸¬å®šï¼‰
# çœŸã®ãƒ¢ãƒ‡ãƒ«: y = 100 + 0.2*T + 50*P - 0.0002*T^2 - 50*P^2 + 0.05*T*P
def true_response(T, P):
    """çœŸã®å¿œç­”é–¢æ•°ï¼ˆæœªçŸ¥ã¨ã—ã¦æ‰±ã†ï¼‰"""
    y = 100 + 0.2*T + 50*P - 0.0002*T**2 - 50*P**2 + 0.05*T*P
    # ãƒã‚¤ã‚ºè¿½åŠ 
    y += np.random.normal(0, 2, len(T))
    return y

design['Response'] = true_response(design['Temperature'], design['Pressure'])

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X = design[['Temperature', 'Pressure']].values
y = design['Response'].values

# å¿œç­”æ›²é¢ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
model, poly = response_surface_model(X, y, degree=2)

# äºˆæ¸¬ã‚°ãƒªãƒƒãƒ‰ç”Ÿæˆ
T_range = np.linspace(300, 500, 50)
P_range = np.linspace(0.2, 0.8, 50)
T_grid, P_grid = np.meshgrid(T_range, P_range)

X_grid = np.c_[T_grid.ravel(), P_grid.ravel()]
X_grid_poly = poly.transform(X_grid)
y_pred_grid = model.predict(X_grid_poly).reshape(T_grid.shape)

# å¯è¦–åŒ–
fig = plt.figure(figsize=(16, 6))

# å·¦å›³: 3Då¿œç­”æ›²é¢
ax1 = fig.add_subplot(1, 3, 1, projection='3d')
surf = ax1.plot_surface(T_grid, P_grid, y_pred_grid, cmap='viridis',
                        alpha=0.8, edgecolor='none')
ax1.scatter(X[:, 0], X[:, 1], y, color='red', s=50, marker='o',
           edgecolors='black', linewidths=1.5, label='Experimental Data')
ax1.set_xlabel('Temperature [Â°C]', fontsize=11)
ax1.set_ylabel('Pressure [Pa]', fontsize=11)
ax1.set_zlabel('Response', fontsize=11)
ax1.set_title('Response Surface (3D)', fontsize=13, fontweight='bold')
fig.colorbar(surf, ax=ax1, shrink=0.5, aspect=10)

# ä¸­å¤®å›³: ç­‰é«˜ç·šå›³
ax2 = fig.add_subplot(1, 3, 2)
contour = ax2.contourf(T_grid, P_grid, y_pred_grid, levels=20, cmap='viridis', alpha=0.8)
contour_lines = ax2.contour(T_grid, P_grid, y_pred_grid, levels=10,
                             colors='white', linewidths=1, alpha=0.5)
ax2.clabel(contour_lines, inline=True, fontsize=8)
ax2.scatter(X[:, 0], X[:, 1], color='red', s=50, marker='o',
           edgecolors='black', linewidths=1.5, label='Exp. Points')

# æœ€é©ç‚¹
optimal_idx = np.argmax(y_pred_grid)
T_opt = T_grid.ravel()[optimal_idx]
P_opt = P_grid.ravel()[optimal_idx]
y_opt = y_pred_grid.ravel()[optimal_idx]

ax2.scatter(T_opt, P_opt, color='yellow', s=300, marker='*',
           edgecolors='black', linewidths=2, label=f'Optimum: {y_opt:.1f}', zorder=5)

ax2.set_xlabel('Temperature [Â°C]', fontsize=12)
ax2.set_ylabel('Pressure [Pa]', fontsize=12)
ax2.set_title('Response Surface (Contour)', fontsize=13, fontweight='bold')
ax2.legend(fontsize=10)
fig.colorbar(contour, ax=ax2, label='Response')

# å³å›³: ä¸»åŠ¹æœãƒ—ãƒ­ãƒƒãƒˆ
ax3 = fig.add_subplot(1, 3, 3)

# æ¸©åº¦ã®ä¸»åŠ¹æœï¼ˆåœ§åŠ›ã‚’ä¸­å¤®å€¤ã«å›ºå®šï¼‰
P_center = np.median(factors['Pressure'])
T_effect = np.linspace(300, 500, 50)
X_effect_T = np.c_[T_effect, np.full(50, P_center)]
X_effect_T_poly = poly.transform(X_effect_T)
y_effect_T = model.predict(X_effect_T_poly)

ax3.plot(T_effect, y_effect_T, 'b-', linewidth=2, label=f'Temperature (P={P_center} Pa)')

# åœ§åŠ›ã®ä¸»åŠ¹æœï¼ˆæ¸©åº¦ã‚’ä¸­å¤®å€¤ã«å›ºå®šï¼‰
T_center = np.median(factors['Temperature'])
P_effect = np.linspace(0.2, 0.8, 50)
X_effect_P = np.c_[np.full(50, T_center), P_effect]
X_effect_P_poly = poly.transform(X_effect_P)
y_effect_P = model.predict(X_effect_P_poly)

# å³è»¸
ax3_twin = ax3.twinx()
ax3_twin.plot(P_effect*500, y_effect_P, 'r-', linewidth=2,
             label=f'Pressure (T={T_center}Â°C)')

ax3.set_xlabel('Temperature [Â°C]', fontsize=12, color='blue')
ax3_twin.set_xlabel('Pressure [Pa] (scaled Ã—500)', fontsize=12, color='red')
ax3.set_ylabel('Response (Temperature effect)', fontsize=12, color='blue')
ax3_twin.set_ylabel('Response (Pressure effect)', fontsize=12, color='red')
ax3.set_title('Main Effects Plot', fontsize=13, fontweight='bold')
ax3.tick_params(axis='x', labelcolor='blue')
ax3_twin.tick_params(axis='x', labelcolor='red')
ax3.grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\nOptimal Conditions:")
print(f"  Temperature: {T_opt:.1f} Â°C")
print(f"  Pressure: {P_opt:.3f} Pa")
print(f"  Predicted Response: {y_opt:.2f}")

# å›å¸°ä¿‚æ•°ã®è¡¨ç¤º
coef_names = poly.get_feature_names_out(['T', 'P'])
print(f"\nRegression Coefficients:")
for name, coef in zip(coef_names, [model.intercept_] + list(model.coef_[1:])):
    print(f"  {name}: {coef:.4f}")
</code></pre>

        <h2>5.4 æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹äºˆæ¸¬</h2>

        <h3>5.4.1 å›å¸°ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹å“è³ªäºˆæ¸¬</h3>
        <p>æ©Ÿæ¢°å­¦ç¿’ã‚’ç”¨ã„ã¦ã€ãƒ—ãƒ­ã‚»ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰è£½å“å“è³ªã‚’äºˆæ¸¬ã—ã¾ã™ã€‚</p>

        <p><strong>ä¸»è¦ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </strong>ï¼š</p>
        <ul>
            <li><strong>ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°</strong>ï¼šé«˜ç²¾åº¦ã€è§£é‡ˆæ€§ï¼ˆç‰¹å¾´é‡é‡è¦åº¦ï¼‰</li>
            <li><strong>å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼ˆXGBoost, LightGBMï¼‰</strong>ï¼šæœ€é«˜ç²¾åº¦</li>
            <li><strong>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</strong>ï¼šéç·šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’</li>
            <li><strong>ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼å›å¸°ï¼ˆSVRï¼‰</strong>ï¼šå°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‘ã‘</li>
        </ul>

        <h4>ã‚³ãƒ¼ãƒ‰ä¾‹5-4: ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹å“è³ªäºˆæ¸¬</h4>
        <pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import seaborn as sns

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆï¼ˆå®Ÿéš›ã«ã¯å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
np.random.seed(42)
n_samples = 200

# ãƒ—ãƒ­ã‚»ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
data = pd.DataFrame({
    'Temperature': np.random.uniform(300, 500, n_samples),
    'Pressure': np.random.uniform(0.2, 0.8, n_samples),
    'Power': np.random.uniform(100, 400, n_samples),
    'Flow_Rate': np.random.uniform(50, 150, n_samples),
    'Time': np.random.uniform(30, 120, n_samples)
})

# ç›®çš„å¤‰æ•°ï¼ˆè†œåšï¼‰ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
# çœŸã®ãƒ¢ãƒ‡ãƒ«: è¤‡é›‘ãªéç·šå½¢é–¢ä¿‚
data['Thickness'] = (
    0.5 * data['Temperature'] +
    100 * data['Pressure'] +
    0.3 * data['Power'] +
    0.2 * data['Flow_Rate'] +
    1.0 * data['Time'] +
    0.001 * data['Temperature'] * data['Pressure'] -
    0.0005 * data['Temperature']**2 +
    np.random.normal(0, 10, n_samples)  # ãƒã‚¤ã‚º
)

print("Dataset shape:", data.shape)
print("\nFeature summary:")
print(data.describe())

# ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®åˆ†å‰²
X = data.drop('Thickness', axis=1)
y = data['Thickness']

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)

rf_model.fit(X_train, y_train)

# äºˆæ¸¬
y_train_pred = rf_model.predict(X_train)
y_test_pred = rf_model.predict(X_test)

# è©•ä¾¡æŒ‡æ¨™
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
test_mae = mean_absolute_error(y_test, y_test_pred)

print("\n" + "="*60)
print("Model Performance:")
print("="*60)
print(f"Train RÂ²: {train_r2:.4f}")
print(f"Test RÂ²: {test_r2:.4f}")
print(f"Train RMSE: {train_rmse:.2f}")
print(f"Test RMSE: {test_rmse:.2f}")
print(f"Test MAE: {test_mae:.2f}")

# äº¤å·®æ¤œè¨¼
cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='r2')
print(f"\n5-Fold CV RÂ² score: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
print("="*60)

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# å·¦ä¸Š: äºˆæ¸¬vså®Ÿæ¸¬ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰
axes[0, 0].scatter(y_train, y_train_pred, alpha=0.6, s=30, edgecolors='black', linewidth=0.5)
axes[0, 0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()],
               'r--', linewidth=2, label='Perfect Prediction')
axes[0, 0].set_xlabel('Actual Thickness', fontsize=12)
axes[0, 0].set_ylabel('Predicted Thickness', fontsize=12)
axes[0, 0].set_title(f'Training Set\nRÂ² = {train_r2:.3f}, RMSE = {train_rmse:.2f}',
                    fontsize=13, fontweight='bold')
axes[0, 0].legend(fontsize=10)
axes[0, 0].grid(alpha=0.3)

# å³ä¸Š: äºˆæ¸¬vså®Ÿæ¸¬ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰
axes[0, 1].scatter(y_test, y_test_pred, alpha=0.6, s=30, color='green',
                  edgecolors='black', linewidth=0.5)
axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
               'r--', linewidth=2, label='Perfect Prediction')
axes[0, 1].set_xlabel('Actual Thickness', fontsize=12)
axes[0, 1].set_ylabel('Predicted Thickness', fontsize=12)
axes[0, 1].set_title(f'Test Set\nRÂ² = {test_r2:.3f}, RMSE = {test_rmse:.2f}',
                    fontsize=13, fontweight='bold')
axes[0, 1].legend(fontsize=10)
axes[0, 1].grid(alpha=0.3)

# å·¦ä¸‹: ç‰¹å¾´é‡é‡è¦åº¦
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

bars = axes[1, 0].barh(feature_importance['Feature'], feature_importance['Importance'],
                       color='skyblue', edgecolor='black')
axes[1, 0].set_xlabel('Importance', fontsize=12)
axes[1, 0].set_title('Feature Importance', fontsize=13, fontweight='bold')
axes[1, 0].grid(alpha=0.3, axis='x')

# å€¤ã‚’ãƒãƒ¼ã«è¡¨ç¤º
for bar, importance in zip(bars, feature_importance['Importance']):
    axes[1, 0].text(importance + 0.01, bar.get_y() + bar.get_height()/2,
                   f'{importance:.3f}', va='center', fontsize=10)

# å³ä¸‹: æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
residuals = y_test - y_test_pred
axes[1, 1].scatter(y_test_pred, residuals, alpha=0.6, s=30, color='orange',
                  edgecolors='black', linewidth=0.5)
axes[1, 1].axhline(0, color='red', linestyle='--', linewidth=2)
axes[1, 1].set_xlabel('Predicted Thickness', fontsize=12)
axes[1, 1].set_ylabel('Residuals (Actual - Predicted)', fontsize=12)
axes[1, 1].set_title('Residual Plot (Test Set)', fontsize=13, fontweight='bold')
axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("\nFeature Importance Ranking:")
for idx, row in feature_importance.iterrows():
    print(f"  {row['Feature']}: {row['Importance']:.4f}")

print("\nInterpretation:")
print("  - Time has the highest importance (longer deposition â†’ thicker film)")
print("  - Temperature and Pressure also significant")
print("  - Model can predict thickness with ~Â±10 nm accuracy")
</code></pre>

        <h3>5.4.2 åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ä¸è‰¯å“æ¤œå‡º</h3>
        <p>ãƒ—ãƒ­ã‚»ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ä¸è‰¯å“ã‚’äº‹å‰ã«äºˆæ¸¬ã—ã¾ã™ã€‚</p>

        <h4>ã‚³ãƒ¼ãƒ‰ä¾‹5-5: ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã«ã‚ˆã‚‹ä¸è‰¯å“äºˆæ¸¬</h4>
        <pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import seaborn as sns

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ
np.random.seed(42)
n_samples = 300

# è‰¯å“ï¼ˆ70%ï¼‰ã¨ä¸è‰¯å“ï¼ˆ30%ï¼‰
data = pd.DataFrame({
    'Temperature': np.random.normal(400, 30, n_samples),
    'Pressure': np.random.normal(0.5, 0.1, n_samples),
    'Power': np.random.normal(250, 50, n_samples)
})

# ä¸è‰¯å“ãƒ©ãƒ™ãƒ«ç”Ÿæˆï¼ˆè¦æ ¼å¤–æ¡ä»¶ã§ä¸è‰¯ç‡ãŒä¸ŠãŒã‚‹ï¼‰
# è‰¯å“æ¡ä»¶: 380<T<420, 0.4<P<0.6, 200<Power<300
good_condition = (
    (data['Temperature'] > 380) & (data['Temperature'] < 420) &
    (data['Pressure'] > 0.4) & (data['Pressure'] < 0.6) &
    (data['Power'] > 200) & (data['Power'] < 300)
)

# æ¡ä»¶å¤–ã§ã‚‚ç¢ºç‡çš„ã«è‰¯å“ã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹
data['Defect'] = 0
data.loc[~good_condition, 'Defect'] = np.random.choice([0, 1], size=(~good_condition).sum(),
                                                        p=[0.3, 0.7])

print(f"Dataset: {len(data)} samples")
print(f"Defect rate: {data['Defect'].mean()*100:.1f}%")

# ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°
X = data[['Temperature', 'Pressure', 'Power']]
y = data['Defect']

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                    stratify=y, random_state=42)

# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«
lr_model = LogisticRegression(max_iter=1000, random_state=42)
lr_model.fit(X_train, y_train)

# äºˆæ¸¬
y_test_pred = lr_model.predict(X_test)
y_test_prob = lr_model.predict_proba(X_test)[:, 1]

# è©•ä¾¡
print("\n" + "="*60)
print("Classification Report:")
print("="*60)
print(classification_report(y_test, y_test_pred, target_names=['Good', 'Defect']))

auc_score = roc_auc_score(y_test, y_test_prob)
print(f"ROC-AUC Score: {auc_score:.3f}")
print("="*60)

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# å·¦ä¸Š: æ··åŒè¡Œåˆ—
cm = confusion_matrix(y_test, y_test_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[0, 0],
           xticklabels=['Good', 'Defect'], yticklabels=['Good', 'Defect'])
axes[0, 0].set_xlabel('Predicted', fontsize=12)
axes[0, 0].set_ylabel('Actual', fontsize=12)
axes[0, 0].set_title('Confusion Matrix', fontsize=13, fontweight='bold')

# å³ä¸Š: ROCæ›²ç·š
fpr, tpr, thresholds = roc_curve(y_test, y_test_prob)
axes[0, 1].plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {auc_score:.3f})')
axes[0, 1].plot([0, 1], [0, 1], 'r--', linewidth=2, label='Random Classifier')
axes[0, 1].set_xlabel('False Positive Rate', fontsize=12)
axes[0, 1].set_ylabel('True Positive Rate', fontsize=12)
axes[0, 1].set_title('ROC Curve', fontsize=13, fontweight='bold')
axes[0, 1].legend(fontsize=11)
axes[0, 1].grid(alpha=0.3)

# å·¦ä¸‹: ç‰¹å¾´é‡ä¿‚æ•°
coef_df = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': lr_model.coef_[0]
}).sort_values('Coefficient', key=abs, ascending=False)

bars = axes[1, 0].barh(coef_df['Feature'], coef_df['Coefficient'],
                       color=['red' if c > 0 else 'blue' for c in coef_df['Coefficient']],
                       edgecolor='black')
axes[1, 0].axvline(0, color='black', linewidth=1)
axes[1, 0].set_xlabel('Coefficient (Defect Risk)', fontsize=12)
axes[1, 0].set_title('Feature Coefficients\n(Positive = Increases Defect Risk)',
                    fontsize=13, fontweight='bold')
axes[1, 0].grid(alpha=0.3, axis='x')

# å³ä¸‹: ç¢ºç‡åˆ†å¸ƒ
axes[1, 1].hist(y_test_prob[y_test == 0], bins=20, alpha=0.6, label='Good',
               color='green', edgecolor='black')
axes[1, 1].hist(y_test_prob[y_test == 1], bins=20, alpha=0.6, label='Defect',
               color='red', edgecolor='black')
axes[1, 1].axvline(0.5, color='black', linestyle='--', linewidth=2,
                  label='Decision Threshold')
axes[1, 1].set_xlabel('Predicted Probability (Defect)', fontsize=12)
axes[1, 1].set_ylabel('Count', fontsize=12)
axes[1, 1].set_title('Predicted Probability Distribution', fontsize=13, fontweight='bold')
axes[1, 1].legend(fontsize=11)
axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("\nModel Interpretation:")
for idx, row in coef_df.iterrows():
    direction = "increases" if row['Coefficient'] > 0 else "decreases"
    print(f"  {row['Feature']}: {row['Coefficient']:+.4f} â†’ {direction} defect risk")
</code></pre>

        <h2>5.5 ç•°å¸¸æ¤œçŸ¥ï¼ˆAnomaly Detectionï¼‰</h2>

        <h3>5.5.1 Isolation Forestã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥</h3>
        <p>Isolation Forestã¯ã€æ•™å¸«ãªã—å­¦ç¿’ã§ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡ºã—ã¾ã™ã€‚æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰é€¸è„±ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ã€Œç•°å¸¸ã€ã¨ã—ã¦è­˜åˆ¥ã—ã¾ã™ã€‚</p>

        <h4>ã‚³ãƒ¼ãƒ‰ä¾‹5-6: Isolation Forestã«ã‚ˆã‚‹ç•°å¸¸ãƒ—ãƒ­ã‚»ã‚¹æ¤œå‡º</h4>
        <pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)

# æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ï¼ˆ200ã‚µãƒ³ãƒ—ãƒ«ï¼‰
normal_data = pd.DataFrame({
    'Temperature': np.random.normal(400, 10, 200),
    'Pressure': np.random.normal(0.5, 0.05, 200),
    'Power': np.random.normal(250, 20, 200),
    'Thickness': np.random.normal(100, 5, 200)
})

# ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ï¼ˆ20ã‚µãƒ³ãƒ—ãƒ«ï¼‰
anomaly_data = pd.DataFrame({
    'Temperature': np.random.uniform(350, 450, 20),
    'Pressure': np.random.uniform(0.3, 0.7, 20),
    'Power': np.random.uniform(150, 350, 20),
    'Thickness': np.random.uniform(70, 130, 20)
})

# ãƒ‡ãƒ¼ã‚¿çµ±åˆ
data = pd.concat([normal_data, anomaly_data], ignore_index=True)
true_labels = np.array([0]*len(normal_data) + [1]*len(anomaly_data))  # 0: normal, 1: anomaly

print(f"Total samples: {len(data)}")
print(f"Anomaly rate: {(true_labels == 1).mean()*100:.1f}%")

# ç‰¹å¾´é‡ã®æ¨™æº–åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(data)

# Isolation Forestãƒ¢ãƒ‡ãƒ«
iso_forest = IsolationForest(
    contamination=0.1,  # æƒ³å®šç•°å¸¸ç‡
    random_state=42,
    n_estimators=100
)

# äºˆæ¸¬ï¼ˆ-1: ç•°å¸¸, 1: æ­£å¸¸ï¼‰
predictions = iso_forest.fit_predict(X_scaled)
anomaly_scores = iso_forest.score_samples(X_scaled)

# äºˆæ¸¬ãƒ©ãƒ™ãƒ«ã‚’0/1ã«å¤‰æ›
pred_labels = (predictions == -1).astype(int)

# è©•ä¾¡ï¼ˆçœŸã®ãƒ©ãƒ™ãƒ«ãŒã‚ã‚‹å ´åˆï¼‰
from sklearn.metrics import classification_report, confusion_matrix

print("\n" + "="*60)
print("Anomaly Detection Results:")
print("="*60)
print(classification_report(true_labels, pred_labels,
                          target_names=['Normal', 'Anomaly']))
print("="*60)

# PCAã§2æ¬¡å…ƒã«åœ§ç¸®ï¼ˆå¯è¦–åŒ–ç”¨ï¼‰
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_scaled)

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# å·¦ä¸Š: PCAç©ºé–“ã§ã®ç•°å¸¸æ¤œçŸ¥çµæœ
scatter1 = axes[0, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=pred_labels,
                             cmap='RdYlGn_r', s=80, alpha=0.7,
                             edgecolors='black', linewidth=1)
axes[0, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)',
                     fontsize=11)
axes[0, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)',
                     fontsize=11)
axes[0, 0].set_title('Anomaly Detection (Predicted)', fontsize=13, fontweight='bold')
cbar1 = plt.colorbar(scatter1, ax=axes[0, 0])
cbar1.set_label('0: Normal, 1: Anomaly', fontsize=10)
axes[0, 0].grid(alpha=0.3)

# å³ä¸Š: çœŸã®ãƒ©ãƒ™ãƒ«ï¼ˆæ¯”è¼ƒç”¨ï¼‰
scatter2 = axes[0, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=true_labels,
                             cmap='RdYlGn_r', s=80, alpha=0.7,
                             edgecolors='black', linewidth=1)
axes[0, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)',
                     fontsize=11)
axes[0, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)',
                     fontsize=11)
axes[0, 1].set_title('Ground Truth Labels', fontsize=13, fontweight='bold')
cbar2 = plt.colorbar(scatter2, ax=axes[0, 1])
cbar2.set_label('0: Normal, 1: Anomaly', fontsize=10)
axes[0, 1].grid(alpha=0.3)

# å·¦ä¸‹: ç•°å¸¸ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
axes[1, 0].hist(anomaly_scores[true_labels == 0], bins=30, alpha=0.6,
               label='Normal', color='green', edgecolor='black')
axes[1, 0].hist(anomaly_scores[true_labels == 1], bins=30, alpha=0.6,
               label='Anomaly', color='red', edgecolor='black')
axes[1, 0].set_xlabel('Anomaly Score (lower = more anomalous)', fontsize=12)
axes[1, 0].set_ylabel('Count', fontsize=12)
axes[1, 0].set_title('Anomaly Score Distribution', fontsize=13, fontweight='bold')
axes[1, 0].legend(fontsize=11)
axes[1, 0].grid(alpha=0.3)

# å³ä¸‹: æ··åŒè¡Œåˆ—
cm = confusion_matrix(true_labels, pred_labels)
import seaborn as sns
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[1, 1],
           xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])
axes[1, 1].set_xlabel('Predicted', fontsize=12)
axes[1, 1].set_ylabel('Actual', fontsize=12)
axes[1, 1].set_title('Confusion Matrix', fontsize=13, fontweight='bold')

plt.tight_layout()
plt.show()

# æœ€ã‚‚ç•°å¸¸ãªã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒªã‚¹ãƒˆ
top_anomalies = np.argsort(anomaly_scores)[:5]
print("\nTop 5 Most Anomalous Samples:")
print(data.iloc[top_anomalies])
print("\nAnomaly Scores:")
print(anomaly_scores[top_anomalies])
</code></pre>

        <h2>5.6 è‡ªå‹•ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ</h2>

        <h3>5.6.1 æ—¥æ¬¡/é€±æ¬¡ãƒ—ãƒ­ã‚»ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã®è‡ªå‹•åŒ–</h3>
        <p>è§£æçµæœã‚’è‡ªå‹•çš„ã«PDFãƒ¬ãƒãƒ¼ãƒˆã‚„HTMLãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«å‡ºåŠ›ã—ã¾ã™ã€‚</p>

        <h4>ã‚³ãƒ¼ãƒ‰ä¾‹5-7: å®Œå…¨çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆãƒ‡ãƒ¼ã‚¿ â†’ è§£æ â†’ ãƒ¬ãƒãƒ¼ãƒˆï¼‰</h4>
        <pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from matplotlib.backends.backend_pdf import PdfPages
import warnings
warnings.filterwarnings('ignore')

class ProcessReportGenerator:
    """
    ãƒ—ãƒ­ã‚»ã‚¹è§£æã®è‡ªå‹•ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¯ãƒ©ã‚¹
    """

    def __init__(self, data, report_title="Process Analysis Report"):
        """
        Parameters
        ----------
        data : pd.DataFrame
            ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿
        report_title : str
            ãƒ¬ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒˆãƒ«
        """
        self.data = data
        self.report_title = report_title
        self.timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    def generate_summary_statistics(self):
        """çµ±è¨ˆã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ"""
        summary = self.data.describe().T
        summary['missing'] = self.data.isnull().sum()
        summary['missing_pct'] = (summary['missing'] / len(self.data) * 100).round(2)

        return summary

    def plot_time_series(self, ax, column):
        """æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆ"""
        if 'timestamp' in self.data.columns:
            x = self.data['timestamp']
        else:
            x = np.arange(len(self.data))

        ax.plot(x, self.data[column], 'b-', linewidth=1.5, alpha=0.7)

        # ç®¡ç†é™ç•Œï¼ˆÂ±3Ïƒï¼‰
        mean = self.data[column].mean()
        std = self.data[column].std()
        ucl = mean + 3*std
        lcl = mean - 3*std

        ax.axhline(mean, color='green', linestyle='-', linewidth=2, label='Mean')
        ax.axhline(ucl, color='red', linestyle='--', linewidth=2, label='UCL (Â±3Ïƒ)')
        ax.axhline(lcl, color='red', linestyle='--', linewidth=2, label='LCL')

        # ç®¡ç†é™ç•Œå¤–ã®ç‚¹ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
        out_of_control = (self.data[column] > ucl) | (self.data[column] < lcl)
        if out_of_control.any():
            ax.scatter(np.where(out_of_control)[0], self.data.loc[out_of_control, column],
                      color='red', s=100, marker='x', linewidths=3, zorder=5,
                      label='Out of Control')

        ax.set_xlabel('Sample Index', fontsize=10)
        ax.set_ylabel(column, fontsize=10)
        ax.set_title(f'Time Series: {column}', fontsize=12, fontweight='bold')
        ax.legend(fontsize=9)
        ax.grid(alpha=0.3)

    def plot_distribution(self, ax, column, bins=30):
        """åˆ†å¸ƒãƒ—ãƒ­ãƒƒãƒˆ"""
        ax.hist(self.data[column], bins=bins, alpha=0.7, color='skyblue',
               edgecolor='black', density=True)

        # æ­£è¦åˆ†å¸ƒãƒ•ã‚£ãƒƒãƒˆ
        from scipy import stats
        mu = self.data[column].mean()
        sigma = self.data[column].std()
        x_range = np.linspace(self.data[column].min(), self.data[column].max(), 100)
        ax.plot(x_range, stats.norm.pdf(x_range, mu, sigma), 'r-', linewidth=2,
               label=f'Normal Fit\nÎ¼={mu:.2f}, Ïƒ={sigma:.2f}')

        ax.set_xlabel(column, fontsize=10)
        ax.set_ylabel('Density', fontsize=10)
        ax.set_title(f'Distribution: {column}', fontsize=12, fontweight='bold')
        ax.legend(fontsize=9)
        ax.grid(alpha=0.3)

    def plot_correlation_matrix(self, ax):
        """ç›¸é–¢è¡Œåˆ—ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—"""
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        corr = self.data[numeric_cols].corr()

        import seaborn as sns
        sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0,
                   square=True, linewidths=1, cbar_kws={"shrink": 0.8}, ax=ax)
        ax.set_title('Correlation Matrix', fontsize=12, fontweight='bold')

    def generate_pdf_report(self, filename='process_report.pdf'):
        """
        PDFãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ

        Parameters
        ----------
        filename : str
            å‡ºåŠ›PDFãƒ•ã‚¡ã‚¤ãƒ«å
        """
        with PdfPages(filename) as pdf:
            # ãƒšãƒ¼ã‚¸1: ã‚¿ã‚¤ãƒˆãƒ«ã¨çµ±è¨ˆã‚µãƒãƒªãƒ¼
            fig = plt.figure(figsize=(11, 8.5))
            fig.suptitle(self.report_title, fontsize=18, fontweight='bold', y=0.98)

            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
            fig.text(0.5, 0.94, f'Generated: {self.timestamp}', ha='center',
                    fontsize=10, style='italic')

            # çµ±è¨ˆã‚µãƒãƒªãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«
            ax_table = fig.add_subplot(111)
            ax_table.axis('off')

            summary = self.generate_summary_statistics()
            summary_display = summary[['mean', 'std', 'min', 'max', 'missing_pct']]
            summary_display.columns = ['Mean', 'Std', 'Min', 'Max', 'Missing%']

            table = ax_table.table(cellText=summary_display.round(2).values,
                                  rowLabels=summary_display.index,
                                  colLabels=summary_display.columns,
                                  cellLoc='center', rowLoc='center',
                                  loc='center', bbox=[0.1, 0.3, 0.8, 0.6])
            table.auto_set_font_size(False)
            table.set_fontsize(9)
            table.scale(1, 2)

            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®è£…é£¾
            for i in range(len(summary_display.columns)):
                table[(0, i)].set_facecolor('#4CAF50')
                table[(0, i)].set_text_props(weight='bold', color='white')

            pdf.savefig(fig, bbox_inches='tight')
            plt.close()

            # ãƒšãƒ¼ã‚¸2-N: å„å¤‰æ•°ã®æ™‚ç³»åˆ—ã¨åˆ†å¸ƒ
            numeric_cols = self.data.select_dtypes(include=[np.number]).columns

            for col in numeric_cols:
                fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(11, 8.5))

                self.plot_time_series(ax1, col)
                self.plot_distribution(ax2, col)

                plt.tight_layout()
                pdf.savefig(fig, bbox_inches='tight')
                plt.close()

            # æœ€çµ‚ãƒšãƒ¼ã‚¸: ç›¸é–¢è¡Œåˆ—
            fig, ax = plt.subplots(figsize=(11, 8.5))
            self.plot_correlation_matrix(ax)

            plt.tight_layout()
            pdf.savefig(fig, bbox_inches='tight')
            plt.close()

        print(f"PDF report generated: {filename}")

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    n_samples = 100

    data = pd.DataFrame({
        'timestamp': pd.date_range('2025-01-01', periods=n_samples, freq='h'),
        'Temperature': np.random.normal(400, 10, n_samples),
        'Pressure': np.random.normal(0.5, 0.05, n_samples),
        'Power': np.random.normal(250, 20, n_samples),
        'Thickness': np.random.normal(100, 5, n_samples),
        'Uniformity': np.random.normal(95, 2, n_samples)
    })

    # ä¸€éƒ¨ã«ç•°å¸¸ã‚’æŒ¿å…¥
    data.loc[50:55, 'Temperature'] = np.random.normal(430, 5, 6)
    data.loc[50:55, 'Thickness'] = np.random.normal(110, 3, 6)

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report_gen = ProcessReportGenerator(data, report_title="Weekly Process Analysis Report")

    # çµ±è¨ˆã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print("Statistical Summary:")
    print(report_gen.generate_summary_statistics())

    # PDFãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report_gen.generate_pdf_report('process_weekly_report.pdf')

    print("\nReport generation complete!")
    print("  - PDF: process_weekly_report.pdf")
    print("  - Contains: Time series, distributions, correlation matrix")
</code></pre>

        <div class="mermaid">
flowchart TD
    A[Raw Process Data<br/>CSV/Excel/JSON] --> B[Data Loading<br/>ProcessDataLoader]
    B --> C[Preprocessing<br/>Clean & Standardize]
    C --> D[SPC Analysis<br/>Control Charts]
    C --> E[DOE/RSM<br/>Optimization]
    C --> F[ML Prediction<br/>Quality Forecast]
    C --> G[Anomaly Detection<br/>Isolation Forest]
    D --> H[Report Generation<br/>PDF/HTML]
    E --> H
    F --> H
    G --> H
    H --> I[Automated Report<br/>Daily/Weekly]

    style A fill:#99ccff,stroke:#0066cc
    style H fill:#f5576c,stroke:#f093fb,stroke-width:2px,color:#fff
    style I fill:#f093fb,stroke:#f5576c,stroke-width:2px,color:#fff
        </div>

        <h2>5.7 æ¼”ç¿’å•é¡Œ</h2>

        <h3>æ¼”ç¿’5-1: Cp/Cpkè¨ˆç®—ï¼ˆæ˜“ï¼‰</h3>
        <p><strong>å•é¡Œ</strong>ï¼šè†œåšãƒ‡ãƒ¼ã‚¿ãŒå¹³å‡100 nmã€æ¨™æº–åå·®3 nmã€è¦æ ¼ãŒ95-105 nmã®ã¨ãã€Cpã¨Cpkã‚’è¨ˆç®—ã›ã‚ˆã€‚ãƒ—ãƒ­ã‚»ã‚¹ã¯é©åˆ‡ã‹ï¼Ÿ</p>

        <details>
            <summary><strong>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</strong></summary>
            <pre><code class="language-python">import numpy as np

mu = 100  # [nm]
sigma = 3  # [nm]
USL = 105  # [nm]
LSL = 95   # [nm]

# Cp
Cp = (USL - LSL) / (6 * sigma)

# Cpk
Cpk_upper = (USL - mu) / (3 * sigma)
Cpk_lower = (mu - LSL) / (3 * sigma)
Cpk = min(Cpk_upper, Cpk_lower)

print(f"Cp = {Cp:.3f}")
print(f"Cpk = {Cpk:.3f}")

if Cpk >= 1.33:
    print("ãƒ—ãƒ­ã‚»ã‚¹èƒ½åŠ›: å„ªç§€")
elif Cpk >= 1.00:
    print("ãƒ—ãƒ­ã‚»ã‚¹èƒ½åŠ›: ååˆ†")
else:
    print("ãƒ—ãƒ­ã‚»ã‚¹èƒ½åŠ›: ä¸ååˆ†ï¼ˆæ”¹å–„å¿…è¦ï¼‰")

# ä¸è‰¯ç‡æ¨å®š
from scipy import stats
ppm = (stats.norm.cdf((LSL - mu) / sigma) +
       (1 - stats.norm.cdf((USL - mu) / sigma))) * 1e6
print(f"æ¨å®šä¸è‰¯ç‡: {ppm:.1f} ppm")
</code></pre>
        </details>

        <h3>æ¼”ç¿’5-2: 2å› å­å®Ÿé¨“è¨ˆç”»ï¼ˆä¸­ï¼‰</h3>
        <p><strong>å•é¡Œ</strong>ï¼šæ¸©åº¦ï¼ˆ300, 400, 500Â°Cï¼‰ã¨åœ§åŠ›ï¼ˆ0.3, 0.5, 0.7 Paï¼‰ã®2å› å­3æ°´æº–å®Ÿé¨“ã‚’è¨­è¨ˆã—ã€å…¨çµ„ã¿åˆã‚ã›ã®å®Ÿé¨“è¨ˆç”»è¡¨ã‚’ä½œæˆã›ã‚ˆã€‚</p>

        <details>
            <summary><strong>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</strong></summary>
            <pre><code class="language-python">import pandas as pd
from itertools import product

# å› å­ã¨æ°´æº–
factors = {
    'Temperature': [300, 400, 500],
    'Pressure': [0.3, 0.5, 0.7]
}

# å…¨çµ„ã¿åˆã‚ã›ç”Ÿæˆ
combinations = list(product(factors['Temperature'], factors['Pressure']))
design = pd.DataFrame(combinations, columns=['Temperature', 'Pressure'])

print("Experimental Design (Full Factorial):")
print(design)
print(f"\nTotal experiments: {len(design)}")
</code></pre>
        </details>

        <h3>æ¼”ç¿’5-3: ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆç‰¹å¾´é‡é‡è¦åº¦ï¼ˆä¸­ï¼‰</h3>
        <p><strong>å•é¡Œ</strong>ï¼š5ã¤ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæ¸©åº¦ã€åœ§åŠ›ã€ãƒ‘ãƒ¯ãƒ¼ã€æµé‡ã€æ™‚é–“ï¼‰ã‹ã‚‰è†œåšã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã€æœ€ã‚‚é‡è¦ãªå› å­ã‚’ç‰¹å®šã›ã‚ˆã€‚</p>

        <details>
            <summary><strong>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</strong></summary>
            <p><strong>ã‚³ãƒ¼ãƒ‰ä¾‹5-4ã®ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«</strong>ã‚’å®Ÿè¡Œã—ã€ç‰¹å¾´é‡é‡è¦åº¦ã‚’ç¢ºèªï¼š</p>
            <pre><code class="language-python"># feature_importanceã‹ã‚‰é‡è¦åº¦ã‚’ç¢ºèª
print(feature_importance)

# å…¸å‹çš„ãªçµæœ:
#   Time: 0.35 (æœ€é‡è¦: æˆè†œæ™‚é–“ãŒé•·ã„ã»ã©åšã„)
#   Temperature: 0.25 (æˆé•·é€Ÿåº¦ã«å½±éŸ¿)
#   Power: 0.20 (ã‚¹ãƒ‘ãƒƒã‚¿åç‡ã«å½±éŸ¿)
#   Pressure: 0.15 (ã‚¬ã‚¹æ•£ä¹±ã«å½±éŸ¿)
#   Flow_Rate: 0.05 (é–“æ¥çš„å½±éŸ¿)
</code></pre>
        </details>

        <h3>æ¼”ç¿’5-4: ç•°å¸¸æ¤œçŸ¥ã®é–¾å€¤è¨­å®šï¼ˆä¸­ï¼‰</h3>
        <p><strong>å•é¡Œ</strong>ï¼šIsolation Forestã®`contamination`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’0.05, 0.1, 0.2ã§å¤‰ãˆãŸå ´åˆã€æ¤œå‡ºã•ã‚Œã‚‹ç•°å¸¸æ•°ã¯ã©ã†å¤‰åŒ–ã™ã‚‹ã‹ï¼Ÿ</p>

        <details>
            <summary><strong>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</strong></summary>
            <pre><code class="language-python">from sklearn.ensemble import IsolationForest
import numpy as np

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
data = np.random.normal(0, 1, (100, 4))

contaminations = [0.05, 0.1, 0.2]

for cont in contaminations:
    iso_forest = IsolationForest(contamination=cont, random_state=42)
    predictions = iso_forest.fit_predict(data)
    n_anomalies = (predictions == -1).sum()

    print(f"Contamination = {cont}: {n_anomalies} anomalies detected ({n_anomalies/len(data)*100:.1f}%)")

# å‡ºåŠ›ä¾‹:
# Contamination = 0.05: 5 anomalies (5.0%)
# Contamination = 0.1: 10 anomalies (10.0%)
# Contamination = 0.2: 20 anomalies (20.0%)

print("\nè§£é‡ˆ: contaminationã¯æƒ³å®šç•°å¸¸ç‡ã€‚é«˜ã„ã»ã©å¤šãæ¤œå‡ºï¼ˆå½é™½æ€§ãƒªã‚¹ã‚¯å¢—ï¼‰")
</code></pre>
        </details>

        <h3>æ¼”ç¿’5-5: å¿œç­”æ›²é¢æ³•ã®æœ€é©åŒ–ï¼ˆé›£ï¼‰</h3>
        <p><strong>å•é¡Œ</strong>ï¼šæ¸©åº¦ã¨åœ§åŠ›ã®2å› å­ã§è†œåšã‚’æœ€å¤§åŒ–ã—ãŸã„ã€‚å¿œç­”æ›²é¢ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æœ€é©æ¡ä»¶ï¼ˆæ¸©åº¦ã€åœ§åŠ›ï¼‰ã‚’æ•°å€¤çš„ã«æ±‚ã‚ã‚ˆã€‚</p>

        <details>
            <summary><strong>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</strong></summary>
            <pre><code class="language-python">from scipy.optimize import minimize

# ã‚³ãƒ¼ãƒ‰ä¾‹5-3ã®ãƒ¢ãƒ‡ãƒ«ã¨polyã‚’ä½¿ç”¨
def objective(x):
    """æœ€å°åŒ–ç›®æ¨™é–¢æ•°ï¼ˆæœ€å¤§åŒ–ã®ãŸã‚è² å·ï¼‰"""
    X_input = np.array(x).reshape(1, -1)
    X_poly = poly.transform(X_input)
    y_pred = model.predict(X_poly)
    return -y_pred[0]  # æœ€å¤§åŒ–ã®ãŸã‚è² å·

# åˆæœŸå€¤ã¨å¢ƒç•Œ
x0 = [400, 0.5]  # [Temperature, Pressure]
bounds = [(300, 500), (0.2, 0.8)]

# æœ€é©åŒ–
result = minimize(objective, x0, bounds=bounds, method='L-BFGS-B')

T_opt = result.x[0]
P_opt = result.x[1]
y_opt = -result.fun

print(f"Optimal conditions:")
print(f"  Temperature: {T_opt:.1f} Â°C")
print(f"  Pressure: {P_opt:.3f} Pa")
print(f"  Predicted maximum response: {y_opt:.2f}")
</code></pre>
        </details>

        <h3>æ¼”ç¿’5-6: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®å½±éŸ¿ï¼ˆé›£ï¼‰</h3>
        <p><strong>å•é¡Œ</strong>ï¼šæ¬ æå€¤ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€(a)æ¬ æè¡Œå‰Šé™¤ã€(b)å¹³å‡å€¤è£œå®Œã€(c)KNNè£œå®Œã®3æ‰‹æ³•ã‚’æ¯”è¼ƒã—ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã¸ã®å½±éŸ¿ã‚’è©•ä¾¡ã›ã‚ˆã€‚</p>

        <details>
            <summary><strong>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</strong></summary>
            <pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¬ æå€¤ã‚ã‚Šï¼‰
np.random.seed(42)
n = 200
data = pd.DataFrame({
    'X1': np.random.normal(0, 1, n),
    'X2': np.random.normal(0, 1, n),
    'X3': np.random.normal(0, 1, n),
    'y': np.random.normal(0, 1, n)
})

# æ¬ æå€¤ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å°å…¥ï¼ˆ10%ï¼‰
for col in ['X1', 'X2', 'X3']:
    missing_idx = np.random.choice(n, size=int(n*0.1), replace=False)
    data.loc[missing_idx, col] = np.nan

print(f"Missing values: {data.isnull().sum().sum()}")

methods = {
    'Dropna': data.dropna(),
    'Mean Imputation': pd.DataFrame(
        SimpleImputer(strategy='mean').fit_transform(data),
        columns=data.columns
    ),
    'KNN Imputation': pd.DataFrame(
        KNNImputer(n_neighbors=5).fit_transform(data),
        columns=data.columns
    )
}

for method_name, df in methods.items():
    X = df.drop('y', axis=1)
    y = df['y']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = RandomForestRegressor(n_estimators=50, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    r2 = r2_score(y_test, y_pred)
    print(f"{method_name}: RÂ² = {r2:.3f}, n_samples = {len(df)}")

print("\nçµè«–: ãƒ‡ãƒ¼ã‚¿é‡ã¨ãƒ¢ãƒ‡ãƒ«ç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’è€ƒæ…®ã—ã¦é¸æŠ")
</code></pre>
        </details>

        <h3>æ¼”ç¿’5-7: SPCç®¡ç†é™ç•Œã®èª¿æ•´ï¼ˆé›£ï¼‰</h3>
        <p><strong>å•é¡Œ</strong>ï¼š3Ïƒç®¡ç†é™ç•Œï¼ˆ99.73%ï¼‰ã®ä»£ã‚ã‚Šã«2Ïƒï¼ˆ95.45%ï¼‰ã‚’ä½¿ã†å ´åˆã€å½è­¦å ±ç‡ã¨è¦‹é€ƒã—ç‡ã¯ã©ã†å¤‰åŒ–ã™ã‚‹ã‹ï¼Ÿã©ã¡ã‚‰ãŒé©åˆ‡ã‹è­°è«–ã›ã‚ˆã€‚</p>

        <details>
            <summary><strong>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</strong></summary>
            <p><strong>ç†è«–çš„æ¯”è¼ƒ</strong>ï¼š</p>
            <ul>
                <li><strong>3Ïƒç®¡ç†é™ç•Œ</strong>ï¼š
                    <ul>
                        <li>å½è­¦å ±ç‡ï¼ˆÎ±ï¼‰: 0.27%ï¼ˆæ­£å¸¸ãªã®ã«ç•°å¸¸ã¨åˆ¤å®šï¼‰</li>
                        <li>è¦‹é€ƒã—ç‡ï¼ˆÎ²ï¼‰: é«˜ã„ï¼ˆç•°å¸¸ãªã®ã«æ¤œå‡ºæ¼ã‚Œï¼‰</li>
                        <li>é©ç”¨: å®‰å®šãƒ—ãƒ­ã‚»ã‚¹ã€èª¿æ•´ã‚³ã‚¹ãƒˆãŒé«˜ã„å ´åˆ</li>
                    </ul>
                </li>
                <li><strong>2Ïƒç®¡ç†é™ç•Œ</strong>ï¼š
                    <ul>
                        <li>å½è­¦å ±ç‡ï¼ˆÎ±ï¼‰: 4.55%ï¼ˆå½è­¦å ±ãŒå¢—ãˆã‚‹ï¼‰</li>
                        <li>è¦‹é€ƒã—ç‡ï¼ˆÎ²ï¼‰: ä½ã„ï¼ˆæ—©æœŸæ¤œå‡ºå¯èƒ½ï¼‰</li>
                        <li>é©ç”¨: ä¸å®‰å®šãƒ—ãƒ­ã‚»ã‚¹ã€æ—©æœŸè­¦å‘ŠãŒé‡è¦ãªå ´åˆ</li>
                    </ul>
                </li>
            </ul>
            <p><strong>å®Ÿè·µçš„é¸æŠ</strong>ï¼š</p>
            <pre><code class="language-python"># å½è­¦å ±ç‡ã®è¨ˆç®—
from scipy import stats

alpha_3sigma = 2 * (1 - stats.norm.cdf(3))
alpha_2sigma = 2 * (1 - stats.norm.cdf(2))

print(f"3Ïƒç®¡ç†é™ç•Œ: å½è­¦å ±ç‡ = {alpha_3sigma*100:.2f}%")
print(f"2Ïƒç®¡ç†é™ç•Œ: å½è­¦å ±ç‡ = {alpha_2sigma*100:.2f}%")

print("\næ¨å¥¨:")
print("  - 3Ïƒ: æˆç†Ÿãƒ—ãƒ­ã‚»ã‚¹ã€èª¿æ•´ã‚³ã‚¹ãƒˆãŒé«˜ã„ï¼ˆåŠå°ä½“è£½é€ ãªã©ï¼‰")
print("  - 2Ïƒ: æ–°è¦ãƒ—ãƒ­ã‚»ã‚¹ã€å“è³ªé‡è¦–ã€æ—©æœŸä»‹å…¥ãŒå¿…è¦ï¼ˆåŒ»è–¬å“ãªã©ï¼‰")
</code></pre>
        </details>

        <h3>æ¼”ç¿’5-8: çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è¨­è¨ˆï¼ˆé›£ï¼‰</h3>
        <p><strong>å•é¡Œ</strong>ï¼šå®Ÿéš›ã®å·¥å ´ã§ã€ãƒ‡ãƒ¼ã‚¿å–å¾— â†’ SPCç›£è¦– â†’ ç•°å¸¸æ¤œçŸ¥ â†’ è‡ªå‹•ã‚¢ãƒ©ãƒ¼ãƒˆ â†’ é€±æ¬¡ãƒ¬ãƒãƒ¼ãƒˆã®å®Œå…¨è‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚’è¨­è¨ˆã›ã‚ˆã€‚å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã‚’ç¤ºã›ã€‚</p>

        <details>
            <summary><strong>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</strong></summary>
            <p><strong>ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</strong>ï¼š</p>
            <ol>
                <li><strong>ãƒ‡ãƒ¼ã‚¿å–å¾—å±¤</strong>ï¼š
                    <ul>
                        <li>è£…ç½®ã‹ã‚‰ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°åé›†ï¼ˆOPC UA, MQTTï¼‰</li>
                        <li>ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ ¼ç´ï¼ˆTimescaleDB, InfluxDBï¼‰</li>
                    </ul>
                </li>
                <li><strong>å‡¦ç†å±¤</strong>ï¼š
                    <ul>
                        <li>å®šæœŸå®Ÿè¡Œï¼ˆcron, Airflowï¼‰</li>
                        <li>SPCè§£æï¼ˆPython + pandasï¼‰</li>
                        <li>ç•°å¸¸æ¤œçŸ¥ï¼ˆIsolation Forestï¼‰</li>
                    </ul>
                </li>
                <li><strong>ã‚¢ãƒ©ãƒ¼ãƒˆå±¤</strong>ï¼š
                    <ul>
                        <li>ç®¡ç†é™ç•Œå¤–æ¤œå‡º â†’ ãƒ¡ãƒ¼ãƒ«/Slacké€šçŸ¥</li>
                        <li>ç•°å¸¸ã‚¹ã‚³ã‚¢é–¾å€¤è¶…é â†’ ç·Šæ€¥ã‚¢ãƒ©ãƒ¼ãƒˆ</li>
                    </ul>
                </li>
                <li><strong>ãƒ¬ãƒãƒ¼ãƒˆå±¤</strong>ï¼š
                    <ul>
                        <li>é€±æ¬¡ãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆï¼ˆPDFï¼‰</li>
                        <li>Webãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆDash, Streamlitï¼‰</li>
                    </ul>
                </li>
            </ol>
            <p><strong>å®Ÿè£…ä¾‹ï¼ˆæ“¬ä¼¼ã‚³ãƒ¼ãƒ‰ï¼‰</strong>ï¼š</p>
            <pre><code class="language-python"># daily_monitoring.py (cron: æ¯æ—¥1å›å®Ÿè¡Œ)
def daily_monitoring():
    # 1. ãƒ‡ãƒ¼ã‚¿å–å¾—
    data = load_data_from_database(start_date=yesterday, end_date=today)

    # 2. SPCè§£æ
    spc = SPCAnalyzer(data)
    out_of_control = spc.detect_out_of_control()

    # 3. ç•°å¸¸æ¤œçŸ¥
    anomalies = detect_anomalies(data)

    # 4. ã‚¢ãƒ©ãƒ¼ãƒˆ
    if out_of_control or anomalies:
        send_alert(subject="Process Anomaly Detected",
                  body=f"Out of control: {out_of_control}\nAnomalies: {anomalies}")

    # 5. ãƒ­ã‚°ä¿å­˜
    save_monitoring_log(data, spc_results, anomalies)

# weekly_report.py (cron: æ¯é€±æœˆæ›œå®Ÿè¡Œ)
def weekly_report():
    data = load_data_from_database(start_date=last_week, end_date=today)
    report_gen = ProcessReportGenerator(data)
    report_gen.generate_pdf_report('weekly_report.pdf')
    send_email(to='manager@example.com', attachment='weekly_report.pdf')
</code></pre>
        </details>

        <h2>5.8 å­¦ç¿’ã®ç¢ºèª</h2>

        <h3>åŸºæœ¬ç†è§£åº¦ãƒã‚§ãƒƒã‚¯</h3>
        <ol>
            <li>CSV, Excel, JSONãªã©å¤šæ§˜ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚ã¾ã™ã‹ï¼Ÿ</li>
            <li>X-barãƒãƒ£ãƒ¼ãƒˆã¨Rãƒãƒ£ãƒ¼ãƒˆã®æ„å‘³ã¨ä½¿ã„åˆ†ã‘ã‚’ç†è§£ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ</li>
            <li>Cp/Cpkã®é•ã„ã¨ã€ãƒ—ãƒ­ã‚»ã‚¹èƒ½åŠ›è©•ä¾¡ã®åŸºæº–ã‚’èª¬æ˜ã§ãã¾ã™ã‹ï¼Ÿ</li>
            <li>å…¨å› å­å®Ÿé¨“ã¨å¿œç­”æ›²é¢æ³•ã®é•ã„ã‚’ç†è§£ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ</li>
            <li>ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ç‰¹å¾´é‡é‡è¦åº¦ã®è§£é‡ˆãŒã§ãã¾ã™ã‹ï¼Ÿ</li>
            <li>Isolation Forestã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥ã®åŸç†ã‚’ç†è§£ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ</li>
        </ol>

        <h3>å®Ÿè·µã‚¹ã‚­ãƒ«ç¢ºèª</h3>
        <ol>
            <li>å®Ÿéš›ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰SPCãƒãƒ£ãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã€ç®¡ç†é™ç•Œå¤–ã®ç‚¹ã‚’ç‰¹å®šã§ãã¾ã™ã‹ï¼Ÿ</li>
            <li>2å› å­ä»¥ä¸Šã®å®Ÿé¨“è¨ˆç”»ã‚’è¨­è¨ˆã—ã€å¿œç­”æ›²é¢ãƒ¢ãƒ‡ãƒ«ã§æœ€é©åŒ–ã§ãã¾ã™ã‹ï¼Ÿ</li>
            <li>æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆå›å¸°ãƒ»åˆ†é¡ï¼‰ã§ãƒ—ãƒ­ã‚»ã‚¹å“è³ªã‚’äºˆæ¸¬ã§ãã¾ã™ã‹ï¼Ÿ</li>
            <li>ç•°å¸¸æ¤œçŸ¥ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ä¸è‰¯å“ã‚’æ—©æœŸç™ºè¦‹ã§ãã¾ã™ã‹ï¼Ÿ</li>
            <li>è§£æçµæœã‚’è‡ªå‹•çš„ã«PDFãƒ¬ãƒãƒ¼ãƒˆã«å‡ºåŠ›ã§ãã¾ã™ã‹ï¼Ÿ</li>
        </ol>

        <h3>å¿œç”¨åŠ›ç¢ºèª</h3>
        <ol>
            <li>å®Œå…¨çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆãƒ‡ãƒ¼ã‚¿ â†’ è§£æ â†’ æœ€é©åŒ– â†’ ãƒ¬ãƒãƒ¼ãƒˆï¼‰ã‚’è¨­è¨ˆãƒ»å®Ÿè£…ã§ãã¾ã™ã‹ï¼Ÿ</li>
            <li>å®Ÿéš›ã®å·¥å ´ãƒ‡ãƒ¼ã‚¿ã§ã€å“è³ªå•é¡Œã®æ ¹æœ¬åŸå› ã‚’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å®šã§ãã¾ã™ã‹ï¼Ÿ</li>
            <li>æ–°è¦ãƒ—ãƒ­ã‚»ã‚¹ã®ç«‹ã¡ä¸Šã’æ™‚ã«ã€DOEã¨MLã‚’çµ„ã¿åˆã‚ã›ãŸæœ€é©åŒ–æˆ¦ç•¥ã‚’ææ¡ˆã§ãã¾ã™ã‹ï¼Ÿ</li>
        </ol>

        <h2>5.9 å‚è€ƒæ–‡çŒ®</h2>
        <ol>
            <li>Montgomery, D.C. (2012). <em>Statistical Quality Control</em> (7th ed.). Wiley. pp. 156-234 (Control charts), pp. 289-345 (Process capability).</li>
            <li>Box, G.E.P., Hunter, J.S., Hunter, W.G. (2005). <em>Statistics for Experimenters: Design, Innovation, and Discovery</em> (2nd ed.). Wiley. pp. 123-189 (Factorial designs), pp. 289-345 (Response surface methods).</li>
            <li>James, G., Witten, D., Hastie, T., Tibshirani, R. (2021). <em>An Introduction to Statistical Learning with Applications in Python</em>. Springer. pp. 303-335 (Random forests), pp. 445-489 (Unsupervised learning).</li>
            <li>Pedregosa, F., et al. (2011). "Scikit-learn: Machine Learning in Python." <em>Journal of Machine Learning Research</em>, 12:2825-2830. - scikit-learn documentation.</li>
            <li>McKinney, W. (2017). <em>Python for Data Analysis</em> (2nd ed.). O'Reilly. pp. 89-156 (pandas basics), pp. 234-289 (Data cleaning and preparation).</li>
            <li>Liu, F.T., Ting, K.M., Zhou, Z.H. (2008). "Isolation Forest." <em>Proceedings of the 8th IEEE International Conference on Data Mining</em>, pp. 413-422. DOI: 10.1109/ICDM.2008.17</li>
            <li>Hunter, J.D. (2007). "Matplotlib: A 2D Graphics Environment." <em>Computing in Science & Engineering</em>, 9(3):90-95. DOI: 10.1109/MCSE.2007.55</li>
            <li>Waskom, M. (2021). "seaborn: statistical data visualization." <em>Journal of Open Source Software</em>, 6(60):3021. DOI: 10.21105/joss.03021</li>
        </ol>

        <h2>5.10 æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h2>
        <p>ã“ã®ç« ã§å­¦ã‚“ã ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿è§£æã®å®Ÿè·µã‚¹ã‚­ãƒ«ã¯ã€ææ–™ç§‘å­¦ç ”ç©¶ã¨ç”£æ¥­å¿œç”¨ã®ä¸¡æ–¹ã§å³åº§ã«æ´»ç”¨ã§ãã¾ã™ã€‚æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ï¼š</p>

        <ul>
            <li><strong>å®Ÿãƒ‡ãƒ¼ã‚¿ã¸ã®é©ç”¨</strong>ï¼šè‡ªèº«ã®ç ”ç©¶å®¤ã‚„å·¥å ´ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã§ã€å­¦ã‚“ã æ‰‹æ³•ã‚’å®Ÿè·µ</li>
            <li><strong>ç™ºå±•çš„æ©Ÿæ¢°å­¦ç¿’</strong>ï¼šæ·±å±¤å­¦ç¿’ï¼ˆLSTM, Transformerï¼‰ã«ã‚ˆã‚‹æ™‚ç³»åˆ—äºˆæ¸¬</li>
            <li><strong>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–</strong>ï¼šã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆApache Kafka, Flinkï¼‰ã®å°å…¥</li>
            <li><strong>çµ±åˆã‚·ã‚¹ãƒ†ãƒ é–‹ç™º</strong>ï¼šWebãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆDash, Streamlitï¼‰ã§ã®å¯è¦–åŒ–</li>
            <li><strong>è‡ªå‹•åŒ–ã®æ·±åŒ–</strong>ï¼šCI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ï¼ˆGitHub Actionsï¼‰ã§å®Œå…¨è‡ªå‹•é‹ç”¨</li>
        </ul>

        <p>ãƒ—ãƒ­ã‚»ã‚¹æŠ€è¡“å…¥é–€ã‚·ãƒªãƒ¼ã‚ºã‚’å®Œèµ°ã•ã‚ŒãŸã‚ãªãŸã¯ã€è–„è†œæˆé•·ãƒ»ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ãƒ»ãƒ‡ãƒ¼ã‚¿è§£æã®å…¨é ˜åŸŸã‚’çµ±åˆçš„ã«ç†è§£ã—ã€å®Ÿè·µã§ãã‚‹åŠ›ã‚’èº«ã«ã¤ã‘ã¾ã—ãŸã€‚ã“ã®çŸ¥è­˜ã‚’åŸºç›¤ã«ã€ã•ã‚‰ãªã‚‹å°‚é–€æ€§ã‚’æ·±ã‚ã€ææ–™ç§‘å­¦ã®æœ€å‰ç·šã§æ´»èºã•ã‚Œã‚‹ã“ã¨ã‚’æœŸå¾…ã—ã¾ã™ï¼</p>

        <div class="navigation">
            <span class="coming-soon">â† ç¬¬4ç« ï¼šè–„è†œæˆé•·ãƒ—ãƒ­ã‚»ã‚¹ï¼ˆæº–å‚™ä¸­ï¼‰</span>
            <a href="index.html" class="nav-button">ç›®æ¬¡ã«æˆ»ã‚‹</a>
        </div>
    </main>

    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€ï¼ˆæ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©ï¼‰ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿ï¼ˆAS ISï¼‰ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶ï¼ˆä¾‹: CC BY 4.0ï¼‰ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: MS Knowledge Hub Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-28</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>&copy; 2025 MS Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
