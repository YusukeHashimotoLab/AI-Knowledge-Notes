---
title: "ç¬¬5ç« : Pythonå®Ÿè·µï¼šåˆ†å…‰ãƒ‡ãƒ¼ã‚¿è§£æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼"
chapter_title: "ç¬¬5ç« : Pythonå®Ÿè·µï¼šåˆ†å…‰ãƒ‡ãƒ¼ã‚¿è§£æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼"
---

ğŸŒ JP | [ğŸ‡¬ğŸ‡§ EN](<../../../en/MS/spectroscopy-introduction/chapter-5.html>) | Last sync: 2025-11-16

[AIå¯ºå­å±‹ãƒˆãƒƒãƒ—](<../../index.html>)â€º[ææ–™ç§‘å­¦](<../../MS/index.html>)â€º[Spectroscopy](<../../MS/spectroscopy-introduction/index.html>)â€ºChapter 5

# ç¬¬5ç« : Pythonå®Ÿè·µï¼šåˆ†å…‰ãƒ‡ãƒ¼ã‚¿è§£æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

**ã“ã®ç« ã§å­¦ã¶ã“ã¨:** æœ¬ç« ã§ã¯ã€ç¬¬1ç« ã‹ã‚‰ç¬¬4ç« ã§å­¦ã‚“ã åˆ†å…‰åˆ†ææ³•ï¼ˆIRã€Ramanã€UV-Visã€XPSï¼‰ã‚’çµ±åˆã—ã€å®Ÿè·µçš„ãªPythonãƒ‡ãƒ¼ã‚¿è§£æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚æ±ç”¨ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã€è‡ªå‹•ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹åˆ†é¡ãƒ»å›å¸°ã€ãƒãƒƒãƒå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯è¦–åŒ–ã¾ã§ã€ç ”ç©¶ç¾å ´ã§å³åº§ã«æ´»ç”¨ã§ãã‚‹å®Ÿç”¨çš„ãªãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆã‚’é–‹ç™ºã—ã¾ã™ã€‚å…¨ã¦ã®ã‚³ãƒ¼ãƒ‰ã¯å†åˆ©ç”¨å¯èƒ½ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å½¢å¼ã§æä¾›ã•ã‚Œã€èª­è€…è‡ªèº«ã®ç ”ç©¶ãƒ‡ãƒ¼ã‚¿ã«å®¹æ˜“ã«é©ç”¨ã§ãã¾ã™ã€‚

## 5.1 çµ±åˆåˆ†å…‰ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®è¨­è¨ˆ

### 5.1.1 æ±ç”¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¯ãƒ©ã‚¹

åˆ†å…‰è£…ç½®ã‹ã‚‰å‡ºåŠ›ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿å½¢å¼ã¯å¤šæ§˜ã§ã™ï¼ˆCSVã€TXTã€è£…ç½®å›ºæœ‰ãƒã‚¤ãƒŠãƒªå½¢å¼ç­‰ï¼‰ã€‚æ±ç”¨çš„ãªãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚¯ãƒ©ã‚¹ã‚’è¨­è¨ˆã™ã‚‹ã“ã¨ã§ã€ç•°ãªã‚‹å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±ä¸€çš„ã«æ‰±ãˆã¾ã™ã€‚
    
    
    ```mermaid
    flowchart TD
            A[åˆ†å…‰ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«] --> B{ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼åˆ¤å®š}
            B -->|CSV| C[CSVãƒ‘ãƒ¼ã‚µãƒ¼]
            B -->|TXT| D[ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ãƒ¼ã‚µãƒ¼]
            B -->|Binary| E[ãƒã‚¤ãƒŠãƒªãƒ‘ãƒ¼ã‚µãƒ¼]
            C --> F[ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–]
            D --> F
            E --> F
            F --> G[SpectralDataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ]
            G --> H[è§£æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³]
    
            style A fill:#e3f2fd
            style B fill:#fff3e0
            style F fill:#e8f5e9
            style G fill:#fce4ec
            style H fill:#ffe0b2
        
    ã‚³ãƒ¼ãƒ‰ä¾‹1: æ±ç”¨ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚¯ãƒ©ã‚¹
    import numpy as np
    import pandas as pd
    from pathlib import Path
    from typing import Union, Tuple, Optional
    
    class SpectralData:
        """
        æ±ç”¨ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
    
        Attributes:
        -----------
        x : array
            æ¨ªè»¸ãƒ‡ãƒ¼ã‚¿ï¼ˆæ³¢é•·ã€æ³¢æ•°ã€çµåˆã‚¨ãƒãƒ«ã‚®ãƒ¼ç­‰ï¼‰
        y : array
            ç¸¦è»¸ãƒ‡ãƒ¼ã‚¿ï¼ˆå¼·åº¦ã€å¸å…‰åº¦ç­‰ï¼‰
        x_label : str
            æ¨ªè»¸ãƒ©ãƒ™ãƒ«
        y_label : str
            ç¸¦è»¸ãƒ©ãƒ™ãƒ«
        metadata : dict
            ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¸¬å®šæ—¥æ™‚ã€è£…ç½®ã€æ¡ä»¶ç­‰ï¼‰
        """
    
        def __init__(self, x: np.ndarray, y: np.ndarray,
                     x_label: str = "X", y_label: str = "Intensity",
                     metadata: Optional[dict] = None):
            """
            ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–
    
            Parameters:
            -----------
            x : array
                æ¨ªè»¸ãƒ‡ãƒ¼ã‚¿
            y : array
                ç¸¦è»¸ãƒ‡ãƒ¼ã‚¿
            x_label : str
                æ¨ªè»¸ãƒ©ãƒ™ãƒ«
            y_label : str
                ç¸¦è»¸ãƒ©ãƒ™ãƒ«
            metadata : dict, optional
                ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            """
            self.x = np.asarray(x)
            self.y = np.asarray(y)
            self.x_label = x_label
            self.y_label = y_label
            self.metadata = metadata or {}
    
            # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºæ¤œè¨¼
            if len(self.x) != len(self.y):
                raise ValueError(f"x and y must have the same length: {len(x)} != {len(y)}")
    
        def __repr__(self):
            return (f"SpectralData(n_points={len(self.x)}, "
                    f"x_range=[{self.x.min():.2f}, {self.x.max():.2f}], "
                    f"y_range=[{self.y.min():.2f}, {self.y.max():.2f}])")
    
        def copy(self):
            """ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ"""
            return SpectralData(self.x.copy(), self.y.copy(),
                               self.x_label, self.y_label,
                               self.metadata.copy())
    
        def trim(self, x_min: float, x_max: float):
            """
            æŒ‡å®šç¯„å›²ã§ãƒ‡ãƒ¼ã‚¿ã‚’åˆ‡ã‚Šå‡ºã—
    
            Parameters:
            -----------
            x_min, x_max : float
                åˆ‡ã‚Šå‡ºã—ç¯„å›²
            """
            mask = (self.x >= x_min) & (self.x <= x_max)
            self.x = self.x[mask]
            self.y = self.y[mask]
    
        def normalize(self, method: str = 'max'):
            """
            ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’æ­£è¦åŒ–
    
            Parameters:
            -----------
            method : str
                'max': æœ€å¤§å€¤ã§æ­£è¦åŒ–
                'minmax': 0-1ç¯„å›²ã«æ­£è¦åŒ–
                'area': é¢ç©ã§æ­£è¦åŒ–
            """
            if method == 'max':
                self.y = self.y / self.y.max()
            elif method == 'minmax':
                self.y = (self.y - self.y.min()) / (self.y.max() - self.y.min())
            elif method == 'area':
                area = np.trapz(self.y, self.x)
                self.y = self.y / area
            else:
                raise ValueError(f"Unknown normalization method: {method}")
    
    
    class SpectralDataLoader:
        """
        æ±ç”¨ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        """
    
        @staticmethod
        def load_csv(filepath: Union[str, Path], x_col: int = 0, y_col: int = 1,
                     delimiter: str = ',', skiprows: int = 0,
                     x_label: str = "X", y_label: str = "Intensity") -> SpectralData:
            """
            CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    
            Parameters:
            -----------
            filepath : str or Path
                ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            x_col : int
                æ¨ªè»¸ãƒ‡ãƒ¼ã‚¿ã®åˆ—ç•ªå·
            y_col : int
                ç¸¦è»¸ãƒ‡ãƒ¼ã‚¿ã®åˆ—ç•ªå·
            delimiter : str
                åŒºåˆ‡ã‚Šæ–‡å­—
            skiprows : int
                ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹è¡Œæ•°
            x_label, y_label : str
                è»¸ãƒ©ãƒ™ãƒ«
    
            Returns:
            --------
            data : SpectralData
                ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            """
            df = pd.read_csv(filepath, delimiter=delimiter, skiprows=skiprows, header=None)
            x = df.iloc[:, x_col].values
            y = df.iloc[:, y_col].values
    
            metadata = {
                'filename': Path(filepath).name,
                'source': 'CSV',
                'columns': f"x={x_col}, y={y_col}"
            }
    
            return SpectralData(x, y, x_label, y_label, metadata)
    
        @staticmethod
        def load_txt(filepath: Union[str, Path], x_col: int = 0, y_col: int = 1,
                     skiprows: int = 0, x_label: str = "X", y_label: str = "Intensity") -> SpectralData:
            """
            ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆç©ºç™½åŒºåˆ‡ã‚Šï¼‰
    
            Parameters:
            -----------
            filepath : str or Path
                ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            x_col, y_col : int
                æ¨ªè»¸ãƒ»ç¸¦è»¸ãƒ‡ãƒ¼ã‚¿ã®åˆ—ç•ªå·
            skiprows : int
                ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹è¡Œæ•°
            x_label, y_label : str
                è»¸ãƒ©ãƒ™ãƒ«
    
            Returns:
            --------
            data : SpectralData
                ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            """
            data_array = np.loadtxt(filepath, skiprows=skiprows)
            x = data_array[:, x_col]
            y = data_array[:, y_col]
    
            metadata = {
                'filename': Path(filepath).name,
                'source': 'TXT',
                'columns': f"x={x_col}, y={y_col}"
            }
    
            return SpectralData(x, y, x_label, y_label, metadata)
    
        @staticmethod
        def auto_load(filepath: Union[str, Path], **kwargs) -> SpectralData:
            """
            ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‹ã‚‰è‡ªå‹•çš„ã«é©åˆ‡ãªãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’é¸æŠ
    
            Parameters:
            -----------
            filepath : str or Path
                ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            **kwargs
                å„ãƒ­ãƒ¼ãƒ€ãƒ¼ã¸ã®è¿½åŠ å¼•æ•°
    
            Returns:
            --------
            data : SpectralData
                ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            """
            filepath = Path(filepath)
            ext = filepath.suffix.lower()
    
            if ext == '.csv':
                return SpectralDataLoader.load_csv(filepath, **kwargs)
            elif ext in ['.txt', '.dat']:
                return SpectralDataLoader.load_txt(filepath, **kwargs)
            else:
                raise ValueError(f"Unsupported file format: {ext}")
    
    
    # ä½¿ç”¨ä¾‹
    if __name__ == "__main__":
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆã¨ä¿å­˜
        x_sim = np.linspace(400, 700, 300)
        y_sim = 0.8 * np.exp(-((x_sim - 550)**2) / (2 * 40**2)) + np.random.normal(0, 0.02, len(x_sim))
    
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
            for xi, yi in zip(x_sim, y_sim):
                f.write(f"{xi},{yi}\n")
            temp_path = f.name
    
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        loader = SpectralDataLoader()
        spectrum = loader.auto_load(temp_path, x_label="Wavelength (nm)", y_label="Absorbance")
    
        print(spectrum)
        print(f"Metadata: {spectrum.metadata}")
    
        # ãƒ‡ãƒ¼ã‚¿æ“ä½œ
        spectrum_copy = spectrum.copy()
        spectrum_copy.trim(500, 600)
        spectrum_copy.normalize(method='max')
    
        print(f"Trimmed and normalized: {spectrum_copy}")
    
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        import os
        os.unlink(temp_path)
    
    5.2 è‡ªå‹•ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã¨ç‰¹å¾´æŠ½å‡º
    5.2.1 ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®çµ±åˆ
    ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‹ã‚‰ãƒ”ãƒ¼ã‚¯ã‚’è‡ªå‹•æ¤œå‡ºã™ã‚‹ã“ã¨ã¯ã€å¤šæ•°ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’åŠ¹ç‡çš„ã«è§£æã™ã‚‹ä¸Šã§é‡è¦ã§ã™ã€‚scipy.signal.find_peaksã‚’æ‹¡å¼µã—ã€åˆ†å…‰åˆ†æã«ç‰¹åŒ–ã—ãŸãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚¯ãƒ©ã‚¹ã‚’å®Ÿè£…ã—ã¾ã™ã€‚
    ã‚³ãƒ¼ãƒ‰ä¾‹2: é«˜åº¦ãªè‡ªå‹•ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 
    import numpy as np
    import matplotlib.pyplot as plt
    from scipy.signal import find_peaks, peak_widths, peak_prominences
    from scipy.optimize import curve_fit
    from dataclasses import dataclass
    from typing import List, Tuple
    
    @dataclass
    class Peak:
        """
        ãƒ”ãƒ¼ã‚¯æƒ…å ±ã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
    
        Attributes:
        -----------
        position : float
            ãƒ”ãƒ¼ã‚¯ä½ç½®ï¼ˆæ¨ªè»¸åº§æ¨™ï¼‰
        height : float
            ãƒ”ãƒ¼ã‚¯é«˜ã•
        width : float
            åŠå€¤å…¨å¹…ï¼ˆFWHMï¼‰
        area : float
            ãƒ”ãƒ¼ã‚¯é¢ç©
        prominence : float
            ãƒ”ãƒ¼ã‚¯ã®å“è¶Šåº¦
        """
        position: float
        height: float
        width: float
        area: float
        prominence: float
    
        def __repr__(self):
            return (f"Peak(pos={self.position:.2f}, height={self.height:.2f}, "
                    f"FWHM={self.width:.2f}, area={self.area:.1f})")
    
    
    class PeakDetector:
        """
        ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ”ãƒ¼ã‚¯ã‚’è‡ªå‹•æ¤œå‡ºã™ã‚‹ã‚¯ãƒ©ã‚¹
        """
    
        def __init__(self, spectral_data: SpectralData):
            """
            Parameters:
            -----------
            spectral_data : SpectralData
                ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            """
            self.data = spectral_data
            self.peaks: List[Peak] = []
    
        def detect_peaks(self, height: float = None, prominence: float = None,
                        distance: int = None, width: Tuple[float, float] = None) -> List[Peak]:
            """
            ãƒ”ãƒ¼ã‚¯ã‚’æ¤œå‡º
    
            Parameters:
            -----------
            height : float, optional
                æœ€å°ãƒ”ãƒ¼ã‚¯é«˜ã•
            prominence : float, optional
                æœ€å°å“è¶Šåº¦
            distance : int, optional
                ãƒ”ãƒ¼ã‚¯é–“ã®æœ€å°è·é›¢ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°ï¼‰
            width : tuple, optional
                ãƒ”ãƒ¼ã‚¯å¹…ã®ç¯„å›² (min_width, max_width)
    
            Returns:
            --------
            peaks : list of Peak
                æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ã®ãƒªã‚¹ãƒˆ
            """
            # ãƒ”ãƒ¼ã‚¯æ¤œå‡º
            peak_indices, properties = find_peaks(self.data.y,
                                                  height=height,
                                                  prominence=prominence,
                                                  distance=distance,
                                                  width=width)
    
            # ãƒ”ãƒ¼ã‚¯å¹…ã®è¨ˆç®—ï¼ˆFWHMï¼‰
            widths, width_heights, left_ips, right_ips = peak_widths(
                self.data.y, peak_indices, rel_height=0.5
            )
    
            # ãƒ”ãƒ¼ã‚¯å“è¶Šåº¦
            prominences, _, _ = peak_prominences(self.data.y, peak_indices)
    
            # Peakã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆä½œæˆ
            self.peaks = []
            for i, idx in enumerate(peak_indices):
                # ãƒ”ãƒ¼ã‚¯ä½ç½®ï¼ˆæ¨ªè»¸åº§æ¨™ï¼‰
                position = self.data.x[idx]
    
                # ãƒ”ãƒ¼ã‚¯é«˜ã•
                height_val = self.data.y[idx]
    
                # FWHMï¼ˆãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆå˜ä½ã‹ã‚‰æ¨ªè»¸å˜ä½ã«å¤‰æ›ï¼‰
                dx = np.mean(np.diff(self.data.x))
                fwhm = widths[i] * dx
    
                # ãƒ”ãƒ¼ã‚¯é¢ç©ã®è¿‘ä¼¼ï¼ˆã‚¬ã‚¦ã‚¹è¿‘ä¼¼: é¢ç© â‰ˆ é«˜ã• Ã— FWHM Ã— sqrt(Ï€/(4ln2))ï¼‰
                area = height_val * fwhm * np.sqrt(np.pi / (4 * np.log(2)))
    
                peak = Peak(
                    position=position,
                    height=height_val,
                    width=fwhm,
                    area=area,
                    prominence=prominences[i]
                )
                self.peaks.append(peak)
    
            return self.peaks
    
        def fit_peaks_gaussian(self, peak_region_width: float = 5.0) -> List[dict]:
            """
            å„ãƒ”ãƒ¼ã‚¯ã‚’ã‚¬ã‚¦ã‚¹é–¢æ•°ã§ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
    
            Parameters:
            -----------
            peak_region_width : float
                å„ãƒ”ãƒ¼ã‚¯ã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°é ˜åŸŸå¹…ï¼ˆä¸¡å´ï¼‰
    
            Returns:
            --------
            fit_results : list of dict
                å„ãƒ”ãƒ¼ã‚¯ã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœ
            """
            fit_results = []
    
            for peak in self.peaks:
                # ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°é ˜åŸŸã®æŠ½å‡º
                mask = (self.data.x >= peak.position - peak_region_width) & \
                       (self.data.x <= peak.position + peak_region_width)
                x_fit = self.data.x[mask]
                y_fit = self.data.y[mask]
    
                if len(x_fit) < 5:
                    continue
    
                # ã‚¬ã‚¦ã‚¹é–¢æ•°ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
                def gaussian(x, A, mu, sigma):
                    return A * np.exp(-((x - mu)**2) / (2 * sigma**2))
    
                try:
                    # åˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                    p0 = [peak.height, peak.position, peak.width / (2 * np.sqrt(2 * np.log(2)))]
                    popt, pcov = curve_fit(gaussian, x_fit, y_fit, p0=p0, maxfev=5000)
    
                    A_fit, mu_fit, sigma_fit = popt
                    fwhm_fit = 2 * np.sqrt(2 * np.log(2)) * sigma_fit
                    area_fit = A_fit * sigma_fit * np.sqrt(2 * np.pi)
    
                    fit_results.append({
                        'peak_position': mu_fit,
                        'amplitude': A_fit,
                        'fwhm': fwhm_fit,
                        'area': area_fit,
                        'fit_quality': 'success'
                    })
                except RuntimeError:
                    fit_results.append({
                        'peak_position': peak.position,
                        'fit_quality': 'failed'
                    })
    
            return fit_results
    
        def plot_detected_peaks(self, show_labels: bool = True):
            """
            æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ã‚’å¯è¦–åŒ–
    
            Parameters:
            -----------
            show_labels : bool
                ãƒ”ãƒ¼ã‚¯ä½ç½®ã®ãƒ©ãƒ™ãƒ«ã‚’è¡¨ç¤ºã™ã‚‹ã‹
            """
            fig, ax = plt.subplots(figsize=(12, 6))
    
            # ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
            ax.plot(self.data.x, self.data.y, 'b-', linewidth=1.5, label='Spectrum', alpha=0.7)
    
            # ãƒ”ãƒ¼ã‚¯ä½ç½®ã‚’ãƒãƒ¼ã‚¯
            if self.peaks:
                peak_positions = [p.position for p in self.peaks]
                peak_heights = [p.height for p in self.peaks]
    
                ax.plot(peak_positions, peak_heights, 'ro', markersize=10,
                       label=f'Detected Peaks (n={len(self.peaks)})', zorder=5)
    
                # ãƒ”ãƒ¼ã‚¯ãƒ©ãƒ™ãƒ«
                if show_labels:
                    for peak in self.peaks:
                        ax.annotate(f'{peak.position:.1f}',
                                   xy=(peak.position, peak.height),
                                   xytext=(0, 10), textcoords='offset points',
                                   ha='center', fontsize=9, fontweight='bold',
                                   bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))
    
            ax.set_xlabel(self.data.x_label, fontsize=12)
            ax.set_ylabel(self.data.y_label, fontsize=12)
            ax.set_title('è‡ªå‹•ãƒ”ãƒ¼ã‚¯æ¤œå‡ºçµæœ', fontsize=14, fontweight='bold')
            ax.legend()
            ax.grid(alpha=0.3)
            plt.tight_layout()
            plt.show()
    
    
    # ä½¿ç”¨ä¾‹
    if __name__ == "__main__":
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«ï¼ˆ3ã¤ã®ãƒ”ãƒ¼ã‚¯ï¼‰
        x = np.linspace(400, 700, 600)
        y = (0.8 * np.exp(-((x - 450)**2) / (2 * 30**2)) +
             0.6 * np.exp(-((x - 550)**2) / (2 * 40**2)) +
             0.9 * np.exp(-((x - 620)**2) / (2  25**2)) +
             np.random.normal(0, 0.02, len(x)))
    
        spectrum = SpectralData(x, y, x_label="Wavelength (nm)", y_label="Absorbance")
    
        # ãƒ”ãƒ¼ã‚¯æ¤œå‡º
        detector = PeakDetector(spectrum)
        peaks = detector.detect_peaks(height=0.2, prominence=0.15, distance=50)
    
        print(f"æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯æ•°: {len(peaks)}")
        for i, peak in enumerate(peaks, 1):
            print(f"  Peak {i}: {peak}")
    
        # ã‚¬ã‚¦ã‚¹ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
        fit_results = detector.fit_peaks_gaussian(peak_region_width=50)
        print("\nã‚¬ã‚¦ã‚¹ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœ:")
        for i, result in enumerate(fit_results, 1):
            if result['fit_quality'] == 'success':
                print(f"  Peak {i}: pos={result['peak_position']:.2f}, "
                      f"FWHM={result['fwhm']:.2f}, area={result['area']:.1f}")
    
        # ãƒ—ãƒ­ãƒƒãƒˆ
        detector.plot_detected_peaks(show_labels=True)
    
    5.3 æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹åˆ†å…‰ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ†é¡
    5.3.1 ã‚¹ãƒšã‚¯ãƒˆãƒ«ç‰¹å¾´é‡ã®æŠ½å‡º
    æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’å…¥åŠ›ã™ã‚‹ã«ã¯ã€é©åˆ‡ãªç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
    
    ã‚¹ãƒšã‚¯ãƒˆãƒ«ç‰¹å¾´é‡ã®ç¨®é¡
    
    ãƒ”ãƒ¼ã‚¯ä½ç½®ãƒ»é«˜ã•ãƒ»å¹…: åŒ–å­¦æ§‹é€ ã«ç›´æ¥é–¢é€£
    ã‚¹ãƒšã‚¯ãƒˆãƒ«å½¢çŠ¶è¨˜è¿°å­: çµ±è¨ˆçš„ç‰¹å¾´ï¼ˆå¹³å‡ã€åˆ†æ•£ã€æ­ªåº¦ã€å°–åº¦ï¼‰
    ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰: æ¬¡å…ƒå‰Šæ¸›ã¨æƒ…å ±åœ§ç¸®
    ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆCNNï¼‰: è‡ªå‹•ç‰¹å¾´æŠ½å‡º
    
    
    ã‚³ãƒ¼ãƒ‰ä¾‹3: ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ†é¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼‰
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import classification_report, confusion_matrix
    from sklearn.decomposition import PCA
    import seaborn as sns
    
    class SpectralClassifier:
        """
        ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æ©Ÿæ¢°å­¦ç¿’ã§åˆ†é¡ã™ã‚‹ã‚¯ãƒ©ã‚¹
        """
    
        def __init__(self, n_estimators: int = 100, use_pca: bool = False, n_components: int = 10):
            """
            Parameters:
            -----------
            n_estimators : int
                ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®æœ¨ã®æ•°
            use_pca : bool
                PCAå‰å‡¦ç†ã‚’ä½¿ç”¨ã™ã‚‹ã‹
            n_components : int
                PCAæˆåˆ†æ•°
            """
            self.classifier = RandomForestClassifier(n_estimators=n_estimators,
                                                     max_depth=10,
                                                     random_state=42)
            self.use_pca = use_pca
            self.pca = PCA(n_components=n_components) if use_pca else None
            self.class_names = None
    
        def extract_features(self, spectra_list: List[SpectralData]) -> np.ndarray:
            """
            ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º
    
            Parameters:
            -----------
            spectra_list : list of SpectralData
                ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
    
            Returns:
            --------
            features : array, shape (n_samples, n_features)
                ç‰¹å¾´é‡è¡Œåˆ—
            """
            features = []
    
            for spectrum in spectra_list:
                # ç‰¹å¾´é‡1: ã‚¹ãƒšã‚¯ãƒˆãƒ«å…¨ä½“ï¼ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
                # å…¨ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’å›ºå®šé•·ï¼ˆä¾‹: 100ç‚¹ï¼‰ã«ãƒªã‚µãƒ³ãƒ—ãƒ«
                x_uniform = np.linspace(spectrum.x.min(), spectrum.x.max(), 100)
                y_resampled = np.interp(x_uniform, spectrum.x, spectrum.y)
    
                # ç‰¹å¾´é‡2: çµ±è¨ˆçš„ç‰¹å¾´
                mean_val = np.mean(spectrum.y)
                std_val = np.std(spectrum.y)
                max_val = np.max(spectrum.y)
                min_val = np.min(spectrum.y)
    
                # çµåˆ
                feature_vector = np.concatenate([
                    y_resampled,
                    [mean_val, std_val, max_val, min_val]
                ])
                features.append(feature_vector)
    
            return np.array(features)
    
        def train(self, X: np.ndarray, y: np.ndarray, class_names: List[str]):
            """
            åˆ†é¡å™¨ã‚’è¨“ç·´
    
            Parameters:
            -----------
            X : array, shape (n_samples, n_features)
                ç‰¹å¾´é‡è¡Œåˆ—
            y : array, shape (n_samples,)
                ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«
            class_names : list of str
                ã‚¯ãƒ©ã‚¹åã®ãƒªã‚¹ãƒˆ
            """
            self.class_names = class_names
    
            # PCAå‰å‡¦ç†
            if self.use_pca:
                X = self.pca.fit_transform(X)
                print(f"PCA: {X.shape[1]} components explain "
                      f"{self.pca.explained_variance_ratio_.sum():.2%} of variance")
    
            # è¨“ç·´
            self.classifier.fit(X, y)
    
            # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            cv_scores = cross_val_score(self.classifier, X, y, cv=5)
            print(f"Cross-validation accuracy: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")
    
        def predict(self, X: np.ndarray) -> np.ndarray:
            """
            ã‚¯ãƒ©ã‚¹ã‚’äºˆæ¸¬
    
            Parameters:
            -----------
            X : array, shape (n_samples, n_features)
                ç‰¹å¾´é‡è¡Œåˆ—
    
            Returns:
            --------
            y_pred : array, shape (n_samples,)
                äºˆæ¸¬ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«
            """
            if self.use_pca:
                X = self.pca.transform(X)
            return self.classifier.predict(X)
    
        def evaluate(self, X_test: np.ndarray, y_test: np.ndarray):
            """
            ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
    
            Parameters:
            -----------
            X_test : array
                ãƒ†ã‚¹ãƒˆç‰¹å¾´é‡
            y_test : array
                ãƒ†ã‚¹ãƒˆãƒ©ãƒ™ãƒ«
            """
            y_pred = self.predict(X_test)
    
            # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
            print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
            print(classification_report(y_test, y_pred, target_names=self.class_names))
    
            # æ··åŒè¡Œåˆ—
            cm = confusion_matrix(y_test, y_pred)
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                       xticklabels=self.class_names, yticklabels=self.class_names)
            plt.xlabel('äºˆæ¸¬ãƒ©ãƒ™ãƒ«', fontsize=12)
            plt.ylabel('çœŸã®ãƒ©ãƒ™ãƒ«', fontsize=12)
            plt.title('æ··åŒè¡Œåˆ—', fontsize=14, fontweight='bold')
            plt.tight_layout()
            plt.show()
    
    
    # ä½¿ç”¨ä¾‹ï¼š3ç¨®é¡ã®ææ–™ã®åˆ†å…‰ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ†é¡
    if __name__ == "__main__":
        np.random.seed(42)
    
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆ3ã‚¯ãƒ©ã‚¹ Ã— å„30ã‚µãƒ³ãƒ—ãƒ«ï¼‰
        def generate_spectrum_class(peak_positions, peak_widths, n_samples=30):
            spectra = []
            x = np.linspace(400, 700, 300)
            for _ in range(n_samples):
                y = np.zeros_like(x)
                for pos, width in zip(peak_positions, peak_widths):
                    # ãƒ”ãƒ¼ã‚¯ä½ç½®ã«ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚º
                    pos_noise = pos + np.random.normal(0, 5)
                    amplitude = np.random.uniform(0.7, 1.0)
                    y += amplitude * np.exp(-((x - pos_noise)**2) / (2 * width**2))
                y += np.random.normal(0, 0.03, len(x))
                spectra.append(SpectralData(x, y, "Wavelength (nm)", "Intensity"))
            return spectra
    
        # ã‚¯ãƒ©ã‚¹1: å˜ä¸€ãƒ”ãƒ¼ã‚¯ï¼ˆ550 nmï¼‰
        class1_spectra = generate_spectrum_class([550], [40], n_samples=30)
    
        # ã‚¯ãƒ©ã‚¹2: 2ã¤ã®ãƒ”ãƒ¼ã‚¯ï¼ˆ450 nm, 620 nmï¼‰
        class2_spectra = generate_spectrum_class([450, 620], [30, 35], n_samples=30)
    
        # ã‚¯ãƒ©ã‚¹3: 3ã¤ã®ãƒ”ãƒ¼ã‚¯ï¼ˆ480 nm, 550 nm, 650 nmï¼‰
        class3_spectra = generate_spectrum_class([480, 550, 650], [25, 30, 30], n_samples=30)
    
        # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        all_spectra = class1_spectra + class2_spectra + class3_spectra
        labels = np.array([0]*30 + [1]*30 + [2]*30)
        class_names = ['Material A (1 peak)', 'Material B (2 peaks)', 'Material C (3 peaks)']
    
        # ç‰¹å¾´é‡æŠ½å‡º
        classifier = SpectralClassifier(n_estimators=100, use_pca=False)
        X = classifier.extract_features(all_spectra)
    
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3,
                                                             random_state=42, stratify=labels)
    
        # è¨“ç·´
        classifier.train(X_train, y_train, class_names)
    
        # è©•ä¾¡
        classifier.evaluate(X_test, y_test)
    
        print(f"\nTotal samples: {len(all_spectra)}")
        print(f"Feature dimension: {X.shape[1]}")
        print(f"Training samples: {len(X_train)}, Test samples: {len(X_test)}")
    
    5.4 ãƒãƒƒãƒå‡¦ç†ã¨ãƒ‡ãƒ¼ã‚¿ç®¡ç†
    5.4.1 è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€æ‹¬å‡¦ç†
    ç ”ç©¶ç¾å ´ã§ã¯ã€æ•°åã€œæ•°ç™¾ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã™ã‚‹ã“ã¨ãŒä¸€èˆ¬çš„ã§ã™ã€‚åŠ¹ç‡çš„ãªãƒãƒƒãƒå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ãŒå¿…è¦ã§ã™ã€‚
    ã‚³ãƒ¼ãƒ‰ä¾‹4: ãƒãƒƒãƒå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    import numpy as np
    import pandas as pd
    from pathlib import Path
    from typing import List, Dict, Callable
    from concurrent.futures import ProcessPoolExecutor, as_completed
    from tqdm import tqdm
    import json
    
    class BatchProcessor:
        """
        è¤‡æ•°ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒãƒå‡¦ç†ã‚¯ãƒ©ã‚¹
        """
    
        def __init__(self, data_dir: Union[str, Path]):
            """
            Parameters:
            -----------
            data_dir : str or Path
                ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            """
            self.data_dir = Path(data_dir)
            self.results = []
    
        def find_files(self, pattern: str = "*.csv") -> List[Path]:
            """
            æŒ‡å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    
            Parameters:
            -----------
            pattern : str
                ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¾‹: "*.csv", "sample_*.txt"ï¼‰
    
            Returns:
            --------
            files : list of Path
                è¦‹ã¤ã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
            """
            files = list(self.data_dir.glob(pattern))
            print(f"Found {len(files)} files matching '{pattern}'")
            return files
    
        def process_file(self, filepath: Path,
                        processing_func: Callable[[SpectralData], Dict]) -> Dict:
            """
            å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    
            Parameters:
            -----------
            filepath : Path
                ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            processing_func : callable
                å‡¦ç†é–¢æ•°ï¼ˆSpectralDataã‚’å—ã‘å–ã‚Šã€çµæœdictã‚’è¿”ã™ï¼‰
    
            Returns:
            --------
            result : dict
                å‡¦ç†çµæœ
            """
            try:
                # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
                loader = SpectralDataLoader()
                spectrum = loader.auto_load(filepath)
    
                # å‡¦ç†å®Ÿè¡Œ
                result = processing_func(spectrum)
    
                # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’è¿½åŠ 
                result['filename'] = filepath.name
                result['status'] = 'success'
    
                return result
    
            except Exception as e:
                return {
                    'filename': filepath.name,
                    'status': 'failed',
                    'error': str(e)
                }
    
        def batch_process(self, files: List[Path],
                         processing_func: Callable[[SpectralData], Dict],
                         n_workers: int = 4) -> pd.DataFrame:
            """
            è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸¦åˆ—å‡¦ç†
    
            Parameters:
            -----------
            files : list of Path
                å‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
            processing_func : callable
                å‡¦ç†é–¢æ•°
            n_workers : int
                ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
    
            Returns:
            --------
            results_df : DataFrame
                å‡¦ç†çµæœã®DataFrame
            """
            results = []
    
            # ä¸¦åˆ—å‡¦ç†
            with ProcessPoolExecutor(max_workers=n_workers) as executor:
                # ã‚¿ã‚¹ã‚¯æŠ•å…¥
                futures = {executor.submit(self.process_file, f, processing_func): f
                          for f in files}
    
                # é€²æ—è¡¨ç¤º
                for future in tqdm(as_completed(futures), total=len(files),
                                  desc="Processing files"):
                    result = future.result()
                    results.append(result)
    
            # DataFrameã«å¤‰æ›
            results_df = pd.DataFrame(results)
            self.results = results_df
    
            # æˆåŠŸ/å¤±æ•—ã®ã‚µãƒãƒªãƒ¼
            success_count = (results_df['status'] == 'success').sum()
            print(f"\nProcessing complete: {success_count}/{len(files)} successful")
    
            return results_df
    
        def save_results(self, output_path: Union[str, Path], format: str = 'csv'):
            """
            çµæœã‚’ä¿å­˜
    
            Parameters:
            -----------
            output_path : str or Path
                å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            format : str
                'csv', 'json', 'excel' ã®ã„ãšã‚Œã‹
            """
            if self.results is None or len(self.results) == 0:
                print("No results to save")
                return
    
            output_path = Path(output_path)
    
            if format == 'csv':
                self.results.to_csv(output_path, index=False)
            elif format == 'json':
                self.results.to_json(output_path, orient='records', indent=2)
            elif format == 'excel':
                self.results.to_excel(output_path, index=False)
            else:
                raise ValueError(f"Unknown format: {format}")
    
            print(f"Results saved to {output_path}")
    
    
    # ä½¿ç”¨ä¾‹ï¼šãƒãƒƒãƒå‡¦ç†ã®å®Ÿè£…
    def example_processing_function(spectrum: SpectralData) -> Dict:
        """
        ä¾‹ï¼šãƒ”ãƒ¼ã‚¯æ¤œå‡ºã¨çµ±è¨ˆæƒ…å ±ã®æŠ½å‡º
    
        Parameters:
        -----------
        spectrum : SpectralData
            ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿
    
        Returns:
        --------
        result : dict
            è§£æçµæœ
        """
        # ãƒ”ãƒ¼ã‚¯æ¤œå‡º
        detector = PeakDetector(spectrum)
        peaks = detector.detect_peaks(height=0.1, prominence=0.05, distance=20)
    
        # çµ±è¨ˆæƒ…å ±
        mean_intensity = np.mean(spectrum.y)
        max_intensity = np.max(spectrum.y)
        std_intensity = np.std(spectrum.y)
    
        # ãƒ”ãƒ¼ã‚¯æƒ…å ±
        peak_positions = [p.position for p in peaks]
        peak_heights = [p.height for p in peaks]
    
        return {
            'n_peaks': len(peaks),
            'peak_positions': peak_positions,
            'peak_heights': peak_heights,
            'mean_intensity': mean_intensity,
            'max_intensity': max_intensity,
            'std_intensity': std_intensity,
            'x_range_min': spectrum.x.min(),
            'x_range_max': spectrum.x.max()
        }
    
    
    if __name__ == "__main__":
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼šè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆã¨å‡¦ç†
        import tempfile
        import shutil
    
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        temp_dir = Path(tempfile.mkdtemp())
        print(f"Temporary directory: {temp_dir}")
    
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆ10ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
        for i in range(10):
            x = np.linspace(400, 700, 300)
            # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ”ãƒ¼ã‚¯
            n_peaks = np.random.randint(1, 4)
            y = np.zeros_like(x)
            for _ in range(n_peaks):
                peak_pos = np.random.uniform(450, 650)
                peak_width = np.random.uniform(20, 40)
                y += np.random.uniform(0.5, 1.0) * np.exp(-((x - peak_pos)**2) / (2 * peak_width**2))
            y += np.random.normal(0, 0.02, len(x))
    
            # CSVä¿å­˜
            filepath = temp_dir / f"sample_{i:03d}.csv"
            with open(filepath, 'w') as f:
                for xi, yi in zip(x, y):
                    f.write(f"{xi},{yi}\n")
    
        # ãƒãƒƒãƒå‡¦ç†
        processor = BatchProcessor(temp_dir)
        files = processor.find_files("*.csv")
        results_df = processor.batch_process(files, example_processing_function, n_workers=2)
    
        print("\nå‡¦ç†çµæœã®ä¸€éƒ¨:")
        print(results_df.head())
    
        # çµæœä¿å­˜
        output_path = temp_dir / "batch_results.csv"
        processor.save_results(output_path, format='csv')
    
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        shutil.rmtree(temp_dir)
        print(f"\nTemporary directory removed")
    
    5.5 ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯è¦–åŒ–ã¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
    5.5.1 Plotlyã«ã‚ˆã‚‹å‹•çš„ãƒ—ãƒ­ãƒƒãƒˆ
    é™çš„ãªMatplotlibã«åŠ ãˆã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªå¯è¦–åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªPlotlyã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ã‚ºãƒ¼ãƒ ã€ãƒ‘ãƒ³ã€ãƒ›ãƒãƒ¼æƒ…å ±è¡¨ç¤ºãªã©ã®æ©Ÿèƒ½ã‚’æŒã¤å‹•çš„ãªãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆã§ãã¾ã™ã€‚
    ã‚³ãƒ¼ãƒ‰ä¾‹5: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ“ãƒ¥ãƒ¼ã‚¢
    import numpy as np
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
    from typing import List
    
    class InteractiveSpectralViewer:
        """
        ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¹ãƒšã‚¯ãƒˆãƒ«å¯è¦–åŒ–ã‚¯ãƒ©ã‚¹ï¼ˆPlotlyä½¿ç”¨ï¼‰
        """
    
        def __init__(self):
            self.fig = None
    
        def plot_single_spectrum(self, spectrum: SpectralData, title: str = "Spectrum"):
            """
            å˜ä¸€ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    
            Parameters:
            -----------
            spectrum : SpectralData
                ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿
            title : str
                ãƒ—ãƒ­ãƒƒãƒˆã‚¿ã‚¤ãƒˆãƒ«
            """
            fig = go.Figure()
    
            fig.add_trace(go.Scatter(
                x=spectrum.x,
                y=spectrum.y,
                mode='lines',
                name='Spectrum',
                line=dict(color='blue', width=2),
                hovertemplate='%{x:.2f}%{y:.3f}'
            ))
    
            fig.update_layout(
                title=title,
                xaxis_title=spectrum.x_label,
                yaxis_title=spectrum.y_label,
                hovermode='x unified',
                template='plotly_white',
                width=1000,
                height=600
            )
    
            self.fig = fig
            return fig
    
        def plot_multiple_spectra(self, spectra_list: List[SpectralData],
                                 labels: List[str], title: str = "Multiple Spectra"):
            """
            è¤‡æ•°ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’é‡ã­åˆã‚ã›ãƒ—ãƒ­ãƒƒãƒˆ
    
            Parameters:
            -----------
            spectra_list : list of SpectralData
                ã‚¹ãƒšã‚¯ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆ
            labels : list of str
                å„ã‚¹ãƒšã‚¯ãƒˆãƒ«ã®ãƒ©ãƒ™ãƒ«
            title : str
                ãƒ—ãƒ­ãƒƒãƒˆã‚¿ã‚¤ãƒˆãƒ«
            """
            fig = go.Figure()
    
            colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']
    
            for i, (spectrum, label) in enumerate(zip(spectra_list, labels)):
                color = colors[i % len(colors)]
    
                fig.add_trace(go.Scatter(
                    x=spectrum.x,
                    y=spectrum.y,
                    mode='lines',
                    name=label,
                    line=dict(color=color, width=2),
                    hovertemplate=f'{label}%{{x:.2f}}%{{y:.3f}}'
                ))
    
            fig.update_layout(
                title=title,
                xaxis_title=spectra_list[0].x_label,
                yaxis_title=spectra_list[0].y_label,
                hovermode='x unified',
                template='plotly_white',
                width=1000,
                height=600,
                legend=dict(x=0.02, y=0.98, bgcolor='rgba(255,255,255,0.8)')
            )
    
            self.fig = fig
            return fig
    
        def plot_with_peaks(self, spectrum: SpectralData, peaks: List[Peak],
                           title: str = "Spectrum with Detected Peaks"):
            """
            ãƒ”ãƒ¼ã‚¯æ¤œå‡ºçµæœã‚’å«ã‚€ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    
            Parameters:
            -----------
            spectrum : SpectralData
                ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿
            peaks : list of Peak
                æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯
            title : str
                ãƒ—ãƒ­ãƒƒãƒˆã‚¿ã‚¤ãƒˆãƒ«
            """
            fig = go.Figure()
    
            # ã‚¹ãƒšã‚¯ãƒˆãƒ«
            fig.add_trace(go.Scatter(
                x=spectrum.x,
                y=spectrum.y,
                mode='lines',
                name='Spectrum',
                line=dict(color='blue', width=2),
                hovertemplate='%{x:.2f}%{y:.3f}'
            ))
    
            # ãƒ”ãƒ¼ã‚¯
            if peaks:
                peak_x = [p.position for p in peaks]
                peak_y = [p.height for p in peaks]
                peak_info = [f"Position: {p.position:.2f}Height: {p.height:.3f}FWHM: {p.width:.2f}"
                            for p in peaks]
    
                fig.add_trace(go.Scatter(
                    x=peak_x,
                    y=peak_y,
                    mode='markers',
                    name=f'Peaks (n={len(peaks)})',
                    marker=dict(color='red', size=12, symbol='diamond'),
                    text=peak_info,
                    hovertemplate='%{text}'
                ))
    
            fig.update_layout(
                title=title,
                xaxis_title=spectrum.x_label,
                yaxis_title=spectrum.y_label,
                hovermode='closest',
                template='plotly_white',
                width=1000,
                height=600
            )
    
            self.fig = fig
            return fig
    
        def plot_comparison_grid(self, spectra_dict: Dict[str, SpectralData],
                                title: str = "Spectral Comparison"):
            """
            è¤‡æ•°ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤º
    
            Parameters:
            -----------
            spectra_dict : dict
                {label: SpectralData} ã®è¾æ›¸
            title : str
                ãƒ—ãƒ­ãƒƒãƒˆã‚¿ã‚¤ãƒˆãƒ«
            """
            n_spectra = len(spectra_dict)
            rows = (n_spectra + 1) // 2
            cols = 2
    
            fig = make_subplots(
                rows=rows, cols=cols,
                subplot_titles=list(spectra_dict.keys()),
                vertical_spacing=0.12,
                horizontal_spacing=0.1
            )
    
            for i, (label, spectrum) in enumerate(spectra_dict.items(), 1):
                row = (i - 1) // cols + 1
                col = (i - 1) % cols + 1
    
                fig.add_trace(
                    go.Scatter(
                        x=spectrum.x,
                        y=spectrum.y,
                        mode='lines',
                        name=label,
                        line=dict(width=2),
                        showlegend=False
                    ),
                    row=row, col=col
                )
    
                fig.update_xaxes(title_text=spectrum.x_label, row=row, col=col)
                fig.update_yaxes(title_text=spectrum.y_label, row=row, col=col)
    
            fig.update_layout(
                title_text=title,
                template='plotly_white',
                width=1200,
                height=300 * rows
            )
    
            self.fig = fig
            return fig
    
        def show(self):
            """ãƒ—ãƒ­ãƒƒãƒˆã‚’è¡¨ç¤º"""
            if self.fig:
                self.fig.show()
    
    
    # ä½¿ç”¨ä¾‹
    if __name__ == "__main__":
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        x = np.linspace(400, 700, 500)
    
        # ã‚¹ãƒšã‚¯ãƒˆãƒ«1
        y1 = 0.8 * np.exp(-((x - 500)**2) / (2 * 40**2)) + np.random.normal(0, 0.02, len(x))
        spectrum1 = SpectralData(x, y1, "Wavelength (nm)", "Absorbance")
    
        # ã‚¹ãƒšã‚¯ãƒˆãƒ«2
        y2 = 0.6 * np.exp(-((x - 550)**2) / (2 * 35**2)) + np.random.normal(0, 0.02, len(x))
        spectrum2 = SpectralData(x, y2, "Wavelength (nm)", "Absorbance")
    
        # ã‚¹ãƒšã‚¯ãƒˆãƒ«3
        y3 = 0.9 * np.exp(-((x - 600)**2) / (2 * 30**2)) + np.random.normal(0, 0.02, len(x))
        spectrum3 = SpectralData(x, y3, "Wavelength (nm)", "Absorbance")
    
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ“ãƒ¥ãƒ¼ã‚¢
        viewer = InteractiveSpectralViewer()
    
        # å˜ä¸€ã‚¹ãƒšã‚¯ãƒˆãƒ«
        print("1. å˜ä¸€ã‚¹ãƒšã‚¯ãƒˆãƒ«ã®ãƒ—ãƒ­ãƒƒãƒˆ")
        fig1 = viewer.plot_single_spectrum(spectrum1, title="Sample Spectrum")
        # fig1.show()  # ãƒ–ãƒ©ã‚¦ã‚¶ã§è¡¨ç¤º
    
        # è¤‡æ•°ã‚¹ãƒšã‚¯ãƒˆãƒ«é‡ã­åˆã‚ã›
        print("2. è¤‡æ•°ã‚¹ãƒšã‚¯ãƒˆãƒ«ã®é‡ã­åˆã‚ã›")
        fig2 = viewer.plot_multiple_spectra(
            [spectrum1, spectrum2, spectrum3],
            ['Sample A (500 nm)', 'Sample B (550 nm)', 'Sample C (600 nm)'],
            title="Multiple Spectra Overlay"
        )
        # fig2.show()
    
        # ãƒ”ãƒ¼ã‚¯æ¤œå‡ºçµæœä»˜ã
        print("3. ãƒ”ãƒ¼ã‚¯æ¤œå‡ºçµæœã®è¡¨ç¤º")
        detector = PeakDetector(spectrum1)
        peaks = detector.detect_peaks(height=0.2, prominence=0.1)
        fig3 = viewer.plot_with_peaks(spectrum1, peaks, title="Spectrum with Peaks")
        # fig3.show()
    
        # ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤º
        print("4. ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤º")
        spectra_dict = {
            'Sample A': spectrum1,
            'Sample B': spectrum2,
            'Sample C': spectrum3
        }
        fig4 = viewer.plot_comparison_grid(spectra_dict, title="Spectral Comparison Grid")
        # fig4.show()
    
        print("\nã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒƒãƒˆã®ä½¿ç”¨æ–¹æ³•:")
        print("  - ãƒã‚¦ã‚¹ãƒ›ãƒãƒ¼ã§ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã®å€¤ã‚’è¡¨ç¤º")
        print("  - ãƒ‰ãƒ©ãƒƒã‚°ã§ã‚ºãƒ¼ãƒ ã€ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯ã§ãƒªã‚»ãƒƒãƒˆ")
        print("  - å‡¡ä¾‹ã‚¯ãƒªãƒƒã‚¯ã§ç³»åˆ—ã®è¡¨ç¤º/éè¡¨ç¤º")
    
    5.6 çµ±åˆè§£æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å®Ÿè£…
    5.6.1 ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    ã“ã‚Œã¾ã§ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’çµ±åˆã—ã€ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‹ã‚‰çµæœå‡ºåŠ›ã¾ã§ã®å®Œå…¨ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
    
        flowchart LR
            A[ç”Ÿãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤] --> B[ãƒãƒƒãƒèª­ã¿è¾¼ã¿]
            B --> C[å‰å‡¦ç†ãƒã‚¤ã‚ºé™¤å»ãƒ»æ­£è¦åŒ–]
            C --> D[ãƒ”ãƒ¼ã‚¯æ¤œå‡º]
            D --> E[ç‰¹å¾´é‡æŠ½å‡º]
            E --> F{è§£æã‚¿ã‚¤ãƒ—}
            F -->|åˆ†é¡| G[æ©Ÿæ¢°å­¦ç¿’åˆ†é¡å™¨]
            F -->|å®šé‡| H[ãƒ”ãƒ¼ã‚¯é¢ç©å®šé‡]
            F -->|æ¯”è¼ƒ| I[çµ±è¨ˆçš„æ¯”è¼ƒ]
            G --> J[çµæœå¯è¦–åŒ–]
            H --> J
            I --> J
            J --> K[ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›CSV/JSON/HTML]
    
            style A fill:#e3f2fd
            style C fill:#fff3e0
            style F fill:#fce4ec
            style J fill:#e8f5e9
            style K fill:#ffe0b2
        
    ã‚³ãƒ¼ãƒ‰ä¾‹6: çµ±åˆã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    import numpy as np
    import pandas as pd
    from pathlib import Path
    from typing import Dict, List
    import json
    
    class SpectralAnalysisPipeline:
        """
        çµ±åˆã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        """
    
        def __init__(self, config: Dict):
            """
            Parameters:
            -----------
            config : dict
                ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­å®š
                {
                    'data_dir': Path,
                    'output_dir': Path,
                    'file_pattern': str,
                    'preprocessing': {...},
                    'peak_detection': {...},
                    'analysis_type': 'classification' or 'quantification'
                }
            """
            self.config = config
            self.data_dir = Path(config['data_dir'])
            self.output_dir = Path(config['output_dir'])
            self.output_dir.mkdir(parents=True, exist_ok=True)
    
            self.spectra = []
            self.results = None
    
        def run_pipeline(self):
            """
            ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’å®Ÿè¡Œ
            """
            print("="*60)
            print("Spectral Analysis Pipeline")
            print("="*60)
    
            # Step 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            print("\n[Step 1/5] Loading data...")
            self._load_data()
    
            # Step 2: å‰å‡¦ç†
            print("\n[Step 2/5] Preprocessing...")
            self._preprocess()
    
            # Step 3: ãƒ”ãƒ¼ã‚¯æ¤œå‡º
            print("\n[Step 3/5] Peak detection...")
            self._detect_peaks()
    
            # Step 4: è§£æ
            print("\n[Step 4/5] Analysis...")
            self._analyze()
    
            # Step 5: çµæœå‡ºåŠ›
            print("\n[Step 5/5] Exporting results...")
            self._export_results()
    
            print("\n" + "="*60)
            print("Pipeline completed successfully!")
            print("="*60)
    
        def _load_data(self):
            """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
            processor = BatchProcessor(self.data_dir)
            files = processor.find_files(self.config.get('file_pattern', '*.csv'))
    
            loader = SpectralDataLoader()
            for file in files:
                try:
                    spectrum = loader.auto_load(file)
                    spectrum.metadata['source_file'] = file.name
                    self.spectra.append(spectrum)
                except Exception as e:
                    print(f"  Warning: Failed to load {file.name}: {e}")
    
            print(f"  Loaded {len(self.spectra)} spectra")
    
        def _preprocess(self):
            """å‰å‡¦ç†"""
            preproc_config = self.config.get('preprocessing', {})
    
            for spectrum in self.spectra:
                # ãƒã‚¤ã‚ºé™¤å»
                if preproc_config.get('smooth', False):
                    from scipy.signal import savgol_filter
                    spectrum.y = savgol_filter(spectrum.y, window_length=11, polyorder=3)
    
                # ãƒˆãƒªãƒŸãƒ³ã‚°
                if 'trim_range' in preproc_config:
                    x_min, x_max = preproc_config['trim_range']
                    spectrum.trim(x_min, x_max)
    
                # æ­£è¦åŒ–
                if 'normalize' in preproc_config:
                    spectrum.normalize(method=preproc_config['normalize'])
    
            print(f"  Preprocessed {len(self.spectra)} spectra")
    
        def _detect_peaks(self):
            """ãƒ”ãƒ¼ã‚¯æ¤œå‡º"""
            peak_config = self.config.get('peak_detection', {})
    
            for spectrum in self.spectra:
                detector = PeakDetector(spectrum)
                peaks = detector.detect_peaks(
                    height=peak_config.get('height'),
                    prominence=peak_config.get('prominence'),
                    distance=peak_config.get('distance')
                )
                spectrum.metadata['peaks'] = peaks
    
            total_peaks = sum(len(s.metadata['peaks']) for s in self.spectra)
            print(f"  Detected {total_peaks} peaks across {len(self.spectra)} spectra")
    
        def _analyze(self):
            """è§£æ"""
            analysis_type = self.config.get('analysis_type', 'quantification')
    
            if analysis_type == 'quantification':
                self._analyze_quantification()
            elif analysis_type == 'classification':
                self._analyze_classification()
            else:
                raise ValueError(f"Unknown analysis type: {analysis_type}")
    
        def _analyze_quantification(self):
            """å®šé‡è§£æ"""
            results = []
    
            for spectrum in self.spectra:
                peaks = spectrum.metadata.get('peaks', [])
    
                result = {
                    'filename': spectrum.metadata.get('source_file', 'unknown'),
                    'n_peaks': len(peaks),
                    'peak_positions': [p.position for p in peaks],
                    'peak_areas': [p.area for p in peaks],
                    'total_area': sum(p.area for p in peaks)
                }
                results.append(result)
    
            self.results = pd.DataFrame(results)
            print(f"  Quantification completed for {len(results)} samples")
    
        def _analyze_classification(self):
            """åˆ†é¡è§£æ"""
            # ç‰¹å¾´é‡æŠ½å‡º
            classifier = SpectralClassifier(n_estimators=100, use_pca=False)
            X = classifier.extract_features(self.spectra)
    
            # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ•™å¸«ãªã—åˆ†é¡ï¼‰
            from sklearn.cluster import KMeans
            n_clusters = self.config.get('n_clusters', 3)
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            labels = kmeans.fit_predict(X)
    
            results = []
            for i, spectrum in enumerate(self.spectra):
                result = {
                    'filename': spectrum.metadata.get('source_file', 'unknown'),
                    'cluster': int(labels[i]),
                    'n_peaks': len(spectrum.metadata.get('peaks', []))
                }
                results.append(result)
    
            self.results = pd.DataFrame(results)
            print(f"  Classification completed: {n_clusters} clusters identified")
    
        def _export_results(self):
            """çµæœå‡ºåŠ›"""
            if self.results is None:
                print("  No results to export")
                return
    
            # CSVå‡ºåŠ›
            csv_path = self.output_dir / 'results.csv'
            self.results.to_csv(csv_path, index=False)
            print(f"  Saved CSV: {csv_path}")
    
            # JSONå‡ºåŠ›
            json_path = self.output_dir / 'results.json'
            self.results.to_json(json_path, orient='records', indent=2)
            print(f"  Saved JSON: {json_path}")
    
            # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
            summary = {
                'total_samples': len(self.spectra),
                'analysis_type': self.config.get('analysis_type'),
                'output_files': [str(csv_path), str(json_path)]
            }
    
            summary_path = self.output_dir / 'summary.json'
            with open(summary_path, 'w') as f:
                json.dump(summary, f, indent=2)
            print(f"  Saved summary: {summary_path}")
    
    
    # ä½¿ç”¨ä¾‹
    if __name__ == "__main__":
        import tempfile
        import shutil
    
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        temp_dir = Path(tempfile.mkdtemp())
        data_dir = temp_dir / 'data'
        output_dir = temp_dir / 'output'
        data_dir.mkdir()
    
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        for i in range(15):
            x = np.linspace(400, 700, 300)
            y = np.zeros_like(x)
            n_peaks = np.random.randint(1, 4)
            for _ in range(n_peaks):
                pos = np.random.uniform(450, 650)
                width = np.random.uniform(20, 40)
                y += np.random.uniform(0.5, 1.0) * np.exp(-((x - pos)**2) / (2 * width**2))
            y += np.random.normal(0, 0.02, len(x))
    
            filepath = data_dir / f"sample_{i:03d}.csv"
            with open(filepath, 'w') as f:
                for xi, yi in zip(x, y):
                    f.write(f"{xi},{yi}\n")
    
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­å®š
        config = {
            'data_dir': data_dir,
            'output_dir': output_dir,
            'file_pattern': '*.csv',
            'preprocessing': {
                'smooth': True,
                'trim_range': (420, 680),
                'normalize': 'max'
            },
            'peak_detection': {
                'height': 0.2,
                'prominence': 0.1,
                'distance': 20
            },
            'analysis_type': 'quantification',  # or 'classification'
            'n_clusters': 3
        }
    
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        pipeline = SpectralAnalysisPipeline(config)
        pipeline.run_pipeline()
    
        # çµæœç¢ºèª
        print("\nçµæœã®ä¸€éƒ¨:")
        print(pipeline.results.head())
    
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        shutil.rmtree(temp_dir)
    
    5.7 å®Ÿè·µæ¼”ç¿’ï¼šå®Ÿãƒ‡ãƒ¼ã‚¿è§£æãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
    5.7.1 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    æœ¬ç« ã§å­¦ã‚“ã ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€å®Ÿéš›ã®ç ”ç©¶ãƒ‡ãƒ¼ã‚¿ã‚’è§£æã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æä¾›ã—ã¾ã™ã€‚
    ã‚³ãƒ¼ãƒ‰ä¾‹7: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆãƒ„ãƒ¼ãƒ«
    import os
    from pathlib import Path
    from typing import Optional
    
    class SpectralProjectTemplate:
        """
        ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç”Ÿæˆ
        """
    
        @staticmethod
        def create_project(project_name: str, base_dir: Optional[Path] = None):
            """
            ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆ
    
            Parameters:
            -----------
            project_name : str
                ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå
            base_dir : Path, optional
                ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰
            """
            if base_dir is None:
                base_dir = Path.cwd()
    
            project_dir = base_dir / project_name
            project_dir.mkdir(parents=True, exist_ok=True)
    
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 
            dirs = [
                'data/raw',
                'data/processed',
                'results',
                'figures',
                'notebooks',
                'scripts',
                'config'
            ]
    
            for dir_path in dirs:
                (project_dir / dir_path).mkdir(parents=True, exist_ok=True)
    
            # READMEãƒ•ã‚¡ã‚¤ãƒ«
            readme_content = f"""# {project_name}
    
    ## Project Structure
    
    - `data/raw/`: ç”Ÿãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ«ã€ç·¨é›†ç¦æ­¢ï¼‰
    - `data/processed/`: å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
    - `results/`: è§£æçµæœï¼ˆCSV, JSONç­‰ï¼‰
    - `figures/`: ã‚°ãƒ©ãƒ•ãƒ»å›³è¡¨
    - `notebooks/`: Jupyter Notebook
    - `scripts/`: Pythonè§£æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
    - `config/`: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
    
    ## Workflow
    
    1. ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ `data/raw/` ã«é…ç½®
    2. `scripts/preprocess.py` ã§å‰å‡¦ç†å®Ÿè¡Œ
    3. `scripts/analyze.py` ã§è§£æå®Ÿè¡Œ
    4. `results/` ã«çµæœå‡ºåŠ›ã€`figures/` ã«å›³è¡¨ä¿å­˜
    
    ## Requirements
    
    ```bash
    pip install numpy scipy matplotlib pandas scikit-learn plotly
    ```
    
    ## Quick Start
    
    ```python
    python scripts/analyze.py --config config/config.json
    ```
    """
            with open(project_dir / 'README.md', 'w') as f:
                f.write(readme_content)
    
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
            config_content = """{
      "data_dir": "data/raw",
      "output_dir": "results",
      "file_pattern": "*.csv",
      "preprocessing": {
        "smooth": true,
        "trim_range": [400, 700],
        "normalize": "max"
      },
      "peak_detection": {
        "height": 0.2,
        "prominence": 0.1,
        "distance": 20
      },
      "analysis_type": "quantification"
    }
    """
            with open(project_dir / 'config/config.json', 'w') as f:
                f.write(config_content)
    
            # è§£æã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
            analyze_script = """#!/usr/bin/env python
    # -*- coding: utf-8 -*-
    \"\"\"
    ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
    \"\"\"
    
    import argparse
    import json
    from pathlib import Path
    
    # ä»¥ä¸‹ã«æœ¬ç« ã®SpectralAnalysisPipelineã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    # from spectral_tools import SpectralAnalysisPipeline
    
    def main():
        parser = argparse.ArgumentParser(description='Spectral Analysis Pipeline')
        parser.add_argument('--config', type=str, required=True,
                           help='Path to config JSON file')
        args = parser.parse_args()
    
        # è¨­å®šèª­ã¿è¾¼ã¿
        with open(args.config, 'r') as f:
            config = json.load(f)
    
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        # pipeline = SpectralAnalysisPipeline(config)
        # pipeline.run_pipeline()
    
        print("Analysis completed!")
    
    if __name__ == "__main__":
        main()
    """
            with open(project_dir / 'scripts/analyze.py', 'w') as f:
                f.write(analyze_script)
    
            # .gitignoreãƒ•ã‚¡ã‚¤ãƒ«
            gitignore_content = """# Data files
    data/raw/*
    !data/raw/.gitkeep
    *.csv
    *.txt
    *.dat
    
    # Python
    __pycache__/
    *.py[cod]
    *.so
    .ipynb_checkpoints/
    
    # Results
    results/*
    !results/.gitkeep
    figures/*
    !figures/.gitkeep
    """
            with open(project_dir / '.gitignore', 'w') as f:
                f.write(gitignore_content)
    
            # .gitkeepãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆç©ºãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’Gitç®¡ç†ä¸‹ã«ç½®ããŸã‚ï¼‰
            for dir_path in ['data/raw', 'results', 'figures']:
                (project_dir / dir_path / '.gitkeep').touch()
    
            print(f"âœ… Project '{project_name}' created successfully!")
            print(f"ğŸ“ Location: {project_dir}")
            print(f"\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
            print(f"  1. cd {project_dir}")
            print(f"  2. ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ data/raw/ ã«é…ç½®")
            print(f"  3. config/config.json ã‚’ç·¨é›†")
            print(f"  4. python scripts/analyze.py --config config/config.json")
    
    
    # ä½¿ç”¨ä¾‹
    if __name__ == "__main__":
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
        SpectralProjectTemplate.create_project("my_spectroscopy_project")
    
    5.8 æ¼”ç¿’å•é¡Œ
    
    åŸºç¤å•é¡Œï¼ˆEasyï¼‰
    å•é¡Œ1: SpectralDataã‚¯ãƒ©ã‚¹ã®ä½¿ç”¨
    æ³¢é•·400-700 nmã€100ç‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŒã¤SpectralDataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã€500-600 nmã®ç¯„å›²ã«ãƒˆãƒªãƒŸãƒ³ã‚°ã—ãŸå¾Œã€æœ€å¤§å€¤ã§æ­£è¦åŒ–ã›ã‚ˆã€‚
    
    è§£ç­”ã‚’è¦‹ã‚‹
    
    è§£ç­”:
    import numpy as np
    
    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    x = np.linspace(400, 700, 100)
    y = np.exp(-((x - 550)**2) / (2 * 50**2))
    
    # SpectralDataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
    spectrum = SpectralData(x, y, "Wavelength (nm)", "Intensity")
    
    # ãƒˆãƒªãƒŸãƒ³ã‚°
    spectrum.trim(500, 600)
    
    # æ­£è¦åŒ–
    spectrum.normalize(method='max')
    
    print(f"ãƒˆãƒªãƒŸãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°: {len(spectrum.x)}")
    print(f"æ­£è¦åŒ–å¾Œã®æœ€å¤§å€¤: {spectrum.y.max()}")  # 1.0
    
    
    
    å•é¡Œ2: ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã®åŸºæœ¬
    ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€é«˜ã•0.3ä»¥ä¸Šã€å“è¶Šåº¦0.2ä»¥ä¸Šã®ãƒ”ãƒ¼ã‚¯ã‚’æ¤œå‡ºã—ã€å„ãƒ”ãƒ¼ã‚¯ã®ä½ç½®ã¨é«˜ã•ã‚’å‡ºåŠ›ã›ã‚ˆã€‚
    
    è§£ç­”ã‚’è¦‹ã‚‹
    
    è§£ç­”:
    # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿
    x = np.linspace(400, 700, 500)
    y = (0.8 * np.exp(-((x - 500)**2) / (2 * 30**2)) +
         0.5 * np.exp(-((x - 600)**2) / (2 * 40**2)))
    
    spectrum = SpectralData(x, y, "Wavelength (nm)", "Intensity")
    
    # ãƒ”ãƒ¼ã‚¯æ¤œå‡º
    detector = PeakDetector(spectrum)
    peaks = detector.detect_peaks(height=0.3, prominence=0.2)
    
    print(f"æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯æ•°: {len(peaks)}")
    for i, peak in enumerate(peaks, 1):
        print(f"  Peak {i}: position={peak.position:.1f} nm, height={peak.height:.3f}")
    
    
    
    å•é¡Œ3: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½¿ç”¨
    CSVãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ2åˆ—: æ³¢é•·ã€å¼·åº¦ï¼‰ã‹ã‚‰ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°ã¨æ³¢é•·ç¯„å›²ã‚’å‡ºåŠ›ã›ã‚ˆã€‚
    
    è§£ç­”ã‚’è¦‹ã‚‹
    
    è§£ç­”:
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    loader = SpectralDataLoader()
    spectrum = loader.load_csv('sample.csv', x_col=0, y_col=1,
                              x_label="Wavelength (nm)", y_label="Intensity")
    
    print(f"ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°: {len(spectrum.x)}")
    print(f"æ³¢é•·ç¯„å›²: {spectrum.x.min():.1f} - {spectrum.x.max():.1f} nm")
    print(f"å¼·åº¦ç¯„å›²: {spectrum.y.min():.3f} - {spectrum.y.max():.3f}")
    
    
    
    
    
    ä¸­ç´šå•é¡Œï¼ˆMediumï¼‰
    å•é¡Œ4: ã‚«ã‚¹ã‚¿ãƒ å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    ä»¥ä¸‹ã®å‰å‡¦ç†ã‚’é †æ¬¡é©ç”¨ã™ã‚‹é–¢æ•°ã‚’ä½œæˆã›ã‚ˆï¼š(1) Savitzky-Golayå¹³æ»‘åŒ–ã€(2) ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è£œæ­£ï¼ˆç·šå½¢ï¼‰ã€(3) æœ€å¤§å€¤æ­£è¦åŒ–ã€‚
    
    è§£ç­”ã‚’è¦‹ã‚‹
    
    è§£ç­”:
    from scipy.signal import savgol_filter
    from scipy.stats import linregress
    
    def custom_preprocessing_pipeline(spectrum: SpectralData) -> SpectralData:
        """
        ã‚«ã‚¹ã‚¿ãƒ å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    
        Parameters:
        -----------
        spectrum : SpectralData
            å…¥åŠ›ã‚¹ãƒšã‚¯ãƒˆãƒ«
    
        Returns:
        --------
        processed_spectrum : SpectralData
            å‰å‡¦ç†æ¸ˆã¿ã‚¹ãƒšã‚¯ãƒˆãƒ«
        """
        # ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ
        processed = spectrum.copy()
    
        # (1) Savitzky-Golayå¹³æ»‘åŒ–
        processed.y = savgol_filter(processed.y, window_length=11, polyorder=3)
    
        # (2) ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è£œæ­£ï¼ˆç·šå½¢ãƒ•ã‚£ãƒƒãƒˆï¼‰
        # ä¸¡ç«¯10%ã®ãƒ‡ãƒ¼ã‚¿ã§ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’æ¨å®š
        n = len(processed.x)
        baseline_indices = list(range(int(n * 0.1))) + list(range(int(n * 0.9), n))
        x_base = processed.x[baseline_indices]
        y_base = processed.y[baseline_indices]
    
        slope, intercept, _, _, _ = linregress(x_base, y_base)
        baseline = slope * processed.x + intercept
        processed.y = processed.y - baseline
    
        # (3) æœ€å¤§å€¤æ­£è¦åŒ–
        processed.normalize(method='max')
    
        return processed
    
    # ä½¿ç”¨ä¾‹
    x = np.linspace(400, 700, 300)
    y = 0.8 * np.exp(-((x - 550)**2) / (2 * 40**2)) + 0.02 * x + np.random.normal(0, 0.02, len(x))
    spectrum = SpectralData(x, y, "Wavelength (nm)", "Intensity")
    
    processed_spectrum = custom_preprocessing_pipeline(spectrum)
    
    print("å‰å‡¦ç†å®Œäº†:")
    print(f"  å…ƒã®yç¯„å›²: [{spectrum.y.min():.3f}, {spectrum.y.max():.3f}]")
    print(f"  å‡¦ç†å¾Œã®yç¯„å›²: [{processed_spectrum.y.min():.3f}, {processed_spectrum.y.max():.3f}]")
    
    
    
    å•é¡Œ5: ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹çµ±è¨ˆè§£æ
    è¤‡æ•°ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ”ãƒ¼ã‚¯æ•°ã€æœ€å¤§ãƒ”ãƒ¼ã‚¯é«˜ã•ã€å¹³å‡å¼·åº¦ã‚’è¨ˆç®—ã—ã€DataFrameã¨ã—ã¦å‡ºåŠ›ã›ã‚ˆã€‚
    
    è§£ç­”ã‚’è¦‹ã‚‹
    
    è§£ç­”:
    def batch_statistical_analysis(spectrum: SpectralData) -> Dict:
        """
        ã‚¹ãƒšã‚¯ãƒˆãƒ«ã®çµ±è¨ˆè§£æ
    
        Parameters:
        -----------
        spectrum : SpectralData
            ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿
    
        Returns:
        --------
        stats : dict
            çµ±è¨ˆæƒ…å ±
        """
        # ãƒ”ãƒ¼ã‚¯æ¤œå‡º
        detector = PeakDetector(spectrum)
        peaks = detector.detect_peaks(height=0.1, prominence=0.05)
    
        # çµ±è¨ˆè¨ˆç®—
        max_peak_height = max([p.height for p in peaks]) if peaks else 0.0
        mean_intensity = np.mean(spectrum.y)
    
        return {
            'n_peaks': len(peaks),
            'max_peak_height': max_peak_height,
            'mean_intensity': mean_intensity,
            'intensity_std': np.std(spectrum.y)
        }
    
    # ãƒãƒƒãƒå‡¦ç†
    processor = BatchProcessor('data_directory')
    files = processor.find_files("*.csv")
    results_df = processor.batch_process(files, batch_statistical_analysis, n_workers=2)
    
    print("çµ±è¨ˆè§£æçµæœ:")
    print(results_df.describe())
    
    
    
    å•é¡Œ6: æ™‚ç³»åˆ—ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æ
    æ™‚é–“çš„ã«é€£ç¶šæ¸¬å®šã•ã‚ŒãŸè¤‡æ•°ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‹ã‚‰ã€ç‰¹å®šãƒ”ãƒ¼ã‚¯ï¼ˆä¾‹: 550 nmï¼‰ã®é«˜ã•ã®æ™‚é–“å¤‰åŒ–ã‚’ãƒ—ãƒ­ãƒƒãƒˆã›ã‚ˆã€‚
    
    è§£ç­”ã‚’è¦‹ã‚‹
    
    è§£ç­”:
    import matplotlib.pyplot as plt
    
    def analyze_time_series_spectra(spectra_list: List[SpectralData],
                                    target_wavelength: float = 550.0) -> np.ndarray:
        """
        æ™‚ç³»åˆ—ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‹ã‚‰ç‰¹å®šæ³¢é•·ã®å¼·åº¦å¤‰åŒ–ã‚’æŠ½å‡º
    
        Parameters:
        -----------
        spectra_list : list of SpectralData
            æ™‚ç³»åˆ—ã‚¹ãƒšã‚¯ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆ
        target_wavelength : float
            è¿½è·¡ã™ã‚‹æ³¢é•· (nm)
    
        Returns:
        --------
        intensities : array
            å„æ™‚åˆ»ã«ãŠã‘ã‚‹å¼·åº¦
        """
        intensities = []
    
        for spectrum in spectra_list:
            # æœ€ã‚‚è¿‘ã„æ³¢é•·ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
            idx = np.argmin(np.abs(spectrum.x - target_wavelength))
            intensities.append(spectrum.y[idx])
    
        return np.array(intensities)
    
    # ä½¿ç”¨ä¾‹ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
    time_points = np.arange(0, 60, 2)  # 0-60åˆ†ã€2åˆ†é–“éš”
    spectra_series = []
    
    for t in time_points:
        x = np.linspace(400, 700, 300)
        # æ™‚é–“ã¨ã¨ã‚‚ã«ãƒ”ãƒ¼ã‚¯é«˜ã•ãŒæ¸›è¡°
        decay = np.exp(-t / 30)
        y = 0.8 * decay * np.exp(-((x - 550)**2) / (2 * 40**2))
        spectra_series.append(SpectralData(x, y, "Wavelength (nm)", "Intensity"))
    
    # 550 nmã®ãƒ”ãƒ¼ã‚¯è¿½è·¡
    intensities = analyze_time_series_spectra(spectra_series, target_wavelength=550)
    
    # ãƒ—ãƒ­ãƒƒãƒˆ
    plt.figure(figsize=(10, 6))
    plt.plot(time_points, intensities, 'o-', linewidth=2, markersize=8)
    plt.xlabel('æ™‚é–“ (åˆ†)', fontsize=12)
    plt.ylabel('å¼·åº¦ @ 550 nm', fontsize=12)
    plt.title('æ™‚ç³»åˆ—ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æï¼šãƒ”ãƒ¼ã‚¯å¼·åº¦ã®æ™‚é–“å¤‰åŒ–', fontsize=14, fontweight='bold')
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    print(f"åˆæœŸå¼·åº¦: {intensities[0]:.3f}")
    print(f"æœ€çµ‚å¼·åº¦: {intensities[-1]:.3f}")
    print(f"æ¸›è¡°ç‡: {(1 - intensities[-1]/intensities[0]) * 100:.1f}%")
    
    
    
    
    
    ä¸Šç´šå•é¡Œï¼ˆHardï¼‰
    å•é¡Œ7: æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ†é¡ï¼ˆCNNï¼‰
    1Dç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆCNNï¼‰ã‚’æ§‹ç¯‰ã—ã€ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’3ã‚¯ãƒ©ã‚¹ã«åˆ†é¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã›ã‚ˆã€‚TensorFlow/Kerasã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚
    
    è§£ç­”ã‚’è¦‹ã‚‹
    
    è§£ç­”:
    # TensorFlow/KerasãŒå¿…è¦: pip install tensorflow
    
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    
    def build_1d_cnn(input_length: int, n_classes: int):
        """
        1D CNNãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
    
        Parameters:
        -----------
        input_length : int
            å…¥åŠ›ã‚¹ãƒšã‚¯ãƒˆãƒ«ã®é•·ã•
        n_classes : int
            ã‚¯ãƒ©ã‚¹æ•°
    
        Returns:
        --------
        model : keras.Model
            CNNãƒ¢ãƒ‡ãƒ«
        """
        model = keras.Sequential([
            # å…¥åŠ›å±¤
            layers.Input(shape=(input_length, 1)),
    
            # ç•³ã¿è¾¼ã¿å±¤1
            layers.Conv1D(filters=32, kernel_size=7, activation='relu'),
            layers.MaxPooling1D(pool_size=2),
            layers.Dropout(0.2),
    
            # ç•³ã¿è¾¼ã¿å±¤2
            layers.Conv1D(filters=64, kernel_size=5, activation='relu'),
            layers.MaxPooling1D(pool_size=2),
            layers.Dropout(0.2),
    
            # ç•³ã¿è¾¼ã¿å±¤3
            layers.Conv1D(filters=128, kernel_size=3, activation='relu'),
            layers.GlobalAveragePooling1D(),
    
            # å…¨çµåˆå±¤
            layers.Dense(64, activation='relu'),
            layers.Dropout(0.3),
    
            # å‡ºåŠ›å±¤
            layers.Dense(n_classes, activation='softmax')
        ])
    
        model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
    
        return model
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
    X_train = []  # shape: (n_samples, seq_length, 1)
    y_train = []
    
    for class_label in range(3):
        for _ in range(100):
            x = np.linspace(0, 1, 100)
            # ã‚¯ãƒ©ã‚¹ã”ã¨ã«ç•°ãªã‚‹ã‚¹ãƒšã‚¯ãƒˆãƒ«å½¢çŠ¶
            if class_label == 0:
                y = np.exp(-((x - 0.5)**2) / 0.1)
            elif class_label == 1:
                y = np.exp(-((x - 0.3)**2) / 0.05) + np.exp(-((x - 0.7)**2) / 0.05)
            else:
                y = np.sin(x * 10) * 0.5 + 0.5
    
            y += np.random.normal(0, 0.05, len(x))
            X_train.append(y)
            y_train.append(class_label)
    
    X_train = np.array(X_train).reshape(-1, 100, 1)
    y_train = np.array(y_train)
    
    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    model = build_1d_cnn(input_length=100, n_classes=3)
    history = model.fit(X_train, y_train, epochs=50, batch_size=32,
                       validation_split=0.2, verbose=0)
    
    print(f"æœ€çµ‚è¨“ç·´ç²¾åº¦: {history.history['accuracy'][-1]:.3f}")
    print(f"æœ€çµ‚æ¤œè¨¼ç²¾åº¦: {history.history['val_accuracy'][-1]:.3f}")
    
    # å­¦ç¿’æ›²ç·šãƒ—ãƒ­ãƒƒãƒˆ
    plt.figure(figsize=(10, 5))
    plt.plot(history.history['accuracy'], label='è¨“ç·´ç²¾åº¦')
    plt.plot(history.history['val_accuracy'], label='æ¤œè¨¼ç²¾åº¦')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('CNNå­¦ç¿’æ›²ç·š')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.show()
    
    ç­”ãˆ: é«˜ç²¾åº¦ï¼ˆ90%ä»¥ä¸Šï¼‰ã§ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ†é¡ã‚’é”æˆ
    
    
    å•é¡Œ8: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒšã‚¯ãƒˆãƒ«ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
    æ–°ã—ã„ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«è¿½åŠ ã•ã‚ŒãŸã‚‰è‡ªå‹•çš„ã«è§£æã‚’å®Ÿè¡Œã—ã€ç•°å¸¸æ¤œçŸ¥ï¼ˆé€šå¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ã®é€¸è„±ï¼‰ã‚’è¡Œã†ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã›ã‚ˆã€‚
    
    è§£ç­”ã‚’è¦‹ã‚‹
    
    è§£ç­”:
    import time
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler
    
    class SpectralMonitor(FileSystemEventHandler):
        """
        ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ãƒ»ç•°å¸¸æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ 
        """
    
        def __init__(self, reference_spectra: List[SpectralData], threshold: float = 0.3):
            """
            Parameters:
            -----------
            reference_spectra : list of SpectralData
                æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®å‚ç…§ã‚¹ãƒšã‚¯ãƒˆãƒ«
            threshold : float
                ç•°å¸¸æ¤œçŸ¥é–¾å€¤ï¼ˆè·é›¢ï¼‰
            """
            self.reference_spectra = reference_spectra
            self.threshold = threshold
    
            # å‚ç…§ã‚¹ãƒšã‚¯ãƒˆãƒ«ã®å¹³å‡ã‚’è¨ˆç®—
            X_ref = np.array([s.y for s in reference_spectra])
            self.reference_mean = np.mean(X_ref, axis=0)
            self.reference_std = np.std(X_ref, axis=0)
    
        def on_created(self, event):
            """ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
            if event.is_directory:
                return
    
            if event.src_path.endswith('.csv'):
                print(f"\næ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º: {event.src_path}")
                time.sleep(0.5)  # ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿å®Œäº†å¾…ã¡
                self.analyze_new_spectrum(event.src_path)
    
        def analyze_new_spectrum(self, filepath: str):
            """æ–°ã—ã„ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’è§£æ"""
            try:
                # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
                loader = SpectralDataLoader()
                spectrum = loader.auto_load(filepath)
    
                # ç•°å¸¸æ¤œçŸ¥ï¼ˆãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ï¼‰
                distance = np.linalg.norm(spectrum.y - self.reference_mean)
    
                if distance > self.threshold:
                    print(f"âš ï¸  ç•°å¸¸æ¤œå‡ºï¼ è·é›¢: {distance:.3f} (é–¾å€¤: {self.threshold:.3f})")
                    print(f"    â†’ ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡: {Path(filepath).name}")
                else:
                    print(f"âœ… æ­£å¸¸ç¯„å›²å†…ã€‚è·é›¢: {distance:.3f}")
    
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ä½¿ç”¨ä¾‹ï¼ˆå®Ÿéš›ã®ä½¿ç”¨æ™‚ã«ã¯ã‚³ãƒ¡ãƒ³ãƒˆè§£é™¤ï¼‰
    """
    # å‚ç…§ãƒ‡ãƒ¼ã‚¿æº–å‚™
    reference_spectra = [...]  # æ­£å¸¸ã‚¹ãƒšã‚¯ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆ
    
    # ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•
    monitor = SpectralMonitor(reference_spectra, threshold=0.3)
    observer = Observer()
    observer.schedule(monitor, path='./watch_directory', recursive=False)
    observer.start()
    
    print("ç›£è¦–é–‹å§‹...")
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
    observer.join()
    """
    
    print("ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå®Ÿè£…å®Œäº†")
    
    ç­”ãˆ: watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ãŸãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
    
    
    å•é¡Œ9: ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
    å¤§é‡ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã€é¡ä¼¼ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¤œç´¢æ©Ÿèƒ½ã‚’å®Ÿè£…ã›ã‚ˆã€‚
    
    è§£ç­”ã‚’è¦‹ã‚‹
    
    è§£ç­”:
    import sqlite3
    import pickle
    
    class SpectralDatabase:
        """
        ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†ã‚¯ãƒ©ã‚¹
        """
    
        def __init__(self, db_path: str = 'spectra.db'):
            """
            Parameters:
            -----------
            db_path : str
                ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            """
            self.db_path = db_path
            self.conn = sqlite3.connect(db_path)
            self._create_table()
    
        def _create_table(self):
            """ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ"""
            cursor = self.conn.cursor()
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS spectra (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    filename TEXT,
                    x_data BLOB,
                    y_data BLOB,
                    metadata TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            self.conn.commit()
    
        def insert_spectrum(self, spectrum: SpectralData):
            """ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’æŒ¿å…¥"""
            cursor = self.conn.cursor()
            cursor.execute("""
                INSERT INTO spectra (filename, x_data, y_data, metadata)
                VALUES (?, ?, ?, ?)
            """, (
                spectrum.metadata.get('source_file', 'unknown'),
                pickle.dumps(spectrum.x),
                pickle.dumps(spectrum.y),
                json.dumps(spectrum.metadata)
            ))
            self.conn.commit()
            return cursor.lastrowid
    
        def search_similar_spectra(self, query_spectrum: SpectralData, top_k: int = 5):
            """
            é¡ä¼¼ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’æ¤œç´¢
    
            Parameters:
            -----------
            query_spectrum : SpectralData
                ã‚¯ã‚¨ãƒªã‚¹ãƒšã‚¯ãƒˆãƒ«
            top_k : int
                ä¸Šä½kä»¶ã‚’è¿”ã™
    
            Returns:
            --------
            results : list of tuples
                (id, filename, similarity_score)
            """
            cursor = self.conn.cursor()
            cursor.execute("SELECT id, filename, y_data FROM spectra")
    
            results = []
            for row in cursor.fetchall():
                id, filename, y_blob = row
                y_data = pickle.loads(y_blob)
    
                # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
                similarity = np.dot(query_spectrum.y, y_data) / \
                            (np.linalg.norm(query_spectrum.y) * np.linalg.norm(y_data))
    
                results.append((id, filename, similarity))
    
            # é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆ
            results.sort(key=lambda x: x[2], reverse=True)
    
            return results[:top_k]
    
        def close(self):
            """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã‚‹"""
            self.conn.close()
    
    
    # ä½¿ç”¨ä¾‹
    db = SpectralDatabase('test_spectra.db')
    
    # ã‚¹ãƒšã‚¯ãƒˆãƒ«æŒ¿å…¥
    for i in range(10):
        x = np.linspace(400, 700, 100)
        y = np.exp(-((x - (500 + i*10))**2) / (2 * 30**2))
        spectrum = SpectralData(x, y, "Wavelength", "Intensity")
        spectrum.metadata['source_file'] = f"sample_{i}.csv"
        db.insert_spectrum(spectrum)
    
    # é¡ä¼¼æ¤œç´¢
    query_x = np.linspace(400, 700, 100)
    query_y = np.exp(-((query_x - 550)**2) / (2 * 30**2))
    query_spectrum = SpectralData(query_x, query_y, "Wavelength", "Intensity")
    
    similar_spectra = db.search_similar_spectra(query_spectrum, top_k=3)
    
    print("é¡ä¼¼ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¤œç´¢çµæœ:")
    for id, filename, score in similar_spectra:
        print(f"  ID: {id}, ãƒ•ã‚¡ã‚¤ãƒ«: {filename}, é¡ä¼¼åº¦: {score:.3f}")
    
    db.close()
    
    ç­”ãˆ: SQLiteã‚’ä½¿ç”¨ã—ãŸã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨é¡ä¼¼æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
    
    
    
    
    å­¦ç¿’ç›®æ¨™ã®ç¢ºèª
    ä»¥ä¸‹ã®é …ç›®ã«ã¤ã„ã¦ã€è‡ªå·±è©•ä¾¡ã—ã¦ãã ã•ã„ï¼š
    ãƒ¬ãƒ™ãƒ«1: åŸºæœ¬ç†è§£
    
    SpectralDataã‚¯ãƒ©ã‚¹ã®åŸºæœ¬çš„ãªä½¿ã„æ–¹ã‚’ç†è§£ã—ã¦ã„ã‚‹
    ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã§CSV/TXTãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚ã‚‹
    è‡ªå‹•ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã®åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç†è§£ã—ã¦ã„ã‚‹
    ãƒãƒƒãƒå‡¦ç†ã®æ¦‚å¿µã‚’ç†è§£ã—ã¦ã„ã‚‹
    
    ãƒ¬ãƒ™ãƒ«2: å®Ÿè·µã‚¹ã‚­ãƒ«
    
    ã‚«ã‚¹ã‚¿ãƒ å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã§ãã‚‹
    è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè£…ã§ãã‚‹
    æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹åˆ†å…‰ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ†é¡ãŒã§ãã‚‹
    Plotlyã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªå¯è¦–åŒ–ãŒã§ãã‚‹
    çµ±åˆè§£æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã§ãã‚‹
    
    ãƒ¬ãƒ™ãƒ«3: å¿œç”¨åŠ›
    
    æ·±å±¤å­¦ç¿’ï¼ˆCNNï¼‰ã«ã‚ˆã‚‹ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ†é¡ãŒã§ãã‚‹
    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ãƒ»ç•°å¸¸æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã§ãã‚‹
    ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨é¡ä¼¼æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹
    å®Ÿéš›ã®ç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«é©ç”¨ã§ãã‚‹
    
    
    
    å‚è€ƒæ–‡çŒ®
    
    McKinney, W. (2017). Python for Data Analysis (2nd ed.). O'Reilly Media, pp. 89-95 (DataFrame operations), pp. 125-145 (data cleaning), pp. 263-270 (time series), pp. 310-325 (aggregation). - Pandasã€NumPyã‚’ç”¨ã„ãŸãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®å®Ÿè·µçš„è§£èª¬
    VanderPlas, J. (2023). Python Data Science Handbook (2nd ed.). O'Reilly Media, pp. 200-225 (NumPy), pp. 280-310 (pandas), pp. 330-365 (matplotlib), pp. 400-435 (scikit-learn), pp. 470-500 (dimensionality reduction). - æ©Ÿæ¢°å­¦ç¿’ã€scikit-learnã€ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ã®åŒ…æ‹¬çš„ã‚¬ã‚¤ãƒ‰
    Eilers, P. H. C., Boelens, H. F. M. (2005). Baseline correction with asymmetric least squares smoothing. Analytical Chemistry, 77(21), 6729-6736. DOI: 10.1021/ac051370e - éå¯¾ç§°æœ€å°äºŒä¹—æ³•ã«ã‚ˆã‚‹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è£œæ­£ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆã‚³ãƒ¼ãƒ‰ä¾‹ã§å®Ÿè£…ï¼‰
    Geladi, P., Kowalski, B. R. (1986). Partial least-squares regression: a tutorial. Analytica Chimica Acta, 185, 1-17. DOI: 10.1016/0003-2670(86)80028-9 - ã‚±ãƒ¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€PLSå›å¸°ã«ã‚ˆã‚‹ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æã®åŸºç¤
    SciPy 1.11 documentation. scipy.signal.find_peaks, scipy.signal.savgol_filter, scipy.signal.peak_widths. https://docs.scipy.org/doc/scipy/reference/signal.html - ä¿¡å·å‡¦ç†ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    scikit-learn 1.3 documentation. RandomForestClassifier, Pipeline, StandardScaler. https://scikit-learn.org/stable/modules/ensemble.html - ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã€æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰
    Plotly 5.x documentation. plotly.express, plotly.graph_objects. https://plotly.com/python/ - ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯è¦–åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
    GÃ©ron, A. (2022). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (3rd ed.). O'Reilly Media, pp. 190-220 (ensemble methods), pp. 450-480 (CNNs), pp. 510-535 (time series with deep learning). - æ·±å±¤å­¦ç¿’ã€CNNã«ã‚ˆã‚‹æ™‚ç³»åˆ—ãƒ»ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿è§£æ
    pandas 2.0 documentation. Chunking large datasets, Dask integration. https://pandas.pydata.org/docs/user_guide/scale.html - å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒãƒå‡¦ç†ã€æœ€é©åŒ–æ‰‹æ³•
    
    
    
    
    å…è²¬äº‹é …
    
    æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
    æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹Code examplesã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚
    å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
    æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
    æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚
    æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚
    ```
