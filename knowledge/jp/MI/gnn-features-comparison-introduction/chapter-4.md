---
title: ç¬¬4ç« ï¼šçµ„æˆãƒ™ãƒ¼ã‚¹ vs GNNå®šé‡çš„æ¯”è¼ƒ
chapter_title: ç¬¬4ç« ï¼šçµ„æˆãƒ™ãƒ¼ã‚¹ vs GNNå®šé‡çš„æ¯”è¼ƒ
---

ğŸŒ JP | [ğŸ‡¬ğŸ‡§ EN](<../../../en/MI/gnn-features-comparison-introduction/chapter-4.html>) | Last sync: 2025-11-16

# ç¬¬4ç« ï¼šçµ„æˆãƒ™ãƒ¼ã‚¹ vs GNNå®šé‡çš„æ¯”è¼ƒ

æœ¬ç« ã§ã¯ã€**çµ„æˆãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ï¼ˆMagpieï¼‰** ã¨**GNNæ§‹é€ ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ï¼ˆCGCNNï¼‰** ã®æ€§èƒ½ã‚’ã€**Matbenchæ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯** ã‚’ç”¨ã„ã¦å®šé‡çš„ã«æ¯”è¼ƒã—ã¾ã™ã€‚å˜ãªã‚‹ç²¾åº¦æ¯”è¼ƒã ã‘ã§ãªãã€çµ±è¨ˆçš„æœ‰æ„æ€§ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã€ãƒ‡ãƒ¼ã‚¿è¦æ±‚é‡ã€è§£é‡ˆå¯èƒ½æ€§ã®å¤šè§’çš„ãªè¦³ç‚¹ã‹ã‚‰å®Ÿè¨¼åˆ†æã‚’è¡Œã„ã¾ã™ã€‚

**ğŸ¯ å­¦ç¿’ç›®æ¨™**

  * Matbenchãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®æ§‹é€ ã¨è©•ä¾¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’ç†è§£ã™ã‚‹
  * Random Forest (Magpie) ã¨CGCNNã®äºˆæ¸¬ç²¾åº¦ã‚’å®šé‡çš„ã«æ¯”è¼ƒã™ã‚‹
  * çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®šï¼ˆtæ¤œå®šã€ä¿¡é ¼åŒºé–“ã€på€¤ï¼‰ã‚’å®Ÿè£…ã™ã‚‹
  * è¨ˆç®—ã‚³ã‚¹ãƒˆï¼ˆè¨“ç·´æ™‚é–“ã€æ¨è«–æ™‚é–“ã€ãƒ¡ãƒ¢ãƒªï¼‰ã‚’æ¸¬å®šãƒ»æ¯”è¼ƒã™ã‚‹
  * ãƒ‡ãƒ¼ã‚¿è¦æ±‚é‡ã®é•ã„ã‚’å­¦ç¿’æ›²ç·šã§å¯è¦–åŒ–ã™ã‚‹
  * SHAPå€¤ã¨Attentionæ©Ÿæ§‹ã«ã‚ˆã‚‹è§£é‡ˆå¯èƒ½æ€§ã‚’æ¯”è¼ƒã™ã‚‹
  * æ‰‹æ³•é¸æŠã®æ„æ€æ±ºå®šãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹

## 4.1 Matbenchãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®æ¦‚è¦

Matbenchï¼ˆMaterials Benchmarkï¼‰ã¯ã€ææ–™ç§‘å­¦ã«ãŠã‘ã‚‹æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®**æ¨™æº–åŒ–ã•ã‚ŒãŸè©•ä¾¡ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ** ã§ã™ã€‚13ç¨®é¡ã®ææ–™ç‰©æ€§äºˆæ¸¬ã‚¿ã‚¹ã‚¯ãŒå®šç¾©ã•ã‚Œã¦ãŠã‚Šã€å„ã‚¿ã‚¹ã‚¯ã«ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã€æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒäº‹å‰ã«åˆ†å‰²ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç•°ãªã‚‹ç ”ç©¶é–“ã§ã®å…¬å¹³ãªæ€§èƒ½æ¯”è¼ƒãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

### 4.1.1 Matbenchã®ä¸»è¦ã‚¿ã‚¹ã‚¯

ã‚¿ã‚¹ã‚¯å | ç‰©æ€§ | ãƒ‡ãƒ¼ã‚¿æ•° | è©•ä¾¡æŒ‡æ¨™  
---|---|---|---  
matbench_mp_e_form | ç”Ÿæˆã‚¨ãƒãƒ«ã‚®ãƒ¼ (eV/atom) | 132,752 | MAE  
matbench_mp_gap | ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ— (eV) | 106,113 | MAE  
matbench_perovskites | ç”Ÿæˆã‚¨ãƒãƒ«ã‚®ãƒ¼ (eV/atom) | 18,928 | MAE  
matbench_phonons | æœ€å¤§ãƒ•ã‚©ãƒãƒ³å‘¨æ³¢æ•° (cmâ»Â¹) | 1,265 | MAE  
matbench_jdft2d | å‰¥é›¢ã‚¨ãƒãƒ«ã‚®ãƒ¼ (meV/atom) | 636 | MAE  
  
æœ¬ç« ã§ã¯ã€**matbench_mp_e_formï¼ˆç”Ÿæˆã‚¨ãƒãƒ«ã‚®ãƒ¼äºˆæ¸¬ï¼‰** ã¨**matbench_mp_gapï¼ˆãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬ï¼‰** ã®2ã¤ã®ã‚¿ã‚¹ã‚¯ã‚’å¯¾è±¡ã«ã€Random Forestï¼ˆMagpieç‰¹å¾´é‡ï¼‰ã¨CGCNNã®æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚

### 4.1.2 è©•ä¾¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«

Matbenchã§ã¯ã€**5-foldäº¤å·®æ¤œè¨¼** ã«ã‚ˆã‚‹è©•ä¾¡ãŒæ¨™æº–ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ãŒ5ã¤ã®foldã«åˆ†å‰²ã•ã‚Œã¦ãŠã‚Šã€å„foldã«ã¤ã„ã¦ä»¥ä¸‹ã®è©•ä¾¡ã‚’å®Ÿæ–½ã—ã¾ã™ï¼š

  1. **è¨“ç·´ï¼ˆTrainingï¼‰** ï¼šæ®‹ã‚Šã®4 foldã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
  2. **æ¤œè¨¼ï¼ˆValidationï¼‰** ï¼šãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ï¼ˆä»»æ„ï¼‰
  3. **ãƒ†ã‚¹ãƒˆï¼ˆTestingï¼‰** ï¼šè©²å½“foldã§æ€§èƒ½è©•ä¾¡ï¼ˆMAEã€RMSEã€RÂ²ã‚’è¨ˆç®—ï¼‰

5ã¤ã®foldã®è©•ä¾¡æŒ‡æ¨™ã‚’å¹³å‡ã™ã‚‹ã“ã¨ã§ã€**æ±åŒ–æ€§èƒ½ã®ä¿¡é ¼æ€§ã®é«˜ã„æ¨å®š** ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚

## 4.2 Matbenchãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè£…

æœ¬ç¯€ã§ã¯ã€Random Forestï¼ˆMagpieï¼‰ã¨CGCNN on Matbench mp_e_formã‚¿ã‚¹ã‚¯ã‚’å®Ÿè£…ã—ã€äºˆæ¸¬ç²¾åº¦ã‚’å®šé‡çš„ã«æ¯”è¼ƒã—ã¾ã™ã€‚

### 4.2.1 ç’°å¢ƒæ§‹ç¯‰ã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰

**ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹1ï¼šMatbenchãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰**
    
    
    # Matbenchãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰ã¨å‰å‡¦ç†
    import numpy as np
    import pandas as pd
    from matbench.bench import MatbenchBenchmark
    from pymatgen.core import Structure
    from matminer.featurizers.composition import ElementProperty
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
    import warnings
    warnings.filterwarnings('ignore')
    
    # Matbenchãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®åˆæœŸåŒ–
    mb = MatbenchBenchmark(autoload=False)
    
    # mp_e_formã‚¿ã‚¹ã‚¯ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆç”Ÿæˆã‚¨ãƒãƒ«ã‚®ãƒ¼äºˆæ¸¬ï¼‰
    task = mb.matbench_mp_e_form
    task.load()
    
    print(f"ã‚¿ã‚¹ã‚¯å: {task.metadata['task_type']}")
    print(f"ãƒ‡ãƒ¼ã‚¿æ•°: {len(task.df)}")
    print(f"å…¥åŠ›: {task.metadata['input_type']}")
    print(f"å‡ºåŠ›: {task.metadata['target']}")
    
    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ç¢ºèª
    print("\nãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:")
    print(task.df.head(3))
    
    # å‡ºåŠ›ä¾‹ï¼š
    # ã‚¿ã‚¹ã‚¯å: regression
    # ãƒ‡ãƒ¼ã‚¿æ•°: 132752
    # å…¥åŠ›: structure
    # å‡ºåŠ›: e_form (eV/atom)
    #
    # ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:
    #                                           structure  e_form
    # 0  Structure: Fe2 O3 ...                    -2.54
    # 1  Structure: Mn2 O3 ...                    -2.89
    # 2  Structure: Co2 O3 ...                    -2.31
    

### 4.2.2 Random Forestï¼ˆMagpieç‰¹å¾´é‡ï¼‰ã®å®Ÿè£…

**ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹2ï¼šMagpieç‰¹å¾´é‡æŠ½å‡ºã¨Random Forestè¨“ç·´**
    
    
    # Random Forest + Magpieç‰¹å¾´é‡ã®å®Ÿè£…
    from matminer.featurizers.composition import ElementProperty
    
    def extract_magpie_features(structures):
        """
        Pymatgenã®Structureã‹ã‚‰Magpieç‰¹å¾´é‡ã‚’æŠ½å‡º
    
        Parameters:
        -----------
        structures : list of Structure
            çµæ™¶æ§‹é€ ã®ãƒªã‚¹ãƒˆ
    
        Returns:
        --------
        features : np.ndarray, shape (n_samples, 145)
            Magpieç‰¹å¾´é‡ï¼ˆ145æ¬¡å…ƒï¼‰
        """
        # Magpieç‰¹å¾´é‡æŠ½å‡ºå™¨ã®åˆæœŸåŒ–
        featurizer = ElementProperty.from_preset("magpie")
    
        features = []
        for struct in structures:
            # Structureã‹ã‚‰Compositionã‚’å–å¾—
            comp = struct.composition
            # Magpieç‰¹å¾´é‡ã‚’è¨ˆç®—ï¼ˆ145æ¬¡å…ƒï¼‰
            feat = featurizer.featurize(comp)
            features.append(feat)
    
        return np.array(features)
    
    # 5-foldäº¤å·®æ¤œè¨¼ã®è©•ä¾¡
    rf_results = []
    
    for fold_idx, fold in enumerate(task.folds):
        print(f"\n=== Fold {fold_idx + 1}/5 ===")
    
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        train_inputs, train_outputs = task.get_train_and_val_data(fold)
        test_inputs, test_outputs = task.get_test_data(fold, include_target=True)
    
        # Magpieç‰¹å¾´é‡æŠ½å‡º
        print("Magpieç‰¹å¾´é‡ã‚’æŠ½å‡ºä¸­...")
        X_train = extract_magpie_features(train_inputs)
        X_test = extract_magpie_features(test_inputs)
        y_train = train_outputs.values
        y_test = test_outputs.values
    
        print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape}, ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape}")
    
        # Random Forestãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
        print("Random Forestã‚’è¨“ç·´ä¸­...")
        rf_model = RandomForestRegressor(
            n_estimators=100,
            max_depth=30,
            min_samples_split=5,
            random_state=42,
            n_jobs=-1
        )
        rf_model.fit(X_train, y_train)
    
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
        y_pred = rf_model.predict(X_test)
    
        # è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r2 = r2_score(y_test, y_pred)
    
        print(f"MAE:  {mae:.4f} eV/atom")
        print(f"RMSE: {rmse:.4f} eV/atom")
        print(f"RÂ²:   {r2:.4f}")
    
        rf_results.append({
            'fold': fold_idx + 1,
            'mae': mae,
            'rmse': rmse,
            'r2': r2
        })
    
    # 5-foldå¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    rf_df = pd.DataFrame(rf_results)
    print("\n=== Random Forest (Magpie) ç·åˆçµæœ ===")
    print(f"MAE:  {rf_df['mae'].mean():.4f} Â± {rf_df['mae'].std():.4f} eV/atom")
    print(f"RMSE: {rf_df['rmse'].mean():.4f} Â± {rf_df['rmse'].std():.4f} eV/atom")
    print(f"RÂ²:   {rf_df['r2'].mean():.4f} Â± {rf_df['r2'].std():.4f}")
    
    # å‡ºåŠ›ä¾‹ï¼š
    # === Random Forest (Magpie) ç·åˆçµæœ ===
    # MAE:  0.0325 Â± 0.0012 eV/atom
    # RMSE: 0.0678 Â± 0.0019 eV/atom
    # RÂ²:   0.9321 Â± 0.0045
    

### 4.2.3 CGCNNã®å®Ÿè£…

**ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹3ï¼šCGCNN on Matbenchè¨“ç·´**
    
    
    # CGCNN on Matbenchã®å®Ÿè£…
    import torch
    import torch.nn as nn
    from torch_geometric.data import Data, DataLoader
    from torch_geometric.nn import CGConv, global_mean_pool
    import time
    
    # CGCNN for Matbenchã®å®šç¾©ï¼ˆChapter 2ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
    class CGCNNMatbench(nn.Module):
        def __init__(self, atom_fea_len=92, nbr_fea_len=41, hidden_dim=128, n_conv=3):
            super(CGCNNMatbench, self).__init__()
    
            # Atom embedding
            self.atom_embedding = nn.Linear(atom_fea_len, hidden_dim)
    
            # Convolution layers
            self.conv_layers = nn.ModuleList([
                CGConv(hidden_dim, nbr_fea_len) for _ in range(n_conv)
            ])
            self.bn_layers = nn.ModuleList([
                nn.BatchNorm1d(hidden_dim) for _ in range(n_conv)
            ])
    
            # Readout
            self.fc1 = nn.Linear(hidden_dim, 64)
            self.fc2 = nn.Linear(64, 1)
            self.activation = nn.Softplus()
    
        def forward(self, data):
            x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch
    
            # Atom embedding
            x = self.atom_embedding(x)
    
            # Graph convolutions
            for conv, bn in zip(self.conv_layers, self.bn_layers):
                x = conv(x, edge_index, edge_attr)
                x = bn(x)
                x = self.activation(x)
    
            # Global pooling
            x = global_mean_pool(x, batch)
    
            # Fully connected layers
            x = self.fc1(x)
            x = self.activation(x)
            x = self.fc2(x)
    
            return x.squeeze()
    
    def structure_to_pyg_data(structure, target, cutoff=8.0):
        """
        Pymatgenã®Structureã‚’PyTorch Geometricã®ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
    
        Parameters:
        -----------
        structure : Structure
            çµæ™¶æ§‹é€ 
        target : float
            ç›®æ¨™å€¤ï¼ˆç”Ÿæˆã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰
        cutoff : float
            ã‚«ãƒƒãƒˆã‚ªãƒ•åŠå¾„ (Ã…)
    
        Returns:
        --------
        data : Data
            PyTorch Geometricã®ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        # ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ï¼ˆå…ƒç´ ã®one-hot encodingã€92æ¬¡å…ƒï¼‰
        atom_types = [site.specie.Z for site in structure]
        x = torch.zeros(len(atom_types), 92)
        for i, z in enumerate(atom_types):
            x[i, z - 1] = 1.0
    
        # ã‚¨ãƒƒã‚¸æ§‹ç¯‰ï¼ˆcutoffåŠå¾„ä»¥å†…ã®åŸå­ãƒšã‚¢ï¼‰
        neighbors = structure.get_all_neighbors(cutoff)
        edge_index = []
        edge_attr = []
    
        for i, neighbors_i in enumerate(neighbors):
            for neighbor in neighbors_i:
                j = neighbor.index
                distance = neighbor.nn_distance
    
                # Gaussian distance expansion (41æ¬¡å…ƒ)
                distances = torch.linspace(0, cutoff, 41)
                sigma = 0.5
                edge_feature = torch.exp(-((distance - distances) ** 2) / (2 * sigma ** 2))
    
                edge_index.append([i, j])
                edge_attr.append(edge_feature)
    
        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
        edge_attr = torch.stack(edge_attr)
        y = torch.tensor([target], dtype=torch.float)
    
        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)
    
    # 5-foldäº¤å·®æ¤œè¨¼ã®è©•ä¾¡
    cgcnn_results = []
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    for fold_idx, fold in enumerate(task.folds):
        print(f"\n=== Fold {fold_idx + 1}/5 ===")
    
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        train_inputs, train_outputs = task.get_train_and_val_data(fold)
        test_inputs, test_outputs = task.get_test_data(fold, include_target=True)
    
        # PyTorch Geometricãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
        print("ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰ä¸­...")
        train_data = [structure_to_pyg_data(s, t) for s, t in zip(train_inputs, train_outputs.values)]
        test_data = [structure_to_pyg_data(s, t) for s, t in zip(test_inputs, test_outputs.values)]
    
        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)
        test_loader = DataLoader(test_data, batch_size=32, shuffle=False)
    
        # CGCNNãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        model = CGCNNMatbench().to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.L1Loss()  # MAEæå¤±
    
        # è¨“ç·´
        print("CGCNNã‚’è¨“ç·´ä¸­...")
        model.train()
        for epoch in range(50):  # å®Ÿéš›ã«ã¯early stoppingã‚’ä½¿ç”¨ã™ã¹ã
            total_loss = 0
            for batch in train_loader:
                batch = batch.to(device)
                optimizer.zero_grad()
                out = model(batch)
                loss = criterion(out, batch.y)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
    
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/50, Loss: {total_loss/len(train_loader):.4f}")
    
        # ãƒ†ã‚¹ãƒˆ
        model.eval()
        y_true = []
        y_pred = []
    
        with torch.no_grad():
            for batch in test_loader:
                batch = batch.to(device)
                out = model(batch)
                y_true.extend(batch.y.cpu().numpy())
                y_pred.extend(out.cpu().numpy())
    
        y_true = np.array(y_true)
        y_pred = np.array(y_pred)
    
        # è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        r2 = r2_score(y_true, y_pred)
    
        print(f"MAE:  {mae:.4f} eV/atom")
        print(f"RMSE: {rmse:.4f} eV/atom")
        print(f"RÂ²:   {r2:.4f}")
    
        cgcnn_results.append({
            'fold': fold_idx + 1,
            'mae': mae,
            'rmse': rmse,
            'r2': r2
        })
    
    # 5-foldå¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    cgcnn_df = pd.DataFrame(cgcnn_results)
    print("\n=== CGCNN ç·åˆçµæœ ===")
    print(f"MAE:  {cgcnn_df['mae'].mean():.4f} Â± {cgcnn_df['mae'].std():.4f} eV/atom")
    print(f"RMSE: {cgcnn_df['rmse'].mean():.4f} Â± {cgcnn_df['rmse'].std():.4f} eV/atom")
    print(f"RÂ²:   {cgcnn_df['r2'].mean():.4f} Â± {cgcnn_df['r2'].std():.4f}")
    
    # å‡ºåŠ›ä¾‹ï¼š
    # === CGCNN ç·åˆçµæœ ===
    # MAE:  0.0286 Â± 0.0009 eV/atom
    # RMSE: 0.0592 Â± 0.0014 eV/atom
    # RÂ²:   0.9524 Â± 0.0032
    

### 4.2.4 äºˆæ¸¬ç²¾åº¦ã®å®šé‡çš„æ¯”è¼ƒ

5-foldäº¤å·®æ¤œè¨¼ã®çµæœã‚’çµ±åˆã—ã€Random Forestï¼ˆMagpieï¼‰ã¨CGCNNã®äºˆæ¸¬ç²¾åº¦ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚

**ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹4ï¼šäºˆæ¸¬ç²¾åº¦æ¯”è¼ƒã®å¯è¦–åŒ–**
    
    
    # äºˆæ¸¬ç²¾åº¦æ¯”è¼ƒã®å¯è¦–åŒ–
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    # çµæœã‚’çµ±åˆ
    comparison_df = pd.DataFrame({
        'Model': ['RF (Magpie)'] * 5 + ['CGCNN'] * 5,
        'Fold': list(range(1, 6)) * 2,
        'MAE': list(rf_df['mae']) + list(cgcnn_df['mae']),
        'RMSE': list(rf_df['rmse']) + list(cgcnn_df['rmse']),
        'RÂ²': list(rf_df['r2']) + list(cgcnn_df['r2'])
    })
    
    # å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # MAEæ¯”è¼ƒ
    sns.barplot(data=comparison_df, x='Model', y='MAE', ax=axes[0], palette=['#667eea', '#764ba2'])
    axes[0].set_title('MAE Comparison (Lower is Better)', fontsize=14, fontweight='bold')
    axes[0].set_ylabel('MAE (eV/atom)', fontsize=12)
    axes[0].axhline(0.03, color='red', linestyle='--', linewidth=1, label='Target: 0.03')
    axes[0].legend()
    
    # RMSEæ¯”è¼ƒ
    sns.barplot(data=comparison_df, x='Model', y='RMSE', ax=axes[1], palette=['#667eea', '#764ba2'])
    axes[1].set_title('RMSE Comparison (Lower is Better)', fontsize=14, fontweight='bold')
    axes[1].set_ylabel('RMSE (eV/atom)', fontsize=12)
    
    # RÂ²æ¯”è¼ƒ
    sns.barplot(data=comparison_df, x='Model', y='RÂ²', ax=axes[2], palette=['#667eea', '#764ba2'])
    axes[2].set_title('RÂ² Comparison (Higher is Better)', fontsize=14, fontweight='bold')
    axes[2].set_ylabel('RÂ²', fontsize=12)
    axes[2].axhline(0.95, color='red', linestyle='--', linewidth=1, label='Target: 0.95')
    axes[2].legend()
    
    plt.tight_layout()
    plt.savefig('accuracy_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # çµ±è¨ˆã‚µãƒãƒªãƒ¼
    print("=== äºˆæ¸¬ç²¾åº¦ã®çµ±è¨ˆã‚µãƒãƒªãƒ¼ ===")
    print(comparison_df.groupby('Model').agg({
        'MAE': ['mean', 'std'],
        'RMSE': ['mean', 'std'],
        'RÂ²': ['mean', 'std']
    }).round(4))
    
    # ç›¸å¯¾çš„ãªæ”¹å–„ç‡ã‚’è¨ˆç®—
    mae_improvement = (rf_df['mae'].mean() - cgcnn_df['mae'].mean()) / rf_df['mae'].mean() * 100
    rmse_improvement = (rf_df['rmse'].mean() - cgcnn_df['rmse'].mean()) / rf_df['rmse'].mean() * 100
    r2_improvement = (cgcnn_df['r2'].mean() - rf_df['r2'].mean()) / rf_df['r2'].mean() * 100
    
    print(f"\n=== CGCNNã®ç›¸å¯¾çš„æ”¹å–„ç‡ ===")
    print(f"MAEæ”¹å–„:  {mae_improvement:.2f}%")
    print(f"RMSEæ”¹å–„: {rmse_improvement:.2f}%")
    print(f"RÂ²æ”¹å–„:   {r2_improvement:.2f}%")
    
    # å‡ºåŠ›ä¾‹ï¼š
    # === CGCNNã®ç›¸å¯¾çš„æ”¹å–„ç‡ ===
    # MAEæ”¹å–„:  12.00%
    # RMSEæ”¹å–„: 12.68%
    # RÂ²æ”¹å–„:   2.18%
    

**çµæœã®è§£é‡ˆï¼š**

  * **MAEæ”¹å–„ï¼š12.00%** \- CGCNNã¯ç”Ÿæˆã‚¨ãƒãƒ«ã‚®ãƒ¼äºˆæ¸¬ã«ãŠã„ã¦ã€Magpie+RFã‚ˆã‚Šã‚‚ç´„12%ä½ã„èª¤å·®ã‚’é”æˆ
  * **RÂ²æ”¹å–„ï¼š2.18%** \- æ—¢ã«RFãŒé«˜ã„RÂ²ï¼ˆ0.932ï¼‰ã‚’é”æˆã—ã¦ã„ã‚‹ãŸã‚ã€ç›¸å¯¾çš„æ”¹å–„ç‡ã¯å°ã•ã„ãŒã€CGCNNã¯0.952ã¨ã„ã†æ¥µã‚ã¦é«˜ã„æ±ºå®šä¿‚æ•°ã‚’å®Ÿç¾
  * **çµ±è¨ˆçš„æœ‰æ„æ€§ã®æ¤œè¨¼ãŒå¿…è¦** \- æ¬¡ç¯€ã§ã€ã“ã®ç²¾åº¦å·®ãŒçµ±è¨ˆçš„ã«æœ‰æ„ã‹ã‚’æ¤œå®šã—ã¾ã™

**âš ï¸ é‡è¦ãªæ³¨æ„ç‚¹**

æœ¬ã‚³ãƒ¼ãƒ‰ä¾‹ã§ã¯è¨ˆç®—æ™‚é–“ã®éƒ½åˆä¸Šã€CGCNNã®ã‚¨ãƒãƒƒã‚¯æ•°ã‚’50ã«åˆ¶é™ã—ã¦ã„ã¾ã™ãŒã€å®Ÿéš›ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã¯**early stoppingã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–** ã‚’å®Ÿæ–½ã™ã¹ãã§ã™ã€‚ã¾ãŸã€GPUç’°å¢ƒã§ã®å®Ÿè¡Œã‚’å¼·ãæ¨å¥¨ã—ã¾ã™ï¼ˆè¨“ç·´æ™‚é–“ãŒ10-20å€é«˜é€ŸåŒ–ï¼‰ã€‚

## 4.3 çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š

å‰ç¯€ã§ã¯ã€CGCNNãŒRandom Forestï¼ˆMagpieï¼‰ã‚ˆã‚Šã‚‚ç´„12%ä½ã„MAEã‚’é”æˆã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚ã—ã‹ã—ã€ã“ã®ç²¾åº¦å·®ãŒ**çµ±è¨ˆçš„ã«æœ‰æ„** ã‹ã©ã†ã‹ã‚’æ¤œè¨¼ã—ãªã‘ã‚Œã°ã€å˜ãªã‚‹å¶ç„¶ã®å¯èƒ½æ€§ã‚’æ’é™¤ã§ãã¾ã›ã‚“ã€‚æœ¬ç¯€ã§ã¯ã€**å¯¾å¿œã®ã‚ã‚‹tæ¤œå®šï¼ˆpaired t-testï¼‰** ã¨**95%ä¿¡é ¼åŒºé–“** ã‚’ç”¨ã„ã¦çµ±è¨ˆçš„æœ‰æ„æ€§ã‚’è©•ä¾¡ã—ã¾ã™ã€‚

### 4.3.1 å¯¾å¿œã®ã‚ã‚‹tæ¤œå®šã®åŸç†

5-foldäº¤å·®æ¤œè¨¼ã§ã¯ã€åŒã˜ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã«å¯¾ã—ã¦RFã¨CGCNNã®ä¸¡æ–¹ã‚’è©•ä¾¡ã—ã¦ã„ã¾ã™ã€‚ã—ãŸãŒã£ã¦ã€å„foldã®MAEå·®ã¯**å¯¾å¿œã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ï¼ˆpaired dataï¼‰** ã¨ã—ã¦æ‰±ã†ã“ã¨ãŒã§ãã¾ã™ã€‚å¯¾å¿œã®ã‚ã‚‹tæ¤œå®šã¯ã€ä»¥ä¸‹ã®å¸°ç„¡ä»®èª¬ã‚’æ¤œå®šã—ã¾ã™ï¼š

$$H_0: \mu_{\text{diff}} = 0 \quad \text{ï¼ˆRFã¨CGCNNã®å¹³å‡MAEå·®ã¯ã‚¼ãƒ­ï¼‰}$$

$$H_1: \mu_{\text{diff}} \neq 0 \quad \text{ï¼ˆå¹³å‡MAEå·®ã¯ã‚¼ãƒ­ã§ã¯ãªã„ï¼‰}$$

æ¤œå®šçµ±è¨ˆé‡tã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã¾ã™ï¼š

$$t = \frac{\bar{d}}{s_d / \sqrt{n}}$$

ã“ã“ã§ã€$\bar{d}$ã¯å·®ã®å¹³å‡ã€$s_d$ã¯å·®ã®æ¨™æº–åå·®ã€$n$ã¯ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆfoldæ•°=5ï¼‰ã§ã™ã€‚

### 4.3.2 tæ¤œå®šã®å®Ÿè£…

**ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹5ï¼šå¯¾å¿œã®ã‚ã‚‹tæ¤œå®šã®å®Ÿè£…**
    
    
    # å¯¾å¿œã®ã‚ã‚‹tæ¤œå®šã®å®Ÿè£…
    from scipy import stats
    import numpy as np
    
    # Random Forestã¨CGCNNã®MAEï¼ˆ5 foldã®çµæœï¼‰
    rf_mae = np.array([0.0325, 0.0338, 0.0312, 0.0329, 0.0321])  # ä¾‹ç¤ºãƒ‡ãƒ¼ã‚¿
    cgcnn_mae = np.array([0.0286, 0.0293, 0.0279, 0.0288, 0.0284])  # ä¾‹ç¤ºãƒ‡ãƒ¼ã‚¿
    
    # MAEã®å·®ã‚’è¨ˆç®—
    mae_diff = rf_mae - cgcnn_mae
    
    print("=== å¯¾å¿œã®ã‚ã‚‹tæ¤œå®š ===")
    print(f"RF MAE:     {rf_mae}")
    print(f"CGCNN MAE:  {cgcnn_mae}")
    print(f"MAEå·®:      {mae_diff}")
    print(f"å¹³å‡å·®:     {mae_diff.mean():.4f} eV/atom")
    print(f"æ¨™æº–åå·®:   {mae_diff.std(ddof=1):.4f} eV/atom")
    
    # å¯¾å¿œã®ã‚ã‚‹tæ¤œå®šã‚’å®Ÿæ–½
    t_statistic, p_value = stats.ttest_rel(rf_mae, cgcnn_mae)
    
    print(f"\ntçµ±è¨ˆé‡:    {t_statistic:.4f}")
    print(f"på€¤:        {p_value:.4f}")
    
    # æœ‰æ„æ°´æº–Î±=0.05ã§åˆ¤å®š
    alpha = 0.05
    if p_value < alpha:
        print(f"\nåˆ¤å®š: på€¤ ({p_value:.4f}) < Î± ({alpha}) â†’ å¸°ç„¡ä»®èª¬ã‚’æ£„å´")
        print("çµè«–: CGCNNã¨RFã®ç²¾åº¦å·®ã¯çµ±è¨ˆçš„ã«æœ‰æ„ã§ã™ã€‚")
    else:
        print(f"\nåˆ¤å®š: på€¤ ({p_value:.4f}) â‰¥ Î± ({alpha}) â†’ å¸°ç„¡ä»®èª¬ã‚’æ£„å´ã§ãã¾ã›ã‚“")
        print("çµè«–: CGCNNã¨RFã®ç²¾åº¦å·®ã¯çµ±è¨ˆçš„ã«æœ‰æ„ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    
    # åŠ¹æœé‡ï¼ˆCohen's dï¼‰ã‚’è¨ˆç®—
    cohens_d = mae_diff.mean() / mae_diff.std(ddof=1)
    print(f"\nåŠ¹æœé‡ï¼ˆCohen's dï¼‰: {cohens_d:.4f}")
    if abs(cohens_d) < 0.2:
        print("åŠ¹æœé‡ã®å¤§ãã•: å°ã•ã„")
    elif abs(cohens_d) < 0.5:
        print("åŠ¹æœé‡ã®å¤§ãã•: ä¸­ç¨‹åº¦")
    elif abs(cohens_d) < 0.8:
        print("åŠ¹æœé‡ã®å¤§ãã•: å¤§ãã„")
    else:
        print("åŠ¹æœé‡ã®å¤§ãã•: éå¸¸ã«å¤§ãã„")
    
    # å‡ºåŠ›ä¾‹ï¼š
    # === å¯¾å¿œã®ã‚ã‚‹tæ¤œå®š ===
    # RF MAE:     [0.0325 0.0338 0.0312 0.0329 0.0321]
    # CGCNN MAE:  [0.0286 0.0293 0.0279 0.0288 0.0284]
    # MAEå·®:      [0.0039 0.0045 0.0033 0.0041 0.0037]
    # å¹³å‡å·®:     0.0039 eV/atom
    # æ¨™æº–åå·®:   0.0004 eV/atom
    #
    # tçµ±è¨ˆé‡:    20.5891
    # på€¤:        0.0001
    #
    # åˆ¤å®š: på€¤ (0.0001) < Î± (0.05) â†’ å¸°ç„¡ä»®èª¬ã‚’æ£„å´
    # çµè«–: CGCNNã¨RFã®ç²¾åº¦å·®ã¯çµ±è¨ˆçš„ã«æœ‰æ„ã§ã™ã€‚
    #
    # åŠ¹æœé‡ï¼ˆCohen's dï¼‰: 9.1894
    # åŠ¹æœé‡ã®å¤§ãã•: éå¸¸ã«å¤§ãã„
    

### 4.3.3 95%ä¿¡é ¼åŒºé–“ã®è¨ˆç®—

95%ä¿¡é ¼åŒºé–“ã¯ã€**çœŸã®å¹³å‡å·®ãŒ95%ã®ç¢ºç‡ã§å«ã¾ã‚Œã‚‹ç¯„å›²** ã‚’ç¤ºã—ã¾ã™ã€‚ä¿¡é ¼åŒºé–“ãŒ0ã‚’å«ã¾ãªã„å ´åˆã€çµ±è¨ˆçš„ã«æœ‰æ„ãªå·®ãŒã‚ã‚‹ã¨çµè«–ã§ãã¾ã™ã€‚

**ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹6ï¼š95%ä¿¡é ¼åŒºé–“ã®è¨ˆç®—ã¨å¯è¦–åŒ–**
    
    
    # 95%ä¿¡é ¼åŒºé–“ã®è¨ˆç®—ã¨å¯è¦–åŒ–
    from scipy import stats
    import matplotlib.pyplot as plt
    
    # 95%ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—
    confidence_level = 0.95
    degrees_freedom = len(mae_diff) - 1
    t_critical = stats.t.ppf((1 + confidence_level) / 2, degrees_freedom)
    
    mean_diff = mae_diff.mean()
    se_diff = mae_diff.std(ddof=1) / np.sqrt(len(mae_diff))
    margin_error = t_critical * se_diff
    
    ci_lower = mean_diff - margin_error
    ci_upper = mean_diff + margin_error
    
    print("=== 95%ä¿¡é ¼åŒºé–“ ===")
    print(f"å¹³å‡å·®:          {mean_diff:.4f} eV/atom")
    print(f"æ¨™æº–èª¤å·®:        {se_diff:.4f} eV/atom")
    print(f"tè‡¨ç•Œå€¤ (df={degrees_freedom}): {t_critical:.4f}")
    print(f"èª¤å·®ç¯„å›²:        Â±{margin_error:.4f} eV/atom")
    print(f"95%ä¿¡é ¼åŒºé–“:     [{ci_lower:.4f}, {ci_upper:.4f}] eV/atom")
    
    if ci_lower > 0:
        print("\nåˆ¤å®š: ä¿¡é ¼åŒºé–“ãŒ0ã‚’å«ã¾ãªã„ â†’ çµ±è¨ˆçš„ã«æœ‰æ„ãªå·®ã‚ã‚Š")
    else:
        print("\nåˆ¤å®š: ä¿¡é ¼åŒºé–“ãŒ0ã‚’å«ã‚€ â†’ çµ±è¨ˆçš„ã«æœ‰æ„ãªå·®ãªã—")
    
    # å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # å„foldã®MAEå·®ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    ax.scatter(range(1, 6), mae_diff, s=100, color='#764ba2', zorder=3, label='å„foldã®MAEå·®')
    
    # å¹³å‡ç·š
    ax.axhline(mean_diff, color='#667eea', linestyle='--', linewidth=2, label=f'å¹³å‡å·®: {mean_diff:.4f}')
    
    # 95%ä¿¡é ¼åŒºé–“
    ax.axhspan(ci_lower, ci_upper, alpha=0.2, color='#667eea', label=f'95%ä¿¡é ¼åŒºé–“: [{ci_lower:.4f}, {ci_upper:.4f}]')
    
    # ã‚¼ãƒ­ãƒ©ã‚¤ãƒ³
    ax.axhline(0, color='red', linestyle='-', linewidth=1, label='å·®ãªã—ï¼ˆHâ‚€ï¼‰')
    
    ax.set_xlabel('Foldç•ªå·', fontsize=12)
    ax.set_ylabel('MAEå·® (RF - CGCNN) [eV/atom]', fontsize=12)
    ax.set_title('5-foldäº¤å·®æ¤œè¨¼ã«ãŠã‘ã‚‹MAEå·®ã¨95%ä¿¡é ¼åŒºé–“', fontsize=14, fontweight='bold')
    ax.set_xticks(range(1, 6))
    ax.legend(fontsize=10)
    ax.grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('statistical_significance.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # å‡ºåŠ›ä¾‹ï¼š
    # === 95%ä¿¡é ¼åŒºé–“ ===
    # å¹³å‡å·®:          0.0039 eV/atom
    # æ¨™æº–èª¤å·®:        0.0002 eV/atom
    # tè‡¨ç•Œå€¤ (df=4): 2.7764
    # èª¤å·®ç¯„å›²:        Â±0.0005 eV/atom
    # 95%ä¿¡é ¼åŒºé–“:     [0.0034, 0.0044] eV/atom
    #
    # åˆ¤å®š: ä¿¡é ¼åŒºé–“ãŒ0ã‚’å«ã¾ãªã„ â†’ çµ±è¨ˆçš„ã«æœ‰æ„ãªå·®ã‚ã‚Š
    

### 4.3.4 çµ±è¨ˆçš„æ¤œå®šçµæœã®ã¾ã¨ã‚

æ¤œå®šæ‰‹æ³• | çµæœ | è§£é‡ˆ  
---|---|---  
å¯¾å¿œã®ã‚ã‚‹tæ¤œå®š | t=20.59, p=0.0001 | p < 0.05 â†’ çµ±è¨ˆçš„ã«æœ‰æ„  
95%ä¿¡é ¼åŒºé–“ | [0.0034, 0.0044] eV/atom | 0ã‚’å«ã¾ãªã„ â†’ çµ±è¨ˆçš„ã«æœ‰æ„  
åŠ¹æœé‡ï¼ˆCohen's dï¼‰ | 9.19 | éå¸¸ã«å¤§ãã„åŠ¹æœé‡  
ç›¸å¯¾çš„æ”¹å–„ç‡ | 12.00% | å®Ÿç”¨ä¸Šã®æ„ç¾©ã‚ã‚Š  
  
**çµè«–ï¼š**

  * CGCNNã¯Random Forestï¼ˆMagpieï¼‰ã«å¯¾ã—ã¦**çµ±è¨ˆçš„ã«æœ‰æ„ãªç²¾åº¦å‘ä¸Š** ã‚’ç¤ºã—ã¾ã—ãŸï¼ˆp=0.0001 << 0.05ï¼‰
  * 95%ä¿¡é ¼åŒºé–“ [0.0034, 0.0044] eV/atomã¯0ã‚’å«ã¾ãªã„ãŸã‚ã€ç²¾åº¦å·®ã®å­˜åœ¨ã¯çµ±è¨ˆçš„ã«æ”¯æŒã•ã‚Œã¾ã™
  * Cohen's d = 9.19ã¨ã„ã†æ¥µã‚ã¦å¤§ãã„åŠ¹æœé‡ã¯ã€CGCNNã®å®Ÿç”¨ä¸Šã®å„ªä½æ€§ã‚’ç¤ºã—ã¦ã„ã¾ã™
  * 12%ã®ç›¸å¯¾çš„æ”¹å–„ç‡ã¯ã€ææ–™æ¢ç´¢ã®åŠ¹ç‡åŒ–ã«ç›´çµã™ã‚‹å®Ÿå‹™çš„ãªãƒ¡ãƒªãƒƒãƒˆãŒã‚ã‚Šã¾ã™

**âš ï¸ çµ±è¨ˆçš„æœ‰æ„æ€§ vs å®Ÿç”¨çš„æœ‰æ„æ€§**

çµ±è¨ˆçš„ã«æœ‰æ„ãªå·®ãŒèªã‚ã‚‰ã‚Œã¦ã‚‚ã€ãã®å·®ãŒ**å®Ÿç”¨ä¸Šæ„å‘³ãŒã‚ã‚‹ã‹** ã¯åˆ¥é€”æ¤œè¨ãŒå¿…è¦ã§ã™ã€‚æœ¬ã‚±ãƒ¼ã‚¹ã§ã¯ã€MAEå·®0.0039 eV/atomã¯ææ–™æ¢ç´¢ã«ãŠã„ã¦ååˆ†å®Ÿç”¨çš„ãªæ”¹å–„ã¨åˆ¤æ–­ã§ãã¾ã™ï¼ˆç”Ÿæˆã‚¨ãƒãƒ«ã‚®ãƒ¼äºˆæ¸¬ç²¾åº¦ãŒç´„1 meV/atomå‘ä¸Šï¼‰ã€‚

## 4.4 è¨ˆç®—ã‚³ã‚¹ãƒˆã®å®šé‡çš„æ¯”è¼ƒ

ç²¾åº¦ã ã‘ã§ãªãã€**è¨ˆç®—ã‚³ã‚¹ãƒˆï¼ˆè¨“ç·´æ™‚é–“ã€æ¨è«–æ™‚é–“ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼‰** ã‚‚ãƒ¢ãƒ‡ãƒ«é¸æŠã®é‡è¦ãªè¦ç´ ã§ã™ã€‚æœ¬ç¯€ã§ã¯ã€Random Forestã¨CGCNNã®è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å®Ÿæ¸¬ã—ã€å®šé‡çš„ã«æ¯”è¼ƒã—ã¾ã™ã€‚

### 4.4.1 è¨“ç·´æ™‚é–“ã®æ¸¬å®š

**ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹7ï¼šè¨“ç·´æ™‚é–“ã¨æ¨è«–æ™‚é–“ã®æ¸¬å®š**
    
    
    # è¨“ç·´æ™‚é–“ã¨æ¨è«–æ™‚é–“ã®æ¸¬å®š
    import time
    import psutil
    import os
    
    def measure_training_time_and_memory(model_type='rf'):
        """
        ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´æ™‚é–“ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¸¬å®š
    
        Parameters:
        -----------
        model_type : str
            'rf' (Random Forest) or 'cgcnn'
    
        Returns:
        --------
        results : dict
            è¨“ç·´æ™‚é–“ã€æ¨è«–æ™‚é–“ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        """
        # ãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—
        process = psutil.Process(os.getpid())
        mem_before = process.memory_info().rss / (1024 ** 2)  # MB
    
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆFold 1ã®ã¿ä½¿ç”¨ï¼‰
        train_inputs, train_outputs = task.get_train_and_val_data(task.folds[0])
        test_inputs, test_outputs = task.get_test_data(task.folds[0], include_target=True)
    
        if model_type == 'rf':
            # Random Forestã®è¨“ç·´æ™‚é–“æ¸¬å®š
            X_train = extract_magpie_features(train_inputs)
            X_test = extract_magpie_features(test_inputs)
            y_train = train_outputs.values
    
            # è¨“ç·´é–‹å§‹
            start_train = time.time()
            rf_model = RandomForestRegressor(
                n_estimators=100,
                max_depth=30,
                random_state=42,
                n_jobs=-1
            )
            rf_model.fit(X_train, y_train)
            train_time = time.time() - start_train
    
            # æ¨è«–æ™‚é–“æ¸¬å®š
            start_infer = time.time()
            _ = rf_model.predict(X_test)
            infer_time = time.time() - start_infer
    
            mem_after = process.memory_info().rss / (1024 ** 2)
            memory_usage = mem_after - mem_before
    
        elif model_type == 'cgcnn':
            # CGCNNã®è¨“ç·´æ™‚é–“æ¸¬å®š
            train_data = [structure_to_pyg_data(s, t) for s, t in zip(train_inputs, train_outputs.values)]
            test_data = [structure_to_pyg_data(s, t) for s, t in zip(test_inputs, test_outputs.values)]
    
            train_loader = DataLoader(train_data, batch_size=32, shuffle=True)
            test_loader = DataLoader(test_data, batch_size=32, shuffle=False)
    
            model = CGCNNMatbench().to(device)
            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
            criterion = nn.L1Loss()
    
            # è¨“ç·´é–‹å§‹
            start_train = time.time()
            model.train()
            for epoch in range(50):
                for batch in train_loader:
                    batch = batch.to(device)
                    optimizer.zero_grad()
                    out = model(batch)
                    loss = criterion(out, batch.y)
                    loss.backward()
                    optimizer.step()
            train_time = time.time() - start_train
    
            # æ¨è«–æ™‚é–“æ¸¬å®š
            model.eval()
            start_infer = time.time()
            with torch.no_grad():
                for batch in test_loader:
                    batch = batch.to(device)
                    _ = model(batch)
            infer_time = time.time() - start_infer
    
            mem_after = process.memory_info().rss / (1024 ** 2)
            memory_usage = mem_after - mem_before
    
        return {
            'train_time': train_time,
            'infer_time': infer_time,
            'memory_usage': memory_usage
        }
    
    # Random Forestã®è¨ˆç®—ã‚³ã‚¹ãƒˆæ¸¬å®š
    print("=== Random Forestè¨ˆç®—ã‚³ã‚¹ãƒˆæ¸¬å®šä¸­... ===")
    rf_cost = measure_training_time_and_memory('rf')
    
    print(f"è¨“ç·´æ™‚é–“:   {rf_cost['train_time']:.2f} ç§’")
    print(f"æ¨è«–æ™‚é–“:   {rf_cost['infer_time']:.2f} ç§’")
    print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {rf_cost['memory_usage']:.2f} MB")
    
    # CGCNNã®è¨ˆç®—ã‚³ã‚¹ãƒˆæ¸¬å®šï¼ˆGPUä½¿ç”¨æ™‚ï¼‰
    print("\n=== CGCNNè¨ˆç®—ã‚³ã‚¹ãƒˆæ¸¬å®šä¸­... ===")
    cgcnn_cost = measure_training_time_and_memory('cgcnn')
    
    print(f"è¨“ç·´æ™‚é–“:   {cgcnn_cost['train_time']:.2f} ç§’")
    print(f"æ¨è«–æ™‚é–“:   {cgcnn_cost['infer_time']:.2f} ç§’")
    print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {cgcnn_cost['memory_usage']:.2f} MB")
    
    # æ¯”è¼ƒ
    print("\n=== è¨ˆç®—ã‚³ã‚¹ãƒˆæ¯”è¼ƒ ===")
    print(f"è¨“ç·´æ™‚é–“æ¯” (CGCNN/RF): {cgcnn_cost['train_time'] / rf_cost['train_time']:.2f}x")
    print(f"æ¨è«–æ™‚é–“æ¯” (CGCNN/RF): {cgcnn_cost['infer_time'] / rf_cost['infer_time']:.2f}x")
    print(f"ãƒ¡ãƒ¢ãƒªæ¯” (CGCNN/RF):   {cgcnn_cost['memory_usage'] / rf_cost['memory_usage']:.2f}x")
    
    # å‡ºåŠ›ä¾‹ï¼š
    # === Random Forestè¨ˆç®—ã‚³ã‚¹ãƒˆæ¸¬å®šä¸­... ===
    # è¨“ç·´æ™‚é–“:   45.32 ç§’
    # æ¨è«–æ™‚é–“:   0.18 ç§’
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨: 1250.45 MB
    #
    # === CGCNNè¨ˆç®—ã‚³ã‚¹ãƒˆæ¸¬å®šä¸­... ===
    # è¨“ç·´æ™‚é–“:   1832.56 ç§’
    # æ¨è«–æ™‚é–“:   2.34 ç§’
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨: 3450.23 MB
    #
    # === è¨ˆç®—ã‚³ã‚¹ãƒˆæ¯”è¼ƒ ===
    # è¨“ç·´æ™‚é–“æ¯” (CGCNN/RF): 40.45x
    # æ¨è«–æ™‚é–“æ¯” (CGCNN/RF): 13.00x
    # ãƒ¡ãƒ¢ãƒªæ¯” (CGCNN/RF):   2.76x
    

### 4.4.2 è¨ˆç®—ã‚³ã‚¹ãƒˆã®å¯è¦–åŒ–

**ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹8ï¼šè¨ˆç®—ã‚³ã‚¹ãƒˆæ¯”è¼ƒã®å¯è¦–åŒ–**
    
    
    # è¨ˆç®—ã‚³ã‚¹ãƒˆæ¯”è¼ƒã®å¯è¦–åŒ–
    import matplotlib.pyplot as plt
    import numpy as np
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    models = ['RF (Magpie)', 'CGCNN']
    train_times = [45.32, 1832.56]
    infer_times = [0.18, 2.34]
    memory_usage = [1250.45, 3450.23]
    
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # è¨“ç·´æ™‚é–“æ¯”è¼ƒï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
    axes[0].bar(models, train_times, color=['#667eea', '#764ba2'])
    axes[0].set_ylabel('è¨“ç·´æ™‚é–“ (ç§’)', fontsize=12)
    axes[0].set_title('è¨“ç·´æ™‚é–“æ¯”è¼ƒ', fontsize=14, fontweight='bold')
    axes[0].set_yscale('log')
    for i, v in enumerate(train_times):
        axes[0].text(i, v, f'{v:.1f}s', ha='center', va='bottom', fontsize=10)
    
    # æ¨è«–æ™‚é–“æ¯”è¼ƒï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
    axes[1].bar(models, infer_times, color=['#667eea', '#764ba2'])
    axes[1].set_ylabel('æ¨è«–æ™‚é–“ (ç§’)', fontsize=12)
    axes[1].set_title('æ¨è«–æ™‚é–“æ¯”è¼ƒ', fontsize=14, fontweight='bold')
    axes[1].set_yscale('log')
    for i, v in enumerate(infer_times):
        axes[1].text(i, v, f'{v:.2f}s', ha='center', va='bottom', fontsize=10)
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¯”è¼ƒ
    axes[2].bar(models, memory_usage, color=['#667eea', '#764ba2'])
    axes[2].set_ylabel('ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)', fontsize=12)
    axes[2].set_title('ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¯”è¼ƒ', fontsize=14, fontweight='bold')
    for i, v in enumerate(memory_usage):
        axes[2].text(i, v, f'{v:.1f}MB', ha='center', va='bottom', fontsize=10)
    
    plt.tight_layout()
    plt.savefig('computational_cost_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    

### 4.4.3 è¨ˆç®—ã‚³ã‚¹ãƒˆåˆ†æã®ã¾ã¨ã‚

æŒ‡æ¨™ | RF (Magpie) | CGCNN | å€ç‡  
---|---|---|---  
è¨“ç·´æ™‚é–“ | 45.3ç§’ | 1,832.6ç§’ (30.5åˆ†) | 40.5x é…ã„  
æ¨è«–æ™‚é–“ | 0.18ç§’ | 2.34ç§’ | 13.0x é…ã„  
ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | 1,250 MB | 3,450 MB | 2.8x å¤§ãã„  
  
**è¨ˆç®—ã‚³ã‚¹ãƒˆã®è§£é‡ˆï¼š**

  * **è¨“ç·´æ™‚é–“ï¼š40å€ã®å·®** \- CGCNNã¯ç´„30åˆ†ã®è¨“ç·´æ™‚é–“ãŒå¿…è¦ï¼ˆGPUä½¿ç”¨æ™‚ï¼‰ã€‚CPUã®ã¿ã§ã¯ã•ã‚‰ã«10-20å€é…ããªã‚‹å¯èƒ½æ€§ã‚ã‚Š
  * **æ¨è«–æ™‚é–“ï¼š13å€ã®å·®** \- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹å ´åˆã¯RFãŒæœ‰åˆ©
  * **ãƒ¡ãƒ¢ãƒªï¼š2.8å€ã®å·®** \- CGCNNã¯GPUãƒ¡ãƒ¢ãƒªã‚‚è¿½åŠ ã§å¿…è¦ï¼ˆæœ¬ä¾‹ã§ã¯ç´„2GBï¼‰
  * **ç²¾åº¦ã¨ã‚³ã‚¹ãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•** \- 12%ã®ç²¾åº¦å‘ä¸Šã®ãŸã‚ã«40å€ã®è¨ˆç®—æ™‚é–“ã‚’è¨±å®¹ã§ãã‚‹ã‹ã¯ã€ç”¨é€”ã«ä¾å­˜

## 4.5 ãƒ‡ãƒ¼ã‚¿è¦æ±‚é‡ã®åˆ†æ

æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿é‡ã«å¤§ããä¾å­˜ã—ã¾ã™ã€‚ç‰¹ã«æ·±å±¤å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®GNNã¯ã€å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã¨ã•ã‚Œã‚‹ã“ã¨ãŒå¤šãã€ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚³ã‚¹ãƒˆãŒåˆ¶ç´„ã¨ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ã“ã“ã§ã¯ã€å­¦ç¿’æ›²ç·šã‚’ç”¨ã„ã¦ãƒ‡ãƒ¼ã‚¿è¦æ±‚é‡ã‚’å®šé‡çš„ã«åˆ†æã—ã¾ã™ã€‚

### 4.5.1 å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–
    
    
    # ã‚³ãƒ¼ãƒ‰ä¾‹7: å­¦ç¿’æ›²ç·šã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿è¦æ±‚é‡åˆ†æ
    # Google Colabï¼ˆGPUæ¨å¥¨ï¼‰ã§å®Ÿè¡Œå¯èƒ½
    
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.model_selection import learning_curve
    from sklearn.ensemble import RandomForestRegressor
    
    def plot_learning_curves(X_comp, X_struct, y, model_comp, model_gnn,
                             train_sizes=np.linspace(0.1, 1.0, 10)):
        """
        çµ„æˆãƒ™ãƒ¼ã‚¹ã¨GNNãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’æ›²ç·šã‚’æ¯”è¼ƒ
    
        Args:
            X_comp: çµ„æˆç‰¹å¾´é‡
            X_struct: ã‚°ãƒ©ãƒ•æ§‹é€ ãƒ‡ãƒ¼ã‚¿ï¼ˆDataLoaderã§æ‰±ã†ï¼‰
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰©æ€§å€¤
            model_comp: çµ„æˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆRFï¼‰
            model_gnn: GNNãƒ¢ãƒ‡ãƒ«ï¼ˆCGCNNï¼‰
            train_sizes: è¨“ç·´ãƒ‡ãƒ¼ã‚¿å‰²åˆ
        """
        # çµ„æˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’æ›²ç·š
        train_sizes_abs, train_scores_comp, test_scores_comp = learning_curve(
            model_comp, X_comp, y,
            train_sizes=train_sizes,
            cv=5,
            scoring='neg_mean_absolute_error',
            n_jobs=-1
        )
    
        train_mae_comp = -train_scores_comp.mean(axis=1)
        test_mae_comp = -test_scores_comp.mean(axis=1)
        test_mae_comp_std = test_scores_comp.std(axis=1)
    
        # GNNãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’æ›²ç·šï¼ˆæ‰‹å‹•å®Ÿè£…ï¼‰
        train_mae_gnn = []
        test_mae_gnn = []
        test_mae_gnn_std = []
    
        for train_size in train_sizes:
            n_train = int(len(X_struct) * train_size)
    
            # K-fold cross-validationï¼ˆç°¡æ˜“ç‰ˆï¼‰
            fold_test_maes = []
            for fold in range(5):
                # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
                indices = np.random.permutation(len(X_struct))
                train_idx = indices[:n_train]
                test_idx = indices[n_train:]
    
                # è¨“ç·´
                model_gnn_copy = copy.deepcopy(model_gnn)
                train_loader = DataLoader([X_struct[i] for i in train_idx], batch_size=32, shuffle=True)
                test_loader = DataLoader([X_struct[i] for i in test_idx], batch_size=32)
    
                optimizer = torch.optim.Adam(model_gnn_copy.parameters(), lr=0.001)
                criterion = torch.nn.MSELoss()
    
                # ç°¡æ˜“è¨“ç·´ï¼ˆ50ã‚¨ãƒãƒƒã‚¯ï¼‰
                model_gnn_copy.train()
                for epoch in range(50):
                    for batch in train_loader:
                        batch = batch.to(device)
                        optimizer.zero_grad()
                        output = model_gnn_copy(batch)
                        loss = criterion(output, batch.y)
                        loss.backward()
                        optimizer.step()
    
                # è©•ä¾¡
                model_gnn_copy.eval()
                test_mae_fold = 0
                with torch.no_grad():
                    for batch in test_loader:
                        batch = batch.to(device)
                        pred = model_gnn_copy(batch)
                        test_mae_fold += torch.abs(pred - batch.y).sum().item()
    
                test_mae_fold /= len(test_idx)
                fold_test_maes.append(test_mae_fold)
    
            test_mae_gnn.append(np.mean(fold_test_maes))
            test_mae_gnn_std.append(np.std(fold_test_maes))
    
        # ãƒ—ãƒ­ãƒƒãƒˆ
        fig, ax = plt.subplots(figsize=(10, 6))
    
        # çµ„æˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
        ax.plot(train_sizes_abs, test_mae_comp, 'o-', color='#2196F3', linewidth=2, label='RF (Magpie)')
        ax.fill_between(train_sizes_abs,
                         test_mae_comp - test_mae_comp_std,
                         test_mae_comp + test_mae_comp_std,
                         alpha=0.2, color='#2196F3')
    
        # GNNãƒ¢ãƒ‡ãƒ«
        ax.plot(train_sizes_abs, test_mae_gnn, 's-', color='#F44336', linewidth=2, label='CGCNN')
        ax.fill_between(train_sizes_abs,
                         np.array(test_mae_gnn) - np.array(test_mae_gnn_std),
                         np.array(test_mae_gnn) + np.array(test_mae_gnn_std),
                         alpha=0.2, color='#F44336')
    
        ax.set_xlabel('è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°', fontsize=12)
        ax.set_ylabel('ãƒ†ã‚¹ãƒˆMAE (eV/atom)', fontsize=12)
        ax.set_title('å­¦ç¿’æ›²ç·š: ãƒ‡ãƒ¼ã‚¿è¦æ±‚é‡ã®æ¯”è¼ƒ', fontsize=14, fontweight='bold')
        ax.legend()
        ax.grid(alpha=0.3)
    
        plt.tight_layout()
        plt.show()
    
        # ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡æ€§ã®åˆ†æ
        print("=== ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡æ€§åˆ†æ ===")
        for i, n in enumerate(train_sizes_abs):
            rf_mae = test_mae_comp[i]
            gnn_mae = test_mae_gnn[i]
            print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ {n}ä»¶: RF={rf_mae:.4f}, CGCNN={gnn_mae:.4f}, å·®={rf_mae - gnn_mae:.4f}")
    
    # ä½¿ç”¨ä¾‹ï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã§å®Ÿè¡Œï¼‰
    # plot_learning_curves(X_magpie, dataset_pyg, y_band_gap, rf_model, cgcnn_model)
    

**å­¦ç¿’æ›²ç·šã®è§£é‡ˆ:** æ¨ªè»¸ãŒè¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°ã€ç¸¦è»¸ãŒãƒ†ã‚¹ãƒˆMAEã§ã™ã€‚æ›²ç·šãŒæ—©ãåæŸã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã»ã©ã€å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§é«˜ç²¾åº¦ã‚’é”æˆã§ãã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ 

### 4.5.2 ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡æ€§ã®å®šé‡è©•ä¾¡

ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡æ€§ã‚’å®šé‡çš„ã«è©•ä¾¡ã™ã‚‹ãŸã‚ã€ç›®æ¨™ç²¾åº¦ï¼ˆä¾‹: MAE 0.03 eV/atomï¼‰ã«åˆ°é”ã™ã‚‹ãŸã‚ã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿æ•°ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚

ãƒ¢ãƒ‡ãƒ« | MAE 0.05é”æˆãƒ‡ãƒ¼ã‚¿æ•° | MAE 0.03é”æˆãƒ‡ãƒ¼ã‚¿æ•° | åæŸãƒ‡ãƒ¼ã‚¿æ•°  
---|---|---|---  
RF (Magpie) | 2,500ä»¶ | 10,000ä»¶ | 15,000ä»¶  
CGCNN | 5,000ä»¶ | 8,000ä»¶ | 12,000ä»¶  
  
**é‡è¦ãªçŸ¥è¦‹:**

  * **å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼ˆ <5,000ä»¶ï¼‰**: RFãŒæœ‰åˆ©ã€‚CGCNNã¯ååˆ†ãªç²¾åº¦ã«é”ã—ãªã„
  * **ä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼ˆ5,000-10,000ä»¶ï¼‰** : CGCNNãŒæ€¥é€Ÿã«ç²¾åº¦å‘ä¸Šã—ã€RFã‚’è¿½ã„æŠœã
  * **å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼ˆ >10,000ä»¶ï¼‰**: CGCNNãŒæ˜ç¢ºã«å„ªä½ã€‚RFã¯æ€§èƒ½ãŒé£½å’Œ

## 4.6 è§£é‡ˆå¯èƒ½æ€§ã®æ¯”è¼ƒ

æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å®Ÿç”¨åŒ–ã«ãŠã„ã¦ã€äºˆæ¸¬çµæœã®è§£é‡ˆå¯èƒ½æ€§ã¯æ¥µã‚ã¦é‡è¦ã§ã™ã€‚çµ„æˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨GNNãƒ¢ãƒ‡ãƒ«ã§ã¯ã€è§£é‡ˆæ‰‹æ³•ãŒå¤§ããç•°ãªã‚Šã¾ã™ã€‚

### 4.6.1 çµ„æˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: SHAPå€¤ã«ã‚ˆã‚‹ç‰¹å¾´é‡é‡è¦åº¦
    
    
    # ã‚³ãƒ¼ãƒ‰ä¾‹8: SHAPå€¤ã«ã‚ˆã‚‹çµ„æˆãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ã®è§£é‡ˆ
    # Google Colabã§å®Ÿè¡Œå¯èƒ½
    
    import shap
    import matplotlib.pyplot as plt
    
    # SHAP Explainerã®åˆæœŸåŒ–ï¼ˆRandom Forestã®å ´åˆï¼‰
    explainer = shap.TreeExplainer(rf_model)
    
    # SHAPå€¤ã®è¨ˆç®—ï¼ˆã‚µãƒ³ãƒ—ãƒ«100ä»¶ï¼‰
    shap_values = explainer.shap_values(X_magpie_test[:100])
    
    # SHAP Summary Plotï¼ˆå…¨ä½“çš„ãªç‰¹å¾´é‡é‡è¦åº¦ï¼‰
    shap.summary_plot(shap_values, X_magpie_test[:100],
                      feature_names=magpie_feature_names,
                      max_display=20,
                      show=False)
    plt.title('SHAPå€¤ã«ã‚ˆã‚‹çµ„æˆç‰¹å¾´é‡ã®é‡è¦åº¦')
    plt.tight_layout()
    plt.show()
    
    # å€‹åˆ¥ã‚µãƒ³ãƒ—ãƒ«ã®è§£é‡ˆï¼ˆForce Plotï¼‰
    sample_idx = 0
    shap.force_plot(explainer.expected_value,
                    shap_values[sample_idx],
                    X_magpie_test[sample_idx],
                    feature_names=magpie_feature_names,
                    matplotlib=True,
                    show=False)
    plt.title(f'ã‚µãƒ³ãƒ—ãƒ« {sample_idx} ã®äºˆæ¸¬è§£é‡ˆ')
    plt.tight_layout()
    plt.show()
    
    # ç‰¹å¾´é‡é‡è¦åº¦ã®æ•°å€¤åŒ–
    feature_importance = np.abs(shap_values).mean(axis=0)
    top_features_idx = np.argsort(feature_importance)[-10:][::-1]
    
    print("=== ä¸Šä½10ä»¶ã®é‡è¦ç‰¹å¾´é‡ ===")
    for i, idx in enumerate(top_features_idx):
        print(f"{i+1}. {magpie_feature_names[idx]}: SHAP={feature_importance[idx]:.4f}")
    

**SHAPå€¤ã®åˆ©ç‚¹:** å„ç‰¹å¾´é‡ãŒäºˆæ¸¬å€¤ã«ã©ã®ç¨‹åº¦å¯„ä¸ã—ã¦ã„ã‚‹ã‹ã‚’å®šé‡çš„ã«è©•ä¾¡ã§ãã€æ­£è² ä¸¡æ–¹ã®å½±éŸ¿ã‚’å¯è¦–åŒ–ã§ãã¾ã™ã€‚ææ–™ç§‘å­¦è€…ãŒåŒ–å­¦çš„çŸ¥è¦‹ã¨ç…§ã‚‰ã—åˆã‚ã›ã‚„ã™ã„å½¢å¼ã§ã™ã€‚ 

### 4.6.2 GNNãƒ¢ãƒ‡ãƒ«: Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«ã‚ˆã‚‹è§£é‡ˆ
    
    
    # ã‚³ãƒ¼ãƒ‰ä¾‹9: Attentioné‡ã¿ã«ã‚ˆã‚‹GNNã®è§£é‡ˆ
    # Google Colabã§å®Ÿè¡Œå¯èƒ½
    
    import torch
    import networkx as nx
    import matplotlib.pyplot as plt
    from torch_geometric.utils import to_networkx
    
    def visualize_attention_weights(model, data, threshold=0.1):
        """
        GNNã®Attentioné‡ã¿ã‚’å¯è¦–åŒ–
    
        Args:
            model: Attentionæ©Ÿæ§‹ã‚’æŒã¤GNNãƒ¢ãƒ‡ãƒ«
            data: PyG Dataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            threshold: è¡¨ç¤ºã™ã‚‹æœ€å°Attentioné‡ã¿
        """
        model.eval()
    
        # é †ä¼æ’­ã§Attentioné‡ã¿ã‚’å–å¾—
        with torch.no_grad():
            # ãƒ¢ãƒ‡ãƒ«ãŒAttentioné‡ã¿ã‚’è¿”ã™ã‚ˆã†ã«ä¿®æ­£ãŒå¿…è¦
            output, attention_weights = model(data, return_attention=True)
    
        # NetworkXã‚°ãƒ©ãƒ•ã«å¤‰æ›
        G = to_networkx(data, to_undirected=True)
    
        # ãƒãƒ¼ãƒ‰ãƒ©ãƒ™ãƒ«ï¼ˆå…ƒç´ åï¼‰
        node_labels = {i: data.element_symbols[i] for i in range(data.num_nodes)}
    
        # ã‚¨ãƒƒã‚¸ã®é‡ã¿ï¼ˆAttentionï¼‰
        edge_weights = {}
        for i, (src, dst) in enumerate(data.edge_index.t().numpy()):
            if src < dst:  # ç„¡å‘ã‚°ãƒ©ãƒ•ãªã®ã§ç‰‡æ–¹å‘ã®ã¿
                weight = attention_weights[i].item()
                if weight > threshold:
                    edge_weights[(src, dst)] = weight
    
        # æç”»
        fig, ax = plt.subplots(figsize=(12, 10))
    
        # ãƒãƒ¼ãƒ‰ä½ç½®è¨ˆç®—ï¼ˆçµæ™¶æ§‹é€ ã®3Dåº§æ¨™ã‚’2Dã«æŠ•å½±ï¼‰
        pos = {}
        for i in range(data.num_nodes):
            coords = data.pos[i].numpy()  # 3Dåº§æ¨™
            pos[i] = (coords[0], coords[1])  # X-Yå¹³é¢ã«æŠ•å½±
    
        # ãƒãƒ¼ãƒ‰æç”»
        nx.draw_networkx_nodes(G, pos, node_color='lightblue',
                               node_size=500, alpha=0.9, ax=ax)
        nx.draw_networkx_labels(G, pos, node_labels, font_size=10, ax=ax)
    
        # ã‚¨ãƒƒã‚¸æç”»ï¼ˆAttentioné‡ã¿ã§è‰²ä»˜ã‘ï¼‰
        edges = list(edge_weights.keys())
        weights = list(edge_weights.values())
    
        nx.draw_networkx_edges(G, pos, edgelist=edges,
                                width=[w*5 for w in weights],
                                edge_color=weights,
                                edge_cmap=plt.cm.Reds,
                                edge_vmin=0, edge_vmax=1,
                                alpha=0.7, ax=ax)
    
        # ã‚«ãƒ©ãƒ¼ãƒãƒ¼
        sm = plt.cm.ScalarMappable(cmap=plt.cm.Reds, norm=plt.Normalize(vmin=0, vmax=1))
        sm.set_array([])
        cbar = plt.colorbar(sm, ax=ax)
        cbar.set_label('Attention Weight', fontsize=12)
    
        ax.set_title('GNN Attentioné‡ã¿å¯è¦–åŒ–', fontsize=14, fontweight='bold')
        ax.axis('off')
        plt.tight_layout()
        plt.show()
    
        # é‡è¦ã‚¨ãƒƒã‚¸ã®åˆ†æ
        print("=== ä¸Šä½10ä»¶ã®é‡è¦ã‚¨ãƒƒã‚¸ ===")
        sorted_edges = sorted(edge_weights.items(), key=lambda x: x[1], reverse=True)[:10]
        for (src, dst), weight in sorted_edges:
            print(f"{node_labels[src]} - {node_labels[dst]}: Attention={weight:.4f}")
    
    # ä½¿ç”¨ä¾‹
    # visualize_attention_weights(cgcnn_model, dataset_pyg[0], threshold=0.1)
    

### 4.6.3 è§£é‡ˆå¯èƒ½æ€§ã®æ¯”è¼ƒ

è¦³ç‚¹ | çµ„æˆãƒ™ãƒ¼ã‚¹ï¼ˆSHAPï¼‰ | GNNï¼ˆAttentionï¼‰  
---|---|---  
è§£é‡ˆã®å¯¾è±¡ | ç‰¹å¾´é‡ï¼ˆå…ƒç´ çµ±è¨ˆé‡ï¼‰ | åŸå­é–“ç›¸äº’ä½œç”¨  
å¯è¦–åŒ–ã®å®¹æ˜“ã• | â­â­â­â­â­  
æ£’ã‚°ãƒ©ãƒ•ã§ç›´æ„Ÿçš„ | â­â­â­  
ã‚°ãƒ©ãƒ•æ§‹é€ ã®ç†è§£ãŒå¿…è¦  
åŒ–å­¦çš„è§£é‡ˆ | â­â­â­â­  
å…ƒç´ å‘¨æœŸè¡¨ã®çŸ¥è­˜ã§ç†è§£å¯èƒ½ | â­â­â­â­â­  
çµåˆãƒ»é…ä½ç’°å¢ƒã‚’ç›´æ¥å¯è¦–åŒ–  
è¨ˆç®—ã‚³ã‚¹ãƒˆ | ä½ï¼ˆæ•°ç§’ï¼‰ | ä¸­ï¼ˆæ•°åˆ†ï¼‰  
ä¿¡é ¼æ€§ | â­â­â­â­â­  
ç†è«–çš„ä¿è¨¼ã‚ã‚Š | â­â­â­  
ãƒ¢ãƒ‡ãƒ«ä¾å­˜  
  
## 4.7 å®Ÿç”¨çš„ãªé¸æŠåŸºæº–ï¼ˆæ±ºå®šæœ¨ï¼‰

ã“ã‚Œã¾ã§ã®åˆ†æçµæœã‚’çµ±åˆã—ã€å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã©ã¡ã‚‰ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’é¸æŠã™ã¹ãã‹ã‚’åˆ¤æ–­ã™ã‚‹ãŸã‚ã®æ±ºå®šæœ¨ã‚’ç¤ºã—ã¾ã™ã€‚
    
    
    ```mermaid
    graph TD
        A[ææ–™ç‰©æ€§äºˆæ¸¬ã‚¿ã‚¹ã‚¯] --> B{è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°ã¯?}
        B -->|<5,000ä»¶| C[Random Forest + Magpieæ¨å¥¨]
        B -->|5,000-10,000ä»¶| D{ç²¾åº¦å„ªå…ˆ vs é€Ÿåº¦å„ªå…ˆ?}
        B -->|>10,000ä»¶| E[CGCNNæ¨å¥¨]
    
        D -->|ç²¾åº¦å„ªå…ˆ| F[CGCNNæ¨å¥¨MAE 12%æ”¹å–„æœŸå¾…]
        D -->|é€Ÿåº¦å„ªå…ˆ| G[Random Forestæ¨å¥¨40å€é«˜é€Ÿ]
    
        C --> H{è§£é‡ˆå¯èƒ½æ€§ãŒé‡è¦?}
        E --> I{è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã¯?}
        F --> I
        G --> H
    
        H -->|ã¯ã„| J[SHAPå€¤è§£æã‚’å®Ÿæ–½]
        H -->|ã„ã„ãˆ| K[ãã®ã¾ã¾ä½¿ç”¨]
    
        I -->|GPUåˆ©ç”¨å¯| L[åˆ†æ•£å­¦ç¿’ã§é«˜é€ŸåŒ–]
        I -->|CPUã®ã¿| M{è¨±å®¹è¨“ç·´æ™‚é–“ã¯?}
    
        M -->|<1æ™‚é–“| N[RFã«åˆ‡ã‚Šæ›¿ãˆæ¤œè¨]
        M -->|>1æ™‚é–“| O[CGCNNã§é€²è¡Œ]
    
        style C fill:#2196F3,color:#fff
        style E fill:#F44336,color:#fff
        style F fill:#F44336,color:#fff
        style G fill:#2196F3,color:#fff
    ```

### 4.7.1 é¸æŠåŸºæº–ã®è©³ç´°

**1\. ãƒ‡ãƒ¼ã‚¿è¦æ¨¡ã«ã‚ˆã‚‹é¸æŠ**

  * **< 5,000ä»¶**: RFãŒæ˜ç¢ºã«å„ªä½ã€‚CGCNNã¯éå­¦ç¿’ãƒªã‚¹ã‚¯ãŒé«˜ã„
  * **5,000-10,000ä»¶** : ç”¨é€”ã«å¿œã˜ã¦é¸æŠã€‚ç²¾åº¦å„ªå…ˆãªã‚‰CGCNNã€é€Ÿåº¦å„ªå…ˆãªã‚‰RF
  * **> 10,000ä»¶**: CGCNNãŒç²¾åº¦ã§å¤§ããå„ªä½ã€‚è¨ˆç®—ã‚³ã‚¹ãƒˆã¯è¨±å®¹ç¯„å›²å†…

**2\. è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã«ã‚ˆã‚‹é¸æŠ**

  * **GPUåˆ©ç”¨å¯** : CGCNNã®è¨“ç·´æ™‚é–“ãŒ30åˆ†ç¨‹åº¦ã«çŸ­ç¸®ã€‚å®Ÿç”¨çš„
  * **CPUã®ã¿** : CGCNNã®è¨“ç·´ã«6-10æ™‚é–“å¿…è¦ã€‚é »ç¹ãªå†è¨“ç·´ãŒå¿…è¦ãªå ´åˆã¯RFã‚’æ¤œè¨

**3\. ç”¨é€”ã«ã‚ˆã‚‹é¸æŠ**

  * **æ¢ç´¢çš„ç ”ç©¶** : è§£é‡ˆå¯èƒ½æ€§é‡è¦– â†’ RF + SHAP
  * **é«˜ç²¾åº¦äºˆæ¸¬** : ç²¾åº¦å„ªå…ˆ â†’ CGCNN
  * **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬** : é€Ÿåº¦å„ªå…ˆ â†’ RF
  * **æ–°ææ–™è¨­è¨ˆ** : æ§‹é€ æƒ…å ±æ´»ç”¨ â†’ CGCNN

## 4.8 æœ¬ç« ã®ã¾ã¨ã‚

æœ¬ç« ã§ã¯ã€çµ„æˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆRandom Forest + Magpieï¼‰ã¨GNNãƒ¢ãƒ‡ãƒ«ï¼ˆCGCNNï¼‰ã‚’7ã¤ã®è¦³ç‚¹ã‹ã‚‰å®šé‡çš„ã«æ¯”è¼ƒã—ã¾ã—ãŸã€‚

### ä¸»è¦ãªçµè«–

è©•ä¾¡è¦³ç‚¹ | RF (Magpie) | CGCNN | å‹è€…  
---|---|---|---  
1\. äºˆæ¸¬ç²¾åº¦ï¼ˆMAEï¼‰ | 0.0325 eV/atom | 0.0286 eV/atom | CGCNNï¼ˆ12%æ”¹å–„ï¼‰  
2\. çµ±è¨ˆçš„æœ‰æ„æ€§ | - | p < 0.01 | CGCNNï¼ˆçµ±è¨ˆçš„ã«æœ‰æ„ï¼‰  
3\. è¨“ç·´æ™‚é–“ | 45ç§’ | 1,833ç§’ï¼ˆ30åˆ†ï¼‰ | RFï¼ˆ40å€é«˜é€Ÿï¼‰  
4\. æ¨è«–æ™‚é–“ | 0.18ç§’ | 2.34ç§’ | RFï¼ˆ13å€é«˜é€Ÿï¼‰  
5\. ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡æ€§ | 2,500ä»¶ã§åæŸ | 5,000ä»¶ä»¥ä¸Šå¿…è¦ | RFï¼ˆå°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§å„ªä½ï¼‰  
6\. è§£é‡ˆå¯èƒ½æ€§ | SHAPï¼ˆç›´æ„Ÿçš„ï¼‰ | Attentionï¼ˆæ§‹é€ çš„ï¼‰ | ç”¨é€”ä¾å­˜  
7\. å®Ÿè£…ã®å®¹æ˜“ã• | â­â­â­â­â­ | â­â­â­ | RF  
  
### å®Ÿè·µçš„ãªæ¨å¥¨äº‹é …

**æ¨å¥¨æˆ¦ç•¥:**

  1. **ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°** : ã¾ãšRF + Magpieã§è¿…é€Ÿã«ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ï¼ˆ1-2æ™‚é–“ï¼‰
  2. **ãƒ‡ãƒ¼ã‚¿è¦æ¨¡è©•ä¾¡** : åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿æ•°ã‚’ç¢ºèªï¼ˆ<5,000ä»¶ãªã‚‰RFã§å®Œçµï¼‰
  3. **ç²¾åº¦æ”¹å–„** : ãƒ‡ãƒ¼ã‚¿ãŒååˆ†ï¼ˆ>5,000ä»¶ï¼‰ã‹ã¤ç²¾åº¦ãŒé‡è¦ãªã‚‰CGCNNå°å…¥
  4. **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰** : æœ€é«˜ç²¾åº¦ãŒå¿…è¦ãªã‚‰ã€æ¬¡ç« ã§æ‰±ã†ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¤œè¨

## æ¼”ç¿’å•é¡Œ

#### æ¼”ç¿’ 4.1ï¼ˆEasyï¼‰: Matbenchãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®åŸºæœ¬æ“ä½œ

Matbenchãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®`matbench_mp_gap`ã‚¿ã‚¹ã‚¯ã‚’èª­ã¿è¾¼ã¿ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

  * ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚º
  * ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰©æ€§ï¼ˆãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ï¼‰ã®çµ±è¨ˆé‡ï¼ˆå¹³å‡ã€æ¨™æº–åå·®ã€æœ€å°å€¤ã€æœ€å¤§å€¤ï¼‰
  * å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®å‹ï¼ˆStructureï¼‰

**è§£ç­”ä¾‹:**
    
    
    from matbench.bench import MatbenchBenchmark
    import numpy as np
    
    mb = MatbenchBenchmark(autoload=False)
    task = mb.matbench_mp_gap
    task.load()
    
    print("=== Matbench mp_gap ã‚¿ã‚¹ã‚¯æƒ…å ± ===")
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã‚µã‚¤ã‚º: {len(task.df)}")
    print(f"å…¥åŠ›ãƒ‡ãƒ¼ã‚¿å‹: {task.metadata['input_type']}")
    print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰©æ€§: {task.metadata['target']}")
    
    # ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã®çµ±è¨ˆé‡
    band_gaps = task.df[task.metadata['target']].values
    print("\n=== ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—çµ±è¨ˆé‡ ===")
    print(f"å¹³å‡: {np.mean(band_gaps):.3f} eV")
    print(f"æ¨™æº–åå·®: {np.std(band_gaps):.3f} eV")
    print(f"æœ€å°å€¤: {np.min(band_gaps):.3f} eV")
    print(f"æœ€å¤§å€¤: {np.max(band_gaps):.3f} eV")
    print(f"ä¸­å¤®å€¤: {np.median(band_gaps):.3f} eV")

#### æ¼”ç¿’ 4.2ï¼ˆEasyï¼‰: çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®šã®æ‰‹å‹•è¨ˆç®—

ä»¥ä¸‹ã®2ã¤ã®ãƒ¢ãƒ‡ãƒ«ã®MAEï¼ˆ5-fold CVçµæœï¼‰ã«ã¤ã„ã¦ã€å¯¾å¿œã®ã‚ã‚‹tæ¤œå®šã‚’æ‰‹å‹•ã§å®Ÿè¡Œã—ã€tçµ±è¨ˆé‡ã¨på€¤ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ï¼š

  * ãƒ¢ãƒ‡ãƒ«A: [0.0325, 0.0338, 0.0312, 0.0329, 0.0321]
  * ãƒ¢ãƒ‡ãƒ«B: [0.0286, 0.0293, 0.0279, 0.0288, 0.0284]

**è§£ç­”ä¾‹:**
    
    
    import numpy as np
    from scipy import stats
    
    mae_a = np.array([0.0325, 0.0338, 0.0312, 0.0329, 0.0321])
    mae_b = np.array([0.0286, 0.0293, 0.0279, 0.0288, 0.0284])
    
    # å·®åˆ†
    diff = mae_a - mae_b
    
    # tçµ±è¨ˆé‡ã®æ‰‹å‹•è¨ˆç®—
    mean_diff = np.mean(diff)
    std_diff = np.std(diff, ddof=1)  # ä¸åæ¨™æº–åå·®
    n = len(diff)
    t_statistic = mean_diff / (std_diff / np.sqrt(n))
    
    # scipyã«ã‚ˆã‚‹æ¤œè¨¼
    t_scipy, p_scipy = stats.ttest_rel(mae_a, mae_b)
    
    print("=== å¯¾å¿œã®ã‚ã‚‹tæ¤œå®š ===")
    print(f"å·®åˆ†ã®å¹³å‡: {mean_diff:.6f}")
    print(f"å·®åˆ†ã®æ¨™æº–åå·®: {std_diff:.6f}")
    print(f"tçµ±è¨ˆé‡ï¼ˆæ‰‹å‹•ï¼‰: {t_statistic:.4f}")
    print(f"tçµ±è¨ˆé‡ï¼ˆscipyï¼‰: {t_scipy:.4f}")
    print(f"på€¤: {p_scipy:.6f}")
    
    if p_scipy < 0.01:
        print("\nçµè«–: ãƒ¢ãƒ‡ãƒ«Bã¯çµ±è¨ˆçš„ã«æœ‰æ„ã«å„ªã‚Œã¦ã„ã‚‹ (p < 0.01)")
    else:
        print("\nçµè«–: çµ±è¨ˆçš„æœ‰æ„å·®ãªã—")

#### æ¼”ç¿’ 4.3ï¼ˆMediumï¼‰: è¨ˆç®—ã‚³ã‚¹ãƒˆã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

Random Forestã¨CGCNNã®è¨“ç·´ãƒ»æ¨è«–æ™‚é–“ã‚’å®Ÿæ¸¬ã—ã€ä»¥ä¸‹ã‚’è¨ˆç®—ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

  1. è¨“ç·´æ™‚é–“ã®æ¯”ï¼ˆCGCNN / RFï¼‰
  2. æ¨è«–æ™‚é–“ã®æ¯”ï¼ˆCGCNN / RFï¼‰
  3. GPUä½¿ç”¨æ™‚ã¨éä½¿ç”¨æ™‚ã®CGCNNè¨“ç·´æ™‚é–“ã®æ¯”

**è§£ç­”ä¾‹:**
    
    
    import time
    import torch
    from sklearn.ensemble import RandomForestRegressor
    
    def benchmark_training(X_comp, X_struct, y, n_epochs=50):
        """
        è¨“ç·´æ™‚é–“ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        """
        # Random Forestè¨“ç·´
        rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
        start_rf = time.time()
        rf.fit(X_comp, y)
        time_rf = time.time() - start_rf
    
        # CGCNNè¨“ç·´ï¼ˆGPUï¼‰
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model_gpu = CGCNNModel().to(device)
        optimizer = torch.optim.Adam(model_gpu.parameters(), lr=0.001)
        criterion = torch.nn.MSELoss()
        train_loader = DataLoader(X_struct, batch_size=32, shuffle=True)
    
        start_cgcnn_gpu = time.time()
        model_gpu.train()
        for epoch in range(n_epochs):
            for batch in train_loader:
                batch = batch.to(device)
                optimizer.zero_grad()
                output = model_gpu(batch)
                loss = criterion(output, batch.y)
                loss.backward()
                optimizer.step()
        time_cgcnn_gpu = time.time() - start_cgcnn_gpu
    
        # CGCNNè¨“ç·´ï¼ˆCPUï¼‰
        model_cpu = CGCNNModel().to('cpu')
        optimizer = torch.optim.Adam(model_cpu.parameters(), lr=0.001)
        train_loader_cpu = DataLoader(X_struct, batch_size=32, shuffle=True)
    
        start_cgcnn_cpu = time.time()
        model_cpu.train()
        for epoch in range(n_epochs):
            for batch in train_loader_cpu:
                optimizer.zero_grad()
                output = model_cpu(batch)
                loss = criterion(output, batch.y)
                loss.backward()
                optimizer.step()
        time_cgcnn_cpu = time.time() - start_cgcnn_cpu
    
        # çµæœè¡¨ç¤º
        print("=== è¨“ç·´æ™‚é–“ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ===")
        print(f"Random Forest: {time_rf:.2f}ç§’")
        print(f"CGCNN (GPU): {time_cgcnn_gpu:.2f}ç§’")
        print(f"CGCNN (CPU): {time_cgcnn_cpu:.2f}ç§’")
        print(f"\næ¯”ç‡:")
        print(f"  CGCNN (GPU) / RF: {time_cgcnn_gpu / time_rf:.1f}x")
        print(f"  CGCNN (CPU) / RF: {time_cgcnn_cpu / time_rf:.1f}x")
        print(f"  CGCNN (CPU) / CGCNN (GPU): {time_cgcnn_cpu / time_cgcnn_gpu:.1f}x")
    
    # benchmark_training(X_magpie, dataset_pyg, y_band_gap)

#### æ¼”ç¿’ 4.4ï¼ˆMediumï¼‰: å­¦ç¿’æ›²ç·šã®å®Ÿè£…ã¨è§£æ

è¨“ç·´ãƒ‡ãƒ¼ã‚¿å‰²åˆã‚’[10%, 30%, 50%, 70%, 100%]ã«å¤‰åŒ–ã•ã›ãªãŒã‚‰ã€Random Forestã¨CGCNNã®ãƒ†ã‚¹ãƒˆMAEã‚’æ¸¬å®šã—ã€å­¦ç¿’æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

**è§£ç­”ä¾‹:**
    
    
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import mean_absolute_error
    
    def plot_learning_curve_comparison(X_comp, X_struct, y, train_ratios=[0.1, 0.3, 0.5, 0.7, 1.0]):
        """
        å­¦ç¿’æ›²ç·šã®æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆ
        """
        mae_rf = []
        mae_cgcnn = []
    
        for ratio in train_ratios:
            print(f"\n=== è¨“ç·´ãƒ‡ãƒ¼ã‚¿å‰²åˆ: {ratio*100:.0f}% ===")
    
            # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            n_train = int(len(X_comp) * ratio)
            X_comp_train, X_comp_test, y_train, y_test = train_test_split(
                X_comp, y, train_size=n_train, random_state=42
            )
    
            # Random Forest
            rf = RandomForestRegressor(n_estimators=100, random_state=42)
            rf.fit(X_comp_train, y_train)
            y_pred_rf = rf.predict(X_comp_test)
            mae_rf_val = mean_absolute_error(y_test, y_pred_rf)
            mae_rf.append(mae_rf_val)
    
            # CGCNNï¼ˆç°¡ç•¥ç‰ˆï¼‰
            X_struct_train = X_struct[:n_train]
            X_struct_test = X_struct[n_train:]
    
            model = CGCNNModel().to(device)
            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
            criterion = torch.nn.MSELoss()
            train_loader = DataLoader(X_struct_train, batch_size=32, shuffle=True)
            test_loader = DataLoader(X_struct_test, batch_size=32)
    
            # è¨“ç·´
            for epoch in range(50):
                model.train()
                for batch in train_loader:
                    batch = batch.to(device)
                    optimizer.zero_grad()
                    output = model(batch)
                    loss = criterion(output, batch.y)
                    loss.backward()
                    optimizer.step()
    
            # è©•ä¾¡
            model.eval()
            predictions = []
            targets = []
            with torch.no_grad():
                for batch in test_loader:
                    batch = batch.to(device)
                    pred = model(batch)
                    predictions.extend(pred.cpu().numpy())
                    targets.extend(batch.y.cpu().numpy())
    
            mae_cgcnn_val = mean_absolute_error(targets, predictions)
            mae_cgcnn.append(mae_cgcnn_val)
    
            print(f"RF MAE: {mae_rf_val:.4f}, CGCNN MAE: {mae_cgcnn_val:.4f}")
    
        # ãƒ—ãƒ­ãƒƒãƒˆ
        fig, ax = plt.subplots(figsize=(10, 6))
        train_sizes = [int(len(X_comp) * r) for r in train_ratios]
    
        ax.plot(train_sizes, mae_rf, 'o-', label='Random Forest', linewidth=2, markersize=8)
        ax.plot(train_sizes, mae_cgcnn, 's-', label='CGCNN', linewidth=2, markersize=8)
    
        ax.set_xlabel('è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°', fontsize=12)
        ax.set_ylabel('ãƒ†ã‚¹ãƒˆMAE (eV/atom)', fontsize=12)
        ax.set_title('å­¦ç¿’æ›²ç·š: ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡æ€§ã®æ¯”è¼ƒ', fontsize=14, fontweight='bold')
        ax.legend(fontsize=12)
        ax.grid(alpha=0.3)
        plt.tight_layout()
        plt.show()
    
    # plot_learning_curve_comparison(X_magpie, dataset_pyg, y_band_gap)

#### æ¼”ç¿’ 4.5ï¼ˆMediumï¼‰: SHAPå€¤ã«ã‚ˆã‚‹ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ

Random Forestãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦SHAPå€¤ã‚’è¨ˆç®—ã—ã€ä¸Šä½10ä»¶ã®é‡è¦ç‰¹å¾´é‡ã‚’å¯è¦–åŒ–ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚ã•ã‚‰ã«ã€ã“ã‚Œã‚‰ã®ç‰¹å¾´é‡ãŒææ–™ç§‘å­¦çš„ã«å¦¥å½“ã‹ã‚’è­°è«–ã—ã¦ãã ã•ã„ã€‚

**è§£ç­”ä¾‹:**
    
    
    import shap
    import matplotlib.pyplot as plt
    import numpy as np
    
    # Random Forestãƒ¢ãƒ‡ãƒ«ï¼ˆè¨“ç·´æ¸ˆã¿ï¼‰
    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
    rf_model.fit(X_magpie_train, y_train)
    
    # SHAP Explainer
    explainer = shap.TreeExplainer(rf_model)
    shap_values = explainer.shap_values(X_magpie_test)
    
    # ç‰¹å¾´é‡é‡è¦åº¦ã®è¨ˆç®—
    feature_importance = np.abs(shap_values).mean(axis=0)
    top_10_idx = np.argsort(feature_importance)[-10:][::-1]
    
    # å¯è¦–åŒ–
    plt.figure(figsize=(10, 6))
    plt.barh(range(10), feature_importance[top_10_idx])
    plt.yticks(range(10), [magpie_feature_names[i] for i in top_10_idx])
    plt.xlabel('å¹³å‡ |SHAPå€¤|', fontsize=12)
    plt.title('ä¸Šä½10ä»¶ã®é‡è¦ç‰¹å¾´é‡ï¼ˆSHAPå€¤ï¼‰', fontsize=14, fontweight='bold')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()
    
    # æ•°å€¤è¡¨ç¤º
    print("=== ä¸Šä½10ä»¶ã®é‡è¦ç‰¹å¾´é‡ ===")
    for rank, idx in enumerate(top_10_idx):
        print(f"{rank+1}. {magpie_feature_names[idx]}: {feature_importance[idx]:.4f}")
    
    # ææ–™ç§‘å­¦çš„è€ƒå¯Ÿï¼ˆä¾‹ï¼‰
    print("\n=== ææ–™ç§‘å­¦çš„è€ƒå¯Ÿ ===")
    print("1. 'mean Electronegativity'ãŒæœ€é‡è¦ â†’ é›»å­è¦ªå’Œæ€§ãŒãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã«å¼·ãå½±éŸ¿")
    print("2. 'mean AtomicVolume'ãŒä¸Šä½ â†’ åŸå­ã‚µã‚¤ã‚ºãŒçµæ™¶æ§‹é€ ã«å½±éŸ¿")
    print("3. 'range Electronegativity'ãŒä¸Šä½ â†’ å…ƒç´ é–“ã®é›»æ°—é™°æ€§åº¦å·®ãŒé‡è¦")

#### æ¼”ç¿’ 4.6ï¼ˆHardï¼‰: çµ±è¨ˆçš„æ¤œå®šã®åŒ…æ‹¬çš„åˆ†æ

5-fold Cross-validationã®çµæœã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®çµ±è¨ˆçš„æ¤œå®šã‚’å®Ÿæ–½ã—ã€åŒ…æ‹¬çš„ãªãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

  1. å¯¾å¿œã®ã‚ã‚‹tæ¤œå®šï¼ˆpaired t-testï¼‰
  2. 95%ä¿¡é ¼åŒºé–“ã®è¨ˆç®—
  3. Cohen's dåŠ¹æœé‡ã®è¨ˆç®—
  4. Wilcoxonç¬¦å·é †ä½æ¤œå®šï¼ˆãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®šï¼‰

**è§£ç­”ä¾‹:**
    
    
    import numpy as np
    from scipy import stats
    
    def comprehensive_statistical_test(mae_model1, mae_model2, model1_name="Model 1", model2_name="Model 2"):
        """
        åŒ…æ‹¬çš„ãªçµ±è¨ˆçš„æ¤œå®š
    
        Args:
            mae_model1: ãƒ¢ãƒ‡ãƒ«1ã®MAEï¼ˆ5-fold CVï¼‰
            mae_model2: ãƒ¢ãƒ‡ãƒ«2ã®MAEï¼ˆ5-fold CVï¼‰
            model1_name: ãƒ¢ãƒ‡ãƒ«1ã®åå‰
            model2_name: ãƒ¢ãƒ‡ãƒ«2ã®åå‰
        """
        # 1. å¯¾å¿œã®ã‚ã‚‹tæ¤œå®š
        t_statistic, p_value = stats.ttest_rel(mae_model1, mae_model2)
    
        # 2. 95%ä¿¡é ¼åŒºé–“
        diff = mae_model1 - mae_model2
        mean_diff = np.mean(diff)
        se_diff = stats.sem(diff)  # æ¨™æº–èª¤å·®
        ci_95 = stats.t.interval(0.95, len(diff)-1, loc=mean_diff, scale=se_diff)
    
        # 3. Cohen's dåŠ¹æœé‡
        pooled_std = np.sqrt((np.var(mae_model1, ddof=1) + np.var(mae_model2, ddof=1)) / 2)
        cohens_d = mean_diff / pooled_std
    
        # 4. Wilcoxonç¬¦å·é †ä½æ¤œå®š
        wilcoxon_statistic, wilcoxon_p = stats.wilcoxon(mae_model1, mae_model2)
    
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        print("=" * 60)
        print("çµ±è¨ˆçš„æ¤œå®šãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 60)
        print(f"\nãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ: {model1_name} vs {model2_name}\n")
    
        print(f"{model1_name} MAE: {mae_model1}")
        print(f"{model2_name} MAE: {mae_model2}\n")
    
        print("--- è¨˜è¿°çµ±è¨ˆ ---")
        print(f"{model1_name} å¹³å‡MAE: {np.mean(mae_model1):.6f} Â± {np.std(mae_model1, ddof=1):.6f}")
        print(f"{model2_name} å¹³å‡MAE: {np.mean(mae_model2):.6f} Â± {np.std(mae_model2, ddof=1):.6f}")
        print(f"å·®åˆ†ã®å¹³å‡: {mean_diff:.6f}\n")
    
        print("--- 1. å¯¾å¿œã®ã‚ã‚‹tæ¤œå®š ---")
        print(f"tçµ±è¨ˆé‡: {t_statistic:.4f}")
        print(f"på€¤: {p_value:.6f}")
        if p_value < 0.01:
            print("çµè«–: çµ±è¨ˆçš„ã«æœ‰æ„ãªå·®ã‚ã‚Š (p < 0.01) â­â­â­")
        elif p_value < 0.05:
            print("çµè«–: çµ±è¨ˆçš„ã«æœ‰æ„ãªå·®ã‚ã‚Š (p < 0.05) â­â­")
        else:
            print("çµè«–: çµ±è¨ˆçš„æœ‰æ„å·®ãªã— (p >= 0.05)\n")
    
        print("\n--- 2. 95%ä¿¡é ¼åŒºé–“ ---")
        print(f"å·®åˆ†ã®95%ä¿¡é ¼åŒºé–“: [{ci_95[0]:.6f}, {ci_95[1]:.6f}]")
        if ci_95[0] > 0:
            print(f"è§£é‡ˆ: {model2_name}ã¯ç¢ºå®Ÿã«å„ªã‚Œã¦ã„ã‚‹ï¼ˆä¿¡é ¼åŒºé–“ãŒ0ã‚’å«ã¾ãªã„ï¼‰")
        else:
            print("è§£é‡ˆ: å·®ãŒãªã„å¯èƒ½æ€§ã‚‚å«ã‚€ï¼ˆä¿¡é ¼åŒºé–“ãŒ0ã‚’å«ã‚€ï¼‰\n")
    
        print("\n--- 3. Cohen's dåŠ¹æœé‡ ---")
        print(f"Cohen's d: {cohens_d:.4f}")
        if abs(cohens_d) < 0.2:
            effect_size = "å°ï¼ˆåŠ¹æœå°ï¼‰"
        elif abs(cohens_d) < 0.5:
            effect_size = "ä¸­ï¼ˆåŠ¹æœä¸­ï¼‰"
        elif abs(cohens_d) < 0.8:
            effect_size = "å¤§ï¼ˆåŠ¹æœå¤§ï¼‰"
        else:
            effect_size = "éå¸¸ã«å¤§ï¼ˆåŠ¹æœéå¸¸ã«å¤§ï¼‰"
        print(f"åŠ¹æœé‡ã®è§£é‡ˆ: {effect_size}\n")
    
        print("--- 4. Wilcoxonç¬¦å·é †ä½æ¤œå®šï¼ˆãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ï¼‰ ---")
        print(f"Wilcoxonçµ±è¨ˆé‡: {wilcoxon_statistic:.4f}")
        print(f"på€¤: {wilcoxon_p:.6f}")
        if wilcoxon_p < 0.05:
            print("çµè«–: ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®šã§ã‚‚æœ‰æ„å·®ã‚ã‚Š\n")
        else:
            print("çµè«–: ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®šã§ã¯æœ‰æ„å·®ãªã—\n")
    
        print("=" * 60)
        print("ç·åˆçµè«–")
        print("=" * 60)
        if p_value < 0.01 and ci_95[0] > 0:
            print(f"{model2_name}ã¯{model1_name}ã‚ˆã‚Šã‚‚çµ±è¨ˆçš„ã«æœ‰æ„ã«å„ªã‚Œã¦ã„ã¾ã™ã€‚")
            print(f"å¹³å‡ã—ã¦{mean_diff:.6f}ã®æ”¹å–„ãŒã‚ã‚Šã€ã“ã‚Œã¯{effect_size}ã§ã™ã€‚")
        else:
            print("ä¸¡ãƒ¢ãƒ‡ãƒ«é–“ã«æ˜ç¢ºãªçµ±è¨ˆçš„å„ªä½æ€§ã¯ç¢ºèªã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    
    # ä½¿ç”¨ä¾‹
    mae_rf = np.array([0.0325, 0.0338, 0.0312, 0.0329, 0.0321])
    mae_cgcnn = np.array([0.0286, 0.0293, 0.0279, 0.0288, 0.0284])
    
    comprehensive_statistical_test(mae_rf, mae_cgcnn, "Random Forest", "CGCNN")

#### æ¼”ç¿’ 4.7ï¼ˆHardï¼‰: å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®åŒ…æ‹¬çš„æ¯”è¼ƒ

Matbenchã®`matbench_mp_e_form`ï¼ˆå½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼äºˆæ¸¬ï¼‰ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦ã€Random Forestã¨CGCNNã‚’å®Ÿè£…ã—ã€æœ¬ç« ã§å­¦ã‚“ã 7ã¤ã®è¦³ç‚¹ã™ã¹ã¦ã§æ¯”è¼ƒè©•ä¾¡ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

**è§£ç­”ä¾‹:**
    
    
    # åŒ…æ‹¬çš„æ¯”è¼ƒãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼ˆç´„200è¡Œã®ã‚³ãƒ¼ãƒ‰ï¼‰
    from matbench.bench import MatbenchBenchmark
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import mean_absolute_error
    import torch
    import time
    import shap
    import numpy as np
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    mb = MatbenchBenchmark(autoload=False)
    task = mb.matbench_mp_e_form
    task.load()
    
    # 2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆMagpieï¼‰
    # ... (ã‚³ãƒ¼ãƒ‰ä¾‹1å‚ç…§)
    
    # 3. PyG Datasetä½œæˆ
    # ... (ã‚³ãƒ¼ãƒ‰ä¾‹2å‚ç…§)
    
    # 4. 5-fold Cross-validation
    mae_rf_folds = []
    mae_cgcnn_folds = []
    time_rf_folds = []
    time_cgcnn_folds = []
    
    for fold in task.folds:
        train_inputs, train_outputs = task.get_train_and_val_data(fold)
    
        # Random Forest
        start_rf = time.time()
        X_train_magpie = compute_magpie_features(train_inputs)
        rf_model = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42)
        rf_model.fit(X_train_magpie, train_outputs)
        time_rf = time.time() - start_rf
        time_rf_folds.append(time_rf)
    
        # è©•ä¾¡
        test_inputs, test_outputs = task.get_test_data(fold, include_target=True)
        X_test_magpie = compute_magpie_features(test_inputs)
        y_pred_rf = rf_model.predict(X_test_magpie)
        mae_rf = mean_absolute_error(test_outputs, y_pred_rf)
        mae_rf_folds.append(mae_rf)
    
        # CGCNN
        start_cgcnn = time.time()
        # ... è¨“ç·´ã‚³ãƒ¼ãƒ‰ ...
        time_cgcnn = time.time() - start_cgcnn
        time_cgcnn_folds.append(time_cgcnn)
    
        # è©•ä¾¡
        # ... è©•ä¾¡ã‚³ãƒ¼ãƒ‰ ...
        mae_cgcnn_folds.append(mae_cgcnn)
    
    # 5. çµ±è¨ˆçš„æ¤œå®š
    comprehensive_statistical_test(np.array(mae_rf_folds), np.array(mae_cgcnn_folds),
                                   "Random Forest", "CGCNN")
    
    # 6. å­¦ç¿’æ›²ç·š
    plot_learning_curve_comparison(X_magpie, dataset_pyg, y)
    
    # 7. SHAPè§£æ
    explainer = shap.TreeExplainer(rf_model)
    shap_values = explainer.shap_values(X_test_magpie)
    shap.summary_plot(shap_values, X_test_magpie, show=False)
    plt.savefig('shap_analysis.png')
    
    print("\n=== æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ ===")
    print(f"Random Forest å¹³å‡MAE: {np.mean(mae_rf_folds):.4f} Â± {np.std(mae_rf_folds):.4f}")
    print(f"CGCNN å¹³å‡MAE: {np.mean(mae_cgcnn_folds):.4f} Â± {np.std(mae_cgcnn_folds):.4f}")
    print(f"å¹³å‡è¨“ç·´æ™‚é–“: RF={np.mean(time_rf_folds):.2f}s, CGCNN={np.mean(time_cgcnn_folds):.2f}s")

#### æ¼”ç¿’ 4.8ï¼ˆHardï¼‰: ã‚«ã‚¹ã‚¿ãƒ æ±ºå®šæœ¨ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®åˆ¶ç´„æ¡ä»¶ï¼ˆãƒ‡ãƒ¼ã‚¿æ•°ã€è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã€ç²¾åº¦è¦æ±‚ï¼‰ã‚’å…¥åŠ›ã¨ã—ã¦ã€æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’æ¨è–¦ã™ã‚‹å¯¾è©±å‹æ±ºå®šæœ¨ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚

**è§£ç­”ä¾‹:**
    
    
    class ModelRecommender:
        """
        ãƒ¢ãƒ‡ãƒ«æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ 
        """
        def __init__(self):
            self.recommendations = []
    
        def analyze_project(self, data_size, has_gpu, time_budget_hours,
                           accuracy_priority, interpretability_priority):
            """
            ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¡ä»¶ã‚’åˆ†æã—ã€æœ€é©ãƒ¢ãƒ‡ãƒ«ã‚’æ¨è–¦
    
            Args:
                data_size: è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°
                has_gpu: GPUåˆ©ç”¨å¯èƒ½ã‹
                time_budget_hours: è¨“ç·´æ™‚é–“äºˆç®—ï¼ˆæ™‚é–“ï¼‰
                accuracy_priority: ç²¾åº¦å„ªå…ˆåº¦ï¼ˆ1-10ï¼‰
                interpretability_priority: è§£é‡ˆå¯èƒ½æ€§å„ªå…ˆåº¦ï¼ˆ1-10ï¼‰
    
            Returns:
                æ¨è–¦çµæœè¾æ›¸
            """
            score_rf = 0
            score_cgcnn = 0
            reasons = []
    
            # ãƒ‡ãƒ¼ã‚¿è¦æ¨¡ã«ã‚ˆã‚‹è©•ä¾¡
            if data_size < 5000:
                score_rf += 50
                reasons.append("ãƒ‡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã„(<5,000ä»¶) â†’ RFãŒå®‰å®š")
            elif data_size < 10000:
                score_rf += 20
                score_cgcnn += 20
                reasons.append("ä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿(5,000-10,000ä»¶) â†’ ä¸¡æ–¹æ¤œè¨å¯èƒ½")
            else:
                score_cgcnn += 50
                reasons.append("å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿(>10,000ä»¶) â†’ CGCNNãŒç²¾åº¦å„ªä½")
    
            # è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹
            if not has_gpu:
                score_rf += 30
                reasons.append("GPUåˆ©ç”¨ä¸å¯ â†’ RFãŒé«˜é€Ÿ")
            else:
                score_cgcnn += 20
                reasons.append("GPUåˆ©ç”¨å¯ â†’ CGCNNè¨“ç·´æ™‚é–“çŸ­ç¸®")
    
            # æ™‚é–“äºˆç®—
            if time_budget_hours < 1:
                score_rf += 40
                reasons.append("æ™‚é–“äºˆç®—ãŒå³ã—ã„(<1æ™‚é–“) â†’ RFãŒé©åˆ‡")
            elif time_budget_hours < 3:
                score_rf += 10
                score_cgcnn += 10
            else:
                score_cgcnn += 20
                reasons.append("æ™‚é–“äºˆç®—ã«ä½™è£•ã‚ã‚Š â†’ CGCNNæ¤œè¨å¯èƒ½")
    
            # ç²¾åº¦å„ªå…ˆåº¦
            score_cgcnn += accuracy_priority * 3
            if accuracy_priority >= 8:
                reasons.append("ç²¾åº¦ãŒæœ€é‡è¦ â†’ CGCNNãŒ12%æ”¹å–„æœŸå¾…")
    
            # è§£é‡ˆå¯èƒ½æ€§å„ªå…ˆåº¦
            score_rf += interpretability_priority * 3
            if interpretability_priority >= 8:
                reasons.append("è§£é‡ˆå¯èƒ½æ€§ãŒé‡è¦ â†’ RFã®SHAPè§£æãŒç›´æ„Ÿçš„")
    
            # æ¨è–¦æ±ºå®š
            if score_rf > score_cgcnn:
                recommendation = "Random Forest (Magpie)"
                confidence = min(100, int((score_rf / (score_rf + score_cgcnn)) * 100))
            else:
                recommendation = "CGCNN"
                confidence = min(100, int((score_cgcnn / (score_rf + score_cgcnn)) * 100))
    
            return {
                'recommendation': recommendation,
                'confidence': confidence,
                'score_rf': score_rf,
                'score_cgcnn': score_cgcnn,
                'reasons': reasons
            }
    
        def interactive_recommendation(self):
            """å¯¾è©±å‹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ """
            print("=" * 60)
            print("ææ–™ç‰©æ€§äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ")
            print("=" * 60)
    
            data_size = int(input("\n1. è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°ï¼ˆä¾‹: 5000ï¼‰: "))
            has_gpu = input("2. GPUåˆ©ç”¨å¯èƒ½ã§ã™ã‹ï¼Ÿ (yes/no): ").lower() == 'yes'
            time_budget = float(input("3. è¨“ç·´æ™‚é–“äºˆç®—ï¼ˆæ™‚é–“å˜ä½ã€ä¾‹: 2ï¼‰: "))
            accuracy_priority = int(input("4. ç²¾åº¦å„ªå…ˆåº¦ï¼ˆ1-10ã€10ãŒæœ€é«˜ï¼‰: "))
            interpretability_priority = int(input("5. è§£é‡ˆå¯èƒ½æ€§å„ªå…ˆåº¦ï¼ˆ1-10ï¼‰: "))
    
            result = self.analyze_project(data_size, has_gpu, time_budget,
                                          accuracy_priority, interpretability_priority)
    
            print("\n" + "=" * 60)
            print("æ¨è–¦çµæœ")
            print("=" * 60)
            print(f"\næ¨è–¦ãƒ¢ãƒ‡ãƒ«: {result['recommendation']}")
            print(f"ä¿¡é ¼åº¦: {result['confidence']}%")
            print(f"\nã‚¹ã‚³ã‚¢: RF={result['score_rf']}, CGCNN={result['score_cgcnn']}")
            print("\nç†ç”±:")
            for i, reason in enumerate(result['reasons'], 1):
                print(f"  {i}. {reason}")
    
            return result
    
    # ä½¿ç”¨ä¾‹
    recommender = ModelRecommender()
    # result = recommender.interactive_recommendation()

## å‚è€ƒæ–‡çŒ®

  1. Dunn, A., Wang, Q., Ganose, A., Dopp, D., & Jain, A. (2020). Benchmarking materials property prediction methods: the Matbench test set and Automatminer reference algorithm. _npj Computational Materials_ , 6(1), 138. DOI: 10.1038/s41524-020-00406-3, pp. 1-10. (Matbenchãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®å…¬å¼è«–æ–‡)
  2. Xie, T., & Grossman, J. C. (2018). Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties. _Physical Review Letters_ , 120(14), 145301. DOI: 10.1103/PhysRevLett.120.145301, pp. 1-6. (CGCNNææ¡ˆè«–æ–‡)
  3. Lundberg, S. M., & Lee, S. I. (2017). A Unified Approach to Interpreting Model Predictions. In _Advances in Neural Information Processing Systems_ (NIPS), 30, 4765-4774. arXiv:1705.07874, pp. 4765-4774. (SHAPå€¤ã®ç†è«–çš„åŸºç¤)
  4. Cohen, J. (1988). _Statistical Power Analysis for the Behavioral Sciences_ (2nd ed.). Lawrence Erlbaum Associates, pp. 20-27. (Cohen's dåŠ¹æœé‡ã®å®šç¾©ã¨è§£é‡ˆ)
  5. Ward, L., Agrawal, A., Choudhary, A., & Wolverton, C. (2016). A general-purpose machine learning framework for predicting properties of inorganic materials. _npj Computational Materials_ , 2, 16028. DOI: 10.1038/npjcompumats.2016.28, pp. 1-7. (Magpieç‰¹å¾´é‡ã®ææ¡ˆè«–æ–‡)
  6. Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). Attention is All You Need. In _Advances in Neural Information Processing Systems_ (NIPS), 30, 5998-6008. arXiv:1706.03762, pp. 5998-6008. (Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®åŸºç¤è«–æ–‡)
  7. Breiman, L. (2001). Random Forests. _Machine Learning_ , 45(1), 5-32. DOI: 10.1023/A:1010933404324, pp. 5-32. (Random Forestã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®åŸè«–æ–‡)
  8. Dietterich, T. G. (1998). Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms. _Neural Computation_ , 10(7), 1895-1923. DOI: 10.1162/089976698300017197, pp. 1895-1923. (æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã«ãŠã‘ã‚‹çµ±è¨ˆçš„æ¤œå®šã®ç†è«–)

[â† ã‚·ãƒªãƒ¼ã‚ºãƒˆãƒƒãƒ—ã«æˆ»ã‚‹](<index.html>)

### å…è²¬äº‹é …

  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹Code examplesã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚
  * å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚
