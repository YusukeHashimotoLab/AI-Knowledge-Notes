---
title: ç¬¬5ç« ï¼šãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
chapter_title: ç¬¬5ç« ï¼šãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
---

ğŸŒ JP | [ğŸ‡¬ğŸ‡§ EN](<../../../en/MI/gnn-features-comparison-introduction/chapter-5.html>) | Last sync: 2025-11-16

# ç¬¬5ç« ï¼šãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

ç¬¬4ç« ã§ã¯ã€çµ„æˆãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ï¼ˆMagpieï¼‰ã¨GNNæ§‹é€ ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ï¼ˆCGCNNï¼‰ã‚’å®šé‡çš„ã«æ¯”è¼ƒã—ã€å„æ‰‹æ³•ã®é•·æ‰€ã¨çŸ­æ‰€ã‚’æ˜ç¢ºã«ã—ã¾ã—ãŸã€‚æœ¬ç« ã§ã¯ã€ã“ã‚Œã‚‰2ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’**çµ±åˆã™ã‚‹ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«** ã‚’æ§‹ç¯‰ã—ã€ã€Œä¸¡æ–¹ã®è‰¯ã„ã¨ã“å–ã‚Šã€ã«ã‚ˆã‚‹æ€§èƒ½å‘ä¸Šã‚’ç›®æŒ‡ã—ã¾ã™ã€‚

**ğŸ¯ å­¦ç¿’ç›®æ¨™**

  * çµ„æˆãƒ™ãƒ¼ã‚¹ã¨GNNæ§‹é€ ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ã‚’çµ±åˆã™ã‚‹ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰¹å¾´é‡ã®è¨­è¨ˆåŸç†ã‚’ç†è§£ã™ã‚‹
  * ALIGNNï¼ˆAtomistic Line Graph Neural Networkï¼‰ã®å®Ÿè£…ã¨æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹
  * MEGNetï¼ˆMaterials Graph Networkï¼‰ã«ã‚ˆã‚‹å¤šç‰©æ€§åŒæ™‚äºˆæ¸¬ã‚’å®Ÿè£…ã™ã‚‹
  * ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«æ±åŒ–æ€§èƒ½ã®å‘ä¸Šã‚’å®Ÿè¨¼ã™ã‚‹
  * Early fusion vs Late fusionã®çµ±åˆæˆ¦ç•¥ã‚’æ¯”è¼ƒã™ã‚‹
  * ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã®å®Ÿç”¨æ€§ã¨é™ç•Œã‚’è©•ä¾¡ã™ã‚‹

## 5.1 ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰¹å¾´é‡ã®è¨­è¨ˆåŸç†

ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ ¸å¿ƒã¯ã€**ç•°ãªã‚‹æƒ…å ±æºã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹ç‰¹å¾´é‡ã‚’åŠ¹æœçš„ã«çµ±åˆ** ã™ã‚‹ã“ã¨ã§ã™ã€‚çµ„æˆãƒ™ãƒ¼ã‚¹ã¨GNNæ§‹é€ ãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡ã¯ã€ç›¸è£œçš„ãªæƒ…å ±ã‚’æŒã£ã¦ã„ã¾ã™ã€‚

### 5.1.1 ç‰¹å¾´é‡ã®ç›¸è£œæ€§

è¦³ç‚¹ | çµ„æˆãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ | GNNæ§‹é€ ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ | ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã®åˆ©ç‚¹  
---|---|---|---  
æƒ…å ±ç²’åº¦ | å…ƒç´ ãƒ¬ãƒ™ãƒ«ï¼ˆå¹³å‡ãƒ»åˆ†æ•£ï¼‰ | åŸå­ãƒ¬ãƒ™ãƒ«ï¼ˆä½ç½®ãƒ»çµåˆï¼‰ | ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«è¡¨ç¾  
ãƒ‡ãƒ¼ã‚¿è¦æ±‚é‡ | å°‘ãªã„ï¼ˆ<10,000ï¼‰ | å¤šã„ï¼ˆ>50,000ï¼‰ | ä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§åŠ¹ç‡åŒ–  
è¨ˆç®—ã‚³ã‚¹ãƒˆ | ä½ã„ï¼ˆç§’ã‚ªãƒ¼ãƒ€ãƒ¼ï¼‰ | é«˜ã„ï¼ˆåˆ†ã‚ªãƒ¼ãƒ€ãƒ¼ï¼‰ | åŠ¹ç‡ã¨ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹  
è§£é‡ˆå¯èƒ½æ€§ | é«˜ã„ï¼ˆå…ƒç´ ç‰¹æ€§ï¼‰ | ä¸­ï¼ˆæ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰ | å¤šè§’çš„ãªè§£é‡ˆ  
æ§‹é€ æ„Ÿåº¦ | ãªã—ï¼ˆåŒç´ ä½“åŒºåˆ¥ä¸å¯ï¼‰ | é«˜ã„ï¼ˆçµæ™¶æ§‹é€ ä¾å­˜ï¼‰ | æ§‹é€ æƒ…å ±ã‚’è€ƒæ…®  
  
### 5.1.2 çµ±åˆæˆ¦ç•¥ã®åˆ†é¡

ç‰¹å¾´é‡ã®çµ±åˆã«ã¯ã€ä¸»ã«3ã¤ã®æˆ¦ç•¥ãŒã‚ã‚Šã¾ã™ï¼š
    
    
    ```mermaid
    graph TD
        A[ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰çµ±åˆæˆ¦ç•¥] --> B[Early Fusionç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«çµ±åˆ]
        A --> C[Late Fusionäºˆæ¸¬ãƒ¬ãƒ™ãƒ«çµ±åˆ]
        A --> D[Intermediate Fusionä¸­é–“å±¤çµ±åˆ]
    
        B --> B1[å˜ç´”é€£çµMagpie + GNN embeddings]
        B --> B2[é‡ã¿ä»˜ãçµåˆAttentionæ©Ÿæ§‹]
    
        C --> C1[å˜ç´”å¹³å‡RFäºˆæ¸¬ + CGCNNäºˆæ¸¬]
        C --> C2[ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«å­¦ç¿’]
    
        D --> D1[ALIGNNLine graph + Atom graph]
        D --> D2[MEGNetMulti-scale aggregation]
    
        style A fill:#667eea,color:#fff
        style B fill:#4caf50,color:#fff
        style C fill:#ff9800,color:#fff
        style D fill:#764ba2,color:#fff
    ```

**Early Fusionï¼ˆç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«çµ±åˆï¼‰** ï¼šçµ„æˆãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ã¨GNNåŸ‹ã‚è¾¼ã¿ã‚’é€£çµã—ã€å˜ä¸€ã®ãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’

$$\mathbf{h}_{\text{hybrid}} = [\mathbf{h}_{\text{composition}}; \mathbf{h}_{\text{GNN}}]$$

**Late Fusionï¼ˆäºˆæ¸¬ãƒ¬ãƒ™ãƒ«çµ±åˆï¼‰** ï¼šå„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’çµ±åˆã—ã¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ã‚’ç”Ÿæˆ

$$\hat{y}_{\text{hybrid}} = \alpha \hat{y}_{\text{RF}} + (1-\alpha) \hat{y}_{\text{CGCNN}}$$

**Intermediate Fusionï¼ˆä¸­é–“å±¤çµ±åˆï¼‰** ï¼šãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ä¸­é–“å±¤ã§ç•°ãªã‚‹è¡¨ç¾ã‚’çµ±åˆ

## 5.2 Early Fusionï¼šç‰¹å¾´é‡é€£çµã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•ã¯ã€çµ„æˆãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ï¼ˆMagpie 145æ¬¡å…ƒï¼‰ã¨GNNåŸ‹ã‚è¾¼ã¿ï¼ˆä¾‹ï¼š128æ¬¡å…ƒï¼‰ã‚’**å˜ç´”ã«é€£çµ** ã™ã‚‹ã“ã¨ã§ã™ã€‚

### 5.2.1 ç‰¹å¾´é‡é€£çµã®å®Ÿè£…

**ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹1ï¼šEarly Fusionï¼ˆç‰¹å¾´é‡é€£çµï¼‰ã®å®Ÿè£…**
    
    
    # Early Fusion: çµ„æˆãƒ™ãƒ¼ã‚¹ + GNNåŸ‹ã‚è¾¼ã¿ã®é€£çµ
    import torch
    import torch.nn as nn
    from torch_geometric.data import Data
    from torch_geometric.nn import CGConv, global_mean_pool
    import numpy as np
    from matminer.featurizers.composition import ElementProperty
    
    class HybridEarlyFusion(nn.Module):
        def __init__(self, composition_dim=145, atom_fea_len=92, nbr_fea_len=41,
                     gnn_hidden=128, n_conv=3):
            super(HybridEarlyFusion, self).__init__()
    
            # GNNéƒ¨åˆ†ï¼ˆCGCNNï¼‰
            self.atom_embedding = nn.Linear(atom_fea_len, gnn_hidden)
            self.conv_layers = nn.ModuleList([
                CGConv(gnn_hidden, nbr_fea_len) for _ in range(n_conv)
            ])
            self.bn_layers = nn.ModuleList([
                nn.BatchNorm1d(gnn_hidden) for _ in range(n_conv)
            ])
    
            # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰çµ±åˆå±¤
            # çµ„æˆç‰¹å¾´é‡ï¼ˆ145æ¬¡å…ƒï¼‰+ GNNåŸ‹ã‚è¾¼ã¿ï¼ˆ128æ¬¡å…ƒï¼‰= 273æ¬¡å…ƒ
            hybrid_dim = composition_dim + gnn_hidden
            self.fc1 = nn.Linear(hybrid_dim, 128)
            self.fc2 = nn.Linear(128, 64)
            self.fc3 = nn.Linear(64, 1)
            self.activation = nn.Softplus()
            self.dropout = nn.Dropout(0.2)
    
        def forward(self, data, composition_features):
            """
            Parameters:
            -----------
            data : torch_geometric.data.Data
                ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ï¼ˆåŸå­ãƒãƒ¼ãƒ‰ã€ã‚¨ãƒƒã‚¸ã€ã‚¨ãƒƒã‚¸ç‰¹å¾´é‡ï¼‰
            composition_features : torch.Tensor, shape (batch_size, 145)
                çµ„æˆãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ï¼ˆMagpieï¼‰
    
            Returns:
            --------
            out : torch.Tensor, shape (batch_size,)
                äºˆæ¸¬å€¤
            """
            x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch
    
            # GNNåŸ‹ã‚è¾¼ã¿ã®è¨ˆç®—
            x = self.atom_embedding(x)
            for conv, bn in zip(self.conv_layers, self.bn_layers):
                x = conv(x, edge_index, edge_attr)
                x = bn(x)
                x = self.activation(x)
    
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼ˆã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«è¡¨ç¾ï¼‰
            gnn_embedding = global_mean_pool(x, batch)  # shape: (batch_size, 128)
    
            # Early Fusion: çµ„æˆç‰¹å¾´é‡ã¨GNNåŸ‹ã‚è¾¼ã¿ã‚’é€£çµ
            hybrid_features = torch.cat([composition_features, gnn_embedding], dim=1)  # (batch_size, 273)
    
            # äºˆæ¸¬å±¤
            h = self.fc1(hybrid_features)
            h = self.activation(h)
            h = self.dropout(h)
            h = self.fc2(h)
            h = self.activation(h)
            h = self.dropout(h)
            out = self.fc3(h)
    
            return out.squeeze()
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™é–¢æ•°
    def prepare_hybrid_data(structures, targets, featurizer):
        """
        PyTorch Geometricãƒ‡ãƒ¼ã‚¿ã¨çµ„æˆç‰¹å¾´é‡ã‚’æº–å‚™
    
        Parameters:
        -----------
        structures : list of Structure
            çµæ™¶æ§‹é€ ã®ãƒªã‚¹ãƒˆ
        targets : np.ndarray
            ç›®æ¨™å€¤
        featurizer : ElementProperty
            Magpieç‰¹å¾´é‡æŠ½å‡ºå™¨
    
        Returns:
        --------
        graph_data : list of Data
            ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        composition_features : torch.Tensor
            çµ„æˆç‰¹å¾´é‡
        """
        graph_data = []
        composition_features = []
    
        for struct, target in zip(structures, targets):
            # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆChapter 4ã®structure_to_pyg_dataé–¢æ•°ã‚’ä½¿ç”¨ï¼‰
            graph = structure_to_pyg_data(struct, target)
            graph_data.append(graph)
    
            # çµ„æˆç‰¹å¾´é‡æŠ½å‡º
            comp = struct.composition
            comp_feat = featurizer.featurize(comp)
            composition_features.append(comp_feat)
    
        composition_features = torch.tensor(composition_features, dtype=torch.float32)
    
        return graph_data, composition_features
    
    # Matbenchã§ã®è¨“ç·´ä¾‹
    from matbench.bench import MatbenchBenchmark
    
    mb = MatbenchBenchmark(autoload=False)
    task = mb.matbench_mp_e_form
    task.load()
    
    # Magpieç‰¹å¾´é‡æŠ½å‡ºå™¨
    featurizer = ElementProperty.from_preset("magpie")
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆFold 0ã®ã¿ï¼‰
    train_inputs, train_outputs = task.get_train_and_val_data(task.folds[0])
    test_inputs, test_outputs = task.get_test_data(task.folds[0], include_target=True)
    
    print("=== ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ä¸­... ===")
    train_graphs, train_comp_feats = prepare_hybrid_data(train_inputs, train_outputs.values, featurizer)
    test_graphs, test_comp_feats = prepare_hybrid_data(test_inputs, test_outputs.values, featurizer)
    
    # ã‚«ã‚¹ã‚¿ãƒ DataLoaderã®å®šç¾©
    from torch.utils.data import Dataset, DataLoader as TorchDataLoader
    from torch_geometric.data import Batch
    
    class HybridDataset(Dataset):
        def __init__(self, graph_data, composition_features):
            self.graph_data = graph_data
            self.composition_features = composition_features
    
        def __len__(self):
            return len(self.graph_data)
    
        def __getitem__(self, idx):
            return self.graph_data[idx], self.composition_features[idx]
    
    def hybrid_collate_fn(batch):
        graphs, comp_feats = zip(*batch)
        batched_graph = Batch.from_data_list(graphs)
        batched_comp_feats = torch.stack(comp_feats)
        return batched_graph, batched_comp_feats
    
    train_dataset = HybridDataset(train_graphs, train_comp_feats)
    test_dataset = HybridDataset(test_graphs, test_comp_feats)
    
    train_loader = TorchDataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=hybrid_collate_fn)
    test_loader = TorchDataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=hybrid_collate_fn)
    
    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = HybridEarlyFusion().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.L1Loss()
    
    print("\n=== ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­... ===")
    model.train()
    for epoch in range(50):
        total_loss = 0
        for batch_graph, batch_comp_feats in train_loader:
            batch_graph = batch_graph.to(device)
            batch_comp_feats = batch_comp_feats.to(device)
    
            optimizer.zero_grad()
            out = model(batch_graph, batch_comp_feats)
            loss = criterion(out, batch_graph.y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
    
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/50, Loss: {total_loss/len(train_loader):.4f}")
    
    # ãƒ†ã‚¹ãƒˆè©•ä¾¡
    model.eval()
    y_true, y_pred = [], []
    
    with torch.no_grad():
        for batch_graph, batch_comp_feats in test_loader:
            batch_graph = batch_graph.to(device)
            batch_comp_feats = batch_comp_feats.to(device)
            out = model(batch_graph, batch_comp_feats)
            y_true.extend(batch_graph.y.cpu().numpy())
            y_pred.extend(out.cpu().numpy())
    
    from sklearn.metrics import mean_absolute_error, r2_score
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    
    print(f"\n=== Hybrid Early Fusionçµæœ ===")
    print(f"MAE:  {mae:.4f} eV/atom")
    print(f"RÂ²:   {r2:.4f}")
    
    # å‡ºåŠ›ä¾‹ï¼š
    # === Hybrid Early Fusionçµæœ ===
    # MAE:  0.0265 eV/atom  # CGCNNå˜ç‹¬ï¼ˆ0.0286ï¼‰ã‚ˆã‚Š7.3%æ”¹å–„
    # RÂ²:   0.9614          # CGCNNå˜ç‹¬ï¼ˆ0.9524ï¼‰ã‚ˆã‚Šå‘ä¸Š
    

### 5.2.2 Early Fusionã®æ€§èƒ½åˆ†æ

**æ€§èƒ½æ¯”è¼ƒï¼ˆMatbench mp_e_formï¼‰ï¼š**

æ‰‹æ³• | MAE (eV/atom) | RÂ² | ç›¸å¯¾æ”¹å–„ç‡  
---|---|---|---  
Random Forestï¼ˆMagpieï¼‰ | 0.0325 | 0.9321 | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³  
CGCNN | 0.0286 | 0.9524 | +12.0%  
**Hybrid Early Fusion** | **0.0265** | **0.9614** | **+18.5%**  
  
**Early Fusionã®åˆ©ç‚¹ï¼š**

  * CGCNNå˜ç‹¬ã‚ˆã‚Š7.3%ç²¾åº¦å‘ä¸Šï¼ˆMAE 0.0286 â†’ 0.0265 eV/atomï¼‰
  * çµ„æˆæƒ…å ±ãŒGNNã®å­¦ç¿’ã‚’è£œåŠ©ã—ã€ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ãŒå‘ä¸Š
  * å®Ÿè£…ãŒå˜ç´”ã§ã€æ—¢å­˜ã®GNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«å®¹æ˜“ã«çµ±åˆå¯èƒ½

**Early Fusionã®èª²é¡Œï¼š**

  * ç‰¹å¾´é‡æ¬¡å…ƒãŒå¢—åŠ ï¼ˆ273æ¬¡å…ƒï¼‰ã—ã€éå­¦ç¿’ãƒªã‚¹ã‚¯ãŒä¸Šæ˜‡
  * çµ„æˆç‰¹å¾´é‡ã¨GNNåŸ‹ã‚è¾¼ã¿ã®ã‚¹ã‚±ãƒ¼ãƒ«ãŒç•°ãªã‚‹å ´åˆã€æ­£è¦åŒ–ãŒå¿…è¦
  * è¨ˆç®—ã‚³ã‚¹ãƒˆã¯CGCNNå˜ç‹¬ã¨ã»ã¼åŒç­‰ï¼ˆçµ„æˆç‰¹å¾´é‡æŠ½å‡ºã¯è»½é‡ï¼‰

## 5.3 Late Fusionï¼šã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬

Late Fusionã¯ã€Random Forestã¨CGCNNã‚’**ç‹¬ç«‹ã«è¨“ç·´** ã—ã€äºˆæ¸¬æ®µéšã§çµ±åˆã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã™ã€‚å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’é‡ã¿ä»˜ãå¹³å‡ã™ã‚‹ã“ã¨ã§ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åŠ¹æœã‚’å¾—ã¾ã™ã€‚

### 5.3.1 Late Fusionã®å®Ÿè£…

**ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹2ï¼šLate Fusionï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ï¼‰ã®å®Ÿè£…**
    
    
    # Late Fusion: Random Forest + CGCNN ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    from sklearn.ensemble import RandomForestRegressor
    import numpy as np
    
    # Random Forestãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ï¼ˆChapter 4ã®ã‚³ãƒ¼ãƒ‰ã‚’å†åˆ©ç”¨ï¼‰
    print("=== Random Forestã‚’è¨“ç·´ä¸­... ===")
    X_train_magpie = extract_magpie_features(train_inputs)
    X_test_magpie = extract_magpie_features(test_inputs)
    y_train = train_outputs.values
    y_test = test_outputs.values
    
    rf_model = RandomForestRegressor(n_estimators=100, max_depth=30, random_state=42, n_jobs=-1)
    rf_model.fit(X_train_magpie, y_train)
    
    # Random Forestã®äºˆæ¸¬
    rf_pred_train = rf_model.predict(X_train_magpie)
    rf_pred_test = rf_model.predict(X_test_magpie)
    
    # CGCNNãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ï¼ˆChapter 4ã®ã‚³ãƒ¼ãƒ‰ã‚’å†åˆ©ç”¨ï¼‰
    print("\n=== CGCNNã‚’è¨“ç·´ä¸­... ===")
    train_data_cgcnn = [structure_to_pyg_data(s, t) for s, t in zip(train_inputs, y_train)]
    test_data_cgcnn = [structure_to_pyg_data(s, t) for s, t in zip(test_inputs, y_test)]
    
    train_loader_cgcnn = DataLoader(train_data_cgcnn, batch_size=32, shuffle=True)
    test_loader_cgcnn = DataLoader(test_data_cgcnn, batch_size=32, shuffle=False)
    
    cgcnn_model = CGCNNMatbench().to(device)
    optimizer_cgcnn = torch.optim.Adam(cgcnn_model.parameters(), lr=0.001)
    criterion = nn.L1Loss()
    
    # CGCNNè¨“ç·´ï¼ˆç°¡ç•¥ç‰ˆï¼š30ã‚¨ãƒãƒƒã‚¯ï¼‰
    cgcnn_model.train()
    for epoch in range(30):
        for batch in train_loader_cgcnn:
            batch = batch.to(device)
            optimizer_cgcnn.zero_grad()
            out = cgcnn_model(batch)
            loss = criterion(out, batch.y)
            loss.backward()
            optimizer_cgcnn.step()
    
    # CGCNNã®äºˆæ¸¬
    cgcnn_model.eval()
    cgcnn_pred_train, cgcnn_pred_test = [], []
    
    with torch.no_grad():
        for batch in train_loader_cgcnn:
            batch = batch.to(device)
            out = cgcnn_model(batch)
            cgcnn_pred_train.extend(out.cpu().numpy())
    
        for batch in test_loader_cgcnn:
            batch = batch.to(device)
            out = cgcnn_model(batch)
            cgcnn_pred_test.extend(out.cpu().numpy())
    
    cgcnn_pred_train = np.array(cgcnn_pred_train)
    cgcnn_pred_test = np.array(cgcnn_pred_test)
    
    # æœ€é©ãªé‡ã¿Î±ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§æ¢ç´¢
    print("\n=== æœ€é©ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿ã‚’æ¢ç´¢ä¸­... ===")
    alphas = np.linspace(0, 1, 21)  # 0.0, 0.05, 0.10, ..., 1.0
    best_alpha = 0
    best_mae = float('inf')
    
    for alpha in alphas:
        ensemble_pred_train = alpha * rf_pred_train + (1 - alpha) * cgcnn_pred_train
        mae_train = mean_absolute_error(y_train, ensemble_pred_train)
    
        if mae_train < best_mae:
            best_mae = mae_train
            best_alpha = alpha
    
    print(f"æœ€é©é‡ã¿ Î± = {best_alpha:.2f}")
    print(f"è¨“ç·´MAE = {best_mae:.4f} eV/atom")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
    ensemble_pred_test = best_alpha * rf_pred_test + (1 - best_alpha) * cgcnn_pred_test
    
    mae_test = mean_absolute_error(y_test, ensemble_pred_test)
    r2_test = r2_score(y_test, ensemble_pred_test)
    
    print(f"\n=== Late Fusionï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰çµæœ ===")
    print(f"RFé‡ã¿: {best_alpha:.2f}, CGCNNé‡ã¿: {1-best_alpha:.2f}")
    print(f"MAE:  {mae_test:.4f} eV/atom")
    print(f"RÂ²:   {r2_test:.4f}")
    
    # å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒ
    rf_mae = mean_absolute_error(y_test, rf_pred_test)
    cgcnn_mae = mean_absolute_error(y_test, cgcnn_pred_test)
    
    print(f"\n=== å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ ===")
    print(f"RFå˜ç‹¬:       MAE = {rf_mae:.4f} eV/atom")
    print(f"CGCNNå˜ç‹¬:    MAE = {cgcnn_mae:.4f} eV/atom")
    print(f"Late Fusion:  MAE = {mae_test:.4f} eV/atom")
    print(f"æ”¹å–„ç‡ï¼ˆRFæ¯”ï¼‰:    {(rf_mae - mae_test) / rf_mae * 100:.2f}%")
    print(f"æ”¹å–„ç‡ï¼ˆCGCNNæ¯”ï¼‰: {(cgcnn_mae - mae_test) / cgcnn_mae * 100:.2f}%")
    
    # å‡ºåŠ›ä¾‹ï¼š
    # æœ€é©é‡ã¿ Î± = 0.25
    # === Late Fusionï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰çµæœ ===
    # RFé‡ã¿: 0.25, CGCNNé‡ã¿: 0.75
    # MAE:  0.0272 eV/atom
    # RÂ²:   0.9582
    #
    # === å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ ===
    # RFå˜ç‹¬:       MAE = 0.0325 eV/atom
    # CGCNNå˜ç‹¬:    MAE = 0.0286 eV/atom
    # Late Fusion:  MAE = 0.0272 eV/atom
    # æ”¹å–„ç‡ï¼ˆRFæ¯”ï¼‰:    16.31%
    # æ”¹å–„ç‡ï¼ˆCGCNNæ¯”ï¼‰: 4.90%
    

### 5.3.2 Late Fusionã®åˆ†æ

**æœ€é©é‡ã¿ã®è§£é‡ˆï¼š**

  * **Î± = 0.25** ï¼šCGCNNã®äºˆæ¸¬ã‚’75%ã€RFã®äºˆæ¸¬ã‚’25%ã§çµ±åˆ
  * CGCNNã®é«˜ç²¾åº¦ã‚’æ´»ã‹ã—ã¤ã¤ã€RFã®å®‰å®šæ€§ã‚’è£œåŠ©çš„ã«åˆ©ç”¨
  * ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„é ˜åŸŸã§ã¯Î±ãŒå¤§ãããªã‚‹å‚¾å‘ï¼ˆRFã¸ã®ä¾å­˜å¢—åŠ ï¼‰

**Late Fusionã®åˆ©ç‚¹ï¼š**

  * å®Ÿè£…ãŒæ¥µã‚ã¦å˜ç´”ï¼ˆå„ãƒ¢ãƒ‡ãƒ«ã‚’ç‹¬ç«‹ã«è¨“ç·´å¯èƒ½ï¼‰
  * ãƒ¢ãƒ‡ãƒ«ã®å¤šæ§˜æ€§ã«ã‚ˆã‚Šã€äºˆæ¸¬ã®å®‰å®šæ€§ãŒå‘ä¸Š
  * ç‰‡æ–¹ã®ãƒ¢ãƒ‡ãƒ«ãŒå¤±æ•—ã—ã¦ã‚‚ã€ã‚‚ã†ä¸€æ–¹ãŒè£œå®Œå¯èƒ½

**Late Fusionã®èª²é¡Œï¼š**

  * Early Fusionã‚ˆã‚Šæ€§èƒ½ãŒåŠ£ã‚‹ï¼ˆMAE 0.0272 vs 0.0265 eV/atomï¼‰
  * 2ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ãƒ»ä¿æŒã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ãŒå¤§ãã„
  * æ¨è«–æ™‚ã«ä¸¡ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ãŒå¿…è¦ã§ã€æ¨è«–é€Ÿåº¦ãŒé…ã„

## 5.4 ALIGNNï¼šæœ€å…ˆç«¯ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«

ALIGNNï¼ˆAtomistic Line Graph Neural Networkï¼‰ã¯ã€**åŸå­ã‚°ãƒ©ãƒ•ï¼ˆatom graphï¼‰** ã¨**ç·šã‚°ãƒ©ãƒ•ï¼ˆline graphï¼‰** ã®ä¸¡æ–¹ã‚’ç”¨ã„ã‚‹æœ€å…ˆç«¯ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰GNNã§ã™ã€‚ç·šã‚°ãƒ©ãƒ•ã§ã¯ã€åŸå­é–“ã®**çµåˆï¼ˆbondï¼‰** ã‚’ãƒãƒ¼ãƒ‰ã¨ã—ã¦æ‰±ã„ã€çµåˆè§’åº¦æƒ…å ±ã‚’æ˜ç¤ºçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¾ã™ã€‚

### 5.4.1 ALIGNNã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
    
    
    ```mermaid
    graph LR
        A[çµæ™¶æ§‹é€ ] --> B[åŸå­ã‚°ãƒ©ãƒ•Atom Graph]
        A --> C[ç·šã‚°ãƒ©ãƒ•Line Graph]
    
        B --> D[Atom GraphConvolution]
        C --> E[Line GraphConvolution]
    
        D --> F[ç›¸äº’ä½œç”¨å±¤Atom-Line Interaction]
        E --> F
    
        F --> G[ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ—ãƒ¼ãƒªãƒ³ã‚°]
        G --> H[äºˆæ¸¬å±¤]
        H --> I[ç‰©æ€§äºˆæ¸¬å€¤]
    
        style A fill:#667eea,color:#fff
        style F fill:#764ba2,color:#fff
        style I fill:#4caf50,color:#fff
    ```

**åŸå­ã‚°ãƒ©ãƒ•ï¼ˆAtom Graphï¼‰ï¼š**

$$G_{\text{atom}} = (V_{\text{atom}}, E_{\text{atom}})$$

ãƒãƒ¼ãƒ‰ï¼šåŸå­ã€ã‚¨ãƒƒã‚¸ï¼šåŸå­é–“çµåˆ

**ç·šã‚°ãƒ©ãƒ•ï¼ˆLine Graphï¼‰ï¼š**

$$G_{\text{line}} = (V_{\text{line}}, E_{\text{line}})$$

ãƒãƒ¼ãƒ‰ï¼šçµåˆã€ã‚¨ãƒƒã‚¸ï¼šçµåˆè§’åº¦ï¼ˆåŒã˜åŸå­ã‚’å…±æœ‰ã™ã‚‹2ã¤ã®çµåˆï¼‰

### 5.4.2 ALIGNNã®ç°¡æ˜“å®Ÿè£…

**ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹3ï¼šALIGNNç°¡æ˜“å®Ÿè£…**
    
    
    # ALIGNNç°¡æ˜“å®Ÿè£…ï¼ˆæ•™è‚²ç›®çš„ï¼‰
    import torch
    import torch.nn as nn
    from torch_geometric.nn import MessagePassing, global_mean_pool
    from torch_geometric.data import Data
    
    class ALIGNNConv(MessagePassing):
        """
        ALIGNNç•³ã¿è¾¼ã¿å±¤ï¼ˆç°¡ç•¥ç‰ˆï¼‰
        """
        def __init__(self, node_dim, edge_dim):
            super(ALIGNNConv, self).__init__(aggr='add')
            self.node_dim = node_dim
            self.edge_dim = edge_dim
    
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¨ˆç®—ç”¨MLP
            self.message_mlp = nn.Sequential(
                nn.Linear(2 * node_dim + edge_dim, node_dim),
                nn.Softplus(),
                nn.Linear(node_dim, node_dim)
            )
    
            # ãƒãƒ¼ãƒ‰æ›´æ–°ç”¨MLP
            self.update_mlp = nn.Sequential(
                nn.Linear(2 * node_dim, node_dim),
                nn.Softplus(),
                nn.Linear(node_dim, node_dim)
            )
    
        def forward(self, x, edge_index, edge_attr):
            """
            Parameters:
            -----------
            x : torch.Tensor, shape (num_nodes, node_dim)
                ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡
            edge_index : torch.Tensor, shape (2, num_edges)
                ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            edge_attr : torch.Tensor, shape (num_edges, edge_dim)
                ã‚¨ãƒƒã‚¸ç‰¹å¾´é‡
    
            Returns:
            --------
            out : torch.Tensor, shape (num_nodes, node_dim)
                æ›´æ–°ã•ã‚ŒãŸãƒãƒ¼ãƒ‰ç‰¹å¾´é‡
            """
            return self.propagate(edge_index, x=x, edge_attr=edge_attr)
    
        def message(self, x_i, x_j, edge_attr):
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: [é€ä¿¡å…ƒãƒãƒ¼ãƒ‰ã€å—ä¿¡å…ˆãƒãƒ¼ãƒ‰ã€ã‚¨ãƒƒã‚¸ç‰¹å¾´é‡]
            msg_input = torch.cat([x_i, x_j, edge_attr], dim=-1)
            return self.message_mlp(msg_input)
    
        def update(self, aggr_out, x):
            # ãƒãƒ¼ãƒ‰æ›´æ–°: [å…ƒã®ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ã€é›†ç´„ã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸]
            update_input = torch.cat([x, aggr_out], dim=-1)
            return self.update_mlp(update_input)
    
    class ALIGNNSimple(nn.Module):
        """
        ALIGNNç°¡æ˜“å®Ÿè£…ï¼ˆåŸå­ã‚°ãƒ©ãƒ•ã®ã¿ã€ç·šã‚°ãƒ©ãƒ•ã¯çœç•¥ï¼‰
        """
        def __init__(self, atom_fea_len=92, nbr_fea_len=41, hidden_dim=128, n_conv=3):
            super(ALIGNNSimple, self).__init__()
    
            # åŸå­åŸ‹ã‚è¾¼ã¿
            self.atom_embedding = nn.Linear(atom_fea_len, hidden_dim)
    
            # ALIGNNç•³ã¿è¾¼ã¿å±¤
            self.conv_layers = nn.ModuleList([
                ALIGNNConv(hidden_dim, nbr_fea_len) for _ in range(n_conv)
            ])
            self.bn_layers = nn.ModuleList([
                nn.BatchNorm1d(hidden_dim) for _ in range(n_conv)
            ])
    
            # äºˆæ¸¬å±¤
            self.fc1 = nn.Linear(hidden_dim, 64)
            self.fc2 = nn.Linear(64, 1)
            self.activation = nn.Softplus()
    
        def forward(self, data):
            x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch
    
            # åŸå­åŸ‹ã‚è¾¼ã¿
            x = self.atom_embedding(x)
    
            # ALIGNNç•³ã¿è¾¼ã¿
            for conv, bn in zip(self.conv_layers, self.bn_layers):
                x_new = conv(x, edge_index, edge_attr)
                x = bn(x_new) + x  # æ®‹å·®æ¥ç¶š
                x = self.activation(x)
    
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ—ãƒ¼ãƒªãƒ³ã‚°
            x = global_mean_pool(x, batch)
    
            # äºˆæ¸¬
            x = self.fc1(x)
            x = self.activation(x)
            x = self.fc2(x)
    
            return x.squeeze()
    
    # ALIGNNè¨“ç·´ï¼ˆMatbench mp_e_formï¼‰
    print("=== ALIGNNç°¡æ˜“ç‰ˆã‚’è¨“ç·´ä¸­... ===")
    alignn_model = ALIGNNSimple().to(device)
    optimizer_alignn = torch.optim.Adam(alignn_model.parameters(), lr=0.001)
    criterion = nn.L1Loss()
    
    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    alignn_model.train()
    for epoch in range(50):
        total_loss = 0
        for batch in train_loader_cgcnn:
            batch = batch.to(device)
            optimizer_alignn.zero_grad()
            out = alignn_model(batch)
            loss = criterion(out, batch.y)
            loss.backward()
            optimizer_alignn.step()
            total_loss += loss.item()
    
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/50, Loss: {total_loss/len(train_loader_cgcnn):.4f}")
    
    # ãƒ†ã‚¹ãƒˆè©•ä¾¡
    alignn_model.eval()
    y_true_alignn, y_pred_alignn = [], []
    
    with torch.no_grad():
        for batch in test_loader_cgcnn:
            batch = batch.to(device)
            out = alignn_model(batch)
            y_true_alignn.extend(batch.y.cpu().numpy())
            y_pred_alignn.extend(out.cpu().numpy())
    
    mae_alignn = mean_absolute_error(y_true_alignn, y_pred_alignn)
    r2_alignn = r2_score(y_true_alignn, y_pred_alignn)
    
    print(f"\n=== ALIGNNç°¡æ˜“ç‰ˆçµæœ ===")
    print(f"MAE:  {mae_alignn:.4f} eV/atom")
    print(f"RÂ²:   {r2_alignn:.4f}")
    
    # å‡ºåŠ›ä¾‹ï¼š
    # === ALIGNNç°¡æ˜“ç‰ˆçµæœ ===
    # MAE:  0.0278 eV/atom
    # RÂ²:   0.9548
    
    # æ³¨ï¼šå®Œå…¨ãªALIGNNã¯ç·šã‚°ãƒ©ãƒ•ã‚‚ä½¿ç”¨ã—ã€ã•ã‚‰ã«é«˜æ€§èƒ½ï¼ˆMAE ~0.025 eV/atomï¼‰
    

**âš ï¸ æ³¨æ„ï¼šç°¡æ˜“å®Ÿè£…ã®é™ç•Œ**

æœ¬ã‚³ãƒ¼ãƒ‰ä¾‹ã¯æ•™è‚²ç›®çš„ã®ç°¡æ˜“å®Ÿè£…ã§ã™ã€‚å®Œå…¨ãªALIGNNå®Ÿè£…ã¯**ç·šã‚°ãƒ©ãƒ•ï¼ˆLine Graphï¼‰** ã‚‚ä½¿ç”¨ã—ã€çµåˆè§’åº¦æƒ…å ±ã‚’æ˜ç¤ºçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¾ã™ã€‚å…¬å¼å®Ÿè£…ï¼ˆ[NIST ALIGNN GitHub](<https://github.com/usnistgov/alignn>)ï¼‰ã§ã¯MAE ~0.025 eV/atomã®æ€§èƒ½ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚

### 5.4.3 ALIGNNã®æ€§èƒ½è©•ä¾¡

æ‰‹æ³• | MAE (eV/atom) | ç‰¹å¾´  
---|---|---  
CGCNN | 0.0286 | åŸå­ã‚°ãƒ©ãƒ•ã®ã¿  
ALIGNNç°¡æ˜“ç‰ˆ | 0.0278 | æ®‹å·®æ¥ç¶š + æ”¹è‰¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°  
**ALIGNNå®Œå…¨ç‰ˆ** | **0.0250** | åŸå­ã‚°ãƒ©ãƒ• + ç·šã‚°ãƒ©ãƒ• + çµåˆè§’åº¦  
  
**ALIGNNã®å„ªä½æ€§ï¼š**

  * çµåˆè§’åº¦æƒ…å ±ã‚’æ˜ç¤ºçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã€æ§‹é€ æ„Ÿåº¦ãŒæ¥µã‚ã¦é«˜ã„
  * Matbench mp_e_formã§**MAE 0.025 eV/atom** ã‚’é”æˆï¼ˆCGCNNæ¯”12.6%æ”¹å–„ï¼‰
  * å¤šãã®Matbenchã‚¿ã‚¹ã‚¯ã§æœ€å…ˆç«¯æ€§èƒ½ã‚’è¨˜éŒ²

**ALIGNNã®èª²é¡Œï¼š**

  * ç·šã‚°ãƒ©ãƒ•æ§‹ç¯‰ã«ã‚ˆã‚Šè¨ˆç®—ã‚³ã‚¹ãƒˆãŒå¢—åŠ ï¼ˆCGCNNæ¯”ç´„1.5-2å€ï¼‰
  * å®Ÿè£…ãŒè¤‡é›‘ã§ã€ãƒ‡ãƒãƒƒã‚°ãŒå›°é›£
  * å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã¯éå­¦ç¿’ãƒªã‚¹ã‚¯ãŒé«˜ã„

## 5.5 MEGNetï¼šãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’

MEGNetï¼ˆMaterials Graph Networkï¼‰ã¯ã€**è¤‡æ•°ã®ææ–™ç‰©æ€§ã‚’åŒæ™‚ã«äºˆæ¸¬** ã™ã‚‹ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ç•°ãªã‚‹ç‰©æ€§é–“ã®ç›¸é–¢ã‚’åˆ©ç”¨ã—ã€ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã¨ãƒ¢ãƒ‡ãƒ«æ±åŒ–æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚

### 5.5.1 ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®åŸç†

ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã§ã¯ã€è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯$T_1, T_2, \ldots, T_K$ã‚’åŒæ™‚ã«å­¦ç¿’ã—ã¾ã™ï¼š

$$\mathcal{L}_{\text{multi}} = \sum_{k=1}^{K} \lambda_k \mathcal{L}_k$$

ã“ã“ã§ã€$\lambda_k$ã¯ã‚¿ã‚¹ã‚¯$k$ã®é‡ã¿ã€$\mathcal{L}_k$ã¯ã‚¿ã‚¹ã‚¯$k$ã®æå¤±é–¢æ•°ã§ã™ã€‚

**ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®åˆ©ç‚¹ï¼š**

  * **ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã®å‘ä¸Š** ï¼šå°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§è¤‡æ•°ç‰©æ€§ã‚’äºˆæ¸¬å¯èƒ½
  * **æ±åŒ–æ€§èƒ½ã®å‘ä¸Š** ï¼šã‚¿ã‚¹ã‚¯é–“ã®ç›¸é–¢ã‚’åˆ©ç”¨ã—ã€éå­¦ç¿’ã‚’æŠ‘åˆ¶
  * **è»¢ç§»å­¦ç¿’ã®åŸºç›¤** ï¼šäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æ´»ç”¨å¯èƒ½

### 5.5.2 MEGNetã®å®Ÿè£…

**ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹4ï¼šMEGNeté¢¨ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®å®Ÿè£…**
    
    
    # MEGNeté¢¨ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯GNNã®å®Ÿè£…
    import torch
    import torch.nn as nn
    from torch_geometric.nn import GATConv, global_mean_pool
    
    class MEGNetMultiTask(nn.Module):
        """
        MEGNeté¢¨ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯GNN
        ç”Ÿæˆã‚¨ãƒãƒ«ã‚®ãƒ¼ã¨ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã‚’åŒæ™‚äºˆæ¸¬
        """
        def __init__(self, atom_fea_len=92, nbr_fea_len=41, hidden_dim=128, n_conv=3, n_tasks=2):
            super(MEGNetMultiTask, self).__init__()
    
            # å…±æœ‰GNNå±¤ï¼ˆå…¨ã‚¿ã‚¹ã‚¯ã§å…±é€šï¼‰
            self.atom_embedding = nn.Linear(atom_fea_len, hidden_dim)
    
            self.conv_layers = nn.ModuleList([
                GATConv(hidden_dim, hidden_dim, heads=4, concat=False, edge_dim=nbr_fea_len)
                for _ in range(n_conv)
            ])
            self.bn_layers = nn.ModuleList([
                nn.BatchNorm1d(hidden_dim) for _ in range(n_conv)
            ])
    
            # ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®äºˆæ¸¬ãƒ˜ãƒƒãƒ‰
            self.task_heads = nn.ModuleList([
                nn.Sequential(
                    nn.Linear(hidden_dim, 64),
                    nn.Softplus(),
                    nn.Linear(64, 1)
                ) for _ in range(n_tasks)
            ])
    
            self.activation = nn.Softplus()
    
        def forward(self, data, task_idx=None):
            """
            Parameters:
            -----------
            data : torch_geometric.data.Data
                ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿
            task_idx : int or None
                äºˆæ¸¬ã™ã‚‹ã‚¿ã‚¹ã‚¯ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆNoneã®å ´åˆã¯å…¨ã‚¿ã‚¹ã‚¯äºˆæ¸¬ï¼‰
    
            Returns:
            --------
            out : torch.Tensor or list of torch.Tensor
                ã‚¿ã‚¹ã‚¯äºˆæ¸¬å€¤
            """
            x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch
    
            # å…±æœ‰GNNåŸ‹ã‚è¾¼ã¿
            x = self.atom_embedding(x)
            for conv, bn in zip(self.conv_layers, self.bn_layers):
                x = conv(x, edge_index, edge_attr)
                x = bn(x)
                x = self.activation(x)
    
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ—ãƒ¼ãƒªãƒ³ã‚°
            graph_embedding = global_mean_pool(x, batch)
    
            # ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®äºˆæ¸¬
            if task_idx is not None:
                # å˜ä¸€ã‚¿ã‚¹ã‚¯äºˆæ¸¬
                return self.task_heads[task_idx](graph_embedding).squeeze()
            else:
                # å…¨ã‚¿ã‚¹ã‚¯äºˆæ¸¬
                return [head(graph_embedding).squeeze() for head in self.task_heads]
    
    # ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆç”Ÿæˆã‚¨ãƒãƒ«ã‚®ãƒ¼ + ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ï¼‰
    from matbench.bench import MatbenchBenchmark
    
    mb = MatbenchBenchmark(autoload=False)
    
    # ã‚¿ã‚¹ã‚¯1: ç”Ÿæˆã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆmp_e_formï¼‰
    task1 = mb.matbench_mp_e_form
    task1.load()
    
    # ã‚¿ã‚¹ã‚¯2: ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ï¼ˆmp_gapï¼‰
    task2 = mb.matbench_mp_gap
    task2.load()
    
    # å…±é€šã®æ§‹é€ ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆå®Ÿè£…ç°¡ç•¥åŒ–ã®ãŸã‚ã€ã“ã“ã§ã¯åŒã˜æ§‹é€ IDã‚’ä»®å®šï¼‰
    # å®Ÿéš›ã«ã¯Materials Project IDã§çµåˆã™ã‚‹
    
    print("=== ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ä¸­... ===")
    
    # Fold 0ã®ã¿ä½¿ç”¨
    train_inputs_1, train_outputs_1 = task1.get_train_and_val_data(task1.folds[0])
    test_inputs_1, test_outputs_1 = task1.get_test_data(task1.folds[0], include_target=True)
    
    train_inputs_2, train_outputs_2 = task2.get_train_and_val_data(task2.folds[0])
    test_inputs_2, test_outputs_2 = task2.get_test_data(task2.folds[0], include_target=True)
    
    # ç°¡ç•¥åŒ–ã®ãŸã‚ã€æœ€åˆã®10,000ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ä½¿ç”¨
    n_samples = 10000
    train_inputs_1 = train_inputs_1[:n_samples]
    train_outputs_1 = train_outputs_1.values[:n_samples]
    train_inputs_2 = train_inputs_2[:n_samples]
    train_outputs_2 = train_outputs_2.values[:n_samples]
    
    # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
    def create_multitask_data(structures, targets_task1, targets_task2):
        """
        ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        """
        data_list = []
        for struct, t1, t2 in zip(structures, targets_task1, targets_task2):
            graph = structure_to_pyg_data(struct, t1)
            graph.y_task1 = torch.tensor([t1], dtype=torch.float)
            graph.y_task2 = torch.tensor([t2], dtype=torch.float)
            data_list.append(graph)
        return data_list
    
    train_data_multi = create_multitask_data(train_inputs_1, train_outputs_1, train_outputs_2)
    test_data_multi = create_multitask_data(test_inputs_1[:1000],
                                             test_outputs_1.values[:1000],
                                             test_outputs_2.values[:1000])
    
    train_loader_multi = DataLoader(train_data_multi, batch_size=32, shuffle=True)
    test_loader_multi = DataLoader(test_data_multi, batch_size=32, shuffle=False)
    
    # MEGNetãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
    print("\n=== MEGNetãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­... ===")
    megnet_model = MEGNetMultiTask(n_tasks=2).to(device)
    optimizer_megnet = torch.optim.Adam(megnet_model.parameters(), lr=0.001)
    
    # ã‚¿ã‚¹ã‚¯é‡ã¿ï¼ˆæå¤±ã®ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ï¼‰
    lambda_task1 = 1.0  # ç”Ÿæˆã‚¨ãƒãƒ«ã‚®ãƒ¼
    lambda_task2 = 0.5  # ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´ï¼‰
    
    megnet_model.train()
    for epoch in range(30):
        total_loss = 0
        for batch in train_loader_multi:
            batch = batch.to(device)
            optimizer_megnet.zero_grad()
    
            # 2ã‚¿ã‚¹ã‚¯ã®äºˆæ¸¬
            pred_task1, pred_task2 = megnet_model(batch)
    
            # ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯æå¤±
            loss_task1 = nn.L1Loss()(pred_task1, batch.y_task1.squeeze())
            loss_task2 = nn.L1Loss()(pred_task2, batch.y_task2.squeeze())
    
            loss = lambda_task1 * loss_task1 + lambda_task2 * loss_task2
            loss.backward()
            optimizer_megnet.step()
            total_loss += loss.item()
    
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/30, Total Loss: {total_loss/len(train_loader_multi):.4f}")
    
    # ãƒ†ã‚¹ãƒˆè©•ä¾¡ï¼ˆå„ã‚¿ã‚¹ã‚¯ï¼‰
    megnet_model.eval()
    y_true_task1, y_pred_task1 = [], []
    y_true_task2, y_pred_task2 = [], []
    
    with torch.no_grad():
        for batch in test_loader_multi:
            batch = batch.to(device)
            pred_task1, pred_task2 = megnet_model(batch)
    
            y_true_task1.extend(batch.y_task1.squeeze().cpu().numpy())
            y_pred_task1.extend(pred_task1.cpu().numpy())
    
            y_true_task2.extend(batch.y_task2.squeeze().cpu().numpy())
            y_pred_task2.extend(pred_task2.cpu().numpy())
    
    mae_task1 = mean_absolute_error(y_true_task1, y_pred_task1)
    mae_task2 = mean_absolute_error(y_true_task2, y_pred_task2)
    
    print(f"\n=== MEGNetãƒãƒ«ãƒã‚¿ã‚¹ã‚¯çµæœ ===")
    print(f"Task 1ï¼ˆç”Ÿæˆã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰: MAE = {mae_task1:.4f} eV/atom")
    print(f"Task 2ï¼ˆãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ï¼‰:  MAE = {mae_task2:.4f} eV")
    
    # å˜ä¸€ã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒï¼ˆå‚è€ƒï¼‰
    print(f"\nå˜ä¸€ã‚¿ã‚¹ã‚¯CGCNNæ¯”è¼ƒ:")
    print(f"Task 1: ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ {mae_task1:.4f} vs å˜ä¸€ã‚¿ã‚¹ã‚¯ ~0.0286 eV/atom")
    print(f"Task 2: ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ {mae_task2:.4f} vs å˜ä¸€ã‚¿ã‚¹ã‚¯ ~0.180 eV")
    
    # å‡ºåŠ›ä¾‹ï¼š
    # === MEGNetãƒãƒ«ãƒã‚¿ã‚¹ã‚¯çµæœ ===
    # Task 1ï¼ˆç”Ÿæˆã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰: MAE = 0.0292 eV/atom
    # Task 2ï¼ˆãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ï¼‰:  MAE = 0.185 eV
    #
    # å˜ä¸€ã‚¿ã‚¹ã‚¯CGCNNæ¯”è¼ƒ:
    # Task 1: ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ 0.0292 vs å˜ä¸€ã‚¿ã‚¹ã‚¯ ~0.0286 eV/atomï¼ˆã‚ãšã‹ã«åŠ£åŒ–ï¼‰
    # Task 2: ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ 0.185 vs å˜ä¸€ã‚¿ã‚¹ã‚¯ ~0.180 eVï¼ˆåŒç¨‹åº¦ï¼‰
    

### 5.5.3 ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®åŠ¹æœ

**ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®ãƒ¡ãƒªãƒƒãƒˆï¼š**

  * **ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã®å‘ä¸Š** ï¼šå˜ä¸€ã‚¿ã‚¹ã‚¯ã§10,000ã‚µãƒ³ãƒ—ãƒ«å¿…è¦ãªã¨ã“ã‚ã€5,000ã‚µãƒ³ãƒ—ãƒ«Ã—2ã‚¿ã‚¹ã‚¯ã§åŒç­‰æ€§èƒ½
  * **æ±åŒ–æ€§èƒ½ã®å‘ä¸Š** ï¼šã‚¿ã‚¹ã‚¯é–“ã®ç›¸é–¢ã‚’åˆ©ç”¨ã—ã€éå­¦ç¿’ã‚’æŠ‘åˆ¶
  * **ãƒ¢ãƒ‡ãƒ«å…±æœ‰** ï¼š1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã§è¤‡æ•°ç‰©æ€§ã‚’äºˆæ¸¬å¯èƒ½

**ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®èª²é¡Œï¼š**

  * **è² ã®è»¢ç§»ï¼ˆNegative Transferï¼‰** ï¼šã‚¿ã‚¹ã‚¯é–“ã®ç›¸é–¢ãŒä½ã„å ´åˆã€æ€§èƒ½ãŒåŠ£åŒ–
  * **ã‚¿ã‚¹ã‚¯é‡ã¿ã®èª¿æ•´** ï¼šé©åˆ‡ãª$\lambda_k$ã®è¨­å®šãŒå›°é›£
  * **ã‚¹ã‚±ãƒ¼ãƒ«ã®é•ã„** ï¼šç”Ÿæˆã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆeV/atomï¼‰ã¨ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ï¼ˆeVï¼‰ã®ã‚¹ã‚±ãƒ¼ãƒ«å·®ã«å¯¾å‡¦ãŒå¿…è¦

## 5.6 ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æ¯”è¼ƒ

æœ¬ç« ã§å®Ÿè£…ã—ãŸå…¨ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•ã®æ€§èƒ½ã‚’çµ±åˆæ¯”è¼ƒã—ã¾ã™ã€‚

**ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹5ï¼šãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•ã®çµ±åˆæ¯”è¼ƒ**
    
    
    # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•ã®çµ±åˆæ¯”è¼ƒ
    import matplotlib.pyplot as plt
    import pandas as pd
    
    # æ€§èƒ½ãƒ‡ãƒ¼ã‚¿ï¼ˆMatbench mp_e_formï¼‰
    results = {
        'Model': [
            'Random Forest (Magpie)',
            'CGCNN',
            'Hybrid Early Fusion',
            'Hybrid Late Fusion',
            'ALIGNN (Simple)',
            'ALIGNN (Full)',
            'MEGNet Multi-Task'
        ],
        'MAE (eV/atom)': [0.0325, 0.0286, 0.0265, 0.0272, 0.0278, 0.0250, 0.0292],
        'RÂ²': [0.9321, 0.9524, 0.9614, 0.9582, 0.9548, 0.9680, 0.9510],
        'Training Time (min)': [0.75, 30.5, 32.0, 31.25, 35.0, 45.0, 50.0],
        'Category': ['Composition', 'GNN', 'Hybrid', 'Hybrid', 'Hybrid', 'Hybrid', 'Multi-Task']
    }
    
    df = pd.DataFrame(results)
    
    # å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # MAEæ¯”è¼ƒ
    colors = {'Composition': '#4caf50', 'GNN': '#667eea', 'Hybrid': '#764ba2', 'Multi-Task': '#ff9800'}
    ax1 = axes[0]
    bars = ax1.barh(df['Model'], df['MAE (eV/atom)'],
                    color=[colors[cat] for cat in df['Category']])
    ax1.set_xlabel('MAE (eV/atom)', fontsize=12)
    ax1.set_title('äºˆæ¸¬ç²¾åº¦æ¯”è¼ƒï¼ˆLower is Betterï¼‰', fontsize=14, fontweight='bold')
    ax1.invert_yaxis()
    
    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒç·š
    ax1.axvline(0.0325, color='red', linestyle='--', linewidth=1, alpha=0.7, label='RF Baseline')
    ax1.legend()
    
    # è¨“ç·´æ™‚é–“ vs MAE
    ax2 = axes[1]
    for idx, row in df.iterrows():
        ax2.scatter(row['Training Time (min)'], row['MAE (eV/atom)'],
                    s=200, color=colors[row['Category']], alpha=0.7, edgecolors='black', linewidth=1.5)
        ax2.text(row['Training Time (min)'], row['MAE (eV/atom)'],
                 row['Model'], fontsize=8, ha='right', va='bottom')
    
    ax2.set_xlabel('è¨“ç·´æ™‚é–“ (åˆ†)', fontsize=12)
    ax2.set_ylabel('MAE (eV/atom)', fontsize=12)
    ax2.set_title('è¨“ç·´æ™‚é–“ vs ç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•', fontsize=14, fontweight='bold')
    ax2.invert_yaxis()
    ax2.grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('hybrid_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # çµ±è¨ˆã‚µãƒãƒªãƒ¼
    print("=== ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•çµ±åˆæ¯”è¼ƒ ===")
    print(df.to_string(index=False))
    
    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®è­˜åˆ¥
    best_mae_idx = df['MAE (eV/atom)'].idxmin()
    best_efficiency_idx = (df['MAE (eV/atom)'] / df['Training Time (min)']).idxmin()
    
    print(f"\næœ€é«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«: {df.loc[best_mae_idx, 'Model']} (MAE = {df.loc[best_mae_idx, 'MAE (eV/atom)']:.4f})")
    print(f"æœ€é«˜åŠ¹ç‡ãƒ¢ãƒ‡ãƒ«: {df.loc[best_efficiency_idx, 'Model']} (MAE/Time = {df.loc[best_efficiency_idx, 'MAE (eV/atom)'] / df.loc[best_efficiency_idx, 'Training Time (min)']:.6f})")
    
    # å‡ºåŠ›ä¾‹ï¼š
    # === ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•çµ±åˆæ¯”è¼ƒ ===
    #                       Model  MAE (eV/atom)     RÂ²  Training Time (min)    Category
    #   Random Forest (Magpie)         0.0325  0.9321                 0.75 Composition
    #                    CGCNN         0.0286  0.9524                30.50         GNN
    #      Hybrid Early Fusion         0.0265  0.9614                32.00      Hybrid
    #       Hybrid Late Fusion         0.0272  0.9582                31.25      Hybrid
    #           ALIGNN (Simple)         0.0278  0.9548                35.00      Hybrid
    #             ALIGNN (Full)         0.0250  0.9680                45.00      Hybrid
    #        MEGNet Multi-Task         0.0292  0.9510                50.00  Multi-Task
    #
    # æœ€é«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«: ALIGNN (Full) (MAE = 0.0250)
    # æœ€é«˜åŠ¹ç‡ãƒ¢ãƒ‡ãƒ«: Random Forest (Magpie) (MAE/Time = 0.043333)
    

### 5.6.1 æ€§èƒ½åˆ†æã®ã¾ã¨ã‚

æ‰‹æ³• | ç²¾åº¦ | åŠ¹ç‡ | å®Ÿè£…é›£æ˜“åº¦ | æ¨å¥¨ã‚·ãƒŠãƒªã‚ª  
---|---|---|---|---  
Hybrid Early Fusion | â­â­â­â­ | â­â­â­ | ä½ | ä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã€å®Ÿè£…å®¹æ˜“æ€§é‡è¦–  
Hybrid Late Fusion | â­â­â­ | â­â­ | ä½ | æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã®çµ±åˆã€å®‰å®šæ€§é‡è¦–  
ALIGNN (Full) | â­â­â­â­â­ | â­â­ | é«˜ | æœ€é«˜ç²¾åº¦ãŒå¿…é ˆã€è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ååˆ†  
MEGNet Multi-Task | â­â­â­ | â­â­â­ | ä¸­ | è¤‡æ•°ç‰©æ€§äºˆæ¸¬ã€ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡é‡è¦–  
  
## 5.7 æœ¬ç« ã®ã¾ã¨ã‚

æœ¬ç« ã§ã¯ã€çµ„æˆãƒ™ãƒ¼ã‚¹ã¨GNNæ§‹é€ ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ã‚’çµ±åˆã™ã‚‹**ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ** ã‚’ä½“ç³»çš„ã«å­¦ã³ã¾ã—ãŸã€‚

### ä¸»è¦ãªçŸ¥è¦‹

  * **Early Fusion** ï¼šç‰¹å¾´é‡é€£çµã«ã‚ˆã‚Šã€CGCNNå˜ç‹¬æ¯”7.3%ç²¾åº¦å‘ä¸Šï¼ˆMAE 0.0265 eV/atomï¼‰
  * **Late Fusion** ï¼šã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ã«ã‚ˆã‚Šã€å®‰å®šæ€§å‘ä¸Šã¨ãƒªã‚¹ã‚¯åˆ†æ•£ã‚’å®Ÿç¾
  * **ALIGNN** ï¼šç·šã‚°ãƒ©ãƒ•ã«ã‚ˆã‚‹çµåˆè§’åº¦æƒ…å ±ã®çµ±åˆã§ã€æœ€å…ˆç«¯ç²¾åº¦ï¼ˆMAE 0.0250 eV/atomï¼‰
  * **MEGNet** ï¼šãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã¨æ±åŒ–æ€§èƒ½ã®å‘ä¸Š

**ğŸ¯ å®Ÿå‹™ä¸Šã®æ¨å¥¨**

  * **ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°** ï¼šHybrid Early Fusionï¼ˆå®Ÿè£…å®¹æ˜“ã€ååˆ†ãªç²¾åº¦ï¼‰
  * **æœ€é«˜ç²¾åº¦è¿½æ±‚** ï¼šALIGNNå®Œå…¨ç‰ˆï¼ˆè¨ˆç®—ã‚³ã‚¹ãƒˆè¨±å®¹ã§ãã‚‹å ´åˆï¼‰
  * **è¤‡æ•°ç‰©æ€§äºˆæ¸¬** ï¼šMEGNetãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ï¼ˆãƒ‡ãƒ¼ã‚¿åŠ¹ç‡é‡è¦–ï¼‰
  * **å®‰å®šæ€§é‡è¦–** ï¼šLate Fusionï¼ˆæ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã®çµ±åˆãŒå®¹æ˜“ï¼‰

## æ¼”ç¿’å•é¡Œ

æ¼”ç¿’1ï¼šEarly Fusionã®ç‰¹å¾´é‡æ¬¡å…ƒ Easy

**å•é¡Œï¼š** Magpieç‰¹å¾´é‡ï¼ˆ145æ¬¡å…ƒï¼‰ã¨GNNåŸ‹ã‚è¾¼ã¿ï¼ˆ256æ¬¡å…ƒï¼‰ã‚’Early Fusionã§çµ±åˆã™ã‚‹å ´åˆã€çµ±åˆå¾Œã®ç‰¹å¾´é‡æ¬¡å…ƒã¯ã„ãã¤ã«ãªã‚‹ã‹ï¼Ÿã¾ãŸã€éå­¦ç¿’ãƒªã‚¹ã‚¯ã‚’ä½æ¸›ã™ã‚‹ãŸã‚ã®æ‰‹æ³•ã‚’2ã¤æŒ™ã’ã‚ˆã€‚

**è§£ç­”ï¼š**

çµ±åˆå¾Œã®ç‰¹å¾´é‡æ¬¡å…ƒ: 145 + 256 = **401æ¬¡å…ƒ**

éå­¦ç¿’ãƒªã‚¹ã‚¯ä½æ¸›æ‰‹æ³•:

  1. **Dropout** : çµ±åˆå±¤ã¨äºˆæ¸¬å±¤ã®é–“ã«Dropoutï¼ˆä¾‹ï¼šp=0.3ï¼‰ã‚’æŒ¿å…¥ã—ã€ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’ç„¡åŠ¹åŒ–
  2. **L2æ­£å‰‡åŒ–** : æå¤±é–¢æ•°ã«é‡ã¿æ¸›è¡°é …ã‚’è¿½åŠ ï¼ˆä¾‹ï¼šweight_decay=1e-4ï¼‰

æ¼”ç¿’2ï¼šLate Fusionã®æœ€é©é‡ã¿ Easy

**å•é¡Œï¼š** Random Forestï¼ˆMAE 0.035 eV/atomï¼‰ã¨CGCNNï¼ˆMAE 0.028 eV/atomï¼‰ã®Late Fusionã§ã€æœ€é©é‡ã¿Î±=0.20ãŒå¾—ã‚‰ã‚ŒãŸã€‚ã“ã®é‡ã¿ã®æ„å‘³ã‚’è§£é‡ˆã—ã€ãªãœCGCNNã®é‡ã¿ãŒé«˜ã„ã®ã‹èª¬æ˜ã›ã‚ˆã€‚

**è§£ç­”ï¼š**

**é‡ã¿ã®æ„å‘³ï¼š**

$$\hat{y}_{\text{ensemble}} = 0.20 \times \hat{y}_{\text{RF}} + 0.80 \times \hat{y}_{\text{CGCNN}}$$

CGCNNã®äºˆæ¸¬ã‚’80%ã€RFã®äºˆæ¸¬ã‚’20%ã§çµ±åˆã€‚

**CGCNNé‡ã¿ãŒé«˜ã„ç†ç”±ï¼š**

  * CGCNNã®å€‹åˆ¥æ€§èƒ½ï¼ˆMAE 0.028ï¼‰ãŒRFï¼ˆMAE 0.035ï¼‰ã‚ˆã‚Š20%å„ªã‚Œã¦ã„ã‚‹
  * ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã§ã¯é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã«å¤§ããªé‡ã¿ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ãŒæœ€é©
  * RFã®äºˆæ¸¬ã¯è£œåŠ©çš„ãªå½¹å‰²ï¼ˆå¤–ã‚Œå€¤è£œæ­£ã€å®‰å®šæ€§å‘ä¸Šï¼‰ã¨ã—ã¦æ´»ç”¨

æ¼”ç¿’3ï¼šALIGNNã®ç·šã‚°ãƒ©ãƒ• Medium

**å•é¡Œï¼š** åŸå­ã‚°ãƒ©ãƒ•ã¨ç·šã‚°ãƒ©ãƒ•ã®é•ã„ã‚’èª¬æ˜ã—ã€ç·šã‚°ãƒ©ãƒ•ãŒçµåˆè§’åº¦æƒ…å ±ã‚’ã©ã®ã‚ˆã†ã«è¡¨ç¾ã™ã‚‹ã‹å…·ä½“ä¾‹ã‚’ç¤ºã›ã€‚

**è§£ç­”ï¼š**

**åŸå­ã‚°ãƒ©ãƒ•ï¼ˆAtom Graphï¼‰ï¼š**

  * ãƒãƒ¼ãƒ‰: åŸå­
  * ã‚¨ãƒƒã‚¸: åŸå­é–“ã®çµåˆï¼ˆè·é›¢ãƒ™ãƒ¼ã‚¹ï¼‰
  * ä¾‹: æ°´åˆ†å­Hâ‚‚O â†’ ãƒãƒ¼ãƒ‰3å€‹ï¼ˆO, H, Hï¼‰ã€ã‚¨ãƒƒã‚¸2æœ¬ï¼ˆO-H, O-Hï¼‰

**ç·šã‚°ãƒ©ãƒ•ï¼ˆLine Graphï¼‰ï¼š**

  * ãƒãƒ¼ãƒ‰: åŸå­é–“ã®çµåˆ
  * ã‚¨ãƒƒã‚¸: çµåˆè§’åº¦ï¼ˆåŒã˜åŸå­ã‚’å…±æœ‰ã™ã‚‹2ã¤ã®çµåˆï¼‰
  * ä¾‹: æ°´åˆ†å­Hâ‚‚O â†’ ãƒãƒ¼ãƒ‰2å€‹ï¼ˆO-Hçµåˆ1, O-Hçµåˆ2ï¼‰ã€ã‚¨ãƒƒã‚¸1æœ¬ï¼ˆçµåˆ1ã¨çµåˆ2ã®è§’åº¦ â‰ˆ 104.5Â°ï¼‰

**çµåˆè§’åº¦æƒ…å ±ã®è¡¨ç¾ï¼š**

ç·šã‚°ãƒ©ãƒ•ã®ã‚¨ãƒƒã‚¸ç‰¹å¾´é‡ã¨ã—ã¦ã€2ã¤ã®çµåˆãŒä½œã‚‹è§’åº¦Î¸ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼š
    
    
    angle_feature = torch.cos(theta)  # cosÎ¸ã‚’ç‰¹å¾´é‡ã«ä½¿ç”¨
    # ä¾‹: H-O-Hè§’åº¦104.5Â° â†’ cos(104.5Â°) â‰ˆ -0.25
    

ã“ã‚Œã«ã‚ˆã‚Šã€ALIGNNã¯ã€Œç›´ç·šçš„ãªçµåˆï¼ˆÎ¸=180Â°ï¼‰ã€ã¨ã€Œå±ˆæ›²ã—ãŸçµåˆï¼ˆÎ¸<120Â°ï¼‰ã€ã‚’æ˜ç¤ºçš„ã«åŒºåˆ¥ã§ãã¾ã™ã€‚

æ¼”ç¿’4ï¼šãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®æå¤±é‡ã¿ Medium

**å•é¡Œï¼š** ç”Ÿæˆã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ï¼š-5ï½5 eV/atomï¼‰ã¨ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ï¼š0ï½10 eVï¼‰ã‚’åŒæ™‚äºˆæ¸¬ã™ã‚‹ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯GNNã«ãŠã„ã¦ã€é©åˆ‡ãªã‚¿ã‚¹ã‚¯é‡ã¿Î»â‚ã€Î»â‚‚ã‚’è¨­è¨ˆã›ã‚ˆã€‚å˜ç´”ã«$\lambda_1 = \lambda_2 = 1.0$ã¨ã—ãŸå ´åˆã®å•é¡Œç‚¹ã‚‚èª¬æ˜ã™ã‚‹ã“ã¨ã€‚

**è§£ç­”ï¼š**

**å•é¡Œç‚¹ï¼ˆ$\lambda_1 = \lambda_2 = 1.0$ï¼‰ï¼š**

ç”Ÿæˆã‚¨ãƒãƒ«ã‚®ãƒ¼ã¨ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã®ã‚¹ã‚±ãƒ¼ãƒ«ãŒç•°ãªã‚‹ãŸã‚ã€æå¤±ã®å¤§ãã•ãŒä¸å‡è¡¡ã«ãªã‚Šã¾ã™ï¼š
    
    
    # ç”Ÿæˆã‚¨ãƒãƒ«ã‚®ãƒ¼ã®å…¸å‹çš„ãªMAE: 0.03 eV/atom
    loss_task1 = 0.03
    
    # ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã®å…¸å‹çš„ãªMAE: 0.18 eV
    loss_task2 = 0.18
    
    # ç·æå¤±ï¼ˆÎ»â‚ = Î»â‚‚ = 1.0ï¼‰
    total_loss = 1.0 * 0.03 + 1.0 * 0.18 = 0.21
    # â†’ ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã®æå¤±ãŒ6å€å¤§ãã„ â†’ ç”Ÿæˆã‚¨ãƒãƒ«ã‚®ãƒ¼ã®å­¦ç¿’ãŒä¸ååˆ†
    

**é©åˆ‡ãªã‚¿ã‚¹ã‚¯é‡ã¿ã®è¨­è¨ˆï¼š**

å„ã‚¿ã‚¹ã‚¯ã®æå¤±ã‚’åŒç¨‹åº¦ã«ã™ã‚‹ãŸã‚ã€ã‚¹ã‚±ãƒ¼ãƒ«ã®é€†æ•°ã§é‡ã¿ä»˜ã‘ï¼š
    
    
    # ã‚¿ã‚¹ã‚¯é‡ã¿ã®è¨­å®š
    lambda_1 = 1.0  # ç”Ÿæˆã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆåŸºæº–ï¼‰
    lambda_2 = 0.03 / 0.18 â‰ˆ 0.17  # ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´ï¼‰
    
    # ã¾ãŸã¯ã€æ¨™æº–åå·®ã®é€†æ•°ã‚’ä½¿ç”¨
    std_task1 = 1.5  # ç”Ÿæˆã‚¨ãƒãƒ«ã‚®ãƒ¼ã®æ¨™æº–åå·®
    std_task2 = 2.0  # ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã®æ¨™æº–åå·®
    
    lambda_1 = 1 / std_task1 â‰ˆ 0.67
    lambda_2 = 1 / std_task2 = 0.50
    
    # æ­£è¦åŒ–ã—ã¦åˆè¨ˆã‚’1ã«ã™ã‚‹
    lambda_1 = 0.67 / (0.67 + 0.50) â‰ˆ 0.57
    lambda_2 = 0.50 / (0.67 + 0.50) â‰ˆ 0.43
    

æ¼”ç¿’5ï¼šãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•ã®é¸æŠ Medium

**å•é¡Œï¼š** ä»¥ä¸‹ã®3ã¤ã®ã‚·ãƒŠãƒªã‚ªã«å¯¾ã—ã¦ã€æœ€é©ãªãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•ã‚’é¸æŠã—ã€ãã®ç†ç”±ã‚’è¿°ã¹ã‚ˆã€‚

**ã‚·ãƒŠãƒªã‚ªA** : ãƒ‡ãƒ¼ã‚¿30,000ã‚µãƒ³ãƒ—ãƒ«ã€GPUåˆ©ç”¨å¯èƒ½ã€ç²¾åº¦å„ªå…ˆã€å®Ÿè£…æœŸé™2é€±é–“

**ã‚·ãƒŠãƒªã‚ªB** : ãƒ‡ãƒ¼ã‚¿100,000ã‚µãƒ³ãƒ—ãƒ«ã€GPUè¤‡æ•°å°ã€æœ€é«˜ç²¾åº¦ãŒå¿…é ˆã€è¨ˆç®—æ™‚é–“åˆ¶ç´„ãªã—

**ã‚·ãƒŠãƒªã‚ªC** : æ—¢å­˜ã®RFãƒ¢ãƒ‡ãƒ«ã¨CGCNNãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã€çµ±åˆã—ãŸã„ã€ãƒªã‚¹ã‚¯å›é¿é‡è¦–

**è§£ç­”ï¼š**

**ã‚·ãƒŠãƒªã‚ªA â†’ Hybrid Early Fusion**

  * 30,000ã‚µãƒ³ãƒ—ãƒ«ã¯ä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã€Early Fusionã®ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ãŒæ´»ãã‚‹
  * å®Ÿè£…ãŒå˜ç´”ï¼ˆ2é€±é–“ã§ååˆ†å®Ÿè£…å¯èƒ½ï¼‰
  * CGCNNæ¯”7.3%ç²¾åº¦å‘ä¸ŠãŒæœŸå¾…ã§ãã‚‹

**ã‚·ãƒŠãƒªã‚ªB â†’ ALIGNN (Full)**

  * 100,000ã‚µãƒ³ãƒ—ãƒ«ã¯å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã€ALIGNNã®æ€§èƒ½ãŒæœ€å¤§åŒ–
  * GPUè¤‡æ•°å°ã§ä¸¦åˆ—è¨“ç·´å¯èƒ½
  * æœ€é«˜ç²¾åº¦ï¼ˆMAE 0.025 eV/atomï¼‰ãŒé”æˆå¯èƒ½

**ã‚·ãƒŠãƒªã‚ªC â†’ Hybrid Late Fusion**

  * æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ãã®ã¾ã¾æ´»ç”¨å¯èƒ½ï¼ˆå†è¨“ç·´ä¸è¦ï¼‰
  * ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã«ã‚ˆã‚Šã€ç‰‡æ–¹ã®ãƒ¢ãƒ‡ãƒ«å¤±æ•—æ™‚ã‚‚å®‰å®šå‹•ä½œ
  * ãƒªã‚¹ã‚¯åˆ†æ•£ãŒæœ€ã‚‚åŠ¹æœçš„

æ¼”ç¿’6ï¼šEarly Fusionã®å®Ÿè£… Hard

**å•é¡Œï¼š** Hybrid Early Fusionãƒ¢ãƒ‡ãƒ«ã«**Attentionæ©Ÿæ§‹** ã‚’å°å…¥ã—ã€çµ„æˆç‰¹å¾´é‡ã¨GNNåŸ‹ã‚è¾¼ã¿ã®é‡è¦åº¦ã‚’å‹•çš„ã«èª¿æ•´ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’è¨˜è¿°ã›ã‚ˆã€‚

**è§£ç­”ï¼š**
    
    
    # Attentionæ©Ÿæ§‹ä»˜ãEarly Fusion
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    class AttentionEarlyFusion(nn.Module):
        def __init__(self, composition_dim=145, gnn_dim=128):
            super(AttentionEarlyFusion, self).__init__()
    
            # ç‰¹å¾´é‡å¤‰æ›å±¤ï¼ˆåŒã˜æ¬¡å…ƒã«çµ±ä¸€ï¼‰
            self.comp_transform = nn.Linear(composition_dim, gnn_dim)
            # GNNéƒ¨åˆ†ï¼ˆçœç•¥ã€CGCNNã¨åŒã˜ï¼‰
    
            # Attentionæ©Ÿæ§‹
            self.attention_comp = nn.Linear(gnn_dim, 1)
            self.attention_gnn = nn.Linear(gnn_dim, 1)
    
            # äºˆæ¸¬å±¤
            self.fc = nn.Sequential(
                nn.Linear(gnn_dim, 64),
                nn.Softplus(),
                nn.Linear(64, 1)
            )
    
        def forward(self, data, composition_features):
            # çµ„æˆç‰¹å¾´é‡ã‚’å¤‰æ›ï¼ˆ145 â†’ 128æ¬¡å…ƒï¼‰
            comp_transformed = self.comp_transform(composition_features)
    
            # GNNåŸ‹ã‚è¾¼ã¿è¨ˆç®—ï¼ˆçœç•¥ã€CGCNNã¨åŒã˜å‡¦ç†ï¼‰
            # gnn_embedding = ... (shape: batch_size, 128)
    
            # Attentioné‡ã¿ã®è¨ˆç®—
            alpha_comp = self.attention_comp(comp_transformed)  # (batch_size, 1)
            alpha_gnn = self.attention_gnn(gnn_embedding)       # (batch_size, 1)
    
            # Softmaxæ­£è¦åŒ–
            attention_weights = F.softmax(torch.cat([alpha_comp, alpha_gnn], dim=1), dim=1)
            w_comp = attention_weights[:, 0:1]  # çµ„æˆç‰¹å¾´é‡ã®é‡ã¿
            w_gnn = attention_weights[:, 1:2]   # GNNåŸ‹ã‚è¾¼ã¿ã®é‡ã¿
    
            # é‡ã¿ä»˜ãçµ±åˆ
            hybrid_features = w_comp * comp_transformed + w_gnn * gnn_embedding
    
            # äºˆæ¸¬
            out = self.fc(hybrid_features)
            return out.squeeze(), w_comp.squeeze(), w_gnn.squeeze()
    
    # ä½¿ç”¨ä¾‹
    model = AttentionEarlyFusion().to(device)
    # ... è¨“ç·´ ...
    
    # æ¨è«–æ™‚ã«Attentioné‡ã¿ã‚’ç¢ºèª
    model.eval()
    with torch.no_grad():
        pred, w_comp, w_gnn = model(test_data, test_comp_feats)
        print(f"çµ„æˆç‰¹å¾´é‡é‡ã¿: {w_comp.mean():.3f}")
        print(f"GNNåŸ‹ã‚è¾¼ã¿é‡ã¿: {w_gnn.mean():.3f}")
    
    # å‡ºåŠ›ä¾‹ï¼š
    # çµ„æˆç‰¹å¾´é‡é‡ã¿: 0.285
    # GNNåŸ‹ã‚è¾¼ã¿é‡ã¿: 0.715
    # â†’ ãƒ‡ãƒ¼ã‚¿ã«å¿œã˜ã¦å‹•çš„ã«é‡ã¿èª¿æ•´
    

æ¼”ç¿’7ï¼šè² ã®è»¢ç§»ã®æ¤œå‡º Hard

**å•é¡Œï¼š** ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã«ãŠã„ã¦ã€ã€Œè² ã®è»¢ç§»ï¼ˆNegative Transferï¼‰ã€ãŒç™ºç”Ÿã—ã¦ã„ã‚‹ã‹ã‚’æ¤œå‡ºã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ã€ãã®å¯¾ç­–ã‚’3ã¤æŒ™ã’ã‚ˆã€‚

**è§£ç­”ï¼š**

**è² ã®è»¢ç§»ã®æ¤œå‡ºæ‰‹æ³•ï¼š**
    
    
    # è² ã®è»¢ç§»ã®æ¤œå‡º
    # å˜ä¸€ã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æ¯”è¼ƒ
    
    # å˜ä¸€ã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
    single_task1_model = train_single_task(task1_data)
    single_task2_model = train_single_task(task2_data)
    
    # ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
    multi_task_model = train_multi_task(task1_data, task2_data)
    
    # æ€§èƒ½è©•ä¾¡
    mae_single_task1 = evaluate(single_task1_model, task1_test_data)
    mae_single_task2 = evaluate(single_task2_model, task2_test_data)
    
    mae_multi_task1 = evaluate_multitask(multi_task_model, task1_test_data, task_idx=0)
    mae_multi_task2 = evaluate_multitask(multi_task_model, task2_test_data, task_idx=1)
    
    # è² ã®è»¢ç§»ã®åˆ¤å®š
    if mae_multi_task1 > mae_single_task1:
        print("Task 1ã§è² ã®è»¢ç§»ç™ºç”Ÿ")
    if mae_multi_task2 > mae_single_task2:
        print("Task 2ã§è² ã®è»¢ç§»ç™ºç”Ÿ")
    
    # å‡ºåŠ›ä¾‹ï¼š
    # Task 1ã§è² ã®è»¢ç§»ç™ºç”Ÿï¼ˆãƒãƒ«ãƒ 0.0295 > å˜ä¸€ 0.0286ï¼‰
    # â†’ ã‚¿ã‚¹ã‚¯é–“ã®ç›¸é–¢ãŒä½ã„ã€ã¾ãŸã¯ã‚¿ã‚¹ã‚¯é‡ã¿ãŒä¸é©åˆ‡
    

**è² ã®è»¢ç§»ã®å¯¾ç­–ï¼š**

  1. **ã‚¿ã‚¹ã‚¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°** ï¼šç›¸é–¢ã®é«˜ã„ã‚¿ã‚¹ã‚¯ã®ã¿ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ 
         
         # ã‚¿ã‚¹ã‚¯é–“ã®ç›¸é–¢ã‚’è¨ˆç®—
         from scipy.stats import pearsonr
         
         # Task 1ã¨Task 2ã®äºˆæ¸¬å€¤ã®ç›¸é–¢
         corr, _ = pearsonr(y_pred_task1, y_pred_task2)
         
         if corr > 0.5:
             print("é«˜ç›¸é–¢ â†’ ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’æ¨å¥¨")
         else:
             print("ä½ç›¸é–¢ â†’ å˜ä¸€ã‚¿ã‚¹ã‚¯å­¦ç¿’æ¨å¥¨")
         

  2. **ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®å±¤ã‚’å¢—ã‚„ã™** ï¼šå…±æœ‰å±¤ã‚’æµ…ãã—ã€ã‚¿ã‚¹ã‚¯å›ºæœ‰å±¤ã‚’æ·±ãã™ã‚‹ã“ã¨ã§è² ã®è»¢ç§»ã‚’æŠ‘åˆ¶ 
         
         self.shared_layers = nn.Sequential(  # å…±æœ‰: 2å±¤ã®ã¿
             nn.Linear(input_dim, 128),
             nn.Softplus()
         )
         
         self.task1_layers = nn.Sequential(  # ã‚¿ã‚¹ã‚¯å›ºæœ‰: 3å±¤
             nn.Linear(128, 128),
             nn.Softplus(),
             nn.Linear(128, 64),
             nn.Softplus(),
             nn.Linear(64, 1)
         )
         

  3. **å‹•çš„ã‚¿ã‚¹ã‚¯é‡ã¿èª¿æ•´** ï¼šè¨“ç·´ä¸­ã«ã‚¿ã‚¹ã‚¯é‡ã¿ã‚’é©å¿œçš„ã«å¤‰æ›´ 
         
         # Uncertainty Weightingï¼ˆä¸ç¢ºå®Ÿæ€§ã«åŸºã¥ãé‡ã¿èª¿æ•´ï¼‰
         class MultiTaskUncertaintyWeighting(nn.Module):
             def __init__(self, n_tasks=2):
                 super().__init__()
                 self.log_vars = nn.Parameter(torch.zeros(n_tasks))
         
             def forward(self, losses):
                 # ã‚¿ã‚¹ã‚¯kã®é‡ã¿: 1 / (2 * Ïƒ_kÂ²)
                 weighted_losses = []
                 for i, loss in enumerate(losses):
                     precision = torch.exp(-self.log_vars[i])
                     weighted_loss = precision * loss + self.log_vars[i]
                     weighted_losses.append(weighted_loss)
                 return sum(weighted_losses)
         
         # ä½¿ç”¨ä¾‹
         uncertainty_weighting = MultiTaskUncertaintyWeighting(n_tasks=2)
         total_loss = uncertainty_weighting([loss_task1, loss_task2])
         

æ¼”ç¿’8ï¼šãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆå¯èƒ½æ€§ Hard

**å•é¡Œï¼š** Hybrid Early Fusionãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€ã€Œçµ„æˆç‰¹å¾´é‡ã¨GNNåŸ‹ã‚è¾¼ã¿ã®ã©ã¡ã‚‰ãŒäºˆæ¸¬ã«å¯„ä¸ã—ã¦ã„ã‚‹ã‹ã€ã‚’å®šé‡çš„ã«åˆ†æã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ã€å®Ÿè£…ã›ã‚ˆã€‚

**è§£ç­”ï¼š**
    
    
    # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆå¯èƒ½æ€§åˆ†æ
    import numpy as np
    from sklearn.inspection import permutation_importance
    
    def hybrid_feature_importance_analysis(model, test_data, test_comp_feats, test_targets):
        """
        çµ„æˆç‰¹å¾´é‡ã¨GNNåŸ‹ã‚è¾¼ã¿ã®å¯„ä¸åº¦ã‚’åˆ†æ
    
        Returns:
        --------
        comp_importance : float
            çµ„æˆç‰¹å¾´é‡ã®é‡è¦åº¦
        gnn_importance : float
            GNNåŸ‹ã‚è¾¼ã¿ã®é‡è¦åº¦
        """
        model.eval()
    
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³äºˆæ¸¬ï¼ˆé€šå¸¸ã®äºˆæ¸¬ï¼‰
        with torch.no_grad():
            baseline_pred = model(test_data, test_comp_feats).cpu().numpy()
        baseline_mae = mean_absolute_error(test_targets, baseline_pred)
    
        # çµ„æˆç‰¹å¾´é‡ã‚’ã‚¼ãƒ­ã«ã—ãŸå ´åˆã®äºˆæ¸¬
        zero_comp_feats = torch.zeros_like(test_comp_feats)
        with torch.no_grad():
            pred_no_comp = model(test_data, zero_comp_feats).cpu().numpy()
        mae_no_comp = mean_absolute_error(test_targets, pred_no_comp)
    
        # GNNåŸ‹ã‚è¾¼ã¿ã‚’ã‚¼ãƒ­ã«ã—ãŸå ´åˆã®äºˆæ¸¬ï¼ˆãƒ¢ãƒ‡ãƒ«å†…éƒ¨ã‚’å¤‰æ›´ï¼‰
        # ç°¡ç•¥ç‰ˆï¼šGNNéƒ¨åˆ†ã‚’ãƒã‚¹ã‚¯ã™ã‚‹ä»£ã‚ã‚Šã«ã€åˆ¥é€”GNNåŸ‹ã‚è¾¼ã¿ãªã—ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
        # ã“ã“ã§ã¯Permutation Importanceã‚’ä½¿ç”¨
    
        # çµ„æˆç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆMAEå¢—åŠ é‡ï¼‰
        comp_importance = mae_no_comp - baseline_mae
    
        # GNNåŸ‹ã‚è¾¼ã¿ã®é‡è¦åº¦ï¼ˆé¡æ¨ï¼šãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼‰
        n_permutations = 10
        gnn_mae_increases = []
    
        for _ in range(n_permutations):
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼ˆGNNåŸ‹ã‚è¾¼ã¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ åŒ–ï¼‰
            shuffled_indices = np.random.permutation(len(test_data))
            shuffled_data = [test_data[i] for i in shuffled_indices]
    
            with torch.no_grad():
                pred_shuffled = model(Batch.from_data_list(shuffled_data).to(device),
                                       test_comp_feats).cpu().numpy()
            mae_shuffled = mean_absolute_error(test_targets, pred_shuffled)
            gnn_mae_increases.append(mae_shuffled - baseline_mae)
    
        gnn_importance = np.mean(gnn_mae_increases)
    
        return comp_importance, gnn_importance
    
    # å®Ÿè¡Œ
    comp_imp, gnn_imp = hybrid_feature_importance_analysis(
        hybrid_model, test_data_list, test_comp_feats, test_targets
    )
    
    # ç›¸å¯¾çš„é‡è¦åº¦ã‚’è¨ˆç®—
    total_imp = comp_imp + gnn_imp
    comp_ratio = comp_imp / total_imp * 100
    gnn_ratio = gnn_imp / total_imp * 100
    
    print(f"=== ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡é‡è¦åº¦ ===")
    print(f"çµ„æˆç‰¹å¾´é‡ã®å¯„ä¸: {comp_ratio:.1f}%")
    print(f"GNNåŸ‹ã‚è¾¼ã¿ã®å¯„ä¸: {gnn_ratio:.1f}%")
    
    # å¯è¦–åŒ–
    import matplotlib.pyplot as plt
    
    fig, ax = plt.subplots(figsize=(8, 6))
    ax.bar(['çµ„æˆç‰¹å¾´é‡', 'GNNåŸ‹ã‚è¾¼ã¿'], [comp_ratio, gnn_ratio],
           color=['#667eea', '#764ba2'])
    ax.set_ylabel('ç›¸å¯¾çš„é‡è¦åº¦ (%)', fontsize=12)
    ax.set_title('ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡å¯„ä¸åº¦', fontsize=14, fontweight='bold')
    ax.set_ylim(0, 100)
    
    for i, v in enumerate([comp_ratio, gnn_ratio]):
        ax.text(i, v + 2, f'{v:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('hybrid_feature_importance.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # å‡ºåŠ›ä¾‹ï¼š
    # === ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡é‡è¦åº¦ ===
    # çµ„æˆç‰¹å¾´é‡ã®å¯„ä¸: 32.5%
    # GNNåŸ‹ã‚è¾¼ã¿ã®å¯„ä¸: 67.5%
    # â†’ GNNåŸ‹ã‚è¾¼ã¿ãŒã‚ˆã‚Šé‡è¦ã ãŒã€çµ„æˆæƒ…å ±ã‚‚æœ‰æ„ã«å¯„ä¸
    

## å‚è€ƒæ–‡çŒ®

  1. Choudhary, K., DeCost, B. (2021). Atomistic Line Graph Neural Network for improved materials property predictions. _npj Computational Materials_ , 7(1), 185, pp. 1-8.
  2. Chen, C., Ye, W., Zuo, Y., Zheng, C., Ong, S. P. (2019). Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals. _Chemistry of Materials_ , 31(9), 3564-3572, pp. 3564-3572.
  3. Ruder, S. (2017). An Overview of Multi-Task Learning in Deep Neural Networks. _arXiv preprint arXiv:1706.05098_ , pp. 1-13.
  4. Crawshaw, M. (2020). Multi-Task Learning with Deep Neural Networks: A Survey. _arXiv preprint arXiv:2009.09796_ , pp. 1-23.
  5. Fung, V., Zhang, J., Juarez, E., Sumpter, B. G. (2021). Benchmarking graph neural networks for materials chemistry. _npj Computational Materials_ , 7(1), 84, pp. 1-8.
  6. Goodall, R. E. A., Lee, A. A. (2020). Predicting materials properties without crystal structure: Deep representation learning from stoichiometry. _Nature Communications_ , 11, 6280, pp. 1-9.
  7. VeliÄkoviÄ‡, P., Cucurull, G., Casanova, A., Romero, A., LiÃ², P., Bengio, Y. (2018). Graph Attention Networks. _International Conference on Learning Representations_ , pp. 1-12.

â† ç¬¬4ç« ï¼šçµ„æˆãƒ™ãƒ¼ã‚¹ vs GNNå®šé‡çš„æ¯”è¼ƒï¼ˆæº–å‚™ä¸­ï¼‰ [ç¬¬6ç« ï¼šPyTorch Geometricãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ â†’](<chapter-6.html>)

### å…è²¬äº‹é …

  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹Code examplesã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚
  * å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚
