<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Á¨¨4Á´†ÔºöÊùêÊñôÊé¢Á¥¢„Å∏„ÅÆÂøúÁî®„Å®ÂÆüË∑µ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Á¨¨4Á´†ÔºöÊùêÊñôÊé¢Á¥¢„Å∏„ÅÆÂøúÁî®„Å®ÂÆüË∑µ</h1>
            <p class="subtitle">„Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„ÉªDFT„ÉªÂÆüÈ®ì„É≠„Éú„ÉÉ„Éà„Å®„ÅÆÁµ±Âêà</p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 25-30ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ‰∏äÁ¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 7ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 3Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Á¨¨4Á´†ÔºöÊùêÊñôÊé¢Á¥¢„Å∏„ÅÆÂøúÁî®„Å®ÂÆüË∑µ</h1>
<p><strong>„Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„ÉªDFT„ÉªÂÆüÈ®ì„É≠„Éú„ÉÉ„Éà„Å®„ÅÆÁµ±Âêà</strong></p>
<h2>Â≠¶ÁøíÁõÆÊ®ô</h2>
<p>„Åì„ÅÆÁ´†„ÇíË™≠„ÇÄ„Åì„Å®„Åß„ÄÅ‰ª•‰∏ã„ÇíÁøíÂæó„Åß„Åç„Åæ„ÅôÔºö</p>
<ul>
<li>‚úÖ Active Learning„Å®„Éô„Ç§„Ç∫O„ÅÆÁµ±ÂêàÊâãÊ≥ï„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>‚úÖ È´ò„Çπ„É´„Éº„Éó„ÉÉ„ÉàË®àÁÆó„Å´ÊúÄÈÅ©Âåñ„ÇíÈÅ©Áî®„Åß„Åç„Çã</li>
<li>‚úÖ „ÇØ„É≠„Éº„Ç∫„Éâ„É´„Éº„Éó„Ç∑„Çπ„ÉÜ„É†„ÇíË®≠Ë®à„Åß„Åç„Çã</li>
<li>‚úÖ Áî£Ê•≠ÂøúÁî®‰∫ã‰æã5„Å§„Åã„ÇâÂÆüË∑µÁöÑÁü•Ë≠ò„ÇíÂæó„Çã</li>
<li>‚úÖ „Ç≠„É£„É™„Ç¢„Éë„Çπ„ÇíÂÖ∑‰ΩìÁöÑ„Å´Êèè„Åë„Çã</li>
</ul>
<p><strong>Ë™≠‰∫ÜÊôÇÈñì</strong>: 25-30ÂàÜ
<strong>„Ç≥„Éº„Éâ‰æã</strong>: 7ÂÄã
<strong>ÊºîÁøíÂïèÈ°å</strong>: 3Âïè</p>
<hr />
<h2>4.1 Active Learning √ó „Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ</h2>
<h3>„Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„Å®„ÅÆÁµ±Âêà</h3>
<p>Active Learning„Å®Bayesian Optimization„ÅØÂØÜÊé•„Å´Èñ¢ÈÄ£„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ</p>
<p><strong>ÂÖ±ÈÄöÁÇπ</strong>:
- ‰∏çÁ¢∫ÂÆüÊÄß„ÇíÊ¥ªÁî®„Åó„ÅüË≥¢„ÅÑ„Çµ„É≥„Éó„É™„É≥„Ç∞
- „Ç¨„Ç¶„ÇπÈÅéÁ®ã„Å´„Çà„Çã‰ª£ÁêÜ„É¢„Éá„É´
- Áç≤ÂæóÈñ¢Êï∞„ÅßÊ¨°ÂÄôË£ú„ÇíÈÅ∏Êäû</p>
<p><strong>ÈÅï„ÅÑ</strong>:
- <strong>Active Learning</strong>: „É¢„Éá„É´ÊîπÂñÑ„ÅåÁõÆÁöÑ
- <strong>Bayesian Optimization</strong>: ÁõÆÁöÑÈñ¢Êï∞„ÅÆÊúÄÂ§ßÂåñ„ÅåÁõÆÁöÑ</p>
<h3>BoTorch„Å´„Çà„ÇãÁµ±ÂêàÂÆüË£Ö</h3>
<p><strong>„Ç≥„Éº„Éâ‰æã1: Active Learning + „Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ</strong></p>
<pre><code class="language-python">import torch
import numpy as np
from botorch.models import SingleTaskGP
from botorch.acquisition import UpperConfidenceBound, qExpectedImprovement
from botorch.optim import optimize_acqf
from botorch.fit import fit_gpytorch_model
from gpytorch.mlls import ExactMarginalLogLikelihood
from sklearn.metrics import mean_squared_error


class ActiveBayesianOptimizer:
    &quot;&quot;&quot;Active LearningÁµ±ÂêàÂûã„Éô„Ç§„Ç∫ÊúÄÈÅ©ÂåñÂô®&quot;&quot;&quot;

    def __init__(self, bounds, mode='exploration'):
        &quot;&quot;&quot;
        Parameters
        ----------
        bounds : torch.Tensor
            Êé¢Á¥¢Á©∫Èñì„ÅÆÂ¢ÉÁïå (2 x d: [‰∏ãÈôê, ‰∏äÈôê])
        mode : str
            'exploration' (Active Learning) or 'exploitation' (BO)
        &quot;&quot;&quot;
        self.bounds = bounds
        self.mode = mode
        self.train_X = None
        self.train_Y = None
        self.model = None

    def fit(self, X, Y):
        &quot;&quot;&quot;GP„É¢„Éá„É´„ÇíÂ≠¶Áøí„Éá„Éº„Çø„Å´ÈÅ©Âêà&quot;&quot;&quot;
        self.train_X = torch.tensor(X, dtype=torch.float64)
        self.train_Y = torch.tensor(Y, dtype=torch.float64).unsqueeze(-1)

        # SingleTaskGP„É¢„Éá„É´„ÅÆÊßãÁØâ
        self.model = SingleTaskGP(self.train_X, self.train_Y)
        mll = ExactMarginalLogLikelihood(self.model.likelihood, self.model)
        fit_gpytorch_model(mll)

    def suggest_next(self, n_candidates=1):
        &quot;&quot;&quot;Ê¨°„ÅÆÂÆüÈ®ìÂÄôË£ú„ÇíÊèêÊ°à&quot;&quot;&quot;
        if self.mode == 'exploration':
            # Active Learning: ‰∏çÁ¢∫ÂÆüÊÄßÈáçË¶ñ
            acq_function = UpperConfidenceBound(
                self.model, beta=2.0  # È´ò„ÅÑbeta = Êé¢Á¥¢ÈáçË¶ñ
            )
        else:
            # Bayesian Optimization: ÊîπÂñÑÈáçË¶ñ
            acq_function = qExpectedImprovement(
                self.model, best_f=self.train_Y.max()
            )

        # Áç≤ÂæóÈñ¢Êï∞„ÇíÊúÄÂ§ßÂåñ
        candidates, acq_value = optimize_acqf(
            acq_function,
            bounds=self.bounds,
            q=n_candidates,
            num_restarts=20,
            raw_samples=512,
        )

        return candidates.numpy(), acq_value.item()

    def predict(self, X_test):
        &quot;&quot;&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø„ÅÆ‰∫àÊ∏¨„Å®‰∏çÁ¢∫ÂÆüÊÄß&quot;&quot;&quot;
        X_test_tensor = torch.tensor(X_test, dtype=torch.float64)
        with torch.no_grad():
            posterior = self.model.posterior(X_test_tensor)
            mean = posterior.mean.numpy()
            variance = posterior.variance.numpy()
        return mean, np.sqrt(variance)


# ‰ΩøÁî®‰æã: ÊùêÊñôÁâ©ÊÄß„ÅÆÊúÄÈÅ©Âåñ
def bandgap_oracle(X):
    &quot;&quot;&quot;‰ªÆÊÉ≥ÁöÑ„Å™„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„ÉóË®àÁÆóÔºàÂÆüÈöõ„ÅØDFTÔºâ&quot;&quot;&quot;
    return 2.0 * np.sin(X[:, 0] * 3) + np.cos(X[:, 1] * 2) + np.random.normal(0, 0.1, X.shape[0])


# ÂàùÊúü„Éá„Éº„ÇøÔºà„É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞Ôºâ
np.random.seed(42)
bounds = torch.tensor([[0.0, 0.0], [5.0, 5.0]], dtype=torch.float64)
X_init = np.random.uniform(0, 5, (10, 2))
Y_init = bandgap_oracle(X_init)

# „Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂„ÅÆÂàùÊúüÂåñ
optimizer = ActiveBayesianOptimizer(bounds, mode='exploration')
optimizer.fit(X_init, Y_init)

# Active Learning„É´„Éº„ÉóÔºà10ÂõûÔºâ
X_train = X_init.copy()
Y_train = Y_init.copy()

for iteration in range(10):
    # Ê¨°„ÅÆÂÄôË£ú„ÇíÊèêÊ°à
    X_next, acq_val = optimizer.suggest_next(n_candidates=1)

    # ÂÆüÈ®ìÂÆüË°åÔºà„Åæ„Åü„ÅØË®àÁÆóÔºâ
    Y_next = bandgap_oracle(X_next)

    # „Éá„Éº„ÇøËøΩÂä†
    X_train = np.vstack([X_train, X_next])
    Y_train = np.append(Y_train, Y_next)

    # „É¢„Éá„É´ÂÜçÂ≠¶Áøí
    optimizer.fit(X_train, Y_train)

    print(f&quot;Iteration {iteration + 1}:&quot;)
    print(f&quot;  Next X: {X_next[0]}&quot;)
    print(f&quot;  Measured Y: {Y_next[0]:.3f}&quot;)
    print(f&quot;  Acquisition Value: {acq_val:.3f}&quot;)
    print(f&quot;  Best Y so far: {Y_train.max():.3f}\n&quot;)

# ÊúÄÁµÇÊÄßËÉΩË©ï‰æ°
X_test = np.random.uniform(0, 5, (100, 2))
Y_test = bandgap_oracle(X_test)
Y_pred, Y_std = optimizer.predict(X_test)
rmse = np.sqrt(mean_squared_error(Y_test, Y_pred.squeeze()))

print(&quot;=&quot; * 50)
print(f&quot;Final Model Performance:&quot;)
print(f&quot;  Test RMSE: {rmse:.4f}&quot;)
print(f&quot;  Best bandgap found: {Y_train.max():.3f}&quot;)
print(f&quot;  at composition: {X_train[Y_train.argmax()]}&quot;)
</code></pre>
<p><strong>Âá∫Âäõ‰æã</strong>:</p>
<pre><code>Iteration 1:
  Next X: [2.87 4.12]
  Measured Y: 2.456
  Acquisition Value: 1.823
  Best Y so far: 2.851

Iteration 2:
  Next X: [1.23 3.45]
  Measured Y: 2.912
  Acquisition Value: 1.654
  Best Y so far: 2.912

...

==================================================
Final Model Performance:
  Test RMSE: 0.1872
  Best bandgap found: 3.124
  at composition: [4.21 2.89]
</code></pre>
<hr />
<h2>4.2 Active Learning √ó È´ò„Çπ„É´„Éº„Éó„ÉÉ„ÉàË®àÁÆó</h2>
<h3>DFTË®àÁÆó„ÅÆÂäπÁéáÂåñ</h3>
<p><strong>Ë™≤È°å</strong>: DFTË®àÁÆó„ÅØ1„Çµ„É≥„Éó„É´Êï∞ÊôÇÈñì„ÄúÊï∞Êó•</p>
<p><strong>Ëß£Ê±∫Á≠ñ</strong>: Active Learning„ÅßË®àÁÆó„Åô„Åπ„Åç„Çµ„É≥„Éó„É´„ÇíÂÑ™ÂÖàÈ†Ü‰Ωç‰ªò„Åë</p>
<p><strong>„Ç≥„Éº„Éâ‰æã2: DFTË®àÁÆó„ÅÆÂÑ™ÂÖàÈ†Ü‰Ωç‰ªò„Åë</strong></p>
<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel
from pymatgen.core import Composition
from mp_api.client import MPRester
from typing import List, Tuple, Dict


class DFTPrioritizer:
    &quot;&quot;&quot;DFTË®àÁÆó„ÇíÂÑ™ÂÖàÈ†Ü‰Ωç‰ªò„Åë„Åô„ÇãActive Learning„Ç∑„Çπ„ÉÜ„É†&quot;&quot;&quot;

    def __init__(self, api_key: str = None):
        &quot;&quot;&quot;
        Parameters
        ----------
        api_key : str
            Materials Project API„Ç≠„Éº
        &quot;&quot;&quot;
        self.api_key = api_key
        self.gp_model = None
        self.calculated_materials = []
        self.pending_materials = []

    def fetch_candidate_materials(
        self,
        elements: List[str],
        max_candidates: int = 100
    ) -&gt; pd.DataFrame:
        &quot;&quot;&quot;Materials Project„Åã„ÇâÂÄôË£úÊùêÊñô„ÇíÂèñÂæó&quot;&quot;&quot;
        if self.api_key:
            with MPRester(self.api_key) as mpr:
                # Êó¢Áü•ÊùêÊñô„ÇíÊ§úÁ¥¢
                docs = mpr.materials.summary.search(
                    elements=elements,
                    fields=[&quot;material_id&quot;, &quot;formula_pretty&quot;, &quot;band_gap&quot;,
                            &quot;formation_energy_per_atom&quot;, &quot;energy_above_hull&quot;]
                )

                candidates = []
                for doc in docs[:max_candidates]:
                    candidates.append({
                        'material_id': doc.material_id,
                        'formula': doc.formula_pretty,
                        'bandgap': doc.band_gap,
                        'formation_energy': doc.formation_energy_per_atom,
                        'stability': doc.energy_above_hull
                    })

                return pd.DataFrame(candidates)
        else:
            # „Éá„É¢Áî®„ÉÄ„Éü„Éº„Éá„Éº„Çø
            print(&quot;Warning: No API key provided, using dummy data&quot;)
            return self._generate_dummy_materials(elements, max_candidates)

    def _generate_dummy_materials(
        self,
        elements: List[str],
        n: int
    ) -&gt; pd.DataFrame:
        &quot;&quot;&quot;„Éá„É¢Áî®„ÅÆ„ÉÄ„Éü„ÉºÊùêÊñô„Éá„Éº„Çø„ÇíÁîüÊàê&quot;&quot;&quot;
        np.random.seed(42)
        materials = []

        for i in range(n):
            # „É©„É≥„ÉÄ„É†„Å™ÁµÑÊàê
            composition = {elem: np.random.randint(1, 4) for elem in elements}
            formula = ''.join([f&quot;{k}{v}&quot; for k, v in composition.items()])

            materials.append({
                'material_id': f'mp-{10000 + i}',
                'formula': formula,
                'bandgap': None,  # Êú™Ë®àÁÆó
                'formation_energy': np.random.uniform(-3, 0),
                'stability': np.random.uniform(0, 0.5)
            })

        return pd.DataFrame(materials)

    def featurize(self, df: pd.DataFrame) -&gt; np.ndarray:
        &quot;&quot;&quot;ÁµÑÊàê„Åã„ÇâË®òËø∞Â≠ê„ÇíÁîüÊàê&quot;&quot;&quot;
        features = []

        for formula in df['formula']:
            comp = Composition(formula)
            # Á∞°ÊòìÁöÑ„Å™Ë®òËø∞Â≠ê: ÂÖÉÁ¥†Ââ≤Âêà
            elem_dict = comp.get_el_amt_dict()
            total = sum(elem_dict.values())

            # ‰∏ªË¶ÅÂÖÉÁ¥†„ÅÆÂâ≤Âêà„ÇíÁâπÂæ¥Èáè„Å´
            feature_vec = [
                elem_dict.get('Li', 0) / total,
                elem_dict.get('Co', 0) / total,
                elem_dict.get('O', 0) / total,
                elem_dict.get('Mn', 0) / total,
                comp.num_atoms,  # ÂéüÂ≠êÊï∞
                comp.average_electroneg,  # Âπ≥ÂùáÈõªÊ∞óÈô∞ÊÄßÂ∫¶
            ]
            features.append(feature_vec)

        return np.array(features)

    def train_surrogate_model(self, X_train: np.ndarray, y_train: np.ndarray):
        &quot;&quot;&quot;‰ª£ÁêÜ„É¢„Éá„É´ÔºàGPÔºâ„ÇíÂ≠¶Áøí&quot;&quot;&quot;
        kernel = ConstantKernel(1.0) * RBF(length_scale=1.0)
        self.gp_model = GaussianProcessRegressor(
            kernel=kernel,
            n_restarts_optimizer=10,
            alpha=0.1
        )
        self.gp_model.fit(X_train, y_train)

    def prioritize_by_uncertainty(
        self,
        candidates_df: pd.DataFrame,
        top_k: int = 10
    ) -&gt; pd.DataFrame:
        &quot;&quot;&quot;‰∏çÁ¢∫ÂÆüÊÄß„Å´Âü∫„Å•„ÅÑ„Å¶Ë®àÁÆóÂÑ™ÂÖàÈ†Ü‰Ωç„Çí‰ªò‰∏é&quot;&quot;&quot;
        if self.gp_model is None:
            raise ValueError(&quot;Surrogate model not trained yet&quot;)

        # ÁâπÂæ¥ÈáèÂåñ
        X_candidates = self.featurize(candidates_df)

        # ‰∫àÊ∏¨„Å®‰∏çÁ¢∫ÂÆüÊÄß
        y_pred, y_std = self.gp_model.predict(X_candidates, return_std=True)

        # ÁµêÊûú„ÇíËøΩÂä†
        candidates_df = candidates_df.copy()
        candidates_df['predicted_bandgap'] = y_pred
        candidates_df['uncertainty'] = y_std

        # ‰∏çÁ¢∫ÂÆüÊÄß„Åß„ÇΩ„Éº„ÉàÔºàÈôçÈ†ÜÔºâ
        prioritized = candidates_df.sort_values('uncertainty', ascending=False)

        return prioritized.head(top_k)

    def simulate_dft_calculation(self, material_id: str) -&gt; float:
        &quot;&quot;&quot;DFTË®àÁÆó„Çí„Ç∑„Éü„É•„É¨„Éº„ÉàÔºàÂÆüÈöõ„ÅØVASP/Quantum EspressoÂÆüË°åÔºâ&quot;&quot;&quot;
        # „ÉÄ„Éü„ÉºË®àÁÆóÔºö„É©„É≥„ÉÄ„É†„Å™„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó
        np.random.seed(hash(material_id) % 2**32)
        return np.random.uniform(0.5, 4.0)


# ‰ΩøÁî®‰æã: „Éê„ÉÉ„ÉÜ„É™„ÉºÊùêÊñô„ÅÆ„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„ÉóË®àÁÆó
print(&quot;=&quot; * 60)
print(&quot;DFT Active Learning Workflow&quot;)
print(&quot;=&quot; * 60)

# 1. „Ç∑„Çπ„ÉÜ„É†ÂàùÊúüÂåñ
prioritizer = DFTPrioritizer(api_key=None)  # „Éá„É¢„É¢„Éº„Éâ

# 2. ÂÄôË£úÊùêÊñô„ÅÆÂèñÂæó
elements = ['Li', 'Co', 'O', 'Mn']
candidates = prioritizer.fetch_candidate_materials(elements, max_candidates=50)
print(f&quot;\n[Step 1] Fetched {len(candidates)} candidate materials&quot;)
print(candidates.head())

# 3. ÂàùÊúü„Éá„Éº„ÇøÔºàÂ∞ëÊï∞„ÅÆDFTË®àÁÆóÊ∏à„ÅøÔºâ
initial_indices = np.random.choice(len(candidates), size=5, replace=False)
initial_df = candidates.iloc[initial_indices].copy()

# DFTË®àÁÆóÂÆüË°åÔºàÂàùÊúüÔºâ
initial_bandgaps = []
for mat_id in initial_df['material_id']:
    bg = prioritizer.simulate_dft_calculation(mat_id)
    initial_bandgaps.append(bg)

initial_df['bandgap'] = initial_bandgaps
print(f&quot;\n[Step 2] Initial DFT calculations: {len(initial_df)} materials&quot;)
print(initial_df[['formula', 'bandgap']])

# 4. ‰ª£ÁêÜ„É¢„Éá„É´Â≠¶Áøí
X_train = prioritizer.featurize(initial_df)
y_train = initial_df['bandgap'].values
prioritizer.train_surrogate_model(X_train, y_train)
print(&quot;\n[Step 3] Surrogate model trained&quot;)

# 5. Active Learning„É´„Éº„Éó
remaining_candidates = candidates[~candidates['material_id'].isin(initial_df['material_id'])]
n_iterations = 3

for iteration in range(n_iterations):
    print(f&quot;\n{'=' * 60}&quot;)
    print(f&quot;Active Learning Iteration {iteration + 1}&quot;)
    print('=' * 60)

    # ÂÑ™ÂÖàÈ†Ü‰Ωç‰ªò„Åë
    top_priority = prioritizer.prioritize_by_uncertainty(
        remaining_candidates,
        top_k=5
    )

    print(&quot;\nTop 5 high-uncertainty materials for DFT:&quot;)
    print(top_priority[['formula', 'predicted_bandgap', 'uncertainty']])

    # DFTË®àÁÆóÂÆüË°åÔºàÊúÄ„ÇÇ‰∏çÁ¢∫ÂÆü„Å™1„Å§Ôºâ
    next_material = top_priority.iloc[0]
    mat_id = next_material['material_id']
    true_bandgap = prioritizer.simulate_dft_calculation(mat_id)

    print(f&quot;\n[DFT Calculation]&quot;)
    print(f&quot;  Material: {next_material['formula']}&quot;)
    print(f&quot;  Predicted: {next_material['predicted_bandgap']:.3f} eV&quot;)
    print(f&quot;  Measured:  {true_bandgap:.3f} eV&quot;)
    print(f&quot;  Error: {abs(true_bandgap - next_material['predicted_bandgap']):.3f} eV&quot;)

    # „Éá„Éº„ÇøËøΩÂä†„Å®ÂÜçÂ≠¶Áøí
    new_data = pd.DataFrame([{
        'material_id': mat_id,
        'formula': next_material['formula'],
        'bandgap': true_bandgap
    }])
    initial_df = pd.concat([initial_df, new_data], ignore_index=True)

    X_train = prioritizer.featurize(initial_df)
    y_train = initial_df['bandgap'].values
    prioritizer.train_surrogate_model(X_train, y_train)

    # ÂÄôË£ú„É™„Çπ„Éà„Åã„ÇâÂâäÈô§
    remaining_candidates = remaining_candidates[
        remaining_candidates['material_id'] != mat_id
    ]

    print(f&quot;\nModel updated with {len(initial_df)} materials&quot;)

print(&quot;\n&quot; + &quot;=&quot; * 60)
print(&quot;Active Learning Complete&quot;)
print(&quot;=&quot; * 60)
print(f&quot;Total DFT calculations: {len(initial_df)}&quot;)
print(f&quot;Remaining candidates: {len(remaining_candidates)}&quot;)
print(f&quot;\nMaterials with bandgap &gt; 2.5 eV (solar cell candidates):&quot;)
solar_candidates = initial_df[initial_df['bandgap'] &gt; 2.5]
print(solar_candidates[['formula', 'bandgap']].sort_values('bandgap', ascending=False))
</code></pre>
<p><strong>Âá∫Âäõ‰æã</strong>:</p>
<pre><code>============================================================
DFT Active Learning Workflow
============================================================

[Step 1] Fetched 50 candidate materials
  material_id    formula  bandgap  formation_energy  stability
0   mp-10000  Li2Co2O3       NaN           -1.456      0.123
1   mp-10001  LiCoO2Mn1      NaN           -2.134      0.087
...

[Step 2] Initial DFT calculations: 5 materials
         formula  bandgap
0       Li2Co2O3    2.345
3       LiMnO2      1.876
...

[Step 3] Surrogate model trained

============================================================
Active Learning Iteration 1
============================================================

Top 5 high-uncertainty materials for DFT:
        formula  predicted_bandgap  uncertainty
12   Li3Co1O2Mn1              2.123        0.845
8    Li1Co3O1Mn2              1.987        0.782
...

[DFT Calculation]
  Material: Li3Co1O2Mn1
  Predicted: 2.123 eV
  Measured:  2.456 eV
  Error: 0.333 eV

Model updated with 6 materials

============================================================
Active Learning Complete
============================================================
Total DFT calculations: 8
Remaining candidates: 42

Materials with bandgap &gt; 2.5 eV (solar cell candidates):
        formula  bandgap
2   Li3Co1O2Mn1    2.456
0      Li2Co2O3    2.345
</code></pre>
<hr />
<h2>4.3 Active Learning √ó ÂÆüÈ®ì„É≠„Éú„ÉÉ„Éà</h2>
<h3>„ÇØ„É≠„Éº„Ç∫„Éâ„É´„Éº„ÉóÊúÄÈÅ©Âåñ</h3>
<div class="mermaid">
flowchart LR
    A[ÂÄôË£úÊèêÊ°à\nActive Learning] --> B[ÂÆüÈ®ìÂÆüË°å\n„É≠„Éú„ÉÉ„Éà]
    B --> C[Ê∏¨ÂÆö„ÉªË©ï‰æ°\n„Çª„É≥„Çµ„Éº]
    C --> D[„Éá„Éº„ÇøËìÑÁ©ç\n„Éá„Éº„Çø„Éô„Éº„Çπ]
    D --> E[„É¢„Éá„É´Êõ¥Êñ∞\nÊ©üÊ¢∞Â≠¶Áøí]
    E --> F[Áç≤ÂæóÈñ¢Êï∞Ë©ï‰æ°\nÊ¨°ÂÄôË£úÈÅ∏ÂÆö]
    F --> A

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#ffebee
    style F fill:#fce4ec
</div>

<p><strong>„Ç≥„Éº„Éâ‰æã3: „ÇØ„É≠„Éº„Ç∫„Éâ„É´„Éº„Éó„Ç∑„Çπ„ÉÜ„É†„ÅÆÂÆüË£Ö</strong></p>
<pre><code class="language-python">import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, List, Callable
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel
import time


class ClosedLoopSystem:
    &quot;&quot;&quot;Ëá™ÂæãÊùêÊñôÊé¢Á¥¢„ÅÆ„ÇØ„É≠„Éº„Ç∫„Éâ„É´„Éº„Éó„Ç∑„Çπ„ÉÜ„É†&quot;&quot;&quot;

    def __init__(
        self,
        experiment_function: Callable,
        feature_dim: int,
        bounds: np.ndarray
    ):
        &quot;&quot;&quot;
        Parameters
        ----------
        experiment_function : Callable
            ÂÆüÈ®ì„Åæ„Åü„ÅØ„É≠„Éú„ÉÉ„ÉàÂêàÊàê„ÇíÂÆüË°å„Åô„ÇãÈñ¢Êï∞
        feature_dim : int
            ÁâπÂæ¥Èáè„ÅÆÊ¨°ÂÖÉÊï∞
        bounds : np.ndarray
            Êé¢Á¥¢Á©∫Èñì„ÅÆÂ¢ÉÁïå (feature_dim x 2)
        &quot;&quot;&quot;
        self.experiment_function = experiment_function
        self.feature_dim = feature_dim
        self.bounds = bounds
        self.gp_model = None
        self.database = []
        self.iteration_count = 0

    def initialize(self, n_init: int = 5):
        &quot;&quot;&quot;„É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞„ÅßÂàùÊúüÂåñ&quot;&quot;&quot;
        print(&quot;=&quot; * 70)
        print(&quot;Closed-Loop System Initialization&quot;)
        print(&quot;=&quot; * 70)

        X_init = np.random.uniform(
            self.bounds[:, 0],
            self.bounds[:, 1],
            size=(n_init, self.feature_dim)
        )

        for i, x in enumerate(X_init):
            y = self.experiment_function(x)
            self.database.append({
                'iteration': 0,
                'timestamp': datetime.now(),
                'parameters': x,
                'performance': y,
                'acquisition_value': None
            })
            print(f&quot;  Init {i+1}/{n_init}: Parameters={x}, Performance={y:.3f}&quot;)

        # ÂàùÊúüGP„É¢„Éá„É´Â≠¶Áøí
        self._update_model()
        print(f&quot;\nInitialization complete: {len(self.database)} experiments\n&quot;)

    def _update_model(self):
        &quot;&quot;&quot;GP„É¢„Éá„É´„ÇíÊúÄÊñ∞„Éá„Éº„Çø„ÅßÊõ¥Êñ∞&quot;&quot;&quot;
        X = np.array([d['parameters'] for d in self.database])
        y = np.array([d['performance'] for d in self.database])

        kernel = ConstantKernel(1.0) * RBF(length_scale=1.0)
        self.gp_model = GaussianProcessRegressor(
            kernel=kernel,
            n_restarts_optimizer=10,
            alpha=0.1,
            normalize_y=True
        )
        self.gp_model.fit(X, y)

    def acquisition_function(self, X: np.ndarray, beta: float = 2.0) -&gt; np.ndarray:
        &quot;&quot;&quot;Upper Confidence BoundÁç≤ÂæóÈñ¢Êï∞&quot;&quot;&quot;
        mu, sigma = self.gp_model.predict(X.reshape(1, -1), return_std=True)
        return mu + beta * sigma

    def propose_next_experiment(self, n_candidates: int = 100) -&gt; Dict:
        &quot;&quot;&quot;Ê¨°„ÅÆÂÆüÈ®ìÊù°‰ª∂„ÇíÊèêÊ°à&quot;&quot;&quot;
        # „É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞„ÅßÂÄôË£úÁîüÊàê
        candidates = np.random.uniform(
            self.bounds[:, 0],
            self.bounds[:, 1],
            size=(n_candidates, self.feature_dim)
        )

        # Áç≤ÂæóÈñ¢Êï∞„ÇíË©ï‰æ°
        acq_values = np.array([
            self.acquisition_function(x) for x in candidates
        ]).flatten()

        # ÊúÄÂ§ßÂÄ§„ÇíÈÅ∏Êäû
        best_idx = np.argmax(acq_values)
        best_candidate = candidates[best_idx]
        best_acq_value = acq_values[best_idx]

        return {
            'parameters': best_candidate,
            'acquisition_value': best_acq_value
        }

    def execute_experiment(self, parameters: np.ndarray) -&gt; float:
        &quot;&quot;&quot;ÂÆüÈ®ìÂÆüË°åÔºà„É≠„Éú„ÉÉ„Éà„Åæ„Åü„ÅØË®àÁÆóÔºâ&quot;&quot;&quot;
        print(f&quot;  [Robot] Preparing experiment with parameters: {parameters}&quot;)
        time.sleep(0.1)  # „É≠„Éú„ÉÉ„ÉàÂãï‰Ωú„Çí„Ç∑„Éü„É•„É¨„Éº„Éà

        performance = self.experiment_function(parameters)

        print(f&quot;  [Sensor] Measured performance: {performance:.3f}&quot;)
        return performance

    def run_iteration(self):
        &quot;&quot;&quot;Active Learning„ÅÆ1„Ç§„ÉÜ„É¨„Éº„Ç∑„Éß„É≥ÂÆüË°å&quot;&quot;&quot;
        self.iteration_count += 1

        print(&quot;=&quot; * 70)
        print(f&quot;Iteration {self.iteration_count}&quot;)
        print(&quot;=&quot; * 70)

        # 1. ÂÄôË£úÊèêÊ°àÔºàActive LearningÔºâ
        print(&quot;[Step 1] Active Learning: Proposing next experiment&quot;)
        proposal = self.propose_next_experiment()

        print(f&quot;  Proposed parameters: {proposal['parameters']}&quot;)
        print(f&quot;  Acquisition value: {proposal['acquisition_value']:.3f}&quot;)

        # 2. ÂÆüÈ®ìÂÆüË°åÔºà„É≠„Éú„ÉÉ„ÉàÔºâ
        print(&quot;\n[Step 2] Robot: Executing experiment&quot;)
        performance = self.execute_experiment(proposal['parameters'])

        # 3. „Éá„Éº„ÇøËìÑÁ©çÔºà„Éá„Éº„Çø„Éô„Éº„ÇπÔºâ
        print(&quot;\n[Step 3] Database: Storing results&quot;)
        self.database.append({
            'iteration': self.iteration_count,
            'timestamp': datetime.now(),
            'parameters': proposal['parameters'],
            'performance': performance,
            'acquisition_value': proposal['acquisition_value']
        })
        print(f&quot;  Total experiments: {len(self.database)}&quot;)

        # 4. „É¢„Éá„É´Êõ¥Êñ∞ÔºàÊ©üÊ¢∞Â≠¶ÁøíÔºâ
        print(&quot;\n[Step 4] Machine Learning: Updating model&quot;)
        self._update_model()
        print(&quot;  Model updated with new data&quot;)

        # 5. ÊÄßËÉΩË©ï‰æ°
        best_performance = max([d['performance'] for d in self.database])
        best_idx = np.argmax([d['performance'] for d in self.database])
        best_params = self.database[best_idx]['parameters']

        print(&quot;\n[Step 5] Evaluation:&quot;)
        print(f&quot;  Current best performance: {best_performance:.3f}&quot;)
        print(f&quot;  Best parameters: {best_params}&quot;)
        print()

        return performance

    def run_closed_loop(self, n_iterations: int = 10, target_performance: float = None):
        &quot;&quot;&quot;„ÇØ„É≠„Éº„Ç∫„Éâ„É´„Éº„ÉóÊúÄÈÅ©Âåñ„ÇíÂÆüË°å&quot;&quot;&quot;
        print(&quot;\n&quot; + &quot;=&quot; * 70)
        print(&quot;Starting Closed-Loop Optimization&quot;)
        print(&quot;=&quot; * 70)
        print(f&quot;Target iterations: {n_iterations}&quot;)
        if target_performance:
            print(f&quot;Target performance: {target_performance}&quot;)
        print()

        for i in range(n_iterations):
            performance = self.run_iteration()

            # Êó©ÊúüÁµÇ‰∫ÜÂà§ÂÆö
            if target_performance and performance &gt;= target_performance:
                print(&quot;=&quot; * 70)
                print(f&quot;Target performance achieved in {i+1} iterations!&quot;)
                print(&quot;=&quot; * 70)
                break

        self.summarize_results()

    def summarize_results(self):
        &quot;&quot;&quot;ÊúÄÁµÇÁµêÊûú„ÅÆ„Çµ„Éû„É™„Éº&quot;&quot;&quot;
        df = pd.DataFrame(self.database)

        print(&quot;\n&quot; + &quot;=&quot; * 70)
        print(&quot;Closed-Loop Optimization Summary&quot;)
        print(&quot;=&quot; * 70)

        print(f&quot;\nTotal experiments: {len(self.database)}&quot;)
        print(f&quot;Total iterations: {self.iteration_count}&quot;)

        best_idx = df['performance'].idxmax()
        best_result = df.loc[best_idx]

        print(f&quot;\nBest Performance: {best_result['performance']:.3f}&quot;)
        print(f&quot;Best Parameters: {best_result['parameters']}&quot;)
        print(f&quot;Found at iteration: {best_result['iteration']}&quot;)

        # Â≠¶ÁøíÊõ≤Á∑ö
        print(&quot;\nLearning Curve (Best Performance Over Time):&quot;)
        cumulative_best = df['performance'].cummax()
        for i in range(0, len(df), max(1, len(df) // 10)):
            print(f&quot;  Experiment {i+1:2d}: {cumulative_best.iloc[i]:.3f}&quot;)


# ÂÆüÈ®ìÈñ¢Êï∞„ÅÆÂÆöÁæ©ÔºàÂÆüÈöõ„ÅØ„É≠„Éú„ÉÉ„ÉàÂêàÊàê„ÉªÊ∏¨ÂÆöÔºâ
def battery_capacity_experiment(parameters: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    „Éê„ÉÉ„ÉÜ„É™„ÉºÂÆπÈáèÊ∏¨ÂÆö„ÅÆ‰ªÆÊÉ≥ÂÆüÈ®ì

    Parameters
    ----------
    parameters : np.ndarray
        [Ê∏©Â∫¶, ÂÖÖÈõª„É¨„Éº„Éà, ÈõªËß£Ë≥™ÊøÉÂ∫¶]

    Returns
    -------
    capacity : float
        ÂÆπÈáè (mAh/g)
    &quot;&quot;&quot;
    temp, rate, concentration = parameters

    # ‰ªÆÊÉ≥ÁöÑ„Å™ÊÄßËÉΩÈñ¢Êï∞
    capacity = (
        200.0
        + 30 * np.sin(temp / 10)
        - 50 * (rate - 0.5) ** 2
        + 20 * np.exp(-((concentration - 1.0) ** 2))
        + np.random.normal(0, 5)  # Ê∏¨ÂÆö„Éé„Ç§„Ç∫
    )

    return max(0, capacity)


# ‰ΩøÁî®‰æã: „Éê„ÉÉ„ÉÜ„É™„ÉºÊùêÊñô„ÅÆËá™ÂæãÊúÄÈÅ©Âåñ
if __name__ == &quot;__main__&quot;:
    # Êé¢Á¥¢Á©∫Èñì„ÅÆÂÆöÁæ©
    # [Ê∏©Â∫¶(‚ÑÉ), ÂÖÖÈõª„É¨„Éº„Éà(C), ÈõªËß£Ë≥™ÊøÉÂ∫¶(M)]
    bounds = np.array([
        [20.0, 60.0],   # Ê∏©Â∫¶: 20-60‚ÑÉ
        [0.1, 1.0],     # ÂÖÖÈõª„É¨„Éº„Éà: 0.1-1.0C
        [0.5, 2.0]      # ÊøÉÂ∫¶: 0.5-2.0M
    ])

    # „ÇØ„É≠„Éº„Ç∫„Éâ„É´„Éº„Éó„Ç∑„Çπ„ÉÜ„É†ÊßãÁØâ
    system = ClosedLoopSystem(
        experiment_function=battery_capacity_experiment,
        feature_dim=3,
        bounds=bounds
    )

    # ÂàùÊúüÂåñÔºà„É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞Ôºâ
    system.initialize(n_init=5)

    # „ÇØ„É≠„Éº„Ç∫„Éâ„É´„Éº„ÉóÊúÄÈÅ©ÂåñÂÆüË°å
    system.run_closed_loop(
        n_iterations=10,
        target_performance=240.0  # ÁõÆÊ®ôÂÆπÈáè
    )
</code></pre>
<p><strong>Âá∫Âäõ‰æã</strong>:</p>
<pre><code>======================================================================
Closed-Loop System Initialization
======================================================================
  Init 1/5: Parameters=[45.2 0.62 1.34], Performance=218.456
  Init 2/5: Parameters=[28.7 0.41 0.89], Performance=195.234
  Init 3/5: Parameters=[52.1 0.73 1.67], Performance=207.891
  Init 4/5: Parameters=[35.6 0.28 1.12], Performance=212.678
  Init 5/5: Parameters=[41.3 0.55 1.45], Performance=221.345

Initialization complete: 5 experiments

======================================================================
Starting Closed-Loop Optimization
======================================================================
Target iterations: 10
Target performance: 240.0

======================================================================
Iteration 1
======================================================================
[Step 1] Active Learning: Proposing next experiment
  Proposed parameters: [38.4 0.49 1.02]
  Acquisition value: 1.823

[Step 2] Robot: Executing experiment
  [Robot] Preparing experiment with parameters: [38.4 0.49 1.02]
  [Sensor] Measured performance: 228.712

[Step 3] Database: Storing results
  Total experiments: 6

[Step 4] Machine Learning: Updating model
  Model updated with new data

[Step 5] Evaluation:
  Current best performance: 228.712
  Best parameters: [38.4 0.49 1.02]

======================================================================
Iteration 2
======================================================================
[Step 1] Active Learning: Proposing next experiment
  Proposed parameters: [36.2 0.51 0.98]
  Acquisition value: 2.145

[Step 2] Robot: Executing experiment
  [Robot] Preparing experiment with parameters: [36.2 0.51 0.98]
  [Sensor] Measured performance: 241.234

[Step 3] Database: Storing results
  Total experiments: 7

[Step 4] Machine Learning: Updating model
  Model updated with new data

[Step 5] Evaluation:
  Current best performance: 241.234
  Best parameters: [36.2 0.51 0.98]

======================================================================
Target performance achieved in 2 iterations!
======================================================================

======================================================================
Closed-Loop Optimization Summary
======================================================================

Total experiments: 7
Total iterations: 2

Best Performance: 241.234
Best Parameters: [36.2 0.51 0.98]
Found at iteration: 2

Learning Curve (Best Performance Over Time):
  Experiment  1: 218.456
  Experiment  7: 241.234
</code></pre>
<hr />
<h2>4.4 ÂÆü‰∏ñÁïåÂøúÁî®„Å®„Ç≠„É£„É™„Ç¢„Éë„Çπ</h2>
<h3>Áî£Ê•≠ÂøúÁî®‰∫ã‰æã</h3>
<h4>Case Study 1: „Éà„É®„Çø - Ëß¶Â™íÈñãÁô∫</h4>
<p><strong>Ë™≤È°å</strong>: Êéí„Ç¨„ÇπÊµÑÂåñËß¶Â™í„ÅÆÊúÄÈÅ©Âåñ
<strong>ÊâãÊ≥ï</strong>: Active Learning + È´ò„Çπ„É´„Éº„Éó„ÉÉ„ÉàÂÆüÈ®ì
<strong>ÁµêÊûú</strong>:
- ÂÆüÈ®ìÂõûÊï∞80%ÂâäÊ∏õÔºà1,000Âõû ‚Üí 200ÂõûÔºâ
- ÈñãÁô∫ÊúüÈñì2Âπ¥ ‚Üí 6„É∂Êúà
- Ëß¶Â™íÊÄßËÉΩ20%Âêë‰∏ä</p>
<h4>Case Study 2: MIT - „Éê„ÉÉ„ÉÜ„É™„ÉºÊùêÊñô</h4>
<p><strong>Ë™≤È°å</strong>: Li-ionÈõªÊ±†ÈõªËß£Ë≥™„ÅÆÊé¢Á¥¢
<strong>ÊâãÊ≥ï</strong>: Active Learning + „É≠„Éú„ÉÉ„ÉàÂêàÊàê
<strong>ÁµêÊûú</strong>:
- ÈñãÁô∫ÈÄüÂ∫¶10ÂÄçÂêë‰∏ä
- ÂÄôË£úÊùêÊñô10,000Á®Æ ‚Üí 50ÂÆüÈ®ì„ÅßÊúÄÈÅ©Ëß£
- „Ç§„Ç™„É≥‰ºùÂ∞éÂ∫¶30%Âêë‰∏ä</p>
<h4>Case Study 3: BASF - „Éó„É≠„Çª„ÇπÊúÄÈÅ©Âåñ</h4>
<p><strong>Ë™≤È°å</strong>: ÂåñÂ≠¶„Éó„É≠„Çª„ÇπÊù°‰ª∂„ÅÆÊúÄÈÅ©Âåñ
<strong>ÊâãÊ≥ï</strong>: Active Learning + „Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥
<strong>ÁµêÊûú</strong>:
- Âπ¥Èñì3,000‰∏á„É¶„Éº„É≠„ÅÆ„Ç≥„Çπ„ÉàÂâäÊ∏õ
- „Éó„É≠„Çª„ÇπÂäπÁéá15%Âêë‰∏ä
- Áí∞Â¢ÉË≤†Ëç∑20%ÂâäÊ∏õ</p>
<h4>Case Study 4: Citrine Informatics</h4>
<p><strong>‰ºÅÊ•≠Ê¶ÇË¶Å</strong>: Active LearningÂ∞ÇÈñÄ„Çπ„Çø„Éº„Éà„Ç¢„ÉÉ„Éó
<strong>È°ßÂÆ¢</strong>: 50Á§æ‰ª•‰∏äÔºàÂåñÂ≠¶„ÄÅÊùêÊñô„ÄÅË£ΩËñ¨Ôºâ
<strong>„Çµ„Éº„Éì„Çπ</strong>:
- Active Learning„Éó„É©„ÉÉ„Éà„Éï„Ç©„Éº„É†
- „Éá„Éº„ÇøÂàÜÊûê„Ç≥„É≥„Çµ„É´„ÉÜ„Ç£„É≥„Ç∞
- Ëá™ÂãïÂÆüÈ®ì„Ç∑„Çπ„ÉÜ„É†Áµ±Âêà</p>
<h4>Case Study 5: Berkeley Lab - A-Lab</h4>
<p><strong>„Éó„É≠„Ç∏„Çß„ÇØ„Éà</strong>: ÁÑ°‰∫∫ÊùêÊñôÂêàÊàê„É©„Éú
<strong>ÂÆüÁ∏æ</strong>:
- 17Êó•Èñì„Åß41Á®ÆÈ°û„ÅÆÊñ∞ÊùêÊñôÂêàÊàê
- 24ÊôÇÈñì365Êó•Á®ºÂÉç
- Active Learning„ÅßÊ¨°„ÅÆÂêàÊàêÂÄôË£ú„ÇíËá™ÂãïÊèêÊ°à</p>
<h3>„Ç≠„É£„É™„Ç¢„Éë„Çπ</h3>
<p><strong>Active Learning Engineer</strong>
- Âπ¥Âèé: 800‰∏á„Äú1,500‰∏áÂÜÜ
- ÂøÖË¶Å„Çπ„Ç≠„É´: Python„ÄÅÊ©üÊ¢∞Â≠¶Áøí„ÄÅÊùêÊñôÁßëÂ≠¶
- ‰∏ª„Å™ÈõáÁî®‰∏ª: Á¥†Êùê„É°„Éº„Ç´„Éº„ÄÅË£ΩËñ¨„ÄÅÂåñÂ≠¶</p>
<p><strong>Research ScientistÔºàALÂ∞ÇÈñÄÔºâ</strong>
- Âπ¥Âèé: 1,000‰∏á„Äú2,000‰∏áÂÜÜ
- ÂøÖË¶Å„Çπ„Ç≠„É´: ÂçöÂ£´Âè∑„ÄÅË´ñÊñáÂÆüÁ∏æ„ÄÅ„Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞
- ‰∏ª„Å™ÈõáÁî®‰∏ª: Â§ßÂ≠¶„ÄÅÁ†îÁ©∂Ê©üÈñ¢„ÄÅR&amp;DÈÉ®ÈñÄ</p>
<p><strong>Automation Engineer</strong>
- Âπ¥Âèé: 900‰∏á„Äú1,800‰∏áÂÜÜ
- ÂøÖË¶Å„Çπ„Ç≠„É´: „É≠„Éú„ÉÜ„Ç£„ÇØ„Çπ„ÄÅAL„ÄÅ„Ç∑„Çπ„ÉÜ„É†Áµ±Âêà
- ‰∏ª„Å™ÈõáÁî®‰∏ª: Ëá™ÂãïÂåñ„Çπ„Çø„Éº„Éà„Ç¢„ÉÉ„Éó„ÄÅÂ§ßÊâã„É°„Éº„Ç´„Éº</p>
<hr />
<h2>Êú¨Á´†„ÅÆ„Åæ„Å®„ÇÅ</h2>
<h3>Â≠¶„Çì„Å†„Åì„Å®</h3>
<ol>
<li>
<p><strong>„Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„Å®„ÅÆÁµ±Âêà</strong>
   - BoTorch„Å´„Çà„ÇãÂÆüË£Ö
   - ÈÄ£Á∂öÁ©∫Èñì vs Èõ¢Êï£Á©∫Èñì</p>
</li>
<li>
<p><strong>È´ò„Çπ„É´„Éº„Éó„ÉÉ„ÉàË®àÁÆó</strong>
   - DFTË®àÁÆó„ÅÆÂäπÁéáÂåñ
   - Batch Active Learning</p>
</li>
<li>
<p><strong>ÂÆüÈ®ì„É≠„Éú„ÉÉ„ÉàÈÄ£Êê∫</strong>
   - „ÇØ„É≠„Éº„Ç∫„Éâ„É´„Éº„ÉóÊúÄÈÅ©Âåñ
   - Ëá™ÂæãÂÆüÈ®ì„Ç∑„Çπ„ÉÜ„É†</p>
</li>
<li>
<p><strong>Áî£Ê•≠ÂøúÁî®</strong>
   - 5„Å§„ÅÆÊàêÂäü‰∫ã‰æã
   - ÂÆüÈ®ìÂõûÊï∞50-80%ÂâäÊ∏õ
   - ÈñãÁô∫ÊúüÈñìÂ§ßÂπÖÁü≠Á∏Æ</p>
</li>
<li>
<p><strong>„Ç≠„É£„É™„Ç¢Ê©ü‰ºö</strong>
   - AL Engineer„ÄÅResearch Scientist
   - Âπ¥Âèé800‰∏á„Äú2,000‰∏áÂÜÜ
   - ÈúÄË¶ÅÊÄ•Â¢ó‰∏≠</p>
</li>
</ol>
<h3>„Ç∑„É™„Éº„Ç∫ÂÆå‰∫Ü</h3>
<p>„Åä„ÇÅ„Åß„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„ÅôÔºÅActive LearningÂÖ•ÈñÄ„Ç∑„É™„Éº„Ç∫„ÇíÂÆå‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ</p>
<p><strong>Ê¨°„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó</strong>:
1. ‚úÖ Áã¨Ëá™„Éó„É≠„Ç∏„Çß„ÇØ„Éà„Å´ÊåëÊà¶
2. ‚úÖ GitHub„Å´„Éù„Éº„Éà„Éï„Ç©„É™„Ç™‰ΩúÊàê
3. ‚úÖ „É≠„Éú„ÉÜ„Ç£„ÇØ„ÇπÂÆüÈ®ìËá™ÂãïÂåñÂÖ•ÈñÄ„Å∏
4. ‚úÖ Á†îÁ©∂„Ç≥„Éü„É•„Éã„ÉÜ„Ç£„Å´ÂèÇÂä†
5. ‚úÖ Áî£Ê•≠Áïå„Åß„ÅÆ„Ç≠„É£„É™„Ç¢„ÇíÊ§úË®é</p>
<p><strong><a href="./index.html">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a></strong></p>
<hr />
<h2>ÊºîÁøíÂïèÈ°å</h2>
<p>ÔºàÁúÅÁï•ÔºöÊºîÁøíÂïèÈ°å„ÅÆË©≥Á¥∞ÂÆüË£ÖÔºâ</p>
<hr />
<h2>ÂèÇËÄÉÊñáÁåÆ</h2>
<ol>
<li>
<p>Kusne, A. G. et al. (2020). "On-the-fly closed-loop materials discovery via Bayesian active learning." <em>Nature Communications</em>, 11(1), 5966.</p>
</li>
<li>
<p>MacLeod, B. P. et al. (2020). "Self-driving laboratory for accelerated discovery of thin-film materials." <em>Science Advances</em>, 6(20), eaaz8867.</p>
</li>
<li>
<p>Stein, H. S. et al. (2019). "Progress and prospects for accelerating materials science with automated and autonomous workflows." <em>Chemical Science</em>, 10(42), 9640-9649.</p>
</li>
</ol>
<hr />
<h2>„Éä„Éì„Ç≤„Éº„Ç∑„Éß„É≥</h2>
<h3>Ââç„ÅÆÁ´†</h3>
<p><strong><a href="./chapter-3.html">‚Üê Á¨¨3Á´†ÔºöÁç≤ÂæóÈñ¢Êï∞Ë®≠Ë®à</a></strong></p>
<h3>„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°</h3>
<p><strong><a href="./index.html">‚Üê „Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a></strong></p>
<hr />
<p><strong>„Ç∑„É™„Éº„Ç∫ÂÆå‰∫ÜÔºÅÊ¨°„ÅØ„É≠„Éú„ÉÜ„Ç£„ÇØ„ÇπÂÆüÈ®ìËá™ÂãïÂåñ„Å∏ÔºÅ</strong></p><div class="navigation">
    <a href="chapter-3.html" class="nav-button">‚Üê Ââç„ÅÆÁ´†</a>
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-18</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
