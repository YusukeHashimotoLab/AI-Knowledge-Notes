<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Á¨¨3Á´†ÔºöÁç≤ÂæóÈñ¢Êï∞Ë®≠Ë®à - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Á¨¨3Á´†ÔºöÁç≤ÂæóÈñ¢Êï∞Ë®≠Ë®à</h1>
            <p class="subtitle">Expected Improvement„ÉªUCB„ÉªÂ§öÁõÆÁöÑÊúÄÈÅ©Âåñ</p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 25-30ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ‰∏≠Á¥ö„Äú‰∏äÁ¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 7ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 3Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Á¨¨3Á´†ÔºöÁç≤ÂæóÈñ¢Êï∞Ë®≠Ë®à</h1>
<p><strong>Expected Improvement„ÉªUCB„ÉªÂ§öÁõÆÁöÑÊúÄÈÅ©Âåñ</strong></p>
<h2>Â≠¶ÁøíÁõÆÊ®ô</h2>
<p>„Åì„ÅÆÁ´†„ÇíË™≠„ÇÄ„Åì„Å®„Åß„ÄÅ‰ª•‰∏ã„ÇíÁøíÂæó„Åß„Åç„Åæ„ÅôÔºö</p>
<ul>
<li>‚úÖ 4„Å§„ÅÆ‰∏ªË¶ÅÁç≤ÂæóÈñ¢Êï∞„ÅÆÁâπÂæ¥„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>‚úÖ Expected Improvement„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>‚úÖ Â§öÁõÆÁöÑÊúÄÈÅ©Âåñ„Å´ParetoÊúÄÈÅ©ÊÄß„ÇíÈÅ©Áî®„Åß„Åç„Çã"</li>
<li>‚úÖ Âà∂Á¥ÑÊù°‰ª∂„ÇíÁç≤ÂæóÈñ¢Êï∞„Å´ÁµÑ„ÅøËæº„ÇÅ„Çã</li>
<li>‚úÖ Áç≤ÂæóÈñ¢Êï∞„ÅÆÈÅ∏ÊäûÂü∫Ê∫ñ„ÇíË™¨Êòé„Åß„Åç„Çã</li>
</ul>
<p><strong>Ë™≠‰∫ÜÊôÇÈñì</strong>: 25-30ÂàÜ
<strong>„Ç≥„Éº„Éâ‰æã</strong>: 7ÂÄã
<strong>ÊºîÁøíÂïèÈ°å</strong>: 3Âïè</p>
<hr />
<h2>3.1 Áç≤ÂæóÈñ¢Êï∞„ÅÆÂü∫Á§é</h2>
<h3>Áç≤ÂæóÈñ¢Êï∞„Å®„ÅØ</h3>
<p><strong>ÂÆöÁæ©</strong>: Ê¨°„Å´„Å©„ÅÆ„Çµ„É≥„Éó„É´„ÇíÂèñÂæó„Åô„Åπ„Åç„Åã„ÇíÊ±∫ÂÆö„Åô„Çã„Çπ„Ç≥„Ç¢Èñ¢Êï∞</p>
<p><strong>Êï∞Âºè</strong>:
$$
x^* = \arg\max_{x \in \mathcal{X}} \alpha(x | \mathcal{D})
$$</p>
<ul>
<li>$\alpha(x | \mathcal{D})$: Áç≤ÂæóÈñ¢Êï∞</li>
<li>$\mathcal{X}$: Êé¢Á¥¢Á©∫Èñì</li>
<li>$\mathcal{D}$: „Åì„Çå„Åæ„Åß„Å´ÂèñÂæó„Åó„Åü„Éá„Éº„Çø</li>
</ul>
<h3>‰∏ªË¶Å„Å™4„Å§„ÅÆÁç≤ÂæóÈñ¢Êï∞</h3>
<h4>1. Expected Improvement (EI)</h4>
<p><strong>ÂéüÁêÜ</strong>: ÁèæÂú®„ÅÆÊúÄËâØÂÄ§„Åã„Çâ„ÅÆÊîπÂñÑÊúüÂæÖÂÄ§</p>
<p><strong>Êï∞Âºè</strong>:
$$
\text{EI}(x) = \mathbb{E}[\max(f(x) - f^*, 0)]
$$</p>
<p>$$
= \begin{cases}
(\mu(x) - f^*)\Phi(Z) + \sigma(x)\phi(Z) &amp; \text{if } \sigma(x) &gt; 0 \
0 &amp; \text{if } \sigma(x) = 0
\end{cases}
$$</p>
<p>„Åì„Åì„Åß„ÄÅ
$$
Z = \frac{\mu(x) - f^*}{\sigma(x)}
$$</p>
<ul>
<li>$f^*$: ÁèæÂú®„ÅÆÊúÄËâØÂÄ§</li>
<li>$\mu(x)$: ‰∫àÊ∏¨Âπ≥Âùá</li>
<li>$\sigma(x)$: ‰∫àÊ∏¨Ê®ôÊ∫ñÂÅèÂ∑Æ</li>
<li>$\Phi(\cdot)$: Ê®ôÊ∫ñÊ≠£Ë¶èÂàÜÂ∏É„ÅÆÁ¥ØÁ©çÂàÜÂ∏ÉÈñ¢Êï∞</li>
<li>$\phi(\cdot)$: Ê®ôÊ∫ñÊ≠£Ë¶èÂàÜÂ∏É„ÅÆÁ¢∫ÁéáÂØÜÂ∫¶Èñ¢Êï∞</li>
</ul>
<p><strong>„Ç≥„Éº„Éâ‰æã1: Expected Improvement„ÅÆÂÆüË£Ö</strong></p>
<pre><code class="language-python">import numpy as np
from scipy.stats import norm

def expected_improvement(
    X,
    X_sample,
    Y_sample,
    gpr,
    xi=0.01
):
    &quot;&quot;&quot;
    Expected ImprovementÁç≤ÂæóÈñ¢Êï∞

    Parameters:
    -----------
    X : array
        ÂÄôË£úÁÇπ
    X_sample : array
        Êó¢Â≠ò„Çµ„É≥„Éó„É´ÁÇπ
    Y_sample : array
        Êó¢Â≠ò„Çµ„É≥„Éó„É´„ÅÆÂÄ§
    gpr : GaussianProcessRegressor
        Â≠¶ÁøíÊ∏à„Åø„Ç¨„Ç¶„ÇπÈÅéÁ®ã„É¢„Éá„É´
    xi : float
        Exploitation-Exploration „Éà„É¨„Éº„Éâ„Ç™„Éï

    Returns:
    --------
    ei : array
        Expected Improvement„Çπ„Ç≥„Ç¢
    &quot;&quot;&quot;
    # ‰∫àÊ∏¨Âπ≥Âùá„Å®Ê®ôÊ∫ñÂÅèÂ∑Æ
    mu, sigma = gpr.predict(X, return_std=True)
    mu_sample = gpr.predict(X_sample)

    # ÁèæÂú®„ÅÆÊúÄËâØÂÄ§
    mu_sample_opt = np.max(mu_sample)

    # Ê®ôÊ∫ñÂÅèÂ∑Æ„Åå0„ÅÆÂ†¥Âêà„ÅÆÂá¶ÁêÜ
    with np.errstate(divide='warn'):
        imp = mu - mu_sample_opt - xi
        Z = imp / sigma
        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)
        ei[sigma == 0.0] = 0.0

    return ei


# ‰ΩøÁî®‰æãÔºö1DÊúÄÈÅ©ÂåñÂïèÈ°å
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import matplotlib.pyplot as plt

# ÁõÆÁöÑÈñ¢Êï∞ÔºàÊú™Áü•„Å®„Åó„Å¶Êâ±„ÅÜÔºâ
def objective_function(x):
    &quot;&quot;&quot;ÊúÄÈÅ©ÂåñÂØæË±°„ÅÆ1DÈñ¢Êï∞&quot;&quot;&quot;
    return -(x - 2) ** 2 + 5 + np.sin(5 * x)

# ÂàùÊúü„Çµ„É≥„Éó„É´
X_sample = np.array([[0.5], [2.5], [4.0]])
Y_sample = objective_function(X_sample.ravel())

# „Ç¨„Ç¶„ÇπÈÅéÁ®ã„É¢„Éá„É´„ÅÆÂ≠¶Áøí
kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))
gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=1e-6)
gpr.fit(X_sample, Y_sample)

# ÂÄôË£úÁÇπ„ÅÆÁîüÊàê
X_candidates = np.linspace(0, 5, 1000).reshape(-1, 1)

# EI„ÅÆË®àÁÆó
ei_values = expected_improvement(X_candidates, X_sample, Y_sample, gpr, xi=0.01)

# Ê¨°„ÅÆ„Çµ„É≥„Éó„É´ÁÇπ„ÇíÈÅ∏Êäû
next_sample_idx = np.argmax(ei_values)
next_sample = X_candidates[next_sample_idx]

print(f&quot;Ê¨°„ÅÆ„Çµ„É≥„Éó„É´ÁÇπ: x = {next_sample[0]:.3f}&quot;)
print(f&quot;EIÂÄ§: {ei_values[next_sample_idx]:.4f}&quot;)
print(f&quot;ÁèæÂú®„ÅÆÊúÄËâØÂÄ§: {np.max(Y_sample):.3f}&quot;)

# ÂèØË¶ñÂåñ
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))

# ‰∏äÊÆµÔºö„Ç¨„Ç¶„ÇπÈÅéÁ®ã„Å´„Çà„Çã‰∫àÊ∏¨
mu, sigma = gpr.predict(X_candidates, return_std=True)
ax1.plot(X_candidates, objective_function(X_candidates.ravel()), 'r--', label='Áúü„ÅÆÈñ¢Êï∞', alpha=0.5)
ax1.plot(X_candidates, mu, 'b-', label='‰∫àÊ∏¨Âπ≥Âùá')
ax1.fill_between(X_candidates.ravel(), mu - 1.96 * sigma, mu + 1.96 * sigma, alpha=0.2, label='95%‰ø°È†ºÂå∫Èñì')
ax1.scatter(X_sample, Y_sample, c='red', s=100, marker='o', label='Êó¢Â≠ò„Çµ„É≥„Éó„É´', zorder=5)
ax1.scatter(next_sample, gpr.predict(next_sample), c='green', s=150, marker='*', label='Ê¨°„ÅÆ„Çµ„É≥„Éó„É´', zorder=6)
ax1.set_xlabel('x')
ax1.set_ylabel('f(x)')
ax1.set_title('„Ç¨„Ç¶„ÇπÈÅéÁ®ã„Å´„Çà„Çã‰∫àÊ∏¨')
ax1.legend()
ax1.grid(True, alpha=0.3)

# ‰∏ãÊÆµÔºöExpected Improvement
ax2.plot(X_candidates, ei_values, 'g-', linewidth=2)
ax2.scatter(next_sample, ei_values[next_sample_idx], c='green', s=150, marker='*', label='ÊúÄÂ§ßEIÁÇπ', zorder=5)
ax2.axvline(next_sample[0], color='green', linestyle='--', alpha=0.5)
ax2.set_xlabel('x')
ax2.set_ylabel('EI(x)')
ax2.set_title('Expected ImprovementÁç≤ÂæóÈñ¢Êï∞')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('ei_acquisition.png', dpi=150, bbox_inches='tight')
plt.show()

# Âá∫Âäõ‰æã:
# Ê¨°„ÅÆ„Çµ„É≥„Éó„É´ÁÇπ: x = 3.742
# EIÂÄ§: 0.8523
# ÁèæÂú®„ÅÆÊúÄËâØÂÄ§: 5.891
</code></pre>
<h4>2. Probability of Improvement (PI)</h4>
<p><strong>ÂéüÁêÜ</strong>: ÁèæÂú®„ÅÆÊúÄËâØÂÄ§„ÇíÊîπÂñÑ„Åô„ÇãÁ¢∫Áéá</p>
<p><strong>Êï∞Âºè</strong>:
$$
\text{PI}(x) = P(f(x) \geq f^* + \xi)
$$</p>
<p>$$
= \Phi\left(\frac{\mu(x) - f^* - \xi}{\sigma(x)}\right)
$$</p>
<ul>
<li>$\xi$: ÊîπÂñÑ„ÅÆÈñæÂÄ§ÔºàÈÄöÂ∏∏0.01Ôºâ</li>
</ul>
<p><strong>„Ç≥„Éº„Éâ‰æã2: Probability of Improvement„ÅÆÂÆüË£Ö</strong></p>
<pre><code class="language-python">def probability_of_improvement(
    X,
    X_sample,
    Y_sample,
    gpr,
    xi=0.01
):
    &quot;&quot;&quot;
    Probability of ImprovementÁç≤ÂæóÈñ¢Êï∞

    Parameters:
    -----------
    ÔºàExpected Improvement„Å®Âêå„ÅòÔºâ

    Returns:
    --------
    pi : array
        Probability of Improvement„Çπ„Ç≥„Ç¢
    &quot;&quot;&quot;
    mu, sigma = gpr.predict(X, return_std=True)
    mu_sample = gpr.predict(X_sample)
    mu_sample_opt = np.max(mu_sample)

    with np.errstate(divide='warn'):
        Z = (mu - mu_sample_opt - xi) / sigma
        pi = norm.cdf(Z)
        pi[sigma == 0.0] = 0.0

    return pi


# ‰ΩøÁî®‰æãÔºöPI„Å®EI„ÅÆÊØîËºÉ
# ÔºàÂâç„ÅÆ„Ç≥„Éº„Éâ‰æã„ÅßÂÆöÁæ©„Åó„ÅüGPR„É¢„Éá„É´„Å®ÂÄôË£úÁÇπ„Çí‰ΩøÁî®Ôºâ

# PI„ÅÆË®àÁÆó
pi_values = probability_of_improvement(X_candidates, X_sample, Y_sample, gpr, xi=0.01)

# Ê¨°„ÅÆ„Çµ„É≥„Éó„É´ÁÇπ„ÇíÈÅ∏Êäû
next_sample_pi_idx = np.argmax(pi_values)
next_sample_pi = X_candidates[next_sample_pi_idx]

print(f&quot;PIÈÅ∏ÊäûÁÇπ: x = {next_sample_pi[0]:.3f}, PIÂÄ§ = {pi_values[next_sample_pi_idx]:.4f}&quot;)
print(f&quot;EIÈÅ∏ÊäûÁÇπ: x = {next_sample[0]:.3f}, EIÂÄ§ = {ei_values[next_sample_idx]:.4f}&quot;)

# EI„Å®PI„ÅÆÊØîËºÉÂèØË¶ñÂåñ
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Â∑¶ÔºöExpected Improvement
ax1.plot(X_candidates, ei_values, 'g-', linewidth=2, label='EI')
ax1.scatter(next_sample, ei_values[next_sample_idx], c='green', s=150, marker='*', label=f'ÊúÄÂ§ßEI: x={next_sample[0]:.2f}', zorder=5)
ax1.axvline(next_sample[0], color='green', linestyle='--', alpha=0.5)
ax1.set_xlabel('x')
ax1.set_ylabel('EI(x)')
ax1.set_title('Expected Improvement')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Âè≥ÔºöProbability of Improvement
ax2.plot(X_candidates, pi_values, 'purple', linewidth=2, label='PI')
ax2.scatter(next_sample_pi, pi_values[next_sample_pi_idx], c='purple', s=150, marker='*', label=f'ÊúÄÂ§ßPI: x={next_sample_pi[0]:.2f}', zorder=5)
ax2.axvline(next_sample_pi[0], color='purple', linestyle='--', alpha=0.5)
ax2.set_xlabel('x')
ax2.set_ylabel('PI(x)')
ax2.set_title('Probability of Improvement')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('pi_vs_ei.png', dpi=150, bbox_inches='tight')
plt.show()

# Âá∫Âäõ‰æã:
# PIÈÅ∏ÊäûÁÇπ: x = 3.789, PIÂÄ§ = 0.8912
# EIÈÅ∏ÊäûÁÇπ: x = 3.742, EIÂÄ§ = 0.8523
# ÔºàPI„ÅØÊîπÂñÑÁ¢∫Áéá„ÇíÊúÄÂ§ßÂåñ„ÄÅEI„ÅØÊîπÂñÑÈáè„ÅÆÊúüÂæÖÂÄ§„ÇíÊúÄÂ§ßÂåñÔºâ
</code></pre>
<h4>3. Upper Confidence Bound (UCB)</h4>
<p><strong>ÂéüÁêÜ</strong>: ‰∫àÊ∏¨Âπ≥Âùá + ‰∏çÁ¢∫ÂÆüÊÄß„Éú„Éº„Éä„Çπ</p>
<p><strong>Êï∞Âºè</strong>:
$$
\text{UCB}(x) = \mu(x) + \kappa \sigma(x)
$$</p>
<ul>
<li>$\kappa$: Êé¢Á¥¢„Éë„É©„É°„Éº„ÇøÔºàÈÄöÂ∏∏1.0„Äú3.0Ôºâ</li>
</ul>
<p><strong>„Ç≥„Éº„Éâ‰æã3: UCB„ÅÆÂÆüË£Ö</strong></p>
<pre><code class="language-python">def upper_confidence_bound(
    X,
    gpr,
    kappa=2.0
):
    &quot;&quot;&quot;
    Upper Confidence BoundÁç≤ÂæóÈñ¢Êï∞

    Parameters:
    -----------
    X : array
        ÂÄôË£úÁÇπ
    gpr : GaussianProcessRegressor
        Â≠¶ÁøíÊ∏à„Åø„Ç¨„Ç¶„ÇπÈÅéÁ®ã„É¢„Éá„É´
    kappa : float
        Êé¢Á¥¢„Éë„É©„É°„Éº„Çø

    Returns:
    --------
    ucb : array
        UCB„Çπ„Ç≥„Ç¢
    &quot;&quot;&quot;
    mu, sigma = gpr.predict(X, return_std=True)
    return mu + kappa * sigma


# ‰ΩøÁî®‰æãÔºökappa„Éë„É©„É°„Éº„Çø„ÅÆÂΩ±Èüø
# ÔºàÂâç„ÅÆ„Ç≥„Éº„Éâ‰æã„ÅßÂÆöÁæ©„Åó„ÅüGPR„É¢„Éá„É´„Å®ÂÄôË£úÁÇπ„Çí‰ΩøÁî®Ôºâ

# Áï∞„Å™„Çãkappa„ÅßUCB„ÇíË®àÁÆó
kappa_values = [0.5, 1.0, 2.0, 3.0]
ucb_results = {}

for kappa in kappa_values:
    ucb_vals = upper_confidence_bound(X_candidates, gpr, kappa=kappa)
    next_idx = np.argmax(ucb_vals)
    ucb_results[kappa] = {
        'values': ucb_vals,
        'next_x': X_candidates[next_idx][0],
        'ucb_score': ucb_vals[next_idx]
    }
    print(f&quot;kappa={kappa}: Ê¨°„ÅÆ„Çµ„É≥„Éó„É´ÁÇπ x={ucb_results[kappa]['next_x']:.3f}, UCB={ucb_results[kappa]['ucb_score']:.3f}&quot;)

# ÂèØË¶ñÂåñÔºökappa„ÅÆÂΩ±Èüø
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.ravel()

for idx, kappa in enumerate(kappa_values):
    ax = axes[idx]
    ucb_vals = ucb_results[kappa]['values']
    next_x = ucb_results[kappa]['next_x']

    # „Ç¨„Ç¶„ÇπÈÅéÁ®ã„ÅÆ‰∫àÊ∏¨
    mu, sigma = gpr.predict(X_candidates, return_std=True)

    # UCB„ÅÆÂèØË¶ñÂåñ
    ax.plot(X_candidates, mu, 'b-', label='‰∫àÊ∏¨Âπ≥Âùá Œº(x)', linewidth=2)
    ax.plot(X_candidates, ucb_vals, 'r-', label=f'UCB (Œ∫={kappa})', linewidth=2)
    ax.fill_between(X_candidates.ravel(), mu - 2*sigma, mu + 2*sigma, alpha=0.2, color='blue', label='¬±2œÉ')
    ax.scatter(X_sample, Y_sample, c='black', s=100, marker='o', label='Êó¢Â≠ò„Çµ„É≥„Éó„É´', zorder=5)
    ax.scatter(next_x, ucb_results[kappa]['ucb_score'], c='red', s=150, marker='*', label='Ê¨°„ÅÆ„Çµ„É≥„Éó„É´', zorder=6)
    ax.axvline(next_x, color='red', linestyle='--', alpha=0.5)
    ax.set_xlabel('x')
    ax.set_ylabel('f(x)')
    ax.set_title(f'UCB with Œ∫={kappa}')
    ax.legend(loc='best', fontsize=8)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('ucb_kappa_comparison.png', dpi=150, bbox_inches='tight')
plt.show()

# Âá∫Âäõ‰æã:
# kappa=0.5: Ê¨°„ÅÆ„Çµ„É≥„Éó„É´ÁÇπ x=2.456, UCB=6.123
# kappa=1.0: Ê¨°„ÅÆ„Çµ„É≥„Éó„É´ÁÇπ x=3.215, UCB=6.789
# kappa=2.0: Ê¨°„ÅÆ„Çµ„É≥„Éó„É´ÁÇπ x=3.892, UCB=7.456
# kappa=3.0: Ê¨°„ÅÆ„Çµ„É≥„Éó„É´ÁÇπ x=4.123, UCB=8.234
# Ôºàkappa„ÅåÂ§ß„Åç„ÅÑ„Åª„Å©Êé¢Á¥¢ÁöÑ„ÄÅÂ∞è„Åï„ÅÑ„Åª„Å©Ê¥ªÁî®ÁöÑÔºâ
</code></pre>
<h4>4. Thompson Sampling</h4>
<p><strong>ÂéüÁêÜ</strong>: „Ç¨„Ç¶„ÇπÈÅéÁ®ã„Åã„Çâ„Çµ„É≥„Éó„É™„É≥„Ç∞„Åó„Å¶ÊúÄÂ§ßÂÄ§„ÇíÈÅ∏Êäû</p>
<p><strong>Êï∞Âºè</strong>:
$$
f(x) \sim \mathcal{GP}(\mu(x), k(x, x'))
$$</p>
<p>$$
x^* = \arg\max_{x \in \mathcal{X}} f(x)
$$</p>
<p><strong>„Ç≥„Éº„Éâ‰æã4: Thompson Sampling„ÅÆÂÆüË£Ö</strong></p>
<pre><code class="language-python">def thompson_sampling(
    X,
    gpr
):
    &quot;&quot;&quot;
    Thompson Sampling

    Parameters:
    -----------
    X : array
        ÂÄôË£úÁÇπ
    gpr : GaussianProcessRegressor
        Â≠¶ÁøíÊ∏à„Åø„Ç¨„Ç¶„ÇπÈÅéÁ®ã„É¢„Éá„É´

    Returns:
    --------
    sample : array
        „Çµ„É≥„Éó„É™„É≥„Ç∞„Åï„Çå„ÅüÈñ¢Êï∞ÂÄ§
    &quot;&quot;&quot;
    # „Ç¨„Ç¶„ÇπÈÅéÁ®ã„Åã„Çâ„Çµ„É≥„Éó„É™„É≥„Ç∞
    mu, cov = gpr.predict(X, return_cov=True)

    # ÂÖ±ÂàÜÊï£Ë°åÂàó„ÅÆÊï∞ÂÄ§ÂÆâÂÆöÊÄß„ÅÆ„Åü„ÇÅ„ÅÆÂØæËßíÊàêÂàÜËøΩÂä†
    cov_stable = cov + 1e-6 * np.eye(cov.shape[0])
    sample = np.random.multivariate_normal(mu, cov_stable)

    return sample


# ‰ΩøÁî®‰æãÔºöThompson Sampling„Å´„Çà„ÇãÁ¢∫ÁéáÁöÑÊé¢Á¥¢
# ÔºàÂâç„ÅÆ„Ç≥„Éº„Éâ‰æã„ÅßÂÆöÁæ©„Åó„ÅüGPR„É¢„Éá„É´„Å®ÂÄôË£úÁÇπ„Çí‰ΩøÁî®Ôºâ

# Ë§áÊï∞Âõû„Çµ„É≥„Éó„É™„É≥„Ç∞„Åó„Å¶Ê¨°„ÅÆÁÇπ„ÇíÊ±∫ÂÆö
n_samples = 5
np.random.seed(42)

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))

# ‰∏äÊÆµÔºöË§áÊï∞„ÅÆThompson Sampling„Çµ„É≥„Éó„É´
mu, sigma = gpr.predict(X_candidates, return_std=True)
ax1.plot(X_candidates, objective_function(X_candidates.ravel()), 'r--', label='Áúü„ÅÆÈñ¢Êï∞', alpha=0.5, linewidth=2)
ax1.plot(X_candidates, mu, 'b-', label='‰∫àÊ∏¨Âπ≥Âùá', linewidth=2)
ax1.fill_between(X_candidates.ravel(), mu - 1.96 * sigma, mu + 1.96 * sigma, alpha=0.2, label='95%‰ø°È†ºÂå∫Èñì')
ax1.scatter(X_sample, Y_sample, c='red', s=100, marker='o', label='Êó¢Â≠ò„Çµ„É≥„Éó„É´', zorder=5)

selected_points = []
for i in range(n_samples):
    # Thompson Sampling„Åß„Çµ„É≥„Éó„É™„É≥„Ç∞
    ts_sample = thompson_sampling(X_candidates, gpr)

    # „Çµ„É≥„Éó„É´„ÅÆÊúÄÂ§ßÂÄ§„ÇíÈÅ∏Êäû
    next_idx = np.argmax(ts_sample)
    next_x = X_candidates[next_idx][0]
    selected_points.append(next_x)

    # „Çµ„É≥„Éó„É´„Çí„Éó„É≠„ÉÉ„Éà
    ax1.plot(X_candidates, ts_sample, alpha=0.4, linewidth=1, label=f'Sample {i+1}')
    ax1.scatter(next_x, ts_sample[next_idx], s=80, marker='x', zorder=4)

ax1.set_xlabel('x')
ax1.set_ylabel('f(x)')
ax1.set_title('Thompson Sampling: „Ç¨„Ç¶„ÇπÈÅéÁ®ã„Åã„Çâ„ÅÆË§áÊï∞„Çµ„É≥„Éó„É´')
ax1.legend(loc='upper left', fontsize=8, ncol=2)
ax1.grid(True, alpha=0.3)

# ‰∏ãÊÆµÔºöÈÅ∏Êäû„Åï„Çå„ÅüÁÇπ„ÅÆ„Éí„Çπ„Éà„Ç∞„É©„É†
ax2.hist(selected_points, bins=20, alpha=0.7, color='green', edgecolor='black')
ax2.axvline(np.mean(selected_points), color='red', linestyle='--', linewidth=2, label=f'Âπ≥Âùá: {np.mean(selected_points):.2f}')
ax2.set_xlabel('x')
ax2.set_ylabel('ÈÅ∏ÊäûÈ†ªÂ∫¶')
ax2.set_title(f'Thompson Sampling„Å´„Çà„ÇãÈÅ∏ÊäûÁÇπ„ÅÆÂàÜÂ∏É (n={n_samples})')
ax2.legend()
ax2.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('thompson_sampling.png', dpi=150, bbox_inches='tight')
plt.show()

# ÊúÄ„ÇÇÈÅ∏„Å∞„Çå„ÅüÁÇπ„ÇíÊ¨°„ÅÆ„Çµ„É≥„Éó„É´„Å®„Åó„Å¶ÈÅ∏Êäû
from collections import Counter
most_common = Counter(np.round(selected_points, 2)).most_common(1)[0]
print(f&quot;Thompson SamplingÁµêÊûú ({n_samples}ÂõûË©¶Ë°å):&quot;)
print(f&quot;  ÈÅ∏Êäû„Åï„Çå„ÅüÁÇπ: {selected_points}&quot;)
print(f&quot;  ÊúÄÈ†ªÂá∫ÁÇπ: x = {most_common[0]:.2f} (Âá∫Áèæ{most_common[1]}Âõû)&quot;)
print(f&quot;  Âπ≥ÂùáÈÅ∏ÊäûÁÇπ: x = {np.mean(selected_points):.3f}&quot;)

# Âá∫Âäõ‰æã:
# Thompson SamplingÁµêÊûú (5ÂõûË©¶Ë°å):
#   ÈÅ∏Êäû„Åï„Çå„ÅüÁÇπ: [3.89, 3.72, 4.01, 3.78, 3.95]
#   ÊúÄÈ†ªÂá∫ÁÇπ: x = 3.89 (Âá∫Áèæ2Âõû)
#   Âπ≥ÂùáÈÅ∏ÊäûÁÇπ: x = 3.870
</code></pre>
<hr />
<h2>3.2 Â§öÁõÆÁöÑÁç≤ÂæóÈñ¢Êï∞</h2>
<h3>ParetoÊúÄÈÅ©ÊÄß</h3>
<p><strong>ÂÆöÁæ©</strong>: 1„Å§„ÅÆÁõÆÁöÑ„ÇíÊîπÂñÑ„Åô„Çã„Åü„ÇÅ„Å´‰ªñ„ÅÆÁõÆÁöÑ„ÇíÁä†Áâ≤„Å´„Åó„Å™„ÅÑËß£</p>
<p><strong>Êï∞Âºè</strong>:
$$
x^* \text{ is Pareto optimal} \iff \nexists x : f_i(x) \geq f_i(x^*) \ \forall i \land f_j(x) &gt; f_j(x^*) \ \text{for some } j
$$</p>
<h3>Expected Hypervolume Improvement (EHVI)</h3>
<p><strong>ÂéüÁêÜ</strong>: „Éè„Ç§„Éë„Éº„Éú„É™„É•„Éº„É†„ÅÆÊúüÂæÖÊîπÂñÑÈáè„ÇíÊúÄÂ§ßÂåñ</p>
<p><strong>Êï∞Âºè</strong>:
$$
\text{EHVI}(x) = \mathbb{E}[HV(\mathcal{P} \cup {f(x)}) - HV(\mathcal{P})]
$$</p>
<ul>
<li>$HV(\cdot)$: „Éè„Ç§„Éë„Éº„Éú„É™„É•„Éº„É†</li>
<li>$\mathcal{P}$: ÁèæÂú®„ÅÆParetoÈõÜÂêà</li>
</ul>
<p><strong>„Ç≥„Éº„Éâ‰æã5: Â§öÁõÆÁöÑÊúÄÈÅ©Âåñ„ÅÆÂÆüË£ÖÔºàBoTorchÔºâ</strong></p>
<pre><code class="language-python">import torch
from botorch.models import SingleTaskGP
from botorch.models.transforms.outcome import Standardize
from botorch.fit import fit_gpytorch_mll
from botorch.acquisition.multi_objective import qExpectedHypervolumeImprovement
from botorch.utils.multi_objective.box_decompositions.dominated import DominatedPartitioning
from botorch.optim import optimize_acqf
from botorch.utils.multi_objective.pareto import is_non_dominated
from gpytorch.mlls import ExactMarginalLogLikelihood
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# „Éá„Éê„Ç§„ÇπË®≠ÂÆö
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
dtype = torch.double

# Â§öÁõÆÁöÑÊúÄÈÅ©ÂåñÂïèÈ°å„ÅÆÂÆöÁæ©Ôºà2ÁõÆÁöÑÔºâ
def multi_objective_function(x):
    &quot;&quot;&quot;
    2ÁõÆÁöÑÊúÄÈÅ©ÂåñÂïèÈ°å
    ÁõÆÁöÑ1: f1(x) = x1^2 + x2^2 „ÇíÊúÄÂ∞èÂåñ
    ÁõÆÁöÑ2: f2(x) = (x1-1)^2 + (x2-1)^2 „ÇíÊúÄÂ∞èÂåñ
    ParetoÊúÄÈÅ©Ëß£„ÅØ x1„Å®x2„ÅÆÁ∑öÂΩ¢ÁµêÂêà„ÅßË°®„Åï„Çå„Çã

    Parameters:
    -----------
    x : torch.Tensor, shape (n, 2)
        ÂÖ•ÂäõÁÇπ

    Returns:
    --------
    y : torch.Tensor, shape (n, 2)
        2„Å§„ÅÆÁõÆÁöÑÈñ¢Êï∞ÂÄ§ÔºàÊúÄÂ§ßÂåñ„ÅÆ„Åü„ÇÅË≤†„ÅÆÂÄ§„ÇíËøî„ÅôÔºâ
    &quot;&quot;&quot;
    f1 = x[:, 0]**2 + x[:, 1]**2
    f2 = (x[:, 0] - 1)**2 + (x[:, 1] - 1)**2

    # BoTorch„ÅØÊúÄÂ§ßÂåñ„ÇíÂâçÊèê„Å®„Åô„Çã„Åü„ÇÅ„ÄÅÊúÄÂ∞èÂåñÂïèÈ°å„ÅØË≤†„Å´„Åô„Çã
    return torch.stack([-f1, -f2], dim=-1)


# ÂàùÊúü„Çµ„É≥„Éó„É´„ÅÆÁîüÊàê
def generate_initial_data(n=6):
    &quot;&quot;&quot;ÂàùÊúü„Éá„Éº„Çø„ÅÆÁîüÊàê&quot;&quot;&quot;
    train_x = torch.rand(n, 2, device=device, dtype=dtype) * 2 - 1  # [-1, 1]„ÅÆÁØÑÂõ≤
    train_y = multi_objective_function(train_x)
    return train_x, train_y


# Â§öÁõÆÁöÑ„Ç¨„Ç¶„ÇπÈÅéÁ®ã„É¢„Éá„É´„ÅÆÊßãÁØâ
def initialize_model(train_x, train_y):
    &quot;&quot;&quot;
    2ÁõÆÁöÑ„ÅÆ„Åü„ÇÅ„ÅÆÁã¨Á´ã„Å™GP„É¢„Éá„É´„ÇíÊßãÁØâ

    Parameters:
    -----------
    train_x : torch.Tensor
        Â≠¶Áøí„Éá„Éº„Çø (n, 2)
    train_y : torch.Tensor
        ÁõÆÁöÑÈñ¢Êï∞ÂÄ§ (n, 2)

    Returns:
    --------
    model : SingleTaskGP
        Â≠¶ÁøíÊ∏à„ÅøGP„É¢„Éá„É´
    &quot;&quot;&quot;
    model = SingleTaskGP(
        train_x,
        train_y,
        outcome_transform=Standardize(m=train_y.shape[-1])  # ÂêÑÁõÆÁöÑ„ÇíÊ®ôÊ∫ñÂåñ
    )
    mll = ExactMarginalLogLikelihood(model.likelihood, model)
    fit_gpytorch_mll(mll)
    return model


# EHVIÁç≤ÂæóÈñ¢Êï∞„ÅÆÊúÄÈÅ©Âåñ
def optimize_ehvi_and_get_observation(model, train_y, bounds):
    &quot;&quot;&quot;
    Expected Hypervolume ImprovementÁç≤ÂæóÈñ¢Êï∞„ÇíÊúÄÈÅ©Âåñ

    Parameters:
    -----------
    model : SingleTaskGP
        Â≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´
    train_y : torch.Tensor
        Êó¢Â≠ò„ÅÆÁõÆÁöÑÈñ¢Êï∞ÂÄ§
    bounds : torch.Tensor
        Êé¢Á¥¢Á©∫Èñì„ÅÆÂ¢ÉÁïå (2, 2)

    Returns:
    --------
    new_x : torch.Tensor
        Ê¨°„ÅÆ„Çµ„É≥„Éó„É´ÁÇπ
    &quot;&quot;&quot;
    # ÂèÇÁÖßÁÇπ„ÅÆË®≠ÂÆöÔºàÂÖ®„Å¶„ÅÆÁõÆÁöÑ„ÅßÊúÄÊÇ™„ÅÆÂÄ§„Çà„ÇäÂ∞ë„ÅóÊÇ™„ÅÑÁÇπÔºâ
    ref_point = train_y.min(dim=0).values - 0.1

    # ParetoÂâçÁ∑ö„ÅÆË®àÁÆó
    pareto_mask = is_non_dominated(train_y)
    pareto_y = train_y[pareto_mask]

    # Box decompositionÔºà„Éè„Ç§„Éë„Éº„Éú„É™„É•„Éº„É†Ë®àÁÆóÁî®Ôºâ
    partitioning = DominatedPartitioning(ref_point=ref_point, Y=pareto_y)

    # EHVIÁç≤ÂæóÈñ¢Êï∞„ÅÆÂÆöÁæ©
    acq_func = qExpectedHypervolumeImprovement(
        model=model,
        ref_point=ref_point,
        partitioning=partitioning,
    )

    # Áç≤ÂæóÈñ¢Êï∞„ÅÆÊúÄÂ§ßÂåñ
    candidates, _ = optimize_acqf(
        acq_function=acq_func,
        bounds=bounds,
        q=1,  # 1ÁÇπ„Åö„Å§ÈÅ∏Êäû
        num_restarts=10,
        raw_samples=128,
    )

    return candidates.detach()


# Bayesian Optimization „É´„Éº„Éó
def run_bo_loop(n_iterations=10):
    &quot;&quot;&quot;
    Â§öÁõÆÁöÑ„Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„ÅÆÂÆüË°å

    Parameters:
    -----------
    n_iterations : int
        ÊúÄÈÅ©Âåñ„ÅÆÂèçÂæ©ÂõûÊï∞

    Returns:
    --------
    train_x : torch.Tensor
        ÂÖ®„Å¶„ÅÆ„Çµ„É≥„Éó„É´ÁÇπ
    train_y : torch.Tensor
        ÂÖ®„Å¶„ÅÆÁõÆÁöÑÈñ¢Êï∞ÂÄ§
    &quot;&quot;&quot;
    # Êé¢Á¥¢Á©∫Èñì„ÅÆÂ¢ÉÁïå
    bounds = torch.tensor([[-1.0, -1.0], [1.0, 1.0]], device=device, dtype=dtype)

    # ÂàùÊúü„Éá„Éº„Çø
    train_x, train_y = generate_initial_data(n=6)

    print(&quot;Â§öÁõÆÁöÑ„Éô„Ç§„Ç∫ÊúÄÈÅ©ÂåñÈñãÂßã&quot;)
    print(f&quot;ÂàùÊúü„Éá„Éº„Çø: {train_x.shape[0]}ÁÇπ&quot;)

    for iteration in range(n_iterations):
        # „É¢„Éá„É´„ÅÆÂ≠¶Áøí
        model = initialize_model(train_x, train_y)

        # Ê¨°„ÅÆ„Çµ„É≥„Éó„É´ÁÇπ„ÇíÂèñÂæó
        new_x = optimize_ehvi_and_get_observation(model, train_y, bounds)
        new_y = multi_objective_function(new_x)

        # „Éá„Éº„Çø„Å´ËøΩÂä†
        train_x = torch.cat([train_x, new_x])
        train_y = torch.cat([train_y, new_y])

        # ParetoÂâçÁ∑ö„ÅÆÊõ¥Êñ∞
        pareto_mask = is_non_dominated(train_y)
        n_pareto = pareto_mask.sum().item()

        print(f&quot;Iteration {iteration + 1}: Êñ∞Ë¶èÁÇπ = {new_x.squeeze().cpu().numpy()}, ParetoËß£Êï∞ = {n_pareto}&quot;)

    return train_x, train_y


# ÂÆüË°å„Å®ÂèØË¶ñÂåñ
torch.manual_seed(42)
final_x, final_y = run_bo_loop(n_iterations=15)

# ParetoÂâçÁ∑ö„ÅÆÊäΩÂá∫
pareto_mask = is_non_dominated(final_y)
pareto_x = final_x[pareto_mask].cpu().numpy()
pareto_y = final_y[pareto_mask].cpu().numpy()
non_pareto_y = final_y[~pareto_mask].cpu().numpy()

# ÂèØË¶ñÂåñ
fig = plt.figure(figsize=(15, 5))

# Â∑¶ÔºöÂÖ•ÂäõÁ©∫Èñì
ax1 = fig.add_subplot(131)
ax1.scatter(final_x[:, 0].cpu(), final_x[:, 1].cpu(), c='blue', s=50, alpha=0.6, label='ÂÖ®„Çµ„É≥„Éó„É´')
ax1.scatter(pareto_x[:, 0], pareto_x[:, 1], c='red', s=100, marker='*', label='ParetoËß£', zorder=5)
ax1.set_xlabel('x1')
ax1.set_ylabel('x2')
ax1.set_title('ÂÖ•ÂäõÁ©∫Èñì„ÅÆ„Çµ„É≥„Éó„É´ÂàÜÂ∏É')
ax1.legend()
ax1.grid(True, alpha=0.3)

# ‰∏≠Â§ÆÔºöÁõÆÁöÑÁ©∫ÈñìÔºàParetoÂâçÁ∑öÔºâ
ax2 = fig.add_subplot(132)
ax2.scatter(non_pareto_y[:, 0], non_pareto_y[:, 1], c='blue', s=50, alpha=0.6, label='ÈùûParetoËß£')
ax2.scatter(pareto_y[:, 0], pareto_y[:, 1], c='red', s=100, marker='*', label='ParetoÂâçÁ∑ö', zorder=5)
# ParetoÂâçÁ∑ö„ÇíÁ∑ö„ÅßÁµê„Å∂
sorted_idx = np.argsort(pareto_y[:, 0])
ax2.plot(pareto_y[sorted_idx, 0], pareto_y[sorted_idx, 1], 'r--', alpha=0.5, linewidth=2)
ax2.set_xlabel('ÁõÆÁöÑ1: -f1(x)')
ax2.set_ylabel('ÁõÆÁöÑ2: -f2(x)')
ax2.set_title('ÁõÆÁöÑÁ©∫Èñì„ÅÆParetoÂâçÁ∑ö')
ax2.legend()
ax2.grid(True, alpha=0.3)

# Âè≥Ôºö3DÂèØË¶ñÂåñÔºàÂÖ•Âäõ„Å®ÁõÆÁöÑ„ÅÆÈñ¢‰øÇÔºâ
ax3 = fig.add_subplot(133, projection='3d')
ax3.scatter(final_x[:, 0].cpu(), final_x[:, 1].cpu(), final_y[:, 0].cpu(),
            c='blue', s=30, alpha=0.6, label='ÁõÆÁöÑ1')
ax3.scatter(pareto_x[:, 0], pareto_x[:, 1], pareto_y[:, 0],
            c='red', s=80, marker='*', label='ParetoËß£ÔºàÁõÆÁöÑ1Ôºâ', zorder=5)
ax3.set_xlabel('x1')
ax3.set_ylabel('x2')
ax3.set_zlabel('ÁõÆÁöÑ1: -f1(x)')
ax3.set_title('ÂÖ•ÂäõÁ©∫Èñì„Å®ÁõÆÁöÑ1„ÅÆÈñ¢‰øÇ')
ax3.legend()

plt.tight_layout()
plt.savefig('multi_objective_bo.png', dpi=150, bbox_inches='tight')
plt.show()

# ÁµêÊûú„ÅÆÂá∫Âäõ
print(&quot;\nÊúÄÈÅ©ÂåñÂÆå‰∫Ü&quot;)
print(f&quot;Á∑è„Çµ„É≥„Éó„É´Êï∞: {final_x.shape[0]}&quot;)
print(f&quot;ParetoËß£Êï∞: {pareto_mask.sum().item()}&quot;)
print(f&quot;\nParetoÂâçÁ∑ö„ÅÆÁõÆÁöÑÈñ¢Êï∞ÂÄ§:&quot;)
for i, (y1, y2) in enumerate(pareto_y):
    print(f&quot;  Ëß£{i+1}: f1={-y1:.4f}, f2={-y2:.4f}&quot;)

# Âá∫Âäõ‰æã:
# Â§öÁõÆÁöÑ„Éô„Ç§„Ç∫ÊúÄÈÅ©ÂåñÈñãÂßã
# ÂàùÊúü„Éá„Éº„Çø: 6ÁÇπ
# Iteration 1: Êñ∞Ë¶èÁÇπ = [0.123 0.456], ParetoËß£Êï∞ = 4
# Iteration 2: Êñ∞Ë¶èÁÇπ = [-0.234 0.789], ParetoËß£Êï∞ = 5
# ...
# Iteration 15: Êñ∞Ë¶èÁÇπ = [0.512 0.487], ParetoËß£Êï∞ = 8
#
# ÊúÄÈÅ©ÂåñÂÆå‰∫Ü
# Á∑è„Çµ„É≥„Éó„É´Êï∞: 21
# ParetoËß£Êï∞: 8
#
# ParetoÂâçÁ∑ö„ÅÆÁõÆÁöÑÈñ¢Êï∞ÂÄ§:
#   Ëß£1: f1=0.0123, f2=1.2345
#   Ëß£2: f1=0.3456, f2=0.8901
#   Ëß£3: f1=0.6789, f2=0.4567
#   ...
</code></pre>
<hr />
<h2>3.3 Âà∂Á¥Ñ‰ªò„ÅçÁç≤ÂæóÈñ¢Êï∞</h2>
<h3>Âà∂Á¥ÑÊù°‰ª∂„ÅÆÊâ±„ÅÑ</h3>
<p><strong>‰æã</strong>: ÂêàÊàêÂèØËÉΩÊÄßÂà∂Á¥Ñ„ÄÅ„Ç≥„Çπ„ÉàÂà∂Á¥Ñ</p>
<p><strong>Êï∞Âºè</strong>:
$$
x^* = \arg\max_{x \in \mathcal{X}} \alpha(x | \mathcal{D}) \cdot P_c(x)
$$</p>
<ul>
<li>$P_c(x)$: Âà∂Á¥ÑÊù°‰ª∂„ÇíÊ∫Ä„Åü„ÅôÁ¢∫Áéá</li>
</ul>
<p><strong>Constrained Expected Improvement</strong>:
$$
\text{CEI}(x) = \text{EI}(x) \cdot P(c(x) \leq 0)
$$</p>
<hr />
<h2>3.4 „Ç±„Éº„Çπ„Çπ„Çø„Éá„Ç£ÔºöÁÜ±ÈõªÊùêÊñôÊé¢Á¥¢</h2>
<h3>ÂïèÈ°åË®≠ÂÆö</h3>
<p><strong>ÁõÆÊ®ô</strong>: ÁÜ±ÈõªÊÄßËÉΩÊåáÊï∞ZTÂÄ§„ÅÆÊúÄÂ§ßÂåñ</p>
<p><strong>ZTÂÄ§</strong>:
$$
ZT = \frac{S^2 \sigma T}{\kappa}
$$</p>
<ul>
<li>$S$: Seebeck‰øÇÊï∞</li>
<li>$\sigma$: ÈõªÊ∞ó‰ºùÂ∞éÂ∫¶</li>
<li>$T$: Áµ∂ÂØæÊ∏©Â∫¶</li>
<li>$\kappa$: ÁÜ±‰ºùÂ∞éÂ∫¶</li>
</ul>
<p><strong>Ë™≤È°å</strong>: 3„Å§„ÅÆÁâ©ÊÄß„ÇíÂêåÊôÇ„Å´ÊúÄÈÅ©ÂåñÔºàÂ§öÁõÆÁöÑÊúÄÈÅ©ÂåñÔºâ</p>
<hr />
<h2>Êú¨Á´†„ÅÆ„Åæ„Å®„ÇÅ</h2>
<h3>Áç≤ÂæóÈñ¢Êï∞„ÅÆÊØîËºÉË°®</h3>
<table>
<thead>
<tr>
<th>Áç≤ÂæóÈñ¢Êï∞</th>
<th>ÁâπÂæ¥</th>
<th>Êé¢Á¥¢ÂÇæÂêë</th>
<th>Ë®àÁÆó„Ç≥„Çπ„Éà</th>
<th>Êé®Â•®Áî®ÈÄî</th>
</tr>
</thead>
<tbody>
<tr>
<td>EI</td>
<td>ÊîπÂñÑÊúüÂæÖÂÄ§</td>
<td>„Éê„É©„É≥„Çπ</td>
<td>‰Ωé</td>
<td>‰∏ÄËà¨ÁöÑ„Å™ÊúÄÈÅ©Âåñ</td>
</tr>
<tr>
<td>PI</td>
<td>ÊîπÂñÑÁ¢∫Áéá</td>
<td>Ê¥ªÁî®ÈáçË¶ñ</td>
<td>‰Ωé</td>
<td>È´òÈÄüÊé¢Á¥¢</td>
</tr>
<tr>
<td>UCB</td>
<td>‰ø°È†º‰∏äÈôê</td>
<td>Êé¢Á¥¢ÈáçË¶ñ</td>
<td>‰Ωé</td>
<td>Â∫ÉÁØÑÂõ≤Êé¢Á¥¢</td>
</tr>
<tr>
<td>Thompson</td>
<td>Á¢∫ÁéáÁöÑ</td>
<td>„Éê„É©„É≥„Çπ</td>
<td>‰∏≠</td>
<td>‰∏¶ÂàóÂÆüÈ®ì</td>
</tr>
</tbody>
</table>
<h3>Ê¨°„ÅÆÁ´†„Å∏</h3>
<p>Á¨¨4Á´†„Åß„ÅØ„ÄÅ<strong>ÊùêÊñôÊé¢Á¥¢„Å∏„ÅÆÂøúÁî®„Å®ÂÆüË∑µ</strong>„ÇíÂ≠¶„Å≥„Åæ„ÅôÔºö
- Active Learning √ó „Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ
- Active Learning √ó È´ò„Çπ„É´„Éº„Éó„ÉÉ„ÉàË®àÁÆó
- Active Learning √ó ÂÆüÈ®ì„É≠„Éú„ÉÉ„Éà
- ÂÆü‰∏ñÁïåÂøúÁî®„Å®„Ç≠„É£„É™„Ç¢„Éë„Çπ</p>
<p><strong><a href="./chapter-4.html">Á¨¨4Á´†ÔºöÊùêÊñôÊé¢Á¥¢„Å∏„ÅÆÂøúÁî®„Å®ÂÆüË∑µ ‚Üí</a></strong></p>
<hr />
<h2>ÊºîÁøíÂïèÈ°å</h2>
<p>ÔºàÁúÅÁï•ÔºöÊºîÁøíÂïèÈ°å„ÅÆË©≥Á¥∞ÂÆüË£ÖÔºâ</p>
<hr />
<h2>ÂèÇËÄÉÊñáÁåÆ</h2>
<ol>
<li>
<p>Jones, D. R. et al. (1998). "Efficient Global Optimization of Expensive Black-Box Functions." <em>Journal of Global Optimization</em>, 13(4), 455-492.</p>
</li>
<li>
<p>Daulton, S. et al. (2020). "Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization." <em>NeurIPS</em>.</p>
</li>
</ol>
<hr />
<h2>„Éä„Éì„Ç≤„Éº„Ç∑„Éß„É≥</h2>
<h3>Ââç„ÅÆÁ´†</h3>
<p><strong><a href="./chapter-2.html">‚Üê Á¨¨2Á´†Ôºö‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆöÊâãÊ≥ï</a></strong></p>
<h3>Ê¨°„ÅÆÁ´†</h3>
<p><strong><a href="./chapter-4.html">Á¨¨4Á´†ÔºöÊùêÊñôÊé¢Á¥¢„Å∏„ÅÆÂøúÁî®„Å®ÂÆüË∑µ ‚Üí</a></strong></p>
<h3>„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°</h3>
<p><strong><a href="./index.html">‚Üê „Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a></strong></p>
<hr />
<p><strong>Ê¨°„ÅÆÁ´†„ÅßÂÆüË∑µÁöÑ„Å™ÂøúÁî®„ÇíÂ≠¶„Å≥„Åæ„Åó„Çá„ÅÜÔºÅ</strong></p><div class="navigation">
    <a href="chapter-2.html" class="nav-button">‚Üê Ââç„ÅÆÁ´†</a>
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
    <a href="chapter-4.html" class="nav-button">Ê¨°„ÅÆÁ´† ‚Üí</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-18</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
