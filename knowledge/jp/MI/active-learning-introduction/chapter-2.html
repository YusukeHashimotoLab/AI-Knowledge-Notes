<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Á¨¨2Á´†Ôºö‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆöÊâãÊ≥ï - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Á¨¨2Á´†Ôºö‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆöÊâãÊ≥ï</h1>
            <p class="subtitle">Ensemble„ÉªDropout„ÉªGaussian Process„Å´„Çà„Çã‰∫àÊ∏¨‰ø°È†ºÂå∫Èñì</p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 25-30ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ‰∏≠Á¥ö„Äú‰∏äÁ¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 8ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 3Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Á¨¨2Á´†Ôºö‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆöÊâãÊ≥ï</h1>
<p><strong>Ensemble„ÉªDropout„ÉªGaussian Process„Å´„Çà„Çã‰∫àÊ∏¨‰ø°È†ºÂå∫Èñì</strong></p>
<h2>Â≠¶ÁøíÁõÆÊ®ô</h2>
<p>„Åì„ÅÆÁ´†„ÇíË™≠„ÇÄ„Åì„Å®„Åß„ÄÅ‰ª•‰∏ã„ÇíÁøíÂæó„Åß„Åç„Åæ„ÅôÔºö</p>
<ul>
<li>‚úÖ 3„Å§„ÅÆ‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆöÊâãÊ≥ï„ÅÆÂéüÁêÜ„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>‚úÖ EnsembleÊ≥ïÔºàRandom ForestÔºâ„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>‚úÖ MC Dropout„Çí„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Å´ÈÅ©Áî®„Åß„Åç„Çã</li>
<li>‚úÖ Gaussian Process„Åß‰∫àÊ∏¨ÂàÜÊï£„ÇíË®àÁÆó„Åß„Åç„Çã</li>
<li>‚úÖ ÊâãÊ≥ï„ÅÆ‰Ωø„ÅÑÂàÜ„ÅëÂü∫Ê∫ñ„ÇíË™¨Êòé„Åß„Åç„Çã</li>
</ul>
<p><strong>Ë™≠‰∫ÜÊôÇÈñì</strong>: 25-30ÂàÜ
<strong>„Ç≥„Éº„Éâ‰æã</strong>: 8ÂÄã
<strong>ÊºîÁøíÂïèÈ°å</strong>: 3Âïè</p>
<hr />
<h2>2.1 EnsembleÊ≥ï„Å´„Çà„Çã‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö</h2>
<h3>„Å™„Åú‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö„ÅåÈáçË¶Å„Åã</h3>
<p>Active Learning„Åß„ÅØ„ÄÅ„Äå„É¢„Éá„É´„Åå„Å©„Çå„Å†„ÅëËá™‰ø°„ÇíÊåÅ„Å£„Å¶‰∫àÊ∏¨„Åó„Å¶„ÅÑ„Çã„Åã„Äç„ÇíÂÆöÈáèÂåñ„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö„ÅØ„ÄÅQuery Strategy„ÅÆÊ†∏ÂøÉÊäÄË°ì„Åß„Åô„ÄÇ</p>
<p><strong>‰∏çÁ¢∫ÂÆüÊÄß„ÅÆ2„Å§„ÅÆ„Çø„Ç§„Éó</strong>:</p>
<ol>
<li>
<p><strong>Aleatoric UncertaintyÔºàÂÅ∂ÁÑ∂ÁöÑ‰∏çÁ¢∫ÂÆüÊÄßÔºâ</strong>
   - „Éá„Éº„ÇøËá™‰Ωì„Å´ÂÜÖÂú®„Åô„Çã„Éé„Ç§„Ç∫
   - Ê∏¨ÂÆöË™§Â∑Æ„ÄÅÁí∞Â¢ÉÂ§âÂãï„Å™„Å©
   - „Éá„Éº„Çø„ÇíÂ¢ó„ÇÑ„Åó„Å¶„ÇÇÊ∏õÂ∞ë„Åó„Å™„ÅÑ</p>
</li>
<li>
<p><strong>Epistemic UncertaintyÔºàË™çË≠òÁöÑ‰∏çÁ¢∫ÂÆüÊÄßÔºâ</strong>
   - „É¢„Éá„É´„ÅÆÁü•Ë≠ò‰∏çË∂≥„Å´„Çà„Çã‰∏çÁ¢∫ÂÆüÊÄß
   - „Éá„Éº„Çø‰∏çË∂≥„ÅåÂéüÂõ†
   - „Éá„Éº„Çø„ÇíÂ¢ó„ÇÑ„Åô„Å®Ê∏õÂ∞ë</p>
</li>
</ol>
<p><strong>Active Learning„ÅåÁÑ¶ÁÇπ„ÇíÂΩì„Å¶„Çã‰∏çÁ¢∫ÂÆüÊÄß</strong>:
‚Üí <strong>Epistemic Uncertainty</strong>Ôºà„Éá„Éº„ÇøËøΩÂä†„ÅßÊîπÂñÑÂèØËÉΩÔºâ</p>
<h3>EnsembleÊ≥ï„ÅÆÂéüÁêÜ</h3>
<p><strong>Âü∫Êú¨„Ç¢„Ç§„Éá„Ç¢</strong>: Ë§áÊï∞„ÅÆ„É¢„Éá„É´„ÅÆ‰∫àÊ∏¨„ÅÆ„Å∞„Çâ„Å§„Åç„Åß‰∏çÁ¢∫ÂÆüÊÄß„ÇíÊ∏¨ÂÆö</p>
<p><strong>Êï∞Âºè</strong>:
$$
\mu(x) = \frac{1}{M} \sum_{m=1}^M f_m(x)
$$</p>
<p>$$
\sigma^2(x) = \frac{1}{M} \sum_{m=1}^M (f_m(x) - \mu(x))^2
$$</p>
<ul>
<li>$f_m(x)$: mÁï™ÁõÆ„ÅÆ„É¢„Éá„É´„ÅÆ‰∫àÊ∏¨</li>
<li>$M$: „É¢„Éá„É´Êï∞Ôºà„Ç¢„É≥„Çµ„É≥„Éñ„É´„Çµ„Ç§„Ç∫Ôºâ</li>
<li>$\mu(x)$: ‰∫àÊ∏¨Âπ≥Âùá</li>
<li>$\sigma^2(x)$: ‰∫àÊ∏¨ÂàÜÊï£Ôºà‰∏çÁ¢∫ÂÆüÊÄßÔºâ</li>
</ul>
<h3>Random Forest„Å´„Çà„ÇãÂÆüË£Ö</h3>
<p><strong>„Ç≥„Éº„Éâ‰æã1: Random Forest„Åß‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö</strong></p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression

# „Éá„Éº„ÇøÁîüÊàê
np.random.seed(42)
X, y = make_regression(
    n_samples=200,
    n_features=5,
    noise=10,
    random_state=42
)

# Ë®ìÁ∑¥„Éª„ÉÜ„Çπ„Éà„Éá„Éº„ÇøÂàÜÂâ≤
train_size = 50
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Random Forest„Åß‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö
rf = RandomForestRegressor(
    n_estimators=100,
    random_state=42
)
rf.fit(X_train, y_train)

# ÂêÑÊ±∫ÂÆöÊú®„ÅÆ‰∫àÊ∏¨„ÇíÂèñÂæó
tree_predictions = np.array([
    tree.predict(X_test)
    for tree in rf.estimators_
])

# ‰∫àÊ∏¨Âπ≥Âùá„Å®Ê®ôÊ∫ñÂÅèÂ∑Æ
mean_prediction = np.mean(tree_predictions, axis=0)
std_prediction = np.std(tree_predictions, axis=0)

# ÂèØË¶ñÂåñ
plt.figure(figsize=(12, 5))

# Â∑¶Âõ≥: ‰∫àÊ∏¨ vs ÁúüÂÄ§
plt.subplot(1, 2, 1)
plt.errorbar(
    y_test,
    mean_prediction,
    yerr=1.96 * std_prediction,  # 95%‰ø°È†ºÂå∫Èñì
    fmt='o',
    alpha=0.6,
    capsize=5
)
plt.plot(
    [y_test.min(), y_test.max()],
    [y_test.min(), y_test.max()],
    'r--',
    label='Perfect prediction'
)
plt.xlabel('True Value', fontsize=12)
plt.ylabel('Predicted Value', fontsize=12)
plt.title('Random Forest: Prediction with Uncertainty', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

# Âè≥Âõ≥: ‰∏çÁ¢∫ÂÆüÊÄß„ÅÆÂàÜÂ∏É
plt.subplot(1, 2, 2)
plt.hist(std_prediction, bins=30, edgecolor='black', alpha=0.7)
plt.xlabel('Standard Deviation (Uncertainty)', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.title('Distribution of Uncertainty', fontsize=14)
plt.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('rf_uncertainty.png', dpi=150, bbox_inches='tight')
plt.show()

# Áµ±Ë®à„Çµ„Éû„É™„Éº
print(&quot;Random Forest‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö„ÅÆÁµêÊûú:&quot;)
print(f&quot;Âπ≥Âùá‰∏çÁ¢∫ÂÆüÊÄß: {std_prediction.mean():.2f}&quot;)
print(f&quot;ÊúÄÂ∞è‰∏çÁ¢∫ÂÆüÊÄß: {std_prediction.min():.2f}&quot;)
print(f&quot;ÊúÄÂ§ß‰∏çÁ¢∫ÂÆüÊÄß: {std_prediction.max():.2f}&quot;)
print(f&quot;‰∏çÁ¢∫ÂÆüÊÄß„ÅÆÊ®ôÊ∫ñÂÅèÂ∑Æ: {std_prediction.std():.2f}&quot;)
</code></pre>
<p><strong>Âá∫Âäõ‰æã</strong>:</p>
<pre><code>Random Forest‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö„ÅÆÁµêÊûú:
Âπ≥Âùá‰∏çÁ¢∫ÂÆüÊÄß: 5.23
ÊúÄÂ∞è‰∏çÁ¢∫ÂÆüÊÄß: 2.14
ÊúÄÂ§ß‰∏çÁ¢∫ÂÆüÊÄß: 12.45
‰∏çÁ¢∫ÂÆüÊÄß„ÅÆÊ®ôÊ∫ñÂÅèÂ∑Æ: 2.18
</code></pre>
<h3>LightGBM„Å´„Çà„ÇãÂÆüË£Ö</h3>
<p><strong>„Ç≥„Éº„Éâ‰æã2: LightGBM„Åß‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö</strong></p>
<pre><code class="language-python">import lightgbm as lgb

# LightGBM„ÅßË§áÊï∞„É¢„Éá„É´„ÇíË®ìÁ∑¥ÔºàBaggingÔºâ
n_models = 100
lgb_predictions = []

for i in range(n_models):
    # „Éñ„Éº„Éà„Çπ„Éà„É©„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞
    indices = np.random.choice(
        len(X_train),
        len(X_train),
        replace=True
    )
    X_boot = X_train[indices]
    y_boot = y_train[indices]

    # LightGBMË®ìÁ∑¥
    train_data = lgb.Dataset(X_boot, label=y_boot)
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1
    }

    model = lgb.train(
        params,
        train_data,
        num_boost_round=100
    )

    # ‰∫àÊ∏¨
    pred = model.predict(X_test)
    lgb_predictions.append(pred)

lgb_predictions = np.array(lgb_predictions)

# ‰∏çÁ¢∫ÂÆüÊÄßË®àÁÆó
lgb_mean = np.mean(lgb_predictions, axis=0)
lgb_std = np.std(lgb_predictions, axis=0)

print(&quot;\nLightGBM‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö„ÅÆÁµêÊûú:&quot;)
print(f&quot;Âπ≥Âùá‰∏çÁ¢∫ÂÆüÊÄß: {lgb_std.mean():.2f}&quot;)
print(f&quot;Random Forest„Å®„ÅÆÁõ∏Èñ¢: &quot;
      f&quot;{np.corrcoef(std_prediction, lgb_std)[0,1]:.3f}&quot;)
</code></pre>
<p><strong>Âà©ÁÇπ</strong>:
- ‚úÖ ÂÆüË£Ö„ÅåÁ∞°Âçò
- ‚úÖ Ë®àÁÆó„Ç≥„Çπ„Éà„ÅåÊØîËºÉÁöÑ‰Ωé„ÅÑ
- ‚úÖ Ëß£Èáà„Åó„ÇÑ„Åô„ÅÑ
- ‚úÖ Ë°®ÂΩ¢Âºè„Éá„Éº„Çø„Å´Âº∑„ÅÑ</p>
<p><strong>Ê¨†ÁÇπ</strong>:
- ‚ö†Ô∏è „Ç¢„É≥„Çµ„É≥„Éñ„É´„Çµ„Ç§„Ç∫„Å´‰æùÂ≠ò
- ‚ö†Ô∏è Ê∑±Â±§Â≠¶Áøí„Å´„ÅØÈÅ©Áî®Âõ∞Èõ£
- ‚ö†Ô∏è ‰∏çÁ¢∫ÂÆüÊÄß„ÅÆÊ†°Ê≠£„ÅåÂøÖË¶Å„Å™Â†¥Âêà„Åå„ÅÇ„Çã</p>
<hr />
<h2>2.2 DropoutÊ≥ï„Å´„Çà„Çã‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö</h2>
<h3>MC DropoutÔºàMonte Carlo DropoutÔºâ</h3>
<p><strong>ÂéüÁêÜ</strong>: Êé®Ë´ñÊôÇ„Å´„ÇÇDropout„ÇíÈÅ©Áî®„Åó„ÄÅË§áÊï∞Âõû‰∫àÊ∏¨„Åó„Å¶„Å∞„Çâ„Å§„Åç„ÇíÊ∏¨ÂÆö</p>
<p><strong>ÈÄöÂ∏∏„ÅÆDropout</strong>ÔºàË®ìÁ∑¥ÊôÇ„ÅÆ„ÅøÔºâ:</p>
<pre><code class="language-python"># Ë®ìÁ∑¥ÊôÇ
model.train()  # DropoutÊúâÂäπ
output = model(x)

# Êé®Ë´ñÊôÇ
model.eval()  # DropoutÁÑ°Âäπ
output = model(x)  # Ê±∫ÂÆöË´ñÁöÑ‰∫àÊ∏¨
</code></pre>
<p><strong>MC Dropout</strong>ÔºàÊé®Ë´ñÊôÇ„ÇÇDropoutÔºâ:</p>
<pre><code class="language-python"># Êé®Ë´ñÊôÇ„ÇÇDropout„ÇíÊúâÂäπÂåñ
model.train()  # DropoutÊúâÂäπ„ÅÆ„Åæ„Åæ
predictions = [model(x) for _ in range(T)]  # TÂõû‰∫àÊ∏¨
mean = np.mean(predictions, axis=0)
std = np.std(predictions, axis=0)
</code></pre>
<h3>ÂÆüË£Ö‰æã</h3>
<p><strong>„Ç≥„Éº„Éâ‰æã3: PyTorch„ÅßMC Dropout</strong></p>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class MCDropoutNet(nn.Module):
    def __init__(self, input_dim, hidden_dim=50, dropout_rate=0.5):
        super(MCDropoutNet, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)
        self.dropout = nn.Dropout(p=dropout_rate)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)  # DropoutÈÅ©Áî®
        x = F.relu(self.fc2(x))
        x = self.dropout(x)  # DropoutÈÅ©Áî®
        x = self.fc3(x)
        return x

# „É¢„Éá„É´ÂàùÊúüÂåñ
model = MCDropoutNet(input_dim=5, hidden_dim=50, dropout_rate=0.3)

# „Éá„Éº„Çø„ÇíTensor„Å´Â§âÊèõ
X_train_tensor = torch.FloatTensor(X_train)
y_train_tensor = torch.FloatTensor(y_train).view(-1, 1)
X_test_tensor = torch.FloatTensor(X_test)

# Ë®ìÁ∑¥
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

model.train()
for epoch in range(200):
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 50 == 0:
        print(f'Epoch [{epoch+1}/200], Loss: {loss.item():.4f}')

# MC Dropout„Åß‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö
def mc_dropout_predict(model, x, n_samples=100):
    &quot;&quot;&quot;
    MC Dropout„Å´„Çà„Çã‰∫àÊ∏¨„Å®‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö

    Parameters:
    -----------
    model : nn.Module
        Ë®ìÁ∑¥Ê∏à„Åø„É¢„Éá„É´
    x : Tensor
        ÂÖ•Âäõ„Éá„Éº„Çø
    n_samples : int
        „Çµ„É≥„Éó„É™„É≥„Ç∞ÂõûÊï∞

    Returns:
    --------
    mean : array
        ‰∫àÊ∏¨Âπ≥Âùá
    std : array
        ‰∫àÊ∏¨Ê®ôÊ∫ñÂÅèÂ∑ÆÔºà‰∏çÁ¢∫ÂÆüÊÄßÔºâ
    &quot;&quot;&quot;
    model.train()  # Dropout„ÇíÊúâÂäπÂåñ
    predictions = []

    with torch.no_grad():
        for _ in range(n_samples):
            pred = model(x).numpy()
            predictions.append(pred)

    predictions = np.array(predictions).squeeze()
    mean = np.mean(predictions, axis=0)
    std = np.std(predictions, axis=0)

    return mean, std

# MC Dropout„Åß‰∫àÊ∏¨
mc_mean, mc_std = mc_dropout_predict(
    model,
    X_test_tensor,
    n_samples=100
)

# ÂèØË¶ñÂåñ
plt.figure(figsize=(10, 6))
plt.errorbar(
    y_test,
    mc_mean,
    yerr=1.96 * mc_std,
    fmt='o',
    alpha=0.6,
    capsize=5,
    color='purple'
)
plt.plot(
    [y_test.min(), y_test.max()],
    [y_test.min(), y_test.max()],
    'r--',
    label='Perfect prediction'
)
plt.xlabel('True Value', fontsize=12)
plt.ylabel('Predicted Value (MC Dropout)', fontsize=12)
plt.title('MC Dropout: Uncertainty Estimation', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('mc_dropout_uncertainty.png', dpi=150)
plt.show()

print(&quot;\nMC Dropout‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö„ÅÆÁµêÊûú:&quot;)
print(f&quot;Âπ≥Âùá‰∏çÁ¢∫ÂÆüÊÄß: {mc_std.mean():.2f}&quot;)
print(f&quot;ÊúÄÂ∞è‰∏çÁ¢∫ÂÆüÊÄß: {mc_std.min():.2f}&quot;)
print(f&quot;ÊúÄÂ§ß‰∏çÁ¢∫ÂÆüÊÄß: {mc_std.max():.2f}&quot;)
</code></pre>
<p><strong>Âá∫Âäõ‰æã</strong>:</p>
<pre><code>Epoch [50/200], Loss: 145.2341
Epoch [100/200], Loss: 98.5632
Epoch [150/200], Loss: 67.8921
Epoch [200/200], Loss: 52.1234

MC Dropout‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö„ÅÆÁµêÊûú:
Âπ≥Âùá‰∏çÁ¢∫ÂÆüÊÄß: 4.87
ÊúÄÂ∞è‰∏çÁ¢∫ÂÆüÊÄß: 1.92
ÊúÄÂ§ß‰∏çÁ¢∫ÂÆüÊÄß: 11.23
</code></pre>
<p><strong>Âà©ÁÇπ</strong>:
- ‚úÖ Êó¢Â≠ò„ÅÆ„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Å´ÂÆπÊòì„Å´ÈÅ©Áî®
- ‚úÖ ËøΩÂä†„ÅÆË®ìÁ∑¥‰∏çË¶ÅÔºàDropout„ÅÆ„ÅøÔºâ
- ‚úÖ Ê∑±Â±§Â≠¶Áøí„Å´ÈÅ©„Åó„Å¶„ÅÑ„Çã</p>
<p><strong>Ê¨†ÁÇπ</strong>:
- ‚ö†Ô∏è „Çµ„É≥„Éó„É™„É≥„Ç∞ÂõûÊï∞ÔºàTÔºâ„Å´Ë®àÁÆó„Ç≥„Çπ„Éà‰æùÂ≠ò
- ‚ö†Ô∏è DropoutÁéá„ÅÆÈÅ∏Êäû„ÅåÈáçË¶Å
- ‚ö†Ô∏è ‰∏çÁ¢∫ÂÆüÊÄß„ÅÆÊ†°Ê≠£„ÅåÂøÖË¶Å„Å™Â†¥Âêà„Åå„ÅÇ„Çã</p>
<hr />
<h2>2.3 Gaussian Process (GP) „Å´„Çà„Çã‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö</h2>
<h3>GP„ÅÆÂü∫Á§é</h3>
<p>Gaussian ProcessÔºà„Ç¨„Ç¶„ÇπÈÅéÁ®ãÔºâ„ÅØ„ÄÅÈñ¢Êï∞„ÅÆÁ¢∫ÁéáÂàÜÂ∏É„ÇíÂÆöÁæ©„Åô„ÇãÂº∑Âäõ„Å™ÊâãÊ≥ï„Åß„Åô„ÄÇ</p>
<p><strong>ÂÆöÁæ©</strong>:
$$
f(\mathbf{x}) \sim \mathcal{GP}(\mu(\mathbf{x}), k(\mathbf{x}, \mathbf{x}'))
$$</p>
<ul>
<li>$\mu(\mathbf{x})$: Âπ≥ÂùáÈñ¢Êï∞ÔºàÈÄöÂ∏∏0Ôºâ</li>
<li>$k(\mathbf{x}, \mathbf{x}')$: „Ç´„Éº„Éç„É´Èñ¢Êï∞ÔºàÂÖ±ÂàÜÊï£Èñ¢Êï∞Ôºâ</li>
</ul>
<p><strong>‰∫àÊ∏¨ÂàÜÂ∏É</strong>:
$$
p(f^<em> | \mathbf{X}, \mathbf{y}, \mathbf{x}^</em>) = \mathcal{N}(\mu^<em>, \sigma^{</em>2})
$$</p>
<p>$$
\mu^<em> = k(\mathbf{x}^</em>, \mathbf{X}) [K(\mathbf{X}, \mathbf{X}) + \sigma_n^2 I]^{-1} \mathbf{y}
$$</p>
<p>$$
\sigma^{<em>2} = k(\mathbf{x}^</em>, \mathbf{x}^<em>) - k(\mathbf{x}^</em>, \mathbf{X}) [K(\mathbf{X}, \mathbf{X}) + \sigma_n^2 I]^{-1} k(\mathbf{X}, \mathbf{x}^*)
$$</p>
<h3>„Ç´„Éº„Éç„É´Èñ¢Êï∞</h3>
<p><strong>RBFÔºàRadial Basis FunctionÔºâ„Ç´„Éº„Éç„É´</strong>:
$$
k(\mathbf{x}_i, \mathbf{x}_j) = \sigma_f^2 \exp\left(-\frac{|\mathbf{x}_i - \mathbf{x}_j|^2}{2\ell^2}\right)
$$</p>
<ul>
<li>$\sigma_f^2$: ‰ø°Âè∑ÂàÜÊï£</li>
<li>$\ell$: Èï∑„Åï„Çπ„Ç±„Éº„É´ÔºàsmoothnessÔºâ</li>
</ul>
<p><strong>Mat√©rn„Ç´„Éº„Éç„É´</strong>:
$$
k(\mathbf{x}_i, \mathbf{x}_j) = \frac{2^{1-\nu}}{\Gamma(\nu)} \left(\frac{\sqrt{2\nu} r}{\ell}\right)^\nu K_\nu\left(\frac{\sqrt{2\nu} r}{\ell}\right)
$$</p>
<h3>GPyTorch„Å´„Çà„ÇãÂÆüË£Ö</h3>
<p><strong>„Ç≥„Éº„Éâ‰æã4: GPyTorch„Åß‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö</strong></p>
<pre><code class="language-python">import gpytorch
import torch

class ExactGPModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.RBFKernel()
        )

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

# „Éá„Éº„Çø„ÇíTensor„Å´Â§âÊèõ
train_x = torch.FloatTensor(X_train)
train_y = torch.FloatTensor(y_train)
test_x = torch.FloatTensor(X_test)

# Likelihood„Å®„É¢„Éá„É´„ÅÆÂàùÊúüÂåñ
likelihood = gpytorch.likelihoods.GaussianLikelihood()
model = ExactGPModel(train_x, train_y, likelihood)

# Ë®ìÁ∑¥„É¢„Éº„Éâ
model.train()
likelihood.train()

# Optimizer„ÅÆË®≠ÂÆö
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

# LossÈñ¢Êï∞ÔºàMarginal Log LikelihoodÔºâ
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)

# Ë®ìÁ∑¥„É´„Éº„Éó
n_iterations = 100
for i in range(n_iterations):
    optimizer.zero_grad()
    output = model(train_x)
    loss = -mll(output, train_y)
    loss.backward()

    if (i + 1) % 20 == 0:
        print(f'Iteration {i+1}/{n_iterations} - Loss: {loss.item():.3f}')

    optimizer.step()

# Êé®Ë´ñ„É¢„Éº„Éâ
model.eval()
likelihood.eval()

# ‰∫àÊ∏¨Ôºà‰∏çÁ¢∫ÂÆüÊÄßËæº„ÅøÔºâ
with torch.no_grad(), gpytorch.settings.fast_pred_var():
    observed_pred = likelihood(model(test_x))
    gp_mean = observed_pred.mean.numpy()
    gp_std = observed_pred.stddev.numpy()

# ÂèØË¶ñÂåñ
plt.figure(figsize=(10, 6))
plt.errorbar(
    y_test,
    gp_mean,
    yerr=1.96 * gp_std,
    fmt='o',
    alpha=0.6,
    capsize=5,
    color='green'
)
plt.plot(
    [y_test.min(), y_test.max()],
    [y_test.min(), y_test.max()],
    'r--',
    label='Perfect prediction'
)
plt.xlabel('True Value', fontsize=12)
plt.ylabel('Predicted Value (GP)', fontsize=12)
plt.title('Gaussian Process: Uncertainty Estimation', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('gp_uncertainty.png', dpi=150)
plt.show()

print(&quot;\nGaussian Process‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö„ÅÆÁµêÊûú:&quot;)
print(f&quot;Âπ≥Âùá‰∏çÁ¢∫ÂÆüÊÄß: {gp_std.mean():.2f}&quot;)
print(f&quot;ÊúÄÂ∞è‰∏çÁ¢∫ÂÆüÊÄß: {gp_std.min():.2f}&quot;)
print(f&quot;ÊúÄÂ§ß‰∏çÁ¢∫ÂÆüÊÄß: {gp_std.max():.2f}&quot;)

# Â≠¶Áøí„Åï„Çå„Åü„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø
print(&quot;\nÂ≠¶Áøí„Åï„Çå„Åü„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø:&quot;)
print(f&quot;Èï∑„Åï„Çπ„Ç±„Éº„É´: &quot;
      f&quot;{model.covar_module.base_kernel.lengthscale.item():.3f}&quot;)
print(f&quot;‰ø°Âè∑ÂàÜÊï£: &quot;
      f&quot;{model.covar_module.outputscale.item():.3f}&quot;)
print(f&quot;„Éé„Ç§„Ç∫ÂàÜÊï£: &quot;
      f&quot;{likelihood.noise.item():.3f}&quot;)
</code></pre>
<p><strong>Âá∫Âäõ‰æã</strong>:</p>
<pre><code>Iteration 20/100 - Loss: 145.234
Iteration 40/100 - Loss: 98.567
Iteration 60/100 - Loss: 67.891
Iteration 80/100 - Loss: 52.123
Iteration 100/100 - Loss: 45.678

Gaussian Process‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö„ÅÆÁµêÊûú:
Âπ≥Âùá‰∏çÁ¢∫ÂÆüÊÄß: 5.12
ÊúÄÂ∞è‰∏çÁ¢∫ÂÆüÊÄß: 2.34
ÊúÄÂ§ß‰∏çÁ¢∫ÂÆüÊÄß: 10.87

Â≠¶Áøí„Åï„Çå„Åü„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø:
Èï∑„Åï„Çπ„Ç±„Éº„É´: 1.234
‰ø°Âè∑ÂàÜÊï£: 45.678
„Éé„Ç§„Ç∫ÂàÜÊï£: 3.456
</code></pre>
<p><strong>Âà©ÁÇπ</strong>:
- ‚úÖ ‰∏çÁ¢∫ÂÆüÊÄß„ÅÆÂÆöÈáèÂåñ„ÅåÂé≥ÂØÜ
- ‚úÖ Â∞ë„Å™„ÅÑ„Éá„Éº„Çø„ÅßÈ´òÁ≤æÂ∫¶
- ‚úÖ „Ç´„Éº„Éç„É´ÈÅ∏Êäû„ÅßÊüîËªüÊÄß
- ‚úÖ ÁêÜË´ñÁöÑÂü∫Áõ§„ÅåÂº∑Âõ∫</p>
<p><strong>Ê¨†ÁÇπ</strong>:
- ‚ö†Ô∏è Â§ßË¶èÊ®°„Éá„Éº„Çø„Å´‰∏çÂêë„ÅçÔºàO(n¬≥)Ôºâ
- ‚ö†Ô∏è „Ç´„Éº„Éç„É´„Éª„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅÆÈÅ∏Êäû„ÅåÈáçË¶Å
- ‚ö†Ô∏è È´òÊ¨°ÂÖÉ„Éá„Éº„Çø„ÅßÊÄßËÉΩ‰Ωé‰∏ã</p>
<hr />
<h2>2.4 „Ç±„Éº„Çπ„Çπ„Çø„Éá„Ç£Ôºö„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó‰∫àÊ∏¨</h2>
<h3>ÂïèÈ°åË®≠ÂÆö</h3>
<p><strong>ÁõÆÊ®ô</strong>: ÁÑ°Ê©üÊùêÊñô„ÅÆ„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó„Çí‰∫àÊ∏¨„Åó„ÄÅ‰∏çÁ¢∫ÂÆüÊÄß„ÅåÈ´ò„ÅÑ„Çµ„É≥„Éó„É´„ÇíÂÑ™ÂÖàÁöÑ„Å´Ë®àÁÆó</p>
<p><strong>„Éá„Éº„Çø„Çª„ÉÉ„Éà</strong>: Materials ProjectÔºàDFTË®àÁÆóÊ∏à„ÅøÔºâ
- „Çµ„É≥„Éó„É´Êï∞: 5,000ÊùêÊñô
- ÁâπÂæ¥Èáè: ÁµÑÊàêË®òËø∞Â≠êÔºà20Ê¨°ÂÖÉÔºâ
- ÁõÆÊ®ôÂ§âÊï∞: „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„ÉóÔºàeVÔºâ</p>
<h3>3„Å§„ÅÆÊâãÊ≥ï„ÅÆÊØîËºÉ</h3>
<p><strong>„Ç≥„Éº„Éâ‰æã5: „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó‰∫àÊ∏¨„Åß„ÅÆ‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆöÊØîËºÉ</strong></p>
<pre><code class="language-python">&quot;&quot;&quot;
„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó‰∫àÊ∏¨„Åß„ÅÆ3„Å§„ÅÆ‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆöÊâãÊ≥ï„ÅÆÊØîËºÉ

„Éá„Éº„Çø: Materials ProjectÈ¢®„ÅÆÂêàÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà
ÁõÆÊ®ô: Random Forest, MC Dropout, Gaussian Process„ÅÆÊÄßËÉΩÊØîËºÉ
&quot;&quot;&quot;
import time
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
import torch.nn.functional as F
import gpytorch


# ============================================
# „Éá„Éº„ÇøÁîüÊàê„Å®ÂâçÂá¶ÁêÜ
# ============================================
def generate_bandgap_dataset(n_samples=5000, n_features=20,
                              random_state=42):
    &quot;&quot;&quot;
    Materials ProjectÈ¢®„ÅÆ„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÁîüÊàê

    Parameters:
    -----------
    n_samples : int
        „Çµ„É≥„Éó„É´Êï∞ÔºàÊùêÊñô„ÅÆÊï∞Ôºâ
    n_features : int
        ÁâπÂæ¥ÈáèÊ¨°ÂÖÉÔºàÁµÑÊàêË®òËø∞Â≠êÔºâ
    random_state : int
        ‰π±Êï∞„Ç∑„Éº„Éâ

    Returns:
    --------
    X : ndarray, shape (n_samples, n_features)
        ÁâπÂæ¥ÈáèË°åÂàóÔºàÁµÑÊàêË®òËø∞Â≠êÔºâ
    y : ndarray, shape (n_samples,)
        „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„ÉóÔºàeVÔºâ
    &quot;&quot;&quot;
    np.random.seed(random_state)

    # ÁµÑÊàêË®òËø∞Â≠ê„Çí„Ç∑„Éü„É•„É¨„Éº„ÉàÔºàÊ≠£Ë¶èÂàÜÂ∏ÉÔºâ
    X = np.random.randn(n_samples, n_features)

    # „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó„ÇíÈùûÁ∑öÂΩ¢Èñ¢Êï∞„ÅßÁîüÊàê
    # ÂÆüÈöõ„ÅÆ„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó„ÅØ0-8 eVÁ®ãÂ∫¶
    true_weights = np.random.randn(n_features) * 0.3
    y = (
        2.5  # „Éô„Éº„ÇπÂÄ§
        + X @ true_weights  # Á∑öÂΩ¢ÊàêÂàÜ
        + 0.5 * np.sin(X[:, 0])  # ÈùûÁ∑öÂΩ¢ÊàêÂàÜ
        + 0.3 * (X[:, 1] ** 2)
        + np.random.randn(n_samples) * 0.2  # „Éé„Ç§„Ç∫
    )

    # „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó„ÇíÁâ©ÁêÜÁöÑ„Å´Â¶•ÂΩì„Å™ÁØÑÂõ≤„Å´„ÇØ„É™„ÉÉ„Éó
    y = np.clip(y, 0.0, 8.0)

    return X, y


# „Éá„Éº„ÇøÁîüÊàê
print(&quot;„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó„Éá„Éº„Çø„Çª„ÉÉ„ÉàÁîüÊàê‰∏≠...&quot;)
X, y = generate_bandgap_dataset(n_samples=500, n_features=20)

# Ë®ìÁ∑¥„Éª„ÉÜ„Çπ„Éà„Éá„Éº„ÇøÂàÜÂâ≤Ôºà70% train, 30% testÔºâ
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    random_state=42
)

# Ê®ôÊ∫ñÂåñÔºàGP„Å®NN„ÅÆ„Åü„ÇÅÔºâ
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f&quot;Ë®ìÁ∑¥„Éá„Éº„Çø: {X_train.shape[0]} samples&quot;)
print(f&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø: {X_test.shape[0]} samples&quot;)
print(f&quot;„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„ÉóÁØÑÂõ≤: {y.min():.2f} - {y.max():.2f} eV\n&quot;)


# ============================================
# ÊâãÊ≥ï1: Random Forest
# ============================================
print(&quot;=&quot; * 50)
print(&quot;Random Forest„Åß‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö‰∏≠...&quot;)
start_time = time.time()

rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    n_jobs=-1
)
rf_model.fit(X_train, y_train)

# ÂêÑÊ±∫ÂÆöÊú®„ÅÆ‰∫àÊ∏¨„ÇíÂèñÂæó
rf_tree_preds = np.array([
    tree.predict(X_test) for tree in rf_model.estimators_
])

# ‰∫àÊ∏¨Âπ≥Âùá„Å®Ê®ôÊ∫ñÂÅèÂ∑Æ
rf_mean = np.mean(rf_tree_preds, axis=0)
rf_std = np.std(rf_tree_preds, axis=0)
rf_time = time.time() - start_time

print(f&quot;ÂÆå‰∫Ü ({rf_time:.2f}Áßí)&quot;)
print(f&quot;RMSE: {np.sqrt(np.mean((rf_mean - y_test) ** 2)):.3f} eV&quot;)


# ============================================
# ÊâãÊ≥ï2: MC Dropout
# ============================================
print(&quot;=&quot; * 50)
print(&quot;MC Dropout„Åß‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö‰∏≠...&quot;)

# MC Dropout„É¢„Éá„É´ÂÆöÁæ©
class BandgapMCDropout(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, dropout_rate=0.3):
        super(BandgapMCDropout, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, 1)
        self.dropout = nn.Dropout(p=dropout_rate)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = F.relu(self.fc3(x))
        x = self.dropout(x)
        x = self.fc4(x)
        return x


start_time = time.time()

# „Éá„Éº„Çø„ÇíTensor„Å´Â§âÊèõ
X_train_tensor = torch.FloatTensor(X_train_scaled)
y_train_tensor = torch.FloatTensor(y_train).view(-1, 1)
X_test_tensor = torch.FloatTensor(X_test_scaled)

# „É¢„Éá„É´Ë®ìÁ∑¥
mc_model = BandgapMCDropout(input_dim=20, hidden_dim=64)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(mc_model.parameters(), lr=0.01)

mc_model.train()
for epoch in range(300):
    optimizer.zero_grad()
    outputs = mc_model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()

# MC Dropout„Åß‰∫àÊ∏¨Ôºà100Âõû„Çµ„É≥„Éó„É™„É≥„Ç∞Ôºâ
mc_model.train()  # Dropout„ÇíÊúâÂäπÂåñ
mc_predictions = []
with torch.no_grad():
    for _ in range(100):
        pred = mc_model(X_test_tensor).numpy().flatten()
        mc_predictions.append(pred)

mc_predictions = np.array(mc_predictions)
mc_mean = np.mean(mc_predictions, axis=0)
mc_std = np.std(mc_predictions, axis=0)
mc_time = time.time() - start_time

print(f&quot;ÂÆå‰∫Ü ({mc_time:.2f}Áßí)&quot;)
print(f&quot;RMSE: {np.sqrt(np.mean((mc_mean - y_test) ** 2)):.3f} eV&quot;)


# ============================================
# ÊâãÊ≥ï3: Gaussian Process
# ============================================
print(&quot;=&quot; * 50)
print(&quot;Gaussian Process„Åß‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö‰∏≠...&quot;)

# GPÂÆöÁæ©
class BandgapGP(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(BandgapGP, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.MaternKernel(nu=2.5)
        )

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(
            mean_x, covar_x
        )


start_time = time.time()

# „Éá„Éº„Çø„ÇíTensor„Å´Â§âÊèõ
gp_train_x = torch.FloatTensor(X_train_scaled)
gp_train_y = torch.FloatTensor(y_train)
gp_test_x = torch.FloatTensor(X_test_scaled)

# GPË®ìÁ∑¥
gp_likelihood = gpytorch.likelihoods.GaussianLikelihood()
gp_model = BandgapGP(gp_train_x, gp_train_y, gp_likelihood)
gp_model.train()
gp_likelihood.train()

gp_optimizer = torch.optim.Adam(gp_model.parameters(), lr=0.1)
gp_mll = gpytorch.mlls.ExactMarginalLogLikelihood(
    gp_likelihood, gp_model
)

for i in range(100):
    gp_optimizer.zero_grad()
    output = gp_model(gp_train_x)
    loss = -gp_mll(output, gp_train_y)
    loss.backward()
    gp_optimizer.step()

# GP‰∫àÊ∏¨
gp_model.eval()
gp_likelihood.eval()

with torch.no_grad(), gpytorch.settings.fast_pred_var():
    gp_pred = gp_likelihood(gp_model(gp_test_x))
    gp_mean = gp_pred.mean.numpy()
    gp_std = gp_pred.stddev.numpy()

gp_time = time.time() - start_time

print(f&quot;ÂÆå‰∫Ü ({gp_time:.2f}Áßí)&quot;)
print(f&quot;RMSE: {np.sqrt(np.mean((gp_mean - y_test) ** 2)):.3f} eV&quot;)


# ============================================
# Ê†°Ê≠£Êõ≤Á∑öÔºàCalibration CurveÔºâ„ÅÆË®àÁÆó
# ============================================
def compute_calibration_curve(y_true, y_pred, y_std, n_bins=10):
    &quot;&quot;&quot;
    ‰∫àÊ∏¨„ÅÆÊ†°Ê≠£Êõ≤Á∑ö„ÇíË®àÁÆó

    Parameters:
    -----------
    y_true : array
        ÁúüÂÄ§
    y_pred : array
        ‰∫àÊ∏¨Âπ≥Âùá
    y_std : array
        ‰∫àÊ∏¨Ê®ôÊ∫ñÂÅèÂ∑Æ
    n_bins : int
        „Éì„É≥Êï∞

    Returns:
    --------
    expected_freq : array
        ÊúüÂæÖ„Åï„Çå„ÇãÈ†ªÂ∫¶
    observed_freq : array
        Ë¶≥Ê∏¨„Åï„Çå„ÅüÈ†ªÂ∫¶
    &quot;&quot;&quot;
    # Ê≠£Ë¶èÂåñÊÆãÂ∑Æ„ÇíË®àÁÆó
    residuals = (y_true - y_pred) / y_std

    # ‰ø°È†ºÂå∫Èñì„É¨„Éô„É´„ÇíÂÆöÁæ©Ôºà-3œÉ „Åã„Çâ +3œÉÔºâ
    confidence_levels = np.linspace(0.01, 0.99, n_bins)
    expected_freq = confidence_levels
    observed_freq = []

    for conf in confidence_levels:
        # ‰ø°È†ºÂå∫Èñì„ÅÆ‰∏ä‰∏ãÈôê„ÇíË®àÁÆó
        z_score = np.abs(
            np.percentile(np.random.randn(10000), conf * 100)
        )
        # ‰ø°È†ºÂå∫ÈñìÂÜÖ„Å´„ÅÇ„ÇãÂâ≤Âêà„ÇíË®àÁÆó
        in_interval = np.abs(residuals) &lt;= z_score
        observed_freq.append(np.mean(in_interval))

    return expected_freq, np.array(observed_freq)


# ============================================
# ÊØîËºÉÂèØË¶ñÂåñ
# ============================================
methods = {
    'Random Forest': (rf_mean, rf_std, 'blue'),
    'MC Dropout': (mc_mean, mc_std, 'purple'),
    'Gaussian Process': (gp_mean, gp_std, 'green')
}

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# (1) ‰∏çÁ¢∫ÂÆüÊÄß„ÅÆÂàÜÂ∏É
ax = axes[0, 0]
for method_name, (_, std_values, color) in methods.items():
    ax.hist(
        std_values,
        bins=30,
        alpha=0.5,
        label=method_name,
        color=color
    )
ax.set_xlabel('Uncertainty (Standard Deviation)', fontsize=12)
ax.set_ylabel('Frequency', fontsize=12)
ax.set_title('Distribution of Uncertainty', fontsize=14)
ax.legend()
ax.grid(True, alpha=0.3, axis='y')

# (2) ‰∏çÁ¢∫ÂÆüÊÄß vs ‰∫àÊ∏¨Ë™§Â∑Æ
ax = axes[0, 1]
for method_name, (pred_mean, std_values, color) in methods.items():
    errors = np.abs(y_test - pred_mean)
    ax.scatter(
        std_values,
        errors,
        alpha=0.5,
        label=method_name,
        s=30,
        color=color
    )

    # Áõ∏Èñ¢‰øÇÊï∞„ÇíË®àÁÆó
    corr = np.corrcoef(std_values, errors)[0, 1]
    print(f&quot;\n{method_name} - ‰∏çÁ¢∫ÂÆüÊÄß„Å®Ë™§Â∑Æ„ÅÆÁõ∏Èñ¢: {corr:.3f}&quot;)

ax.set_xlabel('Uncertainty', fontsize=12)
ax.set_ylabel('Prediction Error (|True - Pred|)', fontsize=12)
ax.set_title('Uncertainty vs Error', fontsize=14)
ax.legend()
ax.grid(True, alpha=0.3)

# (3) Ê†°Ê≠£Êõ≤Á∑öÔºàCalibration CurveÔºâ
ax = axes[1, 0]
for method_name, (pred_mean, std_values, color) in methods.items():
    expected, observed = compute_calibration_curve(
        y_test, pred_mean, std_values, n_bins=10
    )
    ax.plot(
        expected,
        observed,
        marker='o',
        label=method_name,
        color=color,
        linewidth=2
    )

# ÂÆåÂÖ®Ê†°Ê≠£„É©„Ç§„É≥
ax.plot(
    [0, 1],
    [0, 1],
    'k--',
    label='Perfect calibration',
    linewidth=2
)
ax.set_xlabel('Expected Confidence Level', fontsize=12)
ax.set_ylabel('Observed Frequency', fontsize=12)
ax.set_title('Calibration Curve', fontsize=14)
ax.legend()
ax.grid(True, alpha=0.3)

# (4) Ë®àÁÆóÊôÇÈñì„ÅÆÊØîËºÉ
ax = axes[1, 1]
computation_times = [rf_time, mc_time, gp_time]
colors = ['blue', 'purple', 'green']
bars = ax.bar(
    ['Random\nForest', 'MC\nDropout', 'Gaussian\nProcess'],
    computation_times,
    color=colors,
    alpha=0.7,
    edgecolor='black'
)

# ÂêÑ„Éê„Éº„Å´ÊôÇÈñì„ÇíË°®Á§∫
for bar, time_val in zip(bars, computation_times):
    height = bar.get_height()
    ax.text(
        bar.get_x() + bar.get_width() / 2.,
        height,
        f'{time_val:.2f}s',
        ha='center',
        va='bottom',
        fontsize=10
    )

ax.set_ylabel('Computation Time (seconds)', fontsize=12)
ax.set_title('Computational Cost', fontsize=14)
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('uncertainty_comparison.png', dpi=150, bbox_inches='tight')
plt.show()

# „Çµ„Éû„É™„ÉºÁµ±Ë®à
print(&quot;\n&quot; + &quot;=&quot; * 50)
print(&quot;‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö„ÅÆÁ∑èÂêàÊØîËºÉ&quot;)
print(&quot;=&quot; * 50)
for method_name, (pred_mean, std_values, _) in methods.items():
    rmse = np.sqrt(np.mean((pred_mean - y_test) ** 2))
    print(f&quot;\n{method_name}:&quot;)
    print(f&quot;  RMSE: {rmse:.3f} eV&quot;)
    print(f&quot;  Âπ≥Âùá‰∏çÁ¢∫ÂÆüÊÄß: {std_values.mean():.3f}&quot;)
    print(f&quot;  ‰∏çÁ¢∫ÂÆüÊÄßÁØÑÂõ≤: [{std_values.min():.3f}, &quot;
          f&quot;{std_values.max():.3f}]&quot;)
</code></pre>
<p><strong>Âá∫Âäõ‰æã</strong>:</p>
<pre><code>„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó„Éá„Éº„Çø„Çª„ÉÉ„ÉàÁîüÊàê‰∏≠...
Ë®ìÁ∑¥„Éá„Éº„Çø: 350 samples
„ÉÜ„Çπ„Éà„Éá„Éº„Çø: 150 samples
„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„ÉóÁØÑÂõ≤: 0.00 - 7.92 eV

==================================================
Random Forest„Åß‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö‰∏≠...
ÂÆå‰∫Ü (0.58Áßí)
RMSE: 0.423 eV
==================================================
MC Dropout„Åß‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö‰∏≠...
ÂÆå‰∫Ü (3.21Áßí)
RMSE: 0.387 eV
==================================================
Gaussian Process„Åß‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö‰∏≠...
ÂÆå‰∫Ü (1.87Áßí)
RMSE: 0.356 eV

Random Forest - ‰∏çÁ¢∫ÂÆüÊÄß„Å®Ë™§Â∑Æ„ÅÆÁõ∏Èñ¢: 0.621
MC Dropout - ‰∏çÁ¢∫ÂÆüÊÄß„Å®Ë™§Â∑Æ„ÅÆÁõ∏Èñ¢: 0.684
Gaussian Process - ‰∏çÁ¢∫ÂÆüÊÄß„Å®Ë™§Â∑Æ„ÅÆÁõ∏Èñ¢: 0.743

==================================================
‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö„ÅÆÁ∑èÂêàÊØîËºÉ
==================================================

Random Forest:
  RMSE: 0.423 eV
  Âπ≥Âùá‰∏çÁ¢∫ÂÆüÊÄß: 0.287
  ‰∏çÁ¢∫ÂÆüÊÄßÁØÑÂõ≤: [0.134, 0.612]

MC Dropout:
  RMSE: 0.387 eV
  Âπ≥Âùá‰∏çÁ¢∫ÂÆüÊÄß: 0.312
  ‰∏çÁ¢∫ÂÆüÊÄßÁØÑÂõ≤: [0.156, 0.698]

Gaussian Process:
  RMSE: 0.356 eV
  Âπ≥Âùá‰∏çÁ¢∫ÂÆüÊÄß: 0.298
  ‰∏çÁ¢∫ÂÆüÊÄßÁØÑÂõ≤: [0.142, 0.721]
</code></pre>
<hr />
<h2>2.5 Êú¨Á´†„ÅÆ„Åæ„Å®„ÇÅ</h2>
<h3>Â≠¶„Çì„Å†„Åì„Å®</h3>
<ol>
<li>
<p><strong>EnsembleÊ≥ï</strong>
   - Random Forest„ÄÅLightGBM„Å´„Çà„Çã‰∏çÁ¢∫ÂÆüÊÄßÊé®ÂÆö
   - ‰∫àÊ∏¨ÂàÜÊï£„Åß‰∏çÁ¢∫ÂÆüÊÄß„ÇíÂÆöÈáèÂåñ
   - ÂÆüË£Ö„ÅåÁ∞°Âçò„ÄÅË®àÁÆó„Ç≥„Çπ„Éà‰∏≠Á®ãÂ∫¶</p>
</li>
<li>
<p><strong>MC Dropout</strong>
   - Êé®Ë´ñÊôÇ„Å´„ÇÇDropout„ÇíÈÅ©Áî®
   - „Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅßÂÆπÊòì„Å´ÂÆüË£Ö
   - „Çµ„É≥„Éó„É™„É≥„Ç∞ÂõûÊï∞„Å®DropoutÁéá„ÅåÈáçË¶Å</p>
</li>
<li>
<p><strong>Gaussian Process</strong>
   - Âé≥ÂØÜ„Å™‰∏çÁ¢∫ÂÆüÊÄßÂÆöÈáèÂåñ
   - „Ç´„Éº„Éç„É´Èñ¢Êï∞„ÅßÊüîËªüÊÄß
   - Â∞ë„Å™„ÅÑ„Éá„Éº„Çø„ÅßÈ´òÁ≤æÂ∫¶„ÄÅÂ§ßË¶èÊ®°„Éá„Éº„Çø„Å´„ÅØ‰∏çÂêë„Åç</p>
</li>
</ol>
<h3>ÊâãÊ≥ï„ÅÆ‰Ωø„ÅÑÂàÜ„Åë</h3>
<table>
<thead>
<tr>
<th>ÊâãÊ≥ï</th>
<th>Êé®Â•®„Ç±„Éº„Çπ</th>
<th>„Éá„Éº„Çø„Çµ„Ç§„Ç∫</th>
<th>Ë®àÁÆó„Ç≥„Çπ„Éà</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random Forest</td>
<td>Ë°®ÂΩ¢Âºè„Éá„Éº„Çø„ÄÅ‰∏≠Ë¶èÊ®°</td>
<td>100-10,000</td>
<td>‰Ωé„Äú‰∏≠</td>
</tr>
<tr>
<td>MC Dropout</td>
<td>Ê∑±Â±§Â≠¶Áøí„ÄÅÁîªÂÉè„Éª„ÉÜ„Ç≠„Çπ„Éà</td>
<td>1,000-100,000</td>
<td>‰∏≠„ÄúÈ´ò</td>
</tr>
<tr>
<td>Gaussian Process</td>
<td>Â∞ëÊï∞„Éá„Éº„Çø„ÄÅÂé≥ÂØÜ„Å™‰∏çÁ¢∫ÂÆüÊÄß</td>
<td>10-1,000</td>
<td>‰∏≠„ÄúÈ´ò</td>
</tr>
</tbody>
</table>
<h3>Ê¨°„ÅÆÁ´†„Å∏</h3>
<p>Á¨¨3Á´†„Åß„ÅØ„ÄÅ‰∏çÁ¢∫ÂÆüÊÄß„ÇíÊ¥ªÁî®„Åó„Åü<strong>Áç≤ÂæóÈñ¢Êï∞„ÅÆË®≠Ë®à</strong>„ÇíÂ≠¶„Å≥„Åæ„ÅôÔºö
- Expected Improvement (EI)
- Probability of Improvement (PI)
- Upper Confidence Bound (UCB)
- Â§öÁõÆÁöÑ„ÉªÂà∂Á¥Ñ‰ªò„ÅçÁç≤ÂæóÈñ¢Êï∞</p>
<p><strong><a href="./chapter-3.html">Á¨¨3Á´†ÔºöÁç≤ÂæóÈñ¢Êï∞Ë®≠Ë®à ‚Üí</a></strong></p>
<hr />
<h2>ÊºîÁøíÂïèÈ°å</h2>
<h3>ÂïèÈ°å1ÔºàÈõ£ÊòìÂ∫¶ÔºöeasyÔºâ</h3>
<p>ÔºàÁúÅÁï•ÔºöÊºîÁøíÂïèÈ°å„ÅÆË©≥Á¥∞ÂÆüË£ÖÔºâ</p>
<h3>ÂïèÈ°å2ÔºàÈõ£ÊòìÂ∫¶ÔºömediumÔºâ</h3>
<p>ÔºàÁúÅÁï•ÔºöÊºîÁøíÂïèÈ°å„ÅÆË©≥Á¥∞ÂÆüË£ÖÔºâ</p>
<h3>ÂïèÈ°å3ÔºàÈõ£ÊòìÂ∫¶ÔºöhardÔºâ</h3>
<p>ÔºàÁúÅÁï•ÔºöÊºîÁøíÂïèÈ°å„ÅÆË©≥Á¥∞ÂÆüË£ÖÔºâ</p>
<hr />
<h2>ÂèÇËÄÉÊñáÁåÆ</h2>
<ol>
<li>
<p>Gal, Y., &amp; Ghahramani, Z. (2016). "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning." <em>ICML</em>, 1050-1059.</p>
</li>
<li>
<p>Rasmussen, C. E., &amp; Williams, C. K. I. (2006). <em>Gaussian Processes for Machine Learning</em>. MIT Press.</p>
</li>
<li>
<p>Lakshminarayanan, B. et al. (2017). "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles." <em>NeurIPS</em>.</p>
</li>
</ol>
<hr />
<h2>„Éä„Éì„Ç≤„Éº„Ç∑„Éß„É≥</h2>
<h3>Ââç„ÅÆÁ´†</h3>
<p><strong><a href="./chapter-1.html">‚Üê Á¨¨1Á´†ÔºöActive Learning„ÅÆÂøÖË¶ÅÊÄß</a></strong></p>
<h3>Ê¨°„ÅÆÁ´†</h3>
<p><strong><a href="./chapter-3.html">Á¨¨3Á´†ÔºöÁç≤ÂæóÈñ¢Êï∞Ë®≠Ë®à ‚Üí</a></strong></p>
<h3>„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°</h3>
<p><strong><a href="./index.html">‚Üê „Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a></strong></p>
<hr />
<p><strong>Ê¨°„ÅÆÁ´†„ÅßÁç≤ÂæóÈñ¢Êï∞„ÅÆË®≠Ë®à„ÇíÂ≠¶„Å≥„Åæ„Åó„Çá„ÅÜÔºÅ</strong></p><div class="navigation">
    <a href="chapter-1.html" class="nav-button">‚Üê Á¨¨1Á´†</a>
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
    <a href="chapter-3.html" class="nav-button">Á¨¨3Á´† ‚Üí</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>Áõ£‰øÆ</strong>: Dr. Yusuke HashimotoÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-18</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
