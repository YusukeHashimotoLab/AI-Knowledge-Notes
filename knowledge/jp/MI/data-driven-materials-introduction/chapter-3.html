<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
<meta content="Chapter - AI Terakoya" name="description"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/MI/data-driven-materials-introduction/chapter-3.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../MI/index.html">ãƒãƒ†ãƒªã‚¢ãƒ«ã‚ºãƒ»ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹</a><span class="breadcrumb-separator">â€º</span><a href="../../MI/data-driven-materials-introduction/index.html">Data Driven Materials</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 20-25åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 0å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 0å•</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 3: ãƒ¢ãƒ‡ãƒ«é¸æŠã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</h1>
<hr />
<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<p>âœ… ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«å¿œã˜ãŸé©åˆ‡ãªãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆç·šå½¢ã€æœ¨ãƒ™ãƒ¼ã‚¹ã€NNã€GNNï¼‰
âœ… äº¤å·®æ¤œè¨¼ï¼ˆK-Foldã€Stratifiedã€Time Series Splitï¼‰ã®å®Ÿè·µ
âœ… Optunaã«ã‚ˆã‚‹ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’ç”¨ã„ãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è‡ªå‹•æœ€é©åŒ–
âœ… ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ï¼ˆBaggingã€Boostingã€Stackingï¼‰ã®å®Ÿè£…
âœ… Li-ioné›»æ± å®¹é‡äºˆæ¸¬ã«ãŠã‘ã‚‹å®Ÿè·µçš„ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</p>
<hr />
<h2>3.1 ãƒ¢ãƒ‡ãƒ«é¸æŠã®æˆ¦ç•¥</h2>
<p>ææ–™ç§‘å­¦ã«ãŠã‘ã‚‹æ©Ÿæ¢°å­¦ç¿’ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã«å¿œã˜ãŸé©åˆ‡ãªãƒ¢ãƒ‡ãƒ«é¸æŠãŒé‡è¦ã§ã™ã€‚</p>
<h3>ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã¨ãƒ¢ãƒ‡ãƒ«è¤‡é›‘åº¦</h3>
<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import learning_curve
from sklearn.linear_model import Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)

def generate_material_data(n_samples, n_features=20):
    &quot;&quot;&quot;ææ–™ãƒ‡ãƒ¼ã‚¿ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³&quot;&quot;&quot;
    X = np.random.randn(n_samples, n_features)
    # éç·šå½¢é–¢ä¿‚
    y = (
        2 * X[:, 0]**2 +
        3 * X[:, 1] * X[:, 2] -
        1.5 * X[:, 3] +
        np.random.normal(0, 0.5, n_samples)
    )
    return X, y

# ãƒ¢ãƒ‡ãƒ«è¤‡é›‘åº¦ã¨ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®é–¢ä¿‚
sample_sizes = [50, 100, 200, 500, 1000]
models = {
    'Ridge': Ridge(),
    'Random Forest': RandomForestRegressor(n_estimators=50, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=50, random_state=42),
    'Neural Network': MLPRegressor(hidden_layers=(50, 50), max_iter=1000, random_state=42)
}

# å­¦ç¿’æ›²ç·š
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

for idx, (model_name, model) in enumerate(models.items()):
    X, y = generate_material_data(1000, n_features=20)

    train_sizes, train_scores, val_scores = learning_curve(
        model, X, y,
        train_sizes=np.linspace(0.1, 1.0, 10),
        cv=5,
        scoring='neg_mean_squared_error',
        n_jobs=-1
    )

    train_mean = -train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    val_mean = -val_scores.mean(axis=1)
    val_std = val_scores.std(axis=1)

    axes[idx].plot(train_sizes, train_mean, 'o-',
                   color='steelblue', label='Training Error')
    axes[idx].fill_between(train_sizes,
                           train_mean - train_std,
                           train_mean + train_std,
                           alpha=0.2, color='steelblue')

    axes[idx].plot(train_sizes, val_mean, 'o-',
                   color='coral', label='Validation Error')
    axes[idx].fill_between(train_sizes,
                           val_mean - val_std,
                           val_mean + val_std,
                           alpha=0.2, color='coral')

    axes[idx].set_xlabel('Training Size', fontsize=11)
    axes[idx].set_ylabel('MSE', fontsize=11)
    axes[idx].set_title(f'{model_name}', fontsize=12, fontweight='bold')
    axes[idx].legend(loc='upper right')
    axes[idx].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;ãƒ¢ãƒ‡ãƒ«é¸æŠã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ï¼š&quot;)
print(&quot;- å°ãƒ‡ãƒ¼ã‚¿ (&lt;100): Ridge, Lassoï¼ˆæ­£å‰‡åŒ–ç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼‰&quot;)
print(&quot;- ä¸­ãƒ‡ãƒ¼ã‚¿ (100-1000): Random Forest, Gradient Boosting&quot;)
print(&quot;- å¤§ãƒ‡ãƒ¼ã‚¿ (&gt;1000): Neural Network, Deep Learning&quot;)
</code></pre>
<h3>è§£é‡ˆæ€§ vs ç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•</h3>
<pre><code class="language-python"># ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆæ€§ã¨ç²¾åº¦ã®æ¯”è¼ƒ
model_comparison = pd.DataFrame({
    'ãƒ¢ãƒ‡ãƒ«': [
        'Linear Regression',
        'Ridge/Lasso',
        'Decision Tree',
        'Random Forest',
        'Gradient Boosting',
        'Neural Network',
        'GNN'
    ],
    'è§£é‡ˆæ€§': [10, 9, 7, 4, 3, 2, 1],
    'ç²¾åº¦': [4, 5, 5, 8, 9, 9, 10],
    'è¨“ç·´é€Ÿåº¦': [10, 9, 8, 6, 5, 3, 2],
    'æ¨è«–é€Ÿåº¦': [10, 10, 9, 7, 6, 8, 4]
})

# ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
from math import pi

categories = ['è§£é‡ˆæ€§', 'ç²¾åº¦', 'è¨“ç·´é€Ÿåº¦', 'æ¨è«–é€Ÿåº¦']
N = len(categories)

angles = [n / float(N) * 2 * pi for n in range(N)]
angles += angles[:1]

fig, axes = plt.subplots(2, 4, figsize=(18, 10),
                         subplot_kw=dict(projection='polar'))
axes = axes.flatten()

for idx, row in model_comparison.iterrows():
    values = row[categories].tolist()
    values += values[:1]

    axes[idx].plot(angles, values, 'o-', linewidth=2)
    axes[idx].fill(angles, values, alpha=0.25)
    axes[idx].set_xticks(angles[:-1])
    axes[idx].set_xticklabels(categories, size=9)
    axes[idx].set_ylim(0, 10)
    axes[idx].set_title(row['ãƒ¢ãƒ‡ãƒ«'], size=11, fontweight='bold', pad=20)
    axes[idx].grid(True)

plt.tight_layout()
plt.show()

print(&quot;\nãƒ¢ãƒ‡ãƒ«é¸æŠã®åˆ¤æ–­åŸºæº–ï¼š&quot;)
print(&quot;- è§£é‡ˆæ€§é‡è¦–: Ridge, Lasso, Decision Tree&quot;)
print(&quot;- ç²¾åº¦é‡è¦–: Gradient Boosting, Neural Network, GNN&quot;)
print(&quot;- ãƒãƒ©ãƒ³ã‚¹å‹: Random Forest&quot;)
</code></pre>
<h3>ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã€æœ¨ãƒ™ãƒ¼ã‚¹ã€NNã€GNNã®ä½¿ã„åˆ†ã‘</h3>
<pre><code class="language-python"># å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½æ¯”è¼ƒ
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_absolute_error, r2_score

X, y = generate_material_data(500, n_features=20)

models_benchmark = {
    'Ridge': Ridge(alpha=1.0),
    'Lasso': Lasso(alpha=0.1),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'MLP': MLPRegressor(hidden_layers=(100, 50), max_iter=1000, random_state=42)
}

results = []

for model_name, model in models_benchmark.items():
    # äº¤å·®æ¤œè¨¼
    cv_scores = cross_val_score(model, X, y, cv=5,
                                scoring='neg_mean_absolute_error')
    mae = -cv_scores.mean()
    mae_std = cv_scores.std()

    # RÂ²
    cv_r2 = cross_val_score(model, X, y, cv=5, scoring='r2')
    r2 = cv_r2.mean()

    results.append({
        'Model': model_name,
        'MAE': mae,
        'MAE_std': mae_std,
        'RÂ²': r2
    })

results_df = pd.DataFrame(results)
print(&quot;\nãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒï¼š&quot;)
print(results_df.to_string(index=False))

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# MAE
axes[0].barh(results_df['Model'], results_df['MAE'],
             xerr=results_df['MAE_std'],
             color='steelblue', alpha=0.7)
axes[0].set_xlabel('MAE (lower is better)', fontsize=11)
axes[0].set_title('äºˆæ¸¬èª¤å·®ï¼ˆMAEï¼‰', fontsize=12, fontweight='bold')
axes[0].grid(axis='x', alpha=0.3)

# RÂ²
axes[1].barh(results_df['Model'], results_df['RÂ²'],
             color='coral', alpha=0.7)
axes[1].set_xlabel('RÂ² (higher is better)', fontsize=11)
axes[1].set_title('æ±ºå®šä¿‚æ•°ï¼ˆRÂ²ï¼‰', fontsize=12, fontweight='bold')
axes[1].set_xlim(0, 1)
axes[1].grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<hr />
<h2>3.2 äº¤å·®æ¤œè¨¼ï¼ˆCross-Validationï¼‰</h2>
<p>ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–æ€§èƒ½ã‚’é©åˆ‡ã«è©•ä¾¡ã™ã‚‹ãŸã‚ã®æ‰‹æ³•ã§ã™ã€‚</p>
<h3>K-Fold CV</h3>
<pre><code class="language-python">from sklearn.model_selection import KFold, cross_validate

def kfold_cv_demo(X, y, model, k=5):
    &quot;&quot;&quot;
    K-Foldäº¤å·®æ¤œè¨¼ã®ãƒ‡ãƒ¢
    &quot;&quot;&quot;
    kf = KFold(n_splits=k, shuffle=True, random_state=42)

    fold_results = []

    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X)):
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)

        mae = mean_absolute_error(y_val, y_pred)
        r2 = r2_score(y_val, y_pred)

        fold_results.append({
            'Fold': fold_idx + 1,
            'MAE': mae,
            'RÂ²': r2
        })

    return pd.DataFrame(fold_results)

# K-Foldå®Ÿè¡Œ
model = RandomForestRegressor(n_estimators=100, random_state=42)
fold_results = kfold_cv_demo(X, y, model, k=5)

print(&quot;K-Fold CVçµæœï¼š&quot;)
print(fold_results.to_string(index=False))
print(f&quot;\nå¹³å‡MAE: {fold_results['MAE'].mean():.4f} Â± {fold_results['MAE'].std():.4f}&quot;)
print(f&quot;å¹³å‡RÂ²: {fold_results['RÂ²'].mean():.4f} Â± {fold_results['RÂ²'].std():.4f}&quot;)

# å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(10, 6))
x_pos = np.arange(len(fold_results))

ax.bar(x_pos, fold_results['MAE'], color='steelblue', alpha=0.7,
       label='MAE per fold')
ax.axhline(y=fold_results['MAE'].mean(), color='red',
           linestyle='--', linewidth=2, label='Mean MAE')
ax.fill_between(x_pos,
                fold_results['MAE'].mean() - fold_results['MAE'].std(),
                fold_results['MAE'].mean() + fold_results['MAE'].std(),
                color='red', alpha=0.2, label='Â±1 Std')

ax.set_xlabel('Fold', fontsize=12)
ax.set_ylabel('MAE', fontsize=12)
ax.set_title('K-Foldäº¤å·®æ¤œè¨¼çµæœ', fontsize=13, fontweight='bold')
ax.set_xticks(x_pos)
ax.set_xticklabels(fold_results['Fold'])
ax.legend()
ax.grid(alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>Stratified K-Fold</h3>
<pre><code class="language-python">from sklearn.model_selection import StratifiedKFold

# åˆ†é¡å•é¡Œç”¨ãƒ‡ãƒ¼ã‚¿
X_class, _ = generate_material_data(500, n_features=20)
# 3ã‚¯ãƒ©ã‚¹åˆ†é¡
y_class = np.digitize(y, bins=np.percentile(y, [33, 67]))

def compare_kfold_strategies(X, y):
    &quot;&quot;&quot;
    é€šå¸¸K-Fold vs Stratified K-Fold
    &quot;&quot;&quot;
    # é€šå¸¸K-Fold
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    normal_distributions = []

    for train_idx, _ in kf.split(X):
        y_train = y[train_idx]
        class_dist = np.bincount(y_train) / len(y_train)
        normal_distributions.append(class_dist)

    # Stratified K-Fold
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    stratified_distributions = []

    for train_idx, _ in skf.split(X, y):
        y_train = y[train_idx]
        class_dist = np.bincount(y_train) / len(y_train)
        stratified_distributions.append(class_dist)

    return normal_distributions, stratified_distributions

normal_dist, stratified_dist = compare_kfold_strategies(X_class, y_class)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# é€šå¸¸K-Fold
normal_array = np.array(normal_dist)
axes[0].bar(range(len(normal_array)), normal_array[:, 0],
            label='Class 0', alpha=0.7)
axes[0].bar(range(len(normal_array)), normal_array[:, 1],
            bottom=normal_array[:, 0],
            label='Class 1', alpha=0.7)
axes[0].bar(range(len(normal_array)), normal_array[:, 2],
            bottom=normal_array[:, 0] + normal_array[:, 1],
            label='Class 2', alpha=0.7)
axes[0].set_xlabel('Fold', fontsize=11)
axes[0].set_ylabel('Class Distribution', fontsize=11)
axes[0].set_title('é€šå¸¸K-Fold', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].set_ylim(0, 1)

# Stratified K-Fold
stratified_array = np.array(stratified_dist)
axes[1].bar(range(len(stratified_array)), stratified_array[:, 0],
            label='Class 0', alpha=0.7)
axes[1].bar(range(len(stratified_array)), stratified_array[:, 1],
            bottom=stratified_array[:, 0],
            label='Class 1', alpha=0.7)
axes[1].bar(range(len(stratified_array)), stratified_array[:, 2],
            bottom=stratified_array[:, 0] + stratified_array[:, 1],
            label='Class 2', alpha=0.7)
axes[1].set_xlabel('Fold', fontsize=11)
axes[1].set_ylabel('Class Distribution', fontsize=11)
axes[1].set_title('Stratified K-Fold', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].set_ylim(0, 1)

plt.tight_layout()
plt.show()

print(&quot;Stratified K-Foldã®åˆ©ç‚¹ï¼š&quot;)
print(&quot;- å„Foldã§ã‚¯ãƒ©ã‚¹åˆ†å¸ƒãŒå‡ä¸€&quot;)
print(&quot;- ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã§ã‚‚å®‰å®šã—ãŸè©•ä¾¡&quot;)
</code></pre>
<h3>Time Series Splitï¼ˆé€æ¬¡ãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰</h3>
<pre><code class="language-python">from sklearn.model_selection import TimeSeriesSplit

# æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
n_time_points = 200
time = np.arange(n_time_points)
# ãƒˆãƒ¬ãƒ³ãƒ‰ + å­£ç¯€æ€§ + ãƒã‚¤ã‚º
y_timeseries = (
    0.05 * time +
    10 * np.sin(2 * np.pi * time / 50) +
    np.random.normal(0, 2, n_time_points)
)
X_timeseries = np.column_stack([time, np.sin(2 * np.pi * time / 50)])

# Time Series Split
tscv = TimeSeriesSplit(n_splits=5)

# å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(14, 6))

colors = plt.cm.viridis(np.linspace(0, 1, 5))

for fold_idx, (train_idx, test_idx) in enumerate(tscv.split(X_timeseries)):
    # Train
    ax.scatter(time[train_idx], y_timeseries[train_idx],
               c=[colors[fold_idx]], s=20, alpha=0.3,
               label=f'Fold {fold_idx+1} Train')
    # Test
    ax.scatter(time[test_idx], y_timeseries[test_idx],
               c=[colors[fold_idx]], s=50, marker='s',
               label=f'Fold {fold_idx+1} Test')

ax.plot(time, y_timeseries, 'k-', alpha=0.3, linewidth=1)
ax.set_xlabel('Time', fontsize=12)
ax.set_ylabel('Value', fontsize=12)
ax.set_title('Time Series Split', fontsize=13, fontweight='bold')
ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
ax.grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;Time Series Splitã®ç‰¹å¾´ï¼š&quot;)
print(&quot;- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¯å¸¸ã«ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šå‰&quot;)
print(&quot;- æœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ãªã„ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰&quot;)
</code></pre>
<h3>Leave-One-Out CVï¼ˆå°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰</h3>
<pre><code class="language-python">from sklearn.model_selection import LeaveOneOut

def loo_cv_demo(X, y, model):
    &quot;&quot;&quot;
    Leave-One-Out CV
    å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ç”¨
    &quot;&quot;&quot;
    loo = LeaveOneOut()
    predictions = []
    actuals = []

    for train_idx, test_idx in loo.split(X):
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        predictions.append(y_pred[0])
        actuals.append(y_test[0])

    return np.array(actuals), np.array(predictions)

# å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿
X_small, y_small = generate_material_data(50, n_features=10)
model_small = Ridge(alpha=1.0)

y_actual, y_pred_loo = loo_cv_demo(X_small, y_small, model_small)

mae_loo = mean_absolute_error(y_actual, y_pred_loo)
r2_loo = r2_score(y_actual, y_pred_loo)

print(f&quot;LOO CVçµæœ (n={len(X_small)}):&quot;)
print(f&quot;MAE: {mae_loo:.4f}&quot;)
print(f&quot;RÂ²: {r2_loo:.4f}&quot;)

# äºˆæ¸¬ vs å®Ÿæ¸¬
plt.figure(figsize=(8, 8))
plt.scatter(y_actual, y_pred_loo, c='steelblue', s=50, alpha=0.6)
plt.plot([y_actual.min(), y_actual.max()],
         [y_actual.min(), y_actual.max()],
         'r--', linewidth=2, label='Perfect Prediction')
plt.xlabel('Actual', fontsize=12)
plt.ylabel('Predicted', fontsize=12)
plt.title('Leave-One-Out CV Results', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>
<hr />
<h2>3.3 ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</h2>
<p>ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æœ€å¤§åŒ–ã™ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢ã—ã¾ã™ã€‚</p>
<h3>Grid Searchï¼ˆå…¨æ¢ç´¢ï¼‰</h3>
<pre><code class="language-python">from sklearn.model_selection import GridSearchCV

def grid_search_demo(X, y):
    &quot;&quot;&quot;
    Grid Searchã«ã‚ˆã‚‹å…¨æ¢ç´¢
    &quot;&quot;&quot;
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [5, 10, 15, None],
        'min_samples_split': [2, 5, 10]
    }

    model = RandomForestRegressor(random_state=42)

    grid_search = GridSearchCV(
        model,
        param_grid,
        cv=5,
        scoring='neg_mean_absolute_error',
        n_jobs=-1,
        verbose=1
    )

    grid_search.fit(X, y)

    return grid_search

# å®Ÿè¡Œ
grid_result = grid_search_demo(X, y)

print(&quot;Grid Searchçµæœï¼š&quot;)
print(f&quot;æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {grid_result.best_params_}&quot;)
print(f&quot;æœ€è‰¯ã‚¹ã‚³ã‚¢ (MAE): {-grid_result.best_score_:.4f}&quot;)
print(f&quot;\næ¢ç´¢ç©ºé–“ã‚µã‚¤ã‚º: {len(grid_result.cv_results_['params'])}&quot;)

# çµæœå¯è¦–åŒ–ï¼ˆ2æ¬¡å…ƒãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼‰
results = pd.DataFrame(grid_result.cv_results_)

# n_estimators vs max_depth
pivot_table = results.pivot_table(
    values='mean_test_score',
    index='param_max_depth',
    columns='param_n_estimators',
    aggfunc='mean'
)

plt.figure(figsize=(10, 8))
sns.heatmap(-pivot_table, annot=True, fmt='.3f',
            cmap='YlOrRd', cbar_kws={'label': 'MAE'})
plt.xlabel('n_estimators', fontsize=12)
plt.ylabel('max_depth', fontsize=12)
plt.title('Grid Searchçµæœï¼ˆMAEï¼‰', fontsize=13, fontweight='bold')
plt.tight_layout()
plt.show()
</code></pre>
<h3>Random Searchï¼ˆãƒ©ãƒ³ãƒ€ãƒ æ¢ç´¢ï¼‰</h3>
<pre><code class="language-python">from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

def random_search_demo(X, y, n_iter=50):
    &quot;&quot;&quot;
    Random Searchã«ã‚ˆã‚‹ãƒ©ãƒ³ãƒ€ãƒ æ¢ç´¢
    &quot;&quot;&quot;
    param_distributions = {
        'n_estimators': randint(50, 300),
        'max_depth': randint(5, 30),
        'min_samples_split': randint(2, 20),
        'min_samples_leaf': randint(1, 10),
        'max_features': uniform(0.3, 0.7)
    }

    model = RandomForestRegressor(random_state=42)

    random_search = RandomizedSearchCV(
        model,
        param_distributions,
        n_iter=n_iter,
        cv=5,
        scoring='neg_mean_absolute_error',
        n_jobs=-1,
        random_state=42,
        verbose=1
    )

    random_search.fit(X, y)

    return random_search

# å®Ÿè¡Œ
random_result = random_search_demo(X, y, n_iter=50)

print(&quot;\nRandom Searchçµæœï¼š&quot;)
print(f&quot;æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {random_result.best_params_}&quot;)
print(f&quot;æœ€è‰¯ã‚¹ã‚³ã‚¢ (MAE): {-random_result.best_score_:.4f}&quot;)

# Grid vs Randomæ¯”è¼ƒ
print(f&quot;\nGrid Searchæœ€è‰¯ã‚¹ã‚³ã‚¢: {-grid_result.best_score_:.4f}&quot;)
print(f&quot;Random Searchæœ€è‰¯ã‚¹ã‚³ã‚¢: {-random_result.best_score_:.4f}&quot;)
print(f&quot;\nRandom Searchã®æ¢ç´¢åŠ¹ç‡: {50 / len(grid_result.cv_results_['params']) * 100:.1f}%ã®æ¢ç´¢ã§åŒç­‰æ€§èƒ½&quot;)
</code></pre>
<h3>Bayesian Optimizationï¼ˆOptuna, Hyperoptï¼‰</h3>
<pre><code class="language-python">import optuna

def objective(trial, X, y):
    &quot;&quot;&quot;
    Optunaç›®çš„é–¢æ•°
    &quot;&quot;&quot;
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'max_depth': trial.suggest_int('max_depth', 5, 30),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
        'max_features': trial.suggest_float('max_features', 0.3, 1.0),
        'random_state': 42
    }

    model = RandomForestRegressor(**params)

    # äº¤å·®æ¤œè¨¼
    cv_scores = cross_val_score(
        model, X, y, cv=5,
        scoring='neg_mean_absolute_error'
    )

    return -cv_scores.mean()  # æœ€å°åŒ–ã™ã‚‹ãŸã‚è² ã«ã™ã‚‹

# Optunaæœ€é©åŒ–
study = optuna.create_study(direction='minimize')
study.optimize(lambda trial: objective(trial, X, y), n_trials=100, show_progress_bar=True)

print(&quot;\nOptuna Bayesian Optimizationçµæœï¼š&quot;)
print(f&quot;æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {study.best_params}&quot;)
print(f&quot;æœ€è‰¯ã‚¹ã‚³ã‚¢ (MAE): {study.best_value:.4f}&quot;)

# æœ€é©åŒ–å±¥æ­´
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# æœ€é©åŒ–å±¥æ­´
axes[0].plot(study.trials_dataframe()['number'],
             study.trials_dataframe()['value'],
             'o-', color='steelblue', alpha=0.6, label='Trial Score')
axes[0].plot(study.trials_dataframe()['number'],
             study.trials_dataframe()['value'].cummin(),
             'r-', linewidth=2, label='Best Score')
axes[0].set_xlabel('Trial', fontsize=11)
axes[0].set_ylabel('MAE', fontsize=11)
axes[0].set_title('æœ€é©åŒ–å±¥æ­´', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡è¦åº¦
importances = optuna.importance.get_param_importances(study)
axes[1].barh(list(importances.keys()), list(importances.values()),
             color='coral', alpha=0.7)
axes[1].set_xlabel('Importance', fontsize=11)
axes[1].set_title('ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡è¦åº¦', fontsize=12, fontweight='bold')
axes[1].grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>æ—©æœŸçµ‚äº†ï¼ˆEarly Stoppingï¼‰</h3>
<pre><code class="language-python">from sklearn.ensemble import GradientBoostingRegressor

def early_stopping_demo(X, y):
    &quot;&quot;&quot;
    Early Stoppingã®ãƒ‡ãƒ¢
    &quot;&quot;&quot;
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # GradientBoostingã§ã‚¹ãƒ†ãƒ¼ã‚¸ã”ã¨ã®æ€§èƒ½è¿½è·¡
    model = GradientBoostingRegressor(
        n_estimators=500,
        learning_rate=0.1,
        max_depth=5,
        random_state=42
    )

    model.fit(X_train, y_train)

    # ã‚¹ãƒ†ãƒ¼ã‚¸ã”ã¨ã®äºˆæ¸¬
    train_scores = []
    val_scores = []

    for i, y_pred_train in enumerate(model.staged_predict(X_train)):
        y_pred_val = list(model.staged_predict(X_val))[i]

        train_mae = mean_absolute_error(y_train, y_pred_train)
        val_mae = mean_absolute_error(y_val, y_pred_val)

        train_scores.append(train_mae)
        val_scores.append(val_mae)

    # æœ€é©ãªn_estimatorsï¼ˆæ¤œè¨¼èª¤å·®æœ€å°ï¼‰
    best_n_estimators = np.argmin(val_scores) + 1

    return train_scores, val_scores, best_n_estimators

train_curve, val_curve, best_n = early_stopping_demo(X, y)

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(train_curve)+1), train_curve,
         'b-', label='Training Error', linewidth=2)
plt.plot(range(1, len(val_curve)+1), val_curve,
         'r-', label='Validation Error', linewidth=2)
plt.axvline(x=best_n, color='green', linestyle='--',
            label=f'Best n_estimators={best_n}', linewidth=2)
plt.xlabel('n_estimators', fontsize=12)
plt.ylabel('MAE', fontsize=12)
plt.title('Early Stopping', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(f&quot;Early Stoppingçµæœï¼š&quot;)
print(f&quot;æœ€é©n_estimators: {best_n}&quot;)
print(f&quot;æ¤œè¨¼MAE: {val_curve[best_n-1]:.4f}&quot;)
print(f&quot;éå­¦ç¿’é˜²æ­¢: {500 - best_n} ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å‰Šæ¸›&quot;)
</code></pre>
<hr />
<h2>3.4 ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’</h2>
<p>è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã¦äºˆæ¸¬ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚</p>
<h3>Baggingï¼ˆBootstrap Aggregatingï¼‰</h3>
<pre><code class="language-python">from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor

# Bagging
bagging = BaggingRegressor(
    estimator=DecisionTreeRegressor(max_depth=10),
    n_estimators=50,
    max_samples=0.8,
    random_state=42
)

# æ¯”è¼ƒç”¨ï¼šå˜ä¸€Decision Tree
single_tree = DecisionTreeRegressor(max_depth=10, random_state=42)

# è©•ä¾¡
cv_bagging = cross_val_score(bagging, X, y, cv=5,
                             scoring='neg_mean_absolute_error')
cv_single = cross_val_score(single_tree, X, y, cv=5,
                            scoring='neg_mean_absolute_error')

print(&quot;Baggingçµæœï¼š&quot;)
print(f&quot;å˜ä¸€Decision Tree MAE: {-cv_single.mean():.4f} Â± {cv_single.std():.4f}&quot;)
print(f&quot;Bagging MAE: {-cv_bagging.mean():.4f} Â± {cv_bagging.std():.4f}&quot;)
print(f&quot;æ”¹å–„ç‡: {(cv_single.mean() - cv_bagging.mean()) / cv_single.mean() * 100:.1f}%&quot;)
</code></pre>
<h3>Boostingï¼ˆAdaBoost, Gradient Boosting, LightGBM, XGBoostï¼‰</h3>
<pre><code class="language-python">from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor
import lightgbm as lgb
# import xgboost as xgb  # Optional

# å„ç¨®Boostingã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
boosting_models = {
    'AdaBoost': AdaBoostRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1),
    # 'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42)
}

boosting_results = []

for model_name, model in boosting_models.items():
    cv_scores = cross_val_score(model, X, y, cv=5,
                                scoring='neg_mean_absolute_error')
    mae = -cv_scores.mean()
    mae_std = cv_scores.std()

    boosting_results.append({
        'Model': model_name,
        'MAE': mae,
        'MAE_std': mae_std
    })

boosting_df = pd.DataFrame(boosting_results)

print(&quot;\nBoostingæ‰‹æ³•ã®æ¯”è¼ƒï¼š&quot;)
print(boosting_df.to_string(index=False))

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.barh(boosting_df['Model'], boosting_df['MAE'],
         xerr=boosting_df['MAE_std'],
         color='steelblue', alpha=0.7)
plt.xlabel('MAE', fontsize=12)
plt.title('Boostingæ‰‹æ³•ã®æ€§èƒ½æ¯”è¼ƒ', fontsize=13, fontweight='bold')
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>
<h3>Stacking</h3>
<pre><code class="language-python">from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import Ridge

# Base models
base_models = [
    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),
    ('gb', GradientBoostingRegressor(n_estimators=100, random_state=42)),
    ('lgbm', lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1))
]

# Meta model
meta_model = Ridge(alpha=1.0)

# Stacking
stacking = StackingRegressor(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5
)

# è©•ä¾¡
cv_stacking = cross_val_score(stacking, X, y, cv=5,
                              scoring='neg_mean_absolute_error')

print(&quot;\nStackingçµæœï¼š&quot;)
for name, _ in base_models:
    model_cv = cross_val_score(dict(base_models)[name], X, y, cv=5,
                               scoring='neg_mean_absolute_error')
    print(f&quot;{name} MAE: {-model_cv.mean():.4f}&quot;)

print(f&quot;Stacking Ensemble MAE: {-cv_stacking.mean():.4f}&quot;)
print(f&quot;\næ”¹å–„åŠ¹æœ: StackingãŒæœ€è‰¯å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Š &quot;
      f&quot;{((-cv_stacking.mean() / min([cross_val_score(m, X, y, cv=5, scoring='neg_mean_absolute_error').mean() for _, m in base_models])) - 1) * -100:.1f}% æ”¹å–„&quot;)
</code></pre>
<h3>Voting</h3>
<pre><code class="language-python">from sklearn.ensemble import VotingRegressor

# Voting Ensemble
voting = VotingRegressor(
    estimators=base_models,
    weights=[1, 1.5, 1]  # GBã«é«˜ã„é‡ã¿
)

cv_voting = cross_val_score(voting, X, y, cv=5,
                            scoring='neg_mean_absolute_error')

print(f&quot;\nVoting Ensemble MAE: {-cv_voting.mean():.4f}&quot;)

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã®æ¯”è¼ƒ
ensemble_comparison = pd.DataFrame({
    'Method': ['Single Best', 'Bagging', 'Boosting (LightGBM)',
               'Stacking', 'Voting'],
    'MAE': [
        min([cross_val_score(m, X, y, cv=5, scoring='neg_mean_absolute_error').mean() for _, m in base_models]),
        cv_bagging.mean(),
        boosting_df[boosting_df['Model'] == 'LightGBM']['MAE'].values[0],
        cv_stacking.mean(),
        cv_voting.mean()
    ]
})

ensemble_comparison['MAE'] = -ensemble_comparison['MAE']

plt.figure(figsize=(10, 6))
plt.barh(ensemble_comparison['Method'], ensemble_comparison['MAE'],
         color='coral', alpha=0.7)
plt.xlabel('MAE (lower is better)', fontsize=12)
plt.title('ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã®æ€§èƒ½æ¯”è¼ƒ', fontsize=13, fontweight='bold')
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>
<hr />
<h2>3.5 ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ï¼šLi-ioné›»æ± å®¹é‡äºˆæ¸¬</h2>
<p>å®Ÿéš›ã®Li-ioné›»æ± ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«é¸æŠã¨æœ€é©åŒ–ã®å…¨å·¥ç¨‹ã‚’å®Ÿè·µã—ã¾ã™ã€‚</p>
<pre><code class="language-python"># Li-ioné›»æ± å®¹é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
np.random.seed(42)
n_batteries = 300

battery_data = pd.DataFrame({
    'æ­£æ¥µææ–™çµ„æˆ_Li': np.random.uniform(0.9, 1.1, n_batteries),
    'æ­£æ¥µææ–™çµ„æˆ_Co': np.random.uniform(0, 0.6, n_batteries),
    'æ­£æ¥µææ–™çµ„æˆ_Ni': np.random.uniform(0, 0.8, n_batteries),
    'æ­£æ¥µææ–™çµ„æˆ_Mn': np.random.uniform(0, 0.4, n_batteries),
    'è² æ¥µææ–™_é»’é‰›å‰²åˆ': np.random.uniform(0.8, 1.0, n_batteries),
    'é›»è§£è³ªæ¿ƒåº¦': np.random.uniform(0.5, 2.0, n_batteries),
    'é›»æ¥µåšã•': np.random.uniform(50, 200, n_batteries),
    'ç²’å­ã‚µã‚¤ã‚º': np.random.uniform(1, 20, n_batteries),
    'ç„¼æˆæ¸©åº¦': np.random.uniform(700, 1000, n_batteries),
    'BETè¡¨é¢ç©': np.random.uniform(1, 50, n_batteries)
})

# å®¹é‡ï¼ˆçœŸã®é–¢ä¿‚ã¯è¤‡é›‘ãªéç·šå½¢ï¼‰
capacity = (
    150 * battery_data['æ­£æ¥µææ–™çµ„æˆ_Ni'] +
    120 * battery_data['æ­£æ¥µææ–™çµ„æˆ_Co'] +
    80 * battery_data['æ­£æ¥µææ–™çµ„æˆ_Mn'] +
    30 * battery_data['é›»è§£è³ªæ¿ƒåº¦'] -
    0.5 * battery_data['é›»æ¥µåšã•'] +
    2 * battery_data['BETè¡¨é¢ç©'] +
    0.1 * battery_data['ç„¼æˆæ¸©åº¦'] +
    20 * battery_data['æ­£æ¥µææ–™çµ„æˆ_Ni'] * battery_data['é›»è§£è³ªæ¿ƒåº¦'] +
    np.random.normal(0, 5, n_batteries)
)

battery_data['å®¹é‡_mAh/g'] = capacity

print(&quot;=== Li-ioné›»æ± å®¹é‡äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ===&quot;)
print(f&quot;ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(battery_data)}&quot;)
print(f&quot;ç‰¹å¾´é‡æ•°: {battery_data.shape[1] - 1}&quot;)
print(f&quot;\nå®¹é‡çµ±è¨ˆ:&quot;)
print(battery_data['å®¹é‡_mAh/g'].describe())

X_battery = battery_data.drop('å®¹é‡_mAh/g', axis=1)
y_battery = battery_data['å®¹é‡_mAh/g']
</code></pre>
<h3>Step 1: 5ã¤ã®ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ</h3>
<pre><code class="language-python">from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_battery, y_battery, test_size=0.2, random_state=42
)

# 5ã¤ã®ãƒ¢ãƒ‡ãƒ«
models_to_compare = {
    'Ridge': Ridge(alpha=1.0),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1),
    'Stacking': StackingRegressor(
        estimators=[
            ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),
            ('gb', GradientBoostingRegressor(n_estimators=100, random_state=42)),
            ('lgbm', lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1))
        ],
        final_estimator=Ridge(alpha=1.0),
        cv=5
    )
}

comparison_results = []

for model_name, model in models_to_compare.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    comparison_results.append({
        'Model': model_name,
        'MAE': mae,
        'RMSE': rmse,
        'RÂ²': r2
    })

comparison_df = pd.DataFrame(comparison_results)
print(&quot;\n=== Step 1: ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœ ===&quot;)
print(comparison_df.to_string(index=False))
</code></pre>
<h3>Step 2: Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è‡ªå‹•æœ€é©åŒ–</h3>
<pre><code class="language-python">def objective_lightgbm(trial, X, y):
    &quot;&quot;&quot;
    LightGBMãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
    &quot;&quot;&quot;
    param = {
        'objective': 'regression',
        'metric': 'mae',
        'verbosity': -1,
        'boosting_type': 'gbdt',
        'num_leaves': trial.suggest_int('num_leaves', 20, 100),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
        'random_state': 42
    }

    model = lgb.LGBMRegressor(**param)

    cv_scores = cross_val_score(
        model, X, y, cv=5, scoring='neg_mean_absolute_error'
    )

    return -cv_scores.mean()

# Optunaæœ€é©åŒ–
study_battery = optuna.create_study(direction='minimize')
study_battery.optimize(
    lambda trial: objective_lightgbm(trial, X_battery, y_battery),
    n_trials=100,
    show_progress_bar=True
)

print(&quot;\n=== Step 2: Optunaã«ã‚ˆã‚‹æœ€é©åŒ– ===&quot;)
print(f&quot;æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:&quot;)
for key, value in study_battery.best_params.items():
    print(f&quot;  {key}: {value}&quot;)
print(f&quot;\næœ€è‰¯MAE: {study_battery.best_value:.4f} mAh/g&quot;)

# æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§å†è©•ä¾¡
best_model = lgb.LGBMRegressor(**study_battery.best_params, random_state=42)
best_model.fit(X_train, y_train)
y_pred_best = best_model.predict(X_test)

mae_best = mean_absolute_error(y_test, y_pred_best)
r2_best = r2_score(y_test, y_pred_best)

print(f&quot;\næœ€é©åŒ–å¾Œã®æ€§èƒ½:&quot;)
print(f&quot;MAE: {mae_best:.4f} mAh/g&quot;)
print(f&quot;RÂ²: {r2_best:.4f}&quot;)
</code></pre>
<h3>Step 3: Stacking ensembleã§æœ€é«˜æ€§èƒ½é”æˆ</h3>
<pre><code class="language-python"># æœ€é©åŒ–ã•ã‚ŒãŸLightGBMã‚’å«ã‚€Stacking
optimized_stacking = StackingRegressor(
    estimators=[
        ('rf_tuned', RandomForestRegressor(
            n_estimators=200, max_depth=15, min_samples_split=5,
            random_state=42
        )),
        ('lgbm_tuned', lgb.LGBMRegressor(**study_battery.best_params, random_state=42)),
        ('gb_tuned', GradientBoostingRegressor(
            n_estimators=150, learning_rate=0.1, max_depth=7,
            random_state=42
        ))
    ],
    final_estimator=Ridge(alpha=0.5),
    cv=5
)

optimized_stacking.fit(X_train, y_train)
y_pred_stack = optimized_stacking.predict(X_test)

mae_stack = mean_absolute_error(y_test, y_pred_stack)
r2_stack = r2_score(y_test, y_pred_stack)

print(&quot;\n=== Step 3: æœ€çµ‚Stacking Ensemble ===&quot;)
print(f&quot;MAE: {mae_stack:.4f} mAh/g&quot;)
print(f&quot;RÂ²: {r2_stack:.4f}&quot;)

# å…¨å·¥ç¨‹ã®æ¯”è¼ƒ
final_comparison = pd.DataFrame({
    'Stage': [
        'Baseline (Ridge)',
        'Best Single Model',
        'Optuna Optimized',
        'Final Stacking'
    ],
    'MAE': [
        comparison_df[comparison_df['Model'] == 'Ridge']['MAE'].values[0],
        comparison_df['MAE'].min(),
        mae_best,
        mae_stack
    ],
    'RÂ²': [
        comparison_df[comparison_df['Model'] == 'Ridge']['RÂ²'].values[0],
        comparison_df['RÂ²'].max(),
        r2_best,
        r2_stack
    ]
})

print(&quot;\n=== å…¨å·¥ç¨‹ã®æ€§èƒ½æ¨ç§» ===&quot;)
print(final_comparison.to_string(index=False))

# äºˆæ¸¬ vs å®Ÿæ¸¬
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# æœ€é©åŒ–å‰ã®æœ€è‰¯ãƒ¢ãƒ‡ãƒ«
best_single_idx = comparison_df['MAE'].idxmin()
best_single_model = list(models_to_compare.values())[best_single_idx]
y_pred_single = best_single_model.predict(X_test)

axes[0].scatter(y_test, y_pred_single, c='steelblue', s=50, alpha=0.6)
axes[0].plot([y_test.min(), y_test.max()],
             [y_test.min(), y_test.max()],
             'r--', linewidth=2, label='Perfect')
axes[0].set_xlabel('Actual Capacity (mAh/g)', fontsize=11)
axes[0].set_ylabel('Predicted Capacity (mAh/g)', fontsize=11)
axes[0].set_title(f'æœ€è‰¯å˜ä¸€ãƒ¢ãƒ‡ãƒ« (MAE={comparison_df[&quot;MAE&quot;].min():.2f})',
                  fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# æœ€çµ‚Stacking
axes[1].scatter(y_test, y_pred_stack, c='coral', s=50, alpha=0.6)
axes[1].plot([y_test.min(), y_test.max()],
             [y_test.min(), y_test.max()],
             'r--', linewidth=2, label='Perfect')
axes[1].set_xlabel('Actual Capacity (mAh/g)', fontsize=11)
axes[1].set_ylabel('Predicted Capacity (mAh/g)', fontsize=11)
axes[1].set_title(f'æœ€çµ‚Stacking (MAE={mae_stack:.2f})',
                  fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

improvement = (comparison_df['MAE'].min() - mae_stack) / comparison_df['MAE'].min() * 100
print(f&quot;\næœ€çµ‚æ”¹å–„ç‡: {improvement:.1f}%&quot;)
</code></pre>
<hr />
<h2>æ¼”ç¿’å•é¡Œ</h2>
<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦: easyï¼‰</h3>
<p>K-Fold CVã¨Stratified K-Fold CVã‚’ç”¨ã„ã¦ã€ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>
<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python">from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier

# ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
X, y = make_classification(n_samples=200, n_features=20,
                          weights=[0.9, 0.1], random_state=42)

model = RandomForestClassifier(n_estimators=100, random_state=42)

# K-Fold
kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores_kfold = cross_val_score(model, X, y, cv=kf, scoring='f1')

# Stratified K-Fold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores_stratified = cross_val_score(model, X, y, cv=skf, scoring='f1')

print(f&quot;K-Fold F1: {scores_kfold.mean():.4f} Â± {scores_kfold.std():.4f}&quot;)
print(f&quot;Stratified K-Fold F1: {scores_stratified.mean():.4f} Â± {scores_stratified.std():.4f}&quot;)
</code></pre>


</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦: mediumï¼‰</h3>
<p>Optunaã‚’ç”¨ã„ã¦ã€Random Forestã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã—ã¦ãã ã•ã„ã€‚æ¢ç´¢ç©ºé–“ã¯<code>n_estimators</code>, <code>max_depth</code>, <code>min_samples_split</code>, <code>max_features</code>ã®4ã¤ã¨ã—ã¾ã™ã€‚</p>
<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python">import optuna
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score

def objective_rf(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'max_depth': trial.suggest_int('max_depth', 5, 30),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'max_features': trial.suggest_float('max_features', 0.3, 1.0),
        'random_state': 42
    }

    model = RandomForestRegressor(**params)
    cv_scores = cross_val_score(model, X, y, cv=5,
                                scoring='neg_mean_absolute_error')

    return -cv_scores.mean()

study = optuna.create_study(direction='minimize')
study.optimize(objective_rf, n_trials=50)

print(f&quot;æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {study.best_params}&quot;)
print(f&quot;æœ€è‰¯ã‚¹ã‚³ã‚¢: {study.best_value:.4f}&quot;)
</code></pre>


</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦: hardï¼‰</h3>
<p>Stacking Ensembleã‚’æ§‹ç¯‰ã—ã€3ã¤ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆRidge, Random Forest, LightGBMï¼‰ã¨2ã¤ã®ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼ˆRidge, Lassoï¼‰ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚ã©ã®ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ãŒæœ€è‰¯ã®æ€§èƒ½ã‚’ç¤ºã™ã‹è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚</p>
<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python">from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import Ridge, Lasso
import lightgbm as lgb

base_models = [
    ('ridge', Ridge(alpha=1.0)),
    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),
    ('lgbm', lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1))
]

meta_models = {
    'Ridge': Ridge(alpha=1.0),
    'Lasso': Lasso(alpha=0.1)
}

results = []

for meta_name, meta_model in meta_models.items():
    stacking = StackingRegressor(
        estimators=base_models,
        final_estimator=meta_model,
        cv=5
    )

    cv_scores = cross_val_score(stacking, X, y, cv=5,
                                scoring='neg_mean_absolute_error')
    mae = -cv_scores.mean()

    results.append({
        'Meta Model': meta_name,
        'MAE': mae
    })

results_df = pd.DataFrame(results)
print(results_df.to_string(index=False))
</code></pre>


</details>

<hr />
<h2>3.6 ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ç’°å¢ƒã¨å†ç¾æ€§</h2>
<h3>ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†</h3>
<pre><code class="language-python"># ãƒ¢ãƒ‡ãƒ«é¸æŠãƒ»æœ€é©åŒ–ã«å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒãƒ¼ã‚¸ãƒ§ãƒ³
import sys
import sklearn
import optuna
import lightgbm
import pandas as pd
import numpy as np

reproducibility_info = {
    'Python': sys.version,
    'NumPy': np.__version__,
    'Pandas': pd.__version__,
    'scikit-learn': sklearn.__version__,
    'Optuna': optuna.__version__,
    'LightGBM': lightgbm.__version__,
    'Date': '2025-10-19'
}

print(&quot;=== ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ç’°å¢ƒ ===&quot;)
for key, value in reproducibility_info.items():
    print(f&quot;{key}: {value}&quot;)

# æ¨å¥¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³
print(&quot;\nã€æ¨å¥¨ç’°å¢ƒã€‘&quot;)
recommended = &quot;&quot;&quot;
numpy==1.24.3
pandas==2.0.3
scikit-learn==1.3.0
optuna==3.3.0
lightgbm==4.0.0
xgboost==2.0.0  # Optional
matplotlib==3.7.2
seaborn==0.12.2
&quot;&quot;&quot;
print(recommended)

print(&quot;\nã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚³ãƒãƒ³ãƒ‰ã€‘&quot;)
print(&quot;```bash&quot;)
print(&quot;pip install numpy==1.24.3 pandas==2.0.3 scikit-learn==1.3.0&quot;)
print(&quot;pip install optuna==3.3.0 lightgbm==4.0.0&quot;)
print(&quot;```&quot;)

print(&quot;\nã€æ³¨æ„äº‹é …ã€‘&quot;)
print(&quot;âš ï¸ Optunaã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒç•°ãªã‚‹&quot;)
print(&quot;âš ï¸ LightGBM/XGBoostã¯OSä¾å­˜ã®å•é¡Œã‚ã‚Š â†’ åŒä¸€ç’°å¢ƒã§æ¤œè¨¼&quot;)
print(&quot;âš ï¸ è«–æ–‡å†ç¾æ™‚ã¯å¿…ãšãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æ˜è¨˜&quot;)
</code></pre>
<h3>ä¹±æ•°ã‚·ãƒ¼ãƒ‰ç®¡ç†</h3>
<pre><code class="language-python"># å†ç¾æ€§ç¢ºä¿ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰è¨­å®š
def set_all_seeds(seed=42):
    &quot;&quot;&quot;
    å…¨ä¹±æ•°ç”Ÿæˆå™¨ã®ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®š
    &quot;&quot;&quot;
    import random
    import numpy as np

    random.seed(seed)
    np.random.seed(seed)

    # scikit-learnãƒ¢ãƒ‡ãƒ«ã¯random_stateãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å€‹åˆ¥æŒ‡å®š
    print(f&quot;âœ… ä¹±æ•°ã‚·ãƒ¼ãƒ‰ {seed} ã‚’è¨­å®š&quot;)
    print(&quot;ãƒ¢ãƒ‡ãƒ«è¨“ç·´æ™‚ã¯ random_state={seed} ã‚’æ˜ç¤ºçš„ã«æŒ‡å®šã—ã¦ãã ã•ã„&quot;)

set_all_seeds(42)

# Optunaã®ã‚·ãƒ¼ãƒ‰ç®¡ç†
print(&quot;\nã€Optunaã®ã‚·ãƒ¼ãƒ‰ç®¡ç†ã€‘&quot;)
print(&quot;```python&quot;)
print(&quot;study = optuna.create_study(&quot;)
print(&quot;    direction='minimize',&quot;)
print(&quot;    sampler=optuna.samplers.TPESampler(seed=42)  # å†ç¾æ€§ç¢ºä¿&quot;)
print(&quot;)&quot;)
print(&quot;```&quot;)
</code></pre>
<h3>å®Ÿè·µçš„ãªè½ã¨ã—ç©´ï¼ˆãƒ¢ãƒ‡ãƒ«é¸æŠãƒ»æœ€é©åŒ–ç·¨ï¼‰</h3>
<pre><code class="language-python">print(&quot;=== ãƒ¢ãƒ‡ãƒ«é¸æŠãƒ»æœ€é©åŒ–ã®è½ã¨ã—ç©´ ===\n&quot;)

print(&quot;ã€è½ã¨ã—ç©´1: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã€‘&quot;)
print(&quot;âŒ æ‚ªã„ä¾‹ï¼šãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§æœ€é©åŒ–&quot;)
print(&quot;```python&quot;)
print(&quot;X_train, X_test = train_test_split(X, y)&quot;)
print(&quot;# ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã€æœ€è‰¯ã‚’é¸æŠ&quot;)
print(&quot;for model in models:&quot;)
print(&quot;    score = model.score(X_test, y_test)  # NGï¼&quot;)
print(&quot;best_model = models[best_idx]&quot;)
print(&quot;```&quot;)

print(&quot;\nâœ… æ­£ã—ã„ä¾‹ï¼šTrain/Validation/Teståˆ†å‰²&quot;)
print(&quot;```python&quot;)
print(&quot;X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3)&quot;)
print(&quot;X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)&quot;)
print(&quot;# Validationã§æœ€é©åŒ–ã€Testã¯æœ€çµ‚è©•ä¾¡ã®ã¿&quot;)
print(&quot;```&quot;)

print(&quot;\nã€è½ã¨ã—ç©´2: äº¤å·®æ¤œè¨¼æ™‚ã®ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã€‘&quot;)
print(&quot;âš ï¸ å‰å‡¦ç†ï¼ˆStandardScalerã€Imputerï¼‰ã‚’å…¨ãƒ‡ãƒ¼ã‚¿ã§å®Ÿè¡Œ&quot;)
print(&quot;â†’ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ãŒè¨“ç·´æ™‚ã«æ¼ã‚Œã‚‹&quot;)

print(&quot;\nâœ… å¯¾ç­–ï¼šPipelineã§å‰å‡¦ç†ã‚’å†…åŒ…&quot;)
print(&quot;```python&quot;)
print(&quot;from sklearn.pipeline import Pipeline&quot;)
print(&quot;from sklearn.preprocessing import StandardScaler&quot;)
print(&quot;&quot;)
print(&quot;pipeline = Pipeline([&quot;)
print(&quot;    ('scaler', StandardScaler()),&quot;)
print(&quot;    ('model', RandomForestRegressor())&quot;)
print(&quot;])&quot;)
print(&quot;cv_scores = cross_val_score(pipeline, X, y, cv=5)&quot;)
print(&quot;```&quot;)

print(&quot;\nã€è½ã¨ã—ç©´3: Optunaã§ã®éå‰°ãªè©¦è¡Œå›æ•°ã€‘&quot;)
print(&quot;âš ï¸ n_trials=1000ã§æœ€é©åŒ– â†’ éå­¦ç¿’ãƒªã‚¹ã‚¯&quot;)
print(&quot;â†’ Validationã‚»ãƒƒãƒˆã«éé©åˆ&quot;)

print(&quot;\nâœ… å¯¾ç­–ï¼šEarly Stopping + Testè©•ä¾¡&quot;)
print(&quot;```python&quot;)
print(&quot;study.optimize(objective, n_trials=100,&quot;)
print(&quot;    callbacks=[optuna.study.MaxTrialsCallback(100)])&quot;)
print(&quot;# æœ€é©ãƒ¢ãƒ‡ãƒ«ã‚’Testã‚»ãƒƒãƒˆã§æœ€çµ‚è©•ä¾¡&quot;)
print(&quot;```&quot;)

print(&quot;\nã€è½ã¨ã—ç©´4: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®éå‰°ãªè¤‡é›‘åŒ–ã€‘&quot;)
print(&quot;âš ï¸ Stacking + Voting + Blending ã‚’å¤šæ®µã«çµ„ã¿åˆã‚ã›&quot;)
print(&quot;â†’ è¨“ç·´æ™‚é–“å¢—å¤§ã€è§£é‡ˆæ€§å–ªå¤±ã€éå­¦ç¿’&quot;)

print(&quot;\nâœ… å¯¾ç­–ï¼šã‚·ãƒ³ãƒ—ãƒ«ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆ2-3ãƒ¢ãƒ‡ãƒ«ï¼‰&quot;)
print(&quot;```python&quot;)
print(&quot;# ã‚·ãƒ³ãƒ—ãƒ«ãªStackingï¼ˆãƒ™ãƒ¼ã‚¹3ãƒ¢ãƒ‡ãƒ« + ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«1ï¼‰&quot;)
print(&quot;stacking = StackingRegressor(&quot;)
print(&quot;    estimators=[('rf', RF), ('gb', GB), ('lgbm', LGBM)],&quot;)
print(&quot;    final_estimator=Ridge(),&quot;)
print(&quot;    cv=5&quot;)
print(&quot;)&quot;)
print(&quot;```&quot;)

print(&quot;\nã€è½ã¨ã—ç©´5: å°ãƒ‡ãƒ¼ã‚¿ã§ã®è¤‡é›‘ãƒ¢ãƒ‡ãƒ«ã€‘&quot;)
print(&quot;âš ï¸ ã‚µãƒ³ãƒ—ãƒ«æ•°50ã§Neural Networkè¨“ç·´&quot;)
print(&quot;â†’ éå­¦ç¿’ã€ä¸å®‰å®š&quot;)

print(&quot;\nâœ… å¯¾ç­–ï¼šãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«å¿œã˜ãŸãƒ¢ãƒ‡ãƒ«é¸æŠ&quot;)
print(&quot;```python&quot;)
print(&quot;if len(X) &lt; 100:&quot;)
print(&quot;    model = Ridge()  # ç·šå½¢ãƒ¢ãƒ‡ãƒ«&quot;)
print(&quot;elif len(X) &lt; 1000:&quot;)
print(&quot;    model = RandomForest()  # æœ¨ãƒ™ãƒ¼ã‚¹&quot;)
print(&quot;else:&quot;)
print(&quot;    model = NeuralNetwork()  # æ·±å±¤å­¦ç¿’&quot;)
print(&quot;```&quot;)
</code></pre>
<hr />
<h2>ã¾ã¨ã‚</h2>
<p>ã“ã®ç« ã§ã¯ã€<strong>ãƒ¢ãƒ‡ãƒ«é¸æŠã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</strong>ã‚’å­¦ã³ã¾ã—ãŸã€‚</p>
<p><strong>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</strong>ï¼š</p>
<ol>
<li><strong>ãƒ¢ãƒ‡ãƒ«é¸æŠ</strong>ï¼šãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«å¿œã˜ãŸé©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ï¼ˆå°: Ridgeã€ä¸­: RFã€å¤§: NNï¼‰</li>
<li><strong>äº¤å·®æ¤œè¨¼</strong>ï¼šK-Foldã€Stratifiedã€Time Seriesã‚’ä½¿ã„åˆ†ã‘</li>
<li><strong>æœ€é©åŒ–æ‰‹æ³•</strong>ï¼šGrid &lt; Random &lt; Bayesianï¼ˆOptunaï¼‰ã®é †ã§åŠ¹ç‡åŒ–</li>
<li><strong>ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«</strong>ï¼šStacking &gt; Voting &gt; Boosting &gt; Bagging</li>
<li><strong>å®Ÿè·µäº‹ä¾‹</strong>ï¼šLi-ioné›»æ± å®¹é‡äºˆæ¸¬ã§30%ä»¥ä¸Šã®æ€§èƒ½æ”¹å–„</li>
<li><strong>ç’°å¢ƒç®¡ç†</strong>ï¼šãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã®å³å¯†ãªç®¡ç†</li>
<li><strong>å®Ÿè·µçš„è½ã¨ã—ç©´</strong>ï¼šãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®èª¿æ•´ã€ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã€éå‰°æœ€é©åŒ–ã€å°ãƒ‡ãƒ¼ã‚¿ã§ã®è¤‡é›‘ãƒ¢ãƒ‡ãƒ«</li>
</ol>
<p><strong>æ¬¡ç« äºˆå‘Š</strong>ï¼š
Chapter 4ã§ã¯ã€è§£é‡ˆå¯èƒ½AIï¼ˆXAIï¼‰ã‚’å­¦ã³ã¾ã™ã€‚SHAPã€LIMEã€Attentionå¯è¦–åŒ–ã«ã‚ˆã‚Šã€äºˆæ¸¬ã®ç‰©ç†çš„æ„å‘³ã‚’ç†è§£ã—ã€å®Ÿä¸–ç•Œå¿œç”¨ã¨ã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹ã‚’æ¢ã‚Šã¾ã™ã€‚</p>
<hr />
<h2>Chapter 3 ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ</h2>
<h3>ãƒ¢ãƒ‡ãƒ«é¸æŠ</h3>
<ul>
<li>[ ] <strong>ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºè©•ä¾¡</strong></li>
<li>[ ] ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒ100æœªæº€ â†’ Ridge/Lassoï¼ˆæ­£å‰‡åŒ–ç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼‰</li>
<li>[ ] ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒ100-1000 â†’ Random Forest/Gradient Boosting</li>
<li>[ ] ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒ1000ä»¥ä¸Š â†’ Neural Network/Deep Learning</li>
<li>
<p>[ ] ã‚µãƒ³ãƒ—ãƒ«/ç‰¹å¾´é‡æ¯” &gt; 10:1 ã‚’ç¢ºèª</p>
</li>
<li>
<p>[ ] <strong>è§£é‡ˆæ€§ vs ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹</strong></p>
</li>
<li>[ ] è§£é‡ˆæ€§é‡è¦–ï¼ˆææ–™è¨­è¨ˆã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³æŠ½å‡ºï¼‰â†’ Ridge, Decision Tree</li>
<li>[ ] ç²¾åº¦é‡è¦–ï¼ˆäºˆæ¸¬æ€§èƒ½æœ€å¤§åŒ–ï¼‰â†’ Gradient Boosting, Stacking</li>
<li>
<p>[ ] ãƒãƒ©ãƒ³ã‚¹å‹ï¼ˆå®Ÿç”¨çš„å¦¥å”ç‚¹ï¼‰â†’ Random Forest</p>
</li>
<li>
<p>[ ] <strong>ãƒ¢ãƒ‡ãƒ«è¤‡é›‘åº¦ã®å¦¥å½“æ€§</strong></p>
</li>
<li>[ ] å­¦ç¿’æ›²ç·šã§éå­¦ç¿’ã‚’ç¢ºèªï¼ˆè¨“ç·´èª¤å·® &lt;&lt; æ¤œè¨¼èª¤å·®ï¼‰</li>
<li>[ ] æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆalpha, lambdaï¼‰ã‚’èª¿æ•´</li>
<li>[ ] Early Stoppingã§é©åˆ‡ãªã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°ã‚’è¨­å®š</li>
</ul>
<h3>äº¤å·®æ¤œè¨¼</h3>
<ul>
<li>[ ] <strong>K-Fold CVï¼ˆåŸºæœ¬ï¼‰</strong></li>
<li>[ ] å›å¸°å•é¡Œã§æ¨™æº–çš„ã«K=5ã‚’ä½¿ç”¨</li>
<li>[ ] è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ãŒã‚ã‚Œã°K=10ã§ç²¾åº¦å‘ä¸Š</li>
<li>
<p>[ ] shuffle=Trueã§é †åºåŠ¹æœã‚’æ’é™¤</p>
</li>
<li>
<p>[ ] <strong>Stratified K-Foldï¼ˆåˆ†é¡ãƒ»ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ï¼‰</strong></p>
</li>
<li>[ ] ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã§å¿…é ˆ</li>
<li>[ ] å„Foldã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒãŒå…¨ä½“ã¨ä¸€è‡´ã™ã‚‹ã‹ç¢ºèª</li>
<li>
<p>[ ] ãƒã‚¤ãƒãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«æ•° &gt; K ã‚’ç¢ºèª</p>
</li>
<li>
<p>[ ] <strong>Time Series Splitï¼ˆæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼‰</strong></p>
</li>
<li>[ ] é€æ¬¡çš„ãªææ–™å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã«é©ç”¨</li>
<li>[ ] è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒå¸¸ã«ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šéå»ã‹ç¢ºèª</li>
<li>
<p>[ ] æœªæ¥ãƒ‡ãƒ¼ã‚¿ã§ã®è¨“ç·´ã‚’çµ¶å¯¾ã«è¡Œã‚ãªã„ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰</p>
</li>
<li>
<p>[ ] <strong>Leave-One-Out CVï¼ˆå°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰</strong></p>
</li>
<li>[ ] ã‚µãƒ³ãƒ—ãƒ«æ•° &lt; 50 ã§ä½¿ç”¨</li>
<li>[ ] è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ï¼ˆnå›ã®è¨“ç·´ï¼‰ã‚’èªè­˜</li>
<li>[ ] éåº¦ã«æ¥½è¦³çš„ãªè©•ä¾¡ã«æ³¨æ„</li>
</ul>
<h3>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</h3>
<ul>
<li>[ ] <strong>Grid Search</strong></li>
<li>[ ] æ¢ç´¢ç©ºé–“ãŒå°ã•ã„ï¼ˆ&lt;100çµ„ã¿åˆã‚ã›ï¼‰æ™‚ã«ä½¿ç”¨</li>
<li>[ ] é‡è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿2-3å€‹ã«çµã‚‹</li>
<li>
<p>[ ] å…¨çµ„ã¿åˆã‚ã›ã‚’ç¶²ç¾…çš„ã«è©•ä¾¡</p>
</li>
<li>
<p>[ ] <strong>Random Search</strong></p>
</li>
<li>[ ] æ¢ç´¢ç©ºé–“ãŒå¤§ãã„æ™‚ã«ä½¿ç”¨</li>
<li>[ ] Grid Searchã®10-20%ã®è©¦è¡Œã§åŒç­‰æ€§èƒ½</li>
<li>
<p>[ ] é€£ç¶šå€¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„æ¢ç´¢</p>
</li>
<li>
<p>[ ] <strong>Bayesian Optimizationï¼ˆOptunaï¼‰</strong></p>
</li>
<li>[ ] æœ€ã‚‚åŠ¹ç‡çš„ï¼ˆæ¨å¥¨ï¼‰</li>
<li>[ ] n_trials=50-100ã§ååˆ†ãªæ€§èƒ½</li>
<li>[ ] ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡è¦åº¦åˆ†æã§è§£é‡ˆæ€§å‘ä¸Š</li>
<li>
<p>[ ] ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆTPESamplerï¼‰ã®ã‚·ãƒ¼ãƒ‰è¨­å®š</p>
</li>
<li>
<p>[ ] <strong>Early Stopping</strong></p>
</li>
<li>[ ] Boostingç³»ãƒ¢ãƒ‡ãƒ«ã§å¿…é ˆ</li>
<li>[ ] Validationèª¤å·®ã®æœ€å°ç‚¹ã§åœæ­¢</li>
<li>[ ] éå­¦ç¿’é˜²æ­¢ã¨è¨ˆç®—æ™‚é–“å‰Šæ¸›</li>
</ul>
<h3>ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’</h3>
<ul>
<li>[ ] <strong>Bagging</strong></li>
<li>[ ] Random Forestï¼ˆè‡ªå‹•çš„ã«Baggingï¼‰</li>
<li>[ ] é«˜åˆ†æ•£ãƒ¢ãƒ‡ãƒ«ã®å®‰å®šåŒ–</li>
<li>
<p>[ ] Decision Treeã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã§æ”¹å–„ç‡20-30%</p>
</li>
<li>
<p>[ ] <strong>Boosting</strong></p>
</li>
<li>[ ] Gradient Boosting, LightGBM, XGBoostã‚’è©¦è¡Œ</li>
<li>[ ] å­¦ç¿’ç‡ï¼ˆlearning_rateï¼‰ã¨n_estimatorsã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•</li>
<li>
<p>[ ] éå­¦ç¿’ãƒªã‚¹ã‚¯ â†’ Early Stoppingä½µç”¨</p>
</li>
<li>
<p>[ ] <strong>Stacking</strong></p>
</li>
<li>[ ] ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«3-5å€‹ï¼ˆå¤šæ§˜æ€§é‡è¦–ï¼‰</li>
<li>[ ] ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã¯ã‚·ãƒ³ãƒ—ãƒ«ï¼ˆRidge, Lassoï¼‰</li>
<li>[ ] äº¤å·®æ¤œè¨¼ï¼ˆcv=5ï¼‰ã§ãƒ¡ã‚¿ç‰¹å¾´é‡ç”Ÿæˆ</li>
<li>
<p>[ ] æœ€è‰¯å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Š5-10%æ”¹å–„ã‚’ç›®æ¨™</p>
</li>
<li>
<p>[ ] <strong>Voting</strong></p>
</li>
<li>[ ] Stackingã‚ˆã‚Šç°¡æ˜“</li>
<li>[ ] é‡ã¿ä»˜ãå¹³å‡ï¼ˆæ€§èƒ½ã«å¿œã˜ãŸé‡ã¿è¨­å®šï¼‰</li>
<li>[ ] 3ãƒ¢ãƒ‡ãƒ«ç¨‹åº¦ã§ååˆ†</li>
</ul>
<h3>å®Ÿè·µçš„è½ã¨ã—ç©´ã®å›é¿</h3>
<ul>
<li>[ ] <strong>ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®èª¿æ•´ç¦æ­¢</strong></li>
<li>[ ] Train/Validation/Test ã®3åˆ†å‰²</li>
<li>[ ] Validationã§æœ€é©åŒ–ã€Testã¯æœ€çµ‚è©•ä¾¡ã®ã¿</li>
<li>
<p>[ ] ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã¯1åº¦ã ã‘ä½¿ç”¨</p>
</li>
<li>
<p>[ ] <strong>äº¤å·®æ¤œè¨¼æ™‚ã®ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢</strong></p>
</li>
<li>[ ] Pipelineã§å‰å‡¦ç†ã‚’å†…åŒ…</li>
<li>[ ] å„Foldã§ç‹¬ç«‹ã«å‰å‡¦ç†ï¼ˆStandardScaler.fitï¼‰</li>
<li>
<p>[ ] ç‰¹å¾´é‡é¸æŠã‚‚äº¤å·®æ¤œè¨¼å†…ã§å®Ÿè¡Œ</p>
</li>
<li>
<p>[ ] <strong>éå‰°æœ€é©åŒ–ã®å›é¿</strong></p>
</li>
<li>[ ] Optunaã®n_trials &lt; 200ï¼ˆæ¨å¥¨100ï¼‰</li>
<li>[ ] Validationæ€§èƒ½ã¨Testæ€§èƒ½ã®ä¹–é›¢ã‚’ç›£è¦–</li>
<li>
<p>[ ] ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«ã‹ã‚‰é–‹å§‹</p>
</li>
<li>
<p>[ ] <strong>ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®è¤‡é›‘åŒ–å›é¿</strong></p>
</li>
<li>[ ] Stacking 1æ®µã¾ã§ï¼ˆå¤šæ®µã¯ç¦æ­¢ï¼‰</li>
<li>[ ] ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«æ•° â‰¤ 5</li>
<li>
<p>[ ] è¨“ç·´æ™‚é–“ã¨æ€§èƒ½ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’è©•ä¾¡</p>
</li>
<li>
<p>[ ] <strong>å°ãƒ‡ãƒ¼ã‚¿ã§ã®è¤‡é›‘ãƒ¢ãƒ‡ãƒ«å›é¿</strong></p>
</li>
<li>[ ] ã‚µãƒ³ãƒ—ãƒ«æ•° &lt; 100 â†’ ç·šå½¢ãƒ¢ãƒ‡ãƒ«</li>
<li>[ ] ã‚µãƒ³ãƒ—ãƒ«æ•° &lt; 1000 â†’ æœ¨ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«</li>
<li>[ ] æ·±å±¤å­¦ç¿’ã¯å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼ˆ&gt;1000ï¼‰ã®ã¿</li>
</ul>
<h3>å†ç¾æ€§ã®ç¢ºä¿</h3>
<ul>
<li>[ ] <strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†</strong></li>
<li>[ ] scikit-learn, Optuna, LightGBMã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³è¨˜éŒ²</li>
<li>[ ] requirements.txtä½œæˆ</li>
<li>
<p>[ ] Dockerç’°å¢ƒã§çµ±ä¸€ï¼ˆæ¨å¥¨ï¼‰</p>
</li>
<li>
<p>[ ] <strong>ä¹±æ•°ã‚·ãƒ¼ãƒ‰è¨­å®š</strong></p>
</li>
<li>[ ] NumPy: np.random.seed(42)</li>
<li>[ ] scikit-learn: random_state=42</li>
<li>[ ] Optuna: TPESampler(seed=42)</li>
<li>
<p>[ ] ã™ã¹ã¦ã®ãƒ©ãƒ³ãƒ€ãƒ è¦ç´ ã«åŒä¸€ã‚·ãƒ¼ãƒ‰ä½¿ç”¨</p>
</li>
<li>
<p>[ ] <strong>æœ€é©åŒ–å±¥æ­´ã®ä¿å­˜</strong></p>
</li>
<li>[ ] Optunaã®study.trials_dataframe()ã‚’CSVä¿å­˜</li>
<li>[ ] æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’JSONä¿å­˜</li>
<li>
<p>[ ] å­¦ç¿’æ›²ç·šã‚’ç”»åƒä¿å­˜</p>
</li>
<li>
<p>[ ] <strong>ãƒ¢ãƒ‡ãƒ«ã®æ°¸ç¶šåŒ–</strong></p>
</li>
<li>[ ] joblib.dump()ã§è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¿å­˜</li>
<li>[ ] äºˆæ¸¬æ™‚ã«åŒã˜å‰å‡¦ç†ã‚’é©ç”¨</li>
<li>[ ] ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ï¼ˆGit LFSæ¨å¥¨ï¼‰</li>
</ul>
<h3>Li-ioné›»æ± å®¹é‡äºˆæ¸¬ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£</h3>
<ul>
<li>[ ] <strong>ãƒ‡ãƒ¼ã‚¿æº–å‚™</strong></li>
<li>[ ] çµ„æˆãƒ»æ§‹é€ ãƒ»ãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶ã®ç‰¹å¾´é‡ç”Ÿæˆ</li>
<li>[ ] Train/Teståˆ†å‰²ï¼ˆ80/20ï¼‰</li>
<li>
<p>[ ] æ¬ æå€¤ãƒ»å¤–ã‚Œå€¤ã®äº‹å‰å‡¦ç†</p>
</li>
<li>
<p>[ ] <strong>Step 1: ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ</strong></p>
</li>
<li>[ ] 5ã¤ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆRidge, RF, GB, LGBM, Stackingï¼‰ã‚’è©•ä¾¡</li>
<li>[ ] MAE, RMSE, RÂ²ã§æ€§èƒ½æ¯”è¼ƒ</li>
<li>
<p>[ ] æœ€è‰¯å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®š</p>
</li>
<li>
<p>[ ] <strong>Step 2: Optunaæœ€é©åŒ–</strong></p>
</li>
<li>[ ] LightGBMã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿8å€‹ã‚’æœ€é©åŒ–</li>
<li>[ ] n_trials=100ã§å®Ÿè¡Œ</li>
<li>
<p>[ ] ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡è¦åº¦ã‚’åˆ†æ</p>
</li>
<li>
<p>[ ] <strong>Step 3: æœ€çµ‚Stacking</strong></p>
</li>
<li>[ ] æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«3å€‹ã‚’ãƒ™ãƒ¼ã‚¹ã«ä½¿ç”¨</li>
<li>[ ] Ridgeã‚’ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã«é¸æŠ</li>
<li>[ ] æœ€çµ‚æ€§èƒ½ãŒBaselineï¼ˆRidgeï¼‰ã‚ˆã‚Š30%ä»¥ä¸Šæ”¹å–„</li>
</ul>
<h3>ãƒ¢ãƒ‡ãƒ«é¸æŠãƒ»æœ€é©åŒ–å“è³ªæŒ‡æ¨™</h3>
<ul>
<li>[ ] <strong>äºˆæ¸¬ç²¾åº¦</strong></li>
<li>[ ] MAEï¼ˆå›å¸°ï¼‰&lt; ãƒ‡ãƒ¼ã‚¿æ¨™æº–åå·®ã®20%</li>
<li>[ ] RÂ² &gt; 0.8ï¼ˆææ–™ç§‘å­¦ã§ã¯0.7ä»¥ä¸Šã§è‰¯å¥½ï¼‰</li>
<li>
<p>[ ] RMSEç¢ºèªï¼ˆå¤–ã‚Œå€¤ã®å½±éŸ¿è©•ä¾¡ï¼‰</p>
</li>
<li>
<p>[ ] <strong>æ±åŒ–æ€§èƒ½</strong></p>
</li>
<li>[ ] è¨“ç·´èª¤å·®ã¨ãƒ†ã‚¹ãƒˆèª¤å·®ã®å·® &lt; 10%</li>
<li>[ ] äº¤å·®æ¤œè¨¼ã®æ¨™æº–åå·®å°ï¼ˆå®‰å®šæ€§ï¼‰</li>
<li>
<p>[ ] æ–°è¦ãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½ç¶­æŒç¢ºèª</p>
</li>
<li>
<p>[ ] <strong>è¨ˆç®—åŠ¹ç‡</strong></p>
</li>
<li>[ ] è¨“ç·´æ™‚é–“ &lt; 1æ™‚é–“ï¼ˆä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰</li>
<li>[ ] äºˆæ¸¬æ™‚é–“ &lt; 1ç§’/ã‚µãƒ³ãƒ—ãƒ«</li>
<li>
<p>[ ] Optunaã®æœ€é©åŒ–æ™‚é–“ &lt; 10åˆ†ï¼ˆn_trials=100ï¼‰</p>
</li>
<li>
<p>[ ] <strong>è§£é‡ˆæ€§</strong></p>
</li>
<li>[ ] feature_importances_ã§é‡è¦å¤‰æ•°ç‰¹å®š</li>
<li>[ ] SHAPå€¤ï¼ˆæ¬¡ç« ï¼‰ã§ç‰©ç†çš„æ„å‘³ã¥ã‘</li>
<li>[ ] å°‚é–€å®¶æ¤œè¨¼ã§å¦¥å½“æ€§ç¢ºèª</li>
</ul>
<hr />
<h2>å‚è€ƒæ–‡çŒ®</h2>
<ol>
<li>
<p><strong>Akiba, T., Sano, S., Yanase, T., et al.</strong> (2019). Optuna: A Next-generation Hyperparameter Optimization Framework. <em>Proceedings of the 25th ACM SIGKDD</em>, 2623-2631. <a href="https://doi.org/10.1145/3292500.3330701">DOI: 10.1145/3292500.3330701</a></p>
</li>
<li>
<p><strong>Bergstra, J. &amp; Bengio, Y.</strong> (2012). Random search for hyper-parameter optimization. <em>Journal of Machine Learning Research</em>, 13, 281-305.</p>
</li>
<li>
<p><strong>Dietterich, T. G.</strong> (2000). Ensemble methods in machine learning. <em>International Workshop on Multiple Classifier Systems</em>, 1-15. Springer.</p>
</li>
<li>
<p><strong>Wolpert, D. H.</strong> (1992). Stacked generalization. <em>Neural Networks</em>, 5(2), 241-259. <a href="https://doi.org/10.1016/S0893-6080(05)80023-1">DOI: 10.1016/S0893-6080(05)80023-1</a></p>
</li>
</ol>
<hr />
<p><a href="chapter-2.html">â† Chapter 2ã«æˆ»ã‚‹</a> | <a href="chapter-4.html">Chapter 4ã¸é€²ã‚€ â†’</a></p><div class="navigation">
    <a href="chapter-2.html" class="nav-button">â† å‰ã®ç« </a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter-4.html" class="nav-button">æ¬¡ã®ç«  â†’</a>
</div>
    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
