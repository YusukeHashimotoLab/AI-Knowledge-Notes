<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
<meta content="Chapter - AI Terakoya" name="description"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/MI/data-driven-materials-introduction/chapter-2.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../MI/index.html">ãƒãƒ†ãƒªã‚¢ãƒ«ã‚ºãƒ»ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹</a><span class="breadcrumb-separator">â€º</span><a href="../../MI/data-driven-materials-introduction/index.html">Data Driven Materials</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 2</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 20-25åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 0å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 0å•</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 2: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</h1>
<hr />
<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<p>âœ… ææ–™è¨˜è¿°å­ï¼ˆçµ„æˆãƒ»æ§‹é€ ãƒ»é›»å­æ§‹é€ ï¼‰ã®é¸æŠã¨è¨­è¨ˆ
âœ… matminerã‚’æ´»ç”¨ã—ãŸææ–™ç‰¹å¾´é‡ã®è‡ªå‹•ç”Ÿæˆ
âœ… ç‰¹å¾´é‡å¤‰æ›ï¼ˆæ­£è¦åŒ–ã€å¯¾æ•°å¤‰æ›ã€å¤šé …å¼ç‰¹å¾´é‡ï¼‰ã®å®Ÿè·µ
âœ… æ¬¡å…ƒå‰Šæ¸›ï¼ˆPCAã€t-SNEã€UMAPï¼‰ã«ã‚ˆã‚‹å¯è¦–åŒ–ã¨è§£é‡ˆ
âœ… ç‰¹å¾´é‡é¸æŠï¼ˆFilter/Wrapper/Embedded/SHAP-basedï¼‰ã®ä½¿ã„åˆ†ã‘
âœ… ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬ã«ãŠã‘ã‚‹200æ¬¡å…ƒâ†’20æ¬¡å…ƒã¸ã®åŠ¹æœçš„å‰Šæ¸›</p>
<hr />
<h2>2.1 ææ–™è¨˜è¿°å­ã®é¸æŠã¨è¨­è¨ˆ</h2>
<p>ææ–™ã®æ€§è³ªã‚’æ©Ÿæ¢°å­¦ç¿’ã§äºˆæ¸¬ã™ã‚‹ã«ã¯ã€é©åˆ‡ãª<strong>ææ–™è¨˜è¿°å­ï¼ˆMaterial Descriptorsï¼‰</strong>ãŒå¿…è¦ã§ã™ã€‚</p>
<h3>çµ„æˆè¨˜è¿°å­</h3>
<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def calculate_composition_descriptors(formula_dict):
    &quot;&quot;&quot;
    çµ„æˆè¨˜è¿°å­ã®è¨ˆç®—

    Parameters:
    -----------
    formula_dict : dict
        {'å…ƒç´ è¨˜å·': å‰²åˆ} ä¾‹: {'Fe': 0.7, 'Ni': 0.3}

    Returns:
    --------
    dict : çµ„æˆè¨˜è¿°å­
    &quot;&quot;&quot;
    # å…ƒç´ ã®ç‰©æ€§å€¤ï¼ˆç°¡ç•¥ç‰ˆï¼‰
    element_properties = {
        'Fe': {'atomic_mass': 55.845, 'electronegativity': 1.83,
               'atomic_radius': 1.26},
        'Ni': {'atomic_mass': 58.693, 'electronegativity': 1.91,
               'atomic_radius': 1.24},
        'Cu': {'atomic_mass': 63.546, 'electronegativity': 1.90,
               'atomic_radius': 1.28},
        'Zn': {'atomic_mass': 65.38, 'electronegativity': 1.65,
               'atomic_radius': 1.34}
    }

    descriptors = {}

    # å¹³å‡åŸå­é‡
    avg_mass = sum(
        element_properties[el]['atomic_mass'] * frac
        for el, frac in formula_dict.items()
    )
    descriptors['å¹³å‡åŸå­é‡'] = avg_mass

    # å¹³å‡é›»æ°—é™°æ€§åº¦
    avg_electronegativity = sum(
        element_properties[el]['electronegativity'] * frac
        for el, frac in formula_dict.items()
    )
    descriptors['å¹³å‡é›»æ°—é™°æ€§åº¦'] = avg_electronegativity

    # é›»æ°—é™°æ€§åº¦å·®ï¼ˆæœ€å¤§ - æœ€å°ï¼‰
    electronegativities = [
        element_properties[el]['electronegativity']
        for el in formula_dict.keys()
    ]
    descriptors['é›»æ°—é™°æ€§åº¦å·®'] = max(electronegativities) - min(electronegativities)

    # å¹³å‡åŸå­åŠå¾„
    avg_radius = sum(
        element_properties[el]['atomic_radius'] * frac
        for el, frac in formula_dict.items()
    )
    descriptors['å¹³å‡åŸå­åŠå¾„'] = avg_radius

    return descriptors

# ä¾‹ï¼šFe-Niåˆé‡‘
formula = {'Fe': 0.7, 'Ni': 0.3}
descriptors = calculate_composition_descriptors(formula)

print(&quot;çµ„æˆè¨˜è¿°å­ï¼ˆFeâ‚€.â‚‡Niâ‚€.â‚ƒï¼‰ï¼š&quot;)
for key, value in descriptors.items():
    print(f&quot;  {key}: {value:.4f}&quot;)
</code></pre>
<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>çµ„æˆè¨˜è¿°å­ï¼ˆFeâ‚€.â‚‡Niâ‚€.â‚ƒï¼‰ï¼š
  å¹³å‡åŸå­é‡: 56.6984
  å¹³å‡é›»æ°—é™°æ€§åº¦: 1.8540
  é›»æ°—é™°æ€§åº¦å·®: 0.0800
  å¹³å‡åŸå­åŠå¾„: 1.2540
</code></pre>
<h3>matminerã®æ´»ç”¨</h3>
<pre><code class="language-python"># matminerã«ã‚ˆã‚‹ææ–™è¨˜è¿°å­ã®è‡ªå‹•ç”Ÿæˆ
# !pip install matminer pymatgen

from matminer.featurizers.composition import (
    ElementProperty,
    Stoichiometry,
    ValenceOrbital,
    IonProperty
)
from pymatgen.core import Composition

def generate_matminer_features(formula_str):
    &quot;&quot;&quot;
    matminerã§ææ–™è¨˜è¿°å­ã‚’ç”Ÿæˆ

    Parameters:
    -----------
    formula_str : str
        åŒ–å­¦å¼ï¼ˆä¾‹: &quot;Fe2O3&quot;ï¼‰

    Returns:
    --------
    pd.DataFrame : ç‰¹å¾´é‡
    &quot;&quot;&quot;
    comp = Composition(formula_str)

    # å…ƒç´ ç‰©æ€§è¨˜è¿°å­
    ep_feat = ElementProperty.from_preset(&quot;magpie&quot;)
    features_ep = ep_feat.featurize(comp)

    # åŒ–å­¦é‡è«–è¨˜è¿°å­
    stoich_feat = Stoichiometry()
    features_stoich = stoich_feat.featurize(comp)

    # ä¾¡é›»å­è»Œé“è¨˜è¿°å­
    valence_feat = ValenceOrbital()
    features_valence = valence_feat.featurize(comp)

    # ç‰¹å¾´é‡åå–å¾—
    feature_names = (
        ep_feat.feature_labels() +
        stoich_feat.feature_labels() +
        valence_feat.feature_labels()
    )

    # DataFrameã«å¤‰æ›
    all_features = features_ep + features_stoich + features_valence
    df = pd.DataFrame([all_features], columns=feature_names)

    return df

# ä¾‹ï¼šé…¸åŒ–é‰„
formula = &quot;Fe2O3&quot;
features = generate_matminer_features(formula)

print(f&quot;matminerã«ã‚ˆã‚‹ç‰¹å¾´é‡ç”Ÿæˆï¼ˆ{formula}ï¼‰ï¼š&quot;)
print(f&quot;ç‰¹å¾´é‡æ•°: {features.shape[1]}&quot;)
print(f&quot;\næœ€åˆã®10ç‰¹å¾´é‡ï¼š&quot;)
print(features.iloc[:, :10].T)
</code></pre>
<p><strong>matminerã®ä¸»ãªè¨˜è¿°å­</strong>ï¼š</p>
<pre><code class="language-python"># è¨˜è¿°å­ã‚¿ã‚¤ãƒ—ã®æ¯”è¼ƒ
descriptor_types = pd.DataFrame({
    'è¨˜è¿°å­ã‚¿ã‚¤ãƒ—': [
        'ElementProperty',
        'Stoichiometry',
        'ValenceOrbital',
        'IonProperty',
        'OxidationStates',
        'ElectronAffinity'
    ],
    'ç‰¹å¾´é‡æ•°': [132, 7, 10, 32, 3, 1],
    'ç”¨é€”': [
        'å…ƒç´ ã®ç‰©ç†åŒ–å­¦çš„æ€§è³ª',
        'åŒ–å­¦é‡è«–æ¯”',
        'ä¾¡é›»å­è»Œé“',
        'ã‚¤ã‚ªãƒ³ç‰¹æ€§',
        'é…¸åŒ–çŠ¶æ…‹',
        'é›»å­è¦ªå’ŒåŠ›'
    ]
})

# å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(10, 6))
ax.barh(descriptor_types['è¨˜è¿°å­ã‚¿ã‚¤ãƒ—'],
        descriptor_types['ç‰¹å¾´é‡æ•°'],
        color='steelblue', alpha=0.7)
ax.set_xlabel('ç‰¹å¾´é‡æ•°', fontsize=12)
ax.set_title('matminerã®è¨˜è¿°å­ã‚¿ã‚¤ãƒ—', fontsize=13, fontweight='bold')
ax.grid(axis='x', alpha=0.3)

for idx, row in descriptor_types.iterrows():
    ax.text(row['ç‰¹å¾´é‡æ•°'] + 5, idx, row['ç”¨é€”'],
            va='center', fontsize=9, style='italic')

plt.tight_layout()
plt.show()

print(descriptor_types.to_string(index=False))
</code></pre>
<h3>æ§‹é€ è¨˜è¿°å­</h3>
<pre><code class="language-python">def calculate_structure_descriptors(lattice_params):
    &quot;&quot;&quot;
    çµæ™¶æ§‹é€ è¨˜è¿°å­

    Parameters:
    -----------
    lattice_params : dict
        {'a': float, 'b': float, 'c': float,
         'alpha': float, 'beta': float, 'gamma': float}

    Returns:
    --------
    dict : æ§‹é€ è¨˜è¿°å­
    &quot;&quot;&quot;
    a = lattice_params['a']
    b = lattice_params['b']
    c = lattice_params['c']
    alpha = np.radians(lattice_params['alpha'])
    beta = np.radians(lattice_params['beta'])
    gamma = np.radians(lattice_params['gamma'])

    # ä½“ç©
    volume = a * b * c * np.sqrt(
        1 - np.cos(alpha)**2 - np.cos(beta)**2 - np.cos(gamma)**2 +
        2 * np.cos(alpha) * np.cos(beta) * np.cos(gamma)
    )

    # ãƒ‘ãƒƒã‚­ãƒ³ã‚°å¯†åº¦ï¼ˆç°¡ç•¥åŒ–ï¼‰
    packing_density = 0.74  # ä¾‹ï¼šFCCã®å ´åˆ

    descriptors = {
        'æ ¼å­å®šæ•°a': a,
        'æ ¼å­å®šæ•°b': b,
        'æ ¼å­å®šæ•°c': c,
        'ä½“ç©': volume,
        'ãƒ‘ãƒƒã‚­ãƒ³ã‚°å¯†åº¦': packing_density
    }

    return descriptors

# ä¾‹ï¼šç«‹æ–¹æ™¶
lattice = {'a': 5.43, 'b': 5.43, 'c': 5.43,
           'alpha': 90, 'beta': 90, 'gamma': 90}
struct_desc = calculate_structure_descriptors(lattice)

print(&quot;æ§‹é€ è¨˜è¿°å­ï¼ˆç«‹æ–¹æ™¶ï¼‰ï¼š&quot;)
for key, value in struct_desc.items():
    print(f&quot;  {key}: {value:.4f}&quot;)
</code></pre>
<h3>é›»å­æ§‹é€ è¨˜è¿°å­</h3>
<pre><code class="language-python"># é›»å­æ§‹é€ è¨˜è¿°å­ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
def simulate_electronic_descriptors(n_samples=100):
    &quot;&quot;&quot;
    é›»å­æ§‹é€ è¨˜è¿°å­ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    &quot;&quot;&quot;
    np.random.seed(42)

    data = pd.DataFrame({
        'ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—': np.random.uniform(0, 5, n_samples),
        'ãƒ•ã‚§ãƒ«ãƒŸã‚¨ãƒãƒ«ã‚®ãƒ¼': np.random.uniform(-5, 5, n_samples),
        'çŠ¶æ…‹å¯†åº¦_ä¾¡é›»å­å¸¯': np.random.uniform(10, 100, n_samples),
        'çŠ¶æ…‹å¯†åº¦_ä¼å°å¸¯': np.random.uniform(5, 50, n_samples),
        'æœ‰åŠ¹è³ªé‡_é›»å­': np.random.uniform(0.1, 2, n_samples),
        'æœ‰åŠ¹è³ªé‡_æ­£å­”': np.random.uniform(0.1, 2, n_samples)
    })

    return data

# ç”Ÿæˆ
electronic_data = simulate_electronic_descriptors(100)

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

for idx, col in enumerate(electronic_data.columns):
    axes[idx].hist(electronic_data[col], bins=20,
                   color='steelblue', alpha=0.7, edgecolor='black')
    axes[idx].set_xlabel(col, fontsize=11)
    axes[idx].set_ylabel('é »åº¦', fontsize=11)
    axes[idx].set_title(f'{col}ã®åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    axes[idx].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;é›»å­æ§‹é€ è¨˜è¿°å­ã®çµ±è¨ˆï¼š&quot;)
print(electronic_data.describe())
</code></pre>
<hr />
<h2>2.2 ç‰¹å¾´é‡å¤‰æ›</h2>
<p>ç”Ÿã®ç‰¹å¾´é‡ã‚’æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«é©ã—ãŸå½¢ã«å¤‰æ›ã—ã¾ã™ã€‚</p>
<h3>æ­£è¦åŒ–ï¼ˆMin-Max, Z-scoreï¼‰</h3>
<pre><code class="language-python">from sklearn.preprocessing import MinMaxScaler, StandardScaler

def compare_normalization(data):
    &quot;&quot;&quot;
    æ­£è¦åŒ–æ‰‹æ³•ã®æ¯”è¼ƒ
    &quot;&quot;&quot;
    # Min-Maxæ­£è¦åŒ–ï¼ˆ0-1ï¼‰
    minmax_scaler = MinMaxScaler()
    data_minmax = pd.DataFrame(
        minmax_scaler.fit_transform(data),
        columns=data.columns
    )

    # Z-scoreæ­£è¦åŒ–ï¼ˆå¹³å‡0ã€æ¨™æº–åå·®1ï¼‰
    standard_scaler = StandardScaler()
    data_standard = pd.DataFrame(
        standard_scaler.fit_transform(data),
        columns=data.columns
    )

    return data_minmax, data_standard

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
np.random.seed(42)
sample_data = pd.DataFrame({
    'æ ¼å­å®šæ•°': np.random.uniform(3, 7, 100),
    'é›»æ°—ä¼å°åº¦': np.random.lognormal(10, 2, 100),
    'ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—': np.random.uniform(0, 3, 100)
})

# æ­£è¦åŒ–
data_minmax, data_standard = compare_normalization(sample_data)

# å¯è¦–åŒ–
fig, axes = plt.subplots(3, 3, figsize=(15, 12))

for idx, col in enumerate(sample_data.columns):
    # å…ƒãƒ‡ãƒ¼ã‚¿
    axes[idx, 0].hist(sample_data[col], bins=20,
                      color='gray', alpha=0.7, edgecolor='black')
    axes[idx, 0].set_title(f'å…ƒãƒ‡ãƒ¼ã‚¿: {col}', fontsize=11, fontweight='bold')
    axes[idx, 0].set_ylabel('é »åº¦', fontsize=10)

    # Min-Max
    axes[idx, 1].hist(data_minmax[col], bins=20,
                      color='steelblue', alpha=0.7, edgecolor='black')
    axes[idx, 1].set_title(f'Min-Max: {col}', fontsize=11, fontweight='bold')

    # Z-score
    axes[idx, 2].hist(data_standard[col], bins=20,
                      color='coral', alpha=0.7, edgecolor='black')
    axes[idx, 2].set_title(f'Z-score: {col}', fontsize=11, fontweight='bold')

plt.tight_layout()
plt.show()

print(&quot;æ­£è¦åŒ–å¾Œã®çµ±è¨ˆï¼š&quot;)
print(&quot;\nMin-Maxæ­£è¦åŒ–ï¼š&quot;)
print(data_minmax.describe())
print(&quot;\nZ-scoreæ­£è¦åŒ–ï¼š&quot;)
print(data_standard.describe())
</code></pre>
<h3>å¯¾æ•°å¤‰æ›ã€Box-Coxå¤‰æ›</h3>
<pre><code class="language-python">from scipy.stats import boxcox

def apply_transformations(data):
    &quot;&quot;&quot;
    å„ç¨®å¤‰æ›ã®é©ç”¨
    &quot;&quot;&quot;
    # å¯¾æ•°å¤‰æ›
    data_log = np.log1p(data)  # log(1+x)ã§0ã‚’æ‰±ãˆã‚‹

    # Box-Coxå¤‰æ›ï¼ˆæ­£å€¤ã®ã¿ï¼‰
    data_boxcox, lambda_param = boxcox(data + 1)  # +1ã§ã‚¼ãƒ­ã‚’å›é¿

    return data_log, data_boxcox, lambda_param

# åã£ãŸãƒ‡ãƒ¼ã‚¿ï¼ˆé›»æ°—ä¼å°åº¦ãªã©ï¼‰
np.random.seed(42)
conductivity = np.random.lognormal(10, 2, 100)

# å¤‰æ›
cond_log, cond_boxcox, lambda_val = apply_transformations(conductivity)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# å…ƒãƒ‡ãƒ¼ã‚¿
axes[0].hist(conductivity, bins=30, color='gray',
             alpha=0.7, edgecolor='black')
axes[0].set_xlabel('é›»æ°—ä¼å°åº¦ (S/m)', fontsize=11)
axes[0].set_ylabel('é »åº¦', fontsize=11)
axes[0].set_title('å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆæ­ªåº¦ã‚ã‚Šï¼‰', fontsize=12, fontweight='bold')

# å¯¾æ•°å¤‰æ›
axes[1].hist(cond_log, bins=30, color='steelblue',
             alpha=0.7, edgecolor='black')
axes[1].set_xlabel('log(é›»æ°—ä¼å°åº¦+1)', fontsize=11)
axes[1].set_ylabel('é »åº¦', fontsize=11)
axes[1].set_title('å¯¾æ•°å¤‰æ›', fontsize=12, fontweight='bold')

# Box-Coxå¤‰æ›
axes[2].hist(cond_boxcox, bins=30, color='coral',
             alpha=0.7, edgecolor='black')
axes[2].set_xlabel(f'Box-Cox (Î»={lambda_val:.3f})', fontsize=11)
axes[2].set_ylabel('é »åº¦', fontsize=11)
axes[2].set_title('Box-Coxå¤‰æ›', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.show()

# æ­ªåº¦ã®æ¯”è¼ƒ
from scipy.stats import skew
print(f&quot;å…ƒãƒ‡ãƒ¼ã‚¿ã®æ­ªåº¦: {skew(conductivity):.3f}&quot;)
print(f&quot;å¯¾æ•°å¤‰æ›å¾Œã®æ­ªåº¦: {skew(cond_log):.3f}&quot;)
print(f&quot;Box-Coxå¤‰æ›å¾Œã®æ­ªåº¦: {skew(cond_boxcox):.3f}&quot;)
</code></pre>
<h3>å¤šé …å¼ç‰¹å¾´é‡</h3>
<pre><code class="language-python">from sklearn.preprocessing import PolynomialFeatures

def create_polynomial_features(X, degree=2):
    &quot;&quot;&quot;
    å¤šé …å¼ç‰¹å¾´é‡ã®ç”Ÿæˆ

    Parameters:
    -----------
    X : array-like, shape (n_samples, n_features)
    degree : int
        å¤šé …å¼ã®æ¬¡æ•°

    Returns:
    --------
    X_poly : å¤šé …å¼ç‰¹å¾´é‡
    feature_names : ç‰¹å¾´é‡å
    &quot;&quot;&quot;
    poly = PolynomialFeatures(degree=degree, include_bias=False)
    X_poly = poly.fit_transform(X)
    feature_names = poly.get_feature_names_out()

    return X_poly, feature_names

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
np.random.seed(42)
X_original = pd.DataFrame({
    'x1': np.random.uniform(0, 1, 50),
    'x2': np.random.uniform(0, 1, 50)
})

# 2æ¬¡å¤šé …å¼ç‰¹å¾´é‡
X_poly, feature_names = create_polynomial_features(
    X_original.values, degree=2
)

print(f&quot;å…ƒã®ç‰¹å¾´é‡æ•°: {X_original.shape[1]}&quot;)
print(f&quot;å¤šé …å¼ç‰¹å¾´é‡æ•°: {X_poly.shape[1]}&quot;)
print(f&quot;\nç”Ÿæˆã•ã‚ŒãŸç‰¹å¾´é‡ï¼š&quot;)
for name in feature_names:
    print(f&quot;  {name}&quot;)

# éç·šå½¢é–¢ä¿‚ã®å­¦ç¿’ä¾‹
# y = 2*x1^2 + 3*x1*x2 - x2^2 + noise
y_true = (
    2 * X_original['x1']**2 +
    3 * X_original['x1'] * X_original['x2'] -
    X_original['x2']**2 +
    np.random.normal(0, 0.1, 50)
)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# ç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼ˆå…ƒã®ç‰¹å¾´é‡ï¼‰
model_linear = LinearRegression()
model_linear.fit(X_original, y_true)
y_pred_linear = model_linear.predict(X_original)
r2_linear = r2_score(y_true, y_pred_linear)

# ç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼ˆå¤šé …å¼ç‰¹å¾´é‡ï¼‰
model_poly = LinearRegression()
model_poly.fit(X_poly, y_true)
y_pred_poly = model_poly.predict(X_poly)
r2_poly = r2_score(y_true, y_pred_poly)

print(f&quot;\nç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼ˆå…ƒç‰¹å¾´é‡ï¼‰RÂ²: {r2_linear:.4f}&quot;)
print(f&quot;ç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼ˆå¤šé …å¼ç‰¹å¾´é‡ï¼‰RÂ²: {r2_poly:.4f}&quot;)
print(f&quot;æ”¹å–„ç‡: {(r2_poly - r2_linear) / r2_linear * 100:.1f}%&quot;)
</code></pre>
<h3>äº¤äº’ä½œç”¨é …ã®ç”Ÿæˆ</h3>
<pre><code class="language-python">from itertools import combinations

def create_interaction_features(df):
    &quot;&quot;&quot;
    äº¤äº’ä½œç”¨é …ã®ç”Ÿæˆ

    Parameters:
    -----------
    df : pd.DataFrame
        å…ƒã®ç‰¹å¾´é‡

    Returns:
    --------
    df_with_interactions : äº¤äº’ä½œç”¨é …ã‚’è¿½åŠ ã—ãŸDataFrame
    &quot;&quot;&quot;
    df_new = df.copy()

    # 2å¤‰æ•°ã®äº¤äº’ä½œç”¨ï¼ˆç©ï¼‰
    for col1, col2 in combinations(df.columns, 2):
        interaction_name = f&quot;{col1}Ã—{col2}&quot;
        df_new[interaction_name] = df[col1] * df[col2]

    return df_new

# ä¾‹ï¼šç†±é›»ææ–™ã®ç‰¹å¾´é‡
thermoelectric_features = pd.DataFrame({
    'é›»æ°—ä¼å°åº¦': np.random.lognormal(8, 1, 50),
    'ã‚¼ãƒ¼ãƒ™ãƒƒã‚¯ä¿‚æ•°': np.random.normal(200, 50, 50),
    'ç†±ä¼å°åº¦': np.random.uniform(1, 10, 50)
})

# äº¤äº’ä½œç”¨é …è¿½åŠ 
features_with_interactions = create_interaction_features(
    thermoelectric_features
)

print(f&quot;å…ƒã®ç‰¹å¾´é‡: {thermoelectric_features.columns.tolist()}&quot;)
print(f&quot;\nè¿½åŠ ã•ã‚ŒãŸäº¤äº’ä½œç”¨é …:&quot;)
new_cols = [col for col in features_with_interactions.columns
            if col not in thermoelectric_features.columns]
for col in new_cols:
    print(f&quot;  {col}&quot;)

# ZTå€¤äºˆæ¸¬ï¼ˆZT = Ïƒ*SÂ²/Îº ã«è¿‘ã„ï¼‰
thermoelectric_features['ZT'] = (
    thermoelectric_features['é›»æ°—ä¼å°åº¦'] *
    thermoelectric_features['ã‚¼ãƒ¼ãƒ™ãƒƒã‚¯ä¿‚æ•°']**2 /
    thermoelectric_features['ç†±ä¼å°åº¦'] / 1e6 +
    np.random.normal(0, 0.1, 50)
)

print(f&quot;\nç›¸é–¢åˆ†æï¼ˆäº¤äº’ä½œç”¨é …ã¨ã®ç›¸é–¢ï¼‰ï¼š&quot;)
correlations = features_with_interactions.corrwith(
    thermoelectric_features['ZT']
).sort_values(ascending=False)
print(correlations)
</code></pre>
<hr />
<h2>2.3 æ¬¡å…ƒå‰Šæ¸›</h2>
<p>é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ä½æ¬¡å…ƒã«åœ§ç¸®ã—ã€å¯è¦–åŒ–ãƒ»è§£é‡ˆã‚’å®¹æ˜“ã«ã—ã¾ã™ã€‚</p>
<h3>PCA (Principal Component Analysis)</h3>
<pre><code class="language-python">from sklearn.decomposition import PCA

def apply_pca(X, n_components=2):
    &quot;&quot;&quot;
    PCAã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›

    Parameters:
    -----------
    X : array-like
        å…ƒã®ç‰¹å¾´é‡
    n_components : int
        å‰Šæ¸›å¾Œã®æ¬¡å…ƒæ•°

    Returns:
    --------
    X_pca : ä¸»æˆåˆ†å¾—ç‚¹
    pca : PCãŠ object
    &quot;&quot;&quot;
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X)

    return X_pca, pca

# é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆ100æ¬¡å…ƒï¼‰
np.random.seed(42)
n_samples = 200
n_features = 100

# æ½œåœ¨çš„ãª2æ¬¡å…ƒæ§‹é€ ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿
latent = np.random.randn(n_samples, 2)
X_high_dim = latent @ np.random.randn(2, n_features) + np.random.randn(n_samples, n_features) * 0.5

# PCAé©ç”¨
X_pca, pca_model = apply_pca(X_high_dim, n_components=10)

# å¯„ä¸ç‡
explained_var = pca_model.explained_variance_ratio_

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# å¯„ä¸ç‡
axes[0].bar(range(1, 11), explained_var * 100,
            color='steelblue', alpha=0.7)
axes[0].plot(range(1, 11), np.cumsum(explained_var) * 100,
             'ro-', linewidth=2, label='ç´¯ç©å¯„ä¸ç‡')
axes[0].set_xlabel('ä¸»æˆåˆ†', fontsize=12)
axes[0].set_ylabel('å¯„ä¸ç‡ (%)', fontsize=12)
axes[0].set_title('PCAå¯„ä¸ç‡', fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# 2æ¬¡å…ƒãƒ—ãƒ­ãƒƒãƒˆ
axes[1].scatter(X_pca[:, 0], X_pca[:, 1],
                c='steelblue', s=50, alpha=0.6, edgecolors='k')
axes[1].set_xlabel('PC1', fontsize=12)
axes[1].set_ylabel('PC2', fontsize=12)
axes[1].set_title('PCAå¯è¦–åŒ–ï¼ˆ2æ¬¡å…ƒï¼‰', fontsize=13, fontweight='bold')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(f&quot;å…ƒã®æ¬¡å…ƒæ•°: {n_features}&quot;)
print(f&quot;å‰Šæ¸›å¾Œã®æ¬¡å…ƒæ•°: {X_pca.shape[1]}&quot;)
print(f&quot;PC1-PC2ã®ç´¯ç©å¯„ä¸ç‡: {np.sum(explained_var[:2]) * 100:.2f}%&quot;)
print(f&quot;PC1-PC10ã®ç´¯ç©å¯„ä¸ç‡: {np.sum(explained_var) * 100:.2f}%&quot;)
</code></pre>
<h3>t-SNE, UMAP</h3>
<pre><code class="language-python">from sklearn.manifold import TSNE
# !pip install umap-learn
from umap import UMAP

def compare_dimensionality_reduction(X, labels=None):
    &quot;&quot;&quot;
    PCA, t-SNE, UMAPã®æ¯”è¼ƒ
    &quot;&quot;&quot;
    # PCA
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)

    # t-SNE
    tsne = TSNE(n_components=2, random_state=42)
    X_tsne = tsne.fit_transform(X)

    # UMAP
    umap_model = UMAP(n_components=2, random_state=42)
    X_umap = umap_model.fit_transform(X)

    # å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    # PCA
    axes[0].scatter(X_pca[:, 0], X_pca[:, 1],
                    c=labels, cmap='viridis', s=50, alpha=0.6)
    axes[0].set_xlabel('PC1', fontsize=11)
    axes[0].set_ylabel('PC2', fontsize=11)
    axes[0].set_title('PCA', fontsize=12, fontweight='bold')
    axes[0].grid(alpha=0.3)

    # t-SNE
    axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1],
                    c=labels, cmap='viridis', s=50, alpha=0.6)
    axes[1].set_xlabel('t-SNE1', fontsize=11)
    axes[1].set_ylabel('t-SNE2', fontsize=11)
    axes[1].set_title('t-SNE', fontsize=12, fontweight='bold')
    axes[1].grid(alpha=0.3)

    # UMAP
    im = axes[2].scatter(X_umap[:, 0], X_umap[:, 1],
                         c=labels, cmap='viridis', s=50, alpha=0.6)
    axes[2].set_xlabel('UMAP1', fontsize=11)
    axes[2].set_ylabel('UMAP2', fontsize=11)
    axes[2].set_title('UMAP', fontsize=12, fontweight='bold')
    axes[2].grid(alpha=0.3)

    if labels is not None:
        plt.colorbar(im, ax=axes[2], label='ãƒ©ãƒ™ãƒ«')

    plt.tight_layout()
    plt.show()

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆ3ã‚¯ãƒ©ã‚¹ï¼‰
np.random.seed(42)
class1 = np.random.randn(100, 50) + [2, 2] + np.zeros(48)
class2 = np.random.randn(100, 50) + [-2, 2] + np.zeros(48)
class3 = np.random.randn(100, 50) + [0, -2] + np.zeros(48)

X_multi_class = np.vstack([class1, class2, class3])
labels = np.array([0]*100 + [1]*100 + [2]*100)

# æ¯”è¼ƒ
compare_dimensionality_reduction(X_multi_class, labels)

print(&quot;æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã®ç‰¹å¾´ï¼š&quot;)
print(&quot;PCA: ç·šå½¢å¤‰æ›ã€å¤§åŸŸçš„æ§‹é€ ä¿æŒã€é«˜é€Ÿ&quot;)
print(&quot;t-SNE: éç·šå½¢å¤‰æ›ã€å±€æ‰€çš„æ§‹é€ ä¿æŒã€é…ã„&quot;)
print(&quot;UMAP: éç·šå½¢å¤‰æ›ã€å¤§åŸŸ+å±€æ‰€æ§‹é€ ä¿æŒã€ä¸­é€Ÿ&quot;)
</code></pre>
<h3>LDA (Linear Discriminant Analysis)</h3>
<pre><code class="language-python">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

def apply_lda(X, y, n_components=2):
    &quot;&quot;&quot;
    LDAã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›ï¼ˆæ•™å¸«ã‚ã‚Šï¼‰

    Parameters:
    -----------
    X : array-like
        ç‰¹å¾´é‡
    y : array-like
        ãƒ©ãƒ™ãƒ«

    Returns:
    --------
    X_lda : LDAå¤‰æ›å¾Œã®ç‰¹å¾´é‡
    lda : LDAãƒ¢ãƒ‡ãƒ«
    &quot;&quot;&quot;
    lda = LinearDiscriminantAnalysis(n_components=n_components)
    X_lda = lda.fit_transform(X, y)

    return X_lda, lda

# LDAé©ç”¨
X_lda, lda_model = apply_lda(X_multi_class, labels, n_components=2)

# PCA vs LDAæ¯”è¼ƒ
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# PCAï¼ˆæ•™å¸«ãªã—ï¼‰
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_multi_class)

axes[0].scatter(X_pca[:, 0], X_pca[:, 1],
                c=labels, cmap='viridis', s=50, alpha=0.6)
axes[0].set_xlabel('PC1', fontsize=11)
axes[0].set_ylabel('PC2', fontsize=11)
axes[0].set_title('PCAï¼ˆæ•™å¸«ãªã—ï¼‰', fontsize=12, fontweight='bold')
axes[0].grid(alpha=0.3)

# LDAï¼ˆæ•™å¸«ã‚ã‚Šï¼‰
im = axes[1].scatter(X_lda[:, 0], X_lda[:, 1],
                     c=labels, cmap='viridis', s=50, alpha=0.6)
axes[1].set_xlabel('LD1', fontsize=11)
axes[1].set_ylabel('LD2', fontsize=11)
axes[1].set_title('LDAï¼ˆæ•™å¸«ã‚ã‚Šï¼‰', fontsize=12, fontweight='bold')
axes[1].grid(alpha=0.3)

plt.colorbar(im, ax=axes[1], label='ã‚¯ãƒ©ã‚¹')
plt.tight_layout()
plt.show()

print(&quot;LDAã®åˆ©ç‚¹ï¼š&quot;)
print(&quot;- ã‚¯ãƒ©ã‚¹åˆ†é›¢ã‚’æœ€å¤§åŒ–ã™ã‚‹å°„å½±è»¸ã‚’è¦‹ã¤ã‘ã‚‹&quot;)
print(&quot;- åˆ†é¡å•é¡Œã«é©ã—ã¦ã„ã‚‹&quot;)
print(f&quot;- æœ€å¤§æ¬¡å…ƒæ•°: min(n_features, n_classes-1) = {lda_model.n_components}&quot;)
</code></pre>
<hr />
<h2>2.4 ç‰¹å¾´é‡é¸æŠ</h2>
<p>é‡è¦ãªç‰¹å¾´é‡ã®ã¿ã‚’é¸æŠã—ã€ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã¨è§£é‡ˆæ€§ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚</p>
<h3>Filteræ³•ï¼šç›¸é–¢ä¿‚æ•°ã€åˆ†æ•£åˆ†æ</h3>
<pre><code class="language-python">from sklearn.feature_selection import (
    VarianceThreshold,
    SelectKBest,
    f_regression,
    mutual_info_regression
)

def filter_method_selection(X, y, k=10):
    &quot;&quot;&quot;
    Filteræ³•ã«ã‚ˆã‚‹ç‰¹å¾´é‡é¸æŠ

    Parameters:
    -----------
    X : pd.DataFrame
        ç‰¹å¾´é‡
    y : array-like
        ç›®çš„å¤‰æ•°

    Returns:
    --------
    selected_features : é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡å
    scores : å„ç‰¹å¾´é‡ã®ã‚¹ã‚³ã‚¢
    &quot;&quot;&quot;
    # ä½åˆ†æ•£ç‰¹å¾´é‡é™¤å»
    var_threshold = VarianceThreshold(threshold=0.01)
    X_var = var_threshold.fit_transform(X)
    selected_by_var = X.columns[var_threshold.get_support()]

    # Få€¤çµ±è¨ˆé‡
    selector_f = SelectKBest(f_regression, k=k)
    selector_f.fit(X, y)
    scores_f = selector_f.scores_
    selected_by_f = X.columns[selector_f.get_support()]

    # ç›¸äº’æƒ…å ±é‡
    selector_mi = SelectKBest(mutual_info_regression, k=k)
    selector_mi.fit(X, y)
    scores_mi = selector_mi.scores_
    selected_by_mi = X.columns[selector_mi.get_support()]

    return {
        'variance': selected_by_var,
        'f_stat': selected_by_f,
        'mutual_info': selected_by_mi,
        'scores_f': scores_f,
        'scores_mi': scores_mi
    }

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
np.random.seed(42)
n_samples = 200
X_data = pd.DataFrame(
    np.random.randn(n_samples, 30),
    columns=[f'feature_{i}' for i in range(30)]
)

# ç›®çš„å¤‰æ•°ï¼ˆä¸€éƒ¨ã®ç‰¹å¾´é‡ã®ã¿é–¢é€£ï¼‰
y_data = (
    2 * X_data['feature_0'] +
    3 * X_data['feature_5'] -
    1.5 * X_data['feature_10'] +
    np.random.normal(0, 0.5, n_samples)
)

# Filteræ³•å®Ÿè¡Œ
selection_results = filter_method_selection(X_data, y_data, k=10)

# ã‚¹ã‚³ã‚¢å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Få€¤ã‚¹ã‚³ã‚¢
axes[0].bar(range(len(selection_results['scores_f'])),
            selection_results['scores_f'],
            color='steelblue', alpha=0.7)
axes[0].set_xlabel('ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', fontsize=11)
axes[0].set_ylabel('Få€¤ã‚¹ã‚³ã‚¢', fontsize=11)
axes[0].set_title('Få€¤çµ±è¨ˆé‡ã«ã‚ˆã‚‹ç‰¹å¾´é‡è©•ä¾¡', fontsize=12, fontweight='bold')
axes[0].grid(alpha=0.3)

# ç›¸äº’æƒ…å ±é‡ã‚¹ã‚³ã‚¢
axes[1].bar(range(len(selection_results['scores_mi'])),
            selection_results['scores_mi'],
            color='coral', alpha=0.7)
axes[1].set_xlabel('ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', fontsize=11)
axes[1].set_ylabel('ç›¸äº’æƒ…å ±é‡', fontsize=11)
axes[1].set_title('ç›¸äº’æƒ…å ±é‡ã«ã‚ˆã‚‹ç‰¹å¾´é‡è©•ä¾¡', fontsize=12, fontweight='bold')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;Få€¤çµ±è¨ˆé‡ã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡:&quot;)
print(selection_results['f_stat'].tolist())
print(&quot;\nç›¸äº’æƒ…å ±é‡ã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡:&quot;)
print(selection_results['mutual_info'].tolist())
</code></pre>
<h3>Wrapperæ³•ï¼šRFE</h3>
<pre><code class="language-python">from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor

def rfe_selection(X, y, n_features_to_select=10):
    &quot;&quot;&quot;
    RFEï¼ˆRecursive Feature Eliminationï¼‰
    &quot;&quot;&quot;
    estimator = RandomForestRegressor(n_estimators=50, random_state=42)
    selector = RFE(estimator, n_features_to_select=n_features_to_select)
    selector.fit(X, y)

    selected_features = X.columns[selector.support_]
    feature_ranking = selector.ranking_

    return selected_features, feature_ranking

# RFEå®Ÿè¡Œ
selected_rfe, ranking_rfe = rfe_selection(X_data, y_data, n_features_to_select=10)

# ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¯è¦–åŒ–
plt.figure(figsize=(12, 6))
plt.bar(range(len(ranking_rfe)), ranking_rfe,
        color='steelblue', alpha=0.7)
plt.axhline(y=1, color='red', linestyle='--',
            label='é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ (rank=1)', linewidth=2)
plt.xlabel('ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', fontsize=12)
plt.ylabel('ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆä½ã„ã»ã©é‡è¦ï¼‰', fontsize=12)
plt.title('RFEã«ã‚ˆã‚‹ç‰¹å¾´é‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(&quot;RFEã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡:&quot;)
print(selected_rfe.tolist())
</code></pre>
<h3>Embeddedæ³•ï¼šLasso, Random Forest importances</h3>
<pre><code class="language-python">from sklearn.linear_model import Lasso
from sklearn.ensemble import RandomForestRegressor

def embedded_selection(X, y):
    &quot;&quot;&quot;
    Embeddedæ³•ï¼ˆLasso + Random Forestï¼‰
    &quot;&quot;&quot;
    # Lassoï¼ˆL1æ­£å‰‡åŒ–ï¼‰
    lasso = Lasso(alpha=0.1, random_state=42)
    lasso.fit(X, y)
    lasso_coefs = np.abs(lasso.coef_)
    selected_lasso = X.columns[lasso_coefs &gt; 0]

    # Random Forest importances
    rf = RandomForestRegressor(n_estimators=100, random_state=42)
    rf.fit(X, y)
    rf_importances = rf.feature_importances_
    # ä¸Šä½10å€‹é¸æŠ
    top_10_idx = np.argsort(rf_importances)[-10:]
    selected_rf = X.columns[top_10_idx]

    return {
        'lasso': selected_lasso,
        'lasso_coefs': lasso_coefs,
        'rf': selected_rf,
        'rf_importances': rf_importances
    }

# Embeddedæ³•å®Ÿè¡Œ
embedded_results = embedded_selection(X_data, y_data)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Lassoä¿‚æ•°
axes[0].bar(range(len(embedded_results['lasso_coefs'])),
            embedded_results['lasso_coefs'],
            color='steelblue', alpha=0.7)
axes[0].set_xlabel('ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', fontsize=11)
axes[0].set_ylabel('|Lassoä¿‚æ•°|', fontsize=11)
axes[0].set_title('Lassoç‰¹å¾´é‡é¸æŠ', fontsize=12, fontweight='bold')
axes[0].grid(alpha=0.3)

# Random Forest importances
axes[1].bar(range(len(embedded_results['rf_importances'])),
            embedded_results['rf_importances'],
            color='coral', alpha=0.7)
axes[1].set_xlabel('ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', fontsize=11)
axes[1].set_ylabel('Feature Importance', fontsize=11)
axes[1].set_title('Random Foresté‡è¦åº¦', fontsize=12, fontweight='bold')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;Lassoã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡:&quot;)
print(embedded_results['lasso'].tolist())
print(&quot;\nRandom Forestã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ï¼ˆä¸Šä½10ï¼‰:&quot;)
print(embedded_results['rf'].tolist())
</code></pre>
<h3>SHAP-based selection</h3>
<pre><code class="language-python">import shap

def shap_based_selection(X, y, top_k=10):
    &quot;&quot;&quot;
    SHAPã«ã‚ˆã‚‹ç‰¹å¾´é‡é¸æŠ
    &quot;&quot;&quot;
    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X, y)

    # SHAPå€¤è¨ˆç®—
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X)

    # å¹³å‡çµ¶å¯¾SHAPå€¤ã§é‡è¦åº¦è©•ä¾¡
    mean_abs_shap = np.abs(shap_values).mean(axis=0)

    # ä¸Šä½kå€‹é¸æŠ
    top_k_idx = np.argsort(mean_abs_shap)[-top_k:]
    selected_features = X.columns[top_k_idx]

    return selected_features, mean_abs_shap, shap_values

# SHAPé¸æŠ
selected_shap, mean_shap, shap_vals = shap_based_selection(X_data, y_data, top_k=10)

# SHAP Summary Plot
plt.figure(figsize=(10, 8))
shap.summary_plot(shap_vals, X_data, plot_type=&quot;bar&quot;, show=False)
plt.title('SHAPç‰¹å¾´é‡é‡è¦åº¦', fontsize=13, fontweight='bold')
plt.tight_layout()
plt.show()

print(&quot;SHAPã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ï¼ˆä¸Šä½10ï¼‰:&quot;)
print(selected_shap.tolist())

# æ‰‹æ³•æ¯”è¼ƒ
print(&quot;\nç‰¹å¾´é‡é¸æŠæ‰‹æ³•ã®æ¯”è¼ƒï¼š&quot;)
print(f&quot;Filteræ³•ï¼ˆFå€¤ï¼‰: {len(selection_results['f_stat'])} ç‰¹å¾´é‡&quot;)
print(f&quot;Wrapperæ³•ï¼ˆRFEï¼‰: {len(selected_rfe)} ç‰¹å¾´é‡&quot;)
print(f&quot;Embeddedæ³•ï¼ˆLassoï¼‰: {len(embedded_results['lasso'])} ç‰¹å¾´é‡&quot;)
print(f&quot;SHAP-based: {len(selected_shap)} ç‰¹å¾´é‡&quot;)
</code></pre>
<hr />
<h2>2.5 ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ï¼šãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬</h2>
<p>å®Ÿéš›ã®ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã§ã€200æ¬¡å…ƒã‹ã‚‰20æ¬¡å…ƒã¸ã®åŠ¹æœçš„ãªå‰Šæ¸›ã‚’å®Ÿè·µã—ã¾ã™ã€‚</p>
<pre><code class="language-python"># ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
np.random.seed(42)
n_materials = 500

# 200æ¬¡å…ƒã®ææ–™è¨˜è¿°å­ï¼ˆçµ„æˆãƒ»æ§‹é€ ãƒ»é›»å­æ§‹é€ ï¼‰
descriptor_names = (
    [f'çµ„æˆ_{i}' for i in range(80)] +
    [f'æ§‹é€ _{i}' for i in range(60)] +
    [f'é›»å­_{i}' for i in range(60)]
)

X_bandgap = pd.DataFrame(
    np.random.randn(n_materials, 200),
    columns=descriptor_names
)

# ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ï¼ˆä¸€éƒ¨ã®è¨˜è¿°å­ã®ã¿ä¾å­˜ï¼‰
important_features = [
    'çµ„æˆ_5', 'çµ„æˆ_12', 'çµ„æˆ_25',
    'æ§‹é€ _10', 'æ§‹é€ _23',
    'é›»å­_8', 'é›»å­_15', 'é›»å­_30'
]

y_bandgap = np.zeros(n_materials)
for feat in important_features:
    idx = descriptor_names.index(feat)
    y_bandgap += np.random.uniform(0.5, 1.5) * X_bandgap[feat]

y_bandgap = np.abs(y_bandgap) + np.random.normal(0, 0.3, n_materials)
y_bandgap = np.clip(y_bandgap, 0, 6)  # 0-6 eVã®ç¯„å›²

print(&quot;=== ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ===&quot;)
print(f&quot;ææ–™æ•°: {n_materials}&quot;)
print(f&quot;ç‰¹å¾´é‡æ•°: {X_bandgap.shape[1]}&quot;)
print(f&quot;ç›®æ¨™: 200æ¬¡å…ƒ â†’ 20æ¬¡å…ƒã«å‰Šæ¸›&quot;)
</code></pre>
<h3>Step 1: Filteræ³•ã§100æ¬¡å…ƒã«å‰Šæ¸›</h3>
<pre><code class="language-python"># ç›¸äº’æƒ…å ±é‡ã§ãƒˆãƒƒãƒ—100é¸æŠ
selector_mi = SelectKBest(mutual_info_regression, k=100)
X_filtered = selector_mi.fit_transform(X_bandgap, y_bandgap)
selected_features_100 = X_bandgap.columns[selector_mi.get_support()]

print(f&quot;\nStep 1: Filteræ³•ï¼ˆç›¸äº’æƒ…å ±é‡ï¼‰&quot;)
print(f&quot;200æ¬¡å…ƒ â†’ {X_filtered.shape[1]}æ¬¡å…ƒ&quot;)
</code></pre>
<h3>Step 2: PCAã§50æ¬¡å…ƒã«å‰Šæ¸›</h3>
<pre><code class="language-python"># PCAé©ç”¨
pca = PCA(n_components=50)
X_pca = pca.fit_transform(X_filtered)

# ç´¯ç©å¯„ä¸ç‡
cumsum_var = np.cumsum(pca.explained_variance_ratio_)

# 90%ç´¯ç©å¯„ä¸ç‡ã‚’é”æˆã™ã‚‹æ¬¡å…ƒæ•°
n_components_90 = np.argmax(cumsum_var &gt;= 0.90) + 1

plt.figure(figsize=(10, 6))
plt.plot(range(1, 51), cumsum_var * 100, 'b-', linewidth=2)
plt.axhline(y=90, color='red', linestyle='--',
            label='90%ç´¯ç©å¯„ä¸ç‡', linewidth=2)
plt.axvline(x=n_components_90, color='green', linestyle='--',
            label=f'{n_components_90}æ¬¡å…ƒã§90%é”æˆ', linewidth=2)
plt.xlabel('ä¸»æˆåˆ†æ•°', fontsize=12)
plt.ylabel('ç´¯ç©å¯„ä¸ç‡ (%)', fontsize=12)
plt.title('PCAã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(f&quot;\nStep 2: PCA&quot;)
print(f&quot;100æ¬¡å…ƒ â†’ {X_pca.shape[1]}æ¬¡å…ƒ&quot;)
print(f&quot;90%ç´¯ç©å¯„ä¸ç‡é”æˆ: {n_components_90}æ¬¡å…ƒ&quot;)
</code></pre>
<h3>Step 3: Random Forest Importanceã§20æ¬¡å…ƒã«å‰Šæ¸›</h3>
<pre><code class="language-python"># 50æ¬¡å…ƒã®PCAç‰¹å¾´é‡ã§Random Forestè¨“ç·´
X_pca_df = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(50)])

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_pca_df, y_bandgap)

# é‡è¦åº¦ãƒˆãƒƒãƒ—20é¸æŠ
importances = rf.feature_importances_
top_20_idx = np.argsort(importances)[-20:]
X_final = X_pca_df.iloc[:, top_20_idx]

# é‡è¦åº¦å¯è¦–åŒ–
plt.figure(figsize=(12, 6))
plt.bar(range(50), importances, color='steelblue', alpha=0.7)
plt.bar(top_20_idx, importances[top_20_idx],
        color='coral', alpha=0.9, label='é¸æŠã•ã‚ŒãŸ20æ¬¡å…ƒ')
plt.xlabel('ä¸»æˆåˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', fontsize=12)
plt.ylabel('Feature Importance', fontsize=12)
plt.title('Random Forestã«ã‚ˆã‚‹æœ€çµ‚é¸æŠ', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(f&quot;\nStep 3: Random Forest Importance&quot;)
print(f&quot;50æ¬¡å…ƒ â†’ {X_final.shape[1]}æ¬¡å…ƒ&quot;)
print(f&quot;\næœ€çµ‚é¸æŠã•ã‚ŒãŸä¸»æˆåˆ†:&quot;)
print(X_final.columns.tolist())
</code></pre>
<h3>Step 4: äºˆæ¸¬æ€§èƒ½ã®æ¤œè¨¼</h3>
<pre><code class="language-python">from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_absolute_error, r2_score

def evaluate_dimension_reduction(X, y, name):
    &quot;&quot;&quot;
    æ¬¡å…ƒå‰Šæ¸›å¾Œã®äºˆæ¸¬æ€§èƒ½è©•ä¾¡
    &quot;&quot;&quot;
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # äº¤å·®æ¤œè¨¼
    cv_scores = cross_val_score(
        model, X, y, cv=5,
        scoring='neg_mean_absolute_error'
    )
    cv_mae = -cv_scores.mean()

    return {
        'name': name,
        'dimensions': X.shape[1],
        'MAE': mae,
        'R2': r2,
        'CV_MAE': cv_mae
    }

# å„æ®µéšã®æ€§èƒ½è©•ä¾¡
results = []

# å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆ200æ¬¡å…ƒï¼‰
results.append(evaluate_dimension_reduction(X_bandgap, y_bandgap, 'å…ƒãƒ‡ãƒ¼ã‚¿'))

# Filterå¾Œï¼ˆ100æ¬¡å…ƒï¼‰
X_filtered_df = pd.DataFrame(X_filtered)
results.append(evaluate_dimension_reduction(X_filtered_df, y_bandgap, 'Filteræ³•'))

# PCAå¾Œï¼ˆ50æ¬¡å…ƒï¼‰
results.append(evaluate_dimension_reduction(X_pca_df, y_bandgap, 'PCA'))

# æœ€çµ‚ï¼ˆ20æ¬¡å…ƒï¼‰
results.append(evaluate_dimension_reduction(X_final, y_bandgap, 'æœ€çµ‚é¸æŠ'))

# çµæœè¡¨ç¤º
results_df = pd.DataFrame(results)
print(&quot;\n=== æ¬¡å…ƒå‰Šæ¸›ã®å½±éŸ¿è©•ä¾¡ ===&quot;)
print(results_df.to_string(index=False))

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# MAEæ¯”è¼ƒ
axes[0].bar(results_df['name'], results_df['MAE'],
            color=['gray', 'steelblue', 'coral', 'green'], alpha=0.7)
axes[0].set_ylabel('MAE (eV)', fontsize=12)
axes[0].set_title('äºˆæ¸¬èª¤å·®ï¼ˆMAEï¼‰', fontsize=13, fontweight='bold')
axes[0].tick_params(axis='x', rotation=15)
axes[0].grid(axis='y', alpha=0.3)

# RÂ²æ¯”è¼ƒ
axes[1].bar(results_df['name'], results_df['R2'],
            color=['gray', 'steelblue', 'coral', 'green'], alpha=0.7)
axes[1].set_ylabel('RÂ²', fontsize=12)
axes[1].set_ylim(0, 1)
axes[1].set_title('æ±ºå®šä¿‚æ•°ï¼ˆRÂ²ï¼‰', fontsize=13, fontweight='bold')
axes[1].tick_params(axis='x', rotation=15)
axes[1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

print(f&quot;\næ€§èƒ½ç¶­æŒç‡ï¼ˆæœ€çµ‚20æ¬¡å…ƒ vs å…ƒ200æ¬¡å…ƒï¼‰:&quot;)
print(f&quot;RÂ²ç¶­æŒç‡: {results_df.iloc[3]['R2'] / results_df.iloc[0]['R2'] * 100:.1f}%&quot;)
print(f&quot;æ¬¡å…ƒå‰Šæ¸›ç‡: {(1 - 20/200) * 100:.0f}%&quot;)
</code></pre>
<h3>ç‰©ç†çš„æ„å‘³ã¥ã‘</h3>
<pre><code class="language-python"># é¸æŠã•ã‚ŒãŸ20æ¬¡å…ƒã¨å…ƒã®è¨˜è¿°å­ã®å¯¾å¿œã‚’åˆ†æ
def interpret_selected_components(pca_model, original_features, selected_pcs):
    &quot;&quot;&quot;
    é¸æŠã•ã‚ŒãŸä¸»æˆåˆ†ã®ç‰©ç†çš„è§£é‡ˆ
    &quot;&quot;&quot;
    # PCA loadingsï¼ˆä¸»æˆåˆ†è² è·é‡ï¼‰
    loadings = pca_model.components_.T

    interpretations = []

    for pc_name in selected_pcs:
        pc_idx = int(pc_name.replace('PC', '')) - 1

        # ã“ã®ä¸»æˆåˆ†ã¸ã®å¯„ä¸ãŒå¤§ãã„å…ƒã®ç‰¹å¾´é‡
        loading_vector = np.abs(loadings[:, pc_idx])
        top_5_idx = np.argsort(loading_vector)[-5:]

        top_features = [original_features[i] for i in top_5_idx]
        top_loadings = loading_vector[top_5_idx]

        interpretations.append({
            'PC': pc_name,
            'Top_Features': top_features,
            'Loadings': top_loadings
        })

    return interpretations

# è§£é‡ˆå®Ÿè¡Œ
selected_pc_names = X_final.columns.tolist()
interpretations = interpret_selected_components(
    pca,
    selected_features_100.tolist(),
    selected_pc_names[:5]  # æœ€åˆã®5å€‹ã®ã¿è¡¨ç¤º
)

print(&quot;\n=== é¸æŠã•ã‚ŒãŸä¸»æˆåˆ†ã®ç‰©ç†çš„è§£é‡ˆ ===&quot;)
for interp in interpretations:
    print(f&quot;\n{interp['PC']}:&quot;)
    for feat, loading in zip(interp['Top_Features'], interp['Loadings']):
        print(f&quot;  {feat}: {loading:.3f}&quot;)
</code></pre>
<hr />
<h2>2.6 ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å®Ÿè·µã‚¬ã‚¤ãƒ‰</h2>
<h3>matminerãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†</h3>
<pre><code class="language-python"># matminerã¨pymatgenã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
import matminer
import pymatgen

print(&quot;=== ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ç’°å¢ƒ ===&quot;)
print(f&quot;matminer: {matminer.__version__}&quot;)
print(f&quot;pymatgen: {pymatgen.__version__}&quot;)

# æ¨å¥¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³
print(&quot;\nã€æ¨å¥¨ç’°å¢ƒã€‘&quot;)
recommended = {
    'matminer': '0.9.0',
    'pymatgen': '2023.9.10',
    'scikit-learn': '1.3.0',
    'numpy': '1.24.3'
}

for package, version in recommended.items():
    print(f&quot;{package}&gt;={version}&quot;)

print(&quot;\nã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚³ãƒãƒ³ãƒ‰ã€‘&quot;)
print(&quot;```bash&quot;)
print(&quot;pip install matminer==0.9.0 pymatgen==2023.9.10&quot;)
print(&quot;```&quot;)

print(&quot;\nã€æ³¨æ„äº‹é …ã€‘&quot;)
print(&quot;âš ï¸ matminerã®ãƒ—ãƒªã‚»ãƒƒãƒˆï¼ˆmagpie, demlãªã©ï¼‰ã¯ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ç‰¹å¾´é‡æ•°ãŒå¤‰ã‚ã‚‹&quot;)
print(&quot;âš ï¸ è«–æ–‡å†ç¾æ™‚ã¯å¿…ãšãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æ˜è¨˜&quot;)
</code></pre>
<h3>ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ´»ç”¨</h3>
<pre><code class="language-python"># ææ–™ç§‘å­¦ã§ä¸€èˆ¬çš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
benchmark_datasets = pd.DataFrame({
    'ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ': [
        'Matbench',
        'JARVIS-DFT',
        'Materials Project',
        'OQMD',
        'Expt Gap (Zhuo et al.)'
    ],
    'ã‚¿ã‚¹ã‚¯': [
        '13ç¨®é¡ã®å›å¸°/åˆ†é¡',
        '55,000ææ–™ã®ç‰©æ€§äºˆæ¸¬',
        'ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã€å½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼',
        'å®‰å®šæ€§äºˆæ¸¬ã€ç›¸å›³',
        'å®Ÿé¨“ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ï¼ˆ6,354ææ–™ï¼‰'
    ],
    'ã‚µãƒ³ãƒ—ãƒ«æ•°': [
        'æ•°ç™¾ã€œæ•°ä¸‡',
        '55,000+',
        '150,000+',
        '1,000,000+',
        '6,354'
    ],
    'ç”¨é€”': [
        'ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ',
        'GNNãƒ»æ·±å±¤å­¦ç¿’',
        'æ±ç”¨ææ–™æ¢ç´¢',
        'å®‰å®šæ€§è©•ä¾¡',
        'å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼'
    ],
    'URL': [
        'https://matbench.materialsproject.org/',
        'https://jarvis.nist.gov/',
        'https://materialsproject.org/',
        'http://oqmd.org/',
        'DOI: 10.1021/acs.jpclett.8b00124'
    ]
})

print(&quot;=== ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ===&quot;)
print(benchmark_datasets.to_string(index=False))

print(&quot;\nã€ä½¿ç”¨ä¾‹ï¼šMatbenchã€‘&quot;)
print(&quot;```python&quot;)
print(&quot;from matbench.bench import MatbenchBenchmark&quot;)
print(&quot;mb = MatbenchBenchmark(autoload=False)&quot;)
print(&quot;for task in mb.tasks:&quot;)
print(&quot;    task.load()&quot;)
print(&quot;    print(f'{task.dataset_name}: {len(task.df)} samples')&quot;)
print(&quot;```&quot;)
</code></pre>
<h3>å®Ÿè·µçš„ãªè½ã¨ã—ç©´ï¼ˆç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ç·¨ï¼‰</h3>
<pre><code class="language-python">print(&quot;=== ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®è½ã¨ã—ç©´ ===\n&quot;)

print(&quot;ã€è½ã¨ã—ç©´1: Target Leakageï¼ˆç›®çš„å¤‰æ•°ãƒªãƒ¼ã‚¯ï¼‰ã€‘&quot;)
print(&quot;âŒ æ‚ªã„ä¾‹ï¼šç›®çš„å¤‰æ•°ã‹ã‚‰ç‰¹å¾´é‡ã‚’ç”Ÿæˆ&quot;)
print(&quot;```python&quot;)
print(&quot;# ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬ã§ã€ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã«ç›´æ¥é–¢é€£ã™ã‚‹å€¤ã‚’ç‰¹å¾´é‡åŒ–&quot;)
print(&quot;X['bandgap_proxy'] = y_bandgap * 0.9 + noise  # NGï¼&quot;)
print(&quot;```&quot;)

print(&quot;\nâœ… æ­£ã—ã„ä¾‹ï¼šç‹¬ç«‹ãªç‰©æ€§ã®ã¿ã‚’ç‰¹å¾´é‡åŒ–&quot;)
print(&quot;```python&quot;)
print(&quot;# çµ„æˆã€æ§‹é€ ã€é›»å­æ§‹é€ ãªã©ã€ç›®çš„å¤‰æ•°ã¨ã¯ç‹¬ç«‹ãªè¨˜è¿°å­&quot;)
print(&quot;X_features = generate_composition_features(formulas)&quot;)
print(&quot;```&quot;)

print(&quot;\nã€è½ã¨ã—ç©´2: ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒ«ã®ä¸çµ±ä¸€ã€‘&quot;)
print(&quot;âš ï¸ æ ¼å­å®šæ•°ï¼ˆ3-7Ã…ï¼‰ã¨é›»æ°—ä¼å°åº¦ï¼ˆ10Â³-10â¶ S/mï¼‰ã‚’æ··åœ¨&quot;)
print(&quot;â†’ è·é›¢ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆKNNã€SVMï¼‰ã§æ€§èƒ½ä½ä¸‹&quot;)

print(&quot;\nâœ… å¯¾ç­–ï¼šStandardScalerã§æ­£è¦åŒ–&quot;)
print(&quot;```python&quot;)
print(&quot;from sklearn.preprocessing import StandardScaler&quot;)
print(&quot;scaler = StandardScaler()&quot;)
print(&quot;X_scaled = scaler.fit_transform(X)&quot;)
print(&quot;```&quot;)

print(&quot;\nã€è½ã¨ã—ç©´3: æ¬¡å…ƒå‰Šæ¸›ã§ã®æƒ…å ±æå¤±ã€‘&quot;)
print(&quot;âš ï¸ PCAã§95%å¯„ä¸ç‡ â†’ æ®‹ã‚Š5%ã«é‡è¦æƒ…å ±ãŒã‚ã‚‹å¯èƒ½æ€§&quot;)

print(&quot;\nâœ… å¯¾ç­–ï¼šè¤‡æ•°ã®å¯„ä¸ç‡ã§æ€§èƒ½æ¯”è¼ƒ&quot;)
print(&quot;```python&quot;)
print(&quot;for var_ratio in [0.90, 0.95, 0.99]:&quot;)
print(&quot;    pca = PCA(n_components=var_ratio)&quot;)
print(&quot;    X_pca = pca.fit_transform(X)&quot;)
print(&quot;    # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½è©•ä¾¡&quot;)
print(&quot;```&quot;)

print(&quot;\nã€è½ã¨ã—ç©´4: matminerãƒ—ãƒªã‚»ãƒƒãƒˆã®ç„¡æ‰¹åˆ¤ãªä½¿ç”¨ã€‘&quot;)
print(&quot;âš ï¸ magpieãƒ—ãƒªã‚»ãƒƒãƒˆï¼ˆ132ç‰¹å¾´é‡ï¼‰ã‚’ãã®ã¾ã¾ä½¿ç”¨&quot;)
print(&quot;â†’ å†—é•·ãƒ»ç„¡é–¢ä¿‚ãªç‰¹å¾´é‡ãŒå¤šæ•°å«ã¾ã‚Œã‚‹&quot;)

print(&quot;\nâœ… å¯¾ç­–ï¼šãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã§ç‰¹å¾´é‡ã‚’å³é¸&quot;)
print(&quot;```python&quot;)
print(&quot;# ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬ã«é–¢é€£ã™ã‚‹ç‰¹å¾´é‡ã®ã¿é¸æŠ&quot;)
print(&quot;relevant_features = [&quot;)
print(&quot;    'MagpieData mean Electronegativity',&quot;)
print(&quot;    'MagpieData range Electronegativity',&quot;)
print(&quot;    'MagpieData mean NValence'&quot;)
print(&quot;]&quot;)
print(&quot;X_selected = X_all[relevant_features]&quot;)
print(&quot;```&quot;)

print(&quot;\nã€è½ã¨ã—ç©´5: çµ„æˆè¨˜è¿°å­ã®éè¦æ ¼åŒ–ã€‘&quot;)
print(&quot;âš ï¸ Liâ‚€.â‚‰CoOâ‚‚ã¨LiCoOâ‚‚ã§ç•°ãªã‚‹ç‰¹å¾´é‡å€¤&quot;)
print(&quot;â†’ çµ„æˆæ­£è¦åŒ–ãŒå¿…è¦&quot;)

print(&quot;\nâœ… å¯¾ç­–ï¼šçµ„æˆã‚’è¦æ ¼åŒ–ï¼ˆåˆè¨ˆ=1ï¼‰&quot;)
print(&quot;```python&quot;)
print(&quot;from pymatgen.core import Composition&quot;)
print(&quot;comp = Composition('Li0.9CoO2')&quot;)
print(&quot;comp_normalized = comp.fractional_composition  # Liâ‚€.â‚ƒâ‚Coâ‚€.â‚ƒâ‚„Oâ‚€.â‚†â‚‰&quot;)
print(&quot;```&quot;)
</code></pre>
<hr />
<h2>æ¼”ç¿’å•é¡Œ</h2>
<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦: easyï¼‰</h3>
<p>matminerã‚’ä½¿ã£ã¦ã€åŒ–å­¦å¼"Fe2O3"ã¨"TiO2"ã®ææ–™è¨˜è¿°å­ã‚’ç”Ÿæˆã—ã€ã©ã®è¨˜è¿°å­ãŒæœ€ã‚‚ç•°ãªã‚‹ã‹ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>
<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

1. `ElementProperty.from_preset("magpie")`ã‚’ä½¿ç”¨
2. å„åŒ–å­¦å¼ã§ç‰¹å¾´é‡ç”Ÿæˆ
3. å·®ã®çµ¶å¯¾å€¤ã‚’è¨ˆç®—ã—ã€ä¸Šä½10å€‹ã‚’è¡¨ç¤º

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python">from matminer.featurizers.composition import ElementProperty
from pymatgen.core import Composition

# è¨˜è¿°å­ç”Ÿæˆ
ep_feat = ElementProperty.from_preset(&quot;magpie&quot;)

comp1 = Composition(&quot;Fe2O3&quot;)
comp2 = Composition(&quot;TiO2&quot;)

features1 = ep_feat.featurize(comp1)
features2 = ep_feat.featurize(comp2)

feature_names = ep_feat.feature_labels()

# å·®åˆ†è¨ˆç®—
df_comparison = pd.DataFrame({
    'Feature': feature_names,
    'Fe2O3': features1,
    'TiO2': features2,
    'Difference': np.abs(np.array(features1) - np.array(features2))
})

df_comparison_sorted = df_comparison.sort_values(
    'Difference', ascending=False
)

print(&quot;æœ€ã‚‚ç•°ãªã‚‹è¨˜è¿°å­ï¼ˆä¸Šä½10ï¼‰:&quot;)
print(df_comparison_sorted.head(10).to_string(index=False))
</code></pre>


</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦: mediumï¼‰</h3>
<p>PCAã¨UMAPã‚’ç”¨ã„ã¦ã€é«˜æ¬¡å…ƒææ–™ãƒ‡ãƒ¼ã‚¿ã‚’2æ¬¡å…ƒã«å‰Šæ¸›ã—ã€ã©ã¡ã‚‰ãŒã‚¯ãƒ©ã‚¹ã‚¿æ§‹é€ ã‚’ã‚ˆã‚Šæ˜ç¢ºã«å¯è¦–åŒ–ã§ãã‚‹ã‹æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>
<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python">from sklearn.decomposition import PCA
from umap import UMAP
from sklearn.datasets import make_blobs

# ã‚¯ãƒ©ã‚¹ã‚¿æ§‹é€ ã‚’æŒã¤é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
X, y = make_blobs(n_samples=300, n_features=50,
                  centers=3, random_state=42)

# PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# UMAP
umap_model = UMAP(n_components=2, random_state=42)
X_umap = umap_model.fit_transform(X)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

axes[0].scatter(X_pca[:, 0], X_pca[:, 1],
                c=y, cmap='viridis', s=50, alpha=0.6)
axes[0].set_title('PCA', fontsize=12, fontweight='bold')

axes[1].scatter(X_umap[:, 0], X_umap[:, 1],
                c=y, cmap='viridis', s=50, alpha=0.6)
axes[1].set_title('UMAP', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.show()
</code></pre>


</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦: hardï¼‰</h3>
<p>Filteræ³•ã€Wrapperæ³•ã€Embeddedæ³•ã€SHAP-basedã®4ã¤ã®ç‰¹å¾´é‡é¸æŠæ‰‹æ³•ã‚’ç”¨ã„ã¦ã€åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ãã‚Œãã‚Œä¸Šä½10ç‰¹å¾´é‡ã‚’é¸æŠã—ã€é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã®é‡è¤‡åº¦ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚</p>
<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python">from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Lasso
import shap

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
np.random.seed(42)
X = pd.DataFrame(np.random.randn(200, 30),
                 columns=[f'feat_{i}' for i in range(30)])
y = (2*X['feat_0'] + 3*X['feat_5'] - X['feat_10'] +
     np.random.normal(0, 0.5, 200))

# 1. Filteræ³•
selector_filter = SelectKBest(f_regression, k=10)
selector_filter.fit(X, y)
selected_filter = set(X.columns[selector_filter.get_support()])

# 2. Wrapperæ³•ï¼ˆRFEï¼‰
model_rfe = RandomForestRegressor(n_estimators=50, random_state=42)
selector_rfe = RFE(model_rfe, n_features_to_select=10)
selector_rfe.fit(X, y)
selected_rfe = set(X.columns[selector_rfe.support_])

# 3. Embeddedæ³•ï¼ˆLassoï¼‰
lasso = Lasso(alpha=0.1, random_state=42)
lasso.fit(X, y)
lasso_coefs = np.abs(lasso.coef_)
top_10_lasso_idx = np.argsort(lasso_coefs)[-10:]
selected_lasso = set(X.columns[top_10_lasso_idx])

# 4. SHAP-based
rf_shap = RandomForestRegressor(n_estimators=100, random_state=42)
rf_shap.fit(X, y)
explainer = shap.TreeExplainer(rf_shap)
shap_values = explainer.shap_values(X)
mean_abs_shap = np.abs(shap_values).mean(axis=0)
top_10_shap_idx = np.argsort(mean_abs_shap)[-10:]
selected_shap = set(X.columns[top_10_shap_idx])

# é‡è¤‡åˆ†æ
all_methods = {
    'Filter': selected_filter,
    'Wrapper': selected_rfe,
    'Embedded': selected_lasso,
    'SHAP': selected_shap
}

# ãƒ™ãƒ³å›³çš„ãªé‡è¤‡è¨ˆç®—
common_all = selected_filter &amp; selected_rfe &amp; selected_lasso &amp; selected_shap

print(&quot;å„æ‰‹æ³•ã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡:&quot;)
for method, features in all_methods.items():
    print(f&quot;{method}: {sorted(features)}&quot;)

print(f&quot;\nå…¨æ‰‹æ³•å…±é€š: {sorted(common_all)}&quot;)
print(f&quot;å…±é€šç‰¹å¾´é‡æ•°: {len(common_all)} / 10&quot;)
</code></pre>


</details>

<hr />
<h2>ã¾ã¨ã‚</h2>
<p>ã“ã®ç« ã§ã¯ã€<strong>ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</strong>ã®å®Ÿè·µæ‰‹æ³•ã‚’å­¦ã³ã¾ã—ãŸã€‚</p>
<p><strong>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</strong>ï¼š</p>
<ol>
<li><strong>ææ–™è¨˜è¿°å­</strong>ï¼šçµ„æˆãƒ»æ§‹é€ ãƒ»é›»å­æ§‹é€ è¨˜è¿°å­ã‚’matminerã§åŠ¹ç‡çš„ã«ç”Ÿæˆ</li>
<li><strong>ç‰¹å¾´é‡å¤‰æ›</strong>ï¼šæ­£è¦åŒ–ãƒ»å¯¾æ•°å¤‰æ›ãƒ»å¤šé …å¼ç‰¹å¾´é‡ã§éç·šå½¢é–¢ä¿‚ã‚’æ‰ãˆã‚‹</li>
<li><strong>æ¬¡å…ƒå‰Šæ¸›</strong>ï¼šPCAã€t-SNEã€UMAPã§å¯è¦–åŒ–ã¨è¨ˆç®—åŠ¹ç‡åŒ–</li>
<li><strong>ç‰¹å¾´é‡é¸æŠ</strong>ï¼šFilter &lt; Wrapper &lt; Embedded &lt; SHAP ã®é †ã§ç²¾åº¦å‘ä¸Š</li>
<li><strong>å®Ÿè·µäº‹ä¾‹</strong>ï¼š200æ¬¡å…ƒâ†’20æ¬¡å…ƒã§æ€§èƒ½ã‚’ç¶­æŒã—ã¤ã¤è§£é‡ˆæ€§å‘ä¸Š</li>
<li><strong>ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç®¡ç†</strong>ï¼šmatminerã€pymatgenã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã¨å†ç¾æ€§ç¢ºä¿</li>
<li><strong>ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿</strong>ï¼šMatbenchã€JARVIS-DFTãªã©æ¨™æº–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ´»ç”¨</li>
<li><strong>å®Ÿè·µçš„è½ã¨ã—ç©´</strong>ï¼šTarget leakageã€ã‚¹ã‚±ãƒ¼ãƒ«ä¸çµ±ä¸€ã€æƒ…å ±æå¤±ã€çµ„æˆè¦æ ¼åŒ–</li>
</ol>
<p><strong>æ¬¡ç« äºˆå‘Š</strong>ï¼š
Chapter 3ã§ã¯ã€æœ€é©ãªãƒ¢ãƒ‡ãƒ«é¸æŠã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’å­¦ã³ã¾ã™ã€‚Optunaã‚’ç”¨ã„ãŸè‡ªå‹•æœ€é©åŒ–ã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã§äºˆæ¸¬ç²¾åº¦ã‚’æœ€å¤§åŒ–ã—ã¾ã™ã€‚</p>
<hr />
<h2>Chapter 2 ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ</h2>
<h3>ç‰¹å¾´é‡ç”Ÿæˆï¼ˆçµ„æˆè¨˜è¿°å­ï¼‰</h3>
<ul>
<li>[ ] <strong>matmineræ´»ç”¨</strong></li>
<li>[ ] ElementProperty.from_preset("magpie")ã§132ç‰¹å¾´é‡ç”Ÿæˆ</li>
<li>[ ] Stoichiometry()ã§åŒ–å­¦é‡è«–è¨˜è¿°å­ï¼ˆ7ç‰¹å¾´é‡ï¼‰</li>
<li>[ ] ValenceOrbital()ã§ä¾¡é›»å­è»Œé“è¨˜è¿°å­ï¼ˆ10ç‰¹å¾´é‡ï¼‰</li>
<li>
<p>[ ] ãƒãƒ¼ã‚¸ãƒ§ãƒ³è¨˜éŒ²ï¼ˆmatminer 0.9.0ã€pymatgen 2023.9.10ï¼‰</p>
</li>
<li>
<p>[ ] <strong>çµ„æˆã®è¦æ ¼åŒ–</strong></p>
</li>
<li>[ ] Composition.fractional_compositionã§åˆè¨ˆ=1ã«æ­£è¦åŒ–</li>
<li>[ ] Liâ‚€.â‚‰CoOâ‚‚ ã¨LiCoOâ‚‚ã®æ‰±ã„ã‚’çµ±ä¸€</li>
<li>
<p>[ ] ãƒ‰ãƒ¼ãƒ‘ãƒ³ãƒˆæ¿ƒåº¦ã®é©åˆ‡ãªè¡¨ç¾</p>
</li>
<li>
<p>[ ] <strong>ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã®çµ±åˆ</strong></p>
</li>
<li>[ ] é›»æ°—é™°æ€§åº¦ã€åŸå­åŠå¾„ã€ä¾¡é›»å­æ•°ãªã©ã®åŒ–å­¦çš„æ„å‘³ã‚’ç†è§£</li>
<li>[ ] ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬ã«ã¯é›»å­æ§‹é€ è¨˜è¿°å­ã‚’å„ªå…ˆ</li>
<li>[ ] æ©Ÿæ¢°çš„ç‰¹æ€§äºˆæ¸¬ã«ã¯çµæ™¶æ§‹é€ è¨˜è¿°å­ã‚’å„ªå…ˆ</li>
</ul>
<h3>ç‰¹å¾´é‡ç”Ÿæˆï¼ˆæ§‹é€ ãƒ»é›»å­æ§‹é€ è¨˜è¿°å­ï¼‰</h3>
<ul>
<li>[ ] <strong>çµæ™¶æ§‹é€ è¨˜è¿°å­</strong></li>
<li>[ ] æ ¼å­å®šæ•°ï¼ˆa, b, cï¼‰ã€æ ¼å­è§’åº¦ï¼ˆÎ±, Î², Î³ï¼‰</li>
<li>[ ] ä½“ç©ã€ãƒ‘ãƒƒã‚­ãƒ³ã‚°å¯†åº¦</li>
<li>
<p>[ ] ç©ºé–“ç¾¤ã€å¯¾ç§°æ€§</p>
</li>
<li>
<p>[ ] <strong>é›»å­æ§‹é€ è¨˜è¿°å­</strong></p>
</li>
<li>[ ] ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã€ãƒ•ã‚§ãƒ«ãƒŸã‚¨ãƒãƒ«ã‚®ãƒ¼</li>
<li>[ ] çŠ¶æ…‹å¯†åº¦ï¼ˆä¾¡é›»å­å¸¯ãƒ»ä¼å°å¸¯ï¼‰</li>
<li>
<p>[ ] æœ‰åŠ¹è³ªé‡ï¼ˆé›»å­ãƒ»æ­£å­”ï¼‰</p>
</li>
<li>
<p>[ ] <strong>DFTè¨ˆç®—ãƒ‡ãƒ¼ã‚¿é€£æº</strong></p>
</li>
<li>[ ] Materials Project APIã§å½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼å–å¾—</li>
<li>[ ] OQMDã§å®‰å®šæ€§ãƒ‡ãƒ¼ã‚¿å–å¾—</li>
<li>[ ] è¨ˆç®—æ¡ä»¶ï¼ˆæ±é–¢æ•°ã€ã‚«ãƒƒãƒˆã‚ªãƒ•ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰ã‚’è¨˜éŒ²</li>
</ul>
<h3>ç‰¹å¾´é‡å¤‰æ›</h3>
<ul>
<li>[ ] <strong>æ­£è¦åŒ–</strong></li>
<li>[ ] StandardScalerï¼ˆå¹³å‡0ã€æ¨™æº–åå·®1ï¼‰ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨</li>
<li>[ ] MinMaxScalerï¼ˆ0-1ç¯„å›²ï¼‰ã¯è§£é‡ˆæ€§é‡è¦–æ™‚</li>
<li>
<p>[ ] Train/Teståˆ†å‰²<strong>å¾Œ</strong>ã«fitï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰</p>
</li>
<li>
<p>[ ] <strong>å¯¾æ•°å¤‰æ›</strong></p>
</li>
<li>[ ] é›»æ°—ä¼å°åº¦ãªã©æ¡æ•°ãŒå¤§ãã„ç‰¹å¾´é‡ã«é©ç”¨</li>
<li>[ ] log1pï¼ˆlog(1+x)ï¼‰ã§ã‚¼ãƒ­å€¤ã‚’æ‰±ã†</li>
<li>
<p>[ ] æ­ªåº¦ï¼ˆskewnessï¼‰ã‚’ç¢ºèªã—ã€å¤‰æ›å¾Œã«-0.5ã€œ0.5ã«åã‚ã‚‹</p>
</li>
<li>
<p>[ ] <strong>å¤šé …å¼ç‰¹å¾´é‡</strong></p>
</li>
<li>[ ] 2æ¬¡é …ã§éç·šå½¢é–¢ä¿‚ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£</li>
<li>[ ] äº¤äº’ä½œç”¨é …ï¼ˆxâ‚Ã—xâ‚‚ï¼‰ã§ç›¸äº’ä½œç”¨ã‚’æ˜ç¤º</li>
<li>[ ] æ¬¡æ•°3ä»¥ä¸Šã¯éå­¦ç¿’ãƒªã‚¹ã‚¯ â†’ æ…é‡ã«</li>
</ul>
<h3>æ¬¡å…ƒå‰Šæ¸›</h3>
<ul>
<li>[ ] <strong>PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰</strong></li>
<li>[ ] ç´¯ç©å¯„ä¸ç‡90-95%ã‚’ç›®æ¨™</li>
<li>[ ] ä¸»æˆåˆ†è² è·é‡ã§ç‰©ç†çš„è§£é‡ˆ</li>
<li>
<p>[ ] ã‚¹ã‚¯ãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆï¼ˆå¯„ä¸ç‡ã®æ¸›è¡°ï¼‰ã‚’å¯è¦–åŒ–</p>
</li>
<li>
<p>[ ] <strong>t-SNE / UMAP</strong></p>
</li>
<li>[ ] ã‚¯ãƒ©ã‚¹ã‚¿å¯è¦–åŒ–ã«æœ‰åŠ¹ï¼ˆ2æ¬¡å…ƒãƒ—ãƒ­ãƒƒãƒˆï¼‰</li>
<li>[ ] äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡ã«ã¯ä½¿ç”¨ã—ãªã„ï¼ˆéç·šå½¢ã§ä¸å¯é€†ï¼‰</li>
<li>
<p>[ ] ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ï¼ˆt-SNEï¼‰ã€n_neighborsï¼ˆUMAPï¼‰ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</p>
</li>
<li>
<p>[ ] <strong>LDAï¼ˆç·šå½¢åˆ¤åˆ¥åˆ†æï¼‰</strong></p>
</li>
<li>[ ] åˆ†é¡å•é¡Œã§ã‚¯ãƒ©ã‚¹åˆ†é›¢ã‚’æœ€å¤§åŒ–</li>
<li>[ ] æ¬¡å…ƒæ•°ã¯ min(n_features, n_classes-1)</li>
<li>[ ] æ•™å¸«ã‚ã‚Šå­¦ç¿’ãªã®ã§æ±åŒ–æ€§èƒ½ã«æ³¨æ„</li>
</ul>
<h3>ç‰¹å¾´é‡é¸æŠ</h3>
<ul>
<li>[ ] <strong>Filteræ³•</strong></li>
<li>[ ] VarianceThresholdï¼ˆä½åˆ†æ•£ç‰¹å¾´é‡é™¤å»ï¼‰</li>
<li>[ ] SelectKBest + f_regressionï¼ˆFå€¤çµ±è¨ˆé‡ï¼‰</li>
<li>[ ] SelectKBest + mutual_info_regressionï¼ˆç›¸äº’æƒ…å ±é‡ï¼‰</li>
<li>
<p>[ ] è¨ˆç®—é«˜é€Ÿã ãŒã€ç‰¹å¾´é‡é–“ã®ç›¸äº’ä½œç”¨ã‚’è€ƒæ…®ã—ãªã„</p>
</li>
<li>
<p>[ ] <strong>Wrapperæ³•</strong></p>
</li>
<li>[ ] RFEï¼ˆRecursive Feature Eliminationï¼‰</li>
<li>[ ] è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ã„ãŒç²¾åº¦é«˜ã„</li>
<li>
<p>[ ] 10-20ç‰¹å¾´é‡ã¸ã®å‰Šæ¸›ã«æœ‰åŠ¹</p>
</li>
<li>
<p>[ ] <strong>Embeddedæ³•</strong></p>
</li>
<li>[ ] Lassoï¼ˆL1æ­£å‰‡åŒ–ï¼‰ã§ä¿‚æ•°ã‚¼ãƒ­ã®ç‰¹å¾´é‡ã‚’è‡ªå‹•é™¤å»</li>
<li>[ ] Random Forest feature_importances_</li>
<li>[ ] LightGBM feature_importances_</li>
<li>
<p>[ ] è¨“ç·´éç¨‹ã§ç‰¹å¾´é‡é¸æŠãŒè¡Œã‚ã‚Œã‚‹</p>
</li>
<li>
<p>[ ] <strong>SHAP-basedé¸æŠ</strong></p>
</li>
<li>[ ] mean(|SHAPå€¤|)ã§é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°</li>
<li>[ ] Globalè§£é‡ˆã¨ã—ã¦æœ€ã‚‚ä¿¡é ¼æ€§ãŒé«˜ã„</li>
<li>[ ] è¨ˆç®—ã‚³ã‚¹ãƒˆã¯ä¸­ç¨‹åº¦</li>
</ul>
<h3>å®Ÿè·µçš„è½ã¨ã—ç©´ã®å›é¿ï¼ˆç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼‰</h3>
<ul>
<li>[ ] <strong>Target Leakageé˜²æ­¢</strong></li>
<li>[ ] ç›®çš„å¤‰æ•°ã‹ã‚‰ç‰¹å¾´é‡ã‚’ç”Ÿæˆã—ãªã„</li>
<li>[ ] DFTè¨ˆç®—å€¤ï¼ˆãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ãªã©ï¼‰ã‚’ç›´æ¥ä½¿ã‚ãªã„</li>
<li>
<p>[ ] çµ„æˆãƒ»æ§‹é€ ãªã©ç‹¬ç«‹ãªè¨˜è¿°å­ã®ã¿ä½¿ç”¨</p>
</li>
<li>
<p>[ ] <strong>ã‚¹ã‚±ãƒ¼ãƒ«çµ±ä¸€</strong></p>
</li>
<li>[ ] æ ¼å­å®šæ•°ï¼ˆÃ…ï¼‰ã¨é›»æ°—ä¼å°åº¦ï¼ˆS/mï¼‰ã‚’æ­£è¦åŒ–</li>
<li>[ ] è·é›¢ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆKNNã€SVMï¼‰ã§ç‰¹ã«é‡è¦</li>
<li>
<p>[ ] æœ¨ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆRFã€XGBoostï¼‰ã§ã¯ä¸è¦ã ãŒæ¨å¥¨</p>
</li>
<li>
<p>[ ] <strong>æƒ…å ±æå¤±ã®ç›£è¦–</strong></p>
</li>
<li>[ ] PCAå¯„ä¸ç‡ã‚’è¤‡æ•°ï¼ˆ90%, 95%, 99%ï¼‰ã§æ¯”è¼ƒ</li>
<li>[ ] æ¬¡å…ƒå‰Šæ¸›å‰å¾Œã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’è©•ä¾¡</li>
<li>
<p>[ ] å‰Šæ¸›ã—ã™ãã§æ€§èƒ½ä½ä¸‹ãªã‚‰æ¬¡å…ƒæ•°ã‚’å¢—ã‚„ã™</p>
</li>
<li>
<p>[ ] <strong>matminerãƒ—ãƒªã‚»ãƒƒãƒˆã®åŸå‘³</strong></p>
</li>
<li>[ ] magpieï¼ˆ132ç‰¹å¾´é‡ï¼‰ã¯å¤šã™ãã‚‹å ´åˆã‚ã‚Š</li>
<li>[ ] ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã§é–¢é€£ç‰¹å¾´é‡ã‚’é¸å®š</li>
<li>
<p>[ ] ç›¸é–¢è¡Œåˆ—ã§å†—é•·ãªç‰¹å¾´é‡ã‚’é™¤å»</p>
</li>
<li>
<p>[ ] <strong>çµ„æˆè¦æ ¼åŒ–</strong></p>
</li>
<li>[ ] çµ„æˆåˆè¨ˆãŒ1ã«ãªã‚‹ã‚ˆã†æ­£è¦åŒ–</li>
<li>[ ] åŒ–å­¦å¼è¡¨è¨˜ã®æºã‚Œï¼ˆLiâ‚€.â‚‰CoOâ‚‚ vs LiCoOâ‚‚ï¼‰ã‚’çµ±ä¸€</li>
<li>[ ] Composition.fractional_compositionä½¿ç”¨</li>
</ul>
<h3>ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ´»ç”¨</h3>
<ul>
<li>[ ] <strong>Matbench</strong></li>
<li>[ ] 13ç¨®é¡ã®ã‚¿ã‚¹ã‚¯ã§æ¨™æº–çš„æ€§èƒ½è©•ä¾¡</li>
<li>[ ] è«–æ–‡æ¯”è¼ƒæ™‚ã®å…±é€šåŸºæº–</li>
<li>
<p>[ ] ãƒ¢ãƒ‡ãƒ«é¸æŠã®å¦¥å½“æ€§æ¤œè¨¼</p>
</li>
<li>
<p>[ ] <strong>JARVIS-DFT</strong></p>
</li>
<li>[ ] 55,000ææ–™ã®DFTè¨ˆç®—ãƒ‡ãƒ¼ã‚¿</li>
<li>[ ] GNNã‚„Transformerã®è¨“ç·´ã«æœ€é©</li>
<li>
<p>[ ] æ§‹é€ è¨˜è¿°å­ã®è±Šå¯Œã•ãŒç‰¹å¾´</p>
</li>
<li>
<p>[ ] <strong>å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</strong></p>
</li>
<li>[ ] Expt Gapï¼ˆå®Ÿé¨“ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ— 6,354ææ–™ï¼‰</li>
<li>[ ] DFT vs å®Ÿé¨“ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’æ¤œè¨¼</li>
<li>[ ] ãƒ¢ãƒ‡ãƒ«ã®å®Ÿç”¨æ€§è©•ä¾¡</li>
</ul>
<h3>ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å“è³ªæŒ‡æ¨™</h3>
<ul>
<li>[ ] <strong>ç‰¹å¾´é‡ã®è³ª</strong></li>
<li>[ ] æ¬ æç‡ &lt; 5%ï¼ˆç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«ï¼‰</li>
<li>[ ] åˆ†æ•£ &gt; 0.01ï¼ˆä½åˆ†æ•£ç‰¹å¾´é‡ã‚’é™¤å»ï¼‰</li>
<li>
<p>[ ] ç›¸é–¢ä¿‚æ•° |r| &lt; 0.9ï¼ˆé«˜ç›¸é–¢ãƒšã‚¢ã‚’å‰Šæ¸›ï¼‰</p>
</li>
<li>
<p>[ ] <strong>æ¬¡å…ƒå‰Šæ¸›ã®åŠ¹æœ</strong></p>
</li>
<li>[ ] PCAå¯„ä¸ç‡ â‰¥ 90%</li>
<li>[ ] æ¬¡å…ƒå‰Šæ¸›ç‡ 50-90%</li>
<li>
<p>[ ] æ€§èƒ½ç¶­æŒç‡ â‰¥ 95%ï¼ˆMAEã€RÂ²ãªã©ï¼‰</p>
</li>
<li>
<p>[ ] <strong>ç‰¹å¾´é‡é¸æŠã®åŠ¹æœ</strong></p>
</li>
<li>[ ] é¸æŠå¾Œã®ç‰¹å¾´é‡æ•° â‰¤ 20%ï¼ˆå…ƒã®æ•°ï¼‰</li>
<li>[ ] æ€§èƒ½æ”¹å–„ã¾ãŸã¯ç¶­æŒï¼ˆéå­¦ç¿’é˜²æ­¢ï¼‰</li>
<li>[ ] è¨“ç·´æ™‚é–“çŸ­ç¸® â‰¥ 30%</li>
</ul>
<h3>å†ç¾æ€§ã®ç¢ºä¿</h3>
<ul>
<li>[ ] <strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†</strong></li>
<li>[ ] matminerã€pymatgenã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ˜è¨˜</li>
<li>[ ] ãƒ—ãƒªã‚»ãƒƒãƒˆåï¼ˆmagpieã€demlãªã©ï¼‰ã‚’è¨˜éŒ²</li>
<li>
<p>[ ] ç‰¹å¾´é‡ç”Ÿæˆæ—¥æ™‚ã‚’è¨˜éŒ²</p>
</li>
<li>
<p>[ ] <strong>ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã®ä¿å­˜</strong></p>
</li>
<li>[ ] ç”Ÿæˆã•ã‚ŒãŸç‰¹å¾´é‡åã‚’å…¨ã¦CSVä¿å­˜</li>
<li>[ ] ç‰¹å¾´é‡ã®æ„å‘³ï¼ˆèª¬æ˜ï¼‰ã‚’ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–</li>
<li>
<p>[ ] é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã‚‚è¨˜éŒ²</p>
</li>
<li>
<p>[ ] <strong>å¤‰æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¿å­˜</strong></p>
</li>
<li>[ ] StandardScalerã®meanã€stdã‚’pickleä¿å­˜</li>
<li>[ ] PCAã®components_ã‚’ä¿å­˜</li>
<li>[ ] æ–°ãƒ‡ãƒ¼ã‚¿ã¸ã®é©ç”¨æ™‚ã«åŒã˜å¤‰æ›ã‚’é©ç”¨</li>
</ul>
<hr />
<h2>å‚è€ƒæ–‡çŒ®</h2>
<ol>
<li>
<p><strong>Ward, L., Dunn, A., Faghaninia, A., et al.</strong> (2018). Matminer: An open source toolkit for materials data mining. <em>Computational Materials Science</em>, 152, 60-69. <a href="https://doi.org/10.1016/j.commatsci.2018.05.018">DOI: 10.1016/j.commatsci.2018.05.018</a></p>
</li>
<li>
<p><strong>Jolliffe, I. T. &amp; Cadima, J.</strong> (2016). Principal component analysis: a review and recent developments. <em>Philosophical Transactions of the Royal Society A</em>, 374(2065), 20150202. <a href="https://doi.org/10.1098/rsta.2015.0202">DOI: 10.1098/rsta.2015.0202</a></p>
</li>
<li>
<p><strong>McInnes, L., Healy, J., &amp; Melville, J.</strong> (2018). UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. <em>arXiv preprint arXiv:1802.03426</em>.</p>
</li>
<li>
<p><strong>Guyon, I. &amp; Elisseeff, A.</strong> (2003). An introduction to variable and feature selection. <em>Journal of Machine Learning Research</em>, 3, 1157-1182.</p>
</li>
</ol>
<hr />
<p><a href="chapter-1.html">â† Chapter 1ã«æˆ»ã‚‹</a> | <a href="chapter-3.html">Chapter 3ã¸é€²ã‚€ â†’</a></p><div class="navigation">
    <a href="chapter-1.html" class="nav-button">â† å‰ã®ç« </a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter-3.html" class="nav-button">æ¬¡ã®ç«  â†’</a>
</div>
    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
