<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Chapter</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 20-25ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ÂàùÁ¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 0ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 0Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 2: ÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞</h1>
<hr />
<h2>Â≠¶ÁøíÁõÆÊ®ô</h2>
<p>„Åì„ÅÆÁ´†„ÇíË™≠„ÇÄ„Åì„Å®„Åß„ÄÅ‰ª•‰∏ã„ÇíÁøíÂæó„Åß„Åç„Åæ„ÅôÔºö</p>
<p>‚úÖ ÊùêÊñôË®òËø∞Â≠êÔºàÁµÑÊàê„ÉªÊßãÈÄ†„ÉªÈõªÂ≠êÊßãÈÄ†Ôºâ„ÅÆÈÅ∏Êäû„Å®Ë®≠Ë®à
‚úÖ matminer„ÇíÊ¥ªÁî®„Åó„ÅüÊùêÊñôÁâπÂæ¥Èáè„ÅÆËá™ÂãïÁîüÊàê
‚úÖ ÁâπÂæ¥ÈáèÂ§âÊèõÔºàÊ≠£Ë¶èÂåñ„ÄÅÂØæÊï∞Â§âÊèõ„ÄÅÂ§öÈ†ÖÂºèÁâπÂæ¥ÈáèÔºâ„ÅÆÂÆüË∑µ
‚úÖ Ê¨°ÂÖÉÂâäÊ∏õÔºàPCA„ÄÅt-SNE„ÄÅUMAPÔºâ„Å´„Çà„ÇãÂèØË¶ñÂåñ„Å®Ëß£Èáà
‚úÖ ÁâπÂæ¥ÈáèÈÅ∏ÊäûÔºàFilter/Wrapper/Embedded/SHAP-basedÔºâ„ÅÆ‰Ωø„ÅÑÂàÜ„Åë
‚úÖ „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó‰∫àÊ∏¨„Å´„Åä„Åë„Çã200Ê¨°ÂÖÉ‚Üí20Ê¨°ÂÖÉ„Å∏„ÅÆÂäπÊûúÁöÑÂâäÊ∏õ</p>
<hr />
<h2>2.1 ÊùêÊñôË®òËø∞Â≠ê„ÅÆÈÅ∏Êäû„Å®Ë®≠Ë®à</h2>
<p>ÊùêÊñô„ÅÆÊÄßË≥™„ÇíÊ©üÊ¢∞Â≠¶Áøí„Åß‰∫àÊ∏¨„Åô„Çã„Å´„ÅØ„ÄÅÈÅ©Âàá„Å™<strong>ÊùêÊñôË®òËø∞Â≠êÔºàMaterial DescriptorsÔºâ</strong>„ÅåÂøÖË¶Å„Åß„Åô„ÄÇ</p>
<h3>ÁµÑÊàêË®òËø∞Â≠ê</h3>
<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def calculate_composition_descriptors(formula_dict):
    &quot;&quot;&quot;
    ÁµÑÊàêË®òËø∞Â≠ê„ÅÆË®àÁÆó

    Parameters:
    -----------
    formula_dict : dict
        {'ÂÖÉÁ¥†Ë®òÂè∑': Ââ≤Âêà} ‰æã: {'Fe': 0.7, 'Ni': 0.3}

    Returns:
    --------
    dict : ÁµÑÊàêË®òËø∞Â≠ê
    &quot;&quot;&quot;
    # ÂÖÉÁ¥†„ÅÆÁâ©ÊÄßÂÄ§ÔºàÁ∞°Áï•ÁâàÔºâ
    element_properties = {
        'Fe': {'atomic_mass': 55.845, 'electronegativity': 1.83,
               'atomic_radius': 1.26},
        'Ni': {'atomic_mass': 58.693, 'electronegativity': 1.91,
               'atomic_radius': 1.24},
        'Cu': {'atomic_mass': 63.546, 'electronegativity': 1.90,
               'atomic_radius': 1.28},
        'Zn': {'atomic_mass': 65.38, 'electronegativity': 1.65,
               'atomic_radius': 1.34}
    }

    descriptors = {}

    # Âπ≥ÂùáÂéüÂ≠êÈáè
    avg_mass = sum(
        element_properties[el]['atomic_mass'] * frac
        for el, frac in formula_dict.items()
    )
    descriptors['Âπ≥ÂùáÂéüÂ≠êÈáè'] = avg_mass

    # Âπ≥ÂùáÈõªÊ∞óÈô∞ÊÄßÂ∫¶
    avg_electronegativity = sum(
        element_properties[el]['electronegativity'] * frac
        for el, frac in formula_dict.items()
    )
    descriptors['Âπ≥ÂùáÈõªÊ∞óÈô∞ÊÄßÂ∫¶'] = avg_electronegativity

    # ÈõªÊ∞óÈô∞ÊÄßÂ∫¶Â∑ÆÔºàÊúÄÂ§ß - ÊúÄÂ∞èÔºâ
    electronegativities = [
        element_properties[el]['electronegativity']
        for el in formula_dict.keys()
    ]
    descriptors['ÈõªÊ∞óÈô∞ÊÄßÂ∫¶Â∑Æ'] = max(electronegativities) - min(electronegativities)

    # Âπ≥ÂùáÂéüÂ≠êÂçäÂæÑ
    avg_radius = sum(
        element_properties[el]['atomic_radius'] * frac
        for el, frac in formula_dict.items()
    )
    descriptors['Âπ≥ÂùáÂéüÂ≠êÂçäÂæÑ'] = avg_radius

    return descriptors

# ‰æãÔºöFe-NiÂêàÈáë
formula = {'Fe': 0.7, 'Ni': 0.3}
descriptors = calculate_composition_descriptors(formula)

print(&quot;ÁµÑÊàêË®òËø∞Â≠êÔºàFe‚ÇÄ.‚ÇáNi‚ÇÄ.‚ÇÉÔºâÔºö&quot;)
for key, value in descriptors.items():
    print(f&quot;  {key}: {value:.4f}&quot;)
</code></pre>
<p><strong>Âá∫Âäõ</strong>Ôºö</p>
<pre><code>ÁµÑÊàêË®òËø∞Â≠êÔºàFe‚ÇÄ.‚ÇáNi‚ÇÄ.‚ÇÉÔºâÔºö
  Âπ≥ÂùáÂéüÂ≠êÈáè: 56.6984
  Âπ≥ÂùáÈõªÊ∞óÈô∞ÊÄßÂ∫¶: 1.8540
  ÈõªÊ∞óÈô∞ÊÄßÂ∫¶Â∑Æ: 0.0800
  Âπ≥ÂùáÂéüÂ≠êÂçäÂæÑ: 1.2540
</code></pre>
<h3>matminer„ÅÆÊ¥ªÁî®</h3>
<pre><code class="language-python"># matminer„Å´„Çà„ÇãÊùêÊñôË®òËø∞Â≠ê„ÅÆËá™ÂãïÁîüÊàê
# !pip install matminer pymatgen

from matminer.featurizers.composition import (
    ElementProperty,
    Stoichiometry,
    ValenceOrbital,
    IonProperty
)
from pymatgen.core import Composition

def generate_matminer_features(formula_str):
    &quot;&quot;&quot;
    matminer„ÅßÊùêÊñôË®òËø∞Â≠ê„ÇíÁîüÊàê

    Parameters:
    -----------
    formula_str : str
        ÂåñÂ≠¶ÂºèÔºà‰æã: &quot;Fe2O3&quot;Ôºâ

    Returns:
    --------
    pd.DataFrame : ÁâπÂæ¥Èáè
    &quot;&quot;&quot;
    comp = Composition(formula_str)

    # ÂÖÉÁ¥†Áâ©ÊÄßË®òËø∞Â≠ê
    ep_feat = ElementProperty.from_preset(&quot;magpie&quot;)
    features_ep = ep_feat.featurize(comp)

    # ÂåñÂ≠¶ÈáèË´ñË®òËø∞Â≠ê
    stoich_feat = Stoichiometry()
    features_stoich = stoich_feat.featurize(comp)

    # ‰æ°ÈõªÂ≠êËªåÈÅìË®òËø∞Â≠ê
    valence_feat = ValenceOrbital()
    features_valence = valence_feat.featurize(comp)

    # ÁâπÂæ¥ÈáèÂêçÂèñÂæó
    feature_names = (
        ep_feat.feature_labels() +
        stoich_feat.feature_labels() +
        valence_feat.feature_labels()
    )

    # DataFrame„Å´Â§âÊèõ
    all_features = features_ep + features_stoich + features_valence
    df = pd.DataFrame([all_features], columns=feature_names)

    return df

# ‰æãÔºöÈÖ∏ÂåñÈâÑ
formula = &quot;Fe2O3&quot;
features = generate_matminer_features(formula)

print(f&quot;matminer„Å´„Çà„ÇãÁâπÂæ¥ÈáèÁîüÊàêÔºà{formula}ÔºâÔºö&quot;)
print(f&quot;ÁâπÂæ¥ÈáèÊï∞: {features.shape[1]}&quot;)
print(f&quot;\nÊúÄÂàù„ÅÆ10ÁâπÂæ¥ÈáèÔºö&quot;)
print(features.iloc[:, :10].T)
</code></pre>
<p><strong>matminer„ÅÆ‰∏ª„Å™Ë®òËø∞Â≠ê</strong>Ôºö</p>
<pre><code class="language-python"># Ë®òËø∞Â≠ê„Çø„Ç§„Éó„ÅÆÊØîËºÉ
descriptor_types = pd.DataFrame({
    'Ë®òËø∞Â≠ê„Çø„Ç§„Éó': [
        'ElementProperty',
        'Stoichiometry',
        'ValenceOrbital',
        'IonProperty',
        'OxidationStates',
        'ElectronAffinity'
    ],
    'ÁâπÂæ¥ÈáèÊï∞': [132, 7, 10, 32, 3, 1],
    'Áî®ÈÄî': [
        'ÂÖÉÁ¥†„ÅÆÁâ©ÁêÜÂåñÂ≠¶ÁöÑÊÄßË≥™',
        'ÂåñÂ≠¶ÈáèË´ñÊØî',
        '‰æ°ÈõªÂ≠êËªåÈÅì',
        '„Ç§„Ç™„É≥ÁâπÊÄß',
        'ÈÖ∏ÂåñÁä∂ÊÖã',
        'ÈõªÂ≠êË¶™ÂíåÂäõ'
    ]
})

# ÂèØË¶ñÂåñ
fig, ax = plt.subplots(figsize=(10, 6))
ax.barh(descriptor_types['Ë®òËø∞Â≠ê„Çø„Ç§„Éó'],
        descriptor_types['ÁâπÂæ¥ÈáèÊï∞'],
        color='steelblue', alpha=0.7)
ax.set_xlabel('ÁâπÂæ¥ÈáèÊï∞', fontsize=12)
ax.set_title('matminer„ÅÆË®òËø∞Â≠ê„Çø„Ç§„Éó', fontsize=13, fontweight='bold')
ax.grid(axis='x', alpha=0.3)

for idx, row in descriptor_types.iterrows():
    ax.text(row['ÁâπÂæ¥ÈáèÊï∞'] + 5, idx, row['Áî®ÈÄî'],
            va='center', fontsize=9, style='italic')

plt.tight_layout()
plt.show()

print(descriptor_types.to_string(index=False))
</code></pre>
<h3>ÊßãÈÄ†Ë®òËø∞Â≠ê</h3>
<pre><code class="language-python">def calculate_structure_descriptors(lattice_params):
    &quot;&quot;&quot;
    ÁµêÊô∂ÊßãÈÄ†Ë®òËø∞Â≠ê

    Parameters:
    -----------
    lattice_params : dict
        {'a': float, 'b': float, 'c': float,
         'alpha': float, 'beta': float, 'gamma': float}

    Returns:
    --------
    dict : ÊßãÈÄ†Ë®òËø∞Â≠ê
    &quot;&quot;&quot;
    a = lattice_params['a']
    b = lattice_params['b']
    c = lattice_params['c']
    alpha = np.radians(lattice_params['alpha'])
    beta = np.radians(lattice_params['beta'])
    gamma = np.radians(lattice_params['gamma'])

    # ‰ΩìÁ©ç
    volume = a * b * c * np.sqrt(
        1 - np.cos(alpha)**2 - np.cos(beta)**2 - np.cos(gamma)**2 +
        2 * np.cos(alpha) * np.cos(beta) * np.cos(gamma)
    )

    # „Éë„ÉÉ„Ç≠„É≥„Ç∞ÂØÜÂ∫¶ÔºàÁ∞°Áï•ÂåñÔºâ
    packing_density = 0.74  # ‰æãÔºöFCC„ÅÆÂ†¥Âêà

    descriptors = {
        'Ê†ºÂ≠êÂÆöÊï∞a': a,
        'Ê†ºÂ≠êÂÆöÊï∞b': b,
        'Ê†ºÂ≠êÂÆöÊï∞c': c,
        '‰ΩìÁ©ç': volume,
        '„Éë„ÉÉ„Ç≠„É≥„Ç∞ÂØÜÂ∫¶': packing_density
    }

    return descriptors

# ‰æãÔºöÁ´ãÊñπÊô∂
lattice = {'a': 5.43, 'b': 5.43, 'c': 5.43,
           'alpha': 90, 'beta': 90, 'gamma': 90}
struct_desc = calculate_structure_descriptors(lattice)

print(&quot;ÊßãÈÄ†Ë®òËø∞Â≠êÔºàÁ´ãÊñπÊô∂ÔºâÔºö&quot;)
for key, value in struct_desc.items():
    print(f&quot;  {key}: {value:.4f}&quot;)
</code></pre>
<h3>ÈõªÂ≠êÊßãÈÄ†Ë®òËø∞Â≠ê</h3>
<pre><code class="language-python"># ÈõªÂ≠êÊßãÈÄ†Ë®òËø∞Â≠ê„ÅÆ„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥
def simulate_electronic_descriptors(n_samples=100):
    &quot;&quot;&quot;
    ÈõªÂ≠êÊßãÈÄ†Ë®òËø∞Â≠ê„ÅÆ„Çµ„É≥„Éó„É´„Éá„Éº„ÇøÁîüÊàê
    &quot;&quot;&quot;
    np.random.seed(42)

    data = pd.DataFrame({
        '„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó': np.random.uniform(0, 5, n_samples),
        '„Éï„Çß„É´„Éü„Ç®„Éç„É´„ÇÆ„Éº': np.random.uniform(-5, 5, n_samples),
        'Áä∂ÊÖãÂØÜÂ∫¶_‰æ°ÈõªÂ≠êÂ∏Ø': np.random.uniform(10, 100, n_samples),
        'Áä∂ÊÖãÂØÜÂ∫¶_‰ºùÂ∞éÂ∏Ø': np.random.uniform(5, 50, n_samples),
        'ÊúâÂäπË≥™Èáè_ÈõªÂ≠ê': np.random.uniform(0.1, 2, n_samples),
        'ÊúâÂäπË≥™Èáè_Ê≠£Â≠î': np.random.uniform(0.1, 2, n_samples)
    })

    return data

# ÁîüÊàê
electronic_data = simulate_electronic_descriptors(100)

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

for idx, col in enumerate(electronic_data.columns):
    axes[idx].hist(electronic_data[col], bins=20,
                   color='steelblue', alpha=0.7, edgecolor='black')
    axes[idx].set_xlabel(col, fontsize=11)
    axes[idx].set_ylabel('È†ªÂ∫¶', fontsize=11)
    axes[idx].set_title(f'{col}„ÅÆÂàÜÂ∏É', fontsize=12, fontweight='bold')
    axes[idx].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;ÈõªÂ≠êÊßãÈÄ†Ë®òËø∞Â≠ê„ÅÆÁµ±Ë®àÔºö&quot;)
print(electronic_data.describe())
</code></pre>
<hr />
<h2>2.2 ÁâπÂæ¥ÈáèÂ§âÊèõ</h2>
<p>Áîü„ÅÆÁâπÂæ¥Èáè„ÇíÊ©üÊ¢∞Â≠¶Áøí„É¢„Éá„É´„Å´ÈÅ©„Åó„ÅüÂΩ¢„Å´Â§âÊèõ„Åó„Åæ„Åô„ÄÇ</p>
<h3>Ê≠£Ë¶èÂåñÔºàMin-Max, Z-scoreÔºâ</h3>
<pre><code class="language-python">from sklearn.preprocessing import MinMaxScaler, StandardScaler

def compare_normalization(data):
    &quot;&quot;&quot;
    Ê≠£Ë¶èÂåñÊâãÊ≥ï„ÅÆÊØîËºÉ
    &quot;&quot;&quot;
    # Min-MaxÊ≠£Ë¶èÂåñÔºà0-1Ôºâ
    minmax_scaler = MinMaxScaler()
    data_minmax = pd.DataFrame(
        minmax_scaler.fit_transform(data),
        columns=data.columns
    )

    # Z-scoreÊ≠£Ë¶èÂåñÔºàÂπ≥Âùá0„ÄÅÊ®ôÊ∫ñÂÅèÂ∑Æ1Ôºâ
    standard_scaler = StandardScaler()
    data_standard = pd.DataFrame(
        standard_scaler.fit_transform(data),
        columns=data.columns
    )

    return data_minmax, data_standard

# „Çµ„É≥„Éó„É´„Éá„Éº„Çø
np.random.seed(42)
sample_data = pd.DataFrame({
    'Ê†ºÂ≠êÂÆöÊï∞': np.random.uniform(3, 7, 100),
    'ÈõªÊ∞ó‰ºùÂ∞éÂ∫¶': np.random.lognormal(10, 2, 100),
    '„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó': np.random.uniform(0, 3, 100)
})

# Ê≠£Ë¶èÂåñ
data_minmax, data_standard = compare_normalization(sample_data)

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(3, 3, figsize=(15, 12))

for idx, col in enumerate(sample_data.columns):
    # ÂÖÉ„Éá„Éº„Çø
    axes[idx, 0].hist(sample_data[col], bins=20,
                      color='gray', alpha=0.7, edgecolor='black')
    axes[idx, 0].set_title(f'ÂÖÉ„Éá„Éº„Çø: {col}', fontsize=11, fontweight='bold')
    axes[idx, 0].set_ylabel('È†ªÂ∫¶', fontsize=10)

    # Min-Max
    axes[idx, 1].hist(data_minmax[col], bins=20,
                      color='steelblue', alpha=0.7, edgecolor='black')
    axes[idx, 1].set_title(f'Min-Max: {col}', fontsize=11, fontweight='bold')

    # Z-score
    axes[idx, 2].hist(data_standard[col], bins=20,
                      color='coral', alpha=0.7, edgecolor='black')
    axes[idx, 2].set_title(f'Z-score: {col}', fontsize=11, fontweight='bold')

plt.tight_layout()
plt.show()

print(&quot;Ê≠£Ë¶èÂåñÂæå„ÅÆÁµ±Ë®àÔºö&quot;)
print(&quot;\nMin-MaxÊ≠£Ë¶èÂåñÔºö&quot;)
print(data_minmax.describe())
print(&quot;\nZ-scoreÊ≠£Ë¶èÂåñÔºö&quot;)
print(data_standard.describe())
</code></pre>
<h3>ÂØæÊï∞Â§âÊèõ„ÄÅBox-CoxÂ§âÊèõ</h3>
<pre><code class="language-python">from scipy.stats import boxcox

def apply_transformations(data):
    &quot;&quot;&quot;
    ÂêÑÁ®ÆÂ§âÊèõ„ÅÆÈÅ©Áî®
    &quot;&quot;&quot;
    # ÂØæÊï∞Â§âÊèõ
    data_log = np.log1p(data)  # log(1+x)„Åß0„ÇíÊâ±„Åà„Çã

    # Box-CoxÂ§âÊèõÔºàÊ≠£ÂÄ§„ÅÆ„ÅøÔºâ
    data_boxcox, lambda_param = boxcox(data + 1)  # +1„Åß„Çº„É≠„ÇíÂõûÈÅø

    return data_log, data_boxcox, lambda_param

# ÂÅè„Å£„Åü„Éá„Éº„ÇøÔºàÈõªÊ∞ó‰ºùÂ∞éÂ∫¶„Å™„Å©Ôºâ
np.random.seed(42)
conductivity = np.random.lognormal(10, 2, 100)

# Â§âÊèõ
cond_log, cond_boxcox, lambda_val = apply_transformations(conductivity)

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# ÂÖÉ„Éá„Éº„Çø
axes[0].hist(conductivity, bins=30, color='gray',
             alpha=0.7, edgecolor='black')
axes[0].set_xlabel('ÈõªÊ∞ó‰ºùÂ∞éÂ∫¶ (S/m)', fontsize=11)
axes[0].set_ylabel('È†ªÂ∫¶', fontsize=11)
axes[0].set_title('ÂÖÉ„Éá„Éº„ÇøÔºàÊ≠™Â∫¶„ÅÇ„ÇäÔºâ', fontsize=12, fontweight='bold')

# ÂØæÊï∞Â§âÊèõ
axes[1].hist(cond_log, bins=30, color='steelblue',
             alpha=0.7, edgecolor='black')
axes[1].set_xlabel('log(ÈõªÊ∞ó‰ºùÂ∞éÂ∫¶+1)', fontsize=11)
axes[1].set_ylabel('È†ªÂ∫¶', fontsize=11)
axes[1].set_title('ÂØæÊï∞Â§âÊèõ', fontsize=12, fontweight='bold')

# Box-CoxÂ§âÊèõ
axes[2].hist(cond_boxcox, bins=30, color='coral',
             alpha=0.7, edgecolor='black')
axes[2].set_xlabel(f'Box-Cox (Œª={lambda_val:.3f})', fontsize=11)
axes[2].set_ylabel('È†ªÂ∫¶', fontsize=11)
axes[2].set_title('Box-CoxÂ§âÊèõ', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.show()

# Ê≠™Â∫¶„ÅÆÊØîËºÉ
from scipy.stats import skew
print(f&quot;ÂÖÉ„Éá„Éº„Çø„ÅÆÊ≠™Â∫¶: {skew(conductivity):.3f}&quot;)
print(f&quot;ÂØæÊï∞Â§âÊèõÂæå„ÅÆÊ≠™Â∫¶: {skew(cond_log):.3f}&quot;)
print(f&quot;Box-CoxÂ§âÊèõÂæå„ÅÆÊ≠™Â∫¶: {skew(cond_boxcox):.3f}&quot;)
</code></pre>
<h3>Â§öÈ†ÖÂºèÁâπÂæ¥Èáè</h3>
<pre><code class="language-python">from sklearn.preprocessing import PolynomialFeatures

def create_polynomial_features(X, degree=2):
    &quot;&quot;&quot;
    Â§öÈ†ÖÂºèÁâπÂæ¥Èáè„ÅÆÁîüÊàê

    Parameters:
    -----------
    X : array-like, shape (n_samples, n_features)
    degree : int
        Â§öÈ†ÖÂºè„ÅÆÊ¨°Êï∞

    Returns:
    --------
    X_poly : Â§öÈ†ÖÂºèÁâπÂæ¥Èáè
    feature_names : ÁâπÂæ¥ÈáèÂêç
    &quot;&quot;&quot;
    poly = PolynomialFeatures(degree=degree, include_bias=False)
    X_poly = poly.fit_transform(X)
    feature_names = poly.get_feature_names_out()

    return X_poly, feature_names

# „Çµ„É≥„Éó„É´„Éá„Éº„Çø
np.random.seed(42)
X_original = pd.DataFrame({
    'x1': np.random.uniform(0, 1, 50),
    'x2': np.random.uniform(0, 1, 50)
})

# 2Ê¨°Â§öÈ†ÖÂºèÁâπÂæ¥Èáè
X_poly, feature_names = create_polynomial_features(
    X_original.values, degree=2
)

print(f&quot;ÂÖÉ„ÅÆÁâπÂæ¥ÈáèÊï∞: {X_original.shape[1]}&quot;)
print(f&quot;Â§öÈ†ÖÂºèÁâπÂæ¥ÈáèÊï∞: {X_poly.shape[1]}&quot;)
print(f&quot;\nÁîüÊàê„Åï„Çå„ÅüÁâπÂæ¥ÈáèÔºö&quot;)
for name in feature_names:
    print(f&quot;  {name}&quot;)

# ÈùûÁ∑öÂΩ¢Èñ¢‰øÇ„ÅÆÂ≠¶Áøí‰æã
# y = 2*x1^2 + 3*x1*x2 - x2^2 + noise
y_true = (
    2 * X_original['x1']**2 +
    3 * X_original['x1'] * X_original['x2'] -
    X_original['x2']**2 +
    np.random.normal(0, 0.1, 50)
)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Á∑öÂΩ¢„É¢„Éá„É´ÔºàÂÖÉ„ÅÆÁâπÂæ¥ÈáèÔºâ
model_linear = LinearRegression()
model_linear.fit(X_original, y_true)
y_pred_linear = model_linear.predict(X_original)
r2_linear = r2_score(y_true, y_pred_linear)

# Á∑öÂΩ¢„É¢„Éá„É´ÔºàÂ§öÈ†ÖÂºèÁâπÂæ¥ÈáèÔºâ
model_poly = LinearRegression()
model_poly.fit(X_poly, y_true)
y_pred_poly = model_poly.predict(X_poly)
r2_poly = r2_score(y_true, y_pred_poly)

print(f&quot;\nÁ∑öÂΩ¢„É¢„Éá„É´ÔºàÂÖÉÁâπÂæ¥ÈáèÔºâR¬≤: {r2_linear:.4f}&quot;)
print(f&quot;Á∑öÂΩ¢„É¢„Éá„É´ÔºàÂ§öÈ†ÖÂºèÁâπÂæ¥ÈáèÔºâR¬≤: {r2_poly:.4f}&quot;)
print(f&quot;ÊîπÂñÑÁéá: {(r2_poly - r2_linear) / r2_linear * 100:.1f}%&quot;)
</code></pre>
<h3>‰∫§‰∫í‰ΩúÁî®È†Ö„ÅÆÁîüÊàê</h3>
<pre><code class="language-python">from itertools import combinations

def create_interaction_features(df):
    &quot;&quot;&quot;
    ‰∫§‰∫í‰ΩúÁî®È†Ö„ÅÆÁîüÊàê

    Parameters:
    -----------
    df : pd.DataFrame
        ÂÖÉ„ÅÆÁâπÂæ¥Èáè

    Returns:
    --------
    df_with_interactions : ‰∫§‰∫í‰ΩúÁî®È†Ö„ÇíËøΩÂä†„Åó„ÅüDataFrame
    &quot;&quot;&quot;
    df_new = df.copy()

    # 2Â§âÊï∞„ÅÆ‰∫§‰∫í‰ΩúÁî®ÔºàÁ©çÔºâ
    for col1, col2 in combinations(df.columns, 2):
        interaction_name = f&quot;{col1}√ó{col2}&quot;
        df_new[interaction_name] = df[col1] * df[col2]

    return df_new

# ‰æãÔºöÁÜ±ÈõªÊùêÊñô„ÅÆÁâπÂæ¥Èáè
thermoelectric_features = pd.DataFrame({
    'ÈõªÊ∞ó‰ºùÂ∞éÂ∫¶': np.random.lognormal(8, 1, 50),
    '„Çº„Éº„Éô„ÉÉ„ÇØ‰øÇÊï∞': np.random.normal(200, 50, 50),
    'ÁÜ±‰ºùÂ∞éÂ∫¶': np.random.uniform(1, 10, 50)
})

# ‰∫§‰∫í‰ΩúÁî®È†ÖËøΩÂä†
features_with_interactions = create_interaction_features(
    thermoelectric_features
)

print(f&quot;ÂÖÉ„ÅÆÁâπÂæ¥Èáè: {thermoelectric_features.columns.tolist()}&quot;)
print(f&quot;\nËøΩÂä†„Åï„Çå„Åü‰∫§‰∫í‰ΩúÁî®È†Ö:&quot;)
new_cols = [col for col in features_with_interactions.columns
            if col not in thermoelectric_features.columns]
for col in new_cols:
    print(f&quot;  {col}&quot;)

# ZTÂÄ§‰∫àÊ∏¨ÔºàZT = œÉ*S¬≤/Œ∫ „Å´Ëøë„ÅÑÔºâ
thermoelectric_features['ZT'] = (
    thermoelectric_features['ÈõªÊ∞ó‰ºùÂ∞éÂ∫¶'] *
    thermoelectric_features['„Çº„Éº„Éô„ÉÉ„ÇØ‰øÇÊï∞']**2 /
    thermoelectric_features['ÁÜ±‰ºùÂ∞éÂ∫¶'] / 1e6 +
    np.random.normal(0, 0.1, 50)
)

print(f&quot;\nÁõ∏Èñ¢ÂàÜÊûêÔºà‰∫§‰∫í‰ΩúÁî®È†Ö„Å®„ÅÆÁõ∏Èñ¢ÔºâÔºö&quot;)
correlations = features_with_interactions.corrwith(
    thermoelectric_features['ZT']
).sort_values(ascending=False)
print(correlations)
</code></pre>
<hr />
<h2>2.3 Ê¨°ÂÖÉÂâäÊ∏õ</h2>
<p>È´òÊ¨°ÂÖÉ„Éá„Éº„Çø„Çí‰ΩéÊ¨°ÂÖÉ„Å´ÂúßÁ∏Æ„Åó„ÄÅÂèØË¶ñÂåñ„ÉªËß£Èáà„ÇíÂÆπÊòì„Å´„Åó„Åæ„Åô„ÄÇ</p>
<h3>PCA (Principal Component Analysis)</h3>
<pre><code class="language-python">from sklearn.decomposition import PCA

def apply_pca(X, n_components=2):
    &quot;&quot;&quot;
    PCA„Å´„Çà„ÇãÊ¨°ÂÖÉÂâäÊ∏õ

    Parameters:
    -----------
    X : array-like
        ÂÖÉ„ÅÆÁâπÂæ¥Èáè
    n_components : int
        ÂâäÊ∏õÂæå„ÅÆÊ¨°ÂÖÉÊï∞

    Returns:
    --------
    X_pca : ‰∏ªÊàêÂàÜÂæóÁÇπ
    pca : PC„Åä object
    &quot;&quot;&quot;
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X)

    return X_pca, pca

# È´òÊ¨°ÂÖÉ„Éá„Éº„ÇøÁîüÊàêÔºà100Ê¨°ÂÖÉÔºâ
np.random.seed(42)
n_samples = 200
n_features = 100

# ÊΩúÂú®ÁöÑ„Å™2Ê¨°ÂÖÉÊßãÈÄ†„ÇíÊåÅ„Å§„Éá„Éº„Çø
latent = np.random.randn(n_samples, 2)
X_high_dim = latent @ np.random.randn(2, n_features) + np.random.randn(n_samples, n_features) * 0.5

# PCAÈÅ©Áî®
X_pca, pca_model = apply_pca(X_high_dim, n_components=10)

# ÂØÑ‰∏éÁéá
explained_var = pca_model.explained_variance_ratio_

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# ÂØÑ‰∏éÁéá
axes[0].bar(range(1, 11), explained_var * 100,
            color='steelblue', alpha=0.7)
axes[0].plot(range(1, 11), np.cumsum(explained_var) * 100,
             'ro-', linewidth=2, label='Á¥ØÁ©çÂØÑ‰∏éÁéá')
axes[0].set_xlabel('‰∏ªÊàêÂàÜ', fontsize=12)
axes[0].set_ylabel('ÂØÑ‰∏éÁéá (%)', fontsize=12)
axes[0].set_title('PCAÂØÑ‰∏éÁéá', fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# 2Ê¨°ÂÖÉ„Éó„É≠„ÉÉ„Éà
axes[1].scatter(X_pca[:, 0], X_pca[:, 1],
                c='steelblue', s=50, alpha=0.6, edgecolors='k')
axes[1].set_xlabel('PC1', fontsize=12)
axes[1].set_ylabel('PC2', fontsize=12)
axes[1].set_title('PCAÂèØË¶ñÂåñÔºà2Ê¨°ÂÖÉÔºâ', fontsize=13, fontweight='bold')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(f&quot;ÂÖÉ„ÅÆÊ¨°ÂÖÉÊï∞: {n_features}&quot;)
print(f&quot;ÂâäÊ∏õÂæå„ÅÆÊ¨°ÂÖÉÊï∞: {X_pca.shape[1]}&quot;)
print(f&quot;PC1-PC2„ÅÆÁ¥ØÁ©çÂØÑ‰∏éÁéá: {np.sum(explained_var[:2]) * 100:.2f}%&quot;)
print(f&quot;PC1-PC10„ÅÆÁ¥ØÁ©çÂØÑ‰∏éÁéá: {np.sum(explained_var) * 100:.2f}%&quot;)
</code></pre>
<h3>t-SNE, UMAP</h3>
<pre><code class="language-python">from sklearn.manifold import TSNE
# !pip install umap-learn
from umap import UMAP

def compare_dimensionality_reduction(X, labels=None):
    &quot;&quot;&quot;
    PCA, t-SNE, UMAP„ÅÆÊØîËºÉ
    &quot;&quot;&quot;
    # PCA
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)

    # t-SNE
    tsne = TSNE(n_components=2, random_state=42)
    X_tsne = tsne.fit_transform(X)

    # UMAP
    umap_model = UMAP(n_components=2, random_state=42)
    X_umap = umap_model.fit_transform(X)

    # ÂèØË¶ñÂåñ
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    # PCA
    axes[0].scatter(X_pca[:, 0], X_pca[:, 1],
                    c=labels, cmap='viridis', s=50, alpha=0.6)
    axes[0].set_xlabel('PC1', fontsize=11)
    axes[0].set_ylabel('PC2', fontsize=11)
    axes[0].set_title('PCA', fontsize=12, fontweight='bold')
    axes[0].grid(alpha=0.3)

    # t-SNE
    axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1],
                    c=labels, cmap='viridis', s=50, alpha=0.6)
    axes[1].set_xlabel('t-SNE1', fontsize=11)
    axes[1].set_ylabel('t-SNE2', fontsize=11)
    axes[1].set_title('t-SNE', fontsize=12, fontweight='bold')
    axes[1].grid(alpha=0.3)

    # UMAP
    im = axes[2].scatter(X_umap[:, 0], X_umap[:, 1],
                         c=labels, cmap='viridis', s=50, alpha=0.6)
    axes[2].set_xlabel('UMAP1', fontsize=11)
    axes[2].set_ylabel('UMAP2', fontsize=11)
    axes[2].set_title('UMAP', fontsize=12, fontweight='bold')
    axes[2].grid(alpha=0.3)

    if labels is not None:
        plt.colorbar(im, ax=axes[2], label='„É©„Éô„É´')

    plt.tight_layout()
    plt.show()

# „Çµ„É≥„Éó„É´„Éá„Éº„ÇøÔºà3„ÇØ„É©„ÇπÔºâ
np.random.seed(42)
class1 = np.random.randn(100, 50) + [2, 2] + np.zeros(48)
class2 = np.random.randn(100, 50) + [-2, 2] + np.zeros(48)
class3 = np.random.randn(100, 50) + [0, -2] + np.zeros(48)

X_multi_class = np.vstack([class1, class2, class3])
labels = np.array([0]*100 + [1]*100 + [2]*100)

# ÊØîËºÉ
compare_dimensionality_reduction(X_multi_class, labels)

print(&quot;Ê¨°ÂÖÉÂâäÊ∏õÊâãÊ≥ï„ÅÆÁâπÂæ¥Ôºö&quot;)
print(&quot;PCA: Á∑öÂΩ¢Â§âÊèõ„ÄÅÂ§ßÂüüÁöÑÊßãÈÄ†‰øùÊåÅ„ÄÅÈ´òÈÄü&quot;)
print(&quot;t-SNE: ÈùûÁ∑öÂΩ¢Â§âÊèõ„ÄÅÂ±ÄÊâÄÁöÑÊßãÈÄ†‰øùÊåÅ„ÄÅÈÅÖ„ÅÑ&quot;)
print(&quot;UMAP: ÈùûÁ∑öÂΩ¢Â§âÊèõ„ÄÅÂ§ßÂüü+Â±ÄÊâÄÊßãÈÄ†‰øùÊåÅ„ÄÅ‰∏≠ÈÄü&quot;)
</code></pre>
<h3>LDA (Linear Discriminant Analysis)</h3>
<pre><code class="language-python">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

def apply_lda(X, y, n_components=2):
    &quot;&quot;&quot;
    LDA„Å´„Çà„ÇãÊ¨°ÂÖÉÂâäÊ∏õÔºàÊïôÂ∏´„ÅÇ„ÇäÔºâ

    Parameters:
    -----------
    X : array-like
        ÁâπÂæ¥Èáè
    y : array-like
        „É©„Éô„É´

    Returns:
    --------
    X_lda : LDAÂ§âÊèõÂæå„ÅÆÁâπÂæ¥Èáè
    lda : LDA„É¢„Éá„É´
    &quot;&quot;&quot;
    lda = LinearDiscriminantAnalysis(n_components=n_components)
    X_lda = lda.fit_transform(X, y)

    return X_lda, lda

# LDAÈÅ©Áî®
X_lda, lda_model = apply_lda(X_multi_class, labels, n_components=2)

# PCA vs LDAÊØîËºÉ
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# PCAÔºàÊïôÂ∏´„Å™„ÅóÔºâ
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_multi_class)

axes[0].scatter(X_pca[:, 0], X_pca[:, 1],
                c=labels, cmap='viridis', s=50, alpha=0.6)
axes[0].set_xlabel('PC1', fontsize=11)
axes[0].set_ylabel('PC2', fontsize=11)
axes[0].set_title('PCAÔºàÊïôÂ∏´„Å™„ÅóÔºâ', fontsize=12, fontweight='bold')
axes[0].grid(alpha=0.3)

# LDAÔºàÊïôÂ∏´„ÅÇ„ÇäÔºâ
im = axes[1].scatter(X_lda[:, 0], X_lda[:, 1],
                     c=labels, cmap='viridis', s=50, alpha=0.6)
axes[1].set_xlabel('LD1', fontsize=11)
axes[1].set_ylabel('LD2', fontsize=11)
axes[1].set_title('LDAÔºàÊïôÂ∏´„ÅÇ„ÇäÔºâ', fontsize=12, fontweight='bold')
axes[1].grid(alpha=0.3)

plt.colorbar(im, ax=axes[1], label='„ÇØ„É©„Çπ')
plt.tight_layout()
plt.show()

print(&quot;LDA„ÅÆÂà©ÁÇπÔºö&quot;)
print(&quot;- „ÇØ„É©„ÇπÂàÜÈõ¢„ÇíÊúÄÂ§ßÂåñ„Åô„ÇãÂ∞ÑÂΩ±Ëª∏„ÇíË¶ã„Å§„Åë„Çã&quot;)
print(&quot;- ÂàÜÈ°ûÂïèÈ°å„Å´ÈÅ©„Åó„Å¶„ÅÑ„Çã&quot;)
print(f&quot;- ÊúÄÂ§ßÊ¨°ÂÖÉÊï∞: min(n_features, n_classes-1) = {lda_model.n_components}&quot;)
</code></pre>
<hr />
<h2>2.4 ÁâπÂæ¥ÈáèÈÅ∏Êäû</h2>
<p>ÈáçË¶Å„Å™ÁâπÂæ¥Èáè„ÅÆ„Åø„ÇíÈÅ∏Êäû„Åó„ÄÅ„É¢„Éá„É´„ÅÆÁ≤æÂ∫¶„Å®Ëß£ÈáàÊÄß„ÇíÂêë‰∏ä„Åï„Åõ„Åæ„Åô„ÄÇ</p>
<h3>FilterÊ≥ïÔºöÁõ∏Èñ¢‰øÇÊï∞„ÄÅÂàÜÊï£ÂàÜÊûê</h3>
<pre><code class="language-python">from sklearn.feature_selection import (
    VarianceThreshold,
    SelectKBest,
    f_regression,
    mutual_info_regression
)

def filter_method_selection(X, y, k=10):
    &quot;&quot;&quot;
    FilterÊ≥ï„Å´„Çà„ÇãÁâπÂæ¥ÈáèÈÅ∏Êäû

    Parameters:
    -----------
    X : pd.DataFrame
        ÁâπÂæ¥Èáè
    y : array-like
        ÁõÆÁöÑÂ§âÊï∞

    Returns:
    --------
    selected_features : ÈÅ∏Êäû„Åï„Çå„ÅüÁâπÂæ¥ÈáèÂêç
    scores : ÂêÑÁâπÂæ¥Èáè„ÅÆ„Çπ„Ç≥„Ç¢
    &quot;&quot;&quot;
    # ‰ΩéÂàÜÊï£ÁâπÂæ¥ÈáèÈô§Âéª
    var_threshold = VarianceThreshold(threshold=0.01)
    X_var = var_threshold.fit_transform(X)
    selected_by_var = X.columns[var_threshold.get_support()]

    # FÂÄ§Áµ±Ë®àÈáè
    selector_f = SelectKBest(f_regression, k=k)
    selector_f.fit(X, y)
    scores_f = selector_f.scores_
    selected_by_f = X.columns[selector_f.get_support()]

    # Áõ∏‰∫íÊÉÖÂ†±Èáè
    selector_mi = SelectKBest(mutual_info_regression, k=k)
    selector_mi.fit(X, y)
    scores_mi = selector_mi.scores_
    selected_by_mi = X.columns[selector_mi.get_support()]

    return {
        'variance': selected_by_var,
        'f_stat': selected_by_f,
        'mutual_info': selected_by_mi,
        'scores_f': scores_f,
        'scores_mi': scores_mi
    }

# „Çµ„É≥„Éó„É´„Éá„Éº„Çø
np.random.seed(42)
n_samples = 200
X_data = pd.DataFrame(
    np.random.randn(n_samples, 30),
    columns=[f'feature_{i}' for i in range(30)]
)

# ÁõÆÁöÑÂ§âÊï∞Ôºà‰∏ÄÈÉ®„ÅÆÁâπÂæ¥Èáè„ÅÆ„ÅøÈñ¢ÈÄ£Ôºâ
y_data = (
    2 * X_data['feature_0'] +
    3 * X_data['feature_5'] -
    1.5 * X_data['feature_10'] +
    np.random.normal(0, 0.5, n_samples)
)

# FilterÊ≥ïÂÆüË°å
selection_results = filter_method_selection(X_data, y_data, k=10)

# „Çπ„Ç≥„Ç¢ÂèØË¶ñÂåñ
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# FÂÄ§„Çπ„Ç≥„Ç¢
axes[0].bar(range(len(selection_results['scores_f'])),
            selection_results['scores_f'],
            color='steelblue', alpha=0.7)
axes[0].set_xlabel('ÁâπÂæ¥Èáè„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ', fontsize=11)
axes[0].set_ylabel('FÂÄ§„Çπ„Ç≥„Ç¢', fontsize=11)
axes[0].set_title('FÂÄ§Áµ±Ë®àÈáè„Å´„Çà„ÇãÁâπÂæ¥ÈáèË©ï‰æ°', fontsize=12, fontweight='bold')
axes[0].grid(alpha=0.3)

# Áõ∏‰∫íÊÉÖÂ†±Èáè„Çπ„Ç≥„Ç¢
axes[1].bar(range(len(selection_results['scores_mi'])),
            selection_results['scores_mi'],
            color='coral', alpha=0.7)
axes[1].set_xlabel('ÁâπÂæ¥Èáè„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ', fontsize=11)
axes[1].set_ylabel('Áõ∏‰∫íÊÉÖÂ†±Èáè', fontsize=11)
axes[1].set_title('Áõ∏‰∫íÊÉÖÂ†±Èáè„Å´„Çà„ÇãÁâπÂæ¥ÈáèË©ï‰æ°', fontsize=12, fontweight='bold')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;FÂÄ§Áµ±Ë®àÈáè„ÅßÈÅ∏Êäû„Åï„Çå„ÅüÁâπÂæ¥Èáè:&quot;)
print(selection_results['f_stat'].tolist())
print(&quot;\nÁõ∏‰∫íÊÉÖÂ†±Èáè„ÅßÈÅ∏Êäû„Åï„Çå„ÅüÁâπÂæ¥Èáè:&quot;)
print(selection_results['mutual_info'].tolist())
</code></pre>
<h3>WrapperÊ≥ïÔºöRFE</h3>
<pre><code class="language-python">from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor

def rfe_selection(X, y, n_features_to_select=10):
    &quot;&quot;&quot;
    RFEÔºàRecursive Feature EliminationÔºâ
    &quot;&quot;&quot;
    estimator = RandomForestRegressor(n_estimators=50, random_state=42)
    selector = RFE(estimator, n_features_to_select=n_features_to_select)
    selector.fit(X, y)

    selected_features = X.columns[selector.support_]
    feature_ranking = selector.ranking_

    return selected_features, feature_ranking

# RFEÂÆüË°å
selected_rfe, ranking_rfe = rfe_selection(X_data, y_data, n_features_to_select=10)

# „É©„É≥„Ç≠„É≥„Ç∞ÂèØË¶ñÂåñ
plt.figure(figsize=(12, 6))
plt.bar(range(len(ranking_rfe)), ranking_rfe,
        color='steelblue', alpha=0.7)
plt.axhline(y=1, color='red', linestyle='--',
            label='ÈÅ∏Êäû„Åï„Çå„ÅüÁâπÂæ¥Èáè (rank=1)', linewidth=2)
plt.xlabel('ÁâπÂæ¥Èáè„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ', fontsize=12)
plt.ylabel('„É©„É≥„Ç≠„É≥„Ç∞Ôºà‰Ωé„ÅÑ„Åª„Å©ÈáçË¶ÅÔºâ', fontsize=12)
plt.title('RFE„Å´„Çà„ÇãÁâπÂæ¥Èáè„É©„É≥„Ç≠„É≥„Ç∞', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(&quot;RFE„ÅßÈÅ∏Êäû„Åï„Çå„ÅüÁâπÂæ¥Èáè:&quot;)
print(selected_rfe.tolist())
</code></pre>
<h3>EmbeddedÊ≥ïÔºöLasso, Random Forest importances</h3>
<pre><code class="language-python">from sklearn.linear_model import Lasso
from sklearn.ensemble import RandomForestRegressor

def embedded_selection(X, y):
    &quot;&quot;&quot;
    EmbeddedÊ≥ïÔºàLasso + Random ForestÔºâ
    &quot;&quot;&quot;
    # LassoÔºàL1Ê≠£ÂâáÂåñÔºâ
    lasso = Lasso(alpha=0.1, random_state=42)
    lasso.fit(X, y)
    lasso_coefs = np.abs(lasso.coef_)
    selected_lasso = X.columns[lasso_coefs &gt; 0]

    # Random Forest importances
    rf = RandomForestRegressor(n_estimators=100, random_state=42)
    rf.fit(X, y)
    rf_importances = rf.feature_importances_
    # ‰∏ä‰Ωç10ÂÄãÈÅ∏Êäû
    top_10_idx = np.argsort(rf_importances)[-10:]
    selected_rf = X.columns[top_10_idx]

    return {
        'lasso': selected_lasso,
        'lasso_coefs': lasso_coefs,
        'rf': selected_rf,
        'rf_importances': rf_importances
    }

# EmbeddedÊ≥ïÂÆüË°å
embedded_results = embedded_selection(X_data, y_data)

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Lasso‰øÇÊï∞
axes[0].bar(range(len(embedded_results['lasso_coefs'])),
            embedded_results['lasso_coefs'],
            color='steelblue', alpha=0.7)
axes[0].set_xlabel('ÁâπÂæ¥Èáè„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ', fontsize=11)
axes[0].set_ylabel('|Lasso‰øÇÊï∞|', fontsize=11)
axes[0].set_title('LassoÁâπÂæ¥ÈáèÈÅ∏Êäû', fontsize=12, fontweight='bold')
axes[0].grid(alpha=0.3)

# Random Forest importances
axes[1].bar(range(len(embedded_results['rf_importances'])),
            embedded_results['rf_importances'],
            color='coral', alpha=0.7)
axes[1].set_xlabel('ÁâπÂæ¥Èáè„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ', fontsize=11)
axes[1].set_ylabel('Feature Importance', fontsize=11)
axes[1].set_title('Random ForestÈáçË¶ÅÂ∫¶', fontsize=12, fontweight='bold')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;Lasso„ÅßÈÅ∏Êäû„Åï„Çå„ÅüÁâπÂæ¥Èáè:&quot;)
print(embedded_results['lasso'].tolist())
print(&quot;\nRandom Forest„ÅßÈÅ∏Êäû„Åï„Çå„ÅüÁâπÂæ¥ÈáèÔºà‰∏ä‰Ωç10Ôºâ:&quot;)
print(embedded_results['rf'].tolist())
</code></pre>
<h3>SHAP-based selection</h3>
<pre><code class="language-python">import shap

def shap_based_selection(X, y, top_k=10):
    &quot;&quot;&quot;
    SHAP„Å´„Çà„ÇãÁâπÂæ¥ÈáèÈÅ∏Êäû
    &quot;&quot;&quot;
    # „É¢„Éá„É´Ë®ìÁ∑¥
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X, y)

    # SHAPÂÄ§Ë®àÁÆó
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X)

    # Âπ≥ÂùáÁµ∂ÂØæSHAPÂÄ§„ÅßÈáçË¶ÅÂ∫¶Ë©ï‰æ°
    mean_abs_shap = np.abs(shap_values).mean(axis=0)

    # ‰∏ä‰ΩçkÂÄãÈÅ∏Êäû
    top_k_idx = np.argsort(mean_abs_shap)[-top_k:]
    selected_features = X.columns[top_k_idx]

    return selected_features, mean_abs_shap, shap_values

# SHAPÈÅ∏Êäû
selected_shap, mean_shap, shap_vals = shap_based_selection(X_data, y_data, top_k=10)

# SHAP Summary Plot
plt.figure(figsize=(10, 8))
shap.summary_plot(shap_vals, X_data, plot_type=&quot;bar&quot;, show=False)
plt.title('SHAPÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶', fontsize=13, fontweight='bold')
plt.tight_layout()
plt.show()

print(&quot;SHAP„ÅßÈÅ∏Êäû„Åï„Çå„ÅüÁâπÂæ¥ÈáèÔºà‰∏ä‰Ωç10Ôºâ:&quot;)
print(selected_shap.tolist())

# ÊâãÊ≥ïÊØîËºÉ
print(&quot;\nÁâπÂæ¥ÈáèÈÅ∏ÊäûÊâãÊ≥ï„ÅÆÊØîËºÉÔºö&quot;)
print(f&quot;FilterÊ≥ïÔºàFÂÄ§Ôºâ: {len(selection_results['f_stat'])} ÁâπÂæ¥Èáè&quot;)
print(f&quot;WrapperÊ≥ïÔºàRFEÔºâ: {len(selected_rfe)} ÁâπÂæ¥Èáè&quot;)
print(f&quot;EmbeddedÊ≥ïÔºàLassoÔºâ: {len(embedded_results['lasso'])} ÁâπÂæ¥Èáè&quot;)
print(f&quot;SHAP-based: {len(selected_shap)} ÁâπÂæ¥Èáè&quot;)
</code></pre>
<hr />
<h2>2.5 „Ç±„Éº„Çπ„Çπ„Çø„Éá„Ç£Ôºö„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó‰∫àÊ∏¨</h2>
<p>ÂÆüÈöõ„ÅÆ„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó‰∫àÊ∏¨„Çø„Çπ„ÇØ„Åß„ÄÅ200Ê¨°ÂÖÉ„Åã„Çâ20Ê¨°ÂÖÉ„Å∏„ÅÆÂäπÊûúÁöÑ„Å™ÂâäÊ∏õ„ÇíÂÆüË∑µ„Åó„Åæ„Åô„ÄÇ</p>
<pre><code class="language-python"># „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºà„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥Ôºâ
np.random.seed(42)
n_materials = 500

# 200Ê¨°ÂÖÉ„ÅÆÊùêÊñôË®òËø∞Â≠êÔºàÁµÑÊàê„ÉªÊßãÈÄ†„ÉªÈõªÂ≠êÊßãÈÄ†Ôºâ
descriptor_names = (
    [f'ÁµÑÊàê_{i}' for i in range(80)] +
    [f'ÊßãÈÄ†_{i}' for i in range(60)] +
    [f'ÈõªÂ≠ê_{i}' for i in range(60)]
)

X_bandgap = pd.DataFrame(
    np.random.randn(n_materials, 200),
    columns=descriptor_names
)

# „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„ÉóÔºà‰∏ÄÈÉ®„ÅÆË®òËø∞Â≠ê„ÅÆ„Åø‰æùÂ≠òÔºâ
important_features = [
    'ÁµÑÊàê_5', 'ÁµÑÊàê_12', 'ÁµÑÊàê_25',
    'ÊßãÈÄ†_10', 'ÊßãÈÄ†_23',
    'ÈõªÂ≠ê_8', 'ÈõªÂ≠ê_15', 'ÈõªÂ≠ê_30'
]

y_bandgap = np.zeros(n_materials)
for feat in important_features:
    idx = descriptor_names.index(feat)
    y_bandgap += np.random.uniform(0.5, 1.5) * X_bandgap[feat]

y_bandgap = np.abs(y_bandgap) + np.random.normal(0, 0.3, n_materials)
y_bandgap = np.clip(y_bandgap, 0, 6)  # 0-6 eV„ÅÆÁØÑÂõ≤

print(&quot;=== „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó‰∫àÊ∏¨„Éá„Éº„Çø„Çª„ÉÉ„Éà ===&quot;)
print(f&quot;ÊùêÊñôÊï∞: {n_materials}&quot;)
print(f&quot;ÁâπÂæ¥ÈáèÊï∞: {X_bandgap.shape[1]}&quot;)
print(f&quot;ÁõÆÊ®ô: 200Ê¨°ÂÖÉ ‚Üí 20Ê¨°ÂÖÉ„Å´ÂâäÊ∏õ&quot;)
</code></pre>
<h3>Step 1: FilterÊ≥ï„Åß100Ê¨°ÂÖÉ„Å´ÂâäÊ∏õ</h3>
<pre><code class="language-python"># Áõ∏‰∫íÊÉÖÂ†±Èáè„Åß„Éà„ÉÉ„Éó100ÈÅ∏Êäû
selector_mi = SelectKBest(mutual_info_regression, k=100)
X_filtered = selector_mi.fit_transform(X_bandgap, y_bandgap)
selected_features_100 = X_bandgap.columns[selector_mi.get_support()]

print(f&quot;\nStep 1: FilterÊ≥ïÔºàÁõ∏‰∫íÊÉÖÂ†±ÈáèÔºâ&quot;)
print(f&quot;200Ê¨°ÂÖÉ ‚Üí {X_filtered.shape[1]}Ê¨°ÂÖÉ&quot;)
</code></pre>
<h3>Step 2: PCA„Åß50Ê¨°ÂÖÉ„Å´ÂâäÊ∏õ</h3>
<pre><code class="language-python"># PCAÈÅ©Áî®
pca = PCA(n_components=50)
X_pca = pca.fit_transform(X_filtered)

# Á¥ØÁ©çÂØÑ‰∏éÁéá
cumsum_var = np.cumsum(pca.explained_variance_ratio_)

# 90%Á¥ØÁ©çÂØÑ‰∏éÁéá„ÇíÈÅîÊàê„Åô„ÇãÊ¨°ÂÖÉÊï∞
n_components_90 = np.argmax(cumsum_var &gt;= 0.90) + 1

plt.figure(figsize=(10, 6))
plt.plot(range(1, 51), cumsum_var * 100, 'b-', linewidth=2)
plt.axhline(y=90, color='red', linestyle='--',
            label='90%Á¥ØÁ©çÂØÑ‰∏éÁéá', linewidth=2)
plt.axvline(x=n_components_90, color='green', linestyle='--',
            label=f'{n_components_90}Ê¨°ÂÖÉ„Åß90%ÈÅîÊàê', linewidth=2)
plt.xlabel('‰∏ªÊàêÂàÜÊï∞', fontsize=12)
plt.ylabel('Á¥ØÁ©çÂØÑ‰∏éÁéá (%)', fontsize=12)
plt.title('PCA„Å´„Çà„ÇãÊ¨°ÂÖÉÂâäÊ∏õ', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(f&quot;\nStep 2: PCA&quot;)
print(f&quot;100Ê¨°ÂÖÉ ‚Üí {X_pca.shape[1]}Ê¨°ÂÖÉ&quot;)
print(f&quot;90%Á¥ØÁ©çÂØÑ‰∏éÁéáÈÅîÊàê: {n_components_90}Ê¨°ÂÖÉ&quot;)
</code></pre>
<h3>Step 3: Random Forest Importance„Åß20Ê¨°ÂÖÉ„Å´ÂâäÊ∏õ</h3>
<pre><code class="language-python"># 50Ê¨°ÂÖÉ„ÅÆPCAÁâπÂæ¥Èáè„ÅßRandom ForestË®ìÁ∑¥
X_pca_df = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(50)])

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_pca_df, y_bandgap)

# ÈáçË¶ÅÂ∫¶„Éà„ÉÉ„Éó20ÈÅ∏Êäû
importances = rf.feature_importances_
top_20_idx = np.argsort(importances)[-20:]
X_final = X_pca_df.iloc[:, top_20_idx]

# ÈáçË¶ÅÂ∫¶ÂèØË¶ñÂåñ
plt.figure(figsize=(12, 6))
plt.bar(range(50), importances, color='steelblue', alpha=0.7)
plt.bar(top_20_idx, importances[top_20_idx],
        color='coral', alpha=0.9, label='ÈÅ∏Êäû„Åï„Çå„Åü20Ê¨°ÂÖÉ')
plt.xlabel('‰∏ªÊàêÂàÜ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ', fontsize=12)
plt.ylabel('Feature Importance', fontsize=12)
plt.title('Random Forest„Å´„Çà„ÇãÊúÄÁµÇÈÅ∏Êäû', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(f&quot;\nStep 3: Random Forest Importance&quot;)
print(f&quot;50Ê¨°ÂÖÉ ‚Üí {X_final.shape[1]}Ê¨°ÂÖÉ&quot;)
print(f&quot;\nÊúÄÁµÇÈÅ∏Êäû„Åï„Çå„Åü‰∏ªÊàêÂàÜ:&quot;)
print(X_final.columns.tolist())
</code></pre>
<h3>Step 4: ‰∫àÊ∏¨ÊÄßËÉΩ„ÅÆÊ§úË®º</h3>
<pre><code class="language-python">from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_absolute_error, r2_score

def evaluate_dimension_reduction(X, y, name):
    &quot;&quot;&quot;
    Ê¨°ÂÖÉÂâäÊ∏õÂæå„ÅÆ‰∫àÊ∏¨ÊÄßËÉΩË©ï‰æ°
    &quot;&quot;&quot;
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # ‰∫§Â∑ÆÊ§úË®º
    cv_scores = cross_val_score(
        model, X, y, cv=5,
        scoring='neg_mean_absolute_error'
    )
    cv_mae = -cv_scores.mean()

    return {
        'name': name,
        'dimensions': X.shape[1],
        'MAE': mae,
        'R2': r2,
        'CV_MAE': cv_mae
    }

# ÂêÑÊÆµÈöé„ÅÆÊÄßËÉΩË©ï‰æ°
results = []

# ÂÖÉ„Éá„Éº„ÇøÔºà200Ê¨°ÂÖÉÔºâ
results.append(evaluate_dimension_reduction(X_bandgap, y_bandgap, 'ÂÖÉ„Éá„Éº„Çø'))

# FilterÂæåÔºà100Ê¨°ÂÖÉÔºâ
X_filtered_df = pd.DataFrame(X_filtered)
results.append(evaluate_dimension_reduction(X_filtered_df, y_bandgap, 'FilterÊ≥ï'))

# PCAÂæåÔºà50Ê¨°ÂÖÉÔºâ
results.append(evaluate_dimension_reduction(X_pca_df, y_bandgap, 'PCA'))

# ÊúÄÁµÇÔºà20Ê¨°ÂÖÉÔºâ
results.append(evaluate_dimension_reduction(X_final, y_bandgap, 'ÊúÄÁµÇÈÅ∏Êäû'))

# ÁµêÊûúË°®Á§∫
results_df = pd.DataFrame(results)
print(&quot;\n=== Ê¨°ÂÖÉÂâäÊ∏õ„ÅÆÂΩ±ÈüøË©ï‰æ° ===&quot;)
print(results_df.to_string(index=False))

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# MAEÊØîËºÉ
axes[0].bar(results_df['name'], results_df['MAE'],
            color=['gray', 'steelblue', 'coral', 'green'], alpha=0.7)
axes[0].set_ylabel('MAE (eV)', fontsize=12)
axes[0].set_title('‰∫àÊ∏¨Ë™§Â∑ÆÔºàMAEÔºâ', fontsize=13, fontweight='bold')
axes[0].tick_params(axis='x', rotation=15)
axes[0].grid(axis='y', alpha=0.3)

# R¬≤ÊØîËºÉ
axes[1].bar(results_df['name'], results_df['R2'],
            color=['gray', 'steelblue', 'coral', 'green'], alpha=0.7)
axes[1].set_ylabel('R¬≤', fontsize=12)
axes[1].set_ylim(0, 1)
axes[1].set_title('Ê±∫ÂÆö‰øÇÊï∞ÔºàR¬≤Ôºâ', fontsize=13, fontweight='bold')
axes[1].tick_params(axis='x', rotation=15)
axes[1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

print(f&quot;\nÊÄßËÉΩÁ∂≠ÊåÅÁéáÔºàÊúÄÁµÇ20Ê¨°ÂÖÉ vs ÂÖÉ200Ê¨°ÂÖÉÔºâ:&quot;)
print(f&quot;R¬≤Á∂≠ÊåÅÁéá: {results_df.iloc[3]['R2'] / results_df.iloc[0]['R2'] * 100:.1f}%&quot;)
print(f&quot;Ê¨°ÂÖÉÂâäÊ∏õÁéá: {(1 - 20/200) * 100:.0f}%&quot;)
</code></pre>
<h3>Áâ©ÁêÜÁöÑÊÑèÂë≥„Å•„Åë</h3>
<pre><code class="language-python"># ÈÅ∏Êäû„Åï„Çå„Åü20Ê¨°ÂÖÉ„Å®ÂÖÉ„ÅÆË®òËø∞Â≠ê„ÅÆÂØæÂøú„ÇíÂàÜÊûê
def interpret_selected_components(pca_model, original_features, selected_pcs):
    &quot;&quot;&quot;
    ÈÅ∏Êäû„Åï„Çå„Åü‰∏ªÊàêÂàÜ„ÅÆÁâ©ÁêÜÁöÑËß£Èáà
    &quot;&quot;&quot;
    # PCA loadingsÔºà‰∏ªÊàêÂàÜË≤†Ëç∑ÈáèÔºâ
    loadings = pca_model.components_.T

    interpretations = []

    for pc_name in selected_pcs:
        pc_idx = int(pc_name.replace('PC', '')) - 1

        # „Åì„ÅÆ‰∏ªÊàêÂàÜ„Å∏„ÅÆÂØÑ‰∏é„ÅåÂ§ß„Åç„ÅÑÂÖÉ„ÅÆÁâπÂæ¥Èáè
        loading_vector = np.abs(loadings[:, pc_idx])
        top_5_idx = np.argsort(loading_vector)[-5:]

        top_features = [original_features[i] for i in top_5_idx]
        top_loadings = loading_vector[top_5_idx]

        interpretations.append({
            'PC': pc_name,
            'Top_Features': top_features,
            'Loadings': top_loadings
        })

    return interpretations

# Ëß£ÈáàÂÆüË°å
selected_pc_names = X_final.columns.tolist()
interpretations = interpret_selected_components(
    pca,
    selected_features_100.tolist(),
    selected_pc_names[:5]  # ÊúÄÂàù„ÅÆ5ÂÄã„ÅÆ„ÅøË°®Á§∫
)

print(&quot;\n=== ÈÅ∏Êäû„Åï„Çå„Åü‰∏ªÊàêÂàÜ„ÅÆÁâ©ÁêÜÁöÑËß£Èáà ===&quot;)
for interp in interpretations:
    print(f&quot;\n{interp['PC']}:&quot;)
    for feat, loading in zip(interp['Top_Features'], interp['Loadings']):
        print(f&quot;  {feat}: {loading:.3f}&quot;)
</code></pre>
<hr />
<h2>2.6 ÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞„ÅÆÂÆüË∑µ„Ç¨„Ç§„Éâ</h2>
<h3>matminer„É©„Ç§„Éñ„É©„É™„ÅÆ„Éê„Éº„Ç∏„Éß„É≥ÁÆ°ÁêÜ</h3>
<pre><code class="language-python"># matminer„Å®pymatgen„ÅÆ„Éê„Éº„Ç∏„Éß„É≥Á¢∫Ë™ç
import matminer
import pymatgen

print(&quot;=== ÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞Áí∞Â¢É ===&quot;)
print(f&quot;matminer: {matminer.__version__}&quot;)
print(f&quot;pymatgen: {pymatgen.__version__}&quot;)

# Êé®Â•®„Éê„Éº„Ç∏„Éß„É≥
print(&quot;\n„ÄêÊé®Â•®Áí∞Â¢É„Äë&quot;)
recommended = {
    'matminer': '0.9.0',
    'pymatgen': '2023.9.10',
    'scikit-learn': '1.3.0',
    'numpy': '1.24.3'
}

for package, version in recommended.items():
    print(f&quot;{package}&gt;={version}&quot;)

print(&quot;\n„Äê„Ç§„É≥„Çπ„Éà„Éº„É´„Ç≥„Éû„É≥„Éâ„Äë&quot;)
print(&quot;```bash&quot;)
print(&quot;pip install matminer==0.9.0 pymatgen==2023.9.10&quot;)
print(&quot;```&quot;)

print(&quot;\n„ÄêÊ≥®ÊÑè‰∫ãÈ†Ö„Äë&quot;)
print(&quot;‚ö†Ô∏è matminer„ÅÆ„Éó„É™„Çª„ÉÉ„ÉàÔºàmagpie, deml„Å™„Å©Ôºâ„ÅØ„Éê„Éº„Ç∏„Éß„É≥„ÅßÁâπÂæ¥ÈáèÊï∞„ÅåÂ§â„Çè„Çã&quot;)
print(&quot;‚ö†Ô∏è Ë´ñÊñáÂÜçÁèæÊôÇ„ÅØÂøÖ„Åö„Éê„Éº„Ç∏„Éß„É≥„ÇíÊòéË®ò&quot;)
</code></pre>
<h3>„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÊ¥ªÁî®</h3>
<pre><code class="language-python"># ÊùêÊñôÁßëÂ≠¶„Åß‰∏ÄËà¨ÁöÑ„Å™„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Éá„Éº„Çø„Çª„ÉÉ„Éà
benchmark_datasets = pd.DataFrame({
    '„Éá„Éº„Çø„Çª„ÉÉ„Éà': [
        'Matbench',
        'JARVIS-DFT',
        'Materials Project',
        'OQMD',
        'Expt Gap (Zhuo et al.)'
    ],
    '„Çø„Çπ„ÇØ': [
        '13Á®ÆÈ°û„ÅÆÂõûÂ∏∞/ÂàÜÈ°û',
        '55,000ÊùêÊñô„ÅÆÁâ©ÊÄß‰∫àÊ∏¨',
        '„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó„ÄÅÂΩ¢Êàê„Ç®„Éç„É´„ÇÆ„Éº',
        'ÂÆâÂÆöÊÄß‰∫àÊ∏¨„ÄÅÁõ∏Âõ≥',
        'ÂÆüÈ®ì„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„ÉóÔºà6,354ÊùêÊñôÔºâ'
    ],
    '„Çµ„É≥„Éó„É´Êï∞': [
        'Êï∞Áôæ„ÄúÊï∞‰∏á',
        '55,000+',
        '150,000+',
        '1,000,000+',
        '6,354'
    ],
    'Áî®ÈÄî': [
        '„É¢„Éá„É´ÊÄßËÉΩÊØîËºÉ',
        'GNN„ÉªÊ∑±Â±§Â≠¶Áøí',
        'Ê±éÁî®ÊùêÊñôÊé¢Á¥¢',
        'ÂÆâÂÆöÊÄßË©ï‰æ°',
        'ÂÆüÈ®ì„Éá„Éº„ÇøÊ§úË®º'
    ],
    'URL': [
        'https://matbench.materialsproject.org/',
        'https://jarvis.nist.gov/',
        'https://materialsproject.org/',
        'http://oqmd.org/',
        'DOI: 10.1021/acs.jpclett.8b00124'
    ]
})

print(&quot;=== „Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Éá„Éº„Çø„Çª„ÉÉ„Éà ===&quot;)
print(benchmark_datasets.to_string(index=False))

print(&quot;\n„Äê‰ΩøÁî®‰æãÔºöMatbench„Äë&quot;)
print(&quot;```python&quot;)
print(&quot;from matbench.bench import MatbenchBenchmark&quot;)
print(&quot;mb = MatbenchBenchmark(autoload=False)&quot;)
print(&quot;for task in mb.tasks:&quot;)
print(&quot;    task.load()&quot;)
print(&quot;    print(f'{task.dataset_name}: {len(task.df)} samples')&quot;)
print(&quot;```&quot;)
</code></pre>
<h3>ÂÆüË∑µÁöÑ„Å™ËêΩ„Å®„ÅóÁ©¥ÔºàÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞Á∑®Ôºâ</h3>
<pre><code class="language-python">print(&quot;=== ÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞„ÅÆËêΩ„Å®„ÅóÁ©¥ ===\n&quot;)

print(&quot;„ÄêËêΩ„Å®„ÅóÁ©¥1: Target LeakageÔºàÁõÆÁöÑÂ§âÊï∞„É™„Éº„ÇØÔºâ„Äë&quot;)
print(&quot;‚ùå ÊÇ™„ÅÑ‰æãÔºöÁõÆÁöÑÂ§âÊï∞„Åã„ÇâÁâπÂæ¥Èáè„ÇíÁîüÊàê&quot;)
print(&quot;```python&quot;)
print(&quot;# „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó‰∫àÊ∏¨„Åß„ÄÅ„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó„Å´Áõ¥Êé•Èñ¢ÈÄ£„Åô„ÇãÂÄ§„ÇíÁâπÂæ¥ÈáèÂåñ&quot;)
print(&quot;X['bandgap_proxy'] = y_bandgap * 0.9 + noise  # NGÔºÅ&quot;)
print(&quot;```&quot;)

print(&quot;\n‚úÖ Ê≠£„Åó„ÅÑ‰æãÔºöÁã¨Á´ã„Å™Áâ©ÊÄß„ÅÆ„Åø„ÇíÁâπÂæ¥ÈáèÂåñ&quot;)
print(&quot;```python&quot;)
print(&quot;# ÁµÑÊàê„ÄÅÊßãÈÄ†„ÄÅÈõªÂ≠êÊßãÈÄ†„Å™„Å©„ÄÅÁõÆÁöÑÂ§âÊï∞„Å®„ÅØÁã¨Á´ã„Å™Ë®òËø∞Â≠ê&quot;)
print(&quot;X_features = generate_composition_features(formulas)&quot;)
print(&quot;```&quot;)

print(&quot;\n„ÄêËêΩ„Å®„ÅóÁ©¥2: ÁâπÂæ¥Èáè„Çπ„Ç±„Éº„É´„ÅÆ‰∏çÁµ±‰∏Ä„Äë&quot;)
print(&quot;‚ö†Ô∏è Ê†ºÂ≠êÂÆöÊï∞Ôºà3-7√ÖÔºâ„Å®ÈõªÊ∞ó‰ºùÂ∞éÂ∫¶Ôºà10¬≥-10‚Å∂ S/mÔºâ„ÇíÊ∑∑Âú®&quot;)
print(&quot;‚Üí Ë∑ùÈõ¢„Éô„Éº„Çπ„ÅÆ„Ç¢„É´„Ç¥„É™„Ç∫„É†ÔºàKNN„ÄÅSVMÔºâ„ÅßÊÄßËÉΩ‰Ωé‰∏ã&quot;)

print(&quot;\n‚úÖ ÂØæÁ≠ñÔºöStandardScaler„ÅßÊ≠£Ë¶èÂåñ&quot;)
print(&quot;```python&quot;)
print(&quot;from sklearn.preprocessing import StandardScaler&quot;)
print(&quot;scaler = StandardScaler()&quot;)
print(&quot;X_scaled = scaler.fit_transform(X)&quot;)
print(&quot;```&quot;)

print(&quot;\n„ÄêËêΩ„Å®„ÅóÁ©¥3: Ê¨°ÂÖÉÂâäÊ∏õ„Åß„ÅÆÊÉÖÂ†±ÊêçÂ§±„Äë&quot;)
print(&quot;‚ö†Ô∏è PCA„Åß95%ÂØÑ‰∏éÁéá ‚Üí ÊÆã„Çä5%„Å´ÈáçË¶ÅÊÉÖÂ†±„Åå„ÅÇ„ÇãÂèØËÉΩÊÄß&quot;)

print(&quot;\n‚úÖ ÂØæÁ≠ñÔºöË§áÊï∞„ÅÆÂØÑ‰∏éÁéá„ÅßÊÄßËÉΩÊØîËºÉ&quot;)
print(&quot;```python&quot;)
print(&quot;for var_ratio in [0.90, 0.95, 0.99]:&quot;)
print(&quot;    pca = PCA(n_components=var_ratio)&quot;)
print(&quot;    X_pca = pca.fit_transform(X)&quot;)
print(&quot;    # „É¢„Éá„É´ÊÄßËÉΩË©ï‰æ°&quot;)
print(&quot;```&quot;)

print(&quot;\n„ÄêËêΩ„Å®„ÅóÁ©¥4: matminer„Éó„É™„Çª„ÉÉ„Éà„ÅÆÁÑ°ÊâπÂà§„Å™‰ΩøÁî®„Äë&quot;)
print(&quot;‚ö†Ô∏è magpie„Éó„É™„Çª„ÉÉ„ÉàÔºà132ÁâπÂæ¥ÈáèÔºâ„Çí„Åù„ÅÆ„Åæ„Åæ‰ΩøÁî®&quot;)
print(&quot;‚Üí ÂÜóÈï∑„ÉªÁÑ°Èñ¢‰øÇ„Å™ÁâπÂæ¥Èáè„ÅåÂ§öÊï∞Âê´„Åæ„Çå„Çã&quot;)

print(&quot;\n‚úÖ ÂØæÁ≠ñÔºö„Éâ„É°„Ç§„É≥Áü•Ë≠ò„ÅßÁâπÂæ¥Èáè„ÇíÂé≥ÈÅ∏&quot;)
print(&quot;```python&quot;)
print(&quot;# „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó‰∫àÊ∏¨„Å´Èñ¢ÈÄ£„Åô„ÇãÁâπÂæ¥Èáè„ÅÆ„ÅøÈÅ∏Êäû&quot;)
print(&quot;relevant_features = [&quot;)
print(&quot;    'MagpieData mean Electronegativity',&quot;)
print(&quot;    'MagpieData range Electronegativity',&quot;)
print(&quot;    'MagpieData mean NValence'&quot;)
print(&quot;]&quot;)
print(&quot;X_selected = X_all[relevant_features]&quot;)
print(&quot;```&quot;)

print(&quot;\n„ÄêËêΩ„Å®„ÅóÁ©¥5: ÁµÑÊàêË®òËø∞Â≠ê„ÅÆÈùûË¶èÊ†ºÂåñ„Äë&quot;)
print(&quot;‚ö†Ô∏è Li‚ÇÄ.‚ÇâCoO‚ÇÇ„Å®LiCoO‚ÇÇ„ÅßÁï∞„Å™„ÇãÁâπÂæ¥ÈáèÂÄ§&quot;)
print(&quot;‚Üí ÁµÑÊàêÊ≠£Ë¶èÂåñ„ÅåÂøÖË¶Å&quot;)

print(&quot;\n‚úÖ ÂØæÁ≠ñÔºöÁµÑÊàê„ÇíË¶èÊ†ºÂåñÔºàÂêàË®à=1Ôºâ&quot;)
print(&quot;```python&quot;)
print(&quot;from pymatgen.core import Composition&quot;)
print(&quot;comp = Composition('Li0.9CoO2')&quot;)
print(&quot;comp_normalized = comp.fractional_composition  # Li‚ÇÄ.‚ÇÉ‚ÇÅCo‚ÇÄ.‚ÇÉ‚ÇÑO‚ÇÄ.‚ÇÜ‚Çâ&quot;)
print(&quot;```&quot;)
</code></pre>
<hr />
<h2>ÊºîÁøíÂïèÈ°å</h2>
<h3>ÂïèÈ°å1ÔºàÈõ£ÊòìÂ∫¶: easyÔºâ</h3>
<p>matminer„Çí‰Ωø„Å£„Å¶„ÄÅÂåñÂ≠¶Âºè"Fe2O3"„Å®"TiO2"„ÅÆÊùêÊñôË®òËø∞Â≠ê„ÇíÁîüÊàê„Åó„ÄÅ„Å©„ÅÆË®òËø∞Â≠ê„ÅåÊúÄ„ÇÇÁï∞„Å™„Çã„Åã„ÇíÊØîËºÉ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<details>
<summary>„Éí„É≥„Éà</summary>

1. `ElementProperty.from_preset("magpie")`„Çí‰ΩøÁî®
2. ÂêÑÂåñÂ≠¶Âºè„ÅßÁâπÂæ¥ÈáèÁîüÊàê
3. Â∑Æ„ÅÆÁµ∂ÂØæÂÄ§„ÇíË®àÁÆó„Åó„ÄÅ‰∏ä‰Ωç10ÂÄã„ÇíË°®Á§∫

</details>

<details>
<summary>Ëß£Á≠î‰æã</summary>


<pre><code class="language-python">from matminer.featurizers.composition import ElementProperty
from pymatgen.core import Composition

# Ë®òËø∞Â≠êÁîüÊàê
ep_feat = ElementProperty.from_preset(&quot;magpie&quot;)

comp1 = Composition(&quot;Fe2O3&quot;)
comp2 = Composition(&quot;TiO2&quot;)

features1 = ep_feat.featurize(comp1)
features2 = ep_feat.featurize(comp2)

feature_names = ep_feat.feature_labels()

# Â∑ÆÂàÜË®àÁÆó
df_comparison = pd.DataFrame({
    'Feature': feature_names,
    'Fe2O3': features1,
    'TiO2': features2,
    'Difference': np.abs(np.array(features1) - np.array(features2))
})

df_comparison_sorted = df_comparison.sort_values(
    'Difference', ascending=False
)

print(&quot;ÊúÄ„ÇÇÁï∞„Å™„ÇãË®òËø∞Â≠êÔºà‰∏ä‰Ωç10Ôºâ:&quot;)
print(df_comparison_sorted.head(10).to_string(index=False))
</code></pre>


</details>

<h3>ÂïèÈ°å2ÔºàÈõ£ÊòìÂ∫¶: mediumÔºâ</h3>
<p>PCA„Å®UMAP„ÇíÁî®„ÅÑ„Å¶„ÄÅÈ´òÊ¨°ÂÖÉÊùêÊñô„Éá„Éº„Çø„Çí2Ê¨°ÂÖÉ„Å´ÂâäÊ∏õ„Åó„ÄÅ„Å©„Å°„Çâ„Åå„ÇØ„É©„Çπ„ÇøÊßãÈÄ†„Çí„Çà„ÇäÊòéÁ¢∫„Å´ÂèØË¶ñÂåñ„Åß„Åç„Çã„ÅãÊØîËºÉ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<details>
<summary>Ëß£Á≠î‰æã</summary>


<pre><code class="language-python">from sklearn.decomposition import PCA
from umap import UMAP
from sklearn.datasets import make_blobs

# „ÇØ„É©„Çπ„ÇøÊßãÈÄ†„ÇíÊåÅ„Å§È´òÊ¨°ÂÖÉ„Éá„Éº„ÇøÁîüÊàê
X, y = make_blobs(n_samples=300, n_features=50,
                  centers=3, random_state=42)

# PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# UMAP
umap_model = UMAP(n_components=2, random_state=42)
X_umap = umap_model.fit_transform(X)

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

axes[0].scatter(X_pca[:, 0], X_pca[:, 1],
                c=y, cmap='viridis', s=50, alpha=0.6)
axes[0].set_title('PCA', fontsize=12, fontweight='bold')

axes[1].scatter(X_umap[:, 0], X_umap[:, 1],
                c=y, cmap='viridis', s=50, alpha=0.6)
axes[1].set_title('UMAP', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.show()
</code></pre>


</details>

<h3>ÂïèÈ°å3ÔºàÈõ£ÊòìÂ∫¶: hardÔºâ</h3>
<p>FilterÊ≥ï„ÄÅWrapperÊ≥ï„ÄÅEmbeddedÊ≥ï„ÄÅSHAP-based„ÅÆ4„Å§„ÅÆÁâπÂæ¥ÈáèÈÅ∏ÊäûÊâãÊ≥ï„ÇíÁî®„ÅÑ„Å¶„ÄÅÂêå„Åò„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´ÂØæ„Åó„Å¶„Åù„Çå„Åû„Çå‰∏ä‰Ωç10ÁâπÂæ¥Èáè„ÇíÈÅ∏Êäû„Åó„ÄÅÈÅ∏Êäû„Åï„Çå„ÅüÁâπÂæ¥Èáè„ÅÆÈáçË§áÂ∫¶„ÇíÂàÜÊûê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<details>
<summary>Ëß£Á≠î‰æã</summary>


<pre><code class="language-python">from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Lasso
import shap

# „Çµ„É≥„Éó„É´„Éá„Éº„Çø
np.random.seed(42)
X = pd.DataFrame(np.random.randn(200, 30),
                 columns=[f'feat_{i}' for i in range(30)])
y = (2*X['feat_0'] + 3*X['feat_5'] - X['feat_10'] +
     np.random.normal(0, 0.5, 200))

# 1. FilterÊ≥ï
selector_filter = SelectKBest(f_regression, k=10)
selector_filter.fit(X, y)
selected_filter = set(X.columns[selector_filter.get_support()])

# 2. WrapperÊ≥ïÔºàRFEÔºâ
model_rfe = RandomForestRegressor(n_estimators=50, random_state=42)
selector_rfe = RFE(model_rfe, n_features_to_select=10)
selector_rfe.fit(X, y)
selected_rfe = set(X.columns[selector_rfe.support_])

# 3. EmbeddedÊ≥ïÔºàLassoÔºâ
lasso = Lasso(alpha=0.1, random_state=42)
lasso.fit(X, y)
lasso_coefs = np.abs(lasso.coef_)
top_10_lasso_idx = np.argsort(lasso_coefs)[-10:]
selected_lasso = set(X.columns[top_10_lasso_idx])

# 4. SHAP-based
rf_shap = RandomForestRegressor(n_estimators=100, random_state=42)
rf_shap.fit(X, y)
explainer = shap.TreeExplainer(rf_shap)
shap_values = explainer.shap_values(X)
mean_abs_shap = np.abs(shap_values).mean(axis=0)
top_10_shap_idx = np.argsort(mean_abs_shap)[-10:]
selected_shap = set(X.columns[top_10_shap_idx])

# ÈáçË§áÂàÜÊûê
all_methods = {
    'Filter': selected_filter,
    'Wrapper': selected_rfe,
    'Embedded': selected_lasso,
    'SHAP': selected_shap
}

# „Éô„É≥Âõ≥ÁöÑ„Å™ÈáçË§áË®àÁÆó
common_all = selected_filter &amp; selected_rfe &amp; selected_lasso &amp; selected_shap

print(&quot;ÂêÑÊâãÊ≥ï„ÅßÈÅ∏Êäû„Åï„Çå„ÅüÁâπÂæ¥Èáè:&quot;)
for method, features in all_methods.items():
    print(f&quot;{method}: {sorted(features)}&quot;)

print(f&quot;\nÂÖ®ÊâãÊ≥ïÂÖ±ÈÄö: {sorted(common_all)}&quot;)
print(f&quot;ÂÖ±ÈÄöÁâπÂæ¥ÈáèÊï∞: {len(common_all)} / 10&quot;)
</code></pre>


</details>

<hr />
<h2>„Åæ„Å®„ÇÅ</h2>
<p>„Åì„ÅÆÁ´†„Åß„ÅØ„ÄÅ<strong>ÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞</strong>„ÅÆÂÆüË∑µÊâãÊ≥ï„ÇíÂ≠¶„Å≥„Åæ„Åó„Åü„ÄÇ</p>
<p><strong>ÈáçË¶Å„Éù„Ç§„É≥„Éà</strong>Ôºö</p>
<ol>
<li><strong>ÊùêÊñôË®òËø∞Â≠ê</strong>ÔºöÁµÑÊàê„ÉªÊßãÈÄ†„ÉªÈõªÂ≠êÊßãÈÄ†Ë®òËø∞Â≠ê„Çímatminer„ÅßÂäπÁéáÁöÑ„Å´ÁîüÊàê</li>
<li><strong>ÁâπÂæ¥ÈáèÂ§âÊèõ</strong>ÔºöÊ≠£Ë¶èÂåñ„ÉªÂØæÊï∞Â§âÊèõ„ÉªÂ§öÈ†ÖÂºèÁâπÂæ¥Èáè„ÅßÈùûÁ∑öÂΩ¢Èñ¢‰øÇ„ÇíÊçâ„Åà„Çã</li>
<li><strong>Ê¨°ÂÖÉÂâäÊ∏õ</strong>ÔºöPCA„ÄÅt-SNE„ÄÅUMAP„ÅßÂèØË¶ñÂåñ„Å®Ë®àÁÆóÂäπÁéáÂåñ</li>
<li><strong>ÁâπÂæ¥ÈáèÈÅ∏Êäû</strong>ÔºöFilter &lt; Wrapper &lt; Embedded &lt; SHAP „ÅÆÈ†Ü„ÅßÁ≤æÂ∫¶Âêë‰∏ä</li>
<li><strong>ÂÆüË∑µ‰∫ã‰æã</strong>Ôºö200Ê¨°ÂÖÉ‚Üí20Ê¨°ÂÖÉ„ÅßÊÄßËÉΩ„ÇíÁ∂≠ÊåÅ„Åó„Å§„Å§Ëß£ÈáàÊÄßÂêë‰∏ä</li>
<li><strong>„É©„Ç§„Éñ„É©„É™ÁÆ°ÁêÜ</strong>Ôºömatminer„ÄÅpymatgen„ÅÆ„Éê„Éº„Ç∏„Éß„É≥ÁÆ°ÁêÜ„Å®ÂÜçÁèæÊÄßÁ¢∫‰øù</li>
<li><strong>„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Éá„Éº„Çø</strong>ÔºöMatbench„ÄÅJARVIS-DFT„Å™„Å©Ê®ôÊ∫ñ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÊ¥ªÁî®</li>
<li><strong>ÂÆüË∑µÁöÑËêΩ„Å®„ÅóÁ©¥</strong>ÔºöTarget leakage„ÄÅ„Çπ„Ç±„Éº„É´‰∏çÁµ±‰∏Ä„ÄÅÊÉÖÂ†±ÊêçÂ§±„ÄÅÁµÑÊàêË¶èÊ†ºÂåñ</li>
</ol>
<p><strong>Ê¨°Á´†‰∫àÂëä</strong>Ôºö
Chapter 3„Åß„ÅØ„ÄÅÊúÄÈÅ©„Å™„É¢„Éá„É´ÈÅ∏Êäû„Å®„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøÊúÄÈÅ©Âåñ„ÇíÂ≠¶„Å≥„Åæ„Åô„ÄÇOptuna„ÇíÁî®„ÅÑ„ÅüËá™ÂãïÊúÄÈÅ©Âåñ„Å®„Ç¢„É≥„Çµ„É≥„Éñ„É´Â≠¶Áøí„Åß‰∫àÊ∏¨Á≤æÂ∫¶„ÇíÊúÄÂ§ßÂåñ„Åó„Åæ„Åô„ÄÇ</p>
<hr />
<h2>Chapter 2 „ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà</h2>
<h3>ÁâπÂæ¥ÈáèÁîüÊàêÔºàÁµÑÊàêË®òËø∞Â≠êÔºâ</h3>
<ul>
<li>[ ] <strong>matminerÊ¥ªÁî®</strong></li>
<li>[ ] ElementProperty.from_preset("magpie")„Åß132ÁâπÂæ¥ÈáèÁîüÊàê</li>
<li>[ ] Stoichiometry()„ÅßÂåñÂ≠¶ÈáèË´ñË®òËø∞Â≠êÔºà7ÁâπÂæ¥ÈáèÔºâ</li>
<li>[ ] ValenceOrbital()„Åß‰æ°ÈõªÂ≠êËªåÈÅìË®òËø∞Â≠êÔºà10ÁâπÂæ¥ÈáèÔºâ</li>
<li>
<p>[ ] „Éê„Éº„Ç∏„Éß„É≥Ë®òÈå≤Ôºàmatminer 0.9.0„ÄÅpymatgen 2023.9.10Ôºâ</p>
</li>
<li>
<p>[ ] <strong>ÁµÑÊàê„ÅÆË¶èÊ†ºÂåñ</strong></p>
</li>
<li>[ ] Composition.fractional_composition„ÅßÂêàË®à=1„Å´Ê≠£Ë¶èÂåñ</li>
<li>[ ] Li‚ÇÄ.‚ÇâCoO‚ÇÇ „Å®LiCoO‚ÇÇ„ÅÆÊâ±„ÅÑ„ÇíÁµ±‰∏Ä</li>
<li>
<p>[ ] „Éâ„Éº„Éë„É≥„ÉàÊøÉÂ∫¶„ÅÆÈÅ©Âàá„Å™Ë°®Áèæ</p>
</li>
<li>
<p>[ ] <strong>„Éâ„É°„Ç§„É≥Áü•Ë≠ò„ÅÆÁµ±Âêà</strong></p>
</li>
<li>[ ] ÈõªÊ∞óÈô∞ÊÄßÂ∫¶„ÄÅÂéüÂ≠êÂçäÂæÑ„ÄÅ‰æ°ÈõªÂ≠êÊï∞„Å™„Å©„ÅÆÂåñÂ≠¶ÁöÑÊÑèÂë≥„ÇíÁêÜËß£</li>
<li>[ ] „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó‰∫àÊ∏¨„Å´„ÅØÈõªÂ≠êÊßãÈÄ†Ë®òËø∞Â≠ê„ÇíÂÑ™ÂÖà</li>
<li>[ ] Ê©üÊ¢∞ÁöÑÁâπÊÄß‰∫àÊ∏¨„Å´„ÅØÁµêÊô∂ÊßãÈÄ†Ë®òËø∞Â≠ê„ÇíÂÑ™ÂÖà</li>
</ul>
<h3>ÁâπÂæ¥ÈáèÁîüÊàêÔºàÊßãÈÄ†„ÉªÈõªÂ≠êÊßãÈÄ†Ë®òËø∞Â≠êÔºâ</h3>
<ul>
<li>[ ] <strong>ÁµêÊô∂ÊßãÈÄ†Ë®òËø∞Â≠ê</strong></li>
<li>[ ] Ê†ºÂ≠êÂÆöÊï∞Ôºàa, b, cÔºâ„ÄÅÊ†ºÂ≠êËßíÂ∫¶ÔºàŒ±, Œ≤, Œ≥Ôºâ</li>
<li>[ ] ‰ΩìÁ©ç„ÄÅ„Éë„ÉÉ„Ç≠„É≥„Ç∞ÂØÜÂ∫¶</li>
<li>
<p>[ ] Á©∫ÈñìÁæ§„ÄÅÂØæÁß∞ÊÄß</p>
</li>
<li>
<p>[ ] <strong>ÈõªÂ≠êÊßãÈÄ†Ë®òËø∞Â≠ê</strong></p>
</li>
<li>[ ] „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó„ÄÅ„Éï„Çß„É´„Éü„Ç®„Éç„É´„ÇÆ„Éº</li>
<li>[ ] Áä∂ÊÖãÂØÜÂ∫¶Ôºà‰æ°ÈõªÂ≠êÂ∏Ø„Éª‰ºùÂ∞éÂ∏ØÔºâ</li>
<li>
<p>[ ] ÊúâÂäπË≥™ÈáèÔºàÈõªÂ≠ê„ÉªÊ≠£Â≠îÔºâ</p>
</li>
<li>
<p>[ ] <strong>DFTË®àÁÆó„Éá„Éº„ÇøÈÄ£Êê∫</strong></p>
</li>
<li>[ ] Materials Project API„ÅßÂΩ¢Êàê„Ç®„Éç„É´„ÇÆ„ÉºÂèñÂæó</li>
<li>[ ] OQMD„ÅßÂÆâÂÆöÊÄß„Éá„Éº„ÇøÂèñÂæó</li>
<li>[ ] Ë®àÁÆóÊù°‰ª∂ÔºàÊ±éÈñ¢Êï∞„ÄÅ„Ç´„ÉÉ„Éà„Ç™„Éï„Ç®„Éç„É´„ÇÆ„ÉºÔºâ„ÇíË®òÈå≤</li>
</ul>
<h3>ÁâπÂæ¥ÈáèÂ§âÊèõ</h3>
<ul>
<li>[ ] <strong>Ê≠£Ë¶èÂåñ</strong></li>
<li>[ ] StandardScalerÔºàÂπ≥Âùá0„ÄÅÊ®ôÊ∫ñÂÅèÂ∑Æ1Ôºâ„Çí„Éá„Éï„Ç©„É´„Éà‰ΩøÁî®</li>
<li>[ ] MinMaxScalerÔºà0-1ÁØÑÂõ≤Ôºâ„ÅØËß£ÈáàÊÄßÈáçË¶ñÊôÇ</li>
<li>
<p>[ ] Train/TestÂàÜÂâ≤<strong>Âæå</strong>„Å´fitÔºà„Éá„Éº„Çø„É™„Éº„ÇØÈò≤Ê≠¢Ôºâ</p>
</li>
<li>
<p>[ ] <strong>ÂØæÊï∞Â§âÊèõ</strong></p>
</li>
<li>[ ] ÈõªÊ∞ó‰ºùÂ∞éÂ∫¶„Å™„Å©Ê°ÅÊï∞„ÅåÂ§ß„Åç„ÅÑÁâπÂæ¥Èáè„Å´ÈÅ©Áî®</li>
<li>[ ] log1pÔºàlog(1+x)Ôºâ„Åß„Çº„É≠ÂÄ§„ÇíÊâ±„ÅÜ</li>
<li>
<p>[ ] Ê≠™Â∫¶ÔºàskewnessÔºâ„ÇíÁ¢∫Ë™ç„Åó„ÄÅÂ§âÊèõÂæå„Å´-0.5„Äú0.5„Å´Âèé„ÇÅ„Çã</p>
</li>
<li>
<p>[ ] <strong>Â§öÈ†ÖÂºèÁâπÂæ¥Èáè</strong></p>
</li>
<li>[ ] 2Ê¨°È†Ö„ÅßÈùûÁ∑öÂΩ¢Èñ¢‰øÇ„Çí„Ç≠„É£„Éó„ÉÅ„É£</li>
<li>[ ] ‰∫§‰∫í‰ΩúÁî®È†ÖÔºàx‚ÇÅ√óx‚ÇÇÔºâ„ÅßÁõ∏‰∫í‰ΩúÁî®„ÇíÊòéÁ§∫</li>
<li>[ ] Ê¨°Êï∞3‰ª•‰∏ä„ÅØÈÅéÂ≠¶Áøí„É™„Çπ„ÇØ ‚Üí ÊÖéÈáç„Å´</li>
</ul>
<h3>Ê¨°ÂÖÉÂâäÊ∏õ</h3>
<ul>
<li>[ ] <strong>PCAÔºà‰∏ªÊàêÂàÜÂàÜÊûêÔºâ</strong></li>
<li>[ ] Á¥ØÁ©çÂØÑ‰∏éÁéá90-95%„ÇíÁõÆÊ®ô</li>
<li>[ ] ‰∏ªÊàêÂàÜË≤†Ëç∑Èáè„ÅßÁâ©ÁêÜÁöÑËß£Èáà</li>
<li>
<p>[ ] „Çπ„ÇØ„É™„Éº„Éó„É≠„ÉÉ„ÉàÔºàÂØÑ‰∏éÁéá„ÅÆÊ∏õË°∞Ôºâ„ÇíÂèØË¶ñÂåñ</p>
</li>
<li>
<p>[ ] <strong>t-SNE / UMAP</strong></p>
</li>
<li>[ ] „ÇØ„É©„Çπ„ÇøÂèØË¶ñÂåñ„Å´ÊúâÂäπÔºà2Ê¨°ÂÖÉ„Éó„É≠„ÉÉ„ÉàÔºâ</li>
<li>[ ] ‰∫àÊ∏¨„É¢„Éá„É´„ÅÆÁâπÂæ¥Èáè„Å´„ÅØ‰ΩøÁî®„Åó„Å™„ÅÑÔºàÈùûÁ∑öÂΩ¢„Åß‰∏çÂèØÈÄÜÔºâ</li>
<li>
<p>[ ] „Éë„Éº„Éó„É¨„Ç≠„Ç∑„ÉÜ„Ç£Ôºàt-SNEÔºâ„ÄÅn_neighborsÔºàUMAPÔºâ„Çí„ÉÅ„É•„Éº„Éã„É≥„Ç∞</p>
</li>
<li>
<p>[ ] <strong>LDAÔºàÁ∑öÂΩ¢Âà§Âà•ÂàÜÊûêÔºâ</strong></p>
</li>
<li>[ ] ÂàÜÈ°ûÂïèÈ°å„Åß„ÇØ„É©„ÇπÂàÜÈõ¢„ÇíÊúÄÂ§ßÂåñ</li>
<li>[ ] Ê¨°ÂÖÉÊï∞„ÅØ min(n_features, n_classes-1)</li>
<li>[ ] ÊïôÂ∏´„ÅÇ„ÇäÂ≠¶Áøí„Å™„ÅÆ„ÅßÊ±éÂåñÊÄßËÉΩ„Å´Ê≥®ÊÑè</li>
</ul>
<h3>ÁâπÂæ¥ÈáèÈÅ∏Êäû</h3>
<ul>
<li>[ ] <strong>FilterÊ≥ï</strong></li>
<li>[ ] VarianceThresholdÔºà‰ΩéÂàÜÊï£ÁâπÂæ¥ÈáèÈô§ÂéªÔºâ</li>
<li>[ ] SelectKBest + f_regressionÔºàFÂÄ§Áµ±Ë®àÈáèÔºâ</li>
<li>[ ] SelectKBest + mutual_info_regressionÔºàÁõ∏‰∫íÊÉÖÂ†±ÈáèÔºâ</li>
<li>
<p>[ ] Ë®àÁÆóÈ´òÈÄü„Å†„Åå„ÄÅÁâπÂæ¥ÈáèÈñì„ÅÆÁõ∏‰∫í‰ΩúÁî®„ÇíËÄÉÊÖÆ„Åó„Å™„ÅÑ</p>
</li>
<li>
<p>[ ] <strong>WrapperÊ≥ï</strong></p>
</li>
<li>[ ] RFEÔºàRecursive Feature EliminationÔºâ</li>
<li>[ ] Ë®àÁÆó„Ç≥„Çπ„ÉàÈ´ò„ÅÑ„ÅåÁ≤æÂ∫¶È´ò„ÅÑ</li>
<li>
<p>[ ] 10-20ÁâπÂæ¥Èáè„Å∏„ÅÆÂâäÊ∏õ„Å´ÊúâÂäπ</p>
</li>
<li>
<p>[ ] <strong>EmbeddedÊ≥ï</strong></p>
</li>
<li>[ ] LassoÔºàL1Ê≠£ÂâáÂåñÔºâ„Åß‰øÇÊï∞„Çº„É≠„ÅÆÁâπÂæ¥Èáè„ÇíËá™ÂãïÈô§Âéª</li>
<li>[ ] Random Forest feature_importances_</li>
<li>[ ] LightGBM feature_importances_</li>
<li>
<p>[ ] Ë®ìÁ∑¥ÈÅéÁ®ã„ÅßÁâπÂæ¥ÈáèÈÅ∏Êäû„ÅåË°å„Çè„Çå„Çã</p>
</li>
<li>
<p>[ ] <strong>SHAP-basedÈÅ∏Êäû</strong></p>
</li>
<li>[ ] mean(|SHAPÂÄ§|)„ÅßÈáçË¶ÅÂ∫¶„É©„É≥„Ç≠„É≥„Ç∞</li>
<li>[ ] GlobalËß£Èáà„Å®„Åó„Å¶ÊúÄ„ÇÇ‰ø°È†ºÊÄß„ÅåÈ´ò„ÅÑ</li>
<li>[ ] Ë®àÁÆó„Ç≥„Çπ„Éà„ÅØ‰∏≠Á®ãÂ∫¶</li>
</ul>
<h3>ÂÆüË∑µÁöÑËêΩ„Å®„ÅóÁ©¥„ÅÆÂõûÈÅøÔºàÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞Ôºâ</h3>
<ul>
<li>[ ] <strong>Target LeakageÈò≤Ê≠¢</strong></li>
<li>[ ] ÁõÆÁöÑÂ§âÊï∞„Åã„ÇâÁâπÂæ¥Èáè„ÇíÁîüÊàê„Åó„Å™„ÅÑ</li>
<li>[ ] DFTË®àÁÆóÂÄ§Ôºà„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó„Å™„Å©Ôºâ„ÇíÁõ¥Êé•‰Ωø„Çè„Å™„ÅÑ</li>
<li>
<p>[ ] ÁµÑÊàê„ÉªÊßãÈÄ†„Å™„Å©Áã¨Á´ã„Å™Ë®òËø∞Â≠ê„ÅÆ„Åø‰ΩøÁî®</p>
</li>
<li>
<p>[ ] <strong>„Çπ„Ç±„Éº„É´Áµ±‰∏Ä</strong></p>
</li>
<li>[ ] Ê†ºÂ≠êÂÆöÊï∞Ôºà√ÖÔºâ„Å®ÈõªÊ∞ó‰ºùÂ∞éÂ∫¶ÔºàS/mÔºâ„ÇíÊ≠£Ë¶èÂåñ</li>
<li>[ ] Ë∑ùÈõ¢„Éô„Éº„Çπ„É¢„Éá„É´ÔºàKNN„ÄÅSVMÔºâ„ÅßÁâπ„Å´ÈáçË¶Å</li>
<li>
<p>[ ] Êú®„Éô„Éº„Çπ„É¢„Éá„É´ÔºàRF„ÄÅXGBoostÔºâ„Åß„ÅØ‰∏çË¶Å„Å†„ÅåÊé®Â•®</p>
</li>
<li>
<p>[ ] <strong>ÊÉÖÂ†±ÊêçÂ§±„ÅÆÁõ£Ë¶ñ</strong></p>
</li>
<li>[ ] PCAÂØÑ‰∏éÁéá„ÇíË§áÊï∞Ôºà90%, 95%, 99%Ôºâ„ÅßÊØîËºÉ</li>
<li>[ ] Ê¨°ÂÖÉÂâäÊ∏õÂâçÂæå„ÅÆ„É¢„Éá„É´ÊÄßËÉΩ„ÇíË©ï‰æ°</li>
<li>
<p>[ ] ÂâäÊ∏õ„Åó„Åô„Åé„ÅßÊÄßËÉΩ‰Ωé‰∏ã„Å™„ÇâÊ¨°ÂÖÉÊï∞„ÇíÂ¢ó„ÇÑ„Åô</p>
</li>
<li>
<p>[ ] <strong>matminer„Éó„É™„Çª„ÉÉ„Éà„ÅÆÂêüÂë≥</strong></p>
</li>
<li>[ ] magpieÔºà132ÁâπÂæ¥ÈáèÔºâ„ÅØÂ§ö„Åô„Åé„ÇãÂ†¥Âêà„ÅÇ„Çä</li>
<li>[ ] „Éâ„É°„Ç§„É≥Áü•Ë≠ò„ÅßÈñ¢ÈÄ£ÁâπÂæ¥Èáè„ÇíÈÅ∏ÂÆö</li>
<li>
<p>[ ] Áõ∏Èñ¢Ë°åÂàó„ÅßÂÜóÈï∑„Å™ÁâπÂæ¥Èáè„ÇíÈô§Âéª</p>
</li>
<li>
<p>[ ] <strong>ÁµÑÊàêË¶èÊ†ºÂåñ</strong></p>
</li>
<li>[ ] ÁµÑÊàêÂêàË®à„Åå1„Å´„Å™„Çã„Çà„ÅÜÊ≠£Ë¶èÂåñ</li>
<li>[ ] ÂåñÂ≠¶ÂºèË°®Ë®ò„ÅÆÊè∫„ÇåÔºàLi‚ÇÄ.‚ÇâCoO‚ÇÇ vs LiCoO‚ÇÇÔºâ„ÇíÁµ±‰∏Ä</li>
<li>[ ] Composition.fractional_composition‰ΩøÁî®</li>
</ul>
<h3>„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÊ¥ªÁî®</h3>
<ul>
<li>[ ] <strong>Matbench</strong></li>
<li>[ ] 13Á®ÆÈ°û„ÅÆ„Çø„Çπ„ÇØ„ÅßÊ®ôÊ∫ñÁöÑÊÄßËÉΩË©ï‰æ°</li>
<li>[ ] Ë´ñÊñáÊØîËºÉÊôÇ„ÅÆÂÖ±ÈÄöÂü∫Ê∫ñ</li>
<li>
<p>[ ] „É¢„Éá„É´ÈÅ∏Êäû„ÅÆÂ¶•ÂΩìÊÄßÊ§úË®º</p>
</li>
<li>
<p>[ ] <strong>JARVIS-DFT</strong></p>
</li>
<li>[ ] 55,000ÊùêÊñô„ÅÆDFTË®àÁÆó„Éá„Éº„Çø</li>
<li>[ ] GNN„ÇÑTransformer„ÅÆË®ìÁ∑¥„Å´ÊúÄÈÅ©</li>
<li>
<p>[ ] ÊßãÈÄ†Ë®òËø∞Â≠ê„ÅÆË±äÂØå„Åï„ÅåÁâπÂæ¥</p>
</li>
<li>
<p>[ ] <strong>ÂÆüÈ®ì„Éá„Éº„Çø„Çª„ÉÉ„Éà</strong></p>
</li>
<li>[ ] Expt GapÔºàÂÆüÈ®ì„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó 6,354ÊùêÊñôÔºâ</li>
<li>[ ] DFT vs ÂÆüÈ®ì„ÅÆ„ÇÆ„É£„ÉÉ„Éó„ÇíÊ§úË®º</li>
<li>[ ] „É¢„Éá„É´„ÅÆÂÆüÁî®ÊÄßË©ï‰æ°</li>
</ul>
<h3>ÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞ÂìÅË≥™ÊåáÊ®ô</h3>
<ul>
<li>[ ] <strong>ÁâπÂæ¥Èáè„ÅÆË≥™</strong></li>
<li>[ ] Ê¨†ÊêçÁéá &lt; 5%ÔºàÁâπÂæ¥Èáè„É¨„Éô„É´Ôºâ</li>
<li>[ ] ÂàÜÊï£ &gt; 0.01Ôºà‰ΩéÂàÜÊï£ÁâπÂæ¥Èáè„ÇíÈô§ÂéªÔºâ</li>
<li>
<p>[ ] Áõ∏Èñ¢‰øÇÊï∞ |r| &lt; 0.9ÔºàÈ´òÁõ∏Èñ¢„Éö„Ç¢„ÇíÂâäÊ∏õÔºâ</p>
</li>
<li>
<p>[ ] <strong>Ê¨°ÂÖÉÂâäÊ∏õ„ÅÆÂäπÊûú</strong></p>
</li>
<li>[ ] PCAÂØÑ‰∏éÁéá ‚â• 90%</li>
<li>[ ] Ê¨°ÂÖÉÂâäÊ∏õÁéá 50-90%</li>
<li>
<p>[ ] ÊÄßËÉΩÁ∂≠ÊåÅÁéá ‚â• 95%ÔºàMAE„ÄÅR¬≤„Å™„Å©Ôºâ</p>
</li>
<li>
<p>[ ] <strong>ÁâπÂæ¥ÈáèÈÅ∏Êäû„ÅÆÂäπÊûú</strong></p>
</li>
<li>[ ] ÈÅ∏ÊäûÂæå„ÅÆÁâπÂæ¥ÈáèÊï∞ ‚â§ 20%ÔºàÂÖÉ„ÅÆÊï∞Ôºâ</li>
<li>[ ] ÊÄßËÉΩÊîπÂñÑ„Åæ„Åü„ÅØÁ∂≠ÊåÅÔºàÈÅéÂ≠¶ÁøíÈò≤Ê≠¢Ôºâ</li>
<li>[ ] Ë®ìÁ∑¥ÊôÇÈñìÁü≠Á∏Æ ‚â• 30%</li>
</ul>
<h3>ÂÜçÁèæÊÄß„ÅÆÁ¢∫‰øù</h3>
<ul>
<li>[ ] <strong>„Éê„Éº„Ç∏„Éß„É≥ÁÆ°ÁêÜ</strong></li>
<li>[ ] matminer„ÄÅpymatgen„ÅÆ„Éê„Éº„Ç∏„Éß„É≥ÊòéË®ò</li>
<li>[ ] „Éó„É™„Çª„ÉÉ„ÉàÂêçÔºàmagpie„ÄÅdeml„Å™„Å©Ôºâ„ÇíË®òÈå≤</li>
<li>
<p>[ ] ÁâπÂæ¥ÈáèÁîüÊàêÊó•ÊôÇ„ÇíË®òÈå≤</p>
</li>
<li>
<p>[ ] <strong>ÁâπÂæ¥Èáè„É™„Çπ„Éà„ÅÆ‰øùÂ≠ò</strong></p>
</li>
<li>[ ] ÁîüÊàê„Åï„Çå„ÅüÁâπÂæ¥ÈáèÂêç„ÇíÂÖ®„Å¶CSV‰øùÂ≠ò</li>
<li>[ ] ÁâπÂæ¥Èáè„ÅÆÊÑèÂë≥ÔºàË™¨ÊòéÔºâ„Çí„Éâ„Ç≠„É•„É°„É≥„ÉàÂåñ</li>
<li>
<p>[ ] ÈÅ∏Êäû„Åï„Çå„ÅüÁâπÂæ¥Èáè„ÅÆ„Çµ„Éñ„Çª„ÉÉ„Éà„ÇÇË®òÈå≤</p>
</li>
<li>
<p>[ ] <strong>Â§âÊèõ„Éë„É©„É°„Éº„Çø„ÅÆ‰øùÂ≠ò</strong></p>
</li>
<li>[ ] StandardScaler„ÅÆmean„ÄÅstd„Çípickle‰øùÂ≠ò</li>
<li>[ ] PCA„ÅÆcomponents_„Çí‰øùÂ≠ò</li>
<li>[ ] Êñ∞„Éá„Éº„Çø„Å∏„ÅÆÈÅ©Áî®ÊôÇ„Å´Âêå„ÅòÂ§âÊèõ„ÇíÈÅ©Áî®</li>
</ul>
<hr />
<h2>ÂèÇËÄÉÊñáÁåÆ</h2>
<ol>
<li>
<p><strong>Ward, L., Dunn, A., Faghaninia, A., et al.</strong> (2018). Matminer: An open source toolkit for materials data mining. <em>Computational Materials Science</em>, 152, 60-69. <a href="https://doi.org/10.1016/j.commatsci.2018.05.018">DOI: 10.1016/j.commatsci.2018.05.018</a></p>
</li>
<li>
<p><strong>Jolliffe, I. T. &amp; Cadima, J.</strong> (2016). Principal component analysis: a review and recent developments. <em>Philosophical Transactions of the Royal Society A</em>, 374(2065), 20150202. <a href="https://doi.org/10.1098/rsta.2015.0202">DOI: 10.1098/rsta.2015.0202</a></p>
</li>
<li>
<p><strong>McInnes, L., Healy, J., &amp; Melville, J.</strong> (2018). UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. <em>arXiv preprint arXiv:1802.03426</em>.</p>
</li>
<li>
<p><strong>Guyon, I. &amp; Elisseeff, A.</strong> (2003). An introduction to variable and feature selection. <em>Journal of Machine Learning Research</em>, 3, 1157-1182.</p>
</li>
</ol>
<hr />
<p><a href="chapter-1.html">‚Üê Chapter 1„Å´Êàª„Çã</a> | <a href="chapter-3.html">Chapter 3„Å∏ÈÄ≤„ÇÄ ‚Üí</a></p><div class="navigation">
    <a href="chapter-1.html" class="nav-button">‚Üê Á¨¨1Á´†</a>
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
    <a href="chapter-3.html" class="nav-button">Á¨¨3Á´† ‚Üí</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>Áõ£‰øÆ</strong>: Dr. Yusuke HashimotoÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-17</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
