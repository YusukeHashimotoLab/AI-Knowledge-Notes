---
title: ç¬¬5ç« ï¼šç¬¬ä¸€åŸç†è¨ˆç®—ã¨æ©Ÿæ¢°å­¦ç¿’ã®çµ±åˆ
chapter_title: ç¬¬5ç« ï¼šç¬¬ä¸€åŸç†è¨ˆç®—ã¨æ©Ÿæ¢°å­¦ç¿’ã®çµ±åˆ
subtitle: Machine Learning Potential ã¨ Active Learning
reading_time: 20-25åˆ†
difficulty: ä¸Šç´š
code_examples: 6
exercises: 0
---

# ç¬¬5ç« ï¼šç¬¬ä¸€åŸç†è¨ˆç®—ã¨æ©Ÿæ¢°å­¦ç¿’ã®çµ±åˆ

VASP/Quantum ESPRESSO/LAMMPSã‚’ä½¿ã£ãŸæœ€å°å®Ÿè¡Œã®é“ç­‹ã‚’ç¤ºã—ã¾ã™ã€‚å‰å¾Œå‡¦ç†ã®æ¨™æº–ãƒ„ãƒ¼ãƒ«ã‚‚ä¸€è¦§ã§æŠ¼ã•ãˆã¾ã™ã€‚

**ğŸ’¡ è£œè¶³:** ã¾ãšã¯å°ç³»ã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é€šã—ã€å…¥å‡ºåŠ›ã¨å˜ä½ç³»ã«æ…£ã‚Œã‚‹ã®ãŒè¿‘é“ã§ã™ã€‚

## å­¦ç¿’ç›®æ¨™

ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š \- æ©Ÿæ¢°å­¦ç¿’ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ï¼ˆMLPï¼‰ã®åŸºæœ¬æ¦‚å¿µã‚’ç†è§£ã™ã‚‹ \- Classical MDã€AIMDã€MLPã®é•ã„ã¨ä½¿ã„åˆ†ã‘ã‚’èª¬æ˜ã§ãã‚‹ \- DFTè¨ˆç®—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚’è¨“ç·´ã§ãã‚‹ \- Active Learningã«ã‚ˆã‚‹åŠ¹ç‡çš„ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆæˆ¦ç•¥ã‚’ç†è§£ã™ã‚‹ \- æœ€æ–°ã®Universal MLPã‚„Foundation Modelsã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æŠŠæ¡ã™ã‚‹

* * *

## 5.1 ãªãœMachine Learning PotentialãŒå¿…è¦ã‹

### 3ã¤ã®è¨ˆç®—æ‰‹æ³•ã®æ¯”è¼ƒ
    
    
    ```mermaid
    flowchart LR
        A[Classical MD] -->|ç²¾åº¦ vs é€Ÿåº¦| B[AIMD]
        B -->|ç²¾åº¦ vs é€Ÿåº¦| C[Machine Learning Potential]
        C - ãƒ‡ãƒ¼ã‚¿é§†å‹• .-> B
    
        style A fill:#ffcccc
        style B fill:#ccffcc
        style C fill:#ccccff
    ```

é …ç›® | Classical MD | AIMDï¼ˆDFT-MDï¼‰ | MLP-MD  
---|---|---|---  
**åŠ›ã®è¨ˆç®—** | çµŒé¨“çš„åŠ›å ´ | DFTï¼ˆç¬¬ä¸€åŸç†ï¼‰ | æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«  
**ç²¾åº¦** | ä¸­ï¼ˆåŠ›å ´ã«ä¾å­˜ï¼‰ | é«˜ï¼ˆé‡å­åŠ›å­¦çš„ï¼‰ | é«˜ï¼ˆDFTåŒç­‰ï¼‰  
**è¨ˆç®—é€Ÿåº¦** | è¶…é«˜é€Ÿï¼ˆns/æ—¥ï¼‰ | æ¥µã‚ã¦é…ã„ï¼ˆps/æ—¥ï¼‰ | é«˜é€Ÿï¼ˆns/æ—¥ï¼‰  
**ç³»ã®ã‚µã‚¤ã‚º** | æ•°ç™¾ä¸‡åŸå­ | æ•°ç™¾åŸå­ | æ•°åƒã€œæ•°ä¸‡åŸå­  
**é©ç”¨ç¯„å›²** | è¨“ç·´æ¸ˆã¿ç³»ã®ã¿ | æ±ç”¨çš„ | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç¯„å›²å†…  
**é–‹ç™ºã‚³ã‚¹ãƒˆ** | ä½ï¼ˆæ—¢å­˜åŠ›å ´ä½¿ç”¨ï¼‰ | ãªã— | é«˜ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼‰  
  
### MLPã®åˆ©ç‚¹

**ã€ŒDFTç´šã®ç²¾åº¦ã§Classical MDç´šã®é€Ÿåº¦ã€**

  * âœ… åŒ–å­¦åå¿œã‚’æ­£ç¢ºã«è¨˜è¿°ï¼ˆçµåˆã®åˆ‡æ–­ãƒ»ç”Ÿæˆï¼‰
  * âœ… é•·æ™‚é–“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆnsã€œÎ¼sã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
  * âœ… å¤§è¦æ¨¡ç³»ï¼ˆæ•°åƒã€œæ•°ä¸‡åŸå­ï¼‰
  * âœ… åŠ›å ´ãŒå­˜åœ¨ã—ãªã„æ–°è¦ææ–™ã«ã‚‚é©ç”¨å¯èƒ½

**èª²é¡Œ** : \- âŒ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆDFTè¨ˆç®—ï¼‰ã®ç”Ÿæˆã‚³ã‚¹ãƒˆ \- âŒ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²å¤–ã§ã¯ç²¾åº¦ä½ä¸‹ \- âŒ ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã«è¨ˆç®—è³‡æºã¨ãƒã‚¦ãƒã‚¦ãŒå¿…è¦

* * *

## 5.2 æ©Ÿæ¢°å­¦ç¿’ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã®ç¨®é¡

### 1\. Gaussian Approximation Potential (GAP)

**åŸç†** : ã‚«ãƒ¼ãƒãƒ«æ³•ï¼ˆGaussian Processï¼‰

$$ E_{\text{GAP}}(\mathbf{R}) = \sum_{i=1}^N \alpha_i K(\mathbf{R}, \mathbf{R}_i) $$

  * $K$: ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ï¼ˆé¡ä¼¼åº¦ã‚’æ¸¬ã‚‹ï¼‰
  * $\mathbf{R}_i$: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®åŸå­é…ç½®
  * $\alpha_i$: è¨“ç·´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

**ç‰¹å¾´** : \- âœ… ä¸ç¢ºå®Ÿæ€§æ¨å®šãŒå¯èƒ½ï¼ˆActive Learningã«æœ‰åˆ©ï¼‰ \- âœ… å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’å¯èƒ½ \- âŒ è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°ã«æ¯”ä¾‹ã—ã¦è¨ˆç®—ã‚³ã‚¹ãƒˆå¢—åŠ 

### 2\. Neural Network Potential (NNP)

**Behler-Parrinelloå‹** : å„åŸå­ã®ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã‚’è¨˜è¿°å­åŒ–

$$ E_{\text{NNP}} = \sum_{i=1}^{N_{\text{atoms}}} E_i^{\text{NN}}({\mathbf{G}_i}) $$

  * $E_i^{\text{NN}}$: åŸå­$i$ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒãƒ«ã‚®ãƒ¼
  * $\mathbf{G}_i$: å¯¾ç§°é–¢æ•°ï¼ˆSymmetry Functionsï¼‰ã€åŸå­$i$ã®å‘¨å›²ç’°å¢ƒã‚’è¨˜è¿°

**å¯¾ç§°é–¢æ•°ã®ä¾‹** ï¼ˆå‹•å¾„æˆåˆ†ï¼‰:

$$ G_i^{\text{rad}} = \sum_{j \neq i} e^{-\eta(r_{ij} - R_s)^2} f_c(r_{ij}) $$

  * $r_{ij}$: åŸå­é–“è·é›¢
  * $f_c(r)$: ã‚«ãƒƒãƒˆã‚ªãƒ•é–¢æ•°ï¼ˆä¸€å®šè·é›¢ä»¥é ã‚’ç„¡è¦–ï¼‰

**ç‰¹å¾´** : \- âœ… å¤§è¦æ¨¡ç³»ã§ã‚‚é«˜é€Ÿ \- âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°ãŒå¢—ãˆã¦ã‚‚è¨ˆç®—ã‚³ã‚¹ãƒˆä¸€å®š \- âŒ ä¸ç¢ºå®Ÿæ€§æ¨å®šãŒå›°é›£

### 3\. Message Passing Neural Network (MPNN)

ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆGNNï¼‰ã®ä¸€ç¨®ï¼š

$$ \mathbf{h}_i^{(k+1)} = \text{Update}\left(\mathbf{h}_i^{(k)}, \sum_{j \in \mathcal{N}(i)} \text{Message}(\mathbf{h}_i^{(k)}, \mathbf{h}_j^{(k)}, \mathbf{e}_{ij})\right) $$

  * $\mathbf{h}_i^{(k)}$: åŸå­$i$ã®$k$å±¤ç›®ã®éš ã‚ŒçŠ¶æ…‹
  * $\mathcal{N}(i)$: åŸå­$i$ã®è¿‘å‚åŸå­
  * $\mathbf{e}_{ij}$: çµåˆæƒ…å ±ï¼ˆè·é›¢ã€è§’åº¦ï¼‰

**ä»£è¡¨çš„ãƒ¢ãƒ‡ãƒ«** : SchNetã€DimeNetã€GemNetã€MACE

**ç‰¹å¾´** : \- âœ… å›è»¢ãƒ»ä¸¦é€²ä¸å¤‰æ€§ã‚’è‡ªç„¶ã«å®Ÿç¾ \- âœ… é•·è·é›¢ç›¸äº’ä½œç”¨ã‚’åŠ¹ç‡çš„ã«å­¦ç¿’ \- âœ… æœ€æ–°ã®é«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«

### 4\. Moment Tensor Potential (MTP)

**åŸç†** : åŸå­ç’°å¢ƒã‚’å¤šä½“å±•é–‹ã§è¨˜è¿°

$$ E_{\text{MTP}} = \sum_i \sum_{\alpha} c_{\alpha} B_{\alpha}(\mathbf{R}_i) $$

$B_{\alpha}$ã¯ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆãƒ†ãƒ³ã‚½ãƒ«åŸºåº•é–¢æ•°ã€‚

**ç‰¹å¾´** : \- âœ… é«˜é€Ÿï¼ˆç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼‰ \- âœ… è¨“ç·´ãŒå®¹æ˜“ \- âŒ è¡¨ç¾åŠ›ãŒNNPã‚ˆã‚Šä½ã„

* * *

## 5.3 Neural Network Potentialã®è¨“ç·´ï¼ˆå®Ÿè·µï¼‰

### Example 1: AMPã‚’ä½¿ã£ãŸNNPè¨“ç·´ï¼ˆæ°´åˆ†å­ï¼‰
    
    
    import numpy as np
    from ase.build import molecule
    from ase.calculators.emt import EMT
    from gpaw import GPAW, PW
    from amp import Amp
    from amp.descriptor.gaussian import Gaussian
    from amp.model.neuralnetwork import NeuralNetwork
    import matplotlib.pyplot as plt
    
    # Step 1: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆMDã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ + DFTï¼‰
    def generate_training_data(n_samples=50):
        """
        æ°´åˆ†å­ã®æ§˜ã€…ãªé…ç½®ã§DFTè¨ˆç®—
        """
        from ase.md.velocitydistribution import MaxwellBoltzmannDistribution
        from ase.md.verlet import VelocityVerlet
        from ase import units
    
        h2o = molecule('H2O')
        h2o.center(vacuum=5.0)
    
        # DFTè¨ˆç®—æ©Ÿ
        calc = GPAW(mode=PW(300), xc='PBE', txt=None)
        h2o.calc = calc
    
        # åˆæœŸé€Ÿåº¦
        MaxwellBoltzmannDistribution(h2o, temperature_K=500)
    
        # MDã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        dyn = VelocityVerlet(h2o, timestep=1.0*units.fs)
    
        images = []
        for i in range(n_samples):
            dyn.run(10)  # 10ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            atoms_copy = h2o.copy()
            atoms_copy.calc = calc
            atoms_copy.get_potential_energy()  # DFTè¨ˆç®—å®Ÿè¡Œ
            atoms_copy.get_forces()
            images.append(atoms_copy)
            print(f"Sample {i+1}/{n_samples} collected")
    
        return images
    
    print("Generating training data...")
    train_images = generate_training_data(n_samples=50)
    
    # Step 2: NNPã®è¨“ç·´
    print("Training Neural Network Potential...")
    
    # è¨˜è¿°å­: Gaussianå¯¾ç§°é–¢æ•°
    descriptor = Gaussian()
    
    # ãƒ¢ãƒ‡ãƒ«: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    model = NeuralNetwork(hiddenlayers=(10, 10, 10))  # 3å±¤ã€å„10ãƒãƒ¼ãƒ‰
    
    # AMPãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«
    calc_nnp = Amp(descriptor=descriptor,
                   model=model,
                   label='h2o_nnp',
                   dblabel='h2o_nnp')
    
    # è¨“ç·´
    calc_nnp.train(images=train_images,
                   energy_coefficient=1.0,
                   force_coefficient=0.04)
    
    print("Training complete!")
    
    # Step 3: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ç²¾åº¦è©•ä¾¡
    print("\nGenerating test data...")
    test_images = generate_training_data(n_samples=10)
    
    E_dft = []
    E_nnp = []
    F_dft = []
    F_nnp = []
    
    for atoms in test_images:
        # DFT
        atoms.calc = GPAW(mode=PW(300), xc='PBE', txt=None)
        e_dft = atoms.get_potential_energy()
        f_dft = atoms.get_forces().flatten()
    
        # NNP
        atoms.calc = calc_nnp
        e_nnp = atoms.get_potential_energy()
        f_nnp = atoms.get_forces().flatten()
    
        E_dft.append(e_dft)
        E_nnp.append(e_nnp)
        F_dft.extend(f_dft)
        F_nnp.extend(f_nnp)
    
    E_dft = np.array(E_dft)
    E_nnp = np.array(E_nnp)
    F_dft = np.array(F_dft)
    F_nnp = np.array(F_nnp)
    
    # ãƒ—ãƒ­ãƒƒãƒˆ
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # ã‚¨ãƒãƒ«ã‚®ãƒ¼
    axes[0].scatter(E_dft, E_nnp, alpha=0.6)
    axes[0].plot([E_dft.min(), E_dft.max()],
                 [E_dft.min(), E_dft.max()], 'r--', label='Perfect')
    axes[0].set_xlabel('DFT Energy (eV)', fontsize=12)
    axes[0].set_ylabel('NNP Energy (eV)', fontsize=12)
    axes[0].set_title('Energy Prediction', fontsize=14)
    mae_e = np.mean(np.abs(E_dft - E_nnp))
    axes[0].text(0.05, 0.95, f'MAE = {mae_e:.3f} eV',
                transform=axes[0].transAxes, va='top')
    axes[0].legend()
    axes[0].grid(alpha=0.3)
    
    # åŠ›
    axes[1].scatter(F_dft, F_nnp, alpha=0.3, s=10)
    axes[1].plot([F_dft.min(), F_dft.max()],
                 [F_dft.min(), F_dft.max()], 'r--', label='Perfect')
    axes[1].set_xlabel('DFT Force (eV/Ã…)', fontsize=12)
    axes[1].set_ylabel('NNP Force (eV/Ã…)', fontsize=12)
    axes[1].set_title('Force Prediction', fontsize=14)
    mae_f = np.mean(np.abs(F_dft - F_nnp))
    axes[1].text(0.05, 0.95, f'MAE = {mae_f:.3f} eV/Ã…',
                transform=axes[1].transAxes, va='top')
    axes[1].legend()
    axes[1].grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('nnp_accuracy.png', dpi=150)
    plt.show()
    
    print(f"\nNNP Accuracy:")
    print(f"Energy MAE: {mae_e:.4f} eV")
    print(f"Force MAE: {mae_f:.4f} eV/Ã…")
    

**ç›®æ¨™ç²¾åº¦** : \- ã‚¨ãƒãƒ«ã‚®ãƒ¼: MAE < 1 meV/atom \- åŠ›: MAE < 0.1 eV/Ã…

* * *

## 5.4 Active Learning

### åŸºæœ¬çš„ãªè€ƒãˆæ–¹

**å•é¡Œ** : ã™ã¹ã¦ã®é…ç½®ã§DFTè¨ˆç®—ã‚’è¡Œã†ã®ã¯éç¾å®Ÿçš„ï¼ˆè¨ˆç®—ã‚³ã‚¹ãƒˆå¤§ï¼‰

**è§£æ±ºç­–** : **æœ€ã‚‚æƒ…å ±é‡ã®å¤šã„é…ç½®ã‚’å„ªå…ˆçš„ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**
    
    
    ```mermaid
    flowchart TD
        A[åˆæœŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå°‘æ•°] --> B[NNPè¨“ç·´]
        B --> C[MDã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ with NNP]
        C --> D[ä¸ç¢ºå®Ÿæ€§ã®é«˜ã„é…ç½®ã‚’æ¤œå‡º]
        D --> E{è¿½åŠ ãƒ‡ãƒ¼ã‚¿å¿…è¦?}
        E -->|Yes| F[DFTè¨ˆç®—è¿½åŠ ãƒ‡ãƒ¼ã‚¿]
        F --> G[ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«è¿½åŠ ]
        G --> B
        E -->|No| H[è¨“ç·´å®Œäº†]
    
        style A fill:#e3f2fd
        style H fill:#c8e6c9
    ```

### ä¸ç¢ºå®Ÿæ€§æ¨å®šã®æ–¹æ³•

**1\. Ensembleæ³•** : \- è¤‡æ•°ã®NNPã‚’è¨“ç·´ï¼ˆç•°ãªã‚‹åˆæœŸå€¤ã€ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼‰ \- äºˆæ¸¬ã®ã°ã‚‰ã¤ãï¼ˆåˆ†æ•£ï¼‰ã‚’ä¸ç¢ºå®Ÿæ€§ã¨ã™ã‚‹

$$ \sigma_E^2 = \frac{1}{M}\sum_{m=1}^M (E_m - \bar{E})^2 $$

**2\. Dropoutæ³•** : \- è¨“ç·´æ™‚ã«ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒãƒ¼ãƒ‰ã‚’ç„¡åŠ¹åŒ– \- æ¨è«–æ™‚ã«ã‚‚Dropoutã‚’é©ç”¨ã—ã€è¤‡æ•°å›äºˆæ¸¬ \- äºˆæ¸¬ã®ã°ã‚‰ã¤ãã‚’ä¸ç¢ºå®Ÿæ€§ã¨ã™ã‚‹

**3\. Query-by-Committee** : \- ç•°ãªã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¤‡æ•°ä½¿ç”¨ \- äºˆæ¸¬ã®ä¸€è‡´åº¦ãŒä½ã„é…ç½®ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

### Active Learningå®Ÿè£…ä¾‹
    
    
    import numpy as np
    from ase.md.langevin import Langevin
    from ase import units
    
    def active_learning_loop(initial_images, n_iterations=5, n_md_steps=1000):
        """
        Active Learningã«ã‚ˆã‚‹åŠ¹ç‡çš„è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        """
        dataset = initial_images.copy()
    
        for iteration in range(n_iterations):
            print(f"\n--- Iteration {iteration+1}/{n_iterations} ---")
    
            # Step 1: NNPã‚’è¨“ç·´
            print("Training NNP...")
            nnp = train_nnp(dataset)  # å‰è¿°ã®Ampè¨“ç·´
    
            # Step 2: NNPã§MDã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            print("Running MD with NNP...")
            h2o = dataset[0].copy()
            h2o.calc = nnp
    
            # Langevin MDï¼ˆç†±æµ´ä»˜ãï¼‰
            dyn = Langevin(h2o, timestep=1.0*units.fs,
                           temperature_K=500, friction=0.01)
    
            # ä¸ç¢ºå®Ÿæ€§ã®é«˜ã„é…ç½®ã‚’åé›†
            uncertain_images = []
            uncertainties = []
    
            for step in range(n_md_steps):
                dyn.run(1)
    
                # Ensembleã§ä¸ç¢ºå®Ÿæ€§æ¨å®šï¼ˆç°¡ç•¥åŒ–ï¼‰
                # å®Ÿéš›ã«ã¯è¤‡æ•°ã®NNPã§äºˆæ¸¬ã—ã¦ã°ã‚‰ã¤ãã‚’è¨ˆç®—
                uncertainty = estimate_uncertainty(h2o, nnp)  # ä»®æƒ³é–¢æ•°
    
                if uncertainty > threshold:  # é–¾å€¤ä»¥ä¸Šãªã‚‰è¿½åŠ 
                    atoms_copy = h2o.copy()
                    uncertain_images.append(atoms_copy)
                    uncertainties.append(uncertainty)
    
            print(f"Found {len(uncertain_images)} uncertain configurations")
    
            # Step 3: ä¸ç¢ºå®Ÿæ€§ã®é«˜ã„é…ç½®ã§DFTè¨ˆç®—
            print("Running DFT for uncertain configurations...")
            for atoms in uncertain_images[:10]:  # ä¸Šä½10å€‹
                atoms.calc = GPAW(mode=PW(300), xc='PBE', txt=None)
                atoms.get_potential_energy()
                atoms.get_forces()
                dataset.append(atoms)
    
            print(f"Dataset size: {len(dataset)}")
    
        return dataset, nnp
    
    # å®Ÿè¡Œ
    initial_data = generate_training_data(n_samples=20)
    final_dataset, final_nnp = active_learning_loop(initial_data, n_iterations=5)
    
    print(f"\nFinal dataset size: {len(final_dataset)}")
    print(f"vs. random sampling: 50-100 samples would be needed")
    print(f"Efficiency gain: {100/len(final_dataset):.1f}x")
    

**Active Learningã®åˆ©ç‚¹** : \- è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°ã‚’50-90%å‰Šæ¸›å¯èƒ½ \- é‡è¦ãªé…ç½®ï¼ˆç›¸è»¢ç§»ã€åå¿œçµŒè·¯ï¼‰ã‚’å„ªå…ˆçš„ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° \- è¨ˆç®—è³‡æºã®åŠ¹ç‡çš„åˆ©ç”¨

* * *

## 5.5 æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰

### 1\. Universal Machine Learning Potential

**ç›®æ¨™** : 1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã§å¤šæ§˜ãªææ–™ç³»ã‚’ã‚«ãƒãƒ¼

**ä»£è¡¨ä¾‹** : \- **CHGNet** ï¼ˆ2023å¹´ï¼‰: 140ä¸‡ææ–™ã®Materials Projectãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ \- 89å…ƒç´ ã‚’ã‚«ãƒãƒ¼ \- ç£æ€§ã‚‚è€ƒæ…® \- ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹

  * **M3GNet** ï¼ˆ2022å¹´ï¼‰: å¤šä½“ã‚°ãƒ©ãƒ•ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
  * çµæ™¶ã€è¡¨é¢ã€åˆ†å­ã«é©ç”¨å¯èƒ½
  * åŠ›ã€å¿œåŠ›ã€ç£æ°—ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã‚’äºˆæ¸¬

  * **MACE** ï¼ˆ2023å¹´ï¼‰: ç­‰å¤‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°

  * é«˜ç²¾åº¦ï¼ˆDFTèª¤å·®ã®ç´„2å€ç¨‹åº¦ã®èª¤å·®ï¼‰
  * å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´å¯èƒ½

**ä½¿ã„æ–¹** :
    
    
    from chgnet.model import CHGNet
    from pymatgen.core import Structure
    
    # äº‹å‰è¨“ç·´ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    model = CHGNet.load()
    
    # ä»»æ„ã®çµæ™¶æ§‹é€ ã§äºˆæ¸¬
    structure = Structure.from_file('POSCAR')
    energy = model.predict_structure(structure)
    
    print(f"Predicted energy: {energy} eV")
    

### 2\. Foundation Models for Materials

**å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®ææ–™ç§‘å­¦ç‰ˆ** :

  * **MatGPT** : ææ–™ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§äº‹å‰å­¦ç¿’
  * **LLaMat** : çµæ™¶æ§‹é€ â†’ç‰¹æ€§äºˆæ¸¬

**è»¢ç§»å­¦ç¿’** : \- å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’ \- å°‘æ•°ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° \- 10-100ã‚µãƒ³ãƒ—ãƒ«ã§å®Ÿç”¨ç²¾åº¦

### 3\. è‡ªå¾‹å®Ÿé¨“ã¸ã®å¿œç”¨

**ã‚¯ãƒ­ãƒ¼ã‚ºãƒ‰ãƒ«ãƒ¼ãƒ—æœ€é©åŒ–** :
    
    
    MLäºˆæ¸¬ â†’ æœ€é©å€™è£œææ¡ˆ â†’ ãƒ­ãƒœãƒƒãƒˆå®Ÿé¨“ â†’ æ¸¬å®š â†’ ãƒ‡ãƒ¼ã‚¿è“„ç© â†’ MLå†è¨“ç·´
    

**å®Ÿä¾‹** : \- **A-Lab** ï¼ˆBerkeley, 2023å¹´ï¼‰: 41ææ–™ã‚’17æ—¥ã§åˆæˆãƒ»è©•ä¾¡ \- **è‡ªå¾‹ææ–™æ¢ç´¢** : è§¦åª’ã€é›»æ± ææ–™ã€é‡å­ãƒ‰ãƒƒãƒˆ

* * *

## 5.6 MLPã®å®Ÿç”¨ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

### ã„ã¤MLPã‚’ä½¿ã†ã¹ãã‹

**é©ã—ã¦ã„ã‚‹å ´åˆ** : \- âœ… é•·æ™‚é–“MDï¼ˆns-Î¼sï¼‰ãŒå¿…è¦ \- âœ… å¤§è¦æ¨¡ç³»ï¼ˆæ•°åƒåŸå­ä»¥ä¸Šï¼‰ \- âœ… åŒ–å­¦åå¿œã‚’å«ã‚€ \- âœ… åŠ›å ´ãŒå­˜åœ¨ã—ãªã„æ–°è¦ææ–™ \- âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã®è¨ˆç®—è³‡æºãŒã‚ã‚‹

**é©ã•ãªã„å ´åˆ** : \- âŒ 1å›é™ã‚Šã®çŸ­æ™‚é–“MDï¼ˆç›´æ¥AIMDãŒç°¡å˜ï¼‰ \- âŒ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ä»£è¡¨æ€§ã‚’ç¢ºä¿ã§ããªã„ \- âŒ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç¯„å›²å¤–ã®å¤–æŒ¿ãŒå¿…è¦ \- âŒ æ—¢å­˜ã®é«˜ç²¾åº¦åŠ›å ´ãŒã‚ã‚‹ï¼ˆReaxFFã€COMBç­‰ï¼‰

### å®Ÿè£…ã®æµã‚Œ
    
    
    ```mermaid
    flowchart TD
        A[å•é¡Œè¨­å®š] --> B[åˆæœŸãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ 20-100ã‚µãƒ³ãƒ—ãƒ«]
        B --> C[NNPè¨“ç·´]
        C --> D[æ¤œè¨¼ã‚»ãƒƒãƒˆã§ç²¾åº¦è©•ä¾¡]
        D --> E{ç²¾åº¦OK?}
        E -->|No| F[Active Learning]
        F --> G[è¿½åŠ DFTè¨ˆç®—]
        G --> C
        E -->|Yes| H[æœ¬ç•ªMDã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³]
        H --> I[ç‰©æ€§è¨ˆç®—]
    
        style A fill:#e3f2fd
        style I fill:#c8e6c9
    ```

### æ¨å¥¨ãƒ„ãƒ¼ãƒ«

ãƒ„ãƒ¼ãƒ« | æ‰‹æ³• | ç‰¹å¾´  
---|---|---  
**AMP** | NNP | Pythonãƒã‚¤ãƒ†ã‚£ãƒ–ã€ASEçµ±åˆ  
**DeePMD** | NNP | é«˜é€Ÿã€ä¸¦åˆ—åŒ–ã€TensorFlow  
**SchNetPack** | GNN | SchNetã€ç ”ç©¶å‘ã‘  
**MACE** | Equivariant GNN | æœ€æ–°ã€é«˜ç²¾åº¦  
**GAP** | Gaussian Process | ä¸ç¢ºå®Ÿæ€§æ¨å®š  
**MTP** | Moment Tensor | é«˜é€Ÿè¨“ç·´  
**CHGNet** | Universal | äº‹å‰è¨“ç·´æ¸ˆã¿  
  
* * *

## 5.7 æœ¬ç« ã®ã¾ã¨ã‚

### å­¦ã‚“ã ã“ã¨

  1. **MLPã®å¿…è¦æ€§** \- DFTç´šç²¾åº¦ + Classical MDç´šé€Ÿåº¦ \- é•·æ™‚é–“ãƒ»å¤§è¦æ¨¡ç³»ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

  2. **MLPã®ç¨®é¡** \- GAPï¼ˆGaussian Processï¼‰ \- NNPï¼ˆNeural Networkï¼‰ \- MPNNï¼ˆGraph Neural Networkï¼‰ \- MTPï¼ˆMoment Tensorï¼‰

  3. **NNPã®è¨“ç·´** \- DFTãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ \- AMPã§ã®å®Ÿè£… \- ç²¾åº¦è©•ä¾¡

  4. **Active Learning** \- ä¸ç¢ºå®Ÿæ€§æ¨å®š \- åŠ¹ç‡çš„ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ \- 50-90%ã®è¨ˆç®—é‡å‰Šæ¸›

  5. **æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰** \- Universal MLPï¼ˆCHGNetã€M3GNetï¼‰ \- Foundation Models \- è‡ªå¾‹å®Ÿé¨“

### é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

  * MLPã¯è¨ˆç®—ææ–™ç§‘å­¦ã®æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ 
  * Active LearningãŒè¨“ç·´åŠ¹ç‡ã®éµ
  * Universal MLPã§äº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½
  * å®Ÿç”¨åŒ–ãŒé€²ã‚“ã§ã„ã‚‹ï¼ˆè‡ªå¾‹å®Ÿé¨“ã€ææ–™æ¢ç´¢ï¼‰

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

  * è‡ªåˆ†ã®ç ”ç©¶ãƒ†ãƒ¼ãƒã§MLPã‚’è©¦ã™
  * æœ€æ–°è«–æ–‡ã‚’è¿½ã†ï¼ˆ _npj Computational Materials_ , _Nature Materials_ ï¼‰
  * ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ„ãƒ¼ãƒ«ã«è²¢çŒ®
  * å®Ÿé¨“ç ”ç©¶è€…ã¨ã®å…±åŒç ”ç©¶

* * *

## æ¼”ç¿’å•é¡Œ

### å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰

Classical MDã€AIMDã€MLP-MDã®é•ã„ã‚’è¡¨ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

è§£ç­”ä¾‹ | é …ç›® | Classical MD | AIMDï¼ˆDFT-MDï¼‰ | MLP-MD | |-----|-------------|-------------|--------| | **åŠ›ã®è¨ˆç®—æ³•** | çµŒé¨“çš„åŠ›å ´ï¼ˆè§£æå¼ï¼‰ | DFTï¼ˆç¬¬ä¸€åŸç†ï¼‰ | æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ« | | **ç²¾åº¦** | ä¸­ï¼ˆåŠ›å ´ã®è³ªã«ä¾å­˜ï¼‰ | é«˜ï¼ˆé‡å­åŠ›å­¦çš„ã«æ­£ç¢ºï¼‰ | é«˜ï¼ˆDFTã¨åŒç­‰ï¼‰ | | **è¨ˆç®—é€Ÿåº¦** | è¶…é«˜é€Ÿï¼ˆ1 ns/æ—¥ï¼‰ | æ¥µã‚ã¦é…ã„ï¼ˆ10 ps/æ—¥ï¼‰ | é«˜é€Ÿï¼ˆ1 ns/æ—¥ï¼‰ | | **ç³»ã®ã‚µã‚¤ã‚º** | æ•°ç™¾ä¸‡åŸå­ | æ•°ç™¾åŸå­ | æ•°åƒã€œæ•°ä¸‡åŸå­ | | **åŒ–å­¦åå¿œ** | è¨˜è¿°ä¸å¯ï¼ˆReaxFFã¯å¯ï¼‰ | æ­£ç¢ºã«è¨˜è¿° | æ­£ç¢ºã«è¨˜è¿° | | **é©ç”¨ç¯„å›²** | åŠ›å ´ãŒè¨“ç·´ã•ã‚ŒãŸç³»ã®ã¿ | æ±ç”¨çš„ | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç¯„å›²å†… | | **é–‹ç™ºã‚³ã‚¹ãƒˆ** | ä½ï¼ˆæ—¢å­˜åŠ›å ´ï¼‰ | ãªã— | é«˜ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼‰ | | **ç”¨é€”** | æ‹¡æ•£ã€ç›¸è»¢ç§»ã€å¤§è¦æ¨¡ | åŒ–å­¦åå¿œã€é›»å­çŠ¶æ…‹ | åå¿œ+é•·æ™‚é–“MD | **ä½¿ã„åˆ†ã‘ã®ç›®å®‰**: \- æ—¢çŸ¥ã®åŠ›å ´ãŒã‚ã‚‹ â†’ Classical MD \- åŒ–å­¦åå¿œã‚’å«ã‚€çŸ­æ™‚é–“ â†’ AIMD \- åŒ–å­¦åå¿œ+é•·æ™‚é–“ â†’ MLP-MD \- æ–°è¦ææ–™ã®æ¢ç´¢ â†’ AIMD â†’ MLP â†’ å¤§è¦æ¨¡MD 

### å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰

Active LearningãŒãªãœåŠ¹ç‡çš„ãªã®ã‹ã€å…·ä½“ä¾‹ã¨ã¨ã‚‚ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

è§£ç­”ä¾‹ **Active Learningã®åŸºæœ¬åŸç†**: å¾“æ¥ã®æ©Ÿæ¢°å­¦ç¿’ï¼ˆRandom Samplingï¼‰: \- ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° \- å¤šãã®ãƒ‡ãƒ¼ã‚¿ãŒã€Œæ—¢çŸ¥ã®é ˜åŸŸã€ã®é‡è¤‡ \- éåŠ¹ç‡ Active Learningï¼ˆUncertainty Samplingï¼‰: \- ãƒ¢ãƒ‡ãƒ«ãŒã€Œä¸ç¢ºå®Ÿã€ãªé…ç½®ã‚’å„ªå…ˆçš„ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° \- æ–°ã—ã„æƒ…å ±ã‚’åŠ¹ç‡çš„ã«ç²å¾— \- å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§é«˜ç²¾åº¦ **å…·ä½“ä¾‹: æ°´åˆ†å­ã®NNPè¨“ç·´** **Random Samplingï¼ˆå¾“æ¥æ³•ï¼‰**: \- 300Kå¹³è¡¡çŠ¶æ…‹ã‹ã‚‰100é…ç½®ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° \- ãã®ã†ã¡80%ã¯å¹³è¡¡æ§‹é€ ã®è¿‘å‚ï¼ˆé¡ä¼¼é…ç½®ï¼‰ \- æ®‹ã‚Š20%ãŒåå¿œçµŒè·¯ã‚„é«˜ã‚¨ãƒãƒ«ã‚®ãƒ¼é…ç½® \- çµæœ: 100 DFTè¨ˆç®—ã€ç²¾åº¦ MAE = 5 meV/atom **Active Learning**: \- åˆæœŸ20é…ç½®ã‹ã‚‰è¨“ç·´ \- MDã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¸­ã«ä¸ç¢ºå®Ÿæ€§ã®é«˜ã„é…ç½®ã‚’æ¤œå‡º \- O-HçµåˆãŒä¼¸ã³ãŸé…ç½®ï¼ˆè§£é›¢éç¨‹ï¼‰ \- H-O-Hè§’åº¦ãŒå¤§ããæ­ªã‚“ã é…ç½® \- é«˜ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ±èµ·çŠ¶æ…‹ \- ã“ã‚Œã‚‰ã®é…ç½®ã§DFTè¨ˆç®—ï¼ˆ20é…ç½®è¿½åŠ ï¼‰ \- åˆè¨ˆ40 DFTè¨ˆç®—ã€ç²¾åº¦ MAE = 3 meV/atom **åŠ¹ç‡åŒ–ã®ç†ç”±**: 1\. **æƒ…å ±é‡ã®æœ€å¤§åŒ–**: \- é¡ä¼¼é…ç½®ã®é‡è¤‡ã‚’é¿ã‘ã‚‹ \- ãƒ¢ãƒ‡ãƒ«ãŒã€ŒçŸ¥ã‚‰ãªã„ã€é ˜åŸŸã‚’å„ªå…ˆ 2\. **æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹**: \- æ—¢çŸ¥ã®é…ç½®ã§ã®å®‰å®šäºˆæ¸¬ï¼ˆæ´»ç”¨ï¼‰ \- æœªçŸ¥ã®é…ç½®ã§ã®æ–°æƒ…å ±ç²å¾—ï¼ˆæ¢ç´¢ï¼‰ 3\. **é©å¿œçš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**: \- ç³»ã®é‡è¦é ˜åŸŸï¼ˆåå¿œçµŒè·¯ã€ç›¸è»¢ç§»ï¼‰ã‚’è‡ªå‹•æ¤œå‡º \- äººé–“ã®ç›´æ„Ÿã«é ¼ã‚‰ãªã„ **å®Ÿéš›ã®åŠ¹ç‡åŒ–**: \- 50-90%ã®DFTè¨ˆç®—å‰Šæ¸›ï¼ˆæ–‡çŒ®å€¤ï¼‰ \- ç‰¹ã«è¤‡é›‘ãªç³»ï¼ˆå¤šæˆåˆ†ã€åå¿œç³»ï¼‰ã§åŠ¹æœå¤§ \- è¨“ç·´æ™‚é–“å…¨ä½“ã§ã¯10-50å€ã®åŠ¹ç‡åŒ– **ä¾‹: Li-ioné›»æ± é›»è§£æ¶²**: \- Random: 10,000 DFTè¨ˆç®—ã€2ãƒ¶æœˆ \- Active Learning: 2,000 DFTè¨ˆç®—ã€2é€±é–“ \- åŠ¹ç‡åŒ–: 5å€ã€åŒç­‰ç²¾åº¦ 

### å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰

Universal Machine Learning Potentialï¼ˆCHGNetã€M3GNetç­‰ï¼‰ã®åˆ©ç‚¹ã¨é™ç•Œã‚’è­°è«–ã—ã¦ãã ã•ã„ã€‚

è§£ç­”ä¾‹ **Universal MLPï¼ˆä¾‹: CHGNetï¼‰ã®æ¦‚è¦**: \- **è¨“ç·´ãƒ‡ãƒ¼ã‚¿**: Materials Projectï¼ˆ140ä¸‡ææ–™ã€89å…ƒç´ ï¼‰ \- **ãƒ¢ãƒ‡ãƒ«**: Graph Neural Network \- **äºˆæ¸¬**: ã‚¨ãƒãƒ«ã‚®ãƒ¼ã€åŠ›ã€å¿œåŠ›ã€ç£æ°—ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆ **åˆ©ç‚¹**: 1\. **å³åº§ã«ä½¿ãˆã‚‹**: \- äº‹å‰è¨“ç·´æ¸ˆã¿ â†’ è¿½åŠ è¨“ç·´ä¸è¦ \- ä»»æ„ã®çµæ™¶æ§‹é€ ã§äºˆæ¸¬å¯èƒ½ \- æ•°ç§’ã§æ•°åƒææ–™ã‚’ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° 2\. **åºƒã„é©ç”¨ç¯„å›²**: \- 89å…ƒç´ ï¼ˆH-Amã¾ã§ï¼‰ \- é…¸åŒ–ç‰©ã€åˆé‡‘ã€åŠå°ä½“ã€çµ¶ç¸ä½“ \- ç£æ€§ææ–™ã‚‚å¯¾å¿œ 3\. **è»¢ç§»å­¦ç¿’ã®åŸºç›¤**: \- å°‘æ•°ãƒ‡ãƒ¼ã‚¿ï¼ˆ10-100ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° \- ç³»ç‰¹åŒ–ã®é«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹ç‡çš„ã«ä½œæˆ 4\. **ææ–™æ¢ç´¢ã®åŠ é€Ÿ**: \- å¤§è¦æ¨¡å€™è£œã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆ100ä¸‡ææ–™/æ—¥ï¼‰ \- å®Ÿé¨“å€™è£œã®çµã‚Šè¾¼ã¿ \- ãƒã‚¤ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—ã¨ã®çµ„ã¿åˆã‚ã› **é™ç•Œ**: 1\. **ç²¾åº¦ã®é™ç•Œ**: \- DFTèª¤å·®ã®ç´„2-5å€ç¨‹åº¦ï¼ˆCHGNet: MAE ~30 meV/atomï¼‰ \- ç²¾å¯†è¨ˆç®—ã«ã¯ä¸ååˆ† \- ç‰¹å®šç³»ã§ã¯å°‚ç”¨MLPã«åŠ£ã‚‹ 2\. **å¤–æŒ¿ã®å•é¡Œ**: \- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ãªã„é…ç½®ï¼ˆæ¥µç«¯ãªæ¸©åº¦ãƒ»åœ§åŠ›ï¼‰ã§ç²¾åº¦ä½ä¸‹ \- æ–°è¦ææ–™ç³»ï¼ˆè¶…é«˜åœ§ã€æ–°å…ƒç´ çµ„ã¿åˆã‚ã›ï¼‰ã¯ä¸ç¢ºå®Ÿ 3\. **ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ã‚¢ã‚¹**: \- Materials Projectã®è¨ˆç®—æ¡ä»¶ï¼ˆPBEæ±é–¢æ•°ï¼‰ã«ä¾å­˜ \- å®Ÿé¨“ã¨ã®ç³»çµ±çš„ãªãšã‚Œï¼ˆãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—éå°è©•ä¾¡ç­‰ï¼‰ \- ç‰¹å®šææ–™ã‚¯ãƒ©ã‚¹ã®éå‰°/éå°‘è¡¨ç¾ 4\. **ç‰©ç†çš„åˆ¶ç´„ã®æ¬ å¦‚**: \- ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜å‰‡ã®å³å¯†ãªä¿è¨¼ãªã— \- é•·æ™‚é–“MDã§ã®ãƒ‰ãƒªãƒ•ãƒˆ \- å¯¾ç§°æ€§ã®ç ´ã‚Œï¼ˆç¨€ï¼‰ **å®Ÿç”¨çš„æˆ¦ç•¥**: **Scenario 1: ææ–™ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°** \- Universal MLPã§100ä¸‡å€™è£œã‹ã‚‰ä¸Šä½1000ã«çµã‚Šè¾¼ã¿ \- DFTã§ç²¾å¯†è¨ˆç®— \- åŠ¹ç‡åŒ–: 1000å€ **Scenario 2: ç‰¹å®šç³»ã®ç²¾å¯†MD** \- Universal MLPã‹ã‚‰è»¢ç§»å­¦ç¿’ \- ç³»ç‰¹åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆ100ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã§è¿½åŠ è¨“ç·´ \- ç²¾åº¦å‘ä¸Š: MAE 5 meV/atomï¼ˆå®Ÿç”¨ãƒ¬ãƒ™ãƒ«ï¼‰ **Scenario 3: æ–°è¦ææ–™ã‚¯ãƒ©ã‚¹** \- Universal MLPã¯å‚è€ƒç¨‹åº¦ \- ã‚¼ãƒ­ã‹ã‚‰å°‚ç”¨MLPæ§‹ç¯‰ï¼ˆActive Learningï¼‰ \- è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 500-1000ã‚µãƒ³ãƒ—ãƒ« **å°†æ¥å±•æœ›**: 1\. **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ‹¡å……**: \- å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ \- å¤šæ§˜ãªè¨ˆç®—æ‰‹æ³•ï¼ˆGWã€DMFTï¼‰ã®ãƒ‡ãƒ¼ã‚¿ 2\. **Foundation Modelsã¸ã®é€²åŒ–**: \- è‡ªç„¶è¨€èªå‡¦ç†ã®GPTã«ç›¸å½“ \- Few-shot learningï¼ˆæ•°ã‚µãƒ³ãƒ—ãƒ«ã§é©å¿œï¼‰ \- Zero-shot transferï¼ˆè¨“ç·´ãªã—ã§æ–°è¦ç³»ï¼‰ 3\. **å®Ÿé¨“ã¨ã®é€£æº**: \- è‡ªå¾‹å®Ÿé¨“ãƒ«ãƒ¼ãƒ— \- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ **çµè«–**: Universal MLPã¯ææ–™ç§‘å­¦ã®ã€ŒåŸºç›¤ã‚¤ãƒ³ãƒ•ãƒ©ã€ã¨ãªã‚Šã¤ã¤ã‚ã‚‹ãŒã€ä¸‡èƒ½ã§ã¯ãªã„ã€‚ç”¨é€”ã«å¿œã˜ã¦å°‚ç”¨MLPã¨ä½¿ã„åˆ†ã‘ãŒé‡è¦ã€‚ 

* * *

## ãƒ‡ãƒ¼ã‚¿ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¨å¼•ç”¨

### ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

  1. **Materials Project Database** (CC BY 4.0) \- 140ä¸‡ææ–™ã®DFTãƒ‡ãƒ¼ã‚¿ï¼ˆCHGNetè¨“ç·´ï¼‰ \- URL: https://materialsproject.org \- å¼•ç”¨: Jain, A., et al. (2013). _APL Materials_ , 1, 011002.

  2. **Open Catalyst Project** (CC BY 4.0) \- è§¦åª’è¡¨é¢ã®DFTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ \- URL: https://opencatalystproject.org/

  3. **QM9 Dataset** (CC0) \- å°åˆ†å­134,000å€‹ã®DFTãƒ‡ãƒ¼ã‚¿ \- URL: http://quantum-machine.org/datasets/

### ä½¿ç”¨ã—ãŸã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢

  1. **AMP - Atomistic Machine-learning Package** (GPL v3) \- URL: https://amp.readthedocs.io/

  2. **CHGNet** (MIT License) \- Universal ML Potential \- URL: https://github.com/CederGroupHub/chgnet

  3. **M3GNet** (BSD 3-Clause) \- Graph Neural Network Potential \- URL: https://github.com/materialsvirtuallab/m3gnet

  4. **MACE** (MIT License) \- Equivariant Message Passing \- URL: https://github.com/ACEsuit/mace

* * *

## ã‚³ãƒ¼ãƒ‰å†ç¾æ€§ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### ç’°å¢ƒæ§‹ç¯‰
    
    
    # MLPåŸºæœ¬ç’°å¢ƒ
    conda create -n mlp python=3.11
    conda activate mlp
    conda install pytorch torchvision -c pytorch
    conda install -c conda-forge ase gpaw
    
    # å€‹åˆ¥MLPãƒ„ãƒ¼ãƒ«
    pip install amp-atomistics  # AMP
    pip install chgnet  # CHGNet
    pip install m3gnet  # M3GNet
    pip install mace-torch  # MACE
    

### GPUè¦ä»¶ï¼ˆæ¨å¥¨ï¼‰

è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•° | GPU ãƒ¡ãƒ¢ãƒª | è¨“ç·´æ™‚é–“ | æ¨å¥¨GPU  
---|---|---|---  
100ã‚µãƒ³ãƒ—ãƒ« | ~2 GB | ~30åˆ† | GTX 1060  
1,000ã‚µãƒ³ãƒ—ãƒ« | ~8 GB | ~3æ™‚é–“ | RTX 3070  
10,000ã‚µãƒ³ãƒ—ãƒ« | ~16 GB | ~1æ—¥ | RTX 3090/A100  
  
### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

**å•é¡Œ** : CUDA out of memory **è§£æ±º** :
    
    
    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‰Šæ¸›
    model.train(batch_size=8)  # 32 â†’ 8
    

**å•é¡Œ** : è¨“ç·´ãŒåæŸã—ãªã„ **è§£æ±º** :
    
    
    # å­¦ç¿’ç‡ã‚’èª¿æ•´
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # 1e-3 â†’ 1e-4
    

* * *

## å®Ÿè·µçš„ãªè½ã¨ã—ç©´ã¨å¯¾ç­–

### 1\. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®åã‚Š
    
    
    # âŒ é–“é•ã„: å¹³è¡¡æ§‹é€ ã®ã¿
    train_data = [equilibrium_structures]  # ç¯„å›²ãŒç‹­ã™ã
    
    # âœ… æ­£è§£: å¤šæ§˜ãªé…ç½®ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    # - å¹³è¡¡æ§‹é€ 
    # - MDè»Œé“ï¼ˆé«˜æ¸©ï¼‰
    # - æ§‹é€ æœ€é©åŒ–é€”ä¸­
    # - é«˜ã‚¨ãƒãƒ«ã‚®ãƒ¼é…ç½®
    

### 2\. åŠ›ã®é‡ã¿ãŒä¸é©åˆ‡
    
    
    # âŒ ä¸å‡è¡¡: ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®ã¿é‡è¦–
    loss = energy_loss  # åŠ›ã‚’ç„¡è¦–
    
    # âœ… ãƒãƒ©ãƒ³ã‚¹: åŠ›ã‚‚é‡è¦–
    loss = energy_loss + 0.1 * force_loss  # åŠ›ã®é‡ã¿0.1
    

### 3\. å¤–æŒ¿é ˜åŸŸã§ã®ä½¿ç”¨
    
    
    # âŒ å±é™º: è¨“ç·´ç¯„å›²å¤–ã§äºˆæ¸¬
    # è¨“ç·´: 0-1000 K
    # ä½¿ç”¨: 2000 K â†’ ä¸æ­£ç¢º
    
    # âœ… å®‰å…¨: è¨“ç·´ç¯„å›²å†…ã§ä½¿ç”¨
    # ã¾ãŸã¯ä¸ç¢ºå®Ÿæ€§æ¨å®šã§è­¦å‘Š
    

### 4\. Active Learningã®é–¾å€¤è¨­å®š
    
    
    # âŒ é–¾å€¤ãŒé«˜ã™ã â†’ ãƒ‡ãƒ¼ã‚¿ä¸è¶³
    uncertainty_threshold = 10.0  # ç·©ã™ã
    
    # âœ… é©åˆ‡ãªé–¾å€¤
    uncertainty_threshold = 0.1  # ã‚¨ãƒãƒ«ã‚®ãƒ¼[eV/atom]
    

* * *

## å“è³ªä¿è¨¼ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### MLPè¨“ç·´ã®å¦¥å½“æ€§

  * [ ] è¨“ç·´èª¤å·®: Energy MAE < 10 meV/atom
  * [ ] è¨“ç·´èª¤å·®: Force MAE < 0.1 eV/Ã…
  * [ ] ãƒ†ã‚¹ãƒˆèª¤å·®ãŒè¨“ç·´èª¤å·®ã®2å€ä»¥å†…ï¼ˆéå­¦ç¿’ãªã—ï¼‰
  * [ ] æ¤œè¨¼ã‚»ãƒƒãƒˆã§ã®æ€§èƒ½ãŒå®‰å®š

### ç‰©ç†çš„å¦¥å½“æ€§

  * [ ] ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜ï¼ˆNVE MDã§æ¤œè¨¼ï¼‰
  * [ ] ä¸¦é€²ãƒ»å›è»¢ä¸å¤‰æ€§
  * [ ] å¯¾ç§°æ€§ã®ä¿å­˜
  * [ ] ç•°å¸¸ãªåŠ›ï¼ˆ> 10 eV/Ã…ï¼‰ãªã—

### Active Learningã®åŠ¹ç‡

  * [ ] è¨“ç·´ãƒ‡ãƒ¼ã‚¿å‰Šæ¸›ç‡ > 50%
  * [ ] åæŸã¾ã§ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ < 10å›
  * [ ] æœ€çµ‚ç²¾åº¦ãŒãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨åŒç­‰ä»¥ä¸Š

* * *

## å‚è€ƒæ–‡çŒ®

  1. Behler, J., & Parrinello, M. (2007). "Generalized Neural-Network Representation of High-Dimensional Potential-Energy Surfaces." _Physical Review Letters_ , 98, 146401. DOI: [10.1103/PhysRevLett.98.146401](<https://doi.org/10.1103/PhysRevLett.98.146401>)

  2. BartÃ³k, A. P., et al. (2010). "Gaussian Approximation Potentials: The Accuracy of Quantum Mechanics, without the Electrons." _Physical Review Letters_ , 104, 136403. DOI: [10.1103/PhysRevLett.104.136403](<https://doi.org/10.1103/PhysRevLett.104.136403>)

  3. SchÃ¼tt, K. T., et al. (2017). "SchNet: A continuous-filter convolutional neural network for modeling quantum interactions." _NeurIPS_.

  4. Chen, C., & Ong, S. P. (2022). "A universal graph deep learning interatomic potential for the periodic table." _Nature Computational Science_ , 2, 718-728. DOI: [10.1038/s43588-022-00349-3](<https://doi.org/10.1038/s43588-022-00349-3>)

  5. Batatia, I., et al. (2022). "MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields." _NeurIPS_.

  6. CHGNet: https://github.com/CederGroupHub/chgnet

  7. M3GNet: https://github.com/materialsvirtuallab/m3gnet
  8. MACE: https://github.com/ACEsuit/mace

* * *

## è‘—è€…æƒ…å ±

**ä½œæˆè€…** : MI Knowledge Hub Content Team **ä½œæˆæ—¥** : 2025-10-17 **ãƒãƒ¼ã‚¸ãƒ§ãƒ³** : 1.0 **ã‚·ãƒªãƒ¼ã‚º** : è¨ˆç®—ææ–™ç§‘å­¦åŸºç¤å…¥é–€ v1.0

**ãƒ©ã‚¤ã‚»ãƒ³ã‚¹** : Creative Commons BY-NC-SA 4.0

* * *

**ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼è¨ˆç®—ææ–™ç§‘å­¦åŸºç¤å…¥é–€ã‚·ãƒªãƒ¼ã‚ºã‚’å®Œäº†ã—ã¾ã—ãŸï¼**

æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼š \- è‡ªåˆ†ã®ç ”ç©¶ãƒ†ãƒ¼ãƒã§å®Ÿéš›ã«è¨ˆç®—ã‚’å®Ÿè¡Œ \- ãƒã‚¤ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—å…¥é–€ã‚·ãƒªãƒ¼ã‚ºã¸é€²ã‚€ \- æœ€æ–°è«–æ–‡ã‚’èª­ã‚“ã§çŸ¥è­˜ã‚’æ·±ã‚ã‚‹ \- ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«å‚åŠ ï¼ˆGitHubã€å­¦ä¼šï¼‰

**ç¶™ç¶šçš„ãªå­¦ç¿’ãŒææ–™ç§‘å­¦ã®æœªæ¥ã‚’æ‹“ãã¾ã™ï¼**
