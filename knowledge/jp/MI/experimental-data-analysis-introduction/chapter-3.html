<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬3ç« ï¼šç”»åƒãƒ‡ãƒ¼ã‚¿è§£æ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/AI-Knowledge-Notes/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/AI-Knowledge-Notes/knowledge/jp/MI/index.html">ãƒãƒ†ãƒªã‚¢ãƒ«ã‚ºãƒ»ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹</a><span class="breadcrumb-separator">â€º</span><a href="/AI-Knowledge-Notes/knowledge/jp/MI/experimental-data-analysis-introduction/index.html">Experimental Data Analysis</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬3ç« ï¼šç”»åƒãƒ‡ãƒ¼ã‚¿è§£æ</h1>
            <p class="subtitle">SEMãƒ»TEMç”»åƒã®è‡ªå‹•è§£æ - ç²’å­æ¤œå‡ºã‹ã‚‰æ·±å±¤å­¦ç¿’ã¾ã§</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 30-35åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 13å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 3å•</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>ç¬¬3ç« ï¼šç”»åƒãƒ‡ãƒ¼ã‚¿è§£æ</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">SEM/TEMç”»åƒã‹ã‚‰ç²’å­æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹åŸºæœ¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã—ã¾ã™ã€‚ãƒã‚¤ã‚ºã‚„é‡ãªã‚Šã¸ã®å …ç‰¢åŒ–ã®ã‚³ãƒ„ã‚‚ç´¹ä»‹ã—ã¾ã™ã€‚</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>ğŸ’¡ è£œè¶³:</strong> å‰å‡¦ç†ï¼ˆå¹³æ»‘åŒ–ãƒ»äºŒå€¤åŒ–ï¼‰â†’ç‰¹å¾´æŠ½å‡ºâ†’å¾Œå‡¦ç†ã®é †ã§å›ºå®šåŒ–ã€‚ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®è³ªãŒæ€§èƒ½ã‚’å·¦å³ã—ã¾ã™ã€‚</p>





<p><strong>SEMãƒ»TEMç”»åƒã®è‡ªå‹•è§£æ - ç²’å­æ¤œå‡ºã‹ã‚‰æ·±å±¤å­¦ç¿’ã¾ã§</strong></p>
<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… SEMãƒ»TEMç”»åƒã®å‰å‡¦ç†ï¼ˆãƒã‚¤ã‚ºé™¤å»ã€ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆèª¿æ•´ï¼‰ã‚’å®Ÿè¡Œã§ãã‚‹</li>
<li>âœ… Watershedæ³•ã«ã‚ˆã‚‹ç²’å­æ¤œå‡ºã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… ç²’å¾„åˆ†å¸ƒã€å½¢çŠ¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå††å½¢åº¦ã€ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ï¼‰ã‚’å®šé‡ã§ãã‚‹</li>
<li>âœ… CNNã«ã‚ˆã‚‹ææ–™ç”»åƒåˆ†é¡ï¼ˆè»¢ç§»å­¦ç¿’ï¼‰ã‚’å®Ÿè¡Œã§ãã‚‹</li>
<li>âœ… OpenCVãƒ»scikit-imageã‚’ä½¿ã£ãŸç”»åƒè§£æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
</ul>
<p><strong>èª­äº†æ™‚é–“</strong>: 30-35åˆ†
<strong>ã‚³ãƒ¼ãƒ‰ä¾‹</strong>: 13å€‹
<strong>æ¼”ç¿’å•é¡Œ</strong>: 3å•</p>
<hr />
<h2>3.1 ç”»åƒãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´ã¨å‰å‡¦ç†æˆ¦ç•¥</h2>
<h3>SEMãƒ»TEMç”»åƒã®ç‰¹å¾´</h3>
<p>é›»å­é¡•å¾®é¡ç”»åƒã¯ææ–™ã®ãƒŠãƒã€œãƒã‚¤ã‚¯ãƒ­ã‚¹ã‚±ãƒ¼ãƒ«æ§‹é€ ã‚’å¯è¦–åŒ–ã™ã‚‹å¼·åŠ›ãªãƒ„ãƒ¼ãƒ«ã§ã™ã€‚</p>
<table>
<thead>
<tr>
<th>æ¸¬å®šæŠ€è¡“</th>
<th>ç©ºé–“åˆ†è§£èƒ½</th>
<th>å…¸å‹çš„ãªè¦–é‡</th>
<th>ä¸»ãªæƒ…å ±</th>
<th>ç”»åƒç‰¹æ€§</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SEM</strong></td>
<td>æ•°nmï½æ•°Î¼m</td>
<td>10Î¼mï½1mm</td>
<td>è¡¨é¢å½¢æ…‹ã€çµ„ç¹”</td>
<td>æ·±ã„è¢«å†™ç•Œæ·±åº¦ã€å½±åŠ¹æœ</td>
</tr>
<tr>
<td><strong>TEM</strong></td>
<td>åŸå­ãƒ¬ãƒ™ãƒ«</td>
<td>æ•°ånmï½æ•°Î¼m</td>
<td>å†…éƒ¨æ§‹é€ ã€çµæ™¶æ€§</td>
<td>é«˜ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆã€å›æŠ˜åƒ</td>
</tr>
<tr>
<td><strong>STEM</strong></td>
<td>ã‚µãƒ–nm</td>
<td>æ•°ånmï½æ•°ç™¾nm</td>
<td>åŸå­é…åˆ—ã€å…ƒç´ åˆ†å¸ƒ</td>
<td>åŸå­åˆ†è§£èƒ½</td>
</tr>
</tbody>
</table>
<h3>ç”»åƒè§£æã®å…¸å‹çš„ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</h3>
<div class="mermaid">
flowchart TD
    A[ç”»åƒå–å¾—] --> B[å‰å‡¦ç†]
    B --> C[ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³]
    C --> D[ç‰¹å¾´æŠ½å‡º]
    D --> E[å®šé‡è§£æ]
    E --> F[çµ±è¨ˆå‡¦ç†]
    F --> G[å¯è¦–åŒ–ãƒ»å ±å‘Š]

    B --> B1[ãƒã‚¤ã‚ºé™¤å»]
    B --> B2[ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆèª¿æ•´]
    B --> B3[äºŒå€¤åŒ–]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
    style G fill:#fff9c4
</div>

<hr />
<h2>3.2 ãƒ‡ãƒ¼ã‚¿ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¨å†ç¾æ€§</h2>
<h3>ç”»åƒãƒ‡ãƒ¼ã‚¿ãƒªãƒã‚¸ãƒˆãƒªã¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</h3>
<p>é¡•å¾®é¡ç”»åƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ´»ç”¨ã¯ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ é–‹ç™ºã¨æ¤œè¨¼ã«ä¸å¯æ¬ ã§ã™ã€‚</p>
<h4>ä¸»è¦ãªç”»åƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹</h4>
<table>
<thead>
<tr>
<th>ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹</th>
<th>å†…å®¹</th>
<th>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</th>
<th>ã‚¢ã‚¯ã‚»ã‚¹</th>
<th>å¼•ç”¨è¦ä»¶</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>EMPIAR (Electron Microscopy)</strong></td>
<td>TEM/cryo-EMç”»åƒ</td>
<td>CC BY 4.0</td>
<td>ç„¡æ–™</td>
<td>å¿…é ˆ</td>
</tr>
<tr>
<td><strong>NanoMine</strong></td>
<td>ãƒŠãƒã‚³ãƒ³ãƒã‚¸ãƒƒãƒˆSEM</td>
<td>CC BY 4.0</td>
<td>ç„¡æ–™</td>
<td>æ¨å¥¨</td>
</tr>
<tr>
<td><strong>Materials Data Facility</strong></td>
<td>ææ–™ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</td>
<td>Mixed</td>
<td>ç„¡æ–™</td>
<td>å¿…é ˆ</td>
</tr>
<tr>
<td><strong>Kaggle Datasets (Microscopy)</strong></td>
<td>å„ç¨®é¡•å¾®é¡ç”»åƒ</td>
<td>Mixed</td>
<td>ç„¡æ–™</td>
<td>ç¢ºèªå¿…è¦</td>
</tr>
<tr>
<td><strong>NIST SRD</strong></td>
<td>æ¨™æº–å‚ç…§ç”»åƒ</td>
<td>Public Domain</td>
<td>ç„¡æ–™</td>
<td>æ¨å¥¨</td>
</tr>
</tbody>
</table>
<h4>ãƒ‡ãƒ¼ã‚¿åˆ©ç”¨æ™‚ã®æ³¨æ„äº‹é …</h4>
<p><strong>å…¬é–‹ãƒ‡ãƒ¼ã‚¿åˆ©ç”¨ä¾‹</strong>:</p>
<pre><code class="language-python">&quot;&quot;&quot;
SEM image from NanoMine database.
Reference: NanoMine Dataset L123 - Polymer Nanocomposite
Citation: Zhao, H. et al. (2016) Computational Materials Science
License: CC BY 4.0
URL: https://materialsmine.org/nm/L123
Scale bar: 500 nm (calibration: 2.5 nm/pixel)
&quot;&quot;&quot;
</code></pre>
<p><strong>ç”»åƒãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¨˜éŒ²</strong>:</p>
<pre><code class="language-python">IMAGE_METADATA = {
    'instrument': 'FEI Quanta 200',
    'accelerating_voltage': '20 kV',
    'magnification': '10000x',
    'working_distance': '10 mm',
    'pixel_size': 2.5,  # nm/pixel
    'scale_bar': 500,   # nm
    'detector': 'SE detector',
    'acquisition_date': '2025-10-15'
}

# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’JSONä¿å­˜ï¼ˆå†ç¾æ€§æ‹…ä¿ï¼‰
import json
with open('image_metadata.json', 'w') as f:
    json.dump(IMAGE_METADATA, f, indent=2)
</code></pre>
<h3>ã‚³ãƒ¼ãƒ‰å†ç¾æ€§ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>
<h4>ç’°å¢ƒæƒ…å ±ã®è¨˜éŒ²</h4>
<pre><code class="language-python">import sys
import cv2
import numpy as np
from skimage import __version__ as skimage_version
import tensorflow as tf

print(&quot;=== ç”»åƒè§£æç’°å¢ƒ ===&quot;)
print(f&quot;Python: {sys.version}&quot;)
print(f&quot;OpenCV: {cv2.__version__}&quot;)
print(f&quot;NumPy: {np.__version__}&quot;)
print(f&quot;scikit-image: {skimage_version}&quot;)
print(f&quot;TensorFlow: {tf.__version__}&quot;)

# æ¨å¥¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆ2025å¹´10æœˆæ™‚ç‚¹ï¼‰:
# - Python: 3.10ä»¥ä¸Š
# - OpenCV: 4.8ä»¥ä¸Š
# - NumPy: 1.24ä»¥ä¸Š
# - scikit-image: 0.21ä»¥ä¸Š
# - TensorFlow: 2.13ä»¥ä¸Šï¼ˆGPUç‰ˆæ¨å¥¨ï¼‰
</code></pre>
<h4>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ–‡æ›¸åŒ–</h4>
<p><strong>æ‚ªã„ä¾‹</strong>ï¼ˆå†ç¾ä¸å¯èƒ½ï¼‰:</p>
<pre><code class="language-python">denoised = cv2.fastNlMeansDenoising(image, None, 10, 7, 21)  # ãªãœã“ã®å€¤?
</code></pre>
<p><strong>è‰¯ã„ä¾‹</strong>ï¼ˆå†ç¾å¯èƒ½ï¼‰:</p>
<pre><code class="language-python"># Non-Local Meansãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
NLM_H = 10  # ãƒ•ã‚£ãƒ«ã‚¿å¼·åº¦ï¼ˆãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã«å¯¾å¿œã€SEMå…¸å‹å€¤: 5-15ï¼‰
NLM_TEMPLATE_WINDOW = 7  # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºï¼ˆå¥‡æ•°ã€æ¨å¥¨: 7ï¼‰
NLM_SEARCH_WINDOW = 21   # æ¢ç´¢ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºï¼ˆå¥‡æ•°ã€æ¨å¥¨: 21ï¼‰
NLM_DESCRIPTION = &quot;&quot;&quot;
h: ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã«å¿œã˜ã¦èª¿æ•´ã€‚ä½ãƒã‚¤ã‚º: 5-7, é«˜ãƒã‚¤ã‚º: 10-15
templateWindowSize: 7ãŒæ¨™æº–ï¼ˆå°ã•ã„ã¨é€Ÿã„ãŒãƒã‚¤ã‚ºæ®‹å­˜ï¼‰
searchWindowSize: 21ãŒæ¨™æº–ï¼ˆå¤§ãã„ã»ã©é«˜å“è³ªã ãŒé…ã„ï¼‰
&quot;&quot;&quot;
denoised = cv2.fastNlMeansDenoising(
    image, None, NLM_H, NLM_TEMPLATE_WINDOW, NLM_SEARCH_WINDOW
)
</code></pre>
<h4>ç”»åƒã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®è¨˜éŒ²</h4>
<pre><code class="language-python"># ãƒ”ã‚¯ã‚»ãƒ«-ç‰©ç†ã‚µã‚¤ã‚ºå¤‰æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
CALIBRATION_PARAMS = {
    'pixel_size_nm': 2.5,  # nm/pixelï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒ¼ã‹ã‚‰æ ¡æ­£ï¼‰
    'scale_bar_length_nm': 500,  # ã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒ¼ã®ç‰©ç†ã‚µã‚¤ã‚º
    'scale_bar_pixels': 200,  # ã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒ¼ã®ãƒ”ã‚¯ã‚»ãƒ«æ•°
    'calibration_date': '2025-10-15',
    'calibration_method': 'Scale bar measurement',
    'uncertainty': 0.1  # nm/pixelï¼ˆæ ¡æ­£èª¤å·®ï¼‰
}

# ãƒ”ã‚¯ã‚»ãƒ«â†’ç‰©ç†ã‚µã‚¤ã‚ºå¤‰æ›
def pixels_to_nm(pixels, calib_params=CALIBRATION_PARAMS):
    &quot;&quot;&quot;ãƒ”ã‚¯ã‚»ãƒ«æ•°ã‚’ç‰©ç†ã‚µã‚¤ã‚ºï¼ˆnmï¼‰ã«å¤‰æ›&quot;&quot;&quot;
    return pixels * calib_params['pixel_size_nm']

# ä½¿ç”¨ä¾‹
diameter_pixels = 50
diameter_nm = pixels_to_nm(diameter_pixels)
print(f&quot;ç²’å¾„: {diameter_nm:.1f} Â± {CALIBRATION_PARAMS['uncertainty']*diameter_pixels:.1f} nm&quot;)
</code></pre>
<hr />
<h2>3.2 ç”»åƒå‰å‡¦ç†</h2>
<h3>ãƒã‚¤ã‚ºé™¤å»</h3>
<p><strong>ã‚³ãƒ¼ãƒ‰ä¾‹1: å„ç¨®ãƒã‚¤ã‚ºé™¤å»ãƒ•ã‚£ãƒ«ã‚¿ã®æ¯”è¼ƒ</strong></p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
import cv2
from skimage import filters, io
from scipy import ndimage

# ä¹±æ•°ã‚·ãƒ¼ãƒ‰å›ºå®šï¼ˆå†ç¾æ€§æ‹…ä¿ï¼‰
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

# ã‚µãƒ³ãƒ—ãƒ«SEMç”»åƒç”Ÿæˆï¼ˆç²’å­ã‚’æ¨¡æ“¬ï¼‰
def generate_synthetic_sem(size=512, num_particles=30):
    &quot;&quot;&quot;åˆæˆSEMç”»åƒã®ç”Ÿæˆ&quot;&quot;&quot;
    image = np.zeros((size, size), dtype=np.float32)

    # ãƒ©ãƒ³ãƒ€ãƒ ãªç²’å­é…ç½®
    for _ in range(num_particles):
        x = np.random.randint(50, size - 50)
        y = np.random.randint(50, size - 50)
        radius = np.random.randint(15, 35)

        # å††å½¢ç²’å­
        Y, X = np.ogrid[:size, :size]
        mask = (X - x)**2 + (Y - y)**2 &lt;= radius**2
        image[mask] = 200

    # ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºè¿½åŠ 
    noise = np.random.normal(0, 25, image.shape)
    noisy_image = np.clip(image + noise, 0, 255).astype(np.uint8)

    return noisy_image

# ç”»åƒç”Ÿæˆ
noisy_image = generate_synthetic_sem()

# å„ç¨®ãƒã‚¤ã‚ºé™¤å»ãƒ•ã‚£ãƒ«ã‚¿
gaussian_blur = cv2.GaussianBlur(noisy_image, (5, 5), 1.0)
median_filter = cv2.medianBlur(noisy_image, 5)
bilateral_filter = cv2.bilateralFilter(noisy_image, 9, 75, 75)
nlm_filter = cv2.fastNlMeansDenoising(noisy_image, None, 10, 7, 21)

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

axes[0, 0].imshow(noisy_image, cmap='gray')
axes[0, 0].set_title('Noisy SEM Image')
axes[0, 0].axis('off')

axes[0, 1].imshow(gaussian_blur, cmap='gray')
axes[0, 1].set_title('Gaussian Blur')
axes[0, 1].axis('off')

axes[0, 2].imshow(median_filter, cmap='gray')
axes[0, 2].set_title('Median Filter')
axes[0, 2].axis('off')

axes[1, 0].imshow(bilateral_filter, cmap='gray')
axes[1, 0].set_title('Bilateral Filter')
axes[1, 0].axis('off')

axes[1, 1].imshow(nlm_filter, cmap='gray')
axes[1, 1].set_title('Non-Local Means')
axes[1, 1].axis('off')

# ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«æ¯”è¼ƒ
axes[1, 2].bar(['Original', 'Gaussian', 'Median', 'Bilateral', 'NLM'],
               [np.std(noisy_image),
                np.std(gaussian_blur),
                np.std(median_filter),
                np.std(bilateral_filter),
                np.std(nlm_filter)])
axes[1, 2].set_ylabel('Noise Level (std)')
axes[1, 2].set_title('Denoising Performance')
axes[1, 2].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print(&quot;=== ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ï¼ˆæ¨™æº–åå·®ï¼‰ ===&quot;)
print(f&quot;å…ƒç”»åƒ: {np.std(noisy_image):.2f}&quot;)
print(f&quot;Gaussian: {np.std(gaussian_blur):.2f}&quot;)
print(f&quot;Median: {np.std(median_filter):.2f}&quot;)
print(f&quot;Bilateral: {np.std(bilateral_filter):.2f}&quot;)
print(f&quot;NLM: {np.std(nlm_filter):.2f}&quot;)
</code></pre>
<p><strong>ãƒ•ã‚£ãƒ«ã‚¿ã®ä½¿ã„åˆ†ã‘</strong>:
- <strong>Gaussian</strong>: é«˜é€Ÿã€ã‚¨ãƒƒã‚¸ãŒæ»‘ã‚‰ã‹ â†’ ä¸€èˆ¬çš„ãªå‰å‡¦ç†
- <strong>Median</strong>: ã‚¨ãƒƒã‚¸ä¿æŒã€ã‚½ãƒ«ãƒˆï¼†ãƒšãƒƒãƒ‘ãƒ¼ãƒã‚¤ã‚ºã«å¼·ã„
- <strong>Bilateral</strong>: ã‚¨ãƒƒã‚¸ä¿æŒãŒå„ªç§€ã€è¨ˆç®—ã‚³ã‚¹ãƒˆä¸­
- <strong>Non-Local Means</strong>: æœ€é«˜å“è³ªã€è¨ˆç®—ã‚³ã‚¹ãƒˆå¤§</p>
<h3>ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆèª¿æ•´</h3>
<p><strong>ã‚³ãƒ¼ãƒ‰ä¾‹2: ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å‡ç­‰åŒ–ã¨CLAHE</strong></p>
<pre><code class="language-python">from skimage import exposure

# ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆèª¿æ•´
hist_eq = exposure.equalize_hist(noisy_image)
clahe = exposure.equalize_adapthist(noisy_image, clip_limit=0.03)

# ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ è¨ˆç®—
hist_original = np.histogram(noisy_image, bins=256, range=(0, 256))[0]
hist_eq_vals = np.histogram(
    (hist_eq * 255).astype(np.uint8), bins=256, range=(0, 256))[0]
hist_clahe_vals = np.histogram(
    (clahe * 255).astype(np.uint8), bins=256, range=(0, 256))[0]

# å¯è¦–åŒ–
fig = plt.figure(figsize=(16, 10))

# ç”»åƒ
ax1 = plt.subplot(2, 3, 1)
ax1.imshow(noisy_image, cmap='gray')
ax1.set_title('Original')
ax1.axis('off')

ax2 = plt.subplot(2, 3, 2)
ax2.imshow(hist_eq, cmap='gray')
ax2.set_title('Histogram Equalization')
ax2.axis('off')

ax3 = plt.subplot(2, 3, 3)
ax3.imshow(clahe, cmap='gray')
ax3.set_title('CLAHE (Adaptive)')
ax3.axis('off')

# ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
ax4 = plt.subplot(2, 3, 4)
ax4.hist(noisy_image.ravel(), bins=256, range=(0, 256), alpha=0.7)
ax4.set_xlabel('Pixel Value')
ax4.set_ylabel('Frequency')
ax4.set_title('Original Histogram')
ax4.grid(True, alpha=0.3)

ax5 = plt.subplot(2, 3, 5)
ax5.hist((hist_eq * 255).astype(np.uint8).ravel(),
         bins=256, range=(0, 256), alpha=0.7, color='orange')
ax5.set_xlabel('Pixel Value')
ax5.set_ylabel('Frequency')
ax5.set_title('Histogram Eq. Histogram')
ax5.grid(True, alpha=0.3)

ax6 = plt.subplot(2, 3, 6)
ax6.hist((clahe * 255).astype(np.uint8).ravel(),
         bins=256, range=(0, 256), alpha=0.7, color='green')
ax6.set_xlabel('Pixel Value')
ax6.set_ylabel('Frequency')
ax6.set_title('CLAHE Histogram')
ax6.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;=== ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆæŒ‡æ¨™ ===&quot;)
print(f&quot;å…ƒç”»åƒã®ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ: {noisy_image.max() - noisy_image.min()}&quot;)
print(f&quot;Histogram Eq.: {(hist_eq * 255).max() - (hist_eq * 255).min():.1f}&quot;)
print(f&quot;CLAHE: {(clahe * 255).max() - (clahe * 255).min():.1f}&quot;)
</code></pre>
<p><strong>CLAHEï¼ˆContrast Limited Adaptive Histogram Equalizationï¼‰ã®åˆ©ç‚¹</strong>:
- å±€æ‰€çš„ãªã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå‘ä¸Š
- éå‰°ãªå¼·èª¿ã‚’æŠ‘åˆ¶ï¼ˆclip_limitãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
- SEMç”»åƒã®æš—éƒ¨ãƒ»æ˜éƒ¨ä¸¡æ–¹ã§è©³ç´°ãŒè¦‹ãˆã‚‹</p>
<hr />
<h2>3.3 ç²’å­æ¤œå‡ºï¼ˆWatershedæ³•ï¼‰</h2>
<h3>äºŒå€¤åŒ–ã¨è·é›¢å¤‰æ›</h3>
<p><strong>ã‚³ãƒ¼ãƒ‰ä¾‹3: Otsuæ³•ã«ã‚ˆã‚‹è‡ªå‹•äºŒå€¤åŒ–</strong></p>
<pre><code class="language-python">from skimage import morphology, measure
from scipy.ndimage import distance_transform_edt

# ãƒã‚¤ã‚ºé™¤å»å¾Œã®ç”»åƒã‚’ä½¿ç”¨
denoised = cv2.fastNlMeansDenoising(noisy_image, None, 10, 7, 21)

# Otsuæ³•ã«ã‚ˆã‚‹äºŒå€¤åŒ–
threshold = filters.threshold_otsu(denoised)
binary = denoised &gt; threshold

# ãƒ¢ãƒ«ãƒ•ã‚©ãƒ­ã‚¸ãƒ¼æ¼”ç®—ï¼ˆå°ã•ãªãƒã‚¤ã‚ºé™¤å»ï¼‰
binary_cleaned = morphology.remove_small_objects(binary, min_size=50)
binary_cleaned = morphology.remove_small_holes(binary_cleaned, area_threshold=50)

# è·é›¢å¤‰æ›
distance = distance_transform_edt(binary_cleaned)

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(12, 12))

axes[0, 0].imshow(denoised, cmap='gray')
axes[0, 0].set_title('Denoised Image')
axes[0, 0].axis('off')

axes[0, 1].imshow(binary, cmap='gray')
axes[0, 1].set_title(f'Binary (Otsu threshold={threshold:.1f})')
axes[0, 1].axis('off')

axes[1, 0].imshow(binary_cleaned, cmap='gray')
axes[1, 0].set_title('After Morphology')
axes[1, 0].axis('off')

axes[1, 1].imshow(distance, cmap='jet')
axes[1, 1].set_title('Distance Transform')
axes[1, 1].axis('off')
axes[1, 1].colorbar = plt.colorbar(axes[1, 1].imshow(distance, cmap='jet'),
                                   ax=axes[1, 1])

plt.tight_layout()
plt.show()

print(f&quot;=== äºŒå€¤åŒ–çµæœ ===&quot;)
print(f&quot;Otsué–¾å€¤: {threshold:.1f}&quot;)
print(f&quot;ç™½ãƒ”ã‚¯ã‚»ãƒ«å‰²åˆ: {binary_cleaned.sum() / binary_cleaned.size * 100:.1f}%&quot;)
</code></pre>
<h3>Watershedæ³•ã«ã‚ˆã‚‹ç²’å­åˆ†é›¢</h3>
<p><strong>ã‚³ãƒ¼ãƒ‰ä¾‹4: Watershed ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³</strong></p>
<pre><code class="language-python">from skimage.feature import peak_local_max
from skimage.segmentation import watershed

# å±€æ‰€æœ€å¤§å€¤æ¤œå‡ºï¼ˆç²’å­ä¸­å¿ƒã®æ¨å®šï¼‰
local_max = peak_local_max(
    distance,
    min_distance=20,
    threshold_abs=5,
    labels=binary_cleaned
)

# ãƒãƒ¼ã‚«ãƒ¼ä½œæˆ
markers = np.zeros_like(distance, dtype=int)
markers[tuple(local_max.T)] = np.arange(1, len(local_max) + 1)

# Watershedå®Ÿè¡Œ
labels = watershed(-distance, markers, mask=binary_cleaned)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# ãƒãƒ¼ã‚«ãƒ¼è¡¨ç¤º
axes[0].imshow(denoised, cmap='gray')
axes[0].plot(local_max[:, 1], local_max[:, 0], 'r+',
             markersize=12, markeredgewidth=2)
axes[0].set_title(f'Detected Centers ({len(local_max)} particles)')
axes[0].axis('off')

# Watershedãƒ©ãƒ™ãƒ«
axes[1].imshow(labels, cmap='nipy_spectral')
axes[1].set_title('Watershed Segmentation')
axes[1].axis('off')

# è¼ªéƒ­é‡ã­åˆã‚ã›
overlay = denoised.copy()
overlay_rgb = cv2.cvtColor(overlay, cv2.COLOR_GRAY2RGB)
for region in measure.regionprops(labels):
    minr, minc, maxr, maxc = region.bbox
    cv2.rectangle(overlay_rgb, (minc, minr), (maxc, maxr),
                  (255, 0, 0), 2)

axes[2].imshow(overlay_rgb)
axes[2].set_title('Detected Particles')
axes[2].axis('off')

plt.tight_layout()
plt.show()

print(f&quot;=== Watershedçµæœ ===&quot;)
print(f&quot;æ¤œå‡ºã•ã‚ŒãŸç²’å­æ•°: {len(local_max)}&quot;)
print(f&quot;ãƒ©ãƒ™ãƒ«æ•°: {labels.max()}&quot;)
</code></pre>
<hr />
<h2>3.4 ç²’å¾„åˆ†å¸ƒè§£æ</h2>
<h3>ç²’å­ç‰¹å¾´é‡ã®æŠ½å‡º</h3>
<p><strong>ã‚³ãƒ¼ãƒ‰ä¾‹5: ç²’å¾„ãƒ»å½¢çŠ¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨ˆç®—</strong></p>
<pre><code class="language-python"># å„ç²’å­ã®ç‰¹å¾´é‡æŠ½å‡º
particle_data = []

for region in measure.regionprops(labels):
    # é¢ç©ã‹ã‚‰å††ç›¸å½“ç›´å¾„ã‚’è¨ˆç®—
    area = region.area
    equivalent_diameter = np.sqrt(4 * area / np.pi)

    # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”
    major_axis = region.major_axis_length
    minor_axis = region.minor_axis_length
    aspect_ratio = major_axis / (minor_axis + 1e-10)

    # å††å½¢åº¦ï¼ˆ4Ï€Ã—é¢ç©/å‘¨å›²é•·^2ï¼‰
    perimeter = region.perimeter
    circularity = 4 * np.pi * area / (perimeter ** 2 + 1e-10)

    particle_data.append({
        'label': region.label,
        'area': area,
        'diameter': equivalent_diameter,
        'aspect_ratio': aspect_ratio,
        'circularity': circularity,
        'centroid': region.centroid
    })

# DataFrameã«å¤‰æ›
import pandas as pd
df_particles = pd.DataFrame(particle_data)

print(&quot;=== ç²’å­ç‰¹å¾´é‡çµ±è¨ˆ ===&quot;)
print(df_particles[['diameter', 'aspect_ratio', 'circularity']].describe())

# ç²’å¾„åˆ†å¸ƒãƒ—ãƒ­ãƒƒãƒˆ
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
axes[0, 0].hist(df_particles['diameter'], bins=20, alpha=0.7,
                edgecolor='black')
axes[0, 0].set_xlabel('Diameter (pixels)')
axes[0, 0].set_ylabel('Frequency')
axes[0, 0].set_title('Particle Size Distribution')
axes[0, 0].axvline(df_particles['diameter'].mean(), color='red',
                   linestyle='--', label=f'Mean: {df_particles[&quot;diameter&quot;].mean():.1f}')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3, axis='y')

# ç´¯ç©åˆ†å¸ƒ
sorted_diameters = np.sort(df_particles['diameter'])
cumulative = np.arange(1, len(sorted_diameters) + 1) / len(sorted_diameters) * 100
axes[0, 1].plot(sorted_diameters, cumulative, linewidth=2)
axes[0, 1].set_xlabel('Diameter (pixels)')
axes[0, 1].set_ylabel('Cumulative Percentage (%)')
axes[0, 1].set_title('Cumulative Size Distribution')
axes[0, 1].axhline(50, color='red', linestyle='--',
                   label=f'D50: {np.median(df_particles[&quot;diameter&quot;]):.1f}')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# æ•£å¸ƒå›³ï¼ˆç›´å¾„ vs ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ï¼‰
axes[1, 0].scatter(df_particles['diameter'],
                   df_particles['aspect_ratio'],
                   alpha=0.6, s=50)
axes[1, 0].set_xlabel('Diameter (pixels)')
axes[1, 0].set_ylabel('Aspect Ratio')
axes[1, 0].set_title('Diameter vs Aspect Ratio')
axes[1, 0].grid(True, alpha=0.3)

# æ•£å¸ƒå›³ï¼ˆç›´å¾„ vs å††å½¢åº¦ï¼‰
axes[1, 1].scatter(df_particles['diameter'],
                   df_particles['circularity'],
                   alpha=0.6, s=50, color='green')
axes[1, 1].set_xlabel('Diameter (pixels)')
axes[1, 1].set_ylabel('Circularity')
axes[1, 1].set_title('Diameter vs Circularity')
axes[1, 1].axhline(0.8, color='red', linestyle='--',
                   label='Spherical threshold')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ç²’å¾„çµ±è¨ˆ
print(&quot;\n=== ç²’å¾„çµ±è¨ˆ ===&quot;)
print(f&quot;å¹³å‡ç›´å¾„: {df_particles['diameter'].mean():.2f} pixels&quot;)
print(f&quot;ä¸­å¤®å€¤(D50): {df_particles['diameter'].median():.2f} pixels&quot;)
print(f&quot;æ¨™æº–åå·®: {df_particles['diameter'].std():.2f} pixels&quot;)
print(f&quot;æœ€å°ç›´å¾„: {df_particles['diameter'].min():.2f} pixels&quot;)
print(f&quot;æœ€å¤§ç›´å¾„: {df_particles['diameter'].max():.2f} pixels&quot;)
</code></pre>
<h3>ç²’å¾„åˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ï¼ˆå¯¾æ•°æ­£è¦åˆ†å¸ƒï¼‰</h3>
<p><strong>ã‚³ãƒ¼ãƒ‰ä¾‹6: å¯¾æ•°æ­£è¦åˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°</strong></p>
<pre><code class="language-python">from scipy.stats import lognorm

# å¯¾æ•°æ­£è¦åˆ†å¸ƒã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
diameters = df_particles['diameter'].values
shape, loc, scale = lognorm.fit(diameters, floc=0)

# ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœ
x = np.linspace(diameters.min(), diameters.max(), 200)
pdf_fitted = lognorm.pdf(x, shape, loc, scale)

# å¯è¦–åŒ–
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.hist(diameters, bins=20, density=True, alpha=0.6,
         label='Observed', edgecolor='black')
plt.plot(x, pdf_fitted, 'r-', linewidth=2,
         label=f'Log-normal fit (Ïƒ={shape:.2f})')
plt.xlabel('Diameter (pixels)')
plt.ylabel('Probability Density')
plt.title('Particle Size Distribution Fitting')
plt.legend()
plt.grid(True, alpha=0.3)

# Q-Qãƒ—ãƒ­ãƒƒãƒˆï¼ˆé©åˆåº¦ç¢ºèªï¼‰
plt.subplot(1, 2, 2)
theoretical_quantiles = lognorm.ppf(np.linspace(0.01, 0.99, 100),
                                    shape, loc, scale)
observed_quantiles = np.percentile(diameters, np.linspace(1, 99, 100))
plt.scatter(theoretical_quantiles, observed_quantiles, alpha=0.6)
plt.plot([diameters.min(), diameters.max()],
         [diameters.min(), diameters.max()],
         'r--', linewidth=2, label='Perfect fit')
plt.xlabel('Theoretical Quantiles')
plt.ylabel('Observed Quantiles')
plt.title('Q-Q Plot (Log-normal)')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;=== å¯¾æ•°æ­£è¦åˆ†å¸ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===&quot;)
print(f&quot;Shape (Ïƒ): {shape:.3f}&quot;)
print(f&quot;Scale (median): {scale:.2f} pixels&quot;)
</code></pre>
<hr />
<h2>3.5 æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹ç”»åƒåˆ†é¡</h2>
<h3>è»¢ç§»å­¦ç¿’ï¼ˆVGG16ï¼‰ã«ã‚ˆã‚‹ææ–™ç”»åƒåˆ†é¡</h3>
<p><strong>ã‚³ãƒ¼ãƒ‰ä¾‹7: CNNã«ã‚ˆã‚‹ææ–™ç›¸åˆ†é¡</strong></p>
<pre><code class="language-python"># TensorFlow/Kerasã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras.applications import VGG16
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    TENSORFLOW_AVAILABLE = True

    # TensorFlowãƒãƒ¼ã‚¸ãƒ§ãƒ³è¨˜éŒ²ï¼ˆå†ç¾æ€§ï¼‰
    print(f&quot;TensorFlow version: {tf.__version__}&quot;)
    print(f&quot;Keras version: {keras.__version__}&quot;)

    # ä¹±æ•°ã‚·ãƒ¼ãƒ‰å›ºå®š
    RANDOM_SEED = 42
    tf.random.set_seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print(&quot;TensorFlow not available. Skipping this example.&quot;)

if TENSORFLOW_AVAILABLE:
    # ã‚µãƒ³ãƒ—ãƒ«ç”»åƒãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆ3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼‰
    def generate_material_images(num_samples=100, img_size=128):
        &quot;&quot;&quot;
        ææ–™ç”»åƒã®ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ
        ã‚¯ãƒ©ã‚¹0: çƒå½¢ç²’å­
        ã‚¯ãƒ©ã‚¹1: æ£’çŠ¶ç²’å­
        ã‚¯ãƒ©ã‚¹2: ä¸å®šå½¢ç²’å­
        &quot;&quot;&quot;
        images = []
        labels = []

        for class_id in range(3):
            for _ in range(num_samples):
                img = np.zeros((img_size, img_size), dtype=np.uint8)

                if class_id == 0:  # çƒå½¢
                    num_particles = np.random.randint(5, 15)
                    for _ in range(num_particles):
                        x = np.random.randint(20, img_size - 20)
                        y = np.random.randint(20, img_size - 20)
                        r = np.random.randint(8, 15)
                        cv2.circle(img, (x, y), r, 200, -1)

                elif class_id == 1:  # æ£’çŠ¶
                    num_rods = np.random.randint(3, 8)
                    for _ in range(num_rods):
                        x1 = np.random.randint(10, img_size - 10)
                        y1 = np.random.randint(10, img_size - 10)
                        length = np.random.randint(30, 60)
                        angle = np.random.rand() * 2 * np.pi
                        x2 = int(x1 + length * np.cos(angle))
                        y2 = int(y1 + length * np.sin(angle))
                        cv2.line(img, (x1, y1), (x2, y2), 200, 3)

                else:  # ä¸å®šå½¢
                    num_shapes = np.random.randint(5, 12)
                    for _ in range(num_shapes):
                        pts = np.random.randint(10, img_size - 10,
                                                size=(6, 2))
                        cv2.fillPoly(img, [pts], 200)

                # ãƒã‚¤ã‚ºè¿½åŠ 
                noise = np.random.normal(0, 20, img.shape)
                img = np.clip(img + noise, 0, 255).astype(np.uint8)

                # RGBå¤‰æ›
                img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
                images.append(img_rgb)
                labels.append(class_id)

        return np.array(images), np.array(labels)

    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    X_data, y_data = generate_material_images(num_samples=150)

    # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X_data, y_data, test_size=0.2, random_state=RANDOM_SEED
    )

    # ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–
    X_train = X_train.astype('float32') / 255.0
    X_test = X_test.astype('float32') / 255.0

    # VGG16ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆImageNeté‡ã¿ï¼‰
    base_model = VGG16(
        weights='imagenet',
        include_top=False,
        input_shape=(128, 128, 3)
    )

    # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’å›ºå®š
    base_model.trainable = False

    # æ–°ã—ã„åˆ†é¡å±¤ã‚’è¿½åŠ 
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(128, activation='relu')(x)
    predictions = Dense(3, activation='softmax')(x)

    model = Model(inputs=base_model.input, outputs=predictions)

    # ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    history = model.fit(
        X_train, y_train,
        validation_split=0.2,
        epochs=10,
        batch_size=16,
        verbose=0
    )

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)

    print(&quot;=== CNNåˆ†é¡çµæœ ===&quot;)
    print(f&quot;ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_acc * 100:.2f}%&quot;)

    # å­¦ç¿’æ›²ç·šãƒ—ãƒ­ãƒƒãƒˆ
    plt.figure(figsize=(14, 5))

    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # æ··åŒè¡Œåˆ—
    from sklearn.metrics import confusion_matrix, classification_report
    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)

    cm = confusion_matrix(y_test, y_pred_classes)

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.colorbar()
    tick_marks = np.arange(3)
    plt.xticks(tick_marks, ['Spherical', 'Rod', 'Irregular'])
    plt.yticks(tick_marks, ['Spherical', 'Rod', 'Irregular'])

    # æ•°å€¤è¡¨ç¤º
    for i in range(3):
        for j in range(3):
            plt.text(j, i, cm[i, j], ha='center', va='center',
                    color='white' if cm[i, j] &gt; cm.max() / 2 else 'black')

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    plt.show()

    print(&quot;\n=== åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ ===&quot;)
    print(classification_report(
        y_test, y_pred_classes,
        target_names=['Spherical', 'Rod', 'Irregular']
    ))
</code></pre>
<hr />
<h2>3.6 çµ±åˆç”»åƒè§£æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h2>
<h3>è‡ªå‹•è§£æã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰</h3>
<p><strong>ã‚³ãƒ¼ãƒ‰ä¾‹8: ç”»åƒè§£æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹</strong></p>
<pre><code class="language-python">from dataclasses import dataclass
from typing import List, Dict
import json

@dataclass
class ParticleAnalysisResult:
    &quot;&quot;&quot;ç²’å­è§£æçµæœ&quot;&quot;&quot;
    num_particles: int
    mean_diameter: float
    std_diameter: float
    mean_circularity: float
    particle_data: List[Dict]

class SEMImageAnalyzer:
    &quot;&quot;&quot;SEMç”»åƒè‡ªå‹•è§£æã‚·ã‚¹ãƒ†ãƒ &quot;&quot;&quot;

    def __init__(self, img_size=(512, 512)):
        self.img_size = img_size
        self.image = None
        self.binary = None
        self.labels = None
        self.particles = []

    def load_image(self, image: np.ndarray):
        &quot;&quot;&quot;ç”»åƒèª­ã¿è¾¼ã¿&quot;&quot;&quot;
        if len(image.shape) == 3:
            self.image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            self.image = image

        # ãƒªã‚µã‚¤ã‚º
        self.image = cv2.resize(self.image, self.img_size)

    def preprocess(self, denoise_strength=10):
        &quot;&quot;&quot;å‰å‡¦ç†ï¼ˆãƒã‚¤ã‚ºé™¤å»ãƒ»ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆèª¿æ•´ï¼‰&quot;&quot;&quot;
        # ãƒã‚¤ã‚ºé™¤å»
        denoised = cv2.fastNlMeansDenoising(
            self.image, None, denoise_strength, 7, 21
        )

        # CLAHE
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        enhanced = clahe.apply(denoised)

        self.image = enhanced

    def segment_particles(self, min_size=50):
        &quot;&quot;&quot;ç²’å­ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³&quot;&quot;&quot;
        # OtsuäºŒå€¤åŒ–
        threshold = filters.threshold_otsu(self.image)
        binary = self.image &gt; threshold

        # ãƒ¢ãƒ«ãƒ•ã‚©ãƒ­ã‚¸ãƒ¼
        binary_cleaned = morphology.remove_small_objects(
            binary, min_size=min_size
        )
        binary_cleaned = morphology.remove_small_holes(
            binary_cleaned, area_threshold=min_size
        )

        # è·é›¢å¤‰æ›
        distance = distance_transform_edt(binary_cleaned)

        # Watershed
        local_max = peak_local_max(
            distance,
            min_distance=20,
            threshold_abs=5,
            labels=binary_cleaned
        )

        markers = np.zeros_like(distance, dtype=int)
        markers[tuple(local_max.T)] = np.arange(1, len(local_max) + 1)

        self.labels = watershed(-distance, markers, mask=binary_cleaned)
        self.binary = binary_cleaned

    def extract_features(self):
        &quot;&quot;&quot;ç‰¹å¾´é‡æŠ½å‡º&quot;&quot;&quot;
        self.particles = []

        for region in measure.regionprops(self.labels):
            area = region.area
            diameter = np.sqrt(4 * area / np.pi)
            aspect_ratio = region.major_axis_length / \
                          (region.minor_axis_length + 1e-10)
            circularity = 4 * np.pi * area / \
                         (region.perimeter ** 2 + 1e-10)

            self.particles.append({
                'label': region.label,
                'area': area,
                'diameter': diameter,
                'aspect_ratio': aspect_ratio,
                'circularity': circularity,
                'centroid': region.centroid
            })

    def get_results(self) -&gt; ParticleAnalysisResult:
        &quot;&quot;&quot;çµæœå–å¾—&quot;&quot;&quot;
        df = pd.DataFrame(self.particles)

        return ParticleAnalysisResult(
            num_particles=len(self.particles),
            mean_diameter=df['diameter'].mean(),
            std_diameter=df['diameter'].std(),
            mean_circularity=df['circularity'].mean(),
            particle_data=self.particles
        )

    def visualize(self):
        &quot;&quot;&quot;çµæœå¯è¦–åŒ–&quot;&quot;&quot;
        fig, axes = plt.subplots(2, 2, figsize=(14, 14))

        # å…ƒç”»åƒ
        axes[0, 0].imshow(self.image, cmap='gray')
        axes[0, 0].set_title('Preprocessed Image')
        axes[0, 0].axis('off')

        # äºŒå€¤åŒ–
        axes[0, 1].imshow(self.binary, cmap='gray')
        axes[0, 1].set_title('Binary Segmentation')
        axes[0, 1].axis('off')

        # Watershedãƒ©ãƒ™ãƒ«
        axes[1, 0].imshow(self.labels, cmap='nipy_spectral')
        axes[1, 0].set_title(f'Particles ({len(self.particles)})')
        axes[1, 0].axis('off')

        # ç²’å¾„åˆ†å¸ƒ
        df = pd.DataFrame(self.particles)
        axes[1, 1].hist(df['diameter'], bins=20, alpha=0.7,
                       edgecolor='black')
        axes[1, 1].set_xlabel('Diameter (pixels)')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title('Particle Size Distribution')
        axes[1, 1].grid(True, alpha=0.3, axis='y')

        plt.tight_layout()
        plt.show()

    def save_results(self, filename='analysis_results.json'):
        &quot;&quot;&quot;çµæœã‚’JSONä¿å­˜&quot;&quot;&quot;
        results = self.get_results()
        output = {
            'num_particles': results.num_particles,
            'mean_diameter': results.mean_diameter,
            'std_diameter': results.std_diameter,
            'mean_circularity': results.mean_circularity,
            'particles': results.particle_data
        }

        with open(filename, 'w') as f:
            json.dump(output, f, indent=2)

        print(f&quot;Results saved to {filename}&quot;)

# ä½¿ç”¨ä¾‹
analyzer = SEMImageAnalyzer()
analyzer.load_image(noisy_image)
analyzer.preprocess(denoise_strength=10)
analyzer.segment_particles(min_size=50)
analyzer.extract_features()

results = analyzer.get_results()
print(&quot;=== è§£æçµæœ ===&quot;)
print(f&quot;æ¤œå‡ºç²’å­æ•°: {results.num_particles}&quot;)
print(f&quot;å¹³å‡ç›´å¾„: {results.mean_diameter:.2f} Â± {results.std_diameter:.2f} pixels&quot;)
print(f&quot;å¹³å‡å††å½¢åº¦: {results.mean_circularity:.3f}&quot;)

analyzer.visualize()
analyzer.save_results('sem_analysis.json')
</code></pre>
<hr />
<h2>3.7 å®Ÿè·µçš„ãªè½ã¨ã—ç©´ã¨å¯¾ç­–</h2>
<h3>ã‚ˆãã‚ã‚‹å¤±æ•—ä¾‹ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>
<h4>å¤±æ•—1: éåº¦ãªã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆOver-segmentationï¼‰</h4>
<p><strong>ç—‡çŠ¶</strong>: 1ã¤ã®ç²’å­ãŒè¤‡æ•°ã«åˆ†å‰²ã•ã‚Œã‚‹</p>
<p><strong>åŸå› </strong>: Watershedã®min_distanceãŒå°ã•ã™ãã‚‹ã€ã¾ãŸã¯ãƒã‚¤ã‚ºãŒæ®‹å­˜</p>
<p><strong>å¯¾ç­–</strong>:</p>
<pre><code class="language-python"># âŒ æ‚ªã„ä¾‹ï¼šéåº¦ãªã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
local_max = peak_local_max(distance, min_distance=5)  # å°ã•ã™ãã‚‹
# çµæœ: ãƒã‚¤ã‚ºã‚„ç²’å­å†…ã®å¾®å°ãªå‡¹å‡¸ã‚’åˆ¥ç²’å­ã¨èªè­˜

# âœ… è‰¯ã„ä¾‹ï¼šé©åˆ‡ãªmin_distanceè¨­å®š
# ç²’å­ã®å…¸å‹çš„ãªç›´å¾„ã‹ã‚‰è¨­å®š
expected_diameter_pixels = 30
min_distance = int(expected_diameter_pixels * 0.6)  # ç›´å¾„ã®60%ç¨‹åº¦

local_max = peak_local_max(
    distance,
    min_distance=min_distance,
    threshold_abs=5,  # è·é›¢å¤‰æ›å€¤ã®é–¾å€¤
    labels=binary_cleaned
)
print(f&quot;min_distance: {min_distance} pixels&quot;)
print(f&quot;æ¤œå‡ºã•ã‚ŒãŸç²’å­æ•°: {len(local_max)}&quot;)
</code></pre>
<h4>å¤±æ•—2: é–¾å€¤é¸æŠã®èª¤ã‚Š</h4>
<p><strong>ç—‡çŠ¶</strong>: ç²’å­ã®ä¸€éƒ¨ãŒæ¬ æã€ã¾ãŸã¯èƒŒæ™¯ãŒç²’å­ã¨ã—ã¦æ¤œå‡º</p>
<p><strong>åŸå› </strong>: Otsuæ³•ãŒä¸é©åˆ‡ï¼ˆãƒã‚¤ãƒ¢ãƒ¼ãƒ€ãƒ«ã§ãªã„ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰</p>
<p><strong>å¯¾ç­–</strong>:</p>
<pre><code class="language-python"># âŒ æ‚ªã„ä¾‹ï¼šå…¨ç”»åƒã«ç„¡æ¡ä»¶ã«Otsué©ç”¨
threshold = filters.threshold_otsu(image)
binary = image &gt; threshold

# âœ… è‰¯ã„ä¾‹ï¼šãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ æ¤œè¨¼ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
threshold_otsu = filters.threshold_otsu(image)

# ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®å½¢çŠ¶ç¢ºèª
hist, bin_edges = np.histogram(image, bins=256, range=(0, 256))

# ãƒã‚¤ãƒ¢ãƒ¼ãƒ€ãƒ«æ€§ã®ç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼ˆ2ã¤ã®ãƒ”ãƒ¼ã‚¯ï¼‰
from scipy.signal import find_peaks
peaks, _ = find_peaks(hist, prominence=100)

if len(peaks) &gt;= 2:
    # ãƒã‚¤ãƒ¢ãƒ¼ãƒ€ãƒ« â†’ Otsué©ç”¨å¯èƒ½
    threshold = threshold_otsu
    print(f&quot;Otsu threshold: {threshold:.1f} (bimodal histogram)&quot;)
else:
    # å˜å³°æ€§ â†’ ä»£æ›¿æ‰‹æ³•ï¼ˆå¹³å‡+æ¨™æº–åå·®ãªã©ï¼‰
    threshold = image.mean() + image.std()
    print(f&quot;Mean+Std threshold: {threshold:.1f} (unimodal histogram)&quot;)
    print(&quot;è­¦å‘Š: ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ãŒå˜å³°æ€§ã§ã™ã€‚Otsuæ³•ã¯ä¸é©åˆ‡ãªå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™&quot;)

binary = image &gt; threshold
</code></pre>
<h4>å¤±æ•—3: ãƒ”ã‚¯ã‚»ãƒ«æ ¡æ­£ã®æ¬ å¦‚</h4>
<p><strong>ç—‡çŠ¶</strong>: ç‰©ç†ã‚µã‚¤ã‚ºï¼ˆnmã€Î¼mï¼‰ãŒä¸æ˜ã€ç•°ãªã‚‹å€ç‡ã®ãƒ‡ãƒ¼ã‚¿ãŒæ¯”è¼ƒä¸å¯èƒ½</p>
<p><strong>åŸå› </strong>: ã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒ¼æƒ…å ±ã‚’åˆ©ç”¨ã—ã¦ã„ãªã„</p>
<p><strong>å¯¾ç­–</strong>:</p>
<pre><code class="language-python"># âŒ æ‚ªã„ä¾‹ï¼šãƒ”ã‚¯ã‚»ãƒ«å˜ä½ã®ã¾ã¾å ±å‘Š
print(f&quot;å¹³å‡ç²’å¾„: {mean_diameter:.1f} pixels&quot;)  # ç‰©ç†ã‚µã‚¤ã‚ºä¸æ˜

# âœ… è‰¯ã„ä¾‹ï¼šã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒ¼æ ¡æ­£
# ã‚¹ãƒ†ãƒƒãƒ—1: ã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒ¼ã®é•·ã•ã‚’æ¸¬å®šï¼ˆæ‰‹å‹•ã¾ãŸã¯OCRï¼‰
SCALE_BAR_NM = 500  # ã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒ¼ã®ç‰©ç†ã‚µã‚¤ã‚ºï¼ˆnmï¼‰
SCALE_BAR_PIXELS = 200  # ã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒ¼ã®ãƒ”ã‚¯ã‚»ãƒ«æ•°

# ã‚¹ãƒ†ãƒƒãƒ—2: æ ¡æ­£ä¿‚æ•°ã®è¨ˆç®—
nm_per_pixel = SCALE_BAR_NM / SCALE_BAR_PIXELS
print(f&quot;æ ¡æ­£: {nm_per_pixel:.2f} nm/pixel&quot;)

# ã‚¹ãƒ†ãƒƒãƒ—3: ç‰©ç†ã‚µã‚¤ã‚ºã«å¤‰æ›
mean_diameter_nm = mean_diameter * nm_per_pixel
std_diameter_nm = std_diameter * nm_per_pixel

print(f&quot;å¹³å‡ç²’å¾„: {mean_diameter_nm:.1f} Â± {std_diameter_nm:.1f} nm&quot;)

# ã‚¹ãƒ†ãƒƒãƒ—4: æ ¡æ­£æƒ…å ±ã‚’çµæœã«ä¿å­˜
calibration_info = {
    'scale_bar_nm': SCALE_BAR_NM,
    'scale_bar_pixels': SCALE_BAR_PIXELS,
    'nm_per_pixel': nm_per_pixel,
    'calibration_date': '2025-10-19'
}
</code></pre>
<h4>å¤±æ•—4: CNNã®éå­¦ç¿’ï¼ˆãƒã‚¤ã‚¢ã‚¹ã®ã‚ã‚‹è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰</h4>
<p><strong>ç—‡çŠ¶</strong>: è¨“ç·´ç²¾åº¦ã¯é«˜ã„ãŒãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§æ€§èƒ½ä½ä¸‹</p>
<p><strong>åŸå› </strong>: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åã‚Šã€ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µä¸è¶³</p>
<p><strong>å¯¾ç­–</strong>:</p>
<pre><code class="language-python"># âŒ æ‚ªã„ä¾‹ï¼šãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãªã—ã€å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
model.fit(X_train, y_train, epochs=50, batch_size=16)
# çµæœ: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«éå­¦ç¿’

# âœ… è‰¯ã„ä¾‹ï¼šãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã¨Early Stopping
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping

# ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µè¨­å®š
datagen = ImageDataGenerator(
    rotation_range=20,  # ãƒ©ãƒ³ãƒ€ãƒ å›è»¢
    width_shift_range=0.1,  # æ°´å¹³ã‚·ãƒ•ãƒˆ
    height_shift_range=0.1,  # å‚ç›´ã‚·ãƒ•ãƒˆ
    horizontal_flip=True,  # æ°´å¹³åè»¢
    zoom_range=0.1,  # ã‚ºãƒ¼ãƒ 
    fill_mode='nearest'
)

# Early Stoppingï¼ˆæ¤œè¨¼æå¤±ãŒæ”¹å–„ã—ãªã„å ´åˆã«è¨“ç·´åœæ­¢ï¼‰
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# è¨“ç·´
history = model.fit(
    datagen.flow(X_train, y_train, batch_size=16),
    validation_data=(X_test, y_test),
    epochs=50,
    callbacks=[early_stop],
    verbose=1
)

print(f&quot;è¨“ç·´åœæ­¢ã‚¨ãƒãƒƒã‚¯: {len(history.history['loss'])}&quot;)
</code></pre>
<h4>å¤±æ•—5: ãƒãƒƒãƒå‡¦ç†ã§ã®ãƒãƒƒãƒåŠ¹æœï¼ˆBatch Effectï¼‰</h4>
<p><strong>ç—‡çŠ¶</strong>: åŒä¸€æ¡ä»¶ã®ã‚µãƒ³ãƒ—ãƒ«ã§ã‚‚æ¸¬å®šæ—¥ã«ã‚ˆã£ã¦ç²’å¾„ãŒç³»çµ±çš„ã«ç•°ãªã‚‹</p>
<p><strong>åŸå› </strong>: è£…ç½®ã®çµŒæ™‚å¤‰åŒ–ã€æ ¡æ­£ãšã‚Œ</p>
<p><strong>å¯¾ç­–</strong>:</p>
<pre><code class="language-python"># âŒ æ‚ªã„ä¾‹ï¼šãƒãƒƒãƒé–“ã®è£œæ­£ãªã—
results = []
for image_file in image_files:
    analyzer.load_image(image_file)
    analyzer.preprocess()
    results.append(analyzer.get_results())

# âœ… è‰¯ã„ä¾‹ï¼šæ¨™æº–è©¦æ–™ã«ã‚ˆã‚‹è£œæ­£
from datetime import datetime

# æ¨™æº–è©¦æ–™ï¼ˆæ—¢çŸ¥ç²’å¾„ï¼‰ã®æ¸¬å®š
STANDARD_DIAMETER_NM = 100.0  # æ—¢çŸ¥ã®ç‰©ç†ã‚µã‚¤ã‚º

def calibrate_with_standard(standard_image, expected_diameter_nm):
    &quot;&quot;&quot;æ¨™æº–è©¦æ–™ã‹ã‚‰æ ¡æ­£ä¿‚æ•°ã‚’å–å¾—&quot;&quot;&quot;
    analyzer_std = SEMImageAnalyzer()
    analyzer_std.load_image(standard_image)
    analyzer_std.preprocess()
    analyzer_std.segment_particles()
    analyzer_std.extract_features()

    results_std = analyzer_std.get_results()
    measured_diameter_pixels = results_std.mean_diameter

    nm_per_pixel = expected_diameter_nm / measured_diameter_pixels

    calibration = {
        'date': datetime.now().isoformat(),
        'nm_per_pixel': nm_per_pixel,
        'standard_diameter_nm': expected_diameter_nm,
        'measured_pixels': measured_diameter_pixels
    }

    return calibration

# å„ãƒãƒƒãƒã®æœ€åˆã«æ¨™æº–è©¦æ–™ã‚’æ¸¬å®š
batch_calibrations = {}
for batch_id, batch_images in batches.items():
    # æ¨™æº–è©¦æ–™æ¸¬å®š
    standard_image = load_standard_image(batch_id)
    calib = calibrate_with_standard(standard_image, STANDARD_DIAMETER_NM)
    batch_calibrations[batch_id] = calib

    print(f&quot;Batch {batch_id}: {calib['nm_per_pixel']:.3f} nm/pixel&quot;)

    # ãƒãƒƒãƒå†…ã®ã‚µãƒ³ãƒ—ãƒ«è§£æï¼ˆæ ¡æ­£ä¿‚æ•°ã‚’é©ç”¨ï¼‰
    for image_file in batch_images:
        analyzer.load_image(image_file)
        analyzer.preprocess()
        analyzer.segment_particles()
        analyzer.extract_features()

        results = analyzer.get_results()

        # æ ¡æ­£ä¿‚æ•°ã§ç‰©ç†ã‚µã‚¤ã‚ºã«å¤‰æ›
        diameter_nm = results.mean_diameter * calib['nm_per_pixel']
        print(f&quot;  Sample: {diameter_nm:.1f} nm&quot;)
</code></pre>
<hr />
<h2>3.8 ç”»åƒè§£æã‚¹ã‚­ãƒ«ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ</h2>
<h3>ç”»åƒå‰å‡¦ç†ã‚¹ã‚­ãƒ«</h3>
<h4>åŸºç¤ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] SEM/TEMç”»åƒã®ç‰¹å¾´ï¼ˆã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆã€ãƒã‚¤ã‚ºï¼‰ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>[ ] OpenCVã§ç”»åƒã‚’èª­ã¿è¾¼ã¿ã€ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ã§ãã‚‹</li>
<li>[ ] ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å‡ç­‰åŒ–ã¨CLAHEã®é•ã„ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] Gaussianãƒ•ã‚£ãƒ«ã‚¿ã§ãƒã‚¤ã‚ºé™¤å»ã§ãã‚‹</li>
<li>[ ] Otsuæ³•ã§äºŒå€¤åŒ–ã§ãã‚‹</li>
</ul>
<h4>å¿œç”¨ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] Non-Local Meansãƒ•ã‚£ãƒ«ã‚¿ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é©åˆ‡ã«è¨­å®šã§ãã‚‹</li>
<li>[ ] ç”»åƒã®ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆä¸è‰¯ã‚’è¨ºæ–­ã—ã€é©åˆ‡ãªå‰å‡¦ç†ã‚’é¸æŠã§ãã‚‹</li>
<li>[ ] ãƒ¢ãƒ«ãƒ•ã‚©ãƒ­ã‚¸ãƒ¼æ¼”ç®—ï¼ˆopeningã€closingï¼‰ã‚’ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹</li>
<li>[ ] ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®å½¢çŠ¶ã‹ã‚‰é©åˆ‡ãªé–¾å€¤è¨­å®šæ³•ã‚’é¸æŠã§ãã‚‹</li>
<li>[ ] è¤‡æ•°ã®ãƒ•ã‚£ãƒ«ã‚¿ã‚’çµ„ã¿åˆã‚ã›ã¦æœ€é©ãªå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
</ul>
<h4>ä¸Šç´šãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] ãƒãƒƒãƒå‡¦ç†ã§ç”»åƒå“è³ªã®ã°ã‚‰ã¤ãã‚’è‡ªå‹•æ¤œå‡ºã—ã€é©å¿œçš„ã«å‰å‡¦ç†ã§ãã‚‹</li>
<li>[ ] ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆå‘¨æ³¢æ•°é ˜åŸŸãƒ•ã‚£ãƒ«ã‚¿ï¼‰ã‚’è¨­è¨ˆãƒ»å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°ï¼ˆDnCNNï¼‰ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] ç”»åƒå‰å‡¦ç†ãŒå¾Œç¶šè§£æã«ä¸ãˆã‚‹å½±éŸ¿ã‚’å®šé‡è©•ä¾¡ã§ãã‚‹</li>
</ul>
<h3>ç²’å­æ¤œå‡ºãƒ»ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚­ãƒ«</h3>
<h4>åŸºç¤ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] è·é›¢å¤‰æ›ã®æ¦‚å¿µã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] Watershedæ³•ã®åŸºæœ¬åŸç†ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>[ ] ç²’å­ã®å€‹æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã§ãã‚‹</li>
<li>[ ] ç²’å¾„ï¼ˆå††ç›¸å½“ç›´å¾„ï¼‰ã‚’è¨ˆç®—ã§ãã‚‹</li>
<li>[ ] æ¤œå‡ºçµæœã‚’å¯è¦–åŒ–ã§ãã‚‹</li>
</ul>
<h4>å¿œç”¨ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] Watershedã®min_distanceãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç²’å¾„ã«å¿œã˜ã¦è¨­å®šã§ãã‚‹</li>
<li>[ ] éæ¤œå‡ºãƒ»æ¤œå‡ºæ¼ã‚Œã‚’è¨ºæ–­ã—ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã§ãã‚‹</li>
<li>[ ] é‡ãªã£ãŸç²’å­ã‚’åˆ†é›¢ã§ãã‚‹</li>
<li>[ ] å½¢çŠ¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå††å½¢åº¦ã€ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ï¼‰ã‚’è¨ˆç®—ãƒ»è§£é‡ˆã§ãã‚‹</li>
<li>[ ] ç²’å¾„åˆ†å¸ƒã‚’çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ï¼ˆå¯¾æ•°æ­£è¦åˆ†å¸ƒï¼‰ã§ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã§ãã‚‹</li>
</ul>
<h4>ä¸Šç´šãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] è¤‡é›‘ãªå½¢çŠ¶ç²’å­ã«å¯¾ã—ã¦Active Contourãƒ¢ãƒ‡ãƒ«ã‚’é©ç”¨ã§ãã‚‹</li>
<li>[ ] æ©Ÿæ¢°å­¦ç¿’ï¼ˆU-Netï¼‰ã«ã‚ˆã‚‹ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] 3Dç”»åƒï¼ˆXç·šCTï¼‰ã®3æ¬¡å…ƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãŒã§ãã‚‹</li>
<li>[ ] ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç²¾åº¦ã‚’å®šé‡è©•ä¾¡ï¼ˆDiceä¿‚æ•°ã€IoUï¼‰ã§ãã‚‹</li>
</ul>
<h3>ç”»åƒæ¸¬å®šãƒ»æ ¡æ­£ã‚¹ã‚­ãƒ«</h3>
<h4>åŸºç¤ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] ã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒ¼ã®ä½ç½®ã‚’ç‰¹å®šã§ãã‚‹</li>
<li>[ ] æ‰‹å‹•ã§ãƒ”ã‚¯ã‚»ãƒ«-ç‰©ç†ã‚µã‚¤ã‚ºå¤‰æ›ãŒã§ãã‚‹</li>
<li>[ ] ç²’å¾„ã‚’nm/Î¼må˜ä½ã§å ±å‘Šã§ãã‚‹</li>
<li>[ ] æ¸¬å®šèª¤å·®ã®å­˜åœ¨ã‚’èªè­˜ã—ã¦ã„ã‚‹</li>
</ul>
<h4>å¿œç”¨ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] ã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒ¼ã‹ã‚‰è‡ªå‹•ã§æ ¡æ­£ä¿‚æ•°ã‚’è¨ˆç®—ã§ãã‚‹</li>
<li>[ ] ç•°ãªã‚‹å€ç‡ã®ç”»åƒã‚’çµ±ä¸€çš„ã«è§£æã§ãã‚‹</li>
<li>[ ] æ ¡æ­£èª¤å·®ã‚’æ¨å®šã—ã€çµæœã«ä¸ç¢ºã‹ã•ã‚’æ˜ç¤ºã§ãã‚‹</li>
<li>[ ] æ¸¬å®šã®ç¹°ã‚Šè¿”ã—æ€§ï¼ˆå†ç¾æ€§ï¼‰ã‚’è©•ä¾¡ã§ãã‚‹</li>
<li>[ ] æ¨™æº–è©¦æ–™ã«ã‚ˆã‚‹è£…ç½®æ ¡æ­£ã‚’å®Ÿæ–½ã§ãã‚‹</li>
</ul>
<h4>ä¸Šç´šãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] ãƒãƒƒãƒåŠ¹æœã‚’è£œæ­£ã§ãã‚‹ï¼ˆæ¨™æº–è©¦æ–™ã®å®šæœŸæ¸¬å®šï¼‰</li>
<li>[ ] æ¸¬å®šä¸ç¢ºã‹ã•ã‚’ä¼æ’­è¨ˆç®—ã§ãã‚‹ï¼ˆISO GUMï¼‰</li>
<li>[ ] ç•°ãªã‚‹è£…ç½®é–“ã§ã®æ¸¬å®šå€¤ã®äº’æ›æ€§ã‚’æ¤œè¨¼ã§ãã‚‹</li>
<li>[ ] ãƒˆãƒ¬ãƒ¼ã‚µãƒ–ãƒ«ãªæ ¡æ­£ãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
</ul>
<h3>æ·±å±¤å­¦ç¿’ãƒ»ç”»åƒåˆ†é¡ã‚¹ã‚­ãƒ«</h3>
<h4>åŸºç¤ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] CNNã®åŸºæœ¬æ§‹é€ ï¼ˆç•³ã¿è¾¼ã¿å±¤ã€ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ï¼‰ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] è»¢ç§»å­¦ç¿’ã®æ¦‚å¿µã‚’èª¬æ˜ã§ãã‚‹</li>
<li>[ ] è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã§ãã‚‹</li>
<li>[ ] ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’è©•ä¾¡ã§ãã‚‹</li>
<li>[ ] æ··åŒè¡Œåˆ—ã‚’è§£é‡ˆã§ãã‚‹</li>
</ul>
<h4>å¿œç”¨ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆData Augmentationï¼‰ã‚’é©ç”¨ã§ãã‚‹</li>
<li>[ ] Early Stoppingã§éå­¦ç¿’ã‚’é˜²æ­¢ã§ãã‚‹</li>
<li>[ ] ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’ç‡ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼‰ã‚’èª¿æ•´ã§ãã‚‹</li>
<li>[ ] ç•°ãªã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆVGGã€ResNetã€EfficientNetï¼‰ã‚’æ¯”è¼ƒã§ãã‚‹</li>
<li>[ ] Grad-CAMã§åˆ¤æ–­æ ¹æ‹ ã‚’å¯è¦–åŒ–ã§ãã‚‹</li>
</ul>
<h4>ä¸Šç´šãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] ã‚«ã‚¹ã‚¿ãƒ CNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è¨­è¨ˆã§ãã‚‹</li>
<li>[ ] ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã«å¯¾å‡¦ã§ãã‚‹ï¼ˆã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆã€SMOTEï¼‰</li>
<li>[ ] ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ï¼ˆåˆ†é¡ï¼‹å›å¸°ï¼‰ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] ãƒ¢ãƒ‡ãƒ«ã®ä¸ç¢ºã‹ã•ã‚’å®šé‡åŒ–ã§ãã‚‹ï¼ˆBayesian Deep Learningï¼‰</li>
<li>[ ] å°‘é‡ãƒ‡ãƒ¼ã‚¿ã§ã®å­¦ç¿’ï¼ˆFew-shot Learningï¼‰ã‚’å®Ÿè£…ã§ãã‚‹</li>
</ul>
<h3>çµ±åˆã‚¹ã‚­ãƒ«ï¼šå…¨ä½“ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</h3>
<h4>åŸºç¤ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] ç”»åƒèª­ã¿è¾¼ã¿â†’å‰å‡¦ç†â†’ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³â†’ç‰¹å¾´æŠ½å‡ºã®æµã‚Œã‚’å®Ÿè¡Œã§ãã‚‹</li>
<li>[ ] çµæœã‚’è¡¨ã«ã¾ã¨ã‚ã¦å ±å‘Šã§ãã‚‹</li>
<li>[ ] ã‚°ãƒ©ãƒ•ï¼ˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã€æ•£å¸ƒå›³ï¼‰ã§å¯è¦–åŒ–ã§ãã‚‹</li>
</ul>
<h4>å¿œç”¨ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] è¤‡æ•°ç”»åƒã®ãƒãƒƒãƒå‡¦ç†ã‚’è‡ªå‹•åŒ–ã§ãã‚‹</li>
<li>[ ] å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ã‚¯ãƒ©ã‚¹è¨­è¨ˆã§ãã‚‹</li>
<li>[ ] çµæœã‚’JSON/CSVå½¢å¼ã§ä¿å­˜ã§ãã‚‹</li>
<li>[ ] ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ­ã‚°è¨˜éŒ²ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’YAML/JSONãƒ•ã‚¡ã‚¤ãƒ«ã§ç®¡ç†ã§ãã‚‹</li>
</ul>
<h4>ä¸Šç´šãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆStreamlitã€Gradioï¼‰ã§è§£æãƒ„ãƒ¼ãƒ«ã‚’å…¬é–‹ã§ãã‚‹</li>
<li>[ ] ã‚¯ãƒ©ã‚¦ãƒ‰ï¼ˆAWSã€GCPï¼‰ã§ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªãƒãƒƒãƒå‡¦ç†ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
<li>[ ] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆMongoDBï¼‰ã«çµæœã‚’ä¿å­˜ãƒ»æ¤œç´¢ã§ãã‚‹</li>
<li>[ ] CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–ã§ãã‚‹</li>
<li>[ ] è«–æ–‡å“è³ªã®å›³è¡¨ã¨ãƒ¬ãƒãƒ¼ãƒˆã‚’è‡ªå‹•ç”Ÿæˆã§ãã‚‹</li>
</ul>
<hr />
<h2>3.9 ç·åˆã‚¹ã‚­ãƒ«è©•ä¾¡</h2>
<p>ä»¥ä¸‹ã®åŸºæº–ã§è‡ªå·±è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚</p>
<h3>ãƒ¬ãƒ™ãƒ«1ï¼šåˆå­¦è€…ï¼ˆåŸºç¤ã‚¹ã‚­ãƒ«ã®60%ä»¥ä¸Šï¼‰</h3>
<ul>
<li>åŸºæœ¬çš„ãªç”»åƒå‰å‡¦ç†ã¨ç²’å­æ¤œå‡ºãŒã§ãã‚‹</li>
<li>æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã‚’ç†è§£ã—ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãŒã§ãã‚‹</li>
<li>ç°¡å˜ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è§£æã‚’å®Œçµã§ãã‚‹</li>
</ul>
<p><strong>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</strong>:
- å¿œç”¨ãƒ¬ãƒ™ãƒ«ã®æŠ€è¡“ï¼ˆWatershedã€ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼‰ã‚’ç¿’å¾—
- ã‚ˆã‚Šè¤‡é›‘ãªå®Ÿãƒ‡ãƒ¼ã‚¿ã«æŒ‘æˆ¦
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ„å‘³ã‚’æ·±ãç†è§£</p>
<h3>ãƒ¬ãƒ™ãƒ«2ï¼šä¸­ç´šè€…ï¼ˆåŸºç¤100% + å¿œç”¨60%ä»¥ä¸Šï¼‰</h3>
<ul>
<li>è¤‡é›‘ãªç”»åƒãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦é©åˆ‡ãªå‰å‡¦ç†ãƒ»ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é¸æŠã§ãã‚‹</li>
<li>CNNã«ã‚ˆã‚‹ç”»åƒåˆ†é¡ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>ãƒãƒƒãƒå‡¦ç†ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒã§ãã‚‹</li>
</ul>
<p><strong>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</strong>:
- ä¸Šç´šæŠ€è¡“ï¼ˆã‚«ã‚¹ã‚¿ãƒ CNNã€3Dè§£æï¼‰ã«æŒ‘æˆ¦
- ç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§å®Ÿè·µ
- ã‚³ãƒ¼ãƒ‰ã®æœ€é©åŒ–ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£å‘ä¸Š</p>
<h3>ãƒ¬ãƒ™ãƒ«3ï¼šä¸Šç´šè€…ï¼ˆåŸºç¤100% + å¿œç”¨100% + ä¸Šç´š60%ä»¥ä¸Šï¼‰</h3>
<ul>
<li>æ–°ã—ã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è¨­è¨ˆãƒ»å®Ÿè£…ã§ãã‚‹</li>
<li>ç ”ç©¶è«–æ–‡ãƒ¬ãƒ™ãƒ«ã®è§£æã‚’å®Ÿè¡Œã§ãã‚‹</li>
<li>ä»–ã®ç ”ç©¶è€…ã«ãƒ„ãƒ¼ãƒ«ã‚’æä¾›ã§ãã‚‹</li>
</ul>
<p><strong>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</strong>:
- ã‚ªãƒªã‚¸ãƒŠãƒ«ã®æ‰‹æ³•é–‹ç™º
- è«–æ–‡åŸ·ç­†ãƒ»å­¦ä¼šç™ºè¡¨
- ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã¸ã®è²¢çŒ®</p>
<h3>ãƒ¬ãƒ™ãƒ«4ï¼šã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆï¼ˆå…¨é …ç›®90%ä»¥ä¸Šï¼‰</h3>
<ul>
<li>ææ–™ç”»åƒè§£æã®åˆ†é‡ã‚’ãƒªãƒ¼ãƒ‰ã§ãã‚‹</li>
<li>æ–°ã—ã„æ¸¬å®šæŠ€è¡“ã«å¯¾å¿œã—ãŸè§£ææ‰‹æ³•ã‚’é–‹ç™ºã§ãã‚‹</li>
<li>å›½éš›çš„ãªã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«è²¢çŒ®ã—ã¦ã„ã‚‹</li>
</ul>
<p><strong>æ´»å‹•ä¾‹</strong>:
- æ‹›å¾…è¬›æ¼”ãƒ»ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«
- å…±åŒç ”ç©¶ã®ä¸»å°
- æ¨™æº–åŒ–æ´»å‹•ã¸ã®å‚ç”»</p>
<hr />
<h2>3.10 è¡Œå‹•è¨ˆç”»ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ</h2>
<h3>ç¾åœ¨ã®ãƒ¬ãƒ™ãƒ«: <strong><em>_</em></strong>____</h3>
<h3>ç›®æ¨™ãƒ¬ãƒ™ãƒ«ï¼ˆ3ãƒ¶æœˆå¾Œï¼‰: <strong><em>_</em></strong>____</h3>
<h3>é‡ç‚¹å¼·åŒ–ã‚¹ã‚­ãƒ«ï¼ˆ3ã¤é¸æŠï¼‰:</h3>
<ol>
<li>
<hr />
</li>
<li>
<hr />
</li>
<li>
<hr />
</li>
</ol>
<h3>å…·ä½“çš„è¡Œå‹•è¨ˆç”»:</h3>
<p><strong>Week 1-2</strong>:
- [ ] è¡Œå‹•1: <strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em>_
- [ ] è¡Œå‹•2: </strong><strong><em>_</em></strong><strong><em>_</em></strong>____</p>
<p><strong>Week 3-4</strong>:
- [ ] è¡Œå‹•1: <strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em>_
- [ ] è¡Œå‹•2: </strong><strong><em>_</em></strong><strong><em>_</em></strong>____</p>
<p><strong>Week 5-8</strong>:
- [ ] è¡Œå‹•1: <strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em>_
- [ ] è¡Œå‹•2: </strong><strong><em>_</em></strong><strong><em>_</em></strong>____</p>
<p><strong>Week 9-12</strong>:
- [ ] è¡Œå‹•1: <strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em>_
- [ ] è¡Œå‹•2: </strong><strong><em>_</em></strong><strong><em>_</em></strong>____</p>
<h3>è©•ä¾¡æŒ‡æ¨™:</h3>
<ul>
<li>[ ] ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è§£æå®Œäº†</li>
<li>[ ] ç ”ç©¶ä¼š/ã‚¼ãƒŸã§ç™ºè¡¨</li>
<li>[ ] GitHub/è«–æ–‡ã§æˆæœå…¬é–‹</li>
</ul>
<hr />
<h2>3.11 æœ¬ç« ã®ã¾ã¨ã‚</h2>
<h3>å­¦ã‚“ã ã“ã¨</h3>
<ol>
<li>
<p><strong>ãƒ‡ãƒ¼ã‚¿ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¨å†ç¾æ€§</strong>
   - ç”»åƒãƒ‡ãƒ¼ã‚¿ãƒªãƒã‚¸ãƒˆãƒªã®æ´»ç”¨
   - ç”»åƒãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨æ ¡æ­£æƒ…å ±ã®è¨˜éŒ²
   - ã‚³ãƒ¼ãƒ‰å†ç¾æ€§ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</p>
</li>
<li>
<p><strong>ç”»åƒå‰å‡¦ç†</strong>
   - ãƒã‚¤ã‚ºé™¤å»ï¼ˆGaussianã€Medianã€Bilateralã€NLMï¼‰
   - ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆèª¿æ•´ï¼ˆHistogram Equalizationã€CLAHEï¼‰
   - äºŒå€¤åŒ–ï¼ˆOtsuæ³•ï¼‰</p>
</li>
<li>
<p><strong>ç²’å­æ¤œå‡º</strong>
   - Watershedæ³•ã«ã‚ˆã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
   - è·é›¢å¤‰æ›ã¨å±€æ‰€æœ€å¤§å€¤æ¤œå‡º
   - ãƒ¢ãƒ«ãƒ•ã‚©ãƒ­ã‚¸ãƒ¼æ¼”ç®—</p>
</li>
<li>
<p><strong>å®šé‡è§£æ</strong>
   - ç²’å¾„åˆ†å¸ƒï¼ˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã€ç´¯ç©åˆ†å¸ƒï¼‰
   - å½¢çŠ¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå††å½¢åº¦ã€ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ï¼‰
   - å¯¾æ•°æ­£è¦åˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°</p>
</li>
<li>
<p><strong>æ·±å±¤å­¦ç¿’</strong>
   - è»¢ç§»å­¦ç¿’ï¼ˆVGG16ï¼‰
   - ææ–™ç”»åƒåˆ†é¡
   - æ··åŒè¡Œåˆ—ã«ã‚ˆã‚‹æ€§èƒ½è©•ä¾¡</p>
</li>
<li>
<p><strong>å®Ÿè·µçš„ãªè½ã¨ã—ç©´</strong>
   - éåº¦ãªã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®å›é¿
   - ãƒ”ã‚¯ã‚»ãƒ«æ ¡æ­£ã®é‡è¦æ€§
   - ãƒãƒƒãƒåŠ¹æœã®è£œæ­£
   - CNNã®éå­¦ç¿’å¯¾ç­–</p>
</li>
</ol>
<h3>é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ</h3>
<ul>
<li>âœ… ç”»åƒãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆå€ç‡ã€ã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒ¼ï¼‰ã‚’å¿…ãšè¨˜éŒ²</li>
<li>âœ… å‰å‡¦ç†ã®è³ªãŒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç²¾åº¦ã‚’æ±ºå®šã™ã‚‹</li>
<li>âœ… Watershedæ³•ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãŒé‡è¦ï¼ˆmin_distanceã€threshold_absï¼‰</li>
<li>âœ… ç²’å¾„ã¯ç‰©ç†å˜ä½ï¼ˆnmã€Î¼mï¼‰ã§å ±å‘Šã™ã‚‹</li>
<li>âœ… ãƒ”ã‚¯ã‚»ãƒ«æ ¡æ­£ã¯æ¨™æº–è©¦æ–™ã§å®šæœŸçš„ã«æ¤œè¨¼</li>
<li>âœ… è»¢ç§»å­¦ç¿’ã«ã‚ˆã‚Šå°‘é‡ãƒ‡ãƒ¼ã‚¿ã§ã‚‚é«˜ç²¾åº¦åˆ†é¡ãŒå®Ÿç¾</li>
<li>âœ… ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã¨Early Stoppingã§éå­¦ç¿’ã‚’é˜²æ­¢</li>
</ul>
<h3>æ¬¡ã®ç« ã¸</h3>
<p>ç¬¬4ç« ã§ã¯ã€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆè§£æã‚’å­¦ã³ã¾ã™ï¼š
- æ¸©åº¦ãƒ»åœ§åŠ›ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
- ç§»å‹•çª“è§£æ
- ç•°å¸¸æ¤œçŸ¥
- PCAã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›
- sklearn Pipelineã«ã‚ˆã‚‹è‡ªå‹•åŒ–</p>
<p><strong><a href="./chapter-4.html">ç¬¬4ç« ï¼šæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆè§£æ â†’</a></strong></p>
<hr />
<h2>æ¼”ç¿’å•é¡Œ</h2>
<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>
<p>æ¬¡ã®æ–‡ç« ã®æ­£èª¤ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚</p>
<ol>
<li>Bilateral Filterã¯ã‚¨ãƒƒã‚¸ã‚’Gaussian Filterã‚ˆã‚Šã‚‚ä¿æŒã§ãã‚‹</li>
<li>CLAHEã¯ç”»åƒå…¨ä½“ã«åŒä¸€ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å‡ç­‰åŒ–ã‚’é©ç”¨ã™ã‚‹</li>
<li>Watershedæ³•ã§ã¯è·é›¢å¤‰æ›ã®å±€æ‰€æœ€å¤§å€¤ã‚’ç²’å­ä¸­å¿ƒã¨ã¿ãªã™</li>
</ol>
<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

1. Bilateral Filterã®å‹•ä½œåŸç†ï¼ˆç©ºé–“è·é›¢ã¨è¼åº¦å·®ã®ä¸¡æ–¹ã‚’è€ƒæ…®ï¼‰
2. CLAHEã®"Adaptive"ã®æ„å‘³
3. Watershedã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æµã‚Œï¼ˆè·é›¢å¤‰æ›â†’ãƒãƒ¼ã‚«ãƒ¼â†’watershedï¼‰

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

**è§£ç­”**:
1. **æ­£** - Bilateral Filterã¯è¼åº¦å·®ã‚‚è€ƒæ…®ã™ã‚‹ãŸã‚ã€ã‚¨ãƒƒã‚¸ä»˜è¿‘ã§ã¯å¹³æ»‘åŒ–ãŒæŠ‘åˆ¶ã•ã‚Œã‚‹
2. **èª¤** - CLAHEã¯ç”»åƒã‚’å°é ˜åŸŸï¼ˆã‚¿ã‚¤ãƒ«ï¼‰ã«åˆ†å‰²ã—ã€å„é ˜åŸŸã§é©å¿œçš„ã«ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å‡ç­‰åŒ–ã‚’è¡Œã†
3. **æ­£** - è·é›¢å¤‰æ›ã®å±€æ‰€æœ€å¤§å€¤ãŒç²’å­ä¸­å¿ƒã«å¯¾å¿œã—ã€Watershedã®ãƒãƒ¼ã‚«ãƒ¼ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹

**è§£èª¬**:
ç”»åƒå‰å‡¦ç†ã§ã¯ã€å‡¦ç†ã®ç›®çš„ï¼ˆãƒã‚¤ã‚ºé™¤å» vs ã‚¨ãƒƒã‚¸ä¿æŒï¼‰ã«å¿œã˜ã¦é©åˆ‡ãªæ‰‹æ³•ã‚’é¸æŠã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚Watershedã¯è·é›¢å¤‰æ›ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€æ¥è§¦ã—ã¦ã„ã‚‹ç²’å­ã‚‚åˆ†é›¢ã§ãã‚‹å¼·åŠ›ãªæ‰‹æ³•ã§ã™ã€‚

</details>

<hr />
<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>ä»¥ä¸‹ã®SEMç”»åƒãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã€ç²’å­æ¤œå‡ºã¨ç²’å¾„åˆ†å¸ƒè§£æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚</p>
<pre><code class="language-python">import numpy as np

# ã‚µãƒ³ãƒ—ãƒ«SEMç”»åƒç”Ÿæˆ
def generate_sample_sem():
    np.random.seed(100)
    img = np.zeros((512, 512), dtype=np.uint8)

    for _ in range(40):
        x = np.random.randint(30, 482)
        y = np.random.randint(30, 482)
        r = np.random.randint(10, 25)
        cv2.circle(img, (x, y), r, 200, -1)

    noise = np.random.normal(0, 30, img.shape)
    return np.clip(img + noise, 0, 255).astype(np.uint8)

sample_image = generate_sample_sem()
</code></pre>
<p><strong>è¦æ±‚äº‹é …</strong>:
1. Non-Local Meansã§ãƒã‚¤ã‚ºé™¤å»
2. Otsuæ³•ã§äºŒå€¤åŒ–
3. Watershedæ³•ã§ç²’å­æ¤œå‡º
4. ç²’å¾„åˆ†å¸ƒã‚’ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã§ãƒ—ãƒ­ãƒƒãƒˆ
5. å¹³å‡ç²’å¾„ãƒ»æ¨™æº–åå·®ã‚’å‡ºåŠ›</p>
<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

**å‡¦ç†ãƒ•ãƒ­ãƒ¼**:
1. `cv2.fastNlMeansDenoising`ã§ãƒã‚¤ã‚ºé™¤å»
2. `filters.threshold_otsu`ã§é–¾å€¤è¨ˆç®—â†’äºŒå€¤åŒ–
3. `distance_transform_edt` + `peak_local_max` + `watershed`
4. `measure.regionprops`ã§ç²’å­ç‰¹å¾´é‡æŠ½å‡º
5. `matplotlib.pyplot.hist`ã§å¯è¦–åŒ–

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python">import numpy as np
import cv2
import matplotlib.pyplot as plt
from skimage import filters, morphology, measure
from skimage.feature import peak_local_max
from skimage.segmentation import watershed
from scipy.ndimage import distance_transform_edt

# ã‚µãƒ³ãƒ—ãƒ«ç”»åƒç”Ÿæˆ
def generate_sample_sem():
    np.random.seed(100)
    img = np.zeros((512, 512), dtype=np.uint8)

    for _ in range(40):
        x = np.random.randint(30, 482)
        y = np.random.randint(30, 482)
        r = np.random.randint(10, 25)
        cv2.circle(img, (x, y), r, 200, -1)

    noise = np.random.normal(0, 30, img.shape)
    return np.clip(img + noise, 0, 255).astype(np.uint8)

sample_image = generate_sample_sem()

# ã‚¹ãƒ†ãƒƒãƒ—1: ãƒã‚¤ã‚ºé™¤å»
denoised = cv2.fastNlMeansDenoising(sample_image, None, 10, 7, 21)

# ã‚¹ãƒ†ãƒƒãƒ—2: OtsuäºŒå€¤åŒ–
threshold = filters.threshold_otsu(denoised)
binary = denoised &gt; threshold
binary = morphology.remove_small_objects(binary, min_size=30)

# ã‚¹ãƒ†ãƒƒãƒ—3: Watershed
distance = distance_transform_edt(binary)
local_max = peak_local_max(distance, min_distance=15,
                           threshold_abs=3, labels=binary)
markers = np.zeros_like(distance, dtype=int)
markers[tuple(local_max.T)] = np.arange(1, len(local_max) + 1)
labels = watershed(-distance, markers, mask=binary)

# ã‚¹ãƒ†ãƒƒãƒ—4: ç²’å¾„è¨ˆç®—
diameters = []
for region in measure.regionprops(labels):
    area = region.area
    diameter = np.sqrt(4 * area / np.pi)
    diameters.append(diameter)

diameters = np.array(diameters)

# ã‚¹ãƒ†ãƒƒãƒ—5: çµ±è¨ˆãƒ»å¯è¦–åŒ–
print(&quot;=== ç²’å¾„çµ±è¨ˆ ===&quot;)
print(f&quot;æ¤œå‡ºç²’å­æ•°: {len(diameters)}&quot;)
print(f&quot;å¹³å‡ç²’å¾„: {diameters.mean():.2f} pixels&quot;)
print(f&quot;æ¨™æº–åå·®: {diameters.std():.2f} pixels&quot;)

fig, axes = plt.subplots(2, 2, figsize=(14, 14))

axes[0, 0].imshow(sample_image, cmap='gray')
axes[0, 0].set_title('Original Image')
axes[0, 0].axis('off')

axes[0, 1].imshow(binary, cmap='gray')
axes[0, 1].set_title(f'Binary (Otsu={threshold:.1f})')
axes[0, 1].axis('off')

axes[1, 0].imshow(labels, cmap='nipy_spectral')
axes[1, 0].set_title(f'Detected Particles ({len(diameters)})')
axes[1, 0].axis('off')

axes[1, 1].hist(diameters, bins=15, alpha=0.7, edgecolor='black')
axes[1, 1].axvline(diameters.mean(), color='red', linestyle='--',
                  label=f'Mean: {diameters.mean():.1f}')
axes[1, 1].set_xlabel('Diameter (pixels)')
axes[1, 1].set_ylabel('Frequency')
axes[1, 1].set_title('Particle Size Distribution')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
</code></pre>


**å‡ºåŠ›ä¾‹**:

<pre><code>=== ç²’å¾„çµ±è¨ˆ ===
æ¤œå‡ºç²’å­æ•°: 38
å¹³å‡ç²’å¾„: 30.45 pixels
æ¨™æº–åå·®: 8.23 pixels
</code></pre>


**è§£èª¬**:
ã“ã®ä¾‹ã§ã¯ã€Watershedã®min_distance=15ã«ã‚ˆã‚Šã€è¿‘æ¥ç²’å­ã®éæ¤œå‡ºã‚’æŠ‘åˆ¶ã—ã¦ã„ã¾ã™ã€‚ç²’å¾„ã®ã°ã‚‰ã¤ãï¼ˆæ¨™æº–åå·®ï¼‰ã¯åˆæˆéç¨‹ã«èµ·å› ã—ã¾ã™ã€‚å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã¯ã€æ¸¬å®šæ¡ä»¶ã‚„ææ–™ã®ä¸å‡ä¸€æ€§ã‚’åæ˜ ã—ã¾ã™ã€‚

</details>

<hr />
<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>è¤‡æ•°ã®SEMç”»åƒã‚’è‡ªå‹•å‡¦ç†ã—ã€ç²’å¾„åˆ†å¸ƒã®çµ±è¨ˆæ¯”è¼ƒã‚’è¡Œã†ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>èƒŒæ™¯</strong>:
ç•°ãªã‚‹åˆæˆæ¡ä»¶ã§ä½œè£½ã•ã‚ŒãŸææ–™Aã€Bã€Cã®ã‚µãƒ³ãƒ—ãƒ«ã«ã¤ã„ã¦ã€å„10æšã®SEMç”»åƒãŒæ’®å½±ã•ã‚Œã¾ã—ãŸã€‚å„ã‚µãƒ³ãƒ—ãƒ«ã®ç²’å¾„åˆ†å¸ƒã‚’è‡ªå‹•è§£æã—ã€çµ±è¨ˆçš„ã«æ¯”è¼ƒã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚</p>
<p><strong>èª²é¡Œ</strong>:
1. ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚Š30æšã®ç”»åƒã‚’è‡ªå‹•è§£æ
2. ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã®ç²’å¾„åˆ†å¸ƒã‚’å¯è¦–åŒ–
3. åˆ†æ•£åˆ†æï¼ˆANOVAï¼‰ã«ã‚ˆã‚‹çµ±è¨ˆçš„æœ‰æ„å·®æ¤œå®š
4. çµæœã‚’PDFãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦å‡ºåŠ›</p>
<p><strong>åˆ¶ç´„æ¡ä»¶</strong>:
- å„ç”»åƒã®æ¸¬å®šæ¡ä»¶ï¼ˆå€ç‡ã€éœ²å…‰ï¼‰ãŒç•°ãªã‚‹å¯èƒ½æ€§
- ä¸€éƒ¨ç”»åƒã¯ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆä¸è‰¯
- å‡¦ç†æ™‚é–“ï¼š5ç§’ä»¥å†…/ç”»åƒ</p>
<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

**è¨­è¨ˆæ–¹é‡**:
1. `SEMImageAnalyzer`ã‚¯ãƒ©ã‚¹ã‚’æ‹¡å¼µ
2. é©å¿œçš„å‰å‡¦ç†ï¼ˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ è§£æã§ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆåˆ¤å®šï¼‰
3. çµæœã‚’æ§‹é€ åŒ–ã—ã¦ä¿å­˜ï¼ˆJSON/CSVï¼‰
4. `scipy.stats.f_oneway`ã§ANOVA
5. `matplotlib.backends.backend_pdf`ã§PDFç”Ÿæˆ

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

**è§£ç­”ã®æ¦‚è¦**:
ãƒãƒƒãƒå‡¦ç†ã€çµ±è¨ˆè§£æã€ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’å«ã‚€çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

**å®Ÿè£…ã‚³ãƒ¼ãƒ‰**:


<pre><code class="language-python">from scipy.stats import f_oneway
from matplotlib.backends.backend_pdf import PdfPages

class BatchSEMAnalyzer:
    &quot;&quot;&quot;ãƒãƒƒãƒSEMç”»åƒè§£æã‚·ã‚¹ãƒ†ãƒ &quot;&quot;&quot;

    def __init__(self):
        self.results = {}

    def adaptive_preprocess(self, image):
        &quot;&quot;&quot;é©å¿œçš„å‰å‡¦ç†&quot;&quot;&quot;
        # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆè©•ä¾¡
        contrast = image.max() - image.min()

        if contrast &lt; 100:  # ä½ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ
            # CLAHEå¼·åŒ–
            clahe = cv2.createCLAHE(clipLimit=3.0,
                                    tileGridSize=(8, 8))
            image = clahe.apply(image)

        # ãƒã‚¤ã‚ºé™¤å»ï¼ˆé©å¿œçš„å¼·åº¦ï¼‰
        noise_std = np.std(np.diff(image, axis=0))
        h = 10 if noise_std &lt; 20 else 15

        denoised = cv2.fastNlMeansDenoising(image, None, h, 7, 21)
        return denoised

    def analyze_single(self, image, sample_id):
        &quot;&quot;&quot;å˜ä¸€ç”»åƒè§£æ&quot;&quot;&quot;
        # å‰å‡¦ç†
        preprocessed = self.adaptive_preprocess(image)

        # OtsuäºŒå€¤åŒ–
        threshold = filters.threshold_otsu(preprocessed)
        binary = preprocessed &gt; threshold
        binary = morphology.remove_small_objects(binary, min_size=30)

        # Watershed
        distance = distance_transform_edt(binary)
        local_max = peak_local_max(distance, min_distance=15,
                                   labels=binary)
        markers = np.zeros_like(distance, dtype=int)
        markers[tuple(local_max.T)] = np.arange(1, len(local_max) + 1)
        labels = watershed(-distance, markers, mask=binary)

        # ç²’å¾„æŠ½å‡º
        diameters = []
        for region in measure.regionprops(labels):
            area = region.area
            diameter = np.sqrt(4 * area / np.pi)
            diameters.append(diameter)

        return np.array(diameters)

    def batch_analyze(self, image_dict):
        &quot;&quot;&quot;
        ãƒãƒƒãƒè§£æ

        Parameters:
        -----------
        image_dict : dict
            {'sample_A': [img1, img2, ...], 'sample_B': [...]}
        &quot;&quot;&quot;
        for sample_id, images in image_dict.items():
            all_diameters = []

            for img in images:
                diameters = self.analyze_single(img, sample_id)
                all_diameters.extend(diameters)

            self.results[sample_id] = np.array(all_diameters)

    def statistical_comparison(self):
        &quot;&quot;&quot;çµ±è¨ˆçš„æ¯”è¼ƒï¼ˆANOVAï¼‰&quot;&quot;&quot;
        groups = list(self.results.values())
        f_stat, p_value = f_oneway(*groups)

        print(&quot;=== åˆ†æ•£åˆ†æï¼ˆANOVAï¼‰===&quot;)
        print(f&quot;Fçµ±è¨ˆé‡: {f_stat:.3f}&quot;)
        print(f&quot;på€¤: {p_value:.4f}&quot;)

        if p_value &lt; 0.05:
            print(&quot;çµè«–: ã‚µãƒ³ãƒ—ãƒ«é–“ã«æœ‰æ„å·®ã‚ã‚Šï¼ˆp &lt; 0.05ï¼‰&quot;)
        else:
            print(&quot;çµè«–: ã‚µãƒ³ãƒ—ãƒ«é–“ã«æœ‰æ„å·®ãªã—ï¼ˆp â‰¥ 0.05ï¼‰&quot;)

        return f_stat, p_value

    def generate_report(self, filename='sem_report.pdf'):
        &quot;&quot;&quot;PDFãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ&quot;&quot;&quot;
        with PdfPages(filename) as pdf:
            # ãƒšãƒ¼ã‚¸1: ç²’å¾„åˆ†å¸ƒæ¯”è¼ƒ
            fig, axes = plt.subplots(2, 2, figsize=(11, 8.5))

            for i, (sample_id, diameters) in enumerate(self.results.items()):
                ax = axes.ravel()[i]
                ax.hist(diameters, bins=20, alpha=0.7, edgecolor='black')
                ax.axvline(diameters.mean(), color='red',
                          linestyle='--',
                          label=f'Mean: {diameters.mean():.1f}')
                ax.set_xlabel('Diameter (pixels)')
                ax.set_ylabel('Frequency')
                ax.set_title(f'{sample_id} (n={len(diameters)})')
                ax.legend()
                ax.grid(True, alpha=0.3, axis='y')

            # çµ±è¨ˆã‚µãƒãƒªãƒ¼
            ax = axes.ravel()[3]
            ax.axis('off')
            summary_text = &quot;=== Statistical Summary ===\n\n&quot;
            for sample_id, diameters in self.results.items():
                summary_text += f&quot;{sample_id}:\n&quot;
                summary_text += f&quot;  Mean: {diameters.mean():.2f}\n&quot;
                summary_text += f&quot;  Std: {diameters.std():.2f}\n&quot;
                summary_text += f&quot;  n: {len(diameters)}\n\n&quot;

            ax.text(0.1, 0.5, summary_text, fontsize=12,
                   verticalalignment='center', family='monospace')

            plt.tight_layout()
            pdf.savefig(fig)
            plt.close()

            # ãƒšãƒ¼ã‚¸2: Box plotæ¯”è¼ƒ
            fig, ax = plt.subplots(figsize=(11, 8.5))
            data = [self.results[key] for key in self.results.keys()]
            ax.boxplot(data, labels=list(self.results.keys()))
            ax.set_ylabel('Diameter (pixels)')
            ax.set_title('Particle Size Distribution Comparison')
            ax.grid(True, alpha=0.3, axis='y')

            pdf.savefig(fig)
            plt.close()

        print(f&quot;Report saved to {filename}&quot;)

# ãƒ‡ãƒ¢å®Ÿè¡Œ
if __name__ == &quot;__main__&quot;:
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)

    image_dict = {}
    for sample_id, mean_size in [('Sample_A', 25),
                                   ('Sample_B', 35),
                                   ('Sample_C', 30)]:
        images = []
        for _ in range(10):
            img = np.zeros((512, 512), dtype=np.uint8)
            num_particles = np.random.randint(30, 50)

            for _ in range(num_particles):
                x = np.random.randint(30, 482)
                y = np.random.randint(30, 482)
                r = int(np.random.normal(mean_size, 5))
                r = max(10, min(40, r))
                cv2.circle(img, (x, y), r, 200, -1)

            noise = np.random.normal(0, 25, img.shape)
            img = np.clip(img + noise, 0, 255).astype(np.uint8)
            images.append(img)

        image_dict[sample_id] = images

    # ãƒãƒƒãƒè§£æ
    analyzer = BatchSEMAnalyzer()
    analyzer.batch_analyze(image_dict)

    # çµ±è¨ˆæ¯”è¼ƒ
    analyzer.statistical_comparison()

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    analyzer.generate_report('sem_comparison_report.pdf')

    print(&quot;\n=== ã‚µãƒ³ãƒ—ãƒ«åˆ¥çµ±è¨ˆ ===&quot;)
    for sample_id, diameters in analyzer.results.items():
        print(f&quot;{sample_id}:&quot;)
        print(f&quot;  ç²’å­æ•°: {len(diameters)}&quot;)
        print(f&quot;  å¹³å‡: {diameters.mean():.2f} Â± {diameters.std():.2f}&quot;)
</code></pre>


**çµæœä¾‹**:

<pre><code>=== åˆ†æ•£åˆ†æï¼ˆANOVAï¼‰===
Fçµ±è¨ˆé‡: 124.567
på€¤: 0.0001
çµè«–: ã‚µãƒ³ãƒ—ãƒ«é–“ã«æœ‰æ„å·®ã‚ã‚Šï¼ˆp &lt; 0.05ï¼‰

=== ã‚µãƒ³ãƒ—ãƒ«åˆ¥çµ±è¨ˆ ===
Sample_A:
  ç²’å­æ•°: 423
  å¹³å‡: 25.12 Â± 4.89
Sample_B:
  ç²’å­æ•°: 398
  å¹³å‡: 35.34 Â± 5.23
Sample_C:
  ç²’å­æ•°: 415
  å¹³å‡: 30.05 Â± 4.76

Report saved to sem_comparison_report.pdf
</code></pre>


**è©³ç´°ãªè§£èª¬**:
1. **é©å¿œçš„å‰å‡¦ç†**: å„ç”»åƒã®ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆã¨ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã‚’è©•ä¾¡ã—ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è‡ªå‹•èª¿æ•´
2. **çµ±è¨ˆæ¤œå®š**: ANOVAã«ã‚ˆã‚Š3ç¾¤é–“ã®ç²’å¾„å·®ã‚’å®šé‡è©•ä¾¡
3. **PDFå‡ºåŠ›**: è¤‡æ•°ãƒšãƒ¼ã‚¸ã®ãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆï¼ˆè«–æ–‡ãƒ»å ±å‘Šæ›¸ã«ç›´æ¥ä½¿ç”¨å¯èƒ½ï¼‰

**è¿½åŠ ã®æ¤œè¨äº‹é …**:
- Tukey HSDæ¤œå®šã«ã‚ˆã‚‹å¤šé‡æ¯”è¼ƒï¼ˆã©ã®ãƒšã‚¢é–“ã«æœ‰æ„å·®ãŒã‚ã‚‹ã‹ï¼‰
- ç²’å¾„åˆ†å¸ƒã®å½¢çŠ¶æ¯”è¼ƒï¼ˆæ­ªåº¦ã€å°–åº¦ï¼‰
- æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ç”»åƒå“è³ªè©•ä¾¡ï¼ˆä¸è‰¯ç”»åƒã®è‡ªå‹•é™¤å¤–ï¼‰

</details>

<hr />
<h2>å‚è€ƒæ–‡çŒ®</h2>
<ol>
<li>
<p>Bradski, G., &amp; Kaehler, A. (2008). "Learning OpenCV: Computer Vision with the OpenCV Library." O'Reilly Media. ISBN: 978-0596516130</p>
</li>
<li>
<p>van der Walt, S. et al. (2014). "scikit-image: image processing in Python." <em>PeerJ</em>, 2, e453. DOI: <a href="https://doi.org/10.7717/peerj.453">10.7717/peerj.453</a></p>
</li>
<li>
<p>Beucher, S., &amp; Meyer, F. (1993). "The morphological approach to segmentation: the watershed transformation." <em>Mathematical Morphology in Image Processing</em>, 433-481.</p>
</li>
<li>
<p>Simonyan, K., &amp; Zisserman, A. (2015). "Very Deep Convolutional Networks for Large-Scale Image Recognition." <em>ICLR 2015</em>. arXiv: <a href="https://arxiv.org/abs/1409.1556">1409.1556</a></p>
</li>
<li>
<p>OpenCV Documentation: Image Processing. URL: <a href="https://docs.opencv.org/4.x/d2/d96/tutorial_py_table_of_contents_imgproc.html">https://docs.opencv.org/4.x/d2/d96/tutorial_py_table_of_contents_imgproc.html</a></p>
</li>
<li>
<p>EMPIAR (Electron Microscopy Public Image Archive). URL: <a href="https://www.ebi.ac.uk/empiar/">https://www.ebi.ac.uk/empiar/</a></p>
</li>
<li>
<p>NanoMine Database. URL: <a href="https://materialsmine.org/">https://materialsmine.org/</a></p>
</li>
</ol>
<hr />
<h2>ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³</h2>
<h3>å‰ã®ç« </h3>
<p><strong><a href="./chapter-2.html">ç¬¬2ç« :ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿è§£æ â†</a></strong></p>
<h3>æ¬¡ã®ç« </h3>
<p><strong><a href="./chapter-4.html">ç¬¬4ç« ï¼šæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆè§£æ â†’</a></strong></p>
<h3>ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</h3>
<p><strong><a href="./index.html">â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a></strong></p>
<hr />
<h2>è‘—è€…æƒ…å ±</h2>
<p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team
<strong>ä½œæˆæ—¥</strong>: 2025-10-17
<strong>æ›´æ–°æ—¥</strong>: 2025-10-19
<strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.1</p>
<p><strong>æ›´æ–°å±¥æ­´</strong>:
- 2025-10-19: v1.1 ãƒ‡ãƒ¼ã‚¿ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã€å®Ÿè·µçš„ãªè½ã¨ã—ç©´ã€ã‚¹ã‚­ãƒ«ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆè¿½åŠ 
- 2025-10-17: v1.0 åˆç‰ˆå…¬é–‹</p>
<p><strong>ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯</strong>:
- GitHub Issues: [ãƒªãƒã‚¸ãƒˆãƒªURL]/issues
- Email: yusuke.hashimoto.b8@tohoku.ac.jp</p>
<p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
<hr />
<p><strong>æ¬¡ã®ç« ã§å­¦ç¿’ã‚’ç¶šã‘ã¾ã—ã‚‡ã†ï¼</strong></p><div class="navigation">
    <a href="chapter-2.html" class="nav-button">â† å‰ã®ç« </a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter-4.html" class="nav-button">æ¬¡ã®ç«  â†’</a>
</div>
    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.1 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
