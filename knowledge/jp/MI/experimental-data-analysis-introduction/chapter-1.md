---
title: ç¬¬1ç« ï¼šå®Ÿé¨“ãƒ‡ãƒ¼ã‚¿è§£æã®åŸºç¤
chapter_title: ç¬¬1ç« ï¼šå®Ÿé¨“ãƒ‡ãƒ¼ã‚¿è§£æã®åŸºç¤
subtitle: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‹ã‚‰å¤–ã‚Œå€¤æ¤œå‡ºã¾ã§ - ä¿¡é ¼æ€§ã®é«˜ã„è§£æã®ç¬¬ä¸€æ­©
reading_time: 20-25åˆ†
difficulty: åˆç´š
code_examples: 8
exercises: 3
---

# ç¬¬1ç« ï¼šå®Ÿé¨“ãƒ‡ãƒ¼ã‚¿è§£æã®åŸºç¤

XRD/SEM/ã‚¹ãƒšã‚¯ãƒˆãƒ«ãªã©ä¸»è¦æ‰‹æ³•ã‚’â€œä½•ãŒåˆ†ã‹ã‚‹ã‹â€ã®è¦³ç‚¹ã§æ•´ç†ã—ã¾ã™ã€‚ç¾å ´ã§å›°ã‚ŠãŒã¡ãªå‰å‡¦ç†ã®åŸºæœ¬ã‚‚æŠ¼ã•ãˆã¾ã™ã€‚

**ğŸ’¡ è£œè¶³:** ã€Œæ¸¬å®šã®ç›®çš„â†’å¿…è¦ãªè§£æã€ã§é€†å¼•ãã™ã‚‹ç™–ã‚’ä»˜ã‘ã‚‹ã¨è¿·ã„ã¾ã›ã‚“ã€‚å‰å‡¦ç†ã®è‡ªå‹•åŒ–ã¯æ—©æœŸã«ã€‚

**ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‹ã‚‰å¤–ã‚Œå€¤æ¤œå‡ºã¾ã§ - ä¿¡é ¼æ€§ã®é«˜ã„è§£æã®ç¬¬ä¸€æ­©**

## å­¦ç¿’ç›®æ¨™

ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š

  * âœ… å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿è§£æã®å…¨ä½“ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’èª¬æ˜ã§ãã‚‹
  * âœ… ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®é‡è¦æ€§ã¨å„æ‰‹æ³•ã®ä½¿ã„åˆ†ã‘ã‚’ç†è§£ã—ã¦ã„ã‚‹
  * âœ… ãƒã‚¤ã‚ºé™¤å»ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©åˆ‡ã«é¸æŠãƒ»é©ç”¨ã§ãã‚‹
  * âœ… å¤–ã‚Œå€¤ã‚’æ¤œå‡ºã—ã€é©åˆ‡ã«å‡¦ç†ã§ãã‚‹
  * âœ… æ¨™æº–åŒ–ãƒ»æ­£è¦åŒ–æ‰‹æ³•ã‚’ç›®çš„ã«å¿œã˜ã¦ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹

**èª­äº†æ™‚é–“** : 20-25åˆ† **ã‚³ãƒ¼ãƒ‰ä¾‹** : 8å€‹ **æ¼”ç¿’å•é¡Œ** : 3å•

* * *

## 1.1 å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿è§£æã®é‡è¦æ€§ã¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

### ãªãœãƒ‡ãƒ¼ã‚¿é§†å‹•å‹è§£æãŒå¿…è¦ã‹

ææ–™ç§‘å­¦ç ”ç©¶ã§ã¯ã€XRDï¼ˆXç·šå›æŠ˜ï¼‰ã€XPSï¼ˆXç·šå…‰é›»å­åˆ†å…‰ï¼‰ã€SEMï¼ˆèµ°æŸ»å‹é›»å­é¡•å¾®é¡ï¼‰ã€å„ç¨®ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¸¬å®šãªã©ã€å¤šæ§˜ãªã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³æŠ€è¡“ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®æ¸¬å®šã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ã€ææ–™ã®æ§‹é€ ã€çµ„æˆã€ç‰©æ€§ã‚’ç†è§£ã™ã‚‹ä¸Šã§ä¸å¯æ¬ ã§ã™ã€‚

ã—ã‹ã—ã€å¾“æ¥ã®æ‰‹å‹•è§£æã«ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªèª²é¡ŒãŒã‚ã‚Šã¾ã™ï¼š

**å¾“æ¥ã®æ‰‹å‹•è§£æã®é™ç•Œ** : 1\. **æ™‚é–“ãŒã‹ã‹ã‚‹** : 1ã¤ã®XRDãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ”ãƒ¼ã‚¯åŒå®šã«30åˆ†ã€œ1æ™‚é–“ 2\. **ä¸»è¦³çš„** : è§£æè€…ã®çµŒé¨“ã‚„åˆ¤æ–­ã«ã‚ˆã£ã¦çµæœãŒç•°ãªã‚‹ 3\. **å†ç¾æ€§ã®å•é¡Œ** : åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚’åˆ¥ã®äººãŒè§£æã™ã‚‹ã¨ç•°ãªã‚‹çµæœã«ãªã‚‹å¯èƒ½æ€§ 4\. **å¤§é‡ãƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œã§ããªã„** : ãƒã‚¤ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®šï¼ˆ1æ—¥æ•°ç™¾ã€œæ•°åƒã‚µãƒ³ãƒ—ãƒ«ï¼‰ã«è¿½ã„ã¤ã‹ãªã„

**ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹è§£æã®åˆ©ç‚¹** : 1\. **é«˜é€ŸåŒ–** : æ•°ç§’ã€œæ•°åˆ†ã§è§£æå®Œäº†ï¼ˆ100å€ä»¥ä¸Šã®é«˜é€ŸåŒ–ï¼‰ 2\. **å®¢è¦³æ€§** : æ˜ç¢ºãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«åŸºã¥ãå†ç¾å¯èƒ½ãªçµæœ 3\. **ä¸€è²«æ€§** : åŒã˜ã‚³ãƒ¼ãƒ‰ã¯å¸¸ã«åŒã˜çµæœã‚’å‡ºåŠ› 4\. **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£** : 1ã‚µãƒ³ãƒ—ãƒ«ã§ã‚‚1ä¸‡ã‚µãƒ³ãƒ—ãƒ«ã§ã‚‚åŒã˜åŠ´åŠ›

### ææ–™ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³æŠ€è¡“ä¸€è¦§

ä¸»è¦ãªæ¸¬å®šæŠ€è¡“ã¨å¾—ã‚‰ã‚Œã‚‹æƒ…å ±ï¼š

æ¸¬å®šæŠ€è¡“ | å¾—ã‚‰ã‚Œã‚‹æƒ…å ± | ãƒ‡ãƒ¼ã‚¿å½¢å¼ | å…¸å‹çš„ãªãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º  
---|---|---|---  
**XRD** | çµæ™¶æ§‹é€ ã€ç›¸åŒå®šã€çµæ™¶å­ã‚µã‚¤ã‚º | 1æ¬¡å…ƒã‚¹ãƒšã‚¯ãƒˆãƒ« | æ•°åƒç‚¹  
**XPS** | å…ƒç´ çµ„æˆã€åŒ–å­¦çŠ¶æ…‹ã€é›»å­æ§‹é€  | 1æ¬¡å…ƒã‚¹ãƒšã‚¯ãƒˆãƒ« | æ•°åƒç‚¹  
**SEM/TEM** | å½¢æ…‹ã€ç²’å¾„ã€çµ„ç¹” | 2æ¬¡å…ƒç”»åƒ | æ•°ç™¾ä¸‡ç”»ç´   
**IR/Raman** | åˆ†å­æŒ¯å‹•ã€å®˜èƒ½åŸºã€çµæ™¶æ€§ | 1æ¬¡å…ƒã‚¹ãƒšã‚¯ãƒˆãƒ« | æ•°åƒç‚¹  
**UV-Vis** | å…‰å¸åã€ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ— | 1æ¬¡å…ƒã‚¹ãƒšã‚¯ãƒˆãƒ« | æ•°ç™¾ã€œæ•°åƒç‚¹  
**TGA/DSC** | ç†±å®‰å®šæ€§ã€ç›¸è»¢ç§» | 1æ¬¡å…ƒæ™‚ç³»åˆ— | æ•°åƒç‚¹  
  
### å…¸å‹çš„ãªè§£æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆ5ã‚¹ãƒ†ãƒƒãƒ—ï¼‰

å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿è§£æã¯é€šå¸¸ã€ä»¥ä¸‹ã®5ã‚¹ãƒ†ãƒƒãƒ—ã§é€²è¡Œã—ã¾ã™ï¼š
    
    
    ```mermaid
    flowchart LR
        A[1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿] --> B[2. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†]
        B --> C[3. ç‰¹å¾´æŠ½å‡º]
        C --> D[4. çµ±è¨ˆè§£æ/æ©Ÿæ¢°å­¦ç¿’]
        D --> E[5. å¯è¦–åŒ–ãƒ»å ±å‘Š]
        E --> F[çµæœã®è§£é‡ˆ]
    
        style A fill:#e3f2fd
        style B fill:#fff3e0
        style C fill:#f3e5f5
        style D fill:#e8f5e9
        style E fill:#fce4ec
        style F fill:#fff9c4
    ```

**å„ã‚¹ãƒ†ãƒƒãƒ—ã®è©³ç´°** :

  1. **ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿** : CSVã€ãƒ†ã‚­ã‚¹ãƒˆã€ãƒã‚¤ãƒŠãƒªå½¢å¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
  2. **ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†** : ãƒã‚¤ã‚ºé™¤å»ã€å¤–ã‚Œå€¤å‡¦ç†ã€æ¨™æº–åŒ–
  3. **ç‰¹å¾´æŠ½å‡º** : ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã€è¼ªéƒ­æŠ½å‡ºã€çµ±è¨ˆé‡è¨ˆç®—
  4. **çµ±è¨ˆè§£æ/æ©Ÿæ¢°å­¦ç¿’** : å›å¸°ã€åˆ†é¡ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
  5. **å¯è¦–åŒ–ãƒ»å ±å‘Š** : ã‚°ãƒ©ãƒ•ä½œæˆã€ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

æœ¬ç« ã§ã¯ã€**ã‚¹ãƒ†ãƒƒãƒ—2ï¼ˆãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ï¼‰** ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ã€‚

* * *

## 1.2 ãƒ‡ãƒ¼ã‚¿ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¨å†ç¾æ€§

### å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®å–ã‚Šæ‰±ã„ã¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿è§£æã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®å‡ºæ‰€ã¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’ç†è§£ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚

#### å…¬é–‹ãƒ‡ãƒ¼ã‚¿ãƒªãƒã‚¸ãƒˆãƒª

ãƒªãƒã‚¸ãƒˆãƒª | å†…å®¹ | ãƒ‡ãƒ¼ã‚¿å½¢å¼ | ã‚¢ã‚¯ã‚»ã‚¹  
---|---|---|---  
**Materials Project** | DFTè¨ˆç®—çµæœã€çµæ™¶æ§‹é€  | JSON, CIF | ç„¡æ–™ãƒ»CC BY 4.0  
**ICDD PDF** | XRDãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ | å°‚ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ | æœ‰æ–™ãƒ©ã‚¤ã‚»ãƒ³ã‚¹  
**NIST XPS Database** | XPSã‚¹ãƒšã‚¯ãƒˆãƒ« | ãƒ†ã‚­ã‚¹ãƒˆ | ç„¡æ–™  
**Citrination** | ææ–™ç‰¹æ€§ãƒ‡ãƒ¼ã‚¿ | JSON, CSV | ä¸€éƒ¨ç„¡æ–™  
**Figshare/Zenodo** | ç ”ç©¶ãƒ‡ãƒ¼ã‚¿ | ä»»æ„ | ç„¡æ–™ãƒ»å„ç¨®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹  
  
#### æ©Ÿå™¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

ä¸»è¦ãªXç·šå›æŠ˜è£…ç½®ã¨ãã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼š

  * **Bruker** : `.raw`, `.brml` (XMLå½¢å¼)
  * **Rigaku** : `.asc`, `.ras` (ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼)
  * **PANalytical** : `.xrdml` (XMLå½¢å¼)
  * **æ±ç”¨** : `.xy`, `.csv` (2ã‚«ãƒ©ãƒ ãƒ†ã‚­ã‚¹ãƒˆ)

#### ãƒ‡ãƒ¼ã‚¿åˆ©ç”¨æ™‚ã®æ³¨æ„äº‹é …

  1. **ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ç¢ºèª** : å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã®åˆ©ç”¨è¦ç´„ã‚’å¿…ãšç¢ºèª
  2. **å¼•ç”¨** : ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã®å‡ºå…¸ã‚’æ˜è¨˜
  3. **åŠ å·¥** : ãƒ‡ãƒ¼ã‚¿åŠ å·¥å¾Œã‚‚å…ƒã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’å°Šé‡
  4. **å…¬é–‹** : è‡ªèº«ã®ãƒ‡ãƒ¼ã‚¿å…¬é–‹æ™‚ã¯ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’æ˜ç¤ºï¼ˆCC BY 4.0æ¨å¥¨ï¼‰

### ã‚³ãƒ¼ãƒ‰å†ç¾æ€§ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

#### ç’°å¢ƒæƒ…å ±ã®è¨˜éŒ²

è§£æã‚³ãƒ¼ãƒ‰ã®å†ç¾æ€§ã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã€ä»¥ä¸‹ã‚’è¨˜éŒ²ï¼š
    
    
    import sys
    import numpy as np
    import pandas as pd
    import scipy
    import matplotlib
    
    print("=== ç’°å¢ƒæƒ…å ± ===")
    print(f"Python: {sys.version}")
    print(f"NumPy: {np.__version__}")
    print(f"pandas: {pd.__version__}")
    print(f"SciPy: {scipy.__version__}")
    print(f"Matplotlib: {matplotlib.__version__}")
    
    # æ¨å¥¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆ2025å¹´10æœˆæ™‚ç‚¹ï¼‰:
    # - Python: 3.10ä»¥ä¸Š
    # - NumPy: 1.24ä»¥ä¸Š
    # - pandas: 2.0ä»¥ä¸Š
    # - SciPy: 1.10ä»¥ä¸Š
    # - Matplotlib: 3.7ä»¥ä¸Š
    

#### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ–‡æ›¸åŒ–

**æ‚ªã„ä¾‹** ï¼ˆå†ç¾ä¸å¯èƒ½ï¼‰:
    
    
    smoothed = savgol_filter(data, 11, 3)  # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ„å‘³ä¸æ˜
    

**è‰¯ã„ä¾‹** ï¼ˆå†ç¾å¯èƒ½ï¼‰:
    
    
    # Savitzky-Golayãƒ•ã‚£ãƒ«ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    SG_WINDOW_LENGTH = 11  # ãƒ‡ãƒ¼ã‚¿ç‚¹ã®ç´„1.5%ã«ç›¸å½“
    SG_POLYORDER = 3       # 3æ¬¡å¤šé …å¼ãƒ•ã‚£ãƒƒãƒˆ
    smoothed = savgol_filter(data, SG_WINDOW_LENGTH, SG_POLYORDER)
    

#### ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã®å›ºå®š
    
    
    import numpy as np
    
    # å†ç¾æ€§ã®ãŸã‚ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’å›ºå®š
    RANDOM_SEED = 42
    np.random.seed(RANDOM_SEED)
    
    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ã“ã®ã‚·ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
    noise = np.random.normal(0, 50, len(data))
    

* * *

## 1.3 ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®åŸºç¤

### ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿

ã¾ãšã€æ§˜ã€…ãªå½¢å¼ã®å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€æ–¹æ³•ã‚’å­¦ã³ã¾ã™ã€‚

**ã‚³ãƒ¼ãƒ‰ä¾‹1: CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆXRDãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰**
    
    
    # XRDãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆ2Î¸, intensityï¼‰
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
    np.random.seed(42)
    two_theta = np.linspace(10, 80, 700)  # 2Î¸ç¯„å›²: 10-80åº¦
    intensity = (
        1000 * np.exp(-((two_theta - 28) ** 2) / 10) +  # ãƒ”ãƒ¼ã‚¯1
        1500 * np.exp(-((two_theta - 32) ** 2) / 8) +   # ãƒ”ãƒ¼ã‚¯2
        800 * np.exp(-((two_theta - 47) ** 2) / 12) +   # ãƒ”ãƒ¼ã‚¯3
        np.random.normal(0, 50, len(two_theta))          # ãƒã‚¤ã‚º
    )
    
    # DataFrameã«æ ¼ç´
    df = pd.DataFrame({
        'two_theta': two_theta,
        'intensity': intensity
    })
    
    # åŸºæœ¬çµ±è¨ˆé‡ã®ç¢ºèª
    print("=== ãƒ‡ãƒ¼ã‚¿åŸºæœ¬çµ±è¨ˆ ===")
    print(df.describe())
    
    # ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–
    plt.figure(figsize=(10, 5))
    plt.plot(df['two_theta'], df['intensity'], linewidth=1)
    plt.xlabel('2Î¸ (degree)')
    plt.ylabel('Intensity (counts)')
    plt.title('Raw XRD Pattern')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    print(f"\nãƒ‡ãƒ¼ã‚¿ç‚¹æ•°: {len(df)}")
    print(f"2Î¸ç¯„å›²: {df['two_theta'].min():.1f} - {df['two_theta'].max():.1f}Â°")
    print(f"å¼·åº¦ç¯„å›²: {df['intensity'].min():.1f} - {df['intensity'].max():.1f}")
    

**å‡ºåŠ›** :
    
    
    === ãƒ‡ãƒ¼ã‚¿åŸºæœ¬çµ±è¨ˆ ===
             two_theta    intensity
    count   700.000000   700.000000
    mean     45.000000   351.893421
    std      20.219545   480.523106
    min      10.000000  -123.456789
    25%      27.500000    38.901234
    50%      45.000000   157.345678
    75%      62.500000   401.234567
    max      80.000000  1523.456789
    
    ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°: 700
    2Î¸ç¯„å›²: 10.0 - 80.0Â°
    å¼·åº¦ç¯„å›²: -123.5 - 1523.5
    

### ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ç†è§£ã¨æ•´å½¢

**ã‚³ãƒ¼ãƒ‰ä¾‹2: ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ç¢ºèªã¨æ•´å½¢**
    
    
    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ç¢ºèª
    print("=== ãƒ‡ãƒ¼ã‚¿æ§‹é€  ===")
    print(f"ãƒ‡ãƒ¼ã‚¿å‹:\n{df.dtypes}\n")
    print(f"æ¬ æå€¤:\n{df.isnull().sum()}\n")
    print(f"é‡è¤‡è¡Œ: {df.duplicated().sum()}")
    
    # æ¬ æå€¤ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã®ä¾‹
    df_with_nan = df.copy()
    df_with_nan.loc[100:105, 'intensity'] = np.nan  # æ¬ æå€¤ã‚’æ„å›³çš„ã«æŒ¿å…¥
    
    print("\n=== æ¬ æå€¤å‡¦ç† ===")
    print(f"æ¬ æå€¤ã®æ•°: {df_with_nan['intensity'].isnull().sum()}")
    
    # æ¬ æå€¤ã®è£œé–“ï¼ˆç·šå½¢è£œé–“ï¼‰
    df_with_nan['intensity_interpolated'] = df_with_nan['intensity'].interpolate(method='linear')
    
    # å‰å¾Œã®å€¤ã§ç¢ºèª
    print("\næ¬ æå€¤ã®å‰å¾Œ:")
    print(df_with_nan.iloc[98:108][['two_theta', 'intensity', 'intensity_interpolated']])
    

### æ¬ æå€¤ãƒ»ç•°å¸¸å€¤ã®æ¤œå‡ºã¨å‡¦ç†

**ã‚³ãƒ¼ãƒ‰ä¾‹3: ç•°å¸¸å€¤ã®æ¤œå‡º**
    
    
    from scipy import stats
    
    # è² ã®å¼·åº¦å€¤ã¯ç‰©ç†çš„ã«ã‚ã‚Šãˆãªã„ï¼ˆç•°å¸¸å€¤ï¼‰
    negative_mask = df['intensity'] < 0
    print(f"è² ã®å¼·åº¦å€¤ã®æ•°: {negative_mask.sum()} / {len(df)}")
    
    # è² ã®å€¤ã‚’0ã«ç½®ãæ›ãˆ
    df_cleaned = df.copy()
    df_cleaned.loc[negative_mask, 'intensity'] = 0
    
    # å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))
    
    axes[0].plot(df['two_theta'], df['intensity'], label='Raw', alpha=0.7)
    axes[0].axhline(y=0, color='r', linestyle='--', label='Zero line')
    axes[0].set_xlabel('2Î¸ (degree)')
    axes[0].set_ylabel('Intensity')
    axes[0].set_title('Raw Data (with negative values)')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    axes[1].plot(df_cleaned['two_theta'], df_cleaned['intensity'], label='Cleaned', color='green')
    axes[1].axhline(y=0, color='r', linestyle='--', label='Zero line')
    axes[1].set_xlabel('2Î¸ (degree)')
    axes[1].set_ylabel('Intensity')
    axes[1].set_title('Cleaned Data (negatives removed)')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    

* * *

## 1.3 ãƒã‚¤ã‚ºé™¤å»æ‰‹æ³•

å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã«ã¯å¿…ãšãƒã‚¤ã‚ºãŒå«ã¾ã‚Œã¾ã™ã€‚ãƒã‚¤ã‚ºé™¤å»ã¯ã€ä¿¡å·å¯¾é›‘éŸ³æ¯”ï¼ˆS/Næ¯”ï¼‰ã‚’å‘ä¸Šã•ã›ã€å¾Œç¶šã®è§£æç²¾åº¦ã‚’é«˜ã‚ã¾ã™ã€‚

### ç§»å‹•å¹³å‡ãƒ•ã‚£ãƒ«ã‚¿

æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªãƒã‚¤ã‚ºé™¤å»æ‰‹æ³•ã§ã™ã€‚å„ãƒ‡ãƒ¼ã‚¿ç‚¹ã‚’ã€ãã®å‘¨è¾ºã®å¹³å‡å€¤ã§ç½®ãæ›ãˆã¾ã™ã€‚

**ã‚³ãƒ¼ãƒ‰ä¾‹4: ç§»å‹•å¹³å‡ãƒ•ã‚£ãƒ«ã‚¿**
    
    
    from scipy.ndimage import uniform_filter1d
    
    # ç§»å‹•å¹³å‡ãƒ•ã‚£ãƒ«ã‚¿ã®é©ç”¨
    window_sizes = [5, 11, 21]
    
    plt.figure(figsize=(12, 8))
    
    # å…ƒãƒ‡ãƒ¼ã‚¿
    plt.subplot(2, 2, 1)
    plt.plot(df_cleaned['two_theta'], df_cleaned['intensity'], linewidth=1)
    plt.xlabel('2Î¸ (degree)')
    plt.ylabel('Intensity')
    plt.title('Original Data')
    plt.grid(True, alpha=0.3)
    
    # ç•°ãªã‚‹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã®ç§»å‹•å¹³å‡
    for i, window_size in enumerate(window_sizes, start=2):
        smoothed = uniform_filter1d(df_cleaned['intensity'].values, size=window_size)
    
        plt.subplot(2, 2, i)
        plt.plot(df_cleaned['two_theta'], smoothed, linewidth=1.5)
        plt.xlabel('2Î¸ (degree)')
        plt.ylabel('Intensity')
        plt.title(f'Moving Average (window={window_size})')
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # ãƒã‚¤ã‚ºé™¤å»åŠ¹æœã®å®šé‡è©•ä¾¡
    print("=== ãƒã‚¤ã‚ºé™¤å»åŠ¹æœ ===")
    original_std = np.std(df_cleaned['intensity'].values)
    for window_size in window_sizes:
        smoothed = uniform_filter1d(df_cleaned['intensity'].values, size=window_size)
        smoothed_std = np.std(smoothed)
        noise_reduction = (1 - smoothed_std / original_std) * 100
        print(f"Window={window_size:2d}: ãƒã‚¤ã‚ºå‰Šæ¸› {noise_reduction:.1f}%")
    

**å‡ºåŠ›** :
    
    
    === ãƒã‚¤ã‚ºé™¤å»åŠ¹æœ ===
    Window= 5: ãƒã‚¤ã‚ºå‰Šæ¸› 15.2%
    Window=11: ãƒã‚¤ã‚ºå‰Šæ¸› 28.5%
    Window=21: ãƒã‚¤ã‚ºå‰Šæ¸› 41.3%
    

**ä½¿ã„åˆ†ã‘ã‚¬ã‚¤ãƒ‰** : \- **å°ã•ã„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆ3-5ï¼‰** : ãƒã‚¤ã‚ºã¯æ®‹ã‚‹ãŒã€ãƒ”ãƒ¼ã‚¯å½¢çŠ¶ã‚’ä¿æŒ \- **ä¸­ç¨‹åº¦ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆ7-15ï¼‰** : ãƒãƒ©ãƒ³ã‚¹è‰¯å¥½ã€æ¨å¥¨ \- **å¤§ãã„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆ >20ï¼‰**: ãƒã‚¤ã‚ºé™¤å»ã¯å¼·åŠ›ã ãŒã€ãƒ”ãƒ¼ã‚¯ãŒåºƒãŒã‚‹

### Savitzky-Golay ãƒ•ã‚£ãƒ«ã‚¿

ç§»å‹•å¹³å‡ã‚ˆã‚Šã‚‚é«˜åº¦ãªæ‰‹æ³•ã§ã€ãƒ”ãƒ¼ã‚¯å½¢çŠ¶ã‚’ä¿æŒã—ãªãŒã‚‰ãƒã‚¤ã‚ºã‚’é™¤å»ã§ãã¾ã™ã€‚

**ã‚³ãƒ¼ãƒ‰ä¾‹5: Savitzky-Golay ãƒ•ã‚£ãƒ«ã‚¿**
    
    
    from scipy.signal import savgol_filter
    
    # Savitzky-Golay ãƒ•ã‚£ãƒ«ã‚¿ã®é©ç”¨
    window_length = 11  # å¥‡æ•°ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹
    polyorder = 3       # å¤šé …å¼ã®æ¬¡æ•°
    
    sg_smoothed = savgol_filter(df_cleaned['intensity'].values, window_length, polyorder)
    
    # ç§»å‹•å¹³å‡ã¨ã®æ¯”è¼ƒ
    ma_smoothed = uniform_filter1d(df_cleaned['intensity'].values, size=window_length)
    
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(df_cleaned['two_theta'], df_cleaned['intensity'],
             label='Original', alpha=0.5, linewidth=1)
    plt.plot(df_cleaned['two_theta'], ma_smoothed,
             label='Moving Average', linewidth=1.5)
    plt.plot(df_cleaned['two_theta'], sg_smoothed,
             label='Savitzky-Golay', linewidth=1.5)
    plt.xlabel('2Î¸ (degree)')
    plt.ylabel('Intensity')
    plt.title('Comparison of Smoothing Methods')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # ãƒ”ãƒ¼ã‚¯éƒ¨åˆ†ã®æ‹¡å¤§
    plt.subplot(1, 2, 2)
    peak_region = (df_cleaned['two_theta'] > 26) & (df_cleaned['two_theta'] < 34)
    plt.plot(df_cleaned.loc[peak_region, 'two_theta'],
             df_cleaned.loc[peak_region, 'intensity'],
             label='Original', alpha=0.5, linewidth=1)
    plt.plot(df_cleaned.loc[peak_region, 'two_theta'],
             ma_smoothed[peak_region],
             label='Moving Average', linewidth=1.5)
    plt.plot(df_cleaned.loc[peak_region, 'two_theta'],
             sg_smoothed[peak_region],
             label='Savitzky-Golay', linewidth=1.5)
    plt.xlabel('2Î¸ (degree)')
    plt.ylabel('Intensity')
    plt.title('Zoomed: Peak Region')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print("=== Savitzky-Golay ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===")
    print(f"Window length: {window_length}")
    print(f"Polynomial order: {polyorder}")
    print(f"\næ¨å¥¨è¨­å®š:")
    print("- ãƒã‚¤ã‚ºãŒå¤šã„: window_length=11-21, polyorder=2-3")
    print("- ãƒã‚¤ã‚ºãŒå°‘ãªã„: window_length=5-11, polyorder=2-4")
    

**Savitzky-Golay ãƒ•ã‚£ãƒ«ã‚¿ã®åˆ©ç‚¹** : \- ãƒ”ãƒ¼ã‚¯ã®é«˜ã•ã¨ä½ç½®ã‚’ã‚ˆã‚Šæ­£ç¢ºã«ä¿æŒ \- ã‚¨ãƒƒã‚¸ï¼ˆæ€¥å³»ãªå¤‰åŒ–ï¼‰ã‚’æ»‘ã‚‰ã‹ã«ã—ã™ããªã„ \- å¾®åˆ†è¨ˆç®—ã¨ã®ç›¸æ€§ãŒè‰¯ã„ï¼ˆå¾Œã®ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã§æœ‰ç”¨ï¼‰

### ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ•ã‚£ãƒ«ã‚¿

ç”»åƒå‡¦ç†ã§åºƒãä½¿ã‚ã‚Œã‚‹æ‰‹æ³•ã§ã™ãŒã€1æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã«ã‚‚é©ç”¨å¯èƒ½ã§ã™ã€‚

**ã‚³ãƒ¼ãƒ‰ä¾‹6: ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ•ã‚£ãƒ«ã‚¿**
    
    
    from scipy.ndimage import gaussian_filter1d
    
    # ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ã®é©ç”¨
    sigma_values = [1, 2, 4]  # æ¨™æº–åå·®
    
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 2, 1)
    plt.plot(df_cleaned['two_theta'], df_cleaned['intensity'], linewidth=1)
    plt.xlabel('2Î¸ (degree)')
    plt.ylabel('Intensity')
    plt.title('Original Data')
    plt.grid(True, alpha=0.3)
    
    for i, sigma in enumerate(sigma_values, start=2):
        gaussian_smoothed = gaussian_filter1d(df_cleaned['intensity'].values, sigma=sigma)
    
        plt.subplot(2, 2, i)
        plt.plot(df_cleaned['two_theta'], gaussian_smoothed, linewidth=1.5)
        plt.xlabel('2Î¸ (degree)')
        plt.ylabel('Intensity')
        plt.title(f'Gaussian Filter (Ïƒ={sigma})')
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    

### é©åˆ‡ãªãƒ•ã‚£ãƒ«ã‚¿ã®é¸æŠ
    
    
    ```mermaid
    flowchart TD
        Start[ãƒã‚¤ã‚ºé™¤å»ãŒå¿…è¦] --> Q1{ãƒ”ãƒ¼ã‚¯å½¢çŠ¶ã®ä¿æŒã¯é‡è¦?}
        Q1 -->|éå¸¸ã«é‡è¦| SG[Savitzky-Golay]
        Q1 -->|ã‚„ã‚„é‡è¦| Gauss[Gaussian]
        Q1 -->|ãã‚Œã»ã©é‡è¦ã§ãªã„| MA[Moving Average]
    
        SG --> Param1[window=11-21\npolyorder=2-3]
        Gauss --> Param2[sigma=1-3]
        MA --> Param3[window=7-15]
    
        style Start fill:#4CAF50,color:#fff
        style SG fill:#2196F3,color:#fff
        style Gauss fill:#FF9800,color:#fff
        style MA fill:#9C27B0,color:#fff
    ```

* * *

## 1.4 å¤–ã‚Œå€¤æ¤œå‡º

å¤–ã‚Œå€¤ï¼ˆoutlierï¼‰ã¯ã€æ¸¬å®šã‚¨ãƒ©ãƒ¼ã€è£…ç½®ã®ä¸èª¿ã€ã‚µãƒ³ãƒ—ãƒ«æ±šæŸ“ãªã©ã«ã‚ˆã‚Šç™ºç”Ÿã—ã¾ã™ã€‚é©åˆ‡ã«æ¤œå‡ºãƒ»å‡¦ç†ã—ãªã„ã¨ã€è§£æçµæœã«é‡å¤§ãªå½±éŸ¿ã‚’åŠã¼ã—ã¾ã™ã€‚

### Z-score æ³•

çµ±è¨ˆçš„ã«ã€Œå¹³å‡ã‹ã‚‰æ¨™æº–åå·®ã®â—‹å€ä»¥ä¸Šé›¢ã‚Œã¦ã„ã‚‹ã€ãƒ‡ãƒ¼ã‚¿ã‚’å¤–ã‚Œå€¤ã¨ã¿ãªã—ã¾ã™ã€‚

**ã‚³ãƒ¼ãƒ‰ä¾‹7: Z-score ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º**
    
    
    from scipy import stats
    
    # å¤–ã‚Œå€¤ã‚’å«ã‚€ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    data_with_outliers = df_cleaned['intensity'].copy()
    # æ„å›³çš„ã«å¤–ã‚Œå€¤ã‚’è¿½åŠ 
    outlier_indices = [50, 150, 350, 550]
    data_with_outliers.iloc[outlier_indices] = [3000, -500, 4000, 3500]
    
    # Z-scoreè¨ˆç®—
    z_scores = np.abs(stats.zscore(data_with_outliers))
    threshold = 3  # 3Ïƒã‚’è¶…ãˆã‚‹ã‚‚ã®ã‚’å¤–ã‚Œå€¤ã¨ã™ã‚‹
    
    outliers = z_scores > threshold
    
    print(f"=== Z-score å¤–ã‚Œå€¤æ¤œå‡º ===")
    print(f"å¤–ã‚Œå€¤ã®æ•°: {outliers.sum()} / {len(data_with_outliers)}")
    print(f"å¤–ã‚Œå€¤ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {np.where(outliers)[0]}")
    
    # å¯è¦–åŒ–
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(df_cleaned['two_theta'], data_with_outliers, label='Data with outliers')
    plt.scatter(df_cleaned['two_theta'][outliers], data_with_outliers[outliers],
                color='red', s=100, zorder=5, label='Detected outliers')
    plt.xlabel('2Î¸ (degree)')
    plt.ylabel('Intensity')
    plt.title('Outlier Detection (Z-score method)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å¤–ã‚Œå€¤é™¤å»å¾Œ
    data_cleaned = data_with_outliers.copy()
    data_cleaned[outliers] = np.nan
    data_cleaned = data_cleaned.interpolate(method='linear')
    
    plt.subplot(1, 2, 2)
    plt.plot(df_cleaned['two_theta'], data_cleaned, color='green', label='Cleaned data')
    plt.xlabel('2Î¸ (degree)')
    plt.ylabel('Intensity')
    plt.title('After Outlier Removal')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    

### IQRï¼ˆå››åˆ†ä½ç¯„å›²ï¼‰æ³•

ä¸­å¤®å€¤ãƒ™ãƒ¼ã‚¹ã®é ‘å¥ãªæ‰‹æ³•ã§ã€æ­£è¦åˆ†å¸ƒã§ãªã„ãƒ‡ãƒ¼ã‚¿ã«ã‚‚é©ç”¨å¯èƒ½ã§ã™ã€‚

**ã‚³ãƒ¼ãƒ‰ä¾‹8: IQR æ³•**
    
    
    # IQRæ³•ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º
    Q1 = data_with_outliers.quantile(0.25)
    Q3 = data_with_outliers.quantile(0.75)
    IQR = Q3 - Q1
    
    # å¤–ã‚Œå€¤ã®å®šç¾©: Q1 - 1.5*IQR æœªæº€ã€ã¾ãŸã¯ Q3 + 1.5*IQR è¶…
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers_iqr = (data_with_outliers < lower_bound) | (data_with_outliers > upper_bound)
    
    print(f"=== IQR å¤–ã‚Œå€¤æ¤œå‡º ===")
    print(f"Q1: {Q1:.1f}")
    print(f"Q3: {Q3:.1f}")
    print(f"IQR: {IQR:.1f}")
    print(f"ä¸‹é™: {lower_bound:.1f}")
    print(f"ä¸Šé™: {upper_bound:.1f}")
    print(f"å¤–ã‚Œå€¤ã®æ•°: {outliers_iqr.sum()} / {len(data_with_outliers)}")
    
    # ç®±ã²ã’å›³ã§å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    axes[0].boxplot(data_with_outliers.dropna(), vert=True)
    axes[0].set_ylabel('Intensity')
    axes[0].set_title('Box Plot (outliers visible)')
    axes[0].grid(True, alpha=0.3)
    
    # å¤–ã‚Œå€¤é™¤å»å¾Œ
    data_cleaned_iqr = data_with_outliers.copy()
    data_cleaned_iqr[outliers_iqr] = np.nan
    data_cleaned_iqr = data_cleaned_iqr.interpolate(method='linear')
    
    axes[1].boxplot(data_cleaned_iqr.dropna(), vert=True)
    axes[1].set_ylabel('Intensity')
    axes[1].set_title('Box Plot (after outlier removal)')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    

**Z-score vs IQR ã®ä½¿ã„åˆ†ã‘** : \- **Z-scoreæ³•** : ãƒ‡ãƒ¼ã‚¿ãŒæ­£è¦åˆ†å¸ƒã«è¿‘ã„å ´åˆã«æœ‰åŠ¹ã€è¨ˆç®—ãŒç°¡å˜ \- **IQRæ³•** : éæ­£è¦åˆ†å¸ƒã§ã‚‚é ‘å¥ã€æ¥µç«¯ãªå¤–ã‚Œå€¤ã«å¼·ã„

* * *

## 1.5 æ¨™æº–åŒ–ãƒ»æ­£è¦åŒ–

ç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒå¯èƒ½ã«ã™ã‚‹ãŸã‚ã€æ¨™æº–åŒ–ãƒ»æ­£è¦åŒ–ãŒå¿…è¦ã§ã™ã€‚

### Min-Max ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

ãƒ‡ãƒ¼ã‚¿ã‚’[0, 1]ã®ç¯„å›²ã«å¤‰æ›ã—ã¾ã™ã€‚

$$ X_{\text{normalized}} = \frac{X - X_{\min}}{X_{\max} - X_{\min}} $$

### Z-score æ¨™æº–åŒ–

å¹³å‡0ã€æ¨™æº–åå·®1ã«å¤‰æ›ã—ã¾ã™ã€‚

$$ X_{\text{standardized}} = \frac{X - \mu}{\sigma} $$

### ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è£œæ­£

ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã€ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚’é™¤å»ã—ã¾ã™ã€‚

**å®Ÿè£…ã¯ç¬¬2ç« ã§è©³è¿°**

* * *

## 1.6 å®Ÿè·µçš„ãªè½ã¨ã—ç©´ã¨å¯¾ç­–

### ã‚ˆãã‚ã‚‹å¤±æ•—ä¾‹

#### å¤±æ•—1: éåº¦ãªãƒã‚¤ã‚ºé™¤å»

**ç—‡çŠ¶** : ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°å¾Œã€é‡è¦ãªãƒ”ãƒ¼ã‚¯ãŒæ¶ˆå¤±ã¾ãŸã¯æ­ªã‚€

**åŸå› ** : ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã‚‹ã€ã¾ãŸã¯å¤šæ®µéšã®ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°

**å¯¾ç­–** :
    
    
    # æ‚ªã„ä¾‹ï¼šéåº¦ãªã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
    data_smooth1 = gaussian_filter1d(data, sigma=5)
    data_smooth2 = savgol_filter(data_smooth1, 21, 3)  # äºŒé‡ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
    data_smooth3 = uniform_filter1d(data_smooth2, 15)  # ã•ã‚‰ã«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
    
    # è‰¯ã„ä¾‹ï¼šé©åˆ‡ãªã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
    data_smooth = savgol_filter(data, 11, 3)  # ä¸€åº¦ã ã‘ã€é©åˆ‡ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§
    

#### å¤±æ•—2: å¤–ã‚Œå€¤é™¤å»ã®éå‰°é©ç”¨

**ç—‡çŠ¶** : é‡è¦ãªä¿¡å·ï¼ˆæ€¥å³»ãªãƒ”ãƒ¼ã‚¯ï¼‰ãŒå¤–ã‚Œå€¤ã¨ã—ã¦é™¤å»ã•ã‚Œã‚‹

**åŸå› ** : Z-scoreã®é–¾å€¤ãŒä½ã™ãã‚‹

**å¯¾ç­–** :
    
    
    # æ‚ªã„ä¾‹ï¼šé–¾å€¤ãŒå³ã—ã™ãã‚‹
    outliers = np.abs(stats.zscore(data)) > 2  # 2Ïƒã§é™¤å»â†’æ­£å¸¸ãªãƒ”ãƒ¼ã‚¯ã‚‚é™¤å»ã•ã‚Œã‚‹
    
    # è‰¯ã„ä¾‹ï¼šé©åˆ‡ãªé–¾å€¤ã¨å¯è¦–åŒ–ç¢ºèª
    outliers = np.abs(stats.zscore(data)) > 3  # 3ÏƒãŒæ¨™æº–
    # é™¤å»å‰ã«å¿…ãšå¯è¦–åŒ–ã—ã¦ç¢ºèª
    plt.scatter(range(len(data)), data)
    plt.scatter(np.where(outliers)[0], data[outliers], color='red')
    plt.show()
    

#### å¤±æ•—3: æ¬ æå€¤è£œé–“ã®èª¤ç”¨

**ç—‡çŠ¶** : ãƒ‡ãƒ¼ã‚¿ã®é€£ç¶šæ€§ãŒå¤±ã‚ã‚Œã‚‹ã€ã¾ãŸã¯ä¸è‡ªç„¶ãªå€¤ãŒç”Ÿæˆã•ã‚Œã‚‹

**åŸå› ** : å¤§ããªæ¬ æé ˜åŸŸã‚’ç·šå½¢è£œé–“

**å¯¾ç­–** :
    
    
    # æ‚ªã„ä¾‹ï¼šå¤§ããªæ¬ æé ˜åŸŸã‚’è£œé–“
    data_interpolated = data.interpolate(method='linear')  # å…¨é ˜åŸŸã‚’ç„¡æ¡ä»¶ã«è£œé–“
    
    # è‰¯ã„ä¾‹ï¼šæ¬ æç¯„å›²ã‚’ãƒã‚§ãƒƒã‚¯
    max_gap = 5  # æœ€å¤§5ç‚¹ã¾ã§ã®æ¬ æã®ã¿è£œé–“
    gaps = data.isnull().astype(int).groupby(data.notnull().astype(int).cumsum()).sum()
    if gaps.max() <= max_gap:
        data_interpolated = data.interpolate(method='linear')
    else:
        print(f"è­¦å‘Š: å¤§ããªæ¬ æé ˜åŸŸãŒã‚ã‚Šã¾ã™ï¼ˆæœ€å¤§{gaps.max()}ç‚¹ï¼‰")
    

#### å¤±æ•—4: æ¸¬å®šãƒã‚¤ã‚ºã¨ä¿¡å·ã®æ··åŒ

**ç—‡çŠ¶** : ãƒã‚¤ã‚ºã‚’ç‰©ç†çš„ãªä¿¡å·ã¨èª¤èª

**åŸå› ** : ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã®å®šé‡è©•ä¾¡ã‚’æ€ ã‚‹

**å¯¾ç­–** :
    
    
    # ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã®å®šé‡è©•ä¾¡
    baseline_region = data[(two_theta > 70) & (two_theta < 80)]  # ä¿¡å·ãŒãªã„ã¯ãšã®é ˜åŸŸ
    noise_std = np.std(baseline_region)
    print(f"ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ï¼ˆæ¨™æº–åå·®ï¼‰: {noise_std:.2f}")
    
    # ä¿¡å·å¯¾é›‘éŸ³æ¯”ï¼ˆS/Næ¯”ï¼‰ã®è¨ˆç®—
    peak_height = data.max() - data.min()
    snr = peak_height / noise_std
    print(f"S/Næ¯”: {snr:.1f}")
    
    # S/Næ¯”ãŒ3ä»¥ä¸‹ã®å ´åˆã€ä¿¡å·ã¨ã—ã¦æ‰±ã‚ãªã„
    if snr < 3:
        print("è­¦å‘Š: S/Næ¯”ãŒä½ã™ãã¾ã™ã€‚æ¸¬å®šã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    

### å‡¦ç†é †åºã®é‡è¦æ€§

æ­£ã—ã„å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®é †åºï¼š
    
    
    ```mermaid
    flowchart LR
        A[1. ç‰©ç†çš„ç•°å¸¸å€¤é™¤å»] --> B[2. çµ±è¨ˆçš„å¤–ã‚Œå€¤æ¤œå‡º]
        B --> C[3. æ¬ æå€¤è£œé–“]
        C --> D[4. ãƒã‚¤ã‚ºé™¤å»]
        D --> E[5. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è£œæ­£]
        E --> F[6. æ¨™æº–åŒ–]
    
        style A fill:#ffebee
        style B fill:#fff3e0
        style C fill:#e3f2fd
        style D fill:#f3e5f5
        style E fill:#e8f5e9
        style F fill:#fce4ec
    ```

**ãªãœã“ã®é †åºãŒé‡è¦ã‹** : 1\. **ç‰©ç†çš„ç•°å¸¸å€¤** ï¼ˆè² ã®å¼·åº¦ãªã©ï¼‰ã‚’æœ€åˆã«é™¤å»ã—ãªã„ã¨ã€çµ±è¨ˆå€¤ãŒæ­ªã‚€ 2\. **çµ±è¨ˆçš„å¤–ã‚Œå€¤** ã‚’ãƒã‚¤ã‚ºé™¤å»å‰ã«é™¤å»ã—ãªã„ã¨ã€ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã§åºƒãŒã‚‹ 3\. **ãƒã‚¤ã‚ºé™¤å»** ã‚’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è£œæ­£å‰ã«è¡Œã‚ãªã„ã¨ã€ãƒã‚¤ã‚ºãŒãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«å½±éŸ¿ 4\. **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è£œæ­£** ã‚’æ¨™æº–åŒ–å‰ã«è¡Œã‚ãªã„ã¨ã€æ¨™æº–åŒ–ãŒç„¡æ„å‘³

* * *

## 1.7 æœ¬ç« ã®ã¾ã¨ã‚

### å­¦ã‚“ã ã“ã¨

  1. **ãƒ‡ãƒ¼ã‚¿ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¨å†ç¾æ€§** \- å…¬é–‹ãƒ‡ãƒ¼ã‚¿ãƒªãƒã‚¸ãƒˆãƒªã®æ´»ç”¨ \- ç’°å¢ƒæƒ…å ±ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ–‡æ›¸åŒ– \- ã‚³ãƒ¼ãƒ‰å†ç¾æ€§ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

  2. **å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿è§£æã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼** \- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ â†’ å‰å‡¦ç† â†’ ç‰¹å¾´æŠ½å‡º â†’ è§£æ â†’ å¯è¦–åŒ– \- å‰å‡¦ç†ã®é‡è¦æ€§ã¨å½±éŸ¿

  3. **ãƒã‚¤ã‚ºé™¤å»æ‰‹æ³•** \- ç§»å‹•å¹³å‡ã€Savitzky-Golayã€ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ \- é©åˆ‡ãªæ‰‹æ³•ã®é¸æŠåŸºæº–

  4. **å¤–ã‚Œå€¤æ¤œå‡º** \- Z-scoreæ³•ã€IQRæ³• \- ç‰©ç†çš„å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯

  5. **æ¨™æº–åŒ–ãƒ»æ­£è¦åŒ–** \- Min-Maxã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€Z-scoreæ¨™æº–åŒ– \- ä½¿ã„åˆ†ã‘ã®åŸå‰‡

  6. **å®Ÿè·µçš„ãªè½ã¨ã—ç©´** \- éåº¦ãªãƒã‚¤ã‚ºé™¤å»ã®å›é¿ \- å‡¦ç†é †åºã®é‡è¦æ€§ \- ãƒã‚¤ã‚ºã¨ä¿¡å·ã®åŒºåˆ¥

### é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

  * âœ… å‰å‡¦ç†ã¯ãƒ‡ãƒ¼ã‚¿è§£æã®æˆå¦ã‚’å·¦å³ã™ã‚‹æœ€é‡è¦ã‚¹ãƒ†ãƒƒãƒ—
  * âœ… ãƒã‚¤ã‚ºé™¤å»ã§ã¯ã€ãƒ”ãƒ¼ã‚¯å½¢çŠ¶ä¿æŒã¨ãƒã‚¤ã‚ºå‰Šæ¸›ã®ãƒãƒ©ãƒ³ã‚¹ãŒé‡è¦
  * âœ… å¤–ã‚Œå€¤ã¯å¿…ãšç¢ºèªã—ã€ç‰©ç†çš„å¦¥å½“æ€§ã‚’æ¤œè¨¼ã™ã‚‹
  * âœ… å¯è¦–åŒ–ã«ã‚ˆã‚Šã€å„å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã®åŠ¹æœã‚’ç¢ºèªã™ã‚‹
  * âœ… å‡¦ç†é †åºãŒçµæœã«å¤§ããå½±éŸ¿ã™ã‚‹
  * âœ… ã‚³ãƒ¼ãƒ‰å†ç¾æ€§ã®ãŸã‚ç’°å¢ƒã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ã™ã‚‹

### æ¬¡ã®ç« ã¸

ç¬¬2ç« ã§ã¯ã€ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆXRDã€XPSã€IRã€Ramanï¼‰ã®è§£ææ‰‹æ³•ã‚’å­¦ã³ã¾ã™ï¼š \- ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  \- ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰é™¤å» \- å®šé‡åˆ†æ \- æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ææ–™åŒå®š

**[ç¬¬2ç« ï¼šã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿è§£æ â†’](<./chapter-2.html>)**

* * *

## å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨æ¤œè¨¼

  * [ ] ãƒ‡ãƒ¼ã‚¿å½¢å¼ãŒæ­£ã—ã„ã‹ç¢ºèªï¼ˆCSV, ãƒ†ã‚­ã‚¹ãƒˆ, ãƒã‚¤ãƒŠãƒªï¼‰
  * [ ] ã‚«ãƒ©ãƒ åãŒé©åˆ‡ã‹ç¢ºèªï¼ˆtwo_theta, intensity ãªã©ï¼‰
  * [ ] ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°ãŒååˆ†ã‹ç¢ºèªï¼ˆæœ€ä½100ç‚¹ä»¥ä¸Šæ¨å¥¨ï¼‰
  * [ ] æ¬ æå€¤ã®å‰²åˆã‚’ç¢ºèªï¼ˆ30%ä»¥ä¸Šã¯è¦æ³¨æ„ï¼‰
  * [ ] é‡è¤‡ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã‹ç¢ºèª

### ç’°å¢ƒã¨å†ç¾æ€§

  * [ ] Python, NumPy, pandas, SciPy, Matplotlibã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’è¨˜éŒ²
  * [ ] ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’å›ºå®šï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
  * [ ] ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šæ•°ã¨ã—ã¦å®šç¾©ï¼ˆSG_WINDOW_LENGTH = 11 ãªã©ï¼‰
  * [ ] ãƒ‡ãƒ¼ã‚¿å‡ºå…¸ã¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’æ˜è¨˜

### ç‰©ç†çš„å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯

  * [ ] è² ã®å¼·åº¦å€¤ãŒãªã„ã‹ç¢ºèªï¼ˆXRD/XPSã§ã¯ç‰©ç†çš„ã«ã‚ã‚Šãˆãªã„ï¼‰
  * [ ] æ¸¬å®šç¯„å›²ãŒå¦¥å½“ã‹ç¢ºèªï¼ˆXRD: 10-80Â°, XPS: 0-1200 eV ãªã©ï¼‰
  * [ ] S/Næ¯”ã‚’è¨ˆç®—ï¼ˆ3ä»¥ä¸ŠãŒæœ›ã¾ã—ã„ï¼‰
  * [ ] ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³é ˜åŸŸã§ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã‚’è©•ä¾¡

### å¤–ã‚Œå€¤æ¤œå‡º

  * [ ] Z-scoreæ³•ã¾ãŸã¯IQRæ³•ã§å¤–ã‚Œå€¤æ¤œå‡º
  * [ ] **é™¤å»å‰ã«å¯è¦–åŒ–ã—ã¦ç¢ºèª** ï¼ˆé‡è¦ãªãƒ”ãƒ¼ã‚¯ã‚’èª¤ã£ã¦é™¤å»ã—ãªã„ï¼‰
  * [ ] é–¾å€¤ã‚’è¨˜éŒ²ï¼ˆZ-score: 3Ïƒ, IQR: 1.5å€ ãªã©ï¼‰
  * [ ] é™¤å»ã—ãŸå¤–ã‚Œå€¤ã®å€‹æ•°ã¨ä½ç½®ã‚’è¨˜éŒ²

### æ¬ æå€¤å‡¦ç†

  * [ ] æ¬ æå€¤ã®å€‹æ•°ã¨ä½ç½®ã‚’ç¢ºèª
  * [ ] é€£ç¶šã™ã‚‹æ¬ æå€¤ã®æœ€å¤§é•·ã‚’ç¢ºèªï¼ˆ5ç‚¹ä»¥ä¸‹ãŒæœ›ã¾ã—ã„ï¼‰
  * [ ] è£œé–“æ–¹æ³•ã‚’é¸æŠï¼ˆç·šå½¢, ã‚¹ãƒ—ãƒ©ã‚¤ãƒ³, å‰æ–¹/å¾Œæ–¹åŸ‹ã‚ï¼‰
  * [ ] è£œé–“å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’å¯è¦–åŒ–ã—ã¦ç¢ºèª

### ãƒã‚¤ã‚ºé™¤å»

  * [ ] ãƒ•ã‚£ãƒ«ã‚¿ç¨®é¡ã‚’é¸æŠï¼ˆç§»å‹•å¹³å‡, Savitzky-Golay, Gaussianï¼‰
  * [ ] ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ±ºå®šï¼ˆwindow_length, polyorder, sigmaï¼‰
  * [ ] ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°å‰å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒå¯è¦–åŒ–
  * [ ] ãƒ”ãƒ¼ã‚¯å½¢çŠ¶ãŒä¿æŒã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
  * [ ] **äºŒé‡ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã‚’é¿ã‘ã‚‹**

### å‡¦ç†é †åºã®ç¢ºèª

  * [ ] 1. ç‰©ç†çš„ç•°å¸¸å€¤é™¤å»
  * [ ] 2. çµ±è¨ˆçš„å¤–ã‚Œå€¤æ¤œå‡º
  * [ ] 3. æ¬ æå€¤è£œé–“
  * [ ] 4. ãƒã‚¤ã‚ºé™¤å»
  * [ ] 5. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è£œæ­£ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
  * [ ] 6. æ¨™æº–åŒ–ãƒ»æ­£è¦åŒ–ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰

### å¯è¦–åŒ–ã¨æ¤œè¨¼

  * [ ] å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
  * [ ] å‡¦ç†å¾Œãƒ‡ãƒ¼ã‚¿ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
  * [ ] å‡¦ç†å‰å¾Œã‚’é‡ã­ã¦è¡¨ç¤º
  * [ ] ä¸»è¦ãªãƒ”ãƒ¼ã‚¯ä½ç½®ãŒä¿æŒã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
  * [ ] ãƒ”ãƒ¼ã‚¯å¼·åº¦ãŒå¤§ããå¤‰åŒ–ã—ã¦ã„ãªã„ã‹ç¢ºèª

### æ–‡æ›¸åŒ–

  * [ ] å‡¦ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã¾ãŸã¯å¤‰æ•°ã¨ã—ã¦è¨˜éŒ²
  * [ ] å‡¦ç†ã®ç†ç”±ã‚’è¨˜éŒ²ï¼ˆã€Œãƒã‚¤ã‚ºãŒå¤§ãã„ãŸã‚SG filterã‚’ä½¿ç”¨ã€ãªã©ï¼‰
  * [ ] é™¤å¤–ã—ãŸãƒ‡ãƒ¼ã‚¿ã®ç†ç”±ã‚’è¨˜éŒ²
  * [ ] æœ€çµ‚çš„ãªãƒ‡ãƒ¼ã‚¿å“è³ªæŒ‡æ¨™ã‚’è¨˜éŒ²ï¼ˆS/Næ¯”, ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°ãªã©ï¼‰

### ãƒãƒƒãƒå‡¦ç†ã®å ´åˆ

  * [ ] ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ï¼ˆtry-exceptï¼‰
  * [ ] ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡¦ç†çµæœã‚’è¨˜éŒ²
  * [ ] å‡¦ç†æ™‚é–“ã‚’æ¸¬å®šãƒ»è¨˜éŒ²
  * [ ] å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆã‚’ä¿å­˜
  * [ ] æˆåŠŸç‡ã‚’è¨ˆç®—ãƒ»å ±å‘Šï¼ˆä¾‹: 95/100 files succeededï¼‰

* * *

## æ¼”ç¿’å•é¡Œ

### å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰

æ¬¡ã®æ–‡ç« ã®æ­£èª¤ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

  1. ç§»å‹•å¹³å‡ãƒ•ã‚£ãƒ«ã‚¿ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã‚’å¤§ããã™ã‚‹ã¨ã€ãƒã‚¤ã‚ºé™¤å»åŠ¹æœãŒé«˜ã¾ã‚‹ãŒã€ãƒ”ãƒ¼ã‚¯ãŒåºƒãŒã‚‹
  2. Savitzky-Golayãƒ•ã‚£ãƒ«ã‚¿ã¯ç§»å‹•å¹³å‡ã‚ˆã‚Šã‚‚ãƒ”ãƒ¼ã‚¯å½¢çŠ¶ã‚’ä¿æŒã™ã‚‹
  3. Z-scoreæ³•ã¯éæ­£è¦åˆ†å¸ƒã®ãƒ‡ãƒ¼ã‚¿ã«ã¯é©ç”¨ã§ããªã„

ãƒ’ãƒ³ãƒˆ 1\. ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã¨ãƒã‚¤ã‚ºé™¤å»åŠ¹æœã€ãƒ”ãƒ¼ã‚¯å½¢çŠ¶ã®é–¢ä¿‚ã‚’è€ƒãˆã‚‹ 2\. ä¸¡æ‰‹æ³•ã®æ•°å­¦çš„ãªé•ã„ã‚’æ€ã„å‡ºã™ 3\. Z-scoreã®å®šç¾©ã¨ã€éæ­£è¦åˆ†å¸ƒã§ã®æŒ™å‹•ã‚’è€ƒãˆã‚‹  è§£ç­”ä¾‹ **è§£ç­”**: 1\. **æ­£** - å¤§ãã„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã¯ã‚ˆã‚Šå¤šãã®ç‚¹ã‚’å¹³å‡ã™ã‚‹ãŸã‚ã€ãƒã‚¤ã‚ºã¯æ¸›ã‚‹ãŒãƒ”ãƒ¼ã‚¯ã‚‚å¹³æ»‘åŒ–ã•ã‚Œã‚‹ 2\. **æ­£** - Savitzky-Golayã¯å¤šé …å¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚’ä½¿ã†ãŸã‚ã€æ€¥å³»ãªå¤‰åŒ–ã‚’ä¿æŒã—ã‚„ã™ã„ 3\. **èª¤** - Z-scoreã¯è¨ˆç®—å¯èƒ½ã ãŒã€3Ïƒãƒ«ãƒ¼ãƒ«ã®è§£é‡ˆã¯æ­£è¦åˆ†å¸ƒã‚’ä»®å®šã—ã¦ã„ã‚‹ã€‚éæ­£è¦åˆ†å¸ƒã§ã¯IQRæ³•ãŒæ¨å¥¨ã•ã‚Œã‚‹ **è§£èª¬**: ãƒã‚¤ã‚ºé™¤å»ã¯å¸¸ã«ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒã‚ã‚Šã¾ã™ã€‚ãƒã‚¤ã‚ºã‚’å®Œå…¨ã«é™¤å»ã—ã‚ˆã†ã¨ã™ã‚‹ã¨ã€ä¿¡å·ã‚‚æ­ªã¿ã¾ã™ã€‚å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ï¼ˆãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã€ãƒ”ãƒ¼ã‚¯ã®é‹­ã•ï¼‰ã«å¿œã˜ã¦ã€é©åˆ‡ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é¸æŠã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚ 

* * *

### å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰

ä»¥ä¸‹ã®XRDãƒ‡ãƒ¼ã‚¿ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰ã«å¯¾ã—ã¦ã€é©åˆ‡ãªå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ã¦ãã ã•ã„ã€‚
    
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    import numpy as np
    import pandas as pd
    
    np.random.seed(100)
    two_theta = np.linspace(20, 60, 400)
    intensity = (
        800 * np.exp(-((two_theta - 30) ** 2) / 8) +
        1200 * np.exp(-((two_theta - 45) ** 2) / 6) +
        np.random.normal(0, 100, len(two_theta))
    )
    # å¤–ã‚Œå€¤ã‚’è¿½åŠ 
    intensity[50] = 3000
    intensity[200] = -500
    
    df = pd.DataFrame({'two_theta': two_theta, 'intensity': intensity})
    

**è¦æ±‚äº‹é …** : 1\. è² ã®å¼·åº¦å€¤ã‚’0ã«ç½®ãæ›ãˆã‚‹ 2\. Z-scoreã§å¤–ã‚Œå€¤ã‚’æ¤œå‡ºãƒ»é™¤å»ï¼ˆé–¾å€¤3Ïƒï¼‰ 3\. Savitzky-Golayãƒ•ã‚£ãƒ«ã‚¿ã§ãƒã‚¤ã‚ºé™¤å»ï¼ˆwindow=11, polyorder=3ï¼‰ 4\. å‡¦ç†å‰å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’å¯è¦–åŒ–

ãƒ’ãƒ³ãƒˆ **å‡¦ç†ãƒ•ãƒ­ãƒ¼**: 1\. è² ã®å€¤ã®ãƒã‚¹ã‚¯ä½œæˆ â†’ 0ã«ç½®ãæ›ãˆ 2\. `scipy.stats.zscore` ã§å¤–ã‚Œå€¤æ¤œå‡º 3\. å¤–ã‚Œå€¤ã‚’ç·šå½¢è£œé–“ 4\. `scipy.signal.savgol_filter` ã§ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚° 5\. `matplotlib` ã§å…ƒãƒ‡ãƒ¼ã‚¿ã¨å‡¦ç†å¾Œãƒ‡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒ **ä½¿ç”¨ã™ã‚‹é–¢æ•°**: \- `df[condition]` ã§æ¡ä»¶æŠ½å‡º \- `np.abs(stats.zscore())` ã§Z-score \- `interpolate(method='linear')` ã§è£œé–“ \- `savgol_filter()` ã§ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°  è§£ç­”ä¾‹
    
    
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    from scipy import stats
    from scipy.signal import savgol_filter
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    np.random.seed(100)
    two_theta = np.linspace(20, 60, 400)
    intensity = (
        800 * np.exp(-((two_theta - 30) ** 2) / 8) +
        1200 * np.exp(-((two_theta - 45) ** 2) / 6) +
        np.random.normal(0, 100, len(two_theta))
    )
    intensity[50] = 3000
    intensity[200] = -500
    
    df = pd.DataFrame({'two_theta': two_theta, 'intensity': intensity})
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: è² ã®å€¤ã‚’0ã«ç½®ãæ›ãˆ
    df_cleaned = df.copy()
    negative_mask = df_cleaned['intensity'] < 0
    df_cleaned.loc[negative_mask, 'intensity'] = 0
    print(f"è² ã®å€¤ã®æ•°: {negative_mask.sum()}")
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: å¤–ã‚Œå€¤æ¤œå‡ºï¼ˆZ-scoreæ³•ï¼‰
    z_scores = np.abs(stats.zscore(df_cleaned['intensity']))
    outliers = z_scores > 3
    print(f"å¤–ã‚Œå€¤ã®æ•°: {outliers.sum()}")
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: å¤–ã‚Œå€¤ã‚’è£œé–“
    df_cleaned.loc[outliers, 'intensity'] = np.nan
    df_cleaned['intensity'] = df_cleaned['intensity'].interpolate(method='linear')
    
    # ã‚¹ãƒ†ãƒƒãƒ—4: Savitzky-Golayãƒ•ã‚£ãƒ«ã‚¿
    intensity_smoothed = savgol_filter(df_cleaned['intensity'].values, window_length=11, polyorder=3)
    
    # å¯è¦–åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # å…ƒãƒ‡ãƒ¼ã‚¿
    axes[0, 0].plot(df['two_theta'], df['intensity'], linewidth=1)
    axes[0, 0].set_title('Original Data (with outliers)')
    axes[0, 0].set_xlabel('2Î¸ (degree)')
    axes[0, 0].set_ylabel('Intensity')
    axes[0, 0].grid(True, alpha=0.3)
    
    # å¤–ã‚Œå€¤é™¤å»å¾Œ
    axes[0, 1].plot(df_cleaned['two_theta'], df_cleaned['intensity'], linewidth=1, color='orange')
    axes[0, 1].set_title('After Outlier Removal')
    axes[0, 1].set_xlabel('2Î¸ (degree)')
    axes[0, 1].set_ylabel('Intensity')
    axes[0, 1].grid(True, alpha=0.3)
    
    # ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°å¾Œ
    axes[1, 0].plot(df_cleaned['two_theta'], intensity_smoothed, linewidth=1.5, color='green')
    axes[1, 0].set_title('After Savitzky-Golay Smoothing')
    axes[1, 0].set_xlabel('2Î¸ (degree)')
    axes[1, 0].set_ylabel('Intensity')
    axes[1, 0].grid(True, alpha=0.3)
    
    # å…¨æ¯”è¼ƒ
    axes[1, 1].plot(df['two_theta'], df['intensity'], label='Original', alpha=0.4, linewidth=1)
    axes[1, 1].plot(df_cleaned['two_theta'], intensity_smoothed, label='Processed', linewidth=1.5)
    axes[1, 1].set_title('Comparison')
    axes[1, 1].set_xlabel('2Î¸ (degree)')
    axes[1, 1].set_ylabel('Intensity')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    

**å‡ºåŠ›**: 
    
    
    è² ã®å€¤ã®æ•°: 1
    å¤–ã‚Œå€¤ã®æ•°: 2
    

**è§£èª¬**: ã“ã®å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã¯ã€ç‰©ç†çš„ã«ä¸å¯èƒ½ãªè² ã®å€¤ã‚’é™¤å»ã—ã€çµ±è¨ˆçš„å¤–ã‚Œå€¤ã‚’æ¤œå‡ºãƒ»è£œé–“ã—ã€æœ€å¾Œã«ãƒã‚¤ã‚ºã‚’é™¤å»ã—ã¦ã„ã¾ã™ã€‚å‡¦ç†ã®é †åºãŒé‡è¦ã§ã€å¤–ã‚Œå€¤é™¤å»ã‚’ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã®å‰ã«è¡Œã†ã“ã¨ã§ã€å¤–ã‚Œå€¤ã®å½±éŸ¿ã‚’æœ€å°é™ã«æŠ‘ãˆã‚‰ã‚Œã¾ã™ã€‚ 

* * *

### å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰

å®Ÿéš›ã®ææ–™ç ”ç©¶ã‚·ãƒŠãƒªã‚ªï¼šãƒã‚¤ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆXRDæ¸¬å®šã§1000ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚å„ã‚µãƒ³ãƒ—ãƒ«ã«ã¤ã„ã¦ã€å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’è‡ªå‹•åŒ–ã—ã€å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¦ãã ã•ã„ã€‚

**èƒŒæ™¯** : è‡ªå‹•XRDè£…ç½®ã‹ã‚‰æ¯æ—¥100ã‚µãƒ³ãƒ—ãƒ«ã®æ¸¬å®šãƒ‡ãƒ¼ã‚¿ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚æ‰‹å‹•å‡¦ç†ã¯ä¸å¯èƒ½ãªãŸã‚ã€è‡ªå‹•åŒ–ãŒå¿…é ˆã§ã™ã€‚

**èª²é¡Œ** : 1\. è¤‡æ•°ã‚µãƒ³ãƒ—ãƒ«ã®å‰å‡¦ç†ã‚’ä¸€æ‹¬å®Ÿè¡Œã™ã‚‹é–¢æ•°ã‚’ä½œæˆ 2\. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆä¸æ­£ãªãƒ‡ãƒ¼ã‚¿å½¢å¼ã€æ¥µç«¯ãªå¤–ã‚Œå€¤ï¼‰ 3\. å‡¦ç†çµæœã®ãƒ­ã‚°å‡ºåŠ› 4\. å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜

**åˆ¶ç´„æ¡ä»¶** : \- å„ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°ã¯ç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ \- ä¸€éƒ¨ã®ã‚µãƒ³ãƒ—ãƒ«ã¯æ¸¬å®šå¤±æ•—ã§ä¸å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã®å¯èƒ½æ€§ \- å‡¦ç†ã¯5ç§’ä»¥å†…/ã‚µãƒ³ãƒ—ãƒ«

ãƒ’ãƒ³ãƒˆ **ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**: 1\. å‰å‡¦ç†ã‚’ã‚«ãƒ—ã‚»ãƒ«åŒ–ã—ãŸé–¢æ•°ã‚’å®šç¾© 2\. `try-except` ã§ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚° 3\. ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡¦ç†çµæœã‚’è¨˜éŒ² 4\. `pandas.to_csv()` ã§ä¿å­˜ **è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³**: 
    
    
    def preprocess_xrd(data, params):
        """XRDãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†"""
        # 1. ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        # 2. è² ã®å€¤é™¤å»
        # 3. å¤–ã‚Œå€¤æ¤œå‡º
        # 4. ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
        # 5. çµæœã‚’è¿”ã™
        pass
    
    def batch_process(file_list):
        """è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€æ‹¬å‡¦ç†"""
        for file in file_list:
            try:
                # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
                # å‰å‡¦ç†å®Ÿè¡Œ
                # ä¿å­˜
            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
                pass
    

è§£ç­”ä¾‹ **è§£ç­”ã®æ¦‚è¦**: è¤‡æ•°ã‚µãƒ³ãƒ—ãƒ«ã®XRDãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•å‡¦ç†ã™ã‚‹å …ç‰¢ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã€ãƒ­ã‚°å‡ºåŠ›ã€å‡¦ç†æ™‚é–“æ¸¬å®šã‚’å«ã¿ã¾ã™ã€‚ **å®Ÿè£…ã‚³ãƒ¼ãƒ‰**: 
    
    
    import numpy as np
    import pandas as pd
    from scipy import stats
    from scipy.signal import savgol_filter
    import time
    import logging
    from pathlib import Path
    
    # ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('xrd_processing.log'),
            logging.StreamHandler()
        ]
    )
    
    def validate_data(df):
        """ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        if df.empty:
            raise ValueError("Empty DataFrame")
    
        if 'two_theta' not in df.columns or 'intensity' not in df.columns:
            raise ValueError("Missing required columns")
    
        if len(df) < 50:
            raise ValueError(f"Insufficient data points: {len(df)}")
    
        if df['intensity'].isnull().sum() > len(df) * 0.3:
            raise ValueError("Too many missing values (>30%)")
    
        return True
    
    
    def preprocess_xrd(df, params=None):
        """
        XRDãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    
        Parameters:
        -----------
        df : pd.DataFrame
            ã‚«ãƒ©ãƒ : 'two_theta', 'intensity'
        params : dict
            å‰å‡¦ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            - z_threshold: Z-scoreé–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰
            - sg_window: Savitzky-Golayã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 11ï¼‰
            - sg_polyorder: å¤šé …å¼æ¬¡æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰
    
        Returns:
        --------
        df_processed : pd.DataFrame
            å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
        """
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        if params is None:
            params = {
                'z_threshold': 3,
                'sg_window': 11,
                'sg_polyorder': 3
            }
    
        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        validate_data(df)
    
        df_processed = df.copy()
    
        # ã‚¹ãƒ†ãƒƒãƒ—1: è² ã®å€¤ã‚’0ã«
        negative_count = (df_processed['intensity'] < 0).sum()
        df_processed.loc[df_processed['intensity'] < 0, 'intensity'] = 0
    
        # ã‚¹ãƒ†ãƒƒãƒ—2: å¤–ã‚Œå€¤æ¤œå‡ºãƒ»è£œé–“
        z_scores = np.abs(stats.zscore(df_processed['intensity']))
        outliers = z_scores > params['z_threshold']
        outlier_count = outliers.sum()
    
        df_processed.loc[outliers, 'intensity'] = np.nan
        df_processed['intensity'] = df_processed['intensity'].interpolate(method='linear')
    
        # ã‚¹ãƒ†ãƒƒãƒ—3: Savitzky-Golayãƒ•ã‚£ãƒ«ã‚¿
        try:
            intensity_smoothed = savgol_filter(
                df_processed['intensity'].values,
                window_length=params['sg_window'],
                polyorder=params['sg_polyorder']
            )
            df_processed['intensity'] = intensity_smoothed
        except Exception as e:
            logging.warning(f"Savitzky-Golay failed: {e}. Using moving average.")
            from scipy.ndimage import uniform_filter1d
            df_processed['intensity'] = uniform_filter1d(
                df_processed['intensity'].values,
                size=params['sg_window']
            )
    
        # å‡¦ç†çµ±è¨ˆ
        stats_dict = {
            'negative_values': negative_count,
            'outliers': outlier_count,
            'data_points': len(df_processed)
        }
    
        return df_processed, stats_dict
    
    
    def batch_process_xrd(input_files, output_dir, params=None):
        """
        è¤‡æ•°ã®XRDãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ‹¬å‡¦ç†
    
        Parameters:
        -----------
        input_files : list
            å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        output_dir : str or Path
            å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        params : dict
            å‰å‡¦ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    
        Returns:
        --------
        results : dict
            å‡¦ç†çµæœã‚µãƒãƒªãƒ¼
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
    
        results = {
            'total': len(input_files),
            'success': 0,
            'failed': 0,
            'processing_times': []
        }
    
        logging.info(f"Starting batch processing of {len(input_files)} files")
    
        for i, file_path in enumerate(input_files, 1):
            file_path = Path(file_path)
            start_time = time.time()
    
            try:
                # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
                df = pd.read_csv(file_path)
    
                # å‰å‡¦ç†å®Ÿè¡Œ
                df_processed, stats_dict = preprocess_xrd(df, params)
    
                # ä¿å­˜
                output_file = output_dir / f"processed_{file_path.name}"
                df_processed.to_csv(output_file, index=False)
    
                # å‡¦ç†æ™‚é–“
                processing_time = time.time() - start_time
                results['processing_times'].append(processing_time)
                results['success'] += 1
    
                logging.info(
                    f"[{i}/{len(input_files)}] SUCCESS: {file_path.name} "
                    f"({processing_time:.2f}s) - "
                    f"Negatives: {stats_dict['negative_values']}, "
                    f"Outliers: {stats_dict['outliers']}"
                )
    
            except Exception as e:
                results['failed'] += 1
                logging.error(f"[{i}/{len(input_files)}] FAILED: {file_path.name} - {str(e)}")
    
        # ã‚µãƒãƒªãƒ¼
        avg_time = np.mean(results['processing_times']) if results['processing_times'] else 0
        logging.info(
            f"\n=== Batch Processing Complete ===\n"
            f"Total: {results['total']}\n"
            f"Success: {results['success']}\n"
            f"Failed: {results['failed']}\n"
            f"Average processing time: {avg_time:.2f}s"
        )
    
        return results
    
    
    # ==================== ãƒ‡ãƒ¢å®Ÿè¡Œ ====================
    
    if __name__ == "__main__":
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå®Ÿéš›ã¯å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼‰
        np.random.seed(42)
    
        # 10ã‚µãƒ³ãƒ—ãƒ«åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        sample_dir = Path("sample_xrd_data")
        sample_dir.mkdir(exist_ok=True)
    
        for i in range(10):
            two_theta = np.linspace(20, 60, 400)
            intensity = (
                800 * np.exp(-((two_theta - 30) ** 2) / 8) +
                1200 * np.exp(-((two_theta - 45) ** 2) / 6) +
                np.random.normal(0, 100, len(two_theta))
            )
    
            # ãƒ©ãƒ³ãƒ€ãƒ ã«å¤–ã‚Œå€¤ã‚’è¿½åŠ 
            if np.random.rand() > 0.5:
                outlier_idx = np.random.randint(0, len(intensity), size=2)
                intensity[outlier_idx] = np.random.choice([3000, -500], size=2)
    
            df = pd.DataFrame({'two_theta': two_theta, 'intensity': intensity})
            df.to_csv(sample_dir / f"sample_{i:03d}.csv", index=False)
    
        # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
        input_files = list(sample_dir.glob("sample_*.csv"))
        output_dir = Path("processed_xrd_data")
    
        params = {
            'z_threshold': 3,
            'sg_window': 11,
            'sg_polyorder': 3
        }
    
        results = batch_process_xrd(input_files, output_dir, params)
    
        print(f"\nå‡¦ç†å®Œäº†: {results['success']}/{results['total']} files")
    

**çµæœ**: 
    
    
    2025-10-17 10:30:15 - INFO - Starting batch processing of 10 files
    2025-10-17 10:30:15 - INFO - [1/10] SUCCESS: sample_000.csv (0.15s) - Negatives: 1, Outliers: 2
    2025-10-17 10:30:15 - INFO - [2/10] SUCCESS: sample_001.csv (0.12s) - Negatives: 0, Outliers: 1
    ...
    2025-10-17 10:30:16 - INFO -
    === Batch Processing Complete ===
    Total: 10
    Success: 10
    Failed: 0
    Average processing time: 0.13s
    

**è©³ç´°ãªè§£èª¬**: 1\. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: `validate_data()` ã§äº‹å‰ãƒã‚§ãƒƒã‚¯ã€`try-except` ã§å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼æ•æ‰ 2\. **ãƒ­ã‚®ãƒ³ã‚°**: å‡¦ç†çŠ¶æ³ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ä¸¡æ–¹ã«å‡ºåŠ› 3\. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–**: å‰å‡¦ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤–éƒ¨ã‹ã‚‰æŒ‡å®šå¯èƒ½ 4\. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**: å‡¦ç†æ™‚é–“ã‚’æ¸¬å®šã—ã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š 5\. **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã«ä¾ã‚‰ãšå‹•ä½œ **è¿½åŠ ã®æ¤œè¨äº‹é …**: \- ä¸¦åˆ—å‡¦ç†ï¼ˆ`multiprocessing`ï¼‰ã§é«˜é€ŸåŒ– \- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆSQLiteï¼‰ã¸ã®ä¿å­˜ \- Webãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§å‡¦ç†çŠ¶æ³ã‚’å¯è¦–åŒ– \- ã‚¯ãƒ©ã‚¦ãƒ‰ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆS3ã€GCSï¼‰ã¨ã®é€£æº 

* * *

## å‚è€ƒæ–‡çŒ®

  1. VanderPlas, J. (2016). "Python Data Science Handbook." O'Reilly Media. ISBN: 978-1491912058

  2. Savitzky, A., & Golay, M. J. (1964). "Smoothing and Differentiation of Data by Simplified Least Squares Procedures." _Analytical Chemistry_ , 36(8), 1627-1639. DOI: [10.1021/ac60214a047](<https://doi.org/10.1021/ac60214a047>)

  3. Stein, H. S. et al. (2019). "Progress and prospects for accelerating materials science with automated and autonomous workflows." _Chemical Science_ , 10(42), 9640-9649. DOI: [10.1039/C9SC03766G](<https://doi.org/10.1039/C9SC03766G>)

  4. SciPy Documentation: Signal Processing. URL: <https://docs.scipy.org/doc/scipy/reference/signal.html>

  5. pandas Documentation: Data Cleaning. URL: <https://pandas.pydata.org/docs/user_guide/missing_data.html>

* * *

## ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³

### å‰ã®ç« 

ãªã—ï¼ˆç¬¬1ç« ï¼‰

### æ¬¡ã®ç« 

**[ç¬¬2ç« ï¼šã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿è§£æ â†’](<./chapter-2.html>)**

### ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡

**[â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹](<./index.html>)**

* * *

## è‘—è€…æƒ…å ±

**ä½œæˆè€…** : AI Terakoya Content Team **ä½œæˆæ—¥** : 2025-10-17 **ãƒãƒ¼ã‚¸ãƒ§ãƒ³** : 1.0

**æ›´æ–°å±¥æ­´** : \- 2025-10-17: v1.0 åˆç‰ˆå…¬é–‹

**ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯** : \- GitHub Issues: [ãƒªãƒã‚¸ãƒˆãƒªURL]/issues \- Email: yusuke.hashimoto.b8@tohoku.ac.jp

**ãƒ©ã‚¤ã‚»ãƒ³ã‚¹** : Creative Commons BY 4.0

* * *

**æ¬¡ã®ç« ã§å­¦ç¿’ã‚’ç¶šã‘ã¾ã—ã‚‡ã†ï¼**
