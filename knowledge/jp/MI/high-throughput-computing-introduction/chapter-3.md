---
title: ç¬¬3ç« ï¼šã‚¸ãƒ§ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã¨ä¸¦åˆ—åŒ–ï¼ˆSLURM, PBSï¼‰
chapter_title: ç¬¬3ç« ï¼šã‚¸ãƒ§ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã¨ä¸¦åˆ—åŒ–ï¼ˆSLURM, PBSï¼‰
subtitle: 
reading_time: 25-30åˆ†
difficulty: ä¸Šç´š
code_examples: 5
exercises: 0
---

# ç¬¬3ç« ï¼šã‚¸ãƒ§ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã¨ä¸¦åˆ—åŒ–ï¼ˆSLURM, PBSï¼‰

FireWorks/AiiDAã§ã®ä¾å­˜é–¢ä¿‚ç®¡ç†ã¨å†å®Ÿè¡Œæ€§ã‚’ç¢ºä¿ã™ã‚‹è¨­è¨ˆã‚’ç†è§£ã—ã¾ã™ã€‚ãƒ­ã‚°ã¨å¯è¦³æ¸¬æ€§ã®è¦ç‚¹ã‚‚æŠ¼ã•ãˆã¾ã™ã€‚

**ğŸ’¡ è£œè¶³:** DAGï¼ˆæµã‚Œå›³ï¼‰ã§å…¨ä½“åƒã‚’å¯è¦–åŒ–ã€‚ä¸­é–“ç”Ÿæˆç‰©ã®ä¿å­˜å ´æ‰€ã‚’æ±ºã‚ã¦ãŠãã¨å¾©æ—§ãŒé€Ÿã„ã§ã™ã€‚

## å­¦ç¿’ç›®æ¨™

ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š

  * âœ… SLURMã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¦ã‚¸ãƒ§ãƒ–ã‚’æŠ•å…¥ã§ãã‚‹
  * âœ… MPIä¸¦åˆ—è¨ˆç®—ã®åŠ¹ç‡ã‚’è©•ä¾¡ã§ãã‚‹
  * âœ… Pythonã§ã‚¸ãƒ§ãƒ–ç®¡ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’æ›¸ã‘ã‚‹
  * âœ… 1000ææ–™è¦æ¨¡ã®ä¸¦åˆ—è¨ˆç®—ã‚’è¨­è¨ˆã§ãã‚‹
  * âœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«ã‚ˆã‚‹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒã§ãã‚‹

* * *

## 3.1 ã‚¸ãƒ§ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®åŸºç¤

### SLURM vs PBS vs Torque

ç‰¹å¾´ | SLURM | PBS Pro | Torque  
---|---|---|---  
é–‹ç™ºå…ƒ | SchedMD | Altair | Adaptive Computing  
ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ | GPLï¼ˆä¸€éƒ¨å•†ç”¨ï¼‰ | å•†ç”¨ | ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹  
æ¡ç”¨ä¾‹ | TSUBAMEã€TOP500ã®å¤šã | NASAã€DOEå›½ç«‹ç ”ç©¶æ‰€ | å¤šãã®å¤§å­¦  
ã‚³ãƒãƒ³ãƒ‰ | `sbatch`, `squeue` | `qsub`, `qstat` | `qsub`, `qstat`  
æ¨å¥¨ç”¨é€” | å¤§è¦æ¨¡HPC | ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚º | ä¸­å°è¦æ¨¡HPC  
  
### SLURMã®åŸºæœ¬æ¦‚å¿µ
    
    
    ```mermaid
    flowchart TD
        A[ãƒ¦ãƒ¼ã‚¶ãƒ¼] -->|sbatch| B[ã‚¸ãƒ§ãƒ–ã‚­ãƒ¥ãƒ¼]
        B --> C{ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©}
        C -->|ãƒªã‚½ãƒ¼ã‚¹å‰²å½“| D[è¨ˆç®—ãƒãƒ¼ãƒ‰1]
        C -->|ãƒªã‚½ãƒ¼ã‚¹å‰²å½“| E[è¨ˆç®—ãƒãƒ¼ãƒ‰2]
        C -->|ãƒªã‚½ãƒ¼ã‚¹å‰²å½“| F[è¨ˆç®—ãƒãƒ¼ãƒ‰N]
    
        D --> G[ã‚¸ãƒ§ãƒ–å®Œäº†]
        E --> G
        F --> G
    
        style C fill:#4ecdc4
    ```

**ä¸»è¦ã‚³ãƒãƒ³ãƒ‰** :
    
    
    # ã‚¸ãƒ§ãƒ–æŠ•å…¥
    sbatch job.sh
    
    # ã‚¸ãƒ§ãƒ–çŠ¶æ…‹ç¢ºèª
    squeue -u username
    
    # ã‚¸ãƒ§ãƒ–ã‚­ãƒ£ãƒ³ã‚»ãƒ«
    scancel job_id
    
    # ãƒãƒ¼ãƒ‰æƒ…å ±
    sinfo
    
    # ã‚¸ãƒ§ãƒ–è©³ç´°
    scontrol show job job_id
    

* * *

## 3.2 SLURMã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ

### åŸºæœ¬çš„ãªSLURMã‚¹ã‚¯ãƒªãƒ—ãƒˆ
    
    
    #!/bin/bash
    #SBATCH --job-name=vasp_relax       # ã‚¸ãƒ§ãƒ–å
    #SBATCH --output=slurm-%j.out       # æ¨™æº–å‡ºåŠ›ï¼ˆ%j=ã‚¸ãƒ§ãƒ–IDï¼‰
    #SBATCH --error=slurm-%j.err        # æ¨™æº–ã‚¨ãƒ©ãƒ¼
    #SBATCH --nodes=1                   # ãƒãƒ¼ãƒ‰æ•°
    #SBATCH --ntasks-per-node=48        # ãƒãƒ¼ãƒ‰ã‚ãŸã‚ŠMPIãƒ—ãƒ­ã‚»ã‚¹æ•°
    #SBATCH --cpus-per-task=1           # ã‚¿ã‚¹ã‚¯ã‚ãŸã‚Šã‚¹ãƒ¬ãƒƒãƒ‰æ•°
    #SBATCH --time=24:00:00             # æ™‚é–“åˆ¶é™ï¼ˆ24æ™‚é–“ï¼‰
    #SBATCH --partition=standard        # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ï¼ˆã‚­ãƒ¥ãƒ¼ï¼‰
    #SBATCH --account=project_name      # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå
    
    # ç’°å¢ƒè¨­å®š
    module purge
    module load intel/2021.2
    module load vasp/6.3.0
    
    # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    cd $SLURM_SUBMIT_DIR
    
    # VASPå®Ÿè¡Œ
    echo "ã‚¸ãƒ§ãƒ–é–‹å§‹: $(date)"
    echo "ãƒ›ã‚¹ãƒˆ: $(hostname)"
    echo "ãƒãƒ¼ãƒ‰æ•°: $SLURM_JOB_NUM_NODES"
    echo "ãƒ—ãƒ­ã‚»ã‚¹æ•°: $SLURM_NTASKS"
    
    mpirun -np $SLURM_NTASKS vasp_std
    
    echo "ã‚¸ãƒ§ãƒ–çµ‚äº†: $(date)"
    

### ã‚¢ãƒ¬ã‚¤ã‚¸ãƒ§ãƒ–ã«ã‚ˆã‚‹ä¸¦åˆ—å®Ÿè¡Œ

**100ææ–™ã‚’ä¸¦åˆ—è¨ˆç®—** :
    
    
    #!/bin/bash
    #SBATCH --job-name=vasp_array
    #SBATCH --output=logs/slurm-%A_%a.out  # %A=ã‚¢ãƒ¬ã‚¤ã‚¸ãƒ§ãƒ–ID, %a=ã‚¿ã‚¹ã‚¯ID
    #SBATCH --error=logs/slurm-%A_%a.err
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=48
    #SBATCH --time=24:00:00
    #SBATCH --array=1-100%10              # 1-100ã®ã‚¿ã‚¹ã‚¯ã€åŒæ™‚10å€‹ã¾ã§
    
    # ç’°å¢ƒè¨­å®š
    module load vasp/6.3.0
    
    # ææ–™ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿
    MATERIAL_LIST="materials.txt"
    MATERIAL=$(sed -n "${SLURM_ARRAY_TASK_ID}p" $MATERIAL_LIST)
    
    echo "å‡¦ç†ä¸­: ã‚¿ã‚¹ã‚¯ID=${SLURM\_ARRAY\_TASK\_ID}, ææ–™=${MATERIAL}"
    
    # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    WORK_DIR="calculations/${MATERIAL}"
    cd $WORK_DIR
    
    # VASPå®Ÿè¡Œ
    mpirun -np 48 vasp_std
    
    # åæŸãƒã‚§ãƒƒã‚¯
    if grep -q "reached required accuracy" OUTCAR; then
        echo "SUCCESS: ${MATERIAL}" >> ../completed.log
    else
        echo "FAILED: ${MATERIAL}" >> ../failed.log
    fi
    

**materials.txt** ã®ä¾‹:
    
    
    LiCoO2
    LiNiO2
    LiMnO2
    LiFePO4
    ...ï¼ˆ100è¡Œï¼‰
    

### ä¾å­˜é–¢ä¿‚ã®ã‚ã‚‹ã‚¸ãƒ§ãƒ–ãƒã‚§ãƒ¼ãƒ³
    
    
    # Step 1: æ§‹é€ æœ€é©åŒ–
    JOB1=$(sbatch --parsable relax.sh)
    echo "æ§‹é€ æœ€é©åŒ–ã‚¸ãƒ§ãƒ–ID: $JOB1"
    
    # Step 2: é™çš„è¨ˆç®—ï¼ˆæ§‹é€ æœ€é©åŒ–å¾Œã«å®Ÿè¡Œï¼‰
    JOB2=$(sbatch --parsable --dependency=afterok:$JOB1 static.sh)
    echo "é™çš„è¨ˆç®—ã‚¸ãƒ§ãƒ–ID: $JOB2"
    
    # Step 3: ãƒãƒ³ãƒ‰æ§‹é€ ï¼ˆé™çš„è¨ˆç®—å¾Œã«å®Ÿè¡Œï¼‰
    JOB3=$(sbatch --parsable --dependency=afterok:$JOB2 band.sh)
    echo "ãƒãƒ³ãƒ‰æ§‹é€ ã‚¸ãƒ§ãƒ–ID: $JOB3"
    
    # Step 4: ãƒ‡ãƒ¼ã‚¿è§£æï¼ˆå…¨ã¦å®Œäº†å¾Œï¼‰
    sbatch --dependency=afterok:$JOB3 analysis.sh
    

* * *

## 3.3 MPIã«ã‚ˆã‚‹ä¸¦åˆ—è¨ˆç®—

### ä¸¦åˆ—åŒ–ã®ç¨®é¡
    
    
    # 1. ã‚¿ã‚¹ã‚¯ä¸¦åˆ—ï¼ˆæ¨å¥¨ï¼šãƒã‚¤ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—ï¼‰
    # 100ææ–™ã‚’100ãƒãƒ¼ãƒ‰ã§åŒæ™‚è¨ˆç®—
    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡: 100%
    
    # 2. ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ï¼ˆVASP: KPARè¨­å®šï¼‰
    # k-pointã‚’4ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†å‰²
    INCAR: KPAR = 4
    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡: 80-90%
    
    # 3. MPIä¸¦åˆ—ï¼ˆãƒ—ãƒ­ã‚»ã‚¹é–“é€šä¿¡ï¼‰
    # 1è¨ˆç®—ã‚’48ã‚³ã‚¢ã§åˆ†æ•£
    mpirun -np 48 vasp_std
    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡: 50-70%
    

### ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡ã®æ¸¬å®š
    
    
    import subprocess
    import time
    import numpy as np
    import matplotlib.pyplot as plt
    
    def benchmark_scaling(structure, core_counts=[1, 2, 4, 8, 16, 32, 48]):
        """
        ä¸¦åˆ—åŒ–åŠ¹ç‡ã‚’ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    
        Parameters:
        -----------
        structure : str
            ãƒ†ã‚¹ãƒˆæ§‹é€ 
        core_counts : list
            ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚³ã‚¢æ•°
    
        Returns:
        --------
        efficiency : dict
            ã‚³ã‚¢æ•°ã”ã¨ã®åŠ¹ç‡
        """
        timings = {}
    
        for n_cores in core_counts:
            # VASPå®Ÿè¡Œ
            start = time.time()
    
            result = subprocess.run(
                [f"mpirun -np {n_cores} vasp_std"],
                shell=True,
                cwd=f"benchmark_{n_cores}cores"
            )
    
            elapsed = time.time() - start
            timings[n_cores] = elapsed
    
            print(f"{n_cores}ã‚³ã‚¢: {elapsed:.1f}ç§’")
    
        # åŠ¹ç‡è¨ˆç®—
        base_time = timings[1]
        efficiency = {}
    
        for n_cores, t in timings.items():
            ideal_time = base_time / n_cores
            actual_speedup = base_time / t
            ideal_speedup = n_cores
    
            eff = (actual_speedup / ideal_speedup) * 100
            efficiency[n_cores] = eff
    
        return efficiency, timings
    
    # çµæœãƒ—ãƒ­ãƒƒãƒˆ
    def plot_scaling(efficiency, timings):
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
        cores = list(timings.keys())
        times = list(timings.values())
        effs = [efficiency[c] for c in cores]
    
        # ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—
        ax1.plot(cores, [timings[1]/t for t in times], 'o-', label='å®Ÿæ¸¬')
        ax1.plot(cores, cores, '--', label='ç†æƒ³ï¼ˆç·šå½¢ï¼‰')
        ax1.set_xlabel('ã‚³ã‚¢æ•°')
        ax1.set_ylabel('ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—')
        ax1.set_xscale('log', base=2)
        ax1.set_yscale('log', base=2)
        ax1.legend()
        ax1.grid(True)
    
        # åŠ¹ç‡
        ax2.plot(cores, effs, 'o-', color='green')
        ax2.axhline(y=80, color='red', linestyle='--', label='80%ç›®æ¨™')
        ax2.set_xlabel('ã‚³ã‚¢æ•°')
        ax2.set_ylabel('ä¸¦åˆ—åŒ–åŠ¹ç‡ (%)')
        ax2.set_xscale('log', base=2)
        ax2.legend()
        ax2.grid(True)
    
        plt.tight_layout()
        plt.savefig('scaling_benchmark.png', dpi=300)
        plt.show()
    

### VASPã®ä¸¦åˆ—åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
    
    
    def optimize_vasp_parallelization(n_kpoints, n_bands, n_cores=48):
        """
        VASPä¸¦åˆ—åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–
    
        Parameters:
        -----------
        n_kpoints : int
            k-pointæ•°
        n_bands : int
            ãƒãƒ³ãƒ‰æ•°
        n_cores : int
            åˆ©ç”¨å¯èƒ½ãªã‚³ã‚¢æ•°
    
        Returns:
        --------
        params : dict
            æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        # KPAR: k-pointä¸¦åˆ—ï¼ˆæœ€ã‚‚åŠ¹ç‡çš„ï¼‰
        # 2ã®ç´¯ä¹—ã§ã€k-pointæ•°ã®ç´„æ•°
        kpar_options = [1, 2, 4, 8, 16]
        valid_kpar = [k for k in kpar_options if n_kpoints % k == 0 and k <= n_cores]
    
        # NCORE: ãƒãƒ³ãƒ‰ä¸¦åˆ—
        # é€šå¸¸4-8ãŒæœ€é©
        ncore = min(4, n_cores // max(valid_kpar))
    
        # æ¨å¥¨è¨­å®š
        recommended = {
            'KPAR': max(valid_kpar),
            'NCORE': ncore,
            'cores_per_kpar_group': n_cores // max(valid_kpar),
        }
    
        print("ä¸¦åˆ—åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å¥¨å€¤:")
        print(f"  KPAR = {recommended['KPAR']} (k-pointä¸¦åˆ—)")
        print(f"  NCORE = {recommended['NCORE']} (ãƒãƒ³ãƒ‰ä¸¦åˆ—)")
        print(f"  1 KPARã‚°ãƒ«ãƒ¼ãƒ—ã‚ãŸã‚Š{recommended['cores_per_kpar_group']}ã‚³ã‚¢")
    
        return recommended
    
    # ä½¿ç”¨ä¾‹
    params = optimize_vasp_parallelization(n_kpoints=64, n_bands=200, n_cores=48)
    # æ¨å¥¨:
    #   KPAR = 16 (k-pointä¸¦åˆ—)
    #   NCORE = 3 (ãƒãƒ³ãƒ‰ä¸¦åˆ—)
    #   1 KPARã‚°ãƒ«ãƒ¼ãƒ—ã‚ãŸã‚Š3ã‚³ã‚¢
    

* * *

## 3.4 Pythonã«ã‚ˆã‚‹ã‚¸ãƒ§ãƒ–ç®¡ç†

### ã‚¸ãƒ§ãƒ–æŠ•å…¥ã¨ç›£è¦–
    
    
    import subprocess
    import time
    import re
    
    class SLURMJobManager:
        """SLURMã‚¸ãƒ§ãƒ–ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
        def submit_job(self, script_path):
            """
            ã‚¸ãƒ§ãƒ–ã‚’æŠ•å…¥
    
            Returns:
            --------
            job_id : int
                ã‚¸ãƒ§ãƒ–ID
            """
            result = subprocess.run(
                ['sbatch', script_path],
                capture_output=True,
                text=True
            )
    
            # "Submitted batch job 12345"ã‹ã‚‰IDã‚’æŠ½å‡º
            match = re.search(r'(\d+)', result.stdout)
            if match:
                job_id = int(match.group(1))
                print(f"ã‚¸ãƒ§ãƒ–æŠ•å…¥: ID={job_id}")
                return job_id
            else:
                raise RuntimeError(f"ã‚¸ãƒ§ãƒ–æŠ•å…¥å¤±æ•—: {result.stderr}")
    
        def get_job_status(self, job_id):
            """
            ã‚¸ãƒ§ãƒ–çŠ¶æ…‹ã‚’å–å¾—
    
            Returns:
            --------
            status : str
                'PENDING', 'RUNNING', 'COMPLETED', 'FAILED', 'CANCELLED'
            """
            result = subprocess.run(
                ['squeue', '-j', str(job_id), '-h', '-o', '%T'],
                capture_output=True,
                text=True
            )
    
            if result.stdout.strip():
                return result.stdout.strip()
            else:
                # ã‚­ãƒ¥ãƒ¼ã«ãªã„ â†’ å®Œäº†ã¾ãŸã¯å¤±æ•—
                result = subprocess.run(
                    ['sacct', '-j', str(job_id), '-X', '-n', '-o', 'State'],
                    capture_output=True,
                    text=True
                )
                return result.stdout.strip()
    
        def wait_for_completion(self, job_id, check_interval=60):
            """
            ã‚¸ãƒ§ãƒ–å®Œäº†ã‚’å¾…æ©Ÿ
    
            Parameters:
            -----------
            job_id : int
                ã‚¸ãƒ§ãƒ–ID
            check_interval : int
                ãƒã‚§ãƒƒã‚¯é–“éš”ï¼ˆç§’ï¼‰
            """
            while True:
                status = self.get_job_status(job_id)
    
                if status in ['COMPLETED', 'FAILED', 'CANCELLED']:
                    print(f"ã‚¸ãƒ§ãƒ–{job_id}: {status}")
                    return status
    
                print(f"ã‚¸ãƒ§ãƒ–{job_id}: {status}...å¾…æ©Ÿä¸­")
                time.sleep(check_interval)
    
        def submit_array_job(self, script_path, n_tasks, max_concurrent=10):
            """
            ã‚¢ãƒ¬ã‚¤ã‚¸ãƒ§ãƒ–ã‚’æŠ•å…¥
    
            Parameters:
            -----------
            n_tasks : int
                ã‚¿ã‚¹ã‚¯æ•°
            max_concurrent : int
                åŒæ™‚å®Ÿè¡Œæ•°
            """
            result = subprocess.run(
                ['sbatch', f'--array=1-{n_tasks}%{max_concurrent}', script_path],
                capture_output=True,
                text=True
            )
    
            match = re.search(r'(\d+)', result.stdout)
            if match:
                job_id = int(match.group(1))
                print(f"ã‚¢ãƒ¬ã‚¤ã‚¸ãƒ§ãƒ–æŠ•å…¥: ID={job_id}, ã‚¿ã‚¹ã‚¯æ•°={n_tasks}")
                return job_id
            else:
                raise RuntimeError(f"æŠ•å…¥å¤±æ•—: {result.stderr}")
    
    # ä½¿ç”¨ä¾‹
    manager = SLURMJobManager()
    
    # å˜ä¸€ã‚¸ãƒ§ãƒ–
    job_id = manager.submit_job('relax.sh')
    status = manager.wait_for_completion(job_id)
    
    # ã‚¢ãƒ¬ã‚¤ã‚¸ãƒ§ãƒ–
    array_id = manager.submit_array_job('array_job.sh', n_tasks=100, max_concurrent=20)
    

### å¤§è¦æ¨¡ã‚¸ãƒ§ãƒ–ç®¡ç†ï¼ˆ1000ææ–™ï¼‰
    
    
    import os
    from pathlib import Path
    import json
    
    def manage_large_scale_calculation(materials, batch_size=100):
        """
        1000ææ–™ã‚’åŠ¹ç‡çš„ã«ç®¡ç†
    
        Parameters:
        -----------
        materials : list
            ææ–™ã®ãƒªã‚¹ãƒˆ
        batch_size : int
            ãƒãƒƒãƒã‚µã‚¤ã‚º
        """
        manager = SLURMJobManager()
        n_materials = len(materials)
    
        print(f"ç·ææ–™æ•°: {n_materials}")
        print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    
        # ãƒãƒƒãƒã«åˆ†å‰²
        n_batches = (n_materials + batch_size - 1) // batch_size
        print(f"ãƒãƒƒãƒæ•°: {n_batches}")
    
        job_ids = []
    
        for batch_idx in range(n_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, n_materials)
            batch_materials = materials[start_idx:end_idx]
    
            print(f"\nãƒãƒƒãƒ {batch_idx+1}/{n_batches}")
            print(f"  ææ–™æ•°: {len(batch_materials)}")
    
            # ãƒãƒƒãƒç”¨ææ–™ãƒªã‚¹ãƒˆä½œæˆ
            list_file = f"batch_{batch_idx+1}_materials.txt"
            with open(list_file, 'w') as f:
                for mat in batch_materials:
                    f.write(f"{mat}\n")
    
            # ã‚¢ãƒ¬ã‚¤ã‚¸ãƒ§ãƒ–æŠ•å…¥
            job_id = manager.submit_array_job(
                'vasp_array.sh',
                n_tasks=len(batch_materials),
                max_concurrent=20
            )
    
            job_ids.append(job_id)
    
        # é€²æ—ç›£è¦–
        print("\né€²æ—ç›£è¦–ä¸­...")
        completed = 0
    
        while completed < len(job_ids):
            time.sleep(300)  # 5åˆ†ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
    
            for i, job_id in enumerate(job_ids):
                status = manager.get_job_status(job_id)
    
                if status == 'COMPLETED' and i not in completed_jobs:
                    completed += 1
                    print(f"ãƒãƒƒãƒ {i+1} å®Œäº† ({completed}/{len(job_ids)})")
    
        print("å…¨ãƒãƒƒãƒå®Œäº†ï¼")
    
    # ä½¿ç”¨ä¾‹
    materials = [f"material_{i:04d}" for i in range(1, 1001)]
    manage_large_scale_calculation(materials, batch_size=100)
    

* * *

## 3.5 æ¼”ç¿’å•é¡Œ

### å•é¡Œ1ï¼ˆé›£æ˜“åº¦: easyï¼‰

**å•é¡Œ** : ä»¥ä¸‹ã®æ¡ä»¶ã§SLURMã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

  * ã‚¸ãƒ§ãƒ–å: `si_bandgap`
  * ãƒãƒ¼ãƒ‰æ•°: 2
  * ãƒãƒ¼ãƒ‰ã‚ãŸã‚Šãƒ—ãƒ­ã‚»ã‚¹æ•°: 24
  * æ™‚é–“åˆ¶é™: 12æ™‚é–“
  * ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³: `gpu`

è§£ç­”ä¾‹
    
    
    #!/bin/bash
    #SBATCH --job-name=si_bandgap
    #SBATCH --output=slurm-%j.out
    #SBATCH --error=slurm-%j.err
    #SBATCH --nodes=2
    #SBATCH --ntasks-per-node=24
    #SBATCH --time=12:00:00
    #SBATCH --partition=gpu
    
    module load vasp/6.3.0
    
    cd $SLURM_SUBMIT_DIR
    
    mpirun -np 48 vasp_std
    

### å•é¡Œ2ï¼ˆé›£æ˜“åº¦: mediumï¼‰

**å•é¡Œ** : 50å€‹ã®æ§‹é€ æœ€é©åŒ–è¨ˆç®—ã‚’ã€10å€‹ãšã¤åŒæ™‚å®Ÿè¡Œã™ã‚‹ã‚¢ãƒ¬ã‚¤ã‚¸ãƒ§ãƒ–ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

è§£ç­”ä¾‹
    
    
    #!/bin/bash
    #SBATCH --job-name=relax_array
    #SBATCH --output=logs/slurm-%A_%a.out
    #SBATCH --error=logs/slurm-%A_%a.err
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=48
    #SBATCH --time=24:00:00
    #SBATCH --array=1-50%10
    
    module load vasp/6.3.0
    
    MATERIAL=$(sed -n "${SLURM_ARRAY_TASK_ID}p" materials.txt)
    
    cd calculations/${MATERIAL}
    
    mpirun -np 48 vasp_std
    

### å•é¡Œ3ï¼ˆé›£æ˜“åº¦: hardï¼‰

**å•é¡Œ** : Pythonã§ã€1000ææ–™ã®VASPè¨ˆç®—ã‚’ç®¡ç†ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚è¦ä»¶ï¼š

  1. 50ææ–™ãšã¤ãƒãƒƒãƒå‡¦ç†
  2. å„ãƒãƒƒãƒã¯20ã‚¿ã‚¹ã‚¯åŒæ™‚å®Ÿè¡Œ
  3. å®Œäº†ãƒ»å¤±æ•—ã‚’ãƒ­ã‚°è¨˜éŒ²
  4. å¤±æ•—ã—ãŸã‚¿ã‚¹ã‚¯ã‚’è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤

è§£ç­”ä¾‹
    
    
    import os
    import time
    import json
    from pathlib import Path
    
    class HighThroughputManager:
        def __init__(self, materials, batch_size=50, max_concurrent=20):
            self.materials = materials
            self.batch_size = batch_size
            self.max_concurrent = max_concurrent
            self.manager = SLURMJobManager()
    
            self.results = {
                'completed': [],
                'failed': [],
                'retry': []
            }
    
        def run_batch(self, batch_materials, batch_id):
            """ãƒãƒƒãƒå®Ÿè¡Œ"""
            # ææ–™ãƒªã‚¹ãƒˆä½œæˆ
            list_file = f"batch_{batch_id}.txt"
            with open(list_file, 'w') as f:
                for mat in batch_materials:
                    f.write(f"{mat}\n")
    
            # ã‚¸ãƒ§ãƒ–æŠ•å…¥
            job_id = self.manager.submit_array_job(
                'vasp_array.sh',
                n_tasks=len(batch_materials),
                max_concurrent=self.max_concurrent
            )
    
            # å®Œäº†å¾…ã¡
            status = self.manager.wait_for_completion(job_id)
    
            # çµæœãƒã‚§ãƒƒã‚¯
            self.check_results(batch_materials, batch_id)
    
        def check_results(self, batch_materials, batch_id):
            """çµæœç¢ºèª"""
            for mat in batch_materials:
                outcar = f"calculations/{mat}/OUTCAR"
    
                if not os.path.exists(outcar):
                    self.results['failed'].append(mat)
                    continue
    
                with open(outcar, 'r') as f:
                    content = f.read()
                    if 'reached required accuracy' in content:
                        self.results['completed'].append(mat)
                    else:
                        self.results['failed'].append(mat)
    
            # ãƒ­ã‚°ä¿å­˜
            with open(f'batch_{batch_id}_results.json', 'w') as f:
                json.dump(self.results, f, indent=2)
    
        def retry_failed(self, max_retries=2):
            """å¤±æ•—ã‚¿ã‚¹ã‚¯ã‚’ãƒªãƒˆãƒ©ã‚¤"""
            for retry_count in range(max_retries):
                if not self.results['failed']:
                    break
    
                print(f"\nãƒªãƒˆãƒ©ã‚¤ {retry_count+1}/{max_retries}")
                print(f"  å¤±æ•—ã‚¿ã‚¹ã‚¯æ•°: {len(self.results['failed'])}")
    
                failed_materials = self.results['failed'].copy()
                self.results['failed'] = []
    
                self.run_batch(failed_materials, f'retry_{retry_count+1}')
    
        def execute_all(self):
            """å…¨ææ–™ã‚’å®Ÿè¡Œ"""
            n_batches = (len(self.materials) + self.batch_size - 1) // self.batch_size
    
            for batch_idx in range(n_batches):
                start = batch_idx * self.batch_size
                end = min((batch_idx + 1) * self.batch_size, len(self.materials))
                batch_materials = self.materials[start:end]
    
                print(f"\nãƒãƒƒãƒ {batch_idx+1}/{n_batches}")
                self.run_batch(batch_materials, batch_idx+1)
    
            # ãƒªãƒˆãƒ©ã‚¤
            self.retry_failed(max_retries=2)
    
            # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
            print("\næœ€çµ‚çµæœ:")
            print(f"  æˆåŠŸ: {len(self.results['completed'])}")
            print(f"  å¤±æ•—: {len(self.results['failed'])}")
    
            return self.results
    
    # å®Ÿè¡Œ
    materials = [f"material_{i:04d}" for i in range(1, 1001)]
    manager = HighThroughputManager(materials, batch_size=50, max_concurrent=20)
    results = manager.execute_all()
    

* * *

## 3.6 ã¾ã¨ã‚

**ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ** :

  1. **SLURM** : å¤§è¦æ¨¡HPCã§åºƒãä½¿ç”¨ã•ã‚Œã‚‹ã‚¸ãƒ§ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
  2. **ã‚¢ãƒ¬ã‚¤ã‚¸ãƒ§ãƒ–** : å¤šæ•°ã®ææ–™ã‚’åŠ¹ç‡çš„ã«ä¸¦åˆ—å®Ÿè¡Œ
  3. **MPIä¸¦åˆ—** : ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡ã‚’æ¸¬å®šãƒ»æœ€é©åŒ–
  4. **Pythonç®¡ç†** : å¤§è¦æ¨¡è¨ˆç®—ã®è‡ªå‹•åŒ–ã¨ç›£è¦–

**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—** :

ç¬¬4ç« ã§ã¯ã€**FireWorksã¨AiiDAã«ã‚ˆã‚‹ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç†** ã‚’å­¦ã³ã¾ã™ã€‚

**[ç¬¬4ç« : ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã¨ãƒã‚¹ãƒˆãƒ—ãƒ­ã‚»ã‚¹ â†’](<./chapter-4.html>)**

* * *

**ãƒ©ã‚¤ã‚»ãƒ³ã‚¹** : CC BY 4.0 **ä½œæˆæ—¥** : 2025-10-17 **ä½œæˆè€…** : Dr. Yusuke Hashimoto, Tohoku University
