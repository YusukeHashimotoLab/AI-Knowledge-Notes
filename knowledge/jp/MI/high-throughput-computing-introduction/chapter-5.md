---
title: ç¬¬5ç« ï¼šã‚¯ãƒ©ã‚¦ãƒ‰HPCæ´»ç”¨ã¨æœ€é©åŒ–
chapter_title: ç¬¬5ç« ï¼šã‚¯ãƒ©ã‚¦ãƒ‰HPCæ´»ç”¨ã¨æœ€é©åŒ–
subtitle: 
reading_time: 15-20åˆ†
difficulty: ä¸­ç´šã€œä¸Šç´š
code_examples: 5
exercises: 0
---

# ç¬¬5ç« ï¼šã‚¯ãƒ©ã‚¦ãƒ‰HPCæ´»ç”¨ã¨æœ€é©åŒ–

NOMADã‚„DVCã‚’ç”¨ã„ãŸãƒ‡ãƒ¼ã‚¿ç‰ˆâ€œå†ç¾å¯èƒ½ç ”ç©¶â€ã®å®Ÿè·µã‚’å­¦ã³ã¾ã™ã€‚ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨­è¨ˆã¨å…¬é–‹ã®ä½œæ³•ã‚’æŠ¼ã•ãˆã¾ã™ã€‚

**ğŸ’¡ è£œè¶³:** ã€Œèª°ãŒãƒ»ã„ã¤ãƒ»ã©ã†ä½œã£ãŸã‹ã€ã‚’æ®‹ã™ã¨å°†æ¥ã®æ¤œè¨¼ãŒå®¹æ˜“ã€‚å…¬é–‹å‰ã«åŒ¿ååŒ–ã¨æ¨©åˆ©ç¢ºèªã‚’è¡Œã„ã¾ã™ã€‚

## å­¦ç¿’ç›®æ¨™

ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š

  * âœ… AWS Parallel Clusterã‚’æ§‹ç¯‰ã§ãã‚‹
  * âœ… ã‚¹ãƒãƒƒãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã‚³ã‚¹ãƒˆã‚’50%å‰Šæ¸›ã§ãã‚‹
  * âœ… Dockerã§è¨ˆç®—ç’°å¢ƒã‚’å®Œå…¨å†ç¾ã§ãã‚‹
  * âœ… 10,000ææ–™è¦æ¨¡ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’è¨­è¨ˆãƒ»å®Ÿè¡Œã§ãã‚‹
  * âœ… ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ã‚’ç†è§£ã—ã¦ã„ã‚‹

* * *

## 5.1 ã‚¯ãƒ©ã‚¦ãƒ‰HPCã®é¸æŠè‚¢

### ä¸»è¦ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ¯”è¼ƒ

ã‚µãƒ¼ãƒ“ã‚¹ | ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ | ç‰¹å¾´ | åˆæœŸè²»ç”¨ | æ¨å¥¨ç”¨é€”  
---|---|---|---|---  
**AWS Parallel Cluster** | Amazon | æœ€å¤§è¦æ¨¡ã€è±Šå¯Œãªå®Ÿç¸¾ | $0 | å¤§è¦æ¨¡HPC  
**Google Cloud HPC Toolkit** | Google | AI/MLçµ±åˆãŒå¼·åŠ› | $0 | æ©Ÿæ¢°å­¦ç¿’é€£æº  
**Azure CycleCloud** | Microsoft | Windowsçµ±åˆ | $0 | ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚º  
**TSUBAME** | æ±å·¥å¤§ | å›½å†…ãƒˆãƒƒãƒ—ã€å­¦è¡“åˆ©ç”¨ | ç”³è«‹åˆ¶ | å­¦è¡“ç ”ç©¶  
**å¯Œå²³** | ç†ç ” | ä¸–ç•Œãƒˆãƒƒãƒ—500 | ç”³è«‹åˆ¶ | è¶…å¤§è¦æ¨¡è¨ˆç®—  
  
### ã‚³ã‚¹ãƒˆæ¯”è¼ƒï¼ˆ10,000ææ–™ã€1ææ–™=12 CPUæ™‚é–“ã€48ã‚³ã‚¢ï¼‰

ã‚ªãƒ—ã‚·ãƒ§ãƒ³ | è¨ˆç®—æ™‚é–“ | ã‚³ã‚¹ãƒˆ | ãƒ¡ãƒªãƒƒãƒˆ | ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ  
---|---|---|---|---  
ã‚ªãƒ³ãƒ—ãƒ¬HPC | 5,760,000 CPUæ™‚é–“ | $0ï¼ˆæ—¢å­˜è¨­å‚™ï¼‰ | ç„¡æ–™ï¼ˆåˆ©ç”¨æ å†…ï¼‰ | å¾…ã¡æ™‚é–“ã€åˆ¶é™  
AWS ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ | åŒä¸Š | $4,000-6,000 | å³æ™‚åˆ©ç”¨å¯èƒ½ | é«˜ã‚³ã‚¹ãƒˆ  
AWS ã‚¹ãƒãƒƒãƒˆ | åŒä¸Š | $800-1,500 | 70%ã‚³ã‚¹ãƒˆå‰Šæ¸› | ä¸­æ–­ãƒªã‚¹ã‚¯  
Google Cloud Preemptible | åŒä¸Š | $900-1,600 | å®‰ä¾¡ | 24æ™‚é–“åˆ¶é™  
  
* * *

## 5.2 AWS Parallel Clusterã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### å‰ææ¡ä»¶
    
    
    # AWS CLIã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    pip install awscli
    
    # AWSè¨­å®š
    aws configure
    # AWS Access Key ID: [YOUR_KEY]
    # AWS Secret Access Key: [YOUR_SECRET]
    # Default region: us-east-1
    # Default output format: json
    
    # Parallel Cluster CLIã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    pip install aws-parallelcluster
    

### ã‚¯ãƒ©ã‚¹ã‚¿è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

**config.yaml** :
    
    
    Region: us-east-1
    Image:
      Os: alinux2
    
    HeadNode:
      InstanceType: c5.2xlarge  # 8 vCPU, 16 GB RAM
      Networking:
        SubnetId: subnet-12345678
      Ssh:
        KeyName: my-key-pair
    
    Scheduling:
      Scheduler: slurm
      SlurmQueues:
        - Name: compute
          ComputeResources:
            - Name: c5-48xlarge
              InstanceType: c5.24xlarge  # 96 vCPU
              MinCount: 0
              MaxCount: 100  # æœ€å¤§100ãƒãƒ¼ãƒ‰
              DisableSimultaneousMultithreading: true
              Efa:
                Enabled: true  # é«˜é€Ÿãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
          Networking:
            SubnetIds:
              - subnet-12345678
            PlacementGroup:
              Enabled: true  # ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·é…ç½®
    
    SharedStorage:
      - MountDir: /shared
        Name: ebs-shared
        StorageType: Ebs
        EbsSettings:
          VolumeType: gp3
          Size: 1000  # 1 TB
          Encrypted: true
    
      - MountDir: /fsx
        Name: lustre-fs
        StorageType: FsxLustre
        FsxLustreSettings:
          StorageCapacity: 1200  # 1.2 TB
          DeploymentType: SCRATCH_2
    

### ã‚¯ãƒ©ã‚¹ã‚¿ä½œæˆ
    
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ä½œæˆ
    pcluster create-cluster \
      --cluster-name vasp-cluster \
      --cluster-configuration config.yaml
    
    # ä½œæˆçŠ¶æ…‹ç¢ºèª
    pcluster describe-cluster --cluster-name vasp-cluster
    
    # SSHæ¥ç¶š
    pcluster ssh --cluster-name vasp-cluster -i ~/.ssh/my-key-pair.pem
    

### VASPç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ã«SSHæ¥ç¶šå¾Œ
    
    # Intel OneAPI Toolkitï¼ˆVASPã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã«å¿…è¦ï¼‰
    wget https://registrationcenter-download.intel.com/...
    bash l_BaseKit_p_2023.0.0.25537_offline.sh
    
    # VASPã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãŒå¿…è¦ï¼‰
    cd /shared
    tar -xzf vasp.6.3.0.tar.gz
    cd vasp.6.3.0
    
    # makefile.includeç·¨é›†ï¼ˆIntel compilerç”¨ï¼‰
    cp arch/makefile.include.intel makefile.include
    
    # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    make all
    
    # å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…±æœ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®
    cp bin/vasp_std /shared/bin/
    

* * *

## 5.3 ã‚³ã‚¹ãƒˆæœ€é©åŒ–

### ã‚¹ãƒãƒƒãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®æ´»ç”¨

**ã‚¹ãƒãƒƒãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹** ã¯ã€ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ã®60-90%å‰²å¼•ã§åˆ©ç”¨ã§ãã‚‹ä½™å‰°è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã§ã™ã€‚

**config.yamlï¼ˆã‚¹ãƒãƒƒãƒˆè¨­å®šï¼‰** :
    
    
    Scheduling:
      Scheduler: slurm
      SlurmQueues:
        - Name: spot-queue
          CapacityType: SPOT  # ã‚¹ãƒãƒƒãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
          ComputeResources:
            - Name: c5-spot
              InstanceType: c5.24xlarge
              MinCount: 0
              MaxCount: 200
              SpotPrice: 2.50  # æœ€å¤§å…¥æœ­ä¾¡æ ¼ï¼ˆ$/æ™‚ï¼‰
          Networking:
            SubnetIds:
              - subnet-12345678
    

**ã‚¹ãƒãƒƒãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹** :

  1. **ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ** : è¨ˆç®—ã‚’å®šæœŸçš„ã«ä¿å­˜
  2. **è¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—** : ä»£æ›¿ã‚¿ã‚¤ãƒ—ã‚’æŒ‡å®š
  3. **ãƒªãƒˆãƒ©ã‚¤è¨­å®š** : ä¸­æ–­æ™‚ã®è‡ªå‹•ãƒªã‚¹ã‚¿ãƒ¼ãƒˆ

### è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    
    
    Scheduling:
      SlurmSettings:
        ScaledownIdletime: 5  # 5åˆ†ã‚¢ã‚¤ãƒ‰ãƒ«ã§çµ‚äº†
      SlurmQueues:
        - Name: compute
          ComputeResources:
            - Name: c5-instances
              MinCount: 0      # ã‚¢ã‚¤ãƒ‰ãƒ«æ™‚ã¯0ãƒãƒ¼ãƒ‰
              MaxCount: 100    # æœ€å¤§100ãƒãƒ¼ãƒ‰
    

### ã‚³ã‚¹ãƒˆç›£è¦–
    
    
    import boto3
    from datetime import datetime, timedelta
    
    def get_cluster_cost(cluster_name, days=7):
        """
        ã‚¯ãƒ©ã‚¹ã‚¿ã®ã‚³ã‚¹ãƒˆã‚’å–å¾—
    
        Parameters:
        -----------
        cluster_name : str
            ã‚¯ãƒ©ã‚¹ã‚¿å
        days : int
            éå»ä½•æ—¥åˆ†
    
        Returns:
        --------
        cost : float
            ç·ã‚³ã‚¹ãƒˆï¼ˆUSDï¼‰
        """
        ce_client = boto3.client('ce', region_name='us-east-1')
    
        # æœŸé–“è¨­å®š
        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=days)
    
        # Cost Explorer API
        response = ce_client.get_cost_and_usage(
            TimePeriod={
                'Start': start_date.strftime('%Y-%m-%d'),
                'End': end_date.strftime('%Y-%m-%d')
            },
            Granularity='DAILY',
            Metrics=['UnblendedCost'],
            Filter={
                'Tags': {
                    'Key': 'parallelcluster:cluster-name',
                    'Values': [cluster_name]
                }
            }
        )
    
        total_cost = 0
        for result in response['ResultsByTime']:
            cost = float(result['Total']['UnblendedCost']['Amount'])
            total_cost += cost
            print(f"{result['TimePeriod']['Start']}: ${cost:.2f}")
    
        print(f"\nç·ã‚³ã‚¹ãƒˆï¼ˆ{days}æ—¥é–“ï¼‰: ${total_cost:.2f}")
        return total_cost
    
    # ä½¿ç”¨ä¾‹
    get_cluster_cost('vasp-cluster', days=7)
    

* * *

## 5.4 Docker/Singularityã«ã‚ˆã‚‹ã‚³ãƒ³ãƒ†ãƒŠåŒ–

### Dockerfileã®ä½œæˆ

**Dockerfile** :
    
    
    FROM ubuntu:20.04
    
    # åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
    RUN apt-get update && apt-get install -y \
        build-essential \
        gfortran \
        openmpi-bin \
        libopenmpi-dev \
        python3 \
        python3-pip \
        wget \
        && rm -rf /var/lib/apt/lists/*
    
    # Pythonç’°å¢ƒ
    RUN pip3 install --upgrade pip && \
        pip3 install numpy scipy matplotlib \
        ase pymatgen fireworks
    
    # Intel MKLï¼ˆæ•°å€¤è¨ˆç®—ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰
    RUN wget https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB && \
        apt-key add GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB && \
        echo "deb https://apt.repos.intel.com/oneapi all main" > /etc/apt/sources.list.d/oneAPI.list && \
        apt-get update && \
        apt-get install -y intel-oneapi-mkl
    
    # VASPï¼ˆãƒ©ã‚¤ã‚»ãƒ³ã‚¹ä¿æŒè€…ã®ã¿ï¼‰
    # COPY vasp.6.3.0.tar.gz /tmp/
    # RUN cd /tmp && tar -xzf vasp.6.3.0.tar.gz && \
    #     cd vasp.6.3.0 && make all && \
    #     cp bin/vasp_std /usr/local/bin/
    
    # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    WORKDIR /calculations
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚³ãƒãƒ³ãƒ‰
    CMD ["/bin/bash"]
    

### Docker ã‚¤ãƒ¡ãƒ¼ã‚¸ã®ãƒ“ãƒ«ãƒ‰ã¨ãƒ—ãƒƒã‚·ãƒ¥
    
    
    # ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒ“ãƒ«ãƒ‰
    docker build -t my-vasp-env:latest .
    
    # Docker Hubã«ãƒ—ãƒƒã‚·ãƒ¥ï¼ˆå…±æœ‰ç”¨ï¼‰
    docker tag my-vasp-env:latest username/my-vasp-env:latest
    docker push username/my-vasp-env:latest
    
    # Amazon ECRã«ãƒ—ãƒƒã‚·ãƒ¥ï¼ˆAWSç”¨ï¼‰
    aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 123456789012.dkr.ecr.us-east-1.amazonaws.com
    
    docker tag my-vasp-env:latest 123456789012.dkr.ecr.us-east-1.amazonaws.com/my-vasp-env:latest
    docker push 123456789012.dkr.ecr.us-east-1.amazonaws.com/my-vasp-env:latest
    

### Singularityã§HPCå®Ÿè¡Œ

HPCã‚·ã‚¹ãƒ†ãƒ ã§ã¯Dockerã®ä»£ã‚ã‚Šã«Singularityã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
    
    
    # Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã‹ã‚‰Singularityã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ä½œæˆ
    singularity build vasp-env.sif docker://username/my-vasp-env:latest
    
    # Singularityã‚³ãƒ³ãƒ†ãƒŠã§å®Ÿè¡Œ
    singularity exec vasp-env.sif mpirun -np 48 vasp_std
    

**SLURMã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆSingularityä½¿ç”¨ï¼‰** :
    
    
    #!/bin/bash
    #SBATCH --job-name=vasp-singularity
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=48
    #SBATCH --time=24:00:00
    
    # Singularityã‚¤ãƒ¡ãƒ¼ã‚¸
    IMAGE=/shared/containers/vasp-env.sif
    
    # ã‚³ãƒ³ãƒ†ãƒŠå†…ã§VASPå®Ÿè¡Œ
    singularity exec $IMAGE mpirun -np 48 vasp_std
    

* * *

## 5.5 ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹

### ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡ï¼ˆIAMï¼‰

**æœ€å°æ¨©é™ã®åŸå‰‡** :
    
    
    {
      "Version": "2012-10-17",
      "Statement": [
        {
          "Effect": "Allow",
          "Action": [
            "ec2:DescribeInstances",
            "ec2:DescribeVolumes",
            "ec2:RunInstances",
            "ec2:TerminateInstances"
          ],
          "Resource": "*",
          "Condition": {
            "StringEquals": {
              "aws:RequestedRegion": "us-east-1"
            }
          }
        },
        {
          "Effect": "Allow",
          "Action": [
            "s3:GetObject",
            "s3:PutObject"
          ],
          "Resource": "arn:aws:s3:::my-vasp-bucket/*"
        }
      ]
    }
    

### ãƒ‡ãƒ¼ã‚¿æš—å·åŒ–
    
    
    # config.yamlï¼ˆæš—å·åŒ–è¨­å®šï¼‰
    SharedStorage:
      - MountDir: /shared
        Name: ebs-encrypted
        StorageType: Ebs
        EbsSettings:
          VolumeType: gp3
          Size: 1000
          Encrypted: true  # æš—å·åŒ–æœ‰åŠ¹
          KmsKeyId: arn:aws:kms:us-east-1:123456789012:key/12345678-1234-1234-1234-123456789012
    

### å­¦è¡“ãƒ©ã‚¤ã‚»ãƒ³ã‚¹éµå®ˆ

**VASP** ç­‰ã®å•†ç”¨ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚’ã‚¯ãƒ©ã‚¦ãƒ‰ã§ä½¿ç”¨ã™ã‚‹å ´åˆã®æ³¨æ„ç‚¹ï¼š

  1. **ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ç¢ºèª** : ã‚¯ãƒ©ã‚¦ãƒ‰ä½¿ç”¨ãŒè¨±å¯ã•ã‚Œã¦ã„ã‚‹ã‹
  2. **ãƒãƒ¼ãƒ‰ãƒ­ãƒƒã‚¯** : ç‰¹å®šãƒãƒ¼ãƒ‰ã§ã®å®Ÿè¡Œåˆ¶é™
  3. **åŒæ™‚å®Ÿè¡Œæ•°** : ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æ•°ã®åˆ¶é™
  4. **ç›£æŸ»ãƒ­ã‚°** : ä½¿ç”¨å±¥æ­´ã®è¨˜éŒ²

* * *

## 5.6 ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£: 10,000ææ–™ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°

### è¦ä»¶å®šç¾©

**ç›®æ¨™** : 10,000å€‹ã®é…¸åŒ–ç‰©ææ–™ã®ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã‚’6ãƒ¶æœˆä»¥å†…ã«è¨ˆç®—

**åˆ¶ç´„** : \- äºˆç®—: $5,000 \- 1ææ–™ã‚ãŸã‚Šè¨ˆç®—æ™‚é–“: 12æ™‚é–“ï¼ˆ48ã‚³ã‚¢ï¼‰ \- ç·CPUæ™‚é–“: 5,760,000 CPUæ™‚é–“

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ
    
    
    ```mermaid
    flowchart TD
        A[ææ–™ãƒªã‚¹ãƒˆ\n10,000å€‹] --> B[ãƒãƒƒãƒåˆ†å‰²\n100ãƒãƒƒãƒ Ã— 100ææ–™]
        B --> C[AWS Parallel Cluster\nã‚¹ãƒãƒƒãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹]
        C --> D[SLURM ã‚¢ãƒ¬ã‚¤ã‚¸ãƒ§ãƒ–\nåŒæ™‚20ãƒãƒ¼ãƒ‰]
        D --> E[FireWorks\nãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç†]
        E --> F[MongoDB\nçµæœä¿å­˜]
        F --> G[S3\né•·æœŸä¿å­˜]
        G --> H[åˆ†æãƒ»å¯è¦–åŒ–]
    ```

### å®Ÿè£…

**1\. ã‚¯ãƒ©ã‚¹ã‚¿è¨­å®š**
    
    
    # config-10k-materials.yaml
    Scheduling:
      Scheduler: slurm
      SlurmQueues:
        - Name: spot-compute
          CapacityType: SPOT
          ComputeResources:
            - Name: c5-24xlarge-spot
              InstanceType: c5.24xlarge  # 96 vCPU
              MinCount: 0
              MaxCount: 50  # åŒæ™‚50ãƒãƒ¼ãƒ‰ = 4,800ã‚³ã‚¢
              SpotPrice: 2.00
    

**2\. ã‚¸ãƒ§ãƒ–æŠ•å…¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**
    
    
    def run_10k_project():
        """10,000ææ–™ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Ÿè¡Œ"""
    
        # ææ–™ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿
        with open('oxide_materials_10k.txt', 'r') as f:
            materials = [line.strip() for line in f]
    
        print(f"ç·ææ–™æ•°: {len(materials)}")
    
        # 100ãƒãƒƒãƒã«åˆ†å‰²
        batch_size = 100
        n_batches = len(materials) // batch_size
    
        manager = SLURMJobManager()
    
        for batch_id in range(n_batches):
            start = batch_id * batch_size
            end = (batch_id + 1) * batch_size
            batch_materials = materials[start:end]
    
            # ãƒãƒƒãƒç”¨ææ–™ãƒªã‚¹ãƒˆ
            list_file = f'batch_{batch_id:03d}.txt'
            with open(list_file, 'w') as f:
                for mat in batch_materials:
                    f.write(f"{mat}\n")
    
            # ã‚¢ãƒ¬ã‚¤ã‚¸ãƒ§ãƒ–æŠ•å…¥ï¼ˆ100ææ–™ã€åŒæ™‚20ãƒãƒ¼ãƒ‰ï¼‰
            job_id = manager.submit_array_job(
                'vasp_bandgap.sh',
                n_tasks=100,
                max_concurrent=20
            )
    
            print(f"ãƒãƒƒãƒ {batch_id+1}/{n_batches} æŠ•å…¥: ã‚¸ãƒ§ãƒ–ID {job_id}")
    
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆAWS APIåˆ¶é™å¯¾ç­–ï¼‰
            time.sleep(1)
    
    run_10k_project()
    

**3\. ã‚³ã‚¹ãƒˆåˆ†æ**
    
    
    def estimate_project_cost():
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Š"""
    
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        n_materials = 10000
        cpu_hours_per_material = 12
        cores_per_job = 48
    
        total_cpu_hours = n_materials * cpu_hours_per_material
    
        # c5.24xlarge: 96 vCPU, $4.08/æ™‚ï¼ˆã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ï¼‰
        ondemand_cost = total_cpu_hours * (4.08 / 96)
        print(f"ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰: ${ondemand_cost:,.0f}")
    
        # ã‚¹ãƒãƒƒãƒˆ: 70%å‰²å¼•
        spot_cost = ondemand_cost * 0.3
        print(f"ã‚¹ãƒãƒƒãƒˆ: ${spot_cost:,.0f}")
    
        # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸: EBS 1TB Ã— 6ãƒ¶æœˆ
        storage_cost = 0.10 * 1000 * 6  # $0.10/GB/æœˆ
        print(f"ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸: ${storage_cost:,.0f}")
    
        # ãƒ‡ãƒ¼ã‚¿è»¢é€: 500GB
        transfer_cost = 500 * 0.09
        print(f"ãƒ‡ãƒ¼ã‚¿è»¢é€: ${transfer_cost:,.0f}")
    
        total_cost = spot_cost + storage_cost + transfer_cost
        print(f"\nç·ã‚³ã‚¹ãƒˆ: ${total_cost:,.0f}")
    
        return total_cost
    
    estimate_project_cost()
    

**å‡ºåŠ›** :
    
    
    ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰: $5,100
    ã‚¹ãƒãƒƒãƒˆ: $1,530
    ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸: $600
    ãƒ‡ãƒ¼ã‚¿è»¢é€: $45
    
    ç·ã‚³ã‚¹ãƒˆ: $2,175
    

* * *

## 5.7 æ¼”ç¿’å•é¡Œ

### å•é¡Œ1ï¼ˆé›£æ˜“åº¦: mediumï¼‰

**å•é¡Œ** : AWS Parallel Clusterã®ã‚³ã‚¹ãƒˆå‰Šæ¸›ç­–ã‚’3ã¤æŒ™ã’ã€ãã‚Œãã‚Œã®å‰Šæ¸›ç‡ã‚’è¦‹ç©ã‚‚ã£ã¦ãã ã•ã„ã€‚

è§£ç­”ä¾‹ **1. ã‚¹ãƒãƒƒãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½¿ç”¨** \- å‰Šæ¸›ç‡: 70% \- ãƒªã‚¹ã‚¯: ä¸­æ–­ã®å¯èƒ½æ€§ **2. è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ï¼ˆ5åˆ†ã‚¢ã‚¤ãƒ‰ãƒ«ï¼‰** \- å‰Šæ¸›ç‡: 20-30%ï¼ˆã‚¢ã‚¤ãƒ‰ãƒ«æ™‚é–“ã«ã‚ˆã‚‹ï¼‰ \- ãƒªã‚¹ã‚¯: ãªã— **3. ãƒªã‚¶ãƒ¼ãƒ–ãƒ‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆ1å¹´å¥‘ç´„ï¼‰** \- å‰Šæ¸›ç‡: 40% \- ãƒªã‚¹ã‚¯: é•·æœŸã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒ³ãƒˆ **ç·åˆå‰Šæ¸›ç‡**: æœ€å¤§85%ï¼ˆã‚¹ãƒãƒƒãƒˆ + è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰ 

### å•é¡Œ2ï¼ˆé›£æ˜“åº¦: hardï¼‰

**å•é¡Œ** : 5,000ææ–™ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã€äºˆç®—$1,000ã€æœŸé–“3ãƒ¶æœˆã®åˆ¶ç´„ä¸‹ã§æœ€é©ãªå®Ÿè¡Œè¨ˆç”»ã‚’ç«‹ã¦ã¦ãã ã•ã„ã€‚

è§£ç­”ä¾‹ **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: \- ææ–™æ•°: 5,000 \- CPUæ™‚é–“: 5,000 Ã— 12æ™‚é–“ = 60,000æ™‚é–“ \- äºˆç®—: $1,000 \- æœŸé–“: 3ãƒ¶æœˆ = 90æ—¥ **åˆ¶ç´„ã‹ã‚‰é€†ç®—**: ã‚³ã‚¹ãƒˆåˆ¶ç´„: 
    
    
    $1,000 = è¨ˆç®—ã‚³ã‚¹ãƒˆ + ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚³ã‚¹ãƒˆ + è»¢é€ã‚³ã‚¹ãƒˆ
    $1,000 â‰ˆ $800ï¼ˆè¨ˆç®—ï¼‰ + $150ï¼ˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼‰ + $50ï¼ˆè»¢é€ï¼‰
    

c5.24xlarge ã‚¹ãƒãƒƒãƒˆ: $1.22/æ™‚ 
    
    
    ä½¿ç”¨å¯èƒ½æ™‚é–“ = $800 / $1.22 = 656æ™‚é–“
    åŒæ™‚ãƒãƒ¼ãƒ‰æ•° = 60,000 / 656 / 24 = 3.8 â‰ˆ 4ãƒãƒ¼ãƒ‰
    

**å®Ÿè¡Œè¨ˆç”»**: 1\. ã‚¹ãƒãƒƒãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹: c5.24xlarge Ã— 4ãƒãƒ¼ãƒ‰ 2\. åŒæ™‚å®Ÿè¡Œ: 16ææ–™ï¼ˆå„48ã‚³ã‚¢ï¼‰ 3\. 1æ—¥ã‚ãŸã‚Š: 16ææ–™ Ã— 2ãƒãƒƒãƒ = 32ææ–™ 4\. å®Œäº†æœŸé–“: 5,000 / 32 = 156æ—¥ **å•é¡Œ**: æœŸé–“ãŒ3ãƒ¶æœˆï¼ˆ90æ—¥ï¼‰ã‚’è¶…é **è§£æ±ºç­–**: \- åŒæ™‚ãƒãƒ¼ãƒ‰æ•°ã‚’8ã«å¢—åŠ  â†’ ã‚³ã‚¹ãƒˆ$1,600ï¼ˆäºˆç®—è¶…éï¼‰ \- ã¾ãŸã¯ã€1ææ–™ã‚ãŸã‚ŠCPUæ™‚é–“ã‚’8æ™‚é–“ã«å‰Šæ¸›ï¼ˆç²¾åº¦ã¨ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰ 

* * *

## 5.8 ã¾ã¨ã‚

ã“ã®ç« ã§ã¯ã€ã‚¯ãƒ©ã‚¦ãƒ‰HPCã®æ´»ç”¨ã¨ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã‚’å­¦ã³ã¾ã—ãŸã€‚

**ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ** :

  1. **AWS Parallel Cluster** : å¤§è¦æ¨¡HPCç’°å¢ƒã‚’ç°¡å˜ã«æ§‹ç¯‰
  2. **ã‚¹ãƒãƒƒãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹** : 70%ã®ã‚³ã‚¹ãƒˆå‰Šæ¸›
  3. **Docker/Singularity** : ç’°å¢ƒã®å®Œå…¨å†ç¾
  4. **ã‚³ã‚¹ãƒˆç®¡ç†** : è¦‹ç©ã‚‚ã‚Šã¨ç›£è¦–
  5. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£** : æš—å·åŒ–ã¨ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡

**ã‚·ãƒªãƒ¼ã‚ºå®Œäº†ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼**

ã“ã‚Œã§ã€ãƒã‚¤ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—ã®å…¨5ç« ã‚’å®Œäº†ã—ã¾ã—ãŸã€‚ç¬¬1ç« ã®åŸºç¤æ¦‚å¿µã‹ã‚‰ç¬¬5ç« ã®ã‚¯ãƒ©ã‚¦ãƒ‰å®Ÿè£…ã¾ã§ã€å®Ÿè·µçš„ãªã‚¹ã‚­ãƒ«ã‚’ç¿’å¾—ã§ããŸã¯ãšã§ã™ã€‚

**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—** :

  1. å°è¦æ¨¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼ˆ100-1000ææ–™ï¼‰ã‚’å®Ÿè¡Œ
  2. ã‚³ã‚¹ãƒˆã‚’æ¸¬å®šãƒ»æœ€é©åŒ–
  3. çµæœã‚’NOMADã«å…¬é–‹
  4. è«–æ–‡åŸ·ç­†ãƒ»å­¦ä¼šç™ºè¡¨

**[ã‚·ãƒªãƒ¼ã‚ºãƒˆãƒƒãƒ—ã«æˆ»ã‚‹](<./index.html>)**

* * *

## å‚è€ƒæ–‡çŒ®

  1. Amazon Web Services (2023). "AWS Parallel Cluster User Guide." https://docs.aws.amazon.com/parallelcluster/

  2. Kurtzer, G. M., et al. (2017). "Singularity: Scientific containers for mobility of compute." _PLOS ONE_ , 12(5), e0177459.

  3. Merkel, D. (2014). "Docker: lightweight Linux containers for consistent development and deployment." _Linux Journal_ , 2014(239), 2.

  4. NOMAD Laboratory (2023). "NOMAD Repository - FAIR Data Sharing." https://nomad-lab.eu/

  5. Jain, A., et al. (2015). "FireWorks: a dynamic workflow system designed for high-throughput applications." _Concurrency and Computation: Practice and Experience_ , 27(17), 5037-5059.

* * *

**ãƒ©ã‚¤ã‚»ãƒ³ã‚¹** : CC BY 4.0 **ä½œæˆæ—¥** : 2025-10-17 **ä½œæˆè€…** : Dr. Yusuke Hashimoto, Tohoku University

* * *

**ãƒã‚¤ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—å…¥é–€ã‚·ãƒªãƒ¼ã‚ºã‚’å®Œäº†ã—ã¾ã—ãŸï¼**

ææ–™æ¢ç´¢ã®åŠ é€ŸåŒ–ã‚’å®Ÿç¾ã—ã€ä¸–ç•Œã«è²¢çŒ®ã™ã‚‹ç ”ç©¶ã‚’æœŸå¾…ã—ã¦ã„ã¾ã™ã€‚
