---
title: ç¬¬2ç« ï¼šGNNã®åŸºç¤ç†è«–
chapter_title: ç¬¬2ç« ï¼šGNNã®åŸºç¤ç†è«–
subtitle: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã‹ã‚‰ææ–™ç§‘å­¦ç‰¹åŒ–GNNã¾ã§
reading_time: 25-30åˆ†
difficulty: ä¸­ç´š
code_examples: 10
exercises: 3
---

# ç¬¬2ç« ï¼šGNNã®åŸºç¤ç†è«–

ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã®åŸºæœ¬æ©Ÿæ§‹ã‚’æ•°å¼æŠœãã§ã‚‚ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ãã‚‹ã‚ˆã†ã«æ•´ç†ã—ã¾ã™ã€‚ä»£è¡¨ãƒ¢ãƒ‡ãƒ«ã®é•ã„ã¨ä½¿ã„åˆ†ã‘ã‚’æŠ¼ã•ãˆã¾ã™ã€‚

**ğŸ’¡ è£œè¶³:** ä¼ãˆã‚‹é‡ï¼ˆé‡ã¿ï¼‰ã¨å›æ•°ï¼ˆå±¤æ•°ï¼‰ã€å—ã‘å–ã‚Šæ–¹ï¼ˆé›†ç´„ï¼‰ã®ä¸‰ç‚¹ã‚’åˆ†ã‘ã¦è€ƒãˆã‚‹ã¨ç†è§£ãŒæ—©ã„ã§ã™ã€‚

**ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã‹ã‚‰ææ–™ç§‘å­¦ç‰¹åŒ–GNNã¾ã§**

## å­¦ç¿’ç›®æ¨™

ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š

  * âœ… ã‚°ãƒ©ãƒ•ã®æ•°å­¦çš„å®šç¾©ã¨è¡¨ç¾æ–¹æ³•ã‚’ç†è§£ã™ã‚‹
  * âœ… ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã®3ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆé›†ç´„â†’æ›´æ–°â†’å‡ºåŠ›ï¼‰ã‚’èª¬æ˜ã§ãã‚‹
  * âœ… GCNã€GATã€GraphSAGEã®åŸç†ã¨é•ã„ã‚’ç†è§£ã™ã‚‹
  * âœ… ææ–™ç§‘å­¦ç‰¹åŒ–GNNï¼ˆSchNetã€DimeNetï¼‰ã®ç‰¹å¾´ã‚’çŸ¥ã‚‹
  * âœ… ã‚·ãƒ³ãƒ—ãƒ«ãªGNNã‚’PyTorchã§å®Ÿè£…ã§ãã‚‹
  * âœ… ç­‰å¤‰GNNã®é‡è¦æ€§ã‚’ç†è§£ã™ã‚‹

**èª­äº†æ™‚é–“** : 25-30åˆ† **ã‚³ãƒ¼ãƒ‰ä¾‹** : 10å€‹ **æ¼”ç¿’å•é¡Œ** : 3å•

* * *

## 2.1 ã‚°ãƒ©ãƒ•ã®æ•°å­¦çš„å®šç¾©

### ã‚°ãƒ©ãƒ•ã®åŸºæœ¬è¦ç´ 

**å®šç¾©** :

> ã‚°ãƒ©ãƒ• $G = (V, E)$ ã¯ã€é ‚ç‚¹é›†åˆ $V$ ã¨è¾ºé›†åˆ $E \subseteq V \times V$ ã‹ã‚‰ãªã‚‹ã€‚

**è¨˜æ³•** : \- $n = |V|$: é ‚ç‚¹æ•° \- $m = |E|$: è¾ºæ•° \- $\mathcal{N}(v)$: é ‚ç‚¹ $v$ ã®éš£æ¥é ‚ç‚¹é›†åˆ

* * *

### éš£æ¥è¡Œåˆ—ï¼ˆAdjacency Matrixï¼‰

**å®šç¾©** : $$ A \in {0, 1}^{n \times n}, \quad A_{ij} = \begin{cases} 1 & \text{if } (v_i, v_j) \in E \ 0 & \text{otherwise} \end{cases} $$

**Pythonã§ã®å®Ÿè£…** :
    
    
    import numpy as np
    
    # ä¾‹ï¼šä¸‰è§’å½¢ã‚°ãƒ©ãƒ•ï¼ˆ3é ‚ç‚¹ã€3è¾ºï¼‰
    n = 3
    A = np.array([
        [0, 1, 1],  # é ‚ç‚¹0: 1, 2ã«æ¥ç¶š
        [1, 0, 1],  # é ‚ç‚¹1: 0, 2ã«æ¥ç¶š
        [1, 1, 0]   # é ‚ç‚¹2: 0, 1ã«æ¥ç¶š
    ])
    
    print("éš£æ¥è¡Œåˆ—:")
    print(A)
    print(f"\né ‚ç‚¹æ•°: {n}")
    print(f"è¾ºæ•°: {A.sum() // 2}")  # ç„¡å‘ã‚°ãƒ©ãƒ•ã¯2ã§å‰²ã‚‹
    

**å‡ºåŠ›** :
    
    
    éš£æ¥è¡Œåˆ—:
    [[0 1 1]
     [1 0 1]
     [1 1 0]]
    
    é ‚ç‚¹æ•°: 3
    è¾ºæ•°: 3
    

* * *

### æ¬¡æ•°è¡Œåˆ—ï¼ˆDegree Matrixï¼‰

**å®šç¾©** : $$ D \in \mathbb{R}^{n \times n}, \quad D_{ii} = \sum_{j=1}^{n} A_{ij} $$

**ç‰©ç†çš„æ„å‘³** : å„é ‚ç‚¹ã®æ¥ç¶šæ•°ï¼ˆåŒ–å­¦ã§ã¯çµåˆæ•°ï¼‰
    
    
    # æ¬¡æ•°è¡Œåˆ—
    D = np.diag(A.sum(axis=1))
    print("æ¬¡æ•°è¡Œåˆ—:")
    print(D)
    print(f"\nå„é ‚ç‚¹ã®æ¬¡æ•°: {np.diag(D)}")
    

**å‡ºåŠ›** :
    
    
    æ¬¡æ•°è¡Œåˆ—:
    [[2 0 0]
     [0 2 0]
     [0 0 2]]
    
    å„é ‚ç‚¹ã®æ¬¡æ•°: [2 2 2]
    

* * *

### ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³è¡Œåˆ—ï¼ˆLaplacian Matrixï¼‰

**å®šç¾©** : $$ L = D - A $$

**æ­£è¦åŒ–ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³** ï¼ˆGNNã§ã‚ˆãä½¿ç”¨ï¼‰: $$ \tilde{L} = D^{-1/2} L D^{-1/2} = I - D^{-1/2} A D^{-1/2} $$
    
    
    # ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³è¡Œåˆ—
    L = D - A
    print("ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³è¡Œåˆ—:")
    print(L)
    
    # æ­£è¦åŒ–ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³
    D_inv_sqrt = np.diag(1 / np.sqrt(np.diag(D)))
    L_norm = np.eye(n) - D_inv_sqrt @ A @ D_inv_sqrt
    print("\næ­£è¦åŒ–ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³:")
    print(L_norm)
    

**å‡ºåŠ›** :
    
    
    ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³è¡Œåˆ—:
    [[ 2 -1 -1]
     [-1  2 -1]
     [-1 -1  2]]
    
    æ­£è¦åŒ–ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³:
    [[ 1.  -0.5 -0.5]
     [-0.5  1.  -0.5]
     [-0.5 -0.5  1. ]]
    

**ç”¨é€”** : \- ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚°ãƒ©ãƒ•ç†è«– \- ã‚°ãƒ©ãƒ•ãƒ•ãƒ¼ãƒªã‚¨å¤‰æ› \- ã‚°ãƒ©ãƒ•ä¿¡å·å‡¦ç†

* * *

### é ‚ç‚¹ç‰¹å¾´é‡ã¨è¾ºç‰¹å¾´é‡

**é ‚ç‚¹ç‰¹å¾´è¡Œåˆ—** $X \in \mathbb{R}^{n \times d}$: \- å„è¡Œ $x_i \in \mathbb{R}^d$: é ‚ç‚¹ $i$ ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ« \- ææ–™ç§‘å­¦: åŸå­ç•ªå·ã€é›»æ°—é™°æ€§åº¦ã€ä¾¡é›»å­æ•°ãªã©

**è¾ºç‰¹å¾´è¡Œåˆ—** $E \in \mathbb{R}^{m \times d_e}$: \- å„è¡Œ $e_{ij} \in \mathbb{R}^{d_e}$: è¾º $(i, j)$ ã®ç‰¹å¾´ \- ææ–™ç§‘å­¦: çµåˆé•·ã€çµåˆæ¬¡æ•°ã€çµåˆè§’ãªã©
    
    
    # ä¾‹ï¼šæ°´åˆ†å­ï¼ˆHâ‚‚Oï¼‰ã®ç‰¹å¾´é‡
    X = np.array([
        [8, 2.55, 6],   # O: åŸå­ç•ªå·8, é›»æ°—é™°æ€§åº¦2.55, ä¾¡é›»å­6
        [1, 2.20, 1],   # H1
        [1, 2.20, 1]    # H2
    ])
    
    print("é ‚ç‚¹ç‰¹å¾´è¡Œåˆ— (3Ã—3):")
    print(X)
    print(f"å½¢çŠ¶: {X.shape}")
    

* * *

## 2.2 ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã®ä»•çµ„ã¿

### Message Passing Neural Network (MPNN)

GNNã®**çµ±ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯** ã§ã™ï¼ˆGilmer et al., 2017ï¼‰ã€‚

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ** :
    
    
    ```mermaid
    flowchart LR
        A[å…¥åŠ›: é ‚ç‚¹ç‰¹å¾´ X] --> B[ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ]
        B --> C[ã‚¹ãƒ†ãƒƒãƒ—2: é›†ç´„ Aggregation]
        C --> D[ã‚¹ãƒ†ãƒƒãƒ—3: æ›´æ–° Update]
        D --> E{ç¹°ã‚Šè¿”ã—?}
        E -->|Yes| B
        E -->|No| F[å‡ºåŠ›: æ–°ã—ã„ç‰¹å¾´]
    
        style A fill:#e3f2fd
        style B fill:#fff3e0
        style C fill:#f3e5f5
        style D fill:#e8f5e9
        style F fill:#ffebee
    ```

* * *

### ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆï¼ˆMessageï¼‰

**å®šç¾©** : $$ m_{ij}^{(t)} = \text{Message}(h_i^{(t)}, h_j^{(t)}, e_{ij}) $$

  * $h_i^{(t)}$: ãƒ¬ã‚¤ãƒ¤ãƒ¼ $t$ ã§ã®é ‚ç‚¹ $i$ ã®éš ã‚ŒçŠ¶æ…‹
  * $h_j^{(t)}$: éš£æ¥é ‚ç‚¹ $j$ ã®éš ã‚ŒçŠ¶æ…‹
  * $e_{ij}$: è¾º $(i, j)$ ã®ç‰¹å¾´é‡

**æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªå½¢** : $$ m_{ij}^{(t)} = W \cdot h_j^{(t)} $$
    
    
    import torch
    import torch.nn as nn
    
    class MessageFunction(nn.Module):
        def __init__(self, in_dim, out_dim):
            super().__init__()
            self.W = nn.Linear(in_dim, out_dim)
    
        def forward(self, h_j):
            """
            éš£æ¥é ‚ç‚¹ã‹ã‚‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ
    
            Parameters:
            -----------
            h_j : Tensor (num_neighbors, in_dim)
                éš£æ¥é ‚ç‚¹ã®ç‰¹å¾´é‡
    
            Returns:
            --------
            messages : Tensor (num_neighbors, out_dim)
                ç”Ÿæˆã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            """
            return self.W(h_j)
    
    # ä¾‹
    in_dim, out_dim = 16, 32
    msg_fn = MessageFunction(in_dim, out_dim)
    
    # éš£æ¥é ‚ç‚¹ã®ç‰¹å¾´ï¼ˆ3å€‹ã®éš£æ¥é ‚ç‚¹ï¼‰
    h_neighbors = torch.randn(3, in_dim)
    messages = msg_fn(h_neighbors)
    print(f"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢çŠ¶: {messages.shape}")
    # å‡ºåŠ›: torch.Size([3, 32])
    

* * *

### ã‚¹ãƒ†ãƒƒãƒ—2: é›†ç´„ï¼ˆAggregationï¼‰

**å®šç¾©** : $$ m_i^{(t)} = \text{Aggregate}\left( {m_{ij}^{(t)} : j \in \mathcal{N}(i)} \right) $$

**ä»£è¡¨çš„ãªé›†ç´„é–¢æ•°** :

é›†ç´„æ–¹æ³• | æ•°å¼ | ç‰¹å¾´  
---|---|---  
**Sum** | $\sum_{j \in \mathcal{N}(i)} m_{ij}^{(t)}$ | é †åºä¸å¤‰ã€æ¬¡æ•°ã«æ•æ„Ÿ  
**Mean** | $\frac{1}{ | \mathcal{N}(i)  
**Max** | $\max_{j \in \mathcal{N}(i)} m_{ij}^{(t)}$ | æœ€ã‚‚å¼·ã„ç‰¹å¾´ã‚’ä¿æŒ  
**Attention** | $\sum_{j \in \mathcal{N}(i)} \alpha_{ij} m_{ij}^{(t)}$ | é‡è¦åº¦ã§é‡ã¿ä»˜ã‘  
      
    
    class AggregationFunction:
        @staticmethod
        def sum_agg(messages):
            """Sum aggregation"""
            return torch.sum(messages, dim=0)
    
        @staticmethod
        def mean_agg(messages):
            """Mean aggregation"""
            return torch.mean(messages, dim=0)
    
        @staticmethod
        def max_agg(messages):
            """Max aggregation"""
            return torch.max(messages, dim=0)[0]
    
    # ä¾‹
    messages = torch.tensor([
        [1.0, 2.0, 3.0],
        [4.0, 5.0, 6.0],
        [7.0, 8.0, 9.0]
    ])
    
    print("Sum:", AggregationFunction.sum_agg(messages))
    # å‡ºåŠ›: tensor([12., 15., 18.])
    
    print("Mean:", AggregationFunction.mean_agg(messages))
    # å‡ºåŠ›: tensor([4., 5., 6.])
    
    print("Max:", AggregationFunction.max_agg(messages))
    # å‡ºåŠ›: tensor([7., 8., 9.])
    

* * *

### ã‚¹ãƒ†ãƒƒãƒ—3: æ›´æ–°ï¼ˆUpdateï¼‰

**å®šç¾©** : $$ h_i^{(t+1)} = \text{Update}\left( h_i^{(t)}, m_i^{(t)} \right) $$

**å…¸å‹çš„ãªæ›´æ–°å¼** : $$ h_i^{(t+1)} = \sigma\left( W_1 h_i^{(t)} + W_2 m_i^{(t)} \right) $$
    
    
    class UpdateFunction(nn.Module):
        def __init__(self, hidden_dim):
            super().__init__()
            self.W1 = nn.Linear(hidden_dim, hidden_dim)
            self.W2 = nn.Linear(hidden_dim, hidden_dim)
            self.activation = nn.ReLU()
    
        def forward(self, h_i, m_i):
            """
            é ‚ç‚¹ç‰¹å¾´ã‚’æ›´æ–°
    
            Parameters:
            -----------
            h_i : Tensor (hidden_dim,)
                ç¾åœ¨ã®é ‚ç‚¹ç‰¹å¾´
            m_i : Tensor (hidden_dim,)
                é›†ç´„ã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    
            Returns:
            --------
            h_new : Tensor (hidden_dim,)
                æ›´æ–°ã•ã‚ŒãŸé ‚ç‚¹ç‰¹å¾´
            """
            return self.activation(self.W1(h_i) + self.W2(m_i))
    
    # ä¾‹
    hidden_dim = 32
    update_fn = UpdateFunction(hidden_dim)
    
    h_current = torch.randn(hidden_dim)
    m_aggregated = torch.randn(hidden_dim)
    h_new = update_fn(h_current, m_aggregated)
    
    print(f"æ›´æ–°å‰: {h_current[:5]}")
    print(f"æ›´æ–°å¾Œ: {h_new[:5]}")
    

* * *

### ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã®å…¨ä½“åƒ
    
    
    class SimpleGNN(nn.Module):
        def __init__(self, in_dim, hidden_dim, num_layers):
            super().__init__()
            self.num_layers = num_layers
    
            # å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            self.message_fns = nn.ModuleList([
                MessageFunction(hidden_dim, hidden_dim)
                for _ in range(num_layers)
            ])
            self.update_fns = nn.ModuleList([
                UpdateFunction(hidden_dim)
                for _ in range(num_layers)
            ])
    
            # å…¥åŠ›å¤‰æ›
            self.input_proj = nn.Linear(in_dim, hidden_dim)
    
        def forward(self, x, edge_index):
            """
            Parameters:
            -----------
            x : Tensor (num_nodes, in_dim)
                é ‚ç‚¹ç‰¹å¾´è¡Œåˆ—
            edge_index : Tensor (2, num_edges)
                è¾ºã®ãƒªã‚¹ãƒˆ [[src], [dst]]
    
            Returns:
            --------
            h : Tensor (num_nodes, hidden_dim)
                æ›´æ–°ã•ã‚ŒãŸé ‚ç‚¹ç‰¹å¾´
            """
            # å…¥åŠ›å¤‰æ›
            h = self.input_proj(x)
    
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼
            for layer in range(self.num_layers):
                h_new = []
    
                # å„é ‚ç‚¹ã‚’æ›´æ–°
                for i in range(x.size(0)):
                    # éš£æ¥é ‚ç‚¹ã‚’å–å¾—
                    neighbors = edge_index[1][edge_index[0] == i]
    
                    if len(neighbors) > 0:
                        # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
                        messages = self.message_fns[layer](h[neighbors])
    
                        # ã‚¹ãƒ†ãƒƒãƒ—2: é›†ç´„
                        m_i = torch.mean(messages, dim=0)
    
                        # ã‚¹ãƒ†ãƒƒãƒ—3: æ›´æ–°
                        h_i_new = self.update_fns[layer](h[i], m_i)
                    else:
                        # éš£æ¥é ‚ç‚¹ãŒãªã„å ´åˆ
                        h_i_new = h[i]
    
                    h_new.append(h_i_new)
    
                h = torch.stack(h_new)
    
            return h
    
    # ä½¿ç”¨ä¾‹
    model = SimpleGNN(in_dim=16, hidden_dim=32, num_layers=3)
    
    # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ï¼ˆä¸‰è§’å½¢ï¼‰
    x = torch.randn(3, 16)  # 3é ‚ç‚¹ã€16æ¬¡å…ƒç‰¹å¾´
    edge_index = torch.tensor([
        [0, 0, 1, 1, 2, 2],  # å§‹ç‚¹
        [1, 2, 0, 2, 0, 1]   # çµ‚ç‚¹
    ])
    
    # é †ä¼æ’­
    h_out = model(x, edge_index)
    print(f"å‡ºåŠ›å½¢çŠ¶: {h_out.shape}")
    # å‡ºåŠ›: torch.Size([3, 32])
    

* * *

## 2.3 ä»£è¡¨çš„ãªGNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### Graph Convolutional Network (GCN)

**è«–æ–‡** : Kipf & Welling (2017), _ICLR_

**æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢** : ã‚°ãƒ©ãƒ•ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ç•³ã¿è¾¼ã¿

**æ›´æ–°å¼** : $$ H^{(l+1)} = \sigma\left( \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} H^{(l)} W^{(l)} \right) $$

  * $\tilde{A} = A + I$: è‡ªå·±ãƒ«ãƒ¼ãƒ—ä»˜ãéš£æ¥è¡Œåˆ—
  * $\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}$: æ¬¡æ•°è¡Œåˆ—
  * $H^{(l)} \in \mathbb{R}^{n \times d}$: ãƒ¬ã‚¤ãƒ¤ãƒ¼ $l$ ã®ç‰¹å¾´é‡
  * $W^{(l)} \in \mathbb{R}^{d \times d'}$: å­¦ç¿’å¯èƒ½ãªé‡ã¿

**Pythonã§ã®å®Ÿè£…** :
    
    
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    class GCNLayer(nn.Module):
        def __init__(self, in_features, out_features):
            super().__init__()
            self.linear = nn.Linear(in_features, out_features)
    
        def forward(self, X, A):
            """
            Parameters:
            -----------
            X : Tensor (num_nodes, in_features)
                é ‚ç‚¹ç‰¹å¾´è¡Œåˆ—
            A : Tensor (num_nodes, num_nodes)
                éš£æ¥è¡Œåˆ—
    
            Returns:
            --------
            H : Tensor (num_nodes, out_features)
                æ›´æ–°ã•ã‚ŒãŸç‰¹å¾´é‡
            """
            # è‡ªå·±ãƒ«ãƒ¼ãƒ—ã®è¿½åŠ 
            A_tilde = A + torch.eye(A.size(0), device=A.device)
    
            # æ¬¡æ•°è¡Œåˆ—
            D_tilde = torch.diag(A_tilde.sum(dim=1))
    
            # æ­£è¦åŒ–: D^(-1/2) * A * D^(-1/2)
            D_inv_sqrt = torch.diag(1.0 / torch.sqrt(D_tilde.diagonal()))
            A_norm = D_inv_sqrt @ A_tilde @ D_inv_sqrt
    
            # ã‚°ãƒ©ãƒ•ç•³ã¿è¾¼ã¿
            H = A_norm @ X
            H = self.linear(H)
            return F.relu(H)
    
    # ä½¿ç”¨ä¾‹
    gcn = GCNLayer(in_features=16, out_features=32)
    
    # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿
    X = torch.randn(5, 16)  # 5é ‚ç‚¹ã€16æ¬¡å…ƒ
    A = torch.tensor([
        [0, 1, 1, 0, 0],
        [1, 0, 1, 1, 0],
        [1, 1, 0, 1, 1],
        [0, 1, 1, 0, 1],
        [0, 0, 1, 1, 0]
    ], dtype=torch.float32)
    
    H = gcn(X, A)
    print(f"GCNå‡ºåŠ›å½¢çŠ¶: {H.shape}")
    # å‡ºåŠ›: torch.Size([5, 32])
    

**ç‰¹å¾´** : \- âœ… ã‚·ãƒ³ãƒ—ãƒ«ã§é«˜é€Ÿ \- âœ… éå‰°å¹³æ»‘åŒ–ï¼ˆover-smoothingï¼‰ã«æ³¨æ„ \- âœ… å›ºå®šçš„ãªé‡ã¿ï¼ˆå…¨éš£æ¥é ‚ç‚¹ãŒåŒã˜æ‰±ã„ï¼‰

* * *

### Graph Attention Network (GAT)

**è«–æ–‡** : VeliÄkoviÄ‡ et al. (2018), _ICLR_

**æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢** : Attentionã§é‡è¦ãªéš£æ¥é ‚ç‚¹ã‚’é‡è¦–

**Attentionä¿‚æ•°** : $$ \alpha_{ij} = \frac{\exp\left( \text{LeakyReLU}(a^T [W h_i | W h_j]) \right)} {\sum_{k \in \mathcal{N}(i)} \exp\left( \text{LeakyReLU}(a^T [W h_i | W h_k]) \right)} $$

**æ›´æ–°å¼** : $$ h_i^{(l+1)} = \sigma\left( \sum_{j \in \mathcal{N}(i)} \alpha_{ij} W^{(l)} h_j^{(l)} \right) $$
    
    
    class GATLayer(nn.Module):
        def __init__(self, in_features, out_features, dropout=0.6,
                     alpha=0.2):
            super().__init__()
            self.W = nn.Linear(in_features, out_features, bias=False)
            self.a = nn.Parameter(torch.zeros(2 * out_features, 1))
            self.leakyrelu = nn.LeakyReLU(alpha)
            self.dropout = nn.Dropout(dropout)
    
            nn.init.xavier_uniform_(self.a.data, gain=1.414)
    
        def forward(self, X, A):
            """
            Parameters:
            -----------
            X : Tensor (num_nodes, in_features)
            A : Tensor (num_nodes, num_nodes)
    
            Returns:
            --------
            H : Tensor (num_nodes, out_features)
            """
            # ç·šå½¢å¤‰æ›
            Wh = self.W(X)  # (N, out_features)
            N = Wh.size(0)
    
            # Attentionè¨ˆç®—
            # [Wh_i || Wh_j] for all edges
            Wh_repeat_interleave = Wh.repeat_interleave(N, dim=0)
            Wh_repeat = Wh.repeat(N, 1)
            concat = torch.cat([Wh_repeat_interleave, Wh_repeat], dim=1)
            concat = concat.view(N, N, -1)
    
            # Attention score
            e = self.leakyrelu(concat @ self.a).squeeze(2)
    
            # ãƒã‚¹ã‚¯ï¼ˆè¾ºãŒãªã„å ´åˆã¯-infï¼‰
            zero_vec = -9e15 * torch.ones_like(e)
            attention = torch.where(A > 0, e, zero_vec)
    
            # Softmax
            attention = F.softmax(attention, dim=1)
            attention = self.dropout(attention)
    
            # Weighted sum
            H = torch.matmul(attention, Wh)
            return F.elu(H)
    
    # ä½¿ç”¨ä¾‹
    gat = GATLayer(in_features=16, out_features=32)
    H_gat = gat(X, A)
    print(f"GATå‡ºåŠ›å½¢çŠ¶: {H_gat.shape}")
    # å‡ºåŠ›: torch.Size([5, 32])
    

**ç‰¹å¾´** : \- âœ… å‹•çš„ãªé‡ã¿ï¼ˆé‡è¦ãªéš£æ¥é ‚ç‚¹ã‚’è‡ªå‹•å­¦ç¿’ï¼‰ \- âœ… è§£é‡ˆå¯èƒ½æ€§ï¼ˆAttentionä¿‚æ•°ã®å¯è¦–åŒ–ï¼‰ \- âŒ è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ï¼ˆGCNã®ç´„2å€ï¼‰

* * *

### GraphSAGEï¼ˆSAmple and aggreGatEï¼‰

**è«–æ–‡** : Hamilton et al. (2017), _NeurIPS_

**æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢** : ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ã®ãŸã‚ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

**æ›´æ–°å¼** : $$ h_i^{(l+1)} = \sigma\left( W \cdot \text{Concat}\left( h_i^{(l)}, \text{Aggregate}({h_j^{(l)} : j \in \mathcal{S}(i)}) \right) \right) $$

  * $\mathcal{S}(i)$: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸéš£æ¥é ‚ç‚¹ï¼ˆå…¨ã¦ã§ã¯ãªã„ï¼‰

    
    
    class GraphSAGELayer(nn.Module):
        def __init__(self, in_features, out_features, num_samples=10):
            super().__init__()
            self.num_samples = num_samples
            # Concatç‰ˆ: å…¥åŠ›ã¯ in_features * 2
            self.linear = nn.Linear(in_features * 2, out_features)
    
        def forward(self, X, A):
            """
            Parameters:
            -----------
            X : Tensor (num_nodes, in_features)
            A : Tensor (num_nodes, num_nodes)
    
            Returns:
            --------
            H : Tensor (num_nodes, out_features)
            """
            N = X.size(0)
            H_new = []
    
            for i in range(N):
                # éš£æ¥é ‚ç‚¹ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                neighbors = torch.nonzero(A[i]).squeeze()
                if neighbors.numel() > self.num_samples:
                    # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                    perm = torch.randperm(neighbors.numel())
                    sampled = neighbors[perm[:self.num_samples]]
                else:
                    sampled = neighbors
    
                # é›†ç´„ï¼ˆMeanï¼‰
                if sampled.numel() > 0:
                    h_neighbors = X[sampled]
                    h_agg = torch.mean(h_neighbors, dim=0)
                else:
                    h_agg = torch.zeros_like(X[i])
    
                # Concat
                h_concat = torch.cat([X[i], h_agg], dim=0)
    
                # ç·šå½¢å¤‰æ›
                h_new = self.linear(h_concat)
                H_new.append(h_new)
    
            H = torch.stack(H_new)
            return F.relu(H)
    
    # ä½¿ç”¨ä¾‹
    sage = GraphSAGELayer(in_features=16, out_features=32,
                          num_samples=3)
    H_sage = sage(X, A)
    print(f"GraphSAGEå‡ºåŠ›å½¢çŠ¶: {H_sage.shape}")
    # å‡ºåŠ›: torch.Size([5, 32])
    

**ç‰¹å¾´** : \- âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ï¼ˆå¤§è¦æ¨¡ã‚°ãƒ©ãƒ•ã«å¯¾å¿œï¼‰ \- âœ… ãƒŸãƒ‹ãƒãƒƒãƒè¨“ç·´ãŒå¯èƒ½ \- âœ… å¸°ç´çš„å­¦ç¿’ï¼ˆæ–°ã—ã„é ‚ç‚¹ã¸ã®æ±åŒ–ï¼‰

* * *

### 3ã¤ã®GNNã®æ¯”è¼ƒ
    
    
    ```mermaid
    flowchart TD
        A[GNNé¸æŠ] --> B{ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º}
        B -->|å°è¦æ¨¡\n10ké ‚ç‚¹| C[GCN]
        B -->|ä¸­è¦æ¨¡\n10k-100k| D[GAT]
        B -->|å¤§è¦æ¨¡\n100k+| E[GraphSAGE]
    
        C --> F[ã‚·ãƒ³ãƒ—ãƒ«ã€é«˜é€Ÿ]
        D --> G[é«˜ç²¾åº¦ã€è§£é‡ˆæ€§]
        E --> H[ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«]
    
        style A fill:#e3f2fd
        style C fill:#fff3e0
        style D fill:#f3e5f5
        style E fill:#e8f5e9
    ```

æ‰‹æ³• | è¨ˆç®—é‡ | ç²¾åº¦ | ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ | è§£é‡ˆæ€§ | æ¨å¥¨ç”¨é€”  
---|---|---|---|---|---  
**GCN** | $O(m \cdot d^2)$ | ä¸­ | ä½ | ä¸­ | å°è¦æ¨¡ã€ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°  
**GAT** | $O(m \cdot d^2 + n \cdot d)$ | é«˜ | ä¸­ | é«˜ | ä¸­è¦æ¨¡ã€é«˜ç²¾åº¦è¦æ±‚  
**GraphSAGE** | $O(k \cdot s \cdot d^2)$ | ä¸­ã€œé«˜ | é«˜ | ä¸­ | å¤§è¦æ¨¡ã€å®Ÿæ™‚é–“äºˆæ¸¬  
  
  * $m$: è¾ºæ•°
  * $n$: é ‚ç‚¹æ•°
  * $d$: ç‰¹å¾´æ¬¡å…ƒ
  * $k$: ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
  * $s$: ã‚µãƒ³ãƒ—ãƒ«æ•°

* * *

## 2.4 ææ–™ç§‘å­¦ç‰¹åŒ–GNN

### SchNetï¼ˆContinuous-filter Convolutional NNï¼‰

**è«–æ–‡** : SchÃ¼tt et al. (2017), _NeurIPS_

**å¯¾è±¡** : åˆ†å­ãƒ»ææ–™ã®**é‡å­åŒ–å­¦ç‰¹æ€§** äºˆæ¸¬

**æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢** : 1\. **é€£ç¶šãƒ•ã‚£ãƒ«ã‚¿** : é›¢æ•£ã‚°ãƒ©ãƒ•ã§ã¯ãªã3Dç©ºé–“ã§ã®ç•³ã¿è¾¼ã¿ 2\. **è·é›¢ä¾å­˜** : åŸå­é–“è·é›¢ã‚’æ˜ç¤ºçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£** :
    
    
    ```mermaid
    flowchart LR
        A[åŸå­ç‰¹å¾´] --> B[åŸ‹ã‚è¾¼ã¿å±¤]
        B --> C[ç›¸äº’ä½œç”¨ãƒ–ãƒ­ãƒƒã‚¯ 1]
        C --> D[ç›¸äº’ä½œç”¨ãƒ–ãƒ­ãƒƒã‚¯ 2]
        D --> E[ç›¸äº’ä½œç”¨ãƒ–ãƒ­ãƒƒã‚¯ 3]
        E --> F[å‡ºåŠ›å±¤]
    
        G[åŸå­é–“è·é›¢] --> C
        G --> D
        G --> E
    
        style A fill:#e3f2fd
        style B fill:#fff3e0
        style C fill:#f3e5f5
        style D fill:#e8f5e9
        style E fill:#ffebee
        style F fill:#fff9c4
        style G fill:#e1bee7
    ```

**æ•°å¼** : $$ h_i^{(l+1)} = h_i^{(l)} + \sum_{j \in \mathcal{N}(i)} h_j^{(l)} \odot \phi\left( |r_i - r_j| \right) $$

  * $\phi(d)$: **é€£ç¶šãƒ•ã‚£ãƒ«ã‚¿é–¢æ•°** ï¼ˆè·é›¢ $d$ ã«ä¾å­˜ï¼‰
  * $r_i, r_j$: åŸå­ã®3Dåº§æ¨™

**ãƒ•ã‚£ãƒ«ã‚¿é–¢æ•°** : $$ \phi(d) = \sum_{k=1}^{K} w_k \exp\left( -\gamma (d - \mu_k)^2 \right) $$

  * ã‚¬ã‚¦ã‚¹åŸºåº•å±•é–‹ï¼ˆRBF: Radial Basis Functionï¼‰

    
    
    import torch
    import torch.nn as nn
    
    class GaussianBasis(nn.Module):
        def __init__(self, start=0.0, stop=5.0, num_gaussians=50):
            super().__init__()
            self.mu = nn.Parameter(
                torch.linspace(start, stop, num_gaussians),
                requires_grad=False
            )
            self.gamma = nn.Parameter(
                torch.tensor(10.0),
                requires_grad=True
            )
    
        def forward(self, distances):
            """
            Parameters:
            -----------
            distances : Tensor (num_edges,)
                åŸå­é–“è·é›¢
    
            Returns:
            --------
            rbf : Tensor (num_edges, num_gaussians)
                ã‚¬ã‚¦ã‚¹åŸºåº•å±•é–‹
            """
            # (num_edges, 1) - (1, num_gaussians)
            diff = distances.unsqueeze(-1) - self.mu.unsqueeze(0)
            rbf = torch.exp(-self.gamma * diff ** 2)
            return rbf
    
    class SchNetInteraction(nn.Module):
        def __init__(self, hidden_dim, num_gaussians):
            super().__init__()
            self.rbf_layer = GaussianBasis(num_gaussians=num_gaussians)
            self.filter_net = nn.Sequential(
                nn.Linear(num_gaussians, hidden_dim),
                nn.Softplus(),
                nn.Linear(hidden_dim, hidden_dim)
            )
            self.linear = nn.Linear(hidden_dim, hidden_dim)
    
        def forward(self, h, edge_index, distances):
            """
            Parameters:
            -----------
            h : Tensor (num_atoms, hidden_dim)
                åŸå­ç‰¹å¾´
            edge_index : Tensor (2, num_edges)
                è¾ºã®ãƒªã‚¹ãƒˆ
            distances : Tensor (num_edges,)
                åŸå­é–“è·é›¢
    
            Returns:
            --------
            h_new : Tensor (num_atoms, hidden_dim)
                æ›´æ–°ã•ã‚ŒãŸç‰¹å¾´
            """
            # RBFå±•é–‹
            rbf = self.rbf_layer(distances)
    
            # ãƒ•ã‚£ãƒ«ã‚¿ç”Ÿæˆ
            W = self.filter_net(rbf)
    
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°
            src, dst = edge_index
            messages = h[dst] * W  # è¦ç´ ç©
    
            # é›†ç´„
            h_agg = torch.zeros_like(h)
            h_agg.index_add_(0, src, messages)
    
            # æ›´æ–°
            h_new = h + self.linear(h_agg)
            return h_new
    
    # ä½¿ç”¨ä¾‹
    schnet_layer = SchNetInteraction(hidden_dim=128,
                                     num_gaussians=50)
    
    # ãƒ‡ãƒ¼ã‚¿
    num_atoms = 5
    h = torch.randn(num_atoms, 128)
    edge_index = torch.tensor([[0, 1, 2, 3], [1, 2, 3, 4]])
    distances = torch.tensor([1.5, 1.8, 2.0, 1.6])
    
    h_new = schnet_layer(h, edge_index, distances)
    print(f"SchNetå‡ºåŠ›å½¢çŠ¶: {h_new.shape}")
    # å‡ºåŠ›: torch.Size([5, 128])
    

**é©ç”¨ä¾‹** : \- QM9ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆåˆ†å­ç‰¹æ€§äºˆæ¸¬ï¼‰ \- MD17ï¼ˆåˆ†å­å‹•åŠ›å­¦ï¼‰ \- OC20ï¼ˆè§¦åª’å¸ç€ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰

**æ€§èƒ½** :
    
    
    QM9 HOMO-LUMO gap:
    - DFTè¨ˆç®—: 24æ™‚é–“/åˆ†å­
    - SchNet: 0.01ç§’/åˆ†å­ï¼ˆMAE=0.04 eVï¼‰
    

* * *

### DimeNetï¼ˆDirectional Message Passing NNï¼‰

**è«–æ–‡** : Klicpera et al. (2020), _ICLR_

**æ‹¡å¼µ** : **çµåˆè§’** ã‚‚è€ƒæ…®

**æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢** : \- è·é›¢ã ã‘ã§ãªã**è§’åº¦æƒ…å ±** ã‚‚åˆ©ç”¨ \- 3ä½“ç›¸äº’ä½œç”¨ï¼ˆtriplet interactionï¼‰

**æ›´æ–°å¼** : $$ m_{ij} = \sum_{k \in \mathcal{N}(j) \setminus {i}} W\left( d_{ij}, d_{jk}, \theta_{ijk} \right) h_k $$

  * $\theta_{ijk}$: è§’åº¦ $\angle i-j-k$

    
    
    ```mermaid
    flowchart TD
        A[åŸå­ i] --|d_ij| B[åŸå­ j]
        B --|d_jk| C[åŸå­ k]
        A - è§’åº¦Î¸_ijk .-> C
    
        style A fill:#e3f2fd
        style B fill:#fff3e0
        style C fill:#f3e5f5
    ```

**è§’åº¦ã®è¨ˆç®—** :
    
    
    import torch
    
    def compute_angle(pos_i, pos_j, pos_k):
        """
        3åŸå­é–“ã®è§’åº¦ã‚’è¨ˆç®—
    
        Parameters:
        -----------
        pos_i, pos_j, pos_k : Tensor (3,)
            åŸå­ã®3Dåº§æ¨™
    
        Returns:
        --------
        angle : Tensor (1,)
            è§’åº¦ï¼ˆãƒ©ã‚¸ã‚¢ãƒ³ï¼‰
        """
        # ãƒ™ã‚¯ãƒˆãƒ«
        v_ij = pos_j - pos_i
        v_jk = pos_k - pos_j
    
        # å†…ç©
        cos_angle = torch.dot(v_ij, v_jk) / (
            torch.norm(v_ij) * torch.norm(v_jk) + 1e-8
        )
    
        # è§’åº¦
        angle = torch.acos(torch.clamp(cos_angle, -1.0, 1.0))
        return angle
    
    # ä¾‹ï¼šæ°´åˆ†å­ã®çµåˆè§’ï¼ˆH-O-Hï¼‰
    pos_O = torch.tensor([0.0, 0.0, 0.0])
    pos_H1 = torch.tensor([0.96, 0.0, 0.0])
    pos_H2 = torch.tensor([0.24, 0.93, 0.0])
    
    angle = compute_angle(pos_H1, pos_O, pos_H2)
    print(f"H-O-Hè§’åº¦: {torch.rad2deg(angle):.1f}Â°")
    # å‡ºåŠ›: 104.5Â°ï¼ˆå®Ÿæ¸¬å€¤ã¨ã»ã¼ä¸€è‡´ï¼‰
    

**æ€§èƒ½** :
    
    
    QM9ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:
    - SchNet: MAE=0.041 eV
    - DimeNet: MAE=0.033 eVï¼ˆ20%æ”¹å–„ï¼‰
    
    è¨ˆç®—æ™‚é–“:
    - SchNet: 0.01ç§’/åˆ†å­
    - DimeNet: 0.05ç§’/åˆ†å­ï¼ˆ5å€é…ã„ï¼‰
    

* * *

### GemNetï¼ˆGeometric Message Passing NNï¼‰

**è«–æ–‡** : Gasteiger et al. (2021), _NeurIPS_

**ã•ã‚‰ãªã‚‹æ‹¡å¼µ** : **4ä½“ç›¸äº’ä½œç”¨** ï¼ˆäºŒé¢è§’ï¼‰

**å¯¾è±¡** : çµæ™¶æ§‹é€ ã€è¤‡é›‘ãªåˆ†å­

**æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢** : \- äºŒé¢è§’ï¼ˆtorsion angleï¼‰ã®è€ƒæ…® \- ã‚ˆã‚Šé«˜æ¬¡ã®å¹¾ä½•å­¦çš„æƒ…å ±
    
    
    ```mermaid
    flowchart LR
        A[åŸå­ i] --- B[åŸå­ j]
        B --- C[åŸå­ k]
        C --- D[åŸå­ l]
    
        A - äºŒé¢è§’Ï† .-> D
    
        style A fill:#e3f2fd
        style B fill:#fff3e0
        style C fill:#f3e5f5
        style D fill:#e8f5e9
    ```

**æ€§èƒ½** :
    
    
    OC20ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆè§¦åª’ï¼‰:
    - SchNet: MAE=0.61 eV
    - DimeNet++: MAE=0.49 eV
    - GemNet: MAE=0.43 eVï¼ˆæœ€é«˜ç²¾åº¦ï¼‰
    

* * *

### ææ–™ç§‘å­¦GNNã®æ¯”è¼ƒ

æ‰‹æ³• | è€ƒæ…®ã™ã‚‹æƒ…å ± | ç²¾åº¦ | é€Ÿåº¦ | æ¨å¥¨ç”¨é€”  
---|---|---|---|---  
**SchNet** | è·é›¢ | ä¸­ | é€Ÿã„ | åˆ†å­ç‰¹æ€§äºˆæ¸¬  
**DimeNet** | è·é›¢ + è§’åº¦ | é«˜ | ä¸­ | è§¦åª’ã€è¤‡é›‘ãªåˆ†å­  
**GemNet** | è·é›¢ + è§’åº¦ + äºŒé¢è§’ | æœ€é«˜ | é…ã„ | çµæ™¶ã€é«˜ç²¾åº¦è¦æ±‚  
  
* * *

## 2.5 ç­‰å¤‰æ€§ï¼ˆEquivarianceï¼‰ã®é‡è¦æ€§

### ç­‰å¤‰æ€§ã¨ã¯

**å®šç¾©** :

> é–¢æ•° $f$ ãŒå¤‰æ› $T$ ã«å¯¾ã—ã¦**ç­‰å¤‰** ï¼ˆequivariantï¼‰ã§ã‚ã‚‹ã¨ã¯ã€ $$f(T(x)) = T(f(x))$$ ãŒæˆã‚Šç«‹ã¤ã“ã¨ã€‚

**ææ–™ç§‘å­¦ã§ã®æ„å‘³** : \- åˆ†å­ã‚’å›è»¢ãƒ»ä¸¦é€²ã—ã¦ã‚‚ã€äºˆæ¸¬ã¯åŒã˜ï¼ˆã¾ãŸã¯å¯¾å¿œã™ã‚‹å¤‰æ›ï¼‰

* * *

### E(3)ç­‰å¤‰æ€§

**E(3)ç¾¤** : 3æ¬¡å…ƒãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ç©ºé–“ã®ç­‰é•·å¤‰æ› \- å›è»¢ï¼ˆRotationï¼‰ \- ä¸¦é€²ï¼ˆTranslationï¼‰ \- åè»¢ï¼ˆInversionï¼‰

**é‡è¦æ€§** : \- ç‰©ç†æ³•å‰‡ã¯åº§æ¨™ç³»ã«ä¾å­˜ã—ãªã„ \- GNNã‚‚åŒæ§˜ã§ã‚ã‚‹ã¹ã

* * *

### ç­‰å¤‰GNNã®ä¾‹ï¼šNequIPã€MACE

**NequIP** (Batzner et al., 2022): \- **E(3)ç­‰å¤‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°** \- çƒé¢èª¿å’Œé–¢æ•°ï¼ˆSpherical Harmonicsï¼‰ã®åˆ©ç”¨

**æ›´æ–°å¼** : $$ m_{ij} = \phi\left( |r_i - r_j| \right) \otimes Y_l(r_{ij}) $$

  * $Y_l$: çƒé¢èª¿å’Œé–¢æ•°ï¼ˆè§’åº¦æƒ…å ±ã‚’ä¿æŒï¼‰
  * $\otimes$: ãƒ†ãƒ³ã‚½ãƒ«ç©

**MACE** (Batatia et al., 2022): \- **é«˜æ¬¡ã®ç­‰å¤‰æ€§** \- ã‚ˆã‚Šæ­£ç¢ºãªåŠ›å ´ï¼ˆforce fieldï¼‰äºˆæ¸¬

**æ€§èƒ½** :
    
    
    MD17ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆåˆ†å­å‹•åŠ›å­¦ï¼‰:
    - SchNet: MAE(åŠ›) = 0.21 kcal/mol/Ã…
    - NequIP: MAE(åŠ›) = 0.05 kcal/mol/Ã…ï¼ˆ76%æ”¹å–„ï¼‰
    

* * *

### ç­‰å¤‰æ€§ã®ãƒ†ã‚¹ãƒˆ
    
    
    import torch
    import torch.nn as nn
    
    def test_equivariance(model, pos, edge_index):
        """
        ãƒ¢ãƒ‡ãƒ«ã®ç­‰å¤‰æ€§ã‚’ãƒ†ã‚¹ãƒˆ
        """
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®äºˆæ¸¬
        pred_original = model(pos, edge_index)
    
        # å›è»¢è¡Œåˆ—ï¼ˆ90åº¦å›è»¢ï¼‰
        angle = torch.tensor(torch.pi / 2)
        rotation = torch.tensor([
            [torch.cos(angle), -torch.sin(angle), 0],
            [torch.sin(angle), torch.cos(angle), 0],
            [0, 0, 1]
        ])
    
        # åº§æ¨™ã‚’å›è»¢
        pos_rotated = pos @ rotation.T
    
        # å›è»¢å¾Œã®äºˆæ¸¬
        pred_rotated = model(pos_rotated, edge_index)
    
        # äºˆæ¸¬ã‚’å›è»¢
        pred_original_rotated = pred_original @ rotation.T
    
        # èª¤å·®ã‚’è¨ˆç®—
        error = torch.abs(pred_rotated - pred_original_rotated).mean()
        print(f"ç­‰å¤‰æ€§èª¤å·®: {error.item():.6f}")
    
        if error < 1e-5:
            print("âœ… ãƒ¢ãƒ‡ãƒ«ã¯ç­‰å¤‰ã§ã™")
        else:
            print("âŒ ãƒ¢ãƒ‡ãƒ«ã¯ç­‰å¤‰ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
    
    # ä½¿ç”¨ä¾‹ï¼ˆç°¡ç•¥ç‰ˆï¼‰
    class SimpleEquivariantModel(nn.Module):
        def forward(self, pos, edge_index):
            # ç°¡ç•¥åŒ–: åº§æ¨™ã®å·®åˆ†ã‚’è¨ˆç®—ï¼ˆç­‰å¤‰ï¼‰
            src, dst = edge_index
            diff = pos[dst] - pos[src]
            return diff
    
    model = SimpleEquivariantModel()
    pos = torch.randn(5, 3)
    edge_index = torch.tensor([[0, 1, 2], [1, 2, 3]])
    
    test_equivariance(model, pos, edge_index)
    

* * *

## 2.6 ã‚³ãƒ©ãƒ ï¼šãªãœæ·±ã„GNNã¯é›£ã—ã„ã‹

### éå‰°å¹³æ»‘åŒ–ï¼ˆOver-smoothingï¼‰

**å•é¡Œ** : ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’æ·±ãã™ã‚‹ã¨ã€**å…¨ã¦ã®é ‚ç‚¹ãŒåŒã˜ç‰¹å¾´** ã«ãªã‚‹

**åŸå› ** : ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã®ç¹°ã‚Šè¿”ã—ã§æƒ…å ±ãŒæ‹¡æ•£
    
    
    # éå‰°å¹³æ»‘åŒ–ã®ãƒ‡ãƒ¢
    import torch
    import torch.nn.functional as F
    
    def demonstrate_oversmoothing(X, A, num_layers=10):
        """
        éå‰°å¹³æ»‘åŒ–ã®å¯è¦–åŒ–
        """
        H = X
        smoothness = []
    
        for layer in range(num_layers):
            # ç°¡å˜ãªGCNå±¤
            D = torch.diag(A.sum(dim=1))
            D_inv_sqrt = torch.diag(1.0 / torch.sqrt(D.diagonal()))
            A_norm = D_inv_sqrt @ A @ D_inv_sqrt
    
            H = A_norm @ H
            H = F.relu(H)
    
            # å¹³æ»‘åº¦ï¼ˆé ‚ç‚¹é–“ã®é¡ä¼¼åº¦ï¼‰
            similarity = F.cosine_similarity(
                H.unsqueeze(1), H.unsqueeze(0), dim=2
            )
            avg_similarity = similarity[torch.triu_indices(
                H.size(0), H.size(0), offset=1
            )[0], torch.triu_indices(
                H.size(0), H.size(0), offset=1
            )[1]].mean()
    
            smoothness.append(avg_similarity.item())
            print(f"Layer {layer+1}: å¹³å‡é¡ä¼¼åº¦ = {avg_similarity:.4f}")
    
        return smoothness
    
    # å®Ÿè¡Œ
    X = torch.randn(5, 16)
    A = torch.eye(5) + torch.rand(5, 5) > 0.7
    smoothness = demonstrate_oversmoothing(X, A.float(), num_layers=10)
    

**å‡ºåŠ›ä¾‹** :
    
    
    Layer 1: å¹³å‡é¡ä¼¼åº¦ = 0.2341
    Layer 2: å¹³å‡é¡ä¼¼åº¦ = 0.4523
    Layer 3: å¹³å‡é¡ä¼¼åº¦ = 0.6789
    ...
    Layer 10: å¹³å‡é¡ä¼¼åº¦ = 0.9876
    

â†’ ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒæ·±ããªã‚‹ã«ã¤ã‚Œã€å…¨é ‚ç‚¹ãŒä¼¼ã¦ãã‚‹

* * *

### å¯¾ç­–

  1. **Residual Connectionï¼ˆæ®‹å·®æ¥ç¶šï¼‰** : $$h_i^{(l+1)} = h_i^{(l)} + \text{GNN}(h_i^{(l)})$$

  2. **Jumping Knowledge Network** : \- å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å‡ºåŠ›ã‚’çµåˆ

  3. **PairNorm** : \- ç‰¹å¾´é‡ã®æ­£è¦åŒ–

    
    
    class GNNWithResidual(nn.Module):
        def \_\_init\_\_(self, hidden\_dim):
            super().\_\_init\_\_()
            self.conv = GCNLayer(hidden\_dim, hidden\_dim)
    
        def forward(self, X, A):
            # Residual connection
            H = self.conv(X, A)
            return X + H  # ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆ
    

* * *

## 2.7 æœ¬ç« ã®ã¾ã¨ã‚

### å­¦ã‚“ã ã“ã¨

  1. **ã‚°ãƒ©ãƒ•ã®æ•°å­¦çš„å®šç¾©** \- éš£æ¥è¡Œåˆ—ã€æ¬¡æ•°è¡Œåˆ—ã€ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³è¡Œåˆ— \- é ‚ç‚¹ç‰¹å¾´é‡ã¨è¾ºç‰¹å¾´é‡

  2. **ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°** \- 3ã‚¹ãƒ†ãƒƒãƒ—: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ â†’ é›†ç´„ â†’ æ›´æ–° \- é›†ç´„é–¢æ•°: Sumã€Meanã€Maxã€Attention

  3. **ä»£è¡¨çš„GNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£** \- GCN: ã‚·ãƒ³ãƒ—ãƒ«ã€é«˜é€Ÿ \- GAT: Attentionã€é«˜ç²¾åº¦ \- GraphSAGE: ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã€ãƒŸãƒ‹ãƒãƒƒãƒ

  4. **ææ–™ç§‘å­¦ç‰¹åŒ–GNN** \- SchNet: è·é›¢ä¾å­˜ã€é€£ç¶šãƒ•ã‚£ãƒ«ã‚¿ \- DimeNet: è§’åº¦æƒ…å ±ã‚‚è€ƒæ…® \- GemNet: äºŒé¢è§’ã¾ã§è€ƒæ…®

  5. **ç­‰å¤‰æ€§** \- E(3)ç­‰å¤‰æ€§ã®é‡è¦æ€§ \- NequIPã€MACEãªã©æœ€æ–°æ‰‹æ³•

### é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

  * âœ… ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã¯GNNã®**çµ±ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**
  * âœ… é›†ç´„é–¢æ•°ã®é¸æŠãŒæ€§èƒ½ã«å¤§ããå½±éŸ¿
  * âœ… ææ–™ç§‘å­¦ã§ã¯**å¹¾ä½•å­¦çš„æƒ…å ±** ï¼ˆè·é›¢ã€è§’åº¦ï¼‰ãŒé‡è¦
  * âœ… ç­‰å¤‰æ€§ã«ã‚ˆã‚Š**ç‰©ç†æ³•å‰‡ã‚’ä¿è¨¼**
  * âœ… éå‰°å¹³æ»‘åŒ–ã«æ³¨æ„ï¼ˆResidual Connectionã§å¯¾ç­–ï¼‰

### æ¬¡ã®ç« ã¸

ç¬¬3ç« ã§ã¯ã€**PyTorch Geometricå®Ÿè·µ** ã‚’å­¦ã³ã¾ã™ï¼š \- ç’°å¢ƒæ§‹ç¯‰ï¼ˆPyGã€RDKitã€ASEï¼‰ \- QM9ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§åˆ†å­ç‰¹æ€§äºˆæ¸¬ \- Materials Projectãƒ‡ãƒ¼ã‚¿ã§çµæ™¶ç‰¹æ€§äºˆæ¸¬ \- ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° \- å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

**[ç¬¬3ç« ï¼šPyTorch Geometricå®Ÿè·µ â†’](<./chapter-3.html>)**

* * *

## æ¼”ç¿’å•é¡Œ

### å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰

æ¬¡ã®æ–‡ç« ã®æ­£èª¤ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

  1. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã¯ã€é›†ç´„ï¼ˆAggregationï¼‰â†’ æ›´æ–°ï¼ˆUpdateï¼‰â†’ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆã®é †ã§è¡Œã‚ã‚Œã‚‹
  2. GATã¯Attentionã‚’ä½¿ã†ãŸã‚ã€å…¨ã¦ã®éš£æ¥é ‚ç‚¹ã‚’åŒã˜é‡ã¿ã§æ‰±ã†
  3. SchNetã¯åŸå­é–“è·é›¢ã‚’æ˜ç¤ºçš„ã«è€ƒæ…®ã™ã‚‹

ãƒ’ãƒ³ãƒˆ \- ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã®3ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ€ã„å‡ºã—ã¾ã—ã‚‡ã† \- GATã®æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã€Œé‡è¦ãªéš£æ¥é ‚ç‚¹ã‚’é‡è¦–ã€ã§ã™ \- SchNetã®ç‰¹å¾´ã¯ã€Œé€£ç¶šãƒ•ã‚£ãƒ«ã‚¿ã€ã§ã™  è§£ç­”ä¾‹ **è§£ç­”**: 1\. **èª¤** - æ­£ã—ã„é †ç•ªã¯ï¼šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ â†’ é›†ç´„ â†’ æ›´æ–° 2\. **èª¤** - GATã¯ Attentionã§**ç•°ãªã‚‹é‡ã¿**ã‚’å‰²ã‚Šå½“ã¦ã‚‹ 3\. **æ­£** - SchNetã¯RBFï¼ˆã‚¬ã‚¦ã‚¹åŸºåº•ï¼‰ã§è·é›¢ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ **è§£èª¬**: 1ã«ã¤ã„ã¦ï¼š 
    
    
    # æ­£ã—ã„é †åº
    for layer in range(num\_layers):
        # Step 1: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
        messages = message\_function(h\_neighbors)
    
        # Step 2: é›†ç´„
        m\_i = aggregate(messages)
    
        # Step 3: æ›´æ–°
        h\_i = update\_function(h\_i, m\_i)
    

2ã«ã¤ã„ã¦ï¼š \- GAT ã® Attention ä¿‚æ•° $\alpha_{ij}$ ã¯éš£æ¥é ‚ç‚¹ã”ã¨ã«ç•°ãªã‚‹ \- é‡è¦ãªéš£æ¥é ‚ç‚¹ã«ã¯å¤§ããªé‡ã¿ã€ãã†ã§ãªã„ã‚‚ã®ã«ã¯å°ã•ãªé‡ã¿ 3ã«ã¤ã„ã¦ï¼š \- SchNet ã® ãƒ•ã‚£ãƒ«ã‚¿é–¢æ•°: $\phi(d) = \sum_k w_k \exp(-\gamma (d - \mu_k)^2)$ \- è·é›¢ $d$ ãŒç•°ãªã‚Œã°ã€ãƒ•ã‚£ãƒ«ã‚¿ã®å€¤ã‚‚ç•°ãªã‚‹ 

* * *

### å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰

ä»¥ä¸‹ã®ã‚°ãƒ©ãƒ•ã«å¯¾ã—ã¦ã€GCNã®1å±¤ã®é †ä¼æ’­ã‚’æ‰‹è¨ˆç®—ã§æ±‚ã‚ã¦ãã ã•ã„ã€‚

**ã‚°ãƒ©ãƒ•** :
    
    
    é ‚ç‚¹: 3å€‹ï¼ˆv0, v1, v2ï¼‰
    è¾º: v0-v1, v1-v2ï¼ˆç·šå½¢ã‚°ãƒ©ãƒ•ï¼‰
    
    é ‚ç‚¹ç‰¹å¾´:
    X = [[1, 0],
         [0, 1],
         [1, 1]]
    
    éš£æ¥è¡Œåˆ—:
    A = [[0, 1, 0],
         [1, 0, 1],
         [0, 1, 0]]
    
    é‡ã¿è¡Œåˆ—ï¼ˆç°¡ç•¥åŒ–ï¼‰:
    W = [[1, 0],
         [0, 1]]  ï¼ˆæ’ç­‰è¡Œåˆ—ï¼‰
    

**è¦æ±‚äº‹é …** : 1\. $\tilde{A} = A + I$ ã‚’è¨ˆç®— 2\. æ­£è¦åŒ–éš£æ¥è¡Œåˆ— $\hat{A} = \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2}$ ã‚’è¨ˆç®— 3\. GCNå‡ºåŠ› $H = \hat{A} X W$ ã‚’è¨ˆç®—ï¼ˆæ´»æ€§åŒ–é–¢æ•°ãªã—ï¼‰

ãƒ’ãƒ³ãƒˆ **æ‰‹é †**: 1\. è‡ªå·±ãƒ«ãƒ¼ãƒ—ã‚’è¿½åŠ : $\tilde{A}_{ii} = 1$ 2\. æ¬¡æ•°è¡Œåˆ—: $\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}$ 3\. $\tilde{D}^{-1/2}$ ã‚’è¨ˆç®—ï¼ˆå¯¾è§’è¦ç´ ã®é€†æ•°ã®å¹³æ–¹æ ¹ï¼‰ 4\. è¡Œåˆ—ç©ã‚’è¨ˆç®—  è§£ç­”ä¾‹ **Step 1: è‡ªå·±ãƒ«ãƒ¼ãƒ—ä»˜ãéš£æ¥è¡Œåˆ—** $$ \tilde{A} = A + I = \begin{bmatrix} 0 & 1 & 0 \\\ 1 & 0 & 1 \\\ 0 & 1 & 0 \end{bmatrix} + \begin{bmatrix} 1 & 0 & 0 \\\ 0 & 1 & 0 \\\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 1 & 0 \\\ 1 & 1 & 1 \\\ 0 & 1 & 1 \end{bmatrix} $$ **Step 2: æ¬¡æ•°è¡Œåˆ—** $$ \tilde{D} = \begin{bmatrix} 2 & 0 & 0 \\\ 0 & 3 & 0 \\\ 0 & 0 & 2 \end{bmatrix} $$ ï¼ˆå„è¡Œã®å’Œï¼‰ **Step 3: $\tilde{D}^{-1/2}$** $$ \tilde{D}^{-1/2} = \begin{bmatrix} 1/\sqrt{2} & 0 & 0 \\\ 0 & 1/\sqrt{3} & 0 \\\ 0 & 0 & 1/\sqrt{2} \end{bmatrix} \approx \begin{bmatrix} 0.707 & 0 & 0 \\\ 0 & 0.577 & 0 \\\ 0 & 0 & 0.707 \end{bmatrix} $$ **Step 4: æ­£è¦åŒ–éš£æ¥è¡Œåˆ—** $$ \hat{A} = \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} $$ è¨ˆç®—éç¨‹: 
    
    
    import numpy as np
    
    A\_tilde = np.array([
        [1, 1, 0],
        [1, 1, 1],
        [0, 1, 1]
    ], dtype=float)
    
    D\_tilde = np.diag([2, 3, 2])
    D\_inv\_sqrt = np.diag([1/np.sqrt(2), 1/np.sqrt(3), 1/np.sqrt(2)])
    
    A\_hat = D\_inv\_sqrt @ A\_tilde @ D\_inv\_sqrt
    print("æ­£è¦åŒ–éš£æ¥è¡Œåˆ—:")
    print(A\_hat)
    

$$ \hat{A} \approx \begin{bmatrix} 0.500 & 0.408 & 0 \\\ 0.408 & 0.333 & 0.408 \\\ 0 & 0.408 & 0.500 \end{bmatrix} $$ **Step 5: GCNå‡ºåŠ›** $$ H = \hat{A} X W $$ ï¼ˆ$W = I$ ãªã®ã§ $H = \hat{A} X$ï¼‰ 
    
    
    X = np.array([
        [1, 0],
        [0, 1],
        [1, 1]
    ], dtype=float)
    
    H = A\_hat @ X
    print("GCNå‡ºåŠ›:")
    print(H)
    

$$ H \approx \begin{bmatrix} 0.500 & 0.408 \\\ 0.816 & 0.741 \\\ 0.408 & 0.908 \end{bmatrix} $$ **è§£é‡ˆ**: \- é ‚ç‚¹1ï¼ˆä¸­å¿ƒï¼‰: ä¸¡å´ã®éš£æ¥é ‚ç‚¹ã®æƒ…å ±ã‚’é›†ç´„ \- é ‚ç‚¹0,2ï¼ˆç«¯ç‚¹ï¼‰: éš£æ¥é ‚ç‚¹1ã®æƒ…å ±ã‚’ä¸»ã«å–ã‚Šè¾¼ã‚€ **Pythonã§ã®æ¤œè¨¼**: 
    
    
    # å®Œå…¨ãªã‚³ãƒ¼ãƒ‰
    A = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]], dtype=float)
    X = np.array([[1, 0], [0, 1], [1, 1]], dtype=float)
    
    # GCN
    A\_tilde = A + np.eye(3)
    D\_tilde = np.diag(A\_tilde.sum(axis=1))
    D\_inv\_sqrt = np.diag(1.0 / np.sqrt(D\_tilde.diagonal()))
    A\_hat = D\_inv\_sqrt @ A\_tilde @ D\_inv\_sqrt
    
    H = A\_hat @ X
    print("æœ€çµ‚å‡ºåŠ›:")
    print(H)
    

* * *

### å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰

SchNetã®é€£ç¶šãƒ•ã‚£ãƒ«ã‚¿é–¢æ•°ã‚’å®Ÿè£…ã—ã€ç•°ãªã‚‹åŸå­é–“è·é›¢ã«å¯¾ã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿ã®å¿œç­”ã‚’å¯è¦–åŒ–ã—ã¦ãã ã•ã„ã€‚

**è¦æ±‚äº‹é …** : 1\. ã‚¬ã‚¦ã‚¹åŸºåº•ï¼ˆRBFï¼‰é–¢æ•°ã‚’å®Ÿè£… 2\. è·é›¢0.5Ã…ã€œ5.0Ã…ã«å¯¾ã™ã‚‹RBFå¿œç­”ã‚’è¨ˆç®— 3\. ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ– 4\. ãƒ•ã‚£ãƒ«ã‚¿ã®ç‰©ç†çš„æ„å‘³ã‚’è€ƒå¯Ÿ

ãƒ’ãƒ³ãƒˆ **RBF ã®å¼**: $$\phi_k(d) = \exp\left( -\gamma (d - \mu_k)^2 \right)$$ \- $\mu_k$: ã‚¬ã‚¦ã‚¹é–¢æ•°ã®ä¸­å¿ƒï¼ˆ0ã€œ5Ã…ã«å‡ç­‰é…ç½®ï¼‰ \- $\gamma$: åºƒãŒã‚Šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ10ç¨‹åº¦ï¼‰ **å¯è¦–åŒ–ã®ãƒã‚¤ãƒ³ãƒˆ**: \- Xè»¸: è·é›¢ (0.5ã€œ5.0Ã…) \- Yè»¸: RBF ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (0ã€œ49) \- è‰²: RBF å¿œç­”å€¤ (0ã€œ1)  è§£ç­”ä¾‹
    
    
    import torch
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    # ===== å®Ÿè£… =====
    class GaussianBasisFunction:
        def __init__(self, start=0.0, stop=5.0, num_gaussians=50,
                     gamma=10.0):
            """
            ã‚¬ã‚¦ã‚¹åŸºåº•é–¢æ•°ï¼ˆRBFï¼‰
    
            Parameters:
            -----------
            start, stop : float
                è·é›¢ã®ç¯„å›²
            num_gaussians : int
                ã‚¬ã‚¦ã‚¹é–¢æ•°ã®æ•°
            gamma : float
                åºƒãŒã‚Šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            """
            self.mu = torch.linspace(start, stop, num_gaussians)
            self.gamma = gamma
    
        def __call__(self, distances):
            """
            RBF å¿œç­”ã‚’è¨ˆç®—
    
            Parameters:
            -----------
            distances : Tensor (num_distances,)
    
            Returns:
            --------
            rbf : Tensor (num_distances, num_gaussians)
            """
            # (num_distances, 1) - (1, num_gaussians)
            diff = distances.unsqueeze(-1) - self.mu.unsqueeze(0)
            rbf = torch.exp(-self.gamma * diff ** 2)
            return rbf
    
    # ===== å¯è¦–åŒ– =====
    # RBFç”Ÿæˆ
    rbf_layer = GaussianBasisFunction(
        start=0.0, stop=5.0,
        num_gaussians=50, gamma=10.0
    )
    
    # è·é›¢ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ0.5ã€œ5.0Ã…ï¼‰
    distances = torch.linspace(0.5, 5.0, 100)
    
    # RBF å¿œç­”
    rbf_response = rbf_layer(distances)  # (100, 50)
    
    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
    plt.figure(figsize=(12, 6))
    sns.heatmap(
        rbf_response.T.numpy(),  # è»¢ç½®ï¼ˆRBF x è·é›¢ï¼‰
        cmap='viridis',
        xticklabels=10,
        yticklabels=10,
        cbar_kws={'label': 'RBF Response'}
    )
    plt.xlabel('Distance (Ã…)')
    plt.ylabel('RBF Index')
    plt.title('SchNet Continuous Filter: RBF Response')
    
    # Xè»¸ãƒ©ãƒ™ãƒ«ã‚’å®Ÿéš›ã®è·é›¢ã«
    xticks = np.linspace(0, len(distances)-1, 10).astype(int)
    xticklabels = [f'{distances[i]:.1f}' for i in xticks]
    plt.xticks(xticks, xticklabels)
    
    plt.tight_layout()
    plt.savefig('schnet_rbf_heatmap.png', dpi=150)
    plt.show()
    
    # ===== ç‰¹å®šè·é›¢ã®RBFå¿œç­” =====
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    example_distances = [1.0, 1.5, 2.0, 3.0]  # Ã…
    
    for ax, d in zip(axes.flatten(), example_distances):
        d_tensor = torch.tensor([d])
        rbf = rbf_layer(d_tensor).squeeze()
    
        ax.plot(rbf_layer.mu.numpy(), rbf.numpy(),
                marker='o', linewidth=2)
        ax.axvline(d, color='red', linestyle='--',
                   label=f'Distance = {d}Ã…')
        ax.set_xlabel('RBF Center Î¼ (Ã…)')
        ax.set_ylabel('RBF Response')
        ax.set_title(f'RBF Response at d = {d}Ã…')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('schnet_rbf_profiles.png', dpi=150)
    plt.show()
    
    # ===== ç‰©ç†çš„æ„å‘³ã®è€ƒå¯Ÿ =====
    print("\n===== ç‰©ç†çš„æ„å‘³ =====")
    print("1. çŸ­è·é›¢ï¼ˆ0.5-2.0Ã…ï¼‰: å…±æœ‰çµåˆé ˜åŸŸ")
    print("   - C-C: 1.54Ã…, C=C: 1.34Ã…, C-H: 1.09Ã…")
    print("   - RBFã¯æ€¥å³»ã«åå¿œï¼ˆçµåˆã®æœ‰ç„¡ã‚’è­˜åˆ¥ï¼‰")
    
    print("\n2. ä¸­è·é›¢ï¼ˆ2.0-3.5Ã…ï¼‰: éå…±æœ‰çµåˆç›¸äº’ä½œç”¨")
    print("   - æ°´ç´ çµåˆ: 2.8Ã…, ãƒ•ã‚¡ãƒ³ãƒ‡ãƒ«ãƒ¯ãƒ¼ãƒ«ã‚¹åŠ›")
    print("   - RBFã¯ãªã ã‚‰ã‹ã«åå¿œ")
    
    print("\n3. é•·è·é›¢ï¼ˆ3.5-5.0Ã…ï¼‰: å¼±ã„ç›¸äº’ä½œç”¨")
    print("   - é™é›»ç›¸äº’ä½œç”¨ã€åˆ†æ•£åŠ›")
    print("   - RBFã®å¿œç­”ã¯å°ã•ã„")
    
    print("\n4. ã‚¬ã‚¦ã‚¹åŸºåº•ã®å½¹å‰²:")
    print("   - é€£ç¶šçš„ãªè·é›¢è¡¨ç¾ï¼ˆé›¢æ•£åŒ–ãªã—ï¼‰")
    print("   - ä»»æ„ã®è·é›¢ã«å¯¾ã—ã¦å¾®åˆ†å¯èƒ½")
    print("   - æ©Ÿæ¢°å­¦ç¿’ã§æœ€é©åŒ–å¯èƒ½ï¼ˆÎ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰")
    

**å‡ºåŠ›ã®è§£é‡ˆ**: 1\. **ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—**: \- å¯¾è§’ç·šçŠ¶ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå„RBFãŒç‰¹å®šè·é›¢ã§æœ€å¤§å¿œç­”ï¼‰ \- æ»‘ã‚‰ã‹ãªé·ç§»ï¼ˆã‚¬ã‚¦ã‚¹é–¢æ•°ã®é‡ãªã‚Šï¼‰ 2\. **RBFãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«**: \- è·é›¢1.0Ã…: RBF #10ä»˜è¿‘ãŒå¼·ãåå¿œ \- è·é›¢2.0Ã…: RBF #20ä»˜è¿‘ãŒå¼·ãåå¿œ \- ã‚¬ã‚¦ã‚¹å½¢çŠ¶ã«ã‚ˆã‚Šã€éš£æ¥RBFã‚‚å¼±ãåå¿œ 3\. **ç‰©ç†çš„æ„å‘³**: \- **SchNetã¯è·é›¢ã‚’ã€Œåˆ†å¸ƒã€ã¨ã—ã¦è¡¨ç¾** \- é›¢æ•£çš„ãªãƒ“ãƒ³åˆ†ã‘ã§ã¯ãªãã€é€£ç¶šçš„ãªé‡ãªã‚Š \- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒè·é›¢ä¾å­˜æ€§ã‚’å­¦ç¿’ **æ‹¡å¼µèª²é¡Œ**: 1\. $\gamma$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰ãˆã¦ã€RBFã®åºƒãŒã‚Šã‚’èª¿æ•´ 2\. éå¯¾ç§°ãªã‚¬ã‚¦ã‚¹åŸºåº•ï¼ˆçŸ­è·é›¢ã‚’å¯†ã«ã€é•·è·é›¢ã‚’ç–ã«ï¼‰ 3\. å®Ÿéš›ã®åˆ†å­ã§RBFãƒ•ã‚£ãƒ«ã‚¿ã‚’å¯è¦–åŒ– 

* * *

## å‚è€ƒæ–‡çŒ®

  1. Kipf, T. N. & Welling, M. (2017). "Semi-Supervised Classification with Graph Convolutional Networks." _ICLR_. DOI: <https://arxiv.org/abs/1609.02907>

  2. VeliÄkoviÄ‡, P. et al. (2018). "Graph Attention Networks." _ICLR_. DOI: <https://arxiv.org/abs/1710.10903>

  3. Hamilton, W. L. et al. (2017). "Inductive Representation Learning on Large Graphs." _NeurIPS_. DOI: <https://arxiv.org/abs/1706.02216>

  4. SchÃ¼tt, K. T. et al. (2017). "SchNet: A continuous-filter convolutional neural network for modeling quantum interactions." _NeurIPS_. DOI: <https://arxiv.org/abs/1706.08566>

  5. Klicpera, J. et al. (2020). "Directional Message Passing for Molecular Graphs." _ICLR_. DOI: <https://arxiv.org/abs/2003.03123>

  6. Gasteiger, J. et al. (2021). "GemNet: Universal Directional Graph Neural Networks for Molecules." _NeurIPS_. DOI: <https://arxiv.org/abs/2106.08903>

  7. Batzner, S. et al. (2022). "E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials." _Nature Communications_ , 13, 2453. DOI: <https://doi.org/10.1038/s41467-022-29939-5>

* * *

## ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³

### å‰ã®ç« 

**[ç¬¬1ç« ï¼šãªãœææ–™ç§‘å­¦ã«GNNãŒå¿…è¦ã‹ â†](<./chapter-1.html>)**

### æ¬¡ã®ç« 

**[ç¬¬3ç« ï¼šPyTorch Geometricå®Ÿè·µ â†’](<./chapter-3.html>)**

### ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡

**[â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹](<./index.html>)**

* * *

## è‘—è€…æƒ…å ±

**ä½œæˆè€…** : AI Terakoya Content Team **ä½œæˆæ—¥** : 2025-10-17 **ãƒãƒ¼ã‚¸ãƒ§ãƒ³** : 1.0

**æ›´æ–°å±¥æ­´** : \- 2025-10-17: v1.0 åˆç‰ˆå…¬é–‹

**ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯** : \- GitHub Issues: [ãƒªãƒã‚¸ãƒˆãƒªURL]/issues \- Email: yusuke.hashimoto.b8@tohoku.ac.jp

**ãƒ©ã‚¤ã‚»ãƒ³ã‚¹** : Creative Commons BY 4.0

* * *

**ç¬¬3ç« ã§ã€å®Ÿéš›ã«GNNã‚’å‹•ã‹ã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼**
