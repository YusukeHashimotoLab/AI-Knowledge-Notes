<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 3: Pythonå®Ÿè·µãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ« - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Chapter 3: Pythonå®Ÿè·µãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«</h1>
            <p class="subtitle">ãƒŠãƒææ–™ãƒ‡ãƒ¼ã‚¿è§£æã¨æ©Ÿæ¢°å­¦ç¿’</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 30-40åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 0å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 0å•</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 3: Pythonå®Ÿè·µãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã‚‚åŠ¹ãå›å¸°ãƒ¢ãƒ‡ãƒ«ã¨ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§ã€åŠ¹ç‡ã‚ˆãæ¡ä»¶æ¢ç´¢ã™ã‚‹ç­‹è‚‰ã‚’ä»˜ã‘ã¾ã™ã€‚MDãƒ‡ãƒ¼ã‚¿ã®è¦ç‚¹å¯è¦–åŒ–ã¨SHAPã«ã‚ˆã‚‹è§£é‡ˆã¾ã§ä¸€æ°—ã«é€šã—ã¾ã™ã€‚</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>ğŸ’¡ è£œè¶³:</strong> å°‘ãªã„è©¦è¡Œã§è‰¯ã„æ¡ä»¶ã‚’è¦‹ã¤ã‘ã‚‹ã®ãŒç›®æ¨™ã€‚ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã¯â€œé‡‘å±æ¢çŸ¥æ©Ÿâ€çš„ã«å½“ãŸã‚Šã‚’å°ãã¾ã™ã€‚</p>





<p>ãƒŠãƒææ–™ãƒ‡ãƒ¼ã‚¿è§£æã¨æ©Ÿæ¢°å­¦ç¿’</p>
<hr />
<h2>æœ¬ç« ã®å­¦ç¿’ç›®æ¨™</h2>
<p>æœ¬ç« ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€ä»¥ä¸‹ã®ã‚¹ã‚­ãƒ«ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<p>âœ… ãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆãƒ»å¯è¦–åŒ–ãƒ»å‰å‡¦ç†ã®å®Ÿè·µ
âœ… 5ç¨®é¡ã®å›å¸°ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒŠãƒææ–™ç‰©æ€§äºˆæ¸¬
âœ… ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚‹ãƒŠãƒææ–™ã®æœ€é©è¨­è¨ˆ
âœ… SHAPåˆ†æã«ã‚ˆã‚‹æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆ
âœ… å¤šç›®çš„æœ€é©åŒ–ã«ã‚ˆã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•åˆ†æ
âœ… TEMç”»åƒè§£æã¨ã‚µã‚¤ã‚ºåˆ†å¸ƒã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
âœ… ç•°å¸¸æ¤œçŸ¥ã«ã‚ˆã‚‹å“è³ªç®¡ç†ã¸ã®å¿œç”¨</p>
<hr />
<h2>3.1 ç’°å¢ƒæ§‹ç¯‰</h2>
<h3>å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª</h3>
<p>æœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ä½¿ç”¨ã™ã‚‹ä¸»è¦ãªPythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼š</p>
<pre><code class="language-python"># ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»å¯è¦–åŒ–
pandas, numpy, matplotlib, seaborn, scipy

# æ©Ÿæ¢°å­¦ç¿’
scikit-learn, lightgbm

# æœ€é©åŒ–
scikit-optimize

# ãƒ¢ãƒ‡ãƒ«è§£é‡ˆ
shap

# å¤šç›®çš„æœ€é©åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
pymoo
</code></pre>
<h3>ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•</h3>
<h4>Option 1: Anacondaç’°å¢ƒ</h4>
<pre><code class="language-bash"># Anacondaã§æ–°ã—ã„ç’°å¢ƒã‚’ä½œæˆ
conda create -n nanomaterials python=3.10 -y
conda activate nanomaterials

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
conda install pandas numpy matplotlib seaborn scipy scikit-learn -y
conda install -c conda-forge lightgbm scikit-optimize shap -y

# å¤šç›®çš„æœ€é©åŒ–ç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
pip install pymoo
</code></pre>
<h4>Option 2: venv + pipç’°å¢ƒ</h4>
<pre><code class="language-bash"># ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆ
python -m venv nanomaterials_env

# ä»®æƒ³ç’°å¢ƒã‚’æœ‰åŠ¹åŒ–
# macOS/Linux:
source nanomaterials_env/bin/activate
# Windows:
nanomaterials_env\Scripts\activate

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install pandas numpy matplotlib seaborn scipy
pip install scikit-learn lightgbm scikit-optimize shap pymoo
</code></pre>
<h4>Option 3: Google Colab</h4>
<p>Google Colabã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’ã‚»ãƒ«ã§å®Ÿè¡Œï¼š</p>
<pre><code class="language-python"># è¿½åŠ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
!pip install lightgbm scikit-optimize shap pymoo

# ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®ç¢ºèª
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
print(&quot;ç’°å¢ƒæ§‹ç¯‰å®Œäº†ï¼&quot;)
</code></pre>
<hr />
<h2>3.2 ãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã¨å¯è¦–åŒ–</h2>
<h3>ã€ä¾‹1ã€‘åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼šé‡‘ãƒŠãƒç²’å­ã®ã‚µã‚¤ã‚ºã¨å…‰å­¦ç‰¹æ€§</h3>
<p>é‡‘ãƒŠãƒç²’å­ã®å±€åœ¨è¡¨é¢ãƒ—ãƒ©ã‚ºãƒ¢ãƒ³å…±é³´ï¼ˆLSPRï¼‰æ³¢é•·ã¯ã€ç²’å­ã‚µã‚¤ã‚ºã«ä¾å­˜ã—ã¾ã™ã€‚ã“ã®é–¢ä¿‚ã‚’æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ã§è¡¨ç¾ã—ã¾ã™ã€‚</p>
<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã®è¨­å®šï¼ˆå†ç¾æ€§ã®ãŸã‚ï¼‰
np.random.seed(42)

# ã‚µãƒ³ãƒ—ãƒ«æ•°
n_samples = 200

# é‡‘ãƒŠãƒç²’å­ã®ã‚µã‚¤ã‚ºï¼ˆnmï¼‰: å¹³å‡15 nmã€æ¨™æº–åå·®5 nm
size = np.random.normal(15, 5, n_samples)
size = np.clip(size, 5, 50)  # 5-50 nmã®ç¯„å›²ã«åˆ¶é™

# LSPRæ³¢é•·ï¼ˆnmï¼‰: Mieç†è«–ã®ç°¡æ˜“è¿‘ä¼¼
# åŸºæœ¬æ³¢é•·520 nm + ã‚µã‚¤ã‚ºä¾å­˜é … + ãƒã‚¤ã‚º
lspr = 520 + 0.8 * (size - 15) + np.random.normal(0, 5, n_samples)

# åˆæˆæ¡ä»¶
temperature = np.random.uniform(20, 80, n_samples)  # æ¸©åº¦ï¼ˆâ„ƒï¼‰
pH = np.random.uniform(4, 10, n_samples)  # pH

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä½œæˆ
data = pd.DataFrame({
    'size_nm': size,
    'lspr_nm': lspr,
    'temperature_C': temperature,
    'pH': pH
})

print(&quot;=&quot; * 60)
print(&quot;é‡‘ãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆå®Œäº†&quot;)
print(&quot;=&quot; * 60)
print(data.head(10))
print(&quot;\nåŸºæœ¬çµ±è¨ˆé‡:&quot;)
print(data.describe())
</code></pre>
<h3>ã€ä¾‹2ã€‘ã‚µã‚¤ã‚ºåˆ†å¸ƒã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ </h3>
<pre><code class="language-python"># ã‚µã‚¤ã‚ºåˆ†å¸ƒã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
fig, ax = plt.subplots(figsize=(10, 6))

# ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã¨KDEï¼ˆã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®šï¼‰
ax.hist(data['size_nm'], bins=30, alpha=0.6, color='skyblue',
        edgecolor='black', density=True, label='Histogram')

# KDEãƒ—ãƒ­ãƒƒãƒˆ
from scipy.stats import gaussian_kde
kde = gaussian_kde(data['size_nm'])
x_range = np.linspace(data['size_nm'].min(), data['size_nm'].max(), 100)
ax.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')

ax.set_xlabel('Particle Size (nm)', fontsize=12)
ax.set_ylabel('Probability Density', fontsize=12)
ax.set_title('Gold Nanoparticle Size Distribution', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f&quot;å¹³å‡ã‚µã‚¤ã‚º: {data['size_nm'].mean():.2f} nm&quot;)
print(f&quot;æ¨™æº–åå·®: {data['size_nm'].std():.2f} nm&quot;)
print(f&quot;ä¸­å¤®å€¤: {data['size_nm'].median():.2f} nm&quot;)
</code></pre>
<h3>ã€ä¾‹3ã€‘æ•£å¸ƒå›³ãƒãƒˆãƒªãƒƒã‚¯ã‚¹</h3>
<pre><code class="language-python"># ãƒšã‚¢ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæ•£å¸ƒå›³ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼‰
sns.pairplot(data, diag_kind='kde', plot_kws={'alpha': 0.6},
             height=2.5, corner=False)
plt.suptitle('Pairplot of Gold Nanoparticle Data', y=1.01, fontsize=14, fontweight='bold')
plt.show()

print(&quot;å„å¤‰æ•°é–“ã®é–¢ä¿‚ã‚’å¯è¦–åŒ–ã—ã¾ã—ãŸ&quot;)
</code></pre>
<h3>ã€ä¾‹4ã€‘ç›¸é–¢è¡Œåˆ—ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—</h3>
<pre><code class="language-python"># ç›¸é–¢è¡Œåˆ—ã®è¨ˆç®—
correlation_matrix = data.corr()

# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm',
            center=0, square=True, linewidths=1, cbar_kws={&quot;shrink&quot;: 0.8})
ax.set_title('Correlation Matrix', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

print(&quot;ç›¸é–¢ä¿‚æ•°:&quot;)
print(correlation_matrix)
print(f&quot;\nLSPRæ³¢é•·ã¨ã‚µã‚¤ã‚ºã®ç›¸é–¢: {correlation_matrix.loc['lspr_nm', 'size_nm']:.3f}&quot;)
</code></pre>
<h3>ã€ä¾‹5ã€‘3Dãƒ—ãƒ­ãƒƒãƒˆï¼šã‚µã‚¤ã‚º vs æ¸©åº¦ vs LSPR</h3>
<pre><code class="language-python">from mpl_toolkits.mplot3d import Axes3D

# 3Dæ•£å¸ƒå›³
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

# ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—
scatter = ax.scatter(data['size_nm'], data['temperature_C'], data['lspr_nm'],
                     c=data['pH'], cmap='viridis', s=50, alpha=0.6, edgecolors='k')

ax.set_xlabel('Size (nm)', fontsize=11)
ax.set_ylabel('Temperature (Â°C)', fontsize=11)
ax.set_zlabel('LSPR Wavelength (nm)', fontsize=11)
ax.set_title('3D Scatter: Size vs Temperature vs LSPR (colored by pH)',
             fontsize=13, fontweight='bold')

# ã‚«ãƒ©ãƒ¼ãƒãƒ¼
cbar = plt.colorbar(scatter, ax=ax, shrink=0.5, aspect=5)
cbar.set_label('pH', fontsize=10)

plt.tight_layout()
plt.show()

print(&quot;3Dãƒ—ãƒ­ãƒƒãƒˆã§å¤šæ¬¡å…ƒã®é–¢ä¿‚ã‚’å¯è¦–åŒ–ã—ã¾ã—ãŸ&quot;)
</code></pre>
<hr />
<h2>3.3 å‰å‡¦ç†ã¨ãƒ‡ãƒ¼ã‚¿åˆ†å‰²</h2>
<h3>ã€ä¾‹6ã€‘æ¬ æå€¤å‡¦ç†</h3>
<pre><code class="language-python"># æ¬ æå€¤ã‚’äººç‚ºçš„ã«å°å…¥ï¼ˆå®Ÿç¿’ç”¨ï¼‰
data_with_missing = data.copy()
np.random.seed(123)

# ãƒ©ãƒ³ãƒ€ãƒ ã«5%ã®æ¬ æå€¤ã‚’å°å…¥
missing_indices = np.random.choice(data.index, size=int(0.05 * len(data)), replace=False)
data_with_missing.loc[missing_indices, 'temperature_C'] = np.nan

print(&quot;=&quot; * 60)
print(&quot;æ¬ æå€¤ã®ç¢ºèª&quot;)
print(&quot;=&quot; * 60)
print(f&quot;æ¬ æå€¤ã®æ•°:\n{data_with_missing.isnull().sum()}&quot;)

# æ¬ æå€¤ã®å‡¦ç†æ–¹æ³•1: å¹³å‡å€¤ã§è£œå®Œ
data_filled_mean = data_with_missing.fillna(data_with_missing.mean())

# æ¬ æå€¤ã®å‡¦ç†æ–¹æ³•2: ä¸­å¤®å€¤ã§è£œå®Œ
data_filled_median = data_with_missing.fillna(data_with_missing.median())

# æ¬ æå€¤ã®å‡¦ç†æ–¹æ³•3: å‰Šé™¤
data_dropped = data_with_missing.dropna()

print(f&quot;\nå…ƒã®ãƒ‡ãƒ¼ã‚¿: {len(data_with_missing)}è¡Œ&quot;)
print(f&quot;æ¬ æå€¤å‰Šé™¤å¾Œ: {len(data_dropped)}è¡Œ&quot;)
print(f&quot;å¹³å‡å€¤è£œå®Œå¾Œ: {len(data_filled_mean)}è¡Œï¼ˆæ¬ æå€¤ãªã—ï¼‰&quot;)

# ä»¥é™ã®åˆ†æã§ã¯å…ƒã®ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¬ æå€¤ãªã—ï¼‰ã‚’ä½¿ç”¨
data_clean = data.copy()
print(&quot;\nâ†’ ä»¥é™ã¯æ¬ æå€¤ã®ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™&quot;)
</code></pre>
<h3>ã€ä¾‹7ã€‘å¤–ã‚Œå€¤æ¤œå‡ºï¼ˆIQRæ³•ï¼‰</h3>
<pre><code class="language-python"># IQRï¼ˆå››åˆ†ä½ç¯„å›²ï¼‰æ³•ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º
def detect_outliers_iqr(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = (series &lt; lower_bound) | (series &gt; upper_bound)
    return outliers, lower_bound, upper_bound

# ã‚µã‚¤ã‚ºã«ã¤ã„ã¦å¤–ã‚Œå€¤æ¤œå‡º
outliers, lower, upper = detect_outliers_iqr(data_clean['size_nm'])

print(&quot;=&quot; * 60)
print(&quot;å¤–ã‚Œå€¤æ¤œå‡ºï¼ˆIQRæ³•ï¼‰&quot;)
print(&quot;=&quot; * 60)
print(f&quot;æ¤œå‡ºã•ã‚ŒãŸå¤–ã‚Œå€¤ã®æ•°: {outliers.sum()}&quot;)
print(f&quot;ä¸‹é™: {lower:.2f} nm, ä¸Šé™: {upper:.2f} nm&quot;)

# å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(10, 6))
ax.boxplot([data_clean['size_nm']], labels=['Size (nm)'], vert=False)
ax.scatter(data_clean.loc[outliers, 'size_nm'],
           [1] * outliers.sum(), color='red', s=100,
           label=f'Outliers (n={outliers.sum()})', zorder=3)
ax.set_xlabel('Size (nm)', fontsize=12)
ax.set_title('Boxplot with Outliers', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()

print(&quot;â†’ å¤–ã‚Œå€¤ã¯é™¤å»ã›ãšã€å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™&quot;)
</code></pre>
<h3>ã€ä¾‹8ã€‘ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆStandardScalerï¼‰</h3>
<pre><code class="language-python">from sklearn.preprocessing import StandardScaler

# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
X = data_clean[['size_nm', 'temperature_C', 'pH']]
y = data_clean['lspr_nm']

# StandardScalerï¼ˆå¹³å‡0ã€æ¨™æº–åå·®1ã«æ¨™æº–åŒ–ï¼‰
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰å¾Œã®æ¯”è¼ƒ
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)

print(&quot;=&quot; * 60)
print(&quot;ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰ã®çµ±è¨ˆé‡&quot;)
print(&quot;=&quot; * 60)
print(X.describe())

print(&quot;\n&quot; + &quot;=&quot; * 60)
print(&quot;ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œã®çµ±è¨ˆé‡ï¼ˆå¹³å‡â‰ˆ0ã€æ¨™æº–åå·®â‰ˆ1ï¼‰&quot;)
print(&quot;=&quot; * 60)
print(X_scaled_df.describe())

print(&quot;\nâ†’ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ã‚ˆã‚Šå„ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒ«ãŒçµ±ä¸€ã•ã‚Œã¾ã—ãŸ&quot;)
</code></pre>
<h3>ã€ä¾‹9ã€‘è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²</h3>
<pre><code class="language-python">from sklearn.model_selection import train_test_split

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²ï¼ˆ80:20ï¼‰
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

print(&quot;=&quot; * 60)
print(&quot;ãƒ‡ãƒ¼ã‚¿åˆ†å‰²&quot;)
print(&quot;=&quot; * 60)
print(f&quot;å…¨ãƒ‡ãƒ¼ã‚¿æ•°: {len(X)}ã‚µãƒ³ãƒ—ãƒ«&quot;)
print(f&quot;è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train)}ã‚µãƒ³ãƒ—ãƒ« ({len(X_train)/len(X)*100:.1f}%)&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)}ã‚µãƒ³ãƒ—ãƒ« ({len(X_test)/len(X)*100:.1f}%)&quot;)

print(&quot;\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆé‡:&quot;)
print(pd.DataFrame(X_train, columns=X.columns).describe())
</code></pre>
<hr />
<h2>3.4 å›å¸°ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒŠãƒç²’å­ç‰©æ€§äºˆæ¸¬</h2>
<p>ç›®æ¨™ï¼šã‚µã‚¤ã‚ºã€æ¸©åº¦ã€pHã‹ã‚‰LSPRæ³¢é•·ã‚’äºˆæ¸¬</p>
<h3>ã€ä¾‹10ã€‘ç·šå½¢å›å¸°ï¼ˆLinear Regressionï¼‰</h3>
<pre><code class="language-python">from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
model_lr = LinearRegression()
model_lr.fit(X_train, y_train)

# äºˆæ¸¬
y_train_pred_lr = model_lr.predict(X_train)
y_test_pred_lr = model_lr.predict(X_test)

# è©•ä¾¡æŒ‡æ¨™
r2_train_lr = r2_score(y_train, y_train_pred_lr)
r2_test_lr = r2_score(y_test, y_test_pred_lr)
rmse_test_lr = np.sqrt(mean_squared_error(y_test, y_test_pred_lr))
mae_test_lr = mean_absolute_error(y_test, y_test_pred_lr)

print(&quot;=&quot; * 60)
print(&quot;ç·šå½¢å›å¸°ï¼ˆLinear Regressionï¼‰&quot;)
print(&quot;=&quot; * 60)
print(f&quot;è¨“ç·´ãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_train_lr:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_test_lr:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RMSE: {rmse_test_lr:.4f} nm&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ MAE: {mae_test_lr:.4f} nm&quot;)

# å›å¸°ä¿‚æ•°
print(&quot;\nå›å¸°ä¿‚æ•°:&quot;)
for name, coef in zip(X.columns, model_lr.coef_):
    print(f&quot;  {name}: {coef:.4f}&quot;)
print(f&quot;  åˆ‡ç‰‡: {model_lr.intercept_:.4f}&quot;)

# æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# äºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤
axes[0].scatter(y_test, y_test_pred_lr, alpha=0.6, edgecolors='k')
axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
             'r--', lw=2, label='Perfect Prediction')
axes[0].set_xlabel('Actual LSPR (nm)', fontsize=11)
axes[0].set_ylabel('Predicted LSPR (nm)', fontsize=11)
axes[0].set_title(f'Linear Regression (RÂ² = {r2_test_lr:.3f})', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
residuals = y_test - y_test_pred_lr
axes[1].scatter(y_test_pred_lr, residuals, alpha=0.6, edgecolors='k')
axes[1].axhline(y=0, color='r', linestyle='--', lw=2)
axes[1].set_xlabel('Predicted LSPR (nm)', fontsize=11)
axes[1].set_ylabel('Residuals (nm)', fontsize=11)
axes[1].set_title('Residual Plot', fontsize=12, fontweight='bold')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>ã€ä¾‹11ã€‘ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°ï¼ˆRandom Forestï¼‰</h3>
<pre><code class="language-python">from sklearn.ensemble import RandomForestRegressor

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°ãƒ¢ãƒ‡ãƒ«
model_rf = RandomForestRegressor(n_estimators=100, max_depth=10,
                                 random_state=42, n_jobs=-1)
model_rf.fit(X_train, y_train)

# äºˆæ¸¬
y_train_pred_rf = model_rf.predict(X_train)
y_test_pred_rf = model_rf.predict(X_test)

# è©•ä¾¡
r2_train_rf = r2_score(y_train, y_train_pred_rf)
r2_test_rf = r2_score(y_test, y_test_pred_rf)
rmse_test_rf = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))
mae_test_rf = mean_absolute_error(y_test, y_test_pred_rf)

print(&quot;=&quot; * 60)
print(&quot;ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°ï¼ˆRandom Forestï¼‰&quot;)
print(&quot;=&quot; * 60)
print(f&quot;è¨“ç·´ãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_train_rf:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_test_rf:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RMSE: {rmse_test_rf:.4f} nm&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ MAE: {mae_test_rf:.4f} nm&quot;)

# ç‰¹å¾´é‡é‡è¦åº¦
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': model_rf.feature_importances_
}).sort_values('Importance', ascending=False)

print(&quot;\nç‰¹å¾´é‡é‡è¦åº¦:&quot;)
print(feature_importance)

# ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(8, 5))
ax.barh(feature_importance['Feature'], feature_importance['Importance'],
        color='steelblue', edgecolor='black')
ax.set_xlabel('Importance', fontsize=12)
ax.set_title('Feature Importance (Random Forest)', fontsize=13, fontweight='bold')
ax.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()
</code></pre>
<h3>ã€ä¾‹12ã€‘å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼ˆLightGBMï¼‰</h3>
<pre><code class="language-python">import lightgbm as lgb

# LightGBMãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
params = {
    'objective': 'regression',
    'metric': 'rmse',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'n_estimators': 200,
    'random_state': 42,
    'verbose': -1
}

model_lgb = lgb.LGBMRegressor(**params)
model_lgb.fit(X_train, y_train)

# äºˆæ¸¬
y_train_pred_lgb = model_lgb.predict(X_train)
y_test_pred_lgb = model_lgb.predict(X_test)

# è©•ä¾¡
r2_train_lgb = r2_score(y_train, y_train_pred_lgb)
r2_test_lgb = r2_score(y_test, y_test_pred_lgb)
rmse_test_lgb = np.sqrt(mean_squared_error(y_test, y_test_pred_lgb))
mae_test_lgb = mean_absolute_error(y_test, y_test_pred_lgb)

print(&quot;=&quot; * 60)
print(&quot;å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼ˆLightGBMï¼‰&quot;)
print(&quot;=&quot; * 60)
print(f&quot;è¨“ç·´ãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_train_lgb:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_test_lgb:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RMSE: {rmse_test_lgb:.4f} nm&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ MAE: {mae_test_lgb:.4f} nm&quot;)

# äºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤ãƒ—ãƒ­ãƒƒãƒˆ
fig, ax = plt.subplots(figsize=(8, 6))
ax.scatter(y_test, y_test_pred_lgb, alpha=0.6, edgecolors='k', s=60)
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
        'r--', lw=2, label='Perfect Prediction')
ax.set_xlabel('Actual LSPR (nm)', fontsize=12)
ax.set_ylabel('Predicted LSPR (nm)', fontsize=12)
ax.set_title(f'LightGBM Prediction (RÂ² = {r2_test_lgb:.3f})',
             fontsize=13, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>
<h3>ã€ä¾‹13ã€‘ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼å›å¸°ï¼ˆSVRï¼‰</h3>
<pre><code class="language-python">from sklearn.svm import SVR

# SVRãƒ¢ãƒ‡ãƒ«ï¼ˆRBFã‚«ãƒ¼ãƒãƒ«ï¼‰
model_svr = SVR(kernel='rbf', C=10, gamma='scale', epsilon=0.1)
model_svr.fit(X_train, y_train)

# äºˆæ¸¬
y_train_pred_svr = model_svr.predict(X_train)
y_test_pred_svr = model_svr.predict(X_test)

# è©•ä¾¡
r2_train_svr = r2_score(y_train, y_train_pred_svr)
r2_test_svr = r2_score(y_test, y_test_pred_svr)
rmse_test_svr = np.sqrt(mean_squared_error(y_test, y_test_pred_svr))
mae_test_svr = mean_absolute_error(y_test, y_test_pred_svr)

print(&quot;=&quot; * 60)
print(&quot;ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼å›å¸°ï¼ˆSVRï¼‰&quot;)
print(&quot;=&quot; * 60)
print(f&quot;è¨“ç·´ãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_train_svr:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_test_svr:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RMSE: {rmse_test_svr:.4f} nm&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ MAE: {mae_test_svr:.4f} nm&quot;)
print(f&quot;ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼æ•°: {len(model_svr.support_)}&quot;)
</code></pre>
<h3>ã€ä¾‹14ã€‘ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆMLP Regressorï¼‰</h3>
<pre><code class="language-python">from sklearn.neural_network import MLPRegressor

# MLPãƒ¢ãƒ‡ãƒ«
model_mlp = MLPRegressor(hidden_layer_sizes=(100, 50),
                         activation='relu',
                         solver='adam',
                         alpha=0.001,
                         max_iter=500,
                         random_state=42,
                         early_stopping=True,
                         validation_fraction=0.1,
                         verbose=False)

model_mlp.fit(X_train, y_train)

# äºˆæ¸¬
y_train_pred_mlp = model_mlp.predict(X_train)
y_test_pred_mlp = model_mlp.predict(X_test)

# è©•ä¾¡
r2_train_mlp = r2_score(y_train, y_train_pred_mlp)
r2_test_mlp = r2_score(y_test, y_test_pred_mlp)
rmse_test_mlp = np.sqrt(mean_squared_error(y_test, y_test_pred_mlp))
mae_test_mlp = mean_absolute_error(y_test, y_test_pred_mlp)

print(&quot;=&quot; * 60)
print(&quot;ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆMLP Regressorï¼‰&quot;)
print(&quot;=&quot; * 60)
print(f&quot;è¨“ç·´ãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_train_mlp:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_test_mlp:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RMSE: {rmse_test_mlp:.4f} nm&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ MAE: {mae_test_mlp:.4f} nm&quot;)
print(f&quot;åå¾©å›æ•°: {model_mlp.n_iter_}&quot;)
print(f&quot;éš ã‚Œå±¤ã®æ§‹é€ : {model_mlp.hidden_layer_sizes}&quot;)
</code></pre>
<h3>ã€ä¾‹15ã€‘ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ</h3>
<pre><code class="language-python"># å…¨ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ã¾ã¨ã‚ã‚‹
results = pd.DataFrame({
    'Model': ['Linear Regression', 'Random Forest', 'LightGBM', 'SVR', 'MLP'],
    'RÂ² (Train)': [r2_train_lr, r2_train_rf, r2_train_lgb, r2_train_svr, r2_train_mlp],
    'RÂ² (Test)': [r2_test_lr, r2_test_rf, r2_test_lgb, r2_test_svr, r2_test_mlp],
    'RMSE (Test)': [rmse_test_lr, rmse_test_rf, rmse_test_lgb, rmse_test_svr, rmse_test_mlp],
    'MAE (Test)': [mae_test_lr, mae_test_rf, mae_test_lgb, mae_test_svr, mae_test_mlp]
})

results['Overfit'] = results['RÂ² (Train)'] - results['RÂ² (Test)']

print(&quot;=&quot; * 80)
print(&quot;å…¨ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æ¯”è¼ƒ&quot;)
print(&quot;=&quot; * 80)
print(results.to_string(index=False))

# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å®š
best_model_idx = results['RÂ² (Test)'].idxmax()
best_model_name = results.loc[best_model_idx, 'Model']
best_r2 = results.loc[best_model_idx, 'RÂ² (Test)']

print(f&quot;\næœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model_name} (RÂ² = {best_r2:.4f})&quot;)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# RÂ²ã‚¹ã‚³ã‚¢æ¯”è¼ƒ
x_pos = np.arange(len(results))
axes[0].bar(x_pos, results['RÂ² (Test)'], alpha=0.7, color='steelblue',
            edgecolor='black', label='Test RÂ²')
axes[0].set_xticks(x_pos)
axes[0].set_xticklabels(results['Model'], rotation=15, ha='right')
axes[0].set_ylabel('RÂ² Score', fontsize=12)
axes[0].set_title('Model Comparison: RÂ² Score', fontsize=13, fontweight='bold')
axes[0].grid(True, alpha=0.3, axis='y')
axes[0].legend()

# RMSEæ¯”è¼ƒ
axes[1].bar(x_pos, results['RMSE (Test)'], alpha=0.7, color='coral',
            edgecolor='black', label='Test RMSE')
axes[1].set_xticks(x_pos)
axes[1].set_xticklabels(results['Model'], rotation=15, ha='right')
axes[1].set_ylabel('RMSE (nm)', fontsize=12)
axes[1].set_title('Model Comparison: RMSE', fontsize=13, fontweight='bold')
axes[1].grid(True, alpha=0.3, axis='y')
axes[1].legend()

plt.tight_layout()
plt.show()
</code></pre>
<hr />
<h2>3.5 é‡å­ãƒ‰ãƒƒãƒˆç™ºå…‰æ³¢é•·äºˆæ¸¬</h2>
<h3>ã€ä¾‹16ã€‘ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼šCdSeé‡å­ãƒ‰ãƒƒãƒˆ</h3>
<p>CdSeé‡å­ãƒ‰ãƒƒãƒˆã®ç™ºå…‰æ³¢é•·ã¯ã€Brusæ–¹ç¨‹å¼ã«åŸºã¥ãã‚µã‚¤ã‚ºã«ä¾å­˜ã—ã¾ã™ã€‚</p>
<pre><code class="language-python"># CdSeé‡å­ãƒ‰ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
np.random.seed(100)

n_qd_samples = 150

# é‡å­ãƒ‰ãƒƒãƒˆã®ã‚µã‚¤ã‚ºï¼ˆ2-10 nmï¼‰
size_qd = np.random.uniform(2, 10, n_qd_samples)

# Brusæ–¹ç¨‹å¼ã®ç°¡æ˜“è¿‘ä¼¼: emission = 520 + 130/(size^0.8) + noise
emission = 520 + 130 / (size_qd ** 0.8) + np.random.normal(0, 10, n_qd_samples)

# åˆæˆæ¡ä»¶
synthesis_time = np.random.uniform(10, 120, n_qd_samples)  # åˆ†
precursor_ratio = np.random.uniform(0.5, 2.0, n_qd_samples)  # ãƒ¢ãƒ«æ¯”

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
data_qd = pd.DataFrame({
    'size_nm': size_qd,
    'emission_nm': emission,
    'synthesis_time_min': synthesis_time,
    'precursor_ratio': precursor_ratio
})

print(&quot;=&quot; * 60)
print(&quot;CdSeé‡å­ãƒ‰ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆå®Œäº†&quot;)
print(&quot;=&quot; * 60)
print(data_qd.head(10))
print(&quot;\nåŸºæœ¬çµ±è¨ˆé‡:&quot;)
print(data_qd.describe())

# ã‚µã‚¤ã‚ºã¨ç™ºå…‰æ³¢é•·ã®é–¢ä¿‚ãƒ—ãƒ­ãƒƒãƒˆ
fig, ax = plt.subplots(figsize=(10, 6))
scatter = ax.scatter(data_qd['size_nm'], data_qd['emission_nm'],
                     c=data_qd['synthesis_time_min'], cmap='plasma',
                     s=80, alpha=0.7, edgecolors='k')
ax.set_xlabel('Quantum Dot Size (nm)', fontsize=12)
ax.set_ylabel('Emission Wavelength (nm)', fontsize=12)
ax.set_title('CdSe Quantum Dot: Size vs Emission Wavelength',
             fontsize=13, fontweight='bold')
cbar = plt.colorbar(scatter, ax=ax)
cbar.set_label('Synthesis Time (min)', fontsize=10)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>
<h3>ã€ä¾‹17ã€‘é‡å­ãƒ‰ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆLightGBMï¼‰</h3>
<pre><code class="language-python"># ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
X_qd = data_qd[['size_nm', 'synthesis_time_min', 'precursor_ratio']]
y_qd = data_qd['emission_nm']

# ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
scaler_qd = StandardScaler()
X_qd_scaled = scaler_qd.fit_transform(X_qd)

# è¨“ç·´/ãƒ†ã‚¹ãƒˆåˆ†å‰²
X_qd_train, X_qd_test, y_qd_train, y_qd_test = train_test_split(
    X_qd_scaled, y_qd, test_size=0.2, random_state=42
)

# LightGBMãƒ¢ãƒ‡ãƒ«
model_qd = lgb.LGBMRegressor(
    objective='regression',
    num_leaves=31,
    learning_rate=0.05,
    n_estimators=200,
    random_state=42,
    verbose=-1
)

model_qd.fit(X_qd_train, y_qd_train)

# äºˆæ¸¬
y_qd_train_pred = model_qd.predict(X_qd_train)
y_qd_test_pred = model_qd.predict(X_qd_test)

# è©•ä¾¡
r2_qd_train = r2_score(y_qd_train, y_qd_train_pred)
r2_qd_test = r2_score(y_qd_test, y_qd_test_pred)
rmse_qd = np.sqrt(mean_squared_error(y_qd_test, y_qd_test_pred))
mae_qd = mean_absolute_error(y_qd_test, y_qd_test_pred)

print(&quot;=&quot; * 60)
print(&quot;é‡å­ãƒ‰ãƒƒãƒˆç™ºå…‰æ³¢é•·äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ï¼ˆLightGBMï¼‰&quot;)
print(&quot;=&quot; * 60)
print(f&quot;è¨“ç·´ãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_qd_train:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_qd_test:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RMSE: {rmse_qd:.4f} nm&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ MAE: {mae_qd:.4f} nm&quot;)
</code></pre>
<h3>ã€ä¾‹18ã€‘äºˆæ¸¬çµæœã®å¯è¦–åŒ–</h3>
<pre><code class="language-python"># äºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤ãƒ—ãƒ­ãƒƒãƒˆï¼ˆä¿¡é ¼åŒºé–“ä»˜ãï¼‰
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ—ãƒ­ãƒƒãƒˆ
axes[0].scatter(y_qd_test, y_qd_test_pred, alpha=0.6, s=80,
                edgecolors='k', label='Test Data')
axes[0].plot([y_qd_test.min(), y_qd_test.max()],
             [y_qd_test.min(), y_qd_test.max()],
             'r--', lw=2, label='Perfect Prediction')

# Â±10 nm ã®ç¯„å›²ã‚’è¡¨ç¤º
axes[0].fill_between([y_qd_test.min(), y_qd_test.max()],
                     [y_qd_test.min()-10, y_qd_test.max()-10],
                     [y_qd_test.min()+10, y_qd_test.max()+10],
                     alpha=0.2, color='gray', label='Â±10 nm')

axes[0].set_xlabel('Actual Emission (nm)', fontsize=12)
axes[0].set_ylabel('Predicted Emission (nm)', fontsize=12)
axes[0].set_title(f'QD Emission Prediction (RÂ² = {r2_qd_test:.3f})',
                  fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# ã‚µã‚¤ã‚ºåˆ¥ã®äºˆæ¸¬ç²¾åº¦
size_bins = [2, 4, 6, 8, 10]
size_labels = ['2-4 nm', '4-6 nm', '6-8 nm', '8-10 nm']
data_qd_test = pd.DataFrame({
    'size': X_qd.iloc[y_qd_test.index]['size_nm'].values,
    'actual': y_qd_test.values,
    'predicted': y_qd_test_pred
})
data_qd_test['size_bin'] = pd.cut(data_qd_test['size'], bins=size_bins, labels=size_labels)
data_qd_test['error'] = np.abs(data_qd_test['actual'] - data_qd_test['predicted'])

# ã‚µã‚¤ã‚ºãƒ“ãƒ³ã”ã¨ã®å¹³å‡èª¤å·®
error_by_size = data_qd_test.groupby('size_bin')['error'].mean()

axes[1].bar(range(len(error_by_size)), error_by_size.values,
            color='coral', edgecolor='black', alpha=0.7)
axes[1].set_xticks(range(len(error_by_size)))
axes[1].set_xticklabels(error_by_size.index)
axes[1].set_ylabel('Mean Absolute Error (nm)', fontsize=12)
axes[1].set_xlabel('QD Size Range', fontsize=12)
axes[1].set_title('Prediction Error by QD Size', fontsize=13, fontweight='bold')
axes[1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print(f&quot;\nå…¨ä½“ã®å¹³å‡çµ¶å¯¾èª¤å·®: {mae_qd:.2f} nm&quot;)
print(&quot;ã‚µã‚¤ã‚ºåˆ¥ã®å¹³å‡çµ¶å¯¾èª¤å·®:&quot;)
print(error_by_size)
</code></pre>
<hr />
<h2>3.6 ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ</h2>
<h3>ã€ä¾‹19ã€‘ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆLightGBMï¼‰</h3>
<pre><code class="language-python"># LightGBMãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆã‚²ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹ï¼‰
importance_gain = model_lgb.feature_importances_
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': importance_gain
}).sort_values('Importance', ascending=False)

print(&quot;=&quot; * 60)
print(&quot;ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆLightGBMï¼‰&quot;)
print(&quot;=&quot; * 60)
print(importance_df)

# å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(8, 5))
colors = ['steelblue', 'coral', 'lightgreen']
ax.barh(importance_df['Feature'], importance_df['Importance'],
        color=colors, edgecolor='black')
ax.set_xlabel('Feature Importance (Gain)', fontsize=12)
ax.set_title('Feature Importance: LSPR Prediction',
             fontsize=13, fontweight='bold')
ax.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()

print(f&quot;\næœ€ã‚‚é‡è¦ãªç‰¹å¾´é‡: {importance_df.iloc[0]['Feature']}&quot;)
</code></pre>
<h3>ã€ä¾‹20ã€‘SHAPåˆ†æï¼šäºˆæ¸¬è§£é‡ˆ</h3>
<pre><code class="language-python">import shap

# SHAP Explainerã®ä½œæˆ
explainer = shap.Explainer(model_lgb, X_train)
shap_values = explainer(X_test)

print(&quot;=&quot; * 60)
print(&quot;SHAPåˆ†æ&quot;)
print(&quot;=&quot; * 60)
print(&quot;SHAPå€¤ã®è¨ˆç®—å®Œäº†&quot;)
print(f&quot;SHAPå€¤ã®å½¢çŠ¶: {shap_values.values.shape}&quot;)

# SHAP Summary Plot
fig, ax = plt.subplots(figsize=(10, 6))
shap.summary_plot(shap_values, X_test, feature_names=X.columns, show=False)
plt.title('SHAP Summary Plot: Feature Impact on LSPR Prediction',
          fontsize=13, fontweight='bold', pad=20)
plt.tight_layout()
plt.show()

# SHAP Dependence Plotï¼ˆæœ€ã‚‚é‡è¦ãªç‰¹å¾´é‡ï¼‰
top_feature_idx = importance_df.index[0]
top_feature_name = X.columns[top_feature_idx]

fig, ax = plt.subplots(figsize=(10, 6))
shap.dependence_plot(top_feature_idx, shap_values.values, X_test,
                     feature_names=X.columns, show=False)
plt.title(f'SHAP Dependence Plot: {top_feature_name}',
          fontsize=13, fontweight='bold')
plt.tight_layout()
plt.show()

print(f&quot;\nSHAPåˆ†æã«ã‚ˆã‚Šã€{top_feature_name}ãŒLSPRæ³¢é•·äºˆæ¸¬ã«æœ€ã‚‚å½±éŸ¿ã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã¾ã—ãŸ&quot;)
</code></pre>
<hr />
<h2>3.7 ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚‹ãƒŠãƒææ–™è¨­è¨ˆ</h2>
<p>ç›®æ¨™ï¼šç›®æ¨™LSPRæ³¢é•·ï¼ˆ550 nmï¼‰ã‚’å®Ÿç¾ã™ã‚‹æœ€é©ãªåˆæˆæ¡ä»¶ã‚’æ¢ç´¢</p>
<h3>ã€ä¾‹21ã€‘æ¢ç´¢ç©ºé–“ã®å®šç¾©</h3>
<pre><code class="language-python">from skopt.space import Real

# æ¢ç´¢ç©ºé–“ã®å®šç¾©
# ã‚µã‚¤ã‚º: 10-40 nmã€æ¸©åº¦: 20-80Â°Cã€pH: 4-10
search_space = [
    Real(10, 40, name='size_nm'),
    Real(20, 80, name='temperature_C'),
    Real(4, 10, name='pH')
]

print(&quot;=&quot; * 60)
print(&quot;ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ï¼šæ¢ç´¢ç©ºé–“ã®å®šç¾©&quot;)
print(&quot;=&quot; * 60)
for dim in search_space:
    print(f&quot;  {dim.name}: [{dim.bounds[0]}, {dim.bounds[1]}]&quot;)

print(&quot;\nç›®æ¨™: LSPRæ³¢é•· = 550 nm ã‚’å®Ÿç¾ã™ã‚‹æ¡ä»¶ã‚’æ¢ç´¢&quot;)
</code></pre>
<h3>ã€ä¾‹22ã€‘ç›®çš„é–¢æ•°ã®è¨­å®š</h3>
<pre><code class="language-python"># ç›®çš„é–¢æ•°ï¼šäºˆæ¸¬LSPRæ³¢é•·ã¨ç›®æ¨™æ³¢é•·ï¼ˆ550 nmï¼‰ã®å·®ã®çµ¶å¯¾å€¤ã‚’æœ€å°åŒ–
target_lspr = 550.0

def objective_function(params):
    &quot;&quot;&quot;
    ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ç›®çš„é–¢æ•°

    Parameters:
    -----------
    params : list
        [size_nm, temperature_C, pH]

    Returns:
    --------
    float
        ç›®æ¨™æ³¢é•·ã¨ã®èª¤å·®ï¼ˆæœ€å°åŒ–ã™ã‚‹å€¤ï¼‰
    &quot;&quot;&quot;
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
    size, temp, ph = params

    # ç‰¹å¾´é‡ã‚’æ§‹ç¯‰ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°é©ç”¨ï¼‰
    features = np.array([[size, temp, ph]])
    features_scaled = scaler.transform(features)

    # LSPRæ³¢é•·ã‚’äºˆæ¸¬
    predicted_lspr = model_lgb.predict(features_scaled)[0]

    # ç›®æ¨™æ³¢é•·ã¨ã®èª¤å·®ï¼ˆçµ¶å¯¾å€¤ï¼‰
    error = abs(predicted_lspr - target_lspr)

    return error

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
test_params = [20.0, 50.0, 7.0]
test_error = objective_function(test_params)
print(f&quot;\nãƒ†ã‚¹ãƒˆå®Ÿè¡Œ:&quot;)
print(f&quot;  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: size={test_params[0]} nm, temp={test_params[1]}Â°C, pH={test_params[2]}&quot;)
print(f&quot;  ç›®çš„é–¢æ•°å€¤ï¼ˆèª¤å·®ï¼‰: {test_error:.4f} nm&quot;)
</code></pre>
<h3>ã€ä¾‹23ã€‘ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®å®Ÿè¡Œï¼ˆscikit-optimizeï¼‰</h3>
<pre><code class="language-python">from skopt import gp_minimize
from skopt.plots import plot_convergence, plot_objective

# ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®å®Ÿè¡Œ
print(&quot;\n&quot; + &quot;=&quot; * 60)
print(&quot;ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®å®Ÿè¡Œä¸­...&quot;)
print(&quot;=&quot; * 60)

result = gp_minimize(
    func=objective_function,
    dimensions=search_space,
    n_calls=50,  # è©•ä¾¡å›æ•°
    n_initial_points=10,  # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å›æ•°
    random_state=42,
    verbose=False
)

print(&quot;æœ€é©åŒ–å®Œäº†ï¼&quot;)
print(&quot;\n&quot; + &quot;=&quot; * 60)
print(&quot;æœ€é©åŒ–çµæœ&quot;)
print(&quot;=&quot; * 60)
print(f&quot;æœ€å°ç›®çš„é–¢æ•°å€¤ï¼ˆèª¤å·®ï¼‰: {result.fun:.4f} nm&quot;)
print(f&quot;\næœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:&quot;)
print(f&quot;  ã‚µã‚¤ã‚º: {result.x[0]:.2f} nm&quot;)
print(f&quot;  æ¸©åº¦: {result.x[1]:.2f} Â°C&quot;)
print(f&quot;  pH: {result.x[2]:.2f}&quot;)

# æœ€é©æ¡ä»¶ã§ã®äºˆæ¸¬LSPRæ³¢é•·ã‚’è¨ˆç®—
optimal_features = np.array([result.x])
optimal_features_scaled = scaler.transform(optimal_features)
predicted_optimal_lspr = model_lgb.predict(optimal_features_scaled)[0]

print(f&quot;\näºˆæ¸¬ã•ã‚Œã‚‹LSPRæ³¢é•·: {predicted_optimal_lspr:.2f} nm&quot;)
print(f&quot;ç›®æ¨™LSPRæ³¢é•·: {target_lspr} nm&quot;)
print(f&quot;é”æˆç²¾åº¦: {abs(predicted_optimal_lspr - target_lspr):.2f} nm&quot;)
</code></pre>
<h3>ã€ä¾‹24ã€‘æœ€é©åŒ–çµæœã®å¯è¦–åŒ–</h3>
<pre><code class="language-python"># æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã®å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# åæŸãƒ—ãƒ­ãƒƒãƒˆ
plot_convergence(result, ax=axes[0])
axes[0].set_title('Convergence Plot', fontsize=13, fontweight='bold')
axes[0].set_ylabel('Objective Value (Error, nm)', fontsize=11)
axes[0].set_xlabel('Number of Evaluations', fontsize=11)
axes[0].grid(True, alpha=0.3)

# è©•ä¾¡å±¥æ­´ã®ãƒ—ãƒ­ãƒƒãƒˆ
iterations = range(1, len(result.func_vals) + 1)
axes[1].plot(iterations, result.func_vals, 'o-', alpha=0.6, label='Evaluation')
axes[1].plot(iterations, np.minimum.accumulate(result.func_vals),
             'r-', linewidth=2, label='Best So Far')
axes[1].set_xlabel('Iteration', fontsize=11)
axes[1].set_ylabel('Objective Value (Error, nm)', fontsize=11)
axes[1].set_title('Optimization Progress', fontsize=13, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>ã€ä¾‹25ã€‘åæŸãƒ—ãƒ­ãƒƒãƒˆ</h3>
<pre><code class="language-python"># è©³ç´°ãªåæŸãƒ—ãƒ­ãƒƒãƒˆï¼ˆæœ€è‰¯å€¤ã®æ¨ç§»ï¼‰
fig, ax = plt.subplots(figsize=(10, 6))

cumulative_min = np.minimum.accumulate(result.func_vals)
iterations = np.arange(1, len(cumulative_min) + 1)

ax.plot(iterations, cumulative_min, 'b-', linewidth=2, marker='o',
        markersize=4, label='Best Error')
ax.axhline(y=result.fun, color='r', linestyle='--', linewidth=2,
           label=f'Final Best: {result.fun:.2f} nm')
ax.fill_between(iterations, 0, cumulative_min, alpha=0.2, color='blue')

ax.set_xlabel('Iteration', fontsize=12)
ax.set_ylabel('Minimum Error (nm)', fontsize=12)
ax.set_title('Bayesian Optimization: Convergence to Optimal Solution',
             fontsize=13, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f&quot;\n{len(result.func_vals)}å›ã®è©•ä¾¡ã§æœ€é©è§£ã«åæŸã—ã¾ã—ãŸ&quot;)
print(f&quot;åˆæœŸè©•ä¾¡ã§ã®æœ€è‰¯èª¤å·®: {result.func_vals[0]:.2f} nm&quot;)
print(f&quot;æœ€çµ‚çš„ãªæœ€è‰¯èª¤å·®: {result.fun:.2f} nm&quot;)
print(f&quot;æ”¹å–„ç‡: {(1 - result.fun/result.func_vals[0])*100:.1f}%&quot;)
</code></pre>
<hr />
<h2>3.8 å¤šç›®çš„æœ€é©åŒ–ï¼šã‚µã‚¤ã‚ºã¨ç™ºå…‰åŠ¹ç‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•</h2>
<h3>ã€ä¾‹26ã€‘Paretoæœ€é©åŒ–ï¼ˆNSGA-IIï¼‰</h3>
<p>å¤šç›®çš„æœ€é©åŒ–ã§ã¯ã€è¤‡æ•°ã®ç›®çš„ã‚’åŒæ™‚ã«æœ€é©åŒ–ã—ã¾ã™ã€‚ã“ã“ã§ã¯ã€é‡å­ãƒ‰ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã‚’æœ€å°åŒ–ã—ã¤ã¤ã€ç™ºå…‰åŠ¹ç‡ï¼ˆä»®æƒ³çš„ãªæŒ‡æ¨™ï¼‰ã‚’æœ€å¤§åŒ–ã—ã¾ã™ã€‚</p>
<pre><code class="language-python"># pymooã‚’ä½¿ç”¨ã—ãŸå¤šç›®çš„æœ€é©åŒ–
try:
    from pymoo.core.problem import Problem
    from pymoo.algorithms.moo.nsga2 import NSGA2
    from pymoo.optimize import minimize as pymoo_minimize
    from pymoo.operators.crossover.sbx import SBX
    from pymoo.operators.mutation.pm import PM
    from pymoo.operators.sampling.rnd import FloatRandomSampling

    # å¤šç›®çš„æœ€é©åŒ–å•é¡Œã®å®šç¾©
    class QuantumDotOptimization(Problem):
        def __init__(self):
            super().__init__(
                n_var=3,  # å¤‰æ•°æ•°ï¼ˆsize, synthesis_time, precursor_ratioï¼‰
                n_obj=2,  # ç›®çš„é–¢æ•°æ•°ï¼ˆsizeæœ€å°åŒ–ã€emissionåŠ¹ç‡æœ€å¤§åŒ–ï¼‰
                n_constr=0,  # åˆ¶ç´„ãªã—
                xl=np.array([2.0, 10.0, 0.5]),  # ä¸‹é™
                xu=np.array([10.0, 120.0, 2.0])  # ä¸Šé™
            )

        def _evaluate(self, X, out, *args, **kwargs):
            # ç›®çš„é–¢æ•°1: ã‚µã‚¤ã‚ºã®æœ€å°åŒ–
            obj1 = X[:, 0]  # size

            # ç›®çš„é–¢æ•°2: ç™ºå…‰åŠ¹ç‡ã®æœ€å¤§åŒ–ï¼ˆè² ã®å€¤ã§æœ€å°åŒ–å•é¡Œã«å¤‰æ›ï¼‰
            # åŠ¹ç‡ã¯ä»®æƒ³çš„ã«ã€emission wavelengthãŒ550 nmã«è¿‘ã„ã»ã©é«˜ã„ã¨ä»®å®š
            features = X  # [size, synthesis_time, precursor_ratio]
            features_scaled = scaler_qd.transform(features)
            predicted_emission = model_qd.predict(features_scaled)

            # åŠ¹ç‡ï¼š550 nmã‹ã‚‰ã®ãšã‚ŒãŒå°ã•ã„ã»ã©é«˜ã„ï¼ˆè² ã®å€¤ã§æœ€å¤§åŒ–â†’æœ€å°åŒ–ï¼‰
            efficiency = -np.abs(predicted_emission - 550)
            obj2 = -efficiency  # æœ€å¤§åŒ–ã‚’æœ€å°åŒ–å•é¡Œã«å¤‰æ›

            out[&quot;F&quot;] = np.column_stack([obj1, obj2])

    # å•é¡Œã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    problem = QuantumDotOptimization()

    # NSGA-IIã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    algorithm = NSGA2(
        pop_size=40,
        sampling=FloatRandomSampling(),
        crossover=SBX(prob=0.9, eta=15),
        mutation=PM(eta=20),
        eliminate_duplicates=True
    )

    # æœ€é©åŒ–å®Ÿè¡Œ
    print(&quot;=&quot; * 60)
    print(&quot;å¤šç›®çš„æœ€é©åŒ–ï¼ˆNSGA-IIï¼‰å®Ÿè¡Œä¸­...&quot;)
    print(&quot;=&quot; * 60)

    res = pymoo_minimize(
        problem,
        algorithm,
        ('n_gen', 50),  # ä¸–ä»£æ•°
        seed=42,
        verbose=False
    )

    print(&quot;å¤šç›®çš„æœ€é©åŒ–å®Œäº†ï¼&quot;)
    print(f&quot;\nãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ã®æ•°: {len(res.F)}&quot;)

    # ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ã®è¡¨ç¤ºï¼ˆä¸Šä½5ã¤ï¼‰
    print(&quot;\nä»£è¡¨çš„ãªãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ï¼ˆä¸Šä½5ã¤ï¼‰:&quot;)
    pareto_solutions = pd.DataFrame({
        'Size (nm)': res.X[:, 0],
        'Synthesis Time (min)': res.X[:, 1],
        'Precursor Ratio': res.X[:, 2],
        'Obj1: Size': res.F[:, 0],
        'Obj2: -Efficiency': res.F[:, 1]
    }).head(5)
    print(pareto_solutions.to_string(index=False))

    PYMOO_AVAILABLE = True

except ImportError:
    print(&quot;=&quot; * 60)
    print(&quot;pymooãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“&quot;)
    print(&quot;=&quot; * 60)
    print(&quot;å¤šç›®çš„æœ€é©åŒ–ã«ã¯pymooãŒå¿…è¦ã§ã™:&quot;)
    print(&quot;  pip install pymoo&quot;)
    print(&quot;\nä»£ã‚ã‚Šã«ã€ç°¡æ˜“çš„ãªå¤šç›®çš„æœ€é©åŒ–ã®ä¾‹ã‚’è¡¨ç¤ºã—ã¾ã™&quot;)

    # ç°¡æ˜“çš„ãªã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã«ã‚ˆã‚‹å¤šç›®çš„æœ€é©åŒ–ã®æ¨¡æ“¬
    sizes = np.linspace(2, 10, 20)
    times = np.linspace(10, 120, 20)
    ratios = np.linspace(0.5, 2.0, 20)

    # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
    sample_X = []
    sample_F = []

    for size in sizes[::4]:
        for time in times[::4]:
            for ratio in ratios[::4]:
                features = np.array([[size, time, ratio]])
                features_scaled = scaler_qd.transform(features)
                emission = model_qd.predict(features_scaled)[0]

                obj1 = size
                obj2 = abs(emission - 550)

                sample_X.append([size, time, ratio])
                sample_F.append([obj1, obj2])

    sample_X = np.array(sample_X)
    sample_F = np.array(sample_F)

    print(&quot;\nã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã«ã‚ˆã‚‹è§£ã®æ¢ç´¢å®Œäº†&quot;)
    print(f&quot;æ¢ç´¢ã—ãŸè§£ã®æ•°: {len(sample_F)}&quot;)

    res = type('Result', (), {
        'X': sample_X,
        'F': sample_F
    })()

    PYMOO_AVAILABLE = False
</code></pre>
<h3>ã€ä¾‹27ã€‘Paretoãƒ•ãƒ­ãƒ³ãƒˆã®å¯è¦–åŒ–</h3>
<pre><code class="language-python"># Paretoãƒ•ãƒ­ãƒ³ãƒˆã®å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(10, 7))

if PYMOO_AVAILABLE:
    # NSGA-IIã®çµæœã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    ax.scatter(res.F[:, 0], -res.F[:, 1], c='blue', s=80, alpha=0.6,
               edgecolors='black', label='Pareto Optimal Solutions')

    title_suffix = &quot;(NSGA-II)&quot;
else:
    # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã®çµæœã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    ax.scatter(res.F[:, 0], res.F[:, 1], c='blue', s=60, alpha=0.5,
               edgecolors='black', label='Sampled Solutions')

    title_suffix = &quot;(Grid Search)&quot;

ax.set_xlabel('Objective 1: Size (nm) [Minimize]', fontsize=12)

if PYMOO_AVAILABLE:
    ax.set_ylabel('Objective 2: Efficiency [Maximize]', fontsize=12)
else:
    ax.set_ylabel('Objective 2: Deviation from 550nm [Minimize]', fontsize=12)

ax.set_title(f'Pareto Front: Size vs Emission Efficiency {title_suffix}',
             fontsize=13, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;\nãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆ:&quot;)
print(&quot;  ã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹ã¨åŠ¹ç‡ãŒä¸‹ãŒã‚Šã€åŠ¹ç‡ã‚’ä¸Šã’ã‚‹ã¨ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹&quot;)
print(&quot;  â†’ ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•é–¢ä¿‚ãŒæ˜ç¢ºã«&quot;)
</code></pre>
<hr />
<h2>3.9 TEMç”»åƒè§£æã¨ã‚µã‚¤ã‚ºåˆ†å¸ƒ</h2>
<h3>ã€ä¾‹28ã€‘æ¨¡æ“¬TEMãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ</h3>
<p>TEMï¼ˆé€éå‹é›»å­é¡•å¾®é¡ï¼‰ã§æ¸¬å®šã•ã‚Œã‚‹ãƒŠãƒç²’å­ã‚µã‚¤ã‚ºã¯ã€ã—ã°ã—ã°å¯¾æ•°æ­£è¦åˆ†å¸ƒã«å¾“ã„ã¾ã™ã€‚</p>
<pre><code class="language-python">from scipy.stats import lognorm

# å¯¾æ•°æ­£è¦åˆ†å¸ƒã«å¾“ã†TEMã‚µã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
np.random.seed(200)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
mean_size = 20  # å¹³å‡ã‚µã‚¤ã‚ºï¼ˆnmï¼‰
cv = 0.3  # å¤‰å‹•ä¿‚æ•°ï¼ˆæ¨™æº–åå·®/å¹³å‡ï¼‰

# å¯¾æ•°æ­£è¦åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨ˆç®—
sigma = np.sqrt(np.log(1 + cv**2))
mu = np.log(mean_size) - 0.5 * sigma**2

# ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆï¼ˆ500ç²’å­ï¼‰
tem_sizes = lognorm.rvs(s=sigma, scale=np.exp(mu), size=500)

print(&quot;=&quot; * 60)
print(&quot;TEMæ¸¬å®šãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆå¯¾æ•°æ­£è¦åˆ†å¸ƒï¼‰&quot;)
print(&quot;=&quot; * 60)
print(f&quot;ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(tem_sizes)}ç²’å­&quot;)
print(f&quot;å¹³å‡ã‚µã‚¤ã‚º: {tem_sizes.mean():.2f} nm&quot;)
print(f&quot;æ¨™æº–åå·®: {tem_sizes.std():.2f} nm&quot;)
print(f&quot;ä¸­å¤®å€¤: {np.median(tem_sizes):.2f} nm&quot;)
print(f&quot;æœ€å°å€¤: {tem_sizes.min():.2f} nm&quot;)
print(f&quot;æœ€å¤§å€¤: {tem_sizes.max():.2f} nm&quot;)

# ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
fig, ax = plt.subplots(figsize=(10, 6))
ax.hist(tem_sizes, bins=40, alpha=0.7, color='lightblue',
        edgecolor='black', density=True, label='TEM Data')
ax.set_xlabel('Particle Size (nm)', fontsize=12)
ax.set_ylabel('Probability Density', fontsize=12)
ax.set_title('TEM Size Distribution (Lognormal)', fontsize=13, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()
</code></pre>
<h3>ã€ä¾‹29ã€‘å¯¾æ•°æ­£è¦åˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°</h3>
<pre><code class="language-python"># å¯¾æ•°æ­£è¦åˆ†å¸ƒã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
shape_fit, loc_fit, scale_fit = lognorm.fit(tem_sizes, floc=0)

# ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã•ã‚ŒãŸåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
fitted_mean = np.exp(np.log(scale_fit) + 0.5 * shape_fit**2)
fitted_std = fitted_mean * np.sqrt(np.exp(shape_fit**2) - 1)

print(&quot;=&quot; * 60)
print(&quot;å¯¾æ•°æ­£è¦åˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœ&quot;)
print(&quot;=&quot; * 60)
print(f&quot;å½¢çŠ¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (sigma): {shape_fit:.4f}&quot;)
print(f&quot;ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {scale_fit:.4f}&quot;)
print(f&quot;ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã•ã‚ŒãŸå¹³å‡ã‚µã‚¤ã‚º: {fitted_mean:.2f} nm&quot;)
print(f&quot;ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã•ã‚ŒãŸæ¨™æº–åå·®: {fitted_std:.2f} nm&quot;)

# å®Ÿæ¸¬å€¤ã¨ã®æ¯”è¼ƒ
print(f&quot;\nå®Ÿæ¸¬å€¤ã¨ã®æ¯”è¼ƒ:&quot;)
print(f&quot;  å¹³å‡ã‚µã‚¤ã‚º - å®Ÿæ¸¬: {tem_sizes.mean():.2f} nm, ãƒ•ã‚£ãƒƒãƒˆ: {fitted_mean:.2f} nm&quot;)
print(f&quot;  æ¨™æº–åå·® - å®Ÿæ¸¬: {tem_sizes.std():.2f} nm, ãƒ•ã‚£ãƒƒãƒˆ: {fitted_std:.2f} nm&quot;)
</code></pre>
<h3>ã€ä¾‹30ã€‘ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœã®å¯è¦–åŒ–</h3>
<pre><code class="language-python"># ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœã®è©³ç´°å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã¨ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°æ›²ç·š
axes[0].hist(tem_sizes, bins=40, alpha=0.6, color='lightblue',
             edgecolor='black', density=True, label='TEM Data')

# ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã•ã‚ŒãŸå¯¾æ•°æ­£è¦åˆ†å¸ƒ
x_range = np.linspace(0, tem_sizes.max(), 200)
fitted_pdf = lognorm.pdf(x_range, shape_fit, loc=loc_fit, scale=scale_fit)
axes[0].plot(x_range, fitted_pdf, 'r-', linewidth=2,
             label=f'Lognormal Fit (Î¼={fitted_mean:.1f}, Ïƒ={fitted_std:.1f})')

axes[0].set_xlabel('Particle Size (nm)', fontsize=12)
axes[0].set_ylabel('Probability Density', fontsize=12)
axes[0].set_title('TEM Size Distribution with Lognormal Fit',
                  fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3, axis='y')

# Q-Qãƒ—ãƒ­ãƒƒãƒˆï¼ˆåˆ†ä½ç‚¹ãƒ—ãƒ­ãƒƒãƒˆï¼‰
from scipy.stats import probplot

probplot(tem_sizes, dist=lognorm, sparams=(shape_fit, loc_fit, scale_fit),
         plot=axes[1])
axes[1].set_title('Q-Q Plot: Lognormal Distribution',
                  fontsize=13, fontweight='bold')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;\nQ-Qãƒ—ãƒ­ãƒƒãƒˆ: ãƒ‡ãƒ¼ã‚¿ãŒç›´ç·šä¸Šã«ã‚ã‚Œã°ã€å¯¾æ•°æ­£è¦åˆ†å¸ƒã«è‰¯ãå¾“ã£ã¦ã„ã¾ã™&quot;)
</code></pre>
<hr />
<h2>3.10 åˆ†å­å‹•åŠ›å­¦ï¼ˆMDï¼‰ãƒ‡ãƒ¼ã‚¿è§£æ</h2>
<h3>ã€ä¾‹31ã€‘MDã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿</h3>
<p>åˆ†å­å‹•åŠ›å­¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒŠãƒç²’å­ã®åŸå­é…ç½®ã®æ™‚é–“ç™ºå±•ã‚’è¿½è·¡ã—ã¾ã™ã€‚</p>
<pre><code class="language-python"># MDã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®æ¨¡æ“¬ç”Ÿæˆ
# å®Ÿéš›ã®MDãƒ‡ãƒ¼ã‚¿ã¯LAMMPS, GROMACSç­‰ã‹ã‚‰å–å¾—

np.random.seed(300)

n_atoms = 100  # åŸå­æ•°
n_steps = 1000  # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°
dt = 0.001  # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆpsï¼‰

# åˆæœŸä½ç½®ï¼ˆnmï¼‰
positions_initial = np.random.uniform(-1, 1, (n_atoms, 3))

# æ™‚é–“ç™ºå±•ã®æ¨¡æ“¬ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ï¼‰
positions = np.zeros((n_steps, n_atoms, 3))
positions[0] = positions_initial

for t in range(1, n_steps):
    # ãƒ©ãƒ³ãƒ€ãƒ ãªå¤‰ä½
    displacement = np.random.normal(0, 0.01, (n_atoms, 3))
    positions[t] = positions[t-1] + displacement

print(&quot;=&quot; * 60)
print(&quot;MDã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ&quot;)
print(&quot;=&quot; * 60)
print(f&quot;åŸå­æ•°: {n_atoms}&quot;)
print(f&quot;ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°: {n_steps}&quot;)
print(f&quot;ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ™‚é–“: {n_steps * dt:.2f} ps&quot;)
print(f&quot;ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {positions.shape} (time, atoms, xyz)&quot;)

# ä¸­å¿ƒåŸå­ï¼ˆåŸå­0ï¼‰ã®è»Œè·¡ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
fig = plt.figure(figsize=(12, 5))

ax1 = fig.add_subplot(121, projection='3d')
ax1.plot(positions[:, 0, 0], positions[:, 0, 1], positions[:, 0, 2],
         'b-', alpha=0.5, linewidth=0.5)
ax1.scatter(positions[0, 0, 0], positions[0, 0, 1], positions[0, 0, 2],
            c='green', s=100, label='Start', edgecolors='k')
ax1.scatter(positions[-1, 0, 0], positions[-1, 0, 1], positions[-1, 0, 2],
            c='red', s=100, label='End', edgecolors='k')
ax1.set_xlabel('X (nm)')
ax1.set_ylabel('Y (nm)')
ax1.set_zlabel('Z (nm)')
ax1.set_title('Atom Trajectory (Atom 0)', fontweight='bold')
ax1.legend()

ax2 = fig.add_subplot(122)
ax2.plot(np.arange(n_steps) * dt, positions[:, 0, 0], label='X')
ax2.plot(np.arange(n_steps) * dt, positions[:, 0, 1], label='Y')
ax2.plot(np.arange(n_steps) * dt, positions[:, 0, 2], label='Z')
ax2.set_xlabel('Time (ps)', fontsize=11)
ax2.set_ylabel('Position (nm)', fontsize=11)
ax2.set_title('Position vs Time (Atom 0)', fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>ã€ä¾‹32ã€‘å‹•å¾„åˆ†å¸ƒé–¢æ•°ï¼ˆRDFï¼‰ã®è¨ˆç®—</h3>
<p>å‹•å¾„åˆ†å¸ƒé–¢æ•°ï¼ˆRadial Distribution Function, RDFï¼‰ã¯ã€åŸå­é–“è·é›¢ã®åˆ†å¸ƒã‚’è¡¨ã—ã¾ã™ã€‚</p>
<pre><code class="language-python"># å‹•å¾„åˆ†å¸ƒé–¢æ•°ï¼ˆRDFï¼‰ã®è¨ˆç®—
def calculate_rdf(positions, r_max=2.0, n_bins=100):
    &quot;&quot;&quot;
    å‹•å¾„åˆ†å¸ƒé–¢æ•°ã‚’è¨ˆç®—

    Parameters:
    -----------
    positions : ndarray
        åŸå­ä½ç½® (n_atoms, 3)
    r_max : float
        æœ€å¤§è·é›¢ï¼ˆnmï¼‰
    n_bins : int
        ãƒ“ãƒ³æ•°

    Returns:
    --------
    r_bins : ndarray
        è·é›¢ãƒ“ãƒ³
    rdf : ndarray
        å‹•å¾„åˆ†å¸ƒé–¢æ•°
    &quot;&quot;&quot;
    n_atoms = positions.shape[0]

    # å…¨åŸå­ãƒšã‚¢é–“ã®è·é›¢ã‚’è¨ˆç®—
    distances = []
    for i in range(n_atoms):
        for j in range(i+1, n_atoms):
            dist = np.linalg.norm(positions[i] - positions[j])
            if dist &lt; r_max:
                distances.append(dist)

    distances = np.array(distances)

    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    hist, bin_edges = np.histogram(distances, bins=n_bins, range=(0, r_max))
    r_bins = (bin_edges[:-1] + bin_edges[1:]) / 2

    # è¦æ ¼åŒ–ï¼ˆç†æƒ³æ°—ä½“ã¨ã®æ¯”ï¼‰
    dr = r_max / n_bins
    volume_shell = 4 * np.pi * r_bins**2 * dr
    n_ideal = volume_shell * (n_atoms / (4/3 * np.pi * r_max**3))

    rdf = hist / n_ideal / (n_atoms / 2)

    return r_bins, rdf

# æœ€çµ‚ãƒ•ãƒ¬ãƒ¼ãƒ ã§RDFã‚’è¨ˆç®—
final_positions = positions[-1]
r_bins, rdf = calculate_rdf(final_positions, r_max=1.5, n_bins=150)

print(&quot;=&quot; * 60)
print(&quot;å‹•å¾„åˆ†å¸ƒé–¢æ•°ï¼ˆRDFï¼‰&quot;)
print(&quot;=&quot; * 60)
print(f&quot;è¨ˆç®—å®Œäº†: {len(r_bins)}å€‹ã®ãƒ“ãƒ³&quot;)

# RDFã®ãƒ—ãƒ­ãƒƒãƒˆ
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(r_bins, rdf, 'b-', linewidth=2)
ax.axhline(y=1, color='r', linestyle='--', linewidth=1, label='Ideal Gas (g(r)=1)')
ax.set_xlabel('Distance r (nm)', fontsize=12)
ax.set_ylabel('g(r)', fontsize=12)
ax.set_title('Radial Distribution Function (RDF)', fontsize=13, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)
ax.set_ylim(0, max(rdf) * 1.1)

plt.tight_layout()
plt.show()

# ãƒ”ãƒ¼ã‚¯ä½ç½®ã®æ¤œå‡º
from scipy.signal import find_peaks

peaks, _ = find_peaks(rdf, height=1.2, distance=10)
print(f&quot;\nRDFã®ãƒ”ãƒ¼ã‚¯ä½ç½®ï¼ˆç‰¹å¾´çš„ãªåŸå­é–“è·é›¢ï¼‰:&quot;)
for i, peak_idx in enumerate(peaks[:3], 1):
    print(f&quot;  ãƒ”ãƒ¼ã‚¯{i}: r = {r_bins[peak_idx]:.3f} nm, g(r) = {rdf[peak_idx]:.2f}&quot;)
</code></pre>
<h3>ã€ä¾‹33ã€‘æ‹¡æ•£ä¿‚æ•°ã®è¨ˆç®—ï¼ˆMean Squared Displacementï¼‰</h3>
<pre><code class="language-python"># å¹³å‡äºŒä¹—å¤‰ä½ï¼ˆMSDï¼‰ã®è¨ˆç®—
def calculate_msd(positions):
    &quot;&quot;&quot;
    å¹³å‡äºŒä¹—å¤‰ä½ã‚’è¨ˆç®—

    Parameters:
    -----------
    positions : ndarray
        åŸå­ä½ç½® (n_steps, n_atoms, 3)

    Returns:
    --------
    msd : ndarray
        å¹³å‡äºŒä¹—å¤‰ä½ (n_steps,)
    &quot;&quot;&quot;
    n_steps, n_atoms, _ = positions.shape
    msd = np.zeros(n_steps)

    # å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§ã®MSD
    for t in range(n_steps):
        displacement = positions[t] - positions[0]
        squared_displacement = np.sum(displacement**2, axis=1)
        msd[t] = np.mean(squared_displacement)

    return msd

# MSDã®è¨ˆç®—
msd = calculate_msd(positions)
time = np.arange(n_steps) * dt

print(&quot;=&quot; * 60)
print(&quot;å¹³å‡äºŒä¹—å¤‰ä½ï¼ˆMSDï¼‰ã¨æ‹¡æ•£ä¿‚æ•°&quot;)
print(&quot;=&quot; * 60)

# æ‹¡æ•£ä¿‚æ•°ã®è¨ˆç®—ï¼ˆEinsteiné–¢ä¿‚å¼: MSD = 6*D*tï¼‰
# ç·šå½¢ãƒ•ã‚£ãƒƒãƒˆï¼ˆå¾ŒåŠ50%ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
start_idx = n_steps // 2
fit_coeffs = np.polyfit(time[start_idx:], msd[start_idx:], 1)
slope = fit_coeffs[0]
diffusion_coefficient = slope / 6

print(f&quot;æ‹¡æ•£ä¿‚æ•° D = {diffusion_coefficient:.6f} nmÂ²/ps&quot;)
print(f&quot;            = {diffusion_coefficient * 1e3:.6f} Ã— 10â»â¶ cmÂ²/s&quot;)

# MSDãƒ—ãƒ­ãƒƒãƒˆ
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(time, msd, 'b-', linewidth=2, label='MSD')
ax.plot(time[start_idx:], fit_coeffs[0] * time[start_idx:] + fit_coeffs[1],
        'r--', linewidth=2, label=f'Linear Fit (D={diffusion_coefficient:.4f} nmÂ²/ps)')
ax.set_xlabel('Time (ps)', fontsize=12)
ax.set_ylabel('MSD (nmÂ²)', fontsize=12)
ax.set_title('Mean Squared Displacement (MSD)', fontsize=13, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;\næ‹¡æ•£ä¿‚æ•°ã¯ã€ãƒŠãƒç²’å­ã®ç§»å‹•æ€§ã‚’å®šé‡çš„ã«è©•ä¾¡ã™ã‚‹é‡è¦ãªæŒ‡æ¨™ã§ã™&quot;)
</code></pre>
<hr />
<h2>3.11 ç•°å¸¸æ¤œçŸ¥ï¼šå“è³ªç®¡ç†ã¸ã®å¿œç”¨</h2>
<h3>ã€ä¾‹34ã€‘Isolation Forestã«ã‚ˆã‚‹ç•°å¸¸ãƒŠãƒç²’å­æ¤œå‡º</h3>
<p>è£½é€ ãƒ—ãƒ­ã‚»ã‚¹ã§ç”Ÿæˆã•ã‚ŒãŸãƒŠãƒç²’å­ã®å“è³ªç®¡ç†ã«ã€æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥ã‚’é©ç”¨ã—ã¾ã™ã€‚</p>
<pre><code class="language-python">from sklearn.ensemble import IsolationForest

# æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã¨ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã‚’æ··åœ¨ã•ã›ã‚‹
np.random.seed(400)

# æ­£å¸¸ãªé‡‘ãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ï¼ˆ180ã‚µãƒ³ãƒ—ãƒ«ï¼‰
normal_size = np.random.normal(15, 3, 180)
normal_lspr = 520 + 0.8 * (normal_size - 15) + np.random.normal(0, 3, 180)

# ç•°å¸¸ãªãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ï¼ˆ20ã‚µãƒ³ãƒ—ãƒ«ï¼‰ï¼šã‚µã‚¤ã‚ºãŒç•°å¸¸ã«å¤§ãã„orå°ã•ã„
anomaly_size = np.concatenate([
    np.random.uniform(5, 8, 10),  # ç•°å¸¸ã«å°ã•ã„
    np.random.uniform(35, 50, 10)  # ç•°å¸¸ã«å¤§ãã„
])
anomaly_lspr = 520 + 0.8 * (anomaly_size - 15) + np.random.normal(0, 8, 20)

# å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
all_size = np.concatenate([normal_size, anomaly_size])
all_lspr = np.concatenate([normal_lspr, anomaly_lspr])
all_data = np.column_stack([all_size, all_lspr])

# ãƒ©ãƒ™ãƒ«ï¼ˆæ­£å¸¸=0ã€ç•°å¸¸=1ï¼‰
true_labels = np.concatenate([np.zeros(180), np.ones(20)])

print(&quot;=&quot; * 60)
print(&quot;ç•°å¸¸æ¤œçŸ¥ï¼ˆIsolation Forestï¼‰&quot;)
print(&quot;=&quot; * 60)
print(f&quot;å…¨ãƒ‡ãƒ¼ã‚¿æ•°: {len(all_data)}&quot;)
print(f&quot;æ­£å¸¸ãƒ‡ãƒ¼ã‚¿: {int((true_labels == 0).sum())}ã‚µãƒ³ãƒ—ãƒ«&quot;)
print(f&quot;ç•°å¸¸ãƒ‡ãƒ¼ã‚¿: {int((true_labels == 1).sum())}ã‚µãƒ³ãƒ—ãƒ«&quot;)

# Isolation Forestãƒ¢ãƒ‡ãƒ«
iso_forest = IsolationForest(
    contamination=0.1,  # ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆï¼ˆ10%ã¨ä»®å®šï¼‰
    random_state=42,
    n_estimators=100
)

# ç•°å¸¸æ¤œçŸ¥
predictions = iso_forest.fit_predict(all_data)
anomaly_scores = iso_forest.score_samples(all_data)

# äºˆæ¸¬çµæœï¼ˆ1: æ­£å¸¸ã€-1: ç•°å¸¸ï¼‰
predicted_anomalies = (predictions == -1)
true_anomalies = (true_labels == 1)

# è©•ä¾¡æŒ‡æ¨™
from sklearn.metrics import confusion_matrix, classification_report

# äºˆæ¸¬ã‚’0/1ã«å¤‰æ›
predicted_labels = (predictions == -1).astype(int)

print(&quot;\næ··åŒè¡Œåˆ—:&quot;)
cm = confusion_matrix(true_labels, predicted_labels)
print(cm)

print(&quot;\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:&quot;)
print(classification_report(true_labels, predicted_labels,
                            target_names=['Normal', 'Anomaly']))

# æ¤œå‡ºç‡
detected_anomalies = np.sum(predicted_anomalies &amp; true_anomalies)
total_anomalies = np.sum(true_anomalies)
detection_rate = detected_anomalies / total_anomalies * 100

print(f&quot;\nç•°å¸¸æ¤œå‡ºç‡: {detection_rate:.1f}% ({detected_anomalies}/{total_anomalies})&quot;)
</code></pre>
<h3>ã€ä¾‹35ã€‘ç•°å¸¸ã‚µãƒ³ãƒ—ãƒ«ã®å¯è¦–åŒ–</h3>
<pre><code class="language-python"># ç•°å¸¸æ¤œçŸ¥çµæœã®å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# æ•£å¸ƒå›³ï¼ˆçœŸã®ãƒ©ãƒ™ãƒ«ï¼‰
axes[0].scatter(all_size[true_labels == 0], all_lspr[true_labels == 0],
                c='blue', s=60, alpha=0.6, label='Normal', edgecolors='k')
axes[0].scatter(all_size[true_labels == 1], all_lspr[true_labels == 1],
                c='red', s=100, alpha=0.8, marker='^', label='True Anomaly',
                edgecolors='k', linewidths=2)
axes[0].set_xlabel('Size (nm)', fontsize=12)
axes[0].set_ylabel('LSPR Wavelength (nm)', fontsize=12)
axes[0].set_title('True Labels', fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# æ•£å¸ƒå›³ï¼ˆäºˆæ¸¬çµæœï¼‰
normal_mask = ~predicted_anomalies
anomaly_mask = predicted_anomalies

axes[1].scatter(all_size[normal_mask], all_lspr[normal_mask],
                c='blue', s=60, alpha=0.6, label='Predicted Normal', edgecolors='k')
axes[1].scatter(all_size[anomaly_mask], all_lspr[anomaly_mask],
                c='orange', s=100, alpha=0.8, marker='X', label='Predicted Anomaly',
                edgecolors='k', linewidths=2)

# æ­£ã—ãæ¤œå‡ºã•ã‚ŒãŸç•°å¸¸ã‚’å¼·èª¿
correctly_detected = predicted_anomalies &amp; true_anomalies
axes[1].scatter(all_size[correctly_detected], all_lspr[correctly_detected],
                c='red', s=150, marker='*', label='Correctly Detected',
                edgecolors='black', linewidths=1.5, zorder=5)

axes[1].set_xlabel('Size (nm)', fontsize=12)
axes[1].set_ylabel('LSPR Wavelength (nm)', fontsize=12)
axes[1].set_title('Isolation Forest Predictions', fontsize=13, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ç•°å¸¸ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ
fig, ax = plt.subplots(figsize=(10, 6))
ax.hist(anomaly_scores[true_labels == 0], bins=30, alpha=0.6,
        color='blue', label='Normal', edgecolor='black')
ax.hist(anomaly_scores[true_labels == 1], bins=30, alpha=0.6,
        color='red', label='Anomaly', edgecolor='black')
ax.set_xlabel('Anomaly Score', fontsize=12)
ax.set_ylabel('Frequency', fontsize=12)
ax.set_title('Anomaly Score Distribution', fontsize=13, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print(&quot;\nç•°å¸¸ã‚¹ã‚³ã‚¢ãŒä½ã„ï¼ˆè² ã®å€¤ãŒå¤§ãã„ï¼‰ã»ã©ã€ç•°å¸¸ã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„&quot;)
</code></pre>
<hr />
<h2>ã¾ã¨ã‚</h2>
<p>æœ¬ç« ã§ã¯ã€Pythonã‚’ä½¿ã£ãŸãƒŠãƒææ–™ãƒ‡ãƒ¼ã‚¿è§£æã¨æ©Ÿæ¢°å­¦ç¿’ã®å®Ÿè·µçš„æ‰‹æ³•ã‚’35å€‹ã®ã‚³ãƒ¼ãƒ‰ä¾‹ã§å­¦ã³ã¾ã—ãŸã€‚</p>
<h3>ç¿’å¾—ã—ãŸä¸»è¦æŠ€è¡“</h3>
<ol>
<li>
<p><strong>ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã¨å¯è¦–åŒ–</strong>ï¼ˆä¾‹1-5ï¼‰
   - é‡‘ãƒŠãƒç²’å­ã€é‡å­ãƒ‰ãƒƒãƒˆã®åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
   - ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã€æ•£å¸ƒå›³ã€3Dãƒ—ãƒ­ãƒƒãƒˆã€ç›¸é–¢åˆ†æ</p>
</li>
<li>
<p><strong>ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†</strong>ï¼ˆä¾‹6-9ï¼‰
   - æ¬ æå€¤å‡¦ç†ã€å¤–ã‚Œå€¤æ¤œå‡ºã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€ãƒ‡ãƒ¼ã‚¿åˆ†å‰²</p>
</li>
<li>
<p><strong>å›å¸°ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ç‰©æ€§äºˆæ¸¬</strong>ï¼ˆä¾‹10-15ï¼‰
   - ç·šå½¢å›å¸°ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã€LightGBMã€SVRã€MLP
   - ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒï¼ˆRÂ²ã€RMSEã€MAEï¼‰</p>
</li>
<li>
<p><strong>é‡å­ãƒ‰ãƒƒãƒˆç™ºå…‰äºˆæ¸¬</strong>ï¼ˆä¾‹16-18ï¼‰
   - Brusæ–¹ç¨‹å¼ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
   - LightGBMã«ã‚ˆã‚‹äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰</p>
</li>
<li>
<p><strong>ç‰¹å¾´é‡é‡è¦åº¦ã¨ãƒ¢ãƒ‡ãƒ«è§£é‡ˆ</strong>ï¼ˆä¾‹19-20ï¼‰
   - LightGBMç‰¹å¾´é‡é‡è¦åº¦
   - SHAPåˆ†æã«ã‚ˆã‚‹äºˆæ¸¬ã®è§£é‡ˆ</p>
</li>
<li>
<p><strong>ãƒ™ã‚¤ã‚ºæœ€é©åŒ–</strong>ï¼ˆä¾‹21-25ï¼‰
   - ç›®æ¨™LSPRæ³¢é•·ã‚’å®Ÿç¾ã™ã‚‹æœ€é©åˆæˆæ¡ä»¶ã®æ¢ç´¢
   - åæŸãƒ—ãƒ­ãƒƒãƒˆã€æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã®å¯è¦–åŒ–</p>
</li>
<li>
<p><strong>å¤šç›®çš„æœ€é©åŒ–</strong>ï¼ˆä¾‹26-27ï¼‰
   - NSGA-IIã«ã‚ˆã‚‹ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©åŒ–
   - ã‚µã‚¤ã‚ºã¨ç™ºå…‰åŠ¹ç‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•åˆ†æ</p>
</li>
<li>
<p><strong>TEMç”»åƒè§£æ</strong>ï¼ˆä¾‹28-30ï¼‰
   - å¯¾æ•°æ­£è¦åˆ†å¸ƒã«ã‚ˆã‚‹ã‚µã‚¤ã‚ºåˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
   - Q-Qãƒ—ãƒ­ãƒƒãƒˆã«ã‚ˆã‚‹åˆ†å¸ƒã®æ¤œè¨¼</p>
</li>
<li>
<p><strong>åˆ†å­å‹•åŠ›å­¦ãƒ‡ãƒ¼ã‚¿è§£æ</strong>ï¼ˆä¾‹31-33ï¼‰
   - åŸå­è»Œè·¡ã®å¯è¦–åŒ–
   - å‹•å¾„åˆ†å¸ƒé–¢æ•°ï¼ˆRDFï¼‰ã®è¨ˆç®—
   - æ‹¡æ•£ä¿‚æ•°ã®ç®—å‡ºï¼ˆMSDæ³•ï¼‰</p>
</li>
<li>
<p><strong>ç•°å¸¸æ¤œçŸ¥</strong>ï¼ˆä¾‹34-35ï¼‰</p>
<ul>
<li>Isolation Forestã«ã‚ˆã‚‹å“è³ªç®¡ç†</li>
<li>ç•°å¸¸ãƒŠãƒç²’å­ã®è‡ªå‹•æ¤œå‡º</li>
</ul>
</li>
</ol>
<h3>å®Ÿè·µçš„ãªå¿œç”¨</h3>
<p>ã“ã‚Œã‚‰ã®æŠ€è¡“ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå®Ÿéš›ã®ãƒŠãƒææ–™ç ”ç©¶ã«ç›´æ¥å¿œç”¨ã§ãã¾ã™ï¼š</p>
<ul>
<li><strong>ææ–™è¨­è¨ˆ</strong>: æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ç‰©æ€§äºˆæ¸¬ã¨æœ€é©åŒ–ã«ã‚ˆã‚‹é«˜åŠ¹ç‡ææ–™æ¢ç´¢</li>
<li><strong>ãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ–</strong>: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚‹å®Ÿé¨“å›æ•°å‰Šæ¸›ã¨æœ€é©åˆæˆæ¡ä»¶ç™ºè¦‹</li>
<li><strong>å“è³ªç®¡ç†</strong>: ç•°å¸¸æ¤œçŸ¥ã«ã‚ˆã‚‹ä¸è‰¯å“ã®æ—©æœŸç™ºè¦‹ã¨æ­©ç•™ã¾ã‚Šå‘ä¸Š</li>
<li><strong>ãƒ‡ãƒ¼ã‚¿è§£æ</strong>: TEMãƒ‡ãƒ¼ã‚¿ã€MDã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®å®šé‡çš„è§£æ</li>
<li><strong>ãƒ¢ãƒ‡ãƒ«è§£é‡ˆ</strong>: SHAPåˆ†æã«ã‚ˆã‚‹äºˆæ¸¬æ ¹æ‹ ã®å¯è¦–åŒ–ã¨ä¿¡é ¼æ€§å‘ä¸Š</li>
</ul>
<h3>æ¬¡ç« ã®äºˆå‘Š</h3>
<p>Chapter 4ã§ã¯ã€ã“ã‚Œã‚‰ã®æŠ€è¡“ã‚’å®Ÿéš›ã®ãƒŠãƒææ–™ç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«é©ç”¨ã—ãŸ5ã¤ã®è©³ç´°ãªã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ã‚’å­¦ã³ã¾ã™ã€‚ã‚«ãƒ¼ãƒœãƒ³ãƒŠãƒãƒãƒ¥ãƒ¼ãƒ–è¤‡åˆææ–™ã€é‡å­ãƒ‰ãƒƒãƒˆã€é‡‘ãƒŠãƒç²’å­è§¦åª’ã€ã‚°ãƒ©ãƒ•ã‚§ãƒ³ã€ãƒŠãƒåŒ»è–¬ã®å®Ÿç”¨åŒ–äº‹ä¾‹ã‚’é€šã˜ã¦ã€å•é¡Œè§£æ±ºã®å…¨ä½“åƒã‚’ç†è§£ã—ã¾ã™ã€‚</p>
<hr />
<h2>æ¼”ç¿’å•é¡Œ</h2>
<h3>æ¼”ç¿’1: ã‚«ãƒ¼ãƒœãƒ³ãƒŠãƒãƒãƒ¥ãƒ¼ãƒ–ã®é›»æ°—ä¼å°åº¦äºˆæ¸¬</h3>
<p>ã‚«ãƒ¼ãƒœãƒ³ãƒŠãƒãƒãƒ¥ãƒ¼ãƒ–ï¼ˆCNTï¼‰ã®é›»æ°—ä¼å°åº¦ã¯ã€ç›´å¾„ã€ã‚«ã‚¤ãƒ©ãƒªãƒ†ã‚£ã€é•·ã•ã«ä¾å­˜ã—ã¾ã™ã€‚ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã€LightGBMãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>ãƒ‡ãƒ¼ã‚¿ä»•æ§˜</strong>ï¼š
- ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼š150
- ç‰¹å¾´é‡ï¼šç›´å¾„ï¼ˆ1-3 nmï¼‰ã€é•·ã•ï¼ˆ100-1000 nmï¼‰ã€ã‚«ã‚¤ãƒ©ãƒªãƒ†ã‚£æŒ‡æ¨™ï¼ˆ0-1ã®é€£ç¶šå€¤ï¼‰
- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼šé›»æ°—ä¼å°åº¦ï¼ˆ10Â³-10â· S/mã€å¯¾æ•°æ­£è¦åˆ†å¸ƒï¼‰</p>
<p><strong>ã‚¿ã‚¹ã‚¯</strong>ï¼š
1. ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
2. è¨“ç·´/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
3. LightGBMãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨è©•ä¾¡
4. ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–</p>
<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python"># ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(500)
n_samples = 150

diameter = np.random.uniform(1, 3, n_samples)
length = np.random.uniform(100, 1000, n_samples)
chirality = np.random.uniform(0, 1, n_samples)

# é›»æ°—ä¼å°åº¦ï¼ˆç°¡æ˜“ãƒ¢ãƒ‡ãƒ«: ç›´å¾„ã¨ã‚«ã‚¤ãƒ©ãƒªãƒ†ã‚£ã«å¼·ãä¾å­˜ï¼‰
log_conductivity = 3 + 2*diameter + 3*chirality + 0.001*length + np.random.normal(0, 0.5, n_samples)
conductivity = 10 ** log_conductivity  # S/m

data_cnt = pd.DataFrame({
    'diameter_nm': diameter,
    'length_nm': length,
    'chirality': chirality,
    'conductivity_Sm': conductivity
})

# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
X_cnt = data_cnt[['diameter_nm', 'length_nm', 'chirality']]
y_cnt = np.log10(data_cnt['conductivity_Sm'])  # å¯¾æ•°å¤‰æ›

# ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
scaler_cnt = StandardScaler()
X_cnt_scaled = scaler_cnt.fit_transform(X_cnt)

# è¨“ç·´/ãƒ†ã‚¹ãƒˆåˆ†å‰²
X_cnt_train, X_cnt_test, y_cnt_train, y_cnt_test = train_test_split(
    X_cnt_scaled, y_cnt, test_size=0.2, random_state=42
)

# LightGBMãƒ¢ãƒ‡ãƒ«
model_cnt = lgb.LGBMRegressor(num_leaves=31, learning_rate=0.05, n_estimators=200, random_state=42, verbose=-1)
model_cnt.fit(X_cnt_train, y_cnt_train)

# äºˆæ¸¬ã¨è©•ä¾¡
y_cnt_pred = model_cnt.predict(X_cnt_test)
r2_cnt = r2_score(y_cnt_test, y_cnt_pred)
rmse_cnt = np.sqrt(mean_squared_error(y_cnt_test, y_cnt_pred))

print(f&quot;RÂ²: {r2_cnt:.4f}&quot;)
print(f&quot;RMSE: {rmse_cnt:.4f}&quot;)

# ç‰¹å¾´é‡é‡è¦åº¦
importance_cnt = pd.DataFrame({
    'Feature': X_cnt.columns,
    'Importance': model_cnt.feature_importances_
}).sort_values('Importance', ascending=False)

print(&quot;\nç‰¹å¾´é‡é‡è¦åº¦:&quot;)
print(importance_cnt)

# å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(8, 5))
ax.barh(importance_cnt['Feature'], importance_cnt['Importance'], color='steelblue', edgecolor='black')
ax.set_xlabel('Importance')
ax.set_title('Feature Importance: CNT Conductivity Prediction')
plt.tight_layout()
plt.show()
</code></pre>


</details>

<h3>æ¼”ç¿’2: éŠ€ãƒŠãƒç²’å­ã®æœ€é©åˆæˆæ¡ä»¶æ¢ç´¢</h3>
<p>éŠ€ãƒŠãƒç²’å­ã®æŠ—èŒæ´»æ€§ã¯ã€ã‚µã‚¤ã‚ºãŒå°ã•ã„ã»ã©é«˜ããªã‚Šã¾ã™ã€‚ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’ç”¨ã„ã¦ã€ç›®æ¨™ã‚µã‚¤ã‚ºï¼ˆ10 nmï¼‰ã‚’å®Ÿç¾ã™ã‚‹æœ€é©ãªåˆæˆæ¸©åº¦ã¨pHã‚’æ¢ç´¢ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>æ¡ä»¶</strong>ï¼š
- æ¸©åº¦ç¯„å›²ï¼š20-80Â°C
- pHç¯„å›²ï¼š6-11
- ç›®æ¨™ã‚µã‚¤ã‚ºï¼š10 nm</p>
<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python"># éŠ€ãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
np.random.seed(600)
n_ag = 100

temp_ag = np.random.uniform(20, 80, n_ag)
pH_ag = np.random.uniform(6, 11, n_ag)

# ã‚µã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ï¼ˆæ¸©åº¦ãŒé«˜ãã€pHãŒä½ã„ã»ã©å°ã•ããªã‚‹ã¨ä»®å®šï¼‰
size_ag = 15 - 0.1*temp_ag - 0.8*pH_ag + np.random.normal(0, 1, n_ag)
size_ag = np.clip(size_ag, 5, 30)

data_ag = pd.DataFrame({
    'temperature': temp_ag,
    'pH': pH_ag,
    'size': size_ag
})

# ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ï¼ˆLightGBMï¼‰
X_ag = data_ag[['temperature', 'pH']]
y_ag = data_ag['size']

scaler_ag = StandardScaler()
X_ag_scaled = scaler_ag.fit_transform(X_ag)

model_ag = lgb.LGBMRegressor(num_leaves=31, learning_rate=0.05, n_estimators=100, random_state=42, verbose=-1)
model_ag.fit(X_ag_scaled, y_ag)

# ãƒ™ã‚¤ã‚ºæœ€é©åŒ–
from skopt import gp_minimize
from skopt.space import Real

space_ag = [
    Real(20, 80, name='temperature'),
    Real(6, 11, name='pH')
]

target_size = 10.0

def objective_ag(params):
    temp, ph = params
    features = scaler_ag.transform([[temp, ph]])
    predicted_size = model_ag.predict(features)[0]
    return abs(predicted_size - target_size)

result_ag = gp_minimize(objective_ag, space_ag, n_calls=40, random_state=42, verbose=False)

print(&quot;=&quot; * 60)
print(&quot;éŠ€ãƒŠãƒç²’å­ã®æœ€é©åˆæˆæ¡ä»¶&quot;)
print(&quot;=&quot; * 60)
print(f&quot;æœ€å°èª¤å·®: {result_ag.fun:.2f} nm&quot;)
print(f&quot;æœ€é©æ¸©åº¦: {result_ag.x[0]:.1f} Â°C&quot;)
print(f&quot;æœ€é©pH: {result_ag.x[1]:.2f}&quot;)

# æœ€é©æ¡ä»¶ã§ã®äºˆæ¸¬ã‚µã‚¤ã‚º
optimal_features = scaler_ag.transform([result_ag.x])
predicted_size = model_ag.predict(optimal_features)[0]
print(f&quot;äºˆæ¸¬ã‚µã‚¤ã‚º: {predicted_size:.2f} nm&quot;)
</code></pre>


</details>

<h3>æ¼”ç¿’3: é‡å­ãƒ‰ãƒƒãƒˆã®å¤šè‰²ç™ºå…‰è¨­è¨ˆ</h3>
<p>èµ¤ï¼ˆ650 nmï¼‰ã€ç·‘ï¼ˆ550 nmï¼‰ã€é’ï¼ˆ450 nmï¼‰ã®3è‰²ã®ç™ºå…‰ã‚’å®Ÿç¾ã™ã‚‹CdSeé‡å­ãƒ‰ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã‚’ã€ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>ãƒ’ãƒ³ãƒˆ</strong>ï¼š
- å„è‰²ã”ã¨ã«æœ€é©åŒ–ã‚’å®Ÿè¡Œ
- ç™ºå…‰æ³¢é•·ã¨ã‚µã‚¤ã‚ºã®é–¢ä¿‚ã‚’ä½¿ç”¨</p>
<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python"># é‡å­ãƒ‰ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆä¾‹16ã®data_qdã‚’ä½¿ç”¨ï¼‰
# model_qd ã¨ scaler_qd ãŒæ§‹ç¯‰æ¸ˆã¿ã¨ä»®å®š

# 3è‰²ã®ç›®æ¨™æ³¢é•·
target_colors = {
    'Red': 650,
    'Green': 550,
    'Blue': 450
}

results_colors = {}

for color_name, target_emission in target_colors.items():
    # æ¢ç´¢ç©ºé–“
    space_qd = [
        Real(2, 10, name='size_nm'),
        Real(10, 120, name='synthesis_time_min'),
        Real(0.5, 2.0, name='precursor_ratio')
    ]

    # ç›®çš„é–¢æ•°
    def objective_qd(params):
        features = scaler_qd.transform([params])
        predicted_emission = model_qd.predict(features)[0]
        return abs(predicted_emission - target_emission)

    # æœ€é©åŒ–
    result_qd_color = gp_minimize(objective_qd, space_qd, n_calls=30, random_state=42, verbose=False)

    # çµæœä¿å­˜
    optimal_features = scaler_qd.transform([result_qd_color.x])
    predicted_emission = model_qd.predict(optimal_features)[0]

    results_colors[color_name] = {
        'target': target_emission,
        'size': result_qd_color.x[0],
        'time': result_qd_color.x[1],
        'ratio': result_qd_color.x[2],
        'predicted': predicted_emission,
        'error': result_qd_color.fun
    }

# çµæœè¡¨ç¤º
print(&quot;=&quot; * 80)
print(&quot;é‡å­ãƒ‰ãƒƒãƒˆå¤šè‰²ç™ºå…‰è¨­è¨ˆ&quot;)
print(&quot;=&quot; * 80)

results_df = pd.DataFrame(results_colors).T
print(results_df.to_string())

# å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(10, 6))
colors_rgb = {'Red': 'red', 'Green': 'green', 'Blue': 'blue'}

for color_name, result in results_colors.items():
    ax.scatter(result['size'], result['predicted'],
               s=200, color=colors_rgb[color_name],
               edgecolors='black', linewidths=2, label=color_name)

ax.set_xlabel('Quantum Dot Size (nm)', fontsize=12)
ax.set_ylabel('Emission Wavelength (nm)', fontsize=12)
ax.set_title('Multi-Color Quantum Dot Design', fontsize=13, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>


</details>

<hr />
<h2>3.12 ç« æœ«ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼šãƒŠãƒææ–™ãƒ‡ãƒ¼ã‚¿è§£æã‚¹ã‚­ãƒ«ã®å“è³ªä¿è¨¼</h2>
<p>æœ¬ç« ã§å­¦ã‚“ã Pythonã«ã‚ˆã‚‹ãƒŠãƒææ–™ãƒ‡ãƒ¼ã‚¿è§£æã¨æ©Ÿæ¢°å­¦ç¿’ã®å®Ÿè£…ã‚¹ã‚­ãƒ«ã‚’ä½“ç³»çš„ã«ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚</p>
<h3>3.12.1 ç’°å¢ƒæ§‹ç¯‰ã‚¹ã‚­ãƒ«ï¼ˆEnvironment Setupï¼‰</h3>
<h4>åŸºç¤ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] Python 3.9ä»¥ä¸ŠãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹</li>
<li>[ ] 3ã¤ã®ç’°å¢ƒæ§‹ç¯‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆAnaconda/venv/Colabï¼‰ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>[ ] è‡ªåˆ†ã®çŠ¶æ³ã«æœ€é©ãªç’°å¢ƒã‚’é¸æŠã§ãã‚‹</li>
<li>[ ] ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆãƒ»æœ‰åŠ¹åŒ–ãƒ»ç„¡åŠ¹åŒ–ã§ãã‚‹</li>
<li>[ ] pip/condaã§ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã‚‹ï¼ˆpandasã€numpyã€matplotlibã€scikit-learnã€lightgbmï¼‰</li>
<li>[ ] ç’°å¢ƒæ¤œè¨¼ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã€ã‚¨ãƒ©ãƒ¼ãªãå‹•ä½œç¢ºèªã§ãã‚‹</li>
</ul>
<h4>å¿œç”¨ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] requirements.txtã‚’ä½œæˆãƒ»ä½¿ç”¨ã§ãã‚‹</li>
<li>[ ] Google Colabã§Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚ã‚‹</li>
<li>[ ] è¤‡æ•°ã®ä»®æƒ³ç’°å¢ƒã‚’ç”¨é€”åˆ¥ã«ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹</li>
<li>[ ] ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼ã‚’è‡ªåŠ›ã§ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã§ãã‚‹</li>
<li>[ ] ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆpymooã€SHAPï¼‰ã‚’å¿…è¦ã«å¿œã˜ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã‚‹</li>
</ul>
<hr />
<h3>3.12.2 ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»å¯è¦–åŒ–ã‚¹ã‚­ãƒ«ï¼ˆData Processing &amp; Visualizationï¼‰</h3>
<h4>åŸºç¤ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] NumPyã§åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã§ãã‚‹ï¼ˆæ­£è¦åˆ†å¸ƒã€ä¸€æ§˜åˆ†å¸ƒï¼‰</li>
<li>[ ] Pandasã§ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆãƒ»æ“ä½œã§ãã‚‹</li>
<li>[ ] åŸºæœ¬çµ±è¨ˆé‡ï¼ˆmeanã€stdã€medianï¼‰ã‚’è¨ˆç®—ã§ãã‚‹</li>
<li>[ ] ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’ä½œæˆã§ãã‚‹</li>
<li>[ ] æ•£å¸ƒå›³ã‚’ä½œæˆã§ãã‚‹</li>
<li>[ ] æ¬ æå€¤ã‚’æ¤œå‡ºã§ãã‚‹ï¼ˆ<code>isnull().sum()</code>ï¼‰</li>
<li>[ ] æ¬ æå€¤ã‚’å‰Šé™¤ã¾ãŸã¯è£œå®Œã§ãã‚‹ï¼ˆ<code>dropna()</code> or <code>fillna()</code>ï¼‰</li>
</ul>
<h4>å¿œç”¨ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] ç›¸é–¢è¡Œåˆ—ã‚’è¨ˆç®—ãƒ»å¯è¦–åŒ–ã§ãã‚‹ï¼ˆ<code>corr()</code>ã€seaborn.heatmapï¼‰</li>
<li>[ ] ãƒšã‚¢ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæ•£å¸ƒå›³ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼‰ã‚’ä½œæˆã§ãã‚‹ï¼ˆseaborn.pairplotï¼‰</li>
<li>[ ] 3Dæ•£å¸ƒå›³ã‚’ä½œæˆã§ãã‚‹ï¼ˆmpl_toolkits.mplot3dï¼‰</li>
<li>[ ] KDEï¼ˆã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®šï¼‰ã‚’ä½¿ç”¨ã§ãã‚‹</li>
<li>[ ] å¤–ã‚Œå€¤ã‚’IQRæ³•ã§æ¤œå‡ºã§ãã‚‹</li>
<li>[ ] StandardScalerã§ãƒ‡ãƒ¼ã‚¿ã‚’æ¨™æº–åŒ–ã§ãã‚‹</li>
<li>[ ] train_test_splitã§ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã§ãã‚‹ï¼ˆ80% vs 20%ï¼‰</li>
<li>[ ] <code>random_state=42</code>ã§å†ç¾æ€§ã‚’ç¢ºä¿ã—ã¦ã„ã‚‹</li>
</ul>
<h4>ä¸Šç´šãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] å¯¾æ•°æ­£è¦åˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ãŒã§ãã‚‹ï¼ˆscipy.stats.lognormï¼‰</li>
<li>[ ] Q-Qãƒ—ãƒ­ãƒƒãƒˆã§åˆ†å¸ƒã®é©åˆåº¦ã‚’æ¤œè¨¼ã§ãã‚‹</li>
<li>[ ] ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ã‚’åŠ¹æœçš„ã«ä½¿ç”¨ã§ãã‚‹ï¼ˆviridisã€plasmaã€coolwarmï¼‰</li>
<li>[ ] è¤‡æ•°ã®subplotsã‚’ä½¿ã£ãŸé«˜åº¦ãªå¯è¦–åŒ–ãŒã§ãã‚‹</li>
</ul>
<hr />
<h3>3.12.3 æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«å®Ÿè£…ã‚¹ã‚­ãƒ«ï¼ˆML Model Implementationï¼‰</h3>
<h4>åŸºç¤ãƒ¬ãƒ™ãƒ«ï¼ˆ5ã¤ã®ãƒ¢ãƒ‡ãƒ«å®Ÿè£…ï¼‰</h4>
<ul>
<li>[ ] ç·šå½¢å›å¸°ã‚’å®Ÿè£…ã—ã€ä¿‚æ•°ã®æ„å‘³ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>[ ] ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚’å®Ÿè£…ã—ã€<code>n_estimators</code>ã®å½¹å‰²ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>[ ] LightGBMã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] SVRã§æ¨™æº–åŒ–ï¼ˆStandardScalerï¼‰ã®å¿…è¦æ€§ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] MLPRegressorï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰ã‚’å®Ÿè£…ã§ãã‚‹</li>
</ul>
<h4>å¿œç”¨ãƒ¬ãƒ™ãƒ«ï¼ˆãƒ¢ãƒ‡ãƒ«é¸æŠã¨è©•ä¾¡ï¼‰</h4>
<ul>
<li>[ ] MAEã€RÂ²ã€RMSEã‚’è¨ˆç®—ãƒ»è§£é‡ˆã§ãã‚‹</li>
<li>[ ] è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ€§èƒ½å·®ã‚’è©•ä¾¡ã§ãã‚‹</li>
<li>[ ] éå­¦ç¿’ã‚’æ¤œå‡ºã§ãã‚‹ï¼ˆè¨“ç·´RÂ² â‰« ãƒ†ã‚¹ãƒˆRÂ²ï¼‰</li>
<li>[ ] 5ã¤ã®ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æ¯”è¼ƒè¡¨ã§æ•´ç†ã§ãã‚‹</li>
<li>[ ] äºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤ã®æ•£å¸ƒå›³ã‚’ä½œæˆã§ãã‚‹</li>
<li>[ ] æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆã—ã€ãƒ¢ãƒ‡ãƒ«ã®åã‚Šã‚’æ¤œå‡ºã§ãã‚‹</li>
</ul>
<h4>ä¸Šç´šãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«å¿œã˜ã¦æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã§ãã‚‹</li>
<li>ç·šå½¢æ€§ãŒå¼·ã„ â†’ ç·šå½¢å›å¸°</li>
<li>éç·šå½¢æ€§ãŒå¼·ã„ â†’ ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã€LightGBM</li>
<li>ãƒ‡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã„ â†’ SVR</li>
<li>[ ] å„ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½¹å‰²ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼šn_estimatorsã€max_depth</li>
<li>LightGBMï¼šlearning_rateã€num_leaves</li>
<li>SVRï¼šCã€gammaã€epsilon</li>
<li>MLPï¼šhidden_layer_sizesã€alphaã€early_stopping</li>
</ul>
<hr />
<h3>3.12.4 ç‰¹å¾´é‡é‡è¦åº¦ã¨ãƒ¢ãƒ‡ãƒ«è§£é‡ˆã‚¹ã‚­ãƒ«ï¼ˆFeature Importance &amp; Interpretabilityï¼‰</h3>
<h4>åŸºç¤ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—ãƒ»å¯è¦–åŒ–ã§ãã‚‹ï¼ˆ<code>feature_importances_</code>ï¼‰</li>
<li>[ ] LightGBMã®ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—ãƒ»å¯è¦–åŒ–ã§ãã‚‹</li>
<li>[ ] ç‰¹å¾´é‡é‡è¦åº¦ã®çµæœã‚’è§£é‡ˆã§ãã‚‹ï¼ˆæœ€ã‚‚å½±éŸ¿ã™ã‚‹ç‰¹å¾´é‡ã¯ä½•ã‹ï¼‰</li>
</ul>
<h4>å¿œç”¨ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] SHAPãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»ä½¿ç”¨ã§ãã‚‹</li>
<li>[ ] SHAP Explainerã‚’ä½œæˆã§ãã‚‹ï¼ˆ<code>shap.Explainer</code>ï¼‰</li>
<li>[ ] SHAP Summary Plotã‚’ä½œæˆãƒ»è§£é‡ˆã§ãã‚‹</li>
<li>[ ] SHAP Dependence Plotã‚’ä½œæˆãƒ»è§£é‡ˆã§ãã‚‹</li>
<li>[ ] SHAPå€¤ã®æ­£è² ãŒäºˆæ¸¬ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’èª¬æ˜ã§ãã‚‹</li>
</ul>
<h4>ä¸Šç´šãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] è¤‡æ•°ã®è§£é‡ˆæ‰‹æ³•ã‚’ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹</li>
<li>ç‰¹å¾´é‡é‡è¦åº¦ï¼šå…¨ä½“çš„ãªé‡è¦åº¦</li>
<li>SHAPï¼šå€‹åˆ¥ã‚µãƒ³ãƒ—ãƒ«ã®äºˆæ¸¬ç†ç”±</li>
<li>[ ] ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬æ ¹æ‹ ã‚’ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã«èª¬æ˜ã§ãã‚‹</li>
</ul>
<hr />
<h3>3.12.5 ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚¹ã‚­ãƒ«ï¼ˆBayesian Optimizationï¼‰</h3>
<h4>åŸºç¤ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] scikit-optimizeã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã‚‹</li>
<li>[ ] æ¢ç´¢ç©ºé–“ã‚’å®šç¾©ã§ãã‚‹ï¼ˆ<code>Real(min, max, name)</code>ï¼‰</li>
<li>[ ] ç›®çš„é–¢æ•°ã‚’å®šç¾©ã§ãã‚‹ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚Šã€èª¤å·®ã‚’è¿”ã™ï¼‰</li>
<li>[ ] <code>gp_minimize</code>ã‚’å®Ÿè¡Œã§ãã‚‹</li>
<li>[ ] æœ€é©åŒ–çµæœï¼ˆresult.xã€result.funï¼‰ã‚’å–å¾—ã§ãã‚‹</li>
</ul>
<h4>å¿œç”¨ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] n_callsã¨n_initial_pointsã®å½¹å‰²ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>n_callsï¼šè©•ä¾¡å›æ•°</li>
<li>n_initial_pointsï¼šãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å›æ•°</li>
<li>[ ] åæŸãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆã§ãã‚‹ï¼ˆ<code>plot_convergence</code>ï¼‰</li>
<li>[ ] æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã®å¯è¦–åŒ–ãŒã§ãã‚‹ï¼ˆè©•ä¾¡å±¥æ­´ã€æœ€è‰¯å€¤ã®æ¨ç§»ï¼‰</li>
<li>[ ] ç›®æ¨™å€¤ã¸ã®é”æˆç²¾åº¦ã‚’è©•ä¾¡ã§ãã‚‹</li>
</ul>
<h4>ä¸Šç´šãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] è¤‡æ•°ã®ç›®æ¨™ï¼ˆèµ¤ãƒ»ç·‘ãƒ»é’ã®é‡å­ãƒ‰ãƒƒãƒˆï¼‰ã«å¯¾ã—ã¦æœ€é©åŒ–ã‚’å®Ÿè¡Œã§ãã‚‹</li>
<li>[ ] æœ€é©åŒ–çµæœã‚’å®Ÿé¨“æ¤œè¨¼è¨ˆç”»ã«æ´»ç”¨ã§ãã‚‹</li>
<li>[ ] ç²å¾—é–¢æ•°ï¼ˆAcquisition Functionï¼‰ã®æ¦‚å¿µã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
</ul>
<hr />
<h3>3.12.6 å¤šç›®çš„æœ€é©åŒ–ã‚¹ã‚­ãƒ«ï¼ˆMulti-Objective Optimizationï¼‰</h3>
<h4>åŸºç¤ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] pymooãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã‚‹</li>
<li>[ ] å¤šç›®çš„æœ€é©åŒ–å•é¡Œã®æ¦‚å¿µã‚’ç†è§£ã—ã¦ã„ã‚‹ï¼ˆã‚µã‚¤ã‚ºæœ€å°åŒ– vs åŠ¹ç‡æœ€å¤§åŒ–ï¼‰</li>
<li>[ ] ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆã®æ¦‚å¿µã‚’èª¬æ˜ã§ãã‚‹</li>
</ul>
<h4>å¿œç”¨ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] pymoo.core.problemã‚’ç¶™æ‰¿ã—ã¦Problemã‚¯ãƒ©ã‚¹ã‚’å®šç¾©ã§ãã‚‹</li>
<li>[ ] NSGA-IIã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆã‚’å¯è¦–åŒ–ã§ãã‚‹</li>
<li>[ ] ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•é–¢ä¿‚ã‚’è§£é‡ˆã§ãã‚‹</li>
</ul>
<h4>ä¸Šç´šãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] è¤‡æ•°ã®è§£ã‹ã‚‰ç”¨é€”ã«å¿œã˜ãŸæœ€é©è§£ã‚’é¸æŠã§ãã‚‹</li>
<li>é«˜æ€§èƒ½é‡è¦–</li>
<li>ç’°å¢ƒé‡è¦–</li>
<li>ãƒãƒ©ãƒ³ã‚¹å‹</li>
<li>[ ] ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã«ã‚ˆã‚‹ä»£æ›¿å®Ÿè£…ãŒã§ãã‚‹ï¼ˆpymooãŒä½¿ãˆãªã„å ´åˆï¼‰</li>
</ul>
<hr />
<h3>3.12.7 ãƒŠãƒææ–™ç‰¹æœ‰ã®è§£æã‚¹ã‚­ãƒ«ï¼ˆNanomaterial-Specific Analysisï¼‰</h3>
<h4>TEMç”»åƒè§£æ</h4>
<ul>
<li>[ ] å¯¾æ•°æ­£è¦åˆ†å¸ƒã«å¾“ã†ã‚µã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã§ãã‚‹</li>
<li>[ ] å¯¾æ•°æ­£è¦åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆsigmaã€muï¼‰ã‚’è¨ˆç®—ã§ãã‚‹</li>
<li>[ ] <code>lognorm.fit</code>ã§ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã§ãã‚‹</li>
<li>[ ] ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœã‚’å¯è¦–åŒ–ã§ãã‚‹ï¼ˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  + PDFæ›²ç·šï¼‰</li>
<li>[ ] Q-Qãƒ—ãƒ­ãƒƒãƒˆã§åˆ†å¸ƒã®é©åˆåº¦ã‚’è©•ä¾¡ã§ãã‚‹</li>
</ul>
<h4>åˆ†å­å‹•åŠ›å­¦ï¼ˆMDï¼‰ãƒ‡ãƒ¼ã‚¿è§£æ</h4>
<ul>
<li>[ ] åŸå­è»Œè·¡ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã‚’ç†è§£ã—ã¦ã„ã‚‹ï¼ˆn_steps Ã— n_atoms Ã— 3ï¼‰</li>
<li>[ ] 3Dè»Œè·¡ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆã§ãã‚‹</li>
<li>[ ] å‹•å¾„åˆ†å¸ƒé–¢æ•°ï¼ˆRDFï¼‰ã‚’è¨ˆç®—ã§ãã‚‹</li>
<li>[ ] RDFã®ãƒ”ãƒ¼ã‚¯ä½ç½®ã‹ã‚‰ç‰¹å¾´çš„ãªåŸå­é–“è·é›¢ã‚’æŠ½å‡ºã§ãã‚‹</li>
<li>[ ] å¹³å‡äºŒä¹—å¤‰ä½ï¼ˆMSDï¼‰ã‚’è¨ˆç®—ã§ãã‚‹</li>
<li>[ ] MSDã‹ã‚‰æ‹¡æ•£ä¿‚æ•°ã‚’ç®—å‡ºã§ãã‚‹ï¼ˆEinsteiné–¢ä¿‚å¼ï¼‰</li>
</ul>
<h4>ç•°å¸¸æ¤œçŸ¥</h4>
<ul>
<li>[ ] Isolation Forestã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] contaminationï¼ˆç•°å¸¸ãƒ‡ãƒ¼ã‚¿å‰²åˆï¼‰ã‚’è¨­å®šã§ãã‚‹</li>
<li>[ ] ç•°å¸¸ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã§ãã‚‹ï¼ˆ<code>score_samples</code>ï¼‰</li>
<li>[ ] æ··åŒè¡Œåˆ—ã§ç•°å¸¸æ¤œå‡ºç²¾åº¦ã‚’è©•ä¾¡ã§ãã‚‹</li>
<li>[ ] æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã¨ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã‚’å¯è¦–åŒ–ã§ãã‚‹</li>
</ul>
<hr />
<h3>3.12.8 ã‚³ãƒ¼ãƒ‰å“è³ªã‚¹ã‚­ãƒ«ï¼ˆCode Qualityï¼‰</h3>
<h4>åŸºç¤ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] ã™ã¹ã¦ã®ã‚³ãƒ¼ãƒ‰ã«ä¹±æ•°ã‚·ãƒ¼ãƒ‰ï¼ˆ<code>random_state=42</code>ï¼‰ã‚’è¨­å®šã—ã¦ã„ã‚‹</li>
<li>[ ] ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ï¼ˆshapeã€dtypeã€æ¬ æå€¤ã€ç¯„å›²ï¼‰ã‚’å®Ÿæ–½ã—ã¦ã„ã‚‹</li>
<li>[ ] å¤‰æ•°åãŒåˆ†ã‹ã‚Šã‚„ã™ã„ï¼ˆ<code>X_train</code>ã€<code>y_test</code>ã€<code>model_lgb</code>ï¼‰</li>
<li>[ ] ã‚³ãƒ¡ãƒ³ãƒˆã§å‡¦ç†ã®ç›®çš„ã‚’èª¬æ˜ã—ã¦ã„ã‚‹</li>
<li>[ ] ã‚°ãƒ©ãƒ•ã«ã‚¿ã‚¤ãƒˆãƒ«ã€è»¸ãƒ©ãƒ™ãƒ«ã€å‡¡ä¾‹ã‚’è¿½åŠ ã—ã¦ã„ã‚‹</li>
</ul>
<h4>å¿œç”¨ãƒ¬ãƒ™ãƒ«</h4>
<ul>
<li>[ ] é–¢æ•°åŒ–ã—ã¦ã‚³ãƒ¼ãƒ‰ã‚’å†åˆ©ç”¨å¯èƒ½ã«ã—ã¦ã„ã‚‹
  <code>python
  def calculate_rdf(positions, r_max, n_bins):
      ...</code></li>
<li>[ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ–‡å­—åˆ—ï¼ˆDocstringï¼‰ã‚’è¨˜è¿°ã—ã¦ã„ã‚‹</li>
<li>[ ] ã‚°ãƒ©ãƒ•ã®ç¾è¦³ã‚’æ•´ãˆã¦ã„ã‚‹ï¼ˆfontsizeã€gridã€alphaï¼‰</li>
<li>[ ] try-exceptã§ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ã—ã¦ã„ã‚‹ï¼ˆpymooã®ImportErrorå¯¾å¿œï¼‰</li>
</ul>
<hr />
<h3>3.12.9 ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¹ã‚­ãƒ«ï¼ˆTroubleshootingï¼‰</h3>
<h4>åŸºç¤ãƒ¬ãƒ™ãƒ«ï¼ˆã‚¨ãƒ©ãƒ¼å¯¾å‡¦ï¼‰</h4>
<ul>
<li>[ ] <code>ModuleNotFoundError</code>ã‚’è§£æ±ºã§ãã‚‹ï¼ˆ<code>pip install</code>ï¼‰</li>
<li>[ ] <code>ValueError: Input contains NaN</code>ã‚’è§£æ±ºã§ãã‚‹ï¼ˆæ¬ æå€¤å‡¦ç†ï¼‰</li>
<li>[ ] <code>ConvergenceWarning</code>ï¼ˆMLPã®åæŸã‚¨ãƒ©ãƒ¼ï¼‰ã‚’è§£æ±ºã§ãã‚‹</li>
<li><code>max_iter</code>ã‚’å¢—ã‚„ã™</li>
<li>ãƒ‡ãƒ¼ã‚¿ã‚’æ¨™æº–åŒ–ã™ã‚‹</li>
<li>Early Stoppingã‚’æœ‰åŠ¹åŒ–</li>
<li>[ ] ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’èª­ã¿ã€æ¤œç´¢ã—ã¦è§£æ±ºç­–ã‚’è¦‹ã¤ã‘ã‚‰ã‚Œã‚‹</li>
</ul>
<h4>å¿œç”¨ãƒ¬ãƒ™ãƒ«ï¼ˆæ€§èƒ½æ”¹å–„ï¼‰</h4>
<ul>
<li>[ ] RÂ² &lt; 0.7ã®å ´åˆã€3ã¤ä»¥ä¸Šã®æ”¹å–„ç­–ã‚’å®Ÿè¡Œã§ãã‚‹</li>
<li>ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</li>
<li>ãƒ¢ãƒ‡ãƒ«å¤‰æ›´ï¼ˆç·šå½¢â†’éç·šå½¢ï¼‰</li>
<li>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´</li>
<li>[ ] éå­¦ç¿’ã‚’æ¤œå‡ºã§ãã‚‹ï¼ˆè¨“ç·´RÂ² â‰« ãƒ†ã‚¹ãƒˆRÂ²ï¼‰</li>
<li>[ ] æœªå­¦ç¿’ã‚’æ¤œå‡ºã§ãã‚‹ï¼ˆè¨“ç·´RÂ²ã‚‚ãƒ†ã‚¹ãƒˆRÂ²ã‚‚ä½ã„ï¼‰</li>
</ul>
<hr />
<h3>3.12.10 ç·åˆè©•ä¾¡ï¼šç¿’ç†Ÿåº¦ãƒ¬ãƒ™ãƒ«åˆ¤å®š</h3>
<p>ä»¥ä¸‹ã®ãƒ¬ãƒ™ãƒ«åˆ¤å®šã§ã€è‡ªåˆ†ã®åˆ°é”åº¦ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚</p>
<h4>ãƒ¬ãƒ™ãƒ«1ï¼šåˆå¿ƒè€…ï¼ˆBeginnerï¼‰</h4>
<ul>
<li>ç’°å¢ƒæ§‹ç¯‰ã‚¹ã‚­ãƒ«ï¼šåŸºç¤ãƒ¬ãƒ™ãƒ« 100%é”æˆ</li>
<li>ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»å¯è¦–åŒ–ã‚¹ã‚­ãƒ«ï¼šåŸºç¤ãƒ¬ãƒ™ãƒ« 80%ä»¥ä¸Šé”æˆ</li>
<li>æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«å®Ÿè£…ã‚¹ã‚­ãƒ«ï¼šåŸºç¤ãƒ¬ãƒ™ãƒ« 5ã¤ä¸­3ã¤ä»¥ä¸Šå®Ÿè£…</li>
<li>ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼šåŸºç¤ãƒ¬ãƒ™ãƒ«ã®ã‚¨ãƒ©ãƒ¼ã‚’è‡ªåŠ›ã§è§£æ±º</li>
</ul>
<p><strong>åˆ°é”ç›®æ¨™:</strong> ãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆãƒ»å¯è¦–åŒ–ã—ã€ç·šå½¢å›å¸°ã¨ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã§LSPRæ³¢é•·äºˆæ¸¬ã‚’å®Ÿè£…ã§ãã‚‹</p>
<hr />
<h4>ãƒ¬ãƒ™ãƒ«2ï¼šä¸­ç´šè€…ï¼ˆIntermediateï¼‰</h4>
<ul>
<li>ç’°å¢ƒæ§‹ç¯‰ã‚¹ã‚­ãƒ«ï¼šå¿œç”¨ãƒ¬ãƒ™ãƒ« 80%ä»¥ä¸Šé”æˆ</li>
<li>ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»å¯è¦–åŒ–ã‚¹ã‚­ãƒ«ï¼šåŸºç¤ãƒ¬ãƒ™ãƒ« 100%é”æˆ + å¿œç”¨ãƒ¬ãƒ™ãƒ« 70%ä»¥ä¸Š</li>
<li>æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«å®Ÿè£…ã‚¹ã‚­ãƒ«ï¼šåŸºç¤ãƒ¬ãƒ™ãƒ« 100%é”æˆ + å¿œç”¨ãƒ¬ãƒ™ãƒ« 70%ä»¥ä¸Š</li>
<li>ç‰¹å¾´é‡é‡è¦åº¦ã¨ãƒ¢ãƒ‡ãƒ«è§£é‡ˆã‚¹ã‚­ãƒ«ï¼šåŸºç¤ãƒ¬ãƒ™ãƒ« 100%é”æˆ + å¿œç”¨ãƒ¬ãƒ™ãƒ« 50%ä»¥ä¸Š</li>
<li>ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚¹ã‚­ãƒ«ï¼šåŸºç¤ãƒ¬ãƒ™ãƒ« 100%é”æˆ + å¿œç”¨ãƒ¬ãƒ™ãƒ« 50%ä»¥ä¸Š</li>
</ul>
<p><strong>åˆ°é”ç›®æ¨™:</strong> 5ã¤ã®å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã—ã€ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§ç›®æ¨™LSPRæ³¢é•·ï¼ˆ550 nmï¼‰ã‚’é”æˆã™ã‚‹åˆæˆæ¡ä»¶ã‚’ç™ºè¦‹ã§ãã‚‹</p>
<hr />
<h4>ãƒ¬ãƒ™ãƒ«3ï¼šä¸Šç´šè€…ï¼ˆAdvancedï¼‰</h4>
<ul>
<li>å…¨ã‚«ãƒ†ã‚´ãƒªï¼šå¿œç”¨ãƒ¬ãƒ™ãƒ« 100%é”æˆ</li>
<li>ç‰¹å¾´é‡é‡è¦åº¦ã¨ãƒ¢ãƒ‡ãƒ«è§£é‡ˆã‚¹ã‚­ãƒ«ï¼šä¸Šç´šãƒ¬ãƒ™ãƒ« 80%ä»¥ä¸Š</li>
<li>ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚¹ã‚­ãƒ«ï¼šä¸Šç´šãƒ¬ãƒ™ãƒ« 80%ä»¥ä¸Š</li>
<li>å¤šç›®çš„æœ€é©åŒ–ã‚¹ã‚­ãƒ«ï¼šå¿œç”¨ãƒ¬ãƒ™ãƒ« 100%é”æˆ</li>
<li>ãƒŠãƒææ–™ç‰¹æœ‰ã®è§£æã‚¹ã‚­ãƒ«ï¼šTEMã€MDã€ç•°å¸¸æ¤œçŸ¥ã™ã¹ã¦å®Ÿè£…</li>
</ul>
<p><strong>åˆ°é”ç›®æ¨™:</strong> SHAPåˆ†æã§ãƒ¢ãƒ‡ãƒ«è§£é‡ˆã—ã€å¤šç›®çš„æœ€é©åŒ–ã§ã‚µã‚¤ã‚ºã¨åŠ¹ç‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å¯è¦–åŒ–ã§ãã‚‹</p>
<hr />
<h4>ãƒ¬ãƒ™ãƒ«4ï¼šã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆï¼ˆExpertï¼‰</h4>
<ul>
<li>å…¨ã‚«ãƒ†ã‚´ãƒªï¼šä¸Šç´šãƒ¬ãƒ™ãƒ« 80%ä»¥ä¸Šé”æˆ</li>
<li>ã‚³ãƒ¼ãƒ‰å“è³ªï¼šå¿œç”¨ãƒ¬ãƒ™ãƒ« 100%é”æˆ</li>
<li>ç‹¬è‡ªã®ãƒŠãƒææ–™ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯æ–‡çŒ®ãƒ‡ãƒ¼ã‚¿ï¼‰ã«é©ç”¨ã§ãã‚‹</li>
<li>ã‚«ã‚¹ã‚¿ãƒ æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
<li>ç ”ç©¶æˆæœã‚’å­¦ä¼šç™ºè¡¨ãƒ»è«–æ–‡æŠ•ç¨¿ã§ãã‚‹</li>
</ul>
<p><strong>åˆ°é”ç›®æ¨™:</strong>
- å®ŸãƒŠãƒææ–™ãƒ‡ãƒ¼ã‚¿ï¼ˆTEMã€UV-Visã€XRDï¼‰ã‚’çµ±åˆè§£æ
- æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚Šæ–°è¦ãƒŠãƒç²’å­ã®ç‰©æ€§ã‚’90%ä»¥ä¸Šã®ç²¾åº¦ã§äºˆæ¸¬
- ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§å®Ÿé¨“å›æ•°ã‚’å¾“æ¥ã®1/5ã«å‰Šæ¸›</p>
<hr />
<h3>3.12.11 å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ï¼šæ¼”ç¿’å•é¡Œã®å®Œé‚</h3>
<h4>æ¼”ç¿’1å®Œé‚ç¢ºèªï¼ˆCNTé›»æ°—ä¼å°åº¦äºˆæ¸¬ï¼‰</h4>
<ul>
<li>[ ] ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆ150ã‚µãƒ³ãƒ—ãƒ«ã€3ç‰¹å¾´é‡ï¼‰ã‚’å®Ÿè£…</li>
<li>[ ] LightGBMãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ã‚’å®Ÿè£…</li>
<li>[ ] RÂ² &gt; 0.8ã€RMSE &lt; 0.5ã‚’é”æˆ</li>
<li>[ ] ç‰¹å¾´é‡é‡è¦åº¦ã‚’å¯è¦–åŒ–</li>
<li>[ ] çµæœã‚’è§£é‡ˆï¼ˆã©ã®ç‰¹å¾´é‡ãŒæœ€ã‚‚å½±éŸ¿ã™ã‚‹ã‹ï¼‰</li>
</ul>
<h4>æ¼”ç¿’2å®Œé‚ç¢ºèªï¼ˆéŠ€ãƒŠãƒç²’å­æœ€é©åˆæˆæ¡ä»¶ï¼‰</h4>
<ul>
<li>[ ] éŠ€ãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ï¼ˆ100ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã‚’ç”Ÿæˆ</li>
<li>[ ] LightGBMãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰</li>
<li>[ ] ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’å®Ÿè¡Œï¼ˆ40å›è©•ä¾¡ï¼‰</li>
<li>[ ] ç›®æ¨™ã‚µã‚¤ã‚ºï¼ˆ10 nmï¼‰ã¨ã®èª¤å·® &lt; 1 nmã‚’é”æˆ</li>
<li>[ ] æœ€é©æ¸©åº¦ãƒ»pHã‚’ç‰¹å®š</li>
</ul>
<h4>æ¼”ç¿’3å®Œé‚ç¢ºèªï¼ˆé‡å­ãƒ‰ãƒƒãƒˆå¤šè‰²ç™ºå…‰è¨­è¨ˆï¼‰</h4>
<ul>
<li>[ ] èµ¤ãƒ»ç·‘ãƒ»é’ã®3è‰²ã«ã¤ã„ã¦æœ€é©åŒ–ã‚’å®Ÿè¡Œ</li>
<li>[ ] å„è‰²ã®æœ€é©ã‚µã‚¤ã‚ºãƒ»åˆæˆæ¡ä»¶ã‚’ç‰¹å®š</li>
<li>[ ] äºˆæ¸¬æ³¢é•·ãŒç›®æ¨™æ³¢é•·Â±10 nmä»¥å†…ã«åã¾ã£ã¦ã„ã‚‹</li>
<li>[ ] çµæœã‚’å¯è¦–åŒ–ï¼ˆã‚µã‚¤ã‚º vs æ³¢é•·ã®ãƒ—ãƒ­ãƒƒãƒˆï¼‰</li>
</ul>
<hr />
<h3>3.12.12 æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸ã®æº–å‚™åº¦ãƒã‚§ãƒƒã‚¯</h3>
<h4>å®Ÿä¸–ç•Œå¿œç”¨ï¼ˆChapter 4ï¼‰ã¸ã®æº–å‚™</h4>
<ul>
<li>[ ] æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆãƒ‡ãƒ¼ã‚¿æº–å‚™â†’ãƒ¢ãƒ‡ãƒ«è¨“ç·´â†’è©•ä¾¡â†’æœ€é©åŒ–ï¼‰ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] ãƒŠãƒææ–™ç‰¹æœ‰ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚µã‚¤ã‚ºåˆ†å¸ƒã€å…‰å­¦ç‰¹æ€§ã€é›»æ°—ç‰¹æ€§ï¼‰ã‚’æ‰±ãˆã‚‹</li>
<li>[ ] æœ€é©åŒ–æ‰‹æ³•ï¼ˆãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã€å¤šç›®çš„æœ€é©åŒ–ï¼‰ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] ãƒ¢ãƒ‡ãƒ«è§£é‡ˆï¼ˆSHAPï¼‰ã®é‡è¦æ€§ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
</ul>
<h4>æ·±å±¤å­¦ç¿’ãƒ»ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¸ã®æº–å‚™</h4>
<ul>
<li>[ ] ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆMLPï¼‰ã‚’å®Ÿè£…ã—ã€æ´»æ€§åŒ–é–¢æ•°ãƒ»æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] å­¦ç¿’æ›²ç·šã‚’å¯è¦–åŒ–ã—ã€éå­¦ç¿’ã‚’æ¤œå‡ºã§ãã‚‹</li>
<li>[ ] Early Stoppingã®æ¦‚å¿µã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
</ul>
<h4>å®Ÿå‹™ç ”ç©¶ã¸ã®æº–å‚™</h4>
<ul>
<li>[ ] Jupyter Notebookã¾ãŸã¯Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã‚³ãƒ¼ãƒ‰ã‚’ç®¡ç†ã§ãã‚‹</li>
<li>[ ] requirements.txtã§ç’°å¢ƒã‚’å†ç¾å¯èƒ½ã«ã—ã¦ã„ã‚‹</li>
<li>[ ] äºˆæ¸¬çµæœã‚’ã‚°ãƒ©ãƒ•åŒ–ã—ã€ãƒ¬ãƒãƒ¼ãƒˆã«ã¾ã¨ã‚ã‚‰ã‚Œã‚‹</li>
<li>[ ] ã‚³ãƒ¼ãƒ‰ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¨˜è¿°ã—ã¦ã„ã‚‹</li>
</ul>
<hr />
<p><strong>ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆæ´»ç”¨ã®ãƒ’ãƒ³ãƒˆ:</strong>
1. <strong>å®šæœŸçš„ã«è¦‹ç›´ã™</strong>: å­¦ç¿’å¾Œã€1é€±é–“å¾Œã€1ãƒ¶æœˆå¾Œã«å†ãƒã‚§ãƒƒã‚¯
2. <strong>æœªé”æˆé …ç›®ã‚’å„ªå…ˆ</strong>: ãƒã‚§ãƒƒã‚¯ã§ããªã„é …ç›®ã‚’é›†ä¸­å­¦ç¿’
3. <strong>ãƒ¬ãƒ™ãƒ«åˆ¤å®šã‚’è¨˜éŒ²</strong>: æˆé•·ã‚’å¯è¦–åŒ–ã—ã¦ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ç¶­æŒ
4. <strong>å®Ÿãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã®æ´»ç”¨</strong>: ç ”ç©¶ãƒ»é–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–‹å§‹å‰ã«å¿…é ˆã‚¹ã‚­ãƒ«ã‚’ç¢ºèª</p>
<hr />
<h2>å‚è€ƒæ–‡çŒ®</h2>
<ol>
<li>
<p><strong>Pedregosa, F. et al.</strong> (2011). Scikit-learn: Machine Learning in Python. <em>Journal of Machine Learning Research</em>, 12, 2825-2830.</p>
</li>
<li>
<p><strong>Ke, G. et al.</strong> (2017). LightGBM: A highly efficient gradient boosting decision tree. <em>Advances in Neural Information Processing Systems</em>, 30, 3146-3154.</p>
</li>
<li>
<p><strong>Lundberg, S. M. &amp; Lee, S.-I.</strong> (2017). A unified approach to interpreting model predictions. <em>Advances in Neural Information Processing Systems</em>, 30, 4765-4774.</p>
</li>
<li>
<p><strong>Snoek, J., Larochelle, H., &amp; Adams, R. P.</strong> (2012). Practical Bayesian optimization of machine learning algorithms. <em>Advances in Neural Information Processing Systems</em>, 25, 2951-2959.</p>
</li>
<li>
<p><strong>Deb, K. et al.</strong> (2002). A fast and elitist multiobjective genetic algorithm: NSGA-II. <em>IEEE Transactions on Evolutionary Computation</em>, 6(2), 182-197. <a href="https://doi.org/10.1109/4235.996017">DOI: 10.1109/4235.996017</a></p>
</li>
<li>
<p><strong>Frenkel, D. &amp; Smit, B.</strong> (2001). <em>Understanding Molecular Simulation: From Algorithms to Applications</em> (2nd ed.). Academic Press.</p>
</li>
</ol>
<hr />
<p><a href="chapter2-fundamentals.html">â† å‰ç« ï¼šãƒŠãƒææ–™ã®åŸºç¤åŸç†</a> | <a href="chapter4-real-world.html">æ¬¡ç« ï¼šå®Ÿä¸–ç•Œã®å¿œç”¨ã¨ã‚­ãƒ£ãƒªã‚¢ â†’</a></p><div class="navigation">
    <a href="chapter2-fundamentals.html" class="nav-button">â† å‰ã®ç« </a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter4-real-world.html" class="nav-button">æ¬¡ã®ç«  â†’</a>
</div>
    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
