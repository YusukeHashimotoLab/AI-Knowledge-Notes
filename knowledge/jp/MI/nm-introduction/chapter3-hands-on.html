<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 3: PythonÂÆüË∑µ„ÉÅ„É•„Éº„Éà„É™„Ç¢„É´ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Chapter 3: PythonÂÆüË∑µ„ÉÅ„É•„Éº„Éà„É™„Ç¢„É´</h1>
            <p class="subtitle">„Éä„ÉéÊùêÊñô„Éá„Éº„ÇøËß£Êûê„Å®Ê©üÊ¢∞Â≠¶Áøí</p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 30-40ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ÂàùÁ¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 0ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 0Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 3: PythonÂÆüË∑µ„ÉÅ„É•„Éº„Éà„É™„Ç¢„É´</h1>
<p>„Éä„ÉéÊùêÊñô„Éá„Éº„ÇøËß£Êûê„Å®Ê©üÊ¢∞Â≠¶Áøí</p>
<hr />
<h2>Êú¨Á´†„ÅÆÂ≠¶ÁøíÁõÆÊ®ô</h2>
<p>Êú¨Á´†„ÇíÂ≠¶Áøí„Åô„Çã„Åì„Å®„Åß„ÄÅ‰ª•‰∏ã„ÅÆ„Çπ„Ç≠„É´„ÇíÁøíÂæó„Åß„Åç„Åæ„ÅôÔºö</p>
<p>‚úÖ „Éä„ÉéÁ≤íÂ≠ê„Éá„Éº„Çø„ÅÆÁîüÊàê„ÉªÂèØË¶ñÂåñ„ÉªÂâçÂá¶ÁêÜ„ÅÆÂÆüË∑µ
‚úÖ 5Á®ÆÈ°û„ÅÆÂõûÂ∏∞„É¢„Éá„É´„Å´„Çà„Çã„Éä„ÉéÊùêÊñôÁâ©ÊÄß‰∫àÊ∏¨
‚úÖ „Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„Å´„Çà„Çã„Éä„ÉéÊùêÊñô„ÅÆÊúÄÈÅ©Ë®≠Ë®à
‚úÖ SHAPÂàÜÊûê„Å´„Çà„ÇãÊ©üÊ¢∞Â≠¶Áøí„É¢„Éá„É´„ÅÆËß£Èáà
‚úÖ Â§öÁõÆÁöÑÊúÄÈÅ©Âåñ„Å´„Çà„Çã„Éà„É¨„Éº„Éâ„Ç™„ÉïÂàÜÊûê
‚úÖ TEMÁîªÂÉèËß£Êûê„Å®„Çµ„Ç§„Ç∫ÂàÜÂ∏É„ÅÆ„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞
‚úÖ Áï∞Â∏∏Ê§úÁü•„Å´„Çà„ÇãÂìÅË≥™ÁÆ°ÁêÜ„Å∏„ÅÆÂøúÁî®</p>
<hr />
<h2>3.1 Áí∞Â¢ÉÊßãÁØâ</h2>
<h3>ÂøÖË¶Å„Å™„É©„Ç§„Éñ„É©„É™</h3>
<p>Êú¨„ÉÅ„É•„Éº„Éà„É™„Ç¢„É´„Åß‰ΩøÁî®„Åô„Çã‰∏ªË¶Å„Å™Python„É©„Ç§„Éñ„É©„É™Ôºö</p>
<pre><code class="language-python"># „Éá„Éº„ÇøÂá¶ÁêÜ„ÉªÂèØË¶ñÂåñ
pandas, numpy, matplotlib, seaborn, scipy

# Ê©üÊ¢∞Â≠¶Áøí
scikit-learn, lightgbm

# ÊúÄÈÅ©Âåñ
scikit-optimize

# „É¢„Éá„É´Ëß£Èáà
shap

# Â§öÁõÆÁöÑÊúÄÈÅ©ÂåñÔºà„Ç™„Éó„Ç∑„Éß„É≥Ôºâ
pymoo
</code></pre>
<h3>„Ç§„É≥„Çπ„Éà„Éº„É´ÊñπÊ≥ï</h3>
<h4>Option 1: AnacondaÁí∞Â¢É</h4>
<pre><code class="language-bash"># Anaconda„ÅßÊñ∞„Åó„ÅÑÁí∞Â¢É„Çí‰ΩúÊàê
conda create -n nanomaterials python=3.10 -y
conda activate nanomaterials

# ÂøÖË¶Å„Å™„É©„Ç§„Éñ„É©„É™„Çí„Ç§„É≥„Çπ„Éà„Éº„É´
conda install pandas numpy matplotlib seaborn scipy scikit-learn -y
conda install -c conda-forge lightgbm scikit-optimize shap -y

# Â§öÁõÆÁöÑÊúÄÈÅ©ÂåñÁî®Ôºà„Ç™„Éó„Ç∑„Éß„É≥Ôºâ
pip install pymoo
</code></pre>
<h4>Option 2: venv + pipÁí∞Â¢É</h4>
<pre><code class="language-bash"># ‰ªÆÊÉ≥Áí∞Â¢É„Çí‰ΩúÊàê
python -m venv nanomaterials_env

# ‰ªÆÊÉ≥Áí∞Â¢É„ÇíÊúâÂäπÂåñ
# macOS/Linux:
source nanomaterials_env/bin/activate
# Windows:
nanomaterials_env\Scripts\activate

# ÂøÖË¶Å„Å™„É©„Ç§„Éñ„É©„É™„Çí„Ç§„É≥„Çπ„Éà„Éº„É´
pip install pandas numpy matplotlib seaborn scipy
pip install scikit-learn lightgbm scikit-optimize shap pymoo
</code></pre>
<h4>Option 3: Google Colab</h4>
<p>Google Colab„Çí‰ΩøÁî®„Åô„ÇãÂ†¥Âêà„ÄÅ‰ª•‰∏ã„ÅÆ„Ç≥„Éº„Éâ„Çí„Çª„É´„ÅßÂÆüË°åÔºö</p>
<pre><code class="language-python"># ËøΩÂä†„Éë„ÉÉ„Ç±„Éº„Ç∏„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´
!pip install lightgbm scikit-optimize shap pymoo

# „Ç§„É≥„Éù„Éº„Éà„ÅÆÁ¢∫Ë™ç
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
print(&quot;Áí∞Â¢ÉÊßãÁØâÂÆå‰∫ÜÔºÅ&quot;)
</code></pre>
<hr />
<h2>3.2 „Éä„ÉéÁ≤íÂ≠ê„Éá„Éº„Çø„ÅÆÊ∫ñÂÇô„Å®ÂèØË¶ñÂåñ</h2>
<h3>„Äê‰æã1„ÄëÂêàÊàê„Éá„Éº„ÇøÁîüÊàêÔºöÈáë„Éä„ÉéÁ≤íÂ≠ê„ÅÆ„Çµ„Ç§„Ç∫„Å®ÂÖâÂ≠¶ÁâπÊÄß</h3>
<p>Èáë„Éä„ÉéÁ≤íÂ≠ê„ÅÆÂ±ÄÂú®Ë°®Èù¢„Éó„É©„Ç∫„É¢„É≥ÂÖ±È≥¥ÔºàLSPRÔºâÊ≥¢Èï∑„ÅØ„ÄÅÁ≤íÂ≠ê„Çµ„Ç§„Ç∫„Å´‰æùÂ≠ò„Åó„Åæ„Åô„ÄÇ„Åì„ÅÆÈñ¢‰øÇ„ÇíÊ®°Êì¨„Éá„Éº„Çø„ÅßË°®Áèæ„Åó„Åæ„Åô„ÄÇ</p>
<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Êó•Êú¨Ë™û„Éï„Ç©„É≥„ÉàË®≠ÂÆöÔºàÂøÖË¶Å„Å´Âøú„Åò„Å¶Ôºâ
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ‰π±Êï∞„Ç∑„Éº„Éâ„ÅÆË®≠ÂÆöÔºàÂÜçÁèæÊÄß„ÅÆ„Åü„ÇÅÔºâ
np.random.seed(42)

# „Çµ„É≥„Éó„É´Êï∞
n_samples = 200

# Èáë„Éä„ÉéÁ≤íÂ≠ê„ÅÆ„Çµ„Ç§„Ç∫ÔºànmÔºâ: Âπ≥Âùá15 nm„ÄÅÊ®ôÊ∫ñÂÅèÂ∑Æ5 nm
size = np.random.normal(15, 5, n_samples)
size = np.clip(size, 5, 50)  # 5-50 nm„ÅÆÁØÑÂõ≤„Å´Âà∂Èôê

# LSPRÊ≥¢Èï∑ÔºànmÔºâ: MieÁêÜË´ñ„ÅÆÁ∞°ÊòìËøë‰ºº
# Âü∫Êú¨Ê≥¢Èï∑520 nm + „Çµ„Ç§„Ç∫‰æùÂ≠òÈ†Ö + „Éé„Ç§„Ç∫
lspr = 520 + 0.8 * (size - 15) + np.random.normal(0, 5, n_samples)

# ÂêàÊàêÊù°‰ª∂
temperature = np.random.uniform(20, 80, n_samples)  # Ê∏©Â∫¶Ôºà‚ÑÉÔºâ
pH = np.random.uniform(4, 10, n_samples)  # pH

# „Éá„Éº„Çø„Éï„É¨„Éº„É†„ÅÆ‰ΩúÊàê
data = pd.DataFrame({
    'size_nm': size,
    'lspr_nm': lspr,
    'temperature_C': temperature,
    'pH': pH
})

print(&quot;=&quot; * 60)
print(&quot;Èáë„Éä„ÉéÁ≤íÂ≠ê„Éá„Éº„Çø„ÅÆÁîüÊàêÂÆå‰∫Ü&quot;)
print(&quot;=&quot; * 60)
print(data.head(10))
print(&quot;\nÂü∫Êú¨Áµ±Ë®àÈáè:&quot;)
print(data.describe())
</code></pre>
<h3>„Äê‰æã2„Äë„Çµ„Ç§„Ç∫ÂàÜÂ∏É„ÅÆ„Éí„Çπ„Éà„Ç∞„É©„É†</h3>
<pre><code class="language-python"># „Çµ„Ç§„Ç∫ÂàÜÂ∏É„ÅÆ„Éí„Çπ„Éà„Ç∞„É©„É†
fig, ax = plt.subplots(figsize=(10, 6))

# „Éí„Çπ„Éà„Ç∞„É©„É†„Å®KDEÔºà„Ç´„Éº„Éç„É´ÂØÜÂ∫¶Êé®ÂÆöÔºâ
ax.hist(data['size_nm'], bins=30, alpha=0.6, color='skyblue',
        edgecolor='black', density=True, label='Histogram')

# KDE„Éó„É≠„ÉÉ„Éà
from scipy.stats import gaussian_kde
kde = gaussian_kde(data['size_nm'])
x_range = np.linspace(data['size_nm'].min(), data['size_nm'].max(), 100)
ax.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')

ax.set_xlabel('Particle Size (nm)', fontsize=12)
ax.set_ylabel('Probability Density', fontsize=12)
ax.set_title('Gold Nanoparticle Size Distribution', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f&quot;Âπ≥Âùá„Çµ„Ç§„Ç∫: {data['size_nm'].mean():.2f} nm&quot;)
print(f&quot;Ê®ôÊ∫ñÂÅèÂ∑Æ: {data['size_nm'].std():.2f} nm&quot;)
print(f&quot;‰∏≠Â§ÆÂÄ§: {data['size_nm'].median():.2f} nm&quot;)
</code></pre>
<h3>„Äê‰æã3„ÄëÊï£Â∏ÉÂõ≥„Éû„Éà„É™„ÉÉ„ÇØ„Çπ</h3>
<pre><code class="language-python"># „Éö„Ç¢„Éó„É≠„ÉÉ„ÉàÔºàÊï£Â∏ÉÂõ≥„Éû„Éà„É™„ÉÉ„ÇØ„ÇπÔºâ
sns.pairplot(data, diag_kind='kde', plot_kws={'alpha': 0.6},
             height=2.5, corner=False)
plt.suptitle('Pairplot of Gold Nanoparticle Data', y=1.01, fontsize=14, fontweight='bold')
plt.show()

print(&quot;ÂêÑÂ§âÊï∞Èñì„ÅÆÈñ¢‰øÇ„ÇíÂèØË¶ñÂåñ„Åó„Åæ„Åó„Åü&quot;)
</code></pre>
<h3>„Äê‰æã4„ÄëÁõ∏Èñ¢Ë°åÂàó„ÅÆ„Éí„Éº„Éà„Éû„ÉÉ„Éó</h3>
<pre><code class="language-python"># Áõ∏Èñ¢Ë°åÂàó„ÅÆË®àÁÆó
correlation_matrix = data.corr()

# „Éí„Éº„Éà„Éû„ÉÉ„Éó
fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm',
            center=0, square=True, linewidths=1, cbar_kws={&quot;shrink&quot;: 0.8})
ax.set_title('Correlation Matrix', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

print(&quot;Áõ∏Èñ¢‰øÇÊï∞:&quot;)
print(correlation_matrix)
print(f&quot;\nLSPRÊ≥¢Èï∑„Å®„Çµ„Ç§„Ç∫„ÅÆÁõ∏Èñ¢: {correlation_matrix.loc['lspr_nm', 'size_nm']:.3f}&quot;)
</code></pre>
<h3>„Äê‰æã5„Äë3D„Éó„É≠„ÉÉ„ÉàÔºö„Çµ„Ç§„Ç∫ vs Ê∏©Â∫¶ vs LSPR</h3>
<pre><code class="language-python">from mpl_toolkits.mplot3d import Axes3D

# 3DÊï£Â∏ÉÂõ≥
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

# „Ç´„É©„Éº„Éû„ÉÉ„Éó
scatter = ax.scatter(data['size_nm'], data['temperature_C'], data['lspr_nm'],
                     c=data['pH'], cmap='viridis', s=50, alpha=0.6, edgecolors='k')

ax.set_xlabel('Size (nm)', fontsize=11)
ax.set_ylabel('Temperature (¬∞C)', fontsize=11)
ax.set_zlabel('LSPR Wavelength (nm)', fontsize=11)
ax.set_title('3D Scatter: Size vs Temperature vs LSPR (colored by pH)',
             fontsize=13, fontweight='bold')

# „Ç´„É©„Éº„Éê„Éº
cbar = plt.colorbar(scatter, ax=ax, shrink=0.5, aspect=5)
cbar.set_label('pH', fontsize=10)

plt.tight_layout()
plt.show()

print(&quot;3D„Éó„É≠„ÉÉ„Éà„ÅßÂ§öÊ¨°ÂÖÉ„ÅÆÈñ¢‰øÇ„ÇíÂèØË¶ñÂåñ„Åó„Åæ„Åó„Åü&quot;)
</code></pre>
<hr />
<h2>3.3 ÂâçÂá¶ÁêÜ„Å®„Éá„Éº„ÇøÂàÜÂâ≤</h2>
<h3>„Äê‰æã6„ÄëÊ¨†ÊêçÂÄ§Âá¶ÁêÜ</h3>
<pre><code class="language-python"># Ê¨†ÊêçÂÄ§„Çí‰∫∫ÁÇ∫ÁöÑ„Å´Â∞éÂÖ•ÔºàÂÆüÁøíÁî®Ôºâ
data_with_missing = data.copy()
np.random.seed(123)

# „É©„É≥„ÉÄ„É†„Å´5%„ÅÆÊ¨†ÊêçÂÄ§„ÇíÂ∞éÂÖ•
missing_indices = np.random.choice(data.index, size=int(0.05 * len(data)), replace=False)
data_with_missing.loc[missing_indices, 'temperature_C'] = np.nan

print(&quot;=&quot; * 60)
print(&quot;Ê¨†ÊêçÂÄ§„ÅÆÁ¢∫Ë™ç&quot;)
print(&quot;=&quot; * 60)
print(f&quot;Ê¨†ÊêçÂÄ§„ÅÆÊï∞:\n{data_with_missing.isnull().sum()}&quot;)

# Ê¨†ÊêçÂÄ§„ÅÆÂá¶ÁêÜÊñπÊ≥ï1: Âπ≥ÂùáÂÄ§„ÅßË£úÂÆå
data_filled_mean = data_with_missing.fillna(data_with_missing.mean())

# Ê¨†ÊêçÂÄ§„ÅÆÂá¶ÁêÜÊñπÊ≥ï2: ‰∏≠Â§ÆÂÄ§„ÅßË£úÂÆå
data_filled_median = data_with_missing.fillna(data_with_missing.median())

# Ê¨†ÊêçÂÄ§„ÅÆÂá¶ÁêÜÊñπÊ≥ï3: ÂâäÈô§
data_dropped = data_with_missing.dropna()

print(f&quot;\nÂÖÉ„ÅÆ„Éá„Éº„Çø: {len(data_with_missing)}Ë°å&quot;)
print(f&quot;Ê¨†ÊêçÂÄ§ÂâäÈô§Âæå: {len(data_dropped)}Ë°å&quot;)
print(f&quot;Âπ≥ÂùáÂÄ§Ë£úÂÆåÂæå: {len(data_filled_mean)}Ë°åÔºàÊ¨†ÊêçÂÄ§„Å™„ÅóÔºâ&quot;)

# ‰ª•Èôç„ÅÆÂàÜÊûê„Åß„ÅØÂÖÉ„ÅÆ„Éá„Éº„ÇøÔºàÊ¨†ÊêçÂÄ§„Å™„ÅóÔºâ„Çí‰ΩøÁî®
data_clean = data.copy()
print(&quot;\n‚Üí ‰ª•Èôç„ÅØÊ¨†ÊêçÂÄ§„ÅÆ„Å™„ÅÑ„Éá„Éº„Çø„Çí‰ΩøÁî®„Åó„Åæ„Åô&quot;)
</code></pre>
<h3>„Äê‰æã7„ÄëÂ§ñ„ÇåÂÄ§Ê§úÂá∫ÔºàIQRÊ≥ïÔºâ</h3>
<pre><code class="language-python"># IQRÔºàÂõõÂàÜ‰ΩçÁØÑÂõ≤ÔºâÊ≥ï„Å´„Çà„ÇãÂ§ñ„ÇåÂÄ§Ê§úÂá∫
def detect_outliers_iqr(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = (series &lt; lower_bound) | (series &gt; upper_bound)
    return outliers, lower_bound, upper_bound

# „Çµ„Ç§„Ç∫„Å´„Å§„ÅÑ„Å¶Â§ñ„ÇåÂÄ§Ê§úÂá∫
outliers, lower, upper = detect_outliers_iqr(data_clean['size_nm'])

print(&quot;=&quot; * 60)
print(&quot;Â§ñ„ÇåÂÄ§Ê§úÂá∫ÔºàIQRÊ≥ïÔºâ&quot;)
print(&quot;=&quot; * 60)
print(f&quot;Ê§úÂá∫„Åï„Çå„ÅüÂ§ñ„ÇåÂÄ§„ÅÆÊï∞: {outliers.sum()}&quot;)
print(f&quot;‰∏ãÈôê: {lower:.2f} nm, ‰∏äÈôê: {upper:.2f} nm&quot;)

# ÂèØË¶ñÂåñ
fig, ax = plt.subplots(figsize=(10, 6))
ax.boxplot([data_clean['size_nm']], labels=['Size (nm)'], vert=False)
ax.scatter(data_clean.loc[outliers, 'size_nm'],
           [1] * outliers.sum(), color='red', s=100,
           label=f'Outliers (n={outliers.sum()})', zorder=3)
ax.set_xlabel('Size (nm)', fontsize=12)
ax.set_title('Boxplot with Outliers', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()

print(&quot;‚Üí Â§ñ„ÇåÂÄ§„ÅØÈô§Âéª„Åõ„Åö„ÄÅÂÖ®„Éá„Éº„Çø„Çí‰ΩøÁî®„Åó„Åæ„Åô&quot;)
</code></pre>
<h3>„Äê‰æã8„ÄëÁâπÂæ¥Èáè„Çπ„Ç±„Éº„É™„É≥„Ç∞ÔºàStandardScalerÔºâ</h3>
<pre><code class="language-python">from sklearn.preprocessing import StandardScaler

# ÁâπÂæ¥Èáè„Å®„Çø„Éº„Ç≤„ÉÉ„Éà„ÅÆÂàÜÈõ¢
X = data_clean[['size_nm', 'temperature_C', 'pH']]
y = data_clean['lspr_nm']

# StandardScalerÔºàÂπ≥Âùá0„ÄÅÊ®ôÊ∫ñÂÅèÂ∑Æ1„Å´Ê®ôÊ∫ñÂåñÔºâ
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# „Çπ„Ç±„Éº„É™„É≥„Ç∞ÂâçÂæå„ÅÆÊØîËºÉ
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)

print(&quot;=&quot; * 60)
print(&quot;„Çπ„Ç±„Éº„É™„É≥„Ç∞Ââç„ÅÆÁµ±Ë®àÈáè&quot;)
print(&quot;=&quot; * 60)
print(X.describe())

print(&quot;\n&quot; + &quot;=&quot; * 60)
print(&quot;„Çπ„Ç±„Éº„É™„É≥„Ç∞Âæå„ÅÆÁµ±Ë®àÈáèÔºàÂπ≥Âùá‚âà0„ÄÅÊ®ôÊ∫ñÂÅèÂ∑Æ‚âà1Ôºâ&quot;)
print(&quot;=&quot; * 60)
print(X_scaled_df.describe())

print(&quot;\n‚Üí „Çπ„Ç±„Éº„É™„É≥„Ç∞„Å´„Çà„ÇäÂêÑÁâπÂæ¥Èáè„ÅÆ„Çπ„Ç±„Éº„É´„ÅåÁµ±‰∏Ä„Åï„Çå„Åæ„Åó„Åü&quot;)
</code></pre>
<h3>„Äê‰æã9„ÄëË®ìÁ∑¥„Éá„Éº„Çø„Å®„ÉÜ„Çπ„Éà„Éá„Éº„Çø„ÅÆÂàÜÂâ≤</h3>
<pre><code class="language-python">from sklearn.model_selection import train_test_split

# Ë®ìÁ∑¥„Éá„Éº„Çø„Å®„ÉÜ„Çπ„Éà„Éá„Éº„Çø„Å´ÂàÜÂâ≤Ôºà80:20Ôºâ
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

print(&quot;=&quot; * 60)
print(&quot;„Éá„Éº„ÇøÂàÜÂâ≤&quot;)
print(&quot;=&quot; * 60)
print(f&quot;ÂÖ®„Éá„Éº„ÇøÊï∞: {len(X)}„Çµ„É≥„Éó„É´&quot;)
print(f&quot;Ë®ìÁ∑¥„Éá„Éº„Çø: {len(X_train)}„Çµ„É≥„Éó„É´ ({len(X_train)/len(X)*100:.1f}%)&quot;)
print(f&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø: {len(X_test)}„Çµ„É≥„Éó„É´ ({len(X_test)/len(X)*100:.1f}%)&quot;)

print(&quot;\nË®ìÁ∑¥„Éá„Éº„Çø„ÅÆÁµ±Ë®àÈáè:&quot;)
print(pd.DataFrame(X_train, columns=X.columns).describe())
</code></pre>
<hr />
<h2>3.4 ÂõûÂ∏∞„É¢„Éá„É´„Å´„Çà„Çã„Éä„ÉéÁ≤íÂ≠êÁâ©ÊÄß‰∫àÊ∏¨</h2>
<p>ÁõÆÊ®ôÔºö„Çµ„Ç§„Ç∫„ÄÅÊ∏©Â∫¶„ÄÅpH„Åã„ÇâLSPRÊ≥¢Èï∑„Çí‰∫àÊ∏¨</p>
<h3>„Äê‰æã10„ÄëÁ∑öÂΩ¢ÂõûÂ∏∞ÔºàLinear RegressionÔºâ</h3>
<pre><code class="language-python">from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Á∑öÂΩ¢ÂõûÂ∏∞„É¢„Éá„É´„ÅÆÊßãÁØâ
model_lr = LinearRegression()
model_lr.fit(X_train, y_train)

# ‰∫àÊ∏¨
y_train_pred_lr = model_lr.predict(X_train)
y_test_pred_lr = model_lr.predict(X_test)

# Ë©ï‰æ°ÊåáÊ®ô
r2_train_lr = r2_score(y_train, y_train_pred_lr)
r2_test_lr = r2_score(y_test, y_test_pred_lr)
rmse_test_lr = np.sqrt(mean_squared_error(y_test, y_test_pred_lr))
mae_test_lr = mean_absolute_error(y_test, y_test_pred_lr)

print(&quot;=&quot; * 60)
print(&quot;Á∑öÂΩ¢ÂõûÂ∏∞ÔºàLinear RegressionÔºâ&quot;)
print(&quot;=&quot; * 60)
print(f&quot;Ë®ìÁ∑¥„Éá„Éº„Çø R¬≤: {r2_train_lr:.4f}&quot;)
print(f&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø R¬≤: {r2_test_lr:.4f}&quot;)
print(f&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø RMSE: {rmse_test_lr:.4f} nm&quot;)
print(f&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø MAE: {mae_test_lr:.4f} nm&quot;)

# ÂõûÂ∏∞‰øÇÊï∞
print(&quot;\nÂõûÂ∏∞‰øÇÊï∞:&quot;)
for name, coef in zip(X.columns, model_lr.coef_):
    print(f&quot;  {name}: {coef:.4f}&quot;)
print(f&quot;  ÂàáÁâá: {model_lr.intercept_:.4f}&quot;)

# ÊÆãÂ∑Æ„Éó„É≠„ÉÉ„Éà
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# ‰∫àÊ∏¨ÂÄ§ vs ÂÆüÊ∏¨ÂÄ§
axes[0].scatter(y_test, y_test_pred_lr, alpha=0.6, edgecolors='k')
axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
             'r--', lw=2, label='Perfect Prediction')
axes[0].set_xlabel('Actual LSPR (nm)', fontsize=11)
axes[0].set_ylabel('Predicted LSPR (nm)', fontsize=11)
axes[0].set_title(f'Linear Regression (R¬≤ = {r2_test_lr:.3f})', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# ÊÆãÂ∑Æ„Éó„É≠„ÉÉ„Éà
residuals = y_test - y_test_pred_lr
axes[1].scatter(y_test_pred_lr, residuals, alpha=0.6, edgecolors='k')
axes[1].axhline(y=0, color='r', linestyle='--', lw=2)
axes[1].set_xlabel('Predicted LSPR (nm)', fontsize=11)
axes[1].set_ylabel('Residuals (nm)', fontsize=11)
axes[1].set_title('Residual Plot', fontsize=12, fontweight='bold')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>„Äê‰æã11„Äë„É©„É≥„ÉÄ„É†„Éï„Ç©„É¨„Çπ„ÉàÂõûÂ∏∞ÔºàRandom ForestÔºâ</h3>
<pre><code class="language-python">from sklearn.ensemble import RandomForestRegressor

# „É©„É≥„ÉÄ„É†„Éï„Ç©„É¨„Çπ„ÉàÂõûÂ∏∞„É¢„Éá„É´
model_rf = RandomForestRegressor(n_estimators=100, max_depth=10,
                                 random_state=42, n_jobs=-1)
model_rf.fit(X_train, y_train)

# ‰∫àÊ∏¨
y_train_pred_rf = model_rf.predict(X_train)
y_test_pred_rf = model_rf.predict(X_test)

# Ë©ï‰æ°
r2_train_rf = r2_score(y_train, y_train_pred_rf)
r2_test_rf = r2_score(y_test, y_test_pred_rf)
rmse_test_rf = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))
mae_test_rf = mean_absolute_error(y_test, y_test_pred_rf)

print(&quot;=&quot; * 60)
print(&quot;„É©„É≥„ÉÄ„É†„Éï„Ç©„É¨„Çπ„ÉàÂõûÂ∏∞ÔºàRandom ForestÔºâ&quot;)
print(&quot;=&quot; * 60)
print(f&quot;Ë®ìÁ∑¥„Éá„Éº„Çø R¬≤: {r2_train_rf:.4f}&quot;)
print(f&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø R¬≤: {r2_test_rf:.4f}&quot;)
print(f&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø RMSE: {rmse_test_rf:.4f} nm&quot;)
print(f&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø MAE: {mae_test_rf:.4f} nm&quot;)

# ÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': model_rf.feature_importances_
}).sort_values('Importance', ascending=False)

print(&quot;\nÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶:&quot;)
print(feature_importance)

# ÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶„ÅÆÂèØË¶ñÂåñ
fig, ax = plt.subplots(figsize=(8, 5))
ax.barh(feature_importance['Feature'], feature_importance['Importance'],
        color='steelblue', edgecolor='black')
ax.set_xlabel('Importance', fontsize=12)
ax.set_title('Feature Importance (Random Forest)', fontsize=13, fontweight='bold')
ax.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()
</code></pre>
<h3>„Äê‰æã12„ÄëÂãæÈÖç„Éñ„Éº„Çπ„ÉÜ„Ç£„É≥„Ç∞ÔºàLightGBMÔºâ</h3>
<pre><code class="language-python">import lightgbm as lgb

# LightGBM„É¢„Éá„É´„ÅÆÊßãÁØâ
params = {
    'objective': 'regression',
    'metric': 'rmse',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'n_estimators': 200,
    'random_state': 42,
    'verbose': -1
}

model_lgb = lgb.LGBMRegressor(**params)
model_lgb.fit(X_train, y_train)

# ‰∫àÊ∏¨
y_train_pred_lgb = model_lgb.predict(X_train)
y_test_pred_lgb = model_lgb.predict(X_test)

# Ë©ï‰æ°
r2_train_lgb = r2_score(y_train, y_train_pred_lgb)
r2_test_lgb = r2_score(y_test, y_test_pred_lgb)
rmse_test_lgb = np.sqrt(mean_squared_error(y_test, y_test_pred_lgb))
mae_test_lgb = mean_absolute_error(y_test, y_test_pred_lgb)

print(&quot;=&quot; * 60)
print(&quot;ÂãæÈÖç„Éñ„Éº„Çπ„ÉÜ„Ç£„É≥„Ç∞ÔºàLightGBMÔºâ&quot;)
print(&quot;=&quot; * 60)
print(f&quot;Ë®ìÁ∑¥„Éá„Éº„Çø R¬≤: {r2_train_lgb:.4f}&quot;)
print(f&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø R¬≤: {r2_test_lgb:.4f}&quot;)
print(f&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø RMSE: {rmse_test_lgb:.4f} nm&quot;)
print(f&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø MAE: {mae_test_lgb:.4f} nm&quot;)

# ‰∫àÊ∏¨ÂÄ§ vs ÂÆüÊ∏¨ÂÄ§„Éó„É≠„ÉÉ„Éà
fig, ax = plt.subplots(figsize=(8, 6))
ax.scatter(y_test, y_test_pred_lgb, alpha=0.6, edgecolors='k', s=60)
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
        'r--', lw=2, label='Perfect Prediction')
ax.set_xlabel('Actual LSPR (nm)', fontsize=12)
ax.set_ylabel('Predicted LSPR (nm)', fontsize=12)
ax.set_title(f'LightGBM Prediction (R¬≤ = {r2_test_lgb:.3f})',
             fontsize=13, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>
<h3>„Äê‰æã13„Äë„Çµ„Éù„Éº„Éà„Éô„ÇØ„Çø„ÉºÂõûÂ∏∞ÔºàSVRÔºâ</h3>
<pre><code class="language-python">from sklearn.svm import SVR

# SVR„É¢„Éá„É´ÔºàRBF„Ç´„Éº„Éç„É´Ôºâ
model_svr = SVR(kernel='rbf', C=10, gamma='scale', epsilon=0.1)
model_svr.fit(X_train, y_train)

# ‰∫àÊ∏¨
y_train_pred_svr = model_svr.predict(X_train)
y_test_pred_svr = model_svr.predict(X_test)

# Ë©ï‰æ°
r2_train_svr = r2_score(y_train, y_train_pred_svr)
r2_test_svr = r2_score(y_test, y_test_pred_svr)
rmse_test_svr = np.sqrt(mean_squared_error(y_test, y_test_pred_svr))
mae_test_svr = mean_absolute_error(y_test, y_test_pred_svr)

print(&quot;=&quot; * 60)
print(&quot;„Çµ„Éù„Éº„Éà„Éô„ÇØ„Çø„ÉºÂõûÂ∏∞ÔºàSVRÔºâ&quot;)
print(&quot;=&quot; * 60)
print(f&quot;Ë®ìÁ∑¥„Éá„Éº„Çø R¬≤: {r2_train_svr:.4f}&quot;)
print(f&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø R¬≤: {r2_test_svr:.4f}&quot;)
print(f&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø RMSE: {rmse_test_svr:.4f} nm&quot;)
print(f&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø MAE: {mae_test_svr:.4f} nm&quot;)
print(f&quot;„Çµ„Éù„Éº„Éà„Éô„ÇØ„Çø„ÉºÊï∞: {len(model_svr.support_)}&quot;)
</code></pre>
<h3>„Äê‰æã14„Äë„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÔºàMLP RegressorÔºâ</h3>
<pre><code class="language-python">from sklearn.neural_network import MLPRegressor

# MLP„É¢„Éá„É´
model_mlp = MLPRegressor(hidden_layer_sizes=(100, 50),
                         activation='relu',
                         solver='adam',
                         alpha=0.001,
                         max_iter=500,
                         random_state=42,
                         early_stopping=True,
                         validation_fraction=0.1,
                         verbose=False)

model_mlp.fit(X_train, y_train)

# ‰∫àÊ∏¨
y_train_pred_mlp = model_mlp.predict(X_train)
y_test_pred_mlp = model_mlp.predict(X_test)

# Ë©ï‰æ°
r2_train_mlp = r2_score(y_train, y_train_pred_mlp)
r2_test_mlp = r2_score(y_test, y_test_pred_mlp)
rmse_test_mlp = np.sqrt(mean_squared_error(y_test, y_test_pred_mlp))
mae_test_mlp = mean_absolute_error(y_test, y_test_pred_mlp)

print(&quot;=&quot; * 60)
print(&quot;„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÔºàMLP RegressorÔºâ&quot;)
print(&quot;=&quot; * 60)
print(f&quot;Ë®ìÁ∑¥„Éá„Éº„Çø R¬≤: {r2_train_mlp:.4f}&quot;)
print(f&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø R¬≤: {r2_test_mlp:.4f}&quot;)
print(f&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø RMSE: {rmse_test_mlp:.4f} nm&quot;)
print(f&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø MAE: {mae_test_mlp:.4f} nm&quot;)
print(f&quot;ÂèçÂæ©ÂõûÊï∞: {model_mlp.n_iter_}&quot;)
print(f&quot;Èö†„ÇåÂ±§„ÅÆÊßãÈÄ†: {model_mlp.hidden_layer_sizes}&quot;)
</code></pre>
<h3>„Äê‰æã15„Äë„É¢„Éá„É´ÊÄßËÉΩÊØîËºÉ</h3>
<pre><code class="language-python"># ÂÖ®„É¢„Éá„É´„ÅÆÊÄßËÉΩ„Çí„Åæ„Å®„ÇÅ„Çã
results = pd.DataFrame({
    'Model': ['Linear Regression', 'Random Forest', 'LightGBM', 'SVR', 'MLP'],
    'R¬≤ (Train)': [r2_train_lr, r2_train_rf, r2_train_lgb, r2_train_svr, r2_train_mlp],
    'R¬≤ (Test)': [r2_test_lr, r2_test_rf, r2_test_lgb, r2_test_svr, r2_test_mlp],
    'RMSE (Test)': [rmse_test_lr, rmse_test_rf, rmse_test_lgb, rmse_test_svr, rmse_test_mlp],
    'MAE (Test)': [mae_test_lr, mae_test_rf, mae_test_lgb, mae_test_svr, mae_test_mlp]
})

results['Overfit'] = results['R¬≤ (Train)'] - results['R¬≤ (Test)']

print(&quot;=&quot; * 80)
print(&quot;ÂÖ®„É¢„Éá„É´„ÅÆÊÄßËÉΩÊØîËºÉ&quot;)
print(&quot;=&quot; * 80)
print(results.to_string(index=False))

# ÊúÄËâØ„É¢„Éá„É´„ÅÆÁâπÂÆö
best_model_idx = results['R¬≤ (Test)'].idxmax()
best_model_name = results.loc[best_model_idx, 'Model']
best_r2 = results.loc[best_model_idx, 'R¬≤ (Test)']

print(f&quot;\nÊúÄËâØ„É¢„Éá„É´: {best_model_name} (R¬≤ = {best_r2:.4f})&quot;)

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# R¬≤„Çπ„Ç≥„Ç¢ÊØîËºÉ
x_pos = np.arange(len(results))
axes[0].bar(x_pos, results['R¬≤ (Test)'], alpha=0.7, color='steelblue',
            edgecolor='black', label='Test R¬≤')
axes[0].set_xticks(x_pos)
axes[0].set_xticklabels(results['Model'], rotation=15, ha='right')
axes[0].set_ylabel('R¬≤ Score', fontsize=12)
axes[0].set_title('Model Comparison: R¬≤ Score', fontsize=13, fontweight='bold')
axes[0].grid(True, alpha=0.3, axis='y')
axes[0].legend()

# RMSEÊØîËºÉ
axes[1].bar(x_pos, results['RMSE (Test)'], alpha=0.7, color='coral',
            edgecolor='black', label='Test RMSE')
axes[1].set_xticks(x_pos)
axes[1].set_xticklabels(results['Model'], rotation=15, ha='right')
axes[1].set_ylabel('RMSE (nm)', fontsize=12)
axes[1].set_title('Model Comparison: RMSE', fontsize=13, fontweight='bold')
axes[1].grid(True, alpha=0.3, axis='y')
axes[1].legend()

plt.tight_layout()
plt.show()
</code></pre>
<hr />
<h2>3.5 ÈáèÂ≠ê„Éâ„ÉÉ„ÉàÁô∫ÂÖâÊ≥¢Èï∑‰∫àÊ∏¨</h2>
<h3>„Äê‰æã16„Äë„Éá„Éº„ÇøÁîüÊàêÔºöCdSeÈáèÂ≠ê„Éâ„ÉÉ„Éà</h3>
<p>CdSeÈáèÂ≠ê„Éâ„ÉÉ„Éà„ÅÆÁô∫ÂÖâÊ≥¢Èï∑„ÅØ„ÄÅBrusÊñπÁ®ãÂºè„Å´Âü∫„Å•„Åç„Çµ„Ç§„Ç∫„Å´‰æùÂ≠ò„Åó„Åæ„Åô„ÄÇ</p>
<pre><code class="language-python"># CdSeÈáèÂ≠ê„Éâ„ÉÉ„Éà„Éá„Éº„Çø„ÅÆÁîüÊàê
np.random.seed(100)

n_qd_samples = 150

# ÈáèÂ≠ê„Éâ„ÉÉ„Éà„ÅÆ„Çµ„Ç§„Ç∫Ôºà2-10 nmÔºâ
size_qd = np.random.uniform(2, 10, n_qd_samples)

# BrusÊñπÁ®ãÂºè„ÅÆÁ∞°ÊòìËøë‰ºº: emission = 520 + 130/(size^0.8) + noise
emission = 520 + 130 / (size_qd ** 0.8) + np.random.normal(0, 10, n_qd_samples)

# ÂêàÊàêÊù°‰ª∂
synthesis_time = np.random.uniform(10, 120, n_qd_samples)  # ÂàÜ
precursor_ratio = np.random.uniform(0.5, 2.0, n_qd_samples)  # „É¢„É´ÊØî

# „Éá„Éº„Çø„Éï„É¨„Éº„É†‰ΩúÊàê
data_qd = pd.DataFrame({
    'size_nm': size_qd,
    'emission_nm': emission,
    'synthesis_time_min': synthesis_time,
    'precursor_ratio': precursor_ratio
})

print(&quot;=&quot; * 60)
print(&quot;CdSeÈáèÂ≠ê„Éâ„ÉÉ„Éà„Éá„Éº„Çø„ÅÆÁîüÊàêÂÆå‰∫Ü&quot;)
print(&quot;=&quot; * 60)
print(data_qd.head(10))
print(&quot;\nÂü∫Êú¨Áµ±Ë®àÈáè:&quot;)
print(data_qd.describe())

# „Çµ„Ç§„Ç∫„Å®Áô∫ÂÖâÊ≥¢Èï∑„ÅÆÈñ¢‰øÇ„Éó„É≠„ÉÉ„Éà
fig, ax = plt.subplots(figsize=(10, 6))
scatter = ax.scatter(data_qd['size_nm'], data_qd['emission_nm'],
                     c=data_qd['synthesis_time_min'], cmap='plasma',
                     s=80, alpha=0.7, edgecolors='k')
ax.set_xlabel('Quantum Dot Size (nm)', fontsize=12)
ax.set_ylabel('Emission Wavelength (nm)', fontsize=12)
ax.set_title('CdSe Quantum Dot: Size vs Emission Wavelength',
             fontsize=13, fontweight='bold')
cbar = plt.colorbar(scatter, ax=ax)
cbar.set_label('Synthesis Time (min)', fontsize=10)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>
<h3>„Äê‰æã17„ÄëÈáèÂ≠ê„Éâ„ÉÉ„Éà„É¢„Éá„É´ÔºàLightGBMÔºâ</h3>
<pre><code class="language-python"># ÁâπÂæ¥Èáè„Å®„Çø„Éº„Ç≤„ÉÉ„Éà„ÅÆÂàÜÈõ¢
X_qd = data_qd[['size_nm', 'synthesis_time_min', 'precursor_ratio']]
y_qd = data_qd['emission_nm']

# „Çπ„Ç±„Éº„É™„É≥„Ç∞
scaler_qd = StandardScaler()
X_qd_scaled = scaler_qd.fit_transform(X_qd)

# Ë®ìÁ∑¥/„ÉÜ„Çπ„ÉàÂàÜÂâ≤
X_qd_train, X_qd_test, y_qd_train, y_qd_test = train_test_split(
    X_qd_scaled, y_qd, test_size=0.2, random_state=42
)

# LightGBM„É¢„Éá„É´
model_qd = lgb.LGBMRegressor(
    objective='regression',
    num_leaves=31,
    learning_rate=0.05,
    n_estimators=200,
    random_state=42,
    verbose=-1
)

model_qd.fit(X_qd_train, y_qd_train)

# ‰∫àÊ∏¨
y_qd_train_pred = model_qd.predict(X_qd_train)
y_qd_test_pred = model_qd.predict(X_qd_test)

# Ë©ï‰æ°
r2_qd_train = r2_score(y_qd_train, y_qd_train_pred)
r2_qd_test = r2_score(y_qd_test, y_qd_test_pred)
rmse_qd = np.sqrt(mean_squared_error(y_qd_test, y_qd_test_pred))
mae_qd = mean_absolute_error(y_qd_test, y_qd_test_pred)

print(&quot;=&quot; * 60)
print(&quot;ÈáèÂ≠ê„Éâ„ÉÉ„ÉàÁô∫ÂÖâÊ≥¢Èï∑‰∫àÊ∏¨„É¢„Éá„É´ÔºàLightGBMÔºâ&quot;)
print(&quot;=&quot; * 60)
print(f&quot;Ë®ìÁ∑¥„Éá„Éº„Çø R¬≤: {r2_qd_train:.4f}&quot;)
print(f&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø R¬≤: {r2_qd_test:.4f}&quot;)
print(f&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø RMSE: {rmse_qd:.4f} nm&quot;)
print(f&quot;„ÉÜ„Çπ„Éà„Éá„Éº„Çø MAE: {mae_qd:.4f} nm&quot;)
</code></pre>
<h3>„Äê‰æã18„Äë‰∫àÊ∏¨ÁµêÊûú„ÅÆÂèØË¶ñÂåñ</h3>
<pre><code class="language-python"># ‰∫àÊ∏¨ÂÄ§ vs ÂÆüÊ∏¨ÂÄ§„Éó„É≠„ÉÉ„ÉàÔºà‰ø°È†ºÂå∫Èñì‰ªò„ÅçÔºâ
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# „ÉÜ„Çπ„Éà„Éá„Éº„Çø„ÅÆ„Éó„É≠„ÉÉ„Éà
axes[0].scatter(y_qd_test, y_qd_test_pred, alpha=0.6, s=80,
                edgecolors='k', label='Test Data')
axes[0].plot([y_qd_test.min(), y_qd_test.max()],
             [y_qd_test.min(), y_qd_test.max()],
             'r--', lw=2, label='Perfect Prediction')

# ¬±10 nm „ÅÆÁØÑÂõ≤„ÇíË°®Á§∫
axes[0].fill_between([y_qd_test.min(), y_qd_test.max()],
                     [y_qd_test.min()-10, y_qd_test.max()-10],
                     [y_qd_test.min()+10, y_qd_test.max()+10],
                     alpha=0.2, color='gray', label='¬±10 nm')

axes[0].set_xlabel('Actual Emission (nm)', fontsize=12)
axes[0].set_ylabel('Predicted Emission (nm)', fontsize=12)
axes[0].set_title(f'QD Emission Prediction (R¬≤ = {r2_qd_test:.3f})',
                  fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# „Çµ„Ç§„Ç∫Âà•„ÅÆ‰∫àÊ∏¨Á≤æÂ∫¶
size_bins = [2, 4, 6, 8, 10]
size_labels = ['2-4 nm', '4-6 nm', '6-8 nm', '8-10 nm']
data_qd_test = pd.DataFrame({
    'size': X_qd.iloc[y_qd_test.index]['size_nm'].values,
    'actual': y_qd_test.values,
    'predicted': y_qd_test_pred
})
data_qd_test['size_bin'] = pd.cut(data_qd_test['size'], bins=size_bins, labels=size_labels)
data_qd_test['error'] = np.abs(data_qd_test['actual'] - data_qd_test['predicted'])

# „Çµ„Ç§„Ç∫„Éì„É≥„Åî„Å®„ÅÆÂπ≥ÂùáË™§Â∑Æ
error_by_size = data_qd_test.groupby('size_bin')['error'].mean()

axes[1].bar(range(len(error_by_size)), error_by_size.values,
            color='coral', edgecolor='black', alpha=0.7)
axes[1].set_xticks(range(len(error_by_size)))
axes[1].set_xticklabels(error_by_size.index)
axes[1].set_ylabel('Mean Absolute Error (nm)', fontsize=12)
axes[1].set_xlabel('QD Size Range', fontsize=12)
axes[1].set_title('Prediction Error by QD Size', fontsize=13, fontweight='bold')
axes[1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print(f&quot;\nÂÖ®‰Ωì„ÅÆÂπ≥ÂùáÁµ∂ÂØæË™§Â∑Æ: {mae_qd:.2f} nm&quot;)
print(&quot;„Çµ„Ç§„Ç∫Âà•„ÅÆÂπ≥ÂùáÁµ∂ÂØæË™§Â∑Æ:&quot;)
print(error_by_size)
</code></pre>
<hr />
<h2>3.6 ÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶ÂàÜÊûê</h2>
<h3>„Äê‰æã19„ÄëÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶ÔºàLightGBMÔºâ</h3>
<pre><code class="language-python"># LightGBM„É¢„Éá„É´„ÅÆÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶Ôºà„Ç≤„Ç§„É≥„Éô„Éº„ÇπÔºâ
importance_gain = model_lgb.feature_importances_
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': importance_gain
}).sort_values('Importance', ascending=False)

print(&quot;=&quot; * 60)
print(&quot;ÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶ÔºàLightGBMÔºâ&quot;)
print(&quot;=&quot; * 60)
print(importance_df)

# ÂèØË¶ñÂåñ
fig, ax = plt.subplots(figsize=(8, 5))
colors = ['steelblue', 'coral', 'lightgreen']
ax.barh(importance_df['Feature'], importance_df['Importance'],
        color=colors, edgecolor='black')
ax.set_xlabel('Feature Importance (Gain)', fontsize=12)
ax.set_title('Feature Importance: LSPR Prediction',
             fontsize=13, fontweight='bold')
ax.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()

print(f&quot;\nÊúÄ„ÇÇÈáçË¶Å„Å™ÁâπÂæ¥Èáè: {importance_df.iloc[0]['Feature']}&quot;)
</code></pre>
<h3>„Äê‰æã20„ÄëSHAPÂàÜÊûêÔºö‰∫àÊ∏¨Ëß£Èáà</h3>
<pre><code class="language-python">import shap

# SHAP Explainer„ÅÆ‰ΩúÊàê
explainer = shap.Explainer(model_lgb, X_train)
shap_values = explainer(X_test)

print(&quot;=&quot; * 60)
print(&quot;SHAPÂàÜÊûê&quot;)
print(&quot;=&quot; * 60)
print(&quot;SHAPÂÄ§„ÅÆË®àÁÆóÂÆå‰∫Ü&quot;)
print(f&quot;SHAPÂÄ§„ÅÆÂΩ¢Áä∂: {shap_values.values.shape}&quot;)

# SHAP Summary Plot
fig, ax = plt.subplots(figsize=(10, 6))
shap.summary_plot(shap_values, X_test, feature_names=X.columns, show=False)
plt.title('SHAP Summary Plot: Feature Impact on LSPR Prediction',
          fontsize=13, fontweight='bold', pad=20)
plt.tight_layout()
plt.show()

# SHAP Dependence PlotÔºàÊúÄ„ÇÇÈáçË¶Å„Å™ÁâπÂæ¥ÈáèÔºâ
top_feature_idx = importance_df.index[0]
top_feature_name = X.columns[top_feature_idx]

fig, ax = plt.subplots(figsize=(10, 6))
shap.dependence_plot(top_feature_idx, shap_values.values, X_test,
                     feature_names=X.columns, show=False)
plt.title(f'SHAP Dependence Plot: {top_feature_name}',
          fontsize=13, fontweight='bold')
plt.tight_layout()
plt.show()

print(f&quot;\nSHAPÂàÜÊûê„Å´„Çà„Çä„ÄÅ{top_feature_name}„ÅåLSPRÊ≥¢Èï∑‰∫àÊ∏¨„Å´ÊúÄ„ÇÇÂΩ±Èüø„Åô„Çã„Åì„Å®„ÅåÁ¢∫Ë™ç„Åï„Çå„Åæ„Åó„Åü&quot;)
</code></pre>
<hr />
<h2>3.7 „Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„Å´„Çà„Çã„Éä„ÉéÊùêÊñôË®≠Ë®à</h2>
<p>ÁõÆÊ®ôÔºöÁõÆÊ®ôLSPRÊ≥¢Èï∑Ôºà550 nmÔºâ„ÇíÂÆüÁèæ„Åô„ÇãÊúÄÈÅ©„Å™ÂêàÊàêÊù°‰ª∂„ÇíÊé¢Á¥¢</p>
<h3>„Äê‰æã21„ÄëÊé¢Á¥¢Á©∫Èñì„ÅÆÂÆöÁæ©</h3>
<pre><code class="language-python">from skopt.space import Real

# Êé¢Á¥¢Á©∫Èñì„ÅÆÂÆöÁæ©
# „Çµ„Ç§„Ç∫: 10-40 nm„ÄÅÊ∏©Â∫¶: 20-80¬∞C„ÄÅpH: 4-10
search_space = [
    Real(10, 40, name='size_nm'),
    Real(20, 80, name='temperature_C'),
    Real(4, 10, name='pH')
]

print(&quot;=&quot; * 60)
print(&quot;„Éô„Ç§„Ç∫ÊúÄÈÅ©ÂåñÔºöÊé¢Á¥¢Á©∫Èñì„ÅÆÂÆöÁæ©&quot;)
print(&quot;=&quot; * 60)
for dim in search_space:
    print(f&quot;  {dim.name}: [{dim.bounds[0]}, {dim.bounds[1]}]&quot;)

print(&quot;\nÁõÆÊ®ô: LSPRÊ≥¢Èï∑ = 550 nm „ÇíÂÆüÁèæ„Åô„ÇãÊù°‰ª∂„ÇíÊé¢Á¥¢&quot;)
</code></pre>
<h3>„Äê‰æã22„ÄëÁõÆÁöÑÈñ¢Êï∞„ÅÆË®≠ÂÆö</h3>
<pre><code class="language-python"># ÁõÆÁöÑÈñ¢Êï∞Ôºö‰∫àÊ∏¨LSPRÊ≥¢Èï∑„Å®ÁõÆÊ®ôÊ≥¢Èï∑Ôºà550 nmÔºâ„ÅÆÂ∑Æ„ÅÆÁµ∂ÂØæÂÄ§„ÇíÊúÄÂ∞èÂåñ
target_lspr = 550.0

def objective_function(params):
    &quot;&quot;&quot;
    „Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„ÅÆÁõÆÁöÑÈñ¢Êï∞

    Parameters:
    -----------
    params : list
        [size_nm, temperature_C, pH]

    Returns:
    --------
    float
        ÁõÆÊ®ôÊ≥¢Èï∑„Å®„ÅÆË™§Â∑ÆÔºàÊúÄÂ∞èÂåñ„Åô„ÇãÂÄ§Ôºâ
    &quot;&quot;&quot;
    # „Éë„É©„É°„Éº„Çø„ÇíÂèñÂæó
    size, temp, ph = params

    # ÁâπÂæ¥Èáè„ÇíÊßãÁØâÔºà„Çπ„Ç±„Éº„É™„É≥„Ç∞ÈÅ©Áî®Ôºâ
    features = np.array([[size, temp, ph]])
    features_scaled = scaler.transform(features)

    # LSPRÊ≥¢Èï∑„Çí‰∫àÊ∏¨
    predicted_lspr = model_lgb.predict(features_scaled)[0]

    # ÁõÆÊ®ôÊ≥¢Èï∑„Å®„ÅÆË™§Â∑ÆÔºàÁµ∂ÂØæÂÄ§Ôºâ
    error = abs(predicted_lspr - target_lspr)

    return error

# „ÉÜ„Çπ„ÉàÂÆüË°å
test_params = [20.0, 50.0, 7.0]
test_error = objective_function(test_params)
print(f&quot;\n„ÉÜ„Çπ„ÉàÂÆüË°å:&quot;)
print(f&quot;  „Éë„É©„É°„Éº„Çø: size={test_params[0]} nm, temp={test_params[1]}¬∞C, pH={test_params[2]}&quot;)
print(f&quot;  ÁõÆÁöÑÈñ¢Êï∞ÂÄ§ÔºàË™§Â∑ÆÔºâ: {test_error:.4f} nm&quot;)
</code></pre>
<h3>„Äê‰æã23„Äë„Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„ÅÆÂÆüË°åÔºàscikit-optimizeÔºâ</h3>
<pre><code class="language-python">from skopt import gp_minimize
from skopt.plots import plot_convergence, plot_objective

# „Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„ÅÆÂÆüË°å
print(&quot;\n&quot; + &quot;=&quot; * 60)
print(&quot;„Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„ÅÆÂÆüË°å‰∏≠...&quot;)
print(&quot;=&quot; * 60)

result = gp_minimize(
    func=objective_function,
    dimensions=search_space,
    n_calls=50,  # Ë©ï‰æ°ÂõûÊï∞
    n_initial_points=10,  # „É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞ÂõûÊï∞
    random_state=42,
    verbose=False
)

print(&quot;ÊúÄÈÅ©ÂåñÂÆå‰∫ÜÔºÅ&quot;)
print(&quot;\n&quot; + &quot;=&quot; * 60)
print(&quot;ÊúÄÈÅ©ÂåñÁµêÊûú&quot;)
print(&quot;=&quot; * 60)
print(f&quot;ÊúÄÂ∞èÁõÆÁöÑÈñ¢Êï∞ÂÄ§ÔºàË™§Â∑ÆÔºâ: {result.fun:.4f} nm&quot;)
print(f&quot;\nÊúÄÈÅ©„Éë„É©„É°„Éº„Çø:&quot;)
print(f&quot;  „Çµ„Ç§„Ç∫: {result.x[0]:.2f} nm&quot;)
print(f&quot;  Ê∏©Â∫¶: {result.x[1]:.2f} ¬∞C&quot;)
print(f&quot;  pH: {result.x[2]:.2f}&quot;)

# ÊúÄÈÅ©Êù°‰ª∂„Åß„ÅÆ‰∫àÊ∏¨LSPRÊ≥¢Èï∑„ÇíË®àÁÆó
optimal_features = np.array([result.x])
optimal_features_scaled = scaler.transform(optimal_features)
predicted_optimal_lspr = model_lgb.predict(optimal_features_scaled)[0]

print(f&quot;\n‰∫àÊ∏¨„Åï„Çå„ÇãLSPRÊ≥¢Èï∑: {predicted_optimal_lspr:.2f} nm&quot;)
print(f&quot;ÁõÆÊ®ôLSPRÊ≥¢Èï∑: {target_lspr} nm&quot;)
print(f&quot;ÈÅîÊàêÁ≤æÂ∫¶: {abs(predicted_optimal_lspr - target_lspr):.2f} nm&quot;)
</code></pre>
<h3>„Äê‰æã24„ÄëÊúÄÈÅ©ÂåñÁµêÊûú„ÅÆÂèØË¶ñÂåñ</h3>
<pre><code class="language-python"># ÊúÄÈÅ©Âåñ„Éó„É≠„Çª„Çπ„ÅÆÂèØË¶ñÂåñ
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# ÂèéÊùü„Éó„É≠„ÉÉ„Éà
plot_convergence(result, ax=axes[0])
axes[0].set_title('Convergence Plot', fontsize=13, fontweight='bold')
axes[0].set_ylabel('Objective Value (Error, nm)', fontsize=11)
axes[0].set_xlabel('Number of Evaluations', fontsize=11)
axes[0].grid(True, alpha=0.3)

# Ë©ï‰æ°Â±•Ê≠¥„ÅÆ„Éó„É≠„ÉÉ„Éà
iterations = range(1, len(result.func_vals) + 1)
axes[1].plot(iterations, result.func_vals, 'o-', alpha=0.6, label='Evaluation')
axes[1].plot(iterations, np.minimum.accumulate(result.func_vals),
             'r-', linewidth=2, label='Best So Far')
axes[1].set_xlabel('Iteration', fontsize=11)
axes[1].set_ylabel('Objective Value (Error, nm)', fontsize=11)
axes[1].set_title('Optimization Progress', fontsize=13, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>„Äê‰æã25„ÄëÂèéÊùü„Éó„É≠„ÉÉ„Éà</h3>
<pre><code class="language-python"># Ë©≥Á¥∞„Å™ÂèéÊùü„Éó„É≠„ÉÉ„ÉàÔºàÊúÄËâØÂÄ§„ÅÆÊé®ÁßªÔºâ
fig, ax = plt.subplots(figsize=(10, 6))

cumulative_min = np.minimum.accumulate(result.func_vals)
iterations = np.arange(1, len(cumulative_min) + 1)

ax.plot(iterations, cumulative_min, 'b-', linewidth=2, marker='o',
        markersize=4, label='Best Error')
ax.axhline(y=result.fun, color='r', linestyle='--', linewidth=2,
           label=f'Final Best: {result.fun:.2f} nm')
ax.fill_between(iterations, 0, cumulative_min, alpha=0.2, color='blue')

ax.set_xlabel('Iteration', fontsize=12)
ax.set_ylabel('Minimum Error (nm)', fontsize=12)
ax.set_title('Bayesian Optimization: Convergence to Optimal Solution',
             fontsize=13, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f&quot;\n{len(result.func_vals)}Âõû„ÅÆË©ï‰æ°„ÅßÊúÄÈÅ©Ëß£„Å´ÂèéÊùü„Åó„Åæ„Åó„Åü&quot;)
print(f&quot;ÂàùÊúüË©ï‰æ°„Åß„ÅÆÊúÄËâØË™§Â∑Æ: {result.func_vals[0]:.2f} nm&quot;)
print(f&quot;ÊúÄÁµÇÁöÑ„Å™ÊúÄËâØË™§Â∑Æ: {result.fun:.2f} nm&quot;)
print(f&quot;ÊîπÂñÑÁéá: {(1 - result.fun/result.func_vals[0])*100:.1f}%&quot;)
</code></pre>
<hr />
<h2>3.8 Â§öÁõÆÁöÑÊúÄÈÅ©ÂåñÔºö„Çµ„Ç§„Ç∫„Å®Áô∫ÂÖâÂäπÁéá„ÅÆ„Éà„É¨„Éº„Éâ„Ç™„Éï</h2>
<h3>„Äê‰æã26„ÄëParetoÊúÄÈÅ©ÂåñÔºàNSGA-IIÔºâ</h3>
<p>Â§öÁõÆÁöÑÊúÄÈÅ©Âåñ„Åß„ÅØ„ÄÅË§áÊï∞„ÅÆÁõÆÁöÑ„ÇíÂêåÊôÇ„Å´ÊúÄÈÅ©Âåñ„Åó„Åæ„Åô„ÄÇ„Åì„Åì„Åß„ÅØ„ÄÅÈáèÂ≠ê„Éâ„ÉÉ„Éà„ÅÆ„Çµ„Ç§„Ç∫„ÇíÊúÄÂ∞èÂåñ„Åó„Å§„Å§„ÄÅÁô∫ÂÖâÂäπÁéáÔºà‰ªÆÊÉ≥ÁöÑ„Å™ÊåáÊ®ôÔºâ„ÇíÊúÄÂ§ßÂåñ„Åó„Åæ„Åô„ÄÇ</p>
<pre><code class="language-python"># pymoo„Çí‰ΩøÁî®„Åó„ÅüÂ§öÁõÆÁöÑÊúÄÈÅ©Âåñ
try:
    from pymoo.core.problem import Problem
    from pymoo.algorithms.moo.nsga2 import NSGA2
    from pymoo.optimize import minimize as pymoo_minimize
    from pymoo.operators.crossover.sbx import SBX
    from pymoo.operators.mutation.pm import PM
    from pymoo.operators.sampling.rnd import FloatRandomSampling

    # Â§öÁõÆÁöÑÊúÄÈÅ©ÂåñÂïèÈ°å„ÅÆÂÆöÁæ©
    class QuantumDotOptimization(Problem):
        def __init__(self):
            super().__init__(
                n_var=3,  # Â§âÊï∞Êï∞Ôºàsize, synthesis_time, precursor_ratioÔºâ
                n_obj=2,  # ÁõÆÁöÑÈñ¢Êï∞Êï∞ÔºàsizeÊúÄÂ∞èÂåñ„ÄÅemissionÂäπÁéáÊúÄÂ§ßÂåñÔºâ
                n_constr=0,  # Âà∂Á¥Ñ„Å™„Åó
                xl=np.array([2.0, 10.0, 0.5]),  # ‰∏ãÈôê
                xu=np.array([10.0, 120.0, 2.0])  # ‰∏äÈôê
            )

        def _evaluate(self, X, out, *args, **kwargs):
            # ÁõÆÁöÑÈñ¢Êï∞1: „Çµ„Ç§„Ç∫„ÅÆÊúÄÂ∞èÂåñ
            obj1 = X[:, 0]  # size

            # ÁõÆÁöÑÈñ¢Êï∞2: Áô∫ÂÖâÂäπÁéá„ÅÆÊúÄÂ§ßÂåñÔºàË≤†„ÅÆÂÄ§„ÅßÊúÄÂ∞èÂåñÂïèÈ°å„Å´Â§âÊèõÔºâ
            # ÂäπÁéá„ÅØ‰ªÆÊÉ≥ÁöÑ„Å´„ÄÅemission wavelength„Åå550 nm„Å´Ëøë„ÅÑ„Åª„Å©È´ò„ÅÑ„Å®‰ªÆÂÆö
            features = X  # [size, synthesis_time, precursor_ratio]
            features_scaled = scaler_qd.transform(features)
            predicted_emission = model_qd.predict(features_scaled)

            # ÂäπÁéáÔºö550 nm„Åã„Çâ„ÅÆ„Åö„Çå„ÅåÂ∞è„Åï„ÅÑ„Åª„Å©È´ò„ÅÑÔºàË≤†„ÅÆÂÄ§„ÅßÊúÄÂ§ßÂåñ‚ÜíÊúÄÂ∞èÂåñÔºâ
            efficiency = -np.abs(predicted_emission - 550)
            obj2 = -efficiency  # ÊúÄÂ§ßÂåñ„ÇíÊúÄÂ∞èÂåñÂïèÈ°å„Å´Â§âÊèõ

            out[&quot;F&quot;] = np.column_stack([obj1, obj2])

    # ÂïèÈ°å„ÅÆ„Ç§„É≥„Çπ„Çø„É≥„ÇπÂåñ
    problem = QuantumDotOptimization()

    # NSGA-II„Ç¢„É´„Ç¥„É™„Ç∫„É†
    algorithm = NSGA2(
        pop_size=40,
        sampling=FloatRandomSampling(),
        crossover=SBX(prob=0.9, eta=15),
        mutation=PM(eta=20),
        eliminate_duplicates=True
    )

    # ÊúÄÈÅ©ÂåñÂÆüË°å
    print(&quot;=&quot; * 60)
    print(&quot;Â§öÁõÆÁöÑÊúÄÈÅ©ÂåñÔºàNSGA-IIÔºâÂÆüË°å‰∏≠...&quot;)
    print(&quot;=&quot; * 60)

    res = pymoo_minimize(
        problem,
        algorithm,
        ('n_gen', 50),  # ‰∏ñ‰ª£Êï∞
        seed=42,
        verbose=False
    )

    print(&quot;Â§öÁõÆÁöÑÊúÄÈÅ©ÂåñÂÆå‰∫ÜÔºÅ&quot;)
    print(f&quot;\n„Éë„É¨„Éº„ÉàÊúÄÈÅ©Ëß£„ÅÆÊï∞: {len(res.F)}&quot;)

    # „Éë„É¨„Éº„ÉàÊúÄÈÅ©Ëß£„ÅÆË°®Á§∫Ôºà‰∏ä‰Ωç5„Å§Ôºâ
    print(&quot;\n‰ª£Ë°®ÁöÑ„Å™„Éë„É¨„Éº„ÉàÊúÄÈÅ©Ëß£Ôºà‰∏ä‰Ωç5„Å§Ôºâ:&quot;)
    pareto_solutions = pd.DataFrame({
        'Size (nm)': res.X[:, 0],
        'Synthesis Time (min)': res.X[:, 1],
        'Precursor Ratio': res.X[:, 2],
        'Obj1: Size': res.F[:, 0],
        'Obj2: -Efficiency': res.F[:, 1]
    }).head(5)
    print(pareto_solutions.to_string(index=False))

    PYMOO_AVAILABLE = True

except ImportError:
    print(&quot;=&quot; * 60)
    print(&quot;pymoo„Åå„Ç§„É≥„Çπ„Éà„Éº„É´„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì&quot;)
    print(&quot;=&quot; * 60)
    print(&quot;Â§öÁõÆÁöÑÊúÄÈÅ©Âåñ„Å´„ÅØpymoo„ÅåÂøÖË¶Å„Åß„Åô:&quot;)
    print(&quot;  pip install pymoo&quot;)
    print(&quot;\n‰ª£„Çè„Çä„Å´„ÄÅÁ∞°ÊòìÁöÑ„Å™Â§öÁõÆÁöÑÊúÄÈÅ©Âåñ„ÅÆ‰æã„ÇíË°®Á§∫„Åó„Åæ„Åô&quot;)

    # Á∞°ÊòìÁöÑ„Å™„Ç∞„É™„ÉÉ„Éâ„Çµ„Éº„ÉÅ„Å´„Çà„ÇãÂ§öÁõÆÁöÑÊúÄÈÅ©Âåñ„ÅÆÊ®°Êì¨
    sizes = np.linspace(2, 10, 20)
    times = np.linspace(10, 120, 20)
    ratios = np.linspace(0.5, 2.0, 20)

    # „Ç∞„É™„ÉÉ„Éâ„Çµ„Éº„ÉÅÔºà„Çµ„É≥„Éó„É™„É≥„Ç∞Ôºâ
    sample_X = []
    sample_F = []

    for size in sizes[::4]:
        for time in times[::4]:
            for ratio in ratios[::4]:
                features = np.array([[size, time, ratio]])
                features_scaled = scaler_qd.transform(features)
                emission = model_qd.predict(features_scaled)[0]

                obj1 = size
                obj2 = abs(emission - 550)

                sample_X.append([size, time, ratio])
                sample_F.append([obj1, obj2])

    sample_X = np.array(sample_X)
    sample_F = np.array(sample_F)

    print(&quot;\n„Ç∞„É™„ÉÉ„Éâ„Çµ„Éº„ÉÅ„Å´„Çà„ÇãËß£„ÅÆÊé¢Á¥¢ÂÆå‰∫Ü&quot;)
    print(f&quot;Êé¢Á¥¢„Åó„ÅüËß£„ÅÆÊï∞: {len(sample_F)}&quot;)

    res = type('Result', (), {
        'X': sample_X,
        'F': sample_F
    })()

    PYMOO_AVAILABLE = False
</code></pre>
<h3>„Äê‰æã27„ÄëPareto„Éï„É≠„É≥„Éà„ÅÆÂèØË¶ñÂåñ</h3>
<pre><code class="language-python"># Pareto„Éï„É≠„É≥„Éà„ÅÆÂèØË¶ñÂåñ
fig, ax = plt.subplots(figsize=(10, 7))

if PYMOO_AVAILABLE:
    # NSGA-II„ÅÆÁµêÊûú„Çí„Éó„É≠„ÉÉ„Éà
    ax.scatter(res.F[:, 0], -res.F[:, 1], c='blue', s=80, alpha=0.6,
               edgecolors='black', label='Pareto Optimal Solutions')

    title_suffix = &quot;(NSGA-II)&quot;
else:
    # „Ç∞„É™„ÉÉ„Éâ„Çµ„Éº„ÉÅ„ÅÆÁµêÊûú„Çí„Éó„É≠„ÉÉ„Éà
    ax.scatter(res.F[:, 0], res.F[:, 1], c='blue', s=60, alpha=0.5,
               edgecolors='black', label='Sampled Solutions')

    title_suffix = &quot;(Grid Search)&quot;

ax.set_xlabel('Objective 1: Size (nm) [Minimize]', fontsize=12)

if PYMOO_AVAILABLE:
    ax.set_ylabel('Objective 2: Efficiency [Maximize]', fontsize=12)
else:
    ax.set_ylabel('Objective 2: Deviation from 550nm [Minimize]', fontsize=12)

ax.set_title(f'Pareto Front: Size vs Emission Efficiency {title_suffix}',
             fontsize=13, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;\n„Éë„É¨„Éº„Éà„Éï„É≠„É≥„Éà:&quot;)
print(&quot;  „Çµ„Ç§„Ç∫„ÇíÂ∞è„Åï„Åè„Åô„Çã„Å®ÂäπÁéá„Åå‰∏ã„Åå„Çä„ÄÅÂäπÁéá„Çí‰∏ä„Åí„Çã„Å®„Çµ„Ç§„Ç∫„ÅåÂ§ß„Åç„Åè„Å™„Çã&quot;)
print(&quot;  ‚Üí „Éà„É¨„Éº„Éâ„Ç™„ÉïÈñ¢‰øÇ„ÅåÊòéÁ¢∫„Å´&quot;)
</code></pre>
<hr />
<h2>3.9 TEMÁîªÂÉèËß£Êûê„Å®„Çµ„Ç§„Ç∫ÂàÜÂ∏É</h2>
<h3>„Äê‰æã28„ÄëÊ®°Êì¨TEM„Éá„Éº„Çø„ÅÆÁîüÊàê</h3>
<p>TEMÔºàÈÄèÈÅéÂûãÈõªÂ≠êÈ°ïÂæÆÈè°Ôºâ„ÅßÊ∏¨ÂÆö„Åï„Çå„Çã„Éä„ÉéÁ≤íÂ≠ê„Çµ„Ç§„Ç∫„ÅØ„ÄÅ„Åó„Å∞„Åó„Å∞ÂØæÊï∞Ê≠£Ë¶èÂàÜÂ∏É„Å´Âæì„ÅÑ„Åæ„Åô„ÄÇ</p>
<pre><code class="language-python">from scipy.stats import lognorm

# ÂØæÊï∞Ê≠£Ë¶èÂàÜÂ∏É„Å´Âæì„ÅÜTEM„Çµ„Ç§„Ç∫„Éá„Éº„Çø„ÅÆÁîüÊàê
np.random.seed(200)

# „Éë„É©„É°„Éº„Çø
mean_size = 20  # Âπ≥Âùá„Çµ„Ç§„Ç∫ÔºànmÔºâ
cv = 0.3  # Â§âÂãï‰øÇÊï∞ÔºàÊ®ôÊ∫ñÂÅèÂ∑Æ/Âπ≥ÂùáÔºâ

# ÂØæÊï∞Ê≠£Ë¶èÂàÜÂ∏É„ÅÆ„Éë„É©„É°„Éº„ÇøË®àÁÆó
sigma = np.sqrt(np.log(1 + cv**2))
mu = np.log(mean_size) - 0.5 * sigma**2

# „Çµ„É≥„Éó„É´ÁîüÊàêÔºà500Á≤íÂ≠êÔºâ
tem_sizes = lognorm.rvs(s=sigma, scale=np.exp(mu), size=500)

print(&quot;=&quot; * 60)
print(&quot;TEMÊ∏¨ÂÆö„Éá„Éº„Çø„ÅÆÁîüÊàêÔºàÂØæÊï∞Ê≠£Ë¶èÂàÜÂ∏ÉÔºâ&quot;)
print(&quot;=&quot; * 60)
print(f&quot;„Çµ„É≥„Éó„É´Êï∞: {len(tem_sizes)}Á≤íÂ≠ê&quot;)
print(f&quot;Âπ≥Âùá„Çµ„Ç§„Ç∫: {tem_sizes.mean():.2f} nm&quot;)
print(f&quot;Ê®ôÊ∫ñÂÅèÂ∑Æ: {tem_sizes.std():.2f} nm&quot;)
print(f&quot;‰∏≠Â§ÆÂÄ§: {np.median(tem_sizes):.2f} nm&quot;)
print(f&quot;ÊúÄÂ∞èÂÄ§: {tem_sizes.min():.2f} nm&quot;)
print(f&quot;ÊúÄÂ§ßÂÄ§: {tem_sizes.max():.2f} nm&quot;)

# „Éí„Çπ„Éà„Ç∞„É©„É†
fig, ax = plt.subplots(figsize=(10, 6))
ax.hist(tem_sizes, bins=40, alpha=0.7, color='lightblue',
        edgecolor='black', density=True, label='TEM Data')
ax.set_xlabel('Particle Size (nm)', fontsize=12)
ax.set_ylabel('Probability Density', fontsize=12)
ax.set_title('TEM Size Distribution (Lognormal)', fontsize=13, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()
</code></pre>
<h3>„Äê‰æã29„ÄëÂØæÊï∞Ê≠£Ë¶èÂàÜÂ∏É„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞</h3>
<pre><code class="language-python"># ÂØæÊï∞Ê≠£Ë¶èÂàÜÂ∏É„ÅÆ„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞
shape_fit, loc_fit, scale_fit = lognorm.fit(tem_sizes, floc=0)

# „Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞„Åï„Çå„ÅüÂàÜÂ∏É„ÅÆ„Éë„É©„É°„Éº„Çø
fitted_mean = np.exp(np.log(scale_fit) + 0.5 * shape_fit**2)
fitted_std = fitted_mean * np.sqrt(np.exp(shape_fit**2) - 1)

print(&quot;=&quot; * 60)
print(&quot;ÂØæÊï∞Ê≠£Ë¶èÂàÜÂ∏É„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞ÁµêÊûú&quot;)
print(&quot;=&quot; * 60)
print(f&quot;ÂΩ¢Áä∂„Éë„É©„É°„Éº„Çø (sigma): {shape_fit:.4f}&quot;)
print(f&quot;„Çπ„Ç±„Éº„É´„Éë„É©„É°„Éº„Çø: {scale_fit:.4f}&quot;)
print(f&quot;„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞„Åï„Çå„ÅüÂπ≥Âùá„Çµ„Ç§„Ç∫: {fitted_mean:.2f} nm&quot;)
print(f&quot;„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞„Åï„Çå„ÅüÊ®ôÊ∫ñÂÅèÂ∑Æ: {fitted_std:.2f} nm&quot;)

# ÂÆüÊ∏¨ÂÄ§„Å®„ÅÆÊØîËºÉ
print(f&quot;\nÂÆüÊ∏¨ÂÄ§„Å®„ÅÆÊØîËºÉ:&quot;)
print(f&quot;  Âπ≥Âùá„Çµ„Ç§„Ç∫ - ÂÆüÊ∏¨: {tem_sizes.mean():.2f} nm, „Éï„Ç£„ÉÉ„Éà: {fitted_mean:.2f} nm&quot;)
print(f&quot;  Ê®ôÊ∫ñÂÅèÂ∑Æ - ÂÆüÊ∏¨: {tem_sizes.std():.2f} nm, „Éï„Ç£„ÉÉ„Éà: {fitted_std:.2f} nm&quot;)
</code></pre>
<h3>„Äê‰æã30„Äë„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞ÁµêÊûú„ÅÆÂèØË¶ñÂåñ</h3>
<pre><code class="language-python"># „Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞ÁµêÊûú„ÅÆË©≥Á¥∞ÂèØË¶ñÂåñ
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# „Éí„Çπ„Éà„Ç∞„É©„É†„Å®„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞Êõ≤Á∑ö
axes[0].hist(tem_sizes, bins=40, alpha=0.6, color='lightblue',
             edgecolor='black', density=True, label='TEM Data')

# „Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞„Åï„Çå„ÅüÂØæÊï∞Ê≠£Ë¶èÂàÜÂ∏É
x_range = np.linspace(0, tem_sizes.max(), 200)
fitted_pdf = lognorm.pdf(x_range, shape_fit, loc=loc_fit, scale=scale_fit)
axes[0].plot(x_range, fitted_pdf, 'r-', linewidth=2,
             label=f'Lognormal Fit (Œº={fitted_mean:.1f}, œÉ={fitted_std:.1f})')

axes[0].set_xlabel('Particle Size (nm)', fontsize=12)
axes[0].set_ylabel('Probability Density', fontsize=12)
axes[0].set_title('TEM Size Distribution with Lognormal Fit',
                  fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3, axis='y')

# Q-Q„Éó„É≠„ÉÉ„ÉàÔºàÂàÜ‰ΩçÁÇπ„Éó„É≠„ÉÉ„ÉàÔºâ
from scipy.stats import probplot

probplot(tem_sizes, dist=lognorm, sparams=(shape_fit, loc_fit, scale_fit),
         plot=axes[1])
axes[1].set_title('Q-Q Plot: Lognormal Distribution',
                  fontsize=13, fontweight='bold')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;\nQ-Q„Éó„É≠„ÉÉ„Éà: „Éá„Éº„Çø„ÅåÁõ¥Á∑ö‰∏ä„Å´„ÅÇ„Çå„Å∞„ÄÅÂØæÊï∞Ê≠£Ë¶èÂàÜÂ∏É„Å´ËâØ„ÅèÂæì„Å£„Å¶„ÅÑ„Åæ„Åô&quot;)
</code></pre>
<hr />
<h2>3.10 ÂàÜÂ≠êÂãïÂäõÂ≠¶ÔºàMDÔºâ„Éá„Éº„ÇøËß£Êûê</h2>
<h3>„Äê‰æã31„ÄëMD„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥„Éá„Éº„Çø„ÅÆË™≠„ÅøËæº„Åø</h3>
<p>ÂàÜÂ≠êÂãïÂäõÂ≠¶„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥„Åß„ÅØ„ÄÅ„Éä„ÉéÁ≤íÂ≠ê„ÅÆÂéüÂ≠êÈÖçÁΩÆ„ÅÆÊôÇÈñìÁô∫Â±ï„ÇíËøΩË∑°„Åó„Åæ„Åô„ÄÇ</p>
<pre><code class="language-python"># MD„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥„Éá„Éº„Çø„ÅÆÊ®°Êì¨ÁîüÊàê
# ÂÆüÈöõ„ÅÆMD„Éá„Éº„Çø„ÅØLAMMPS, GROMACSÁ≠â„Åã„ÇâÂèñÂæó

np.random.seed(300)

n_atoms = 100  # ÂéüÂ≠êÊï∞
n_steps = 1000  # „Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„ÉóÊï∞
dt = 0.001  # „Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„ÉóÔºàpsÔºâ

# ÂàùÊúü‰ΩçÁΩÆÔºànmÔºâ
positions_initial = np.random.uniform(-1, 1, (n_atoms, 3))

# ÊôÇÈñìÁô∫Â±ï„ÅÆÊ®°Êì¨Ôºà„É©„É≥„ÉÄ„É†„Ç¶„Ç©„Éº„ÇØÔºâ
positions = np.zeros((n_steps, n_atoms, 3))
positions[0] = positions_initial

for t in range(1, n_steps):
    # „É©„É≥„ÉÄ„É†„Å™Â§â‰Ωç
    displacement = np.random.normal(0, 0.01, (n_atoms, 3))
    positions[t] = positions[t-1] + displacement

print(&quot;=&quot; * 60)
print(&quot;MD„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥„Éá„Éº„Çø„ÅÆÁîüÊàê&quot;)
print(&quot;=&quot; * 60)
print(f&quot;ÂéüÂ≠êÊï∞: {n_atoms}&quot;)
print(f&quot;„Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„ÉóÊï∞: {n_steps}&quot;)
print(f&quot;„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥ÊôÇÈñì: {n_steps * dt:.2f} ps&quot;)
print(f&quot;„Éá„Éº„ÇøÂΩ¢Áä∂: {positions.shape} (time, atoms, xyz)&quot;)

# ‰∏≠ÂøÉÂéüÂ≠êÔºàÂéüÂ≠ê0Ôºâ„ÅÆËªåË∑°„Çí„Éó„É≠„ÉÉ„Éà
fig = plt.figure(figsize=(12, 5))

ax1 = fig.add_subplot(121, projection='3d')
ax1.plot(positions[:, 0, 0], positions[:, 0, 1], positions[:, 0, 2],
         'b-', alpha=0.5, linewidth=0.5)
ax1.scatter(positions[0, 0, 0], positions[0, 0, 1], positions[0, 0, 2],
            c='green', s=100, label='Start', edgecolors='k')
ax1.scatter(positions[-1, 0, 0], positions[-1, 0, 1], positions[-1, 0, 2],
            c='red', s=100, label='End', edgecolors='k')
ax1.set_xlabel('X (nm)')
ax1.set_ylabel('Y (nm)')
ax1.set_zlabel('Z (nm)')
ax1.set_title('Atom Trajectory (Atom 0)', fontweight='bold')
ax1.legend()

ax2 = fig.add_subplot(122)
ax2.plot(np.arange(n_steps) * dt, positions[:, 0, 0], label='X')
ax2.plot(np.arange(n_steps) * dt, positions[:, 0, 1], label='Y')
ax2.plot(np.arange(n_steps) * dt, positions[:, 0, 2], label='Z')
ax2.set_xlabel('Time (ps)', fontsize=11)
ax2.set_ylabel('Position (nm)', fontsize=11)
ax2.set_title('Position vs Time (Atom 0)', fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>„Äê‰æã32„ÄëÂãïÂæÑÂàÜÂ∏ÉÈñ¢Êï∞ÔºàRDFÔºâ„ÅÆË®àÁÆó</h3>
<p>ÂãïÂæÑÂàÜÂ∏ÉÈñ¢Êï∞ÔºàRadial Distribution Function, RDFÔºâ„ÅØ„ÄÅÂéüÂ≠êÈñìË∑ùÈõ¢„ÅÆÂàÜÂ∏É„ÇíË°®„Åó„Åæ„Åô„ÄÇ</p>
<pre><code class="language-python"># ÂãïÂæÑÂàÜÂ∏ÉÈñ¢Êï∞ÔºàRDFÔºâ„ÅÆË®àÁÆó
def calculate_rdf(positions, r_max=2.0, n_bins=100):
    &quot;&quot;&quot;
    ÂãïÂæÑÂàÜÂ∏ÉÈñ¢Êï∞„ÇíË®àÁÆó

    Parameters:
    -----------
    positions : ndarray
        ÂéüÂ≠ê‰ΩçÁΩÆ (n_atoms, 3)
    r_max : float
        ÊúÄÂ§ßË∑ùÈõ¢ÔºànmÔºâ
    n_bins : int
        „Éì„É≥Êï∞

    Returns:
    --------
    r_bins : ndarray
        Ë∑ùÈõ¢„Éì„É≥
    rdf : ndarray
        ÂãïÂæÑÂàÜÂ∏ÉÈñ¢Êï∞
    &quot;&quot;&quot;
    n_atoms = positions.shape[0]

    # ÂÖ®ÂéüÂ≠ê„Éö„Ç¢Èñì„ÅÆË∑ùÈõ¢„ÇíË®àÁÆó
    distances = []
    for i in range(n_atoms):
        for j in range(i+1, n_atoms):
            dist = np.linalg.norm(positions[i] - positions[j])
            if dist &lt; r_max:
                distances.append(dist)

    distances = np.array(distances)

    # „Éí„Çπ„Éà„Ç∞„É©„É†
    hist, bin_edges = np.histogram(distances, bins=n_bins, range=(0, r_max))
    r_bins = (bin_edges[:-1] + bin_edges[1:]) / 2

    # Ë¶èÊ†ºÂåñÔºàÁêÜÊÉ≥Ê∞ó‰Ωì„Å®„ÅÆÊØîÔºâ
    dr = r_max / n_bins
    volume_shell = 4 * np.pi * r_bins**2 * dr
    n_ideal = volume_shell * (n_atoms / (4/3 * np.pi * r_max**3))

    rdf = hist / n_ideal / (n_atoms / 2)

    return r_bins, rdf

# ÊúÄÁµÇ„Éï„É¨„Éº„É†„ÅßRDF„ÇíË®àÁÆó
final_positions = positions[-1]
r_bins, rdf = calculate_rdf(final_positions, r_max=1.5, n_bins=150)

print(&quot;=&quot; * 60)
print(&quot;ÂãïÂæÑÂàÜÂ∏ÉÈñ¢Êï∞ÔºàRDFÔºâ&quot;)
print(&quot;=&quot; * 60)
print(f&quot;Ë®àÁÆóÂÆå‰∫Ü: {len(r_bins)}ÂÄã„ÅÆ„Éì„É≥&quot;)

# RDF„ÅÆ„Éó„É≠„ÉÉ„Éà
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(r_bins, rdf, 'b-', linewidth=2)
ax.axhline(y=1, color='r', linestyle='--', linewidth=1, label='Ideal Gas (g(r)=1)')
ax.set_xlabel('Distance r (nm)', fontsize=12)
ax.set_ylabel('g(r)', fontsize=12)
ax.set_title('Radial Distribution Function (RDF)', fontsize=13, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)
ax.set_ylim(0, max(rdf) * 1.1)

plt.tight_layout()
plt.show()

# „Éî„Éº„ÇØ‰ΩçÁΩÆ„ÅÆÊ§úÂá∫
from scipy.signal import find_peaks

peaks, _ = find_peaks(rdf, height=1.2, distance=10)
print(f&quot;\nRDF„ÅÆ„Éî„Éº„ÇØ‰ΩçÁΩÆÔºàÁâπÂæ¥ÁöÑ„Å™ÂéüÂ≠êÈñìË∑ùÈõ¢Ôºâ:&quot;)
for i, peak_idx in enumerate(peaks[:3], 1):
    print(f&quot;  „Éî„Éº„ÇØ{i}: r = {r_bins[peak_idx]:.3f} nm, g(r) = {rdf[peak_idx]:.2f}&quot;)
</code></pre>
<h3>„Äê‰æã33„ÄëÊã°Êï£‰øÇÊï∞„ÅÆË®àÁÆóÔºàMean Squared DisplacementÔºâ</h3>
<pre><code class="language-python"># Âπ≥Âùá‰∫å‰πóÂ§â‰ΩçÔºàMSDÔºâ„ÅÆË®àÁÆó
def calculate_msd(positions):
    &quot;&quot;&quot;
    Âπ≥Âùá‰∫å‰πóÂ§â‰Ωç„ÇíË®àÁÆó

    Parameters:
    -----------
    positions : ndarray
        ÂéüÂ≠ê‰ΩçÁΩÆ (n_steps, n_atoms, 3)

    Returns:
    --------
    msd : ndarray
        Âπ≥Âùá‰∫å‰πóÂ§â‰Ωç (n_steps,)
    &quot;&quot;&quot;
    n_steps, n_atoms, _ = positions.shape
    msd = np.zeros(n_steps)

    # ÂêÑ„Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„Éó„Åß„ÅÆMSD
    for t in range(n_steps):
        displacement = positions[t] - positions[0]
        squared_displacement = np.sum(displacement**2, axis=1)
        msd[t] = np.mean(squared_displacement)

    return msd

# MSD„ÅÆË®àÁÆó
msd = calculate_msd(positions)
time = np.arange(n_steps) * dt

print(&quot;=&quot; * 60)
print(&quot;Âπ≥Âùá‰∫å‰πóÂ§â‰ΩçÔºàMSDÔºâ„Å®Êã°Êï£‰øÇÊï∞&quot;)
print(&quot;=&quot; * 60)

# Êã°Êï£‰øÇÊï∞„ÅÆË®àÁÆóÔºàEinsteinÈñ¢‰øÇÂºè: MSD = 6*D*tÔºâ
# Á∑öÂΩ¢„Éï„Ç£„ÉÉ„ÉàÔºàÂæåÂçä50%„ÅÆ„Éá„Éº„Çø„Çí‰ΩøÁî®Ôºâ
start_idx = n_steps // 2
fit_coeffs = np.polyfit(time[start_idx:], msd[start_idx:], 1)
slope = fit_coeffs[0]
diffusion_coefficient = slope / 6

print(f&quot;Êã°Êï£‰øÇÊï∞ D = {diffusion_coefficient:.6f} nm¬≤/ps&quot;)
print(f&quot;            = {diffusion_coefficient * 1e3:.6f} √ó 10‚Åª‚Å∂ cm¬≤/s&quot;)

# MSD„Éó„É≠„ÉÉ„Éà
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(time, msd, 'b-', linewidth=2, label='MSD')
ax.plot(time[start_idx:], fit_coeffs[0] * time[start_idx:] + fit_coeffs[1],
        'r--', linewidth=2, label=f'Linear Fit (D={diffusion_coefficient:.4f} nm¬≤/ps)')
ax.set_xlabel('Time (ps)', fontsize=12)
ax.set_ylabel('MSD (nm¬≤)', fontsize=12)
ax.set_title('Mean Squared Displacement (MSD)', fontsize=13, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;\nÊã°Êï£‰øÇÊï∞„ÅØ„ÄÅ„Éä„ÉéÁ≤íÂ≠ê„ÅÆÁßªÂãïÊÄß„ÇíÂÆöÈáèÁöÑ„Å´Ë©ï‰æ°„Åô„ÇãÈáçË¶Å„Å™ÊåáÊ®ô„Åß„Åô&quot;)
</code></pre>
<hr />
<h2>3.11 Áï∞Â∏∏Ê§úÁü•ÔºöÂìÅË≥™ÁÆ°ÁêÜ„Å∏„ÅÆÂøúÁî®</h2>
<h3>„Äê‰æã34„ÄëIsolation Forest„Å´„Çà„ÇãÁï∞Â∏∏„Éä„ÉéÁ≤íÂ≠êÊ§úÂá∫</h3>
<p>Ë£ΩÈÄ†„Éó„É≠„Çª„Çπ„ÅßÁîüÊàê„Åï„Çå„Åü„Éä„ÉéÁ≤íÂ≠ê„ÅÆÂìÅË≥™ÁÆ°ÁêÜ„Å´„ÄÅÊ©üÊ¢∞Â≠¶Áøí„Å´„Çà„ÇãÁï∞Â∏∏Ê§úÁü•„ÇíÈÅ©Áî®„Åó„Åæ„Åô„ÄÇ</p>
<pre><code class="language-python">from sklearn.ensemble import IsolationForest

# Ê≠£Â∏∏„Éá„Éº„Çø„Å®Áï∞Â∏∏„Éá„Éº„Çø„ÇíÊ∑∑Âú®„Åï„Åõ„Çã
np.random.seed(400)

# Ê≠£Â∏∏„Å™Èáë„Éä„ÉéÁ≤íÂ≠ê„Éá„Éº„ÇøÔºà180„Çµ„É≥„Éó„É´Ôºâ
normal_size = np.random.normal(15, 3, 180)
normal_lspr = 520 + 0.8 * (normal_size - 15) + np.random.normal(0, 3, 180)

# Áï∞Â∏∏„Å™„Éä„ÉéÁ≤íÂ≠ê„Éá„Éº„ÇøÔºà20„Çµ„É≥„Éó„É´ÔºâÔºö„Çµ„Ç§„Ç∫„ÅåÁï∞Â∏∏„Å´Â§ß„Åç„ÅÑorÂ∞è„Åï„ÅÑ
anomaly_size = np.concatenate([
    np.random.uniform(5, 8, 10),  # Áï∞Â∏∏„Å´Â∞è„Åï„ÅÑ
    np.random.uniform(35, 50, 10)  # Áï∞Â∏∏„Å´Â§ß„Åç„ÅÑ
])
anomaly_lspr = 520 + 0.8 * (anomaly_size - 15) + np.random.normal(0, 8, 20)

# ÂÖ®„Éá„Éº„Çø„ÇíÁµêÂêà
all_size = np.concatenate([normal_size, anomaly_size])
all_lspr = np.concatenate([normal_lspr, anomaly_lspr])
all_data = np.column_stack([all_size, all_lspr])

# „É©„Éô„É´ÔºàÊ≠£Â∏∏=0„ÄÅÁï∞Â∏∏=1Ôºâ
true_labels = np.concatenate([np.zeros(180), np.ones(20)])

print(&quot;=&quot; * 60)
print(&quot;Áï∞Â∏∏Ê§úÁü•ÔºàIsolation ForestÔºâ&quot;)
print(&quot;=&quot; * 60)
print(f&quot;ÂÖ®„Éá„Éº„ÇøÊï∞: {len(all_data)}&quot;)
print(f&quot;Ê≠£Â∏∏„Éá„Éº„Çø: {int((true_labels == 0).sum())}„Çµ„É≥„Éó„É´&quot;)
print(f&quot;Áï∞Â∏∏„Éá„Éº„Çø: {int((true_labels == 1).sum())}„Çµ„É≥„Éó„É´&quot;)

# Isolation Forest„É¢„Éá„É´
iso_forest = IsolationForest(
    contamination=0.1,  # Áï∞Â∏∏„Éá„Éº„Çø„ÅÆÂâ≤ÂêàÔºà10%„Å®‰ªÆÂÆöÔºâ
    random_state=42,
    n_estimators=100
)

# Áï∞Â∏∏Ê§úÁü•
predictions = iso_forest.fit_predict(all_data)
anomaly_scores = iso_forest.score_samples(all_data)

# ‰∫àÊ∏¨ÁµêÊûúÔºà1: Ê≠£Â∏∏„ÄÅ-1: Áï∞Â∏∏Ôºâ
predicted_anomalies = (predictions == -1)
true_anomalies = (true_labels == 1)

# Ë©ï‰æ°ÊåáÊ®ô
from sklearn.metrics import confusion_matrix, classification_report

# ‰∫àÊ∏¨„Çí0/1„Å´Â§âÊèõ
predicted_labels = (predictions == -1).astype(int)

print(&quot;\nÊ∑∑ÂêåË°åÂàó:&quot;)
cm = confusion_matrix(true_labels, predicted_labels)
print(cm)

print(&quot;\nÂàÜÈ°û„É¨„Éù„Éº„Éà:&quot;)
print(classification_report(true_labels, predicted_labels,
                            target_names=['Normal', 'Anomaly']))

# Ê§úÂá∫Áéá
detected_anomalies = np.sum(predicted_anomalies &amp; true_anomalies)
total_anomalies = np.sum(true_anomalies)
detection_rate = detected_anomalies / total_anomalies * 100

print(f&quot;\nÁï∞Â∏∏Ê§úÂá∫Áéá: {detection_rate:.1f}% ({detected_anomalies}/{total_anomalies})&quot;)
</code></pre>
<h3>„Äê‰æã35„ÄëÁï∞Â∏∏„Çµ„É≥„Éó„É´„ÅÆÂèØË¶ñÂåñ</h3>
<pre><code class="language-python"># Áï∞Â∏∏Ê§úÁü•ÁµêÊûú„ÅÆÂèØË¶ñÂåñ
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Êï£Â∏ÉÂõ≥ÔºàÁúü„ÅÆ„É©„Éô„É´Ôºâ
axes[0].scatter(all_size[true_labels == 0], all_lspr[true_labels == 0],
                c='blue', s=60, alpha=0.6, label='Normal', edgecolors='k')
axes[0].scatter(all_size[true_labels == 1], all_lspr[true_labels == 1],
                c='red', s=100, alpha=0.8, marker='^', label='True Anomaly',
                edgecolors='k', linewidths=2)
axes[0].set_xlabel('Size (nm)', fontsize=12)
axes[0].set_ylabel('LSPR Wavelength (nm)', fontsize=12)
axes[0].set_title('True Labels', fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Êï£Â∏ÉÂõ≥Ôºà‰∫àÊ∏¨ÁµêÊûúÔºâ
normal_mask = ~predicted_anomalies
anomaly_mask = predicted_anomalies

axes[1].scatter(all_size[normal_mask], all_lspr[normal_mask],
                c='blue', s=60, alpha=0.6, label='Predicted Normal', edgecolors='k')
axes[1].scatter(all_size[anomaly_mask], all_lspr[anomaly_mask],
                c='orange', s=100, alpha=0.8, marker='X', label='Predicted Anomaly',
                edgecolors='k', linewidths=2)

# Ê≠£„Åó„ÅèÊ§úÂá∫„Åï„Çå„ÅüÁï∞Â∏∏„ÇíÂº∑Ë™ø
correctly_detected = predicted_anomalies &amp; true_anomalies
axes[1].scatter(all_size[correctly_detected], all_lspr[correctly_detected],
                c='red', s=150, marker='*', label='Correctly Detected',
                edgecolors='black', linewidths=1.5, zorder=5)

axes[1].set_xlabel('Size (nm)', fontsize=12)
axes[1].set_ylabel('LSPR Wavelength (nm)', fontsize=12)
axes[1].set_title('Isolation Forest Predictions', fontsize=13, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Áï∞Â∏∏„Çπ„Ç≥„Ç¢„ÅÆÂàÜÂ∏É
fig, ax = plt.subplots(figsize=(10, 6))
ax.hist(anomaly_scores[true_labels == 0], bins=30, alpha=0.6,
        color='blue', label='Normal', edgecolor='black')
ax.hist(anomaly_scores[true_labels == 1], bins=30, alpha=0.6,
        color='red', label='Anomaly', edgecolor='black')
ax.set_xlabel('Anomaly Score', fontsize=12)
ax.set_ylabel('Frequency', fontsize=12)
ax.set_title('Anomaly Score Distribution', fontsize=13, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print(&quot;\nÁï∞Â∏∏„Çπ„Ç≥„Ç¢„Åå‰Ωé„ÅÑÔºàË≤†„ÅÆÂÄ§„ÅåÂ§ß„Åç„ÅÑÔºâ„Åª„Å©„ÄÅÁï∞Â∏∏„Åß„ÅÇ„ÇãÂèØËÉΩÊÄß„ÅåÈ´ò„ÅÑ&quot;)
</code></pre>
<hr />
<h2>„Åæ„Å®„ÇÅ</h2>
<p>Êú¨Á´†„Åß„ÅØ„ÄÅPython„Çí‰Ωø„Å£„Åü„Éä„ÉéÊùêÊñô„Éá„Éº„ÇøËß£Êûê„Å®Ê©üÊ¢∞Â≠¶Áøí„ÅÆÂÆüË∑µÁöÑÊâãÊ≥ï„Çí35ÂÄã„ÅÆ„Ç≥„Éº„Éâ‰æã„ÅßÂ≠¶„Å≥„Åæ„Åó„Åü„ÄÇ</p>
<h3>ÁøíÂæó„Åó„Åü‰∏ªË¶ÅÊäÄË°ì</h3>
<ol>
<li>
<p><strong>„Éá„Éº„ÇøÁîüÊàê„Å®ÂèØË¶ñÂåñ</strong>Ôºà‰æã1-5Ôºâ
   - Èáë„Éä„ÉéÁ≤íÂ≠ê„ÄÅÈáèÂ≠ê„Éâ„ÉÉ„Éà„ÅÆÂêàÊàê„Éá„Éº„ÇøÁîüÊàê
   - „Éí„Çπ„Éà„Ç∞„É©„É†„ÄÅÊï£Â∏ÉÂõ≥„ÄÅ3D„Éó„É≠„ÉÉ„Éà„ÄÅÁõ∏Èñ¢ÂàÜÊûê</p>
</li>
<li>
<p><strong>„Éá„Éº„ÇøÂâçÂá¶ÁêÜ</strong>Ôºà‰æã6-9Ôºâ
   - Ê¨†ÊêçÂÄ§Âá¶ÁêÜ„ÄÅÂ§ñ„ÇåÂÄ§Ê§úÂá∫„ÄÅ„Çπ„Ç±„Éº„É™„É≥„Ç∞„ÄÅ„Éá„Éº„ÇøÂàÜÂâ≤</p>
</li>
<li>
<p><strong>ÂõûÂ∏∞„É¢„Éá„É´„Å´„Çà„ÇãÁâ©ÊÄß‰∫àÊ∏¨</strong>Ôºà‰æã10-15Ôºâ
   - Á∑öÂΩ¢ÂõûÂ∏∞„ÄÅ„É©„É≥„ÉÄ„É†„Éï„Ç©„É¨„Çπ„Éà„ÄÅLightGBM„ÄÅSVR„ÄÅMLP
   - „É¢„Éá„É´ÊÄßËÉΩÊØîËºÉÔºàR¬≤„ÄÅRMSE„ÄÅMAEÔºâ</p>
</li>
<li>
<p><strong>ÈáèÂ≠ê„Éâ„ÉÉ„ÉàÁô∫ÂÖâ‰∫àÊ∏¨</strong>Ôºà‰æã16-18Ôºâ
   - BrusÊñπÁ®ãÂºè„Å´Âü∫„Å•„Åè„Éá„Éº„ÇøÁîüÊàê
   - LightGBM„Å´„Çà„Çã‰∫àÊ∏¨„É¢„Éá„É´ÊßãÁØâ</p>
</li>
<li>
<p><strong>ÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶„Å®„É¢„Éá„É´Ëß£Èáà</strong>Ôºà‰æã19-20Ôºâ
   - LightGBMÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶
   - SHAPÂàÜÊûê„Å´„Çà„Çã‰∫àÊ∏¨„ÅÆËß£Èáà</p>
</li>
<li>
<p><strong>„Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ</strong>Ôºà‰æã21-25Ôºâ
   - ÁõÆÊ®ôLSPRÊ≥¢Èï∑„ÇíÂÆüÁèæ„Åô„ÇãÊúÄÈÅ©ÂêàÊàêÊù°‰ª∂„ÅÆÊé¢Á¥¢
   - ÂèéÊùü„Éó„É≠„ÉÉ„Éà„ÄÅÊúÄÈÅ©Âåñ„Éó„É≠„Çª„Çπ„ÅÆÂèØË¶ñÂåñ</p>
</li>
<li>
<p><strong>Â§öÁõÆÁöÑÊúÄÈÅ©Âåñ</strong>Ôºà‰æã26-27Ôºâ
   - NSGA-II„Å´„Çà„Çã„Éë„É¨„Éº„ÉàÊúÄÈÅ©Âåñ
   - „Çµ„Ç§„Ç∫„Å®Áô∫ÂÖâÂäπÁéá„ÅÆ„Éà„É¨„Éº„Éâ„Ç™„ÉïÂàÜÊûê</p>
</li>
<li>
<p><strong>TEMÁîªÂÉèËß£Êûê</strong>Ôºà‰æã28-30Ôºâ
   - ÂØæÊï∞Ê≠£Ë¶èÂàÜÂ∏É„Å´„Çà„Çã„Çµ„Ç§„Ç∫ÂàÜÂ∏É„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞
   - Q-Q„Éó„É≠„ÉÉ„Éà„Å´„Çà„ÇãÂàÜÂ∏É„ÅÆÊ§úË®º</p>
</li>
<li>
<p><strong>ÂàÜÂ≠êÂãïÂäõÂ≠¶„Éá„Éº„ÇøËß£Êûê</strong>Ôºà‰æã31-33Ôºâ
   - ÂéüÂ≠êËªåË∑°„ÅÆÂèØË¶ñÂåñ
   - ÂãïÂæÑÂàÜÂ∏ÉÈñ¢Êï∞ÔºàRDFÔºâ„ÅÆË®àÁÆó
   - Êã°Êï£‰øÇÊï∞„ÅÆÁÆóÂá∫ÔºàMSDÊ≥ïÔºâ</p>
</li>
<li>
<p><strong>Áï∞Â∏∏Ê§úÁü•</strong>Ôºà‰æã34-35Ôºâ</p>
<ul>
<li>Isolation Forest„Å´„Çà„ÇãÂìÅË≥™ÁÆ°ÁêÜ</li>
<li>Áï∞Â∏∏„Éä„ÉéÁ≤íÂ≠ê„ÅÆËá™ÂãïÊ§úÂá∫</li>
</ul>
</li>
</ol>
<h3>ÂÆüË∑µÁöÑ„Å™ÂøúÁî®</h3>
<p>„Åì„Çå„Çâ„ÅÆÊäÄË°ì„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å™ÂÆüÈöõ„ÅÆ„Éä„ÉéÊùêÊñôÁ†îÁ©∂„Å´Áõ¥Êé•ÂøúÁî®„Åß„Åç„Åæ„ÅôÔºö</p>
<ul>
<li><strong>ÊùêÊñôË®≠Ë®à</strong>: Ê©üÊ¢∞Â≠¶Áøí„Å´„Çà„ÇãÁâ©ÊÄß‰∫àÊ∏¨„Å®ÊúÄÈÅ©Âåñ„Å´„Çà„ÇãÈ´òÂäπÁéáÊùêÊñôÊé¢Á¥¢</li>
<li><strong>„Éó„É≠„Çª„ÇπÊúÄÈÅ©Âåñ</strong>: „Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„Å´„Çà„ÇãÂÆüÈ®ìÂõûÊï∞ÂâäÊ∏õ„Å®ÊúÄÈÅ©ÂêàÊàêÊù°‰ª∂Áô∫Ë¶ã</li>
<li><strong>ÂìÅË≥™ÁÆ°ÁêÜ</strong>: Áï∞Â∏∏Ê§úÁü•„Å´„Çà„Çã‰∏çËâØÂìÅ„ÅÆÊó©ÊúüÁô∫Ë¶ã„Å®Ê≠©Áïô„Åæ„ÇäÂêë‰∏ä</li>
<li><strong>„Éá„Éº„ÇøËß£Êûê</strong>: TEM„Éá„Éº„Çø„ÄÅMD„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥„Éá„Éº„Çø„ÅÆÂÆöÈáèÁöÑËß£Êûê</li>
<li><strong>„É¢„Éá„É´Ëß£Èáà</strong>: SHAPÂàÜÊûê„Å´„Çà„Çã‰∫àÊ∏¨Ê†πÊã†„ÅÆÂèØË¶ñÂåñ„Å®‰ø°È†ºÊÄßÂêë‰∏ä</li>
</ul>
<h3>Ê¨°Á´†„ÅÆ‰∫àÂëä</h3>
<p>Chapter 4„Åß„ÅØ„ÄÅ„Åì„Çå„Çâ„ÅÆÊäÄË°ì„ÇíÂÆüÈöõ„ÅÆ„Éä„ÉéÊùêÊñôÁ†îÁ©∂„Éó„É≠„Ç∏„Çß„ÇØ„Éà„Å´ÈÅ©Áî®„Åó„Åü5„Å§„ÅÆË©≥Á¥∞„Å™„Ç±„Éº„Çπ„Çπ„Çø„Éá„Ç£„ÇíÂ≠¶„Å≥„Åæ„Åô„ÄÇ„Ç´„Éº„Éú„É≥„Éä„Éé„ÉÅ„É•„Éº„ÉñË§áÂêàÊùêÊñô„ÄÅÈáèÂ≠ê„Éâ„ÉÉ„Éà„ÄÅÈáë„Éä„ÉéÁ≤íÂ≠êËß¶Â™í„ÄÅ„Ç∞„É©„Éï„Çß„É≥„ÄÅ„Éä„ÉéÂåªËñ¨„ÅÆÂÆüÁî®Âåñ‰∫ã‰æã„ÇíÈÄö„Åò„Å¶„ÄÅÂïèÈ°åËß£Ê±∫„ÅÆÂÖ®‰ΩìÂÉè„ÇíÁêÜËß£„Åó„Åæ„Åô„ÄÇ</p>
<hr />
<h2>ÊºîÁøíÂïèÈ°å</h2>
<h3>ÊºîÁøí1: „Ç´„Éº„Éú„É≥„Éä„Éé„ÉÅ„É•„Éº„Éñ„ÅÆÈõªÊ∞ó‰ºùÂ∞éÂ∫¶‰∫àÊ∏¨</h3>
<p>„Ç´„Éº„Éú„É≥„Éä„Éé„ÉÅ„É•„Éº„ÉñÔºàCNTÔºâ„ÅÆÈõªÊ∞ó‰ºùÂ∞éÂ∫¶„ÅØ„ÄÅÁõ¥ÂæÑ„ÄÅ„Ç´„Ç§„É©„É™„ÉÜ„Ç£„ÄÅÈï∑„Åï„Å´‰æùÂ≠ò„Åó„Åæ„Åô„ÄÇ‰ª•‰∏ã„ÅÆ„Éá„Éº„Çø„ÇíÁîüÊàê„Åó„ÄÅLightGBM„É¢„Éá„É´„Åß‰∫àÊ∏¨„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<p><strong>„Éá„Éº„Çø‰ªïÊßò</strong>Ôºö
- „Çµ„É≥„Éó„É´Êï∞Ôºö150
- ÁâπÂæ¥ÈáèÔºöÁõ¥ÂæÑÔºà1-3 nmÔºâ„ÄÅÈï∑„ÅïÔºà100-1000 nmÔºâ„ÄÅ„Ç´„Ç§„É©„É™„ÉÜ„Ç£ÊåáÊ®ôÔºà0-1„ÅÆÈÄ£Á∂öÂÄ§Ôºâ
- „Çø„Éº„Ç≤„ÉÉ„ÉàÔºöÈõªÊ∞ó‰ºùÂ∞éÂ∫¶Ôºà10¬≥-10‚Å∑ S/m„ÄÅÂØæÊï∞Ê≠£Ë¶èÂàÜÂ∏ÉÔºâ</p>
<p><strong>„Çø„Çπ„ÇØ</strong>Ôºö
1. „Éá„Éº„ÇøÁîüÊàê
2. Ë®ìÁ∑¥/„ÉÜ„Çπ„Éà„Éá„Éº„ÇøÂàÜÂâ≤
3. LightGBM„É¢„Éá„É´„ÅÆÊßãÁØâ„Å®Ë©ï‰æ°
4. ÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶„ÅÆÂèØË¶ñÂåñ</p>
<details>
<summary>Ëß£Á≠î‰æã</summary>


<pre><code class="language-python"># „Éá„Éº„ÇøÁîüÊàê
np.random.seed(500)
n_samples = 150

diameter = np.random.uniform(1, 3, n_samples)
length = np.random.uniform(100, 1000, n_samples)
chirality = np.random.uniform(0, 1, n_samples)

# ÈõªÊ∞ó‰ºùÂ∞éÂ∫¶ÔºàÁ∞°Êòì„É¢„Éá„É´: Áõ¥ÂæÑ„Å®„Ç´„Ç§„É©„É™„ÉÜ„Ç£„Å´Âº∑„Åè‰æùÂ≠òÔºâ
log_conductivity = 3 + 2*diameter + 3*chirality + 0.001*length + np.random.normal(0, 0.5, n_samples)
conductivity = 10 ** log_conductivity  # S/m

data_cnt = pd.DataFrame({
    'diameter_nm': diameter,
    'length_nm': length,
    'chirality': chirality,
    'conductivity_Sm': conductivity
})

# ÁâπÂæ¥Èáè„Å®„Çø„Éº„Ç≤„ÉÉ„Éà
X_cnt = data_cnt[['diameter_nm', 'length_nm', 'chirality']]
y_cnt = np.log10(data_cnt['conductivity_Sm'])  # ÂØæÊï∞Â§âÊèõ

# „Çπ„Ç±„Éº„É™„É≥„Ç∞
scaler_cnt = StandardScaler()
X_cnt_scaled = scaler_cnt.fit_transform(X_cnt)

# Ë®ìÁ∑¥/„ÉÜ„Çπ„ÉàÂàÜÂâ≤
X_cnt_train, X_cnt_test, y_cnt_train, y_cnt_test = train_test_split(
    X_cnt_scaled, y_cnt, test_size=0.2, random_state=42
)

# LightGBM„É¢„Éá„É´
model_cnt = lgb.LGBMRegressor(num_leaves=31, learning_rate=0.05, n_estimators=200, random_state=42, verbose=-1)
model_cnt.fit(X_cnt_train, y_cnt_train)

# ‰∫àÊ∏¨„Å®Ë©ï‰æ°
y_cnt_pred = model_cnt.predict(X_cnt_test)
r2_cnt = r2_score(y_cnt_test, y_cnt_pred)
rmse_cnt = np.sqrt(mean_squared_error(y_cnt_test, y_cnt_pred))

print(f&quot;R¬≤: {r2_cnt:.4f}&quot;)
print(f&quot;RMSE: {rmse_cnt:.4f}&quot;)

# ÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶
importance_cnt = pd.DataFrame({
    'Feature': X_cnt.columns,
    'Importance': model_cnt.feature_importances_
}).sort_values('Importance', ascending=False)

print(&quot;\nÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶:&quot;)
print(importance_cnt)

# ÂèØË¶ñÂåñ
fig, ax = plt.subplots(figsize=(8, 5))
ax.barh(importance_cnt['Feature'], importance_cnt['Importance'], color='steelblue', edgecolor='black')
ax.set_xlabel('Importance')
ax.set_title('Feature Importance: CNT Conductivity Prediction')
plt.tight_layout()
plt.show()
</code></pre>


</details>

<h3>ÊºîÁøí2: ÈäÄ„Éä„ÉéÁ≤íÂ≠ê„ÅÆÊúÄÈÅ©ÂêàÊàêÊù°‰ª∂Êé¢Á¥¢</h3>
<p>ÈäÄ„Éä„ÉéÁ≤íÂ≠ê„ÅÆÊäóËèåÊ¥ªÊÄß„ÅØ„ÄÅ„Çµ„Ç§„Ç∫„ÅåÂ∞è„Åï„ÅÑ„Åª„Å©È´ò„Åè„Å™„Çä„Åæ„Åô„ÄÇ„Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„ÇíÁî®„ÅÑ„Å¶„ÄÅÁõÆÊ®ô„Çµ„Ç§„Ç∫Ôºà10 nmÔºâ„ÇíÂÆüÁèæ„Åô„ÇãÊúÄÈÅ©„Å™ÂêàÊàêÊ∏©Â∫¶„Å®pH„ÇíÊé¢Á¥¢„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<p><strong>Êù°‰ª∂</strong>Ôºö
- Ê∏©Â∫¶ÁØÑÂõ≤Ôºö20-80¬∞C
- pHÁØÑÂõ≤Ôºö6-11
- ÁõÆÊ®ô„Çµ„Ç§„Ç∫Ôºö10 nm</p>
<details>
<summary>Ëß£Á≠î‰æã</summary>


<pre><code class="language-python"># ÈäÄ„Éä„ÉéÁ≤íÂ≠ê„Éá„Éº„Çø„ÅÆÁîüÊàê
np.random.seed(600)
n_ag = 100

temp_ag = np.random.uniform(20, 80, n_ag)
pH_ag = np.random.uniform(6, 11, n_ag)

# „Çµ„Ç§„Ç∫„É¢„Éá„É´ÔºàÊ∏©Â∫¶„ÅåÈ´ò„Åè„ÄÅpH„Åå‰Ωé„ÅÑ„Åª„Å©Â∞è„Åï„Åè„Å™„Çã„Å®‰ªÆÂÆöÔºâ
size_ag = 15 - 0.1*temp_ag - 0.8*pH_ag + np.random.normal(0, 1, n_ag)
size_ag = np.clip(size_ag, 5, 30)

data_ag = pd.DataFrame({
    'temperature': temp_ag,
    'pH': pH_ag,
    'size': size_ag
})

# „É¢„Éá„É´ÊßãÁØâÔºàLightGBMÔºâ
X_ag = data_ag[['temperature', 'pH']]
y_ag = data_ag['size']

scaler_ag = StandardScaler()
X_ag_scaled = scaler_ag.fit_transform(X_ag)

model_ag = lgb.LGBMRegressor(num_leaves=31, learning_rate=0.05, n_estimators=100, random_state=42, verbose=-1)
model_ag.fit(X_ag_scaled, y_ag)

# „Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ
from skopt import gp_minimize
from skopt.space import Real

space_ag = [
    Real(20, 80, name='temperature'),
    Real(6, 11, name='pH')
]

target_size = 10.0

def objective_ag(params):
    temp, ph = params
    features = scaler_ag.transform([[temp, ph]])
    predicted_size = model_ag.predict(features)[0]
    return abs(predicted_size - target_size)

result_ag = gp_minimize(objective_ag, space_ag, n_calls=40, random_state=42, verbose=False)

print(&quot;=&quot; * 60)
print(&quot;ÈäÄ„Éä„ÉéÁ≤íÂ≠ê„ÅÆÊúÄÈÅ©ÂêàÊàêÊù°‰ª∂&quot;)
print(&quot;=&quot; * 60)
print(f&quot;ÊúÄÂ∞èË™§Â∑Æ: {result_ag.fun:.2f} nm&quot;)
print(f&quot;ÊúÄÈÅ©Ê∏©Â∫¶: {result_ag.x[0]:.1f} ¬∞C&quot;)
print(f&quot;ÊúÄÈÅ©pH: {result_ag.x[1]:.2f}&quot;)

# ÊúÄÈÅ©Êù°‰ª∂„Åß„ÅÆ‰∫àÊ∏¨„Çµ„Ç§„Ç∫
optimal_features = scaler_ag.transform([result_ag.x])
predicted_size = model_ag.predict(optimal_features)[0]
print(f&quot;‰∫àÊ∏¨„Çµ„Ç§„Ç∫: {predicted_size:.2f} nm&quot;)
</code></pre>


</details>

<h3>ÊºîÁøí3: ÈáèÂ≠ê„Éâ„ÉÉ„Éà„ÅÆÂ§öËâ≤Áô∫ÂÖâË®≠Ë®à</h3>
<p>Ëµ§Ôºà650 nmÔºâ„ÄÅÁ∑ëÔºà550 nmÔºâ„ÄÅÈùíÔºà450 nmÔºâ„ÅÆ3Ëâ≤„ÅÆÁô∫ÂÖâ„ÇíÂÆüÁèæ„Åô„ÇãCdSeÈáèÂ≠ê„Éâ„ÉÉ„Éà„ÅÆ„Çµ„Ç§„Ç∫„Çí„ÄÅ„Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„ÅßË®≠Ë®à„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<p><strong>„Éí„É≥„Éà</strong>Ôºö
- ÂêÑËâ≤„Åî„Å®„Å´ÊúÄÈÅ©Âåñ„ÇíÂÆüË°å
- Áô∫ÂÖâÊ≥¢Èï∑„Å®„Çµ„Ç§„Ç∫„ÅÆÈñ¢‰øÇ„Çí‰ΩøÁî®</p>
<details>
<summary>Ëß£Á≠î‰æã</summary>


<pre><code class="language-python"># ÈáèÂ≠ê„Éâ„ÉÉ„Éà„Éá„Éº„ÇøÔºà‰æã16„ÅÆdata_qd„Çí‰ΩøÁî®Ôºâ
# model_qd „Å® scaler_qd „ÅåÊßãÁØâÊ∏à„Åø„Å®‰ªÆÂÆö

# 3Ëâ≤„ÅÆÁõÆÊ®ôÊ≥¢Èï∑
target_colors = {
    'Red': 650,
    'Green': 550,
    'Blue': 450
}

results_colors = {}

for color_name, target_emission in target_colors.items():
    # Êé¢Á¥¢Á©∫Èñì
    space_qd = [
        Real(2, 10, name='size_nm'),
        Real(10, 120, name='synthesis_time_min'),
        Real(0.5, 2.0, name='precursor_ratio')
    ]

    # ÁõÆÁöÑÈñ¢Êï∞
    def objective_qd(params):
        features = scaler_qd.transform([params])
        predicted_emission = model_qd.predict(features)[0]
        return abs(predicted_emission - target_emission)

    # ÊúÄÈÅ©Âåñ
    result_qd_color = gp_minimize(objective_qd, space_qd, n_calls=30, random_state=42, verbose=False)

    # ÁµêÊûú‰øùÂ≠ò
    optimal_features = scaler_qd.transform([result_qd_color.x])
    predicted_emission = model_qd.predict(optimal_features)[0]

    results_colors[color_name] = {
        'target': target_emission,
        'size': result_qd_color.x[0],
        'time': result_qd_color.x[1],
        'ratio': result_qd_color.x[2],
        'predicted': predicted_emission,
        'error': result_qd_color.fun
    }

# ÁµêÊûúË°®Á§∫
print(&quot;=&quot; * 80)
print(&quot;ÈáèÂ≠ê„Éâ„ÉÉ„ÉàÂ§öËâ≤Áô∫ÂÖâË®≠Ë®à&quot;)
print(&quot;=&quot; * 80)

results_df = pd.DataFrame(results_colors).T
print(results_df.to_string())

# ÂèØË¶ñÂåñ
fig, ax = plt.subplots(figsize=(10, 6))
colors_rgb = {'Red': 'red', 'Green': 'green', 'Blue': 'blue'}

for color_name, result in results_colors.items():
    ax.scatter(result['size'], result['predicted'],
               s=200, color=colors_rgb[color_name],
               edgecolors='black', linewidths=2, label=color_name)

ax.set_xlabel('Quantum Dot Size (nm)', fontsize=12)
ax.set_ylabel('Emission Wavelength (nm)', fontsize=12)
ax.set_title('Multi-Color Quantum Dot Design', fontsize=13, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>


</details>

<hr />
<h2>3.12 Á´†Êú´„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„ÉàÔºö„Éä„ÉéÊùêÊñô„Éá„Éº„ÇøËß£Êûê„Çπ„Ç≠„É´„ÅÆÂìÅË≥™‰øùË®º</h2>
<p>Êú¨Á´†„ÅßÂ≠¶„Çì„Å†Python„Å´„Çà„Çã„Éä„ÉéÊùêÊñô„Éá„Éº„ÇøËß£Êûê„Å®Ê©üÊ¢∞Â≠¶Áøí„ÅÆÂÆüË£Ö„Çπ„Ç≠„É´„Çí‰ΩìÁ≥ªÁöÑ„Å´„ÉÅ„Çß„ÉÉ„ÇØ„Åó„Åæ„Åô„ÄÇ</p>
<h3>3.12.1 Áí∞Â¢ÉÊßãÁØâ„Çπ„Ç≠„É´ÔºàEnvironment SetupÔºâ</h3>
<h4>Âü∫Á§é„É¨„Éô„É´</h4>
<ul>
<li>[ ] Python 3.9‰ª•‰∏ä„Åå„Ç§„É≥„Çπ„Éà„Éº„É´„Åï„Çå„Å¶„ÅÑ„Çã</li>
<li>[ ] 3„Å§„ÅÆÁí∞Â¢ÉÊßãÁØâ„Ç™„Éó„Ç∑„Éß„É≥ÔºàAnaconda/venv/ColabÔºâ„ÅÆÈÅï„ÅÑ„ÇíË™¨Êòé„Åß„Åç„Çã</li>
<li>[ ] Ëá™ÂàÜ„ÅÆÁä∂Ê≥Å„Å´ÊúÄÈÅ©„Å™Áí∞Â¢É„ÇíÈÅ∏Êäû„Åß„Åç„Çã</li>
<li>[ ] ‰ªÆÊÉ≥Áí∞Â¢É„Çí‰ΩúÊàê„ÉªÊúâÂäπÂåñ„ÉªÁÑ°ÂäπÂåñ„Åß„Åç„Çã</li>
<li>[ ] pip/conda„Åß„É©„Ç§„Éñ„É©„É™„Çí„Ç§„É≥„Çπ„Éà„Éº„É´„Åß„Åç„ÇãÔºàpandas„ÄÅnumpy„ÄÅmatplotlib„ÄÅscikit-learn„ÄÅlightgbmÔºâ</li>
<li>[ ] Áí∞Â¢ÉÊ§úË®º„Ç≥„Éº„Éâ„ÇíÂÆüË°å„Åó„ÄÅ„Ç®„É©„Éº„Å™„ÅèÂãï‰ΩúÁ¢∫Ë™ç„Åß„Åç„Çã</li>
</ul>
<h4>ÂøúÁî®„É¨„Éô„É´</h4>
<ul>
<li>[ ] requirements.txt„Çí‰ΩúÊàê„Éª‰ΩøÁî®„Åß„Åç„Çã</li>
<li>[ ] Google Colab„ÅßGoogle Drive„Çí„Éû„Ç¶„É≥„Éà„Åó„Å¶„Éá„Éº„Çø„ÇíË™≠„ÅøËæº„ÇÅ„Çã</li>
<li>[ ] Ë§áÊï∞„ÅÆ‰ªÆÊÉ≥Áí∞Â¢É„ÇíÁî®ÈÄîÂà•„Å´‰Ωø„ÅÑÂàÜ„Åë„Çâ„Çå„Çã</li>
<li>[ ] „Ç§„É≥„Çπ„Éà„Éº„É´„Ç®„É©„Éº„ÇíËá™Âäõ„Åß„Éà„É©„Éñ„É´„Ç∑„É•„Éº„ÉÜ„Ç£„É≥„Ç∞„Åß„Åç„Çã</li>
<li>[ ] „Ç™„Éó„Ç∑„Éß„É≥„É©„Ç§„Éñ„É©„É™Ôºàpymoo„ÄÅSHAPÔºâ„ÇíÂøÖË¶Å„Å´Âøú„Åò„Å¶„Ç§„É≥„Çπ„Éà„Éº„É´„Åß„Åç„Çã</li>
</ul>
<hr />
<h3>3.12.2 „Éá„Éº„ÇøÂá¶ÁêÜ„ÉªÂèØË¶ñÂåñ„Çπ„Ç≠„É´ÔºàData Processing &amp; VisualizationÔºâ</h3>
<h4>Âü∫Á§é„É¨„Éô„É´</h4>
<ul>
<li>[ ] NumPy„ÅßÂêàÊàê„Éá„Éº„Çø„ÇíÁîüÊàê„Åß„Åç„ÇãÔºàÊ≠£Ë¶èÂàÜÂ∏É„ÄÅ‰∏ÄÊßòÂàÜÂ∏ÉÔºâ</li>
<li>[ ] Pandas„Åß„Éá„Éº„Çø„Éï„É¨„Éº„É†„Çí‰ΩúÊàê„ÉªÊìç‰Ωú„Åß„Åç„Çã</li>
<li>[ ] Âü∫Êú¨Áµ±Ë®àÈáèÔºàmean„ÄÅstd„ÄÅmedianÔºâ„ÇíË®àÁÆó„Åß„Åç„Çã</li>
<li>[ ] „Éí„Çπ„Éà„Ç∞„É©„É†„Çí‰ΩúÊàê„Åß„Åç„Çã</li>
<li>[ ] Êï£Â∏ÉÂõ≥„Çí‰ΩúÊàê„Åß„Åç„Çã</li>
<li>[ ] Ê¨†ÊêçÂÄ§„ÇíÊ§úÂá∫„Åß„Åç„ÇãÔºà<code>isnull().sum()</code>Ôºâ</li>
<li>[ ] Ê¨†ÊêçÂÄ§„ÇíÂâäÈô§„Åæ„Åü„ÅØË£úÂÆå„Åß„Åç„ÇãÔºà<code>dropna()</code> or <code>fillna()</code>Ôºâ</li>
</ul>
<h4>ÂøúÁî®„É¨„Éô„É´</h4>
<ul>
<li>[ ] Áõ∏Èñ¢Ë°åÂàó„ÇíË®àÁÆó„ÉªÂèØË¶ñÂåñ„Åß„Åç„ÇãÔºà<code>corr()</code>„ÄÅseaborn.heatmapÔºâ</li>
<li>[ ] „Éö„Ç¢„Éó„É≠„ÉÉ„ÉàÔºàÊï£Â∏ÉÂõ≥„Éû„Éà„É™„ÉÉ„ÇØ„ÇπÔºâ„Çí‰ΩúÊàê„Åß„Åç„ÇãÔºàseaborn.pairplotÔºâ</li>
<li>[ ] 3DÊï£Â∏ÉÂõ≥„Çí‰ΩúÊàê„Åß„Åç„ÇãÔºàmpl_toolkits.mplot3dÔºâ</li>
<li>[ ] KDEÔºà„Ç´„Éº„Éç„É´ÂØÜÂ∫¶Êé®ÂÆöÔºâ„Çí‰ΩøÁî®„Åß„Åç„Çã</li>
<li>[ ] Â§ñ„ÇåÂÄ§„ÇíIQRÊ≥ï„ÅßÊ§úÂá∫„Åß„Åç„Çã</li>
<li>[ ] StandardScaler„Åß„Éá„Éº„Çø„ÇíÊ®ôÊ∫ñÂåñ„Åß„Åç„Çã</li>
<li>[ ] train_test_split„Åß„Éá„Éº„Çø„ÇíÂàÜÂâ≤„Åß„Åç„ÇãÔºà80% vs 20%Ôºâ</li>
<li>[ ] <code>random_state=42</code>„ÅßÂÜçÁèæÊÄß„ÇíÁ¢∫‰øù„Åó„Å¶„ÅÑ„Çã</li>
</ul>
<h4>‰∏äÁ¥ö„É¨„Éô„É´</h4>
<ul>
<li>[ ] ÂØæÊï∞Ê≠£Ë¶èÂàÜÂ∏É„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞„Åå„Åß„Åç„ÇãÔºàscipy.stats.lognormÔºâ</li>
<li>[ ] Q-Q„Éó„É≠„ÉÉ„Éà„ÅßÂàÜÂ∏É„ÅÆÈÅ©ÂêàÂ∫¶„ÇíÊ§úË®º„Åß„Åç„Çã</li>
<li>[ ] „Ç´„É©„Éº„Éû„ÉÉ„Éó„ÇíÂäπÊûúÁöÑ„Å´‰ΩøÁî®„Åß„Åç„ÇãÔºàviridis„ÄÅplasma„ÄÅcoolwarmÔºâ</li>
<li>[ ] Ë§áÊï∞„ÅÆsubplots„Çí‰Ωø„Å£„ÅüÈ´òÂ∫¶„Å™ÂèØË¶ñÂåñ„Åå„Åß„Åç„Çã</li>
</ul>
<hr />
<h3>3.12.3 Ê©üÊ¢∞Â≠¶Áøí„É¢„Éá„É´ÂÆüË£Ö„Çπ„Ç≠„É´ÔºàML Model ImplementationÔºâ</h3>
<h4>Âü∫Á§é„É¨„Éô„É´Ôºà5„Å§„ÅÆ„É¢„Éá„É´ÂÆüË£ÖÔºâ</h4>
<ul>
<li>[ ] Á∑öÂΩ¢ÂõûÂ∏∞„ÇíÂÆüË£Ö„Åó„ÄÅ‰øÇÊï∞„ÅÆÊÑèÂë≥„ÇíË™¨Êòé„Åß„Åç„Çã</li>
<li>[ ] „É©„É≥„ÉÄ„É†„Éï„Ç©„É¨„Çπ„Éà„ÇíÂÆüË£Ö„Åó„ÄÅ<code>n_estimators</code>„ÅÆÂΩπÂâ≤„ÇíË™¨Êòé„Åß„Åç„Çã</li>
<li>[ ] LightGBM„Çí„Ç§„É≥„Çπ„Éà„Éº„É´„ÉªÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] SVR„ÅßÊ®ôÊ∫ñÂåñÔºàStandardScalerÔºâ„ÅÆÂøÖË¶ÅÊÄß„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] MLPRegressorÔºà„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÔºâ„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
</ul>
<h4>ÂøúÁî®„É¨„Éô„É´Ôºà„É¢„Éá„É´ÈÅ∏Êäû„Å®Ë©ï‰æ°Ôºâ</h4>
<ul>
<li>[ ] MAE„ÄÅR¬≤„ÄÅRMSE„ÇíË®àÁÆó„ÉªËß£Èáà„Åß„Åç„Çã</li>
<li>[ ] Ë®ìÁ∑¥„Éá„Éº„Çø„Å®„ÉÜ„Çπ„Éà„Éá„Éº„Çø„ÅÆÊÄßËÉΩÂ∑Æ„ÇíË©ï‰æ°„Åß„Åç„Çã</li>
<li>[ ] ÈÅéÂ≠¶Áøí„ÇíÊ§úÂá∫„Åß„Åç„ÇãÔºàË®ìÁ∑¥R¬≤ ‚â´ „ÉÜ„Çπ„ÉàR¬≤Ôºâ</li>
<li>[ ] 5„Å§„ÅÆ„É¢„Éá„É´„ÅÆÊÄßËÉΩ„ÇíÊØîËºÉË°®„ÅßÊï¥ÁêÜ„Åß„Åç„Çã</li>
<li>[ ] ‰∫àÊ∏¨ÂÄ§ vs ÂÆüÊ∏¨ÂÄ§„ÅÆÊï£Â∏ÉÂõ≥„Çí‰ΩúÊàê„Åß„Åç„Çã</li>
<li>[ ] ÊÆãÂ∑Æ„Éó„É≠„ÉÉ„Éà„Çí‰ΩúÊàê„Åó„ÄÅ„É¢„Éá„É´„ÅÆÂÅè„Çä„ÇíÊ§úÂá∫„Åß„Åç„Çã</li>
</ul>
<h4>‰∏äÁ¥ö„É¨„Éô„É´</h4>
<ul>
<li>[ ] „Éá„Éº„ÇøÁâπÊÄß„Å´Âøú„Åò„Å¶ÊúÄÈÅ©„Å™„É¢„Éá„É´„ÇíÈÅ∏Êäû„Åß„Åç„Çã</li>
<li>Á∑öÂΩ¢ÊÄß„ÅåÂº∑„ÅÑ ‚Üí Á∑öÂΩ¢ÂõûÂ∏∞</li>
<li>ÈùûÁ∑öÂΩ¢ÊÄß„ÅåÂº∑„ÅÑ ‚Üí „É©„É≥„ÉÄ„É†„Éï„Ç©„É¨„Çπ„Éà„ÄÅLightGBM</li>
<li>„Éá„Éº„ÇøÊï∞„ÅåÂ∞ë„Å™„ÅÑ ‚Üí SVR</li>
<li>[ ] ÂêÑ„É¢„Éá„É´„ÅÆ„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅÆÂΩπÂâ≤„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>„É©„É≥„ÉÄ„É†„Éï„Ç©„É¨„Çπ„ÉàÔºön_estimators„ÄÅmax_depth</li>
<li>LightGBMÔºölearning_rate„ÄÅnum_leaves</li>
<li>SVRÔºöC„ÄÅgamma„ÄÅepsilon</li>
<li>MLPÔºöhidden_layer_sizes„ÄÅalpha„ÄÅearly_stopping</li>
</ul>
<hr />
<h3>3.12.4 ÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶„Å®„É¢„Éá„É´Ëß£Èáà„Çπ„Ç≠„É´ÔºàFeature Importance &amp; InterpretabilityÔºâ</h3>
<h4>Âü∫Á§é„É¨„Éô„É´</h4>
<ul>
<li>[ ] „É©„É≥„ÉÄ„É†„Éï„Ç©„É¨„Çπ„Éà„ÅÆÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶„ÇíÂèñÂæó„ÉªÂèØË¶ñÂåñ„Åß„Åç„ÇãÔºà<code>feature_importances_</code>Ôºâ</li>
<li>[ ] LightGBM„ÅÆÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶„ÇíÂèñÂæó„ÉªÂèØË¶ñÂåñ„Åß„Åç„Çã</li>
<li>[ ] ÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶„ÅÆÁµêÊûú„ÇíËß£Èáà„Åß„Åç„ÇãÔºàÊúÄ„ÇÇÂΩ±Èüø„Åô„ÇãÁâπÂæ¥Èáè„ÅØ‰Ωï„ÅãÔºâ</li>
</ul>
<h4>ÂøúÁî®„É¨„Éô„É´</h4>
<ul>
<li>[ ] SHAP„É©„Ç§„Éñ„É©„É™„Çí„Ç§„É≥„Çπ„Éà„Éº„É´„Éª‰ΩøÁî®„Åß„Åç„Çã</li>
<li>[ ] SHAP Explainer„Çí‰ΩúÊàê„Åß„Åç„ÇãÔºà<code>shap.Explainer</code>Ôºâ</li>
<li>[ ] SHAP Summary Plot„Çí‰ΩúÊàê„ÉªËß£Èáà„Åß„Åç„Çã</li>
<li>[ ] SHAP Dependence Plot„Çí‰ΩúÊàê„ÉªËß£Èáà„Åß„Åç„Çã</li>
<li>[ ] SHAPÂÄ§„ÅÆÊ≠£Ë≤†„Åå‰∫àÊ∏¨„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÇíË™¨Êòé„Åß„Åç„Çã</li>
</ul>
<h4>‰∏äÁ¥ö„É¨„Éô„É´</h4>
<ul>
<li>[ ] Ë§áÊï∞„ÅÆËß£ÈáàÊâãÊ≥ï„Çí‰Ωø„ÅÑÂàÜ„Åë„Çâ„Çå„Çã</li>
<li>ÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶ÔºöÂÖ®‰ΩìÁöÑ„Å™ÈáçË¶ÅÂ∫¶</li>
<li>SHAPÔºöÂÄãÂà•„Çµ„É≥„Éó„É´„ÅÆ‰∫àÊ∏¨ÁêÜÁî±</li>
<li>[ ] „É¢„Éá„É´„ÅÆ‰∫àÊ∏¨Ê†πÊã†„Çí„Çπ„ÉÜ„Éº„ÇØ„Éõ„É´„ÉÄ„Éº„Å´Ë™¨Êòé„Åß„Åç„Çã</li>
</ul>
<hr />
<h3>3.12.5 „Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„Çπ„Ç≠„É´ÔºàBayesian OptimizationÔºâ</h3>
<h4>Âü∫Á§é„É¨„Éô„É´</h4>
<ul>
<li>[ ] scikit-optimize„Çí„Ç§„É≥„Çπ„Éà„Éº„É´„Åß„Åç„Çã</li>
<li>[ ] Êé¢Á¥¢Á©∫Èñì„ÇíÂÆöÁæ©„Åß„Åç„ÇãÔºà<code>Real(min, max, name)</code>Ôºâ</li>
<li>[ ] ÁõÆÁöÑÈñ¢Êï∞„ÇíÂÆöÁæ©„Åß„Åç„ÇãÔºà„Éë„É©„É°„Éº„Çø„ÇíÂèó„ÅëÂèñ„Çä„ÄÅË™§Â∑Æ„ÇíËøî„ÅôÔºâ</li>
<li>[ ] <code>gp_minimize</code>„ÇíÂÆüË°å„Åß„Åç„Çã</li>
<li>[ ] ÊúÄÈÅ©ÂåñÁµêÊûúÔºàresult.x„ÄÅresult.funÔºâ„ÇíÂèñÂæó„Åß„Åç„Çã</li>
</ul>
<h4>ÂøúÁî®„É¨„Éô„É´</h4>
<ul>
<li>[ ] n_calls„Å®n_initial_points„ÅÆÂΩπÂâ≤„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>n_callsÔºöË©ï‰æ°ÂõûÊï∞</li>
<li>n_initial_pointsÔºö„É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞ÂõûÊï∞</li>
<li>[ ] ÂèéÊùü„Éó„É≠„ÉÉ„Éà„Çí‰ΩúÊàê„Åß„Åç„ÇãÔºà<code>plot_convergence</code>Ôºâ</li>
<li>[ ] ÊúÄÈÅ©Âåñ„Éó„É≠„Çª„Çπ„ÅÆÂèØË¶ñÂåñ„Åå„Åß„Åç„ÇãÔºàË©ï‰æ°Â±•Ê≠¥„ÄÅÊúÄËâØÂÄ§„ÅÆÊé®ÁßªÔºâ</li>
<li>[ ] ÁõÆÊ®ôÂÄ§„Å∏„ÅÆÈÅîÊàêÁ≤æÂ∫¶„ÇíË©ï‰æ°„Åß„Åç„Çã</li>
</ul>
<h4>‰∏äÁ¥ö„É¨„Éô„É´</h4>
<ul>
<li>[ ] Ë§áÊï∞„ÅÆÁõÆÊ®ôÔºàËµ§„ÉªÁ∑ë„ÉªÈùí„ÅÆÈáèÂ≠ê„Éâ„ÉÉ„ÉàÔºâ„Å´ÂØæ„Åó„Å¶ÊúÄÈÅ©Âåñ„ÇíÂÆüË°å„Åß„Åç„Çã</li>
<li>[ ] ÊúÄÈÅ©ÂåñÁµêÊûú„ÇíÂÆüÈ®ìÊ§úË®ºË®àÁîª„Å´Ê¥ªÁî®„Åß„Åç„Çã</li>
<li>[ ] Áç≤ÂæóÈñ¢Êï∞ÔºàAcquisition FunctionÔºâ„ÅÆÊ¶ÇÂøµ„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
</ul>
<hr />
<h3>3.12.6 Â§öÁõÆÁöÑÊúÄÈÅ©Âåñ„Çπ„Ç≠„É´ÔºàMulti-Objective OptimizationÔºâ</h3>
<h4>Âü∫Á§é„É¨„Éô„É´</h4>
<ul>
<li>[ ] pymoo„É©„Ç§„Éñ„É©„É™„Çí„Ç§„É≥„Çπ„Éà„Éº„É´„Åß„Åç„Çã</li>
<li>[ ] Â§öÁõÆÁöÑÊúÄÈÅ©ÂåñÂïèÈ°å„ÅÆÊ¶ÇÂøµ„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„ÇãÔºà„Çµ„Ç§„Ç∫ÊúÄÂ∞èÂåñ vs ÂäπÁéáÊúÄÂ§ßÂåñÔºâ</li>
<li>[ ] „Éë„É¨„Éº„Éà„Éï„É≠„É≥„Éà„ÅÆÊ¶ÇÂøµ„ÇíË™¨Êòé„Åß„Åç„Çã</li>
</ul>
<h4>ÂøúÁî®„É¨„Éô„É´</h4>
<ul>
<li>[ ] pymoo.core.problem„ÇíÁ∂ôÊâø„Åó„Å¶Problem„ÇØ„É©„Çπ„ÇíÂÆöÁæ©„Åß„Åç„Çã</li>
<li>[ ] NSGA-II„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] „Éë„É¨„Éº„Éà„Éï„É≠„É≥„Éà„ÇíÂèØË¶ñÂåñ„Åß„Åç„Çã</li>
<li>[ ] „Éà„É¨„Éº„Éâ„Ç™„ÉïÈñ¢‰øÇ„ÇíËß£Èáà„Åß„Åç„Çã</li>
</ul>
<h4>‰∏äÁ¥ö„É¨„Éô„É´</h4>
<ul>
<li>[ ] Ë§áÊï∞„ÅÆËß£„Åã„ÇâÁî®ÈÄî„Å´Âøú„Åò„ÅüÊúÄÈÅ©Ëß£„ÇíÈÅ∏Êäû„Åß„Åç„Çã</li>
<li>È´òÊÄßËÉΩÈáçË¶ñ</li>
<li>Áí∞Â¢ÉÈáçË¶ñ</li>
<li>„Éê„É©„É≥„ÇπÂûã</li>
<li>[ ] „Ç∞„É™„ÉÉ„Éâ„Çµ„Éº„ÉÅ„Å´„Çà„Çã‰ª£ÊõøÂÆüË£Ö„Åå„Åß„Åç„ÇãÔºàpymoo„Åå‰Ωø„Åà„Å™„ÅÑÂ†¥ÂêàÔºâ</li>
</ul>
<hr />
<h3>3.12.7 „Éä„ÉéÊùêÊñôÁâπÊúâ„ÅÆËß£Êûê„Çπ„Ç≠„É´ÔºàNanomaterial-Specific AnalysisÔºâ</h3>
<h4>TEMÁîªÂÉèËß£Êûê</h4>
<ul>
<li>[ ] ÂØæÊï∞Ê≠£Ë¶èÂàÜÂ∏É„Å´Âæì„ÅÜ„Çµ„Ç§„Ç∫„Éá„Éº„Çø„ÇíÁîüÊàê„Åß„Åç„Çã</li>
<li>[ ] ÂØæÊï∞Ê≠£Ë¶èÂàÜÂ∏É„ÅÆ„Éë„É©„É°„Éº„ÇøÔºàsigma„ÄÅmuÔºâ„ÇíË®àÁÆó„Åß„Åç„Çã</li>
<li>[ ] <code>lognorm.fit</code>„Åß„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞„Åß„Åç„Çã</li>
<li>[ ] „Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞ÁµêÊûú„ÇíÂèØË¶ñÂåñ„Åß„Åç„ÇãÔºà„Éí„Çπ„Éà„Ç∞„É©„É† + PDFÊõ≤Á∑öÔºâ</li>
<li>[ ] Q-Q„Éó„É≠„ÉÉ„Éà„ÅßÂàÜÂ∏É„ÅÆÈÅ©ÂêàÂ∫¶„ÇíË©ï‰æ°„Åß„Åç„Çã</li>
</ul>
<h4>ÂàÜÂ≠êÂãïÂäõÂ≠¶ÔºàMDÔºâ„Éá„Éº„ÇøËß£Êûê</h4>
<ul>
<li>[ ] ÂéüÂ≠êËªåË∑°„Éá„Éº„Çø„ÅÆÊßãÈÄ†„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„ÇãÔºàn_steps √ó n_atoms √ó 3Ôºâ</li>
<li>[ ] 3DËªåË∑°„Éó„É≠„ÉÉ„Éà„Çí‰ΩúÊàê„Åß„Åç„Çã</li>
<li>[ ] ÂãïÂæÑÂàÜÂ∏ÉÈñ¢Êï∞ÔºàRDFÔºâ„ÇíË®àÁÆó„Åß„Åç„Çã</li>
<li>[ ] RDF„ÅÆ„Éî„Éº„ÇØ‰ΩçÁΩÆ„Åã„ÇâÁâπÂæ¥ÁöÑ„Å™ÂéüÂ≠êÈñìË∑ùÈõ¢„ÇíÊäΩÂá∫„Åß„Åç„Çã</li>
<li>[ ] Âπ≥Âùá‰∫å‰πóÂ§â‰ΩçÔºàMSDÔºâ„ÇíË®àÁÆó„Åß„Åç„Çã</li>
<li>[ ] MSD„Åã„ÇâÊã°Êï£‰øÇÊï∞„ÇíÁÆóÂá∫„Åß„Åç„ÇãÔºàEinsteinÈñ¢‰øÇÂºèÔºâ</li>
</ul>
<h4>Áï∞Â∏∏Ê§úÁü•</h4>
<ul>
<li>[ ] Isolation Forest„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] contaminationÔºàÁï∞Â∏∏„Éá„Éº„ÇøÂâ≤ÂêàÔºâ„ÇíË®≠ÂÆö„Åß„Åç„Çã</li>
<li>[ ] Áï∞Â∏∏„Çπ„Ç≥„Ç¢„ÇíË®àÁÆó„Åß„Åç„ÇãÔºà<code>score_samples</code>Ôºâ</li>
<li>[ ] Ê∑∑ÂêåË°åÂàó„ÅßÁï∞Â∏∏Ê§úÂá∫Á≤æÂ∫¶„ÇíË©ï‰æ°„Åß„Åç„Çã</li>
<li>[ ] Ê≠£Â∏∏„Éá„Éº„Çø„Å®Áï∞Â∏∏„Éá„Éº„Çø„ÅÆÂàÜÂ∏É„ÇíÂèØË¶ñÂåñ„Åß„Åç„Çã</li>
</ul>
<hr />
<h3>3.12.8 „Ç≥„Éº„ÉâÂìÅË≥™„Çπ„Ç≠„É´ÔºàCode QualityÔºâ</h3>
<h4>Âü∫Á§é„É¨„Éô„É´</h4>
<ul>
<li>[ ] „Åô„Åπ„Å¶„ÅÆ„Ç≥„Éº„Éâ„Å´‰π±Êï∞„Ç∑„Éº„ÉâÔºà<code>random_state=42</code>Ôºâ„ÇíË®≠ÂÆö„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] „Éá„Éº„ÇøÊ§úË®ºÔºàshape„ÄÅdtype„ÄÅÊ¨†ÊêçÂÄ§„ÄÅÁØÑÂõ≤Ôºâ„ÇíÂÆüÊñΩ„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] Â§âÊï∞Âêç„ÅåÂàÜ„Åã„Çä„ÇÑ„Åô„ÅÑÔºà<code>X_train</code>„ÄÅ<code>y_test</code>„ÄÅ<code>model_lgb</code>Ôºâ</li>
<li>[ ] „Ç≥„É°„É≥„Éà„ÅßÂá¶ÁêÜ„ÅÆÁõÆÁöÑ„ÇíË™¨Êòé„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] „Ç∞„É©„Éï„Å´„Çø„Ç§„Éà„É´„ÄÅËª∏„É©„Éô„É´„ÄÅÂá°‰æã„ÇíËøΩÂä†„Åó„Å¶„ÅÑ„Çã</li>
</ul>
<h4>ÂøúÁî®„É¨„Éô„É´</h4>
<ul>
<li>[ ] Èñ¢Êï∞Âåñ„Åó„Å¶„Ç≥„Éº„Éâ„ÇíÂÜçÂà©Áî®ÂèØËÉΩ„Å´„Åó„Å¶„ÅÑ„Çã
  <code>python
  def calculate_rdf(positions, r_max, n_bins):
      ...</code></li>
<li>[ ] „Éâ„Ç≠„É•„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥ÊñáÂ≠óÂàóÔºàDocstringÔºâ„ÇíË®òËø∞„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] „Ç∞„É©„Éï„ÅÆÁæéË¶≥„ÇíÊï¥„Åà„Å¶„ÅÑ„ÇãÔºàfontsize„ÄÅgrid„ÄÅalphaÔºâ</li>
<li>[ ] try-except„Åß„Ç®„É©„Éº„Éè„É≥„Éâ„É™„É≥„Ç∞„ÇíÂÆüË£Ö„Åó„Å¶„ÅÑ„ÇãÔºàpymoo„ÅÆImportErrorÂØæÂøúÔºâ</li>
</ul>
<hr />
<h3>3.12.9 „Éà„É©„Éñ„É´„Ç∑„É•„Éº„ÉÜ„Ç£„É≥„Ç∞„Çπ„Ç≠„É´ÔºàTroubleshootingÔºâ</h3>
<h4>Âü∫Á§é„É¨„Éô„É´Ôºà„Ç®„É©„ÉºÂØæÂá¶Ôºâ</h4>
<ul>
<li>[ ] <code>ModuleNotFoundError</code>„ÇíËß£Ê±∫„Åß„Åç„ÇãÔºà<code>pip install</code>Ôºâ</li>
<li>[ ] <code>ValueError: Input contains NaN</code>„ÇíËß£Ê±∫„Åß„Åç„ÇãÔºàÊ¨†ÊêçÂÄ§Âá¶ÁêÜÔºâ</li>
<li>[ ] <code>ConvergenceWarning</code>ÔºàMLP„ÅÆÂèéÊùü„Ç®„É©„ÉºÔºâ„ÇíËß£Ê±∫„Åß„Åç„Çã</li>
<li><code>max_iter</code>„ÇíÂ¢ó„ÇÑ„Åô</li>
<li>„Éá„Éº„Çø„ÇíÊ®ôÊ∫ñÂåñ„Åô„Çã</li>
<li>Early Stopping„ÇíÊúâÂäπÂåñ</li>
<li>[ ] „Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË™≠„Åø„ÄÅÊ§úÁ¥¢„Åó„Å¶Ëß£Ê±∫Á≠ñ„ÇíË¶ã„Å§„Åë„Çâ„Çå„Çã</li>
</ul>
<h4>ÂøúÁî®„É¨„Éô„É´ÔºàÊÄßËÉΩÊîπÂñÑÔºâ</h4>
<ul>
<li>[ ] R¬≤ &lt; 0.7„ÅÆÂ†¥Âêà„ÄÅ3„Å§‰ª•‰∏ä„ÅÆÊîπÂñÑÁ≠ñ„ÇíÂÆüË°å„Åß„Åç„Çã</li>
<li>ÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞</li>
<li>„É¢„Éá„É´Â§âÊõ¥ÔºàÁ∑öÂΩ¢‚ÜíÈùûÁ∑öÂΩ¢Ôºâ</li>
<li>„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøË™øÊï¥</li>
<li>[ ] ÈÅéÂ≠¶Áøí„ÇíÊ§úÂá∫„Åß„Åç„ÇãÔºàË®ìÁ∑¥R¬≤ ‚â´ „ÉÜ„Çπ„ÉàR¬≤Ôºâ</li>
<li>[ ] Êú™Â≠¶Áøí„ÇíÊ§úÂá∫„Åß„Åç„ÇãÔºàË®ìÁ∑¥R¬≤„ÇÇ„ÉÜ„Çπ„ÉàR¬≤„ÇÇ‰Ωé„ÅÑÔºâ</li>
</ul>
<hr />
<h3>3.12.10 Á∑èÂêàË©ï‰æ°ÔºöÁøíÁÜüÂ∫¶„É¨„Éô„É´Âà§ÂÆö</h3>
<p>‰ª•‰∏ã„ÅÆ„É¨„Éô„É´Âà§ÂÆö„Åß„ÄÅËá™ÂàÜ„ÅÆÂà∞ÈÅîÂ∫¶„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<h4>„É¨„Éô„É´1ÔºöÂàùÂøÉËÄÖÔºàBeginnerÔºâ</h4>
<ul>
<li>Áí∞Â¢ÉÊßãÁØâ„Çπ„Ç≠„É´ÔºöÂü∫Á§é„É¨„Éô„É´ 100%ÈÅîÊàê</li>
<li>„Éá„Éº„ÇøÂá¶ÁêÜ„ÉªÂèØË¶ñÂåñ„Çπ„Ç≠„É´ÔºöÂü∫Á§é„É¨„Éô„É´ 80%‰ª•‰∏äÈÅîÊàê</li>
<li>Ê©üÊ¢∞Â≠¶Áøí„É¢„Éá„É´ÂÆüË£Ö„Çπ„Ç≠„É´ÔºöÂü∫Á§é„É¨„Éô„É´ 5„Å§‰∏≠3„Å§‰ª•‰∏äÂÆüË£Ö</li>
<li>„Éà„É©„Éñ„É´„Ç∑„É•„Éº„ÉÜ„Ç£„É≥„Ç∞ÔºöÂü∫Á§é„É¨„Éô„É´„ÅÆ„Ç®„É©„Éº„ÇíËá™Âäõ„ÅßËß£Ê±∫</li>
</ul>
<p><strong>Âà∞ÈÅîÁõÆÊ®ô:</strong> „Éä„ÉéÁ≤íÂ≠ê„Éá„Éº„Çø„ÇíÁîüÊàê„ÉªÂèØË¶ñÂåñ„Åó„ÄÅÁ∑öÂΩ¢ÂõûÂ∏∞„Å®„É©„É≥„ÉÄ„É†„Éï„Ç©„É¨„Çπ„Éà„ÅßLSPRÊ≥¢Èï∑‰∫àÊ∏¨„ÇíÂÆüË£Ö„Åß„Åç„Çã</p>
<hr />
<h4>„É¨„Éô„É´2Ôºö‰∏≠Á¥öËÄÖÔºàIntermediateÔºâ</h4>
<ul>
<li>Áí∞Â¢ÉÊßãÁØâ„Çπ„Ç≠„É´ÔºöÂøúÁî®„É¨„Éô„É´ 80%‰ª•‰∏äÈÅîÊàê</li>
<li>„Éá„Éº„ÇøÂá¶ÁêÜ„ÉªÂèØË¶ñÂåñ„Çπ„Ç≠„É´ÔºöÂü∫Á§é„É¨„Éô„É´ 100%ÈÅîÊàê + ÂøúÁî®„É¨„Éô„É´ 70%‰ª•‰∏ä</li>
<li>Ê©üÊ¢∞Â≠¶Áøí„É¢„Éá„É´ÂÆüË£Ö„Çπ„Ç≠„É´ÔºöÂü∫Á§é„É¨„Éô„É´ 100%ÈÅîÊàê + ÂøúÁî®„É¨„Éô„É´ 70%‰ª•‰∏ä</li>
<li>ÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶„Å®„É¢„Éá„É´Ëß£Èáà„Çπ„Ç≠„É´ÔºöÂü∫Á§é„É¨„Éô„É´ 100%ÈÅîÊàê + ÂøúÁî®„É¨„Éô„É´ 50%‰ª•‰∏ä</li>
<li>„Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„Çπ„Ç≠„É´ÔºöÂü∫Á§é„É¨„Éô„É´ 100%ÈÅîÊàê + ÂøúÁî®„É¨„Éô„É´ 50%‰ª•‰∏ä</li>
</ul>
<p><strong>Âà∞ÈÅîÁõÆÊ®ô:</strong> 5„Å§„ÅÆÂõûÂ∏∞„É¢„Éá„É´„ÇíÊØîËºÉ„Åó„ÄÅ„Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„ÅßÁõÆÊ®ôLSPRÊ≥¢Èï∑Ôºà550 nmÔºâ„ÇíÈÅîÊàê„Åô„ÇãÂêàÊàêÊù°‰ª∂„ÇíÁô∫Ë¶ã„Åß„Åç„Çã</p>
<hr />
<h4>„É¨„Éô„É´3Ôºö‰∏äÁ¥öËÄÖÔºàAdvancedÔºâ</h4>
<ul>
<li>ÂÖ®„Ç´„ÉÜ„Ç¥„É™ÔºöÂøúÁî®„É¨„Éô„É´ 100%ÈÅîÊàê</li>
<li>ÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶„Å®„É¢„Éá„É´Ëß£Èáà„Çπ„Ç≠„É´Ôºö‰∏äÁ¥ö„É¨„Éô„É´ 80%‰ª•‰∏ä</li>
<li>„Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„Çπ„Ç≠„É´Ôºö‰∏äÁ¥ö„É¨„Éô„É´ 80%‰ª•‰∏ä</li>
<li>Â§öÁõÆÁöÑÊúÄÈÅ©Âåñ„Çπ„Ç≠„É´ÔºöÂøúÁî®„É¨„Éô„É´ 100%ÈÅîÊàê</li>
<li>„Éä„ÉéÊùêÊñôÁâπÊúâ„ÅÆËß£Êûê„Çπ„Ç≠„É´ÔºöTEM„ÄÅMD„ÄÅÁï∞Â∏∏Ê§úÁü•„Åô„Åπ„Å¶ÂÆüË£Ö</li>
</ul>
<p><strong>Âà∞ÈÅîÁõÆÊ®ô:</strong> SHAPÂàÜÊûê„Åß„É¢„Éá„É´Ëß£Èáà„Åó„ÄÅÂ§öÁõÆÁöÑÊúÄÈÅ©Âåñ„Åß„Çµ„Ç§„Ç∫„Å®ÂäπÁéá„ÅÆ„Éà„É¨„Éº„Éâ„Ç™„Éï„ÇíÂèØË¶ñÂåñ„Åß„Åç„Çã</p>
<hr />
<h4>„É¨„Éô„É´4Ôºö„Ç®„Ç≠„Çπ„Éë„Éº„ÉàÔºàExpertÔºâ</h4>
<ul>
<li>ÂÖ®„Ç´„ÉÜ„Ç¥„É™Ôºö‰∏äÁ¥ö„É¨„Éô„É´ 80%‰ª•‰∏äÈÅîÊàê</li>
<li>„Ç≥„Éº„ÉâÂìÅË≥™ÔºöÂøúÁî®„É¨„Éô„É´ 100%ÈÅîÊàê</li>
<li>Áã¨Ëá™„ÅÆ„Éä„ÉéÊùêÊñô„Éá„Éº„ÇøÔºàÂÆüÈ®ì„Éá„Éº„Çø„Åæ„Åü„ÅØÊñáÁåÆ„Éá„Éº„ÇøÔºâ„Å´ÈÅ©Áî®„Åß„Åç„Çã</li>
<li>„Ç´„Çπ„Çø„É†Ê©üÊ¢∞Â≠¶Áøí„Éë„Ç§„Éó„É©„Ç§„É≥„ÇíÊßãÁØâ„Åß„Åç„Çã</li>
<li>Á†îÁ©∂ÊàêÊûú„ÇíÂ≠¶‰ºöÁô∫Ë°®„ÉªË´ñÊñáÊäïÁ®ø„Åß„Åç„Çã</li>
</ul>
<p><strong>Âà∞ÈÅîÁõÆÊ®ô:</strong>
- ÂÆü„Éä„ÉéÊùêÊñô„Éá„Éº„ÇøÔºàTEM„ÄÅUV-Vis„ÄÅXRDÔºâ„ÇíÁµ±ÂêàËß£Êûê
- Ê©üÊ¢∞Â≠¶Áøí„Å´„Çà„ÇäÊñ∞Ë¶è„Éä„ÉéÁ≤íÂ≠ê„ÅÆÁâ©ÊÄß„Çí90%‰ª•‰∏ä„ÅÆÁ≤æÂ∫¶„Åß‰∫àÊ∏¨
- „Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„ÅßÂÆüÈ®ìÂõûÊï∞„ÇíÂæìÊù•„ÅÆ1/5„Å´ÂâäÊ∏õ</p>
<hr />
<h3>3.12.11 ÂÆüË∑µ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÉÅ„Çß„ÉÉ„ÇØÔºöÊºîÁøíÂïèÈ°å„ÅÆÂÆåÈÅÇ</h3>
<h4>ÊºîÁøí1ÂÆåÈÅÇÁ¢∫Ë™çÔºàCNTÈõªÊ∞ó‰ºùÂ∞éÂ∫¶‰∫àÊ∏¨Ôºâ</h4>
<ul>
<li>[ ] „Éá„Éº„ÇøÁîüÊàêÔºà150„Çµ„É≥„Éó„É´„ÄÅ3ÁâπÂæ¥ÈáèÔºâ„ÇíÂÆüË£Ö</li>
<li>[ ] LightGBM„É¢„Éá„É´„Åß‰∫àÊ∏¨„ÇíÂÆüË£Ö</li>
<li>[ ] R¬≤ &gt; 0.8„ÄÅRMSE &lt; 0.5„ÇíÈÅîÊàê</li>
<li>[ ] ÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶„ÇíÂèØË¶ñÂåñ</li>
<li>[ ] ÁµêÊûú„ÇíËß£ÈáàÔºà„Å©„ÅÆÁâπÂæ¥Èáè„ÅåÊúÄ„ÇÇÂΩ±Èüø„Åô„Çã„ÅãÔºâ</li>
</ul>
<h4>ÊºîÁøí2ÂÆåÈÅÇÁ¢∫Ë™çÔºàÈäÄ„Éä„ÉéÁ≤íÂ≠êÊúÄÈÅ©ÂêàÊàêÊù°‰ª∂Ôºâ</h4>
<ul>
<li>[ ] ÈäÄ„Éä„ÉéÁ≤íÂ≠ê„Éá„Éº„ÇøÔºà100„Çµ„É≥„Éó„É´Ôºâ„ÇíÁîüÊàê</li>
<li>[ ] LightGBM„É¢„Éá„É´„ÇíÊßãÁØâ</li>
<li>[ ] „Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„ÇíÂÆüË°åÔºà40ÂõûË©ï‰æ°Ôºâ</li>
<li>[ ] ÁõÆÊ®ô„Çµ„Ç§„Ç∫Ôºà10 nmÔºâ„Å®„ÅÆË™§Â∑Æ &lt; 1 nm„ÇíÈÅîÊàê</li>
<li>[ ] ÊúÄÈÅ©Ê∏©Â∫¶„ÉªpH„ÇíÁâπÂÆö</li>
</ul>
<h4>ÊºîÁøí3ÂÆåÈÅÇÁ¢∫Ë™çÔºàÈáèÂ≠ê„Éâ„ÉÉ„ÉàÂ§öËâ≤Áô∫ÂÖâË®≠Ë®àÔºâ</h4>
<ul>
<li>[ ] Ëµ§„ÉªÁ∑ë„ÉªÈùí„ÅÆ3Ëâ≤„Å´„Å§„ÅÑ„Å¶ÊúÄÈÅ©Âåñ„ÇíÂÆüË°å</li>
<li>[ ] ÂêÑËâ≤„ÅÆÊúÄÈÅ©„Çµ„Ç§„Ç∫„ÉªÂêàÊàêÊù°‰ª∂„ÇíÁâπÂÆö</li>
<li>[ ] ‰∫àÊ∏¨Ê≥¢Èï∑„ÅåÁõÆÊ®ôÊ≥¢Èï∑¬±10 nm‰ª•ÂÜÖ„Å´Âèé„Åæ„Å£„Å¶„ÅÑ„Çã</li>
<li>[ ] ÁµêÊûú„ÇíÂèØË¶ñÂåñÔºà„Çµ„Ç§„Ç∫ vs Ê≥¢Èï∑„ÅÆ„Éó„É≠„ÉÉ„ÉàÔºâ</li>
</ul>
<hr />
<h3>3.12.12 Ê¨°„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó„Å∏„ÅÆÊ∫ñÂÇôÂ∫¶„ÉÅ„Çß„ÉÉ„ÇØ</h3>
<h4>ÂÆü‰∏ñÁïåÂøúÁî®ÔºàChapter 4Ôºâ„Å∏„ÅÆÊ∫ñÂÇô</h4>
<ul>
<li>[ ] Ê©üÊ¢∞Â≠¶Áøí„ÅÆÂü∫Êú¨ÁöÑ„Å™„ÉØ„Éº„ÇØ„Éï„É≠„ÉºÔºà„Éá„Éº„ÇøÊ∫ñÂÇô‚Üí„É¢„Éá„É´Ë®ìÁ∑¥‚ÜíË©ï‰æ°‚ÜíÊúÄÈÅ©ÂåñÔºâ„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] „Éä„ÉéÊùêÊñôÁâπÊúâ„ÅÆ„Éá„Éº„ÇøÔºà„Çµ„Ç§„Ç∫ÂàÜÂ∏É„ÄÅÂÖâÂ≠¶ÁâπÊÄß„ÄÅÈõªÊ∞óÁâπÊÄßÔºâ„ÇíÊâ±„Åà„Çã</li>
<li>[ ] ÊúÄÈÅ©ÂåñÊâãÊ≥ïÔºà„Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„ÄÅÂ§öÁõÆÁöÑÊúÄÈÅ©ÂåñÔºâ„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] „É¢„Éá„É´Ëß£ÈáàÔºàSHAPÔºâ„ÅÆÈáçË¶ÅÊÄß„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
</ul>
<h4>Ê∑±Â±§Â≠¶Áøí„Éª„Ç∞„É©„Éï„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Å∏„ÅÆÊ∫ñÂÇô</h4>
<ul>
<li>[ ] „Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÔºàMLPÔºâ„ÇíÂÆüË£Ö„Åó„ÄÅÊ¥ªÊÄßÂåñÈñ¢Êï∞„ÉªÊúÄÈÅ©Âåñ„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] Â≠¶ÁøíÊõ≤Á∑ö„ÇíÂèØË¶ñÂåñ„Åó„ÄÅÈÅéÂ≠¶Áøí„ÇíÊ§úÂá∫„Åß„Åç„Çã</li>
<li>[ ] Early Stopping„ÅÆÊ¶ÇÂøµ„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
</ul>
<h4>ÂÆüÂãôÁ†îÁ©∂„Å∏„ÅÆÊ∫ñÂÇô</h4>
<ul>
<li>[ ] Jupyter Notebook„Åæ„Åü„ÅØPython„Çπ„ÇØ„É™„Éó„Éà„Åß„Ç≥„Éº„Éâ„ÇíÁÆ°ÁêÜ„Åß„Åç„Çã</li>
<li>[ ] requirements.txt„ÅßÁí∞Â¢É„ÇíÂÜçÁèæÂèØËÉΩ„Å´„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] ‰∫àÊ∏¨ÁµêÊûú„Çí„Ç∞„É©„ÉïÂåñ„Åó„ÄÅ„É¨„Éù„Éº„Éà„Å´„Åæ„Å®„ÇÅ„Çâ„Çå„Çã</li>
<li>[ ] „Ç≥„Éº„Éâ„Å´„Éâ„Ç≠„É•„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥„ÇíË®òËø∞„Åó„Å¶„ÅÑ„Çã</li>
</ul>
<hr />
<p><strong>„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„ÉàÊ¥ªÁî®„ÅÆ„Éí„É≥„Éà:</strong>
1. <strong>ÂÆöÊúüÁöÑ„Å´Ë¶ãÁõ¥„Åô</strong>: Â≠¶ÁøíÂæå„ÄÅ1ÈÄ±ÈñìÂæå„ÄÅ1„É∂ÊúàÂæå„Å´ÂÜç„ÉÅ„Çß„ÉÉ„ÇØ
2. <strong>Êú™ÈÅîÊàêÈ†ÖÁõÆ„ÇíÂÑ™ÂÖà</strong>: „ÉÅ„Çß„ÉÉ„ÇØ„Åß„Åç„Å™„ÅÑÈ†ÖÁõÆ„ÇíÈõÜ‰∏≠Â≠¶Áøí
3. <strong>„É¨„Éô„É´Âà§ÂÆö„ÇíË®òÈå≤</strong>: ÊàêÈï∑„ÇíÂèØË¶ñÂåñ„Åó„Å¶„É¢„ÉÅ„Éô„Éº„Ç∑„Éß„É≥Á∂≠ÊåÅ
4. <strong>ÂÆü„Éó„É≠„Ç∏„Çß„ÇØ„Éà„Åß„ÅÆÊ¥ªÁî®</strong>: Á†îÁ©∂„ÉªÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„ÉàÈñãÂßãÂâç„Å´ÂøÖÈ†à„Çπ„Ç≠„É´„ÇíÁ¢∫Ë™ç</p>
<hr />
<h2>ÂèÇËÄÉÊñáÁåÆ</h2>
<ol>
<li>
<p><strong>Pedregosa, F. et al.</strong> (2011). Scikit-learn: Machine Learning in Python. <em>Journal of Machine Learning Research</em>, 12, 2825-2830.</p>
</li>
<li>
<p><strong>Ke, G. et al.</strong> (2017). LightGBM: A highly efficient gradient boosting decision tree. <em>Advances in Neural Information Processing Systems</em>, 30, 3146-3154.</p>
</li>
<li>
<p><strong>Lundberg, S. M. &amp; Lee, S.-I.</strong> (2017). A unified approach to interpreting model predictions. <em>Advances in Neural Information Processing Systems</em>, 30, 4765-4774.</p>
</li>
<li>
<p><strong>Snoek, J., Larochelle, H., &amp; Adams, R. P.</strong> (2012). Practical Bayesian optimization of machine learning algorithms. <em>Advances in Neural Information Processing Systems</em>, 25, 2951-2959.</p>
</li>
<li>
<p><strong>Deb, K. et al.</strong> (2002). A fast and elitist multiobjective genetic algorithm: NSGA-II. <em>IEEE Transactions on Evolutionary Computation</em>, 6(2), 182-197. <a href="https://doi.org/10.1109/4235.996017">DOI: 10.1109/4235.996017</a></p>
</li>
<li>
<p><strong>Frenkel, D. &amp; Smit, B.</strong> (2001). <em>Understanding Molecular Simulation: From Algorithms to Applications</em> (2nd ed.). Academic Press.</p>
</li>
</ol>
<hr />
<p><a href="chapter2-fundamentals.html">‚Üê ÂâçÁ´†Ôºö„Éä„ÉéÊùêÊñô„ÅÆÂü∫Á§éÂéüÁêÜ</a> | <a href="chapter4-real-world.html">Ê¨°Á´†ÔºöÂÆü‰∏ñÁïå„ÅÆÂøúÁî®„Å®„Ç≠„É£„É™„Ç¢ ‚Üí</a></p><div class="navigation">
    <a href="chapter2-fundamentals.html" class="nav-button">‚Üê Ââç„ÅÆÁ´†</a>
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
    <a href="chapter4-real-world.html" class="nav-button">Ê¨°„ÅÆÁ´† ‚Üí</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>Áõ£‰øÆ</strong>: Dr. Yusuke HashimotoÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-17</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
