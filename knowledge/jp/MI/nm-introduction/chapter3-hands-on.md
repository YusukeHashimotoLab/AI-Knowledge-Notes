---
title: "Chapter 3: Pythonå®Ÿè·µãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«"
chapter_title: "Chapter 3: Pythonå®Ÿè·µãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«"
subtitle: ãƒŠãƒææ–™ãƒ‡ãƒ¼ã‚¿è§£æã¨æ©Ÿæ¢°å­¦ç¿’
reading_time: 30-40åˆ†
difficulty: åˆç´š
code_examples: 0
exercises: 0
---

# Chapter 3: Pythonå®Ÿè·µãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«

å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã‚‚åŠ¹ãå›å¸°ãƒ¢ãƒ‡ãƒ«ã¨ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§ã€åŠ¹ç‡ã‚ˆãæ¡ä»¶æ¢ç´¢ã™ã‚‹ç­‹è‚‰ã‚’ä»˜ã‘ã¾ã™ã€‚MDãƒ‡ãƒ¼ã‚¿ã®è¦ç‚¹å¯è¦–åŒ–ã¨SHAPã«ã‚ˆã‚‹è§£é‡ˆã¾ã§ä¸€æ°—ã«é€šã—ã¾ã™ã€‚

**ğŸ’¡ è£œè¶³:** å°‘ãªã„è©¦è¡Œã§è‰¯ã„æ¡ä»¶ã‚’è¦‹ã¤ã‘ã‚‹ã®ãŒç›®æ¨™ã€‚ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã¯â€œé‡‘å±æ¢çŸ¥æ©Ÿâ€çš„ã«å½“ãŸã‚Šã‚’å°ãã¾ã™ã€‚

ãƒŠãƒææ–™ãƒ‡ãƒ¼ã‚¿è§£æã¨æ©Ÿæ¢°å­¦ç¿’

* * *

## æœ¬ç« ã®å­¦ç¿’ç›®æ¨™

æœ¬ç« ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€ä»¥ä¸‹ã®ã‚¹ã‚­ãƒ«ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š

âœ… ãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆãƒ»å¯è¦–åŒ–ãƒ»å‰å‡¦ç†ã®å®Ÿè·µ âœ… 5ç¨®é¡ã®å›å¸°ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒŠãƒææ–™ç‰©æ€§äºˆæ¸¬ âœ… ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚‹ãƒŠãƒææ–™ã®æœ€é©è¨­è¨ˆ âœ… SHAPåˆ†æã«ã‚ˆã‚‹æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆ âœ… å¤šç›®çš„æœ€é©åŒ–ã«ã‚ˆã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•åˆ†æ âœ… TEMç”»åƒè§£æã¨ã‚µã‚¤ã‚ºåˆ†å¸ƒã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚° âœ… ç•°å¸¸æ¤œçŸ¥ã«ã‚ˆã‚‹å“è³ªç®¡ç†ã¸ã®å¿œç”¨

* * *

## 3.1 ç’°å¢ƒæ§‹ç¯‰

### å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª

æœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ä½¿ç”¨ã™ã‚‹ä¸»è¦ãªPythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼š
    
    
    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»å¯è¦–åŒ–
    pandas, numpy, matplotlib, seaborn, scipy
    
    # æ©Ÿæ¢°å­¦ç¿’
    scikit-learn, lightgbm
    
    # æœ€é©åŒ–
    scikit-optimize
    
    # ãƒ¢ãƒ‡ãƒ«è§£é‡ˆ
    shap
    
    # å¤šç›®çš„æœ€é©åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    pymoo
    

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•

#### Option 1: Anacondaç’°å¢ƒ
    
    
    # Anacondaã§æ–°ã—ã„ç’°å¢ƒã‚’ä½œæˆ
    conda create -n nanomaterials python=3.10 -y
    conda activate nanomaterials
    
    # å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    conda install pandas numpy matplotlib seaborn scipy scikit-learn -y
    conda install -c conda-forge lightgbm scikit-optimize shap -y
    
    # å¤šç›®çš„æœ€é©åŒ–ç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    pip install pymoo
    

#### Option 2: venv + pipç’°å¢ƒ
    
    
    # ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆ
    python -m venv nanomaterials_env
    
    # ä»®æƒ³ç’°å¢ƒã‚’æœ‰åŠ¹åŒ–
    # macOS/Linux:
    source nanomaterials_env/bin/activate
    # Windows:
    nanomaterials_env\Scripts\activate
    
    # å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    pip install pandas numpy matplotlib seaborn scipy
    pip install scikit-learn lightgbm scikit-optimize shap pymoo
    

#### Option 3: Google Colab

Google Colabã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’ã‚»ãƒ«ã§å®Ÿè¡Œï¼š
    
    
    # è¿½åŠ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    !pip install lightgbm scikit-optimize shap pymoo
    
    # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®ç¢ºèª
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    print("ç’°å¢ƒæ§‹ç¯‰å®Œäº†ï¼")
    

* * *

## 3.2 ãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã¨å¯è¦–åŒ–

### ã€ä¾‹1ã€‘åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼šé‡‘ãƒŠãƒç²’å­ã®ã‚µã‚¤ã‚ºã¨å…‰å­¦ç‰¹æ€§

é‡‘ãƒŠãƒç²’å­ã®å±€åœ¨è¡¨é¢ãƒ—ãƒ©ã‚ºãƒ¢ãƒ³å…±é³´ï¼ˆLSPRï¼‰æ³¢é•·ã¯ã€ç²’å­ã‚µã‚¤ã‚ºã«ä¾å­˜ã—ã¾ã™ã€‚ã“ã®é–¢ä¿‚ã‚’æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ã§è¡¨ç¾ã—ã¾ã™ã€‚
    
    
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã®è¨­å®šï¼ˆå†ç¾æ€§ã®ãŸã‚ï¼‰
    np.random.seed(42)
    
    # ã‚µãƒ³ãƒ—ãƒ«æ•°
    n_samples = 200
    
    # é‡‘ãƒŠãƒç²’å­ã®ã‚µã‚¤ã‚ºï¼ˆnmï¼‰: å¹³å‡15 nmã€æ¨™æº–åå·®5 nm
    size = np.random.normal(15, 5, n_samples)
    size = np.clip(size, 5, 50)  # 5-50 nmã®ç¯„å›²ã«åˆ¶é™
    
    # LSPRæ³¢é•·ï¼ˆnmï¼‰: Mieç†è«–ã®ç°¡æ˜“è¿‘ä¼¼
    # åŸºæœ¬æ³¢é•·520 nm + ã‚µã‚¤ã‚ºä¾å­˜é … + ãƒã‚¤ã‚º
    lspr = 520 + 0.8 * (size - 15) + np.random.normal(0, 5, n_samples)
    
    # åˆæˆæ¡ä»¶
    temperature = np.random.uniform(20, 80, n_samples)  # æ¸©åº¦ï¼ˆâ„ƒï¼‰
    pH = np.random.uniform(4, 10, n_samples)  # pH
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä½œæˆ
    data = pd.DataFrame({
        'size_nm': size,
        'lspr_nm': lspr,
        'temperature_C': temperature,
        'pH': pH
    })
    
    print("=" * 60)
    print("é‡‘ãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆå®Œäº†")
    print("=" * 60)
    print(data.head(10))
    print("\nåŸºæœ¬çµ±è¨ˆé‡:")
    print(data.describe())
    

### ã€ä¾‹2ã€‘ã‚µã‚¤ã‚ºåˆ†å¸ƒã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    
    
    # ã‚µã‚¤ã‚ºåˆ†å¸ƒã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã¨KDEï¼ˆã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®šï¼‰
    ax.hist(data['size_nm'], bins=30, alpha=0.6, color='skyblue',
            edgecolor='black', density=True, label='Histogram')
    
    # KDEãƒ—ãƒ­ãƒƒãƒˆ
    from scipy.stats import gaussian_kde
    kde = gaussian_kde(data['size_nm'])
    x_range = np.linspace(data['size_nm'].min(), data['size_nm'].max(), 100)
    ax.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')
    
    ax.set_xlabel('Particle Size (nm)', fontsize=12)
    ax.set_ylabel('Probability Density', fontsize=12)
    ax.set_title('Gold Nanoparticle Size Distribution', fontsize=14, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print(f"å¹³å‡ã‚µã‚¤ã‚º: {data['size_nm'].mean():.2f} nm")
    print(f"æ¨™æº–åå·®: {data['size_nm'].std():.2f} nm")
    print(f"ä¸­å¤®å€¤: {data['size_nm'].median():.2f} nm")
    

### ã€ä¾‹3ã€‘æ•£å¸ƒå›³ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
    
    
    # ãƒšã‚¢ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæ•£å¸ƒå›³ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼‰
    sns.pairplot(data, diag_kind='kde', plot_kws={'alpha': 0.6},
                 height=2.5, corner=False)
    plt.suptitle('Pairplot of Gold Nanoparticle Data', y=1.01, fontsize=14, fontweight='bold')
    plt.show()
    
    print("å„å¤‰æ•°é–“ã®é–¢ä¿‚ã‚’å¯è¦–åŒ–ã—ã¾ã—ãŸ")
    

### ã€ä¾‹4ã€‘ç›¸é–¢è¡Œåˆ—ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
    
    
    # ç›¸é–¢è¡Œåˆ—ã®è¨ˆç®—
    correlation_matrix = data.corr()
    
    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
    fig, ax = plt.subplots(figsize=(8, 6))
    sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm',
                center=0, square=True, linewidths=1, cbar_kws={"shrink": 0.8})
    ax.set_title('Correlation Matrix', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()
    
    print("ç›¸é–¢ä¿‚æ•°:")
    print(correlation_matrix)
    print(f"\nLSPRæ³¢é•·ã¨ã‚µã‚¤ã‚ºã®ç›¸é–¢: {correlation_matrix.loc['lspr_nm', 'size_nm']:.3f}")
    

### ã€ä¾‹5ã€‘3Dãƒ—ãƒ­ãƒƒãƒˆï¼šã‚µã‚¤ã‚º vs æ¸©åº¦ vs LSPR
    
    
    from mpl_toolkits.mplot3d import Axes3D
    
    # 3Dæ•£å¸ƒå›³
    fig = plt.figure(figsize=(12, 8))
    ax = fig.add_subplot(111, projection='3d')
    
    # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—
    scatter = ax.scatter(data['size_nm'], data['temperature_C'], data['lspr_nm'],
                         c=data['pH'], cmap='viridis', s=50, alpha=0.6, edgecolors='k')
    
    ax.set_xlabel('Size (nm)', fontsize=11)
    ax.set_ylabel('Temperature (Â°C)', fontsize=11)
    ax.set_zlabel('LSPR Wavelength (nm)', fontsize=11)
    ax.set_title('3D Scatter: Size vs Temperature vs LSPR (colored by pH)',
                 fontsize=13, fontweight='bold')
    
    # ã‚«ãƒ©ãƒ¼ãƒãƒ¼
    cbar = plt.colorbar(scatter, ax=ax, shrink=0.5, aspect=5)
    cbar.set_label('pH', fontsize=10)
    
    plt.tight_layout()
    plt.show()
    
    print("3Dãƒ—ãƒ­ãƒƒãƒˆã§å¤šæ¬¡å…ƒã®é–¢ä¿‚ã‚’å¯è¦–åŒ–ã—ã¾ã—ãŸ")
    

* * *

## 3.3 å‰å‡¦ç†ã¨ãƒ‡ãƒ¼ã‚¿åˆ†å‰²

### ã€ä¾‹6ã€‘æ¬ æå€¤å‡¦ç†
    
    
    # æ¬ æå€¤ã‚’äººç‚ºçš„ã«å°å…¥ï¼ˆå®Ÿç¿’ç”¨ï¼‰
    data_with_missing = data.copy()
    np.random.seed(123)
    
    # ãƒ©ãƒ³ãƒ€ãƒ ã«5%ã®æ¬ æå€¤ã‚’å°å…¥
    missing_indices = np.random.choice(data.index, size=int(0.05 * len(data)), replace=False)
    data_with_missing.loc[missing_indices, 'temperature_C'] = np.nan
    
    print("=" * 60)
    print("æ¬ æå€¤ã®ç¢ºèª")
    print("=" * 60)
    print(f"æ¬ æå€¤ã®æ•°:\n{data_with_missing.isnull().sum()}")
    
    # æ¬ æå€¤ã®å‡¦ç†æ–¹æ³•1: å¹³å‡å€¤ã§è£œå®Œ
    data_filled_mean = data_with_missing.fillna(data_with_missing.mean())
    
    # æ¬ æå€¤ã®å‡¦ç†æ–¹æ³•2: ä¸­å¤®å€¤ã§è£œå®Œ
    data_filled_median = data_with_missing.fillna(data_with_missing.median())
    
    # æ¬ æå€¤ã®å‡¦ç†æ–¹æ³•3: å‰Šé™¤
    data_dropped = data_with_missing.dropna()
    
    print(f"\nå…ƒã®ãƒ‡ãƒ¼ã‚¿: {len(data_with_missing)}è¡Œ")
    print(f"æ¬ æå€¤å‰Šé™¤å¾Œ: {len(data_dropped)}è¡Œ")
    print(f"å¹³å‡å€¤è£œå®Œå¾Œ: {len(data_filled_mean)}è¡Œï¼ˆæ¬ æå€¤ãªã—ï¼‰")
    
    # ä»¥é™ã®åˆ†æã§ã¯å…ƒã®ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¬ æå€¤ãªã—ï¼‰ã‚’ä½¿ç”¨
    data_clean = data.copy()
    print("\nâ†’ ä»¥é™ã¯æ¬ æå€¤ã®ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™")
    

### ã€ä¾‹7ã€‘å¤–ã‚Œå€¤æ¤œå‡ºï¼ˆIQRæ³•ï¼‰
    
    
    # IQRï¼ˆå››åˆ†ä½ç¯„å›²ï¼‰æ³•ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º
    def detect_outliers_iqr(series):
        Q1 = series.quantile(0.25)
        Q3 = series.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = (series < lower_bound) | (series > upper_bound)
        return outliers, lower_bound, upper_bound
    
    # ã‚µã‚¤ã‚ºã«ã¤ã„ã¦å¤–ã‚Œå€¤æ¤œå‡º
    outliers, lower, upper = detect_outliers_iqr(data_clean['size_nm'])
    
    print("=" * 60)
    print("å¤–ã‚Œå€¤æ¤œå‡ºï¼ˆIQRæ³•ï¼‰")
    print("=" * 60)
    print(f"æ¤œå‡ºã•ã‚ŒãŸå¤–ã‚Œå€¤ã®æ•°: {outliers.sum()}")
    print(f"ä¸‹é™: {lower:.2f} nm, ä¸Šé™: {upper:.2f} nm")
    
    # å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.boxplot([data_clean['size_nm']], labels=['Size (nm)'], vert=False)
    ax.scatter(data_clean.loc[outliers, 'size_nm'],
               [1] * outliers.sum(), color='red', s=100,
               label=f'Outliers (n={outliers.sum()})', zorder=3)
    ax.set_xlabel('Size (nm)', fontsize=12)
    ax.set_title('Boxplot with Outliers', fontsize=14, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3, axis='x')
    plt.tight_layout()
    plt.show()
    
    print("â†’ å¤–ã‚Œå€¤ã¯é™¤å»ã›ãšã€å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™")
    

### ã€ä¾‹8ã€‘ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆStandardScalerï¼‰
    
    
    from sklearn.preprocessing import StandardScaler
    
    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
    X = data_clean[['size_nm', 'temperature_C', 'pH']]
    y = data_clean['lspr_nm']
    
    # StandardScalerï¼ˆå¹³å‡0ã€æ¨™æº–åå·®1ã«æ¨™æº–åŒ–ï¼‰
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰å¾Œã®æ¯”è¼ƒ
    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)
    
    print("=" * 60)
    print("ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰ã®çµ±è¨ˆé‡")
    print("=" * 60)
    print(X.describe())
    
    print("\n" + "=" * 60)
    print("ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œã®çµ±è¨ˆé‡ï¼ˆå¹³å‡â‰ˆ0ã€æ¨™æº–åå·®â‰ˆ1ï¼‰")
    print("=" * 60)
    print(X_scaled_df.describe())
    
    print("\nâ†’ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ã‚ˆã‚Šå„ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒ«ãŒçµ±ä¸€ã•ã‚Œã¾ã—ãŸ")
    

### ã€ä¾‹9ã€‘è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²
    
    
    from sklearn.model_selection import train_test_split
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²ï¼ˆ80:20ï¼‰
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42
    )
    
    print("=" * 60)
    print("ãƒ‡ãƒ¼ã‚¿åˆ†å‰²")
    print("=" * 60)
    print(f"å…¨ãƒ‡ãƒ¼ã‚¿æ•°: {len(X)}ã‚µãƒ³ãƒ—ãƒ«")
    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train)}ã‚µãƒ³ãƒ—ãƒ« ({len(X_train)/len(X)*100:.1f}%)")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)}ã‚µãƒ³ãƒ—ãƒ« ({len(X_test)/len(X)*100:.1f}%)")
    
    print("\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆé‡:")
    print(pd.DataFrame(X_train, columns=X.columns).describe())
    

* * *

## 3.4 å›å¸°ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒŠãƒç²’å­ç‰©æ€§äºˆæ¸¬

ç›®æ¨™ï¼šã‚µã‚¤ã‚ºã€æ¸©åº¦ã€pHã‹ã‚‰LSPRæ³¢é•·ã‚’äºˆæ¸¬

### ã€ä¾‹10ã€‘ç·šå½¢å›å¸°ï¼ˆLinear Regressionï¼‰
    
    
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
    
    # ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
    model_lr = LinearRegression()
    model_lr.fit(X_train, y_train)
    
    # äºˆæ¸¬
    y_train_pred_lr = model_lr.predict(X_train)
    y_test_pred_lr = model_lr.predict(X_test)
    
    # è©•ä¾¡æŒ‡æ¨™
    r2_train_lr = r2_score(y_train, y_train_pred_lr)
    r2_test_lr = r2_score(y_test, y_test_pred_lr)
    rmse_test_lr = np.sqrt(mean_squared_error(y_test, y_test_pred_lr))
    mae_test_lr = mean_absolute_error(y_test, y_test_pred_lr)
    
    print("=" * 60)
    print("ç·šå½¢å›å¸°ï¼ˆLinear Regressionï¼‰")
    print("=" * 60)
    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_train_lr:.4f}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_test_lr:.4f}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RMSE: {rmse_test_lr:.4f} nm")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ MAE: {mae_test_lr:.4f} nm")
    
    # å›å¸°ä¿‚æ•°
    print("\nå›å¸°ä¿‚æ•°:")
    for name, coef in zip(X.columns, model_lr.coef_):
        print(f"  {name}: {coef:.4f}")
    print(f"  åˆ‡ç‰‡: {model_lr.intercept_:.4f}")
    
    # æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # äºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤
    axes[0].scatter(y_test, y_test_pred_lr, alpha=0.6, edgecolors='k')
    axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                 'r--', lw=2, label='Perfect Prediction')
    axes[0].set_xlabel('Actual LSPR (nm)', fontsize=11)
    axes[0].set_ylabel('Predicted LSPR (nm)', fontsize=11)
    axes[0].set_title(f'Linear Regression (RÂ² = {r2_test_lr:.3f})', fontsize=12, fontweight='bold')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
    residuals = y_test - y_test_pred_lr
    axes[1].scatter(y_test_pred_lr, residuals, alpha=0.6, edgecolors='k')
    axes[1].axhline(y=0, color='r', linestyle='--', lw=2)
    axes[1].set_xlabel('Predicted LSPR (nm)', fontsize=11)
    axes[1].set_ylabel('Residuals (nm)', fontsize=11)
    axes[1].set_title('Residual Plot', fontsize=12, fontweight='bold')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    

### ã€ä¾‹11ã€‘ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°ï¼ˆRandom Forestï¼‰
    
    
    from sklearn.ensemble import RandomForestRegressor
    
    # ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°ãƒ¢ãƒ‡ãƒ«
    model_rf = RandomForestRegressor(n_estimators=100, max_depth=10,
                                     random_state=42, n_jobs=-1)
    model_rf.fit(X_train, y_train)
    
    # äºˆæ¸¬
    y_train_pred_rf = model_rf.predict(X_train)
    y_test_pred_rf = model_rf.predict(X_test)
    
    # è©•ä¾¡
    r2_train_rf = r2_score(y_train, y_train_pred_rf)
    r2_test_rf = r2_score(y_test, y_test_pred_rf)
    rmse_test_rf = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))
    mae_test_rf = mean_absolute_error(y_test, y_test_pred_rf)
    
    print("=" * 60)
    print("ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°ï¼ˆRandom Forestï¼‰")
    print("=" * 60)
    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_train_rf:.4f}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_test_rf:.4f}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RMSE: {rmse_test_rf:.4f} nm")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ MAE: {mae_test_rf:.4f} nm")
    
    # ç‰¹å¾´é‡é‡è¦åº¦
    feature_importance = pd.DataFrame({
        'Feature': X.columns,
        'Importance': model_rf.feature_importances_
    }).sort_values('Importance', ascending=False)
    
    print("\nç‰¹å¾´é‡é‡è¦åº¦:")
    print(feature_importance)
    
    # ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(8, 5))
    ax.barh(feature_importance['Feature'], feature_importance['Importance'],
            color='steelblue', edgecolor='black')
    ax.set_xlabel('Importance', fontsize=12)
    ax.set_title('Feature Importance (Random Forest)', fontsize=13, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='x')
    plt.tight_layout()
    plt.show()
    

### ã€ä¾‹12ã€‘å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼ˆLightGBMï¼‰
    
    
    import lightgbm as lgb
    
    # LightGBMãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'n_estimators': 200,
        'random_state': 42,
        'verbose': -1
    }
    
    model_lgb = lgb.LGBMRegressor(**params)
    model_lgb.fit(X_train, y_train)
    
    # äºˆæ¸¬
    y_train_pred_lgb = model_lgb.predict(X_train)
    y_test_pred_lgb = model_lgb.predict(X_test)
    
    # è©•ä¾¡
    r2_train_lgb = r2_score(y_train, y_train_pred_lgb)
    r2_test_lgb = r2_score(y_test, y_test_pred_lgb)
    rmse_test_lgb = np.sqrt(mean_squared_error(y_test, y_test_pred_lgb))
    mae_test_lgb = mean_absolute_error(y_test, y_test_pred_lgb)
    
    print("=" * 60)
    print("å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼ˆLightGBMï¼‰")
    print("=" * 60)
    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_train_lgb:.4f}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_test_lgb:.4f}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RMSE: {rmse_test_lgb:.4f} nm")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ MAE: {mae_test_lgb:.4f} nm")
    
    # äºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤ãƒ—ãƒ­ãƒƒãƒˆ
    fig, ax = plt.subplots(figsize=(8, 6))
    ax.scatter(y_test, y_test_pred_lgb, alpha=0.6, edgecolors='k', s=60)
    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
            'r--', lw=2, label='Perfect Prediction')
    ax.set_xlabel('Actual LSPR (nm)', fontsize=12)
    ax.set_ylabel('Predicted LSPR (nm)', fontsize=12)
    ax.set_title(f'LightGBM Prediction (RÂ² = {r2_test_lgb:.3f})',
                 fontsize=13, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    

### ã€ä¾‹13ã€‘ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼å›å¸°ï¼ˆSVRï¼‰
    
    
    from sklearn.svm import SVR
    
    # SVRãƒ¢ãƒ‡ãƒ«ï¼ˆRBFã‚«ãƒ¼ãƒãƒ«ï¼‰
    model_svr = SVR(kernel='rbf', C=10, gamma='scale', epsilon=0.1)
    model_svr.fit(X_train, y_train)
    
    # äºˆæ¸¬
    y_train_pred_svr = model_svr.predict(X_train)
    y_test_pred_svr = model_svr.predict(X_test)
    
    # è©•ä¾¡
    r2_train_svr = r2_score(y_train, y_train_pred_svr)
    r2_test_svr = r2_score(y_test, y_test_pred_svr)
    rmse_test_svr = np.sqrt(mean_squared_error(y_test, y_test_pred_svr))
    mae_test_svr = mean_absolute_error(y_test, y_test_pred_svr)
    
    print("=" * 60)
    print("ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼å›å¸°ï¼ˆSVRï¼‰")
    print("=" * 60)
    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_train_svr:.4f}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_test_svr:.4f}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RMSE: {rmse_test_svr:.4f} nm")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ MAE: {mae_test_svr:.4f} nm")
    print(f"ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼æ•°: {len(model_svr.support_)}")
    

### ã€ä¾‹14ã€‘ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆMLP Regressorï¼‰
    
    
    from sklearn.neural_network import MLPRegressor
    
    # MLPãƒ¢ãƒ‡ãƒ«
    model_mlp = MLPRegressor(hidden_layer_sizes=(100, 50),
                             activation='relu',
                             solver='adam',
                             alpha=0.001,
                             max_iter=500,
                             random_state=42,
                             early_stopping=True,
                             validation_fraction=0.1,
                             verbose=False)
    
    model_mlp.fit(X_train, y_train)
    
    # äºˆæ¸¬
    y_train_pred_mlp = model_mlp.predict(X_train)
    y_test_pred_mlp = model_mlp.predict(X_test)
    
    # è©•ä¾¡
    r2_train_mlp = r2_score(y_train, y_train_pred_mlp)
    r2_test_mlp = r2_score(y_test, y_test_pred_mlp)
    rmse_test_mlp = np.sqrt(mean_squared_error(y_test, y_test_pred_mlp))
    mae_test_mlp = mean_absolute_error(y_test, y_test_pred_mlp)
    
    print("=" * 60)
    print("ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆMLP Regressorï¼‰")
    print("=" * 60)
    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_train_mlp:.4f}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_test_mlp:.4f}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RMSE: {rmse_test_mlp:.4f} nm")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ MAE: {mae_test_mlp:.4f} nm")
    print(f"åå¾©å›æ•°: {model_mlp.n_iter_}")
    print(f"éš ã‚Œå±¤ã®æ§‹é€ : {model_mlp.hidden_layer_sizes}")
    

### ã€ä¾‹15ã€‘ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ
    
    
    # å…¨ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ã¾ã¨ã‚ã‚‹
    results = pd.DataFrame({
        'Model': ['Linear Regression', 'Random Forest', 'LightGBM', 'SVR', 'MLP'],
        'RÂ² (Train)': [r2_train_lr, r2_train_rf, r2_train_lgb, r2_train_svr, r2_train_mlp],
        'RÂ² (Test)': [r2_test_lr, r2_test_rf, r2_test_lgb, r2_test_svr, r2_test_mlp],
        'RMSE (Test)': [rmse_test_lr, rmse_test_rf, rmse_test_lgb, rmse_test_svr, rmse_test_mlp],
        'MAE (Test)': [mae_test_lr, mae_test_rf, mae_test_lgb, mae_test_svr, mae_test_mlp]
    })
    
    results['Overfit'] = results['RÂ² (Train)'] - results['RÂ² (Test)']
    
    print("=" * 80)
    print("å…¨ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æ¯”è¼ƒ")
    print("=" * 80)
    print(results.to_string(index=False))
    
    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å®š
    best_model_idx = results['RÂ² (Test)'].idxmax()
    best_model_name = results.loc[best_model_idx, 'Model']
    best_r2 = results.loc[best_model_idx, 'RÂ² (Test)']
    
    print(f"\næœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model_name} (RÂ² = {best_r2:.4f})")
    
    # å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # RÂ²ã‚¹ã‚³ã‚¢æ¯”è¼ƒ
    x_pos = np.arange(len(results))
    axes[0].bar(x_pos, results['RÂ² (Test)'], alpha=0.7, color='steelblue',
                edgecolor='black', label='Test RÂ²')
    axes[0].set_xticks(x_pos)
    axes[0].set_xticklabels(results['Model'], rotation=15, ha='right')
    axes[0].set_ylabel('RÂ² Score', fontsize=12)
    axes[0].set_title('Model Comparison: RÂ² Score', fontsize=13, fontweight='bold')
    axes[0].grid(True, alpha=0.3, axis='y')
    axes[0].legend()
    
    # RMSEæ¯”è¼ƒ
    axes[1].bar(x_pos, results['RMSE (Test)'], alpha=0.7, color='coral',
                edgecolor='black', label='Test RMSE')
    axes[1].set_xticks(x_pos)
    axes[1].set_xticklabels(results['Model'], rotation=15, ha='right')
    axes[1].set_ylabel('RMSE (nm)', fontsize=12)
    axes[1].set_title('Model Comparison: RMSE', fontsize=13, fontweight='bold')
    axes[1].grid(True, alpha=0.3, axis='y')
    axes[1].legend()
    
    plt.tight_layout()
    plt.show()
    

* * *

## 3.5 é‡å­ãƒ‰ãƒƒãƒˆç™ºå…‰æ³¢é•·äºˆæ¸¬

### ã€ä¾‹16ã€‘ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼šCdSeé‡å­ãƒ‰ãƒƒãƒˆ

CdSeé‡å­ãƒ‰ãƒƒãƒˆã®ç™ºå…‰æ³¢é•·ã¯ã€Brusæ–¹ç¨‹å¼ã«åŸºã¥ãã‚µã‚¤ã‚ºã«ä¾å­˜ã—ã¾ã™ã€‚
    
    
    # CdSeé‡å­ãƒ‰ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
    np.random.seed(100)
    
    n_qd_samples = 150
    
    # é‡å­ãƒ‰ãƒƒãƒˆã®ã‚µã‚¤ã‚ºï¼ˆ2-10 nmï¼‰
    size_qd = np.random.uniform(2, 10, n_qd_samples)
    
    # Brusæ–¹ç¨‹å¼ã®ç°¡æ˜“è¿‘ä¼¼: emission = 520 + 130/(size^0.8) + noise
    emission = 520 + 130 / (size_qd ** 0.8) + np.random.normal(0, 10, n_qd_samples)
    
    # åˆæˆæ¡ä»¶
    synthesis_time = np.random.uniform(10, 120, n_qd_samples)  # åˆ†
    precursor_ratio = np.random.uniform(0.5, 2.0, n_qd_samples)  # ãƒ¢ãƒ«æ¯”
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    data_qd = pd.DataFrame({
        'size_nm': size_qd,
        'emission_nm': emission,
        'synthesis_time_min': synthesis_time,
        'precursor_ratio': precursor_ratio
    })
    
    print("=" * 60)
    print("CdSeé‡å­ãƒ‰ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆå®Œäº†")
    print("=" * 60)
    print(data_qd.head(10))
    print("\nåŸºæœ¬çµ±è¨ˆé‡:")
    print(data_qd.describe())
    
    # ã‚µã‚¤ã‚ºã¨ç™ºå…‰æ³¢é•·ã®é–¢ä¿‚ãƒ—ãƒ­ãƒƒãƒˆ
    fig, ax = plt.subplots(figsize=(10, 6))
    scatter = ax.scatter(data_qd['size_nm'], data_qd['emission_nm'],
                         c=data_qd['synthesis_time_min'], cmap='plasma',
                         s=80, alpha=0.7, edgecolors='k')
    ax.set_xlabel('Quantum Dot Size (nm)', fontsize=12)
    ax.set_ylabel('Emission Wavelength (nm)', fontsize=12)
    ax.set_title('CdSe Quantum Dot: Size vs Emission Wavelength',
                 fontsize=13, fontweight='bold')
    cbar = plt.colorbar(scatter, ax=ax)
    cbar.set_label('Synthesis Time (min)', fontsize=10)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    

### ã€ä¾‹17ã€‘é‡å­ãƒ‰ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆLightGBMï¼‰
    
    
    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
    X_qd = data_qd[['size_nm', 'synthesis_time_min', 'precursor_ratio']]
    y_qd = data_qd['emission_nm']
    
    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    scaler_qd = StandardScaler()
    X_qd_scaled = scaler_qd.fit_transform(X_qd)
    
    # è¨“ç·´/ãƒ†ã‚¹ãƒˆåˆ†å‰²
    X_qd_train, X_qd_test, y_qd_train, y_qd_test = train_test_split(
        X_qd_scaled, y_qd, test_size=0.2, random_state=42
    )
    
    # LightGBMãƒ¢ãƒ‡ãƒ«
    model_qd = lgb.LGBMRegressor(
        objective='regression',
        num_leaves=31,
        learning_rate=0.05,
        n_estimators=200,
        random_state=42,
        verbose=-1
    )
    
    model_qd.fit(X_qd_train, y_qd_train)
    
    # äºˆæ¸¬
    y_qd_train_pred = model_qd.predict(X_qd_train)
    y_qd_test_pred = model_qd.predict(X_qd_test)
    
    # è©•ä¾¡
    r2_qd_train = r2_score(y_qd_train, y_qd_train_pred)
    r2_qd_test = r2_score(y_qd_test, y_qd_test_pred)
    rmse_qd = np.sqrt(mean_squared_error(y_qd_test, y_qd_test_pred))
    mae_qd = mean_absolute_error(y_qd_test, y_qd_test_pred)
    
    print("=" * 60)
    print("é‡å­ãƒ‰ãƒƒãƒˆç™ºå…‰æ³¢é•·äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ï¼ˆLightGBMï¼‰")
    print("=" * 60)
    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_qd_train:.4f}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_qd_test:.4f}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RMSE: {rmse_qd:.4f} nm")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ MAE: {mae_qd:.4f} nm")
    

### ã€ä¾‹18ã€‘äºˆæ¸¬çµæœã®å¯è¦–åŒ–
    
    
    # äºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤ãƒ—ãƒ­ãƒƒãƒˆï¼ˆä¿¡é ¼åŒºé–“ä»˜ãï¼‰
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ—ãƒ­ãƒƒãƒˆ
    axes[0].scatter(y_qd_test, y_qd_test_pred, alpha=0.6, s=80,
                    edgecolors='k', label='Test Data')
    axes[0].plot([y_qd_test.min(), y_qd_test.max()],
                 [y_qd_test.min(), y_qd_test.max()],
                 'r--', lw=2, label='Perfect Prediction')
    
    # Â±10 nm ã®ç¯„å›²ã‚’è¡¨ç¤º
    axes[0].fill_between([y_qd_test.min(), y_qd_test.max()],
                         [y_qd_test.min()-10, y_qd_test.max()-10],
                         [y_qd_test.min()+10, y_qd_test.max()+10],
                         alpha=0.2, color='gray', label='Â±10 nm')
    
    axes[0].set_xlabel('Actual Emission (nm)', fontsize=12)
    axes[0].set_ylabel('Predicted Emission (nm)', fontsize=12)
    axes[0].set_title(f'QD Emission Prediction (RÂ² = {r2_qd_test:.3f})',
                      fontsize=13, fontweight='bold')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # ã‚µã‚¤ã‚ºåˆ¥ã®äºˆæ¸¬ç²¾åº¦
    size_bins = [2, 4, 6, 8, 10]
    size_labels = ['2-4 nm', '4-6 nm', '6-8 nm', '8-10 nm']
    data_qd_test = pd.DataFrame({
        'size': X_qd.iloc[y_qd_test.index]['size_nm'].values,
        'actual': y_qd_test.values,
        'predicted': y_qd_test_pred
    })
    data_qd_test['size_bin'] = pd.cut(data_qd_test['size'], bins=size_bins, labels=size_labels)
    data_qd_test['error'] = np.abs(data_qd_test['actual'] - data_qd_test['predicted'])
    
    # ã‚µã‚¤ã‚ºãƒ“ãƒ³ã”ã¨ã®å¹³å‡èª¤å·®
    error_by_size = data_qd_test.groupby('size_bin')['error'].mean()
    
    axes[1].bar(range(len(error_by_size)), error_by_size.values,
                color='coral', edgecolor='black', alpha=0.7)
    axes[1].set_xticks(range(len(error_by_size)))
    axes[1].set_xticklabels(error_by_size.index)
    axes[1].set_ylabel('Mean Absolute Error (nm)', fontsize=12)
    axes[1].set_xlabel('QD Size Range', fontsize=12)
    axes[1].set_title('Prediction Error by QD Size', fontsize=13, fontweight='bold')
    axes[1].grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.show()
    
    print(f"\nå…¨ä½“ã®å¹³å‡çµ¶å¯¾èª¤å·®: {mae_qd:.2f} nm")
    print("ã‚µã‚¤ã‚ºåˆ¥ã®å¹³å‡çµ¶å¯¾èª¤å·®:")
    print(error_by_size)
    

* * *

## 3.6 ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ

### ã€ä¾‹19ã€‘ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆLightGBMï¼‰
    
    
    # LightGBMãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆã‚²ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹ï¼‰
    importance_gain = model_lgb.feature_importances_
    importance_df = pd.DataFrame({
        'Feature': X.columns,
        'Importance': importance_gain
    }).sort_values('Importance', ascending=False)
    
    print("=" * 60)
    print("ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆLightGBMï¼‰")
    print("=" * 60)
    print(importance_df)
    
    # å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(8, 5))
    colors = ['steelblue', 'coral', 'lightgreen']
    ax.barh(importance_df['Feature'], importance_df['Importance'],
            color=colors, edgecolor='black')
    ax.set_xlabel('Feature Importance (Gain)', fontsize=12)
    ax.set_title('Feature Importance: LSPR Prediction',
                 fontsize=13, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='x')
    plt.tight_layout()
    plt.show()
    
    print(f"\næœ€ã‚‚é‡è¦ãªç‰¹å¾´é‡: {importance_df.iloc[0]['Feature']}")
    

### ã€ä¾‹20ã€‘SHAPåˆ†æï¼šäºˆæ¸¬è§£é‡ˆ
    
    
    import shap
    
    # SHAP Explainerã®ä½œæˆ
    explainer = shap.Explainer(model_lgb, X_train)
    shap_values = explainer(X_test)
    
    print("=" * 60)
    print("SHAPåˆ†æ")
    print("=" * 60)
    print("SHAPå€¤ã®è¨ˆç®—å®Œäº†")
    print(f"SHAPå€¤ã®å½¢çŠ¶: {shap_values.values.shape}")
    
    # SHAP Summary Plot
    fig, ax = plt.subplots(figsize=(10, 6))
    shap.summary_plot(shap_values, X_test, feature_names=X.columns, show=False)
    plt.title('SHAP Summary Plot: Feature Impact on LSPR Prediction',
              fontsize=13, fontweight='bold', pad=20)
    plt.tight_layout()
    plt.show()
    
    # SHAP Dependence Plotï¼ˆæœ€ã‚‚é‡è¦ãªç‰¹å¾´é‡ï¼‰
    top_feature_idx = importance_df.index[0]
    top_feature_name = X.columns[top_feature_idx]
    
    fig, ax = plt.subplots(figsize=(10, 6))
    shap.dependence_plot(top_feature_idx, shap_values.values, X_test,
                         feature_names=X.columns, show=False)
    plt.title(f'SHAP Dependence Plot: {top_feature_name}',
              fontsize=13, fontweight='bold')
    plt.tight_layout()
    plt.show()
    
    print(f"\nSHAPåˆ†æã«ã‚ˆã‚Šã€{top_feature_name}ãŒLSPRæ³¢é•·äºˆæ¸¬ã«æœ€ã‚‚å½±éŸ¿ã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã¾ã—ãŸ")
    

* * *

## 3.7 ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚‹ãƒŠãƒææ–™è¨­è¨ˆ

ç›®æ¨™ï¼šç›®æ¨™LSPRæ³¢é•·ï¼ˆ550 nmï¼‰ã‚’å®Ÿç¾ã™ã‚‹æœ€é©ãªåˆæˆæ¡ä»¶ã‚’æ¢ç´¢

### ã€ä¾‹21ã€‘æ¢ç´¢ç©ºé–“ã®å®šç¾©
    
    
    from skopt.space import Real
    
    # æ¢ç´¢ç©ºé–“ã®å®šç¾©
    # ã‚µã‚¤ã‚º: 10-40 nmã€æ¸©åº¦: 20-80Â°Cã€pH: 4-10
    search_space = [
        Real(10, 40, name='size_nm'),
        Real(20, 80, name='temperature_C'),
        Real(4, 10, name='pH')
    ]
    
    print("=" * 60)
    print("ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ï¼šæ¢ç´¢ç©ºé–“ã®å®šç¾©")
    print("=" * 60)
    for dim in search_space:
        print(f"  {dim.name}: [{dim.bounds[0]}, {dim.bounds[1]}]")
    
    print("\nç›®æ¨™: LSPRæ³¢é•· = 550 nm ã‚’å®Ÿç¾ã™ã‚‹æ¡ä»¶ã‚’æ¢ç´¢")
    

### ã€ä¾‹22ã€‘ç›®çš„é–¢æ•°ã®è¨­å®š
    
    
    # ç›®çš„é–¢æ•°ï¼šäºˆæ¸¬LSPRæ³¢é•·ã¨ç›®æ¨™æ³¢é•·ï¼ˆ550 nmï¼‰ã®å·®ã®çµ¶å¯¾å€¤ã‚’æœ€å°åŒ–
    target_lspr = 550.0
    
    def objective_function(params):
        """
        ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ç›®çš„é–¢æ•°
    
        Parameters:
        -----------
        params : list
            [size_nm, temperature_C, pH]
    
        Returns:
        --------
        float
            ç›®æ¨™æ³¢é•·ã¨ã®èª¤å·®ï¼ˆæœ€å°åŒ–ã™ã‚‹å€¤ï¼‰
        """
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        size, temp, ph = params
    
        # ç‰¹å¾´é‡ã‚’æ§‹ç¯‰ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°é©ç”¨ï¼‰
        features = np.array([[size, temp, ph]])
        features_scaled = scaler.transform(features)
    
        # LSPRæ³¢é•·ã‚’äºˆæ¸¬
        predicted_lspr = model_lgb.predict(features_scaled)[0]
    
        # ç›®æ¨™æ³¢é•·ã¨ã®èª¤å·®ï¼ˆçµ¶å¯¾å€¤ï¼‰
        error = abs(predicted_lspr - target_lspr)
    
        return error
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_params = [20.0, 50.0, 7.0]
    test_error = objective_function(test_params)
    print(f"\nãƒ†ã‚¹ãƒˆå®Ÿè¡Œ:")
    print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: size={test_params[0]} nm, temp={test_params[1]}Â°C, pH={test_params[2]}")
    print(f"  ç›®çš„é–¢æ•°å€¤ï¼ˆèª¤å·®ï¼‰: {test_error:.4f} nm")
    

### ã€ä¾‹23ã€‘ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®å®Ÿè¡Œï¼ˆscikit-optimizeï¼‰
    
    
    from skopt import gp_minimize
    from skopt.plots import plot_convergence, plot_objective
    
    # ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®å®Ÿè¡Œ
    print("\n" + "=" * 60)
    print("ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®å®Ÿè¡Œä¸­...")
    print("=" * 60)
    
    result = gp_minimize(
        func=objective_function,
        dimensions=search_space,
        n_calls=50,  # è©•ä¾¡å›æ•°
        n_initial_points=10,  # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å›æ•°
        random_state=42,
        verbose=False
    )
    
    print("æœ€é©åŒ–å®Œäº†ï¼")
    print("\n" + "=" * 60)
    print("æœ€é©åŒ–çµæœ")
    print("=" * 60)
    print(f"æœ€å°ç›®çš„é–¢æ•°å€¤ï¼ˆèª¤å·®ï¼‰: {result.fun:.4f} nm")
    print(f"\næœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
    print(f"  ã‚µã‚¤ã‚º: {result.x[0]:.2f} nm")
    print(f"  æ¸©åº¦: {result.x[1]:.2f} Â°C")
    print(f"  pH: {result.x[2]:.2f}")
    
    # æœ€é©æ¡ä»¶ã§ã®äºˆæ¸¬LSPRæ³¢é•·ã‚’è¨ˆç®—
    optimal_features = np.array([result.x])
    optimal_features_scaled = scaler.transform(optimal_features)
    predicted_optimal_lspr = model_lgb.predict(optimal_features_scaled)[0]
    
    print(f"\näºˆæ¸¬ã•ã‚Œã‚‹LSPRæ³¢é•·: {predicted_optimal_lspr:.2f} nm")
    print(f"ç›®æ¨™LSPRæ³¢é•·: {target_lspr} nm")
    print(f"é”æˆç²¾åº¦: {abs(predicted_optimal_lspr - target_lspr):.2f} nm")
    

### ã€ä¾‹24ã€‘æœ€é©åŒ–çµæœã®å¯è¦–åŒ–
    
    
    # æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã®å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # åæŸãƒ—ãƒ­ãƒƒãƒˆ
    plot_convergence(result, ax=axes[0])
    axes[0].set_title('Convergence Plot', fontsize=13, fontweight='bold')
    axes[0].set_ylabel('Objective Value (Error, nm)', fontsize=11)
    axes[0].set_xlabel('Number of Evaluations', fontsize=11)
    axes[0].grid(True, alpha=0.3)
    
    # è©•ä¾¡å±¥æ­´ã®ãƒ—ãƒ­ãƒƒãƒˆ
    iterations = range(1, len(result.func_vals) + 1)
    axes[1].plot(iterations, result.func_vals, 'o-', alpha=0.6, label='Evaluation')
    axes[1].plot(iterations, np.minimum.accumulate(result.func_vals),
                 'r-', linewidth=2, label='Best So Far')
    axes[1].set_xlabel('Iteration', fontsize=11)
    axes[1].set_ylabel('Objective Value (Error, nm)', fontsize=11)
    axes[1].set_title('Optimization Progress', fontsize=13, fontweight='bold')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    

### ã€ä¾‹25ã€‘åæŸãƒ—ãƒ­ãƒƒãƒˆ
    
    
    # è©³ç´°ãªåæŸãƒ—ãƒ­ãƒƒãƒˆï¼ˆæœ€è‰¯å€¤ã®æ¨ç§»ï¼‰
    fig, ax = plt.subplots(figsize=(10, 6))
    
    cumulative_min = np.minimum.accumulate(result.func_vals)
    iterations = np.arange(1, len(cumulative_min) + 1)
    
    ax.plot(iterations, cumulative_min, 'b-', linewidth=2, marker='o',
            markersize=4, label='Best Error')
    ax.axhline(y=result.fun, color='r', linestyle='--', linewidth=2,
               label=f'Final Best: {result.fun:.2f} nm')
    ax.fill_between(iterations, 0, cumulative_min, alpha=0.2, color='blue')
    
    ax.set_xlabel('Iteration', fontsize=12)
    ax.set_ylabel('Minimum Error (nm)', fontsize=12)
    ax.set_title('Bayesian Optimization: Convergence to Optimal Solution',
                 fontsize=13, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print(f"\n{len(result.func_vals)}å›ã®è©•ä¾¡ã§æœ€é©è§£ã«åæŸã—ã¾ã—ãŸ")
    print(f"åˆæœŸè©•ä¾¡ã§ã®æœ€è‰¯èª¤å·®: {result.func_vals[0]:.2f} nm")
    print(f"æœ€çµ‚çš„ãªæœ€è‰¯èª¤å·®: {result.fun:.2f} nm")
    print(f"æ”¹å–„ç‡: {(1 - result.fun/result.func_vals[0])*100:.1f}%")
    

* * *

## 3.8 å¤šç›®çš„æœ€é©åŒ–ï¼šã‚µã‚¤ã‚ºã¨ç™ºå…‰åŠ¹ç‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

### ã€ä¾‹26ã€‘Paretoæœ€é©åŒ–ï¼ˆNSGA-IIï¼‰

å¤šç›®çš„æœ€é©åŒ–ã§ã¯ã€è¤‡æ•°ã®ç›®çš„ã‚’åŒæ™‚ã«æœ€é©åŒ–ã—ã¾ã™ã€‚ã“ã“ã§ã¯ã€é‡å­ãƒ‰ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã‚’æœ€å°åŒ–ã—ã¤ã¤ã€ç™ºå…‰åŠ¹ç‡ï¼ˆä»®æƒ³çš„ãªæŒ‡æ¨™ï¼‰ã‚’æœ€å¤§åŒ–ã—ã¾ã™ã€‚
    
    
    # pymooã‚’ä½¿ç”¨ã—ãŸå¤šç›®çš„æœ€é©åŒ–
    try:
        from pymoo.core.problem import Problem
        from pymoo.algorithms.moo.nsga2 import NSGA2
        from pymoo.optimize import minimize as pymoo_minimize
        from pymoo.operators.crossover.sbx import SBX
        from pymoo.operators.mutation.pm import PM
        from pymoo.operators.sampling.rnd import FloatRandomSampling
    
        # å¤šç›®çš„æœ€é©åŒ–å•é¡Œã®å®šç¾©
        class QuantumDotOptimization(Problem):
            def __init__(self):
                super().__init__(
                    n_var=3,  # å¤‰æ•°æ•°ï¼ˆsize, synthesis_time, precursor_ratioï¼‰
                    n_obj=2,  # ç›®çš„é–¢æ•°æ•°ï¼ˆsizeæœ€å°åŒ–ã€emissionåŠ¹ç‡æœ€å¤§åŒ–ï¼‰
                    n_constr=0,  # åˆ¶ç´„ãªã—
                    xl=np.array([2.0, 10.0, 0.5]),  # ä¸‹é™
                    xu=np.array([10.0, 120.0, 2.0])  # ä¸Šé™
                )
    
            def _evaluate(self, X, out, *args, **kwargs):
                # ç›®çš„é–¢æ•°1: ã‚µã‚¤ã‚ºã®æœ€å°åŒ–
                obj1 = X[:, 0]  # size
    
                # ç›®çš„é–¢æ•°2: ç™ºå…‰åŠ¹ç‡ã®æœ€å¤§åŒ–ï¼ˆè² ã®å€¤ã§æœ€å°åŒ–å•é¡Œã«å¤‰æ›ï¼‰
                # åŠ¹ç‡ã¯ä»®æƒ³çš„ã«ã€emission wavelengthãŒ550 nmã«è¿‘ã„ã»ã©é«˜ã„ã¨ä»®å®š
                features = X  # [size, synthesis_time, precursor_ratio]
                features_scaled = scaler_qd.transform(features)
                predicted_emission = model_qd.predict(features_scaled)
    
                # åŠ¹ç‡ï¼š550 nmã‹ã‚‰ã®ãšã‚ŒãŒå°ã•ã„ã»ã©é«˜ã„ï¼ˆè² ã®å€¤ã§æœ€å¤§åŒ–â†’æœ€å°åŒ–ï¼‰
                efficiency = -np.abs(predicted_emission - 550)
                obj2 = -efficiency  # æœ€å¤§åŒ–ã‚’æœ€å°åŒ–å•é¡Œã«å¤‰æ›
    
                out["F"] = np.column_stack([obj1, obj2])
    
        # å•é¡Œã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
        problem = QuantumDotOptimization()
    
        # NSGA-IIã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
        algorithm = NSGA2(
            pop_size=40,
            sampling=FloatRandomSampling(),
            crossover=SBX(prob=0.9, eta=15),
            mutation=PM(eta=20),
            eliminate_duplicates=True
        )
    
        # æœ€é©åŒ–å®Ÿè¡Œ
        print("=" * 60)
        print("å¤šç›®çš„æœ€é©åŒ–ï¼ˆNSGA-IIï¼‰å®Ÿè¡Œä¸­...")
        print("=" * 60)
    
        res = pymoo_minimize(
            problem,
            algorithm,
            ('n_gen', 50),  # ä¸–ä»£æ•°
            seed=42,
            verbose=False
        )
    
        print("å¤šç›®çš„æœ€é©åŒ–å®Œäº†ï¼")
        print(f"\nãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ã®æ•°: {len(res.F)}")
    
        # ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ã®è¡¨ç¤ºï¼ˆä¸Šä½5ã¤ï¼‰
        print("\nä»£è¡¨çš„ãªãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ï¼ˆä¸Šä½5ã¤ï¼‰:")
        pareto_solutions = pd.DataFrame({
            'Size (nm)': res.X[:, 0],
            'Synthesis Time (min)': res.X[:, 1],
            'Precursor Ratio': res.X[:, 2],
            'Obj1: Size': res.F[:, 0],
            'Obj2: -Efficiency': res.F[:, 1]
        }).head(5)
        print(pareto_solutions.to_string(index=False))
    
        PYMOO_AVAILABLE = True
    
    except ImportError:
        print("=" * 60)
        print("pymooãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("=" * 60)
        print("å¤šç›®çš„æœ€é©åŒ–ã«ã¯pymooãŒå¿…è¦ã§ã™:")
        print("  pip install pymoo")
        print("\nä»£ã‚ã‚Šã«ã€ç°¡æ˜“çš„ãªå¤šç›®çš„æœ€é©åŒ–ã®ä¾‹ã‚’è¡¨ç¤ºã—ã¾ã™")
    
        # ç°¡æ˜“çš„ãªã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã«ã‚ˆã‚‹å¤šç›®çš„æœ€é©åŒ–ã®æ¨¡æ“¬
        sizes = np.linspace(2, 10, 20)
        times = np.linspace(10, 120, 20)
        ratios = np.linspace(0.5, 2.0, 20)
    
        # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
        sample_X = []
        sample_F = []
    
        for size in sizes[::4]:
            for time in times[::4]:
                for ratio in ratios[::4]:
                    features = np.array([[size, time, ratio]])
                    features_scaled = scaler_qd.transform(features)
                    emission = model_qd.predict(features_scaled)[0]
    
                    obj1 = size
                    obj2 = abs(emission - 550)
    
                    sample_X.append([size, time, ratio])
                    sample_F.append([obj1, obj2])
    
        sample_X = np.array(sample_X)
        sample_F = np.array(sample_F)
    
        print("\nã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã«ã‚ˆã‚‹è§£ã®æ¢ç´¢å®Œäº†")
        print(f"æ¢ç´¢ã—ãŸè§£ã®æ•°: {len(sample_F)}")
    
        res = type('Result', (), {
            'X': sample_X,
            'F': sample_F
        })()
    
        PYMOO_AVAILABLE = False
    

### ã€ä¾‹27ã€‘Paretoãƒ•ãƒ­ãƒ³ãƒˆã®å¯è¦–åŒ–
    
    
    # Paretoãƒ•ãƒ­ãƒ³ãƒˆã®å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(10, 7))
    
    if PYMOO_AVAILABLE:
        # NSGA-IIã®çµæœã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        ax.scatter(res.F[:, 0], -res.F[:, 1], c='blue', s=80, alpha=0.6,
                   edgecolors='black', label='Pareto Optimal Solutions')
    
        title_suffix = "(NSGA-II)"
    else:
        # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã®çµæœã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        ax.scatter(res.F[:, 0], res.F[:, 1], c='blue', s=60, alpha=0.5,
                   edgecolors='black', label='Sampled Solutions')
    
        title_suffix = "(Grid Search)"
    
    ax.set_xlabel('Objective 1: Size (nm) [Minimize]', fontsize=12)
    
    if PYMOO_AVAILABLE:
        ax.set_ylabel('Objective 2: Efficiency [Maximize]', fontsize=12)
    else:
        ax.set_ylabel('Objective 2: Deviation from 550nm [Minimize]', fontsize=12)
    
    ax.set_title(f'Pareto Front: Size vs Emission Efficiency {title_suffix}',
                 fontsize=13, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print("\nãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆ:")
    print("  ã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹ã¨åŠ¹ç‡ãŒä¸‹ãŒã‚Šã€åŠ¹ç‡ã‚’ä¸Šã’ã‚‹ã¨ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹")
    print("  â†’ ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•é–¢ä¿‚ãŒæ˜ç¢ºã«")
    

* * *

## 3.9 TEMç”»åƒè§£æã¨ã‚µã‚¤ã‚ºåˆ†å¸ƒ

### ã€ä¾‹28ã€‘æ¨¡æ“¬TEMãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ

TEMï¼ˆé€éå‹é›»å­é¡•å¾®é¡ï¼‰ã§æ¸¬å®šã•ã‚Œã‚‹ãƒŠãƒç²’å­ã‚µã‚¤ã‚ºã¯ã€ã—ã°ã—ã°å¯¾æ•°æ­£è¦åˆ†å¸ƒã«å¾“ã„ã¾ã™ã€‚
    
    
    from scipy.stats import lognorm
    
    # å¯¾æ•°æ­£è¦åˆ†å¸ƒã«å¾“ã†TEMã‚µã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
    np.random.seed(200)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    mean_size = 20  # å¹³å‡ã‚µã‚¤ã‚ºï¼ˆnmï¼‰
    cv = 0.3  # å¤‰å‹•ä¿‚æ•°ï¼ˆæ¨™æº–åå·®/å¹³å‡ï¼‰
    
    # å¯¾æ•°æ­£è¦åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨ˆç®—
    sigma = np.sqrt(np.log(1 + cv**2))
    mu = np.log(mean_size) - 0.5 * sigma**2
    
    # ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆï¼ˆ500ç²’å­ï¼‰
    tem_sizes = lognorm.rvs(s=sigma, scale=np.exp(mu), size=500)
    
    print("=" * 60)
    print("TEMæ¸¬å®šãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆå¯¾æ•°æ­£è¦åˆ†å¸ƒï¼‰")
    print("=" * 60)
    print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(tem_sizes)}ç²’å­")
    print(f"å¹³å‡ã‚µã‚¤ã‚º: {tem_sizes.mean():.2f} nm")
    print(f"æ¨™æº–åå·®: {tem_sizes.std():.2f} nm")
    print(f"ä¸­å¤®å€¤: {np.median(tem_sizes):.2f} nm")
    print(f"æœ€å°å€¤: {tem_sizes.min():.2f} nm")
    print(f"æœ€å¤§å€¤: {tem_sizes.max():.2f} nm")
    
    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.hist(tem_sizes, bins=40, alpha=0.7, color='lightblue',
            edgecolor='black', density=True, label='TEM Data')
    ax.set_xlabel('Particle Size (nm)', fontsize=12)
    ax.set_ylabel('Probability Density', fontsize=12)
    ax.set_title('TEM Size Distribution (Lognormal)', fontsize=13, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3, axis='y')
    plt.tight_layout()
    plt.show()
    

### ã€ä¾‹29ã€‘å¯¾æ•°æ­£è¦åˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
    
    
    # å¯¾æ•°æ­£è¦åˆ†å¸ƒã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
    shape_fit, loc_fit, scale_fit = lognorm.fit(tem_sizes, floc=0)
    
    # ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã•ã‚ŒãŸåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    fitted_mean = np.exp(np.log(scale_fit) + 0.5 * shape_fit**2)
    fitted_std = fitted_mean * np.sqrt(np.exp(shape_fit**2) - 1)
    
    print("=" * 60)
    print("å¯¾æ•°æ­£è¦åˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœ")
    print("=" * 60)
    print(f"å½¢çŠ¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (sigma): {shape_fit:.4f}")
    print(f"ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {scale_fit:.4f}")
    print(f"ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã•ã‚ŒãŸå¹³å‡ã‚µã‚¤ã‚º: {fitted_mean:.2f} nm")
    print(f"ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã•ã‚ŒãŸæ¨™æº–åå·®: {fitted_std:.2f} nm")
    
    # å®Ÿæ¸¬å€¤ã¨ã®æ¯”è¼ƒ
    print(f"\nå®Ÿæ¸¬å€¤ã¨ã®æ¯”è¼ƒ:")
    print(f"  å¹³å‡ã‚µã‚¤ã‚º - å®Ÿæ¸¬: {tem_sizes.mean():.2f} nm, ãƒ•ã‚£ãƒƒãƒˆ: {fitted_mean:.2f} nm")
    print(f"  æ¨™æº–åå·® - å®Ÿæ¸¬: {tem_sizes.std():.2f} nm, ãƒ•ã‚£ãƒƒãƒˆ: {fitted_std:.2f} nm")
    

### ã€ä¾‹30ã€‘ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœã®å¯è¦–åŒ–
    
    
    # ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœã®è©³ç´°å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã¨ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°æ›²ç·š
    axes[0].hist(tem_sizes, bins=40, alpha=0.6, color='lightblue',
                 edgecolor='black', density=True, label='TEM Data')
    
    # ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã•ã‚ŒãŸå¯¾æ•°æ­£è¦åˆ†å¸ƒ
    x_range = np.linspace(0, tem_sizes.max(), 200)
    fitted_pdf = lognorm.pdf(x_range, shape_fit, loc=loc_fit, scale=scale_fit)
    axes[0].plot(x_range, fitted_pdf, 'r-', linewidth=2,
                 label=f'Lognormal Fit (Î¼={fitted_mean:.1f}, Ïƒ={fitted_std:.1f})')
    
    axes[0].set_xlabel('Particle Size (nm)', fontsize=12)
    axes[0].set_ylabel('Probability Density', fontsize=12)
    axes[0].set_title('TEM Size Distribution with Lognormal Fit',
                      fontsize=13, fontweight='bold')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3, axis='y')
    
    # Q-Qãƒ—ãƒ­ãƒƒãƒˆï¼ˆåˆ†ä½ç‚¹ãƒ—ãƒ­ãƒƒãƒˆï¼‰
    from scipy.stats import probplot
    
    probplot(tem_sizes, dist=lognorm, sparams=(shape_fit, loc_fit, scale_fit),
             plot=axes[1])
    axes[1].set_title('Q-Q Plot: Lognormal Distribution',
                      fontsize=13, fontweight='bold')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print("\nQ-Qãƒ—ãƒ­ãƒƒãƒˆ: ãƒ‡ãƒ¼ã‚¿ãŒç›´ç·šä¸Šã«ã‚ã‚Œã°ã€å¯¾æ•°æ­£è¦åˆ†å¸ƒã«è‰¯ãå¾“ã£ã¦ã„ã¾ã™")
    

* * *

## 3.10 åˆ†å­å‹•åŠ›å­¦ï¼ˆMDï¼‰ãƒ‡ãƒ¼ã‚¿è§£æ

### ã€ä¾‹31ã€‘MDã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿

åˆ†å­å‹•åŠ›å­¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒŠãƒç²’å­ã®åŸå­é…ç½®ã®æ™‚é–“ç™ºå±•ã‚’è¿½è·¡ã—ã¾ã™ã€‚
    
    
    # MDã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®æ¨¡æ“¬ç”Ÿæˆ
    # å®Ÿéš›ã®MDãƒ‡ãƒ¼ã‚¿ã¯LAMMPS, GROMACSç­‰ã‹ã‚‰å–å¾—
    
    np.random.seed(300)
    
    n_atoms = 100  # åŸå­æ•°
    n_steps = 1000  # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°
    dt = 0.001  # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆpsï¼‰
    
    # åˆæœŸä½ç½®ï¼ˆnmï¼‰
    positions_initial = np.random.uniform(-1, 1, (n_atoms, 3))
    
    # æ™‚é–“ç™ºå±•ã®æ¨¡æ“¬ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ï¼‰
    positions = np.zeros((n_steps, n_atoms, 3))
    positions[0] = positions_initial
    
    for t in range(1, n_steps):
        # ãƒ©ãƒ³ãƒ€ãƒ ãªå¤‰ä½
        displacement = np.random.normal(0, 0.01, (n_atoms, 3))
        positions[t] = positions[t-1] + displacement
    
    print("=" * 60)
    print("MDã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ")
    print("=" * 60)
    print(f"åŸå­æ•°: {n_atoms}")
    print(f"ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°: {n_steps}")
    print(f"ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ™‚é–“: {n_steps * dt:.2f} ps")
    print(f"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {positions.shape} (time, atoms, xyz)")
    
    # ä¸­å¿ƒåŸå­ï¼ˆåŸå­0ï¼‰ã®è»Œè·¡ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    fig = plt.figure(figsize=(12, 5))
    
    ax1 = fig.add_subplot(121, projection='3d')
    ax1.plot(positions[:, 0, 0], positions[:, 0, 1], positions[:, 0, 2],
             'b-', alpha=0.5, linewidth=0.5)
    ax1.scatter(positions[0, 0, 0], positions[0, 0, 1], positions[0, 0, 2],
                c='green', s=100, label='Start', edgecolors='k')
    ax1.scatter(positions[-1, 0, 0], positions[-1, 0, 1], positions[-1, 0, 2],
                c='red', s=100, label='End', edgecolors='k')
    ax1.set_xlabel('X (nm)')
    ax1.set_ylabel('Y (nm)')
    ax1.set_zlabel('Z (nm)')
    ax1.set_title('Atom Trajectory (Atom 0)', fontweight='bold')
    ax1.legend()
    
    ax2 = fig.add_subplot(122)
    ax2.plot(np.arange(n_steps) * dt, positions[:, 0, 0], label='X')
    ax2.plot(np.arange(n_steps) * dt, positions[:, 0, 1], label='Y')
    ax2.plot(np.arange(n_steps) * dt, positions[:, 0, 2], label='Z')
    ax2.set_xlabel('Time (ps)', fontsize=11)
    ax2.set_ylabel('Position (nm)', fontsize=11)
    ax2.set_title('Position vs Time (Atom 0)', fontweight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    

### ã€ä¾‹32ã€‘å‹•å¾„åˆ†å¸ƒé–¢æ•°ï¼ˆRDFï¼‰ã®è¨ˆç®—

å‹•å¾„åˆ†å¸ƒé–¢æ•°ï¼ˆRadial Distribution Function, RDFï¼‰ã¯ã€åŸå­é–“è·é›¢ã®åˆ†å¸ƒã‚’è¡¨ã—ã¾ã™ã€‚
    
    
    # å‹•å¾„åˆ†å¸ƒé–¢æ•°ï¼ˆRDFï¼‰ã®è¨ˆç®—
    def calculate_rdf(positions, r_max=2.0, n_bins=100):
        """
        å‹•å¾„åˆ†å¸ƒé–¢æ•°ã‚’è¨ˆç®—
    
        Parameters:
        -----------
        positions : ndarray
            åŸå­ä½ç½® (n_atoms, 3)
        r_max : float
            æœ€å¤§è·é›¢ï¼ˆnmï¼‰
        n_bins : int
            ãƒ“ãƒ³æ•°
    
        Returns:
        --------
        r_bins : ndarray
            è·é›¢ãƒ“ãƒ³
        rdf : ndarray
            å‹•å¾„åˆ†å¸ƒé–¢æ•°
        """
        n_atoms = positions.shape[0]
    
        # å…¨åŸå­ãƒšã‚¢é–“ã®è·é›¢ã‚’è¨ˆç®—
        distances = []
        for i in range(n_atoms):
            for j in range(i+1, n_atoms):
                dist = np.linalg.norm(positions[i] - positions[j])
                if dist < r_max:
                    distances.append(dist)
    
        distances = np.array(distances)
    
        # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
        hist, bin_edges = np.histogram(distances, bins=n_bins, range=(0, r_max))
        r_bins = (bin_edges[:-1] + bin_edges[1:]) / 2
    
        # è¦æ ¼åŒ–ï¼ˆç†æƒ³æ°—ä½“ã¨ã®æ¯”ï¼‰
        dr = r_max / n_bins
        volume_shell = 4 * np.pi * r_bins**2 * dr
        n_ideal = volume_shell * (n_atoms / (4/3 * np.pi * r_max**3))
    
        rdf = hist / n_ideal / (n_atoms / 2)
    
        return r_bins, rdf
    
    # æœ€çµ‚ãƒ•ãƒ¬ãƒ¼ãƒ ã§RDFã‚’è¨ˆç®—
    final_positions = positions[-1]
    r_bins, rdf = calculate_rdf(final_positions, r_max=1.5, n_bins=150)
    
    print("=" * 60)
    print("å‹•å¾„åˆ†å¸ƒé–¢æ•°ï¼ˆRDFï¼‰")
    print("=" * 60)
    print(f"è¨ˆç®—å®Œäº†: {len(r_bins)}å€‹ã®ãƒ“ãƒ³")
    
    # RDFã®ãƒ—ãƒ­ãƒƒãƒˆ
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.plot(r_bins, rdf, 'b-', linewidth=2)
    ax.axhline(y=1, color='r', linestyle='--', linewidth=1, label='Ideal Gas (g(r)=1)')
    ax.set_xlabel('Distance r (nm)', fontsize=12)
    ax.set_ylabel('g(r)', fontsize=12)
    ax.set_title('Radial Distribution Function (RDF)', fontsize=13, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_ylim(0, max(rdf) * 1.1)
    
    plt.tight_layout()
    plt.show()
    
    # ãƒ”ãƒ¼ã‚¯ä½ç½®ã®æ¤œå‡º
    from scipy.signal import find_peaks
    
    peaks, _ = find_peaks(rdf, height=1.2, distance=10)
    print(f"\nRDFã®ãƒ”ãƒ¼ã‚¯ä½ç½®ï¼ˆç‰¹å¾´çš„ãªåŸå­é–“è·é›¢ï¼‰:")
    for i, peak_idx in enumerate(peaks[:3], 1):
        print(f"  ãƒ”ãƒ¼ã‚¯{i}: r = {r_bins[peak_idx]:.3f} nm, g(r) = {rdf[peak_idx]:.2f}")
    

### ã€ä¾‹33ã€‘æ‹¡æ•£ä¿‚æ•°ã®è¨ˆç®—ï¼ˆMean Squared Displacementï¼‰
    
    
    # å¹³å‡äºŒä¹—å¤‰ä½ï¼ˆMSDï¼‰ã®è¨ˆç®—
    def calculate_msd(positions):
        """
        å¹³å‡äºŒä¹—å¤‰ä½ã‚’è¨ˆç®—
    
        Parameters:
        -----------
        positions : ndarray
            åŸå­ä½ç½® (n_steps, n_atoms, 3)
    
        Returns:
        --------
        msd : ndarray
            å¹³å‡äºŒä¹—å¤‰ä½ (n_steps,)
        """
        n_steps, n_atoms, _ = positions.shape
        msd = np.zeros(n_steps)
    
        # å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§ã®MSD
        for t in range(n_steps):
            displacement = positions[t] - positions[0]
            squared_displacement = np.sum(displacement**2, axis=1)
            msd[t] = np.mean(squared_displacement)
    
        return msd
    
    # MSDã®è¨ˆç®—
    msd = calculate_msd(positions)
    time = np.arange(n_steps) * dt
    
    print("=" * 60)
    print("å¹³å‡äºŒä¹—å¤‰ä½ï¼ˆMSDï¼‰ã¨æ‹¡æ•£ä¿‚æ•°")
    print("=" * 60)
    
    # æ‹¡æ•£ä¿‚æ•°ã®è¨ˆç®—ï¼ˆEinsteiné–¢ä¿‚å¼: MSD = 6*D*tï¼‰
    # ç·šå½¢ãƒ•ã‚£ãƒƒãƒˆï¼ˆå¾ŒåŠ50%ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
    start_idx = n_steps // 2
    fit_coeffs = np.polyfit(time[start_idx:], msd[start_idx:], 1)
    slope = fit_coeffs[0]
    diffusion_coefficient = slope / 6
    
    print(f"æ‹¡æ•£ä¿‚æ•° D = {diffusion_coefficient:.6f} nmÂ²/ps")
    print(f"            = {diffusion_coefficient * 1e3:.6f} Ã— 10â»â¶ cmÂ²/s")
    
    # MSDãƒ—ãƒ­ãƒƒãƒˆ
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.plot(time, msd, 'b-', linewidth=2, label='MSD')
    ax.plot(time[start_idx:], fit_coeffs[0] * time[start_idx:] + fit_coeffs[1],
            'r--', linewidth=2, label=f'Linear Fit (D={diffusion_coefficient:.4f} nmÂ²/ps)')
    ax.set_xlabel('Time (ps)', fontsize=12)
    ax.set_ylabel('MSD (nmÂ²)', fontsize=12)
    ax.set_title('Mean Squared Displacement (MSD)', fontsize=13, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print("\næ‹¡æ•£ä¿‚æ•°ã¯ã€ãƒŠãƒç²’å­ã®ç§»å‹•æ€§ã‚’å®šé‡çš„ã«è©•ä¾¡ã™ã‚‹é‡è¦ãªæŒ‡æ¨™ã§ã™")
    

* * *

## 3.11 ç•°å¸¸æ¤œçŸ¥ï¼šå“è³ªç®¡ç†ã¸ã®å¿œç”¨

### ã€ä¾‹34ã€‘Isolation Forestã«ã‚ˆã‚‹ç•°å¸¸ãƒŠãƒç²’å­æ¤œå‡º

è£½é€ ãƒ—ãƒ­ã‚»ã‚¹ã§ç”Ÿæˆã•ã‚ŒãŸãƒŠãƒç²’å­ã®å“è³ªç®¡ç†ã«ã€æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥ã‚’é©ç”¨ã—ã¾ã™ã€‚
    
    
    from sklearn.ensemble import IsolationForest
    
    # æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã¨ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã‚’æ··åœ¨ã•ã›ã‚‹
    np.random.seed(400)
    
    # æ­£å¸¸ãªé‡‘ãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ï¼ˆ180ã‚µãƒ³ãƒ—ãƒ«ï¼‰
    normal_size = np.random.normal(15, 3, 180)
    normal_lspr = 520 + 0.8 * (normal_size - 15) + np.random.normal(0, 3, 180)
    
    # ç•°å¸¸ãªãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ï¼ˆ20ã‚µãƒ³ãƒ—ãƒ«ï¼‰ï¼šã‚µã‚¤ã‚ºãŒç•°å¸¸ã«å¤§ãã„orå°ã•ã„
    anomaly_size = np.concatenate([
        np.random.uniform(5, 8, 10),  # ç•°å¸¸ã«å°ã•ã„
        np.random.uniform(35, 50, 10)  # ç•°å¸¸ã«å¤§ãã„
    ])
    anomaly_lspr = 520 + 0.8 * (anomaly_size - 15) + np.random.normal(0, 8, 20)
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    all_size = np.concatenate([normal_size, anomaly_size])
    all_lspr = np.concatenate([normal_lspr, anomaly_lspr])
    all_data = np.column_stack([all_size, all_lspr])
    
    # ãƒ©ãƒ™ãƒ«ï¼ˆæ­£å¸¸=0ã€ç•°å¸¸=1ï¼‰
    true_labels = np.concatenate([np.zeros(180), np.ones(20)])
    
    print("=" * 60)
    print("ç•°å¸¸æ¤œçŸ¥ï¼ˆIsolation Forestï¼‰")
    print("=" * 60)
    print(f"å…¨ãƒ‡ãƒ¼ã‚¿æ•°: {len(all_data)}")
    print(f"æ­£å¸¸ãƒ‡ãƒ¼ã‚¿: {int((true_labels == 0).sum())}ã‚µãƒ³ãƒ—ãƒ«")
    print(f"ç•°å¸¸ãƒ‡ãƒ¼ã‚¿: {int((true_labels == 1).sum())}ã‚µãƒ³ãƒ—ãƒ«")
    
    # Isolation Forestãƒ¢ãƒ‡ãƒ«
    iso_forest = IsolationForest(
        contamination=0.1,  # ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆï¼ˆ10%ã¨ä»®å®šï¼‰
        random_state=42,
        n_estimators=100
    )
    
    # ç•°å¸¸æ¤œçŸ¥
    predictions = iso_forest.fit_predict(all_data)
    anomaly_scores = iso_forest.score_samples(all_data)
    
    # äºˆæ¸¬çµæœï¼ˆ1: æ­£å¸¸ã€-1: ç•°å¸¸ï¼‰
    predicted_anomalies = (predictions == -1)
    true_anomalies = (true_labels == 1)
    
    # è©•ä¾¡æŒ‡æ¨™
    from sklearn.metrics import confusion_matrix, classification_report
    
    # äºˆæ¸¬ã‚’0/1ã«å¤‰æ›
    predicted_labels = (predictions == -1).astype(int)
    
    print("\næ··åŒè¡Œåˆ—:")
    cm = confusion_matrix(true_labels, predicted_labels)
    print(cm)
    
    print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
    print(classification_report(true_labels, predicted_labels,
                                target_names=['Normal', 'Anomaly']))
    
    # æ¤œå‡ºç‡
    detected_anomalies = np.sum(predicted_anomalies & true_anomalies)
    total_anomalies = np.sum(true_anomalies)
    detection_rate = detected_anomalies / total_anomalies * 100
    
    print(f"\nç•°å¸¸æ¤œå‡ºç‡: {detection_rate:.1f}% ({detected_anomalies}/{total_anomalies})")
    

### ã€ä¾‹35ã€‘ç•°å¸¸ã‚µãƒ³ãƒ—ãƒ«ã®å¯è¦–åŒ–
    
    
    # ç•°å¸¸æ¤œçŸ¥çµæœã®å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # æ•£å¸ƒå›³ï¼ˆçœŸã®ãƒ©ãƒ™ãƒ«ï¼‰
    axes[0].scatter(all_size[true_labels == 0], all_lspr[true_labels == 0],
                    c='blue', s=60, alpha=0.6, label='Normal', edgecolors='k')
    axes[0].scatter(all_size[true_labels == 1], all_lspr[true_labels == 1],
                    c='red', s=100, alpha=0.8, marker='^', label='True Anomaly',
                    edgecolors='k', linewidths=2)
    axes[0].set_xlabel('Size (nm)', fontsize=12)
    axes[0].set_ylabel('LSPR Wavelength (nm)', fontsize=12)
    axes[0].set_title('True Labels', fontsize=13, fontweight='bold')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # æ•£å¸ƒå›³ï¼ˆäºˆæ¸¬çµæœï¼‰
    normal_mask = ~predicted_anomalies
    anomaly_mask = predicted_anomalies
    
    axes[1].scatter(all_size[normal_mask], all_lspr[normal_mask],
                    c='blue', s=60, alpha=0.6, label='Predicted Normal', edgecolors='k')
    axes[1].scatter(all_size[anomaly_mask], all_lspr[anomaly_mask],
                    c='orange', s=100, alpha=0.8, marker='X', label='Predicted Anomaly',
                    edgecolors='k', linewidths=2)
    
    # æ­£ã—ãæ¤œå‡ºã•ã‚ŒãŸç•°å¸¸ã‚’å¼·èª¿
    correctly_detected = predicted_anomalies & true_anomalies
    axes[1].scatter(all_size[correctly_detected], all_lspr[correctly_detected],
                    c='red', s=150, marker='*', label='Correctly Detected',
                    edgecolors='black', linewidths=1.5, zorder=5)
    
    axes[1].set_xlabel('Size (nm)', fontsize=12)
    axes[1].set_ylabel('LSPR Wavelength (nm)', fontsize=12)
    axes[1].set_title('Isolation Forest Predictions', fontsize=13, fontweight='bold')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # ç•°å¸¸ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.hist(anomaly_scores[true_labels == 0], bins=30, alpha=0.6,
            color='blue', label='Normal', edgecolor='black')
    ax.hist(anomaly_scores[true_labels == 1], bins=30, alpha=0.6,
            color='red', label='Anomaly', edgecolor='black')
    ax.set_xlabel('Anomaly Score', fontsize=12)
    ax.set_ylabel('Frequency', fontsize=12)
    ax.set_title('Anomaly Score Distribution', fontsize=13, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.show()
    
    print("\nç•°å¸¸ã‚¹ã‚³ã‚¢ãŒä½ã„ï¼ˆè² ã®å€¤ãŒå¤§ãã„ï¼‰ã»ã©ã€ç•°å¸¸ã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„")
    

* * *

## ã¾ã¨ã‚

æœ¬ç« ã§ã¯ã€Pythonã‚’ä½¿ã£ãŸãƒŠãƒææ–™ãƒ‡ãƒ¼ã‚¿è§£æã¨æ©Ÿæ¢°å­¦ç¿’ã®å®Ÿè·µçš„æ‰‹æ³•ã‚’35å€‹ã®ã‚³ãƒ¼ãƒ‰ä¾‹ã§å­¦ã³ã¾ã—ãŸã€‚

### ç¿’å¾—ã—ãŸä¸»è¦æŠ€è¡“

  1. **ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã¨å¯è¦–åŒ–** ï¼ˆä¾‹1-5ï¼‰ \- é‡‘ãƒŠãƒç²’å­ã€é‡å­ãƒ‰ãƒƒãƒˆã®åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ \- ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã€æ•£å¸ƒå›³ã€3Dãƒ—ãƒ­ãƒƒãƒˆã€ç›¸é–¢åˆ†æ

  2. **ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†** ï¼ˆä¾‹6-9ï¼‰ \- æ¬ æå€¤å‡¦ç†ã€å¤–ã‚Œå€¤æ¤œå‡ºã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€ãƒ‡ãƒ¼ã‚¿åˆ†å‰²

  3. **å›å¸°ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ç‰©æ€§äºˆæ¸¬** ï¼ˆä¾‹10-15ï¼‰ \- ç·šå½¢å›å¸°ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã€LightGBMã€SVRã€MLP \- ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒï¼ˆRÂ²ã€RMSEã€MAEï¼‰

  4. **é‡å­ãƒ‰ãƒƒãƒˆç™ºå…‰äºˆæ¸¬** ï¼ˆä¾‹16-18ï¼‰ \- Brusæ–¹ç¨‹å¼ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿ç”Ÿæˆ \- LightGBMã«ã‚ˆã‚‹äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰

  5. **ç‰¹å¾´é‡é‡è¦åº¦ã¨ãƒ¢ãƒ‡ãƒ«è§£é‡ˆ** ï¼ˆä¾‹19-20ï¼‰ \- LightGBMç‰¹å¾´é‡é‡è¦åº¦ \- SHAPåˆ†æã«ã‚ˆã‚‹äºˆæ¸¬ã®è§£é‡ˆ

  6. **ãƒ™ã‚¤ã‚ºæœ€é©åŒ–** ï¼ˆä¾‹21-25ï¼‰ \- ç›®æ¨™LSPRæ³¢é•·ã‚’å®Ÿç¾ã™ã‚‹æœ€é©åˆæˆæ¡ä»¶ã®æ¢ç´¢ \- åæŸãƒ—ãƒ­ãƒƒãƒˆã€æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã®å¯è¦–åŒ–

  7. **å¤šç›®çš„æœ€é©åŒ–** ï¼ˆä¾‹26-27ï¼‰ \- NSGA-IIã«ã‚ˆã‚‹ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©åŒ– \- ã‚µã‚¤ã‚ºã¨ç™ºå…‰åŠ¹ç‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•åˆ†æ

  8. **TEMç”»åƒè§£æ** ï¼ˆä¾‹28-30ï¼‰ \- å¯¾æ•°æ­£è¦åˆ†å¸ƒã«ã‚ˆã‚‹ã‚µã‚¤ã‚ºåˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚° \- Q-Qãƒ—ãƒ­ãƒƒãƒˆã«ã‚ˆã‚‹åˆ†å¸ƒã®æ¤œè¨¼

  9. **åˆ†å­å‹•åŠ›å­¦ãƒ‡ãƒ¼ã‚¿è§£æ** ï¼ˆä¾‹31-33ï¼‰ \- åŸå­è»Œè·¡ã®å¯è¦–åŒ– \- å‹•å¾„åˆ†å¸ƒé–¢æ•°ï¼ˆRDFï¼‰ã®è¨ˆç®— \- æ‹¡æ•£ä¿‚æ•°ã®ç®—å‡ºï¼ˆMSDæ³•ï¼‰

  10. **ç•°å¸¸æ¤œçŸ¥** ï¼ˆä¾‹34-35ï¼‰

     * Isolation Forestã«ã‚ˆã‚‹å“è³ªç®¡ç†
     * ç•°å¸¸ãƒŠãƒç²’å­ã®è‡ªå‹•æ¤œå‡º

### å®Ÿè·µçš„ãªå¿œç”¨

ã“ã‚Œã‚‰ã®æŠ€è¡“ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå®Ÿéš›ã®ãƒŠãƒææ–™ç ”ç©¶ã«ç›´æ¥å¿œç”¨ã§ãã¾ã™ï¼š

  * **ææ–™è¨­è¨ˆ** : æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ç‰©æ€§äºˆæ¸¬ã¨æœ€é©åŒ–ã«ã‚ˆã‚‹é«˜åŠ¹ç‡ææ–™æ¢ç´¢
  * **ãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ–** : ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚‹å®Ÿé¨“å›æ•°å‰Šæ¸›ã¨æœ€é©åˆæˆæ¡ä»¶ç™ºè¦‹
  * **å“è³ªç®¡ç†** : ç•°å¸¸æ¤œçŸ¥ã«ã‚ˆã‚‹ä¸è‰¯å“ã®æ—©æœŸç™ºè¦‹ã¨æ­©ç•™ã¾ã‚Šå‘ä¸Š
  * **ãƒ‡ãƒ¼ã‚¿è§£æ** : TEMãƒ‡ãƒ¼ã‚¿ã€MDã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®å®šé‡çš„è§£æ
  * **ãƒ¢ãƒ‡ãƒ«è§£é‡ˆ** : SHAPåˆ†æã«ã‚ˆã‚‹äºˆæ¸¬æ ¹æ‹ ã®å¯è¦–åŒ–ã¨ä¿¡é ¼æ€§å‘ä¸Š

### æ¬¡ç« ã®äºˆå‘Š

Chapter 4ã§ã¯ã€ã“ã‚Œã‚‰ã®æŠ€è¡“ã‚’å®Ÿéš›ã®ãƒŠãƒææ–™ç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«é©ç”¨ã—ãŸ5ã¤ã®è©³ç´°ãªã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ã‚’å­¦ã³ã¾ã™ã€‚ã‚«ãƒ¼ãƒœãƒ³ãƒŠãƒãƒãƒ¥ãƒ¼ãƒ–è¤‡åˆææ–™ã€é‡å­ãƒ‰ãƒƒãƒˆã€é‡‘ãƒŠãƒç²’å­è§¦åª’ã€ã‚°ãƒ©ãƒ•ã‚§ãƒ³ã€ãƒŠãƒåŒ»è–¬ã®å®Ÿç”¨åŒ–äº‹ä¾‹ã‚’é€šã˜ã¦ã€å•é¡Œè§£æ±ºã®å…¨ä½“åƒã‚’ç†è§£ã—ã¾ã™ã€‚

* * *

## æ¼”ç¿’å•é¡Œ

### æ¼”ç¿’1: ã‚«ãƒ¼ãƒœãƒ³ãƒŠãƒãƒãƒ¥ãƒ¼ãƒ–ã®é›»æ°—ä¼å°åº¦äºˆæ¸¬

ã‚«ãƒ¼ãƒœãƒ³ãƒŠãƒãƒãƒ¥ãƒ¼ãƒ–ï¼ˆCNTï¼‰ã®é›»æ°—ä¼å°åº¦ã¯ã€ç›´å¾„ã€ã‚«ã‚¤ãƒ©ãƒªãƒ†ã‚£ã€é•·ã•ã«ä¾å­˜ã—ã¾ã™ã€‚ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã€LightGBMãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ã—ã¦ãã ã•ã„ã€‚

**ãƒ‡ãƒ¼ã‚¿ä»•æ§˜** ï¼š \- ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼š150 \- ç‰¹å¾´é‡ï¼šç›´å¾„ï¼ˆ1-3 nmï¼‰ã€é•·ã•ï¼ˆ100-1000 nmï¼‰ã€ã‚«ã‚¤ãƒ©ãƒªãƒ†ã‚£æŒ‡æ¨™ï¼ˆ0-1ã®é€£ç¶šå€¤ï¼‰ \- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼šé›»æ°—ä¼å°åº¦ï¼ˆ10Â³-10â· S/mã€å¯¾æ•°æ­£è¦åˆ†å¸ƒï¼‰

**ã‚¿ã‚¹ã‚¯** ï¼š 1\. ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ 2\. è¨“ç·´/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰² 3\. LightGBMãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨è©•ä¾¡ 4\. ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–

è§£ç­”ä¾‹
    
    
    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(500)
    n_samples = 150
    
    diameter = np.random.uniform(1, 3, n_samples)
    length = np.random.uniform(100, 1000, n_samples)
    chirality = np.random.uniform(0, 1, n_samples)
    
    # é›»æ°—ä¼å°åº¦ï¼ˆç°¡æ˜“ãƒ¢ãƒ‡ãƒ«: ç›´å¾„ã¨ã‚«ã‚¤ãƒ©ãƒªãƒ†ã‚£ã«å¼·ãä¾å­˜ï¼‰
    log_conductivity = 3 + 2*diameter + 3*chirality + 0.001*length + np.random.normal(0, 0.5, n_samples)
    conductivity = 10 ** log_conductivity  # S/m
    
    data_cnt = pd.DataFrame({
        'diameter_nm': diameter,
        'length_nm': length,
        'chirality': chirality,
        'conductivity_Sm': conductivity
    })
    
    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
    X_cnt = data_cnt[['diameter_nm', 'length_nm', 'chirality']]
    y_cnt = np.log10(data_cnt['conductivity_Sm'])  # å¯¾æ•°å¤‰æ›
    
    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    scaler_cnt = StandardScaler()
    X_cnt_scaled = scaler_cnt.fit_transform(X_cnt)
    
    # è¨“ç·´/ãƒ†ã‚¹ãƒˆåˆ†å‰²
    X_cnt_train, X_cnt_test, y_cnt_train, y_cnt_test = train_test_split(
        X_cnt_scaled, y_cnt, test_size=0.2, random_state=42
    )
    
    # LightGBMãƒ¢ãƒ‡ãƒ«
    model_cnt = lgb.LGBMRegressor(num_leaves=31, learning_rate=0.05, n_estimators=200, random_state=42, verbose=-1)
    model_cnt.fit(X_cnt_train, y_cnt_train)
    
    # äºˆæ¸¬ã¨è©•ä¾¡
    y_cnt_pred = model_cnt.predict(X_cnt_test)
    r2_cnt = r2_score(y_cnt_test, y_cnt_pred)
    rmse_cnt = np.sqrt(mean_squared_error(y_cnt_test, y_cnt_pred))
    
    print(f"RÂ²: {r2_cnt:.4f}")
    print(f"RMSE: {rmse_cnt:.4f}")
    
    # ç‰¹å¾´é‡é‡è¦åº¦
    importance_cnt = pd.DataFrame({
        'Feature': X_cnt.columns,
        'Importance': model_cnt.feature_importances_
    }).sort_values('Importance', ascending=False)
    
    print("\nç‰¹å¾´é‡é‡è¦åº¦:")
    print(importance_cnt)
    
    # å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(8, 5))
    ax.barh(importance_cnt['Feature'], importance_cnt['Importance'], color='steelblue', edgecolor='black')
    ax.set_xlabel('Importance')
    ax.set_title('Feature Importance: CNT Conductivity Prediction')
    plt.tight_layout()
    plt.show()
    

### æ¼”ç¿’2: éŠ€ãƒŠãƒç²’å­ã®æœ€é©åˆæˆæ¡ä»¶æ¢ç´¢

éŠ€ãƒŠãƒç²’å­ã®æŠ—èŒæ´»æ€§ã¯ã€ã‚µã‚¤ã‚ºãŒå°ã•ã„ã»ã©é«˜ããªã‚Šã¾ã™ã€‚ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’ç”¨ã„ã¦ã€ç›®æ¨™ã‚µã‚¤ã‚ºï¼ˆ10 nmï¼‰ã‚’å®Ÿç¾ã™ã‚‹æœ€é©ãªåˆæˆæ¸©åº¦ã¨pHã‚’æ¢ç´¢ã—ã¦ãã ã•ã„ã€‚

**æ¡ä»¶** ï¼š \- æ¸©åº¦ç¯„å›²ï¼š20-80Â°C \- pHç¯„å›²ï¼š6-11 \- ç›®æ¨™ã‚µã‚¤ã‚ºï¼š10 nm

è§£ç­”ä¾‹
    
    
    # éŠ€ãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
    np.random.seed(600)
    n_ag = 100
    
    temp_ag = np.random.uniform(20, 80, n_ag)
    pH_ag = np.random.uniform(6, 11, n_ag)
    
    # ã‚µã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ï¼ˆæ¸©åº¦ãŒé«˜ãã€pHãŒä½ã„ã»ã©å°ã•ããªã‚‹ã¨ä»®å®šï¼‰
    size_ag = 15 - 0.1*temp_ag - 0.8*pH_ag + np.random.normal(0, 1, n_ag)
    size_ag = np.clip(size_ag, 5, 30)
    
    data_ag = pd.DataFrame({
        'temperature': temp_ag,
        'pH': pH_ag,
        'size': size_ag
    })
    
    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ï¼ˆLightGBMï¼‰
    X_ag = data_ag[['temperature', 'pH']]
    y_ag = data_ag['size']
    
    scaler_ag = StandardScaler()
    X_ag_scaled = scaler_ag.fit_transform(X_ag)
    
    model_ag = lgb.LGBMRegressor(num_leaves=31, learning_rate=0.05, n_estimators=100, random_state=42, verbose=-1)
    model_ag.fit(X_ag_scaled, y_ag)
    
    # ãƒ™ã‚¤ã‚ºæœ€é©åŒ–
    from skopt import gp_minimize
    from skopt.space import Real
    
    space_ag = [
        Real(20, 80, name='temperature'),
        Real(6, 11, name='pH')
    ]
    
    target_size = 10.0
    
    def objective_ag(params):
        temp, ph = params
        features = scaler_ag.transform([[temp, ph]])
        predicted_size = model_ag.predict(features)[0]
        return abs(predicted_size - target_size)
    
    result_ag = gp_minimize(objective_ag, space_ag, n_calls=40, random_state=42, verbose=False)
    
    print("=" * 60)
    print("éŠ€ãƒŠãƒç²’å­ã®æœ€é©åˆæˆæ¡ä»¶")
    print("=" * 60)
    print(f"æœ€å°èª¤å·®: {result_ag.fun:.2f} nm")
    print(f"æœ€é©æ¸©åº¦: {result_ag.x[0]:.1f} Â°C")
    print(f"æœ€é©pH: {result_ag.x[1]:.2f}")
    
    # æœ€é©æ¡ä»¶ã§ã®äºˆæ¸¬ã‚µã‚¤ã‚º
    optimal_features = scaler_ag.transform([result_ag.x])
    predicted_size = model_ag.predict(optimal_features)[0]
    print(f"äºˆæ¸¬ã‚µã‚¤ã‚º: {predicted_size:.2f} nm")
    

### æ¼”ç¿’3: é‡å­ãƒ‰ãƒƒãƒˆã®å¤šè‰²ç™ºå…‰è¨­è¨ˆ

èµ¤ï¼ˆ650 nmï¼‰ã€ç·‘ï¼ˆ550 nmï¼‰ã€é’ï¼ˆ450 nmï¼‰ã®3è‰²ã®ç™ºå…‰ã‚’å®Ÿç¾ã™ã‚‹CdSeé‡å­ãƒ‰ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã‚’ã€ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚

**ãƒ’ãƒ³ãƒˆ** ï¼š \- å„è‰²ã”ã¨ã«æœ€é©åŒ–ã‚’å®Ÿè¡Œ \- ç™ºå…‰æ³¢é•·ã¨ã‚µã‚¤ã‚ºã®é–¢ä¿‚ã‚’ä½¿ç”¨

è§£ç­”ä¾‹
    
    
    # é‡å­ãƒ‰ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆä¾‹16ã®data_qdã‚’ä½¿ç”¨ï¼‰
    # model_qd ã¨ scaler_qd ãŒæ§‹ç¯‰æ¸ˆã¿ã¨ä»®å®š
    
    # 3è‰²ã®ç›®æ¨™æ³¢é•·
    target_colors = {
        'Red': 650,
        'Green': 550,
        'Blue': 450
    }
    
    results_colors = {}
    
    for color_name, target_emission in target_colors.items():
        # æ¢ç´¢ç©ºé–“
        space_qd = [
            Real(2, 10, name='size_nm'),
            Real(10, 120, name='synthesis_time_min'),
            Real(0.5, 2.0, name='precursor_ratio')
        ]
    
        # ç›®çš„é–¢æ•°
        def objective_qd(params):
            features = scaler_qd.transform([params])
            predicted_emission = model_qd.predict(features)[0]
            return abs(predicted_emission - target_emission)
    
        # æœ€é©åŒ–
        result_qd_color = gp_minimize(objective_qd, space_qd, n_calls=30, random_state=42, verbose=False)
    
        # çµæœä¿å­˜
        optimal_features = scaler_qd.transform([result_qd_color.x])
        predicted_emission = model_qd.predict(optimal_features)[0]
    
        results_colors[color_name] = {
            'target': target_emission,
            'size': result_qd_color.x[0],
            'time': result_qd_color.x[1],
            'ratio': result_qd_color.x[2],
            'predicted': predicted_emission,
            'error': result_qd_color.fun
        }
    
    # çµæœè¡¨ç¤º
    print("=" * 80)
    print("é‡å­ãƒ‰ãƒƒãƒˆå¤šè‰²ç™ºå…‰è¨­è¨ˆ")
    print("=" * 80)
    
    results_df = pd.DataFrame(results_colors).T
    print(results_df.to_string())
    
    # å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(10, 6))
    colors_rgb = {'Red': 'red', 'Green': 'green', 'Blue': 'blue'}
    
    for color_name, result in results_colors.items():
        ax.scatter(result['size'], result['predicted'],
                   s=200, color=colors_rgb[color_name],
                   edgecolors='black', linewidths=2, label=color_name)
    
    ax.set_xlabel('Quantum Dot Size (nm)', fontsize=12)
    ax.set_ylabel('Emission Wavelength (nm)', fontsize=12)
    ax.set_title('Multi-Color Quantum Dot Design', fontsize=13, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    

* * *

## 3.12 ç« æœ«ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼šãƒŠãƒææ–™ãƒ‡ãƒ¼ã‚¿è§£æã‚¹ã‚­ãƒ«ã®å“è³ªä¿è¨¼

æœ¬ç« ã§å­¦ã‚“ã Pythonã«ã‚ˆã‚‹ãƒŠãƒææ–™ãƒ‡ãƒ¼ã‚¿è§£æã¨æ©Ÿæ¢°å­¦ç¿’ã®å®Ÿè£…ã‚¹ã‚­ãƒ«ã‚’ä½“ç³»çš„ã«ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚

### 3.12.1 ç’°å¢ƒæ§‹ç¯‰ã‚¹ã‚­ãƒ«ï¼ˆEnvironment Setupï¼‰

#### åŸºç¤ãƒ¬ãƒ™ãƒ«

  * [ ] Python 3.9ä»¥ä¸ŠãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹
  * [ ] 3ã¤ã®ç’°å¢ƒæ§‹ç¯‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆAnaconda/venv/Colabï¼‰ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
  * [ ] è‡ªåˆ†ã®çŠ¶æ³ã«æœ€é©ãªç’°å¢ƒã‚’é¸æŠã§ãã‚‹
  * [ ] ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆãƒ»æœ‰åŠ¹åŒ–ãƒ»ç„¡åŠ¹åŒ–ã§ãã‚‹
  * [ ] pip/condaã§ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã‚‹ï¼ˆpandasã€numpyã€matplotlibã€scikit-learnã€lightgbmï¼‰
  * [ ] ç’°å¢ƒæ¤œè¨¼ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã€ã‚¨ãƒ©ãƒ¼ãªãå‹•ä½œç¢ºèªã§ãã‚‹

#### å¿œç”¨ãƒ¬ãƒ™ãƒ«

  * [ ] requirements.txtã‚’ä½œæˆãƒ»ä½¿ç”¨ã§ãã‚‹
  * [ ] Google Colabã§Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚ã‚‹
  * [ ] è¤‡æ•°ã®ä»®æƒ³ç’°å¢ƒã‚’ç”¨é€”åˆ¥ã«ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹
  * [ ] ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼ã‚’è‡ªåŠ›ã§ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã§ãã‚‹
  * [ ] ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆpymooã€SHAPï¼‰ã‚’å¿…è¦ã«å¿œã˜ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã‚‹

* * *

### 3.12.2 ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»å¯è¦–åŒ–ã‚¹ã‚­ãƒ«ï¼ˆData Processing & Visualizationï¼‰

#### åŸºç¤ãƒ¬ãƒ™ãƒ«

  * [ ] NumPyã§åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã§ãã‚‹ï¼ˆæ­£è¦åˆ†å¸ƒã€ä¸€æ§˜åˆ†å¸ƒï¼‰
  * [ ] Pandasã§ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆãƒ»æ“ä½œã§ãã‚‹
  * [ ] åŸºæœ¬çµ±è¨ˆé‡ï¼ˆmeanã€stdã€medianï¼‰ã‚’è¨ˆç®—ã§ãã‚‹
  * [ ] ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’ä½œæˆã§ãã‚‹
  * [ ] æ•£å¸ƒå›³ã‚’ä½œæˆã§ãã‚‹
  * [ ] æ¬ æå€¤ã‚’æ¤œå‡ºã§ãã‚‹ï¼ˆ`isnull().sum()`ï¼‰
  * [ ] æ¬ æå€¤ã‚’å‰Šé™¤ã¾ãŸã¯è£œå®Œã§ãã‚‹ï¼ˆ`dropna()` or `fillna()`ï¼‰

#### å¿œç”¨ãƒ¬ãƒ™ãƒ«

  * [ ] ç›¸é–¢è¡Œåˆ—ã‚’è¨ˆç®—ãƒ»å¯è¦–åŒ–ã§ãã‚‹ï¼ˆ`corr()`ã€seaborn.heatmapï¼‰
  * [ ] ãƒšã‚¢ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæ•£å¸ƒå›³ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼‰ã‚’ä½œæˆã§ãã‚‹ï¼ˆseaborn.pairplotï¼‰
  * [ ] 3Dæ•£å¸ƒå›³ã‚’ä½œæˆã§ãã‚‹ï¼ˆmpl_toolkits.mplot3dï¼‰
  * [ ] KDEï¼ˆã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®šï¼‰ã‚’ä½¿ç”¨ã§ãã‚‹
  * [ ] å¤–ã‚Œå€¤ã‚’IQRæ³•ã§æ¤œå‡ºã§ãã‚‹
  * [ ] StandardScalerã§ãƒ‡ãƒ¼ã‚¿ã‚’æ¨™æº–åŒ–ã§ãã‚‹
  * [ ] train_test_splitã§ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã§ãã‚‹ï¼ˆ80% vs 20%ï¼‰
  * [ ] `random_state=42`ã§å†ç¾æ€§ã‚’ç¢ºä¿ã—ã¦ã„ã‚‹

#### ä¸Šç´šãƒ¬ãƒ™ãƒ«

  * [ ] å¯¾æ•°æ­£è¦åˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ãŒã§ãã‚‹ï¼ˆscipy.stats.lognormï¼‰
  * [ ] Q-Qãƒ—ãƒ­ãƒƒãƒˆã§åˆ†å¸ƒã®é©åˆåº¦ã‚’æ¤œè¨¼ã§ãã‚‹
  * [ ] ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ã‚’åŠ¹æœçš„ã«ä½¿ç”¨ã§ãã‚‹ï¼ˆviridisã€plasmaã€coolwarmï¼‰
  * [ ] è¤‡æ•°ã®subplotsã‚’ä½¿ã£ãŸé«˜åº¦ãªå¯è¦–åŒ–ãŒã§ãã‚‹

* * *

### 3.12.3 æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«å®Ÿè£…ã‚¹ã‚­ãƒ«ï¼ˆML Model Implementationï¼‰

#### åŸºç¤ãƒ¬ãƒ™ãƒ«ï¼ˆ5ã¤ã®ãƒ¢ãƒ‡ãƒ«å®Ÿè£…ï¼‰

  * [ ] ç·šå½¢å›å¸°ã‚’å®Ÿè£…ã—ã€ä¿‚æ•°ã®æ„å‘³ã‚’èª¬æ˜ã§ãã‚‹
  * [ ] ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚’å®Ÿè£…ã—ã€`n_estimators`ã®å½¹å‰²ã‚’èª¬æ˜ã§ãã‚‹
  * [ ] LightGBMã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»å®Ÿè£…ã§ãã‚‹
  * [ ] SVRã§æ¨™æº–åŒ–ï¼ˆStandardScalerï¼‰ã®å¿…è¦æ€§ã‚’ç†è§£ã—ã¦ã„ã‚‹
  * [ ] MLPRegressorï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰ã‚’å®Ÿè£…ã§ãã‚‹

#### å¿œç”¨ãƒ¬ãƒ™ãƒ«ï¼ˆãƒ¢ãƒ‡ãƒ«é¸æŠã¨è©•ä¾¡ï¼‰

  * [ ] MAEã€RÂ²ã€RMSEã‚’è¨ˆç®—ãƒ»è§£é‡ˆã§ãã‚‹
  * [ ] è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ€§èƒ½å·®ã‚’è©•ä¾¡ã§ãã‚‹
  * [ ] éå­¦ç¿’ã‚’æ¤œå‡ºã§ãã‚‹ï¼ˆè¨“ç·´RÂ² â‰« ãƒ†ã‚¹ãƒˆRÂ²ï¼‰
  * [ ] 5ã¤ã®ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æ¯”è¼ƒè¡¨ã§æ•´ç†ã§ãã‚‹
  * [ ] äºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤ã®æ•£å¸ƒå›³ã‚’ä½œæˆã§ãã‚‹
  * [ ] æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆã—ã€ãƒ¢ãƒ‡ãƒ«ã®åã‚Šã‚’æ¤œå‡ºã§ãã‚‹

#### ä¸Šç´šãƒ¬ãƒ™ãƒ«

  * [ ] ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«å¿œã˜ã¦æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã§ãã‚‹
  * ç·šå½¢æ€§ãŒå¼·ã„ â†’ ç·šå½¢å›å¸°
  * éç·šå½¢æ€§ãŒå¼·ã„ â†’ ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã€LightGBM
  * ãƒ‡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã„ â†’ SVR
  * [ ] å„ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½¹å‰²ã‚’ç†è§£ã—ã¦ã„ã‚‹
  * ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼šn_estimatorsã€max_depth
  * LightGBMï¼šlearning_rateã€num_leaves
  * SVRï¼šCã€gammaã€epsilon
  * MLPï¼šhidden_layer_sizesã€alphaã€early_stopping

* * *

### 3.12.4 ç‰¹å¾´é‡é‡è¦åº¦ã¨ãƒ¢ãƒ‡ãƒ«è§£é‡ˆã‚¹ã‚­ãƒ«ï¼ˆFeature Importance & Interpretabilityï¼‰

#### åŸºç¤ãƒ¬ãƒ™ãƒ«

  * [ ] ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—ãƒ»å¯è¦–åŒ–ã§ãã‚‹ï¼ˆ`feature_importances_`ï¼‰
  * [ ] LightGBMã®ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—ãƒ»å¯è¦–åŒ–ã§ãã‚‹
  * [ ] ç‰¹å¾´é‡é‡è¦åº¦ã®çµæœã‚’è§£é‡ˆã§ãã‚‹ï¼ˆæœ€ã‚‚å½±éŸ¿ã™ã‚‹ç‰¹å¾´é‡ã¯ä½•ã‹ï¼‰

#### å¿œç”¨ãƒ¬ãƒ™ãƒ«

  * [ ] SHAPãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»ä½¿ç”¨ã§ãã‚‹
  * [ ] SHAP Explainerã‚’ä½œæˆã§ãã‚‹ï¼ˆ`shap.Explainer`ï¼‰
  * [ ] SHAP Summary Plotã‚’ä½œæˆãƒ»è§£é‡ˆã§ãã‚‹
  * [ ] SHAP Dependence Plotã‚’ä½œæˆãƒ»è§£é‡ˆã§ãã‚‹
  * [ ] SHAPå€¤ã®æ­£è² ãŒäºˆæ¸¬ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’èª¬æ˜ã§ãã‚‹

#### ä¸Šç´šãƒ¬ãƒ™ãƒ«

  * [ ] è¤‡æ•°ã®è§£é‡ˆæ‰‹æ³•ã‚’ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹
  * ç‰¹å¾´é‡é‡è¦åº¦ï¼šå…¨ä½“çš„ãªé‡è¦åº¦
  * SHAPï¼šå€‹åˆ¥ã‚µãƒ³ãƒ—ãƒ«ã®äºˆæ¸¬ç†ç”±
  * [ ] ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬æ ¹æ‹ ã‚’ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã«èª¬æ˜ã§ãã‚‹

* * *

### 3.12.5 ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚¹ã‚­ãƒ«ï¼ˆBayesian Optimizationï¼‰

#### åŸºç¤ãƒ¬ãƒ™ãƒ«

  * [ ] scikit-optimizeã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã‚‹
  * [ ] æ¢ç´¢ç©ºé–“ã‚’å®šç¾©ã§ãã‚‹ï¼ˆ`Real(min, max, name)`ï¼‰
  * [ ] ç›®çš„é–¢æ•°ã‚’å®šç¾©ã§ãã‚‹ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚Šã€èª¤å·®ã‚’è¿”ã™ï¼‰
  * [ ] `gp_minimize`ã‚’å®Ÿè¡Œã§ãã‚‹
  * [ ] æœ€é©åŒ–çµæœï¼ˆresult.xã€result.funï¼‰ã‚’å–å¾—ã§ãã‚‹

#### å¿œç”¨ãƒ¬ãƒ™ãƒ«

  * [ ] n_callsã¨n_initial_pointsã®å½¹å‰²ã‚’ç†è§£ã—ã¦ã„ã‚‹
  * n_callsï¼šè©•ä¾¡å›æ•°
  * n_initial_pointsï¼šãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å›æ•°
  * [ ] åæŸãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆã§ãã‚‹ï¼ˆ`plot_convergence`ï¼‰
  * [ ] æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã®å¯è¦–åŒ–ãŒã§ãã‚‹ï¼ˆè©•ä¾¡å±¥æ­´ã€æœ€è‰¯å€¤ã®æ¨ç§»ï¼‰
  * [ ] ç›®æ¨™å€¤ã¸ã®é”æˆç²¾åº¦ã‚’è©•ä¾¡ã§ãã‚‹

#### ä¸Šç´šãƒ¬ãƒ™ãƒ«

  * [ ] è¤‡æ•°ã®ç›®æ¨™ï¼ˆèµ¤ãƒ»ç·‘ãƒ»é’ã®é‡å­ãƒ‰ãƒƒãƒˆï¼‰ã«å¯¾ã—ã¦æœ€é©åŒ–ã‚’å®Ÿè¡Œã§ãã‚‹
  * [ ] æœ€é©åŒ–çµæœã‚’å®Ÿé¨“æ¤œè¨¼è¨ˆç”»ã«æ´»ç”¨ã§ãã‚‹
  * [ ] ç²å¾—é–¢æ•°ï¼ˆAcquisition Functionï¼‰ã®æ¦‚å¿µã‚’ç†è§£ã—ã¦ã„ã‚‹

* * *

### 3.12.6 å¤šç›®çš„æœ€é©åŒ–ã‚¹ã‚­ãƒ«ï¼ˆMulti-Objective Optimizationï¼‰

#### åŸºç¤ãƒ¬ãƒ™ãƒ«

  * [ ] pymooãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã‚‹
  * [ ] å¤šç›®çš„æœ€é©åŒ–å•é¡Œã®æ¦‚å¿µã‚’ç†è§£ã—ã¦ã„ã‚‹ï¼ˆã‚µã‚¤ã‚ºæœ€å°åŒ– vs åŠ¹ç‡æœ€å¤§åŒ–ï¼‰
  * [ ] ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆã®æ¦‚å¿µã‚’èª¬æ˜ã§ãã‚‹

#### å¿œç”¨ãƒ¬ãƒ™ãƒ«

  * [ ] pymoo.core.problemã‚’ç¶™æ‰¿ã—ã¦Problemã‚¯ãƒ©ã‚¹ã‚’å®šç¾©ã§ãã‚‹
  * [ ] NSGA-IIã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿè£…ã§ãã‚‹
  * [ ] ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆã‚’å¯è¦–åŒ–ã§ãã‚‹
  * [ ] ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•é–¢ä¿‚ã‚’è§£é‡ˆã§ãã‚‹

#### ä¸Šç´šãƒ¬ãƒ™ãƒ«

  * [ ] è¤‡æ•°ã®è§£ã‹ã‚‰ç”¨é€”ã«å¿œã˜ãŸæœ€é©è§£ã‚’é¸æŠã§ãã‚‹
  * é«˜æ€§èƒ½é‡è¦–
  * ç’°å¢ƒé‡è¦–
  * ãƒãƒ©ãƒ³ã‚¹å‹
  * [ ] ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã«ã‚ˆã‚‹ä»£æ›¿å®Ÿè£…ãŒã§ãã‚‹ï¼ˆpymooãŒä½¿ãˆãªã„å ´åˆï¼‰

* * *

### 3.12.7 ãƒŠãƒææ–™ç‰¹æœ‰ã®è§£æã‚¹ã‚­ãƒ«ï¼ˆNanomaterial-Specific Analysisï¼‰

#### TEMç”»åƒè§£æ

  * [ ] å¯¾æ•°æ­£è¦åˆ†å¸ƒã«å¾“ã†ã‚µã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã§ãã‚‹
  * [ ] å¯¾æ•°æ­£è¦åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆsigmaã€muï¼‰ã‚’è¨ˆç®—ã§ãã‚‹
  * [ ] `lognorm.fit`ã§ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã§ãã‚‹
  * [ ] ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœã‚’å¯è¦–åŒ–ã§ãã‚‹ï¼ˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  + PDFæ›²ç·šï¼‰
  * [ ] Q-Qãƒ—ãƒ­ãƒƒãƒˆã§åˆ†å¸ƒã®é©åˆåº¦ã‚’è©•ä¾¡ã§ãã‚‹

#### åˆ†å­å‹•åŠ›å­¦ï¼ˆMDï¼‰ãƒ‡ãƒ¼ã‚¿è§£æ

  * [ ] åŸå­è»Œè·¡ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã‚’ç†è§£ã—ã¦ã„ã‚‹ï¼ˆn_steps Ã— n_atoms Ã— 3ï¼‰
  * [ ] 3Dè»Œè·¡ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆã§ãã‚‹
  * [ ] å‹•å¾„åˆ†å¸ƒé–¢æ•°ï¼ˆRDFï¼‰ã‚’è¨ˆç®—ã§ãã‚‹
  * [ ] RDFã®ãƒ”ãƒ¼ã‚¯ä½ç½®ã‹ã‚‰ç‰¹å¾´çš„ãªåŸå­é–“è·é›¢ã‚’æŠ½å‡ºã§ãã‚‹
  * [ ] å¹³å‡äºŒä¹—å¤‰ä½ï¼ˆMSDï¼‰ã‚’è¨ˆç®—ã§ãã‚‹
  * [ ] MSDã‹ã‚‰æ‹¡æ•£ä¿‚æ•°ã‚’ç®—å‡ºã§ãã‚‹ï¼ˆEinsteiné–¢ä¿‚å¼ï¼‰

#### ç•°å¸¸æ¤œçŸ¥

  * [ ] Isolation Forestã‚’å®Ÿè£…ã§ãã‚‹
  * [ ] contaminationï¼ˆç•°å¸¸ãƒ‡ãƒ¼ã‚¿å‰²åˆï¼‰ã‚’è¨­å®šã§ãã‚‹
  * [ ] ç•°å¸¸ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã§ãã‚‹ï¼ˆ`score_samples`ï¼‰
  * [ ] æ··åŒè¡Œåˆ—ã§ç•°å¸¸æ¤œå‡ºç²¾åº¦ã‚’è©•ä¾¡ã§ãã‚‹
  * [ ] æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã¨ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã‚’å¯è¦–åŒ–ã§ãã‚‹

* * *

### 3.12.8 ã‚³ãƒ¼ãƒ‰å“è³ªã‚¹ã‚­ãƒ«ï¼ˆCode Qualityï¼‰

#### åŸºç¤ãƒ¬ãƒ™ãƒ«

  * [ ] ã™ã¹ã¦ã®ã‚³ãƒ¼ãƒ‰ã«ä¹±æ•°ã‚·ãƒ¼ãƒ‰ï¼ˆ`random_state=42`ï¼‰ã‚’è¨­å®šã—ã¦ã„ã‚‹
  * [ ] ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ï¼ˆshapeã€dtypeã€æ¬ æå€¤ã€ç¯„å›²ï¼‰ã‚’å®Ÿæ–½ã—ã¦ã„ã‚‹
  * [ ] å¤‰æ•°åãŒåˆ†ã‹ã‚Šã‚„ã™ã„ï¼ˆ`X_train`ã€`y_test`ã€`model_lgb`ï¼‰
  * [ ] ã‚³ãƒ¡ãƒ³ãƒˆã§å‡¦ç†ã®ç›®çš„ã‚’èª¬æ˜ã—ã¦ã„ã‚‹
  * [ ] ã‚°ãƒ©ãƒ•ã«ã‚¿ã‚¤ãƒˆãƒ«ã€è»¸ãƒ©ãƒ™ãƒ«ã€å‡¡ä¾‹ã‚’è¿½åŠ ã—ã¦ã„ã‚‹

#### å¿œç”¨ãƒ¬ãƒ™ãƒ«

  * [ ] é–¢æ•°åŒ–ã—ã¦ã‚³ãƒ¼ãƒ‰ã‚’å†åˆ©ç”¨å¯èƒ½ã«ã—ã¦ã„ã‚‹ `python def calculate_rdf(positions, r_max, n_bins): ...`
  * [ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ–‡å­—åˆ—ï¼ˆDocstringï¼‰ã‚’è¨˜è¿°ã—ã¦ã„ã‚‹
  * [ ] ã‚°ãƒ©ãƒ•ã®ç¾è¦³ã‚’æ•´ãˆã¦ã„ã‚‹ï¼ˆfontsizeã€gridã€alphaï¼‰
  * [ ] try-exceptã§ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ã—ã¦ã„ã‚‹ï¼ˆpymooã®ImportErrorå¯¾å¿œï¼‰

* * *

### 3.12.9 ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¹ã‚­ãƒ«ï¼ˆTroubleshootingï¼‰

#### åŸºç¤ãƒ¬ãƒ™ãƒ«ï¼ˆã‚¨ãƒ©ãƒ¼å¯¾å‡¦ï¼‰

  * [ ] `ModuleNotFoundError`ã‚’è§£æ±ºã§ãã‚‹ï¼ˆ`pip install`ï¼‰
  * [ ] `ValueError: Input contains NaN`ã‚’è§£æ±ºã§ãã‚‹ï¼ˆæ¬ æå€¤å‡¦ç†ï¼‰
  * [ ] `ConvergenceWarning`ï¼ˆMLPã®åæŸã‚¨ãƒ©ãƒ¼ï¼‰ã‚’è§£æ±ºã§ãã‚‹
  * `max_iter`ã‚’å¢—ã‚„ã™
  * ãƒ‡ãƒ¼ã‚¿ã‚’æ¨™æº–åŒ–ã™ã‚‹
  * Early Stoppingã‚’æœ‰åŠ¹åŒ–
  * [ ] ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’èª­ã¿ã€æ¤œç´¢ã—ã¦è§£æ±ºç­–ã‚’è¦‹ã¤ã‘ã‚‰ã‚Œã‚‹

#### å¿œç”¨ãƒ¬ãƒ™ãƒ«ï¼ˆæ€§èƒ½æ”¹å–„ï¼‰

  * [ ] RÂ² < 0.7ã®å ´åˆã€3ã¤ä»¥ä¸Šã®æ”¹å–„ç­–ã‚’å®Ÿè¡Œã§ãã‚‹
  * ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
  * ãƒ¢ãƒ‡ãƒ«å¤‰æ›´ï¼ˆç·šå½¢â†’éç·šå½¢ï¼‰
  * ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
  * [ ] éå­¦ç¿’ã‚’æ¤œå‡ºã§ãã‚‹ï¼ˆè¨“ç·´RÂ² â‰« ãƒ†ã‚¹ãƒˆRÂ²ï¼‰
  * [ ] æœªå­¦ç¿’ã‚’æ¤œå‡ºã§ãã‚‹ï¼ˆè¨“ç·´RÂ²ã‚‚ãƒ†ã‚¹ãƒˆRÂ²ã‚‚ä½ã„ï¼‰

* * *

### 3.12.10 ç·åˆè©•ä¾¡ï¼šç¿’ç†Ÿåº¦ãƒ¬ãƒ™ãƒ«åˆ¤å®š

ä»¥ä¸‹ã®ãƒ¬ãƒ™ãƒ«åˆ¤å®šã§ã€è‡ªåˆ†ã®åˆ°é”åº¦ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

#### ãƒ¬ãƒ™ãƒ«1ï¼šåˆå¿ƒè€…ï¼ˆBeginnerï¼‰

  * ç’°å¢ƒæ§‹ç¯‰ã‚¹ã‚­ãƒ«ï¼šåŸºç¤ãƒ¬ãƒ™ãƒ« 100%é”æˆ
  * ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»å¯è¦–åŒ–ã‚¹ã‚­ãƒ«ï¼šåŸºç¤ãƒ¬ãƒ™ãƒ« 80%ä»¥ä¸Šé”æˆ
  * æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«å®Ÿè£…ã‚¹ã‚­ãƒ«ï¼šåŸºç¤ãƒ¬ãƒ™ãƒ« 5ã¤ä¸­3ã¤ä»¥ä¸Šå®Ÿè£…
  * ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼šåŸºç¤ãƒ¬ãƒ™ãƒ«ã®ã‚¨ãƒ©ãƒ¼ã‚’è‡ªåŠ›ã§è§£æ±º

**åˆ°é”ç›®æ¨™:** ãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆãƒ»å¯è¦–åŒ–ã—ã€ç·šå½¢å›å¸°ã¨ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã§LSPRæ³¢é•·äºˆæ¸¬ã‚’å®Ÿè£…ã§ãã‚‹

* * *

#### ãƒ¬ãƒ™ãƒ«2ï¼šä¸­ç´šè€…ï¼ˆIntermediateï¼‰

  * ç’°å¢ƒæ§‹ç¯‰ã‚¹ã‚­ãƒ«ï¼šå¿œç”¨ãƒ¬ãƒ™ãƒ« 80%ä»¥ä¸Šé”æˆ
  * ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»å¯è¦–åŒ–ã‚¹ã‚­ãƒ«ï¼šåŸºç¤ãƒ¬ãƒ™ãƒ« 100%é”æˆ + å¿œç”¨ãƒ¬ãƒ™ãƒ« 70%ä»¥ä¸Š
  * æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«å®Ÿè£…ã‚¹ã‚­ãƒ«ï¼šåŸºç¤ãƒ¬ãƒ™ãƒ« 100%é”æˆ + å¿œç”¨ãƒ¬ãƒ™ãƒ« 70%ä»¥ä¸Š
  * ç‰¹å¾´é‡é‡è¦åº¦ã¨ãƒ¢ãƒ‡ãƒ«è§£é‡ˆã‚¹ã‚­ãƒ«ï¼šåŸºç¤ãƒ¬ãƒ™ãƒ« 100%é”æˆ + å¿œç”¨ãƒ¬ãƒ™ãƒ« 50%ä»¥ä¸Š
  * ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚¹ã‚­ãƒ«ï¼šåŸºç¤ãƒ¬ãƒ™ãƒ« 100%é”æˆ + å¿œç”¨ãƒ¬ãƒ™ãƒ« 50%ä»¥ä¸Š

**åˆ°é”ç›®æ¨™:** 5ã¤ã®å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã—ã€ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§ç›®æ¨™LSPRæ³¢é•·ï¼ˆ550 nmï¼‰ã‚’é”æˆã™ã‚‹åˆæˆæ¡ä»¶ã‚’ç™ºè¦‹ã§ãã‚‹

* * *

#### ãƒ¬ãƒ™ãƒ«3ï¼šä¸Šç´šè€…ï¼ˆAdvancedï¼‰

  * å…¨ã‚«ãƒ†ã‚´ãƒªï¼šå¿œç”¨ãƒ¬ãƒ™ãƒ« 100%é”æˆ
  * ç‰¹å¾´é‡é‡è¦åº¦ã¨ãƒ¢ãƒ‡ãƒ«è§£é‡ˆã‚¹ã‚­ãƒ«ï¼šä¸Šç´šãƒ¬ãƒ™ãƒ« 80%ä»¥ä¸Š
  * ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚¹ã‚­ãƒ«ï¼šä¸Šç´šãƒ¬ãƒ™ãƒ« 80%ä»¥ä¸Š
  * å¤šç›®çš„æœ€é©åŒ–ã‚¹ã‚­ãƒ«ï¼šå¿œç”¨ãƒ¬ãƒ™ãƒ« 100%é”æˆ
  * ãƒŠãƒææ–™ç‰¹æœ‰ã®è§£æã‚¹ã‚­ãƒ«ï¼šTEMã€MDã€ç•°å¸¸æ¤œçŸ¥ã™ã¹ã¦å®Ÿè£…

**åˆ°é”ç›®æ¨™:** SHAPåˆ†æã§ãƒ¢ãƒ‡ãƒ«è§£é‡ˆã—ã€å¤šç›®çš„æœ€é©åŒ–ã§ã‚µã‚¤ã‚ºã¨åŠ¹ç‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å¯è¦–åŒ–ã§ãã‚‹

* * *

#### ãƒ¬ãƒ™ãƒ«4ï¼šã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆï¼ˆExpertï¼‰

  * å…¨ã‚«ãƒ†ã‚´ãƒªï¼šä¸Šç´šãƒ¬ãƒ™ãƒ« 80%ä»¥ä¸Šé”æˆ
  * ã‚³ãƒ¼ãƒ‰å“è³ªï¼šå¿œç”¨ãƒ¬ãƒ™ãƒ« 100%é”æˆ
  * ç‹¬è‡ªã®ãƒŠãƒææ–™ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯æ–‡çŒ®ãƒ‡ãƒ¼ã‚¿ï¼‰ã«é©ç”¨ã§ãã‚‹
  * ã‚«ã‚¹ã‚¿ãƒ æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã§ãã‚‹
  * ç ”ç©¶æˆæœã‚’å­¦ä¼šç™ºè¡¨ãƒ»è«–æ–‡æŠ•ç¨¿ã§ãã‚‹

**åˆ°é”ç›®æ¨™:** \- å®ŸãƒŠãƒææ–™ãƒ‡ãƒ¼ã‚¿ï¼ˆTEMã€UV-Visã€XRDï¼‰ã‚’çµ±åˆè§£æ \- æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚Šæ–°è¦ãƒŠãƒç²’å­ã®ç‰©æ€§ã‚’90%ä»¥ä¸Šã®ç²¾åº¦ã§äºˆæ¸¬ \- ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§å®Ÿé¨“å›æ•°ã‚’å¾“æ¥ã®1/5ã«å‰Šæ¸›

* * *

### 3.12.11 å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ï¼šæ¼”ç¿’å•é¡Œã®å®Œé‚

#### æ¼”ç¿’1å®Œé‚ç¢ºèªï¼ˆCNTé›»æ°—ä¼å°åº¦äºˆæ¸¬ï¼‰

  * [ ] ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆ150ã‚µãƒ³ãƒ—ãƒ«ã€3ç‰¹å¾´é‡ï¼‰ã‚’å®Ÿè£…
  * [ ] LightGBMãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ã‚’å®Ÿè£…
  * [ ] RÂ² > 0.8ã€RMSE < 0.5ã‚’é”æˆ
  * [ ] ç‰¹å¾´é‡é‡è¦åº¦ã‚’å¯è¦–åŒ–
  * [ ] çµæœã‚’è§£é‡ˆï¼ˆã©ã®ç‰¹å¾´é‡ãŒæœ€ã‚‚å½±éŸ¿ã™ã‚‹ã‹ï¼‰

#### æ¼”ç¿’2å®Œé‚ç¢ºèªï¼ˆéŠ€ãƒŠãƒç²’å­æœ€é©åˆæˆæ¡ä»¶ï¼‰

  * [ ] éŠ€ãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ï¼ˆ100ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã‚’ç”Ÿæˆ
  * [ ] LightGBMãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰
  * [ ] ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’å®Ÿè¡Œï¼ˆ40å›è©•ä¾¡ï¼‰
  * [ ] ç›®æ¨™ã‚µã‚¤ã‚ºï¼ˆ10 nmï¼‰ã¨ã®èª¤å·® < 1 nmã‚’é”æˆ
  * [ ] æœ€é©æ¸©åº¦ãƒ»pHã‚’ç‰¹å®š

#### æ¼”ç¿’3å®Œé‚ç¢ºèªï¼ˆé‡å­ãƒ‰ãƒƒãƒˆå¤šè‰²ç™ºå…‰è¨­è¨ˆï¼‰

  * [ ] èµ¤ãƒ»ç·‘ãƒ»é’ã®3è‰²ã«ã¤ã„ã¦æœ€é©åŒ–ã‚’å®Ÿè¡Œ
  * [ ] å„è‰²ã®æœ€é©ã‚µã‚¤ã‚ºãƒ»åˆæˆæ¡ä»¶ã‚’ç‰¹å®š
  * [ ] äºˆæ¸¬æ³¢é•·ãŒç›®æ¨™æ³¢é•·Â±10 nmä»¥å†…ã«åã¾ã£ã¦ã„ã‚‹
  * [ ] çµæœã‚’å¯è¦–åŒ–ï¼ˆã‚µã‚¤ã‚º vs æ³¢é•·ã®ãƒ—ãƒ­ãƒƒãƒˆï¼‰

* * *

### 3.12.12 æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸ã®æº–å‚™åº¦ãƒã‚§ãƒƒã‚¯

#### å®Ÿä¸–ç•Œå¿œç”¨ï¼ˆChapter 4ï¼‰ã¸ã®æº–å‚™

  * [ ] æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆãƒ‡ãƒ¼ã‚¿æº–å‚™â†’ãƒ¢ãƒ‡ãƒ«è¨“ç·´â†’è©•ä¾¡â†’æœ€é©åŒ–ï¼‰ã‚’ç†è§£ã—ã¦ã„ã‚‹
  * [ ] ãƒŠãƒææ–™ç‰¹æœ‰ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚µã‚¤ã‚ºåˆ†å¸ƒã€å…‰å­¦ç‰¹æ€§ã€é›»æ°—ç‰¹æ€§ï¼‰ã‚’æ‰±ãˆã‚‹
  * [ ] æœ€é©åŒ–æ‰‹æ³•ï¼ˆãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã€å¤šç›®çš„æœ€é©åŒ–ï¼‰ã‚’å®Ÿè£…ã§ãã‚‹
  * [ ] ãƒ¢ãƒ‡ãƒ«è§£é‡ˆï¼ˆSHAPï¼‰ã®é‡è¦æ€§ã‚’ç†è§£ã—ã¦ã„ã‚‹

#### æ·±å±¤å­¦ç¿’ãƒ»ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¸ã®æº–å‚™

  * [ ] ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆMLPï¼‰ã‚’å®Ÿè£…ã—ã€æ´»æ€§åŒ–é–¢æ•°ãƒ»æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç†è§£ã—ã¦ã„ã‚‹
  * [ ] å­¦ç¿’æ›²ç·šã‚’å¯è¦–åŒ–ã—ã€éå­¦ç¿’ã‚’æ¤œå‡ºã§ãã‚‹
  * [ ] Early Stoppingã®æ¦‚å¿µã‚’ç†è§£ã—ã¦ã„ã‚‹

#### å®Ÿå‹™ç ”ç©¶ã¸ã®æº–å‚™

  * [ ] Jupyter Notebookã¾ãŸã¯Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã‚³ãƒ¼ãƒ‰ã‚’ç®¡ç†ã§ãã‚‹
  * [ ] requirements.txtã§ç’°å¢ƒã‚’å†ç¾å¯èƒ½ã«ã—ã¦ã„ã‚‹
  * [ ] äºˆæ¸¬çµæœã‚’ã‚°ãƒ©ãƒ•åŒ–ã—ã€ãƒ¬ãƒãƒ¼ãƒˆã«ã¾ã¨ã‚ã‚‰ã‚Œã‚‹
  * [ ] ã‚³ãƒ¼ãƒ‰ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¨˜è¿°ã—ã¦ã„ã‚‹

* * *

**ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆæ´»ç”¨ã®ãƒ’ãƒ³ãƒˆ:** 1\. **å®šæœŸçš„ã«è¦‹ç›´ã™** : å­¦ç¿’å¾Œã€1é€±é–“å¾Œã€1ãƒ¶æœˆå¾Œã«å†ãƒã‚§ãƒƒã‚¯ 2\. **æœªé”æˆé …ç›®ã‚’å„ªå…ˆ** : ãƒã‚§ãƒƒã‚¯ã§ããªã„é …ç›®ã‚’é›†ä¸­å­¦ç¿’ 3\. **ãƒ¬ãƒ™ãƒ«åˆ¤å®šã‚’è¨˜éŒ²** : æˆé•·ã‚’å¯è¦–åŒ–ã—ã¦ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ç¶­æŒ 4\. **å®Ÿãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã®æ´»ç”¨** : ç ”ç©¶ãƒ»é–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–‹å§‹å‰ã«å¿…é ˆã‚¹ã‚­ãƒ«ã‚’ç¢ºèª

* * *

## å‚è€ƒæ–‡çŒ®

  1. **Pedregosa, F. et al.** (2011). Scikit-learn: Machine Learning in Python. _Journal of Machine Learning Research_ , 12, 2825-2830.

  2. **Ke, G. et al.** (2017). LightGBM: A highly efficient gradient boosting decision tree. _Advances in Neural Information Processing Systems_ , 30, 3146-3154.

  3. **Lundberg, S. M. & Lee, S.-I.** (2017). A unified approach to interpreting model predictions. _Advances in Neural Information Processing Systems_ , 30, 4765-4774.

  4. **Snoek, J., Larochelle, H., & Adams, R. P.** (2012). Practical Bayesian optimization of machine learning algorithms. _Advances in Neural Information Processing Systems_ , 25, 2951-2959.

  5. **Deb, K. et al.** (2002). A fast and elitist multiobjective genetic algorithm: NSGA-II. _IEEE Transactions on Evolutionary Computation_ , 6(2), 182-197. [DOI: 10.1109/4235.996017](<https://doi.org/10.1109/4235.996017>)

  6. **Frenkel, D. & Smit, B.** (2001). _Understanding Molecular Simulation: From Algorithms to Applications_ (2nd ed.). Academic Press.

* * *

[â† å‰ç« ï¼šãƒŠãƒææ–™ã®åŸºç¤åŸç†](<chapter2-fundamentals.html>) | [æ¬¡ç« ï¼šå®Ÿä¸–ç•Œã®å¿œç”¨ã¨ã‚­ãƒ£ãƒªã‚¢ â†’](<chapter4-real-world.html>)
