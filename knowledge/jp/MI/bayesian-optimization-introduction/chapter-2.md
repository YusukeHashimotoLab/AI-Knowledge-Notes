---
title: ç¬¬2ç« ï¼šãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ç†è«–
chapter_title: ç¬¬2ç« ï¼šãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ç†è«–
subtitle: ã‚¬ã‚¦ã‚¹éç¨‹ã¨ç²å¾—é–¢æ•°ã§æ¢ç´¢ã‚’æœ€é©åŒ–ã™ã‚‹
reading_time: 25-30åˆ†
difficulty: åˆç´š
code_examples: 10
exercises: 3
---

# ç¬¬2ç« ï¼šãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ç†è«–

ã‚¬ã‚¦ã‚¹éç¨‹ã¨ç²å¾—é–¢æ•°ï¼ˆEI/UCB/PIï¼‰ã®å½¹å‰²åˆ†æ‹…ã‚’å›³è§£ã‚¤ãƒ¡ãƒ¼ã‚¸ã§æ´ã¿ã¾ã™ã€‚æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹ã®ä»˜ã‘æ–¹ã‚’å­¦ã³ã¾ã™ã€‚

**ğŸ’¡ è£œè¶³:** EIã¯â€œè‰¯ã•ãã†ãªæ‰€ã‚’æ·±æ˜ã‚Šâ€ã€UCBã¯â€œä¸ç¢ºã‹ãªæ‰€ã‚’ç¢ºèªâ€ã€‚çŠ¶æ³ã«å¿œã˜ã¦åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚

**ã‚¬ã‚¦ã‚¹éç¨‹ã¨ç²å¾—é–¢æ•°ã§æ¢ç´¢ã‚’æœ€é©åŒ–ã™ã‚‹**

## å­¦ç¿’ç›®æ¨™

ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š

  * âœ… ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã®åŸºæœ¬åŸç†ã‚’ç†è§£ã§ãã‚‹
  * âœ… ä»£ç†ãƒ¢ãƒ‡ãƒ«ã®å½¹å‰²ã¨æ§‹ç¯‰æ–¹æ³•ã‚’èª¬æ˜ã§ãã‚‹
  * âœ… 3ã¤ã®ä¸»è¦ãªç²å¾—é–¢æ•°ï¼ˆEIã€PIã€UCBï¼‰ã‚’å®Ÿè£…ã§ãã‚‹
  * âœ… æ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æ•°å¼ã§è¡¨ç¾ã§ãã‚‹
  * âœ… ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–ã¨ãã®é‡è¦æ€§ã‚’ç†è§£ã§ãã‚‹

**èª­äº†æ™‚é–“** : 25-30åˆ† **ã‚³ãƒ¼ãƒ‰ä¾‹** : 10å€‹ **æ¼”ç¿’å•é¡Œ** : 3å•

* * *

## 2.1 ä»£ç†ãƒ¢ãƒ‡ãƒ«ã¨ã¯

### ãªãœä»£ç†ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã‹

ææ–™æ¢ç´¢ã«ãŠã„ã¦ã€çœŸã®ç›®çš„é–¢æ•°ï¼ˆä¾‹ï¼šã‚¤ã‚ªãƒ³ä¼å°åº¦ã€è§¦åª’æ´»æ€§ï¼‰ã‚’è©•ä¾¡ã™ã‚‹ã«ã¯**å®Ÿé¨“ãŒå¿…è¦** ã§ã™ã€‚ã—ã‹ã—ã€å®Ÿé¨“ã¯ï¼š

  * **æ™‚é–“ãŒã‹ã‹ã‚‹** : 1ã‚µãƒ³ãƒ—ãƒ«æ•°æ™‚é–“ï½æ•°æ—¥
  * **ã‚³ã‚¹ãƒˆãŒé«˜ã„** : ææ–™è²»ã€è£…ç½®è²»ã€äººä»¶è²»
  * **å›æ•°ã«åˆ¶é™** : äºˆç®—ãƒ»æ™‚é–“ã®åˆ¶ç´„

ãã“ã§ã€**å°‘æ•°ã®å®Ÿé¨“çµæœã‹ã‚‰ç›®çš„é–¢æ•°ã‚’æ¨å®šã™ã‚‹ãƒ¢ãƒ‡ãƒ«** ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚ã“ã‚ŒãŒ**ä»£ç†ãƒ¢ãƒ‡ãƒ«ï¼ˆSurrogate Modelï¼‰** ã§ã™ã€‚

### ä»£ç†ãƒ¢ãƒ‡ãƒ«ã®å½¹å‰²
    
    
    ```mermaid
    flowchart LR
        A[å°‘æ•°ã®å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿\nä¾‹: 10-20ç‚¹] --> B[ä»£ç†ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰\nã‚¬ã‚¦ã‚¹éç¨‹å›å¸°]
        B --> C[æœªçŸ¥é ˜åŸŸã®äºˆæ¸¬\nå¹³å‡ + ä¸ç¢ºå®Ÿæ€§]
        C --> D[ç²å¾—é–¢æ•°ã®è¨ˆç®—\næ¬¡ã®å®Ÿé¨“ç‚¹ã‚’ææ¡ˆ]
        D --> E[å®Ÿé¨“å®Ÿè¡Œ\næ–°ãƒ‡ãƒ¼ã‚¿å–å¾—]
        E --> B
    
        style A fill:#e3f2fd
        style B fill:#fff3e0
        style C fill:#f3e5f5
        style D fill:#e8f5e9
        style E fill:#fce4ec
    ```

**ä»£ç†ãƒ¢ãƒ‡ãƒ«ã®è¦ä»¶** : 1\. **å°‘æ•°ãƒ‡ãƒ¼ã‚¿ã§ã‚‚æ©Ÿèƒ½** : 10-20ç‚¹ç¨‹åº¦ã§äºˆæ¸¬å¯èƒ½ 2\. **ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–** : äºˆæ¸¬ã®ä¿¡é ¼æ€§ã‚’è©•ä¾¡ 3\. **é«˜é€Ÿ** : ä½•åƒç‚¹ã§ã‚‚ç¬æ™‚ã«äºˆæ¸¬ 4\. **æŸ”è»Ÿ** : è¤‡é›‘ãªé–¢æ•°å½¢çŠ¶ã«å¯¾å¿œ

**ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ï¼ˆGaussian Process Regressionï¼‰** ã¯ã€ã“ã‚Œã‚‰ã®è¦ä»¶ã‚’æº€ãŸã™å¼·åŠ›ãªæ‰‹æ³•ã§ã™ã€‚

* * *

## 2.2 ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã®åŸºç¤

### ã‚¬ã‚¦ã‚¹éç¨‹ã¨ã¯

**ã‚¬ã‚¦ã‚¹éç¨‹ï¼ˆGaussian Process, GPï¼‰** ã¯ã€é–¢æ•°ã®ç¢ºç‡åˆ†å¸ƒã‚’å®šç¾©ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚

**å®šç¾©** :

> ã‚¬ã‚¦ã‚¹éç¨‹ã¨ã¯ã€ä»»æ„ã®æœ‰é™å€‹ã®ç‚¹ã§ã®é–¢æ•°å€¤ãŒ**å¤šå¤‰é‡ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã«å¾“ã†** ã‚ˆã†ãªç¢ºç‡éç¨‹ã§ã‚ã‚‹ã€‚

**ç›´æ„Ÿçš„ç†è§£** : \- 1ã¤ã®é–¢æ•°ã§ã¯ãªãã€**ç„¡æ•°ã®é–¢æ•°ã®åˆ†å¸ƒ** ã‚’è€ƒãˆã‚‹ \- è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã€é–¢æ•°ã®åˆ†å¸ƒã‚’æ›´æ–° \- å„ç‚¹ã§ã®äºˆæ¸¬å€¤ã¯**å¹³å‡ã¨åˆ†æ•£** ã§è¡¨ç¾

### ã‚¬ã‚¦ã‚¹éç¨‹ã®æ•°å­¦çš„å®šç¾©

ã‚¬ã‚¦ã‚¹éç¨‹ã¯ã€**å¹³å‡é–¢æ•°** $m(x)$ã¨**ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ï¼ˆå…±åˆ†æ•£é–¢æ•°ï¼‰** $k(x, x')$ã§å®Œå…¨ã«å®šç¾©ã•ã‚Œã¾ã™ï¼š

$$ f(x) \sim \mathcal{GP}(m(x), k(x, x')) $$

**å¹³å‡é–¢æ•°** $m(x)$: \- é€šå¸¸ã¯ $m(x) = 0$ ã¨ä»®å®šï¼ˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ï¼‰

**ã‚«ãƒ¼ãƒãƒ«é–¢æ•°** $k(x, x')$: \- 2ç‚¹é–“ã®ã€Œé¡ä¼¼åº¦ã€ã‚’è¡¨ã™ \- å…¥åŠ›ãŒè¿‘ã„ã»ã©ã€å‡ºåŠ›ã‚‚ä¼¼ã¦ã„ã‚‹ã¨ä»®å®š

### ä»£è¡¨çš„ãªã‚«ãƒ¼ãƒãƒ«é–¢æ•°

**1\. RBFï¼ˆRadial Basis Functionï¼‰ã‚«ãƒ¼ãƒãƒ«**

$$ k(x, x') = \sigma^2 \exp\left(-\frac{||x - x'||^2}{2\ell^2}\right) $$

  * $\sigma^2$: åˆ†æ•£ï¼ˆå‡ºåŠ›ã®ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
  * $\ell$: é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆã©ã‚Œã ã‘æ»‘ã‚‰ã‹ã‹ï¼‰

**ç‰¹å¾´** : \- æœ€ã‚‚ä¸€èˆ¬çš„ \- ç„¡é™å›å¾®åˆ†å¯èƒ½ï¼ˆæ»‘ã‚‰ã‹ãªé–¢æ•°ï¼‰ \- ææ–™ç‰¹æ€§äºˆæ¸¬ã«é©ã—ã¦ã„ã‚‹

**ã‚³ãƒ¼ãƒ‰ä¾‹1: RBFã‚«ãƒ¼ãƒãƒ«ã®å¯è¦–åŒ–**
    
    
    # RBFã‚«ãƒ¼ãƒãƒ«ã®å¯è¦–åŒ–
    import numpy as np
    import matplotlib.pyplot as plt
    
    def rbf_kernel(x1, x2, sigma=1.0, length_scale=1.0):
        """
        RBFã‚«ãƒ¼ãƒãƒ«é–¢æ•°
    
        Parameters:
        -----------
        x1, x2 : array
            å…¥åŠ›ç‚¹
        sigma : float
            æ¨™æº–åå·®ï¼ˆå‡ºåŠ›ã®ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        length_scale : float
            é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆå…¥åŠ›ã®ç›¸é–¢è·é›¢ï¼‰
    
        Returns:
        --------
        float : ã‚«ãƒ¼ãƒãƒ«å€¤ï¼ˆé¡ä¼¼åº¦ï¼‰
        """
        distance = np.abs(x1 - x2)
        return sigma**2 * np.exp(-0.5 * (distance / length_scale)**2)
    
    # ç•°ãªã‚‹é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ã§ã‚«ãƒ¼ãƒãƒ«ã‚’å¯è¦–åŒ–
    x_ref = 0.5  # å‚ç…§ç‚¹
    x_range = np.linspace(0, 1, 100)
    
    plt.figure(figsize=(12, 4))
    
    # å·¦å›³: é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ã®å½±éŸ¿
    plt.subplot(1, 3, 1)
    for length_scale in [0.05, 0.1, 0.2, 0.5]:
        k_values = [rbf_kernel(x_ref, x, sigma=1.0,
                               length_scale=length_scale)
                    for x in x_range]
        plt.plot(x_range, k_values,
                 label=f'$\\ell$ = {length_scale}', linewidth=2)
    plt.axvline(x_ref, color='black', linestyle='--', alpha=0.5)
    plt.xlabel('x', fontsize=12)
    plt.ylabel('k(0.5, x)', fontsize=12)
    plt.title('é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ã®å½±éŸ¿', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # ä¸­å¤®å›³: è¤‡æ•°ã®å‚ç…§ç‚¹
    plt.subplot(1, 3, 2)
    for x_ref_temp in [0.2, 0.5, 0.8]:
        k_values = [rbf_kernel(x_ref_temp, x, sigma=1.0,
                               length_scale=0.1)
                    for x in x_range]
        plt.plot(x_range, k_values,
                 label=f'x_ref = {x_ref_temp}', linewidth=2)
    plt.xlabel('x', fontsize=12)
    plt.ylabel('k(x_ref, x)', fontsize=12)
    plt.title('å‚ç…§ç‚¹ã®å½±éŸ¿', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å³å›³: ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—ã®å¯è¦–åŒ–
    plt.subplot(1, 3, 3)
    x_grid = np.linspace(0, 1, 50)
    K = np.zeros((len(x_grid), len(x_grid)))
    for i, x1 in enumerate(x_grid):
        for j, x2 in enumerate(x_grid):
            K[i, j] = rbf_kernel(x1, x2, sigma=1.0, length_scale=0.1)
    
    plt.imshow(K, cmap='viridis', origin='lower', extent=[0, 1, 0, 1])
    plt.colorbar(label='ã‚«ãƒ¼ãƒãƒ«å€¤')
    plt.xlabel('x', fontsize=12)
    plt.ylabel("x'", fontsize=12)
    plt.title('ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—', fontsize=14)
    
    plt.tight_layout()
    plt.savefig('rbf_kernel_visualization.png', dpi=150,
                bbox_inches='tight')
    plt.show()
    
    print("RBFã‚«ãƒ¼ãƒãƒ«ã®ç‰¹æ€§:")
    print("  - é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ãŒå°ã•ã„ â†’ å±€æ‰€çš„ãªç›¸é–¢ï¼ˆé‹­ã„ãƒ”ãƒ¼ã‚¯ï¼‰")
    print("  - é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ãŒå¤§ãã„ â†’ åºƒç¯„å›²ã®ç›¸é–¢ï¼ˆãªã ã‚‰ã‹ãªæ›²ç·šï¼‰")
    print("  - å¯¾è§’ç·šä¸Šï¼ˆx = x'ï¼‰ã§ã‚«ãƒ¼ãƒãƒ«å€¤ãŒæœ€å¤§")
    

**é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ** : \- **é•·ã•ã‚¹ã‚±ãƒ¼ãƒ« $\ell$** : é–¢æ•°ã®æ»‘ã‚‰ã‹ã•ã‚’åˆ¶å¾¡ \- å°ã•ã„ $\ell$ â†’ æ€¥å³»ãªå¤‰åŒ–ã‚’è¨±å®¹ \- å¤§ãã„ $\ell$ â†’ æ»‘ã‚‰ã‹ãªé–¢æ•°ã‚’ä»®å®š \- **ææ–™ç§‘å­¦ã§ã®æ„å‘³** : çµ„æˆã‚„æ¡ä»¶ãŒè¿‘ã„ã¨ã€ç‰¹æ€§ã‚‚ä¼¼ã¦ã„ã‚‹ã¨ä»®å®š

* * *

### ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã®äºˆæ¸¬å¼

è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ $\mathcal{D} = {(x_1, y_1), \ldots, (x_n, y_n)}$ ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€æ–°ã—ã„ç‚¹ $x_*$ ã§ã®äºˆæ¸¬ã¯ï¼š

**äºˆæ¸¬å¹³å‡** : $$ \mu(x_*) = k_* K^{-1} \mathbf{y} $$

**äºˆæ¸¬åˆ†æ•£** : $$ \sigma^2(x_*) = k(x_*, x_*) - k_*^T K^{-1} k_* $$

ã“ã“ã§ï¼š \- $K$: è¦³æ¸¬ç‚¹é–“ã®ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ— $K_{ij} = k(x_i, x_j)$ \- $k_*$: æ–°ã—ã„ç‚¹ã¨è¦³æ¸¬ç‚¹é–“ã®ã‚«ãƒ¼ãƒãƒ«ãƒ™ã‚¯ãƒˆãƒ« \- $\mathbf{y}$: è¦³æ¸¬å€¤ã®ãƒ™ã‚¯ãƒˆãƒ«

**äºˆæ¸¬åˆ†å¸ƒ** : $$ f(x_*) | \mathcal{D} \sim \mathcal{N}(\mu(x_*), \sigma^2(x_*)) $$

**ã‚³ãƒ¼ãƒ‰ä¾‹2: ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã®å®Ÿè£…ã¨å¯è¦–åŒ–**
    
    
    # ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã®å®Ÿè£…
    import numpy as np
    import matplotlib.pyplot as plt
    from scipy.spatial.distance import cdist
    
    class GaussianProcessRegressor:
        """
        ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã®ç°¡æ˜“å®Ÿè£…
    
        Parameters:
        -----------
        kernel : str
            ã‚«ãƒ¼ãƒãƒ«ç¨®é¡ï¼ˆ'rbf'ã®ã¿ã‚µãƒãƒ¼ãƒˆï¼‰
        sigma : float
            ã‚«ãƒ¼ãƒãƒ«ã®æ¨™æº–åå·®
        length_scale : float
            ã‚«ãƒ¼ãƒãƒ«ã®é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«
        noise : float
            è¦³æ¸¬ãƒã‚¤ã‚ºã®æ¨™æº–åå·®
        """
    
        def __init__(self, kernel='rbf', sigma=1.0,
                     length_scale=0.1, noise=0.01):
            self.kernel = kernel
            self.sigma = sigma
            self.length_scale = length_scale
            self.noise = noise
            self.X_train = None
            self.y_train = None
            self.K_inv = None
    
        def _kernel(self, X1, X2):
            """ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—ã®è¨ˆç®—"""
            if self.kernel == 'rbf':
                dists = cdist(X1, X2, 'sqeuclidean')
                K = self.sigma**2 * np.exp(-0.5 * dists /
                                            self.length_scale**2)
                return K
            else:
                raise ValueError(f"Unknown kernel: {self.kernel}")
    
        def fit(self, X_train, y_train):
            """
            ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
    
            Parameters:
            -----------
            X_train : array (n_samples, n_features)
                è¨“ç·´å…¥åŠ›
            y_train : array (n_samples,)
                è¨“ç·´å‡ºåŠ›
            """
            self.X_train = X_train
            self.y_train = y_train
    
            # ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—ã‚’è¨ˆç®—ï¼ˆãƒã‚¤ã‚ºé …ã‚’è¿½åŠ ï¼‰
            K = self._kernel(X_train, X_train)
            K += self.noise**2 * np.eye(len(X_train))
    
            # é€†è¡Œåˆ—ã‚’è¨ˆç®—ï¼ˆäºˆæ¸¬ã§ä½¿ç”¨ï¼‰
            self.K_inv = np.linalg.inv(K)
    
        def predict(self, X_test, return_std=False):
            """
            äºˆæ¸¬ã‚’å®Ÿè¡Œ
    
            Parameters:
            -----------
            X_test : array (n_test, n_features)
                ãƒ†ã‚¹ãƒˆå…¥åŠ›
            return_std : bool
                æ¨™æº–åå·®ã‚‚è¿”ã™ã‹
    
            Returns:
            --------
            mean : array (n_test,)
                äºˆæ¸¬å¹³å‡
            std : array (n_test,) (if return_std=True)
                äºˆæ¸¬æ¨™æº–åå·®
            """
            # k_* = k(X_test, X_train)
            k_star = self._kernel(X_test, self.X_train)
    
            # äºˆæ¸¬å¹³å‡: Î¼(x_*) = k_* K^{-1} y
            mean = k_star @ self.K_inv @ self.y_train
    
            if return_std:
                # k(x_*, x_*)
                k_starstar = self._kernel(X_test, X_test)
    
                # äºˆæ¸¬åˆ†æ•£: ÏƒÂ²(x_*) = k(x_*, x_*) - k_*^T K^{-1} k_*
                variance = np.diag(k_starstar) - np.sum(
                    (k_star @ self.K_inv) * k_star, axis=1
                )
                std = np.sqrt(np.maximum(variance, 0))  # æ•°å€¤èª¤å·®å¯¾ç­–
                return mean, std
            else:
                return mean
    
    # ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: ææ–™ã®ã‚¤ã‚ªãƒ³ä¼å°åº¦äºˆæ¸¬
    np.random.seed(42)
    
    # çœŸã®é–¢æ•°ï¼ˆæœªçŸ¥ã¨ä»®å®šï¼‰
    def true_function(x):
        """Li-ioné›»æ± é›»è§£è³ªã®ã‚¤ã‚ªãƒ³ä¼å°åº¦ï¼ˆä»®æƒ³ï¼‰"""
        return (
            np.sin(3 * x) * np.exp(-x) +
            0.7 * np.exp(-((x - 0.5) / 0.2)**2)
        )
    
    # è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ï¼ˆå°‘æ•°ã®å®Ÿé¨“çµæœï¼‰
    n_observations = 8
    X_train = np.random.uniform(0, 1, n_observations).reshape(-1, 1)
    y_train = true_function(X_train).ravel() + np.random.normal(0, 0.05,
                                                                 n_observations)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆäºˆæ¸¬ã—ãŸã„ç‚¹ï¼‰
    X_test = np.linspace(0, 1, 200).reshape(-1, 1)
    y_true = true_function(X_test).ravel()
    
    # ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
    gp = GaussianProcessRegressor(sigma=1.0, length_scale=0.15, noise=0.05)
    gp.fit(X_train, y_train)
    
    # äºˆæ¸¬
    y_pred, y_std = gp.predict(X_test, return_std=True)
    
    # å¯è¦–åŒ–
    plt.figure(figsize=(12, 6))
    
    # çœŸã®é–¢æ•°
    plt.plot(X_test, y_true, 'k--', linewidth=2, label='çœŸã®é–¢æ•°')
    
    # è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿
    plt.scatter(X_train, y_train, c='red', s=100, zorder=10,
                edgecolors='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿé¨“çµæœï¼‰')
    
    # äºˆæ¸¬å¹³å‡
    plt.plot(X_test, y_pred, 'b-', linewidth=2, label='äºˆæ¸¬å¹³å‡')
    
    # ä¸ç¢ºå®Ÿæ€§ï¼ˆ95%ä¿¡é ¼åŒºé–“ï¼‰
    plt.fill_between(
        X_test.ravel(),
        y_pred - 1.96 * y_std,
        y_pred + 1.96 * y_std,
        alpha=0.3,
        color='blue',
        label='95%ä¿¡é ¼åŒºé–“'
    )
    
    plt.xlabel('çµ„æˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
    plt.ylabel('ã‚¤ã‚ªãƒ³ä¼å°åº¦ (mS/cm)', fontsize=12)
    plt.title('ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã«ã‚ˆã‚‹ææ–™ç‰¹æ€§äºˆæ¸¬', fontsize=14)
    plt.legend(loc='best')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('gp_regression_demo.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã®çµæœ:")
    print(f"  è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿æ•°: {n_observations}")
    print(f"  äºˆæ¸¬ç‚¹æ•°: {len(X_test)}")
    print(f"  RMSE: {np.sqrt(np.mean((y_pred - y_true)**2)):.4f}")
    print("\nç‰¹å¾´:")
    print("  - è¦³æ¸¬ç‚¹ä»˜è¿‘: ä¸ç¢ºå®Ÿæ€§ãŒå°ã•ã„ï¼ˆä¿¡é ¼åŒºé–“ãŒç‹­ã„ï¼‰")
    print("  - æœªè¦³æ¸¬é ˜åŸŸ: ä¸ç¢ºå®Ÿæ€§ãŒå¤§ãã„ï¼ˆä¿¡é ¼åŒºé–“ãŒåºƒã„ï¼‰")
    print("  - ã“ã®ä¸ç¢ºå®Ÿæ€§æƒ…å ±ãŒç²å¾—é–¢æ•°ã§æ´»ç”¨ã•ã‚Œã‚‹")
    

**é‡è¦ãªè¦³å¯Ÿ** : 1\. **è¦³æ¸¬ç‚¹ã®è¿‘ã** : äºˆæ¸¬ç²¾åº¦ãŒé«˜ã„ï¼ˆä¸ç¢ºå®Ÿæ€§å°ï¼‰ 2\. **æœªè¦³æ¸¬é ˜åŸŸ** : ä¸ç¢ºå®Ÿæ€§ãŒå¤§ãã„ 3\. **ãƒ‡ãƒ¼ã‚¿ãŒå¢—ãˆã‚‹ã»ã©** : äºˆæ¸¬ç²¾åº¦å‘ä¸Š 4\. **ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–** : ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®éµ

* * *

## 2.3 ç²å¾—é–¢æ•°ï¼šæ¬¡ã®å®Ÿé¨“ç‚¹ã‚’ã©ã†é¸ã¶ã‹

### ç²å¾—é–¢æ•°ã®å½¹å‰²

**ç²å¾—é–¢æ•°ï¼ˆAcquisition Functionï¼‰** ã¯ã€ã€Œæ¬¡ã«ã©ã“ã‚’å®Ÿé¨“ã™ã¹ãã‹ã€ã‚’æ•°å­¦çš„ã«æ±ºå®šã—ã¾ã™ã€‚

**è¨­è¨ˆæ€æƒ³** : \- **é«˜ã„äºˆæ¸¬å€¤ã®å ´æ‰€** ã‚’æ¢ç´¢ï¼ˆExploitation: æ´»ç”¨ï¼‰ \- **ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„å ´æ‰€** ã‚’æ¢ç´¢ï¼ˆExploration: æ¢ç´¢ï¼‰ \- ã“ã®**2ã¤ã®ãƒãƒ©ãƒ³ã‚¹** ã‚’æœ€é©åŒ–

### ç²å¾—é–¢æ•°ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
    
    
    ```mermaid
    flowchart TB
        A[ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«] --> B[äºˆæ¸¬å¹³å‡ Î¼(x)]
        A --> C[äºˆæ¸¬æ¨™æº–åå·® Ïƒ(x)]
        B --> D[ç²å¾—é–¢æ•° Î±(x)]
        C --> D
        D --> E[ç²å¾—é–¢æ•°ã‚’æœ€å¤§åŒ–]
        E --> F[æ¬¡ã®å®Ÿé¨“ç‚¹ x_next]
    
        style A fill:#e3f2fd
        style D fill:#fff3e0
        style F fill:#e8f5e9
    ```

* * *

### ä¸»è¦ãªç²å¾—é–¢æ•°

#### 1\. Expected Improvementï¼ˆEIï¼‰

**å®šç¾©** : ç¾åœ¨ã®æœ€è‰¯å€¤ $f_{\text{best}}$ ã‹ã‚‰ã®æ”¹å–„é‡ã®æœŸå¾…å€¤ã‚’æœ€å¤§åŒ–

$$ \text{EI}(x) = \mathbb{E}[\max(0, f(x) - f_{\text{best}})] $$

**è§£æè§£** : $$ \text{EI}(x) = \begin{cases} (\mu(x) - f_{\text{best}}) \Phi(Z) + \sigma(x) \phi(Z) & \text{if } \sigma(x) > 0 \ 0 & \text{if } \sigma(x) = 0 \end{cases} $$

ã“ã“ã§ï¼š $$ Z = \frac{\mu(x) - f_{\text{best}}}{\sigma(x)} $$ \- $\Phi$: æ¨™æº–æ­£è¦åˆ†å¸ƒã®ç´¯ç©åˆ†å¸ƒé–¢æ•° \- $\phi$: æ¨™æº–æ­£è¦åˆ†å¸ƒã®ç¢ºç‡å¯†åº¦é–¢æ•°

**ç‰¹å¾´** : \- **ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„** : æ¢ç´¢ã¨æ´»ç”¨ã‚’è‡ªå‹•èª¿æ•´ \- **æœ€ã‚‚ä¸€èˆ¬çš„** : ææ–™ç§‘å­¦ã§åºƒãä½¿ç”¨ \- **è§£æçš„** : è¨ˆç®—ãŒé«˜é€Ÿ

**ã‚³ãƒ¼ãƒ‰ä¾‹3: Expected Improvementã®å®Ÿè£…**
    
    
    # Expected Improvementã®å®Ÿè£…
    from scipy.stats import norm
    
    def expected_improvement(X, gp, f_best, xi=0.01):
        """
        Expected Improvementç²å¾—é–¢æ•°
    
        Parameters:
        -----------
        X : array (n_samples, n_features)
            è©•ä¾¡ç‚¹
        gp : GaussianProcessRegressor
            å­¦ç¿’æ¸ˆã¿ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«
        f_best : float
            ç¾åœ¨ã®æœ€è‰¯å€¤
        xi : float
            æ¢ç´¢ã®å¼·ã•ï¼ˆexploration parameterï¼‰
    
        Returns:
        --------
        ei : array (n_samples,)
            EIå€¤
        """
        mu, sigma = gp.predict(X, return_std=True)
    
        # æ”¹å–„é‡
        improvement = mu - f_best - xi
    
        # æ¨™æº–åŒ–
        Z = improvement / (sigma + 1e-9)  # ã‚¼ãƒ­é™¤ç®—å›é¿
    
        # Expected Improvement
        ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)
    
        # Ïƒ = 0ã®å ´åˆã¯EI = 0
        ei[sigma == 0.0] = 0.0
    
        return ei
    
    # ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    np.random.seed(42)
    
    # è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿
    X_train = np.array([0.1, 0.3, 0.5, 0.7, 0.9]).reshape(-1, 1)
    y_train = true_function(X_train).ravel()
    
    # ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«
    gp = GaussianProcessRegressor(sigma=1.0, length_scale=0.2, noise=0.01)
    gp.fit(X_train, y_train)
    
    # ãƒ†ã‚¹ãƒˆç‚¹
    X_test = np.linspace(0, 1, 500).reshape(-1, 1)
    
    # äºˆæ¸¬
    y_pred, y_std = gp.predict(X_test, return_std=True)
    
    # ç¾åœ¨ã®æœ€è‰¯å€¤
    f_best = np.max(y_train)
    
    # EIã‚’è¨ˆç®—
    ei = expected_improvement(X_test, gp, f_best, xi=0.01)
    
    # æ¬¡ã®å®Ÿé¨“ç‚¹ã‚’ææ¡ˆ
    next_idx = np.argmax(ei)
    next_x = X_test[next_idx]
    
    # å¯è¦–åŒ–
    fig, axes = plt.subplots(2, 1, figsize=(12, 10))
    
    # ä¸Šå›³: ã‚¬ã‚¦ã‚¹éç¨‹ã®äºˆæ¸¬
    ax1 = axes[0]
    ax1.plot(X_test, true_function(X_test), 'k--',
             linewidth=2, label='çœŸã®é–¢æ•°')
    ax1.scatter(X_train, y_train, c='red', s=100, zorder=10,
                edgecolors='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')
    ax1.plot(X_test, y_pred, 'b-', linewidth=2, label='äºˆæ¸¬å¹³å‡')
    ax1.fill_between(X_test.ravel(), y_pred - 1.96 * y_std,
                     y_pred + 1.96 * y_std, alpha=0.3, color='blue',
                     label='95%ä¿¡é ¼åŒºé–“')
    ax1.axhline(f_best, color='green', linestyle=':',
                linewidth=2, label=f'ç¾åœ¨ã®æœ€è‰¯å€¤ = {f_best:.3f}')
    ax1.axvline(next_x, color='orange', linestyle='--',
                linewidth=2, label=f'ææ¡ˆç‚¹ = {next_x[0]:.3f}')
    ax1.set_ylabel('ç›®çš„é–¢æ•°', fontsize=12)
    ax1.set_title('ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã®äºˆæ¸¬', fontsize=14)
    ax1.legend(loc='best')
    ax1.grid(True, alpha=0.3)
    
    # ä¸‹å›³: Expected Improvement
    ax2 = axes[1]
    ax2.plot(X_test, ei, 'r-', linewidth=2, label='Expected Improvement')
    ax2.axvline(next_x, color='orange', linestyle='--',
                linewidth=2, label=f'æœ€å¤§EIç‚¹ = {next_x[0]:.3f}')
    ax2.fill_between(X_test.ravel(), 0, ei, alpha=0.3, color='red')
    ax2.set_xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
    ax2.set_ylabel('EI(x)', fontsize=12)
    ax2.set_title('Expected Improvementç²å¾—é–¢æ•°', fontsize=14)
    ax2.legend(loc='best')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('expected_improvement_demo.png', dpi=150,
                bbox_inches='tight')
    plt.show()
    
    print(f"Expected Improvementã«ã‚ˆã‚‹ææ¡ˆ:")
    print(f"  æ¬¡ã®å®Ÿé¨“ç‚¹: x = {next_x[0]:.3f}")
    print(f"  EIå€¤: {np.max(ei):.4f}")
    print(f"  äºˆæ¸¬å¹³å‡: {y_pred[next_idx]:.3f}")
    print(f"  äºˆæ¸¬æ¨™æº–åå·®: {y_std[next_idx]:.3f}")
    

**EIã®è§£é‡ˆ** : \- **é«˜ã„å¹³å‡å€¤** ã®å ´æ‰€ â†’ æ´»ç”¨ï¼ˆExploitationï¼‰ \- **é«˜ã„ä¸ç¢ºå®Ÿæ€§** ã®å ´æ‰€ â†’ æ¢ç´¢ï¼ˆExplorationï¼‰ \- **ä¸¡æ–¹ã‚’è€ƒæ…®** ã—ã¦ãƒãƒ©ãƒ³ã‚¹

* * *

#### 2\. Upper Confidence Boundï¼ˆUCBï¼‰

**å®šç¾©** : äºˆæ¸¬å¹³å‡ã«ä¸ç¢ºå®Ÿæ€§ã‚’åŠ ãˆãŸã€Œæ¥½è¦³çš„ãªæ¨å®šå€¤ã€ã‚’æœ€å¤§åŒ–

$$ \text{UCB}(x) = \mu(x) + \kappa \sigma(x) $$

  * $\kappa$: æ¢ç´¢ã®å¼·ã•ã‚’åˆ¶å¾¡ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé€šå¸¸ $\kappa = 2$ï¼‰

**ç‰¹å¾´** : \- **ã‚·ãƒ³ãƒ—ãƒ«** : å®Ÿè£…ãŒå®¹æ˜“ \- **ç›´æ„Ÿçš„** : æ¥½è¦³ä¸»ç¾©ã®åŸå‰‡ï¼ˆOptimism in the Face of Uncertaintyï¼‰ \- **èª¿æ•´å¯èƒ½** : $\kappa$ã§æ¢ç´¢åº¦åˆã„ã‚’åˆ¶å¾¡

**$\kappa$ã®å½±éŸ¿** : \- $\kappa$ãŒå¤§ãã„ â†’ æ¢ç´¢é‡è¦–ï¼ˆãƒªã‚¹ã‚¯ã‚’å–ã‚‹ï¼‰ \- $\kappa$ãŒå°ã•ã„ â†’ æ´»ç”¨é‡è¦–ï¼ˆå®‰å…¨ç­–ï¼‰

**ã‚³ãƒ¼ãƒ‰ä¾‹4: Upper Confidence Boundã®å®Ÿè£…**
    
    
    # Upper Confidence Boundã®å®Ÿè£…
    def upper_confidence_bound(X, gp, kappa=2.0):
        """
        Upper Confidence Boundç²å¾—é–¢æ•°
    
        Parameters:
        -----------
        X : array (n_samples, n_features)
            è©•ä¾¡ç‚¹
        gp : GaussianProcessRegressor
            å­¦ç¿’æ¸ˆã¿ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«
        kappa : float
            æ¢ç´¢ã®å¼·ã•ï¼ˆé€šå¸¸2.0ï¼‰
    
        Returns:
        --------
        ucb : array (n_samples,)
            UCBå€¤
        """
        mu, sigma = gp.predict(X, return_std=True)
        ucb = mu + kappa * sigma
        return ucb
    
    # ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: ç•°ãªã‚‹Îºã§ã®æ¯”è¼ƒ
    fig, axes = plt.subplots(3, 1, figsize=(12, 12))
    
    kappa_values = [0.5, 2.0, 5.0]
    
    for i, kappa in enumerate(kappa_values):
        ax = axes[i]
    
        # UCBã‚’è¨ˆç®—
        ucb = upper_confidence_bound(X_test, gp, kappa=kappa)
    
        # æ¬¡ã®å®Ÿé¨“ç‚¹
        next_idx = np.argmax(ucb)
        next_x = X_test[next_idx]
    
        # äºˆæ¸¬å¹³å‡ã¨ä¿¡é ¼åŒºé–“
        ax.plot(X_test, y_pred, 'b-', linewidth=2, label='äºˆæ¸¬å¹³å‡ Î¼(x)')
        ax.fill_between(X_test.ravel(),
                        y_pred - 1.96 * y_std,
                        y_pred + 1.96 * y_std,
                        alpha=0.2, color='blue',
                        label='95%ä¿¡é ¼åŒºé–“')
    
        # UCB
        ax.plot(X_test, ucb, 'r-', linewidth=2,
                label=f'UCB(x) (Îº={kappa})')
    
        # è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿
        ax.scatter(X_train, y_train, c='red', s=100, zorder=10,
                   edgecolors='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')
    
        # ææ¡ˆç‚¹
        ax.axvline(next_x, color='orange', linestyle='--',
                   linewidth=2, label=f'ææ¡ˆç‚¹ = {next_x[0]:.3f}')
    
        ax.set_ylabel('ç›®çš„é–¢æ•°', fontsize=12)
        ax.set_title(f'UCB with Îº = {kappa}', fontsize=14)
        ax.legend(loc='best')
        ax.grid(True, alpha=0.3)
    
        if i == 2:
            ax.set_xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
    
    plt.tight_layout()
    plt.savefig('ucb_kappa_comparison.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("Îºã®å½±éŸ¿:")
    print("  Îº = 0.5: æ´»ç”¨é‡è¦–ï¼ˆè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿è¿‘ãã‚’æ¢ç´¢ï¼‰")
    print("  Îº = 2.0: ãƒãƒ©ãƒ³ã‚¹ï¼ˆæ¨™æº–çš„ãªè¨­å®šï¼‰")
    print("  Îº = 5.0: æ¢ç´¢é‡è¦–ï¼ˆæœªçŸ¥é ˜åŸŸã‚’ç©æ¥µæ¢ç´¢ï¼‰")
    

* * *

#### 3\. Probability of Improvementï¼ˆPIï¼‰

**å®šç¾©** : ç¾åœ¨ã®æœ€è‰¯å€¤ã‚’è¶…ãˆã‚‹ç¢ºç‡ã‚’æœ€å¤§åŒ–

$$ \text{PI}(x) = P(f(x) > f_{\text{best}}) = \Phi\left(\frac{\mu(x) - f_{\text{best}}}{\sigma(x)}\right) $$

**ç‰¹å¾´** : \- **æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«** : è§£é‡ˆãŒå®¹æ˜“ \- **ä¿å®ˆçš„** : å¤§ããªæ”¹å–„ã‚’æœŸå¾…ã—ãªã„ \- **å®Ÿç”¨çš„** : å°ã•ãªæ”¹å–„ã‚’ç©ã¿é‡ã­ã‚‹æˆ¦ç•¥

**ã‚³ãƒ¼ãƒ‰ä¾‹5: Probability of Improvementã®å®Ÿè£…**
    
    
    # Probability of Improvementã®å®Ÿè£…
    def probability_of_improvement(X, gp, f_best, xi=0.01):
        """
        Probability of Improvementç²å¾—é–¢æ•°
    
        Parameters:
        -----------
        X : array (n_samples, n_features)
            è©•ä¾¡ç‚¹
        gp : GaussianProcessRegressor
            å­¦ç¿’æ¸ˆã¿ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«
        f_best : float
            ç¾åœ¨ã®æœ€è‰¯å€¤
        xi : float
            æ¢ç´¢ã®å¼·ã•
    
        Returns:
        --------
        pi : array (n_samples,)
            PIå€¤
        """
        mu, sigma = gp.predict(X, return_std=True)
    
        # æ”¹å–„é‡
        improvement = mu - f_best - xi
    
        # æ¨™æº–åŒ–
        Z = improvement / (sigma + 1e-9)
    
        # Probability of Improvement
        pi = norm.cdf(Z)
    
        return pi
    
    # PIã‚’è¨ˆç®—
    pi = probability_of_improvement(X_test, gp, f_best, xi=0.01)
    
    # å¯è¦–åŒ–
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(X_test, pi, 'g-', linewidth=2, label='PI(x)')
    plt.axvline(X_test[np.argmax(pi)], color='orange',
                linestyle='--', linewidth=2,
                label=f'æœ€å¤§PIç‚¹ = {X_test[np.argmax(pi)][0]:.3f}')
    plt.fill_between(X_test.ravel(), 0, pi, alpha=0.3, color='green')
    plt.xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
    plt.ylabel('PI(x)', fontsize=12)
    plt.title('Probability of Improvement', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # æ¯”è¼ƒ: EI vs PI vs UCB
    plt.subplot(1, 2, 2)
    ei_normalized = ei / np.max(ei)
    pi_normalized = pi / np.max(pi)
    ucb_normalized = upper_confidence_bound(X_test, gp, kappa=2.0)
    ucb_normalized = (ucb_normalized - np.min(ucb_normalized)) / \
                     (np.max(ucb_normalized) - np.min(ucb_normalized))
    
    plt.plot(X_test, ei_normalized, 'r-', linewidth=2, label='EI (æ­£è¦åŒ–)')
    plt.plot(X_test, pi_normalized, 'g-', linewidth=2, label='PI (æ­£è¦åŒ–)')
    plt.plot(X_test, ucb_normalized, 'b-', linewidth=2, label='UCB (æ­£è¦åŒ–)')
    plt.scatter(X_train, [0.5]*len(X_train), c='red', s=100,
                zorder=10, edgecolors='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')
    plt.xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
    plt.ylabel('ç²å¾—é–¢æ•°å€¤ï¼ˆæ­£è¦åŒ–ï¼‰', fontsize=12)
    plt.title('ç²å¾—é–¢æ•°ã®æ¯”è¼ƒ', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('acquisition_functions_comparison.png', dpi=150,
                bbox_inches='tight')
    plt.show()
    

* * *

### ç²å¾—é–¢æ•°ã®æ¯”è¼ƒè¡¨

ç²å¾—é–¢æ•° | ç‰¹å¾´ | é•·æ‰€ | çŸ­æ‰€ | æ¨å¥¨ç”¨é€”  
---|---|---|---|---  
**EI** | æ”¹å–„é‡ã®æœŸå¾…å€¤ | ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„ã€å®Ÿç¸¾è±Šå¯Œ | ã‚„ã‚„è¤‡é›‘ | ä¸€èˆ¬çš„ãªæœ€é©åŒ–  
**UCB** | æ¥½è¦³çš„æ¨å®š | ã‚·ãƒ³ãƒ—ãƒ«ã€èª¿æ•´å¯èƒ½ | Îºã®èª¿æ•´ãŒå¿…è¦ | æ¢ç´¢åº¦åˆã„åˆ¶å¾¡  
**PI** | æ”¹å–„ç¢ºç‡ | éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ« | ä¿å®ˆçš„ | å®‰å…¨ãªæ¢ç´¢  
  
**ææ–™ç§‘å­¦ã§ã®æ¨å¥¨** : \- **åˆå¿ƒè€…** : EIï¼ˆãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ãã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å„ªç§€ï¼‰ \- **æ¢ç´¢é‡è¦–** : UCBï¼ˆÎº = 2-5ï¼‰ \- **å®‰å…¨ç­–** : PIï¼ˆå°ã•ãªæ”¹å–„ã‚’ç¢ºå®Ÿã«ï¼‰

* * *

## 2.4 æ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

### æ•°å­¦çš„ãªå®šå¼åŒ–

ç²å¾—é–¢æ•°ã¯ã€ä»¥ä¸‹ã®2ã¤ã®é …ã«åˆ†è§£ã§ãã¾ã™ï¼š

$$ \alpha(x) = \underbrace{\mu(x)}_{\text{Exploitation}} + \underbrace{\lambda \sigma(x)}_{\text{Exploration}} $$

  * **Exploitationé …** $\mu(x)$: äºˆæ¸¬å¹³å‡ãŒé«˜ã„å ´æ‰€
  * **Explorationé …** $\lambda \sigma(x)$: ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„å ´æ‰€

### ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®å¯è¦–åŒ–
    
    
    ```mermaid
    flowchart LR
        subgraph æ´»ç”¨Exploitation
        A1[æ—¢çŸ¥ã®è‰¯ã„é ˜åŸŸ]
        A2[é«˜ã„äºˆæ¸¬å€¤ Î¼(x)]
        A3[ä½ã„ä¸ç¢ºå®Ÿæ€§ Ïƒ(x)]
        A1 --> A2
        A1 --> A3
        end
    
        subgraph æ¢ç´¢Exploration
        B1[æœªçŸ¥ã®é ˜åŸŸ]
        B2[æœªçŸ¥ã®äºˆæ¸¬å€¤ Î¼(x)]
        B3[é«˜ã„ä¸ç¢ºå®Ÿæ€§ Ïƒ(x)]
        B1 --> B2
        B1 --> B3
        end
    
        subgraph æœ€é©ãªãƒãƒ©ãƒ³ã‚¹
        C1[ç²å¾—é–¢æ•°]
        C2[Î¼(x) + Î»Ïƒ(x)]
        C3[æ¬¡ã®å®Ÿé¨“ç‚¹]
        C1 --> C2
        C2 --> C3
        end
    
        A2 --> C1
        A3 --> C1
        B2 --> C1
        B3 --> C1
    
        style A1 fill:#fff3e0
        style B1 fill:#e3f2fd
        style C3 fill:#e8f5e9
    ```

**ã‚³ãƒ¼ãƒ‰ä¾‹6: æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹å¯è¦–åŒ–**
    
    
    # æ¢ç´¢ã¨æ´»ç”¨ã®åˆ†è§£
    def decompose_acquisition(X, gp, f_best, xi=0.01):
        """
        ç²å¾—é–¢æ•°ã‚’æ¢ç´¢é …ã¨æ´»ç”¨é …ã«åˆ†è§£
    
        Returns:
        --------
        exploitation : æ´»ç”¨é …ï¼ˆäºˆæ¸¬å¹³å‡ãƒ™ãƒ¼ã‚¹ï¼‰
        exploration : æ¢ç´¢é …ï¼ˆä¸ç¢ºå®Ÿæ€§ãƒ™ãƒ¼ã‚¹ï¼‰
        """
        mu, sigma = gp.predict(X, return_std=True)
    
        # æ´»ç”¨é …ï¼ˆå¹³å‡ãŒé«˜ã„ã»ã©å¤§ãã„ï¼‰
        exploitation = mu
    
        # æ¢ç´¢é …ï¼ˆä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„ã»ã©å¤§ãã„ï¼‰
        exploration = sigma
    
        return exploitation, exploration
    
    # åˆ†è§£
    exploitation, exploration = decompose_acquisition(X_test, gp, f_best)
    
    # å¯è¦–åŒ–
    fig, axes = plt.subplots(4, 1, figsize=(12, 14))
    
    # 1. ã‚¬ã‚¦ã‚¹éç¨‹ã®äºˆæ¸¬
    ax1 = axes[0]
    ax1.plot(X_test, y_pred, 'b-', linewidth=2, label='äºˆæ¸¬å¹³å‡ Î¼(x)')
    ax1.fill_between(X_test.ravel(), y_pred - 1.96 * y_std,
                     y_pred + 1.96 * y_std, alpha=0.3, color='blue',
                     label='95%ä¿¡é ¼åŒºé–“')
    ax1.scatter(X_train, y_train, c='red', s=100, zorder=10,
                edgecolors='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')
    ax1.set_ylabel('ç›®çš„é–¢æ•°', fontsize=12)
    ax1.set_title('ã‚¬ã‚¦ã‚¹éç¨‹ã®äºˆæ¸¬', fontsize=14)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. æ´»ç”¨é …ï¼ˆExploitationï¼‰
    ax2 = axes[1]
    ax2.plot(X_test, exploitation, 'g-', linewidth=2,
             label='æ´»ç”¨é …ï¼ˆäºˆæ¸¬å¹³å‡ï¼‰')
    ax2.scatter(X_train, y_train, c='red', s=100, zorder=10,
                edgecolors='black', alpha=0.5)
    ax2.set_ylabel('æ´»ç”¨é …', fontsize=12)
    ax2.set_title('Exploitation: æ—¢çŸ¥ã®è‰¯ã„é ˜åŸŸã‚’é‡è¦–', fontsize=14)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. æ¢ç´¢é …ï¼ˆExplorationï¼‰
    ax3 = axes[2]
    ax3.plot(X_test, exploration, 'orange', linewidth=2,
             label='æ¢ç´¢é …ï¼ˆä¸ç¢ºå®Ÿæ€§ï¼‰')
    ax3.scatter(X_train, [0]*len(X_train), c='red', s=100,
                zorder=10, edgecolors='black', alpha=0.5,
                label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ä½ç½®')
    ax3.set_ylabel('æ¢ç´¢é …', fontsize=12)
    ax3.set_title('Exploration: æœªçŸ¥ã®é ˜åŸŸã‚’é‡è¦–', fontsize=14)
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. çµ±åˆï¼ˆEIï¼‰
    ax4 = axes[3]
    ei_values = expected_improvement(X_test, gp, f_best, xi=0.01)
    ax4.plot(X_test, ei_values, 'r-', linewidth=2,
             label='Expected Improvement')
    next_x = X_test[np.argmax(ei_values)]
    ax4.axvline(next_x, color='purple', linestyle='--',
                linewidth=2, label=f'ææ¡ˆç‚¹ = {next_x[0]:.3f}')
    ax4.fill_between(X_test.ravel(), 0, ei_values,
                     alpha=0.3, color='red')
    ax4.set_xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
    ax4.set_ylabel('EI(x)', fontsize=12)
    ax4.set_title('çµ±åˆ: ä¸¡è€…ã®ãƒãƒ©ãƒ³ã‚¹ã‚’æœ€é©åŒ–', fontsize=14)
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('exploitation_exploration_tradeoff.png', dpi=150,
                bbox_inches='tight')
    plt.show()
    
    print("æ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•:")
    print(f"  ææ¡ˆç‚¹ x = {next_x[0]:.3f}")
    print(f"    äºˆæ¸¬å¹³å‡ï¼ˆæ´»ç”¨ï¼‰: {y_pred[np.argmax(ei_values)]:.3f}")
    print(f"    ä¸ç¢ºå®Ÿæ€§ï¼ˆæ¢ç´¢ï¼‰: {y_std[np.argmax(ei_values)]:.3f}")
    print(f"    EIå€¤: {np.max(ei_values):.4f}")
    

* * *

## 2.5 åˆ¶ç´„ä»˜ãæœ€é©åŒ–ã¨å¤šç›®çš„æœ€é©åŒ–

### åˆ¶ç´„ä»˜ããƒ™ã‚¤ã‚ºæœ€é©åŒ–

å®Ÿéš›ã®ææ–™é–‹ç™ºã§ã¯ã€**åˆ¶ç´„æ¡ä»¶** ãŒå­˜åœ¨ã—ã¾ã™ï¼š

**ä¾‹ï¼šLi-ioné›»æ± é›»è§£è³ª** \- ã‚¤ã‚ªãƒ³ä¼å°åº¦ã‚’æœ€å¤§åŒ–ï¼ˆç›®çš„é–¢æ•°ï¼‰ \- ç²˜åº¦ < 10 cPï¼ˆåˆ¶ç´„1ï¼‰ \- å¼•ç«ç‚¹ > 100Â°Cï¼ˆåˆ¶ç´„2ï¼‰ \- ã‚³ã‚¹ãƒˆ < $50/kgï¼ˆåˆ¶ç´„3ï¼‰

**æ•°å­¦çš„å®šå¼åŒ–** : $$ \begin{align} \max_{x} \quad & f(x) \ \text{s.t.} \quad & g_i(x) \leq 0, \quad i = 1, \ldots, m \ & h_j(x) = 0, \quad j = 1, \ldots, p \end{align} $$

**ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ** : 1\. **åˆ¶ç´„é–¢æ•°ã‚‚ã‚¬ã‚¦ã‚¹éç¨‹ã§ãƒ¢ãƒ‡ãƒ«åŒ–** 2\. **åˆ¶ç´„ã‚’æº€ãŸã™ç¢ºç‡ã‚’ç²å¾—é–¢æ•°ã«çµ„ã¿è¾¼ã‚€**

**ã‚³ãƒ¼ãƒ‰ä¾‹7: åˆ¶ç´„ä»˜ããƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ãƒ‡ãƒ¢**
    
    
    # åˆ¶ç´„ä»˜ããƒ™ã‚¤ã‚ºæœ€é©åŒ–
    def constrained_expected_improvement(X, gp_obj, gp_constraint,
                                         f_best, constraint_threshold=0):
        """
        åˆ¶ç´„ä»˜ãExpected Improvement
    
        Parameters:
        -----------
        gp_obj : ã‚¬ã‚¦ã‚¹éç¨‹ï¼ˆç›®çš„é–¢æ•°ï¼‰
        gp_constraint : ã‚¬ã‚¦ã‚¹éç¨‹ï¼ˆåˆ¶ç´„é–¢æ•°ï¼‰
        constraint_threshold : åˆ¶ç´„ã®é–¾å€¤ï¼ˆâ‰¤ 0ãŒå®Ÿè¡Œå¯èƒ½ï¼‰
        """
        # ç›®çš„é–¢æ•°ã®EI
        ei = expected_improvement(X, gp_obj, f_best, xi=0.01)
    
        # åˆ¶ç´„ã‚’æº€ãŸã™ç¢ºç‡
        mu_c, sigma_c = gp_constraint.predict(X, return_std=True)
        prob_feasible = norm.cdf((constraint_threshold - mu_c) /
                                 (sigma_c + 1e-9))
    
        # åˆ¶ç´„ä»˜ãEI = EI Ã— åˆ¶ç´„æº€è¶³ç¢ºç‡
        cei = ei * prob_feasible
    
        return cei
    
    # ãƒ‡ãƒ¢: åˆ¶ç´„é–¢æ•°ã‚’å®šç¾©
    def constraint_function(x):
        """åˆ¶ç´„é–¢æ•°ï¼ˆä¾‹ï¼šç²˜åº¦ã®ä¸Šé™ï¼‰"""
        return 0.5 - x  # x < 0.5 ãŒå®Ÿè¡Œå¯èƒ½é ˜åŸŸ
    
    # åˆ¶ç´„ãƒ‡ãƒ¼ã‚¿
    y_constraint = constraint_function(X_train).ravel()
    
    # åˆ¶ç´„é–¢æ•°ç”¨ã®ã‚¬ã‚¦ã‚¹éç¨‹
    gp_constraint = GaussianProcessRegressor(sigma=0.5, length_scale=0.2,
                                             noise=0.01)
    gp_constraint.fit(X_train, y_constraint)
    
    # åˆ¶ç´„ä»˜ãEIã‚’è¨ˆç®—
    cei = constrained_expected_improvement(X_test, gp, gp_constraint,
                                           f_best, constraint_threshold=0)
    
    # å¯è¦–åŒ–
    fig, axes = plt.subplots(3, 1, figsize=(12, 12))
    
    # ä¸Šå›³: ç›®çš„é–¢æ•°
    ax1 = axes[0]
    ax1.plot(X_test, y_pred, 'b-', linewidth=2, label='ç›®çš„é–¢æ•°ã®äºˆæ¸¬')
    ax1.fill_between(X_test.ravel(), y_pred - 1.96 * y_std,
                     y_pred + 1.96 * y_std, alpha=0.3, color='blue')
    ax1.scatter(X_train, y_train, c='red', s=100, zorder=10,
                edgecolors='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')
    ax1.set_ylabel('ç›®çš„é–¢æ•°', fontsize=12)
    ax1.set_title('ç›®çš„é–¢æ•°ï¼ˆã‚¤ã‚ªãƒ³ä¼å°åº¦ï¼‰', fontsize=14)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # ä¸­å›³: åˆ¶ç´„é–¢æ•°
    ax2 = axes[1]
    mu_c, sigma_c = gp_constraint.predict(X_test, return_std=True)
    ax2.plot(X_test, mu_c, 'g-', linewidth=2, label='åˆ¶ç´„é–¢æ•°ã®äºˆæ¸¬')
    ax2.fill_between(X_test.ravel(), mu_c - 1.96 * sigma_c,
                     mu_c + 1.96 * sigma_c, alpha=0.3, color='green')
    ax2.axhline(0, color='red', linestyle='--', linewidth=2,
                label='åˆ¶ç´„å¢ƒç•Œï¼ˆâ‰¤ 0 ãŒå®Ÿè¡Œå¯èƒ½ï¼‰')
    ax2.axhspan(-10, 0, alpha=0.2, color='green',
                label='å®Ÿè¡Œå¯èƒ½é ˜åŸŸ')
    ax2.scatter(X_train, y_constraint, c='red', s=100, zorder=10,
                edgecolors='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')
    ax2.set_ylabel('åˆ¶ç´„é–¢æ•°', fontsize=12)
    ax2.set_title('åˆ¶ç´„é–¢æ•°ï¼ˆç²˜åº¦ä¸Šé™ï¼‰', fontsize=14)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # ä¸‹å›³: åˆ¶ç´„ä»˜ãEI
    ax3 = axes[2]
    ei_unconstrained = expected_improvement(X_test, gp, f_best, xi=0.01)
    ax3.plot(X_test, ei_unconstrained, 'r--', linewidth=2,
             label='EIï¼ˆåˆ¶ç´„ãªã—ï¼‰', alpha=0.5)
    ax3.plot(X_test, cei, 'r-', linewidth=2, label='åˆ¶ç´„ä»˜ãEI')
    next_x = X_test[np.argmax(cei)]
    ax3.axvline(next_x, color='purple', linestyle='--', linewidth=2,
                label=f'ææ¡ˆç‚¹ = {next_x[0]:.3f}')
    ax3.fill_between(X_test.ravel(), 0, cei, alpha=0.3, color='red')
    ax3.set_xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
    ax3.set_ylabel('ç²å¾—é–¢æ•°', fontsize=12)
    ax3.set_title('åˆ¶ç´„ä»˜ãExpected Improvement', fontsize=14)
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('constrained_bayesian_optimization.png', dpi=150,
                bbox_inches='tight')
    plt.show()
    
    print("åˆ¶ç´„ä»˜ãæœ€é©åŒ–ã®çµæœ:")
    print(f"  ææ¡ˆç‚¹: x = {next_x[0]:.3f}")
    print(f"  åˆ¶ç´„ãªã—EIã®æœ€å¤§ç‚¹: x = {X_test[np.argmax(ei_unconstrained)][0]:.3f}")
    print(f"  â†’ åˆ¶ç´„ã‚’è€ƒæ…®ã—ã¦ææ¡ˆç‚¹ãŒå¤‰åŒ–")
    

* * *

### å¤šç›®çš„æœ€é©åŒ–

ææ–™é–‹ç™ºã§ã¯ã€**è¤‡æ•°ã®ç‰¹æ€§ã‚’åŒæ™‚ã«æœ€é©åŒ–** ã—ãŸã„å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

**ä¾‹ï¼šç†±é›»ææ–™** \- ã‚¼ãƒ¼ãƒ™ãƒƒã‚¯ä¿‚æ•°ã‚’æœ€å¤§åŒ– \- é›»æ°—æŠµæŠ—ç‡ã‚’æœ€å°åŒ– \- ç†±ä¼å°ç‡ã‚’æœ€å°åŒ–

**ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢** : \- ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒã‚ã‚‹å ´åˆã€å˜ä¸€ã®æœ€é©è§£ã¯å­˜åœ¨ã—ãªã„ \- **ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ã®é›†åˆ** ã‚’æ±‚ã‚ã‚‹

**ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ** : 1\. **ã‚¹ã‚«ãƒ©ãƒ¼åŒ–** : é‡ã¿ä»˜ãå’Œ $f(x) = w_1 f_1(x) + w_2 f_2(x)$ 2\. **ParEGO** : ãƒ©ãƒ³ãƒ€ãƒ ãªé‡ã¿ã§ã‚¹ã‚«ãƒ©ãƒ¼åŒ–ã‚’ç¹°ã‚Šè¿”ã™ 3\. **EHVI** : Expected Hypervolume Improvement

**ã‚³ãƒ¼ãƒ‰ä¾‹8: å¤šç›®çš„æœ€é©åŒ–ã®å¯è¦–åŒ–**
    
    
    # å¤šç›®çš„æœ€é©åŒ–ã®ãƒ‡ãƒ¢
    def objective1(x):
        """ç›®çš„1: ã‚¤ã‚ªãƒ³ä¼å°åº¦ï¼ˆæœ€å¤§åŒ–ï¼‰"""
        return true_function(x)
    
    def objective2(x):
        """ç›®çš„2: ç²˜åº¦ï¼ˆæœ€å°åŒ–ï¼‰"""
        return 0.5 + 0.3 * np.sin(5 * x)
    
    # ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ã‚’è¨ˆç®—
    x_grid = np.linspace(0, 1, 1000)
    obj1_values = objective1(x_grid)
    obj2_values = objective2(x_grid)
    
    # ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©åˆ¤å®š
    def is_pareto_optimal(costs):
        """
        ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ã‚’åˆ¤å®š
    
        Parameters:
        -----------
        costs : array (n_samples, n_objectives)
            å„ç‚¹ã®ã‚³ã‚¹ãƒˆï¼ˆæœ€å°åŒ–å•é¡Œã¨ã—ã¦ï¼‰
    
        Returns:
        --------
        pareto_mask : array (n_samples,)
            TrueãŒãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©
        """
        is_pareto = np.ones(len(costs), dtype=bool)
        for i, c in enumerate(costs):
            if is_pareto[i]:
                # ä»–ã®ç‚¹ã«æ”¯é…ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
                is_pareto[is_pareto] = np.any(
                    costs[is_pareto] < c, axis=1
                )
                is_pareto[i] = True
        return is_pareto
    
    # ã‚³ã‚¹ãƒˆãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼ˆæœ€å°åŒ–å•é¡Œã¨ã—ã¦ï¼‰
    costs = np.column_stack([
        -obj1_values,  # æœ€å¤§åŒ– â†’ æœ€å°åŒ–
        obj2_values    # æœ€å°åŒ–
    ])
    
    # ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£
    pareto_mask = is_pareto_optimal(costs)
    pareto_x = x_grid[pareto_mask]
    pareto_obj1 = obj1_values[pareto_mask]
    pareto_obj2 = obj2_values[pareto_mask]
    
    # å¯è¦–åŒ–
    fig = plt.figure(figsize=(14, 6))
    
    # å·¦å›³: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“
    ax1 = plt.subplot(1, 2, 1)
    ax1.plot(x_grid, obj1_values, 'b-', linewidth=2,
             label='ç›®çš„1ï¼ˆã‚¤ã‚ªãƒ³ä¼å°åº¦ï¼‰')
    ax1.plot(x_grid, obj2_values, 'r-', linewidth=2,
             label='ç›®çš„2ï¼ˆç²˜åº¦ï¼‰')
    ax1.scatter(pareto_x, pareto_obj1, c='blue', s=50, alpha=0.6,
                label='ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©ï¼ˆç›®çš„1ï¼‰')
    ax1.scatter(pareto_x, pareto_obj2, c='red', s=50, alpha=0.6,
                label='ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©ï¼ˆç›®çš„2ï¼‰')
    ax1.set_xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
    ax1.set_ylabel('ç›®çš„é–¢æ•°å€¤', fontsize=12)
    ax1.set_title('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“', fontsize=14)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # å³å›³: ç›®çš„ç©ºé–“ï¼ˆãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ï¼‰
    ax2 = plt.subplot(1, 2, 2)
    ax2.scatter(obj1_values, obj2_values, c='lightgray', s=20,
                alpha=0.5, label='å…¨æ¢ç´¢ç‚¹')
    ax2.scatter(pareto_obj1, pareto_obj2, c='red', s=100,
                edgecolors='black', zorder=10,
                label='ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢')
    ax2.plot(pareto_obj1, pareto_obj2, 'r--', linewidth=2, alpha=0.5)
    ax2.set_xlabel('ç›®çš„1ï¼ˆã‚¤ã‚ªãƒ³ä¼å°åº¦ï¼‰â†’ æœ€å¤§åŒ–', fontsize=12)
    ax2.set_ylabel('ç›®çš„2ï¼ˆç²˜åº¦ï¼‰â†’ æœ€å°åŒ–', fontsize=12)
    ax2.set_title('ç›®çš„ç©ºé–“ã¨ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢', fontsize=14)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('multi_objective_optimization.png', dpi=150,
                bbox_inches='tight')
    plt.show()
    
    print(f"ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£æ•°: {np.sum(pareto_mask)}")
    print("ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®ä¾‹:")
    print(f"  é«˜ä¼å°åº¦: ç›®çš„1 = {np.max(pareto_obj1):.3f}, "
          f"ç›®çš„2 = {pareto_obj2[np.argmax(pareto_obj1)]:.3f}")
    print(f"  ä½ç²˜åº¦: ç›®çš„1 = {pareto_obj1[np.argmin(pareto_obj2)]:.3f}, "
          f"ç›®çš„2 = {np.min(pareto_obj2):.3f}")
    

* * *

## 2.6 ã‚³ãƒ©ãƒ ï¼šã‚«ãƒ¼ãƒãƒ«é¸æŠã®å®Ÿå‹™

### ã‚«ãƒ¼ãƒãƒ«ã®ç¨®é¡ã¨ç‰¹æ€§

RBFä»¥å¤–ã«ã‚‚ã€å¤šæ§˜ãªã‚«ãƒ¼ãƒãƒ«ãŒå­˜åœ¨ã—ã¾ã™ï¼š

**MatÃ©rn ã‚«ãƒ¼ãƒãƒ«** : $$ k(x, x') = \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)} \left(\frac{\sqrt{2\nu}||x - x'||}{\ell}\right)^\nu K_\nu\left(\frac{\sqrt{2\nu}||x - x'||}{\ell}\right) $$

  * $\nu$: æ»‘ã‚‰ã‹ã•ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
  * $\nu = 1/2$: æŒ‡æ•°ã‚«ãƒ¼ãƒãƒ«ï¼ˆç²—ã„é–¢æ•°ï¼‰
  * $\nu = 3/2, 5/2$: ä¸­ç¨‹åº¦ã®æ»‘ã‚‰ã‹ã•
  * $\nu \to \infty$: RBFã‚«ãƒ¼ãƒãƒ«ï¼ˆéå¸¸ã«æ»‘ã‚‰ã‹ï¼‰

**ææ–™ç§‘å­¦ã§ã®é¸æŠæŒ‡é‡** : \- **DFTè¨ˆç®—çµæœ** : RBFï¼ˆæ»‘ã‚‰ã‹ï¼‰ \- **å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿** : MatÃ©rn 3/2 or 5/2ï¼ˆãƒã‚¤ã‚ºè€ƒæ…®ï¼‰ \- **çµ„æˆæœ€é©åŒ–** : RBF \- **ãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶** : MatÃ©rnï¼ˆæ€¥å³»ãªå¤‰åŒ–ã‚ã‚Šï¼‰

**å‘¨æœŸçš„ç¾è±¡** : Periodic kernel $$ k(x, x') = \sigma^2 \exp\left(-\frac{2\sin^2(\pi|x - x'|/p)}{\ell^2}\right) $$ \- çµæ™¶æ§‹é€ ï¼ˆå‘¨æœŸæ€§ã‚ã‚Šï¼‰ \- æ¸©åº¦ã‚µã‚¤ã‚¯ãƒ«

* * *

## 2.7 ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºç­–

**å•é¡Œ1: ç²å¾—é–¢æ•°ãŒå¸¸ã«åŒã˜å ´æ‰€ã‚’ææ¡ˆã™ã‚‹**

**åŸå› ** : \- é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ãŒå¤§ãã™ãã‚‹ â†’ å…¨ä½“ãŒæ»‘ã‚‰ã‹ã™ã \- ãƒã‚¤ã‚ºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå°ã•ã™ãã‚‹ â†’ è¦³æ¸¬ç‚¹ã‚’éä¿¡

**è§£æ±ºç­–** :
    
    
    # é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ã‚’èª¿æ•´
    gp = GaussianProcessRegressor(length_scale=0.05, noise=0.1)
    
    # ã¾ãŸã¯ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è‡ªå‹•èª¿æ•´
    from sklearn.gaussian_process import GaussianProcessRegressor as SKGP
    from sklearn.gaussian_process.kernels import RBF, WhiteKernel
    
    kernel = RBF(length_scale=0.1) + WhiteKernel(noise_level=0.1)
    gp = SKGP(kernel=kernel, n_restarts_optimizer=10)
    gp.fit(X_train, y_train)
    

**å•é¡Œ2: äºˆæ¸¬ãŒä¸å®‰å®šï¼ˆä¿¡é ¼åŒºé–“ãŒç•°å¸¸ã«åºƒã„ï¼‰**

**åŸå› ** : \- ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã‚‹ \- ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—ãŒæ•°å€¤çš„ã«ä¸å®‰å®š

**è§£æ±ºç­–** :
    
    
    # æ­£å‰‡åŒ–é …ã‚’è¿½åŠ 
    K = kernel_matrix + 1e-6 * np.eye(n_samples)  # ã‚¸ãƒƒã‚¿ãƒ¼ã‚’è¿½åŠ 
    
    # ã¾ãŸã¯Choleskyåˆ†è§£ã‚’ä½¿ç”¨ï¼ˆæ•°å€¤å®‰å®šæ€§å‘ä¸Šï¼‰
    from scipy.linalg import cho_solve, cho_factor
    
    L = cho_factor(K, lower=True)
    alpha = cho_solve(L, y_train)
    

**å•é¡Œ3: è¨ˆç®—ãŒé…ã„ï¼ˆå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰**

**åŸå› ** : \- ã‚¬ã‚¦ã‚¹éç¨‹ã®è¨ˆç®—é‡: $O(n^3)$ï¼ˆn = ãƒ‡ãƒ¼ã‚¿æ•°ï¼‰

**è§£æ±ºç­–** :
    
    
    # 1. ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¬ã‚¦ã‚¹éç¨‹
    # ä»£è¡¨ç‚¹ï¼ˆInducing pointsï¼‰ã‚’ä½¿ç”¨
    
    # 2. è¿‘ä¼¼æ‰‹æ³•
    # - Sparse GP
    # - Local GPï¼ˆé ˜åŸŸåˆ†å‰²ï¼‰
    
    # 3. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æ´»ç”¨
    # GPyTorchï¼ˆGPUé«˜é€ŸåŒ–ï¼‰
    # GPflowï¼ˆTensorFlow backendï¼‰
    

* * *

## 2.8 æœ¬ç« ã®ã¾ã¨ã‚

### å­¦ã‚“ã ã“ã¨

  1. **ä»£ç†ãƒ¢ãƒ‡ãƒ«ã®å½¹å‰²** \- å°‘æ•°ã®å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›®çš„é–¢æ•°ã‚’æ¨å®š \- ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ãŒæœ€ã‚‚ä¸€èˆ¬çš„ \- ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–ãŒå¯èƒ½

  2. **ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°** \- ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã§ç‚¹é–“ã®é¡ä¼¼åº¦ã‚’å®šç¾© \- RBFã‚«ãƒ¼ãƒãƒ«ãŒææ–™ç§‘å­¦ã§æ¨™æº–çš„ \- äºˆæ¸¬å¹³å‡ã¨äºˆæ¸¬åˆ†æ•£ã‚’è¨ˆç®—

  3. **ç²å¾—é–¢æ•°** \- æ¬¡ã®å®Ÿé¨“ç‚¹ã‚’æ±ºå®šã™ã‚‹æ•°å­¦çš„åŸºæº– \- EIï¼ˆExpected Improvementï¼‰: ãƒãƒ©ãƒ³ã‚¹å‹ \- UCBï¼ˆUpper Confidence Boundï¼‰: èª¿æ•´å¯èƒ½ \- PIï¼ˆProbability of Improvementï¼‰: ã‚·ãƒ³ãƒ—ãƒ«

  4. **æ¢ç´¢ã¨æ´»ç”¨** \- Exploitation: æ—¢çŸ¥ã®è‰¯ã„é ˜åŸŸã‚’æ´»ç”¨ \- Exploration: æœªçŸ¥ã®é ˜åŸŸã‚’æ¢ç´¢ \- ç²å¾—é–¢æ•°ãŒè‡ªå‹•çš„ã«ãƒãƒ©ãƒ³ã‚¹èª¿æ•´

  5. **ç™ºå±•çš„ãƒˆãƒ”ãƒƒã‚¯** \- åˆ¶ç´„ä»˜ãæœ€é©åŒ–: å®Ÿå‹™ã§é‡è¦ \- å¤šç›®çš„æœ€é©åŒ–: ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®å¯è¦–åŒ–

### é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

  * âœ… ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã¯**ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–** ã§ãã‚‹
  * âœ… ç²å¾—é–¢æ•°ã¯**æ¢ç´¢ã¨æ´»ç”¨ã‚’æ•°å­¦çš„ã«æœ€é©åŒ–**
  * âœ… EIãŒ**æœ€ã‚‚ä¸€èˆ¬çš„ã§å®Ÿç¸¾è±Šå¯Œ**
  * âœ… ã‚«ãƒ¼ãƒãƒ«ã®é¸æŠãŒ**ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å·¦å³**
  * âœ… åˆ¶ç´„ãƒ»å¤šç›®çš„ã¸ã®**æ‹¡å¼µãŒå¯èƒ½**

### æ¬¡ã®ç« ã¸

ç¬¬3ç« ã§ã¯ã€Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ãŸå®Ÿè£…ã‚’å­¦ã³ã¾ã™ï¼š \- scikit-optimizeï¼ˆskoptï¼‰ã®ä½¿ã„æ–¹ \- BoTorchï¼ˆPyTorchç‰ˆï¼‰ã®å®Ÿè£… \- å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®ææ–™æ¢ç´¢ \- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

**[ç¬¬3ç« ï¼šPythonå®Ÿè·µ â†’](<./chapter-3.html>)**

* * *

## æ¼”ç¿’å•é¡Œ

### å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰

RBFã‚«ãƒ¼ãƒãƒ«ã®é•·ã•ã‚¹ã‚±ãƒ¼ãƒ« $\ell$ ãŒã€ã‚¬ã‚¦ã‚¹éç¨‹ã®äºˆæ¸¬ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’èª¿ã¹ã¦ãã ã•ã„ã€‚

**ã‚¿ã‚¹ã‚¯** : 1\. 5ç‚¹ã®è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ 2\. $\ell = 0.05, 0.1, 0.2, 0.5$ ã§ã‚¬ã‚¦ã‚¹éç¨‹ã‚’å­¦ç¿’ 3\. äºˆæ¸¬å¹³å‡ã¨ä¿¡é ¼åŒºé–“ã‚’ãƒ—ãƒ­ãƒƒãƒˆ 4\. é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ã®å½±éŸ¿ã‚’èª¬æ˜

ãƒ’ãƒ³ãƒˆ \- `GaussianProcessRegressor`ã®`length_scale`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰æ›´ \- `predict(return_std=True)`ã§æ¨™æº–åå·®ã‚’å–å¾— \- é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ãŒå°ã•ã„ â†’ å±€æ‰€çš„ã«ãƒ•ã‚£ãƒƒãƒˆ \- é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ãŒå¤§ãã„ â†’ æ»‘ã‚‰ã‹ãªæ›²ç·š  è§£ç­”ä¾‹
    
    
    import numpy as np
    import matplotlib.pyplot as plt
    
    # è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿
    np.random.seed(42)
    X_train = np.array([0.1, 0.3, 0.5, 0.7, 0.9]).reshape(-1, 1)
    y_train = true_function(X_train).ravel()
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    X_test = np.linspace(0, 1, 200).reshape(-1, 1)
    y_true = true_function(X_test).ravel()
    
    # ç•°ãªã‚‹é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ã§å­¦ç¿’
    length_scales = [0.05, 0.1, 0.2, 0.5]
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.ravel()
    
    for i, ls in enumerate(length_scales):
        ax = axes[i]
    
        # ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«
        gp = GaussianProcessRegressor(sigma=1.0, length_scale=ls,
                                       noise=0.01)
        gp.fit(X_train, y_train)
    
        # äºˆæ¸¬
        y_pred, y_std = gp.predict(X_test, return_std=True)
    
        # ãƒ—ãƒ­ãƒƒãƒˆ
        ax.plot(X_test, y_true, 'k--', linewidth=2, label='çœŸã®é–¢æ•°')
        ax.scatter(X_train, y_train, c='red', s=100, zorder=10,
                   edgecolors='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')
        ax.plot(X_test, y_pred, 'b-', linewidth=2, label='äºˆæ¸¬å¹³å‡')
        ax.fill_between(X_test.ravel(), y_pred - 1.96 * y_std,
                        y_pred + 1.96 * y_std, alpha=0.3, color='blue',
                        label='95%ä¿¡é ¼åŒºé–“')
        ax.set_title(f'é•·ã•ã‚¹ã‚±ãƒ¼ãƒ« = {ls}', fontsize=14)
        ax.set_xlabel('x', fontsize=12)
        ax.set_ylabel('y', fontsize=12)
        ax.legend(loc='best')
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('length_scale_effect.png', dpi=150,
                bbox_inches='tight')
    plt.show()
    
    print("é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ã®å½±éŸ¿:")
    print("  å°ã•ã„ (0.05): è¦³æ¸¬ç‚¹ã«ã´ã£ãŸã‚Šãƒ•ã‚£ãƒƒãƒˆã€é–“ãŒä¸å®‰å®š")
    print("  ä¸­ç¨‹åº¦ (0.1-0.2): ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„")
    print("  å¤§ãã„ (0.5): æ»‘ã‚‰ã‹ã ãŒã€è¦³æ¸¬ç‚¹ã‹ã‚‰ä¹–é›¢")
    

**è§£èª¬**: \- **$\ell$ = 0.05**: ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒˆæ°—å‘³ã€è¦³æ¸¬ç‚¹é–“ãŒä¸å®‰å®š \- **$\ell$ = 0.1-0.2**: é©åº¦ãªæ»‘ã‚‰ã‹ã•ã€å®Ÿç”¨çš„ \- **$\ell$ = 0.5**: ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ã‚£ãƒƒãƒˆã€æ»‘ã‚‰ã‹ã™ã **ææ–™ç§‘å­¦ã¸ã®ç¤ºå”†**: \- å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿: $\ell$ = 0.1-0.3 ãŒä¸€èˆ¬çš„ \- DFTè¨ˆç®—: ã‚ˆã‚Šæ»‘ã‚‰ã‹ï¼ˆ$\ell$ = 0.3-0.5ï¼‰ \- ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§æœ€é©å€¤ã‚’æ±ºå®š 

* * *

### å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰

3ã¤ã®ç²å¾—é–¢æ•°ï¼ˆEIã€UCBã€PIï¼‰ã‚’å®Ÿè£…ã—ã€åŒã˜ãƒ‡ãƒ¼ã‚¿ã§æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚

**ã‚¿ã‚¹ã‚¯** : 1\. åŒã˜è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ 2\. å„ç²å¾—é–¢æ•°ã§æ¬¡ã®å®Ÿé¨“ç‚¹ã‚’ææ¡ˆ 3\. ææ¡ˆç‚¹ã®é•ã„ã‚’å¯è¦–åŒ– 4\. ãã‚Œãã‚Œã®ç‰¹å¾´ã‚’èª¬æ˜

ãƒ’ãƒ³ãƒˆ \- åŒã˜ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«ã‚’3ã¤ã®ç²å¾—é–¢æ•°ã§è©•ä¾¡ \- `np.argmax()`ã§æœ€å¤§å€¤ã®ä½ç½®ã‚’å–å¾— \- UCBã®$\kappa = 2.0$ã‚’ä½¿ç”¨  è§£ç­”ä¾‹
    
    
    # è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿
    np.random.seed(42)
    X_train = np.array([0.15, 0.4, 0.6, 0.85]).reshape(-1, 1)
    y_train = true_function(X_train).ravel()
    
    # ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«
    gp = GaussianProcessRegressor(sigma=1.0, length_scale=0.15,
                                   noise=0.01)
    gp.fit(X_train, y_train)
    
    # ç¾åœ¨ã®æœ€è‰¯å€¤
    f_best = np.max(y_train)
    
    # ãƒ†ã‚¹ãƒˆç‚¹
    X_test = np.linspace(0, 1, 500).reshape(-1, 1)
    y_pred, y_std = gp.predict(X_test, return_std=True)
    
    # ç²å¾—é–¢æ•°ã‚’è¨ˆç®—
    ei = expected_improvement(X_test, gp, f_best, xi=0.01)
    ucb = upper_confidence_bound(X_test, gp, kappa=2.0)
    pi = probability_of_improvement(X_test, gp, f_best, xi=0.01)
    
    # ææ¡ˆç‚¹
    next_x_ei = X_test[np.argmax(ei)]
    next_x_ucb = X_test[np.argmax(ucb)]
    next_x_pi = X_test[np.argmax(pi)]
    
    # å¯è¦–åŒ–
    fig, axes = plt.subplots(4, 1, figsize=(12, 14))
    
    # 1. ã‚¬ã‚¦ã‚¹éç¨‹ã®äºˆæ¸¬
    ax1 = axes[0]
    ax1.plot(X_test, y_pred, 'b-', linewidth=2, label='äºˆæ¸¬å¹³å‡')
    ax1.fill_between(X_test.ravel(), y_pred - 1.96 * y_std,
                     y_pred + 1.96 * y_std, alpha=0.3, color='blue')
    ax1.scatter(X_train, y_train, c='red', s=100, zorder=10,
                edgecolors='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')
    ax1.axhline(f_best, color='green', linestyle=':', linewidth=2,
                label=f'æœ€è‰¯å€¤ = {f_best:.3f}')
    ax1.set_ylabel('ç›®çš„é–¢æ•°', fontsize=12)
    ax1.set_title('ã‚¬ã‚¦ã‚¹éç¨‹ã®äºˆæ¸¬', fontsize=14)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Expected Improvement
    ax2 = axes[1]
    ax2.plot(X_test, ei, 'r-', linewidth=2, label='EI')
    ax2.axvline(next_x_ei, color='red', linestyle='--', linewidth=2,
                label=f'ææ¡ˆç‚¹ = {next_x_ei[0]:.3f}')
    ax2.fill_between(X_test.ravel(), 0, ei, alpha=0.3, color='red')
    ax2.set_ylabel('EI(x)', fontsize=12)
    ax2.set_title('Expected Improvement', fontsize=14)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. Upper Confidence Bound
    ax3 = axes[2]
    # UCBã‚’æ­£è¦åŒ–ï¼ˆæ¯”è¼ƒã—ã‚„ã™ãã™ã‚‹ãŸã‚ï¼‰
    ucb_normalized = (ucb - np.min(ucb)) / (np.max(ucb) - np.min(ucb))
    ax3.plot(X_test, ucb_normalized, 'b-', linewidth=2, label='UCB (æ­£è¦åŒ–)')
    ax3.axvline(next_x_ucb, color='blue', linestyle='--', linewidth=2,
                label=f'ææ¡ˆç‚¹ = {next_x_ucb[0]:.3f}')
    ax3.fill_between(X_test.ravel(), 0, ucb_normalized, alpha=0.3,
                     color='blue')
    ax3.set_ylabel('UCB(x)', fontsize=12)
    ax3.set_title('Upper Confidence Bound (Îº=2.0)', fontsize=14)
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. Probability of Improvement
    ax4 = axes[3]
    ax4.plot(X_test, pi, 'g-', linewidth=2, label='PI')
    ax4.axvline(next_x_pi, color='green', linestyle='--', linewidth=2,
                label=f'ææ¡ˆç‚¹ = {next_x_pi[0]:.3f}')
    ax4.fill_between(X_test.ravel(), 0, pi, alpha=0.3, color='green')
    ax4.set_xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
    ax4.set_ylabel('PI(x)', fontsize=12)
    ax4.set_title('Probability of Improvement', fontsize=14)
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('acquisition_functions_detailed_comparison.png',
                dpi=150, bbox_inches='tight')
    plt.show()
    
    # çµæœã®ã‚µãƒãƒªãƒ¼
    print("ç²å¾—é–¢æ•°åˆ¥ã®ææ¡ˆç‚¹:")
    print(f"  EI:  x = {next_x_ei[0]:.3f}")
    print(f"  UCB: x = {next_x_ucb[0]:.3f}")
    print(f"  PI:  x = {next_x_pi[0]:.3f}")
    
    print("\nç‰¹å¾´:")
    print("  EI: ãƒãƒ©ãƒ³ã‚¹å‹ã€æ”¹å–„é‡ã®æœŸå¾…å€¤ã‚’æœ€å¤§åŒ–")
    print("  UCB: æ¢ç´¢é‡è¦–ã€ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„é ˜åŸŸã‚’å¥½ã‚€")
    print("  PI: ä¿å®ˆçš„ã€å°ã•ãªæ”¹å–„ã§ã‚‚ç©æ¥µçš„")
    

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›**: 
    
    
    ç²å¾—é–¢æ•°åˆ¥ã®ææ¡ˆç‚¹:
      EI:  x = 0.722
      UCB: x = 0.108
      PI:  x = 0.752
    
    ç‰¹å¾´:
      EI: ãƒãƒ©ãƒ³ã‚¹å‹ã€æ”¹å–„é‡ã®æœŸå¾…å€¤ã‚’æœ€å¤§åŒ–
      UCB: æ¢ç´¢é‡è¦–ã€ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„é ˜åŸŸã‚’å¥½ã‚€
      PI: ä¿å®ˆçš„ã€å°ã•ãªæ”¹å–„ã§ã‚‚ç©æ¥µçš„
    

**è©³ç´°ãªè§£èª¬**: \- **EI**: æœªè¦³æ¸¬é ˜åŸŸã¨äºˆæ¸¬ãŒè‰¯ã„é ˜åŸŸã®ä¸­é–“ã‚’ææ¡ˆ \- **UCB**: ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å·¦ç«¯ã‚’æ¢ç´¢ï¼ˆä¸ç¢ºå®Ÿæ€§é‡è¦–ï¼‰ \- **PI**: äºˆæ¸¬å¹³å‡ãŒæœ€è‰¯å€¤ã‚’è¶…ãˆãã†ãªå ´æ‰€ã‚’ææ¡ˆ **å®Ÿå‹™ã§ã®é¸æŠ**: \- ä¸€èˆ¬çš„ãªæœ€é©åŒ– â†’ EI \- åˆæœŸæ¢ç´¢ãƒ•ã‚§ãƒ¼ã‚º â†’ UCBï¼ˆÎºå¤§ãã‚ï¼‰ \- åæŸãƒ•ã‚§ãƒ¼ã‚º â†’ PI or EI 

* * *

### å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰

åˆ¶ç´„ä»˜ããƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’å®Ÿè£…ã—ã€åˆ¶ç´„ãŒãªã„å ´åˆã¨æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚

**èƒŒæ™¯** : Li-ioné›»æ± é›»è§£è³ªã®æœ€é©åŒ– \- ç›®çš„: ã‚¤ã‚ªãƒ³ä¼å°åº¦ã‚’æœ€å¤§åŒ– \- åˆ¶ç´„: ç²˜åº¦ < 10 cP

**ã‚¿ã‚¹ã‚¯** : 1\. ç›®çš„é–¢æ•°ã¨åˆ¶ç´„é–¢æ•°ã‚’å®šç¾© 2\. åˆ¶ç´„ãªã—ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’å®Ÿè¡Œï¼ˆ10å›ï¼‰ 3\. åˆ¶ç´„ä»˜ããƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’å®Ÿè¡Œï¼ˆ10å›ï¼‰ 4\. æ¢ç´¢ã®è»Œè·¡ã‚’æ¯”è¼ƒ 5\. æœ€çµ‚çš„ã«è¦‹ã¤ã‹ã£ãŸè§£ã‚’è©•ä¾¡

ãƒ’ãƒ³ãƒˆ **ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**: 1\. åˆæœŸãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ3ç‚¹ï¼‰ 2\. ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«ã‚’2ã¤æ§‹ç¯‰ï¼ˆç›®çš„é–¢æ•°ç”¨ã€åˆ¶ç´„é–¢æ•°ç”¨ï¼‰ 3\. ãƒ«ãƒ¼ãƒ—ã§é€æ¬¡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° 4\. åˆ¶ç´„ä»˜ãEIã‚’ä½¿ç”¨ **ä½¿ç”¨ã™ã‚‹é–¢æ•°**: \- `constrained_expected_improvement()`  è§£ç­”ä¾‹
    
    
    # ç›®çš„é–¢æ•°ã¨åˆ¶ç´„é–¢æ•°ã‚’å®šç¾©
    def objective_conductivity(x):
        """ã‚¤ã‚ªãƒ³ä¼å°åº¦ï¼ˆæœ€å¤§åŒ–ï¼‰"""
        return true_function(x)
    
    def constraint_viscosity(x):
        """ç²˜åº¦ã®åˆ¶ç´„ï¼ˆâ‰¤ 10 cPã‚’0ã«æ­£è¦åŒ–ï¼‰"""
        viscosity = 15 - 10 * x  # ç²˜åº¦ã®ãƒ¢ãƒ‡ãƒ«
        return viscosity - 10  # 10 cPä»¥ä¸‹ãŒå®Ÿè¡Œå¯èƒ½ï¼ˆâ‰¤ 0ï¼‰
    
    # ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    def run_bayesian_optimization(n_iterations=10,
                                   use_constraint=False):
        """
        ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’å®Ÿè¡Œ
    
        Parameters:
        -----------
        n_iterations : int
            æœ€é©åŒ–ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°
        use_constraint : bool
            åˆ¶ç´„ã‚’ä½¿ç”¨ã™ã‚‹ã‹
    
        Returns:
        --------
        X_sampled : å®Ÿé¨“ç‚¹
        y_sampled : ç›®çš„é–¢æ•°å€¤
        c_sampled : åˆ¶ç´„é–¢æ•°å€¤ï¼ˆåˆ¶ç´„ã‚ã‚Šæ™‚ã®ã¿ï¼‰
        """
        # åˆæœŸãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        np.random.seed(42)
        X_sampled = np.random.uniform(0, 1, 3).reshape(-1, 1)
        y_sampled = objective_conductivity(X_sampled).ravel()
        c_sampled = constraint_viscosity(X_sampled).ravel()
    
        # é€æ¬¡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        for i in range(n_iterations - 3):
            # ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«ï¼ˆç›®çš„é–¢æ•°ï¼‰
            gp_obj = GaussianProcessRegressor(sigma=1.0,
                                               length_scale=0.15,
                                               noise=0.01)
            gp_obj.fit(X_sampled, y_sampled)
    
            # å€™è£œç‚¹
            X_candidate = np.linspace(0, 1, 1000).reshape(-1, 1)
    
            if use_constraint:
                # ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«ï¼ˆåˆ¶ç´„é–¢æ•°ï¼‰
                gp_constraint = GaussianProcessRegressor(sigma=0.5,
                                                         length_scale=0.2,
                                                         noise=0.01)
                gp_constraint.fit(X_sampled, c_sampled)
    
                # åˆ¶ç´„ä»˜ãEI
                f_best = np.max(y_sampled)
                acq = constrained_expected_improvement(
                    X_candidate, gp_obj, gp_constraint, f_best,
                    constraint_threshold=0
                )
            else:
                # åˆ¶ç´„ãªã—EI
                f_best = np.max(y_sampled)
                acq = expected_improvement(X_candidate, gp_obj,
                                           f_best, xi=0.01)
    
            # æ¬¡ã®å®Ÿé¨“ç‚¹
            next_x = X_candidate[np.argmax(acq)]
    
            # å®Ÿé¨“å®Ÿè¡Œ
            next_y = objective_conductivity(next_x).ravel()[0]
            next_c = constraint_viscosity(next_x).ravel()[0]
    
            # ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
            X_sampled = np.vstack([X_sampled, next_x])
            y_sampled = np.append(y_sampled, next_y)
            c_sampled = np.append(c_sampled, next_c)
    
        return X_sampled, y_sampled, c_sampled
    
    # 2ã¤ã®ã‚·ãƒŠãƒªã‚ªã‚’å®Ÿè¡Œ
    X_unconst, y_unconst, c_unconst = run_bayesian_optimization(
        n_iterations=10, use_constraint=False
    )
    X_const, y_const, c_const = run_bayesian_optimization(
        n_iterations=10, use_constraint=True
    )
    
    # å¯è¦–åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # å·¦ä¸Š: ç›®çš„é–¢æ•°
    ax1 = axes[0, 0]
    x_fine = np.linspace(0, 1, 500)
    y_fine = objective_conductivity(x_fine)
    ax1.plot(x_fine, y_fine, 'k-', linewidth=2, label='çœŸã®é–¢æ•°')
    ax1.scatter(X_unconst, y_unconst, c='blue', s=100, alpha=0.6,
                label='åˆ¶ç´„ãªã—', marker='o')
    ax1.scatter(X_const, y_const, c='red', s=100, alpha=0.6,
                label='åˆ¶ç´„ä»˜ã', marker='^')
    ax1.set_xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
    ax1.set_ylabel('ã‚¤ã‚ªãƒ³ä¼å°åº¦', fontsize=12)
    ax1.set_title('ç›®çš„é–¢æ•°ã®æ¢ç´¢', fontsize=14)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # å³ä¸Š: åˆ¶ç´„é–¢æ•°
    ax2 = axes[0, 1]
    c_fine = constraint_viscosity(x_fine)
    ax2.plot(x_fine, c_fine, 'k-', linewidth=2, label='åˆ¶ç´„é–¢æ•°')
    ax2.axhline(0, color='red', linestyle='--', linewidth=2,
                label='åˆ¶ç´„å¢ƒç•Œï¼ˆâ‰¤ 0ãŒå®Ÿè¡Œå¯èƒ½ï¼‰')
    ax2.axhspan(-20, 0, alpha=0.2, color='green',
                label='å®Ÿè¡Œå¯èƒ½é ˜åŸŸ')
    ax2.scatter(X_unconst, c_unconst, c='blue', s=100, alpha=0.6,
                label='åˆ¶ç´„ãªã—', marker='o')
    ax2.scatter(X_const, c_const, c='red', s=100, alpha=0.6,
                label='åˆ¶ç´„ä»˜ã', marker='^')
    ax2.set_xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
    ax2.set_ylabel('åˆ¶ç´„é–¢æ•°å€¤', fontsize=12)
    ax2.set_title('åˆ¶ç´„ã®æº€è¶³åº¦', fontsize=14)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # å·¦ä¸‹: æœ€è‰¯å€¤ã®æ¨ç§»
    ax3 = axes[1, 0]
    best_unconst = np.maximum.accumulate(y_unconst)
    best_const = np.maximum.accumulate(y_const)
    ax3.plot(range(1, 11), best_unconst, 'o-', color='blue',
             linewidth=2, markersize=8, label='åˆ¶ç´„ãªã—')
    ax3.plot(range(1, 11), best_const, '^-', color='red',
             linewidth=2, markersize=8, label='åˆ¶ç´„ä»˜ã')
    ax3.set_xlabel('å®Ÿé¨“å›æ•°', fontsize=12)
    ax3.set_ylabel('ã“ã‚Œã¾ã§ã®æœ€è‰¯å€¤', fontsize=12)
    ax3.set_title('æœ€è‰¯å€¤ã®æ¨ç§»', fontsize=14)
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # å³ä¸‹: åˆ¶ç´„æº€è¶³åº¦ã®æ¨ç§»
    ax4 = axes[1, 1]
    # åˆ¶ç´„ã‚’æº€ãŸã™ã‚µãƒ³ãƒ—ãƒ«ã®æ•°
    feasible_unconst = np.cumsum(c_unconst <= 0)
    feasible_const = np.cumsum(c_const <= 0)
    ax4.plot(range(1, 11), feasible_unconst, 'o-', color='blue',
             linewidth=2, markersize=8, label='åˆ¶ç´„ãªã—')
    ax4.plot(range(1, 11), feasible_const, '^-', color='red',
             linewidth=2, markersize=8, label='åˆ¶ç´„ä»˜ã')
    ax4.set_xlabel('å®Ÿé¨“å›æ•°', fontsize=12)
    ax4.set_ylabel('å®Ÿè¡Œå¯èƒ½è§£ã®ç´¯ç©æ•°', fontsize=12)
    ax4.set_title('åˆ¶ç´„æº€è¶³åº¦ã®æ¨ç§»', fontsize=14)
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('constrained_bo_comparison.png', dpi=150,
                bbox_inches='tight')
    plt.show()
    
    # çµæœã®ã‚µãƒãƒªãƒ¼
    print("æœ€é©åŒ–çµæœã®æ¯”è¼ƒ:")
    print("\nåˆ¶ç´„ãªã—ãƒ™ã‚¤ã‚ºæœ€é©åŒ–:")
    print(f"  æœ€è‰¯å€¤: {np.max(y_unconst):.4f}")
    print(f"  å¯¾å¿œã™ã‚‹x: {X_unconst[np.argmax(y_unconst)][0]:.3f}")
    print(f"  åˆ¶ç´„å€¤: {c_unconst[np.argmax(y_unconst)]:.4f}")
    print(f"  åˆ¶ç´„æº€è¶³: {'Yes' if c_unconst[np.argmax(y_unconst)] <= 0 else 'No'}")
    print(f"  å®Ÿè¡Œå¯èƒ½è§£ã®æ•°: {np.sum(c_unconst <= 0)}/10")
    
    print("\nåˆ¶ç´„ä»˜ããƒ™ã‚¤ã‚ºæœ€é©åŒ–:")
    # åˆ¶ç´„ã‚’æº€ãŸã™è§£ã®ä¸­ã§æœ€è‰¯ã®ã‚‚ã®ã‚’æ¢ã™
    feasible_indices = np.where(c_const <= 0)[0]
    if len(feasible_indices) > 0:
        best_feasible_idx = feasible_indices[np.argmax(y_const[feasible_indices])]
        print(f"  æœ€è‰¯å€¤: {y_const[best_feasible_idx]:.4f}")
        print(f"  å¯¾å¿œã™ã‚‹x: {X_const[best_feasible_idx][0]:.3f}")
        print(f"  åˆ¶ç´„å€¤: {c_const[best_feasible_idx]:.4f}")
        print(f"  åˆ¶ç´„æº€è¶³: Yes")
    else:
        print("  å®Ÿè¡Œå¯èƒ½è§£ãªã—")
    print(f"  å®Ÿè¡Œå¯èƒ½è§£ã®æ•°: {np.sum(c_const <= 0)}/10")
    
    print("\nè€ƒå¯Ÿ:")
    print("  - åˆ¶ç´„ä»˜ãã¯å®Ÿè¡Œå¯èƒ½é ˜åŸŸã«é›†ä¸­ã—ã¦æ¢ç´¢")
    print("  - åˆ¶ç´„ãªã—ã¯é«˜ã„ç›®çš„é–¢æ•°å€¤ã‚’ç™ºè¦‹ã™ã‚‹ãŒã€åˆ¶ç´„é•åã®å¯èƒ½æ€§")
    print("  - å®Ÿå‹™ã§ã¯åˆ¶ç´„ã‚’è€ƒæ…®ã—ãŸæœ€é©åŒ–ãŒå¿…é ˆ")
    

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›**: 
    
    
    æœ€é©åŒ–çµæœã®æ¯”è¼ƒ:
    
    åˆ¶ç´„ãªã—ãƒ™ã‚¤ã‚ºæœ€é©åŒ–:
      æœ€è‰¯å€¤: 0.8234
      å¯¾å¿œã™ã‚‹x: 0.312
      åˆ¶ç´„å€¤: 1.876
      åˆ¶ç´„æº€è¶³: No
      å®Ÿè¡Œå¯èƒ½è§£ã®æ•°: 4/10
    
    åˆ¶ç´„ä»˜ããƒ™ã‚¤ã‚ºæœ€é©åŒ–:
      æœ€è‰¯å€¤: 0.7456
      å¯¾å¿œã™ã‚‹x: 0.523
      åˆ¶ç´„å€¤: -0.234
      åˆ¶ç´„æº€è¶³: Yes
      å®Ÿè¡Œå¯èƒ½è§£ã®æ•°: 8/10
    
    è€ƒå¯Ÿ:
      - åˆ¶ç´„ä»˜ãã¯å®Ÿè¡Œå¯èƒ½é ˜åŸŸã«é›†ä¸­ã—ã¦æ¢ç´¢
      - åˆ¶ç´„ãªã—ã¯é«˜ã„ç›®çš„é–¢æ•°å€¤ã‚’ç™ºè¦‹ã™ã‚‹ãŒã€åˆ¶ç´„é•åã®å¯èƒ½æ€§
      - å®Ÿå‹™ã§ã¯åˆ¶ç´„ã‚’è€ƒæ…®ã—ãŸæœ€é©åŒ–ãŒå¿…é ˆ
    

**é‡è¦ãªæ´å¯Ÿ**: 1\. **åˆ¶ç´„ãªã—**: ã‚ˆã‚Šé«˜ã„ç›®çš„é–¢æ•°å€¤ã‚’ç™ºè¦‹ã™ã‚‹ãŒã€å®Ÿè¡Œä¸å¯èƒ½ 2\. **åˆ¶ç´„ä»˜ã**: ã‚„ã‚„ä½ã„ç›®çš„é–¢æ•°å€¤ã ãŒã€å®Ÿè¡Œå¯èƒ½ 3\. **å®Ÿå‹™**: åˆ¶ç´„ã‚’æº€ãŸã•ãªã„è§£ã¯ç„¡æ„å‘³ï¼ˆææ–™ãŒä½¿ãˆãªã„ï¼‰ 4\. **åŠ¹ç‡**: åˆ¶ç´„ä»˜ãã¯å®Ÿè¡Œå¯èƒ½é ˜åŸŸã«é›†ä¸­ã—ã€ç„¡é§„ãŒå°‘ãªã„ 

* * *

## å‚è€ƒæ–‡çŒ®

  1. Rasmussen, C. E. & Williams, C. K. I. (2006). _Gaussian Processes for Machine Learning_. MIT Press. [Onlineç‰ˆ](<http://gaussianprocess.org/gpml/>)

  2. Brochu, E. et al. (2010). "A Tutorial on Bayesian Optimization of Expensive Cost Functions." _arXiv:1012.2599_. [arXiv:1012.2599](<https://arxiv.org/abs/1012.2599>)

  3. Mockus, J. (1974). "On Bayesian Methods for Seeking the Extremum." _Optimization Techniques IFIP Technical Conference_ , 400-404.

  4. Srinivas, N. et al. (2010). "Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design." _ICML 2010_. [arXiv:0912.3995](<https://arxiv.org/abs/0912.3995>)

  5. Gelbart, M. A. et al. (2014). "Bayesian Optimization with Unknown Constraints." _UAI 2014_.

  6. æŒæ©‹å¤§åœ°ãƒ»å¤§ç¾½æˆå¾ (2019). ã€ã‚¬ã‚¦ã‚¹éç¨‹ã¨æ©Ÿæ¢°å­¦ç¿’ã€è¬›è«‡ç¤¾. ISBN: 978-4061529267

* * *

## ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³

### å‰ã®ç« 

**[â† ç¬¬1ç« ï¼šãªãœææ–™æ¢ç´¢ã«æœ€é©åŒ–ãŒå¿…è¦ã‹](<./chapter-1.html>)**

### æ¬¡ã®ç« 

**[ç¬¬3ç« ï¼šPythonå®Ÿè·µ â†’](<./chapter-3.html>)**

### ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡

**[â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹](<./index.html>)**

* * *

## è‘—è€…æƒ…å ±

**ä½œæˆè€…** : AI Terakoya Content Team **ä½œæˆæ—¥** : 2025-10-17 **ãƒãƒ¼ã‚¸ãƒ§ãƒ³** : 1.0

**æ›´æ–°å±¥æ­´** : \- 2025-10-17: v1.0 åˆç‰ˆå…¬é–‹

**ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯** : \- GitHub Issues: [AI_Homepage/issues](<https://github.com/your-repo/AI_Homepage/issues>) \- Email: yusuke.hashimoto.b8@tohoku.ac.jp

**ãƒ©ã‚¤ã‚»ãƒ³ã‚¹** : Creative Commons BY 4.0

* * *

**æ¬¡ã®ç« ã§å®Ÿè£…ã‚’å­¦ã³ã¾ã—ã‚‡ã†ï¼**
