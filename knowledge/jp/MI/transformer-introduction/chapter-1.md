# ç¬¬1ç« : Transformeré©å‘½ã¨ææ–™ç§‘å­¦

**å­¦ç¿’æ™‚é–“**: 20-30åˆ† | **é›£æ˜“åº¦**: ä¸­ç´š

## ğŸ“‹ ã“ã®ç« ã§å­¦ã¶ã“ã¨

- Attentionæ©Ÿæ§‹ã®åŸç†ã¨æ•°å­¦çš„ç†è§£
- Self-Attentionã¨Multi-Head Attentionã®ä»•çµ„ã¿
- TransformerãŒRNN/CNNã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ç†ç”±
- BERTã€GPTã®åŸºæœ¬æ§‹é€ ã¨é•ã„
- ææ–™ç§‘å­¦ã§ã®æˆåŠŸäº‹ä¾‹

---

## 1.1 ãªãœTransformerãŒé©å‘½ã‚’èµ·ã“ã—ãŸã®ã‹

### å¾“æ¥ã®RNN/CNNã®é™ç•Œ

**RNNï¼ˆRecurrent Neural Networkï¼‰ã®å•é¡Œ**:
- é•·ã„ç³»åˆ—ã§ã®å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™º
- ä¸¦åˆ—åŒ–ãŒå›°é›£ï¼ˆé€æ¬¡å‡¦ç†ãŒå¿…è¦ï¼‰
- é•·æœŸä¾å­˜é–¢ä¿‚ã®æ•æ‰ãŒé›£ã—ã„

**CNNï¼ˆConvolutional Neural Networkï¼‰ã®å•é¡Œ**:
- å±€æ‰€çš„ãªç‰¹å¾´ã—ã‹æ‰ãˆã‚‰ã‚Œãªã„
- é•·è·é›¢ã®é–¢ä¿‚æ€§ã‚’æ‰ãˆã‚‹ã«ã¯æ·±ã„å±¤ãŒå¿…è¦
- åˆ†å­ãƒ»ææ–™ã®ã‚ˆã†ãªä¸è¦å‰‡ãªæ§‹é€ ã«ã¯ä¸å‘ã

### Transformerã®é©æ–°æ€§

**2017å¹´ã€"Attention Is All You Need"è«–æ–‡ã§ç™»å ´**:
- âœ… **å…¨è¦ç´ é–“ã®é–¢ä¿‚ã‚’ç›´æ¥ãƒ¢ãƒ‡ãƒ«åŒ–**ï¼ˆAttentionæ©Ÿæ§‹ï¼‰
- âœ… **å®Œå…¨ä¸¦åˆ—åŒ–å¯èƒ½**ï¼ˆGPUã‚’æœ€å¤§é™æ´»ç”¨ï¼‰
- âœ… **é•·è·é›¢ä¾å­˜é–¢ä¿‚ã‚’åŠ¹ç‡çš„ã«æ•æ‰**
- âœ… **è§£é‡ˆæ€§**ï¼ˆAttentioné‡ã¿ã§é‡è¦ãªéƒ¨åˆ†ã‚’å¯è¦–åŒ–ï¼‰

<div class="mermaid">
graph LR
    A[å…¥åŠ›ç³»åˆ—] --> B[Self-Attention]
    B --> C[Feed Forward]
    C --> D[å‡ºåŠ›]

    B -.-> E[ã™ã¹ã¦ã®è¦ç´ é–“ã®é–¢ä¿‚ã‚’è¨ˆç®—]
    E -.-> B

    style B fill:#e1f5ff
</div>

---

## 1.2 Attentionæ©Ÿæ§‹ã®åŸç†

### Attentionæ©Ÿæ§‹ã¨ã¯

**åŸºæœ¬æ¦‚å¿µ**: å…¥åŠ›ã®ä¸­ã§ã€Œã©ã“ã«æ³¨ç›®ã™ã¹ãã‹ã€ã‚’å­¦ç¿’ã™ã‚‹ä»•çµ„ã¿

**æ•°å¼**:
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

- **Q (Query)**: ã€Œä½•ã‚’æ¢ã—ã¦ã„ã‚‹ã‹ã€
- **K (Key)**: ã€Œä½•ã‚’æŒã£ã¦ã„ã‚‹ã‹ã€
- **V (Value)**: ã€Œå®Ÿéš›ã®å†…å®¹ã€
- $d_k$: Keyã®æ¬¡å…ƒï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å› å­ï¼‰

### ç›´æ„Ÿçš„ç†è§£

**å›³æ›¸é¤¨ã®ä¾‹ãˆ**:
- **Query**: ã€Œæ©Ÿæ¢°å­¦ç¿’ã®æœ¬ã‚’æ¢ã—ã¦ã„ã‚‹ã€
- **Key**: å„æœ¬ã®ç›®æ¬¡ãƒ»ã‚¿ã‚¤ãƒˆãƒ«
- **Value**: æœ¬ã®å®Ÿéš›ã®å†…å®¹
- **Attention**: é–¢é€£æ€§ãŒé«˜ã„æœ¬ã«ã€Œæ³¨ç›®ã€ã—ã¦èª­ã‚€

### Pythonå®Ÿè£…: åŸºæœ¬çš„ãªAttention

```python
import torch
import torch.nn.functional as F

def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Scaled Dot-Product Attention

    Args:
        Q: Query (batch_size, seq_len, d_k)
        K: Key (batch_size, seq_len, d_k)
        V: Value (batch_size, seq_len, d_v)
        mask: ãƒã‚¹ã‚¯ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    """
    d_k = Q.size(-1)

    # 1. Qã¨Kã®å†…ç©ã‚’è¨ˆç®—ï¼ˆé¡ä¼¼åº¦ï¼‰
    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
    # scores shape: (batch_size, seq_len_q, seq_len_k)

    # 2. ãƒã‚¹ã‚¯é©ç”¨ï¼ˆå¿…è¦ãªå ´åˆï¼‰
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)

    # 3. Softmaxã§æ­£è¦åŒ–ï¼ˆAttentioné‡ã¿ï¼‰
    attention_weights = F.softmax(scores, dim=-1)

    # 4. Attentioné‡ã¿ã§Valueã‚’é‡ã¿ä»˜ã‘å’Œ
    output = torch.matmul(attention_weights, V)

    return output, attention_weights

# ä½¿ç”¨ä¾‹
batch_size, seq_len, d_model = 2, 5, 64
Q = torch.randn(batch_size, seq_len, d_model)
K = torch.randn(batch_size, seq_len, d_model)
V = torch.randn(batch_size, seq_len, d_model)

output, attn_weights = scaled_dot_product_attention(Q, K, V)
print(f"Output shape: {output.shape}")  # (2, 5, 64)
print(f"Attention weights shape: {attn_weights.shape}")  # (2, 5, 5)
```

### Attentioné‡ã¿ã®å¯è¦–åŒ–

```python
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_attention(attention_weights, tokens=None):
    """
    Attentioné‡ã¿ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–

    Args:
        attention_weights: (seq_len, seq_len)ã®Attentioné‡ã¿
        tokens: ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    """
    plt.figure(figsize=(8, 6))

    # æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã®æœ€åˆã®ãƒ˜ãƒƒãƒ‰ã®Attentioné‡ã¿ã‚’å–å¾—
    attn = attention_weights[0].detach().numpy()

    sns.heatmap(attn, cmap='YlOrRd', cbar=True, square=True,
                xticklabels=tokens if tokens else range(attn.shape[0]),
                yticklabels=tokens if tokens else range(attn.shape[0]))

    plt.xlabel('Key (å‚ç…§å…ˆ)')
    plt.ylabel('Query (æ³¨ç›®å…ƒ)')
    plt.title('Attention Weights')
    plt.tight_layout()
    plt.show()

# ä½¿ç”¨ä¾‹
tokens = ['H', 'C', 'C', 'O', 'H']
visualize_attention(attn_weights, tokens)
```

---

## 1.3 Self-Attention: è‡ªå·±æ³¨æ„æ©Ÿæ§‹

### Self-Attentionã¨ã¯

**å®šç¾©**: å…¥åŠ›ç³»åˆ—è‡ªèº«ã«å¯¾ã—ã¦Attentionã‚’é©ç”¨ã™ã‚‹ä»•çµ„ã¿

**ç‰¹å¾´**:
- Queryã€Keyã€Valueã™ã¹ã¦åŒã˜å…¥åŠ›ã‹ã‚‰ç”Ÿæˆ
- ç³»åˆ—å†…ã®ä»»æ„ã®2è¦ç´ é–“ã®é–¢ä¿‚ã‚’ç›´æ¥ãƒ¢ãƒ‡ãƒ«åŒ–
- ä½ç½®ã«é–¢ã‚ã‚‰ãšã€é–¢é€£æ€§ã®é«˜ã„è¦ç´ ã«æ³¨ç›®

### åˆ†å­ã«ãŠã‘ã‚‹Self-Attentionã®ä¾‹

**ãƒ¡ã‚¿ãƒãƒ¼ãƒ« (CHâ‚ƒOH)ã®ä¾‹**:
```python
# åŸå­: C, H, H, H, O, H
# Self-Attentionã§å„åŸå­ãŒä»–ã®åŸå­ã¨ã®é–¢ä¿‚ã‚’å­¦ç¿’
# ä¾‹: OåŸå­ã¯CåŸå­ã¨å¼·ã„é–¢ä¿‚æ€§ã‚’æŒã¤
```

### Self-Attentionå®Ÿè£…

```python
import torch.nn as nn

class SelfAttention(nn.Module):
    def __init__(self, d_model):
        super(SelfAttention, self).__init__()
        self.d_model = d_model

        # Q, K, Vã¸ã®ç·šå½¢å¤‰æ›
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)

    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len, d_model)
        """
        # Q, K, Vã‚’ç”Ÿæˆ
        Q = self.W_q(x)
        K = self.W_k(x)
        V = self.W_v(x)

        # Scaled Dot-Product Attention
        output, attn_weights = scaled_dot_product_attention(Q, K, V)

        return output, attn_weights

# ä½¿ç”¨ä¾‹
d_model = 128
seq_len = 10
batch_size = 4

self_attn = SelfAttention(d_model)
x = torch.randn(batch_size, seq_len, d_model)
output, attn_weights = self_attn(x)

print(f"Input shape: {x.shape}")          # (4, 10, 128)
print(f"Output shape: {output.shape}")    # (4, 10, 128)
print(f"Attention shape: {attn_weights.shape}")  # (4, 10, 10)
```

---

## 1.4 Multi-Head Attention: å¤šé ­æ³¨æ„æ©Ÿæ§‹

### ãªãœMulti-HeadãŒå¿…è¦ã‹

**å˜ä¸€ã®Attentionãƒ˜ãƒƒãƒ‰ã®é™ç•Œ**:
- 1ã¤ã®è¦–ç‚¹ã‹ã‚‰ã—ã‹é–¢ä¿‚æ€§ã‚’è¦‹ã‚‰ã‚Œãªã„
- è¤‡é›‘ãªé–¢ä¿‚æ€§ï¼ˆåŒ–å­¦çµåˆã€ç«‹ä½“é…åº§ãªã©ï¼‰ã‚’æ‰ãˆãã‚Œãªã„

**Multi-Head Attentionã®åˆ©ç‚¹**:
- è¤‡æ•°ã®ç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰é–¢ä¿‚æ€§ã‚’å­¦ç¿’
- å„ãƒ˜ãƒƒãƒ‰ãŒç•°ãªã‚‹ç‰¹å¾´ï¼ˆçµåˆã€è·é›¢ã€è§’åº¦ãªã©ï¼‰ã‚’æ‰ãˆã‚‹
- ã‚ˆã‚Šè±Šã‹ãªè¡¨ç¾ãŒå¯èƒ½

### æ•°å¼

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
$$

where $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$

### å®Ÿè£…

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0, "d_modelã¯num_headsã§å‰²ã‚Šåˆ‡ã‚Œã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # Q, K, Vå¤‰æ›
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)

        # å‡ºåŠ›å¤‰æ›
        self.W_o = nn.Linear(d_model, d_model)

    def forward(self, x, mask=None):
        batch_size = x.size(0)

        # 1. Q, K, Vã‚’ç”Ÿæˆã—ã¦ã€ãƒ˜ãƒƒãƒ‰ã”ã¨ã«åˆ†å‰²
        Q = self.W_q(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        # Shape: (batch_size, num_heads, seq_len, d_k)

        # 2. å„ãƒ˜ãƒƒãƒ‰ã§Scaled Dot-Product Attention
        output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)
        # output: (batch_size, num_heads, seq_len, d_k)

        # 3. ãƒ˜ãƒƒãƒ‰ã‚’é€£çµ
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        # Shape: (batch_size, seq_len, d_model)

        # 4. å‡ºåŠ›å¤‰æ›
        output = self.W_o(output)

        return output, attn_weights

# ä½¿ç”¨ä¾‹
d_model = 512
num_heads = 8
seq_len = 20
batch_size = 2

mha = MultiHeadAttention(d_model, num_heads)
x = torch.randn(batch_size, seq_len, d_model)
output, attn_weights = mha(x)

print(f"Input shape: {x.shape}")          # (2, 20, 512)
print(f"Output shape: {output.shape}")    # (2, 20, 512)
print(f"Attention shape: {attn_weights.shape}")  # (2, 8, 20, 20)
```

---

## 1.5 Positional Encoding: ä½ç½®æƒ…å ±ã®åŸ‹ã‚è¾¼ã¿

### ãªãœå¿…è¦ã‹

**å•é¡Œ**: Self-Attentionã«ã¯é †åºã®æ¦‚å¿µãŒãªã„
- "H-C-O" ã¨ "O-C-H" ã‚’åŒºåˆ¥ã§ããªã„
- åˆ†å­ã‚„ææ–™ã§ã¯åŸå­ã®é…ç½®é †åºãŒé‡è¦

**è§£æ±ºç­–**: Positional Encodingã§ä½ç½®æƒ…å ±ã‚’è¿½åŠ 

### æ•°å¼

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

### å®Ÿè£…

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()

        # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¡Œåˆ—ã‚’ä½œæˆ
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len, d_model)
        """
        seq_len = x.size(1)
        x = x + self.pe[:, :seq_len, :]
        return x

# ä½¿ç”¨ä¾‹ã¨å¯è¦–åŒ–
d_model = 128
max_len = 100

pos_enc = PositionalEncoding(d_model, max_len)

# ãƒ€ãƒŸãƒ¼å…¥åŠ›
x = torch.zeros(1, 50, d_model)
output = pos_enc(x)

# å¯è¦–åŒ–
plt.figure(figsize=(12, 4))
plt.plot(pos_enc.pe[0, :50, :8].numpy())
plt.xlabel('Position')
plt.ylabel('Encoding Value')
plt.title('Positional Encoding (first 8 dimensions)')
plt.legend([f'dim {i}' for i in range(8)])
plt.tight_layout()
plt.show()
```

---

## 1.6 Transformerã¨BERT/GPT

### Transformerå…¨ä½“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

<div class="mermaid">
graph TB
    subgraph Encoder
        E1[Input Embedding] --> E2[Positional Encoding]
        E2 --> E3[Multi-Head Attention]
        E3 --> E4[Add and Norm]
        E4 --> E5[Feed Forward]
        E5 --> E6[Add and Norm]
    end

    subgraph Decoder
        D1[Output Embedding] --> D2[Positional Encoding]
        D2 --> D3[Masked Multi-Head Attention]
        D3 --> D4[Add and Norm]
        D4 --> D5[Multi-Head Attention]
        D5 --> D6[Add and Norm]
        D6 --> D7[Feed Forward]
        D7 --> D8[Add and Norm]
    end

    E6 -.-> D5
    D8 --> O[Output]

    style E3 fill:#e1f5ff
    style D3 fill:#ffe1e1
    style D5 fill:#e1ffe1
</div>

### BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰

**ç‰¹å¾´**:
- **Encoderã®ã¿**ä½¿ç”¨
- **åŒæ–¹å‘**ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç†è§£
- **äº‹å‰å­¦ç¿’ã‚¿ã‚¹ã‚¯**: Masked Language Model (MLM) + Next Sentence Prediction (NSP)
- **ç”¨é€”**: åˆ†é¡ã€ç‰¹å¾´æŠ½å‡ºã€è³ªå•å¿œç­”

**ææ–™ç§‘å­¦ã§ã®å¿œç”¨**:
- MatBERT: ææ–™ã®çµ„æˆå¼ã‹ã‚‰ç‰¹æ€§äºˆæ¸¬
- ChemBERTa: åˆ†å­SMILESè¡¨ç¾å­¦ç¿’

### GPTï¼ˆGenerative Pre-trained Transformerï¼‰

**ç‰¹å¾´**:
- **Decoderã®ã¿**ä½¿ç”¨
- **å˜æ–¹å‘**ï¼ˆå·¦ã‹ã‚‰å³ï¼‰ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
- **äº‹å‰å­¦ç¿’ã‚¿ã‚¹ã‚¯**: æ¬¡ã®å˜èªäºˆæ¸¬
- **ç”¨é€”**: ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã€å¯¾è©±ã€å‰µé€ çš„ã‚¿ã‚¹ã‚¯

**ææ–™ç§‘å­¦ã§ã®å¿œç”¨**:
- åˆ†å­ç”Ÿæˆï¼ˆSMILESæ–‡å­—åˆ—ç”Ÿæˆï¼‰
- ææ–™è¨˜è¿°æ–‡ã®è‡ªå‹•ç”Ÿæˆ
- åˆæˆçµŒè·¯ã®ææ¡ˆ

---

## 1.7 ææ–™ç§‘å­¦ã§ã®æˆåŠŸäº‹ä¾‹

### 1. ChemBERTa: åˆ†å­è¡¨ç¾å­¦ç¿’

**æ¦‚è¦**: SMILESã‚’BERTã§å­¦ç¿’
```python
# åˆ†å­SMILES: CC(C)Cc1ccc(cc1)C(C)C(=O)O (ã‚¤ãƒ–ãƒ—ãƒ­ãƒ•ã‚§ãƒ³)
# ChemBERTaã§åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ› â†’ ç‰¹æ€§äºˆæ¸¬
```

**æˆæœ**:
- å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®é«˜ç²¾åº¦äºˆæ¸¬
- è»¢ç§»å­¦ç¿’ã«ã‚ˆã‚Šé–‹ç™ºæœŸé–“çŸ­ç¸®
- è§£é‡ˆå¯èƒ½æ€§ï¼ˆAttentionã§é‡è¦éƒ¨åˆ†å¯è¦–åŒ–ï¼‰

### 2. Matformer: ææ–™ç‰¹æ€§äºˆæ¸¬

**æ¦‚è¦**: çµæ™¶æ§‹é€ ã‚’Transformerã§å‡¦ç†
```python
# å…¥åŠ›: åŸå­åº§æ¨™ã€åŸå­ç•ªå·ã€æ ¼å­å®šæ•°
# å‡ºåŠ›: ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã€å½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼
```

**æˆæœ**:
- Materials Projectãƒ‡ãƒ¼ã‚¿ã§é«˜ç²¾åº¦
- GNNã¨åŒç­‰ä»¥ä¸Šã®æ€§èƒ½
- è¨ˆç®—åŠ¹ç‡ãŒè‰¯ã„

### 3. æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹åˆ†å­ç”Ÿæˆ

**æ¦‚è¦**: æ¡ä»¶ä»˜ãæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã§æ–°è¦åˆ†å­ç”Ÿæˆ
```python
# æ¡ä»¶: æº¶è§£åº¦ > 5 mg/mL, LogP < 3
# ç”Ÿæˆ: æ¡ä»¶ã‚’æº€ãŸã™åˆ†å­SMILES
```

**æˆæœ**:
- å‰µè–¬ã§æœ‰æœ›ãªå€™è£œåˆ†å­ç™ºè¦‹
- å¾“æ¥æ‰‹æ³•ã‚ˆã‚Šå¤šæ§˜æ€§ãŒé«˜ã„
- åˆæˆå¯èƒ½æ€§ã‚‚è€ƒæ…®

---

## 1.8 ã¾ã¨ã‚

### é‡è¦ãƒã‚¤ãƒ³ãƒˆ

1. **Attentionæ©Ÿæ§‹**: ç³»åˆ—å†…ã®ä»»æ„ã®è¦ç´ é–“ã®é–¢ä¿‚ã‚’ç›´æ¥ãƒ¢ãƒ‡ãƒ«åŒ–
2. **Self-Attention**: å…¥åŠ›ç³»åˆ—è‡ªèº«ã«å¯¾ã™ã‚‹Attention
3. **Multi-Head Attention**: è¤‡æ•°ã®è¦–ç‚¹ã‹ã‚‰é–¢ä¿‚æ€§ã‚’å­¦ç¿’
4. **Positional Encoding**: ä½ç½®æƒ…å ±ã‚’åŸ‹ã‚è¾¼ã¿
5. **BERT/GPT**: Transformer based ã®ä»£è¡¨çš„äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«
6. **ææ–™ç§‘å­¦å¿œç”¨**: åˆ†å­ãƒ»ææ–™è¡¨ç¾å­¦ç¿’ã€ç‰¹æ€§äºˆæ¸¬ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«

### æ¬¡ç« ã¸ã®æº–å‚™

ç¬¬2ç« ã§ã¯ã€ææ–™ç§‘å­¦ã«ç‰¹åŒ–ã—ãŸTransformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆMatformerã€CrystalFormerã€ChemBERTaï¼‰ã‚’è©³ã—ãå­¦ã³ã¾ã™ã€‚

---

## ğŸ“ æ¼”ç¿’å•é¡Œ

### å•é¡Œ1: åŸºç¤ç†è§£ï¼ˆæ¦‚å¿µï¼‰
Attentionæ©Ÿæ§‹ã«ãŠã‘ã‚‹ Queryã€Keyã€Value ã®å½¹å‰²ã‚’ã€å›³æ›¸é¤¨ã®ä¾‹ãˆä»¥å¤–ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

<details>
<summary>è§£ç­”ä¾‹</summary>

**æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®ä¾‹ãˆ**:
- **Query**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
- **Key**: å„Webãƒšãƒ¼ã‚¸ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã€è¦ç´„ï¼‰
- **Value**: Webãƒšãƒ¼ã‚¸ã®å®Ÿéš›ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
- **Attention**: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã®é–¢é€£æ€§ãŒé«˜ã„ãƒšãƒ¼ã‚¸ã‚’ä¸Šä½è¡¨ç¤º

**åˆ†å­ã®ä¾‹ãˆ**:
- **Query**: ã‚ã‚‹åŸå­ãŒã€Œã©ã®åŸå­ã¨ç›¸äº’ä½œç”¨ã—ãŸã„ã‹ã€
- **Key**: å„åŸå­ã®ç‰¹å¾´ï¼ˆåŸå­ç•ªå·ã€é›»è·ã€ä½ç½®ï¼‰
- **Value**: å„åŸå­ã®è©³ç´°ãªæƒ…å ±
- **Attention**: åŒ–å­¦çµåˆã‚„ç›¸äº’ä½œç”¨ã®å¼·ã•ã‚’è¡¨ç¾
</details>

### å•é¡Œ2: å®Ÿè£…ï¼ˆã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã®ç©ºæ¬„ã‚’åŸ‹ã‚ã¦ã€Simple Attentionï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãªã—ã€ãƒã‚¹ã‚¯ãªã—ï¼‰ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚

```python
def simple_attention(Q, K, V):
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªAttentionæ©Ÿæ§‹

    Args:
        Q: Query (batch_size, seq_len, d_k)
        K: Key (batch_size, seq_len, d_k)
        V: Value (batch_size, seq_len, d_v)

    Returns:
        output: (batch_size, seq_len, d_v)
        attention_weights: (batch_size, seq_len, seq_len)
    """
    # 1. Qã¨Kã®å†…ç©ã‚’è¨ˆç®—
    scores = torch.matmul(______, ______.transpose(-2, -1))

    # 2. Softmaxã§æ­£è¦åŒ–
    attention_weights = F.softmax(______, dim=-1)

    # 3. Attentioné‡ã¿ã§Valueã‚’é‡ã¿ä»˜ã‘å’Œ
    output = torch.matmul(______, ______)

    return output, attention_weights
```

<details>
<summary>è§£ç­”ä¾‹</summary>

```python
def simple_attention(Q, K, V):
    # 1. Qã¨Kã®å†…ç©ã‚’è¨ˆç®—
    scores = torch.matmul(Q, K.transpose(-2, -1))

    # 2. Softmaxã§æ­£è¦åŒ–
    attention_weights = F.softmax(scores, dim=-1)

    # 3. Attentioné‡ã¿ã§Valueã‚’é‡ã¿ä»˜ã‘å’Œ
    output = torch.matmul(attention_weights, V)

    return output, attention_weights
```
</details>

### å•é¡Œ3: å¿œç”¨ï¼ˆè€ƒå¯Ÿï¼‰
åˆ†å­ "CCO"ï¼ˆã‚¨ã‚¿ãƒãƒ¼ãƒ«ï¼‰ã«ãŠã‘ã‚‹Self-Attentionã‚’è€ƒãˆã¾ã™ã€‚ä»¥ä¸‹ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ï¼š

1. ã©ã®åŸå­é–“ã®Attentioné‡ã¿ãŒæœ€ã‚‚é«˜ããªã‚‹ã¨äºˆæƒ³ã•ã‚Œã¾ã™ã‹ï¼Ÿ
2. ãã®ç†ç”±ã‚’åŒ–å­¦çš„è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
3. Multi-Head Attentionã§ã¯ã€å„ãƒ˜ãƒƒãƒ‰ãŒã©ã®ã‚ˆã†ãªç•°ãªã‚‹æƒ…å ±ã‚’æ‰ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ

<details>
<summary>è§£ç­”ä¾‹</summary>

1. **æœ€ã‚‚é«˜ã„Attentioné‡ã¿**: C-Cçµåˆã€C-Oçµåˆ

2. **åŒ–å­¦çš„ç†ç”±**:
   - å…±æœ‰çµåˆã«ã‚ˆã‚Šå¼·ã„ç›¸äº’ä½œç”¨ãŒã‚ã‚‹
   - é›»å­ã®å…±æœ‰ã«ã‚ˆã‚Šé›»å­å¯†åº¦ãŒé«˜ã„
   - OåŸå­ã¯CåŸå­ã¨æ¥µæ€§çµåˆã‚’å½¢æˆ

3. **å„ãƒ˜ãƒƒãƒ‰ãŒæ‰ãˆã‚‹æƒ…å ±ã®ä¾‹**:
   - **ãƒ˜ãƒƒãƒ‰1**: åŒ–å­¦çµåˆï¼ˆ1æ¬¡çµåˆï¼‰
   - **ãƒ˜ãƒƒãƒ‰2**: 2æ¬¡çµåˆï¼ˆC-C-Oè§’åº¦ï¼‰
   - **ãƒ˜ãƒƒãƒ‰3**: é›»å­å¯†åº¦åˆ†å¸ƒ
   - **ãƒ˜ãƒƒãƒ‰4**: åŸå­ã®ç¨®é¡ï¼ˆC vs O vs Hï¼‰
   - **ãƒ˜ãƒƒãƒ‰5**: ç«‹ä½“é…åº§æƒ…å ±
   - **ãƒ˜ãƒƒãƒ‰6**: æ¥µæ€§ç›¸äº’ä½œç”¨

   å„ãƒ˜ãƒƒãƒ‰ãŒç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰åˆ†å­ã‚’ç†è§£ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šè±Šã‹ãªè¡¨ç¾ãŒå¯èƒ½ã«ãªã‚‹ã€‚
</details>

---

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¨åˆ©ç”¨è¦ç´„

### è¨€èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- **WikiText-103**: [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)
- **BookCorpus**: ç ”ç©¶ç›®çš„ã®ã¿ã€å†é…å¸ƒä¸å¯
- **Common Crawl**: [Common Crawl Terms of Use](https://commoncrawl.org/terms-of-use/)

### ææ–™ç§‘å­¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- **Materials Project**: [CC BY 4.0](https://materialsproject.org/about/terms)
  - è«–æ–‡å¼•ç”¨: `Jain, A. et al. APL Materials 1, 011002 (2013)`
- **SMILESåˆ†å­ãƒ‡ãƒ¼ã‚¿**:
  - **ZINC**: å­¦è¡“åˆ©ç”¨å¯ã€å•†ç”¨ã¯è¦ç¢ºèª
  - **ChEMBL**: [CC BY-SA 3.0](https://chembl.gitbook.io/chembl-interface-documentation/about#data-licensing)
  - **PubChem**: ãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³
- **çµæ™¶æ§‹é€ ãƒ‡ãƒ¼ã‚¿**:
  - **ICSD**: ãƒ©ã‚¤ã‚»ãƒ³ã‚¹è³¼å…¥å¿…è¦
  - **COD (Crystallography Open Database)**: ãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³

### ãƒ©ã‚¤ã‚»ãƒ³ã‚¹éµå®ˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
```python
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½¿ç”¨æ™‚ã®å¼•ç”¨ä¾‹
"""
This work uses data from Materials Project (materialsproject.org),
which is released under CC BY 4.0 license.

Citation:
Jain, A., Ong, S. P., Hautier, G., Chen, W., Richards, W. D.,
Dacek, S., ... & Persson, K. A. (2013).
Commentary: The Materials Project: A materials genome approach
to accelerating materials innovation. APL materials, 1(1).
"""
```

---

## ğŸ”§ ã‚³ãƒ¼ãƒ‰å†ç¾æ€§ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

### ç’°å¢ƒè¨­å®š
```python
# requirements.txt
torch==2.0.1
transformers==4.30.2
numpy==1.24.3
matplotlib==3.7.1
seaborn==0.12.2

# æ¨å¥¨ï¼šå®Œå…¨ãªç’°å¢ƒå†ç¾
# conda env export > environment.yml
```

### å†ç¾æ€§ã®ãŸã‚ã®è¨­å®š
```python
import torch
import numpy as np
import random

def set_seed(seed=42):
    """
    å®Œå…¨ãªå†ç¾æ€§ã‚’ä¿è¨¼

    Args:
        seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    # CuDNNã®æŒ™å‹•ã‚’æ±ºå®šçš„ã«ã™ã‚‹ï¼ˆé€Ÿåº¦ã¯ä½ä¸‹ï¼‰
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# ä½¿ç”¨ä¾‹
set_seed(42)

# ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã®è¨˜éŒ²
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")
```

### Transformerãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ˜ç¤º
```python
# å®Ÿé¨“è¨­å®šã‚’è¾æ›¸ã§ç®¡ç†
config = {
    'model': {
        'd_model': 512,
        'num_heads': 8,
        'num_layers': 6,
        'd_ff': 2048,
        'dropout': 0.1,
        'max_seq_len': 512
    },
    'training': {
        'batch_size': 32,
        'learning_rate': 1e-4,
        'num_epochs': 100,
        'warmup_steps': 4000,
        'optimizer': 'Adam',
        'weight_decay': 0.01
    },
    'data': {
        'train_split': 0.8,
        'val_split': 0.1,
        'test_split': 0.1,
        'tokenizer': 'BPE',
        'vocab_size': 50000
    },
    'seed': 42
}

# è¨­å®šã®ä¿å­˜
import json
with open('experiment_config.json', 'w') as f:
    json.dump(config, f, indent=2)
```

### Attentionãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è©³ç´°è¨­å®š
```python
class ReproducibleMultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1, bias=True):
        """
        å†ç¾æ€§ã‚’é‡è¦–ã—ãŸMulti-Head Attention

        Args:
            d_model: ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒï¼ˆ512æ¨å¥¨ï¼‰
            num_heads: ãƒ˜ãƒƒãƒ‰æ•°ï¼ˆ8æ¨å¥¨ã€d_modelã§å‰²ã‚Šåˆ‡ã‚Œã‚‹å¿…è¦ï¼‰
            dropout: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡ï¼ˆ0.1æ¨å¥¨ï¼‰
            bias: ç·šå½¢å±¤ã«ãƒã‚¤ã‚¢ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        """
        super().__init__()
        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # åˆæœŸåŒ–æ–¹æ³•ã‚’æ˜ç¤º
        self.W_q = nn.Linear(d_model, d_model, bias=bias)
        self.W_k = nn.Linear(d_model, d_model, bias=bias)
        self.W_v = nn.Linear(d_model, d_model, bias=bias)
        self.W_o = nn.Linear(d_model, d_model, bias=bias)

        # XavieråˆæœŸåŒ–
        for module in [self.W_q, self.W_k, self.W_v, self.W_o]:
            nn.init.xavier_uniform_(module.weight)
            if bias:
                nn.init.zeros_(module.bias)

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # å®Ÿè£…ã¯å‰è¿°ã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜
        pass
```

---

## âš ï¸ å®Ÿè·µçš„ãªè½ã¨ã—ç©´ã¨å¯¾å‡¦æ³•

### 1. Attentionãƒã‚¹ã‚¯ã®èª¤ã‚Š
**å•é¡Œ**: ãƒã‚¹ã‚¯ã®é©ç”¨ãƒŸã‚¹ã§æœªæ¥ã®æƒ…å ±ãŒæ¼æ´©

```python
# âŒ é–“é•ã„: ãƒã‚¹ã‚¯ãŒæ­£ã—ãé©ç”¨ã•ã‚Œã¦ã„ãªã„
def wrong_attention(Q, K, V, mask):
    scores = torch.matmul(Q, K.transpose(-2, -1))
    # maskã®å€¤ãŒé€†ã«ãªã£ã¦ã„ã‚‹
    scores = scores.masked_fill(mask == 1, -1e9)  # é–“é•ã„ï¼
    return F.softmax(scores, dim=-1)

# âœ… æ­£ã—ã„: ãƒã‚¹ã‚¯ã¯0ã®ä½ç½®ã‚’ç„¡è¦–
def correct_attention(Q, K, V, mask):
    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(Q.size(-1))
    if mask is not None:
        # mask==0ã®ä½ç½®ã‚’-infã«ã™ã‚‹
        scores = scores.masked_fill(mask == 0, float('-inf'))
    return F.softmax(scores, dim=-1)

# ãƒ‡ãƒãƒƒã‚°æ–¹æ³•
print("Attention scores before mask:", scores)
print("Mask:", mask)
print("Attention scores after mask:", scores.masked_fill(mask == 0, float('-inf')))
```

### 2. Positional Encodingã®å®Ÿè£…ãƒŸã‚¹
**å•é¡Œ**: sinã¨cosã®æ¬¡å…ƒãŒé–“é•ã£ã¦ã„ã‚‹

```python
# âŒ é–“é•ã„: æ¬¡å…ƒã®å‰²ã‚Šå½“ã¦ãŒé€†
class WrongPositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))

        pe[:, 1::2] = torch.sin(position * div_term)  # é–“é•ã„ï¼
        pe[:, 0::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

# âœ… æ­£ã—ã„: sinã¯å¶æ•°æ¬¡å…ƒã€cosã¯å¥‡æ•°æ¬¡å…ƒ
class CorrectPositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)  # æ­£ã—ã„
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)
```

### 3. ãƒ¡ãƒ¢ãƒªã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼
**å•é¡Œ**: é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§OOMï¼ˆOut of Memoryï¼‰

```python
# âŒ å•é¡Œ: å…¨ç³»åˆ—ã‚’ä¸€åº¦ã«å‡¦ç†
def memory_intensive_attention(x):
    # x: (batch=64, seq_len=10000, d_model=512)
    # Attentionè¡Œåˆ—: (64, 10000, 10000) = ç´„24GBï¼
    return multi_head_attention(x)

# âœ… è§£æ±ºç­–1: Gradient checkpointing
from torch.utils.checkpoint import checkpoint

def memory_efficient_attention(x):
    return checkpoint(multi_head_attention, x)

# âœ… è§£æ±ºç­–2: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åˆ†å‰²
def chunked_attention(x, chunk_size=512):
    batch, seq_len, d_model = x.shape
    outputs = []

    for i in range(0, seq_len, chunk_size):
        chunk = x[:, i:i+chunk_size, :]
        output = multi_head_attention(chunk)
        outputs.append(output)

    return torch.cat(outputs, dim=1)

# âœ… è§£æ±ºç­–3: Sparse Attentionï¼ˆé•·è·é›¢ã‚¿ã‚¹ã‚¯å‘ã‘ï¼‰
# Longformerã€BigBirdãªã©ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨
```

### 4. ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®å•é¡Œï¼ˆææ–™ç§‘å­¦ç‰¹æœ‰ï¼‰
**å•é¡Œ**: SMILESã®æ‹¬å¼§ã‚„åˆ†å²ãŒæ­£ã—ãå‡¦ç†ã•ã‚Œãªã„

```python
# âŒ é–“é•ã„: å˜ç´”ãªæ–‡å­—åˆ†å‰²
def wrong_smiles_tokenize(smiles):
    return list(smiles)  # "C(C)O" â†’ ['C', '(', 'C', ')', 'O']

# âœ… æ­£ã—ã„: SMILESãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨
from transformers import RobertaTokenizer

tokenizer = RobertaTokenizer.from_pretrained("seyonec/ChemBERTa-zinc-base-v1")

# ã¾ãŸã¯æ­£è¦è¡¨ç¾ãƒ™ãƒ¼ã‚¹
import re
def correct_smiles_tokenize(smiles):
    pattern = r'(\[[^\]]+\]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\(|\)|\.|=|#|-|\+|\\|\/|:|~|@|\?|>|\*|\$|\%[0-9]{2}|[0-9])'
    return re.findall(pattern, smiles)

# ãƒ†ã‚¹ãƒˆ
smiles = "CC(C)Cc1ccc(cc1)C(C)C(=O)O"  # ã‚¤ãƒ–ãƒ—ãƒ­ãƒ•ã‚§ãƒ³
tokens = correct_smiles_tokenize(smiles)
print(f"Tokens: {tokens}")
```

### 5. æ•°å€¤ä¸å®‰å®šæ€§
**å•é¡Œ**: Softmaxã§ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼/ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼

```python
# âŒ å•é¡Œ: å¤§ããªã‚¹ã‚³ã‚¢ã§exp()ãŒã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼
def unstable_softmax(x):
    return torch.exp(x) / torch.sum(torch.exp(x), dim=-1, keepdim=True)

# âœ… è§£æ±ºç­–: æ•°å€¤å®‰å®šç‰ˆsoftmaxï¼ˆPyTorchã¯å†…éƒ¨ã§å®Ÿè£…æ¸ˆã¿ï¼‰
def stable_softmax(x):
    # æœ€å¤§å€¤ã‚’å¼•ã„ã¦æ•°å€¤å®‰å®šæ€§ã‚’ç¢ºä¿
    x_max = torch.max(x, dim=-1, keepdim=True)[0]
    exp_x = torch.exp(x - x_max)
    return exp_x / torch.sum(exp_x, dim=-1, keepdim=True)

# PyTorchã®F.softmax()ã‚’ä½¿ã†ã®ãŒæœ€ã‚‚å®‰å…¨
import torch.nn.functional as F
safe_output = F.softmax(x, dim=-1)
```

---

## âœ… ç¬¬1ç« å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### æ¦‚å¿µç†è§£ï¼ˆ10é …ç›®ï¼‰
- [ ] Attentionæ©Ÿæ§‹ã®Queryã€Keyã€Valueã®å½¹å‰²ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Self-Attentionã¨é€šå¸¸ã®Attentionã®é•ã„ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] Multi-Head AttentionãŒãªãœè¤‡æ•°ãƒ˜ãƒƒãƒ‰å¿…è¦ã‹èª¬æ˜ã§ãã‚‹
- [ ] Positional Encodingã®å¿…è¦æ€§ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] TransformerãŒä¸¦åˆ—åŒ–å¯èƒ½ãªç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] RNN/CNNã«å¯¾ã™ã‚‹Transformerã®åˆ©ç‚¹ã‚’3ã¤ä»¥ä¸ŠæŒ™ã’ã‚‰ã‚Œã‚‹
- [ ] BERTã¨GPTã®é•ã„ï¼ˆEncoder vs Decoderï¼‰ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] Scaled Dot-Product Attentionã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å› å­ï¼ˆâˆšd_kï¼‰ã®æ„å‘³ã‚’çŸ¥ã£ã¦ã„ã‚‹
- [ ] Attentioné‡ã¿ã®å¯è¦–åŒ–ã‹ã‚‰ä½•ãŒåˆ†ã‹ã‚‹ã‹ç†è§£ã—ã¦ã„ã‚‹
- [ ] ææ–™ç§‘å­¦ã§TransformerãŒæœ‰åŠ¹ãªç†ç”±ã‚’èª¬æ˜ã§ãã‚‹

### æ•°å¼ç†è§£ï¼ˆ5é …ç›®ï¼‰
- [ ] Attention(Q,K,V)ã®æ•°å¼ã‚’æ›¸ã‘ã‚‹
- [ ] Positional Encodingã®æ•°å¼ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] Multi-Head Attentionã®æ•°å¼ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] Softmaxã®æ•°å¼ã¨æ„å‘³ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] è¡Œåˆ—ã®æ¬¡å…ƒï¼ˆshapeï¼‰ã‚’æ­£ã—ãè¨ˆç®—ã§ãã‚‹

### å®Ÿè£…ã‚¹ã‚­ãƒ«ï¼ˆ15é …ç›®ï¼‰
- [ ] `scaled_dot_product_attention`ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] `SelfAttention`ã‚¯ãƒ©ã‚¹ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] `MultiHeadAttention`ã‚¯ãƒ©ã‚¹ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] `PositionalEncoding`ã‚¯ãƒ©ã‚¹ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] Attentioné‡ã¿ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–ã§ãã‚‹
- [ ] ãƒã‚¹ã‚¯ã‚’æ­£ã—ãé©ç”¨ã§ãã‚‹ï¼ˆpadding maskã€causal maskï¼‰
- [ ] PyTorchã®ãƒ†ãƒ³ã‚½ãƒ«æ“ä½œï¼ˆviewã€transposeã€matmulï¼‰ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] `nn.Linear`ã€`nn.Embedding`ã®ä½¿ã„æ–¹ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] ãƒãƒƒãƒå‡¦ç†ã‚’æ­£ã—ãå®Ÿè£…ã§ãã‚‹
- [ ] ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†ï¼ˆCPU/GPUï¼‰ã‚’é©åˆ‡ã«è¡Œãˆã‚‹
- [ ] ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ãŒã§ãã‚‹
- [ ] Gradientã®è¨ˆç®—ã¨é€†ä¼æ’­ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®é©ç”¨ä½ç½®ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] Layer Normalizationã®å½¹å‰²ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] åˆæœŸåŒ–æ–¹æ³•ï¼ˆXavierã€Kaimingãªã©ï¼‰ã‚’é¸æŠã§ãã‚‹

### ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚­ãƒ«ï¼ˆ5é …ç›®ï¼‰
- [ ] ãƒ†ãƒ³ã‚½ãƒ«ã®shapeã‚¨ãƒ©ãƒ¼ã‚’ãƒ‡ãƒãƒƒã‚°ã§ãã‚‹
- [ ] Attentionãƒã‚¹ã‚¯ã®èª¤ã‚Šã‚’æ¤œå‡ºãƒ»ä¿®æ­£ã§ãã‚‹
- [ ] ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼ï¼ˆOOMï¼‰ã®åŸå› ã‚’ç‰¹å®šã§ãã‚‹
- [ ] æ•°å€¤ä¸å®‰å®šæ€§ï¼ˆNaNã€infï¼‰ã‚’æ¤œå‡ºãƒ»å¯¾å‡¦ã§ãã‚‹
- [ ] ä¸­é–“å‡ºåŠ›ã‚’å¯è¦–åŒ–ã—ã¦ãƒã‚°ã‚’ç™ºè¦‹ã§ãã‚‹

### å¿œç”¨åŠ›ï¼ˆ5é …ç›®ï¼‰
- [ ] åˆ†å­ãƒ‡ãƒ¼ã‚¿ï¼ˆSMILESï¼‰ã«Attentionã‚’é©ç”¨ã™ã‚‹æ–¹æ³•ã‚’è€ƒãˆã‚‰ã‚Œã‚‹
- [ ] ææ–™ã®çµ„æˆå¼ã«Transformerã‚’é©ç”¨ã™ã‚‹æ–¹æ³•ã‚’è€ƒãˆã‚‰ã‚Œã‚‹
- [ ] æ—¢å­˜ã®Transformerãƒ¢ãƒ‡ãƒ«ï¼ˆBERTã€GPTï¼‰ã‚’ææ–™ç§‘å­¦ã«é©å¿œã•ã›ã‚‹æˆ¦ç•¥ã‚’ç«‹ã¦ã‚‰ã‚Œã‚‹
- [ ] Attentioné‡ã¿ã‹ã‚‰åŒ–å­¦çš„ã«æ„å‘³ã®ã‚ã‚‹æƒ…å ±ã‚’æŠ½å‡ºã§ãã‚‹
- [ ] æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã«å¿…è¦ãªãƒ¢ãƒ‡ãƒ«ä¿®æ­£ã‚’è¨­è¨ˆã§ãã‚‹

### ç†è«–çš„èƒŒæ™¯ï¼ˆ5é …ç›®ï¼‰
- [ ] Transformerã®å…ƒè«–æ–‡ï¼ˆ"Attention Is All You Need"ï¼‰ã‚’èª­ã‚“ã 
- [ ] BERTã®è«–æ–‡ã‚’èª­ã‚“ã 
- [ ] ææ–™ç§‘å­¦ã§ã®Transformerå¿œç”¨è«–æ–‡ã‚’1æœ¬ä»¥ä¸Šèª­ã‚“ã 
- [ ] è¨ˆç®—é‡ã‚ªãƒ¼ãƒ€ãƒ¼ï¼ˆO(nÂ²)ï¼‰ã®æ„å‘³ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] å¸°ç´ãƒã‚¤ã‚¢ã‚¹ï¼ˆinductive biasï¼‰ã®æ¦‚å¿µã‚’ç†è§£ã—ã¦ã„ã‚‹

### å†ç¾æ€§ï¼ˆ5é …ç›®ï¼‰
- [ ] ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®šã—ã¦å†ç¾æ€§ã‚’ç¢ºä¿ã§ãã‚‹
- [ ] å®Ÿé¨“è¨­å®šã‚’JSONã§ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ã§ãã‚‹
- [ ] ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’ç¢ºèªã—ã¦ã„ã‚‹
- [ ] ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’è¨˜éŒ²ã—ã¦ã„ã‚‹
- [ ] ã‚³ãƒ¼ãƒ‰ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆdocstringï¼‰ã‚’æ›¸ã„ã¦ã„ã‚‹

### å®Œäº†åŸºæº–
- **æœ€ä½åŸºæº–**: 40é …ç›®ä»¥ä¸Šé”æˆï¼ˆ80%ï¼‰
- **æ¨å¥¨åŸºæº–**: 45é …ç›®ä»¥ä¸Šé”æˆï¼ˆ90%ï¼‰
- **å„ªç§€åŸºæº–**: 50é …ç›®å…¨ã¦é”æˆï¼ˆ100%ï¼‰

---

## ğŸ”— å‚è€ƒè³‡æ–™

### è«–æ–‡
- Vaswani et al. (2017) "Attention Is All You Need" [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)
- Devlin et al. (2019) "BERT: Pre-training of Deep Bidirectional Transformers" [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)

### ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [PyTorch Transformer Tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)

### æ¬¡ç« 
**[ç¬¬2ç« : ææ–™å‘ã‘Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](chapter-2.html)** ã§ã€Matformerã€ChemBERTaãªã©ææ–™ç§‘å­¦ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ã³ã¾ã™ã€‚

---

**ä½œæˆè€…**: æ©‹æœ¬ä½‘ä»‹ï¼ˆæ±åŒ—å¤§å­¦ï¼‰
**æœ€çµ‚æ›´æ–°**: 2025å¹´10æœˆ19æ—¥
