<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
        <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/wp/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/MI/index.html">ãƒãƒ†ãƒªã‚¢ãƒ«ã‚ºãƒ»ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/MI/transformer-introduction/index.html">Transformer</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

    <header>
        <div class="header-content">
            <h1>Chapter</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 20-25åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 0å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 0å•</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>ç¬¬3ç« : äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¨è»¢ç§»å­¦ç¿’</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">MatBERT/ChemBERTaãªã©ææ–™ãƒ»åŒ–å­¦ç‰¹åŒ–ã®äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ä½¿ã„ã©ã“ã‚ã‚’æ•´ç†ã—ã¾ã™ã€‚å°‘ãƒ‡ãƒ¼ã‚¿å¾®èª¿æ•´ã®å‹˜æ‰€ã‚’å­¦ã³ã¾ã™ã€‚</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>ğŸ’¡ è£œè¶³:</strong> å‡çµãƒ»éƒ¨åˆ†å‡çµãƒ»å…¨å±¤å­¦ç¿’ã‚’æ¯”è¼ƒã—ã€è¨ˆç®—è³‡æºã¨ç²¾åº¦ã®æœ€é©ç‚¹ã‚’è¦‹ã¤ã‘ã¾ã™ã€‚</p>





<p><strong>å­¦ç¿’æ™‚é–“</strong>: 25-30åˆ† | <strong>é›£æ˜“åº¦</strong>: ä¸­ç´šã€œä¸Šç´š</p>
<h2>ğŸ“‹ ã“ã®ç« ã§å­¦ã¶ã“ã¨</h2>
<ul>
<li>äº‹å‰å­¦ç¿’ï¼ˆPre-trainingï¼‰ã®é‡è¦æ€§ã¨åŸç†</li>
<li>MatBERTã€MolBERTãªã©ææ–™ç§‘å­¦å‘ã‘äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«</li>
<li>ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆFine-tuningï¼‰ã®æˆ¦ç•¥</li>
<li>Few-shotå­¦ç¿’ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</li>
<li>ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œï¼ˆDomain Adaptationï¼‰</li>
</ul>
<hr />
<h2>3.1 äº‹å‰å­¦ç¿’ã®é‡è¦æ€§</h2>
<h3>ãªãœäº‹å‰å­¦ç¿’ãŒå¿…è¦ã‹</h3>
<p><strong>ææ–™ç§‘å­¦ã®èª²é¡Œ</strong>:
- âŒ ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ï¼ˆå®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã¯é«˜ã‚³ã‚¹ãƒˆï¼‰
- âŒ ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®çŸ¥è­˜ãŒå¿…è¦
- âŒ ã‚¼ãƒ­ã‹ã‚‰å­¦ç¿’ã™ã‚‹ã¨æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹</p>
<p><strong>äº‹å‰å­¦ç¿’ã®åˆ©ç‚¹</strong>:
- âœ… å¤§è¦æ¨¡ãª<strong>ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿</strong>ã§ä¸€èˆ¬çš„ãªçŸ¥è­˜ã‚’ç²å¾—
- âœ… å°‘é‡ã®ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã§<strong>é«˜ç²¾åº¦</strong>ã‚’å®Ÿç¾
- âœ… é–‹ç™ºæœŸé–“ã®<strong>å¤§å¹…çŸ­ç¸®</strong>ï¼ˆæ•°é€±é–“â†’æ•°æ™‚é–“ï¼‰</p>
<div class="mermaid">
flowchart LR
    A[å¤§è¦æ¨¡ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿] --> B[äº‹å‰å­¦ç¿’]
    B --> C[æ±ç”¨è¡¨ç¾ãƒ¢ãƒ‡ãƒ«]
    C --> D[ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°]
    E[å°‘é‡ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿] --> D
    D --> F[ã‚¿ã‚¹ã‚¯ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«]

    style B fill:#e1f5ff
    style D fill:#ffe1e1
</div>

<h3>äº‹å‰å­¦ç¿’ã®ã‚¿ã‚¹ã‚¯</h3>
<p><strong>è‡ªç„¶è¨€èªå‡¦ç†ã§ã®ä¾‹</strong>:
- <strong>Masked Language Model (MLM)</strong>: ä¸€éƒ¨ã®å˜èªã‚’ãƒã‚¹ã‚¯ã—ã¦äºˆæ¸¬
- <strong>Next Sentence Prediction (NSP)</strong>: 2æ–‡ã®é€£ç¶šæ€§ã‚’äºˆæ¸¬</p>
<p><strong>ææ–™ç§‘å­¦ã§ã®å¿œç”¨</strong>:
- <strong>Masked Atom Prediction</strong>: ä¸€éƒ¨ã®åŸå­ã‚’ãƒã‚¹ã‚¯ã—ã¦äºˆæ¸¬
- <strong>Property Prediction</strong>: è¤‡æ•°ã®ææ–™ç‰¹æ€§ã‚’åŒæ™‚äºˆæ¸¬
- <strong>Contrastive Learning</strong>: é¡ä¼¼ææ–™ã‚’è¿‘ãã€ç•°ãªã‚‹ææ–™ã‚’é ãã«é…ç½®</p>
<hr />
<h2>3.2 MatBERT: Materials BERT</h2>
<h3>æ¦‚è¦</h3>
<p><strong>MatBERT</strong>ã¯ã€ææ–™ã®çµ„æˆå¼ã‚’BERTã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</p>
<p><strong>ç‰¹å¾´</strong>:
- <strong>500kææ–™</strong>ã®çµ„æˆå¼ã§äº‹å‰å­¦ç¿’
- <strong>ãƒã‚¹ã‚¯åŸå­äºˆæ¸¬</strong>ã‚¿ã‚¹ã‚¯
- è»¢ç§»å­¦ç¿’ã§æ§˜ã€…ãªç‰¹æ€§äºˆæ¸¬ã«é©ç”¨å¯èƒ½</p>
<h3>çµ„æˆå¼ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
from transformers import BertTokenizer, BertModel

class CompositionTokenizer:
    def __init__(self):
        # ã‚«ã‚¹ã‚¿ãƒ èªå½™ï¼ˆå‘¨æœŸè¡¨ã®å…ƒç´ ï¼‰
        self.vocab = ['[PAD]', '[CLS]', '[SEP]', '[MASK]'] + [
            'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne',
            'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'Ar', 'K', 'Ca',
            # ... å…¨å…ƒç´ 
        ]
        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}
        self.id_to_token = {i: token for i, token in enumerate(self.vocab)}

    def tokenize(self, composition):
        &quot;&quot;&quot;
        çµ„æˆå¼ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–

        Args:
            composition: 'Fe2O3' ã®ã‚ˆã†ãªçµ„æˆå¼
        Returns:
            tokens: ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆ
        &quot;&quot;&quot;
        import re
        # å…ƒç´ ã¨æ•°å­—ã‚’åˆ†å‰²
        pattern = r'([A-Z][a-z]?)(\d*\.?\d*)'
        matches = re.findall(pattern, composition)

        tokens = ['[CLS]']
        for element, count in matches:
            if element in self.vocab:
                # å…ƒç´ ã‚’è¿½åŠ 
                tokens.append(element)
                # æ•°ãŒ1ã‚ˆã‚Šå¤§ãã„å ´åˆã€ãã®å›æ•°ã ã‘ç¹°ã‚Šè¿”ã™ï¼ˆç°¡ç•¥åŒ–ï¼‰
                if count and float(count) &gt; 1:
                    for _ in range(int(float(count)) - 1):
                        tokens.append(element)
        tokens.append('[SEP]')

        return tokens

    def encode(self, compositions, max_length=32):
        &quot;&quot;&quot;
        çµ„æˆå¼ã‚’IDã«å¤‰æ›

        Args:
            compositions: çµ„æˆå¼ã®ãƒªã‚¹ãƒˆ
            max_length: æœ€å¤§é•·
        Returns:
            input_ids: (batch_size, max_length)
            attention_mask: (batch_size, max_length)
        &quot;&quot;&quot;
        batch_input_ids = []
        batch_attention_mask = []

        for comp in compositions:
            tokens = self.tokenize(comp)
            ids = [self.token_to_id.get(token, 0) for token in tokens]

            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            attention_mask = [1] * len(ids)
            while len(ids) &lt; max_length:
                ids.append(0)  # [PAD]
                attention_mask.append(0)

            # ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ã‚·ãƒ§ãƒ³
            ids = ids[:max_length]
            attention_mask = attention_mask[:max_length]

            batch_input_ids.append(ids)
            batch_attention_mask.append(attention_mask)

        return torch.tensor(batch_input_ids), torch.tensor(batch_attention_mask)

# ä½¿ç”¨ä¾‹
tokenizer = CompositionTokenizer()

compositions = [
    'Fe2O3',     # é…¸åŒ–é‰„
    'LiCoO2',    # ãƒªãƒã‚¦ãƒ ã‚³ãƒãƒ«ãƒˆé…¸åŒ–ç‰©ï¼ˆé›»æ± ææ–™ï¼‰
    'BaTiO3'     # ãƒã‚¿ãƒ³é…¸ãƒãƒªã‚¦ãƒ ï¼ˆèª˜é›»ä½“ï¼‰
]

input_ids, attention_mask = tokenizer.encode(compositions)
print(f&quot;Input IDs shape: {input_ids.shape}&quot;)
print(f&quot;First composition tokens: {input_ids[0][:10]}&quot;)
</code></pre>
<h3>MatBERTãƒ¢ãƒ‡ãƒ«</h3>
<pre><code class="language-python">class MatBERT(nn.Module):
    def __init__(self, vocab_size, d_model=512, num_layers=6, num_heads=8):
        super(MatBERT, self).__init__()

        # Embedding
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(512, d_model)

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=num_heads,
            dim_feedforward=2048,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)

        self.d_model = d_model

    def forward(self, input_ids, attention_mask):
        &quot;&quot;&quot;
        Args:
            input_ids: (batch_size, seq_len)
            attention_mask: (batch_size, seq_len)
        Returns:
            embeddings: (batch_size, seq_len, d_model)
        &quot;&quot;&quot;
        batch_size, seq_len = input_ids.shape

        # Token embedding
        token_embeddings = self.embedding(input_ids)

        # Positional embedding
        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)
        position_embeddings = self.position_embedding(positions)

        # åˆè¨ˆ
        embeddings = token_embeddings + position_embeddings

        # Transformer
        # attention_maskã‚’Transformerç”¨ã«å¤‰æ›ï¼ˆ0â†’-inf, 1â†’0ï¼‰
        transformer_mask = (1 - attention_mask).bool()
        output = self.transformer_encoder(embeddings, src_key_padding_mask=transformer_mask)

        return output

# ä½¿ç”¨ä¾‹
vocab_size = len(tokenizer.vocab)
model = MatBERT(vocab_size, d_model=512, num_layers=6, num_heads=8)

embeddings = model(input_ids, attention_mask)
print(f&quot;Embeddings shape: {embeddings.shape}&quot;)  # (3, 32, 512)
</code></pre>
<h3>äº‹å‰å­¦ç¿’: Masked Atom Prediction</h3>
<pre><code class="language-python">def masked_atom_prediction_loss(model, input_ids, attention_mask, mask_prob=0.15):
    &quot;&quot;&quot;
    ãƒã‚¹ã‚¯åŸå­äºˆæ¸¬ã«ã‚ˆã‚‹äº‹å‰å­¦ç¿’

    Args:
        model: MatBERTãƒ¢ãƒ‡ãƒ«
        input_ids: (batch_size, seq_len)
        attention_mask: (batch_size, seq_len)
        mask_prob: ãƒã‚¹ã‚¯ã™ã‚‹ç¢ºç‡
    Returns:
        loss: æå¤±
    &quot;&quot;&quot;
    batch_size, seq_len = input_ids.shape

    # ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒã‚¹ã‚¯
    mask_token_id = tokenizer.token_to_id['[MASK]']
    mask = torch.rand(batch_size, seq_len) &lt; mask_prob
    mask = mask &amp; (attention_mask == 1)  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°éƒ¨åˆ†ã¯é™¤å¤–

    # å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿å­˜
    original_input_ids = input_ids.clone()

    # ãƒã‚¹ã‚¯ã‚’é©ç”¨
    input_ids[mask] = mask_token_id

    # Forward
    embeddings = model(input_ids, attention_mask)

    # äºˆæ¸¬ãƒ˜ãƒƒãƒ‰
    prediction_head = nn.Linear(model.d_model, vocab_size)
    logits = prediction_head(embeddings)

    # æå¤±è¨ˆç®—ï¼ˆãƒã‚¹ã‚¯ã•ã‚ŒãŸä½ç½®ã®ã¿ï¼‰
    criterion = nn.CrossEntropyLoss(ignore_index=-100)
    labels = original_input_ids.clone()
    labels[~mask] = -100  # ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ãªã„éƒ¨åˆ†ã¯ç„¡è¦–

    loss = criterion(logits.view(-1, vocab_size), labels.view(-1))

    return loss

# äº‹å‰å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆç°¡ç•¥ç‰ˆï¼‰
def pretrain_matbert(model, dataloader, epochs=10):
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for input_ids, attention_mask in dataloader:
            loss = masked_atom_prediction_loss(model, input_ids, attention_mask)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        print(f&quot;Epoch {epoch+1}, Pretraining Loss: {avg_loss:.4f}&quot;)

    return model
</code></pre>
<hr />
<h2>3.3 ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥</h2>
<h3>ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã¯</h3>
<p><strong>å®šç¾©</strong>: äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®šã‚¿ã‚¹ã‚¯ã«é©å¿œã•ã›ã‚‹è¿½åŠ å­¦ç¿’</p>
<p><strong>æˆ¦ç•¥</strong>:
1. <strong>Full Fine-tuning</strong>: ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°
2. <strong>Feature Extraction</strong>: åŸ‹ã‚è¾¼ã¿å±¤ã®ã¿ä½¿ç”¨ã€äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ã®ã¿å­¦ç¿’
3. <strong>Partial Fine-tuning</strong>: ä¸€éƒ¨ã®å±¤ã®ã¿æ›´æ–°</p>
<div class="mermaid">
flowchart TD
    A[äº‹å‰å­¦ç¿’æ¸ˆã¿MatBERT] --> B{ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥}
    B --> C[Full Fine-tuning]
    B --> D[Feature Extraction]
    B --> E[Partial Fine-tuning]

    C --> F[å…¨å±¤ã‚’æ›´æ–°]
    D --> G[åŸ‹ã‚è¾¼ã¿å›ºå®šã€äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ã®ã¿å­¦ç¿’]
    E --> H[ä¸Šä½å±¤ã®ã¿æ›´æ–°]

    style C fill:#ffe1e1
    style D fill:#e1f5ff
    style E fill:#f5ffe1
</div>

<h3>å®Ÿè£…: ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬</h3>
<pre><code class="language-python">class MatBERTForBandgap(nn.Module):
    def __init__(self, matbert_model, d_model=512):
        super(MatBERTForBandgap, self).__init__()
        self.matbert = matbert_model

        # äºˆæ¸¬ãƒ˜ãƒƒãƒ‰
        self.bandgap_predictor = nn.Sequential(
            nn.Linear(d_model, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 1)
        )

    def forward(self, input_ids, attention_mask):
        # MatBERTåŸ‹ã‚è¾¼ã¿
        embeddings = self.matbert(input_ids, attention_mask)

        # [CLS]ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨
        cls_embedding = embeddings[:, 0, :]

        # ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬
        bandgap = self.bandgap_predictor(cls_embedding)
        return bandgap

# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
def finetune_for_bandgap(pretrained_model, train_loader, val_loader, strategy='full'):
    &quot;&quot;&quot;
    ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬ã¸ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

    Args:
        pretrained_model: äº‹å‰å­¦ç¿’æ¸ˆã¿MatBERT
        train_loader: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        val_loader: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        strategy: 'full', 'feature', 'partial'
    &quot;&quot;&quot;
    model = MatBERTForBandgap(pretrained_model)

    # æˆ¦ç•¥ã«å¿œã˜ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å›ºå®š
    if strategy == 'feature':
        # MatBERTã‚’å›ºå®š
        for param in model.matbert.parameters():
            param.requires_grad = False
    elif strategy == 'partial':
        # ä¸‹ä½å±¤ã‚’å›ºå®šã€ä¸Šä½å±¤ã®ã¿æ›´æ–°
        for i, layer in enumerate(model.matbert.transformer_encoder.layers):
            if i &lt; 3:  # ä¸‹ä½3å±¤ã‚’å›ºå®š
                for param in layer.parameters():
                    param.requires_grad = False

    # æœ€é©åŒ–
    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)
    criterion = nn.MSELoss()

    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    best_val_loss = float('inf')
    for epoch in range(20):
        model.train()
        train_loss = 0
        for input_ids, attention_mask, bandgaps in train_loader:
            predictions = model(input_ids, attention_mask)
            loss = criterion(predictions, bandgaps)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        # æ¤œè¨¼
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for input_ids, attention_mask, bandgaps in val_loader:
                predictions = model(input_ids, attention_mask)
                loss = criterion(predictions, bandgaps)
                val_loss += loss.item()

        train_loss /= len(train_loader)
        val_loss /= len(val_loader)

        print(f&quot;Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}&quot;)

        if val_loss &lt; best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), 'best_matbert_bandgap.pt')

    return model
</code></pre>
<hr />
<h2>3.4 Few-shotå­¦ç¿’</h2>
<h3>æ¦‚è¦</h3>
<p><strong>Few-shotå­¦ç¿’</strong>: å°‘é‡ã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæ•°å€‹ã€œæ•°åå€‹ï¼‰ã§æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’å­¦ç¿’</p>
<p><strong>ææ–™ç§‘å­¦ã§ã®é‡è¦æ€§</strong>:
- æ–°è¦ææ–™ã®ãƒ‡ãƒ¼ã‚¿ã¯éå¸¸ã«å°‘ãªã„
- å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¯é«˜ã‚³ã‚¹ãƒˆ
- è¿…é€Ÿãªãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°ãŒå¿…è¦</p>
<h3>Prototypical Networks</h3>
<pre><code class="language-python">class PrototypicalNetwork(nn.Module):
    def __init__(self, matbert_model, d_model=512):
        super(PrototypicalNetwork, self).__init__()
        self.encoder = matbert_model

    def forward(self, support_ids, support_mask, query_ids, query_mask, support_labels):
        &quot;&quot;&quot;
        Prototypical Networksã«ã‚ˆã‚‹åˆ†é¡

        Args:
            support_ids: ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆå…¥åŠ› (n_support, seq_len)
            support_mask: ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆãƒã‚¹ã‚¯
            query_ids: ã‚¯ã‚¨ãƒªå…¥åŠ› (n_query, seq_len)
            query_mask: ã‚¯ã‚¨ãƒªãƒã‚¹ã‚¯
            support_labels: ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆãƒ©ãƒ™ãƒ« (n_support,)
        Returns:
            predictions: ã‚¯ã‚¨ãƒªã®äºˆæ¸¬ãƒ©ãƒ™ãƒ«
        &quot;&quot;&quot;
        # ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆã¨ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿
        support_embeddings = self.encoder(support_ids, support_mask)[:, 0, :]  # [CLS]
        query_embeddings = self.encoder(query_ids, query_mask)[:, 0, :]

        # å„ã‚¯ãƒ©ã‚¹ã®ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆå¹³å‡åŸ‹ã‚è¾¼ã¿ï¼‰ã‚’è¨ˆç®—
        unique_labels = torch.unique(support_labels)
        prototypes = []
        for label in unique_labels:
            mask = (support_labels == label)
            prototype = support_embeddings[mask].mean(dim=0)
            prototypes.append(prototype)

        prototypes = torch.stack(prototypes)  # (num_classes, d_model)

        # ã‚¯ã‚¨ãƒªã¨ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—é–“ã®è·é›¢
        distances = torch.cdist(query_embeddings, prototypes)  # (n_query, num_classes)

        # æœ€ã‚‚è¿‘ã„ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã®ã‚¯ãƒ©ã‚¹ã‚’äºˆæ¸¬
        predictions = torch.argmin(distances, dim=1)

        return predictions

# ä½¿ç”¨ä¾‹: 3-way 5-shotåˆ†é¡
# 3ã‚¯ãƒ©ã‚¹ã€å„ã‚¯ãƒ©ã‚¹5ã‚µãƒ³ãƒ—ãƒ«
n_classes = 3
n_support_per_class = 5
n_query = 10

support_ids = torch.randint(0, vocab_size, (n_classes * n_support_per_class, 32))
support_mask = torch.ones_like(support_ids)
support_labels = torch.arange(n_classes).repeat_interleave(n_support_per_class)

query_ids = torch.randint(0, vocab_size, (n_query, 32))
query_mask = torch.ones_like(query_ids)

proto_net = PrototypicalNetwork(model)
predictions = proto_net(support_ids, support_mask, query_ids, query_mask, support_labels)
print(f&quot;Predictions: {predictions}&quot;)
</code></pre>
<hr />
<h2>3.5 ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</h2>
<h3>ææ–™ç§‘å­¦ã§ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ</h3>
<p><strong>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ</strong>: ãƒ¢ãƒ‡ãƒ«ã«è¿½åŠ æƒ…å ±ã‚’ä¸ãˆã¦æ€§èƒ½ã‚’å‘ä¸Š</p>
<p><strong>ä¾‹</strong>:</p>
<pre><code class="language-python"># é€šå¸¸: 'Fe2O3'
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»˜ã: '[OXIDE] Fe2O3 [BANDGAP]'
</code></pre>
<h3>å®Ÿè£…</h3>
<pre><code class="language-python">class PromptedMatBERT(nn.Module):
    def __init__(self, matbert_model, d_model=512):
        super(PromptedMatBERT, self).__init__()
        self.matbert = matbert_model

        # ã‚¿ã‚¹ã‚¯åˆ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåŸ‹ã‚è¾¼ã¿ï¼ˆå­¦ç¿’å¯èƒ½ï¼‰
        self.task_prompts = nn.Parameter(torch.randn(10, d_model))  # 10ç¨®é¡ã®ã‚¿ã‚¹ã‚¯

    def forward(self, input_ids, attention_mask, task_id=0):
        &quot;&quot;&quot;
        Args:
            input_ids: (batch_size, seq_len)
            attention_mask: (batch_size, seq_len)
            task_id: ã‚¿ã‚¹ã‚¯ID (0-9)
        &quot;&quot;&quot;
        batch_size = input_ids.size(0)

        # é€šå¸¸ã®åŸ‹ã‚è¾¼ã¿
        embeddings = self.matbert(input_ids, attention_mask)

        # ã‚¿ã‚¹ã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…ˆé ­ã«è¿½åŠ 
        task_prompt = self.task_prompts[task_id].unsqueeze(0).expand(batch_size, -1, -1)
        embeddings = torch.cat([task_prompt, embeddings], dim=1)

        return embeddings

# ä½¿ç”¨ä¾‹
prompted_model = PromptedMatBERT(model)

# ã‚¿ã‚¹ã‚¯0: ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬
embeddings_task0 = prompted_model(input_ids, attention_mask, task_id=0)

# ã‚¿ã‚¹ã‚¯1: å½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼äºˆæ¸¬
embeddings_task1 = prompted_model(input_ids, attention_mask, task_id=1)

print(f&quot;Embeddings with prompt shape: {embeddings_task0.shape}&quot;)
</code></pre>
<hr />
<h2>3.6 ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œ</h2>
<h3>æ¦‚è¦</h3>
<p><strong>ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œ</strong>: ã‚½ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ã§è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³ã«é©å¿œ</p>
<p><strong>ä¾‹</strong>:
- ã‚½ãƒ¼ã‚¹: ç„¡æ©Ÿææ–™ãƒ‡ãƒ¼ã‚¿
- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: æœ‰æ©Ÿåˆ†å­ãƒ‡ãƒ¼ã‚¿</p>
<h3>Adversarial Domain Adaptation</h3>
<pre><code class="language-python">class DomainClassifier(nn.Module):
    def __init__(self, d_model=512):
        super(DomainClassifier, self).__init__()
        self.classifier = nn.Sequential(
            nn.Linear(d_model, 256),
            nn.ReLU(),
            nn.Linear(256, 2)  # ã‚½ãƒ¼ã‚¹ or ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
        )

    def forward(self, embeddings):
        return self.classifier(embeddings)

class DomainAdaptiveMatBERT(nn.Module):
    def __init__(self, matbert_model):
        super(DomainAdaptiveMatBERT, self).__init__()
        self.matbert = matbert_model
        self.domain_classifier = DomainClassifier()
        self.task_predictor = nn.Linear(512, 1)  # ä¾‹: ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬

    def forward(self, input_ids, attention_mask, alpha=1.0):
        &quot;&quot;&quot;
        Args:
            alpha: ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œã®å¼·ã•
        &quot;&quot;&quot;
        embeddings = self.matbert(input_ids, attention_mask)[:, 0, :]

        # ã‚¿ã‚¹ã‚¯äºˆæ¸¬
        task_output = self.task_predictor(embeddings)

        # ãƒ‰ãƒ¡ã‚¤ãƒ³äºˆæ¸¬ï¼ˆå‹¾é…åè»¢å±¤ã‚’ä½¿ç”¨ï¼‰
        # ã“ã“ã§ã¯ç°¡ç•¥åŒ–ã®ãŸã‚çœç•¥
        domain_output = self.domain_classifier(embeddings)

        return task_output, domain_output

# è¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆç°¡ç•¥ç‰ˆï¼‰
def train_domain_adaptive(model, source_loader, target_loader, epochs=20):
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
    task_criterion = nn.MSELoss()
    domain_criterion = nn.CrossEntropyLoss()

    for epoch in range(epochs):
        for (source_ids, source_mask, source_labels), (target_ids, target_mask, _) in zip(source_loader, target_loader):
            # ã‚½ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³
            source_task, source_domain = model(source_ids, source_mask)
            source_domain_labels = torch.zeros(source_ids.size(0), dtype=torch.long)  # ã‚½ãƒ¼ã‚¹ = 0

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³
            target_task, target_domain = model(target_ids, target_mask)
            target_domain_labels = torch.ones(target_ids.size(0), dtype=torch.long)  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ = 1

            # æå¤±
            task_loss = task_criterion(source_task, source_labels)
            domain_loss = domain_criterion(source_domain, source_domain_labels) + \
                          domain_criterion(target_domain, target_domain_labels)

            total_loss = task_loss + 0.1 * domain_loss

            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()

        print(f&quot;Epoch {epoch+1}, Task Loss: {task_loss.item():.4f}, Domain Loss: {domain_loss.item():.4f}&quot;)
</code></pre>
<hr />
<h2>3.7 ã¾ã¨ã‚</h2>
<h3>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</h3>
<ol>
<li><strong>äº‹å‰å­¦ç¿’</strong>: å¤§è¦æ¨¡ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã§ä¸€èˆ¬çš„çŸ¥è­˜ã‚’ç²å¾—</li>
<li><strong>ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</strong>: å°‘é‡ãƒ‡ãƒ¼ã‚¿ã§ã‚¿ã‚¹ã‚¯ç‰¹åŒ–</li>
<li><strong>Few-shotå­¦ç¿’</strong>: æ•°å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã§æ–°ã‚¿ã‚¹ã‚¯å­¦ç¿’</li>
<li><strong>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</strong>: ã‚¿ã‚¹ã‚¯æƒ…å ±ã‚’åŸ‹ã‚è¾¼ã¿ã§è¡¨ç¾</li>
<li><strong>ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œ</strong>: ç•°ãªã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³é–“ã§çŸ¥è­˜è»¢ç§»</li>
</ol>
<h3>æ¬¡ç« ã¸ã®æº–å‚™</h3>
<p>ç¬¬4ç« ã§ã¯ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹åˆ†å­ç”Ÿæˆã¨ææ–™é€†è¨­è¨ˆã‚’å­¦ã³ã¾ã™ã€‚</p>
<hr />
<h2>ğŸ“ æ¼”ç¿’å•é¡Œ</h2>
<h3>å•é¡Œ1: æ¦‚å¿µç†è§£</h3>
<p>ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®3ã¤ã®æˆ¦ç•¥ï¼ˆFullã€Feature Extractionã€Partialï¼‰ã«ã¤ã„ã¦ã€ãã‚Œãã‚Œã©ã®ã‚ˆã†ãªå ´åˆã«é©ã—ã¦ã„ã‚‹ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>
<details>
<summary>è§£ç­”ä¾‹</summary>

1. **Full Fine-tuning**:
   - **é©ç”¨å ´é¢**: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ãŒæ¯”è¼ƒçš„å¤šã„ï¼ˆæ•°åƒã‚µãƒ³ãƒ—ãƒ«ä»¥ä¸Šï¼‰
   - **åˆ©ç‚¹**: æœ€é«˜ç²¾åº¦ã‚’é”æˆå¯èƒ½
   - **æ¬ ç‚¹**: éå­¦ç¿’ãƒªã‚¹ã‚¯ã€è¨ˆç®—ã‚³ã‚¹ãƒˆå¤§

2. **Feature Extraction**:
   - **é©ç”¨å ´é¢**: ãƒ‡ãƒ¼ã‚¿ãŒéå¸¸ã«å°‘ãªã„ï¼ˆæ•°åã€œæ•°ç™¾ã‚µãƒ³ãƒ—ãƒ«ï¼‰
   - **åˆ©ç‚¹**: éå­¦ç¿’ã‚’é˜²ãã‚„ã™ã„ã€é«˜é€Ÿ
   - **æ¬ ç‚¹**: ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒå¤§ããç•°ãªã‚‹å ´åˆã¯ç²¾åº¦ä½ä¸‹

3. **Partial Fine-tuning**:
   - **é©ç”¨å ´é¢**: ä¸­ç¨‹åº¦ã®ãƒ‡ãƒ¼ã‚¿é‡ã€ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒé¡ä¼¼
   - **åˆ©ç‚¹**: ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸæ€§èƒ½ã¨ã‚³ã‚¹ãƒˆ
   - **æ¬ ç‚¹**: ã©ã®å±¤ã‚’æ›´æ–°ã™ã‚‹ã‹é¸æŠãŒé›£ã—ã„
</details>

<h3>å•é¡Œ2: å®Ÿè£…</h3>
<p>ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã®ç©ºæ¬„ã‚’åŸ‹ã‚ã¦ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹é–¢æ•°ã‚’å®Œæˆã•ã›ã¦ãã ã•ã„ã€‚</p>
<pre><code class="language-python">def load_and_finetune(pretrained_path, train_loader, val_loader):
    # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    matbert = MatBERT(vocab_size=______, d_model=512)
    matbert.load_state_dict(torch.load(______))

    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰
    model = MatBERTForBandgap(______)

    # æœ€é©åŒ–
    optimizer = torch.optim.Adam(______.parameters(), lr=1e-5)
    criterion = nn.MSELoss()

    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    for epoch in range(10):
        model.train()
        for input_ids, attention_mask, targets in train_loader:
            predictions = model(______, ______)
            loss = ______(predictions, targets)

            optimizer.zero_grad()
            ______.backward()
            optimizer.step()

    return model
</code></pre>
<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python">def load_and_finetune(pretrained_path, train_loader, val_loader):
    # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    matbert = MatBERT(vocab_size=len(tokenizer.vocab), d_model=512)
    matbert.load_state_dict(torch.load(pretrained_path))

    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰
    model = MatBERTForBandgap(matbert)

    # æœ€é©åŒ–
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
    criterion = nn.MSELoss()

    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    for epoch in range(10):
        model.train()
        for input_ids, attention_mask, targets in train_loader:
            predictions = model(input_ids, attention_mask)
            loss = criterion(predictions, targets)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    return model
</code></pre>

</details>

<h3>å•é¡Œ3: å¿œç”¨</h3>
<p>ææ–™ç§‘å­¦ã§ Few-shotå­¦ç¿’ãŒç‰¹ã«æœ‰ç”¨ãª3ã¤ã®ã‚·ãƒŠãƒªã‚ªã‚’æŒ™ã’ã€ãã‚Œãã‚Œã®ç†ç”±ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>
<details>
<summary>è§£ç­”ä¾‹</summary>

1. **æ–°è¦ææ–™ã®è¿…é€Ÿè©•ä¾¡**:
   - **ã‚·ãƒŠãƒªã‚ª**: æ–°ã—ã„ã‚¯ãƒ©ã‚¹ã®ææ–™ï¼ˆä¾‹: æ–°å‹ãƒšãƒ­ãƒ–ã‚¹ã‚«ã‚¤ãƒˆï¼‰
   - **ç†ç”±**: å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ãŒã¾ã å°‘ãªãã€æ•°ã‚µãƒ³ãƒ—ãƒ«ã§ç‰¹æ€§äºˆæ¸¬ãŒå¿…è¦

2. **å®Ÿé¨“è¨ˆç”»ã®åŠ¹ç‡åŒ–**:
   - **ã‚·ãƒŠãƒªã‚ª**: é«˜ã‚³ã‚¹ãƒˆãªå®Ÿé¨“ï¼ˆå˜çµæ™¶æˆé•·ã€é«˜åœ§åˆæˆï¼‰
   - **ç†ç”±**: å°‘æ•°ã®å®Ÿé¨“çµæœã‹ã‚‰æ¬¡ã®å®Ÿé¨“æ¡ä»¶ã‚’ææ¡ˆ

3. **ä¼æ¥­ã®ç‹¬è‡ªææ–™é–‹ç™º**:
   - **ã‚·ãƒŠãƒªã‚ª**: ç«¶åˆã«å…¬é–‹ã§ããªã„ç‹¬è‡ªææ–™
   - **ç†ç”±**: ç¤¾å†…ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§å­¦ç¿’ã€å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã¯ä½¿ãˆãªã„
</details>

<hr />
<h2>ğŸš€ å®Ÿè£…æ¼”ç¿’: Transformer for Materials</h2>
<h3>æ¼”ç¿’1: MatBERTå®Ÿè£…ï¼ˆBERT for Materialsï¼‰</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
from transformers import BertConfig, BertModel

class MaterialsBERT(nn.Module):
    def __init__(self, vocab_size=120, d_model=768, num_layers=12, num_heads=12):
        &quot;&quot;&quot;
        Materials BERT implementation

        Args:
            vocab_size: åŸå­ç¨®æ•° + ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
            d_model: éš ã‚Œå±¤æ¬¡å…ƒ
            num_layers: Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
            num_heads: Attentionãƒ˜ãƒƒãƒ‰æ•°
        &quot;&quot;&quot;
        super().__init__()

        # BERT configuration
        config = BertConfig(
            vocab_size=vocab_size,
            hidden_size=d_model,
            num_hidden_layers=num_layers,
            num_attention_heads=num_heads,
            intermediate_size=d_model * 4,
            hidden_dropout_prob=0.1,
            attention_probs_dropout_prob=0.1,
            max_position_embeddings=512
        )

        self.bert = BertModel(config)

    def forward(self, input_ids, attention_mask=None, token_type_ids=None):
        &quot;&quot;&quot;
        Args:
            input_ids: (batch_size, seq_len) åŸå­ç•ªå·ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
            attention_mask: (batch_size, seq_len)
            token_type_ids: (batch_size, seq_len)
        Returns:
            outputs: BERT outputs with pooler_output
        &quot;&quot;&quot;
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids
        )

        return outputs

# ä½¿ç”¨ä¾‹
mat_bert = MaterialsBERT(vocab_size=120, d_model=768)

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿: Fe2O3 (é…¸åŒ–é‰„)
# [CLS] Fe Fe O O O [SEP]
input_ids = torch.tensor([[101, 26, 26, 8, 8, 8, 102]])  # 101=[CLS], 102=[SEP]
attention_mask = torch.ones_like(input_ids)

outputs = mat_bert(input_ids, attention_mask)
print(f&quot;Last hidden state shape: {outputs.last_hidden_state.shape}&quot;)  # (1, 7, 768)
print(f&quot;Pooler output shape: {outputs.pooler_output.shape}&quot;)  # (1, 768)
</code></pre>
<h3>æ¼”ç¿’2: MatGPTå®Ÿè£…ï¼ˆGPT for Materials Generationï¼‰</h3>
<pre><code class="language-python">from transformers import GPT2Config, GPT2LMHeadModel

class MaterialsGPT(nn.Module):
    def __init__(self, vocab_size=120, d_model=768, num_layers=12, num_heads=12):
        &quot;&quot;&quot;
        Materials GPT for generative tasks

        Args:
            vocab_size: åŸå­ç¨®æ•° + ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
            d_model: éš ã‚Œå±¤æ¬¡å…ƒ
            num_layers: Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
            num_heads: Attentionãƒ˜ãƒƒãƒ‰æ•°
        &quot;&quot;&quot;
        super().__init__()

        config = GPT2Config(
            vocab_size=vocab_size,
            n_positions=512,
            n_embd=d_model,
            n_layer=num_layers,
            n_head=num_heads,
            resid_pdrop=0.1,
            embd_pdrop=0.1,
            attn_pdrop=0.1
        )

        self.gpt = GPT2LMHeadModel(config)

    def forward(self, input_ids, labels=None):
        &quot;&quot;&quot;
        Args:
            input_ids: (batch_size, seq_len)
            labels: (batch_size, seq_len) for training
        &quot;&quot;&quot;
        outputs = self.gpt(input_ids=input_ids, labels=labels)
        return outputs

    def generate_composition(self, start_tokens, max_length=50, temperature=1.0):
        &quot;&quot;&quot;
        çµ„æˆå¼ç”Ÿæˆ

        Args:
            start_tokens: (1, start_len) é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³
            max_length: æœ€å¤§ç”Ÿæˆé•·
            temperature: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦
        &quot;&quot;&quot;
        self.eval()
        with torch.no_grad():
            for _ in range(max_length - start_tokens.size(1)):
                outputs = self.gpt(start_tokens)
                logits = outputs.logits[:, -1, :] / temperature

                probs = torch.softmax(logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)

                start_tokens = torch.cat([start_tokens, next_token], dim=1)

                # [SEP]ãƒˆãƒ¼ã‚¯ãƒ³ã§åœæ­¢
                if next_token.item() == 102:
                    break

        return start_tokens

# ä½¿ç”¨ä¾‹
mat_gpt = MaterialsGPT(vocab_size=120, d_model=768)

# ç”Ÿæˆ: [CLS] Fe ... (é…¸åŒ–ç‰©ã‚’ç”Ÿæˆ)
start = torch.tensor([[101, 26]])  # [CLS] Fe
generated = mat_gpt.generate_composition(start, max_length=20)
print(f&quot;Generated sequence: {generated}&quot;)
</code></pre>
<h3>æ¼”ç¿’3: MatT5å®Ÿè£…ï¼ˆT5 for Materials Seq2Seqï¼‰</h3>
<pre><code class="language-python">from transformers import T5Config, T5ForConditionalGeneration

class MaterialsT5(nn.Module):
    def __init__(self, vocab_size=120, d_model=512, num_layers=6):
        &quot;&quot;&quot;
        Materials T5 for sequence-to-sequence tasks
        (e.g., composition â†’ properties description)

        Args:
            vocab_size: èªå½™ã‚µã‚¤ã‚º
            d_model: ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ
            num_layers: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€å±¤æ•°
        &quot;&quot;&quot;
        super().__init__()

        config = T5Config(
            vocab_size=vocab_size,
            d_model=d_model,
            d_kv=64,
            d_ff=d_model * 4,
            num_layers=num_layers,
            num_decoder_layers=num_layers,
            num_heads=8,
            dropout_rate=0.1
        )

        self.t5 = T5ForConditionalGeneration(config)

    def forward(self, input_ids, attention_mask=None, labels=None):
        &quot;&quot;&quot;
        Args:
            input_ids: (batch_size, src_len) å…¥åŠ›ç³»åˆ—
            labels: (batch_size, tgt_len) ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç³»åˆ—
        &quot;&quot;&quot;
        outputs = self.t5(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        return outputs

    def predict_properties(self, composition_ids, max_length=50):
        &quot;&quot;&quot;
        çµ„æˆå¼ã‹ã‚‰ç‰¹æ€§è¨˜è¿°ã‚’ç”Ÿæˆ

        Args:
            composition_ids: (batch_size, seq_len) çµ„æˆå¼
            max_length: æœ€å¤§ç”Ÿæˆé•·
        &quot;&quot;&quot;
        self.eval()
        with torch.no_grad():
            outputs = self.t5.generate(
                composition_ids,
                max_length=max_length,
                num_beams=4,
                early_stopping=True
            )
        return outputs

# ä½¿ç”¨ä¾‹
mat_t5 = MaterialsT5(vocab_size=120, d_model=512)

# å…¥åŠ›: Fe2O3 â†’ å‡ºåŠ›: &quot;semiconductor bandgap 2.0 eV&quot;
input_ids = torch.tensor([[26, 26, 8, 8, 8]])  # Fe Fe O O O
outputs = mat_t5.predict_properties(input_ids, max_length=20)
print(f&quot;Predicted properties: {outputs}&quot;)
</code></pre>
<hr />
<h2>ğŸ§ª SMILES/SELFIES ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®å®Ÿè£…</h2>
<h3>SMILES Tokenizer</h3>
<pre><code class="language-python">import re
from typing import List, Dict

class SMILESTokenizer:
    &quot;&quot;&quot;
    SMILESæ–‡å­—åˆ—ã®å®Œå…¨ãƒˆãƒ¼ã‚¯ãƒ³åŒ–

    å¯¾å¿œ:
    - èŠ³é¦™æ—æ€§ (c, n, o, s)
    - ç«‹ä½“åŒ–å­¦ (@, @@, /, \\)
    - åˆ†å² ((, ))
    - çµåˆ (-, =, #, :)
    - ç’° (æ•°å­—)
    &quot;&quot;&quot;

    def __init__(self):
        # æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå„ªå…ˆé †ä½é †ï¼‰
        self.pattern = r'(\[[^\]]+\]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\(|\)|\.|=|#|-|\+|\\|\/|:|~|@|\?|&gt;|\*|\$|\%[0-9]{2}|[0-9])'

        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
        self.special_tokens = {
            '[PAD]': 0,
            '[CLS]': 1,
            '[SEP]': 2,
            '[MASK]': 3,
            '[UNK]': 4
        }

        # èªå½™ã®æ§‹ç¯‰
        self.vocab = self._build_vocab()
        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}
        self.id_to_token = {i: token for token, i in self.token_to_id.items()}

    def _build_vocab(self) -&gt; List[str]:
        &quot;&quot;&quot;èªå½™ã‚’æ§‹ç¯‰&quot;&quot;&quot;
        vocab = list(self.special_tokens.keys())

        # å…ƒç´ è¨˜å·
        elements = ['C', 'N', 'O', 'S', 'P', 'F', 'Cl', 'Br', 'I',
                   'c', 'n', 'o', 's', 'p']  # èŠ³é¦™æ—

        # è¨˜å·
        symbols = ['(', ')', '[', ']', '=', '#', '-', '+', '\\', '/',
                  ':', '.', '@', '@@']

        # æ•°å­—
        numbers = [str(i) for i in range(10)]

        vocab.extend(elements + symbols + numbers)

        return vocab

    def tokenize(self, smiles: str) -&gt; List[str]:
        &quot;&quot;&quot;
        SMILESæ–‡å­—åˆ—ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–

        Args:
            smiles: SMILESæ–‡å­—åˆ—

        Returns:
            tokens: ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆ

        Examples:
            &gt;&gt;&gt; tokenizer = SMILESTokenizer()
            &gt;&gt;&gt; tokenizer.tokenize(&quot;CC(C)Cc1ccc(cc1)C(C)C(=O)O&quot;)
            ['C', 'C', '(', 'C', ')', 'C', 'c', '1', 'c', 'c', 'c', '(', ...]
        &quot;&quot;&quot;
        tokens = re.findall(self.pattern, smiles)
        return ['[CLS]'] + tokens + ['[SEP]']

    def encode(self, smiles: str, max_length: int = 128) -&gt; Dict[str, torch.Tensor]:
        &quot;&quot;&quot;
        SMILESæ–‡å­—åˆ—ã‚’IDã«å¤‰æ›

        Args:
            smiles: SMILESæ–‡å­—åˆ—
            max_length: æœ€å¤§é•·

        Returns:
            encoding: input_ids, attention_mask
        &quot;&quot;&quot;
        tokens = self.tokenize(smiles)

        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’IDã«å¤‰æ›
        ids = [self.token_to_id.get(token, self.token_to_id['[UNK]'])
               for token in tokens]

        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        attention_mask = [1] * len(ids)
        while len(ids) &lt; max_length:
            ids.append(self.token_to_id['[PAD]'])
            attention_mask.append(0)

        # ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ã‚·ãƒ§ãƒ³
        ids = ids[:max_length]
        attention_mask = attention_mask[:max_length]

        return {
            'input_ids': torch.tensor([ids]),
            'attention_mask': torch.tensor([attention_mask])
        }

    def decode(self, ids: List[int]) -&gt; str:
        &quot;&quot;&quot;IDã‹ã‚‰SMILESæ–‡å­—åˆ—ã«å¾©å…ƒ&quot;&quot;&quot;
        tokens = [self.id_to_token.get(id, '[UNK]') for id in ids]
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å»
        tokens = [t for t in tokens if t not in self.special_tokens]
        return ''.join(tokens)

# ä½¿ç”¨ä¾‹
tokenizer = SMILESTokenizer()

# ã‚¤ãƒ–ãƒ—ãƒ­ãƒ•ã‚§ãƒ³
smiles = &quot;CC(C)Cc1ccc(cc1)C(C)C(=O)O&quot;
tokens = tokenizer.tokenize(smiles)
print(f&quot;Tokens: {tokens[:10]}...&quot;)

encoding = tokenizer.encode(smiles)
print(f&quot;Input IDs shape: {encoding['input_ids'].shape}&quot;)
print(f&quot;First 10 IDs: {encoding['input_ids'][0][:10]}&quot;)

# ãƒ‡ã‚³ãƒ¼ãƒ‰
decoded = tokenizer.decode(encoding['input_ids'][0].tolist())
print(f&quot;Decoded: {decoded}&quot;)
</code></pre>
<h3>SELFIES Tokenizer</h3>
<pre><code class="language-python">try:
    import selfies as sf
except ImportError:
    print(&quot;Install selfies: pip install selfies&quot;)

class SELFIESTokenizer:
    &quot;&quot;&quot;
    SELFIES (SELF-referencIng Embedded Strings) Tokenizer

    åˆ©ç‚¹:
    - 100%æœ‰åŠ¹ãªåˆ†å­ã‚’ç”Ÿæˆ
    - æ–‡æ³•çš„ã«æ­£ã—ã„
    - SMILESã‚ˆã‚Šé ‘å¥
    &quot;&quot;&quot;

    def __init__(self):
        self.special_tokens = {
            '[PAD]': 0,
            '[CLS]': 1,
            '[SEP]': 2,
            '[MASK]': 3
        }

        # ä¸€èˆ¬çš„ãªSELFIESãƒˆãƒ¼ã‚¯ãƒ³
        self.vocab = self._build_vocab()
        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}
        self.id_to_token = {i: token for token, i in self.token_to_id.items()}

    def _build_vocab(self) -&gt; List[str]:
        &quot;&quot;&quot;
        SELFIESèªå½™ã‚’æ§‹ç¯‰

        ä¸€èˆ¬çš„ãªãƒˆãƒ¼ã‚¯ãƒ³:
        [C], [N], [O], [=C], [=N], [Ring1], [Branch1], etc.
        &quot;&quot;&quot;
        vocab = list(self.special_tokens.keys())

        # åŸºæœ¬ãƒˆãƒ¼ã‚¯ãƒ³
        common_tokens = [
            '[C]', '[N]', '[O]', '[S]', '[P]', '[F]', '[Cl]', '[Br]', '[I]',
            '[=C]', '[=N]', '[=O]', '[#C]', '[#N]',
            '[Ring1]', '[Ring2]', '[Branch1]', '[Branch2]',
            '[O-1]', '[N+1]', '[nop]'
        ]

        vocab.extend(common_tokens)
        return vocab

    def smiles_to_selfies(self, smiles: str) -&gt; str:
        &quot;&quot;&quot;SMILESã‚’SELFIESã«å¤‰æ›&quot;&quot;&quot;
        try:
            selfies = sf.encoder(smiles)
            return selfies
        except Exception as e:
            print(f&quot;Encoding error: {e}&quot;)
            return &quot;&quot;

    def selfies_to_smiles(self, selfies: str) -&gt; str:
        &quot;&quot;&quot;SELFIESã‚’SMILESã«å¤‰æ›&quot;&quot;&quot;
        try:
            smiles = sf.decoder(selfies)
            return smiles
        except Exception as e:
            print(f&quot;Decoding error: {e}&quot;)
            return &quot;&quot;

    def tokenize(self, selfies: str) -&gt; List[str]:
        &quot;&quot;&quot;
        SELFIESæ–‡å­—åˆ—ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–

        Args:
            selfies: SELFIESæ–‡å­—åˆ—

        Returns:
            tokens: ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆ

        Examples:
            &gt;&gt;&gt; tokenizer = SELFIESTokenizer()
            &gt;&gt;&gt; tokenizer.tokenize(&quot;[C][C][Branch1][C][C][C]&quot;)
            ['[CLS]', '[C]', '[C]', '[Branch1]', '[C]', '[C]', '[C]', '[SEP]']
        &quot;&quot;&quot;
        tokens = list(sf.split_selfies(selfies))
        return ['[CLS]'] + tokens + ['[SEP]']

    def encode(self, selfies: str, max_length: int = 128) -&gt; Dict[str, torch.Tensor]:
        &quot;&quot;&quot;SELFIESæ–‡å­—åˆ—ã‚’IDã«å¤‰æ›&quot;&quot;&quot;
        tokens = self.tokenize(selfies)

        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’IDã«å¤‰æ›ï¼ˆæœªçŸ¥ãƒˆãƒ¼ã‚¯ãƒ³ã¯å‹•çš„ã«è¿½åŠ ï¼‰
        ids = []
        for token in tokens:
            if token not in self.token_to_id:
                new_id = len(self.vocab)
                self.vocab.append(token)
                self.token_to_id[token] = new_id
                self.id_to_token[new_id] = token
            ids.append(self.token_to_id[token])

        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        attention_mask = [1] * len(ids)
        while len(ids) &lt; max_length:
            ids.append(self.token_to_id['[PAD]'])
            attention_mask.append(0)

        # ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ã‚·ãƒ§ãƒ³
        ids = ids[:max_length]
        attention_mask = attention_mask[:max_length]

        return {
            'input_ids': torch.tensor([ids]),
            'attention_mask': torch.tensor([attention_mask])
        }

# ä½¿ç”¨ä¾‹
if 'sf' in dir():
    tokenizer_selfies = SELFIESTokenizer()

    # SMILESã‹ã‚‰SELFIESã«å¤‰æ›
    smiles = &quot;CC(C)Cc1ccc(cc1)C(C)C(=O)O&quot;
    selfies = tokenizer_selfies.smiles_to_selfies(smiles)
    print(f&quot;SELFIES: {selfies}&quot;)

    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    tokens = tokenizer_selfies.tokenize(selfies)
    print(f&quot;Tokens: {tokens[:10]}...&quot;)

    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    encoding = tokenizer_selfies.encode(selfies)
    print(f&quot;Encoded shape: {encoding['input_ids'].shape}&quot;)
</code></pre>
<hr />
<h2>âš ï¸ å®Ÿè·µçš„ãªè½ã¨ã—ç©´ã¨å¯¾å‡¦æ³•</h2>
<h3>1. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®éå­¦ç¿’</h3>
<p><strong>å•é¡Œ</strong>: å°‘é‡ãƒ‡ãƒ¼ã‚¿ã§ã®è¨“ç·´ã§æ¤œè¨¼æå¤±ãŒç™ºæ•£</p>
<pre><code class="language-python"># âŒ å•é¡Œ: å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤§ããªå­¦ç¿’ç‡ã§æ›´æ–°
def wrong_finetuning():
    model = MatBERTForBandgap(pretrained_matbert)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # å¤§ãã™ãï¼

    for epoch in range(100):  # ã‚¨ãƒãƒƒã‚¯æ•°ã‚‚å¤šã™ã
        for batch in train_loader:
            loss = compute_loss(batch)
            loss.backward()
            optimizer.step()

# âœ… è§£æ±ºç­–: Layer-wise learning rate decay + Early stopping
def correct_finetuning():
    model = MatBERTForBandgap(pretrained_matbert)

    # Layer-wise learning rate
    no_decay = ['bias', 'LayerNorm.weight']
    optimizer_grouped_parameters = [
        {
            'params': [p for n, p in model.matbert.named_parameters()
                      if not any(nd in n for nd in no_decay)],
            'weight_decay': 0.01,
            'lr': 2e-5  # äº‹å‰å­¦ç¿’éƒ¨åˆ†ã¯å°ã•ã
        },
        {
            'params': [p for n, p in model.matbert.named_parameters()
                      if any(nd in n for nd in no_decay)],
            'weight_decay': 0.0,
            'lr': 2e-5
        },
        {
            'params': model.bandgap_predictor.parameters(),
            'lr': 1e-4  # äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ã¯å¤§ãã
        }
    ]

    optimizer = torch.optim.AdamW(optimizer_grouped_parameters)

    # Early stopping
    best_val_loss = float('inf')
    patience = 5
    patience_counter = 0

    for epoch in range(100):
        train_loss = train_epoch(model, train_loader, optimizer)
        val_loss = validate(model, val_loader)

        if val_loss &lt; best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), 'best_model.pt')
            patience_counter = 0
        else:
            patience_counter += 1

        if patience_counter &gt;= patience:
            print(f&quot;Early stopping at epoch {epoch}&quot;)
            break

    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’å¾©å…ƒ
    model.load_state_dict(torch.load('best_model.pt'))
    return model
</code></pre>
<h3>2. ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚·ãƒ•ãƒˆã®å•é¡Œ</h3>
<p><strong>å•é¡Œ</strong>: ç„¡æ©Ÿææ–™ã§äº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æœ‰æ©Ÿåˆ†å­ã«é©ç”¨</p>
<pre><code class="language-python"># âŒ å•é¡Œ: ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒç•°ãªã‚‹ã®ã«ç›´æ¥é©ç”¨
def wrong_domain_adaptation():
    # ç„¡æ©Ÿææ–™ã§äº‹å‰å­¦ç¿’
    matbert = pretrained_on_inorganic_materials()

    # æœ‰æ©Ÿåˆ†å­ãƒ‡ãƒ¼ã‚¿ã§ç›´æ¥ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
    # â†’ æ€§èƒ½ãŒä½ã„ï¼
    finetune_on_organic_molecules(matbert)

# âœ… è§£æ±ºç­–: Intermediate task transfer
def correct_domain_adaptation():
    # Step 1: ç„¡æ©Ÿææ–™ã§äº‹å‰å­¦ç¿’
    matbert = pretrained_on_inorganic_materials()

    # Step 2: ä¸­é–“ã‚¿ã‚¹ã‚¯ï¼ˆç„¡æ©Ÿã¨æœ‰æ©Ÿã®ä¸­é–“ï¼‰ã§ç¶™ç¶šå­¦ç¿’
    # ä¾‹: é‡‘å±æœ‰æ©Ÿéª¨æ ¼ (MOF) ãƒ‡ãƒ¼ã‚¿
    matbert = continual_pretrain_on_mof(matbert)

    # Step 3: æœ‰æ©Ÿåˆ†å­ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
    model = finetune_on_organic_molecules(matbert)

    return model

# ã¾ãŸã¯: Domain-adversarial training
class DomainAdversarialTraining:
    def train(self, source_data, target_data):
        for source_batch, target_batch in zip(source_data, target_data):
            # Source domain: ã‚¿ã‚¹ã‚¯æå¤±
            source_output = model(source_batch)
            task_loss = compute_task_loss(source_output, source_batch.labels)

            # Both domains: ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†é¡æå¤±ï¼ˆé€†è»¢å‹¾é…ï¼‰
            source_domain_pred = domain_classifier(source_output, reverse_gradient=True)
            target_domain_pred = domain_classifier(target_output, reverse_gradient=True)

            domain_loss = compute_domain_loss(source_domain_pred, target_domain_pred)

            total_loss = task_loss + 0.1 * domain_loss
            total_loss.backward()
            optimizer.step()
</code></pre>
<h3>3. Masked Language Modelingã®ãƒã‚¹ã‚¯æˆ¦ç•¥ãƒŸã‚¹</h3>
<p><strong>å•é¡Œ</strong>: ãƒã‚¹ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒåã£ã¦ã„ã‚‹</p>
<pre><code class="language-python"># âŒ å•é¡Œ: ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒã‚¹ã‚¯ï¼ˆåŒ–å­¦çš„ã«ç„¡æ„å‘³ï¼‰
def wrong_masking(composition_ids):
    mask_prob = 0.15
    mask = torch.rand(composition_ids.shape) &lt; mask_prob
    composition_ids[mask] = MASK_TOKEN_ID
    return composition_ids

# âœ… è§£æ±ºç­–: åŒ–å­¦çš„ã«æ„å‘³ã®ã‚ã‚‹ãƒã‚¹ã‚¯
def chemically_aware_masking(composition_ids, element_groups):
    &quot;&quot;&quot;
    å…ƒç´ ã‚°ãƒ«ãƒ¼ãƒ—ã‚’è€ƒæ…®ã—ãŸãƒã‚¹ã‚¯

    Args:
        composition_ids: (batch, seq_len)
        element_groups: {group_id: [element_ids]}
            ä¾‹: {0: [26, 27, 28], 1: [8, 16]}  # é·ç§»é‡‘å±ã€ã‚«ãƒ«ã‚³ã‚²ãƒ³
    &quot;&quot;&quot;
    mask_prob = 0.15
    masked_ids = composition_ids.clone()

    for i in range(composition_ids.size(0)):
        # åŒ–å­¦çš„ã‚°ãƒ«ãƒ¼ãƒ—å˜ä½ã§ãƒã‚¹ã‚¯
        for group_id, element_ids in element_groups.items():
            group_positions = torch.isin(composition_ids[i], torch.tensor(element_ids))
            if group_positions.sum() &gt; 0:
                # ã‚°ãƒ«ãƒ¼ãƒ—å†…ã®ä¸€éƒ¨ã‚’ãƒã‚¹ã‚¯
                mask_within_group = torch.rand(group_positions.sum()) &lt; mask_prob
                group_indices = torch.where(group_positions)[0]
                masked_positions = group_indices[mask_within_group]
                masked_ids[i, masked_positions] = MASK_TOKEN_ID

    return masked_ids

# ä½¿ç”¨ä¾‹
element_groups = {
    0: [26, 27, 28, 29],  # Fe, Co, Ni, Cuï¼ˆé·ç§»é‡‘å±ï¼‰
    1: [8, 16, 34],       # O, S, Seï¼ˆã‚«ãƒ«ã‚³ã‚²ãƒ³ï¼‰
    2: [3, 11, 19]        # Li, Na, Kï¼ˆã‚¢ãƒ«ã‚«ãƒªé‡‘å±ï¼‰
}

masked_composition = chemically_aware_masking(composition_ids, element_groups)
</code></pre>
<h3>4. Few-shotå­¦ç¿’ã®ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆé¸æŠãƒŸã‚¹</h3>
<p><strong>å•é¡Œ</strong>: ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆãŒåã£ã¦ã„ã‚‹</p>
<pre><code class="language-python"># âŒ å•é¡Œ: ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆã‚’é¸æŠ
def wrong_support_selection(dataset, k=5):
    indices = torch.randperm(len(dataset))[:k]
    return dataset[indices]

# âœ… è§£æ±ºç­–: å¤šæ§˜æ€§ã‚’è€ƒæ…®ã—ãŸã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆé¸æŠ
def diverse_support_selection(dataset, embeddings, k=5):
    &quot;&quot;&quot;
    K-meansã§å¤šæ§˜ãªã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ

    Args:
        dataset: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        embeddings: (N, d) ã‚µãƒ³ãƒ—ãƒ«ã®åŸ‹ã‚è¾¼ã¿
        k: ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆ size
    &quot;&quot;&quot;
    from sklearn.cluster import KMeans

    # K-meansã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(embeddings.numpy())

    # å„ã‚¯ãƒ©ã‚¹ã‚¿ã®ä¸­å¿ƒã«æœ€ã‚‚è¿‘ã„ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ
    support_indices = []
    for i in range(k):
        cluster_indices = torch.where(torch.tensor(labels) == i)[0]
        cluster_embeddings = embeddings[cluster_indices]
        cluster_center = kmeans.cluster_centers_[i]

        # ä¸­å¿ƒã«æœ€ã‚‚è¿‘ã„ã‚µãƒ³ãƒ—ãƒ«
        distances = torch.norm(cluster_embeddings - torch.tensor(cluster_center), dim=1)
        closest_idx = cluster_indices[torch.argmin(distances)]
        support_indices.append(closest_idx.item())

    return dataset[support_indices]

# ä½¿ç”¨ä¾‹
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŸ‹ã‚è¾¼ã¿ã‚’äº‹å‰è¨ˆç®—
embeddings = compute_embeddings(dataset, matbert)
support_set = diverse_support_selection(dataset, embeddings, k=10)
</code></pre>
<h3>5. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®æœ€é©åŒ–ä¸è¶³</h3>
<p><strong>å•é¡Œ</strong>: å›ºå®šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§æ€§èƒ½ãŒä½ã„</p>
<pre><code class="language-python"># âŒ å•é¡Œ: æ‰‹å‹•ã§è¨­è¨ˆã—ãŸå›ºå®šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
class FixedPromptModel(nn.Module):
    def __init__(self, matbert):
        super().__init__()
        self.matbert = matbert
        # å›ºå®šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        self.prompt = nn.Parameter(torch.randn(1, 10, 768), requires_grad=False)

# âœ… è§£æ±ºç­–: å­¦ç¿’å¯èƒ½ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆPrefix-Tuningï¼‰
class LearnablePromptModel(nn.Module):
    def __init__(self, matbert, prompt_length=10, num_tasks=5):
        super().__init__()
        self.matbert = matbert
        self.prompt_length = prompt_length

        # ã‚¿ã‚¹ã‚¯åˆ¥ã®å­¦ç¿’å¯èƒ½ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        self.task_prompts = nn.Parameter(torch.randn(num_tasks, prompt_length, 768))

        # MatBERTã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å›ºå®š
        for param in self.matbert.parameters():
            param.requires_grad = False

    def forward(self, input_ids, task_id=0):
        batch_size = input_ids.size(0)

        # å…¥åŠ›åŸ‹ã‚è¾¼ã¿
        input_embeddings = self.matbert.embeddings(input_ids)

        # ã‚¿ã‚¹ã‚¯å›ºæœ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
        prompt = self.task_prompts[task_id].unsqueeze(0).expand(batch_size, -1, -1)
        embeddings = torch.cat([prompt, input_embeddings], dim=1)

        # Transformerã«é€šã™
        outputs = self.matbert.encoder(embeddings)

        return outputs

# è¨“ç·´
model = LearnablePromptModel(pretrained_matbert, prompt_length=10, num_tasks=5)

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã¿æœ€é©åŒ–ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å¤§å¹…å‰Šæ¸›ï¼‰
optimizer = torch.optim.Adam([model.task_prompts], lr=1e-3)
</code></pre>
<hr />
<h2>âœ… ç¬¬3ç« å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ</h2>
<h3>æ¦‚å¿µç†è§£ï¼ˆ10é …ç›®ï¼‰</h3>
<ul>
<li>[ ] äº‹å‰å­¦ç¿’ã®é‡è¦æ€§ã¨åˆ©ç‚¹ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>[ ] Masked Language Modelingã®åŸç†ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] Full/Feature Extraction/Partial Fine-tuningã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>[ ] Few-shotå­¦ç¿’ã®åŸç†ï¼ˆPrototypical Networksï¼‰ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®æ¦‚å¿µã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œã®å¿…è¦æ€§ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>[ ] äº‹å‰å­¦ç¿’ã‚¿ã‚¹ã‚¯ã¨ä¸‹æµã‚¿ã‚¹ã‚¯ã®é–¢ä¿‚ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] Transfer Learningã®åŠ¹æœã‚’å®šé‡çš„ã«è©•ä¾¡ã§ãã‚‹</li>
<li>[ ] MatBERTã€MolBERTãªã©ææ–™ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] BERT/GPT/T5ã®é•ã„ã¨é©ç”¨å ´é¢ã‚’èª¬æ˜ã§ãã‚‹</li>
</ul>
<h3>å®Ÿè£…ã‚¹ã‚­ãƒ«ï¼ˆ15é …ç›®ï¼‰</h3>
<ul>
<li>[ ] <code>MatBERT</code>ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] <code>MatGPT</code>ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] <code>MatT5</code>ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] SMILESãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] SELFIESãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] Masked Atom Predictionã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] Fine-tuningæˆ¦ç•¥ï¼ˆFull/Feature/Partialï¼‰ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] Prototypical Networksã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] å­¦ç¿’å¯èƒ½ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] Domain-adversarial trainingã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] Early stoppingã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] Layer-wise learning rateã‚’è¨­å®šã§ãã‚‹</li>
<li>[ ] äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ã§ãã‚‹</li>
<li>[ ] Hugging Face Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’æ´»ç”¨ã§ãã‚‹</li>
<li>[ ] ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’Transformersã«çµ±åˆã§ãã‚‹</li>
</ul>
<h3>ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚­ãƒ«ï¼ˆ5é …ç›®ï¼‰</h3>
<ul>
<li>[ ] éå­¦ç¿’ã‚’æ¤œå‡ºã—ã€æ­£å‰‡åŒ–ã§å¯¾å‡¦ã§ãã‚‹</li>
<li>[ ] ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚·ãƒ•ãƒˆã‚’æ¤œå‡ºã—ã€é©å¿œæ‰‹æ³•ã‚’é©ç”¨ã§ãã‚‹</li>
<li>[ ] ãƒã‚¹ã‚¯æˆ¦ç•¥ã®å¦¥å½“æ€§ã‚’è©•ä¾¡ã§ãã‚‹</li>
<li>[ ] Few-shotã®ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆå“è³ªã‚’è©•ä¾¡ã§ãã‚‹</li>
<li>[ ] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®åŠ¹æœã‚’å¯è¦–åŒ–ãƒ»åˆ†æã§ãã‚‹</li>
</ul>
<h3>å¿œç”¨åŠ›ï¼ˆ5é …ç›®ï¼‰</h3>
<ul>
<li>[ ] æ–°ã—ã„ææ–™ç‰¹æ€§äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã«äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’é©ç”¨ã§ãã‚‹</li>
<li>[ ] è¤‡æ•°ã®äº‹å‰å­¦ç¿’ã‚¿ã‚¹ã‚¯ã‚’çµ„ã¿åˆã‚ã›ã¦æ€§èƒ½å‘ä¸Šã§ãã‚‹</li>
<li>[ ] ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œæˆ¦ç•¥ã‚’è¨­è¨ˆã§ãã‚‹</li>
<li>[ ] Few-shotå­¦ç¿’ã‚’ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã¨çµ„ã¿åˆã‚ã›ã‚‰ã‚Œã‚‹</li>
<li>[ ] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã§æ€§èƒ½ã‚’æœ€é©åŒ–ã§ãã‚‹</li>
</ul>
<h3>ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆ5é …ç›®ï¼‰</h3>
<ul>
<li>[ ] SMILESãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ã§ãã‚‹</li>
<li>[ ] SELFIESã«å¤‰æ›ã§ãã‚‹</li>
<li>[ ] ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆSMILES enumerationï¼‰ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥ã«ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã§ãã‚‹</li>
<li>[ ] Few-shotç”¨ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã§ãã‚‹</li>
</ul>
<h3>è©•ä¾¡ã‚¹ã‚­ãƒ«ï¼ˆ5é …ç›®ï¼‰</h3>
<ul>
<li>[ ] äº‹å‰å­¦ç¿’ã®åŠ¹æœã‚’å®šé‡è©•ä¾¡ã§ãã‚‹ï¼ˆvs from scratchï¼‰</li>
<li>[ ] Fine-tuningæˆ¦ç•¥ã‚’æ¯”è¼ƒè©•ä¾¡ã§ãã‚‹</li>
<li>[ ] Few-shotæ€§èƒ½ã‚’é©åˆ‡ã«è©•ä¾¡ã§ãã‚‹ï¼ˆN-way K-shotï¼‰</li>
<li>[ ] ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œã®åŠ¹æœã‚’æ¸¬å®šã§ãã‚‹</li>
<li>[ ] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å½±éŸ¿ã‚’åˆ†æã§ãã‚‹</li>
</ul>
<h3>ç†è«–çš„èƒŒæ™¯ï¼ˆ5é …ç›®ï¼‰</h3>
<ul>
<li>[ ] MatBERT/MolBERTè«–æ–‡ã‚’èª­ã‚“ã </li>
<li>[ ] BERTè«–æ–‡ï¼ˆDevlin et al., 2019ï¼‰ã‚’èª­ã‚“ã </li>
<li>[ ] GPTè«–æ–‡ã‚’èª­ã‚“ã </li>
<li>[ ] Few-shotå­¦ç¿’ã®è«–æ–‡ã‚’1æœ¬ä»¥ä¸Šèª­ã‚“ã </li>
<li>[ ] Transfer Learningç†è«–ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
</ul>
<h3>å®Œäº†åŸºæº–</h3>
<ul>
<li><strong>æœ€ä½åŸºæº–</strong>: 40é …ç›®ä»¥ä¸Šé”æˆï¼ˆ80%ï¼‰</li>
<li><strong>æ¨å¥¨åŸºæº–</strong>: 45é …ç›®ä»¥ä¸Šé”æˆï¼ˆ90%ï¼‰</li>
<li><strong>å„ªç§€åŸºæº–</strong>: 50é …ç›®å…¨ã¦é”æˆï¼ˆ100%ï¼‰</li>
</ul>
<hr />
<p><strong>æ¬¡ç« </strong>: <strong><a href="chapter-4.html">ç¬¬4ç« : ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¨é€†è¨­è¨ˆ</a></strong></p>
<hr />
<p><strong>ä½œæˆè€…</strong>: æ©‹æœ¬ä½‘ä»‹ï¼ˆæ±åŒ—å¤§å­¦ï¼‰
<strong>æœ€çµ‚æ›´æ–°</strong>: 2025å¹´10æœˆ19æ—¥</p><div class="navigation">
    <a href="chapter-2.html" class="nav-button">â† å‰ã®ç« </a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter-4.html" class="nav-button">æ¬¡ã®ç«  â†’</a>
</div>
    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
