<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
        <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/wp/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/MI/index.html">ãƒãƒ†ãƒªã‚¢ãƒ«ã‚ºãƒ»ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/MI/transformer-introduction/index.html">Transformer</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 2</span>
        </div>
    </nav>

    <header>
        <div class="header-content">
            <h1>Chapter</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 20-25åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 0å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 0å•</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>ç¬¬2ç« : ææ–™å‘ã‘Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">Matformerç­‰ã®ä»£è¡¨ãƒ¢ãƒ‡ãƒ«ã®é•ã„ã¨é©ç”¨ç¯„å›²ã‚’ä¿¯ç°ã—ã¾ã™ã€‚çµæ™¶ã‚„ã‚°ãƒ©ãƒ•ã¨ã®æ¥ç¶šã‚‚æŠ¼ã•ãˆã¾ã™ã€‚</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>ğŸ’¡ è£œè¶³:</strong> å…¥åŠ›è¡¨ç¾ï¼ˆæ§‹é€ /çµ„æˆ/ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã¨äº‹å‰å­¦ç¿’ã‚¿ã‚¹ã‚¯ã®è¨­è¨ˆã§æ€§èƒ½ãŒå¤§ããå¤‰ã‚ã‚Šã¾ã™ã€‚</p>





<p><strong>å­¦ç¿’æ™‚é–“</strong>: 30-35åˆ† | <strong>é›£æ˜“åº¦</strong>: ä¸­ç´šã€œä¸Šç´š</p>
<h2>ğŸ“‹ ã“ã®ç« ã§å­¦ã¶ã“ã¨</h2>
<ul>
<li>ææ–™ç§‘å­¦ã«ç‰¹åŒ–ã—ãŸTransformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¨­è¨ˆåŸç†</li>
<li>Matformer: Materials Transformer for Property Prediction</li>
<li>CrystalFormer: Crystal Structure Representation</li>
<li>ChemBERTa: åˆ†å­SMILESè¡¨ç¾å­¦ç¿’</li>
<li>Perceiver IO: å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿çµ±åˆ</li>
<li>å®Ÿè£…æ¼”ç¿’: Matformerã§ææ–™ç‰¹æ€§äºˆæ¸¬</li>
</ul>
<hr />
<h2>2.1 ææ–™ç§‘å­¦ç‰¹åŒ–Transformerã®å¿…è¦æ€§</h2>
<h3>æ±ç”¨Transformerã®é™ç•Œ</h3>
<p><strong>è‡ªç„¶è¨€èªå‡¦ç†ç”¨Transformerã‚’ãã®ã¾ã¾ä½¿ã†å•é¡Œ</strong>:
- âŒ åˆ†å­ãƒ»ææ–™ã®3Dæ§‹é€ æƒ…å ±ãŒå¤±ã‚ã‚Œã‚‹
- âŒ åŒ–å­¦çµåˆã‚„åŸå­é–“è·é›¢ã‚’è€ƒæ…®ã§ããªã„
- âŒ å‘¨æœŸçš„å¢ƒç•Œæ¡ä»¶ï¼ˆçµæ™¶ï¼‰ã‚’æ‰±ãˆãªã„
- âŒ ç‰©ç†çš„åˆ¶ç´„ï¼ˆä¿å­˜å‰‡ã€å¯¾ç§°æ€§ï¼‰ã‚’ç„¡è¦–</p>
<h3>ææ–™ç‰¹åŒ–Transformerã®ç‰¹å¾´</h3>
<p><strong>å¿…è¦ãªæ‹¡å¼µ</strong>:
- âœ… <strong>3Dæ§‹é€ ã®åŸ‹ã‚è¾¼ã¿</strong>: åŸå­åº§æ¨™ã€è·é›¢ã€è§’åº¦
- âœ… <strong>å‘¨æœŸçš„å¢ƒç•Œæ¡ä»¶</strong>: çµæ™¶æ ¼å­ã®ç¹°ã‚Šè¿”ã—
- âœ… <strong>ç‰©ç†çš„åˆ¶ç´„</strong>: å¯¾ç§°æ€§ã€ç­‰å¤‰æ€§
- âœ… <strong>å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿çµ±åˆ</strong>: æ§‹é€  + çµ„æˆ + å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿</p>
<div class="mermaid">
flowchart TD
    A[æ±ç”¨Transformer] --> B[ææ–™ç‰¹åŒ–Transformer]
    B --> C[3Dæ§‹é€ åŸ‹ã‚è¾¼ã¿]
    B --> D[å‘¨æœŸå¢ƒç•Œæ¡ä»¶]
    B --> E[ç‰©ç†åˆ¶ç´„]
    B --> F[å¤šæ§˜ãƒ‡ãƒ¼ã‚¿çµ±åˆ]

    C --> G[Matformer]
    D --> G
    E --> H[CrystalFormer]
    F --> I[Perceiver IO]

    style G fill:#e1f5ff
    style H fill:#ffe1f5
    style I fill:#f5ffe1
</div>

<hr />
<h2>2.2 Matformer: Materials Transformer</h2>
<h3>æ¦‚è¦</h3>
<p><strong>Matformer</strong> (Chen et al., 2022)ã¯ã€ææ–™ã®çµæ™¶æ§‹é€ ã‹ã‚‰ç‰¹æ€§ã‚’äºˆæ¸¬ã™ã‚‹Transformerãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</p>
<p><strong>ç‰¹å¾´</strong>:
- <strong>Nested Transformer</strong>: åŸå­ãƒ¬ãƒ™ãƒ«ã¨ã‚¯ãƒªã‚¹ã‚¿ãƒ«ãƒ¬ãƒ™ãƒ«ã®éšå±¤çš„å‡¦ç†
- <strong>Distance-aware Attention</strong>: åŸå­é–“è·é›¢ã‚’è€ƒæ…®
- <strong>Elastic Inference</strong>: è¨ˆç®—é‡ã¨ãƒ¡ãƒ¢ãƒªã‚’å‹•çš„ã«èª¿æ•´</p>
<h3>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h3>
<div class="mermaid">
flowchart TB
    subgraph Input
        A1[åŸå­åº§æ¨™] --> B[åŸå­åŸ‹ã‚è¾¼ã¿]
        A2[åŸå­ç•ªå·] --> B
        A3[æ ¼å­å®šæ•°] --> B
    end

    B --> C[Positional Encoding]
    C --> D[Distance Matrix]

    subgraph "Nested Transformer"
        D --> E1[Atom-level Attention]
        E1 --> E2[Structure-level Attention]
    end

    E2 --> F[Pooling]
    F --> G[Prediction Head]
    G --> H[ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—/å½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼]

    style E1 fill:#e1f5ff
    style E2 fill:#ffe1e1
</div>

<h3>åŸå­åŸ‹ã‚è¾¼ã¿ï¼ˆAtom Embeddingï¼‰</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import numpy as np

class AtomEmbedding(nn.Module):
    def __init__(self, num_atoms=118, d_model=256):
        &quot;&quot;&quot;
        åŸå­åŸ‹ã‚è¾¼ã¿å±¤

        Args:
            num_atoms: åŸå­ã®ç¨®é¡æ•°ï¼ˆå‘¨æœŸè¡¨ã€118å…ƒç´ ï¼‰
            d_model: åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
        &quot;&quot;&quot;
        super(AtomEmbedding, self).__init__()
        self.embedding = nn.Embedding(num_atoms, d_model)

    def forward(self, atomic_numbers):
        &quot;&quot;&quot;
        Args:
            atomic_numbers: (batch_size, num_atoms) åŸå­ç•ªå·
        Returns:
            embeddings: (batch_size, num_atoms, d_model)
        &quot;&quot;&quot;
        return self.embedding(atomic_numbers)

# ä½¿ç”¨ä¾‹: NaClçµæ™¶
batch_size = 2
num_atoms = 8  # å˜ä½æ ¼å­å†…ã®åŸå­æ•°

# åŸå­ç•ªå·: Na(11), Cl(17)
atomic_numbers = torch.tensor([
    [11, 17, 11, 17, 11, 17, 11, 17],  # ã‚µãƒ³ãƒ—ãƒ«1
    [11, 17, 11, 17, 11, 17, 11, 17]   # ã‚µãƒ³ãƒ—ãƒ«2
])

atom_emb = AtomEmbedding(num_atoms=118, d_model=256)
embeddings = atom_emb(atomic_numbers)
print(f&quot;Atom embeddings shape: {embeddings.shape}&quot;)  # (2, 8, 256)
</code></pre>
<h3>Distance-aware Attention</h3>
<p><strong>åŸå­é–“è·é›¢ã‚’è€ƒæ…®ã—ãŸAttention</strong>:</p>
<pre><code class="language-python">class DistanceAwareAttention(nn.Module):
    def __init__(self, d_model, num_heads, max_distance=10.0):
        &quot;&quot;&quot;
        è·é›¢ã‚’è€ƒæ…®ã—ãŸAttention

        Args:
            d_model: ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ
            num_heads: Attentionãƒ˜ãƒƒãƒ‰æ•°
            max_distance: æœ€å¤§è·é›¢ï¼ˆÃ…ï¼‰
        &quot;&quot;&quot;
        super(DistanceAwareAttention, self).__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        self.max_distance = max_distance

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

        # è·é›¢åŸ‹ã‚è¾¼ã¿
        self.distance_embedding = nn.Linear(1, num_heads)

    def forward(self, x, distance_matrix):
        &quot;&quot;&quot;
        Args:
            x: (batch_size, num_atoms, d_model)
            distance_matrix: (batch_size, num_atoms, num_atoms) åŸå­é–“è·é›¢ï¼ˆÃ…ï¼‰
        &quot;&quot;&quot;
        batch_size = x.size(0)

        # Q, K, V
        Q = self.W_q(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)

        # Attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)

        # è·é›¢ãƒã‚¤ã‚¢ã‚¹
        # è·é›¢ãŒè¿‘ã„ã»ã©å¤§ããªå€¤ã€é ã„ã»ã©å°ã•ãªå€¤
        distance_bias = self.distance_embedding(distance_matrix.unsqueeze(-1))  # (batch, num_atoms, num_atoms, num_heads)
        distance_bias = distance_bias.permute(0, 3, 1, 2)  # (batch, num_heads, num_atoms, num_atoms)

        # ã‚¬ã‚¦ã‚¹é–¢æ•°ã§è·é›¢ã‚’å¤‰æ›ï¼ˆè¿‘ã„åŸå­ã»ã©é«˜ã„ã‚¹ã‚³ã‚¢ï¼‰
        distance_factor = torch.exp(-distance_matrix.unsqueeze(1) / 2.0)  # (batch, 1, num_atoms, num_atoms)

        scores = scores + distance_bias * distance_factor

        # Softmax
        attention_weights = torch.softmax(scores, dim=-1)

        # Attentionã®é©ç”¨
        output = torch.matmul(attention_weights, V)
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        output = self.W_o(output)

        return output, attention_weights

# ä½¿ç”¨ä¾‹
d_model = 256
num_heads = 8
num_atoms = 8

dist_attn = DistanceAwareAttention(d_model, num_heads)

x = torch.randn(2, num_atoms, d_model)
# NaClçµæ™¶ã®åŸå­é–“è·é›¢ï¼ˆç°¡ç•¥ç‰ˆï¼‰
distance_matrix = torch.tensor([
    [[0.0, 2.8, 3.9, 4.8, 3.9, 5.5, 4.8, 6.7],  # åŸå­1ã‹ã‚‰ã®è·é›¢
     [2.8, 0.0, 2.8, 3.9, 5.5, 3.9, 6.7, 4.8],
     # ... çœç•¥
     [6.7, 4.8, 5.5, 3.9, 4.8, 3.9, 2.8, 0.0]]
]).repeat(2, 1, 1)  # batch_sizeåˆ†è¤‡è£½

output, attn_weights = dist_attn(x, distance_matrix)
print(f&quot;Output shape: {output.shape}&quot;)  # (2, 8, 256)
</code></pre>
<h3>Matformerãƒ–ãƒ­ãƒƒã‚¯</h3>
<pre><code class="language-python">class MatformerBlock(nn.Module):
    def __init__(self, d_model, num_heads, d_ff=1024, dropout=0.1):
        &quot;&quot;&quot;
        Matformerã®åŸºæœ¬ãƒ–ãƒ­ãƒƒã‚¯

        Args:
            d_model: ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ
            num_heads: Attentionãƒ˜ãƒƒãƒ‰æ•°
            d_ff: Feed-Forwardå±¤ã®ä¸­é–“æ¬¡å…ƒ
            dropout: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        &quot;&quot;&quot;
        super(MatformerBlock, self).__init__()

        self.distance_attention = DistanceAwareAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)

        # Feed-Forward Network
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x, distance_matrix):
        # Distance-aware Attention + Residual
        attn_output, _ = self.distance_attention(x, distance_matrix)
        x = self.norm1(x + self.dropout1(attn_output))

        # Feed-Forward + Residual
        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout2(ffn_output))

        return x
</code></pre>
<hr />
<h2>2.3 CrystalFormer: çµæ™¶æ§‹é€ Transformer</h2>
<h3>æ¦‚è¦</h3>
<p><strong>CrystalFormer</strong>ã¯ã€çµæ™¶ã®å‘¨æœŸçš„å¢ƒç•Œæ¡ä»¶ã‚’è€ƒæ…®ã—ãŸTransformerã§ã™ã€‚</p>
<p><strong>ç‰¹å¾´</strong>:
- <strong>Wyckoffä½ç½®åŸ‹ã‚è¾¼ã¿</strong>: çµæ™¶ã®å¯¾ç§°æ€§ã‚’è€ƒæ…®
- <strong>Fractional Coordinates</strong>: åˆ†æ•°åº§æ¨™ã§ã®è¡¨ç¾
- <strong>Space Group Encoding</strong>: ç©ºé–“ç¾¤æƒ…å ±ã®åŸ‹ã‚è¾¼ã¿</p>
<h3>åˆ†æ•°åº§æ¨™åŸ‹ã‚è¾¼ã¿</h3>
<pre><code class="language-python">class FractionalCoordinateEncoding(nn.Module):
    def __init__(self, d_model):
        super(FractionalCoordinateEncoding, self).__init__()
        self.coord_linear = nn.Linear(3, d_model)

    def forward(self, fractional_coords):
        &quot;&quot;&quot;
        Args:
            fractional_coords: (batch_size, num_atoms, 3) åˆ†æ•°åº§æ¨™ [0, 1)
        Returns:
            encoding: (batch_size, num_atoms, d_model)
        &quot;&quot;&quot;
        # ä¸‰è§’é–¢æ•°åŸ‹ã‚è¾¼ã¿
        freqs = torch.arange(1, d_model // 6 + 1, dtype=torch.float32)
        coords_expanded = fractional_coords.unsqueeze(-1) * freqs

        encoding = torch.cat([
            torch.sin(2 * np.pi * coords_expanded),
            torch.cos(2 * np.pi * coords_expanded)
        ], dim=-1)

        # ç·šå½¢å¤‰æ›ã§æ¬¡å…ƒèª¿æ•´
        encoding = encoding.flatten(start_dim=2)
        encoding = self.coord_linear(encoding)

        return encoding
</code></pre>
<h3>å‘¨æœŸå¢ƒç•Œæ¡ä»¶ã®è€ƒæ…®</h3>
<pre><code class="language-python">def compute_periodic_distance(coords1, coords2, lattice_matrix):
    &quot;&quot;&quot;
    å‘¨æœŸå¢ƒç•Œæ¡ä»¶ã‚’è€ƒæ…®ã—ãŸè·é›¢è¨ˆç®—

    Args:
        coords1: (num_atoms1, 3) åˆ†æ•°åº§æ¨™
        coords2: (num_atoms2, 3) åˆ†æ•°åº§æ¨™
        lattice_matrix: (3, 3) æ ¼å­ãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ—
    Returns:
        distances: (num_atoms1, num_atoms2) æœ€çŸ­è·é›¢ï¼ˆÃ…ï¼‰
    &quot;&quot;&quot;
    # ãƒ‡ã‚«ãƒ«ãƒˆåº§æ¨™ã«å¤‰æ›
    cart1 = torch.matmul(coords1, lattice_matrix)
    cart2 = torch.matmul(coords2, lattice_matrix)

    # ã™ã¹ã¦ã®å‘¨æœŸã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’è€ƒæ…®ï¼ˆ-1, 0, 1ã®ç¯„å›²ï¼‰
    offsets = torch.tensor([
        [i, j, k] for i in [-1, 0, 1]
                  for j in [-1, 0, 1]
                  for k in [-1, 0, 1]
    ], dtype=torch.float32)  # 27é€šã‚Š

    min_distances = []
    for offset in offsets:
        offset_cart = torch.matmul(offset, lattice_matrix)
        shifted_cart2 = cart2 + offset_cart

        # è·é›¢è¨ˆç®—
        diff = cart1.unsqueeze(1) - shifted_cart2.unsqueeze(0)
        distances = torch.norm(diff, dim=-1)
        min_distances.append(distances)

    # æœ€çŸ­è·é›¢ã‚’é¸æŠ
    min_distances = torch.stack(min_distances, dim=-1)
    min_distances, _ = torch.min(min_distances, dim=-1)

    return min_distances

# ä½¿ç”¨ä¾‹: å˜ç´”ç«‹æ–¹æ ¼å­
fractional_coords = torch.tensor([
    [0.0, 0.0, 0.0],  # åŸå­1
    [0.5, 0.5, 0.5]   # åŸå­2
])

lattice_matrix = torch.tensor([
    [5.0, 0.0, 0.0],
    [0.0, 5.0, 0.0],
    [0.0, 0.0, 5.0]
])  # 5Ã…ã®ç«‹æ–¹æ ¼å­

distances = compute_periodic_distance(fractional_coords, fractional_coords, lattice_matrix)
print(&quot;Distance matrix (Ã…):&quot;)
print(distances)
</code></pre>
<hr />
<h2>2.4 ChemBERTa: åˆ†å­SMILESè¡¨ç¾å­¦ç¿’</h2>
<h3>æ¦‚è¦</h3>
<p><strong>ChemBERTa</strong>ã¯ã€åˆ†å­ã®SMILESæ–‡å­—åˆ—ã‚’BERTã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</p>
<p><strong>ç‰¹å¾´</strong>:
- <strong>RoBERTa</strong>ãƒ™ãƒ¼ã‚¹ï¼ˆBERTæ”¹è‰¯ç‰ˆï¼‰
- <strong>10Måˆ†å­</strong>ã§äº‹å‰å­¦ç¿’
- <strong>è»¢ç§»å­¦ç¿’</strong>ã§å°‘é‡ãƒ‡ãƒ¼ã‚¿ã§ã‚‚é«˜ç²¾åº¦</p>
<h3>SMILESãƒˆãƒ¼ã‚¯ãƒ³åŒ–</h3>
<pre><code class="language-python">from transformers import RobertaTokenizer

class SMILESTokenizer:
    def __init__(self):
        # ChemBERTaç”¨ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶
        self.tokenizer = RobertaTokenizer.from_pretrained(&quot;seyonec/ChemBERTa-zinc-base-v1&quot;)

    def encode(self, smiles_list):
        &quot;&quot;&quot;
        SMILESæ–‡å­—åˆ—ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–

        Args:
            smiles_list: SMILESã®ãƒªã‚¹ãƒˆ
        Returns:
            input_ids: ãƒˆãƒ¼ã‚¯ãƒ³ID
            attention_mask: ãƒã‚¹ã‚¯
        &quot;&quot;&quot;
        encoded = self.tokenizer(
            smiles_list,
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors='pt'
        )
        return encoded['input_ids'], encoded['attention_mask']

# ä½¿ç”¨ä¾‹
smiles_list = [
    'CCO',  # ã‚¨ã‚¿ãƒãƒ¼ãƒ«
    'CC(C)Cc1ccc(cc1)C(C)C(=O)O',  # ã‚¤ãƒ–ãƒ—ãƒ­ãƒ•ã‚§ãƒ³
    'CN1C=NC2=C1C(=O)N(C(=O)N2C)C'  # ã‚«ãƒ•ã‚§ã‚¤ãƒ³
]

tokenizer = SMILESTokenizer()
input_ids, attention_mask = tokenizer.encode(smiles_list)

print(f&quot;Input IDs shape: {input_ids.shape}&quot;)
print(f&quot;Attention mask shape: {attention_mask.shape}&quot;)
print(f&quot;First molecule tokens: {input_ids[0][:10]}&quot;)
</code></pre>
<h3>ChemBERTaãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨</h3>
<pre><code class="language-python">from transformers import RobertaModel

class ChemBERTaEmbedding(nn.Module):
    def __init__(self, pretrained_model=&quot;seyonec/ChemBERTa-zinc-base-v1&quot;):
        super(ChemBERTaEmbedding, self).__init__()
        self.bert = RobertaModel.from_pretrained(pretrained_model)

    def forward(self, input_ids, attention_mask):
        &quot;&quot;&quot;
        Args:
            input_ids: (batch_size, seq_len)
            attention_mask: (batch_size, seq_len)
        Returns:
            embeddings: (batch_size, hidden_size)
        &quot;&quot;&quot;
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)

        # [CLS]ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨
        cls_embedding = outputs.last_hidden_state[:, 0, :]

        return cls_embedding

# åˆ†å­ç‰¹æ€§äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
class MoleculePropertyPredictor(nn.Module):
    def __init__(self, hidden_size=768, num_properties=1):
        super(MoleculePropertyPredictor, self).__init__()
        self.chemberta = ChemBERTaEmbedding()
        self.predictor = nn.Sequential(
            nn.Linear(hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, num_properties)
        )

    def forward(self, input_ids, attention_mask):
        embeddings = self.chemberta(input_ids, attention_mask)
        predictions = self.predictor(embeddings)
        return predictions

# ä½¿ç”¨ä¾‹
model = MoleculePropertyPredictor(num_properties=1)  # ä¾‹: logPäºˆæ¸¬
predictions = model(input_ids, attention_mask)
print(f&quot;Predictions shape: {predictions.shape}&quot;)  # (3, 1)
</code></pre>
<hr />
<h2>2.5 Perceiver IO: å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿çµ±åˆ</h2>
<h3>æ¦‚è¦</h3>
<p><strong>Perceiver IO</strong>ã¯ã€ç•°ãªã‚‹ç¨®é¡ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¦å‡¦ç†ã§ãã‚‹Transformerã§ã™ã€‚</p>
<p><strong>ææ–™ç§‘å­¦ã§ã®å¿œç”¨</strong>:
- æ§‹é€ ãƒ‡ãƒ¼ã‚¿ + çµ„æˆãƒ‡ãƒ¼ã‚¿
- å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ + è¨ˆç®—ãƒ‡ãƒ¼ã‚¿
- ç”»åƒ + ãƒ†ã‚­ã‚¹ãƒˆ + æ•°å€¤</p>
<h3>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h3>
<div class="mermaid">
flowchart TB
    A1[æ§‹é€ ãƒ‡ãƒ¼ã‚¿] --> C[Cross-Attention]
    A2[çµ„æˆãƒ‡ãƒ¼ã‚¿] --> C
    A3[å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿] --> C

    B[Latent Array] --> C
    C --> D[Latent Transformer]
    D --> E[Cross-Attention Decoder]
    E --> F[äºˆæ¸¬çµæœ]

    style C fill:#e1f5ff
    style D fill:#ffe1e1
</div>

<h3>ç°¡æ˜“å®Ÿè£…</h3>
<pre><code class="language-python">class PerceiverBlock(nn.Module):
    def __init__(self, latent_dim, input_dim, num_latents=64):
        super(PerceiverBlock, self).__init__()
        self.num_latents = num_latents
        self.latent_dim = latent_dim

        # Latent arrayï¼ˆå­¦ç¿’å¯èƒ½ï¼‰
        self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))

        # Cross-Attention: Latent â†’ Input
        self.cross_attn = nn.MultiheadAttention(latent_dim, num_heads=8, batch_first=True)

        # Self-Attention: Latent â†’ Latent
        self.self_attn = nn.MultiheadAttention(latent_dim, num_heads=8, batch_first=True)

        # å…¥åŠ›ã‚’åŸ‹ã‚è¾¼ã¿
        self.input_projection = nn.Linear(input_dim, latent_dim)

    def forward(self, x):
        &quot;&quot;&quot;
        Args:
            x: (batch_size, seq_len, input_dim) å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        Returns:
            latents: (batch_size, num_latents, latent_dim)
        &quot;&quot;&quot;
        batch_size = x.size(0)

        # å…¥åŠ›ã‚’åŸ‹ã‚è¾¼ã¿
        x_embed = self.input_projection(x)

        # Latentã‚’è¤‡è£½
        latents = self.latents.unsqueeze(0).repeat(batch_size, 1, 1)

        # Cross-Attention: Latent (Query) â† Input (Key, Value)
        latents, _ = self.cross_attn(latents, x_embed, x_embed)

        # Self-Attention: Latentå†…éƒ¨
        latents, _ = self.self_attn(latents, latents, latents)

        return latents

# ä½¿ç”¨ä¾‹: æ§‹é€ ãƒ‡ãƒ¼ã‚¿ã¨çµ„æˆãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
batch_size = 2
seq_len = 20
input_dim = 128
latent_dim = 256

perceiver = PerceiverBlock(latent_dim, input_dim, num_latents=32)

# æ§‹é€ ãƒ‡ãƒ¼ã‚¿ï¼ˆä¾‹: åŸå­åº§æ¨™ï¼‰
structure_data = torch.randn(batch_size, seq_len, input_dim)

latents = perceiver(structure_data)
print(f&quot;Latent representation shape: {latents.shape}&quot;)  # (2, 32, 256)
</code></pre>
<hr />
<h2>2.6 å®Ÿè£…æ¼”ç¿’: Matformerã§ææ–™ç‰¹æ€§äºˆæ¸¬</h2>
<h3>å®Œå…¨ãªå®Ÿè£…ä¾‹</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
class MaterialsDataset(Dataset):
    def __init__(self, num_samples=100):
        self.num_samples = num_samples

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã¯Materials Projectãªã©ã‹ã‚‰å–å¾—ï¼‰
        num_atoms = 8
        atomic_numbers = torch.randint(1, 30, (num_atoms,))  # åŸå­ç•ªå·
        positions = torch.randn(num_atoms, 3)  # åŸå­åº§æ¨™ï¼ˆÃ…ï¼‰
        distance_matrix = torch.cdist(positions, positions)  # è·é›¢è¡Œåˆ—

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ï¼ˆeVï¼‰
        target = torch.randn(1)

        return atomic_numbers, distance_matrix, target

# Matformerãƒ¢ãƒ‡ãƒ«ï¼ˆç°¡ç•¥ç‰ˆï¼‰
class SimpleMatformer(nn.Module):
    def __init__(self, d_model=256, num_heads=8, num_layers=4):
        super(SimpleMatformer, self).__init__()

        self.atom_embedding = AtomEmbedding(num_atoms=118, d_model=d_model)

        self.layers = nn.ModuleList([
            MatformerBlock(d_model, num_heads)
            for _ in range(num_layers)
        ])

        self.pooling = nn.AdaptiveAvgPool1d(1)
        self.predictor = nn.Sequential(
            nn.Linear(d_model, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, atomic_numbers, distance_matrix):
        # åŸå­åŸ‹ã‚è¾¼ã¿
        x = self.atom_embedding(atomic_numbers)

        # Matformerãƒ–ãƒ­ãƒƒã‚¯
        for layer in self.layers:
            x = layer(x, distance_matrix)

        # Global pooling
        x = x.transpose(1, 2)  # (batch, d_model, num_atoms)
        x = self.pooling(x).squeeze(-1)  # (batch, d_model)

        # äºˆæ¸¬
        output = self.predictor(x)
        return output

# è¨“ç·´
def train_matformer():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # ãƒ‡ãƒ¼ã‚¿
    dataset = MaterialsDataset(num_samples=100)
    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

    # ãƒ¢ãƒ‡ãƒ«
    model = SimpleMatformer(d_model=256, num_heads=8, num_layers=4).to(device)

    # æœ€é©åŒ–
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    model.train()
    for epoch in range(5):
        total_loss = 0
        for atomic_numbers, distance_matrix, target in dataloader:
            atomic_numbers = atomic_numbers.to(device)
            distance_matrix = distance_matrix.to(device)
            target = target.to(device)

            # Forward
            predictions = model(atomic_numbers, distance_matrix)
            loss = criterion(predictions, target)

            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        print(f&quot;Epoch {epoch+1}, Loss: {avg_loss:.4f}&quot;)

    return model

# å®Ÿè¡Œ
trained_model = train_matformer()
</code></pre>
<hr />
<h2>2.7 ã¾ã¨ã‚</h2>
<h3>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</h3>
<ol>
<li><strong>Matformer</strong>: è·é›¢ã‚’è€ƒæ…®ã—ãŸAttentionã€éšå±¤çš„æ§‹é€ </li>
<li><strong>CrystalFormer</strong>: å‘¨æœŸå¢ƒç•Œæ¡ä»¶ã€åˆ†æ•°åº§æ¨™ã€ç©ºé–“ç¾¤</li>
<li><strong>ChemBERTa</strong>: SMILESè¡¨ç¾å­¦ç¿’ã€è»¢ç§»å­¦ç¿’</li>
<li><strong>Perceiver IO</strong>: å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿çµ±åˆ</li>
</ol>
<h3>æ¬¡ç« ã¸ã®æº–å‚™</h3>
<p>ç¬¬3ç« ã§ã¯ã€äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆMatBERTã€MolBERTï¼‰ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å­¦ã³ã¾ã™ã€‚</p>
<hr />
<h2>ğŸ“ æ¼”ç¿’å•é¡Œ</h2>
<h3>å•é¡Œ1: æ¦‚å¿µç†è§£</h3>
<p>Distance-aware AttentionãŒé€šå¸¸ã®Attentionã‚ˆã‚Šææ–™ç§‘å­¦ã§å„ªã‚Œã¦ã„ã‚‹ç†ç”±ã‚’3ã¤æŒ™ã’ã¦ãã ã•ã„ã€‚</p>
<details>
<summary>è§£ç­”ä¾‹</summary>

1. **åŒ–å­¦çµåˆã®è€ƒæ…®**: åŸå­é–“è·é›¢ãŒè¿‘ã„ã»ã©ç›¸äº’ä½œç”¨ãŒå¼·ã„ã¨ã„ã†ç‰©ç†æ³•å‰‡ã‚’åæ˜ 
2. **é•·è·é›¢ç›¸äº’ä½œç”¨ã®æŠ‘åˆ¶**: é ã„åŸå­ã¸ã®ä¸è¦ãªAttentionã‚’æ¸›ã‚‰ã—ã€è¨ˆç®—åŠ¹ç‡å‘ä¸Š
3. **è§£é‡ˆæ€§ã®å‘ä¸Š**: Attentioné‡ã¿ãŒåŒ–å­¦çš„ã«æ„å‘³ã®ã‚ã‚‹çµåˆå¼·åº¦ã¨å¯¾å¿œ
</details>

<h3>å•é¡Œ2: å®Ÿè£…</h3>
<p>å‘¨æœŸå¢ƒç•Œæ¡ä»¶ã‚’è€ƒæ…®ã›ãšã«è·é›¢ã‚’è¨ˆç®—ã™ã‚‹å˜ç´”ãªé–¢æ•°ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>
<pre><code class="language-python">def compute_simple_distance(coords1, coords2):
    &quot;&quot;&quot;
    å˜ç´”ãªè·é›¢è¨ˆç®—ï¼ˆå‘¨æœŸå¢ƒç•Œæ¡ä»¶ãªã—ï¼‰

    Args:
        coords1: (num_atoms1, 3)
        coords2: (num_atoms2, 3)
    Returns:
        distances: (num_atoms1, num_atoms2)
    &quot;&quot;&quot;
    # ã“ã“ã«å®Ÿè£…
    pass
</code></pre>
<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python">def compute_simple_distance(coords1, coords2):
    diff = coords1.unsqueeze(1) - coords2.unsqueeze(0)
    distances = torch.norm(diff, dim=-1)
    return distances
</code></pre>

</details>

<h3>å•é¡Œ3: å¿œç”¨</h3>
<p>ChemBERTaã‚’ä½¿ã£ã¦ã€åˆ†å­ã®æ°´æº¶è§£åº¦ã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚å¿…è¦ãªå±¤ã¨æ§‹æˆã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>
<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python">class SolubilityPredictor(nn.Module):
    def __init__(self):
        super(SolubilityPredictor, self).__init__()
        self.chemberta = ChemBERTaEmbedding()  # 768æ¬¡å…ƒ

        self.predictor = nn.Sequential(
            nn.Linear(768, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 1)  # æº¶è§£åº¦ï¼ˆé€£ç¶šå€¤ï¼‰
        )

    def forward(self, input_ids, attention_mask):
        embeddings = self.chemberta(input_ids, attention_mask)
        solubility = self.predictor(embeddings)
        return solubility
</code></pre>


**è¨­è¨ˆç†ç”±**:
- ChemBERTaã§åˆ†å­ã®ä¸€èˆ¬çš„ãªç‰¹å¾´ã‚’æŠ½å‡º
- 3å±¤ã®å…¨çµåˆå±¤ã§æº¶è§£åº¦ã«ç‰¹åŒ–ã—ãŸè¡¨ç¾ã«å¤‰æ›
- Dropoutã§éå­¦ç¿’ã‚’é˜²æ­¢
- å‡ºåŠ›ã¯é€£ç¶šå€¤ï¼ˆlog10(mol/L)ãªã©ï¼‰
</details>

<hr />
<h2>ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¨åˆ©ç”¨è¦ç´„</h2>
<h3>ææ–™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</h3>
<ul>
<li><strong>Materials Project</strong>: <a href="https://materialsproject.org/about/terms">CC BY 4.0</a></li>
<li>69ä¸‡ä»¥ä¸Šã®ææ–™ã®è¨ˆç®—ãƒ‡ãƒ¼ã‚¿</li>
<li>å¼•ç”¨: <code>Jain, A. et al. APL Materials 1, 011002 (2013)</code></li>
<li><strong>OQMD (Open Quantum Materials Database)</strong>: å­¦è¡“åˆ©ç”¨å¯</li>
<li>100ä¸‡ä»¥ä¸Šã®DFTè¨ˆç®—ãƒ‡ãƒ¼ã‚¿</li>
<li>å¼•ç”¨: <code>Saal, J. E. et al. JOM 65, 1501-1509 (2013)</code></li>
<li><strong>AFLOW</strong>: <a href="http://aflowlib.org/">CC BY 4.0</a></li>
<li>350ä¸‡ä»¥ä¸Šã®ææ–™ãƒ‡ãƒ¼ã‚¿</li>
</ul>
<h3>åˆ†å­ãƒ»åŒ–å­¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</h3>
<ul>
<li><strong>ZINC15</strong>: å­¦è¡“åˆ©ç”¨ç„¡æ–™ã€å•†ç”¨ã¯è¦å•ã„åˆã‚ã›</li>
<li>10å„„ä»¥ä¸Šã®å¸‚è²©åŒ–åˆç‰©</li>
<li><strong>ChEMBL</strong>: <a href="https://chembl.gitbook.io/chembl-interface-documentation/about#data-licensing">CC BY-SA 3.0</a></li>
<li>200ä¸‡ä»¥ä¸Šã®ç”Ÿç†æ´»æ€§åŒ–åˆç‰©</li>
<li><strong>QM9</strong>: <a href="https://figshare.com/collections/Quantum_chemistry_structures_and_properties_of_134_kilo_molecules/978904">CC0 1.0</a></li>
<li>13ä¸‡åˆ†å­ã®é‡å­åŒ–å­¦è¨ˆç®—ãƒ‡ãƒ¼ã‚¿</li>
</ul>
<h3>çµæ™¶æ§‹é€ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</h3>
<ul>
<li><strong>COD (Crystallography Open Database)</strong>: ãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³</li>
<li>50ä¸‡ä»¥ä¸Šã®çµæ™¶æ§‹é€ </li>
<li><strong>ICSD (Inorganic Crystal Structure Database)</strong>: ãƒ©ã‚¤ã‚»ãƒ³ã‚¹è³¼å…¥å¿…è¦</li>
<li>25ä¸‡ä»¥ä¸Šã®ç„¡æ©Ÿçµæ™¶æ§‹é€ </li>
</ul>
<hr />
<h2>ğŸ”§ ã‚³ãƒ¼ãƒ‰å†ç¾æ€§ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³</h2>
<h3>ç’°å¢ƒè¨­å®šï¼ˆææ–™ç§‘å­¦å‘ã‘ï¼‰</h3>
<pre><code class="language-python"># requirements.txt
torch==2.0.1
transformers==4.30.2
pymatgen==2023.8.10  # ææ–™ç§‘å­¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
rdkit==2023.3.2  # åŒ–å­¦è¨ˆç®—
ase==3.22.1  # åŸå­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒ
numpy==1.24.3
scipy==1.11.1
matplotlib==3.7.1
</code></pre>
<h3>è·é›¢è¡Œåˆ—è¨ˆç®—ã®å†ç¾æ€§</h3>
<pre><code class="language-python">import numpy as np
from pymatgen.core import Structure
from pymatgen.analysis.local_env import CrystalNN

def reproducible_distance_matrix(structure, cutoff=10.0, seed=42):
    &quot;&quot;&quot;
    å†ç¾å¯èƒ½ãªè·é›¢è¡Œåˆ—è¨ˆç®—

    Args:
        structure: pymatgen Structure object
        cutoff: è·é›¢ã‚«ãƒƒãƒˆã‚ªãƒ•ï¼ˆÃ…ï¼‰
        seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
    &quot;&quot;&quot;
    np.random.seed(seed)

    # åŸå­åº§æ¨™å–å¾—
    positions = structure.cart_coords

    # è·é›¢è¡Œåˆ—è¨ˆç®—ï¼ˆå‘¨æœŸå¢ƒç•Œæ¡ä»¶è€ƒæ…®ï¼‰
    distance_matrix = structure.distance_matrix

    # ã‚«ãƒƒãƒˆã‚ªãƒ•é©ç”¨
    distance_matrix[distance_matrix &gt; cutoff] = cutoff

    return distance_matrix

# ä½¿ç”¨ä¾‹
from pymatgen.core import Lattice, Structure

# NaClæ§‹é€ 
lattice = Lattice.cubic(5.64)
species = [&quot;Na&quot;, &quot;Cl&quot;]
coords = [[0, 0, 0], [0.5, 0.5, 0.5]]
structure = Structure(lattice, species, coords)

dist_matrix = reproducible_distance_matrix(structure)
print(f&quot;Distance matrix shape: {dist_matrix.shape}&quot;)
</code></pre>
<h3>Matformerã®è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</h3>
<pre><code class="language-python"># Matformerè¨­å®š
matformer_config = {
    'model': {
        'd_model': 256,
        'num_heads': 8,
        'num_layers': 4,
        'd_ff': 1024,
        'dropout': 0.1,
        'max_atoms': 50,  # æœ€å¤§åŸå­æ•°
        'max_distance': 10.0  # è·é›¢ã‚«ãƒƒãƒˆã‚ªãƒ•ï¼ˆÃ…ï¼‰
    },
    'distance_embedding': {
        'num_gaussians': 50,
        'gaussian_start': 0.0,
        'gaussian_end': 10.0
    },
    'training': {
        'batch_size': 32,
        'learning_rate': 1e-4,
        'num_epochs': 100,
        'scheduler': 'cosine_with_warmup',
        'warmup_epochs': 10
    },
    'seed': 42
}
</code></pre>
<hr />
<h2>âš ï¸ å®Ÿè·µçš„ãªè½ã¨ã—ç©´ã¨å¯¾å‡¦æ³•</h2>
<h3>1. å‘¨æœŸå¢ƒç•Œæ¡ä»¶ã®è¨ˆç®—ãƒŸã‚¹</h3>
<p><strong>å•é¡Œ</strong>: æ ¼å­å¢ƒç•Œã‚’è¶…ãˆãŸåŸå­é–“è·é›¢ãŒæ­£ã—ãè¨ˆç®—ã•ã‚Œãªã„</p>
<pre><code class="language-python"># âŒ é–“é•ã„: å˜ç´”ãªãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢
def wrong_periodic_distance(pos1, pos2, lattice):
    return np.linalg.norm(pos1 - pos2)

# âœ… æ­£ã—ã„: æœ€å°ã‚¤ãƒ¡ãƒ¼ã‚¸è¦ç´„
def correct_periodic_distance(frac1, frac2, lattice_matrix):
    &quot;&quot;&quot;
    å‘¨æœŸå¢ƒç•Œæ¡ä»¶ã‚’è€ƒæ…®ã—ãŸè·é›¢è¨ˆç®—

    Args:
        frac1, frac2: åˆ†æ•°åº§æ¨™ (3,)
        lattice_matrix: æ ¼å­ãƒ™ã‚¯ãƒˆãƒ« (3, 3)
    &quot;&quot;&quot;
    # æœ€å°ã‚¤ãƒ¡ãƒ¼ã‚¸è¦ç´„: -0.5 &lt;= delta &lt; 0.5
    delta_frac = frac1 - frac2
    delta_frac = delta_frac - np.floor(delta_frac + 0.5)

    # ãƒ‡ã‚«ãƒ«ãƒˆåº§æ¨™ã«å¤‰æ›
    delta_cart = np.dot(delta_frac, lattice_matrix)

    return np.linalg.norm(delta_cart)

# ãƒ‡ãƒãƒƒã‚°æ–¹æ³•
print(&quot;Fractional coordinates:&quot;, frac1, frac2)
print(&quot;Delta (before wrapping):&quot;, frac1 - frac2)
print(&quot;Delta (after wrapping):&quot;, delta_frac)
print(&quot;Minimum distance:&quot;, correct_periodic_distance(frac1, frac2, lattice_matrix))
</code></pre>
<h3>2. åŸå­ç•ªå·åŸ‹ã‚è¾¼ã¿ã®æ¬ æå€¤å‡¦ç†</h3>
<p><strong>å•é¡Œ</strong>: æœªçŸ¥ã®åŸå­ç¨®ã‚„ç©ºã‚µã‚¤ãƒˆã®å‡¦ç†</p>
<pre><code class="language-python"># âŒ å•é¡Œ: æœªçŸ¥åŸå­ã§ã‚¨ãƒ©ãƒ¼
class NaiveAtomEmbedding(nn.Module):
    def __init__(self):
        self.embedding = nn.Embedding(118, 256)  # å…ƒç´ 118ç¨®

    def forward(self, atomic_numbers):
        return self.embedding(atomic_numbers)  # 118ä»¥ä¸Šã§ã‚¨ãƒ©ãƒ¼ï¼

# âœ… è§£æ±ºç­–: æœªçŸ¥ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ 
class RobustAtomEmbedding(nn.Module):
    def __init__(self, num_atoms=118, d_model=256):
        super().__init__()
        # +1ã¯æœªçŸ¥åŸå­ç”¨
        self.embedding = nn.Embedding(num_atoms + 1, d_model, padding_idx=0)
        self.num_atoms = num_atoms

    def forward(self, atomic_numbers):
        # ç¯„å›²å¤–ã®åŸå­ç•ªå·ã‚’ã‚¯ãƒªãƒƒãƒ—
        atomic_numbers = torch.clamp(atomic_numbers, 0, self.num_atoms)
        return self.embedding(atomic_numbers)

# ãƒ†ã‚¹ãƒˆ
embedding = RobustAtomEmbedding()
test_atoms = torch.tensor([1, 6, 8, 200])  # 200ã¯ç¯„å›²å¤–
output = embedding(test_atoms)
print(f&quot;Output shape: {output.shape}&quot;)  # ã‚¨ãƒ©ãƒ¼ãªã—
</code></pre>
<h3>3. Distance-aware Attentionã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡</h3>
<p><strong>å•é¡Œ</strong>: è·é›¢è¡Œåˆ—ãŒå¤§ãã™ãã¦OOM</p>
<pre><code class="language-python"># âŒ å•é¡Œ: å…¨åŸå­ãƒšã‚¢ã®è·é›¢ã‚’ä¿æŒ
def memory_intensive_distance_attention(x, all_distances):
    # all_distances: (batch, max_atoms, max_atoms)
    # ãƒ¡ãƒ¢ãƒª: batch * max_atoms^2 * 4 bytes
    pass

# âœ… è§£æ±ºç­–: ã‚¹ãƒ‘ãƒ¼ã‚¹è·é›¢è¡Œåˆ—
def sparse_distance_attention(x, positions, cutoff=8.0):
    &quot;&quot;&quot;
    ã‚«ãƒƒãƒˆã‚ªãƒ•è·é›¢å†…ã®ã¿è¨ˆç®—

    Args:
        x: åŸå­ç‰¹å¾´é‡ (batch, num_atoms, d_model)
        positions: åŸå­åº§æ¨™ (batch, num_atoms, 3)
        cutoff: è·é›¢ã‚«ãƒƒãƒˆã‚ªãƒ•ï¼ˆÃ…ï¼‰
    &quot;&quot;&quot;
    batch_size, num_atoms, _ = positions.shape

    # è·é›¢è¡Œåˆ—è¨ˆç®—
    diff = positions.unsqueeze(2) - positions.unsqueeze(1)
    distances = torch.norm(diff, dim=-1)

    # ã‚«ãƒƒãƒˆã‚ªãƒ•ãƒã‚¹ã‚¯
    mask = distances &lt; cutoff

    # Attentionã‚¹ã‚³ã‚¢ï¼ˆã‚«ãƒƒãƒˆã‚ªãƒ•å¤–ã¯-infï¼‰
    scores = compute_attention_scores(x)
    scores = scores.masked_fill(~mask, float('-inf'))

    return scores
</code></pre>
<h3>4. SMILESãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®ç‰¹æ®Šæ–‡å­—å‡¦ç†</h3>
<p><strong>å•é¡Œ</strong>: èŠ³é¦™æ—æ€§ã€ç«‹ä½“åŒ–å­¦ã®è¨˜å·ãŒå¤±ã‚ã‚Œã‚‹</p>
<pre><code class="language-python"># âŒ é–“é•ã„: ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã¨æ•°å­—ã®ã¿
def wrong_smiles_tokenize(smiles):
    return list(filter(str.isalnum, smiles))

# âœ… æ­£ã—ã„: ç‰¹æ®Šæ–‡å­—ã‚’ä¿æŒ
def correct_smiles_tokenize(smiles):
    &quot;&quot;&quot;
    SMILESå®Œå…¨ãƒˆãƒ¼ã‚¯ãƒ³åŒ–

    èŠ³é¦™æ—æ€§: c, n, o, s (å°æ–‡å­—)
    ç«‹ä½“åŒ–å­¦: @, @@
    åˆ†å²: (), []
    çµåˆ: -, =, #, :, /,
    &quot;&quot;&quot;
    import re
    pattern = r'(\[[^\]]+\]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\(|\)|\.|=|#|-|\+|\\|\/|:|~|@|\?|&gt;|\*|\$|\%[0-9]{2}|[0-9])'
    tokens = re.findall(pattern, smiles)

    return tokens

# ãƒ†ã‚¹ãƒˆ: è¤‡é›‘ãªSMILES
smiles = &quot;C[C@H](N)C(=O)O&quot;  # L-ã‚¢ãƒ©ãƒ‹ãƒ³ï¼ˆç«‹ä½“åŒ–å­¦ã‚ã‚Šï¼‰
tokens = correct_smiles_tokenize(smiles)
print(f&quot;Tokens: {tokens}&quot;)
# ['C', '[C@H]', '(', 'N', ')', 'C', '(', '=', 'O', ')', 'O']
</code></pre>
<h3>5. CrystalFormerã®ç©ºé–“ç¾¤ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°</h3>
<p><strong>å•é¡Œ</strong>: 230å€‹ã®ç©ºé–“ç¾¤ã®åŠ¹ç‡çš„è¡¨ç¾</p>
<pre><code class="language-python"># âŒ éåŠ¹ç‡: One-hot encoding (230æ¬¡å…ƒ)
def inefficient_space_group_encoding(space_group_number):
    encoding = torch.zeros(230)
    encoding[space_group_number - 1] = 1
    return encoding

# âœ… åŠ¹ç‡çš„: Learnable embedding
class SpaceGroupEmbedding(nn.Module):
    def __init__(self, d_model=64):
        super().__init__()
        # 230å€‹ã®ç©ºé–“ç¾¤ + æœªçŸ¥ç”¨
        self.embedding = nn.Embedding(231, d_model)

    def forward(self, space_group_numbers):
        &quot;&quot;&quot;
        Args:
            space_group_numbers: (batch,) 1-230ã®ç©ºé–“ç¾¤ç•ªå·
        &quot;&quot;&quot;
        return self.embedding(space_group_numbers)

# ã•ã‚‰ã«: ç©ºé–“ç¾¤ã®éšå±¤æ§‹é€ ã‚’åˆ©ç”¨
class HierarchicalSpaceGroupEmbedding(nn.Module):
    def __init__(self, d_model=64):
        super().__init__()
        # çµæ™¶ç³»ï¼ˆ7ç¨®é¡ï¼‰
        self.crystal_system_emb = nn.Embedding(8, d_model // 2)
        # ç‚¹ç¾¤ï¼ˆ32ç¨®é¡ï¼‰
        self.point_group_emb = nn.Embedding(33, d_model // 2)

    def forward(self, crystal_system, point_group):
        cs_emb = self.crystal_system_emb(crystal_system)
        pg_emb = self.point_group_emb(point_group)
        return torch.cat([cs_emb, pg_emb], dim=-1)
</code></pre>
<hr />
<h2>âœ… ç¬¬2ç« å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ</h2>
<h3>æ¦‚å¿µç†è§£ï¼ˆ10é …ç›®ï¼‰</h3>
<ul>
<li>[ ] æ±ç”¨Transformerã¨ææ–™ç‰¹åŒ–Transformerã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>[ ] Matformerã®éšå±¤çš„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] Distance-aware Attentionã®åŸç†ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] å‘¨æœŸå¢ƒç•Œæ¡ä»¶ã®é‡è¦æ€§ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] åˆ†æ•°åº§æ¨™ã¨ãƒ‡ã‚«ãƒ«ãƒˆåº§æ¨™ã®å¤‰æ›ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] CrystalFormerã®ç©ºé–“ç¾¤ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] ChemBERTaã¨RoBERTaã®é–¢ä¿‚ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] Perceiver IOã®ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] ææ–™ç‰¹åŒ–TransformerãŒå¿…è¦ãªç†ç”±ã‚’3ã¤ä»¥ä¸ŠæŒ™ã’ã‚‰ã‚Œã‚‹</li>
<li>[ ] å„ãƒ¢ãƒ‡ãƒ«ï¼ˆMatformerã€CrystalFormerã€ChemBERTaï¼‰ã®é©ç”¨å ´é¢ã‚’èª¬æ˜ã§ãã‚‹</li>
</ul>
<h3>æ•°å¼ãƒ»ç‰©ç†ç†è§£ï¼ˆ5é …ç›®ï¼‰</h3>
<ul>
<li>[ ] å‘¨æœŸå¢ƒç•Œæ¡ä»¶ä¸‹ã®è·é›¢è¨ˆç®—å¼ã‚’å°å‡ºã§ãã‚‹</li>
<li>[ ] åˆ†æ•°åº§æ¨™ã‹ã‚‰ãƒ‡ã‚«ãƒ«ãƒˆåº§æ¨™ã¸ã®å¤‰æ›è¡Œåˆ—ã‚’æ›¸ã‘ã‚‹</li>
<li>[ ] ã‚¬ã‚¦ã‚¹åŸºåº•é–¢æ•°ã«ã‚ˆã‚‹è·é›¢åŸ‹ã‚è¾¼ã¿ã®æ•°å¼ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] æ ¼å­ãƒ™ã‚¯ãƒˆãƒ«ã¨é€†æ ¼å­ãƒ™ã‚¯ãƒˆãƒ«ã®é–¢ä¿‚ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] Wyckoffä½ç½®ã®æ¦‚å¿µã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
</ul>
<h3>å®Ÿè£…ã‚¹ã‚­ãƒ«ï¼ˆ15é …ç›®ï¼‰</h3>
<ul>
<li>[ ] <code>AtomEmbedding</code>ã‚¯ãƒ©ã‚¹ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] <code>DistanceAwareAttention</code>ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] <code>MatformerBlock</code>ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] å‘¨æœŸå¢ƒç•Œæ¡ä»¶ã‚’è€ƒæ…®ã—ãŸè·é›¢è¨ˆç®—ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] åˆ†æ•°åº§æ¨™ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] <code>ChemBERTaEmbedding</code>ã‚’ä½¿ãˆã‚‹ã‚ˆã†ã«ã§ãã‚‹</li>
<li>[ ] SMILESãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å®Ÿè£…ãƒ»ä½¿ç”¨ã§ãã‚‹</li>
<li>[ ] <code>PerceiverBlock</code>ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] PyMatGenã‚’ä½¿ã£ã¦çµæ™¶æ§‹é€ ã‚’èª­ã¿è¾¼ã‚ã‚‹</li>
<li>[ ] Materials Projectãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã‚‹</li>
<li>[ ] è·é›¢è¡Œåˆ—ã‚’å¯è¦–åŒ–ã§ãã‚‹</li>
<li>[ ] Attentioné‡ã¿ã‚’ææ–™ç§‘å­¦çš„ã«è§£é‡ˆã§ãã‚‹</li>
<li>[ ] ãƒãƒƒãƒå‡¦ç†ã§ç•°ãªã‚‹åŸå­æ•°ã‚’æ‰±ãˆã‚‹ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼‰</li>
<li>[ ] ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] äºˆæ¸¬çµæœã‚’è©•ä¾¡ã§ãã‚‹ï¼ˆMAEã€RMSEï¼‰</li>
</ul>
<h3>ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚­ãƒ«ï¼ˆ5é …ç›®ï¼‰</h3>
<ul>
<li>[ ] å‘¨æœŸå¢ƒç•Œæ¡ä»¶ã®è¨ˆç®—ãƒŸã‚¹ã‚’æ¤œå‡ºã§ãã‚‹</li>
<li>[ ] åŸå­ç•ªå·ã®ç¯„å›²å¤–ã‚¨ãƒ©ãƒ¼ã‚’ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã§ãã‚‹</li>
<li>[ ] ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®å•é¡Œã‚’ç‰¹å®šãƒ»è§£æ±ºã§ãã‚‹</li>
<li>[ ] SMILESãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®èª¤ã‚Šã‚’ãƒ‡ãƒãƒƒã‚°ã§ãã‚‹</li>
<li>[ ] è·é›¢è¡Œåˆ—ã®å¯¾ç§°æ€§ã‚’æ¤œè¨¼ã§ãã‚‹</li>
</ul>
<h3>å¿œç”¨åŠ›ï¼ˆ5é …ç›®ï¼‰</h3>
<ul>
<li>[ ] æ–°ã—ã„ææ–™ç‰¹æ€§äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã«Matformerã‚’é©ç”¨ã§ãã‚‹</li>
<li>[ ] ChemBERTaã‚’åˆ†å­ç‰¹æ€§äºˆæ¸¬ã«å¿œç”¨ã§ãã‚‹</li>
<li>[ ] è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆæ§‹é€ +çµ„æˆï¼‰ã‚’çµ±åˆã§ãã‚‹</li>
<li>[ ] æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’æ‹¡å¼µã—ã¦æ–°æ©Ÿèƒ½ã‚’è¿½åŠ ã§ãã‚‹</li>
<li>[ ] äºˆæ¸¬çµæœã‚’å®Ÿé¨“è¨­è¨ˆã«æ´»ç”¨ã™ã‚‹æ–¹æ³•ã‚’ææ¡ˆã§ãã‚‹</li>
</ul>
<h3>ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆ5é …ç›®ï¼‰</h3>
<ul>
<li>[ ] Materials Projectãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ã§ãã‚‹</li>
<li>[ ] SMILESã‚’æ­£è¦åŒ–ï¼ˆcanonicalizeï¼‰ã§ãã‚‹</li>
<li>[ ] çµæ™¶æ§‹é€ ã‚’æ¨™æº–åŒ–ï¼ˆprimitive cellï¼‰ã§ãã‚‹</li>
<li>[ ] ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆaugmentationï¼‰ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] è¨“ç·´/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’é©åˆ‡ã«åˆ†å‰²ã§ãã‚‹</li>
</ul>
<h3>ç†è«–çš„èƒŒæ™¯ï¼ˆ5é …ç›®ï¼‰</h3>
<ul>
<li>[ ] Matformerè«–æ–‡ï¼ˆChen et al., 2022ï¼‰ã‚’èª­ã‚“ã </li>
<li>[ ] ChemBERTaè«–æ–‡ã‚’èª­ã‚“ã </li>
<li>[ ] Perceiverè«–æ–‡ã‚’èª­ã‚“ã </li>
<li>[ ] çµæ™¶å­¦ã®åŸºç¤ï¼ˆBravaisæ ¼å­ã€ç©ºé–“ç¾¤ï¼‰ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] ææ–™ç§‘å­¦ã®åŸºç¤ï¼ˆçµåˆã€æ ¼å­æ¬ é™¥ï¼‰ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
</ul>
<h3>å®Œäº†åŸºæº–</h3>
<ul>
<li><strong>æœ€ä½åŸºæº–</strong>: 40é …ç›®ä»¥ä¸Šé”æˆï¼ˆ80%ï¼‰</li>
<li><strong>æ¨å¥¨åŸºæº–</strong>: 45é …ç›®ä»¥ä¸Šé”æˆï¼ˆ90%ï¼‰</li>
<li><strong>å„ªç§€åŸºæº–</strong>: 50é …ç›®å…¨ã¦é”æˆï¼ˆ100%ï¼‰</li>
</ul>
<hr />
<p><strong>æ¬¡ç« </strong>: <strong><a href="chapter-3.html">ç¬¬3ç« : äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¨è»¢ç§»å­¦ç¿’</a></strong></p>
<hr />
<p><strong>ä½œæˆè€…</strong>: æ©‹æœ¬ä½‘ä»‹ï¼ˆæ±åŒ—å¤§å­¦ï¼‰
<strong>æœ€çµ‚æ›´æ–°</strong>: 2025å¹´10æœˆ19æ—¥</p><div class="navigation">
    <a href="chapter-1.html" class="nav-button">â† å‰ã®ç« </a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter-3.html" class="nav-button">æ¬¡ã®ç«  â†’</a>
</div>
    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
