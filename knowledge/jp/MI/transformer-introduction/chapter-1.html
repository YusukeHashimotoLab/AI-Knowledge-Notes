<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Chapter</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 20-25ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ÂàùÁ¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 0ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 0Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Á¨¨1Á´†: TransformerÈù©ÂëΩ„Å®ÊùêÊñôÁßëÂ≠¶</h1>
<p><strong>Â≠¶ÁøíÊôÇÈñì</strong>: 20-30ÂàÜ | <strong>Èõ£ÊòìÂ∫¶</strong>: ‰∏≠Á¥ö</p>
<h2>üìã „Åì„ÅÆÁ´†„ÅßÂ≠¶„Å∂„Åì„Å®</h2>
<ul>
<li>AttentionÊ©üÊßã„ÅÆÂéüÁêÜ„Å®Êï∞Â≠¶ÁöÑÁêÜËß£</li>
<li>Self-Attention„Å®Multi-Head Attention„ÅÆ‰ªïÁµÑ„Åø</li>
<li>Transformer„ÅåRNN/CNN„Çà„ÇäÂÑ™„Çå„Å¶„ÅÑ„ÇãÁêÜÁî±</li>
<li>BERT„ÄÅGPT„ÅÆÂü∫Êú¨ÊßãÈÄ†„Å®ÈÅï„ÅÑ</li>
<li>ÊùêÊñôÁßëÂ≠¶„Åß„ÅÆÊàêÂäü‰∫ã‰æã</li>
</ul>
<hr />
<h2>1.1 „Å™„ÅúTransformer„ÅåÈù©ÂëΩ„ÇíËµ∑„Åì„Åó„Åü„ÅÆ„Åã</h2>
<h3>ÂæìÊù•„ÅÆRNN/CNN„ÅÆÈôêÁïå</h3>
<p><strong>RNNÔºàRecurrent Neural NetworkÔºâ„ÅÆÂïèÈ°å</strong>:
- Èï∑„ÅÑÁ≥ªÂàó„Åß„ÅÆÂãæÈÖçÊ∂àÂ§±„ÉªÁàÜÁô∫
- ‰∏¶ÂàóÂåñ„ÅåÂõ∞Èõ£ÔºàÈÄêÊ¨°Âá¶ÁêÜ„ÅåÂøÖË¶ÅÔºâ
- Èï∑Êúü‰æùÂ≠òÈñ¢‰øÇ„ÅÆÊçïÊçâ„ÅåÈõ£„Åó„ÅÑ</p>
<p><strong>CNNÔºàConvolutional Neural NetworkÔºâ„ÅÆÂïèÈ°å</strong>:
- Â±ÄÊâÄÁöÑ„Å™ÁâπÂæ¥„Åó„ÅãÊçâ„Åà„Çâ„Çå„Å™„ÅÑ
- Èï∑Ë∑ùÈõ¢„ÅÆÈñ¢‰øÇÊÄß„ÇíÊçâ„Åà„Çã„Å´„ÅØÊ∑±„ÅÑÂ±§„ÅåÂøÖË¶Å
- ÂàÜÂ≠ê„ÉªÊùêÊñô„ÅÆ„Çà„ÅÜ„Å™‰∏çË¶èÂâá„Å™ÊßãÈÄ†„Å´„ÅØ‰∏çÂêë„Åç</p>
<h3>Transformer„ÅÆÈù©Êñ∞ÊÄß</h3>
<p><strong>2017Âπ¥„ÄÅ"Attention Is All You Need"Ë´ñÊñá„ÅßÁôªÂ†¥</strong>:
- ‚úÖ <strong>ÂÖ®Ë¶ÅÁ¥†Èñì„ÅÆÈñ¢‰øÇ„ÇíÁõ¥Êé•„É¢„Éá„É´Âåñ</strong>ÔºàAttentionÊ©üÊßãÔºâ
- ‚úÖ <strong>ÂÆåÂÖ®‰∏¶ÂàóÂåñÂèØËÉΩ</strong>ÔºàGPU„ÇíÊúÄÂ§ßÈôêÊ¥ªÁî®Ôºâ
- ‚úÖ <strong>Èï∑Ë∑ùÈõ¢‰æùÂ≠òÈñ¢‰øÇ„ÇíÂäπÁéáÁöÑ„Å´ÊçïÊçâ</strong>
- ‚úÖ <strong>Ëß£ÈáàÊÄß</strong>ÔºàAttentionÈáç„Åø„ÅßÈáçË¶Å„Å™ÈÉ®ÂàÜ„ÇíÂèØË¶ñÂåñÔºâ</p>
<div class="mermaid">
graph LR
    A[ÂÖ•ÂäõÁ≥ªÂàó] --> B[Self-Attention]
    B --> C[Feed Forward]
    C --> D[Âá∫Âäõ]

    B -.-> E[„Åô„Åπ„Å¶„ÅÆË¶ÅÁ¥†Èñì„ÅÆÈñ¢‰øÇ„ÇíË®àÁÆó]
    E -.-> B

    style B fill:#e1f5ff
</div>

<hr />
<h2>1.2 AttentionÊ©üÊßã„ÅÆÂéüÁêÜ</h2>
<h3>AttentionÊ©üÊßã„Å®„ÅØ</h3>
<p><strong>Âü∫Êú¨Ê¶ÇÂøµ</strong>: ÂÖ•Âäõ„ÅÆ‰∏≠„Åß„Äå„Å©„Åì„Å´Ê≥®ÁõÆ„Åô„Åπ„Åç„Åã„Äç„ÇíÂ≠¶Áøí„Åô„Çã‰ªïÁµÑ„Åø</p>
<p><strong>Êï∞Âºè</strong>:
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$</p>
<ul>
<li><strong>Q (Query)</strong>: „Äå‰Ωï„ÇíÊé¢„Åó„Å¶„ÅÑ„Çã„Åã„Äç</li>
<li><strong>K (Key)</strong>: „Äå‰Ωï„ÇíÊåÅ„Å£„Å¶„ÅÑ„Çã„Åã„Äç</li>
<li><strong>V (Value)</strong>: „ÄåÂÆüÈöõ„ÅÆÂÜÖÂÆπ„Äç</li>
<li>$d_k$: Key„ÅÆÊ¨°ÂÖÉÔºà„Çπ„Ç±„Éº„É™„É≥„Ç∞Âõ†Â≠êÔºâ</li>
</ul>
<h3>Áõ¥ÊÑüÁöÑÁêÜËß£</h3>
<p><strong>Âõ≥Êõ∏È§®„ÅÆ‰æã„Åà</strong>:
- <strong>Query</strong>: „ÄåÊ©üÊ¢∞Â≠¶Áøí„ÅÆÊú¨„ÇíÊé¢„Åó„Å¶„ÅÑ„Çã„Äç
- <strong>Key</strong>: ÂêÑÊú¨„ÅÆÁõÆÊ¨°„Éª„Çø„Ç§„Éà„É´
- <strong>Value</strong>: Êú¨„ÅÆÂÆüÈöõ„ÅÆÂÜÖÂÆπ
- <strong>Attention</strong>: Èñ¢ÈÄ£ÊÄß„ÅåÈ´ò„ÅÑÊú¨„Å´„ÄåÊ≥®ÁõÆ„Äç„Åó„Å¶Ë™≠„ÇÄ</p>
<h3>PythonÂÆüË£Ö: Âü∫Êú¨ÁöÑ„Å™Attention</h3>
<pre><code class="language-python">import torch
import torch.nn.functional as F

def scaled_dot_product_attention(Q, K, V, mask=None):
    &quot;&quot;&quot;
    Scaled Dot-Product Attention

    Args:
        Q: Query (batch_size, seq_len, d_k)
        K: Key (batch_size, seq_len, d_k)
        V: Value (batch_size, seq_len, d_v)
        mask: „Éû„Çπ„ÇØÔºà„Ç™„Éó„Ç∑„Éß„É≥Ôºâ
    &quot;&quot;&quot;
    d_k = Q.size(-1)

    # 1. Q„Å®K„ÅÆÂÜÖÁ©ç„ÇíË®àÁÆóÔºàÈ°û‰ººÂ∫¶Ôºâ
    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
    # scores shape: (batch_size, seq_len_q, seq_len_k)

    # 2. „Éû„Çπ„ÇØÈÅ©Áî®ÔºàÂøÖË¶Å„Å™Â†¥ÂêàÔºâ
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)

    # 3. Softmax„ÅßÊ≠£Ë¶èÂåñÔºàAttentionÈáç„ÅøÔºâ
    attention_weights = F.softmax(scores, dim=-1)

    # 4. AttentionÈáç„Åø„ÅßValue„ÇíÈáç„Åø‰ªò„ÅëÂíå
    output = torch.matmul(attention_weights, V)

    return output, attention_weights

# ‰ΩøÁî®‰æã
batch_size, seq_len, d_model = 2, 5, 64
Q = torch.randn(batch_size, seq_len, d_model)
K = torch.randn(batch_size, seq_len, d_model)
V = torch.randn(batch_size, seq_len, d_model)

output, attn_weights = scaled_dot_product_attention(Q, K, V)
print(f&quot;Output shape: {output.shape}&quot;)  # (2, 5, 64)
print(f&quot;Attention weights shape: {attn_weights.shape}&quot;)  # (2, 5, 5)
</code></pre>
<h3>AttentionÈáç„Åø„ÅÆÂèØË¶ñÂåñ</h3>
<pre><code class="language-python">import matplotlib.pyplot as plt
import seaborn as sns

def visualize_attention(attention_weights, tokens=None):
    &quot;&quot;&quot;
    AttentionÈáç„Åø„Çí„Éí„Éº„Éà„Éû„ÉÉ„Éó„ÅßÂèØË¶ñÂåñ

    Args:
        attention_weights: (seq_len, seq_len)„ÅÆAttentionÈáç„Åø
        tokens: „Éà„Éº„ÇØ„É≥„ÅÆ„É™„Çπ„ÉàÔºà„Ç™„Éó„Ç∑„Éß„É≥Ôºâ
    &quot;&quot;&quot;
    plt.figure(figsize=(8, 6))

    # ÊúÄÂàù„ÅÆ„Çµ„É≥„Éó„É´„ÅÆÊúÄÂàù„ÅÆ„Éò„ÉÉ„Éâ„ÅÆAttentionÈáç„Åø„ÇíÂèñÂæó
    attn = attention_weights[0].detach().numpy()

    sns.heatmap(attn, cmap='YlOrRd', cbar=True, square=True,
                xticklabels=tokens if tokens else range(attn.shape[0]),
                yticklabels=tokens if tokens else range(attn.shape[0]))

    plt.xlabel('Key (ÂèÇÁÖßÂÖà)')
    plt.ylabel('Query (Ê≥®ÁõÆÂÖÉ)')
    plt.title('Attention Weights')
    plt.tight_layout()
    plt.show()

# ‰ΩøÁî®‰æã
tokens = ['H', 'C', 'C', 'O', 'H']
visualize_attention(attn_weights, tokens)
</code></pre>
<hr />
<h2>1.3 Self-Attention: Ëá™Â∑±Ê≥®ÊÑèÊ©üÊßã</h2>
<h3>Self-Attention„Å®„ÅØ</h3>
<p><strong>ÂÆöÁæ©</strong>: ÂÖ•ÂäõÁ≥ªÂàóËá™Ë∫´„Å´ÂØæ„Åó„Å¶Attention„ÇíÈÅ©Áî®„Åô„Çã‰ªïÁµÑ„Åø</p>
<p><strong>ÁâπÂæ¥</strong>:
- Query„ÄÅKey„ÄÅValue„Åô„Åπ„Å¶Âêå„ÅòÂÖ•Âäõ„Åã„ÇâÁîüÊàê
- Á≥ªÂàóÂÜÖ„ÅÆ‰ªªÊÑè„ÅÆ2Ë¶ÅÁ¥†Èñì„ÅÆÈñ¢‰øÇ„ÇíÁõ¥Êé•„É¢„Éá„É´Âåñ
- ‰ΩçÁΩÆ„Å´Èñ¢„Çè„Çâ„Åö„ÄÅÈñ¢ÈÄ£ÊÄß„ÅÆÈ´ò„ÅÑË¶ÅÁ¥†„Å´Ê≥®ÁõÆ</p>
<h3>ÂàÜÂ≠ê„Å´„Åä„Åë„ÇãSelf-Attention„ÅÆ‰æã</h3>
<p><strong>„É°„Çø„Éé„Éº„É´ (CH‚ÇÉOH)„ÅÆ‰æã</strong>:</p>
<pre><code class="language-python"># ÂéüÂ≠ê: C, H, H, H, O, H
# Self-Attention„ÅßÂêÑÂéüÂ≠ê„Åå‰ªñ„ÅÆÂéüÂ≠ê„Å®„ÅÆÈñ¢‰øÇ„ÇíÂ≠¶Áøí
# ‰æã: OÂéüÂ≠ê„ÅØCÂéüÂ≠ê„Å®Âº∑„ÅÑÈñ¢‰øÇÊÄß„ÇíÊåÅ„Å§
</code></pre>
<h3>Self-AttentionÂÆüË£Ö</h3>
<pre><code class="language-python">import torch.nn as nn

class SelfAttention(nn.Module):
    def __init__(self, d_model):
        super(SelfAttention, self).__init__()
        self.d_model = d_model

        # Q, K, V„Å∏„ÅÆÁ∑öÂΩ¢Â§âÊèõ
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)

    def forward(self, x):
        &quot;&quot;&quot;
        Args:
            x: (batch_size, seq_len, d_model)
        &quot;&quot;&quot;
        # Q, K, V„ÇíÁîüÊàê
        Q = self.W_q(x)
        K = self.W_k(x)
        V = self.W_v(x)

        # Scaled Dot-Product Attention
        output, attn_weights = scaled_dot_product_attention(Q, K, V)

        return output, attn_weights

# ‰ΩøÁî®‰æã
d_model = 128
seq_len = 10
batch_size = 4

self_attn = SelfAttention(d_model)
x = torch.randn(batch_size, seq_len, d_model)
output, attn_weights = self_attn(x)

print(f&quot;Input shape: {x.shape}&quot;)          # (4, 10, 128)
print(f&quot;Output shape: {output.shape}&quot;)    # (4, 10, 128)
print(f&quot;Attention shape: {attn_weights.shape}&quot;)  # (4, 10, 10)
</code></pre>
<hr />
<h2>1.4 Multi-Head Attention: Â§öÈ†≠Ê≥®ÊÑèÊ©üÊßã</h2>
<h3>„Å™„ÅúMulti-Head„ÅåÂøÖË¶Å„Åã</h3>
<p><strong>Âçò‰∏Ä„ÅÆAttention„Éò„ÉÉ„Éâ„ÅÆÈôêÁïå</strong>:
- 1„Å§„ÅÆË¶ñÁÇπ„Åã„Çâ„Åó„ÅãÈñ¢‰øÇÊÄß„ÇíË¶ã„Çâ„Çå„Å™„ÅÑ
- Ë§áÈõë„Å™Èñ¢‰øÇÊÄßÔºàÂåñÂ≠¶ÁµêÂêà„ÄÅÁ´ã‰ΩìÈÖçÂ∫ß„Å™„Å©Ôºâ„ÇíÊçâ„Åà„Åç„Çå„Å™„ÅÑ</p>
<p><strong>Multi-Head Attention„ÅÆÂà©ÁÇπ</strong>:
- Ë§áÊï∞„ÅÆÁï∞„Å™„ÇãË¶ñÁÇπ„Åã„ÇâÈñ¢‰øÇÊÄß„ÇíÂ≠¶Áøí
- ÂêÑ„Éò„ÉÉ„Éâ„ÅåÁï∞„Å™„ÇãÁâπÂæ¥ÔºàÁµêÂêà„ÄÅË∑ùÈõ¢„ÄÅËßíÂ∫¶„Å™„Å©Ôºâ„ÇíÊçâ„Åà„Çã
- „Çà„ÇäË±ä„Åã„Å™Ë°®Áèæ„ÅåÂèØËÉΩ</p>
<h3>Êï∞Âºè</h3>
<p>$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
$$</p>
<p>where $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$</p>
<h3>ÂÆüË£Ö</h3>
<pre><code class="language-python">class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0, &quot;d_model„ÅØnum_heads„ÅßÂâ≤„ÇäÂàá„Çå„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô&quot;

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # Q, K, VÂ§âÊèõ
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)

        # Âá∫ÂäõÂ§âÊèõ
        self.W_o = nn.Linear(d_model, d_model)

    def forward(self, x, mask=None):
        batch_size = x.size(0)

        # 1. Q, K, V„ÇíÁîüÊàê„Åó„Å¶„ÄÅ„Éò„ÉÉ„Éâ„Åî„Å®„Å´ÂàÜÂâ≤
        Q = self.W_q(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        # Shape: (batch_size, num_heads, seq_len, d_k)

        # 2. ÂêÑ„Éò„ÉÉ„Éâ„ÅßScaled Dot-Product Attention
        output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)
        # output: (batch_size, num_heads, seq_len, d_k)

        # 3. „Éò„ÉÉ„Éâ„ÇíÈÄ£Áµê
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        # Shape: (batch_size, seq_len, d_model)

        # 4. Âá∫ÂäõÂ§âÊèõ
        output = self.W_o(output)

        return output, attn_weights

# ‰ΩøÁî®‰æã
d_model = 512
num_heads = 8
seq_len = 20
batch_size = 2

mha = MultiHeadAttention(d_model, num_heads)
x = torch.randn(batch_size, seq_len, d_model)
output, attn_weights = mha(x)

print(f&quot;Input shape: {x.shape}&quot;)          # (2, 20, 512)
print(f&quot;Output shape: {output.shape}&quot;)    # (2, 20, 512)
print(f&quot;Attention shape: {attn_weights.shape}&quot;)  # (2, 8, 20, 20)
</code></pre>
<hr />
<h2>1.5 Positional Encoding: ‰ΩçÁΩÆÊÉÖÂ†±„ÅÆÂüã„ÇÅËæº„Åø</h2>
<h3>„Å™„ÅúÂøÖË¶Å„Åã</h3>
<p><strong>ÂïèÈ°å</strong>: Self-Attention„Å´„ÅØÈ†ÜÂ∫è„ÅÆÊ¶ÇÂøµ„Åå„Å™„ÅÑ
- "H-C-O" „Å® "O-C-H" „ÇíÂå∫Âà•„Åß„Åç„Å™„ÅÑ
- ÂàÜÂ≠ê„ÇÑÊùêÊñô„Åß„ÅØÂéüÂ≠ê„ÅÆÈÖçÁΩÆÈ†ÜÂ∫è„ÅåÈáçË¶Å</p>
<p><strong>Ëß£Ê±∫Á≠ñ</strong>: Positional Encoding„Åß‰ΩçÁΩÆÊÉÖÂ†±„ÇíËøΩÂä†</p>
<h3>Êï∞Âºè</h3>
<p>$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$</p>
<p>$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$</p>
<h3>ÂÆüË£Ö</h3>
<pre><code class="language-python">class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()

        # ‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞Ë°åÂàó„Çí‰ΩúÊàê
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        &quot;&quot;&quot;
        Args:
            x: (batch_size, seq_len, d_model)
        &quot;&quot;&quot;
        seq_len = x.size(1)
        x = x + self.pe[:, :seq_len, :]
        return x

# ‰ΩøÁî®‰æã„Å®ÂèØË¶ñÂåñ
d_model = 128
max_len = 100

pos_enc = PositionalEncoding(d_model, max_len)

# „ÉÄ„Éü„ÉºÂÖ•Âäõ
x = torch.zeros(1, 50, d_model)
output = pos_enc(x)

# ÂèØË¶ñÂåñ
plt.figure(figsize=(12, 4))
plt.plot(pos_enc.pe[0, :50, :8].numpy())
plt.xlabel('Position')
plt.ylabel('Encoding Value')
plt.title('Positional Encoding (first 8 dimensions)')
plt.legend([f'dim {i}' for i in range(8)])
plt.tight_layout()
plt.show()
</code></pre>
<hr />
<h2>1.6 Transformer„Å®BERT/GPT</h2>
<h3>TransformerÂÖ®‰Ωì„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£</h3>
<div class="mermaid">
graph TB
    subgraph Encoder
        E1[Input Embedding] --> E2[Positional Encoding]
        E2 --> E3[Multi-Head Attention]
        E3 --> E4[Add and Norm]
        E4 --> E5[Feed Forward]
        E5 --> E6[Add and Norm]
    end

    subgraph Decoder
        D1[Output Embedding] --> D2[Positional Encoding]
        D2 --> D3[Masked Multi-Head Attention]
        D3 --> D4[Add and Norm]
        D4 --> D5[Multi-Head Attention]
        D5 --> D6[Add and Norm]
        D6 --> D7[Feed Forward]
        D7 --> D8[Add and Norm]
    end

    E6 -.-> D5
    D8 --> O[Output]

    style E3 fill:#e1f5ff
    style D3 fill:#ffe1e1
    style D5 fill:#e1ffe1
</div>

<h3>BERTÔºàBidirectional Encoder Representations from TransformersÔºâ</h3>
<p><strong>ÁâπÂæ¥</strong>:
- <strong>Encoder„ÅÆ„Åø</strong>‰ΩøÁî®
- <strong>ÂèåÊñπÂêë</strong>„Åß„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„ÇíÁêÜËß£
- <strong>‰∫ãÂâçÂ≠¶Áøí„Çø„Çπ„ÇØ</strong>: Masked Language Model (MLM) + Next Sentence Prediction (NSP)
- <strong>Áî®ÈÄî</strong>: ÂàÜÈ°û„ÄÅÁâπÂæ¥ÊäΩÂá∫„ÄÅË≥™ÂïèÂøúÁ≠î</p>
<p><strong>ÊùêÊñôÁßëÂ≠¶„Åß„ÅÆÂøúÁî®</strong>:
- MatBERT: ÊùêÊñô„ÅÆÁµÑÊàêÂºè„Åã„ÇâÁâπÊÄß‰∫àÊ∏¨
- ChemBERTa: ÂàÜÂ≠êSMILESË°®ÁèæÂ≠¶Áøí</p>
<h3>GPTÔºàGenerative Pre-trained TransformerÔºâ</h3>
<p><strong>ÁâπÂæ¥</strong>:
- <strong>Decoder„ÅÆ„Åø</strong>‰ΩøÁî®
- <strong>ÂçòÊñπÂêë</strong>ÔºàÂ∑¶„Åã„ÇâÂè≥Ôºâ„Åß„ÉÜ„Ç≠„Çπ„ÉàÁîüÊàê
- <strong>‰∫ãÂâçÂ≠¶Áøí„Çø„Çπ„ÇØ</strong>: Ê¨°„ÅÆÂçòË™û‰∫àÊ∏¨
- <strong>Áî®ÈÄî</strong>: „ÉÜ„Ç≠„Çπ„ÉàÁîüÊàê„ÄÅÂØæË©±„ÄÅÂâµÈÄ†ÁöÑ„Çø„Çπ„ÇØ</p>
<p><strong>ÊùêÊñôÁßëÂ≠¶„Åß„ÅÆÂøúÁî®</strong>:
- ÂàÜÂ≠êÁîüÊàêÔºàSMILESÊñáÂ≠óÂàóÁîüÊàêÔºâ
- ÊùêÊñôË®òËø∞Êñá„ÅÆËá™ÂãïÁîüÊàê
- ÂêàÊàêÁµåË∑Ø„ÅÆÊèêÊ°à</p>
<hr />
<h2>1.7 ÊùêÊñôÁßëÂ≠¶„Åß„ÅÆÊàêÂäü‰∫ã‰æã</h2>
<h3>1. ChemBERTa: ÂàÜÂ≠êË°®ÁèæÂ≠¶Áøí</h3>
<p><strong>Ê¶ÇË¶Å</strong>: SMILES„ÇíBERT„ÅßÂ≠¶Áøí</p>
<pre><code class="language-python"># ÂàÜÂ≠êSMILES: CC(C)Cc1ccc(cc1)C(C)C(=O)O („Ç§„Éñ„Éó„É≠„Éï„Çß„É≥)
# ChemBERTa„ÅßÂüã„ÇÅËæº„Åø„Éô„ÇØ„Éà„É´„Å´Â§âÊèõ ‚Üí ÁâπÊÄß‰∫àÊ∏¨
</code></pre>
<p><strong>ÊàêÊûú</strong>:
- Â∞èË¶èÊ®°„Éá„Éº„Çø„Åß„ÅÆÈ´òÁ≤æÂ∫¶‰∫àÊ∏¨
- Ëª¢ÁßªÂ≠¶Áøí„Å´„Çà„ÇäÈñãÁô∫ÊúüÈñìÁü≠Á∏Æ
- Ëß£ÈáàÂèØËÉΩÊÄßÔºàAttention„ÅßÈáçË¶ÅÈÉ®ÂàÜÂèØË¶ñÂåñÔºâ</p>
<h3>2. Matformer: ÊùêÊñôÁâπÊÄß‰∫àÊ∏¨</h3>
<p><strong>Ê¶ÇË¶Å</strong>: ÁµêÊô∂ÊßãÈÄ†„ÇíTransformer„ÅßÂá¶ÁêÜ</p>
<pre><code class="language-python"># ÂÖ•Âäõ: ÂéüÂ≠êÂ∫ßÊ®ô„ÄÅÂéüÂ≠êÁï™Âè∑„ÄÅÊ†ºÂ≠êÂÆöÊï∞
# Âá∫Âäõ: „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó„ÄÅÂΩ¢Êàê„Ç®„Éç„É´„ÇÆ„Éº
</code></pre>
<p><strong>ÊàêÊûú</strong>:
- Materials Project„Éá„Éº„Çø„ÅßÈ´òÁ≤æÂ∫¶
- GNN„Å®ÂêåÁ≠â‰ª•‰∏ä„ÅÆÊÄßËÉΩ
- Ë®àÁÆóÂäπÁéá„ÅåËâØ„ÅÑ</p>
<h3>3. Êã°Êï£„É¢„Éá„É´„Å´„Çà„ÇãÂàÜÂ≠êÁîüÊàê</h3>
<p><strong>Ê¶ÇË¶Å</strong>: Êù°‰ª∂‰ªò„ÅçÊã°Êï£„É¢„Éá„É´„ÅßÊñ∞Ë¶èÂàÜÂ≠êÁîüÊàê</p>
<pre><code class="language-python"># Êù°‰ª∂: Ê∫∂Ëß£Â∫¶ &gt; 5 mg/mL, LogP &lt; 3
# ÁîüÊàê: Êù°‰ª∂„ÇíÊ∫Ä„Åü„ÅôÂàÜÂ≠êSMILES
</code></pre>
<p><strong>ÊàêÊûú</strong>:
- ÂâµËñ¨„ÅßÊúâÊúõ„Å™ÂÄôË£úÂàÜÂ≠êÁô∫Ë¶ã
- ÂæìÊù•ÊâãÊ≥ï„Çà„ÇäÂ§öÊßòÊÄß„ÅåÈ´ò„ÅÑ
- ÂêàÊàêÂèØËÉΩÊÄß„ÇÇËÄÉÊÖÆ</p>
<hr />
<h2>1.8 „Åæ„Å®„ÇÅ</h2>
<h3>ÈáçË¶Å„Éù„Ç§„É≥„Éà</h3>
<ol>
<li><strong>AttentionÊ©üÊßã</strong>: Á≥ªÂàóÂÜÖ„ÅÆ‰ªªÊÑè„ÅÆË¶ÅÁ¥†Èñì„ÅÆÈñ¢‰øÇ„ÇíÁõ¥Êé•„É¢„Éá„É´Âåñ</li>
<li><strong>Self-Attention</strong>: ÂÖ•ÂäõÁ≥ªÂàóËá™Ë∫´„Å´ÂØæ„Åô„ÇãAttention</li>
<li><strong>Multi-Head Attention</strong>: Ë§áÊï∞„ÅÆË¶ñÁÇπ„Åã„ÇâÈñ¢‰øÇÊÄß„ÇíÂ≠¶Áøí</li>
<li><strong>Positional Encoding</strong>: ‰ΩçÁΩÆÊÉÖÂ†±„ÇíÂüã„ÇÅËæº„Åø</li>
<li><strong>BERT/GPT</strong>: Transformer based „ÅÆ‰ª£Ë°®ÁöÑ‰∫ãÂâçÂ≠¶Áøí„É¢„Éá„É´</li>
<li><strong>ÊùêÊñôÁßëÂ≠¶ÂøúÁî®</strong>: ÂàÜÂ≠ê„ÉªÊùêÊñôË°®ÁèæÂ≠¶Áøí„ÄÅÁâπÊÄß‰∫àÊ∏¨„ÄÅÁîüÊàê„É¢„Éá„É´</li>
</ol>
<h3>Ê¨°Á´†„Å∏„ÅÆÊ∫ñÂÇô</h3>
<p>Á¨¨2Á´†„Åß„ÅØ„ÄÅÊùêÊñôÁßëÂ≠¶„Å´ÁâπÂåñ„Åó„ÅüTransformer„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£ÔºàMatformer„ÄÅCrystalFormer„ÄÅChemBERTaÔºâ„ÇíË©≥„Åó„ÅèÂ≠¶„Å≥„Åæ„Åô„ÄÇ</p>
<hr />
<h2>üìù ÊºîÁøíÂïèÈ°å</h2>
<h3>ÂïèÈ°å1: Âü∫Á§éÁêÜËß£ÔºàÊ¶ÇÂøµÔºâ</h3>
<p>AttentionÊ©üÊßã„Å´„Åä„Åë„Çã Query„ÄÅKey„ÄÅValue „ÅÆÂΩπÂâ≤„Çí„ÄÅÂõ≥Êõ∏È§®„ÅÆ‰æã„Åà‰ª•Â§ñ„ÅßË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<details>
<summary>Ëß£Á≠î‰æã</summary>

**Ê§úÁ¥¢„Ç®„É≥„Ç∏„É≥„ÅÆ‰æã„Åà**:
- **Query**: „É¶„Éº„Ç∂„Éº„ÅåÂÖ•Âäõ„Åó„ÅüÊ§úÁ¥¢„Ç≠„Éº„ÉØ„Éº„Éâ
- **Key**: ÂêÑWeb„Éö„Éº„Ç∏„ÅÆ„É°„Çø„Éá„Éº„ÇøÔºà„Çø„Ç§„Éà„É´„ÄÅË¶ÅÁ¥ÑÔºâ
- **Value**: Web„Éö„Éº„Ç∏„ÅÆÂÆüÈöõ„ÅÆ„Ç≥„É≥„ÉÜ„É≥„ÉÑ
- **Attention**: Ê§úÁ¥¢„Ç≠„Éº„ÉØ„Éº„Éâ„Å®„ÅÆÈñ¢ÈÄ£ÊÄß„ÅåÈ´ò„ÅÑ„Éö„Éº„Ç∏„Çí‰∏ä‰ΩçË°®Á§∫

**ÂàÜÂ≠ê„ÅÆ‰æã„Åà**:
- **Query**: „ÅÇ„ÇãÂéüÂ≠ê„Åå„Äå„Å©„ÅÆÂéüÂ≠ê„Å®Áõ∏‰∫í‰ΩúÁî®„Åó„Åü„ÅÑ„Åã„Äç
- **Key**: ÂêÑÂéüÂ≠ê„ÅÆÁâπÂæ¥ÔºàÂéüÂ≠êÁï™Âè∑„ÄÅÈõªËç∑„ÄÅ‰ΩçÁΩÆÔºâ
- **Value**: ÂêÑÂéüÂ≠ê„ÅÆË©≥Á¥∞„Å™ÊÉÖÂ†±
- **Attention**: ÂåñÂ≠¶ÁµêÂêà„ÇÑÁõ∏‰∫í‰ΩúÁî®„ÅÆÂº∑„Åï„ÇíË°®Áèæ
</details>

<h3>ÂïèÈ°å2: ÂÆüË£ÖÔºà„Ç≥„Éº„Éá„Ç£„É≥„Ç∞Ôºâ</h3>
<p>‰ª•‰∏ã„ÅÆ„Ç≥„Éº„Éâ„ÅÆÁ©∫Ê¨Ñ„ÇíÂüã„ÇÅ„Å¶„ÄÅSimple AttentionÔºà„Çπ„Ç±„Éº„É™„É≥„Ç∞„Å™„Åó„ÄÅ„Éû„Çπ„ÇØ„Å™„ÅóÔºâ„ÇíÂÆüË£Ö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<pre><code class="language-python">def simple_attention(Q, K, V):
    &quot;&quot;&quot;
    „Ç∑„É≥„Éó„É´„Å™AttentionÊ©üÊßã

    Args:
        Q: Query (batch_size, seq_len, d_k)
        K: Key (batch_size, seq_len, d_k)
        V: Value (batch_size, seq_len, d_v)

    Returns:
        output: (batch_size, seq_len, d_v)
        attention_weights: (batch_size, seq_len, seq_len)
    &quot;&quot;&quot;
    # 1. Q„Å®K„ÅÆÂÜÖÁ©ç„ÇíË®àÁÆó
    scores = torch.matmul(______, ______.transpose(-2, -1))

    # 2. Softmax„ÅßÊ≠£Ë¶èÂåñ
    attention_weights = F.softmax(______, dim=-1)

    # 3. AttentionÈáç„Åø„ÅßValue„ÇíÈáç„Åø‰ªò„ÅëÂíå
    output = torch.matmul(______, ______)

    return output, attention_weights
</code></pre>
<details>
<summary>Ëß£Á≠î‰æã</summary>


<pre><code class="language-python">def simple_attention(Q, K, V):
    # 1. Q„Å®K„ÅÆÂÜÖÁ©ç„ÇíË®àÁÆó
    scores = torch.matmul(Q, K.transpose(-2, -1))

    # 2. Softmax„ÅßÊ≠£Ë¶èÂåñ
    attention_weights = F.softmax(scores, dim=-1)

    # 3. AttentionÈáç„Åø„ÅßValue„ÇíÈáç„Åø‰ªò„ÅëÂíå
    output = torch.matmul(attention_weights, V)

    return output, attention_weights
</code></pre>

</details>

<h3>ÂïèÈ°å3: ÂøúÁî®ÔºàËÄÉÂØüÔºâ</h3>
<p>ÂàÜÂ≠ê "CCO"Ôºà„Ç®„Çø„Éé„Éº„É´Ôºâ„Å´„Åä„Åë„ÇãSelf-Attention„ÇíËÄÉ„Åà„Åæ„Åô„ÄÇ‰ª•‰∏ã„ÅÆË≥™Âïè„Å´Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑÔºö</p>
<ol>
<li>„Å©„ÅÆÂéüÂ≠êÈñì„ÅÆAttentionÈáç„Åø„ÅåÊúÄ„ÇÇÈ´ò„Åè„Å™„Çã„Å®‰∫àÊÉ≥„Åï„Çå„Åæ„Åô„ÅãÔºü</li>
<li>„Åù„ÅÆÁêÜÁî±„ÇíÂåñÂ≠¶ÁöÑË¶≥ÁÇπ„Åã„ÇâË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</li>
<li>Multi-Head Attention„Åß„ÅØ„ÄÅÂêÑ„Éò„ÉÉ„Éâ„Åå„Å©„ÅÆ„Çà„ÅÜ„Å™Áï∞„Å™„ÇãÊÉÖÂ†±„ÇíÊçâ„Åà„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÅãÔºü</li>
</ol>
<details>
<summary>Ëß£Á≠î‰æã</summary>

1. **ÊúÄ„ÇÇÈ´ò„ÅÑAttentionÈáç„Åø**: C-CÁµêÂêà„ÄÅC-OÁµêÂêà

2. **ÂåñÂ≠¶ÁöÑÁêÜÁî±**:
   - ÂÖ±ÊúâÁµêÂêà„Å´„Çà„ÇäÂº∑„ÅÑÁõ∏‰∫í‰ΩúÁî®„Åå„ÅÇ„Çã
   - ÈõªÂ≠ê„ÅÆÂÖ±Êúâ„Å´„Çà„ÇäÈõªÂ≠êÂØÜÂ∫¶„ÅåÈ´ò„ÅÑ
   - OÂéüÂ≠ê„ÅØCÂéüÂ≠ê„Å®Ê•µÊÄßÁµêÂêà„ÇíÂΩ¢Êàê

3. **ÂêÑ„Éò„ÉÉ„Éâ„ÅåÊçâ„Åà„ÇãÊÉÖÂ†±„ÅÆ‰æã**:
   - **„Éò„ÉÉ„Éâ1**: ÂåñÂ≠¶ÁµêÂêàÔºà1Ê¨°ÁµêÂêàÔºâ
   - **„Éò„ÉÉ„Éâ2**: 2Ê¨°ÁµêÂêàÔºàC-C-OËßíÂ∫¶Ôºâ
   - **„Éò„ÉÉ„Éâ3**: ÈõªÂ≠êÂØÜÂ∫¶ÂàÜÂ∏É
   - **„Éò„ÉÉ„Éâ4**: ÂéüÂ≠ê„ÅÆÁ®ÆÈ°ûÔºàC vs O vs HÔºâ
   - **„Éò„ÉÉ„Éâ5**: Á´ã‰ΩìÈÖçÂ∫ßÊÉÖÂ†±
   - **„Éò„ÉÉ„Éâ6**: Ê•µÊÄßÁõ∏‰∫í‰ΩúÁî®

   ÂêÑ„Éò„ÉÉ„Éâ„ÅåÁï∞„Å™„ÇãË¶ñÁÇπ„Åã„ÇâÂàÜÂ≠ê„ÇíÁêÜËß£„Åô„Çã„Åì„Å®„Åß„ÄÅ„Çà„ÇäË±ä„Åã„Å™Ë°®Áèæ„ÅåÂèØËÉΩ„Å´„Å™„Çã„ÄÇ
</details>

<hr />
<h2>üìä „Éá„Éº„Çø„É©„Ç§„Çª„É≥„Çπ„Å®Âà©Áî®Ë¶èÁ¥Ñ</h2>
<h3>Ë®ÄË™û„Éá„Éº„Çø„Çª„ÉÉ„Éà</h3>
<ul>
<li><strong>WikiText-103</strong>: <a href="https://creativecommons.org/licenses/by-sa/3.0/">CC BY-SA 3.0</a></li>
<li><strong>BookCorpus</strong>: Á†îÁ©∂ÁõÆÁöÑ„ÅÆ„Åø„ÄÅÂÜçÈÖçÂ∏É‰∏çÂèØ</li>
<li><strong>Common Crawl</strong>: <a href="https://commoncrawl.org/terms-of-use/">Common Crawl Terms of Use</a></li>
</ul>
<h3>ÊùêÊñôÁßëÂ≠¶„Éá„Éº„Çø„Çª„ÉÉ„Éà</h3>
<ul>
<li><strong>Materials Project</strong>: <a href="https://materialsproject.org/about/terms">CC BY 4.0</a></li>
<li>Ë´ñÊñáÂºïÁî®: <code>Jain, A. et al. APL Materials 1, 011002 (2013)</code></li>
<li><strong>SMILESÂàÜÂ≠ê„Éá„Éº„Çø</strong>:</li>
<li><strong>ZINC</strong>: Â≠¶Ë°ìÂà©Áî®ÂèØ„ÄÅÂïÜÁî®„ÅØË¶ÅÁ¢∫Ë™ç</li>
<li><strong>ChEMBL</strong>: <a href="https://chembl.gitbook.io/chembl-interface-documentation/about#data-licensing">CC BY-SA 3.0</a></li>
<li><strong>PubChem</strong>: „Éë„Éñ„É™„ÉÉ„ÇØ„Éâ„É°„Ç§„É≥</li>
<li><strong>ÁµêÊô∂ÊßãÈÄ†„Éá„Éº„Çø</strong>:</li>
<li><strong>ICSD</strong>: „É©„Ç§„Çª„É≥„ÇπË≥ºÂÖ•ÂøÖË¶Å</li>
<li><strong>COD (Crystallography Open Database)</strong>: „Éë„Éñ„É™„ÉÉ„ÇØ„Éâ„É°„Ç§„É≥</li>
</ul>
<h3>„É©„Ç§„Çª„É≥„ÇπÈÅµÂÆà„ÅÆ„Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„Çπ</h3>
<pre><code class="language-python"># „Éá„Éº„Çø„Çª„ÉÉ„Éà‰ΩøÁî®ÊôÇ„ÅÆÂºïÁî®‰æã
&quot;&quot;&quot;
This work uses data from Materials Project (materialsproject.org),
which is released under CC BY 4.0 license.

Citation:
Jain, A., Ong, S. P., Hautier, G., Chen, W., Richards, W. D.,
Dacek, S., ... &amp; Persson, K. A. (2013).
Commentary: The Materials Project: A materials genome approach
to accelerating materials innovation. APL materials, 1(1).
&quot;&quot;&quot;
</code></pre>
<hr />
<h2>üîß „Ç≥„Éº„ÉâÂÜçÁèæÊÄß„Ç¨„Ç§„Éâ„É©„Ç§„É≥</h2>
<h3>Áí∞Â¢ÉË®≠ÂÆö</h3>
<pre><code class="language-python"># requirements.txt
torch==2.0.1
transformers==4.30.2
numpy==1.24.3
matplotlib==3.7.1
seaborn==0.12.2

# Êé®Â•®ÔºöÂÆåÂÖ®„Å™Áí∞Â¢ÉÂÜçÁèæ
# conda env export &gt; environment.yml
</code></pre>
<h3>ÂÜçÁèæÊÄß„ÅÆ„Åü„ÇÅ„ÅÆË®≠ÂÆö</h3>
<pre><code class="language-python">import torch
import numpy as np
import random

def set_seed(seed=42):
    &quot;&quot;&quot;
    ÂÆåÂÖ®„Å™ÂÜçÁèæÊÄß„Çí‰øùË®º

    Args:
        seed: „É©„É≥„ÉÄ„É†„Ç∑„Éº„Éâ
    &quot;&quot;&quot;
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    # CuDNN„ÅÆÊåôÂãï„ÇíÊ±∫ÂÆöÁöÑ„Å´„Åô„ÇãÔºàÈÄüÂ∫¶„ÅØ‰Ωé‰∏ãÔºâ
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# ‰ΩøÁî®‰æã
set_seed(42)

# „Éê„Éº„Ç∏„Éß„É≥ÊÉÖÂ†±„ÅÆË®òÈå≤
print(f&quot;PyTorch version: {torch.__version__}&quot;)
print(f&quot;CUDA available: {torch.cuda.is_available()}&quot;)
if torch.cuda.is_available():
    print(f&quot;CUDA version: {torch.version.cuda}&quot;)
</code></pre>
<h3>Transformer„Éë„É©„É°„Éº„Çø„ÅÆÊòéÁ§∫</h3>
<pre><code class="language-python"># ÂÆüÈ®ìË®≠ÂÆö„ÇíËæûÊõ∏„ÅßÁÆ°ÁêÜ
config = {
    'model': {
        'd_model': 512,
        'num_heads': 8,
        'num_layers': 6,
        'd_ff': 2048,
        'dropout': 0.1,
        'max_seq_len': 512
    },
    'training': {
        'batch_size': 32,
        'learning_rate': 1e-4,
        'num_epochs': 100,
        'warmup_steps': 4000,
        'optimizer': 'Adam',
        'weight_decay': 0.01
    },
    'data': {
        'train_split': 0.8,
        'val_split': 0.1,
        'test_split': 0.1,
        'tokenizer': 'BPE',
        'vocab_size': 50000
    },
    'seed': 42
}

# Ë®≠ÂÆö„ÅÆ‰øùÂ≠ò
import json
with open('experiment_config.json', 'w') as f:
    json.dump(config, f, indent=2)
</code></pre>
<h3>Attention„Éë„É©„É°„Éº„Çø„ÅÆË©≥Á¥∞Ë®≠ÂÆö</h3>
<pre><code class="language-python">class ReproducibleMultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1, bias=True):
        &quot;&quot;&quot;
        ÂÜçÁèæÊÄß„ÇíÈáçË¶ñ„Åó„ÅüMulti-Head Attention

        Args:
            d_model: „É¢„Éá„É´Ê¨°ÂÖÉÔºà512Êé®Â•®Ôºâ
            num_heads: „Éò„ÉÉ„ÉâÊï∞Ôºà8Êé®Â•®„ÄÅd_model„ÅßÂâ≤„ÇäÂàá„Çå„ÇãÂøÖË¶ÅÔºâ
            dropout: „Éâ„É≠„ÉÉ„Éó„Ç¢„Ç¶„ÉàÁéáÔºà0.1Êé®Â•®Ôºâ
            bias: Á∑öÂΩ¢Â±§„Å´„Éê„Ç§„Ç¢„Çπ„Çí‰ΩøÁî®„Åô„Çã„Åã
        &quot;&quot;&quot;
        super().__init__()
        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # ÂàùÊúüÂåñÊñπÊ≥ï„ÇíÊòéÁ§∫
        self.W_q = nn.Linear(d_model, d_model, bias=bias)
        self.W_k = nn.Linear(d_model, d_model, bias=bias)
        self.W_v = nn.Linear(d_model, d_model, bias=bias)
        self.W_o = nn.Linear(d_model, d_model, bias=bias)

        # XavierÂàùÊúüÂåñ
        for module in [self.W_q, self.W_k, self.W_v, self.W_o]:
            nn.init.xavier_uniform_(module.weight)
            if bias:
                nn.init.zeros_(module.bias)

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # ÂÆüË£Ö„ÅØÂâçËø∞„ÅÆ„Ç≥„Éº„Éâ„Å®Âêå„Åò
        pass
</code></pre>
<hr />
<h2>‚ö†Ô∏è ÂÆüË∑µÁöÑ„Å™ËêΩ„Å®„ÅóÁ©¥„Å®ÂØæÂá¶Ê≥ï</h2>
<h3>1. Attention„Éû„Çπ„ÇØ„ÅÆË™§„Çä</h3>
<p><strong>ÂïèÈ°å</strong>: „Éû„Çπ„ÇØ„ÅÆÈÅ©Áî®„Éü„Çπ„ÅßÊú™Êù•„ÅÆÊÉÖÂ†±„ÅåÊºèÊ¥©</p>
<pre><code class="language-python"># ‚ùå ÈñìÈÅï„ÅÑ: „Éû„Çπ„ÇØ„ÅåÊ≠£„Åó„ÅèÈÅ©Áî®„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ
def wrong_attention(Q, K, V, mask):
    scores = torch.matmul(Q, K.transpose(-2, -1))
    # mask„ÅÆÂÄ§„ÅåÈÄÜ„Å´„Å™„Å£„Å¶„ÅÑ„Çã
    scores = scores.masked_fill(mask == 1, -1e9)  # ÈñìÈÅï„ÅÑÔºÅ
    return F.softmax(scores, dim=-1)

# ‚úÖ Ê≠£„Åó„ÅÑ: „Éû„Çπ„ÇØ„ÅØ0„ÅÆ‰ΩçÁΩÆ„ÇíÁÑ°Ë¶ñ
def correct_attention(Q, K, V, mask):
    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(Q.size(-1))
    if mask is not None:
        # mask==0„ÅÆ‰ΩçÁΩÆ„Çí-inf„Å´„Åô„Çã
        scores = scores.masked_fill(mask == 0, float('-inf'))
    return F.softmax(scores, dim=-1)

# „Éá„Éê„ÉÉ„Ç∞ÊñπÊ≥ï
print(&quot;Attention scores before mask:&quot;, scores)
print(&quot;Mask:&quot;, mask)
print(&quot;Attention scores after mask:&quot;, scores.masked_fill(mask == 0, float('-inf')))
</code></pre>
<h3>2. Positional Encoding„ÅÆÂÆüË£Ö„Éü„Çπ</h3>
<p><strong>ÂïèÈ°å</strong>: sin„Å®cos„ÅÆÊ¨°ÂÖÉ„ÅåÈñìÈÅï„Å£„Å¶„ÅÑ„Çã</p>
<pre><code class="language-python"># ‚ùå ÈñìÈÅï„ÅÑ: Ê¨°ÂÖÉ„ÅÆÂâ≤„ÇäÂΩì„Å¶„ÅåÈÄÜ
class WrongPositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))

        pe[:, 1::2] = torch.sin(position * div_term)  # ÈñìÈÅï„ÅÑÔºÅ
        pe[:, 0::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

# ‚úÖ Ê≠£„Åó„ÅÑ: sin„ÅØÂÅ∂Êï∞Ê¨°ÂÖÉ„ÄÅcos„ÅØÂ•áÊï∞Ê¨°ÂÖÉ
class CorrectPositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)  # Ê≠£„Åó„ÅÑ
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)
</code></pre>
<h3>3. „É°„É¢„É™„Ç™„Éº„Éê„Éº„Éï„É≠„Éº</h3>
<p><strong>ÂïèÈ°å</strong>: Èï∑„ÅÑ„Ç∑„Éº„Ç±„É≥„Çπ„ÅßOOMÔºàOut of MemoryÔºâ</p>
<pre><code class="language-python"># ‚ùå ÂïèÈ°å: ÂÖ®Á≥ªÂàó„Çí‰∏ÄÂ∫¶„Å´Âá¶ÁêÜ
def memory_intensive_attention(x):
    # x: (batch=64, seq_len=10000, d_model=512)
    # AttentionË°åÂàó: (64, 10000, 10000) = Á¥Ñ24GBÔºÅ
    return multi_head_attention(x)

# ‚úÖ Ëß£Ê±∫Á≠ñ1: Gradient checkpointing
from torch.utils.checkpoint import checkpoint

def memory_efficient_attention(x):
    return checkpoint(multi_head_attention, x)

# ‚úÖ Ëß£Ê±∫Á≠ñ2: „Ç∑„Éº„Ç±„É≥„Çπ„ÇíÂàÜÂâ≤
def chunked_attention(x, chunk_size=512):
    batch, seq_len, d_model = x.shape
    outputs = []

    for i in range(0, seq_len, chunk_size):
        chunk = x[:, i:i+chunk_size, :]
        output = multi_head_attention(chunk)
        outputs.append(output)

    return torch.cat(outputs, dim=1)

# ‚úÖ Ëß£Ê±∫Á≠ñ3: Sparse AttentionÔºàÈï∑Ë∑ùÈõ¢„Çø„Çπ„ÇØÂêë„ÅëÔºâ
# Longformer„ÄÅBigBird„Å™„Å©„ÅÆ„É©„Ç§„Éñ„É©„É™„Çí‰ΩøÁî®
</code></pre>
<h3>4. „Éà„Éº„ÇØ„É≥Âåñ„ÅÆÂïèÈ°åÔºàÊùêÊñôÁßëÂ≠¶ÁâπÊúâÔºâ</h3>
<p><strong>ÂïèÈ°å</strong>: SMILES„ÅÆÊã¨Âºß„ÇÑÂàÜÂ≤ê„ÅåÊ≠£„Åó„ÅèÂá¶ÁêÜ„Åï„Çå„Å™„ÅÑ</p>
<pre><code class="language-python"># ‚ùå ÈñìÈÅï„ÅÑ: ÂçòÁ¥î„Å™ÊñáÂ≠óÂàÜÂâ≤
def wrong_smiles_tokenize(smiles):
    return list(smiles)  # &quot;C(C)O&quot; ‚Üí ['C', '(', 'C', ')', 'O']

# ‚úÖ Ê≠£„Åó„ÅÑ: SMILES„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„Çí‰ΩøÁî®
from transformers import RobertaTokenizer

tokenizer = RobertaTokenizer.from_pretrained(&quot;seyonec/ChemBERTa-zinc-base-v1&quot;)

# „Åæ„Åü„ÅØÊ≠£Ë¶èË°®Áèæ„Éô„Éº„Çπ
import re
def correct_smiles_tokenize(smiles):
    pattern = r'(\[[^\]]+\]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\(|\)|\.|=|#|-|\+|\\|\/|:|~|@|\?|&gt;|\*|\$|\%[0-9]{2}|[0-9])'
    return re.findall(pattern, smiles)

# „ÉÜ„Çπ„Éà
smiles = &quot;CC(C)Cc1ccc(cc1)C(C)C(=O)O&quot;  # „Ç§„Éñ„Éó„É≠„Éï„Çß„É≥
tokens = correct_smiles_tokenize(smiles)
print(f&quot;Tokens: {tokens}&quot;)
</code></pre>
<h3>5. Êï∞ÂÄ§‰∏çÂÆâÂÆöÊÄß</h3>
<p><strong>ÂïèÈ°å</strong>: Softmax„Åß„Ç™„Éº„Éê„Éº„Éï„É≠„Éº/„Ç¢„É≥„ÉÄ„Éº„Éï„É≠„Éº</p>
<pre><code class="language-python"># ‚ùå ÂïèÈ°å: Â§ß„Åç„Å™„Çπ„Ç≥„Ç¢„Åßexp()„Åå„Ç™„Éº„Éê„Éº„Éï„É≠„Éº
def unstable_softmax(x):
    return torch.exp(x) / torch.sum(torch.exp(x), dim=-1, keepdim=True)

# ‚úÖ Ëß£Ê±∫Á≠ñ: Êï∞ÂÄ§ÂÆâÂÆöÁâàsoftmaxÔºàPyTorch„ÅØÂÜÖÈÉ®„ÅßÂÆüË£ÖÊ∏à„ÅøÔºâ
def stable_softmax(x):
    # ÊúÄÂ§ßÂÄ§„ÇíÂºï„ÅÑ„Å¶Êï∞ÂÄ§ÂÆâÂÆöÊÄß„ÇíÁ¢∫‰øù
    x_max = torch.max(x, dim=-1, keepdim=True)[0]
    exp_x = torch.exp(x - x_max)
    return exp_x / torch.sum(exp_x, dim=-1, keepdim=True)

# PyTorch„ÅÆF.softmax()„Çí‰Ωø„ÅÜ„ÅÆ„ÅåÊúÄ„ÇÇÂÆâÂÖ®
import torch.nn.functional as F
safe_output = F.softmax(x, dim=-1)
</code></pre>
<hr />
<h2>‚úÖ Á¨¨1Á´†ÂÆå‰∫Ü„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà</h2>
<h3>Ê¶ÇÂøµÁêÜËß£Ôºà10È†ÖÁõÆÔºâ</h3>
<ul>
<li>[ ] AttentionÊ©üÊßã„ÅÆQuery„ÄÅKey„ÄÅValue„ÅÆÂΩπÂâ≤„ÇíË™¨Êòé„Åß„Åç„Çã</li>
<li>[ ] Self-Attention„Å®ÈÄöÂ∏∏„ÅÆAttention„ÅÆÈÅï„ÅÑ„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] Multi-Head Attention„Åå„Å™„ÅúË§áÊï∞„Éò„ÉÉ„ÉâÂøÖË¶Å„ÅãË™¨Êòé„Åß„Åç„Çã</li>
<li>[ ] Positional Encoding„ÅÆÂøÖË¶ÅÊÄß„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] Transformer„Åå‰∏¶ÂàóÂåñÂèØËÉΩ„Å™ÁêÜÁî±„ÇíË™¨Êòé„Åß„Åç„Çã</li>
<li>[ ] RNN/CNN„Å´ÂØæ„Åô„ÇãTransformer„ÅÆÂà©ÁÇπ„Çí3„Å§‰ª•‰∏äÊåô„Åí„Çâ„Çå„Çã</li>
<li>[ ] BERT„Å®GPT„ÅÆÈÅï„ÅÑÔºàEncoder vs DecoderÔºâ„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] Scaled Dot-Product Attention„ÅÆ„Çπ„Ç±„Éº„É™„É≥„Ç∞Âõ†Â≠êÔºà‚àöd_kÔºâ„ÅÆÊÑèÂë≥„ÇíÁü•„Å£„Å¶„ÅÑ„Çã</li>
<li>[ ] AttentionÈáç„Åø„ÅÆÂèØË¶ñÂåñ„Åã„Çâ‰Ωï„ÅåÂàÜ„Åã„Çã„ÅãÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] ÊùêÊñôÁßëÂ≠¶„ÅßTransformer„ÅåÊúâÂäπ„Å™ÁêÜÁî±„ÇíË™¨Êòé„Åß„Åç„Çã</li>
</ul>
<h3>Êï∞ÂºèÁêÜËß£Ôºà5È†ÖÁõÆÔºâ</h3>
<ul>
<li>[ ] Attention(Q,K,V)„ÅÆÊï∞Âºè„ÇíÊõ∏„Åë„Çã</li>
<li>[ ] Positional Encoding„ÅÆÊï∞Âºè„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] Multi-Head Attention„ÅÆÊï∞Âºè„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] Softmax„ÅÆÊï∞Âºè„Å®ÊÑèÂë≥„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] Ë°åÂàó„ÅÆÊ¨°ÂÖÉÔºàshapeÔºâ„ÇíÊ≠£„Åó„ÅèË®àÁÆó„Åß„Åç„Çã</li>
</ul>
<h3>ÂÆüË£Ö„Çπ„Ç≠„É´Ôºà15È†ÖÁõÆÔºâ</h3>
<ul>
<li>[ ] <code>scaled_dot_product_attention</code>„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] <code>SelfAttention</code>„ÇØ„É©„Çπ„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] <code>MultiHeadAttention</code>„ÇØ„É©„Çπ„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] <code>PositionalEncoding</code>„ÇØ„É©„Çπ„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] AttentionÈáç„Åø„Çí„Éí„Éº„Éà„Éû„ÉÉ„Éó„ÅßÂèØË¶ñÂåñ„Åß„Åç„Çã</li>
<li>[ ] „Éû„Çπ„ÇØ„ÇíÊ≠£„Åó„ÅèÈÅ©Áî®„Åß„Åç„ÇãÔºàpadding mask„ÄÅcausal maskÔºâ</li>
<li>[ ] PyTorch„ÅÆ„ÉÜ„É≥„ÇΩ„É´Êìç‰ΩúÔºàview„ÄÅtranspose„ÄÅmatmulÔºâ„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] <code>nn.Linear</code>„ÄÅ<code>nn.Embedding</code>„ÅÆ‰Ωø„ÅÑÊñπ„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] „Éê„ÉÉ„ÉÅÂá¶ÁêÜ„ÇíÊ≠£„Åó„ÅèÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] „Éá„Éê„Ç§„ÇπÁÆ°ÁêÜÔºàCPU/GPUÔºâ„ÇíÈÅ©Âàá„Å´Ë°å„Åà„Çã</li>
<li>[ ] „É¢„Éá„É´„ÅÆ‰øùÂ≠ò„ÉªË™≠„ÅøËæº„Åø„Åå„Åß„Åç„Çã</li>
<li>[ ] Gradient„ÅÆË®àÁÆó„Å®ÈÄÜ‰ºùÊí≠„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] „Éâ„É≠„ÉÉ„Éó„Ç¢„Ç¶„Éà„ÅÆÈÅ©Áî®‰ΩçÁΩÆ„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] Layer Normalization„ÅÆÂΩπÂâ≤„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] ÂàùÊúüÂåñÊñπÊ≥ïÔºàXavier„ÄÅKaiming„Å™„Å©Ôºâ„ÇíÈÅ∏Êäû„Åß„Åç„Çã</li>
</ul>
<h3>„Éá„Éê„ÉÉ„Ç∞„Çπ„Ç≠„É´Ôºà5È†ÖÁõÆÔºâ</h3>
<ul>
<li>[ ] „ÉÜ„É≥„ÇΩ„É´„ÅÆshape„Ç®„É©„Éº„Çí„Éá„Éê„ÉÉ„Ç∞„Åß„Åç„Çã</li>
<li>[ ] Attention„Éû„Çπ„ÇØ„ÅÆË™§„Çä„ÇíÊ§úÂá∫„Éª‰øÆÊ≠£„Åß„Åç„Çã</li>
<li>[ ] „É°„É¢„É™„Ç®„É©„ÉºÔºàOOMÔºâ„ÅÆÂéüÂõ†„ÇíÁâπÂÆö„Åß„Åç„Çã</li>
<li>[ ] Êï∞ÂÄ§‰∏çÂÆâÂÆöÊÄßÔºàNaN„ÄÅinfÔºâ„ÇíÊ§úÂá∫„ÉªÂØæÂá¶„Åß„Åç„Çã</li>
<li>[ ] ‰∏≠ÈñìÂá∫Âäõ„ÇíÂèØË¶ñÂåñ„Åó„Å¶„Éê„Ç∞„ÇíÁô∫Ë¶ã„Åß„Åç„Çã</li>
</ul>
<h3>ÂøúÁî®ÂäõÔºà5È†ÖÁõÆÔºâ</h3>
<ul>
<li>[ ] ÂàÜÂ≠ê„Éá„Éº„ÇøÔºàSMILESÔºâ„Å´Attention„ÇíÈÅ©Áî®„Åô„ÇãÊñπÊ≥ï„ÇíËÄÉ„Åà„Çâ„Çå„Çã</li>
<li>[ ] ÊùêÊñô„ÅÆÁµÑÊàêÂºè„Å´Transformer„ÇíÈÅ©Áî®„Åô„ÇãÊñπÊ≥ï„ÇíËÄÉ„Åà„Çâ„Çå„Çã</li>
<li>[ ] Êó¢Â≠ò„ÅÆTransformer„É¢„Éá„É´ÔºàBERT„ÄÅGPTÔºâ„ÇíÊùêÊñôÁßëÂ≠¶„Å´ÈÅ©Âøú„Åï„Åõ„ÇãÊà¶Áï•„ÇíÁ´ã„Å¶„Çâ„Çå„Çã</li>
<li>[ ] AttentionÈáç„Åø„Åã„ÇâÂåñÂ≠¶ÁöÑ„Å´ÊÑèÂë≥„ÅÆ„ÅÇ„ÇãÊÉÖÂ†±„ÇíÊäΩÂá∫„Åß„Åç„Çã</li>
<li>[ ] Êñ∞„Åó„ÅÑ„Çø„Çπ„ÇØ„Å´ÂøÖË¶Å„Å™„É¢„Éá„É´‰øÆÊ≠£„ÇíË®≠Ë®à„Åß„Åç„Çã</li>
</ul>
<h3>ÁêÜË´ñÁöÑËÉåÊôØÔºà5È†ÖÁõÆÔºâ</h3>
<ul>
<li>[ ] Transformer„ÅÆÂÖÉË´ñÊñáÔºà"Attention Is All You Need"Ôºâ„ÇíË™≠„Çì„Å†</li>
<li>[ ] BERT„ÅÆË´ñÊñá„ÇíË™≠„Çì„Å†</li>
<li>[ ] ÊùêÊñôÁßëÂ≠¶„Åß„ÅÆTransformerÂøúÁî®Ë´ñÊñá„Çí1Êú¨‰ª•‰∏äË™≠„Çì„Å†</li>
<li>[ ] Ë®àÁÆóÈáè„Ç™„Éº„ÉÄ„ÉºÔºàO(n¬≤)Ôºâ„ÅÆÊÑèÂë≥„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] Â∏∞Á¥ç„Éê„Ç§„Ç¢„ÇπÔºàinductive biasÔºâ„ÅÆÊ¶ÇÂøµ„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
</ul>
<h3>ÂÜçÁèæÊÄßÔºà5È†ÖÁõÆÔºâ</h3>
<ul>
<li>[ ] „É©„É≥„ÉÄ„É†„Ç∑„Éº„Éâ„ÇíË®≠ÂÆö„Åó„Å¶ÂÜçÁèæÊÄß„ÇíÁ¢∫‰øù„Åß„Åç„Çã</li>
<li>[ ] ÂÆüÈ®ìË®≠ÂÆö„ÇíJSON„Åß‰øùÂ≠ò„ÉªË™≠„ÅøËæº„Åø„Åß„Åç„Çã</li>
<li>[ ] „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ„É©„Ç§„Çª„É≥„Çπ„ÇíÁ¢∫Ë™ç„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] „Éê„Éº„Ç∏„Éß„É≥ÊÉÖÂ†±„ÇíË®òÈå≤„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] „Ç≥„Éº„Éâ„Å´„Éâ„Ç≠„É•„É°„É≥„ÉàÔºàdocstringÔºâ„ÇíÊõ∏„ÅÑ„Å¶„ÅÑ„Çã</li>
</ul>
<h3>ÂÆå‰∫ÜÂü∫Ê∫ñ</h3>
<ul>
<li><strong>ÊúÄ‰ΩéÂü∫Ê∫ñ</strong>: 40È†ÖÁõÆ‰ª•‰∏äÈÅîÊàêÔºà80%Ôºâ</li>
<li><strong>Êé®Â•®Âü∫Ê∫ñ</strong>: 45È†ÖÁõÆ‰ª•‰∏äÈÅîÊàêÔºà90%Ôºâ</li>
<li><strong>ÂÑ™ÁßÄÂü∫Ê∫ñ</strong>: 50È†ÖÁõÆÂÖ®„Å¶ÈÅîÊàêÔºà100%Ôºâ</li>
</ul>
<hr />
<h2>üîó ÂèÇËÄÉË≥áÊñô</h2>
<h3>Ë´ñÊñá</h3>
<ul>
<li>Vaswani et al. (2017) "Attention Is All You Need" <a href="https://arxiv.org/abs/1706.03762">arXiv:1706.03762</a></li>
<li>Devlin et al. (2019) "BERT: Pre-training of Deep Bidirectional Transformers" <a href="https://arxiv.org/abs/1810.04805">arXiv:1810.04805</a></li>
</ul>
<h3>„ÉÅ„É•„Éº„Éà„É™„Ç¢„É´</h3>
<ul>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
<li><a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html">PyTorch Transformer Tutorial</a></li>
</ul>
<h3>Ê¨°Á´†</h3>
<p><strong><a href="chapter-2.html">Á¨¨2Á´†: ÊùêÊñôÂêë„ÅëTransformer„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£</a></strong> „Åß„ÄÅMatformer„ÄÅChemBERTa„Å™„Å©ÊùêÊñôÁßëÂ≠¶ÁâπÂåñ„É¢„Éá„É´„ÇíÂ≠¶„Å≥„Åæ„Åô„ÄÇ</p>
<hr />
<p><strong>‰ΩúÊàêËÄÖ</strong>: Ê©ãÊú¨‰Ωë‰ªãÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ
<strong>ÊúÄÁµÇÊõ¥Êñ∞</strong>: 2025Âπ¥10Êúà19Êó•</p><div class="navigation">
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
    <a href="chapter-2.html" class="nav-button">Ê¨°„ÅÆÁ´† ‚Üí</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>Áõ£‰øÆ</strong>: Dr. Yusuke HashimotoÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-17</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
