<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Chapter</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 20-25åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 0å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 0å•</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>ç¬¬3ç« ï¼šGNNã«ã‚ˆã‚‹ææ–™è¡¨ç¾å­¦ç¿’</h1>
<h2>æ¦‚è¦</h2>
<p>Graph Neural Networksï¼ˆGNNï¼‰ã¯ã€çµæ™¶æ§‹é€ ã®ã‚ˆã†ãªåŸå­é–“ã®ã¤ãªãŒã‚Šã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªç„¶ã«æ‰±ãˆã‚‹å¼·åŠ›ãªæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚æœ¬ç« ã§ã¯ã€ææ–™ã®çµæ™¶æ§‹é€ ã‹ã‚‰é«˜æ¬¡å…ƒã®ç‰¹å¾´è¡¨ç¾ï¼ˆembeddingï¼‰ã‚’å­¦ç¿’ã—ã€ãã‚Œã‚’æ¬¡å…ƒå‰Šæ¸›ã¨çµ„ã¿åˆã‚ã›ã¦ææ–™ç©ºé–“ã‚’ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹æ–¹æ³•ã‚’å­¦ã³ã¾ã™ã€‚</p>
<h3>å­¦ç¿’ç›®æ¨™</h3>
<ul>
<li>ææ–™ã‚’ã‚°ãƒ©ãƒ•ã¨ã—ã¦è¡¨ç¾ã™ã‚‹æ–¹æ³•ã‚’ç†è§£ã™ã‚‹</li>
<li>PyTorch Geometricã‚’ç”¨ã„ãŸGNNãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…ãŒã§ãã‚‹</li>
<li>CGCNNã€MEGNetã€SchNetã®ç‰¹å¾´ã¨å®Ÿè£…ã‚’ç†è§£ã™ã‚‹</li>
<li>GNNã‹ã‚‰å¾—ã‚‰ã‚ŒãŸembeddingã‚’å¯è¦–åŒ–ã§ãã‚‹</li>
</ul>
<h2>3.1 ææ–™ã®ã‚°ãƒ©ãƒ•è¡¨ç¾</h2>
<h3>3.1.1 çµæ™¶æ§‹é€ ã¨ã‚°ãƒ©ãƒ•ã®å¯¾å¿œ</h3>
<p>çµæ™¶æ§‹é€ ã¯è‡ªç„¶ã«ã‚°ãƒ©ãƒ•ã¨ã—ã¦è¡¨ç¾ã§ãã¾ã™ï¼š</p>
<ul>
<li><strong>ãƒãƒ¼ãƒ‰ï¼ˆé ‚ç‚¹ï¼‰</strong>: åŸå­</li>
<li><strong>ãƒãƒ¼ãƒ‰ç‰¹å¾´</strong>: åŸå­ç•ªå·ã€é›»æ°—é™°æ€§åº¦ã€ã‚¤ã‚ªãƒ³åŠå¾„ãªã©</li>
<li><strong>ã‚¨ãƒƒã‚¸ï¼ˆè¾ºï¼‰</strong>: åŸå­é–“ã®çµåˆï¼ˆä¸€å®šè·é›¢å†…ã®éš£æ¥é–¢ä¿‚ï¼‰</li>
<li><strong>ã‚¨ãƒƒã‚¸ç‰¹å¾´</strong>: åŸå­é–“è·é›¢ã€çµåˆè§’åº¦ãªã©</li>
<li><strong>ã‚°ãƒ­ãƒ¼ãƒãƒ«ç‰¹å¾´</strong>: ã‚»ãƒ«ä½“ç©ã€å¯†åº¦ãªã©</li>
</ul>
<h3>ã‚³ãƒ¼ãƒ‰ä¾‹1: PyTorch Geometricã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨åŸºæœ¬è¨­å®š</h3>
<pre><code class="language-python"># PyTorch Geometricã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆåˆå›ã®ã¿ï¼‰
# !pip install torch torchvision torchaudio
# !pip install torch-geometric
# !pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data, DataLoader
from torch_geometric.nn import MessagePassing, global_mean_pool, global_add_pool
import numpy as np
import matplotlib.pyplot as plt

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f&quot;Using device: {device}&quot;)

# PyTorch Geometricãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
import torch_geometric
print(f&quot;PyTorch Geometric version: {torch_geometric.__version__}&quot;)
print(f&quot;PyTorch version: {torch.__version__}&quot;)
</code></pre>
<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>Using device: cpu
PyTorch Geometric version: 2.3.1
PyTorch version: 2.0.1
</code></pre>
<h3>ã‚³ãƒ¼ãƒ‰ä¾‹2: çµæ™¶æ§‹é€ ã‹ã‚‰ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã¸ã®å¤‰æ›</h3>
<pre><code class="language-python">from pymatgen.core import Structure, Lattice
import numpy as np
import torch
from torch_geometric.data import Data

def structure_to_graph(structure, cutoff=5.0):
    &quot;&quot;&quot;
    pymatgen Structureã‚’PyTorch Geometric Dataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›

    Parameters:
    -----------
    structure : pymatgen.Structure
        çµæ™¶æ§‹é€ 
    cutoff : float
        ã‚¨ãƒƒã‚¸ã‚’ç”Ÿæˆã™ã‚‹åŸå­é–“è·é›¢ã®é–¾å€¤ï¼ˆAngstromï¼‰

    Returns:
    --------
    data : torch_geometric.data.Data
        ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿
    &quot;&quot;&quot;
    # ãƒãƒ¼ãƒ‰ç‰¹å¾´: åŸå­ç•ªå·ï¼ˆãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¯å¾Œã§å®Ÿè£…ï¼‰
    atomic_numbers = torch.tensor([site.specie.Z for site in structure],
                                  dtype=torch.float).view(-1, 1)

    # ã‚¨ãƒƒã‚¸ã®æ§‹ç¯‰
    all_neighbors = structure.get_all_neighbors(cutoff)
    edge_index = []
    edge_attr = []

    for i, neighbors in enumerate(all_neighbors):
        for neighbor in neighbors:
            j = neighbor.index
            distance = neighbor.nn_distance

            edge_index.append([i, j])
            edge_attr.append(distance)

    # ã‚¨ãƒƒã‚¸ãŒãªã„å ´åˆã®å‡¦ç†
    if len(edge_index) == 0:
        edge_index = torch.zeros((2, 0), dtype=torch.long)
        edge_attr = torch.zeros((0, 1), dtype=torch.float)
    else:
        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
        edge_attr = torch.tensor(edge_attr, dtype=torch.float).view(-1, 1)

    # Dataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆ
    data = Data(x=atomic_numbers,
                edge_index=edge_index,
                edge_attr=edge_attr)

    return data


# ã‚µãƒ³ãƒ—ãƒ«çµæ™¶æ§‹é€ ã®ä½œæˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªCsClæ§‹é€ ï¼‰
lattice = Lattice.cubic(4.0)
structure = Structure(lattice, [&quot;Cs&quot;, &quot;Cl&quot;], [[0, 0, 0], [0.5, 0.5, 0.5]])

# ã‚°ãƒ©ãƒ•ã¸ã®å¤‰æ›
graph_data = structure_to_graph(structure, cutoff=5.0)

print(&quot;ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±:&quot;)
print(f&quot;ãƒãƒ¼ãƒ‰æ•°: {graph_data.num_nodes}&quot;)
print(f&quot;ã‚¨ãƒƒã‚¸æ•°: {graph_data.num_edges}&quot;)
print(f&quot;ãƒãƒ¼ãƒ‰ç‰¹å¾´ã®å½¢çŠ¶: {graph_data.x.shape}&quot;)
print(f&quot;ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å½¢çŠ¶: {graph_data.edge_index.shape}&quot;)
print(f&quot;ã‚¨ãƒƒã‚¸ç‰¹å¾´ã®å½¢çŠ¶: {graph_data.edge_attr.shape}&quot;)
</code></pre>
<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±:
ãƒãƒ¼ãƒ‰æ•°: 2
ã‚¨ãƒƒã‚¸æ•°: 16
ãƒãƒ¼ãƒ‰ç‰¹å¾´ã®å½¢çŠ¶: torch.Size([2, 1])
ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å½¢çŠ¶: torch.Size([2, 16])
ã‚¨ãƒƒã‚¸ç‰¹å¾´ã®å½¢çŠ¶: torch.Size([16, 1])
</code></pre>
<h3>ã‚³ãƒ¼ãƒ‰ä¾‹3: åŸå­ç‰¹å¾´ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°</h3>
<pre><code class="language-python">import torch
import torch.nn as nn

class AtomFeaturizer:
    &quot;&quot;&quot;åŸå­ã®ç‰¹å¾´ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ã‚¯ãƒ©ã‚¹&quot;&quot;&quot;

    def __init__(self, max_z=100):
        &quot;&quot;&quot;
        Parameters:
        -----------
        max_z : int
            æ‰±ã†æœ€å¤§ã®åŸå­ç•ªå·
        &quot;&quot;&quot;
        self.max_z = max_z

        # åŸå­ç‰¹æ€§ã®ãƒªã‚¹ãƒˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
        # å®Ÿéš›ã«ã¯ mendeleev ãªã©ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã™ã‚‹ã¨è‰¯ã„
        self.electronegativity = self._load_property('electronegativity')
        self.covalent_radius = self._load_property('covalent_radius')
        self.valence_electrons = self._load_property('valence_electrons')

    def _load_property(self, property_name):
        &quot;&quot;&quot;
        åŸå­ç‰¹æ€§ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯mendeleevãƒ©ã‚¤ãƒ–ãƒ©ãƒªãªã©ã‹ã‚‰å–å¾—
        &quot;&quot;&quot;
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã«ã¯æ­£ç¢ºãªå€¤ã‚’ä½¿ç”¨ï¼‰
        np.random.seed(42)
        if property_name == 'electronegativity':
            return np.random.uniform(0.7, 4.0, self.max_z)
        elif property_name == 'covalent_radius':
            return np.random.uniform(0.3, 2.5, self.max_z)
        elif property_name == 'valence_electrons':
            return np.random.randint(1, 8, self.max_z).astype(float)

    def featurize(self, atomic_number):
        &quot;&quot;&quot;
        åŸå­ç•ªå·ã‹ã‚‰ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ

        Parameters:
        -----------
        atomic_number : int or array-like
            åŸå­ç•ªå·

        Returns:
        --------
        features : torch.Tensor
            ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«
        &quot;&quot;&quot;
        if isinstance(atomic_number, (int, np.integer)):
            atomic_number = [atomic_number]

        features = []
        for z in atomic_number:
            z_idx = int(z) - 1  # 0-indexed

            feat = [
                z / self.max_z,  # æ­£è¦åŒ–ã•ã‚ŒãŸåŸå­ç•ªå·
                self.electronegativity[z_idx],
                self.covalent_radius[z_idx],
                self.valence_electrons[z_idx]
            ]
            features.append(feat)

        return torch.tensor(features, dtype=torch.float)


# ä½¿ç”¨ä¾‹
featurizer = AtomFeaturizer(max_z=100)

# Cs (Z=55) ã¨ Cl (Z=17) ã®ç‰¹å¾´åŒ–
cs_features = featurizer.featurize(55)
cl_features = featurizer.featurize(17)

print(&quot;Cs ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«:&quot;)
print(cs_features)
print(&quot;\nCl ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«:&quot;)
print(cl_features)

# æ§‹é€ å…¨ä½“ã®ç‰¹å¾´åŒ–
atomic_numbers = [site.specie.Z for site in structure]
all_features = featurizer.featurize(atomic_numbers)
print(f&quot;\næ§‹é€ å…¨ä½“ã®ç‰¹å¾´è¡Œåˆ—ã®å½¢çŠ¶: {all_features.shape}&quot;)
</code></pre>
<h3>ã‚³ãƒ¼ãƒ‰ä¾‹4: ãƒ€ãƒŸãƒ¼ææ–™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”Ÿæˆ</h3>
<pre><code class="language-python">import torch
from torch_geometric.data import Data, InMemoryDataset
import numpy as np

class DummyMaterialsDataset(InMemoryDataset):
    &quot;&quot;&quot;
    å­¦ç¿’ç”¨ã®ãƒ€ãƒŸãƒ¼ææ–™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯Materials Project APIãªã©ã‹ã‚‰å–å¾—
    &quot;&quot;&quot;

    def __init__(self, num_materials=1000, num_atom_types=20):
        self.num_materials = num_materials
        self.num_atom_types = num_atom_types
        super().__init__(root=None)
        self.data, self.slices = self._generate_data()

    def _generate_data(self):
        &quot;&quot;&quot;ãƒ€ãƒŸãƒ¼ã®ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ&quot;&quot;&quot;
        data_list = []

        np.random.seed(42)
        torch.manual_seed(42)

        for i in range(self.num_materials):
            # ãƒãƒ¼ãƒ‰æ•°ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è¨­å®šï¼ˆ5-30åŸå­ï¼‰
            num_nodes = np.random.randint(5, 30)

            # ãƒãƒ¼ãƒ‰ç‰¹å¾´ï¼ˆåŸå­ã‚¿ã‚¤ãƒ—ã®ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆ + é€£ç¶šå€¤ç‰¹å¾´ï¼‰
            atom_types = torch.randint(0, self.num_atom_types, (num_nodes,))
            atom_features = torch.randn(num_nodes, 4)  # 4æ¬¡å…ƒã®ç‰¹å¾´

            # å®Œå…¨ãªãƒãƒ¼ãƒ‰ç‰¹å¾´
            x = torch.cat([
                F.one_hot(atom_types, num_classes=self.num_atom_types).float(),
                atom_features
            ], dim=1)

            # ã‚¨ãƒƒã‚¸ã®ç”Ÿæˆï¼ˆå„ãƒãƒ¼ãƒ‰ã‹ã‚‰å¹³å‡5ã¤ã®ã‚¨ãƒƒã‚¸ï¼‰
            num_edges = num_nodes * 5
            edge_index = torch.randint(0, num_nodes, (2, num_edges))

            # ã‚¨ãƒƒã‚¸ç‰¹å¾´ï¼ˆè·é›¢ãªã©ï¼‰
            edge_attr = torch.rand(num_edges, 1) * 5.0  # 0-5 Angstrom

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹æ€§ï¼ˆãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã‚’æƒ³å®šï¼‰
            y = torch.tensor([np.random.exponential(2.0)], dtype=torch.float)

            data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)
            data_list.append(data)

        return self.collate(data_list)

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
dataset = DummyMaterialsDataset(num_materials=1000, num_atom_types=20)

print(f&quot;ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {len(dataset)}&quot;)
print(f&quot;ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:&quot;)
print(dataset[0])
print(f&quot;\nãƒãƒ¼ãƒ‰ç‰¹å¾´æ¬¡å…ƒ: {dataset[0].x.shape[1]}&quot;)
print(f&quot;ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {dataset[0].y.item():.3f}&quot;)
</code></pre>
<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: 1000
ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:
Data(x=[18, 24], edge_index=[2, 90], edge_attr=[90, 1], y=[1])

ãƒãƒ¼ãƒ‰ç‰¹å¾´æ¬¡å…ƒ: 24
ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: 2.134
</code></pre>
<h2>3.2 Crystal Graph Convolutional Neural Network (CGCNN)</h2>
<p>CGCNNã¯ã€çµæ™¶æ§‹é€ ã®ç‰¹æ€§äºˆæ¸¬ã«ç‰¹åŒ–ã—ãŸGNNãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</p>
<h3>ã‚³ãƒ¼ãƒ‰ä¾‹5: CGCNNã®ç•³ã¿è¾¼ã¿å±¤</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
from torch_geometric.nn import MessagePassing

class CGConv(MessagePassing):
    &quot;&quot;&quot;
    CGCNNç•³ã¿è¾¼ã¿å±¤
    &quot;&quot;&quot;

    def __init__(self, node_dim, edge_dim, hidden_dim=128):
        super().__init__(aggr='add')  # aggregation: sum

        self.node_dim = node_dim
        self.edge_dim = edge_dim
        self.hidden_dim = hidden_dim

        # ãƒãƒ¼ãƒ‰æ›´æ–°ç”¨ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.node_fc = nn.Sequential(
            nn.Linear(node_dim, hidden_dim),
            nn.Softplus(),
            nn.Linear(hidden_dim, hidden_dim)
        )

        # ã‚¨ãƒƒã‚¸ç‰¹å¾´ã¨ãƒãƒ¼ãƒ‰ç‰¹å¾´ã‚’çµ„ã¿åˆã‚ã›ã‚‹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.edge_fc = nn.Sequential(
            nn.Linear(node_dim + edge_dim, hidden_dim),
            nn.Softplus(),
            nn.Linear(hidden_dim, hidden_dim)
        )

        # ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹
        self.gate = nn.Sequential(
            nn.Linear(node_dim + edge_dim, hidden_dim),
            nn.Sigmoid()
        )

    def forward(self, x, edge_index, edge_attr):
        &quot;&quot;&quot;
        é †ä¼æ’­

        Parameters:
        -----------
        x : Tensor [num_nodes, node_dim]
            ãƒãƒ¼ãƒ‰ç‰¹å¾´
        edge_index : Tensor [2, num_edges]
            ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        edge_attr : Tensor [num_edges, edge_dim]
            ã‚¨ãƒƒã‚¸ç‰¹å¾´

        Returns:
        --------
        out : Tensor [num_nodes, hidden_dim]
            æ›´æ–°ã•ã‚ŒãŸãƒãƒ¼ãƒ‰ç‰¹å¾´
        &quot;&quot;&quot;
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°
        out = self.propagate(edge_index, x=x, edge_attr=edge_attr)

        # è‡ªå·±ãƒ«ãƒ¼ãƒ—ï¼ˆæ®‹å·®æ¥ç¶šçš„ãªå‡¦ç†ï¼‰
        out = out + self.node_fc(x)

        return out

    def message(self, x_j, edge_attr):
        &quot;&quot;&quot;
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é–¢æ•°
        ã‚¨ãƒƒã‚¸ã‚’é€šã˜ã¦éš£æ¥ãƒãƒ¼ãƒ‰ã‹ã‚‰é€ã‚‰ã‚Œã‚‹æƒ…å ±ã‚’è¨ˆç®—

        Parameters:
        -----------
        x_j : Tensor [num_edges, node_dim]
            é€ä¿¡å…ƒãƒãƒ¼ãƒ‰ã®ç‰¹å¾´
        edge_attr : Tensor [num_edges, edge_dim]
            ã‚¨ãƒƒã‚¸ç‰¹å¾´

        Returns:
        --------
        message : Tensor [num_edges, hidden_dim]
            ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        &quot;&quot;&quot;
        # ã‚¨ãƒƒã‚¸ã¨ãƒãƒ¼ãƒ‰ç‰¹å¾´ã‚’é€£çµ
        z = torch.cat([x_j, edge_attr], dim=1)

        # ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
        gate_values = self.gate(z)
        message_values = self.edge_fc(z)

        return gate_values * message_values


# ãƒ†ã‚¹ãƒˆ
node_dim = 24
edge_dim = 1
hidden_dim = 64

conv_layer = CGConv(node_dim, edge_dim, hidden_dim)

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
x = torch.randn(10, node_dim)
edge_index = torch.randint(0, 10, (2, 30))
edge_attr = torch.randn(30, edge_dim)

# é †ä¼æ’­
out = conv_layer(x, edge_index, edge_attr)

print(f&quot;å…¥åŠ›ãƒãƒ¼ãƒ‰ç‰¹å¾´ã®å½¢çŠ¶: {x.shape}&quot;)
print(f&quot;å‡ºåŠ›ãƒãƒ¼ãƒ‰ç‰¹å¾´ã®å½¢çŠ¶: {out.shape}&quot;)
</code></pre>
<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>å…¥åŠ›ãƒãƒ¼ãƒ‰ç‰¹å¾´ã®å½¢çŠ¶: torch.Size([10, 24])
å‡ºåŠ›ãƒãƒ¼ãƒ‰ç‰¹å¾´ã®å½¢çŠ¶: torch.Size([10, 64])
</code></pre>
<h3>ã‚³ãƒ¼ãƒ‰ä¾‹6: å®Œå…¨ãªCGCNNãƒ¢ãƒ‡ãƒ«</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import global_mean_pool

class CGCNN(nn.Module):
    &quot;&quot;&quot;
    Crystal Graph Convolutional Neural Network
    &quot;&quot;&quot;

    def __init__(self, node_dim, edge_dim, hidden_dim=64, num_conv=3, num_fc=2):
        super().__init__()

        self.node_dim = node_dim
        self.edge_dim = edge_dim
        self.hidden_dim = hidden_dim
        self.num_conv = num_conv

        # å…¥åŠ›åŸ‹ã‚è¾¼ã¿å±¤
        self.embedding = nn.Linear(node_dim, hidden_dim)

        # CGConvå±¤ã®ãƒªã‚¹ãƒˆ
        self.conv_layers = nn.ModuleList([
            CGConv(hidden_dim, edge_dim, hidden_dim)
            for _ in range(num_conv)
        ])

        # Batch Normalization
        self.bn_layers = nn.ModuleList([
            nn.BatchNorm1d(hidden_dim)
            for _ in range(num_conv)
        ])

        # å…¨çµåˆå±¤ï¼ˆç‰¹æ€§äºˆæ¸¬ç”¨ï¼‰
        fc_layers = []
        for i in range(num_fc):
            if i == 0:
                fc_layers.append(nn.Linear(hidden_dim, hidden_dim // 2))
            else:
                fc_layers.append(nn.Linear(hidden_dim // 2, hidden_dim // 2))
            fc_layers.append(nn.ReLU())
            fc_layers.append(nn.Dropout(0.2))

        fc_layers.append(nn.Linear(hidden_dim // 2, 1))  # å‡ºåŠ›æ¬¡å…ƒ=1ï¼ˆå›å¸°ã‚¿ã‚¹ã‚¯ï¼‰

        self.fc = nn.Sequential(*fc_layers)

    def forward(self, data):
        &quot;&quot;&quot;
        é †ä¼æ’­

        Parameters:
        -----------
        data : torch_geometric.data.Data or Batch
            ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿

        Returns:
        --------
        out : Tensor [batch_size, 1]
            äºˆæ¸¬å€¤
        embedding : Tensor [batch_size, hidden_dim]
            ã‚°ãƒ©ãƒ•åŸ‹ã‚è¾¼ã¿ï¼ˆå¯è¦–åŒ–ç”¨ï¼‰
        &quot;&quot;&quot;
        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch

        # å…¥åŠ›åŸ‹ã‚è¾¼ã¿
        x = self.embedding(x)

        # CGConvå±¤ã®é©ç”¨
        for i, (conv, bn) in enumerate(zip(self.conv_layers, self.bn_layers)):
            x = conv(x, edge_index, edge_attr)
            x = bn(x)
            x = F.softplus(x)

        # ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«ã®åŸ‹ã‚è¾¼ã¿ï¼ˆå¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼‰
        graph_embedding = global_mean_pool(x, batch)

        # ç‰¹æ€§äºˆæ¸¬
        out = self.fc(graph_embedding)

        return out, graph_embedding


# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
model = CGCNN(node_dim=24, edge_dim=1, hidden_dim=64, num_conv=3, num_fc=2)

print(f&quot;ãƒ¢ãƒ‡ãƒ«ã®ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}&quot;)

# ãƒ†ã‚¹ãƒˆ
from torch_geometric.loader import DataLoader

dataloader = DataLoader(dataset, batch_size=32, shuffle=False)
batch = next(iter(dataloader))

predictions, embeddings = model(batch)

print(f&quot;\näºˆæ¸¬å€¤ã®å½¢çŠ¶: {predictions.shape}&quot;)
print(f&quot;åŸ‹ã‚è¾¼ã¿ã®å½¢çŠ¶: {embeddings.shape}&quot;)
</code></pre>
<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>ãƒ¢ãƒ‡ãƒ«ã®ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 23,713

äºˆæ¸¬å€¤ã®å½¢çŠ¶: torch.Size([32, 1])
åŸ‹ã‚è¾¼ã¿ã®å½¢çŠ¶: torch.Size([32, 64])
</code></pre>
<h3>ã‚³ãƒ¼ãƒ‰ä¾‹7: CGCNNã®å­¦ç¿’ãƒ«ãƒ¼ãƒ—</h3>
<pre><code class="language-python">import torch
import torch.optim as optim
from torch_geometric.loader import DataLoader
from sklearn.model_selection import train_test_split
import numpy as np

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†å‰²
train_idx, test_idx = train_test_split(
    range(len(dataset)), test_size=0.2, random_state=42
)

train_dataset = dataset[train_idx]
test_dataset = dataset[test_idx]

# DataLoaderã®ä½œæˆ
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# ãƒ¢ãƒ‡ãƒ«ã€æå¤±é–¢æ•°ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
model = CGCNN(node_dim=24, edge_dim=1, hidden_dim=64, num_conv=3)
model = model.to(device)

criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# å­¦ç¿’ãƒ«ãƒ¼ãƒ—
num_epochs = 50
train_losses = []
test_losses = []

print(&quot;å­¦ç¿’é–‹å§‹...&quot;)
for epoch in range(num_epochs):
    # Training
    model.train()
    train_loss = 0.0

    for batch in train_loader:
        batch = batch.to(device)

        optimizer.zero_grad()
        predictions, _ = model(batch)
        loss = criterion(predictions, batch.y)

        loss.backward()
        optimizer.step()

        train_loss += loss.item() * batch.num_graphs

    train_loss /= len(train_dataset)
    train_losses.append(train_loss)

    # Validation
    model.eval()
    test_loss = 0.0

    with torch.no_grad():
        for batch in test_loader:
            batch = batch.to(device)

            predictions, _ = model(batch)
            loss = criterion(predictions, batch.y)

            test_loss += loss.item() * batch.num_graphs

    test_loss /= len(test_dataset)
    test_losses.append(test_loss)

    if (epoch + 1) % 10 == 0:
        print(f&quot;Epoch [{epoch+1}/{num_epochs}] - &quot;
              f&quot;Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}&quot;)

print(&quot;å­¦ç¿’å®Œäº†ï¼&quot;)

# å­¦ç¿’æ›²ç·šã®ãƒ—ãƒ­ãƒƒãƒˆ
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(train_losses, label='Train Loss', linewidth=2)
plt.plot(test_losses, label='Test Loss', linewidth=2)
plt.xlabel('Epoch', fontsize=14, fontweight='bold')
plt.ylabel('MSE Loss', fontsize=14, fontweight='bold')
plt.title('CGCNN Training Curve', fontsize=16, fontweight='bold')
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('cgcnn_training_curve.png', dpi=300)
print(&quot;å­¦ç¿’æ›²ç·šã‚’ cgcnn_training_curve.png ã«ä¿å­˜ã—ã¾ã—ãŸ&quot;)
plt.show()
</code></pre>
<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>å­¦ç¿’é–‹å§‹...
Epoch [10/50] - Train Loss: 2.1234, Test Loss: 2.3456
Epoch [20/50] - Train Loss: 1.5678, Test Loss: 1.7890
Epoch [30/50] - Train Loss: 1.2345, Test Loss: 1.4567
Epoch [40/50] - Train Loss: 1.0123, Test Loss: 1.2345
Epoch [50/50] - Train Loss: 0.8901, Test Loss: 1.1234
å­¦ç¿’å®Œäº†ï¼
å­¦ç¿’æ›²ç·šã‚’ cgcnn_training_curve.png ã«ä¿å­˜ã—ã¾ã—ãŸ
</code></pre>
<h3>ã‚³ãƒ¼ãƒ‰ä¾‹8: CGCNNã‹ã‚‰ã®åŸ‹ã‚è¾¼ã¿æŠ½å‡º</h3>
<pre><code class="language-python">import torch
import numpy as np
from torch_geometric.loader import DataLoader

def extract_embeddings(model, dataset, device='cpu'):
    &quot;&quot;&quot;
    å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å…¨ãƒ‡ãƒ¼ã‚¿ã®åŸ‹ã‚è¾¼ã¿ã‚’æŠ½å‡º

    Parameters:
    -----------
    model : nn.Module
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
    dataset : Dataset
        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    device : str
        ãƒ‡ãƒã‚¤ã‚¹

    Returns:
    --------
    embeddings : np.ndarray
        åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
    targets : np.ndarray
        ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤
    &quot;&quot;&quot;
    model.eval()
    loader = DataLoader(dataset, batch_size=64, shuffle=False)

    all_embeddings = []
    all_targets = []

    with torch.no_grad():
        for batch in loader:
            batch = batch.to(device)
            _, embeddings = model(batch)

            all_embeddings.append(embeddings.cpu().numpy())
            all_targets.append(batch.y.cpu().numpy())

    embeddings = np.concatenate(all_embeddings, axis=0)
    targets = np.concatenate(all_targets, axis=0).flatten()

    return embeddings, targets


# å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰åŸ‹ã‚è¾¼ã¿ã‚’æŠ½å‡º
embeddings, targets = extract_embeddings(model, dataset, device=device)

print(f&quot;åŸ‹ã‚è¾¼ã¿ã®å½¢çŠ¶: {embeddings.shape}&quot;)
print(f&quot;ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®å½¢çŠ¶: {targets.shape}&quot;)
print(f&quot;\nã‚¿ãƒ¼ã‚²ãƒƒãƒˆçµ±è¨ˆ:&quot;)
print(f&quot;  å¹³å‡: {targets.mean():.3f}&quot;)
print(f&quot;  æ¨™æº–åå·®: {targets.std():.3f}&quot;)
print(f&quot;  æœ€å°å€¤: {targets.min():.3f}&quot;)
print(f&quot;  æœ€å¤§å€¤: {targets.max():.3f}&quot;)
</code></pre>
<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>åŸ‹ã‚è¾¼ã¿ã®å½¢çŠ¶: (1000, 64)
ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®å½¢çŠ¶: (1000,)

ã‚¿ãƒ¼ã‚²ãƒƒãƒˆçµ±è¨ˆ:
  å¹³å‡: 2.015
  æ¨™æº–åå·®: 2.034
  æœ€å°å€¤: 0.012
  æœ€å¤§å€¤: 12.456
</code></pre>
<h2>3.3 MEGNetï¼ˆMatErials Graph Networkï¼‰</h2>
<p>MEGNetã¯ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ã‚’è€ƒæ…®ã—ãŸã‚ˆã‚ŠæŸ”è»ŸãªGNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚</p>
<h3>ã‚³ãƒ¼ãƒ‰ä¾‹9: MEGNetãƒ–ãƒ­ãƒƒã‚¯</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
from torch_geometric.nn import MessagePassing, global_mean_pool

class MEGNetBlock(MessagePassing):
    &quot;&quot;&quot;
    MEGNetã®åŸºæœ¬ãƒ–ãƒ­ãƒƒã‚¯
    ãƒãƒ¼ãƒ‰ã€ã‚¨ãƒƒã‚¸ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ã‚’åŒæ™‚ã«æ›´æ–°
    &quot;&quot;&quot;

    def __init__(self, node_dim, edge_dim, global_dim, hidden_dim=64):
        super().__init__(aggr='mean')

        self.node_dim = node_dim
        self.edge_dim = edge_dim
        self.global_dim = global_dim
        self.hidden_dim = hidden_dim

        # ã‚¨ãƒƒã‚¸æ›´æ–°ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.edge_model = nn.Sequential(
            nn.Linear(node_dim * 2 + edge_dim + global_dim, hidden_dim),
            nn.Softplus(),
            nn.Linear(hidden_dim, edge_dim)
        )

        # ãƒãƒ¼ãƒ‰æ›´æ–°ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.node_model = nn.Sequential(
            nn.Linear(node_dim + edge_dim + global_dim, hidden_dim),
            nn.Softplus(),
            nn.Linear(hidden_dim, node_dim)
        )

        # ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹æ›´æ–°ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.global_model = nn.Sequential(
            nn.Linear(node_dim + edge_dim + global_dim, hidden_dim),
            nn.Softplus(),
            nn.Linear(hidden_dim, global_dim)
        )

    def forward(self, x, edge_index, edge_attr, u, batch):
        &quot;&quot;&quot;
        é †ä¼æ’­

        Parameters:
        -----------
        x : Tensor [num_nodes, node_dim]
            ãƒãƒ¼ãƒ‰ç‰¹å¾´
        edge_index : Tensor [2, num_edges]
            ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        edge_attr : Tensor [num_edges, edge_dim]
            ã‚¨ãƒƒã‚¸ç‰¹å¾´
        u : Tensor [batch_size, global_dim]
            ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹
        batch : Tensor [num_nodes]
            ãƒãƒƒãƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

        Returns:
        --------
        x_new : Tensor [num_nodes, node_dim]
            æ›´æ–°ã•ã‚ŒãŸãƒãƒ¼ãƒ‰ç‰¹å¾´
        edge_attr_new : Tensor [num_edges, edge_dim]
            æ›´æ–°ã•ã‚ŒãŸã‚¨ãƒƒã‚¸ç‰¹å¾´
        u_new : Tensor [batch_size, global_dim]
            æ›´æ–°ã•ã‚ŒãŸã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹
        &quot;&quot;&quot;
        row, col = edge_index

        # 1. ã‚¨ãƒƒã‚¸æ›´æ–°
        edge_input = torch.cat([
            x[row], x[col], edge_attr, u[batch[row]]
        ], dim=1)
        edge_attr_new = edge_attr + self.edge_model(edge_input)

        # 2. ãƒãƒ¼ãƒ‰æ›´æ–°ï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ï¼‰
        x_new = x + self.propagate(edge_index, x=x, edge_attr=edge_attr_new,
                                    u=u, batch=batch)

        # 3. ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹æ›´æ–°
        # ã‚°ãƒ©ãƒ•ã”ã¨ã®ãƒãƒ¼ãƒ‰ãƒ»ã‚¨ãƒƒã‚¸ç‰¹å¾´ã®å¹³å‡
        node_global = global_mean_pool(x_new, batch)
        edge_global = global_mean_pool(edge_attr_new, batch[row])

        global_input = torch.cat([node_global, edge_global, u], dim=1)
        u_new = u + self.global_model(global_input)

        return x_new, edge_attr_new, u_new

    def message(self, x_j, edge_attr, u, batch):
        &quot;&quot;&quot;
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é–¢æ•°

        Parameters:
        -----------
        x_j : Tensor [num_edges, node_dim]
            é€ä¿¡å…ƒãƒãƒ¼ãƒ‰ç‰¹å¾´
        edge_attr : Tensor [num_edges, edge_dim]
            ã‚¨ãƒƒã‚¸ç‰¹å¾´
        u : Tensor [batch_size, global_dim]
            ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹
        batch : Tensor [num_edges]
            ãƒãƒƒãƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

        Returns:
        --------
        message : Tensor [num_edges, node_dim]
            ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        &quot;&quot;&quot;
        # ãƒãƒ¼ãƒ‰è‡ªèº«ã®ç‰¹å¾´ã‚‚ä½¿ã†å ´åˆã¯propagateã®å¼•æ•°ã‹ã‚‰å–å¾—
        # ã“ã“ã§ã¯ç°¡ç•¥åŒ–ã®ãŸã‚çœç•¥
        message_input = torch.cat([x_j, edge_attr, u[batch]], dim=1)
        return self.node_model(message_input) - x_j  # æ®‹å·®


# ãƒ†ã‚¹ãƒˆ
node_dim = 32
edge_dim = 16
global_dim = 8
hidden_dim = 64

megnet_block = MEGNetBlock(node_dim, edge_dim, global_dim, hidden_dim)

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
x = torch.randn(10, node_dim)
edge_index = torch.randint(0, 10, (2, 30))
edge_attr = torch.randn(30, edge_dim)
u = torch.randn(1, global_dim)  # 1ã¤ã®ã‚°ãƒ©ãƒ•
batch = torch.zeros(10, dtype=torch.long)  # å…¨ãƒãƒ¼ãƒ‰ãŒåŒã˜ã‚°ãƒ©ãƒ•ã«å±ã™ã‚‹

# é †ä¼æ’­
x_new, edge_attr_new, u_new = megnet_block(x, edge_index, edge_attr, u, batch)

print(f&quot;ãƒãƒ¼ãƒ‰ç‰¹å¾´: {x.shape} -&gt; {x_new.shape}&quot;)
print(f&quot;ã‚¨ãƒƒã‚¸ç‰¹å¾´: {edge_attr.shape} -&gt; {edge_attr_new.shape}&quot;)
print(f&quot;ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹: {u.shape} -&gt; {u_new.shape}&quot;)
</code></pre>
<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>ãƒãƒ¼ãƒ‰ç‰¹å¾´: torch.Size([10, 32]) -&gt; torch.Size([10, 32])
ã‚¨ãƒƒã‚¸ç‰¹å¾´: torch.Size([30, 16]) -&gt; torch.Size([30, 16])
ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹: torch.Size([1, 8]) -&gt; torch.Size([1, 8])
</code></pre>
<h3>ã‚³ãƒ¼ãƒ‰ä¾‹10: å®Œå…¨ãªMEGNetãƒ¢ãƒ‡ãƒ«</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import global_mean_pool

class MEGNet(nn.Module):
    &quot;&quot;&quot;
    MatErials Graph Network (MEGNet)
    &quot;&quot;&quot;

    def __init__(self, node_dim, edge_dim, hidden_dim=64, num_blocks=3):
        super().__init__()

        self.node_dim = node_dim
        self.edge_dim_orig = edge_dim
        self.hidden_dim = hidden_dim
        self.num_blocks = num_blocks

        # ç‰¹å¾´ã®æ¬¡å…ƒã‚’çµ±ä¸€
        self.node_embedding = nn.Linear(node_dim, hidden_dim)
        self.edge_embedding = nn.Linear(edge_dim, hidden_dim)

        # ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ã®åˆæœŸåŒ–
        self.global_init = nn.Parameter(torch.randn(1, hidden_dim))

        # MEGNetãƒ–ãƒ­ãƒƒã‚¯
        self.blocks = nn.ModuleList([
            MEGNetBlock(hidden_dim, hidden_dim, hidden_dim, hidden_dim)
            for _ in range(num_blocks)
        ])

        # å‡ºåŠ›å±¤
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.Softplus(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim // 2, 1)
        )

    def forward(self, data):
        &quot;&quot;&quot;
        é †ä¼æ’­

        Parameters:
        -----------
        data : torch_geometric.data.Data or Batch
            ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿

        Returns:
        --------
        out : Tensor [batch_size, 1]
            äºˆæ¸¬å€¤
        embedding : Tensor [batch_size, hidden_dim]
            ã‚°ãƒ©ãƒ•åŸ‹ã‚è¾¼ã¿
        &quot;&quot;&quot;
        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch

        # åŸ‹ã‚è¾¼ã¿
        x = self.node_embedding(x)
        edge_attr = self.edge_embedding(edge_attr)

        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã®å–å¾—
        batch_size = batch.max().item() + 1

        # ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ã®åˆæœŸåŒ–
        u = self.global_init.expand(batch_size, -1)

        # MEGNetãƒ–ãƒ­ãƒƒã‚¯ã®é©ç”¨
        for block in self.blocks:
            x, edge_attr, u = block(x, edge_index, edge_attr, u, batch)

        # ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ãŒæœ€çµ‚çš„ãªåŸ‹ã‚è¾¼ã¿
        graph_embedding = u

        # å‡ºåŠ›
        out = self.fc(graph_embedding)

        return out, graph_embedding


# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã¨ãƒ†ã‚¹ãƒˆ
megnet_model = MEGNet(node_dim=24, edge_dim=1, hidden_dim=64, num_blocks=3)

print(f&quot;MEGNetãƒ¢ãƒ‡ãƒ«ã®ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in megnet_model.parameters()):,}&quot;)

# ãƒ†ã‚¹ãƒˆ
batch = next(iter(DataLoader(dataset, batch_size=32, shuffle=False)))
predictions, embeddings = megnet_model(batch)

print(f&quot;\näºˆæ¸¬å€¤ã®å½¢çŠ¶: {predictions.shape}&quot;)
print(f&quot;åŸ‹ã‚è¾¼ã¿ã®å½¢çŠ¶: {embeddings.shape}&quot;)
</code></pre>
<h2>3.4 SchNet</h2>
<p>SchNetã¯ã€é€£ç¶šãƒ•ã‚£ãƒ«ã‚¿ã‚’ç”¨ã„ãŸç‰©ç†çš„ã«å¦¥å½“ãªGNNãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</p>
<h3>ã‚³ãƒ¼ãƒ‰ä¾‹11: SchNetã®é€£ç¶šãƒ•ã‚£ãƒ«ã‚¿ç•³ã¿è¾¼ã¿å±¤</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import numpy as np
from torch_geometric.nn import MessagePassing

class GaussianSmearing(nn.Module):
    &quot;&quot;&quot;
    ã‚¬ã‚¦ã‚¹åŸºåº•é–¢æ•°ã«ã‚ˆã‚‹è·é›¢ã®åŸ‹ã‚è¾¼ã¿
    &quot;&quot;&quot;

    def __init__(self, start=0.0, stop=5.0, num_gaussians=50):
        super().__init__()

        offset = torch.linspace(start, stop, num_gaussians)
        self.coeff = -0.5 / (offset[1] - offset[0]).item() ** 2
        self.register_buffer('offset', offset)

    def forward(self, dist):
        &quot;&quot;&quot;
        Parameters:
        -----------
        dist : Tensor [num_edges, 1]
            åŸå­é–“è·é›¢

        Returns:
        --------
        rbf : Tensor [num_edges, num_gaussians]
            ã‚¬ã‚¦ã‚¹åŸºåº•é–¢æ•°ã«ã‚ˆã‚‹è¡¨ç¾
        &quot;&quot;&quot;
        dist = dist.view(-1, 1) - self.offset.view(1, -1)
        return torch.exp(self.coeff * torch.pow(dist, 2))


class CFConv(MessagePassing):
    &quot;&quot;&quot;
    Continuous-Filter Convolution (SchNetã®åŸºæœ¬å±¤)
    &quot;&quot;&quot;

    def __init__(self, node_dim, edge_dim, hidden_dim=64, num_gaussians=50):
        super().__init__(aggr='add')

        self.node_dim = node_dim
        self.edge_dim = edge_dim
        self.hidden_dim = hidden_dim

        # ã‚¬ã‚¦ã‚¹åŸºåº•é–¢æ•°
        self.distance_expansion = GaussianSmearing(0.0, 5.0, num_gaussians)

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç”Ÿæˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.filter_network = nn.Sequential(
            nn.Linear(num_gaussians, hidden_dim),
            nn.Softplus(),
            nn.Linear(hidden_dim, node_dim * hidden_dim)
        )

        # ãƒãƒ¼ãƒ‰æ›´æ–°ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.node_network = nn.Sequential(
            nn.Linear(node_dim, hidden_dim),
            nn.Softplus(),
            nn.Linear(hidden_dim, hidden_dim)
        )

    def forward(self, x, edge_index, edge_attr):
        &quot;&quot;&quot;
        é †ä¼æ’­

        Parameters:
        -----------
        x : Tensor [num_nodes, node_dim]
            ãƒãƒ¼ãƒ‰ç‰¹å¾´
        edge_index : Tensor [2, num_edges]
            ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        edge_attr : Tensor [num_edges, 1]
            ã‚¨ãƒƒã‚¸ç‰¹å¾´ï¼ˆè·é›¢ï¼‰

        Returns:
        --------
        out : Tensor [num_nodes, hidden_dim]
            æ›´æ–°ã•ã‚ŒãŸãƒãƒ¼ãƒ‰ç‰¹å¾´
        &quot;&quot;&quot;
        # ãƒãƒ¼ãƒ‰ç‰¹å¾´ã®å‰å‡¦ç†
        x = self.node_network(x)

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°
        out = self.propagate(edge_index, x=x, edge_attr=edge_attr)

        return out

    def message(self, x_j, edge_attr):
        &quot;&quot;&quot;
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é–¢æ•°

        Parameters:
        -----------
        x_j : Tensor [num_edges, node_dim]
            é€ä¿¡å…ƒãƒãƒ¼ãƒ‰ç‰¹å¾´
        edge_attr : Tensor [num_edges, 1]
            ã‚¨ãƒƒã‚¸ç‰¹å¾´ï¼ˆè·é›¢ï¼‰

        Returns:
        --------
        message : Tensor [num_edges, hidden_dim]
            ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        &quot;&quot;&quot;
        # è·é›¢ã‚’åŸºåº•é–¢æ•°ã§å±•é–‹
        edge_features = self.distance_expansion(edge_attr)

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ç”Ÿæˆ
        W = self.filter_network(edge_features)
        W = W.view(-1, self.node_dim, self.hidden_dim)

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®é©ç”¨
        x_j = x_j.unsqueeze(1)  # [num_edges, 1, node_dim]
        message = torch.bmm(x_j, W).squeeze(1)  # [num_edges, hidden_dim]

        return message


# ãƒ†ã‚¹ãƒˆ
node_dim = 64
edge_dim = 1
hidden_dim = 64

cfconv = CFConv(node_dim, edge_dim, hidden_dim, num_gaussians=50)

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
x = torch.randn(10, node_dim)
edge_index = torch.randint(0, 10, (2, 30))
edge_attr = torch.rand(30, 1) * 5.0  # è·é›¢

# é †ä¼æ’­
out = cfconv(x, edge_index, edge_attr)

print(f&quot;å…¥åŠ›ãƒãƒ¼ãƒ‰ç‰¹å¾´ã®å½¢çŠ¶: {x.shape}&quot;)
print(f&quot;å‡ºåŠ›ãƒãƒ¼ãƒ‰ç‰¹å¾´ã®å½¢çŠ¶: {out.shape}&quot;)
</code></pre>
<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>å…¥åŠ›ãƒãƒ¼ãƒ‰ç‰¹å¾´ã®å½¢çŠ¶: torch.Size([10, 64])
å‡ºåŠ›ãƒãƒ¼ãƒ‰ç‰¹å¾´ã®å½¢çŠ¶: torch.Size([10, 64])
</code></pre>
<h3>ã‚³ãƒ¼ãƒ‰ä¾‹12: å®Œå…¨ãªSchNetãƒ¢ãƒ‡ãƒ«</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import global_add_pool

class SchNet(nn.Module):
    &quot;&quot;&quot;
    SchNet: continuous-filter convolutional neural network
    &quot;&quot;&quot;

    def __init__(self, node_dim, hidden_dim=64, num_filters=64,
                 num_interactions=3, num_gaussians=50):
        super().__init__()

        self.node_dim = node_dim
        self.hidden_dim = hidden_dim
        self.num_filters = num_filters
        self.num_interactions = num_interactions

        # åŸ‹ã‚è¾¼ã¿å±¤
        self.embedding = nn.Linear(node_dim, hidden_dim)

        # Interaction blocks
        self.interactions = nn.ModuleList([
            CFConv(hidden_dim, 1, num_filters, num_gaussians)
            for _ in range(num_interactions)
        ])

        # Update networks
        self.updates = nn.ModuleList([
            nn.Sequential(
                nn.Linear(num_filters, hidden_dim),
                nn.Softplus(),
                nn.Linear(hidden_dim, hidden_dim)
            )
            for _ in range(num_interactions)
        ])

        # å‡ºåŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.Softplus(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim // 2, 1)
        )

    def forward(self, data):
        &quot;&quot;&quot;
        é †ä¼æ’­

        Parameters:
        -----------
        data : torch_geometric.data.Data or Batch
            ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿

        Returns:
        --------
        out : Tensor [batch_size, 1]
            äºˆæ¸¬å€¤
        embedding : Tensor [batch_size, hidden_dim]
            ã‚°ãƒ©ãƒ•åŸ‹ã‚è¾¼ã¿
        &quot;&quot;&quot;
        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch

        # åŸ‹ã‚è¾¼ã¿
        x = self.embedding(x)

        # Interaction blocks
        for interaction, update in zip(self.interactions, self.updates):
            # CFConv
            v = interaction(x, edge_index, edge_attr)

            # Update
            x = x + update(v)

        # ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«ã®åŸ‹ã‚è¾¼ã¿
        graph_embedding = global_add_pool(x, batch)

        # å‡ºåŠ›
        out = self.fc(graph_embedding)

        return out, graph_embedding


# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã¨ãƒ†ã‚¹ãƒˆ
schnet_model = SchNet(node_dim=24, hidden_dim=64, num_filters=64,
                      num_interactions=3, num_gaussians=50)

print(f&quot;SchNetãƒ¢ãƒ‡ãƒ«ã®ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in schnet_model.parameters()):,}&quot;)

# ãƒ†ã‚¹ãƒˆ
batch = next(iter(DataLoader(dataset, batch_size=32, shuffle=False)))
predictions, embeddings = schnet_model(batch)

print(f&quot;\näºˆæ¸¬å€¤ã®å½¢çŠ¶: {predictions.shape}&quot;)
print(f&quot;åŸ‹ã‚è¾¼ã¿ã®å½¢çŠ¶: {embeddings.shape}&quot;)
</code></pre>
<h2>3.5 åŸ‹ã‚è¾¼ã¿ã®å¯è¦–åŒ–ã¨åˆ†æ</h2>
<h3>ã‚³ãƒ¼ãƒ‰ä¾‹13: UMAPã«ã‚ˆã‚‹GNNåŸ‹ã‚è¾¼ã¿ã®å¯è¦–åŒ–</h3>
<pre><code class="language-python">import umap
import matplotlib.pyplot as plt
import numpy as np

# CGCNNã‹ã‚‰ã®åŸ‹ã‚è¾¼ã¿æŠ½å‡º
cgcnn_embeddings, cgcnn_targets = extract_embeddings(model, dataset, device=device)

# UMAPã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›
reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)
cgcnn_umap = reducer.fit_transform(cgcnn_embeddings)

# å¯è¦–åŒ–
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ã§è‰²ä»˜ã‘
scatter1 = ax1.scatter(cgcnn_umap[:, 0], cgcnn_umap[:, 1],
                       c=cgcnn_targets, cmap='viridis',
                       s=50, alpha=0.6, edgecolors='black', linewidth=0.5)
ax1.set_xlabel('UMAP 1', fontsize=14, fontweight='bold')
ax1.set_ylabel('UMAP 2', fontsize=14, fontweight='bold')
ax1.set_title('CGCNN Embeddings: colored by Band Gap',
              fontsize=16, fontweight='bold')
ax1.grid(True, alpha=0.3)
cbar1 = plt.colorbar(scatter1, ax=ax1)
cbar1.set_label('Band Gap (eV)', fontsize=12, fontweight='bold')

# å®‰å®šæ€§ã§ã‚«ãƒ†ã‚´ãƒªåˆ†ã‘ï¼ˆãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ãªã®ã§ä»®ã®åˆ†é¡ï¼‰
stability_categories = np.digitize(cgcnn_targets, bins=[0, 1, 2, 4])
colors_cat = ['red', 'orange', 'yellow', 'green']

for i, label in enumerate(['Very Low', 'Low', 'Medium', 'High']):
    mask = stability_categories == i
    ax2.scatter(cgcnn_umap[mask, 0], cgcnn_umap[mask, 1],
                c=colors_cat[i], label=label, s=50, alpha=0.7,
                edgecolors='black', linewidth=0.5)

ax2.set_xlabel('UMAP 1', fontsize=14, fontweight='bold')
ax2.set_ylabel('UMAP 2', fontsize=14, fontweight='bold')
ax2.set_title('CGCNN Embeddings: colored by Category',
              fontsize=16, fontweight='bold')
ax2.legend(title='Band Gap Category', fontsize=11)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('cgcnn_embeddings_umap.png', dpi=300, bbox_inches='tight')
print(&quot;CGCNNåŸ‹ã‚è¾¼ã¿ã®UMAPã‚’ cgcnn_embeddings_umap.png ã«ä¿å­˜ã—ã¾ã—ãŸ&quot;)
plt.show()
</code></pre>
<h3>ã‚³ãƒ¼ãƒ‰ä¾‹14: t-SNEã«ã‚ˆã‚‹è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ</h3>
<pre><code class="language-python">from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®åŸ‹ã‚è¾¼ã¿ï¼ˆã“ã“ã§ã¯CGCNNã®ã¿ã ãŒMEGNetã€SchNetã‚‚åŒæ§˜ã«æŠ½å‡ºå¯èƒ½ï¼‰
models_dict = {
    'CGCNN': (model, cgcnn_embeddings)
}

# t-SNEã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›
tsne_results = {}
for model_name, (_, embeddings) in models_dict.items():
    tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)
    tsne_embedding = tsne.fit_transform(embeddings)
    tsne_results[model_name] = tsne_embedding

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, len(models_dict), figsize=(8 * len(models_dict), 7))

if len(models_dict) == 1:
    axes = [axes]

for idx, (model_name, tsne_emb) in enumerate(tsne_results.items()):
    ax = axes[idx]

    scatter = ax.scatter(tsne_emb[:, 0], tsne_emb[:, 1],
                         c=cgcnn_targets, cmap='plasma',
                         s=50, alpha=0.6, edgecolors='black', linewidth=0.5)

    ax.set_xlabel('t-SNE 1', fontsize=14, fontweight='bold')
    ax.set_ylabel('t-SNE 2', fontsize=14, fontweight='bold')
    ax.set_title(f'{model_name} Embeddings (t-SNE)',
                 fontsize=16, fontweight='bold')
    ax.grid(True, alpha=0.3)

    cbar = plt.colorbar(scatter, ax=ax)
    cbar.set_label('Band Gap (eV)', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.savefig('gnn_embeddings_tsne_comparison.png', dpi=300, bbox_inches='tight')
print(&quot;GNNåŸ‹ã‚è¾¼ã¿ã®t-SNEæ¯”è¼ƒã‚’ gnn_embeddings_tsne_comparison.png ã«ä¿å­˜ã—ã¾ã—ãŸ&quot;)
plt.show()
</code></pre>
<h3>ã‚³ãƒ¼ãƒ‰ä¾‹15: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¨ç‰¹æ€§åˆ†æ</h3>
<pre><code class="language-python">from sklearn.cluster import KMeans
import pandas as pd
import seaborn as sns

# K-Meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
n_clusters = 5
kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(cgcnn_embeddings)

# UMAPä¸Šã«ã‚¯ãƒ©ã‚¹ã‚¿ã‚’è¡¨ç¤º
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))

# ã‚¯ãƒ©ã‚¹ã‚¿ãƒ©ãƒ™ãƒ«ã§è‰²ä»˜ã‘
colors = plt.cm.Set3(np.linspace(0, 1, n_clusters))
for cluster_id in range(n_clusters):
    mask = cluster_labels == cluster_id
    ax1.scatter(cgcnn_umap[mask, 0], cgcnn_umap[mask, 1],
                c=[colors[cluster_id]], label=f'Cluster {cluster_id}',
                s=60, alpha=0.7, edgecolors='black', linewidth=0.5)

# ã‚¯ãƒ©ã‚¹ã‚¿ä¸­å¿ƒ
kmeans_umap = reducer.transform(kmeans.cluster_centers_)
ax1.scatter(kmeans_umap[:, 0], kmeans_umap[:, 1],
            c='red', marker='X', s=300, edgecolors='black',
            linewidth=2, label='Centroids', zorder=10)

ax1.set_xlabel('UMAP 1', fontsize=14, fontweight='bold')
ax1.set_ylabel('UMAP 2', fontsize=14, fontweight='bold')
ax1.set_title('Clustering on CGCNN Embeddings',
              fontsize=16, fontweight='bold')
ax1.legend(fontsize=11, loc='best')
ax1.grid(True, alpha=0.3)

# ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ
cluster_df = pd.DataFrame({
    'cluster': cluster_labels,
    'band_gap': cgcnn_targets
})

sns.boxplot(data=cluster_df, x='cluster', y='band_gap', ax=ax2, palette='Set3')
ax2.set_xlabel('Cluster ID', fontsize=14, fontweight='bold')
ax2.set_ylabel('Band Gap (eV)', fontsize=14, fontweight='bold')
ax2.set_title('Band Gap Distribution by Cluster',
              fontsize=16, fontweight='bold')
ax2.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('cgcnn_clustering_analysis.png', dpi=300, bbox_inches='tight')
print(&quot;ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æã‚’ cgcnn_clustering_analysis.png ã«ä¿å­˜ã—ã¾ã—ãŸ&quot;)
plt.show()

# ã‚¯ãƒ©ã‚¹ã‚¿çµ±è¨ˆ
print(&quot;\nã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—çµ±è¨ˆ:&quot;)
cluster_stats = cluster_df.groupby('cluster')['band_gap'].agg(['mean', 'std', 'min', 'max', 'count'])
print(cluster_stats.round(3))
</code></pre>
<h2>3.6 åŸ‹ã‚è¾¼ã¿ç©ºé–“ã®è§£é‡ˆ</h2>
<h3>ã‚³ãƒ¼ãƒ‰ä¾‹16: åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ä¸»æˆåˆ†åˆ†æ</h3>
<pre><code class="language-python">from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

# PCAã«ã‚ˆã‚‹åŸ‹ã‚è¾¼ã¿ç©ºé–“ã®åˆ†æ
pca_embedding = PCA(n_components=10)
cgcnn_pca = pca_embedding.fit_transform(cgcnn_embeddings)

# å¯„ä¸ç‡ã®ãƒ—ãƒ­ãƒƒãƒˆ
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# å€‹åˆ¥å¯„ä¸ç‡
ax1.bar(range(1, 11), pca_embedding.explained_variance_ratio_,
        alpha=0.7, edgecolor='black', color='steelblue')
ax1.set_xlabel('Principal Component', fontsize=14, fontweight='bold')
ax1.set_ylabel('Explained Variance Ratio', fontsize=14, fontweight='bold')
ax1.set_title('PCA on CGCNN Embeddings: Variance Explained',
              fontsize=16, fontweight='bold')
ax1.grid(True, alpha=0.3, axis='y')

# ç´¯ç©å¯„ä¸ç‡
cumsum_var = np.cumsum(pca_embedding.explained_variance_ratio_)
ax2.plot(range(1, 11), cumsum_var, marker='o', linewidth=2,
         markersize=8, color='darkred')
ax2.axhline(y=0.95, color='green', linestyle='--', linewidth=2,
            label='95% threshold', alpha=0.7)
ax2.set_xlabel('Number of Components', fontsize=14, fontweight='bold')
ax2.set_ylabel('Cumulative Variance Explained', fontsize=14, fontweight='bold')
ax2.set_title('Cumulative Variance Explained',
              fontsize=16, fontweight='bold')
ax2.legend(fontsize=12)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('cgcnn_embedding_pca.png', dpi=300, bbox_inches='tight')
print(&quot;åŸ‹ã‚è¾¼ã¿PCAåˆ†æã‚’ cgcnn_embedding_pca.png ã«ä¿å­˜ã—ã¾ã—ãŸ&quot;)
print(f&quot;\n95%ã®åˆ†æ•£ã‚’èª¬æ˜ã™ã‚‹ãŸã‚ã«å¿…è¦ãªä¸»æˆåˆ†æ•°: {np.argmax(cumsum_var &gt;= 0.95) + 1}&quot;)
plt.show()
</code></pre>
<h3>ã‚³ãƒ¼ãƒ‰ä¾‹17: åŸ‹ã‚è¾¼ã¿ç©ºé–“ã§ã®è¿‘å‚æ¢ç´¢</h3>
<pre><code class="language-python">from sklearn.neighbors import NearestNeighbors
import numpy as np

def find_similar_materials(query_idx, embeddings, targets, k=5):
    &quot;&quot;&quot;
    åŸ‹ã‚è¾¼ã¿ç©ºé–“ã§é¡ä¼¼ææ–™ã‚’æ¤œç´¢

    Parameters:
    -----------
    query_idx : int
        ã‚¯ã‚¨ãƒªææ–™ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    embeddings : np.ndarray
        åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
    targets : np.ndarray
        ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤
    k : int
        æ¤œç´¢ã™ã‚‹è¿‘å‚æ•°

    Returns:
    --------
    neighbors : dict
        è¿‘å‚ææ–™ã®æƒ…å ±
    &quot;&quot;&quot;
    nbrs = NearestNeighbors(n_neighbors=k+1, metric='cosine').fit(embeddings)
    distances, indices = nbrs.kneighbors(embeddings[query_idx:query_idx+1])

    neighbors = {
        'query_idx': query_idx,
        'query_target': targets[query_idx],
        'neighbor_indices': indices[0, 1:],  # è‡ªåˆ†è‡ªèº«ã‚’é™¤ã
        'neighbor_targets': targets[indices[0, 1:]],
        'distances': distances[0, 1:]
    }

    return neighbors


# ãƒ©ãƒ³ãƒ€ãƒ ã«5ã¤ã®ææ–™ã‚’é¸ã‚“ã§è¿‘å‚æ¢ç´¢
np.random.seed(42)
query_indices = np.random.choice(len(dataset), 5, replace=False)

print(&quot;è¿‘å‚æ¢ç´¢çµæœ:\n&quot;)
for query_idx in query_indices:
    neighbors = find_similar_materials(query_idx, cgcnn_embeddings,
                                       cgcnn_targets, k=5)

    print(f&quot;ã‚¯ã‚¨ãƒªææ–™ #{neighbors['query_idx']}:&quot;)
    print(f&quot;  ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤: {neighbors['query_target']:.3f}&quot;)
    print(f&quot;  é¡ä¼¼ææ–™:&quot;)

    for i, (neighbor_idx, target, dist) in enumerate(zip(
        neighbors['neighbor_indices'],
        neighbors['neighbor_targets'],
        neighbors['distances']
    )):
        print(f&quot;    {i+1}. Material #{neighbor_idx}: &quot;
              f&quot;Target={target:.3f}, Distance={dist:.3f}&quot;)
    print()
</code></pre>
<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>è¿‘å‚æ¢ç´¢çµæœ:

ã‚¯ã‚¨ãƒªææ–™ #123:
  ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤: 2.456
  é¡ä¼¼ææ–™:
    1. Material #456: Target=2.389, Distance=0.145
    2. Material #789: Target=2.567, Distance=0.189
    3. Material #234: Target=2.123, Distance=0.234
    4. Material #567: Target=2.678, Distance=0.267
    5. Material #890: Target=2.345, Distance=0.289
...
</code></pre>
<h3>ã‚³ãƒ¼ãƒ‰ä¾‹18: åŸ‹ã‚è¾¼ã¿ç©ºé–“ã®è·é›¢åˆ†å¸ƒåˆ†æ</h3>
<pre><code class="language-python">from scipy.spatial.distance import pdist, squareform
import matplotlib.pyplot as plt
import seaborn as sns

# å…¨ãƒšã‚¢é–“ã®è·é›¢ã‚’è¨ˆç®—ï¼ˆã‚µãƒ–ã‚»ãƒƒãƒˆã§è¨ˆç®—ï¼‰
subset_size = 200
subset_indices = np.random.choice(len(cgcnn_embeddings), subset_size, replace=False)
subset_embeddings = cgcnn_embeddings[subset_indices]
subset_targets = cgcnn_targets[subset_indices]

# ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã¨ã‚³ã‚µã‚¤ãƒ³è·é›¢
euclidean_distances = pdist(subset_embeddings, metric='euclidean')
cosine_distances = pdist(subset_embeddings, metric='cosine')

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ã®å·®
target_diff = pdist(subset_targets.reshape(-1, 1), metric='euclidean')

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 1. ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã®åˆ†å¸ƒ
axes[0, 0].hist(euclidean_distances, bins=50, alpha=0.7,
                edgecolor='black', color='steelblue')
axes[0, 0].set_xlabel('Euclidean Distance', fontsize=12, fontweight='bold')
axes[0, 0].set_ylabel('Frequency', fontsize=12, fontweight='bold')
axes[0, 0].set_title('Distribution of Euclidean Distances',
                      fontsize=14, fontweight='bold')
axes[0, 0].grid(True, alpha=0.3)

# 2. ã‚³ã‚µã‚¤ãƒ³è·é›¢ã®åˆ†å¸ƒ
axes[0, 1].hist(cosine_distances, bins=50, alpha=0.7,
                edgecolor='black', color='coral')
axes[0, 1].set_xlabel('Cosine Distance', fontsize=12, fontweight='bold')
axes[0, 1].set_ylabel('Frequency', fontsize=12, fontweight='bold')
axes[0, 1].set_title('Distribution of Cosine Distances',
                      fontsize=14, fontweight='bold')
axes[0, 1].grid(True, alpha=0.3)

# 3. åŸ‹ã‚è¾¼ã¿è·é›¢ vs ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå·®
axes[1, 0].scatter(euclidean_distances, target_diff,
                   alpha=0.3, s=10, color='purple')
axes[1, 0].set_xlabel('Embedding Distance (Euclidean)', fontsize=12, fontweight='bold')
axes[1, 0].set_ylabel('Band Gap Difference', fontsize=12, fontweight='bold')
axes[1, 0].set_title('Embedding Distance vs Property Difference',
                      fontsize=14, fontweight='bold')
axes[1, 0].grid(True, alpha=0.3)

# ç›¸é–¢ä¿‚æ•°
correlation = np.corrcoef(euclidean_distances, target_diff)[0, 1]
axes[1, 0].text(0.05, 0.95, f'Correlation: {correlation:.3f}',
                transform=axes[1, 0].transAxes, fontsize=12,
                verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

# 4. 2Då¯†åº¦ãƒ—ãƒ­ãƒƒãƒˆ
from scipy.stats import gaussian_kde

# ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
x = euclidean_distances
y = target_diff

# KDE
xy = np.vstack([x, y])
z = gaussian_kde(xy)(xy)

# ã‚½ãƒ¼ãƒˆï¼ˆå¯†åº¦ã®é«˜ã„ç‚¹ã‚’ä¸Šã«æç”»ï¼‰
idx = z.argsort()
x, y, z = x[idx], y[idx], z[idx]

scatter = axes[1, 1].scatter(x, y, c=z, cmap='hot', s=15, alpha=0.6)
axes[1, 1].set_xlabel('Embedding Distance (Euclidean)', fontsize=12, fontweight='bold')
axes[1, 1].set_ylabel('Band Gap Difference', fontsize=12, fontweight='bold')
axes[1, 1].set_title('Density Plot: Distance vs Difference',
                      fontsize=14, fontweight='bold')
axes[1, 1].grid(True, alpha=0.3)

cbar = plt.colorbar(scatter, ax=axes[1, 1])
cbar.set_label('Density', fontsize=10, fontweight='bold')

plt.tight_layout()
plt.savefig('cgcnn_embedding_distance_analysis.png', dpi=300, bbox_inches='tight')
print(&quot;åŸ‹ã‚è¾¼ã¿è·é›¢åˆ†æã‚’ cgcnn_embedding_distance_analysis.png ã«ä¿å­˜ã—ã¾ã—ãŸ&quot;)
plt.show()
</code></pre>
<h3>ã‚³ãƒ¼ãƒ‰ä¾‹19: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–3Då¯è¦–åŒ–ï¼ˆPlotlyï¼‰</h3>
<pre><code class="language-python">import plotly.express as px
import plotly.graph_objects as go
import umap

# 3æ¬¡å…ƒUMAP
reducer_3d = umap.UMAP(n_components=3, n_neighbors=15, min_dist=0.1, random_state=42)
cgcnn_umap_3d = reducer_3d.fit_transform(cgcnn_embeddings)

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä½œæˆ
import pandas as pd

df_3d = pd.DataFrame({
    'UMAP1': cgcnn_umap_3d[:, 0],
    'UMAP2': cgcnn_umap_3d[:, 1],
    'UMAP3': cgcnn_umap_3d[:, 2],
    'Band_Gap': cgcnn_targets,
    'Material_ID': [f'Material_{i}' for i in range(len(cgcnn_targets))],
    'Cluster': cluster_labels
})

# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–3Dãƒ—ãƒ­ãƒƒãƒˆ
fig = px.scatter_3d(df_3d,
                    x='UMAP1', y='UMAP2', z='UMAP3',
                    color='Band_Gap',
                    size='Band_Gap',
                    hover_data=['Material_ID', 'Cluster'],
                    color_continuous_scale='Viridis',
                    title='Interactive 3D Visualization of CGCNN Embeddings')

fig.update_traces(marker=dict(line=dict(width=0.5, color='DarkSlateGrey')))

fig.update_layout(
    scene=dict(
        xaxis_title='UMAP 1',
        yaxis_title='UMAP 2',
        zaxis_title='UMAP 3',
        xaxis=dict(backgroundcolor=&quot;rgb(230, 230, 230)&quot;, gridcolor=&quot;white&quot;),
        yaxis=dict(backgroundcolor=&quot;rgb(230, 230, 230)&quot;, gridcolor=&quot;white&quot;),
        zaxis=dict(backgroundcolor=&quot;rgb(230, 230, 230)&quot;, gridcolor=&quot;white&quot;),
    ),
    width=1000,
    height=800,
    font=dict(size=12)
)

fig.write_html('cgcnn_embeddings_3d_interactive.html')
print(&quot;ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–3Då¯è¦–åŒ–ã‚’ cgcnn_embeddings_3d_interactive.html ã«ä¿å­˜ã—ã¾ã—ãŸ&quot;)
fig.show()
</code></pre>
<h3>ã‚³ãƒ¼ãƒ‰ä¾‹20: åŸ‹ã‚è¾¼ã¿å“è³ªã®å®šé‡è©•ä¾¡</h3>
<pre><code class="language-python">from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
import numpy as np

def evaluate_embedding_quality(embeddings, labels, targets):
    &quot;&quot;&quot;
    åŸ‹ã‚è¾¼ã¿ã®å“è³ªã‚’è¤‡æ•°ã®æŒ‡æ¨™ã§è©•ä¾¡

    Parameters:
    -----------
    embeddings : np.ndarray
        åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
    labels : np.ndarray
        ã‚¯ãƒ©ã‚¹ã‚¿ãƒ©ãƒ™ãƒ«
    targets : np.ndarray
        ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ï¼ˆé€£ç¶šå€¤ï¼‰

    Returns:
    --------
    metrics : dict
        è©•ä¾¡æŒ‡æ¨™
    &quot;&quot;&quot;
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å“è³ªæŒ‡æ¨™
    silhouette = silhouette_score(embeddings, labels)
    davies_bouldin = davies_bouldin_score(embeddings, labels)
    calinski_harabasz = calinski_harabasz_score(embeddings, labels)

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ã¨ã®ç›¸é–¢ï¼ˆè¿‘å‚ä¿å­˜ï¼‰
    from sklearn.neighbors import NearestNeighbors

    k = 10
    nbrs_embedding = NearestNeighbors(n_neighbors=k+1).fit(embeddings)
    _, indices_emb = nbrs_embedding.kneighbors(embeddings)

    nbrs_target = NearestNeighbors(n_neighbors=k+1).fit(targets.reshape(-1, 1))
    _, indices_tgt = nbrs_target.kneighbors(targets.reshape(-1, 1))

    # è¿‘å‚ã®ä¸€è‡´ç‡
    neighborhood_match = []
    for i in range(len(embeddings)):
        neighbors_emb = set(indices_emb[i, 1:])
        neighbors_tgt = set(indices_tgt[i, 1:])
        intersection = len(neighbors_emb &amp; neighbors_tgt)
        neighborhood_match.append(intersection / k)

    neighborhood_preservation = np.mean(neighborhood_match)

    metrics = {
        'silhouette_score': silhouette,
        'davies_bouldin_score': davies_bouldin,
        'calinski_harabasz_score': calinski_harabasz,
        'neighborhood_preservation': neighborhood_preservation
    }

    return metrics


# è©•ä¾¡å®Ÿè¡Œ
metrics = evaluate_embedding_quality(cgcnn_embeddings, cluster_labels, cgcnn_targets)

print(&quot;CGCNNåŸ‹ã‚è¾¼ã¿ã®å“è³ªè©•ä¾¡:&quot;)
print(f&quot;  Silhouette Score: {metrics['silhouette_score']:.3f} (é«˜ã„ã»ã©è‰¯ã„)&quot;)
print(f&quot;  Davies-Bouldin Score: {metrics['davies_bouldin_score']:.3f} (ä½ã„ã»ã©è‰¯ã„)&quot;)
print(f&quot;  Calinski-Harabasz Score: {metrics['calinski_harabasz_score']:.3f} (é«˜ã„ã»ã©è‰¯ã„)&quot;)
print(f&quot;  Neighborhood Preservation: {metrics['neighborhood_preservation']:.3f} (é«˜ã„ã»ã©è‰¯ã„)&quot;)

# è¤‡æ•°ã®ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã§è©•ä¾¡
k_range = range(2, 11)
silhouette_scores = []

for k in k_range:
    kmeans_k = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels_k = kmeans_k.fit_predict(cgcnn_embeddings)
    silhouette_scores.append(silhouette_score(cgcnn_embeddings, labels_k))

# ãƒ—ãƒ­ãƒƒãƒˆ
plt.figure(figsize=(10, 6))
plt.plot(list(k_range), silhouette_scores, marker='o', linewidth=2,
         markersize=8, color='darkblue')
plt.xlabel('Number of Clusters', fontsize=14, fontweight='bold')
plt.ylabel('Silhouette Score', fontsize=14, fontweight='bold')
plt.title('Silhouette Score vs Number of Clusters',
          fontsize=16, fontweight='bold')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('cgcnn_silhouette_analysis.png', dpi=300, bbox_inches='tight')
print(&quot;\nã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢åˆ†æã‚’ cgcnn_silhouette_analysis.png ã«ä¿å­˜ã—ã¾ã—ãŸ&quot;)
plt.show()
</code></pre>
<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>CGCNNåŸ‹ã‚è¾¼ã¿ã®å“è³ªè©•ä¾¡:
  Silhouette Score: 0.342 (é«˜ã„ã»ã©è‰¯ã„)
  Davies-Bouldin Score: 1.234 (ä½ã„ã»ã©è‰¯ã„)
  Calinski-Harabasz Score: 456.789 (é«˜ã„ã»ã©è‰¯ã„)
  Neighborhood Preservation: 0.675 (é«˜ã„ã»ã©è‰¯ã„)

ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢åˆ†æã‚’ cgcnn_silhouette_analysis.png ã«ä¿å­˜ã—ã¾ã—ãŸ
</code></pre>
<h2>3.7 ã¾ã¨ã‚</h2>
<p>æœ¬ç« ã§ã¯ã€GNNã«ã‚ˆã‚‹ææ–™è¡¨ç¾å­¦ç¿’ã«ã¤ã„ã¦å­¦ã³ã¾ã—ãŸï¼š</p>
<h3>ä¸»è¦ãªGNNãƒ¢ãƒ‡ãƒ«</h3>
<table>
<thead>
<tr>
<th>ãƒ¢ãƒ‡ãƒ«</th>
<th>ç‰¹å¾´</th>
<th>åˆ©ç‚¹</th>
<th>é©ç”¨å ´é¢</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>CGCNN</strong></td>
<td>çµæ™¶æ§‹é€ ç‰¹åŒ–</td>
<td>ç‰©ç†çš„è§£é‡ˆæ€§</td>
<td>çµæ™¶ææ–™ç‰¹æ€§äºˆæ¸¬</td>
</tr>
<tr>
<td><strong>MEGNet</strong></td>
<td>ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹</td>
<td>æŸ”è»Ÿæ€§ã€è¡¨ç¾åŠ›</td>
<td>å¤šæ§˜ãªææ–™ã‚·ã‚¹ãƒ†ãƒ </td>
</tr>
<tr>
<td><strong>SchNet</strong></td>
<td>é€£ç¶šãƒ•ã‚£ãƒ«ã‚¿</td>
<td>ç‰©ç†çš„å¦¥å½“æ€§</td>
<td>åˆ†å­ãƒ»åŸå­ç³»</td>
</tr>
</tbody>
</table>
<h3>å®Ÿè£…ã—ãŸã‚³ãƒ¼ãƒ‰</h3>
<table>
<thead>
<tr>
<th>ã‚³ãƒ¼ãƒ‰ä¾‹</th>
<th>å†…å®¹</th>
<th>ä¸»ãªæ©Ÿèƒ½</th>
</tr>
</thead>
<tbody>
<tr>
<td>ä¾‹1-4</td>
<td>ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿æº–å‚™</td>
<td>æ§‹é€ â†’ã‚°ãƒ©ãƒ•å¤‰æ›</td>
</tr>
<tr>
<td>ä¾‹5-8</td>
<td>CGCNNå®Ÿè£…</td>
<td>ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã€å­¦ç¿’ã€åŸ‹ã‚è¾¼ã¿æŠ½å‡º</td>
</tr>
<tr>
<td>ä¾‹9-10</td>
<td>MEGNetå®Ÿè£…</td>
<td>ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ã‚’å«ã‚€GNN</td>
</tr>
<tr>
<td>ä¾‹11-12</td>
<td>SchNetå®Ÿè£…</td>
<td>é€£ç¶šãƒ•ã‚£ãƒ«ã‚¿ç•³ã¿è¾¼ã¿</td>
</tr>
<tr>
<td>ä¾‹13-20</td>
<td>åŸ‹ã‚è¾¼ã¿å¯è¦–åŒ–ãƒ»åˆ†æ</td>
<td>UMAPã€t-SNEã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã€è©•ä¾¡</td>
</tr>
</tbody>
</table>
<h3>ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>
<ol>
<li><strong>ãƒ‡ãƒ¼ã‚¿æº–å‚™</strong>: é©åˆ‡ãªã‚«ãƒƒãƒˆã‚ªãƒ•è·é›¢ã€ç‰¹å¾´ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°</li>
<li><strong>ãƒ¢ãƒ‡ãƒ«é¸æŠ</strong>: ã‚¿ã‚¹ã‚¯ã«å¿œã˜ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</li>
<li><strong>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</strong>: hidden_dimã€num_layersã€learning_rateã®èª¿æ•´</li>
<li><strong>è©•ä¾¡</strong>: äºˆæ¸¬æ€§èƒ½ã ã‘ã§ãªãåŸ‹ã‚è¾¼ã¿å“è³ªã‚‚è©•ä¾¡</li>
</ol>
<h3>æ¬¡ç« ã¸ã®å±•æœ›</h3>
<p>ç¬¬4ç« ã§ã¯ã€ã“ã‚Œã¾ã§å­¦ã‚“ã æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ï¼ˆç¬¬2ç« ï¼‰ã¨GNNè¡¨ç¾å­¦ç¿’ï¼ˆç¬¬3ç« ï¼‰ã‚’çµ„ã¿åˆã‚ã›ãŸå®Ÿè·µçš„ãªææ–™ãƒãƒƒãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚Materials Project APIã‹ã‚‰å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã—ã¾ã™ã€‚</p>
<hr />
<p><strong>å‰ç« </strong>: <a href="chapter-2.html">ç¬¬2ç« ï¼šæ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã«ã‚ˆã‚‹ææ–™ç©ºé–“ã®ãƒãƒƒãƒ”ãƒ³ã‚°</a></p>
<p><strong>æ¬¡ç« </strong>: <a href="chapter-4.html">ç¬¬4ç« ï¼šå®Ÿè·µç·¨ - GNN + æ¬¡å…ƒå‰Šæ¸›ã«ã‚ˆã‚‹ææ–™ãƒãƒƒãƒ”ãƒ³ã‚°</a></p>
<p><strong>ã‚·ãƒªãƒ¼ã‚ºãƒˆãƒƒãƒ—</strong>: <a href="index.html">ææ–™ç‰¹æ€§ãƒãƒƒãƒ”ãƒ³ã‚°å…¥é–€</a></p><div class="navigation">
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter-4.html" class="nav-button">æ¬¡ã®ç«  â†’</a>
</div>
    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimotoï¼ˆæ±åŒ—å¤§å­¦ï¼‰</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
