<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Á¨¨2Á´†: Âº∑ÂåñÂ≠¶Áøí„ÅÆÂü∫Á§éÁêÜË´ñ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Á¨¨2Á´†: Âº∑ÂåñÂ≠¶Áøí„ÅÆÂü∫Á§éÁêÜË´ñ</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 20-25ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ÂàùÁ¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 8ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 3Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Á¨¨2Á´†: Âº∑ÂåñÂ≠¶Áøí„ÅÆÂü∫Á§éÁêÜË´ñ</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">QÂ≠¶Áøí/DQN/PPO„Å™„Å©‰ª£Ë°®ÊâãÊ≥ï„ÅÆÁõ¥Ë¶≥„Å®ÈÅï„ÅÑ„ÇíÊï¥ÁêÜ„Åó„Åæ„Åô„ÄÇ„Å©„ÅÆË™≤È°å„Å´„Å©„Çå„ÇíË©¶„Åô„Åã„ÅÆÂΩì„Åü„Çä„Çí‰ªò„Åë„Åæ„Åô„ÄÇ</p>




<h2>Â≠¶ÁøíÁõÆÊ®ô</h2>
<p>„Åì„ÅÆÁ´†„Åß„ÅØ„ÄÅ‰ª•‰∏ã„ÇíÁøíÂæó„Åó„Åæ„ÅôÔºö</p>
<ul>
<li>ÊñπÁ≠ñÂãæÈÖçÊ≥ïÔºàPolicy Gradient MethodsÔºâ„ÅÆÁêÜË´ñ„Å®ÂÆüË£Ö</li>
<li>Actor-Critic„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÅÆ‰ªïÁµÑ„Åø</li>
<li>Proximal Policy OptimizationÔºàPPOÔºâ„ÅÆË©≥Á¥∞</li>
<li>Stable Baselines3„Å´„Çà„ÇãÂÆüË∑µÁöÑÂÆüË£Ö</li>
</ul>
<hr />
<h2>2.1 ÊñπÁ≠ñÂãæÈÖçÊ≥ïÔºàPolicy Gradient MethodsÔºâ</h2>
<h3>QÂ≠¶Áøí„ÅÆÈôêÁïå</h3>
<p>Á¨¨1Á´†„ÅÆQÂ≠¶Áøí„ÉªDQN„ÅØ<strong>‰æ°ÂÄ§„Éô„Éº„Çπ</strong>„ÅÆÊâãÊ≥ï„Åß„Åó„Åü„ÄÇ„Åì„Çå„Çâ„Å´„ÅØ‰ª•‰∏ã„ÅÆÈôêÁïå„Åå„ÅÇ„Çä„Åæ„ÅôÔºö</p>
<ol>
<li><strong>Èõ¢Êï£Ë°åÂãï„ÅÆ„Åø</strong>: $\arg\max_a Q(s,a)$„ÅØÈÄ£Á∂öË°åÂãïÁ©∫Èñì„ÅßÂõ∞Èõ£</li>
<li><strong>Ê±∫ÂÆöÁöÑÊñπÁ≠ñ</strong>: Â∏∏„Å´Âêå„ÅòË°åÂãï„ÇíÈÅ∏ÊäûÔºàÁ¢∫ÁéáÁöÑÊñπÁ≠ñ„ÅåÂ≠¶Áøí„Åß„Åç„Å™„ÅÑÔºâ</li>
<li><strong>Â∞è„Åï„Å™Â§âÂåñ„Å´ËÑÜÂº±</strong>: QÂÄ§„ÅÆÂæÆÂ∞è„Å™Â§âÂåñ„ÅßÊñπÁ≠ñ„ÅåÂ§ß„Åç„ÅèÂ§â„Çè„Çã</li>
</ol>
<p>ÊùêÊñôÁßëÂ≠¶„Åß„ÅØ„ÄÅ<strong>ÈÄ£Á∂öÁöÑ„Å™Âà∂Âæ°</strong>ÔºàÊ∏©Â∫¶„Çí0.5Â∫¶‰∏ä„Åí„Çã„ÄÅÁµÑÊàêÊØî„Çí2%Â§â„Åà„ÇãÔºâ„ÅåÈáçË¶Å„Åß„Åô„ÄÇ</p>
<h3>ÊñπÁ≠ñÂãæÈÖçÊ≥ï„ÅÆÂü∫Êú¨„Ç¢„Ç§„Éá„Ç¢</h3>
<p>ÊñπÁ≠ñÂãæÈÖçÊ≥ï„ÅØ„ÄÅ<strong>ÊñπÁ≠ñ„ÇíÁõ¥Êé•ÊúÄÈÅ©Âåñ</strong>„Åó„Åæ„ÅôÔºö</p>
<p>$$
\pi_\theta(a|s) = P(a|s; \theta)
$$</p>
<ul>
<li>$\theta$: ÊñπÁ≠ñ„ÅÆ„Éë„É©„É°„Éº„ÇøÔºà„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÈáç„ÅøÔºâ</li>
</ul>
<p><strong>ÁõÆÁöÑ</strong>: ÊúüÂæÖÁ¥ØÁ©çÂ†±ÈÖ¨$J(\theta)$„ÇíÊúÄÂ§ßÂåñ</p>
<p>$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T r_t \right]
$$</p>
<ul>
<li>$\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots)$: ËªåË∑°ÔºàtrajectoryÔºâ</li>
</ul>
<h3>ÊñπÁ≠ñÂãæÈÖçÂÆöÁêÜ</h3>
<p><strong>REINFORCE</strong>„Ç¢„É´„Ç¥„É™„Ç∫„É†ÔºàWilliams, 1992Ôºâ„ÅØ„ÄÅÂãæÈÖç„Çí‰ª•‰∏ã„ÅßË®àÁÆó„Åó„Åæ„ÅôÔºö</p>
<p>$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot R_t \right]
$$</p>
<ul>
<li>$R_t = \sum_{k=t}^T \gamma^{k-t} r_k$: ÊôÇÂàª$t$„Åã„Çâ„ÅÆÁ¥ØÁ©çÂ†±ÈÖ¨Ôºà„É™„Çø„Éº„É≥Ôºâ</li>
</ul>
<p><strong>Áõ¥ÊÑüÁöÑÊÑèÂë≥</strong>:
- È´ò„ÅÑÂ†±ÈÖ¨„ÇíÂæó„ÅüË°åÂãï„ÅÆÁ¢∫Áéá„Çí‰∏ä„Åí„Çã
- ‰Ωé„ÅÑÂ†±ÈÖ¨„ÇíÂæó„ÅüË°åÂãï„ÅÆÁ¢∫Áéá„Çí‰∏ã„Åí„Çã</p>
<h3>REINFORCE„ÅÆÂÆüË£Ö</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

class PolicyNetwork(nn.Module):
    &quot;&quot;&quot;ÊñπÁ≠ñ„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ

    Áä∂ÊÖã„ÇíÂÖ•Âäõ„Åó„ÄÅÂêÑË°åÂãï„ÅÆÁ¢∫Áéá„ÇíÂá∫Âäõ
    &quot;&quot;&quot;
    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return torch.softmax(self.fc3(x), dim=-1)  # Á¢∫ÁéáÂàÜÂ∏É


class REINFORCEAgent:
    &quot;&quot;&quot;REINFORCE„Ç¢„É´„Ç¥„É™„Ç∫„É†&quot;&quot;&quot;
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):
        self.gamma = gamma
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)

        # „Ç®„Éî„ÇΩ„Éº„ÉâÂÜÖ„ÅÆ„É≠„Ç∞„Çí‰øùÂ≠ò
        self.log_probs = []
        self.rewards = []

    def select_action(self, state):
        &quot;&quot;&quot;ÊñπÁ≠ñ„Å´Âæì„Å£„Å¶Ë°åÂãï„Çí„Çµ„É≥„Éó„É™„É≥„Ç∞&quot;&quot;&quot;
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        probs = self.policy(state_tensor)

        # Á¢∫ÁéáÂàÜÂ∏É„Åã„Çâ„Çµ„É≥„Éó„É™„É≥„Ç∞
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()

        # logÁ¢∫Áéá„Çí‰øùÂ≠òÔºàÂãæÈÖçË®àÁÆóÁî®Ôºâ
        self.log_probs.append(action_dist.log_prob(action))

        return action.item()

    def store_reward(self, reward):
        &quot;&quot;&quot;Â†±ÈÖ¨„Çí‰øùÂ≠ò&quot;&quot;&quot;
        self.rewards.append(reward)

    def update(self):
        &quot;&quot;&quot;„Ç®„Éî„ÇΩ„Éº„ÉâÁµÇ‰∫ÜÂæå„Å´ÊñπÁ≠ñ„ÇíÊõ¥Êñ∞&quot;&quot;&quot;
        # „É™„Çø„Éº„É≥ÔºàÁ¥ØÁ©çÂ†±ÈÖ¨Ôºâ„ÇíË®àÁÆó
        returns = []
        R = 0
        for r in reversed(self.rewards):
            R = r + self.gamma * R
            returns.insert(0, R)

        returns = torch.FloatTensor(returns)

        # Ê≠£Ë¶èÂåñÔºàÂ≠¶Áøí„ÇíÂÆâÂÆöÂåñÔºâ
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)

        # ÊñπÁ≠ñÂãæÈÖç
        policy_loss = []
        for log_prob, R in zip(self.log_probs, returns):
            policy_loss.append(-log_prob * R)

        # ÂãæÈÖçÈôç‰∏ã
        self.optimizer.zero_grad()
        loss = torch.stack(policy_loss).sum()
        loss.backward()
        self.optimizer.step()

        # „É≠„Ç∞„Çí„É™„Çª„ÉÉ„Éà
        self.log_probs = []
        self.rewards = []


# Á∞°Âçò„Å™ÊùêÊñôÊé¢Á¥¢Áí∞Â¢ÉÔºàÈõ¢Êï£Ë°åÂãïÁâàÔºâ
class DiscreteMaterialsEnv:
    &quot;&quot;&quot;Èõ¢Êï£Ë°åÂãï„ÅÆÊùêÊñôÊé¢Á¥¢Áí∞Â¢É&quot;&quot;&quot;
    def __init__(self, state_dim=4):
        self.state_dim = state_dim
        self.target = np.array([3.0, 5.0, 2.5, 4.0])
        self.state = None

    def reset(self):
        self.state = np.random.uniform(0, 10, self.state_dim)
        return self.state

    def step(self, action):
        # Ë°åÂãï: 0=Ê¨°ÂÖÉ0Â¢óÂä†, 1=Ê¨°ÂÖÉ0Ê∏õÂ∞ë, 2=Ê¨°ÂÖÉ1Â¢óÂä†, 3=Ê¨°ÂÖÉ1Ê∏õÂ∞ë
        dim = action // 2
        delta = 0.5 if action % 2 == 0 else -0.5

        self.state[dim] = np.clip(self.state[dim] + delta, 0, 10)

        # Â†±ÈÖ¨: ÁõÆÊ®ô„Å®„ÅÆË∑ùÈõ¢
        distance = np.linalg.norm(self.state - self.target)
        reward = -distance

        done = distance &lt; 0.5

        return self.state, reward, done


# REINFORCE„ÅÆË®ìÁ∑¥
env = DiscreteMaterialsEnv()
agent = REINFORCEAgent(state_dim=4, action_dim=4)

episodes = 1000
rewards_history = []

for episode in range(episodes):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action = agent.select_action(state)
        next_state, reward, done = env.step(action)

        agent.store_reward(reward)
        state = next_state
        total_reward += reward

    # „Ç®„Éî„ÇΩ„Éº„ÉâÁµÇ‰∫ÜÂæå„Å´Êõ¥Êñ∞
    agent.update()
    rewards_history.append(total_reward)

    if (episode + 1) % 100 == 0:
        avg_reward = np.mean(rewards_history[-100:])
        print(f&quot;Episode {episode+1}: Avg Reward = {avg_reward:.2f}&quot;)

# Â≠¶ÁøíÊõ≤Á∑ö
plt.figure(figsize=(10, 6))
plt.plot(np.convolve(rewards_history, np.ones(20)/20, mode='valid'))
plt.xlabel('Episode')
plt.ylabel('Average Reward (20 episodes)')
plt.title('REINFORCE: ÊñπÁ≠ñÂãæÈÖçÊ≥ï„Å´„Çà„ÇãÊùêÊñôÊé¢Á¥¢')
plt.grid(True)
plt.show()
</code></pre>
<p><strong>Âá∫Âäõ‰æã</strong>:</p>
<pre><code>Episode 100: Avg Reward = -38.24
Episode 200: Avg Reward = -28.15
Episode 500: Avg Reward = -15.32
Episode 1000: Avg Reward = -7.89
</code></pre>
<hr />
<h2>2.2 „Éô„Éº„Çπ„É©„Ç§„É≥„Å®ÂàÜÊï£ÂâäÊ∏õ</h2>
<h3>REINFORCE„ÅÆÂïèÈ°åÁÇπ</h3>
<p>REINFORCE„ÅØ<strong>È´òÂàÜÊï£</strong>Ôºàhigh varianceÔºâ„Åß„Åô„ÄÇÂêå„ÅòÊñπÁ≠ñ„Åß„ÇÇ„ÄÅÈÅã„ÅåËâØ„ÅÑ„ÅãÊÇ™„ÅÑ„Åã„Åß„É™„Çø„Éº„É≥$R_t$„ÅåÂ§ß„Åç„ÅèÂ§âÂãï„Åó„Åæ„Åô„ÄÇ</p>
<h3>„Éô„Éº„Çπ„É©„Ç§„É≥„ÅÆÂ∞éÂÖ•</h3>
<p><strong>„Éô„Éº„Çπ„É©„Ç§„É≥</strong> $b(s)$„ÇíÂºï„Åè„Åì„Å®„Åß„ÄÅÂàÜÊï£„ÇíÂâäÊ∏õÔºö</p>
<p>$$
\nabla_\theta J(\theta) = \mathbb{E} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot (R_t - b(s_t)) \right]
$$</p>
<p><strong>ÊúÄÈÅ©„Å™„Éô„Éº„Çπ„É©„Ç§„É≥</strong>: Áä∂ÊÖã‰æ°ÂÄ§Èñ¢Êï∞$V(s)$</p>
<p>$$
b(s_t) = V(s_t) = \mathbb{E}_{\pi} \left[ \sum_{k=t}^T \gamma^{k-t} r_k \mid s_t \right]
$$</p>
<p><strong>„Ç¢„Éâ„Éê„É≥„ÉÜ„Éº„Ç∏Èñ¢Êï∞</strong> $A(s, a)$:
$$
A(s, a) = Q(s, a) - V(s) = R_t - V(s_t)
$$</p>
<p>„Äå„Åì„ÅÆË°åÂãï„ÅØÂπ≥Âùá„Çà„Çä„Å©„Çå„Å†„ÅëËâØ„ÅÑ„Åã„Äç„ÇíË°®„Åó„Åæ„Åô„ÄÇ</p>
<h3>„Éô„Éº„Çπ„É©„Ç§„É≥‰ªò„ÅçREINFORCE</h3>
<pre><code class="language-python">class ValueNetwork(nn.Module):
    &quot;&quot;&quot;‰æ°ÂÄ§„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÔºà„Éô„Éº„Çπ„É©„Ç§„É≥Ôºâ&quot;&quot;&quot;
    def __init__(self, state_dim, hidden_dim=64):
        super(ValueNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)  # Áä∂ÊÖã‰æ°ÂÄ§„ÇíÂá∫Âäõ

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)


class REINFORCEWithBaseline:
    &quot;&quot;&quot;„Éô„Éº„Çπ„É©„Ç§„É≥‰ªò„ÅçREINFORCE&quot;&quot;&quot;
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):
        self.gamma = gamma
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.value = ValueNetwork(state_dim)

        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        self.value_optimizer = optim.Adam(self.value.parameters(), lr=lr)

        self.log_probs = []
        self.rewards = []
        self.states = []

    def select_action(self, state):
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        probs = self.policy(state_tensor)

        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()

        self.log_probs.append(action_dist.log_prob(action))
        self.states.append(state)

        return action.item()

    def store_reward(self, reward):
        self.rewards.append(reward)

    def update(self):
        # „É™„Çø„Éº„É≥Ë®àÁÆó
        returns = []
        R = 0
        for r in reversed(self.rewards):
            R = r + self.gamma * R
            returns.insert(0, R)

        returns = torch.FloatTensor(returns)
        states = torch.FloatTensor(self.states)

        # ‰æ°ÂÄ§„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÂá∫ÂäõÔºà„Éô„Éº„Çπ„É©„Ç§„É≥Ôºâ
        values = self.value(states).squeeze()

        # „Ç¢„Éâ„Éê„É≥„ÉÜ„Éº„Ç∏ = „É™„Çø„Éº„É≥ - „Éô„Éº„Çπ„É©„Ç§„É≥
        advantages = returns - values.detach()

        # ÊñπÁ≠ñÂãæÈÖçÊêçÂ§±
        policy_loss = []
        for log_prob, adv in zip(self.log_probs, advantages):
            policy_loss.append(-log_prob * adv)

        # ‰æ°ÂÄ§„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÊêçÂ§±ÔºàMSEÔºâ
        value_loss = nn.MSELoss()(values, returns)

        # ÊúÄÈÅ©Âåñ
        self.policy_optimizer.zero_grad()
        torch.stack(policy_loss).sum().backward()
        self.policy_optimizer.step()

        self.value_optimizer.zero_grad()
        value_loss.backward()
        self.value_optimizer.step()

        # „É™„Çª„ÉÉ„Éà
        self.log_probs = []
        self.rewards = []
        self.states = []


# Ë®ìÁ∑¥Ôºà„Éô„Éº„Çπ„É©„Ç§„É≥‰ªò„ÅçÔºâ
agent_baseline = REINFORCEWithBaseline(state_dim=4, action_dim=4)

rewards_baseline = []
for episode in range(1000):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action = agent_baseline.select_action(state)
        next_state, reward, done = env.step(action)

        agent_baseline.store_reward(reward)
        state = next_state
        total_reward += reward

    agent_baseline.update()
    rewards_baseline.append(total_reward)

# ÊØîËºÉ
plt.figure(figsize=(10, 6))
plt.plot(np.convolve(rewards_history, np.ones(20)/20, mode='valid'), label='REINFORCE')
plt.plot(np.convolve(rewards_baseline, np.ones(20)/20, mode='valid'), label='REINFORCE + Baseline')
plt.xlabel('Episode')
plt.ylabel('Average Reward')
plt.title('„Éô„Éº„Çπ„É©„Ç§„É≥„Å´„Çà„ÇãÂ≠¶ÁøíÂÆâÂÆöÂåñ')
plt.legend()
plt.grid(True)
plt.show()
</code></pre>
<p><strong>ÁµêÊûú</strong>: „Éô„Éº„Çπ„É©„Ç§„É≥„Å´„Çà„ÇäÂ≠¶Áøí„Åå<strong>„Çà„ÇäÂÆâÂÆö</strong>„Åó„ÄÅÂèéÊùü„Åå<strong>ÈÄü„Åè</strong>„Å™„Çä„Åæ„Åô„ÄÇ</p>
<hr />
<h2>2.3 Actor-Critic„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£</h2>
<h3>Actor-Critic„ÅÆÊ¶ÇÂøµ</h3>
<p><strong>ActorÔºàÊñπÁ≠ñÔºâ</strong> „Å® <strong>CriticÔºà‰æ°ÂÄ§Èñ¢Êï∞Ôºâ</strong> „ÇíÂêåÊôÇ„Å´Â≠¶ÁøíÔºö</p>
<ul>
<li><strong>Actor</strong> $\pi_\theta(a|s)$: Ë°åÂãï„ÇíÈÅ∏Êäû</li>
<li><strong>Critic</strong> $V_\phi(s)$: Áä∂ÊÖã„ÅÆ‰æ°ÂÄ§„ÇíË©ï‰æ°</li>
</ul>
<div class="mermaid">
graph LR
    S[Áä∂ÊÖã s] --> A[Actor: œÄŒ∏]
    S --> C[Critic: Vœï]
    A -->|Ë°åÂãï a| E[Áí∞Â¢É]
    E -->|Â†±ÈÖ¨ r| C
    C -->|TDË™§Â∑Æ| A
    C -->|‰æ°ÂÄ§Ë©ï‰æ°| C

    style A fill:#e1f5ff
    style C fill:#ffe1cc
</div>

<h3>TDË™§Â∑Æ„Å®„Ç¢„Éâ„Éê„É≥„ÉÜ„Éº„Ç∏</h3>
<p><strong>TDË™§Â∑Æ</strong>ÔºàTemporal Difference ErrorÔºâ:
$$
\delta_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)
$$</p>
<p>„Åì„Çå„ÅØ<strong>1„Çπ„ÉÜ„ÉÉ„Éó„ÅÆ„Ç¢„Éâ„Éê„É≥„ÉÜ„Éº„Ç∏Êé®ÂÆö</strong>„Å®„Åó„Å¶‰Ωø„Åà„Åæ„Åô„ÄÇ</p>
<h3>A2CÔºàAdvantage Actor-CriticÔºâ</h3>
<pre><code class="language-python">class A2CAgent:
    &quot;&quot;&quot;Advantage Actor-Critic&quot;&quot;&quot;
    def __init__(self, state_dim, action_dim, lr_actor=1e-3, lr_critic=1e-3, gamma=0.99):
        self.gamma = gamma

        self.actor = PolicyNetwork(state_dim, action_dim)
        self.critic = ValueNetwork(state_dim)

        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)

    def select_action(self, state):
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        probs = self.actor(state_tensor)

        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()

        return action.item(), action_dist.log_prob(action)

    def update(self, state, action_log_prob, reward, next_state, done):
        &quot;&quot;&quot;1„Çπ„ÉÜ„ÉÉ„Éó„Åî„Å®„Å´Êõ¥Êñ∞&quot;&quot;&quot;
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)

        # ÁèæÂú®„Å®Ê¨°„ÅÆÁä∂ÊÖã‰æ°ÂÄ§
        value = self.critic(state_tensor)
        next_value = self.critic(next_state_tensor)

        # TDÁõÆÊ®ô„Å®TDË™§Â∑Æ
        td_target = reward + (1 - done) * self.gamma * next_value.item()
        td_error = td_target - value.item()

        # CriticÊêçÂ§±ÔºàMSEÔºâ
        critic_loss = (torch.FloatTensor([td_target]) - value).pow(2)

        # ActorÊêçÂ§±ÔºàÊñπÁ≠ñÂãæÈÖç √ó „Ç¢„Éâ„Éê„É≥„ÉÜ„Éº„Ç∏Ôºâ
        actor_loss = -action_log_prob * td_error

        # ÊúÄÈÅ©Âåñ
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()


# A2CË®ìÁ∑¥
agent_a2c = A2CAgent(state_dim=4, action_dim=4)

rewards_a2c = []
for episode in range(1000):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action, log_prob = agent_a2c.select_action(state)
        next_state, reward, done = env.step(action)

        # 1„Çπ„ÉÜ„ÉÉ„Éó„Åî„Å®„Å´Êõ¥Êñ∞
        agent_a2c.update(state, log_prob, reward, next_state, done)

        state = next_state
        total_reward += reward

    rewards_a2c.append(total_reward)

    if (episode + 1) % 100 == 0:
        avg_reward = np.mean(rewards_a2c[-100:])
        print(f&quot;Episode {episode+1}: Avg Reward = {avg_reward:.2f}&quot;)
</code></pre>
<p><strong>Âà©ÁÇπ</strong>:
- „Ç®„Éî„ÇΩ„Éº„ÉâÁµÇ‰∫Ü„ÇíÂæÖ„Åü„Åö„Å´<strong>„Ç™„É≥„É©„Ç§„É≥Â≠¶Áøí</strong>
- TDË™§Â∑Æ„Å´„Çà„Çä<strong>‰ΩéÂàÜÊï£</strong></p>
<hr />
<h2>2.4 Proximal Policy OptimizationÔºàPPOÔºâ</h2>
<h3>Trust Region Methods</h3>
<p>ÊñπÁ≠ñÂãæÈÖçÊ≥ï„Åß„ÅØ„ÄÅ<strong>Êõ¥Êñ∞„ÅåÂ§ß„Åç„Åô„Åé„Çã„Å®ÊñπÁ≠ñ„ÅåÂ¥©Â£ä</strong>„Åô„Çã„Åì„Å®„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ</p>
<p><strong>Trust Region Policy OptimizationÔºàTRPOÔºâ</strong> „ÅØ„ÄÅÊñπÁ≠ñ„ÅÆÂ§âÂåñ„ÇíÂà∂Á¥ÑÔºö</p>
<p>$$
\max_\theta \mathbb{E} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} A(s, a) \right] \quad \text{s.t.} \quad D_{\text{KL}}(\pi_{\theta_{\text{old}}} | \pi_\theta) \leq \delta
$$</p>
<p>„Åó„Åã„Åó„ÄÅKL„ÉÄ„Ç§„Éê„Éº„Ç∏„Çß„É≥„ÇπÂà∂Á¥Ñ„ÅÆÊúÄÈÅ©Âåñ„ÅØË§áÈõë„Åß„Åô„ÄÇ</p>
<h3>PPO„ÅÆÁ∞°Áï•Âåñ</h3>
<p><strong>PPO</strong>ÔºàSchulman et al., 2017Ôºâ„ÅØ„ÄÅÂà∂Á¥Ñ„Çí<strong>ÊêçÂ§±Èñ¢Êï∞ÂÜÖ„ÅÆ„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞</strong>„ÅßÂÆüÁèæÔºö</p>
<p>$$
L^{\text{CLIP}}(\theta) = \mathbb{E} \left[ \min \left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right]
$$</p>
<ul>
<li>$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$: ÈáçË¶ÅÂ∫¶ÊØîÁéáÔºàimportance ratioÔºâ</li>
<li>$\epsilon$: „ÇØ„É™„ÉÉ„Éî„É≥„Ç∞ÁØÑÂõ≤ÔºàÈÄöÂ∏∏0.1„Äú0.2Ôºâ</li>
</ul>
<p><strong>Áõ¥ÊÑü</strong>:
- „Ç¢„Éâ„Éê„É≥„ÉÜ„Éº„Ç∏„ÅåÊ≠£ÔºàËâØ„ÅÑË°åÂãïÔºâ‚Üí $r_t$„ÇíÂ¢ó„ÇÑ„Åô„Åå„ÄÅ$1+\epsilon$„Åß‰∏äÈôê
- „Ç¢„Éâ„Éê„É≥„ÉÜ„Éº„Ç∏„ÅåË≤†ÔºàÊÇ™„ÅÑË°åÂãïÔºâ‚Üí $r_t$„ÇíÊ∏õ„Çâ„Åô„Åå„ÄÅ$1-\epsilon$„Åß‰∏ãÈôê
- ÊÄ•ÊøÄ„Å™ÊñπÁ≠ñÂ§âÂåñ„ÇíÈò≤„Åê</p>
<h3>„Ç®„É≥„Éà„É≠„Éî„Éº„Éú„Éº„Éä„Çπ</h3>
<p>Êé¢Á¥¢„Çí‰øÉÈÄ≤„Åô„Çã„Åü„ÇÅ„ÄÅ<strong>„Ç®„É≥„Éà„É≠„Éî„Éº</strong>„ÇíÊêçÂ§±„Å´ËøΩÂä†Ôºö</p>
<p>$$
L^{\text{PPO}}(\theta) = L^{\text{CLIP}}(\theta) + c_1 L^{\text{VF}}(\theta) - c_2 H[\pi_\theta]
$$</p>
<ul>
<li>$L^{\text{VF}}$: ‰æ°ÂÄ§Èñ¢Êï∞„ÅÆÊêçÂ§±</li>
<li>$H[\pi_\theta] = -\sum_a \pi_\theta(a|s) \log \pi_\theta(a|s)$: „Ç®„É≥„Éà„É≠„Éî„ÉºÔºàÁ¢∫ÁéáÂàÜÂ∏É„ÅÆ‰∏çÁ¢∫ÂÆüÊÄßÔºâ</li>
<li>$c_2$: „Ç®„É≥„Éà„É≠„Éî„Éº‰øÇÊï∞ÔºàÈÄöÂ∏∏0.01Ôºâ</li>
</ul>
<h3>PPO„ÅÆÂÆüË£ÖÔºàStable Baselines3‰ΩøÁî®Ôºâ</h3>
<pre><code class="language-python">from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
import gym

# GymÁí∞Â¢É„É©„ÉÉ„Éë„Éº
class GymMaterialsEnv(gym.Env):
    &quot;&quot;&quot;OpenAI Gym‰∫íÊèõ„ÅÆÊùêÊñôÊé¢Á¥¢Áí∞Â¢É&quot;&quot;&quot;
    def __init__(self):
        super(GymMaterialsEnv, self).__init__()
        self.state_dim = 4
        self.target = np.array([3.0, 5.0, 2.5, 4.0])

        # Ë°åÂãï„ÉªÁä∂ÊÖãÁ©∫Èñì„ÅÆÂÆöÁæ©
        self.action_space = gym.spaces.Discrete(4)
        self.observation_space = gym.spaces.Box(
            low=0, high=10, shape=(self.state_dim,), dtype=np.float32
        )

        self.state = None

    def reset(self):
        self.state = np.random.uniform(0, 10, self.state_dim).astype(np.float32)
        return self.state

    def step(self, action):
        dim = action // 2
        delta = 0.5 if action % 2 == 0 else -0.5

        self.state[dim] = np.clip(self.state[dim] + delta, 0, 10)

        distance = np.linalg.norm(self.state - self.target)
        reward = -distance
        done = distance &lt; 0.5

        return self.state, reward, done, {}

    def render(self, mode='human'):
        pass


# Áí∞Â¢É‰ΩúÊàê
env = DummyVecEnv([lambda: GymMaterialsEnv()])

# PPO„É¢„Éá„É´
model = PPO(
    &quot;MlpPolicy&quot;,                # Â§öÂ±§„Éë„Éº„Çª„Éó„Éà„É≠„É≥ÊñπÁ≠ñ
    env,
    learning_rate=3e-4,
    n_steps=2048,               # Êõ¥Êñ∞Ââç„ÅÆ„Çπ„ÉÜ„ÉÉ„ÉóÊï∞
    batch_size=64,
    n_epochs=10,                # ÂêÑÊõ¥Êñ∞„Åß„ÅÆÊúÄÈÅ©Âåñ„Ç®„Éù„ÉÉ„ÇØÊï∞
    gamma=0.99,
    gae_lambda=0.95,            # GAEÔºàGeneralized Advantage EstimationÔºâ
    clip_range=0.2,             # PPO„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞ÁØÑÂõ≤
    ent_coef=0.01,              # „Ç®„É≥„Éà„É≠„Éî„Éº‰øÇÊï∞
    verbose=1,
    tensorboard_log=&quot;./ppo_materials_tensorboard/&quot;
)

# Ë®ìÁ∑¥
model.learn(total_timesteps=100000)

# ‰øùÂ≠ò
model.save(&quot;ppo_materials_agent&quot;)

# Ë©ï‰æ°
eval_env = GymMaterialsEnv()
state = eval_env.reset()
total_reward = 0

for _ in range(100):
    action, _ = model.predict(state, deterministic=True)
    state, reward, done, _ = eval_env.step(action)
    total_reward += reward

    if done:
        break

print(f&quot;Ë©ï‰æ°ÁµêÊûú: Total Reward = {total_reward:.2f}&quot;)
print(f&quot;ÊúÄÁµÇÁä∂ÊÖã: {state}&quot;)
print(f&quot;ÁõÆÊ®ô: {eval_env.target}&quot;)
</code></pre>
<p><strong>Âá∫Âäõ‰æã</strong>:</p>
<pre><code>---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.2     |
|    ep_rew_mean     | -15.3    |
| time/              |          |
|    fps             | 1024     |
|    iterations      | 50       |
|    time_elapsed    | 97       |
|    total_timesteps | 102400   |
---------------------------------

Ë©ï‰æ°ÁµêÊûú: Total Reward = -5.23
ÊúÄÁµÇÁä∂ÊÖã: [3.02 4.98 2.47 3.95]
ÁõÆÊ®ô: [3.  5.  2.5 4. ]
</code></pre>
<p><strong>Ëß£Ë™¨</strong>:
- Stable Baselines3„Å´„Çà„Çä„ÄÅ„Çè„Åö„ÅãÊï∞Ë°å„ÅßPPO„ÇíÂÆüË£Ö
- TensorBoard„ÅßÂ≠¶ÁøíÈÄ≤Êçó„ÇíÂèØË¶ñÂåñÂèØËÉΩ
- ÁõÆÊ®ô„Å´ÈùûÂ∏∏„Å´Ëøë„ÅÑÊùêÊñô„ÇíÁô∫Ë¶ã</p>
<hr />
<h2>2.5 ÈÄ£Á∂öË°åÂãïÁ©∫Èñì„Å∏„ÅÆÊã°Âºµ</h2>
<h3>„Ç¨„Ç¶„ÇπÊñπÁ≠ñ</h3>
<p>ÊùêÊñôÁßëÂ≠¶„Åß„ÅØ„ÄÅÊ∏©Â∫¶„ÇÑÁµÑÊàêÊØî„Å™„Å©<strong>ÈÄ£Á∂öÁöÑ„Å™Âà∂Âæ°</strong>„ÅåÂøÖË¶Å„Åß„Åô„ÄÇ</p>
<p>ÈÄ£Á∂öË°åÂãï„Å´„ÅØ<strong>„Ç¨„Ç¶„ÇπÂàÜÂ∏ÉÊñπÁ≠ñ</strong>„Çí‰ΩøÁî®Ôºö</p>
<p>$$
\pi_\theta(a|s) = \mathcal{N}(\mu_\theta(s), \sigma_\theta(s))
$$</p>
<ul>
<li>$\mu_\theta(s)$: Âπ≥ÂùáÔºà„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅßÂá∫ÂäõÔºâ</li>
<li>$\sigma_\theta(s)$: Ê®ôÊ∫ñÂÅèÂ∑ÆÔºàÂ≠¶ÁøíÂèØËÉΩ„Åæ„Åü„ÅØÂõ∫ÂÆöÔºâ</li>
</ul>
<h3>ÈÄ£Á∂öË°åÂãïÁâàPPO</h3>
<pre><code class="language-python"># ÈÄ£Á∂öË°åÂãïÁí∞Â¢É
class ContinuousGymMaterialsEnv(gym.Env):
    &quot;&quot;&quot;ÈÄ£Á∂öË°åÂãï„ÅÆÊùêÊñôÊé¢Á¥¢Áí∞Â¢É&quot;&quot;&quot;
    def __init__(self):
        super(ContinuousGymMaterialsEnv, self).__init__()
        self.state_dim = 4
        self.target = np.array([3.0, 5.0, 2.5, 4.0])

        # ÈÄ£Á∂öË°åÂãïÁ©∫ÈñìÔºà4Ê¨°ÂÖÉ„Éô„ÇØ„Éà„É´„ÄÅÁØÑÂõ≤ [-1, 1]Ôºâ
        self.action_space = gym.spaces.Box(
            low=-1, high=1, shape=(self.state_dim,), dtype=np.float32
        )
        self.observation_space = gym.spaces.Box(
            low=0, high=10, shape=(self.state_dim,), dtype=np.float32
        )

        self.state = None

    def reset(self):
        self.state = np.random.uniform(0, 10, self.state_dim).astype(np.float32)
        return self.state

    def step(self, action):
        # Ë°åÂãï„ÇíÁä∂ÊÖãÂ§âÂåñ„Å´„Éû„ÉÉ„Éî„É≥„Ç∞Ôºà-1„Äú1 ‚Üí -0.5„Äú0.5Ôºâ
        delta = action * 0.5
        self.state = np.clip(self.state + delta, 0, 10)

        distance = np.linalg.norm(self.state - self.target)
        reward = -distance
        done = distance &lt; 0.3

        return self.state, reward, done, {}

    def render(self, mode='human'):
        pass


# ÈÄ£Á∂öË°åÂãïÁâàPPO
env_continuous = DummyVecEnv([lambda: ContinuousGymMaterialsEnv()])

model_continuous = PPO(
    &quot;MlpPolicy&quot;,
    env_continuous,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    clip_range=0.2,
    verbose=1
)

model_continuous.learn(total_timesteps=100000)

# Ë©ï‰æ°
eval_env_cont = ContinuousGymMaterialsEnv()
state = eval_env_cont.reset()

for _ in range(50):
    action, _ = model_continuous.predict(state, deterministic=True)
    state, reward, done, _ = eval_env_cont.step(action)

    if done:
        break

print(f&quot;ÊúÄÁµÇÁä∂ÊÖã: {state}&quot;)
print(f&quot;ÁõÆÊ®ô: {eval_env_cont.target}&quot;)
print(f&quot;Ë∑ùÈõ¢: {np.linalg.norm(state - eval_env_cont.target):.4f}&quot;)
</code></pre>
<p><strong>Âá∫Âäõ‰æã</strong>:</p>
<pre><code>ÊúÄÁµÇÁä∂ÊÖã: [3.001 5.003 2.498 3.997]
ÁõÆÊ®ô: [3.  5.  2.5 4. ]
Ë∑ùÈõ¢: 0.0054
</code></pre>
<p><strong>Ëß£Ë™¨</strong>: ÈÄ£Á∂öË°åÂãï„Å´„Çà„Çä„ÄÅÁõÆÊ®ô„Å∏„ÅÆ<strong>Á≤æÂØÜ„Å™Âà∂Âæ°</strong>„ÅåÂèØËÉΩ</p>
<hr />
<h2>ÊºîÁøíÂïèÈ°å</h2>
<h3>ÂïèÈ°å1 (Èõ£ÊòìÂ∫¶: easy)</h3>
<p>„Éô„Éº„Çπ„É©„Ç§„É≥„Çí‰Ωø„ÅÜ„Å®ÂàÜÊï£„ÅåÊ∏õ„ÇãÁêÜÁî±„Çí„ÄÅ‰ª•‰∏ã„ÅÆÂºè„Çí‰Ωø„Å£„Å¶Ë™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<p>$$
\text{Var}[R_t] \quad \text{vs.} \quad \text{Var}[R_t - b(s_t)]
$$</p>
<details>
<summary>„Éí„É≥„Éà</summary>

ÂàÜÊï£„ÅÆÊÄßË≥™: $\text{Var}[X - c] = \text{Var}[X]$ÔºàÂÆöÊï∞$c$„ÇíÂºï„ÅÑ„Å¶„ÇÇÂàÜÊï£„ÅØÂ§â„Çè„Çâ„Å™„ÅÑÔºâ„Åß„Åô„Åå„ÄÅ$b(s\_t)$„ÅØÁä∂ÊÖã‰æùÂ≠ò„Å™„ÅÆ„ÅßÂÆöÊï∞„Åß„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ

</details>

<details>
<summary>Ëß£Á≠î‰æã</summary>

„Éô„Éº„Çπ„É©„Ç§„É≥$b(s\_t)$„ÅåÁä∂ÊÖã‰æ°ÂÄ§$V(s\_t)$„Å´Ëøë„ÅÑ„Å®„ÅçÔºö

- **„É™„Çø„Éº„É≥** $R\_t$„ÅØÁä∂ÊÖã„Å´„Çà„Å£„Å¶Â§ß„Åç„ÅèÂ§âÂãïÔºàÈÅã„Å´„Çà„ÇãÂΩ±ÈüøÂ§ßÔºâ
- **„Ç¢„Éâ„Éê„É≥„ÉÜ„Éº„Ç∏** $R\_t - V(s\_t)$„ÅØ„ÄåÂπ≥Âùá„Åã„Çâ„ÅÆ„Ç∫„É¨„Äç„Å™„ÅÆ„ÅßÂ§âÂãï„ÅåÂ∞è„Åï„ÅÑ

Êï∞Â≠¶ÁöÑ„Å´„ÅØÔºö
$$
\text{Var}[R\_t - V(s\_t)] \leq \text{Var}[R\_t]
$$

„Åì„Çå„ÅØ$V(s\_t)$„Åå„ÄåÁä∂ÊÖã$s\_t$„Åã„Çâ„ÅÆÊúüÂæÖÁ¥ØÁ©çÂ†±ÈÖ¨„Äç„Å™„ÅÆ„Åß„ÄÅÈÅã„ÅÆÂΩ±Èüø„Çí„Ç≠„É£„É≥„Çª„É´„Åô„Çã„Åü„ÇÅ„Åß„Åô„ÄÇ

**ÂÖ∑‰Ωì‰æã**:
- Áä∂ÊÖãA„Åã„Çâ„ÅÆ„É™„Çø„Éº„É≥: 100, 105, 95 ‚Üí ÂàÜÊï£ = 25
- Áä∂ÊÖãA„ÅÆ‰æ°ÂÄ§: 100
- „Ç¢„Éâ„Éê„É≥„ÉÜ„Éº„Ç∏: 0, 5, -5 ‚Üí ÂàÜÊï£ = 25ÔºàÂêå„ÅòÔºâ

„Åó„Åã„Åó„ÄÅË§áÊï∞„ÅÆÁä∂ÊÖã„ÇíËÄÉ„Åà„Çã„Å®Ôºö
- Áä∂ÊÖãA„ÅÆ„É™„Çø„Éº„É≥: 100¬±5
- Áä∂ÊÖãB„ÅÆ„É™„Çø„Éº„É≥: 50¬±5
- ÂÖ®‰Ωì„ÅÆÂàÜÊï£: Â§ß„Åç„ÅÑ

„Éô„Éº„Çπ„É©„Ç§„É≥„ÅßÁä∂ÊÖã„Åî„Å®„ÅÆÂπ≥Âùá„ÇíÂºï„Åè„Å®„ÄÅÁä∂ÊÖãÈñì„ÅÆÂ∑Æ„ÅåÊ∂à„Åà„ÄÅÂàÜÊï£„ÅåÊ∏õ„Çä„Åæ„Åô„ÄÇ

</details>

<hr />
<h3>ÂïèÈ°å2 (Èõ£ÊòìÂ∫¶: medium)</h3>
<p>PPO„ÅÆ„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞ÁØÑÂõ≤$\epsilon$„ÇíÂ§ß„Åç„Åè„Åô„Çã„Å®‰Ωï„ÅåËµ∑„Åì„Çã„Åã„ÄÅ„Åæ„Åü$\epsilon=0$„ÅÆÊ•µÁ´Ø„Å™„Ç±„Éº„Çπ„Åß„ÅØ„Å©„ÅÜ„Å™„Çã„ÅãË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<details>
<summary>„Éí„É≥„Éà</summary>

„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞Âºè„ÇíË¶ãÁõ¥„Åó„ÄÅ$r\_t(\theta)$„ÅÆÂ§âÂåñ„Åå„Å©„ÅÜÂà∂Èôê„Åï„Çå„Çã„ÅãËÄÉ„Åà„Å¶„Åø„Åæ„Åó„Çá„ÅÜ„ÄÇ

</details>

<details>
<summary>Ëß£Á≠î‰æã</summary>

**$\epsilon$„ÇíÂ§ß„Åç„Åè„Åô„Çã„Å®**:
- „ÇØ„É™„ÉÉ„Éî„É≥„Ç∞ÁØÑÂõ≤„ÅåÂ∫É„Åå„Çä„ÄÅÊñπÁ≠ñ„ÅÆÂ§âÂåñ„ÅåÂ§ß„Åç„Åè„Å™„Çã
- Â≠¶Áøí„ÅåÈÄü„ÅÑ„Åå‰∏çÂÆâÂÆö„Å´„Å™„Çä„ÇÑ„Åô„ÅÑ
- Ê•µÁ´Ø„Å™Â†¥Âêà„ÄÅÊñπÁ≠ñ„ÅåÂ¥©Â£ä„Åô„ÇãÂèØËÉΩÊÄß

**$\epsilon=0$„ÅÆÂ†¥Âêà**:
$$
\text{clip}(r\_t, 1, 1) = 1
$$

- ÈáçË¶ÅÂ∫¶ÊØîÁéá„ÅåÂ∏∏„Å´1„Å´„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞
- ÊñπÁ≠ñ„ÅåÂÖ®„ÅèÊõ¥Êñ∞„Åï„Çå„Å™„ÅÑÔºà$\pi\_\theta = \pi\_{\theta\_{\text{old}}}$„ÇíÂº∑Âà∂Ôºâ

**ÂÆüË∑µÁöÑ„Å™ÂÄ§**: $\epsilon = 0.1 \sim 0.2$„Åå‰∏ÄËà¨ÁöÑ

**ÂÆüÈ®ì„Ç≥„Éº„Éâ**:

<pre><code class="language-python"># Œµ=0.05ÔºàÂé≥„Åó„ÅÑÂà∂Á¥ÑÔºâ
model_tight = PPO(&quot;MlpPolicy&quot;, env, clip_range=0.05)

# Œµ=0.5ÔºàÁ∑©„ÅÑÂà∂Á¥ÑÔºâ
model_loose = PPO(&quot;MlpPolicy&quot;, env, clip_range=0.5)

# Â≠¶ÁøíÊõ≤Á∑ö„ÇíÊØîËºÉ
# ‚Üí model_tight„ÅØÂÆâÂÆö„Å†„ÅåÈÅÖ„ÅÑ
# ‚Üí model_loose„ÅØÈÄü„ÅÑ„ÅåÊåØÂãï„Åô„Çã
</code></pre>


</details>

<hr />
<h3>ÂïèÈ°å3 (Èõ£ÊòìÂ∫¶: hard)</h3>
<p>ÊùêÊñôÊé¢Á¥¢„Å´„Åä„ÅÑ„Å¶„ÄÅ‰ª•‰∏ã„ÅÆ2„Å§„ÅÆÂ†±ÈÖ¨Ë®≠Ë®à„ÇíÊØîËºÉ„Åó„ÄÅ„Åù„Çå„Åû„Çå„ÅÆÈï∑ÊâÄ„ÉªÁü≠ÊâÄ„ÇíËø∞„Åπ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Åæ„Åü„ÄÅÂÆüÈöõ„Å´„Ç≥„Éº„Éâ„ÅßÂÆüÈ®ì„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<p><strong>Â†±ÈÖ¨AÔºàÁñéÂ†±ÈÖ¨Ôºâ</strong>: ÁõÆÊ®ô„Å´Âà∞ÈÅî„Åó„Åü„Å®„Åç„ÅÆ„ÅøÂ†±ÈÖ¨1„ÄÅ„Åù„Çå‰ª•Â§ñ„ÅØ0
<strong>Â†±ÈÖ¨BÔºàÂØÜÂ†±ÈÖ¨Ôºâ</strong>: ÁõÆÊ®ô„Å®„ÅÆË∑ùÈõ¢„Å´Âøú„Åò„ÅüÈÄ£Á∂öÁöÑ„Å™Â†±ÈÖ¨</p>
<details>
<summary>„Éí„É≥„Éà</summary>

ÁñéÂ†±ÈÖ¨„ÅØÊé¢Á¥¢„ÅåÂõ∞Èõ£„Åß„Åô„Åå„ÄÅÂØÜÂ†±ÈÖ¨„ÅØÂ±ÄÊâÄÊúÄÈÅ©Ëß£„Å´Èô•„Çä„ÇÑ„Åô„ÅÑ„Åß„Åô„ÄÇ„Ç®„É≥„Éà„É≠„Éî„Éº„Éú„Éº„Éä„Çπ„ÅÆÂΩ±Èüø„ÇÇËÄÉ„Åà„Å¶„Åø„Åæ„Åó„Çá„ÅÜ„ÄÇ

</details>

<details>
<summary>Ëß£Á≠î‰æã</summary>

**Â†±ÈÖ¨AÔºàÁñéÂ†±ÈÖ¨Ôºâ„ÅÆÈï∑ÊâÄ„ÉªÁü≠ÊâÄ**:

**Èï∑ÊâÄ**:
- ÊòéÁ¢∫„Å™ÁõÆÊ®ôÔºàÊõñÊòß„Åï„Åå„Å™„ÅÑÔºâ
- Â±ÄÊâÄÊúÄÈÅ©Ëß£„Å´Èô•„Çä„Å´„Åè„ÅÑÔºà‰∏≠ÈñìÂ†±ÈÖ¨„Å´ÊÉë„Çè„Åï„Çå„Å™„ÅÑÔºâ

**Áü≠ÊâÄ**:
- Êé¢Á¥¢„ÅåÈùûÂ∏∏„Å´Âõ∞Èõ£ÔºàÂ≠¶Áøí„Ç∑„Ç∞„Éä„É´„ÅåÂº±„ÅÑÔºâ
- Â≠¶Áøí„Å´ÊôÇÈñì„Åå„Åã„Åã„Çã

**Â†±ÈÖ¨BÔºàÂØÜÂ†±ÈÖ¨Ôºâ„ÅÆÈï∑ÊâÄ„ÉªÁü≠ÊâÄ**:

**Èï∑ÊâÄ**:
- Êé¢Á¥¢„ÅåÂÆπÊòìÔºàÊØé„Çπ„ÉÜ„ÉÉ„Éó„Åß„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØÔºâ
- Â≠¶Áøí„ÅåÈÄü„ÅÑ

**Áü≠ÊâÄ**:
- Â†±ÈÖ¨Ë®≠Ë®à„ÅåÈõ£„Åó„ÅÑÔºàË∑ùÈõ¢„Å†„Åë„Åß„ÅØ‰∏çÂçÅÂàÜ„Å™Â†¥Âêà„ÇÇÔºâ
- Â±ÄÊâÄÊúÄÈÅ©Ëß£„Å´Èô•„Çä„ÇÑ„Åô„ÅÑ

**ÂÆüÈ®ì„Ç≥„Éº„Éâ**:

<pre><code class="language-python"># Â†±ÈÖ¨AÔºàÁñéÂ†±ÈÖ¨Ôºâ
class SparseRewardEnv(gym.Env):
    def step(self, action):
        # ... (Áä∂ÊÖãÊõ¥Êñ∞) ...
        distance = np.linalg.norm(self.state - self.target)

        if distance &lt; 0.5:
            reward = 1.0  # Âà∞ÈÅî
            done = True
        else:
            reward = 0.0  # „Åù„Çå‰ª•Â§ñ
            done = False

        return self.state, reward, done, {}

# Â†±ÈÖ¨BÔºàÂØÜÂ†±ÈÖ¨Ôºâ
class DenseRewardEnv(gym.Env):
    def step(self, action):
        # ... (Áä∂ÊÖãÊõ¥Êñ∞) ...
        distance = np.linalg.norm(self.state - self.target)
        reward = -distance  # ÈÄ£Á∂öÁöÑ„Å™Â†±ÈÖ¨
        done = distance &lt; 0.5

        return self.state, reward, done, {}

# ÊØîËºÉÂÆüÈ®ì
model_sparse = PPO(&quot;MlpPolicy&quot;, DummyVecEnv([lambda: SparseRewardEnv()]))
model_dense = PPO(&quot;MlpPolicy&quot;, DummyVecEnv([lambda: DenseRewardEnv()]))

model_sparse.learn(total_timesteps=100000)
model_dense.learn(total_timesteps=100000)

# ÁµêÊûú: model_dense„ÅÆÊñπ„ÅåÂ≠¶Áøí„ÅåÈÄü„ÅÑ„Åå„ÄÅ
# Ë§áÈõë„Å™Áí∞Â¢É„Åß„ÅØmodel_sparse„ÅÆÊñπ„ÅåËâØ„ÅÑËß£„ÇíË¶ã„Å§„Åë„Çã„Åì„Å®„ÇÇ
</code></pre>


**„Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„Çπ**: ÂØÜÂ†±ÈÖ¨„Åã„ÇâÂßã„ÇÅ„ÄÅÂïèÈ°å„Å´Âøú„Åò„Å¶ÁñéÂ†±ÈÖ¨„ÇÑ**Â†±ÈÖ¨„Ç∑„Çß„Ç§„Éî„É≥„Ç∞**Ôºà‰∏≠ÈñìÂ†±ÈÖ¨„ÅÆËøΩÂä†Ôºâ„ÇíÊ§úË®é„ÄÇ

</details>

<hr />
<h2>„Åì„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„ÅÆ„Åæ„Å®„ÇÅ</h2>
<ul>
<li><strong>ÊñπÁ≠ñÂãæÈÖçÊ≥ï</strong>„ÅØÊñπÁ≠ñ„ÇíÁõ¥Êé•ÊúÄÈÅ©Âåñ„Åó„ÄÅÈÄ£Á∂öË°åÂãï„Å´ÂØæÂøú</li>
<li><strong>REINFORCE„Ç¢„É´„Ç¥„É™„Ç∫„É†</strong>„ÅØÈ´òÂàÜÊï£„Å†„Åå„ÄÅ„Éô„Éº„Çπ„É©„Ç§„É≥„ÅßÊîπÂñÑ</li>
<li><strong>Actor-Critic</strong>„ÅØActor„Å®Critic„ÇíÂêåÊôÇÂ≠¶Áøí„Åó„ÄÅ‰ΩéÂàÜÊï£„Éª„Ç™„É≥„É©„Ç§„É≥Â≠¶Áøí</li>
<li><strong>PPO</strong>„ÅØ„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞„Å´„Çà„ÇäÂÆâÂÆö„Åó„ÅüÂ≠¶Áøí„ÇíÂÆüÁèæ„ÄÅÊúÄÂÖàÁ´Ø„ÅÆÂÆüÁî®ÁöÑÊâãÊ≥ï</li>
<li><strong>Stable Baselines3</strong>„Å´„Çà„Çä„ÄÅ„Çè„Åö„ÅãÊï∞Ë°å„ÅßPPO„ÇíÂÆüË£ÖÂèØËÉΩ</li>
<li>ÈÄ£Á∂öË°åÂãïÁ©∫Èñì„Åß„ÅØ<strong>„Ç¨„Ç¶„ÇπÊñπÁ≠ñ</strong>„Çí‰ΩøÁî®</li>
</ul>
<p>Ê¨°Á´†„Åß„ÅØ„ÄÅÊùêÊñôÊé¢Á¥¢„Å´ÁâπÂåñ„Åó„Åü„Ç´„Çπ„Çø„É†Áí∞Â¢É„ÅÆÊßãÁØâ„Å®Â†±ÈÖ¨Ë®≠Ë®à„ÇíÂ≠¶„Å≥„Åæ„Åô„ÄÇ</p>
<hr />
<h2>ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„ÉàÔºöÊñπÁ≠ñÂãæÈÖçÊ≥ï„ÅÆÂÆüË£ÖÁ¢∫Ë™ç</h2>
<h3>ÁêÜË´ñÁêÜËß£„Çπ„Ç≠„É´</h3>
<ul>
<li>[ ] ÊñπÁ≠ñÂãæÈÖçÂÆöÁêÜ„ÇíÊï∞Âºè„ÅßË™¨Êòé„Åß„Åç„Çã</li>
<li>[ ] REINFORCE„ÅÆÊõ¥Êñ∞Âºè„ÇíÂ∞éÂá∫„Åß„Åç„Çã</li>
<li>[ ] „Éô„Éº„Çπ„É©„Ç§„É≥„ÅåÂàÜÊï£„ÇíÊ∏õ„Çâ„ÅôÁêÜÁî±„ÇíË™¨Êòé„Åß„Åç„Çã</li>
<li>[ ] PPO„ÅÆ„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞„ÅÆÂΩπÂâ≤„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
</ul>
<h3>ÂÆüË£Ö„Çπ„Ç≠„É´</h3>
<ul>
<li>[ ] PyTorch„ÅßÊñπÁ≠ñ„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] „É™„Çø„Éº„É≥ÔºàÁ¥ØÁ©çÂ†±ÈÖ¨Ôºâ„ÅÆË®àÁÆó„Åå„Åß„Åç„Çã</li>
<li>[ ] „Ç¢„Éâ„Éê„É≥„ÉÜ„Éº„Ç∏Èñ¢Êï∞„ÅÆË®àÁÆó„Åå„Åß„Åç„Çã</li>
<li>[ ] Stable Baselines3„ÅßPPO„Çí‰ΩøÁî®„Åß„Åç„Çã</li>
</ul>
<h3>ÊùêÊñôÊé¢Á¥¢„Å∏„ÅÆÂøúÁî®</h3>
<ul>
<li>[ ] Ê∏©Â∫¶„ÉªÂúßÂäõ„Å™„Å©„ÅÆÈÄ£Á∂öÂà∂Âæ°Â§âÊï∞„ÇíË°åÂãïÁ©∫Èñì„Å®„Åó„Å¶Ë®≠Ë®à„Åß„Åç„Çã</li>
<li>[ ] Â§öÁõÆÁöÑÂ†±ÈÖ¨ÔºàÂèéÁéá„ÉªÈÅ∏ÊäûÊÄßÔºâ„ÇíÈÅ©Âàá„Å´Èáç„Åø‰ªò„Åë„Åß„Åç„Çã</li>
<li>[ ] ÂÆâÂÖ®Âà∂Á¥ÑÔºàÊ∏©Â∫¶‰∏äÈôê„Å™„Å©Ôºâ„ÇíÂ†±ÈÖ¨„Å´ÁµÑ„ÅøËæº„ÇÅ„Çã</li>
</ul>
<h3>„Éá„Éê„ÉÉ„Ç∞„Çπ„Ç≠„É´</h3>
<ul>
<li>[ ] ÊñπÁ≠ñÂãæÈÖç„ÅÆÂàÜÊï£„ÅåÂ§ß„Åç„ÅÑÂ†¥Âêà„ÅÆÂØæÂá¶Ê≥ï„ÇíÁü•„Å£„Å¶„ÅÑ„Çã</li>
<li>[ ] PPO„ÅåÂèéÊùü„Åó„Å™„ÅÑÂ†¥Âêà„ÅÆÂéüÂõ†„ÇíÁâπÂÆö„Åß„Åç„Çã</li>
<li>[ ] „Ç®„É≥„Éà„É≠„Éî„Éº„Éú„Éº„Éä„Çπ„ÅÆË™øÊï¥„Åå„Åß„Åç„Çã</li>
</ul>
<hr />
<h2>ÂèÇËÄÉÊñáÁåÆ</h2>
<ol>
<li>Williams "Simple statistical gradient-following algorithms for connectionist reinforcement learning" <em>Machine Learning</em> (1992) - REINFORCE</li>
<li>Mnih et al. "Asynchronous methods for deep reinforcement learning" <em>ICML</em> (2016) - A3C/A2C</li>
<li>Schulman et al. "Proximal policy optimization algorithms" <em>arXiv</em> (2017) - PPO</li>
<li>Schulman et al. "Trust region policy optimization" <em>ICML</em> (2015) - TRPO</li>
<li>Raffin et al. "Stable-Baselines3: Reliable reinforcement learning implementations" <em>JMLR</em> (2021)</li>
</ol>
<hr />
<p><strong>Ê¨°Á´†</strong>: <a href="chapter-3.html">Á¨¨3Á´†: ÊùêÊñôÊé¢Á¥¢Áí∞Â¢É„ÅÆÊßãÁØâ</a></p><div class="navigation">
    <a href="chapter-1.html" class="nav-button">‚Üê Ââç„ÅÆÁ´†</a>
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
    <a href="chapter-3.html" class="nav-button">Ê¨°„ÅÆÁ´† ‚Üí</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>Áõ£‰øÆ</strong>: Dr. Yusuke HashimotoÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 2.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-17</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
