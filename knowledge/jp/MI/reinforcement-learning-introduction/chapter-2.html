<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬2ç« : å¼·åŒ–å­¦ç¿’ã®åŸºç¤ç†è«– - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬2ç« : å¼·åŒ–å­¦ç¿’ã®åŸºç¤ç†è«–</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 20-25åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 3å•</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>ç¬¬2ç« : å¼·åŒ–å­¦ç¿’ã®åŸºç¤ç†è«–</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">Qå­¦ç¿’/DQN/PPOãªã©ä»£è¡¨æ‰‹æ³•ã®ç›´è¦³ã¨é•ã„ã‚’æ•´ç†ã—ã¾ã™ã€‚ã©ã®èª²é¡Œã«ã©ã‚Œã‚’è©¦ã™ã‹ã®å½“ãŸã‚Šã‚’ä»˜ã‘ã¾ã™ã€‚</p>




<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã§ã¯ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã—ã¾ã™ï¼š</p>
<ul>
<li>æ–¹ç­–å‹¾é…æ³•ï¼ˆPolicy Gradient Methodsï¼‰ã®ç†è«–ã¨å®Ÿè£…</li>
<li>Actor-Criticã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ä»•çµ„ã¿</li>
<li>Proximal Policy Optimizationï¼ˆPPOï¼‰ã®è©³ç´°</li>
<li>Stable Baselines3ã«ã‚ˆã‚‹å®Ÿè·µçš„å®Ÿè£…</li>
</ul>
<hr />
<h2>2.1 æ–¹ç­–å‹¾é…æ³•ï¼ˆPolicy Gradient Methodsï¼‰</h2>
<h3>Qå­¦ç¿’ã®é™ç•Œ</h3>
<p>ç¬¬1ç« ã®Qå­¦ç¿’ãƒ»DQNã¯<strong>ä¾¡å€¤ãƒ™ãƒ¼ã‚¹</strong>ã®æ‰‹æ³•ã§ã—ãŸã€‚ã“ã‚Œã‚‰ã«ã¯ä»¥ä¸‹ã®é™ç•ŒãŒã‚ã‚Šã¾ã™ï¼š</p>
<ol>
<li><strong>é›¢æ•£è¡Œå‹•ã®ã¿</strong>: $\arg\max_a Q(s,a)$ã¯é€£ç¶šè¡Œå‹•ç©ºé–“ã§å›°é›£</li>
<li><strong>æ±ºå®šçš„æ–¹ç­–</strong>: å¸¸ã«åŒã˜è¡Œå‹•ã‚’é¸æŠï¼ˆç¢ºç‡çš„æ–¹ç­–ãŒå­¦ç¿’ã§ããªã„ï¼‰</li>
<li><strong>å°ã•ãªå¤‰åŒ–ã«è„†å¼±</strong>: Qå€¤ã®å¾®å°ãªå¤‰åŒ–ã§æ–¹ç­–ãŒå¤§ããå¤‰ã‚ã‚‹</li>
</ol>
<p>ææ–™ç§‘å­¦ã§ã¯ã€<strong>é€£ç¶šçš„ãªåˆ¶å¾¡</strong>ï¼ˆæ¸©åº¦ã‚’0.5åº¦ä¸Šã’ã‚‹ã€çµ„æˆæ¯”ã‚’2%å¤‰ãˆã‚‹ï¼‰ãŒé‡è¦ã§ã™ã€‚</p>
<h3>æ–¹ç­–å‹¾é…æ³•ã®åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢</h3>
<p>æ–¹ç­–å‹¾é…æ³•ã¯ã€<strong>æ–¹ç­–ã‚’ç›´æ¥æœ€é©åŒ–</strong>ã—ã¾ã™ï¼š</p>
<p>$$
\pi_\theta(a|s) = P(a|s; \theta)
$$</p>
<ul>
<li>$\theta$: æ–¹ç­–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡ã¿ï¼‰</li>
</ul>
<p><strong>ç›®çš„</strong>: æœŸå¾…ç´¯ç©å ±é…¬$J(\theta)$ã‚’æœ€å¤§åŒ–</p>
<p>$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T r_t \right]
$$</p>
<ul>
<li>$\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots)$: è»Œè·¡ï¼ˆtrajectoryï¼‰</li>
</ul>
<h3>æ–¹ç­–å‹¾é…å®šç†</h3>
<p><strong>REINFORCE</strong>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆWilliams, 1992ï¼‰ã¯ã€å‹¾é…ã‚’ä»¥ä¸‹ã§è¨ˆç®—ã—ã¾ã™ï¼š</p>
<p>$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot R_t \right]
$$</p>
<ul>
<li>$R_t = \sum_{k=t}^T \gamma^{k-t} r_k$: æ™‚åˆ»$t$ã‹ã‚‰ã®ç´¯ç©å ±é…¬ï¼ˆãƒªã‚¿ãƒ¼ãƒ³ï¼‰</li>
</ul>
<p><strong>ç›´æ„Ÿçš„æ„å‘³</strong>:
- é«˜ã„å ±é…¬ã‚’å¾—ãŸè¡Œå‹•ã®ç¢ºç‡ã‚’ä¸Šã’ã‚‹
- ä½ã„å ±é…¬ã‚’å¾—ãŸè¡Œå‹•ã®ç¢ºç‡ã‚’ä¸‹ã’ã‚‹</p>
<h3>REINFORCEã®å®Ÿè£…</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

class PolicyNetwork(nn.Module):
    &quot;&quot;&quot;æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

    çŠ¶æ…‹ã‚’å…¥åŠ›ã—ã€å„è¡Œå‹•ã®ç¢ºç‡ã‚’å‡ºåŠ›
    &quot;&quot;&quot;
    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return torch.softmax(self.fc3(x), dim=-1)  # ç¢ºç‡åˆ†å¸ƒ


class REINFORCEAgent:
    &quot;&quot;&quot;REINFORCEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ &quot;&quot;&quot;
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):
        self.gamma = gamma
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)

        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†…ã®ãƒ­ã‚°ã‚’ä¿å­˜
        self.log_probs = []
        self.rewards = []

    def select_action(self, state):
        &quot;&quot;&quot;æ–¹ç­–ã«å¾“ã£ã¦è¡Œå‹•ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°&quot;&quot;&quot;
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        probs = self.policy(state_tensor)

        # ç¢ºç‡åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()

        # logç¢ºç‡ã‚’ä¿å­˜ï¼ˆå‹¾é…è¨ˆç®—ç”¨ï¼‰
        self.log_probs.append(action_dist.log_prob(action))

        return action.item()

    def store_reward(self, reward):
        &quot;&quot;&quot;å ±é…¬ã‚’ä¿å­˜&quot;&quot;&quot;
        self.rewards.append(reward)

    def update(self):
        &quot;&quot;&quot;ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å¾Œã«æ–¹ç­–ã‚’æ›´æ–°&quot;&quot;&quot;
        # ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆç´¯ç©å ±é…¬ï¼‰ã‚’è¨ˆç®—
        returns = []
        R = 0
        for r in reversed(self.rewards):
            R = r + self.gamma * R
            returns.insert(0, R)

        returns = torch.FloatTensor(returns)

        # æ­£è¦åŒ–ï¼ˆå­¦ç¿’ã‚’å®‰å®šåŒ–ï¼‰
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)

        # æ–¹ç­–å‹¾é…
        policy_loss = []
        for log_prob, R in zip(self.log_probs, returns):
            policy_loss.append(-log_prob * R)

        # å‹¾é…é™ä¸‹
        self.optimizer.zero_grad()
        loss = torch.stack(policy_loss).sum()
        loss.backward()
        self.optimizer.step()

        # ãƒ­ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
        self.log_probs = []
        self.rewards = []


# ç°¡å˜ãªææ–™æ¢ç´¢ç’°å¢ƒï¼ˆé›¢æ•£è¡Œå‹•ç‰ˆï¼‰
class DiscreteMaterialsEnv:
    &quot;&quot;&quot;é›¢æ•£è¡Œå‹•ã®ææ–™æ¢ç´¢ç’°å¢ƒ&quot;&quot;&quot;
    def __init__(self, state_dim=4):
        self.state_dim = state_dim
        self.target = np.array([3.0, 5.0, 2.5, 4.0])
        self.state = None

    def reset(self):
        self.state = np.random.uniform(0, 10, self.state_dim)
        return self.state

    def step(self, action):
        # è¡Œå‹•: 0=æ¬¡å…ƒ0å¢—åŠ , 1=æ¬¡å…ƒ0æ¸›å°‘, 2=æ¬¡å…ƒ1å¢—åŠ , 3=æ¬¡å…ƒ1æ¸›å°‘
        dim = action // 2
        delta = 0.5 if action % 2 == 0 else -0.5

        self.state[dim] = np.clip(self.state[dim] + delta, 0, 10)

        # å ±é…¬: ç›®æ¨™ã¨ã®è·é›¢
        distance = np.linalg.norm(self.state - self.target)
        reward = -distance

        done = distance &lt; 0.5

        return self.state, reward, done


# REINFORCEã®è¨“ç·´
env = DiscreteMaterialsEnv()
agent = REINFORCEAgent(state_dim=4, action_dim=4)

episodes = 1000
rewards_history = []

for episode in range(episodes):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action = agent.select_action(state)
        next_state, reward, done = env.step(action)

        agent.store_reward(reward)
        state = next_state
        total_reward += reward

    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å¾Œã«æ›´æ–°
    agent.update()
    rewards_history.append(total_reward)

    if (episode + 1) % 100 == 0:
        avg_reward = np.mean(rewards_history[-100:])
        print(f&quot;Episode {episode+1}: Avg Reward = {avg_reward:.2f}&quot;)

# å­¦ç¿’æ›²ç·š
plt.figure(figsize=(10, 6))
plt.plot(np.convolve(rewards_history, np.ones(20)/20, mode='valid'))
plt.xlabel('Episode')
plt.ylabel('Average Reward (20 episodes)')
plt.title('REINFORCE: æ–¹ç­–å‹¾é…æ³•ã«ã‚ˆã‚‹ææ–™æ¢ç´¢')
plt.grid(True)
plt.show()
</code></pre>
<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>Episode 100: Avg Reward = -38.24
Episode 200: Avg Reward = -28.15
Episode 500: Avg Reward = -15.32
Episode 1000: Avg Reward = -7.89
</code></pre>
<hr />
<h2>2.2 ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨åˆ†æ•£å‰Šæ¸›</h2>
<h3>REINFORCEã®å•é¡Œç‚¹</h3>
<p>REINFORCEã¯<strong>é«˜åˆ†æ•£</strong>ï¼ˆhigh varianceï¼‰ã§ã™ã€‚åŒã˜æ–¹ç­–ã§ã‚‚ã€é‹ãŒè‰¯ã„ã‹æ‚ªã„ã‹ã§ãƒªã‚¿ãƒ¼ãƒ³$R_t$ãŒå¤§ããå¤‰å‹•ã—ã¾ã™ã€‚</p>
<h3>ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®å°å…¥</h3>
<p><strong>ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³</strong> $b(s)$ã‚’å¼•ãã“ã¨ã§ã€åˆ†æ•£ã‚’å‰Šæ¸›ï¼š</p>
<p>$$
\nabla_\theta J(\theta) = \mathbb{E} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot (R_t - b(s_t)) \right]
$$</p>
<p><strong>æœ€é©ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³</strong>: çŠ¶æ…‹ä¾¡å€¤é–¢æ•°$V(s)$</p>
<p>$$
b(s_t) = V(s_t) = \mathbb{E}_{\pi} \left[ \sum_{k=t}^T \gamma^{k-t} r_k \mid s_t \right]
$$</p>
<p><strong>ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸é–¢æ•°</strong> $A(s, a)$:
$$
A(s, a) = Q(s, a) - V(s) = R_t - V(s_t)
$$</p>
<p>ã€Œã“ã®è¡Œå‹•ã¯å¹³å‡ã‚ˆã‚Šã©ã‚Œã ã‘è‰¯ã„ã‹ã€ã‚’è¡¨ã—ã¾ã™ã€‚</p>
<h3>ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä»˜ãREINFORCE</h3>
<pre><code class="language-python">class ValueNetwork(nn.Module):
    &quot;&quot;&quot;ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰&quot;&quot;&quot;
    def __init__(self, state_dim, hidden_dim=64):
        super(ValueNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)  # çŠ¶æ…‹ä¾¡å€¤ã‚’å‡ºåŠ›

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)


class REINFORCEWithBaseline:
    &quot;&quot;&quot;ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä»˜ãREINFORCE&quot;&quot;&quot;
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):
        self.gamma = gamma
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.value = ValueNetwork(state_dim)

        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        self.value_optimizer = optim.Adam(self.value.parameters(), lr=lr)

        self.log_probs = []
        self.rewards = []
        self.states = []

    def select_action(self, state):
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        probs = self.policy(state_tensor)

        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()

        self.log_probs.append(action_dist.log_prob(action))
        self.states.append(state)

        return action.item()

    def store_reward(self, reward):
        self.rewards.append(reward)

    def update(self):
        # ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—
        returns = []
        R = 0
        for r in reversed(self.rewards):
            R = r + self.gamma * R
            returns.insert(0, R)

        returns = torch.FloatTensor(returns)
        states = torch.FloatTensor(self.states)

        # ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‡ºåŠ›ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
        values = self.value(states).squeeze()

        # ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ = ãƒªã‚¿ãƒ¼ãƒ³ - ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
        advantages = returns - values.detach()

        # æ–¹ç­–å‹¾é…æå¤±
        policy_loss = []
        for log_prob, adv in zip(self.log_probs, advantages):
            policy_loss.append(-log_prob * adv)

        # ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æå¤±ï¼ˆMSEï¼‰
        value_loss = nn.MSELoss()(values, returns)

        # æœ€é©åŒ–
        self.policy_optimizer.zero_grad()
        torch.stack(policy_loss).sum().backward()
        self.policy_optimizer.step()

        self.value_optimizer.zero_grad()
        value_loss.backward()
        self.value_optimizer.step()

        # ãƒªã‚»ãƒƒãƒˆ
        self.log_probs = []
        self.rewards = []
        self.states = []


# è¨“ç·´ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä»˜ãï¼‰
agent_baseline = REINFORCEWithBaseline(state_dim=4, action_dim=4)

rewards_baseline = []
for episode in range(1000):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action = agent_baseline.select_action(state)
        next_state, reward, done = env.step(action)

        agent_baseline.store_reward(reward)
        state = next_state
        total_reward += reward

    agent_baseline.update()
    rewards_baseline.append(total_reward)

# æ¯”è¼ƒ
plt.figure(figsize=(10, 6))
plt.plot(np.convolve(rewards_history, np.ones(20)/20, mode='valid'), label='REINFORCE')
plt.plot(np.convolve(rewards_baseline, np.ones(20)/20, mode='valid'), label='REINFORCE + Baseline')
plt.xlabel('Episode')
plt.ylabel('Average Reward')
plt.title('ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«ã‚ˆã‚‹å­¦ç¿’å®‰å®šåŒ–')
plt.legend()
plt.grid(True)
plt.show()
</code></pre>
<p><strong>çµæœ</strong>: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«ã‚ˆã‚Šå­¦ç¿’ãŒ<strong>ã‚ˆã‚Šå®‰å®š</strong>ã—ã€åæŸãŒ<strong>é€Ÿã</strong>ãªã‚Šã¾ã™ã€‚</p>
<hr />
<h2>2.3 Actor-Criticã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h2>
<h3>Actor-Criticã®æ¦‚å¿µ</h3>
<p><strong>Actorï¼ˆæ–¹ç­–ï¼‰</strong> ã¨ <strong>Criticï¼ˆä¾¡å€¤é–¢æ•°ï¼‰</strong> ã‚’åŒæ™‚ã«å­¦ç¿’ï¼š</p>
<ul>
<li><strong>Actor</strong> $\pi_\theta(a|s)$: è¡Œå‹•ã‚’é¸æŠ</li>
<li><strong>Critic</strong> $V_\phi(s)$: çŠ¶æ…‹ã®ä¾¡å€¤ã‚’è©•ä¾¡</li>
</ul>
<div class="mermaid">
graph LR
    S[çŠ¶æ…‹ s] --> A[Actor: Ï€Î¸]
    S --> C[Critic: VÏ•]
    A -->|è¡Œå‹• a| E[ç’°å¢ƒ]
    E -->|å ±é…¬ r| C
    C -->|TDèª¤å·®| A
    C -->|ä¾¡å€¤è©•ä¾¡| C

    style A fill:#e1f5ff
    style C fill:#ffe1cc
</div>

<h3>TDèª¤å·®ã¨ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸</h3>
<p><strong>TDèª¤å·®</strong>ï¼ˆTemporal Difference Errorï¼‰:
$$
\delta_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)
$$</p>
<p>ã“ã‚Œã¯<strong>1ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸æ¨å®š</strong>ã¨ã—ã¦ä½¿ãˆã¾ã™ã€‚</p>
<h3>A2Cï¼ˆAdvantage Actor-Criticï¼‰</h3>
<pre><code class="language-python">class A2CAgent:
    &quot;&quot;&quot;Advantage Actor-Critic&quot;&quot;&quot;
    def __init__(self, state_dim, action_dim, lr_actor=1e-3, lr_critic=1e-3, gamma=0.99):
        self.gamma = gamma

        self.actor = PolicyNetwork(state_dim, action_dim)
        self.critic = ValueNetwork(state_dim)

        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)

    def select_action(self, state):
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        probs = self.actor(state_tensor)

        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()

        return action.item(), action_dist.log_prob(action)

    def update(self, state, action_log_prob, reward, next_state, done):
        &quot;&quot;&quot;1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«æ›´æ–°&quot;&quot;&quot;
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)

        # ç¾åœ¨ã¨æ¬¡ã®çŠ¶æ…‹ä¾¡å€¤
        value = self.critic(state_tensor)
        next_value = self.critic(next_state_tensor)

        # TDç›®æ¨™ã¨TDèª¤å·®
        td_target = reward + (1 - done) * self.gamma * next_value.item()
        td_error = td_target - value.item()

        # Criticæå¤±ï¼ˆMSEï¼‰
        critic_loss = (torch.FloatTensor([td_target]) - value).pow(2)

        # Actoræå¤±ï¼ˆæ–¹ç­–å‹¾é… Ã— ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ï¼‰
        actor_loss = -action_log_prob * td_error

        # æœ€é©åŒ–
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()


# A2Cè¨“ç·´
agent_a2c = A2CAgent(state_dim=4, action_dim=4)

rewards_a2c = []
for episode in range(1000):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action, log_prob = agent_a2c.select_action(state)
        next_state, reward, done = env.step(action)

        # 1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«æ›´æ–°
        agent_a2c.update(state, log_prob, reward, next_state, done)

        state = next_state
        total_reward += reward

    rewards_a2c.append(total_reward)

    if (episode + 1) % 100 == 0:
        avg_reward = np.mean(rewards_a2c[-100:])
        print(f&quot;Episode {episode+1}: Avg Reward = {avg_reward:.2f}&quot;)
</code></pre>
<p><strong>åˆ©ç‚¹</strong>:
- ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ã‚’å¾…ãŸãšã«<strong>ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’</strong>
- TDèª¤å·®ã«ã‚ˆã‚Š<strong>ä½åˆ†æ•£</strong></p>
<hr />
<h2>2.4 Proximal Policy Optimizationï¼ˆPPOï¼‰</h2>
<h3>Trust Region Methods</h3>
<p>æ–¹ç­–å‹¾é…æ³•ã§ã¯ã€<strong>æ›´æ–°ãŒå¤§ãã™ãã‚‹ã¨æ–¹ç­–ãŒå´©å£Š</strong>ã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</p>
<p><strong>Trust Region Policy Optimizationï¼ˆTRPOï¼‰</strong> ã¯ã€æ–¹ç­–ã®å¤‰åŒ–ã‚’åˆ¶ç´„ï¼š</p>
<p>$$
\max_\theta \mathbb{E} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} A(s, a) \right] \quad \text{s.t.} \quad D_{\text{KL}}(\pi_{\theta_{\text{old}}} | \pi_\theta) \leq \delta
$$</p>
<p>ã—ã‹ã—ã€KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹åˆ¶ç´„ã®æœ€é©åŒ–ã¯è¤‡é›‘ã§ã™ã€‚</p>
<h3>PPOã®ç°¡ç•¥åŒ–</h3>
<p><strong>PPO</strong>ï¼ˆSchulman et al., 2017ï¼‰ã¯ã€åˆ¶ç´„ã‚’<strong>æå¤±é–¢æ•°å†…ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°</strong>ã§å®Ÿç¾ï¼š</p>
<p>$$
L^{\text{CLIP}}(\theta) = \mathbb{E} \left[ \min \left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right]
$$</p>
<ul>
<li>$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$: é‡è¦åº¦æ¯”ç‡ï¼ˆimportance ratioï¼‰</li>
<li>$\epsilon$: ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç¯„å›²ï¼ˆé€šå¸¸0.1ã€œ0.2ï¼‰</li>
</ul>
<p><strong>ç›´æ„Ÿ</strong>:
- ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ãŒæ­£ï¼ˆè‰¯ã„è¡Œå‹•ï¼‰â†’ $r_t$ã‚’å¢—ã‚„ã™ãŒã€$1+\epsilon$ã§ä¸Šé™
- ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ãŒè² ï¼ˆæ‚ªã„è¡Œå‹•ï¼‰â†’ $r_t$ã‚’æ¸›ã‚‰ã™ãŒã€$1-\epsilon$ã§ä¸‹é™
- æ€¥æ¿€ãªæ–¹ç­–å¤‰åŒ–ã‚’é˜²ã</p>
<h3>ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒœãƒ¼ãƒŠã‚¹</h3>
<p>æ¢ç´¢ã‚’ä¿ƒé€²ã™ã‚‹ãŸã‚ã€<strong>ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼</strong>ã‚’æå¤±ã«è¿½åŠ ï¼š</p>
<p>$$
L^{\text{PPO}}(\theta) = L^{\text{CLIP}}(\theta) + c_1 L^{\text{VF}}(\theta) - c_2 H[\pi_\theta]
$$</p>
<ul>
<li>$L^{\text{VF}}$: ä¾¡å€¤é–¢æ•°ã®æå¤±</li>
<li>$H[\pi_\theta] = -\sum_a \pi_\theta(a|s) \log \pi_\theta(a|s)$: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆç¢ºç‡åˆ†å¸ƒã®ä¸ç¢ºå®Ÿæ€§ï¼‰</li>
<li>$c_2$: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ä¿‚æ•°ï¼ˆé€šå¸¸0.01ï¼‰</li>
</ul>
<h3>PPOã®å®Ÿè£…ï¼ˆStable Baselines3ä½¿ç”¨ï¼‰</h3>
<pre><code class="language-python">from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
import gym

# Gymç’°å¢ƒãƒ©ãƒƒãƒ‘ãƒ¼
class GymMaterialsEnv(gym.Env):
    &quot;&quot;&quot;OpenAI Gymäº’æ›ã®ææ–™æ¢ç´¢ç’°å¢ƒ&quot;&quot;&quot;
    def __init__(self):
        super(GymMaterialsEnv, self).__init__()
        self.state_dim = 4
        self.target = np.array([3.0, 5.0, 2.5, 4.0])

        # è¡Œå‹•ãƒ»çŠ¶æ…‹ç©ºé–“ã®å®šç¾©
        self.action_space = gym.spaces.Discrete(4)
        self.observation_space = gym.spaces.Box(
            low=0, high=10, shape=(self.state_dim,), dtype=np.float32
        )

        self.state = None

    def reset(self):
        self.state = np.random.uniform(0, 10, self.state_dim).astype(np.float32)
        return self.state

    def step(self, action):
        dim = action // 2
        delta = 0.5 if action % 2 == 0 else -0.5

        self.state[dim] = np.clip(self.state[dim] + delta, 0, 10)

        distance = np.linalg.norm(self.state - self.target)
        reward = -distance
        done = distance &lt; 0.5

        return self.state, reward, done, {}

    def render(self, mode='human'):
        pass


# ç’°å¢ƒä½œæˆ
env = DummyVecEnv([lambda: GymMaterialsEnv()])

# PPOãƒ¢ãƒ‡ãƒ«
model = PPO(
    &quot;MlpPolicy&quot;,                # å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³æ–¹ç­–
    env,
    learning_rate=3e-4,
    n_steps=2048,               # æ›´æ–°å‰ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°
    batch_size=64,
    n_epochs=10,                # å„æ›´æ–°ã§ã®æœ€é©åŒ–ã‚¨ãƒãƒƒã‚¯æ•°
    gamma=0.99,
    gae_lambda=0.95,            # GAEï¼ˆGeneralized Advantage Estimationï¼‰
    clip_range=0.2,             # PPOã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç¯„å›²
    ent_coef=0.01,              # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ä¿‚æ•°
    verbose=1,
    tensorboard_log=&quot;./ppo_materials_tensorboard/&quot;
)

# è¨“ç·´
model.learn(total_timesteps=100000)

# ä¿å­˜
model.save(&quot;ppo_materials_agent&quot;)

# è©•ä¾¡
eval_env = GymMaterialsEnv()
state = eval_env.reset()
total_reward = 0

for _ in range(100):
    action, _ = model.predict(state, deterministic=True)
    state, reward, done, _ = eval_env.step(action)
    total_reward += reward

    if done:
        break

print(f&quot;è©•ä¾¡çµæœ: Total Reward = {total_reward:.2f}&quot;)
print(f&quot;æœ€çµ‚çŠ¶æ…‹: {state}&quot;)
print(f&quot;ç›®æ¨™: {eval_env.target}&quot;)
</code></pre>
<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.2     |
|    ep_rew_mean     | -15.3    |
| time/              |          |
|    fps             | 1024     |
|    iterations      | 50       |
|    time_elapsed    | 97       |
|    total_timesteps | 102400   |
---------------------------------

è©•ä¾¡çµæœ: Total Reward = -5.23
æœ€çµ‚çŠ¶æ…‹: [3.02 4.98 2.47 3.95]
ç›®æ¨™: [3.  5.  2.5 4. ]
</code></pre>
<p><strong>è§£èª¬</strong>:
- Stable Baselines3ã«ã‚ˆã‚Šã€ã‚ãšã‹æ•°è¡Œã§PPOã‚’å®Ÿè£…
- TensorBoardã§å­¦ç¿’é€²æ—ã‚’å¯è¦–åŒ–å¯èƒ½
- ç›®æ¨™ã«éå¸¸ã«è¿‘ã„ææ–™ã‚’ç™ºè¦‹</p>
<hr />
<h2>2.5 é€£ç¶šè¡Œå‹•ç©ºé–“ã¸ã®æ‹¡å¼µ</h2>
<h3>ã‚¬ã‚¦ã‚¹æ–¹ç­–</h3>
<p>ææ–™ç§‘å­¦ã§ã¯ã€æ¸©åº¦ã‚„çµ„æˆæ¯”ãªã©<strong>é€£ç¶šçš„ãªåˆ¶å¾¡</strong>ãŒå¿…è¦ã§ã™ã€‚</p>
<p>é€£ç¶šè¡Œå‹•ã«ã¯<strong>ã‚¬ã‚¦ã‚¹åˆ†å¸ƒæ–¹ç­–</strong>ã‚’ä½¿ç”¨ï¼š</p>
<p>$$
\pi_\theta(a|s) = \mathcal{N}(\mu_\theta(s), \sigma_\theta(s))
$$</p>
<ul>
<li>$\mu_\theta(s)$: å¹³å‡ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§å‡ºåŠ›ï¼‰</li>
<li>$\sigma_\theta(s)$: æ¨™æº–åå·®ï¼ˆå­¦ç¿’å¯èƒ½ã¾ãŸã¯å›ºå®šï¼‰</li>
</ul>
<h3>é€£ç¶šè¡Œå‹•ç‰ˆPPO</h3>
<pre><code class="language-python"># é€£ç¶šè¡Œå‹•ç’°å¢ƒ
class ContinuousGymMaterialsEnv(gym.Env):
    &quot;&quot;&quot;é€£ç¶šè¡Œå‹•ã®ææ–™æ¢ç´¢ç’°å¢ƒ&quot;&quot;&quot;
    def __init__(self):
        super(ContinuousGymMaterialsEnv, self).__init__()
        self.state_dim = 4
        self.target = np.array([3.0, 5.0, 2.5, 4.0])

        # é€£ç¶šè¡Œå‹•ç©ºé–“ï¼ˆ4æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã€ç¯„å›² [-1, 1]ï¼‰
        self.action_space = gym.spaces.Box(
            low=-1, high=1, shape=(self.state_dim,), dtype=np.float32
        )
        self.observation_space = gym.spaces.Box(
            low=0, high=10, shape=(self.state_dim,), dtype=np.float32
        )

        self.state = None

    def reset(self):
        self.state = np.random.uniform(0, 10, self.state_dim).astype(np.float32)
        return self.state

    def step(self, action):
        # è¡Œå‹•ã‚’çŠ¶æ…‹å¤‰åŒ–ã«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆ-1ã€œ1 â†’ -0.5ã€œ0.5ï¼‰
        delta = action * 0.5
        self.state = np.clip(self.state + delta, 0, 10)

        distance = np.linalg.norm(self.state - self.target)
        reward = -distance
        done = distance &lt; 0.3

        return self.state, reward, done, {}

    def render(self, mode='human'):
        pass


# é€£ç¶šè¡Œå‹•ç‰ˆPPO
env_continuous = DummyVecEnv([lambda: ContinuousGymMaterialsEnv()])

model_continuous = PPO(
    &quot;MlpPolicy&quot;,
    env_continuous,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    clip_range=0.2,
    verbose=1
)

model_continuous.learn(total_timesteps=100000)

# è©•ä¾¡
eval_env_cont = ContinuousGymMaterialsEnv()
state = eval_env_cont.reset()

for _ in range(50):
    action, _ = model_continuous.predict(state, deterministic=True)
    state, reward, done, _ = eval_env_cont.step(action)

    if done:
        break

print(f&quot;æœ€çµ‚çŠ¶æ…‹: {state}&quot;)
print(f&quot;ç›®æ¨™: {eval_env_cont.target}&quot;)
print(f&quot;è·é›¢: {np.linalg.norm(state - eval_env_cont.target):.4f}&quot;)
</code></pre>
<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>æœ€çµ‚çŠ¶æ…‹: [3.001 5.003 2.498 3.997]
ç›®æ¨™: [3.  5.  2.5 4. ]
è·é›¢: 0.0054
</code></pre>
<p><strong>è§£èª¬</strong>: é€£ç¶šè¡Œå‹•ã«ã‚ˆã‚Šã€ç›®æ¨™ã¸ã®<strong>ç²¾å¯†ãªåˆ¶å¾¡</strong>ãŒå¯èƒ½</p>
<hr />
<h2>æ¼”ç¿’å•é¡Œ</h2>
<h3>å•é¡Œ1 (é›£æ˜“åº¦: easy)</h3>
<p>ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä½¿ã†ã¨åˆ†æ•£ãŒæ¸›ã‚‹ç†ç”±ã‚’ã€ä»¥ä¸‹ã®å¼ã‚’ä½¿ã£ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>
<p>$$
\text{Var}[R_t] \quad \text{vs.} \quad \text{Var}[R_t - b(s_t)]
$$</p>
<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

åˆ†æ•£ã®æ€§è³ª: $\text{Var}[X - c] = \text{Var}[X]$ï¼ˆå®šæ•°$c$ã‚’å¼•ã„ã¦ã‚‚åˆ†æ•£ã¯å¤‰ã‚ã‚‰ãªã„ï¼‰ã§ã™ãŒã€$b(s\_t)$ã¯çŠ¶æ…‹ä¾å­˜ãªã®ã§å®šæ•°ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³$b(s\_t)$ãŒçŠ¶æ…‹ä¾¡å€¤$V(s\_t)$ã«è¿‘ã„ã¨ãï¼š

- **ãƒªã‚¿ãƒ¼ãƒ³** $R\_t$ã¯çŠ¶æ…‹ã«ã‚ˆã£ã¦å¤§ããå¤‰å‹•ï¼ˆé‹ã«ã‚ˆã‚‹å½±éŸ¿å¤§ï¼‰
- **ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸** $R\_t - V(s\_t)$ã¯ã€Œå¹³å‡ã‹ã‚‰ã®ã‚ºãƒ¬ã€ãªã®ã§å¤‰å‹•ãŒå°ã•ã„

æ•°å­¦çš„ã«ã¯ï¼š
$$
\text{Var}[R\_t - V(s\_t)] \leq \text{Var}[R\_t]
$$

ã“ã‚Œã¯$V(s\_t)$ãŒã€ŒçŠ¶æ…‹$s\_t$ã‹ã‚‰ã®æœŸå¾…ç´¯ç©å ±é…¬ã€ãªã®ã§ã€é‹ã®å½±éŸ¿ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã™ã‚‹ãŸã‚ã§ã™ã€‚

**å…·ä½“ä¾‹**:
- çŠ¶æ…‹Aã‹ã‚‰ã®ãƒªã‚¿ãƒ¼ãƒ³: 100, 105, 95 â†’ åˆ†æ•£ = 25
- çŠ¶æ…‹Aã®ä¾¡å€¤: 100
- ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸: 0, 5, -5 â†’ åˆ†æ•£ = 25ï¼ˆåŒã˜ï¼‰

ã—ã‹ã—ã€è¤‡æ•°ã®çŠ¶æ…‹ã‚’è€ƒãˆã‚‹ã¨ï¼š
- çŠ¶æ…‹Aã®ãƒªã‚¿ãƒ¼ãƒ³: 100Â±5
- çŠ¶æ…‹Bã®ãƒªã‚¿ãƒ¼ãƒ³: 50Â±5
- å…¨ä½“ã®åˆ†æ•£: å¤§ãã„

ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã§çŠ¶æ…‹ã”ã¨ã®å¹³å‡ã‚’å¼•ãã¨ã€çŠ¶æ…‹é–“ã®å·®ãŒæ¶ˆãˆã€åˆ†æ•£ãŒæ¸›ã‚Šã¾ã™ã€‚

</details>

<hr />
<h3>å•é¡Œ2 (é›£æ˜“åº¦: medium)</h3>
<p>PPOã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç¯„å›²$\epsilon$ã‚’å¤§ããã™ã‚‹ã¨ä½•ãŒèµ·ã“ã‚‹ã‹ã€ã¾ãŸ$\epsilon=0$ã®æ¥µç«¯ãªã‚±ãƒ¼ã‚¹ã§ã¯ã©ã†ãªã‚‹ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>
<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å¼ã‚’è¦‹ç›´ã—ã€$r\_t(\theta)$ã®å¤‰åŒ–ãŒã©ã†åˆ¶é™ã•ã‚Œã‚‹ã‹è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

**$\epsilon$ã‚’å¤§ããã™ã‚‹ã¨**:
- ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç¯„å›²ãŒåºƒãŒã‚Šã€æ–¹ç­–ã®å¤‰åŒ–ãŒå¤§ãããªã‚‹
- å­¦ç¿’ãŒé€Ÿã„ãŒä¸å®‰å®šã«ãªã‚Šã‚„ã™ã„
- æ¥µç«¯ãªå ´åˆã€æ–¹ç­–ãŒå´©å£Šã™ã‚‹å¯èƒ½æ€§

**$\epsilon=0$ã®å ´åˆ**:
$$
\text{clip}(r\_t, 1, 1) = 1
$$

- é‡è¦åº¦æ¯”ç‡ãŒå¸¸ã«1ã«ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
- æ–¹ç­–ãŒå…¨ãæ›´æ–°ã•ã‚Œãªã„ï¼ˆ$\pi\_\theta = \pi\_{\theta\_{\text{old}}}$ã‚’å¼·åˆ¶ï¼‰

**å®Ÿè·µçš„ãªå€¤**: $\epsilon = 0.1 \sim 0.2$ãŒä¸€èˆ¬çš„

**å®Ÿé¨“ã‚³ãƒ¼ãƒ‰**:

<pre><code class="language-python"># Îµ=0.05ï¼ˆå³ã—ã„åˆ¶ç´„ï¼‰
model_tight = PPO(&quot;MlpPolicy&quot;, env, clip_range=0.05)

# Îµ=0.5ï¼ˆç·©ã„åˆ¶ç´„ï¼‰
model_loose = PPO(&quot;MlpPolicy&quot;, env, clip_range=0.5)

# å­¦ç¿’æ›²ç·šã‚’æ¯”è¼ƒ
# â†’ model_tightã¯å®‰å®šã ãŒé…ã„
# â†’ model_looseã¯é€Ÿã„ãŒæŒ¯å‹•ã™ã‚‹
</code></pre>


</details>

<hr />
<h3>å•é¡Œ3 (é›£æ˜“åº¦: hard)</h3>
<p>ææ–™æ¢ç´¢ã«ãŠã„ã¦ã€ä»¥ä¸‹ã®2ã¤ã®å ±é…¬è¨­è¨ˆã‚’æ¯”è¼ƒã—ã€ãã‚Œãã‚Œã®é•·æ‰€ãƒ»çŸ­æ‰€ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚ã¾ãŸã€å®Ÿéš›ã«ã‚³ãƒ¼ãƒ‰ã§å®Ÿé¨“ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>å ±é…¬Aï¼ˆç–å ±é…¬ï¼‰</strong>: ç›®æ¨™ã«åˆ°é”ã—ãŸã¨ãã®ã¿å ±é…¬1ã€ãã‚Œä»¥å¤–ã¯0
<strong>å ±é…¬Bï¼ˆå¯†å ±é…¬ï¼‰</strong>: ç›®æ¨™ã¨ã®è·é›¢ã«å¿œã˜ãŸé€£ç¶šçš„ãªå ±é…¬</p>
<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

ç–å ±é…¬ã¯æ¢ç´¢ãŒå›°é›£ã§ã™ãŒã€å¯†å ±é…¬ã¯å±€æ‰€æœ€é©è§£ã«é™¥ã‚Šã‚„ã™ã„ã§ã™ã€‚ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒœãƒ¼ãƒŠã‚¹ã®å½±éŸ¿ã‚‚è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

**å ±é…¬Aï¼ˆç–å ±é…¬ï¼‰ã®é•·æ‰€ãƒ»çŸ­æ‰€**:

**é•·æ‰€**:
- æ˜ç¢ºãªç›®æ¨™ï¼ˆæ›–æ˜§ã•ãŒãªã„ï¼‰
- å±€æ‰€æœ€é©è§£ã«é™¥ã‚Šã«ãã„ï¼ˆä¸­é–“å ±é…¬ã«æƒ‘ã‚ã•ã‚Œãªã„ï¼‰

**çŸ­æ‰€**:
- æ¢ç´¢ãŒéå¸¸ã«å›°é›£ï¼ˆå­¦ç¿’ã‚·ã‚°ãƒŠãƒ«ãŒå¼±ã„ï¼‰
- å­¦ç¿’ã«æ™‚é–“ãŒã‹ã‹ã‚‹

**å ±é…¬Bï¼ˆå¯†å ±é…¬ï¼‰ã®é•·æ‰€ãƒ»çŸ­æ‰€**:

**é•·æ‰€**:
- æ¢ç´¢ãŒå®¹æ˜“ï¼ˆæ¯ã‚¹ãƒ†ãƒƒãƒ—ã§ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼‰
- å­¦ç¿’ãŒé€Ÿã„

**çŸ­æ‰€**:
- å ±é…¬è¨­è¨ˆãŒé›£ã—ã„ï¼ˆè·é›¢ã ã‘ã§ã¯ä¸ååˆ†ãªå ´åˆã‚‚ï¼‰
- å±€æ‰€æœ€é©è§£ã«é™¥ã‚Šã‚„ã™ã„

**å®Ÿé¨“ã‚³ãƒ¼ãƒ‰**:

<pre><code class="language-python"># å ±é…¬Aï¼ˆç–å ±é…¬ï¼‰
class SparseRewardEnv(gym.Env):
    def step(self, action):
        # ... (çŠ¶æ…‹æ›´æ–°) ...
        distance = np.linalg.norm(self.state - self.target)

        if distance &lt; 0.5:
            reward = 1.0  # åˆ°é”
            done = True
        else:
            reward = 0.0  # ãã‚Œä»¥å¤–
            done = False

        return self.state, reward, done, {}

# å ±é…¬Bï¼ˆå¯†å ±é…¬ï¼‰
class DenseRewardEnv(gym.Env):
    def step(self, action):
        # ... (çŠ¶æ…‹æ›´æ–°) ...
        distance = np.linalg.norm(self.state - self.target)
        reward = -distance  # é€£ç¶šçš„ãªå ±é…¬
        done = distance &lt; 0.5

        return self.state, reward, done, {}

# æ¯”è¼ƒå®Ÿé¨“
model_sparse = PPO(&quot;MlpPolicy&quot;, DummyVecEnv([lambda: SparseRewardEnv()]))
model_dense = PPO(&quot;MlpPolicy&quot;, DummyVecEnv([lambda: DenseRewardEnv()]))

model_sparse.learn(total_timesteps=100000)
model_dense.learn(total_timesteps=100000)

# çµæœ: model_denseã®æ–¹ãŒå­¦ç¿’ãŒé€Ÿã„ãŒã€
# è¤‡é›‘ãªç’°å¢ƒã§ã¯model_sparseã®æ–¹ãŒè‰¯ã„è§£ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ã‚‚
</code></pre>


**ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**: å¯†å ±é…¬ã‹ã‚‰å§‹ã‚ã€å•é¡Œã«å¿œã˜ã¦ç–å ±é…¬ã‚„**å ±é…¬ã‚·ã‚§ã‚¤ãƒ”ãƒ³ã‚°**ï¼ˆä¸­é–“å ±é…¬ã®è¿½åŠ ï¼‰ã‚’æ¤œè¨ã€‚

</details>

<hr />
<h2>ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã¾ã¨ã‚</h2>
<ul>
<li><strong>æ–¹ç­–å‹¾é…æ³•</strong>ã¯æ–¹ç­–ã‚’ç›´æ¥æœ€é©åŒ–ã—ã€é€£ç¶šè¡Œå‹•ã«å¯¾å¿œ</li>
<li><strong>REINFORCEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </strong>ã¯é«˜åˆ†æ•£ã ãŒã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã§æ”¹å–„</li>
<li><strong>Actor-Critic</strong>ã¯Actorã¨Criticã‚’åŒæ™‚å­¦ç¿’ã—ã€ä½åˆ†æ•£ãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’</li>
<li><strong>PPO</strong>ã¯ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚Šå®‰å®šã—ãŸå­¦ç¿’ã‚’å®Ÿç¾ã€æœ€å…ˆç«¯ã®å®Ÿç”¨çš„æ‰‹æ³•</li>
<li><strong>Stable Baselines3</strong>ã«ã‚ˆã‚Šã€ã‚ãšã‹æ•°è¡Œã§PPOã‚’å®Ÿè£…å¯èƒ½</li>
<li>é€£ç¶šè¡Œå‹•ç©ºé–“ã§ã¯<strong>ã‚¬ã‚¦ã‚¹æ–¹ç­–</strong>ã‚’ä½¿ç”¨</li>
</ul>
<p>æ¬¡ç« ã§ã¯ã€ææ–™æ¢ç´¢ã«ç‰¹åŒ–ã—ãŸã‚«ã‚¹ã‚¿ãƒ ç’°å¢ƒã®æ§‹ç¯‰ã¨å ±é…¬è¨­è¨ˆã‚’å­¦ã³ã¾ã™ã€‚</p>
<hr />
<h2>å“è³ªãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼šæ–¹ç­–å‹¾é…æ³•ã®å®Ÿè£…ç¢ºèª</h2>
<h3>ç†è«–ç†è§£ã‚¹ã‚­ãƒ«</h3>
<ul>
<li>[ ] æ–¹ç­–å‹¾é…å®šç†ã‚’æ•°å¼ã§èª¬æ˜ã§ãã‚‹</li>
<li>[ ] REINFORCEã®æ›´æ–°å¼ã‚’å°å‡ºã§ãã‚‹</li>
<li>[ ] ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãŒåˆ†æ•£ã‚’æ¸›ã‚‰ã™ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>[ ] PPOã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®å½¹å‰²ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
</ul>
<h3>å®Ÿè£…ã‚¹ã‚­ãƒ«</h3>
<ul>
<li>[ ] PyTorchã§æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆç´¯ç©å ±é…¬ï¼‰ã®è¨ˆç®—ãŒã§ãã‚‹</li>
<li>[ ] ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸é–¢æ•°ã®è¨ˆç®—ãŒã§ãã‚‹</li>
<li>[ ] Stable Baselines3ã§PPOã‚’ä½¿ç”¨ã§ãã‚‹</li>
</ul>
<h3>ææ–™æ¢ç´¢ã¸ã®å¿œç”¨</h3>
<ul>
<li>[ ] æ¸©åº¦ãƒ»åœ§åŠ›ãªã©ã®é€£ç¶šåˆ¶å¾¡å¤‰æ•°ã‚’è¡Œå‹•ç©ºé–“ã¨ã—ã¦è¨­è¨ˆã§ãã‚‹</li>
<li>[ ] å¤šç›®çš„å ±é…¬ï¼ˆåç‡ãƒ»é¸æŠæ€§ï¼‰ã‚’é©åˆ‡ã«é‡ã¿ä»˜ã‘ã§ãã‚‹</li>
<li>[ ] å®‰å…¨åˆ¶ç´„ï¼ˆæ¸©åº¦ä¸Šé™ãªã©ï¼‰ã‚’å ±é…¬ã«çµ„ã¿è¾¼ã‚ã‚‹</li>
</ul>
<h3>ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚­ãƒ«</h3>
<ul>
<li>[ ] æ–¹ç­–å‹¾é…ã®åˆ†æ•£ãŒå¤§ãã„å ´åˆã®å¯¾å‡¦æ³•ã‚’çŸ¥ã£ã¦ã„ã‚‹</li>
<li>[ ] PPOãŒåæŸã—ãªã„å ´åˆã®åŸå› ã‚’ç‰¹å®šã§ãã‚‹</li>
<li>[ ] ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒœãƒ¼ãƒŠã‚¹ã®èª¿æ•´ãŒã§ãã‚‹</li>
</ul>
<hr />
<h2>å‚è€ƒæ–‡çŒ®</h2>
<ol>
<li>Williams "Simple statistical gradient-following algorithms for connectionist reinforcement learning" <em>Machine Learning</em> (1992) - REINFORCE</li>
<li>Mnih et al. "Asynchronous methods for deep reinforcement learning" <em>ICML</em> (2016) - A3C/A2C</li>
<li>Schulman et al. "Proximal policy optimization algorithms" <em>arXiv</em> (2017) - PPO</li>
<li>Schulman et al. "Trust region policy optimization" <em>ICML</em> (2015) - TRPO</li>
<li>Raffin et al. "Stable-Baselines3: Reliable reinforcement learning implementations" <em>JMLR</em> (2021)</li>
</ol>
<hr />
<p><strong>æ¬¡ç« </strong>: <a href="chapter-3.html">ç¬¬3ç« : ææ–™æ¢ç´¢ç’°å¢ƒã®æ§‹ç¯‰</a></p><div class="navigation">
    <a href="chapter-1.html" class="nav-button">â† å‰ã®ç« </a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter-3.html" class="nav-button">æ¬¡ã®ç«  â†’</a>
</div>
    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimotoï¼ˆæ±åŒ—å¤§å­¦ï¼‰</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 2.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
