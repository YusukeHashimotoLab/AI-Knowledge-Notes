---
title: "ç¬¬2ç« : å¼·åŒ–å­¦ç¿’ã®åŸºç¤ç†è«–"
chapter_title: "ç¬¬2ç« : å¼·åŒ–å­¦ç¿’ã®åŸºç¤ç†è«–"
subtitle: 
reading_time: 20-25åˆ†
difficulty: åˆç´š
code_examples: 8
exercises: 3
---

# ç¬¬2ç« : å¼·åŒ–å­¦ç¿’ã®åŸºç¤ç†è«–

Qå­¦ç¿’/DQN/PPOãªã©ä»£è¡¨æ‰‹æ³•ã®ç›´è¦³ã¨é•ã„ã‚’æ•´ç†ã—ã¾ã™ã€‚ã©ã®èª²é¡Œã«ã©ã‚Œã‚’è©¦ã™ã‹ã®å½“ãŸã‚Šã‚’ä»˜ã‘ã¾ã™ã€‚

**ğŸ’¡ è£œè¶³:** é€£ç¶šåˆ¶å¾¡ã¯PPOãªã©ã®ãƒãƒªã‚·ãƒ¼å‹¾é…ç³»ãŒç›¸æ€§è‰¯ã€‚é›¢æ•£é¸æŠãªã‚‰Qç³»ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ã€‚

## å­¦ç¿’ç›®æ¨™

ã“ã®ç« ã§ã¯ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã—ã¾ã™ï¼š

  * æ–¹ç­–å‹¾é…æ³•ï¼ˆPolicy Gradient Methodsï¼‰ã®ç†è«–ã¨å®Ÿè£…
  * Actor-Criticã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ä»•çµ„ã¿
  * Proximal Policy Optimizationï¼ˆPPOï¼‰ã®è©³ç´°
  * Stable Baselines3ã«ã‚ˆã‚‹å®Ÿè·µçš„å®Ÿè£…

* * *

## 2.1 æ–¹ç­–å‹¾é…æ³•ï¼ˆPolicy Gradient Methodsï¼‰

### Qå­¦ç¿’ã®é™ç•Œ

ç¬¬1ç« ã®Qå­¦ç¿’ãƒ»DQNã¯**ä¾¡å€¤ãƒ™ãƒ¼ã‚¹** ã®æ‰‹æ³•ã§ã—ãŸã€‚ã“ã‚Œã‚‰ã«ã¯ä»¥ä¸‹ã®é™ç•ŒãŒã‚ã‚Šã¾ã™ï¼š

  1. **é›¢æ•£è¡Œå‹•ã®ã¿** : $\arg\max_a Q(s,a)$ã¯é€£ç¶šè¡Œå‹•ç©ºé–“ã§å›°é›£
  2. **æ±ºå®šçš„æ–¹ç­–** : å¸¸ã«åŒã˜è¡Œå‹•ã‚’é¸æŠï¼ˆç¢ºç‡çš„æ–¹ç­–ãŒå­¦ç¿’ã§ããªã„ï¼‰
  3. **å°ã•ãªå¤‰åŒ–ã«è„†å¼±** : Qå€¤ã®å¾®å°ãªå¤‰åŒ–ã§æ–¹ç­–ãŒå¤§ããå¤‰ã‚ã‚‹

ææ–™ç§‘å­¦ã§ã¯ã€**é€£ç¶šçš„ãªåˆ¶å¾¡** ï¼ˆæ¸©åº¦ã‚’0.5åº¦ä¸Šã’ã‚‹ã€çµ„æˆæ¯”ã‚’2%å¤‰ãˆã‚‹ï¼‰ãŒé‡è¦ã§ã™ã€‚

### æ–¹ç­–å‹¾é…æ³•ã®åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢

æ–¹ç­–å‹¾é…æ³•ã¯ã€**æ–¹ç­–ã‚’ç›´æ¥æœ€é©åŒ–** ã—ã¾ã™ï¼š

$$ \pi_\theta(a|s) = P(a|s; \theta) $$

  * $\theta$: æ–¹ç­–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡ã¿ï¼‰

**ç›®çš„** : æœŸå¾…ç´¯ç©å ±é…¬$J(\theta)$ã‚’æœ€å¤§åŒ–

$$ J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T r_t \right] $$

  * $\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots)$: è»Œè·¡ï¼ˆtrajectoryï¼‰

### æ–¹ç­–å‹¾é…å®šç†

**REINFORCE** ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆWilliams, 1992ï¼‰ã¯ã€å‹¾é…ã‚’ä»¥ä¸‹ã§è¨ˆç®—ã—ã¾ã™ï¼š

$$ \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot R_t \right] $$

  * $R_t = \sum_{k=t}^T \gamma^{k-t} r_k$: æ™‚åˆ»$t$ã‹ã‚‰ã®ç´¯ç©å ±é…¬ï¼ˆãƒªã‚¿ãƒ¼ãƒ³ï¼‰

**ç›´æ„Ÿçš„æ„å‘³** : \- é«˜ã„å ±é…¬ã‚’å¾—ãŸè¡Œå‹•ã®ç¢ºç‡ã‚’ä¸Šã’ã‚‹ \- ä½ã„å ±é…¬ã‚’å¾—ãŸè¡Œå‹•ã®ç¢ºç‡ã‚’ä¸‹ã’ã‚‹

### REINFORCEã®å®Ÿè£…
    
    
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import numpy as np
    import matplotlib.pyplot as plt
    
    class PolicyNetwork(nn.Module):
        """æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    
        çŠ¶æ…‹ã‚’å…¥åŠ›ã—ã€å„è¡Œå‹•ã®ç¢ºç‡ã‚’å‡ºåŠ›
        """
        def __init__(self, state_dim, action_dim, hidden_dim=64):
            super(PolicyNetwork, self).__init__()
            self.fc1 = nn.Linear(state_dim, hidden_dim)
            self.fc2 = nn.Linear(hidden_dim, hidden_dim)
            self.fc3 = nn.Linear(hidden_dim, action_dim)
    
        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = torch.relu(self.fc2(x))
            return torch.softmax(self.fc3(x), dim=-1)  # ç¢ºç‡åˆ†å¸ƒ
    
    
    class REINFORCEAgent:
        """REINFORCEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ """
        def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):
            self.gamma = gamma
            self.policy = PolicyNetwork(state_dim, action_dim)
            self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)
    
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†…ã®ãƒ­ã‚°ã‚’ä¿å­˜
            self.log_probs = []
            self.rewards = []
    
        def select_action(self, state):
            """æ–¹ç­–ã«å¾“ã£ã¦è¡Œå‹•ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            probs = self.policy(state_tensor)
    
            # ç¢ºç‡åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            action_dist = torch.distributions.Categorical(probs)
            action = action_dist.sample()
    
            # logç¢ºç‡ã‚’ä¿å­˜ï¼ˆå‹¾é…è¨ˆç®—ç”¨ï¼‰
            self.log_probs.append(action_dist.log_prob(action))
    
            return action.item()
    
        def store_reward(self, reward):
            """å ±é…¬ã‚’ä¿å­˜"""
            self.rewards.append(reward)
    
        def update(self):
            """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å¾Œã«æ–¹ç­–ã‚’æ›´æ–°"""
            # ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆç´¯ç©å ±é…¬ï¼‰ã‚’è¨ˆç®—
            returns = []
            R = 0
            for r in reversed(self.rewards):
                R = r + self.gamma * R
                returns.insert(0, R)
    
            returns = torch.FloatTensor(returns)
    
            # æ­£è¦åŒ–ï¼ˆå­¦ç¿’ã‚’å®‰å®šåŒ–ï¼‰
            returns = (returns - returns.mean()) / (returns.std() + 1e-8)
    
            # æ–¹ç­–å‹¾é…
            policy_loss = []
            for log_prob, R in zip(self.log_probs, returns):
                policy_loss.append(-log_prob * R)
    
            # å‹¾é…é™ä¸‹
            self.optimizer.zero_grad()
            loss = torch.stack(policy_loss).sum()
            loss.backward()
            self.optimizer.step()
    
            # ãƒ­ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
            self.log_probs = []
            self.rewards = []
    
    
    # ç°¡å˜ãªææ–™æ¢ç´¢ç’°å¢ƒï¼ˆé›¢æ•£è¡Œå‹•ç‰ˆï¼‰
    class DiscreteMaterialsEnv:
        """é›¢æ•£è¡Œå‹•ã®ææ–™æ¢ç´¢ç’°å¢ƒ"""
        def __init__(self, state_dim=4):
            self.state_dim = state_dim
            self.target = np.array([3.0, 5.0, 2.5, 4.0])
            self.state = None
    
        def reset(self):
            self.state = np.random.uniform(0, 10, self.state_dim)
            return self.state
    
        def step(self, action):
            # è¡Œå‹•: 0=æ¬¡å…ƒ0å¢—åŠ , 1=æ¬¡å…ƒ0æ¸›å°‘, 2=æ¬¡å…ƒ1å¢—åŠ , 3=æ¬¡å…ƒ1æ¸›å°‘
            dim = action // 2
            delta = 0.5 if action % 2 == 0 else -0.5
    
            self.state[dim] = np.clip(self.state[dim] + delta, 0, 10)
    
            # å ±é…¬: ç›®æ¨™ã¨ã®è·é›¢
            distance = np.linalg.norm(self.state - self.target)
            reward = -distance
    
            done = distance < 0.5
    
            return self.state, reward, done
    
    
    # REINFORCEã®è¨“ç·´
    env = DiscreteMaterialsEnv()
    agent = REINFORCEAgent(state_dim=4, action_dim=4)
    
    episodes = 1000
    rewards_history = []
    
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False
    
        while not done:
            action = agent.select_action(state)
            next_state, reward, done = env.step(action)
    
            agent.store_reward(reward)
            state = next_state
            total_reward += reward
    
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å¾Œã«æ›´æ–°
        agent.update()
        rewards_history.append(total_reward)
    
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(rewards_history[-100:])
            print(f"Episode {episode+1}: Avg Reward = {avg_reward:.2f}")
    
    # å­¦ç¿’æ›²ç·š
    plt.figure(figsize=(10, 6))
    plt.plot(np.convolve(rewards_history, np.ones(20)/20, mode='valid'))
    plt.xlabel('Episode')
    plt.ylabel('Average Reward (20 episodes)')
    plt.title('REINFORCE: æ–¹ç­–å‹¾é…æ³•ã«ã‚ˆã‚‹ææ–™æ¢ç´¢')
    plt.grid(True)
    plt.show()
    

**å‡ºåŠ›ä¾‹** :
    
    
    Episode 100: Avg Reward = -38.24
    Episode 200: Avg Reward = -28.15
    Episode 500: Avg Reward = -15.32
    Episode 1000: Avg Reward = -7.89
    

* * *

## 2.2 ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨åˆ†æ•£å‰Šæ¸›

### REINFORCEã®å•é¡Œç‚¹

REINFORCEã¯**é«˜åˆ†æ•£** ï¼ˆhigh varianceï¼‰ã§ã™ã€‚åŒã˜æ–¹ç­–ã§ã‚‚ã€é‹ãŒè‰¯ã„ã‹æ‚ªã„ã‹ã§ãƒªã‚¿ãƒ¼ãƒ³$R_t$ãŒå¤§ããå¤‰å‹•ã—ã¾ã™ã€‚

### ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®å°å…¥

**ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³** $b(s)$ã‚’å¼•ãã“ã¨ã§ã€åˆ†æ•£ã‚’å‰Šæ¸›ï¼š

$$ \nabla_\theta J(\theta) = \mathbb{E} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot (R_t - b(s_t)) \right] $$

**æœ€é©ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³** : çŠ¶æ…‹ä¾¡å€¤é–¢æ•°$V(s)$

$$ b(s_t) = V(s_t) = \mathbb{E}_{\pi} \left[ \sum_{k=t}^T \gamma^{k-t} r_k \mid s_t \right] $$

**ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸é–¢æ•°** $A(s, a)$: $$ A(s, a) = Q(s, a) - V(s) = R_t - V(s_t) $$

ã€Œã“ã®è¡Œå‹•ã¯å¹³å‡ã‚ˆã‚Šã©ã‚Œã ã‘è‰¯ã„ã‹ã€ã‚’è¡¨ã—ã¾ã™ã€‚

### ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä»˜ãREINFORCE
    
    
    class ValueNetwork(nn.Module):
        """ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰"""
        def __init__(self, state_dim, hidden_dim=64):
            super(ValueNetwork, self).__init__()
            self.fc1 = nn.Linear(state_dim, hidden_dim)
            self.fc2 = nn.Linear(hidden_dim, hidden_dim)
            self.fc3 = nn.Linear(hidden_dim, 1)  # çŠ¶æ…‹ä¾¡å€¤ã‚’å‡ºåŠ›
    
        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = torch.relu(self.fc2(x))
            return self.fc3(x)
    
    
    class REINFORCEWithBaseline:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä»˜ãREINFORCE"""
        def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):
            self.gamma = gamma
            self.policy = PolicyNetwork(state_dim, action_dim)
            self.value = ValueNetwork(state_dim)
    
            self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)
            self.value_optimizer = optim.Adam(self.value.parameters(), lr=lr)
    
            self.log_probs = []
            self.rewards = []
            self.states = []
    
        def select_action(self, state):
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            probs = self.policy(state_tensor)
    
            action_dist = torch.distributions.Categorical(probs)
            action = action_dist.sample()
    
            self.log_probs.append(action_dist.log_prob(action))
            self.states.append(state)
    
            return action.item()
    
        def store_reward(self, reward):
            self.rewards.append(reward)
    
        def update(self):
            # ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—
            returns = []
            R = 0
            for r in reversed(self.rewards):
                R = r + self.gamma * R
                returns.insert(0, R)
    
            returns = torch.FloatTensor(returns)
            states = torch.FloatTensor(self.states)
    
            # ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‡ºåŠ›ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
            values = self.value(states).squeeze()
    
            # ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ = ãƒªã‚¿ãƒ¼ãƒ³ - ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
            advantages = returns - values.detach()
    
            # æ–¹ç­–å‹¾é…æå¤±
            policy_loss = []
            for log_prob, adv in zip(self.log_probs, advantages):
                policy_loss.append(-log_prob * adv)
    
            # ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æå¤±ï¼ˆMSEï¼‰
            value_loss = nn.MSELoss()(values, returns)
    
            # æœ€é©åŒ–
            self.policy_optimizer.zero_grad()
            torch.stack(policy_loss).sum().backward()
            self.policy_optimizer.step()
    
            self.value_optimizer.zero_grad()
            value_loss.backward()
            self.value_optimizer.step()
    
            # ãƒªã‚»ãƒƒãƒˆ
            self.log_probs = []
            self.rewards = []
            self.states = []
    
    
    # è¨“ç·´ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä»˜ãï¼‰
    agent_baseline = REINFORCEWithBaseline(state_dim=4, action_dim=4)
    
    rewards_baseline = []
    for episode in range(1000):
        state = env.reset()
        total_reward = 0
        done = False
    
        while not done:
            action = agent_baseline.select_action(state)
            next_state, reward, done = env.step(action)
    
            agent_baseline.store_reward(reward)
            state = next_state
            total_reward += reward
    
        agent_baseline.update()
        rewards_baseline.append(total_reward)
    
    # æ¯”è¼ƒ
    plt.figure(figsize=(10, 6))
    plt.plot(np.convolve(rewards_history, np.ones(20)/20, mode='valid'), label='REINFORCE')
    plt.plot(np.convolve(rewards_baseline, np.ones(20)/20, mode='valid'), label='REINFORCE + Baseline')
    plt.xlabel('Episode')
    plt.ylabel('Average Reward')
    plt.title('ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«ã‚ˆã‚‹å­¦ç¿’å®‰å®šåŒ–')
    plt.legend()
    plt.grid(True)
    plt.show()
    

**çµæœ** : ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«ã‚ˆã‚Šå­¦ç¿’ãŒ**ã‚ˆã‚Šå®‰å®š** ã—ã€åæŸãŒ**é€Ÿã** ãªã‚Šã¾ã™ã€‚

* * *

## 2.3 Actor-Criticã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### Actor-Criticã®æ¦‚å¿µ

**Actorï¼ˆæ–¹ç­–ï¼‰** ã¨ **Criticï¼ˆä¾¡å€¤é–¢æ•°ï¼‰** ã‚’åŒæ™‚ã«å­¦ç¿’ï¼š

  * **Actor** $\pi_\theta(a|s)$: è¡Œå‹•ã‚’é¸æŠ
  * **Critic** $V_\phi(s)$: çŠ¶æ…‹ã®ä¾¡å€¤ã‚’è©•ä¾¡

    
    
    ```mermaid
    flowchart LR
        S[çŠ¶æ…‹ s] --> A[Actor: Ï€Î¸]
        S --> C[Critic: VÏ•]
        A -->|è¡Œå‹• a| E[ç’°å¢ƒ]
        E -->|å ±é…¬ r| C
        C -->|TDèª¤å·®| A
        C -->|ä¾¡å€¤è©•ä¾¡| C
    
        style A fill:#e1f5ff
        style C fill:#ffe1cc
    ```

### TDèª¤å·®ã¨ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸

**TDèª¤å·®** ï¼ˆTemporal Difference Errorï¼‰: $$ \delta_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t) $$

ã“ã‚Œã¯**1ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸æ¨å®š** ã¨ã—ã¦ä½¿ãˆã¾ã™ã€‚

### A2Cï¼ˆAdvantage Actor-Criticï¼‰
    
    
    class A2CAgent:
        """Advantage Actor-Critic"""
        def __init__(self, state_dim, action_dim, lr_actor=1e-3, lr_critic=1e-3, gamma=0.99):
            self.gamma = gamma
    
            self.actor = PolicyNetwork(state_dim, action_dim)
            self.critic = ValueNetwork(state_dim)
    
            self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)
            self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)
    
        def select_action(self, state):
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            probs = self.actor(state_tensor)
    
            action_dist = torch.distributions.Categorical(probs)
            action = action_dist.sample()
    
            return action.item(), action_dist.log_prob(action)
    
        def update(self, state, action_log_prob, reward, next_state, done):
            """1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«æ›´æ–°"""
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)
    
            # ç¾åœ¨ã¨æ¬¡ã®çŠ¶æ…‹ä¾¡å€¤
            value = self.critic(state_tensor)
            next_value = self.critic(next_state_tensor)
    
            # TDç›®æ¨™ã¨TDèª¤å·®
            td_target = reward + (1 - done) * self.gamma * next_value.item()
            td_error = td_target - value.item()
    
            # Criticæå¤±ï¼ˆMSEï¼‰
            critic_loss = (torch.FloatTensor([td_target]) - value).pow(2)
    
            # Actoræå¤±ï¼ˆæ–¹ç­–å‹¾é… Ã— ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ï¼‰
            actor_loss = -action_log_prob * td_error
    
            # æœ€é©åŒ–
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            self.actor_optimizer.step()
    
            self.critic_optimizer.zero_grad()
            critic_loss.backward()
            self.critic_optimizer.step()
    
    
    # A2Cè¨“ç·´
    agent_a2c = A2CAgent(state_dim=4, action_dim=4)
    
    rewards_a2c = []
    for episode in range(1000):
        state = env.reset()
        total_reward = 0
        done = False
    
        while not done:
            action, log_prob = agent_a2c.select_action(state)
            next_state, reward, done = env.step(action)
    
            # 1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«æ›´æ–°
            agent_a2c.update(state, log_prob, reward, next_state, done)
    
            state = next_state
            total_reward += reward
    
        rewards_a2c.append(total_reward)
    
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(rewards_a2c[-100:])
            print(f"Episode {episode+1}: Avg Reward = {avg_reward:.2f}")
    

**åˆ©ç‚¹** : \- ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ã‚’å¾…ãŸãšã«**ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’** \- TDèª¤å·®ã«ã‚ˆã‚Š**ä½åˆ†æ•£**

* * *

## 2.4 Proximal Policy Optimizationï¼ˆPPOï¼‰

### Trust Region Methods

æ–¹ç­–å‹¾é…æ³•ã§ã¯ã€**æ›´æ–°ãŒå¤§ãã™ãã‚‹ã¨æ–¹ç­–ãŒå´©å£Š** ã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

**Trust Region Policy Optimizationï¼ˆTRPOï¼‰** ã¯ã€æ–¹ç­–ã®å¤‰åŒ–ã‚’åˆ¶ç´„ï¼š

$$ \max_\theta \mathbb{E} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} A(s, a) \right] \quad \text{s.t.} \quad D_{\text{KL}}(\pi_{\theta_{\text{old}}} | \pi_\theta) \leq \delta $$

ã—ã‹ã—ã€KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹åˆ¶ç´„ã®æœ€é©åŒ–ã¯è¤‡é›‘ã§ã™ã€‚

### PPOã®ç°¡ç•¥åŒ–

**PPO** ï¼ˆSchulman et al., 2017ï¼‰ã¯ã€åˆ¶ç´„ã‚’**æå¤±é–¢æ•°å†…ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°** ã§å®Ÿç¾ï¼š

$$ L^{\text{CLIP}}(\theta) = \mathbb{E} \left[ \min \left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right] $$

  * $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$: é‡è¦åº¦æ¯”ç‡ï¼ˆimportance ratioï¼‰
  * $\epsilon$: ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç¯„å›²ï¼ˆé€šå¸¸0.1ã€œ0.2ï¼‰

**ç›´æ„Ÿ** : \- ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ãŒæ­£ï¼ˆè‰¯ã„è¡Œå‹•ï¼‰â†’ $r_t$ã‚’å¢—ã‚„ã™ãŒã€$1+\epsilon$ã§ä¸Šé™ \- ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ãŒè² ï¼ˆæ‚ªã„è¡Œå‹•ï¼‰â†’ $r_t$ã‚’æ¸›ã‚‰ã™ãŒã€$1-\epsilon$ã§ä¸‹é™ \- æ€¥æ¿€ãªæ–¹ç­–å¤‰åŒ–ã‚’é˜²ã

### ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒœãƒ¼ãƒŠã‚¹

æ¢ç´¢ã‚’ä¿ƒé€²ã™ã‚‹ãŸã‚ã€**ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼** ã‚’æå¤±ã«è¿½åŠ ï¼š

$$ L^{\text{PPO}}(\theta) = L^{\text{CLIP}}(\theta) + c_1 L^{\text{VF}}(\theta) - c_2 H[\pi_\theta] $$

  * $L^{\text{VF}}$: ä¾¡å€¤é–¢æ•°ã®æå¤±
  * $H[\pi_\theta] = -\sum_a \pi_\theta(a|s) \log \pi_\theta(a|s)$: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆç¢ºç‡åˆ†å¸ƒã®ä¸ç¢ºå®Ÿæ€§ï¼‰
  * $c_2$: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ä¿‚æ•°ï¼ˆé€šå¸¸0.01ï¼‰

### PPOã®å®Ÿè£…ï¼ˆStable Baselines3ä½¿ç”¨ï¼‰
    
    
    from stable_baselines3 import PPO
    from stable_baselines3.common.vec_env import DummyVecEnv
    import gym
    
    # Gymç’°å¢ƒãƒ©ãƒƒãƒ‘ãƒ¼
    class GymMaterialsEnv(gym.Env):
        """OpenAI Gymäº’æ›ã®ææ–™æ¢ç´¢ç’°å¢ƒ"""
        def __init__(self):
            super(GymMaterialsEnv, self).__init__()
            self.state_dim = 4
            self.target = np.array([3.0, 5.0, 2.5, 4.0])
    
            # è¡Œå‹•ãƒ»çŠ¶æ…‹ç©ºé–“ã®å®šç¾©
            self.action_space = gym.spaces.Discrete(4)
            self.observation_space = gym.spaces.Box(
                low=0, high=10, shape=(self.state_dim,), dtype=np.float32
            )
    
            self.state = None
    
        def reset(self):
            self.state = np.random.uniform(0, 10, self.state_dim).astype(np.float32)
            return self.state
    
        def step(self, action):
            dim = action // 2
            delta = 0.5 if action % 2 == 0 else -0.5
    
            self.state[dim] = np.clip(self.state[dim] + delta, 0, 10)
    
            distance = np.linalg.norm(self.state - self.target)
            reward = -distance
            done = distance < 0.5
    
            return self.state, reward, done, {}
    
        def render(self, mode='human'):
            pass
    
    
    # ç’°å¢ƒä½œæˆ
    env = DummyVecEnv([lambda: GymMaterialsEnv()])
    
    # PPOãƒ¢ãƒ‡ãƒ«
    model = PPO(
        "MlpPolicy",                # å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³æ–¹ç­–
        env,
        learning_rate=3e-4,
        n_steps=2048,               # æ›´æ–°å‰ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°
        batch_size=64,
        n_epochs=10,                # å„æ›´æ–°ã§ã®æœ€é©åŒ–ã‚¨ãƒãƒƒã‚¯æ•°
        gamma=0.99,
        gae_lambda=0.95,            # GAEï¼ˆGeneralized Advantage Estimationï¼‰
        clip_range=0.2,             # PPOã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç¯„å›²
        ent_coef=0.01,              # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ä¿‚æ•°
        verbose=1,
        tensorboard_log="./ppo_materials_tensorboard/"
    )
    
    # è¨“ç·´
    model.learn(total_timesteps=100000)
    
    # ä¿å­˜
    model.save("ppo_materials_agent")
    
    # è©•ä¾¡
    eval_env = GymMaterialsEnv()
    state = eval_env.reset()
    total_reward = 0
    
    for _ in range(100):
        action, _ = model.predict(state, deterministic=True)
        state, reward, done, _ = eval_env.step(action)
        total_reward += reward
    
        if done:
            break
    
    print(f"è©•ä¾¡çµæœ: Total Reward = {total_reward:.2f}")
    print(f"æœ€çµ‚çŠ¶æ…‹: {state}")
    print(f"ç›®æ¨™: {eval_env.target}")
    

**å‡ºåŠ›ä¾‹** :
    
    
    ---------------------------------
    | rollout/           |          |
    |    ep_len_mean     | 45.2     |
    |    ep_rew_mean     | -15.3    |
    | time/              |          |
    |    fps             | 1024     |
    |    iterations      | 50       |
    |    time_elapsed    | 97       |
    |    total_timesteps | 102400   |
    ---------------------------------
    
    è©•ä¾¡çµæœ: Total Reward = -5.23
    æœ€çµ‚çŠ¶æ…‹: [3.02 4.98 2.47 3.95]
    ç›®æ¨™: [3.  5.  2.5 4. ]
    

**è§£èª¬** : \- Stable Baselines3ã«ã‚ˆã‚Šã€ã‚ãšã‹æ•°è¡Œã§PPOã‚’å®Ÿè£… \- TensorBoardã§å­¦ç¿’é€²æ—ã‚’å¯è¦–åŒ–å¯èƒ½ \- ç›®æ¨™ã«éå¸¸ã«è¿‘ã„ææ–™ã‚’ç™ºè¦‹

* * *

## 2.5 é€£ç¶šè¡Œå‹•ç©ºé–“ã¸ã®æ‹¡å¼µ

### ã‚¬ã‚¦ã‚¹æ–¹ç­–

ææ–™ç§‘å­¦ã§ã¯ã€æ¸©åº¦ã‚„çµ„æˆæ¯”ãªã©**é€£ç¶šçš„ãªåˆ¶å¾¡** ãŒå¿…è¦ã§ã™ã€‚

é€£ç¶šè¡Œå‹•ã«ã¯**ã‚¬ã‚¦ã‚¹åˆ†å¸ƒæ–¹ç­–** ã‚’ä½¿ç”¨ï¼š

$$ \pi_\theta(a|s) = \mathcal{N}(\mu_\theta(s), \sigma_\theta(s)) $$

  * $\mu_\theta(s)$: å¹³å‡ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§å‡ºåŠ›ï¼‰
  * $\sigma_\theta(s)$: æ¨™æº–åå·®ï¼ˆå­¦ç¿’å¯èƒ½ã¾ãŸã¯å›ºå®šï¼‰

### é€£ç¶šè¡Œå‹•ç‰ˆPPO
    
    
    # é€£ç¶šè¡Œå‹•ç’°å¢ƒ
    class ContinuousGymMaterialsEnv(gym.Env):
        """é€£ç¶šè¡Œå‹•ã®ææ–™æ¢ç´¢ç’°å¢ƒ"""
        def __init__(self):
            super(ContinuousGymMaterialsEnv, self).__init__()
            self.state_dim = 4
            self.target = np.array([3.0, 5.0, 2.5, 4.0])
    
            # é€£ç¶šè¡Œå‹•ç©ºé–“ï¼ˆ4æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã€ç¯„å›² [-1, 1]ï¼‰
            self.action_space = gym.spaces.Box(
                low=-1, high=1, shape=(self.state_dim,), dtype=np.float32
            )
            self.observation_space = gym.spaces.Box(
                low=0, high=10, shape=(self.state_dim,), dtype=np.float32
            )
    
            self.state = None
    
        def reset(self):
            self.state = np.random.uniform(0, 10, self.state_dim).astype(np.float32)
            return self.state
    
        def step(self, action):
            # è¡Œå‹•ã‚’çŠ¶æ…‹å¤‰åŒ–ã«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆ-1ã€œ1 â†’ -0.5ã€œ0.5ï¼‰
            delta = action * 0.5
            self.state = np.clip(self.state + delta, 0, 10)
    
            distance = np.linalg.norm(self.state - self.target)
            reward = -distance
            done = distance < 0.3
    
            return self.state, reward, done, {}
    
        def render(self, mode='human'):
            pass
    
    
    # é€£ç¶šè¡Œå‹•ç‰ˆPPO
    env_continuous = DummyVecEnv([lambda: ContinuousGymMaterialsEnv()])
    
    model_continuous = PPO(
        "MlpPolicy",
        env_continuous,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        clip_range=0.2,
        verbose=1
    )
    
    model_continuous.learn(total_timesteps=100000)
    
    # è©•ä¾¡
    eval_env_cont = ContinuousGymMaterialsEnv()
    state = eval_env_cont.reset()
    
    for _ in range(50):
        action, _ = model_continuous.predict(state, deterministic=True)
        state, reward, done, _ = eval_env_cont.step(action)
    
        if done:
            break
    
    print(f"æœ€çµ‚çŠ¶æ…‹: {state}")
    print(f"ç›®æ¨™: {eval_env_cont.target}")
    print(f"è·é›¢: {np.linalg.norm(state - eval_env_cont.target):.4f}")
    

**å‡ºåŠ›ä¾‹** :
    
    
    æœ€çµ‚çŠ¶æ…‹: [3.001 5.003 2.498 3.997]
    ç›®æ¨™: [3.  5.  2.5 4. ]
    è·é›¢: 0.0054
    

**è§£èª¬** : é€£ç¶šè¡Œå‹•ã«ã‚ˆã‚Šã€ç›®æ¨™ã¸ã®**ç²¾å¯†ãªåˆ¶å¾¡** ãŒå¯èƒ½

* * *

## æ¼”ç¿’å•é¡Œ

### å•é¡Œ1 (é›£æ˜“åº¦: easy)

ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä½¿ã†ã¨åˆ†æ•£ãŒæ¸›ã‚‹ç†ç”±ã‚’ã€ä»¥ä¸‹ã®å¼ã‚’ä½¿ã£ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

$$ \text{Var}[R_t] \quad \text{vs.} \quad \text{Var}[R_t - b(s_t)] $$

ãƒ’ãƒ³ãƒˆ åˆ†æ•£ã®æ€§è³ª: $\text{Var}[X - c] = \text{Var}[X]$ï¼ˆå®šæ•°$c$ã‚’å¼•ã„ã¦ã‚‚åˆ†æ•£ã¯å¤‰ã‚ã‚‰ãªã„ï¼‰ã§ã™ãŒã€$b(s_t)$ã¯çŠ¶æ…‹ä¾å­˜ãªã®ã§å®šæ•°ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚  è§£ç­”ä¾‹ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³$b(s_t)$ãŒçŠ¶æ…‹ä¾¡å€¤$V(s_t)$ã«è¿‘ã„ã¨ãï¼š \- **ãƒªã‚¿ãƒ¼ãƒ³** $R_t$ã¯çŠ¶æ…‹ã«ã‚ˆã£ã¦å¤§ããå¤‰å‹•ï¼ˆé‹ã«ã‚ˆã‚‹å½±éŸ¿å¤§ï¼‰ \- **ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸** $R_t - V(s_t)$ã¯ã€Œå¹³å‡ã‹ã‚‰ã®ã‚ºãƒ¬ã€ãªã®ã§å¤‰å‹•ãŒå°ã•ã„ æ•°å­¦çš„ã«ã¯ï¼š $$ \text{Var}[R_t - V(s_t)] \leq \text{Var}[R_t] $$ ã“ã‚Œã¯$V(s_t)$ãŒã€ŒçŠ¶æ…‹$s_t$ã‹ã‚‰ã®æœŸå¾…ç´¯ç©å ±é…¬ã€ãªã®ã§ã€é‹ã®å½±éŸ¿ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã™ã‚‹ãŸã‚ã§ã™ã€‚ **å…·ä½“ä¾‹**: \- çŠ¶æ…‹Aã‹ã‚‰ã®ãƒªã‚¿ãƒ¼ãƒ³: 100, 105, 95 â†’ åˆ†æ•£ = 25 \- çŠ¶æ…‹Aã®ä¾¡å€¤: 100 \- ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸: 0, 5, -5 â†’ åˆ†æ•£ = 25ï¼ˆåŒã˜ï¼‰ ã—ã‹ã—ã€è¤‡æ•°ã®çŠ¶æ…‹ã‚’è€ƒãˆã‚‹ã¨ï¼š \- çŠ¶æ…‹Aã®ãƒªã‚¿ãƒ¼ãƒ³: 100Â±5 \- çŠ¶æ…‹Bã®ãƒªã‚¿ãƒ¼ãƒ³: 50Â±5 \- å…¨ä½“ã®åˆ†æ•£: å¤§ãã„ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã§çŠ¶æ…‹ã”ã¨ã®å¹³å‡ã‚’å¼•ãã¨ã€çŠ¶æ…‹é–“ã®å·®ãŒæ¶ˆãˆã€åˆ†æ•£ãŒæ¸›ã‚Šã¾ã™ã€‚ 

* * *

### å•é¡Œ2 (é›£æ˜“åº¦: medium)

PPOã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç¯„å›²$\epsilon$ã‚’å¤§ããã™ã‚‹ã¨ä½•ãŒèµ·ã“ã‚‹ã‹ã€ã¾ãŸ$\epsilon=0$ã®æ¥µç«¯ãªã‚±ãƒ¼ã‚¹ã§ã¯ã©ã†ãªã‚‹ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

ãƒ’ãƒ³ãƒˆ ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å¼ã‚’è¦‹ç›´ã—ã€$r_t(\theta)$ã®å¤‰åŒ–ãŒã©ã†åˆ¶é™ã•ã‚Œã‚‹ã‹è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚  è§£ç­”ä¾‹ **$\epsilon$ã‚’å¤§ããã™ã‚‹ã¨**: \- ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç¯„å›²ãŒåºƒãŒã‚Šã€æ–¹ç­–ã®å¤‰åŒ–ãŒå¤§ãããªã‚‹ \- å­¦ç¿’ãŒé€Ÿã„ãŒä¸å®‰å®šã«ãªã‚Šã‚„ã™ã„ \- æ¥µç«¯ãªå ´åˆã€æ–¹ç­–ãŒå´©å£Šã™ã‚‹å¯èƒ½æ€§ **$\epsilon=0$ã®å ´åˆ**: $$ \text{clip}(r_t, 1, 1) = 1 $$ \- é‡è¦åº¦æ¯”ç‡ãŒå¸¸ã«1ã«ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚° \- æ–¹ç­–ãŒå…¨ãæ›´æ–°ã•ã‚Œãªã„ï¼ˆ$\pi_\theta = \pi_{\theta_{\text{old}}}$ã‚’å¼·åˆ¶ï¼‰ **å®Ÿè·µçš„ãªå€¤**: $\epsilon = 0.1 \sim 0.2$ãŒä¸€èˆ¬çš„ **å®Ÿé¨“ã‚³ãƒ¼ãƒ‰**: 
    
    
    # Îµ=0.05ï¼ˆå³ã—ã„åˆ¶ç´„ï¼‰
    model_tight = PPO("MlpPolicy", env, clip_range=0.05)
    
    # Îµ=0.5ï¼ˆç·©ã„åˆ¶ç´„ï¼‰
    model_loose = PPO("MlpPolicy", env, clip_range=0.5)
    
    # å­¦ç¿’æ›²ç·šã‚’æ¯”è¼ƒ
    # â†’ model_tightã¯å®‰å®šã ãŒé…ã„
    # â†’ model_looseã¯é€Ÿã„ãŒæŒ¯å‹•ã™ã‚‹
    

* * *

### å•é¡Œ3 (é›£æ˜“åº¦: hard)

ææ–™æ¢ç´¢ã«ãŠã„ã¦ã€ä»¥ä¸‹ã®2ã¤ã®å ±é…¬è¨­è¨ˆã‚’æ¯”è¼ƒã—ã€ãã‚Œãã‚Œã®é•·æ‰€ãƒ»çŸ­æ‰€ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚ã¾ãŸã€å®Ÿéš›ã«ã‚³ãƒ¼ãƒ‰ã§å®Ÿé¨“ã—ã¦ãã ã•ã„ã€‚

**å ±é…¬Aï¼ˆç–å ±é…¬ï¼‰** : ç›®æ¨™ã«åˆ°é”ã—ãŸã¨ãã®ã¿å ±é…¬1ã€ãã‚Œä»¥å¤–ã¯0 **å ±é…¬Bï¼ˆå¯†å ±é…¬ï¼‰** : ç›®æ¨™ã¨ã®è·é›¢ã«å¿œã˜ãŸé€£ç¶šçš„ãªå ±é…¬

ãƒ’ãƒ³ãƒˆ ç–å ±é…¬ã¯æ¢ç´¢ãŒå›°é›£ã§ã™ãŒã€å¯†å ±é…¬ã¯å±€æ‰€æœ€é©è§£ã«é™¥ã‚Šã‚„ã™ã„ã§ã™ã€‚ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒœãƒ¼ãƒŠã‚¹ã®å½±éŸ¿ã‚‚è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚  è§£ç­”ä¾‹ **å ±é…¬Aï¼ˆç–å ±é…¬ï¼‰ã®é•·æ‰€ãƒ»çŸ­æ‰€**: **é•·æ‰€**: \- æ˜ç¢ºãªç›®æ¨™ï¼ˆæ›–æ˜§ã•ãŒãªã„ï¼‰ \- å±€æ‰€æœ€é©è§£ã«é™¥ã‚Šã«ãã„ï¼ˆä¸­é–“å ±é…¬ã«æƒ‘ã‚ã•ã‚Œãªã„ï¼‰ **çŸ­æ‰€**: \- æ¢ç´¢ãŒéå¸¸ã«å›°é›£ï¼ˆå­¦ç¿’ã‚·ã‚°ãƒŠãƒ«ãŒå¼±ã„ï¼‰ \- å­¦ç¿’ã«æ™‚é–“ãŒã‹ã‹ã‚‹ **å ±é…¬Bï¼ˆå¯†å ±é…¬ï¼‰ã®é•·æ‰€ãƒ»çŸ­æ‰€**: **é•·æ‰€**: \- æ¢ç´¢ãŒå®¹æ˜“ï¼ˆæ¯ã‚¹ãƒ†ãƒƒãƒ—ã§ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼‰ \- å­¦ç¿’ãŒé€Ÿã„ **çŸ­æ‰€**: \- å ±é…¬è¨­è¨ˆãŒé›£ã—ã„ï¼ˆè·é›¢ã ã‘ã§ã¯ä¸ååˆ†ãªå ´åˆã‚‚ï¼‰ \- å±€æ‰€æœ€é©è§£ã«é™¥ã‚Šã‚„ã™ã„ **å®Ÿé¨“ã‚³ãƒ¼ãƒ‰**: 
    
    
    # å ±é…¬Aï¼ˆç–å ±é…¬ï¼‰
    class SparseRewardEnv(gym.Env):
        def step(self, action):
            # ... (çŠ¶æ…‹æ›´æ–°) ...
            distance = np.linalg.norm(self.state - self.target)
    
            if distance < 0.5:
                reward = 1.0  # åˆ°é”
                done = True
            else:
                reward = 0.0  # ãã‚Œä»¥å¤–
                done = False
    
            return self.state, reward, done, {}
    
    # å ±é…¬Bï¼ˆå¯†å ±é…¬ï¼‰
    class DenseRewardEnv(gym.Env):
        def step(self, action):
            # ... (çŠ¶æ…‹æ›´æ–°) ...
            distance = np.linalg.norm(self.state - self.target)
            reward = -distance  # é€£ç¶šçš„ãªå ±é…¬
            done = distance < 0.5
    
            return self.state, reward, done, {}
    
    # æ¯”è¼ƒå®Ÿé¨“
    model_sparse = PPO("MlpPolicy", DummyVecEnv([lambda: SparseRewardEnv()]))
    model_dense = PPO("MlpPolicy", DummyVecEnv([lambda: DenseRewardEnv()]))
    
    model_sparse.learn(total_timesteps=100000)
    model_dense.learn(total_timesteps=100000)
    
    # çµæœ: model_denseã®æ–¹ãŒå­¦ç¿’ãŒé€Ÿã„ãŒã€
    # è¤‡é›‘ãªç’°å¢ƒã§ã¯model_sparseã®æ–¹ãŒè‰¯ã„è§£ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ã‚‚
    

**ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**: å¯†å ±é…¬ã‹ã‚‰å§‹ã‚ã€å•é¡Œã«å¿œã˜ã¦ç–å ±é…¬ã‚„**å ±é…¬ã‚·ã‚§ã‚¤ãƒ”ãƒ³ã‚°**ï¼ˆä¸­é–“å ±é…¬ã®è¿½åŠ ï¼‰ã‚’æ¤œè¨ã€‚ 

* * *

## ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã¾ã¨ã‚

  * **æ–¹ç­–å‹¾é…æ³•** ã¯æ–¹ç­–ã‚’ç›´æ¥æœ€é©åŒ–ã—ã€é€£ç¶šè¡Œå‹•ã«å¯¾å¿œ
  * **REINFORCEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ** ã¯é«˜åˆ†æ•£ã ãŒã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã§æ”¹å–„
  * **Actor-Critic** ã¯Actorã¨Criticã‚’åŒæ™‚å­¦ç¿’ã—ã€ä½åˆ†æ•£ãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’
  * **PPO** ã¯ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚Šå®‰å®šã—ãŸå­¦ç¿’ã‚’å®Ÿç¾ã€æœ€å…ˆç«¯ã®å®Ÿç”¨çš„æ‰‹æ³•
  * **Stable Baselines3** ã«ã‚ˆã‚Šã€ã‚ãšã‹æ•°è¡Œã§PPOã‚’å®Ÿè£…å¯èƒ½
  * é€£ç¶šè¡Œå‹•ç©ºé–“ã§ã¯**ã‚¬ã‚¦ã‚¹æ–¹ç­–** ã‚’ä½¿ç”¨

æ¬¡ç« ã§ã¯ã€ææ–™æ¢ç´¢ã«ç‰¹åŒ–ã—ãŸã‚«ã‚¹ã‚¿ãƒ ç’°å¢ƒã®æ§‹ç¯‰ã¨å ±é…¬è¨­è¨ˆã‚’å­¦ã³ã¾ã™ã€‚

* * *

## å“è³ªãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼šæ–¹ç­–å‹¾é…æ³•ã®å®Ÿè£…ç¢ºèª

### ç†è«–ç†è§£ã‚¹ã‚­ãƒ«

  * [ ] æ–¹ç­–å‹¾é…å®šç†ã‚’æ•°å¼ã§èª¬æ˜ã§ãã‚‹
  * [ ] REINFORCEã®æ›´æ–°å¼ã‚’å°å‡ºã§ãã‚‹
  * [ ] ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãŒåˆ†æ•£ã‚’æ¸›ã‚‰ã™ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
  * [ ] PPOã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®å½¹å‰²ã‚’ç†è§£ã—ã¦ã„ã‚‹

### å®Ÿè£…ã‚¹ã‚­ãƒ«

  * [ ] PyTorchã§æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å®Ÿè£…ã§ãã‚‹
  * [ ] ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆç´¯ç©å ±é…¬ï¼‰ã®è¨ˆç®—ãŒã§ãã‚‹
  * [ ] ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸é–¢æ•°ã®è¨ˆç®—ãŒã§ãã‚‹
  * [ ] Stable Baselines3ã§PPOã‚’ä½¿ç”¨ã§ãã‚‹

### ææ–™æ¢ç´¢ã¸ã®å¿œç”¨

  * [ ] æ¸©åº¦ãƒ»åœ§åŠ›ãªã©ã®é€£ç¶šåˆ¶å¾¡å¤‰æ•°ã‚’è¡Œå‹•ç©ºé–“ã¨ã—ã¦è¨­è¨ˆã§ãã‚‹
  * [ ] å¤šç›®çš„å ±é…¬ï¼ˆåç‡ãƒ»é¸æŠæ€§ï¼‰ã‚’é©åˆ‡ã«é‡ã¿ä»˜ã‘ã§ãã‚‹
  * [ ] å®‰å…¨åˆ¶ç´„ï¼ˆæ¸©åº¦ä¸Šé™ãªã©ï¼‰ã‚’å ±é…¬ã«çµ„ã¿è¾¼ã‚ã‚‹

### ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚­ãƒ«

  * [ ] æ–¹ç­–å‹¾é…ã®åˆ†æ•£ãŒå¤§ãã„å ´åˆã®å¯¾å‡¦æ³•ã‚’çŸ¥ã£ã¦ã„ã‚‹
  * [ ] PPOãŒåæŸã—ãªã„å ´åˆã®åŸå› ã‚’ç‰¹å®šã§ãã‚‹
  * [ ] ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒœãƒ¼ãƒŠã‚¹ã®èª¿æ•´ãŒã§ãã‚‹

* * *

## å‚è€ƒæ–‡çŒ®

  1. Williams "Simple statistical gradient-following algorithms for connectionist reinforcement learning" _Machine Learning_ (1992) - REINFORCE
  2. Mnih et al. "Asynchronous methods for deep reinforcement learning" _ICML_ (2016) - A3C/A2C
  3. Schulman et al. "Proximal policy optimization algorithms" _arXiv_ (2017) - PPO
  4. Schulman et al. "Trust region policy optimization" _ICML_ (2015) - TRPO
  5. Raffin et al. "Stable-Baselines3: Reliable reinforcement learning implementations" _JMLR_ (2021)

* * *

**æ¬¡ç« ** : [ç¬¬3ç« : ææ–™æ¢ç´¢ç’°å¢ƒã®æ§‹ç¯‰](<chapter-3.html>)
