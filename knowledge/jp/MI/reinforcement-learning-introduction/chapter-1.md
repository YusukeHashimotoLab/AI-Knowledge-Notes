---
title: "ç¬¬1ç« : ãªãœææ–™ç§‘å­¦ã«å¼·åŒ–å­¦ç¿’ã‹"
chapter_title: "ç¬¬1ç« : ãªãœææ–™ç§‘å­¦ã«å¼·åŒ–å­¦ç¿’ã‹"
subtitle: 
reading_time: 20-25åˆ†
difficulty: åˆç´š
code_examples: 6
exercises: 3
---

# ç¬¬1ç« : ãªãœææ–™ç§‘å­¦ã«å¼·åŒ–å­¦ç¿’ã‹

é€æ¬¡æ„æ€æ±ºå®šã®æž çµ„ã¿ã§ææ–™ãƒ»ãƒ—ãƒ­ã‚»ã‚¹ã‚’æœ€é©åŒ–ã™ã‚‹è€ƒãˆæ–¹ã‚’æŽ´ã¿ã¾ã™ã€‚å ±é…¬è¨­è¨ˆã®è½ã¨ã—ç©´ã‚‚ç´¹ä»‹ã—ã¾ã™ã€‚

**ðŸ’¡ è£œè¶³:** å ±é…¬ã¯â€œè¡Œå‹•ã®ã”è¤’ç¾Žâ€ã€‚çŸ­æœŸã¨é•·æœŸã®ã”è¤’ç¾Žã®ãƒãƒ©ãƒ³ã‚¹ã‚’èª¤ã‚‹ã¨å­¦ç¿’ãŒé€¸ã‚Œã¾ã™ã€‚

## å­¦ç¿’ç›®æ¨™

ã“ã®ç« ã§ã¯ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã—ã¾ã™ï¼š

  * ææ–™æŽ¢ç´¢ã«ãŠã‘ã‚‹å¾“æ¥æ‰‹æ³•ã®é™ç•Œã¨å¼·åŒ–å­¦ç¿’ã®å½¹å‰²
  * ãƒžãƒ«ã‚³ãƒ•æ±ºå®šéŽç¨‹ï¼ˆMDPï¼‰ã®åŸºæœ¬æ¦‚å¿µ
  * Qå­¦ç¿’ã¨Deep Q-Networkï¼ˆDQNï¼‰ã®ä»•çµ„ã¿
  * ç°¡å˜ãªææ–™æŽ¢ç´¢ã‚¿ã‚¹ã‚¯ã¸ã®å®Ÿè£…

* * *

## 1.1 ææ–™æŽ¢ç´¢ã®èª²é¡Œã¨å¼·åŒ–å­¦ç¿’ã®å½¹å‰²

### å¾“æ¥ã®ææ–™æŽ¢ç´¢ã®é™ç•Œ

æ–°ææ–™é–‹ç™ºã«ã¯ã€è†¨å¤§ãªæŽ¢ç´¢ç©ºé–“ï¼ˆçµ„æˆã€æ§‹é€ ã€ãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶ï¼‰ãŒã‚ã‚Šã¾ã™ï¼š

  * **çµ„æˆæŽ¢ç´¢** : å…ƒç´ å‘¨æœŸè¡¨ã‹ã‚‰3å…ƒç´ ã‚’é¸ã¶ã ã‘ã§$\binom{118}{3} \approx 267,000$é€šã‚Š
  * **æ§‹é€ æŽ¢ç´¢** : çµæ™¶æ§‹é€ ã ã‘ã§230ç¨®ã®ç©ºé–“ç¾¤
  * **ãƒ—ãƒ­ã‚»ã‚¹æŽ¢ç´¢** : æ¸©åº¦ãƒ»åœ§åŠ›ãƒ»æ™‚é–“ã®çµ„ã¿åˆã‚ã›ã¯ç„¡é™

å¾“æ¥ã®**è©¦è¡ŒéŒ¯èª¤ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ** ã§ã¯ï¼š \- ç ”ç©¶è€…ã®çµŒé¨“ã¨å‹˜ã«ä¾å­˜ \- è©•ä¾¡ã«æ™‚é–“ã¨ã‚³ã‚¹ãƒˆï¼ˆ1ææ–™ã‚ãŸã‚Šæ•°é€±é–“ã€œæ•°ãƒ¶æœˆï¼‰ \- å±€æ‰€æœ€é©è§£ã«é™¥ã‚Šã‚„ã™ã„
    
    
    ```mermaid
    flowchart LR
        A[ç ”ç©¶è€…] -->|çµŒé¨“ãƒ»å‹˜| B[ææ–™å€™è£œé¸æŠž]
        B -->|åˆæˆãƒ»è©•ä¾¡| C[çµæžœ]
        C -->|è§£é‡ˆ| A
    
        style A fill:#ffcccc
        style B fill:#ffcccc
        style C fill:#ffcccc
    ```

**å•é¡Œç‚¹** : 1\. **åŠ¹çŽ‡ãŒæ‚ªã„** : åŒã˜ã‚ˆã†ãªææ–™ã‚’ç¹°ã‚Šè¿”ã—è©¦ã™ 2\. **æŽ¢ç´¢ãŒç‹­ã„** : ç ”ç©¶è€…ã®çŸ¥è­˜ç¯„å›²ã«é™å®š 3\. **å†ç¾æ€§ãŒä½Žã„** : æš—é»™çŸ¥ã«ä¾å­˜

### å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹è§£æ±ºç­–

å¼·åŒ–å­¦ç¿’ã¯ã€**ç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦æœ€é©ãªè¡Œå‹•ã‚’å­¦ç¿’** ã™ã‚‹æž çµ„ã¿ã§ã™ï¼š
    
    
    ```mermaid
    flowchart LR
        A[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ: RL Algorithm] -->|è¡Œå‹•: ææ–™å€™è£œ| B[ç’°å¢ƒ: å®Ÿé¨“/è¨ˆç®—]
        B -->|å ±é…¬: ç‰¹æ€§è©•ä¾¡| A
        B -->|çŠ¶æ…‹: ç¾åœ¨ã®çŸ¥è¦‹| A
    
        style A fill:#e1f5ff
        style B fill:#ffe1cc
    ```

**å¼·åŒ–å­¦ç¿’ã®åˆ©ç‚¹** : 1\. **è‡ªå‹•æœ€é©åŒ–** : è©¦è¡ŒéŒ¯èª¤ã‚’è‡ªå‹•åŒ–ã—ã€åŠ¹çŽ‡çš„ãªæŽ¢ç´¢æˆ¦ç•¥ã‚’å­¦ç¿’ 2\. **æŽ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹** : æœªçŸ¥é ˜åŸŸã®æŽ¢ç´¢ã¨æ—¢çŸ¥ã®è‰¯ã„é ˜åŸŸã®æ´»ç”¨ã‚’èª¿æ•´ 3\. **é€æ¬¡çš„æ”¹å–„** : å„è©•ä¾¡çµæžœã‹ã‚‰å­¦ç¿’ã—ã€æ¬¡ã®é¸æŠžã‚’æ”¹å–„ 4\. **ã‚¯ãƒ­ãƒ¼ã‚ºãƒ‰ãƒ«ãƒ¼ãƒ—** : å®Ÿé¨“è£…ç½®ã¨çµ±åˆã—24æ™‚é–“ç¨¼åƒå¯èƒ½

### ææ–™ç§‘å­¦ã§ã®æˆåŠŸäº‹ä¾‹

**ä¾‹1: Li-ioné›»æ± é›»è§£æ¶²ã®æœ€é©åŒ–** (MIT, 2022) \- **èª²é¡Œ** : 5æˆåˆ†ã®é…åˆæ¯”çŽ‡ã‚’æœ€é©åŒ–ï¼ˆæŽ¢ç´¢ç©ºé–“ > $10^6$ï¼‰ \- **æ‰‹æ³•** : DQNã§é€æ¬¡çš„ã«é…åˆã‚’é¸æŠž \- **çµæžœ** : å¾“æ¥æ‰‹æ³•ã®5å€ã®é€Ÿåº¦ã§æœ€é©è§£ç™ºè¦‹ã€ã‚¤ã‚ªãƒ³ä¼å°Žåº¦30%å‘ä¸Š

**ä¾‹2: æœ‰æ©Ÿå¤ªé™½é›»æ± ãƒ‰ãƒŠãƒ¼ææ–™** (Torontoå¤§, 2021) \- **èª²é¡Œ** : åˆ†å­æ§‹é€ ã®æœ€é©åŒ–ï¼ˆ10^23é€šã‚Šã®å€™è£œï¼‰ \- **æ‰‹æ³•** : Actor-Criticã§åˆ†å­ç”Ÿæˆã¨è©•ä¾¡ã‚’çµ±åˆ \- **çµæžœ** : å…‰é›»å¤‰æ›åŠ¹çŽ‡15%ã®æ–°ææ–™ã‚’3ãƒ¶æœˆã§ç™ºè¦‹ï¼ˆå¾“æ¥ã¯2å¹´ï¼‰

* * *

## 1.2 ãƒžãƒ«ã‚³ãƒ•æ±ºå®šéŽç¨‹ï¼ˆMDPï¼‰ã®åŸºç¤Ž

### MDPã¨ã¯

å¼·åŒ–å­¦ç¿’ã®æ•°å­¦çš„åŸºç›¤ã¯ã€**ãƒžãƒ«ã‚³ãƒ•æ±ºå®šéŽç¨‹** ï¼ˆMarkov Decision Process, MDPï¼‰ã§ã™ã€‚MDPã¯ä»¥ä¸‹ã®5ã¤çµ„ã§å®šç¾©ã•ã‚Œã¾ã™ï¼š

$$ \text{MDP} = (S, A, P, R, \gamma) $$

  * $S$: **çŠ¶æ…‹ç©ºé–“** ï¼ˆä¾‹: ç¾åœ¨è©¦ã—ãŸææ–™ã®ç‰¹æ€§ï¼‰
  * $A$: **è¡Œå‹•ç©ºé–“** ï¼ˆä¾‹: æ¬¡ã«è©¦ã™ææ–™å€™è£œï¼‰
  * $P(s'|s, a)$: **çŠ¶æ…‹é·ç§»ç¢ºçŽ‡** ï¼ˆè¡Œå‹•$a$ã‚’å–ã£ãŸã¨ãã«çŠ¶æ…‹$s$ã‹ã‚‰$s'$ã¸é·ç§»ã™ã‚‹ç¢ºçŽ‡ï¼‰
  * $R(s, a, s')$: **å ±é…¬é–¢æ•°** ï¼ˆçŠ¶æ…‹é·ç§»ã§å¾—ã‚‰ã‚Œã‚‹å ±é…¬ï¼‰
  * $\gamma \in [0, 1)$: **å‰²å¼•çŽ‡** ï¼ˆå°†æ¥ã®å ±é…¬ã®é‡è¦åº¦ï¼‰

### ææ–™æŽ¢ç´¢ã¸ã®ãƒžãƒƒãƒ”ãƒ³ã‚°

MDPè¦ç´  | ææ–™æŽ¢ç´¢ã§ã®æ„å‘³ | å…·ä½“ä¾‹  
---|---|---  
çŠ¶æ…‹ $s$ | ç¾åœ¨ã®çŸ¥è¦‹ï¼ˆã“ã‚Œã¾ã§ã®è©•ä¾¡çµæžœï¼‰ | "ææ–™A: ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—2.1eVã€ææ–™B: 2.5eV"  
è¡Œå‹• $a$ | æ¬¡ã«è©¦ã™ææ–™ | "Ti-Ni-Oçµ„æˆã®ææ–™C"  
å ±é…¬ $r$ | ææ–™ç‰¹æ€§ã®è©•ä¾¡å€¤ | "ææ–™Cã®ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—2.8eVï¼ˆç›®æ¨™3.0eVã«è¿‘ã„ï¼‰"  
æ–¹ç­– $\pi$ | ææ–™é¸æŠžæˆ¦ç•¥ | "ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ãŒç›®æ¨™ã«è¿‘ã„å…ƒç´ çµ„æˆã‚’å„ªå…ˆ"  
  
### ãƒžãƒ«ã‚³ãƒ•æ€§

MDPã®é‡è¦ãªä»®å®šã¯**ãƒžãƒ«ã‚³ãƒ•æ€§** ã§ã™ï¼š

$$ P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, \dots) = P(s_{t+1}|s_t, a_t) $$

ã¤ã¾ã‚Šã€**æ¬¡ã®çŠ¶æ…‹ã¯ç¾åœ¨ã®çŠ¶æ…‹ã¨è¡Œå‹•ã®ã¿ã«ä¾å­˜ã—ã€éŽåŽ»ã®å±¥æ­´ã¯ä¸è¦** ã§ã™ã€‚

ææ–™æŽ¢ç´¢ã§ã¯ã€ç¾åœ¨ã®è©•ä¾¡çµæžœï¼ˆçŠ¶æ…‹ï¼‰ã«åŸºã¥ã„ã¦æ¬¡ã®ææ–™ï¼ˆè¡Œå‹•ï¼‰ã‚’é¸ã¹ã°ã€éŽåŽ»ã®å…¨å±¥æ­´ã‚’è¦šãˆã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

### æ–¹ç­–ã¨ä¾¡å€¤é–¢æ•°

**æ–¹ç­–** $\pi(a|s)$: çŠ¶æ…‹$s$ã§è¡Œå‹•$a$ã‚’é¸ã¶ç¢ºçŽ‡

**çŠ¶æ…‹ä¾¡å€¤é–¢æ•°** $V^\pi(s)$: çŠ¶æ…‹$s$ã‹ã‚‰æ–¹ç­–$\pi$ã«å¾“ã£ã¦è¡Œå‹•ã—ãŸã¨ãã®æœŸå¾…ç´¯ç©å ±é…¬

$$ V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s \right] $$

**è¡Œå‹•ä¾¡å€¤é–¢æ•°ï¼ˆQé–¢æ•°ï¼‰** $Q^\pi(s, a)$: çŠ¶æ…‹$s$ã§è¡Œå‹•$a$ã‚’å–ã‚Šã€ãã®å¾Œæ–¹ç­–$\pi$ã«å¾“ã£ãŸã¨ãã®æœŸå¾…ç´¯ç©å ±é…¬

$$ Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s, a_0 = a \right] $$

**æœ€é©æ–¹ç­–** $\pi^*$: ã™ã¹ã¦ã®çŠ¶æ…‹ã§ä¾¡å€¤é–¢æ•°ã‚’æœ€å¤§åŒ–ã™ã‚‹æ–¹ç­–

$$ \pi^* = \arg\max_\pi V^\pi(s) \quad \forall s \in S $$

* * *

## 1.3 Qå­¦ç¿’ï¼ˆQ-Learningï¼‰

### Qå­¦ç¿’ã®åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢

Qå­¦ç¿’ã¯ã€**Qé–¢æ•°ã‚’ç›´æŽ¥å­¦ç¿’** ã™ã‚‹å¼·åŒ–å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚

**ãƒ™ãƒ«ãƒžãƒ³æ–¹ç¨‹å¼** : $$ Q^*(s, a) = \mathbb{E}_{s'} \left[ r + \gamma \max_{a'} Q^*(s', a') \mid s, a \right] $$

ã“ã‚Œã¯ã€Œæœ€é©ãªQé–¢æ•°ã¯ã€å³åº§ã®å ±é…¬$r$ã¨æ¬¡ã®çŠ¶æ…‹ã§ã®æœ€å¤§Qå€¤ã®å‰²å¼•å’Œã«ç­‰ã—ã„ã€ã¨ã„ã†æ„å‘³ã§ã™ã€‚

### Qå­¦ç¿’ã®æ›´æ–°å¼

è¦³æ¸¬ã•ã‚ŒãŸé·ç§»$(s, a, r, s')$ã«åŸºã¥ã„ã¦ã€Qå€¤ã‚’æ›´æ–°ï¼š

$$ Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right] $$

  * $\alpha$: å­¦ç¿’çŽ‡ï¼ˆ0ã€œ1ï¼‰
  * $r + \gamma \max_{a'} Q(s', a')$: **TDç›®æ¨™** ï¼ˆTemporal Difference Targetï¼‰
  * $r + \gamma \max_{a'} Q(s', a') - Q(s, a)$: **TDèª¤å·®**

### Pythonã«ã‚ˆã‚‹å®Ÿè£…

ç°¡å˜ãªã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ï¼ˆææ–™æŽ¢ç´¢ç©ºé–“ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ï¼‰ã§Qå­¦ç¿’ã‚’å®Ÿè£…ã—ã¾ã™ï¼š
    
    
    """
    Qå­¦ç¿’ã«ã‚ˆã‚‹ææ–™æŽ¢ç´¢ç’°å¢ƒã®å®Ÿè£…
    
    Dependencies (ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³):
    - Python: 3.9+
    - numpy: 1.24+
    - matplotlib: 3.7+
    
    Reproducibility (å†ç¾æ€§):
    - Random seedå›ºå®š: 42ï¼ˆã™ã¹ã¦ã®ä¹±æ•°æ“ä½œã§çµ±ä¸€ï¼‰
    - ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: 1000ï¼ˆåŽæŸç¢ºèªæ¸ˆã¿ï¼‰
    - å­¦ç¿’çŽ‡Î±: 0.1ï¼ˆéŽåº¦ãªæ›´æ–°ã‚’é˜²ãï¼‰
    - å‰²å¼•çŽ‡Î³: 0.99ï¼ˆé•·æœŸå ±é…¬ã‚’é‡è¦–ï¼‰
    - Îµ-greedy: Îµ=0.1å›ºå®šï¼ˆæŽ¢ç´¢10%ã€æ´»ç”¨90%ï¼‰
    
    Pitfalls (å®Ÿè·µçš„ãªè½ã¨ã—ç©´):
    1. Îµå›ºå®šã®ãŸã‚å­¦ç¿’å¾ŒæœŸã‚‚æŽ¢ç´¢ã‚’ç¶šã‘ã‚‹ï¼ˆæœ€é©åŒ–ã®ä½™åœ°ã‚ã‚Šï¼‰
    2. Q-tableã‚µã‚¤ã‚ºã¯5x5x4=100è¦ç´ ï¼ˆå°è¦æ¨¡ç’°å¢ƒã®ã¿å¯¾å¿œï¼‰
    3. å ±é…¬ãŒç–Žï¼ˆã‚´ãƒ¼ãƒ«ã®ã¿+10ï¼‰ãªãŸã‚æŽ¢ç´¢ãŒå›°é›£ãªå¯èƒ½æ€§
    """
    
    import numpy as np
    import matplotlib.pyplot as plt
    
    # ä¹±æ•°ã‚·ãƒ¼ãƒ‰å›ºå®šï¼ˆå†ç¾æ€§ç¢ºä¿ï¼‰
    RANDOM_SEED = 42
    np.random.seed(RANDOM_SEED)
    
    class SimpleMaterialsEnv:
        """ç°¡å˜ãªææ–™æŽ¢ç´¢ç’°å¢ƒï¼ˆã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ï¼‰
    
        - 5x5ã®ã‚°ãƒªãƒƒãƒ‰
        - å„ã‚»ãƒ«ã¯ææ–™å€™è£œã‚’è¡¨ã™
        - ç›®æ¨™: æœ€é«˜ç‰¹æ€§ã®ææ–™ï¼ˆã‚´ãƒ¼ãƒ«ï¼‰ã«åˆ°é”
        """
        def __init__(self):
            self.grid_size = 5
            self.state = (0, 0)  # ã‚¹ã‚¿ãƒ¼ãƒˆä½ç½®
            self.goal = (4, 4)   # ã‚´ãƒ¼ãƒ«ä½ç½®ï¼ˆæœ€é©ææ–™ï¼‰
    
        def reset(self):
            """åˆæœŸçŠ¶æ…‹ã«ãƒªã‚»ãƒƒãƒˆ"""
            self.state = (0, 0)
            return self.state
    
        def step(self, action):
            """è¡Œå‹•ã‚’å®Ÿè¡Œ
    
            Args:
                action: 0=ä¸Š, 1=ä¸‹, 2=å·¦, 3=å³
    
            Returns:
                next_state, reward, done
            """
            x, y = self.state
    
            # è¡Œå‹•ã«å¿œã˜ã¦ç§»å‹•
            if action == 0 and x > 0:  # ä¸Š
                x -= 1
            elif action == 1 and x < self.grid_size - 1:  # ä¸‹
                x += 1
            elif action == 2 and y > 0:  # å·¦
                y -= 1
            elif action == 3 and y < self.grid_size - 1:  # å³
                y += 1
    
            self.state = (x, y)
    
            # å ±é…¬è¨­è¨ˆ
            if self.state == self.goal:
                reward = 10.0  # ã‚´ãƒ¼ãƒ«åˆ°é”ï¼ˆæœ€é©ææ–™ç™ºè¦‹ï¼‰
                done = True
            else:
                reward = -0.1  # å„ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚³ã‚¹ãƒˆï¼ˆå®Ÿé¨“ã‚³ã‚¹ãƒˆï¼‰
                done = False
    
            return self.state, reward, done
    
        def get_state_space(self):
            """çŠ¶æ…‹ç©ºé–“ã®ã‚µã‚¤ã‚º"""
            return self.grid_size * self.grid_size
    
        def get_action_space(self):
            """è¡Œå‹•ç©ºé–“ã®ã‚µã‚¤ã‚º"""
            return 4
    
    
    def q_learning(env, episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1):
        """Qå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    
        Args:
            env: ç’°å¢ƒ
            episodes: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°
            alpha: å­¦ç¿’çŽ‡
            gamma: å‰²å¼•çŽ‡
            epsilon: Îµ-greedyæŽ¢ç´¢ã®ç¢ºçŽ‡
    
        Returns:
            å­¦ç¿’ã—ãŸQ-table
        """
        # Q-tableã®åˆæœŸåŒ–ï¼ˆçŠ¶æ…‹Ã—è¡Œå‹•ï¼‰
        Q = np.zeros((env.grid_size, env.grid_size, env.get_action_space()))
    
        rewards_per_episode = []
    
        for episode in range(episodes):
            state = env.reset()
            total_reward = 0
            done = False
    
            while not done:
                # Îµ-greedyæŽ¢ç´¢
                if np.random.random() < epsilon:
                    action = np.random.randint(env.get_action_space())  # ãƒ©ãƒ³ãƒ€ãƒ æŽ¢ç´¢
                else:
                    action = np.argmax(Q[state[0], state[1], :])  # æœ€è‰¯ã®è¡Œå‹•ã‚’æ´»ç”¨
    
                # è¡Œå‹•å®Ÿè¡Œ
                next_state, reward, done = env.step(action)
                total_reward += reward
    
                # Qå€¤æ›´æ–°ï¼ˆãƒ™ãƒ«ãƒžãƒ³æ–¹ç¨‹å¼ï¼‰
                current_q = Q[state[0], state[1], action]
                max_next_q = np.max(Q[next_state[0], next_state[1], :])
                new_q = current_q + alpha * (reward + gamma * max_next_q - current_q)
                Q[state[0], state[1], action] = new_q
    
                state = next_state
    
            rewards_per_episode.append(total_reward)
    
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(rewards_per_episode[-100:])
                print(f"Episode {episode+1}: Avg Reward = {avg_reward:.2f}")
    
        return Q, rewards_per_episode
    
    
    # å®Ÿè¡Œ
    env = SimpleMaterialsEnv()
    Q, rewards = q_learning(env, episodes=1000)
    
    # å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–
    plt.figure(figsize=(10, 6))
    plt.plot(np.convolve(rewards, np.ones(50)/50, mode='valid'))
    plt.xlabel('Episode')
    plt.ylabel('Average Reward (50 episodes)')
    plt.title('Q-Learning: ææ–™æŽ¢ç´¢ç’°å¢ƒã§ã®å­¦ç¿’é€²æ—')
    plt.grid(True)
    plt.show()
    
    # å­¦ç¿’ã—ãŸQå€¤ã®å¯è¦–åŒ–
    policy = np.argmax(Q, axis=2)
    print("\nå­¦ç¿’ã—ãŸæ–¹ç­–ï¼ˆå„ã‚»ãƒ«ã§ã®æœ€è‰¯è¡Œå‹•ï¼‰:")
    print("0=ä¸Š, 1=ä¸‹, 2=å·¦, 3=å³")
    print(policy)
    

**å‡ºåŠ›ä¾‹** :
    
    
    Episode 100: Avg Reward = -4.52
    Episode 200: Avg Reward = -3.21
    Episode 500: Avg Reward = -1.85
    Episode 1000: Avg Reward = -1.12
    
    å­¦ç¿’ã—ãŸæ–¹ç­–ï¼ˆå„ã‚»ãƒ«ã§ã®æœ€è‰¯è¡Œå‹•ï¼‰:
    [[1 1 1 1 1]
     [1 1 1 1 1]
     [1 1 1 1 1]
     [1 1 1 1 1]
     [3 3 3 3 0]]
    

**è§£èª¬** : \- åˆæœŸã¯å ±é…¬ãŒä½Žã„ï¼ˆ-4.52ï¼‰ãŒã€å­¦ç¿’ãŒé€²ã‚€ã¨æ”¹å–„ï¼ˆ-1.12ï¼‰ \- æœ€çµ‚çš„ã«ã€ã‚´ãƒ¼ãƒ«ã¸ã®æœ€çŸ­çµŒè·¯ã‚’å­¦ç¿’ï¼ˆä¸‹â†’å³ã®æ–¹ç­–ï¼‰

* * *

## 1.4 Deep Q-Networkï¼ˆDQNï¼‰

### Qå­¦ç¿’ã®é™ç•Œ

Qå­¦ç¿’ã¯ã€**çŠ¶æ…‹ã¨è¡Œå‹•ãŒé›¢æ•£çš„ã‹ã¤å°‘æ•°** ã®å ´åˆã«æœ‰åŠ¹ã§ã™ã€‚ã—ã‹ã—ã€ææ–™ç§‘å­¦ã§ã¯ï¼š

  * **çŠ¶æ…‹ç©ºé–“ãŒå·¨å¤§** : ææ–™è¨˜è¿°å­ï¼ˆ100æ¬¡å…ƒä»¥ä¸Šï¼‰
  * **é€£ç¶šå€¤** : çµ„æˆæ¯”çŽ‡ã€æ¸©åº¦ã€åœ§åŠ›ãªã©
  * **Q-tableãŒéžç¾å®Ÿçš„** : $10^{100}$å€‹ã®ã‚»ãƒ«ã‚’ä¿å­˜ã§ããªã„

### DQNã®è§£æ±ºç­–

DQNã¯ã€**ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§Qé–¢æ•°ã‚’è¿‘ä¼¼** ã—ã¾ã™ï¼š

$$ Q(s, a; \theta) \approx Q^*(s, a) $$

  * $\theta$: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

**æå¤±é–¢æ•°** : $$ L(\theta) = \mathbb{E}_{(s,a,r,s') \sim D} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta) \right)^2 \right] $$

  * $D$: **çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡** ï¼ˆéŽåŽ»ã®é·ç§»ã‚’ä¿å­˜ï¼‰
  * $\theta^-$: **ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯** ï¼ˆå­¦ç¿’ã®å®‰å®šåŒ–ï¼‰

### DQNã®é‡è¦æŠ€è¡“

  1. **çµŒé¨“å†ç”Ÿï¼ˆExperience Replayï¼‰** : éŽåŽ»ã®é·ç§»ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ãƒ‡ãƒ¼ã‚¿ã®ç›¸é–¢ã‚’æ¸›ã‚‰ã™
  2. **ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯** : å›ºå®šã•ã‚ŒãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§TDç›®æ¨™ã‚’è¨ˆç®—ã—ã€å­¦ç¿’ã‚’å®‰å®šåŒ–
  3. **Îµ-greedyæŽ¢ç´¢** : æŽ¢ç´¢ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ï¼‰ã¨æ´»ç”¨ï¼ˆæœ€è‰¯è¡Œå‹•ï¼‰ã®ãƒãƒ©ãƒ³ã‚¹

### PyTorchã«ã‚ˆã‚‹DQNå®Ÿè£…
    
    
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from collections import deque
    import random
    
    class DQN(nn.Module):
        """Deep Q-Network
    
        çŠ¶æ…‹ã‚’å…¥åŠ›ã—ã€å„è¡Œå‹•ã®Qå€¤ã‚’å‡ºåŠ›
        """
        def __init__(self, state_dim, action_dim, hidden_dim=64):
            super(DQN, self).__init__()
            self.fc1 = nn.Linear(state_dim, hidden_dim)
            self.fc2 = nn.Linear(hidden_dim, hidden_dim)
            self.fc3 = nn.Linear(hidden_dim, action_dim)
    
        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = torch.relu(self.fc2(x))
            return self.fc3(x)
    
    
    class ReplayBuffer:
        """çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡"""
        def __init__(self, capacity=10000):
            self.buffer = deque(maxlen=capacity)
    
        def push(self, state, action, reward, next_state, done):
            self.buffer.append((state, action, reward, next_state, done))
    
        def sample(self, batch_size):
            return random.sample(self.buffer, batch_size)
    
        def __len__(self):
            return len(self.buffer)
    
    
    class DQNAgent:
        """DQNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
        def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):
            self.action_dim = action_dim
            self.gamma = gamma
            self.epsilon = epsilon
            self.epsilon_decay = epsilon_decay
            self.epsilon_min = epsilon_min
    
            # ãƒ¡ã‚¤ãƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
            self.policy_net = DQN(state_dim, action_dim)
            self.target_net = DQN(state_dim, action_dim)
            self.target_net.load_state_dict(self.policy_net.state_dict())
            self.target_net.eval()
    
            self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)
            self.buffer = ReplayBuffer()
    
        def select_action(self, state):
            """Îµ-greedyè¡Œå‹•é¸æŠž"""
            if np.random.random() < self.epsilon:
                return np.random.randint(self.action_dim)
            else:
                with torch.no_grad():
                    state_tensor = torch.FloatTensor(state).unsqueeze(0)
                    q_values = self.policy_net(state_tensor)
                    return q_values.argmax().item()
    
        def train(self, batch_size=64):
            """ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’"""
            if len(self.buffer) < batch_size:
                return
    
            # ãƒŸãƒ‹ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            batch = self.buffer.sample(batch_size)
            states, actions, rewards, next_states, dones = zip(*batch)
    
            states = torch.FloatTensor(states)
            actions = torch.LongTensor(actions).unsqueeze(1)
            rewards = torch.FloatTensor(rewards).unsqueeze(1)
            next_states = torch.FloatTensor(next_states)
            dones = torch.FloatTensor(dones).unsqueeze(1)
    
            # ç¾åœ¨ã®Qå€¤
            current_q = self.policy_net(states).gather(1, actions)
    
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆQå€¤
            with torch.no_grad():
                max_next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)
                target_q = rewards + (1 - dones) * self.gamma * max_next_q
    
            # æå¤±è¨ˆç®—ã¨æœ€é©åŒ–
            loss = nn.MSELoss()(current_q, target_q)
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
    
            # Îµã®æ¸›è¡°
            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
    
        def update_target_network(self):
            """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ›´æ–°"""
            self.target_net.load_state_dict(self.policy_net.state_dict())
    
    
    # ææ–™æŽ¢ç´¢ç’°å¢ƒï¼ˆé€£ç¶šçŠ¶æ…‹ç‰ˆï¼‰
    class ContinuousMaterialsEnv:
        """é€£ç¶šçŠ¶æ…‹ç©ºé–“ã®ææ–™æŽ¢ç´¢ç’°å¢ƒ"""
        def __init__(self, state_dim=4):
            self.state_dim = state_dim
            self.target = np.array([3.0, 5.0, 2.5, 4.0])  # ç›®æ¨™ç‰¹æ€§
            self.state = None
    
        def reset(self):
            self.state = np.random.uniform(0, 10, self.state_dim)
            return self.state
    
        def step(self, action):
            # è¡Œå‹•: 0=å¢—åŠ , 1=æ¸›å°‘, 2=å¤§å¹…å¢—åŠ , 3=å¤§å¹…æ¸›å°‘
            delta = [0.1, -0.1, 0.5, -0.5][action]
    
            # ãƒ©ãƒ³ãƒ€ãƒ ãªæ¬¡å…ƒã‚’å¤‰æ›´
            dim = np.random.randint(self.state_dim)
            self.state[dim] = np.clip(self.state[dim] + delta, 0, 10)
    
            # å ±é…¬: ç›®æ¨™ã¨ã®è·é›¢ï¼ˆè² ã®å€¤ã€è¿‘ã„ã»ã©è‰¯ã„ï¼‰
            distance = np.linalg.norm(self.state - self.target)
            reward = -distance
    
            # çµ‚äº†æ¡ä»¶: ç›®æ¨™ã«ååˆ†è¿‘ã„
            done = distance < 0.5
    
            return self.state, reward, done
    
    
    # DQNè¨“ç·´
    env = ContinuousMaterialsEnv()
    agent = DQNAgent(state_dim=4, action_dim=4)
    
    episodes = 500
    rewards_history = []
    
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False
    
        while not done:
            action = agent.select_action(state)
            next_state, reward, done = env.step(action)
    
            agent.buffer.push(state, action, reward, next_state, done)
            agent.train()
    
            state = next_state
            total_reward += reward
    
        rewards_history.append(total_reward)
    
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°
        if (episode + 1) % 10 == 0:
            agent.update_target_network()
    
        if (episode + 1) % 50 == 0:
            avg_reward = np.mean(rewards_history[-50:])
            print(f"Episode {episode+1}: Avg Reward = {avg_reward:.2f}, Îµ = {agent.epsilon:.3f}")
    
    # å­¦ç¿’æ›²ç·š
    plt.figure(figsize=(10, 6))
    plt.plot(np.convolve(rewards_history, np.ones(20)/20, mode='valid'))
    plt.xlabel('Episode')
    plt.ylabel('Average Reward (20 episodes)')
    plt.title('DQN: é€£ç¶šçŠ¶æ…‹ææ–™æŽ¢ç´¢ã§ã®å­¦ç¿’é€²æ—')
    plt.grid(True)
    plt.show()
    

**å‡ºåŠ›ä¾‹** :
    
    
    Episode 50: Avg Reward = -45.23, Îµ = 0.779
    Episode 100: Avg Reward = -32.15, Îµ = 0.606
    Episode 200: Avg Reward = -18.92, Îµ = 0.365
    Episode 500: Avg Reward = -8.45, Îµ = 0.010
    

**è§£èª¬** : \- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒé€£ç¶šçŠ¶æ…‹ã®Qé–¢æ•°ã‚’å­¦ç¿’ \- ÎµãŒæ¸›è¡°ã—ã€æŽ¢ç´¢ã‹ã‚‰æ´»ç”¨ã¸ã‚·ãƒ•ãƒˆ \- æœ€çµ‚çš„ã«ç›®æ¨™ç‰¹æ€§ã«è¿‘ã„ææ–™ã‚’åŠ¹çŽ‡çš„ã«ç™ºè¦‹

* * *

## æ¼”ç¿’å•é¡Œ

### å•é¡Œ1 (é›£æ˜“åº¦: easy)

Qå­¦ç¿’ã®æ›´æ–°å¼ã«ãŠã„ã¦ã€å­¦ç¿’çŽ‡$\alpha$ã‚’å¤§ããã™ã‚‹ã¨ä½•ãŒèµ·ã“ã‚‹ã‹èª¬æ˜Žã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€$\alpha=0$ã¨$\alpha=1$ã®æ¥µç«¯ãªã‚±ãƒ¼ã‚¹ã§ã¯ã©ã†ãªã‚‹ã‹ç­”ãˆã¦ãã ã•ã„ã€‚

ãƒ’ãƒ³ãƒˆ å­¦ç¿’çŽ‡ã¯ã€Œæ–°ã—ã„æƒ…å ±ã‚’ã©ã‚Œã ã‘é‡è¦–ã™ã‚‹ã‹ã€ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚æ›´æ–°å¼ã‚’è¦‹ç›´ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚  è§£ç­”ä¾‹ **$\alpha$ã‚’å¤§ããã™ã‚‹ã¨**: \- æ–°ã—ã„è¦³æ¸¬ï¼ˆTDç›®æ¨™ï¼‰ã‚’å¼·ãåæ˜ ã—ã€Qå€¤ãŒå¤§ããå¤‰åŒ– \- å­¦ç¿’ãŒé€Ÿã„ãŒä¸å®‰å®šã«ãªã‚Šã‚„ã™ã„ **æ¥µç«¯ãªã‚±ãƒ¼ã‚¹**: \- **$\alpha=0$**: Qå€¤ãŒå…¨ãæ›´æ–°ã•ã‚Œãªã„ï¼ˆå­¦ç¿’ã—ãªã„ï¼‰ $$Q(s,a) \leftarrow Q(s,a) + 0 \cdot [\cdots] = Q(s,a)$$ \- **$\alpha=1$**: Qå€¤ãŒå®Œå…¨ã«TDç›®æ¨™ã§ç½®ãæ›ãˆã‚‰ã‚Œã‚‹ $$Q(s,a) \leftarrow r + \gamma \max_{a'} Q(s', a')$$ éŽåŽ»ã®æƒ…å ±ãŒå®Œå…¨ã«æ¶ˆãˆã€æœ€æ–°ã®è¦³æ¸¬ã®ã¿ã«ä¾å­˜ **å®Ÿè·µçš„ã«ã¯**: $\alpha = 0.01 \sim 0.1$ãŒä¸€èˆ¬çš„ 

* * *

### å•é¡Œ2 (é›£æ˜“åº¦: medium)

ææ–™æŽ¢ç´¢ã«ãŠã„ã¦ã€å ±é…¬é–¢æ•°ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«è¨­è¨ˆã—ã¾ã—ãŸã€‚ã“ã®è¨­è¨ˆã®å•é¡Œç‚¹ã¨æ”¹å–„æ¡ˆã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚
    
    
    def reward_function(material_property, target=3.0):
        if material_property == target:
            return 1.0
        else:
            return 0.0
    

ãƒ’ãƒ³ãƒˆ ã“ã®å ±é…¬ã¯ã€Œã‚¹ãƒ‘ãƒ¼ã‚¹å ±é…¬ã€ã¨å‘¼ã°ã‚Œã€ç›®æ¨™ã«åˆ°é”ã—ãªã„é™ã‚Šã™ã¹ã¦0ã§ã™ã€‚å­¦ç¿’ã«ã©ã†å½±éŸ¿ã™ã‚‹ã‹è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚  è§£ç­”ä¾‹ **å•é¡Œç‚¹**: 1\. **ã‚¹ãƒ‘ãƒ¼ã‚¹å ±é…¬**: ã»ã¨ã‚“ã©ã®å ´åˆå ±é…¬ãŒ0ã§ã€å­¦ç¿’ã‚·ã‚°ãƒŠãƒ«ãŒå¼±ã„ 2\. **æŽ¢ç´¢ãŒå›°é›£**: ã©ã®æ–¹å‘ã«é€²ã‚ã°è‰¯ã„ã‹ã‚ã‹ã‚‰ãªã„ 3\. **åŽ³å¯†ãªä¸€è‡´**: å®Ÿæ•°å€¤ã§å®Œå…¨ä¸€è‡´ã¯ã»ã¼ä¸å¯èƒ½ **æ”¹å–„æ¡ˆ**: 
    
    
    def improved_reward_function(material_property, target=3.0):
        # ç›®æ¨™ã¨ã®è·é›¢ã«åŸºã¥ãé€£ç¶šçš„ãªå ±é…¬
        distance = abs(material_property - target)
    
        if distance < 0.1:
            return 10.0  # éžå¸¸ã«è¿‘ã„ï¼ˆãƒœãƒ¼ãƒŠã‚¹ï¼‰
        elif distance < 0.5:
            return 5.0   # è¿‘ã„
        else:
            return -distance  # é ã„ã»ã©ãƒšãƒŠãƒ«ãƒ†ã‚£
    

**ã•ã‚‰ãªã‚‹æ”¹å–„**: \- **ã‚·ã‚§ã‚¤ãƒ”ãƒ³ã‚°å ±é…¬**: ç›®æ¨™ã¸ã®é€²æ—ã«å¿œã˜ã¦ä¸­é–“å ±é…¬ã‚’ä¸Žãˆã‚‹ \- **å¤šç›®çš„å ±é…¬**: è¤‡æ•°ã®ç‰¹æ€§ã‚’è€ƒæ…®ï¼ˆãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ— + å®‰å®šæ€§ï¼‰ 

* * *

### å•é¡Œ3 (é›£æ˜“åº¦: hard)

DQNã«ãŠã‘ã‚‹ã€ŒçµŒé¨“å†ç”Ÿã€ã¨ã€Œã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ã®å½¹å‰²ã‚’èª¬æ˜Žã—ã€ãã‚Œãžã‚ŒãŒãªã„ã¨ã©ã®ã‚ˆã†ãªå•é¡ŒãŒèµ·ã“ã‚‹ã‹ã€Pythonã‚³ãƒ¼ãƒ‰ã§å®Ÿé¨“ã—ã¦ãã ã•ã„ã€‚

ãƒ’ãƒ³ãƒˆ çµŒé¨“å†ç”Ÿã‚’ã‚ªãƒ•ã«ã™ã‚‹ã«ã¯`buffer.sample()`ã®ä»£ã‚ã‚Šã«æœ€æ–°ã®é·ç§»ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ã‚ªãƒ•ã«ã™ã‚‹ã«ã¯ã€TDç›®æ¨™ã®è¨ˆç®—ã§`self.policy_net`ã‚’ä½¿ã„ã¾ã™ã€‚  è§£ç­”ä¾‹ **çµŒé¨“å†ç”Ÿã®å½¹å‰²**: \- éŽåŽ»ã®é·ç§»ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ãƒ‡ãƒ¼ã‚¿ã®ç›¸é–¢ã‚’æ¸›ã‚‰ã™ \- ãªã„ã¨ã€é€£ç¶šã—ãŸé·ç§»ã ã‘ã§å­¦ç¿’ã—ã€ç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«éŽå­¦ç¿’ **ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å½¹å‰²**: \- å›ºå®šã•ã‚ŒãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§TDç›®æ¨™ã‚’è¨ˆç®—ã—ã€å­¦ç¿’ã‚’å®‰å®šåŒ– \- ãªã„ã¨ã€Qå€¤ãŒæŒ¯å‹•ã—ã¦åŽæŸã—ã«ãã„ **å®Ÿé¨“ã‚³ãƒ¼ãƒ‰**: 
    
    
    # çµŒé¨“å†ç”Ÿãªã—ç‰ˆ
    class DQNAgentNoReplay(DQNAgent):
        def train_no_replay(self, state, action, reward, next_state, done):
            # æœ€æ–°ã®é·ç§»ã®ã¿ã§å­¦ç¿’
            states = torch.FloatTensor([state])
            actions = torch.LongTensor([action]).unsqueeze(1)
            rewards = torch.FloatTensor([reward]).unsqueeze(1)
            next_states = torch.FloatTensor([next_state])
            dones = torch.FloatTensor([done]).unsqueeze(1)
    
            current_q = self.policy_net(states).gather(1, actions)
            with torch.no_grad():
                max_next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)
                target_q = rewards + (1 - dones) * self.gamma * max_next_q
    
            loss = nn.MSELoss()(current_q, target_q)
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãªã—ç‰ˆï¼ˆTDç›®æ¨™ã§policy_netã‚’ä½¿ç”¨ï¼‰
    # â†’ å­¦ç¿’ãŒä¸å®‰å®šã«ãªã‚‹
    
    # çµæžœ: çµŒé¨“å†ç”Ÿãªã—ã§ã¯åŽæŸãŒé…ãã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãªã—ã§ã¯æŒ¯å‹•ã™ã‚‹
    

* * *

## ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã¾ã¨ã‚

  * ææ–™æŽ¢ç´¢ã¯æŽ¢ç´¢ç©ºé–“ãŒåºƒå¤§ã§ã€å¾“æ¥ã®è©¦è¡ŒéŒ¯èª¤ã¯éžåŠ¹çŽ‡
  * å¼·åŒ–å­¦ç¿’ã¯**ç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦æœ€é©ãªæŽ¢ç´¢æˆ¦ç•¥ã‚’å­¦ç¿’**
  * **ãƒžãƒ«ã‚³ãƒ•æ±ºå®šéŽç¨‹ï¼ˆMDPï¼‰** ãŒå¼·åŒ–å­¦ç¿’ã®æ•°å­¦çš„åŸºç›¤
  * **Qå­¦ç¿’** ã¯é›¢æ•£çŠ¶æ…‹ãƒ»è¡Œå‹•ã§æœ‰åŠ¹ã€Q-tableã§ä¾¡å€¤ã‚’è¨˜éŒ²
  * **DQN** ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§Qé–¢æ•°ã‚’è¿‘ä¼¼ã—ã€å·¨å¤§ãªçŠ¶æ…‹ç©ºé–“ã«å¯¾å¿œ
  * **çµŒé¨“å†ç”Ÿ** ã¨**ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯** ãŒDQNå­¦ç¿’ã‚’å®‰å®šåŒ–

æ¬¡ç« ã§ã¯ã€ã‚ˆã‚Šé«˜åº¦ãªæ–¹ç­–å‹¾é…æ³•ï¼ˆPolicy Gradientï¼‰ã¨Actor-Criticæ‰‹æ³•ã‚’å­¦ã³ã¾ã™ã€‚

* * *

## å“è³ªãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼šææ–™æŽ¢ç´¢RLã®å®Ÿè£…ç¢ºèª

### MDPå®šå¼åŒ–ã‚¹ã‚­ãƒ«

  * [ ] ææ–™æŽ¢ç´¢ã‚¿ã‚¹ã‚¯ã‚’çŠ¶æ…‹ãƒ»è¡Œå‹•ãƒ»å ±é…¬ã§å®šå¼åŒ–ã§ãã‚‹
  * [ ] ãƒžãƒ«ã‚³ãƒ•æ€§ã®ä»®å®šãŒå¦¥å½“ã‹ã‚’åˆ¤æ–­ã§ãã‚‹
  * [ ] å ±é…¬é–¢æ•°ãŒæŽ¢ç´¢ç›®æ¨™ã‚’æ­£ã—ãè¡¨ç¾ã—ã¦ã„ã‚‹ã‹æ¤œè¨¼ã§ãã‚‹
  * [ ] å‰²å¼•çŽ‡Î³ã®é¸æŠžç†ç”±ã‚’èª¬æ˜Žã§ãã‚‹ï¼ˆææ–™æŽ¢ç´¢ã§ã¯é€šå¸¸0.95-0.99ï¼‰

### Qå­¦ç¿’å®Ÿè£…ã‚¹ã‚­ãƒ«

  * [ ] Îµ-greedyæŽ¢ç´¢ã®å®Ÿè£…ãŒã§ãã‚‹
  * [ ] TDèª¤å·®ã®è¨ˆç®—ã¨ Q å€¤æ›´æ–°ã‚’å®Ÿè£…ã§ãã‚‹
  * [ ] å­¦ç¿’æ›²ç·šã‹ã‚‰åŽæŸã‚’åˆ¤æ–­ã§ãã‚‹
  * [ ] Q-tableã®ã‚µã‚¤ã‚ºåˆ¶ç´„ï¼ˆçŠ¶æ…‹Ã—è¡Œå‹•æ•°ï¼‰ã‚’ç†è§£ã—ã¦ã„ã‚‹

### DQNå®Ÿè£…ã‚¹ã‚­ãƒ«

  * [ ] PyTorchã§ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å®šç¾©ã§ãã‚‹
  * [ ] çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡ã®å®Ÿè£…ãŒã§ãã‚‹
  * [ ] ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ›´æ–°ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’è¨­å®šã§ãã‚‹
  * [ ] Îµã®æ¸›è¡°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é©åˆ‡ã«è¨­å®šã§ãã‚‹

### ææ–™æŽ¢ç´¢ç‰¹æœ‰ã®æ³¨æ„ç‚¹

  * [ ] çµ„æˆãƒ™ãƒ¼ã‚¹ vs æ§‹é€ ãƒ™ãƒ¼ã‚¹è¨˜è¿°å­ã®é•ã„ã‚’ç†è§£ã—ã¦ã„ã‚‹
  * [ ] ææ–™å¤šå½¢ï¼ˆpolymorphsï¼‰ã‚’åŒºåˆ¥ã™ã‚‹çŠ¶æ…‹è¨­è¨ˆãŒã§ãã‚‹
  * [ ] ææ–™ç‰¹æ€§ã®ç‰©ç†çš„åˆ¶ç´„ã‚’å ±é…¬é–¢æ•°ã«çµ„ã¿è¾¼ã‚ã‚‹
  * [ ] DFTè¨ˆç®—ã‚³ã‚¹ãƒˆã‚’è€ƒæ…®ã—ãŸæŽ¢ç´¢æˆ¦ç•¥ã‚’è¨­è¨ˆã§ãã‚‹

### ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚­ãƒ«

  * [ ] Qå€¤ãŒç™ºæ•£ã™ã‚‹å ´åˆã®åŽŸå› ã‚’ç‰¹å®šã§ãã‚‹
  * [ ] æŽ¢ç´¢ãŒé€²ã¾ãªã„å ´åˆã®å¯¾å‡¦æ³•ã‚’çŸ¥ã£ã¦ã„ã‚‹
  * [ ] å ±é…¬ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å•é¡Œã‚’æ¤œå‡ºãƒ»ä¿®æ­£ã§ãã‚‹
  * [ ] ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ã‚’ä½“ç³»çš„ã«èª¿æŸ»ã§ãã‚‹

### ã‚³ãƒ¼ãƒ‰å“è³ª

  * [ ] å…¨ã‚³ãƒ¼ãƒ‰ã«ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’è¨˜è¼‰ã—ã¦ã„ã‚‹
  * [ ] ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’å›ºå®šã—ã¦å†ç¾æ€§ã‚’ç¢ºä¿ã—ã¦ã„ã‚‹
  * [ ] ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ãƒ»åž‹ãƒ»ç¯„å›²ã®æ¤œè¨¼ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ã„ã‚‹
  * [ ] ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆä¾‹å¤–å‡¦ç†ï¼‰ã‚’å®Ÿè£…ã—ã¦ã„ã‚‹

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**é”æˆåº¦80%æœªæº€ã®å ´åˆ:** \- æœ¬ç« ã‚’å†èª­ã—ã€æ¼”ç¿’å•é¡Œã‚’è§£ãç›´ã™ \- ç°¡å˜ãªã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ã§æ‰‹ã‚’å‹•ã‹ã—ã¦å®Ÿè£…çµŒé¨“ã‚’ç©ã‚€

**é”æˆåº¦80-95%ã®å ´åˆ:** \- ç¬¬2ç« ï¼ˆæ–¹ç­–å‹¾é…æ³•ï¼‰ã«é€²ã‚€æº–å‚™OK \- DQNã®ã‚³ãƒ¼ãƒ‰ã‚’è‡ªåˆ†ã§æœ€åˆã‹ã‚‰å®Ÿè£…ã—ã¦ã¿ã‚‹

**é”æˆåº¦95%ä»¥ä¸Šã®å ´åˆ:** \- ç¬¬2ç« ã«é€²ã¿ã€ã‚ˆã‚Šé«˜åº¦ãªæ‰‹æ³•ã‚’å­¦ã¶ \- å®Ÿéš›ã®ææ–™æŽ¢ç´¢ã‚¿ã‚¹ã‚¯ã§RLã‚’è©¦ã™

* * *

## å‚è€ƒæ–‡çŒ®

  1. Mnih et al. "Playing Atari with Deep Reinforcement Learning" _arXiv_ (2013) - DQNåŽŸè«–æ–‡
  2. Sutton & Barto "Reinforcement Learning: An Introduction" MIT Press (2018) - RLæ•™ç§‘æ›¸
  3. Zhou et al. "Optimization of molecules via deep reinforcement learning" _Scientific Reports_ (2019)
  4. Ling et al. "High-dimensional materials and process optimization using data-driven experimental design with well-calibrated uncertainty estimates" _Integrating Materials and Manufacturing Innovation_ (2017)

* * *

**æ¬¡ç« ** : [ç¬¬2ç« : å¼·åŒ–å­¦ç¿’ã®åŸºç¤Žç†è«–](<chapter-2.html>)
