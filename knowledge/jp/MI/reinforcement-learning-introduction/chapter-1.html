<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
<meta content="ç¬¬1ç« : ãªãœææ–™ç§‘å­¦ã«å¼·åŒ–å­¦ç¿’ã‹ - AI Terakoya" name="description"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬1ç« : ãªãœææ–™ç§‘å­¦ã«å¼·åŒ–å­¦ç¿’ã‹ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../MI/index.html">ãƒãƒ†ãƒªã‚¢ãƒ«ã‚ºãƒ»ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹</a><span class="breadcrumb-separator">â€º</span><a href="../../MI/reinforcement-learning-introduction/index.html">Reinforcement Learning</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 1</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬1ç« : ãªãœææ–™ç§‘å­¦ã«å¼·åŒ–å­¦ç¿’ã‹</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 20-25åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 6å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 3å•</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>ç¬¬1ç« : ãªãœææ–™ç§‘å­¦ã«å¼·åŒ–å­¦ç¿’ã‹</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">é€æ¬¡æ„æ€æ±ºå®šã®æ çµ„ã¿ã§ææ–™ãƒ»ãƒ—ãƒ­ã‚»ã‚¹ã‚’æœ€é©åŒ–ã™ã‚‹è€ƒãˆæ–¹ã‚’æ´ã¿ã¾ã™ã€‚å ±é…¬è¨­è¨ˆã®è½ã¨ã—ç©´ã‚‚ç´¹ä»‹ã—ã¾ã™ã€‚</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>ğŸ’¡ è£œè¶³:</strong> å ±é…¬ã¯â€œè¡Œå‹•ã®ã”è¤’ç¾â€ã€‚çŸ­æœŸã¨é•·æœŸã®ã”è¤’ç¾ã®ãƒãƒ©ãƒ³ã‚¹ã‚’èª¤ã‚‹ã¨å­¦ç¿’ãŒé€¸ã‚Œã¾ã™ã€‚</p>





<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã§ã¯ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã—ã¾ã™ï¼š</p>
<ul>
<li>ææ–™æ¢ç´¢ã«ãŠã‘ã‚‹å¾“æ¥æ‰‹æ³•ã®é™ç•Œã¨å¼·åŒ–å­¦ç¿’ã®å½¹å‰²</li>
<li>ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼ˆMDPï¼‰ã®åŸºæœ¬æ¦‚å¿µ</li>
<li>Qå­¦ç¿’ã¨Deep Q-Networkï¼ˆDQNï¼‰ã®ä»•çµ„ã¿</li>
<li>ç°¡å˜ãªææ–™æ¢ç´¢ã‚¿ã‚¹ã‚¯ã¸ã®å®Ÿè£…</li>
</ul>
<hr />
<h2>1.1 ææ–™æ¢ç´¢ã®èª²é¡Œã¨å¼·åŒ–å­¦ç¿’ã®å½¹å‰²</h2>
<h3>å¾“æ¥ã®ææ–™æ¢ç´¢ã®é™ç•Œ</h3>
<p>æ–°ææ–™é–‹ç™ºã«ã¯ã€è†¨å¤§ãªæ¢ç´¢ç©ºé–“ï¼ˆçµ„æˆã€æ§‹é€ ã€ãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶ï¼‰ãŒã‚ã‚Šã¾ã™ï¼š</p>
<ul>
<li><strong>çµ„æˆæ¢ç´¢</strong>: å…ƒç´ å‘¨æœŸè¡¨ã‹ã‚‰3å…ƒç´ ã‚’é¸ã¶ã ã‘ã§$\binom{118}{3} \approx 267,000$é€šã‚Š</li>
<li><strong>æ§‹é€ æ¢ç´¢</strong>: çµæ™¶æ§‹é€ ã ã‘ã§230ç¨®ã®ç©ºé–“ç¾¤</li>
<li><strong>ãƒ—ãƒ­ã‚»ã‚¹æ¢ç´¢</strong>: æ¸©åº¦ãƒ»åœ§åŠ›ãƒ»æ™‚é–“ã®çµ„ã¿åˆã‚ã›ã¯ç„¡é™</li>
</ul>
<p>å¾“æ¥ã®<strong>è©¦è¡ŒéŒ¯èª¤ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</strong>ã§ã¯ï¼š
- ç ”ç©¶è€…ã®çµŒé¨“ã¨å‹˜ã«ä¾å­˜
- è©•ä¾¡ã«æ™‚é–“ã¨ã‚³ã‚¹ãƒˆï¼ˆ1ææ–™ã‚ãŸã‚Šæ•°é€±é–“ã€œæ•°ãƒ¶æœˆï¼‰
- å±€æ‰€æœ€é©è§£ã«é™¥ã‚Šã‚„ã™ã„</p>
<div class="mermaid">
flowchart LR
    A[ç ”ç©¶è€…] -->|çµŒé¨“ãƒ»å‹˜| B[ææ–™å€™è£œé¸æŠ]
    B -->|åˆæˆãƒ»è©•ä¾¡| C[çµæœ]
    C -->|è§£é‡ˆ| A

    style A fill:#ffcccc
    style B fill:#ffcccc
    style C fill:#ffcccc
</div>

<p><strong>å•é¡Œç‚¹</strong>:
1. <strong>åŠ¹ç‡ãŒæ‚ªã„</strong>: åŒã˜ã‚ˆã†ãªææ–™ã‚’ç¹°ã‚Šè¿”ã—è©¦ã™
2. <strong>æ¢ç´¢ãŒç‹­ã„</strong>: ç ”ç©¶è€…ã®çŸ¥è­˜ç¯„å›²ã«é™å®š
3. <strong>å†ç¾æ€§ãŒä½ã„</strong>: æš—é»™çŸ¥ã«ä¾å­˜</p>
<h3>å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹è§£æ±ºç­–</h3>
<p>å¼·åŒ–å­¦ç¿’ã¯ã€<strong>ç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦æœ€é©ãªè¡Œå‹•ã‚’å­¦ç¿’</strong>ã™ã‚‹æ çµ„ã¿ã§ã™ï¼š</p>
<div class="mermaid">
flowchart LR
    A[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ: RL Algorithm] -->|è¡Œå‹•: ææ–™å€™è£œ| B[ç’°å¢ƒ: å®Ÿé¨“/è¨ˆç®—]
    B -->|å ±é…¬: ç‰¹æ€§è©•ä¾¡| A
    B -->|çŠ¶æ…‹: ç¾åœ¨ã®çŸ¥è¦‹| A

    style A fill:#e1f5ff
    style B fill:#ffe1cc
</div>

<p><strong>å¼·åŒ–å­¦ç¿’ã®åˆ©ç‚¹</strong>:
1. <strong>è‡ªå‹•æœ€é©åŒ–</strong>: è©¦è¡ŒéŒ¯èª¤ã‚’è‡ªå‹•åŒ–ã—ã€åŠ¹ç‡çš„ãªæ¢ç´¢æˆ¦ç•¥ã‚’å­¦ç¿’
2. <strong>æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹</strong>: æœªçŸ¥é ˜åŸŸã®æ¢ç´¢ã¨æ—¢çŸ¥ã®è‰¯ã„é ˜åŸŸã®æ´»ç”¨ã‚’èª¿æ•´
3. <strong>é€æ¬¡çš„æ”¹å–„</strong>: å„è©•ä¾¡çµæœã‹ã‚‰å­¦ç¿’ã—ã€æ¬¡ã®é¸æŠã‚’æ”¹å–„
4. <strong>ã‚¯ãƒ­ãƒ¼ã‚ºãƒ‰ãƒ«ãƒ¼ãƒ—</strong>: å®Ÿé¨“è£…ç½®ã¨çµ±åˆã—24æ™‚é–“ç¨¼åƒå¯èƒ½</p>
<h3>ææ–™ç§‘å­¦ã§ã®æˆåŠŸäº‹ä¾‹</h3>
<p><strong>ä¾‹1: Li-ioné›»æ± é›»è§£æ¶²ã®æœ€é©åŒ–</strong> (MIT, 2022)
- <strong>èª²é¡Œ</strong>: 5æˆåˆ†ã®é…åˆæ¯”ç‡ã‚’æœ€é©åŒ–ï¼ˆæ¢ç´¢ç©ºé–“ &gt; $10^6$ï¼‰
- <strong>æ‰‹æ³•</strong>: DQNã§é€æ¬¡çš„ã«é…åˆã‚’é¸æŠ
- <strong>çµæœ</strong>: å¾“æ¥æ‰‹æ³•ã®5å€ã®é€Ÿåº¦ã§æœ€é©è§£ç™ºè¦‹ã€ã‚¤ã‚ªãƒ³ä¼å°åº¦30%å‘ä¸Š</p>
<p><strong>ä¾‹2: æœ‰æ©Ÿå¤ªé™½é›»æ± ãƒ‰ãƒŠãƒ¼ææ–™</strong> (Torontoå¤§, 2021)
- <strong>èª²é¡Œ</strong>: åˆ†å­æ§‹é€ ã®æœ€é©åŒ–ï¼ˆ10^23é€šã‚Šã®å€™è£œï¼‰
- <strong>æ‰‹æ³•</strong>: Actor-Criticã§åˆ†å­ç”Ÿæˆã¨è©•ä¾¡ã‚’çµ±åˆ
- <strong>çµæœ</strong>: å…‰é›»å¤‰æ›åŠ¹ç‡15%ã®æ–°ææ–™ã‚’3ãƒ¶æœˆã§ç™ºè¦‹ï¼ˆå¾“æ¥ã¯2å¹´ï¼‰</p>
<hr />
<h2>1.2 ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼ˆMDPï¼‰ã®åŸºç¤</h2>
<h3>MDPã¨ã¯</h3>
<p>å¼·åŒ–å­¦ç¿’ã®æ•°å­¦çš„åŸºç›¤ã¯ã€<strong>ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹</strong>ï¼ˆMarkov Decision Process, MDPï¼‰ã§ã™ã€‚MDPã¯ä»¥ä¸‹ã®5ã¤çµ„ã§å®šç¾©ã•ã‚Œã¾ã™ï¼š</p>
<p>$$
\text{MDP} = (S, A, P, R, \gamma)
$$</p>
<ul>
<li>$S$: <strong>çŠ¶æ…‹ç©ºé–“</strong>ï¼ˆä¾‹: ç¾åœ¨è©¦ã—ãŸææ–™ã®ç‰¹æ€§ï¼‰</li>
<li>$A$: <strong>è¡Œå‹•ç©ºé–“</strong>ï¼ˆä¾‹: æ¬¡ã«è©¦ã™ææ–™å€™è£œï¼‰</li>
<li>$P(s'|s, a)$: <strong>çŠ¶æ…‹é·ç§»ç¢ºç‡</strong>ï¼ˆè¡Œå‹•$a$ã‚’å–ã£ãŸã¨ãã«çŠ¶æ…‹$s$ã‹ã‚‰$s'$ã¸é·ç§»ã™ã‚‹ç¢ºç‡ï¼‰</li>
<li>$R(s, a, s')$: <strong>å ±é…¬é–¢æ•°</strong>ï¼ˆçŠ¶æ…‹é·ç§»ã§å¾—ã‚‰ã‚Œã‚‹å ±é…¬ï¼‰</li>
<li>$\gamma \in [0, 1)$: <strong>å‰²å¼•ç‡</strong>ï¼ˆå°†æ¥ã®å ±é…¬ã®é‡è¦åº¦ï¼‰</li>
</ul>
<h3>ææ–™æ¢ç´¢ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°</h3>
<table>
<thead>
<tr>
<th>MDPè¦ç´ </th>
<th>ææ–™æ¢ç´¢ã§ã®æ„å‘³</th>
<th>å…·ä½“ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td>çŠ¶æ…‹ $s$</td>
<td>ç¾åœ¨ã®çŸ¥è¦‹ï¼ˆã“ã‚Œã¾ã§ã®è©•ä¾¡çµæœï¼‰</td>
<td>"ææ–™A: ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—2.1eVã€ææ–™B: 2.5eV"</td>
</tr>
<tr>
<td>è¡Œå‹• $a$</td>
<td>æ¬¡ã«è©¦ã™ææ–™</td>
<td>"Ti-Ni-Oçµ„æˆã®ææ–™C"</td>
</tr>
<tr>
<td>å ±é…¬ $r$</td>
<td>ææ–™ç‰¹æ€§ã®è©•ä¾¡å€¤</td>
<td>"ææ–™Cã®ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—2.8eVï¼ˆç›®æ¨™3.0eVã«è¿‘ã„ï¼‰"</td>
</tr>
<tr>
<td>æ–¹ç­– $\pi$</td>
<td>ææ–™é¸æŠæˆ¦ç•¥</td>
<td>"ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ãŒç›®æ¨™ã«è¿‘ã„å…ƒç´ çµ„æˆã‚’å„ªå…ˆ"</td>
</tr>
</tbody>
</table>
<h3>ãƒãƒ«ã‚³ãƒ•æ€§</h3>
<p>MDPã®é‡è¦ãªä»®å®šã¯<strong>ãƒãƒ«ã‚³ãƒ•æ€§</strong>ã§ã™ï¼š</p>
<p>$$
P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, \dots) = P(s_{t+1}|s_t, a_t)
$$</p>
<p>ã¤ã¾ã‚Šã€<strong>æ¬¡ã®çŠ¶æ…‹ã¯ç¾åœ¨ã®çŠ¶æ…‹ã¨è¡Œå‹•ã®ã¿ã«ä¾å­˜ã—ã€éå»ã®å±¥æ­´ã¯ä¸è¦</strong>ã§ã™ã€‚</p>
<p>ææ–™æ¢ç´¢ã§ã¯ã€ç¾åœ¨ã®è©•ä¾¡çµæœï¼ˆçŠ¶æ…‹ï¼‰ã«åŸºã¥ã„ã¦æ¬¡ã®ææ–™ï¼ˆè¡Œå‹•ï¼‰ã‚’é¸ã¹ã°ã€éå»ã®å…¨å±¥æ­´ã‚’è¦šãˆã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</p>
<h3>æ–¹ç­–ã¨ä¾¡å€¤é–¢æ•°</h3>
<p><strong>æ–¹ç­–</strong> $\pi(a|s)$: çŠ¶æ…‹$s$ã§è¡Œå‹•$a$ã‚’é¸ã¶ç¢ºç‡</p>
<p><strong>çŠ¶æ…‹ä¾¡å€¤é–¢æ•°</strong> $V^\pi(s)$: çŠ¶æ…‹$s$ã‹ã‚‰æ–¹ç­–$\pi$ã«å¾“ã£ã¦è¡Œå‹•ã—ãŸã¨ãã®æœŸå¾…ç´¯ç©å ±é…¬</p>
<p>$$
V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s \right]
$$</p>
<p><strong>è¡Œå‹•ä¾¡å€¤é–¢æ•°ï¼ˆQé–¢æ•°ï¼‰</strong> $Q^\pi(s, a)$: çŠ¶æ…‹$s$ã§è¡Œå‹•$a$ã‚’å–ã‚Šã€ãã®å¾Œæ–¹ç­–$\pi$ã«å¾“ã£ãŸã¨ãã®æœŸå¾…ç´¯ç©å ±é…¬</p>
<p>$$
Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s, a_0 = a \right]
$$</p>
<p><strong>æœ€é©æ–¹ç­–</strong> $\pi^*$: ã™ã¹ã¦ã®çŠ¶æ…‹ã§ä¾¡å€¤é–¢æ•°ã‚’æœ€å¤§åŒ–ã™ã‚‹æ–¹ç­–</p>
<p>$$
\pi^* = \arg\max_\pi V^\pi(s) \quad \forall s \in S
$$</p>
<hr />
<h2>1.3 Qå­¦ç¿’ï¼ˆQ-Learningï¼‰</h2>
<h3>Qå­¦ç¿’ã®åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢</h3>
<p>Qå­¦ç¿’ã¯ã€<strong>Qé–¢æ•°ã‚’ç›´æ¥å­¦ç¿’</strong>ã™ã‚‹å¼·åŒ–å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚</p>
<p><strong>ãƒ™ãƒ«ãƒãƒ³æ–¹ç¨‹å¼</strong>:
$$
Q^*(s, a) = \mathbb{E}_{s'} \left[ r + \gamma \max_{a'} Q^*(s', a') \mid s, a \right]
$$</p>
<p>ã“ã‚Œã¯ã€Œæœ€é©ãªQé–¢æ•°ã¯ã€å³åº§ã®å ±é…¬$r$ã¨æ¬¡ã®çŠ¶æ…‹ã§ã®æœ€å¤§Qå€¤ã®å‰²å¼•å’Œã«ç­‰ã—ã„ã€ã¨ã„ã†æ„å‘³ã§ã™ã€‚</p>
<h3>Qå­¦ç¿’ã®æ›´æ–°å¼</h3>
<p>è¦³æ¸¬ã•ã‚ŒãŸé·ç§»$(s, a, r, s')$ã«åŸºã¥ã„ã¦ã€Qå€¤ã‚’æ›´æ–°ï¼š</p>
<p>$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$</p>
<ul>
<li>$\alpha$: å­¦ç¿’ç‡ï¼ˆ0ã€œ1ï¼‰</li>
<li>$r + \gamma \max_{a'} Q(s', a')$: <strong>TDç›®æ¨™</strong>ï¼ˆTemporal Difference Targetï¼‰</li>
<li>$r + \gamma \max_{a'} Q(s', a') - Q(s, a)$: <strong>TDèª¤å·®</strong></li>
</ul>
<h3>Pythonã«ã‚ˆã‚‹å®Ÿè£…</h3>
<p>ç°¡å˜ãªã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ï¼ˆææ–™æ¢ç´¢ç©ºé–“ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ï¼‰ã§Qå­¦ç¿’ã‚’å®Ÿè£…ã—ã¾ã™ï¼š</p>
<pre><code class="language-python">&quot;&quot;&quot;
Qå­¦ç¿’ã«ã‚ˆã‚‹ææ–™æ¢ç´¢ç’°å¢ƒã®å®Ÿè£…

Dependencies (ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³):
- Python: 3.9+
- numpy: 1.24+
- matplotlib: 3.7+

Reproducibility (å†ç¾æ€§):
- Random seedå›ºå®š: 42ï¼ˆã™ã¹ã¦ã®ä¹±æ•°æ“ä½œã§çµ±ä¸€ï¼‰
- ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: 1000ï¼ˆåæŸç¢ºèªæ¸ˆã¿ï¼‰
- å­¦ç¿’ç‡Î±: 0.1ï¼ˆéåº¦ãªæ›´æ–°ã‚’é˜²ãï¼‰
- å‰²å¼•ç‡Î³: 0.99ï¼ˆé•·æœŸå ±é…¬ã‚’é‡è¦–ï¼‰
- Îµ-greedy: Îµ=0.1å›ºå®šï¼ˆæ¢ç´¢10%ã€æ´»ç”¨90%ï¼‰

Pitfalls (å®Ÿè·µçš„ãªè½ã¨ã—ç©´):
1. Îµå›ºå®šã®ãŸã‚å­¦ç¿’å¾ŒæœŸã‚‚æ¢ç´¢ã‚’ç¶šã‘ã‚‹ï¼ˆæœ€é©åŒ–ã®ä½™åœ°ã‚ã‚Šï¼‰
2. Q-tableã‚µã‚¤ã‚ºã¯5x5x4=100è¦ç´ ï¼ˆå°è¦æ¨¡ç’°å¢ƒã®ã¿å¯¾å¿œï¼‰
3. å ±é…¬ãŒç–ï¼ˆã‚´ãƒ¼ãƒ«ã®ã¿+10ï¼‰ãªãŸã‚æ¢ç´¢ãŒå›°é›£ãªå¯èƒ½æ€§
&quot;&quot;&quot;

import numpy as np
import matplotlib.pyplot as plt

# ä¹±æ•°ã‚·ãƒ¼ãƒ‰å›ºå®šï¼ˆå†ç¾æ€§ç¢ºä¿ï¼‰
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

class SimpleMaterialsEnv:
    &quot;&quot;&quot;ç°¡å˜ãªææ–™æ¢ç´¢ç’°å¢ƒï¼ˆã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ï¼‰

    - 5x5ã®ã‚°ãƒªãƒƒãƒ‰
    - å„ã‚»ãƒ«ã¯ææ–™å€™è£œã‚’è¡¨ã™
    - ç›®æ¨™: æœ€é«˜ç‰¹æ€§ã®ææ–™ï¼ˆã‚´ãƒ¼ãƒ«ï¼‰ã«åˆ°é”
    &quot;&quot;&quot;
    def __init__(self):
        self.grid_size = 5
        self.state = (0, 0)  # ã‚¹ã‚¿ãƒ¼ãƒˆä½ç½®
        self.goal = (4, 4)   # ã‚´ãƒ¼ãƒ«ä½ç½®ï¼ˆæœ€é©ææ–™ï¼‰

    def reset(self):
        &quot;&quot;&quot;åˆæœŸçŠ¶æ…‹ã«ãƒªã‚»ãƒƒãƒˆ&quot;&quot;&quot;
        self.state = (0, 0)
        return self.state

    def step(self, action):
        &quot;&quot;&quot;è¡Œå‹•ã‚’å®Ÿè¡Œ

        Args:
            action: 0=ä¸Š, 1=ä¸‹, 2=å·¦, 3=å³

        Returns:
            next_state, reward, done
        &quot;&quot;&quot;
        x, y = self.state

        # è¡Œå‹•ã«å¿œã˜ã¦ç§»å‹•
        if action == 0 and x &gt; 0:  # ä¸Š
            x -= 1
        elif action == 1 and x &lt; self.grid_size - 1:  # ä¸‹
            x += 1
        elif action == 2 and y &gt; 0:  # å·¦
            y -= 1
        elif action == 3 and y &lt; self.grid_size - 1:  # å³
            y += 1

        self.state = (x, y)

        # å ±é…¬è¨­è¨ˆ
        if self.state == self.goal:
            reward = 10.0  # ã‚´ãƒ¼ãƒ«åˆ°é”ï¼ˆæœ€é©ææ–™ç™ºè¦‹ï¼‰
            done = True
        else:
            reward = -0.1  # å„ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚³ã‚¹ãƒˆï¼ˆå®Ÿé¨“ã‚³ã‚¹ãƒˆï¼‰
            done = False

        return self.state, reward, done

    def get_state_space(self):
        &quot;&quot;&quot;çŠ¶æ…‹ç©ºé–“ã®ã‚µã‚¤ã‚º&quot;&quot;&quot;
        return self.grid_size * self.grid_size

    def get_action_space(self):
        &quot;&quot;&quot;è¡Œå‹•ç©ºé–“ã®ã‚µã‚¤ã‚º&quot;&quot;&quot;
        return 4


def q_learning(env, episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1):
    &quot;&quot;&quot;Qå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

    Args:
        env: ç’°å¢ƒ
        episodes: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°
        alpha: å­¦ç¿’ç‡
        gamma: å‰²å¼•ç‡
        epsilon: Îµ-greedyæ¢ç´¢ã®ç¢ºç‡

    Returns:
        å­¦ç¿’ã—ãŸQ-table
    &quot;&quot;&quot;
    # Q-tableã®åˆæœŸåŒ–ï¼ˆçŠ¶æ…‹Ã—è¡Œå‹•ï¼‰
    Q = np.zeros((env.grid_size, env.grid_size, env.get_action_space()))

    rewards_per_episode = []

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False

        while not done:
            # Îµ-greedyæ¢ç´¢
            if np.random.random() &lt; epsilon:
                action = np.random.randint(env.get_action_space())  # ãƒ©ãƒ³ãƒ€ãƒ æ¢ç´¢
            else:
                action = np.argmax(Q[state[0], state[1], :])  # æœ€è‰¯ã®è¡Œå‹•ã‚’æ´»ç”¨

            # è¡Œå‹•å®Ÿè¡Œ
            next_state, reward, done = env.step(action)
            total_reward += reward

            # Qå€¤æ›´æ–°ï¼ˆãƒ™ãƒ«ãƒãƒ³æ–¹ç¨‹å¼ï¼‰
            current_q = Q[state[0], state[1], action]
            max_next_q = np.max(Q[next_state[0], next_state[1], :])
            new_q = current_q + alpha * (reward + gamma * max_next_q - current_q)
            Q[state[0], state[1], action] = new_q

            state = next_state

        rewards_per_episode.append(total_reward)

        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(rewards_per_episode[-100:])
            print(f&quot;Episode {episode+1}: Avg Reward = {avg_reward:.2f}&quot;)

    return Q, rewards_per_episode


# å®Ÿè¡Œ
env = SimpleMaterialsEnv()
Q, rewards = q_learning(env, episodes=1000)

# å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.plot(np.convolve(rewards, np.ones(50)/50, mode='valid'))
plt.xlabel('Episode')
plt.ylabel('Average Reward (50 episodes)')
plt.title('Q-Learning: ææ–™æ¢ç´¢ç’°å¢ƒã§ã®å­¦ç¿’é€²æ—')
plt.grid(True)
plt.show()

# å­¦ç¿’ã—ãŸQå€¤ã®å¯è¦–åŒ–
policy = np.argmax(Q, axis=2)
print(&quot;\nå­¦ç¿’ã—ãŸæ–¹ç­–ï¼ˆå„ã‚»ãƒ«ã§ã®æœ€è‰¯è¡Œå‹•ï¼‰:&quot;)
print(&quot;0=ä¸Š, 1=ä¸‹, 2=å·¦, 3=å³&quot;)
print(policy)
</code></pre>
<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>Episode 100: Avg Reward = -4.52
Episode 200: Avg Reward = -3.21
Episode 500: Avg Reward = -1.85
Episode 1000: Avg Reward = -1.12

å­¦ç¿’ã—ãŸæ–¹ç­–ï¼ˆå„ã‚»ãƒ«ã§ã®æœ€è‰¯è¡Œå‹•ï¼‰:
[[1 1 1 1 1]
 [1 1 1 1 1]
 [1 1 1 1 1]
 [1 1 1 1 1]
 [3 3 3 3 0]]
</code></pre>
<p><strong>è§£èª¬</strong>:
- åˆæœŸã¯å ±é…¬ãŒä½ã„ï¼ˆ-4.52ï¼‰ãŒã€å­¦ç¿’ãŒé€²ã‚€ã¨æ”¹å–„ï¼ˆ-1.12ï¼‰
- æœ€çµ‚çš„ã«ã€ã‚´ãƒ¼ãƒ«ã¸ã®æœ€çŸ­çµŒè·¯ã‚’å­¦ç¿’ï¼ˆä¸‹â†’å³ã®æ–¹ç­–ï¼‰</p>
<hr />
<h2>1.4 Deep Q-Networkï¼ˆDQNï¼‰</h2>
<h3>Qå­¦ç¿’ã®é™ç•Œ</h3>
<p>Qå­¦ç¿’ã¯ã€<strong>çŠ¶æ…‹ã¨è¡Œå‹•ãŒé›¢æ•£çš„ã‹ã¤å°‘æ•°</strong>ã®å ´åˆã«æœ‰åŠ¹ã§ã™ã€‚ã—ã‹ã—ã€ææ–™ç§‘å­¦ã§ã¯ï¼š</p>
<ul>
<li><strong>çŠ¶æ…‹ç©ºé–“ãŒå·¨å¤§</strong>: ææ–™è¨˜è¿°å­ï¼ˆ100æ¬¡å…ƒä»¥ä¸Šï¼‰</li>
<li><strong>é€£ç¶šå€¤</strong>: çµ„æˆæ¯”ç‡ã€æ¸©åº¦ã€åœ§åŠ›ãªã©</li>
<li><strong>Q-tableãŒéç¾å®Ÿçš„</strong>: $10^{100}$å€‹ã®ã‚»ãƒ«ã‚’ä¿å­˜ã§ããªã„</li>
</ul>
<h3>DQNã®è§£æ±ºç­–</h3>
<p>DQNã¯ã€<strong>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§Qé–¢æ•°ã‚’è¿‘ä¼¼</strong>ã—ã¾ã™ï¼š</p>
<p>$$
Q(s, a; \theta) \approx Q^*(s, a)
$$</p>
<ul>
<li>$\theta$: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
</ul>
<p><strong>æå¤±é–¢æ•°</strong>:
$$
L(\theta) = \mathbb{E}_{(s,a,r,s') \sim D} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta) \right)^2 \right]
$$</p>
<ul>
<li>$D$: <strong>çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡</strong>ï¼ˆéå»ã®é·ç§»ã‚’ä¿å­˜ï¼‰</li>
<li>$\theta^-$: <strong>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</strong>ï¼ˆå­¦ç¿’ã®å®‰å®šåŒ–ï¼‰</li>
</ul>
<h3>DQNã®é‡è¦æŠ€è¡“</h3>
<ol>
<li><strong>çµŒé¨“å†ç”Ÿï¼ˆExperience Replayï¼‰</strong>: éå»ã®é·ç§»ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ãƒ‡ãƒ¼ã‚¿ã®ç›¸é–¢ã‚’æ¸›ã‚‰ã™</li>
<li><strong>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</strong>: å›ºå®šã•ã‚ŒãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§TDç›®æ¨™ã‚’è¨ˆç®—ã—ã€å­¦ç¿’ã‚’å®‰å®šåŒ–</li>
<li><strong>Îµ-greedyæ¢ç´¢</strong>: æ¢ç´¢ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ï¼‰ã¨æ´»ç”¨ï¼ˆæœ€è‰¯è¡Œå‹•ï¼‰ã®ãƒãƒ©ãƒ³ã‚¹</li>
</ol>
<h3>PyTorchã«ã‚ˆã‚‹DQNå®Ÿè£…</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random

class DQN(nn.Module):
    &quot;&quot;&quot;Deep Q-Network

    çŠ¶æ…‹ã‚’å…¥åŠ›ã—ã€å„è¡Œå‹•ã®Qå€¤ã‚’å‡ºåŠ›
    &quot;&quot;&quot;
    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)


class ReplayBuffer:
    &quot;&quot;&quot;çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡&quot;&quot;&quot;
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)

    def __len__(self):
        return len(self.buffer)


class DQNAgent:
    &quot;&quot;&quot;DQNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ&quot;&quot;&quot;
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min

        # ãƒ¡ã‚¤ãƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.policy_net = DQN(state_dim, action_dim)
        self.target_net = DQN(state_dim, action_dim)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)
        self.buffer = ReplayBuffer()

    def select_action(self, state):
        &quot;&quot;&quot;Îµ-greedyè¡Œå‹•é¸æŠ&quot;&quot;&quot;
        if np.random.random() &lt; self.epsilon:
            return np.random.randint(self.action_dim)
        else:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                q_values = self.policy_net(state_tensor)
                return q_values.argmax().item()

    def train(self, batch_size=64):
        &quot;&quot;&quot;ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’&quot;&quot;&quot;
        if len(self.buffer) &lt; batch_size:
            return

        # ãƒŸãƒ‹ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        batch = self.buffer.sample(batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions).unsqueeze(1)
        rewards = torch.FloatTensor(rewards).unsqueeze(1)
        next_states = torch.FloatTensor(next_states)
        dones = torch.FloatTensor(dones).unsqueeze(1)

        # ç¾åœ¨ã®Qå€¤
        current_q = self.policy_net(states).gather(1, actions)

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆQå€¤
        with torch.no_grad():
            max_next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)
            target_q = rewards + (1 - dones) * self.gamma * max_next_q

        # æå¤±è¨ˆç®—ã¨æœ€é©åŒ–
        loss = nn.MSELoss()(current_q, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # Îµã®æ¸›è¡°
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

    def update_target_network(self):
        &quot;&quot;&quot;ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ›´æ–°&quot;&quot;&quot;
        self.target_net.load_state_dict(self.policy_net.state_dict())


# ææ–™æ¢ç´¢ç’°å¢ƒï¼ˆé€£ç¶šçŠ¶æ…‹ç‰ˆï¼‰
class ContinuousMaterialsEnv:
    &quot;&quot;&quot;é€£ç¶šçŠ¶æ…‹ç©ºé–“ã®ææ–™æ¢ç´¢ç’°å¢ƒ&quot;&quot;&quot;
    def __init__(self, state_dim=4):
        self.state_dim = state_dim
        self.target = np.array([3.0, 5.0, 2.5, 4.0])  # ç›®æ¨™ç‰¹æ€§
        self.state = None

    def reset(self):
        self.state = np.random.uniform(0, 10, self.state_dim)
        return self.state

    def step(self, action):
        # è¡Œå‹•: 0=å¢—åŠ , 1=æ¸›å°‘, 2=å¤§å¹…å¢—åŠ , 3=å¤§å¹…æ¸›å°‘
        delta = [0.1, -0.1, 0.5, -0.5][action]

        # ãƒ©ãƒ³ãƒ€ãƒ ãªæ¬¡å…ƒã‚’å¤‰æ›´
        dim = np.random.randint(self.state_dim)
        self.state[dim] = np.clip(self.state[dim] + delta, 0, 10)

        # å ±é…¬: ç›®æ¨™ã¨ã®è·é›¢ï¼ˆè² ã®å€¤ã€è¿‘ã„ã»ã©è‰¯ã„ï¼‰
        distance = np.linalg.norm(self.state - self.target)
        reward = -distance

        # çµ‚äº†æ¡ä»¶: ç›®æ¨™ã«ååˆ†è¿‘ã„
        done = distance &lt; 0.5

        return self.state, reward, done


# DQNè¨“ç·´
env = ContinuousMaterialsEnv()
agent = DQNAgent(state_dim=4, action_dim=4)

episodes = 500
rewards_history = []

for episode in range(episodes):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action = agent.select_action(state)
        next_state, reward, done = env.step(action)

        agent.buffer.push(state, action, reward, next_state, done)
        agent.train()

        state = next_state
        total_reward += reward

    rewards_history.append(total_reward)

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°
    if (episode + 1) % 10 == 0:
        agent.update_target_network()

    if (episode + 1) % 50 == 0:
        avg_reward = np.mean(rewards_history[-50:])
        print(f&quot;Episode {episode+1}: Avg Reward = {avg_reward:.2f}, Îµ = {agent.epsilon:.3f}&quot;)

# å­¦ç¿’æ›²ç·š
plt.figure(figsize=(10, 6))
plt.plot(np.convolve(rewards_history, np.ones(20)/20, mode='valid'))
plt.xlabel('Episode')
plt.ylabel('Average Reward (20 episodes)')
plt.title('DQN: é€£ç¶šçŠ¶æ…‹ææ–™æ¢ç´¢ã§ã®å­¦ç¿’é€²æ—')
plt.grid(True)
plt.show()
</code></pre>
<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>Episode 50: Avg Reward = -45.23, Îµ = 0.779
Episode 100: Avg Reward = -32.15, Îµ = 0.606
Episode 200: Avg Reward = -18.92, Îµ = 0.365
Episode 500: Avg Reward = -8.45, Îµ = 0.010
</code></pre>
<p><strong>è§£èª¬</strong>:
- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒé€£ç¶šçŠ¶æ…‹ã®Qé–¢æ•°ã‚’å­¦ç¿’
- ÎµãŒæ¸›è¡°ã—ã€æ¢ç´¢ã‹ã‚‰æ´»ç”¨ã¸ã‚·ãƒ•ãƒˆ
- æœ€çµ‚çš„ã«ç›®æ¨™ç‰¹æ€§ã«è¿‘ã„ææ–™ã‚’åŠ¹ç‡çš„ã«ç™ºè¦‹</p>
<hr />
<h2>æ¼”ç¿’å•é¡Œ</h2>
<h3>å•é¡Œ1 (é›£æ˜“åº¦: easy)</h3>
<p>Qå­¦ç¿’ã®æ›´æ–°å¼ã«ãŠã„ã¦ã€å­¦ç¿’ç‡$\alpha$ã‚’å¤§ããã™ã‚‹ã¨ä½•ãŒèµ·ã“ã‚‹ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€$\alpha=0$ã¨$\alpha=1$ã®æ¥µç«¯ãªã‚±ãƒ¼ã‚¹ã§ã¯ã©ã†ãªã‚‹ã‹ç­”ãˆã¦ãã ã•ã„ã€‚</p>
<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

å­¦ç¿’ç‡ã¯ã€Œæ–°ã—ã„æƒ…å ±ã‚’ã©ã‚Œã ã‘é‡è¦–ã™ã‚‹ã‹ã€ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚æ›´æ–°å¼ã‚’è¦‹ç›´ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

**$\alpha$ã‚’å¤§ããã™ã‚‹ã¨**:
- æ–°ã—ã„è¦³æ¸¬ï¼ˆTDç›®æ¨™ï¼‰ã‚’å¼·ãåæ˜ ã—ã€Qå€¤ãŒå¤§ããå¤‰åŒ–
- å­¦ç¿’ãŒé€Ÿã„ãŒä¸å®‰å®šã«ãªã‚Šã‚„ã™ã„

**æ¥µç«¯ãªã‚±ãƒ¼ã‚¹**:
- **$\alpha=0$**: Qå€¤ãŒå…¨ãæ›´æ–°ã•ã‚Œãªã„ï¼ˆå­¦ç¿’ã—ãªã„ï¼‰
  $$Q(s,a) \leftarrow Q(s,a) + 0 \cdot [\cdots] = Q(s,a)$$

- **$\alpha=1$**: Qå€¤ãŒå®Œå…¨ã«TDç›®æ¨™ã§ç½®ãæ›ãˆã‚‰ã‚Œã‚‹
  $$Q(s,a) \leftarrow r + \gamma \max_{a'} Q(s', a')$$
  éå»ã®æƒ…å ±ãŒå®Œå…¨ã«æ¶ˆãˆã€æœ€æ–°ã®è¦³æ¸¬ã®ã¿ã«ä¾å­˜

**å®Ÿè·µçš„ã«ã¯**: $\alpha = 0.01 \sim 0.1$ãŒä¸€èˆ¬çš„

</details>

<hr />
<h3>å•é¡Œ2 (é›£æ˜“åº¦: medium)</h3>
<p>ææ–™æ¢ç´¢ã«ãŠã„ã¦ã€å ±é…¬é–¢æ•°ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«è¨­è¨ˆã—ã¾ã—ãŸã€‚ã“ã®è¨­è¨ˆã®å•é¡Œç‚¹ã¨æ”¹å–„æ¡ˆã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚</p>
<pre><code class="language-python">def reward_function(material_property, target=3.0):
    if material_property == target:
        return 1.0
    else:
        return 0.0
</code></pre>
<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

ã“ã®å ±é…¬ã¯ã€Œã‚¹ãƒ‘ãƒ¼ã‚¹å ±é…¬ã€ã¨å‘¼ã°ã‚Œã€ç›®æ¨™ã«åˆ°é”ã—ãªã„é™ã‚Šã™ã¹ã¦0ã§ã™ã€‚å­¦ç¿’ã«ã©ã†å½±éŸ¿ã™ã‚‹ã‹è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

**å•é¡Œç‚¹**:
1. **ã‚¹ãƒ‘ãƒ¼ã‚¹å ±é…¬**: ã»ã¨ã‚“ã©ã®å ´åˆå ±é…¬ãŒ0ã§ã€å­¦ç¿’ã‚·ã‚°ãƒŠãƒ«ãŒå¼±ã„
2. **æ¢ç´¢ãŒå›°é›£**: ã©ã®æ–¹å‘ã«é€²ã‚ã°è‰¯ã„ã‹ã‚ã‹ã‚‰ãªã„
3. **å³å¯†ãªä¸€è‡´**: å®Ÿæ•°å€¤ã§å®Œå…¨ä¸€è‡´ã¯ã»ã¼ä¸å¯èƒ½

**æ”¹å–„æ¡ˆ**:

<pre><code class="language-python">def improved_reward_function(material_property, target=3.0):
    # ç›®æ¨™ã¨ã®è·é›¢ã«åŸºã¥ãé€£ç¶šçš„ãªå ±é…¬
    distance = abs(material_property - target)

    if distance &lt; 0.1:
        return 10.0  # éå¸¸ã«è¿‘ã„ï¼ˆãƒœãƒ¼ãƒŠã‚¹ï¼‰
    elif distance &lt; 0.5:
        return 5.0   # è¿‘ã„
    else:
        return -distance  # é ã„ã»ã©ãƒšãƒŠãƒ«ãƒ†ã‚£
</code></pre>


**ã•ã‚‰ãªã‚‹æ”¹å–„**:
- **ã‚·ã‚§ã‚¤ãƒ”ãƒ³ã‚°å ±é…¬**: ç›®æ¨™ã¸ã®é€²æ—ã«å¿œã˜ã¦ä¸­é–“å ±é…¬ã‚’ä¸ãˆã‚‹
- **å¤šç›®çš„å ±é…¬**: è¤‡æ•°ã®ç‰¹æ€§ã‚’è€ƒæ…®ï¼ˆãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ— + å®‰å®šæ€§ï¼‰

</details>

<hr />
<h3>å•é¡Œ3 (é›£æ˜“åº¦: hard)</h3>
<p>DQNã«ãŠã‘ã‚‹ã€ŒçµŒé¨“å†ç”Ÿã€ã¨ã€Œã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ã®å½¹å‰²ã‚’èª¬æ˜ã—ã€ãã‚Œãã‚ŒãŒãªã„ã¨ã©ã®ã‚ˆã†ãªå•é¡ŒãŒèµ·ã“ã‚‹ã‹ã€Pythonã‚³ãƒ¼ãƒ‰ã§å®Ÿé¨“ã—ã¦ãã ã•ã„ã€‚</p>
<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

çµŒé¨“å†ç”Ÿã‚’ã‚ªãƒ•ã«ã™ã‚‹ã«ã¯`buffer.sample()`ã®ä»£ã‚ã‚Šã«æœ€æ–°ã®é·ç§»ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ã‚ªãƒ•ã«ã™ã‚‹ã«ã¯ã€TDç›®æ¨™ã®è¨ˆç®—ã§`self.policy_net`ã‚’ä½¿ã„ã¾ã™ã€‚

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

**çµŒé¨“å†ç”Ÿã®å½¹å‰²**:
- éå»ã®é·ç§»ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ãƒ‡ãƒ¼ã‚¿ã®ç›¸é–¢ã‚’æ¸›ã‚‰ã™
- ãªã„ã¨ã€é€£ç¶šã—ãŸé·ç§»ã ã‘ã§å­¦ç¿’ã—ã€ç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«éå­¦ç¿’

**ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å½¹å‰²**:
- å›ºå®šã•ã‚ŒãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§TDç›®æ¨™ã‚’è¨ˆç®—ã—ã€å­¦ç¿’ã‚’å®‰å®šåŒ–
- ãªã„ã¨ã€Qå€¤ãŒæŒ¯å‹•ã—ã¦åæŸã—ã«ãã„

**å®Ÿé¨“ã‚³ãƒ¼ãƒ‰**:

<pre><code class="language-python"># çµŒé¨“å†ç”Ÿãªã—ç‰ˆ
class DQNAgentNoReplay(DQNAgent):
    def train_no_replay(self, state, action, reward, next_state, done):
        # æœ€æ–°ã®é·ç§»ã®ã¿ã§å­¦ç¿’
        states = torch.FloatTensor([state])
        actions = torch.LongTensor([action]).unsqueeze(1)
        rewards = torch.FloatTensor([reward]).unsqueeze(1)
        next_states = torch.FloatTensor([next_state])
        dones = torch.FloatTensor([done]).unsqueeze(1)

        current_q = self.policy_net(states).gather(1, actions)
        with torch.no_grad():
            max_next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)
            target_q = rewards + (1 - dones) * self.gamma * max_next_q

        loss = nn.MSELoss()(current_q, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãªã—ç‰ˆï¼ˆTDç›®æ¨™ã§policy_netã‚’ä½¿ç”¨ï¼‰
# â†’ å­¦ç¿’ãŒä¸å®‰å®šã«ãªã‚‹

# çµæœ: çµŒé¨“å†ç”Ÿãªã—ã§ã¯åæŸãŒé…ãã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãªã—ã§ã¯æŒ¯å‹•ã™ã‚‹
</code></pre>


</details>

<hr />
<h2>ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã¾ã¨ã‚</h2>
<ul>
<li>ææ–™æ¢ç´¢ã¯æ¢ç´¢ç©ºé–“ãŒåºƒå¤§ã§ã€å¾“æ¥ã®è©¦è¡ŒéŒ¯èª¤ã¯éåŠ¹ç‡</li>
<li>å¼·åŒ–å­¦ç¿’ã¯<strong>ç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦æœ€é©ãªæ¢ç´¢æˆ¦ç•¥ã‚’å­¦ç¿’</strong></li>
<li><strong>ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼ˆMDPï¼‰</strong>ãŒå¼·åŒ–å­¦ç¿’ã®æ•°å­¦çš„åŸºç›¤</li>
<li><strong>Qå­¦ç¿’</strong>ã¯é›¢æ•£çŠ¶æ…‹ãƒ»è¡Œå‹•ã§æœ‰åŠ¹ã€Q-tableã§ä¾¡å€¤ã‚’è¨˜éŒ²</li>
<li><strong>DQN</strong>ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§Qé–¢æ•°ã‚’è¿‘ä¼¼ã—ã€å·¨å¤§ãªçŠ¶æ…‹ç©ºé–“ã«å¯¾å¿œ</li>
<li><strong>çµŒé¨“å†ç”Ÿ</strong>ã¨<strong>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</strong>ãŒDQNå­¦ç¿’ã‚’å®‰å®šåŒ–</li>
</ul>
<p>æ¬¡ç« ã§ã¯ã€ã‚ˆã‚Šé«˜åº¦ãªæ–¹ç­–å‹¾é…æ³•ï¼ˆPolicy Gradientï¼‰ã¨Actor-Criticæ‰‹æ³•ã‚’å­¦ã³ã¾ã™ã€‚</p>
<hr />
<h2>å“è³ªãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼šææ–™æ¢ç´¢RLã®å®Ÿè£…ç¢ºèª</h2>
<h3>MDPå®šå¼åŒ–ã‚¹ã‚­ãƒ«</h3>
<ul>
<li>[ ] ææ–™æ¢ç´¢ã‚¿ã‚¹ã‚¯ã‚’çŠ¶æ…‹ãƒ»è¡Œå‹•ãƒ»å ±é…¬ã§å®šå¼åŒ–ã§ãã‚‹</li>
<li>[ ] ãƒãƒ«ã‚³ãƒ•æ€§ã®ä»®å®šãŒå¦¥å½“ã‹ã‚’åˆ¤æ–­ã§ãã‚‹</li>
<li>[ ] å ±é…¬é–¢æ•°ãŒæ¢ç´¢ç›®æ¨™ã‚’æ­£ã—ãè¡¨ç¾ã—ã¦ã„ã‚‹ã‹æ¤œè¨¼ã§ãã‚‹</li>
<li>[ ] å‰²å¼•ç‡Î³ã®é¸æŠç†ç”±ã‚’èª¬æ˜ã§ãã‚‹ï¼ˆææ–™æ¢ç´¢ã§ã¯é€šå¸¸0.95-0.99ï¼‰</li>
</ul>
<h3>Qå­¦ç¿’å®Ÿè£…ã‚¹ã‚­ãƒ«</h3>
<ul>
<li>[ ] Îµ-greedyæ¢ç´¢ã®å®Ÿè£…ãŒã§ãã‚‹</li>
<li>[ ] TDèª¤å·®ã®è¨ˆç®—ã¨ Q å€¤æ›´æ–°ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>[ ] å­¦ç¿’æ›²ç·šã‹ã‚‰åæŸã‚’åˆ¤æ–­ã§ãã‚‹</li>
<li>[ ] Q-tableã®ã‚µã‚¤ã‚ºåˆ¶ç´„ï¼ˆçŠ¶æ…‹Ã—è¡Œå‹•æ•°ï¼‰ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
</ul>
<h3>DQNå®Ÿè£…ã‚¹ã‚­ãƒ«</h3>
<ul>
<li>[ ] PyTorchã§ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å®šç¾©ã§ãã‚‹</li>
<li>[ ] çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡ã®å®Ÿè£…ãŒã§ãã‚‹</li>
<li>[ ] ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ›´æ–°ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’è¨­å®šã§ãã‚‹</li>
<li>[ ] Îµã®æ¸›è¡°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é©åˆ‡ã«è¨­å®šã§ãã‚‹</li>
</ul>
<h3>ææ–™æ¢ç´¢ç‰¹æœ‰ã®æ³¨æ„ç‚¹</h3>
<ul>
<li>[ ] çµ„æˆãƒ™ãƒ¼ã‚¹ vs æ§‹é€ ãƒ™ãƒ¼ã‚¹è¨˜è¿°å­ã®é•ã„ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
<li>[ ] ææ–™å¤šå½¢ï¼ˆpolymorphsï¼‰ã‚’åŒºåˆ¥ã™ã‚‹çŠ¶æ…‹è¨­è¨ˆãŒã§ãã‚‹</li>
<li>[ ] ææ–™ç‰¹æ€§ã®ç‰©ç†çš„åˆ¶ç´„ã‚’å ±é…¬é–¢æ•°ã«çµ„ã¿è¾¼ã‚ã‚‹</li>
<li>[ ] DFTè¨ˆç®—ã‚³ã‚¹ãƒˆã‚’è€ƒæ…®ã—ãŸæ¢ç´¢æˆ¦ç•¥ã‚’è¨­è¨ˆã§ãã‚‹</li>
</ul>
<h3>ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚­ãƒ«</h3>
<ul>
<li>[ ] Qå€¤ãŒç™ºæ•£ã™ã‚‹å ´åˆã®åŸå› ã‚’ç‰¹å®šã§ãã‚‹</li>
<li>[ ] æ¢ç´¢ãŒé€²ã¾ãªã„å ´åˆã®å¯¾å‡¦æ³•ã‚’çŸ¥ã£ã¦ã„ã‚‹</li>
<li>[ ] å ±é…¬ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å•é¡Œã‚’æ¤œå‡ºãƒ»ä¿®æ­£ã§ãã‚‹</li>
<li>[ ] ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ã‚’ä½“ç³»çš„ã«èª¿æŸ»ã§ãã‚‹</li>
</ul>
<h3>ã‚³ãƒ¼ãƒ‰å“è³ª</h3>
<ul>
<li>[ ] å…¨ã‚³ãƒ¼ãƒ‰ã«ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’è¨˜è¼‰ã—ã¦ã„ã‚‹</li>
<li>[ ] ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’å›ºå®šã—ã¦å†ç¾æ€§ã‚’ç¢ºä¿ã—ã¦ã„ã‚‹</li>
<li>[ ] ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ãƒ»å‹ãƒ»ç¯„å›²ã®æ¤œè¨¼ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ã„ã‚‹</li>
<li>[ ] ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆä¾‹å¤–å‡¦ç†ï¼‰ã‚’å®Ÿè£…ã—ã¦ã„ã‚‹</li>
</ul>
<h3>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>
<p><strong>é”æˆåº¦80%æœªæº€ã®å ´åˆ:</strong>
- æœ¬ç« ã‚’å†èª­ã—ã€æ¼”ç¿’å•é¡Œã‚’è§£ãç›´ã™
- ç°¡å˜ãªã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ã§æ‰‹ã‚’å‹•ã‹ã—ã¦å®Ÿè£…çµŒé¨“ã‚’ç©ã‚€</p>
<p><strong>é”æˆåº¦80-95%ã®å ´åˆ:</strong>
- ç¬¬2ç« ï¼ˆæ–¹ç­–å‹¾é…æ³•ï¼‰ã«é€²ã‚€æº–å‚™OK
- DQNã®ã‚³ãƒ¼ãƒ‰ã‚’è‡ªåˆ†ã§æœ€åˆã‹ã‚‰å®Ÿè£…ã—ã¦ã¿ã‚‹</p>
<p><strong>é”æˆåº¦95%ä»¥ä¸Šã®å ´åˆ:</strong>
- ç¬¬2ç« ã«é€²ã¿ã€ã‚ˆã‚Šé«˜åº¦ãªæ‰‹æ³•ã‚’å­¦ã¶
- å®Ÿéš›ã®ææ–™æ¢ç´¢ã‚¿ã‚¹ã‚¯ã§RLã‚’è©¦ã™</p>
<hr />
<h2>å‚è€ƒæ–‡çŒ®</h2>
<ol>
<li>Mnih et al. "Playing Atari with Deep Reinforcement Learning" <em>arXiv</em> (2013) - DQNåŸè«–æ–‡</li>
<li>Sutton &amp; Barto "Reinforcement Learning: An Introduction" MIT Press (2018) - RLæ•™ç§‘æ›¸</li>
<li>Zhou et al. "Optimization of molecules via deep reinforcement learning" <em>Scientific Reports</em> (2019)</li>
<li>Ling et al. "High-dimensional materials and process optimization using data-driven experimental design with well-calibrated uncertainty estimates" <em>Integrating Materials and Manufacturing Innovation</em> (2017)</li>
</ol>
<hr />
<p><strong>æ¬¡ç« </strong>: <a href="chapter-2.html">ç¬¬2ç« : å¼·åŒ–å­¦ç¿’ã®åŸºç¤ç†è«–</a></p><div class="navigation">
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter-2.html" class="nav-button">æ¬¡ã®ç«  â†’</a>
</div>
    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 2.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
