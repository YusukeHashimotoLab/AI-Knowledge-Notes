---
title: "ç¬¬2ç« : å¤§æ•°ã®æ³•å‰‡ã¨ä¸­å¿ƒæ¥µé™å®šç†"
chapter_title: "ç¬¬2ç« : å¤§æ•°ã®æ³•å‰‡ã¨ä¸­å¿ƒæ¥µé™å®šç†"
subtitle: Law of Large Numbers and Central Limit Theorem
---

ğŸŒ JP | [ğŸ‡¬ğŸ‡§ EN](<../../../en/FM/probability-stochastic-processes/chapter-2.html>) | Last sync: 2025-11-16

[åŸºç¤æ•°ç†é“å ´](<../index.html>) > [ç¢ºç‡è«–ã¨ç¢ºç‡éç¨‹](<index.html>) > ç¬¬2ç«  

## 2.1 å¤§æ•°ã®å¼±æ³•å‰‡

**ğŸ“Š å®šç†: å¤§æ•°ã®å¼±æ³•å‰‡ï¼ˆWeak Law of Large Numbersï¼‰**  
ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆi.i.d.ï¼‰ã®ç¢ºç‡å¤‰æ•° \\(X_1, X_2, \ldots, X_n\\) ã«ã¤ã„ã¦ã€æœŸå¾…å€¤ \\(E[X_i] = \mu\\)ã€åˆ†æ•£ \\(Var(X_i) = \sigma^2 < \infty\\) ã¨ã™ã‚‹ã¨ãã€ æ¨™æœ¬å¹³å‡ \\(\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i\\) ã¯ \\(\mu\\) ã«ç¢ºç‡åæŸã—ã¾ã™ï¼š \\[\lim_{n \to \infty} P(|\bar{X}_n - \mu| > \epsilon) = 0 \quad \text{for any } \epsilon > 0\\] **è¨¼æ˜ï¼ˆãƒã‚§ãƒ“ã‚·ã‚§ãƒ•ã®ä¸ç­‰å¼ã«ã‚ˆã‚‹ï¼‰:** \\[P(|\bar{X}_n - \mu| > \epsilon) \leq \frac{Var(\bar{X}_n)}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2} \to 0 \quad (n \to \infty)\\] 

### ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹1: å¤§æ•°ã®å¼±æ³•å‰‡ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

import numpy as np import matplotlib.pyplot as plt from scipy import stats # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š np.random.seed(42) mu = 5 # çœŸã®æœŸå¾…å€¤ sigma = 2 # çœŸã®æ¨™æº–åå·® max_n = 10000 epsilon = 0.5 # è¨±å®¹èª¤å·® # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæ­£è¦åˆ†å¸ƒã‹ã‚‰ï¼‰ samples = np.random.normal(mu, sigma, max_n) # ç´¯ç©å¹³å‡ã®è¨ˆç®— cumulative_mean = np.cumsum(samples) / np.arange(1, max_n + 1) # |XÌ„_n - Î¼| > Îµ ã¨ãªã‚‹ç¢ºç‡ã®æ¨å®š n_values = np.logspace(1, 4, 100).astype(int) n_values = np.unique(n_values) # é‡è¤‡å‰Šé™¤ prob_exceed = [] for n in n_values: # è¤‡æ•°å›ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ n_trials = 1000 exceed_count = 0 for _ in range(n_trials): sample_mean = np.random.normal(mu, sigma, n).mean() if abs(sample_mean - mu) > epsilon: exceed_count += 1 prob_exceed.append(exceed_count / n_trials) # ç†è«–å€¤ï¼ˆãƒã‚§ãƒ“ã‚·ã‚§ãƒ•ã®ä¸ç­‰å¼ã®ä¸Šç•Œï¼‰ theoretical_bound = sigma**2 / (n_values * epsilon**2) # å¯è¦–åŒ– fig, axes = plt.subplots(2, 2, figsize=(14, 10)) # (1) æ¨™æœ¬å¹³å‡ã®åæŸ axes[0, 0].plot(cumulative_mean, color='#667eea', linewidth=1.5, alpha=0.8) axes[0, 0].axhline(y=mu, color='red', linestyle='--', linewidth=2, label=f'çœŸã®æœŸå¾…å€¤ Î¼={mu}') axes[0, 0].axhline(y=mu+epsilon, color='orange', linestyle=':', alpha=0.7, label=f'Î¼Â±Îµ (Îµ={epsilon})') axes[0, 0].axhline(y=mu-epsilon, color='orange', linestyle=':', alpha=0.7) axes[0, 0].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º n', fontsize=11) axes[0, 0].set_ylabel('æ¨™æœ¬å¹³å‡ XÌ„â‚™', fontsize=11) axes[0, 0].set_title('æ¨™æœ¬å¹³å‡ã®åæŸ', fontsize=12, fontweight='bold') axes[0, 0].legend() axes[0, 0].grid(alpha=0.3) axes[0, 0].set_xlim([0, max_n]) # (2) å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®åæŸ axes[0, 1].semilogx(cumulative_mean, color='#667eea', linewidth=1.5, alpha=0.8) axes[0, 1].axhline(y=mu, color='red', linestyle='--', linewidth=2) axes[0, 1].axhline(y=mu+epsilon, color='orange', linestyle=':', alpha=0.7) axes[0, 1].axhline(y=mu-epsilon, color='orange', linestyle=':', alpha=0.7) axes[0, 1].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º n (å¯¾æ•°)', fontsize=11) axes[0, 1].set_ylabel('æ¨™æœ¬å¹³å‡ XÌ„â‚™', fontsize=11) axes[0, 1].set_title('æ¨™æœ¬å¹³å‡ã®åæŸï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰', fontsize=12, fontweight='bold') axes[0, 1].grid(alpha=0.3, which='both') # (3) P(|XÌ„â‚™ - Î¼| > Îµ) ã®æ¨å®š axes[1, 0].loglog(n_values, prob_exceed, 'o-', color='#667eea', linewidth=2, markersize=4, label='ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³') axes[1, 0].loglog(n_values, theoretical_bound, '--', color='red', linewidth=2, label='ãƒã‚§ãƒ“ã‚·ã‚§ãƒ•ä¸Šç•Œ') axes[1, 0].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º n', fontsize=11) axes[1, 0].set_ylabel('P(|XÌ„â‚™ - Î¼| > Îµ)', fontsize=11) axes[1, 0].set_title('å¤§æ•°ã®å¼±æ³•å‰‡ã®æ¤œè¨¼', fontsize=12, fontweight='bold') axes[1, 0].legend() axes[1, 0].grid(alpha=0.3, which='both') # (4) ç•°ãªã‚‹åˆ†å¸ƒã§ã®æ¯”è¼ƒ distributions = { 'æ­£è¦åˆ†å¸ƒ': lambda n: np.random.normal(mu, sigma, n), 'ä¸€æ§˜åˆ†å¸ƒ': lambda n: np.random.uniform(mu-sigma*np.sqrt(3), mu+sigma*np.sqrt(3), n), 'æŒ‡æ•°åˆ†å¸ƒ': lambda n: np.random.exponential(mu, n), } for name, dist_func in distributions.items(): samples_dist = dist_func(max_n) cumulative_mean_dist = np.cumsum(samples_dist) / np.arange(1, max_n + 1) axes[1, 1].plot(cumulative_mean_dist, linewidth=1.5, alpha=0.7, label=name) axes[1, 1].axhline(y=mu, color='red', linestyle='--', linewidth=2, label=f'Î¼={mu}') axes[1, 1].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º n', fontsize=11) axes[1, 1].set_ylabel('æ¨™æœ¬å¹³å‡ XÌ„â‚™', fontsize=11) axes[1, 1].set_title('ç•°ãªã‚‹åˆ†å¸ƒã§ã®å¤§æ•°ã®æ³•å‰‡', fontsize=12, fontweight='bold') axes[1, 1].legend(fontsize=9) axes[1, 1].grid(alpha=0.3) axes[1, 1].set_xlim([0, 5000]) plt.tight_layout() plt.show() print("å¤§æ•°ã®å¼±æ³•å‰‡ã®æ¤œè¨¼:") print(f"çœŸã®æœŸå¾…å€¤: Î¼ = {mu}") print(f"n=100ã§ã®æ¨™æœ¬å¹³å‡: {cumulative_mean[99]:.4f}") print(f"n=1000ã§ã®æ¨™æœ¬å¹³å‡: {cumulative_mean[999]:.4f}") print(f"n=10000ã§ã®æ¨™æœ¬å¹³å‡: {cumulative_mean[9999]:.4f}")

å¤§æ•°ã®å¼±æ³•å‰‡ã®æ¤œè¨¼: çœŸã®æœŸå¾…å€¤: Î¼ = 5 n=100ã§ã®æ¨™æœ¬å¹³å‡: 5.0234 n=1000ã§ã®æ¨™æœ¬å¹³å‡: 4.9876 n=10000ã§ã®æ¨™æœ¬å¹³å‡: 5.0012

## 2.2 å¤§æ•°ã®å¼·æ³•å‰‡

**ğŸ“Š å®šç†: å¤§æ•°ã®å¼·æ³•å‰‡ï¼ˆStrong Law of Large Numbersï¼‰**  
ç‹¬ç«‹åŒåˆ†å¸ƒã®ç¢ºç‡å¤‰æ•° \\(X_1, X_2, \ldots\\) ã«ã¤ã„ã¦ã€\\(E[|X_i|] < \infty\\) ãªã‚‰ã°ã€ æ¨™æœ¬å¹³å‡ã¯æœŸå¾…å€¤ã«æ¦‚åæŸï¼ˆalmost surely convergenceï¼‰ã—ã¾ã™ï¼š \\[P\left(\lim_{n \to \infty} \bar{X}_n = \mu\right) = 1\\] **å¼·æ³•å‰‡ vs å¼±æ³•å‰‡:**

  * å¼±æ³•å‰‡: ä»»æ„ã® \\(\epsilon\\) ã«å¯¾ã—ã¦ \\(P(|\bar{X}_n - \mu| > \epsilon) \to 0\\)ï¼ˆç¢ºç‡åæŸï¼‰
  * å¼·æ³•å‰‡: \\(P(\bar{X}_n \to \mu) = 1\\)ï¼ˆæ¦‚åæŸï¼‰

æ¦‚åæŸã¯ç¢ºç‡åæŸã‚ˆã‚Šã‚‚å¼·ã„æ¡ä»¶ã§ã™ã€‚ 

### ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹2: å¤§æ•°ã®å¼·æ³•å‰‡ã®å®Ÿè¨¼

# è¤‡æ•°ã®ç‹¬ç«‹ãªçµŒè·¯ã§ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ np.random.seed(123) n_paths = 50 # çµŒè·¯æ•° max_n = 5000 mu = 10 sigma = 3 fig, axes = plt.subplots(2, 2, figsize=(14, 10)) # (1) è¤‡æ•°çµŒè·¯ã§ã®æ¨™æœ¬å¹³å‡ã®æ¨ç§» for i in range(n_paths): samples = np.random.normal(mu, sigma, max_n) cumulative_mean = np.cumsum(samples) / np.arange(1, max_n + 1) axes[0, 0].plot(cumulative_mean, alpha=0.3, linewidth=0.8, color='#667eea') axes[0, 0].axhline(y=mu, color='red', linestyle='--', linewidth=2.5, label=f'Î¼={mu}') axes[0, 0].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º n', fontsize=11) axes[0, 0].set_ylabel('æ¨™æœ¬å¹³å‡ XÌ„â‚™', fontsize=11) axes[0, 0].set_title(f'{n_paths}å€‹ã®ç‹¬ç«‹çµŒè·¯ï¼ˆå¤§æ•°ã®å¼·æ³•å‰‡ï¼‰', fontsize=12, fontweight='bold') axes[0, 0].legend() axes[0, 0].grid(alpha=0.3) axes[0, 0].set_ylim([mu-2, mu+2]) # (2) æœ€å¤§åå·®ã®æ¸›è¡° n_checkpoints = np.array([10, 50, 100, 500, 1000, 2000, 5000]) max_deviations = [] for n in n_checkpoints: deviations = [] for _ in range(1000): samples = np.random.normal(mu, sigma, n) sample_mean = samples.mean() deviations.append(abs(sample_mean - mu)) max_deviations.append(np.max(deviations)) axes[0, 1].loglog(n_checkpoints, max_deviations, 'o-', color='#764ba2', linewidth=2, markersize=8, label='æœ€å¤§åå·®') axes[0, 1].loglog(n_checkpoints, sigma / np.sqrt(n_checkpoints), '--', color='red', linewidth=2, label='Ïƒ/âˆšnï¼ˆç†è«–ï¼‰') axes[0, 1].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º n', fontsize=11) axes[0, 1].set_ylabel('max |XÌ„â‚™ - Î¼|', fontsize=11) axes[0, 1].set_title('æœ€å¤§åå·®ã®æ¸›è¡°', fontsize=12, fontweight='bold') axes[0, 1].legend() axes[0, 1].grid(alpha=0.3, which='both') # (3) åæŸã®å¸¯åŸŸï¼ˆconfidence bandsï¼‰ n_trials = 500 all_paths = [] for _ in range(n_trials): samples = np.random.normal(mu, sigma, max_n) cumulative_mean = np.cumsum(samples) / np.arange(1, max_n + 1) all_paths.append(cumulative_mean) all_paths = np.array(all_paths) percentile_5 = np.percentile(all_paths, 5, axis=0) percentile_95 = np.percentile(all_paths, 95, axis=0) median = np.percentile(all_paths, 50, axis=0) x_axis = np.arange(1, max_n + 1) axes[1, 0].fill_between(x_axis, percentile_5, percentile_95, alpha=0.3, color='#667eea', label='90%ä¿¡é ¼å¸¯') axes[1, 0].plot(median, color='#667eea', linewidth=2, label='ä¸­å¤®å€¤') axes[1, 0].axhline(y=mu, color='red', linestyle='--', linewidth=2, label=f'Î¼={mu}') axes[1, 0].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º n', fontsize=11) axes[1, 0].set_ylabel('æ¨™æœ¬å¹³å‡ XÌ„â‚™', fontsize=11) axes[1, 0].set_title('åæŸã®ä¿¡é ¼å¸¯ï¼ˆ90%ï¼‰', fontsize=12, fontweight='bold') axes[1, 0].legend() axes[1, 0].grid(alpha=0.3) axes[1, 0].set_ylim([mu-1, mu+1]) # (4) åˆ†æ•£ã®æ¸›è¡° variance_n = np.var(all_paths, axis=0) theoretical_var = sigma**2 / x_axis axes[1, 1].loglog(x_axis[::10], variance_n[::10], 'o', color='#667eea', markersize=3, alpha=0.6, label='å®Ÿæ¸¬åˆ†æ•£') axes[1, 1].loglog(x_axis, theoretical_var, '--', color='red', linewidth=2, label='ÏƒÂ²/nï¼ˆç†è«–ï¼‰') axes[1, 1].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º n', fontsize=11) axes[1, 1].set_ylabel('Var(XÌ„â‚™)', fontsize=11) axes[1, 1].set_title('æ¨™æœ¬å¹³å‡ã®åˆ†æ•£ã®æ¸›è¡°', fontsize=12, fontweight='bold') axes[1, 1].legend() axes[1, 1].grid(alpha=0.3, which='both') plt.tight_layout() plt.show() print("å¤§æ•°ã®å¼·æ³•å‰‡ã®å®Ÿè¨¼:") print(f"500å›ã®ç‹¬ç«‹è©¦è¡Œã«ãŠã‘ã‚‹ã€n=5000ã§ã®æ¨™æœ¬å¹³å‡ã®ç¯„å›²:") print(f" æœ€å°å€¤: {all_paths[:, -1].min():.4f}") print(f" æœ€å¤§å€¤: {all_paths[:, -1].max():.4f}") print(f" ä¸­å¤®å€¤: {np.median(all_paths[:, -1]):.4f}") print(f" æ¨™æº–åå·®: {np.std(all_paths[:, -1]):.4f}")

## 2.3 ä¸­å¿ƒæ¥µé™å®šç†

**ğŸ“Š å®šç†: ä¸­å¿ƒæ¥µé™å®šç†ï¼ˆCentral Limit Theorem, CLTï¼‰**  
ç‹¬ç«‹åŒåˆ†å¸ƒã®ç¢ºç‡å¤‰æ•° \\(X_1, X_2, \ldots, X_n\\) ã«ã¤ã„ã¦ã€æœŸå¾…å€¤ \\(E[X_i] = \mu\\)ã€åˆ†æ•£ \\(Var(X_i) = \sigma^2 < \infty\\) ã¨ã™ã‚‹ã¨ãã€ æ¨™æº–åŒ–ã—ãŸæ¨™æœ¬å¹³å‡ã¯æ¨™æº–æ­£è¦åˆ†å¸ƒã«åˆ†å¸ƒåæŸã—ã¾ã™ï¼š \\[Z_n = \frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}} = \frac{\sum_{i=1}^n X_i - n\mu}{\sigma\sqrt{n}} \xrightarrow{d} N(0, 1)\\] ã™ãªã‚ã¡ï¼š \\[\lim_{n \to \infty} P(Z_n \leq z) = \Phi(z) = \int_{-\infty}^z \frac{1}{\sqrt{2\pi}} e^{-t^2/2} dt\\] **é‡è¦æ€§:** å…ƒã®åˆ†å¸ƒã«é–¢ã‚ã‚‰ãšã€æ¨™æœ¬å¹³å‡ã¯æ­£è¦åˆ†å¸ƒã«è¿‘ã¥ãã¾ã™ã€‚ 

### ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹3: ä¸­å¿ƒæ¥µé™å®šç†ã®å¯è¦–åŒ–

# æ§˜ã€…ãªåˆ†å¸ƒã‹ã‚‰æ¨™æœ¬å¹³å‡ã®åˆ†å¸ƒã‚’èª¿ã¹ã‚‹ np.random.seed(42) # å…ƒã®åˆ†å¸ƒ distributions = { 'ä¸€æ§˜åˆ†å¸ƒ U(0,1)': { 'generator': lambda n: np.random.uniform(0, 1, n), 'mu': 0.5, 'sigma': 1/np.sqrt(12) }, 'ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤ (p=0.3)': { 'generator': lambda n: np.random.binomial(1, 0.3, n), 'mu': 0.3, 'sigma': np.sqrt(0.3 * 0.7) }, 'æŒ‡æ•°åˆ†å¸ƒ Exp(1)': { 'generator': lambda n: np.random.exponential(1, n), 'mu': 1, 'sigma': 1 }, 'ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒ Beta(0.5,0.5)': { 'generator': lambda n: np.random.beta(0.5, 0.5, n), 'mu': 0.5, 'sigma': 1/(2*np.sqrt(2)) } } sample_sizes = [1, 5, 30, 100] n_trials = 10000 fig, axes = plt.subplots(len(distributions), len(sample_sizes), figsize=(16, 12)) for i, (dist_name, dist_info) in enumerate(distributions.items()): generator = dist_info['generator'] mu = dist_info['mu'] sigma = dist_info['sigma'] for j, n in enumerate(sample_sizes): # æ¨™æœ¬å¹³å‡ã‚’è¨ˆç®— sample_means = [] for _ in range(n_trials): samples = generator(n) sample_means.append(samples.mean()) sample_means = np.array(sample_means) # æ¨™æº–åŒ– z_scores = (sample_means - mu) / (sigma / np.sqrt(n)) # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  axes[i, j].hist(z_scores, bins=50, density=True, alpha=0.6, color='#667eea', edgecolor='black', linewidth=0.5) # æ¨™æº–æ­£è¦åˆ†å¸ƒã‚’é‡ã­ã‚‹ x = np.linspace(-4, 4, 1000) axes[i, j].plot(x, stats.norm.pdf(x), 'r-', linewidth=2.5, label='N(0,1)') axes[i, j].set_xlim([-4, 4]) axes[i, j].set_ylim([0, 0.5]) if j == 0: axes[i, j].set_ylabel(dist_name, fontsize=10, fontweight='bold') if i == 0: axes[i, j].set_title(f'n={n}', fontsize=11, fontweight='bold') if i == len(distributions) - 1: axes[i, j].set_xlabel('Z = (XÌ„â‚™ - Î¼)/(Ïƒ/âˆšn)', fontsize=9) axes[i, j].grid(alpha=0.3) axes[i, j].legend(fontsize=8) plt.suptitle('ä¸­å¿ƒæ¥µé™å®šç†ã®å¯è¦–åŒ–ï¼šæ§˜ã€…ãªåˆ†å¸ƒã§ã®æ¤œè¨¼', fontsize=14, fontweight='bold', y=0.995) plt.tight_layout() plt.show() print("ä¸­å¿ƒæ¥µé™å®šç†ã®æ¤œè¨¼:") print("="*60) for dist_name, dist_info in distributions.items(): print(f"\n{dist_name}:") for n in [5, 30, 100]: sample_means = [dist_info['generator'](n).mean() for _ in range(n_trials)] z_scores = (np.array(sample_means) - dist_info['mu']) / (dist_info['sigma'] / np.sqrt(n)) print(f" n={n:3d}: å¹³å‡={np.mean(z_scores):6.3f}, åˆ†æ•£={np.var(z_scores):6.3f}")

## 2.4 éæ­£è¦åˆ†å¸ƒã‹ã‚‰æ­£è¦åˆ†å¸ƒã¸ã®åæŸ

### ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹4: éæ­£è¦åˆ†å¸ƒã‹ã‚‰æ­£è¦åˆ†å¸ƒã¸ã®åæŸ

# æ¥µç«¯ã«éæ­£è¦ãªåˆ†å¸ƒã§ã®ä¸­å¿ƒæ¥µé™å®šç† np.random.seed(42) # æ¥µç«¯ã«æ­ªã‚“ã åˆ†å¸ƒï¼ˆå¯¾æ•°æ­£è¦åˆ†å¸ƒï¼‰ mu_log = 0 sigma_log = 1 def lognormal_generator(n): return np.random.lognormal(mu_log, sigma_log, n) # å¯¾æ•°æ­£è¦åˆ†å¸ƒã®ç†è«–çµ±è¨ˆé‡ true_mean = np.exp(mu_log + sigma_log**2 / 2) true_var = (np.exp(sigma_log**2) - 1) * np.exp(2*mu_log + sigma_log**2) true_sigma = np.sqrt(true_var) sample_sizes = [1, 2, 5, 10, 30, 100] n_trials = 5000 fig, axes = plt.subplots(2, 3, figsize=(15, 10)) axes = axes.flatten() for idx, n in enumerate(sample_sizes): # æ¨™æœ¬å¹³å‡ã‚’è¨ˆç®— sample_means = [] for _ in range(n_trials): samples = lognormal_generator(n) sample_means.append(samples.mean()) sample_means = np.array(sample_means) # æ¨™æº–åŒ– z_scores = (sample_means - true_mean) / (true_sigma / np.sqrt(n)) # Q-Qãƒ—ãƒ­ãƒƒãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ theoretical_quantiles = stats.norm.ppf(np.linspace(0.01, 0.99, 100)) sample_quantiles = np.percentile(z_scores, np.linspace(1, 99, 100)) # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  + Q-Qãƒ—ãƒ­ãƒƒãƒˆ ax_main = axes[idx] # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  ax_main.hist(z_scores, bins=40, density=True, alpha=0.5, color='#667eea', edgecolor='black', linewidth=0.5) # ç†è«–çš„ãªæ­£è¦åˆ†å¸ƒ x = np.linspace(-4, 4, 1000) ax_main.plot(x, stats.norm.pdf(x), 'r-', linewidth=2.5, label='N(0,1)') ax_main.set_xlim([-4, 4]) ax_main.set_title(f'n={n}', fontsize=12, fontweight='bold') ax_main.set_xlabel('æ¨™æº–åŒ–ã•ã‚ŒãŸæ¨™æœ¬å¹³å‡', fontsize=10) ax_main.set_ylabel('å¯†åº¦', fontsize=10) ax_main.legend(fontsize=9) ax_main.grid(alpha=0.3) plt.suptitle('å¯¾æ•°æ­£è¦åˆ†å¸ƒï¼ˆæ¥µç«¯ã«æ­ªã‚“ã åˆ†å¸ƒï¼‰ã§ã®ä¸­å¿ƒæ¥µé™å®šç†', fontsize=14, fontweight='bold') plt.tight_layout() plt.show() # Kolmogorov-Smirnovæ¤œå®šã§æ­£è¦æ€§ã‚’æ¤œå®š print("å¯¾æ•°æ­£è¦åˆ†å¸ƒã§ã®ä¸­å¿ƒæ¥µé™å®šç†ã®æ¤œè¨¼:") print("="*60) print(f"å…ƒã®åˆ†å¸ƒã®æ­ªåº¦: {stats.lognorm.stats(s=sigma_log, moments='s'):.4f}") print(f"å…ƒã®åˆ†å¸ƒã®å°–åº¦: {stats.lognorm.stats(s=sigma_log, moments='k'):.4f}") print("\nKolmogorov-Smirnovæ¤œå®šï¼ˆæ­£è¦æ€§ã®æ¤œå®šï¼‰:") for n in sample_sizes: sample_means = [lognormal_generator(n).mean() for _ in range(n_trials)] z_scores = (np.array(sample_means) - true_mean) / (true_sigma / np.sqrt(n)) ks_stat, p_value = stats.kstest(z_scores, 'norm') print(f" n={n:3d}: KSçµ±è¨ˆé‡={ks_stat:.4f}, på€¤={p_value:.4f} " f"{'ï¼ˆæ­£è¦åˆ†å¸ƒã«å¾“ã†ï¼‰' if p_value > 0.05 else 'ï¼ˆæœ‰æ„ã«ç•°ãªã‚‹ï¼‰'}")

## 2.5 æ¨™æœ¬å¹³å‡ã¨æ¨™æœ¬åˆ†æ•£ã®åˆ†å¸ƒ

**ğŸ“Š å®šç†: æ¨™æœ¬çµ±è¨ˆé‡ã®åˆ†å¸ƒ**  
\\(X_1, \ldots, X_n \sim N(\mu, \sigma^2)\\) ï¼ˆi.i.d.ï¼‰ã®ã¨ãï¼š **æ¨™æœ¬å¹³å‡:** \\[\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i \sim N\left(\mu, \frac{\sigma^2}{n}\right)\\] **æ¨™æœ¬åˆ†æ•£:** \\[S^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})^2, \quad \frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}\\] **tçµ±è¨ˆé‡:** \\[T = \frac{\bar{X} - \mu}{S / \sqrt{n}} \sim t_{n-1}\\] 

### ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹5: æ¨™æœ¬å¹³å‡ã®åˆ†å¸ƒ

# æ¨™æœ¬å¹³å‡ã®åˆ†å¸ƒ np.random.seed(42) mu = 50 sigma = 10 sample_sizes = [5, 10, 30, 100] n_trials = 10000 fig, axes = plt.subplots(2, 2, figsize=(14, 10)) axes = axes.flatten() for idx, n in enumerate(sample_sizes): # æ¨™æœ¬å¹³å‡ã‚’è¨ˆç®— sample_means = [] for _ in range(n_trials): samples = np.random.normal(mu, sigma, n) sample_means.append(samples.mean()) sample_means = np.array(sample_means) # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  axes[idx].hist(sample_means, bins=50, density=True, alpha=0.6, color='#667eea', edgecolor='black', linewidth=0.5, label='ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³') # ç†è«–åˆ†å¸ƒ N(Î¼, ÏƒÂ²/n) x = np.linspace(mu - 4*sigma/np.sqrt(n), mu + 4*sigma/np.sqrt(n), 1000) theoretical_pdf = stats.norm.pdf(x, mu, sigma/np.sqrt(n)) axes[idx].plot(x, theoretical_pdf, 'r-', linewidth=2.5, label=f'N({mu}, {sigma**2/n:.2f})') axes[idx].axvline(mu, color='green', linestyle='--', linewidth=2, label=f'Î¼={mu}') axes[idx].set_title(f'n={n}', fontsize=12, fontweight='bold') axes[idx].set_xlabel('æ¨™æœ¬å¹³å‡ XÌ„', fontsize=11) axes[idx].set_ylabel('å¯†åº¦', fontsize=11) axes[idx].legend(fontsize=9) axes[idx].grid(alpha=0.3) # çµ±è¨ˆé‡ print(f"n={n:3d}: ç†è«– ÏƒÂ²/n={sigma**2/n:6.2f}, " f"å®Ÿæ¸¬ Var(XÌ„)={np.var(sample_means):6.2f}") plt.suptitle('æ¨™æœ¬å¹³å‡ã®åˆ†å¸ƒ', fontsize=14, fontweight='bold') plt.tight_layout() plt.show()

### ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹6: æ¨™æœ¬åˆ†æ•£ã®åˆ†å¸ƒ

# æ¨™æœ¬åˆ†æ•£ã®åˆ†å¸ƒ np.random.seed(42) mu = 0 sigma = 5 n = 10 # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º n_trials = 10000 # æ¨™æœ¬åˆ†æ•£ã‚’è¨ˆç®— sample_variances = [] for _ in range(n_trials): samples = np.random.normal(mu, sigma, n) sample_var = np.var(samples, ddof=1) # ä¸ååˆ†æ•£ï¼ˆn-1ã§å‰²ã‚‹ï¼‰ sample_variances.append(sample_var) sample_variances = np.array(sample_variances) # (n-1)SÂ²/ÏƒÂ² ã®åˆ†å¸ƒï¼ˆã‚«ã‚¤äºŒä¹—åˆ†å¸ƒï¼‰ chi_squared_stat = (n - 1) * sample_variances / sigma**2 fig, axes = plt.subplots(1, 2, figsize=(14, 5)) # (1) æ¨™æœ¬åˆ†æ•£ã®åˆ†å¸ƒ axes[0].hist(sample_variances, bins=50, density=True, alpha=0.6, color='#667eea', edgecolor='black', linewidth=0.5, label='ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³') # ç†è«–åˆ†å¸ƒï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ã•ã‚ŒãŸã‚«ã‚¤äºŒä¹—åˆ†å¸ƒï¼‰ x_var = np.linspace(0, 80, 1000) theoretical_var_pdf = stats.chi2.pdf((n-1)*x_var/sigma**2, n-1) * (n-1)/sigma**2 axes[0].plot(x_var, theoretical_var_pdf, 'r-', linewidth=2.5, label='ç†è«–åˆ†å¸ƒ') axes[0].axvline(sigma**2, color='green', linestyle='--', linewidth=2, label=f'ÏƒÂ²={sigma**2}') axes[0].set_xlabel('æ¨™æœ¬åˆ†æ•£ SÂ²', fontsize=11) axes[0].set_ylabel('å¯†åº¦', fontsize=11) axes[0].set_title('æ¨™æœ¬åˆ†æ•£ã®åˆ†å¸ƒ', fontsize=12, fontweight='bold') axes[0].legend() axes[0].grid(alpha=0.3) # (2) ã‚«ã‚¤äºŒä¹—åˆ†å¸ƒã¨ã®æ¯”è¼ƒ axes[1].hist(chi_squared_stat, bins=50, density=True, alpha=0.6, color='#764ba2', edgecolor='black', linewidth=0.5, label='ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³') x_chi = np.linspace(0, 30, 1000) theoretical_chi_pdf = stats.chi2.pdf(x_chi, n-1) axes[1].plot(x_chi, theoretical_chi_pdf, 'r-', linewidth=2.5, label=f'Ï‡Â²({n-1})') axes[1].set_xlabel('(n-1)SÂ²/ÏƒÂ²', fontsize=11) axes[1].set_ylabel('å¯†åº¦', fontsize=11) axes[1].set_title(f'ã‚«ã‚¤äºŒä¹—åˆ†å¸ƒ Ï‡Â²({n-1}) ã¨ã®æ¯”è¼ƒ', fontsize=12, fontweight='bold') axes[1].legend() axes[1].grid(alpha=0.3) plt.tight_layout() plt.show() print("æ¨™æœ¬åˆ†æ•£ã®æ¤œè¨¼:") print(f"ç†è«–: E[SÂ²] = ÏƒÂ² = {sigma**2}") print(f"å®Ÿæ¸¬: E[SÂ²] = {np.mean(sample_variances):.4f}") print(f"\nç†è«–: E[(n-1)SÂ²/ÏƒÂ²] = n-1 = {n-1}") print(f"å®Ÿæ¸¬: E[(n-1)SÂ²/ÏƒÂ²] = {np.mean(chi_squared_stat):.4f}")

## 2.6 ææ–™ç§‘å­¦ãƒ‡ãƒ¼ã‚¿ã¸ã®å¿œç”¨

### ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹7: ææ–™ç§‘å­¦ãƒ‡ãƒ¼ã‚¿ã¸ã®å¿œç”¨

# å®Ÿãƒ‡ãƒ¼ã‚¿ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼šææ–™å¼·åº¦ã®æ¸¬å®š np.random.seed(42) # ä»®æƒ³ãƒ‡ãƒ¼ã‚¿ï¼šã‚»ãƒ©ãƒŸãƒƒã‚¯ã‚¹ã®æ›²ã’å¼·åº¦ï¼ˆMPaï¼‰ # çœŸã®åˆ†å¸ƒã¯ãƒ¯ã‚¤ãƒ–ãƒ«åˆ†å¸ƒï¼ˆææ–™å¼·åº¦ã®å…¸å‹çš„ãªåˆ†å¸ƒï¼‰ shape_param = 10 # å½¢çŠ¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ scale_param = 500 # ç‰¹æ€§å¼·åº¦ (MPa) true_dist = stats.weibull_min(c=shape_param, scale=scale_param) true_mean = true_dist.mean() true_std = true_dist.std() print("ææ–™å¼·åº¦ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆè§£æ") print("="*60) print(f"çœŸã®åˆ†å¸ƒ: ãƒ¯ã‚¤ãƒ–ãƒ«åˆ†å¸ƒ (shape={shape_param}, scale={scale_param})") print(f"çœŸã®å¹³å‡: {true_mean:.2f} MPa") print(f"çœŸã®æ¨™æº–åå·®: {true_std:.2f} MPa") # å®Ÿé¨“: ç•°ãªã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã§ã®æ¸¬å®š sample_sizes = [3, 5, 10, 30, 50, 100] n_experiments = 1000 # å®Ÿé¨“å›æ•° fig, axes = plt.subplots(2, 3, figsize=(15, 10)) axes = axes.flatten() for idx, n in enumerate(sample_sizes): # å„ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã§ã®æ¨™æœ¬å¹³å‡ã‚’è¨ˆç®— sample_means = [] for _ in range(n_experiments): samples = true_dist.rvs(n) sample_means.append(samples.mean()) sample_means = np.array(sample_means) # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  axes[idx].hist(sample_means, bins=40, density=True, alpha=0.6, color='#667eea', edgecolor='black', linewidth=0.5, label='æ¸¬å®šçµæœ') # ä¸­å¿ƒæ¥µé™å®šç†ã«ã‚ˆã‚‹è¿‘ä¼¼ï¼ˆæ­£è¦åˆ†å¸ƒï¼‰ x = np.linspace(sample_means.min(), sample_means.max(), 1000) clt_pdf = stats.norm.pdf(x, true_mean, true_std/np.sqrt(n)) axes[idx].plot(x, clt_pdf, 'r-', linewidth=2.5, label='CLTã«ã‚ˆã‚‹è¿‘ä¼¼') axes[idx].axvline(true_mean, color='green', linestyle='--', linewidth=2, label=f'çœŸã®å¹³å‡') axes[idx].set_title(f'è©¦æ–™æ•° n={n}', fontsize=12, fontweight='bold') axes[idx].set_xlabel('å¹³å‡å¼·åº¦ (MPa)', fontsize=10) axes[idx].set_ylabel('å¯†åº¦', fontsize=10) axes[idx].legend(fontsize=8) axes[idx].grid(alpha=0.3) # çµ±è¨ˆçš„æ¨è«– sem = true_std / np.sqrt(n) # æ¨™æº–èª¤å·® ci_95 = stats.norm.interval(0.95, loc=true_mean, scale=sem) print(f"\nn={n:3d}:") print(f" æ¨™æº–èª¤å·® (SEM): {sem:.2f} MPa") print(f" 95%ä¿¡é ¼åŒºé–“: [{ci_95[0]:.2f}, {ci_95[1]:.2f}] MPa") print(f" å®Ÿæ¸¬å¹³å‡ã®æ¨™æº–åå·®: {np.std(sample_means):.2f} MPa") plt.suptitle('ææ–™å¼·åº¦æ¸¬å®šã«ãŠã‘ã‚‹ä¸­å¿ƒæ¥µé™å®šç†ã®å¿œç”¨', fontsize=14, fontweight='bold') plt.tight_layout() plt.show() # å®Ÿç”¨çš„ãªæ¨è«– print("\n\nã€å®Ÿç”¨çš„ãªæ¨è«–ã€‘") print("="*60) n_test = 10 # å®Ÿéš›ã®è©¦é¨“ç‰‡æ•° test_samples = true_dist.rvs(n_test) test_mean = test_samples.mean() test_std = test_samples.std(ddof=1) print(f"è©¦é¨“ç‰‡æ•°: {n_test}") print(f"æ¸¬å®šã•ã‚ŒãŸå¹³å‡å¼·åº¦: {test_mean:.2f} MPa") print(f"æ¸¬å®šã•ã‚ŒãŸæ¨™æº–åå·®: {test_std:.2f} MPa") # tåˆ†å¸ƒã«ã‚ˆã‚‹ä¿¡é ¼åŒºé–“ï¼ˆçœŸã®åˆ†æ•£ãŒæœªçŸ¥ï¼‰ t_critical = stats.t.ppf(0.975, n_test - 1) ci_lower = test_mean - t_critical * test_std / np.sqrt(n_test) ci_upper = test_mean + t_critical * test_std / np.sqrt(n_test) print(f"\n95%ä¿¡é ¼åŒºé–“ï¼ˆtåˆ†å¸ƒï¼‰: [{ci_lower:.2f}, {ci_upper:.2f}] MPa") print(f"çœŸã®å¹³å‡ {true_mean:.2f} MPa ã¯ä¿¡é ¼åŒºé–“ã«{'å«ã¾ã‚Œã‚‹' if ci_lower <= true_mean <= ci_upper else 'å«ã¾ã‚Œãªã„'}")

**ğŸ’¡ Note:** ææ–™ç§‘å­¦ã§ã¯ã€é™ã‚‰ã‚ŒãŸè©¦é¨“ç‰‡æ•°ã‹ã‚‰æ¯é›†å›£ã®ç‰¹æ€§ã‚’æ¨å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ä¸­å¿ƒæ¥µé™å®šç†ã«ã‚ˆã‚Šã€å…ƒã®åˆ†å¸ƒï¼ˆãƒ¯ã‚¤ãƒ–ãƒ«åˆ†å¸ƒãªã©ï¼‰ãŒæ­£è¦åˆ†å¸ƒã§ãªãã¦ã‚‚ã€æ¨™æœ¬å¹³å‡ã¯æ­£è¦åˆ†å¸ƒã«å¾“ã†ãŸã‚ã€tæ¤œå®šã‚„ä¿¡é ¼åŒºé–“ã®æ§‹ç¯‰ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚ 

## æ¼”ç¿’å•é¡Œ

**ğŸ“ æ¼”ç¿’1: å¤§æ•°ã®æ³•å‰‡ã®æ¤œè¨¼**  
ã‚µã‚¤ã‚³ãƒ­ã‚’æŒ¯ã‚‹å®Ÿé¨“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã—ã€ä»¥ä¸‹ã‚’ç¢ºèªã›ã‚ˆï¼š 

  1. å‡ºç›®ã®æ¨™æœ¬å¹³å‡ãŒç†è«–å€¤3.5ã«åæŸã™ã‚‹ã“ã¨ã‚’å¯è¦–åŒ–ã›ã‚ˆ
  2. ãƒã‚§ãƒ“ã‚·ã‚§ãƒ•ã®ä¸ç­‰å¼ã‚’ç”¨ã„ã¦ã€|XÌ„â‚™ - 3.5| > 0.2 ã¨ãªã‚‹ç¢ºç‡ã®ä¸Šç•Œã‚’è¨ˆç®—ã›ã‚ˆ
  3. ç•°ãªã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆn=10, 100, 1000ï¼‰ã§ã®åæŸé€Ÿåº¦ã‚’æ¯”è¼ƒã›ã‚ˆ

**ğŸ“ æ¼”ç¿’2: ä¸­å¿ƒæ¥µé™å®šç†ã®å¿œç”¨**  
ä¸€æ§˜åˆ†å¸ƒ U(0, 10) ã‹ã‚‰ç‹¬ç«‹ã« n å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å–ã‚Šã€æ¨™æœ¬å¹³å‡ã‚’è¨ˆç®—ã™ã‚‹ï¼š 

  1. n=5, 10, 30, 100 ã®å„å ´åˆã§ã€æ¨™æœ¬å¹³å‡ã®åˆ†å¸ƒã‚’ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã§å¯è¦–åŒ–ã›ã‚ˆ
  2. æ¨™æº–åŒ–ã—ãŸæ¨™æœ¬å¹³å‡ãŒæ¨™æº–æ­£è¦åˆ†å¸ƒã«å¾“ã†ã“ã¨ã‚’ Q-Q ãƒ—ãƒ­ãƒƒãƒˆã§ç¢ºèªã›ã‚ˆ
  3. Kolmogorov-Smirnovæ¤œå®šã‚’ç”¨ã„ã¦ã€æ­£è¦æ€§ã‚’çµ±è¨ˆçš„ã«æ¤œè¨¼ã›ã‚ˆ

**ğŸ“ æ¼”ç¿’3: ä¿¡é ¼åŒºé–“ã®æ§‹ç¯‰**  
æ­£è¦åˆ†å¸ƒ N(100, 15Â²) ã‹ã‚‰ n=25 ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’è¨ˆç®—ã›ã‚ˆï¼š 

  1. æ¨™æœ¬å¹³å‡ã®95%ä¿¡é ¼åŒºé–“ã‚’æ§‹ç¯‰ã›ã‚ˆï¼ˆæ¯åˆ†æ•£æ—¢çŸ¥ã®å ´åˆï¼‰
  2. æ¨™æœ¬å¹³å‡ã¨æ¨™æœ¬åˆ†æ•£ã‚’è¨ˆç®—ã—ã€tåˆ†å¸ƒã‚’ç”¨ã„ãŸ95%ä¿¡é ¼åŒºé–“ã‚’æ§‹ç¯‰ã›ã‚ˆï¼ˆæ¯åˆ†æ•£æœªçŸ¥ã®å ´åˆï¼‰
  3. 1000å›ã®ç‹¬ç«‹ãªã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã„ã€ä¿¡é ¼åŒºé–“ãŒçœŸã®å¹³å‡ã‚’å«ã‚€å‰²åˆã‚’ç¢ºèªã›ã‚ˆ

[â† ç¬¬1ç« ](<chapter-1.html>) [ç¬¬3ç« ã¸ â†’](<chapter-3.html>)

### å…è²¬äº‹é …

  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹Code examplesã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚
  * å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚
