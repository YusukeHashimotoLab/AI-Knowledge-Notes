<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬1ç« : æ¨å®šç†è«–ã®åŸºç¤ | æ¨æ¸¬çµ±è¨ˆå­¦ã¨ãƒ™ã‚¤ã‚ºçµ±è¨ˆ</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>
            <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; line-height: 1.8; color: #333; background: #f5f5f5; }
        header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 1.5rem; text-align: center; }
        h1 { font-size: 1.8rem; margin-bottom: 0.5rem; }
        .subtitle { opacity: 0.9; }
        .container { max-width: 900px; margin: 2rem auto; padding: 0 1rem; }
        .breadcrumb { margin-bottom: 1.5rem; font-size: 0.9rem; }
        .breadcrumb a { color: #667eea; text-decoration: none; }
        .content { background: white; padding: 2.5rem; border-radius: 12px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); margin-bottom: 2rem; }
        h2 { color: #667eea; margin: 2rem 0 1rem 0; padding-bottom: 0.5rem; border-bottom: 2px solid #e0e0e0; }
        h3 { color: #764ba2; margin: 1.5rem 0 0.8rem 0; }
        .definition { background: #e7f3ff; border-left: 4px solid #667eea; padding: 1rem 1.5rem; margin: 1.5rem 0; border-radius: 4px; }
        .theorem { background: #f3e5f5; border-left: 4px solid #764ba2; padding: 1rem 1.5rem; margin: 1.5rem 0; border-radius: 4px; }
        .example { background: #fff3e0; border-left: 4px solid #ff9800; padding: 1rem 1.5rem; margin: 1.5rem 0; border-radius: 4px; }
        .code-title {
            background: #667eea;
            color: white;
            padding: 0.5rem 1rem;
            border-radius: 6px 6px 0 0;
            font-weight: 600;
            margin-top: 1.5rem;
        }
        .code-example {
            background: #1e1e1e;
            color: #d4d4d4;
            padding: 1.5rem;
            border-radius: 0 0 8px 8px;
            overflow-x: auto;
            margin: 0 0 1rem 0;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.6;
            white-space: pre-wrap;
        }
        .code-block {
            background: #1e1e1e;
            color: #d4d4d4;
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1rem 0;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.6;
            white-space: pre-wrap;
        }
        .code-block code {
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.6;
            white-space: pre-wrap;
        }
        .output { background: #f8f9fa; border: 1px solid #dee2e6; padding: 1rem; border-radius: 6px; margin: 1rem 0; font-family: monospace; font-size: 0.9rem; }
        table { width: 100%; border-collapse: collapse; margin: 1.5rem 0; }
        th, td { padding: 0.8rem; text-align: left; border: 1px solid #ddd; }
        th { background: #667eea; color: white; }
        .note { background: #fff3cd; border-left: 4px solid #ffc107; padding: 1rem 1.5rem; margin: 1.5rem 0; border-radius: 4px; }
        .exercise { background: #d4edda; border-left: 4px solid #28a745; padding: 1rem 1.5rem; margin: 1.5rem 0; border-radius: 4px; }
        .nav-buttons { display: flex; justify-content: space-between; margin: 2rem 0; }
        .nav-button { padding: 0.8rem 1.5rem; background: #667eea; color: white; text-decoration: none; border-radius: 6px; font-weight: 600; }
        .nav-button:hover { background: #764ba2; }
        footer { background: #2c3e50; color: white; text-align: center; padding: 2rem 1rem; margin-top: 3rem; }
        @media (max-width: 768px) { .content { padding: 1.5rem; } h1 { font-size: 1.5rem; } }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>ç¬¬1ç« : æ¨å®šç†è«–ã®åŸºç¤</h1>
        <p class="subtitle">Point Estimation, Maximum Likelihood, and Estimation Theory</p>
    </header>

    <div class="container">
        <div class="breadcrumb">
            <a href="../index.html">åŸºç¤æ•°ç†é“å ´</a> &gt;
            <a href="index.html">æ¨æ¸¬çµ±è¨ˆå­¦ã¨ãƒ™ã‚¤ã‚ºçµ±è¨ˆ</a> &gt;
            ç¬¬1ç« 
        </div>


        <div class="content">
            <h2>1.1 æ¨å®šç†è«–ã®æ¦‚è¦</h2>
            <p>
                <strong>æ¨å®šç†è«–ï¼ˆEstimation Theoryï¼‰</strong>ã¯ã€æ¨™æœ¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¯é›†å›£ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨æ¸¬ã™ã‚‹çµ±è¨ˆå­¦ã®åŸºç¤ã§ã™ã€‚
                ææ–™ç§‘å­¦ã§ã¯ã€å°‘æ•°ã®è©¦é¨“ç‰‡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ææ–™å…¨ä½“ã®ç‰¹æ€§ï¼ˆå¹³å‡å¼·åº¦ã€åˆ†æ•£ï¼‰ã‚’æ¨å®šã™ã‚‹å ´é¢ã§å¿…é ˆã¨ãªã‚Šã¾ã™ã€‚
            </p>

            <div class="theory-box">
                <h4>ğŸ“˜ æ¨å®šç†è«–ã®åŸºæœ¬æ¦‚å¿µ</h4>
                <p><strong>æ¯é›†å›£ï¼ˆPopulationï¼‰</strong>ï¼šèª¿æŸ»å¯¾è±¡ã®å…¨ä½“é›†åˆ</p>
                <p><strong>æ¨™æœ¬ï¼ˆSampleï¼‰</strong>ï¼šæ¯é›†å›£ã‹ã‚‰æŠ½å‡ºã—ãŸä¸€éƒ¨ã®ãƒ‡ãƒ¼ã‚¿</p>
                <p><strong>æ¨å®šé‡ï¼ˆEstimatorï¼‰</strong>ï¼šæ¨™æœ¬ã‹ã‚‰è¨ˆç®—ã•ã‚Œã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¨å®šå€¤ã‚’ä¸ãˆã‚‹é–¢æ•°</p>
                <p><strong>æ¨å®šå€¤ï¼ˆEstimateï¼‰</strong>ï¼šæ¨å®šé‡ã«æ¨™æœ¬ãƒ‡ãƒ¼ã‚¿ã‚’ä»£å…¥ã—ã¦å¾—ã‚‰ã‚Œã‚‹å…·ä½“çš„ãªæ•°å€¤</p>
            </div>

            <p>
                ä¾‹ãˆã°ã€æ¯å¹³å‡ \( \mu \) ã®æ¨å®šé‡ã¨ã—ã¦æ¨™æœ¬å¹³å‡ \( \bar{X} = \frac{1}{n}\sum_{i=1}^n X_i \) ã‚’ç”¨ã„ã‚‹ã¨ãã€
                å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ \( x_1, \ldots, x_n \) ã‹ã‚‰è¨ˆç®—ã•ã‚Œã‚‹ \( \bar{x} \) ãŒæ¨å®šå€¤ã§ã™ã€‚
            </p>

            <h2>1.2 ç‚¹æ¨å®šã¨æ¨å®šé‡ã®æ€§è³ª</h2>

            <h3>1.2.1 ä¸åæ€§ï¼ˆUnbiasednessï¼‰</h3>
            <div class="theory-box">
                <h4>ğŸ“˜ ä¸åæ¨å®šé‡ã®å®šç¾©</h4>
                <p>æ¨å®šé‡ \( \hat{\theta} \) ãŒæ¯æ•° \( \theta \) ã®<strong>ä¸åæ¨å®šé‡</strong>ã§ã‚ã‚‹ã¨ã¯ï¼š</p>
                <div class="formula">
                    $$ E[\hat{\theta}] = \theta $$
                </div>
                <p>ã™ãªã‚ã¡ã€æ¨å®šé‡ã®æœŸå¾…å€¤ãŒçœŸã®æ¯æ•°ã¨ä¸€è‡´ã™ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚</p>
            </div>

            <div class="example">
                <h4>ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹1: æ¨™æœ¬å¹³å‡ã¨æ¨™æœ¬åˆ†æ•£ã®ä¸åæ€§æ¤œè¨¼</h4>
                <pre><code>import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# çœŸã®æ¯é›†å›£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
mu_true = 50  # æ¯å¹³å‡
sigma_true = 10  # æ¯æ¨™æº–åå·®

# ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
n_samples = 30  # æ¨™æœ¬ã‚µã‚¤ã‚º
n_simulations = 10000  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å›æ•°

# æ¨å®šé‡ã®åˆ†å¸ƒã‚’è¨˜éŒ²
sample_means = []
sample_vars_biased = []  # ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Šï¼ˆn ã§å‰²ã‚‹ï¼‰
sample_vars_unbiased = []  # ä¸åæ¨å®šé‡ï¼ˆn-1 ã§å‰²ã‚‹ï¼‰

np.random.seed(42)
for _ in range(n_simulations):
    # æ¨™æœ¬æŠ½å‡º
    sample = np.random.normal(mu_true, sigma_true, n_samples)

    # æ¨™æœ¬å¹³å‡
    sample_means.append(np.mean(sample))

    # æ¨™æœ¬åˆ†æ•£ï¼ˆãƒã‚¤ã‚¢ã‚¹ã‚ã‚Šï¼‰
    sample_vars_biased.append(np.var(sample, ddof=0))

    # æ¨™æœ¬åˆ†æ•£ï¼ˆä¸åæ¨å®šé‡ï¼‰
    sample_vars_unbiased.append(np.var(sample, ddof=1))

# çµæœã®æ¤œè¨¼
print("=== ä¸åæ€§ã®æ¤œè¨¼ ===")
print(f"æ¯å¹³å‡: {mu_true}")
print(f"æ¨™æœ¬å¹³å‡ã®æœŸå¾…å€¤: {np.mean(sample_means):.4f}")
print(f"æ¨™æœ¬å¹³å‡ã®æ¨™æº–èª¤å·®: {np.std(sample_means):.4f}")
print()
print(f"æ¯åˆ†æ•£: {sigma_true**2}")
print(f"ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Šæ¨™æœ¬åˆ†æ•£ã®æœŸå¾…å€¤: {np.mean(sample_vars_biased):.4f}")
print(f"ä¸åæ¨™æœ¬åˆ†æ•£ã®æœŸå¾…å€¤: {np.mean(sample_vars_unbiased):.4f}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# æ¨™æœ¬å¹³å‡ã®åˆ†å¸ƒ
axes[0].hist(sample_means, bins=50, density=True, alpha=0.7,
             color='skyblue', edgecolor='black')
axes[0].axvline(mu_true, color='red', linestyle='--', linewidth=2,
                label=f'çœŸã®æ¯å¹³å‡: {mu_true}')
axes[0].axvline(np.mean(sample_means), color='blue', linestyle='--',
                linewidth=2, label=f'æ¨å®šé‡ã®æœŸå¾…å€¤: {np.mean(sample_means):.2f}')
axes[0].set_xlabel('æ¨™æœ¬å¹³å‡')
axes[0].set_ylabel('å¯†åº¦')
axes[0].set_title('æ¨™æœ¬å¹³å‡ã®åˆ†å¸ƒï¼ˆä¸åæ€§ã®ç¢ºèªï¼‰')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# æ¨™æœ¬åˆ†æ•£ã®åˆ†å¸ƒ
axes[1].hist(sample_vars_biased, bins=50, density=True, alpha=0.5,
             color='orange', edgecolor='black', label='ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Š (ddof=0)')
axes[1].hist(sample_vars_unbiased, bins=50, density=True, alpha=0.5,
             color='green', edgecolor='black', label='ä¸åæ¨å®šé‡ (ddof=1)')
axes[1].axvline(sigma_true**2, color='red', linestyle='--',
                linewidth=2, label=f'çœŸã®æ¯åˆ†æ•£: {sigma_true**2}')
axes[1].set_xlabel('æ¨™æœ¬åˆ†æ•£')
axes[1].set_ylabel('å¯†åº¦')
axes[1].set_title('æ¨™æœ¬åˆ†æ•£ã®åˆ†å¸ƒï¼ˆä¸åæ€§ã®æ¯”è¼ƒï¼‰')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ç†è«–çš„ãªæ¨™æº–èª¤å·®ã¨ã®æ¯”è¼ƒ
theoretical_se = sigma_true / np.sqrt(n_samples)
print(f"\nç†è«–çš„æ¨™æº–èª¤å·®: {theoretical_se:.4f}")
print(f"å®Ÿæ¸¬æ¨™æº–èª¤å·®: {np.std(sample_means):.4f}")</code></pre>
            </div>

            <div class="note">
                <strong>ğŸ“Œ é‡è¦ãƒã‚¤ãƒ³ãƒˆ</strong><br>
                æ¨™æœ¬å¹³å‡ã¯æ¯å¹³å‡ã®ä¸åæ¨å®šé‡ã§ã™ãŒã€æ¨™æœ¬åˆ†æ•£ã¯ \( n-1 \) ã§å‰²ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼ˆBesselã®è£œæ­£ï¼‰ã€‚
                ã“ã‚Œã¯æ¨™æœ¬åˆ†æ•£ãŒéå°æ¨å®šã™ã‚‹å‚¾å‘ã‚’è£œæ­£ã™ã‚‹ãŸã‚ã§ã™ã€‚
            </div>

            <h3>1.2.2 ä¸€è‡´æ€§ï¼ˆConsistencyï¼‰</h3>
            <div class="theory-box">
                <h4>ğŸ“˜ ä¸€è‡´æ¨å®šé‡ã®å®šç¾©</h4>
                <p>æ¨å®šé‡ \( \hat{\theta}_n \) ãŒ<strong>ä¸€è‡´æ¨å®šé‡</strong>ã§ã‚ã‚‹ã¨ã¯ã€æ¨™æœ¬ã‚µã‚¤ã‚º \( n \to \infty \) ã®ã¨ãï¼š</p>
                <div class="formula">
                    $$ \hat{\theta}_n \xrightarrow{P} \theta $$
                </div>
                <p>ã™ãªã‚ã¡ã€æ¨™æœ¬ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹ã«ã¤ã‚Œã¦çœŸã®æ¯æ•°ã«ç¢ºç‡åæŸã—ã¾ã™ã€‚</p>
            </div>

            <h3>1.2.3 æœ‰åŠ¹æ€§ï¼ˆEfficiencyï¼‰</h3>
            <p>
                è¤‡æ•°ã®ä¸åæ¨å®šé‡ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€<strong>åˆ†æ•£ãŒæœ€å°</strong>ã®ã‚‚ã®ãŒæœ€ã‚‚æœ‰åŠ¹ã§ã™ã€‚
                CramÃ©r-Raoä¸‹ç•Œã¯ã€ä¸åæ¨å®šé‡ã®åˆ†æ•£ãŒé”æˆã§ãã‚‹ç†è«–çš„ä¸‹é™ã‚’ä¸ãˆã¾ã™ã€‚
            </p>

            <h2>1.3 æœ€å°¤æ¨å®šæ³•ï¼ˆMaximum Likelihood Estimationï¼‰</h2>

            <div class="theory-box">
                <h4>ğŸ“˜ æœ€å°¤æ¨å®šæ³•ã®åŸç†</h4>
                <p>è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ \( x_1, \ldots, x_n \) ãŒå¾—ã‚‰ã‚ŒãŸã¨ãã€ã“ã‚Œã‚‰ãŒæœ€ã‚‚ç”Ÿã˜ã‚„ã™ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ \( \theta \) ã‚’æ¨å®šã™ã‚‹æ–¹æ³•ã§ã™ã€‚</p>
                <p><strong>å°¤åº¦é–¢æ•°ï¼ˆLikelihood Functionï¼‰</strong>ï¼š</p>
                <div class="formula">
                    $$ L(\theta; x_1, \ldots, x_n) = \prod_{i=1}^n f(x_i; \theta) $$
                </div>
                <p><strong>å¯¾æ•°å°¤åº¦é–¢æ•°ï¼ˆLog-Likelihoodï¼‰</strong>ï¼š</p>
                <div class="formula">
                    $$ \ell(\theta) = \log L(\theta) = \sum_{i=1}^n \log f(x_i; \theta) $$
                </div>
                <p><strong>æœ€å°¤æ¨å®šé‡ï¼ˆMLEï¼‰</strong>ã¯ \( \ell(\theta) \) ã‚’æœ€å¤§åŒ–ã™ã‚‹ \( \hat{\theta}_{\text{MLE}} \) ã§ã™ã€‚</p>
            </div>

            <div class="example">
                <h4>ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹2: æ­£è¦åˆ†å¸ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€å°¤æ¨å®š</h4>
                <pre><code>import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize
from scipy import stats

# çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
mu_true = 100
sigma_true = 15

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
n = 50
data = np.random.normal(mu_true, sigma_true, n)

# å¯¾æ•°å°¤åº¦é–¢æ•°ï¼ˆæ­£è¦åˆ†å¸ƒï¼‰
def neg_log_likelihood(params, data):
    """è² ã®å¯¾æ•°å°¤åº¦ï¼ˆæœ€å°åŒ–ã®ãŸã‚ï¼‰"""
    mu, sigma = params
    if sigma <= 0:
        return np.inf
    n = len(data)
    return n/2 * np.log(2*np.pi*sigma**2) + np.sum((data - mu)**2) / (2*sigma**2)

# æœ€å°¤æ¨å®šï¼ˆæ•°å€¤æœ€é©åŒ–ï¼‰
initial_guess = [np.mean(data), np.std(data)]
result = minimize(neg_log_likelihood, initial_guess, args=(data,),
                  method='Nelder-Mead')

mu_mle, sigma_mle = result.x

print("=== æœ€å°¤æ¨å®šçµæœ ===")
print(f"çœŸã®æ¯å¹³å‡: {mu_true}, MLE: {mu_mle:.4f}")
print(f"çœŸã®æ¯æ¨™æº–åå·®: {sigma_true}, MLE: {sigma_mle:.4f}")
print(f"æ¨™æœ¬å¹³å‡ï¼ˆè§£æè§£ï¼‰: {np.mean(data):.4f}")
print(f"æ¨™æœ¬æ¨™æº–åå·®ï¼ˆè§£æè§£, ddof=0ï¼‰: {np.std(data, ddof=0):.4f}")

# å°¤åº¦é–¢æ•°ã®å¯è¦–åŒ–
mu_grid = np.linspace(90, 110, 100)
sigma_grid = np.linspace(10, 20, 100)
MU, SIGMA = np.meshgrid(mu_grid, sigma_grid)

# å¯¾æ•°å°¤åº¦ã®è¨ˆç®—
log_likelihood = np.zeros_like(MU)
for i in range(len(mu_grid)):
    for j in range(len(sigma_grid)):
        log_likelihood[j, i] = -neg_log_likelihood([MU[j, i], SIGMA[j, i]], data)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# å¯¾æ•°å°¤åº¦ã®ç­‰é«˜ç·šå›³
contour = axes[0].contourf(MU, SIGMA, log_likelihood, levels=30, cmap='viridis')
axes[0].plot(mu_true, sigma_true, 'r*', markersize=15, label='çœŸã®å€¤')
axes[0].plot(mu_mle, sigma_mle, 'wo', markersize=10, label='MLE')
axes[0].set_xlabel('Î¼ (å¹³å‡)')
axes[0].set_ylabel('Ïƒ (æ¨™æº–åå·®)')
axes[0].set_title('å¯¾æ•°å°¤åº¦é–¢æ•°ã®ç­‰é«˜ç·šå›³')
axes[0].legend()
axes[0].grid(True, alpha=0.3)
plt.colorbar(contour, ax=axes[0], label='å¯¾æ•°å°¤åº¦')

# ãƒ‡ãƒ¼ã‚¿ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã¨æ¨å®šã•ã‚ŒãŸåˆ†å¸ƒ
axes[1].hist(data, bins=15, density=True, alpha=0.7,
             color='skyblue', edgecolor='black', label='ãƒ‡ãƒ¼ã‚¿')
x_range = np.linspace(data.min(), data.max(), 200)
axes[1].plot(x_range, stats.norm.pdf(x_range, mu_true, sigma_true),
             'r-', linewidth=2, label=f'çœŸã®åˆ†å¸ƒ N({mu_true}, {sigma_true}Â²)')
axes[1].plot(x_range, stats.norm.pdf(x_range, mu_mle, sigma_mle),
             'b--', linewidth=2, label=f'MLEåˆ†å¸ƒ N({mu_mle:.1f}, {sigma_mle:.1f}Â²)')
axes[1].set_xlabel('å€¤')
axes[1].set_ylabel('ç¢ºç‡å¯†åº¦')
axes[1].set_title('ãƒ‡ãƒ¼ã‚¿ã¨æ¨å®šã•ã‚ŒãŸåˆ†å¸ƒ')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()</code></pre>
            </div>

            <div class="note">
                <strong>ğŸ“Œ é‡è¦ãƒã‚¤ãƒ³ãƒˆ</strong><br>
                æ­£è¦åˆ†å¸ƒã®å ´åˆã€MLEã®è§£æè§£ã¯æ¨™æœ¬å¹³å‡ã¨æ¨™æœ¬åˆ†æ•£ï¼ˆ\( n \)ã§å‰²ã£ãŸã‚‚ã®ï¼‰ã«ä¸€è‡´ã—ã¾ã™ã€‚
                ä¸€èˆ¬çš„ãªåˆ†å¸ƒã§ã¯æ•°å€¤æœ€é©åŒ–ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚
            </div>

            <div class="example">
                <h4>ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹3: äºŒé …åˆ†å¸ƒãƒ»ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒã®æœ€å°¤æ¨å®š</h4>
                <pre><code>import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# === äºŒé …åˆ†å¸ƒã®æœ€å°¤æ¨å®š ===
print("=== äºŒé …åˆ†å¸ƒã®æœ€å°¤æ¨å®š ===")
# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆn=10ã®è©¦è¡Œã‚’50å›å®Ÿæ–½ï¼‰
n_trials = 10
p_true = 0.3
np.random.seed(42)
data_binomial = np.random.binomial(n_trials, p_true, 50)

# MLEã®è§£æè§£: p_hat = (ç·æˆåŠŸæ•°) / (ç·è©¦è¡Œæ•°)
p_mle = np.sum(data_binomial) / (len(data_binomial) * n_trials)
print(f"çœŸã®p: {p_true}, MLE: {p_mle:.4f}")

# === ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒã®æœ€å°¤æ¨å®š ===
print("\n=== ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒã®æœ€å°¤æ¨å®š ===")
# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
lambda_true = 5.0
data_poisson = np.random.poisson(lambda_true, 100)

# MLEã®è§£æè§£: lambda_hat = æ¨™æœ¬å¹³å‡
lambda_mle = np.mean(data_poisson)
print(f"çœŸã®Î»: {lambda_true}, MLE: {lambda_mle:.4f}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# äºŒé …åˆ†å¸ƒ
x_binom = np.arange(0, n_trials+1)
axes[0].hist(data_binomial, bins=np.arange(-0.5, n_trials+1.5, 1),
             density=True, alpha=0.7, color='skyblue',
             edgecolor='black', label='ãƒ‡ãƒ¼ã‚¿')
axes[0].plot(x_binom, stats.binom.pmf(x_binom, n_trials, p_true),
             'ro-', markersize=8, linewidth=2, label=f'çœŸã®åˆ†å¸ƒ B({n_trials}, {p_true})')
axes[0].plot(x_binom, stats.binom.pmf(x_binom, n_trials, p_mle),
             'b^--', markersize=6, linewidth=2, label=f'MLEåˆ†å¸ƒ B({n_trials}, {p_mle:.2f})')
axes[0].set_xlabel('æˆåŠŸå›æ•°')
axes[0].set_ylabel('ç¢ºç‡è³ªé‡')
axes[0].set_title('äºŒé …åˆ†å¸ƒã®æœ€å°¤æ¨å®š')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒ
x_poisson = np.arange(0, max(data_poisson)+1)
axes[1].hist(data_poisson, bins=np.arange(-0.5, max(data_poisson)+1.5, 1),
             density=True, alpha=0.7, color='lightgreen',
             edgecolor='black', label='ãƒ‡ãƒ¼ã‚¿')
axes[1].plot(x_poisson, stats.poisson.pmf(x_poisson, lambda_true),
             'ro-', markersize=6, linewidth=2, label=f'çœŸã®åˆ†å¸ƒ Poisson({lambda_true})')
axes[1].plot(x_poisson, stats.poisson.pmf(x_poisson, lambda_mle),
             'b^--', markersize=5, linewidth=2, label=f'MLEåˆ†å¸ƒ Poisson({lambda_mle:.2f})')
axes[1].set_xlabel('ç™ºç”Ÿå›æ•°')
axes[1].set_ylabel('ç¢ºç‡è³ªé‡')
axes[1].set_title('ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒã®æœ€å°¤æ¨å®š')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()</code></pre>
            </div>

            <h2>1.4 ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ³•ï¼ˆMethod of Momentsï¼‰</h2>

            <div class="theory-box">
                <h4>ğŸ“˜ ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ³•ã®åŸç†</h4>
                <p>æ¯é›†å›£ã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆï¼ˆç©ç‡ï¼‰ã¨æ¨™æœ¬ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã‚’ç­‰å€¤ã™ã‚‹ã“ã¨ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®šã—ã¾ã™ã€‚</p>
                <p><strong>kæ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆ</strong>ï¼š</p>
                <div class="formula">
                    $$ m_k = E[X^k], \quad \hat{m}_k = \frac{1}{n}\sum_{i=1}^n X_i^k $$
                </div>
                <p>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¨åŒæ•°ã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ–¹ç¨‹å¼ã‚’ç«‹ã¦ã¦è§£ãã¾ã™ã€‚</p>
            </div>

            <div class="example">
                <h4>ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹4: ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ³•ã«ã‚ˆã‚‹æ¨å®šï¼ˆã‚¬ãƒ³ãƒåˆ†å¸ƒï¼‰</h4>
                <pre><code>import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from scipy.optimize import fsolve

# çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚¬ãƒ³ãƒåˆ†å¸ƒï¼‰
# ã‚¬ãƒ³ãƒåˆ†å¸ƒ: Gamma(k, Î¸), E[X] = kÎ¸, Var[X] = kÎ¸Â²
k_true = 3.0  # å½¢çŠ¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
theta_true = 2.0  # ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
n = 200
data = np.random.gamma(k_true, theta_true, n)

# ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ³•ã«ã‚ˆã‚‹æ¨å®š
# E[X] = kÎ¸, Var[X] = kÎ¸Â² ã‚ˆã‚Š
# m1 = kÎ¸, m2 - m1Â² = kÎ¸Â²
m1 = np.mean(data)
m2 = np.mean(data**2)
var_sample = m2 - m1**2

# é€£ç«‹æ–¹ç¨‹å¼: m1 = kÎ¸, var = kÎ¸Â²
# var/m1 = Î¸ ã‚ˆã‚Š Î¸_hat = var/m1
# k_hat = m1/Î¸_hat = m1Â²/var
theta_mom = var_sample / m1
k_mom = m1**2 / var_sample

print("=== ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ³•ã«ã‚ˆã‚‹æ¨å®šï¼ˆã‚¬ãƒ³ãƒåˆ†å¸ƒï¼‰ ===")
print(f"çœŸã®å½¢çŠ¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ k: {k_true}")
print(f"ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ³•æ¨å®š k: {k_mom:.4f}")
print(f"çœŸã®ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ Î¸: {theta_true}")
print(f"ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ³•æ¨å®š Î¸: {theta_mom:.4f}")

# æœ€å°¤æ¨å®šã¨ã®æ¯”è¼ƒï¼ˆæ•°å€¤è§£æ³•ãŒå¿…è¦ï¼‰
def gamma_mle_equations(params, data):
    """ã‚¬ãƒ³ãƒåˆ†å¸ƒã®MLEã®æ–¹ç¨‹å¼"""
    k, theta = params
    n = len(data)
    eq1 = n * np.log(theta) + np.sum(np.log(data)) - n * (np.log(k) + stats.digamma(k))
    eq2 = np.sum(data) - n * k * theta
    return [eq1, eq2]

# åˆæœŸå€¤ã¨ã—ã¦ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ³•ã®çµæœã‚’ä½¿ç”¨
initial_guess = [k_mom, theta_mom]
k_mle, theta_mle = fsolve(gamma_mle_equations, initial_guess, args=(data,))

print(f"\næœ€å°¤æ¨å®š k: {k_mle:.4f}")
print(f"æœ€å°¤æ¨å®š Î¸: {theta_mle:.4f}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# ãƒ‡ãƒ¼ã‚¿ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã¨æ¨å®šåˆ†å¸ƒ
x_range = np.linspace(0, data.max(), 200)
axes[0].hist(data, bins=30, density=True, alpha=0.7,
             color='skyblue', edgecolor='black', label='ãƒ‡ãƒ¼ã‚¿')
axes[0].plot(x_range, stats.gamma.pdf(x_range, k_true, scale=theta_true),
             'r-', linewidth=2.5, label=f'çœŸã®åˆ†å¸ƒ Î“({k_true}, {theta_true})')
axes[0].plot(x_range, stats.gamma.pdf(x_range, k_mom, scale=theta_mom),
             'g--', linewidth=2, label=f'ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ³• Î“({k_mom:.2f}, {theta_mom:.2f})')
axes[0].plot(x_range, stats.gamma.pdf(x_range, k_mle, scale=theta_mle),
             'b:', linewidth=2, label=f'MLE Î“({k_mle:.2f}, {theta_mle:.2f})')
axes[0].set_xlabel('å€¤')
axes[0].set_ylabel('ç¢ºç‡å¯†åº¦')
axes[0].set_title('ã‚¬ãƒ³ãƒåˆ†å¸ƒã®æ¨å®šï¼ˆãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ³• vs MLEï¼‰')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Q-Qãƒ—ãƒ­ãƒƒãƒˆï¼ˆæ¨å®šç²¾åº¦ã®è¦–è¦šçš„è©•ä¾¡ï¼‰
theoretical_quantiles = np.linspace(0.01, 0.99, 100)
data_sorted = np.sort(data)
empirical_quantiles = np.linspace(0, 1, len(data_sorted))

for method, k, theta, color, linestyle in [
    ('çœŸã®åˆ†å¸ƒ', k_true, theta_true, 'red', '-'),
    ('ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ³•', k_mom, theta_mom, 'green', '--'),
    ('MLE', k_mle, theta_mle, 'blue', ':')
]:
    theoretical_values = stats.gamma.ppf(theoretical_quantiles, k, scale=theta)
    axes[1].plot(theoretical_values,
                 np.quantile(data, theoretical_quantiles),
                 color=color, linestyle=linestyle, linewidth=2, label=method)

axes[1].plot([0, data.max()], [0, data.max()], 'k--', alpha=0.5, label='y=x')
axes[1].set_xlabel('ç†è«–åˆ†ä½æ•°')
axes[1].set_ylabel('æ¨™æœ¬åˆ†ä½æ•°')
axes[1].set_title('Q-Qãƒ—ãƒ­ãƒƒãƒˆï¼ˆæ¨å®šç²¾åº¦ã®æ¯”è¼ƒï¼‰')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()</code></pre>
            </div>

            <h2>1.5 æ¨å®šé‡ã®ãƒã‚¤ã‚¢ã‚¹-åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•</h2>

            <div class="theory-box">
                <h4>ğŸ“˜ å¹³å‡äºŒä¹—èª¤å·®ï¼ˆMSEï¼‰ã®åˆ†è§£</h4>
                <p>æ¨å®šé‡ \( \hat{\theta} \) ã®<strong>å¹³å‡äºŒä¹—èª¤å·®ï¼ˆMean Squared Errorï¼‰</strong>ã¯ï¼š</p>
                <div class="formula">
                    $$ \text{MSE}(\hat{\theta}) = E[(\hat{\theta} - \theta)^2] = \text{Bias}^2(\hat{\theta}) + \text{Var}(\hat{\theta}) $$
                </div>
                <p>ã“ã“ã§ï¼š</p>
                <ul>
                    <li><strong>ãƒã‚¤ã‚¢ã‚¹</strong>: \( \text{Bias}(\hat{\theta}) = E[\hat{\theta}] - \theta \)</li>
                    <li><strong>åˆ†æ•£</strong>: \( \text{Var}(\hat{\theta}) = E[(\hat{\theta} - E[\hat{\theta}])^2] \)</li>
                </ul>
                <p>ä¸åæ¨å®šé‡ã§ã‚‚åˆ†æ•£ãŒå¤§ãã‘ã‚Œã°MSEãŒå¤§ãããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚</p>
            </div>

            <div class="example">
                <h4>ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹5: ãƒã‚¤ã‚¢ã‚¹-åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®å¯è¦–åŒ–</h4>
                <pre><code>import numpy as np
import matplotlib.pyplot as plt

# çœŸã®æ¯å¹³å‡ã¨æ¯åˆ†æ•£
mu_true = 100
sigma_true = 20

# æ¨™æœ¬ã‚µã‚¤ã‚º
n = 20

# ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
np.random.seed(42)
n_simulations = 5000

# 3ç¨®é¡ã®æ¨å®šé‡
# 1. æ¨™æœ¬å¹³å‡ï¼ˆä¸åï¼‰
# 2. å®šæ•°æ¨å®šé‡ï¼ˆãƒã‚¤ã‚¢ã‚¹ã‚ã‚Šã€åˆ†æ•£ã‚¼ãƒ­ï¼‰
# 3. Shrinkageæ¨å®šé‡ï¼ˆãƒã‚¤ã‚¢ã‚¹ã‚ã‚Šã€åˆ†æ•£å°ï¼‰

estimates_unbiased = []
estimates_constant = []
estimates_shrinkage = []

constant_value = 95  # äº‹å‰çŸ¥è­˜ã‹ã‚‰ã®æ¨æ¸¬å€¤
shrinkage_factor = 0.7  # ç¸®å°ä¿‚æ•°

for _ in range(n_simulations):
    sample = np.random.normal(mu_true, sigma_true, n)

    # ä¸åæ¨å®šé‡ï¼ˆæ¨™æœ¬å¹³å‡ï¼‰
    estimates_unbiased.append(np.mean(sample))

    # å®šæ•°æ¨å®šé‡
    estimates_constant.append(constant_value)

    # Shrinkageæ¨å®šé‡: æ¨™æœ¬å¹³å‡ã‚’å®šæ•°å€¤ã«ç¸®å°
    estimates_shrinkage.append(
        shrinkage_factor * np.mean(sample) + (1 - shrinkage_factor) * constant_value
    )

# MSEã®è¨ˆç®—
def compute_mse_components(estimates, true_value):
    estimates = np.array(estimates)
    bias = np.mean(estimates) - true_value
    variance = np.var(estimates)
    mse = np.mean((estimates - true_value)**2)
    return bias, variance, mse

bias_u, var_u, mse_u = compute_mse_components(estimates_unbiased, mu_true)
bias_c, var_c, mse_c = compute_mse_components(estimates_constant, mu_true)
bias_s, var_s, mse_s = compute_mse_components(estimates_shrinkage, mu_true)

print("=== ãƒã‚¤ã‚¢ã‚¹-åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ• ===")
print(f"\nä¸åæ¨å®šé‡ï¼ˆæ¨™æœ¬å¹³å‡ï¼‰:")
print(f"  ãƒã‚¤ã‚¢ã‚¹: {bias_u:.4f}, åˆ†æ•£: {var_u:.4f}, MSE: {mse_u:.4f}")
print(f"\nå®šæ•°æ¨å®šé‡:")
print(f"  ãƒã‚¤ã‚¢ã‚¹: {bias_c:.4f}, åˆ†æ•£: {var_c:.4f}, MSE: {mse_c:.4f}")
print(f"\nShrinkageæ¨å®šé‡:")
print(f"  ãƒã‚¤ã‚¢ã‚¹: {bias_s:.4f}, åˆ†æ•£: {var_s:.4f}, MSE: {mse_s:.4f}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# æ¨å®šé‡ã®åˆ†å¸ƒ
axes[0, 0].hist(estimates_unbiased, bins=50, density=True, alpha=0.6,
                color='blue', edgecolor='black', label='ä¸åæ¨å®šé‡')
axes[0, 0].hist(estimates_shrinkage, bins=50, density=True, alpha=0.6,
                color='green', edgecolor='black', label='Shrinkageæ¨å®šé‡')
axes[0, 0].axvline(mu_true, color='red', linestyle='--', linewidth=2,
                   label=f'çœŸã®å€¤: {mu_true}')
axes[0, 0].axvline(constant_value, color='orange', linestyle='--',
                   linewidth=2, label=f'å®šæ•°å€¤: {constant_value}')
axes[0, 0].set_xlabel('æ¨å®šå€¤')
axes[0, 0].set_ylabel('å¯†åº¦')
axes[0, 0].set_title('æ¨å®šé‡ã®åˆ†å¸ƒ')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# MSEåˆ†è§£ã®æ£’ã‚°ãƒ©ãƒ•
methods = ['ä¸åæ¨å®šé‡', 'å®šæ•°æ¨å®šé‡', 'Shrinkageæ¨å®šé‡']
biases_sq = [bias_u**2, bias_c**2, bias_s**2]
variances = [var_u, var_c, var_s]
x_pos = np.arange(len(methods))

axes[0, 1].bar(x_pos, biases_sq, width=0.35, label='ãƒã‚¤ã‚¢ã‚¹Â²',
               color='orange', alpha=0.8)
axes[0, 1].bar(x_pos, variances, width=0.35, bottom=biases_sq,
               label='åˆ†æ•£', color='skyblue', alpha=0.8)
axes[0, 1].set_xticks(x_pos)
axes[0, 1].set_xticklabels(methods, rotation=15, ha='right')
axes[0, 1].set_ylabel('å€¤')
axes[0, 1].set_title('MSEã®åˆ†è§£ï¼ˆãƒã‚¤ã‚¢ã‚¹Â² + åˆ†æ•£ï¼‰')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3, axis='y')

# äºŒä¹—èª¤å·®ã®åˆ†å¸ƒ
sq_errors_u = (np.array(estimates_unbiased) - mu_true)**2
sq_errors_s = (np.array(estimates_shrinkage) - mu_true)**2

axes[1, 0].hist(sq_errors_u, bins=50, density=True, alpha=0.6,
                color='blue', edgecolor='black', label='ä¸åæ¨å®šé‡')
axes[1, 0].hist(sq_errors_s, bins=50, density=True, alpha=0.6,
                color='green', edgecolor='black', label='Shrinkageæ¨å®šé‡')
axes[1, 0].axvline(mse_u, color='blue', linestyle='--', linewidth=2)
axes[1, 0].axvline(mse_s, color='green', linestyle='--', linewidth=2)
axes[1, 0].set_xlabel('äºŒä¹—èª¤å·®')
axes[1, 0].set_ylabel('å¯†åº¦')
axes[1, 0].set_title('äºŒä¹—èª¤å·®ã®åˆ†å¸ƒ')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# Shrinkageä¿‚æ•°ã¨MSEã®é–¢ä¿‚
shrinkage_factors = np.linspace(0, 1, 50)
mse_by_shrinkage = []

for sf in shrinkage_factors:
    est = [sf * e + (1-sf) * constant_value for e in estimates_unbiased]
    _, _, mse = compute_mse_components(est, mu_true)
    mse_by_shrinkage.append(mse)

axes[1, 1].plot(shrinkage_factors, mse_by_shrinkage, 'b-', linewidth=2)
axes[1, 1].axvline(shrinkage_factor, color='green', linestyle='--',
                   linewidth=2, label=f'é¸æŠã—ãŸä¿‚æ•°: {shrinkage_factor}')
axes[1, 1].axhline(mse_u, color='blue', linestyle=':', linewidth=2,
                   label=f'ä¸åæ¨å®šé‡ã®MSE: {mse_u:.2f}')
optimal_sf = shrinkage_factors[np.argmin(mse_by_shrinkage)]
axes[1, 1].axvline(optimal_sf, color='red', linestyle='--',
                   linewidth=2, label=f'æœ€é©ä¿‚æ•°: {optimal_sf:.2f}')
axes[1, 1].set_xlabel('Shrinkageä¿‚æ•°')
axes[1, 1].set_ylabel('MSE')
axes[1, 1].set_title('Shrinkageä¿‚æ•°ã¨MSEã®é–¢ä¿‚')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\næœ€é©Shrinkageä¿‚æ•°: {optimal_sf:.4f}")</code></pre>
            </div>

            <div class="note">
                <strong>ğŸ“Œ é‡è¦ãƒã‚¤ãƒ³ãƒˆ</strong><br>
                ä¸åæ¨å®šé‡ãŒå¸¸ã«æœ€è‰¯ã¨ã¯é™ã‚Šã¾ã›ã‚“ã€‚å°‘ã—ãƒã‚¤ã‚¢ã‚¹ã‚’è¨±å®¹ã™ã‚‹ã“ã¨ã§åˆ†æ•£ã‚’å¤§ããæ¸›ã‚‰ã›ã‚‹å ´åˆã€
                MSEã¯å°ã•ããªã‚Šã¾ã™ï¼ˆJames-Steinæ¨å®šé‡ãªã©ï¼‰ã€‚
            </div>

            <h2>1.6 Fisheræƒ…å ±é‡ã¨CramÃ©r-Raoä¸‹ç•Œ</h2>

            <div class="theory-box">
                <h4>ğŸ“˜ Fisheræƒ…å ±é‡</h4>
                <p>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ \( \theta \) ã®<strong>Fisheræƒ…å ±é‡</strong>ã¯ï¼š</p>
                <div class="formula">
                    $$ I(\theta) = E\left[\left(\frac{\partial \log f(X;\theta)}{\partial \theta}\right)^2\right] = -E\left[\frac{\partial^2 \log f(X;\theta)}{\partial \theta^2}\right] $$
                </div>
                <p>ãƒ‡ãƒ¼ã‚¿ãŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«é–¢ã—ã¦æŒã¤æƒ…å ±é‡ã‚’è¡¨ã—ã¾ã™ã€‚</p>

                <h4>ğŸ“˜ CramÃ©r-Raoä¸‹ç•Œ</h4>
                <p>ä»»æ„ã®ä¸åæ¨å®šé‡ \( \hat{\theta} \) ã®åˆ†æ•£ã¯ï¼š</p>
                <div class="formula">
                    $$ \text{Var}(\hat{\theta}) \geq \frac{1}{nI(\theta)} $$
                </div>
                <p>ã“ã®ä¸‹ç•Œã‚’é”æˆã™ã‚‹æ¨å®šé‡ãŒ<strong>æœ‰åŠ¹æ¨å®šé‡</strong>ã§ã™ã€‚</p>
            </div>

            <div class="example">
                <h4>ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹6: Fisheræƒ…å ±é‡ã¨CramÃ©r-Raoä¸‹ç•Œã®è¨ˆç®—</h4>
                <pre><code>import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# æ­£è¦åˆ†å¸ƒ N(Î¼, ÏƒÂ²) ã®Fisheræƒ…å ±é‡
# I(Î¼) = 1/ÏƒÂ², I(ÏƒÂ²) = 1/(2Ïƒâ´)

def normal_fisher_info_mean(sigma):
    """æ­£è¦åˆ†å¸ƒã®å¹³å‡ã«é–¢ã™ã‚‹Fisheræƒ…å ±é‡"""
    return 1 / sigma**2

def normal_fisher_info_var(sigma):
    """æ­£è¦åˆ†å¸ƒã®åˆ†æ•£ã«é–¢ã™ã‚‹Fisheræƒ…å ±é‡"""
    return 1 / (2 * sigma**4)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
mu_true = 50
sigma_true = 10
sample_sizes = np.array([10, 20, 50, 100, 200, 500])

# ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
n_simulations = 10000
np.random.seed(42)

results = []
for n in sample_sizes:
    sample_means = []
    sample_vars = []

    for _ in range(n_simulations):
        sample = np.random.normal(mu_true, sigma_true, n)
        sample_means.append(np.mean(sample))
        sample_vars.append(np.var(sample, ddof=1))

    # å®Ÿæ¸¬åˆ†æ•£
    empirical_var_mean = np.var(sample_means)
    empirical_var_var = np.var(sample_vars)

    # CramÃ©r-Raoä¸‹ç•Œ
    cr_bound_mean = 1 / (n * normal_fisher_info_mean(sigma_true))
    cr_bound_var = 1 / (n * normal_fisher_info_var(sigma_true))

    results.append({
        'n': n,
        'empirical_var_mean': empirical_var_mean,
        'cr_bound_mean': cr_bound_mean,
        'empirical_var_var': empirical_var_var,
        'cr_bound_var': cr_bound_var
    })

# çµæœã®è¡¨ç¤º
print("=== Fisheræƒ…å ±é‡ã¨CramÃ©r-Raoä¸‹ç•Œ ===")
print(f"æ­£è¦åˆ†å¸ƒ N({mu_true}, {sigma_true}Â²)")
print(f"\nFisheræƒ…å ±é‡ï¼ˆ1æ¨™æœ¬ã‚ãŸã‚Šï¼‰:")
print(f"  I(Î¼) = {normal_fisher_info_mean(sigma_true):.6f}")
print(f"  I(ÏƒÂ²) = {normal_fisher_info_var(sigma_true):.10f}")
print()

for r in results:
    print(f"n={r['n']}:")
    print(f"  å¹³å‡ã®æ¨å®šé‡: å®Ÿæ¸¬åˆ†æ•£={r['empirical_var_mean']:.4f}, "
          f"CRä¸‹ç•Œ={r['cr_bound_mean']:.4f}, "
          f"åŠ¹ç‡={(r['cr_bound_mean']/r['empirical_var_mean'])*100:.2f}%")
    print(f"  åˆ†æ•£ã®æ¨å®šé‡: å®Ÿæ¸¬åˆ†æ•£={r['empirical_var_var']:.4f}, "
          f"CRä¸‹ç•Œ={r['cr_bound_var']:.4f}, "
          f"åŠ¹ç‡={(r['cr_bound_var']/r['empirical_var_var'])*100:.2f}%")
    print()

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# å¹³å‡ã®æ¨å®šé‡
empirical_vars_mean = [r['empirical_var_mean'] for r in results]
cr_bounds_mean = [r['cr_bound_mean'] for r in results]

axes[0].plot(sample_sizes, empirical_vars_mean, 'bo-',
             markersize=8, linewidth=2, label='å®Ÿæ¸¬åˆ†æ•£')
axes[0].plot(sample_sizes, cr_bounds_mean, 'r^--',
             markersize=8, linewidth=2, label='CramÃ©r-Raoä¸‹ç•Œ')
axes[0].set_xlabel('æ¨™æœ¬ã‚µã‚¤ã‚º n')
axes[0].set_ylabel('åˆ†æ•£')
axes[0].set_title('æ¨™æœ¬å¹³å‡ã®åˆ†æ•£ vs CramÃ©r-Raoä¸‹ç•Œ')
axes[0].set_xscale('log')
axes[0].set_yscale('log')
axes[0].legend()
axes[0].grid(True, alpha=0.3, which='both')

# åˆ†æ•£ã®æ¨å®šé‡
empirical_vars_var = [r['empirical_var_var'] for r in results]
cr_bounds_var = [r['cr_bound_var'] for r in results]

axes[1].plot(sample_sizes, empirical_vars_var, 'bo-',
             markersize=8, linewidth=2, label='å®Ÿæ¸¬åˆ†æ•£')
axes[1].plot(sample_sizes, cr_bounds_var, 'r^--',
             markersize=8, linewidth=2, label='CramÃ©r-Raoä¸‹ç•Œ')
axes[1].set_xlabel('æ¨™æœ¬ã‚µã‚¤ã‚º n')
axes[1].set_ylabel('åˆ†æ•£')
axes[1].set_title('æ¨™æœ¬åˆ†æ•£ã®åˆ†æ•£ vs CramÃ©r-Raoä¸‹ç•Œ')
axes[1].set_xscale('log')
axes[1].set_yscale('log')
axes[1].legend()
axes[1].grid(True, alpha=0.3, which='both')

plt.tight_layout()
plt.show()</code></pre>
            </div>

            <div class="note">
                <strong>ğŸ“Œ é‡è¦ãƒã‚¤ãƒ³ãƒˆ</strong><br>
                æ­£è¦åˆ†å¸ƒã®å ´åˆã€æ¨™æœ¬å¹³å‡ã¯CramÃ©r-Raoä¸‹ç•Œã‚’é”æˆã™ã‚‹ï¼ˆ100%åŠ¹ç‡çš„ï¼‰ã®ã«å¯¾ã—ã€
                æ¨™æœ¬åˆ†æ•£ã¯æ¼¸è¿‘çš„ã«ã®ã¿ä¸‹ç•Œã‚’é”æˆã—ã¾ã™ã€‚
            </div>

            <h2>1.7 ææ–™ç‰¹æ€§ãƒ‡ãƒ¼ã‚¿ã®æ¨å®š</h2>

            <div class="example">
                <h4>ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹7: ææ–™å¼·åº¦ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆçš„æ¨å®š</h4>
                <pre><code>import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from scipy.optimize import minimize

# å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆå®Ÿéš›ã«ã¯å®Ÿé¨“ã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹ï¼‰
# ææ–™å¼·åº¦ã®åˆ†å¸ƒã¯æ­£è¦åˆ†å¸ƒã¾ãŸã¯Weibullåˆ†å¸ƒã§ãƒ¢ãƒ‡ãƒ«åŒ–ã•ã‚Œã‚‹ã“ã¨ãŒå¤šã„
np.random.seed(42)

# ã‚·ãƒŠãƒªã‚ª1: æ­£è¦åˆ†å¸ƒã§ãƒ¢ãƒ‡ãƒ«åŒ–
n_samples = 30
true_mean_strength = 450  # MPa
true_std_strength = 30  # MPa
strength_data_normal = np.random.normal(true_mean_strength, true_std_strength, n_samples)

# ã‚·ãƒŠãƒªã‚ª2: Weibullåˆ†å¸ƒã§ãƒ¢ãƒ‡ãƒ«åŒ–ï¼ˆè„†æ€§ææ–™ã«é©ã—ã¦ã„ã‚‹ï¼‰
# Weibullåˆ†å¸ƒ: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (k: å½¢çŠ¶, Î»: ã‚¹ã‚±ãƒ¼ãƒ«)
k_true = 15  # å½¢çŠ¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå¤§ãã„ã»ã©åˆ†æ•£ãŒå°ã•ã„ï¼‰
lambda_true = 470  # ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
strength_data_weibull = np.random.weibull(k_true, n_samples) * lambda_true

print("=== ææ–™å¼·åº¦ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆçš„æ¨å®š ===")
print("\nã€æ­£è¦åˆ†å¸ƒãƒ¢ãƒ‡ãƒ«ã€‘")
# æ­£è¦åˆ†å¸ƒã®æ¨å®š
mean_est = np.mean(strength_data_normal)
std_est = np.std(strength_data_normal, ddof=1)

print(f"æ¨™æœ¬å¹³å‡ï¼ˆMLEï¼‰: {mean_est:.2f} MPa (çœŸå€¤: {true_mean_strength} MPa)")
print(f"æ¨™æœ¬æ¨™æº–åå·®: {std_est:.2f} MPa (çœŸå€¤: {true_std_strength} MPa)")

# 95%ä¿¡é ¼åŒºé–“ï¼ˆæ¬¡ç« ã§è©³ã—ãæ‰±ã†ï¼‰
se = std_est / np.sqrt(n_samples)
ci_95 = stats.t.interval(0.95, n_samples-1, loc=mean_est, scale=se)
print(f"å¹³å‡å¼·åº¦ã®95%ä¿¡é ¼åŒºé–“: [{ci_95[0]:.2f}, {ci_95[1]:.2f}] MPa")

print("\nã€Weibullåˆ†å¸ƒãƒ¢ãƒ‡ãƒ«ã€‘")
# Weibullåˆ†å¸ƒã®MLEï¼ˆSciPyã®é–¢æ•°ã‚’ä½¿ç”¨ï¼‰
# scipy.stats.weibull_min.fit() ã¯ (c, loc, scale) ã‚’è¿”ã™
# c = k (å½¢çŠ¶), scale = Î» (ã‚¹ã‚±ãƒ¼ãƒ«)
params = stats.weibull_min.fit(strength_data_weibull, floc=0)
k_est, loc_est, lambda_est = params

print(f"å½¢çŠ¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿kï¼ˆMLEï¼‰: {k_est:.2f} (çœŸå€¤: {k_true})")
print(f"ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Î»ï¼ˆMLEï¼‰: {lambda_est:.2f} (çœŸå€¤: {lambda_true})")
print(f"å¹³å‡å¼·åº¦ï¼ˆWeibullï¼‰: {lambda_est * stats.gamma(1 + 1/k_est):.2f} MPa")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# æ­£è¦åˆ†å¸ƒãƒ¢ãƒ‡ãƒ«
axes[0, 0].hist(strength_data_normal, bins=12, density=True, alpha=0.7,
                color='skyblue', edgecolor='black', label='å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿')
x_range = np.linspace(strength_data_normal.min(), strength_data_normal.max(), 200)
axes[0, 0].plot(x_range, stats.norm.pdf(x_range, true_mean_strength, true_std_strength),
                'r-', linewidth=2.5, label=f'çœŸã®åˆ†å¸ƒ N({true_mean_strength}, {true_std_strength}Â²)')
axes[0, 0].plot(x_range, stats.norm.pdf(x_range, mean_est, std_est),
                'b--', linewidth=2, label=f'æ¨å®šåˆ†å¸ƒ N({mean_est:.1f}, {std_est:.1f}Â²)')
axes[0, 0].set_xlabel('å¼·åº¦ [MPa]')
axes[0, 0].set_ylabel('ç¢ºç‡å¯†åº¦')
axes[0, 0].set_title('æ­£è¦åˆ†å¸ƒãƒ¢ãƒ‡ãƒ«: ææ–™å¼·åº¦ã®æ¨å®š')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# æ­£è¦åˆ†å¸ƒã®Q-Qãƒ—ãƒ­ãƒƒãƒˆ
stats.probplot(strength_data_normal, dist="norm", plot=axes[0, 1])
axes[0, 1].set_title('æ­£è¦Q-Qãƒ—ãƒ­ãƒƒãƒˆï¼ˆæ­£è¦æ€§ã®ç¢ºèªï¼‰')
axes[0, 1].grid(True, alpha=0.3)

# Weibullåˆ†å¸ƒãƒ¢ãƒ‡ãƒ«
axes[1, 0].hist(strength_data_weibull, bins=12, density=True, alpha=0.7,
                color='lightgreen', edgecolor='black', label='å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿')
x_range_w = np.linspace(0, strength_data_weibull.max(), 200)
axes[1, 0].plot(x_range_w, stats.weibull_min.pdf(x_range_w, k_true, scale=lambda_true),
                'r-', linewidth=2.5, label=f'çœŸã®åˆ†å¸ƒ Weibull({k_true}, {lambda_true})')
axes[1, 0].plot(x_range_w, stats.weibull_min.pdf(x_range_w, k_est, scale=lambda_est),
                'b--', linewidth=2, label=f'æ¨å®šåˆ†å¸ƒ Weibull({k_est:.1f}, {lambda_est:.1f})')
axes[1, 0].set_xlabel('å¼·åº¦ [MPa]')
axes[1, 0].set_ylabel('ç¢ºç‡å¯†åº¦')
axes[1, 0].set_title('Weibullåˆ†å¸ƒãƒ¢ãƒ‡ãƒ«: ææ–™å¼·åº¦ã®æ¨å®š')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# Weibullç¢ºç‡ãƒ—ãƒ­ãƒƒãƒˆ
sorted_data = np.sort(strength_data_weibull)
n = len(sorted_data)
empirical_cdf = np.arange(1, n+1) / (n+1)
weibull_y = np.log(-np.log(1 - empirical_cdf))
weibull_x = np.log(sorted_data)

axes[1, 1].plot(weibull_x, weibull_y, 'bo', markersize=6, label='å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿')
# æ¨å®šã•ã‚ŒãŸWeibullåˆ†å¸ƒã®ç†è«–ç›´ç·š
x_fit = np.array([weibull_x.min(), weibull_x.max()])
y_fit = k_est * (x_fit - np.log(lambda_est))
axes[1, 1].plot(x_fit, y_fit, 'r-', linewidth=2, label='æ¨å®šWeibullç›´ç·š')
axes[1, 1].set_xlabel('ln(å¼·åº¦)')
axes[1, 1].set_ylabel('ln(-ln(1-F))')
axes[1, 1].set_title('Weibullç¢ºç‡ãƒ—ãƒ­ãƒƒãƒˆï¼ˆé©åˆåº¦ã®ç¢ºèªï¼‰')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ç ´å£Šç¢ºç‡ã®æ¨å®šï¼ˆå·¥å­¦çš„ã«é‡è¦ï¼‰
print("\nã€ç ´å£Šç¢ºç‡ã®æ¨å®šã€‘")
critical_strength = 400  # MPa
prob_failure_normal = stats.norm.cdf(critical_strength, mean_est, std_est)
prob_failure_weibull = stats.weibull_min.cdf(critical_strength, k_est, scale=lambda_est)

print(f"400 MPaä»¥ä¸‹ã§ç ´å£Šã™ã‚‹ç¢ºç‡:")
print(f"  æ­£è¦åˆ†å¸ƒãƒ¢ãƒ‡ãƒ«: {prob_failure_normal*100:.2f}%")
print(f"  Weibullãƒ¢ãƒ‡ãƒ«: {prob_failure_weibull*100:.2f}%")</code></pre>
            </div>

            <div class="note">
                <strong>ğŸ“Œ å®Ÿè·µçš„ãƒã‚¤ãƒ³ãƒˆ</strong><br>
                ææ–™å¼·åº¦ãƒ‡ãƒ¼ã‚¿ã®è§£æã§ã¯ï¼š
                <ul>
                    <li>å»¶æ€§ææ–™ï¼ˆé‡‘å±ãªã©ï¼‰â†’ æ­£è¦åˆ†å¸ƒã¾ãŸã¯ãƒ­ã‚°æ­£è¦åˆ†å¸ƒ</li>
                    <li>è„†æ€§ææ–™ï¼ˆã‚»ãƒ©ãƒŸãƒƒã‚¯ã‚¹ãªã©ï¼‰â†’ Weibullåˆ†å¸ƒ</li>
                    <li>Q-Qãƒ—ãƒ­ãƒƒãƒˆã‚„ç¢ºç‡ãƒ—ãƒ­ãƒƒãƒˆã§åˆ†å¸ƒã®é©åˆåº¦ã‚’ç¢ºèª</li>
                    <li>ç ´å£Šç¢ºç‡ã‚„ä¿¡é ¼æ€§è©•ä¾¡ã«æ¨å®šçµæœã‚’æ´»ç”¨</li>
                </ul>
            </div>

            <div class="exercise">
                <h4>ğŸ“ ç·´ç¿’å•é¡Œ</h4>
                <ol>
                    <li>æ¨™æœ¬ä¸­å¤®å€¤ã¯æ¯å¹³å‡ã®ä¸åæ¨å®šé‡ã‹èª¿ã¹ã¦ãã ã•ã„ï¼ˆæ­£è¦åˆ†å¸ƒã®å ´åˆï¼‰ã€‚</li>
                    <li>æŒ‡æ•°åˆ†å¸ƒ \( f(x; \lambda) = \lambda e^{-\lambda x} \) ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ \( \lambda \) ã®æœ€å°¤æ¨å®šé‡ã‚’å°å‡ºã—ã¦ãã ã•ã„ã€‚</li>
                    <li>ä¸€æ§˜åˆ†å¸ƒ \( U(0, \theta) \) ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ \( \theta \) ã«å¯¾ã—ã¦ã€\( \max(X_1, \ldots, X_n) \) ãŒä¸€è‡´æ¨å®šé‡ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚</li>
                    <li>ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ \( p \) ã«é–¢ã™ã‚‹Fisheræƒ…å ±é‡ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚</li>
                </ol>
            </div>

            <h2>ã¾ã¨ã‚</h2>
            <ul>
                <li>æ¨å®šç†è«–ã¯æ¨™æœ¬ã‹ã‚‰æ¯é›†å›£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨æ¸¬ã™ã‚‹çµ±è¨ˆå­¦ã®åŸºç¤ã§ã‚ã‚‹</li>
                <li>ä¸åæ€§ãƒ»ä¸€è‡´æ€§ãƒ»æœ‰åŠ¹æ€§ã¯æ¨å®šé‡ã®é‡è¦ãªæ€§è³ªã§ã‚ã‚‹</li>
                <li>æœ€å°¤æ¨å®šæ³•ã¯æœ€ã‚‚ä¸€èˆ¬çš„ã‹ã¤å¼·åŠ›ãªæ¨å®šæ‰‹æ³•ã§ã‚ã‚‹</li>
                <li>ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ³•ã¯è¨ˆç®—ãŒç°¡å˜ã§åˆæœŸæ¨å®šå€¤ã¨ã—ã¦æœ‰ç”¨ã§ã‚ã‚‹</li>
                <li>ãƒã‚¤ã‚¢ã‚¹-åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã¯æ¨å®šé‡ã®è¨­è¨ˆã§é‡è¦ãªè€ƒæ…®äº‹é …ã§ã‚ã‚‹</li>
                <li>Fisheræƒ…å ±é‡ã¨CramÃ©r-Raoä¸‹ç•Œã¯æ¨å®šã®ç†è«–çš„é™ç•Œã‚’ä¸ãˆã‚‹</li>
                <li>ææ–™ç§‘å­¦ã§ã¯æ­£è¦åˆ†å¸ƒã‚„Weibullåˆ†å¸ƒã‚’ç”¨ã„ãŸæ¨å®šãŒé‡è¦ã§ã‚ã‚‹</li>
            </ul>
        </div>

        <div class="nav-buttons">
            <a href="index.html" class="nav-button">â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a>
            <a href="chapter-2.html" class="nav-button">ç¬¬2ç« : åŒºé–“æ¨å®šã¨ä¿¡é ¼åŒºé–“ â†’</a>
        </div>
    </div>

    <footer>
        <p>&copy; 2025 AI Terakoya - Fundamentals of Mathematics & Physics Dojo</p>
    </footer>
</body>
</html>
