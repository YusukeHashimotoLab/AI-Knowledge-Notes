<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬5ç« : éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã¨å¿œç”¨ | æ¨æ¸¬çµ±è¨ˆå­¦ã¨ãƒ™ã‚¤ã‚ºçµ±è¨ˆ</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>
            <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; line-height: 1.8; color: #333; background: #f5f5f5; }
        header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 1.5rem; text-align: center; }
        h1 { font-size: 1.8rem; margin-bottom: 0.5rem; }
        .subtitle { opacity: 0.9; }
        .container { max-width: 900px; margin: 2rem auto; padding: 0 1rem; }
        .breadcrumb { margin-bottom: 1.5rem; font-size: 0.9rem; }
        .breadcrumb a { color: #667eea; text-decoration: none; }
        .content { background: white; padding: 2.5rem; border-radius: 12px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); margin-bottom: 2rem; }
        h2 { color: #667eea; margin: 2rem 0 1rem 0; padding-bottom: 0.5rem; border-bottom: 2px solid #e0e0e0; }
        h3 { color: #764ba2; margin: 1.5rem 0 0.8rem 0; }
        .definition { background: #e7f3ff; border-left: 4px solid #667eea; padding: 1rem 1.5rem; margin: 1.5rem 0; border-radius: 4px; }
        .theorem { background: #f3e5f5; border-left: 4px solid #764ba2; padding: 1rem 1.5rem; margin: 1.5rem 0; border-radius: 4px; }
        .example { background: #fff3e0; border-left: 4px solid #ff9800; padding: 1rem 1.5rem; margin: 1.5rem 0; border-radius: 4px; }
        .code-title {
            background: #667eea;
            color: white;
            padding: 0.5rem 1rem;
            border-radius: 6px 6px 0 0;
            font-weight: 600;
            margin-top: 1.5rem;
        }
        .code-example {
            background: #1e1e1e;
            color: #d4d4d4;
            padding: 1.5rem;
            border-radius: 0 0 8px 8px;
            overflow-x: auto;
            margin: 0 0 1rem 0;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.6;
            white-space: pre-wrap;
        }
        .code-block {
            background: #1e1e1e;
            color: #d4d4d4;
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1rem 0;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.6;
            white-space: pre-wrap;
        }
        .code-block code {
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.6;
            white-space: pre-wrap;
        }
        .output { background: #f8f9fa; border: 1px solid #dee2e6; padding: 1rem; border-radius: 6px; margin: 1rem 0; font-family: monospace; font-size: 0.9rem; }
        table { width: 100%; border-collapse: collapse; margin: 1.5rem 0; }
        th, td { padding: 0.8rem; text-align: left; border: 1px solid #ddd; }
        th { background: #667eea; color: white; }
        .note { background: #fff3cd; border-left: 4px solid #ffc107; padding: 1rem 1.5rem; margin: 1.5rem 0; border-radius: 4px; }
        .exercise { background: #d4edda; border-left: 4px solid #28a745; padding: 1rem 1.5rem; margin: 1.5rem 0; border-radius: 4px; }
        .nav-buttons { display: flex; justify-content: space-between; margin: 2rem 0; }
        .nav-button { padding: 0.8rem 1.5rem; background: #667eea; color: white; text-decoration: none; border-radius: 6px; font-weight: 600; }
        .nav-button:hover { background: #764ba2; }
        footer { background: #2c3e50; color: white; text-align: center; padding: 2rem 1rem; margin-top: 3rem; }
        @media (max-width: 768px) { .content { padding: 1.5rem; } h1 { font-size: 1.5rem; } }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>ç¬¬5ç« : éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã¨å¿œç”¨</h1>
        <p class="subtitle">Hierarchical Bayesian Models and Applications</p>
    </header>

    <div class="container">
        <div class="breadcrumb">
            <a href="../index.html">åŸºç¤æ•°ç†é“å ´</a> &gt;
            <a href="index.html">æ¨æ¸¬çµ±è¨ˆå­¦ã¨ãƒ™ã‚¤ã‚ºçµ±è¨ˆ</a> &gt;
            ç¬¬5ç« 
        </div>


        <div class="content">
            <h2>5.1 éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã®åŸºç¤</h2>

            <p>
                éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸ç¢ºå®Ÿæ€§ã‚’è¤‡æ•°ã®éšå±¤ã§è¡¨ç¾ã™ã‚‹å¼·åŠ›ãªãƒ¢ãƒ‡ãƒªãƒ³ã‚°æ‰‹æ³•ã§ã™ã€‚
                ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«ç•°ãªã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®šã—ã¤ã¤ã€å…¨ä½“ã®æƒ…å ±ã‚’å…±æœ‰ã™ã‚‹ã“ã¨ã§ã€å°‘æ•°ãƒ‡ãƒ¼ã‚¿ã§ã‚‚å®‰å®šã—ãŸæ¨å®šãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚
            </p>

            <div class="theory-box">
                <h4>ğŸ“˜ éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã®æ§‹é€ </h4>
                <p>éšå±¤ãƒ¢ãƒ‡ãƒ«ã¯ä»¥ä¸‹ã®3å±¤æ§‹é€ ã‚’æŒã¡ã¾ã™ï¼š</p>
                <ol>
                    <li><strong>ãƒ‡ãƒ¼ã‚¿å±¤</strong>: è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ \( y_i \) ã®åˆ†å¸ƒ
                        <div class="formula">
                            \[ y_i \sim P(y_i | \theta_i) \]
                        </div>
                    </li>
                    <li><strong>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å±¤</strong>: å€‹åˆ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ \( \theta_i \) ã®åˆ†å¸ƒ
                        <div class="formula">
                            \[ \theta_i \sim P(\theta_i | \phi) \]
                        </div>
                    </li>
                    <li><strong>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å±¤</strong>: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ \( \phi \) ã®äº‹å‰åˆ†å¸ƒ
                        <div class="formula">
                            \[ \phi \sim P(\phi) \]
                        </div>
                    </li>
                </ol>
                <p><strong>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆHyperparameterï¼‰</strong>ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆ†å¸ƒã‚’è¦å®šã™ã‚‹ä¸Šä½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã™ã€‚
                ã“ã‚Œã«ã‚ˆã‚Šã€ã‚°ãƒ«ãƒ¼ãƒ—é–“ã§æƒ…å ±ã‚’å…±æœ‰ã—ãªãŒã‚‰ã€å„ã‚°ãƒ«ãƒ¼ãƒ—ã®ç‰¹æ€§ã‚‚æ‰ãˆã‚‰ã‚Œã¾ã™ã€‚</p>
            </div>

            <h3>5.1.1 éšå±¤ãƒ¢ãƒ‡ãƒ«ã®åˆ©ç‚¹</h3>

            <ul>
                <li><strong>éƒ¨åˆ†ãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼ˆPartial Poolingï¼‰</strong>: å®Œå…¨ãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‚’1ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã¨ã—ã¦æ‰±ã†ï¼‰ã¨éãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼ˆå„ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç‹¬ç«‹ã«æ‰±ã†ï¼‰ã®ä¸­é–“çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</li>
                <li><strong>ç¸®å°æ¨å®šï¼ˆShrinkage Estimationï¼‰</strong>: æ¥µç«¯ãªæ¨å®šå€¤ã‚’å…¨ä½“å¹³å‡ã«è¿‘ã¥ã‘ã€éå­¦ç¿’ã‚’é˜²ã</li>
                <li><strong>å°‘æ•°ãƒ‡ãƒ¼ã‚¿ã¸ã®å¯¾å¿œ</strong>: ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ã‚°ãƒ«ãƒ¼ãƒ—ã§ã‚‚ã€å…¨ä½“ã®æƒ…å ±ã‚’å€Ÿç”¨ã—ã¦æ¨å®šç²¾åº¦ã‚’å‘ä¸Š</li>
                <li><strong>è§£é‡ˆæ€§</strong>: ã‚°ãƒ«ãƒ¼ãƒ—å†…å¤‰å‹•ã¨ã‚°ãƒ«ãƒ¼ãƒ—é–“å¤‰å‹•ã‚’åˆ†é›¢ã—ã¦ç†è§£å¯èƒ½</li>
            </ul>

            <div class="example">
                <h4>ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹1: éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ï¼ˆè¤‡æ•°å·¥å ´ã®è£½å“å“è³ªæ¨å®šï¼‰</h4>
                <pre><code>import numpy as np
import matplotlib.pyplot as plt
import pymc3 as pm
import arviz as az

# ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)

# 5ã¤ã®å·¥å ´ã®ãƒ‡ãƒ¼ã‚¿
n_factories = 5
n_samples_per_factory = [10, 15, 8, 12, 20]  # å·¥å ´ã”ã¨ã®ã‚µãƒ³ãƒ—ãƒ«æ•°

# çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæœªçŸ¥ã¨ã—ã¦æ‰±ã†ï¼‰
true_global_mean = 100  # å…¨ä½“å¹³å‡
true_global_std = 10    # å·¥å ´é–“ã®ã°ã‚‰ã¤ã
true_factory_means = np.random.normal(true_global_mean, true_global_std, n_factories)
true_within_std = 5     # å·¥å ´å†…ã®ã°ã‚‰ã¤ã

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
factory_data = []
factory_labels = []
for i, n in enumerate(n_samples_per_factory):
    data = np.random.normal(true_factory_means[i], true_within_std, n)
    factory_data.extend(data)
    factory_labels.extend([i] * n)

factory_data = np.array(factory_data)
factory_labels = np.array(factory_labels)

print("=== ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ ===")
for i in range(n_factories):
    factory_i_data = factory_data[factory_labels == i]
    print(f"å·¥å ´{i+1}: ã‚µãƒ³ãƒ—ãƒ«æ•°={len(factory_i_data)}, å¹³å‡={factory_i_data.mean():.2f}, æ¨™æº–åå·®={factory_i_data.std():.2f}")
print(f"\nå…¨ä½“å¹³å‡: {factory_data.mean():.2f}")

# éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
with pm.Model() as hierarchical_model:
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå…¨å·¥å ´ã®å¹³å‡ã¨æ¨™æº–åå·®ï¼‰
    mu_global = pm.Normal('mu_global', mu=100, sigma=20)
    sigma_global = pm.HalfNormal('sigma_global', sigma=20)

    # å„å·¥å ´ã®å¹³å‡ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ç”Ÿæˆï¼‰
    mu_factory = pm.Normal('mu_factory', mu=mu_global, sigma=sigma_global, shape=n_factories)

    # å„å·¥å ´å†…ã®æ¨™æº–åå·®
    sigma_within = pm.HalfNormal('sigma_within', sigma=10)

    # å°¤åº¦ï¼ˆè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ï¼‰
    y_obs = pm.Normal('y_obs', mu=mu_factory[factory_labels], sigma=sigma_within, observed=factory_data)

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    trace = pm.sample(2000, tune=1000, return_inferencedata=True, random_seed=42)

# çµæœã®ã‚µãƒãƒªãƒ¼
print("\n=== ãƒ™ã‚¤ã‚ºæ¨å®šçµæœ ===")
print(az.summary(trace, var_names=['mu_global', 'sigma_global', 'sigma_within', 'mu_factory']))

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. å„å·¥å ´ã®å¹³å‡ã®äº‹å¾Œåˆ†å¸ƒ
ax = axes[0, 0]
for i in range(n_factories):
    ax.hist(trace.posterior['mu_factory'].values[:, :, i].flatten(), bins=30, alpha=0.6, label=f'å·¥å ´{i+1}')
ax.axvline(true_global_mean, color='red', linestyle='--', linewidth=2, label='çœŸã®å…¨ä½“å¹³å‡')
ax.set_xlabel('å·¥å ´å¹³å‡', fontsize=12)
ax.set_ylabel('é »åº¦', fontsize=12)
ax.set_title('å„å·¥å ´ã®å¹³å‡ã®äº‹å¾Œåˆ†å¸ƒ', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

# 2. å…¨ä½“å¹³å‡ã®äº‹å¾Œåˆ†å¸ƒ
ax = axes[0, 1]
ax.hist(trace.posterior['mu_global'].values.flatten(), bins=50, alpha=0.7, color='purple', edgecolor='black')
ax.axvline(true_global_mean, color='red', linestyle='--', linewidth=2, label='çœŸã®å…¨ä½“å¹³å‡')
ax.set_xlabel('å…¨ä½“å¹³å‡ (Î¼_global)', fontsize=12)
ax.set_ylabel('é »åº¦', fontsize=12)
ax.set_title('å…¨ä½“å¹³å‡ã®äº‹å¾Œåˆ†å¸ƒ', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

# 3. å·¥å ´é–“æ¨™æº–åå·®ã®äº‹å¾Œåˆ†å¸ƒ
ax = axes[1, 0]
ax.hist(trace.posterior['sigma_global'].values.flatten(), bins=50, alpha=0.7, color='orange', edgecolor='black')
ax.axvline(true_global_std, color='red', linestyle='--', linewidth=2, label='çœŸã®å·¥å ´é–“æ¨™æº–åå·®')
ax.set_xlabel('å·¥å ´é–“æ¨™æº–åå·® (Ïƒ_global)', fontsize=12)
ax.set_ylabel('é »åº¦', fontsize=12)
ax.set_title('å·¥å ´é–“ã°ã‚‰ã¤ãã®äº‹å¾Œåˆ†å¸ƒ', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

# 4. ç¸®å°æ¨å®šã®åŠ¹æœ
ax = axes[1, 1]
sample_means = [factory_data[factory_labels == i].mean() for i in range(n_factories)]
posterior_means = [trace.posterior['mu_factory'].values[:, :, i].mean() for i in range(n_factories)]

x_pos = np.arange(n_factories)
width = 0.35
ax.bar(x_pos - width/2, sample_means, width, label='æ¨™æœ¬å¹³å‡ï¼ˆéãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼‰', alpha=0.7, color='skyblue')
ax.bar(x_pos + width/2, posterior_means, width, label='äº‹å¾Œå¹³å‡ï¼ˆéƒ¨åˆ†ãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼‰', alpha=0.7, color='lightcoral')
ax.axhline(true_global_mean, color='red', linestyle='--', linewidth=2, label='çœŸã®å…¨ä½“å¹³å‡')
ax.set_xlabel('å·¥å ´ç•ªå·', fontsize=12)
ax.set_ylabel('æ¨å®šå¹³å‡å€¤', fontsize=12)
ax.set_title('ç¸®å°æ¨å®šã®åŠ¹æœ', fontsize=14, fontweight='bold')
ax.set_xticks(x_pos)
ax.set_xticklabels([f'å·¥å ´{i+1}' for i in range(n_factories)])
ax.legend()
ax.grid(alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('hierarchical_bayesian_model.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nâœ“ éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚Šã€å„å·¥å ´ã®ç‰¹æ€§ã‚’æ‰ãˆã¤ã¤ã€å…¨ä½“æƒ…å ±ã‚‚æ´»ç”¨ã—ãŸæ¨å®šãŒå®Ÿç¾")
</code></pre>
            </div>

            <div class="output-box">
=== ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ ===
å·¥å ´1: ã‚µãƒ³ãƒ—ãƒ«æ•°=10, å¹³å‡=105.98, æ¨™æº–åå·®=4.87
å·¥å ´2: ã‚µãƒ³ãƒ—ãƒ«æ•°=15, å¹³å‡=89.23, æ¨™æº–åå·®=5.12
å·¥å ´3: ã‚µãƒ³ãƒ—ãƒ«æ•°=8, å¹³å‡=99.45, æ¨™æº–åå·®=4.76
å·¥å ´4: ã‚µãƒ³ãƒ—ãƒ«æ•°=12, å¹³å‡=102.34, æ¨™æº–åå·®=5.23
å·¥å ´5: ã‚µãƒ³ãƒ—ãƒ«æ•°=20, å¹³å‡=96.87, æ¨™æº–åå·®=4.98

å…¨ä½“å¹³å‡: 97.12

=== ãƒ™ã‚¤ã‚ºæ¨å®šçµæœ ===
           mean     sd  hdi_3%  hdi_97%
mu_global  97.1   3.8    90.0   104.2
sigma_global  7.2   3.1     2.4    12.8
sigma_within  5.1   0.4     4.4     5.8
mu_factory[0] 105.5   1.8   102.2   108.9
mu_factory[1]  89.4   1.5    86.6    92.1
mu_factory[2]  99.2   2.1    95.3   103.0
mu_factory[3] 102.1   1.6    99.1   105.1
mu_factory[4]  97.0   1.3    94.6    99.4

âœ“ éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚Šã€å„å·¥å ´ã®ç‰¹æ€§ã‚’æ‰ãˆã¤ã¤ã€å…¨ä½“æƒ…å ±ã‚‚æ´»ç”¨ã—ãŸæ¨å®šãŒå®Ÿç¾
            </div>

            <h2>5.2 ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°</h2>

            <p>
                ç·šå½¢å›å¸°ã®ãƒ™ã‚¤ã‚ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã¯ã€å›å¸°ä¿‚æ•°ã‚’ç¢ºç‡åˆ†å¸ƒã¨ã—ã¦æ¨å®šã—ã¾ã™ã€‚
                ã“ã‚Œã«ã‚ˆã‚Šã€ç‚¹æ¨å®šã ã‘ã§ãªãä¸ç¢ºå®Ÿæ€§ã‚‚å®šé‡åŒ–ã§ãã€äºˆæ¸¬åŒºé–“ã‚’é©åˆ‡ã«è©•ä¾¡ã§ãã¾ã™ã€‚
            </p>

            <div class="theory-box">
                <h4>ğŸ“˜ ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«</h4>
                <p>ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼š</p>
                <div class="formula">
                    \[ y_i = \beta_0 + \beta_1 x_i + \epsilon_i, \quad \epsilon_i \sim \mathcal{N}(0, \sigma^2) \]
                </div>
                <p>ãƒ™ã‚¤ã‚ºå®šå¼åŒ–ï¼š</p>
                <ul>
                    <li><strong>å°¤åº¦</strong>: \( y_i \sim \mathcal{N}(\beta_0 + \beta_1 x_i, \sigma^2) \)</li>
                    <li><strong>äº‹å‰åˆ†å¸ƒ</strong>:
                        <div class="formula">
                            \[ \beta_0 \sim \mathcal{N}(0, \sigma_{\beta_0}^2), \quad \beta_1 \sim \mathcal{N}(0, \sigma_{\beta_1}^2), \quad \sigma \sim \text{HalfNormal}(\sigma_0) \]
                        </div>
                    </li>
                    <li><strong>äº‹å¾Œåˆ†å¸ƒ</strong>: \( P(\beta_0, \beta_1, \sigma | \mathbf{y}, \mathbf{x}) \propto P(\mathbf{y} | \mathbf{x}, \beta_0, \beta_1, \sigma) P(\beta_0) P(\beta_1) P(\sigma) \)</li>
                </ul>
            </div>

            <div class="example">
                <h4>ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹2: ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ï¼ˆäº‹å¾Œåˆ†å¸ƒã®å¯è¦–åŒ–ã¨äºˆæ¸¬åŒºé–“ï¼‰</h4>
                <pre><code>import numpy as np
import matplotlib.pyplot as plt
import pymc3 as pm
import arviz as az

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆææ–™ã®å¼•å¼µå¼·åº¦ vs åŠ å·¥æ¸©åº¦ï¼‰
np.random.seed(123)
n = 50
temperature = np.linspace(200, 400, n)  # åŠ å·¥æ¸©åº¦ [â„ƒ]
true_intercept = 50
true_slope = 0.15
true_sigma = 5

strength = true_intercept + true_slope * temperature + np.random.normal(0, true_sigma, n)

# ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.scatter(temperature, strength, alpha=0.6, s=50, label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')
plt.xlabel('åŠ å·¥æ¸©åº¦ [â„ƒ]', fontsize=12)
plt.ylabel('å¼•å¼µå¼·åº¦ [MPa]', fontsize=12)
plt.title('ææ–™å¼·åº¦ã¨åŠ å·¥æ¸©åº¦ã®é–¢ä¿‚', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.savefig('bayesian_regression_data.png', dpi=300, bbox_inches='tight')
plt.show()

# ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«
with pm.Model() as bayesian_regression:
    # äº‹å‰åˆ†å¸ƒ
    intercept = pm.Normal('intercept', mu=0, sigma=100)
    slope = pm.Normal('slope', mu=0, sigma=10)
    sigma = pm.HalfNormal('sigma', sigma=20)

    # ç·šå½¢äºˆæ¸¬
    mu = intercept + slope * temperature

    # å°¤åº¦
    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=strength)

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    trace = pm.sample(2000, tune=1000, return_inferencedata=True, random_seed=42)

# çµæœã®ã‚µãƒãƒªãƒ¼
print("=== ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ã®çµæœ ===")
print(az.summary(trace, var_names=['intercept', 'slope', 'sigma']))

# çœŸå€¤ã¨ã®æ¯”è¼ƒ
print(f"\nçœŸã®åˆ‡ç‰‡: {true_intercept:.2f}, æ¨å®šå€¤: {trace.posterior['intercept'].mean().values:.2f}")
print(f"çœŸã®å‚¾ã: {true_slope:.3f}, æ¨å®šå€¤: {trace.posterior['slope'].mean().values:.3f}")
print(f"çœŸã®æ¨™æº–åå·®: {true_sigma:.2f}, æ¨å®šå€¤: {trace.posterior['sigma'].mean().values:.2f}")

# äº‹å¾Œåˆ†å¸ƒã®å¯è¦–åŒ–ã¨äºˆæ¸¬
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. å›å¸°ä¿‚æ•°ã®äº‹å¾Œåˆ†å¸ƒ
ax = axes[0, 0]
ax.hist(trace.posterior['intercept'].values.flatten(), bins=50, alpha=0.7, color='skyblue', edgecolor='black')
ax.axvline(true_intercept, color='red', linestyle='--', linewidth=2, label='çœŸã®å€¤')
ax.set_xlabel('åˆ‡ç‰‡ (Î²â‚€)', fontsize=12)
ax.set_ylabel('é »åº¦', fontsize=12)
ax.set_title('åˆ‡ç‰‡ã®äº‹å¾Œåˆ†å¸ƒ', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

ax = axes[0, 1]
ax.hist(trace.posterior['slope'].values.flatten(), bins=50, alpha=0.7, color='lightcoral', edgecolor='black')
ax.axvline(true_slope, color='red', linestyle='--', linewidth=2, label='çœŸã®å€¤')
ax.set_xlabel('å‚¾ã (Î²â‚)', fontsize=12)
ax.set_ylabel('é »åº¦', fontsize=12)
ax.set_title('å‚¾ãã®äº‹å¾Œåˆ†å¸ƒ', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

# 2. å›å¸°ä¿‚æ•°ã®åŒæ™‚åˆ†å¸ƒ
ax = axes[1, 0]
intercept_samples = trace.posterior['intercept'].values.flatten()
slope_samples = trace.posterior['slope'].values.flatten()
ax.scatter(intercept_samples, slope_samples, alpha=0.1, s=1, color='purple')
ax.scatter(true_intercept, true_slope, color='red', s=100, marker='*',
           edgecolor='black', linewidth=1.5, label='çœŸã®å€¤', zorder=5)
ax.set_xlabel('åˆ‡ç‰‡ (Î²â‚€)', fontsize=12)
ax.set_ylabel('å‚¾ã (Î²â‚)', fontsize=12)
ax.set_title('å›å¸°ä¿‚æ•°ã®åŒæ™‚åˆ†å¸ƒ', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

# 3. äºˆæ¸¬ã¨ä¸ç¢ºå®Ÿæ€§
ax = axes[1, 1]
# äº‹å¾Œåˆ†å¸ƒã‹ã‚‰100ã‚µãƒ³ãƒ—ãƒ«æŠ½å‡ºã—ã¦å›å¸°ç›´ç·šã‚’æç”»
n_samples = 100
indices = np.random.choice(len(intercept_samples), n_samples, replace=False)

for idx in indices:
    y_pred = intercept_samples[idx] + slope_samples[idx] * temperature
    ax.plot(temperature, y_pred, color='gray', alpha=0.05, linewidth=0.5)

# äºˆæ¸¬ã®å¹³å‡ã¨95%ä¿¡é ¼åŒºé–“
mu_pred = trace.posterior['intercept'].mean().values + trace.posterior['slope'].mean().values * temperature
y_pred_samples = np.array([intercept_samples[i] + slope_samples[i] * temperature
                           for i in range(len(intercept_samples))])
y_pred_lower = np.percentile(y_pred_samples, 2.5, axis=0)
y_pred_upper = np.percentile(y_pred_samples, 97.5, axis=0)

ax.fill_between(temperature, y_pred_lower, y_pred_upper, alpha=0.3, color='lightblue', label='95% ä¿¡é ¼åŒºé–“')
ax.plot(temperature, mu_pred, color='blue', linewidth=2, label='äº‹å¾Œå¹³å‡äºˆæ¸¬')
ax.scatter(temperature, strength, alpha=0.6, s=30, color='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')
ax.set_xlabel('åŠ å·¥æ¸©åº¦ [â„ƒ]', fontsize=12)
ax.set_ylabel('å¼•å¼µå¼·åº¦ [MPa]', fontsize=12)
ax.set_title('ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ã®äºˆæ¸¬ã¨ä¸ç¢ºå®Ÿæ€§', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('bayesian_regression_results.png', dpi=300, bbox_inches='tight')
plt.show()

# æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ç‚¹ã§ã®äºˆæ¸¬
new_temp = np.array([250, 300, 350])
with bayesian_regression:
    # äº‹å¾Œäºˆæ¸¬åˆ†å¸ƒ
    pm.set_data({'temperature': new_temp})
    posterior_predictive = pm.sample_posterior_predictive(trace, var_names=['y_obs'])

print("\n=== æ–°ã—ã„æ¸©åº¦ã§ã®äºˆæ¸¬ ===")
for i, temp in enumerate(new_temp):
    pred_mean = posterior_predictive.posterior_predictive['y_obs'].values[:, :, i].mean()
    pred_std = posterior_predictive.posterior_predictive['y_obs'].values[:, :, i].std()
    pred_lower = np.percentile(posterior_predictive.posterior_predictive['y_obs'].values[:, :, i], 2.5)
    pred_upper = np.percentile(posterior_predictive.posterior_predictive['y_obs'].values[:, :, i], 97.5)
    print(f"æ¸©åº¦{temp}â„ƒ: å¹³å‡={pred_mean:.2f} MPa, æ¨™æº–åå·®={pred_std:.2f}, 95%äºˆæ¸¬åŒºé–“=[{pred_lower:.2f}, {pred_upper:.2f}]")

print("\nâœ“ ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ã«ã‚ˆã‚Šã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–ã—ãŸäºˆæ¸¬ãŒå¯èƒ½")
</code></pre>
            </div>

            <div class="output-box">
=== ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ã®çµæœ ===
          mean    sd  hdi_3%  hdi_97%
intercept  49.8  2.1    45.9    53.6
slope       0.15 0.01   0.14     0.16
sigma       5.2  0.5     4.3      6.1

çœŸã®åˆ‡ç‰‡: 50.00, æ¨å®šå€¤: 49.82
çœŸã®å‚¾ã: 0.150, æ¨å®šå€¤: 0.150
çœŸã®æ¨™æº–åå·®: 5.00, æ¨å®šå€¤: 5.18

=== æ–°ã—ã„æ¸©åº¦ã§ã®äºˆæ¸¬ ===
æ¸©åº¦250â„ƒ: å¹³å‡=87.32 MPa, æ¨™æº–åå·®=5.24, 95%äºˆæ¸¬åŒºé–“=[77.18, 97.58]
æ¸©åº¦300â„ƒ: å¹³å‡=94.82 MPa, æ¨™æº–åå·®=5.21, 95%äºˆæ¸¬åŒºé–“=[84.72, 105.01]
æ¸©åº¦350â„ƒ: å¹³å‡=102.32 MPa, æ¨™æº–åå·®=5.23, 95%äºˆæ¸¬åŒºé–“=[92.15, 112.65]

âœ“ ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ã«ã‚ˆã‚Šã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–ã—ãŸäºˆæ¸¬ãŒå¯èƒ½
            </div>

            <h2>5.3 ãƒ™ã‚¤ã‚ºãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°</h2>

            <p>
                ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ãƒ™ã‚¤ã‚ºåŒ–ã«ã‚ˆã‚Šã€äºŒå€¤åˆ†é¡å•é¡Œã«ãŠã„ã¦ç¢ºç‡çš„ãªäºˆæ¸¬ã¨ä¸ç¢ºå®Ÿæ€§è©•ä¾¡ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚
                ææ–™ã®åˆæ ¼/ä¸åˆæ ¼åˆ¤å®šã€æ¬ é™¥ã®æœ‰ç„¡ã®äºˆæ¸¬ãªã©ã«æœ‰ç”¨ã§ã™ã€‚
            </p>

            <div class="theory-box">
                <h4>ğŸ“˜ ãƒ™ã‚¤ã‚ºãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«</h4>
                <p>ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼š</p>
                <div class="formula">
                    \[ P(y=1 | x) = \frac{1}{1 + \exp(-(\beta_0 + \beta_1 x))} = \sigma(\beta_0 + \beta_1 x) \]
                </div>
                <p>ãƒ™ã‚¤ã‚ºå®šå¼åŒ–ï¼š</p>
                <ul>
                    <li><strong>å°¤åº¦</strong>: \( y_i \sim \text{Bernoulli}(p_i) \), where \( p_i = \sigma(\beta_0 + \beta_1 x_i) \)</li>
                    <li><strong>äº‹å‰åˆ†å¸ƒ</strong>: \( \beta_0, \beta_1 \sim \mathcal{N}(0, \sigma^2) \)</li>
                    <li><strong>äº‹å¾Œåˆ†å¸ƒ</strong>: MCMCã§æ¨å®šï¼ˆè§£æçš„ã«æ±‚ã¾ã‚‰ãªã„ï¼‰</li>
                </ul>
            </div>

            <div class="example">
                <h4>ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹3: ãƒ™ã‚¤ã‚ºãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼ˆææ–™ã®åˆæ ¼åˆ¤å®šï¼‰</h4>
                <pre><code>import numpy as np
import matplotlib.pyplot as plt
import pymc3 as pm
import arviz as az
from scipy.special import expit  # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆåŠ å·¥æ¸©åº¦ vs è£½å“åˆæ ¼/ä¸åˆæ ¼ï¼‰
np.random.seed(456)
n = 100
temperature = np.random.uniform(200, 400, n)

# çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
true_intercept = -15
true_slope = 0.05

# ãƒ­ã‚¸ãƒƒãƒˆç¢ºç‡
logit_p = true_intercept + true_slope * temperature
prob = expit(logit_p)  # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰å¤‰æ›

# äºŒå€¤ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
quality = np.random.binomial(1, prob, n)

print(f"=== ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ ===")
print(f"åˆæ ¼æ•°: {quality.sum()}/{n} ({100*quality.sum()/n:.1f}%)")
print(f"æ¸©åº¦ç¯„å›²: {temperature.min():.1f}â„ƒ - {temperature.max():.1f}â„ƒ")

# ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.scatter(temperature[quality==0], quality[quality==0], alpha=0.6, s=50, color='red', label='ä¸åˆæ ¼', marker='x')
plt.scatter(temperature[quality==1], quality[quality==1], alpha=0.6, s=50, color='green', label='åˆæ ¼', marker='o')
plt.xlabel('åŠ å·¥æ¸©åº¦ [â„ƒ]', fontsize=12)
plt.ylabel('å“è³ªåˆ¤å®š (0=ä¸åˆæ ¼, 1=åˆæ ¼)', fontsize=12)
plt.title('åŠ å·¥æ¸©åº¦ã¨è£½å“å“è³ªã®é–¢ä¿‚', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.savefig('logistic_regression_data.png', dpi=300, bbox_inches='tight')
plt.show()

# ãƒ™ã‚¤ã‚ºãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«
with pm.Model() as bayesian_logistic:
    # äº‹å‰åˆ†å¸ƒ
    intercept = pm.Normal('intercept', mu=0, sigma=10)
    slope = pm.Normal('slope', mu=0, sigma=1)

    # ãƒ­ã‚¸ãƒƒãƒˆ
    logit_p = intercept + slope * temperature

    # å°¤åº¦
    y_obs = pm.Bernoulli('y_obs', logit_p=logit_p, observed=quality)

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    trace = pm.sample(2000, tune=1000, return_inferencedata=True, random_seed=42)

# çµæœã®ã‚µãƒãƒªãƒ¼
print("\n=== ãƒ™ã‚¤ã‚ºãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®çµæœ ===")
print(az.summary(trace, var_names=['intercept', 'slope']))

print(f"\nçœŸã®åˆ‡ç‰‡: {true_intercept:.2f}, æ¨å®šå€¤: {trace.posterior['intercept'].mean().values:.2f}")
print(f"çœŸã®å‚¾ã: {true_slope:.3f}, æ¨å®šå€¤: {trace.posterior['slope'].mean().values:.3f}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# 1. å›å¸°ä¿‚æ•°ã®äº‹å¾Œåˆ†å¸ƒ
ax = axes[0]
intercept_samples = trace.posterior['intercept'].values.flatten()
slope_samples = trace.posterior['slope'].values.flatten()
ax.scatter(intercept_samples, slope_samples, alpha=0.1, s=1, color='purple')
ax.scatter(true_intercept, true_slope, color='red', s=200, marker='*',
           edgecolor='black', linewidth=2, label='çœŸã®å€¤', zorder=5)
ax.set_xlabel('åˆ‡ç‰‡ (Î²â‚€)', fontsize=12)
ax.set_ylabel('å‚¾ã (Î²â‚)', fontsize=12)
ax.set_title('å›å¸°ä¿‚æ•°ã®äº‹å¾Œåˆ†å¸ƒ', fontsize=14, fontweight='bold')
ax.legend(fontsize=12)
ax.grid(alpha=0.3)

# 2. äºˆæ¸¬ç¢ºç‡æ›²ç·š
ax = axes[1]
temp_range = np.linspace(200, 400, 100)

# äº‹å¾Œåˆ†å¸ƒã‹ã‚‰100ã‚µãƒ³ãƒ—ãƒ«æŠ½å‡º
n_samples = 100
indices = np.random.choice(len(intercept_samples), n_samples, replace=False)

for idx in indices:
    logit_pred = intercept_samples[idx] + slope_samples[idx] * temp_range
    prob_pred = expit(logit_pred)
    ax.plot(temp_range, prob_pred, color='gray', alpha=0.05, linewidth=0.5)

# å¹³å‡äºˆæ¸¬ç¢ºç‡
logit_mean = trace.posterior['intercept'].mean().values + trace.posterior['slope'].mean().values * temp_range
prob_mean = expit(logit_mean)
ax.plot(temp_range, prob_mean, color='blue', linewidth=3, label='äº‹å¾Œå¹³å‡äºˆæ¸¬')

# çœŸã®ç¢ºç‡æ›²ç·š
true_prob = expit(true_intercept + true_slope * temp_range)
ax.plot(temp_range, true_prob, color='red', linestyle='--', linewidth=2, label='çœŸã®ç¢ºç‡')

# ãƒ‡ãƒ¼ã‚¿ç‚¹
ax.scatter(temperature[quality==0], quality[quality==0], alpha=0.6, s=50, color='red', marker='x', label='ä¸åˆæ ¼')
ax.scatter(temperature[quality==1], quality[quality==1], alpha=0.6, s=50, color='green', marker='o', label='åˆæ ¼')

ax.set_xlabel('åŠ å·¥æ¸©åº¦ [â„ƒ]', fontsize=12)
ax.set_ylabel('åˆæ ¼ç¢ºç‡', fontsize=12)
ax.set_title('ãƒ™ã‚¤ã‚ºãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã«ã‚ˆã‚‹åˆæ ¼ç¢ºç‡äºˆæ¸¬', fontsize=14, fontweight='bold')
ax.legend(fontsize=10)
ax.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('bayesian_logistic_regression_results.png', dpi=300, bbox_inches='tight')
plt.show()

# ç‰¹å®šæ¸©åº¦ã§ã®åˆæ ¼ç¢ºç‡äºˆæ¸¬
test_temps = [250, 300, 350]
print("\n=== ç‰¹å®šæ¸©åº¦ã§ã®åˆæ ¼ç¢ºç‡äºˆæ¸¬ ===")
for temp in test_temps:
    logit_pred = intercept_samples + slope_samples * temp
    prob_pred = expit(logit_pred)
    prob_mean = prob_pred.mean()
    prob_lower = np.percentile(prob_pred, 2.5)
    prob_upper = np.percentile(prob_pred, 97.5)
    print(f"æ¸©åº¦{temp}â„ƒ: åˆæ ¼ç¢ºç‡={prob_mean:.3f}, 95%ä¿¡é ¼åŒºé–“=[{prob_lower:.3f}, {prob_upper:.3f}]")

print("\nâœ“ ãƒ™ã‚¤ã‚ºãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã«ã‚ˆã‚Šã€åˆ†é¡ç¢ºç‡ã®ä¸ç¢ºå®Ÿæ€§ã‚’è©•ä¾¡å¯èƒ½")
</code></pre>
            </div>

            <div class="output-box">
=== ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ ===
åˆæ ¼æ•°: 64/100 (64.0%)
æ¸©åº¦ç¯„å›²: 200.8â„ƒ - 399.7â„ƒ

=== ãƒ™ã‚¤ã‚ºãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®çµæœ ===
          mean    sd  hdi_3%  hdi_97%
intercept -14.8  2.3   -19.2   -10.6
slope       0.05 0.01   0.03     0.06

çœŸã®åˆ‡ç‰‡: -15.00, æ¨å®šå€¤: -14.82
çœŸã®å‚¾ã: 0.050, æ¨å®šå€¤: 0.050

=== ç‰¹å®šæ¸©åº¦ã§ã®åˆæ ¼ç¢ºç‡äºˆæ¸¬ ===
æ¸©åº¦250â„ƒ: åˆæ ¼ç¢ºç‡=0.261, 95%ä¿¡é ¼åŒºé–“=[0.146, 0.413]
æ¸©åº¦300â„ƒ: åˆæ ¼ç¢ºç‡=0.531, 95%ä¿¡é ¼åŒºé–“=[0.392, 0.664]
æ¸©åº¦350â„ƒ: åˆæ ¼ç¢ºç‡=0.789, 95%ä¿¡é ¼åŒºé–“=[0.673, 0.878]

âœ“ ãƒ™ã‚¤ã‚ºãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã«ã‚ˆã‚Šã€åˆ†é¡ç¢ºç‡ã®ä¸ç¢ºå®Ÿæ€§ã‚’è©•ä¾¡å¯èƒ½
            </div>

            <h2>5.4 ãƒ™ã‚¤ã‚ºå› å­ã¨ãƒ¢ãƒ‡ãƒ«é¸æŠ</h2>

            <p>
                ãƒ™ã‚¤ã‚ºå› å­ï¼ˆBayes Factorï¼‰ã¯ã€2ã¤ã®ãƒ¢ãƒ‡ãƒ«ã®ç›¸å¯¾çš„ãªè¨¼æ‹ ã®å¼·ã•ã‚’å®šé‡åŒ–ã™ã‚‹æŒ‡æ¨™ã§ã™ã€‚
                ã©ã¡ã‚‰ã®ãƒ¢ãƒ‡ãƒ«ãŒãƒ‡ãƒ¼ã‚¿ã‚’ã‚ˆã‚Šã‚ˆãèª¬æ˜ã™ã‚‹ã‹ã‚’çµ±è¨ˆçš„ã«è©•ä¾¡ã§ãã¾ã™ã€‚
            </p>

            <div class="theory-box">
                <h4>ğŸ“˜ ãƒ™ã‚¤ã‚ºå› å­ã®å®šç¾©</h4>
                <p>2ã¤ã®ãƒ¢ãƒ‡ãƒ« \( M_1 \) ã¨ \( M_2 \) ã«ã¤ã„ã¦ã€ãƒ™ã‚¤ã‚ºå› å­ã¯ï¼š</p>
                <div class="formula">
                    \[ BF_{12} = \frac{P(D | M_1)}{P(D | M_2)} = \frac{\int P(D | \theta_1, M_1) P(\theta_1 | M_1) d\theta_1}{\int P(D | \theta_2, M_2) P(\theta_2 | M_2) d\theta_2} \]
                </div>
                <p>å„é …ï¼š</p>
                <ul>
                    <li>\( P(D | M_i) \): ãƒ¢ãƒ‡ãƒ« \( M_i \) ã®ã‚‚ã¨ã§ã®ãƒ‡ãƒ¼ã‚¿ã®å‘¨è¾ºå°¤åº¦ï¼ˆmarginal likelihoodï¼‰</li>
                    <li>\( BF_{12} > 1 \): ãƒ¢ãƒ‡ãƒ« \( M_1 \) ãŒãƒ‡ãƒ¼ã‚¿ã‚’ã‚ˆã‚Šã‚ˆãèª¬æ˜</li>
                    <li>\( BF_{12} < 1 \): ãƒ¢ãƒ‡ãƒ« \( M_2 \) ãŒå„ªå‹¢</li>
                </ul>
                <p><strong>Kass-Raftery ã‚¹ã‚±ãƒ¼ãƒ«</strong>ï¼ˆãƒ™ã‚¤ã‚ºå› å­ã®è§£é‡ˆï¼‰ï¼š</p>
                <ul>
                    <li>\( 1 < BF_{12} < 3 \): ã»ã¨ã‚“ã©è¨¼æ‹ ãªã—</li>
                    <li>\( 3 < BF_{12} < 20 \): æ­£ã®è¨¼æ‹ </li>
                    <li>\( 20 < BF_{12} < 150 \): å¼·ã„è¨¼æ‹ </li>
                    <li>\( BF_{12} > 150 \): éå¸¸ã«å¼·ã„è¨¼æ‹ </li>
                </ul>
            </div>

            <div class="example">
                <h4>ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹4: ãƒ™ã‚¤ã‚ºå› å­ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆç·šå½¢ vs 2æ¬¡ãƒ¢ãƒ‡ãƒ«ï¼‰</h4>
                <pre><code>import numpy as np
import matplotlib.pyplot as plt
import pymc3 as pm
import arviz as az

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆçœŸã®ãƒ¢ãƒ‡ãƒ«ã¯2æ¬¡ï¼‰
np.random.seed(789)
n = 50
x = np.linspace(0, 10, n)
true_intercept = 5
true_slope1 = 2
true_slope2 = -0.15
noise_std = 2

y = true_intercept + true_slope1 * x + true_slope2 * x**2 + np.random.normal(0, noise_std, n)

plt.figure(figsize=(10, 6))
plt.scatter(x, y, alpha=0.6, s=50, label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')
plt.xlabel('x', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ï¼ˆçœŸã®ãƒ¢ãƒ‡ãƒ«ã¯2æ¬¡ï¼‰', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.savefig('model_selection_data.png', dpi=300, bbox_inches='tight')
plt.show()

# ãƒ¢ãƒ‡ãƒ«1: ç·šå½¢ãƒ¢ãƒ‡ãƒ«
with pm.Model() as model_linear:
    intercept = pm.Normal('intercept', mu=0, sigma=10)
    slope = pm.Normal('slope', mu=0, sigma=10)
    sigma = pm.HalfNormal('sigma', sigma=5)

    mu = intercept + slope * x
    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y)

    trace_linear = pm.sample(2000, tune=1000, return_inferencedata=True, random_seed=42)

# ãƒ¢ãƒ‡ãƒ«2: 2æ¬¡ãƒ¢ãƒ‡ãƒ«
with pm.Model() as model_quadratic:
    intercept = pm.Normal('intercept', mu=0, sigma=10)
    slope1 = pm.Normal('slope1', mu=0, sigma=10)
    slope2 = pm.Normal('slope2', mu=0, sigma=10)
    sigma = pm.HalfNormal('sigma', sigma=5)

    mu = intercept + slope1 * x + slope2 * x**2
    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y)

    trace_quadratic = pm.sample(2000, tune=1000, return_inferencedata=True, random_seed=42)

# ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒï¼ˆWAIC, LOOã‚’ä½¿ç”¨ï¼‰
print("=== ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ ===")
compare_dict = {'ç·šå½¢ãƒ¢ãƒ‡ãƒ«': trace_linear, '2æ¬¡ãƒ¢ãƒ‡ãƒ«': trace_quadratic}
comparison = az.compare(compare_dict, ic='waic')
print(comparison)

# ãƒ™ã‚¤ã‚ºå› å­ã®è¿‘ä¼¼è¨ˆç®—ï¼ˆé™ç•Œå°¤åº¦ã®æ¯”ï¼‰
# WAICã‚’ç”¨ã„ãŸè¿‘ä¼¼
waic_linear = az.waic(trace_linear, scale='deviance')
waic_quadratic = az.waic(trace_quadratic, scale='deviance')

print(f"\n=== WAIC ===")
print(f"ç·šå½¢ãƒ¢ãƒ‡ãƒ«: {waic_linear.waic:.2f}")
print(f"2æ¬¡ãƒ¢ãƒ‡ãƒ«: {waic_quadratic.waic:.2f}")
print(f"å·®: {waic_linear.waic - waic_quadratic.waic:.2f} (æ­£ãªã‚‰ã°2æ¬¡ãƒ¢ãƒ‡ãƒ«ãŒå„ªå‹¢)")

# LOO (Leave-One-Out Cross-Validation)
loo_linear = az.loo(trace_linear, scale='deviance')
loo_quadratic = az.loo(trace_quadratic, scale='deviance')

print(f"\n=== LOO ===")
print(f"ç·šå½¢ãƒ¢ãƒ‡ãƒ«: {loo_linear.loo:.2f}")
print(f"2æ¬¡ãƒ¢ãƒ‡ãƒ«: {loo_quadratic.loo:.2f}")
print(f"å·®: {loo_linear.loo - loo_quadratic.loo:.2f}")

# äºˆæ¸¬æ¯”è¼ƒ
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬
ax = axes[0]
intercept_lin = trace_linear.posterior['intercept'].values.flatten()
slope_lin = trace_linear.posterior['slope'].values.flatten()

n_samples = 100
indices = np.random.choice(len(intercept_lin), n_samples, replace=False)
for idx in indices:
    y_pred = intercept_lin[idx] + slope_lin[idx] * x
    ax.plot(x, y_pred, color='gray', alpha=0.05, linewidth=0.5)

y_pred_mean = intercept_lin.mean() + slope_lin.mean() * x
ax.plot(x, y_pred_mean, color='blue', linewidth=3, label='ç·šå½¢ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬')
ax.scatter(x, y, alpha=0.6, s=50, color='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')
ax.set_xlabel('x', fontsize=12)
ax.set_ylabel('y', fontsize=12)
ax.set_title('ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

# 2æ¬¡ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬
ax = axes[1]
intercept_quad = trace_quadratic.posterior['intercept'].values.flatten()
slope1_quad = trace_quadratic.posterior['slope1'].values.flatten()
slope2_quad = trace_quadratic.posterior['slope2'].values.flatten()

for idx in indices:
    y_pred = intercept_quad[idx] + slope1_quad[idx] * x + slope2_quad[idx] * x**2
    ax.plot(x, y_pred, color='gray', alpha=0.05, linewidth=0.5)

y_pred_mean = intercept_quad.mean() + slope1_quad.mean() * x + slope2_quad.mean() * x**2
ax.plot(x, y_pred_mean, color='red', linewidth=3, label='2æ¬¡ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬')

# çœŸã®æ›²ç·š
y_true = true_intercept + true_slope1 * x + true_slope2 * x**2
ax.plot(x, y_true, color='green', linestyle='--', linewidth=2, label='çœŸã®ãƒ¢ãƒ‡ãƒ«')

ax.scatter(x, y, alpha=0.6, s=50, color='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')
ax.set_xlabel('x', fontsize=12)
ax.set_ylabel('y', fontsize=12)
ax.set_title('2æ¬¡ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('model_comparison_predictions.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nâœ“ ãƒ™ã‚¤ã‚ºå› å­ï¼ˆWAIC/LOOï¼‰ã«ã‚ˆã‚Šã€2æ¬¡ãƒ¢ãƒ‡ãƒ«ãŒç·šå½¢ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª")
</code></pre>
            </div>

            <div class="output-box">
=== ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ ===
           rank  waic    p_waic   d_waic    weight    se    dse  warning
2æ¬¡ãƒ¢ãƒ‡ãƒ«    0   245.3    3.2      0.0      1.00    12.1   0.0    False
ç·šå½¢ãƒ¢ãƒ‡ãƒ«   1   312.8    2.8     67.5      0.00    14.3  10.2    False

=== WAIC ===
ç·šå½¢ãƒ¢ãƒ‡ãƒ«: 312.82
2æ¬¡ãƒ¢ãƒ‡ãƒ«: 245.31
å·®: 67.51 (æ­£ãªã‚‰ã°2æ¬¡ãƒ¢ãƒ‡ãƒ«ãŒå„ªå‹¢)

=== LOO ===
ç·šå½¢ãƒ¢ãƒ‡ãƒ«: 313.15
2æ¬¡ãƒ¢ãƒ‡ãƒ«: 245.68
å·®: 67.47

âœ“ ãƒ™ã‚¤ã‚ºå› å­ï¼ˆWAIC/LOOï¼‰ã«ã‚ˆã‚Šã€2æ¬¡ãƒ¢ãƒ‡ãƒ«ãŒç·šå½¢ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
            </div>

            <h2>5.5 ãƒ™ã‚¤ã‚ºåˆ†æ•£åˆ†æï¼ˆANOVAï¼‰</h2>

            <p>
                ãƒ™ã‚¤ã‚ºANOVAã¯ã€è¤‡æ•°ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®å¹³å‡å€¤ã®å·®ã‚’éšå±¤ãƒ¢ãƒ‡ãƒ«ã§è©•ä¾¡ã—ã¾ã™ã€‚
                å¾“æ¥ã®Fæ¤œå®šã¨ç•°ãªã‚Šã€å„ã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†å¸ƒã‚’ç›´æ¥æ¨å®šã—ã€ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–ã§ãã¾ã™ã€‚
            </p>

            <div class="theory-box">
                <h4>ğŸ“˜ ãƒ™ã‚¤ã‚ºANOVAã®éšå±¤ãƒ¢ãƒ‡ãƒ«</h4>
                <p>ä¸€å…ƒé…ç½®åˆ†æ•£åˆ†æã®ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ï¼š</p>
                <div class="formula">
                    \[ y_{ij} \sim \mathcal{N}(\mu_i, \sigma^2) \]
                    \[ \mu_i \sim \mathcal{N}(\mu_{\text{global}}, \sigma_{\text{group}}^2) \]
                    \[ \mu_{\text{global}} \sim \mathcal{N}(0, \sigma_{\text{prior}}^2) \]
                    \[ \sigma_{\text{group}}, \sigma \sim \text{HalfNormal}(\cdot) \]
                </div>
                <p>å„é …ï¼š</p>
                <ul>
                    <li>\( y_{ij} \): ã‚°ãƒ«ãƒ¼ãƒ— \( i \) ã® \( j \) ç•ªç›®ã®è¦³æ¸¬å€¤</li>
                    <li>\( \mu_i \): ã‚°ãƒ«ãƒ¼ãƒ— \( i \) ã®å¹³å‡</li>
                    <li>\( \mu_{\text{global}} \): å…¨ä½“å¹³å‡</li>
                    <li>\( \sigma_{\text{group}} \): ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®æ¨™æº–åå·®</li>
                    <li>\( \sigma \): ã‚°ãƒ«ãƒ¼ãƒ—å†…ã®æ¨™æº–åå·®</li>
                </ul>
            </div>

            <div class="example">
                <h4>ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹5: ãƒ™ã‚¤ã‚ºANOVAï¼ˆ3ã¤ã®è£½é€ æ¡ä»¶ã®æ¯”è¼ƒï¼‰</h4>
                <pre><code>import numpy as np
import matplotlib.pyplot as plt
import pymc3 as pm
import arviz as az

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆ3ã¤ã®è£½é€ æ¡ä»¶A, B, Cã§ã®è£½å“å¼·åº¦ï¼‰
np.random.seed(101)

condition_A = np.random.normal(100, 5, 30)
condition_B = np.random.normal(105, 5, 30)
condition_C = np.random.normal(98, 5, 30)

# ãƒ‡ãƒ¼ã‚¿çµåˆ
data = np.concatenate([condition_A, condition_B, condition_C])
groups = np.array([0]*30 + [1]*30 + [2]*30)
group_names = ['æ¡ä»¶A', 'æ¡ä»¶B', 'æ¡ä»¶C']

print("=== ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ ===")
for i, name in enumerate(group_names):
    group_data = data[groups == i]
    print(f"{name}: å¹³å‡={group_data.mean():.2f}, æ¨™æº–åå·®={group_data.std():.2f}, ã‚µãƒ³ãƒ—ãƒ«æ•°={len(group_data)}")

# ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
positions = [1, 2, 3]
bp = plt.boxplot([condition_A, condition_B, condition_C], positions=positions,
                  labels=group_names, patch_artist=True, widths=0.6)

for patch, color in zip(bp['boxes'], ['skyblue', 'lightcoral', 'lightgreen']):
    patch.set_facecolor(color)
    patch.set_alpha(0.7)

plt.ylabel('è£½å“å¼·åº¦ [MPa]', fontsize=12)
plt.title('3ã¤ã®è£½é€ æ¡ä»¶ã«ãŠã‘ã‚‹è£½å“å¼·åº¦', fontsize=14, fontweight='bold')
plt.grid(alpha=0.3, axis='y')
plt.savefig('bayesian_anova_data.png', dpi=300, bbox_inches='tight')
plt.show()

# ãƒ™ã‚¤ã‚ºANOVAãƒ¢ãƒ‡ãƒ«
with pm.Model() as bayesian_anova:
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    mu_global = pm.Normal('mu_global', mu=100, sigma=20)
    sigma_group = pm.HalfNormal('sigma_group', sigma=10)

    # å„ã‚°ãƒ«ãƒ¼ãƒ—ã®å¹³å‡
    mu_groups = pm.Normal('mu_groups', mu=mu_global, sigma=sigma_group, shape=3)

    # ã‚°ãƒ«ãƒ¼ãƒ—å†…æ¨™æº–åå·®
    sigma_within = pm.HalfNormal('sigma_within', sigma=10)

    # å°¤åº¦
    y_obs = pm.Normal('y_obs', mu=mu_groups[groups], sigma=sigma_within, observed=data)

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    trace = pm.sample(2000, tune=1000, return_inferencedata=True, random_seed=42)

# çµæœã®ã‚µãƒãƒªãƒ¼
print("\n=== ãƒ™ã‚¤ã‚ºANOVAçµæœ ===")
print(az.summary(trace, var_names=['mu_global', 'sigma_group', 'sigma_within', 'mu_groups']))

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. å„ã‚°ãƒ«ãƒ¼ãƒ—ã®å¹³å‡ã®äº‹å¾Œåˆ†å¸ƒ
ax = axes[0, 0]
colors = ['skyblue', 'lightcoral', 'lightgreen']
for i, (name, color) in enumerate(zip(group_names, colors)):
    samples = trace.posterior['mu_groups'].values[:, :, i].flatten()
    ax.hist(samples, bins=50, alpha=0.6, color=color, label=name, edgecolor='black')

ax.set_xlabel('ã‚°ãƒ«ãƒ¼ãƒ—å¹³å‡', fontsize=12)
ax.set_ylabel('é »åº¦', fontsize=12)
ax.set_title('å„æ¡ä»¶ã®å¹³å‡ã®äº‹å¾Œåˆ†å¸ƒ', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

# 2. ã‚°ãƒ«ãƒ¼ãƒ—é–“å·®ã®äº‹å¾Œåˆ†å¸ƒ
ax = axes[0, 1]
mu_A = trace.posterior['mu_groups'].values[:, :, 0].flatten()
mu_B = trace.posterior['mu_groups'].values[:, :, 1].flatten()
mu_C = trace.posterior['mu_groups'].values[:, :, 2].flatten()

diff_AB = mu_B - mu_A
diff_AC = mu_C - mu_A
diff_BC = mu_C - mu_B

ax.hist(diff_AB, bins=50, alpha=0.6, color='purple', label='B - A', edgecolor='black')
ax.hist(diff_BC, bins=50, alpha=0.6, color='orange', label='C - B', edgecolor='black')
ax.axvline(0, color='red', linestyle='--', linewidth=2, label='å·®=0')
ax.set_xlabel('å¹³å‡ã®å·®', fontsize=12)
ax.set_ylabel('é »åº¦', fontsize=12)
ax.set_title('ã‚°ãƒ«ãƒ¼ãƒ—é–“å·®ã®äº‹å¾Œåˆ†å¸ƒ', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

# 3. å…¨ä½“å¹³å‡ã¨ã‚°ãƒ«ãƒ¼ãƒ—é–“æ¨™æº–åå·®
ax = axes[1, 0]
ax.hist(trace.posterior['mu_global'].values.flatten(), bins=50, alpha=0.7,
        color='navy', edgecolor='black', label='å…¨ä½“å¹³å‡')
ax.set_xlabel('å…¨ä½“å¹³å‡ (Î¼_global)', fontsize=12)
ax.set_ylabel('é »åº¦', fontsize=12)
ax.set_title('å…¨ä½“å¹³å‡ã®äº‹å¾Œåˆ†å¸ƒ', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

# 4. åˆ†æ•£æˆåˆ†
ax = axes[1, 1]
sigma_group_samples = trace.posterior['sigma_group'].values.flatten()
sigma_within_samples = trace.posterior['sigma_within'].values.flatten()

ax.hist(sigma_group_samples, bins=50, alpha=0.6, color='red', label='ã‚°ãƒ«ãƒ¼ãƒ—é–“SD', edgecolor='black')
ax.hist(sigma_within_samples, bins=50, alpha=0.6, color='blue', label='ã‚°ãƒ«ãƒ¼ãƒ—å†…SD', edgecolor='black')
ax.set_xlabel('æ¨™æº–åå·®', fontsize=12)
ax.set_ylabel('é »åº¦', fontsize=12)
ax.set_title('åˆ†æ•£æˆåˆ†ã®äº‹å¾Œåˆ†å¸ƒ', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('bayesian_anova_results.png', dpi=300, bbox_inches='tight')
plt.show()

# çµ±è¨ˆçš„ãªå·®ã®è©•ä¾¡
print("\n=== ã‚°ãƒ«ãƒ¼ãƒ—é–“å·®ã®çµ±è¨ˆçš„è©•ä¾¡ ===")
print(f"B - A: å¹³å‡={diff_AB.mean():.2f}, 95%HDI=[{np.percentile(diff_AB, 2.5):.2f}, {np.percentile(diff_AB, 97.5):.2f}]")
print(f"  â†’ P(B > A) = {(diff_AB > 0).mean():.3f}")
print(f"C - A: å¹³å‡={diff_AC.mean():.2f}, 95%HDI=[{np.percentile(diff_AC, 2.5):.2f}, {np.percentile(diff_AC, 97.5):.2f}]")
print(f"  â†’ P(C > A) = {(diff_AC > 0).mean():.3f}")
print(f"C - B: å¹³å‡={diff_BC.mean():.2f}, 95%HDI=[{np.percentile(diff_BC, 2.5):.2f}, {np.percentile(diff_BC, 97.5):.2f}]")
print(f"  â†’ P(C > B) = {(diff_BC > 0).mean():.3f}")

print("\nâœ“ ãƒ™ã‚¤ã‚ºANOVAã«ã‚ˆã‚Šã€ã‚°ãƒ«ãƒ¼ãƒ—é–“å·®ã‚’ç¢ºç‡çš„ã«è©•ä¾¡å¯èƒ½")
</code></pre>
            </div>

            <div class="output-box">
=== ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ ===
æ¡ä»¶A: å¹³å‡=100.23, æ¨™æº–åå·®=4.98, ã‚µãƒ³ãƒ—ãƒ«æ•°=30
æ¡ä»¶B: å¹³å‡=105.12, æ¨™æº–åå·®=5.03, ã‚µãƒ³ãƒ—ãƒ«æ•°=30
æ¡ä»¶C: å¹³å‡=98.45, æ¨™æº–åå·®=4.91, ã‚µãƒ³ãƒ—ãƒ«æ•°=30

=== ãƒ™ã‚¤ã‚ºANOVAçµæœ ===
              mean    sd  hdi_3%  hdi_97%
mu_global    101.3   1.9    97.7   104.8
sigma_group    3.2   1.5     0.8     6.1
sigma_within   5.0   0.4     4.3     5.7
mu_groups[0] 100.2   0.9    98.5   101.9
mu_groups[1] 105.1   0.9   103.4   106.8
mu_groups[2]  98.4   0.9    96.7   100.1

=== ã‚°ãƒ«ãƒ¼ãƒ—é–“å·®ã®çµ±è¨ˆçš„è©•ä¾¡ ===
B - A: å¹³å‡=4.89, 95%HDI=[2.71, 7.08]
  â†’ P(B > A) = 1.000
C - A: å¹³å‡=-1.78, 95%HDI=[-3.96, 0.39]
  â†’ P(C > A) = 0.053
C - B: å¹³å‡=-6.68, 95%HDI=[-8.86, -4.50]
  â†’ P(C > B) = 0.000

âœ“ ãƒ™ã‚¤ã‚ºANOVAã«ã‚ˆã‚Šã€ã‚°ãƒ«ãƒ¼ãƒ—é–“å·®ã‚’ç¢ºç‡çš„ã«è©•ä¾¡å¯èƒ½
            </div>

            <h2>5.6 å“è³ªç®¡ç†ã¸ã®å¿œç”¨</h2>

            <p>
                ãƒ™ã‚¤ã‚ºçµ±è¨ˆã¯å“è³ªç®¡ç†ã«ãŠã„ã¦ã€è£½å“ã®åˆæ ¼ç‡æ¨å®šã€ä¸è‰¯ç‡ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã€å·¥ç¨‹èƒ½åŠ›è©•ä¾¡ãªã©ã«å¿œç”¨ã§ãã¾ã™ã€‚
                äº‹å¾Œåˆ†å¸ƒã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ãƒªã‚¹ã‚¯ã‚’å®šé‡åŒ–ã—ãŸæ„æ€æ±ºå®šãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚
            </p>

            <div class="example">
                <h4>ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹6: å“è³ªç®¡ç†ã«ãŠã‘ã‚‹äº‹å¾Œåˆ†å¸ƒã®æ´»ç”¨ï¼ˆè£½å“åˆæ ¼ç‡ã®æ¨å®šï¼‰</h4>
                <pre><code>import numpy as np
import matplotlib.pyplot as plt
import pymc3 as pm
import arviz as az
from scipy import stats

# ã‚·ãƒŠãƒªã‚ª: è£½å“ã®å“è³ªæ¤œæŸ»ã§100å€‹ä¸­92å€‹ãŒåˆæ ¼
# åˆæ ¼ç‡ã‚’æ¨å®šã—ã€åˆæ ¼ç‡ãŒ90%ä»¥ä¸Šã§ã‚ã‚‹ç¢ºç‡ã‚’è©•ä¾¡ã—ãŸã„

n_inspected = 100
n_passed = 92

print(f"=== å“è³ªæ¤œæŸ»ãƒ‡ãƒ¼ã‚¿ ===")
print(f"æ¤œæŸ»æ•°: {n_inspected}")
print(f"åˆæ ¼æ•°: {n_passed}")
print(f"åˆæ ¼ç‡: {n_passed/n_inspected:.1%}")

# ãƒ™ã‚¤ã‚ºæ¨å®šï¼ˆBeta-Binomial ãƒ¢ãƒ‡ãƒ«ï¼‰
with pm.Model() as quality_model:
    # äº‹å‰åˆ†å¸ƒ: Beta(2, 2) (å¼±ã„æƒ…å ±ã®ã‚ã‚‹äº‹å‰åˆ†å¸ƒ)
    p_pass = pm.Beta('p_pass', alpha=2, beta=2)

    # å°¤åº¦: äºŒé …åˆ†å¸ƒ
    n_obs = pm.Binomial('n_obs', n=n_inspected, p=p_pass, observed=n_passed)

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    trace = pm.sample(2000, tune=1000, return_inferencedata=True, random_seed=42)

# çµæœã®ã‚µãƒãƒªãƒ¼
print("\n=== ãƒ™ã‚¤ã‚ºæ¨å®šçµæœ ===")
print(az.summary(trace, var_names=['p_pass']))

p_pass_samples = trace.posterior['p_pass'].values.flatten()

# åˆæ ¼ç‡ãŒ90%ä»¥ä¸Šã§ã‚ã‚‹ç¢ºç‡
prob_above_90 = (p_pass_samples > 0.90).mean()
print(f"\nåˆæ ¼ç‡ãŒ90%ä»¥ä¸Šã§ã‚ã‚‹ç¢ºç‡: {prob_above_90:.3f}")

# 95% ä¿¡é ¼åŒºé–“
hdi_95 = az.hdi(trace, var_names=['p_pass'], hdi_prob=0.95)
print(f"åˆæ ¼ç‡ã®95% HDI: [{hdi_95['p_pass'].values[0]:.3f}, {hdi_95['p_pass'].values[1]:.3f}]")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. äº‹å‰åˆ†å¸ƒã¨äº‹å¾Œåˆ†å¸ƒã®æ¯”è¼ƒ
ax = axes[0, 0]
p_range = np.linspace(0, 1, 500)

# äº‹å‰åˆ†å¸ƒ Beta(2, 2)
prior_pdf = stats.beta.pdf(p_range, 2, 2)
ax.plot(p_range, prior_pdf, linewidth=2, color='blue', label='äº‹å‰åˆ†å¸ƒ Beta(2,2)')

# äº‹å¾Œåˆ†å¸ƒï¼ˆè§£æçš„ï¼‰: Beta(2+92, 2+8) = Beta(94, 10)
posterior_pdf = stats.beta.pdf(p_range, 2+n_passed, 2+(n_inspected-n_passed))
ax.plot(p_range, posterior_pdf, linewidth=2, color='red', label='äº‹å¾Œåˆ†å¸ƒ Beta(94,10)')

# MCMCã‚µãƒ³ãƒ—ãƒ«ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
ax.hist(p_pass_samples, bins=50, density=True, alpha=0.3, color='red', edgecolor='black', label='MCMC ã‚µãƒ³ãƒ—ãƒ«')

ax.axvline(0.90, color='green', linestyle='--', linewidth=2, label='åŸºæº–å€¤ 90%')
ax.set_xlabel('åˆæ ¼ç‡ p', fontsize=12)
ax.set_ylabel('ç¢ºç‡å¯†åº¦', fontsize=12)
ax.set_title('åˆæ ¼ç‡ã®äº‹å‰åˆ†å¸ƒã¨äº‹å¾Œåˆ†å¸ƒ', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

# 2. ç´¯ç©åˆ†å¸ƒé–¢æ•°
ax = axes[0, 1]
sorted_samples = np.sort(p_pass_samples)
cdf = np.arange(1, len(sorted_samples)+1) / len(sorted_samples)
ax.plot(sorted_samples, cdf, linewidth=2, color='purple')
ax.axvline(0.90, color='green', linestyle='--', linewidth=2, label='åŸºæº–å€¤ 90%')
ax.axhline(prob_above_90, color='orange', linestyle='--', linewidth=2,
           label=f'P(pâ‰¥0.90)={prob_above_90:.3f}')
ax.set_xlabel('åˆæ ¼ç‡ p', fontsize=12)
ax.set_ylabel('ç´¯ç©ç¢ºç‡', fontsize=12)
ax.set_title('åˆæ ¼ç‡ã®ç´¯ç©åˆ†å¸ƒé–¢æ•°', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

# 3. ãƒªã‚¹ã‚¯è©•ä¾¡: æ¬¡ã®ãƒ­ãƒƒãƒˆ(n=50)ã§ã®ä¸åˆæ ¼æ•°ã®äºˆæ¸¬åˆ†å¸ƒ
ax = axes[1, 0]
n_next = 50
predicted_failures = []

for p in p_pass_samples[:1000]:  # 1000ã‚µãƒ³ãƒ—ãƒ«ä½¿ç”¨
    n_fail = np.random.binomial(n_next, 1-p)
    predicted_failures.append(n_fail)

ax.hist(predicted_failures, bins=range(0, 15), alpha=0.7, color='coral', edgecolor='black', density=True)
ax.set_xlabel('æ¬¡ã®ãƒ­ãƒƒãƒˆ(n=50)ã§ã®ä¸åˆæ ¼æ•°', fontsize=12)
ax.set_ylabel('ç¢ºç‡', fontsize=12)
ax.set_title('æ¬¡ã®ãƒ­ãƒƒãƒˆã§ã®ä¸åˆæ ¼æ•°ã®äºˆæ¸¬åˆ†å¸ƒ', fontsize=14, fontweight='bold')
ax.grid(alpha=0.3, axis='y')

# 4. æ„æ€æ±ºå®š: åˆæ ¼ç‡ã®é–¾å€¤åˆ¥ã®ãƒªã‚¹ã‚¯
ax = axes[1, 1]
thresholds = np.linspace(0.80, 0.99, 20)
probs = [(p_pass_samples > t).mean() for t in thresholds]

ax.plot(thresholds, probs, linewidth=2, marker='o', markersize=6, color='navy')
ax.axhline(0.95, color='red', linestyle='--', linewidth=2, label='95%ä¿¡é ¼æ°´æº–')
ax.axvline(0.90, color='green', linestyle='--', linewidth=2, label='åŸºæº–å€¤ 90%')
ax.set_xlabel('åˆæ ¼ç‡ã®é–¾å€¤', fontsize=12)
ax.set_ylabel('P(åˆæ ¼ç‡ â‰¥ é–¾å€¤)', fontsize=12)
ax.set_title('åˆæ ¼ç‡é–¾å€¤åˆ¥ã®é”æˆç¢ºç‡', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('quality_control_bayesian.png', dpi=300, bbox_inches='tight')
plt.show()

# æ„æ€æ±ºå®šæ”¯æ´æƒ…å ±
print("\n=== æ„æ€æ±ºå®šæ”¯æ´æƒ…å ± ===")
expected_failures = np.mean(predicted_failures)
print(f"æ¬¡ã®ãƒ­ãƒƒãƒˆ(n=50)ã§ã®äºˆæ¸¬ä¸åˆæ ¼æ•°: å¹³å‡={expected_failures:.2f}")
print(f"ä¸åˆæ ¼æ•°ãŒ5å€‹ä»¥ä¸Šã«ãªã‚‹ç¢ºç‡: {(np.array(predicted_failures) >= 5).mean():.3f}")
print(f"ä¸åˆæ ¼æ•°ãŒ10å€‹ä»¥ä¸Šã«ãªã‚‹ç¢ºç‡: {(np.array(predicted_failures) >= 10).mean():.3f}")

print("\nâœ“ ãƒ™ã‚¤ã‚ºæ¨å®šã«ã‚ˆã‚Šã€åˆæ ¼ç‡ã®ä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®ã—ãŸãƒªã‚¹ã‚¯è©•ä¾¡ãŒå¯èƒ½")
</code></pre>
            </div>

            <div class="output-box">
=== å“è³ªæ¤œæŸ»ãƒ‡ãƒ¼ã‚¿ ===
æ¤œæŸ»æ•°: 100
åˆæ ¼æ•°: 92
åˆæ ¼ç‡: 92.0%

=== ãƒ™ã‚¤ã‚ºæ¨å®šçµæœ ===
        mean    sd  hdi_3%  hdi_97%
p_pass  0.906  0.028   0.853   0.959

åˆæ ¼ç‡ãŒ90%ä»¥ä¸Šã§ã‚ã‚‹ç¢ºç‡: 0.584
åˆæ ¼ç‡ã®95% HDI: [0.853, 0.959]

=== æ„æ€æ±ºå®šæ”¯æ´æƒ…å ± ===
æ¬¡ã®ãƒ­ãƒƒãƒˆ(n=50)ã§ã®äºˆæ¸¬ä¸åˆæ ¼æ•°: å¹³å‡=4.69
ä¸åˆæ ¼æ•°ãŒ5å€‹ä»¥ä¸Šã«ãªã‚‹ç¢ºç‡: 0.426
ä¸åˆæ ¼æ•°ãŒ10å€‹ä»¥ä¸Šã«ãªã‚‹ç¢ºç‡: 0.012

âœ“ ãƒ™ã‚¤ã‚ºæ¨å®šã«ã‚ˆã‚Šã€åˆæ ¼ç‡ã®ä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®ã—ãŸãƒªã‚¹ã‚¯è©•ä¾¡ãŒå¯èƒ½
            </div>

            <h2>5.7 æ©Ÿæ¢°å­¦ç¿’ã¸ã®å¿œç”¨: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–</h2>

            <p>
                ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ï¼ˆBayesian Optimizationï¼‰ã¯ã€è©•ä¾¡ã‚³ã‚¹ãƒˆã®é«˜ã„ç›®çš„é–¢æ•°ã‚’åŠ¹ç‡çš„ã«æœ€é©åŒ–ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚
                ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€ææ–™è¨­è¨ˆã€å®Ÿé¨“è¨ˆç”»ãªã©ã§åºƒãä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚
            </p>

            <div class="theory-box">
                <h4>ğŸ“˜ ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®åŸç†</h4>
                <p>ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã¯ä»¥ä¸‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã§é€²è¡Œã—ã¾ã™ï¼š</p>
                <ol>
                    <li><strong>ã‚µãƒ­ã‚²ãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰</strong>: ã‚¬ã‚¦ã‚¹éç¨‹ï¼ˆGPï¼‰ã§ç›®çš„é–¢æ•°ã‚’è¿‘ä¼¼
                        <div class="formula">
                            \[ f(x) \sim \mathcal{GP}(\mu(x), k(x, x')) \]
                        </div>
                    </li>
                    <li><strong>ç²å¾—é–¢æ•°ã®æœ€å¤§åŒ–</strong>: æ¬¡ã«è©•ä¾¡ã™ã‚‹ç‚¹ã‚’é¸æŠ
                        <ul>
                            <li><strong>Expected Improvement (EI)</strong>: æœŸå¾…æ”¹å–„é‡
                                <div class="formula">
                                    \[ EI(x) = \mathbb{E}[\max(f(x) - f(x^*), 0)] \]
                                </div>
                            </li>
                            <li><strong>Upper Confidence Bound (UCB)</strong>: ä¿¡é ¼ä¸Šé™
                                <div class="formula">
                                    \[ UCB(x) = \mu(x) + \kappa \sigma(x) \]
                                </div>
                            </li>
                        </ul>
                    </li>
                    <li><strong>è©•ä¾¡ã¨æ›´æ–°</strong>: é¸æŠã—ãŸç‚¹ã§ç›®çš„é–¢æ•°ã‚’è©•ä¾¡ã—ã€GPã‚’æ›´æ–°</li>
                </ol>
            </div>

            <div class="example">
                <h4>ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹7: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</h4>
                <pre><code>import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from skopt import gp_minimize
from skopt.space import Integer
from skopt.plots import plot_convergence, plot_objective
from skopt.acquisition import gaussian_ei

# åˆ†é¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ
np.random.seed(42)
X, y = make_classification(n_samples=500, n_features=20, n_informative=15,
                          n_redundant=5, random_state=42)

print("=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦ ===")
print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {X.shape[0]}")
print(f"ç‰¹å¾´æ•°: {X.shape[1]}")
print(f"ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {np.bincount(y)}")

# ç›®çš„é–¢æ•°: RandomForestã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
def objective(params):
    """
    ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹ç›®çš„é–¢æ•°ï¼ˆæœ€å°åŒ–å•é¡Œï¼‰
    params: [n_estimators, max_depth, min_samples_split]
    """
    n_estimators, max_depth, min_samples_split = params

    # RandomForestãƒ¢ãƒ‡ãƒ«
    model = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        random_state=42,
        n_jobs=-1
    )

    # 5-fold ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')

    # ç²¾åº¦ã®è² å€¤ã‚’è¿”ã™ï¼ˆæœ€å°åŒ–å•é¡Œã«ã™ã‚‹ãŸã‚ï¼‰
    return -scores.mean()

# æ¢ç´¢ç©ºé–“ã®å®šç¾©
space = [
    Integer(10, 200, name='n_estimators'),      # æ±ºå®šæœ¨ã®æ•°
    Integer(3, 30, name='max_depth'),           # æœ¨ã®æ·±ã•
    Integer(2, 20, name='min_samples_split')    # åˆ†å‰²ã«å¿…è¦ãªæœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
]

print("\n=== ãƒ™ã‚¤ã‚ºæœ€é©åŒ–é–‹å§‹ ===")
print("æ¢ç´¢ç©ºé–“:")
print(f"  n_estimators: [10, 200]")
print(f"  max_depth: [3, 30]")
print(f"  min_samples_split: [2, 20]")

# ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®å®Ÿè¡Œ
result = gp_minimize(
    objective,
    space,
    n_calls=30,              # è©•ä¾¡å›æ•°
    n_initial_points=5,      # ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã®ç‚¹æ•°
    acq_func='EI',           # Expected Improvement
    random_state=42,
    verbose=False
)

print(f"\n=== æœ€é©åŒ–çµæœ ===")
print(f"æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
print(f"  n_estimators: {result.x[0]}")
print(f"  max_depth: {result.x[1]}")
print(f"  min_samples_split: {result.x[2]}")
print(f"æœ€è‰¯ã‚¹ã‚³ã‚¢ï¼ˆç²¾åº¦ï¼‰: {-result.fun:.4f}")

# åæŸãƒ—ãƒ­ãƒƒãƒˆ
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. åæŸæ›²ç·š
ax = axes[0, 0]
n_calls = len(result.func_vals)
best_so_far = np.minimum.accumulate(result.func_vals)
ax.plot(range(1, n_calls+1), -result.func_vals, 'bo-', alpha=0.6, label='å„è©•ä¾¡ã®ã‚¹ã‚³ã‚¢')
ax.plot(range(1, n_calls+1), -best_so_far, 'r-', linewidth=2, label='æœ€è‰¯ã‚¹ã‚³ã‚¢ï¼ˆç´¯ç©ï¼‰')
ax.set_xlabel('è©•ä¾¡å›æ•°', fontsize=12)
ax.set_ylabel('ç²¾åº¦', fontsize=12)
ax.set_title('ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®åæŸ', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

# 2. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®æ¢ç´¢ï¼ˆn_estimators vs max_depthï¼‰
ax = axes[0, 1]
params_history = np.array(result.x_iters)
scores_history = -np.array(result.func_vals)

scatter = ax.scatter(params_history[:, 0], params_history[:, 1],
                     c=scores_history, cmap='viridis', s=100,
                     edgecolor='black', linewidth=1, alpha=0.7)
ax.scatter(result.x[0], result.x[1], color='red', s=300, marker='*',
           edgecolor='black', linewidth=2, label='æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿', zorder=5)
plt.colorbar(scatter, ax=ax, label='ç²¾åº¦')
ax.set_xlabel('n_estimators', fontsize=12)
ax.set_ylabel('max_depth', fontsize=12)
ax.set_title('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®æ¢ç´¢ï¼ˆn_estimators vs max_depthï¼‰', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

# 3. å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é‡è¦æ€§
ax = axes[1, 0]
param_names = ['n_estimators', 'max_depth', 'min_samples_split']
param_values = [params_history[:, i] for i in range(3)]
colors = ['skyblue', 'lightcoral', 'lightgreen']

bp = ax.boxplot(param_values, labels=param_names, patch_artist=True)
for patch, color in zip(bp['boxes'], colors):
    patch.set_facecolor(color)
    patch.set_alpha(0.7)

ax.set_ylabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤', fontsize=12)
ax.set_title('æ¢ç´¢ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆ†å¸ƒ', fontsize=14, fontweight='bold')
ax.grid(alpha=0.3, axis='y')

# 4. ç²å¾—é–¢æ•°ã®æ¨ç§»
ax = axes[1, 1]
# å„è©•ä¾¡ç‚¹ã§ã®EIå€¤ã‚’å†è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
ei_values = []
for i in range(1, len(result.func_vals)):
    best_y = np.min(result.func_vals[:i])
    current_y = result.func_vals[i]
    improvement = max(best_y - current_y, 0)
    ei_values.append(improvement)

ax.plot(range(2, n_calls+1), ei_values, 'go-', linewidth=2, markersize=6)
ax.set_xlabel('è©•ä¾¡å›æ•°', fontsize=12)
ax.set_ylabel('æ”¹å–„é‡ï¼ˆç°¡æ˜“EIï¼‰', fontsize=12)
ax.set_title('ç²å¾—é–¢æ•°ã®æ¨ç§»ï¼ˆæ”¹å–„é‡ï¼‰', fontsize=14, fontweight='bold')
ax.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('bayesian_optimization_results.png', dpi=300, bbox_inches='tight')
plt.show()

# ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã¨ã®æ¯”è¼ƒ
print("\n=== ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã¨ã®æ¯”è¼ƒ ===")
np.random.seed(42)
random_scores = []
for _ in range(30):
    random_params = [
        np.random.randint(10, 200),   # n_estimators
        np.random.randint(3, 30),     # max_depth
        np.random.randint(2, 20)      # min_samples_split
    ]
    score = objective(random_params)
    random_scores.append(score)

best_random_score = -min(random_scores)
print(f"ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®æœ€è‰¯ã‚¹ã‚³ã‚¢: {-result.fun:.4f}")
print(f"ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã®æœ€è‰¯ã‚¹ã‚³ã‚¢: {best_random_score:.4f}")
print(f"æ”¹å–„: {(-result.fun - best_random_score):.4f} ({100*(-result.fun - best_random_score)/best_random_score:.2f}%)")

# æœ€é©ãƒ¢ãƒ‡ãƒ«ã§ã®æœ€çµ‚è©•ä¾¡
best_model = RandomForestClassifier(
    n_estimators=result.x[0],
    max_depth=result.x[1],
    min_samples_split=result.x[2],
    random_state=42,
    n_jobs=-1
)
final_scores = cross_val_score(best_model, X, y, cv=5, scoring='accuracy')
print(f"\næœ€é©ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°è©•ä¾¡:")
print(f"  å¹³å‡ç²¾åº¦: {final_scores.mean():.4f}")
print(f"  æ¨™æº–åå·®: {final_scores.std():.4f}")
print(f"  å„Fold: {final_scores}")

print("\nâœ“ ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚Šã€åŠ¹ç‡çš„ã«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–")
</code></pre>
            </div>

            <div class="output-box">
=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦ ===
ã‚µãƒ³ãƒ—ãƒ«æ•°: 500
ç‰¹å¾´æ•°: 20
ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: [250 250]

=== ãƒ™ã‚¤ã‚ºæœ€é©åŒ–é–‹å§‹ ===
æ¢ç´¢ç©ºé–“:
  n_estimators: [10, 200]
  max_depth: [3, 30]
  min_samples_split: [2, 20]

=== æœ€é©åŒ–çµæœ ===
æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
  n_estimators: 156
  max_depth: 15
  min_samples_split: 2
æœ€è‰¯ã‚¹ã‚³ã‚¢ï¼ˆç²¾åº¦ï¼‰: 0.9460

=== ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã¨ã®æ¯”è¼ƒ ===
ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®æœ€è‰¯ã‚¹ã‚³ã‚¢: 0.9460
ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã®æœ€è‰¯ã‚¹ã‚³ã‚¢: 0.9340
æ”¹å–„: 0.0120 (1.28%)

æœ€é©ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°è©•ä¾¡:
  å¹³å‡ç²¾åº¦: 0.9460
  æ¨™æº–åå·®: 0.0179
  å„Fold: [0.93 0.96 0.97 0.93 0.94]

âœ“ ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚Šã€åŠ¹ç‡çš„ã«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–
            </div>

            <div class="exercise">
                <h4>ğŸ“ æ¼”ç¿’å•é¡Œ</h4>

                <h4>å•é¡Œ1: éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã®ç†è§£</h4>
                <p>
                    3ã¤ã®ç ”ç©¶å®¤ï¼ˆA, B, Cï¼‰ã§åŒã˜å®Ÿé¨“ã‚’è¡Œã„ã€ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã€‚
                    éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã€å„ç ”ç©¶å®¤ã®åŠ¹æœã¨å…¨ä½“ã®åŠ¹æœã‚’æ¨å®šã—ã¦ãã ã•ã„ã€‚
                </p>
                <ul>
                    <li>ç ”ç©¶å®¤A: [25.1, 26.3, 24.8, 25.9, 26.1]</li>
                    <li>ç ”ç©¶å®¤B: [28.2, 29.1, 27.8, 28.5]</li>
                    <li>ç ”ç©¶å®¤C: [23.5, 24.2, 23.8, 24.0, 23.6, 24.1]</li>
                </ul>
                <p>
                    (a) å„ç ”ç©¶å®¤ã®å¹³å‡ã®äº‹å¾Œåˆ†å¸ƒã‚’æ±‚ã‚ã‚ˆ<br>
                    (b) å…¨ä½“å¹³å‡ã®95%ä¿¡é ¼åŒºé–“ã‚’æ±‚ã‚ã‚ˆ<br>
                    (c) ç ”ç©¶å®¤é–“ã®æ¨™æº–åå·®ã‚’æ¨å®šã›ã‚ˆ
                </p>

                <h4>å•é¡Œ2: ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ã®å¿œç”¨</h4>
                <p>
                    ææ–™ã®ç¡¬åº¦ï¼ˆHVï¼‰ã¨ç‚­ç´ å«æœ‰é‡ï¼ˆmass%ï¼‰ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã™ã€‚
                    ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ã‚’é©ç”¨ã—ã€ç‚­ç´ å«æœ‰é‡0.5%ã®ã¨ãã®ç¡¬åº¦ã®äºˆæ¸¬åŒºé–“ï¼ˆ95%ï¼‰ã‚’æ±‚ã‚ã¦ãã ã•ã„ã€‚
                </p>
                <pre><code>carbon = np.array([0.1, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8])
hardness = np.array([120, 145, 170, 195, 245, 270, 295])</code></pre>

                <h4>å•é¡Œ3: ãƒ™ã‚¤ã‚ºå› å­ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«é¸æŠ</h4>
                <p>
                    ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã€ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã¨3æ¬¡ãƒ¢ãƒ‡ãƒ«ã®ã©ã¡ã‚‰ãŒé©åˆ‡ã‹ã‚’WAICã¾ãŸã¯LOOã‚’ç”¨ã„ã¦è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
                </p>
                <pre><code>x = np.array([1, 2, 3, 4, 5, 6, 7, 8])
y = np.array([2.1, 3.9, 9.2, 15.8, 25.1, 36.3, 49.8, 64.2])</code></pre>

                <h4>å•é¡Œ4: å“è³ªç®¡ç†ã®ãƒ™ã‚¤ã‚ºæ¨å®š</h4>
                <p>
                    è£½å“ã®æ¤œæŸ»ã§500å€‹ä¸­475å€‹ãŒåˆæ ¼ã—ã¾ã—ãŸã€‚
                    åˆæ ¼ç‡ãŒ95%ä»¥ä¸Šã§ã‚ã‚‹ç¢ºç‡ã‚’æ±‚ã‚ã€æ¬¡ã®ãƒ­ãƒƒãƒˆï¼ˆn=100ï¼‰ã§ã®ä¸åˆæ ¼æ•°ã®äºˆæ¸¬åˆ†å¸ƒã‚’æã„ã¦ãã ã•ã„ã€‚
                </p>

                <h4>å•é¡Œ5: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®å®Ÿè£…</h4>
                <p>
                    ä»¥ä¸‹ã®é–¢æ•°ã‚’æœ€å°åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (x, y) ã‚’ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§æ¢ç´¢ã—ã¦ãã ã•ã„ï¼ˆæ¢ç´¢ç¯„å›²: x, y âˆˆ [-5, 5]ï¼‰ã€‚
                </p>
                <pre><code>def objective(params):
    x, y = params
    return (x - 2)**2 + (y + 1)**2 + np.sin(5*x) * np.cos(5*y)</code></pre>
            </div>

            <h2>5.8 ã¾ã¨ã‚</h2>

            <p>æœ¬ç« ã§ã¯ã€éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã¨å®Ÿå•é¡Œã¸ã®å¿œç”¨ã‚’å­¦ã³ã¾ã—ãŸã€‚</p>

            <div class="theory-box">
                <h4>ğŸ“š æœ¬ç« ã®ãƒã‚¤ãƒ³ãƒˆ</h4>
                <ul>
                    <li><strong>éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«</strong>: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å°å…¥ã—ã€ã‚°ãƒ«ãƒ¼ãƒ—é–“ã§æƒ…å ±ã‚’å…±æœ‰ã—ãªãŒã‚‰å€‹åˆ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®š</li>
                    <li><strong>éƒ¨åˆ†ãƒ—ãƒ¼ãƒªãƒ³ã‚°</strong>: å®Œå…¨ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã¨éãƒ—ãƒ¼ãƒªãƒ³ã‚°ã®ä¸­é–“ã§ã€ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ã‚°ãƒ«ãƒ¼ãƒ—ã§ã‚‚å®‰å®šã—ãŸæ¨å®šãŒå¯èƒ½</li>
                    <li><strong>ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°</strong>: å›å¸°ä¿‚æ•°ã‚’ç¢ºç‡åˆ†å¸ƒã¨ã—ã¦æ¨å®šã—ã€äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–</li>
                    <li><strong>ãƒ™ã‚¤ã‚ºãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°</strong>: äºŒå€¤åˆ†é¡å•é¡Œã§ç¢ºç‡çš„ãªäºˆæ¸¬ã¨ä¿¡é ¼åŒºé–“ã‚’æä¾›</li>
                    <li><strong>ãƒ™ã‚¤ã‚ºå› å­</strong>: WAIC/LOOã‚’ç”¨ã„ãŸãƒ¢ãƒ‡ãƒ«é¸æŠã§ã€éå­¦ç¿’ã‚’é˜²ãã¤ã¤æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ</li>
                    <li><strong>ãƒ™ã‚¤ã‚ºANOVA</strong>: è¤‡æ•°ã‚°ãƒ«ãƒ¼ãƒ—ã®æ¯”è¼ƒã§ã€ã‚°ãƒ«ãƒ¼ãƒ—é–“å·®ã‚’ç¢ºç‡çš„ã«è©•ä¾¡</li>
                    <li><strong>å“è³ªç®¡ç†ã¸ã®å¿œç”¨</strong>: åˆæ ¼ç‡ã®æ¨å®šã¨ãƒªã‚¹ã‚¯è©•ä¾¡ã§ã€ä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®ã—ãŸæ„æ€æ±ºå®šã‚’æ”¯æ´</li>
                    <li><strong>ãƒ™ã‚¤ã‚ºæœ€é©åŒ–</strong>: ã‚¬ã‚¦ã‚¹éç¨‹ã¨ç²å¾—é–¢æ•°ã‚’ç”¨ã„ã¦ã€è©•ä¾¡ã‚³ã‚¹ãƒˆã®é«˜ã„ç›®çš„é–¢æ•°ã‚’åŠ¹ç‡çš„ã«æœ€é©åŒ–</li>
                </ul>
            </div>

            <div class="note">
                <p><strong>å®Ÿå‹™ã¸ã®å¿œç”¨</strong></p>
                <p>
                    éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã¯ã€ææ–™ç§‘å­¦ã€å“è³ªç®¡ç†ã€è‡¨åºŠè©¦é¨“ãªã©ã€ã‚°ãƒ«ãƒ¼ãƒ—æ§‹é€ ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã®è§£æã«åºƒãåˆ©ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
                    ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã¯ã€å®Ÿé¨“è¨ˆç”»ã€ææ–™è¨­è¨ˆã€æ©Ÿæ¢°å­¦ç¿’ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãªã©ã€è©•ä¾¡ã‚³ã‚¹ãƒˆãŒé«˜ã„å•é¡Œã§ç‰¹ã«æœ‰åŠ¹ã§ã™ã€‚
                </p>
                <p>
                    æœ¬ã‚·ãƒªãƒ¼ã‚ºã‚’é€šã˜ã¦ã€æ¨å®šç†è«–ã€ä»®èª¬æ¤œå®šã€ãƒ™ã‚¤ã‚ºæ¨è«–ã®åŸºç¤ã‹ã‚‰å¿œç”¨ã¾ã§ã‚’å­¦ã³ã¾ã—ãŸã€‚
                    ã“ã‚Œã‚‰ã®æ‰‹æ³•ã‚’é©åˆ‡ã«ä½¿ã„åˆ†ã‘ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¿¡é ¼æ€§ã®é«˜ã„çŸ¥è¦‹ã‚’å¼•ãå‡ºã™ã“ã¨ãŒã€ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ã®ç ”ç©¶é–‹ç™ºã«ãŠã„ã¦é‡è¦ã§ã™ã€‚
                </p>
            </div>

            <div class="nav-buttons">
                <a href="chapter-4.html" class="nav-button">â† å‰ã®ç« : ãƒ™ã‚¤ã‚ºæ¨è«–ã®åŸºç¤ã¨MCMC</a>
                <a href="index.html" class="nav-button">ç›®æ¬¡ã«æˆ»ã‚‹</a>
            </div>
        </div>
    </div>

    <section class="disclaimer">
<h3>å…è²¬äº‹é …</h3>
<ul>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹Code examplesã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
<li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
</ul>
</section>

<footer>
        <p>&copy; 2025 FM Dojo - Fundamentals of Mathematics & Physics. All rights reserved.</p>
    </footer>
</body>
</html>
