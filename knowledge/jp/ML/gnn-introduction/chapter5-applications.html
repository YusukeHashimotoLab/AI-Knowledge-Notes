<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬5ç« ï¼šGNNã®å¿œç”¨ (GNN Applications) - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
        <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/wp/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/ML/gnn-introduction/index.html">Gnn</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 5</span>
        </div>
    </nav>

    <header>
        <div class="header-content">
            <h1>ç¬¬5ç« ï¼šGNNã®å¿œç”¨ (GNN Applications)</h1>
            <p class="subtitle">åˆ†å­äºˆæ¸¬ã‹ã‚‰æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¾ã§ã®å®Ÿè·µçš„å¿œç”¨</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… ãƒãƒ¼ãƒ‰åˆ†é¡ã‚¿ã‚¹ã‚¯ã‚’åŠæ•™å¸«ã‚ã‚Šå­¦ç¿’ã§å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… ã‚°ãƒ©ãƒ•åˆ†é¡ã«ã‚ˆã‚‹åˆ†å­ç‰¹æ€§äºˆæ¸¬ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… ãƒªãƒ³ã‚¯äºˆæ¸¬ã«ã‚ˆã‚‹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
<li>âœ… å‰µè–¬å¿œç”¨ã«ãŠã‘ã‚‹è–¬å‰¤-æ¨™çš„ç›¸äº’ä½œç”¨äºˆæ¸¬ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… çŸ¥è­˜ã‚°ãƒ©ãƒ•ã¨äº¤é€šãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¸ã®å¿œç”¨ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… å®Ÿè·µçš„ãªGNNãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’è¨­è¨ˆãƒ»å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… GNNã®é™ç•Œã¨å¯¾å‡¦æ³•ã‚’ç†è§£ã§ãã‚‹</li>
</ul>

<hr>

<h2>5.1 ãƒãƒ¼ãƒ‰åˆ†é¡ (Node Classification)</h2>

<h3>åŠæ•™å¸«ã‚ã‚Šå­¦ç¿’ã«ã‚ˆã‚‹ãƒãƒ¼ãƒ‰åˆ†é¡</h3>

<p><strong>ãƒãƒ¼ãƒ‰åˆ†é¡</strong>ã¯ã€ã‚°ãƒ©ãƒ•å†…ã®å„ãƒãƒ¼ãƒ‰ã«ãƒ©ãƒ™ãƒ«ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚å…¸å‹çš„ãªå¿œç”¨ã¨ã—ã¦ã€ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãŠã‘ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³åˆ†é¡ã€è«–æ–‡å¼•ç”¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãŠã‘ã‚‹ç ”ç©¶åˆ†é‡åˆ†é¡ãªã©ãŒã‚ã‚Šã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[å…¥åŠ›ã‚°ãƒ©ãƒ•] --> B[GNN Layers<br/>ç‰¹å¾´ä¼æ’­]
    B --> C[ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿]
    C --> D[åˆ†é¡å™¨<br/>Linear + Softmax]
    D --> E[ãƒãƒ¼ãƒ‰ãƒ©ãƒ™ãƒ«äºˆæ¸¬]

    F[è¨“ç·´ãƒãƒ¼ãƒ‰<br/>ãƒ©ãƒ™ãƒ«ä»˜ã] --> G[æå¤±è¨ˆç®—<br/>Cross-Entropy]
    E --> G
    G --> H[ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³]

    style A fill:#e3f2fd
    style E fill:#c8e6c9
    style F fill:#fff9c4
    style G fill:#ffccbc
</div>

<h4>åŠæ•™å¸«ã‚ã‚Šå­¦ç¿’ã®åˆ©ç‚¹</h4>

<table>
<thead>
<tr>
<th>ç‰¹å¾´</th>
<th>èª¬æ˜</th>
<th>åˆ©ç‚¹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å°‘é‡ã®ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿</strong></td>
<td>ä¸€éƒ¨ã®ãƒãƒ¼ãƒ‰ã®ã¿ãƒ©ãƒ™ãƒ«ä»˜ã</td>
<td>ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ã‚¹ãƒˆå‰Šæ¸›</td>
</tr>
<tr>
<td><strong>ã‚°ãƒ©ãƒ•æ§‹é€ æ´»ç”¨</strong></td>
<td>éš£æ¥ãƒãƒ¼ãƒ‰ã‹ã‚‰æƒ…å ±ä¼æ’­</td>
<td>ç²¾åº¦å‘ä¸Šã€æ±åŒ–æ€§èƒ½æ”¹å–„</td>
</tr>
<tr>
<td><strong>Transductiveå­¦ç¿’</strong></td>
<td>ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚‚ã‚°ãƒ©ãƒ•ã«å«ã¾ã‚Œã‚‹</td>
<td>ã‚°ãƒ©ãƒ•å…¨ä½“ã®æ§‹é€ ã‚’åˆ©ç”¨å¯èƒ½</td>
</tr>
<tr>
<td><strong>Homophilyæ´»ç”¨</strong></td>
<td>é¡ä¼¼ãƒãƒ¼ãƒ‰ã¯ç¹‹ãŒã‚Šã‚„ã™ã„</td>
<td>ãƒ©ãƒ™ãƒ«ä¼æ’­ãŒåŠ¹æœçš„</td>
</tr>
</tbody>
</table>

<h3>Coraãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã‚‹ãƒãƒ¼ãƒ‰åˆ†é¡å®Ÿè£…</h3>

<p><strong>Coraãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</strong>ã¯ã€æ©Ÿæ¢°å­¦ç¿’è«–æ–‡ã®å¼•ç”¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚å„ãƒãƒ¼ãƒ‰ï¼ˆè«–æ–‡ï¼‰ã¯7ã¤ã®ç ”ç©¶åˆ†é‡ã®ã„ãšã‚Œã‹ã«åˆ†é¡ã•ã‚Œã¾ã™ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.datasets import Planetoid
from torch_geometric.nn import GCNConv, GATConv
from torch_geometric.utils import to_networkx
import matplotlib.pyplot as plt
import networkx as nx
from sklearn.manifold import TSNE
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import numpy as np

class NodeClassificationGCN(nn.Module):
    """
    Graph Convolutional Networkã«ã‚ˆã‚‹ãƒãƒ¼ãƒ‰åˆ†é¡ãƒ¢ãƒ‡ãƒ«

    Architecture:
    - 2-layer GCN
    - Dropout for regularization
    - Log-softmax output for multi-class classification
    """

    def __init__(self, num_features, hidden_dim, num_classes, dropout=0.5):
        """
        Args:
            num_features: å…¥åŠ›ç‰¹å¾´é‡æ¬¡å…ƒ
            hidden_dim: éš ã‚Œå±¤ã®æ¬¡å…ƒ
            num_classes: ã‚¯ãƒ©ã‚¹æ•°
            dropout: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        """
        super(NodeClassificationGCN, self).__init__()

        self.conv1 = GCNConv(num_features, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, num_classes)
        self.dropout = dropout

    def forward(self, data):
        """
        Args:
            data: PyG Data object (x, edge_index)

        Returns:
            log_probs: ãƒãƒ¼ãƒ‰ã”ã¨ã®ã‚¯ãƒ©ã‚¹ç¢ºç‡ï¼ˆlog-spaceï¼‰
        """
        x, edge_index = data.x, data.edge_index

        # ç¬¬1å±¤: GCN + ReLU + Dropout
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=self.training)

        # ç¬¬2å±¤: GCN
        x = self.conv2(x, edge_index)

        return F.log_softmax(x, dim=1)

    def get_embeddings(self, data):
        """
        ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—ï¼ˆå¯è¦–åŒ–ç”¨ï¼‰

        Returns:
            ç¬¬1å±¤ã®GCNå‡ºåŠ›ï¼ˆéš ã‚Œè¡¨ç¾ï¼‰
        """
        x, edge_index = data.x, data.edge_index
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        return x.detach()


class NodeClassificationGAT(nn.Module):
    """
    Graph Attention Networkã«ã‚ˆã‚‹ãƒãƒ¼ãƒ‰åˆ†é¡ãƒ¢ãƒ‡ãƒ«

    Features:
    - Multi-head attention mechanism
    - Learnable attention weights
    - Better handling of heterophilic graphs
    """

    def __init__(self, num_features, hidden_dim, num_classes,
                 heads=8, dropout=0.6):
        """
        Args:
            num_features: å…¥åŠ›ç‰¹å¾´é‡æ¬¡å…ƒ
            hidden_dim: éš ã‚Œå±¤ã®æ¬¡å…ƒï¼ˆå„ãƒ˜ãƒƒãƒ‰ã‚ãŸã‚Šï¼‰
            num_classes: ã‚¯ãƒ©ã‚¹æ•°
            heads: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°
            dropout: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        """
        super(NodeClassificationGAT, self).__init__()

        # ç¬¬1å±¤: Multi-head GAT
        self.conv1 = GATConv(
            num_features,
            hidden_dim,
            heads=heads,
            dropout=dropout
        )

        # ç¬¬2å±¤: Single-head GAT for classification
        self.conv2 = GATConv(
            hidden_dim * heads,  # ç¬¬1å±¤ã®å…¨ãƒ˜ãƒƒãƒ‰å‡ºåŠ›ã‚’çµåˆ
            num_classes,
            heads=1,
            concat=False,
            dropout=dropout
        )

        self.dropout = dropout

    def forward(self, data):
        """
        Args:
            data: PyG Data object

        Returns:
            log_probs: ã‚¯ãƒ©ã‚¹ç¢ºç‡
        """
        x, edge_index = data.x, data.edge_index

        # ç¬¬1å±¤: GAT + ELU + Dropout
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv1(x, edge_index)
        x = F.elu(x)

        # ç¬¬2å±¤: GAT
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)

        return F.log_softmax(x, dim=1)


class NodeClassificationTrainer:
    """
    ãƒãƒ¼ãƒ‰åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ãƒ»è©•ä¾¡ã‚¯ãƒ©ã‚¹
    """

    def __init__(self, model, data, device='cuda'):
        """
        Args:
            model: GNNãƒ¢ãƒ‡ãƒ«ï¼ˆGCN or GATï¼‰
            data: PyG Data object
            device: è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹
        """
        self.device = device if torch.cuda.is_available() else 'cpu'
        self.model = model.to(self.device)
        self.data = data.to(self.device)

        self.optimizer = torch.optim.Adam(
            model.parameters(),
            lr=0.01,
            weight_decay=5e-4
        )

        self.train_losses = []
        self.val_accs = []

    def train_epoch(self):
        """
        1ã‚¨ãƒãƒƒã‚¯ã®è¨“ç·´

        Returns:
            loss: è¨“ç·´æå¤±
        """
        self.model.train()
        self.optimizer.zero_grad()

        # é †ä¼æ’­
        out = self.model(self.data)

        # è¨“ç·´ãƒãƒ¼ãƒ‰ã®ã¿ã§æå¤±è¨ˆç®—ï¼ˆåŠæ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼‰
        loss = F.nll_loss(out[self.data.train_mask], self.data.y[self.data.train_mask])

        # é€†ä¼æ’­
        loss.backward()
        self.optimizer.step()

        return loss.item()

    @torch.no_grad()
    def evaluate(self, mask):
        """
        è©•ä¾¡

        Args:
            mask: è©•ä¾¡å¯¾è±¡ã®ãƒãƒ¼ãƒ‰ãƒã‚¹ã‚¯ï¼ˆtrain/val/testï¼‰

        Returns:
            accuracy: ç²¾åº¦
        """
        self.model.eval()
        out = self.model(self.data)
        pred = out.argmax(dim=1)

        correct = (pred[mask] == self.data.y[mask]).sum()
        acc = int(correct) / int(mask.sum())

        return acc

    def train(self, epochs=200, early_stopping_patience=20, verbose=True):
        """
        è¨“ç·´ãƒ«ãƒ¼ãƒ—

        Args:
            epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            early_stopping_patience: Early stoppingã®è¨±å®¹ã‚¨ãƒãƒƒã‚¯æ•°
            verbose: ãƒ­ã‚°è¡¨ç¤º

        Returns:
            best_val_acc: æœ€è‰¯ã®Validationç²¾åº¦
        """
        best_val_acc = 0
        patience_counter = 0

        for epoch in range(1, epochs + 1):
            # è¨“ç·´
            loss = self.train_epoch()
            self.train_losses.append(loss)

            # è©•ä¾¡
            train_acc = self.evaluate(self.data.train_mask)
            val_acc = self.evaluate(self.data.val_mask)
            self.val_accs.append(val_acc)

            # Early stopping
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
                self.best_model_state = self.model.state_dict()
            else:
                patience_counter += 1

            if patience_counter >= early_stopping_patience:
                if verbose:
                    print(f'Early stopping at epoch {epoch}')
                break

            # ãƒ­ã‚°å‡ºåŠ›
            if verbose and epoch % 10 == 0:
                print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, '
                      f'Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')

        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        self.model.load_state_dict(self.best_model_state)

        return best_val_acc

    @torch.no_grad()
    def test(self):
        """
        ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§æœ€çµ‚è©•ä¾¡

        Returns:
            test_acc: ãƒ†ã‚¹ãƒˆç²¾åº¦
            predictions: å…¨ãƒãƒ¼ãƒ‰ã®äºˆæ¸¬ãƒ©ãƒ™ãƒ«
            embeddings: ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿
        """
        self.model.eval()

        # äºˆæ¸¬
        out = self.model(self.data)
        pred = out.argmax(dim=1)

        # ãƒ†ã‚¹ãƒˆç²¾åº¦
        test_acc = self.evaluate(self.data.test_mask)

        # åŸ‹ã‚è¾¼ã¿å–å¾—ï¼ˆGCNã®å ´åˆï¼‰
        embeddings = None
        if hasattr(self.model, 'get_embeddings'):
            embeddings = self.model.get_embeddings(self.data).cpu().numpy()

        return test_acc, pred.cpu().numpy(), embeddings

    def plot_training_history(self):
        """
        è¨“ç·´å±¥æ­´ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        """
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))

        # æå¤±
        axes[0].plot(self.train_losses, label='Training Loss')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Loss')
        axes[0].set_title('Training Loss over Time')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # Validationç²¾åº¦
        axes[1].plot(self.val_accs, label='Validation Accuracy', color='orange')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('Accuracy')
        axes[1].set_title('Validation Accuracy over Time')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    def visualize_embeddings(self, embeddings, labels, title="Node Embeddings"):
        """
        t-SNEã«ã‚ˆã‚‹åŸ‹ã‚è¾¼ã¿å¯è¦–åŒ–

        Args:
            embeddings: ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿ (N, D)
            labels: ãƒãƒ¼ãƒ‰ãƒ©ãƒ™ãƒ« (N,)
            title: ãƒ—ãƒ­ãƒƒãƒˆã‚¿ã‚¤ãƒˆãƒ«
        """
        # t-SNEã§2æ¬¡å…ƒã«å‰Šæ¸›
        tsne = TSNE(n_components=2, random_state=42)
        embeddings_2d = tsne.fit_transform(embeddings)

        # ãƒ—ãƒ­ãƒƒãƒˆ
        plt.figure(figsize=(10, 8))
        scatter = plt.scatter(
            embeddings_2d[:, 0],
            embeddings_2d[:, 1],
            c=labels,
            cmap='tab10',
            alpha=0.6,
            s=50
        )
        plt.colorbar(scatter, label='Class')
        plt.title(title)
        plt.xlabel('t-SNE Component 1')
        plt.ylabel('t-SNE Component 2')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    dataset = Planetoid(root='/tmp/Cora', name='Cora')
    data = dataset[0]

    print(f"Dataset: {dataset.name}")
    print(f"Number of nodes: {data.num_nodes}")
    print(f"Number of edges: {data.num_edges}")
    print(f"Number of features: {dataset.num_features}")
    print(f"Number of classes: {dataset.num_classes}")
    print(f"Training nodes: {data.train_mask.sum().item()}")
    print(f"Validation nodes: {data.val_mask.sum().item()}")
    print(f"Test nodes: {data.test_mask.sum().item()}")

    # GCNãƒ¢ãƒ‡ãƒ«
    print("\n=== Training GCN ===")
    gcn_model = NodeClassificationGCN(
        num_features=dataset.num_features,
        hidden_dim=16,
        num_classes=dataset.num_classes,
        dropout=0.5
    )

    gcn_trainer = NodeClassificationTrainer(gcn_model, data)
    best_val_acc = gcn_trainer.train(epochs=200, early_stopping_patience=20)

    # ãƒ†ã‚¹ãƒˆè©•ä¾¡
    test_acc, predictions, embeddings = gcn_trainer.test()
    print(f"\nGCN Test Accuracy: {test_acc:.4f}")

    # è¨“ç·´å±¥æ­´ãƒ—ãƒ­ãƒƒãƒˆ
    gcn_trainer.plot_training_history()

    # åŸ‹ã‚è¾¼ã¿å¯è¦–åŒ–
    if embeddings is not None:
        gcn_trainer.visualize_embeddings(
            embeddings,
            data.y.cpu().numpy(),
            title="GCN Node Embeddings (t-SNE)"
        )

    # GATãƒ¢ãƒ‡ãƒ«
    print("\n=== Training GAT ===")
    gat_model = NodeClassificationGAT(
        num_features=dataset.num_features,
        hidden_dim=8,
        num_classes=dataset.num_classes,
        heads=8,
        dropout=0.6
    )

    gat_trainer = NodeClassificationTrainer(gat_model, data)
    best_val_acc = gat_trainer.train(epochs=200, early_stopping_patience=20)

    # ãƒ†ã‚¹ãƒˆè©•ä¾¡
    test_acc, predictions, _ = gat_trainer.test()
    print(f"\nGAT Test Accuracy: {test_acc:.4f}")

    # Confusion matrix
    y_true = data.y[data.test_mask].cpu().numpy()
    y_pred = predictions[data.test_mask.cpu().numpy()]

    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix (GAT on Test Set)')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.show()
</code></pre>

<blockquote>
<p><strong>Transductive vs Inductiveå­¦ç¿’</strong>:</p>
<ul>
<li><strong>Transductive</strong>: ãƒ†ã‚¹ãƒˆãƒãƒ¼ãƒ‰ã‚‚ã‚°ãƒ©ãƒ•ã«å«ã¾ã‚Œã‚‹ã€‚è¨“ç·´æ™‚ã«å…¨ã‚°ãƒ©ãƒ•æ§‹é€ ã‚’åˆ©ç”¨å¯èƒ½ã€‚</li>
<li><strong>Inductive</strong>: ãƒ†ã‚¹ãƒˆãƒãƒ¼ãƒ‰ã¯æœªçŸ¥ã€‚æ–°ã—ã„ã‚°ãƒ©ãƒ•ã¸ã®æ±åŒ–ãŒå¿…è¦ã€‚GraphSAGEãªã©ãŒå¯¾å¿œã€‚</li>
</ul>
</blockquote>

<hr>

<h2>5.2 ã‚°ãƒ©ãƒ•åˆ†é¡ (Graph Classification)</h2>

<h3>åˆ†å­ç‰¹æ€§äºˆæ¸¬ã¸ã®å¿œç”¨</h3>

<p><strong>ã‚°ãƒ©ãƒ•åˆ†é¡</strong>ã¯ã€ã‚°ãƒ©ãƒ•å…¨ä½“ã‚’åˆ†é¡ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚å…¸å‹çš„ãªå¿œç”¨ã¨ã—ã¦ã€åˆ†å­ã®ç”Ÿç‰©æ´»æ€§äºˆæ¸¬ã€ã‚¿ãƒ³ãƒ‘ã‚¯è³ªã®æ©Ÿèƒ½åˆ†é¡ã€ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£åˆ†é¡ãªã©ãŒã‚ã‚Šã¾ã™ã€‚</p>

<div class="mermaid">
graph TB
    A[åˆ†å­ã‚°ãƒ©ãƒ•] --> B[GNN Layers<br/>åŸå­ç‰¹å¾´ä¼æ’­]
    B --> C[ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿]
    C --> D[Graph Pooling<br/>ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«è¡¨ç¾]
    D --> E[åˆ†é¡å™¨<br/>MLP]
    E --> F[åˆ†å­ç‰¹æ€§äºˆæ¸¬<br/>æ¯’æ€§/æ´»æ€§ãªã©]

    style A fill:#e3f2fd
    style D fill:#fff9c4
    style F fill:#c8e6c9
</div>

<h4>Graph Poolingã®ç¨®é¡</h4>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>èª¬æ˜</th>
<th>è¨ˆç®—å¼</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Global Mean Pooling</strong></td>
<td>å…¨ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿ã®å¹³å‡</td>
<td>$\mathbf{h}_G = \frac{1}{|\mathcal{V}|} \sum_{v \in \mathcal{V}} \mathbf{h}_v$</td>
</tr>
<tr>
<td><strong>Global Max Pooling</strong></td>
<td>å„æ¬¡å…ƒã®æœ€å¤§å€¤</td>
<td>$\mathbf{h}_G = \max_{v \in \mathcal{V}} \mathbf{h}_v$</td>
</tr>
<tr>
<td><strong>Global Sum Pooling</strong></td>
<td>å…¨ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿ã®å’Œ</td>
<td>$\mathbf{h}_G = \sum_{v \in \mathcal{V}} \mathbf{h}_v$</td>
</tr>
<tr>
<td><strong>Attention Pooling</strong></td>
<td>ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ä»˜ãå’Œ</td>
<td>$\mathbf{h}_G = \sum_{v \in \mathcal{V}} \alpha_v \mathbf{h}_v$</td>
</tr>
</tbody>
</table>

<h3>MUTAGåˆ†å­ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã‚‹å®Ÿè£…</h3>

<p><strong>MUTAGãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</strong>ã¯ã€188å€‹ã®åŒ–åˆç‰©ã®å¤‰ç•°åŸæ€§ï¼ˆDNAå¤‰ç•°ã‚’å¼•ãèµ·ã“ã™æ€§è³ªï¼‰ã‚’åˆ†é¡ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.datasets import TUDataset
from torch_geometric.loader import DataLoader
from torch_geometric.nn import GCNConv, GINConv, global_mean_pool, global_max_pool, global_add_pool
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support
import matplotlib.pyplot as plt
import numpy as np

class MolecularGCN(nn.Module):
    """
    åˆ†å­ã‚°ãƒ©ãƒ•åˆ†é¡ã®ãŸã‚ã®GCNãƒ¢ãƒ‡ãƒ«

    Architecture:
    - Multiple GCN layers for feature propagation
    - Global pooling for graph-level representation
    - MLP classifier for prediction
    """

    def __init__(self, num_features, hidden_dim, num_classes,
                 num_layers=3, dropout=0.5, pooling='mean'):
        """
        Args:
            num_features: ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡æ¬¡å…ƒ
            hidden_dim: éš ã‚Œå±¤æ¬¡å…ƒ
            num_classes: ã‚¯ãƒ©ã‚¹æ•°
            num_layers: GCNå±¤ã®æ•°
            dropout: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
            pooling: Poolingæ–¹å¼ ('mean', 'max', 'sum')
        """
        super(MolecularGCN, self).__init__()

        # GCNå±¤
        self.convs = nn.ModuleList()
        self.convs.append(GCNConv(num_features, hidden_dim))
        for _ in range(num_layers - 1):
            self.convs.append(GCNConv(hidden_dim, hidden_dim))

        # åˆ†é¡å™¨
        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)
        self.fc2 = nn.Linear(hidden_dim // 2, num_classes)

        self.dropout = dropout
        self.pooling = pooling

    def forward(self, data):
        """
        Args:
            data: PyG Batch object (x, edge_index, batch)

        Returns:
            logits: ã‚°ãƒ©ãƒ•åˆ†é¡ãƒ­ã‚¸ãƒƒãƒˆ
        """
        x, edge_index, batch = data.x, data.edge_index, data.batch

        # GCNå±¤ã§ç‰¹å¾´ä¼æ’­
        for i, conv in enumerate(self.convs):
            x = conv(x, edge_index)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)

        # Graph pooling
        if self.pooling == 'mean':
            x = global_mean_pool(x, batch)
        elif self.pooling == 'max':
            x = global_max_pool(x, batch)
        elif self.pooling == 'sum':
            x = global_add_pool(x, batch)

        # åˆ†é¡å™¨
        x = F.relu(self.fc1(x))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.fc2(x)

        return x


class MolecularGIN(nn.Module):
    """
    Graph Isomorphism Network (GIN) for molecular graphs

    GINã¯ã‚°ãƒ©ãƒ•ã®åŒå‹æ€§ã‚’è­˜åˆ¥ã§ãã‚‹è¡¨ç¾åŠ›ã‚’æŒã¤
    WL testã¨åŒç­‰ã®è­˜åˆ¥èƒ½åŠ›
    """

    def __init__(self, num_features, hidden_dim, num_classes,
                 num_layers=5, dropout=0.5):
        """
        Args:
            num_features: ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡æ¬¡å…ƒ
            hidden_dim: éš ã‚Œå±¤æ¬¡å…ƒ
            num_classes: ã‚¯ãƒ©ã‚¹æ•°
            num_layers: GINå±¤ã®æ•°
            dropout: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        """
        super(MolecularGIN, self).__init__()

        # GINå±¤
        self.convs = nn.ModuleList()
        self.batch_norms = nn.ModuleList()

        # åˆæœŸå±¤
        mlp = nn.Sequential(
            nn.Linear(num_features, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        self.convs.append(GINConv(mlp, train_eps=True))
        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))

        # ä¸­é–“å±¤
        for _ in range(num_layers - 1):
            mlp = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim)
            )
            self.convs.append(GINConv(mlp, train_eps=True))
            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))

        # åˆ†é¡å™¨
        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)
        self.fc2 = nn.Linear(hidden_dim // 2, num_classes)

        self.dropout = dropout

    def forward(self, data):
        """
        Args:
            data: PyG Batch object

        Returns:
            logits: ã‚°ãƒ©ãƒ•åˆ†é¡ãƒ­ã‚¸ãƒƒãƒˆ
        """
        x, edge_index, batch = data.x, data.edge_index, data.batch

        # GINå±¤
        for conv, bn in zip(self.convs, self.batch_norms):
            x = conv(x, edge_index)
            x = bn(x)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)

        # Global sum pooling (GINã®æ¨™æº–)
        x = global_add_pool(x, batch)

        # åˆ†é¡å™¨
        x = F.relu(self.fc1(x))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.fc2(x)

        return x


class GraphClassificationTrainer:
    """
    ã‚°ãƒ©ãƒ•åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ãƒ»è©•ä¾¡ã‚¯ãƒ©ã‚¹
    """

    def __init__(self, model, train_loader, val_loader, test_loader,
                 num_classes, device='cuda'):
        """
        Args:
            model: GNNãƒ¢ãƒ‡ãƒ«
            train_loader, val_loader, test_loader: DataLoader
            num_classes: ã‚¯ãƒ©ã‚¹æ•°
            device: è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹
        """
        self.device = device if torch.cuda.is_available() else 'cpu'
        self.model = model.to(self.device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader
        self.num_classes = num_classes

        self.optimizer = torch.optim.Adam(
            model.parameters(),
            lr=0.001,
            weight_decay=5e-4
        )

        # æå¤±é–¢æ•°
        if num_classes == 2:
            self.criterion = nn.BCEWithLogitsLoss()
        else:
            self.criterion = nn.CrossEntropyLoss()

        self.train_losses = []
        self.val_accs = []

    def train_epoch(self):
        """
        1ã‚¨ãƒãƒƒã‚¯ã®è¨“ç·´

        Returns:
            avg_loss: å¹³å‡æå¤±
        """
        self.model.train()
        total_loss = 0

        for data in self.train_loader:
            data = data.to(self.device)
            self.optimizer.zero_grad()

            # é †ä¼æ’­
            out = self.model(data)

            # æå¤±è¨ˆç®—
            if self.num_classes == 2:
                loss = self.criterion(out.squeeze(), data.y.float())
            else:
                loss = self.criterion(out, data.y)

            # é€†ä¼æ’­
            loss.backward()
            self.optimizer.step()

            total_loss += loss.item() * data.num_graphs

        return total_loss / len(self.train_loader.dataset)

    @torch.no_grad()
    def evaluate(self, loader):
        """
        è©•ä¾¡

        Args:
            loader: DataLoader

        Returns:
            accuracy: ç²¾åº¦
            all_preds, all_labels: äºˆæ¸¬ã¨ãƒ©ãƒ™ãƒ«ï¼ˆROC AUCè¨ˆç®—ç”¨ï¼‰
        """
        self.model.eval()
        correct = 0
        all_preds = []
        all_labels = []

        for data in loader:
            data = data.to(self.device)
            out = self.model(data)

            if self.num_classes == 2:
                pred = (torch.sigmoid(out.squeeze()) > 0.5).long()
                all_preds.extend(torch.sigmoid(out.squeeze()).cpu().numpy())
            else:
                pred = out.argmax(dim=1)
                all_preds.extend(F.softmax(out, dim=1)[:, 1].cpu().numpy())

            correct += (pred == data.y).sum().item()
            all_labels.extend(data.y.cpu().numpy())

        acc = correct / len(loader.dataset)
        return acc, np.array(all_preds), np.array(all_labels)

    def train(self, epochs=100, early_stopping_patience=10, verbose=True):
        """
        è¨“ç·´ãƒ«ãƒ¼ãƒ—

        Args:
            epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            early_stopping_patience: Early stoppingè¨±å®¹ã‚¨ãƒãƒƒã‚¯æ•°
            verbose: ãƒ­ã‚°è¡¨ç¤º

        Returns:
            best_val_acc: æœ€è‰¯ã®Validationç²¾åº¦
        """
        best_val_acc = 0
        patience_counter = 0

        for epoch in range(1, epochs + 1):
            # è¨“ç·´
            loss = self.train_epoch()
            self.train_losses.append(loss)

            # è©•ä¾¡
            train_acc, _, _ = self.evaluate(self.train_loader)
            val_acc, _, _ = self.evaluate(self.val_loader)
            self.val_accs.append(val_acc)

            # Early stopping
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                self.best_model_state = self.model.state_dict()
            else:
                patience_counter += 1

            if patience_counter >= early_stopping_patience:
                if verbose:
                    print(f'Early stopping at epoch {epoch}')
                break

            # ãƒ­ã‚°
            if verbose and epoch % 10 == 0:
                print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, '
                      f'Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')

        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        self.model.load_state_dict(self.best_model_state)
        return best_val_acc

    def test(self):
        """
        ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§æœ€çµ‚è©•ä¾¡

        Returns:
            test_acc: ãƒ†ã‚¹ãƒˆç²¾åº¦
            test_auc: ROC AUCï¼ˆäºŒå€¤åˆ†é¡ã®å ´åˆï¼‰
        """
        test_acc, preds, labels = self.evaluate(self.test_loader)

        # ROC AUCï¼ˆäºŒå€¤åˆ†é¡ã®ã¿ï¼‰
        test_auc = None
        if self.num_classes == 2:
            test_auc = roc_auc_score(labels, preds)

        return test_acc, test_auc


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
    dataset = TUDataset(root='/tmp/MUTAG', name='MUTAG')

    print(f"Dataset: {dataset.name}")
    print(f"Number of graphs: {len(dataset)}")
    print(f"Number of features: {dataset.num_features}")
    print(f"Number of classes: {dataset.num_classes}")

    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    indices = list(range(len(dataset)))
    train_indices, test_indices = train_test_split(
        indices, test_size=0.2, random_state=42, stratify=[data.y.item() for data in dataset]
    )
    train_indices, val_indices = train_test_split(
        train_indices, test_size=0.2, random_state=42, stratify=[dataset[i].y.item() for i in train_indices]
    )

    # DataLoaderä½œæˆ
    train_loader = DataLoader([dataset[i] for i in train_indices], batch_size=32, shuffle=True)
    val_loader = DataLoader([dataset[i] for i in val_indices], batch_size=32)
    test_loader = DataLoader([dataset[i] for i in test_indices], batch_size=32)

    # GCNãƒ¢ãƒ‡ãƒ«
    print("\n=== Training GCN ===")
    gcn_model = MolecularGCN(
        num_features=dataset.num_features,
        hidden_dim=64,
        num_classes=dataset.num_classes,
        num_layers=3,
        dropout=0.5,
        pooling='mean'
    )

    gcn_trainer = GraphClassificationTrainer(
        gcn_model, train_loader, val_loader, test_loader, dataset.num_classes
    )
    gcn_trainer.train(epochs=100, early_stopping_patience=10)

    # ãƒ†ã‚¹ãƒˆè©•ä¾¡
    test_acc, test_auc = gcn_trainer.test()
    print(f"\nGCN Test Accuracy: {test_acc:.4f}")
    if test_auc:
        print(f"GCN Test ROC AUC: {test_auc:.4f}")

    # GINãƒ¢ãƒ‡ãƒ«
    print("\n=== Training GIN ===")
    gin_model = MolecularGIN(
        num_features=dataset.num_features,
        hidden_dim=64,
        num_classes=dataset.num_classes,
        num_layers=5,
        dropout=0.5
    )

    gin_trainer = GraphClassificationTrainer(
        gin_model, train_loader, val_loader, test_loader, dataset.num_classes
    )
    gin_trainer.train(epochs=100, early_stopping_patience=10)

    # ãƒ†ã‚¹ãƒˆè©•ä¾¡
    test_acc, test_auc = gin_trainer.test()
    print(f"\nGIN Test Accuracy: {test_acc:.4f}")
    if test_auc:
        print(f"GIN Test ROC AUC: {test_auc:.4f}")

    # è¨“ç·´å±¥æ­´æ¯”è¼ƒ
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    axes[0].plot(gcn_trainer.train_losses, label='GCN', alpha=0.7)
    axes[0].plot(gin_trainer.train_losses, label='GIN', alpha=0.7)
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('Training Loss Comparison')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    axes[1].plot(gcn_trainer.val_accs, label='GCN', alpha=0.7)
    axes[1].plot(gin_trainer.val_accs, label='GIN', alpha=0.7)
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Accuracy')
    axes[1].set_title('Validation Accuracy Comparison')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()
</code></pre>

<hr>

<h2>5.3 ãƒªãƒ³ã‚¯äºˆæ¸¬ (Link Prediction)</h2>

<h3>æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¸ã®å¿œç”¨</h3>

<p><strong>ãƒªãƒ³ã‚¯äºˆæ¸¬</strong>ã¯ã€ã‚°ãƒ©ãƒ•å†…ã®2ãƒãƒ¼ãƒ‰é–“ã«ã‚¨ãƒƒã‚¸ãŒå­˜åœ¨ã™ã‚‹ç¢ºç‡ã‚’äºˆæ¸¬ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ï¼ˆUser-Itemã‚°ãƒ©ãƒ•ï¼‰ã€çŸ¥è­˜ã‚°ãƒ©ãƒ•è£œå®Œã€ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æãªã©ã«å¿œç”¨ã•ã‚Œã¾ã™ã€‚</p>

<div class="mermaid">
graph TB
    A[ã‚°ãƒ©ãƒ•<br/>ä¸€éƒ¨ã‚¨ãƒƒã‚¸ã‚’éš ã™] --> B[GNN Encoder<br/>ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿å­¦ç¿’]
    B --> C[ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿<br/>u, v]
    C --> D[Link Decoder<br/>ã‚¹ã‚³ã‚¢è¨ˆç®—]
    D --> E[ãƒªãƒ³ã‚¯ç¢ºç‡<br/>p(u,v)]

    F[æ­£ä¾‹ã‚¨ãƒƒã‚¸] --> G[æå¤±è¨ˆç®—<br/>BCE Loss]
    H[è² ä¾‹ã‚¨ãƒƒã‚¸<br/>ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°] --> G
    E --> G
    G --> I[ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°]

    style A fill:#e3f2fd
    style C fill:#fff9c4
    style E fill:#c8e6c9
    style G fill:#ffccbc
</div>

<h4>Link Decoderã®ç¨®é¡</h4>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>è¨ˆç®—å¼</th>
<th>ç‰¹å¾´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Inner Product</strong></td>
<td>$s(u,v) = \mathbf{z}_u^\top \mathbf{z}_v$</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ã€è¨ˆç®—åŠ¹ç‡è‰¯ã„</td>
</tr>
<tr>
<td><strong>Cosine Similarity</strong></td>
<td>$s(u,v) = \frac{\mathbf{z}_u^\top \mathbf{z}_v}{\|\mathbf{z}_u\| \|\mathbf{z}_v\|}$</td>
<td>æ­£è¦åŒ–æ¸ˆã¿ã€ã‚¹ã‚±ãƒ¼ãƒ«ä¸å¤‰</td>
</tr>
<tr>
<td><strong>MLP Decoder</strong></td>
<td>$s(u,v) = \text{MLP}([\mathbf{z}_u; \mathbf{z}_v])$</td>
<td>é«˜è¡¨ç¾åŠ›ã€éç·šå½¢é–¢ä¿‚ãƒ¢ãƒ‡ãƒ«åŒ–</td>
</tr>
<tr>
<td><strong>DistMult</strong></td>
<td>$s(u,r,v) = \mathbf{z}_u^\top \mathbf{R}_r \mathbf{z}_v$</td>
<td>é–¢ä¿‚ã‚¿ã‚¤ãƒ—è€ƒæ…®ï¼ˆçŸ¥è­˜ã‚°ãƒ©ãƒ•ï¼‰</td>
</tr>
</tbody>
</table>

<h3>æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv
from torch_geometric.utils import negative_sampling, train_test_split_edges
from sklearn.metrics import roc_auc_score, average_precision_score
import matplotlib.pyplot as plt
import numpy as np

class LinkPredictionGNN(nn.Module):
    """
    ãƒªãƒ³ã‚¯äºˆæ¸¬ã®ãŸã‚ã®GNNã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼

    GraphSAGEã‚’ä½¿ç”¨ã—ã¦ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿ã‚’å­¦ç¿’
    """

    def __init__(self, num_features, hidden_dim, embedding_dim, num_layers=2, dropout=0.5):
        """
        Args:
            num_features: ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡æ¬¡å…ƒ
            hidden_dim: éš ã‚Œå±¤æ¬¡å…ƒ
            embedding_dim: æœ€çµ‚åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
            num_layers: GraphSAGEå±¤ã®æ•°
            dropout: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        """
        super(LinkPredictionGNN, self).__init__()

        # GraphSAGEå±¤
        self.convs = nn.ModuleList()
        self.convs.append(SAGEConv(num_features, hidden_dim))

        for _ in range(num_layers - 2):
            self.convs.append(SAGEConv(hidden_dim, hidden_dim))

        self.convs.append(SAGEConv(hidden_dim, embedding_dim))

        self.dropout = dropout

    def encode(self, x, edge_index):
        """
        ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰

        Args:
            x: ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡
            edge_index: ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

        Returns:
            z: ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿
        """
        for i, conv in enumerate(self.convs):
            x = conv(x, edge_index)
            if i < len(self.convs) - 1:
                x = F.relu(x)
                x = F.dropout(x, p=self.dropout, training=self.training)

        return x

    def decode(self, z, edge_index, method='inner_product'):
        """
        ã‚¨ãƒƒã‚¸ã‚¹ã‚³ã‚¢ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰

        Args:
            z: ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿
            edge_index: ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            method: ãƒ‡ã‚³ãƒ¼ãƒ‰æ–¹å¼ ('inner_product', 'cosine', 'mlp')

        Returns:
            scores: ã‚¨ãƒƒã‚¸ã‚¹ã‚³ã‚¢
        """
        src, dst = edge_index

        if method == 'inner_product':
            # å†…ç©
            scores = (z[src] * z[dst]).sum(dim=1)
        elif method == 'cosine':
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
            scores = F.cosine_similarity(z[src], z[dst])
        else:
            raise ValueError(f"Unknown decode method: {method}")

        return scores

    def forward(self, x, edge_index, decode_edge_index=None):
        """
        é †ä¼æ’­

        Args:
            x: ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡
            edge_index: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ç”¨ã‚¨ãƒƒã‚¸
            decode_edge_index: ã‚¹ã‚³ã‚¢è¨ˆç®—å¯¾è±¡ã‚¨ãƒƒã‚¸ï¼ˆNoneã®å ´åˆã¯edge_indexï¼‰

        Returns:
            scores: ã‚¨ãƒƒã‚¸ã‚¹ã‚³ã‚¢
        """
        z = self.encode(x, edge_index)

        if decode_edge_index is None:
            decode_edge_index = edge_index

        scores = self.decode(z, decode_edge_index)
        return scores


class MLPDecoder(nn.Module):
    """
    MLPãƒ™ãƒ¼ã‚¹ã®ãƒªãƒ³ã‚¯ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼

    ã‚ˆã‚Šè¤‡é›‘ãªéç·šå½¢é–¢ä¿‚ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–
    """

    def __init__(self, embedding_dim, hidden_dim=128):
        """
        Args:
            embedding_dim: ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
            hidden_dim: MLPéš ã‚Œå±¤æ¬¡å…ƒ
        """
        super(MLPDecoder, self).__init__()

        self.mlp = nn.Sequential(
            nn.Linear(embedding_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_dim // 2, 1)
        )

    def forward(self, z, edge_index):
        """
        Args:
            z: ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿
            edge_index: ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

        Returns:
            scores: ã‚¨ãƒƒã‚¸ã‚¹ã‚³ã‚¢
        """
        src, dst = edge_index

        # ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿ã‚’é€£çµ
        edge_features = torch.cat([z[src], z[dst]], dim=1)

        # MLPã§ã‚¹ã‚³ã‚¢è¨ˆç®—
        scores = self.mlp(edge_features).squeeze()

        return scores


class RecommendationSystem:
    """
    GNNãƒ™ãƒ¼ã‚¹ã®æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ 

    User-Itemã‚°ãƒ©ãƒ•ã§ãƒªãƒ³ã‚¯äºˆæ¸¬ã‚’è¡Œã„ã€ã‚¢ã‚¤ãƒ†ãƒ æ¨è–¦ã‚’å®Ÿç¾
    """

    def __init__(self, model, decoder=None, device='cuda'):
        """
        Args:
            model: GNNã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
            decoder: ãƒªãƒ³ã‚¯ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆNoneã®å ´åˆã¯inner productï¼‰
            device: è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹
        """
        self.device = device if torch.cuda.is_available() else 'cpu'
        self.model = model.to(self.device)
        self.decoder = decoder.to(self.device) if decoder else None

        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        params = list(model.parameters())
        if decoder:
            params += list(decoder.parameters())
        self.optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=1e-5)

        self.train_losses = []
        self.val_aucs = []

    def train_epoch(self, data, train_pos_edge_index):
        """
        1ã‚¨ãƒãƒƒã‚¯ã®è¨“ç·´

        Args:
            data: PyG Data object
            train_pos_edge_index: è¨“ç·´ç”¨æ­£ä¾‹ã‚¨ãƒƒã‚¸

        Returns:
            loss: æå¤±å€¤
        """
        self.model.train()
        if self.decoder:
            self.decoder.train()

        self.optimizer.zero_grad()

        # ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿å–å¾—
        z = self.model.encode(data.x, train_pos_edge_index)

        # è² ä¾‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        neg_edge_index = negative_sampling(
            edge_index=train_pos_edge_index,
            num_nodes=data.num_nodes,
            num_neg_samples=train_pos_edge_index.size(1)
        )

        # æ­£ä¾‹ã‚¹ã‚³ã‚¢
        if self.decoder:
            pos_scores = self.decoder(z, train_pos_edge_index)
            neg_scores = self.decoder(z, neg_edge_index)
        else:
            pos_scores = self.model.decode(z, train_pos_edge_index)
            neg_scores = self.model.decode(z, neg_edge_index)

        # æå¤±è¨ˆç®—ï¼ˆBCE Lossï¼‰
        pos_loss = F.binary_cross_entropy_with_logits(
            pos_scores, torch.ones_like(pos_scores)
        )
        neg_loss = F.binary_cross_entropy_with_logits(
            neg_scores, torch.zeros_like(neg_scores)
        )
        loss = pos_loss + neg_loss

        # é€†ä¼æ’­
        loss.backward()
        self.optimizer.step()

        return loss.item()

    @torch.no_grad()
    def evaluate(self, data, pos_edge_index, neg_edge_index):
        """
        è©•ä¾¡

        Args:
            data: PyG Data object
            pos_edge_index: æ­£ä¾‹ã‚¨ãƒƒã‚¸
            neg_edge_index: è² ä¾‹ã‚¨ãƒƒã‚¸

        Returns:
            auc: ROC AUC
            ap: Average Precision
        """
        self.model.eval()
        if self.decoder:
            self.decoder.eval()

        # å…¨ã‚¨ãƒƒã‚¸ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        z = self.model.encode(data.x, data.train_pos_edge_index)

        # ã‚¹ã‚³ã‚¢è¨ˆç®—
        if self.decoder:
            pos_scores = self.decoder(z, pos_edge_index).cpu().numpy()
            neg_scores = self.decoder(z, neg_edge_index).cpu().numpy()
        else:
            pos_scores = self.model.decode(z, pos_edge_index).cpu().numpy()
            neg_scores = self.model.decode(z, neg_edge_index).cpu().numpy()

        # ãƒ©ãƒ™ãƒ«ã¨ã‚¹ã‚³ã‚¢
        scores = np.concatenate([pos_scores, neg_scores])
        labels = np.concatenate([
            np.ones(pos_scores.shape[0]),
            np.zeros(neg_scores.shape[0])
        ])

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        auc = roc_auc_score(labels, scores)
        ap = average_precision_score(labels, scores)

        return auc, ap

    def train(self, data, epochs=100, early_stopping_patience=10, verbose=True):
        """
        è¨“ç·´ãƒ«ãƒ¼ãƒ—

        Args:
            data: PyG Data object (train_test_split_edgesæ¸ˆã¿)
            epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            early_stopping_patience: Early stoppingè¨±å®¹ã‚¨ãƒãƒƒã‚¯æ•°
            verbose: ãƒ­ã‚°è¡¨ç¤º

        Returns:
            best_val_auc: æœ€è‰¯ã®Validation AUC
        """
        best_val_auc = 0
        patience_counter = 0

        for epoch in range(1, epochs + 1):
            # è¨“ç·´
            loss = self.train_epoch(data, data.train_pos_edge_index)
            self.train_losses.append(loss)

            # è©•ä¾¡
            val_auc, val_ap = self.evaluate(
                data, data.val_pos_edge_index, data.val_neg_edge_index
            )
            self.val_aucs.append(val_auc)

            # Early stopping
            if val_auc > best_val_auc:
                best_val_auc = val_auc
                patience_counter = 0
                self.best_model_state = self.model.state_dict()
                if self.decoder:
                    self.best_decoder_state = self.decoder.state_dict()
            else:
                patience_counter += 1

            if patience_counter >= early_stopping_patience:
                if verbose:
                    print(f'Early stopping at epoch {epoch}')
                break

            # ãƒ­ã‚°
            if verbose and epoch % 10 == 0:
                print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, '
                      f'Val AUC: {val_auc:.4f}, Val AP: {val_ap:.4f}')

        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        self.model.load_state_dict(self.best_model_state)
        if self.decoder:
            self.decoder.load_state_dict(self.best_decoder_state)

        return best_val_auc

    def test(self, data):
        """
        ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§æœ€çµ‚è©•ä¾¡

        Returns:
            test_auc: ROC AUC
            test_ap: Average Precision
        """
        test_auc, test_ap = self.evaluate(
            data, data.test_pos_edge_index, data.test_neg_edge_index
        )
        return test_auc, test_ap

    @torch.no_grad()
    def recommend_items(self, data, user_id, k=10, exclude_known=True):
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ¨è–¦

        Args:
            data: PyG Data object
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒãƒ¼ãƒ‰ID
            k: æ¨è–¦æ•°
            exclude_known: æ—¢çŸ¥ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’é™¤å¤–ã™ã‚‹ã‹

        Returns:
            recommended_items: æ¨è–¦ã‚¢ã‚¤ãƒ†ãƒ IDã®ãƒªã‚¹ãƒˆ
            scores: å¯¾å¿œã™ã‚‹ã‚¹ã‚³ã‚¢
        """
        self.model.eval()
        if self.decoder:
            self.decoder.eval()

        # ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿å–å¾—
        z = self.model.encode(data.x, data.train_pos_edge_index)

        # å…¨ã‚¢ã‚¤ãƒ†ãƒ ã¨ã®ã‚¹ã‚³ã‚¢è¨ˆç®—
        num_nodes = data.num_nodes
        user_embedding = z[user_id].unsqueeze(0).repeat(num_nodes, 1)

        if self.decoder:
            edge_index = torch.stack([
                torch.full((num_nodes,), user_id, dtype=torch.long),
                torch.arange(num_nodes)
            ]).to(self.device)
            scores = self.decoder(z, edge_index)
        else:
            scores = (user_embedding * z).sum(dim=1)

        scores = torch.sigmoid(scores).cpu().numpy()

        # æ—¢çŸ¥ã‚¢ã‚¤ãƒ†ãƒ ã‚’é™¤å¤–
        if exclude_known:
            known_items = data.train_pos_edge_index[1][
                data.train_pos_edge_index[0] == user_id
            ].cpu().numpy()
            scores[known_items] = -1

        # Top-ké¸æŠ
        top_k_indices = np.argsort(scores)[-k:][::-1]
        top_k_scores = scores[top_k_indices]

        return top_k_indices, top_k_scores


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆä¾‹ï¼šCoraãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰
    from torch_geometric.datasets import Planetoid

    dataset = Planetoid(root='/tmp/Cora', name='Cora')
    data = dataset[0]

    # ãƒªãƒ³ã‚¯äºˆæ¸¬ç”¨ã«ã‚¨ãƒƒã‚¸åˆ†å‰²
    data = train_test_split_edges(data, val_ratio=0.1, test_ratio=0.1)

    print(f"Number of nodes: {data.num_nodes}")
    print(f"Training edges: {data.train_pos_edge_index.size(1)}")
    print(f"Validation edges: {data.val_pos_edge_index.size(1)}")
    print(f"Test edges: {data.test_pos_edge_index.size(1)}")

    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆInner Product Decoderï¼‰
    print("\n=== Training with Inner Product Decoder ===")
    model_ip = LinkPredictionGNN(
        num_features=dataset.num_features,
        hidden_dim=128,
        embedding_dim=64,
        num_layers=2,
        dropout=0.5
    )

    rec_system_ip = RecommendationSystem(model_ip)
    rec_system_ip.train(data, epochs=100, early_stopping_patience=10)

    # ãƒ†ã‚¹ãƒˆè©•ä¾¡
    test_auc, test_ap = rec_system_ip.test(data)
    print(f"\nInner Product - Test AUC: {test_auc:.4f}, Test AP: {test_ap:.4f}")

    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆMLP Decoderï¼‰
    print("\n=== Training with MLP Decoder ===")
    model_mlp = LinkPredictionGNN(
        num_features=dataset.num_features,
        hidden_dim=128,
        embedding_dim=64,
        num_layers=2,
        dropout=0.5
    )

    mlp_decoder = MLPDecoder(embedding_dim=64, hidden_dim=128)
    rec_system_mlp = RecommendationSystem(model_mlp, mlp_decoder)
    rec_system_mlp.train(data, epochs=100, early_stopping_patience=10)

    # ãƒ†ã‚¹ãƒˆè©•ä¾¡
    test_auc, test_ap = rec_system_mlp.test(data)
    print(f"\nMLP Decoder - Test AUC: {test_auc:.4f}, Test AP: {test_ap:.4f}")

    # æ¨è–¦ä¾‹
    user_id = 0
    recommended_items, scores = rec_system_mlp.recommend_items(
        data, user_id, k=10, exclude_known=True
    )

    print(f"\nTop-10 Recommendations for User {user_id}:")
    for idx, (item, score) in enumerate(zip(recommended_items, scores), 1):
        print(f"{idx}. Item {item}: Score {score:.4f}")

    # è¨“ç·´å±¥æ­´æ¯”è¼ƒ
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    axes[0].plot(rec_system_ip.train_losses, label='Inner Product', alpha=0.7)
    axes[0].plot(rec_system_mlp.train_losses, label='MLP Decoder', alpha=0.7)
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('Training Loss Comparison')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    axes[1].plot(rec_system_ip.val_aucs, label='Inner Product', alpha=0.7)
    axes[1].plot(rec_system_mlp.val_aucs, label='MLP Decoder', alpha=0.7)
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('ROC AUC')
    axes[1].set_title('Validation AUC Comparison')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()
</code></pre>

<hr>

<h2>5.4 å‰µè–¬å¿œç”¨ (Drug Discovery)</h2>

<h3>è–¬å‰¤-æ¨™çš„ç›¸äº’ä½œç”¨äºˆæ¸¬</h3>

<p><strong>è–¬å‰¤-æ¨™çš„ç›¸äº’ä½œç”¨ï¼ˆDrug-Target Interaction, DTIï¼‰äºˆæ¸¬</strong>ã¯ã€è–¬å‰¤åˆ†å­ãŒã©ã®ã‚¿ãƒ³ãƒ‘ã‚¯è³ªæ¨™çš„ã¨ç›¸äº’ä½œç”¨ã™ã‚‹ã‹ã‚’äºˆæ¸¬ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚å‰µè–¬ã«ãŠã„ã¦é‡è¦ãªå¿œç”¨ã§ã‚ã‚Šã€å€™è£œåŒ–åˆç‰©ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“ã¨ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã¾ã™ã€‚</p>

<div class="mermaid">
graph TB
    A[è–¬å‰¤åˆ†å­ã‚°ãƒ©ãƒ•] --> B[Molecular GNN<br/>è–¬å‰¤åŸ‹ã‚è¾¼ã¿]
    C[æ¨™çš„ã‚¿ãƒ³ãƒ‘ã‚¯è³ª<br/>é…åˆ—/æ§‹é€ ] --> D[Protein Encoder<br/>æ¨™çš„åŸ‹ã‚è¾¼ã¿]

    B --> E[è–¬å‰¤ç‰¹å¾´<br/>z_drug]
    D --> F[æ¨™çš„ç‰¹å¾´<br/>z_target]

    E --> G[Interaction Predictor<br/>MLP/Bilinear]
    F --> G

    G --> H[ç›¸äº’ä½œç”¨ã‚¹ã‚³ã‚¢<br/>Binding Affinity]

    style A fill:#e3f2fd
    style C fill:#fff9c4
    style H fill:#c8e6c9
</div>

<h4>DTIäºˆæ¸¬ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</h4>

<table>
<thead>
<tr>
<th>ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</th>
<th>æ‰‹æ³•</th>
<th>èª¬æ˜</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>è–¬å‰¤ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼</strong></td>
<td>GCN, GIN, AttentiveFP</td>
<td>åˆ†å­ã‚°ãƒ©ãƒ•ã‹ã‚‰ç‰¹å¾´æŠ½å‡º</td>
</tr>
<tr>
<td><strong>æ¨™çš„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼</strong></td>
<td>CNN, LSTM, Transformer</td>
<td>ã‚¢ãƒŸãƒé…¸é…åˆ—/3Dæ§‹é€ ã‹ã‚‰ç‰¹å¾´æŠ½å‡º</td>
</tr>
<tr>
<td><strong>ç›¸äº’ä½œç”¨äºˆæ¸¬</strong></td>
<td>MLP, Bilinear, Attention</td>
<td>è–¬å‰¤-æ¨™çš„ãƒšã‚¢ã®ã‚¹ã‚³ã‚¢è¨ˆç®—</td>
</tr>
<tr>
<td><strong>æå¤±é–¢æ•°</strong></td>
<td>BCE, MSE, Ranking Loss</td>
<td>çµåˆè¦ªå’Œæ€§ã¾ãŸã¯åˆ†é¡ãƒ©ãƒ™ãƒ«</td>
</tr>
</tbody>
</table>

<h3>DTIäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.data import Data, Batch
import numpy as np
from sklearn.metrics import roc_auc_score, precision_recall_curve, auc

class DrugEncoder(nn.Module):
    """
    è–¬å‰¤åˆ†å­ã‚°ãƒ©ãƒ•ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼

    GCNã‚’ä½¿ã£ã¦åˆ†å­ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’å­¦ç¿’
    """

    def __init__(self, num_atom_features, hidden_dim, embedding_dim, num_layers=3):
        """
        Args:
            num_atom_features: åŸå­ç‰¹å¾´é‡æ¬¡å…ƒ
            hidden_dim: éš ã‚Œå±¤æ¬¡å…ƒ
            embedding_dim: æœ€çµ‚åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
            num_layers: GCNå±¤ã®æ•°
        """
        super(DrugEncoder, self).__init__()

        # GCNå±¤
        self.convs = nn.ModuleList()
        self.convs.append(GCNConv(num_atom_features, hidden_dim))

        for _ in range(num_layers - 2):
            self.convs.append(GCNConv(hidden_dim, hidden_dim))

        self.convs.append(GCNConv(hidden_dim, embedding_dim))

        self.batch_norms = nn.ModuleList([
            nn.BatchNorm1d(hidden_dim if i < num_layers - 1 else embedding_dim)
            for i in range(num_layers)
        ])

    def forward(self, data):
        """
        Args:
            data: PyG Batch objectï¼ˆè¤‡æ•°åˆ†å­ã‚°ãƒ©ãƒ•ï¼‰

        Returns:
            drug_embeddings: åˆ†å­åŸ‹ã‚è¾¼ã¿ (batch_size, embedding_dim)
        """
        x, edge_index, batch = data.x, data.edge_index, data.batch

        # GCNå±¤ã§ç‰¹å¾´ä¼æ’­
        for i, (conv, bn) in enumerate(zip(self.convs, self.batch_norms)):
            x = conv(x, edge_index)
            x = bn(x)
            if i < len(self.convs) - 1:
                x = F.relu(x)
                x = F.dropout(x, p=0.2, training=self.training)

        # Graph poolingã§ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«è¡¨ç¾
        drug_embeddings = global_mean_pool(x, batch)

        return drug_embeddings


class ProteinEncoder(nn.Module):
    """
    ã‚¿ãƒ³ãƒ‘ã‚¯è³ªé…åˆ—ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼

    1D CNNã‚’ä½¿ã£ã¦ã‚¢ãƒŸãƒé…¸é…åˆ—ã‹ã‚‰ç‰¹å¾´æŠ½å‡º
    """

    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_filters=128, kernel_sizes=[3, 5, 7]):
        """
        Args:
            vocab_size: ã‚¢ãƒŸãƒé…¸ã®ç¨®é¡æ•°ï¼ˆé€šå¸¸20 + ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
            embedding_dim: ã‚¢ãƒŸãƒé…¸åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
            hidden_dim: æœ€çµ‚éš ã‚Œå±¤æ¬¡å…ƒ
            num_filters: CNNãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ•°
            kernel_sizes: ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚ºã®ãƒªã‚¹ãƒˆ
        """
        super(ProteinEncoder, self).__init__()

        # ã‚¢ãƒŸãƒé…¸åŸ‹ã‚è¾¼ã¿
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)

        # Multi-scale 1D CNN
        self.convs = nn.ModuleList([
            nn.Conv1d(embedding_dim, num_filters, kernel_size=k, padding=k//2)
            for k in kernel_sizes
        ])

        # å…¨çµåˆå±¤
        self.fc = nn.Sequential(
            nn.Linear(num_filters * len(kernel_sizes), hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim)
        )

    def forward(self, protein_sequences):
        """
        Args:
            protein_sequences: ã‚¢ãƒŸãƒé…¸é…åˆ— (batch_size, seq_len)

        Returns:
            protein_embeddings: ã‚¿ãƒ³ãƒ‘ã‚¯è³ªåŸ‹ã‚è¾¼ã¿ (batch_size, hidden_dim)
        """
        # ã‚¢ãƒŸãƒé…¸åŸ‹ã‚è¾¼ã¿
        x = self.embedding(protein_sequences)  # (batch, seq_len, emb_dim)
        x = x.transpose(1, 2)  # (batch, emb_dim, seq_len)

        # Multi-scale CNN
        conv_outputs = []
        for conv in self.convs:
            conv_out = F.relu(conv(x))  # (batch, num_filters, seq_len)
            # Global max pooling
            pooled = F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2)
            conv_outputs.append(pooled)

        # çµåˆ
        x = torch.cat(conv_outputs, dim=1)  # (batch, num_filters * len(kernel_sizes))

        # å…¨çµåˆå±¤
        protein_embeddings = self.fc(x)

        return protein_embeddings


class DTIPredictor(nn.Module):
    """
    è–¬å‰¤-æ¨™çš„ç›¸äº’ä½œç”¨äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«

    è–¬å‰¤ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¨æ¨™çš„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’çµ„ã¿åˆã‚ã›ã¦
    ç›¸äº’ä½œç”¨ã‚¹ã‚³ã‚¢ã‚’äºˆæ¸¬
    """

    def __init__(self, drug_encoder, protein_encoder, hidden_dim):
        """
        Args:
            drug_encoder: DrugEncoder instance
            protein_encoder: ProteinEncoder instance
            hidden_dim: ç›¸äº’ä½œç”¨äºˆæ¸¬ç”¨éš ã‚Œå±¤æ¬¡å…ƒ
        """
        super(DTIPredictor, self).__init__()

        self.drug_encoder = drug_encoder
        self.protein_encoder = protein_encoder

        # ç›¸äº’ä½œç”¨äºˆæ¸¬MLP
        combined_dim = drug_encoder.convs[-1].out_channels + hidden_dim
        self.interaction_mlp = nn.Sequential(
            nn.Linear(combined_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim // 2, 1)
        )

    def forward(self, drug_data, protein_sequences):
        """
        Args:
            drug_data: PyG Batchï¼ˆè–¬å‰¤åˆ†å­ã‚°ãƒ©ãƒ•ï¼‰
            protein_sequences: ã‚¿ãƒ³ãƒ‘ã‚¯è³ªé…åˆ— (batch_size, seq_len)

        Returns:
            interaction_scores: ç›¸äº’ä½œç”¨ã‚¹ã‚³ã‚¢ (batch_size,)
        """
        # è–¬å‰¤åŸ‹ã‚è¾¼ã¿
        drug_emb = self.drug_encoder(drug_data)

        # æ¨™çš„åŸ‹ã‚è¾¼ã¿
        protein_emb = self.protein_encoder(protein_sequences)

        # çµåˆ
        combined = torch.cat([drug_emb, protein_emb], dim=1)

        # ç›¸äº’ä½œç”¨ã‚¹ã‚³ã‚¢
        scores = self.interaction_mlp(combined).squeeze()

        return scores


class DTIPredictionSystem:
    """
    è–¬å‰¤-æ¨™çš„ç›¸äº’ä½œç”¨äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
    """

    def __init__(self, model, device='cuda'):
        """
        Args:
            model: DTIPredictor instance
            device: è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹
        """
        self.device = device if torch.cuda.is_available() else 'cpu'
        self.model = model.to(self.device)

        self.optimizer = torch.optim.Adam(
            model.parameters(),
            lr=0.0001,
            weight_decay=1e-5
        )

        self.criterion = nn.BCEWithLogitsLoss()

        self.train_losses = []
        self.val_aucs = []

    def train_epoch(self, train_loader):
        """
        1ã‚¨ãƒãƒƒã‚¯ã®è¨“ç·´

        Args:
            train_loader: DataLoader (drug_data, protein_seq, label)

        Returns:
            avg_loss: å¹³å‡æå¤±
        """
        self.model.train()
        total_loss = 0

        for drug_batch, protein_batch, labels in train_loader:
            # ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€
            protein_batch = protein_batch.to(self.device)
            labels = labels.to(self.device).float()

            self.optimizer.zero_grad()

            # é †ä¼æ’­
            scores = self.model(drug_batch, protein_batch)

            # æå¤±è¨ˆç®—
            loss = self.criterion(scores, labels)

            # é€†ä¼æ’­
            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()

        return total_loss / len(train_loader)

    @torch.no_grad()
    def evaluate(self, loader):
        """
        è©•ä¾¡

        Args:
            loader: DataLoader

        Returns:
            auc: ROC AUC
            auprc: Area Under Precision-Recall Curve
        """
        self.model.eval()
        all_scores = []
        all_labels = []

        for drug_batch, protein_batch, labels in loader:
            protein_batch = protein_batch.to(self.device)
            labels = labels.to(self.device).float()

            scores = self.model(drug_batch, protein_batch)
            scores = torch.sigmoid(scores)

            all_scores.extend(scores.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

        all_scores = np.array(all_scores)
        all_labels = np.array(all_labels)

        # ROC AUC
        auc_score = roc_auc_score(all_labels, all_scores)

        # AUPRC
        precision, recall, _ = precision_recall_curve(all_labels, all_scores)
        auprc = auc(recall, precision)

        return auc_score, auprc

    def train(self, train_loader, val_loader, epochs=50,
              early_stopping_patience=10, verbose=True):
        """
        è¨“ç·´ãƒ«ãƒ¼ãƒ—

        Args:
            train_loader, val_loader: DataLoader
            epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            early_stopping_patience: Early stoppingè¨±å®¹ã‚¨ãƒãƒƒã‚¯æ•°
            verbose: ãƒ­ã‚°è¡¨ç¤º

        Returns:
            best_val_auc: æœ€è‰¯ã®Validation AUC
        """
        best_val_auc = 0
        patience_counter = 0

        for epoch in range(1, epochs + 1):
            # è¨“ç·´
            loss = self.train_epoch(train_loader)
            self.train_losses.append(loss)

            # è©•ä¾¡
            val_auc, val_auprc = self.evaluate(val_loader)
            self.val_aucs.append(val_auc)

            # Early stopping
            if val_auc > best_val_auc:
                best_val_auc = val_auc
                patience_counter = 0
                self.best_model_state = self.model.state_dict()
            else:
                patience_counter += 1

            if patience_counter >= early_stopping_patience:
                if verbose:
                    print(f'Early stopping at epoch {epoch}')
                break

            # ãƒ­ã‚°
            if verbose and epoch % 5 == 0:
                print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, '
                      f'Val AUC: {val_auc:.4f}, Val AUPRC: {val_auprc:.4f}')

        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        self.model.load_state_dict(self.best_model_state)
        return best_val_auc

    def test(self, test_loader):
        """
        ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§æœ€çµ‚è©•ä¾¡

        Returns:
            test_auc: ROC AUC
            test_auprc: AUPRC
        """
        return self.evaluate(test_loader)

    def predict_interaction(self, drug_graph, protein_sequence):
        """
        å˜ä¸€ã®è–¬å‰¤-æ¨™çš„ãƒšã‚¢ã®ç›¸äº’ä½œç”¨ã‚’äºˆæ¸¬

        Args:
            drug_graph: PyG Data objectï¼ˆåˆ†å­ã‚°ãƒ©ãƒ•ï¼‰
            protein_sequence: ã‚¢ãƒŸãƒé…¸é…åˆ—ãƒ†ãƒ³ã‚½ãƒ«

        Returns:
            interaction_prob: ç›¸äº’ä½œç”¨ç¢ºç‡
        """
        self.model.eval()

        with torch.no_grad():
            # ãƒãƒƒãƒåŒ–
            drug_batch = Batch.from_data_list([drug_graph]).to(self.device)
            protein_batch = protein_sequence.unsqueeze(0).to(self.device)

            # äºˆæ¸¬
            score = self.model(drug_batch, protein_batch)
            prob = torch.sigmoid(score).item()

        return prob


# ä½¿ç”¨ä¾‹ï¼ˆãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼‰
if __name__ == "__main__":
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    def create_dummy_molecule():
        """ãƒ€ãƒŸãƒ¼åˆ†å­ã‚°ãƒ©ãƒ•ç”Ÿæˆ"""
        num_atoms = np.random.randint(10, 30)
        x = torch.randn(num_atoms, 9)  # åŸå­ç‰¹å¾´é‡
        edge_index = torch.randint(0, num_atoms, (2, num_atoms * 2))
        return Data(x=x, edge_index=edge_index)

    def create_dummy_protein(max_len=1000):
        """ãƒ€ãƒŸãƒ¼ã‚¿ãƒ³ãƒ‘ã‚¯è³ªé…åˆ—ç”Ÿæˆ"""
        seq_len = np.random.randint(500, max_len)
        # ã‚¢ãƒŸãƒé…¸ID (1-20, 0ã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°)
        seq = torch.randint(1, 21, (seq_len,))
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        if seq_len < max_len:
            padding = torch.zeros(max_len - seq_len, dtype=torch.long)
            seq = torch.cat([seq, padding])
        return seq

    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    num_samples = 1000
    drug_graphs = [create_dummy_molecule() for _ in range(num_samples)]
    protein_seqs = torch.stack([create_dummy_protein() for _ in range(num_samples)])
    labels = torch.randint(0, 2, (num_samples,))  # ãƒ©ãƒ³ãƒ€ãƒ ãƒ©ãƒ™ãƒ«

    # DataLoaderä½œæˆï¼ˆç°¡ç•¥ç‰ˆï¼‰
    class DTIDataset(torch.utils.data.Dataset):
        def __init__(self, drug_graphs, protein_seqs, labels):
            self.drug_graphs = drug_graphs
            self.protein_seqs = protein_seqs
            self.labels = labels

        def __len__(self):
            return len(self.labels)

        def __getitem__(self, idx):
            return self.drug_graphs[idx], self.protein_seqs[idx], self.labels[idx]

    def collate_fn(batch):
        drugs, proteins, labels = zip(*batch)
        drug_batch = Batch.from_data_list(drugs)
        protein_batch = torch.stack(proteins)
        label_batch = torch.tensor(labels)
        return drug_batch, protein_batch, label_batch

    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    train_size = int(0.7 * num_samples)
    val_size = int(0.15 * num_samples)

    train_dataset = DTIDataset(
        drug_graphs[:train_size],
        protein_seqs[:train_size],
        labels[:train_size]
    )
    val_dataset = DTIDataset(
        drug_graphs[train_size:train_size+val_size],
        protein_seqs[train_size:train_size+val_size],
        labels[train_size:train_size+val_size]
    )
    test_dataset = DTIDataset(
        drug_graphs[train_size+val_size:],
        protein_seqs[train_size+val_size:],
        labels[train_size+val_size:]
    )

    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn
    )
    val_loader = torch.utils.data.DataLoader(
        val_dataset, batch_size=32, collate_fn=collate_fn
    )
    test_loader = torch.utils.data.DataLoader(
        test_dataset, batch_size=32, collate_fn=collate_fn
    )

    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    drug_encoder = DrugEncoder(
        num_atom_features=9,
        hidden_dim=128,
        embedding_dim=128,
        num_layers=3
    )

    protein_encoder = ProteinEncoder(
        vocab_size=21,  # 20 amino acids + padding
        embedding_dim=128,
        hidden_dim=128,
        num_filters=128,
        kernel_sizes=[3, 5, 7, 9]
    )

    dti_model = DTIPredictor(
        drug_encoder=drug_encoder,
        protein_encoder=protein_encoder,
        hidden_dim=128
    )

    # è¨“ç·´
    print("=== Training DTI Prediction Model ===")
    dti_system = DTIPredictionSystem(dti_model)
    best_val_auc = dti_system.train(
        train_loader, val_loader, epochs=50, early_stopping_patience=10
    )

    # ãƒ†ã‚¹ãƒˆè©•ä¾¡
    test_auc, test_auprc = dti_system.test(test_loader)
    print(f"\nTest AUC: {test_auc:.4f}")
    print(f"Test AUPRC: {test_auprc:.4f}")

    # è¨“ç·´å±¥æ­´ãƒ—ãƒ­ãƒƒãƒˆ
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    axes[0].plot(dti_system.train_losses)
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('Training Loss')
    axes[0].grid(True, alpha=0.3)

    axes[1].plot(dti_system.val_aucs)
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('ROC AUC')
    axes[1].set_title('Validation AUC')
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # å˜ä¸€äºˆæ¸¬ä¾‹
    test_drug = create_dummy_molecule()
    test_protein = create_dummy_protein()

    interaction_prob = dti_system.predict_interaction(test_drug, test_protein)
    print(f"\nPredicted interaction probability: {interaction_prob:.4f}")
</code></pre>

<blockquote>
<p><strong>å‰µè–¬ã«ãŠã‘ã‚‹GNNã®åˆ©ç‚¹</strong>:</p>
<ul>
<li><strong>åˆ†å­æ§‹é€ ã®ç›´æ¥ãƒ¢ãƒ‡ãƒ«åŒ–</strong>: ã‚°ãƒ©ãƒ•è¡¨ç¾ã«ã‚ˆã‚ŠåŸå­é–“ã®çµåˆé–¢ä¿‚ã‚’è‡ªç„¶ã«æ‰±ãˆã‚‹</li>
<li><strong>è»¢ç§»å­¦ç¿’</strong>: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’ã—ã€å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</li>
<li><strong>è§£é‡ˆå¯èƒ½æ€§</strong>: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹ã«ã‚ˆã‚Šé‡è¦ãªéƒ¨åˆ†æ§‹é€ ã‚’ç‰¹å®šå¯èƒ½</li>
<li><strong>é«˜é€Ÿã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°</strong>: æ•°ç™¾ä¸‡ã®åŒ–åˆç‰©ã‚’çŸ­æ™‚é–“ã§è©•ä¾¡å¯èƒ½</li>
</ul>
</blockquote>

<hr>

<h2>5.5 ãã®ä»–ã®å¿œç”¨</h2>

<h3>çŸ¥è­˜ã‚°ãƒ©ãƒ• (Knowledge Graphs)</h3>

<p><strong>çŸ¥è­˜ã‚°ãƒ©ãƒ•</strong>ã¯ã€ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ï¼ˆãƒãƒ¼ãƒ‰ï¼‰ã¨ãã®é–¢ä¿‚ï¼ˆã‚¨ãƒƒã‚¸ï¼‰ã‚’è¡¨ç¾ã—ãŸã‚°ãƒ©ãƒ•ã§ã™ã€‚GNNã¯çŸ¥è­˜ã‚°ãƒ©ãƒ•è£œå®Œï¼ˆmissing link predictionï¼‰ã€è³ªå•å¿œç­”ã€æ¨è«–ã‚¿ã‚¹ã‚¯ã«å¿œç”¨ã•ã‚Œã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[Entity: Barack Obama] -->|born_in| B[Entity: Hawaii]
    A -->|president_of| C[Entity: USA]
    C -->|capital| D[Entity: Washington D.C.]
    A -->|married_to| E[Entity: Michelle Obama]
    E -->|born_in| F[Entity: Chicago]

    style A fill:#e3f2fd
    style B fill:#c8e6c9
    style C fill:#fff9c4
    style D fill:#ffccbc
    style E fill:#f8bbd0
    style F fill:#c8e6c9
</div>

<h4>çŸ¥è­˜ã‚°ãƒ©ãƒ•è£œå®Œã®å®Ÿè£…ä¾‹</h4>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import RGCNConv

class KnowledgeGraphCompletion(nn.Module):
    """
    Relational GCN (R-GCN) ã«ã‚ˆã‚‹çŸ¥è­˜ã‚°ãƒ©ãƒ•è£œå®Œ

    ç•°ãªã‚‹é–¢ä¿‚ã‚¿ã‚¤ãƒ—ã”ã¨ã«ç•°ãªã‚‹é‡ã¿è¡Œåˆ—ã‚’ä½¿ç”¨
    """

    def __init__(self, num_entities, num_relations, hidden_dim, num_layers=2):
        """
        Args:
            num_entities: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ•°
            num_relations: é–¢ä¿‚ã‚¿ã‚¤ãƒ—æ•°
            hidden_dim: éš ã‚Œå±¤æ¬¡å…ƒ
            num_layers: R-GCNå±¤ã®æ•°
        """
        super(KnowledgeGraphCompletion, self).__init__()

        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åŸ‹ã‚è¾¼ã¿
        self.entity_embedding = nn.Embedding(num_entities, hidden_dim)

        # R-GCNå±¤
        self.convs = nn.ModuleList()
        for _ in range(num_layers):
            self.convs.append(
                RGCNConv(hidden_dim, hidden_dim, num_relations=num_relations)
            )

        # é–¢ä¿‚åŸ‹ã‚è¾¼ã¿ï¼ˆãƒªãƒ³ã‚¯äºˆæ¸¬ç”¨ï¼‰
        self.relation_embedding = nn.Embedding(num_relations, hidden_dim)

    def forward(self, edge_index, edge_type):
        """
        Args:
            edge_index: ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            edge_type: ã‚¨ãƒƒã‚¸ã‚¿ã‚¤ãƒ—ï¼ˆé–¢ä¿‚IDï¼‰

        Returns:
            entity_embeddings: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åŸ‹ã‚è¾¼ã¿
        """
        x = self.entity_embedding.weight

        # R-GCNå±¤ã§ç‰¹å¾´ä¼æ’­
        for conv in self.convs:
            x = conv(x, edge_index, edge_type)
            x = F.relu(x)

        return x

    def score_triple(self, head, relation, tail, entity_embeddings):
        """
        ãƒˆãƒªãƒ—ãƒ« (head, relation, tail) ã®ã‚¹ã‚³ã‚¢è¨ˆç®—

        DistMultã‚¹ã‚³ã‚¢é–¢æ•°ã‚’ä½¿ç”¨:
        score(h, r, t) = h^T R_r t

        Args:
            head: ãƒ˜ãƒƒãƒ‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ID
            relation: é–¢ä¿‚ID
            tail: ãƒ†ãƒ¼ãƒ«ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ID
            entity_embeddings: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åŸ‹ã‚è¾¼ã¿

        Returns:
            scores: ãƒˆãƒªãƒ—ãƒ«ã‚¹ã‚³ã‚¢
        """
        h_emb = entity_embeddings[head]
        r_emb = self.relation_embedding(relation)
        t_emb = entity_embeddings[tail]

        # DistMult: element-wise product
        scores = (h_emb * r_emb * t_emb).sum(dim=1)

        return scores

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ€ãƒŸãƒ¼çŸ¥è­˜ã‚°ãƒ©ãƒ•
    num_entities = 100
    num_relations = 10

    # ãƒ€ãƒŸãƒ¼ã‚¨ãƒƒã‚¸
    edge_index = torch.randint(0, num_entities, (2, 500))
    edge_type = torch.randint(0, num_relations, (500,))

    # ãƒ¢ãƒ‡ãƒ«
    kg_model = KnowledgeGraphCompletion(
        num_entities=num_entities,
        num_relations=num_relations,
        hidden_dim=64,
        num_layers=2
    )

    # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åŸ‹ã‚è¾¼ã¿å–å¾—
    entity_emb = kg_model(edge_index, edge_type)

    # ãƒˆãƒªãƒ—ãƒ«ã‚¹ã‚³ã‚¢è¨ˆç®—ä¾‹
    head = torch.tensor([0, 1, 2])
    relation = torch.tensor([0, 1, 2])
    tail = torch.tensor([10, 11, 12])

    scores = kg_model.score_triple(head, relation, tail, entity_emb)
    print(f"Triple scores: {scores}")
</code></pre>

<h3>äº¤é€šãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (Traffic Networks)</h3>

<p><strong>äº¤é€šãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</strong>ã§ã¯ã€é“è·¯ã‚„äº¤å·®ç‚¹ã‚’ãƒãƒ¼ãƒ‰ã€é“è·¯ã‚’ã‚¨ãƒƒã‚¸ã¨ã—ã¦ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¾ã™ã€‚GNNã¯äº¤é€šæµäºˆæ¸¬ã€æ¸‹æ»äºˆæ¸¬ã€æœ€é©çµŒè·¯æ¢ç´¢ã«å¿œç”¨ã•ã‚Œã¾ã™ã€‚</p>

<h4>æ™‚ç©ºé–“ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (Spatial-Temporal GNN)</h4>

<p>äº¤é€šäºˆæ¸¬ã§ã¯ç©ºé–“çš„ä¾å­˜é–¢ä¿‚ï¼ˆé“è·¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰ã¨æ™‚é–“çš„ä¾å­˜é–¢ä¿‚ï¼ˆæ™‚ç³»åˆ—ï¼‰ã®ä¸¡æ–¹ã‚’è€ƒæ…®ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv

class SpatialTemporalGNN(nn.Module):
    """
    æ™‚ç©ºé–“ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

    GCNï¼ˆç©ºé–“ï¼‰+ LSTMï¼ˆæ™‚é–“ï¼‰ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰
    """

    def __init__(self, num_nodes, node_features, hidden_dim, output_dim, num_timesteps):
        """
        Args:
            num_nodes: ãƒãƒ¼ãƒ‰æ•°ï¼ˆäº¤å·®ç‚¹/ã‚»ãƒ³ã‚µãƒ¼æ•°ï¼‰
            node_features: ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡æ¬¡å…ƒ
            hidden_dim: éš ã‚Œå±¤æ¬¡å…ƒ
            output_dim: å‡ºåŠ›æ¬¡å…ƒï¼ˆäºˆæ¸¬å¯¾è±¡ã€ä¾‹: äº¤é€šé‡ï¼‰
            num_timesteps: æ™‚ç³»åˆ—é•·
        """
        super(SpatialTemporalGNN, self).__init__()

        # ç©ºé–“ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆGCNï¼‰
        self.gcn1 = GCNConv(node_features, hidden_dim)
        self.gcn2 = GCNConv(hidden_dim, hidden_dim)

        # æ™‚é–“ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆLSTMï¼‰
        self.lstm = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            dropout=0.2
        )

        # å‡ºåŠ›å±¤
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x_seq, edge_index):
        """
        Args:
            x_seq: æ™‚ç³»åˆ—ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ (batch, num_timesteps, num_nodes, features)
            edge_index: ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆé™çš„ã‚°ãƒ©ãƒ•æ§‹é€ ï¼‰

        Returns:
            predictions: äºˆæ¸¬å€¤ (batch, num_nodes, output_dim)
        """
        batch_size, num_timesteps, num_nodes, _ = x_seq.size()

        # å„æ™‚åˆ»ã§GCNã‚’é©ç”¨
        spatial_features = []
        for t in range(num_timesteps):
            x_t = x_seq[:, t, :, :].reshape(-1, x_seq.size(-1))  # (batch*nodes, features)

            # GCNå±¤
            x_t = self.gcn1(x_t, edge_index)
            x_t = F.relu(x_t)
            x_t = self.gcn2(x_t, edge_index)
            x_t = F.relu(x_t)

            # (batch, nodes, hidden_dim)ã«å¾©å…ƒ
            x_t = x_t.view(batch_size, num_nodes, -1)
            spatial_features.append(x_t)

        # (batch, num_timesteps, nodes, hidden_dim)
        spatial_features = torch.stack(spatial_features, dim=1)

        # ãƒãƒ¼ãƒ‰ã”ã¨ã«LSTMã‚’é©ç”¨
        predictions = []
        for node_idx in range(num_nodes):
            node_seq = spatial_features[:, :, node_idx, :]  # (batch, timesteps, hidden)

            # LSTM
            lstm_out, _ = self.lstm(node_seq)  # (batch, timesteps, hidden)

            # æœ€å¾Œã®æ™‚åˆ»ã®å‡ºåŠ›
            last_output = lstm_out[:, -1, :]  # (batch, hidden)

            # äºˆæ¸¬
            pred = self.fc(last_output)  # (batch, output_dim)
            predictions.append(pred)

        # (batch, nodes, output_dim)
        predictions = torch.stack(predictions, dim=1)

        return predictions

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ€ãƒŸãƒ¼äº¤é€šãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    num_nodes = 50  # äº¤å·®ç‚¹æ•°
    node_features = 4  # ç‰¹å¾´é‡ï¼ˆé€Ÿåº¦ã€å¯†åº¦ã€æµé‡ãªã©ï¼‰
    num_timesteps = 12  # éå»12æ™‚é–“

    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
    batch_size = 8
    x_seq = torch.randn(batch_size, num_timesteps, num_nodes, node_features)
    edge_index = torch.randint(0, num_nodes, (2, num_nodes * 3))

    # ãƒ¢ãƒ‡ãƒ«
    st_gnn = SpatialTemporalGNN(
        num_nodes=num_nodes,
        node_features=node_features,
        hidden_dim=64,
        output_dim=1,  # äº¤é€šé‡äºˆæ¸¬
        num_timesteps=num_timesteps
    )

    # äºˆæ¸¬
    predictions = st_gnn(x_seq, edge_index)
    print(f"Predictions shape: {predictions.shape}")  # (batch, nodes, 1)
</code></pre>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<details>
<summary><strong>æ¼”ç¿’1: ãƒãƒ¼ãƒ‰åˆ†é¡ã®æ”¹å–„</strong></summary>

<p><strong>èª²é¡Œ</strong>: NodeClassificationGCNãƒ¢ãƒ‡ãƒ«ã‚’æ”¹å–„ã—ã€ã‚ˆã‚Šé«˜ã„ç²¾åº¦ã‚’é”æˆã—ã¦ãã ã•ã„ã€‚</p>

<p><strong>æ”¹å–„æ¡ˆ</strong>:</p>
<ol>
<li><strong>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å¤‰æ›´</strong>: ã‚ˆã‚Šæ·±ã„å±¤ã€Residualæ¥ç¶šã€Attentionæ©Ÿæ§‹ã®è¿½åŠ </li>
<li><strong>æ­£å‰‡åŒ–å¼·åŒ–</strong>: Dropoutã®èª¿æ•´ã€L2æ­£å‰‡åŒ–ã€DropEdge</li>
<li><strong>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</strong>: å­¦ç¿’ç‡ã€éš ã‚Œå±¤æ¬¡å…ƒã€å±¤æ•°ã®èª¿æ•´</li>
<li><strong>ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ</strong>: ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ã®æ­£è¦åŒ–ã€ã‚°ãƒ©ãƒ•æ‹¡å¼µæŠ€è¡“</li>
</ol>

<p><strong>è©•ä¾¡åŸºæº–</strong>: Coraãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ†ã‚¹ãƒˆç²¾åº¦85%ä»¥ä¸Šã‚’ç›®æŒ‡ã™</p>
</details>

<details>
<summary><strong>æ¼”ç¿’2: åˆ†å­ç‰¹æ€§äºˆæ¸¬ã®æ‹¡å¼µ</strong></summary>

<p><strong>èª²é¡Œ</strong>: MolecularGCNãƒ¢ãƒ‡ãƒ«ã‚’æ‹¡å¼µã—ã€è¤‡æ•°ã®åˆ†å­ç‰¹æ€§ã‚’åŒæ™‚ã«äºˆæ¸¬ã™ã‚‹ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>

<p><strong>ä»•æ§˜</strong>:</p>
<ul>
<li>è¤‡æ•°ã®å‡ºåŠ›ãƒ˜ãƒƒãƒ‰ï¼ˆæ¯’æ€§ã€æº¶è§£åº¦ã€æ´»æ€§ãªã©ï¼‰</li>
<li>ã‚¿ã‚¹ã‚¯é–“ã®å…±æœ‰è¡¨ç¾å­¦ç¿’</li>
<li>ã‚¿ã‚¹ã‚¯åˆ¥ã®æå¤±é‡ã¿ä»˜ã‘</li>
<li>å„ã‚¿ã‚¹ã‚¯ã®è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆAUC, RMSEï¼‰</li>
</ul>

<p><strong>ãƒ’ãƒ³ãƒˆ</strong>: å…±æœ‰GCNå±¤ + ã‚¿ã‚¹ã‚¯åˆ¥MLPåˆ†é¡å™¨ã®æ§‹æˆã‚’æ¤œè¨</p>
</details>

<details>
<summary><strong>æ¼”ç¿’3: æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®Cold-Startå•é¡Œ</strong></summary>

<p><strong>èª²é¡Œ</strong>: ãƒªãƒ³ã‚¯äºˆæ¸¬ãƒ™ãƒ¼ã‚¹ã®æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹Cold-Startå•é¡Œï¼ˆæ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼/ã‚¢ã‚¤ãƒ†ãƒ ï¼‰ã¸ã®å¯¾å‡¦æ³•ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>

<p><strong>å®Ÿè£…é …ç›®</strong>:</p>
<ol>
<li><strong>Side Informationæ´»ç”¨</strong>: ãƒ¦ãƒ¼ã‚¶ãƒ¼/ã‚¢ã‚¤ãƒ†ãƒ ã®å±æ€§æƒ…å ±ã‚’ç‰¹å¾´é‡ã¨ã—ã¦ä½¿ç”¨</li>
<li><strong>Inductiveå­¦ç¿’</strong>: GraphSAGEãªã©ã§æ–°è¦ãƒãƒ¼ãƒ‰ã«å¯¾å¿œ</li>
<li><strong>Content-based Filteringçµ±åˆ</strong>: GNNæ¨è–¦ã¨Content-basedã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰</li>
<li><strong>Meta-learning</strong>: Few-shot learningã§æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ç´ æ—©ãé©å¿œ</li>
</ol>

<p><strong>è©•ä¾¡</strong>: æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼/ã‚¢ã‚¤ãƒ†ãƒ ã§ã®æ¨è–¦ç²¾åº¦ã‚’æ¸¬å®š</p>
</details>

<details>
<summary><strong>æ¼”ç¿’4: DTIäºˆæ¸¬ã®è§£é‡ˆå¯èƒ½æ€§</strong></summary>

<p><strong>èª²é¡Œ</strong>: DTIPredictorãƒ¢ãƒ‡ãƒ«ã«è§£é‡ˆå¯èƒ½æ€§æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚</p>

<p><strong>å®Ÿè£…å†…å®¹</strong>:</p>
<ul>
<li><strong>ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å¯è¦–åŒ–</strong>: é‡è¦ãªåŸå­/éƒ¨åˆ†æ§‹é€ ã‚’ç‰¹å®š</li>
<li><strong>GradCAMé©ç”¨</strong>: äºˆæ¸¬ã«é‡è¦ãªé ˜åŸŸã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§è¡¨ç¤º</li>
<li><strong>éƒ¨åˆ†æ§‹é€ åˆ†æ</strong>: ç›¸äº’ä½œç”¨ã«å¯„ä¸ã™ã‚‹åˆ†å­æ–­ç‰‡ã‚’æŠ½å‡º</li>
<li><strong>ã‚¢ãƒŸãƒé…¸é‡è¦åº¦</strong>: çµåˆéƒ¨ä½ã®ç‰¹å®š</li>
</ul>

<p><strong>å‡ºåŠ›</strong>: åˆ†å­ã‚°ãƒ©ãƒ•ä¸Šã®é‡è¦åº¦ã‚¹ã‚³ã‚¢ã‚’å¯è¦–åŒ–</p>
</details>

<details>
<summary><strong>æ¼”ç¿’5: æ™‚ç©ºé–“ã‚°ãƒ©ãƒ•äºˆæ¸¬ã®å®Ÿè£…</strong></summary>

<p><strong>èª²é¡Œ</strong>: å®Ÿéš›ã®äº¤é€šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆä¾‹: METR-LAï¼‰ã‚’ä½¿ã£ã¦æ™‚ç©ºé–“GNNã«ã‚ˆã‚‹äº¤é€šé‡äºˆæ¸¬ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>

<p><strong>è¦ä»¶</strong>:</p>
<ol>
<li><strong>ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†</strong>: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–ã€æ¬ æå€¤å‡¦ç†</li>
<li><strong>ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰</strong>: GCN + LSTM/GRU/Transformerã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰</li>
<li><strong>ãƒãƒ«ãƒã‚¹ãƒ†ãƒƒãƒ—äºˆæ¸¬</strong>: 1æ™‚é–“å…ˆã€3æ™‚é–“å…ˆã€6æ™‚é–“å…ˆã‚’äºˆæ¸¬</li>
<li><strong>è©•ä¾¡</strong>: MAE, RMSE, MAPEã§æ€§èƒ½è©•ä¾¡</li>
</ol>

<p><strong>ç™ºå±•èª²é¡Œ</strong>: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹ã«ã‚ˆã‚‹å‹•çš„ã‚°ãƒ©ãƒ•æ§‹é€ å­¦ç¿’ã‚’å®Ÿè£…</p>
</details>

<hr>

<h2>ã¾ã¨ã‚</h2>

<p>ã“ã®ç« ã§ã¯ã€ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å®Ÿè·µçš„å¿œç”¨ã«ã¤ã„ã¦å­¦ç¿’ã—ã¾ã—ãŸï¼š</p>

<ul>
<li><strong>ãƒãƒ¼ãƒ‰åˆ†é¡</strong>: åŠæ•™å¸«ã‚ã‚Šå­¦ç¿’ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªãƒãƒ¼ãƒ‰ãƒ©ãƒ™ãƒ«äºˆæ¸¬</li>
<li><strong>ã‚°ãƒ©ãƒ•åˆ†é¡</strong>: åˆ†å­ç‰¹æ€§äºˆæ¸¬ãªã©ã‚°ãƒ©ãƒ•å…¨ä½“ã®åˆ†é¡ã‚¿ã‚¹ã‚¯</li>
<li><strong>ãƒªãƒ³ã‚¯äºˆæ¸¬</strong>: æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ãªã©ã‚¨ãƒƒã‚¸å­˜åœ¨ç¢ºç‡ã®äºˆæ¸¬</li>
<li><strong>å‰µè–¬å¿œç”¨</strong>: è–¬å‰¤-æ¨™çš„ç›¸äº’ä½œç”¨äºˆæ¸¬ã«ã‚ˆã‚‹å‰µè–¬æ”¯æ´</li>
<li><strong>ãã®ä»–å¿œç”¨</strong>: çŸ¥è­˜ã‚°ãƒ©ãƒ•ã€äº¤é€šãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãªã©å¤šæ§˜ãªé ˜åŸŸ</li>
</ul>

<p>GNNã¯æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†å¼·åŠ›ãªãƒ„ãƒ¼ãƒ«ã§ã‚ã‚Šã€æ§˜ã€…ãªå®Ÿä¸–ç•Œã®å•é¡Œã«å¿œç”¨ã§ãã¾ã™ã€‚å„ã‚¿ã‚¹ã‚¯ã®ç‰¹æ€§ã‚’ç†è§£ã—ã€é©åˆ‡ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨å­¦ç¿’æˆ¦ç•¥ã‚’é¸æŠã™ã‚‹ã“ã¨ãŒæˆåŠŸã®éµã§ã™ã€‚</p>

<div class="navigation">
    <a href="chapter4-advanced-architectures.html" class="nav-button">â† ç¬¬4ç« : é«˜åº¦ãªGNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</a>
    <a href="../index.html" class="nav-button">ã‚³ãƒ¼ã‚¹ç›®æ¬¡ã«æˆ»ã‚‹</a>
</div>

</main>


    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
    <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
    <p>ML-A05: ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å…¥é–€ | ç¬¬5ç« ï¼šGNNã®å¿œç”¨</p>
</footer>

</body>
</html>
