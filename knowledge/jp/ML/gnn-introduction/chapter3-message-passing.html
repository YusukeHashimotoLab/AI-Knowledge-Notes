<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬3ç« ï¼šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã¨GNN - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: var(--font-body); line-height: 1.7; color: var(--color-text); background-color: var(--color-bg); font-size: 16px; }
        header { background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; padding: var(--spacing-xl) var(--spacing-md); margin-bottom: var(--spacing-xl); box-shadow: var(--box-shadow); }
        .header-content { max-width: 900px; margin: 0 auto; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: var(--spacing-sm); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; opacity: 0.95; font-weight: 400; margin-bottom: var(--spacing-md); }
        .meta { display: flex; flex-wrap: wrap; gap: var(--spacing-md); font-size: 0.9rem; opacity: 0.9; }
        .meta-item { display: flex; align-items: center; gap: 0.3rem; }
        .container { max-width: 900px; margin: 0 auto; padding: 0 var(--spacing-md) var(--spacing-xl); }
        h2 { font-size: 1.75rem; color: var(--color-primary); margin-top: var(--spacing-xl); margin-bottom: var(--spacing-md); padding-bottom: var(--spacing-xs); border-bottom: 3px solid var(--color-accent); }
        h3 { font-size: 1.4rem; color: var(--color-primary); margin-top: var(--spacing-lg); margin-bottom: var(--spacing-sm); }
        h4 { font-size: 1.1rem; color: var(--color-primary-dark); margin-top: var(--spacing-md); margin-bottom: var(--spacing-sm); }
        p { margin-bottom: var(--spacing-md); color: var(--color-text); }
        a { color: var(--color-link); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--color-link-hover); text-decoration: underline; }
        ul, ol { margin-left: var(--spacing-lg); margin-bottom: var(--spacing-md); }
        li { margin-bottom: var(--spacing-xs); color: var(--color-text); }
        pre { background-color: var(--color-code-bg); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); overflow-x: auto; margin-bottom: var(--spacing-md); font-family: var(--font-mono); font-size: 0.9rem; line-height: 1.5; }
        code { font-family: var(--font-mono); font-size: 0.9em; background-color: var(--color-code-bg); padding: 0.2em 0.4em; border-radius: 3px; }
        pre code { background-color: transparent; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: var(--spacing-md); font-size: 0.95rem; }
        th, td { border: 1px solid var(--color-border); padding: var(--spacing-sm); text-align: left; }
        th { background-color: var(--color-bg-alt); font-weight: 600; color: var(--color-primary); }
        blockquote { border-left: 4px solid var(--color-accent); padding-left: var(--spacing-md); margin: var(--spacing-md) 0; color: var(--color-text-light); font-style: italic; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        .mermaid { text-align: center; margin: var(--spacing-lg) 0; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        details { background-color: var(--color-bg-alt); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); margin-bottom: var(--spacing-md); }
        summary { cursor: pointer; font-weight: 600; color: var(--color-primary); user-select: none; padding: var(--spacing-xs); margin: calc(-1 * var(--spacing-md)); padding: var(--spacing-md); border-radius: var(--border-radius); }
        summary:hover { background-color: rgba(123, 44, 191, 0.1); }
        details[open] summary { margin-bottom: var(--spacing-md); border-bottom: 1px solid var(--color-border); }
        .navigation { display: flex; justify-content: space-between; gap: var(--spacing-md); margin: var(--spacing-xl) 0; padding-top: var(--spacing-lg); border-top: 2px solid var(--color-border); }
        .nav-button { flex: 1; padding: var(--spacing-md); background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; border-radius: var(--border-radius); text-align: center; font-weight: 600; transition: transform 0.2s, box-shadow 0.2s; box-shadow: var(--box-shadow); }
        .nav-button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); text-decoration: none; }
        footer { margin-top: var(--spacing-xl); padding: var(--spacing-lg) var(--spacing-md); background-color: var(--color-bg-alt); border-top: 1px solid var(--color-border); text-align: center; font-size: 0.9rem; color: var(--color-text-light); }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }

        @media (max-width: 768px) { h1 { font-size: 1.5rem; } h2 { font-size: 1.4rem; } h3 { font-size: 1.2rem; } .meta { font-size: 0.85rem; } .navigation { flex-direction: column; } table { font-size: 0.85rem; } th, td { padding: var(--spacing-xs); } }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬3ç« ï¼šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã¨GNN</h1>
            <p class="subtitle">ä¸€èˆ¬åŒ–ã•ã‚ŒãŸGNNãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ - GraphSAGEã€GINã€PyTorch Geometricå®Ÿè£…</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®åŸºæœ¬æ§‹é€ ï¼ˆMessageã€Aggregateã€Updateï¼‰ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… ä¸€èˆ¬åŒ–ã•ã‚ŒãŸGNNï¼ˆMPNNï¼‰ã®æ•°å­¦çš„å®šå¼åŒ–ã‚’ç¿’å¾—ã™ã‚‹</li>
<li>âœ… GraphSAGEã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ™ãƒ¼ã‚¹é›†ç´„ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… å„ç¨®Aggregatorï¼ˆMeanã€Poolã€LSTMï¼‰ã®ç‰¹æ€§ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… GINï¼ˆGraph Isomorphism Networkï¼‰ã¨WL testã®é–¢ä¿‚ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… GNNã®è­˜åˆ¥èƒ½åŠ›ï¼ˆExpressive Powerï¼‰ã‚’è©•ä¾¡ã§ãã‚‹</li>
<li>âœ… PyTorch Geometricã§ã®åŠ¹ç‡çš„ãªå®Ÿè£…æ–¹æ³•ã‚’ç¿’å¾—ã™ã‚‹</li>
<li>âœ… ã‚°ãƒ©ãƒ•åˆ†é¡ã‚¿ã‚¹ã‚¯ã®å®Ÿè£…ã¨ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè£…ã§ãã‚‹</li>
</ul>

<hr>

<h2>3.1 ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯</h2>

<h3>ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã®æ¦‚å¿µ</h3>

<p><strong>ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ï¼ˆMessage Passingï¼‰</strong>ã¯ã€GNNã«ãŠã‘ã‚‹æƒ…å ±ä¼æ’­ã‚’çµ±ä¸€çš„ã«è¨˜è¿°ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ãƒãƒ¼ãƒ‰é–“ã§ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€å—ä¿¡ã—ã€é›†ç´„ã™ã‚‹ã“ã¨ã§ç‰¹å¾´ã‚’æ›´æ–°ã—ã¾ã™ã€‚</p>

<blockquote>
<p>ã€Œãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€ã‚ã‚‰ã‚†ã‚‹GNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’3ã¤ã®åŸºæœ¬æ“ä½œï¼ˆMessageã€Aggregateã€Updateï¼‰ã§è¨˜è¿°ã™ã‚‹çµ±ä¸€çš„ãªæ–¹æ³•ã‚’æä¾›ã™ã‚‹ã€</p>
</blockquote>

<h3>3ã¤ã®åŸºæœ¬æ“ä½œ</h3>

<p>ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã¯ä»¥ä¸‹ã®3ã‚¹ãƒ†ãƒƒãƒ—ã§æ§‹æˆã•ã‚Œã¾ã™ï¼š</p>

<div class="mermaid">
graph LR
    A[1. Message<br/>ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ] --> B[2. Aggregate<br/>ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é›†ç´„]
    B --> C[3. Update<br/>ç‰¹å¾´æ›´æ–°]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#e8f5e9
</div>

<h4>ã‚¹ãƒ†ãƒƒãƒ—1: Messageï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆï¼‰</h4>

<p>éš£æ¥ãƒãƒ¼ãƒ‰ã‹ã‚‰ä¸­å¿ƒãƒãƒ¼ãƒ‰ã¸é€ä¿¡ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆã—ã¾ã™ï¼š</p>

<p>$$
\mathbf{m}_{j \to i}^{(k)} = \text{MESSAGE}^{(k)}\left(\mathbf{h}_i^{(k-1)}, \mathbf{h}_j^{(k-1)}, \mathbf{e}_{ji}\right)
$$</p>

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$\mathbf{m}_{j \to i}^{(k)}$ï¼šãƒãƒ¼ãƒ‰$j$ã‹ã‚‰ãƒãƒ¼ãƒ‰$i$ã¸ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸</li>
<li>$\mathbf{h}_i^{(k-1)}$ï¼šå—ä¿¡ãƒãƒ¼ãƒ‰$i$ã®å‰å±¤ã®ç‰¹å¾´</li>
<li>$\mathbf{h}_j^{(k-1)}$ï¼šé€ä¿¡ãƒãƒ¼ãƒ‰$j$ã®å‰å±¤ã®ç‰¹å¾´</li>
<li>$\mathbf{e}_{ji}$ï¼šã‚¨ãƒƒã‚¸$(j, i)$ã®ç‰¹å¾´ï¼ˆoptionalï¼‰</li>
</ul>

<h4>ã‚¹ãƒ†ãƒƒãƒ—2: Aggregateï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é›†ç´„ï¼‰</h4>

<p>å—ä¿¡ã—ãŸå…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é›†ç´„ã—ã¾ã™ï¼š</p>

<p>$$
\mathbf{m}_i^{(k)} = \text{AGGREGATE}^{(k)}\left(\left\{\mathbf{m}_{j \to i}^{(k)} : j \in \mathcal{N}(i)\right\}\right)
$$</p>

<p>ä»£è¡¨çš„ãªé›†ç´„é–¢æ•°ï¼š</p>
<ul>
<li><strong>Sum</strong>: $\text{AGGREGATE} = \sum_{j \in \mathcal{N}(i)} \mathbf{m}_{j \to i}$</li>
<li><strong>Mean</strong>: $\text{AGGREGATE} = \frac{1}{|\mathcal{N}(i)|} \sum_{j \in \mathcal{N}(i)} \mathbf{m}_{j \to i}$</li>
<li><strong>Max</strong>: $\text{AGGREGATE} = \max_{j \in \mathcal{N}(i)} \mathbf{m}_{j \to i}$</li>
</ul>

<h4>ã‚¹ãƒ†ãƒƒãƒ—3: Updateï¼ˆç‰¹å¾´æ›´æ–°ï¼‰</h4>

<p>é›†ç´„ã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨è‡ªèº«ã®æƒ…å ±ã‚’çµ„ã¿åˆã‚ã›ã¦ç‰¹å¾´ã‚’æ›´æ–°ã—ã¾ã™ï¼š</p>

<p>$$
\mathbf{h}_i^{(k)} = \text{UPDATE}^{(k)}\left(\mathbf{h}_i^{(k-1)}, \mathbf{m}_i^{(k)}\right)
$$</p>

<h3>ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã®å¯è¦–åŒ–</h3>

<div class="mermaid">
graph TB
    subgraph "ã‚¹ãƒ†ãƒƒãƒ—1: Message"
        N1[ãƒãƒ¼ãƒ‰ v] --> M1[m<sub>1â†’v</sub>]
        N2[ãƒãƒ¼ãƒ‰ 1] --> M1
        N3[ãƒãƒ¼ãƒ‰ 2] --> M2[m<sub>2â†’v</sub>]
        N4[ãƒãƒ¼ãƒ‰ 3] --> M3[m<sub>3â†’v</sub>]
    end

    subgraph "ã‚¹ãƒ†ãƒƒãƒ—2: Aggregate"
        M1 --> AGG[Î£ / Mean / Max]
        M2 --> AGG
        M3 --> AGG
        AGG --> AM[é›†ç´„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸]
    end

    subgraph "ã‚¹ãƒ†ãƒƒãƒ—3: Update"
        N1 --> UPD[UPDATEé–¢æ•°]
        AM --> UPD
        UPD --> H[h<sub>v</sub><sup>(k)</sup>]
    end

    style M1 fill:#e3f2fd
    style M2 fill:#e3f2fd
    style M3 fill:#e3f2fd
    style AGG fill:#fff3e0
    style UPD fill:#e8f5e9
    style H fill:#c8e6c9
</div>

<h3>å®Ÿè£…ä¾‹1: åŸºæœ¬çš„ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

print("=== ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ åŸºæœ¬å®Ÿè£… ===\n")

class MessagePassingLayer(nn.Module):
    """åŸºæœ¬çš„ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°å±¤"""

    def __init__(self, in_dim, out_dim, aggr='mean'):
        super(MessagePassingLayer, self).__init__()
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.aggr = aggr

        # Messageé–¢æ•°ï¼ˆç·šå½¢å¤‰æ›ï¼‰
        self.message_nn = nn.Linear(in_dim, out_dim)

        # Updateé–¢æ•°ï¼ˆç·šå½¢å¤‰æ› + æ´»æ€§åŒ–ï¼‰
        self.update_nn = nn.Sequential(
            nn.Linear(in_dim + out_dim, out_dim),
            nn.ReLU()
        )

    def message(self, h_j):
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ"""
        return self.message_nn(h_j)

    def aggregate(self, messages, edge_index, num_nodes):
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é›†ç´„"""
        # edge_index[1]: å—ä¿¡ãƒãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        target_nodes = edge_index[1]

        # å„ãƒãƒ¼ãƒ‰ã¸ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é›†ç´„
        aggregated = torch.zeros(num_nodes, self.out_dim)

        if self.aggr == 'sum':
            aggregated.index_add_(0, target_nodes, messages)
        elif self.aggr == 'mean':
            aggregated.index_add_(0, target_nodes, messages)
            # æ¬¡æ•°ã§æ­£è¦åŒ–
            degree = torch.bincount(target_nodes, minlength=num_nodes).float()
            degree = degree.clamp(min=1).view(-1, 1)
            aggregated = aggregated / degree
        elif self.aggr == 'max':
            # Max pooling
            for i in range(num_nodes):
                mask = (target_nodes == i)
                if mask.any():
                    aggregated[i] = messages[mask].max(dim=0)[0]

        return aggregated

    def update(self, h_i, aggregated):
        """ç‰¹å¾´æ›´æ–°"""
        combined = torch.cat([h_i, aggregated], dim=-1)
        return self.update_nn(combined)

    def forward(self, x, edge_index):
        """
        Args:
            x: ãƒãƒ¼ãƒ‰ç‰¹å¾´ [num_nodes, in_dim]
            edge_index: ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ [2, num_edges]
        """
        num_nodes = x.size(0)

        # Step 1: Message
        # edge_index[0]: é€ä¿¡ãƒãƒ¼ãƒ‰
        h_j = x[edge_index[0]]  # é€ä¿¡ãƒãƒ¼ãƒ‰ã®ç‰¹å¾´
        messages = self.message(h_j)

        # Step 2: Aggregate
        aggregated = self.aggregate(messages, edge_index, num_nodes)

        # Step 3: Update
        h_new = self.update(x, aggregated)

        return h_new


# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
print("--- ãƒ†ã‚¹ãƒˆã‚°ãƒ©ãƒ•ã®ä½œæˆ ---")
# 5ãƒãƒ¼ãƒ‰ã®ã‚°ãƒ©ãƒ•
num_nodes = 5
in_dim = 4
out_dim = 8

# ãƒãƒ¼ãƒ‰ç‰¹å¾´ï¼ˆãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ï¼‰
x = torch.randn(num_nodes, in_dim)
print(f"ãƒãƒ¼ãƒ‰ç‰¹å¾´å½¢çŠ¶: {x.shape}")

# ã‚¨ãƒƒã‚¸ãƒªã‚¹ãƒˆï¼ˆ0â†’1, 1â†’2, 2â†’3, 3â†’4, 1â†’3ï¼‰
edge_index = torch.tensor([
    [0, 1, 2, 3, 1],  # é€ä¿¡ãƒãƒ¼ãƒ‰
    [1, 2, 3, 4, 3]   # å—ä¿¡ãƒãƒ¼ãƒ‰
], dtype=torch.long)
print(f"ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å½¢çŠ¶: {edge_index.shape}")
print(f"ã‚¨ãƒƒã‚¸æ•°: {edge_index.size(1)}\n")

# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°å±¤ã®ä½œæˆã¨å®Ÿè¡Œ
print("--- å„é›†ç´„æ–¹æ³•ã§ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚° ---")
for aggr in ['sum', 'mean', 'max']:
    print(f"\n{aggr.upper()} é›†ç´„:")
    mp_layer = MessagePassingLayer(in_dim, out_dim, aggr=aggr)
    h_new = mp_layer(x, edge_index)
    print(f"  å‡ºåŠ›å½¢çŠ¶: {h_new.shape}")
    print(f"  å‡ºåŠ›å€¤ã®ç¯„å›²: [{h_new.min():.3f}, {h_new.max():.3f}]")
    print(f"  å„ãƒãƒ¼ãƒ‰ã®å‡ºåŠ›ä¾‹:")
    for i in range(min(3, num_nodes)):
        print(f"    ãƒãƒ¼ãƒ‰{i}: å¹³å‡={h_new[i].mean():.3f}, æ¨™æº–åå·®={h_new[i].std():.3f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ åŸºæœ¬å®Ÿè£… ===

--- ãƒ†ã‚¹ãƒˆã‚°ãƒ©ãƒ•ã®ä½œæˆ ---
ãƒãƒ¼ãƒ‰ç‰¹å¾´å½¢çŠ¶: torch.Size([5, 4])
ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å½¢çŠ¶: torch.Size([2, 5])
ã‚¨ãƒƒã‚¸æ•°: 5

--- å„é›†ç´„æ–¹æ³•ã§ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚° ---

SUM é›†ç´„:
  å‡ºåŠ›å½¢çŠ¶: torch.Size([5, 8])
  å‡ºåŠ›å€¤ã®ç¯„å›²: [-1.234, 2.456]
  å„ãƒãƒ¼ãƒ‰ã®å‡ºåŠ›ä¾‹:
    ãƒãƒ¼ãƒ‰0: å¹³å‡=0.123, æ¨™æº–åå·®=0.876
    ãƒãƒ¼ãƒ‰1: å¹³å‡=0.234, æ¨™æº–åå·®=0.945
    ãƒãƒ¼ãƒ‰2: å¹³å‡=-0.089, æ¨™æº–åå·®=0.823

MEAN é›†ç´„:
  å‡ºåŠ›å½¢çŠ¶: torch.Size([5, 8])
  å‡ºåŠ›å€¤ã®ç¯„å›²: [-0.987, 1.876]
  å„ãƒãƒ¼ãƒ‰ã®å‡ºåŠ›ä¾‹:
    ãƒãƒ¼ãƒ‰0: å¹³å‡=0.098, æ¨™æº–åå·®=0.734
    ãƒãƒ¼ãƒ‰1: å¹³å‡=0.187, æ¨™æº–åå·®=0.812
    ãƒãƒ¼ãƒ‰2: å¹³å‡=-0.045, æ¨™æº–åå·®=0.698

MAX é›†ç´„:
  å‡ºåŠ›å½¢çŠ¶: torch.Size([5, 8])
  å‡ºåŠ›å€¤ã®ç¯„å›²: [-0.756, 2.123]
  å„ãƒãƒ¼ãƒ‰ã®å‡ºåŠ›ä¾‹:
    ãƒãƒ¼ãƒ‰0: å¹³å‡=0.156, æ¨™æº–åå·®=0.923
    ãƒãƒ¼ãƒ‰1: å¹³å‡=0.267, æ¨™æº–åå·®=1.012
    ãƒãƒ¼ãƒ‰2: å¹³å‡=0.034, æ¨™æº–åå·®=0.876
</code></pre>

<h3>ä¸€èˆ¬åŒ–ã•ã‚ŒãŸGNNï¼ˆMPNNï¼‰</h3>

<p><strong>Message Passing Neural Network (MPNN)</strong>ã¯ã€å¤šãã®GNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’çµ±ä¸€çš„ã«è¨˜è¿°ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚</p>

<p>MPNNã®ä¸€èˆ¬å½¢å¼ï¼š</p>

<p>$$
\begin{align}
\mathbf{m}_i^{(k+1)} &= \sum_{j \in \mathcal{N}(i)} M_k\left(\mathbf{h}_i^{(k)}, \mathbf{h}_j^{(k)}, \mathbf{e}_{ji}\right) \\
\mathbf{h}_i^{(k+1)} &= U_k\left(\mathbf{h}_i^{(k)}, \mathbf{m}_i^{(k+1)}\right)
\end{align}
$$</p>

<p>ä»£è¡¨çš„ãªGNNã®MPNNè¡¨ç¾ï¼š</p>

<table>
<thead>
<tr>
<th>ãƒ¢ãƒ‡ãƒ«</th>
<th>MESSAGEé–¢æ•° $M_k$</th>
<th>UPDATEé–¢æ•° $U_k$</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>GCN</strong></td>
<td>$\frac{1}{\sqrt{d_i d_j}} \mathbf{W}^{(k)} \mathbf{h}_j^{(k)}$</td>
<td>$\sigma(\mathbf{m}_i^{(k+1)})$</td>
</tr>
<tr>
<td><strong>GraphSAGE</strong></td>
<td>$\mathbf{h}_j^{(k)}$</td>
<td>$\sigma(\mathbf{W} \cdot [\mathbf{h}_i^{(k)} \| \text{AGG}(\mathbf{m}_i^{(k+1)})])$</td>
</tr>
<tr>
<td><strong>GAT</strong></td>
<td>$\alpha_{ij} \mathbf{W} \mathbf{h}_j^{(k)}$</td>
<td>$\sigma(\mathbf{m}_i^{(k+1)})$</td>
</tr>
<tr>
<td><strong>GIN</strong></td>
<td>$\mathbf{h}_j^{(k)}$</td>
<td>$\text{MLP}((1+\epsilon) \mathbf{h}_i^{(k)} + \mathbf{m}_i^{(k+1)})$</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.2 GraphSAGE</h2>

<h3>GraphSAGEã®æ¦‚è¦</h3>

<p><strong>GraphSAGE (SAmple and aggreGatE)</strong>ã¯ã€å¤§è¦æ¨¡ã‚°ãƒ©ãƒ•ã«å¯¾å¿œã—ãŸã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ™ãƒ¼ã‚¹ã®GNNã§ã™ã€‚å…¨è¿‘å‚ã§ã¯ãªãã€å›ºå®šæ•°ã®è¿‘å‚ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦é›†ç´„ã—ã¾ã™ã€‚</p>

<blockquote>
<p>ã€ŒGraphSAGEã¯ã€è¿‘å‚ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ã‚’å¯èƒ½ã«ã—ã€å¤§è¦æ¨¡ã‚°ãƒ©ãƒ•ã¸ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’å®Ÿç¾ã™ã‚‹ã€</p>
</blockquote>

<h3>ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ™ãƒ¼ã‚¹ã®é›†ç´„</h3>

<p>GraphSAGEã®ç‰¹å¾´ï¼š</p>
<ol>
<li><strong>è¿‘å‚ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</strong>ï¼šå„ãƒãƒ¼ãƒ‰ã®è¿‘å‚ã‹ã‚‰å›ºå®šæ•°ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</li>
<li><strong>å¤šæ§˜ãªAggregator</strong>ï¼šMeanã€Poolã€LSTMãªã©ã®é›†ç´„é–¢æ•°</li>
<li><strong>Inductiveå­¦ç¿’</strong>ï¼šè¨“ç·´æ™‚ã«è¦‹ã¦ã„ãªã„ãƒãƒ¼ãƒ‰ã«ã‚‚é©ç”¨å¯èƒ½</li>
</ol>

<div class="mermaid">
graph TB
    subgraph "æ¨™æº–GNNï¼ˆå…¨è¿‘å‚ï¼‰"
        V1[ä¸­å¿ƒãƒãƒ¼ãƒ‰] --> N1[è¿‘å‚1]
        V1 --> N2[è¿‘å‚2]
        V1 --> N3[è¿‘å‚3]
        V1 --> N4[è¿‘å‚4]
        V1 --> N5[è¿‘å‚5]
        V1 --> N6[è¿‘å‚6]
    end

    subgraph "GraphSAGEï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰"
        V2[ä¸­å¿ƒãƒãƒ¼ãƒ‰] --> S1[ã‚µãƒ³ãƒ—ãƒ«1]
        V2 --> S2[ã‚µãƒ³ãƒ—ãƒ«2]
        V2 --> S3[ã‚µãƒ³ãƒ—ãƒ«3]
        N7[è¿‘å‚4] -.x.- V2
        N8[è¿‘å‚5] -.x.- V2
        N9[è¿‘å‚6] -.x.- V2
    end

    style V1 fill:#fff3e0
    style V2 fill:#fff3e0
    style S1 fill:#e3f2fd
    style S2 fill:#e3f2fd
    style S3 fill:#e3f2fd
</div>

<h3>GraphSAGEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h3>

<p>GraphSAGEã®æ›´æ–°å¼ï¼š</p>

<p>$$
\begin{align}
\mathbf{h}_{\mathcal{N}(i)}^{(k)} &= \text{AGGREGATE}_k\left(\left\{\mathbf{h}_j^{(k-1)}, \forall j \in \mathcal{S}_{\mathcal{N}(i)}\right\}\right) \\
\mathbf{h}_i^{(k)} &= \sigma\left(\mathbf{W}^{(k)} \cdot \left[\mathbf{h}_i^{(k-1)} \| \mathbf{h}_{\mathcal{N}(i)}^{(k)}\right]\right) \\
\mathbf{h}_i^{(k)} &= \frac{\mathbf{h}_i^{(k)}}{\|\mathbf{h}_i^{(k)}\|_2}
\end{align}
$$</p>

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$\mathcal{S}_{\mathcal{N}(i)}$ï¼šãƒãƒ¼ãƒ‰$i$ã®è¿‘å‚ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸéƒ¨åˆ†é›†åˆ</li>
<li>$\|$ï¼šç‰¹å¾´ã®é€£çµï¼ˆconcatenationï¼‰</li>
<li>æœ€çµ‚è¡Œï¼šL2æ­£è¦åŒ–</li>
</ul>

<h3>å„ç¨®Aggregator</h3>

<h4>1. Mean Aggregator</h4>

<p>$$
\text{AGGREGATE}_{\text{mean}} = \frac{1}{|\mathcal{S}_{\mathcal{N}(i)}|} \sum_{j \in \mathcal{S}_{\mathcal{N}(i)}} \mathbf{h}_j^{(k-1)}
$$</p>

<p>ç‰¹å¾´ï¼šã‚·ãƒ³ãƒ—ãƒ«ã§åŠ¹ç‡çš„ã€GCNã«è¿‘ã„å‹•ä½œ</p>

<h4>2. Pool Aggregator</h4>

<p>$$
\text{AGGREGATE}_{\text{pool}} = \max\left(\left\{\sigma\left(\mathbf{W}_{\text{pool}} \mathbf{h}_j^{(k-1)} + \mathbf{b}\right), \forall j \in \mathcal{S}_{\mathcal{N}(i)}\right\}\right)
$$</p>

<p>ç‰¹å¾´ï¼šè¦ç´ ã”ã¨ã®max-poolingã€éå¯¾ç§°ãªè¿‘å‚æƒ…å ±ã‚’æ‰ãˆã‚‹</p>

<h4>3. LSTM Aggregator</h4>

<p>$$
\text{AGGREGATE}_{\text{LSTM}} = \text{LSTM}\left(\left[\mathbf{h}_j^{(k-1)}, \forall j \in \pi(\mathcal{S}_{\mathcal{N}(i)})\right]\right)
$$</p>

<p>ã“ã“ã§$\pi$ã¯ãƒ©ãƒ³ãƒ€ãƒ é †åˆ—ã€‚ç‰¹å¾´ï¼šè¡¨ç¾åŠ›ãŒé«˜ã„ãŒã€é †åˆ—ä¾å­˜æ€§ã«æ³¨æ„ãŒå¿…è¦</p>

<h3>å®Ÿè£…ä¾‹2: GraphSAGEå®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

print("\n=== GraphSAGE å®Ÿè£… ===\n")

class SAGEConv(nn.Module):
    """GraphSAGEå±¤"""

    def __init__(self, in_dim, out_dim, aggr='mean'):
        super(SAGEConv, self).__init__()
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.aggr = aggr

        # ç·šå½¢å¤‰æ›ï¼ˆè‡ªèº«ã®ç‰¹å¾´ + è¿‘å‚ã®ç‰¹å¾´ã‚’é€£çµå¾Œï¼‰
        if aggr == 'lstm':
            self.lstm = nn.LSTM(in_dim, in_dim, batch_first=True)
            self.lin = nn.Linear(2 * in_dim, out_dim)
        elif aggr == 'pool':
            self.pool_nn = nn.Linear(in_dim, in_dim)
            self.lin = nn.Linear(2 * in_dim, out_dim)
        else:  # mean
            self.lin = nn.Linear(2 * in_dim, out_dim)

    def aggregate_mean(self, h_neighbors, edge_index, num_nodes):
        """Meané›†ç´„"""
        target_nodes = edge_index[1]
        aggregated = torch.zeros(num_nodes, self.in_dim)

        aggregated.index_add_(0, target_nodes, h_neighbors)
        degree = torch.bincount(target_nodes, minlength=num_nodes).float()
        degree = degree.clamp(min=1).view(-1, 1)

        return aggregated / degree

    def aggregate_pool(self, h_neighbors, edge_index, num_nodes):
        """Max-poolingé›†ç´„"""
        target_nodes = edge_index[1]

        # å„è¿‘å‚ç‰¹å¾´ã‚’å¤‰æ›
        transformed = torch.relu(self.pool_nn(h_neighbors))

        # Max-pooling
        aggregated = torch.zeros(num_nodes, self.in_dim)
        for i in range(num_nodes):
            mask = (target_nodes == i)
            if mask.any():
                aggregated[i] = transformed[mask].max(dim=0)[0]

        return aggregated

    def aggregate_lstm(self, h_neighbors, edge_index, num_nodes):
        """LSTMé›†ç´„"""
        target_nodes = edge_index[1]
        aggregated = torch.zeros(num_nodes, self.in_dim)

        for i in range(num_nodes):
            mask = (target_nodes == i)
            if mask.any():
                # ãƒ©ãƒ³ãƒ€ãƒ é †åˆ—ã§LSTMã«å…¥åŠ›
                neighbors = h_neighbors[mask]
                perm = torch.randperm(neighbors.size(0))
                neighbors = neighbors[perm].unsqueeze(0)

                _, (h_n, _) = self.lstm(neighbors)
                aggregated[i] = h_n.squeeze(0)

        return aggregated

    def forward(self, x, edge_index):
        num_nodes = x.size(0)

        # è¿‘å‚ç‰¹å¾´ã®å–å¾—
        h_neighbors = x[edge_index[0]]

        # é›†ç´„
        if self.aggr == 'mean':
            h_neigh = self.aggregate_mean(h_neighbors, edge_index, num_nodes)
        elif self.aggr == 'pool':
            h_neigh = self.aggregate_pool(h_neighbors, edge_index, num_nodes)
        elif self.aggr == 'lstm':
            h_neigh = self.aggregate_lstm(h_neighbors, edge_index, num_nodes)

        # è‡ªèº«ã®ç‰¹å¾´ã¨é€£çµ
        h_concat = torch.cat([x, h_neigh], dim=-1)

        # ç·šå½¢å¤‰æ›
        out = self.lin(h_concat)

        # L2æ­£è¦åŒ–
        out = F.normalize(out, p=2, dim=-1)

        return out


class GraphSAGE(nn.Module):
    """GraphSAGEãƒ¢ãƒ‡ãƒ«ï¼ˆ2å±¤ï¼‰"""

    def __init__(self, in_dim, hidden_dim, out_dim, aggr='mean'):
        super(GraphSAGE, self).__init__()
        self.conv1 = SAGEConv(in_dim, hidden_dim, aggr)
        self.conv2 = SAGEConv(hidden_dim, out_dim, aggr)

    def forward(self, x, edge_index):
        # ç¬¬1å±¤
        h = self.conv1(x, edge_index)
        h = F.relu(h)
        h = F.dropout(h, p=0.5, training=self.training)

        # ç¬¬2å±¤
        h = self.conv2(h, edge_index)

        return h


# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
print("--- GraphSAGEãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ ---")
num_nodes = 10
in_dim = 8
hidden_dim = 16
out_dim = 4

x = torch.randn(num_nodes, in_dim)
edge_index = torch.tensor([
    [0, 1, 2, 3, 4, 1, 2, 5, 6, 7],
    [1, 2, 3, 4, 5, 0, 1, 6, 7, 8]
], dtype=torch.long)

print(f"ãƒãƒ¼ãƒ‰æ•°: {num_nodes}")
print(f"å…¥åŠ›æ¬¡å…ƒ: {in_dim}")
print(f"éš ã‚Œå±¤æ¬¡å…ƒ: {hidden_dim}")
print(f"å‡ºåŠ›æ¬¡å…ƒ: {out_dim}\n")

# å„Aggregatorã§ãƒ†ã‚¹ãƒˆ
for aggr in ['mean', 'pool', 'lstm']:
    print(f"--- {aggr.upper()} Aggregator ---")
    model = GraphSAGE(in_dim, hidden_dim, out_dim, aggr=aggr)
    model.eval()

    with torch.no_grad():
        out = model(x, edge_index)

    print(f"å‡ºåŠ›å½¢çŠ¶: {out.shape}")
    print(f"å‡ºåŠ›L2ãƒãƒ«ãƒ : {out.norm(dim=-1)[:5].numpy()}")
    print(f"å‡ºåŠ›å€¤ã®ç¯„å›²: [{out.min():.3f}, {out.max():.3f}]\n")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>
=== GraphSAGE å®Ÿè£… ===

--- GraphSAGEãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ ---
ãƒãƒ¼ãƒ‰æ•°: 10
å…¥åŠ›æ¬¡å…ƒ: 8
éš ã‚Œå±¤æ¬¡å…ƒ: 16
å‡ºåŠ›æ¬¡å…ƒ: 4

--- MEAN Aggregator ---
å‡ºåŠ›å½¢çŠ¶: torch.Size([10, 4])
å‡ºåŠ›L2ãƒãƒ«ãƒ : [1. 1. 1. 1. 1.]
å‡ºåŠ›å€¤ã®ç¯„å›²: [-0.876, 0.923]

--- POOL Aggregator ---
å‡ºåŠ›å½¢çŠ¶: torch.Size([10, 4])
å‡ºåŠ›L2ãƒãƒ«ãƒ : [1. 1. 1. 1. 1.]
å‡ºåŠ›å€¤ã®ç¯„å›²: [-0.845, 0.891]

--- LSTM Aggregator ---
å‡ºåŠ›å½¢çŠ¶: torch.Size([10, 4])
å‡ºåŠ›L2ãƒãƒ«ãƒ : [1. 1. 1. 1. 1.]
å‡ºåŠ›å€¤ã®ç¯„å›²: [-0.912, 0.867]
</code></pre>

<hr>

<h2>3.3 Graph Isomorphism Network (GIN)</h2>

<h3>GINã®å‹•æ©Ÿï¼šè­˜åˆ¥èƒ½åŠ›ã®å‘ä¸Š</h3>

<p><strong>Graph Isomorphism Network (GIN)</strong>ã¯ã€Weisfeiler-Lehman (WL) testã¨åŒç­‰ã®è­˜åˆ¥èƒ½åŠ›ã‚’æŒã¤ã‚ˆã†ã«è¨­è¨ˆã•ã‚ŒãŸGNNã§ã™ã€‚</p>

<blockquote>
<p>ã€ŒGINã¯ã€GNNãŒç†è«–çš„ã«é”æˆå¯èƒ½ãªæœ€å¤§ã®è­˜åˆ¥èƒ½åŠ›ã‚’æŒã¤ã€‚ã¤ã¾ã‚Šã€GINã§åŒºåˆ¥ã§ããªã„ã‚°ãƒ©ãƒ•ã¯ã€WL testã§ã‚‚åŒºåˆ¥ã§ããªã„ã€</p>
</blockquote>

<h3>Weisfeiler-Lehman (WL) Test</h3>

<p><strong>WL test</strong>ã¯ã€ã‚°ãƒ©ãƒ•åŒå‹æ€§ã‚’åˆ¤å®šã™ã‚‹ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚å¤šãã®å ´åˆã€ã‚°ãƒ©ãƒ•ã®åŒå‹æ€§ã‚’åŠ¹ç‡çš„ã«åˆ¤å®šã§ãã¾ã™ã€‚</p>

<p>WL testã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼š</p>
<ol>
<li>å„ãƒãƒ¼ãƒ‰ã«åˆæœŸãƒ©ãƒ™ãƒ«ã‚’å‰²ã‚Šå½“ã¦</li>
<li>å„ãƒãƒ¼ãƒ‰ã®ãƒ©ãƒ™ãƒ«ã‚’ã€è‡ªèº«ã®ãƒ©ãƒ™ãƒ«ã¨è¿‘å‚ã®ãƒ©ãƒ™ãƒ«ã®å¤šé‡é›†åˆã§æ›´æ–°</li>
<li>ãƒ©ãƒ™ãƒ«ã‚’ãƒãƒƒã‚·ãƒ¥åŒ–ã—ã¦æ–°ã—ã„ãƒ©ãƒ™ãƒ«ã¨ã™ã‚‹</li>
<li>åæŸã™ã‚‹ã¾ã§ç¹°ã‚Šè¿”ã™</li>
</ol>

<div class="mermaid">
graph TB
    subgraph "åå¾©1"
        A1[1] --- B1[1]
        A1 --- C1[1]
        B1 --- C1
    end

    subgraph "åå¾©2"
        A2[2] --- B2[3]
        A2 --- C2[3]
        B2 --- C2[2]
    end

    subgraph "åå¾©3"
        A3[4] --- B3[5]
        A3 --- C3[5]
        B3 --- C3[4]
    end

    A1 --> A2 --> A3
    B1 --> B2 --> B3
    C1 --> C2 --> C3

    style A1 fill:#e3f2fd
    style A2 fill:#fff3e0
    style A3 fill:#e8f5e9
</div>

<h3>GINã®å®šå¼åŒ–</h3>

<p>GINã®æ›´æ–°å¼ï¼š</p>

<p>$$
\mathbf{h}_i^{(k)} = \text{MLP}^{(k)}\left(\left(1 + \epsilon^{(k)}\right) \cdot \mathbf{h}_i^{(k-1)} + \sum_{j \in \mathcal{N}(i)} \mathbf{h}_j^{(k-1)}\right)
$$</p>

<p>é‡è¦ãªãƒã‚¤ãƒ³ãƒˆï¼š</p>
<ul>
<li><strong>Sumé›†ç´„</strong>ï¼šå¤šé‡é›†åˆã‚’ä¿æŒã§ãã‚‹å”¯ä¸€ã®å˜å°„çš„é›†ç´„é–¢æ•°</li>
<li><strong>$(1 + \epsilon)$ä¿‚æ•°</strong>ï¼šè‡ªèº«ã®ç‰¹å¾´ã¨è¿‘å‚ã®ç‰¹å¾´ã‚’åŒºåˆ¥</li>
<li><strong>MLP</strong>ï¼šååˆ†ãªè¡¨ç¾åŠ›ã‚’æŒã¤æ›´æ–°é–¢æ•°</li>
</ul>

<h3>ãªãœGINãŒæœ€ã‚‚è­˜åˆ¥èƒ½åŠ›ãŒé«˜ã„ã®ã‹</h3>

<p>GNNã®è­˜åˆ¥èƒ½åŠ›ã¯ã€ä»¥ä¸‹ã®é †åºé–¢ä¿‚ãŒã‚ã‚Šã¾ã™ï¼š</p>

<p>$$
\text{Sum} > \text{Mean} > \text{Max}
$$</p>

<table>
<thead>
<tr>
<th>é›†ç´„é–¢æ•°</th>
<th>å¤šé‡é›†åˆã®ä¿æŒ</th>
<th>ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Sum</strong></td>
<td>âœ… å˜å°„çš„ï¼ˆå¤šé‡åº¦ã‚’ä¿æŒï¼‰</td>
<td>$\{1, 1, 2\} \to 4 \neq 3 \leftarrow \{1, 2\}$</td>
</tr>
<tr>
<td><strong>Mean</strong></td>
<td>âŒ æƒ…å ±æå¤±ã‚ã‚Š</td>
<td>$\{1, 1, 2\} \to 1.33 \neq 1.5 \leftarrow \{1, 2\}$</td>
</tr>
<tr>
<td><strong>Max</strong></td>
<td>âŒ æœ€å¤§å€¤ã®ã¿ä¿æŒ</td>
<td>$\{1, 1, 2\} \to 2 = 2 \leftarrow \{1, 2\}$ âš ï¸</td>
</tr>
</tbody>
</table>

<h3>å®Ÿè£…ä¾‹3: GINå®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

print("\n=== Graph Isomorphism Network (GIN) å®Ÿè£… ===\n")

class GINConv(nn.Module):
    """GINå±¤"""

    def __init__(self, in_dim, out_dim, epsilon=0.0, train_eps=False):
        super(GINConv, self).__init__()

        # Epsilonï¼ˆå­¦ç¿’å¯èƒ½ã«ã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if train_eps:
            self.epsilon = nn.Parameter(torch.Tensor([epsilon]))
        else:
            self.register_buffer('epsilon', torch.Tensor([epsilon]))

        # MLP (2å±¤)
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, 2 * out_dim),
            nn.BatchNorm1d(2 * out_dim),
            nn.ReLU(),
            nn.Linear(2 * out_dim, out_dim)
        )

    def forward(self, x, edge_index):
        num_nodes = x.size(0)

        # Sumé›†ç´„
        h_neighbors = x[edge_index[0]]
        target_nodes = edge_index[1]

        aggregated = torch.zeros_like(x)
        aggregated.index_add_(0, target_nodes, h_neighbors)

        # (1 + epsilon) * h_i + sum(h_j)
        out = (1 + self.epsilon) * x + aggregated

        # MLPé©ç”¨
        out = self.mlp(out)

        return out


class GIN(nn.Module):
    """GINãƒ¢ãƒ‡ãƒ«ï¼ˆã‚°ãƒ©ãƒ•åˆ†é¡ç”¨ï¼‰"""

    def __init__(self, in_dim, hidden_dim, out_dim, num_layers=3,
                 dropout=0.5, train_eps=False):
        super(GIN, self).__init__()

        self.num_layers = num_layers
        self.dropout = dropout

        # GINå±¤
        self.convs = nn.ModuleList()
        self.batch_norms = nn.ModuleList()

        # ç¬¬1å±¤
        self.convs.append(GINConv(in_dim, hidden_dim, train_eps=train_eps))
        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))

        # ä¸­é–“å±¤
        for _ in range(num_layers - 2):
            self.convs.append(GINConv(hidden_dim, hidden_dim, train_eps=train_eps))
            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))

        # æœ€çµ‚å±¤
        self.convs.append(GINConv(hidden_dim, hidden_dim, train_eps=train_eps))
        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))

        # ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«åˆ†é¡ç”¨
        self.graph_pred_linear = nn.Linear(hidden_dim, out_dim)

    def forward(self, x, edge_index, batch=None):
        # ãƒãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã®æ›´æ–°
        h = x
        for i in range(self.num_layers):
            h = self.convs[i](h, edge_index)
            h = self.batch_norms[i](h)
            h = F.relu(h)
            h = F.dropout(h, p=self.dropout, training=self.training)

        # ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«ã®poolingï¼ˆå¹³å‡ï¼‰
        if batch is None:
            # å˜ä¸€ã‚°ãƒ©ãƒ•ã®å ´åˆ
            h_graph = h.mean(dim=0, keepdim=True)
        else:
            # ãƒãƒƒãƒã‚°ãƒ©ãƒ•ã®å ´åˆ
            num_graphs = batch.max().item() + 1
            h_graph = torch.zeros(num_graphs, h.size(1))
            for i in range(num_graphs):
                mask = (batch == i)
                h_graph[i] = h[mask].mean(dim=0)

        # åˆ†é¡
        out = self.graph_pred_linear(h_graph)

        return out


# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
print("--- GINãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ ---")
in_dim = 10
hidden_dim = 32
out_dim = 5  # 5ã‚¯ãƒ©ã‚¹åˆ†é¡
num_layers = 3

model = GIN(in_dim, hidden_dim, out_dim, num_layers, train_eps=True)
print(f"ãƒ¢ãƒ‡ãƒ«æ§‹é€ :\n{model}\n")

# å˜ä¸€ã‚°ãƒ©ãƒ•ã§ã®ãƒ†ã‚¹ãƒˆ
num_nodes = 20
x = torch.randn(num_nodes, in_dim)
edge_index = torch.randint(0, num_nodes, (2, 50))

print("--- å˜ä¸€ã‚°ãƒ©ãƒ•ã§ã®æ¨è«– ---")
model.eval()
with torch.no_grad():
    out = model(x, edge_index)

print(f"å…¥åŠ›ãƒãƒ¼ãƒ‰æ•°: {num_nodes}")
print(f"å…¥åŠ›ç‰¹å¾´æ¬¡å…ƒ: {in_dim}")
print(f"å‡ºåŠ›å½¢çŠ¶: {out.shape}")
print(f"å‡ºåŠ›ï¼ˆãƒ­ã‚¸ãƒƒãƒˆï¼‰: {out[0].numpy()}\n")

# ãƒãƒƒãƒã‚°ãƒ©ãƒ•ã§ã®ãƒ†ã‚¹ãƒˆ
print("--- ãƒãƒƒãƒã‚°ãƒ©ãƒ•ã§ã®æ¨è«– ---")
# 3ã¤ã®ã‚°ãƒ©ãƒ•ã‚’ãƒãƒƒãƒå‡¦ç†
x_batch = torch.randn(50, in_dim)  # åˆè¨ˆ50ãƒãƒ¼ãƒ‰
edge_index_batch = torch.randint(0, 50, (2, 100))
batch = torch.tensor([0]*15 + [1]*20 + [2]*15)  # ã‚°ãƒ©ãƒ•1: 15ãƒãƒ¼ãƒ‰, ã‚°ãƒ©ãƒ•2: 20ãƒãƒ¼ãƒ‰, ã‚°ãƒ©ãƒ•3: 15ãƒãƒ¼ãƒ‰

with torch.no_grad():
    out_batch = model(x_batch, edge_index_batch, batch)

print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: 3")
print(f"ç·ãƒãƒ¼ãƒ‰æ•°: {x_batch.size(0)}")
print(f"å‡ºåŠ›å½¢çŠ¶: {out_batch.shape}")
print(f"å„ã‚°ãƒ©ãƒ•ã®äºˆæ¸¬:")
for i in range(3):
    pred_class = out_batch[i].argmax().item()
    print(f"  ã‚°ãƒ©ãƒ•{i+1}: ã‚¯ãƒ©ã‚¹ {pred_class} (ã‚¹ã‚³ã‚¢={out_batch[i, pred_class]:.3f})")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>
=== Graph Isomorphism Network (GIN) å®Ÿè£… ===

--- GINãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ ---
ãƒ¢ãƒ‡ãƒ«æ§‹é€ :
GIN(
  (convs): ModuleList(
    (0-2): 3 x GINConv(...)
  )
  (batch_norms): ModuleList(
    (0-2): 3 x BatchNorm1d(32, eps=1e-05, momentum=0.1)
  )
  (graph_pred_linear): Linear(in_features=32, out_features=5, bias=True)
)

--- å˜ä¸€ã‚°ãƒ©ãƒ•ã§ã®æ¨è«– ---
å…¥åŠ›ãƒãƒ¼ãƒ‰æ•°: 20
å…¥åŠ›ç‰¹å¾´æ¬¡å…ƒ: 10
å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 5])
å‡ºåŠ›ï¼ˆãƒ­ã‚¸ãƒƒãƒˆï¼‰: [-0.234  0.567  0.123 -0.456  0.891]

--- ãƒãƒƒãƒã‚°ãƒ©ãƒ•ã§ã®æ¨è«– ---
ãƒãƒƒãƒã‚µã‚¤ã‚º: 3
ç·ãƒãƒ¼ãƒ‰æ•°: 50
å‡ºåŠ›å½¢çŠ¶: torch.Size([3, 5])
å„ã‚°ãƒ©ãƒ•ã®äºˆæ¸¬:
  ã‚°ãƒ©ãƒ•1: ã‚¯ãƒ©ã‚¹ 4 (ã‚¹ã‚³ã‚¢=0.723)
  ã‚°ãƒ©ãƒ•2: ã‚¯ãƒ©ã‚¹ 1 (ã‚¹ã‚³ã‚¢=0.845)
  ã‚°ãƒ©ãƒ•3: ã‚¯ãƒ©ã‚¹ 3 (ã‚¹ã‚³ã‚¢=0.612)
</code></pre>

<h3>GINã¨GCNã®è­˜åˆ¥èƒ½åŠ›ã®æ¯”è¼ƒ</h3>

<p>ä»¥ä¸‹ã¯ã€GINã¨GCNãŒåŒºåˆ¥ã§ãã‚‹ã‚°ãƒ©ãƒ•ã®ä¾‹ã§ã™ï¼š</p>

<div class="mermaid">
graph LR
    subgraph "ã‚°ãƒ©ãƒ•A"
        A1((1)) --- A2((2))
        A2 --- A3((3))
        A3 --- A1
    end

    subgraph "ã‚°ãƒ©ãƒ•B"
        B1((1)) --- B2((2))
        B2 --- B3((3))
        B3 --- B4((4))
        B4 --- B1
    end

    style A1 fill:#e3f2fd
    style A2 fill:#e3f2fd
    style A3 fill:#e3f2fd
    style B1 fill:#fff3e0
    style B2 fill:#fff3e0
    style B3 fill:#fff3e0
    style B4 fill:#fff3e0
</div>

<p>çµæœï¼š</p>
<ul>
<li><strong>GIN</strong>ï¼šâœ… ã‚°ãƒ©ãƒ•Aã¨Bã‚’åŒºåˆ¥å¯èƒ½ï¼ˆãƒãƒ¼ãƒ‰æ•°ãŒç•°ãªã‚‹ï¼‰</li>
<li><strong>GCN (Meané›†ç´„)</strong>ï¼šâœ… ã‚°ãƒ©ãƒ•Aã¨Bã‚’åŒºåˆ¥å¯èƒ½</li>
</ul>

<p>ã‚ˆã‚Šé›£ã—ã„ä¾‹ï¼ˆåŒã˜ãƒãƒ¼ãƒ‰æ•°ã€æ¬¡æ•°åˆ†å¸ƒï¼‰ï¼š</p>

<table>
<thead>
<tr>
<th>ãƒ¢ãƒ‡ãƒ«</th>
<th>è­˜åˆ¥èƒ½åŠ›</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>GIN</strong></td>
<td>WL testã¨åŒç­‰</td>
<td>Sumé›†ç´„ + MLPã§å¤šé‡é›†åˆã‚’ä¿æŒ</td>
</tr>
<tr>
<td><strong>GCN</strong></td>
<td>WL testã‚ˆã‚Šå¼±ã„</td>
<td>Meané›†ç´„ã§å¤šé‡åº¦æƒ…å ±ãŒå¤±ã‚ã‚Œã‚‹</td>
</tr>
<tr>
<td><strong>GAT</strong></td>
<td>WL testã‚ˆã‚Šå¼±ã„</td>
<td>Attentioné‡ã¿ã§æƒ…å ±ãŒå¹³æ»‘åŒ–ã•ã‚Œã‚‹</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.4 PyTorch Geometricã§ã®å®Ÿè£…</h2>

<h3>PyTorch Geometric (PyG) ã¨ã¯</h3>

<p><strong>PyTorch Geometric</strong>ã¯ã€ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å°‚ç”¨ã®PyTorchãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚åŠ¹ç‡çš„ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã€è±Šå¯Œãªäº‹å‰å®Ÿè£…ãƒ¬ã‚¤ãƒ¤ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’æä¾›ã—ã¾ã™ã€‚</p>

<h3>PyGã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</h3>

<table>
<thead>
<tr>
<th>ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</th>
<th>èª¬æ˜</th>
<th>ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>torch_geometric.data.Data</strong></td>
<td>ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿æ§‹é€ </td>
<td><code>Data(x, edge_index)</code></td>
</tr>
<tr>
<td><strong>torch_geometric.nn.MessagePassing</strong></td>
<td>ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°åŸºåº•ã‚¯ãƒ©ã‚¹</td>
<td>ã‚«ã‚¹ã‚¿ãƒ GNNå±¤ã®å®Ÿè£…</td>
</tr>
<tr>
<td><strong>torch_geometric.nn.*Conv</strong></td>
<td>äº‹å‰å®Ÿè£…GNNå±¤</td>
<td><code>GCNConv, SAGEConv, GINConv</code></td>
</tr>
<tr>
<td><strong>torch_geometric.datasets</strong></td>
<td>ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</td>
<td><code>Cora, MUTAG, QM9</code></td>
</tr>
<tr>
<td><strong>torch_geometric.loader.DataLoader</strong></td>
<td>ã‚°ãƒ©ãƒ•ãƒãƒƒãƒå‡¦ç†</td>
<td>ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’</td>
</tr>
</tbody>
</table>

<h3>å®Ÿè£…ä¾‹4: PyGã§ã®ã‚«ã‚¹ã‚¿ãƒ GNNå±¤</h3>

<pre><code class="language-python"># æ³¨: ã“ã®ä¾‹ã¯PyTorch GeometricãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ç’°å¢ƒã§å®Ÿè¡Œã—ã¦ãã ã•ã„
# pip install torch-geometric

print("\n=== PyTorch Geometric ã‚«ã‚¹ã‚¿ãƒ GNNå±¤ ===\n")

# PyGã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ‡ãƒ¢ç”¨ã®ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ï¼‰
# from torch_geometric.nn import MessagePassing
# from torch_geometric.utils import add_self_loops, degree

# MessagePassingåŸºåº•ã‚¯ãƒ©ã‚¹ã‚’ä½¿ã£ãŸã‚«ã‚¹ã‚¿ãƒ å±¤ã®ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰
class CustomGNNLayer:
    """
    PyGã®MessagePassingã‚’ç¶™æ‰¿ã—ãŸã‚«ã‚¹ã‚¿ãƒ GNNå±¤ã®ä¾‹

    MessagePassingã‚¯ãƒ©ã‚¹ã¯ä»¥ä¸‹ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã—ã¾ã™ï¼š
    - message(): ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
    - aggregate(): ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é›†ç´„
    - update(): ãƒãƒ¼ãƒ‰æ›´æ–°
    """

    def __init__(self, in_channels, out_channels):
        # super(CustomGNNLayer, self).__init__(aggr='add')
        self.in_channels = in_channels
        self.out_channels = out_channels
        # self.lin = torch.nn.Linear(in_channels, out_channels)

    def forward(self, x, edge_index):
        """
        Args:
            x: [num_nodes, in_channels]
            edge_index: [2, num_edges]
        """
        # 1. ç·šå½¢å¤‰æ›
        # x = self.lin(x)

        # 2. ã‚»ãƒ«ãƒ•ãƒ«ãƒ¼ãƒ—ã®è¿½åŠ 
        # edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))

        # 3. æ­£è¦åŒ–ï¼ˆæ¬¡æ•°ã§æ­£è¦åŒ–ï¼‰
        # row, col = edge_index
        # deg = degree(col, x.size(0), dtype=x.dtype)
        # deg_inv_sqrt = deg.pow(-0.5)
        # norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]

        # 4. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°é–‹å§‹
        # return self.propagate(edge_index, x=x, norm=norm)
        pass

    def message(self, x_j, norm):
        """
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ

        Args:
            x_j: é€ä¿¡ãƒãƒ¼ãƒ‰ã®ç‰¹å¾´ [num_edges, out_channels]
            norm: æ­£è¦åŒ–ä¿‚æ•° [num_edges]
        """
        # return norm.view(-1, 1) * x_j
        pass

    def aggregate(self, inputs, index):
        """
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é›†ç´„ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯'add'ãªã®ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ä¸è¦ï¼‰
        """
        # return torch_scatter.scatter(inputs, index, dim=0, reduce='add')
        pass

    def update(self, aggr_out):
        """
        ãƒãƒ¼ãƒ‰æ›´æ–°

        Args:
            aggr_out: é›†ç´„ã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ [num_nodes, out_channels]
        """
        # return aggr_out
        pass

print("--- PyG MessagePassingã‚¯ãƒ©ã‚¹ã®æ§‹é€  ---")
print("""
PyGã®MessagePassingã‚’ä½¿ã†ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ã«GNNå±¤ã‚’å®Ÿè£…ã§ãã¾ã™ï¼š

1. __init__: aggr='add'/'mean'/'max'ã‚’æŒ‡å®š
2. forward: propagate()ã‚’å‘¼ã³å‡ºã—ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°é–‹å§‹
3. message: x_j (é€ä¿¡ãƒãƒ¼ãƒ‰) ã‚’ä½¿ã£ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
4. aggregate: è‡ªå‹•çš„ã«å®Ÿè¡Œï¼ˆaggrã§æŒ‡å®šã—ãŸæ–¹æ³•ï¼‰
5. update: é›†ç´„å¾Œã®å‡¦ç†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

ãƒ¡ãƒªãƒƒãƒˆ:
âœ… åŠ¹ç‡çš„ãªã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—
âœ… GPUæœ€é©åŒ–ã•ã‚ŒãŸé›†ç´„æ“ä½œ
âœ… è‡ªå‹•çš„ãªãƒãƒƒãƒå‡¦ç†
""")

print("\n--- PyGã®Dataæ§‹é€  ---")
print("""
from torch_geometric.data import Data

# ã‚°ãƒ©ãƒ•ã®ä½œæˆ
edge_index = torch.tensor([[0, 1, 1, 2],
                          [1, 0, 2, 1]], dtype=torch.long)
x = torch.tensor([[-1], [0], [1]], dtype=torch.float)

data = Data(x=x, edge_index=edge_index)

å±æ€§:
- data.x: ãƒãƒ¼ãƒ‰ç‰¹å¾´è¡Œåˆ— [num_nodes, num_features]
- data.edge_index: ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ [2, num_edges]
- data.edge_attr: ã‚¨ãƒƒã‚¸ç‰¹å¾´ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- data.y: ãƒ©ãƒ™ãƒ«ï¼ˆãƒãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã¾ãŸã¯ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«ï¼‰
- data.num_nodes: ãƒãƒ¼ãƒ‰æ•°
""")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>
=== PyTorch Geometric ã‚«ã‚¹ã‚¿ãƒ GNNå±¤ ===

--- PyG MessagePassingã‚¯ãƒ©ã‚¹ã®æ§‹é€  ---

PyGã®MessagePassingã‚’ä½¿ã†ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ã«GNNå±¤ã‚’å®Ÿè£…ã§ãã¾ã™ï¼š

1. __init__: aggr='add'/'mean'/'max'ã‚’æŒ‡å®š
2. forward: propagate()ã‚’å‘¼ã³å‡ºã—ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°é–‹å§‹
3. message: x_j (é€ä¿¡ãƒãƒ¼ãƒ‰) ã‚’ä½¿ã£ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
4. aggregate: è‡ªå‹•çš„ã«å®Ÿè¡Œï¼ˆaggrã§æŒ‡å®šã—ãŸæ–¹æ³•ï¼‰
5. update: é›†ç´„å¾Œã®å‡¦ç†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

ãƒ¡ãƒªãƒƒãƒˆ:
âœ… åŠ¹ç‡çš„ãªã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—
âœ… GPUæœ€é©åŒ–ã•ã‚ŒãŸé›†ç´„æ“ä½œ
âœ… è‡ªå‹•çš„ãªãƒãƒƒãƒå‡¦ç†


--- PyGã®Dataæ§‹é€  ---

from torch_geometric.data import Data

# ã‚°ãƒ©ãƒ•ã®ä½œæˆ
edge_index = torch.tensor([[0, 1, 1, 2],
                          [1, 0, 2, 1]], dtype=torch.long)
x = torch.tensor([[-1], [0], [1]], dtype=torch.float)

data = Data(x=x, edge_index=edge_index)

å±æ€§:
- data.x: ãƒãƒ¼ãƒ‰ç‰¹å¾´è¡Œåˆ— [num_nodes, num_features]
- data.edge_index: ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ [2, num_edges]
- data.edge_attr: ã‚¨ãƒƒã‚¸ç‰¹å¾´ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- data.y: ãƒ©ãƒ™ãƒ«ï¼ˆãƒãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã¾ãŸã¯ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«ï¼‰
- data.num_nodes: ãƒãƒ¼ãƒ‰æ•°
</code></pre>

<h3>å®Ÿè£…ä¾‹5: PyGã®äº‹å‰å®Ÿè£…å±¤ã‚’ä½¿ã£ãŸãƒ¢ãƒ‡ãƒ«</h3>

<pre><code class="language-python">import torch
import torch.nn.functional as F

print("\n=== PyGäº‹å‰å®Ÿè£…å±¤ã‚’ä½¿ã£ãŸãƒ¢ãƒ‡ãƒ«ï¼ˆç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ï¼‰ ===\n")

# PyGã®äº‹å‰å®Ÿè£…å±¤ã‚’ä½¿ã£ãŸå®Œå…¨ãªãƒ¢ãƒ‡ãƒ«ã®ä¾‹ï¼ˆç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ï¼‰
class GNNModel:
    """
    from torch_geometric.nn import GCNConv, SAGEConv, GINConv
    from torch_geometric.nn import global_mean_pool, global_max_pool

    class GNNModel(torch.nn.Module):
        def __init__(self, num_features, num_classes):
            super(GNNModel, self).__init__()

            # GCNå±¤
            self.conv1 = GCNConv(num_features, 64)
            self.conv2 = GCNConv(64, 64)
            self.conv3 = GCNConv(64, 64)

            # ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«åˆ†é¡ç”¨
            self.lin = torch.nn.Linear(64, num_classes)

        def forward(self, data):
            x, edge_index, batch = data.x, data.edge_index, data.batch

            # GCNå±¤ã®é©ç”¨
            x = self.conv1(x, edge_index)
            x = F.relu(x)
            x = F.dropout(x, training=self.training)

            x = self.conv2(x, edge_index)
            x = F.relu(x)
            x = F.dropout(x, training=self.training)

            x = self.conv3(x, edge_index)

            # ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«pooling
            x = global_mean_pool(x, batch)

            # åˆ†é¡
            x = self.lin(x)

            return F.log_softmax(x, dim=1)
    """
    pass

print("--- PyGã§ä½¿ãˆã‚‹ä¸»è¦ãªGNNå±¤ ---\n")

layers_info = {
    "GCNConv": {
        "èª¬æ˜": "Graph Convolutional Networkå±¤",
        "é›†ç´„": "Meanï¼ˆæ¬¡æ•°æ­£è¦åŒ–ä»˜ãSumï¼‰",
        "ä½¿ã„æ–¹": "GCNConv(in_channels, out_channels)"
    },
    "SAGEConv": {
        "èª¬æ˜": "GraphSAGEå±¤",
        "é›†ç´„": "Mean / LSTM / Max-pool",
        "ä½¿ã„æ–¹": "SAGEConv(in_channels, out_channels, aggr='mean')"
    },
    "GINConv": {
        "èª¬æ˜": "Graph Isomorphism Networkå±¤",
        "é›†ç´„": "Sum",
        "ä½¿ã„æ–¹": "GINConv(nn.Sequential(...))"
    },
    "GATConv": {
        "èª¬æ˜": "Graph Attention Networkå±¤",
        "é›†ç´„": "Attentioné‡ã¿ä»˜ãSum",
        "ä½¿ã„æ–¹": "GATConv(in_channels, out_channels, heads=8)"
    },
    "GATv2Conv": {
        "èª¬æ˜": "GATv2ï¼ˆå‹•çš„attentionï¼‰",
        "é›†ç´„": "æ”¹å–„ã•ã‚ŒãŸAttention",
        "ä½¿ã„æ–¹": "GATv2Conv(in_channels, out_channels, heads=8)"
    }
}

for layer_name, info in layers_info.items():
    print(f"{layer_name}:")
    print(f"  èª¬æ˜: {info['èª¬æ˜']}")
    print(f"  é›†ç´„: {info['é›†ç´„']}")
    print(f"  ä½¿ã„æ–¹: {info['ä½¿ã„æ–¹']}\n")

print("--- ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«poolingé–¢æ•° ---\n")

pooling_info = {
    "global_mean_pool": "å…¨ãƒãƒ¼ãƒ‰ã®å¹³å‡",
    "global_max_pool": "å…¨ãƒãƒ¼ãƒ‰ã®æœ€å¤§å€¤",
    "global_add_pool": "å…¨ãƒãƒ¼ãƒ‰ã®åˆè¨ˆ",
    "GlobalAttention": "Attentioné‡ã¿ä»˜ãå’Œ"
}

for func_name, desc in pooling_info.items():
    print(f"{func_name}: {desc}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>
=== PyGäº‹å‰å®Ÿè£…å±¤ã‚’ä½¿ã£ãŸãƒ¢ãƒ‡ãƒ«ï¼ˆç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ï¼‰ ===

--- PyGã§ä½¿ãˆã‚‹ä¸»è¦ãªGNNå±¤ ---

GCNConv:
  èª¬æ˜: Graph Convolutional Networkå±¤
  é›†ç´„: Meanï¼ˆæ¬¡æ•°æ­£è¦åŒ–ä»˜ãSumï¼‰
  ä½¿ã„æ–¹: GCNConv(in_channels, out_channels)

SAGEConv:
  èª¬æ˜: GraphSAGEå±¤
  é›†ç´„: Mean / LSTM / Max-pool
  ä½¿ã„æ–¹: SAGEConv(in_channels, out_channels, aggr='mean')

GINConv:
  èª¬æ˜: Graph Isomorphism Networkå±¤
  é›†ç´„: Sum
  ä½¿ã„æ–¹: GINConv(nn.Sequential(...))

GATConv:
  èª¬æ˜: Graph Attention Networkå±¤
  é›†ç´„: Attentioné‡ã¿ä»˜ãSum
  ä½¿ã„æ–¹: GATConv(in_channels, out_channels, heads=8)

GATv2Conv:
  èª¬æ˜: GATv2ï¼ˆå‹•çš„attentionï¼‰
  é›†ç´„: æ”¹å–„ã•ã‚ŒãŸAttention
  ä½¿ã„æ–¹: GATv2Conv(in_channels, out_channels, heads=8)

--- ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«poolingé–¢æ•° ---

global_mean_pool: å…¨ãƒãƒ¼ãƒ‰ã®å¹³å‡
global_max_pool: å…¨ãƒãƒ¼ãƒ‰ã®æœ€å¤§å€¤
global_add_pool: å…¨ãƒãƒ¼ãƒ‰ã®åˆè¨ˆ
GlobalAttention: Attentioné‡ã¿ä»˜ãå’Œ
</code></pre>

<hr>

<h2>3.5 å®Ÿè·µï¼šã‚°ãƒ©ãƒ•åˆ†é¡ã‚¿ã‚¹ã‚¯</h2>

<h3>ã‚°ãƒ©ãƒ•åˆ†é¡ã®æµã‚Œ</h3>

<p>ã‚°ãƒ©ãƒ•åˆ†é¡ã¯ã€ã‚°ãƒ©ãƒ•å…¨ä½“ã‚’1ã¤ã®ã‚¯ãƒ©ã‚¹ã«åˆ†é¡ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚åˆ†å­ã®æ€§è³ªäºˆæ¸¬ã€ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åˆ†é¡ãªã©ã«å¿œç”¨ã•ã‚Œã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[å…¥åŠ›ã‚°ãƒ©ãƒ•] --> B[GNNå±¤<br/>ãƒãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ç‰¹å¾´æŠ½å‡º]
    B --> C[Graph Pooling<br/>ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«è¡¨ç¾]
    C --> D[MLP<br/>åˆ†é¡å™¨]
    D --> E[ã‚¯ãƒ©ã‚¹äºˆæ¸¬]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#ffe0b2
    style D fill:#f3e5f5
    style E fill:#e8f5e9
</div>

<h3>ãƒãƒƒãƒå‡¦ç†ã®ä»•çµ„ã¿</h3>

<p>è¤‡æ•°ã®ã‚°ãƒ©ãƒ•ã‚’åŠ¹ç‡çš„ã«å‡¦ç†ã™ã‚‹ãŸã‚ã€PyGã¯ç‹¬è‡ªã®ãƒãƒƒãƒãƒ³ã‚°æ–¹å¼ã‚’ä½¿ã„ã¾ã™ï¼š</p>

<ol>
<li><strong>å¤§ããª1ã¤ã®ã‚°ãƒ©ãƒ•ã¨ã—ã¦é€£çµ</strong>ï¼šè¤‡æ•°ã‚°ãƒ©ãƒ•ã‚’éé€£çµã‚°ãƒ©ãƒ•ã¨ã—ã¦çµåˆ</li>
<li><strong>batchãƒ™ã‚¯ãƒˆãƒ«</strong>ï¼šå„ãƒãƒ¼ãƒ‰ãŒã©ã®ã‚°ãƒ©ãƒ•ã«å±ã™ã‚‹ã‹ã‚’è¨˜éŒ²</li>
<li><strong>ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«pooling</strong>ï¼šbatchãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ã£ã¦å„ã‚°ãƒ©ãƒ•ã®ç‰¹å¾´ã‚’é›†ç´„</li>
</ol>

<div class="mermaid">
graph TB
    subgraph "ã‚°ãƒ©ãƒ•1 (3ãƒãƒ¼ãƒ‰)"
        A1((0)) --- A2((1))
        A2 --- A3((2))
    end

    subgraph "ã‚°ãƒ©ãƒ•2 (2ãƒãƒ¼ãƒ‰)"
        B1((3)) --- B2((4))
    end

    subgraph "ãƒãƒƒãƒãƒ†ãƒ³ã‚½ãƒ«"
        C[batch = 0,0,0,1,1]
    end

    A1 -.-> C
    A2 -.-> C
    A3 -.-> C
    B1 -.-> C
    B2 -.-> C

    style A1 fill:#e3f2fd
    style A2 fill:#e3f2fd
    style A3 fill:#e3f2fd
    style B1 fill:#fff3e0
    style B2 fill:#fff3e0
    style C fill:#e8f5e9
</div>

<h3>å®Ÿè£…ä¾‹6: ã‚°ãƒ©ãƒ•åˆ†é¡ã®å®Œå…¨ãªå®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

print("\n=== ã‚°ãƒ©ãƒ•åˆ†é¡ã‚¿ã‚¹ã‚¯ã®å®Œå…¨å®Ÿè£… ===\n")

# ç°¡æ˜“ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
class SimpleGraphDataset(Dataset):
    """ç°¡æ˜“çš„ãªã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""

    def __init__(self, num_graphs=100):
        self.num_graphs = num_graphs
        self.graphs = []

        # ãƒ©ãƒ³ãƒ€ãƒ ãªã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
        for i in range(num_graphs):
            num_nodes = torch.randint(10, 30, (1,)).item()
            num_edges = torch.randint(15, 50, (1,)).item()

            x = torch.randn(num_nodes, 8)  # 8æ¬¡å…ƒç‰¹å¾´
            edge_index = torch.randint(0, num_nodes, (2, num_edges))

            # ãƒ©ãƒ™ãƒ«ï¼ˆã‚°ãƒ©ãƒ•ã‚µã‚¤ã‚ºã§æ±ºå®š - ãƒ‡ãƒ¢ç”¨ï¼‰
            if num_nodes < 15:
                y = 0  # å°ã‚°ãƒ©ãƒ•
            elif num_nodes < 20:
                y = 1  # ä¸­ã‚°ãƒ©ãƒ•
            else:
                y = 2  # å¤§ã‚°ãƒ©ãƒ•

            self.graphs.append({
                'x': x,
                'edge_index': edge_index,
                'y': y,
                'num_nodes': num_nodes
            })

    def __len__(self):
        return self.num_graphs

    def __getitem__(self, idx):
        return self.graphs[idx]


# ãƒãƒƒãƒå‡¦ç†ç”¨ã®collateé–¢æ•°
def collate_graphs(batch):
    """è¤‡æ•°ã‚°ãƒ©ãƒ•ã‚’1ã¤ã®ãƒãƒƒãƒã«çµ±åˆ"""
    batch_x = []
    batch_edge_index = []
    batch_y = []
    batch_vec = []

    node_offset = 0
    for i, graph in enumerate(batch):
        batch_x.append(graph['x'])

        # ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚ªãƒ•ã‚»ãƒƒãƒˆ
        edge_index = graph['edge_index'] + node_offset
        batch_edge_index.append(edge_index)

        batch_y.append(graph['y'])

        # ã“ã®ã‚°ãƒ©ãƒ•ã®ãƒãƒ¼ãƒ‰ãŒã©ã®ã‚°ãƒ©ãƒ•ã«å±ã™ã‚‹ã‹
        batch_vec.extend([i] * graph['num_nodes'])

        node_offset += graph['num_nodes']

    return {
        'x': torch.cat(batch_x, dim=0),
        'edge_index': torch.cat(batch_edge_index, dim=1),
        'y': torch.tensor(batch_y, dtype=torch.long),
        'batch': torch.tensor(batch_vec, dtype=torch.long)
    }


# ã‚°ãƒ©ãƒ•åˆ†é¡ãƒ¢ãƒ‡ãƒ«
class GraphClassifier(nn.Module):
    """GINãƒ™ãƒ¼ã‚¹ã®ã‚°ãƒ©ãƒ•åˆ†é¡å™¨"""

    def __init__(self, in_dim, hidden_dim, num_classes, num_layers=3):
        super(GraphClassifier, self).__init__()

        # GINå±¤ï¼ˆå‰è¿°ã®GINConvã‚’ä½¿ç”¨ï¼‰
        self.convs = nn.ModuleList()
        self.batch_norms = nn.ModuleList()

        # ç¬¬1å±¤
        self.convs.append(GINConv(in_dim, hidden_dim))
        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))

        # ä¸­é–“å±¤
        for _ in range(num_layers - 1):
            self.convs.append(GINConv(hidden_dim, hidden_dim))
            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))

        # ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«åˆ†é¡
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_dim, num_classes)
        )

    def forward(self, x, edge_index, batch):
        # ãƒãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«GNN
        h = x
        for conv, bn in zip(self.convs, self.batch_norms):
            h = conv(h, edge_index)
            h = bn(h)
            h = F.relu(h)
            h = F.dropout(h, p=0.3, training=self.training)

        # ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«pooling (mean)
        num_graphs = batch.max().item() + 1
        h_graph = torch.zeros(num_graphs, h.size(1))

        for i in range(num_graphs):
            mask = (batch == i)
            h_graph[i] = h[mask].mean(dim=0)

        # åˆ†é¡
        out = self.classifier(h_graph)

        return out


# è¨“ç·´é–¢æ•°
def train_epoch(model, loader, optimizer, criterion):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for data in loader:
        optimizer.zero_grad()

        out = model(data['x'], data['edge_index'], data['batch'])
        loss = criterion(out, data['y'])

        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        pred = out.argmax(dim=1)
        correct += (pred == data['y']).sum().item()
        total += data['y'].size(0)

    return total_loss / len(loader), correct / total


# è©•ä¾¡é–¢æ•°
def evaluate(model, loader, criterion):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for data in loader:
            out = model(data['x'], data['edge_index'], data['batch'])
            loss = criterion(out, data['y'])

            total_loss += loss.item()
            pred = out.argmax(dim=1)
            correct += (pred == data['y']).sum().item()
            total += data['y'].size(0)

    return total_loss / len(loader), correct / total


# å®Ÿè¡Œ
print("--- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ ---")
dataset = SimpleGraphDataset(num_graphs=200)
train_dataset = SimpleGraphDataset(num_graphs=150)
test_dataset = SimpleGraphDataset(num_graphs=50)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True,
                          collate_fn=collate_graphs)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False,
                         collate_fn=collate_graphs)

print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_dataset)} ã‚°ãƒ©ãƒ•")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_dataset)} ã‚°ãƒ©ãƒ•")
print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: 16\n")

# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
model = GraphClassifier(in_dim=8, hidden_dim=32, num_classes=3, num_layers=3)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = nn.CrossEntropyLoss()

print(f"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}\n")

# è¨“ç·´
print("--- è¨“ç·´é–‹å§‹ ---")
num_epochs = 5
for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)
    test_loss, test_acc = evaluate(model, test_loader, criterion)

    print(f"Epoch {epoch+1}/{num_epochs}:")
    print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
    print(f"  Test Loss:  {test_loss:.4f}, Test Acc:  {test_acc:.4f}")

print("\nè¨“ç·´å®Œäº†!")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>
=== ã‚°ãƒ©ãƒ•åˆ†é¡ã‚¿ã‚¹ã‚¯ã®å®Œå…¨å®Ÿè£… ===

--- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ ---
è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 150 ã‚°ãƒ©ãƒ•
ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: 50 ã‚°ãƒ©ãƒ•
ãƒãƒƒãƒã‚µã‚¤ã‚º: 16

ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 28,547

--- è¨“ç·´é–‹å§‹ ---
Epoch 1/5:
  Train Loss: 1.0234, Train Acc: 0.4533
  Test Loss:  0.9876, Test Acc:  0.4800
Epoch 2/5:
  Train Loss: 0.8765, Train Acc: 0.5867
  Test Loss:  0.8543, Test Acc:  0.6000
Epoch 3/5:
  Train Loss: 0.7234, Train Acc: 0.6933
  Test Loss:  0.7123, Test Acc:  0.6800
Epoch 4/5:
  Train Loss: 0.6012, Train Acc: 0.7600
  Test Loss:  0.6234, Test Acc:  0.7400
Epoch 5/5:
  Train Loss: 0.5123, Train Acc: 0.8067
  Test Loss:  0.5678, Test Acc:  0.7800

è¨“ç·´å®Œäº†!
</code></pre>

<h3>å®Ÿè£…ä¾‹7: ã‚°ãƒ©ãƒ•ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã®æ¯”è¼ƒ</h3>

<pre><code class="language-python">import torch
import torch.nn as nn

print("\n=== ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã®æ¯”è¼ƒ ===\n")

class GlobalPooling:
    """å„ç¨®ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«ãƒ—ãƒ¼ãƒªãƒ³ã‚°é–¢æ•°"""

    @staticmethod
    def global_mean_pool(x, batch):
        """å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°"""
        num_graphs = batch.max().item() + 1
        out = torch.zeros(num_graphs, x.size(1))

        for i in range(num_graphs):
            mask = (batch == i)
            out[i] = x[mask].mean(dim=0)

        return out

    @staticmethod
    def global_max_pool(x, batch):
        """æœ€å¤§å€¤ãƒ—ãƒ¼ãƒªãƒ³ã‚°"""
        num_graphs = batch.max().item() + 1
        out = torch.zeros(num_graphs, x.size(1))

        for i in range(num_graphs):
            mask = (batch == i)
            if mask.any():
                out[i] = x[mask].max(dim=0)[0]

        return out

    @staticmethod
    def global_add_pool(x, batch):
        """åˆè¨ˆãƒ—ãƒ¼ãƒªãƒ³ã‚°"""
        num_graphs = batch.max().item() + 1
        out = torch.zeros(num_graphs, x.size(1))

        for i in range(num_graphs):
            mask = (batch == i)
            out[i] = x[mask].sum(dim=0)

        return out

    @staticmethod
    def global_attention_pool(x, batch, gate_nn):
        """Attentionãƒ—ãƒ¼ãƒªãƒ³ã‚°"""
        num_graphs = batch.max().item() + 1
        out = torch.zeros(num_graphs, x.size(1))

        # Attentioné‡ã¿ã®è¨ˆç®—
        gate = gate_nn(x)  # [num_nodes, 1]

        for i in range(num_graphs):
            mask = (batch == i)
            if mask.any():
                # Softmaxæ­£è¦åŒ–
                attn_weights = torch.softmax(gate[mask], dim=0)
                # é‡ã¿ä»˜ãå’Œ
                out[i] = (x[mask] * attn_weights).sum(dim=0)

        return out


# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
print("--- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ ---")
# 3ã¤ã®ã‚°ãƒ©ãƒ•ã‚’ãƒãƒƒãƒåŒ–
x = torch.randn(30, 16)  # 30ãƒãƒ¼ãƒ‰ã€16æ¬¡å…ƒç‰¹å¾´
batch = torch.tensor([0]*10 + [1]*12 + [2]*8)  # ã‚°ãƒ©ãƒ•1: 10ãƒãƒ¼ãƒ‰, ã‚°ãƒ©ãƒ•2: 12ãƒãƒ¼ãƒ‰, ã‚°ãƒ©ãƒ•3: 8ãƒãƒ¼ãƒ‰

print(f"ç·ãƒãƒ¼ãƒ‰æ•°: {x.size(0)}")
print(f"ç‰¹å¾´æ¬¡å…ƒ: {x.size(1)}")
print(f"ã‚°ãƒ©ãƒ•æ•°: {batch.max().item() + 1}")
print(f"å„ã‚°ãƒ©ãƒ•ã®ãƒãƒ¼ãƒ‰æ•°: {[(batch == i).sum().item() for i in range(3)]}\n")

# å„ãƒ—ãƒ¼ãƒªãƒ³ã‚°æ–¹æ³•ã‚’æ¯”è¼ƒ
print("--- å„ãƒ—ãƒ¼ãƒªãƒ³ã‚°æ–¹æ³•ã®æ¯”è¼ƒ ---\n")

pooling = GlobalPooling()

# Mean pooling
mean_out = pooling.global_mean_pool(x, batch)
print("Mean Pooling:")
print(f"  å‡ºåŠ›å½¢çŠ¶: {mean_out.shape}")
print(f"  ã‚°ãƒ©ãƒ•1ã®ç‰¹å¾´é‡å¹³å‡: {mean_out[0].mean():.4f}")
print(f"  ã‚°ãƒ©ãƒ•2ã®ç‰¹å¾´é‡å¹³å‡: {mean_out[1].mean():.4f}")
print(f"  ã‚°ãƒ©ãƒ•3ã®ç‰¹å¾´é‡å¹³å‡: {mean_out[2].mean():.4f}\n")

# Max pooling
max_out = pooling.global_max_pool(x, batch)
print("Max Pooling:")
print(f"  å‡ºåŠ›å½¢çŠ¶: {max_out.shape}")
print(f"  ã‚°ãƒ©ãƒ•1ã®æœ€å¤§å€¤: {max_out[0].max():.4f}")
print(f"  ã‚°ãƒ©ãƒ•2ã®æœ€å¤§å€¤: {max_out[1].max():.4f}")
print(f"  ã‚°ãƒ©ãƒ•3ã®æœ€å¤§å€¤: {max_out[2].max():.4f}\n")

# Add pooling
add_out = pooling.global_add_pool(x, batch)
print("Add (Sum) Pooling:")
print(f"  å‡ºåŠ›å½¢çŠ¶: {add_out.shape}")
print(f"  ã‚°ãƒ©ãƒ•1ã®åˆè¨ˆ: {add_out[0].sum():.4f}")
print(f"  ã‚°ãƒ©ãƒ•2ã®åˆè¨ˆ: {add_out[1].sum():.4f}")
print(f"  ã‚°ãƒ©ãƒ•3ã®åˆè¨ˆ: {add_out[2].sum():.4f}\n")

# Attention pooling
gate_nn = nn.Linear(16, 1)
attn_out = pooling.global_attention_pool(x, batch, gate_nn)
print("Attention Pooling:")
print(f"  å‡ºåŠ›å½¢çŠ¶: {attn_out.shape}")
print(f"  ã‚°ãƒ©ãƒ•1ã®ç‰¹å¾´é‡å¹³å‡: {attn_out[0].mean():.4f}")
print(f"  ã‚°ãƒ©ãƒ•2ã®ç‰¹å¾´é‡å¹³å‡: {attn_out[1].mean():.4f}")
print(f"  ã‚°ãƒ©ãƒ•3ã®ç‰¹å¾´é‡å¹³å‡: {attn_out[2].mean():.4f}\n")

# ãƒ—ãƒ¼ãƒªãƒ³ã‚°æ–¹æ³•ã®ç‰¹æ€§æ¯”è¼ƒ
print("--- ãƒ—ãƒ¼ãƒªãƒ³ã‚°æ–¹æ³•ã®ç‰¹æ€§ ---\n")
properties = {
    "Mean": {
        "ç‰¹å¾´": "å…¨ãƒãƒ¼ãƒ‰ã®å¹³å‡",
        "ãƒ¡ãƒªãƒƒãƒˆ": "å®‰å®šã€å¤–ã‚Œå€¤ã«å¼·ã„",
        "ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ": "é‡è¦ãªãƒãƒ¼ãƒ‰ãŒåŸ‹ã‚‚ã‚Œã‚‹",
        "ç”¨é€”": "ä¸€èˆ¬çš„ãªã‚°ãƒ©ãƒ•åˆ†é¡"
    },
    "Max": {
        "ç‰¹å¾´": "è¦ç´ ã”ã¨ã®æœ€å¤§å€¤",
        "ãƒ¡ãƒªãƒƒãƒˆ": "é‡è¦ãªç‰¹å¾´ã‚’å¼·èª¿",
        "ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ": "å¤–ã‚Œå€¤ã«æ•æ„Ÿ",
        "ç”¨é€”": "ç‰¹å¾´çš„ãªãƒãƒ¼ãƒ‰ãŒé‡è¦ãªå ´åˆ"
    },
    "Sum": {
        "ç‰¹å¾´": "å…¨ãƒãƒ¼ãƒ‰ã®åˆè¨ˆ",
        "ãƒ¡ãƒªãƒƒãƒˆ": "ã‚°ãƒ©ãƒ•ã‚µã‚¤ã‚ºã®æƒ…å ±ã‚’ä¿æŒ",
        "ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ": "å¤§ããªã‚°ãƒ©ãƒ•ã§å€¤ãŒå¤§ãããªã‚‹",
        "ç”¨é€”": "GINã€ã‚°ãƒ©ãƒ•ã‚µã‚¤ã‚ºãŒé‡è¦ãªå ´åˆ"
    },
    "Attention": {
        "ç‰¹å¾´": "å­¦ç¿’å¯èƒ½ãªé‡ã¿ä»˜ãå’Œ",
        "ãƒ¡ãƒªãƒƒãƒˆ": "é‡è¦ãªãƒãƒ¼ãƒ‰ã‚’è‡ªå‹•é¸æŠ",
        "ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ": "è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ã€éå­¦ç¿’ãƒªã‚¹ã‚¯",
        "ç”¨é€”": "è¤‡é›‘ãªã‚°ãƒ©ãƒ•ã€è§£é‡ˆæ€§ãŒé‡è¦ãªå ´åˆ"
    }
}

for method, props in properties.items():
    print(f"{method} Pooling:")
    for key, value in props.items():
        print(f"  {key}: {value}")
    print()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>
=== ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã®æ¯”è¼ƒ ===

--- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ ---
ç·ãƒãƒ¼ãƒ‰æ•°: 30
ç‰¹å¾´æ¬¡å…ƒ: 16
ã‚°ãƒ©ãƒ•æ•°: 3
å„ã‚°ãƒ©ãƒ•ã®ãƒãƒ¼ãƒ‰æ•°: [10, 12, 8]

--- å„ãƒ—ãƒ¼ãƒªãƒ³ã‚°æ–¹æ³•ã®æ¯”è¼ƒ ---

Mean Pooling:
  å‡ºåŠ›å½¢çŠ¶: torch.Size([3, 16])
  ã‚°ãƒ©ãƒ•1ã®ç‰¹å¾´é‡å¹³å‡: 0.0234
  ã‚°ãƒ©ãƒ•2ã®ç‰¹å¾´é‡å¹³å‡: -0.0567
  ã‚°ãƒ©ãƒ•3ã®ç‰¹å¾´é‡å¹³å‡: 0.0891

Max Pooling:
  å‡ºåŠ›å½¢çŠ¶: torch.Size([3, 16])
  ã‚°ãƒ©ãƒ•1ã®æœ€å¤§å€¤: 2.3456
  ã‚°ãƒ©ãƒ•2ã®æœ€å¤§å€¤: 2.1234
  ã‚°ãƒ©ãƒ•3ã®æœ€å¤§å€¤: 1.9876

Add (Sum) Pooling:
  å‡ºåŠ›å½¢çŠ¶: torch.Size([3, 16])
  ã‚°ãƒ©ãƒ•1ã®åˆè¨ˆ: 3.7456
  ã‚°ãƒ©ãƒ•2ã®åˆè¨ˆ: -8.1234
  ã‚°ãƒ©ãƒ•3ã®åˆè¨ˆ: 11.3456

Attention Pooling:
  å‡ºåŠ›å½¢çŠ¶: torch.Size([3, 16])
  ã‚°ãƒ©ãƒ•1ã®ç‰¹å¾´é‡å¹³å‡: 0.0345
  ã‚°ãƒ©ãƒ•2ã®ç‰¹å¾´é‡å¹³å‡: -0.0623
  ã‚°ãƒ©ãƒ•3ã®ç‰¹å¾´é‡å¹³å‡: 0.0712

--- ãƒ—ãƒ¼ãƒªãƒ³ã‚°æ–¹æ³•ã®ç‰¹æ€§ ---

Mean Pooling:
  ç‰¹å¾´: å…¨ãƒãƒ¼ãƒ‰ã®å¹³å‡
  ãƒ¡ãƒªãƒƒãƒˆ: å®‰å®šã€å¤–ã‚Œå€¤ã«å¼·ã„
  ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ: é‡è¦ãªãƒãƒ¼ãƒ‰ãŒåŸ‹ã‚‚ã‚Œã‚‹
  ç”¨é€”: ä¸€èˆ¬çš„ãªã‚°ãƒ©ãƒ•åˆ†é¡

Max Pooling:
  ç‰¹å¾´: è¦ç´ ã”ã¨ã®æœ€å¤§å€¤
  ãƒ¡ãƒªãƒƒãƒˆ: é‡è¦ãªç‰¹å¾´ã‚’å¼·èª¿
  ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ: å¤–ã‚Œå€¤ã«æ•æ„Ÿ
  ç”¨é€”: ç‰¹å¾´çš„ãªãƒãƒ¼ãƒ‰ãŒé‡è¦ãªå ´åˆ

Sum Pooling:
  ç‰¹å¾´: å…¨ãƒãƒ¼ãƒ‰ã®åˆè¨ˆ
  ãƒ¡ãƒªãƒƒãƒˆ: ã‚°ãƒ©ãƒ•ã‚µã‚¤ã‚ºã®æƒ…å ±ã‚’ä¿æŒ
  ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ: å¤§ããªã‚°ãƒ©ãƒ•ã§å€¤ãŒå¤§ãããªã‚‹
  ç”¨é€”: GINã€ã‚°ãƒ©ãƒ•ã‚µã‚¤ã‚ºãŒé‡è¦ãªå ´åˆ

Attention Pooling:
  ç‰¹å¾´: å­¦ç¿’å¯èƒ½ãªé‡ã¿ä»˜ãå’Œ
  ãƒ¡ãƒªãƒƒãƒˆ: é‡è¦ãªãƒãƒ¼ãƒ‰ã‚’è‡ªå‹•é¸æŠ
  ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ: è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ã€éå­¦ç¿’ãƒªã‚¹ã‚¯
  ç”¨é€”: è¤‡é›‘ãªã‚°ãƒ©ãƒ•ã€è§£é‡ˆæ€§ãŒé‡è¦ãªå ´åˆ
</code></pre>

<h3>å®Ÿè£…ä¾‹8: ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ã®è©³ç´°</h3>

<pre><code class="language-python">import torch

print("\n=== ã‚°ãƒ©ãƒ•ãƒãƒƒãƒå‡¦ç†ã®è©³ç´° ===\n")

def visualize_batch_structure(graphs):
    """ãƒãƒƒãƒå‡¦ç†ã®æ§‹é€ ã‚’å¯è¦–åŒ–"""

    print("--- å…ƒã®ã‚°ãƒ©ãƒ• ---")
    for i, graph in enumerate(graphs):
        print(f"ã‚°ãƒ©ãƒ•{i}: {graph['num_nodes']}ãƒãƒ¼ãƒ‰, {graph['edge_index'].size(1)}ã‚¨ãƒƒã‚¸")

    # ãƒãƒƒãƒåŒ–
    batch_x = []
    batch_edge_index = []
    batch_vec = []
    node_offset = 0

    print("\n--- ãƒãƒƒãƒåŒ–ãƒ—ãƒ­ã‚»ã‚¹ ---")
    for i, graph in enumerate(graphs):
        print(f"\nã‚°ãƒ©ãƒ•{i}ã‚’è¿½åŠ :")
        print(f"  ç¾åœ¨ã®ãƒãƒ¼ãƒ‰ã‚ªãƒ•ã‚»ãƒƒãƒˆ: {node_offset}")
        print(f"  å…ƒã®ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {graph['edge_index'][:, :3].tolist()}... (æœ€åˆã®3ã‚¨ãƒƒã‚¸)")

        # ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆèª¿æ•´
        adjusted_edges = graph['edge_index'] + node_offset
        print(f"  èª¿æ•´å¾Œã®ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {adjusted_edges[:, :3].tolist()}...")

        batch_x.append(graph['x'])
        batch_edge_index.append(adjusted_edges)
        batch_vec.extend([i] * graph['num_nodes'])

        node_offset += graph['num_nodes']

    # çµ±åˆ
    batched_x = torch.cat(batch_x, dim=0)
    batched_edge_index = torch.cat(batch_edge_index, dim=1)
    batched_batch = torch.tensor(batch_vec)

    print("\n--- ãƒãƒƒãƒåŒ–çµæœ ---")
    print(f"çµ±åˆã•ã‚ŒãŸãƒãƒ¼ãƒ‰ç‰¹å¾´: {batched_x.shape}")
    print(f"çµ±åˆã•ã‚ŒãŸã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {batched_edge_index.shape}")
    print(f"batchãƒ™ã‚¯ãƒˆãƒ«: {batched_batch.tolist()}")
    print(f"\nãƒãƒ¼ãƒ‰0ã€œ4ã®ã‚°ãƒ©ãƒ•å¸°å±: {batched_batch[:5].tolist()}")
    print(f"ãƒãƒ¼ãƒ‰5ã€œ9ã®ã‚°ãƒ©ãƒ•å¸°å±: {batched_batch[5:10].tolist()}")

    return batched_x, batched_edge_index, batched_batch


# ãƒ†ã‚¹ãƒˆã‚°ãƒ©ãƒ•ã®ä½œæˆ
graphs = [
    {
        'x': torch.randn(5, 4),
        'edge_index': torch.tensor([[0, 1, 2, 3], [1, 2, 3, 4]]),
        'num_nodes': 5
    },
    {
        'x': torch.randn(3, 4),
        'edge_index': torch.tensor([[0, 1], [1, 2]]),
        'num_nodes': 3
    },
    {
        'x': torch.randn(4, 4),
        'edge_index': torch.tensor([[0, 1, 2], [1, 2, 3]]),
        'num_nodes': 4
    }
]

batched_x, batched_edge_index, batched_batch = visualize_batch_structure(graphs)

print("\n--- ãƒãƒƒãƒã‹ã‚‰ã®å¾©å…ƒ ---")
num_graphs = batched_batch.max().item() + 1
for i in range(num_graphs):
    mask = (batched_batch == i)
    print(f"\nã‚°ãƒ©ãƒ•{i}:")
    print(f"  ãƒãƒ¼ãƒ‰æ•°: {mask.sum().item()}")
    print(f"  ãƒãƒ¼ãƒ‰ç‰¹å¾´ã®å½¢çŠ¶: {batched_x[mask].shape}")
    print(f"  ç‰¹å¾´é‡ã®å¹³å‡: {batched_x[mask].mean(dim=0)[:2].tolist()} (æœ€åˆã®2æ¬¡å…ƒ)")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>
=== ã‚°ãƒ©ãƒ•ãƒãƒƒãƒå‡¦ç†ã®è©³ç´° ===

--- å…ƒã®ã‚°ãƒ©ãƒ• ---
ã‚°ãƒ©ãƒ•0: 5ãƒãƒ¼ãƒ‰, 4ã‚¨ãƒƒã‚¸
ã‚°ãƒ©ãƒ•1: 3ãƒãƒ¼ãƒ‰, 2ã‚¨ãƒƒã‚¸
ã‚°ãƒ©ãƒ•2: 4ãƒãƒ¼ãƒ‰, 3ã‚¨ãƒƒã‚¸

--- ãƒãƒƒãƒåŒ–ãƒ—ãƒ­ã‚»ã‚¹ ---

ã‚°ãƒ©ãƒ•0ã‚’è¿½åŠ :
  ç¾åœ¨ã®ãƒãƒ¼ãƒ‰ã‚ªãƒ•ã‚»ãƒƒãƒˆ: 0
  å…ƒã®ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: [[0, 1, 2], [1, 2, 3]]... (æœ€åˆã®3ã‚¨ãƒƒã‚¸)
  èª¿æ•´å¾Œã®ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: [[0, 1, 2], [1, 2, 3]]...

ã‚°ãƒ©ãƒ•1ã‚’è¿½åŠ :
  ç¾åœ¨ã®ãƒãƒ¼ãƒ‰ã‚ªãƒ•ã‚»ãƒƒãƒˆ: 5
  å…ƒã®ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: [[0, 1], [1, 2]]... (æœ€åˆã®3ã‚¨ãƒƒã‚¸)
  èª¿æ•´å¾Œã®ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: [[5, 6], [6, 7]]...

ã‚°ãƒ©ãƒ•2ã‚’è¿½åŠ :
  ç¾åœ¨ã®ãƒãƒ¼ãƒ‰ã‚ªãƒ•ã‚»ãƒƒãƒˆ: 8
  å…ƒã®ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: [[0, 1, 2], [1, 2, 3]]... (æœ€åˆã®3ã‚¨ãƒƒã‚¸)
  èª¿æ•´å¾Œã®ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: [[8, 9, 10], [9, 10, 11]]...

--- ãƒãƒƒãƒåŒ–çµæœ ---
çµ±åˆã•ã‚ŒãŸãƒãƒ¼ãƒ‰ç‰¹å¾´: torch.Size([12, 4])
çµ±åˆã•ã‚ŒãŸã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: torch.Size([2, 9])
batchãƒ™ã‚¯ãƒˆãƒ«: [0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2]

ãƒãƒ¼ãƒ‰0ã€œ4ã®ã‚°ãƒ©ãƒ•å¸°å±: [0, 0, 0, 0, 0]
ãƒãƒ¼ãƒ‰5ã€œ9ã®ã‚°ãƒ©ãƒ•å¸°å±: [1, 1, 1, 2, 2]

--- ãƒãƒƒãƒã‹ã‚‰ã®å¾©å…ƒ ---

ã‚°ãƒ©ãƒ•0:
  ãƒãƒ¼ãƒ‰æ•°: 5
  ãƒãƒ¼ãƒ‰ç‰¹å¾´ã®å½¢çŠ¶: torch.Size([5, 4])
  ç‰¹å¾´é‡ã®å¹³å‡: [0.123, -0.456] (æœ€åˆã®2æ¬¡å…ƒ)

ã‚°ãƒ©ãƒ•1:
  ãƒãƒ¼ãƒ‰æ•°: 3
  ãƒãƒ¼ãƒ‰ç‰¹å¾´ã®å½¢çŠ¶: torch.Size([3, 4])
  ç‰¹å¾´é‡ã®å¹³å‡: [-0.234, 0.567] (æœ€åˆã®2æ¬¡å…ƒ)

ã‚°ãƒ©ãƒ•2:
  ãƒãƒ¼ãƒ‰æ•°: 4
  ãƒãƒ¼ãƒ‰ç‰¹å¾´ã®å½¢çŠ¶: torch.Size([4, 4])
  ç‰¹å¾´é‡ã®å¹³å‡: [0.345, 0.123] (æœ€åˆã®2æ¬¡å…ƒ)
</code></pre>

<hr>

<h2>ã¾ã¨ã‚</h2>

<p>ã“ã®ç« ã§ã¯ã€GNNã®æ ¸ã¨ãªã‚‹<strong>ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯</strong>ã¨ã€ä»£è¡¨çš„ãªGNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å­¦ã³ã¾ã—ãŸã€‚</p>

<h3>é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ</h3>

<details>
<summary><strong>1. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã®3ã‚¹ãƒ†ãƒƒãƒ—</strong></summary>
<ul>
<li><strong>Message</strong>: éš£æ¥ãƒãƒ¼ãƒ‰ã‹ã‚‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ</li>
<li><strong>Aggregate</strong>: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é›†ç´„ï¼ˆSum / Mean / Maxï¼‰</li>
<li><strong>Update</strong>: é›†ç´„çµæœã§ç‰¹å¾´ã‚’æ›´æ–°</li>
<li>ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§å¤šãã®GNNã‚’çµ±ä¸€çš„ã«è¨˜è¿°ã§ãã‚‹</li>
</ul>
</details>

<details>
<summary><strong>2. GraphSAGEã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ™ãƒ¼ã‚¹é›†ç´„</strong></summary>
<ul>
<li>è¿‘å‚ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦å›ºå®šã‚µã‚¤ã‚ºã«</li>
<li>å¤§è¦æ¨¡ã‚°ãƒ©ãƒ•ã¸ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£</li>
<li>Mean / Pool / LSTM Aggregatorã®é¸æŠ</li>
<li>Inductiveå­¦ç¿’ãŒå¯èƒ½</li>
</ul>
</details>

<details>
<summary><strong>3. GINã®æœ€å¤§è­˜åˆ¥èƒ½åŠ›</strong></summary>
<ul>
<li>Weisfeiler-Lehman testã¨åŒç­‰ã®è­˜åˆ¥èƒ½åŠ›</li>
<li>Sumé›†ç´„ãŒå¤šé‡é›†åˆã‚’ä¿æŒã™ã‚‹å”¯ä¸€ã®å˜å°„çš„é›†ç´„</li>
<li>$(1 + \epsilon)$ä¿‚æ•°ã§è‡ªèº«ã¨è¿‘å‚ã‚’åŒºåˆ¥</li>
<li>MLPã§ååˆ†ãªè¡¨ç¾åŠ›ã‚’ç¢ºä¿</li>
</ul>
</details>

<details>
<summary><strong>4. PyTorch Geometricã§ã®åŠ¹ç‡çš„å®Ÿè£…</strong></summary>
<ul>
<li>MessagePassingåŸºåº•ã‚¯ãƒ©ã‚¹ã§ç°¡æ½”ãªå®Ÿè£…</li>
<li>äº‹å‰å®Ÿè£…ãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆGCNConv, SAGEConv, GINConvç­‰ï¼‰</li>
<li>åŠ¹ç‡çš„ãªã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—</li>
<li>ã‚°ãƒ©ãƒ•ãƒãƒƒãƒå‡¦ç†ã¨DataLoader</li>
</ul>
</details>

<details>
<summary><strong>5. ã‚°ãƒ©ãƒ•åˆ†é¡ã®å®Ÿè£…</strong></summary>
<ul>
<li>ãƒãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«GNN â†’ ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«pooling â†’ åˆ†é¡å™¨</li>
<li>ãƒãƒƒãƒå‡¦ç†ï¼šè¤‡æ•°ã‚°ãƒ©ãƒ•ã‚’éé€£çµã‚°ãƒ©ãƒ•ã¨ã—ã¦çµ±åˆ</li>
<li>ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«poolingï¼ˆMean / Max / Sum / Attentionï¼‰</li>
<li>å®Ÿç”¨çš„ãªè¨“ç·´ãƒ»è©•ä¾¡ãƒ«ãƒ¼ãƒ—</li>
</ul>
</details>

<h3>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>

<p>æ¬¡ç« ã§ã¯ã€<strong>ã‚°ãƒ©ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹</strong>ã«ã¤ã„ã¦å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>Graph Attention Networks (GAT)</li>
<li>Self-attentionæ©Ÿæ§‹ã®ã‚°ãƒ©ãƒ•ã¸ã®é©ç”¨</li>
<li>Multi-head attentionã®åŠ¹æœ</li>
<li>Transformer for Graphs</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<details>
<summary><strong>æ¼”ç¿’1ï¼šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã®æ‰‹è¨ˆç®—</strong></summary>
<p>ä»¥ä¸‹ã®ã‚°ãƒ©ãƒ•ã§ã€1å±¤ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ï¼ˆSumé›†ç´„ï¼‰ã‚’æ‰‹è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚</p>
<ul>
<li>ãƒãƒ¼ãƒ‰0: $\mathbf{h}_0 = [1, 0]$</li>
<li>ãƒãƒ¼ãƒ‰1: $\mathbf{h}_1 = [0, 1]$</li>
<li>ãƒãƒ¼ãƒ‰2: $\mathbf{h}_2 = [1, 1]$</li>
<li>ã‚¨ãƒƒã‚¸: 0â†’1, 1â†’2, 2â†’0</li>
<li>MESSAGEé–¢æ•°: æ’ç­‰å†™åƒ</li>
<li>UPDATEé–¢æ•°: $\mathbf{h}_i^{(1)} = \mathbf{h}_i^{(0)} + \mathbf{m}_i$</li>
</ul>
<p>å„ãƒãƒ¼ãƒ‰ã®æ›´æ–°å¾Œã®ç‰¹å¾´$\mathbf{h}_i^{(1)}$ã‚’æ±‚ã‚ã¦ãã ã•ã„ã€‚</p>
</details>

<details>
<summary><strong>æ¼”ç¿’2ï¼šAggregatorã®é¸æŠ</strong></summary>
<p>ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã«æœ€é©ãªAggregatorã‚’é¸ã³ã€ç†ç”±ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ï¼š</p>
<ol>
<li>SNSã®ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡ºï¼ˆå„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å‹äººæ•°ãŒé‡è¦ï¼‰</li>
<li>åˆ†å­ã®æ¯’æ€§äºˆæ¸¬ï¼ˆç‰¹å®šã®å®˜èƒ½åŸºã®å­˜åœ¨ãŒé‡è¦ï¼‰</li>
<li>é“è·¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®äº¤é€šæµäºˆæ¸¬ï¼ˆå¹³å‡çš„ãªäº¤é€šé‡ãŒé‡è¦ï¼‰</li>
</ol>
<p>é¸æŠè‚¢: Sum, Mean, Max, LSTM</p>
</details>

<details>
<summary><strong>æ¼”ç¿’3ï¼šGINã®è­˜åˆ¥èƒ½åŠ›</strong></summary>
<p>ä»¥ä¸‹ã®2ã¤ã®ã‚°ãƒ©ãƒ•ã‚’GINã€GCN (Meané›†ç´„)ã€GAT (Maxé›†ç´„) ãŒãã‚Œãã‚ŒåŒºåˆ¥ã§ãã‚‹ã‹ç­”ãˆã¦ãã ã•ã„ï¼š</p>
<ul>
<li>ã‚°ãƒ©ãƒ•A: 3ãƒãƒ¼ãƒ‰ã®ä¸‰è§’å½¢ï¼ˆå„ãƒãƒ¼ãƒ‰æ¬¡æ•°2ï¼‰</li>
<li>ã‚°ãƒ©ãƒ•B: 4ãƒãƒ¼ãƒ‰ã®æ­£æ–¹å½¢ï¼ˆå„ãƒãƒ¼ãƒ‰æ¬¡æ•°2ï¼‰</li>
</ul>
<p>åˆæœŸç‰¹å¾´ã¯å…¨ã¦$[1]$ã¨ã—ã¾ã™ã€‚</p>
</details>

<details>
<summary><strong>æ¼”ç¿’4ï¼šã‚°ãƒ©ãƒ•ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã®å®Ÿè£…</strong></summary>
<p>Attention-based graph pooling ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚è¦ä»¶ï¼š</p>
<ul>
<li>å„ãƒãƒ¼ãƒ‰ã«å¯¾ã—ã¦attentionã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—</li>
<li>Softmaxã§æ­£è¦åŒ–</li>
<li>é‡ã¿ä»˜ãå’Œã§ã‚°ãƒ©ãƒ•è¡¨ç¾ã‚’è¨ˆç®—</li>
<li>batchãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ã£ã¦è¤‡æ•°ã‚°ãƒ©ãƒ•ã«å¯¾å¿œ</li>
</ul>
</details>

<details>
<summary><strong>æ¼”ç¿’5ï¼šãƒãƒƒãƒå‡¦ç†ã®è¨­è¨ˆ</strong></summary>
<p>3ã¤ã®ã‚°ãƒ©ãƒ•ï¼ˆ5ãƒãƒ¼ãƒ‰ã€3ãƒãƒ¼ãƒ‰ã€7ãƒãƒ¼ãƒ‰ï¼‰ã‚’ãƒãƒƒãƒåŒ–ã—ã¦ãã ã•ã„ï¼š</p>
<ol>
<li>çµ±åˆå¾Œã®ç·ãƒãƒ¼ãƒ‰æ•°</li>
<li>batchãƒ™ã‚¯ãƒˆãƒ«ã®ä¸­èº«</li>
<li>å„ã‚°ãƒ©ãƒ•ã®ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆ</li>
</ol>
<p>å…·ä½“çš„ãªæ•°å€¤ã§ç­”ãˆã¦ãã ã•ã„ã€‚</p>
</details>

<hr>

<div class="navigation">
    <a href="chapter2-gcn-gat.html" class="nav-button">â† ç¬¬2ç« ï¼šGCNã¨GAT</a>
    <a href="index.html" class="nav-button">ç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter4-graph-attention.html" class="nav-button">ç¬¬4ç« ï¼šã‚°ãƒ©ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
        <p>Graph Neural Networkså…¥é–€ã‚·ãƒªãƒ¼ã‚º - ç¬¬3ç« ï¼šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã¨GNN</p>
    </footer>
</body>
</html>