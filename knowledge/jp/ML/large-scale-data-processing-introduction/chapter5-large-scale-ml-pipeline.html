<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬5ç« ï¼šå¤§è¦æ¨¡æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .info-box {
            background-color: #e7f3ff;
            border-left: 4px solid #2196F3;
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .warning-box {
            background-color: #fff3e0;
            border-left: 4px solid #ff9800;
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .success-box {
            background-color: #e8f5e9;
            border-left: 4px solid #4caf50;
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .exercise-box {
            background-color: var(--color-bg-alt);
            border: 2px solid var(--color-accent);
            padding: var(--spacing-md);
            margin: var(--spacing-lg) 0;
            border-radius: var(--border-radius);
        }

        .exercise-box h3 {
            color: var(--color-accent);
            margin-top: 0;
        }

        .nav-links {
            display: flex;
            justify-content: space-between;
            margin-top: var(--spacing-xl);
            padding-top: var(--spacing-lg);
            border-top: 1px solid var(--color-border);
            flex-wrap: wrap;
            gap: var(--spacing-sm);
        }

        .nav-button {
            display: inline-block;
            padding: var(--spacing-sm) var(--spacing-md);
            background-color: var(--color-accent);
            color: white;
            border-radius: var(--border-radius);
            text-decoration: none;
            transition: background-color 0.2s;
        }

        .nav-button:hover {
            background-color: var(--color-accent-light);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            color: var(--color-text-light);
            font-size: 0.9rem;
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                flex-direction: column;
                gap: var(--spacing-xs);
            }

            .nav-links {
                flex-direction: column;
            }

            .nav-button {
                text-align: center;
                width: 100%;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/large-scale-data-processing-introduction/chapter5-large-scale-ml-pipeline.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/large-scale-data-processing-introduction/index.html">Large Scale Data Processing</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 5</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬5ç« ï¼šå¤§è¦æ¨¡æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h1>
            <p class="subtitle">ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®åˆ†æ•£æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 35-40åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… å¤§è¦æ¨¡æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è¨­è¨ˆåŸå‰‡ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Sparkã¨PyTorchã‚’çµ±åˆã—ãŸåˆ†æ•£ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
<li>âœ… åˆ†æ•£ç’°å¢ƒã§ã®ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… å¤§è¦æ¨¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–æ‰‹æ³•ã‚’é©ç”¨ã§ãã‚‹</li>
<li>âœ… ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®æœ¬ç•ªç’°å¢ƒå¯¾å¿œMLã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
<li>âœ… ç›£è¦–ãƒ»ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æˆ¦ç•¥ã‚’å®Ÿè£…ã§ãã‚‹</li>
</ul>

<hr>

<h2>5.1 ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­è¨ˆåŸå‰‡</h2>

<h3>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªMLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è¦ä»¶</h3>

<p>å¤§è¦æ¨¡æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’è¨­è¨ˆã™ã‚‹éš›ã®ä¸»è¦ãªè€ƒæ…®äº‹é …ï¼š</p>

<table>
<thead>
<tr>
<th>è¦ä»¶</th>
<th>èª¬æ˜</th>
<th>å®Ÿè£…æŠ€è¡“</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£</strong></td>
<td>TBã€œPBã‚¹ã‚±ãƒ¼ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†</td>
<td>Sparkã€Daskã€åˆ†æ•£ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸</td>
</tr>
<tr>
<td><strong>è¨ˆç®—ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£</strong></td>
<td>ãƒãƒ«ãƒãƒãƒ¼ãƒ‰ä¸¦åˆ—å‡¦ç†</td>
<td>Rayã€Horovodã€Kubernetes</td>
</tr>
<tr>
<td><strong>ãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ã‚¹</strong></td>
<td>éšœå®³æ™‚ã®è‡ªå‹•ãƒªã‚«ãƒãƒªãƒ¼</td>
<td>ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã€å†—é•·åŒ–</td>
</tr>
<tr>
<td><strong>å†ç¾æ€§</strong></td>
<td>å®Ÿé¨“ãƒ»ãƒ¢ãƒ‡ãƒ«ã®å®Œå…¨ãªå†ç¾</td>
<td>ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã€ã‚·ãƒ¼ãƒ‰å›ºå®š</td>
</tr>
<tr>
<td><strong>ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°</strong></td>
<td>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½ç›£è¦–</td>
<td>Prometheusã€Grafanaã€MLflow</td>
</tr>
<tr>
<td><strong>ã‚³ã‚¹ãƒˆåŠ¹ç‡</strong></td>
<td>ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ã®æœ€é©åŒ–</td>
<td>ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€ã‚¹ãƒãƒƒãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹</td>
</tr>
</tbody>
</table>

<h3>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ‘ã‚¿ãƒ¼ãƒ³</h3>

<pre class="mermaid">
graph TB
    subgraph "ãƒ‡ãƒ¼ã‚¿å±¤"
        A[ç”Ÿãƒ‡ãƒ¼ã‚¿<br/>HDFS/S3] --> B[ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼<br/>Great Expectations]
        B --> C[åˆ†æ•£ETL<br/>Apache Spark]
    end

    subgraph "ç‰¹å¾´é‡å±¤"
        C --> D[ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°<br/>Spark ML]
        D --> E[ç‰¹å¾´é‡ã‚¹ãƒˆã‚¢<br/>Feast/Tecton]
    end

    subgraph "è¨“ç·´å±¤"
        E --> F[åˆ†æ•£è¨“ç·´<br/>Ray/Horovod]
        F --> G[ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–<br/>Ray Tune]
        G --> H[ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒª<br/>MLflow]
    end

    subgraph "æ¨è«–å±¤"
        H --> I[ãƒ¢ãƒ‡ãƒ«é…ä¿¡<br/>Kubernetes]
        I --> J[äºˆæ¸¬ã‚µãƒ¼ãƒ“ã‚¹<br/>TorchServe]
    end

    subgraph "ç›£è¦–å±¤"
        J --> K[ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†<br/>Prometheus]
        K --> L[å¯è¦–åŒ–<br/>Grafana]
        L --> M[ã‚¢ãƒ©ãƒ¼ãƒˆ<br/>PagerDuty]
    end
</pre>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹1: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­å®šã‚¯ãƒ©ã‚¹</h4>

<pre><code>"""
å¤§è¦æ¨¡MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è¨­å®šç®¡ç†
"""
from dataclasses import dataclass, field
from typing import Dict, List, Optional
import yaml
from pathlib import Path


@dataclass
class DataConfig:
    """ãƒ‡ãƒ¼ã‚¿å‡¦ç†è¨­å®š"""
    source_path: str
    output_path: str
    num_partitions: int = 1000
    file_format: str = "parquet"
    compression: str = "snappy"
    validation_rules: Dict[str, any] = field(default_factory=dict)


@dataclass
class TrainingConfig:
    """è¨“ç·´è¨­å®š"""
    model_type: str
    num_workers: int = 4
    num_gpus_per_worker: int = 1
    batch_size: int = 256
    max_epochs: int = 100
    learning_rate: float = 0.001
    checkpoint_freq: int = 10
    early_stopping_patience: int = 5


@dataclass
class ResourceConfig:
    """ãƒªã‚½ãƒ¼ã‚¹è¨­å®š"""
    num_nodes: int = 4
    cpus_per_node: int = 16
    memory_per_node: str = "64GB"
    gpus_per_node: int = 4
    storage_type: str = "ssd"
    network_bandwidth: str = "10Gbps"


@dataclass
class MonitoringConfig:
    """ç›£è¦–è¨­å®š"""
    metrics_interval: int = 60  # ç§’
    log_level: str = "INFO"
    alert_thresholds: Dict[str, float] = field(default_factory=dict)
    dashboard_url: Optional[str] = None


@dataclass
class PipelineConfig:
    """çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­å®š"""
    pipeline_name: str
    version: str
    data: DataConfig
    training: TrainingConfig
    resources: ResourceConfig
    monitoring: MonitoringConfig

    @classmethod
    def from_yaml(cls, config_path: Path) -> 'PipelineConfig':
        """YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã‚€"""
        with open(config_path) as f:
            config_dict = yaml.safe_load(f)

        return cls(
            pipeline_name=config_dict['pipeline_name'],
            version=config_dict['version'],
            data=DataConfig(**config_dict['data']),
            training=TrainingConfig(**config_dict['training']),
            resources=ResourceConfig(**config_dict['resources']),
            monitoring=MonitoringConfig(**config_dict['monitoring'])
        )

    def to_yaml(self, output_path: Path):
        """è¨­å®šã‚’YAMLãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        config_dict = {
            'pipeline_name': self.pipeline_name,
            'version': self.version,
            'data': self.data.__dict__,
            'training': self.training.__dict__,
            'resources': self.resources.__dict__,
            'monitoring': self.monitoring.__dict__
        }

        with open(output_path, 'w') as f:
            yaml.dump(config_dict, f, default_flow_style=False)


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # è¨­å®šä½œæˆ
    config = PipelineConfig(
        pipeline_name="customer_churn_prediction",
        version="v1.0.0",
        data=DataConfig(
            source_path="s3://my-bucket/raw-data/",
            output_path="s3://my-bucket/processed-data/",
            num_partitions=2000,
            validation_rules={
                "min_records": 1000000,
                "required_columns": ["customer_id", "features", "label"]
            }
        ),
        training=TrainingConfig(
            model_type="neural_network",
            num_workers=8,
            num_gpus_per_worker=2,
            batch_size=512,
            max_epochs=50
        ),
        resources=ResourceConfig(
            num_nodes=8,
            cpus_per_node=32,
            memory_per_node="128GB",
            gpus_per_node=4
        ),
        monitoring=MonitoringConfig(
            alert_thresholds={
                "accuracy": 0.85,
                "latency_p99": 100.0,  # ãƒŸãƒªç§’
                "error_rate": 0.01
            }
        )
    )

    # è¨­å®šä¿å­˜
    config.to_yaml(Path("pipeline_config.yaml"))

    # è¨­å®šèª­ã¿è¾¼ã¿
    loaded_config = PipelineConfig.from_yaml(Path("pipeline_config.yaml"))
    print(f"Pipeline: {loaded_config.pipeline_name}")
    print(f"Workers: {loaded_config.training.num_workers}")
    print(f"Partitions: {loaded_config.data.num_partitions}")
</code></pre>

<div class="info-box">
<strong>ğŸ’¡ è¨­è¨ˆã®ãƒã‚¤ãƒ³ãƒˆï¼š</strong>
<ul>
<li>è¨­å®šã‚’ç’°å¢ƒå¤‰æ•°ã‚„ã‚³ãƒ¼ãƒ‰ã«åŸ‹ã‚è¾¼ã¾ãšã€YAMLã§å¤–éƒ¨åŒ–</li>
<li>ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã§å‹å®‰å…¨æ€§ã‚’ç¢ºä¿</li>
<li>ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã§è¨­å®šã®å±¥æ­´ã‚’è¿½è·¡</li>
<li>ç’°å¢ƒï¼ˆé–‹ç™º/æœ¬ç•ªï¼‰ã”ã¨ã«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†é›¢</li>
</ul>
</div>

<h3>ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥</h3>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹2: å …ç‰¢ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯</h4>

<pre><code>"""
ãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ãƒˆãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
"""
import time
import logging
from typing import Callable, Any, Optional, List
from functools import wraps
from dataclasses import dataclass
from enum import Enum


class TaskStatus(Enum):
    """ã‚¿ã‚¹ã‚¯å®Ÿè¡ŒçŠ¶æ…‹"""
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    RETRYING = "retrying"


@dataclass
class TaskResult:
    """ã‚¿ã‚¹ã‚¯å®Ÿè¡Œçµæœ"""
    status: TaskStatus
    result: Any = None
    error: Optional[Exception] = None
    execution_time: float = 0.0
    retry_count: int = 0


class RetryStrategy:
    """ãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥"""

    def __init__(
        self,
        max_retries: int = 3,
        initial_delay: float = 1.0,
        backoff_factor: float = 2.0,
        max_delay: float = 60.0,
        retryable_exceptions: List[type] = None
    ):
        self.max_retries = max_retries
        self.initial_delay = initial_delay
        self.backoff_factor = backoff_factor
        self.max_delay = max_delay
        self.retryable_exceptions = retryable_exceptions or [Exception]

    def get_delay(self, retry_count: int) -> float:
        """ãƒªãƒˆãƒ©ã‚¤å¾…æ©Ÿæ™‚é–“ã‚’è¨ˆç®—ï¼ˆæŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‰"""
        delay = self.initial_delay * (self.backoff_factor ** retry_count)
        return min(delay, self.max_delay)

    def should_retry(self, exception: Exception) -> bool:
        """ãƒªãƒˆãƒ©ã‚¤ã™ã¹ãã‹åˆ¤å®š"""
        return any(isinstance(exception, exc_type)
                  for exc_type in self.retryable_exceptions)


def with_retry(retry_strategy: RetryStrategy):
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ã‚’è¿½åŠ ã™ã‚‹ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs) -> TaskResult:
            retry_count = 0
            start_time = time.time()

            while retry_count <= retry_strategy.max_retries:
                try:
                    result = func(*args, **kwargs)
                    execution_time = time.time() - start_time

                    return TaskResult(
                        status=TaskStatus.SUCCESS,
                        result=result,
                        execution_time=execution_time,
                        retry_count=retry_count
                    )

                except Exception as e:
                    if (retry_count < retry_strategy.max_retries and
                        retry_strategy.should_retry(e)):

                        delay = retry_strategy.get_delay(retry_count)
                        logging.warning(
                            f"Task failed (attempt {retry_count + 1}/"
                            f"{retry_strategy.max_retries + 1}): {str(e)}"
                            f"\nRetrying in {delay:.1f} seconds..."
                        )

                        time.sleep(delay)
                        retry_count += 1
                    else:
                        execution_time = time.time() - start_time
                        return TaskResult(
                            status=TaskStatus.FAILED,
                            error=e,
                            execution_time=execution_time,
                            retry_count=retry_count
                        )

            return TaskResult(
                status=TaskStatus.FAILED,
                error=Exception("Max retries exceeded"),
                execution_time=time.time() - start_time,
                retry_count=retry_count
            )

        return wrapper
    return decorator


class PipelineExecutor:
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self, checkpoint_dir: str = "./checkpoints"):
        self.checkpoint_dir = checkpoint_dir
        self.logger = logging.getLogger(__name__)

    def execute_stage(
        self,
        stage_name: str,
        task_func: Callable,
        retry_strategy: Optional[RetryStrategy] = None,
        checkpoint: bool = True
    ) -> TaskResult:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’å®Ÿè¡Œ"""
        self.logger.info(f"Starting stage: {stage_name}")

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚Œã°å¾©å…ƒ
        if checkpoint and self._checkpoint_exists(stage_name):
            self.logger.info(f"Restoring from checkpoint: {stage_name}")
            return self._load_checkpoint(stage_name)

        # ãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥ã‚’é©ç”¨
        if retry_strategy:
            task_func = with_retry(retry_strategy)(task_func)

        # ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
        result = task_func()

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        if checkpoint and result.status == TaskStatus.SUCCESS:
            self._save_checkpoint(stage_name, result)

        self.logger.info(
            f"Stage {stage_name} completed: {result.status.value} "
            f"(time: {result.execution_time:.2f}s, "
            f"retries: {result.retry_count})"
        )

        return result

    def _checkpoint_exists(self, stage_name: str) -> bool:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å­˜åœ¨ç¢ºèª"""
        from pathlib import Path
        checkpoint_path = Path(self.checkpoint_dir) / f"{stage_name}.ckpt"
        return checkpoint_path.exists()

    def _save_checkpoint(self, stage_name: str, result: TaskResult):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        import pickle
        from pathlib import Path

        Path(self.checkpoint_dir).mkdir(exist_ok=True)
        checkpoint_path = Path(self.checkpoint_dir) / f"{stage_name}.ckpt"

        with open(checkpoint_path, 'wb') as f:
            pickle.dump(result, f)

    def _load_checkpoint(self, stage_name: str) -> TaskResult:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿"""
        import pickle
        from pathlib import Path

        checkpoint_path = Path(self.checkpoint_dir) / f"{stage_name}.ckpt"

        with open(checkpoint_path, 'rb') as f:
            return pickle.load(f)


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # ãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥å®šç¾©
    retry_strategy = RetryStrategy(
        max_retries=3,
        initial_delay=2.0,
        backoff_factor=2.0,
        retryable_exceptions=[ConnectionError, TimeoutError]
    )

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    executor = PipelineExecutor(checkpoint_dir="./pipeline_checkpoints")

    # ã‚¹ãƒ†ãƒ¼ã‚¸1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆå¤±æ•—ã™ã‚‹å¯èƒ½æ€§ã‚ã‚Šï¼‰
    def load_data():
        import random
        if random.random() < 0.3:  # 30%ã®ç¢ºç‡ã§å¤±æ•—
            raise ConnectionError("Failed to connect to data source")
        return {"data": list(range(1000))}

    result1 = executor.execute_stage(
        "data_loading",
        load_data,
        retry_strategy=retry_strategy
    )

    if result1.status == TaskStatus.SUCCESS:
        print(f"Data loaded: {len(result1.result['data'])} records")

        # ã‚¹ãƒ†ãƒ¼ã‚¸2: ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        def process_data():
            data = result1.result['data']
            processed = [x * 2 for x in data]
            return {"processed_data": processed}

        result2 = executor.execute_stage(
            "data_processing",
            process_data,
            checkpoint=True
        )

        if result2.status == TaskStatus.SUCCESS:
            print(f"Processing completed successfully")
</code></pre>

<div class="warning-box">
<strong>âš ï¸ ãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥ã®æ³¨æ„ç‚¹ï¼š</strong>
<ul>
<li>å†ªç­‰æ€§ï¼šåŒã˜æ“ä½œã‚’è¤‡æ•°å›å®Ÿè¡Œã—ã¦ã‚‚çµæœãŒå¤‰ã‚ã‚‰ãªã„ã‚ˆã†ã«è¨­è¨ˆ</li>
<li>ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼šç„¡é™ãƒ«ãƒ¼ãƒ—ã‚’é˜²ããŸã‚æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’è¨­å®š</li>
<li>ãƒãƒƒã‚¯ã‚ªãƒ•ï¼šã‚µãƒ¼ãƒ“ã‚¹éè² è·ã‚’é˜²ããŸã‚å¾…æ©Ÿæ™‚é–“ã‚’æŒ‡æ•°çš„ã«å¢—åŠ </li>
<li>é¸æŠçš„ãƒªãƒˆãƒ©ã‚¤ï¼šä¸€æ™‚çš„ãªã‚¨ãƒ©ãƒ¼ã®ã¿ãƒªãƒˆãƒ©ã‚¤ã—ã€æ°¸ç¶šçš„ãªã‚¨ãƒ©ãƒ¼ã¯å³åº§ã«å¤±æ•—</li>
</ul>
</div>

<hr>

<h2>5.2 ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h2>

<h3>åˆ†æ•£ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰</h3>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹3: Sparkãƒ™ãƒ¼ã‚¹ã®å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ETL</h4>

<pre><code>"""
Apache Sparkã«ã‚ˆã‚‹å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
"""
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType
from pyspark.ml.feature import VectorAssembler, StandardScaler
from typing import List, Dict
import logging


class DistributedETLPipeline:
    """åˆ†æ•£ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(
        self,
        app_name: str = "MLDataPipeline",
        master: str = "spark://master:7077",
        num_partitions: int = 1000
    ):
        self.spark = SparkSession.builder \
            .appName(app_name) \
            .master(master) \
            .config("spark.sql.shuffle.partitions", num_partitions) \
            .config("spark.default.parallelism", num_partitions) \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .getOrCreate()

        self.logger = logging.getLogger(__name__)

    def extract(
        self,
        source_path: str,
        file_format: str = "parquet",
        schema: StructType = None
    ) -> DataFrame:
        """ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        self.logger.info(f"Extracting data from {source_path}")

        if schema:
            df = self.spark.read.schema(schema).format(file_format).load(source_path)
        else:
            df = self.spark.read.format(file_format).load(source_path)

        self.logger.info(f"Extracted {df.count()} records with {len(df.columns)} columns")
        return df

    def validate(self, df: DataFrame, validation_rules: Dict) -> DataFrame:
        """ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼"""
        self.logger.info("Validating data quality")

        # å¿…é ˆã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯
        if "required_columns" in validation_rules:
            missing_cols = set(validation_rules["required_columns"]) - set(df.columns)
            if missing_cols:
                raise ValueError(f"Missing required columns: {missing_cols}")

        # Nullå€¤ãƒã‚§ãƒƒã‚¯
        if "non_null_columns" in validation_rules:
            for col in validation_rules["non_null_columns"]:
                null_count = df.filter(F.col(col).isNull()).count()
                if null_count > 0:
                    self.logger.warning(f"Column {col} has {null_count} null values")

        # å€¤ç¯„å›²ãƒã‚§ãƒƒã‚¯
        if "value_ranges" in validation_rules:
            for col, (min_val, max_val) in validation_rules["value_ranges"].items():
                df = df.filter(
                    (F.col(col) >= min_val) & (F.col(col) <= max_val)
                )

        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        if "unique_columns" in validation_rules:
            unique_cols = validation_rules["unique_columns"]
            initial_count = df.count()
            df = df.dropDuplicates(unique_cols)
            duplicates_removed = initial_count - df.count()
            if duplicates_removed > 0:
                self.logger.warning(f"Removed {duplicates_removed} duplicate records")

        return df

    def transform(
        self,
        df: DataFrame,
        feature_columns: List[str],
        label_column: str = None,
        normalize: bool = True
    ) -> DataFrame:
        """ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
        self.logger.info("Transforming data")

        # æ¬ æå€¤å‡¦ç†
        df = self._handle_missing_values(df, feature_columns)

        # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        df = self._encode_categorical_features(df, feature_columns)

        # ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ä½œæˆ
        assembler = VectorAssembler(
            inputCols=feature_columns,
            outputCol="features_raw"
        )
        df = assembler.transform(df)

        # æ­£è¦åŒ–
        if normalize:
            scaler = StandardScaler(
                inputCol="features_raw",
                outputCol="features",
                withMean=True,
                withStd=True
            )
            scaler_model = scaler.fit(df)
            df = scaler_model.transform(df)
        else:
            df = df.withColumnRenamed("features_raw", "features")

        # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿é¸æŠ
        select_cols = ["features"]
        if label_column:
            select_cols.append(label_column)

        return df.select(select_cols)

    def _handle_missing_values(
        self,
        df: DataFrame,
        columns: List[str],
        strategy: str = "mean"
    ) -> DataFrame:
        """æ¬ æå€¤å‡¦ç†"""
        for col in columns:
            if strategy == "mean":
                mean_val = df.select(F.mean(col)).first()[0]
                df = df.fillna({col: mean_val})
            elif strategy == "median":
                median_val = df.approxQuantile(col, [0.5], 0.01)[0]
                df = df.fillna({col: median_val})
            elif strategy == "drop":
                df = df.dropna(subset=[col])

        return df

    def _encode_categorical_features(
        self,
        df: DataFrame,
        feature_columns: List[str]
    ) -> DataFrame:
        """ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        from pyspark.ml.feature import StringIndexer, OneHotEncoder

        categorical_cols = [
            col for col in feature_columns
            if dict(df.dtypes)[col] == 'string'
        ]

        for col in categorical_cols:
            # StringIndexer
            indexer = StringIndexer(
                inputCol=col,
                outputCol=f"{col}_index",
                handleInvalid="keep"
            )
            df = indexer.fit(df).transform(df)

            # OneHotEncoder
            encoder = OneHotEncoder(
                inputCol=f"{col}_index",
                outputCol=f"{col}_encoded"
            )
            df = encoder.fit(df).transform(df)

        return df

    def load(
        self,
        df: DataFrame,
        output_path: str,
        file_format: str = "parquet",
        mode: str = "overwrite",
        partition_by: List[str] = None
    ):
        """ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿"""
        self.logger.info(f"Loading data to {output_path}")

        writer = df.write.mode(mode).format(file_format)

        if partition_by:
            writer = writer.partitionBy(partition_by)

        writer.save(output_path)
        self.logger.info(f"Data successfully loaded to {output_path}")

    def run_etl(
        self,
        source_path: str,
        output_path: str,
        feature_columns: List[str],
        label_column: str = None,
        validation_rules: Dict = None,
        partition_by: List[str] = None
    ):
        """å®Œå…¨ãªETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        # Extract
        df = self.extract(source_path)

        # Validate
        if validation_rules:
            df = self.validate(df, validation_rules)

        # Transform
        df = self.transform(df, feature_columns, label_column)

        # Load
        self.load(df, output_path, partition_by=partition_by)

        return df


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
    pipeline = DistributedETLPipeline(
        app_name="CustomerChurnETL",
        num_partitions=2000
    )

    # æ¤œè¨¼ãƒ«ãƒ¼ãƒ«å®šç¾©
    validation_rules = {
        "required_columns": ["customer_id", "age", "balance", "churn"],
        "non_null_columns": ["customer_id", "churn"],
        "value_ranges": {
            "age": (18, 100),
            "balance": (0, 1000000)
        },
        "unique_columns": ["customer_id"]
    }

    # ETLå®Ÿè¡Œ
    feature_columns = [
        "age", "balance", "num_products", "credit_score",
        "country", "gender", "is_active_member"
    ]

    pipeline.run_etl(
        source_path="s3://my-bucket/raw-data/customers/",
        output_path="s3://my-bucket/processed-data/customers/",
        feature_columns=feature_columns,
        label_column="churn",
        validation_rules=validation_rules,
        partition_by=["country"]
    )
</code></pre>

<div class="success-box">
<strong>âœ… Spark ETLã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ï¼š</strong>
<ul>
<li><strong>Adaptive Query Execution (AQE)</strong>: ã‚¯ã‚¨ãƒªå®Ÿè¡Œè¨ˆç”»ã‚’å‹•çš„ã«æœ€é©åŒ–</li>
<li><strong>ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æœ€é©åŒ–</strong>: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«å¿œã˜ã¦é©åˆ‡ãªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã‚’è¨­å®š</li>
<li><strong>ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°</strong>: ç¹°ã‚Šè¿”ã—ä½¿ç”¨ã™ã‚‹DataFrameã‚’ãƒ¡ãƒ¢ãƒªã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥</li>
<li><strong>ã‚¹ã‚­ãƒ¼ãƒæ¨è«–å›é¿</strong>: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã¯æ˜ç¤ºçš„ãªã‚¹ã‚­ãƒ¼ãƒå®šç¾©ã‚’ä½¿ç”¨</li>
</ul>
</div>

<hr>

<h2>5.3 åˆ†æ•£ãƒ¢ãƒ‡ãƒ«è¨“ç·´</h2>

<h3>ãƒãƒ«ãƒãƒãƒ¼ãƒ‰è¨“ç·´ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</h3>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹4: Ray Tuneã«ã‚ˆã‚‹åˆ†æ•£ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</h4>

<pre><code>"""
Ray Tuneã‚’ä½¿ã£ãŸå¤§è¦æ¨¡ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
"""
import ray
from ray import tune
from ray.tune import CLIReporter
from ray.tune.schedulers import ASHAScheduler
from ray.tune.search.optuna import OptunaSearch
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from typing import Dict


class NeuralNetwork(nn.Module):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""

    def __init__(self, input_dim: int, hidden_dims: list, output_dim: int, dropout: float):
        super().__init__()

        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))
            prev_dim = hidden_dim

        layers.append(nn.Linear(prev_dim, output_dim))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)


def train_model(config: Dict, checkpoint_dir=None):
    """
    è¨“ç·´é–¢æ•°ï¼ˆRay Tuneã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹ï¼‰

    Args:
        config: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        checkpoint_dir: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆå®Ÿéš›ã¯Sparkã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼‰
    np.random.seed(42)
    X_train = np.random.randn(10000, 50).astype(np.float32)
    y_train = np.random.randint(0, 2, 10000).astype(np.int64)
    X_val = np.random.randn(2000, 50).astype(np.float32)
    y_val = np.random.randint(0, 2, 2000).astype(np.int64)

    train_dataset = TensorDataset(
        torch.from_numpy(X_train),
        torch.from_numpy(y_train)
    )
    val_dataset = TensorDataset(
        torch.from_numpy(X_val),
        torch.from_numpy(y_val)
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=config["batch_size"],
        shuffle=True,
        num_workers=2
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config["batch_size"],
        num_workers=2
    )

    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = NeuralNetwork(
        input_dim=50,
        hidden_dims=config["hidden_dims"],
        output_dim=2,
        dropout=config["dropout"]
    ).to(device)

    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨æå¤±é–¢æ•°
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=config["lr"],
        weight_decay=config["weight_decay"]
    )
    criterion = nn.CrossEntropyLoss()

    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å¾©å…ƒ
    if checkpoint_dir:
        checkpoint = torch.load(checkpoint_dir + "/checkpoint.pt")
        model.load_state_dict(checkpoint["model_state"])
        optimizer.load_state_dict(checkpoint["optimizer_state"])
        start_epoch = checkpoint["epoch"]
    else:
        start_epoch = 0

    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    for epoch in range(start_epoch, config["max_epochs"]):
        # è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚º
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()

        # æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)

                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        train_acc = train_correct / train_total
        val_acc = val_correct / val_total

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        with tune.checkpoint_dir(step=epoch) as checkpoint_dir:
            torch.save({
                "epoch": epoch + 1,
                "model_state": model.state_dict(),
                "optimizer_state": optimizer.state_dict(),
            }, checkpoint_dir + "/checkpoint.pt")

        # Ray Tuneã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹å ±å‘Š
        tune.report(
            train_loss=train_loss / len(train_loader),
            train_accuracy=train_acc,
            val_loss=val_loss / len(val_loader),
            val_accuracy=val_acc
        )


def run_hyperparameter_optimization():
    """åˆ†æ•£ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–å®Ÿè¡Œ"""

    # RayåˆæœŸåŒ–
    ray.init(
        address="auto",  # æ—¢å­˜ã®Rayã‚¯ãƒ©ã‚¹ã‚¿ã«æ¥ç¶š
        _redis_password="password"
    )

    # æ¢ç´¢ç©ºé–“å®šç¾©
    config = {
        "lr": tune.loguniform(1e-5, 1e-2),
        "batch_size": tune.choice([128, 256, 512]),
        "hidden_dims": tune.choice([
            [128, 64],
            [256, 128],
            [512, 256, 128]
        ]),
        "dropout": tune.uniform(0.1, 0.5),
        "weight_decay": tune.loguniform(1e-6, 1e-3),
        "max_epochs": 50
    }

    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼ˆæ—©æœŸåœæ­¢ï¼‰
    scheduler = ASHAScheduler(
        metric="val_accuracy",
        mode="max",
        max_t=50,
        grace_period=10,
        reduction_factor=2
    )

    # ã‚µãƒ¼ãƒã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆOptunaï¼‰
    search_alg = OptunaSearch(
        metric="val_accuracy",
        mode="max"
    )

    # ãƒ¬ãƒãƒ¼ã‚¿ãƒ¼è¨­å®š
    reporter = CLIReporter(
        metric_columns=["train_loss", "train_accuracy", "val_loss", "val_accuracy"],
        max_progress_rows=20
    )

    # ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
    analysis = tune.run(
        train_model,
        config=config,
        num_samples=100,  # è©¦è¡Œå›æ•°
        scheduler=scheduler,
        search_alg=search_alg,
        progress_reporter=reporter,
        resources_per_trial={
            "cpu": 4,
            "gpu": 1
        },
        checkpoint_at_end=True,
        checkpoint_freq=10,
        local_dir="./ray_results",
        name="neural_network_hpo"
    )

    # æœ€è‰¯ã®è¨­å®šã‚’å–å¾—
    best_config = analysis.get_best_config(metric="val_accuracy", mode="max")
    best_trial = analysis.get_best_trial(metric="val_accuracy", mode="max")

    print("\n" + "="*80)
    print("Best Hyperparameters:")
    print("="*80)
    for key, value in best_config.items():
        print(f"{key:20s}: {value}")

    print(f"\nBest Validation Accuracy: {best_trial.last_result['val_accuracy']:.4f}")

    # çµæœã‚’DataFrameã¨ã—ã¦å–å¾—
    df = analysis.dataframe()
    df.to_csv("hpo_results.csv", index=False)

    ray.shutdown()

    return best_config, analysis


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    best_config, analysis = run_hyperparameter_optimization()
</code></pre>

<h3>åˆ†æ•£è¨“ç·´æˆ¦ç•¥</h3>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹5: PyTorch Distributed Data Parallel (DDP)</h4>

<pre><code>"""
PyTorch DDPã«ã‚ˆã‚‹ãƒãƒ«ãƒãƒãƒ¼ãƒ‰åˆ†æ•£è¨“ç·´
"""
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler
import os
from typing import Optional


def setup_distributed(rank: int, world_size: int, backend: str = "nccl"):
    """
    åˆ†æ•£ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

    Args:
        rank: ç¾åœ¨ã®ãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ©ãƒ³ã‚¯
        world_size: ç·ãƒ—ãƒ­ã‚»ã‚¹æ•°
        backend: é€šä¿¡ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ï¼ˆnccl, gloo, mpiï¼‰
    """
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'

    # ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—åˆæœŸåŒ–
    dist.init_process_group(backend, rank=rank, world_size=world_size)

    # GPUãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    torch.cuda.set_device(rank)


def cleanup_distributed():
    """åˆ†æ•£ç’°å¢ƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    dist.destroy_process_group()


class DistributedTrainer:
    """åˆ†æ•£è¨“ç·´ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""

    def __init__(
        self,
        model: nn.Module,
        train_dataset,
        val_dataset,
        rank: int,
        world_size: int,
        batch_size: int = 256,
        learning_rate: float = 0.001,
        checkpoint_dir: str = "./checkpoints"
    ):
        self.rank = rank
        self.world_size = world_size
        self.checkpoint_dir = checkpoint_dir

        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        self.device = torch.device(f"cuda:{rank}")

        # ãƒ¢ãƒ‡ãƒ«ã‚’DDPã§ãƒ©ãƒƒãƒ—
        self.model = model.to(self.device)
        self.model = DDP(self.model, device_ids=[rank])

        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆDistributedSamplerã‚’ä½¿ç”¨ï¼‰
        train_sampler = DistributedSampler(
            train_dataset,
            num_replicas=world_size,
            rank=rank,
            shuffle=True
        )

        self.train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            sampler=train_sampler,
            num_workers=4,
            pin_memory=True
        )

        val_sampler = DistributedSampler(
            val_dataset,
            num_replicas=world_size,
            rank=rank,
            shuffle=False
        )

        self.val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            sampler=val_sampler,
            num_workers=4,
            pin_memory=True
        )

        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨æå¤±é–¢æ•°
        self.optimizer = torch.optim.Adam(
            self.model.parameters(),
            lr=learning_rate
        )
        self.criterion = nn.CrossEntropyLoss()

    def train_epoch(self, epoch: int):
        """1ã‚¨ãƒãƒƒã‚¯è¨“ç·´"""
        self.model.train()
        self.train_loader.sampler.set_epoch(epoch)  # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã®ã‚·ãƒ¼ãƒ‰è¨­å®š

        total_loss = 0.0
        total_correct = 0
        total_samples = 0

        for batch_idx, (inputs, labels) in enumerate(self.train_loader):
            inputs, labels = inputs.to(self.device), labels.to(self.device)

            # Forward pass
            self.optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = self.criterion(outputs, labels)

            # Backward pass
            loss.backward()
            self.optimizer.step()

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total_correct += predicted.eq(labels).sum().item()
            total_samples += labels.size(0)

            if batch_idx % 100 == 0 and self.rank == 0:
                print(f"Epoch {epoch} [{batch_idx}/{len(self.train_loader)}] "
                      f"Loss: {loss.item():.4f}")

        # å…¨ãƒ—ãƒ­ã‚»ã‚¹ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é›†ç´„
        avg_loss = self._aggregate_metric(total_loss / len(self.train_loader))
        accuracy = self._aggregate_metric(total_correct / total_samples)

        return avg_loss, accuracy

    def validate(self):
        """æ¤œè¨¼"""
        self.model.eval()

        total_loss = 0.0
        total_correct = 0
        total_samples = 0

        with torch.no_grad():
            for inputs, labels in self.val_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)

                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total_correct += predicted.eq(labels).sum().item()
                total_samples += labels.size(0)

        # å…¨ãƒ—ãƒ­ã‚»ã‚¹ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é›†ç´„
        avg_loss = self._aggregate_metric(total_loss / len(self.val_loader))
        accuracy = self._aggregate_metric(total_correct / total_samples)

        return avg_loss, accuracy

    def _aggregate_metric(self, value: float) -> float:
        """å…¨ãƒ—ãƒ­ã‚»ã‚¹ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é›†ç´„"""
        tensor = torch.tensor(value).to(self.device)
        dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
        return (tensor / self.world_size).item()

    def save_checkpoint(self, epoch: int, val_accuracy: float):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ï¼ˆãƒ©ãƒ³ã‚¯0ã®ã¿ï¼‰"""
        if self.rank == 0:
            os.makedirs(self.checkpoint_dir, exist_ok=True)
            checkpoint_path = os.path.join(
                self.checkpoint_dir,
                f"checkpoint_epoch_{epoch}.pt"
            )

            torch.save({
                'epoch': epoch,
                'model_state_dict': self.model.module.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'val_accuracy': val_accuracy,
            }, checkpoint_path)

            print(f"Checkpoint saved: {checkpoint_path}")

    def load_checkpoint(self, checkpoint_path: str):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        self.model.module.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        return checkpoint['epoch']

    def train(self, num_epochs: int, save_freq: int = 10):
        """å®Œå…¨ãªè¨“ç·´ãƒ«ãƒ¼ãƒ—"""
        for epoch in range(num_epochs):
            # è¨“ç·´
            train_loss, train_acc = self.train_epoch(epoch)

            # æ¤œè¨¼
            val_loss, val_acc = self.validate()

            # ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ©ãƒ³ã‚¯0ã®ã¿ï¼‰
            if self.rank == 0:
                print(f"\nEpoch {epoch + 1}/{num_epochs}")
                print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
                print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
            if (epoch + 1) % save_freq == 0:
                self.save_checkpoint(epoch + 1, val_acc)


def distributed_training_worker(
    rank: int,
    world_size: int,
    model_class,
    train_dataset,
    val_dataset
):
    """åˆ†æ•£è¨“ç·´ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°"""
    # åˆ†æ•£ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    setup_distributed(rank, world_size)

    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    model = model_class(input_dim=50, hidden_dims=[256, 128], output_dim=2, dropout=0.3)

    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–
    trainer = DistributedTrainer(
        model=model,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        rank=rank,
        world_size=world_size,
        batch_size=256,
        learning_rate=0.001
    )

    # è¨“ç·´å®Ÿè¡Œ
    trainer.train(num_epochs=50, save_freq=10)

    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    cleanup_distributed()


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    import torch.multiprocessing as mp
    from torch.utils.data import TensorDataset
    import numpy as np

    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    np.random.seed(42)
    X_train = torch.from_numpy(np.random.randn(100000, 50).astype(np.float32))
    y_train = torch.from_numpy(np.random.randint(0, 2, 100000).astype(np.int64))
    X_val = torch.from_numpy(np.random.randn(20000, 50).astype(np.float32))
    y_val = torch.from_numpy(np.random.randint(0, 2, 20000).astype(np.int64))

    train_dataset = TensorDataset(X_train, y_train)
    val_dataset = TensorDataset(X_val, y_val)

    # åˆ†æ•£è¨“ç·´èµ·å‹•ï¼ˆ4 GPUsï¼‰
    world_size = 4
    mp.spawn(
        distributed_training_worker,
        args=(world_size, NeuralNetwork, train_dataset, val_dataset),
        nprocs=world_size,
        join=True
    )
</code></pre>

<div class="info-box">
<strong>ğŸ’¡ åˆ†æ•£è¨“ç·´ã®é¸æŠåŸºæº–ï¼š</strong>
<ul>
<li><strong>Data Parallel (DP)</strong>: å˜ä¸€ãƒãƒ¼ãƒ‰ã€è¤‡æ•°GPUï¼ˆã‚·ãƒ³ãƒ—ãƒ«ã ãŒé…ã„ï¼‰</li>
<li><strong>Distributed Data Parallel (DDP)</strong>: ãƒãƒ«ãƒãƒãƒ¼ãƒ‰ã€åŠ¹ç‡çš„ãªå‹¾é…åŒæœŸ</li>
<li><strong>Fully Sharded Data Parallel (FSDP)</strong>: è¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆGPT-3ã‚¯ãƒ©ã‚¹ï¼‰</li>
<li><strong>Model Parallel</strong>: å˜ä¸€GPUã«ä¹—ã‚‰ãªã„å·¨å¤§ãƒ¢ãƒ‡ãƒ«</li>
</ul>
</div>

<hr>

<h2>5.4 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–</h2>

<h3>ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã¨ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š</h3>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹6: åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°</h4>

<pre><code>"""
åˆ†æ•£æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã¨æœ€é©åŒ–
"""
import time
import psutil
import torch
from contextlib import contextmanager
from typing import Dict, List
import json
from dataclasses import dataclass, asdict
import numpy as np


@dataclass
class ProfileMetrics:
    """ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    stage_name: str
    execution_time: float
    cpu_percent: float
    memory_mb: float
    gpu_memory_mb: float = 0.0
    io_read_mb: float = 0.0
    io_write_mb: float = 0.0
    network_sent_mb: float = 0.0
    network_recv_mb: float = 0.0


class PerformanceProfiler:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼"""

    def __init__(self, enable_gpu: bool = True):
        self.enable_gpu = enable_gpu and torch.cuda.is_available()
        self.metrics: List[ProfileMetrics] = []
        self.process = psutil.Process()

    @contextmanager
    def profile(self, stage_name: str):
        """ã‚¹ãƒ†ãƒ¼ã‚¸ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°"""
        # é–‹å§‹æ™‚ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        start_time = time.time()
        start_cpu = self.process.cpu_percent()
        start_memory = self.process.memory_info().rss / 1024 / 1024  # MB

        io_start = self.process.io_counters()
        net_start = psutil.net_io_counters()

        if self.enable_gpu:
            torch.cuda.reset_peak_memory_stats()
            start_gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024

        try:
            yield
        finally:
            # çµ‚äº†æ™‚ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            end_time = time.time()
            end_cpu = self.process.cpu_percent()
            end_memory = self.process.memory_info().rss / 1024 / 1024

            io_end = self.process.io_counters()
            net_end = psutil.net_io_counters()

            # GPU ãƒ¡ãƒ¢ãƒª
            if self.enable_gpu:
                end_gpu_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
            else:
                end_gpu_memory = 0.0

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
            metrics = ProfileMetrics(
                stage_name=stage_name,
                execution_time=end_time - start_time,
                cpu_percent=(start_cpu + end_cpu) / 2,
                memory_mb=end_memory - start_memory,
                gpu_memory_mb=end_gpu_memory - start_gpu_memory if self.enable_gpu else 0.0,
                io_read_mb=(io_end.read_bytes - io_start.read_bytes) / 1024 / 1024,
                io_write_mb=(io_end.write_bytes - io_start.write_bytes) / 1024 / 1024,
                network_sent_mb=(net_end.bytes_sent - net_start.bytes_sent) / 1024 / 1024,
                network_recv_mb=(net_end.bytes_recv - net_start.bytes_recv) / 1024 / 1024
            )

            self.metrics.append(metrics)

    def print_summary(self):
        """ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print("\n" + "="*100)
        print("Performance Profiling Summary")
        print("="*100)
        print(f"{'Stage':<30} {'Time (s)':<12} {'CPU %':<10} {'Mem (MB)':<12} "
              f"{'GPU (MB)':<12} {'I/O Read':<12} {'I/O Write':<12}")
        print("-"*100)

        total_time = 0.0
        for m in self.metrics:
            print(f"{m.stage_name:<30} {m.execution_time:<12.2f} {m.cpu_percent:<10.1f} "
                  f"{m.memory_mb:<12.1f} {m.gpu_memory_mb:<12.1f} "
                  f"{m.io_read_mb:<12.1f} {m.io_write_mb:<12.1f}")
            total_time += m.execution_time

        print("-"*100)
        print(f"{'Total':<30} {total_time:<12.2f}")
        print("="*100)

    def get_bottlenecks(self, top_k: int = 3) -> List[ProfileMetrics]:
        """ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š"""
        sorted_metrics = sorted(
            self.metrics,
            key=lambda m: m.execution_time,
            reverse=True
        )
        return sorted_metrics[:top_k]

    def export_json(self, output_path: str):
        """çµæœã‚’JSONã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        data = [asdict(m) for m in self.metrics]
        with open(output_path, 'w') as f:
            json.dump(data, f, indent=2)


class DataLoaderOptimizer:
    """DataLoaderæœ€é©åŒ–ãƒ˜ãƒ«ãƒ‘ãƒ¼"""

    @staticmethod
    def benchmark_dataloader(
        dataset,
        batch_sizes: List[int],
        num_workers_list: List[int],
        num_iterations: int = 100
    ) -> Dict:
        """DataLoaderè¨­å®šã®æœ€é©åŒ–"""
        results = []

        for batch_size in batch_sizes:
            for num_workers in num_workers_list:
                loader = torch.utils.data.DataLoader(
                    dataset,
                    batch_size=batch_size,
                    num_workers=num_workers,
                    pin_memory=True
                )

                start_time = time.time()
                for i, batch in enumerate(loader):
                    if i >= num_iterations:
                        break
                    _ = batch  # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
                elapsed = time.time() - start_time

                throughput = (batch_size * num_iterations) / elapsed

                results.append({
                    'batch_size': batch_size,
                    'num_workers': num_workers,
                    'throughput': throughput,
                    'time_per_batch': elapsed / num_iterations
                })

        # æœ€é©è¨­å®šã‚’è¦‹ã¤ã‘ã‚‹
        best_config = max(results, key=lambda x: x['throughput'])

        print("\nDataLoader Optimization Results:")
        print(f"Best Configuration: batch_size={best_config['batch_size']}, "
              f"num_workers={best_config['num_workers']}")
        print(f"Throughput: {best_config['throughput']:.2f} samples/sec")

        return best_config


class GPUOptimizer:
    """GPUæœ€é©åŒ–ãƒ˜ãƒ«ãƒ‘ãƒ¼"""

    @staticmethod
    def optimize_memory():
        """GPU ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–"""
        if torch.cuda.is_available():
            # æœªä½¿ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
            torch.cuda.empty_cache()

            # ãƒ¡ãƒ¢ãƒªçµ±è¨ˆè¡¨ç¤º
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3

            print(f"\nGPU Memory Status:")
            print(f"Allocated: {allocated:.2f} GB")
            print(f"Reserved: {reserved:.2f} GB")

    @staticmethod
    def enable_auto_mixed_precision():
        """è‡ªå‹•æ··åˆç²¾åº¦è¨“ç·´ã‚’æœ‰åŠ¹åŒ–"""
        return torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 7

    @staticmethod
    def benchmark_precision(model, sample_input, num_iterations: int = 100):
        """ç²¾åº¦åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ"""
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
        sample_input = sample_input.to(device)

        results = {}

        # FP32
        model.float()
        start = time.time()
        for _ in range(num_iterations):
            with torch.no_grad():
                _ = model(sample_input.float())
        torch.cuda.synchronize()
        results['fp32'] = time.time() - start

        # FP16 (if available)
        if GPUOptimizer.enable_auto_mixed_precision():
            model.half()
            start = time.time()
            for _ in range(num_iterations):
                with torch.no_grad():
                    _ = model(sample_input.half())
            torch.cuda.synchronize()
            results['fp16'] = time.time() - start

        print("\nPrecision Benchmark:")
        for precision, elapsed in results.items():
            print(f"{precision.upper()}: {elapsed:.4f}s "
                  f"({num_iterations/elapsed:.2f} iter/s)")

        return results


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼åˆæœŸåŒ–
    profiler = PerformanceProfiler(enable_gpu=True)

    # ãƒ‡ãƒ¼ã‚¿æº–å‚™ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
    with profiler.profile("Data Loading"):
        data = torch.randn(100000, 50)
        time.sleep(0.5)  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

    # å‰å‡¦ç†ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
    with profiler.profile("Preprocessing"):
        normalized_data = (data - data.mean()) / data.std()
        time.sleep(0.3)

    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
    with profiler.profile("Model Training"):
        model = torch.nn.Linear(50, 10)
        optimizer = torch.optim.Adam(model.parameters())

        for _ in range(100):
            outputs = model(normalized_data)
            loss = outputs.sum()
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

    # çµæœè¡¨ç¤º
    profiler.print_summary()

    # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š
    bottlenecks = profiler.get_bottlenecks(top_k=2)
    print("\nTop Bottlenecks:")
    for i, m in enumerate(bottlenecks, 1):
        print(f"{i}. {m.stage_name}: {m.execution_time:.2f}s")

    # çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    profiler.export_json("profiling_results.json")

    # DataLoaderæœ€é©åŒ–
    dataset = torch.utils.data.TensorDataset(data, torch.zeros(len(data)))
    DataLoaderOptimizer.benchmark_dataloader(
        dataset,
        batch_sizes=[128, 256, 512],
        num_workers_list=[2, 4, 8],
        num_iterations=50
    )

    # GPUæœ€é©åŒ–
    GPUOptimizer.optimize_memory()
    if torch.cuda.is_available():
        sample_model = torch.nn.Sequential(
            torch.nn.Linear(50, 128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, 10)
        )
        sample_input = torch.randn(32, 50)
        GPUOptimizer.benchmark_precision(sample_model, sample_input)
</code></pre>

<h3>I/Oæœ€é©åŒ–ã¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åŠ¹ç‡åŒ–</h3>

<div class="info-box">
<strong>ğŸ’¡ I/Oæœ€é©åŒ–ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ï¼š</strong>
<ul>
<li><strong>ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ</strong>: Parquetï¼ˆåˆ—æŒ‡å‘ï¼‰ã¯è¡ŒæŒ‡å‘ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆCSVï¼‰ã‚ˆã‚Šé«˜é€Ÿ</li>
<li><strong>åœ§ç¸®</strong>: Snappyåœ§ç¸®ã§èª­ã¿è¾¼ã¿é€Ÿåº¦ã¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åŠ¹ç‡ã‚’ãƒãƒ©ãƒ³ã‚¹</li>
<li><strong>ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒ</strong>: DataLoaderã®`num_workers`ã§ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ­ãƒ¼ãƒ‰</li>
<li><strong>ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ”ãƒ³ã‚°</strong>: å¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ã§åŠ¹ç‡çš„ã«ã‚¢ã‚¯ã‚»ã‚¹</li>
<li><strong>ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°</strong>: ãƒ‡ãƒ¼ã‚¿ã‚’è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã—ä¸¦åˆ—èª­ã¿è¾¼ã¿</li>
</ul>
</div>

<hr>

<h2>5.5 ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®å®Ÿè£…ä¾‹</h2>

<h3>å®Œå…¨ãªå¤§è¦æ¨¡MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h3>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹7: Spark + PyTorch çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h4>

<pre><code>"""
Sparkã¨PyTorchã‚’çµ±åˆã—ãŸå¤§è¦æ¨¡MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
"""
from pyspark.sql import SparkSession
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
from typing import List, Tuple
import pickle


class SparkDatasetConverter:
    """Spark DataFrameã‚’PyTorch Datasetã«å¤‰æ›"""

    @staticmethod
    def spark_to_pytorch(
        spark_df,
        feature_column: str = "features",
        label_column: str = "label",
        output_path: str = "/tmp/pytorch_data"
    ):
        """
        Spark DataFrameã‚’PyTorchã§èª­ã¿è¾¼ã¿å¯èƒ½ãªå½¢å¼ã§ä¿å­˜

        Args:
            spark_df: Spark DataFrame
            feature_column: ç‰¹å¾´é‡ã‚«ãƒ©ãƒ å
            label_column: ãƒ©ãƒ™ãƒ«ã‚«ãƒ©ãƒ å
            output_path: å‡ºåŠ›ãƒ‘ã‚¹
        """
        # PandasçµŒç”±ã§å¤‰æ›ï¼ˆå°ã€œä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰
        # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã”ã¨ã«å‡¦ç†

        def convert_partition(iterator):
            """å„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’å¤‰æ›"""
            data_list = []
            for row in iterator:
                features = row[feature_column].toArray()
                label = row[label_column]
                data_list.append((features, label))

            # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã”ã¨ã«ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            import random
            partition_id = random.randint(0, 10000)
            output_file = f"{output_path}/partition_{partition_id}.pkl"

            with open(output_file, 'wb') as f:
                pickle.dump(data_list, f)

            yield (output_file, len(data_list))

        # å„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’å‡¦ç†
        spark_df.rdd.mapPartitions(convert_partition).collect()


class DistributedDataset(Dataset):
    """åˆ†æ•£ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€Dataset"""

    def __init__(self, data_dir: str):
        from pathlib import Path

        self.data_files = list(Path(data_dir).glob("partition_*.pkl"))

        # å…¨ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰
        self.file_indices = []
        for file_path in self.data_files:
            with open(file_path, 'rb') as f:
                data = pickle.load(f)
                self.file_indices.append((file_path, len(data)))

        self.total_samples = sum(count for _, count in self.file_indices)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ¡ãƒ¢ãƒªã«ä½™è£•ãŒã‚ã‚Œã°ï¼‰
        self.cache = {}

    def __len__(self):
        return self.total_samples

    def __getitem__(self, idx):
        # ã©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã©ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹è¨ˆç®—
        file_idx = 0
        cumsum = 0

        for i, (_, count) in enumerate(self.file_indices):
            if idx < cumsum + count:
                file_idx = i
                local_idx = idx - cumsum
                break
            cumsum += count

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        file_path = self.file_indices[file_idx][0]

        if file_path not in self.cache:
            with open(file_path, 'rb') as f:
                self.cache[file_path] = pickle.load(f)

        features, label = self.cache[file_path][local_idx]

        return torch.FloatTensor(features), torch.LongTensor([label])[0]


class EndToEndMLPipeline:
    """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(
        self,
        spark_master: str = "local[*]",
        app_name: str = "EndToEndML"
    ):
        # SparkåˆæœŸåŒ–
        self.spark = SparkSession.builder \
            .appName(app_name) \
            .master(spark_master) \
            .config("spark.sql.adaptive.enabled", "true") \
            .getOrCreate()

        self.profiler = PerformanceProfiler()

    def run_pipeline(
        self,
        data_path: str,
        model_class,
        model_params: dict,
        training_config: dict,
        output_dir: str = "./pipeline_output"
    ):
        """å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""

        # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆSparkï¼‰
        with self.profiler.profile("1. Data Loading (Spark)"):
            raw_df = self.spark.read.parquet(data_path)
            print(f"Loaded {raw_df.count()} records")

        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        with self.profiler.profile("2. Data Validation"):
            # åŸºæœ¬çµ±è¨ˆé‡
            raw_df.describe().show()

            # Nullå€¤ãƒã‚§ãƒƒã‚¯
            null_counts = raw_df.select([
                F.count(F.when(F.col(c).isNull(), c)).alias(c)
                for c in raw_df.columns
            ])
            null_counts.show()

        # ã‚¹ãƒ†ãƒƒãƒ—3: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆSparkï¼‰
        with self.profiler.profile("3. Feature Engineering (Spark)"):
            from pyspark.ml.feature import VectorAssembler, StandardScaler

            feature_cols = [c for c in raw_df.columns if c != 'label']

            assembler = VectorAssembler(
                inputCols=feature_cols,
                outputCol="features_raw"
            )
            df_assembled = assembler.transform(raw_df)

            scaler = StandardScaler(
                inputCol="features_raw",
                outputCol="features",
                withMean=True,
                withStd=True
            )
            scaler_model = scaler.fit(df_assembled)
            df_scaled = scaler_model.transform(df_assembled)

            # è¨“ç·´/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            train_df, test_df = df_scaled.randomSplit([0.8, 0.2], seed=42)

        # ã‚¹ãƒ†ãƒƒãƒ—4: PyTorchç”¨ãƒ‡ãƒ¼ã‚¿å¤‰æ›
        with self.profiler.profile("4. Spark to PyTorch Conversion"):
            train_data_dir = f"{output_dir}/train_data"
            test_data_dir = f"{output_dir}/test_data"

            SparkDatasetConverter.spark_to_pytorch(
                train_df.select("features", "label"),
                output_path=train_data_dir
            )
            SparkDatasetConverter.spark_to_pytorch(
                test_df.select("features", "label"),
                output_path=test_data_dir
            )

        # ã‚¹ãƒ†ãƒƒãƒ—5: PyTorch Dataset/DataLoaderä½œæˆ
        with self.profiler.profile("5. PyTorch DataLoader Setup"):
            train_dataset = DistributedDataset(train_data_dir)
            test_dataset = DistributedDataset(test_data_dir)

            train_loader = DataLoader(
                train_dataset,
                batch_size=training_config['batch_size'],
                shuffle=True,
                num_workers=4,
                pin_memory=True
            )
            test_loader = DataLoader(
                test_dataset,
                batch_size=training_config['batch_size'],
                num_workers=4
            )

        # ã‚¹ãƒ†ãƒƒãƒ—6: ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        with self.profiler.profile("6. Model Training"):
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            model = model_class(**model_params).to(device)

            optimizer = torch.optim.Adam(
                model.parameters(),
                lr=training_config['learning_rate']
            )
            criterion = nn.CrossEntropyLoss()

            # è¨“ç·´ãƒ«ãƒ¼ãƒ—
            for epoch in range(training_config['num_epochs']):
                model.train()
                train_loss = 0.0

                for inputs, labels in train_loader:
                    inputs, labels = inputs.to(device), labels.to(device)

                    optimizer.zero_grad()
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    loss.backward()
                    optimizer.step()

                    train_loss += loss.item()

                if (epoch + 1) % 10 == 0:
                    print(f"Epoch {epoch+1}: Loss = {train_loss/len(train_loader):.4f}")

        # ã‚¹ãƒ†ãƒƒãƒ—7: ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
        with self.profiler.profile("7. Model Evaluation"):
            model.eval()
            correct = 0
            total = 0

            with torch.no_grad():
                for inputs, labels in test_loader:
                    inputs, labels = inputs.to(device), labels.to(device)
                    outputs = model(inputs)
                    _, predicted = outputs.max(1)
                    total += labels.size(0)
                    correct += predicted.eq(labels).sum().item()

            accuracy = correct / total
            print(f"\nTest Accuracy: {accuracy:.4f}")

        # ã‚¹ãƒ†ãƒƒãƒ—8: ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        with self.profiler.profile("8. Model Saving"):
            model_path = f"{output_dir}/model.pt"
            torch.save({
                'model_state_dict': model.state_dict(),
                'model_params': model_params,
                'accuracy': accuracy
            }, model_path)
            print(f"Model saved to {model_path}")

        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°çµæœ
        self.profiler.print_summary()
        self.profiler.export_json(f"{output_dir}/profiling.json")

        return model, accuracy


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    pipeline = EndToEndMLPipeline(
        spark_master="spark://master:7077",
        app_name="CustomerChurnPrediction"
    )

    # ãƒ¢ãƒ‡ãƒ«å®šç¾©
    class ChurnPredictor(nn.Module):
        def __init__(self, input_dim, hidden_dims, output_dim):
            super().__init__()
            layers = []
            prev_dim = input_dim
            for hidden_dim in hidden_dims:
                layers.extend([
                    nn.Linear(prev_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Dropout(0.3)
                ])
                prev_dim = hidden_dim
            layers.append(nn.Linear(prev_dim, output_dim))
            self.network = nn.Sequential(*layers)

        def forward(self, x):
            return self.network(x)

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    model, accuracy = pipeline.run_pipeline(
        data_path="s3://my-bucket/customer-data/",
        model_class=ChurnPredictor,
        model_params={
            'input_dim': 50,
            'hidden_dims': [256, 128, 64],
            'output_dim': 2
        },
        training_config={
            'batch_size': 512,
            'learning_rate': 0.001,
            'num_epochs': 50
        },
        output_dir="./churn_prediction_output"
    )
</code></pre>

<h3>ç›£è¦–ã¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹</h3>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹8: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ </h4>

<pre><code>"""
MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ 
"""
import time
import threading
from dataclasses import dataclass
from typing import Dict, List, Callable, Optional
from datetime import datetime
import json


@dataclass
class Metric:
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    name: str
    value: float
    timestamp: datetime
    tags: Dict[str, str] = None


class MetricsCollector:
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""

    def __init__(self):
        self.metrics: List[Metric] = []
        self.lock = threading.Lock()

    def record(self, name: str, value: float, tags: Dict[str, str] = None):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²"""
        with self.lock:
            metric = Metric(
                name=name,
                value=value,
                timestamp=datetime.now(),
                tags=tags or {}
            )
            self.metrics.append(metric)

    def get_latest(self, name: str, n: int = 1) -> List[Metric]:
        """æœ€æ–°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—"""
        with self.lock:
            filtered = [m for m in self.metrics if m.name == name]
            return sorted(filtered, key=lambda m: m.timestamp, reverse=True)[:n]

    def get_average(self, name: str, window_seconds: int = 60) -> Optional[float]:
        """æœŸé–“å†…ã®å¹³å‡å€¤"""
        now = datetime.now()
        with self.lock:
            recent = [
                m for m in self.metrics
                if m.name == name and (now - m.timestamp).total_seconds() <= window_seconds
            ]
            if not recent:
                return None
            return sum(m.value for m in recent) / len(recent)


class AlertRule:
    """ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«"""

    def __init__(
        self,
        name: str,
        metric_name: str,
        threshold: float,
        comparison: str = "greater",  # greater, less, equal
        window_seconds: int = 60,
        callback: Callable = None
    ):
        self.name = name
        self.metric_name = metric_name
        self.threshold = threshold
        self.comparison = comparison
        self.window_seconds = window_seconds
        self.callback = callback or self.default_callback

    def check(self, collector: MetricsCollector) -> bool:
        """ãƒ«ãƒ¼ãƒ«ãƒã‚§ãƒƒã‚¯"""
        avg_value = collector.get_average(self.metric_name, self.window_seconds)

        if avg_value is None:
            return False

        if self.comparison == "greater":
            triggered = avg_value > self.threshold
        elif self.comparison == "less":
            triggered = avg_value < self.threshold
        else:  # equal
            triggered = abs(avg_value - self.threshold) < 0.0001

        if triggered:
            self.callback(self.name, self.metric_name, avg_value, self.threshold)

        return triggered

    def default_callback(self, rule_name, metric_name, value, threshold):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¢ãƒ©ãƒ¼ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        print(f"\nâš ï¸  ALERT: {rule_name}")
        print(f"   Metric: {metric_name} = {value:.4f}")
        print(f"   Threshold: {threshold:.4f}")
        print(f"   Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


class MonitoringSystem:
    """çµ±åˆç›£è¦–ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, check_interval: int = 10):
        self.collector = MetricsCollector()
        self.alert_rules: List[AlertRule] = []
        self.check_interval = check_interval
        self.running = False
        self.monitor_thread = None

    def add_alert_rule(self, rule: AlertRule):
        """ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«è¿½åŠ """
        self.alert_rules.append(rule)

    def start(self):
        """ç›£è¦–é–‹å§‹"""
        self.running = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        print("Monitoring system started")

    def stop(self):
        """ç›£è¦–åœæ­¢"""
        self.running = False
        if self.monitor_thread:
            self.monitor_thread.join()
        print("Monitoring system stopped")

    def _monitor_loop(self):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        while self.running:
            for rule in self.alert_rules:
                rule.check(self.collector)
            time.sleep(self.check_interval)

    def export_metrics(self, output_path: str):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        with self.collector.lock:
            data = [
                {
                    'name': m.name,
                    'value': m.value,
                    'timestamp': m.timestamp.isoformat(),
                    'tags': m.tags
                }
                for m in self.collector.metrics
            ]

        with open(output_path, 'w') as f:
            json.dump(data, f, indent=2)


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    monitor = MonitoringSystem(check_interval=5)

    # ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«è¨­å®š
    monitor.add_alert_rule(AlertRule(
        name="High Training Loss",
        metric_name="train_loss",
        threshold=0.5,
        comparison="greater",
        window_seconds=30
    ))

    monitor.add_alert_rule(AlertRule(
        name="Low Validation Accuracy",
        metric_name="val_accuracy",
        threshold=0.80,
        comparison="less",
        window_seconds=60
    ))

    monitor.add_alert_rule(AlertRule(
        name="High GPU Memory Usage",
        metric_name="gpu_memory_percent",
        threshold=90.0,
        comparison="greater",
        window_seconds=30
    ))

    # ç›£è¦–é–‹å§‹
    monitor.start()

    # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
    try:
        for i in range(50):
            # è¨“ç·´ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆå¾ã€…ã«æ”¹å–„ï¼‰
            train_loss = 1.0 / (i + 1) + 0.1
            val_acc = min(0.95, 0.5 + i * 0.01)
            gpu_mem = 70 + (i % 5) * 5

            monitor.collector.record("train_loss", train_loss)
            monitor.collector.record("val_accuracy", val_acc)
            monitor.collector.record("gpu_memory_percent", gpu_mem)

            # ç•°å¸¸å€¤ã‚’æ™‚ã€…æ³¨å…¥
            if i == 20:
                monitor.collector.record("train_loss", 0.8)  # ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç«
            if i == 30:
                monitor.collector.record("val_accuracy", 0.75)  # ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç«
            if i == 40:
                monitor.collector.record("gpu_memory_percent", 95.0)  # ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç«

            time.sleep(1)

    except KeyboardInterrupt:
        pass
    finally:
        # ç›£è¦–åœæ­¢ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        monitor.stop()
        monitor.export_metrics("monitoring_metrics.json")
        print("\nMetrics exported to monitoring_metrics.json")
</code></pre>

<div class="success-box">
<strong>âœ… æœ¬ç•ªç’°å¢ƒã®ç›£è¦–é …ç›®ï¼š</strong>
<ul>
<li><strong>ãƒ¢ãƒ‡ãƒ«æ€§èƒ½</strong>: ç²¾åº¦ã€F1ã‚¹ã‚³ã‚¢ã€AUC-ROCã€æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·</li>
<li><strong>ãƒ‡ãƒ¼ã‚¿å“è³ª</strong>: æ¬ æç‡ã€ç•°å¸¸å€¤ç‡ã€ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º</li>
<li><strong>ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹</strong>: CPU/GPUä½¿ç”¨ç‡ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã€ãƒ‡ã‚£ã‚¹ã‚¯I/O</li>
<li><strong>å¯ç”¨æ€§</strong>: ã‚¢ãƒƒãƒ—ã‚¿ã‚¤ãƒ ã€ã‚¨ãƒ©ãƒ¼ç‡ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ</li>
<li><strong>ã‚³ã‚¹ãƒˆ</strong>: ã‚¯ãƒ©ã‚¦ãƒ‰ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã€æ¨è«–ã‚³ã‚¹ãƒˆ per request</li>
</ul>
</div>

<hr>

<h2>ç·´ç¿’å•é¡Œ</h2>

<div class="exercise-box">
<h3>å•é¡Œ1: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­è¨ˆ</h3>
<p><strong>å•é¡Œ:</strong> 1æ—¥ã‚ãŸã‚Š10TBã®æ–°è¦ãƒ‡ãƒ¼ã‚¿ãŒç”Ÿæˆã•ã‚Œã‚‹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®è¦ä»¶ã‚’æº€ãŸã™ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ææ¡ˆã—ã¦ãã ã•ã„ï¼š</p>
<ul>
<li>ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿ã‹ã‚‰æ¨è–¦ç”Ÿæˆã¾ã§4æ™‚é–“ä»¥å†…</li>
<li>99.9%ã®å¯ç”¨æ€§</li>
<li>A/Bãƒ†ã‚¹ãƒˆæ©Ÿèƒ½</li>
<li>ãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•å†è¨“ç·´</li>
</ul>

<p><strong>ãƒ’ãƒ³ãƒˆ:</strong></p>
<ul>
<li>å¢—åˆ†å­¦ç¿’ï¼ˆIncremental Learningï¼‰ã‚’æ¤œè¨</li>
<li>ç‰¹å¾´é‡ã‚¹ãƒˆã‚¢ï¼ˆFeature Storeï¼‰ã§ãƒ‡ãƒ¼ã‚¿å†åˆ©ç”¨</li>
<li>ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆ for A/Bãƒ†ã‚¹ãƒˆ</li>
<li>ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®è‡ªå‹•ç›£è¦–ã¨ãƒˆãƒªã‚¬ãƒ¼</li>
</ul>
</div>

<div class="exercise-box">
<h3>å•é¡Œ2: åˆ†æ•£è¨“ç·´ã®æœ€é©åŒ–</h3>
<p><strong>å•é¡Œ:</strong> 8ãƒãƒ¼ãƒ‰ x 4 GPU (åˆè¨ˆ32 GPU) ã®ã‚¯ãƒ©ã‚¹ã‚¿ã§ç”»åƒåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¾ã™ã€‚ä»¥ä¸‹ã®æœ€é©åŒ–ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š</p>
<ul>
<li>åŠ¹ç‡çš„ãªå‹¾é…åŒæœŸæˆ¦ç•¥</li>
<li>ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</li>
<li>æ··åˆç²¾åº¦è¨“ç·´ã®é©ç”¨</li>
<li>ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæˆ¦ç•¥</li>
</ul>

<p><strong>æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„:</strong></p>
<ul>
<li>è¨“ç·´æ™‚é–“ã‚’å˜ä¸€GPUæ¯”ã§25å€ä»¥ä¸Šã«çŸ­ç¸®</li>
<li>GPUä½¿ç”¨ç‡ã‚’90%ä»¥ä¸Šã«ç¶­æŒ</li>
<li>ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¸¯åŸŸå¹…ã®åŠ¹ç‡çš„åˆ©ç”¨</li>
</ul>
</div>

<div class="exercise-box">
<h3>å•é¡Œ3: ã‚³ã‚¹ãƒˆæœ€é©åŒ–</h3>
<p><strong>å•é¡Œ:</strong> æœˆé–“ã®ã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ã‚¹ãƒˆãŒ$50,000ã«é”ã—ã¦ã„ã‚‹MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒã‚ã‚Šã¾ã™ã€‚ã‚³ã‚¹ãƒˆã‚’30%å‰Šæ¸›ã—ã¤ã¤ã€ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’ç¶­æŒã™ã‚‹æˆ¦ç•¥ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚</p>

<p><strong>è€ƒæ…®ã™ã¹ãè¦ç´ :</strong></p>
<ul>
<li>ã‚¹ãƒãƒƒãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®æ´»ç”¨</li>
<li>ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°è¨­å®š</li>
<li>ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸éšå±¤åŒ–ï¼ˆHot/Cold dataï¼‰</li>
<li>è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã®æœ€é©åŒ–</li>
<li>ä¸è¦ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã®å‰Šæ¸›</li>
</ul>
</div>

<div class="exercise-box">
<h3>å•é¡Œ4: ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º</h3>
<p><strong>å•é¡Œ:</strong> æœ¬ç•ªç’°å¢ƒã§ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã‚’è‡ªå‹•æ¤œå‡ºã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã‚’å«ã‚ã¦ãã ã•ã„ï¼š</p>
<ul>
<li>çµ±è¨ˆçš„æ¤œå®šï¼ˆKSæ¤œå®šã€ã‚«ã‚¤äºŒä¹—æ¤œå®šï¼‰</li>
<li>åˆ†å¸ƒã®å¯è¦–åŒ–</li>
<li>ã‚¢ãƒ©ãƒ¼ãƒˆé–¾å€¤ã®è¨­å®š</li>
<li>è‡ªå‹•å†è¨“ç·´ã®ãƒˆãƒªã‚¬ãƒ¼</li>
</ul>

<p><strong>å®Ÿè£…ä¾‹:</strong></p>
<pre><code>class DataDriftDetector:
    def detect_drift(self, reference_data, current_data):
        # KSæ¤œå®šå®Ÿè£…
        pass

    def visualize_distributions(self, feature_name):
        # åˆ†å¸ƒæ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆ
        pass

    def trigger_retraining(self, drift_score, threshold=0.05):
        # å†è¨“ç·´ãƒˆãƒªã‚¬ãƒ¼
        pass
</code></pre>
</div>

<div class="exercise-box">
<h3>å•é¡Œ5: ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè£…</h3>
<p><strong>å•é¡Œ:</strong> ä»¥ä¸‹ã®è¦ä»¶ã‚’æº€ãŸã™å®Œå…¨ãªMLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š</p>

<p><strong>ãƒ‡ãƒ¼ã‚¿:</strong> Kaggleã® "Credit Card Fraud Detection" ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ284,807ä»¶ï¼‰</p>

<p><strong>è¦ä»¶:</strong></p>
<ul>
<li>Sparkã§ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ï¼ˆæ¬ æå€¤å‡¦ç†ã€æ­£è¦åŒ–ã€ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰</li>
<li>Ray Tuneã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ï¼ˆ100è©¦è¡Œï¼‰</li>
<li>åˆ†æ•£è¨“ç·´ï¼ˆè¤‡æ•°GPUå¯¾å¿œï¼‰</li>
<li>ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ï¼ˆPrecision, Recall, F1, AUC-ROCï¼‰</li>
<li>ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°</li>
<li>ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆ</li>
</ul>

<p><strong>è©•ä¾¡åŸºæº–:</strong></p>
<ul>
<li>F1ã‚¹ã‚³ã‚¢ > 0.85</li>
<li>è¨“ç·´æ™‚é–“ < 30åˆ†ï¼ˆ4 GPUç’°å¢ƒï¼‰</li>
<li>å®Œå…¨ãªå†ç¾æ€§ï¼ˆã‚·ãƒ¼ãƒ‰å›ºå®šï¼‰</li>
<li>ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ</li>
</ul>
</div>

<hr>

<h2>ã¾ã¨ã‚</h2>

<p>ã“ã®ç« ã§ã¯ã€å¤§è¦æ¨¡æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è¨­è¨ˆã¨å®Ÿè£…ã«ã¤ã„ã¦å­¦ã³ã¾ã—ãŸï¼š</p>

<table>
<thead>
<tr>
<th>ãƒˆãƒ”ãƒƒã‚¯</th>
<th>ä¸»è¦ãªå­¦ç¿’å†…å®¹</th>
<th>å®Ÿè·µã‚¹ã‚­ãƒ«</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­è¨ˆ</strong></td>
<td>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ã‚¹ã€ç›£è¦–</td>
<td>è¨­å®šç®¡ç†ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ</td>
</tr>
<tr>
<td><strong>ãƒ‡ãƒ¼ã‚¿å‡¦ç†</strong></td>
<td>Spark ETLã€ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã€ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</td>
<td>åˆ†æ•£ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã€å“è³ªãƒã‚§ãƒƒã‚¯ã€æœ€é©åŒ–</td>
</tr>
<tr>
<td><strong>åˆ†æ•£è¨“ç·´</strong></td>
<td>DDPã€Ray Tuneã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</td>
<td>ãƒãƒ«ãƒãƒãƒ¼ãƒ‰è¨“ç·´ã€åŠ¹ç‡çš„ãªHPO</td>
</tr>
<tr>
<td><strong>ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–</strong></td>
<td>ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã€I/Oæœ€é©åŒ–ã€GPUæ´»ç”¨</td>
<td>ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®šã€æ··åˆç²¾åº¦è¨“ç·´</td>
</tr>
<tr>
<td><strong>æœ¬ç•ªé‹ç”¨</strong></td>
<td>ç›£è¦–ã€ã‚¢ãƒ©ãƒ¼ãƒˆã€ã‚³ã‚¹ãƒˆæœ€é©åŒ–</td>
<td>ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã€ç•°å¸¸æ¤œçŸ¥ã€è‡ªå‹•åŒ–</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>

<ul>
<li><strong>Kubernetesã§ã®ãƒ‡ãƒ—ãƒ­ã‚¤</strong>: MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ã‚³ãƒ³ãƒ†ãƒŠåŒ–ã—ã€K8sã§é‹ç”¨</li>
<li><strong>ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†</strong>: Kafka + Spark Streamingã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ML</li>
<li><strong>MLOpsçµ±åˆ</strong>: MLflowã€Kubeflowã€SageMakerã¨ã®çµ±åˆ</li>
<li><strong>ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒ“ãƒ³ã‚°</strong>: TorchServeã€TensorFlow Servingã§ã®æ¨è«–æœ€é©åŒ–</li>
<li><strong>AutoML</strong>: è‡ªå‹•ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¢ç´¢</li>
</ul>

<div class="info-box">
<strong>ğŸ’¡ å®Ÿå‹™ã§ã®å¿œç”¨:</strong>
<p>å¤§è¦æ¨¡MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã€ä¸æ­£æ¤œçŸ¥ã€éœ€è¦äºˆæ¸¬ãªã©ã€ãƒ“ã‚¸ãƒã‚¹ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚æœ¬ç« ã§å­¦ã‚“ã æŠ€è¡“ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã‹ã‚‰MLã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¸ã®ã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹ã«ãŠã„ã¦é‡è¦ãªã‚¹ã‚­ãƒ«ã§ã™ã€‚</p>
</div>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<h3>æ›¸ç±</h3>
<ul>
<li>Kleppmann, M. (2017). <em>Designing Data-Intensive Applications</em>. O'Reilly Media.</li>
<li>Ryza, S. et al. (2017). <em>Advanced Analytics with Spark</em> (2nd ed.). O'Reilly Media.</li>
<li>Huyen, C. (2022). <em>Designing Machine Learning Systems</em>. O'Reilly Media.</li>
<li>Gift, N. & Deza, A. (2021). <em>Practical MLOps</em>. O'Reilly Media.</li>
</ul>

<h3>è«–æ–‡</h3>
<ul>
<li>Zaharia, M. et al. (2016). "Apache Spark: A Unified Engine for Big Data Processing". <em>Communications of the ACM</em>, 59(11), 56-65.</li>
<li>Li, M. et al. (2014). "Scaling Distributed Machine Learning with the Parameter Server". <em>OSDI</em>, 14, 583-598.</li>
<li>Goyal, P. et al. (2017). "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour". <em>arXiv:1706.02677</em>.</li>
<li>Liaw, R. et al. (2018). "Tune: A Research Platform for Distributed Model Selection and Training". <em>arXiv:1807.05118</em>.</li>
</ul>

<h3>å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ</h3>
<ul>
<li><a href="https://spark.apache.org/docs/latest/">Apache Spark Documentation</a></li>
<li><a href="https://pytorch.org/tutorials/beginner/dist_overview.html">PyTorch Distributed Overview</a></li>
<li><a href="https://docs.ray.io/en/latest/tune/index.html">Ray Tune Documentation</a></li>
<li><a href="https://mlflow.org/docs/latest/index.html">MLflow Documentation</a></li>
</ul>

<h3>ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹</h3>
<ul>
<li><a href="https://github.com/databricks/spark-deep-learning">Databricks Spark Deep Learning</a></li>
<li><a href="https://www.tensorflow.org/guide/distributed_training">TensorFlow Distributed Training Guide</a></li>
<li><a href="https://engineering.fb.com/2020/08/06/ml-applications/scaling-machine-learning-infrastructure/">Facebook: Scaling ML Infrastructure</a></li>
<li><a href="https://netflixtechblog.com/distributed-time-travel-for-feature-generation-389cccdd3907">Netflix: Distributed Feature Generation</a></li>
</ul>

<h3>ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯</h3>
<ul>
<li><strong>Apache Spark</strong>: <a href="https://spark.apache.org/">https://spark.apache.org/</a></li>
<li><strong>Ray</strong>: <a href="https://www.ray.io/">https://www.ray.io/</a></li>
<li><strong>Horovod</strong>: <a href="https://horovod.ai/">https://horovod.ai/</a></li>
<li><strong>Kubeflow</strong>: <a href="https://www.kubeflow.org/">https://www.kubeflow.org/</a></li>
<li><strong>Feast (Feature Store)</strong>: <a href="https://feast.dev/">https://feast.dev/</a></li>
</ul>

    <section class="disclaimer">
<h3>å…è²¬äº‹é …</h3>
<ul>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹Code examplesã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
<li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
</ul>
</section>

</main>

    <div class="nav-links">
        <a href="chapter4-distributed-ml-frameworks.html" class="nav-button">â† ç¬¬4ç« : åˆ†æ•£æ©Ÿæ¢°å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯</a>
        <a href="index.html" class="nav-button">ç›®æ¬¡ã«æˆ»ã‚‹</a>
    </div>

    <footer>
        <p>&copy; 2024 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
