<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第1章：大規模データ処理の基礎 - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>第1章：大規模データ処理の基礎</h1>
            <p class="subtitle">スケーラビリティと分散処理の原理を理解する</p>
            <div class="meta">
                <span class="meta-item">📖 読了時間: 25-30分</span>
                <span class="meta-item">📊 難易度: 初級〜中級</span>
                <span class="meta-item">💻 コード例: 7個</span>
                <span class="meta-item">📝 演習問題: 5問</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>学習目標</h2>
<p>この章を読むことで、以下を習得できます：</p>
<ul>
<li>✅ 大規模データ処理におけるスケーラビリティの課題を理解する</li>
<li>✅ 分散処理の基本概念とアーキテクチャを把握する</li>
<li>✅ 並列化戦略の種類と使い分けを学ぶ</li>
<li>✅ 分散システムの課題と解決策を知る</li>
<li>✅ 主要な分散処理ツールとエコシステムを理解する</li>
<li>✅ 実際のコードで並列化を実装できる</li>
</ul>

<hr>

<h2>1.1 スケーラビリティの課題</h2>

<h3>データサイズの増大</h3>

<p>現代の機械学習プロジェクトでは、データ量が爆発的に増加しています。</p>

<blockquote>
<p>「単一マシンでは処理しきれないデータ量に直面することは、もはや例外ではなく標準となっています。」</p>
</blockquote>

<table>
<thead>
<tr>
<th>データ規模</th>
<th>サイズの目安</th>
<th>処理方法</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>小規模</strong></td>
<td>〜1GB</td>
<td>単一マシンのメモリ内処理</td>
</tr>
<tr>
<td><strong>中規模</strong></td>
<td>1GB〜100GB</td>
<td>メモリ最適化、チャンク処理</td>
</tr>
<tr>
<td><strong>大規模</strong></td>
<td>100GB〜1TB</td>
<td>分散処理、並列化</td>
</tr>
<tr>
<td><strong>超大規模</strong></td>
<td>1TB以上</td>
<td>クラスタ、分散ファイルシステム</td>
</tr>
</tbody>
</table>

<h3>メモリ制約</h3>

<p>最も一般的な問題は、データがメモリに収まらないことです。</p>

<pre><code class="language-python">import numpy as np
import sys

# メモリ使用量の確認
def memory_usage_mb(data):
    """データのメモリ使用量をMBで返す"""
    return sys.getsizeof(data) / (1024 ** 2)

# 大規模データの例
n_samples = 10_000_000  # 1000万サンプル
n_features = 100

# 通常の配列（全データをメモリに読み込み）
# これは約7.5GBのメモリを消費
# data = np.random.random((n_samples, n_features))  # メモリ不足の可能性

# より小さいデータで確認
n_samples_small = 1_000_000  # 100万サンプル
data_small = np.random.random((n_samples_small, n_features))

print("=== メモリ使用量の確認 ===")
print(f"データ形状: {data_small.shape}")
print(f"メモリ使用量: {memory_usage_mb(data_small):.2f} MB")
print(f"\n推定: {n_samples:,}サンプルの場合")
print(f"メモリ使用量: {memory_usage_mb(data_small) * 10:.2f} MB ({memory_usage_mb(data_small) * 10 / 1024:.2f} GB)")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== メモリ使用量の確認 ===
データ形状: (1000000, 100)
メモリ使用量: 762.94 MB

推定: 10,000,000サンプルの場合
メモリ使用量: 7629.40 MB (7.45 GB)
</code></pre>

<h3>計算時間の問題</h3>

<p>データサイズが増加すると、計算時間が非線形に増加します。</p>

<pre><code class="language-python">import time
import numpy as np
import matplotlib.pyplot as plt

# 異なるサイズでの計算時間を測定
sizes = [1000, 5000, 10000, 50000, 100000]
times = []

print("=== 計算時間の測定 ===")
for size in sizes:
    X = np.random.random((size, 100))

    start = time.time()
    # 簡単な行列演算
    result = X @ X.T
    elapsed = time.time() - start

    times.append(elapsed)
    print(f"サイズ {size:6d}: {elapsed:.4f}秒")

# 可視化
plt.figure(figsize=(10, 6))
plt.plot(sizes, times, marker='o', linewidth=2, markersize=8)
plt.xlabel('データサイズ（サンプル数）', fontsize=12)
plt.ylabel('計算時間（秒）', fontsize=12)
plt.title('データサイズと計算時間の関係', fontsize=14)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# 時間複雑度の推定
print(f"\n10倍のサイズ増加による時間増加率: {times[-1] / times[0]:.1f}x")
</code></pre>

<h3>I/Oボトルネック</h3>

<p>ディスクI/Oは、大規模データ処理における主要なボトルネックです。</p>

<table>
<thead>
<tr>
<th>ストレージ種類</th>
<th>読み取り速度</th>
<th>相対性能</th>
</tr>
</thead>
<tbody>
<tr>
<td>メモリ（RAM）</td>
<td>〜50 GB/s</td>
<td>1,000倍</td>
</tr>
<tr>
<td>SSD</td>
<td>〜500 MB/s</td>
<td>10倍</td>
</tr>
<tr>
<td>HDD</td>
<td>〜100 MB/s</td>
<td>1倍（基準）</td>
</tr>
<tr>
<td>ネットワーク（1Gbps）</td>
<td>〜125 MB/s</td>
<td>1.25倍</td>
</tr>
</tbody>
</table>

<hr>

<h2>1.2 分散処理の概念</h2>

<h3>水平スケーリング vs 垂直スケーリング</h3>

<p>スケーラビリティを実現するアプローチには2種類あります。</p>

<table>
<thead>
<tr>
<th>種類</th>
<th>説明</th>
<th>長所</th>
<th>短所</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>垂直スケーリング</strong><br>(Scale Up)</td>
<td>単一マシンの性能向上<br>（CPU、メモリ、ストレージ増強）</td>
<td>・シンプルな実装<br>・通信オーバーヘッドなし</td>
<td>・物理的限界あり<br>・コストが非線形に増加</td>
</tr>
<tr>
<td><strong>水平スケーリング</strong><br>(Scale Out)</td>
<td>複数マシンで分散処理<br>（ノード数を増やす）</td>
<td>・理論上無限に拡張可能<br>・耐障害性向上</td>
<td>・実装が複雑<br>・通信コスト発生</td>
</tr>
</tbody>
</table>

<blockquote>
<p><strong>実務での選択</strong>: 通常、垂直スケーリングを限界まで行い、その後に水平スケーリングに移行します。</p>
</blockquote>

<h3>マスター・ワーカーアーキテクチャ</h3>

<p>分散処理の最も一般的なパターンは、<strong>マスター・ワーカー（Master-Worker）</strong>アーキテクチャです。</p>

<div class="mermaid">
graph TD
    M[マスターノード<br/>タスク分配・結果集約] --> W1[ワーカー 1<br/>部分計算]
    M --> W2[ワーカー 2<br/>部分計算]
    M --> W3[ワーカー 3<br/>部分計算]
    M --> W4[ワーカー 4<br/>部分計算]

    W1 --> R[結果統合]
    W2 --> R
    W3 --> R
    W4 --> R

    style M fill:#9d4edd
    style W1 fill:#e3f2fd
    style W2 fill:#e3f2fd
    style W3 fill:#e3f2fd
    style W4 fill:#e3f2fd
    style R fill:#c8e6c9
</div>

<h4>役割分担</h4>

<ul>
<li><strong>マスターノード</strong>:
<ul>
<li>タスクの分割と割り当て</li>
<li>ワーカーの監視</li>
<li>結果の集約</li>
<li>障害の検出と回復</li>
</ul></li>
<li><strong>ワーカーノード</strong>:
<ul>
<li>割り当てられたタスクの実行</li>
<li>結果の返送</li>
<li>ステータスの報告</li>
</ul></li>
</ul>

<h3>データ分割とシャーディング</h3>

<p><strong>シャーディング（Sharding）</strong>は、データを複数のパーティションに分割する技術です。</p>

<h4>分割戦略</h4>

<table>
<thead>
<tr>
<th>戦略</th>
<th>説明</th>
<th>使用例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>水平分割</strong></td>
<td>行単位で分割</td>
<td>時系列データ、ユーザーデータ</td>
</tr>
<tr>
<td><strong>垂直分割</strong></td>
<td>列単位で分割</td>
<td>特徴量が多い場合</td>
</tr>
<tr>
<td><strong>ハッシュベース</strong></td>
<td>キーのハッシュ値で分割</td>
<td>均等な分散が必要</td>
</tr>
<tr>
<td><strong>範囲ベース</strong></td>
<td>値の範囲で分割</td>
<td>ソート済みデータ</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">import numpy as np
import pandas as pd

# サンプルデータ
n_samples = 1000
data = pd.DataFrame({
    'user_id': np.random.randint(1, 100, n_samples),
    'timestamp': pd.date_range('2024-01-01', periods=n_samples, freq='1min'),
    'value': np.random.random(n_samples)
})

# 1. 水平分割（行単位）
n_partitions = 4
partition_size = len(data) // n_partitions

horizontal_shards = []
for i in range(n_partitions):
    start = i * partition_size
    end = start + partition_size if i < n_partitions - 1 else len(data)
    shard = data.iloc[start:end]
    horizontal_shards.append(shard)
    print(f"シャード {i+1}: {len(shard)}行")

# 2. ハッシュベース分割
def hash_partition(user_id, n_partitions):
    return hash(user_id) % n_partitions

data['partition'] = data['user_id'].apply(lambda x: hash_partition(x, n_partitions))

hash_shards = []
for i in range(n_partitions):
    shard = data[data['partition'] == i]
    hash_shards.append(shard)
    print(f"ハッシュシャード {i+1}: {len(shard)}行")

# 分散の確認
print("\n=== 分割バランスの確認 ===")
hash_sizes = [len(shard) for shard in hash_shards]
print(f"最小: {min(hash_sizes)}, 最大: {max(hash_sizes)}, 平均: {np.mean(hash_sizes):.1f}")
</code></pre>

<h3>分散処理アーキテクチャの図解</h3>

<div class="mermaid">
graph LR
    subgraph "入力データ"
        D[大規模データセット<br/>1TB]
    end

    subgraph "分散ストレージ"
        S1[シャード 1<br/>250GB]
        S2[シャード 2<br/>250GB]
        S3[シャード 3<br/>250GB]
        S4[シャード 4<br/>250GB]
    end

    subgraph "並列処理"
        P1[プロセス 1]
        P2[プロセス 2]
        P3[プロセス 3]
        P4[プロセス 4]
    end

    subgraph "結果集約"
        A[集約・統合]
    end

    D --> S1
    D --> S2
    D --> S3
    D --> S4

    S1 --> P1
    S2 --> P2
    S3 --> P3
    S4 --> P4

    P1 --> A
    P2 --> A
    P3 --> A
    P4 --> A

    style D fill:#ffebee
    style S1 fill:#e3f2fd
    style S2 fill:#e3f2fd
    style S3 fill:#e3f2fd
    style S4 fill:#e3f2fd
    style P1 fill:#fff3e0
    style P2 fill:#fff3e0
    style P3 fill:#fff3e0
    style P4 fill:#fff3e0
    style A fill:#c8e6c9
</div>

<hr>

<h2>1.3 並列化戦略</h2>

<h3>データ並列化</h3>

<p><strong>データ並列化（Data Parallelism）</strong>は、データを分割し、各パーティションで同じ処理を並列実行します。</p>

<blockquote>
<p>最も一般的で実装しやすい並列化手法です。</p>
</blockquote>

<pre><code class="language-python">import numpy as np
import multiprocessing as mp
import time

# 処理関数
def process_chunk(data_chunk):
    """データチャンクに対する処理（例：平均計算）"""
    return np.mean(data_chunk, axis=0)

# シングルプロセス版
def single_process_compute(data):
    start = time.time()
    result = np.mean(data, axis=0)
    elapsed = time.time() - start
    return result, elapsed

# マルチプロセス版（データ並列化）
def multi_process_compute(data, n_workers=4):
    start = time.time()

    # データを分割
    chunks = np.array_split(data, n_workers)

    # 並列処理
    with mp.Pool(n_workers) as pool:
        results = pool.map(process_chunk, chunks)

    # 結果を統合
    final_result = np.mean(results, axis=0)
    elapsed = time.time() - start

    return final_result, elapsed

# テスト
if __name__ == '__main__':
    # サンプルデータ
    data = np.random.random((10_000_000, 10))

    print("=== データ並列化の比較 ===")
    print(f"データサイズ: {data.shape}")

    # シングルプロセス
    result_single, time_single = single_process_compute(data)
    print(f"\nシングルプロセス: {time_single:.4f}秒")

    # マルチプロセス
    n_workers = mp.cpu_count()
    result_multi, time_multi = multi_process_compute(data, n_workers)
    print(f"マルチプロセス ({n_workers}ワーカー): {time_multi:.4f}秒")

    # スピードアップ
    speedup = time_single / time_multi
    print(f"\nスピードアップ: {speedup:.2f}x")
    print(f"効率: {speedup / n_workers * 100:.1f}%")
</code></pre>

<h3>モデル並列化</h3>

<p><strong>モデル並列化（Model Parallelism）</strong>は、モデル自体を分割して複数デバイスに配置します。</p>

<p>大規模なニューラルネットワークで使用されます：</p>

<ul>
<li>各層を異なるGPUに配置</li>
<li>モデルパラメータが単一デバイスのメモリに収まらない場合</li>
</ul>

<pre><code class="language-python">import numpy as np

# 概念的な例：大規模モデルの分割
class DistributedModel:
    """モデル並列化の概念実装"""

    def __init__(self, layer_sizes):
        self.layers = []
        for i in range(len(layer_sizes) - 1):
            # 各層を異なるデバイス（ここでは配列）に配置
            weight = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01
            self.layers.append({
                'weight': weight,
                'device': f'GPU_{i % 4}'  # 4つのGPUに分散
            })

    def forward(self, x):
        """順伝播（各層は異なるデバイスで実行）"""
        activation = x
        for i, layer in enumerate(self.layers):
            print(f"層 {i+1} を {layer['device']} で実行")
            activation = np.dot(activation, layer['weight'])
            activation = np.maximum(0, activation)  # ReLU
        return activation

# 使用例
print("=== モデル並列化の例 ===")
layer_sizes = [1000, 2000, 2000, 2000, 100]  # 大規模モデル
model = DistributedModel(layer_sizes)

# 入力データ
x = np.random.randn(1, 1000)
output = model.forward(x)

print(f"\n入力形状: {x.shape}")
print(f"出力形状: {output.shape}")
print(f"総パラメータ数: {sum(layer['weight'].size for layer in model.layers):,}")
</code></pre>

<h3>パイプライン並列化</h3>

<p><strong>パイプライン並列化（Pipeline Parallelism）</strong>は、処理を複数のステージに分割し、各ステージを並列実行します。</p>

<pre><code class="language-python">import multiprocessing as mp
from queue import Queue
import time

# パイプラインのステージ
def stage1_preprocess(input_queue, output_queue):
    """ステージ1: 前処理"""
    while True:
        item = input_queue.get()
        if item is None:
            output_queue.put(None)
            break
        # 前処理（例：正規化）
        processed = item / 255.0
        output_queue.put(processed)

def stage2_feature_extract(input_queue, output_queue):
    """ステージ2: 特徴抽出"""
    while True:
        item = input_queue.get()
        if item is None:
            output_queue.put(None)
            break
        # 特徴抽出（例：統計量計算）
        features = [item.mean(), item.std(), item.max(), item.min()]
        output_queue.put(features)

def stage3_predict(input_queue, results):
    """ステージ3: 予測"""
    while True:
        item = input_queue.get()
        if item is None:
            break
        # 予測（簡易版）
        prediction = sum(item) > 2.0
        results.append(prediction)

# パイプライン並列化の実装例（概念）
print("=== パイプライン並列化の概念 ===")
print("ステージ1: 前処理 → ステージ2: 特徴抽出 → ステージ3: 予測")
print("\n各ステージが異なるプロセスで並列実行されることで、")
print("スループットが向上します。")
</code></pre>

<h3>並列化戦略の比較</h3>

<table>
<thead>
<tr>
<th>戦略</th>
<th>適用場面</th>
<th>長所</th>
<th>短所</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>データ並列化</strong></td>
<td>大規模データ、同一処理</td>
<td>実装が簡単、スケーラブル</td>
<td>通信コスト、メモリ複製</td>
</tr>
<tr>
<td><strong>モデル並列化</strong></td>
<td>大規模モデル、GPU制約</td>
<td>メモリ制約を回避</td>
<td>実装が複雑、デバイス間通信</td>
</tr>
<tr>
<td><strong>パイプライン並列化</strong></td>
<td>多段階処理、ETL</td>
<td>スループット向上</td>
<td>レイテンシ増加、バランス調整</td>
</tr>
</tbody>
</table>

<hr>

<h2>1.4 分散システムの課題</h2>

<h3>通信コスト</h3>

<p>分散処理における最大のオーバーヘッドは、ノード間の通信です。</p>

<blockquote>
<p><strong>Amdahlの法則</strong>: 並列化できない部分（通信など）が全体の性能を制限します。</p>
</blockquote>

<p>$$
\text{Speedup} = \frac{1}{(1-P) + \frac{P}{N}}
$$</p>

<ul>
<li>$P$: 並列化可能な部分の割合</li>
<li>$N$: プロセッサ数</li>
</ul>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Amdahlの法則の可視化
def amdahl_speedup(P, N):
    """Amdahlの法則によるスピードアップ計算"""
    return 1 / ((1 - P) + P / N)

# 並列化率の異なるケース
P_values = [0.5, 0.75, 0.9, 0.95, 0.99]
N_range = np.arange(1, 65)

plt.figure(figsize=(10, 6))
for P in P_values:
    speedups = [amdahl_speedup(P, N) for N in N_range]
    plt.plot(N_range, speedups, label=f'P = {P:.0%}', linewidth=2)

plt.xlabel('プロセッサ数', fontsize=12)
plt.ylabel('スピードアップ', fontsize=12)
plt.title('Amdahlの法則：並列化率と性能', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print("=== Amdahlの法則の示唆 ===")
print("並列化率が低いと、プロセッサを増やしても性能向上は限定的")
print(f"例: P=90%, 64プロセッサで最大{amdahl_speedup(0.9, 64):.1f}倍")
</code></pre>

<h3>同期 vs 非同期</h3>

<table>
<thead>
<tr>
<th>方式</th>
<th>説明</th>
<th>長所</th>
<th>短所</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>同期処理</strong></td>
<td>全ワーカーが完了を待つ</td>
<td>実装がシンプル、一貫性保証</td>
<td>最も遅いワーカーに依存</td>
</tr>
<tr>
<td><strong>非同期処理</strong></td>
<td>完了を待たずに次の処理</td>
<td>スループット向上</td>
<td>一貫性の管理が複雑</td>
</tr>
</tbody>
</table>

<h3>耐障害性</h3>

<p>分散システムでは、ノードやネットワークの障害が避けられません。</p>

<h4>障害対策の手法</h4>

<ul>
<li><strong>チェックポイント</strong>: 定期的に状態を保存</li>
<li><strong>レプリケーション</strong>: データを複数ノードに複製</li>
<li><strong>リトライ</strong>: 失敗したタスクを再実行</li>
<li><strong>冗長性</strong>: 複数ノードで同じ処理を実行</li>
</ul>

<h3>デバッグの困難さ</h3>

<p>分散システムのデバッグは、以下の理由で困難です：</p>

<ul>
<li>非決定的な実行順序</li>
<li>タイミングに依存するバグ</li>
<li>複数ノードにまたがるログ</li>
<li>再現が困難</li>
</ul>

<hr>

<h2>1.5 ツールとエコシステム</h2>

<h3>Apache Hadoop / Spark</h3>

<p><strong>Apache Hadoop</strong>と<strong>Apache Spark</strong>は、大規模データ処理のデファクトスタンダードです。</p>

<table>
<thead>
<tr>
<th>ツール</th>
<th>特徴</th>
<th>使用例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Hadoop</strong></td>
<td>・MapReduceベース<br>・ディスク中心の処理<br>・バッチ処理向き</td>
<td>大規模ETL、ログ解析</td>
</tr>
<tr>
<td><strong>Spark</strong></td>
<td>・インメモリ処理<br>・高速（Hadoopの100倍）<br>・機械学習ライブラリ（MLlib）</td>
<td>反復計算、機械学習</td>
</tr>
</tbody>
</table>

<pre><code class="language-python"># Apache Spark の概念的な使用例（PySpark）
# 注: 実行にはSparkのインストールが必要

"""
from pyspark.sql import SparkSession

# Sparkセッション作成
spark = SparkSession.builder \
    .appName("LargeScaleProcessing") \
    .getOrCreate()

# 大規模データの読み込み
df = spark.read.parquet("hdfs://path/to/large/data")

# 分散処理
result = df.groupBy("category") \
    .agg({"value": "mean"}) \
    .orderBy("category")

# 結果の保存
result.write.parquet("hdfs://path/to/output")

spark.stop()
"""

print("=== Apache Spark の特徴 ===")
print("1. 遅延評価（Lazy Evaluation）: 最適な実行計画を自動生成")
print("2. インメモリ処理: 中間結果をメモリにキャッシュ")
print("3. 耐障害性: RDD（Resilient Distributed Dataset）による自動復旧")
print("4. 統合API: SQL、機械学習、グラフ処理を統一的に扱える")
</code></pre>

<h3>Dask</h3>

<p><strong>Dask</strong>は、Pythonネイティブの並列処理ライブラリです。</p>

<pre><code class="language-python">import numpy as np

# Daskの概念的な使用例
"""
import dask.array as da
import dask.dataframe as dd

# Dask配列（NumPyライクなAPI）
x = da.random.random((100000, 10000), chunks=(1000, 1000))
result = x.mean(axis=0).compute()  # 遅延評価 → 実行

# Dask DataFrame（pandasライクなAPI）
df = dd.read_csv('large_file_*.csv')
result = df.groupby('category').value.mean().compute()
"""

print("=== Dask の特徴 ===")
print("1. NumPy/pandas互換API: 既存コードの移行が容易")
print("2. タスクグラフ: 処理の依存関係を自動管理")
print("3. スケーラブル: 単一マシン → クラスタへシームレスに拡張")
print("4. Pythonエコシステムとの統合: scikit-learn、XGBoostなど")

# 簡単なDask風の処理例（概念）
print("\n=== チャンク処理の例 ===")
data = np.random.random((10000, 100))
chunk_size = 1000
results = []

for i in range(0, len(data), chunk_size):
    chunk = data[i:i+chunk_size]
    result = np.mean(chunk, axis=0)
    results.append(result)

final_result = np.mean(results, axis=0)
print(f"チャンク数: {len(results)}")
print(f"最終結果形状: {final_result.shape}")
</code></pre>

<h3>Ray</h3>

<p><strong>Ray</strong>は、分散アプリケーション用の統合フレームワークです。</p>

<pre><code class="language-python"># Rayの概念的な使用例
"""
import ray

# Ray初期化
ray.init()

# リモート関数
@ray.remote
def process_data(data):
    return data.sum()

# 並列実行
data_chunks = [np.random.random(1000) for _ in range(10)]
futures = [process_data.remote(chunk) for chunk in data_chunks]
results = ray.get(futures)  # 結果を取得

ray.shutdown()
"""

print("=== Ray の特徴 ===")
print("1. 低レベル制御: タスク・アクターモデルで柔軟な並列化")
print("2. 高性能: 分散スケジューリングと共有メモリ")
print("3. エコシステム: Ray Tune（ハイパーパラメータ調整）、RLlib（強化学習）")
print("4. 使いやすさ: Pythonデコレータで簡単に並列化")
</code></pre>

<h3>選択基準</h3>

<table>
<thead>
<tr>
<th>状況</th>
<th>推奨ツール</th>
<th>理由</th>
</tr>
</thead>
<tbody>
<tr>
<td>大規模バッチ処理（TB級）</td>
<td>Apache Spark</td>
<td>成熟したエコシステム、耐障害性</td>
</tr>
<tr>
<td>Python中心の開発</td>
<td>Dask</td>
<td>NumPy/pandas互換、学習コスト低</td>
</tr>
<tr>
<td>複雑な分散アプリ</td>
<td>Ray</td>
<td>柔軟な制御、高性能</td>
</tr>
<tr>
<td>単一マシン高速化</td>
<td>multiprocessing, joblib</td>
<td>シンプル、追加インストール不要</td>
</tr>
<tr>
<td>機械学習パイプライン</td>
<td>Spark MLlib, Ray Tune</td>
<td>統合された機械学習ツール</td>
</tr>
</tbody>
</table>

<hr>

<h2>1.6 本章のまとめ</h2>

<h3>学んだこと</h3>

<ol>
<li><p><strong>スケーラビリティの課題</strong></p>
<ul>
<li>データサイズ、メモリ、計算時間、I/Oの制約</li>
<li>単一マシンの限界と分散処理の必要性</li>
</ul></li>

<li><p><strong>分散処理の概念</strong></p>
<ul>
<li>水平スケーリング vs 垂直スケーリング</li>
<li>マスター・ワーカーアーキテクチャ</li>
<li>データ分割とシャーディング戦略</li>
</ul></li>

<li><p><strong>並列化戦略</strong></p>
<ul>
<li>データ並列化：最も一般的、実装が容易</li>
<li>モデル並列化：大規模モデル向け</li>
<li>パイプライン並列化：多段階処理に有効</li>
</ul></li>

<li><p><strong>分散システムの課題</strong></p>
<ul>
<li>通信コストとAmdahlの法則</li>
<li>同期 vs 非同期のトレードオフ</li>
<li>耐障害性とデバッグの困難さ</li>
</ul></li>

<li><p><strong>ツールとエコシステム</strong></p>
<ul>
<li>Hadoop/Spark: 大規模バッチ処理</li>
<li>Dask: Python中心の並列化</li>
<li>Ray: 柔軟な分散アプリケーション</li>
</ul></li>
</ol>

<h3>重要な原則</h3>

<table>
<thead>
<tr>
<th>原則</th>
<th>説明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>最適化の順序</strong></td>
<td>まずアルゴリズム、次に実装、最後に並列化</td>
</tr>
<tr>
<td><strong>通信最小化</strong></td>
<td>ノード間通信を減らすことが性能向上の鍵</td>
</tr>
<tr>
<td><strong>適切な粒度</strong></td>
<td>タスク分割が細かすぎるとオーバーヘッド増</td>
</tr>
<tr>
<td><strong>測定駆動</strong></td>
<td>推測せず、実際の性能を測定して判断</td>
</tr>
<tr>
<td><strong>段階的スケーリング</strong></td>
<td>小規模で検証してから大規模化</td>
</tr>
</tbody>
</table>

<h3>次の章へ</h3>

<p>第2章では、<strong>MapReduceとSpark基礎</strong>を学びます：</p>
<ul>
<li>MapReduceプログラミングモデル</li>
<li>Apache Sparkの基本操作</li>
<li>RDDとDataFrame</li>
<li>実践的な分散処理パイプライン</li>
</ul>

<hr>

<h2>演習問題</h2>

<h3>問題1（難易度：easy）</h3>
<p>水平スケーリングと垂直スケーリングの違いを説明し、それぞれの長所と短所を述べてください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>

<p><strong>垂直スケーリング（Scale Up）</strong>：</p>
<ul>
<li>定義: 単一マシンの性能を向上（CPU、メモリ、ストレージの増強）</li>
<li>長所:
<ul>
<li>実装がシンプル（既存コードをそのまま使用可能）</li>
<li>ノード間通信のオーバーヘッドがない</li>
<li>データの一貫性が保ちやすい</li>
</ul></li>
<li>短所:
<ul>
<li>物理的な限界がある</li>
<li>コストが非線形に増加（高性能ハードウェアは割高）</li>
<li>単一障害点（SPOF）のリスク</li>
</ul></li>
</ul>

<p><strong>水平スケーリング（Scale Out）</strong>：</p>
<ul>
<li>定義: マシン（ノード）の数を増やして分散処理</li>
<li>長所:
<ul>
<li>理論上無限に拡張可能</li>
<li>コストが線形（通常のサーバーを追加）</li>
<li>耐障害性が高い（ノードの冗長化）</li>
</ul></li>
<li>短所:
<ul>
<li>実装が複雑（分散処理のロジック必要）</li>
<li>ノード間通信のコスト発生</li>
<li>データの一貫性管理が難しい</li>
</ul></li>
</ul>

<p><strong>実務での選択</strong>：通常、垂直スケーリングを限界まで行い、その後に水平スケーリングに移行するのが一般的です。</p>

</details>

<h3>問題2（難易度：medium）</h3>
<p>Amdahlの法則を用いて、並列化率が80%のプログラムを16プロセッサで実行した場合のスピードアップを計算してください。</p>

<details>
<summary>解答例</summary>

<pre><code class="language-python">def amdahl_speedup(P, N):
    """
    Amdahlの法則によるスピードアップ計算

    Parameters:
    P: 並列化可能な部分の割合（0〜1）
    N: プロセッサ数

    Returns:
    スピードアップ倍率
    """
    return 1 / ((1 - P) + P / N)

# 問題の計算
P = 0.8  # 80%
N = 16   # 16プロセッサ

speedup = amdahl_speedup(P, N)

print("=== Amdahlの法則による計算 ===")
print(f"並列化率: {P:.0%}")
print(f"プロセッサ数: {N}")
print(f"スピードアップ: {speedup:.2f}x")
print(f"\n解説:")
print(f"理論上の最大スピードアップ（無限プロセッサ）: {1/(1-P):.2f}x")
print(f"効率: {speedup/N*100:.1f}%")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== Amdahlの法則による計算 ===
並列化率: 80%
プロセッサ数: 16
スピードアップ: 4.21x

解説:
理論上の最大スピードアップ（無限プロセッサ）: 5.00x
効率: 26.3%
</code></pre>

<p><strong>計算式</strong>：</p>
<p>$$
\text{Speedup} = \frac{1}{(1-0.8) + \frac{0.8}{16}} = \frac{1}{0.2 + 0.05} = \frac{1}{0.25} = 4
$$</p>

<p><strong>解説</strong>：</p>
<ul>
<li>16プロセッサでも4.21倍のスピードアップにとどまる</li>
<li>並列化できない20%の部分が性能のボトルネックになる</li>
<li>プロセッサを増やしても5倍以上のスピードアップは不可能</li>
</ul>

</details>

<h3>問題3（難易度：medium）</h3>
<p>以下のコードを、multiprocessingを使ってデータ並列化してください。</p>

<pre><code class="language-python">import numpy as np

# 処理対象のデータ
data = np.random.random((1_000_000, 10))

# 各行の統計量を計算
result = []
for row in data:
    stats = {
        'mean': row.mean(),
        'std': row.std(),
        'max': row.max(),
        'min': row.min()
    }
    result.append(stats)
</code></pre>

<details>
<summary>解答例</summary>

<pre><code class="language-python">import numpy as np
import multiprocessing as mp
import time

# 処理関数
def compute_stats(chunk):
    """データチャンクの統計量を計算"""
    result = []
    for row in chunk:
        stats = {
            'mean': row.mean(),
            'std': row.std(),
            'max': row.max(),
            'min': row.min()
        }
        result.append(stats)
    return result

# シングルプロセス版
def single_process_version(data):
    start = time.time()
    result = []
    for row in data:
        stats = {
            'mean': row.mean(),
            'std': row.std(),
            'max': row.max(),
            'min': row.min()
        }
        result.append(stats)
    elapsed = time.time() - start
    return result, elapsed

# マルチプロセス版（データ並列化）
def multi_process_version(data, n_workers=4):
    start = time.time()

    # データをチャンクに分割
    chunks = np.array_split(data, n_workers)

    # 並列処理
    with mp.Pool(n_workers) as pool:
        results = pool.map(compute_stats, chunks)

    # 結果を統合
    final_result = []
    for chunk_result in results:
        final_result.extend(chunk_result)

    elapsed = time.time() - start
    return final_result, elapsed

if __name__ == '__main__':
    # テストデータ
    data = np.random.random((100_000, 10))  # サイズを調整

    print("=== データ並列化の実装 ===")
    print(f"データ形状: {data.shape}")

    # シングルプロセス
    result_single, time_single = single_process_version(data)
    print(f"\nシングルプロセス: {time_single:.4f}秒")

    # マルチプロセス
    n_workers = mp.cpu_count()
    result_multi, time_multi = multi_process_version(data, n_workers)
    print(f"マルチプロセス ({n_workers}ワーカー): {time_multi:.4f}秒")

    # 検証
    assert len(result_single) == len(result_multi), "結果の長さが異なります"
    print(f"\nスピードアップ: {time_single / time_multi:.2f}x")
    print(f"✓ 並列化成功")
</code></pre>

<p><strong>ポイント</strong>：</p>
<ul>
<li><code>np.array_split()</code>でデータを均等に分割</li>
<li><code>multiprocessing.Pool</code>でワーカープールを作成</li>
<li><code>pool.map()</code>で各チャンクを並列処理</li>
<li>結果を<code>extend()</code>で統合</li>
</ul>

</details>

<h3>問題4（難易度：hard）</h3>
<p>1000万行のデータセットを処理する場合、メモリ制約を考慮したチャンク処理を実装してください。各チャンクは100万行とし、結果を集約してください。</p>

<details>
<summary>解答例</summary>

<pre><code class="language-python">import numpy as np
import time

def process_chunk(chunk):
    """チャンクごとの処理（例：平均、標準偏差、合計）"""
    stats = {
        'mean': chunk.mean(axis=0),
        'std': chunk.std(axis=0),
        'sum': chunk.sum(axis=0),
        'count': len(chunk)
    }
    return stats

def aggregate_results(chunk_results):
    """チャンク結果を最終的に集約"""
    # 全体の合計を計算
    total_sum = sum(r['sum'] for r in chunk_results)
    total_count = sum(r['count'] for r in chunk_results)

    # 全体の平均
    global_mean = total_sum / total_count

    # 全体の標準偏差（加重平均）
    # より正確には分散の加重平均の平方根
    weighted_var = sum(
        r['count'] * (r['std']**2 + (r['mean'] - global_mean)**2)
        for r in chunk_results
    ) / total_count
    global_std = np.sqrt(weighted_var)

    return {
        'global_mean': global_mean,
        'global_std': global_std,
        'total_count': total_count
    }

def chunked_processing(n_samples=10_000_000, n_features=10, chunk_size=1_000_000):
    """メモリ効率的なチャンク処理"""
    print(f"=== チャンク処理 ===")
    print(f"総サンプル数: {n_samples:,}")
    print(f"チャンクサイズ: {chunk_size:,}")
    print(f"チャンク数: {n_samples // chunk_size}")

    start = time.time()
    chunk_results = []

    # チャンクごとに処理
    for i in range(0, n_samples, chunk_size):
        # チャンクのサイズを決定
        current_chunk_size = min(chunk_size, n_samples - i)

        # チャンクデータを生成（実際にはファイルから読み込み）
        chunk = np.random.random((current_chunk_size, n_features))

        # チャンク処理
        result = process_chunk(chunk)
        chunk_results.append(result)

        print(f"チャンク {len(chunk_results)}: {current_chunk_size:,}行処理完了")

    # 結果を集約
    final_result = aggregate_results(chunk_results)

    elapsed = time.time() - start

    print(f"\n=== 処理結果 ===")
    print(f"処理時間: {elapsed:.2f}秒")
    print(f"全体の平均（最初の3次元）: {final_result['global_mean'][:3]}")
    print(f"全体の標準偏差（最初の3次元）: {final_result['global_std'][:3]}")
    print(f"総サンプル数: {final_result['total_count']:,}")

    # メモリ効率の確認
    import sys
    chunk_memory = sys.getsizeof(np.random.random((chunk_size, n_features))) / (1024**2)
    print(f"\nチャンクあたりのメモリ使用量: {chunk_memory:.2f} MB")
    print(f"（全データを一度に読み込む場合の1/{n_samples//chunk_size}のメモリで処理）")

if __name__ == '__main__':
    # 実行（実際のサイズだと時間がかかるため、縮小版）
    chunked_processing(n_samples=1_000_000, n_features=10, chunk_size=100_000)
</code></pre>

<p><strong>出力例</strong>：</p>
<pre><code>=== チャンク処理 ===
総サンプル数: 1,000,000
チャンクサイズ: 100,000
チャンク数: 10
チャンク 1: 100,000行処理完了
チャンク 2: 100,000行処理完了
...
チャンク 10: 100,000行処理完了

=== 処理結果 ===
処理時間: 2.45秒
全体の平均（最初の3次元）: [0.500 0.499 0.501]
全体の標準偏差（最初の3次元）: [0.289 0.288 0.290]
総サンプル数: 1,000,000

チャンクあたりのメモリ使用量: 7.63 MB
（全データを一度に読み込む場合の1/10のメモリで処理）
</code></pre>

<p><strong>ポイント</strong>：</p>
<ul>
<li>チャンク単位で処理してメモリ使用量を制限</li>
<li>各チャンクの統計量を保存</li>
<li>最後に統計的に正しい方法で集約（加重平均）</li>
<li>実際にはファイルI/Oやデータベースクエリと組み合わせる</li>
</ul>

</details>

<h3>問題5（難易度：hard）</h3>
<p>データ並列化、モデル並列化、パイプライン並列化の3つの戦略について、それぞれの適用場面と、組み合わせて使う場合の考慮点を説明してください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>

<h4>1. データ並列化（Data Parallelism）</h4>

<p><strong>適用場面</strong>：</p>
<ul>
<li>大規模データセット（TB級）の処理</li>
<li>各サンプルが独立に処理可能</li>
<li>同じモデルを異なるデータで訓練（ミニバッチ学習）</li>
</ul>

<p><strong>例</strong>：</p>
<ul>
<li>画像分類の大規模データセット訓練</li>
<li>ログデータの集約・分析</li>
<li>バッチ予測処理</li>
</ul>

<h4>2. モデル並列化（Model Parallelism）</h4>

<p><strong>適用場面</strong>：</p>
<ul>
<li>モデルが単一デバイスのメモリに収まらない</li>
<li>大規模なニューラルネットワーク（数十億パラメータ）</li>
<li>計算グラフを分割できる場合</li>
</ul>

<p><strong>例</strong>：</p>
<ul>
<li>GPT-3のような大規模言語モデル</li>
<li>高解像度画像処理ネットワーク</li>
<li>グラフニューラルネットワーク</li>
</ul>

<h4>3. パイプライン並列化（Pipeline Parallelism）</h4>

<p><strong>適用場面</strong>：</p>
<ul>
<li>複数の処理ステージがある</li>
<li>各ステージが異なるリソース要件</li>
<li>ストリーミングデータ処理</li>
</ul>

<p><strong>例</strong>：</p>
<ul>
<li>ETLパイプライン（抽出→変換→ロード）</li>
<li>リアルタイムデータ処理（前処理→推論→後処理）</li>
<li>深層学習の層間パイプライン</li>
</ul>

<h4>組み合わせ使用の考慮点</h4>

<p><strong>1. データ並列化 + モデル並列化</strong>：</p>
<pre><code class="language-python">"""
超大規模モデルの訓練

例: GPT-3の訓練
- モデル並列化: 各層を複数GPUに分割
- データ並列化: 異なるミニバッチを複数のモデルレプリカで処理
- 結果: 数千GPUで効率的に訓練可能
"""
</code></pre>

<p><strong>考慮点</strong>：</p>
<ul>
<li>通信パターンの複雑化（層間通信 + レプリカ間勾配同期）</li>
<li>メモリ管理の最適化（アクティベーション、勾配の保存場所）</li>
<li>負荷バランシング（データとモデルの両方）</li>
</ul>

<p><strong>2. データ並列化 + パイプライン並列化</strong>：</p>
<pre><code class="language-python">"""
大規模ETLと機械学習パイプライン

例: ストリーミング予測システム
- パイプライン: データ取得 → 前処理 → 推論 → 後処理
- データ並列: 各ステージで複数ワーカーが並列実行
- 結果: 高スループットの予測システム
"""
</code></pre>

<p><strong>考慮点</strong>：</p>
<ul>
<li>ステージ間のバッファ管理</li>
<li>背圧制御（遅いステージが速いステージをブロック）</li>
<li>エンドツーエンドのレイテンシ管理</li>
</ul>

<p><strong>3. 3つ全ての組み合わせ</strong>：</p>
<pre><code class="language-python">"""
超大規模分散訓練システム

例: 大規模推薦システム
- データ並列: 異なるユーザーセグメントを並列処理
- モデル並列: 埋め込み層を複数GPUに分散
- パイプライン: 特徴抽出 → モデル訓練 → 評価のステージ化
"""
</code></pre>

<p><strong>考慮点</strong>：</p>
<ul>
<li>システム複雑性の管理（デバッグ、監視が困難）</li>
<li>全体的な効率の最適化（どの戦略が最もインパクトがあるか）</li>
<li>段階的な実装（まず単純な戦略から開始）</li>
</ul>

<h4>選択ガイドライン</h4>

<table>
<thead>
<tr>
<th>ボトルネック</th>
<th>推奨戦略</th>
<th>優先順位</th>
</tr>
</thead>
<tbody>
<tr>
<td>データサイズが大きい</td>
<td>データ並列化</td>
<td>1st</td>
</tr>
<tr>
<td>モデルサイズが大きい</td>
<td>モデル並列化</td>
<td>1st</td>
</tr>
<tr>
<td>処理ステージが多い</td>
<td>パイプライン並列化</td>
<td>2nd</td>
</tr>
<tr>
<td>両方大きい</td>
<td>データ+モデル並列化</td>
<td>段階的</td>
</tr>
<tr>
<td>リアルタイム要件</td>
<td>パイプライン並列化</td>
<td>1st</td>
</tr>
</tbody>
</table>

<p><strong>実装の原則</strong>：</p>
<ol>
<li>まず単一戦略で最適化</li>
<li>測定して次のボトルネックを特定</li>
<li>必要に応じて追加の戦略を導入</li>
<li>常にシステム全体の効率を測定</li>
</ol>

</details>

<hr>

<h2>参考文献</h2>

<ol>
<li>Dean, J., & Ghemawat, S. (2008). MapReduce: Simplified data processing on large clusters. <em>Communications of the ACM</em>, 51(1), 107-113.</li>
<li>Zaharia, M., et al. (2016). Apache Spark: A unified engine for big data processing. <em>Communications of the ACM</em>, 59(11), 56-65.</li>
<li>Moritz, P., et al. (2018). Ray: A distributed framework for emerging AI applications. <em>OSDI</em>, 561-577.</li>
<li>Rocklin, M. (2015). Dask: Parallel computation with blocked algorithms and task scheduling. <em>SciPy</em>, 126-132.</li>
<li>Barroso, L. A., Clidaras, J., & Hölzle, U. (2013). <em>The datacenter as a computer</em> (2nd ed.). Morgan & Claypool Publishers.</li>
</ol>

<div class="navigation">
    <a href="index.html" class="nav-button">← シリーズ目次</a>
    <a href="chapter2-mapreduce-spark.html" class="nav-button">次の章: MapReduceとSpark基礎 →</a>
</div>

    </main>

    <footer>
        <p><strong>作成者</strong>: AI Terakoya Content Team</p>
        <p><strong>バージョン</strong>: 1.0 | <strong>作成日</strong>: 2025-10-21</p>
        <p><strong>ライセンス</strong>: Creative Commons BY 4.0</p>
        <p>© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
