<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/large-scale-data-processing-introduction/chapter4-distributed-deep-learning.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/large-scale-data-processing-introduction/index.html">Large Scale Data Processing</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 4</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>åˆ†æ•£æ·±å±¤å­¦ç¿’ã®åŸºç¤ã¨å®Ÿè·µ</h1>
            <p class="subtitle">PyTorch DDPã¨Horovodã«ã‚ˆã‚‹å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 45-50åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šï½ä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>ç¬¬4ç« ï¼šåˆ†æ•£æ·±å±¤å­¦ç¿’ã®åŸºç¤ã¨å®Ÿè·µ</h1>

<div class="learning-objectives">
    <h2>å­¦ç¿’ç›®æ¨™</h2>
    <ul>
        <li>åˆ†æ•£å­¦ç¿’ã®ä¸»è¦æˆ¦ç•¥ï¼ˆData/Model/Pipeline Parallelismï¼‰ã‚’ç†è§£ã™ã‚‹</li>
        <li>PyTorch DDPã«ã‚ˆã‚‹ãƒãƒ«ãƒGPUè¨“ç·´ã‚’å®Ÿè£…ã§ãã‚‹</li>
        <li>Horovodã®AllReduceã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç†è§£ã—ã€å®Ÿè£…ã§ãã‚‹</li>
        <li>å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ï¼ˆAMPã€Gradient Accumulationï¼‰ã‚’ç¿’å¾—ã™ã‚‹</li>
        <li>åˆ†æ•£å­¦ç¿’ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã¨ãƒ‡ãƒãƒƒã‚°æ‰‹æ³•ã‚’å­¦ã¶</li>
    </ul>
</div>

<p><strong>èª­äº†æ™‚é–“</strong>: 45-50åˆ†</p>

---

<h2>4.1 åˆ†æ•£å­¦ç¿’ã®æˆ¦ç•¥</h2>

<h3>4.1.1 ãªãœåˆ†æ•£å­¦ç¿’ãŒå¿…è¦ã‹</h3>

<p><strong>ç¾ä»£ã®æ·±å±¤å­¦ç¿’ã®èª²é¡Œ:</strong></p>
<ul>
    <li><strong>ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®å¢—å¤§</strong>: GPT-3ï¼ˆ175B parametersï¼‰ã€BERT-Largeï¼ˆ340M parametersï¼‰</li>
    <li><strong>ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å·¨å¤§åŒ–</strong>: ImageNet-21Kï¼ˆ14Mç”»åƒï¼‰ã€Common Crawlï¼ˆæ•°ç™¾TBï¼‰</li>
    <li><strong>è¨“ç·´æ™‚é–“ã®å•é¡Œ</strong>: å˜ä¸€GPUè¨“ç·´ã§ã¯æ•°é€±é–“ï½æ•°ãƒ¶æœˆ</li>
</ul>

<p><strong>åˆ†æ•£å­¦ç¿’ã«ã‚ˆã‚‹è§£æ±º:</strong></p>
<ul>
    <li><strong>è¨“ç·´æ™‚é–“ã®çŸ­ç¸®</strong>: 8 GPUä¸¦åˆ—ã§ç†æƒ³çš„ã«ã¯8å€é«˜é€ŸåŒ–</li>
    <li><strong>å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®å®Ÿç¾</strong>: ãƒ¡ãƒ¢ãƒªã‚’è¤‡æ•°GPU/ãƒãƒ¼ãƒ‰ã«åˆ†æ•£</li>
    <li><strong>ã‚³ã‚¹ãƒˆåŠ¹ç‡</strong>: ã‚¯ãƒ©ã‚¦ãƒ‰ç’°å¢ƒã§ã®åŠ¹ç‡çš„ãªãƒªã‚½ãƒ¼ã‚¹åˆ©ç”¨</li>
</ul>

<h3>4.1.2 Data Parallelismï¼ˆãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ï¼‰</h3>

<p><strong>åŸºæœ¬åŸç†:</strong></p>
<ul>
    <li>ãƒ¢ãƒ‡ãƒ«ã®å®Œå…¨ãªã‚³ãƒ”ãƒ¼ã‚’å„GPUã«é…ç½®</li>
    <li>ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒã‚’åˆ†å‰²ã—ã¦å„GPUã«é…åˆ†</li>
    <li>å„GPUã§ç‹¬ç«‹ã«é †ä¼æ’­ãƒ»é€†ä¼æ’­</li>
    <li>å‹¾é…ã‚’å…¨GPUã§é›†ç´„ï¼ˆAllReduceï¼‰</li>
    <li>çµ±åˆã•ã‚ŒãŸå‹¾é…ã§ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°</li>
</ul>

<p><strong>ãƒ¡ãƒªãƒƒãƒˆ:</strong></p>
<ul>
    <li>å®Ÿè£…ãŒæ¯”è¼ƒçš„ç°¡å˜</li>
    <li>ãƒ¢ãƒ‡ãƒ«ãŒGPUãƒ¡ãƒ¢ãƒªã«åã¾ã‚‹å ´åˆã«æœ‰åŠ¹</li>
    <li>é«˜ã„ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ï¼ˆæ•°ç™¾GPUã¾ã§ï¼‰</li>
</ul>

<p><strong>ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ:</strong></p>
<ul>
    <li>å„GPUã«ãƒ¢ãƒ‡ãƒ«å…¨ä½“ãŒå¿…è¦ï¼ˆãƒ¡ãƒ¢ãƒªåˆ¶ç´„ï¼‰</li>
    <li>å‹¾é…åŒæœŸã®é€šä¿¡ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰</li>
</ul>

<h3>4.1.3 Model Parallelismï¼ˆãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—ï¼‰</h3>

<p><strong>åŸºæœ¬åŸç†:</strong></p>
<ul>
    <li>ãƒ¢ãƒ‡ãƒ«ã‚’è¤‡æ•°ã®GPUã«åˆ†å‰²</li>
    <li>å„GPUãŒç•°ãªã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼/ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ‹…å½“</li>
    <li>ãƒ‡ãƒ¼ã‚¿ã¯å…¨GPUã§å…±æœ‰</li>
</ul>

<p><strong>åˆ†å‰²æ–¹æ³•:</strong></p>
<ul>
    <li><strong>å±¤å˜ä½åˆ†å‰²</strong>: ãƒ¬ã‚¤ãƒ¤ãƒ¼1-5ã‚’GPU0ã€6-10ã‚’GPU1</li>
    <li><strong>ãƒ†ãƒ³ã‚½ãƒ«åˆ†å‰²</strong>: å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é‡ã¿è¡Œåˆ—ã‚’åˆ†å‰²ï¼ˆMegatron-LMï¼‰</li>
</ul>

<p><strong>ãƒ¡ãƒªãƒƒãƒˆ:</strong></p>
<ul>
    <li>GPUãƒ¡ãƒ¢ãƒªã‚’è¶…ãˆã‚‹å·¨å¤§ãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œ</li>
    <li>å‹¾é…åŒæœŸä¸è¦ï¼ˆå±¤é–“é€šä¿¡ã®ã¿ï¼‰</li>
</ul>

<p><strong>ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ:</strong></p>
<ul>
    <li>GPUé–“ã®ä¾å­˜é–¢ä¿‚ã§ä¸¦åˆ—åº¦ãŒä½ä¸‹</li>
    <li>å®Ÿè£…ãŒè¤‡é›‘</li>
    <li>é€šä¿¡ãƒœãƒˆãƒ«ãƒãƒƒã‚¯</li>
</ul>

<h3>4.1.4 Pipeline Parallelismï¼ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—ï¼‰</h3>

<p><strong>åŸºæœ¬åŸç†:</strong></p>
<ul>
    <li>ãƒ¢ãƒ‡ãƒ«ã‚’è¤‡æ•°ã‚¹ãƒ†ãƒ¼ã‚¸ã«åˆ†å‰²ï¼ˆå„GPUãŒæ‹…å½“ï¼‰</li>
    <li>ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã«åˆ†å‰²</li>
    <li>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çš„ã«é †æ¬¡å‡¦ç†</li>
    <li>GPUã®ã‚¢ã‚¤ãƒ‰ãƒ«æ™‚é–“ã‚’å‰Šæ¸›</li>
</ul>

<p><strong>GPipeæ‰‹æ³•:</strong></p>
<ul>
    <li>ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒåˆ†å‰²ã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŠ¹ç‡ã‚’å‘ä¸Š</li>
    <li>å‹¾é…ç´¯ç©ï¼ˆGradient Accumulationï¼‰ã¨çµ„ã¿åˆã‚ã›</li>
    <li>å†è¨ˆç®—ï¼ˆRecomputationï¼‰ã§ãƒ¡ãƒ¢ãƒªå‰Šæ¸›</li>
</ul>

<p><strong>ãƒ¡ãƒªãƒƒãƒˆ:</strong></p>
<ul>
    <li>ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—ã‚ˆã‚Šé«˜ã„ä¸¦åˆ—åº¦</li>
    <li>GPUåˆ©ç”¨ç‡ã®å‘ä¸Š</li>
</ul>

<p><strong>ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ:</strong></p>
<ul>
    <li>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒãƒ–ãƒ«ï¼ˆã‚¢ã‚¤ãƒ‰ãƒ«æ™‚é–“ï¼‰</li>
    <li>å®Ÿè£…ã®è¤‡é›‘ã•</li>
</ul>

<h3>4.1.5 Hybrid Approachesï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰</h3>

<p><strong>3D Parallelismï¼ˆMegatron-LMï¼‰:</strong></p>
<ul>
    <li><strong>Data Parallelism</strong>: ãƒãƒ¼ãƒ‰é–“</li>
    <li><strong>Model Parallelism</strong>: ãƒãƒ¼ãƒ‰å†…GPUé–“ï¼ˆãƒ†ãƒ³ã‚½ãƒ«åˆ†å‰²ï¼‰</li>
    <li><strong>Pipeline Parallelism</strong>: ãƒ¬ã‚¤ãƒ¤ãƒ¼åˆ†å‰²</li>
</ul>

<p><strong>ZeROï¼ˆDeepSpeedï¼‰:</strong></p>
<ul>
    <li>ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã®åˆ†å‰²ï¼ˆZeRO-1ï¼‰</li>
    <li>å‹¾é…ã®åˆ†å‰²ï¼ˆZeRO-2ï¼‰</li>
    <li>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆ†å‰²ï¼ˆZeRO-3ï¼‰</li>
    <li>Data Parallelismã®åŠ¹ç‡ã‚’æœ€å¤§åŒ–</li>
</ul>

<h3>4.1.6 æˆ¦ç•¥ã®æ¯”è¼ƒå›³</h3>

<div class="mermaid">
graph TB
    subgraph "Data Parallelism"
        D1[GPU 0<br/>Model Copy<br/>Data Batch 1]
        D2[GPU 1<br/>Model Copy<br/>Data Batch 2]
        D3[GPU 2<br/>Model Copy<br/>Data Batch 3]
        D1 -.AllReduce.-> D2
        D2 -.AllReduce.-> D3
    end

    subgraph "Model Parallelism"
        M1[GPU 0<br/>Layer 1-3]
        M2[GPU 1<br/>Layer 4-6]
        M3[GPU 2<br/>Layer 7-9]
        M1 -->|Forward| M2
        M2 -->|Forward| M3
        M3 -.Backward.-> M2
        M2 -.Backward.-> M1
    end

    subgraph "Pipeline Parallelism"
        P1[GPU 0<br/>Stage 1<br/>Micro-batch 1,2,3]
        P2[GPU 1<br/>Stage 2<br/>Micro-batch 1,2,3]
        P3[GPU 2<br/>Stage 3<br/>Micro-batch 1,2,3]
        P1 ==>|Pipeline| P2
        P2 ==>|Pipeline| P3
    end
</div>

---

<h2>4.2 PyTorch Distributed Data Parallel (DDP)</h2>

<h3>4.2.1 torch.distributed ã®åŸºç¤</h3>

<p><strong>ä¸»è¦æ¦‚å¿µ:</strong></p>
<ul>
    <li><strong>Process Group</strong>: ä¸¦åˆ—ãƒ—ãƒ­ã‚»ã‚¹ã®é›†åˆ</li>
    <li><strong>Rank</strong>: ãƒ—ãƒ­ã‚»ã‚¹ã®ä¸€æ„ãªIDï¼ˆ0, 1, 2, ...ï¼‰</li>
    <li><strong>World Size</strong>: ç·ãƒ—ãƒ­ã‚»ã‚¹æ•°</li>
    <li><strong>Backend</strong>: é€šä¿¡ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆNCCL, Gloo, MPIï¼‰</li>
</ul>

<p><strong>ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®é¸æŠ:</strong></p>
<ul>
    <li><strong>NCCL</strong>: GPUé–“é€šä¿¡ã«æœ€é©ï¼ˆæ¨å¥¨ï¼‰</li>
    <li><strong>Gloo</strong>: CPUã¨GPUã®ä¸¡æ–¹ã«å¯¾å¿œ</li>
    <li><strong>MPI</strong>: HPCã‚¯ãƒ©ã‚¹ã‚¿ã§ä½¿ç”¨</li>
</ul>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹1: åŸºæœ¬çš„ãªDistributedåˆæœŸåŒ–</h4>

<details>
<summary>distributed_init.py - åˆ†æ•£ç’°å¢ƒã®åˆæœŸåŒ–</summary>

<pre><code>import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp

def setup(rank, world_size):
    """
    åˆ†æ•£ç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

    Args:
        rank: ãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ©ãƒ³ã‚¯ï¼ˆ0ã‹ã‚‰world_size-1ï¼‰
        world_size: ç·ãƒ—ãƒ­ã‚»ã‚¹æ•°
    """
    # ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'

    # ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã®åˆæœŸåŒ–
    dist.init_process_group(
        backend='nccl',  # GPUé–“é€šä¿¡ã«NCCLä½¿ç”¨
        rank=rank,
        world_size=world_size
    )

    # å„ãƒ—ãƒ­ã‚»ã‚¹ã‚’å¯¾å¿œã™ã‚‹GPUã«å‰²ã‚Šå½“ã¦
    torch.cuda.set_device(rank)

    print(f"Process {rank}/{world_size} initialized on GPU {rank}")

def cleanup():
    """åˆ†æ•£ç’°å¢ƒã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    dist.destroy_process_group()

def demo_basic_operations(rank, world_size):
    """
    åŸºæœ¬çš„ãªåˆ†æ•£æ“ä½œã®ãƒ‡ãƒ¢
    """
    setup(rank, world_size)

    # å„ãƒ—ãƒ­ã‚»ã‚¹ã§ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆ
    tensor = torch.ones(2, 2).cuda(rank) * (rank + 1)
    print(f"Rank {rank} - Original tensor:\n{tensor}")

    # AllReduce: å…¨ãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’åˆè¨ˆ
    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
    print(f"Rank {rank} - After AllReduce:\n{tensor}")

    # Broadcast: Rank 0ã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’å…¨ãƒ—ãƒ­ã‚»ã‚¹ã«é…å¸ƒ
    if rank == 0:
        broadcast_tensor = torch.tensor([100.0, 200.0]).cuda(rank)
    else:
        broadcast_tensor = torch.zeros(2).cuda(rank)

    dist.broadcast(broadcast_tensor, src=0)
    print(f"Rank {rank} - After Broadcast: {broadcast_tensor}")

    cleanup()

if __name__ == "__main__":
    world_size = 4  # 4ã¤ã®GPUã‚’ä½¿ç”¨
    mp.spawn(
        demo_basic_operations,
        args=(world_size,),
        nprocs=world_size,
        join=True
    )
</code></pre>
</details>

<p><strong>å®Ÿè¡Œæ–¹æ³•:</strong></p>
<pre><code># å˜ä¸€ãƒãƒ¼ãƒ‰ã€4 GPU
python distributed_init.py

# è¤‡æ•°ãƒãƒ¼ãƒ‰ï¼ˆãƒãƒ¼ãƒ‰ã‚ãŸã‚Š4 GPUã€2ãƒãƒ¼ãƒ‰ï¼‰
# Node 0:
python -m torch.distributed.launch \
    --nproc_per_node=4 \
    --nnodes=2 \
    --node_rank=0 \
    --master_addr="192.168.1.1" \
    --master_port=12355 \
    distributed_init.py

# Node 1:
python -m torch.distributed.launch \
    --nproc_per_node=4 \
    --nnodes=2 \
    --node_rank=1 \
    --master_addr="192.168.1.1" \
    --master_port=12355 \
    distributed_init.py
</code></pre>

<h3>4.2.2 DDPå®Ÿè£…</h3>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹2: PyTorch DDPã«ã‚ˆã‚‹ç”»åƒåˆ†é¡è¨“ç·´</h4>

<details>
<summary>ddp_training.py - ResNet18ã®DDPè¨“ç·´</summary>

<pre><code>import os
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, Dataset
from torch.utils.data.distributed import DistributedSampler
import torchvision
import torchvision.transforms as transforms

def setup(rank, world_size):
    """åˆ†æ•£ç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

def cleanup():
    """åˆ†æ•£ç’°å¢ƒã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    dist.destroy_process_group()

def prepare_dataloader(rank, world_size, batch_size=32):
    """
    åˆ†æ•£ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™

    DistributedSamplerã‚’ä½¿ç”¨ã—ã¦å„ãƒ—ãƒ­ã‚»ã‚¹ã«ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å‰²ã‚Šå½“ã¦
    """
    # ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
    transform = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    # CIFAR-10ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    dataset = torchvision.datasets.CIFAR10(
        root='./data',
        train=True,
        download=True,
        transform=transform
    )

    # DistributedSampler: ãƒ‡ãƒ¼ã‚¿ã‚’world_sizeå€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
    sampler = DistributedSampler(
        dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=True,
        drop_last=False
    )

    # DataLoader
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        sampler=sampler,
        num_workers=4,
        pin_memory=True
    )

    return dataloader, sampler

def train_epoch(model, dataloader, optimizer, criterion, rank, epoch):
    """
    1ã‚¨ãƒãƒƒã‚¯ã®è¨“ç·´
    """
    model.train()
    total_loss = 0.0
    correct = 0
    total = 0

    for batch_idx, (data, target) in enumerate(dataloader):
        data, target = data.cuda(rank), target.cuda(rank)

        # é †ä¼æ’­
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)

        # é€†ä¼æ’­ï¼ˆDDPãŒè‡ªå‹•çš„ã«å‹¾é…ã‚’åŒæœŸï¼‰
        loss.backward()
        optimizer.step()

        # çµ±è¨ˆ
        total_loss += loss.item()
        _, predicted = output.max(1)
        total += target.size(0)
        correct += predicted.eq(target).sum().item()

        if rank == 0 and batch_idx % 100 == 0:
            print(f"Epoch {epoch}, Batch {batch_idx}, "
                  f"Loss: {loss.item():.4f}, "
                  f"Acc: {100.*correct/total:.2f}%")

    avg_loss = total_loss / len(dataloader)
    accuracy = 100. * correct / total

    return avg_loss, accuracy

def main(rank, world_size):
    """
    ãƒ¡ã‚¤ãƒ³è¨“ç·´ãƒ«ãƒ¼ãƒ—
    """
    print(f"Running DDP on rank {rank}.")
    setup(rank, world_size)

    # ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
    model = torchvision.models.resnet18(num_classes=10).cuda(rank)

    # DDPãƒ©ãƒƒãƒ‘ãƒ¼ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ©ãƒƒãƒ—
    model = DDP(model, device_ids=[rank])

    # æå¤±é–¢æ•°ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
    criterion = nn.CrossEntropyLoss().cuda(rank)
    optimizer = torch.optim.SGD(
        model.parameters(),
        lr=0.1,
        momentum=0.9,
        weight_decay=5e-4
    )

    # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=200
    )

    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™
    dataloader, sampler = prepare_dataloader(rank, world_size, batch_size=128)

    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    num_epochs = 100
    for epoch in range(num_epochs):
        # ã‚¨ãƒãƒƒã‚¯é–‹å§‹æ™‚ã«samplerã®ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®šï¼ˆã‚·ãƒ£ãƒƒãƒ•ãƒ«ã®å†ç¾æ€§ï¼‰
        sampler.set_epoch(epoch)

        # è¨“ç·´
        avg_loss, accuracy = train_epoch(
            model, dataloader, optimizer, criterion, rank, epoch
        )

        # å­¦ç¿’ç‡æ›´æ–°
        scheduler.step()

        # Rank 0ã®ã¿ãŒãƒ­ã‚°ã‚’å‡ºåŠ›
        if rank == 0:
            print(f"Epoch {epoch}/{num_epochs} - "
                  f"Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%")

            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆRank 0ã®ã¿ï¼‰
            if (epoch + 1) % 10 == 0:
                torch.save(
                    model.module.state_dict(),  # model.moduleã§å…ƒã®ãƒ¢ãƒ‡ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹
                    f'checkpoint_epoch_{epoch+1}.pth'
                )

    cleanup()

if __name__ == "__main__":
    import torch.multiprocessing as mp

    world_size = torch.cuda.device_count()  # åˆ©ç”¨å¯èƒ½ãªGPUæ•°
    print(f"Training with {world_size} GPUs")

    mp.spawn(main, args=(world_size,), nprocs=world_size, join=True)
</code></pre>
</details>

<p><strong>DDPã®é‡è¦ãƒã‚¤ãƒ³ãƒˆ:</strong></p>
<ul>
    <li><strong>DistributedSampler</strong>: å„ãƒ—ãƒ­ã‚»ã‚¹ã«ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å‰²ã‚Šå½“ã¦</li>
    <li><strong>sampler.set_epoch()</strong>: å„ã‚¨ãƒãƒƒã‚¯ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã‚’å¤‰ãˆã‚‹</li>
    <li><strong>model.module</strong>: DDPãƒ©ãƒƒãƒ‘ãƒ¼ã®å…ƒã®ãƒ¢ãƒ‡ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹</li>
    <li><strong>Rank 0ã®ã¿ä¿å­˜</strong>: ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã¯1ã¤ã®ãƒ—ãƒ­ã‚»ã‚¹ã®ã¿ã§å®Ÿè¡Œ</li>
</ul>

<h3>4.2.3 ãƒãƒ«ãƒãƒãƒ¼ãƒ‰GPUè¨“ç·´</h3>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹3: Slurmã«ã‚ˆã‚‹ãƒãƒ«ãƒãƒãƒ¼ãƒ‰DDP</h4>

<details>
<summary>slurm_ddp.sh - Slurmã‚¹ã‚¯ãƒªãƒ—ãƒˆ</summary>

<pre><code>#!/bin/bash
#SBATCH --job-name=ddp_training
#SBATCH --nodes=4                    # 4ãƒãƒ¼ãƒ‰
#SBATCH --ntasks-per-node=4          # ãƒãƒ¼ãƒ‰ã‚ãŸã‚Š4ãƒ—ãƒ­ã‚»ã‚¹ï¼ˆ4 GPUï¼‰
#SBATCH --cpus-per-task=8            # ãƒ—ãƒ­ã‚»ã‚¹ã‚ãŸã‚Š8 CPU
#SBATCH --gres=gpu:4                 # ãƒãƒ¼ãƒ‰ã‚ãŸã‚Š4 GPU
#SBATCH --time=24:00:00
#SBATCH --output=logs/ddp_%j.out
#SBATCH --error=logs/ddp_%j.err

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
module load cuda/11.8
module load anaconda3

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
export MASTER_PORT=12340
export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n 1)
export WORLD_SIZE=$SLURM_NTASKS
export NCCL_DEBUG=INFO

# å„ãƒãƒ¼ãƒ‰ã§è¨“ç·´ã‚’å®Ÿè¡Œ
srun python -u ddp_training_multi_node.py \
    --epochs 100 \
    --batch-size 128 \
    --lr 0.1
</code></pre>
</details>

<details>
<summary>ddp_training_multi_node.py - ãƒãƒ«ãƒãƒãƒ¼ãƒ‰å¯¾å¿œç‰ˆ</summary>

<pre><code>import os
import argparse
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup():
    """
    Slurmç’°å¢ƒå¤‰æ•°ã‹ã‚‰åˆ†æ•£è¨­å®šã‚’èª­ã¿è¾¼ã¿
    """
    # SlurmãŒè¨­å®šã™ã‚‹ç’°å¢ƒå¤‰æ•°
    rank = int(os.environ['SLURM_PROCID'])
    world_size = int(os.environ['SLURM_NTASKS'])
    local_rank = int(os.environ['SLURM_LOCALID'])

    # ãƒã‚¹ã‚¿ãƒ¼ã‚¢ãƒ‰ãƒ¬ã‚¹ã¨ãƒãƒ¼ãƒˆ
    master_addr = os.environ['MASTER_ADDR']
    master_port = os.environ['MASTER_PORT']

    # ç’°å¢ƒå¤‰æ•°è¨­å®š
    os.environ['MASTER_ADDR'] = master_addr
    os.environ['MASTER_PORT'] = master_port

    # åˆæœŸåŒ–
    dist.init_process_group(
        backend='nccl',
        init_method='env://',
        world_size=world_size,
        rank=rank
    )

    # ãƒ­ãƒ¼ã‚«ãƒ«GPUè¨­å®š
    torch.cuda.set_device(local_rank)

    return rank, world_size, local_rank

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--batch-size', type=int, default=128)
    parser.add_argument('--lr', type=float, default=0.1)
    args = parser.parse_args()

    # åˆ†æ•£ç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    rank, world_size, local_rank = setup()

    if rank == 0:
        print(f"Training with {world_size} processes across "
              f"{world_size // torch.cuda.device_count()} nodes")

    # ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã€è¨“ç·´ãƒ«ãƒ¼ãƒ—ã¯å‰è¿°ã¨åŒæ§˜
    # ...

    dist.destroy_process_group()

if __name__ == "__main__":
    main()
</code></pre>
</details>

---

<h2>4.3 Horovod</h2>

<h3>4.3.1 AllReduceã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h3>

<p><strong>Horovodã¨ã¯:</strong></p>
<ul>
    <li>Uberé–‹ç™ºã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åˆ†æ•£è¨“ç·´ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯</li>
    <li>TensorFlowã€PyTorchã€Kerasã€MXNetã«å¯¾å¿œ</li>
    <li>MPIãƒ™ãƒ¼ã‚¹ã®åŠ¹ç‡çš„ãªAllReduceé€šä¿¡</li>
</ul>

<p><strong>AllReduceã®ä»•çµ„ã¿:</strong></p>
<ul>
    <li><strong>Ring-AllReduce</strong>: ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªãƒ³ã‚°çŠ¶ã«é€šä¿¡</li>
    <li><strong>é€šä¿¡é‡</strong>: O(N)ï¼ˆNã¯å‹¾é…ã‚µã‚¤ã‚ºï¼‰ã€ãƒ—ãƒ­ã‚»ã‚¹æ•°ã«ä¾å­˜ã—ãªã„</li>
    <li><strong>å¸¯åŸŸå¹…åŠ¹ç‡</strong>: å…¨å¸¯åŸŸã‚’æ´»ç”¨</li>
</ul>

<h4>Ring-AllReduceã®å‹•ä½œ</h4>

<div class="mermaid">
sequenceDiagram
    participant GPU0
    participant GPU1
    participant GPU2
    participant GPU3

    Note over GPU0,GPU3: Step 1: Scatter-Reduce
    GPU0->>GPU1: Send chunk A
    GPU1->>GPU2: Send chunk B
    GPU2->>GPU3: Send chunk C
    GPU3->>GPU0: Send chunk D

    Note over GPU0,GPU3: Step 2: AllGather
    GPU0->>GPU1: Send reduced A
    GPU1->>GPU2: Send reduced B
    GPU2->>GPU3: Send reduced C
    GPU3->>GPU0: Send reduced D

    Note over GPU0,GPU3: All GPUs have complete reduced gradients
</div>

<h3>4.3.2 Horovod API</h3>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹4: Horovodã«ã‚ˆã‚‹PyTorchè¨“ç·´</h4>

<details>
<summary>horovod_training.py - ResNet18ã®Horovodè¨“ç·´</summary>

<pre><code>import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import horovod.torch as hvd

def train_horovod():
    """
    Horovodã‚’ä½¿ã£ãŸåˆ†æ•£è¨“ç·´
    """
    # Horovodã®åˆæœŸåŒ–
    hvd.init()

    # å„ãƒ—ãƒ­ã‚»ã‚¹ã‚’å¯¾å¿œã™ã‚‹GPUã«å‰²ã‚Šå½“ã¦
    torch.cuda.set_device(hvd.local_rank())
    device = torch.device('cuda')

    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™
    transform = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    dataset = torchvision.datasets.CIFAR10(
        root='./data',
        train=True,
        download=True,
        transform=transform
    )

    # Horovodç”¨ã‚µãƒ³ãƒ—ãƒ©ãƒ¼
    train_sampler = torch.utils.data.distributed.DistributedSampler(
        dataset,
        num_replicas=hvd.size(),
        rank=hvd.rank()
    )

    train_loader = DataLoader(
        dataset,
        batch_size=128,
        sampler=train_sampler,
        num_workers=4,
        pin_memory=True
    )

    # ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
    model = torchvision.models.resnet18(num_classes=10).to(device)

    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
    optimizer = torch.optim.SGD(
        model.parameters(),
        lr=0.1 * hvd.size(),  # å­¦ç¿’ç‡ã‚’ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã§ã‚¹ã‚±ãƒ¼ãƒ«
        momentum=0.9,
        weight_decay=5e-4
    )

    # Horovodã§ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’ãƒ©ãƒƒãƒ—
    optimizer = hvd.DistributedOptimizer(
        optimizer,
        named_parameters=model.named_parameters(),
        compression=hvd.Compression.fp16,  # FP16åœ§ç¸®ã§é€šä¿¡é‡å‰Šæ¸›
        op=hvd.Average  # å‹¾é…ã®å¹³å‡ã‚’å–ã‚‹
    )

    # åˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆï¼ˆå…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã§åŒã˜åˆæœŸå€¤ï¼‰
    hvd.broadcast_parameters(model.state_dict(), root_rank=0)
    hvd.broadcast_optimizer_state(optimizer, root_rank=0)

    # æå¤±é–¢æ•°
    criterion = nn.CrossEntropyLoss()

    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    num_epochs = 100
    for epoch in range(num_epochs):
        model.train()
        train_sampler.set_epoch(epoch)

        epoch_loss = 0.0
        correct = 0
        total = 0

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()

            # HorovodãŒè‡ªå‹•çš„ã«å‹¾é…ã‚’AllReduce
            optimizer.step()

            # çµ±è¨ˆ
            epoch_loss += loss.item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()

        # å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã§çµ±è¨ˆã‚’é›†ç´„
        epoch_loss = metric_average(epoch_loss, 'avg_loss')
        accuracy = metric_average(correct / total, 'avg_accuracy')

        # Rank 0ã®ã¿ãƒ­ã‚°å‡ºåŠ›
        if hvd.rank() == 0:
            print(f"Epoch {epoch}/{num_epochs} - "
                  f"Loss: {epoch_loss:.4f}, Accuracy: {accuracy*100:.2f}%")

            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            if (epoch + 1) % 10 == 0:
                torch.save(model.state_dict(),
                          f'horovod_checkpoint_epoch_{epoch+1}.pth')

def metric_average(val, name):
    """
    å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å¹³å‡
    """
    tensor = torch.tensor(val)
    avg_tensor = hvd.allreduce(tensor, name=name)
    return avg_tensor.item()

if __name__ == "__main__":
    train_horovod()
</code></pre>
</details>

<p><strong>å®Ÿè¡Œæ–¹æ³•:</strong></p>
<pre><code># å˜ä¸€ãƒãƒ¼ãƒ‰ã€4 GPU
horovodrun -np 4 python horovod_training.py

# è¤‡æ•°ãƒãƒ¼ãƒ‰ï¼ˆå„ãƒãƒ¼ãƒ‰4 GPUã€2ãƒãƒ¼ãƒ‰ï¼‰
horovodrun -np 8 -H node1:4,node2:4 python horovod_training.py

# Slurmã‚¯ãƒ©ã‚¹ã‚¿
srun --ntasks=8 --gres=gpu:4 python horovod_training.py
</code></pre>

<h3>4.3.3 TensorFlow/PyTorchçµ±åˆ</h3>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹5: Horovodã«ã‚ˆã‚‹TensorFlowè¨“ç·´</h4>

<details>
<summary>horovod_tensorflow.py - TensorFlowã§ã®Horovodä½¿ç”¨</summary>

<pre><code>import tensorflow as tf
import horovod.tensorflow as hvd

def train_tensorflow_horovod():
    """
    Horovod + TensorFlowã§ã®åˆ†æ•£è¨“ç·´
    """
    # HorovodåˆæœŸåŒ–
    hvd.init()

    # GPUãƒ¡ãƒ¢ãƒªæˆé•·ã‚’æœ‰åŠ¹åŒ–
    gpus = tf.config.experimental.list_physical_devices('GPU')
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    if gpus:
        tf.config.experimental.set_visible_devices(
            gpus[hvd.local_rank()], 'GPU'
        )

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
    x_train = x_train.astype('float32') / 255.0
    x_test = x_test.astype('float32') / 255.0

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆå„ãƒ¯ãƒ¼ã‚«ãƒ¼ã«ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ï¼‰
    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
    dataset = dataset.shard(num_shards=hvd.size(), index=hvd.rank())
    dataset = dataset.shuffle(10000).batch(128).prefetch(tf.data.AUTOTUNE)

    # ãƒ¢ãƒ‡ãƒ«
    model = tf.keras.applications.ResNet50(
        include_top=True,
        weights=None,
        classes=10,
        input_shape=(32, 32, 3)
    )

    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
    optimizer = tf.keras.optimizers.SGD(
        learning_rate=0.1 * hvd.size(),
        momentum=0.9
    )

    # Horovodã§ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’ãƒ©ãƒƒãƒ—
    optimizer = hvd.DistributedOptimizer(
        optimizer,
        compression=hvd.Compression.fp16
    )

    # æå¤±é–¢æ•°ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    @tf.function
    def training_step(images, labels, first_batch):
        with tf.GradientTape() as tape:
            predictions = model(images, training=True)
            loss = loss_fn(labels, predictions)

        # HorovodãŒå‹¾é…ã‚’AllReduce
        tape = hvd.DistributedGradientTape(tape, compression=hvd.Compression.fp16)
        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))

        # åˆå›ãƒãƒƒãƒã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ
        if first_batch:
            hvd.broadcast_variables(model.variables, root_rank=0)
            hvd.broadcast_variables(optimizer.variables(), root_rank=0)

        return loss

    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    for epoch in range(100):
        epoch_loss = 0.0
        for batch_idx, (images, labels) in enumerate(dataset):
            loss = training_step(images, labels, batch_idx == 0 and epoch == 0)
            epoch_loss += loss.numpy()

        # å¹³å‡æå¤±ã‚’è¨ˆç®—
        epoch_loss = hvd.allreduce(
            tf.constant(epoch_loss / len(dataset)),
            average=True
        ).numpy()

        if hvd.rank() == 0:
            print(f"Epoch {epoch}, Loss: {epoch_loss:.4f}")

            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            if (epoch + 1) % 10 == 0:
                model.save(f'tf_horovod_model_epoch_{epoch+1}.h5')

if __name__ == "__main__":
    train_tensorflow_horovod()
</code></pre>
</details>

<h3>4.3.4 æ€§èƒ½æ¯”è¼ƒ: PyTorch DDP vs Horovod</h3>

<table>
    <thead>
        <tr>
            <th>é …ç›®</th>
            <th>PyTorch DDP</th>
            <th>Horovod</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><strong>é€šä¿¡ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰</strong></td>
            <td>NCCL, Gloo, MPI</td>
            <td>MPI, NCCL</td>
        </tr>
        <tr>
            <td><strong>ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å¯¾å¿œ</strong></td>
            <td>PyTorchå°‚ç”¨</td>
            <td>TensorFlow, PyTorch, Keras, MXNet</td>
        </tr>
        <tr>
            <td><strong>å®Ÿè£…ã®è¤‡é›‘ã•</strong></td>
            <td>ä¸­ç¨‹åº¦</td>
            <td>ã‚·ãƒ³ãƒ—ãƒ«</td>
        </tr>
        <tr>
            <td><strong>é€šä¿¡åŠ¹ç‡</strong></td>
            <td>é«˜ã„ï¼ˆNCCLæœ€é©åŒ–ï¼‰</td>
            <td>é«˜ã„ï¼ˆRing-AllReduceï¼‰</td>
        </tr>
        <tr>
            <td><strong>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£</strong></td>
            <td>æ•°ç™¾GPU</td>
            <td>æ•°åƒGPUï¼ˆMPIãƒ™ãƒ¼ã‚¹ï¼‰</td>
        </tr>
        <tr>
            <td><strong>å‹¾é…åœ§ç¸®</strong></td>
            <td>æ‰‹å‹•å®Ÿè£…</td>
            <td>æ¨™æº–ã‚µãƒãƒ¼ãƒˆï¼ˆFP16ï¼‰</td>
        </tr>
        <tr>
            <td><strong>å‹•çš„ã‚°ãƒ©ãƒ•å¯¾å¿œ</strong></td>
            <td>å®Œå…¨å¯¾å¿œ</td>
            <td>å®Œå…¨å¯¾å¿œ</td>
        </tr>
        <tr>
            <td><strong>ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ </strong></td>
            <td>PyTorchå…¬å¼</td>
            <td>ç‹¬ç«‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ</td>
        </tr>
    </tbody>
</table>

<p><strong>ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœï¼ˆResNet-50ã€ImageNetã€8 GPUï¼‰:</strong></p>
<ul>
    <li><strong>PyTorch DDP</strong>: 2,400 images/secï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡ 92%ï¼‰</li>
    <li><strong>Horovod</strong>: 2,350 images/secï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡ 90%ï¼‰</li>
</ul>

<p><strong>æ¨å¥¨äº‹é …:</strong></p>
<ul>
    <li><strong>PyTorchã®ã¿ä½¿ç”¨</strong> â†’ PyTorch DDP</li>
    <li><strong>è¤‡æ•°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯</strong> â†’ Horovod</li>
    <li><strong>å¤§è¦æ¨¡ã‚¯ãƒ©ã‚¹ã‚¿ï¼ˆ100+ GPUï¼‰</strong> â†’ Horovodï¼ˆMPIã®å®‰å®šæ€§ï¼‰</li>
</ul>

---

<h2>4.4 å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</h2>

<h3>4.4.1 Gradient Accumulationï¼ˆå‹¾é…ç´¯ç©ï¼‰</h3>

<p><strong>ç›®çš„:</strong></p>
<ul>
    <li>GPUãƒ¡ãƒ¢ãƒªåˆ¶ç´„ä¸‹ã§å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å®Ÿç¾</li>
    <li>å°ãƒãƒƒãƒã‚’è¤‡æ•°å›å®Ÿè¡Œã—ã€å‹¾é…ã‚’ç´¯ç©</li>
</ul>

<p><strong>æ•°å¼:</strong></p>
$$
\nabla_\theta L_{\text{effective}} = \frac{1}{K} \sum_{k=1}^{K} \nabla_\theta L(\text{mini-batch}_k)
$$

<p>$K$: ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—æ•°ã€å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º = $K \times$ mini-batch size</p>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹6: Gradient Accumulationã®å®Ÿè£…</h4>

<details>
<summary>gradient_accumulation.py - å‹¾é…ç´¯ç©</summary>

<pre><code>import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision

def train_with_gradient_accumulation(
    model, dataloader, optimizer, criterion,
    accumulation_steps=4, device='cuda'
):
    """
    å‹¾é…ç´¯ç©ã‚’ä½¿ã£ãŸè¨“ç·´

    Args:
        accumulation_steps: å‹¾é…ã‚’ç´¯ç©ã™ã‚‹ã‚¹ãƒ†ãƒƒãƒ—æ•°
    """
    model.train()
    optimizer.zero_grad()

    for batch_idx, (data, target) in enumerate(dataloader):
        data, target = data.to(device), target.to(device)

        # é †ä¼æ’­
        output = model(data)
        loss = criterion(output, target)

        # æå¤±ã‚’ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—æ•°ã§å‰²ã‚‹
        loss = loss / accumulation_steps

        # é€†ä¼æ’­ï¼ˆå‹¾é…ã‚’ç´¯ç©ï¼‰
        loss.backward()

        # accumulation_stepsã”ã¨ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
        if (batch_idx + 1) % accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()

            print(f"Batch {batch_idx+1}, Updated parameters")

    # æœ€å¾Œã®ä½™ã‚Šã®ãƒãƒƒãƒã‚’å‡¦ç†
    if (batch_idx + 1) % accumulation_steps != 0:
        optimizer.step()
        optimizer.zero_grad()

# ä½¿ç”¨ä¾‹
model = torchvision.models.resnet50().cuda()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
criterion = nn.CrossEntropyLoss()

# å°ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆ16ï¼‰Ã— ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆ4ï¼‰= å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆ64ï¼‰
train_loader = DataLoader(dataset, batch_size=16, shuffle=True)

train_with_gradient_accumulation(
    model, train_loader, optimizer, criterion,
    accumulation_steps=4
)
</code></pre>
</details>

<p><strong>ãƒ¡ãƒªãƒƒãƒˆ:</strong></p>
<ul>
    <li>GPUãƒ¡ãƒ¢ãƒªåˆ¶ç´„ã‚’å›é¿</li>
    <li>å¤§ãƒãƒƒãƒã‚µã‚¤ã‚ºã®è¨“ç·´ã‚’å¯èƒ½ã«</li>
</ul>

<p><strong>ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ:</strong></p>
<ul>
    <li>è¨“ç·´é€Ÿåº¦ã®ä½ä¸‹ï¼ˆé€šä¿¡ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å‰Šæ¸›ã®åŠ¹æœã¯å°ï¼‰</li>
    <li>Batch Normalizationã®æŒ™å‹•ã«æ³¨æ„ï¼ˆå°ãƒãƒƒãƒã§ã®çµ±è¨ˆï¼‰</li>
</ul>

<h3>4.4.2 Mixed Precision Training (AMP)</h3>

<p><strong>æ¦‚è¦:</strong></p>
<ul>
    <li>FP16ï¼ˆåŠç²¾åº¦æµ®å‹•å°æ•°ç‚¹ï¼‰ã¨FP32ã‚’æ··åœ¨ã•ã›ã¦è¨“ç·´</li>
    <li>è¨ˆç®—é«˜é€ŸåŒ–ã¨ãƒ¡ãƒ¢ãƒªå‰Šæ¸›</li>
    <li>æå¤±ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã§æ•°å€¤å®‰å®šæ€§ã‚’ç¢ºä¿</li>
</ul>

<p><strong>åŠ¹æœ:</strong></p>
<ul>
    <li><strong>é«˜é€ŸåŒ–</strong>: 1.5ï½2å€ï¼ˆTensor Coreæ´»ç”¨ï¼‰</li>
    <li><strong>ãƒ¡ãƒ¢ãƒªå‰Šæ¸›</strong>: ç´„50%</li>
</ul>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹7: PyTorch AMPã®ä½¿ç”¨</h4>

<details>
<summary>amp_training.py - Automatic Mixed Precision</summary>

<pre><code>import torch
import torch.nn as nn
from torch.cuda.amp import autocast, GradScaler
import torchvision

def train_with_amp(model, dataloader, optimizer, criterion, device='cuda'):
    """
    Automatic Mixed Precision (AMP) ã‚’ä½¿ã£ãŸè¨“ç·´
    """
    model.train()

    # GradScaler: FP16ã®å‹¾é…ã‚’é©åˆ‡ã«ã‚¹ã‚±ãƒ¼ãƒ«
    scaler = GradScaler()

    for batch_idx, (data, target) in enumerate(dataloader):
        data, target = data.to(device), target.to(device)

        optimizer.zero_grad()

        # autocast: è‡ªå‹•çš„ã«æœ€é©ãªç²¾åº¦ã‚’é¸æŠ
        with autocast():
            output = model(data)
            loss = criterion(output, target)

        # ã‚¹ã‚±ãƒ¼ãƒ«ã—ãŸæå¤±ã§é€†ä¼æ’­
        scaler.scale(loss).backward()

        # å‹¾é…ã‚’ã‚¢ãƒ³ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
        scaler.step(optimizer)
        scaler.update()

        if batch_idx % 100 == 0:
            print(f"Batch {batch_idx}, Loss: {loss.item():.4f}")

# ä½¿ç”¨ä¾‹
model = torchvision.models.resnet50().cuda()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
criterion = nn.CrossEntropyLoss()

train_loader = torch.utils.data.DataLoader(dataset, batch_size=128)

train_with_amp(model, train_loader, optimizer, criterion)
</code></pre>
</details>

<p><strong>AMP + Gradient Accumulation + DDP</strong>ã®çµ„ã¿åˆã‚ã›:</p>

<details>
<summary>amp_grad_accum_ddp.py - å®Œå…¨ãªæœ€é©åŒ–</summary>

<pre><code>import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler

def train_optimized(
    rank, world_size, model, dataset,
    batch_size=32, accumulation_steps=4, epochs=100
):
    """
    AMP + Gradient Accumulation + DDP ã®å®Œå…¨ãªå®Ÿè£…
    """
    # åˆ†æ•£ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

    # ãƒ¢ãƒ‡ãƒ«ã‚’DDPã§ãƒ©ãƒƒãƒ—
    model = model.cuda(rank)
    model = DDP(model, device_ids=[rank])

    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)
    dataloader = DataLoader(
        dataset, batch_size=batch_size, sampler=sampler, num_workers=4
    )

    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨æå¤±é–¢æ•°
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss()
    scaler = GradScaler()

    for epoch in range(epochs):
        sampler.set_epoch(epoch)
        model.train()
        optimizer.zero_grad()

        for batch_idx, (data, target) in enumerate(dataloader):
            data, target = data.cuda(rank), target.cuda(rank)

            # AMPã«ã‚ˆã‚‹é †ä¼æ’­
            with autocast():
                output = model(data)
                loss = criterion(output, target) / accumulation_steps

            # å‹¾é…ç´¯ç©
            scaler.scale(loss).backward()

            # ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
            if (batch_idx + 1) % accumulation_steps == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()

        # æœ€å¾Œã®ä½™ã‚Šã‚’å‡¦ç†
        if (batch_idx + 1) % accumulation_steps != 0:
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

        if rank == 0:
            print(f"Epoch {epoch} completed")

    dist.destroy_process_group()

# å®Ÿè¡Œ
if __name__ == "__main__":
    import torch.multiprocessing as mp

    world_size = torch.cuda.device_count()
    model = torchvision.models.resnet50()

    mp.spawn(
        train_optimized,
        args=(world_size, model, dataset),
        nprocs=world_size,
        join=True
    )
</code></pre>
</details>

<h3>4.4.3 Gradient Checkpointingï¼ˆå‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼‰</h3>

<p><strong>æ¦‚è¦:</strong></p>
<ul>
    <li>é †ä¼æ’­æ™‚ã«ä¸­é–“æ´»æ€§åŒ–ã‚’ä¿å­˜ã›ãšã€é€†ä¼æ’­æ™‚ã«å†è¨ˆç®—</li>
    <li>ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å¤§å¹…å‰Šæ¸›ï¼ˆè¨ˆç®—æ™‚é–“ã¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰</li>
</ul>

<p><strong>åŠ¹æœ:</strong></p>
<ul>
    <li><strong>ãƒ¡ãƒ¢ãƒªå‰Šæ¸›</strong>: O(n) â†’ O(âˆšn)ï¼ˆnã¯ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ï¼‰</li>
    <li><strong>è¨ˆç®—å¢—åŠ </strong>: ç´„20-30%</li>
</ul>

<pre><code>from torch.utils.checkpoint import checkpoint

class CheckpointedResNet(nn.Module):
    def __init__(self, original_model):
        super().__init__()
        self.layer1 = original_model.layer1
        self.layer2 = original_model.layer2
        self.layer3 = original_model.layer3
        self.layer4 = original_model.layer4
        self.fc = original_model.fc

    def forward(self, x):
        # å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§checkpointä½¿ç”¨
        x = checkpoint(self.layer1, x)
        x = checkpoint(self.layer2, x)
        x = checkpoint(self.layer3, x)
        x = checkpoint(self.layer4, x)
        x = self.fc(x)
        return x
</code></pre>

<h3>4.4.4 DeepSpeed ZeRO</h3>

<p><strong>ZeROï¼ˆZero Redundancy Optimizerï¼‰:</strong></p>
<ul>
    <li>Microsofté–‹ç™ºã®è¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯</li>
    <li>ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã€å‹¾é…ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆ†æ•£</li>
</ul>

<p><strong>ZeROã®3æ®µéš:</strong></p>
<ul>
    <li><strong>ZeRO-1</strong>: ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã®åˆ†å‰²ï¼ˆ4å€ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰</li>
    <li><strong>ZeRO-2</strong>: + å‹¾é…ã®åˆ†å‰²ï¼ˆ8å€ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰</li>
    <li><strong>ZeRO-3</strong>: + ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆ†å‰²ï¼ˆNãƒ¯ãƒ¼ã‚«ãƒ¼ã§Nå€å‰Šæ¸›ï¼‰</li>
</ul>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹8: DeepSpeed ZeROã®ä½¿ç”¨</h4>

<details>
<summary>deepspeed_zero.py - DeepSpeedè¨“ç·´</summary>

<pre><code>import torch
import torch.nn as nn
import deepspeed
from transformers import GPT2LMHeadModel, GPT2Tokenizer

def train_with_deepspeed():
    """
    DeepSpeed ZeROã‚’ä½¿ã£ãŸå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    """
    # DeepSpeedè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
    ds_config = {
        "train_batch_size": 64,
        "gradient_accumulation_steps": 4,
        "optimizer": {
            "type": "Adam",
            "params": {
                "lr": 1e-5,
                "betas": [0.9, 0.999],
                "eps": 1e-8,
                "weight_decay": 0.01
            }
        },
        "fp16": {
            "enabled": True,
            "loss_scale": 0,
            "loss_scale_window": 1000,
            "hysteresis": 2,
            "min_loss_scale": 1
        },
        "zero_optimization": {
            "stage": 3,  # ZeRO-3: æœ€å¤§ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
            "offload_optimizer": {
                "device": "cpu",  # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã‚’CPUã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰
                "pin_memory": True
            },
            "offload_param": {
                "device": "cpu",  # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’CPUã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰
                "pin_memory": True
            },
            "overlap_comm": True,
            "contiguous_gradients": True,
            "sub_group_size": 1e9,
            "reduce_bucket_size": 5e8,
            "stage3_prefetch_bucket_size": 5e8,
            "stage3_param_persistence_threshold": 1e6,
            "stage3_max_live_parameters": 1e9,
            "stage3_max_reuse_distance": 1e9,
            "stage3_gather_fp16_weights_on_model_save": True
        },
        "gradient_clipping": 1.0,
        "wall_clock_breakdown": False
    }

    # å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆGPT-2 Large: 774M parametersï¼‰
    model = GPT2LMHeadModel.from_pretrained('gpt2-large')

    # DeepSpeedã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–
    model_engine, optimizer, _, _ = deepspeed.initialize(
        model=model,
        model_parameters=model.parameters(),
        config=ds_config
    )

    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    train_loader = ...  # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼æº–å‚™

    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    for epoch in range(10):
        for batch in train_loader:
            inputs = batch['input_ids'].to(model_engine.local_rank)
            labels = batch['labels'].to(model_engine.local_rank)

            # é †ä¼æ’­
            outputs = model_engine(inputs, labels=labels)
            loss = outputs.loss

            # DeepSpeedãŒè‡ªå‹•ã§é€†ä¼æ’­ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
            model_engine.backward(loss)
            model_engine.step()

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆå…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é›†ç´„ï¼‰
    model_engine.save_checkpoint('./checkpoints')

if __name__ == "__main__":
    # DeepSpeedèµ·å‹•
    # deepspeed --num_gpus=8 deepspeed_zero.py
    train_with_deepspeed()
</code></pre>
</details>

<p><strong>å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰:</strong></p>
<pre><code># å˜ä¸€ãƒãƒ¼ãƒ‰ã€8 GPU
deepspeed --num_gpus=8 deepspeed_zero.py

# è¤‡æ•°ãƒãƒ¼ãƒ‰ï¼ˆ2ãƒãƒ¼ãƒ‰ã€å„8 GPUï¼‰
deepspeed --num_nodes=2 --num_gpus=8 --hostfile=hostfile deepspeed_zero.py
</code></pre>

<p><strong>ZeRO-3ã®åŠ¹æœï¼ˆGPT-3 175B parametersï¼‰:</strong></p>
<ul>
    <li><strong>å¾“æ¥ã®DDP</strong>: è¨“ç·´ä¸å¯ï¼ˆGPUãƒ¡ãƒ¢ãƒªä¸è¶³ï¼‰</li>
    <li><strong>ZeRO-3</strong>: 128 GPUï¼ˆå„40GBï¼‰ã§è¨“ç·´å¯èƒ½</li>
    <li><strong>ãƒ¡ãƒ¢ãƒªå‰Šæ¸›</strong>: 64å€ï¼ˆ128ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼‰</li>
</ul>

---

<h2>4.5 åˆ†æ•£å­¦ç¿’ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h2>

<h3>4.5.1 é€šä¿¡æœ€é©åŒ–</h3>

<p><strong>å‹¾é…ãƒã‚±ãƒƒãƒ†ã‚£ãƒ³ã‚°ï¼ˆGradient Bucketingï¼‰:</strong></p>
<ul>
    <li>å°ã•ãªå‹¾é…ã‚’ã¾ã¨ã‚ã¦é€šä¿¡ï¼ˆãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å‰Šæ¸›ï¼‰</li>
    <li>PyTorch DDPã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ‰åŠ¹</li>
</ul>

<pre><code># DDPã®ãƒã‚±ãƒƒãƒˆã‚µã‚¤ã‚ºè¨­å®š
model = DDP(
    model,
    device_ids=[rank],
    bucket_cap_mb=25,  # ãƒã‚±ãƒƒãƒˆã‚µã‚¤ã‚ºï¼ˆMBï¼‰
    find_unused_parameters=False  # ä½¿ç”¨ã—ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¤œå‡ºã‚’ç„¡åŠ¹åŒ–
)
</code></pre>

<p><strong>å‹¾é…åœ§ç¸®:</strong></p>
<ul>
    <li>FP16åœ§ç¸®ã§é€šä¿¡é‡50%å‰Šæ¸›</li>
    <li>ã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–ã€é‡å­åŒ–</li>
</ul>

<pre><code># Horovodã®fp16åœ§ç¸®
optimizer = hvd.DistributedOptimizer(
    optimizer,
    compression=hvd.Compression.fp16
)
</code></pre>

<p><strong>éšå±¤çš„AllReduce:</strong></p>
<ul>
    <li>ãƒãƒ¼ãƒ‰å†…NCCL â†’ ãƒãƒ¼ãƒ‰é–“MPI</li>
    <li>Horovod Hierarchical AllReduce</li>
</ul>

<h3>4.5.2 ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨å­¦ç¿’ç‡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°</h3>

<p><strong>Linear Scaling Ruleï¼ˆç·šå½¢ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ï¼‰:</strong></p>
$$
\text{LR}_{\text{distributed}} = \text{LR}_{\text{base}} \times \frac{\text{Batch}_{\text{distributed}}}{\text{Batch}_{\text{base}}}
$$

<p><strong>ä¾‹:</strong></p>
<ul>
    <li>ãƒ™ãƒ¼ã‚¹: LR=0.1, Batch=256ï¼ˆå˜ä¸€GPUï¼‰</li>
    <li>8 GPU: LR=0.8, Batch=2,048ï¼ˆ256Ã—8ï¼‰</li>
</ul>

<p><strong>Warmupï¼ˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼‰:</strong></p>
<ul>
    <li>æœ€åˆã®ã‚¨ãƒãƒƒã‚¯ã§å­¦ç¿’ç‡ã‚’å¾ã€…ã«å¢—åŠ </li>
    <li>å¤§ãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã®è¨“ç·´å®‰å®šåŒ–</li>
</ul>

<pre><code>def linear_warmup_cosine_decay(step, warmup_steps, total_steps, base_lr, max_lr):
    """
    Warmup + Cosine Decay å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
    """
    if step < warmup_steps:
        # Warmup: ç·šå½¢å¢—åŠ 
        lr = base_lr + (max_lr - base_lr) * step / warmup_steps
    else:
        # Cosine Decay
        progress = (step - warmup_steps) / (total_steps - warmup_steps)
        lr = base_lr + 0.5 * (max_lr - base_lr) * (1 + math.cos(math.pi * progress))
    return lr
</code></pre>

<h3>4.5.3 å­¦ç¿’ç‡èª¿æ•´ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>

<p><strong>LARSï¼ˆLayer-wise Adaptive Rate Scalingï¼‰:</strong></p>
<ul>
    <li>ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«å­¦ç¿’ç‡ã‚’é©å¿œèª¿æ•´</li>
    <li>è¶…å¤§ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆ32Kï½64Kï¼‰ã§æœ‰åŠ¹</li>
</ul>

<p><strong>LAMBï¼ˆLayer-wise Adaptive Moments optimizer for Batch trainingï¼‰:</strong></p>
<ul>
    <li>BERTè¨“ç·´ã§ãƒãƒƒãƒã‚µã‚¤ã‚º65,536ã‚’å®Ÿç¾</li>
    <li>Adamãƒ™ãƒ¼ã‚¹ã®LARSæ‹¡å¼µ</li>
</ul>

<h3>4.5.4 ãƒ‡ãƒãƒƒã‚°åˆ†æ•£ã‚³ãƒ¼ãƒ‰</h3>

<p><strong>ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼:</strong></p>

<p><strong>1. ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯:</strong></p>
<ul>
    <li>å…¨ãƒ—ãƒ­ã‚»ã‚¹ã§åŒã˜é›†å›£é€šä¿¡ã‚’å®Ÿè¡Œ</li>
    <li>ä¸€éƒ¨ã®ãƒ—ãƒ­ã‚»ã‚¹ã®ã¿ãŒé€šä¿¡ã™ã‚‹ã¨ãƒãƒ³ã‚°</li>
</ul>

<pre><code># æ‚ªã„ä¾‹: Rank 0ã®ã¿ãŒallreduce
if rank == 0:
    dist.all_reduce(tensor)  # ä»–ã®ãƒ—ãƒ­ã‚»ã‚¹ãŒå¾…æ©Ÿ â†’ ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯

# æ­£ã—ã„ä¾‹: å…¨ãƒ—ãƒ­ã‚»ã‚¹ãŒallreduce
dist.all_reduce(tensor)
if rank == 0:
    print(tensor)
</code></pre>

<p><strong>2. ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯:</strong></p>
<ul>
    <li>å‹¾é…ç´¯ç©æ™‚ã®detachå¿˜ã‚Œ</li>
    <li>è¨ˆç®—ã‚°ãƒ©ãƒ•ãŒä¿æŒã•ã‚Œç¶šã‘ã‚‹</li>
</ul>

<pre><code># å‹¾é…ç´¯ç©æ™‚ã®æ­£ã—ã„å®Ÿè£…
loss = loss / accumulation_steps
loss.backward()

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—æ™‚ã¯detach
total_loss += loss.detach().item()
</code></pre>

<p><strong>3. å†ç¾æ€§ã®æ¬ å¦‚:</strong></p>
<ul>
    <li>ã‚·ãƒ¼ãƒ‰è¨­å®šãŒä¸ååˆ†</li>
    <li>å„ãƒ—ãƒ­ã‚»ã‚¹ã§ç•°ãªã‚‹åˆæœŸåŒ–</li>
</ul>

<pre><code>def set_seed(seed, rank):
    """
    å†ç¾æ€§ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰è¨­å®š
    """
    torch.manual_seed(seed + rank)  # ãƒ©ãƒ³ã‚¯ã”ã¨ã«ç•°ãªã‚‹ã‚·ãƒ¼ãƒ‰
    torch.cuda.manual_seed_all(seed + rank)
    np.random.seed(seed + rank)
    random.seed(seed + rank)

    # CuDNNã®æ±ºå®šçš„å‹•ä½œï¼ˆé€Ÿåº¦ä½ä¸‹ã‚ã‚Šï¼‰
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
</code></pre>

<p><strong>ãƒ‡ãƒãƒƒã‚°ãƒ„ãƒ¼ãƒ«:</strong></p>

<p><strong>NCCLç’°å¢ƒå¤‰æ•°:</strong></p>
<pre><code>export NCCL_DEBUG=INFO           # NCCLã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±
export NCCL_DEBUG_SUBSYS=ALL     # å…¨ã‚µãƒ–ã‚·ã‚¹ãƒ†ãƒ ã‚’ãƒ­ã‚°
export NCCL_IB_DISABLE=1         # InfiniBandã‚’ç„¡åŠ¹åŒ–ï¼ˆãƒ‡ãƒãƒƒã‚°æ™‚ï¼‰
export NCCL_P2P_DISABLE=1        # P2Pé€šä¿¡ã‚’ç„¡åŠ¹åŒ–
</code></pre>

<p><strong>PyTorchãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©:</strong></p>
<pre><code>from torch.profiler import profile, ProfilerActivity

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    record_shapes=True,
    with_stack=True
) as prof:
    for data, target in train_loader:
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
</code></pre>

---

<h2>4.6 ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
    <li>
        <p><strong>åˆ†æ•£å­¦ç¿’ã®æˆ¦ç•¥:</strong></p>
        <ul>
            <li>Data Parallelism: ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ”ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã€å‹¾é…é›†ç´„</li>
            <li>Model Parallelism: ãƒ¢ãƒ‡ãƒ«åˆ†å‰²ã€ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ã®è§£æ±º</li>
            <li>Pipeline Parallelism: ã‚¹ãƒ†ãƒ¼ã‚¸åˆ†å‰²ã€GPUåˆ©ç”¨ç‡å‘ä¸Š</li>
            <li>Hybrid Approaches: 3D Parallelismã€ZeRO</li>
        </ul>
    </li>
    <li>
        <p><strong>PyTorch DDP:</strong></p>
        <ul>
            <li>torch.distributedã®åŸºç¤ï¼ˆRankã€World Sizeã€Backendï¼‰</li>
            <li>DistributedSamplerã§ãƒ‡ãƒ¼ã‚¿åˆ†å‰²</li>
            <li>DDPãƒ©ãƒƒãƒ‘ãƒ¼ã§è‡ªå‹•å‹¾é…åŒæœŸ</li>
            <li>ãƒãƒ«ãƒãƒãƒ¼ãƒ‰è¨“ç·´ï¼ˆSlurmã€SSHï¼‰</li>
        </ul>
    </li>
    <li>
        <p><strong>Horovod:</strong></p>
        <ul>
            <li>Ring-AllReduceã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</li>
            <li>MPI/NCCLãƒ™ãƒ¼ã‚¹ã®åŠ¹ç‡çš„é€šä¿¡</li>
            <li>TensorFlow/PyTorch/Keraså¯¾å¿œ</li>
            <li>FP16åœ§ç¸®ã§ã•ã‚‰ãªã‚‹é«˜é€ŸåŒ–</li>
        </ul>
    </li>
    <li>
        <p><strong>å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´:</strong></p>
        <ul>
            <li>Gradient Accumulation: ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ã®å›é¿</li>
            <li>Mixed Precision (AMP): 1.5ï½2å€é«˜é€ŸåŒ–ã€50%ãƒ¡ãƒ¢ãƒªå‰Šæ¸›</li>
            <li>Gradient Checkpointing: O(n) â†’ O(âˆšn)ãƒ¡ãƒ¢ãƒªå‰Šæ¸›</li>
            <li>DeepSpeed ZeRO: è¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆ175B parametersï¼‰è¨“ç·´</li>
        </ul>
    </li>
    <li>
        <p><strong>ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹:</strong></p>
        <ul>
            <li>é€šä¿¡æœ€é©åŒ–: ãƒã‚±ãƒƒãƒ†ã‚£ãƒ³ã‚°ã€åœ§ç¸®ã€éšå±¤çš„AllReduce</li>
            <li>ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: Linear Scaling Ruleã€Warmup</li>
            <li>å­¦ç¿’ç‡èª¿æ•´: LARSã€LAMB</li>
            <li>ãƒ‡ãƒãƒƒã‚°: ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯å›é¿ã€ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯å¯¾ç­–ã€å†ç¾æ€§</li>
        </ul>
    </li>
</ol>

<h3>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>

<p>ç¬¬5ç« ã§ã¯ã€å®Ÿä¸–ç•Œã®å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å­¦ã³ã¾ã™:</p>
<ul>
    <li>æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®åˆ†æ•£è¨“ç·´ï¼ˆNetflixã€Amazonè¦æ¨¡ï¼‰</li>
    <li>å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ï¼ˆBERTã€GPTï¼‰</li>
    <li>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ï¼ˆApache Kafka + Spark Streamingï¼‰</li>
    <li>ãƒãƒ†ãƒªã‚¢ãƒ«ã‚ºãƒ»ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹ã§ã®å¤§è¦æ¨¡ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°</li>
</ul>

---

<h2>æ¼”ç¿’å•é¡Œ</h2>

<p><strong>å•1:</strong> Data Parallelismã¨Model Parallelismã®é•ã„ã‚’ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨é€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚</p>

<p><strong>å•2:</strong> 8 GPUã§è¨“ç·´ã™ã‚‹å ´åˆã€å˜ä¸€GPUã§ãƒãƒƒãƒã‚µã‚¤ã‚º32ã€å­¦ç¿’ç‡0.1ã ã£ãŸå ´åˆã€åˆ†æ•£è¨“ç·´ã§ã®é©åˆ‡ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã¨å­¦ç¿’ç‡ã‚’è¨ˆç®—ã›ã‚ˆã€‚</p>

<p><strong>å•3:</strong> Gradient Accumulationã‚’ä½¿ã£ã¦ã€å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º256ã‚’å®Ÿç¾ã—ãŸã„ã€‚GPUãƒ¡ãƒ¢ãƒªåˆ¶ç´„ã§ãƒãƒƒãƒã‚µã‚¤ã‚º32ã—ã‹æ‰±ãˆãªã„å ´åˆã€accumulation_stepsã‚’ã„ãã¤ã«è¨­å®šã™ã¹ãã‹ã€‚</p>

<p><strong>å•4:</strong> Mixed Precision Training (AMP) ãŒæ•°å€¤å®‰å®šæ€§ã‚’ä¿ã¤ãŸã‚ã«ä½¿ç”¨ã™ã‚‹ã€Œæå¤±ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€ã®ä»•çµ„ã¿ã‚’èª¬æ˜ã›ã‚ˆã€‚</p>

<p><strong>å•5:</strong> DeepSpeed ZeRO-3ãŒã€å¾“æ¥ã®Data Parallelismã¨æ¯”ã¹ã¦ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒé«˜ã„ç†ç”±ã‚’ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã€å‹¾é…ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆ†æ•£ã¨ã„ã†è¦³ç‚¹ã‹ã‚‰è«–ã˜ã‚ˆï¼ˆ500å­—ä»¥å†…ï¼‰ã€‚</p>

---

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
    <li>Goyal, P. et al. "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour." <em>arXiv:1706.02677</em> (2017).</li>
    <li>Sergeev, A. & Del Balso, M. "Horovod: fast and easy distributed deep learning in TensorFlow." <em>arXiv:1802.05799</em> (2018).</li>
    <li>Li, S. et al. "PyTorch Distributed: Experiences on Accelerating Data Parallel Training." <em>VLDB</em> (2020).</li>
    <li>Rajbhandari, S. et al. "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models." <em>SC'20</em> (2020).</li>
    <li>Huang, Y. et al. "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism." <em>NeurIPS</em> (2019).</li>
    <li>You, Y. et al. "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes." <em>ICLR</em> (2020).</li>
    <li>Micikevicius, P. et al. "Mixed Precision Training." <em>ICLR</em> (2018).</li>
    <li>Shoeybi, M. et al. "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism." <em>arXiv:1909.08053</em> (2019).</li>
</ol>

---

<p><strong>æ¬¡ç« </strong>: <a href="chapter5-real-world-applications.html">ç¬¬5ç« ï¼šå®Ÿä¸–ç•Œã®å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³</a></p>

<p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: ã“ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯CC BY 4.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§æä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚</p>

<div class="navigation">
    <a href="chapter3-mapreduce-and-spark.html" class="nav-button">â† å‰ã®ç« </a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter5-real-world-applications.html" class="nav-button">æ¬¡ã®ç«  â†’</a>
</div>
    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
