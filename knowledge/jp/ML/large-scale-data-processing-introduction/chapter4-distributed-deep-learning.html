<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ÂàÜÊï£Ê∑±Â±§Â≠¶Áøí„ÅÆÂü∫Á§é„Å®ÂÆüË∑µ</h1>
            <p class="subtitle">PyTorch DDP„Å®Horovod„Å´„Çà„ÇãÂ§ßË¶èÊ®°„É¢„Éá„É´„ÅÆË®ìÁ∑¥</p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 45-50ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ‰∏≠Á¥öÔΩû‰∏äÁ¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 8ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 5Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Á¨¨4Á´†ÔºöÂàÜÊï£Ê∑±Â±§Â≠¶Áøí„ÅÆÂü∫Á§é„Å®ÂÆüË∑µ</h1>

<div class="learning-objectives">
    <h2>Â≠¶ÁøíÁõÆÊ®ô</h2>
    <ul>
        <li>ÂàÜÊï£Â≠¶Áøí„ÅÆ‰∏ªË¶ÅÊà¶Áï•ÔºàData/Model/Pipeline ParallelismÔºâ„ÇíÁêÜËß£„Åô„Çã</li>
        <li>PyTorch DDP„Å´„Çà„Çã„Éû„É´„ÉÅGPUË®ìÁ∑¥„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
        <li>Horovod„ÅÆAllReduce„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÇíÁêÜËß£„Åó„ÄÅÂÆüË£Ö„Åß„Åç„Çã</li>
        <li>Â§ßË¶èÊ®°„É¢„Éá„É´Ë®ìÁ∑¥„ÅÆ„ÉÜ„ÇØ„Éã„ÉÉ„ÇØÔºàAMP„ÄÅGradient AccumulationÔºâ„ÇíÁøíÂæó„Åô„Çã</li>
        <li>ÂàÜÊï£Â≠¶Áøí„ÅÆ„Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„Çπ„Å®„Éá„Éê„ÉÉ„Ç∞ÊâãÊ≥ï„ÇíÂ≠¶„Å∂</li>
    </ul>
</div>

<p><strong>Ë™≠‰∫ÜÊôÇÈñì</strong>: 45-50ÂàÜ</p>

---

<h2>4.1 ÂàÜÊï£Â≠¶Áøí„ÅÆÊà¶Áï•</h2>

<h3>4.1.1 „Å™„ÅúÂàÜÊï£Â≠¶Áøí„ÅåÂøÖË¶Å„Åã</h3>

<p><strong>Áèæ‰ª£„ÅÆÊ∑±Â±§Â≠¶Áøí„ÅÆË™≤È°å:</strong></p>
<ul>
    <li><strong>„É¢„Éá„É´„Çµ„Ç§„Ç∫„ÅÆÂ¢óÂ§ß</strong>: GPT-3Ôºà175B parametersÔºâ„ÄÅBERT-LargeÔºà340M parametersÔºâ</li>
    <li><strong>„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂ∑®Â§ßÂåñ</strong>: ImageNet-21KÔºà14MÁîªÂÉèÔºâ„ÄÅCommon CrawlÔºàÊï∞ÁôæTBÔºâ</li>
    <li><strong>Ë®ìÁ∑¥ÊôÇÈñì„ÅÆÂïèÈ°å</strong>: Âçò‰∏ÄGPUË®ìÁ∑¥„Åß„ÅØÊï∞ÈÄ±ÈñìÔΩûÊï∞„É∂Êúà</li>
</ul>

<p><strong>ÂàÜÊï£Â≠¶Áøí„Å´„Çà„ÇãËß£Ê±∫:</strong></p>
<ul>
    <li><strong>Ë®ìÁ∑¥ÊôÇÈñì„ÅÆÁü≠Á∏Æ</strong>: 8 GPU‰∏¶Âàó„ÅßÁêÜÊÉ≥ÁöÑ„Å´„ÅØ8ÂÄçÈ´òÈÄüÂåñ</li>
    <li><strong>Â§ßË¶èÊ®°„É¢„Éá„É´„ÅÆÂÆüÁèæ</strong>: „É°„É¢„É™„ÇíË§áÊï∞GPU/„Éé„Éº„Éâ„Å´ÂàÜÊï£</li>
    <li><strong>„Ç≥„Çπ„ÉàÂäπÁéá</strong>: „ÇØ„É©„Ç¶„ÉâÁí∞Â¢É„Åß„ÅÆÂäπÁéáÁöÑ„Å™„É™„ÇΩ„Éº„ÇπÂà©Áî®</li>
</ul>

<h3>4.1.2 Data ParallelismÔºà„Éá„Éº„Çø‰∏¶ÂàóÔºâ</h3>

<p><strong>Âü∫Êú¨ÂéüÁêÜ:</strong></p>
<ul>
    <li>„É¢„Éá„É´„ÅÆÂÆåÂÖ®„Å™„Ç≥„Éî„Éº„ÇíÂêÑGPU„Å´ÈÖçÁΩÆ</li>
    <li>„Éá„Éº„Çø„Éê„ÉÉ„ÉÅ„ÇíÂàÜÂâ≤„Åó„Å¶ÂêÑGPU„Å´ÈÖçÂàÜ</li>
    <li>ÂêÑGPU„ÅßÁã¨Á´ã„Å´È†Ü‰ºùÊí≠„ÉªÈÄÜ‰ºùÊí≠</li>
    <li>ÂãæÈÖç„ÇíÂÖ®GPU„ÅßÈõÜÁ¥ÑÔºàAllReduceÔºâ</li>
    <li>Áµ±Âêà„Åï„Çå„ÅüÂãæÈÖç„Åß„É¢„Éá„É´„ÇíÊõ¥Êñ∞</li>
</ul>

<p><strong>„É°„É™„ÉÉ„Éà:</strong></p>
<ul>
    <li>ÂÆüË£Ö„ÅåÊØîËºÉÁöÑÁ∞°Âçò</li>
    <li>„É¢„Éá„É´„ÅåGPU„É°„É¢„É™„Å´Âèé„Åæ„ÇãÂ†¥Âêà„Å´ÊúâÂäπ</li>
    <li>È´ò„ÅÑ„Çπ„Ç±„Éº„É©„Éì„É™„ÉÜ„Ç£ÔºàÊï∞ÁôæGPU„Åæ„ÅßÔºâ</li>
</ul>

<p><strong>„Éá„É°„É™„ÉÉ„Éà:</strong></p>
<ul>
    <li>ÂêÑGPU„Å´„É¢„Éá„É´ÂÖ®‰Ωì„ÅåÂøÖË¶ÅÔºà„É°„É¢„É™Âà∂Á¥ÑÔºâ</li>
    <li>ÂãæÈÖçÂêåÊúü„ÅÆÈÄö‰ø°„Ç™„Éº„Éê„Éº„Éò„ÉÉ„Éâ</li>
</ul>

<h3>4.1.3 Model ParallelismÔºà„É¢„Éá„É´‰∏¶ÂàóÔºâ</h3>

<p><strong>Âü∫Êú¨ÂéüÁêÜ:</strong></p>
<ul>
    <li>„É¢„Éá„É´„ÇíË§áÊï∞„ÅÆGPU„Å´ÂàÜÂâ≤</li>
    <li>ÂêÑGPU„ÅåÁï∞„Å™„Çã„É¨„Ç§„É§„Éº/„Éë„É©„É°„Éº„Çø„ÇíÊãÖÂΩì</li>
    <li>„Éá„Éº„Çø„ÅØÂÖ®GPU„ÅßÂÖ±Êúâ</li>
</ul>

<p><strong>ÂàÜÂâ≤ÊñπÊ≥ï:</strong></p>
<ul>
    <li><strong>Â±§Âçò‰ΩçÂàÜÂâ≤</strong>: „É¨„Ç§„É§„Éº1-5„ÇíGPU0„ÄÅ6-10„ÇíGPU1</li>
    <li><strong>„ÉÜ„É≥„ÇΩ„É´ÂàÜÂâ≤</strong>: ÂêÑ„É¨„Ç§„É§„Éº„ÅÆÈáç„ÅøË°åÂàó„ÇíÂàÜÂâ≤ÔºàMegatron-LMÔºâ</li>
</ul>

<p><strong>„É°„É™„ÉÉ„Éà:</strong></p>
<ul>
    <li>GPU„É°„É¢„É™„ÇíË∂Ö„Åà„ÇãÂ∑®Â§ß„É¢„Éá„É´„Å´ÂØæÂøú</li>
    <li>ÂãæÈÖçÂêåÊúü‰∏çË¶ÅÔºàÂ±§ÈñìÈÄö‰ø°„ÅÆ„ÅøÔºâ</li>
</ul>

<p><strong>„Éá„É°„É™„ÉÉ„Éà:</strong></p>
<ul>
    <li>GPUÈñì„ÅÆ‰æùÂ≠òÈñ¢‰øÇ„Åß‰∏¶ÂàóÂ∫¶„Åå‰Ωé‰∏ã</li>
    <li>ÂÆüË£Ö„ÅåË§áÈõë</li>
    <li>ÈÄö‰ø°„Éú„Éà„É´„Éç„ÉÉ„ÇØ</li>
</ul>

<h3>4.1.4 Pipeline ParallelismÔºà„Éë„Ç§„Éó„É©„Ç§„É≥‰∏¶ÂàóÔºâ</h3>

<p><strong>Âü∫Êú¨ÂéüÁêÜ:</strong></p>
<ul>
    <li>„É¢„Éá„É´„ÇíË§áÊï∞„Çπ„ÉÜ„Éº„Ç∏„Å´ÂàÜÂâ≤ÔºàÂêÑGPU„ÅåÊãÖÂΩìÔºâ</li>
    <li>„Éá„Éº„Çø„Çí„Éû„Ç§„ÇØ„É≠„Éê„ÉÉ„ÉÅ„Å´ÂàÜÂâ≤</li>
    <li>„Éë„Ç§„Éó„É©„Ç§„É≥ÁöÑ„Å´È†ÜÊ¨°Âá¶ÁêÜ</li>
    <li>GPU„ÅÆ„Ç¢„Ç§„Éâ„É´ÊôÇÈñì„ÇíÂâäÊ∏õ</li>
</ul>

<p><strong>GPipeÊâãÊ≥ï:</strong></p>
<ul>
    <li>„Éû„Ç§„ÇØ„É≠„Éê„ÉÉ„ÉÅÂàÜÂâ≤„Åß„Éë„Ç§„Éó„É©„Ç§„É≥ÂäπÁéá„ÇíÂêë‰∏ä</li>
    <li>ÂãæÈÖçÁ¥ØÁ©çÔºàGradient AccumulationÔºâ„Å®ÁµÑ„ÅøÂêà„Çè„Åõ</li>
    <li>ÂÜçË®àÁÆóÔºàRecomputationÔºâ„Åß„É°„É¢„É™ÂâäÊ∏õ</li>
</ul>

<p><strong>„É°„É™„ÉÉ„Éà:</strong></p>
<ul>
    <li>„É¢„Éá„É´‰∏¶Âàó„Çà„ÇäÈ´ò„ÅÑ‰∏¶ÂàóÂ∫¶</li>
    <li>GPUÂà©Áî®Áéá„ÅÆÂêë‰∏ä</li>
</ul>

<p><strong>„Éá„É°„É™„ÉÉ„Éà:</strong></p>
<ul>
    <li>„Éë„Ç§„Éó„É©„Ç§„É≥„Éê„Éñ„É´Ôºà„Ç¢„Ç§„Éâ„É´ÊôÇÈñìÔºâ</li>
    <li>ÂÆüË£Ö„ÅÆË§áÈõë„Åï</li>
</ul>

<h3>4.1.5 Hybrid ApproachesÔºà„Éè„Ç§„Éñ„É™„ÉÉ„Éâ„Ç¢„Éó„É≠„Éº„ÉÅÔºâ</h3>

<p><strong>3D ParallelismÔºàMegatron-LMÔºâ:</strong></p>
<ul>
    <li><strong>Data Parallelism</strong>: „Éé„Éº„ÉâÈñì</li>
    <li><strong>Model Parallelism</strong>: „Éé„Éº„ÉâÂÜÖGPUÈñìÔºà„ÉÜ„É≥„ÇΩ„É´ÂàÜÂâ≤Ôºâ</li>
    <li><strong>Pipeline Parallelism</strong>: „É¨„Ç§„É§„ÉºÂàÜÂâ≤</li>
</ul>

<p><strong>ZeROÔºàDeepSpeedÔºâ:</strong></p>
<ul>
    <li>„Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂Áä∂ÊÖã„ÅÆÂàÜÂâ≤ÔºàZeRO-1Ôºâ</li>
    <li>ÂãæÈÖç„ÅÆÂàÜÂâ≤ÔºàZeRO-2Ôºâ</li>
    <li>„Éë„É©„É°„Éº„Çø„ÅÆÂàÜÂâ≤ÔºàZeRO-3Ôºâ</li>
    <li>Data Parallelism„ÅÆÂäπÁéá„ÇíÊúÄÂ§ßÂåñ</li>
</ul>

<h3>4.1.6 Êà¶Áï•„ÅÆÊØîËºÉÂõ≥</h3>

<div class="mermaid">
graph TB
    subgraph "Data Parallelism"
        D1[GPU 0<br/>Model Copy<br/>Data Batch 1]
        D2[GPU 1<br/>Model Copy<br/>Data Batch 2]
        D3[GPU 2<br/>Model Copy<br/>Data Batch 3]
        D1 -.AllReduce.-> D2
        D2 -.AllReduce.-> D3
    end

    subgraph "Model Parallelism"
        M1[GPU 0<br/>Layer 1-3]
        M2[GPU 1<br/>Layer 4-6]
        M3[GPU 2<br/>Layer 7-9]
        M1 -->|Forward| M2
        M2 -->|Forward| M3
        M3 -.Backward.-> M2
        M2 -.Backward.-> M1
    end

    subgraph "Pipeline Parallelism"
        P1[GPU 0<br/>Stage 1<br/>Micro-batch 1,2,3]
        P2[GPU 1<br/>Stage 2<br/>Micro-batch 1,2,3]
        P3[GPU 2<br/>Stage 3<br/>Micro-batch 1,2,3]
        P1 ==>|Pipeline| P2
        P2 ==>|Pipeline| P3
    end
</div>

---

<h2>4.2 PyTorch Distributed Data Parallel (DDP)</h2>

<h3>4.2.1 torch.distributed „ÅÆÂü∫Á§é</h3>

<p><strong>‰∏ªË¶ÅÊ¶ÇÂøµ:</strong></p>
<ul>
    <li><strong>Process Group</strong>: ‰∏¶Âàó„Éó„É≠„Çª„Çπ„ÅÆÈõÜÂêà</li>
    <li><strong>Rank</strong>: „Éó„É≠„Çª„Çπ„ÅÆ‰∏ÄÊÑè„Å™IDÔºà0, 1, 2, ...Ôºâ</li>
    <li><strong>World Size</strong>: Á∑è„Éó„É≠„Çª„ÇπÊï∞</li>
    <li><strong>Backend</strong>: ÈÄö‰ø°„É©„Ç§„Éñ„É©„É™ÔºàNCCL, Gloo, MPIÔºâ</li>
</ul>

<p><strong>„Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ„ÅÆÈÅ∏Êäû:</strong></p>
<ul>
    <li><strong>NCCL</strong>: GPUÈñìÈÄö‰ø°„Å´ÊúÄÈÅ©ÔºàÊé®Â•®Ôºâ</li>
    <li><strong>Gloo</strong>: CPU„Å®GPU„ÅÆ‰∏°Êñπ„Å´ÂØæÂøú</li>
    <li><strong>MPI</strong>: HPC„ÇØ„É©„Çπ„Çø„Åß‰ΩøÁî®</li>
</ul>

<h4>„Ç≥„Éº„Éâ‰æã1: Âü∫Êú¨ÁöÑ„Å™DistributedÂàùÊúüÂåñ</h4>

<details>
<summary>distributed_init.py - ÂàÜÊï£Áí∞Â¢É„ÅÆÂàùÊúüÂåñ</summary>

<pre><code>import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp

def setup(rank, world_size):
    """
    ÂàÜÊï£Áí∞Â¢É„ÅÆ„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó

    Args:
        rank: „Éó„É≠„Çª„Çπ„ÅÆ„É©„É≥„ÇØÔºà0„Åã„Çâworld_size-1Ôºâ
        world_size: Á∑è„Éó„É≠„Çª„ÇπÊï∞
    """
    # Áí∞Â¢ÉÂ§âÊï∞„ÅÆË®≠ÂÆö
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'

    # „Éó„É≠„Çª„Çπ„Ç∞„É´„Éº„Éó„ÅÆÂàùÊúüÂåñ
    dist.init_process_group(
        backend='nccl',  # GPUÈñìÈÄö‰ø°„Å´NCCL‰ΩøÁî®
        rank=rank,
        world_size=world_size
    )

    # ÂêÑ„Éó„É≠„Çª„Çπ„ÇíÂØæÂøú„Åô„ÇãGPU„Å´Ââ≤„ÇäÂΩì„Å¶
    torch.cuda.set_device(rank)

    print(f"Process {rank}/{world_size} initialized on GPU {rank}")

def cleanup():
    """ÂàÜÊï£Áí∞Â¢É„ÅÆ„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó"""
    dist.destroy_process_group()

def demo_basic_operations(rank, world_size):
    """
    Âü∫Êú¨ÁöÑ„Å™ÂàÜÊï£Êìç‰Ωú„ÅÆ„Éá„É¢
    """
    setup(rank, world_size)

    # ÂêÑ„Éó„É≠„Çª„Çπ„Åß„ÉÜ„É≥„ÇΩ„É´„Çí‰ΩúÊàê
    tensor = torch.ones(2, 2).cuda(rank) * (rank + 1)
    print(f"Rank {rank} - Original tensor:\n{tensor}")

    # AllReduce: ÂÖ®„Éó„É≠„Çª„Çπ„ÅÆ„ÉÜ„É≥„ÇΩ„É´„ÇíÂêàË®à
    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
    print(f"Rank {rank} - After AllReduce:\n{tensor}")

    # Broadcast: Rank 0„ÅÆ„ÉÜ„É≥„ÇΩ„É´„ÇíÂÖ®„Éó„É≠„Çª„Çπ„Å´ÈÖçÂ∏É
    if rank == 0:
        broadcast_tensor = torch.tensor([100.0, 200.0]).cuda(rank)
    else:
        broadcast_tensor = torch.zeros(2).cuda(rank)

    dist.broadcast(broadcast_tensor, src=0)
    print(f"Rank {rank} - After Broadcast: {broadcast_tensor}")

    cleanup()

if __name__ == "__main__":
    world_size = 4  # 4„Å§„ÅÆGPU„Çí‰ΩøÁî®
    mp.spawn(
        demo_basic_operations,
        args=(world_size,),
        nprocs=world_size,
        join=True
    )
</code></pre>
</details>

<p><strong>ÂÆüË°åÊñπÊ≥ï:</strong></p>
<pre><code># Âçò‰∏Ä„Éé„Éº„Éâ„ÄÅ4 GPU
python distributed_init.py

# Ë§áÊï∞„Éé„Éº„ÉâÔºà„Éé„Éº„Éâ„ÅÇ„Åü„Çä4 GPU„ÄÅ2„Éé„Éº„ÉâÔºâ
# Node 0:
python -m torch.distributed.launch \
    --nproc_per_node=4 \
    --nnodes=2 \
    --node_rank=0 \
    --master_addr="192.168.1.1" \
    --master_port=12355 \
    distributed_init.py

# Node 1:
python -m torch.distributed.launch \
    --nproc_per_node=4 \
    --nnodes=2 \
    --node_rank=1 \
    --master_addr="192.168.1.1" \
    --master_port=12355 \
    distributed_init.py
</code></pre>

<h3>4.2.2 DDPÂÆüË£Ö</h3>

<h4>„Ç≥„Éº„Éâ‰æã2: PyTorch DDP„Å´„Çà„ÇãÁîªÂÉèÂàÜÈ°ûË®ìÁ∑¥</h4>

<details>
<summary>ddp_training.py - ResNet18„ÅÆDDPË®ìÁ∑¥</summary>

<pre><code>import os
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, Dataset
from torch.utils.data.distributed import DistributedSampler
import torchvision
import torchvision.transforms as transforms

def setup(rank, world_size):
    """ÂàÜÊï£Áí∞Â¢É„ÅÆ„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

def cleanup():
    """ÂàÜÊï£Áí∞Â¢É„ÅÆ„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó"""
    dist.destroy_process_group()

def prepare_dataloader(rank, world_size, batch_size=32):
    """
    ÂàÜÊï£„Éá„Éº„Çø„É≠„Éº„ÉÄ„Éº„ÅÆÊ∫ñÂÇô

    DistributedSampler„Çí‰ΩøÁî®„Åó„Å¶ÂêÑ„Éó„É≠„Çª„Çπ„Å´Áï∞„Å™„Çã„Éá„Éº„Çø„ÇíÂâ≤„ÇäÂΩì„Å¶
    """
    # „Éá„Éº„Çø„ÅÆÂâçÂá¶ÁêÜ
    transform = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    # CIFAR-10„Éá„Éº„Çø„Çª„ÉÉ„Éà
    dataset = torchvision.datasets.CIFAR10(
        root='./data',
        train=True,
        download=True,
        transform=transform
    )

    # DistributedSampler: „Éá„Éº„Çø„Çíworld_sizeÂÄã„ÅÆ„ÉÅ„É£„É≥„ÇØ„Å´ÂàÜÂâ≤
    sampler = DistributedSampler(
        dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=True,
        drop_last=False
    )

    # DataLoader
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        sampler=sampler,
        num_workers=4,
        pin_memory=True
    )

    return dataloader, sampler

def train_epoch(model, dataloader, optimizer, criterion, rank, epoch):
    """
    1„Ç®„Éù„ÉÉ„ÇØ„ÅÆË®ìÁ∑¥
    """
    model.train()
    total_loss = 0.0
    correct = 0
    total = 0

    for batch_idx, (data, target) in enumerate(dataloader):
        data, target = data.cuda(rank), target.cuda(rank)

        # È†Ü‰ºùÊí≠
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)

        # ÈÄÜ‰ºùÊí≠ÔºàDDP„ÅåËá™ÂãïÁöÑ„Å´ÂãæÈÖç„ÇíÂêåÊúüÔºâ
        loss.backward()
        optimizer.step()

        # Áµ±Ë®à
        total_loss += loss.item()
        _, predicted = output.max(1)
        total += target.size(0)
        correct += predicted.eq(target).sum().item()

        if rank == 0 and batch_idx % 100 == 0:
            print(f"Epoch {epoch}, Batch {batch_idx}, "
                  f"Loss: {loss.item():.4f}, "
                  f"Acc: {100.*correct/total:.2f}%")

    avg_loss = total_loss / len(dataloader)
    accuracy = 100. * correct / total

    return avg_loss, accuracy

def main(rank, world_size):
    """
    „É°„Ç§„É≥Ë®ìÁ∑¥„É´„Éº„Éó
    """
    print(f"Running DDP on rank {rank}.")
    setup(rank, world_size)

    # „É¢„Éá„É´„ÅÆ‰ΩúÊàê
    model = torchvision.models.resnet18(num_classes=10).cuda(rank)

    # DDP„É©„ÉÉ„Éë„Éº„Åß„É¢„Éá„É´„Çí„É©„ÉÉ„Éó
    model = DDP(model, device_ids=[rank])

    # ÊêçÂ§±Èñ¢Êï∞„Å®„Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂
    criterion = nn.CrossEntropyLoss().cuda(rank)
    optimizer = torch.optim.SGD(
        model.parameters(),
        lr=0.1,
        momentum=0.9,
        weight_decay=5e-4
    )

    # Â≠¶ÁøíÁéá„Çπ„Ç±„Ç∏„É•„Éº„É©
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=200
    )

    # „Éá„Éº„Çø„É≠„Éº„ÉÄ„Éº„ÅÆÊ∫ñÂÇô
    dataloader, sampler = prepare_dataloader(rank, world_size, batch_size=128)

    # Ë®ìÁ∑¥„É´„Éº„Éó
    num_epochs = 100
    for epoch in range(num_epochs):
        # „Ç®„Éù„ÉÉ„ÇØÈñãÂßãÊôÇ„Å´sampler„ÅÆ„Ç∑„Éº„Éâ„ÇíË®≠ÂÆöÔºà„Ç∑„É£„ÉÉ„Éï„É´„ÅÆÂÜçÁèæÊÄßÔºâ
        sampler.set_epoch(epoch)

        # Ë®ìÁ∑¥
        avg_loss, accuracy = train_epoch(
            model, dataloader, optimizer, criterion, rank, epoch
        )

        # Â≠¶ÁøíÁéáÊõ¥Êñ∞
        scheduler.step()

        # Rank 0„ÅÆ„Åø„Åå„É≠„Ç∞„ÇíÂá∫Âäõ
        if rank == 0:
            print(f"Epoch {epoch}/{num_epochs} - "
                  f"Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%")

            # „É¢„Éá„É´‰øùÂ≠òÔºàRank 0„ÅÆ„ÅøÔºâ
            if (epoch + 1) % 10 == 0:
                torch.save(
                    model.module.state_dict(),  # model.module„ÅßÂÖÉ„ÅÆ„É¢„Éá„É´„Å´„Ç¢„ÇØ„Çª„Çπ
                    f'checkpoint_epoch_{epoch+1}.pth'
                )

    cleanup()

if __name__ == "__main__":
    import torch.multiprocessing as mp

    world_size = torch.cuda.device_count()  # Âà©Áî®ÂèØËÉΩ„Å™GPUÊï∞
    print(f"Training with {world_size} GPUs")

    mp.spawn(main, args=(world_size,), nprocs=world_size, join=True)
</code></pre>
</details>

<p><strong>DDP„ÅÆÈáçË¶Å„Éù„Ç§„É≥„Éà:</strong></p>
<ul>
    <li><strong>DistributedSampler</strong>: ÂêÑ„Éó„É≠„Çª„Çπ„Å´Áï∞„Å™„Çã„Éá„Éº„Çø„ÇíÂâ≤„ÇäÂΩì„Å¶</li>
    <li><strong>sampler.set_epoch()</strong>: ÂêÑ„Ç®„Éù„ÉÉ„ÇØ„Åß„Ç∑„É£„ÉÉ„Éï„É´„ÇíÂ§â„Åà„Çã</li>
    <li><strong>model.module</strong>: DDP„É©„ÉÉ„Éë„Éº„ÅÆÂÖÉ„ÅÆ„É¢„Éá„É´„Å´„Ç¢„ÇØ„Çª„Çπ</li>
    <li><strong>Rank 0„ÅÆ„Åø‰øùÂ≠ò</strong>: „É¢„Éá„É´‰øùÂ≠ò„ÅØ1„Å§„ÅÆ„Éó„É≠„Çª„Çπ„ÅÆ„Åø„ÅßÂÆüË°å</li>
</ul>

<h3>4.2.3 „Éû„É´„ÉÅ„Éé„Éº„ÉâGPUË®ìÁ∑¥</h3>

<h4>„Ç≥„Éº„Éâ‰æã3: Slurm„Å´„Çà„Çã„Éû„É´„ÉÅ„Éé„Éº„ÉâDDP</h4>

<details>
<summary>slurm_ddp.sh - Slurm„Çπ„ÇØ„É™„Éó„Éà</summary>

<pre><code>#!/bin/bash
#SBATCH --job-name=ddp_training
#SBATCH --nodes=4                    # 4„Éé„Éº„Éâ
#SBATCH --ntasks-per-node=4          # „Éé„Éº„Éâ„ÅÇ„Åü„Çä4„Éó„É≠„Çª„ÇπÔºà4 GPUÔºâ
#SBATCH --cpus-per-task=8            # „Éó„É≠„Çª„Çπ„ÅÇ„Åü„Çä8 CPU
#SBATCH --gres=gpu:4                 # „Éé„Éº„Éâ„ÅÇ„Åü„Çä4 GPU
#SBATCH --time=24:00:00
#SBATCH --output=logs/ddp_%j.out
#SBATCH --error=logs/ddp_%j.err

# „É¢„Ç∏„É•„Éº„É´„ÅÆ„É≠„Éº„Éâ
module load cuda/11.8
module load anaconda3

# Áí∞Â¢ÉÂ§âÊï∞„ÅÆË®≠ÂÆö
export MASTER_PORT=12340
export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n 1)
export WORLD_SIZE=$SLURM_NTASKS
export NCCL_DEBUG=INFO

# ÂêÑ„Éé„Éº„Éâ„ÅßË®ìÁ∑¥„ÇíÂÆüË°å
srun python -u ddp_training_multi_node.py \
    --epochs 100 \
    --batch-size 128 \
    --lr 0.1
</code></pre>
</details>

<details>
<summary>ddp_training_multi_node.py - „Éû„É´„ÉÅ„Éé„Éº„ÉâÂØæÂøúÁâà</summary>

<pre><code>import os
import argparse
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup():
    """
    SlurmÁí∞Â¢ÉÂ§âÊï∞„Åã„ÇâÂàÜÊï£Ë®≠ÂÆö„ÇíË™≠„ÅøËæº„Åø
    """
    # Slurm„ÅåË®≠ÂÆö„Åô„ÇãÁí∞Â¢ÉÂ§âÊï∞
    rank = int(os.environ['SLURM_PROCID'])
    world_size = int(os.environ['SLURM_NTASKS'])
    local_rank = int(os.environ['SLURM_LOCALID'])

    # „Éû„Çπ„Çø„Éº„Ç¢„Éâ„É¨„Çπ„Å®„Éù„Éº„Éà
    master_addr = os.environ['MASTER_ADDR']
    master_port = os.environ['MASTER_PORT']

    # Áí∞Â¢ÉÂ§âÊï∞Ë®≠ÂÆö
    os.environ['MASTER_ADDR'] = master_addr
    os.environ['MASTER_PORT'] = master_port

    # ÂàùÊúüÂåñ
    dist.init_process_group(
        backend='nccl',
        init_method='env://',
        world_size=world_size,
        rank=rank
    )

    # „É≠„Éº„Ç´„É´GPUË®≠ÂÆö
    torch.cuda.set_device(local_rank)

    return rank, world_size, local_rank

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--batch-size', type=int, default=128)
    parser.add_argument('--lr', type=float, default=0.1)
    args = parser.parse_args()

    # ÂàÜÊï£Áí∞Â¢É„ÅÆ„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó
    rank, world_size, local_rank = setup()

    if rank == 0:
        print(f"Training with {world_size} processes across "
              f"{world_size // torch.cuda.device_count()} nodes")

    # „É¢„Éá„É´„ÄÅ„Éá„Éº„Çø„É≠„Éº„ÉÄ„Éº„ÄÅË®ìÁ∑¥„É´„Éº„Éó„ÅØÂâçËø∞„Å®ÂêåÊßò
    # ...

    dist.destroy_process_group()

if __name__ == "__main__":
    main()
</code></pre>
</details>

---

<h2>4.3 Horovod</h2>

<h3>4.3.1 AllReduce„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£</h3>

<p><strong>Horovod„Å®„ÅØ:</strong></p>
<ul>
    <li>UberÈñãÁô∫„ÅÆ„Ç™„Éº„Éó„É≥„ÇΩ„Éº„ÇπÂàÜÊï£Ë®ìÁ∑¥„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ</li>
    <li>TensorFlow„ÄÅPyTorch„ÄÅKeras„ÄÅMXNet„Å´ÂØæÂøú</li>
    <li>MPI„Éô„Éº„Çπ„ÅÆÂäπÁéáÁöÑ„Å™AllReduceÈÄö‰ø°</li>
</ul>

<p><strong>AllReduce„ÅÆ‰ªïÁµÑ„Åø:</strong></p>
<ul>
    <li><strong>Ring-AllReduce</strong>: „Éá„Éº„Çø„Çí„É™„É≥„Ç∞Áä∂„Å´ÈÄö‰ø°</li>
    <li><strong>ÈÄö‰ø°Èáè</strong>: O(N)ÔºàN„ÅØÂãæÈÖç„Çµ„Ç§„Ç∫Ôºâ„ÄÅ„Éó„É≠„Çª„ÇπÊï∞„Å´‰æùÂ≠ò„Åó„Å™„ÅÑ</li>
    <li><strong>Â∏ØÂüüÂπÖÂäπÁéá</strong>: ÂÖ®Â∏ØÂüü„ÇíÊ¥ªÁî®</li>
</ul>

<h4>Ring-AllReduce„ÅÆÂãï‰Ωú</h4>

<div class="mermaid">
sequenceDiagram
    participant GPU0
    participant GPU1
    participant GPU2
    participant GPU3

    Note over GPU0,GPU3: Step 1: Scatter-Reduce
    GPU0->>GPU1: Send chunk A
    GPU1->>GPU2: Send chunk B
    GPU2->>GPU3: Send chunk C
    GPU3->>GPU0: Send chunk D

    Note over GPU0,GPU3: Step 2: AllGather
    GPU0->>GPU1: Send reduced A
    GPU1->>GPU2: Send reduced B
    GPU2->>GPU3: Send reduced C
    GPU3->>GPU0: Send reduced D

    Note over GPU0,GPU3: All GPUs have complete reduced gradients
</div>

<h3>4.3.2 Horovod API</h3>

<h4>„Ç≥„Éº„Éâ‰æã4: Horovod„Å´„Çà„ÇãPyTorchË®ìÁ∑¥</h4>

<details>
<summary>horovod_training.py - ResNet18„ÅÆHorovodË®ìÁ∑¥</summary>

<pre><code>import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import horovod.torch as hvd

def train_horovod():
    """
    Horovod„Çí‰Ωø„Å£„ÅüÂàÜÊï£Ë®ìÁ∑¥
    """
    # Horovod„ÅÆÂàùÊúüÂåñ
    hvd.init()

    # ÂêÑ„Éó„É≠„Çª„Çπ„ÇíÂØæÂøú„Åô„ÇãGPU„Å´Ââ≤„ÇäÂΩì„Å¶
    torch.cuda.set_device(hvd.local_rank())
    device = torch.device('cuda')

    # „Éá„Éº„Çø„É≠„Éº„ÉÄ„Éº„ÅÆÊ∫ñÂÇô
    transform = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    dataset = torchvision.datasets.CIFAR10(
        root='./data',
        train=True,
        download=True,
        transform=transform
    )

    # HorovodÁî®„Çµ„É≥„Éó„É©„Éº
    train_sampler = torch.utils.data.distributed.DistributedSampler(
        dataset,
        num_replicas=hvd.size(),
        rank=hvd.rank()
    )

    train_loader = DataLoader(
        dataset,
        batch_size=128,
        sampler=train_sampler,
        num_workers=4,
        pin_memory=True
    )

    # „É¢„Éá„É´„ÅÆ‰ΩúÊàê
    model = torchvision.models.resnet18(num_classes=10).to(device)

    # „Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂
    optimizer = torch.optim.SGD(
        model.parameters(),
        lr=0.1 * hvd.size(),  # Â≠¶ÁøíÁéá„Çí„ÉØ„Éº„Ç´„ÉºÊï∞„Åß„Çπ„Ç±„Éº„É´
        momentum=0.9,
        weight_decay=5e-4
    )

    # Horovod„Åß„Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂„Çí„É©„ÉÉ„Éó
    optimizer = hvd.DistributedOptimizer(
        optimizer,
        named_parameters=model.named_parameters(),
        compression=hvd.Compression.fp16,  # FP16ÂúßÁ∏Æ„ÅßÈÄö‰ø°ÈáèÂâäÊ∏õ
        op=hvd.Average  # ÂãæÈÖç„ÅÆÂπ≥Âùá„ÇíÂèñ„Çã
    )

    # ÂàùÊúü„Éë„É©„É°„Éº„Çø„Çí„Éñ„É≠„Éº„Éâ„Ç≠„É£„Çπ„ÉàÔºàÂÖ®„ÉØ„Éº„Ç´„Éº„ÅßÂêå„ÅòÂàùÊúüÂÄ§Ôºâ
    hvd.broadcast_parameters(model.state_dict(), root_rank=0)
    hvd.broadcast_optimizer_state(optimizer, root_rank=0)

    # ÊêçÂ§±Èñ¢Êï∞
    criterion = nn.CrossEntropyLoss()

    # Ë®ìÁ∑¥„É´„Éº„Éó
    num_epochs = 100
    for epoch in range(num_epochs):
        model.train()
        train_sampler.set_epoch(epoch)

        epoch_loss = 0.0
        correct = 0
        total = 0

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()

            # Horovod„ÅåËá™ÂãïÁöÑ„Å´ÂãæÈÖç„ÇíAllReduce
            optimizer.step()

            # Áµ±Ë®à
            epoch_loss += loss.item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()

        # ÂÖ®„ÉØ„Éº„Ç´„Éº„ÅßÁµ±Ë®à„ÇíÈõÜÁ¥Ñ
        epoch_loss = metric_average(epoch_loss, 'avg_loss')
        accuracy = metric_average(correct / total, 'avg_accuracy')

        # Rank 0„ÅÆ„Åø„É≠„Ç∞Âá∫Âäõ
        if hvd.rank() == 0:
            print(f"Epoch {epoch}/{num_epochs} - "
                  f"Loss: {epoch_loss:.4f}, Accuracy: {accuracy*100:.2f}%")

            # „É¢„Éá„É´‰øùÂ≠ò
            if (epoch + 1) % 10 == 0:
                torch.save(model.state_dict(),
                          f'horovod_checkpoint_epoch_{epoch+1}.pth')

def metric_average(val, name):
    """
    ÂÖ®„ÉØ„Éº„Ç´„Éº„Åß„É°„Éà„É™„ÇØ„Çπ„ÇíÂπ≥Âùá
    """
    tensor = torch.tensor(val)
    avg_tensor = hvd.allreduce(tensor, name=name)
    return avg_tensor.item()

if __name__ == "__main__":
    train_horovod()
</code></pre>
</details>

<p><strong>ÂÆüË°åÊñπÊ≥ï:</strong></p>
<pre><code># Âçò‰∏Ä„Éé„Éº„Éâ„ÄÅ4 GPU
horovodrun -np 4 python horovod_training.py

# Ë§áÊï∞„Éé„Éº„ÉâÔºàÂêÑ„Éé„Éº„Éâ4 GPU„ÄÅ2„Éé„Éº„ÉâÔºâ
horovodrun -np 8 -H node1:4,node2:4 python horovod_training.py

# Slurm„ÇØ„É©„Çπ„Çø
srun --ntasks=8 --gres=gpu:4 python horovod_training.py
</code></pre>

<h3>4.3.3 TensorFlow/PyTorchÁµ±Âêà</h3>

<h4>„Ç≥„Éº„Éâ‰æã5: Horovod„Å´„Çà„ÇãTensorFlowË®ìÁ∑¥</h4>

<details>
<summary>horovod_tensorflow.py - TensorFlow„Åß„ÅÆHorovod‰ΩøÁî®</summary>

<pre><code>import tensorflow as tf
import horovod.tensorflow as hvd

def train_tensorflow_horovod():
    """
    Horovod + TensorFlow„Åß„ÅÆÂàÜÊï£Ë®ìÁ∑¥
    """
    # HorovodÂàùÊúüÂåñ
    hvd.init()

    # GPU„É°„É¢„É™ÊàêÈï∑„ÇíÊúâÂäπÂåñ
    gpus = tf.config.experimental.list_physical_devices('GPU')
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    if gpus:
        tf.config.experimental.set_visible_devices(
            gpus[hvd.local_rank()], 'GPU'
        )

    # „Éá„Éº„Çø„Çª„ÉÉ„Éà
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
    x_train = x_train.astype('float32') / 255.0
    x_test = x_test.astype('float32') / 255.0

    # „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ„Ç∑„É£„Éº„Éá„Ç£„É≥„Ç∞ÔºàÂêÑ„ÉØ„Éº„Ç´„Éº„Å´Áï∞„Å™„Çã„Éá„Éº„ÇøÔºâ
    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
    dataset = dataset.shard(num_shards=hvd.size(), index=hvd.rank())
    dataset = dataset.shuffle(10000).batch(128).prefetch(tf.data.AUTOTUNE)

    # „É¢„Éá„É´
    model = tf.keras.applications.ResNet50(
        include_top=True,
        weights=None,
        classes=10,
        input_shape=(32, 32, 3)
    )

    # „Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂
    optimizer = tf.keras.optimizers.SGD(
        learning_rate=0.1 * hvd.size(),
        momentum=0.9
    )

    # Horovod„Åß„Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂„Çí„É©„ÉÉ„Éó
    optimizer = hvd.DistributedOptimizer(
        optimizer,
        compression=hvd.Compression.fp16
    )

    # ÊêçÂ§±Èñ¢Êï∞„Å®„É°„Éà„É™„ÇØ„Çπ
    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    @tf.function
    def training_step(images, labels, first_batch):
        with tf.GradientTape() as tape:
            predictions = model(images, training=True)
            loss = loss_fn(labels, predictions)

        # Horovod„ÅåÂãæÈÖç„ÇíAllReduce
        tape = hvd.DistributedGradientTape(tape, compression=hvd.Compression.fp16)
        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))

        # ÂàùÂõû„Éê„ÉÉ„ÉÅ„Åß„Éë„É©„É°„Éº„Çø„Çí„Éñ„É≠„Éº„Éâ„Ç≠„É£„Çπ„Éà
        if first_batch:
            hvd.broadcast_variables(model.variables, root_rank=0)
            hvd.broadcast_variables(optimizer.variables(), root_rank=0)

        return loss

    # Ë®ìÁ∑¥„É´„Éº„Éó
    for epoch in range(100):
        epoch_loss = 0.0
        for batch_idx, (images, labels) in enumerate(dataset):
            loss = training_step(images, labels, batch_idx == 0 and epoch == 0)
            epoch_loss += loss.numpy()

        # Âπ≥ÂùáÊêçÂ§±„ÇíË®àÁÆó
        epoch_loss = hvd.allreduce(
            tf.constant(epoch_loss / len(dataset)),
            average=True
        ).numpy()

        if hvd.rank() == 0:
            print(f"Epoch {epoch}, Loss: {epoch_loss:.4f}")

            # „É¢„Éá„É´‰øùÂ≠ò
            if (epoch + 1) % 10 == 0:
                model.save(f'tf_horovod_model_epoch_{epoch+1}.h5')

if __name__ == "__main__":
    train_tensorflow_horovod()
</code></pre>
</details>

<h3>4.3.4 ÊÄßËÉΩÊØîËºÉ: PyTorch DDP vs Horovod</h3>

<table>
    <thead>
        <tr>
            <th>È†ÖÁõÆ</th>
            <th>PyTorch DDP</th>
            <th>Horovod</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><strong>ÈÄö‰ø°„Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ</strong></td>
            <td>NCCL, Gloo, MPI</td>
            <td>MPI, NCCL</td>
        </tr>
        <tr>
            <td><strong>„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØÂØæÂøú</strong></td>
            <td>PyTorchÂ∞ÇÁî®</td>
            <td>TensorFlow, PyTorch, Keras, MXNet</td>
        </tr>
        <tr>
            <td><strong>ÂÆüË£Ö„ÅÆË§áÈõë„Åï</strong></td>
            <td>‰∏≠Á®ãÂ∫¶</td>
            <td>„Ç∑„É≥„Éó„É´</td>
        </tr>
        <tr>
            <td><strong>ÈÄö‰ø°ÂäπÁéá</strong></td>
            <td>È´ò„ÅÑÔºàNCCLÊúÄÈÅ©ÂåñÔºâ</td>
            <td>È´ò„ÅÑÔºàRing-AllReduceÔºâ</td>
        </tr>
        <tr>
            <td><strong>„Çπ„Ç±„Éº„É©„Éì„É™„ÉÜ„Ç£</strong></td>
            <td>Êï∞ÁôæGPU</td>
            <td>Êï∞ÂçÉGPUÔºàMPI„Éô„Éº„ÇπÔºâ</td>
        </tr>
        <tr>
            <td><strong>ÂãæÈÖçÂúßÁ∏Æ</strong></td>
            <td>ÊâãÂãïÂÆüË£Ö</td>
            <td>Ê®ôÊ∫ñ„Çµ„Éù„Éº„ÉàÔºàFP16Ôºâ</td>
        </tr>
        <tr>
            <td><strong>ÂãïÁöÑ„Ç∞„É©„ÉïÂØæÂøú</strong></td>
            <td>ÂÆåÂÖ®ÂØæÂøú</td>
            <td>ÂÆåÂÖ®ÂØæÂøú</td>
        </tr>
        <tr>
            <td><strong>„Ç®„Ç≥„Ç∑„Çπ„ÉÜ„É†</strong></td>
            <td>PyTorchÂÖ¨Âºè</td>
            <td>Áã¨Á´ã„Éó„É≠„Ç∏„Çß„ÇØ„Éà</td>
        </tr>
    </tbody>
</table>

<p><strong>„Éô„É≥„ÉÅ„Éû„Éº„ÇØÁµêÊûúÔºàResNet-50„ÄÅImageNet„ÄÅ8 GPUÔºâ:</strong></p>
<ul>
    <li><strong>PyTorch DDP</strong>: 2,400 images/secÔºà„Çπ„Ç±„Éº„É™„É≥„Ç∞ÂäπÁéá 92%Ôºâ</li>
    <li><strong>Horovod</strong>: 2,350 images/secÔºà„Çπ„Ç±„Éº„É™„É≥„Ç∞ÂäπÁéá 90%Ôºâ</li>
</ul>

<p><strong>Êé®Â•®‰∫ãÈ†Ö:</strong></p>
<ul>
    <li><strong>PyTorch„ÅÆ„Åø‰ΩøÁî®</strong> ‚Üí PyTorch DDP</li>
    <li><strong>Ë§áÊï∞„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ</strong> ‚Üí Horovod</li>
    <li><strong>Â§ßË¶èÊ®°„ÇØ„É©„Çπ„ÇøÔºà100+ GPUÔºâ</strong> ‚Üí HorovodÔºàMPI„ÅÆÂÆâÂÆöÊÄßÔºâ</li>
</ul>

---

<h2>4.4 Â§ßË¶èÊ®°„É¢„Éá„É´„ÅÆË®ìÁ∑¥„ÉÜ„ÇØ„Éã„ÉÉ„ÇØ</h2>

<h3>4.4.1 Gradient AccumulationÔºàÂãæÈÖçÁ¥ØÁ©çÔºâ</h3>

<p><strong>ÁõÆÁöÑ:</strong></p>
<ul>
    <li>GPU„É°„É¢„É™Âà∂Á¥Ñ‰∏ã„ÅßÂ§ß„Åç„Å™„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„ÇíÂÆüÁèæ</li>
    <li>Â∞è„Éê„ÉÉ„ÉÅ„ÇíË§áÊï∞ÂõûÂÆüË°å„Åó„ÄÅÂãæÈÖç„ÇíÁ¥ØÁ©ç</li>
</ul>

<p><strong>Êï∞Âºè:</strong></p>
$$
\nabla_\theta L_{\text{effective}} = \frac{1}{K} \sum_{k=1}^{K} \nabla_\theta L(\text{mini-batch}_k)
$$

<p>$K$: Á¥ØÁ©ç„Çπ„ÉÜ„ÉÉ„ÉóÊï∞„ÄÅÂÆüÂäπ„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫ = $K \times$ mini-batch size</p>

<h4>„Ç≥„Éº„Éâ‰æã6: Gradient Accumulation„ÅÆÂÆüË£Ö</h4>

<details>
<summary>gradient_accumulation.py - ÂãæÈÖçÁ¥ØÁ©ç</summary>

<pre><code>import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision

def train_with_gradient_accumulation(
    model, dataloader, optimizer, criterion,
    accumulation_steps=4, device='cuda'
):
    """
    ÂãæÈÖçÁ¥ØÁ©ç„Çí‰Ωø„Å£„ÅüË®ìÁ∑¥

    Args:
        accumulation_steps: ÂãæÈÖç„ÇíÁ¥ØÁ©ç„Åô„Çã„Çπ„ÉÜ„ÉÉ„ÉóÊï∞
    """
    model.train()
    optimizer.zero_grad()

    for batch_idx, (data, target) in enumerate(dataloader):
        data, target = data.to(device), target.to(device)

        # È†Ü‰ºùÊí≠
        output = model(data)
        loss = criterion(output, target)

        # ÊêçÂ§±„ÇíÁ¥ØÁ©ç„Çπ„ÉÜ„ÉÉ„ÉóÊï∞„ÅßÂâ≤„Çã
        loss = loss / accumulation_steps

        # ÈÄÜ‰ºùÊí≠ÔºàÂãæÈÖç„ÇíÁ¥ØÁ©çÔºâ
        loss.backward()

        # accumulation_steps„Åî„Å®„Å´„Éë„É©„É°„Éº„ÇøÊõ¥Êñ∞
        if (batch_idx + 1) % accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()

            print(f"Batch {batch_idx+1}, Updated parameters")

    # ÊúÄÂæå„ÅÆ‰Ωô„Çä„ÅÆ„Éê„ÉÉ„ÉÅ„ÇíÂá¶ÁêÜ
    if (batch_idx + 1) % accumulation_steps != 0:
        optimizer.step()
        optimizer.zero_grad()

# ‰ΩøÁî®‰æã
model = torchvision.models.resnet50().cuda()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
criterion = nn.CrossEntropyLoss()

# Â∞è„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫Ôºà16Ôºâ√ó Á¥ØÁ©ç„Çπ„ÉÜ„ÉÉ„ÉóÔºà4Ôºâ= ÂÆüÂäπ„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫Ôºà64Ôºâ
train_loader = DataLoader(dataset, batch_size=16, shuffle=True)

train_with_gradient_accumulation(
    model, train_loader, optimizer, criterion,
    accumulation_steps=4
)
</code></pre>
</details>

<p><strong>„É°„É™„ÉÉ„Éà:</strong></p>
<ul>
    <li>GPU„É°„É¢„É™Âà∂Á¥Ñ„ÇíÂõûÈÅø</li>
    <li>Â§ß„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„ÅÆË®ìÁ∑¥„ÇíÂèØËÉΩ„Å´</li>
</ul>

<p><strong>„Éá„É°„É™„ÉÉ„Éà:</strong></p>
<ul>
    <li>Ë®ìÁ∑¥ÈÄüÂ∫¶„ÅÆ‰Ωé‰∏ãÔºàÈÄö‰ø°„Ç™„Éº„Éê„Éº„Éò„ÉÉ„ÉâÂâäÊ∏õ„ÅÆÂäπÊûú„ÅØÂ∞èÔºâ</li>
    <li>Batch Normalization„ÅÆÊåôÂãï„Å´Ê≥®ÊÑèÔºàÂ∞è„Éê„ÉÉ„ÉÅ„Åß„ÅÆÁµ±Ë®àÔºâ</li>
</ul>

<h3>4.4.2 Mixed Precision Training (AMP)</h3>

<p><strong>Ê¶ÇË¶Å:</strong></p>
<ul>
    <li>FP16ÔºàÂçäÁ≤æÂ∫¶ÊµÆÂãïÂ∞èÊï∞ÁÇπÔºâ„Å®FP32„ÇíÊ∑∑Âú®„Åï„Åõ„Å¶Ë®ìÁ∑¥</li>
    <li>Ë®àÁÆóÈ´òÈÄüÂåñ„Å®„É°„É¢„É™ÂâäÊ∏õ</li>
    <li>ÊêçÂ§±„Çπ„Ç±„Éº„É™„É≥„Ç∞„ÅßÊï∞ÂÄ§ÂÆâÂÆöÊÄß„ÇíÁ¢∫‰øù</li>
</ul>

<p><strong>ÂäπÊûú:</strong></p>
<ul>
    <li><strong>È´òÈÄüÂåñ</strong>: 1.5ÔΩû2ÂÄçÔºàTensor CoreÊ¥ªÁî®Ôºâ</li>
    <li><strong>„É°„É¢„É™ÂâäÊ∏õ</strong>: Á¥Ñ50%</li>
</ul>

<h4>„Ç≥„Éº„Éâ‰æã7: PyTorch AMP„ÅÆ‰ΩøÁî®</h4>

<details>
<summary>amp_training.py - Automatic Mixed Precision</summary>

<pre><code>import torch
import torch.nn as nn
from torch.cuda.amp import autocast, GradScaler
import torchvision

def train_with_amp(model, dataloader, optimizer, criterion, device='cuda'):
    """
    Automatic Mixed Precision (AMP) „Çí‰Ωø„Å£„ÅüË®ìÁ∑¥
    """
    model.train()

    # GradScaler: FP16„ÅÆÂãæÈÖç„ÇíÈÅ©Âàá„Å´„Çπ„Ç±„Éº„É´
    scaler = GradScaler()

    for batch_idx, (data, target) in enumerate(dataloader):
        data, target = data.to(device), target.to(device)

        optimizer.zero_grad()

        # autocast: Ëá™ÂãïÁöÑ„Å´ÊúÄÈÅ©„Å™Á≤æÂ∫¶„ÇíÈÅ∏Êäû
        with autocast():
            output = model(data)
            loss = criterion(output, target)

        # „Çπ„Ç±„Éº„É´„Åó„ÅüÊêçÂ§±„ÅßÈÄÜ‰ºùÊí≠
        scaler.scale(loss).backward()

        # ÂãæÈÖç„Çí„Ç¢„É≥„Çπ„Ç±„Éº„É´„Åó„Å¶„Éë„É©„É°„Éº„ÇøÊõ¥Êñ∞
        scaler.step(optimizer)
        scaler.update()

        if batch_idx % 100 == 0:
            print(f"Batch {batch_idx}, Loss: {loss.item():.4f}")

# ‰ΩøÁî®‰æã
model = torchvision.models.resnet50().cuda()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
criterion = nn.CrossEntropyLoss()

train_loader = torch.utils.data.DataLoader(dataset, batch_size=128)

train_with_amp(model, train_loader, optimizer, criterion)
</code></pre>
</details>

<p><strong>AMP + Gradient Accumulation + DDP</strong>„ÅÆÁµÑ„ÅøÂêà„Çè„Åõ:</p>

<details>
<summary>amp_grad_accum_ddp.py - ÂÆåÂÖ®„Å™ÊúÄÈÅ©Âåñ</summary>

<pre><code>import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler

def train_optimized(
    rank, world_size, model, dataset,
    batch_size=32, accumulation_steps=4, epochs=100
):
    """
    AMP + Gradient Accumulation + DDP „ÅÆÂÆåÂÖ®„Å™ÂÆüË£Ö
    """
    # ÂàÜÊï£Áí∞Â¢É„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

    # „É¢„Éá„É´„ÇíDDP„Åß„É©„ÉÉ„Éó
    model = model.cuda(rank)
    model = DDP(model, device_ids=[rank])

    # „Éá„Éº„Çø„É≠„Éº„ÉÄ„Éº
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)
    dataloader = DataLoader(
        dataset, batch_size=batch_size, sampler=sampler, num_workers=4
    )

    # „Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂„Å®ÊêçÂ§±Èñ¢Êï∞
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss()
    scaler = GradScaler()

    for epoch in range(epochs):
        sampler.set_epoch(epoch)
        model.train()
        optimizer.zero_grad()

        for batch_idx, (data, target) in enumerate(dataloader):
            data, target = data.cuda(rank), target.cuda(rank)

            # AMP„Å´„Çà„ÇãÈ†Ü‰ºùÊí≠
            with autocast():
                output = model(data)
                loss = criterion(output, target) / accumulation_steps

            # ÂãæÈÖçÁ¥ØÁ©ç
            scaler.scale(loss).backward()

            # Á¥ØÁ©ç„Çπ„ÉÜ„ÉÉ„Éó„Åî„Å®„Å´„Éë„É©„É°„Éº„ÇøÊõ¥Êñ∞
            if (batch_idx + 1) % accumulation_steps == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()

        # ÊúÄÂæå„ÅÆ‰Ωô„Çä„ÇíÂá¶ÁêÜ
        if (batch_idx + 1) % accumulation_steps != 0:
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

        if rank == 0:
            print(f"Epoch {epoch} completed")

    dist.destroy_process_group()

# ÂÆüË°å
if __name__ == "__main__":
    import torch.multiprocessing as mp

    world_size = torch.cuda.device_count()
    model = torchvision.models.resnet50()

    mp.spawn(
        train_optimized,
        args=(world_size, model, dataset),
        nprocs=world_size,
        join=True
    )
</code></pre>
</details>

<h3>4.4.3 Gradient CheckpointingÔºàÂãæÈÖç„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„ÉàÔºâ</h3>

<p><strong>Ê¶ÇË¶Å:</strong></p>
<ul>
    <li>È†Ü‰ºùÊí≠ÊôÇ„Å´‰∏≠ÈñìÊ¥ªÊÄßÂåñ„Çí‰øùÂ≠ò„Åõ„Åö„ÄÅÈÄÜ‰ºùÊí≠ÊôÇ„Å´ÂÜçË®àÁÆó</li>
    <li>„É°„É¢„É™‰ΩøÁî®Èáè„ÇíÂ§ßÂπÖÂâäÊ∏õÔºàË®àÁÆóÊôÇÈñì„Å®„ÅÆ„Éà„É¨„Éº„Éâ„Ç™„ÉïÔºâ</li>
</ul>

<p><strong>ÂäπÊûú:</strong></p>
<ul>
    <li><strong>„É°„É¢„É™ÂâäÊ∏õ</strong>: O(n) ‚Üí O(‚àön)Ôºàn„ÅØ„É¨„Ç§„É§„ÉºÊï∞Ôºâ</li>
    <li><strong>Ë®àÁÆóÂ¢óÂä†</strong>: Á¥Ñ20-30%</li>
</ul>

<pre><code>from torch.utils.checkpoint import checkpoint

class CheckpointedResNet(nn.Module):
    def __init__(self, original_model):
        super().__init__()
        self.layer1 = original_model.layer1
        self.layer2 = original_model.layer2
        self.layer3 = original_model.layer3
        self.layer4 = original_model.layer4
        self.fc = original_model.fc

    def forward(self, x):
        # ÂêÑ„É¨„Ç§„É§„Éº„Åßcheckpoint‰ΩøÁî®
        x = checkpoint(self.layer1, x)
        x = checkpoint(self.layer2, x)
        x = checkpoint(self.layer3, x)
        x = checkpoint(self.layer4, x)
        x = self.fc(x)
        return x
</code></pre>

<h3>4.4.4 DeepSpeed ZeRO</h3>

<p><strong>ZeROÔºàZero Redundancy OptimizerÔºâ:</strong></p>
<ul>
    <li>MicrosoftÈñãÁô∫„ÅÆË∂ÖÂ§ßË¶èÊ®°„É¢„Éá„É´Ë®ìÁ∑¥„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ</li>
    <li>„Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂Áä∂ÊÖã„ÄÅÂãæÈÖç„ÄÅ„Éë„É©„É°„Éº„Çø„ÇíÂàÜÊï£</li>
</ul>

<p><strong>ZeRO„ÅÆ3ÊÆµÈöé:</strong></p>
<ul>
    <li><strong>ZeRO-1</strong>: „Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂Áä∂ÊÖã„ÅÆÂàÜÂâ≤Ôºà4ÂÄç„É°„É¢„É™ÂâäÊ∏õÔºâ</li>
    <li><strong>ZeRO-2</strong>: + ÂãæÈÖç„ÅÆÂàÜÂâ≤Ôºà8ÂÄç„É°„É¢„É™ÂâäÊ∏õÔºâ</li>
    <li><strong>ZeRO-3</strong>: + „Éë„É©„É°„Éº„Çø„ÅÆÂàÜÂâ≤ÔºàN„ÉØ„Éº„Ç´„Éº„ÅßNÂÄçÂâäÊ∏õÔºâ</li>
</ul>

<h4>„Ç≥„Éº„Éâ‰æã8: DeepSpeed ZeRO„ÅÆ‰ΩøÁî®</h4>

<details>
<summary>deepspeed_zero.py - DeepSpeedË®ìÁ∑¥</summary>

<pre><code>import torch
import torch.nn as nn
import deepspeed
from transformers import GPT2LMHeadModel, GPT2Tokenizer

def train_with_deepspeed():
    """
    DeepSpeed ZeRO„Çí‰Ωø„Å£„ÅüÂ§ßË¶èÊ®°„É¢„Éá„É´Ë®ìÁ∑¥
    """
    # DeepSpeedË®≠ÂÆö„Éï„Ç°„Ç§„É´
    ds_config = {
        "train_batch_size": 64,
        "gradient_accumulation_steps": 4,
        "optimizer": {
            "type": "Adam",
            "params": {
                "lr": 1e-5,
                "betas": [0.9, 0.999],
                "eps": 1e-8,
                "weight_decay": 0.01
            }
        },
        "fp16": {
            "enabled": True,
            "loss_scale": 0,
            "loss_scale_window": 1000,
            "hysteresis": 2,
            "min_loss_scale": 1
        },
        "zero_optimization": {
            "stage": 3,  # ZeRO-3: ÊúÄÂ§ß„É°„É¢„É™ÂâäÊ∏õ
            "offload_optimizer": {
                "device": "cpu",  # „Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂Áä∂ÊÖã„ÇíCPU„Å´„Ç™„Éï„É≠„Éº„Éâ
                "pin_memory": True
            },
            "offload_param": {
                "device": "cpu",  # „Éë„É©„É°„Éº„Çø„ÇíCPU„Å´„Ç™„Éï„É≠„Éº„Éâ
                "pin_memory": True
            },
            "overlap_comm": True,
            "contiguous_gradients": True,
            "sub_group_size": 1e9,
            "reduce_bucket_size": 5e8,
            "stage3_prefetch_bucket_size": 5e8,
            "stage3_param_persistence_threshold": 1e6,
            "stage3_max_live_parameters": 1e9,
            "stage3_max_reuse_distance": 1e9,
            "stage3_gather_fp16_weights_on_model_save": True
        },
        "gradient_clipping": 1.0,
        "wall_clock_breakdown": False
    }

    # Â§ßË¶èÊ®°„É¢„Éá„É´ÔºàGPT-2 Large: 774M parametersÔºâ
    model = GPT2LMHeadModel.from_pretrained('gpt2-large')

    # DeepSpeed„Ç®„É≥„Ç∏„É≥„ÅÆÂàùÊúüÂåñ
    model_engine, optimizer, _, _ = deepspeed.initialize(
        model=model,
        model_parameters=model.parameters(),
        config=ds_config
    )

    # „Éá„Éº„Çø„É≠„Éº„ÉÄ„Éº
    train_loader = ...  # „Éá„Éº„Çø„É≠„Éº„ÉÄ„ÉºÊ∫ñÂÇô

    # Ë®ìÁ∑¥„É´„Éº„Éó
    for epoch in range(10):
        for batch in train_loader:
            inputs = batch['input_ids'].to(model_engine.local_rank)
            labels = batch['labels'].to(model_engine.local_rank)

            # È†Ü‰ºùÊí≠
            outputs = model_engine(inputs, labels=labels)
            loss = outputs.loss

            # DeepSpeed„ÅåËá™Âãï„ÅßÈÄÜ‰ºùÊí≠„Å®„Éë„É©„É°„Éº„ÇøÊõ¥Êñ∞
            model_engine.backward(loss)
            model_engine.step()

    # „É¢„Éá„É´‰øùÂ≠òÔºàÂÖ®„Éë„É©„É°„Éº„Çø„ÇíÈõÜÁ¥ÑÔºâ
    model_engine.save_checkpoint('./checkpoints')

if __name__ == "__main__":
    # DeepSpeedËµ∑Âãï
    # deepspeed --num_gpus=8 deepspeed_zero.py
    train_with_deepspeed()
</code></pre>
</details>

<p><strong>ÂÆüË°å„Ç≥„Éû„É≥„Éâ:</strong></p>
<pre><code># Âçò‰∏Ä„Éé„Éº„Éâ„ÄÅ8 GPU
deepspeed --num_gpus=8 deepspeed_zero.py

# Ë§áÊï∞„Éé„Éº„ÉâÔºà2„Éé„Éº„Éâ„ÄÅÂêÑ8 GPUÔºâ
deepspeed --num_nodes=2 --num_gpus=8 --hostfile=hostfile deepspeed_zero.py
</code></pre>

<p><strong>ZeRO-3„ÅÆÂäπÊûúÔºàGPT-3 175B parametersÔºâ:</strong></p>
<ul>
    <li><strong>ÂæìÊù•„ÅÆDDP</strong>: Ë®ìÁ∑¥‰∏çÂèØÔºàGPU„É°„É¢„É™‰∏çË∂≥Ôºâ</li>
    <li><strong>ZeRO-3</strong>: 128 GPUÔºàÂêÑ40GBÔºâ„ÅßË®ìÁ∑¥ÂèØËÉΩ</li>
    <li><strong>„É°„É¢„É™ÂâäÊ∏õ</strong>: 64ÂÄçÔºà128„ÉØ„Éº„Ç´„ÉºÔºâ</li>
</ul>

---

<h2>4.5 ÂàÜÊï£Â≠¶Áøí„ÅÆ„Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„Çπ</h2>

<h3>4.5.1 ÈÄö‰ø°ÊúÄÈÅ©Âåñ</h3>

<p><strong>ÂãæÈÖç„Éê„Ç±„ÉÉ„ÉÜ„Ç£„É≥„Ç∞ÔºàGradient BucketingÔºâ:</strong></p>
<ul>
    <li>Â∞è„Åï„Å™ÂãæÈÖç„Çí„Åæ„Å®„ÇÅ„Å¶ÈÄö‰ø°Ôºà„É¨„Ç§„ÉÜ„É≥„Ç∑ÂâäÊ∏õÔºâ</li>
    <li>PyTorch DDP„ÅØ„Éá„Éï„Ç©„É´„Éà„ÅßÊúâÂäπ</li>
</ul>

<pre><code># DDP„ÅÆ„Éê„Ç±„ÉÉ„Éà„Çµ„Ç§„Ç∫Ë®≠ÂÆö
model = DDP(
    model,
    device_ids=[rank],
    bucket_cap_mb=25,  # „Éê„Ç±„ÉÉ„Éà„Çµ„Ç§„Ç∫ÔºàMBÔºâ
    find_unused_parameters=False  # ‰ΩøÁî®„Åó„Å™„ÅÑ„Éë„É©„É°„Éº„ÇøÊ§úÂá∫„ÇíÁÑ°ÂäπÂåñ
)
</code></pre>

<p><strong>ÂãæÈÖçÂúßÁ∏Æ:</strong></p>
<ul>
    <li>FP16ÂúßÁ∏Æ„ÅßÈÄö‰ø°Èáè50%ÂâäÊ∏õ</li>
    <li>„Çπ„Éë„Éº„ÇπÂåñ„ÄÅÈáèÂ≠êÂåñ</li>
</ul>

<pre><code># Horovod„ÅÆfp16ÂúßÁ∏Æ
optimizer = hvd.DistributedOptimizer(
    optimizer,
    compression=hvd.Compression.fp16
)
</code></pre>

<p><strong>ÈöéÂ±§ÁöÑAllReduce:</strong></p>
<ul>
    <li>„Éé„Éº„ÉâÂÜÖNCCL ‚Üí „Éé„Éº„ÉâÈñìMPI</li>
    <li>Horovod Hierarchical AllReduce</li>
</ul>

<h3>4.5.2 „Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„Å®Â≠¶ÁøíÁéá„ÅÆ„Çπ„Ç±„Éº„É™„É≥„Ç∞</h3>

<p><strong>Linear Scaling RuleÔºàÁ∑öÂΩ¢„Çπ„Ç±„Éº„É™„É≥„Ç∞ÂâáÔºâ:</strong></p>
$$
\text{LR}_{\text{distributed}} = \text{LR}_{\text{base}} \times \frac{\text{Batch}_{\text{distributed}}}{\text{Batch}_{\text{base}}}
$$

<p><strong>‰æã:</strong></p>
<ul>
    <li>„Éô„Éº„Çπ: LR=0.1, Batch=256ÔºàÂçò‰∏ÄGPUÔºâ</li>
    <li>8 GPU: LR=0.8, Batch=2,048Ôºà256√ó8Ôºâ</li>
</ul>

<p><strong>WarmupÔºà„Ç¶„Ç©„Éº„É†„Ç¢„ÉÉ„ÉóÔºâ:</strong></p>
<ul>
    <li>ÊúÄÂàù„ÅÆ„Ç®„Éù„ÉÉ„ÇØ„ÅßÂ≠¶ÁøíÁéá„ÇíÂæê„ÄÖ„Å´Â¢óÂä†</li>
    <li>Â§ß„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„Åß„ÅÆË®ìÁ∑¥ÂÆâÂÆöÂåñ</li>
</ul>

<pre><code>def linear_warmup_cosine_decay(step, warmup_steps, total_steps, base_lr, max_lr):
    """
    Warmup + Cosine Decay Â≠¶ÁøíÁéá„Çπ„Ç±„Ç∏„É•„Éº„É©
    """
    if step < warmup_steps:
        # Warmup: Á∑öÂΩ¢Â¢óÂä†
        lr = base_lr + (max_lr - base_lr) * step / warmup_steps
    else:
        # Cosine Decay
        progress = (step - warmup_steps) / (total_steps - warmup_steps)
        lr = base_lr + 0.5 * (max_lr - base_lr) * (1 + math.cos(math.pi * progress))
    return lr
</code></pre>

<h3>4.5.3 Â≠¶ÁøíÁéáË™øÊï¥„ÅÆ„Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„Çπ</h3>

<p><strong>LARSÔºàLayer-wise Adaptive Rate ScalingÔºâ:</strong></p>
<ul>
    <li>„É¨„Ç§„É§„Éº„Åî„Å®„Å´Â≠¶ÁøíÁéá„ÇíÈÅ©ÂøúË™øÊï¥</li>
    <li>Ë∂ÖÂ§ß„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫Ôºà32KÔΩû64KÔºâ„ÅßÊúâÂäπ</li>
</ul>

<p><strong>LAMBÔºàLayer-wise Adaptive Moments optimizer for Batch trainingÔºâ:</strong></p>
<ul>
    <li>BERTË®ìÁ∑¥„Åß„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫65,536„ÇíÂÆüÁèæ</li>
    <li>Adam„Éô„Éº„Çπ„ÅÆLARSÊã°Âºµ</li>
</ul>

<h3>4.5.4 „Éá„Éê„ÉÉ„Ç∞ÂàÜÊï£„Ç≥„Éº„Éâ</h3>

<p><strong>„Çà„Åè„ÅÇ„Çã„Ç®„É©„Éº:</strong></p>

<p><strong>1. „Éá„ÉÉ„Éâ„É≠„ÉÉ„ÇØ:</strong></p>
<ul>
    <li>ÂÖ®„Éó„É≠„Çª„Çπ„ÅßÂêå„ÅòÈõÜÂõ£ÈÄö‰ø°„ÇíÂÆüË°å</li>
    <li>‰∏ÄÈÉ®„ÅÆ„Éó„É≠„Çª„Çπ„ÅÆ„Åø„ÅåÈÄö‰ø°„Åô„Çã„Å®„Éè„É≥„Ç∞</li>
</ul>

<pre><code># ÊÇ™„ÅÑ‰æã: Rank 0„ÅÆ„Åø„Ååallreduce
if rank == 0:
    dist.all_reduce(tensor)  # ‰ªñ„ÅÆ„Éó„É≠„Çª„Çπ„ÅåÂæÖÊ©ü ‚Üí „Éá„ÉÉ„Éâ„É≠„ÉÉ„ÇØ

# Ê≠£„Åó„ÅÑ‰æã: ÂÖ®„Éó„É≠„Çª„Çπ„Ååallreduce
dist.all_reduce(tensor)
if rank == 0:
    print(tensor)
</code></pre>

<p><strong>2. „É°„É¢„É™„É™„Éº„ÇØ:</strong></p>
<ul>
    <li>ÂãæÈÖçÁ¥ØÁ©çÊôÇ„ÅÆdetachÂøò„Çå</li>
    <li>Ë®àÁÆó„Ç∞„É©„Éï„Åå‰øùÊåÅ„Åï„ÇåÁ∂ö„Åë„Çã</li>
</ul>

<pre><code># ÂãæÈÖçÁ¥ØÁ©çÊôÇ„ÅÆÊ≠£„Åó„ÅÑÂÆüË£Ö
loss = loss / accumulation_steps
loss.backward()

# „É°„Éà„É™„ÇØ„ÇπË®àÁÆóÊôÇ„ÅØdetach
total_loss += loss.detach().item()
</code></pre>

<p><strong>3. ÂÜçÁèæÊÄß„ÅÆÊ¨†Â¶Ç:</strong></p>
<ul>
    <li>„Ç∑„Éº„ÉâË®≠ÂÆö„Åå‰∏çÂçÅÂàÜ</li>
    <li>ÂêÑ„Éó„É≠„Çª„Çπ„ÅßÁï∞„Å™„ÇãÂàùÊúüÂåñ</li>
</ul>

<pre><code>def set_seed(seed, rank):
    """
    ÂÜçÁèæÊÄß„ÅÆ„Åü„ÇÅ„ÅÆ„Ç∑„Éº„ÉâË®≠ÂÆö
    """
    torch.manual_seed(seed + rank)  # „É©„É≥„ÇØ„Åî„Å®„Å´Áï∞„Å™„Çã„Ç∑„Éº„Éâ
    torch.cuda.manual_seed_all(seed + rank)
    np.random.seed(seed + rank)
    random.seed(seed + rank)

    # CuDNN„ÅÆÊ±∫ÂÆöÁöÑÂãï‰ΩúÔºàÈÄüÂ∫¶‰Ωé‰∏ã„ÅÇ„ÇäÔºâ
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
</code></pre>

<p><strong>„Éá„Éê„ÉÉ„Ç∞„ÉÑ„Éº„É´:</strong></p>

<p><strong>NCCLÁí∞Â¢ÉÂ§âÊï∞:</strong></p>
<pre><code>export NCCL_DEBUG=INFO           # NCCL„ÅÆ„Éá„Éê„ÉÉ„Ç∞ÊÉÖÂ†±
export NCCL_DEBUG_SUBSYS=ALL     # ÂÖ®„Çµ„Éñ„Ç∑„Çπ„ÉÜ„É†„Çí„É≠„Ç∞
export NCCL_IB_DISABLE=1         # InfiniBand„ÇíÁÑ°ÂäπÂåñÔºà„Éá„Éê„ÉÉ„Ç∞ÊôÇÔºâ
export NCCL_P2P_DISABLE=1        # P2PÈÄö‰ø°„ÇíÁÑ°ÂäπÂåñ
</code></pre>

<p><strong>PyTorch„Éó„É≠„Éï„Ç°„Ç§„É©:</strong></p>
<pre><code>from torch.profiler import profile, ProfilerActivity

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    record_shapes=True,
    with_stack=True
) as prof:
    for data, target in train_loader:
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
</code></pre>

---

<h2>4.6 „Åæ„Å®„ÇÅ</h2>

<h3>Â≠¶„Çì„Å†„Åì„Å®</h3>

<ol>
    <li>
        <p><strong>ÂàÜÊï£Â≠¶Áøí„ÅÆÊà¶Áï•:</strong></p>
        <ul>
            <li>Data Parallelism: „É¢„Éá„É´„Ç≥„Éî„Éº„ÄÅ„Éá„Éº„ÇøÂàÜÂâ≤„ÄÅÂãæÈÖçÈõÜÁ¥Ñ</li>
            <li>Model Parallelism: „É¢„Éá„É´ÂàÜÂâ≤„ÄÅ„É°„É¢„É™Âà∂Á¥Ñ„ÅÆËß£Ê±∫</li>
            <li>Pipeline Parallelism: „Çπ„ÉÜ„Éº„Ç∏ÂàÜÂâ≤„ÄÅGPUÂà©Áî®ÁéáÂêë‰∏ä</li>
            <li>Hybrid Approaches: 3D Parallelism„ÄÅZeRO</li>
        </ul>
    </li>
    <li>
        <p><strong>PyTorch DDP:</strong></p>
        <ul>
            <li>torch.distributed„ÅÆÂü∫Á§éÔºàRank„ÄÅWorld Size„ÄÅBackendÔºâ</li>
            <li>DistributedSampler„Åß„Éá„Éº„ÇøÂàÜÂâ≤</li>
            <li>DDP„É©„ÉÉ„Éë„Éº„ÅßËá™ÂãïÂãæÈÖçÂêåÊúü</li>
            <li>„Éû„É´„ÉÅ„Éé„Éº„ÉâË®ìÁ∑¥ÔºàSlurm„ÄÅSSHÔºâ</li>
        </ul>
    </li>
    <li>
        <p><strong>Horovod:</strong></p>
        <ul>
            <li>Ring-AllReduce„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£</li>
            <li>MPI/NCCL„Éô„Éº„Çπ„ÅÆÂäπÁéáÁöÑÈÄö‰ø°</li>
            <li>TensorFlow/PyTorch/KerasÂØæÂøú</li>
            <li>FP16ÂúßÁ∏Æ„Åß„Åï„Çâ„Å™„ÇãÈ´òÈÄüÂåñ</li>
        </ul>
    </li>
    <li>
        <p><strong>Â§ßË¶èÊ®°„É¢„Éá„É´Ë®ìÁ∑¥:</strong></p>
        <ul>
            <li>Gradient Accumulation: „É°„É¢„É™Âà∂Á¥Ñ„ÅÆÂõûÈÅø</li>
            <li>Mixed Precision (AMP): 1.5ÔΩû2ÂÄçÈ´òÈÄüÂåñ„ÄÅ50%„É°„É¢„É™ÂâäÊ∏õ</li>
            <li>Gradient Checkpointing: O(n) ‚Üí O(‚àön)„É°„É¢„É™ÂâäÊ∏õ</li>
            <li>DeepSpeed ZeRO: Ë∂ÖÂ§ßË¶èÊ®°„É¢„Éá„É´Ôºà175B parametersÔºâË®ìÁ∑¥</li>
        </ul>
    </li>
    <li>
        <p><strong>„Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„Çπ:</strong></p>
        <ul>
            <li>ÈÄö‰ø°ÊúÄÈÅ©Âåñ: „Éê„Ç±„ÉÉ„ÉÜ„Ç£„É≥„Ç∞„ÄÅÂúßÁ∏Æ„ÄÅÈöéÂ±§ÁöÑAllReduce</li>
            <li>„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„Çπ„Ç±„Éº„É™„É≥„Ç∞: Linear Scaling Rule„ÄÅWarmup</li>
            <li>Â≠¶ÁøíÁéáË™øÊï¥: LARS„ÄÅLAMB</li>
            <li>„Éá„Éê„ÉÉ„Ç∞: „Éá„ÉÉ„Éâ„É≠„ÉÉ„ÇØÂõûÈÅø„ÄÅ„É°„É¢„É™„É™„Éº„ÇØÂØæÁ≠ñ„ÄÅÂÜçÁèæÊÄß</li>
        </ul>
    </li>
</ol>

<h3>Ê¨°„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó</h3>

<p>Á¨¨5Á´†„Åß„ÅØ„ÄÅÂÆü‰∏ñÁïå„ÅÆÂ§ßË¶èÊ®°„Éá„Éº„ÇøÂá¶ÁêÜ„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÂ≠¶„Å≥„Åæ„Åô:</p>
<ul>
    <li>Êé®Ëñ¶„Ç∑„Çπ„ÉÜ„É†„ÅÆÂàÜÊï£Ë®ìÁ∑¥ÔºàNetflix„ÄÅAmazonË¶èÊ®°Ôºâ</li>
    <li>Â§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÅÆ‰∫ãÂâçÂ≠¶ÁøíÔºàBERT„ÄÅGPTÔºâ</li>
    <li>„É™„Ç¢„É´„Çø„Ç§„É†„Çπ„Éà„É™„Éº„É†Âá¶ÁêÜÔºàApache Kafka + Spark StreamingÔºâ</li>
    <li>„Éû„ÉÜ„É™„Ç¢„É´„Ç∫„Éª„Ç§„É≥„Éï„Ç©„Éû„ÉÜ„Ç£„ÇØ„Çπ„Åß„ÅÆÂ§ßË¶èÊ®°„Çπ„ÇØ„É™„Éº„Éã„É≥„Ç∞</li>
</ul>

---

<h2>ÊºîÁøíÂïèÈ°å</h2>

<p><strong>Âïè1:</strong> Data Parallelism„Å®Model Parallelism„ÅÆÈÅï„ÅÑ„Çí„ÄÅ„É°„É¢„É™‰ΩøÁî®Èáè„Å®ÈÄö‰ø°„Éë„Çø„Éº„É≥„ÅÆË¶≥ÁÇπ„Åã„ÇâË™¨Êòé„Åõ„Çà„ÄÇ</p>

<p><strong>Âïè2:</strong> 8 GPU„ÅßË®ìÁ∑¥„Åô„ÇãÂ†¥Âêà„ÄÅÂçò‰∏ÄGPU„Åß„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫32„ÄÅÂ≠¶ÁøíÁéá0.1„Å†„Å£„ÅüÂ†¥Âêà„ÄÅÂàÜÊï£Ë®ìÁ∑¥„Åß„ÅÆÈÅ©Âàá„Å™„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„Å®Â≠¶ÁøíÁéá„ÇíË®àÁÆó„Åõ„Çà„ÄÇ</p>

<p><strong>Âïè3:</strong> Gradient Accumulation„Çí‰Ωø„Å£„Å¶„ÄÅÂÆüÂäπ„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫256„ÇíÂÆüÁèæ„Åó„Åü„ÅÑ„ÄÇGPU„É°„É¢„É™Âà∂Á¥Ñ„Åß„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫32„Åó„ÅãÊâ±„Åà„Å™„ÅÑÂ†¥Âêà„ÄÅaccumulation_steps„Çí„ÅÑ„Åè„Å§„Å´Ë®≠ÂÆö„Åô„Åπ„Åç„Åã„ÄÇ</p>

<p><strong>Âïè4:</strong> Mixed Precision Training (AMP) „ÅåÊï∞ÂÄ§ÂÆâÂÆöÊÄß„Çí‰øù„Å§„Åü„ÇÅ„Å´‰ΩøÁî®„Åô„Çã„ÄåÊêçÂ§±„Çπ„Ç±„Éº„É™„É≥„Ç∞„Äç„ÅÆ‰ªïÁµÑ„Åø„ÇíË™¨Êòé„Åõ„Çà„ÄÇ</p>

<p><strong>Âïè5:</strong> DeepSpeed ZeRO-3„Åå„ÄÅÂæìÊù•„ÅÆData Parallelism„Å®ÊØî„Åπ„Å¶„É°„É¢„É™ÂäπÁéá„ÅåÈ´ò„ÅÑÁêÜÁî±„Çí„ÄÅ„Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂Áä∂ÊÖã„ÄÅÂãæÈÖç„ÄÅ„Éë„É©„É°„Éº„Çø„ÅÆÂàÜÊï£„Å®„ÅÑ„ÅÜË¶≥ÁÇπ„Åã„ÇâË´ñ„Åò„ÇàÔºà500Â≠ó‰ª•ÂÜÖÔºâ„ÄÇ</p>

---

<h2>ÂèÇËÄÉÊñáÁåÆ</h2>

<ol>
    <li>Goyal, P. et al. "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour." <em>arXiv:1706.02677</em> (2017).</li>
    <li>Sergeev, A. & Del Balso, M. "Horovod: fast and easy distributed deep learning in TensorFlow." <em>arXiv:1802.05799</em> (2018).</li>
    <li>Li, S. et al. "PyTorch Distributed: Experiences on Accelerating Data Parallel Training." <em>VLDB</em> (2020).</li>
    <li>Rajbhandari, S. et al. "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models." <em>SC'20</em> (2020).</li>
    <li>Huang, Y. et al. "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism." <em>NeurIPS</em> (2019).</li>
    <li>You, Y. et al. "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes." <em>ICLR</em> (2020).</li>
    <li>Micikevicius, P. et al. "Mixed Precision Training." <em>ICLR</em> (2018).</li>
    <li>Shoeybi, M. et al. "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism." <em>arXiv:1909.08053</em> (2019).</li>
</ol>

---

<p><strong>Ê¨°Á´†</strong>: <a href="chapter5-real-world-applications.html">Á¨¨5Á´†ÔºöÂÆü‰∏ñÁïå„ÅÆÂ§ßË¶èÊ®°„Éá„Éº„ÇøÂá¶ÁêÜ„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥</a></p>

<p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: „Åì„ÅÆ„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÅØCC BY 4.0„É©„Ç§„Çª„É≥„Çπ„ÅÆ‰∏ã„ÅßÊèê‰æõ„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ</p>

<div class="navigation">
    <a href="chapter3-mapreduce-and-spark.html" class="nav-button">‚Üê Ââç„ÅÆÁ´†</a>
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
    <a href="chapter5-real-world-applications.html" class="nav-button">Ê¨°„ÅÆÁ´† ‚Üí</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-21</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
