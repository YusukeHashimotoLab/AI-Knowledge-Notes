<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 4 - Ê∑±Â±§Â≠¶Áøí„Å´„Çà„ÇãÁï∞Â∏∏Ê§úÁü• - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Á¨¨4Á´†ÔºöÊ∑±Â±§Â≠¶Áøí„Å´„Çà„ÇãÁï∞Â∏∏Ê§úÁü•</h1>
            <p class="subtitle">Autoencoder„ÄÅVAE„ÄÅGAN„ÄÅÊôÇÁ≥ªÂàóÁï∞Â∏∏Ê§úÁü•</p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 80-90ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ‰∏≠Á¥ö„Äú‰∏äÁ¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 9ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 6Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">

<div class="learning-objectives">
<h2>Â≠¶ÁøíÁõÆÊ®ô</h2>
<ul>
<li>Autoencoder„Å´„Çà„ÇãÁï∞Â∏∏Ê§úÁü•„ÅÆÂéüÁêÜ„Å®ÂÆüË£Ö„ÇíÁêÜËß£„Åô„Çã</li>
<li>Variational Autoencoder (VAE) „ÅÆÁ¢∫ÁéáÁöÑ„Ç¢„Éó„É≠„Éº„ÉÅ„ÇíÂ≠¶„Å∂</li>
<li>GAN-basedÁï∞Â∏∏Ê§úÁü•ÔºàAnoGANÔºâ„ÅÆ‰ªïÁµÑ„Åø„ÇíÁêÜËß£„Åô„Çã</li>
<li>LSTM Autoencoder„ÅßÊôÇÁ≥ªÂàóÁï∞Â∏∏Ê§úÁü•„ÇíÂÆüË£Ö„Åô„Çã</li>
<li>„Ç®„É≥„Éâ„ÉÑ„Éº„Ç®„É≥„Éâ„ÅÆÁï∞Â∏∏Ê§úÁü•„Éë„Ç§„Éó„É©„Ç§„É≥„ÇíÊßãÁØâ„Åß„Åç„Çã</li>
</ul>
</div>

<h2>4.1 Autoencoder„Å´„Çà„ÇãÁï∞Â∏∏Ê§úÁü•</h2>

<h3>4.1.1 Autoencoder„ÅÆÂü∫Á§é</h3>

<p><strong>AutoencoderÔºàËá™Â∑±Á¨¶Âè∑ÂåñÂô®Ôºâ</strong>„ÅØ„ÄÅÂÖ•Âäõ„Éá„Éº„Çø„ÇíÂúßÁ∏Æ„Åó„ÄÅÂÜçÊßãÊàê„Åô„ÇãÊïôÂ∏´„Å™„ÅóÂ≠¶Áøí„É¢„Éá„É´„Åß„Åô„ÄÇÊ≠£Â∏∏„Éá„Éº„Çø„ÅßË®ìÁ∑¥„Åô„Çã„Åì„Å®„Åß„ÄÅÁï∞Â∏∏„Éá„Éº„Çø„ÅÆÂÜçÊßãÊàêË™§Â∑Æ„ÅåÂ§ß„Åç„Åè„Å™„Çã„Åì„Å®„ÇíÂà©Áî®„Åó„Å¶Áï∞Â∏∏Ê§úÁü•„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ</p>

<p><strong>„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£Ôºö</strong></p>

<pre><code>Input (x)
    ‚Üì
Encoder: x ‚Üí z (ÊΩúÂú®Ë°®Áèæ)
    ‚Üì
Latent Space (z)
    ‚Üì
Decoder: z ‚Üí xÃÇ (ÂÜçÊßãÊàê)
    ‚Üì
Reconstruction Error: ||x - xÃÇ||¬≤
</code></pre>

<p><strong>Áï∞Â∏∏Ê§úÁü•„ÅÆÂéüÁêÜÔºö</strong></p>

<ul>
<li>Ê≠£Â∏∏„Éá„Éº„Çø„ÅßË®ìÁ∑¥: Ê≠£Â∏∏„Éë„Çø„Éº„É≥„ÇíÂ≠¶Áøí</li>
<li>ÂÜçÊßãÊàêË™§Â∑Æ„ÅåÂ∞è„Åï„ÅÑ: Ê≠£Â∏∏„Éá„Éº„Çø</li>
<li>ÂÜçÊßãÊàêË™§Â∑Æ„ÅåÂ§ß„Åç„ÅÑ: Áï∞Â∏∏„Éá„Éº„ÇøÔºàÂ≠¶Áøí„Åó„Å¶„ÅÑ„Å™„ÅÑ„Éë„Çø„Éº„É≥Ôºâ</li>
</ul>

<p><strong>Êï∞ÂºèË°®ÁèæÔºö</strong></p>

<p>$$
\text{Anomaly Score} = \|x - \text{Decoder}(\text{Encoder}(x))\|^2
$$</p>

<h3>4.1.2 ÂÜçÊßãÊàêË™§Â∑Æ„Å®ThresholdÈÅ∏Êäû</h3>

<p>Áï∞Â∏∏Âà§ÂÆö„Å´„ÅØ„ÄÅÂÜçÊßãÊàêË™§Â∑Æ„Å´ÂØæ„Åô„Çãthreshold„ÇíË®≠ÂÆö„Åó„Åæ„Åô„ÄÇ</p>

<p><strong>ThresholdË®≠ÂÆöÊâãÊ≥ïÔºö</strong></p>

<table>
<tr>
<th>ÊâãÊ≥ï</th>
<th>Ë™¨Êòé</th>
<th>ÈÅ©Áî®Â†¥Èù¢</th>
</tr>
<tr>
<td>ÁôæÂàÜ‰ΩçÊï∞Ê≥ï</td>
<td>Ë®ìÁ∑¥„Éá„Éº„Çø„ÅÆÂÜçÊßãÊàêË™§Â∑Æ„ÅÆ95%ÁÇπ</td>
<td>Ê≠£Â∏∏„Éá„Éº„Çø„ÅÆ„Åø„ÅßÂ≠¶Áøí</td>
</tr>
<tr>
<td>Áµ±Ë®àÁöÑÊâãÊ≥ï</td>
<td>Âπ≥Âùá + 3œÉ</td>
<td>Ê≠£Ë¶èÂàÜÂ∏É„Çí‰ªÆÂÆö</td>
</tr>
<tr>
<td>ROCÊõ≤Á∑ö</td>
<td>Ê§úË®º„Éá„Éº„Çø„ÅßAUCÊúÄÂ§ßÂåñ</td>
<td>Â∞ëÈáè„ÅÆÁï∞Â∏∏„É©„Éô„É´„ÅÇ„Çä</td>
</tr>
<tr>
<td>Ê•≠ÂãôË¶Å‰ª∂</td>
<td>False PositiveÁéá„ÇíÊåáÂÆö</td>
<td>ÂÆüÈÅãÁî®ÈáçË¶ñ</td>
</tr>
</table>

<h3>4.1.3 PyTorchÂÆüË£ÖÔºàÂÆåÂÖ®ÁâàÔºâ</h3>

<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, precision_recall_curve

# Autoencoder„É¢„Éá„É´ÂÆöÁæ©
class Autoencoder(nn.Module):
    """„Ç∑„É≥„Éó„É´„Å™Autoencoder"""
    def __init__(self, input_dim=784, hidden_dims=[128, 64, 32]):
        super(Autoencoder, self).__init__()

        # Encoder
        encoder_layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            encoder_layers.append(nn.Linear(prev_dim, hidden_dim))
            encoder_layers.append(nn.ReLU())
            prev_dim = hidden_dim

        self.encoder = nn.Sequential(*encoder_layers)

        # DecoderÔºàEncoder„ÅÆÈÄÜÈ†ÜÔºâ
        decoder_layers = []
        for i in range(len(hidden_dims) - 1, 0, -1):
            decoder_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i-1]))
            decoder_layers.append(nn.ReLU())

        decoder_layers.append(nn.Linear(hidden_dims[0], input_dim))
        decoder_layers.append(nn.Sigmoid())  # Âá∫Âäõ„Çí[0,1]„Å´Ê≠£Ë¶èÂåñ

        self.decoder = nn.Sequential(*decoder_layers)

    def forward(self, x):
        """È†Ü‰ºùÊí≠"""
        z = self.encoder(x)  # „Ç®„É≥„Ç≥„Éº„Éâ
        x_reconstructed = self.decoder(z)  # „Éá„Ç≥„Éº„Éâ
        return x_reconstructed

    def encode(self, x):
        """ÊΩúÂú®Ë°®Áèæ„ÅÆÂèñÂæó"""
        return self.encoder(x)


def train_autoencoder(model, train_loader, n_epochs=50, lr=0.001):
    """Autoencoder„ÅÆË®ìÁ∑¥"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    criterion = nn.MSELoss()  # ÂÜçÊßãÊàêË™§Â∑ÆÔºàMean Squared ErrorÔºâ
    optimizer = optim.Adam(model.parameters(), lr=lr)

    train_losses = []

    for epoch in range(n_epochs):
        model.train()
        epoch_loss = 0.0

        for batch_x, in train_loader:
            batch_x = batch_x.to(device)

            # Forward pass
            reconstructed = model(batch_x)
            loss = criterion(reconstructed, batch_x)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(train_loader)
        train_losses.append(avg_loss)

        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{n_epochs}], Loss: {avg_loss:.6f}")

    return model, train_losses


def compute_reconstruction_errors(model, data_loader):
    """ÂÜçÊßãÊàêË™§Â∑Æ„ÅÆË®àÁÆó"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.eval()

    errors = []

    with torch.no_grad():
        for batch_x, in data_loader:
            batch_x = batch_x.to(device)
            reconstructed = model(batch_x)

            # „Çµ„É≥„Éó„É´„Åî„Å®„ÅÆÂÜçÊßãÊàêË™§Â∑ÆÔºàMSEÔºâ
            batch_errors = torch.mean((batch_x - reconstructed) ** 2, dim=1)
            errors.extend(batch_errors.cpu().numpy())

    return np.array(errors)


def detect_anomalies(model, test_loader, threshold):
    """Áï∞Â∏∏Ê§úÁü•„ÅÆÂÆüË°å"""
    errors = compute_reconstruction_errors(model, test_loader)
    predictions = (errors > threshold).astype(int)
    return predictions, errors


# ‰ΩøÁî®‰æã
if __name__ == "__main__":
    # „Çµ„É≥„Éó„É´„Éá„Éº„ÇøÁîüÊàêÔºàÊ≠£Ë¶èÂàÜÂ∏É„ÅÆÊ≠£Â∏∏„Éá„Éº„ÇøÔºâ
    np.random.seed(42)
    torch.manual_seed(42)

    # Ê≠£Â∏∏„Éá„Éº„ÇøÔºà28x28 = 784Ê¨°ÂÖÉÔºâ
    n_normal = 1000
    normal_data = np.random.randn(n_normal, 784) * 0.5 + 0.5
    normal_data = np.clip(normal_data, 0, 1)

    # Áï∞Â∏∏„Éá„Éº„ÇøÔºàÊ≠£Â∏∏„Å®„ÅØÁï∞„Å™„ÇãÂàÜÂ∏ÉÔºâ
    n_anomaly = 50
    anomaly_data = np.random.uniform(0, 1, (n_anomaly, 784))

    # PyTorch Dataset
    train_dataset = TensorDataset(torch.FloatTensor(normal_data))
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

    test_data = np.vstack([normal_data[:100], anomaly_data])
    test_labels = np.array([0] * 100 + [1] * n_anomaly)  # 0: Ê≠£Â∏∏, 1: Áï∞Â∏∏

    test_dataset = TensorDataset(torch.FloatTensor(test_data))
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

    # „É¢„Éá„É´Ë®ìÁ∑¥
    print("=== AutoencoderË®ìÁ∑¥ÈñãÂßã ===")
    model = Autoencoder(input_dim=784, hidden_dims=[256, 128, 64])
    trained_model, losses = train_autoencoder(model, train_loader, n_epochs=50, lr=0.001)

    # ThresholdË®≠ÂÆöÔºàË®ìÁ∑¥„Éá„Éº„Çø„ÅÆ95%ÁÇπÔºâ
    train_errors = compute_reconstruction_errors(trained_model, train_loader)
    threshold = np.percentile(train_errors, 95)
    print(f"\nÈñæÂÄ§Ôºà95%ÁÇπÔºâ: {threshold:.6f}")

    # „ÉÜ„Çπ„Éà„Éá„Éº„Çø„ÅßÁï∞Â∏∏Ê§úÁü•
    predictions, test_errors = detect_anomalies(trained_model, test_loader, threshold)

    # Ë©ï‰æ°
    from sklearn.metrics import classification_report, roc_auc_score

    print("\n=== Áï∞Â∏∏Ê§úÁü•ÁµêÊûú ===")
    print(classification_report(test_labels, predictions,
                                target_names=['Normal', 'Anomaly']))

    auc_score = roc_auc_score(test_labels, test_errors)
    print(f"ROC-AUC: {auc_score:.3f}")

    # ÂèØË¶ñÂåñ
    plt.figure(figsize=(12, 4))

    # Â≠¶ÁøíÊõ≤Á∑ö
    plt.subplot(1, 2, 1)
    plt.plot(losses)
    plt.xlabel('Epoch')
    plt.ylabel('Reconstruction Loss')
    plt.title('Training Loss Curve')
    plt.grid(True)

    # ÂÜçÊßãÊàêË™§Â∑ÆÂàÜÂ∏É
    plt.subplot(1, 2, 2)
    plt.hist(test_errors[test_labels == 0], bins=30, alpha=0.6, label='Normal')
    plt.hist(test_errors[test_labels == 1], bins=30, alpha=0.6, label='Anomaly')
    plt.axvline(threshold, color='r', linestyle='--', label=f'Threshold={threshold:.3f}')
    plt.xlabel('Reconstruction Error')
    plt.ylabel('Frequency')
    plt.title('Reconstruction Error Distribution')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig('autoencoder_anomaly_detection.png', dpi=150)
    print("\n„Ç∞„É©„Éï„Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü: autoencoder_anomaly_detection.png")
</code></pre>

<h3>4.1.4 „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÅÆÈÅ∏Êäû</h3>

<p><strong>„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£Ë®≠Ë®à„ÅÆ„Éù„Ç§„É≥„ÉàÔºö</strong></p>

<table>
<tr>
<th>Ë¶ÅÁ¥†</th>
<th>Êé®Â•®ÂÄ§</th>
<th>ÁêÜÁî±</th>
</tr>
<tr>
<td>ÊΩúÂú®Ê¨°ÂÖÉ</td>
<td>ÂÖ•ÂäõÊ¨°ÂÖÉ„ÅÆ10-30%</td>
<td>ÈÅéÂ∫¶„Å™ÂúßÁ∏Æ„ÅØÊÉÖÂ†±ÊêçÂ§±„ÄÅÂ§ß„Åç„Åô„Åé„Çã„Å®ÊÅíÁ≠âÂÜôÂÉè</td>
</tr>
<tr>
<td>Èö†„ÇåÂ±§Êï∞</td>
<td>2-4Â±§</td>
<td>Ê∑±„Åô„Åé„Çã„Å®Ë®ìÁ∑¥Âõ∞Èõ£„ÄÅÊµÖ„Åô„Åé„Çã„Å®Ë°®ÁèæÂäõ‰∏çË∂≥</td>
</tr>
<tr>
<td>Ê¥ªÊÄßÂåñÈñ¢Êï∞</td>
<td>ReLUÔºàÈö†„ÇåÂ±§Ôºâ„ÄÅSigmoidÔºàÂá∫ÂäõÔºâ</td>
<td>ÂãæÈÖçÊ∂àÂ§±„ÇíÈò≤„Åê„ÄÅÂá∫ÂäõÁØÑÂõ≤„ÇíÂà∂Èôê</td>
</tr>
<tr>
<td>Dropout</td>
<td>0.2-0.3</td>
<td>ÈÅéÂ≠¶ÁøíÈò≤Ê≠¢Ôºà„Åü„Å†„ÅóÁï∞Â∏∏Ê§úÁü•„Åß„ÅØÊÖéÈáç„Å´Ôºâ</td>
</tr>
</table>

<hr>

<h2>4.2 Variational Autoencoder (VAE)</h2>

<h3>4.2.1 VAE„ÅÆÂãïÊ©ü</h3>

<p><strong>ÈÄöÂ∏∏„ÅÆAutoencoder„ÅÆË™≤È°åÔºö</strong></p>
<ul>
<li>ÊΩúÂú®Á©∫Èñì„Åå‰∏çÈÄ£Á∂ö„Åß„ÄÅÊÑèÂë≥„ÅÆ„ÅÇ„ÇãÊßãÈÄ†„ÇíÊåÅ„Åü„Å™„ÅÑ</li>
<li>Â≠¶Áøí„Éá„Éº„Çø„Å´ÈÅéÂâ∞ÈÅ©Âêà„Åó„ÇÑ„Åô„ÅÑ</li>
<li>ÁîüÊàêËÉΩÂäõ„ÅåÈôêÂÆöÁöÑ</li>
</ul>

<p><strong>VAE„ÅÆÁâπÂæ¥Ôºö</strong></p>
<ul>
<li>ÊΩúÂú®Â§âÊï∞„ÇíÁ¢∫ÁéáÂàÜÂ∏É„Å®„Åó„Å¶„É¢„Éá„É´Âåñ</li>
<li>Ê≠£ÂâáÂåñ„Å´„Çà„ÇäÊªë„Çâ„Åã„Å™ÊΩúÂú®Á©∫Èñì„ÇíÂ≠¶Áøí</li>
<li>ÁîüÊàê„É¢„Éá„É´„Å®„Åó„Å¶„ÇÇÊ©üËÉΩ</li>
</ul>

<h3>4.2.2 VAE„ÅÆÊï∞ÁêÜ</h3>

<p><strong>Á¢∫ÁéáÁöÑ„Ç®„É≥„Ç≥„Éº„ÉÄÔºö</strong></p>

<p>$$
q_\phi(z|x) = \mathcal{N}(z; \mu(x), \sigma^2(x))
$$</p>

<p>„Ç®„É≥„Ç≥„Éº„ÉÄ„ÅØÂπ≥Âùá$\mu(x)$„Å®ÂàÜÊï£$\sigma^2(x)$„ÇíÂá∫Âäõ„Åó„Åæ„Åô„ÄÇ</p>

<p><strong>„Éá„Ç≥„Éº„ÉÄÔºö</strong></p>

<p>$$
p_\theta(x|z) = \mathcal{N}(x; \mu_{\text{dec}}(z), \sigma^2_{\text{dec}})
$$</p>

<p><strong>ÊêçÂ§±Èñ¢Êï∞ÔºàELBOÔºâÔºö</strong></p>

<p>$$
\mathcal{L} = \underbrace{\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]}_{\text{Reconstruction Loss}} - \underbrace{D_{KL}(q_\phi(z|x) \| p(z))}_{\text{KL Divergence}}
$$</p>

<ul>
<li>Á¨¨1È†Ö: ÂÜçÊßãÊàêÊêçÂ§±ÔºàAutoencoder„Å®Âêå„ÅòÔºâ</li>
<li>Á¨¨2È†Ö: KL„ÉÄ„Ç§„Éê„Éº„Ç∏„Çß„É≥„ÇπÔºàÊ≠£ÂâáÂåñÈ†ÖÔºâ„ÄÅ$p(z) = \mathcal{N}(0, I)$„Çí‰ªÆÂÆö</li>
</ul>

<p><strong>KL„ÉÄ„Ç§„Éê„Éº„Ç∏„Çß„É≥„Çπ„ÅÆËß£ÊûêËß£Ôºö</strong></p>

<p>$$
D_{KL} = -\frac{1}{2} \sum_{j=1}^{J} (1 + \log(\sigma_j^2) - \mu_j^2 - \sigma_j^2)
$$</p>

<h3>4.2.3 VAE„Å´„Çà„ÇãÁï∞Â∏∏Ê§úÁü•</h3>

<p>VAE„Åß„ÅØ„ÄÅÂÜçÊßãÊàêË™§Â∑Æ„Å®KL„ÉÄ„Ç§„Éê„Éº„Ç∏„Çß„É≥„Çπ„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„Å¶Áï∞Â∏∏„Çπ„Ç≥„Ç¢„ÇíË®àÁÆó„Åó„Åæ„Åô„ÄÇ</p>

<p>$$
\text{Anomaly Score} = \text{Reconstruction Error} + \beta \cdot D_{KL}
$$</p>

<h3>4.2.4 PyTorchÂÆüË£Ö</h3>

<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

class VAE(nn.Module):
    """Variational Autoencoder"""
    def __init__(self, input_dim=784, latent_dim=32, hidden_dims=[256, 128]):
        super(VAE, self).__init__()

        self.latent_dim = latent_dim

        # Encoder
        encoder_layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            encoder_layers.append(nn.Linear(prev_dim, hidden_dim))
            encoder_layers.append(nn.ReLU())
            prev_dim = hidden_dim

        self.encoder = nn.Sequential(*encoder_layers)

        # ÊΩúÂú®ÂàÜÂ∏É„ÅÆ„Éë„É©„É°„Éº„ÇøÔºàÂπ≥Âùá„Å®ÂàÜÊï£Ôºâ
        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)
        self.fc_logvar = nn.Linear(hidden_dims[-1], latent_dim)

        # Decoder
        decoder_layers = []
        decoder_layers.append(nn.Linear(latent_dim, hidden_dims[-1]))
        decoder_layers.append(nn.ReLU())

        for i in range(len(hidden_dims) - 1, 0, -1):
            decoder_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i-1]))
            decoder_layers.append(nn.ReLU())

        decoder_layers.append(nn.Linear(hidden_dims[0], input_dim))
        decoder_layers.append(nn.Sigmoid())

        self.decoder = nn.Sequential(*decoder_layers)

    def encode(self, x):
        """„Ç®„É≥„Ç≥„Éº„Éâ: Âπ≥Âùá„Å®ÂØæÊï∞ÂàÜÊï£„ÇíÂá∫Âäõ"""
        h = self.encoder(x)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        """ÂÜç„Éë„É©„É°„Éº„ÇøÂåñ„Éà„É™„ÉÉ„ÇØ"""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)  # N(0, 1)„Åã„Çâ„Çµ„É≥„Éó„É™„É≥„Ç∞
        z = mu + eps * std
        return z

    def decode(self, z):
        """„Éá„Ç≥„Éº„Éâ"""
        return self.decoder(z)

    def forward(self, x):
        """È†Ü‰ºùÊí≠"""
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        x_reconstructed = self.decode(z)
        return x_reconstructed, mu, logvar


def vae_loss(x, x_reconstructed, mu, logvar, beta=1.0):
    """VAEÊêçÂ§±Èñ¢Êï∞

    Args:
        beta: KL„ÉÄ„Ç§„Éê„Éº„Ç∏„Çß„É≥„Çπ„ÅÆÈáç„ÅøÔºàŒ≤-VAEÔºâ
    """
    # Reconstruction lossÔºà„Éê„Ç§„Éä„É™„ÇØ„É≠„Çπ„Ç®„É≥„Éà„É≠„Éî„ÉºÔºâ
    recon_loss = F.binary_cross_entropy(x_reconstructed, x, reduction='sum')

    # KL Divergence
    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

    # Total loss
    total_loss = recon_loss + beta * kl_div

    return total_loss, recon_loss, kl_div


def train_vae(model, train_loader, n_epochs=50, lr=0.001, beta=1.0):
    """VAE„ÅÆË®ìÁ∑¥"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(n_epochs):
        model.train()
        train_loss = 0.0

        for batch_x, in train_loader:
            batch_x = batch_x.to(device)

            # Forward pass
            x_recon, mu, logvar = model(batch_x)
            loss, recon, kl = vae_loss(batch_x, x_recon, mu, logvar, beta)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        avg_loss = train_loss / len(train_loader.dataset)

        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{n_epochs}], Loss: {avg_loss:.4f}")

    return model


def vae_anomaly_score(model, x, beta=1.0):
    """VAE„Å´„Çà„ÇãÁï∞Â∏∏„Çπ„Ç≥„Ç¢Ë®àÁÆó"""
    model.eval()
    device = next(model.parameters()).device

    with torch.no_grad():
        x = x.to(device)
        x_recon, mu, logvar = model(x)

        # Reconstruction errorÔºà„Çµ„É≥„Éó„É´„Åî„Å®Ôºâ
        recon_error = F.binary_cross_entropy(x_recon, x, reduction='none').sum(dim=1)

        # KL divergenceÔºà„Çµ„É≥„Éó„É´„Åî„Å®Ôºâ
        kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)

        # Anomaly score
        anomaly_scores = recon_error + beta * kl_div

    return anomaly_scores.cpu().numpy()


# ‰ΩøÁî®‰æã
if __name__ == "__main__":
    # „Éá„Éº„ÇøÊ∫ñÂÇôÔºàÂâçËø∞„Å®Âêå„ÅòÔºâ
    train_dataset = TensorDataset(torch.FloatTensor(normal_data))
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

    # VAE„É¢„Éá„É´
    print("=== VAEË®ìÁ∑¥ÈñãÂßã ===")
    vae_model = VAE(input_dim=784, latent_dim=32, hidden_dims=[256, 128])
    trained_vae = train_vae(vae_model, train_loader, n_epochs=50, lr=0.001, beta=1.0)

    # Áï∞Â∏∏„Çπ„Ç≥„Ç¢Ë®àÁÆó
    test_tensor = torch.FloatTensor(test_data)
    anomaly_scores = vae_anomaly_score(trained_vae, test_tensor, beta=1.0)

    # ThresholdË®≠ÂÆö„Å®Ë©ï‰æ°
    threshold = np.percentile(anomaly_scores[:100], 95)  # Ê≠£Â∏∏„Éá„Éº„Çø„ÅÆ95%ÁÇπ
    predictions = (anomaly_scores > threshold).astype(int)

    print("\n=== VAEÁï∞Â∏∏Ê§úÁü•ÁµêÊûú ===")
    print(classification_report(test_labels, predictions,
                                target_names=['Normal', 'Anomaly']))
    print(f"ROC-AUC: {roc_auc_score(test_labels, anomaly_scores):.3f}")
</code></pre>

<h3>4.2.5 ÊΩúÂú®Á©∫Èñì„ÅÆÂàÜÊûê</h3>

<p>VAE„ÅÆÊΩúÂú®Á©∫Èñì„ÅØ„ÄÅÊ≠£Â∏∏„Éá„Éº„Çø„ÅåÊªë„Çâ„Åã„Å´ÂàÜÂ∏É„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇÁï∞Â∏∏„Éá„Éº„Çø„ÅØÊΩúÂú®Á©∫Èñì„ÅÆÂ§ñ„Çå„ÅüÈ†òÂüü„Å´ÈÖçÁΩÆ„Åï„Çå„Çã„Åì„Å®„ÅåÊúüÂæÖ„Åï„Çå„Åæ„Åô„ÄÇ</p>

<pre><code>import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

def visualize_latent_space(model, data, labels):
    """ÊΩúÂú®Á©∫Èñì„ÅÆÂèØË¶ñÂåñÔºà2Ê¨°ÂÖÉÊäïÂΩ±Ôºâ"""
    model.eval()
    device = next(model.parameters()).device

    with torch.no_grad():
        data_tensor = torch.FloatTensor(data).to(device)
        mu, _ = model.encode(data_tensor)
        latent_codes = mu.cpu().numpy()

    # 2Ê¨°ÂÖÉ„Å´ÂúßÁ∏ÆÔºàÊΩúÂú®Ê¨°ÂÖÉ„Åå2„Çà„ÇäÂ§ß„Åç„ÅÑÂ†¥ÂêàÔºâ
    if latent_codes.shape[1] > 2:
        pca = PCA(n_components=2)
        latent_2d = pca.fit_transform(latent_codes)
    else:
        latent_2d = latent_codes

    # „Éó„É≠„ÉÉ„Éà
    plt.figure(figsize=(8, 6))
    plt.scatter(latent_2d[labels == 0, 0], latent_2d[labels == 0, 1],
                c='blue', alpha=0.5, label='Normal')
    plt.scatter(latent_2d[labels == 1, 0], latent_2d[labels == 1, 1],
                c='red', alpha=0.5, label='Anomaly')
    plt.xlabel('Latent Dimension 1')
    plt.ylabel('Latent Dimension 2')
    plt.title('VAE Latent Space Visualization')
    plt.legend()
    plt.grid(True)
    plt.savefig('vae_latent_space.png', dpi=150)
    print("ÊΩúÂú®Á©∫Èñì„ÅÆÂèØË¶ñÂåñ„Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü: vae_latent_space.png")

# ‰ΩøÁî®‰æã
visualize_latent_space(trained_vae, test_data, test_labels)
</code></pre>

<hr>

<h2>4.3 GAN-basedÁï∞Â∏∏Ê§úÁü•</h2>

<h3>4.3.1 AnoGANÔºàAnomaly Detection with GANÔºâ</h3>

<p><strong>AnoGAN</strong>„ÅØ„ÄÅGAN„ÇíÁî®„ÅÑ„Å¶Ê≠£Â∏∏„Éá„Éº„Çø„ÅÆÁîüÊàê„É¢„Éá„É´„ÇíÂ≠¶Áøí„Åó„ÄÅ„ÉÜ„Çπ„Éà„Éá„Éº„Çø„Åå„Å©„ÅÆÁ®ãÂ∫¶„Åù„ÅÆÁîüÊàêÂàÜÂ∏É„Åã„ÇâÈÄ∏ËÑ±„Åó„Å¶„ÅÑ„Çã„Åã„ÅßÁï∞Â∏∏„ÇíÊ§úÁü•„Åó„Åæ„Åô„ÄÇ</p>

<p><strong>Ë®ìÁ∑¥„Éï„Çß„Éº„Ç∫Ôºö</strong></p>
<ul>
<li>Ê≠£Â∏∏„Éá„Éº„Çø„ÅßGAN„ÇíË®ìÁ∑¥</li>
<li>Generator G„ÅåÊ≠£Â∏∏„Éá„Éº„Çø„ÅÆÂàÜÂ∏É„ÇíÂ≠¶Áøí</li>
</ul>

<p><strong>„ÉÜ„Çπ„Éà„Éï„Çß„Éº„Ç∫Ôºö</strong></p>
<ol>
<li>„ÉÜ„Çπ„Éà„Çµ„É≥„Éó„É´$x$„Å´ÂØæ„Åó„Å¶„ÄÅÊΩúÂú®Â§âÊï∞$z$„ÇíÊúÄÈÅ©Âåñ: $G(z) \approx x$</li>
<li>Áï∞Â∏∏„Çπ„Ç≥„Ç¢„ÇíË®àÁÆó: Residual Loss + Discrimination Loss</li>
</ol>

<h3>4.3.2 Áï∞Â∏∏„Çπ„Ç≥„Ç¢„ÅÆÂÆöÁæ©</h3>

<p>$$
A(x) = (1 - \lambda) \cdot L_R(x) + \lambda \cdot L_D(x)
$$</p>

<ul>
<li>$L_R(x) = \|x - G(z^*)\|_1$: Residual LossÔºàÂÜçÊßãÊàêË™§Â∑ÆÔºâ</li>
<li>$L_D(x) = \|f(x) - f(G(z^*))\|_1$: Discrimination LossÔºàÁâπÂæ¥Á©∫Èñì„Åß„ÅÆË∑ùÈõ¢Ôºâ</li>
<li>$f(\cdot)$: Discriminator„ÅÆ‰∏≠ÈñìÂ±§„ÅÆÁâπÂæ¥</li>
</ul>

<h3>4.3.3 ÊΩúÂú®Â§âÊï∞„ÅÆÊúÄÈÅ©Âåñ</h3>

<p>„ÉÜ„Çπ„Éà„Çµ„É≥„Éó„É´$x$„Å´ÂØæ„Åó„Å¶„ÄÅ$G(z) \approx x$„Å®„Å™„Çã$z$„ÇíÂãæÈÖçÈôç‰∏ãÊ≥ï„ÅßÊé¢Á¥¢Ôºö</p>

<p>$$
z^* = \arg\min_z \|x - G(z)\|_1 + \lambda \|f(x) - f(G(z))\|_1
$$</p>

<h3>4.3.4 ÂÆüË£ÖÊ¶ÇË¶Å</h3>

<pre><code>import torch
import torch.nn as nn

class Generator(nn.Module):
    """GAN Generator"""
    def __init__(self, latent_dim=100, output_dim=784):
        super(Generator, self).__init__()

        self.model = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, output_dim),
            nn.Sigmoid()
        )

    def forward(self, z):
        return self.model(z)


class Discriminator(nn.Module):
    """GAN DiscriminatorÔºà‰∏≠ÈñìÂ±§„ÅÆÁâπÂæ¥„ÇÇÂèñÂæóÔºâ"""
    def __init__(self, input_dim=784):
        super(Discriminator, self).__init__()

        self.features = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2)
        )

        self.classifier = nn.Sequential(
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, x, return_features=False):
        feat = self.features(x)
        output = self.classifier(feat)

        if return_features:
            return output, feat
        return output


def find_latent_code(generator, discriminator, x, n_iterations=500, lr=0.01, lambda_weight=0.1):
    """„ÉÜ„Çπ„Éà„Çµ„É≥„Éó„É´x„Å´ÂØæ„Åô„ÇãÊúÄÈÅ©„Å™ÊΩúÂú®Â§âÊï∞z„ÇíÊé¢Á¥¢"""
    device = next(generator.parameters()).device

    # ÂàùÊúüÂåñ
    z = torch.randn(x.size(0), generator.model[0].in_features, device=device, requires_grad=True)
    optimizer = torch.optim.Adam([z], lr=lr)

    for i in range(n_iterations):
        optimizer.zero_grad()

        # ÁîüÊàê
        G_z = generator(z)

        # Residual Loss
        residual_loss = torch.mean(torch.abs(x - G_z))

        # Discrimination LossÔºàÁâπÂæ¥Á©∫Èñì„Åß„ÅÆË∑ùÈõ¢Ôºâ
        _, feat_real = discriminator(x, return_features=True)
        _, feat_fake = discriminator(G_z, return_features=True)
        discrimination_loss = torch.mean(torch.abs(feat_real - feat_fake))

        # Total loss
        loss = (1 - lambda_weight) * residual_loss + lambda_weight * discrimination_loss

        loss.backward()
        optimizer.step()

    # Áï∞Â∏∏„Çπ„Ç≥„Ç¢
    with torch.no_grad():
        G_z_final = generator(z)
        residual = torch.mean(torch.abs(x - G_z_final), dim=1)

        _, feat_real = discriminator(x, return_features=True)
        _, feat_fake = discriminator(G_z_final, return_features=True)
        discrimination = torch.mean(torch.abs(feat_real - feat_fake), dim=1)

        anomaly_scores = (1 - lambda_weight) * residual + lambda_weight * discrimination

    return anomaly_scores.cpu().numpy()


# Ê≥®ÊÑè: GAN„ÅÆË®ìÁ∑¥„Ç≥„Éº„Éâ„ÅØÁúÅÁï•ÔºàÊ®ôÊ∫ñÁöÑ„Å™GANË®ìÁ∑¥„ÇíÂÆüÊñΩÔºâ
# ÂÆüÈöõ„ÅÆ‰ΩøÁî®„Åß„ÅØ„ÄÅ„Åæ„ÅöGAN„ÇíÊ≠£Â∏∏„Éá„Éº„Çø„ÅßË®ìÁ∑¥„Åó„ÄÅ„Åù„ÅÆÂæå‰∏äË®ò„ÅÆÈñ¢Êï∞„ÅßÁï∞Â∏∏Ê§úÁü•„ÇíË°å„ÅÜ
</code></pre>

<blockquote>
<p><strong>Ê≥®ÊÑè</strong>: AnoGAN„ÅØÊΩúÂú®Â§âÊï∞„ÅÆÊúÄÈÅ©Âåñ„Å´ÊôÇÈñì„Åå„Åã„Åã„Çã„Åü„ÇÅ„ÄÅ„É™„Ç¢„É´„Çø„Ç§„É†Áï∞Â∏∏Ê§úÁü•„Å´„ÅØ‰∏çÂêë„Åç„Åß„Åô„ÄÇ„Åì„ÅÆÂïèÈ°å„ÇíËß£Ê±∫„Åô„Çã„Åü„ÇÅ„Å´„ÄÅFast-AnoGAN„ÇÑEGBAd„Å™„Å©„ÅÆÊîπËâØÊâãÊ≥ï„ÅåÊèêÊ°à„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ</p>
</blockquote>

<hr>

<h2>4.4 ÊôÇÁ≥ªÂàóÁï∞Â∏∏Ê§úÁü•</h2>

<h3>4.4.1 ÊôÇÁ≥ªÂàó„Éá„Éº„Çø„ÅÆÁâπÂæ¥</h3>

<p>ÊôÇÁ≥ªÂàó„Éá„Éº„Çø„ÅÆÁï∞Â∏∏Ê§úÁü•„Åß„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆÁâπÊÄß„ÇíËÄÉÊÖÆ„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„ÅôÔºö</p>

<ul>
<li><strong>ÊôÇÈñìÁöÑ‰æùÂ≠òÊÄß</strong>: ÈÅéÂéª„ÅÆÂÄ§„ÅåÊú™Êù•„Å´ÂΩ±Èüø</li>
<li><strong>Â≠£ÁØÄÊÄß„ÉªÂë®ÊúüÊÄß</strong>: Êó•Ê¨°„ÄÅÈÄ±Ê¨°„ÄÅÂπ¥Ê¨°„Éë„Çø„Éº„É≥</li>
<li><strong>„Éà„É¨„É≥„Éâ</strong>: Èï∑ÊúüÁöÑ„Å™‰∏äÊòá„Éª‰∏ãÈôçÂÇæÂêë</li>
<li><strong>Â§öÂ§âÈáèÊÄß</strong>: Ë§áÊï∞„ÅÆ„Çª„É≥„Çµ„ÉºÂÄ§„ÅåÁõ∏‰∫í„Å´Èñ¢ÈÄ£</li>
</ul>

<h3>4.4.2 LSTM Autoencoder</h3>

<p><strong>LSTM Autoencoder</strong>„ÅØ„ÄÅLSTM„ÇíÁî®„ÅÑ„Å¶ÊôÇÁ≥ªÂàó„ÅÆÊôÇÈñìÁöÑ„Éë„Çø„Éº„É≥„ÇíÂ≠¶Áøí„Åó„ÄÅÂÜçÊßãÊàêË™§Â∑Æ„ÅßÁï∞Â∏∏„ÇíÊ§úÁü•„Åó„Åæ„Åô„ÄÇ</p>

<p><strong>„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£Ôºö</strong></p>

<pre><code>Input: (batch, seq_len, features)
    ‚Üì
LSTM Encoder: ÊôÇÁ≥ªÂàó„ÇíÂõ∫ÂÆöÈï∑„Éô„ÇØ„Éà„É´„Å´ÂúßÁ∏Æ
    ‚Üì
Latent Vector: (batch, latent_dim)
    ‚Üì
LSTM Decoder: ÊΩúÂú®„Éô„ÇØ„Éà„É´„Åã„ÇâÊôÇÁ≥ªÂàó„ÇíÂÜçÊßãÊàê
    ‚Üì
Output: (batch, seq_len, features)
</code></pre>

<h3>4.4.3 PyTorchÂÆüË£Ö</h3>

<pre><code>import torch
import torch.nn as nn

class LSTMAutoencoder(nn.Module):
    """LSTM-based Autoencoder for time series"""
    def __init__(self, input_dim, hidden_dim=64, num_layers=2, latent_dim=32):
        super(LSTMAutoencoder, self).__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.latent_dim = latent_dim

        # Encoder LSTM
        self.encoder_lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True
        )

        # ÊΩúÂú®Ë°®Áèæ„Å∏„ÅÆÂúßÁ∏Æ
        self.encoder_fc = nn.Linear(hidden_dim, latent_dim)

        # DecoderÁî®„ÅÆFCÔºàÊΩúÂú®Ë°®Áèæ„Åã„ÇâLSTMÂàùÊúüÁä∂ÊÖã„Å∏Ôºâ
        self.decoder_fc = nn.Linear(latent_dim, hidden_dim)

        # Decoder LSTM
        self.decoder_lstm = nn.LSTM(
            input_size=latent_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True
        )

        # Âá∫ÂäõÂ±§
        self.output_fc = nn.Linear(hidden_dim, input_dim)

    def encode(self, x):
        """„Ç®„É≥„Ç≥„Éº„Éâ: ÊôÇÁ≥ªÂàó ‚Üí ÊΩúÂú®„Éô„ÇØ„Éà„É´"""
        # x: (batch, seq_len, input_dim)
        lstm_out, (hidden, cell) = self.encoder_lstm(x)

        # ÊúÄÂæå„ÅÆÈö†„ÇåÁä∂ÊÖã„Çí‰ΩøÁî®
        last_hidden = hidden[-1]  # (batch, hidden_dim)

        # ÊΩúÂú®„Éô„ÇØ„Éà„É´„Å´ÂúßÁ∏Æ
        z = self.encoder_fc(last_hidden)  # (batch, latent_dim)

        return z

    def decode(self, z, seq_len):
        """„Éá„Ç≥„Éº„Éâ: ÊΩúÂú®„Éô„ÇØ„Éà„É´ ‚Üí ÊôÇÁ≥ªÂàó"""
        batch_size = z.size(0)

        # „Éá„Ç≥„Éº„ÉÄ„ÅÆLSTMÂàùÊúüÁä∂ÊÖã
        hidden = self.decoder_fc(z).unsqueeze(0)  # (1, batch, hidden_dim)
        hidden = hidden.repeat(self.num_layers, 1, 1)  # (num_layers, batch, hidden_dim)
        cell = torch.zeros_like(hidden)

        # „Éá„Ç≥„Éº„ÉÄ„ÅÆÂÖ•ÂäõÔºàÊΩúÂú®„Éô„ÇØ„Éà„É´„Çíseq_lenÂõûÁπ∞„ÇäËøî„ÅóÔºâ
        decoder_input = z.unsqueeze(1).repeat(1, seq_len, 1)  # (batch, seq_len, latent_dim)

        # LSTM Decoder
        lstm_out, _ = self.decoder_lstm(decoder_input, (hidden, cell))
        # lstm_out: (batch, seq_len, hidden_dim)

        # Âá∫ÂäõÂ±§
        output = self.output_fc(lstm_out)  # (batch, seq_len, input_dim)

        return output

    def forward(self, x):
        """È†Ü‰ºùÊí≠"""
        seq_len = x.size(1)

        z = self.encode(x)
        x_reconstructed = self.decode(z, seq_len)

        return x_reconstructed


def train_lstm_autoencoder(model, train_loader, n_epochs=50, lr=0.001):
    """LSTM Autoencoder„ÅÆË®ìÁ∑¥"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(n_epochs):
        model.train()
        epoch_loss = 0.0

        for batch_x, in train_loader:
            batch_x = batch_x.to(device)

            # Forward pass
            reconstructed = model(batch_x)
            loss = criterion(reconstructed, batch_x)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(train_loader)

        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{n_epochs}], Loss: {avg_loss:.6f}")

    return model


def detect_ts_anomalies(model, data_loader, threshold):
    """ÊôÇÁ≥ªÂàóÁï∞Â∏∏Ê§úÁü•"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.eval()

    all_errors = []

    with torch.no_grad():
        for batch_x, in data_loader:
            batch_x = batch_x.to(device)
            reconstructed = model(batch_x)

            # Á≥ªÂàóÂÖ®‰Ωì„ÅÆÂÜçÊßãÊàêË™§Â∑ÆÔºàÂπ≥ÂùáÔºâ
            errors = torch.mean((batch_x - reconstructed) ** 2, dim=(1, 2))
            all_errors.extend(errors.cpu().numpy())

    all_errors = np.array(all_errors)
    predictions = (all_errors > threshold).astype(int)

    return predictions, all_errors


# ‰ΩøÁî®‰æã
if __name__ == "__main__":
    # „Çµ„É≥„Éó„É´ÊôÇÁ≥ªÂàó„Éá„Éº„ÇøÁîüÊàêÔºàÊ≠£Â∏∏ÔºöÊ≠£Âº¶Ê≥¢„ÄÅÁï∞Â∏∏Ôºö„Éé„Ç§„Ç∫Ôºâ
    np.random.seed(42)
    torch.manual_seed(42)

    seq_len = 50
    input_dim = 5  # 5„Å§„ÅÆ„Çª„É≥„Çµ„Éº

    # Ê≠£Â∏∏„Éá„Éº„ÇøÔºàÊ≠£Âº¶Ê≥¢„Éô„Éº„ÇπÔºâ
    n_normal_sequences = 500
    t = np.linspace(0, 4*np.pi, seq_len)
    normal_sequences = []
    for _ in range(n_normal_sequences):
        seq = np.array([np.sin(t + np.random.randn() * 0.1) for _ in range(input_dim)]).T
        seq += np.random.randn(seq_len, input_dim) * 0.1
        normal_sequences.append(seq)

    normal_sequences = np.array(normal_sequences)  # (n_normal, seq_len, input_dim)

    # Áï∞Â∏∏„Éá„Éº„ÇøÔºà„É©„É≥„ÉÄ„É†„Éé„Ç§„Ç∫Ôºâ
    n_anomaly_sequences = 50
    anomaly_sequences = np.random.randn(n_anomaly_sequences, seq_len, input_dim)

    # Dataset
    train_dataset = TensorDataset(torch.FloatTensor(normal_sequences))
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

    test_sequences = np.vstack([normal_sequences[:50], anomaly_sequences])
    test_labels = np.array([0] * 50 + [1] * n_anomaly_sequences)

    test_dataset = TensorDataset(torch.FloatTensor(test_sequences))
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

    # „É¢„Éá„É´Ë®ìÁ∑¥
    print("=== LSTM AutoencoderË®ìÁ∑¥ÈñãÂßã ===")
    lstm_ae = LSTMAutoencoder(input_dim=input_dim, hidden_dim=64, num_layers=2, latent_dim=32)
    trained_lstm_ae = train_lstm_autoencoder(lstm_ae, train_loader, n_epochs=50, lr=0.001)

    # ThresholdË®≠ÂÆö
    train_errors = []
    trained_lstm_ae.eval()
    with torch.no_grad():
        for batch_x, in train_loader:
            reconstructed = trained_lstm_ae(batch_x)
            errors = torch.mean((batch_x - reconstructed) ** 2, dim=(1, 2))
            train_errors.extend(errors.cpu().numpy())

    threshold = np.percentile(train_errors, 95)
    print(f"\nÈñæÂÄ§Ôºà95%ÁÇπÔºâ: {threshold:.6f}")

    # Áï∞Â∏∏Ê§úÁü•
    predictions, test_errors = detect_ts_anomalies(trained_lstm_ae, test_loader, threshold)

    print("\n=== LSTM AutoencoderÁï∞Â∏∏Ê§úÁü•ÁµêÊûú ===")
    print(classification_report(test_labels, predictions,
                                target_names=['Normal', 'Anomaly']))
    print(f"ROC-AUC: {roc_auc_score(test_labels, test_errors):.3f}")
</code></pre>

<h3>4.4.4 Â§öÂ§âÈáèÊôÇÁ≥ªÂàóÁï∞Â∏∏Ê§úÁü•</h3>

<p>Ë§áÊï∞„ÅÆ„Çª„É≥„Çµ„Éº„Åã„Çâ„ÅÆ„Éá„Éº„Çø„ÇíÂêåÊôÇ„Å´Êâ±„ÅÜÂ†¥Âêà„ÄÅÂêÑÂ§âÊï∞Èñì„ÅÆÁõ∏Èñ¢Èñ¢‰øÇ„ÇÇËÄÉÊÖÆ„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ</p>

<p><strong>ÊâãÊ≥ïÔºö</strong></p>
<ul>
<li><strong>LSTM Autoencoder</strong>: ‰∏äË®ò„ÅÆÂÆüË£Ö„ÅßÂØæÂøúÊ∏à„Åø</li>
<li><strong>AttentionÊ©üÊßã</strong>: „Å©„ÅÆÂ§âÊï∞„ÅåÁï∞Â∏∏„Å´ÂØÑ‰∏é„Åó„Å¶„ÅÑ„Çã„Åã„ÇíËß£Èáà</li>
<li><strong>Transformer</strong>: Èï∑Êúü‰æùÂ≠òÈñ¢‰øÇ„ÅÆÂ≠¶Áøí</li>
</ul>

<hr>

<h2>4.5 „Ç®„É≥„Éâ„ÉÑ„Éº„Ç®„É≥„ÉâÂÆüË∑µ</h2>

<h3>4.5.1 „Éá„Éº„ÇøÊ∫ñÂÇô</h3>

<p>ÂÆü‰∏ñÁïå„ÅÆÁï∞Â∏∏Ê§úÁü•„Åß„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó„Åß„Éá„Éº„Çø„ÇíÊ∫ñÂÇô„Åó„Åæ„Åô„ÄÇ</p>

<pre><code>import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

class AnomalyDetectionPipeline:
    """Áï∞Â∏∏Ê§úÁü•„Éë„Ç§„Éó„É©„Ç§„É≥"""
    def __init__(self, model_type='autoencoder'):
        self.model_type = model_type
        self.scaler = StandardScaler()
        self.model = None
        self.threshold = None

    def preprocess(self, data, fit_scaler=False):
        """ÂâçÂá¶ÁêÜ: Ê≠£Ë¶èÂåñ„ÄÅÊ¨†ÊêçÂÄ§Âá¶ÁêÜ„Å™„Å©"""
        # Ê¨†ÊêçÂÄ§Ë£úÂÆåÔºàÂπ≥ÂùáÂÄ§Ôºâ
        data = data.fillna(data.mean())

        # Ê®ôÊ∫ñÂåñ
        if fit_scaler:
            data_scaled = self.scaler.fit_transform(data)
        else:
            data_scaled = self.scaler.transform(data)

        return data_scaled

    def create_sequences(self, data, seq_len=50):
        """ÊôÇÁ≥ªÂàó„Éá„Éº„Çø„Çí„Ç∑„Éº„Ç±„É≥„Çπ„Å´ÂàÜÂâ≤"""
        sequences = []
        for i in range(len(data) - seq_len + 1):
            sequences.append(data[i:i+seq_len])

        return np.array(sequences)

    def train(self, normal_data, seq_len=50, n_epochs=50):
        """„É¢„Éá„É´Ë®ìÁ∑¥"""
        # ÂâçÂá¶ÁêÜ
        normal_scaled = self.preprocess(normal_data, fit_scaler=True)

        # „Ç∑„Éº„Ç±„É≥„Çπ‰ΩúÊàê
        if self.model_type in ['lstm_ae', 'transformer']:
            sequences = self.create_sequences(normal_scaled, seq_len)
            train_dataset = TensorDataset(torch.FloatTensor(sequences))
        else:
            # Autoencoder„ÅÆÂ†¥Âêà
            train_dataset = TensorDataset(torch.FloatTensor(normal_scaled))

        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

        # „É¢„Éá„É´ÈÅ∏Êäû„Å®Ë®ìÁ∑¥
        if self.model_type == 'autoencoder':
            self.model = Autoencoder(input_dim=normal_scaled.shape[1])
            self.model, _ = train_autoencoder(self.model, train_loader, n_epochs)
        elif self.model_type == 'vae':
            self.model = VAE(input_dim=normal_scaled.shape[1])
            self.model = train_vae(self.model, train_loader, n_epochs)
        elif self.model_type == 'lstm_ae':
            self.model = LSTMAutoencoder(input_dim=normal_scaled.shape[1])
            self.model = train_lstm_autoencoder(self.model, train_loader, n_epochs)

        # ThresholdË®≠ÂÆöÔºàË®ìÁ∑¥„Éá„Éº„Çø„ÅÆ95%ÁÇπÔºâ
        if self.model_type == 'vae':
            scores = vae_anomaly_score(self.model, torch.FloatTensor(normal_scaled))
        else:
            scores = compute_reconstruction_errors(self.model, train_loader)

        self.threshold = np.percentile(scores, 95)
        print(f"ÈñæÂÄ§Ë®≠ÂÆö: {self.threshold:.6f}")

    def predict(self, test_data, seq_len=50):
        """Áï∞Â∏∏‰∫àÊ∏¨"""
        # ÂâçÂá¶ÁêÜ
        test_scaled = self.preprocess(test_data, fit_scaler=False)

        # „Ç∑„Éº„Ç±„É≥„Çπ‰ΩúÊàê
        if self.model_type in ['lstm_ae', 'transformer']:
            sequences = self.create_sequences(test_scaled, seq_len)
            test_dataset = TensorDataset(torch.FloatTensor(sequences))
        else:
            test_dataset = TensorDataset(torch.FloatTensor(test_scaled))

        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

        # Áï∞Â∏∏„Çπ„Ç≥„Ç¢Ë®àÁÆó
        if self.model_type == 'vae':
            scores = vae_anomaly_score(self.model, torch.FloatTensor(test_scaled))
        else:
            scores = compute_reconstruction_errors(self.model, test_loader)

        # Áï∞Â∏∏Âà§ÂÆö
        predictions = (scores > self.threshold).astype(int)

        return predictions, scores


# ‰ΩøÁî®‰æã
if __name__ == "__main__":
    # ‰ªÆ„ÅÆ„Éá„Éº„Çø„Éï„É¨„Éº„É†
    normal_df = pd.DataFrame(np.random.randn(1000, 10))
    test_df = pd.DataFrame(np.random.randn(100, 10))

    # „Éë„Ç§„Éó„É©„Ç§„É≥
    pipeline = AnomalyDetectionPipeline(model_type='autoencoder')
    pipeline.train(normal_df, n_epochs=30)

    predictions, scores = pipeline.predict(test_df)
    print(f"\nÁï∞Â∏∏Ê§úÂá∫Êï∞: {predictions.sum()} / {len(predictions)}")
</code></pre>

<h3>4.5.2 „É¢„Éá„É´ÈÅ∏Êäû</h3>

<p>„Éá„Éº„ÇøÁâπÊÄß„Å´Âøú„Åò„ÅüÈÅ©Âàá„Å™„É¢„Éá„É´„ÇíÈÅ∏Êäû„Åó„Åæ„Åô„ÄÇ</p>

<table>
<tr>
<th>„Éá„Éº„ÇøÁ®ÆÈ°û</th>
<th>Êé®Â•®„É¢„Éá„É´</th>
<th>ÁêÜÁî±</th>
</tr>
<tr>
<td>ÁîªÂÉè„Éá„Éº„Çø</td>
<td>Convolutional AE„ÄÅVAE</td>
<td>Á©∫ÈñìÊßãÈÄ†„Çí‰øùÊåÅ</td>
</tr>
<tr>
<td>ÊôÇÁ≥ªÂàó„Éá„Éº„Çø</td>
<td>LSTM AE„ÄÅTransformer</td>
<td>ÊôÇÈñìÁöÑ‰æùÂ≠òÊÄß„ÇíÊçâ„Åà„Çã</td>
</tr>
<tr>
<td>Ë°®ÂΩ¢Âºè„Éá„Éº„Çø</td>
<td>Autoencoder„ÄÅVAE</td>
<td>„Ç∑„É≥„Éó„É´„ÅßÂäπÊûúÁöÑ</td>
</tr>
<tr>
<td>È´òÊ¨°ÂÖÉ„Çπ„Éë„Éº„Çπ</td>
<td>Sparse AE„ÄÅVAE</td>
<td>Ê¨°ÂÖÉÂâäÊ∏õ„Å®Ê≠£ÂâáÂåñ</td>
</tr>
</table>

<h3>4.5.3 ThresholdË™øÊï¥</h3>

<p>ÂÆüÈÅãÁî®„Åß„ÅØ„ÄÅÊ•≠ÂãôË¶Å‰ª∂„Å´Âøú„Åò„Å¶Threshold„ÇíË™øÊï¥„Åó„Åæ„Åô„ÄÇ</p>

<ul>
<li><strong>False PositiveÁéá„ÇíÈáçË¶ñ</strong>: Threshold„ÇíÈ´ò„ÅèË®≠ÂÆöÔºàË™§Ê§úÁü•„ÇíÊ∏õ„Çâ„ÅôÔºâ</li>
<li><strong>RecallÈáçË¶ñ</strong>: Threshold„Çí‰Ωé„ÅèË®≠ÂÆöÔºàÁï∞Â∏∏„ÇíË¶ãÈÄÉ„Åï„Å™„ÅÑÔºâ</li>
<li><strong>F1ÊúÄÂ§ßÂåñ</strong>: Ê§úË®º„Éá„Éº„Çø„ÅßF1„Çπ„Ç≥„Ç¢„ÅåÊúÄÂ§ß„Å®„Å™„ÇãÁÇπ„ÇíÈÅ∏Êäû</li>
</ul>

<h3>4.5.4 Production Deployment</h3>

<p><strong>„É™„Ç¢„É´„Çø„Ç§„É†Áï∞Â∏∏Ê§úÁü•„Ç∑„Çπ„ÉÜ„É†„ÅÆÊßãÊàêÔºö</strong></p>

<pre><code>„Éá„Éº„ÇøÂèéÈõÜÔºà„Çª„É≥„Çµ„Éº„ÄÅ„É≠„Ç∞Ôºâ
    ‚Üì
ÂâçÂá¶ÁêÜ„Éë„Ç§„Éó„É©„Ç§„É≥ÔºàÊ≠£Ë¶èÂåñ„ÄÅ„Ç∑„Éº„Ç±„É≥„ÇπÂåñÔºâ
    ‚Üì
Áï∞Â∏∏Ê§úÁü•„É¢„Éá„É´ÔºàPyTorch ‚Üí ONNX ‚Üí TorchScriptÔºâ
    ‚Üì
ThresholdÂà§ÂÆö
    ‚Üì
„Ç¢„É©„Éº„Éà„ÉªÂèØË¶ñÂåñÔºàGrafana„ÄÅSlackÈÄöÁü•Ôºâ
</code></pre>

<p><strong>„Éá„Éó„É≠„Ç§„É°„É≥„Éà„ÅÆ„Éù„Ç§„É≥„ÉàÔºö</strong></p>
<ul>
<li><strong>„É¢„Éá„É´„ÅÆËªΩÈáèÂåñ</strong>: TorchScript„ÄÅONNXÂ§âÊèõ„ÅßÊé®Ë´ñÈ´òÈÄüÂåñ</li>
<li><strong>„Éê„ÉÉ„ÉÅÂá¶ÁêÜ</strong>: „É™„Ç¢„É´„Çø„Ç§„É†ÊÄß„ÅåÊ±Ç„ÇÅ„Çâ„Çå„Å™„ÅÑÂ†¥Âêà„ÅØ„Éê„ÉÉ„ÉÅ„ÅßÂäπÁéáÂåñ</li>
<li><strong>ÂÆöÊúüÁöÑ„Å™ÂÜçÂ≠¶Áøí</strong>: „Éá„Éº„ÇøÂàÜÂ∏É„ÅÆÂ§âÂåñ„Å´ÂØæÂøúÔºàConcept DriftÔºâ</li>
<li><strong>ÈñæÂÄ§„ÅÆËá™ÂãïË™øÊï¥</strong>: ÈÅãÁî®„Éá„Éº„Çø„Åã„ÇâÈÅ©ÂøúÁöÑ„Å´Ë™øÊï¥</li>
</ul>

<h3>4.5.5 „É¢„Éã„Çø„É™„É≥„Ç∞„Å®„Ç¢„É©„Éº„Éà</h3>

<pre><code>import logging
from datetime import datetime

class AnomalyMonitor:
    """Áï∞Â∏∏Ê§úÁü•„É¢„Éã„Çø„É™„É≥„Ç∞"""
    def __init__(self, alert_threshold=0.9):
        self.alert_threshold = alert_threshold
        self.logger = self._setup_logger()

    def _setup_logger(self):
        logger = logging.getLogger('AnomalyDetection')
        logger.setLevel(logging.INFO)

        # „Éï„Ç°„Ç§„É´„Éè„É≥„Éâ„É©
        fh = logging.FileHandler('anomaly_detection.log')
        fh.setLevel(logging.INFO)

        # „Éï„Ç©„Éº„Éû„ÉÉ„Éà
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        fh.setFormatter(formatter)

        logger.addHandler(fh)
        return logger

    def log_anomaly(self, timestamp, anomaly_score, features):
        """Áï∞Â∏∏„Çí„É≠„Ç∞„Å´Ë®òÈå≤"""
        self.logger.info(f"Anomaly detected - Time: {timestamp}, Score: {anomaly_score:.4f}")
        self.logger.info(f"Features: {features}")

    def send_alert(self, anomaly_score, message):
        """„Ç¢„É©„Éº„ÉàÈÄÅ‰ø°ÔºàÂÆüË£Ö‰æãÔºâ"""
        if anomaly_score > self.alert_threshold:
            # Slack„ÄÅEmail„ÄÅPagerDuty„Å™„Å©„Å´ÈÄöÁü•
            print(f"[ALERT] High anomaly detected: {message}")
            self.logger.warning(f"High severity alert: {message}")

    def monitor(self, pipeline, data_stream):
        """„É™„Ç¢„É´„Çø„Ç§„É†„É¢„Éã„Çø„É™„É≥„Ç∞"""
        for timestamp, data in data_stream:
            predictions, scores = pipeline.predict(data)

            if predictions.any():
                self.log_anomaly(timestamp, scores.max(), data)
                self.send_alert(scores.max(), f"Anomaly at {timestamp}")


# ‰ΩøÁî®‰æãÔºà‰ªÆÊÉ≥„Éá„Éº„Çø„Çπ„Éà„É™„Éº„É†Ôºâ
monitor = AnomalyMonitor(alert_threshold=0.9)

# ‰ªÆÊÉ≥„Éá„Éº„Çø„Çπ„Éà„É™„Éº„É†
def data_stream_generator():
    for i in range(10):
        timestamp = datetime.now()
        data = pd.DataFrame(np.random.randn(1, 10))
        yield timestamp, data

# „É¢„Éã„Çø„É™„É≥„Ç∞ÂÆüË°å
# monitor.monitor(pipeline, data_stream_generator())
</code></pre>

<hr>

<h2>„Åæ„Å®„ÇÅ</h2>

<p>Êú¨Á´†„ÅßÂ≠¶„Çì„Å†„Åì„Å®Ôºö</p>

<ol>
<li><strong>Autoencoder„Å´„Çà„ÇãÁï∞Â∏∏Ê§úÁü•:</strong>
<ul>
<li>ÂÜçÊßãÊàêË™§Â∑Æ„ÅßÁï∞Â∏∏„ÇíÊ§úÂá∫</li>
<li>„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÅÆË®≠Ë®à</li>
<li>ThresholdÈÅ∏ÊäûÊâãÊ≥ï</li>
<li>PyTorch„Åß„ÅÆÂÆåÂÖ®ÂÆüË£Ö</li>
</ul>
</li>

<li><strong>Variational Autoencoder (VAE):</strong>
<ul>
<li>Á¢∫ÁéáÁöÑÊΩúÂú®Ë°®Áèæ„Å´„Çà„ÇãÁï∞Â∏∏Ê§úÁü•</li>
<li>ÂÜçÊßãÊàêË™§Â∑Æ + KL„ÉÄ„Ç§„Éê„Éº„Ç∏„Çß„É≥„Çπ</li>
<li>ÊΩúÂú®Á©∫Èñì„ÅÆÂèØË¶ñÂåñ„Å®ÂàÜÊûê</li>
<li>Œ≤-VAE„Å´„Çà„ÇãË™øÊï¥</li>
</ul>
</li>

<li><strong>GAN-basedÁï∞Â∏∏Ê§úÁü•:</strong>
<ul>
<li>AnoGAN„ÅÆÂéüÁêÜ„Å®ÂÆüË£Ö</li>
<li>ÊΩúÂú®Â§âÊï∞„ÅÆÊúÄÈÅ©Âåñ</li>
<li>Discriminator„ÅÆÁâπÂæ¥„ÇíÂà©Áî®</li>
<li>Fast-AnoGAN„Å™„Å©„ÅÆÊîπËâØÊâãÊ≥ï</li>
</ul>
</li>

<li><strong>ÊôÇÁ≥ªÂàóÁï∞Â∏∏Ê§úÁü•:</strong>
<ul>
<li>LSTM Autoencoder„ÅÆÂÆüË£Ö</li>
<li>ÊôÇÈñìÁöÑ„Éë„Çø„Éº„É≥„ÅÆÂ≠¶Áøí</li>
<li>Â§öÂ§âÈáèÊôÇÁ≥ªÂàó„Å∏„ÅÆÂØæÂøú</li>
<li>„Ç∑„Éº„Ç±„É≥„Çπ„Éá„Éº„Çø„ÅÆÂá¶ÁêÜ</li>
</ul>
</li>

<li><strong>„Ç®„É≥„Éâ„ÉÑ„Éº„Ç®„É≥„ÉâÂÆüË∑µ:</strong>
<ul>
<li>„Éá„Éº„ÇøÂâçÂá¶ÁêÜ„Éë„Ç§„Éó„É©„Ç§„É≥</li>
<li>„É¢„Éá„É´ÈÅ∏Êäû„ÅÆÊåáÈáù</li>
<li>ThresholdË™øÊï¥ÊâãÊ≥ï</li>
<li>Production deployment„ÅÆË®≠Ë®à</li>
<li>„É¢„Éã„Çø„É™„É≥„Ç∞„Å®„Ç¢„É©„Éº„Éà</li>
</ul>
</li>
</ol>

<hr>

<h2>ÊºîÁøíÂïèÈ°å</h2>

<p><strong>Âïè1:</strong> Autoencoder„Å´„Çà„ÇãÁï∞Â∏∏Ê§úÁü•„Åß„ÄÅÊΩúÂú®Ê¨°ÂÖÉ„ÇíÂÖ•ÂäõÊ¨°ÂÖÉ„ÅÆ10%„Å´Ë®≠ÂÆö„Åô„ÇãÁêÜÁî±„ÇíË™¨Êòé„Åõ„Çà„ÄÇ</p>

<p><strong>Âïè2:</strong> VAE„ÅÆÊêçÂ§±Èñ¢Êï∞„Å´„Åä„Åë„ÇãKL„ÉÄ„Ç§„Éê„Éº„Ç∏„Çß„É≥„ÇπÈ†Ö„ÅÆÂΩπÂâ≤„Çí„ÄÅÊΩúÂú®Á©∫Èñì„ÅÆË¶≥ÁÇπ„Åã„ÇâË™¨Êòé„Åõ„Çà„ÄÇ</p>

<p><strong>Âïè3:</strong> AnoGAN„Å®ÈÄöÂ∏∏„ÅÆAutoencoder„ÅÆÁï∞Â∏∏Ê§úÁü•„Å´„Åä„Åë„Çã‰∏ª„Å™ÈÅï„ÅÑ„Çí3„Å§Êåô„Åí„Çà„ÄÇ</p>

<p><strong>Âïè4:</strong> LSTM Autoencoder„ÅßÊôÇÁ≥ªÂàóÁï∞Â∏∏Ê§úÁü•„ÇíË°å„ÅÜÈöõ„ÄÅ„Ç∑„Éº„Ç±„É≥„ÇπÈï∑„Çí„Å©„ÅÆ„Çà„ÅÜ„Å´Ê±∫ÂÆö„Åô„Åπ„Åç„Åã„ÄÅ3„Å§„ÅÆË¶≥ÁÇπ„Åã„ÇâË´ñ„Åò„Çà„ÄÇ</p>

<p><strong>Âïè5:</strong> Áï∞Â∏∏Ê§úÁü•„É¢„Éá„É´„ÅÆThresholdË®≠ÂÆö„Å´„Åä„ÅÑ„Å¶„ÄÅFalse PositiveÁéá„Åå5%‰ª•‰∏ã„Å®„ÅÑ„ÅÜÊ•≠ÂãôË¶Å‰ª∂„Åå„ÅÇ„ÇãÂ†¥Âêà„ÄÅ„Å©„ÅÆ„Çà„ÅÜ„Å´Threshold„ÇíÊ±∫ÂÆö„Åô„Åπ„Åç„ÅãÂÖ∑‰ΩìÁöÑ„Å´Ë™¨Êòé„Åõ„Çà„ÄÇ</p>

<p><strong>Âïè6:</strong> „É™„Ç¢„É´„Çø„Ç§„É†Áï∞Â∏∏Ê§úÁü•„Ç∑„Çπ„ÉÜ„É†„ÇíÊßãÁØâ„Åô„ÇãÈöõ„ÄÅËÄÉÊÖÆ„Åô„Åπ„ÅçÊäÄË°ìÁöÑË™≤È°å„Çí5„Å§Êåô„Åí„ÄÅ„Åù„Çå„Åû„Çå„ÅÆÂØæÂá¶Ê≥ï„ÇíÊèêÊ°à„Åõ„Çà„ÄÇ</p>

<hr>

<h2>ÂèÇËÄÉÊñáÁåÆ</h2>

<ol>
<li>Goodfellow, I. et al. "Deep Learning." MIT Press (2016).</li>
<li>Kingma, D. P., & Welling, M. "Auto-Encoding Variational Bayes." <em>ICLR</em> (2014).</li>
<li>Schlegl, T. et al. "Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery." <em>IPMI</em> (2017). [AnoGAN]</li>
<li>Malhotra, P. et al. "LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection." <em>ICML Anomaly Detection Workshop</em> (2016).</li>
<li>Park, D. et al. "A Multimodal Anomaly Detector for Robot-Assisted Feeding Using an LSTM-based Variational Autoencoder." <em>IEEE Robotics and Automation Letters</em> (2018).</li>
<li>Su, Y. et al. "Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network." <em>KDD</em> (2019).</li>
<li>Vaswani, A. et al. "Attention is All You Need." <em>NeurIPS</em> (2017). [Transformer]</li>
<li>Audibert, J. et al. "USAD: UnSupervised Anomaly Detection on Multivariate Time Series." <em>KDD</em> (2020).</li>
</ol>

<hr>

<div class="navigation">
    <a href="chapter3-ml-based-anomaly.html" class="nav-button">‚Üê Ââç„ÅÆÁ´†</a>
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
</div>

    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-21</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
