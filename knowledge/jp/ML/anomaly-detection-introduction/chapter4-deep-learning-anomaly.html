<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 4 - æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/anomaly-detection-introduction/index.html">Anomaly Detection</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 4</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬4ç« ï¼šæ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥</h1>
            <p class="subtitle">Autoencoderã€VAEã€GANã€æ™‚ç³»åˆ—ç•°å¸¸æ¤œçŸ¥</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 80-90åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 9å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 6å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<div class="learning-objectives">
<h2>å­¦ç¿’ç›®æ¨™</h2>
<ul>
<li>Autoencoderã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥ã®åŸç†ã¨å®Ÿè£…ã‚’ç†è§£ã™ã‚‹</li>
<li>Variational Autoencoder (VAE) ã®ç¢ºç‡çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å­¦ã¶</li>
<li>GAN-basedç•°å¸¸æ¤œçŸ¥ï¼ˆAnoGANï¼‰ã®ä»•çµ„ã¿ã‚’ç†è§£ã™ã‚‹</li>
<li>LSTM Autoencoderã§æ™‚ç³»åˆ—ç•°å¸¸æ¤œçŸ¥ã‚’å®Ÿè£…ã™ã‚‹</li>
<li>ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ç•°å¸¸æ¤œçŸ¥ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
</ul>
</div>

<h2>4.1 Autoencoderã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥</h2>

<h3>4.1.1 Autoencoderã®åŸºç¤</h3>

<p><strong>Autoencoderï¼ˆè‡ªå·±ç¬¦å·åŒ–å™¨ï¼‰</strong>ã¯ã€å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’åœ§ç¸®ã—ã€å†æ§‹æˆã™ã‚‹æ•™å¸«ãªã—å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã™ã‚‹ã“ã¨ã§ã€ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã®å†æ§‹æˆèª¤å·®ãŒå¤§ãããªã‚‹ã“ã¨ã‚’åˆ©ç”¨ã—ã¦ç•°å¸¸æ¤œçŸ¥ã‚’è¡Œã„ã¾ã™ã€‚</p>

<p><strong>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼š</strong></p>

<pre><code>Input (x)
    â†“
Encoder: x â†’ z (æ½œåœ¨è¡¨ç¾)
    â†“
Latent Space (z)
    â†“
Decoder: z â†’ xÌ‚ (å†æ§‹æˆ)
    â†“
Reconstruction Error: ||x - xÌ‚||Â²
</code></pre>

<p><strong>ç•°å¸¸æ¤œçŸ¥ã®åŸç†ï¼š</strong></p>

<ul>
<li>æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´: æ­£å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’</li>
<li>å†æ§‹æˆèª¤å·®ãŒå°ã•ã„: æ­£å¸¸ãƒ‡ãƒ¼ã‚¿</li>
<li>å†æ§‹æˆèª¤å·®ãŒå¤§ãã„: ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’ã—ã¦ã„ãªã„ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰</li>
</ul>

<p><strong>æ•°å¼è¡¨ç¾ï¼š</strong></p>

<p>$$
\text{Anomaly Score} = \|x - \text{Decoder}(\text{Encoder}(x))\|^2
$$</p>

<h3>4.1.2 å†æ§‹æˆèª¤å·®ã¨Thresholdé¸æŠ</h3>

<p>ç•°å¸¸åˆ¤å®šã«ã¯ã€å†æ§‹æˆèª¤å·®ã«å¯¾ã™ã‚‹thresholdã‚’è¨­å®šã—ã¾ã™ã€‚</p>

<p><strong>Thresholdè¨­å®šæ‰‹æ³•ï¼š</strong></p>

<table>
<tr>
<th>æ‰‹æ³•</th>
<th>èª¬æ˜</th>
<th>é©ç”¨å ´é¢</th>
</tr>
<tr>
<td>ç™¾åˆ†ä½æ•°æ³•</td>
<td>è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å†æ§‹æˆèª¤å·®ã®95%ç‚¹</td>
<td>æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§å­¦ç¿’</td>
</tr>
<tr>
<td>çµ±è¨ˆçš„æ‰‹æ³•</td>
<td>å¹³å‡ + 3Ïƒ</td>
<td>æ­£è¦åˆ†å¸ƒã‚’ä»®å®š</td>
</tr>
<tr>
<td>ROCæ›²ç·š</td>
<td>æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§AUCæœ€å¤§åŒ–</td>
<td>å°‘é‡ã®ç•°å¸¸ãƒ©ãƒ™ãƒ«ã‚ã‚Š</td>
</tr>
<tr>
<td>æ¥­å‹™è¦ä»¶</td>
<td>False Positiveç‡ã‚’æŒ‡å®š</td>
<td>å®Ÿé‹ç”¨é‡è¦–</td>
</tr>
</table>

<h3>4.1.3 PyTorchå®Ÿè£…ï¼ˆå®Œå…¨ç‰ˆï¼‰</h3>

<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, precision_recall_curve

# Autoencoderãƒ¢ãƒ‡ãƒ«å®šç¾©
class Autoencoder(nn.Module):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªAutoencoder"""
    def __init__(self, input_dim=784, hidden_dims=[128, 64, 32]):
        super(Autoencoder, self).__init__()

        # Encoder
        encoder_layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            encoder_layers.append(nn.Linear(prev_dim, hidden_dim))
            encoder_layers.append(nn.ReLU())
            prev_dim = hidden_dim

        self.encoder = nn.Sequential(*encoder_layers)

        # Decoderï¼ˆEncoderã®é€†é †ï¼‰
        decoder_layers = []
        for i in range(len(hidden_dims) - 1, 0, -1):
            decoder_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i-1]))
            decoder_layers.append(nn.ReLU())

        decoder_layers.append(nn.Linear(hidden_dims[0], input_dim))
        decoder_layers.append(nn.Sigmoid())  # å‡ºåŠ›ã‚’[0,1]ã«æ­£è¦åŒ–

        self.decoder = nn.Sequential(*decoder_layers)

    def forward(self, x):
        """é †ä¼æ’­"""
        z = self.encoder(x)  # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        x_reconstructed = self.decoder(z)  # ãƒ‡ã‚³ãƒ¼ãƒ‰
        return x_reconstructed

    def encode(self, x):
        """æ½œåœ¨è¡¨ç¾ã®å–å¾—"""
        return self.encoder(x)


def train_autoencoder(model, train_loader, n_epochs=50, lr=0.001):
    """Autoencoderã®è¨“ç·´"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    criterion = nn.MSELoss()  # å†æ§‹æˆèª¤å·®ï¼ˆMean Squared Errorï¼‰
    optimizer = optim.Adam(model.parameters(), lr=lr)

    train_losses = []

    for epoch in range(n_epochs):
        model.train()
        epoch_loss = 0.0

        for batch_x, in train_loader:
            batch_x = batch_x.to(device)

            # Forward pass
            reconstructed = model(batch_x)
            loss = criterion(reconstructed, batch_x)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(train_loader)
        train_losses.append(avg_loss)

        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{n_epochs}], Loss: {avg_loss:.6f}")

    return model, train_losses


def compute_reconstruction_errors(model, data_loader):
    """å†æ§‹æˆèª¤å·®ã®è¨ˆç®—"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.eval()

    errors = []

    with torch.no_grad():
        for batch_x, in data_loader:
            batch_x = batch_x.to(device)
            reconstructed = model(batch_x)

            # ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã®å†æ§‹æˆèª¤å·®ï¼ˆMSEï¼‰
            batch_errors = torch.mean((batch_x - reconstructed) ** 2, dim=1)
            errors.extend(batch_errors.cpu().numpy())

    return np.array(errors)


def detect_anomalies(model, test_loader, threshold):
    """ç•°å¸¸æ¤œçŸ¥ã®å®Ÿè¡Œ"""
    errors = compute_reconstruction_errors(model, test_loader)
    predictions = (errors > threshold).astype(int)
    return predictions, errors


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆæ­£è¦åˆ†å¸ƒã®æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ï¼‰
    np.random.seed(42)
    torch.manual_seed(42)

    # æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ï¼ˆ28x28 = 784æ¬¡å…ƒï¼‰
    n_normal = 1000
    normal_data = np.random.randn(n_normal, 784) * 0.5 + 0.5
    normal_data = np.clip(normal_data, 0, 1)

    # ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ï¼ˆæ­£å¸¸ã¨ã¯ç•°ãªã‚‹åˆ†å¸ƒï¼‰
    n_anomaly = 50
    anomaly_data = np.random.uniform(0, 1, (n_anomaly, 784))

    # PyTorch Dataset
    train_dataset = TensorDataset(torch.FloatTensor(normal_data))
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

    test_data = np.vstack([normal_data[:100], anomaly_data])
    test_labels = np.array([0] * 100 + [1] * n_anomaly)  # 0: æ­£å¸¸, 1: ç•°å¸¸

    test_dataset = TensorDataset(torch.FloatTensor(test_data))
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    print("=== Autoencoderè¨“ç·´é–‹å§‹ ===")
    model = Autoencoder(input_dim=784, hidden_dims=[256, 128, 64])
    trained_model, losses = train_autoencoder(model, train_loader, n_epochs=50, lr=0.001)

    # Thresholdè¨­å®šï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®95%ç‚¹ï¼‰
    train_errors = compute_reconstruction_errors(trained_model, train_loader)
    threshold = np.percentile(train_errors, 95)
    print(f"\né–¾å€¤ï¼ˆ95%ç‚¹ï¼‰: {threshold:.6f}")

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ç•°å¸¸æ¤œçŸ¥
    predictions, test_errors = detect_anomalies(trained_model, test_loader, threshold)

    # è©•ä¾¡
    from sklearn.metrics import classification_report, roc_auc_score

    print("\n=== ç•°å¸¸æ¤œçŸ¥çµæœ ===")
    print(classification_report(test_labels, predictions,
                                target_names=['Normal', 'Anomaly']))

    auc_score = roc_auc_score(test_labels, test_errors)
    print(f"ROC-AUC: {auc_score:.3f}")

    # å¯è¦–åŒ–
    plt.figure(figsize=(12, 4))

    # å­¦ç¿’æ›²ç·š
    plt.subplot(1, 2, 1)
    plt.plot(losses)
    plt.xlabel('Epoch')
    plt.ylabel('Reconstruction Loss')
    plt.title('Training Loss Curve')
    plt.grid(True)

    # å†æ§‹æˆèª¤å·®åˆ†å¸ƒ
    plt.subplot(1, 2, 2)
    plt.hist(test_errors[test_labels == 0], bins=30, alpha=0.6, label='Normal')
    plt.hist(test_errors[test_labels == 1], bins=30, alpha=0.6, label='Anomaly')
    plt.axvline(threshold, color='r', linestyle='--', label=f'Threshold={threshold:.3f}')
    plt.xlabel('Reconstruction Error')
    plt.ylabel('Frequency')
    plt.title('Reconstruction Error Distribution')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig('autoencoder_anomaly_detection.png', dpi=150)
    print("\nã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: autoencoder_anomaly_detection.png")
</code></pre>

<h3>4.1.4 ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é¸æŠ</h3>

<p><strong>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆã®ãƒã‚¤ãƒ³ãƒˆï¼š</strong></p>

<table>
<tr>
<th>è¦ç´ </th>
<th>æ¨å¥¨å€¤</th>
<th>ç†ç”±</th>
</tr>
<tr>
<td>æ½œåœ¨æ¬¡å…ƒ</td>
<td>å…¥åŠ›æ¬¡å…ƒã®10-30%</td>
<td>éåº¦ãªåœ§ç¸®ã¯æƒ…å ±æå¤±ã€å¤§ãã™ãã‚‹ã¨æ’ç­‰å†™åƒ</td>
</tr>
<tr>
<td>éš ã‚Œå±¤æ•°</td>
<td>2-4å±¤</td>
<td>æ·±ã™ãã‚‹ã¨è¨“ç·´å›°é›£ã€æµ…ã™ãã‚‹ã¨è¡¨ç¾åŠ›ä¸è¶³</td>
</tr>
<tr>
<td>æ´»æ€§åŒ–é–¢æ•°</td>
<td>ReLUï¼ˆéš ã‚Œå±¤ï¼‰ã€Sigmoidï¼ˆå‡ºåŠ›ï¼‰</td>
<td>å‹¾é…æ¶ˆå¤±ã‚’é˜²ãã€å‡ºåŠ›ç¯„å›²ã‚’åˆ¶é™</td>
</tr>
<tr>
<td>Dropout</td>
<td>0.2-0.3</td>
<td>éå­¦ç¿’é˜²æ­¢ï¼ˆãŸã ã—ç•°å¸¸æ¤œçŸ¥ã§ã¯æ…é‡ã«ï¼‰</td>
</tr>
</table>

<hr>

<h2>4.2 Variational Autoencoder (VAE)</h2>

<h3>4.2.1 VAEã®å‹•æ©Ÿ</h3>

<p><strong>é€šå¸¸ã®Autoencoderã®èª²é¡Œï¼š</strong></p>
<ul>
<li>æ½œåœ¨ç©ºé–“ãŒä¸é€£ç¶šã§ã€æ„å‘³ã®ã‚ã‚‹æ§‹é€ ã‚’æŒãŸãªã„</li>
<li>å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«éå‰°é©åˆã—ã‚„ã™ã„</li>
<li>ç”Ÿæˆèƒ½åŠ›ãŒé™å®šçš„</li>
</ul>

<p><strong>VAEã®ç‰¹å¾´ï¼š</strong></p>
<ul>
<li>æ½œåœ¨å¤‰æ•°ã‚’ç¢ºç‡åˆ†å¸ƒã¨ã—ã¦ãƒ¢ãƒ‡ãƒ«åŒ–</li>
<li>æ­£å‰‡åŒ–ã«ã‚ˆã‚Šæ»‘ã‚‰ã‹ãªæ½œåœ¨ç©ºé–“ã‚’å­¦ç¿’</li>
<li>ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã‚‚æ©Ÿèƒ½</li>
</ul>

<h3>4.2.2 VAEã®æ•°ç†</h3>

<p><strong>ç¢ºç‡çš„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼š</strong></p>

<p>$$
q_\phi(z|x) = \mathcal{N}(z; \mu(x), \sigma^2(x))
$$</p>

<p>ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯å¹³å‡$\mu(x)$ã¨åˆ†æ•£$\sigma^2(x)$ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚</p>

<p><strong>ãƒ‡ã‚³ãƒ¼ãƒ€ï¼š</strong></p>

<p>$$
p_\theta(x|z) = \mathcal{N}(x; \mu_{\text{dec}}(z), \sigma^2_{\text{dec}})
$$</p>

<p><strong>æå¤±é–¢æ•°ï¼ˆELBOï¼‰ï¼š</strong></p>

<p>$$
\mathcal{L} = \underbrace{\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]}_{\text{Reconstruction Loss}} - \underbrace{D_{KL}(q_\phi(z|x) \| p(z))}_{\text{KL Divergence}}
$$</p>

<ul>
<li>ç¬¬1é …: å†æ§‹æˆæå¤±ï¼ˆAutoencoderã¨åŒã˜ï¼‰</li>
<li>ç¬¬2é …: KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ï¼ˆæ­£å‰‡åŒ–é …ï¼‰ã€$p(z) = \mathcal{N}(0, I)$ã‚’ä»®å®š</li>
</ul>

<p><strong>KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®è§£æè§£ï¼š</strong></p>

<p>$$
D_{KL} = -\frac{1}{2} \sum_{j=1}^{J} (1 + \log(\sigma_j^2) - \mu_j^2 - \sigma_j^2)
$$</p>

<h3>4.2.3 VAEã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥</h3>

<p>VAEã§ã¯ã€å†æ§‹æˆèª¤å·®ã¨KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’çµ„ã¿åˆã‚ã›ã¦ç•°å¸¸ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã¾ã™ã€‚</p>

<p>$$
\text{Anomaly Score} = \text{Reconstruction Error} + \beta \cdot D_{KL}
$$</p>

<h3>4.2.4 PyTorchå®Ÿè£…</h3>

<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

class VAE(nn.Module):
    """Variational Autoencoder"""
    def __init__(self, input_dim=784, latent_dim=32, hidden_dims=[256, 128]):
        super(VAE, self).__init__()

        self.latent_dim = latent_dim

        # Encoder
        encoder_layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            encoder_layers.append(nn.Linear(prev_dim, hidden_dim))
            encoder_layers.append(nn.ReLU())
            prev_dim = hidden_dim

        self.encoder = nn.Sequential(*encoder_layers)

        # æ½œåœ¨åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå¹³å‡ã¨åˆ†æ•£ï¼‰
        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)
        self.fc_logvar = nn.Linear(hidden_dims[-1], latent_dim)

        # Decoder
        decoder_layers = []
        decoder_layers.append(nn.Linear(latent_dim, hidden_dims[-1]))
        decoder_layers.append(nn.ReLU())

        for i in range(len(hidden_dims) - 1, 0, -1):
            decoder_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i-1]))
            decoder_layers.append(nn.ReLU())

        decoder_layers.append(nn.Linear(hidden_dims[0], input_dim))
        decoder_layers.append(nn.Sigmoid())

        self.decoder = nn.Sequential(*decoder_layers)

    def encode(self, x):
        """ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰: å¹³å‡ã¨å¯¾æ•°åˆ†æ•£ã‚’å‡ºåŠ›"""
        h = self.encoder(x)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        """å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯"""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)  # N(0, 1)ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        z = mu + eps * std
        return z

    def decode(self, z):
        """ãƒ‡ã‚³ãƒ¼ãƒ‰"""
        return self.decoder(z)

    def forward(self, x):
        """é †ä¼æ’­"""
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        x_reconstructed = self.decode(z)
        return x_reconstructed, mu, logvar


def vae_loss(x, x_reconstructed, mu, logvar, beta=1.0):
    """VAEæå¤±é–¢æ•°

    Args:
        beta: KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®é‡ã¿ï¼ˆÎ²-VAEï¼‰
    """
    # Reconstruction lossï¼ˆãƒã‚¤ãƒŠãƒªã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰
    recon_loss = F.binary_cross_entropy(x_reconstructed, x, reduction='sum')

    # KL Divergence
    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

    # Total loss
    total_loss = recon_loss + beta * kl_div

    return total_loss, recon_loss, kl_div


def train_vae(model, train_loader, n_epochs=50, lr=0.001, beta=1.0):
    """VAEã®è¨“ç·´"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(n_epochs):
        model.train()
        train_loss = 0.0

        for batch_x, in train_loader:
            batch_x = batch_x.to(device)

            # Forward pass
            x_recon, mu, logvar = model(batch_x)
            loss, recon, kl = vae_loss(batch_x, x_recon, mu, logvar, beta)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        avg_loss = train_loss / len(train_loader.dataset)

        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{n_epochs}], Loss: {avg_loss:.4f}")

    return model


def vae_anomaly_score(model, x, beta=1.0):
    """VAEã«ã‚ˆã‚‹ç•°å¸¸ã‚¹ã‚³ã‚¢è¨ˆç®—"""
    model.eval()
    device = next(model.parameters()).device

    with torch.no_grad():
        x = x.to(device)
        x_recon, mu, logvar = model(x)

        # Reconstruction errorï¼ˆã‚µãƒ³ãƒ—ãƒ«ã”ã¨ï¼‰
        recon_error = F.binary_cross_entropy(x_recon, x, reduction='none').sum(dim=1)

        # KL divergenceï¼ˆã‚µãƒ³ãƒ—ãƒ«ã”ã¨ï¼‰
        kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)

        # Anomaly score
        anomaly_scores = recon_error + beta * kl_div

    return anomaly_scores.cpu().numpy()


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆå‰è¿°ã¨åŒã˜ï¼‰
    train_dataset = TensorDataset(torch.FloatTensor(normal_data))
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

    # VAEãƒ¢ãƒ‡ãƒ«
    print("=== VAEè¨“ç·´é–‹å§‹ ===")
    vae_model = VAE(input_dim=784, latent_dim=32, hidden_dims=[256, 128])
    trained_vae = train_vae(vae_model, train_loader, n_epochs=50, lr=0.001, beta=1.0)

    # ç•°å¸¸ã‚¹ã‚³ã‚¢è¨ˆç®—
    test_tensor = torch.FloatTensor(test_data)
    anomaly_scores = vae_anomaly_score(trained_vae, test_tensor, beta=1.0)

    # Thresholdè¨­å®šã¨è©•ä¾¡
    threshold = np.percentile(anomaly_scores[:100], 95)  # æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®95%ç‚¹
    predictions = (anomaly_scores > threshold).astype(int)

    print("\n=== VAEç•°å¸¸æ¤œçŸ¥çµæœ ===")
    print(classification_report(test_labels, predictions,
                                target_names=['Normal', 'Anomaly']))
    print(f"ROC-AUC: {roc_auc_score(test_labels, anomaly_scores):.3f}")
</code></pre>

<h3>4.2.5 æ½œåœ¨ç©ºé–“ã®åˆ†æ</h3>

<p>VAEã®æ½œåœ¨ç©ºé–“ã¯ã€æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ãŒæ»‘ã‚‰ã‹ã«åˆ†å¸ƒã—ã¦ã„ã¾ã™ã€‚ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã¯æ½œåœ¨ç©ºé–“ã®å¤–ã‚ŒãŸé ˜åŸŸã«é…ç½®ã•ã‚Œã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚</p>

<pre><code>import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

def visualize_latent_space(model, data, labels):
    """æ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ–ï¼ˆ2æ¬¡å…ƒæŠ•å½±ï¼‰"""
    model.eval()
    device = next(model.parameters()).device

    with torch.no_grad():
        data_tensor = torch.FloatTensor(data).to(device)
        mu, _ = model.encode(data_tensor)
        latent_codes = mu.cpu().numpy()

    # 2æ¬¡å…ƒã«åœ§ç¸®ï¼ˆæ½œåœ¨æ¬¡å…ƒãŒ2ã‚ˆã‚Šå¤§ãã„å ´åˆï¼‰
    if latent_codes.shape[1] > 2:
        pca = PCA(n_components=2)
        latent_2d = pca.fit_transform(latent_codes)
    else:
        latent_2d = latent_codes

    # ãƒ—ãƒ­ãƒƒãƒˆ
    plt.figure(figsize=(8, 6))
    plt.scatter(latent_2d[labels == 0, 0], latent_2d[labels == 0, 1],
                c='blue', alpha=0.5, label='Normal')
    plt.scatter(latent_2d[labels == 1, 0], latent_2d[labels == 1, 1],
                c='red', alpha=0.5, label='Anomaly')
    plt.xlabel('Latent Dimension 1')
    plt.ylabel('Latent Dimension 2')
    plt.title('VAE Latent Space Visualization')
    plt.legend()
    plt.grid(True)
    plt.savefig('vae_latent_space.png', dpi=150)
    print("æ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ–ã‚’ä¿å­˜ã—ã¾ã—ãŸ: vae_latent_space.png")

# ä½¿ç”¨ä¾‹
visualize_latent_space(trained_vae, test_data, test_labels)
</code></pre>

<hr>

<h2>4.3 GAN-basedç•°å¸¸æ¤œçŸ¥</h2>

<h3>4.3.1 AnoGANï¼ˆAnomaly Detection with GANï¼‰</h3>

<p><strong>AnoGAN</strong>ã¯ã€GANã‚’ç”¨ã„ã¦æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã©ã®ç¨‹åº¦ãã®ç”Ÿæˆåˆ†å¸ƒã‹ã‚‰é€¸è„±ã—ã¦ã„ã‚‹ã‹ã§ç•°å¸¸ã‚’æ¤œçŸ¥ã—ã¾ã™ã€‚</p>

<p><strong>è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚ºï¼š</strong></p>
<ul>
<li>æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã§GANã‚’è¨“ç·´</li>
<li>Generator GãŒæ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã‚’å­¦ç¿’</li>
</ul>

<p><strong>ãƒ†ã‚¹ãƒˆãƒ•ã‚§ãƒ¼ã‚ºï¼š</strong></p>
<ol>
<li>ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«$x$ã«å¯¾ã—ã¦ã€æ½œåœ¨å¤‰æ•°$z$ã‚’æœ€é©åŒ–: $G(z) \approx x$</li>
<li>ç•°å¸¸ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—: Residual Loss + Discrimination Loss</li>
</ol>

<h3>4.3.2 ç•°å¸¸ã‚¹ã‚³ã‚¢ã®å®šç¾©</h3>

<p>$$
A(x) = (1 - \lambda) \cdot L_R(x) + \lambda \cdot L_D(x)
$$</p>

<ul>
<li>$L_R(x) = \|x - G(z^*)\|_1$: Residual Lossï¼ˆå†æ§‹æˆèª¤å·®ï¼‰</li>
<li>$L_D(x) = \|f(x) - f(G(z^*))\|_1$: Discrimination Lossï¼ˆç‰¹å¾´ç©ºé–“ã§ã®è·é›¢ï¼‰</li>
<li>$f(\cdot)$: Discriminatorã®ä¸­é–“å±¤ã®ç‰¹å¾´</li>
</ul>

<h3>4.3.3 æ½œåœ¨å¤‰æ•°ã®æœ€é©åŒ–</h3>

<p>ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«$x$ã«å¯¾ã—ã¦ã€$G(z) \approx x$ã¨ãªã‚‹$z$ã‚’å‹¾é…é™ä¸‹æ³•ã§æ¢ç´¢ï¼š</p>

<p>$$
z^* = \arg\min_z \|x - G(z)\|_1 + \lambda \|f(x) - f(G(z))\|_1
$$</p>

<h3>4.3.4 å®Ÿè£…æ¦‚è¦</h3>

<pre><code>import torch
import torch.nn as nn

class Generator(nn.Module):
    """GAN Generator"""
    def __init__(self, latent_dim=100, output_dim=784):
        super(Generator, self).__init__()

        self.model = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, output_dim),
            nn.Sigmoid()
        )

    def forward(self, z):
        return self.model(z)


class Discriminator(nn.Module):
    """GAN Discriminatorï¼ˆä¸­é–“å±¤ã®ç‰¹å¾´ã‚‚å–å¾—ï¼‰"""
    def __init__(self, input_dim=784):
        super(Discriminator, self).__init__()

        self.features = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2)
        )

        self.classifier = nn.Sequential(
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, x, return_features=False):
        feat = self.features(x)
        output = self.classifier(feat)

        if return_features:
            return output, feat
        return output


def find_latent_code(generator, discriminator, x, n_iterations=500, lr=0.01, lambda_weight=0.1):
    """ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«xã«å¯¾ã™ã‚‹æœ€é©ãªæ½œåœ¨å¤‰æ•°zã‚’æ¢ç´¢"""
    device = next(generator.parameters()).device

    # åˆæœŸåŒ–
    z = torch.randn(x.size(0), generator.model[0].in_features, device=device, requires_grad=True)
    optimizer = torch.optim.Adam([z], lr=lr)

    for i in range(n_iterations):
        optimizer.zero_grad()

        # ç”Ÿæˆ
        G_z = generator(z)

        # Residual Loss
        residual_loss = torch.mean(torch.abs(x - G_z))

        # Discrimination Lossï¼ˆç‰¹å¾´ç©ºé–“ã§ã®è·é›¢ï¼‰
        _, feat_real = discriminator(x, return_features=True)
        _, feat_fake = discriminator(G_z, return_features=True)
        discrimination_loss = torch.mean(torch.abs(feat_real - feat_fake))

        # Total loss
        loss = (1 - lambda_weight) * residual_loss + lambda_weight * discrimination_loss

        loss.backward()
        optimizer.step()

    # ç•°å¸¸ã‚¹ã‚³ã‚¢
    with torch.no_grad():
        G_z_final = generator(z)
        residual = torch.mean(torch.abs(x - G_z_final), dim=1)

        _, feat_real = discriminator(x, return_features=True)
        _, feat_fake = discriminator(G_z_final, return_features=True)
        discrimination = torch.mean(torch.abs(feat_real - feat_fake), dim=1)

        anomaly_scores = (1 - lambda_weight) * residual + lambda_weight * discrimination

    return anomaly_scores.cpu().numpy()


# æ³¨æ„: GANã®è¨“ç·´ã‚³ãƒ¼ãƒ‰ã¯çœç•¥ï¼ˆæ¨™æº–çš„ãªGANè¨“ç·´ã‚’å®Ÿæ–½ï¼‰
# å®Ÿéš›ã®ä½¿ç”¨ã§ã¯ã€ã¾ãšGANã‚’æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã—ã€ãã®å¾Œä¸Šè¨˜ã®é–¢æ•°ã§ç•°å¸¸æ¤œçŸ¥ã‚’è¡Œã†
</code></pre>

<blockquote>
<p><strong>æ³¨æ„</strong>: AnoGANã¯æ½œåœ¨å¤‰æ•°ã®æœ€é©åŒ–ã«æ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç•°å¸¸æ¤œçŸ¥ã«ã¯ä¸å‘ãã§ã™ã€‚ã“ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€Fast-AnoGANã‚„EGBAdãªã©ã®æ”¹è‰¯æ‰‹æ³•ãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>4.4 æ™‚ç³»åˆ—ç•°å¸¸æ¤œçŸ¥</h2>

<h3>4.4.1 æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´</h3>

<p>æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸æ¤œçŸ¥ã§ã¯ã€ä»¥ä¸‹ã®ç‰¹æ€§ã‚’è€ƒæ…®ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š</p>

<ul>
<li><strong>æ™‚é–“çš„ä¾å­˜æ€§</strong>: éå»ã®å€¤ãŒæœªæ¥ã«å½±éŸ¿</li>
<li><strong>å­£ç¯€æ€§ãƒ»å‘¨æœŸæ€§</strong>: æ—¥æ¬¡ã€é€±æ¬¡ã€å¹´æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³</li>
<li><strong>ãƒˆãƒ¬ãƒ³ãƒ‰</strong>: é•·æœŸçš„ãªä¸Šæ˜‡ãƒ»ä¸‹é™å‚¾å‘</li>
<li><strong>å¤šå¤‰é‡æ€§</strong>: è¤‡æ•°ã®ã‚»ãƒ³ã‚µãƒ¼å€¤ãŒç›¸äº’ã«é–¢é€£</li>
</ul>

<h3>4.4.2 LSTM Autoencoder</h3>

<p><strong>LSTM Autoencoder</strong>ã¯ã€LSTMã‚’ç”¨ã„ã¦æ™‚ç³»åˆ—ã®æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã€å†æ§‹æˆèª¤å·®ã§ç•°å¸¸ã‚’æ¤œçŸ¥ã—ã¾ã™ã€‚</p>

<p><strong>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼š</strong></p>

<pre><code>Input: (batch, seq_len, features)
    â†“
LSTM Encoder: æ™‚ç³»åˆ—ã‚’å›ºå®šé•·ãƒ™ã‚¯ãƒˆãƒ«ã«åœ§ç¸®
    â†“
Latent Vector: (batch, latent_dim)
    â†“
LSTM Decoder: æ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰æ™‚ç³»åˆ—ã‚’å†æ§‹æˆ
    â†“
Output: (batch, seq_len, features)
</code></pre>

<h3>4.4.3 PyTorchå®Ÿè£…</h3>

<pre><code>import torch
import torch.nn as nn

class LSTMAutoencoder(nn.Module):
    """LSTM-based Autoencoder for time series"""
    def __init__(self, input_dim, hidden_dim=64, num_layers=2, latent_dim=32):
        super(LSTMAutoencoder, self).__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.latent_dim = latent_dim

        # Encoder LSTM
        self.encoder_lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True
        )

        # æ½œåœ¨è¡¨ç¾ã¸ã®åœ§ç¸®
        self.encoder_fc = nn.Linear(hidden_dim, latent_dim)

        # Decoderç”¨ã®FCï¼ˆæ½œåœ¨è¡¨ç¾ã‹ã‚‰LSTMåˆæœŸçŠ¶æ…‹ã¸ï¼‰
        self.decoder_fc = nn.Linear(latent_dim, hidden_dim)

        # Decoder LSTM
        self.decoder_lstm = nn.LSTM(
            input_size=latent_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True
        )

        # å‡ºåŠ›å±¤
        self.output_fc = nn.Linear(hidden_dim, input_dim)

    def encode(self, x):
        """ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰: æ™‚ç³»åˆ— â†’ æ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ«"""
        # x: (batch, seq_len, input_dim)
        lstm_out, (hidden, cell) = self.encoder_lstm(x)

        # æœ€å¾Œã®éš ã‚ŒçŠ¶æ…‹ã‚’ä½¿ç”¨
        last_hidden = hidden[-1]  # (batch, hidden_dim)

        # æ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ«ã«åœ§ç¸®
        z = self.encoder_fc(last_hidden)  # (batch, latent_dim)

        return z

    def decode(self, z, seq_len):
        """ãƒ‡ã‚³ãƒ¼ãƒ‰: æ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ« â†’ æ™‚ç³»åˆ—"""
        batch_size = z.size(0)

        # ãƒ‡ã‚³ãƒ¼ãƒ€ã®LSTMåˆæœŸçŠ¶æ…‹
        hidden = self.decoder_fc(z).unsqueeze(0)  # (1, batch, hidden_dim)
        hidden = hidden.repeat(self.num_layers, 1, 1)  # (num_layers, batch, hidden_dim)
        cell = torch.zeros_like(hidden)

        # ãƒ‡ã‚³ãƒ¼ãƒ€ã®å…¥åŠ›ï¼ˆæ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ«ã‚’seq_lenå›ç¹°ã‚Šè¿”ã—ï¼‰
        decoder_input = z.unsqueeze(1).repeat(1, seq_len, 1)  # (batch, seq_len, latent_dim)

        # LSTM Decoder
        lstm_out, _ = self.decoder_lstm(decoder_input, (hidden, cell))
        # lstm_out: (batch, seq_len, hidden_dim)

        # å‡ºåŠ›å±¤
        output = self.output_fc(lstm_out)  # (batch, seq_len, input_dim)

        return output

    def forward(self, x):
        """é †ä¼æ’­"""
        seq_len = x.size(1)

        z = self.encode(x)
        x_reconstructed = self.decode(z, seq_len)

        return x_reconstructed


def train_lstm_autoencoder(model, train_loader, n_epochs=50, lr=0.001):
    """LSTM Autoencoderã®è¨“ç·´"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(n_epochs):
        model.train()
        epoch_loss = 0.0

        for batch_x, in train_loader:
            batch_x = batch_x.to(device)

            # Forward pass
            reconstructed = model(batch_x)
            loss = criterion(reconstructed, batch_x)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(train_loader)

        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{n_epochs}], Loss: {avg_loss:.6f}")

    return model


def detect_ts_anomalies(model, data_loader, threshold):
    """æ™‚ç³»åˆ—ç•°å¸¸æ¤œçŸ¥"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.eval()

    all_errors = []

    with torch.no_grad():
        for batch_x, in data_loader:
            batch_x = batch_x.to(device)
            reconstructed = model(batch_x)

            # ç³»åˆ—å…¨ä½“ã®å†æ§‹æˆèª¤å·®ï¼ˆå¹³å‡ï¼‰
            errors = torch.mean((batch_x - reconstructed) ** 2, dim=(1, 2))
            all_errors.extend(errors.cpu().numpy())

    all_errors = np.array(all_errors)
    predictions = (all_errors > threshold).astype(int)

    return predictions, all_errors


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ã‚µãƒ³ãƒ—ãƒ«æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆæ­£å¸¸ï¼šæ­£å¼¦æ³¢ã€ç•°å¸¸ï¼šãƒã‚¤ã‚ºï¼‰
    np.random.seed(42)
    torch.manual_seed(42)

    seq_len = 50
    input_dim = 5  # 5ã¤ã®ã‚»ãƒ³ã‚µãƒ¼

    # æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ï¼ˆæ­£å¼¦æ³¢ãƒ™ãƒ¼ã‚¹ï¼‰
    n_normal_sequences = 500
    t = np.linspace(0, 4*np.pi, seq_len)
    normal_sequences = []
    for _ in range(n_normal_sequences):
        seq = np.array([np.sin(t + np.random.randn() * 0.1) for _ in range(input_dim)]).T
        seq += np.random.randn(seq_len, input_dim) * 0.1
        normal_sequences.append(seq)

    normal_sequences = np.array(normal_sequences)  # (n_normal, seq_len, input_dim)

    # ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚ºï¼‰
    n_anomaly_sequences = 50
    anomaly_sequences = np.random.randn(n_anomaly_sequences, seq_len, input_dim)

    # Dataset
    train_dataset = TensorDataset(torch.FloatTensor(normal_sequences))
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

    test_sequences = np.vstack([normal_sequences[:50], anomaly_sequences])
    test_labels = np.array([0] * 50 + [1] * n_anomaly_sequences)

    test_dataset = TensorDataset(torch.FloatTensor(test_sequences))
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    print("=== LSTM Autoencoderè¨“ç·´é–‹å§‹ ===")
    lstm_ae = LSTMAutoencoder(input_dim=input_dim, hidden_dim=64, num_layers=2, latent_dim=32)
    trained_lstm_ae = train_lstm_autoencoder(lstm_ae, train_loader, n_epochs=50, lr=0.001)

    # Thresholdè¨­å®š
    train_errors = []
    trained_lstm_ae.eval()
    with torch.no_grad():
        for batch_x, in train_loader:
            reconstructed = trained_lstm_ae(batch_x)
            errors = torch.mean((batch_x - reconstructed) ** 2, dim=(1, 2))
            train_errors.extend(errors.cpu().numpy())

    threshold = np.percentile(train_errors, 95)
    print(f"\né–¾å€¤ï¼ˆ95%ç‚¹ï¼‰: {threshold:.6f}")

    # ç•°å¸¸æ¤œçŸ¥
    predictions, test_errors = detect_ts_anomalies(trained_lstm_ae, test_loader, threshold)

    print("\n=== LSTM Autoencoderç•°å¸¸æ¤œçŸ¥çµæœ ===")
    print(classification_report(test_labels, predictions,
                                target_names=['Normal', 'Anomaly']))
    print(f"ROC-AUC: {roc_auc_score(test_labels, test_errors):.3f}")
</code></pre>

<h3>4.4.4 å¤šå¤‰é‡æ™‚ç³»åˆ—ç•°å¸¸æ¤œçŸ¥</h3>

<p>è¤‡æ•°ã®ã‚»ãƒ³ã‚µãƒ¼ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’åŒæ™‚ã«æ‰±ã†å ´åˆã€å„å¤‰æ•°é–“ã®ç›¸é–¢é–¢ä¿‚ã‚‚è€ƒæ…®ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚</p>

<p><strong>æ‰‹æ³•ï¼š</strong></p>
<ul>
<li><strong>LSTM Autoencoder</strong>: ä¸Šè¨˜ã®å®Ÿè£…ã§å¯¾å¿œæ¸ˆã¿</li>
<li><strong>Attentionæ©Ÿæ§‹</strong>: ã©ã®å¤‰æ•°ãŒç•°å¸¸ã«å¯„ä¸ã—ã¦ã„ã‚‹ã‹ã‚’è§£é‡ˆ</li>
<li><strong>Transformer</strong>: é•·æœŸä¾å­˜é–¢ä¿‚ã®å­¦ç¿’</li>
</ul>

<hr>

<h2>4.5 ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰å®Ÿè·µ</h2>

<h3>4.5.1 ãƒ‡ãƒ¼ã‚¿æº–å‚™</h3>

<p>å®Ÿä¸–ç•Œã®ç•°å¸¸æ¤œçŸ¥ã§ã¯ã€ä»¥ä¸‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¾ã™ã€‚</p>

<pre><code>import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

class AnomalyDetectionPipeline:
    """ç•°å¸¸æ¤œçŸ¥ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    def __init__(self, model_type='autoencoder'):
        self.model_type = model_type
        self.scaler = StandardScaler()
        self.model = None
        self.threshold = None

    def preprocess(self, data, fit_scaler=False):
        """å‰å‡¦ç†: æ­£è¦åŒ–ã€æ¬ æå€¤å‡¦ç†ãªã©"""
        # æ¬ æå€¤è£œå®Œï¼ˆå¹³å‡å€¤ï¼‰
        data = data.fillna(data.mean())

        # æ¨™æº–åŒ–
        if fit_scaler:
            data_scaled = self.scaler.fit_transform(data)
        else:
            data_scaled = self.scaler.transform(data)

        return data_scaled

    def create_sequences(self, data, seq_len=50):
        """æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«åˆ†å‰²"""
        sequences = []
        for i in range(len(data) - seq_len + 1):
            sequences.append(data[i:i+seq_len])

        return np.array(sequences)

    def train(self, normal_data, seq_len=50, n_epochs=50):
        """ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        # å‰å‡¦ç†
        normal_scaled = self.preprocess(normal_data, fit_scaler=True)

        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ
        if self.model_type in ['lstm_ae', 'transformer']:
            sequences = self.create_sequences(normal_scaled, seq_len)
            train_dataset = TensorDataset(torch.FloatTensor(sequences))
        else:
            # Autoencoderã®å ´åˆ
            train_dataset = TensorDataset(torch.FloatTensor(normal_scaled))

        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

        # ãƒ¢ãƒ‡ãƒ«é¸æŠã¨è¨“ç·´
        if self.model_type == 'autoencoder':
            self.model = Autoencoder(input_dim=normal_scaled.shape[1])
            self.model, _ = train_autoencoder(self.model, train_loader, n_epochs)
        elif self.model_type == 'vae':
            self.model = VAE(input_dim=normal_scaled.shape[1])
            self.model = train_vae(self.model, train_loader, n_epochs)
        elif self.model_type == 'lstm_ae':
            self.model = LSTMAutoencoder(input_dim=normal_scaled.shape[1])
            self.model = train_lstm_autoencoder(self.model, train_loader, n_epochs)

        # Thresholdè¨­å®šï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®95%ç‚¹ï¼‰
        if self.model_type == 'vae':
            scores = vae_anomaly_score(self.model, torch.FloatTensor(normal_scaled))
        else:
            scores = compute_reconstruction_errors(self.model, train_loader)

        self.threshold = np.percentile(scores, 95)
        print(f"é–¾å€¤è¨­å®š: {self.threshold:.6f}")

    def predict(self, test_data, seq_len=50):
        """ç•°å¸¸äºˆæ¸¬"""
        # å‰å‡¦ç†
        test_scaled = self.preprocess(test_data, fit_scaler=False)

        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ
        if self.model_type in ['lstm_ae', 'transformer']:
            sequences = self.create_sequences(test_scaled, seq_len)
            test_dataset = TensorDataset(torch.FloatTensor(sequences))
        else:
            test_dataset = TensorDataset(torch.FloatTensor(test_scaled))

        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

        # ç•°å¸¸ã‚¹ã‚³ã‚¢è¨ˆç®—
        if self.model_type == 'vae':
            scores = vae_anomaly_score(self.model, torch.FloatTensor(test_scaled))
        else:
            scores = compute_reconstruction_errors(self.model, test_loader)

        # ç•°å¸¸åˆ¤å®š
        predictions = (scores > self.threshold).astype(int)

        return predictions, scores


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ä»®ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    normal_df = pd.DataFrame(np.random.randn(1000, 10))
    test_df = pd.DataFrame(np.random.randn(100, 10))

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    pipeline = AnomalyDetectionPipeline(model_type='autoencoder')
    pipeline.train(normal_df, n_epochs=30)

    predictions, scores = pipeline.predict(test_df)
    print(f"\nç•°å¸¸æ¤œå‡ºæ•°: {predictions.sum()} / {len(predictions)}")
</code></pre>

<h3>4.5.2 ãƒ¢ãƒ‡ãƒ«é¸æŠ</h3>

<p>ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«å¿œã˜ãŸé©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¾ã™ã€‚</p>

<table>
<tr>
<th>ãƒ‡ãƒ¼ã‚¿ç¨®é¡</th>
<th>æ¨å¥¨ãƒ¢ãƒ‡ãƒ«</th>
<th>ç†ç”±</th>
</tr>
<tr>
<td>ç”»åƒãƒ‡ãƒ¼ã‚¿</td>
<td>Convolutional AEã€VAE</td>
<td>ç©ºé–“æ§‹é€ ã‚’ä¿æŒ</td>
</tr>
<tr>
<td>æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿</td>
<td>LSTM AEã€Transformer</td>
<td>æ™‚é–“çš„ä¾å­˜æ€§ã‚’æ‰ãˆã‚‹</td>
</tr>
<tr>
<td>è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿</td>
<td>Autoencoderã€VAE</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ã§åŠ¹æœçš„</td>
</tr>
<tr>
<td>é«˜æ¬¡å…ƒã‚¹ãƒ‘ãƒ¼ã‚¹</td>
<td>Sparse AEã€VAE</td>
<td>æ¬¡å…ƒå‰Šæ¸›ã¨æ­£å‰‡åŒ–</td>
</tr>
</table>

<h3>4.5.3 Thresholdèª¿æ•´</h3>

<p>å®Ÿé‹ç”¨ã§ã¯ã€æ¥­å‹™è¦ä»¶ã«å¿œã˜ã¦Thresholdã‚’èª¿æ•´ã—ã¾ã™ã€‚</p>

<ul>
<li><strong>False Positiveç‡ã‚’é‡è¦–</strong>: Thresholdã‚’é«˜ãè¨­å®šï¼ˆèª¤æ¤œçŸ¥ã‚’æ¸›ã‚‰ã™ï¼‰</li>
<li><strong>Recallé‡è¦–</strong>: Thresholdã‚’ä½ãè¨­å®šï¼ˆç•°å¸¸ã‚’è¦‹é€ƒã•ãªã„ï¼‰</li>
<li><strong>F1æœ€å¤§åŒ–</strong>: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§F1ã‚¹ã‚³ã‚¢ãŒæœ€å¤§ã¨ãªã‚‹ç‚¹ã‚’é¸æŠ</li>
</ul>

<h3>4.5.4 Production Deployment</h3>

<p><strong>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç•°å¸¸æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹æˆï¼š</strong></p>

<pre><code>ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆã‚»ãƒ³ã‚µãƒ¼ã€ãƒ­ã‚°ï¼‰
    â†“
å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆæ­£è¦åŒ–ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–ï¼‰
    â†“
ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«ï¼ˆPyTorch â†’ ONNX â†’ TorchScriptï¼‰
    â†“
Thresholdåˆ¤å®š
    â†“
ã‚¢ãƒ©ãƒ¼ãƒˆãƒ»å¯è¦–åŒ–ï¼ˆGrafanaã€Slacké€šçŸ¥ï¼‰
</code></pre>

<p><strong>ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®ãƒã‚¤ãƒ³ãƒˆï¼š</strong></p>
<ul>
<li><strong>ãƒ¢ãƒ‡ãƒ«ã®è»½é‡åŒ–</strong>: TorchScriptã€ONNXå¤‰æ›ã§æ¨è«–é«˜é€ŸåŒ–</li>
<li><strong>ãƒãƒƒãƒå‡¦ç†</strong>: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§ãŒæ±‚ã‚ã‚‰ã‚Œãªã„å ´åˆã¯ãƒãƒƒãƒã§åŠ¹ç‡åŒ–</li>
<li><strong>å®šæœŸçš„ãªå†å­¦ç¿’</strong>: ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®å¤‰åŒ–ã«å¯¾å¿œï¼ˆConcept Driftï¼‰</li>
<li><strong>é–¾å€¤ã®è‡ªå‹•èª¿æ•´</strong>: é‹ç”¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é©å¿œçš„ã«èª¿æ•´</li>
</ul>

<h3>4.5.5 ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨ã‚¢ãƒ©ãƒ¼ãƒˆ</h3>

<pre><code>import logging
from datetime import datetime

class AnomalyMonitor:
    """ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°"""
    def __init__(self, alert_threshold=0.9):
        self.alert_threshold = alert_threshold
        self.logger = self._setup_logger()

    def _setup_logger(self):
        logger = logging.getLogger('AnomalyDetection')
        logger.setLevel(logging.INFO)

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©
        fh = logging.FileHandler('anomaly_detection.log')
        fh.setLevel(logging.INFO)

        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        fh.setFormatter(formatter)

        logger.addHandler(fh)
        return logger

    def log_anomaly(self, timestamp, anomaly_score, features):
        """ç•°å¸¸ã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        self.logger.info(f"Anomaly detected - Time: {timestamp}, Score: {anomaly_score:.4f}")
        self.logger.info(f"Features: {features}")

    def send_alert(self, anomaly_score, message):
        """ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡ï¼ˆå®Ÿè£…ä¾‹ï¼‰"""
        if anomaly_score > self.alert_threshold:
            # Slackã€Emailã€PagerDutyãªã©ã«é€šçŸ¥
            print(f"[ALERT] High anomaly detected: {message}")
            self.logger.warning(f"High severity alert: {message}")

    def monitor(self, pipeline, data_stream):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°"""
        for timestamp, data in data_stream:
            predictions, scores = pipeline.predict(data)

            if predictions.any():
                self.log_anomaly(timestamp, scores.max(), data)
                self.send_alert(scores.max(), f"Anomaly at {timestamp}")


# ä½¿ç”¨ä¾‹ï¼ˆä»®æƒ³ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼‰
monitor = AnomalyMonitor(alert_threshold=0.9)

# ä»®æƒ³ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒªãƒ¼ãƒ 
def data_stream_generator():
    for i in range(10):
        timestamp = datetime.now()
        data = pd.DataFrame(np.random.randn(1, 10))
        yield timestamp, data

# ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
# monitor.monitor(pipeline, data_stream_generator())
</code></pre>

<hr>

<h2>ã¾ã¨ã‚</h2>

<p>æœ¬ç« ã§å­¦ã‚“ã ã“ã¨ï¼š</p>

<ol>
<li><strong>Autoencoderã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥:</strong>
<ul>
<li>å†æ§‹æˆèª¤å·®ã§ç•°å¸¸ã‚’æ¤œå‡º</li>
<li>ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¨­è¨ˆ</li>
<li>Thresholdé¸æŠæ‰‹æ³•</li>
<li>PyTorchã§ã®å®Œå…¨å®Ÿè£…</li>
</ul>
</li>

<li><strong>Variational Autoencoder (VAE):</strong>
<ul>
<li>ç¢ºç‡çš„æ½œåœ¨è¡¨ç¾ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥</li>
<li>å†æ§‹æˆèª¤å·® + KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹</li>
<li>æ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ–ã¨åˆ†æ</li>
<li>Î²-VAEã«ã‚ˆã‚‹èª¿æ•´</li>
</ul>
</li>

<li><strong>GAN-basedç•°å¸¸æ¤œçŸ¥:</strong>
<ul>
<li>AnoGANã®åŸç†ã¨å®Ÿè£…</li>
<li>æ½œåœ¨å¤‰æ•°ã®æœ€é©åŒ–</li>
<li>Discriminatorã®ç‰¹å¾´ã‚’åˆ©ç”¨</li>
<li>Fast-AnoGANãªã©ã®æ”¹è‰¯æ‰‹æ³•</li>
</ul>
</li>

<li><strong>æ™‚ç³»åˆ—ç•°å¸¸æ¤œçŸ¥:</strong>
<ul>
<li>LSTM Autoencoderã®å®Ÿè£…</li>
<li>æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’</li>
<li>å¤šå¤‰é‡æ™‚ç³»åˆ—ã¸ã®å¯¾å¿œ</li>
<li>ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†</li>
</ul>
</li>

<li><strong>ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰å®Ÿè·µ:</strong>
<ul>
<li>ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</li>
<li>ãƒ¢ãƒ‡ãƒ«é¸æŠã®æŒ‡é‡</li>
<li>Thresholdèª¿æ•´æ‰‹æ³•</li>
<li>Production deploymentã®è¨­è¨ˆ</li>
<li>ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨ã‚¢ãƒ©ãƒ¼ãƒˆ</li>
</ul>
</li>
</ol>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<p><strong>å•1:</strong> Autoencoderã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥ã§ã€æ½œåœ¨æ¬¡å…ƒã‚’å…¥åŠ›æ¬¡å…ƒã®10%ã«è¨­å®šã™ã‚‹ç†ç”±ã‚’èª¬æ˜ã›ã‚ˆã€‚</p>

<p><strong>å•2:</strong> VAEã®æå¤±é–¢æ•°ã«ãŠã‘ã‚‹KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹é …ã®å½¹å‰²ã‚’ã€æ½œåœ¨ç©ºé–“ã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚</p>

<p><strong>å•3:</strong> AnoGANã¨é€šå¸¸ã®Autoencoderã®ç•°å¸¸æ¤œçŸ¥ã«ãŠã‘ã‚‹ä¸»ãªé•ã„ã‚’3ã¤æŒ™ã’ã‚ˆã€‚</p>

<p><strong>å•4:</strong> LSTM Autoencoderã§æ™‚ç³»åˆ—ç•°å¸¸æ¤œçŸ¥ã‚’è¡Œã†éš›ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’ã©ã®ã‚ˆã†ã«æ±ºå®šã™ã¹ãã‹ã€3ã¤ã®è¦³ç‚¹ã‹ã‚‰è«–ã˜ã‚ˆã€‚</p>

<p><strong>å•5:</strong> ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«ã®Thresholdè¨­å®šã«ãŠã„ã¦ã€False Positiveç‡ãŒ5%ä»¥ä¸‹ã¨ã„ã†æ¥­å‹™è¦ä»¶ãŒã‚ã‚‹å ´åˆã€ã©ã®ã‚ˆã†ã«Thresholdã‚’æ±ºå®šã™ã¹ãã‹å…·ä½“çš„ã«èª¬æ˜ã›ã‚ˆã€‚</p>

<p><strong>å•6:</strong> ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç•°å¸¸æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹éš›ã€è€ƒæ…®ã™ã¹ãæŠ€è¡“çš„èª²é¡Œã‚’5ã¤æŒ™ã’ã€ãã‚Œãã‚Œã®å¯¾å‡¦æ³•ã‚’ææ¡ˆã›ã‚ˆã€‚</p>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Goodfellow, I. et al. "Deep Learning." MIT Press (2016).</li>
<li>Kingma, D. P., & Welling, M. "Auto-Encoding Variational Bayes." <em>ICLR</em> (2014).</li>
<li>Schlegl, T. et al. "Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery." <em>IPMI</em> (2017). [AnoGAN]</li>
<li>Malhotra, P. et al. "LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection." <em>ICML Anomaly Detection Workshop</em> (2016).</li>
<li>Park, D. et al. "A Multimodal Anomaly Detector for Robot-Assisted Feeding Using an LSTM-based Variational Autoencoder." <em>IEEE Robotics and Automation Letters</em> (2018).</li>
<li>Su, Y. et al. "Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network." <em>KDD</em> (2019).</li>
<li>Vaswani, A. et al. "Attention is All You Need." <em>NeurIPS</em> (2017). [Transformer]</li>
<li>Audibert, J. et al. "USAD: UnSupervised Anomaly Detection on Multivariate Time Series." <em>KDD</em> (2020).</li>
</ol>

<hr>

<div class="navigation">
    <a href="chapter3-ml-based-anomaly.html" class="nav-button">â† å‰ã®ç« </a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
