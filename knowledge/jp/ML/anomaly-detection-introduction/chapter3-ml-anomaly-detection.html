<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Á¨¨3Á´†ÔºöÊ©üÊ¢∞Â≠¶Áøí„Éô„Éº„ÇπÁï∞Â∏∏Ê§úÁü• - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Á¨¨3Á´†ÔºöÊ©üÊ¢∞Â≠¶Áøí„Éô„Éº„ÇπÁï∞Â∏∏Ê§úÁü•</h1>
            <p class="subtitle">Isolation Forest„ÄÅLOF„ÄÅOne-Class SVM„Å´„Çà„ÇãÁï∞Â∏∏Ê§úÂá∫</p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 70-80ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ‰∏≠Á¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 10ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 5Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Á¨¨3Á´†ÔºöÊ©üÊ¢∞Â≠¶Áøí„Éô„Éº„ÇπÁï∞Â∏∏Ê§úÁü•</h1>

<div class="learning-objectives">
<h2>Â≠¶ÁøíÁõÆÊ®ô</h2>
<ul>
<li>Isolation Forest„ÅÆ„Ç¢„É´„Ç¥„É™„Ç∫„É†ÂéüÁêÜ„ÇíÁêÜËß£„Åô„Çã</li>
<li>LOFÔºàLocal Outlier FactorÔºâ„ÅßÂ±ÄÊâÄÁöÑ„Å™Áï∞Â∏∏„ÇíÊ§úÂá∫„Åß„Åç„Çã</li>
<li>One-Class SVM„Å´„Çà„ÇãÊ≠£Â∏∏„Éá„Éº„Çø„ÅÆÂ¢ÉÁïåÂ≠¶Áøí„ÇíÁøíÂæó„Åô„Çã</li>
<li>DBSCAN„ÇÑ„Åù„ÅÆ‰ªñ„ÅÆÊâãÊ≥ï„ÇíÁï∞Â∏∏Ê§úÁü•„Å´ÈÅ©Áî®„Åß„Åç„Çã</li>
<li>„Ç¢„É≥„Çµ„É≥„Éñ„É´Áï∞Â∏∏Ê§úÁü•„ÅÆÂÆüË£ÖÊñπÊ≥ï„ÇíÂ≠¶„Å∂</li>
</ul>
</div>

<p><strong>Ë™≠‰∫ÜÊôÇÈñì</strong>: 70-80ÂàÜ</p>

<hr />

<h2>3.1 Isolation ForestÔºàÂ≠§Á´ãÊ£ÆÊûóÔºâ</h2>

<p>Isolation Forest„ÅØ„ÄÅÁï∞Â∏∏„Éá„Éº„Çø„ÅåÊ≠£Â∏∏„Éá„Éº„Çø„Çà„Çä„ÄåÂàÜÈõ¢„Åó„ÇÑ„Åô„ÅÑ„Äç„Å®„ÅÑ„ÅÜÊÄßË≥™„ÇíÂà©Áî®„Åó„ÅüÁï∞Â∏∏Ê§úÁü•„Ç¢„É´„Ç¥„É™„Ç∫„É†„Åß„Åô„ÄÇ2008Âπ¥„Å´Liu et al.„Å´„Çà„Å£„Å¶ÊèêÊ°à„Åï„Çå„ÄÅÈ´òÊ¨°ÂÖÉ„Éá„Éº„Çø„Å´„ÇÇÂäπÊûúÁöÑ„Å´ÈÅ©Áî®„Åß„Åç„Åæ„Åô„ÄÇ</p>

<h3>3.1.1 „Ç¢„É´„Ç¥„É™„Ç∫„É†ÂéüÁêÜ</h3>

<p><strong>Âü∫Êú¨„Ç¢„Ç§„Éá„Ç¢:</strong></p>
<ul>
<li>Áï∞Â∏∏„Éá„Éº„Çø„ÅØÊï∞„ÅåÂ∞ë„Å™„Åè„ÄÅÊ≠£Â∏∏„Éá„Éº„Çø„Å®„ÅØÁï∞„Å™„ÇãÁâπÂæ¥ÂÄ§„ÇíÊåÅ„Å§</li>
<li>„É©„É≥„ÉÄ„É†„Å´ÈÅ∏„Çì„Å†ÁâπÂæ¥Èáè„ÅßÂàÜÂâ≤„ÇíÁπ∞„ÇäËøî„Åô„Å®„ÄÅÁï∞Â∏∏„Éá„Éº„Çø„ÅØ„Çà„ÇäÊó©„ÅèÂ≠§Á´ã„Åô„Çã</li>
<li>Â≠§Á´ã„Åæ„Åß„ÅÆÂàÜÂâ≤ÂõûÊï∞Ôºà„Éë„ÇπÈï∑Ôºâ„ÅåÁü≠„ÅÑ„Åª„Å©Áï∞Â∏∏Â∫¶„ÅåÈ´ò„ÅÑ</li>
</ul>

<p><strong>„Ç¢„É´„Ç¥„É™„Ç∫„É†„Çπ„ÉÜ„ÉÉ„Éó:</strong></p>

<pre><code>1. „É©„É≥„ÉÄ„É†„Å´ÁâπÂæ¥Èáè„ÇíÈÅ∏Êäû
2. „Åù„ÅÆÁâπÂæ¥Èáè„ÅÆÊúÄÂ∞èÂÄ§„Å®ÊúÄÂ§ßÂÄ§„ÅÆÈñì„Åß„É©„É≥„ÉÄ„É†„Å´ÂàÜÂâ≤ÁÇπ„ÇíÈÅ∏„Å∂
3. „Éá„Éº„Çø„Çí2„Å§„ÅÆ„Ç∞„É´„Éº„Éó„Å´ÂàÜ„Åë„Çã
4. ÂêÑ„Ç∞„É´„Éº„Éó„Å´ÂØæ„Åó„Å¶ÂÜçÂ∏∞ÁöÑ„Å´1-3„ÇíÁπ∞„ÇäËøî„Åô
5. ÂêÑ„Éá„Éº„Çø„Éù„Ç§„É≥„Éà„ÅåÂ≠§Á´ã„Åô„Çã„Åæ„Åß„ÅÆ„Éë„ÇπÈï∑„ÇíË®òÈå≤
6. Ë§áÊï∞„ÅÆÊú®ÔºàÊ£ÆÔºâ„ÇíÊßãÁØâ„Åó„ÄÅÂπ≥Âùá„Éë„ÇπÈï∑„Åã„ÇâÁï∞Â∏∏„Çπ„Ç≥„Ç¢„ÇíË®àÁÆó
</code></pre>

<h3>3.1.2 „Éë„ÇπÈï∑„Å®Áï∞Â∏∏„Çπ„Ç≥„Ç¢</h3>

<p><strong>„Éë„ÇπÈï∑ÔºàPath LengthÔºâ:</strong></p>

<p>„Éá„Éº„Çø„Éù„Ç§„É≥„Éà $x$ „ÅåÂ≠§Á´ã„Åô„Çã„Åæ„Åß„ÅÆÂàÜÂâ≤ÂõûÊï∞„Çí $h(x)$ „Å®„Åô„Çã„Å®„ÄÅÊ≠£Â∏∏„Éá„Éº„Çø„ÅØÊ∑±„ÅÑ‰ΩçÁΩÆÔºàÂ§ß„Åç„Å™ $h(x)$Ôºâ„ÄÅÁï∞Â∏∏„Éá„Éº„Çø„ÅØÊµÖ„ÅÑ‰ΩçÁΩÆÔºàÂ∞è„Åï„Å™ $h(x)$Ôºâ„Å´Â≠§Á´ã„Åó„Åæ„Åô„ÄÇ</p>

<p><strong>Áï∞Â∏∏„Çπ„Ç≥„Ç¢„ÅÆË®àÁÆó:</strong></p>

$$
s(x, n) = 2^{-\frac{E[h(x)]}{c(n)}}
$$

<p>„Åì„Åì„Åß:</p>
<ul>
<li>$E[h(x)]$: Ë§áÊï∞„ÅÆÊú®„Å´„Åä„Åë„ÇãÂπ≥Âùá„Éë„ÇπÈï∑</li>
<li>$c(n)$: „Çµ„É≥„Éó„É´„Çµ„Ç§„Ç∫ $n$ „Å´„Åä„Åë„ÇãÂπ≥Âùá„Éë„ÇπÈï∑„ÅÆÊ≠£Ë¶èÂåñÂÆöÊï∞</li>
<li>$c(n) = 2H(n-1) - \frac{2(n-1)}{n}$ Ôºà$H(i)$ „ÅØË™øÂíåÊï∞Ôºâ</li>
</ul>

<p><strong>„Çπ„Ç≥„Ç¢„ÅÆËß£Èáà:</strong></p>
<ul>
<li>$s \approx 1$: Áï∞Â∏∏ÔºàÊòéÁ¢∫„Å™Áï∞Â∏∏ÂÄ§Ôºâ</li>
<li>$s \approx 0.5$: Ê≠£Â∏∏ÔºàÂπ≥ÂùáÁöÑ„Å™„Éë„ÇπÈï∑Ôºâ</li>
<li>$s < 0.5$: Ê≠£Â∏∏ÔºàÂπ≥Âùá„Çà„ÇäÊ∑±„ÅÑ‰ΩçÁΩÆÔºâ</li>
</ul>

<h3>3.1.3 „Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøË™øÊï¥</h3>

<p><strong>‰∏ªË¶Å„Éë„É©„É°„Éº„Çø:</strong></p>

<table>
<thead>
<tr>
<th>„Éë„É©„É°„Éº„Çø</th>
<th>Ë™¨Êòé</th>
<th>Êé®Â•®ÂÄ§</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>n_estimators</code></td>
<td>Êú®„ÅÆÊï∞</td>
<td>100-200Ôºà„Éá„Éï„Ç©„É´„Éà: 100Ôºâ</td>
</tr>
<tr>
<td><code>max_samples</code></td>
<td>ÂêÑÊú®„Åß„Çµ„É≥„Éó„É™„É≥„Ç∞„Åô„Çã„Éá„Éº„ÇøÊï∞</td>
<td>256Ôºà„Éá„Éï„Ç©„É´„Éà: autoÔºâ</td>
</tr>
<tr>
<td><code>contamination</code></td>
<td>Áï∞Â∏∏„Éá„Éº„Çø„ÅÆÂâ≤Âêà</td>
<td>0.1Ôºà„Éá„Éº„Çø„Å´‰æùÂ≠òÔºâ</td>
</tr>
<tr>
<td><code>max_features</code></td>
<td>ÂêÑÂàÜÂâ≤„ÅßËÄÉÊÖÆ„Åô„ÇãÁâπÂæ¥ÈáèÊï∞</td>
<td>1.0ÔºàÂÖ®ÁâπÂæ¥ÈáèÔºâ</td>
</tr>
</tbody>
</table>

<p><strong>„Éë„É©„É°„Éº„ÇøÈÅ∏Êäû„ÅÆ„Ç¨„Ç§„Éâ„É©„Ç§„É≥:</strong></p>
<ul>
<li><code>n_estimators</code>: Â§ö„ÅÑ„Åª„Å©ÂÆâÂÆö„Åô„Çã„ÅåË®àÁÆó„Ç≥„Çπ„ÉàÂ¢óÂä†Ôºà100-200„ÅßÂçÅÂàÜÔºâ</li>
<li><code>max_samples</code>: 256„ÅåÊé®Â•®ÔºàË´ñÊñá„ÅÆ„Éá„Éï„Ç©„É´„ÉàÔºâ„ÄÅÂ§ßË¶èÊ®°„Éá„Éº„Çø„Åß„ÅØÂ∞è„Åï„Åè„Åó„Å¶„Çπ„Éî„Éº„Éâ„Ç¢„ÉÉ„Éó</li>
<li><code>contamination</code>: ‰∫ãÂâç„Å´Áï∞Â∏∏Áéá„Åå„Çè„Åã„Å£„Å¶„ÅÑ„Çå„Å∞„Åù„Çå„ÇíË®≠ÂÆö„ÄÅ‰∏çÊòé„Å™„Çâ0.1</li>
</ul>

<h3>3.1.4 scikit-learnÂÆüË£Ö</h3>

<p><strong>Âü∫Êú¨ÂÆüË£Ö:</strong></p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.datasets import make_blobs

# „Çµ„É≥„Éó„É´„Éá„Éº„ÇøÁîüÊàêÔºàÊ≠£Â∏∏„Éá„Éº„Çø + Áï∞Â∏∏„Éá„Éº„ÇøÔºâ
np.random.seed(42)
X_normal, _ = make_blobs(n_samples=300, centers=1, cluster_std=0.5, random_state=42)
X_anomaly = np.random.uniform(low=-4, high=4, size=(20, 2))  # Áï∞Â∏∏„Éá„Éº„Çø
X = np.vstack([X_normal, X_anomaly])

# Isolation Forest„É¢„Éá„É´
iso_forest = IsolationForest(
    n_estimators=100,
    max_samples=256,
    contamination=0.1,  # 10%„ÅåÁï∞Â∏∏„Å®ÊÉ≥ÂÆö
    random_state=42
)

# Â≠¶Áøí„Å®‰∫àÊ∏¨
y_pred = iso_forest.fit_predict(X)  # -1: Áï∞Â∏∏„ÄÅ1: Ê≠£Â∏∏
scores = iso_forest.score_samples(X)  # Áï∞Â∏∏„Çπ„Ç≥„Ç¢Ôºà‰Ωé„ÅÑ„Åª„Å©Áï∞Â∏∏Ôºâ

# ÂèØË¶ñÂåñ
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='coolwarm', edgecolors='k')
plt.title('Isolation Forest: Áï∞Â∏∏Ê§úÁü•ÁµêÊûú')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Prediction (-1: Anomaly, 1: Normal)')

plt.subplot(1, 2, 2)
plt.scatter(X[:, 0], X[:, 1], c=scores, cmap='viridis', edgecolors='k')
plt.title('Isolation Forest: Áï∞Â∏∏„Çπ„Ç≥„Ç¢')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Anomaly Score')

plt.tight_layout()
plt.show()

print(f"Ê§úÂá∫„Åï„Çå„ÅüÁï∞Â∏∏„Éá„Éº„ÇøÊï∞: {np.sum(y_pred == -1)}")
print(f"Áï∞Â∏∏„Çπ„Ç≥„Ç¢ÁØÑÂõ≤: [{scores.min():.3f}, {scores.max():.3f}]")
</code></pre>

<p><strong>Âá∫Âäõ‰æã:</strong></p>
<pre><code>Ê§úÂá∫„Åï„Çå„ÅüÁï∞Â∏∏„Éá„Éº„ÇøÊï∞: 32
Áï∞Â∏∏„Çπ„Ç≥„Ç¢ÁØÑÂõ≤: [-0.234, 0.178]
</code></pre>

<p><strong>ÂÆü„Éá„Éº„Çø„Å∏„ÅÆÈÅ©Áî®‰æãÔºà„ÇØ„É¨„Ç∏„ÉÉ„Éà„Ç´„Éº„Éâ‰∏çÊ≠£Ê§úÁü•Ôºâ:</strong></p>

<pre><code class="language-python">import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# „Éá„Éº„ÇøË™≠„ÅøËæº„ÅøÔºà‰ªÆÊÉ≥ÁöÑ„Å™‰æãÔºâ
# ÂÆüÈöõ„ÅÆ„Éá„Éº„Çø„ÅØ Kaggle Credit Card Fraud Detection „Å™„Å©„Çí‰ΩøÁî®
# URL: https://www.kaggle.com/mlg-ulb/creditcardfraud

# „Çµ„É≥„Éó„É´„Éá„Éº„ÇøÁîüÊàêÔºàÂÆü„Éá„Éº„Çø„ÅÆ‰ª£ÊõøÔºâ
np.random.seed(42)
n_normal = 1000
n_fraud = 50

# Ê≠£Â∏∏ÂèñÂºïÔºàÈáëÈ°çÂ∞è„ÄÅÂõûÊï∞Â§ö„ÄÅÂú∞ÁêÜÁöÑ„Å´ÈõÜ‰∏≠Ôºâ
normal_features = np.random.randn(n_normal, 5) * [10, 5, 2, 1, 0.5]
normal_labels = np.zeros(n_normal)

# ‰∏çÊ≠£ÂèñÂºïÔºàÈáëÈ°çÂ§ß„ÄÅÂõûÊï∞Â∞ë„ÄÅÂú∞ÁêÜÁöÑ„Å´ÂàÜÊï£Ôºâ
fraud_features = np.random.randn(n_fraud, 5) * [50, 1, 10, 5, 3] + [100, 0, 50, 20, 10]
fraud_labels = np.ones(n_fraud)

X = np.vstack([normal_features, fraud_features])
y = np.hstack([normal_labels, fraud_labels])

# Ë®ìÁ∑¥„Éª„ÉÜ„Çπ„ÉàÂàÜÂâ≤
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

# Isolation ForestÔºàÊ≠£Â∏∏„Éá„Éº„Çø„ÅÆ„Åø„ÅßÂ≠¶ÁøíÔºâ
iso_forest = IsolationForest(
    n_estimators=100,
    contamination=0.05,  # 5%„Åå‰∏çÊ≠£„Å®ÊÉ≥ÂÆö
    random_state=42
)

# Ë®ìÁ∑¥„Éá„Éº„Çø„ÅßÂ≠¶Áøí
iso_forest.fit(X_train)

# „ÉÜ„Çπ„Éà„Éá„Éº„Çø„Åß‰∫àÊ∏¨
y_pred = iso_forest.predict(X_test)
y_pred = np.where(y_pred == -1, 1, 0)  # -1„Çí1Ôºà‰∏çÊ≠£Ôºâ„Å´Â§âÊèõ

# Ë©ï‰æ°
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['Normal', 'Fraud']))
</code></pre>

<p><strong>Âá∫Âäõ‰æã:</strong></p>
<pre><code>Confusion Matrix:
[[285  15]
 [  3  12]]

Classification Report:
              precision    recall  f1-score   support

      Normal       0.99      0.95      0.97       300
       Fraud       0.44      0.80      0.57        15

    accuracy                           0.94       315
   macro avg       0.72      0.88      0.77       315
weighted avg       0.96      0.94      0.95       315
</code></pre>

<hr />

<h2>3.2 LOFÔºàLocal Outlier FactorÔºâ</h2>

<p>LOF„ÅØ„ÄÅÂêÑ„Éá„Éº„Çø„Éù„Ç§„É≥„Éà„ÅÆÂ±ÄÊâÄÁöÑ„Å™ÂØÜÂ∫¶„Å´Âü∫„Å•„ÅÑ„Å¶Áï∞Â∏∏„ÇíÊ§úÂá∫„Åô„ÇãÊâãÊ≥ï„Åß„Åô„ÄÇ2000Âπ¥„Å´Breunig et al.„Å´„Çà„Å£„Å¶ÊèêÊ°à„Åï„Çå„Åæ„Åó„Åü„ÄÇ</p>

<h3>3.2.1 ÂØÜÂ∫¶„Éô„Éº„ÇπÁï∞Â∏∏Ê§úÁü•</h3>

<p><strong>Âü∫Êú¨ÂéüÁêÜ:</strong></p>
<ul>
<li>Ê≠£Â∏∏„Éá„Éº„Çø„ÅØÈ´òÂØÜÂ∫¶È†òÂüü„Å´Â≠òÂú®„Åô„Çã</li>
<li>Áï∞Â∏∏„Éá„Éº„Çø„ÅØ‰ΩéÂØÜÂ∫¶È†òÂüü„Å´Â≠òÂú®„Åô„Çã</li>
<li>ÂêÑÁÇπ„ÅÆÂØÜÂ∫¶„ÇíËøëÂÇçÁÇπ„ÅÆÂØÜÂ∫¶„Å®ÊØîËºÉ„Åó„Å¶Áï∞Â∏∏Â∫¶„ÇíË®àÁÆó</li>
</ul>

<p><strong>„Å™„Åú„ÄåÂ±ÄÊâÄÁöÑ„Äç„Åã:</strong></p>
<ul>
<li>„Ç∞„É≠„Éº„Éê„É´„Å™ÂØÜÂ∫¶„Åß„ÅØÊ§úÂá∫„Åß„Åç„Å™„ÅÑÁï∞Â∏∏„ÇÇÊ§úÂá∫ÂèØËÉΩ</li>
<li>ÂØÜÂ∫¶„ÅåÁï∞„Å™„ÇãË§áÊï∞„ÅÆ„ÇØ„É©„Çπ„Çø„ÅåÂ≠òÂú®„Åô„ÇãÂ†¥Âêà„Å´ÊúâÂäπ</li>
<li>ÂêÑÁÇπ„ÅÆÂë®Ëæ∫Áí∞Â¢É„ÇíËÄÉÊÖÆ„Åó„ÅüÁõ∏ÂØæÁöÑ„Å™Áï∞Â∏∏Â∫¶„ÇíÁÆóÂá∫</li>
</ul>

<h3>3.2.2 Â±ÄÊâÄÂà∞ÈÅîÂèØËÉΩÂØÜÂ∫¶ÔºàLocal Reachability DensityÔºâ</h3>

<p><strong>kË∑ùÈõ¢Ôºàk-distanceÔºâ:</strong></p>

<p>ÁÇπ $p$ „Åã„Çâ kÁï™ÁõÆ„Å´Ëøë„ÅÑÁÇπ„Åæ„Åß„ÅÆË∑ùÈõ¢„Çí $d_k(p)$ „Å®„Åó„Åæ„Åô„ÄÇ</p>

<p><strong>Âà∞ÈÅîÂèØËÉΩË∑ùÈõ¢ÔºàReachability DistanceÔºâ:</strong></p>

$$
\text{reach-dist}_k(p, o) = \max\{d_k(o), d(p, o)\}
$$

<ul>
<li>$d(p, o)$: ÁÇπ $p$ „Å® $o$ Èñì„ÅÆÂÆüÈöõ„ÅÆË∑ùÈõ¢</li>
<li>ËøëÂÇçÁÇπ $o$ „ÅåÂØÜ„Å™Â†¥Âêà„ÄÅÂà∞ÈÅîÂèØËÉΩË∑ùÈõ¢„ÅØ $d_k(o)$ „Åß‰∏ãÈôê„ÅåË®≠ÂÆö„Åï„Çå„Çã</li>
</ul>

<p><strong>Â±ÄÊâÄÂà∞ÈÅîÂèØËÉΩÂØÜÂ∫¶ÔºàLRDÔºâ:</strong></p>

$$
\text{LRD}_k(p) = \frac{1}{\frac{\sum_{o \in N_k(p)} \text{reach-dist}_k(p, o)}{|N_k(p)|}}
$$

<ul>
<li>$N_k(p)$: ÁÇπ $p$ „ÅÆ kËøëÂÇçÁÇπ„ÅÆÈõÜÂêà</li>
<li>Âà∞ÈÅîÂèØËÉΩË∑ùÈõ¢„ÅÆÂπ≥Âùá„ÅÆÈÄÜÊï∞ = ÂØÜÂ∫¶</li>
</ul>

<h3>3.2.3 LOF„Çπ„Ç≥„Ç¢„ÅÆË®àÁÆó</h3>

<p><strong>LOFÔºàLocal Outlier FactorÔºâ:</strong></p>

$$
\text{LOF}_k(p) = \frac{\sum_{o \in N_k(p)} \frac{\text{LRD}_k(o)}{\text{LRD}_k(p)}}{|N_k(p)|}
$$

<p><strong>„Çπ„Ç≥„Ç¢„ÅÆËß£Èáà:</strong></p>
<ul>
<li>$\text{LOF} \approx 1$: Ê≠£Â∏∏ÔºàËøëÂÇç„Å®ÂêåÁ®ãÂ∫¶„ÅÆÂØÜÂ∫¶Ôºâ</li>
<li>$\text{LOF} \gg 1$: Áï∞Â∏∏ÔºàËøëÂÇç„Çà„ÇäÂØÜÂ∫¶„Åå‰Ωé„ÅÑÔºâ</li>
<li>$\text{LOF} < 1$: Ê≠£Â∏∏ÔºàËøëÂÇç„Çà„ÇäÂØÜÂ∫¶„ÅåÈ´ò„ÅÑÔºâ</li>
</ul>

<p>‰∏ÄËà¨ÁöÑ„Å´ $\text{LOF} > 1.5$ „ÇíÁï∞Â∏∏„Å®„Åø„Å™„Åó„Åæ„Åô„ÄÇ</p>

<h3>3.2.4 ÂÆåÂÖ®„Å™ÂÆüË£Ö‰æã</h3>

<p><strong>Âü∫Êú¨ÂÆüË£Ö:</strong></p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import LocalOutlierFactor
from sklearn.datasets import make_moons

# „Çµ„É≥„Éó„É´„Éá„Éº„ÇøÁîüÊàêÔºàÊúàÂûã„Éá„Éº„Çø + Áï∞Â∏∏ÁÇπÔºâ
np.random.seed(42)
X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)
X_outliers = np.random.uniform(low=-1, high=2, size=(20, 2))
X = np.vstack([X, X_outliers])

# LOF„É¢„Éá„É´
lof = LocalOutlierFactor(
    n_neighbors=20,  # ËøëÂÇçÁÇπÊï∞
    contamination=0.1,  # Áï∞Â∏∏Áéá
    novelty=False  # Êñ∞Ë¶è„Éá„Éº„Çø‰∫àÊ∏¨„Å´„ÅØTrue
)

# ‰∫àÊ∏¨
y_pred = lof.fit_predict(X)  # -1: Áï∞Â∏∏„ÄÅ1: Ê≠£Â∏∏
scores = lof.negative_outlier_factor_  # Ë≤†„ÅÆÁï∞Â∏∏Â∫¶„Çπ„Ç≥„Ç¢Ôºà‰Ωé„ÅÑ„Åª„Å©Áï∞Â∏∏Ôºâ

# ÂèØË¶ñÂåñ
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='coolwarm', edgecolors='k')
plt.title('LOF: Áï∞Â∏∏Ê§úÁü•ÁµêÊûú')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Prediction (-1: Anomaly, 1: Normal)')

plt.subplot(1, 2, 2)
plt.scatter(X[:, 0], X[:, 1], c=scores, cmap='viridis', edgecolors='k')
plt.title('LOF: Áï∞Â∏∏„Çπ„Ç≥„Ç¢')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Negative Outlier Factor')

plt.tight_layout()
plt.show()

print(f"Ê§úÂá∫„Åï„Çå„ÅüÁï∞Â∏∏„Éá„Éº„ÇøÊï∞: {np.sum(y_pred == -1)}")
print(f"Áï∞Â∏∏„Çπ„Ç≥„Ç¢ÁØÑÂõ≤: [{scores.min():.3f}, {scores.max():.3f}]")
</code></pre>

<p><strong>n_neighbors„Éë„É©„É°„Éº„Çø„ÅÆÂΩ±Èüø:</strong></p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import LocalOutlierFactor

# „Éá„Éº„ÇøÁîüÊàê
np.random.seed(42)
X_normal = np.random.randn(200, 2) * 0.5
X_outliers = np.random.uniform(low=-3, high=3, size=(10, 2))
X = np.vstack([X_normal, X_outliers])

# Áï∞„Å™„Çãn_neighbors„ÅßÊØîËºÉ
n_neighbors_list = [5, 20, 50]

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for idx, n_neighbors in enumerate(n_neighbors_list):
    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=0.1)
    y_pred = lof.fit_predict(X)

    axes[idx].scatter(X[:, 0], X[:, 1], c=y_pred, cmap='coolwarm', edgecolors='k')
    axes[idx].set_title(f'LOF (n_neighbors={n_neighbors})')
    axes[idx].set_xlabel('Feature 1')
    axes[idx].set_ylabel('Feature 2')

    anomaly_count = np.sum(y_pred == -1)
    axes[idx].text(0.05, 0.95, f'Anomalies: {anomaly_count}',
                   transform=axes[idx].transAxes, verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>Êñ∞Ë¶è„Éá„Éº„Çø„ÅÆÁï∞Â∏∏Ê§úÁü•Ôºànovelty=TrueÔºâ:</strong></p>

<pre><code class="language-python">from sklearn.neighbors import LocalOutlierFactor
from sklearn.model_selection import train_test_split

# „Éá„Éº„ÇøÊ∫ñÂÇô
np.random.seed(42)
X_train = np.random.randn(500, 2) * 0.5  # Ê≠£Â∏∏„Éá„Éº„Çø„ÅÆ„Åø
X_test_normal = np.random.randn(100, 2) * 0.5
X_test_outliers = np.random.uniform(low=-3, high=3, size=(10, 2))
X_test = np.vstack([X_test_normal, X_test_outliers])

# LOFÔºànovelty=True: Êñ∞Ë¶è„Éá„Éº„Çø‰∫àÊ∏¨„É¢„Éº„ÉâÔºâ
lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1, novelty=True)
lof.fit(X_train)  # Ê≠£Â∏∏„Éá„Éº„Çø„ÅÆ„Åø„ÅßÂ≠¶Áøí

# Êñ∞Ë¶è„Éá„Éº„Çø„ÅÆ‰∫àÊ∏¨
y_pred = lof.predict(X_test)
scores = lof.score_samples(X_test)

# ÂèØË¶ñÂåñ
plt.figure(figsize=(10, 6))
plt.scatter(X_train[:, 0], X_train[:, 1], alpha=0.3, label='Training Data', color='blue')
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap='coolwarm',
            edgecolors='k', s=100, label='Test Data')
plt.title('LOF: Êñ∞Ë¶è„Éá„Éº„Çø„ÅÆÁï∞Â∏∏Ê§úÁü•Ôºànovelty=TrueÔºâ')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.colorbar(label='Prediction (-1: Anomaly, 1: Normal)')
plt.show()

print(f"Ê§úÂá∫„Åï„Çå„ÅüÁï∞Â∏∏„Éá„Éº„ÇøÊï∞: {np.sum(y_pred == -1)}/{len(y_pred)}")
</code></pre>

<hr />

<h2>3.3 One-Class SVM</h2>

<p>One-Class SVM„ÅØ„ÄÅÊ≠£Â∏∏„Éá„Éº„Çø„ÅÆÂ¢ÉÁïå„ÇíÂ≠¶Áøí„Åó„ÄÅ„Åù„ÅÆÂ¢ÉÁïå„ÅÆÂ§ñÂÅ¥„Å´„ÅÇ„Çã„Éá„Éº„Çø„ÇíÁï∞Â∏∏„Å®„Åó„Å¶Ê§úÂá∫„Åô„ÇãÊâãÊ≥ï„Åß„Åô„ÄÇ</p>

<h3>3.3.1 ÊúÄÂ§ß„Éû„Éº„Ç∏„É≥Ë∂ÖÂπ≥Èù¢</h3>

<p><strong>Âü∫Êú¨ÂéüÁêÜ:</strong></p>
<ul>
<li>Ê≠£Â∏∏„Éá„Éº„Çø„ÇíÂéüÁÇπ„Åã„ÇâÊúÄ„ÇÇ„Çà„ÅèÂàÜÈõ¢„Åô„ÇãË∂ÖÂπ≥Èù¢„ÇíË¶ã„Å§„Åë„Çã</li>
<li>Ë∂ÖÂπ≥Èù¢„Å®„Éá„Éº„ÇøÁÇπ„ÅÆÈñì„ÅÆ„Éû„Éº„Ç∏„É≥„ÇíÊúÄÂ§ßÂåñ</li>
<li>„Ç´„Éº„Éç„É´„Éà„É™„ÉÉ„ÇØ„ÅßÈùûÁ∑öÂΩ¢„Å™Â¢ÉÁïå„ÇíÂ≠¶Áøí</li>
</ul>

<p><strong>Êï∞ÂºèÂÆöÁæ©:</strong></p>

<p>Ê±∫ÂÆöÈñ¢Êï∞:</p>

$$
f(x) = \text{sign}(w \cdot \phi(x) - \rho)
$$

<ul>
<li>$w$: Ê≥ïÁ∑ö„Éô„ÇØ„Éà„É´</li>
<li>$\phi(x)$: „Ç´„Éº„Éç„É´Â§âÊèõÂæå„ÅÆÁâπÂæ¥„Éô„ÇØ„Éà„É´</li>
<li>$\rho$: „Éê„Ç§„Ç¢„ÇπÈ†Ö</li>
</ul>

<p>ÊúÄÈÅ©ÂåñÂïèÈ°å:</p>

$$
\min_{w, \rho, \xi} \frac{1}{2} \|w\|^2 + \frac{1}{\nu n} \sum_{i=1}^{n} \xi_i - \rho
$$

<p>Âà∂Á¥Ñ:</p>

$$
w \cdot \phi(x_i) \geq \rho - \xi_i, \quad \xi_i \geq 0
$$

<h3>3.3.2 „Ç´„Éº„Éç„É´„Éà„É™„ÉÉ„ÇØ</h3>

<p><strong>Á∑öÂΩ¢„Ç´„Éº„Éç„É´:</strong></p>

$$
K(x, x') = x \cdot x'
$$

<ul>
<li>È´òÈÄü„ÄÅËß£Èáà„Åó„ÇÑ„Åô„ÅÑ</li>
<li>Á∑öÂΩ¢ÂàÜÈõ¢ÂèØËÉΩ„Å™„Éá„Éº„Çø„Å´ÈÅ©Áî®</li>
</ul>

<p><strong>RBFÔºà„Ç¨„Ç¶„ÇπÔºâ„Ç´„Éº„Éç„É´:</strong></p>

$$
K(x, x') = \exp\left(-\gamma \|x - x'\|^2\right)
$$

<ul>
<li>ÈùûÁ∑öÂΩ¢Â¢ÉÁïå„ÇíÂ≠¶ÁøíÂèØËÉΩ</li>
<li>ÊúÄ„ÇÇ„Çà„Åè‰Ωø„Çè„Çå„Çã„Ç´„Éº„Éç„É´</li>
<li>$\gamma$: „Ç´„Éº„Éç„É´ÂπÖÔºàÂ§ß„Åç„ÅÑ„Åª„Å©Ë§áÈõë„Å™Â¢ÉÁïåÔºâ</li>
</ul>

<p><strong>Â§öÈ†ÖÂºè„Ç´„Éº„Éç„É´:</strong></p>

$$
K(x, x') = (\gamma x \cdot x' + r)^d
$$

<ul>
<li>Ê¨°Êï∞ $d$ „ÅÆÂ§öÈ†ÖÂºèÂ¢ÉÁïå</li>
<li>RBF„Çà„ÇäÂà∂Á¥Ñ„ÅåÂº∑„ÅÑ</li>
</ul>

<h3>3.3.3 nu„Éë„É©„É°„Éº„Çø</h3>

<p><strong>nu„ÅÆÊÑèÂë≥:</strong></p>

<p>$\nu \in (0, 1]$ „ÅØ‰ª•‰∏ã„ÅÆ2„Å§„ÅÆÈáè„ÅÆ‰∏äÈôê„Å®‰∏ãÈôê„ÇíÂà∂Âæ°„Åó„Åæ„Åô:</p>
<ul>
<li>Ë®ìÁ∑¥„Éá„Éº„Çø„Å´„Åä„Åë„ÇãÁï∞Â∏∏ÂÄ§„ÅÆÂâ≤Âêà„ÅÆ<strong>‰∏äÈôê</strong></li>
<li>„Çµ„Éù„Éº„Éà„Éô„ÇØ„Çø„Éº„ÅÆÂâ≤Âêà„ÅÆ<strong>‰∏ãÈôê</strong></li>
</ul>

<p><strong>Êé®Â•®ÂÄ§:</strong></p>
<ul>
<li>$\nu = 0.1$: 10%„ÅåÁï∞Â∏∏„Å®ÊÉ≥ÂÆö</li>
<li>$\nu = 0.05$: 5%„ÅåÁï∞Â∏∏„Å®ÊÉ≥ÂÆö</li>
<li>$\nu = 0.01$: 1%„ÅåÁï∞Â∏∏„Å®ÊÉ≥ÂÆö</li>
</ul>

<p><strong>Ê≥®ÊÑèÁÇπ:</strong></p>
<ul>
<li>$\nu$ „ÇíÂ∞è„Åï„Åè„Åó„Åô„Åé„Çã„Å®„ÄÅ„Åª„Å®„Çì„Å©Áï∞Â∏∏„ÅåÊ§úÂá∫„Åï„Çå„Å™„ÅÑ</li>
<li>$\nu$ „ÇíÂ§ß„Åç„Åè„Åó„Åô„Åé„Çã„Å®„ÄÅÊ≠£Â∏∏„Éá„Éº„Çø„ÇÇÁï∞Â∏∏„Å®Âà§ÂÆö„Åï„Çå„Çã</li>
<li>„Éâ„É°„Ç§„É≥Áü•Ë≠ò„ÇÑ‰∫ãÂâç„ÅÆÁï∞Â∏∏Áéá„Åã„ÇâË®≠ÂÆö</li>
</ul>

<h3>3.3.4 scikit-learnÂÆüË£Ö</h3>

<p><strong>Âü∫Êú¨ÂÆüË£Ö:</strong></p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import OneClassSVM

# „Éá„Éº„ÇøÁîüÊàê
np.random.seed(42)
X_train = np.random.randn(200, 2) * 0.5  # Ê≠£Â∏∏„Éá„Éº„Çø
X_test_normal = np.random.randn(50, 2) * 0.5
X_test_outliers = np.random.uniform(low=-3, high=3, size=(10, 2))
X_test = np.vstack([X_test_normal, X_test_outliers])

# One-Class SVM
oc_svm = OneClassSVM(
    kernel='rbf',  # RBF„Ç´„Éº„Éç„É´
    gamma='auto',  # gamma = 1 / n_features
    nu=0.1  # 10%„ÅåÁï∞Â∏∏„Å®ÊÉ≥ÂÆö
)

# Â≠¶Áøí
oc_svm.fit(X_train)

# ‰∫àÊ∏¨
y_pred_train = oc_svm.predict(X_train)
y_pred_test = oc_svm.predict(X_test)
scores_test = oc_svm.decision_function(X_test)

# Ê±∫ÂÆöÂ¢ÉÁïå„ÅÆÂèØË¶ñÂåñ
xx, yy = np.meshgrid(np.linspace(-3, 3, 500), np.linspace(-3, 3, 500))
Z = oc_svm.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.Blues_r)
plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')
plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=20, edgecolors='k', label='Training')
plt.title('One-Class SVM: Ê±∫ÂÆöÂ¢ÉÁïå')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()

plt.subplot(1, 2, 2)
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_test, cmap='coolwarm',
            edgecolors='k', s=100)
plt.title('One-Class SVM: „ÉÜ„Çπ„Éà„Éá„Éº„Çø‰∫àÊ∏¨')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Prediction (-1: Anomaly, 1: Normal)')

plt.tight_layout()
plt.show()

print(f"Ë®ìÁ∑¥„Éá„Éº„Çø„ÅÆÁï∞Â∏∏Êï∞: {np.sum(y_pred_train == -1)}/{len(y_pred_train)}")
print(f"„ÉÜ„Çπ„Éà„Éá„Éº„Çø„ÅÆÁï∞Â∏∏Êï∞: {np.sum(y_pred_test == -1)}/{len(y_pred_test)}")
</code></pre>

<p><strong>gamma„Éë„É©„É°„Éº„Çø„ÅÆÂΩ±Èüø:</strong></p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import OneClassSVM

# „Éá„Éº„ÇøÁîüÊàê
np.random.seed(42)
X_train = np.random.randn(200, 2) * 0.5

# Áï∞„Å™„Çãgamma„ÅßÊØîËºÉ
gamma_list = [0.01, 0.1, 1.0]

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for idx, gamma in enumerate(gamma_list):
    oc_svm = OneClassSVM(kernel='rbf', gamma=gamma, nu=0.1)
    oc_svm.fit(X_train)

    # Ê±∫ÂÆöÂ¢ÉÁïå
    xx, yy = np.meshgrid(np.linspace(-3, 3, 300), np.linspace(-3, 3, 300))
    Z = oc_svm.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    axes[idx].contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.Blues_r)
    axes[idx].contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')
    axes[idx].scatter(X_train[:, 0], X_train[:, 1], c='white', s=20, edgecolors='k')
    axes[idx].set_title(f'One-Class SVM (gamma={gamma})')
    axes[idx].set_xlabel('Feature 1')
    axes[idx].set_ylabel('Feature 2')

plt.tight_layout()
plt.show()
</code></pre>

<hr />

<h2>3.4 „Åù„ÅÆ‰ªñ„ÅÆÊ©üÊ¢∞Â≠¶ÁøíÊâãÊ≥ï</h2>

<h3>3.4.1 DBSCANÔºàÂØÜÂ∫¶„Éô„Éº„Çπ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞Ôºâ</h3>

<p><strong>ÂéüÁêÜ:</strong></p>
<ul>
<li>ÂØÜÂ∫¶„ÅÆÈ´ò„ÅÑÈ†òÂüü„Çí„ÇØ„É©„Çπ„Çø„Å®„Åó„Å¶Ê§úÂá∫</li>
<li>„Å©„ÅÆ„ÇØ„É©„Çπ„Çø„Å´„ÇÇÂ±û„Åï„Å™„ÅÑÁÇπ„Çí„Éé„Ç§„Ç∫ÔºàÁï∞Â∏∏Ôºâ„Å®„Åø„Å™„Åô</li>
<li>„ÇØ„É©„Çπ„ÇøÊï∞„Çí‰∫ãÂâç„Å´ÊåáÂÆö„Åô„ÇãÂøÖË¶Å„Åå„Å™„ÅÑ</li>
</ul>

<p><strong>‰∏ªË¶Å„Éë„É©„É°„Éº„Çø:</strong></p>
<ul>
<li><code>eps</code>: ËøëÂÇç„ÅÆÂçäÂæÑÔºàË∑ùÈõ¢„ÅÆÈñæÂÄ§Ôºâ</li>
<li><code>min_samples</code>: „Ç≥„Ç¢ÁÇπ„Å´„Å™„Çã„Åü„ÇÅ„ÅÆÊúÄÂ∞èËøëÂÇçÁÇπÊï∞</li>
</ul>

<p><strong>ÂÆüË£Ö‰æã:</strong></p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN

# „Éá„Éº„ÇøÁîüÊàê
np.random.seed(42)
X_cluster1 = np.random.randn(100, 2) * 0.3 + [0, 0]
X_cluster2 = np.random.randn(100, 2) * 0.3 + [3, 3]
X_outliers = np.random.uniform(low=-2, high=5, size=(20, 2))
X = np.vstack([X_cluster1, X_cluster2, X_outliers])

# DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X)

# „É©„Éô„É´-1„Åå„Éé„Ç§„Ç∫ÔºàÁï∞Â∏∏Ôºâ
outliers = labels == -1

# ÂèØË¶ñÂåñ
plt.figure(figsize=(10, 6))
plt.scatter(X[~outliers, 0], X[~outliers, 1], c=labels[~outliers],
            cmap='viridis', edgecolors='k', label='Clusters')
plt.scatter(X[outliers, 0], X[outliers, 1], c='red', marker='x',
            s=100, label='Outliers (Anomalies)')
plt.title('DBSCAN: ÂØÜÂ∫¶„Éô„Éº„ÇπÁï∞Â∏∏Ê§úÁü•')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.show()

print(f"Ê§úÂá∫„Åï„Çå„Åü„ÇØ„É©„Çπ„ÇøÊï∞: {len(set(labels)) - (1 if -1 in labels else 0)}")
print(f"Áï∞Â∏∏„Éá„Éº„ÇøÊï∞: {np.sum(outliers)}")
</code></pre>

<h3>3.4.2 Elliptic EnvelopeÔºàÊ•ïÂÜÜ„Ç®„É≥„Éô„É≠„Éº„ÉóÔºâ</h3>

<p><strong>ÂéüÁêÜ:</strong></p>
<ul>
<li>Ê≠£Ë¶èÂàÜÂ∏É„Çí‰ªÆÂÆö„Åó„ÄÅ„Éá„Éº„Çø„ÅÆ‰∏≠ÂøÉ„Å®ÂÖ±ÂàÜÊï£„ÇíÊé®ÂÆö</li>
<li>„Éû„Éè„É©„Éé„Éì„ÇπË∑ùÈõ¢„ÅßÁï∞Â∏∏„ÇíÊ§úÂá∫</li>
<li>„É≠„Éê„Çπ„ÉàÊé®ÂÆöÔºàMinimum Covariance DeterminantÔºâ„ÅßÂ§ñ„ÇåÂÄ§„ÅÆÂΩ±Èüø„ÇíÊäë„Åà„Çã</li>
</ul>

<p><strong>ÂÆüË£Ö‰æã:</strong></p>

<pre><code class="language-python">from sklearn.covariance import EllipticEnvelope
import numpy as np
import matplotlib.pyplot as plt

# „Éá„Éº„ÇøÁîüÊàê
np.random.seed(42)
X_normal = np.random.randn(200, 2)
X_outliers = np.random.uniform(low=-5, high=5, size=(10, 2))
X = np.vstack([X_normal, X_outliers])

# Elliptic Envelope
elliptic = EllipticEnvelope(contamination=0.1, random_state=42)
y_pred = elliptic.fit_predict(X)

# ÂèØË¶ñÂåñ
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='coolwarm', edgecolors='k')
plt.title('Elliptic Envelope: Áï∞Â∏∏Ê§úÁü•')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Prediction (-1: Anomaly, 1: Normal)')
plt.show()

print(f"Ê§úÂá∫„Åï„Çå„ÅüÁï∞Â∏∏„Éá„Éº„ÇøÊï∞: {np.sum(y_pred == -1)}")
</code></pre>

<h3>3.4.3 Robust CovarianceÔºà„É≠„Éê„Çπ„ÉàÂÖ±ÂàÜÊï£Êé®ÂÆöÔºâ</h3>

<p><strong>Minimum Covariance DeterminantÔºàMCDÔºâ:</strong></p>
<ul>
<li>ÂÖ±ÂàÜÊï£Ë°åÂàó„ÅÆË°åÂàóÂºè„ÇíÊúÄÂ∞èÂåñ„Åô„Çã„Çµ„Éñ„Çª„ÉÉ„Éà„ÇíÊé¢Á¥¢</li>
<li>Â§ñ„ÇåÂÄ§„Å´ÂØæ„Åó„Å¶„É≠„Éê„Çπ„Éà„Å™Êé®ÂÆö</li>
<li>„Éû„Éè„É©„Éé„Éì„ÇπË∑ùÈõ¢„ÅÆË®àÁÆó„Å´‰ΩøÁî®</li>
</ul>

<pre><code class="language-python">from sklearn.covariance import MinCovDet
import numpy as np

# „Éá„Éº„ÇøÁîüÊàê
np.random.seed(42)
X = np.random.randn(100, 2)
X[:5] = X[:5] + 5  # Â§ñ„ÇåÂÄ§ËøΩÂä†

# MCDÊé®ÂÆö
mcd = MinCovDet(random_state=42)
mcd.fit(X)

# „Éû„Éè„É©„Éé„Éì„ÇπË∑ùÈõ¢„ÅÆË®àÁÆó
distances = mcd.mahalanobis(X)

# Áï∞Â∏∏Âà§ÂÆöÔºà„Ç´„Ç§‰∫å‰πóÂàÜÂ∏É„ÅÆ95„Éë„Éº„Çª„É≥„Çø„Ç§„É´Ôºâ
from scipy import stats
threshold = stats.chi2.ppf(0.95, df=2)
outliers = distances > threshold

print(f"Áï∞Â∏∏„Éá„Éº„ÇøÊï∞: {np.sum(outliers)}")
print(f"Ë∑ùÈõ¢„ÅÆÈñæÂÄ§: {threshold:.2f}")
</code></pre>

<h3>3.4.4 PyOD„É©„Ç§„Éñ„É©„É™</h3>

<p><strong>PyODÔºàPython Outlier DetectionÔºâ</strong>„ÅØ„ÄÅÁï∞Â∏∏Ê§úÁü•Â∞ÇÈñÄ„ÅÆ„É©„Ç§„Éñ„É©„É™„Åß40‰ª•‰∏ä„ÅÆ„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÇíÊèê‰æõ„Åó„Åæ„Åô„ÄÇ</p>

<p><strong>„Ç§„É≥„Çπ„Éà„Éº„É´:</strong></p>

<pre><code class="language-bash">pip install pyod
</code></pre>

<p><strong>‰ΩøÁî®‰æã:</strong></p>

<pre><code class="language-python">from pyod.models.knn import KNN
from pyod.models.iforest import IForest
from pyod.models.lof import LOF
from pyod.utils.data import generate_data
from pyod.utils.utility import standardizer
import numpy as np

# „Éá„Éº„ÇøÁîüÊàê
X_train, X_test, y_train, y_test = generate_data(
    n_train=200, n_test=100, n_features=2,
    contamination=0.1, random_state=42
)

# „Éá„Éº„ÇøÊ®ôÊ∫ñÂåñ
X_train = standardizer(X_train)
X_test = standardizer(X_test)

# Ë§áÊï∞„É¢„Éá„É´„ÅÆÊØîËºÉ
models = {
    'KNN': KNN(contamination=0.1),
    'IForest': IForest(contamination=0.1, random_state=42),
    'LOF': LOF(contamination=0.1)
}

for name, model in models.items():
    model.fit(X_train)
    y_pred = model.predict(X_test)
    scores = model.decision_function(X_test)

    # Ë©ï‰æ°Ôºà‰ªÆÊÉ≥ÁöÑ„Å™Ê≠£Ëß£„É©„Éô„É´„ÅßÔºâ
    from sklearn.metrics import roc_auc_score
    auc = roc_auc_score(y_test, scores)

    print(f"{name}:")
    print(f"  AUC-ROC: {auc:.3f}")
    print(f"  Ê§úÂá∫Áï∞Â∏∏Êï∞: {np.sum(y_pred == 1)}")
    print()
</code></pre>

<p><strong>Âá∫Âäõ‰æã:</strong></p>
<pre><code>KNN:
  AUC-ROC: 0.892
  Ê§úÂá∫Áï∞Â∏∏Êï∞: 10

IForest:
  AUC-ROC: 0.915
  Ê§úÂá∫Áï∞Â∏∏Êï∞: 10

LOF:
  AUC-ROC: 0.903
  Ê§úÂá∫Áï∞Â∏∏Êï∞: 10
</code></pre>

<hr />

<h2>3.5 „Ç¢„É≥„Çµ„É≥„Éñ„É´Áï∞Â∏∏Ê§úÁü•</h2>

<p>Ë§áÊï∞„ÅÆÁï∞Â∏∏Ê§úÁü•„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„Çã„Åì„Å®„Åß„ÄÅ„Çà„ÇäÈ´òÁ≤æÂ∫¶„ÅßÂÆâÂÆö„Åó„ÅüÁï∞Â∏∏Ê§úÁü•„ÅåÂèØËÉΩ„Å´„Å™„Çä„Åæ„Åô„ÄÇ</p>

<h3>3.5.1 Feature Bagging</h3>

<p><strong>ÂéüÁêÜ:</strong></p>
<ul>
<li>ÁâπÂæ¥Èáè„ÅÆ„Çµ„Éñ„Çª„ÉÉ„Éà„Çí„É©„É≥„ÉÄ„É†„Å´ÈÅ∏Êäû</li>
<li>ÂêÑ„Çµ„Éñ„Çª„ÉÉ„Éà„ÅßÁï∞Â∏∏Ê§úÁü•„É¢„Éá„É´„ÇíË®ìÁ∑¥</li>
<li>Ë§áÊï∞„É¢„Éá„É´„ÅÆ‰∫àÊ∏¨„ÇíÈõÜÁ¥Ñ</li>
</ul>

<p><strong>ÂÆüË£Ö‰æã:</strong></p>

<pre><code class="language-python">from pyod.models.feature_bagging import FeatureBagging
from pyod.models.lof import LOF
from pyod.utils.data import generate_data
from sklearn.metrics import roc_auc_score

# „Éá„Éº„ÇøÁîüÊàê
X_train, X_test, y_train, y_test = generate_data(
    n_train=200, n_test=100, n_features=10,
    contamination=0.1, random_state=42
)

# Feature BaggingÔºà„Éô„Éº„Çπ„É¢„Éá„É´: LOFÔºâ
fb = FeatureBagging(
    base_estimator=LOF(),
    n_estimators=10,  # „É¢„Éá„É´Êï∞
    contamination=0.1,
    random_state=42
)

# Â≠¶Áøí„Å®‰∫àÊ∏¨
fb.fit(X_train)
y_pred = fb.predict(X_test)
scores = fb.decision_function(X_test)

# Ë©ï‰æ°
auc = roc_auc_score(y_test, scores)
print(f"Feature Bagging AUC-ROC: {auc:.3f}")
print(f"Ê§úÂá∫Áï∞Â∏∏Êï∞: {np.sum(y_pred == 1)}")
</code></pre>

<h3>3.5.2 „É¢„Éá„É´Âπ≥ÂùáÂåñÔºàModel AveragingÔºâ</h3>

<p><strong>ÂéüÁêÜ:</strong></p>
<ul>
<li>Ë§áÊï∞„ÅÆÁï∞„Å™„Çã„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÇíË®ìÁ∑¥</li>
<li>ÂêÑ„É¢„Éá„É´„ÅÆÁï∞Â∏∏„Çπ„Ç≥„Ç¢„ÇíÂπ≥ÂùáÂåñ</li>
<li>Âçò‰∏Ä„É¢„Éá„É´„Çà„ÇäÈ†ëÂÅ•„Å™‰∫àÊ∏¨</li>
</ul>

<p><strong>ÂÆüË£Ö‰æã:</strong></p>

<pre><code class="language-python">from pyod.models.combination import average, maximization
from pyod.models.knn import KNN
from pyod.models.iforest import IForest
from pyod.models.lof import LOF
from pyod.utils.data import generate_data
import numpy as np

# „Éá„Éº„ÇøÁîüÊàê
X_train, X_test, y_train, y_test = generate_data(
    n_train=200, n_test=100, n_features=5,
    contamination=0.1, random_state=42
)

# Ë§áÊï∞„É¢„Éá„É´„ÅÆË®ìÁ∑¥
models = [
    KNN(contamination=0.1),
    IForest(contamination=0.1, random_state=42),
    LOF(contamination=0.1)
]

# ÂêÑ„É¢„Éá„É´„ÅÆ„Çπ„Ç≥„Ç¢„ÇíË®àÁÆó
scores_list = []
for model in models:
    model.fit(X_train)
    scores = model.decision_function(X_test)
    scores_list.append(scores)

scores_array = np.array(scores_list)

# „Çπ„Ç≥„Ç¢ÈõÜÁ¥ÑÔºàÂπ≥ÂùáÔºâ
scores_avg = average(scores_array)

# „Çπ„Ç≥„Ç¢ÈõÜÁ¥ÑÔºàÊúÄÂ§ßÂÄ§Ôºâ
scores_max = maximization(scores_array)

# Ë©ï‰æ°
from sklearn.metrics import roc_auc_score
auc_avg = roc_auc_score(y_test, scores_avg)
auc_max = roc_auc_score(y_test, scores_max)

print(f"Average Combination AUC-ROC: {auc_avg:.3f}")
print(f"Maximum Combination AUC-ROC: {auc_max:.3f}")
</code></pre>

<h3>3.5.3 Isolation-Based Ensemble</h3>

<p><strong>LSCPÔºàLocally Selective Combination in ParallelÔºâ:</strong></p>
<ul>
<li>ÂêÑ„ÉÜ„Çπ„Éà„Çµ„É≥„Éó„É´„Å´ÂØæ„Åó„Å¶Â±ÄÊâÄÁöÑ„Å´ÊúÄÈÅ©„Å™„É¢„Éá„É´„ÇíÈÅ∏Êäû</li>
<li>ËøëÂÇç„Åß„ÅÆÊÄßËÉΩ„Å´Âü∫„Å•„ÅÑ„Å¶„É¢„Éá„É´„ÇíÈáç„Åø‰ªò„Åë</li>
<li>„Ç∞„É≠„Éº„Éê„É´„Å™Âπ≥ÂùáÂåñ„Çà„ÇäÈ´òÁ≤æÂ∫¶</li>
</ul>

<pre><code class="language-python">from pyod.models.lscp import LSCP
from pyod.models.knn import KNN
from pyod.models.iforest import IForest
from pyod.models.lof import LOF
from pyod.utils.data import generate_data

# „Éá„Éº„ÇøÁîüÊàê
X_train, X_test, y_train, y_test = generate_data(
    n_train=200, n_test=100, n_features=5,
    contamination=0.1, random_state=42
)

# „Éô„Éº„Çπ„É¢„Éá„É´„ÅÆ„É™„Çπ„Éà
detector_list = [
    KNN(),
    IForest(random_state=42),
    LOF()
]

# LSCP
lscp = LSCP(detector_list, contamination=0.1, random_state=42)
lscp.fit(X_train)

# ‰∫àÊ∏¨
y_pred = lscp.predict(X_test)
scores = lscp.decision_function(X_test)

# Ë©ï‰æ°
from sklearn.metrics import roc_auc_score
auc = roc_auc_score(y_test, scores)
print(f"LSCP AUC-ROC: {auc:.3f}")
print(f"Ê§úÂá∫Áï∞Â∏∏Êï∞: {np.sum(y_pred == 1)}")
</code></pre>

<h3>3.5.4 ÂÆåÂÖ®„Å™„Éë„Ç§„Éó„É©„Ç§„É≥‰æã</h3>

<p><strong>„Éá„Éº„ÇøÂâçÂá¶ÁêÜ ‚Üí Ë§áÊï∞„É¢„Éá„É´Ë®ìÁ∑¥ ‚Üí „Ç¢„É≥„Çµ„É≥„Éñ„É´ ‚Üí Ë©ï‰æ°:</strong></p>

<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from pyod.models.knn import KNN
from pyod.models.iforest import IForest
from pyod.models.lof import LOF
from pyod.models.ocsvm import OCSVM
from pyod.models.combination import average
from sklearn.metrics import classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# „Éá„Éº„ÇøÁîüÊàêÔºàÂÆü„Éá„Éº„Çø„ÅÆ‰ª£ÊõøÔºâ
np.random.seed(42)
n_samples = 1000
n_features = 10
contamination = 0.05

# Ê≠£Â∏∏„Éá„Éº„Çø
X_normal = np.random.randn(int(n_samples * (1 - contamination)), n_features)
# Áï∞Â∏∏„Éá„Éº„Çø
X_anomaly = np.random.uniform(low=-5, high=5, size=(int(n_samples * contamination), n_features))
X = np.vstack([X_normal, X_anomaly])
y = np.hstack([np.zeros(len(X_normal)), np.ones(len(X_anomaly))])

# Ë®ìÁ∑¥„Éª„ÉÜ„Çπ„ÉàÂàÜÂâ≤
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

# „Éá„Éº„ÇøÊ®ôÊ∫ñÂåñ
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Ë§áÊï∞„É¢„Éá„É´„ÅÆË®ìÁ∑¥
models = {
    'KNN': KNN(contamination=contamination),
    'IForest': IForest(contamination=contamination, random_state=42),
    'LOF': LOF(contamination=contamination),
    'OCSVM': OCSVM(contamination=contamination)
}

scores_dict = {}
predictions_dict = {}

for name, model in models.items():
    model.fit(X_train_scaled)
    scores = model.decision_function(X_test_scaled)
    y_pred = model.predict(X_test_scaled)

    scores_dict[name] = scores
    predictions_dict[name] = y_pred

    auc = roc_auc_score(y_test, scores)
    print(f"{name} AUC-ROC: {auc:.3f}")

# „Ç¢„É≥„Çµ„É≥„Éñ„É´ÔºàÂπ≥ÂùáÔºâ
scores_list = [scores_dict[name] for name in models.keys()]
scores_ensemble = average(np.array(scores_list))
auc_ensemble = roc_auc_score(y_test, scores_ensemble)
print(f"\nEnsemble AUC-ROC: {auc_ensemble:.3f}")

# ROCÊõ≤Á∑ö„ÅÆÂèØË¶ñÂåñ
plt.figure(figsize=(10, 6))

for name, scores in scores_dict.items():
    fpr, tpr, _ = roc_curve(y_test, scores)
    auc_val = roc_auc_score(y_test, scores)
    plt.plot(fpr, tpr, label=f'{name} (AUC={auc_val:.3f})')

# „Ç¢„É≥„Çµ„É≥„Éñ„É´„ÅÆROC
fpr_ens, tpr_ens, _ = roc_curve(y_test, scores_ensemble)
plt.plot(fpr_ens, tpr_ens, 'k--', linewidth=2,
         label=f'Ensemble (AUC={auc_ensemble:.3f})')

plt.plot([0, 1], [0, 1], 'r--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves: Ë§áÊï∞„É¢„Éá„É´„Å®„Ç¢„É≥„Çµ„É≥„Éñ„É´')
plt.legend()
plt.grid(alpha=0.3)
plt.show()
</code></pre>

<hr />

<h2>3.6 „Åæ„Å®„ÇÅ</h2>

<h3>Êú¨Á´†„ÅßÂ≠¶„Çì„Å†„Åì„Å®</h3>

<ol>
<li>
<p><strong>Isolation Forest:</strong></p>
<ul>
<li>„É©„É≥„ÉÄ„É†ÂàÜÈõ¢„Å´„Çà„ÇãÁï∞Â∏∏Ê§úÁü•</li>
<li>„Éë„ÇπÈï∑„Åã„ÇâÁï∞Â∏∏„Çπ„Ç≥„Ç¢„ÇíË®àÁÆó</li>
<li>„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøÔºàn_estimators, max_samples, contaminationÔºâ</li>
<li>È´òÊ¨°ÂÖÉ„Éá„Éº„Çø„Å´ÂäπÊûúÁöÑ</li>
</ul>
</li>
<li>
<p><strong>LOFÔºàLocal Outlier FactorÔºâ:</strong></p>
<ul>
<li>Â±ÄÊâÄÂØÜÂ∫¶„Å´Âü∫„Å•„ÅèÁï∞Â∏∏Ê§úÁü•</li>
<li>Âà∞ÈÅîÂèØËÉΩË∑ùÈõ¢„Å®Â±ÄÊâÄÂà∞ÈÅîÂèØËÉΩÂØÜÂ∫¶</li>
<li>LOF„Çπ„Ç≥„Ç¢„ÅÆË®àÁÆó„Å®Ëß£Èáà</li>
<li>ÂØÜÂ∫¶„ÅåÁï∞„Å™„Çã„ÇØ„É©„Çπ„Çø„Å´ÂØæÂøú</li>
</ul>
</li>
<li>
<p><strong>One-Class SVM:</strong></p>
<ul>
<li>ÊúÄÂ§ß„Éû„Éº„Ç∏„É≥Ë∂ÖÂπ≥Èù¢„Å´„Çà„ÇãÂ¢ÉÁïåÂ≠¶Áøí</li>
<li>„Ç´„Éº„Éç„É´„Éà„É™„ÉÉ„ÇØÔºàRBF, Á∑öÂΩ¢, Â§öÈ†ÖÂºèÔºâ</li>
<li>nu„Éë„É©„É°„Éº„Çø„Å´„Çà„ÇãÁï∞Â∏∏ÁéáÂà∂Âæ°</li>
<li>ÈùûÁ∑öÂΩ¢Â¢ÉÁïå„ÅÆÂ≠¶Áøí</li>
</ul>
</li>
<li>
<p><strong>„Åù„ÅÆ‰ªñ„ÅÆÊâãÊ≥ï:</strong></p>
<ul>
<li>DBSCANÔºàÂØÜÂ∫¶„Éô„Éº„Çπ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞Ôºâ</li>
<li>Elliptic EnvelopeÔºàÊ•ïÂÜÜ„Ç®„É≥„Éô„É≠„Éº„ÉóÔºâ</li>
<li>Robust CovarianceÔºà„É≠„Éê„Çπ„ÉàÂÖ±ÂàÜÊï£Êé®ÂÆöÔºâ</li>
<li>PyOD„É©„Ç§„Éñ„É©„É™Ôºà40‰ª•‰∏ä„ÅÆ„Ç¢„É´„Ç¥„É™„Ç∫„É†Ôºâ</li>
</ul>
</li>
<li>
<p><strong>„Ç¢„É≥„Çµ„É≥„Éñ„É´Áï∞Â∏∏Ê§úÁü•:</strong></p>
<ul>
<li>Feature BaggingÔºàÁâπÂæ¥Èáè„Çµ„Éñ„Çª„ÉÉ„ÉàÔºâ</li>
<li>Model AveragingÔºà„Çπ„Ç≥„Ç¢Âπ≥ÂùáÂåñÔºâ</li>
<li>Isolation-Based EnsembleÔºàLSCPÔºâ</li>
<li>Ë§áÊï∞„É¢„Éá„É´„ÅÆÁµÑ„ÅøÂêà„Çè„Åõ„Å´„Çà„ÇãÁ≤æÂ∫¶Âêë‰∏ä</li>
</ul>
</li>
</ol>

<h3>ÊâãÊ≥ï„ÅÆ‰Ωø„ÅÑÂàÜ„Åë</h3>

<table>
<thead>
<tr>
<th>ÊâãÊ≥ï</th>
<th>ÈÅ©Áî®Â†¥Èù¢</th>
<th>Èï∑ÊâÄ</th>
<th>Áü≠ÊâÄ</th>
</tr>
</thead>
<tbody>
<tr>
<td>Isolation Forest</td>
<td>È´òÊ¨°ÂÖÉ„Éá„Éº„Çø„ÄÅÂ§ßË¶èÊ®°„Éá„Éº„Çø</td>
<td>È´òÈÄü„ÄÅ„Çπ„Ç±„Éº„É©„Éñ„É´</td>
<td>„Éë„É©„É°„Éº„ÇøË™øÊï¥„ÅåÂøÖË¶Å</td>
</tr>
<tr>
<td>LOF</td>
<td>ÂØÜÂ∫¶„ÅåÁï∞„Å™„Çã„ÇØ„É©„Çπ„Çø</td>
<td>Â±ÄÊâÄÁöÑ„Å™Áï∞Â∏∏„ÇíÊ§úÂá∫</td>
<td>Ë®àÁÆó„Ç≥„Çπ„Éà„ÅåÈ´ò„ÅÑ</td>
</tr>
<tr>
<td>One-Class SVM</td>
<td>ÈùûÁ∑öÂΩ¢Â¢ÉÁïå„ÄÅÁêÜË´ñÁöÑ‰øùË®º</td>
<td>È†ëÂÅ•„ÄÅÁêÜË´ñÁöÑÂü∫Áõ§</td>
<td>Â§ßË¶èÊ®°„Éá„Éº„Çø„ÅßÈÅÖ„ÅÑ</td>
</tr>
<tr>
<td>DBSCAN</td>
<td>„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞ + Áï∞Â∏∏Ê§úÁü•</td>
<td>„ÇØ„É©„Çπ„ÇøÊï∞‰∏çË¶Å</td>
<td>„Éë„É©„É°„Éº„Çø„Å´ÊïèÊÑü</td>
</tr>
<tr>
<td>„Ç¢„É≥„Çµ„É≥„Éñ„É´</td>
<td>È´òÁ≤æÂ∫¶„ÅåÂøÖË¶Å„Å™Â†¥Èù¢</td>
<td>È†ëÂÅ•„ÄÅÈ´òÁ≤æÂ∫¶</td>
<td>Ë®àÁÆó„Ç≥„Çπ„ÉàÂ¢óÂä†</td>
</tr>
</tbody>
</table>

<h3>Ê¨°„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó</h3>

<p>Á¨¨4Á´†„Åß„ÅØ„ÄÅÊ∑±Â±§Â≠¶Áøí„Å´„Çà„ÇãÁï∞Â∏∏Ê§úÁü•„ÇíÂ≠¶„Å≥„Åæ„ÅôÔºö</p>
<ul>
<li>AutoencoderÔºàÂÜçÊßãÊàêË™§Â∑Æ„Éô„Éº„ÇπÔºâ</li>
<li>VAEÔºàVariational AutoencoderÔºâ</li>
<li>GANÔºàGenerative Adversarial NetworkÔºâ</li>
<li>LSTM AutoencoderÔºàÊôÇÁ≥ªÂàóÁï∞Â∏∏Ê§úÁü•Ôºâ</li>
<li>TransformerÔºàAttentionÊ©üÊßãÔºâ</li>
</ul>

<hr />

<h2>ÊºîÁøíÂïèÈ°å</h2>

<details>
<summary><strong>Âïè1:</strong> Isolation Forest„Åß„ÄÅÁï∞Â∏∏„Çπ„Ç≥„Ç¢ $s(x, n) = 0.8$ „ÅÆ„Éá„Éº„Çø„Éù„Ç§„É≥„Éà„ÅØÁï∞Â∏∏„Å®Âà§ÂÆö„Åô„Åπ„Åç„ÅãÔºüÁêÜÁî±„Å®„Å®„ÇÇ„Å´Á≠î„Åà„Çà„ÄÇ</summary>

<p><strong>Ëß£Á≠î:</strong></p>
<p>„ÅØ„ÅÑ„ÄÅÁï∞Â∏∏„Å®Âà§ÂÆö„Åô„Åπ„Åç„Åß„Åô„ÄÇ</p>

<p><strong>ÁêÜÁî±:</strong></p>
<ul>
<li>Áï∞Â∏∏„Çπ„Ç≥„Ç¢ $s \approx 1$ „ÅØÊòéÁ¢∫„Å™Áï∞Â∏∏„ÇíÁ§∫„Åô</li>
<li>$s \approx 0.5$ „ÅØÊ≠£Â∏∏ÔºàÂπ≥ÂùáÁöÑ„Å™„Éë„ÇπÈï∑Ôºâ</li>
<li>$s = 0.8$ „ÅØ1„Å´Ëøë„Åè„ÄÅÈÄöÂ∏∏„Çà„ÇäÊó©„ÅèÂ≠§Á´ã„Åó„Å¶„ÅÑ„Çã„Åì„Å®„ÇíÊÑèÂë≥„Åô„Çã</li>
<li>‰∏ÄËà¨ÁöÑ„Å´ $s > 0.6$ „ÇíÁï∞Â∏∏„Å®„Åø„Å™„ÅôÈñæÂÄ§„Å®„Åó„Å¶‰ΩøÁî®</li>
</ul>
</details>

<details>
<summary><strong>Âïè2:</strong> LOF„Çπ„Ç≥„Ç¢„Åå $\text{LOF}_k(p) = 2.5$ „ÅÆÂ†¥Âêà„ÄÅ„Åì„ÅÆÁÇπ„ÅØÁï∞Â∏∏„ÅãÔºü„Åæ„Åü„ÄÅ„Åì„ÅÆ„Çπ„Ç≥„Ç¢„ÅåÊÑèÂë≥„Åô„Çã„Åì„Å®„ÇíË™¨Êòé„Åõ„Çà„ÄÇ</summary>

<p><strong>Ëß£Á≠î:</strong></p>
<p>„ÅØ„ÅÑ„ÄÅÁï∞Â∏∏„Åß„Åô„ÄÇ</p>

<p><strong>ÊÑèÂë≥:</strong></p>
<ul>
<li>$\text{LOF} \approx 1$ „ÅØËøëÂÇç„Å®ÂêåÁ®ãÂ∫¶„ÅÆÂØÜÂ∫¶ÔºàÊ≠£Â∏∏Ôºâ</li>
<li>$\text{LOF} > 1$ „ÅØËøëÂÇç„Çà„ÇäÂØÜÂ∫¶„Åå‰Ωé„ÅÑÔºàÁï∞Â∏∏„ÅÆÂèØËÉΩÊÄßÔºâ</li>
<li>$\text{LOF} = 2.5$ „ÅØ„ÄÅ„Åì„ÅÆÁÇπ„ÅÆÂØÜÂ∫¶„ÅåËøëÂÇç„ÅÆÂπ≥ÂùáÂØÜÂ∫¶„ÅÆÁ¥Ñ1/2.5„Åß„ÅÇ„Çã„Åì„Å®„ÇíÁ§∫„Åô</li>
<li>‰∏ÄËà¨ÁöÑ„Å´ $\text{LOF} > 1.5$ „ÇíÁï∞Â∏∏„Å®„Åø„Å™„Åô„Åü„ÇÅ„ÄÅ2.5„ÅØÊòéÁ¢∫„Å™Áï∞Â∏∏</li>
</ul>
</details>

<details>
<summary><strong>Âïè3:</strong> One-Class SVM„ÅÆnu„Éë„É©„É°„Éº„Çø„Çí0.05„Å´Ë®≠ÂÆö„Åó„ÅüÂ†¥Âêà„ÄÅË®ìÁ∑¥„Éá„Éº„Çø„ÅÆ‰Ωï%„ÅåÁï∞Â∏∏„Å®Âà§ÂÆö„Åï„Çå„Çã„ÅãÔºü„Åæ„Åü„ÄÅnu„ÇíÂ§ß„Åç„Åè„Åó„ÅüÂ†¥Âêà„ÅÆÂΩ±Èüø„ÇíË™¨Êòé„Åõ„Çà„ÄÇ</summary>

<p><strong>Ëß£Á≠î:</strong></p>
<p>Ë®ìÁ∑¥„Éá„Éº„Çø„ÅÆÊúÄÂ§ß5%„ÅåÁï∞Â∏∏„Å®Âà§ÂÆö„Åï„Çå„Åæ„Åô„ÄÇ</p>

<p><strong>nu„ÇíÂ§ß„Åç„Åè„Åó„ÅüÂ†¥Âêà„ÅÆÂΩ±Èüø:</strong></p>
<ul>
<li>$\nu = 0.1$: ÊúÄÂ§ß10%„ÅåÁï∞Â∏∏„Å®Âà§ÂÆö„Åï„Çå„Çã</li>
<li>$\nu = 0.2$: ÊúÄÂ§ß20%„ÅåÁï∞Â∏∏„Å®Âà§ÂÆö„Åï„Çå„Çã</li>
<li>nu„ÇíÂ§ß„Åç„Åè„Åô„Çã„Å®„ÄÅ„Çà„ÇäÂ§ö„Åè„ÅÆ„Éá„Éº„Çø„ÅåÁï∞Â∏∏„Å®Âà§ÂÆö„Åï„Çå„Çã</li>
<li>Ê≠£Â∏∏„Éá„Éº„Çø„ÇÇÁï∞Â∏∏„Å®Ë™§Âà§ÂÆö„Åï„Çå„Çã„É™„Çπ„ÇØ„ÅåÂ¢óÂä†ÔºàÂÅΩÈôΩÊÄßÂ¢óÂä†Ôºâ</li>
<li>Áï∞Â∏∏Ê§úÁü•„ÅÆÊÑüÂ∫¶„ÅåÈ´ò„Åè„Å™„Çã„Åå„ÄÅÁ≤æÂ∫¶„ÅØ‰Ωé‰∏ã„Åô„ÇãÂèØËÉΩÊÄß</li>
</ul>
</details>

<details>
<summary><strong>Âïè4:</strong> DBSCAN„ÅßÁï∞Â∏∏Ê§úÁü•„ÇíË°å„ÅÜÈöõ„ÄÅeps„Å®min_samples„Éë„É©„É°„Éº„Çø„Çí„Å©„ÅÆ„Çà„ÅÜ„Å´ÈÅ∏Êäû„Åô„Åπ„Åç„ÅãÔºüÂÖ∑‰ΩìÁöÑ„Å™ÈÅ∏ÊäûÊñπÊ≥ï„Çí3„Å§Ëø∞„Åπ„Çà„ÄÇ</summary>

<p><strong>Ëß£Á≠î:</strong></p>

<ol>
<li>
<p><strong>KË∑ùÈõ¢„Ç∞„É©„ÉïÊ≥ï:</strong></p>
<ul>
<li>ÂêÑÁÇπ„ÅÆkÁï™ÁõÆ„ÅÆÊúÄËøëÂÇçË∑ùÈõ¢„ÇíË®àÁÆóÔºàk„ÅØmin_samples„ÅÆÂÄôË£úÔºâ</li>
<li>Ë∑ùÈõ¢„ÇíÈôçÈ†Ü„Å´„ÇΩ„Éº„Éà„Åó„Å¶„Éó„É≠„ÉÉ„Éà</li>
<li>ÊÄ•ÊøÄ„Å´Â¢óÂä†„Åô„ÇãÁÇπÔºà„Ç®„É´„Éú„ÉºÁÇπÔºâ„Çíeps„Å®„Åó„Å¶ÈÅ∏Êäû</li>
</ul>
</li>
<li>
<p><strong>„Éâ„É°„Ç§„É≥Áü•Ë≠ò„Å´Âü∫„Å•„ÅèÈÅ∏Êäû:</strong></p>
<ul>
<li>„Éá„Éº„Çø„ÅÆÊÄßË≥™„Åã„ÇâÈÅ©Âàá„Å™ËøëÂÇç„Çµ„Ç§„Ç∫„ÇíÊé®ÂÆö</li>
<li>‰æã: 2Ê¨°ÂÖÉ„Éá„Éº„Çø„Å™„Çâ min_samples=4„ÄÅÈ´òÊ¨°ÂÖÉ„Å™„Çâ min_samples=2√óÊ¨°ÂÖÉÊï∞</li>
<li>eps„ÅØ„Éá„Éº„Çø„ÅÆ„Çπ„Ç±„Éº„É´„Å´Âøú„Åò„Å¶Ë™øÊï¥</li>
</ul>
</li>
<li>
<p><strong>„Ç∞„É™„ÉÉ„Éâ„Çµ„Éº„ÉÅ:</strong></p>
<ul>
<li>Ë§áÊï∞„ÅÆ(eps, min_samples)„ÅÆÁµÑ„ÅøÂêà„Çè„Åõ„ÇíË©¶„Åô</li>
<li>„Ç∑„É´„Ç®„ÉÉ„Éà„Çπ„Ç≥„Ç¢„ÇÑ„ÇØ„É©„Çπ„ÇøÊï∞„ÅßË©ï‰æ°</li>
<li>ÊúÄÈÅ©„Å™ÁµÑ„ÅøÂêà„Çè„Åõ„ÇíÈÅ∏Êäû</li>
</ul>
</li>
</ol>
</details>

<details>
<summary><strong>Âïè5:</strong> „Ç¢„É≥„Çµ„É≥„Éñ„É´Áï∞Â∏∏Ê§úÁü•„Åß„ÄÅFeature Bagging„Å®„É¢„Éá„É´Âπ≥ÂùáÂåñ„ÅÆÈÅï„ÅÑ„ÇíË™¨Êòé„Åó„ÄÅ„Åù„Çå„Åû„Çå„Åå„Å©„ÅÆ„Çà„ÅÜ„Å™Áä∂Ê≥Å„ÅßÊúâÂäπ„ÅãË´ñ„Åò„ÇàÔºà300Â≠ó‰ª•ÂÜÖÔºâ„ÄÇ</summary>

<p><strong>Ëß£Á≠î‰æã:</strong></p>

<p>Feature Bagging„ÅØÁâπÂæ¥Èáè„ÅÆ„Çµ„Éñ„Çª„ÉÉ„Éà„ÅßË§áÊï∞„É¢„Éá„É´„ÇíË®ìÁ∑¥„Åó„ÄÅÈ´òÊ¨°ÂÖÉ„Éá„Éº„Çø„Å´„Åä„Åë„ÇãÁâπÂæ¥ÈáèÈñì„ÅÆÁõ∏Èñ¢„ÇÑÂÜóÈï∑ÊÄß„Å´ÂØæÂá¶„Åó„Åæ„Åô„ÄÇÁâπÂæ¥Èáè„ÅåÂ§ö„ÅèÁõ∏Èñ¢„ÅåÂº∑„ÅÑÂ†¥Âêà„Å´ÊúâÂäπ„Åß„Åô„ÄÇ‰∏ÄÊñπ„ÄÅ„É¢„Éá„É´Âπ≥ÂùáÂåñ„ÅØÁï∞„Å™„Çã„Ç¢„É´„Ç¥„É™„Ç∫„É†ÔºàKNN„ÄÅIsolation Forest„ÄÅLOF„Å™„Å©Ôºâ„ÅÆ‰∫àÊ∏¨„ÇíÈõÜÁ¥Ñ„Åó„ÄÅÂêÑÊâãÊ≥ï„ÅÆÈï∑ÊâÄ„ÇíÊ¥ª„Åã„Åó„Åæ„Åô„ÄÇ„Éá„Éº„Çø„ÅÆÊÄßË≥™„Åå‰∏çÊòéÁ¢∫„Åß„ÄÅ„Å©„ÅÆÊâãÊ≥ï„ÅåÊúÄÈÅ©„Åã‰∫ãÂâç„Å´„Çè„Åã„Çâ„Å™„ÅÑÂ†¥Âêà„Å´ÊúâÂäπ„Åß„Åô„ÄÇFeature Bagging„ÅØÂêå‰∏Ä„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅÆÂ§öÊßòÊÄß„ÇíÈ´ò„ÇÅ„ÄÅ„É¢„Éá„É´Âπ≥ÂùáÂåñ„ÅØ„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅÆÂ§öÊßòÊÄß„ÇíÊ¥ªÁî®„Åô„ÇãÁÇπ„Åå‰∏ª„Å™ÈÅï„ÅÑ„Åß„Åô„ÄÇÂÆüÂãô„Åß„ÅØ‰∏°Êñπ„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„Çã„Åì„Å®„Åß„ÄÅ„Çà„ÇäÈ†ëÂÅ•„Å™Áï∞Â∏∏Ê§úÁü•„Ç∑„Çπ„ÉÜ„É†„ÇíÊßãÁØâ„Åß„Åç„Åæ„Åô„ÄÇ</p>
</details>

<hr />

<h2>ÂèÇËÄÉÊñáÁåÆ</h2>

<ol>
<li>Liu, F. T., Ting, K. M., &amp; Zhou, Z. H. (2008). "Isolation Forest." <em>IEEE International Conference on Data Mining (ICDM)</em>.</li>
<li>Breunig, M. M., Kriegel, H. P., Ng, R. T., &amp; Sander, J. (2000). "LOF: Identifying Density-Based Local Outliers." <em>ACM SIGMOD International Conference on Management of Data</em>.</li>
<li>Sch√∂lkopf, B., Platt, J. C., Shawe-Taylor, J., Smola, A. J., &amp; Williamson, R. C. (2001). "Estimating the Support of a High-Dimensional Distribution." <em>Neural Computation</em>, 13(7), 1443-1471.</li>
<li>Ester, M., Kriegel, H. P., Sander, J., &amp; Xu, X. (1996). "A Density-Based Algorithm for Discovering Clusters." <em>KDD</em>, 96(34), 226-231.</li>
<li>Zhao, Y., Nasrullah, Z., &amp; Li, Z. (2019). "PyOD: A Python Toolbox for Scalable Outlier Detection." <em>Journal of Machine Learning Research</em>, 20(96), 1-7.</li>
</ol>

<hr />

<p><strong>Ê¨°Á´†</strong>: <a href="chapter4-deep-learning-anomaly.html">Á¨¨4Á´†ÔºöÊ∑±Â±§Â≠¶Áøí„Å´„Çà„ÇãÁï∞Â∏∏Ê§úÁü•</a></p>

<p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: „Åì„ÅÆ„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÅØCC BY 4.0„É©„Ç§„Çª„É≥„Çπ„ÅÆ‰∏ã„ÅßÊèê‰æõ„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ</p>

<div class="navigation">
    <a href="chapter2-statistical-methods.html" class="nav-button">‚Üê Ââç„ÅÆÁ´†</a>
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
    <a href="chapter4-deep-learning-anomaly.html" class="nav-button">Ê¨°„ÅÆÁ´† ‚Üí</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-21</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
