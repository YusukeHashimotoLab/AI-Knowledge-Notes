<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="æ©Ÿæ¢°å­¦ç¿’ã®æ•°ç†å…¥é–€ã‚·ãƒªãƒ¼ã‚º ç¬¬4ç« ï¼šæƒ…å ±ç†è«– - ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã€ç›¸äº’æƒ…å ±é‡ã‚’ç†è«–ã¨å®Ÿè£…ã§å­¦ã¶">
    <title>ç¬¬4ç« ï¼šæƒ…å ±ç†è«– - æ©Ÿæ¢°å­¦ç¿’ã®æ•°ç†å…¥é–€ã‚·ãƒªãƒ¼ã‚º</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script>
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }
        .container { max-width: 900px; margin: 0 auto; padding: 2rem 1.5rem; }
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        header .container { padding: 0 1.5rem; }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; font-weight: 700; }
        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }
        .meta span { display: inline-flex; align-items-center; gap: 0.3rem; }
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }
        h3 { font-size: 1.4rem; margin-top: 2rem; margin-bottom: 0.8rem; color: var(--primary-color); }
        h4 { font-size: 1.2rem; margin-top: 1.5rem; margin-bottom: 0.6rem; color: var(--primary-color); }
        p { margin-bottom: 1.2rem; }
        a { color: var(--link-color); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--link-hover); text-decoration: underline; }
        ul, ol { margin-left: 2rem; margin-bottom: 1.2rem; }
        li { margin-bottom: 0.5rem; }
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }
        pre {
            background: var(--code-bg);
            padding: 1.2rem;
            border-radius: 6px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border-left: 4px solid var(--secondary-color);
        }
        pre code {
            background: none;
            padding: 0;
            font-size: 0.85em;
            line-height: 1.6;
        }
        .info-box {
            background: #e8f4f8;
            border-left: 4px solid var(--secondary-color);
            padding: 1rem 1.2rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }
        .info-box strong {
            color: var(--secondary-color);
            display: block;
            margin-bottom: 0.5rem;
        }
        .warning-box {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 1rem 1.2rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }
        .warning-box strong {
            color: #856404;
            display: block;
            margin-bottom: 0.5rem;
        }
        .math-block {
            margin: 1.5rem 0;
            padding: 1rem;
            text-align: center;
            overflow-x: auto;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
        }
        th, td {
            border: 1px solid var(--border-color);
            padding: 0.8rem;
            text-align: left;
        }
        th {
            background: #f8f9fa;
            font-weight: 600;
        }
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }
        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }
        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
            text-decoration: none;
        }
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }
        @media (max-width: 768px) {
            .container { padding: 1rem; }
            h1 { font-size: 1.6rem; }
            h2 { font-size: 1.4rem; }
            .meta { font-size: 0.85rem; }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>ç¬¬4ç« ï¼šæƒ…å ±ç†è«–</h1>
            <p style="font-size: 1.1rem; margin-top: 0.5rem; opacity: 0.95;">æ©Ÿæ¢°å­¦ç¿’ã®æ•°ç†å…¥é–€ã‚·ãƒªãƒ¼ã‚º v1.0</p>
            <div class="meta">
                <span>ğŸ“– å­¦ç¿’æ™‚é–“: 25-30åˆ†</span>
                <span>ğŸ“Š ãƒ¬ãƒ™ãƒ«: ä¸Šç´š</span>
                <span>ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 6å€‹</span>
                <span>ğŸ¯ ã‚·ãƒªãƒ¼ã‚º: ML-P05</span>
            </div>
        </div>
    </header>

    <main class="container">
        <p><strong>æ©Ÿæ¢°å­¦ç¿’ã‚’æ”¯ãˆã‚‹æƒ…å ±ç†è«–ã‚’ã€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‹ã‚‰VAEã¾ã§ç†è«–ã¨å®Ÿè£…ã§æ·±ãç†è§£ã™ã‚‹</strong></p>

        <div class="info-box">
            <strong>ã“ã®ç« ã§å­¦ã¹ã‚‹ã“ã¨</strong>
            <ul style="margin-left: 1.5rem; margin-bottom: 0;">
                <li>ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¨æƒ…å ±é‡ã®æ•°å­¦çš„å®šç¾©ã¨ç›´æ„Ÿçš„ç†è§£</li>
                <li>KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®é–¢ä¿‚</li>
                <li>ç›¸äº’æƒ…å ±é‡ã«ã‚ˆã‚‹ç‰¹å¾´é¸æŠã®ç†è«–çš„åŸºç¤</li>
                <li>VAEã¨æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®æƒ…å ±ç†è«–çš„è§£é‡ˆ</li>
                <li>æ©Ÿæ¢°å­¦ç¿’ã«ãŠã‘ã‚‹æå¤±é–¢æ•°ã®æƒ…å ±ç†è«–çš„æ„å‘³</li>
            </ul>
        </div>

        <h2>1. ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼</h2>

        <h3>1.1 æƒ…å ±é‡ã¨ã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼</h3>

        <p>æƒ…å ±ç†è«–ã®åŸºç¤ã¯ã€ã€Œã‚ã‚‹äº‹è±¡ã®æƒ…å ±é‡ã€ã‚’å®šé‡åŒ–ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã¾ã‚Šã¾ã™ã€‚äº‹è±¡xãŒèµ·ã“ã‚‹ç¢ºç‡ã‚’P(x)ã¨ã—ãŸã¨ãã€ãã®<strong>è‡ªå·±æƒ…å ±é‡</strong>ã¯æ¬¡ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã¾ã™ã€‚</p>

        <div class="math-block">
            $$I(x) = -\log_2 P(x) \quad \text{[bits]}$$
        </div>

        <p>ã“ã®å®šç¾©ã«ã¯æ·±ã„æ„å‘³ãŒã‚ã‚Šã¾ã™ï¼š</p>
        <ul>
            <li><strong>ç¢ºç‡ãŒä½ã„äº‹è±¡ã»ã©æƒ…å ±é‡ãŒå¤§ãã„</strong>: çã—ã„äº‹è±¡ã»ã©é©šããŒå¤§ãã„</li>
            <li><strong>ç¢ºå®Ÿãªäº‹è±¡ï¼ˆP(x)=1ï¼‰ã¯æƒ…å ±é‡ã‚¼ãƒ­</strong>: äºˆæ¸¬ã§ãã‚‹ã“ã¨ã«é©šãã¯ãªã„</li>
            <li><strong>ç‹¬ç«‹äº‹è±¡ã®æƒ…å ±é‡ã¯åŠ ç®—çš„</strong>: I(x,y) = I(x) + I(y)</li>
        </ul>

        <p><strong>ã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼</strong>ã¯ã€ç¢ºç‡åˆ†å¸ƒå…¨ä½“ã®å¹³å‡çš„ãªæƒ…å ±é‡ã‚’è¡¨ã—ã¾ã™ã€‚</p>

        <div class="math-block">
            $$H(X) = -\sum_{x} P(x) \log_2 P(x) = \mathbb{E}_{x \sim P}[-\log P(x)]$$
        </div>

        <div class="info-box">
            <strong>ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®ç›´æ„Ÿçš„ç†è§£</strong>
            ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¯ã€Œä¸ç¢ºå®Ÿæ€§ã€ã‚„ã€Œãƒ©ãƒ³ãƒ€ãƒ ã•ã€ã®å°ºåº¦ã§ã™ã€‚ä¸€æ§˜åˆ†å¸ƒã¯æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’æŒã¡ï¼ˆæœ€ã‚‚äºˆæ¸¬å›°é›£ï¼‰ã€ç¢ºå®šçš„ãªåˆ†å¸ƒã¯æœ€å°ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼=0ï¼‰ã‚’æŒã¡ã¾ã™ã€‚
        </div>

        <h3>1.2 æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼</h3>

        <p>å¤‰æ•°YãŒä¸ãˆã‚‰ã‚ŒãŸæ¡ä»¶ã®ä¸‹ã§ã®Xã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’<strong>æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼</strong>ã¨å‘¼ã³ã¾ã™ã€‚</p>

        <div class="math-block">
            $$H(X|Y) = \sum_{y} P(y) H(X|Y=y) = -\sum_{x,y} P(x,y) \log P(x|y)$$
        </div>

        <p>é‡è¦ãªæ€§è³ªã¨ã—ã¦ã€<strong>é€£é–å¾‹ï¼ˆchain ruleï¼‰</strong>ãŒæˆã‚Šç«‹ã¡ã¾ã™ï¼š</p>

        <div class="math-block">
            $$H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)$$
        </div>

        <h3>å®Ÿè£…ä¾‹1ï¼šã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®è¨ˆç®—ã¨å¯è¦–åŒ–</h3>

<pre><code>import numpy as np
import matplotlib.pyplot as plt

class InformationMeasures:
    """æƒ…å ±ç†è«–ã®åŸºæœ¬é‡ã‚’è¨ˆç®—ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    @staticmethod
    def entropy(p, base=2):
        """
        ã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®è¨ˆç®—

        Parameters:
        -----------
        p : array-like
            ç¢ºç‡åˆ†å¸ƒï¼ˆåˆè¨ˆãŒ1ã«ãªã‚‹å¿…è¦ãŒã‚ã‚‹ï¼‰
        base : int
            å¯¾æ•°ã®åº•ï¼ˆ2: bits, e: natsï¼‰

        Returns:
        --------
        float : ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        """
        p = np.array(p)
        # 0 * log(0) = 0 ã¨å®šç¾©ï¼ˆæ•°å€¤çš„ã«å®‰å®šãªå®Ÿè£…ï¼‰
        p = p[p > 0]  # æ­£ã®ç¢ºç‡ã®ã¿ã‚’è€ƒæ…®

        if base == 2:
            return -np.sum(p * np.log2(p))
        elif base == np.e:
            return -np.sum(p * np.log(p))
        else:
            return -np.sum(p * np.log(p)) / np.log(base)

    @staticmethod
    def conditional_entropy(joint_p, axis=1):
        """
        æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ H(X|Y) ã®è¨ˆç®—

        Parameters:
        -----------
        joint_p : ndarray
            åŒæ™‚ç¢ºç‡åˆ†å¸ƒ P(X,Y)
        axis : int
            æ¡ä»¶ã¨ã™ã‚‹å¤‰æ•°ã®è»¸ï¼ˆ0: H(Y|X), 1: H(X|Y)ï¼‰

        Returns:
        --------
        float : æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        """
        joint_p = np.array(joint_p)

        # å‘¨è¾ºç¢ºç‡ã®è¨ˆç®—
        marginal_p = np.sum(joint_p, axis=axis)

        # æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®è¨ˆç®—
        h_cond = 0
        for i, p_y in enumerate(marginal_p):
            if p_y > 0:
                if axis == 1:
                    conditional_p = joint_p[:, i] / p_y
                else:
                    conditional_p = joint_p[i, :] / p_y
                h_cond += p_y * InformationMeasures.entropy(conditional_p)

        return h_cond

    @staticmethod
    def joint_entropy(joint_p):
        """
        åŒæ™‚ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ H(X,Y) ã®è¨ˆç®—
        """
        joint_p = np.array(joint_p).flatten()
        return InformationMeasures.entropy(joint_p)

# ä½¿ç”¨ä¾‹1: äºŒå€¤å¤‰æ•°ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
print("=" * 50)
print("äºŒå€¤å¤‰æ•°ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼")
print("=" * 50)

# ã‚³ã‚¤ãƒ³ã®ç¢ºç‡åˆ†å¸ƒ
probs = np.linspace(0.01, 0.99, 99)
entropies = [InformationMeasures.entropy([p, 1-p]) for p in probs]

plt.figure(figsize=(10, 5))
plt.plot(probs, entropies, 'b-', linewidth=2)
plt.axvline(x=0.5, color='r', linestyle='--', label='æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ (p=0.5)')
plt.xlabel('ç¢ºç‡ P(X=1)')
plt.ylabel('ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ H(X) [bits]')
plt.title('äºŒå€¤ç¢ºç‡å¤‰æ•°ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼')
plt.grid(True, alpha=0.3)
plt.legend()
plt.tight_layout()
plt.savefig('entropy_binary.png', dpi=150, bbox_inches='tight')
print(f"æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: {max(entropies):.4f} bits (p=0.5)")

# ä½¿ç”¨ä¾‹2: åŒæ™‚ç¢ºç‡åˆ†å¸ƒã¨æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
print("\n" + "=" * 50)
print("æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®ä¾‹")
print("=" * 50)

# åŒæ™‚ç¢ºç‡åˆ†å¸ƒ P(X,Y)
joint_prob = np.array([
    [0.2, 0.1],  # P(X=0, Y=0), P(X=0, Y=1)
    [0.15, 0.55] # P(X=1, Y=0), P(X=1, Y=1)
])

# å‘¨è¾ºç¢ºç‡
p_x = np.sum(joint_prob, axis=1)
p_y = np.sum(joint_prob, axis=0)

# å„ç¨®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
h_x = InformationMeasures.entropy(p_x)
h_y = InformationMeasures.entropy(p_y)
h_xy = InformationMeasures.joint_entropy(joint_prob)
h_x_given_y = InformationMeasures.conditional_entropy(joint_prob, axis=1)
h_y_given_x = InformationMeasures.conditional_entropy(joint_prob, axis=0)

print(f"H(X) = {h_x:.4f} bits")
print(f"H(Y) = {h_y:.4f} bits")
print(f"H(X,Y) = {h_xy:.4f} bits")
print(f"H(X|Y) = {h_x_given_y:.4f} bits")
print(f"H(Y|X) = {h_y_given_x:.4f} bits")

# é€£é–å¾‹ã®æ¤œè¨¼: H(X,Y) = H(X) + H(Y|X)
print(f"\né€£é–å¾‹ã®æ¤œè¨¼:")
print(f"H(X) + H(Y|X) = {h_x + h_y_given_x:.4f}")
print(f"H(X,Y) = {h_xy:.4f}")
print(f"å·®: {abs(h_xy - (h_x + h_y_given_x)):.10f}")
</code></pre>

        <h2>2. KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼</h2>

        <h3>2.1 KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ï¼ˆKullback-Leibler Divergenceï¼‰</h3>

        <p>KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯ã€2ã¤ã®ç¢ºç‡åˆ†å¸ƒP(x)ã¨Q(x)ã®ã€Œå·®ç•°ã€ã‚’æ¸¬ã‚‹æŒ‡æ¨™ã§ã™ã€‚</p>

        <div class="math-block">
            $$D_{KL}(P||Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)} = \mathbb{E}_{x \sim P}\left[\log \frac{P(x)}{Q(x)}\right]$$
        </div>

        <p><strong>é‡è¦ãªæ€§è³ªï¼š</strong></p>
        <ul>
            <li><strong>éå¯¾ç§°æ€§</strong>: \(D_{KL}(P||Q) \neq D_{KL}(Q||P)\)ï¼ˆè·é›¢ã§ã¯ãªã„ï¼‰</li>
            <li><strong>éè² æ€§</strong>: \(D_{KL}(P||Q) \geq 0\)ã€ç­‰å·æˆç«‹ã¯ \(P = Q\) ã®ã¨ã</li>
            <li><strong>ã‚®ãƒ–ã‚¹ã®ä¸ç­‰å¼</strong>: \(\mathbb{E}_P[\log P(x)] \geq \mathbb{E}_P[\log Q(x)]\)</li>
        </ul>

        <h3>2.2 äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆCross Entropyï¼‰</h3>

        <p>äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¯ã€çœŸã®åˆ†å¸ƒPã®ä¸‹ã§ãƒ¢ãƒ‡ãƒ«åˆ†å¸ƒQã‚’ä½¿ã£ã¦ç¬¦å·åŒ–ã™ã‚‹éš›ã®å¹³å‡ãƒ“ãƒƒãƒˆæ•°ã§ã™ã€‚</p>

        <div class="math-block">
            $$H(P, Q) = -\sum_{x} P(x) \log Q(x) = H(P) + D_{KL}(P||Q)$$
        </div>

        <p>ã“ã®é–¢ä¿‚å¼ã‹ã‚‰ã€<strong>äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã¯KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã¨ç­‰ä¾¡</strong>ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ï¼ˆH(P)ã¯å®šæ•°ã®ãŸã‚ï¼‰ã€‚</p>

        <div class="info-box">
            <strong>æ©Ÿæ¢°å­¦ç¿’ã§ã®å¿œç”¨</strong>
            åˆ†é¡å•é¡Œã®æå¤±é–¢æ•°ã¨ã—ã¦äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒåºƒãä½¿ã‚ã‚Œã¾ã™ã€‚çœŸã®ãƒ©ãƒ™ãƒ«åˆ†å¸ƒPï¼ˆone-hotï¼‰ã¨ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬åˆ†å¸ƒQï¼ˆã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹å‡ºåŠ›ï¼‰ã®å·®ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã§å­¦ç¿’ãŒé€²ã¿ã¾ã™ã€‚
        </div>

        <h3>å®Ÿè£…ä¾‹2ï¼šKLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼</h3>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
from scipy.special import softmax

class DivergenceMeasures:
    """ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹æŒ‡æ¨™ã®è¨ˆç®—"""

    @staticmethod
    def kl_divergence(p, q, epsilon=1e-10):
        """
        KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ D_KL(P||Q) ã®è¨ˆç®—

        Parameters:
        -----------
        p, q : array-like
            ç¢ºç‡åˆ†å¸ƒï¼ˆæ­£è¦åŒ–ã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼‰
        epsilon : float
            æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã®å°ã•ãªå€¤

        Returns:
        --------
        float : KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ [nats ã¾ãŸã¯ bits]
        """
        p = np.array(p)
        q = np.array(q)

        # ã‚¼ãƒ­é™¤ç®—ã‚’é˜²ã
        q = np.clip(q, epsilon, 1.0)
        p = np.clip(p, epsilon, 1.0)

        return np.sum(p * np.log(p / q))

    @staticmethod
    def cross_entropy(p, q, epsilon=1e-10):
        """
        äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ H(P,Q) ã®è¨ˆç®—

        Returns:
        --------
        float : äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        """
        p = np.array(p)
        q = np.array(q)

        q = np.clip(q, epsilon, 1.0)

        return -np.sum(p * np.log(q))

    @staticmethod
    def js_divergence(p, q):
        """
        Jensen-Shannon ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ï¼ˆå¯¾ç§°ç‰ˆã®KLï¼‰

        JS(P||Q) = 0.5 * D_KL(P||M) + 0.5 * D_KL(Q||M)
        where M = 0.5 * (P + Q)
        """
        p = np.array(p)
        q = np.array(q)
        m = 0.5 * (p + q)

        return 0.5 * DivergenceMeasures.kl_divergence(p, m) + \
               0.5 * DivergenceMeasures.kl_divergence(q, m)

# ä½¿ç”¨ä¾‹1: ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹
print("=" * 50)
print("ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹")
print("=" * 50)

# 2ã¤ã®æ­£è¦åˆ†å¸ƒ
x = np.linspace(-5, 5, 1000)
p = np.exp(-0.5 * x**2) / np.sqrt(2 * np.pi)  # N(0, 1)
q = np.exp(-0.5 * (x - 1)**2) / np.sqrt(2 * np.pi)  # N(1, 1)

# æ­£è¦åŒ–
p = p / np.sum(p)
q = q / np.sum(q)

kl_pq = DivergenceMeasures.kl_divergence(p, q)
kl_qp = DivergenceMeasures.kl_divergence(q, p)
js = DivergenceMeasures.js_divergence(p, q)

print(f"D_KL(P||Q) = {kl_pq:.4f}")
print(f"D_KL(Q||P) = {kl_qp:.4f}")
print(f"JS(P||Q) = {js:.4f}")
print(f"éå¯¾ç§°æ€§: |D_KL(P||Q) - D_KL(Q||P)| = {abs(kl_pq - kl_qp):.4f}")

# å¯è¦–åŒ–
plt.figure(figsize=(10, 5))
plt.plot(x, p * 1000, 'b-', linewidth=2, label='P: N(0,1)')
plt.plot(x, q * 1000, 'r-', linewidth=2, label='Q: N(1,1)')
plt.xlabel('x')
plt.ylabel('ç¢ºç‡å¯†åº¦')
plt.title(f'KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹: D_KL(P||Q) = {kl_pq:.4f}')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('kl_divergence.png', dpi=150, bbox_inches='tight')

# ä½¿ç”¨ä¾‹2: åˆ†é¡å•é¡Œã§ã®äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±
print("\n" + "=" * 50)
print("åˆ†é¡å•é¡Œã§ã®äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±")
print("=" * 50)

# çœŸã®ãƒ©ãƒ™ãƒ«ï¼ˆone-hot encodingï¼‰
true_label = np.array([0, 1, 0, 0])  # ã‚¯ãƒ©ã‚¹1ãŒæ­£è§£

# ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ï¼ˆãƒ­ã‚¸ãƒƒãƒˆï¼‰
logits_good = np.array([1.0, 3.5, 0.5, 0.8])   # è‰¯ã„äºˆæ¸¬
logits_bad = np.array([2.0, 0.5, 1.5, 1.0])    # æ‚ªã„äºˆæ¸¬

# ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã§ç¢ºç‡ã«å¤‰æ›
pred_good = softmax(logits_good)
pred_bad = softmax(logits_bad)

# äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±
ce_good = DivergenceMeasures.cross_entropy(true_label, pred_good)
ce_bad = DivergenceMeasures.cross_entropy(true_label, pred_bad)

print(f"è‰¯ã„äºˆæ¸¬ã®ç¢ºç‡åˆ†å¸ƒ: {pred_good}")
print(f"äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±: {ce_good:.4f}\n")

print(f"æ‚ªã„äºˆæ¸¬ã®ç¢ºç‡åˆ†å¸ƒ: {pred_bad}")
print(f"äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±: {ce_bad:.4f}\n")

print(f"æå¤±ã®å·®: {ce_bad - ce_good:.4f}")
print("â†’ è‰¯ã„äºˆæ¸¬ã®æ–¹ãŒæå¤±ãŒå°ã•ã„")
</code></pre>

        <h2>3. ç›¸äº’æƒ…å ±é‡</h2>

        <h3>3.1 ç›¸äº’æƒ…å ±é‡ã®å®šç¾©</h3>

        <p><strong>ç›¸äº’æƒ…å ±é‡ï¼ˆMutual Informationï¼‰</strong>ã¯ã€2ã¤ã®ç¢ºç‡å¤‰æ•°Xã¨Yã®é–“ã®çµ±è¨ˆçš„ä¾å­˜æ€§ã‚’æ¸¬ã‚‹æŒ‡æ¨™ã§ã™ã€‚</p>

        <div class="math-block">
            $$I(X;Y) = \sum_{x,y} P(x,y) \log \frac{P(x,y)}{P(x)P(y)} = D_{KL}(P(X,Y)||P(X)P(Y))$$
        </div>

        <p>ç›¸äº’æƒ…å ±é‡ã¯ã€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’ä½¿ã£ã¦æ¬¡ã®ã‚ˆã†ã«ã‚‚è¡¨ã›ã¾ã™ï¼š</p>

        <div class="math-block">
            $$I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X,Y)$$
        </div>

        <p><strong>é‡è¦ãªæ€§è³ªï¼š</strong></p>
        <ul>
            <li><strong>å¯¾ç§°æ€§</strong>: \(I(X;Y) = I(Y;X)\)</li>
            <li><strong>éè² æ€§</strong>: \(I(X;Y) \geq 0\)ã€ç­‰å·æˆç«‹ã¯Xã¨YãŒç‹¬ç«‹ã®ã¨ã</li>
            <li><strong>æƒ…å ±ã®æ¸›å°‘</strong>: \(I(X;Y) \leq \min(H(X), H(Y))\)</li>
        </ul>

        <div class="info-box">
            <strong>ç›´æ„Ÿçš„ç†è§£</strong>
            ç›¸äº’æƒ…å ±é‡ã¯ã€ŒXã‚’è¦³æ¸¬ã™ã‚‹ã“ã¨ã§ã€Yã«ã¤ã„ã¦ã©ã‚Œã ã‘ã®æƒ…å ±ãŒå¾—ã‚‰ã‚Œã‚‹ã‹ã€ã‚’è¡¨ã—ã¾ã™ã€‚I(X;Y)ãŒå¤§ãã„ã»ã©ã€Xã¨Yã¯å¼·ãä¾å­˜ã—ã¦ã„ã¾ã™ã€‚
        </div>

        <h3>3.2 ç‰¹å¾´é¸æŠã¸ã®å¿œç”¨</h3>

        <p>æ©Ÿæ¢°å­¦ç¿’ã§ã¯ã€ç›¸äº’æƒ…å ±é‡ã‚’ä½¿ã£ã¦é‡è¦ãªç‰¹å¾´ã‚’é¸æŠã§ãã¾ã™ã€‚ç›®æ¨™å¤‰æ•°Yã¨å„ç‰¹å¾´X_iã®ç›¸äº’æƒ…å ±é‡I(X_i;Y)ã‚’è¨ˆç®—ã—ã€å€¤ãŒå¤§ãã„ç‰¹å¾´ã‚’é¸æŠã—ã¾ã™ã€‚</p>

        <h3>å®Ÿè£…ä¾‹3ï¼šç›¸äº’æƒ…å ±é‡ã«ã‚ˆã‚‹ç‰¹å¾´é¸æŠ</h3>

<pre><code>import numpy as np
from sklearn.feature_selection import mutual_info_classif
from sklearn.datasets import make_classification

class MutualInformation:
    """ç›¸äº’æƒ…å ±é‡ã®è¨ˆç®—ã¨ç‰¹å¾´é¸æŠ"""

    @staticmethod
    def mutual_information_discrete(x, y):
        """
        é›¢æ•£å¤‰æ•°ã®ç›¸äº’æƒ…å ±é‡ã‚’è¨ˆç®—

        I(X;Y) = H(X) + H(Y) - H(X,Y)

        Parameters:
        -----------
        x, y : array-like
            é›¢æ•£å€¤ã®ç¢ºç‡å¤‰æ•°

        Returns:
        --------
        float : ç›¸äº’æƒ…å ±é‡
        """
        x = np.array(x)
        y = np.array(y)

        # åŒæ™‚é »åº¦è¡Œåˆ—ã‚’ä½œæˆ
        xy = np.c_[x, y]
        unique_xy, counts_xy = np.unique(xy, axis=0, return_counts=True)
        joint_prob = counts_xy / len(x)

        # å‘¨è¾ºç¢ºç‡
        unique_x, counts_x = np.unique(x, return_counts=True)
        p_x = counts_x / len(x)

        unique_y, counts_y = np.unique(y, return_counts=True)
        p_y = counts_y / len(y)

        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®è¨ˆç®—
        from scipy.stats import entropy
        h_x = entropy(p_x, base=2)
        h_y = entropy(p_y, base=2)
        h_xy = entropy(joint_prob, base=2)

        # ç›¸äº’æƒ…å ±é‡
        mi = h_x + h_y - h_xy

        return mi

    @staticmethod
    def feature_selection_by_mi(X, y, n_features=5):
        """
        ç›¸äº’æƒ…å ±é‡ã«ã‚ˆã‚‹ç‰¹å¾´é¸æŠ

        Parameters:
        -----------
        X : ndarray of shape (n_samples, n_features)
            ç‰¹å¾´é‡è¡Œåˆ—
        y : array-like
            ç›®æ¨™å¤‰æ•°
        n_features : int
            é¸æŠã™ã‚‹ç‰¹å¾´æ•°

        Returns:
        --------
        selected_indices : array
            é¸æŠã•ã‚ŒãŸç‰¹å¾´ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        mi_scores : array
            å„ç‰¹å¾´ã®ç›¸äº’æƒ…å ±é‡ã‚¹ã‚³ã‚¢
        """
        # scikit-learnã®ç›¸äº’æƒ…å ±é‡è¨ˆç®—
        mi_scores = mutual_info_classif(X, y, random_state=42)

        # ã‚¹ã‚³ã‚¢ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
        selected_indices = np.argsort(mi_scores)[::-1][:n_features]

        return selected_indices, mi_scores

# ä½¿ç”¨ä¾‹1: é›¢æ•£å¤‰æ•°ã®ç›¸äº’æƒ…å ±é‡
print("=" * 50)
print("é›¢æ•£å¤‰æ•°ã®ç›¸äº’æƒ…å ±é‡")
print("=" * 50)

# ä¾‹: å¤©æ°—ï¼ˆXï¼‰ã¨å‚˜ã®ä½¿ç”¨ï¼ˆYï¼‰
# 0: æ™´ã‚Œ/ä½¿ã‚ãªã„, 1: é›¨/ä½¿ã†
np.random.seed(42)

# å¼·ã„ç›¸é–¢ãŒã‚ã‚‹å ´åˆ
weather = np.array([0, 0, 0, 0, 1, 1, 1, 1, 0, 1] * 10)
umbrella_corr = np.array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1] * 10)  # å¤©æ°—ã¨ç›¸é–¢ã‚ã‚Š
umbrella_rand = np.random.randint(0, 2, 100)  # ãƒ©ãƒ³ãƒ€ãƒ 

mi_corr = MutualInformation.mutual_information_discrete(weather, umbrella_corr)
mi_rand = MutualInformation.mutual_information_discrete(weather, umbrella_rand)

print(f"å¤©æ°—ã¨å‚˜ï¼ˆç›¸é–¢ã‚ã‚Šï¼‰ã®ç›¸äº’æƒ…å ±é‡: {mi_corr:.4f} bits")
print(f"å¤©æ°—ã¨å‚˜ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ï¼‰ã®ç›¸äº’æƒ…å ±é‡: {mi_rand:.4f} bits")
print(f"â†’ ç›¸é–¢ãŒã‚ã‚‹å ´åˆã®æ–¹ãŒç›¸äº’æƒ…å ±é‡ãŒå¤§ãã„")

# ä½¿ç”¨ä¾‹2: ç‰¹å¾´é¸æŠ
print("\n" + "=" * 50)
print("ç›¸äº’æƒ…å ±é‡ã«ã‚ˆã‚‹ç‰¹å¾´é¸æŠ")
print("=" * 50)

# åˆæˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆ20ç‰¹å¾´ã€ã†ã¡5å€‹ãŒæœ‰ç”¨ï¼‰
X, y = make_classification(
    n_samples=1000,
    n_features=20,
    n_informative=5,  # æœ‰ç”¨ãªç‰¹å¾´
    n_redundant=5,    # å†—é•·ãªç‰¹å¾´
    n_repeated=0,
    n_classes=2,
    random_state=42
)

# ç›¸äº’æƒ…å ±é‡ã§ç‰¹å¾´é¸æŠ
selected_idx, mi_scores = MutualInformation.feature_selection_by_mi(X, y, n_features=5)

print(f"å…¨ç‰¹å¾´æ•°: {X.shape[1]}")
print(f"\nç›¸äº’æƒ…å ±é‡ã‚¹ã‚³ã‚¢ï¼ˆä¸Šä½10ç‰¹å¾´ï¼‰:")
for i in range(10):
    print(f"  ç‰¹å¾´ {i}: MI = {mi_scores[i]:.4f}")

print(f"\né¸æŠã•ã‚ŒãŸç‰¹å¾´ï¼ˆä¸Šä½5å€‹ï¼‰: {selected_idx}")
print(f"é¸æŠã•ã‚ŒãŸç‰¹å¾´ã®MIã‚¹ã‚³ã‚¢: {mi_scores[selected_idx]}")

# å¯è¦–åŒ–
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.bar(range(len(mi_scores)), mi_scores, color='steelblue')
plt.bar(selected_idx, mi_scores[selected_idx], color='crimson', label='é¸æŠã•ã‚ŒãŸç‰¹å¾´')
plt.xlabel('ç‰¹å¾´ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹')
plt.ylabel('ç›¸äº’æƒ…å ±é‡ã‚¹ã‚³ã‚¢')
plt.title('å„ç‰¹å¾´ã®ç›¸äº’æƒ…å ±é‡')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
sorted_idx = np.argsort(mi_scores)[::-1]
plt.plot(range(1, len(mi_scores)+1), mi_scores[sorted_idx], 'o-', linewidth=2)
plt.axvline(x=5, color='r', linestyle='--', label='é¸æŠæ•°=5')
plt.xlabel('ãƒ©ãƒ³ã‚¯')
plt.ylabel('ç›¸äº’æƒ…å ±é‡ã‚¹ã‚³ã‚¢')
plt.title('ç›¸äº’æƒ…å ±é‡ã‚¹ã‚³ã‚¢ã®é †ä½')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('mutual_information.png', dpi=150, bbox_inches='tight')
print("\nç›¸äº’æƒ…å ±é‡ã®å¯è¦–åŒ–ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
</code></pre>

        <h2>4. æƒ…å ±ç†è«–ã¨æ©Ÿæ¢°å­¦ç¿’</h2>

        <h3>4.1 å¤‰åˆ†ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆVAEï¼‰</h3>

        <p>VAEã¯æƒ…å ±ç†è«–ã®è¦³ç‚¹ã‹ã‚‰ç†è§£ã§ãã¾ã™ã€‚æ½œåœ¨å¤‰æ•°zã¨ãƒ‡ãƒ¼ã‚¿xã®é–¢ä¿‚ã‚’å­¦ç¿’ã™ã‚‹éš›ã€æ¬¡ã®ç›®çš„é–¢æ•°ã‚’æœ€å¤§åŒ–ã—ã¾ã™ã€‚</p>

        <div class="math-block">
            $$\log p(x) \geq \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{KL}(q(z|x)||p(z))$$
        </div>

        <p>ã“ã®å³è¾ºã¯<strong>ELBOï¼ˆEvidence Lower BOundï¼‰</strong>ã¨å‘¼ã°ã‚Œï¼š</p>
        <ul>
            <li><strong>ç¬¬1é …ï¼ˆå†æ§‹æˆé …ï¼‰</strong>: ãƒ‡ãƒ¼ã‚¿ã®å†æ§‹æˆå“è³ª</li>
            <li><strong>ç¬¬2é …ï¼ˆKLé …ï¼‰</strong>: æ½œåœ¨åˆ†å¸ƒã¨äº‹å‰åˆ†å¸ƒã®è¿‘ã•</li>
        </ul>

        <h3>4.2 æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç†è«–</h3>

        <p>æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç†è«–ã¯ã€è¡¨ç¾å­¦ç¿’ã‚’æƒ…å ±ç†è«–çš„ã«å®šå¼åŒ–ã—ã¾ã™ã€‚å…¥åŠ›Xã¨ãƒ©ãƒ™ãƒ«Yã«å¯¾ã—ã€è¡¨ç¾Zã¯æ¬¡ã‚’æº€ãŸã™ã¹ãã§ã™ï¼š</p>

        <div class="math-block">
            $$\min_{Z} I(X;Z) - \beta I(Z;Y)$$
        </div>

        <p>ã“ã‚Œã¯ã€Œå…¥åŠ›æƒ…å ±ã‚’åœ§ç¸®ã—ã¤ã¤ï¼ˆI(X;Z)ã‚’æœ€å°åŒ–ï¼‰ã€ãƒ©ãƒ™ãƒ«æƒ…å ±ã‚’ä¿æŒã™ã‚‹ï¼ˆI(Z;Y)ã‚’æœ€å¤§åŒ–ï¼‰ã€ã¨ã„ã†ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’è¡¨ç¾ã—ã¦ã„ã¾ã™ã€‚</p>

        <h3>å®Ÿè£…ä¾‹4ï¼šVAEã®ELBOè¨ˆç®—</h3>

<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class VAE(nn.Module):
    """å¤‰åˆ†ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆVariational Autoencoderï¼‰"""

    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):
        """
        Parameters:
        -----------
        input_dim : int
            å…¥åŠ›æ¬¡å…ƒï¼ˆä¾‹: 28x28=784 for MNISTï¼‰
        hidden_dim : int
            éš ã‚Œå±¤ã®æ¬¡å…ƒ
        latent_dim : int
            æ½œåœ¨å¤‰æ•°ã®æ¬¡å…ƒ
        """
        super(VAE, self).__init__()

        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ q(z|x)
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)

        # ãƒ‡ã‚³ãƒ¼ãƒ€ p(x|z)
        self.fc3 = nn.Linear(latent_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, input_dim)

    def encode(self, x):
        """
        ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€: x â†’ (Î¼, log ÏƒÂ²)

        Returns:
        --------
        mu, logvar : æ½œåœ¨åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        h = F.relu(self.fc1(x))
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        """
        å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯: z = Î¼ + Ïƒ * Îµ, Îµ ~ N(0,1)

        ã“ã‚Œã«ã‚ˆã‚Šã€ç¢ºç‡çš„ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å¾®åˆ†å¯èƒ½ã«ã™ã‚‹
        """
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std
        return z

    def decode(self, z):
        """
        ãƒ‡ã‚³ãƒ¼ãƒ€: z â†’ xÌ‚
        """
        h = F.relu(self.fc3(z))
        x_recon = torch.sigmoid(self.fc4(h))
        return x_recon

    def forward(self, x):
        """
        é †ä¼æ’­: x â†’ z â†’ xÌ‚
        """
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        x_recon = self.decode(z)
        return x_recon, mu, logvar

def vae_loss(x, x_recon, mu, logvar, beta=1.0):
    """
    VAEã®æå¤±é–¢æ•°ï¼ˆELBO ã®è² å€¤ï¼‰

    ELBO = E[log p(x|z)] - Î² * D_KL(q(z|x)||p(z))

    Parameters:
    -----------
    x : Tensor
        å…ƒã®ãƒ‡ãƒ¼ã‚¿
    x_recon : Tensor
        å†æ§‹æˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
    mu, logvar : Tensor
        æ½œåœ¨åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    beta : float
        KLé …ã®é‡ã¿ï¼ˆÎ²-VAEï¼‰

    Returns:
    --------
    loss, recon_loss, kl_loss : ç·æå¤±ã€å†æ§‹æˆæå¤±ã€KLæå¤±
    """
    # å†æ§‹æˆæå¤±ï¼ˆè² ã®å¯¾æ•°å°¤åº¦ï¼‰
    # ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã®å ´åˆ: BCE loss
    recon_loss = F.binary_cross_entropy(
        x_recon, x.view(-1, 784), reduction='sum'
    )

    # KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹æå¤±
    # D_KL(N(Î¼,ÏƒÂ²)||N(0,1)) = 0.5 * Î£(Î¼Â² + ÏƒÂ² - log(ÏƒÂ²) - 1)
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

    # ç·æå¤±ï¼ˆELBOã®è² å€¤ï¼‰
    total_loss = recon_loss + beta * kl_loss

    return total_loss, recon_loss, kl_loss

# ä½¿ç”¨ä¾‹
print("=" * 50)
print("VAEã®ELBOè¨ˆç®—")
print("=" * 50)

# ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
vae = VAE(input_dim=784, hidden_dim=400, latent_dim=20)

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚º32ã®28x28ç”»åƒï¼‰
torch.manual_seed(42)
x = torch.rand(32, 1, 28, 28)

# é †ä¼æ’­
x_recon, mu, logvar = vae(x)

# æå¤±è¨ˆç®—
loss, recon_loss, kl_loss = vae_loss(x, x_recon, mu, logvar, beta=1.0)

print(f"ç·æå¤±ï¼ˆ-ELBOï¼‰: {loss.item():.2f}")
print(f"  å†æ§‹æˆæå¤±: {recon_loss.item():.2f}")
print(f"  KLæå¤±: {kl_loss.item():.2f}")

# Î²ã®å½±éŸ¿ã‚’ç¢ºèª
print("\n" + "=" * 50)
print("Î²-VAE: Î²ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿")
print("=" * 50)

betas = [0.5, 1.0, 2.0, 5.0]
for beta in betas:
    loss, recon, kl = vae_loss(x, x_recon, mu, logvar, beta=beta)
    print(f"Î²={beta:.1f}: ç·æå¤±={loss.item():.2f}, "
          f"å†æ§‹æˆ={recon.item():.2f}, KL={kl.item():.2f}")

print("\nè§£é‡ˆ:")
print("- Î²ãŒå¤§ãã„: KLé …ã‚’é‡è¦– â†’ æ½œåœ¨ç©ºé–“ãŒæ­£è¦åˆ†å¸ƒã«è¿‘ã¥ã")
print("- Î²ãŒå°ã•ã„: å†æ§‹æˆã‚’é‡è¦– â†’ å†æ§‹æˆå“è³ªãŒå‘ä¸Š")
</code></pre>

        <h2>5. å®Ÿè·µå¿œç”¨</h2>

        <h3>5.1 äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±é–¢æ•°</h3>

        <p>åˆ†é¡å•é¡Œã§æœ€ã‚‚ä¸€èˆ¬çš„ãªæå¤±é–¢æ•°ã¯ã€äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã§ã™ã€‚</p>

        <div class="math-block">
            $$\mathcal{L}_{CE} = -\sum_{i=1}^{N} \sum_{c=1}^{C} y_{i,c} \log \hat{y}_{i,c}$$
        </div>

        <p>ã“ã“ã§ã€y_{i,c}ã¯çœŸã®ãƒ©ãƒ™ãƒ«ï¼ˆone-hotï¼‰ã€\(\hat{y}_{i,c}\)ã¯ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡ã§ã™ã€‚</p>

        <h3>5.2 KLæå¤±ã¨ãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°</h3>

        <p>ãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã¯ã€éä¿¡ã‚’é˜²ãæ­£å‰‡åŒ–æ‰‹æ³•ã§ã™ã€‚ãƒãƒ¼ãƒ‰ãƒ©ãƒ™ãƒ«[0,1,0]ã‚’[Îµ/K, 1-Îµ+Îµ/K, Îµ/K]ã«å¤‰æ›ã—ã¾ã™ã€‚</p>

        <h3>å®Ÿè£…ä¾‹5ï¼šäº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¨KLæå¤±</h3>

<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt

class LossFunctions:
    """æƒ…å ±ç†è«–ã«åŸºã¥ãæå¤±é–¢æ•°"""

    @staticmethod
    def cross_entropy_loss(logits, targets):
        """
        äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±

        Parameters:
        -----------
        logits : Tensor of shape (batch_size, num_classes)
            ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ï¼ˆã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹å‰ï¼‰
        targets : Tensor of shape (batch_size,)
            çœŸã®ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«

        Returns:
        --------
        loss : Tensor
            äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±
        """
        return F.cross_entropy(logits, targets)

    @staticmethod
    def kl_div_loss(logits, target_dist):
        """
        KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹æå¤±

        D_KL(target || pred) ã‚’è¨ˆç®—

        Parameters:
        -----------
        logits : Tensor
            ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›
        target_dist : Tensor
            ç›®æ¨™åˆ†å¸ƒï¼ˆç¢ºç‡åˆ†å¸ƒï¼‰

        Returns:
        --------
        loss : Tensor
            KLæå¤±
        """
        log_pred = F.log_softmax(logits, dim=-1)
        return F.kl_div(log_pred, target_dist, reduction='batchmean')

    @staticmethod
    def label_smoothing_loss(logits, targets, smoothing=0.1):
        """
        ãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ä»˜ãäº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼

        Parameters:
        -----------
        smoothing : float
            ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ0: ãªã—, 1: å®Œå…¨ã«ä¸€æ§˜ï¼‰
        """
        n_classes = logits.size(-1)
        log_pred = F.log_softmax(logits, dim=-1)

        # ãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
        # çœŸã®ã‚¯ãƒ©ã‚¹: 1 - Îµ + Îµ/K
        # ä»–ã®ã‚¯ãƒ©ã‚¹: Îµ/K
        with torch.no_grad():
            true_dist = torch.zeros_like(log_pred)
            true_dist.fill_(smoothing / (n_classes - 1))
            true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - smoothing)

        return torch.mean(torch.sum(-true_dist * log_pred, dim=-1))

# ä½¿ç”¨ä¾‹1: æå¤±é–¢æ•°ã®æ¯”è¼ƒ
print("=" * 50)
print("æå¤±é–¢æ•°ã®æ¯”è¼ƒ")
print("=" * 50)

torch.manual_seed(42)

# ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
batch_size = 4
num_classes = 3

logits = torch.randn(batch_size, num_classes) * 2
targets = torch.tensor([0, 1, 2, 1])

# å„æå¤±ã®è¨ˆç®—
ce_loss = LossFunctions.cross_entropy_loss(logits, targets)

# ãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°æå¤±
ls_loss_01 = LossFunctions.label_smoothing_loss(logits, targets, smoothing=0.1)
ls_loss_03 = LossFunctions.label_smoothing_loss(logits, targets, smoothing=0.3)

print(f"äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±: {ce_loss.item():.4f}")
print(f"ãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°æå¤±ï¼ˆÎµ=0.1ï¼‰: {ls_loss_01.item():.4f}")
print(f"ãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°æå¤±ï¼ˆÎµ=0.3ï¼‰: {ls_loss_03.item():.4f}")

# äºˆæ¸¬ç¢ºç‡ã®è¡¨ç¤º
probs = F.softmax(logits, dim=-1)
print(f"\näºˆæ¸¬ç¢ºç‡:")
for i in range(batch_size):
    print(f"  ã‚µãƒ³ãƒ—ãƒ«{i} (çœŸã®ãƒ©ãƒ™ãƒ«={targets[i]}): {probs[i].numpy()}")

# ä½¿ç”¨ä¾‹2: ä¿¡é ¼åº¦ã¨æå¤±ã®é–¢ä¿‚
print("\n" + "=" * 50)
print("ãƒ¢ãƒ‡ãƒ«ã®ä¿¡é ¼åº¦ã¨æå¤±ã®é–¢ä¿‚")
print("=" * 50)

# ä¿¡é ¼åº¦ã‚’å¤‰åŒ–ã•ã›ã‚‹
confidences = np.linspace(0.1, 0.99, 50)
losses = []

for conf in confidences:
    # æ­£ã—ã„ã‚¯ãƒ©ã‚¹ã¸ã®ç¢ºç‡ãŒconfã®ãƒ­ã‚¸ãƒƒãƒˆã‚’ä½œæˆ
    # [conf, (1-conf)/2, (1-conf)/2]
    logits_conf = torch.tensor([[
        np.log(conf),
        np.log((1-conf)/2),
        np.log((1-conf)/2)
    ]])
    target = torch.tensor([0])

    loss = LossFunctions.cross_entropy_loss(logits_conf, target)
    losses.append(loss.item())

# å¯è¦–åŒ–
plt.figure(figsize=(10, 5))
plt.plot(confidences, losses, 'b-', linewidth=2)
plt.xlabel('æ­£è§£ã‚¯ãƒ©ã‚¹ã¸ã®äºˆæ¸¬ç¢ºç‡')
plt.ylabel('äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±')
plt.title('äºˆæ¸¬ä¿¡é ¼åº¦ã¨æå¤±ã®é–¢ä¿‚')
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='r', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.savefig('confidence_loss.png', dpi=150, bbox_inches='tight')
print("ä¿¡é ¼åº¦ã¨æå¤±ã®é–¢ä¿‚ã‚’å¯è¦–åŒ–ã—ã¾ã—ãŸ")

print(f"\nè¦³å¯Ÿ:")
print(f"- ä¿¡é ¼åº¦0.5ã®æå¤±: {losses[20]:.4f}")
print(f"- ä¿¡é ¼åº¦0.9ã®æå¤±: {losses[40]:.4f}")
print(f"- ä¿¡é ¼åº¦0.99ã®æå¤±: {losses[-1]:.4f}")
print("â†’ ä¿¡é ¼åº¦ãŒé«˜ã„ã»ã©æå¤±ãŒå°ã•ããªã‚‹ï¼ˆæŒ‡æ•°çš„ã«æ¸›å°‘ï¼‰")
</code></pre>

        <h3>5.3 ELBOï¼ˆEvidence Lower Boundï¼‰</h3>

        <p>ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã§é‡è¦ãªELBOã¯ã€å¯¾æ•°å‘¨è¾ºå°¤åº¦ã®ä¸‹ç•Œã‚’ä¸ãˆã¾ã™ã€‚</p>

        <div class="math-block">
            $$\text{ELBO} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{KL}(q(z|x)||p(z))$$
        </div>

        <h3>å®Ÿè£…ä¾‹6ï¼šELBOã®è©³ç´°è¨ˆç®—</h3>

<pre><code>import torch
import numpy as np
import matplotlib.pyplot as plt

class ELBOAnalysis:
    """ELBOã®è©³ç´°åˆ†æ"""

    @staticmethod
    def compute_elbo_components(x, x_recon, mu, logvar, n_samples=1000):
        """
        ELBOã®å„æˆåˆ†ã‚’è©³ç´°ã«è¨ˆç®—

        Parameters:
        -----------
        x : Tensor
            å…ƒã®ãƒ‡ãƒ¼ã‚¿
        x_recon : Tensor
            å†æ§‹æˆãƒ‡ãƒ¼ã‚¿
        mu, logvar : Tensor
            ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›ï¼ˆæ½œåœ¨åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
        n_samples : int
            ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚µãƒ³ãƒ—ãƒ«æ•°

        Returns:
        --------
        dict : ELBOã®å„æˆåˆ†
        """
        batch_size = x.size(0)
        latent_dim = mu.size(1)

        # 1. å†æ§‹æˆé …: E_q[log p(x|z)]
        # ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã®å ´åˆã®å¯¾æ•°å°¤åº¦
        recon_term = -F.binary_cross_entropy(
            x_recon, x.view(batch_size, -1), reduction='sum'
        ) / batch_size

        # 2. KLé …ï¼ˆè§£æçš„è¨ˆç®—ï¼‰: D_KL(q(z|x)||p(z))
        # q(z|x) = N(Î¼, ÏƒÂ²), p(z) = N(0, I)
        kl_term = -0.5 * torch.sum(
            1 + logvar - mu.pow(2) - logvar.exp()
        ) / batch_size

        # 3. ELBOã®è¨ˆç®—
        elbo = recon_term - kl_term

        # 4. ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ¨å®šã«ã‚ˆã‚‹æ¤œè¨¼
        # E_q[log p(x|z)] ã®ã‚µãƒ³ãƒ—ãƒ«æ¨å®š
        std = torch.exp(0.5 * logvar)
        recon_mc = 0
        for _ in range(n_samples):
            eps = torch.randn_like(std)
            z = mu + eps * std
            # ç°¡ç•¥åŒ–ã—ãŸå†æ§‹æˆå°¤åº¦
            recon_mc += -F.binary_cross_entropy(
                x_recon, x.view(batch_size, -1), reduction='sum'
            )
        recon_mc = recon_mc / (n_samples * batch_size)

        return {
            'elbo': elbo.item(),
            'reconstruction': recon_term.item(),
            'kl_divergence': kl_term.item(),
            'reconstruction_mc': recon_mc.item(),
            'log_marginal_lower_bound': elbo.item()
        }

    @staticmethod
    def analyze_latent_distribution(mu, logvar):
        """
        æ½œåœ¨åˆ†å¸ƒã®çµ±è¨ˆã‚’åˆ†æ

        Returns:
        --------
        dict : çµ±è¨ˆé‡
        """
        std = torch.exp(0.5 * logvar)

        return {
            'mean_mu': mu.mean().item(),
            'std_mu': mu.std().item(),
            'mean_sigma': std.mean().item(),
            'std_sigma': std.std().item(),
            'min_sigma': std.min().item(),
            'max_sigma': std.max().item()
        }

# ä½¿ç”¨ä¾‹
print("=" * 50)
print("ELBOã®è©³ç´°åˆ†æ")
print("=" * 50)

# ãƒ€ãƒŸãƒ¼VAEãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›
torch.manual_seed(42)
batch_size = 16
input_dim = 784
latent_dim = 20

x = torch.rand(batch_size, 1, 28, 28)
mu = torch.randn(batch_size, latent_dim) * 0.5
logvar = torch.randn(batch_size, latent_dim) * 0.5

# å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–
std = torch.exp(0.5 * logvar)
z = mu + torch.randn_like(std) * std
x_recon = torch.sigmoid(torch.randn(batch_size, input_dim))

# ELBOã®è¨ˆç®—
elbo_components = ELBOAnalysis.compute_elbo_components(
    x, x_recon, mu, logvar, n_samples=100
)

print("ELBOã®å„æˆåˆ†:")
for key, value in elbo_components.items():
    print(f"  {key}: {value:.4f}")

# æ½œåœ¨åˆ†å¸ƒã®åˆ†æ
latent_stats = ELBOAnalysis.analyze_latent_distribution(mu, logvar)

print("\næ½œåœ¨åˆ†å¸ƒã®çµ±è¨ˆ:")
for key, value in latent_stats.items():
    print(f"  {key}: {value:.4f}")

# å¯è¦–åŒ–: æ½œåœ¨æ¬¡å…ƒã”ã¨ã®Î¼ã¨Ïƒ
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# å¹³å‡Î¼ã®åˆ†å¸ƒ
axes[0].hist(mu.detach().numpy().flatten(), bins=30, alpha=0.7, color='blue')
axes[0].set_xlabel('Î¼')
axes[0].set_ylabel('é »åº¦')
axes[0].set_title('æ½œåœ¨å¤‰æ•°ã®å¹³å‡ï¼ˆÎ¼ï¼‰ã®åˆ†å¸ƒ')
axes[0].axvline(x=0, color='r', linestyle='--', label='äº‹å‰åˆ†å¸ƒã®å¹³å‡')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# æ¨™æº–åå·®Ïƒã®åˆ†å¸ƒ
axes[1].hist(std.detach().numpy().flatten(), bins=30, alpha=0.7, color='green')
axes[1].set_xlabel('Ïƒ')
axes[1].set_ylabel('é »åº¦')
axes[1].set_title('æ½œåœ¨å¤‰æ•°ã®æ¨™æº–åå·®ï¼ˆÏƒï¼‰ã®åˆ†å¸ƒ')
axes[1].axvline(x=1, color='r', linestyle='--', label='äº‹å‰åˆ†å¸ƒã®æ¨™æº–åå·®')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('elbo_analysis.png', dpi=150, bbox_inches='tight')
print("\nELBOåˆ†æã®å¯è¦–åŒ–ã‚’ä¿å­˜ã—ã¾ã—ãŸ")

print("\n" + "=" * 50)
print("æƒ…å ±ç†è«–çš„è§£é‡ˆ")
print("=" * 50)
print("ELBO = å†æ§‹æˆé … - KLé …")
print("  å†æ§‹æˆé …: ãƒ‡ãƒ¼ã‚¿ã‚’æ½œåœ¨å¤‰æ•°ã§èª¬æ˜ã™ã‚‹èƒ½åŠ›")
print("  KLé …: æ½œåœ¨åˆ†å¸ƒãŒäº‹å‰åˆ†å¸ƒã‹ã‚‰ã©ã‚Œã ã‘é›¢ã‚Œã¦ã„ã‚‹ã‹")
print("  â†’ ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•: è‰¯ã„å†æ§‹æˆ vs æ­£å‰‡åŒ–ã•ã‚ŒãŸæ½œåœ¨ç©ºé–“")
</code></pre>

        <h2>ã¾ã¨ã‚</h2>

        <p>ã“ã®ç« ã§ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã‚’æ”¯ãˆã‚‹æƒ…å ±ç†è«–ã®åŸºç¤ã‚’å­¦ã³ã¾ã—ãŸã€‚</p>

        <div class="info-box">
            <strong>å­¦ç¿’ã—ãŸå†…å®¹</strong>
            <ul style="margin-left: 1.5rem; margin-bottom: 0;">
                <li><strong>ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼</strong>: ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–ã¨æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼</li>
                <li><strong>KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹</strong>: ç¢ºç‡åˆ†å¸ƒã®å·®ç•°ã‚’æ¸¬ã‚‹éå¯¾ç§°ãªæŒ‡æ¨™</li>
                <li><strong>äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼</strong>: åˆ†é¡å•é¡Œã®æå¤±é–¢æ•°ã®ç†è«–çš„åŸºç¤</li>
                <li><strong>ç›¸äº’æƒ…å ±é‡</strong>: å¤‰æ•°é–“ã®ä¾å­˜æ€§ã¨ç‰¹å¾´é¸æŠã¸ã®å¿œç”¨</li>
                <li><strong>VAEã¨ELBO</strong>: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ç†è«–çš„è§£é‡ˆ</li>
            </ul>
        </div>

        <div class="warning-box">
            <strong>æ¬¡ç« ã¸ã®æº–å‚™</strong>
            ç¬¬5ç« ã§ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã®å­¦ç¿’ç†è«–ã‚’å­¦ã³ã¾ã™ã€‚ã“ã®ç« ã§å­¦ã‚“ã KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚„ç›¸äº’æƒ…å ±é‡ã¯ã€æ±åŒ–èª¤å·®ã‚„ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘åº¦ã‚’ç†è§£ã™ã‚‹ä¸Šã§é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚
        </div>

        <h3>æ¼”ç¿’å•é¡Œ</h3>

        <ol>
            <li>ã‚µã‚¤ã‚³ãƒ­ï¼ˆ6é¢ï¼‰ã¨åã£ãŸã‚³ã‚¤ãƒ³ï¼ˆP(è¡¨)=0.8ï¼‰ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’è¨ˆç®—ã—ã€æ¯”è¼ƒã—ã¦ãã ã•ã„</li>
            <li>2ã¤ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒN(0,1)ã¨N(2,2)ã®KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’è§£æçš„ã«è¨ˆç®—ã—ã¦ãã ã•ã„</li>
            <li>ç›¸äº’æƒ…å ±é‡I(X;Y)=0ã¨ãªã‚‹ã®ã¯ã€Xã¨YãŒã©ã®ã‚ˆã†ãªé–¢ä¿‚ã®ã¨ãã‹èª¬æ˜ã—ã¦ãã ã•ã„</li>
            <li>Î²-VAEã§Î²=0.5, 1.0, 2.0ã®ã¨ãã®æ½œåœ¨ç©ºé–“ã®é•ã„ã‚’å®Ÿé¨“ã§ç¢ºèªã—ã¦ãã ã•ã„</li>
            <li>ãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ãŒãªãœãƒ¢ãƒ‡ãƒ«ã®éä¿¡ã‚’é˜²ãã®ã‹ã€æƒ…å ±ç†è«–ã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã—ã¦ãã ã•ã„</li>
        </ol>

        <h3>å‚è€ƒæ–‡çŒ®</h3>

        <ul>
            <li>Claude E. Shannon, "A Mathematical Theory of Communication" (1948)</li>
            <li>Thomas M. Cover and Joy A. Thomas, "Elements of Information Theory" (2006)</li>
            <li>D.P. Kingma and M. Welling, "Auto-Encoding Variational Bayes" (2013)</li>
            <li>Naftali Tishby et al., "The Information Bottleneck Method" (2000)</li>
        </ul>

        <div class="nav-buttons">
            <a href="./chapter3-optimization.html" class="nav-button">â† ç¬¬3ç« ï¼šæœ€é©åŒ–ç†è«–</a>
            <a href="./chapter5-learning-theory.html" class="nav-button">ç¬¬5ç« ï¼šå­¦ç¿’ç†è«– â†’</a>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
