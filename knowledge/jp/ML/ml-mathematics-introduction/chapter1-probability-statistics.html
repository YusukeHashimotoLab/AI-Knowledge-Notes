<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="æ©Ÿæ¢°å­¦ç¿’ã®æ•°ç†å…¥é–€ã‚·ãƒªãƒ¼ã‚º ç¬¬1ç« ï¼šç¢ºç‡çµ±è¨ˆã®åŸºç¤ - ãƒ™ã‚¤ã‚ºã®å®šç†ã€ç¢ºç‡åˆ†å¸ƒã€æœ€å°¤æ¨å®šã‚’ç†è«–ã¨å®Ÿè£…ã§å­¦ã¶">
    <title>ç¬¬1ç« ï¼šç¢ºç‡çµ±è¨ˆã®åŸºç¤ - æ©Ÿæ¢°å­¦ç¿’ã®æ•°ç†å…¥é–€ã‚·ãƒªãƒ¼ã‚º</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script>
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }
        .container { max-width: 900px; margin: 0 auto; padding: 2rem 1.5rem; }
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        header .container { padding: 0 1.5rem; }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; font-weight: 700; }
        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }
        .meta span { display: inline-flex; align-items: center; gap: 0.3rem; }
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }
        h3 { font-size: 1.4rem; margin-top: 2rem; margin-bottom: 0.8rem; color: var(--primary-color); }
        h4 { font-size: 1.2rem; margin-top: 1.5rem; margin-bottom: 0.6rem; color: var(--primary-color); }
        p { margin-bottom: 1.2rem; }
        a { color: var(--link-color); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--link-hover); text-decoration: underline; }
        ul, ol { margin-left: 2rem; margin-bottom: 1.2rem; }
        li { margin-bottom: 0.5rem; }
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }
        pre {
            background: var(--code-bg);
            padding: 1.2rem;
            border-radius: 6px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border-left: 4px solid var(--secondary-color);
        }
        pre code {
            background: none;
            padding: 0;
            font-size: 0.85em;
            line-height: 1.6;
        }
        .info-box {
            background: #e8f4f8;
            border-left: 4px solid var(--secondary-color);
            padding: 1rem 1.2rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }
        .info-box strong {
            color: var(--secondary-color);
            display: block;
            margin-bottom: 0.5rem;
        }
        .warning-box {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 1rem 1.2rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }
        .warning-box strong {
            color: #856404;
            display: block;
            margin-bottom: 0.5rem;
        }
        .math-block {
            margin: 1.5rem 0;
            padding: 1rem;
            text-align: center;
            overflow-x: auto;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
        }
        th, td {
            border: 1px solid var(--border-color);
            padding: 0.8rem;
            text-align: left;
        }
        th {
            background: #f8f9fa;
            font-weight: 600;
        }
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }
        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }
        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
            text-decoration: none;
        }
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }
        @media (max-width: 768px) {
            .container { padding: 1rem; }
            h1 { font-size: 1.6rem; }
            h2 { font-size: 1.4rem; }
            .meta { font-size: 0.85rem; }
        }
    
        .feedback-notice {
            background: #fff3cd;
            border: 2px solid #ffc107;
            border-radius: 8px;
            padding: 2rem;
            margin: 3rem auto;
            max-width: 900px;
        }

        .feedback-notice h3 {
            color: #856404;
            font-size: 1.3rem;
            margin-bottom: 1rem;
            text-align: center;
        }

        .feedback-notice p {
            color: #856404;
            font-size: 1rem;
            margin-bottom: 1.5rem;
            text-align: center;
        }

        .feedback-options {
            display: flex;
            justify-content: center;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .feedback-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 600;
            transition: all 0.3s;
        }

        .feedback-button:hover {
            background: #2980b9;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/ml-mathematics-introduction/index.html">Ml Mathematics</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 1</span>
        </div>
    </nav>

        <header>
        <div class="container">
            <h1>ç¬¬1ç« ï¼šç¢ºç‡çµ±è¨ˆã®åŸºç¤</h1>
            <p style="font-size: 1.1rem; margin-top: 0.5rem; opacity: 0.95;">æ©Ÿæ¢°å­¦ç¿’ã®æ•°ç†å…¥é–€ã‚·ãƒªãƒ¼ã‚º v1.0</p>
            <div class="meta">
                <span>ğŸ“– å­¦ç¿’æ™‚é–“: 30-35åˆ†</span>
                <span>ğŸ“Š ãƒ¬ãƒ™ãƒ«: ä¸Šç´š</span>
                <span>ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 6å€‹</span>
                <span>ğŸ¯ ã‚·ãƒªãƒ¼ã‚º: ML-P05</span>
            </div>
        </div>
    </header>

    <main class="container">
        <p><strong>æ©Ÿæ¢°å­¦ç¿’ã®åŸºç›¤ã¨ãªã‚‹ç¢ºç‡çµ±è¨ˆã‚’ç†è«–ã¨å®Ÿè£…ã®ä¸¡é¢ã‹ã‚‰æ·±ãç†è§£ã™ã‚‹</strong></p>

        <div class="info-box">
            <strong>ã“ã®ç« ã§å­¦ã¹ã‚‹ã“ã¨</strong>
            <ul style="margin-left: 1.5rem; margin-bottom: 0;">
                <li>ãƒ™ã‚¤ã‚ºã®å®šç†ã¨æ¡ä»¶ä»˜ãç¢ºç‡ã®æ•°å­¦çš„ç†è§£</li>
                <li>æ­£è¦åˆ†å¸ƒã¨å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®æ€§è³ªã¨å®Ÿè£…</li>
                <li>æœŸå¾…å€¤ã€åˆ†æ•£ã€å…±åˆ†æ•£ã®è¨ˆç®—ã¨å¹¾ä½•çš„æ„å‘³</li>
                <li>æœ€å°¤æ¨å®šã¨ãƒ™ã‚¤ã‚ºæ¨å®šã®ç†è«–çš„é•ã„</li>
                <li>æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¸ã®ç¢ºç‡çµ±è¨ˆã®å¿œç”¨</li>
            </ul>
        </div>

        <h2>1. ç¢ºç‡ã®åŸºç¤</h2>

        <h3>1.1 æ¡ä»¶ä»˜ãç¢ºç‡ã¨ãƒ™ã‚¤ã‚ºã®å®šç†</h3>

        <p>æ¡ä»¶ä»˜ãç¢ºç‡ã¯ã€ã‚ã‚‹äº‹è±¡BãŒèµ·ããŸã¨ã„ã†æ¡ä»¶ã®ä¸‹ã§ã®äº‹è±¡Aã®ç¢ºç‡ã‚’è¡¨ã—ã¾ã™ã€‚</p>

        <div class="math-block">
            $$P(A|B) = \frac{P(A \cap B)}{P(B)}$$
        </div>

        <p>ãƒ™ã‚¤ã‚ºã®å®šç†ã¯ã€äº‹å‰ç¢ºç‡ã¨å°¤åº¦ã‹ã‚‰äº‹å¾Œç¢ºç‡ã‚’è¨ˆç®—ã™ã‚‹åŸºæœ¬å®šç†ã§ã™ã€‚</p>

        <div class="math-block">
            $$P(A|B) = \frac{P(B|A)P(A)}{P(B)} = \frac{P(B|A)P(A)}{\sum_{i} P(B|A_i)P(A_i)}$$
        </div>

        <p>ã“ã“ã§ï¼š</p>
        <ul>
            <li><strong>P(A)</strong>: äº‹å‰ç¢ºç‡ï¼ˆpriorï¼‰- è¦³æ¸¬å‰ã®ä»®èª¬ã®ç¢ºç‡</li>
            <li><strong>P(B|A)</strong>: å°¤åº¦ï¼ˆlikelihoodï¼‰- ä»®èª¬Aã®ä¸‹ã§ãƒ‡ãƒ¼ã‚¿BãŒè¦³æ¸¬ã•ã‚Œã‚‹ç¢ºç‡</li>
            <li><strong>P(A|B)</strong>: äº‹å¾Œç¢ºç‡ï¼ˆposteriorï¼‰- ãƒ‡ãƒ¼ã‚¿è¦³æ¸¬å¾Œã®ä»®èª¬ã®ç¢ºç‡</li>
            <li><strong>P(B)</strong>: å‘¨è¾ºå°¤åº¦ï¼ˆevidenceï¼‰- ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–å®šæ•°</li>
        </ul>

        <div class="info-box">
            <strong>æ©Ÿæ¢°å­¦ç¿’ã§ã®å¿œç”¨</strong>
            ãƒ™ã‚¤ã‚ºã®å®šç†ã¯ã€ãƒŠã‚¤ãƒ¼ãƒ–ãƒ™ã‚¤ã‚ºåˆ†é¡å™¨ã€ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ã€ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ãªã©ã€å¤šãã®æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®åŸºç›¤ã¨ãªã£ã¦ã„ã¾ã™ã€‚
        </div>

        <h3>å®Ÿè£…ä¾‹1ï¼šãƒŠã‚¤ãƒ¼ãƒ–ãƒ™ã‚¤ã‚ºåˆ†é¡å™¨</h3>

        <p>ãƒ™ã‚¤ã‚ºã®å®šç†ã‚’ç”¨ã„ãŸæ–‡æ›¸åˆ†é¡ã®å®Ÿè£…ä¾‹ã§ã™ã€‚å„å˜èªã®å‡ºç¾ãŒç‹¬ç«‹ã¨ä»®å®šã—ã¾ã™ã€‚</p>

<pre><code>import numpy as np
from collections import defaultdict

class NaiveBayesClassifier:
    """ãƒŠã‚¤ãƒ¼ãƒ–ãƒ™ã‚¤ã‚ºåˆ†é¡å™¨ã®å®Ÿè£…"""

    def __init__(self, alpha=1.0):
        """
        Parameters:
        -----------
        alpha : float
            ãƒ©ãƒ—ãƒ©ã‚¹å¹³æ»‘åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆåŠ ç®—å¹³æ»‘åŒ–ï¼‰
        """
        self.alpha = alpha
        self.class_priors = {}
        self.word_probs = defaultdict(dict)
        self.vocab = set()

    def fit(self, X, y):
        """
        è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç¢ºç‡ã‚’å­¦ç¿’

        Parameters:
        -----------
        X : list of list
            å„æ–‡æ›¸ã®å˜èªãƒªã‚¹ãƒˆ
        y : list
            å„æ–‡æ›¸ã®ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«
        """
        n_docs = len(X)
        class_counts = defaultdict(int)
        word_counts = defaultdict(lambda: defaultdict(int))

        # ã‚¯ãƒ©ã‚¹ã”ã¨ã®æ–‡æ›¸æ•°ã¨å˜èªå‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        for doc, label in zip(X, y):
            class_counts[label] += 1
            for word in doc:
                self.vocab.add(word)
                word_counts[label][word] += 1

        # äº‹å‰ç¢ºç‡ P(class) ã‚’è¨ˆç®—
        for label, count in class_counts.items():
            self.class_priors[label] = count / n_docs

        # å°¤åº¦ P(word|class) ã‚’è¨ˆç®—ï¼ˆãƒ©ãƒ—ãƒ©ã‚¹å¹³æ»‘åŒ–é©ç”¨ï¼‰
        vocab_size = len(self.vocab)
        for label in class_counts:
            total_words = sum(word_counts[label].values())
            for word in self.vocab:
                word_count = word_counts[label].get(word, 0)
                # P(word|class) with Laplace smoothing
                self.word_probs[label][word] = (
                    (word_count + self.alpha) /
                    (total_words + self.alpha * vocab_size)
                )

    def predict(self, X):
        """
        ãƒ™ã‚¤ã‚ºã®å®šç†ã§äº‹å¾Œç¢ºç‡ã‚’è¨ˆç®—ã—ã€æœ€ã‚‚ç¢ºç‡ã®é«˜ã„ã‚¯ãƒ©ã‚¹ã‚’äºˆæ¸¬

        log P(class|doc) = log P(class) + Î£ log P(word|class)
        """
        predictions = []
        for doc in X:
            class_scores = {}
            for label in self.class_priors:
                # å¯¾æ•°äº‹å¾Œç¢ºç‡ã‚’è¨ˆç®—ï¼ˆæ•°å€¤å®‰å®šæ€§ã®ãŸã‚ï¼‰
                score = np.log(self.class_priors[label])
                for word in doc:
                    if word in self.vocab:
                        score += np.log(self.word_probs[label][word])
                class_scores[label] = score
            predictions.append(max(class_scores, key=class_scores.get))
        return predictions

# ä½¿ç”¨ä¾‹
X_train = [
    ['æ©Ÿæ¢°', 'å­¦ç¿’', 'æ·±å±¤', 'å­¦ç¿’'],
    ['çµ±è¨ˆ', 'ç¢ºç‡', 'åˆ†å¸ƒ'],
    ['æ·±å±¤', 'ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«', 'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯'],
    ['ç¢ºç‡', 'ãƒ™ã‚¤ã‚º', 'çµ±è¨ˆ']
]
y_train = ['ML', 'Stats', 'ML', 'Stats']

nb = NaiveBayesClassifier(alpha=1.0)
nb.fit(X_train, y_train)

X_test = [['æ©Ÿæ¢°', 'å­¦ç¿’'], ['ãƒ™ã‚¤ã‚º', 'ç¢ºç‡']]
predictions = nb.predict(X_test)
print(f"äºˆæ¸¬çµæœ: {predictions}")  # ['ML', 'Stats']
</code></pre>

        <h2>2. ç¢ºç‡åˆ†å¸ƒ</h2>

        <h3>2.1 æ­£è¦åˆ†å¸ƒï¼ˆã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼‰</h3>

        <p>æ­£è¦åˆ†å¸ƒã¯è‡ªç„¶ç•Œã‚„æ¸¬å®šèª¤å·®ãªã©ã€å¤šãã®ç¾è±¡ã§è¦³æ¸¬ã•ã‚Œã‚‹æœ€ã‚‚é‡è¦ãªé€£ç¶šç¢ºç‡åˆ†å¸ƒã§ã™ã€‚</p>

        <div class="math-block">
            $$\mathcal{N}(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$
        </div>

        <p>ã“ã“ã§ï¼š</p>
        <ul>
            <li><strong>Î¼</strong>: å¹³å‡ï¼ˆæœŸå¾…å€¤ï¼‰- åˆ†å¸ƒã®ä¸­å¿ƒä½ç½®</li>
            <li><strong>ÏƒÂ²</strong>: åˆ†æ•£ - ãƒ‡ãƒ¼ã‚¿ã®æ•£ã‚‰ã°ã‚Šå…·åˆ</li>
            <li><strong>Ïƒ</strong>: æ¨™æº–åå·® - åˆ†æ•£ã®å¹³æ–¹æ ¹</li>
        </ul>

        <div class="info-box">
            <strong>ä¸­å¿ƒæ¥µé™å®šç†</strong>
            ç‹¬ç«‹åŒåˆ†å¸ƒãªç¢ºç‡å¤‰æ•°ã®å’Œã¯ã€å…ƒã®åˆ†å¸ƒã®å½¢ã«é–¢ã‚ã‚‰ãšã€ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå¤§ãããªã‚‹ã¨æ­£è¦åˆ†å¸ƒã«è¿‘ã¥ãã¾ã™ã€‚ã“ã‚ŒãŒæ­£è¦åˆ†å¸ƒãŒé‡è¦ãªç†ç”±ã®ä¸€ã¤ã§ã™ã€‚
        </div>

        <h3>2.2 å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒ</h3>

        <p>å¤šæ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã®ç¢ºç‡åˆ†å¸ƒã‚’è¨˜è¿°ã™ã‚‹å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã¯ã€æ©Ÿæ¢°å­¦ç¿’ã§é »ç¹ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚</p>

        <div class="math-block">
            $$\mathcal{N}(\mathbf{x}|\boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{(2\pi)^{D/2}|\boldsymbol{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)$$
        </div>

        <p>ã“ã“ã§ï¼š</p>
        <ul>
            <li><strong>Î¼</strong>: Dæ¬¡å…ƒå¹³å‡ãƒ™ã‚¯ãƒˆãƒ«</li>
            <li><strong>Î£</strong>: DÃ—Då…±åˆ†æ•£è¡Œåˆ—ï¼ˆå¯¾ç§°æ­£å®šå€¤è¡Œåˆ—ï¼‰</li>
            <li><strong>|Î£|</strong>: å…±åˆ†æ•£è¡Œåˆ—ã®è¡Œåˆ—å¼</li>
        </ul>

        <h3>å®Ÿè£…ä¾‹2ï¼šå¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®å¯è¦–åŒ–</h3>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal

def plot_multivariate_gaussian():
    """2æ¬¡å…ƒå¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®å¯è¦–åŒ–"""

    # å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«ã¨å…±åˆ†æ•£è¡Œåˆ—ã‚’å®šç¾©
    mu = np.array([0, 0])

    # ç•°ãªã‚‹å…±åˆ†æ•£è¡Œåˆ—ã®ã‚±ãƒ¼ã‚¹
    covariances = [
        np.array([[1, 0], [0, 1]]),           # ç‹¬ç«‹ãƒ»ç­‰åˆ†æ•£
        np.array([[2, 0], [0, 0.5]]),         # ç‹¬ç«‹ãƒ»ç•°åˆ†æ•£
        np.array([[1, 0.8], [0.8, 1]]),       # æ­£ã®ç›¸é–¢
        np.array([[1, -0.8], [-0.8, 1]])      # è² ã®ç›¸é–¢
    ]

    titles = ['ç‹¬ç«‹ãƒ»ç­‰åˆ†æ•£', 'ç‹¬ç«‹ãƒ»ç•°åˆ†æ•£', 'æ­£ã®ç›¸é–¢', 'è² ã®ç›¸é–¢']

    # ã‚°ãƒªãƒƒãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ç”Ÿæˆ
    x = np.linspace(-3, 3, 100)
    y = np.linspace(-3, 3, 100)
    X, Y = np.meshgrid(x, y)
    pos = np.dstack((X, Y))

    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    axes = axes.ravel()

    for idx, (cov, title) in enumerate(zip(covariances, titles)):
        # å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®è¨ˆç®—
        rv = multivariate_normal(mu, cov)
        Z = rv.pdf(pos)

        # ç­‰é«˜ç·šãƒ—ãƒ­ãƒƒãƒˆ
        axes[idx].contour(X, Y, Z, levels=10, cmap='viridis')
        axes[idx].set_title(f'{title}\nÎ£ = {cov.tolist()}')
        axes[idx].set_xlabel('xâ‚')
        axes[idx].set_ylabel('xâ‚‚')
        axes[idx].grid(True, alpha=0.3)
        axes[idx].axis('equal')

    plt.tight_layout()
    plt.savefig('multivariate_gaussian.png', dpi=150, bbox_inches='tight')
    print("å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®å¯è¦–åŒ–ã‚’ä¿å­˜ã—ã¾ã—ãŸ")

# å®Ÿè¡Œ
plot_multivariate_gaussian()

# å›ºæœ‰å€¤åˆ†è§£ã«ã‚ˆã‚‹å…±åˆ†æ•£è¡Œåˆ—ã®è§£æ
cov = np.array([[2, 1], [1, 2]])
eigenvalues, eigenvectors = np.linalg.eig(cov)
print(f"\nå…±åˆ†æ•£è¡Œåˆ—:\n{cov}")
print(f"å›ºæœ‰å€¤: {eigenvalues}")
print(f"å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«:\n{eigenvectors}")
print(f"ä¸»è»¸ã®æ–¹å‘ï¼ˆç¬¬1å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ï¼‰: {eigenvectors[:, 0]}")
</code></pre>

        <h2>3. æœŸå¾…å€¤ã¨åˆ†æ•£</h2>

        <h3>3.1 æœŸå¾…å€¤</h3>

        <p>æœŸå¾…å€¤ï¼ˆå¹³å‡ï¼‰ã¯ç¢ºç‡å¤‰æ•°ã®ã€Œä¸­å¿ƒçš„ãªå€¤ã€ã‚’è¡¨ã—ã¾ã™ã€‚</p>

        <div class="math-block">
            $$\mathbb{E}[X] = \sum_{x} x \cdot P(X=x) \quad \text{ï¼ˆé›¢æ•£ï¼‰}$$
            $$\mathbb{E}[X] = \int_{-\infty}^{\infty} x \cdot p(x) dx \quad \text{ï¼ˆé€£ç¶šï¼‰}$$
        </div>

        <p><strong>æœŸå¾…å€¤ã®æ€§è³ªï¼š</strong></p>
        <ul>
            <li>ç·šå½¢æ€§: \(\mathbb{E}[aX + bY] = a\mathbb{E}[X] + b\mathbb{E}[Y]\)</li>
            <li>ç‹¬ç«‹å¤‰æ•°ã®ç©: \(\mathbb{E}[XY] = \mathbb{E}[X]\mathbb{E}[Y]\) ï¼ˆX, Y ãŒç‹¬ç«‹ã®å ´åˆï¼‰</li>
        </ul>

        <h3>3.2 åˆ†æ•£ã¨å…±åˆ†æ•£</h3>

        <p>åˆ†æ•£ã¯ãƒ‡ãƒ¼ã‚¿ã®æ•£ã‚‰ã°ã‚Šå…·åˆã‚’è¡¨ã™æŒ‡æ¨™ã§ã™ã€‚</p>

        <div class="math-block">
            $$\text{Var}[X] = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$$
        </div>

        <p>å…±åˆ†æ•£ã¯2ã¤ã®ç¢ºç‡å¤‰æ•°ã®åŒæ™‚å¤‰å‹•ã‚’è¡¨ã—ã¾ã™ã€‚</p>

        <div class="math-block">
            $$\text{Cov}[X, Y] = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])] = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]$$
        </div>

        <p>ç›¸é–¢ä¿‚æ•°ã¯å…±åˆ†æ•£ã‚’æ­£è¦åŒ–ã—ãŸå€¤ã§ã€-1ã‹ã‚‰1ã®ç¯„å›²ã‚’å–ã‚Šã¾ã™ã€‚</p>

        <div class="math-block">
            $$\rho_{X,Y} = \frac{\text{Cov}[X, Y]}{\sqrt{\text{Var}[X]\text{Var}[Y]}}$$
        </div>

        <h3>å®Ÿè£…ä¾‹3ï¼šæœŸå¾…å€¤ãƒ»åˆ†æ•£ãƒ»å…±åˆ†æ•£ã®è¨ˆç®—</h3>

<pre><code>import numpy as np

class StatisticsCalculator:
    """ç¢ºç‡çµ±è¨ˆã®åŸºæœ¬é‡ã‚’è¨ˆç®—ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    @staticmethod
    def expectation(X, P=None):
        """
        æœŸå¾…å€¤ã®è¨ˆç®—

        Parameters:
        -----------
        X : array-like
            ç¢ºç‡å¤‰æ•°ã®å€¤
        P : array-like, optional
            å„å€¤ã®ç¢ºç‡ï¼ˆNoneã®å ´åˆã¯ä¸€æ§˜åˆ†å¸ƒã¨ä»®å®šï¼‰

        Returns:
        --------
        float : æœŸå¾…å€¤
        """
        X = np.array(X)
        if P is None:
            return np.mean(X)
        else:
            P = np.array(P)
            assert abs(np.sum(P) - 1.0) < 1e-10, "ç¢ºç‡ã®å’Œã¯1ã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“"
            return np.sum(X * P)

    @staticmethod
    def variance(X, P=None):
        """
        åˆ†æ•£ã®è¨ˆç®—: Var[X] = E[XÂ²] - (E[X])Â²
        """
        X = np.array(X)
        E_X = StatisticsCalculator.expectation(X, P)
        E_X2 = StatisticsCalculator.expectation(X**2, P)
        return E_X2 - E_X**2

    @staticmethod
    def covariance(X, Y):
        """
        å…±åˆ†æ•£ã®è¨ˆç®—: Cov[X,Y] = E[XY] - E[X]E[Y]
        """
        X, Y = np.array(X), np.array(Y)
        assert len(X) == len(Y), "Xã¨Yã®é•·ã•ãŒä¸€è‡´ã—ã¾ã›ã‚“"

        E_X = np.mean(X)
        E_Y = np.mean(Y)
        E_XY = np.mean(X * Y)

        return E_XY - E_X * E_Y

    @staticmethod
    def correlation(X, Y):
        """
        ç›¸é–¢ä¿‚æ•°ã®è¨ˆç®—: Ï = Cov[X,Y] / (Ïƒ_X * Ïƒ_Y)
        """
        cov_XY = StatisticsCalculator.covariance(X, Y)
        std_X = np.sqrt(StatisticsCalculator.variance(X))
        std_Y = np.sqrt(StatisticsCalculator.variance(Y))

        return cov_XY / (std_X * std_Y)

    @staticmethod
    def covariance_matrix(data):
        """
        å…±åˆ†æ•£è¡Œåˆ—ã®è¨ˆç®—

        Parameters:
        -----------
        data : ndarray of shape (n_samples, n_features)
            ãƒ‡ãƒ¼ã‚¿è¡Œåˆ—

        Returns:
        --------
        ndarray : å…±åˆ†æ•£è¡Œåˆ— (n_features, n_features)
        """
        data = np.array(data)
        n_samples, n_features = data.shape

        # å„ç‰¹å¾´é‡ã®å¹³å‡ã‚’è¨ˆç®—
        means = np.mean(data, axis=0)

        # ä¸­å¿ƒåŒ–
        centered_data = data - means

        # å…±åˆ†æ•£è¡Œåˆ—: (1/n) * X^T X
        cov_matrix = (centered_data.T @ centered_data) / n_samples

        return cov_matrix

# ä½¿ç”¨ä¾‹
calc = StatisticsCalculator()

# é›¢æ•£ç¢ºç‡å¤‰æ•°ï¼ˆã‚µã‚¤ã‚³ãƒ­ï¼‰
X = [1, 2, 3, 4, 5, 6]
P = [1/6] * 6
print(f"ã‚µã‚¤ã‚³ãƒ­ã®æœŸå¾…å€¤: {calc.expectation(X, P):.2f}")
print(f"ã‚µã‚¤ã‚³ãƒ­ã®åˆ†æ•£: {calc.variance(X, P):.2f}")

# é€£ç¶šãƒ‡ãƒ¼ã‚¿
np.random.seed(42)
data = np.random.randn(1000, 3)  # 3æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿
cov_matrix = calc.covariance_matrix(data)
print(f"\nå…±åˆ†æ•£è¡Œåˆ—:\n{cov_matrix}")

# NumPyã®é–¢æ•°ã¨æ¯”è¼ƒ
cov_numpy = np.cov(data.T)
print(f"\nNumPyã®å…±åˆ†æ•£è¡Œåˆ—:\n{cov_numpy}")
print(f"å·®ã®æœ€å¤§å€¤: {np.max(np.abs(cov_matrix - cov_numpy)):.10f}")
</code></pre>

        <h2>4. æœ€å°¤æ¨å®šã¨ãƒ™ã‚¤ã‚ºæ¨å®š</h2>

        <h3>4.1 æœ€å°¤æ¨å®šï¼ˆMaximum Likelihood Estimationï¼‰</h3>

        <p>æœ€å°¤æ¨å®šã¯ã€è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ãŒå¾—ã‚‰ã‚Œã‚‹ç¢ºç‡ï¼ˆå°¤åº¦ï¼‰ã‚’æœ€å¤§åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ±‚ã‚ã‚‹æ–¹æ³•ã§ã™ã€‚</p>

        <div class="math-block">
            $$\hat{\theta}_{ML} = \arg\max_{\theta} P(D|\theta) = \arg\max_{\theta} \prod_{i=1}^{N} P(x_i|\theta)$$
        </div>

        <p>å¯¾æ•°å°¤åº¦ã‚’ä½¿ã†ã¨è¨ˆç®—ãŒç°¡å˜ã«ãªã‚Šã¾ã™ï¼š</p>

        <div class="math-block">
            $$\hat{\theta}_{ML} = \arg\max_{\theta} \log P(D|\theta) = \arg\max_{\theta} \sum_{i=1}^{N} \log P(x_i|\theta)$$
        </div>

        <div class="info-box">
            <strong>æ­£è¦åˆ†å¸ƒã®æœ€å°¤æ¨å®š</strong>
            æ­£è¦åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Î¼ã¨ÏƒÂ²ã®æœ€å°¤æ¨å®šå€¤ã¯ã€æ¨™æœ¬å¹³å‡ã¨æ¨™æœ¬åˆ†æ•£ã«ä¸€è‡´ã—ã¾ã™ï¼š
            $$\hat{\mu}_{ML} = \frac{1}{N}\sum_{i=1}^{N}x_i, \quad \hat{\sigma}^2_{ML} = \frac{1}{N}\sum_{i=1}^{N}(x_i - \hat{\mu})^2$$
        </div>

        <h3>4.2 ãƒ™ã‚¤ã‚ºæ¨å®šã¨MAPæ¨å®š</h3>

        <p>ãƒ™ã‚¤ã‚ºæ¨å®šã§ã¯ã€äº‹å‰åˆ†å¸ƒã¨ãƒ‡ãƒ¼ã‚¿ã®å°¤åº¦ã‹ã‚‰äº‹å¾Œåˆ†å¸ƒã‚’è¨ˆç®—ã—ã¾ã™ã€‚</p>

        <div class="math-block">
            $$P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)} \propto P(D|\theta)P(\theta)$$
        </div>

        <p>MAPæ¨å®šï¼ˆMaximum A Posterioriï¼‰ã¯äº‹å¾Œç¢ºç‡ã‚’æœ€å¤§åŒ–ã—ã¾ã™ï¼š</p>

        <div class="math-block">
            $$\hat{\theta}_{MAP} = \arg\max_{\theta} P(\theta|D) = \arg\max_{\theta} P(D|\theta)P(\theta)$$
        </div>

        <h3>å®Ÿè£…ä¾‹4ï¼šæ­£è¦åˆ†å¸ƒã®æœ€å°¤æ¨å®šã¨MAPæ¨å®š</h3>

<pre><code>import numpy as np
from scipy.stats import norm

class GaussianEstimator:
    """æ­£è¦åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š"""

    @staticmethod
    def mle(data):
        """
        æœ€å°¤æ¨å®šï¼ˆMaximum Likelihood Estimationï¼‰

        Parameters:
        -----------
        data : array-like
            è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿

        Returns:
        --------
        tuple : (å¹³å‡ã®æ¨å®šå€¤, åˆ†æ•£ã®æ¨å®šå€¤)
        """
        data = np.array(data)
        n = len(data)

        # MLE: æ¨™æœ¬å¹³å‡ã¨æ¨™æœ¬åˆ†æ•£
        mu_mle = np.mean(data)
        sigma2_mle = np.mean((data - mu_mle)**2)  # 1/n * Î£(x - Î¼)Â²

        return mu_mle, sigma2_mle

    @staticmethod
    def map_estimation(data, prior_mu=0, prior_sigma=1, prior_alpha=1, prior_beta=1):
        """
        MAPæ¨å®šï¼ˆMaximum A Posterioriï¼‰

        äº‹å‰åˆ†å¸ƒ:
        - Î¼ ~ N(prior_mu, prior_sigmaÂ²)
        - ÏƒÂ² ~ InverseGamma(prior_alpha, prior_beta)

        Parameters:
        -----------
        data : array-like
            è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿
        prior_mu : float
            å¹³å‡ã®äº‹å‰åˆ†å¸ƒã®å¹³å‡
        prior_sigma : float
            å¹³å‡ã®äº‹å‰åˆ†å¸ƒã®æ¨™æº–åå·®
        prior_alpha, prior_beta : float
            åˆ†æ•£ã®äº‹å‰åˆ†å¸ƒï¼ˆé€†ã‚¬ãƒ³ãƒåˆ†å¸ƒï¼‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
        --------
        tuple : (å¹³å‡ã®MAPæ¨å®šå€¤, åˆ†æ•£ã®MAPæ¨å®šå€¤)
        """
        data = np.array(data)
        n = len(data)
        sample_mean = np.mean(data)
        sample_var = np.mean((data - sample_mean)**2)

        # å¹³å‡ã®MAPæ¨å®šï¼ˆäº‹å‰åˆ†å¸ƒãŒæ­£è¦åˆ†å¸ƒã®å ´åˆï¼‰
        # äº‹å¾Œåˆ†å¸ƒã®å¹³å‡ã¯ã€äº‹å‰åˆ†å¸ƒã¨å°¤åº¦ã®ç²¾åº¦åŠ é‡å¹³å‡
        precision_prior = 1 / prior_sigma**2
        precision_likelihood = n / sample_var

        mu_map = (precision_prior * prior_mu + precision_likelihood * sample_mean) / \
                 (precision_prior + precision_likelihood)

        # åˆ†æ•£ã®MAPæ¨å®šï¼ˆäº‹å‰åˆ†å¸ƒãŒé€†ã‚¬ãƒ³ãƒåˆ†å¸ƒã®å ´åˆï¼‰
        # ç°¡æ˜“çš„ã«ã€äº‹å‰åˆ†å¸ƒã®å½±éŸ¿ã‚’è€ƒæ…®ã—ãŸæ¨å®š
        alpha_post = prior_alpha + n / 2
        beta_post = prior_beta + 0.5 * np.sum((data - mu_map)**2)

        sigma2_map = beta_post / (alpha_post + 1)

        return mu_map, sigma2_map

    @staticmethod
    def compare_estimators(data, true_mu=0, true_sigma=1):
        """MLEæ¨å®šã¨MAPæ¨å®šã®æ¯”è¼ƒ"""

        mu_mle, sigma2_mle = GaussianEstimator.mle(data)
        mu_map, sigma2_map = GaussianEstimator.map_estimation(
            data, prior_mu=true_mu, prior_sigma=true_sigma
        )

        print(f"çœŸã®å€¤: Î¼={true_mu:.3f}, ÏƒÂ²={true_sigma**2:.3f}")
        print(f"\nMLEæ¨å®š:")
        print(f"  Î¼Ì‚_MLE = {mu_mle:.3f}, ÏƒÌ‚Â²_MLE = {sigma2_mle:.3f}")
        print(f"  èª¤å·®: |Î¼-Î¼Ì‚|={abs(true_mu-mu_mle):.3f}, |ÏƒÂ²-ÏƒÌ‚Â²|={abs(true_sigma**2-sigma2_mle):.3f}")

        print(f"\nMAPæ¨å®š:")
        print(f"  Î¼Ì‚_MAP = {mu_map:.3f}, ÏƒÌ‚Â²_MAP = {sigma2_map:.3f}")
        print(f"  èª¤å·®: |Î¼-Î¼Ì‚|={abs(true_mu-mu_map):.3f}, |ÏƒÂ²-ÏƒÌ‚Â²|={abs(true_sigma**2-sigma2_map):.3f}")

        return (mu_mle, sigma2_mle), (mu_map, sigma2_map)

# ä½¿ç”¨ä¾‹
np.random.seed(42)

# ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã„å ´åˆï¼ˆMAPæ¨å®šãŒæœ‰åˆ©ï¼‰
print("=" * 50)
print("å°ã‚µãƒ³ãƒ—ãƒ«ï¼ˆn=10ï¼‰ã§ã®æ¯”è¼ƒ")
print("=" * 50)
data_small = np.random.normal(0, 1, size=10)
GaussianEstimator.compare_estimators(data_small)

# ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã„å ´åˆï¼ˆMLEã¨MAPãŒè¿‘ä¼¼ï¼‰
print("\n" + "=" * 50)
print("å¤§ã‚µãƒ³ãƒ—ãƒ«ï¼ˆn=1000ï¼‰ã§ã®æ¯”è¼ƒ")
print("=" * 50)
data_large = np.random.normal(0, 1, size=1000)
GaussianEstimator.compare_estimators(data_large)
</code></pre>

        <h3>4.3 MLEã¨ãƒ™ã‚¤ã‚ºæ¨å®šã®æ¯”è¼ƒ</h3>

        <table>
            <thead>
                <tr>
                    <th>è¦³ç‚¹</th>
                    <th>æœ€å°¤æ¨å®šï¼ˆMLEï¼‰</th>
                    <th>ãƒ™ã‚¤ã‚ºæ¨å®š</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ‰±ã„</td>
                    <td>å›ºå®šå€¤ï¼ˆç‚¹æ¨å®šï¼‰</td>
                    <td>ç¢ºç‡å¤‰æ•°ï¼ˆåˆ†å¸ƒæ¨å®šï¼‰</td>
                </tr>
                <tr>
                    <td>äº‹å‰çŸ¥è­˜</td>
                    <td>ä½¿ç”¨ã—ãªã„</td>
                    <td>äº‹å‰åˆ†å¸ƒã§è¡¨ç¾</td>
                </tr>
                <tr>
                    <td>ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å ´åˆ</td>
                    <td>éå­¦ç¿’ã—ã‚„ã™ã„</td>
                    <td>äº‹å‰çŸ¥è­˜ã§è£œå®Œ</td>
                </tr>
                <tr>
                    <td>è¨ˆç®—ã‚³ã‚¹ãƒˆ</td>
                    <td>ä½ã„</td>
                    <td>é«˜ã„ï¼ˆç©åˆ†ãŒå¿…è¦ï¼‰</td>
                </tr>
                <tr>
                    <td>ä¸ç¢ºå®Ÿæ€§ã®è¡¨ç¾</td>
                    <td>ç‚¹æ¨å®šã®ã¿</td>
                    <td>äº‹å¾Œåˆ†å¸ƒå…¨ä½“</td>
                </tr>
            </tbody>
        </table>

        <h2>5. å®Ÿè·µå¿œç”¨</h2>

        <h3>5.1 æ··åˆã‚¬ã‚¦ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆGMMï¼‰</h3>

        <p>æ··åˆã‚¬ã‚¦ã‚¹ãƒ¢ãƒ‡ãƒ«ã¯ã€è¤‡æ•°ã®æ­£è¦åˆ†å¸ƒã®é‡ã¿ä»˜ãå’Œã§è¤‡é›‘ãªåˆ†å¸ƒã‚’è¡¨ç¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</p>

        <div class="math-block">
            $$P(x) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)$$
        </div>

        <p>ã“ã“ã§ã€\(\pi_k\)ã¯å„ã‚¬ã‚¦ã‚¹æˆåˆ†ã®æ··åˆä¿‚æ•°ï¼ˆ\(\sum_k \pi_k = 1\)ï¼‰ã§ã™ã€‚</p>

        <h3>å®Ÿè£…ä¾‹5ï¼šGMMã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°</h3>

<pre><code>import numpy as np
from scipy.stats import multivariate_normal

class GaussianMixtureModel:
    """æ··åˆã‚¬ã‚¦ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆEM ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹å­¦ç¿’ï¼‰"""

    def __init__(self, n_components=2, max_iter=100, tol=1e-4):
        """
        Parameters:
        -----------
        n_components : int
            ã‚¬ã‚¦ã‚¹æˆåˆ†ã®æ•°
        max_iter : int
            æœ€å¤§åå¾©å›æ•°
        tol : float
            åæŸåˆ¤å®šã®é–¾å€¤
        """
        self.n_components = n_components
        self.max_iter = max_iter
        self.tol = tol

    def initialize_parameters(self, X):
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸåŒ–"""
        n_samples, n_features = X.shape

        # ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ‡ãƒ¼ã‚¿ç‚¹ã‚’é¸ã‚“ã§åˆæœŸå¹³å‡ã¨ã™ã‚‹
        random_idx = np.random.choice(n_samples, self.n_components, replace=False)
        self.means = X[random_idx]

        # å…±åˆ†æ•£è¡Œåˆ—ã‚’å˜ä½è¡Œåˆ—ã§åˆæœŸåŒ–
        self.covariances = np.array([np.eye(n_features) for _ in range(self.n_components)])

        # æ··åˆä¿‚æ•°ã‚’å‡ç­‰ã«åˆæœŸåŒ–
        self.weights = np.ones(self.n_components) / self.n_components

    def e_step(self, X):
        """
        Eã‚¹ãƒ†ãƒƒãƒ—: è²¬ä»»åº¦ï¼ˆå„ãƒ‡ãƒ¼ã‚¿ãŒã©ã®ã‚¬ã‚¦ã‚¹æˆåˆ†ã«å±ã™ã‚‹ã‹ï¼‰ã‚’è¨ˆç®—

        Î³(z_nk) = Ï€_k N(x_n|Î¼_k,Î£_k) / Î£_j Ï€_j N(x_n|Î¼_j,Î£_j)
        """
        n_samples = X.shape[0]
        responsibilities = np.zeros((n_samples, self.n_components))

        for k in range(self.n_components):
            # å„æˆåˆ†ã®å°¤åº¦ã‚’è¨ˆç®—
            rv = multivariate_normal(self.means[k], self.covariances[k])
            responsibilities[:, k] = self.weights[k] * rv.pdf(X)

        # æ­£è¦åŒ–ã—ã¦è²¬ä»»åº¦ã‚’è¨ˆç®—
        responsibilities /= responsibilities.sum(axis=1, keepdims=True)

        return responsibilities

    def m_step(self, X, responsibilities):
        """
        Mã‚¹ãƒ†ãƒƒãƒ—: è²¬ä»»åº¦ã‚’ä½¿ã£ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°
        """
        n_samples, n_features = X.shape

        # å„æˆåˆ†ã®æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«æ•°
        N_k = responsibilities.sum(axis=0)

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´æ–°
        for k in range(self.n_components):
            # æ··åˆä¿‚æ•°ã®æ›´æ–°
            self.weights[k] = N_k[k] / n_samples

            # å¹³å‡ã®æ›´æ–°
            self.means[k] = (responsibilities[:, k].reshape(-1, 1) * X).sum(axis=0) / N_k[k]

            # å…±åˆ†æ•£è¡Œåˆ—ã®æ›´æ–°
            diff = X - self.means[k]
            self.covariances[k] = (responsibilities[:, k].reshape(-1, 1, 1) *
                                  (diff[:, :, np.newaxis] @ diff[:, np.newaxis, :])).sum(axis=0) / N_k[k]

            # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã«å°ã•ãªå€¤ã‚’åŠ ãˆã‚‹
            self.covariances[k] += np.eye(n_features) * 1e-6

    def compute_log_likelihood(self, X):
        """å¯¾æ•°å°¤åº¦ã®è¨ˆç®—"""
        n_samples = X.shape[0]
        log_likelihood = 0

        for i in range(n_samples):
            likelihood = 0
            for k in range(self.n_components):
                rv = multivariate_normal(self.means[k], self.covariances[k])
                likelihood += self.weights[k] * rv.pdf(X[i])
            log_likelihood += np.log(likelihood)

        return log_likelihood

    def fit(self, X):
        """EMã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å­¦ç¿’"""
        self.initialize_parameters(X)

        prev_log_likelihood = -np.inf

        for iteration in range(self.max_iter):
            # Eã‚¹ãƒ†ãƒƒãƒ—
            responsibilities = self.e_step(X)

            # Mã‚¹ãƒ†ãƒƒãƒ—
            self.m_step(X, responsibilities)

            # å¯¾æ•°å°¤åº¦ã®è¨ˆç®—
            log_likelihood = self.compute_log_likelihood(X)

            # åæŸåˆ¤å®š
            if abs(log_likelihood - prev_log_likelihood) < self.tol:
                print(f"åæŸã—ã¾ã—ãŸï¼ˆ{iteration+1}å›ã®åå¾©ï¼‰")
                break

            prev_log_likelihood = log_likelihood

            if (iteration + 1) % 10 == 0:
                print(f"åå¾© {iteration+1}: å¯¾æ•°å°¤åº¦ = {log_likelihood:.4f}")

        return self

    def predict(self, X):
        """æœ€ã‚‚è²¬ä»»åº¦ã®é«˜ã„ã‚¯ãƒ©ã‚¹ã‚¿ã‚’äºˆæ¸¬"""
        responsibilities = self.e_step(X)
        return np.argmax(responsibilities, axis=1)

# ä½¿ç”¨ä¾‹
np.random.seed(42)

# 2ã¤ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
data1 = np.random.multivariate_normal([0, 0], [[1, 0], [0, 1]], 150)
data2 = np.random.multivariate_normal([4, 4], [[1, 0.5], [0.5, 1]], 150)
X = np.vstack([data1, data2])

# GMMã®å­¦ç¿’
gmm = GaussianMixtureModel(n_components=2, max_iter=50)
gmm.fit(X)

# ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ
labels = gmm.predict(X)
print(f"\nå­¦ç¿’ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
for k in range(gmm.n_components):
    print(f"æˆåˆ† {k+1}: é‡ã¿={gmm.weights[k]:.3f}, å¹³å‡={gmm.means[k]}")
</code></pre>

        <h3>5.2 ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°</h3>

        <p>ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ã§ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ç¢ºç‡åˆ†å¸ƒã‚’è€ƒãˆã€äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ã‚‚æ¨å®šã§ãã¾ã™ã€‚</p>

        <div class="math-block">
            $$P(\mathbf{w}|D) = \frac{P(D|\mathbf{w})P(\mathbf{w})}{P(D)}$$
        </div>

        <p>äºˆæ¸¬åˆ†å¸ƒã¯äº‹å¾Œåˆ†å¸ƒã‚’å‘¨è¾ºåŒ–ã—ã¦å¾—ã‚‰ã‚Œã¾ã™ï¼š</p>

        <div class="math-block">
            $$P(y^*|x^*, D) = \int P(y^*|x^*, \mathbf{w})P(\mathbf{w}|D)d\mathbf{w}$$
        </div>

        <h3>å®Ÿè£…ä¾‹6ï¼šãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°</h3>

<pre><code>import numpy as np
import matplotlib.pyplot as plt

class BayesianLinearRegression:
    """ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ã®å®Ÿè£…"""

    def __init__(self, alpha=1.0, beta=1.0):
        """
        Parameters:
        -----------
        alpha : float
            é‡ã¿ã®äº‹å‰åˆ†å¸ƒã®ç²¾åº¦ï¼ˆÎ» = Î± * Iï¼‰
        beta : float
            è¦³æ¸¬ãƒã‚¤ã‚ºã®ç²¾åº¦ï¼ˆ1/ÏƒÂ²ï¼‰
        """
        self.alpha = alpha  # é‡ã¿ã®äº‹å‰ç²¾åº¦
        self.beta = beta    # ãƒã‚¤ã‚ºã®ç²¾åº¦

    def fit(self, X, y):
        """
        è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰äº‹å¾Œåˆ†å¸ƒã‚’è¨ˆç®—

        äº‹å¾Œåˆ†å¸ƒ: P(w|D) = N(w|m_N, S_N)
        S_N = (Î±*I + Î²*X^T*X)^(-1)
        m_N = Î²*S_N*X^T*y
        """
        X = np.array(X)
        y = np.array(y).reshape(-1, 1)

        n_samples, n_features = X.shape

        # äº‹å‰åˆ†å¸ƒã®ç²¾åº¦è¡Œåˆ—
        prior_precision = self.alpha * np.eye(n_features)

        # äº‹å¾Œåˆ†å¸ƒã®å…±åˆ†æ•£è¡Œåˆ—ï¼ˆç²¾åº¦è¡Œåˆ—ã®é€†è¡Œåˆ—ï¼‰
        posterior_precision = prior_precision + self.beta * (X.T @ X)
        self.posterior_cov = np.linalg.inv(posterior_precision)

        # äº‹å¾Œåˆ†å¸ƒã®å¹³å‡
        self.posterior_mean = self.beta * (self.posterior_cov @ X.T @ y)

        return self

    def predict(self, X_test, return_std=False):
        """
        äºˆæ¸¬åˆ†å¸ƒã‚’è¨ˆç®—

        äºˆæ¸¬åˆ†å¸ƒ: P(y*|x*, D) = N(y*|m_N^T*x*, Ïƒ_NÂ²(x*))
        Ïƒ_NÂ²(x*) = 1/Î² + x*^T*S_N*x*
        """
        X_test = np.array(X_test)

        # äºˆæ¸¬å¹³å‡
        y_pred = X_test @ self.posterior_mean

        if return_std:
            # äºˆæ¸¬åˆ†æ•£ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒã‚¤ã‚º + ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸ç¢ºå®Ÿæ€§ï¼‰
            y_var = 1/self.beta + np.sum(X_test @ self.posterior_cov * X_test, axis=1, keepdims=True)
            y_std = np.sqrt(y_var)
            return y_pred.flatten(), y_std.flatten()
        else:
            return y_pred.flatten()

    def sample_weights(self, n_samples=10):
        """äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        return np.random.multivariate_normal(
            self.posterior_mean.flatten(),
            self.posterior_cov,
            size=n_samples
        )

# ä½¿ç”¨ä¾‹ã¨ãƒ™ã‚¤ã‚ºæ¨å®šã®å¯è¦–åŒ–
np.random.seed(42)

# çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: y = 2x + 1 + ãƒã‚¤ã‚º
def true_function(x):
    return 2 * x + 1

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
X_train = np.linspace(0, 1, 10).reshape(-1, 1)
X_train = np.hstack([np.ones_like(X_train), X_train])  # ãƒã‚¤ã‚¢ã‚¹é …ã‚’è¿½åŠ 
y_train = true_function(X_train[:, 1]) + np.random.randn(10) * 0.3

# ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ã®å­¦ç¿’
bayesian_lr = BayesianLinearRegression(alpha=2.0, beta=10.0)
bayesian_lr.fit(X_train, y_train)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
X_test = np.linspace(-0.2, 1.2, 100).reshape(-1, 1)
X_test_with_bias = np.hstack([np.ones_like(X_test), X_test])

# äºˆæ¸¬ï¼ˆå¹³å‡ã¨æ¨™æº–åå·®ï¼‰
y_pred, y_std = bayesian_lr.predict(X_test_with_bias, return_std=True)

# äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
weight_samples = bayesian_lr.sample_weights(n_samples=20)

# å¯è¦–åŒ–
plt.figure(figsize=(12, 5))

# å·¦å›³: äºˆæ¸¬åˆ†å¸ƒã¨ä¿¡é ¼åŒºé–“
plt.subplot(1, 2, 1)
plt.scatter(X_train[:, 1], y_train, c='red', s=50, label='è¨“ç·´ãƒ‡ãƒ¼ã‚¿', zorder=3)
plt.plot(X_test, true_function(X_test), 'g--', linewidth=2, label='çœŸã®é–¢æ•°', zorder=2)
plt.plot(X_test, y_pred, 'b-', linewidth=2, label='äºˆæ¸¬å¹³å‡', zorder=2)
plt.fill_between(X_test.flatten(), y_pred - 2*y_std, y_pred + 2*y_std,
                 alpha=0.3, label='95%ä¿¡é ¼åŒºé–“', zorder=1)
plt.xlabel('x')
plt.ylabel('y')
plt.title('ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°: äºˆæ¸¬åˆ†å¸ƒ')
plt.legend()
plt.grid(True, alpha=0.3)

# å³å›³: äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãŸé–¢æ•°
plt.subplot(1, 2, 2)
plt.scatter(X_train[:, 1], y_train, c='red', s=50, label='è¨“ç·´ãƒ‡ãƒ¼ã‚¿', zorder=3)
for i, w in enumerate(weight_samples):
    y_sample = X_test_with_bias @ w
    plt.plot(X_test, y_sample, 'b-', alpha=0.3, linewidth=1,
             label='ã‚µãƒ³ãƒ—ãƒ«' if i == 0 else '')
plt.plot(X_test, true_function(X_test), 'g--', linewidth=2, label='çœŸã®é–¢æ•°')
plt.xlabel('x')
plt.ylabel('y')
plt.title('äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãŸé–¢æ•°')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('bayesian_regression.png', dpi=150, bbox_inches='tight')
print("ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ã®å¯è¦–åŒ–ã‚’ä¿å­˜ã—ã¾ã—ãŸ")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®äº‹å¾Œåˆ†å¸ƒ
print(f"\nãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®äº‹å¾Œåˆ†å¸ƒ:")
print(f"å¹³å‡: {bayesian_lr.posterior_mean.flatten()}")
print(f"æ¨™æº–åå·®: {np.sqrt(np.diag(bayesian_lr.posterior_cov))}")
</code></pre>

        <h2>ã¾ã¨ã‚</h2>

        <p>ã“ã®ç« ã§ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã®åŸºç›¤ã¨ãªã‚‹ç¢ºç‡çµ±è¨ˆã®åŸºç¤ã‚’å­¦ã³ã¾ã—ãŸã€‚</p>

        <div class="info-box">
            <strong>å­¦ç¿’ã—ãŸå†…å®¹</strong>
            <ul style="margin-left: 1.5rem; margin-bottom: 0;">
                <li><strong>ãƒ™ã‚¤ã‚ºã®å®šç†</strong>: äº‹å‰çŸ¥è­˜ã¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰äº‹å¾Œç¢ºç‡ã‚’è¨ˆç®—ã™ã‚‹åŸºæœ¬åŸç†</li>
                <li><strong>ç¢ºç‡åˆ†å¸ƒ</strong>: æ­£è¦åˆ†å¸ƒã¨å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®æ€§è³ªã¨å®Ÿè£…</li>
                <li><strong>çµ±è¨ˆé‡</strong>: æœŸå¾…å€¤ã€åˆ†æ•£ã€å…±åˆ†æ•£ã®è¨ˆç®—ã¨è§£é‡ˆ</li>
                <li><strong>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š</strong>: æœ€å°¤æ¨å®šã¨ãƒ™ã‚¤ã‚ºæ¨å®šã®é•ã„ã¨é©ç”¨</li>
                <li><strong>å®Ÿè·µå¿œç”¨</strong>: ãƒŠã‚¤ãƒ¼ãƒ–ãƒ™ã‚¤ã‚ºã€GMMã€ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°</li>
            </ul>
        </div>

        <div class="warning-box">
            <strong>æ¬¡ç« ã¸ã®æº–å‚™</strong>
            ç¬¬2ç« ã§ã¯ã€ç·šå½¢ä»£æ•°ã®åŸºç¤ã‚’å­¦ã³ã¾ã™ã€‚ç‰¹ã«ã€è¡Œåˆ—åˆ†è§£ï¼ˆå›ºæœ‰å€¤åˆ†è§£ã€SVDï¼‰ã¨PCAã¯ã€ã“ã®ç« ã§å­¦ã‚“ã å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®å…±åˆ†æ•£è¡Œåˆ—ã¨æ·±ãé–¢é€£ã—ã¾ã™ã€‚
        </div>

        <h3>æ¼”ç¿’å•é¡Œ</h3>

        <ol>
            <li>ãƒ™ã‚¤ã‚ºã®å®šç†ã‚’ä½¿ã£ã¦ã€ã‚¹ãƒ‘ãƒ ãƒ¡ãƒ¼ãƒ«æ¤œå‡ºå™¨ã®ç²¾åº¦ã‚’è¨ˆç®—ã—ã¦ã¿ã¾ã—ã‚‡ã†</li>
            <li>2æ¬¡å…ƒæ­£è¦åˆ†å¸ƒã§ã€ç›¸é–¢ä¿‚æ•°ãŒ0.9ã®å ´åˆã¨-0.9ã®å ´åˆã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã€å¯è¦–åŒ–ã—ã¦ãã ã•ã„</li>
            <li>å°ã‚µãƒ³ãƒ—ãƒ«ï¼ˆn=5ï¼‰ã¨å¤§ã‚µãƒ³ãƒ—ãƒ«ï¼ˆn=1000ï¼‰ã§ã€æœ€å°¤æ¨å®šã¨MAPæ¨å®šã®é•ã„ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„</li>
            <li>GMMã‚’3æˆåˆ†ã«æ‹¡å¼µã—ã€3ã¤ã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã§å‹•ä½œã‚’ç¢ºèªã—ã¦ãã ã•ã„</li>
            <li>ãƒ™ã‚¤ã‚ºç·šå½¢å›å¸°ã§ã€äº‹å‰åˆ†å¸ƒã®ç²¾åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Î±ã‚’å¤‰ãˆã‚‹ã¨äºˆæ¸¬ãŒã©ã†å¤‰ã‚ã‚‹ã‹å®Ÿé¨“ã—ã¦ãã ã•ã„</li>
        </ol>

        <h3>å‚è€ƒæ–‡çŒ®</h3>

        <ul>
            <li>C.M. Bishop, "Pattern Recognition and Machine Learning" (2006)</li>
            <li>Kevin P. Murphy, "Machine Learning: A Probabilistic Perspective" (2012)</li>
            <li>æ‰å±±å°†, "çµ±è¨ˆçš„æ©Ÿæ¢°å­¦ç¿’ã®æ•°ç†100å• with Python" (2020)</li>
        </ul>

        <div class="nav-buttons">
            <a href="./index.html" class="nav-button">â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
            <a href="./chapter2-linear-algebra.html" class="nav-button">ç¬¬2ç« ï¼šç·šå½¢ä»£æ•°ã®åŸºç¤ â†’</a>
        </div>
    </main>


        <div class="feedback-notice">
            <h3>âš ï¸ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å“è³ªå‘ä¸Šã«ã”å”åŠ›ãã ã•ã„</h3>
            <p>ã“ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯AIã‚’æ´»ç”¨ã—ã¦ä½œæˆã•ã‚Œã¦ã„ã¾ã™ã€‚èª¤ã‚Šã‚„æ”¹å–„ç‚¹ã‚’è¦‹ã¤ã‘ã‚‰ã‚ŒãŸå ´åˆã¯ã€ä»¥ä¸‹ã®æ–¹æ³•ã§ã”å ±å‘Šãã ã•ã„ï¼š</p>
            <div class="feedback-options">
                <a href="https://forms.gle/9GfVBa2Qa7Uy9taQA" target="_blank" class="feedback-button">
                    ğŸ“ ä¿®æ­£ä¾é ¼ãƒ•ã‚©ãƒ¼ãƒ 
                </a>
                <a href="mailto:yusuke.hashimoto.d8@tohoku.ac.jp" class="feedback-button">
                    âœ‰ï¸ ãƒ¡ãƒ¼ãƒ«ã§é€£çµ¡
                </a>
            </div>
        </div>

    <footer>
        <div class="container">
            <p>&copy; 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
