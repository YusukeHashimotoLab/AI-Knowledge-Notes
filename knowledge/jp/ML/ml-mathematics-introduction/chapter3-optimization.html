<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬3ç« ï¼šæœ€é©åŒ–ç†è«– - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬3ç« ï¼šæœ€é©åŒ–ç†è«–</h1>
            <p class="subtitle">æ©Ÿæ¢°å­¦ç¿’ã®ãŸã‚ã®æ•°å­¦åŸºç¤ - å‹¾é…é™ä¸‹æ³•ãƒ»åˆ¶ç´„ä»˜ãæœ€é©åŒ–ãƒ»å‡¸æœ€é©åŒ–</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 30-40åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 6å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… æœ€é©åŒ–å•é¡Œã®å®šå¼åŒ–ã¨å‡¸æ€§ã®æ¦‚å¿µã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… å‹¾é…é™ä¸‹æ³•ã®åŸç†ã¨å®Ÿè£…æ–¹æ³•ã‚’ç¿’å¾—ã™ã‚‹</li>
<li>âœ… Momentumã€Adamç­‰ã®æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹</li>
<li>âœ… ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ä¹—æ•°æ³•ã¨KKTæ¡ä»¶ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… å‡¸æœ€é©åŒ–ã®ç†è«–ã¨å®Ÿè·µã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–ã‚’å®Ÿè£…ã§ãã‚‹</li>
</ul>

<hr>

<h2>3.1 æœ€é©åŒ–ã®åŸºç¤</h2>

<h3>æœ€é©åŒ–å•é¡Œã®å®šå¼åŒ–</h3>

<p><strong>æœ€é©åŒ–å•é¡Œï¼ˆOptimization Problemï¼‰</strong>ã¯ã€ç›®çš„é–¢æ•°ã‚’æœ€å°åŒ–ã¾ãŸã¯æœ€å¤§åŒ–ã™ã‚‹å•é¡Œã§ã™ã€‚</p>

<p>$$
\begin{aligned}
\min_{\mathbf{x}} \quad & f(\mathbf{x}) \\
\text{subject to} \quad & g_i(\mathbf{x}) \leq 0, \quad i = 1, \ldots, m \\
& h_j(\mathbf{x}) = 0, \quad j = 1, \ldots, p
\end{aligned}
$$</p>

<ul>
<li>$f(\mathbf{x})$: ç›®çš„é–¢æ•°ï¼ˆæœ€å°åŒ–å¯¾è±¡ï¼‰</li>
<li>$g_i(\mathbf{x}) \leq 0$: ä¸ç­‰å¼åˆ¶ç´„</li>
<li>$h_j(\mathbf{x}) = 0$: ç­‰å¼åˆ¶ç´„</li>
</ul>

<h3>å‡¸é–¢æ•°ã¨å‡¸é›†åˆ</h3>

<p><strong>å‡¸é›†åˆï¼ˆConvex Setï¼‰</strong>ï¼š2ç‚¹ã‚’çµã¶ç·šåˆ†ãŒé›†åˆå†…ã«å«ã¾ã‚Œã‚‹</p>

<p>$$
\mathbf{x}, \mathbf{y} \in C, \ \theta \in [0, 1] \Rightarrow \theta \mathbf{x} + (1-\theta) \mathbf{y} \in C
$$</p>

<p><strong>å‡¸é–¢æ•°ï¼ˆConvex Functionï¼‰</strong>ï¼š2ç‚¹é–“ã®é–¢æ•°å€¤ãŒç·šåˆ†ä»¥ä¸‹</p>

<p>$$
f(\theta \mathbf{x} + (1-\theta) \mathbf{y}) \leq \theta f(\mathbf{x}) + (1-\theta) f(\mathbf{y})
$$</p>

<blockquote>
<p><strong>é‡è¦æ€§</strong>ï¼šå‡¸æœ€é©åŒ–å•é¡Œã¯å±€æ‰€æœ€é©è§£ = å¤§åŸŸæœ€é©è§£ãŒä¿è¨¼ã•ã‚Œã‚‹</p>
</blockquote>

<h3>å‡¸æ€§ã®å¯è¦–åŒ–</h3>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# å‡¸é–¢æ•°ã®ä¾‹ï¼šäºŒæ¬¡é–¢æ•°
def convex_function(x, y):
    return x**2 + y**2

# éå‡¸é–¢æ•°ã®ä¾‹ï¼šHimmelblaué–¢æ•°
def non_convex_function(x, y):
    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2

# ã‚°ãƒªãƒƒãƒ‰ã®ä½œæˆ
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)

Z_convex = convex_function(X, Y)
Z_non_convex = non_convex_function(X, Y)

# å¯è¦–åŒ–
fig = plt.figure(figsize=(15, 6))

# å‡¸é–¢æ•°
ax1 = fig.add_subplot(121, projection='3d')
ax1.plot_surface(X, Y, Z_convex, cmap='viridis', alpha=0.8)
ax1.set_xlabel('x')
ax1.set_ylabel('y')
ax1.set_zlabel('f(x, y)')
ax1.set_title('å‡¸é–¢æ•°: $f(x, y) = x^2 + y^2$', fontsize=14)

# éå‡¸é–¢æ•°
ax2 = fig.add_subplot(122, projection='3d')
ax2.plot_surface(X, Y, Z_non_convex, cmap='plasma', alpha=0.8)
ax2.set_xlabel('x')
ax2.set_ylabel('y')
ax2.set_zlabel('f(x, y)')
ax2.set_title('éå‡¸é–¢æ•°: Himmelblaué–¢æ•°', fontsize=14)

plt.tight_layout()
plt.show()

print("=== å‡¸æ€§ã®ç¢ºèª ===")
print("å‡¸é–¢æ•°: å˜ä¸€ã®å¤§åŸŸæœ€é©è§£ï¼ˆåŸç‚¹ï¼‰")
print("éå‡¸é–¢æ•°: è¤‡æ•°ã®å±€æ‰€æœ€é©è§£ãŒå­˜åœ¨")
</code></pre>

<h3>å‹¾é…ã¨ãƒ˜ã‚·ã‚¢ãƒ³</h3>

<p><strong>å‹¾é…ï¼ˆGradientï¼‰</strong>ï¼šé–¢æ•°ã®å¤‰åŒ–ç‡ã‚’è¡¨ã™ãƒ™ã‚¯ãƒˆãƒ«</p>

<p>$$
\nabla f(\mathbf{x}) = \begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_n}
\end{bmatrix}
$$</p>

<p><strong>ãƒ˜ã‚·ã‚¢ãƒ³è¡Œåˆ—ï¼ˆHessian Matrixï¼‰</strong>ï¼š2æ¬¡åå¾®åˆ†ã®è¡Œåˆ—</p>

<p>$$
\mathbf{H}(f) = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots \\
\vdots & \vdots & \ddots
\end{bmatrix}
$$</p>

<h3>æœ€é©æ€§æ¡ä»¶</h3>

<p><strong>1æ¬¡æ¡ä»¶ï¼ˆå¿…è¦æ¡ä»¶ï¼‰</strong>ï¼š</p>
<p>$$\nabla f(\mathbf{x}^*) = \mathbf{0}$$</p>

<p><strong>2æ¬¡æ¡ä»¶ï¼ˆååˆ†æ¡ä»¶ï¼‰</strong>ï¼š</p>
<p>$$\mathbf{H}(f)(\mathbf{x}^*) \succeq 0 \quad \text{(åŠæ­£å®šå€¤)}$$</p>

<hr>

<h2>3.2 å‹¾é…é™ä¸‹æ³•</h2>

<h3>å‹¾é…é™ä¸‹æ³•ã®åŸç†</h3>

<p><strong>å‹¾é…é™ä¸‹æ³•ï¼ˆGradient Descentï¼‰</strong>ã¯ã€å‹¾é…ã®é€†æ–¹å‘ã«åå¾©çš„ã«ç§»å‹•ã—ã¦æœ€é©è§£ã‚’æ¢ç´¢ã—ã¾ã™ã€‚</p>

<p>$$
\mathbf{x}_{t+1} = \mathbf{x}_t - \alpha \nabla f(\mathbf{x}_t)
$$</p>

<ul>
<li>$\alpha$: å­¦ç¿’ç‡ï¼ˆã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºï¼‰</li>
<li>$\nabla f(\mathbf{x}_t)$: ç¾åœ¨ä½ç½®ã§ã®å‹¾é…</li>
</ul>

<h3>å­¦ç¿’ç‡ã®é¸æŠ</h3>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# ç›®çš„é–¢æ•°ï¼šf(x) = x^2 + 4x + 4
def f(x):
    return x**2 + 4*x + 4

# å‹¾é…ï¼šf'(x) = 2x + 4
def grad_f(x):
    return 2*x + 4

# å‹¾é…é™ä¸‹æ³•
def gradient_descent(x0, lr, n_iterations):
    x = x0
    trajectory = [x]

    for _ in range(n_iterations):
        x = x - lr * grad_f(x)
        trajectory.append(x)

    return np.array(trajectory)

# ç•°ãªã‚‹å­¦ç¿’ç‡ã§ã®å®Ÿé¨“
learning_rates = [0.1, 0.5, 0.9, 1.1]
x0 = 5.0
n_iter = 20

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

x_range = np.linspace(-3, 6, 100)
y_range = f(x_range)

for i, lr in enumerate(learning_rates):
    trajectory = gradient_descent(x0, lr, n_iter)

    axes[i].plot(x_range, y_range, 'b-', linewidth=2, label='$f(x) = x^2 + 4x + 4$')
    axes[i].plot(trajectory, f(trajectory), 'ro-', markersize=6,
                 linewidth=1.5, alpha=0.7, label='æœ€é©åŒ–ã®è»Œè·¡')
    axes[i].plot(-2, 0, 'g*', markersize=20, label='æœ€é©è§£')

    axes[i].set_xlabel('x', fontsize=12)
    axes[i].set_ylabel('f(x)', fontsize=12)
    axes[i].set_title(f'å­¦ç¿’ç‡ Î± = {lr}', fontsize=14)
    axes[i].legend()
    axes[i].grid(True, alpha=0.3)

    # åæŸåˆ¤å®š
    if abs(trajectory[-1] - (-2)) < 0.01:
        status = "âœ“ åæŸ"
    elif lr >= 1.0:
        status = "âœ— ç™ºæ•£"
    else:
        status = "â–³ åæŸãŒé…ã„"

    axes[i].text(0.05, 0.95, status, transform=axes[i].transAxes,
                fontsize=12, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.tight_layout()
plt.show()

print("=== å­¦ç¿’ç‡ã®å½±éŸ¿ ===")
print("Î± = 0.1: åæŸãŒé…ã„")
print("Î± = 0.5: é©åˆ‡ãªåæŸ")
print("Î± = 0.9: é«˜é€ŸåæŸ")
print("Î± = 1.1: ç™ºæ•£ï¼ˆå­¦ç¿’ç‡ãŒå¤§ãã™ãã‚‹ï¼‰")
</code></pre>

<h3>ç¢ºç‡çš„å‹¾é…é™ä¸‹æ³•ï¼ˆSGDï¼‰</h3>

<p><strong>ãƒãƒƒãƒå‹¾é…é™ä¸‹æ³•</strong>ï¼šå…¨ãƒ‡ãƒ¼ã‚¿ã§å‹¾é…è¨ˆç®—ï¼ˆé…ã„ï¼‰</p>
<p><strong>ç¢ºç‡çš„å‹¾é…é™ä¸‹æ³•ï¼ˆSGDï¼‰</strong>ï¼š1ã‚µãƒ³ãƒ—ãƒ«ã§å‹¾é…è¨ˆç®—ï¼ˆé€Ÿã„ã€ãƒã‚¤ã‚ºã‚ã‚Šï¼‰</p>
<p><strong>ãƒŸãƒ‹ãƒãƒƒãƒå‹¾é…é™ä¸‹æ³•</strong>ï¼šå°ãƒãƒƒãƒã§å‹¾é…è¨ˆç®—ï¼ˆãƒãƒ©ãƒ³ã‚¹è‰¯ã„ï¼‰</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆç·šå½¢å›å¸°ï¼‰
np.random.seed(42)
n_samples = 100
X = 2 * np.random.rand(n_samples, 1)
y = 4 + 3 * X + np.random.randn(n_samples, 1)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆæœŸåŒ–
theta_batch = np.random.randn(2, 1)
theta_sgd = theta_batch.copy()
theta_minibatch = theta_batch.copy()

# ãƒã‚¤ã‚¢ã‚¹é …è¿½åŠ 
X_b = np.c_[np.ones((n_samples, 1)), X]

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
n_epochs = 50
learning_rate = 0.01
batch_size = 10

# æå¤±é–¢æ•°ï¼ˆMSEï¼‰
def compute_loss(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    loss = (1/(2*m)) * np.sum((predictions - y)**2)
    return loss

# å‹¾é…è¨ˆç®—
def compute_gradient(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    gradient = (1/m) * X.T.dot(predictions - y)
    return gradient

# å­¦ç¿’å±¥æ­´
history_batch = []
history_sgd = []
history_minibatch = []

# ãƒãƒƒãƒå‹¾é…é™ä¸‹æ³•
for epoch in range(n_epochs):
    gradient = compute_gradient(X_b, y, theta_batch)
    theta_batch -= learning_rate * gradient
    history_batch.append(compute_loss(X_b, y, theta_batch))

# ç¢ºç‡çš„å‹¾é…é™ä¸‹æ³•
for epoch in range(n_epochs):
    for i in range(n_samples):
        random_index = np.random.randint(n_samples)
        xi = X_b[random_index:random_index+1]
        yi = y[random_index:random_index+1]
        gradient = compute_gradient(xi, yi, theta_sgd)
        theta_sgd -= learning_rate * gradient
    history_sgd.append(compute_loss(X_b, y, theta_sgd))

# ãƒŸãƒ‹ãƒãƒƒãƒå‹¾é…é™ä¸‹æ³•
for epoch in range(n_epochs):
    shuffled_indices = np.random.permutation(n_samples)
    X_shuffled = X_b[shuffled_indices]
    y_shuffled = y[shuffled_indices]

    for i in range(0, n_samples, batch_size):
        xi = X_shuffled[i:i+batch_size]
        yi = y_shuffled[i:i+batch_size]
        gradient = compute_gradient(xi, yi, theta_minibatch)
        theta_minibatch -= learning_rate * gradient
    history_minibatch.append(compute_loss(X_b, y, theta_minibatch))

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# å­¦ç¿’æ›²ç·š
axes[0].plot(history_batch, label='ãƒãƒƒãƒGD', linewidth=2)
axes[0].plot(history_sgd, label='SGD', alpha=0.7, linewidth=2)
axes[0].plot(history_minibatch, label='ãƒŸãƒ‹ãƒãƒƒãƒGD', linewidth=2)
axes[0].set_xlabel('ã‚¨ãƒãƒƒã‚¯', fontsize=12)
axes[0].set_ylabel('æå¤±ï¼ˆMSEï¼‰', fontsize=12)
axes[0].set_title('å­¦ç¿’æ›²ç·šã®æ¯”è¼ƒ', fontsize=14)
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)

# å›å¸°ç›´ç·š
axes[1].scatter(X, y, alpha=0.5, label='ãƒ‡ãƒ¼ã‚¿')
x_plot = np.array([[0], [2]])
x_plot_b = np.c_[np.ones((2, 1)), x_plot]

axes[1].plot(x_plot, x_plot_b.dot(theta_batch), 'r-',
             linewidth=2, label=f'ãƒãƒƒãƒGD')
axes[1].plot(x_plot, x_plot_b.dot(theta_sgd), 'g--',
             linewidth=2, label=f'SGD')
axes[1].plot(x_plot, x_plot_b.dot(theta_minibatch), 'b:',
             linewidth=2, label=f'ãƒŸãƒ‹ãƒãƒƒãƒGD')
axes[1].set_xlabel('X', fontsize=12)
axes[1].set_ylabel('y', fontsize=12)
axes[1].set_title('å­¦ç¿’ã•ã‚ŒãŸå›å¸°ç›´ç·š', fontsize=14)
axes[1].legend(fontsize=11)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== æœ€çµ‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===")
print(f"ãƒãƒƒãƒGD:      Î¸0={theta_batch[0][0]:.3f}, Î¸1={theta_batch[1][0]:.3f}")
print(f"SGD:           Î¸0={theta_sgd[0][0]:.3f}, Î¸1={theta_sgd[1][0]:.3f}")
print(f"ãƒŸãƒ‹ãƒãƒƒãƒGD:  Î¸0={theta_minibatch[0][0]:.3f}, Î¸1={theta_minibatch[1][0]:.3f}")
print(f"\nçœŸã®å€¤:        Î¸0=4.000, Î¸1=3.000")
</code></pre>

<h3>é«˜åº¦ãªæœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h3>

<h4>Momentum</h4>

<p>å‹¾é…ã®ç§»å‹•å¹³å‡ã‚’ä½¿ã„ã€æŒ¯å‹•ã‚’æŠ‘åˆ¶ã—ã¾ã™ã€‚</p>

<p>$$
\begin{aligned}
\mathbf{v}_{t+1} &= \beta \mathbf{v}_t - \alpha \nabla f(\mathbf{x}_t) \\
\mathbf{x}_{t+1} &= \mathbf{x}_t + \mathbf{v}_{t+1}
\end{aligned}
$$</p>

<h4>Adamï¼ˆAdaptive Moment Estimationï¼‰</h4>

<p>å‹¾é…ã®1æ¬¡ã¨2æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã‚’é©å¿œçš„ã«èª¿æ•´ã—ã¾ã™ã€‚</p>

<p>$$
\begin{aligned}
\mathbf{m}_t &= \beta_1 \mathbf{m}_{t-1} + (1-\beta_1) \nabla f(\mathbf{x}_t) \\
\mathbf{v}_t &= \beta_2 \mathbf{v}_{t-1} + (1-\beta_2) (\nabla f(\mathbf{x}_t))^2 \\
\hat{\mathbf{m}}_t &= \frac{\mathbf{m}_t}{1-\beta_1^t} \\
\hat{\mathbf{v}}_t &= \frac{\mathbf{v}_t}{1-\beta_2^t} \\
\mathbf{x}_{t+1} &= \mathbf{x}_t - \alpha \frac{\hat{\mathbf{m}}_t}{\sqrt{\hat{\mathbf{v}}_t} + \epsilon}
\end{aligned}
$$</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Rosenbrocké–¢æ•°ï¼ˆæœ€é©åŒ–ã®é›£ã—ã„éå‡¸é–¢æ•°ï¼‰
def rosenbrock(x, y):
    return (1 - x)**2 + 100 * (y - x**2)**2

def rosenbrock_grad(x, y):
    dx = -2 * (1 - x) - 400 * x * (y - x**2)
    dy = 200 * (y - x**2)
    return np.array([dx, dy])

# æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè£…
def sgd(start, grad_func, lr=0.001, n_iterations=1000):
    x = start.copy()
    trajectory = [x.copy()]

    for _ in range(n_iterations):
        grad = grad_func(x[0], x[1])
        x -= lr * grad
        trajectory.append(x.copy())

    return np.array(trajectory)

def momentum(start, grad_func, lr=0.001, beta=0.9, n_iterations=1000):
    x = start.copy()
    v = np.zeros_like(x)
    trajectory = [x.copy()]

    for _ in range(n_iterations):
        grad = grad_func(x[0], x[1])
        v = beta * v - lr * grad
        x += v
        trajectory.append(x.copy())

    return np.array(trajectory)

def adam(start, grad_func, lr=0.01, beta1=0.9, beta2=0.999,
         epsilon=1e-8, n_iterations=1000):
    x = start.copy()
    m = np.zeros_like(x)
    v = np.zeros_like(x)
    trajectory = [x.copy()]

    for t in range(1, n_iterations + 1):
        grad = grad_func(x[0], x[1])

        m = beta1 * m + (1 - beta1) * grad
        v = beta2 * v + (1 - beta2) * (grad ** 2)

        m_hat = m / (1 - beta1 ** t)
        v_hat = v / (1 - beta2 ** t)

        x -= lr * m_hat / (np.sqrt(v_hat) + epsilon)
        trajectory.append(x.copy())

    return np.array(trajectory)

# æœ€é©åŒ–ã®å®Ÿè¡Œ
start_point = np.array([-1.0, 1.0])
n_iter = 500

traj_sgd = sgd(start_point, rosenbrock_grad, lr=0.0005, n_iterations=n_iter)
traj_momentum = momentum(start_point, rosenbrock_grad, lr=0.0005, n_iterations=n_iter)
traj_adam = adam(start_point, rosenbrock_grad, lr=0.01, n_iterations=n_iter)

# ç­‰é«˜ç·šãƒ—ãƒ­ãƒƒãƒˆ
x = np.linspace(-1.5, 1.5, 100)
y = np.linspace(-0.5, 2.5, 100)
X, Y = np.meshgrid(x, y)
Z = rosenbrock(X, Y)

plt.figure(figsize=(15, 5))

# SGD
plt.subplot(131)
plt.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='viridis', alpha=0.6)
plt.plot(traj_sgd[:, 0], traj_sgd[:, 1], 'r.-', markersize=3,
         linewidth=1, alpha=0.7, label='SGD')
plt.plot(1, 1, 'g*', markersize=20, label='æœ€é©è§£')
plt.xlabel('x')
plt.ylabel('y')
plt.title('SGD', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

# Momentum
plt.subplot(132)
plt.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='viridis', alpha=0.6)
plt.plot(traj_momentum[:, 0], traj_momentum[:, 1], 'b.-', markersize=3,
         linewidth=1, alpha=0.7, label='Momentum')
plt.plot(1, 1, 'g*', markersize=20, label='æœ€é©è§£')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Momentum', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

# Adam
plt.subplot(133)
plt.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='viridis', alpha=0.6)
plt.plot(traj_adam[:, 0], traj_adam[:, 1], 'm.-', markersize=3,
         linewidth=1, alpha=0.7, label='Adam')
plt.plot(1, 1, 'g*', markersize=20, label='æœ€é©è§£')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Adam', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¯”è¼ƒ ===")
print(f"SGDæœ€çµ‚ä½ç½®:      ({traj_sgd[-1][0]:.4f}, {traj_sgd[-1][1]:.4f})")
print(f"Momentumæœ€çµ‚ä½ç½®: ({traj_momentum[-1][0]:.4f}, {traj_momentum[-1][1]:.4f})")
print(f"Adamæœ€çµ‚ä½ç½®:     ({traj_adam[-1][0]:.4f}, {traj_adam[-1][1]:.4f})")
print(f"çœŸã®æœ€é©è§£:       (1.0000, 1.0000)")
</code></pre>

<h3>æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </th>
<th>é•·æ‰€</th>
<th>çŸ­æ‰€</th>
<th>æ¨å¥¨ç”¨é€”</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SGD</strong></td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡</td>
<td>åæŸãŒé…ã„ã€æŒ¯å‹•</td>
<td>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿</td>
</tr>
<tr>
<td><strong>Momentum</strong></td>
<td>æŒ¯å‹•æŠ‘åˆ¶ã€é«˜é€ŸåæŸ</td>
<td>æ…£æ€§ã§è¡Œãéã</td>
<td>è°·é–“ã®é–¢æ•°</td>
</tr>
<tr>
<td><strong>AdaGrad</strong></td>
<td>ç‰¹å¾´ã”ã¨ã«å­¦ç¿’ç‡èª¿æ•´</td>
<td>å­¦ç¿’ç‡ãŒæ€¥æ¸›</td>
<td>ç–ãªãƒ‡ãƒ¼ã‚¿</td>
</tr>
<tr>
<td><strong>RMSprop</strong></td>
<td>å­¦ç¿’ç‡æ¸›è¡°ç·©å’Œ</td>
<td>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´å¿…è¦</td>
<td>RNN</td>
</tr>
<tr>
<td><strong>Adam</strong></td>
<td>é©å¿œçš„ã€é«˜æ€§èƒ½</td>
<td>éå­¦ç¿’ãƒªã‚¹ã‚¯</td>
<td>æ±ç”¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.3 åˆ¶ç´„ä»˜ãæœ€é©åŒ–</h2>

<h3>ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ä¹—æ•°æ³•</h3>

<p><strong>ç­‰å¼åˆ¶ç´„ä»˜ãæœ€é©åŒ–</strong>ï¼š</p>

<p>$$
\begin{aligned}
\min_{\mathbf{x}} \quad & f(\mathbf{x}) \\
\text{subject to} \quad & h(\mathbf{x}) = 0
\end{aligned}
$$</p>

<p><strong>ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥é–¢æ•°</strong>ï¼š</p>

<p>$$
\mathcal{L}(\mathbf{x}, \lambda) = f(\mathbf{x}) + \lambda h(\mathbf{x})
$$</p>

<p><strong>æœ€é©æ€§æ¡ä»¶</strong>ï¼š</p>

<p>$$
\begin{aligned}
\nabla_{\mathbf{x}} \mathcal{L} &= 0 \\
\nabla_{\lambda} \mathcal{L} &= 0
\end{aligned}
$$</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize

# ç›®çš„é–¢æ•°ï¼šf(x, y) = (x - 2)^2 + (y - 1)^2
def objective(x):
    return (x[0] - 2)**2 + (x[1] - 1)**2

# ç­‰å¼åˆ¶ç´„ï¼šh(x, y) = x + y - 2 = 0
def constraint_eq(x):
    return x[0] + x[1] - 2

# åˆ¶ç´„ã‚’è¾æ›¸å½¢å¼ã§å®šç¾©
constraints = {'type': 'eq', 'fun': constraint_eq}

# åˆæœŸç‚¹
x0 = np.array([0.0, 0.0])

# æœ€é©åŒ–
result = minimize(objective, x0, method='SLSQP', constraints=constraints)

# å¯è¦–åŒ–
x_range = np.linspace(-1, 4, 100)
y_range = np.linspace(-1, 4, 100)
X, Y = np.meshgrid(x_range, y_range)
Z = (X - 2)**2 + (Y - 1)**2

plt.figure(figsize=(10, 8))
plt.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.6)
plt.colorbar(label='$f(x, y)$')

# åˆ¶ç´„ç·š
x_constraint = np.linspace(-1, 4, 100)
y_constraint = 2 - x_constraint
plt.plot(x_constraint, y_constraint, 'r-', linewidth=3, label='åˆ¶ç´„: $x + y = 2$')

# æœ€é©ç‚¹
plt.plot(result.x[0], result.x[1], 'r*', markersize=20, label='æœ€é©è§£')

# åˆ¶ç´„ãªã—æœ€é©ç‚¹
plt.plot(2, 1, 'g*', markersize=20, label='åˆ¶ç´„ãªã—æœ€é©è§£')

plt.xlabel('x', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ä¹—æ•°æ³•ã«ã‚ˆã‚‹ç­‰å¼åˆ¶ç´„ä»˜ãæœ€é©åŒ–', fontsize=14)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.axis('equal')
plt.tight_layout()
plt.show()

print("=== ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ä¹—æ•°æ³•ã®çµæœ ===")
print(f"æœ€é©è§£: x = {result.x[0]:.4f}, y = {result.x[1]:.4f}")
print(f"ç›®çš„é–¢æ•°å€¤: f(x*) = {result.fun:.4f}")
print(f"åˆ¶ç´„å……è¶³: h(x*) = {constraint_eq(result.x):.6f}")
print(f"åˆ¶ç´„ãªã—æœ€é©è§£: x = 2.0, y = 1.0, f = 0.0")
</code></pre>

<h3>KKTæ¡ä»¶</h3>

<p><strong>ä¸ç­‰å¼åˆ¶ç´„ä»˜ãæœ€é©åŒ–</strong>ã®ãŸã‚ã®å¿…è¦æ¡ä»¶</p>

<p>$$
\begin{aligned}
\min_{\mathbf{x}} \quad & f(\mathbf{x}) \\
\text{subject to} \quad & g_i(\mathbf{x}) \leq 0
\end{aligned}
$$</p>

<p><strong>KKTæ¡ä»¶ï¼ˆKarush-Kuhn-Tucker Conditionsï¼‰</strong>ï¼š</p>

<ol>
<li><strong>å®šå¸¸æ€§</strong>: $\nabla f(\mathbf{x}^*) + \sum_i \mu_i \nabla g_i(\mathbf{x}^*) = 0$</li>
<li><strong>åŸåˆå®Ÿè¡Œå¯èƒ½æ€§</strong>: $g_i(\mathbf{x}^*) \leq 0$</li>
<li><strong>åŒå¯¾å®Ÿè¡Œå¯èƒ½æ€§</strong>: $\mu_i \geq 0$</li>
<li><strong>ç›¸è£œæ€§</strong>: $\mu_i g_i(\mathbf{x}^*) = 0$</li>
</ol>

<h3>SVMã¸ã®å¿œç”¨</h3>

<p><strong>ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³ï¼ˆSVMï¼‰</strong>ã¯åˆ¶ç´„ä»˜ãæœ€é©åŒ–å•é¡Œã¨ã—ã¦å®šå¼åŒ–ã•ã‚Œã¾ã™ã€‚</p>

<p>$$
\begin{aligned}
\min_{\mathbf{w}, b} \quad & \frac{1}{2} \|\mathbf{w}\|^2 \\
\text{subject to} \quad & y_i(\mathbf{w}^T \mathbf{x}_i + b) \geq 1, \quad i = 1, \ldots, n
\end{aligned}
$$</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.datasets import make_blobs

# ç·šå½¢åˆ†é›¢å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
X, y = make_blobs(n_samples=100, centers=2, n_features=2,
                  cluster_std=1.0, center_box=(-5, 5))

# SVMãƒ¢ãƒ‡ãƒ«ï¼ˆç·šå½¢ã‚«ãƒ¼ãƒãƒ«ï¼‰
svm = SVC(kernel='linear', C=1000)  # å¤§ããªCã§ãƒãƒ¼ãƒ‰ãƒãƒ¼ã‚¸ãƒ³è¿‘ä¼¼
svm.fit(X, y)

# æ±ºå®šå¢ƒç•Œã®å¯è¦–åŒ–
def plot_svm_decision_boundary(ax, X, y, model):
    # ã‚°ãƒªãƒƒãƒ‰ä½œæˆ
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                         np.linspace(y_min, y_max, 100))

    # æ±ºå®šå¢ƒç•Œ
    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    # ãƒ—ãƒ­ãƒƒãƒˆ
    ax.contour(xx, yy, Z, levels=[-1, 0, 1],
               linestyles=['--', '-', '--'], colors=['r', 'k', 'b'], linewidths=2)
    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm',
               s=50, edgecolors='k', alpha=0.7)

    # ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼
    ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],
               s=200, facecolors='none', edgecolors='g', linewidths=2,
               label='ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼')

    ax.set_xlabel('$x_1$', fontsize=12)
    ax.set_ylabel('$x_2$', fontsize=12)
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.figure(figsize=(10, 8))
ax = plt.gca()
plot_svm_decision_boundary(ax, X, y, svm)
plt.title('SVMã«ã‚ˆã‚‹ç·šå½¢åˆ†é›¢ï¼ˆKKTæ¡ä»¶ã«ã‚ˆã‚‹æœ€é©åŒ–ï¼‰', fontsize=14)
plt.tight_layout()
plt.show()

print("=== SVMæœ€é©åŒ–ã®çµæœ ===")
print(f"é‡ã¿ãƒ™ã‚¯ãƒˆãƒ« w: {svm.coef_[0]}")
print(f"ãƒã‚¤ã‚¢ã‚¹ b: {svm.intercept_[0]:.4f}")
print(f"ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼æ•°: {len(svm.support_vectors_)}")
print(f"ãƒãƒ¼ã‚¸ãƒ³: {2 / np.linalg.norm(svm.coef_):.4f}")
</code></pre>

<hr>

<h2>3.4 å‡¸æœ€é©åŒ–</h2>

<h3>å‡¸æœ€é©åŒ–ã®æ€§è³ª</h3>

<blockquote>
<p><strong>é‡è¦ãªæ€§è³ª</strong>ï¼šå‡¸æœ€é©åŒ–å•é¡Œã§ã¯ã€å±€æ‰€æœ€é©è§£ = å¤§åŸŸæœ€é©è§£</p>
</blockquote>

<p>ã“ã‚Œã«ã‚ˆã‚Šã€åŠ¹ç‡çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ç¢ºå®Ÿã«æœ€é©è§£ã‚’ç™ºè¦‹ã§ãã¾ã™ã€‚</p>

<h3>ç·šå½¢è¨ˆç”»æ³•ï¼ˆLinear Programmingï¼‰</h3>

<p>$$
\begin{aligned}
\min_{\mathbf{x}} \quad & \mathbf{c}^T \mathbf{x} \\
\text{subject to} \quad & \mathbf{A} \mathbf{x} \leq \mathbf{b} \\
& \mathbf{x} \geq 0
\end{aligned}
$$</p>

<pre><code class="language-python">import numpy as np
from scipy.optimize import linprog
import matplotlib.pyplot as plt

# ç·šå½¢è¨ˆç”»å•é¡Œ
# ç›®çš„é–¢æ•°ï¼šæœ€å°åŒ– -x - 2y ï¼ˆ= æœ€å¤§åŒ– x + 2yï¼‰
c = [-1, -2]

# ä¸ç­‰å¼åˆ¶ç´„ï¼šA_ub * x <= b_ub
# åˆ¶ç´„1: x + y <= 4
# åˆ¶ç´„2: 2x + y <= 5
# åˆ¶ç´„3: x >= 0, y >= 0ï¼ˆbounds ã§æŒ‡å®šï¼‰
A_ub = np.array([[1, 1],
                 [2, 1]])
b_ub = np.array([4, 5])

# å¤‰æ•°ã®ç¯„å›²
bounds = [(0, None), (0, None)]

# æœ€é©åŒ–
result = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')

# å¯è¦–åŒ–
x = np.linspace(0, 5, 100)

plt.figure(figsize=(10, 8))

# åˆ¶ç´„ã®å¯è¦–åŒ–
y1 = 4 - x
y2 = 5 - 2*x

plt.plot(x, y1, 'r-', linewidth=2, label='$x + y \leq 4$')
plt.plot(x, y2, 'b-', linewidth=2, label='$2x + y \leq 5$')
plt.axhline(y=0, color='k', linewidth=0.5)
plt.axvline(x=0, color='k', linewidth=0.5)

# å®Ÿè¡Œå¯èƒ½é ˜åŸŸ
x_fill = np.linspace(0, 2.5, 100)
y_upper = np.minimum(4 - x_fill, 5 - 2*x_fill)
y_upper = np.maximum(y_upper, 0)

plt.fill_between(x_fill, 0, y_upper, alpha=0.3, color='green',
                 label='å®Ÿè¡Œå¯èƒ½é ˜åŸŸ')

# ç›®çš„é–¢æ•°ã®ç­‰é«˜ç·š
X, Y = np.meshgrid(np.linspace(0, 5, 100), np.linspace(0, 5, 100))
Z = X + 2*Y
plt.contour(X, Y, Z, levels=10, alpha=0.3, cmap='viridis')

# æœ€é©è§£
plt.plot(result.x[0], result.x[1], 'r*', markersize=20, label='æœ€é©è§£')

plt.xlabel('x', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('ç·šå½¢è¨ˆç”»æ³•', fontsize=14)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.xlim(0, 5)
plt.ylim(0, 5)
plt.tight_layout()
plt.show()

print("=== ç·šå½¢è¨ˆç”»æ³•ã®çµæœ ===")
print(f"æœ€é©è§£: x = {result.x[0]:.4f}, y = {result.x[1]:.4f}")
print(f"ç›®çš„é–¢æ•°å€¤ï¼ˆæœ€å¤§åŒ–ï¼‰: {-result.fun:.4f}")
print(f"æœ€é©åŒ–æˆåŠŸ: {result.success}")
</code></pre>

<h3>äºŒæ¬¡è¨ˆç”»æ³•ï¼ˆQuadratic Programmingï¼‰</h3>

<p>$$
\begin{aligned}
\min_{\mathbf{x}} \quad & \frac{1}{2} \mathbf{x}^T \mathbf{Q} \mathbf{x} + \mathbf{c}^T \mathbf{x} \\
\text{subject to} \quad & \mathbf{A} \mathbf{x} \leq \mathbf{b}
\end{aligned}
$$</p>

<h3>CVXPYã«ã‚ˆã‚‹å‡¸æœ€é©åŒ–</h3>

<pre><code class="language-python">import cvxpy as cp
import numpy as np
import matplotlib.pyplot as plt

# ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæœ€é©åŒ–å•é¡Œ
# ç›®çš„ï¼šãƒªã‚¹ã‚¯ã‚’æœ€å°åŒ–ã—ã¤ã¤ã€æœŸå¾…ãƒªã‚¿ãƒ¼ãƒ³ã‚’ç›®æ¨™å€¤ä»¥ä¸Šã«ã™ã‚‹

# è³‡ç”£ã®æœŸå¾…ãƒªã‚¿ãƒ¼ãƒ³ã¨å…±åˆ†æ•£è¡Œåˆ—ï¼ˆã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼‰
np.random.seed(42)
n_assets = 5
expected_returns = np.random.uniform(0.05, 0.15, n_assets)
cov_matrix = np.random.randn(n_assets, n_assets)
cov_matrix = cov_matrix @ cov_matrix.T / 100  # æ­£å®šå€¤è¡Œåˆ—

# å¤‰æ•°ï¼šè³‡ç”£é…åˆ†
w = cp.Variable(n_assets)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼šç›®æ¨™ãƒªã‚¿ãƒ¼ãƒ³
target_return = 0.10

# ç›®çš„é–¢æ•°ï¼šãƒªã‚¹ã‚¯ï¼ˆåˆ†æ•£ï¼‰ã®æœ€å°åŒ–
risk = cp.quad_form(w, cov_matrix)

# åˆ¶ç´„
constraints = [
    cp.sum(w) == 1,           # è³‡ç”£é…åˆ†ã®åˆè¨ˆ = 1
    w >= 0,                    # ã‚·ãƒ§ãƒ¼ãƒˆç¦æ­¢
    expected_returns @ w >= target_return  # ç›®æ¨™ãƒªã‚¿ãƒ¼ãƒ³é”æˆ
]

# å•é¡Œã®å®šç¾©ã¨è§£æ±º
problem = cp.Problem(cp.Minimize(risk), constraints)
problem.solve()

# åŠ¹ç‡çš„ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã®è¨ˆç®—
target_returns = np.linspace(0.06, 0.14, 20)
risks = []
portfolios = []

for target in target_returns:
    constraints = [
        cp.sum(w) == 1,
        w >= 0,
        expected_returns @ w >= target
    ]
    problem = cp.Problem(cp.Minimize(risk), constraints)
    problem.solve()

    if problem.status == 'optimal':
        risks.append(np.sqrt(problem.value))
        portfolios.append(w.value)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# åŠ¹ç‡çš„ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢
axes[0].plot(risks, target_returns, 'b-', linewidth=2, label='åŠ¹ç‡çš„ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢')
axes[0].plot(np.sqrt(problem.value), target_return, 'r*',
             markersize=15, label=f'é¸æŠãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ª (ãƒªã‚¿ãƒ¼ãƒ³={target_return})')
axes[0].set_xlabel('ãƒªã‚¹ã‚¯ï¼ˆæ¨™æº–åå·®ï¼‰', fontsize=12)
axes[0].set_ylabel('æœŸå¾…ãƒªã‚¿ãƒ¼ãƒ³', fontsize=12)
axes[0].set_title('åŠ¹ç‡çš„ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢', fontsize=14)
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)

# è³‡ç”£é…åˆ†
if problem.status == 'optimal':
    axes[1].bar(range(n_assets), w.value, alpha=0.7, edgecolor='black')
    axes[1].set_xlabel('è³‡ç”£', fontsize=12)
    axes[1].set_ylabel('é…åˆ†æ¯”ç‡', fontsize=12)
    axes[1].set_title('æœ€é©è³‡ç”£é…åˆ†', fontsize=14)
    axes[1].set_xticks(range(n_assets))
    axes[1].set_xticklabels([f'è³‡ç”£{i+1}' for i in range(n_assets)])
    axes[1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print("=== ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæœ€é©åŒ–ã®çµæœ ===")
print(f"ç›®æ¨™ãƒªã‚¿ãƒ¼ãƒ³: {target_return:.2%}")
print(f"é”æˆãƒªã‚¿ãƒ¼ãƒ³: {(expected_returns @ w.value):.2%}")
print(f"ãƒªã‚¹ã‚¯ï¼ˆæ¨™æº–åå·®ï¼‰: {np.sqrt(problem.value):.2%}")
print(f"\næœ€é©è³‡ç”£é…åˆ†:")
for i, weight in enumerate(w.value):
    print(f"  è³‡ç”£{i+1}: {weight:.2%}")
</code></pre>

<blockquote>
<p><strong>æ³¨æ„</strong>ï¼šCVXPYã‚’ä½¿ã†ã«ã¯ã€<code>pip install cvxpy</code> ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™ã€‚</p>
</blockquote>

<hr>

<h2>3.5 å®Ÿè·µï¼šæ©Ÿæ¢°å­¦ç¿’ã¸ã®å¿œç”¨</h2>

<h3>ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®æœ€é©åŒ–</h3>

<p>ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã¯å‡¸æœ€é©åŒ–å•é¡Œã¨ã—ã¦è§£ã‘ã¾ã™ã€‚</p>

<p>$$
\min_{\mathbf{w}} \sum_{i=1}^n \log(1 + \exp(-y_i \mathbf{w}^T \mathbf{x}_i)) + \lambda \|\mathbf{w}\|^2
$$</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,
                          n_redundant=0, n_clusters_per_class=1,
                          random_state=42)
y = 2 * y - 1  # {0, 1} -> {-1, 1}

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# æ¨™æº–åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°
def sigmoid(z):
    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))

# æå¤±é–¢æ•°ï¼ˆäº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ + L2æ­£å‰‡åŒ–ï¼‰
def logistic_loss(w, X, y, lambda_reg=0.01):
    z = X @ w
    loss = np.mean(np.log(1 + np.exp(-y * z)))
    reg = lambda_reg * np.sum(w**2)
    return loss + reg

# å‹¾é…
def logistic_gradient(w, X, y, lambda_reg=0.01):
    z = X @ w
    grad = -X.T @ (y * sigmoid(-y * z)) / len(y)
    grad += 2 * lambda_reg * w
    return grad

# å‹¾é…é™ä¸‹æ³•ã§ã®æœ€é©åŒ–
def train_logistic_regression(X, y, lr=0.1, n_iterations=1000, lambda_reg=0.01):
    n_features = X.shape[1]
    w = np.zeros(n_features)

    losses = []

    for i in range(n_iterations):
        grad = logistic_gradient(w, X, y, lambda_reg)
        w -= lr * grad

        if i % 10 == 0:
            loss = logistic_loss(w, X, y, lambda_reg)
            losses.append(loss)

    return w, losses

# å­¦ç¿’
w_optimal, losses = train_logistic_regression(
    X_train_scaled, y_train, lr=0.5, n_iterations=1000, lambda_reg=0.01
)

# äºˆæ¸¬
def predict(X, w):
    z = X @ w
    return np.sign(z)

y_pred_train = predict(X_train_scaled, w_optimal)
y_pred_test = predict(X_test_scaled, w_optimal)

train_acc = np.mean(y_pred_train == y_train)
test_acc = np.mean(y_pred_test == y_test)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# å­¦ç¿’æ›²ç·š
axes[0].plot(losses, linewidth=2)
axes[0].set_xlabel('ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆÃ—10ï¼‰', fontsize=12)
axes[0].set_ylabel('æå¤±', fontsize=12)
axes[0].set_title('å­¦ç¿’æ›²ç·š', fontsize=14)
axes[0].grid(True, alpha=0.3)

# æ±ºå®šå¢ƒç•Œ
x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1
y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                     np.linspace(y_min, y_max, 100))

Z = predict(np.c_[xx.ravel(), yy.ravel()], w_optimal)
Z = Z.reshape(xx.shape)

axes[1].contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm', levels=1)
axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],
                c=y_train, cmap='coolwarm', edgecolors='k', s=50, alpha=0.7)
axes[1].set_xlabel('ç‰¹å¾´é‡1', fontsize=12)
axes[1].set_ylabel('ç‰¹å¾´é‡2', fontsize=12)
axes[1].set_title(f'æ±ºå®šå¢ƒç•Œï¼ˆç²¾åº¦: {train_acc:.2%}ï¼‰', fontsize=14)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®çµæœ ===")
print(f"è¨“ç·´ç²¾åº¦: {train_acc:.2%}")
print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_acc:.2%}")
print(f"æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {w_optimal}")
</code></pre>

<h3>æ­£å‰‡åŒ–é …ã®åŠ¹æœ</h3>

<p>æ­£å‰‡åŒ–ã¯éå­¦ç¿’ã‚’é˜²ãã¾ã™ã€‚</p>

<ul>
<li><strong>L1æ­£å‰‡åŒ–ï¼ˆLassoï¼‰</strong>ï¼š$\lambda \|\mathbf{w}\|_1$ â†’ ã‚¹ãƒ‘ãƒ¼ã‚¹è§£</li>
<li><strong>L2æ­£å‰‡åŒ–ï¼ˆRidgeï¼‰</strong>ï¼š$\lambda \|\mathbf{w}\|_2^2$ â†’ é‡ã¿æ¸›è¡°</li>
</ul>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge, Lasso
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
X, y = make_regression(n_samples=100, n_features=50, n_informative=10,
                       noise=10, random_state=42)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ç•°ãªã‚‹æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã®å­¦ç¿’
alphas = np.logspace(-3, 3, 50)

ridge_train_scores = []
ridge_test_scores = []
lasso_train_scores = []
lasso_test_scores = []

for alpha in alphas:
    # Ridge
    ridge = Ridge(alpha=alpha)
    ridge.fit(X_train_scaled, y_train)
    ridge_train_scores.append(ridge.score(X_train_scaled, y_train))
    ridge_test_scores.append(ridge.score(X_test_scaled, y_test))

    # Lasso
    lasso = Lasso(alpha=alpha, max_iter=10000)
    lasso.fit(X_train_scaled, y_train)
    lasso_train_scores.append(lasso.score(X_train_scaled, y_train))
    lasso_test_scores.append(lasso.score(X_test_scaled, y_test))

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Ridge
axes[0].semilogx(alphas, ridge_train_scores, 'b-', linewidth=2, label='è¨“ç·´')
axes[0].semilogx(alphas, ridge_test_scores, 'r-', linewidth=2, label='ãƒ†ã‚¹ãƒˆ')
axes[0].set_xlabel('æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ Î±', fontsize=12)
axes[0].set_ylabel('RÂ² ã‚¹ã‚³ã‚¢', fontsize=12)
axes[0].set_title('Ridgeå›å¸°ï¼ˆL2æ­£å‰‡åŒ–ï¼‰', fontsize=14)
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)

# Lasso
axes[1].semilogx(alphas, lasso_train_scores, 'b-', linewidth=2, label='è¨“ç·´')
axes[1].semilogx(alphas, lasso_test_scores, 'r-', linewidth=2, label='ãƒ†ã‚¹ãƒˆ')
axes[1].set_xlabel('æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ Î±', fontsize=12)
axes[1].set_ylabel('RÂ² ã‚¹ã‚³ã‚¢', fontsize=12)
axes[1].set_title('Lassoå›å¸°ï¼ˆL1æ­£å‰‡åŒ–ï¼‰', fontsize=14)
axes[1].legend(fontsize=11)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# æœ€é©ãªÎ±ã§ã®ä¿‚æ•°æ¯”è¼ƒ
ridge_best = Ridge(alpha=1.0)
ridge_best.fit(X_train_scaled, y_train)

lasso_best = Lasso(alpha=0.1, max_iter=10000)
lasso_best.fit(X_train_scaled, y_train)

print("=== æ­£å‰‡åŒ–ã®åŠ¹æœ ===")
print(f"Ridge - éã‚¼ãƒ­ä¿‚æ•°: {np.sum(np.abs(ridge_best.coef_) > 0.01)}/{len(ridge_best.coef_)}")
print(f"Lasso - éã‚¼ãƒ­ä¿‚æ•°: {np.sum(np.abs(lasso_best.coef_) > 0.01)}/{len(lasso_best.coef_)}")
print(f"\nRidgeæœ€è‰¯ã‚¹ã‚³ã‚¢: {ridge_best.score(X_test_scaled, y_test):.3f}")
print(f"Lassoæœ€è‰¯ã‚¹ã‚³ã‚¢: {lasso_best.score(X_test_scaled, y_test):.3f}")
</code></pre>

<hr>

<h2>3.6 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>æœ€é©åŒ–ã®åŸºç¤</strong></p>
<ul>
<li>å‡¸æ€§ã®é‡è¦æ€§ï¼šå‡¸æœ€é©åŒ–ã¯å¤§åŸŸæœ€é©è§£ãŒä¿è¨¼</li>
<li>å‹¾é…ã¨ãƒ˜ã‚·ã‚¢ãƒ³ã«ã‚ˆã‚‹æœ€é©æ€§ã®åˆ¤å®š</li>
</ul></li>

<li><p><strong>å‹¾é…é™ä¸‹æ³•</strong></p>
<ul>
<li>å­¦ç¿’ç‡ã®é¸æŠãŒåæŸé€Ÿåº¦ã¨å®‰å®šæ€§ã‚’æ±ºå®š</li>
<li>SGDã€Momentumã€Adamãªã©ã®ç™ºå±•æ‰‹æ³•</li>
<li>å®Ÿè£…ã¨åæŸæ€§ã®ç†è§£</li>
</ul></li>

<li><p><strong>åˆ¶ç´„ä»˜ãæœ€é©åŒ–</strong></p>
<ul>
<li>ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ä¹—æ•°æ³•ã«ã‚ˆã‚‹ç­‰å¼åˆ¶ç´„ã®æ‰±ã„</li>
<li>KKTæ¡ä»¶ã«ã‚ˆã‚‹ä¸ç­‰å¼åˆ¶ç´„ã®ç†è«–</li>
<li>SVMã¸ã®å¿œç”¨</li>
</ul></li>

<li><p><strong>å‡¸æœ€é©åŒ–</strong></p>
<ul>
<li>ç·šå½¢è¨ˆç”»æ³•ã¨äºŒæ¬¡è¨ˆç”»æ³•</li>
<li>CVXPYã«ã‚ˆã‚‹å®Ÿè£…</li>
<li>ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæœ€é©åŒ–ãªã©ã®å¿œç”¨</li>
</ul></li>

<li><p><strong>æ©Ÿæ¢°å­¦ç¿’ã¸ã®å¿œç”¨</strong></p>
<ul>
<li>ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®æœ€é©åŒ–</li>
<li>æ­£å‰‡åŒ–ã«ã‚ˆã‚‹éå­¦ç¿’é˜²æ­¢</li>
<li>å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®å®Ÿè£…</li>
</ul></li>
</ol>

<h3>æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é¸æŠæŒ‡é‡</h3>

<table>
<thead>
<tr>
<th>å•é¡Œã®æ€§è³ª</th>
<th>æ¨å¥¨æ‰‹æ³•</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td>å‡¸é–¢æ•°ã€å°è¦æ¨¡</td>
<td>å‹¾é…é™ä¸‹æ³•</td>
<td>ç¢ºå®Ÿã«æœ€é©è§£ç™ºè¦‹</td>
</tr>
<tr>
<td>å‡¸é–¢æ•°ã€å¤§è¦æ¨¡</td>
<td>SGDã€Adam</td>
<td>è¨ˆç®—åŠ¹ç‡</td>
</tr>
<tr>
<td>éå‡¸ã€æ·±å±¤å­¦ç¿’</td>
<td>Adamã€RMSprop</td>
<td>å±€æ‰€æœ€é©è§£å›é¿</td>
</tr>
<tr>
<td>ç­‰å¼åˆ¶ç´„ã‚ã‚Š</td>
<td>ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ä¹—æ•°æ³•</td>
<td>åˆ¶ç´„å……è¶³ä¿è¨¼</td>
</tr>
<tr>
<td>ä¸ç­‰å¼åˆ¶ç´„ã‚ã‚Š</td>
<td>KKTæ¡ä»¶ã€SLSQP</td>
<td>å®Ÿè¡Œå¯èƒ½è§£</td>
</tr>
<tr>
<td>ç·šå½¢ãƒ»å‡¸äºŒæ¬¡</td>
<td>å°‚ç”¨ã‚½ãƒ«ãƒãƒ¼ï¼ˆCVXPYï¼‰</td>
<td>é«˜é€Ÿãƒ»å®‰å®š</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ç« ã¸</h3>

<p>ç¬¬4ç« ã§ã¯ã€<strong>ç¢ºç‡ãƒ»çµ±è¨ˆã®åŸºç¤</strong>ã‚’å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>ç¢ºç‡åˆ†å¸ƒã¨æœŸå¾…å€¤</li>
<li>æœ€å°¤æ¨å®šã¨ãƒ™ã‚¤ã‚ºæ¨å®š</li>
<li>ä»®èª¬æ¤œå®šã¨ä¿¡é ¼åŒºé–“</li>
<li>æƒ…å ±ç†è«–ã®åŸºç¤</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>
<p>é–¢æ•° $f(x) = x^2 + 4x + 4$ ã‚’æœ€å°åŒ–ã™ã‚‹ $x$ ã‚’ã€å‹¾é…ã‚’ç”¨ã„ã¦æ±‚ã‚ã¦ãã ã•ã„ï¼ˆè§£æçš„ã«ï¼‰ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p>å‹¾é…ã‚’0ã«ã™ã‚‹ç‚¹ã‚’æ±‚ã‚ã¾ã™ã€‚</p>

<p>$$
\nabla f(x) = \frac{df}{dx} = 2x + 4
$$</p>

<p>æœ€é©æ€§æ¡ä»¶ï¼š</p>

<p>$$
2x + 4 = 0 \Rightarrow x^* = -2
$$</p>

<p>2æ¬¡æ¡ä»¶ï¼ˆååˆ†æ¡ä»¶ï¼‰ï¼š</p>

<p>$$
\frac{d^2 f}{dx^2} = 2 > 0
$$</p>

<p>ã‚ˆã£ã¦ã€$x^* = -2$ ã§æœ€å°å€¤ $f(x^*) = 0$ ã‚’å–ã‚Šã¾ã™ã€‚</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# é–¢æ•°ã¨å‹¾é…
def f(x):
    return x**2 + 4*x + 4

def grad_f(x):
    return 2*x + 4

# å¯è¦–åŒ–
x = np.linspace(-5, 2, 100)
y = f(x)

plt.figure(figsize=(10, 6))
plt.plot(x, y, 'b-', linewidth=2, label='$f(x) = x^2 + 4x + 4$')
plt.plot(-2, 0, 'r*', markersize=20, label='æœ€é©è§£: $x^* = -2$')
plt.axhline(y=0, color='k', linewidth=0.5, linestyle='--')
plt.axvline(x=-2, color='r', linewidth=0.5, linestyle='--')
plt.xlabel('x', fontsize=12)
plt.ylabel('f(x)', fontsize=12)
plt.title('æœ€é©åŒ–å•é¡Œã®è§£æè§£', fontsize=14)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.show()

print(f"æœ€é©è§£: x* = -2")
print(f"æœ€å°å€¤: f(x*) = {f(-2)}")
print(f"å‹¾é…: f'(x*) = {grad_f(-2)}")
</code></pre>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>å­¦ç¿’ç‡ $\alpha = 0.1$ ã§ã€åˆæœŸå€¤ $x_0 = 5$ ã‹ã‚‰å‹¾é…é™ä¸‹æ³•ã‚’ç”¨ã„ã¦ $f(x) = x^2$ ã‚’æœ€å°åŒ–ã—ã¦ãã ã•ã„ã€‚5å›ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã® $x$ ã®å€¤ã‚’å ±å‘Šã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import numpy as np

# ç›®çš„é–¢æ•°ã¨å‹¾é…
def f(x):
    return x**2

def grad_f(x):
    return 2*x

# å‹¾é…é™ä¸‹æ³•
x = 5.0
alpha = 0.1
n_iterations = 5

print("=== å‹¾é…é™ä¸‹æ³•ã®è»Œè·¡ ===")
print(f"ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ 0: x = {x:.6f}, f(x) = {f(x):.6f}")

for i in range(1, n_iterations + 1):
    grad = grad_f(x)
    x = x - alpha * grad
    print(f"ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ {i}: x = {x:.6f}, f(x) = {f(x):.6f}, grad = {grad:.6f}")

print(f"\næœ€çµ‚å€¤: x = {x:.6f}")
print(f"çœŸã®æœ€é©è§£: x* = 0")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== å‹¾é…é™ä¸‹æ³•ã®è»Œè·¡ ===
ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ 0: x = 5.000000, f(x) = 25.000000
ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ 1: x = 4.000000, f(x) = 16.000000, grad = 10.000000
ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ 2: x = 3.200000, f(x) = 10.240000, grad = 8.000000
ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ 3: x = 2.560000, f(x) = 6.553600, grad = 6.400000
ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ 4: x = 2.048000, f(x) = 4.194304, grad = 5.120000
ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ 5: x = 1.638400, f(x) = 2.684355, grad = 4.096000

æœ€çµ‚å€¤: x = 1.638400
çœŸã®æœ€é©è§£: x* = 0
</code></pre>

<p>5å›ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯å®Œå…¨ã«ã¯åæŸã—ã¦ã„ã¾ã›ã‚“ãŒã€æœ€é©è§£ã«è¿‘ã¥ã„ã¦ã„ã¾ã™ã€‚</p>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>ç­‰å¼åˆ¶ç´„ $x + y = 1$ ã®ä¸‹ã§ã€$f(x, y) = x^2 + y^2$ ã‚’æœ€å°åŒ–ã™ã‚‹å•é¡Œã‚’ã€ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ä¹—æ•°æ³•ã§è§£ã„ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p>ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥é–¢æ•°ï¼š</p>

<p>$$
\mathcal{L}(x, y, \lambda) = x^2 + y^2 + \lambda(x + y - 1)
$$</p>

<p>æœ€é©æ€§æ¡ä»¶ï¼š</p>

<p>$$
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial x} &= 2x + \lambda = 0 \\
\frac{\partial \mathcal{L}}{\partial y} &= 2y + \lambda = 0 \\
\frac{\partial \mathcal{L}}{\partial \lambda} &= x + y - 1 = 0
\end{aligned}
$$</p>

<p>1å¼ã¨2å¼ã‹ã‚‰ $x = y$ã€3å¼ã«ä»£å…¥ã—ã¦ $2x = 1 \Rightarrow x = y = 0.5$</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize

# ç›®çš„é–¢æ•°
def objective(vars):
    x, y = vars
    return x**2 + y**2

# ç­‰å¼åˆ¶ç´„
def constraint(vars):
    x, y = vars
    return x + y - 1

# åˆ¶ç´„ã®å®šç¾©
constraints = {'type': 'eq', 'fun': constraint}

# åˆæœŸç‚¹
x0 = [0.0, 0.0]

# æœ€é©åŒ–
result = minimize(objective, x0, method='SLSQP', constraints=constraints)

print("=== ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ä¹—æ•°æ³•ã®è§£ ===")
print(f"æœ€é©è§£: x = {result.x[0]:.4f}, y = {result.x[1]:.4f}")
print(f"ç›®çš„é–¢æ•°å€¤: f(x*, y*) = {result.fun:.4f}")
print(f"åˆ¶ç´„å……è¶³: x + y = {result.x[0] + result.x[1]:.6f}")

# å¯è¦–åŒ–
x = np.linspace(-0.5, 1.5, 100)
y = np.linspace(-0.5, 1.5, 100)
X, Y = np.meshgrid(x, y)
Z = X**2 + Y**2

plt.figure(figsize=(10, 8))
plt.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.6)
plt.colorbar(label='$f(x, y)$')

# åˆ¶ç´„ç·š
x_line = np.linspace(-0.5, 1.5, 100)
y_line = 1 - x_line
plt.plot(x_line, y_line, 'r-', linewidth=3, label='åˆ¶ç´„: $x + y = 1$')

# æœ€é©ç‚¹
plt.plot(result.x[0], result.x[1], 'r*', markersize=20, label='æœ€é©è§£')

plt.xlabel('x', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('ç­‰å¼åˆ¶ç´„ä»˜ãæœ€é©åŒ–', fontsize=14)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.axis('equal')
plt.tight_layout()
plt.show()
</code></pre>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>Momentumæ³•ï¼ˆ$\beta = 0.9$ï¼‰ã‚’ç”¨ã„ã¦ã€Rosenbrocké–¢æ•° $f(x, y) = (1-x)^2 + 100(y-x^2)^2$ ã‚’åˆæœŸç‚¹ $(-1, 1)$ ã‹ã‚‰æœ€é©åŒ–ã—ã¦ãã ã•ã„ã€‚å­¦ç¿’ç‡ $\alpha = 0.001$ ã§100å›ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã€è»Œè·¡ã‚’å¯è¦–åŒ–ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Rosenbrocké–¢æ•°
def rosenbrock(x, y):
    return (1 - x)**2 + 100 * (y - x**2)**2

def rosenbrock_grad(x, y):
    dx = -2 * (1 - x) - 400 * x * (y - x**2)
    dy = 200 * (y - x**2)
    return np.array([dx, dy])

# Momentumæ³•
def momentum(start, grad_func, lr=0.001, beta=0.9, n_iterations=100):
    pos = start.copy()
    velocity = np.zeros_like(pos)
    trajectory = [pos.copy()]

    for i in range(n_iterations):
        grad = grad_func(pos[0], pos[1])
        velocity = beta * velocity - lr * grad
        pos += velocity
        trajectory.append(pos.copy())

    return np.array(trajectory)

# æœ€é©åŒ–
start = np.array([-1.0, 1.0])
trajectory = momentum(start, rosenbrock_grad, lr=0.001, beta=0.9, n_iterations=100)

# å¯è¦–åŒ–
x = np.linspace(-1.5, 1.5, 100)
y = np.linspace(-0.5, 2.5, 100)
X, Y = np.meshgrid(x, y)
Z = rosenbrock(X, Y)

plt.figure(figsize=(12, 9))
plt.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='viridis', alpha=0.6)
plt.colorbar(label='$f(x, y)$')

plt.plot(trajectory[:, 0], trajectory[:, 1], 'r.-', markersize=4,
         linewidth=1.5, alpha=0.7, label='Momentumè»Œè·¡')
plt.plot(start[0], start[1], 'bo', markersize=10, label='é–‹å§‹ç‚¹')
plt.plot(1, 1, 'g*', markersize=20, label='æœ€é©è§£')
plt.plot(trajectory[-1, 0], trajectory[-1, 1], 'rs', markersize=10, label='çµ‚äº†ç‚¹')

plt.xlabel('x', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('Momentumæ³•ã«ã‚ˆã‚‹Rosenbrocké–¢æ•°ã®æœ€é©åŒ–', fontsize=14)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print("=== Momentumæ³•ã®çµæœ ===")
print(f"åˆæœŸç‚¹: ({start[0]:.2f}, {start[1]:.2f})")
print(f"æœ€çµ‚ç‚¹: ({trajectory[-1][0]:.4f}, {trajectory[-1][1]:.4f})")
print(f"æœ€é©è§£: (1.0000, 1.0000)")
print(f"æœ€çµ‚é–¢æ•°å€¤: {rosenbrock(trajectory[-1][0], trajectory[-1][1]):.6f}")
</code></pre>

</details>

<h3>å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>L1æ­£å‰‡åŒ–ã¨L2æ­£å‰‡åŒ–ã®é•ã„ã‚’èª¬æ˜ã—ã€ã©ã¡ã‚‰ãŒã‚¹ãƒ‘ãƒ¼ã‚¹è§£ï¼ˆå¤šãã®ä¿‚æ•°ãŒ0ï¼‰ã‚’ç”Ÿæˆã—ã‚„ã™ã„ã‹ã€ç†ç”±ã¨ã¨ã‚‚ã«è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>L2æ­£å‰‡åŒ–ï¼ˆRidgeï¼‰</strong>ï¼š</p>
<ul>
<li>ãƒšãƒŠãƒ«ãƒ†ã‚£é …: $\lambda \sum_i w_i^2$</li>
<li>ç‰¹å¾´: ä¿‚æ•°ã‚’å°ã•ãã™ã‚‹ãŒã€0ã«ã¯ã—ãªã„</li>
<li>å‹¾é…: $\nabla (\lambda w^2) = 2\lambda w$ ï¼ˆé€£ç¶šçš„ã«0ã«è¿‘ã¥ãï¼‰</li>
</ul>

<p><strong>L1æ­£å‰‡åŒ–ï¼ˆLassoï¼‰</strong>ï¼š</p>
<ul>
<li>ãƒšãƒŠãƒ«ãƒ†ã‚£é …: $\lambda \sum_i |w_i|$</li>
<li>ç‰¹å¾´: ä¿‚æ•°ã‚’æ­£ç¢ºã«0ã«ã™ã‚‹ï¼ˆã‚¹ãƒ‘ãƒ¼ã‚¹è§£ï¼‰</li>
<li>å‹¾é…: $\nabla (\lambda |w|) = \lambda \cdot \text{sign}(w)$ ï¼ˆ0ä»˜è¿‘ã§éé€£ç¶šï¼‰</li>
</ul>

<p><strong>ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã®ç†ç”±</strong>ï¼š</p>
<ol>
<li><strong>å¹¾ä½•å­¦çš„è§£é‡ˆ</strong>ï¼šL1åˆ¶ç´„ã®å®Ÿè¡Œå¯èƒ½é ˜åŸŸã¯è§’ã‚’æŒã¤ï¼ˆä¾‹ï¼šè±å½¢ï¼‰ã€‚ç›®çš„é–¢æ•°ã®ç­‰é«˜ç·šãŒè§’ã¨äº¤ã‚ã‚Šã‚„ã™ãã€ãã“ã§ã¯ä¸€éƒ¨ã®ä¿‚æ•°ãŒ0ã«ãªã‚‹ã€‚</li>
<li><strong>å‹¾é…ã®æ€§è³ª</strong>ï¼šL1ã®å‹¾é…ã¯ä¿‚æ•°ã®å¤§ãã•ã«ä¾å­˜ã›ãšä¸€å®šï¼ˆ$\pm \lambda$ï¼‰ã®ãŸã‚ã€å°ã•ã„ä¿‚æ•°ã‚‚å¤§ãã„ä¿‚æ•°ã‚‚åŒã˜ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’å—ã‘ã€å°ã•ã„ä¿‚æ•°ãŒ0ã«æŠ¼ã—è¾¼ã¾ã‚Œã‚„ã™ã„ã€‚</li>
</ol>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# L1ã¨L2ã®åˆ¶ç´„é ˜åŸŸã®å¯è¦–åŒ–
theta = np.linspace(0, 2*np.pi, 1000)

# L1åˆ¶ç´„ï¼ˆ|w1| + |w2| <= 1ï¼‰
w1_l1 = np.cos(theta) * (np.abs(np.cos(theta)) + np.abs(np.sin(theta)))
w2_l1 = np.sin(theta) * (np.abs(np.cos(theta)) + np.abs(np.sin(theta)))

# L2åˆ¶ç´„ï¼ˆw1^2 + w2^2 <= 1ï¼‰
w1_l2 = np.cos(theta)
w2_l2 = np.sin(theta)

# ç›®çš„é–¢æ•°ã®ç­‰é«˜ç·šï¼ˆä»®æƒ³çš„ãªä¾‹ï¼‰
w1 = np.linspace(-2, 2, 100)
w2 = np.linspace(-2, 2, 100)
W1, W2 = np.meshgrid(w1, w2)
Z = (W1 - 1.5)**2 + (W2 - 1.0)**2

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# L1æ­£å‰‡åŒ–
axes[0].contour(W1, W2, Z, levels=15, alpha=0.6, cmap='viridis')
axes[0].fill(w1_l1, w2_l1, alpha=0.3, color='red', label='L1åˆ¶ç´„é ˜åŸŸ')
axes[0].plot(0, 0, 'r*', markersize=20, label='ã‚¹ãƒ‘ãƒ¼ã‚¹è§£ï¼ˆw1=0ï¼‰')
axes[0].set_xlabel('$w_1$', fontsize=12)
axes[0].set_ylabel('$w_2$', fontsize=12)
axes[0].set_title('L1æ­£å‰‡åŒ–ï¼ˆLassoï¼‰', fontsize=14)
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)
axes[0].axis('equal')

# L2æ­£å‰‡åŒ–
axes[1].contour(W1, W2, Z, levels=15, alpha=0.6, cmap='viridis')
axes[1].fill(w1_l2, w2_l2, alpha=0.3, color='blue', label='L2åˆ¶ç´„é ˜åŸŸ')
circle_intersect_x = 0.8
circle_intersect_y = 0.6
axes[1].plot(circle_intersect_x, circle_intersect_y, 'b*',
             markersize=20, label='éã‚¹ãƒ‘ãƒ¼ã‚¹è§£')
axes[1].set_xlabel('$w_1$', fontsize=12)
axes[1].set_ylabel('$w_2$', fontsize=12)
axes[1].set_title('L2æ­£å‰‡åŒ–ï¼ˆRidgeï¼‰', fontsize=14)
axes[1].legend(fontsize=11)
axes[1].grid(True, alpha=0.3)
axes[1].axis('equal')

plt.tight_layout()
plt.show()

print("=== L1 vs L2æ­£å‰‡åŒ– ===")
print("L1ï¼ˆLassoï¼‰: ã‚¹ãƒ‘ãƒ¼ã‚¹è§£ã‚’ç”Ÿæˆï¼ˆå¤šãã®ä¿‚æ•°ãŒ0ï¼‰")
print("L2ï¼ˆRidgeï¼‰: å°ã•ã„ãŒéã‚¼ãƒ­ã®ä¿‚æ•°ã‚’ç”Ÿæˆ")
print("\nç”¨é€”:")
print("- L1: ç‰¹å¾´é¸æŠãŒå¿…è¦ãªå ´åˆ")
print("- L2: ã™ã¹ã¦ã®ç‰¹å¾´ã‚’ä½¿ã„ã¤ã¤éå­¦ç¿’ã‚’é˜²ãå ´åˆ")
</code></pre>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Boyd, S., & Vandenberghe, L. (2004). <em>Convex Optimization</em>. Cambridge University Press.</li>
<li>Nocedal, J., & Wright, S. (2006). <em>Numerical Optimization</em> (2nd ed.). Springer.</li>
<li>Goodfellow, I., Bengio, Y., & Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</li>
<li>Ruder, S. (2016). "An overview of gradient descent optimization algorithms". arXiv:1609.04747.</li>
</ol>

<div class="navigation">
    <a href="chapter2-linear-algebra.html" class="nav-button">â† å‰ã®ç« : ç·šå½¢ä»£æ•°ã®åŸºç¤</a>
    <a href="chapter4-information-theory.html" class="nav-button">æ¬¡ã®ç« : æƒ…å ±ç†è«– â†’</a>
</div>

    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimotoï¼ˆæ±åŒ—å¤§å­¦ï¼‰</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-25</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
