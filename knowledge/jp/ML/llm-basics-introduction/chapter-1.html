<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="第1章: LLMとは何か - 大規模言語モデルの定義、歴史、代表的モデルを学ぶ">
    <title>第1章: LLMとは何か - LLM基礎入門 - AI Terakoya</title>

    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header .container {
            padding: 0 1.5rem;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            margin-bottom: 1rem;
        }

        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }

        .meta span {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
        }

        /* Typography */
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            color: var(--primary-color);
        }

        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.6rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1.2rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--link-hover);
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1.2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Code blocks */
        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1.5rem 0;
            border: 1px solid var(--border-color);
        }

        code {
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.9rem;
        }

        p code, li code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-size: 0.85rem;
        }

        /* Info boxes */
        .info-box {
            background: #e3f2fd;
            border-left: 4px solid #2196F3;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }

        .info-box h4 {
            margin-top: 0;
            color: #1976D2;
        }

        .warning-box {
            background: #fff3e0;
            border-left: 4px solid #ff9800;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }

        .warning-box h4 {
            margin-top: 0;
            color: #f57c00;
        }

        .example-box {
            background: #f3e5f5;
            border-left: 4px solid #9c27b0;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }

        .example-box h4 {
            margin-top: 0;
            color: #7b1fa2;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.9rem;
        }

        th, td {
            padding: 0.8rem;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        th {
            background: var(--code-bg);
            font-weight: 600;
            color: var(--primary-color);
        }

        tr:nth-child(even) {
            background: #f9f9f9;
        }

        /* Mermaid diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
            background: white;
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        /* Navigation buttons */
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }

        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
            text-decoration: none;
        }

        /* Footer */
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }

        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "⚠️";
            position: absolute;
            left: 0;
        }

        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            h1 {
                font-size: 1.6rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            pre {
                padding: 1rem;
            }
        }
    </style>

    <!-- MathJax for equations -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>

    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({ startOnLoad: true, theme: 'default' });
            }
        });
    </script>
</head>
<body>
    <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI寺子屋トップ</a><span class="breadcrumb-separator">›</span><a href="../index.html">機械学習</a><span class="breadcrumb-separator">›</span><a href="./index.html">LLM基礎入門</a><span class="breadcrumb-separator">›</span><span class="breadcrumb-current">第1章</span>
        </div>
    </nav>

    <header>
        <div class="container">
            <h1>🤖 第1章：LLMとは何か</h1>
            <p class="subtitle">大規模言語モデルの定義、歴史、そして未来</p>
            <div class="meta">
                <span>📖 読了時間: 25-30分</span>
                <span>📊 難易度: 初級</span>
                <span>💻 コード例: 5個</span>
                <span>📝 演習: 3問</span>
            </div>
        </div>
    </header>

    <main class="container">
        <h2 id="intro">はじめに</h2>
        <p>2023年以降、<strong>ChatGPT</strong>の登場により、AI技術が一般社会に急速に浸透しました。ChatGPTの背後にある技術が<strong>大規模言語モデル（Large Language Model: LLM）</strong>です。</p>

        <p>この章では、LLMとは何か、どのような歴史を経て現在の形になったのか、そして代表的なモデルにはどのようなものがあるのかを学びます。</p>

        <h2 id="definition">1.1 LLMの定義</h2>

        <h3>大規模言語モデル（LLM）とは</h3>
        <p><strong>大規模言語モデル（Large Language Model: LLM）</strong>とは、膨大なテキストデータで訓練された、自然言語の理解と生成を行う深層学習モデルです。</p>

        <div class="info-box">
            <h4>📌 LLMの主な特徴</h4>
            <ul>
                <li><strong>大規模</strong>: 数十億〜数兆のパラメータを持つ</li>
                <li><strong>事前学習</strong>: インターネット上の大量テキストで訓練</li>
                <li><strong>汎用性</strong>: 様々なタスク（要約、翻訳、質問応答など）に対応</li>
                <li><strong>Few-Shot Learning</strong>: 少数の例から学習できる</li>
                <li><strong>コンテキスト理解</strong>: 長い文脈を考慮した応答</li>
            </ul>
        </div>

        <h3>LLMの基本構造</h3>
        <p>現代のLLMの多くは<strong>Transformer</strong>アーキテクチャをベースにしています。Transformerは2017年にGoogleが発表した革新的なニューラルネットワーク構造です。</p>

        <div class="mermaid">
graph TD
    A[入力テキスト] --> B[トークン化]
    B --> C[埋め込み層]
    C --> D[Transformer層 x N]
    D --> E[出力層]
    E --> F[予測テキスト]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
    style F fill:#e3f2fd
        </div>

        <h2 id="history">1.2 LLMの歴史</h2>

        <h3>言語モデルの進化</h3>
        <p>言語モデルは長い歴史を持ちますが、2018年以降に急速に発展しました。</p>

        <div class="mermaid">
timeline
    title LLMの進化
    2017 : Transformer登場（Vaswani et al.）
    2018 : BERT（Google）、GPT-1（OpenAI）
    2019 : GPT-2（OpenAI）、T5（Google）
    2020 : GPT-3（1750億パラメータ）
    2021 : Codex（GitHub Copilot）
    2022 : ChatGPT公開（GPT-3.5ベース）
    2023 : GPT-4、Claude、LLaMA、Gemini
    2024 : GPT-4 Turbo、Claude 3、LLaMA 3
        </div>

        <h3>主要なマイルストーン</h3>

        <h4>2017年：Transformer（トランスフォーマー）</h4>
        <p>Googleの論文 "Attention is All You Need" で提案された<strong>Transformer</strong>が、LLMの基礎となるアーキテクチャになりました。</p>
        <ul>
            <li><strong>革新点</strong>: Self-Attention機構により、文中の全単語間の関係を並列に計算</li>
            <li><strong>利点</strong>: 長距離依存関係の学習、並列処理による高速化</li>
        </ul>

        <h4>2018年：BERT（Bidirectional Encoder Representations from Transformers）</h4>
        <p>Googleが発表した<strong>双方向</strong>の言語モデル。文脈の前後両方を考慮できる点が画期的でした。</p>
        <ul>
            <li><strong>特徴</strong>: Masked Language Modeling（単語をマスクして予測）</li>
            <li><strong>用途</strong>: 文分類、固有表現認識、質問応答など</li>
        </ul>

        <h4>2018年：GPT-1（Generative Pre-trained Transformer）</h4>
        <p>OpenAIが発表した<strong>生成型</strong>言語モデル。事前学習+ファインチューニングのアプローチを確立しました。</p>
        <ul>
            <li><strong>パラメータ数</strong>: 1.17億</li>
            <li><strong>特徴</strong>: 次の単語を予測する自己回帰的生成</li>
        </ul>

        <h4>2020年：GPT-3</h4>
        <p>GPTシリーズの第3世代。パラメータ数の飛躍的増加により、Few-Shot Learningが可能になりました。</p>
        <ul>
            <li><strong>パラメータ数</strong>: 1750億（GPT-1の約1500倍）</li>
            <li><strong>革新点</strong>: 少数の例示だけで新しいタスクを実行可能</li>
        </ul>

        <h4>2022年：ChatGPT</h4>
        <p>GPT-3.5をベースに、人間のフィードバックで調整されたチャットボット。AI技術の大衆化のきっかけとなりました。</p>
        <ul>
            <li><strong>特徴</strong>: RLHF（Reinforcement Learning from Human Feedback）による調整</li>
            <li><strong>インパクト</strong>: 公開2ヶ月で1億ユーザー達成</li>
        </ul>

        <h4>2023年：GPT-4</h4>
        <p>OpenAIの最新モデル（執筆時点）。マルチモーダル（テキスト+画像）に対応しました。</p>
        <ul>
            <li><strong>改善点</strong>: より正確な推論、長文理解、創造性の向上</li>
            <li><strong>安全性</strong>: より堅牢な安全機能と倫理的配慮</li>
        </ul>

        <h2 id="models">1.3 代表的なLLMモデル</h2>

        <h3>主要なLLMの比較</h3>

        <table>
            <thead>
                <tr>
                    <th>モデル</th>
                    <th>開発元</th>
                    <th>パラメータ数</th>
                    <th>特徴</th>
                    <th>公開状況</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>GPT-4</strong></td>
                    <td>OpenAI</td>
                    <td>非公開（推定1兆+）</td>
                    <td>マルチモーダル、高精度</td>
                    <td>API経由</td>
                </tr>
                <tr>
                    <td><strong>Claude 3</strong></td>
                    <td>Anthropic</td>
                    <td>非公開</td>
                    <td>長文理解、安全性重視</td>
                    <td>API経由</td>
                </tr>
                <tr>
                    <td><strong>Gemini</strong></td>
                    <td>Google</td>
                    <td>非公開</td>
                    <td>マルチモーダル、統合型</td>
                    <td>API経由</td>
                </tr>
                <tr>
                    <td><strong>LLaMA 3</strong></td>
                    <td>Meta</td>
                    <td>8B, 70B, 405B</td>
                    <td>オープンソース、高効率</td>
                    <td>完全公開</td>
                </tr>
                <tr>
                    <td><strong>Mistral</strong></td>
                    <td>Mistral AI</td>
                    <td>7B, 8x7B</td>
                    <td>小型高性能、MoE</td>
                    <td>オープンソース</td>
                </tr>
            </tbody>
        </table>

        <div class="info-box">
            <h4>💡 パラメータ数の表記</h4>
            <ul>
                <li><strong>B</strong>: Billion（10億） - 例: 7B = 70億パラメータ</li>
                <li><strong>M</strong>: Million（100万） - 例: 340M = 3.4億パラメータ</li>
                <li>パラメータ数が多いほど高性能ですが、計算コストも増加します</li>
            </ul>
        </div>

        <h3>各モデルの詳細</h3>

        <h4>GPT-4（OpenAI）</h4>
        <ul>
            <li><strong>リリース</strong>: 2023年3月</li>
            <li><strong>強み</strong>: 複雑な推論、創造的タスク、マルチモーダル対応</li>
            <li><strong>弱み</strong>: 高コスト、API経由のみ、知識カットオフあり</li>
            <li><strong>用途</strong>: コード生成、文書作成、複雑な問題解決</li>
        </ul>

        <h4>Claude 3（Anthropic）</h4>
        <ul>
            <li><strong>リリース</strong>: 2024年3月</li>
            <li><strong>強み</strong>: 長文理解（200k+ トークン）、安全性、正確性</li>
            <li><strong>モデル種類</strong>: Opus（最高性能）、Sonnet（バランス型）、Haiku（高速）</li>
            <li><strong>用途</strong>: 長文分析、安全性が重要なアプリケーション</li>
        </ul>

        <h4>Gemini（Google）</h4>
        <ul>
            <li><strong>リリース</strong>: 2023年12月</li>
            <li><strong>強み</strong>: Googleサービス統合、マルチモーダル、高速</li>
            <li><strong>モデル種類</strong>: Ultra、Pro、Nano</li>
            <li><strong>用途</strong>: Google Workspaceとの連携、検索統合</li>
        </ul>

        <h4>LLaMA 3（Meta）</h4>
        <ul>
            <li><strong>リリース</strong>: 2024年4月</li>
            <li><strong>強み</strong>: オープンソース、商用利用可能、高効率</li>
            <li><strong>サイズ</strong>: 8B（小型）、70B（中型）、405B（大型）</li>
            <li><strong>用途</strong>: 自社環境での運用、カスタマイズ、研究</li>
        </ul>

        <h2 id="tokenization">1.4 トークン化の仕組み</h2>

        <h3>トークンとは</h3>
        <p>LLMは文字列をそのまま処理せず、<strong>トークン</strong>という単位に分割します。トークンは単語の一部、単語全体、句読点などになります。</p>

        <div class="example-box">
            <h4>🔍 トークン化の例</h4>
            <p><strong>入力テキスト</strong>: "ChatGPTは素晴らしいAIです"</p>
            <p><strong>トークン分割</strong>: ["Chat", "G", "PT", "は", "素晴らしい", "AI", "です"]</p>
            <p>→ 7トークン</p>
        </div>

        <h3>主なトークン化手法</h3>

        <h4>1. BPE (Byte Pair Encoding)</h4>
        <ul>
            <li>GPTシリーズで使用</li>
            <li>頻出する文字ペアを繰り返し結合</li>
            <li>未知語に強い（サブワード分割）</li>
        </ul>

        <h4>2. WordPiece</h4>
        <ul>
            <li>BERTで使用</li>
            <li>BPEの改良版</li>
            <li>尤度ベースで最適な分割を選択</li>
        </ul>

        <h4>3. SentencePiece</h4>
        <ul>
            <li>多言語対応</li>
            <li>言語に依存しないトークン化</li>
            <li>LLaMA、T5等で使用</li>
        </ul>

        <h3>トークン化のPythonコード例</h3>

        <pre><code class="language-python"># Hugging Face transformersを使ったトークン化
from transformers import AutoTokenizer

# GPT-2のトークナイザーを読み込み
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# テキストをトークン化
text = "ChatGPTは素晴らしいAIです"
tokens = tokenizer.tokenize(text)
print("トークン:", tokens)
# 出力例: ['Chat', 'G', 'PT', 'は', '素', '晴', 'らしい', 'AI', 'です']

# トークンIDに変換
token_ids = tokenizer.encode(text)
print("トークンID:", token_ids)

# トークン数を確認
print(f"トークン数: {len(token_ids)}")
</code></pre>

        <div class="warning-box">
            <h4>⚠️ トークン数の重要性</h4>
            <p>多くのLLM APIは<strong>トークン数</strong>で課金されます。また、モデルには最大トークン数（コンテキスト長）の制限があります。</p>
            <ul>
                <li><strong>GPT-3.5</strong>: 4,096トークン（約3,000語）</li>
                <li><strong>GPT-4</strong>: 8,192トークン、または32,768トークン</li>
                <li><strong>Claude 3</strong>: 200,000トークン（約15万語）</li>
            </ul>
        </div>

        <h2 id="architecture">1.5 Transformerアーキテクチャの基礎</h2>

        <h3>Transformerの基本構造</h3>
        <p>TransformerはEncoderとDecoderから構成されますが、LLMの多くは<strong>Decoder-Only</strong>アーキテクチャを採用しています。</p>

        <div class="mermaid">
graph TD
    A[入力トークン] --> B[埋め込み + 位置エンコーディング]
    B --> C[Multi-Head Self-Attention]
    C --> D[Add & Norm]
    D --> E[Feed-Forward Network]
    E --> F[Add & Norm]
    F --> G[次の層へ or 出力]

    style A fill:#e3f2fd
    style C fill:#fff3e0
    style E fill:#f3e5f5
    style G fill:#e8f5e9
        </div>

        <h3>主要コンポーネント</h3>

        <h4>1. Self-Attention（自己注意機構）</h4>
        <p>文中の各単語が他のすべての単語との関連性を学習する仕組みです。</p>
        <ul>
            <li><strong>Query（クエリ）</strong>: 注目したい単語</li>
            <li><strong>Key（キー）</strong>: 比較対象の単語</li>
            <li><strong>Value（値）</strong>: 取得する情報</li>
        </ul>

        <div class="example-box">
            <h4>🔍 Self-Attentionの例</h4>
            <p><strong>文</strong>: "猫が魚を食べた"</p>
            <p><strong>「食べた」に注目</strong>すると：</p>
            <ul>
                <li>"猫" → 高い注意（主語）</li>
                <li>"魚" → 高い注意（目的語）</li>
                <li>"が" → 中程度の注意</li>
                <li>"を" → 中程度の注意</li>
            </ul>
            <p>→ モデルは文法構造を自動的に学習</p>
        </div>

        <h4>2. Multi-Head Attention（多頭注意機構）</h4>
        <p>複数の異なる観点（head）から注意を計算し、並列に処理します。</p>
        <ul>
            <li><strong>利点</strong>: 異なる種類の関係性を同時に学習</li>
            <li><strong>典型的なhead数</strong>: 8〜16個</li>
        </ul>

        <h4>3. Position Encoding（位置エンコーディング）</h4>
        <p>Transformerは並列処理のため、単語の順序情報を明示的に与える必要があります。</p>
        <ul>
            <li><strong>絶対位置エンコーディング</strong>: 各位置に固有のベクトル</li>
            <li><strong>相対位置エンコーディング</strong>: 単語間の相対距離を考慮</li>
        </ul>

        <h4>4. Feed-Forward Network（順伝播ネットワーク）</h4>
        <p>各トークンの表現を独立に変換する全結合層です。</p>

        <h3>Decoder-Only vs Encoder-Decoder</h3>

        <table>
            <thead>
                <tr>
                    <th>アーキテクチャ</th>
                    <th>代表モデル</th>
                    <th>特徴</th>
                    <th>主な用途</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>Decoder-Only</strong></td>
                    <td>GPT-3, GPT-4, LLaMA</td>
                    <td>自己回帰的生成</td>
                    <td>テキスト生成、チャット</td>
                </tr>
                <tr>
                    <td><strong>Encoder-Only</strong></td>
                    <td>BERT</td>
                    <td>双方向理解</td>
                    <td>文分類、固有表現認識</td>
                </tr>
                <tr>
                    <td><strong>Encoder-Decoder</strong></td>
                    <td>T5, BART</td>
                    <td>入力→出力変換</td>
                    <td>翻訳、要約</td>
                </tr>
            </tbody>
        </table>

        <h2 id="use-cases">1.6 LLMの活用事例</h2>

        <h3>主な活用領域</h3>

        <h4>1. コンテンツ生成</h4>
        <ul>
            <li>記事、ブログ投稿の作成</li>
            <li>マーケティングコピー</li>
            <li>メール返信の下書き</li>
            <li>創作（小説、詩、脚本）</li>
        </ul>

        <h4>2. コード生成・支援</h4>
        <ul>
            <li>GitHub Copilot（Codexベース）</li>
            <li>バグ修正の提案</li>
            <li>コードレビュー</li>
            <li>ドキュメント生成</li>
        </ul>

        <h4>3. 質問応答・カスタマーサポート</h4>
        <ul>
            <li>FAQボット</li>
            <li>技術サポート</li>
            <li>社内ナレッジベース検索</li>
        </ul>

        <h4>4. 翻訳と要約</h4>
        <ul>
            <li>多言語翻訳</li>
            <li>文書要約</li>
            <li>会議議事録の自動生成</li>
        </ul>

        <h4>5. 教育支援</h4>
        <ul>
            <li>学習チューター</li>
            <li>問題生成</li>
            <li>採点とフィードバック</li>
        </ul>

        <h3>LLMを使ってみる：簡単なコード例</h3>

        <pre><code class="language-python"># Hugging Face transformersでGPT-2を使った文章生成
from transformers import pipeline

# テキスト生成パイプラインを作成
generator = pipeline('text-generation', model='gpt2')

# プロンプトを与えて文章生成
prompt = "人工知能の未来について考えると"
result = generator(
    prompt,
    max_length=100,
    num_return_sequences=1,
    temperature=0.7
)

print(result[0]['generated_text'])
</code></pre>

        <div class="info-box">
            <h4>💡 パラメータの説明</h4>
            <ul>
                <li><strong>max_length</strong>: 生成する最大トークン数</li>
                <li><strong>num_return_sequences</strong>: 生成する候補の数</li>
                <li><strong>temperature</strong>: ランダム性（0=決定的、1=創造的）</li>
            </ul>
        </div>

        <h2 id="limitations">1.7 LLMの限界と課題</h2>

        <h3>主な課題</h3>

        <h4>1. ハルシネーション（幻覚）</h4>
        <p>LLMは存在しない情報を、もっともらしく生成することがあります。</p>
        <div class="warning-box">
            <h4>⚠️ ハルシネーションの例</h4>
            <p><strong>質問</strong>: "2024年のノーベル物理学賞受賞者は誰ですか？"</p>
            <p><strong>誤った回答例</strong>: "山田太郎博士が量子コンピュータの研究で受賞しました"</p>
            <p>→ モデルは知らないことを「知らない」と言えず、もっともらしい嘘をつくことがある</p>
        </div>

        <h4>2. バイアスと公平性</h4>
        <ul>
            <li>訓練データに含まれる社会的バイアスを学習</li>
            <li>性別、人種、年齢等に関する偏見</li>
            <li>倫理的配慮の必要性</li>
        </ul>

        <h4>3. 知識のカットオフ</h4>
        <ul>
            <li>訓練データの期限以降の情報を知らない</li>
            <li>例：GPT-4（2023年版）は2023年4月以降の出来事を知らない</li>
        </ul>

        <h4>4. 計算コストとエネルギー</h4>
        <ul>
            <li>訓練に数百万ドル〜数千万ドルのコスト</li>
            <li>推論にも高い計算資源が必要</li>
            <li>環境への影響</li>
        </ul>

        <h4>5. プライバシーとセキュリティ</h4>
        <ul>
            <li>訓練データからの情報漏洩リスク</li>
            <li>悪用の可能性（フィッシング、偽情報）</li>
            <li>著作権の問題</li>
        </ul>

        <h3>対策と緩和策</h3>
        <ul>
            <li><strong>RLHF（人間フィードバックからの強化学習）</strong>: ChatGPT等で採用</li>
            <li><strong>RAG（検索拡張生成）</strong>: 外部知識ベースと統合</li>
            <li><strong>ファクトチェック機構</strong>: 生成内容の検証</li>
            <li><strong>透明性とドキュメント</strong>: モデルの限界を明示</li>
        </ul>

        <h2 id="future">1.8 LLMの未来</h2>

        <h3>今後の発展方向</h3>

        <h4>1. マルチモーダルAI</h4>
        <p>テキストだけでなく、画像、音声、動画を統合的に理解・生成するモデル。</p>
        <ul>
            <li>GPT-4V（Vision）: 画像理解</li>
            <li>Gemini: 生まれつきマルチモーダル</li>
        </ul>

        <h4>2. より効率的なモデル</h4>
        <p>小型でも高性能なモデルの開発。</p>
        <ul>
            <li>Mistral 7B: 70億パラメータで高性能</li>
            <li>量子化、プルーニング、蒸留</li>
        </ul>

        <h4>3. エージェント型AI</h4>
        <p>ツールを使い、計画を立て、行動できるAI。</p>
        <ul>
            <li>AutoGPT、BabyAGI</li>
            <li>関数呼び出し（Function Calling）</li>
        </ul>

        <h4>4. パーソナライズ</h4>
        <p>個人に最適化されたAIアシスタント。</p>
        <ul>
            <li>ユーザーの嗜好学習</li>
            <li>カスタムGPTs</li>
        </ul>

        <h4>5. オープンソース化</h4>
        <p>より多くのモデルがオープンソースとして公開される傾向。</p>
        <ul>
            <li>LLaMA、Mistral、Falcon等</li>
            <li>研究と開発の民主化</li>
        </ul>

        <h2 id="summary">まとめ</h2>

        <p>この章では、大規模言語モデル（LLM）の基礎を学びました。</p>

        <div class="info-box">
            <h4>📌 重要ポイント</h4>
            <ul>
                <li>LLMは<strong>Transformer</strong>アーキテクチャをベースとした大規模ニューラルネットワーク</li>
                <li>2017年のTransformer登場から急速に発展し、2023年のChatGPTで一般化</li>
                <li><strong>GPT-4、Claude、Gemini、LLaMA</strong>など多様なモデルが存在</li>
                <li><strong>トークン化</strong>により文字列を数値に変換して処理</li>
                <li><strong>Self-Attention</strong>により文脈理解を実現</li>
                <li>多様な活用事例があるが、<strong>ハルシネーション</strong>などの課題も存在</li>
                <li>今後はマルチモーダル、効率化、エージェント型へ発展</li>
            </ul>
        </div>

        <h2 id="exercises">演習問題</h2>

        <div class="example-box">
            <h4>📝 演習1：基礎知識確認</h4>
            <p><strong>問題</strong>: 以下の質問に答えてください。</p>
            <ol>
                <li>LLMの「大規模」とは何を指しますか？</li>
                <li>Transformerアーキテクチャの主な利点を2つ挙げてください。</li>
                <li>Decoder-OnlyとEncoder-Onlyモデルの違いを説明してください。</li>
            </ol>
        </div>

        <div class="example-box">
            <h4>📝 演習2：トークン化の実践</h4>
            <p><strong>課題</strong>: 以下のコードを実行し、異なるテキストのトークン数を比較してください。</p>
            <pre><code class="language-python">from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")

texts = [
    "こんにちは",
    "Hello",
    "人工知能は素晴らしい技術です",
    "Artificial Intelligence is amazing"
]

for text in texts:
    tokens = tokenizer.encode(text)
    print(f"'{text}' → {len(tokens)} tokens")
</code></pre>
            <p><strong>考察</strong>: 日本語と英語でトークン数に違いはありますか？ その理由を考えてください。</p>
        </div>

        <div class="example-box">
            <h4>📝 演習3：モデル比較</h4>
            <p><strong>課題</strong>: GPT-4、Claude、LLaMAの中から1つ選び、以下を調査してください。</p>
            <ul>
                <li>開発元と開発の背景</li>
                <li>主な特徴と強み</li>
                <li>利用方法（API、オープンソース等）</li>
                <li>代表的な活用事例</li>
            </ul>
            <p><strong>発展</strong>: 選んだモデルの公式ドキュメントを読み、技術的詳細をまとめてください。</p>
        </div>

        <h2 id="next">次の章へ</h2>
        <p>次の章では、LLMの中核技術である<strong>Transformerアーキテクチャ</strong>を詳しく学びます。Self-Attention、Multi-Head Attention、位置エンコーディングなどの仕組みを理解し、実際に動くコードで体験します。</p>

        <div class="nav-buttons">
            <a href="./index.html" class="nav-button">← シリーズ概要</a>
            <a href="./index.html" class="nav-button">第2章: Transformerアーキテクチャ（準備中）</a>
        </div>

    </main>

    <section class="disclaimer">
        <h3>免責事項</h3>
        <ul>
            <li>本コンテンツは教育・研究・情報提供のみを目的としており、専門的な助言(法律・会計・技術的保証など)を提供するものではありません。</li>
            <li>本コンテンツおよび付随するコード例は「現状有姿(AS IS)」で提供され、明示または黙示を問わず、商品性、特定目的適合性、権利非侵害、正確性・完全性、動作・安全性等いかなる保証もしません。</li>
            <li>外部リンク、第三者が提供するデータ・ツール・ライブラリ等の内容・可用性・安全性について、作成者および東北大学は一切の責任を負いません。</li>
            <li>本コンテンツの利用・実行・解釈により直接的・間接的・付随的・特別・結果的・懲罰的損害が生じた場合でも、適用法で許容される最大限の範囲で、作成者および東北大学は責任を負いません。</li>
            <li>本コンテンツの内容は、予告なく変更・更新・提供停止されることがあります。</li>
            <li>本コンテンツの著作権・ライセンスは明記された条件(例: CC BY 4.0)に従います。当該ライセンスは通常、無保証条項を含みます。</li>
        </ul>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
