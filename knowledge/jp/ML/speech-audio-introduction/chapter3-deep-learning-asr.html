<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬3ç« ï¼šæ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹éŸ³å£°èªè­˜ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>
</head>
<body>
        <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/wp/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/ML/speech-audio-introduction/index.html">Speech Audio</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

    <header>
        <div class="header-content">
            <h1>ç¬¬3ç« ï¼šæ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹éŸ³å£°èªè­˜</h1>
            <p class="subtitle">CTCã€Attentionã€RNN-Tã€Whisperã‚’ç”¨ã„ãŸæœ€æ–°ã®éŸ³å£°èªè­˜æŠ€è¡“</p>
            <div class="meta">
                <span class="meta-item">ğŸ“š éŸ³å£°ãƒ»ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªå‡¦ç†å…¥é–€</span>
                <span class="meta-item">â±ï¸ 60åˆ†</span>
                <span class="meta-item">ğŸ·ï¸ ML-D03</span>
            </div>
        </div>
    </header>

    <div class="container">
        <h2>ã“ã®ç« ã§å­¦ã¶ã“ã¨</h2>
        <p>æœ¬ç« ã§ã¯ã€æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹ç¾ä»£çš„ãªéŸ³å£°èªè­˜(ASR)æŠ€è¡“ã‚’å­¦ã³ã¾ã™ã€‚CTCã‚„Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãªã©ã®åŸºæœ¬çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‹ã‚‰ã€æœ€æ–°ã®Whisperãƒ¢ãƒ‡ãƒ«ã¾ã§ã€å®Ÿè·µçš„ãªã‚³ãƒ¼ãƒ‰ä¾‹ã¨ã¨ã‚‚ã«è§£èª¬ã—ã¾ã™ã€‚</p>

        <ul>
            <li>CTC (Connectionist Temporal Classification)ã«ã‚ˆã‚‹å­¦ç¿’ã¨æ¨è«–</li>
            <li>Attentionæ©Ÿæ§‹ã‚’ç”¨ã„ãŸEncoder-Decoderãƒ¢ãƒ‡ãƒ«</li>
            <li>RNN-Transducerã«ã‚ˆã‚‹ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°éŸ³å£°èªè­˜</li>
            <li>OpenAI Whisperã®æ´»ç”¨ã¨æ—¥æœ¬èªéŸ³å£°èªè­˜</li>
            <li>å®Ÿè·µçš„ãªASRã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æ§‹ç¯‰</li>
        </ul>

        <h2>1. CTC (Connectionist Temporal Classification)</h2>

        <h3>1.1 CTCã¨ã¯</h3>
        <p>CTCã¯ã€å…¥åŠ›ã¨å‡ºåŠ›ã®é•·ã•ãŒç•°ãªã‚‹ç³»åˆ—å¤‰æ›å•é¡Œã«ãŠã„ã¦ã€æ˜ç¤ºçš„ãªã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆæƒ…å ±ãªã—ã«å­¦ç¿’ã§ãã‚‹æ‰‹æ³•ã§ã™ã€‚éŸ³å£°èªè­˜ã§ã¯ã€éŸ³éŸ¿ç‰¹å¾´é‡ãƒ•ãƒ¬ãƒ¼ãƒ (å…¥åŠ›)ã¨ãƒ†ã‚­ã‚¹ãƒˆ(å‡ºåŠ›)ã®å¯¾å¿œé–¢ä¿‚ã‚’è‡ªå‹•çš„ã«å­¦ç¿’ã—ã¾ã™ã€‚</p>

        <h4>CTCã®ä¸»è¦æ¦‚å¿µ</h4>
        <ul>
            <li><strong>Blankè¨˜å·</strong>: å‡ºåŠ›ã—ãªã„ä½ç½®ã‚’è¡¨ã™ç‰¹æ®Šè¨˜å·(é€šå¸¸ã¯ã€Œ-ã€)</li>
            <li><strong>ç¹°ã‚Šè¿”ã—ã®é™¤å»</strong>: é€£ç¶šã™ã‚‹åŒã˜è¨˜å·ã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹</li>
            <li><strong>å¤šå¯¾ä¸€ãƒãƒƒãƒ”ãƒ³ã‚°</strong>: è¤‡æ•°ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãŒåŒã˜æ–‡å­—ã«å¯¾å¿œå¯èƒ½</li>
            <li><strong>æ¡ä»¶ä»˜ãç‹¬ç«‹æ€§</strong>: å„æ™‚åˆ»ã®å‡ºåŠ›ã¯ç‹¬ç«‹ã«ç”Ÿæˆã•ã‚Œã‚‹</li>
        </ul>

        <h3>1.2 CTCã®æå¤±é–¢æ•°</h3>
        <p>CTCæå¤±ã¯ã€å…¨ã¦ã®å¯èƒ½ãªã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ‘ã‚¹ã®ç¢ºç‡ã‚’å‘¨è¾ºåŒ–ã™ã‚‹ã“ã¨ã§è¨ˆç®—ã•ã‚Œã¾ã™ã€‚Forward-Backwardã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç”¨ã„ã¦åŠ¹ç‡çš„ã«è¨ˆç®—ã§ãã¾ã™ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class CTCASRModel(nn.Module):
    """CTCæå¤±ã‚’ç”¨ã„ãŸéŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«"""

    def __init__(self, input_dim=80, hidden_dim=256, num_classes=29, num_layers=3):
        """
        Args:
            input_dim: å…¥åŠ›ç‰¹å¾´é‡ã®æ¬¡å…ƒæ•°(MFCCã‚„ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ )
            hidden_dim: LSTMéš ã‚Œå±¤ã®æ¬¡å…ƒæ•°
            num_classes: å‡ºåŠ›ã‚¯ãƒ©ã‚¹æ•°(ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ + blank)
            num_layers: LSTMã®å±¤æ•°
        """
        super(CTCASRModel, self).__init__()

        self.lstm = nn.LSTM(
            input_dim,
            hidden_dim,
            num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=0.2
        )

        # åŒæ–¹å‘LSTMãªã®ã§å‡ºåŠ›æ¬¡å…ƒã¯2å€
        self.classifier = nn.Linear(hidden_dim * 2, num_classes)

    def forward(self, x, lengths):
        """
        Args:
            x: (batch, time, features)
            lengths: å„ã‚µãƒ³ãƒ—ãƒ«ã®å®Ÿéš›ã®é•·ã•
        Returns:
            log_probs: (time, batch, num_classes)
            output_lengths: å„ã‚µãƒ³ãƒ—ãƒ«ã®å‡ºåŠ›é•·
        """
        # PackedSequenceã§å¯å¤‰é•·å…¥åŠ›ã‚’åŠ¹ç‡çš„ã«å‡¦ç†
        packed = nn.utils.rnn.pack_padded_sequence(
            x, lengths.cpu(), batch_first=True, enforce_sorted=False
        )

        packed_output, _ = self.lstm(packed)
        output, output_lengths = nn.utils.rnn.pad_packed_sequence(
            packed_output, batch_first=True
        )

        # CTCç”¨ã«log softmaxé©ç”¨
        logits = self.classifier(output)
        log_probs = F.log_softmax(logits, dim=-1)

        # CTCã¯(T, N, C)ã®å½¢å¼ã‚’æœŸå¾…
        log_probs = log_probs.transpose(0, 1)

        return log_probs, output_lengths


# è¨“ç·´ä¾‹
def train_ctc_model():
    """CTC ASRãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ä¾‹"""

    # ãƒ¢ãƒ‡ãƒ«ã¨CTCæå¤±ã®åˆæœŸåŒ–
    model = CTCASRModel(input_dim=80, hidden_dim=256, num_classes=29)
    ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿(å®Ÿéš›ã«ã¯ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰å–å¾—)
    batch_size = 4
    max_time = 100
    features = torch.randn(batch_size, max_time, 80)
    feature_lengths = torch.tensor([100, 95, 90, 85])

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ†ã‚­ã‚¹ãƒˆ(æ•°å€¤åŒ–æ¸ˆã¿)
    targets = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 0], [8, 9, 10, 0], [11, 12, 0, 0]])
    target_lengths = torch.tensor([4, 3, 3, 2])

    model.train()

    # Forward pass
    log_probs, output_lengths = model(features, feature_lengths)

    # CTCæå¤±è¨ˆç®—
    loss = ctc_loss(log_probs, targets, output_lengths, target_lengths)

    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print(f"CTC Loss: {loss.item():.4f}")

    return model

# å®Ÿè¡Œä¾‹
if __name__ == "__main__":
    model = train_ctc_model()
    print("âœ“ CTCãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ãŒå®Œäº†ã—ã¾ã—ãŸ")
</code></pre>

        <h3>1.3 CTCãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°</h3>
        <p>è¨“ç·´ã•ã‚ŒãŸCTCãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å¾—ã‚‹ã«ã¯ã€ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†ãŒå¿…è¦ã§ã™ã€‚ä¸»ãªæ‰‹æ³•ã«ã¯ã€Greedy Decodingã¨ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒãŒã‚ã‚Šã¾ã™ã€‚</p>

<pre><code class="language-python">import numpy as np
from collections import defaultdict

class CTCDecoder:
    """CTCãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼(Greedy & Beam Search)"""

    def __init__(self, labels, blank_idx=0):
        """
        Args:
            labels: æ–‡å­—ãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆ
            blank_idx: ãƒ–ãƒ©ãƒ³ã‚¯è¨˜å·ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        """
        self.labels = labels
        self.blank_idx = blank_idx

    def greedy_decode(self, log_probs):
        """
        Greedy Decoding: å„æ™‚åˆ»ã§æœ€ã‚‚ç¢ºç‡ã®é«˜ã„ãƒ©ãƒ™ãƒ«ã‚’é¸æŠ

        Args:
            log_probs: (time, num_classes) ã®å¯¾æ•°ç¢ºç‡
        Returns:
            decoded_text: ãƒ‡ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        # å„æ™‚åˆ»ã§æœ€ã‚‚ç¢ºç‡ã®é«˜ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
        best_path = torch.argmax(log_probs, dim=-1)

        # é€£ç¶šã™ã‚‹é‡è¤‡ã¨blankã‚’é™¤å»
        decoded = []
        prev_idx = self.blank_idx

        for idx in best_path:
            idx = idx.item()
            if idx != self.blank_idx and idx != prev_idx:
                decoded.append(self.labels[idx])
            prev_idx = idx

        return ''.join(decoded)

    def beam_search_decode(self, log_probs, beam_width=10):
        """
        Beam Search Decoding: ã‚ˆã‚Šæ­£ç¢ºãªãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

        Args:
            log_probs: (time, num_classes) ã®å¯¾æ•°ç¢ºç‡
            beam_width: ãƒ“ãƒ¼ãƒ å¹…
        Returns:
            decoded_text: ãƒ‡ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        T, C = log_probs.shape
        log_probs = log_probs.cpu().numpy()

        # ãƒ“ãƒ¼ãƒ : {sequence: probability}
        beams = {('', self.blank_idx): 0.0}  # (text, last_char): log_prob

        for t in range(T):
            new_beams = defaultdict(lambda: float('-inf'))

            for (text, last_char), log_prob in beams.items():
                for c in range(C):
                    new_log_prob = log_prob + log_probs[t, c]

                    if c == self.blank_idx:
                        # Blankã®å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’å¤‰æ›´ã—ãªã„
                        new_beams[(text, c)] = np.logaddexp(
                            new_beams[(text, c)], new_log_prob
                        )
                    else:
                        if c == last_char:
                            # å‰ã®æ–‡å­—ã¨åŒã˜å ´åˆã¯ç¹°ã‚Šè¿”ã•ãªã„
                            new_beams[(text, c)] = np.logaddexp(
                                new_beams[(text, c)], new_log_prob
                            )
                        else:
                            # æ–°ã—ã„æ–‡å­—ã‚’è¿½åŠ 
                            new_text = text + self.labels[c]
                            new_beams[(new_text, c)] = np.logaddexp(
                                new_beams[(new_text, c)], new_log_prob
                            )

            # ä¸Šä½beam_widthå€‹ã®ãƒ“ãƒ¼ãƒ ã‚’ä¿æŒ
            beams = dict(sorted(new_beams.items(), key=lambda x: x[1], reverse=True)[:beam_width])

        # æœ€ã‚‚ç¢ºç‡ã®é«˜ã„ãƒ“ãƒ¼ãƒ ã‚’è¿”ã™
        best_beam = max(beams.items(), key=lambda x: x[1])
        return best_beam[0][0]


# ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä¾‹
def decode_example():
    """CTC ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å®Ÿè¡Œä¾‹"""

    # ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ(0ã¯blank)
    labels = ['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',
              'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ', "'"]

    decoder = CTCDecoder(labels, blank_idx=0)

    # ãƒ€ãƒŸãƒ¼ã®å¯¾æ•°ç¢ºç‡(å®Ÿéš›ã¯ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›)
    T = 50
    C = len(labels)
    log_probs = torch.randn(T, C)
    log_probs = F.log_softmax(log_probs, dim=-1)

    # Greedy Decoding
    greedy_text = decoder.greedy_decode(log_probs)
    print(f"Greedy Decode: {greedy_text}")

    # Beam Search Decoding
    beam_text = decoder.beam_search_decode(log_probs, beam_width=10)
    print(f"Beam Search Decode: {beam_text}")

decode_example()
</code></pre>

        <h2>2. Attention-based Models</h2>

        <h3>2.1 Listen, Attend and Spell (LAS)</h3>
        <p>LASã¯ã€Encoder-Decoderã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’çµ„ã¿åˆã‚ã›ãŸéŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚EncoderãŒéŸ³éŸ¿ç‰¹å¾´é‡ã‚’é«˜ãƒ¬ãƒ™ãƒ«è¡¨ç¾ã«å¤‰æ›ã—ã€DecoderãŒAttentionã§å¿…è¦ãªæƒ…å ±ã«æ³¨ç›®ã—ãªãŒã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class ListenAttendSpell(nn.Module):
    """Listen, Attend and Spell ãƒ¢ãƒ‡ãƒ«"""

    def __init__(self, input_dim=80, encoder_hidden=256, decoder_hidden=512,
                 vocab_size=29, num_layers=3):
        super(ListenAttendSpell, self).__init__()

        self.encoder = Listener(input_dim, encoder_hidden, num_layers)
        self.decoder = Speller(encoder_hidden * 2, decoder_hidden, vocab_size)

    def forward(self, inputs, input_lengths, targets=None, teacher_forcing_ratio=0.9):
        """
        Args:
            inputs: (batch, time, features)
            input_lengths: å„ã‚µãƒ³ãƒ—ãƒ«ã®é•·ã•
            targets: (batch, target_len) ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            teacher_forcing_ratio: Teacher Forcingã®å‰²åˆ
        """
        # Encoder
        encoder_outputs, encoder_lengths = self.encoder(inputs, input_lengths)

        # Decoder
        if targets is not None:
            outputs = self.decoder(encoder_outputs, encoder_lengths, targets,
                                  teacher_forcing_ratio)
        else:
            outputs = self.decoder.inference(encoder_outputs, encoder_lengths)

        return outputs


class Listener(nn.Module):
    """Encoder: éŸ³éŸ¿ç‰¹å¾´é‡ã‚’é«˜ãƒ¬ãƒ™ãƒ«è¡¨ç¾ã«å¤‰æ›"""

    def __init__(self, input_dim, hidden_dim, num_layers):
        super(Listener, self).__init__()

        self.lstm = nn.LSTM(
            input_dim,
            hidden_dim,
            num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=0.2
        )

        # Pyramidal LSTM: æ™‚é–“æ–¹å‘ã‚’åœ§ç¸®ã—ã¦è¨ˆç®—åŠ¹ç‡ã‚’å‘ä¸Š
        self.pyramid_lstm = nn.LSTM(
            hidden_dim * 4,  # 2ã¤ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’çµåˆ + åŒæ–¹å‘
            hidden_dim,
            num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=0.2
        )

    def forward(self, x, lengths):
        """
        Args:
            x: (batch, time, features)
            lengths: å„ã‚µãƒ³ãƒ—ãƒ«ã®é•·ã•
        """
        # ç¬¬1å±¤LSTM
        packed = nn.utils.rnn.pack_padded_sequence(
            x, lengths.cpu(), batch_first=True, enforce_sorted=False
        )
        output, _ = self.lstm(packed)
        output, lengths = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)

        # Pyramidal: 2ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’1ã¤ã«çµåˆã—ã¦æ™‚é–“ã‚’åŠåˆ†ã«
        batch, time, features = output.size()
        if time % 2 == 1:
            output = output[:, :-1, :]
            time -= 1

        output = output.reshape(batch, time // 2, features * 2)
        lengths = lengths // 2

        # ç¬¬2å±¤LSTM
        packed = nn.utils.rnn.pack_padded_sequence(
            output, lengths.cpu(), batch_first=True, enforce_sorted=False
        )
        output, _ = self.pyramid_lstm(packed)
        output, lengths = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)

        return output, lengths


class Speller(nn.Module):
    """Decoder: Attentionã‚’ç”¨ã„ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""

    def __init__(self, encoder_dim, hidden_dim, vocab_size):
        super(Speller, self).__init__()

        self.vocab_size = vocab_size
        self.embedding = nn.Embedding(vocab_size, hidden_dim)
        self.lstm = nn.LSTMCell(hidden_dim + encoder_dim, hidden_dim)
        self.attention = BahdanauAttention(encoder_dim, hidden_dim)
        self.classifier = nn.Linear(hidden_dim + encoder_dim, vocab_size)

    def forward(self, encoder_outputs, encoder_lengths, targets, teacher_forcing_ratio=0.9):
        """
        Teacher Forcingä»˜ãè¨“ç·´
        """
        batch_size = encoder_outputs.size(0)
        max_len = targets.size(1)

        # åˆæœŸçŠ¶æ…‹
        hidden = torch.zeros(batch_size, self.lstm.hidden_size, device=encoder_outputs.device)
        cell = torch.zeros(batch_size, self.lstm.hidden_size, device=encoder_outputs.device)

        # é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³
        input_token = torch.zeros(batch_size, dtype=torch.long, device=encoder_outputs.device)

        outputs = []

        for t in range(max_len):
            # Embedding
            embedded = self.embedding(input_token)

            # Attention
            context, _ = self.attention(hidden, encoder_outputs, encoder_lengths)

            # LSTM
            lstm_input = torch.cat([embedded, context], dim=1)
            hidden, cell = self.lstm(lstm_input, (hidden, cell))

            # å‡ºåŠ›
            output = self.classifier(torch.cat([hidden, context], dim=1))
            outputs.append(output)

            # Teacher Forcing
            use_teacher_forcing = torch.rand(1).item() < teacher_forcing_ratio
            if use_teacher_forcing:
                input_token = targets[:, t]
            else:
                input_token = output.argmax(dim=1)

        return torch.stack(outputs, dim=1)

    def inference(self, encoder_outputs, encoder_lengths, max_len=100):
        """æ¨è«–æ™‚ã®ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        batch_size = encoder_outputs.size(0)

        hidden = torch.zeros(batch_size, self.lstm.hidden_size, device=encoder_outputs.device)
        cell = torch.zeros(batch_size, self.lstm.hidden_size, device=encoder_outputs.device)

        input_token = torch.zeros(batch_size, dtype=torch.long, device=encoder_outputs.device)

        outputs = []

        for t in range(max_len):
            embedded = self.embedding(input_token)
            context, _ = self.attention(hidden, encoder_outputs, encoder_lengths)

            lstm_input = torch.cat([embedded, context], dim=1)
            hidden, cell = self.lstm(lstm_input, (hidden, cell))

            output = self.classifier(torch.cat([hidden, context], dim=1))
            outputs.append(output)

            input_token = output.argmax(dim=1)

            # çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã§åœæ­¢
            if (input_token == self.vocab_size - 1).all():
                break

        return torch.stack(outputs, dim=1)


class BahdanauAttention(nn.Module):
    """Bahdanau Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ """

    def __init__(self, encoder_dim, decoder_dim):
        super(BahdanauAttention, self).__init__()

        self.encoder_projection = nn.Linear(encoder_dim, decoder_dim)
        self.decoder_projection = nn.Linear(decoder_dim, decoder_dim)
        self.v = nn.Linear(decoder_dim, 1)

    def forward(self, decoder_hidden, encoder_outputs, encoder_lengths):
        """
        Args:
            decoder_hidden: (batch, decoder_dim)
            encoder_outputs: (batch, time, encoder_dim)
            encoder_lengths: (batch,)
        Returns:
            context: (batch, encoder_dim)
            attention_weights: (batch, time)
        """
        batch_size, time_steps, _ = encoder_outputs.size()

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
        encoder_proj = self.encoder_projection(encoder_outputs)  # (batch, time, decoder_dim)
        decoder_proj = self.decoder_projection(decoder_hidden).unsqueeze(1)  # (batch, 1, decoder_dim)

        # Energyè¨ˆç®—
        energy = self.v(torch.tanh(encoder_proj + decoder_proj)).squeeze(-1)  # (batch, time)

        # ãƒã‚¹ã‚¯(ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°éƒ¨åˆ†ã‚’ç„¡è¦–)
        mask = torch.arange(time_steps, device=encoder_outputs.device).expand(
            batch_size, time_steps
        ) < encoder_lengths.unsqueeze(1)

        energy = energy.masked_fill(~mask, float('-inf'))

        # Attention weights
        attention_weights = F.softmax(energy, dim=1)  # (batch, time)

        # Context vector
        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)

        return context, attention_weights


# ä½¿ç”¨ä¾‹
def las_example():
    """LASãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ä¾‹"""

    model = ListenAttendSpell(
        input_dim=80,
        encoder_hidden=256,
        decoder_hidden=512,
        vocab_size=29
    )

    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
    batch_size = 4
    inputs = torch.randn(batch_size, 100, 80)
    input_lengths = torch.tensor([100, 95, 90, 85])
    targets = torch.randint(0, 29, (batch_size, 20))

    # è¨“ç·´ãƒ¢ãƒ¼ãƒ‰
    outputs = model(inputs, input_lengths, targets, teacher_forcing_ratio=0.9)
    print(f"Training output shape: {outputs.shape}")

    # æ¨è«–ãƒ¢ãƒ¼ãƒ‰
    model.eval()
    with torch.no_grad():
        predictions = model(inputs, input_lengths, targets=None)
        print(f"Inference output shape: {predictions.shape}")

las_example()
</code></pre>

        <h3>2.2 Transformer for ASR</h3>
        <p>Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ASRã«é©ç”¨ã™ã‚‹ã“ã¨ã§ã€ä¸¦åˆ—å‡¦ç†ãŒå¯èƒ½ã«ãªã‚Šã€é•·è·é›¢ä¾å­˜é–¢ä¿‚ã‚’ã‚ˆã‚ŠåŠ¹æœçš„ã«æ‰ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import math

class TransformerASR(nn.Module):
    """Transformer based ASR model"""

    def __init__(self, input_dim=80, d_model=512, nhead=8, num_encoder_layers=6,
                 num_decoder_layers=6, dim_feedforward=2048, vocab_size=29, dropout=0.1):
        super(TransformerASR, self).__init__()

        # å…¥åŠ›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
        self.input_projection = nn.Linear(input_dim, d_model)

        # Positional Encoding
        self.pos_encoder = PositionalEncoding(d_model, dropout)

        # Transformer
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=nhead,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True
        )

        # å‡ºåŠ›å±¤
        self.output_projection = nn.Linear(d_model, vocab_size)

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.d_model = d_model
        self.vocab_size = vocab_size

        # Embedding (ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼å…¥åŠ›ç”¨)
        self.tgt_embedding = nn.Embedding(vocab_size, d_model)

    def forward(self, src, tgt, src_mask=None, tgt_mask=None,
                src_key_padding_mask=None, tgt_key_padding_mask=None):
        """
        Args:
            src: (batch, src_len, input_dim)
            tgt: (batch, tgt_len)
            src_mask: Encoder self-attention mask
            tgt_mask: Decoder self-attention mask (causal)
            src_key_padding_mask: (batch, src_len) ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯
            tgt_key_padding_mask: (batch, tgt_len) ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯
        """
        # Encoderå…¥åŠ›ã®æº–å‚™
        src = self.input_projection(src) * math.sqrt(self.d_model)
        src = self.pos_encoder(src)

        # Decoderå…¥åŠ›ã®æº–å‚™
        tgt_embedded = self.tgt_embedding(tgt) * math.sqrt(self.d_model)
        tgt_embedded = self.pos_encoder(tgt_embedded)

        # Transformer
        output = self.transformer(
            src, tgt_embedded,
            src_mask=src_mask,
            tgt_mask=tgt_mask,
            src_key_padding_mask=src_key_padding_mask,
            tgt_key_padding_mask=tgt_key_padding_mask
        )

        # å‡ºåŠ›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
        output = self.output_projection(output)

        return output

    def generate_square_subsequent_mask(self, sz):
        """Causal maskç”Ÿæˆ(æœªæ¥ã®æƒ…å ±ã‚’è¦‹ãªã„ã‚ˆã†ã«ã™ã‚‹)"""
        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()
        return mask


class PositionalEncoding(nn.Module):
    """Positional Encoding for Transformer"""

    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Positional encodingã®äº‹å‰è¨ˆç®—
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                            (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)

        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        Args:
            x: (batch, seq_len, d_model)
        """
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)


# è¨“ç·´ä¾‹
def train_transformer_asr():
    """Transformer ASRã®è¨“ç·´ä¾‹"""

    model = TransformerASR(
        input_dim=80,
        d_model=512,
        nhead=8,
        num_encoder_layers=6,
        num_decoder_layers=6,
        vocab_size=29
    )

    criterion = nn.CrossEntropyLoss(ignore_index=0)  # 0ã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98))

    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
    batch_size = 8
    src = torch.randn(batch_size, 100, 80)
    tgt = torch.randint(1, 29, (batch_size, 30))

    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯
    src_key_padding_mask = torch.zeros(batch_size, 100).bool()
    tgt_key_padding_mask = torch.zeros(batch_size, 30).bool()

    # Causal mask
    tgt_mask = model.generate_square_subsequent_mask(30).to(tgt.device)

    # Forward
    output = model(src, tgt[:, :-1], tgt_mask=tgt_mask,
                   src_key_padding_mask=src_key_padding_mask,
                   tgt_key_padding_mask=tgt_key_padding_mask[:, :-1])

    # Lossè¨ˆç®—
    loss = criterion(output.reshape(-1, model.vocab_size), tgt[:, 1:].reshape(-1))

    # Backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print(f"Transformer ASR Loss: {loss.item():.4f}")

    return model

train_transformer_asr()
</code></pre>

        <h3>2.3 Joint CTC-Attention</h3>
        <p>CTCã¨Attentionã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ãã‚Œãã‚Œã®é•·æ‰€ã‚’æ´»ã‹ã™ã“ã¨ãŒã§ãã¾ã™ã€‚CTCã¯ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã®å­¦ç¿’ã‚’åŠ©ã‘ã€Attentionã¯æ–‡è„ˆæƒ…å ±ã‚’æ´»ç”¨ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">class JointCTCAttention(nn.Module):
    """CTC ã¨ Attention ã‚’çµ„ã¿åˆã‚ã›ãŸãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«"""

    def __init__(self, input_dim=80, encoder_hidden=256, decoder_hidden=512,
                 vocab_size=29, num_layers=3, ctc_weight=0.3):
        super(JointCTCAttention, self).__init__()

        # å…±æœ‰Encoder
        self.encoder = Listener(input_dim, encoder_hidden, num_layers)

        # CTCç”¨ã®åˆ†é¡å™¨
        self.ctc_classifier = nn.Linear(encoder_hidden * 2, vocab_size)

        # Attention-based Decoder
        self.decoder = Speller(encoder_hidden * 2, decoder_hidden, vocab_size)

        # CTCæå¤±ã®é‡ã¿
        self.ctc_weight = ctc_weight
        self.ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)

    def forward(self, inputs, input_lengths, targets, target_lengths=None,
                teacher_forcing_ratio=0.9):
        """
        Args:
            inputs: (batch, time, features)
            input_lengths: å„ã‚µãƒ³ãƒ—ãƒ«ã®éŸ³éŸ¿ç‰¹å¾´é‡ã®é•·ã•
            targets: (batch, target_len) ãƒ†ã‚­ã‚¹ãƒˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            target_lengths: å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®é•·ã•(CTCç”¨)
            teacher_forcing_ratio: Teacher Forcingã®å‰²åˆ
        Returns:
            ctc_loss: CTCæå¤±
            attention_loss: Attentionæå¤±
            combined_loss: çµ„ã¿åˆã‚ã›ãŸæå¤±
        """
        # Encoder (å…±æœ‰)
        encoder_outputs, encoder_lengths = self.encoder(inputs, input_lengths)

        # CTCåˆ†å²
        ctc_logits = self.ctc_classifier(encoder_outputs)
        ctc_log_probs = F.log_softmax(ctc_logits, dim=-1)
        ctc_log_probs = ctc_log_probs.transpose(0, 1)  # (T, N, C)

        # Attentionåˆ†å²
        attention_outputs = self.decoder(encoder_outputs, encoder_lengths,
                                        targets, teacher_forcing_ratio)

        return ctc_log_probs, encoder_lengths, attention_outputs

    def compute_loss(self, ctc_log_probs, encoder_lengths, attention_outputs,
                     targets, target_lengths):
        """
        æå¤±ã®è¨ˆç®—
        """
        # CTCæå¤±
        ctc_loss = self.ctc_loss(ctc_log_probs, targets, encoder_lengths, target_lengths)

        # Attentionæå¤± (Cross Entropy)
        attention_loss = F.cross_entropy(
            attention_outputs.reshape(-1, attention_outputs.size(-1)),
            targets.reshape(-1),
            ignore_index=0
        )

        # çµ„ã¿åˆã‚ã›ãŸæå¤±
        combined_loss = self.ctc_weight * ctc_loss + (1 - self.ctc_weight) * attention_loss

        return ctc_loss, attention_loss, combined_loss

    def recognize(self, inputs, input_lengths, beam_width=10):
        """
        æ¨è«–: CTCã¨Attentionã®ä¸¡æ–¹ã‚’ä½¿ç”¨ã—ãŸãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        """
        self.eval()
        with torch.no_grad():
            # Encoder
            encoder_outputs, encoder_lengths = self.encoder(inputs, input_lengths)

            # CTCåˆ†å² (ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒç”¨)
            ctc_logits = self.ctc_classifier(encoder_outputs)
            ctc_probs = F.softmax(ctc_logits, dim=-1)

            # Attentionåˆ†å²
            attention_outputs = self.decoder.inference(encoder_outputs, encoder_lengths)
            attention_probs = F.softmax(attention_outputs, dim=-1)

            # CTC ã¨ Attentionã®ã‚¹ã‚³ã‚¢ã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ‡ã‚³ãƒ¼ãƒ‰
            # (å®Ÿéš›ã«ã¯ã‚ˆã‚Šè¤‡é›‘ãªãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨)
            combined_probs = (self.ctc_weight * ctc_probs[0] +
                            (1 - self.ctc_weight) * attention_probs)

            predictions = combined_probs.argmax(dim=-1)

            return predictions


# è¨“ç·´ä¾‹
def train_joint_model():
    """Joint CTC-Attention ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""

    model = JointCTCAttention(
        input_dim=80,
        encoder_hidden=256,
        decoder_hidden=512,
        vocab_size=29,
        ctc_weight=0.3
    )

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
    batch_size = 4
    inputs = torch.randn(batch_size, 100, 80)
    input_lengths = torch.tensor([100, 95, 90, 85])
    targets = torch.randint(1, 29, (batch_size, 20))
    target_lengths = torch.tensor([20, 18, 17, 15])

    # Forward
    ctc_log_probs, encoder_lengths, attention_outputs = model(
        inputs, input_lengths, targets, target_lengths
    )

    # æå¤±è¨ˆç®—
    ctc_loss, attention_loss, combined_loss = model.compute_loss(
        ctc_log_probs, encoder_lengths, attention_outputs, targets, target_lengths
    )

    # Backward
    optimizer.zero_grad()
    combined_loss.backward()
    optimizer.step()

    print(f"CTC Loss: {ctc_loss.item():.4f}")
    print(f"Attention Loss: {attention_loss.item():.4f}")
    print(f"Combined Loss: {combined_loss.item():.4f}")

    return model

train_joint_model()
</code></pre>

        <h2>3. RNN-Transducer (RNN-T)</h2>

        <h3>3.1 RNN-Tã¨ã¯</h3>
        <p>RNN-Transducerã¯ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°éŸ³å£°èªè­˜ã«é©ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚éŸ³éŸ¿ãƒ¢ãƒ‡ãƒ«(Encoder)ã€è¨€èªãƒ¢ãƒ‡ãƒ«(Prediction Network)ã€ãã—ã¦Joint Networkã®3ã¤ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‹ã‚‰æ§‹æˆã•ã‚Œã¾ã™ã€‚CTCã¨ç•°ãªã‚Šã€è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’æ˜ç¤ºçš„ã«çµ„ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚</p>

        <h4>RNN-Tã®ç‰¹å¾´</h4>
        <ul>
            <li><strong>ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œ</strong>: ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‡¦ç†ãŒå¯èƒ½</li>
            <li><strong>è¨€èªãƒ¢ãƒ‡ãƒ«çµ±åˆ</strong>: Prediction NetworkãŒè¨€èªæƒ…å ±ã‚’æä¾›</li>
            <li><strong>æŸ”è»Ÿãªã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ</strong>: CTCã‚ˆã‚ŠæŸ”è»Ÿãªå‡ºåŠ›ã‚¿ã‚¤ãƒŸãƒ³ã‚°</li>
            <li><strong>Blankè¨˜å·</strong>: å‡ºåŠ›ãªã—(å¾…æ©Ÿ)ã‚’è¡¨ç¾å¯èƒ½</li>
        </ul>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class RNNTransducer(nn.Module):
    """RNN-Transducer ãƒ¢ãƒ‡ãƒ«"""

    def __init__(self, input_dim=80, encoder_dim=256, pred_dim=256,
                 joint_dim=512, vocab_size=29, num_layers=3):
        """
        Args:
            input_dim: å…¥åŠ›ç‰¹å¾´é‡ã®æ¬¡å…ƒ
            encoder_dim: Encoderã®éš ã‚Œå±¤æ¬¡å…ƒ
            pred_dim: Prediction Networkã®éš ã‚Œå±¤æ¬¡å…ƒ
            joint_dim: Joint Networkã®éš ã‚Œå±¤æ¬¡å…ƒ
            vocab_size: èªå½™ã‚µã‚¤ã‚º(blankå«ã‚€)
        """
        super(RNNTransducer, self).__init__()

        # Encoder (Transcription Network)
        self.encoder = nn.LSTM(
            input_dim,
            encoder_dim,
            num_layers,
            batch_first=True,
            bidirectional=False,  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”¨ã«å˜æ–¹å‘
            dropout=0.2
        )

        # Prediction Network (Language Model)
        self.embedding = nn.Embedding(vocab_size, pred_dim)
        self.prediction = nn.LSTM(
            pred_dim,
            pred_dim,
            num_layers,
            batch_first=True,
            dropout=0.2
        )

        # Joint Network
        self.joint = JointNetwork(encoder_dim, pred_dim, joint_dim, vocab_size)

        self.vocab_size = vocab_size
        self.blank_idx = 0

    def forward(self, inputs, input_lengths, targets, target_lengths):
        """
        Args:
            inputs: (batch, time, features) éŸ³éŸ¿ç‰¹å¾´é‡
            input_lengths: å„ã‚µãƒ³ãƒ—ãƒ«ã®å…¥åŠ›é•·
            targets: (batch, target_len) ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ©ãƒ™ãƒ«
            target_lengths: å„ã‚µãƒ³ãƒ—ãƒ«ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé•·
        Returns:
            joint_output: (batch, time, target_len+1, vocab_size)
        """
        # Encoder
        encoder_out, _ = self.encoder(inputs)  # (batch, time, encoder_dim)

        # Prediction Network
        # é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ 
        batch_size = targets.size(0)
        start_tokens = torch.zeros(batch_size, 1, dtype=torch.long, device=targets.device)
        pred_input = torch.cat([start_tokens, targets], dim=1)

        pred_embedded = self.embedding(pred_input)
        pred_out, _ = self.prediction(pred_embedded)  # (batch, target_len+1, pred_dim)

        # Joint Network
        joint_output = self.joint(encoder_out, pred_out)

        return joint_output

    def greedy_decode(self, inputs, input_lengths, max_len=100):
        """
        Greedy decoding for inference
        """
        self.eval()
        with torch.no_grad():
            batch_size = inputs.size(0)

            # Encoder
            encoder_out, _ = self.encoder(inputs)
            time_steps = encoder_out.size(1)

            # åˆæœŸåŒ–
            predictions = []
            pred_hidden = None

            # é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³
            pred_input = torch.zeros(batch_size, 1, dtype=torch.long, device=inputs.device)

            for t in range(time_steps):
                # ç¾åœ¨ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼å‡ºåŠ›
                enc_t = encoder_out[:, t:t+1, :]  # (batch, 1, encoder_dim)

                # Prediction Network
                pred_embedded = self.embedding(pred_input)
                pred_out, pred_hidden = self.prediction(pred_embedded, pred_hidden)

                # Joint Network
                joint_out = self.joint(enc_t, pred_out)  # (batch, 1, 1, vocab_size)

                # æœ€ã‚‚ç¢ºç‡ã®é«˜ã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠ
                prob = F.softmax(joint_out.squeeze(1).squeeze(1), dim=-1)
                pred_token = prob.argmax(dim=-1)

                # Blankã§ãªã„å ´åˆã®ã¿å‡ºåŠ›ã«è¿½åŠ 
                if pred_token.item() != self.blank_idx:
                    predictions.append(pred_token.item())
                    pred_input = pred_token.unsqueeze(1)

                if len(predictions) >= max_len:
                    break

            return predictions


class JointNetwork(nn.Module):
    """Joint Network: Encoderã¨Prediction Networkã®å‡ºåŠ›ã‚’çµåˆ"""

    def __init__(self, encoder_dim, pred_dim, joint_dim, vocab_size):
        super(JointNetwork, self).__init__()

        self.encoder_proj = nn.Linear(encoder_dim, joint_dim)
        self.pred_proj = nn.Linear(pred_dim, joint_dim)
        self.output_proj = nn.Linear(joint_dim, vocab_size)

    def forward(self, encoder_out, pred_out):
        """
        Args:
            encoder_out: (batch, time, encoder_dim)
            pred_out: (batch, target_len, pred_dim)
        Returns:
            joint_out: (batch, time, target_len, vocab_size)
        """
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
        enc_proj = self.encoder_proj(encoder_out)  # (batch, time, joint_dim)
        pred_proj = self.pred_proj(pred_out)  # (batch, target_len, joint_dim)

        # ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã—ã¦åŠ ç®—
        # (batch, time, 1, joint_dim) + (batch, 1, target_len, joint_dim)
        joint = torch.tanh(
            enc_proj.unsqueeze(2) + pred_proj.unsqueeze(1)
        )  # (batch, time, target_len, joint_dim)

        # å‡ºåŠ›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
        output = self.output_proj(joint)  # (batch, time, target_len, vocab_size)

        return output


# RNN-T Loss (ç°¡æ˜“ç‰ˆ)
class RNNTLoss(nn.Module):
    """RNN-Tæå¤±é–¢æ•° (Forward-Backward ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ )"""

    def __init__(self, blank_idx=0):
        super(RNNTLoss, self).__init__()
        self.blank_idx = blank_idx

    def forward(self, logits, targets, input_lengths, target_lengths):
        """
        Args:
            logits: (batch, time, target_len+1, vocab_size)
            targets: (batch, target_len)
            input_lengths: (batch,)
            target_lengths: (batch,)
        """
        # PyTorchã®torchaudio.functional.rnnt_lossã‚’ä½¿ç”¨
        # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«CTCæå¤±ã§ä»£ç”¨
        batch_size, time, _, vocab_size = logits.size()

        # Greedy path approximation
        log_probs = F.log_softmax(logits, dim=-1)

        # å„æ™‚åˆ»ã§ã®blankã¨éblankã®ç¢ºç‡ã‚’è¨ˆç®—
        loss = 0
        for b in range(batch_size):
            for t in range(input_lengths[b]):
                for u in range(target_lengths[b] + 1):
                    if u < target_lengths[b]:
                        # éblank: æ­£ã—ã„ãƒ©ãƒ™ãƒ«ã®ç¢ºç‡
                        target_label = targets[b, u]
                        loss -= log_probs[b, t, u, target_label]
                    else:
                        # blank
                        loss -= log_probs[b, t, u, self.blank_idx]

        return loss / batch_size


# ä½¿ç”¨ä¾‹
def train_rnnt():
    """RNN-T ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ä¾‹"""

    model = RNNTransducer(
        input_dim=80,
        encoder_dim=256,
        pred_dim=256,
        joint_dim=512,
        vocab_size=29
    )

    criterion = RNNTLoss(blank_idx=0)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
    batch_size = 4
    inputs = torch.randn(batch_size, 100, 80)
    input_lengths = torch.tensor([100, 95, 90, 85])
    targets = torch.randint(1, 29, (batch_size, 20))
    target_lengths = torch.tensor([20, 18, 17, 15])

    # Forward
    logits = model(inputs, input_lengths, targets, target_lengths)

    # Loss
    loss = criterion(logits, targets, input_lengths, target_lengths)

    # Backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print(f"RNN-T Loss: {loss.item():.4f}")

    # æ¨è«–ä¾‹
    predictions = model.greedy_decode(inputs[:1], input_lengths[:1])
    print(f"Predicted tokens: {predictions}")

    return model

train_rnnt()
</code></pre>

        <h3>3.2 CTCã¨RNN-Tã®æ¯”è¼ƒ</h3>

        <table>
            <thead>
                <tr>
                    <th>ç‰¹å¾´</th>
                    <th>CTC</th>
                    <th>RNN-Transducer</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>è¨€èªãƒ¢ãƒ‡ãƒ«</td>
                    <td>å¤–éƒ¨LMãŒå¿…è¦</td>
                    <td>Prediction Networkã§å†…éƒ¨çµ±åˆ</td>
                </tr>
                <tr>
                    <td>ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°</td>
                    <td>å¯èƒ½(å˜æ–¹å‘LSTMä½¿ç”¨æ™‚)</td>
                    <td>è¨­è¨ˆä¸Šã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œ</td>
                </tr>
                <tr>
                    <td>è¨ˆç®—ã‚³ã‚¹ãƒˆ</td>
                    <td>ä½ã„</td>
                    <td>ã‚„ã‚„é«˜ã„(Joint Network)</td>
                </tr>
                <tr>
                    <td>ç²¾åº¦</td>
                    <td>ä¸­ç¨‹åº¦</td>
                    <td>é«˜ã„(è¨€èªãƒ¢ãƒ‡ãƒ«çµ±åˆåŠ¹æœ)</td>
                </tr>
                <tr>
                    <td>è¨“ç·´ã®å®‰å®šæ€§</td>
                    <td>æ¯”è¼ƒçš„å®‰å®š</td>
                    <td>ã‚„ã‚„ä¸å®‰å®šãªå ´åˆã‚ã‚Š</td>
                </tr>
            </tbody>
        </table>

        <h2>4. Whisper</h2>

        <h3>4.1 OpenAI Whisperã¨ã¯</h3>
        <p>Whisperã¯ã€OpenAIãŒé–‹ç™ºã—ãŸå¤šè¨€èªéŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚68ä¸‡æ™‚é–“ã®å¤šè¨€èªãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã•ã‚Œã¦ãŠã‚Šã€99è¨€èªã®éŸ³å£°èªè­˜ã€éŸ³å£°ç¿»è¨³ã€è¨€èªè­˜åˆ¥ã‚¿ã‚¹ã‚¯ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚</p>

        <h4>Whisperã®ç‰¹å¾´</h4>
        <ul>
            <li><strong>å¤šè¨€èªå¯¾å¿œ</strong>: 99è¨€èªã‚’ã‚µãƒãƒ¼ãƒˆ</li>
            <li><strong>ãƒ­ãƒã‚¹ãƒˆæ€§</strong>: ãƒã‚¤ã‚ºã‚„è¨›ã‚Šã«å¼·ã„</li>
            <li><strong>ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ€§èƒ½</strong>: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãªã—ã§é«˜ç²¾åº¦</li>
            <li><strong>å¤šæ©Ÿèƒ½</strong>: éŸ³å£°èªè­˜ã€ç¿»è¨³ã€è¨€èªè­˜åˆ¥ã‚’çµ±ä¸€çš„ã«å‡¦ç†</li>
            <li><strong>è¤‡æ•°ã®ã‚µã‚¤ã‚º</strong>: tiny, base, small, medium, large</li>
        </ul>

<pre><code class="language-python">import whisper
import torch
import numpy as np

# Whisperãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬çš„ãªä½¿ç”¨
def basic_whisper_usage():
    """Whisperã®åŸºæœ¬çš„ãªä½¿ã„æ–¹"""

    # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ (base, small, medium, large ã‹ã‚‰é¸æŠ)
    model = whisper.load_model("base")

    print(f"ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {model.dims}")
    print(f"å¯¾å¿œè¨€èªæ•°: {len(whisper.tokenizer.LANGUAGES)}")

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨æ–‡å­—èµ·ã“ã—
    # audio = whisper.load_audio("audio.mp3")
    # audio = whisper.pad_or_trim(audio)

    # ãƒ€ãƒŸãƒ¼éŸ³å£°(å®Ÿéš›ã«ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿)
    audio = np.random.randn(16000 * 10).astype(np.float32)  # 10ç§’ã®éŸ³å£°

    # ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã«å¤‰æ›
    mel = whisper.log_mel_spectrogram(torch.from_numpy(audio)).to(model.device)

    # è¨€èªæ¤œå‡º
    _, probs = model.detect_language(mel)
    detected_language = max(probs, key=probs.get)
    print(f"æ¤œå‡ºã•ã‚ŒãŸè¨€èª: {detected_language} (ç¢ºç‡: {probs[detected_language]:.2f})")

    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
    options = whisper.DecodingOptions(
        language="ja",  # æ—¥æœ¬èªã‚’æŒ‡å®š
        task="transcribe",  # transcribe or translate
        fp16=False  # FP16ã‚’ä½¿ç”¨ã™ã‚‹ã‹
    )

    # ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    result = whisper.decode(model, mel, options)

    print(f"æ–‡å­—èµ·ã“ã—çµæœ: {result.text}")
    print(f"å¹³å‡å¯¾æ•°ç¢ºç‡: {result.avg_logprob:.4f}")
    print(f"åœ§ç¸®ç‡: {result.compression_ratio:.2f}")

    return model, result


# ã‚ˆã‚Šé«˜ãƒ¬ãƒ™ãƒ«ãªAPI
def transcribe_audio_file(audio_path, model_size="base"):
    """
    éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—

    Args:
        audio_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        model_size: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º (tiny, base, small, medium, large)
    """
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    model = whisper.load_model(model_size)

    # æ–‡å­—èµ·ã“ã—
    result = model.transcribe(
        audio_path,
        language="ja",  # æ—¥æœ¬èª
        task="transcribe",
        verbose=True,  # é€²æ—è¡¨ç¤º
        temperature=0.0,  # æ¸©åº¦(å¤šæ§˜æ€§ã®åˆ¶å¾¡)
        best_of=5,  # ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã®å€™è£œæ•°
        beam_size=5,  # ãƒ“ãƒ¼ãƒ å¹…
        patience=1.0,  # ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã®å¿è€åº¦
        length_penalty=1.0,  # é•·ã•ãƒšãƒŠãƒ«ãƒ†ã‚£
        compression_ratio_threshold=2.4,  # åœ§ç¸®ç‡ã®é–¾å€¤
        logprob_threshold=-1.0,  # å¯¾æ•°ç¢ºç‡ã®é–¾å€¤
        no_speech_threshold=0.6  # ç„¡éŸ³åˆ¤å®šã®é–¾å€¤
    )

    # çµæœã®è¡¨ç¤º
    print("=" * 50)
    print("æ–‡å­—èµ·ã“ã—çµæœ:")
    print("=" * 50)
    print(result["text"])
    print()

    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå˜ä½ã®çµæœ
    print("ã‚»ã‚°ãƒ¡ãƒ³ãƒˆè©³ç´°:")
    for segment in result["segments"]:
        start = segment["start"]
        end = segment["end"]
        text = segment["text"]
        print(f"[{start:.2f}s - {end:.2f}s] {text}")

    return result


# ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãæ–‡å­—èµ·ã“ã—
def transcribe_with_timestamps(audio_path):
    """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã§æ–‡å­—èµ·ã“ã—"""

    model = whisper.load_model("base")

    # word_timestamps=Trueã§å˜èªãƒ¬ãƒ™ãƒ«ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
    result = model.transcribe(
        audio_path,
        language="ja",
        word_timestamps=True  # å˜èªãƒ¬ãƒ™ãƒ«ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    )

    # å˜èªãƒ¬ãƒ™ãƒ«ã®çµæœã‚’è¡¨ç¤º
    for segment in result["segments"]:
        if "words" in segment:
            for word_info in segment["words"]:
                word = word_info["word"]
                start = word_info["start"]
                end = word_info["end"]
                probability = word_info.get("probability", 0.0)
                print(f"{word:15s} [{start:6.2f}s - {end:6.2f}s] (ç¢ºç‡: {probability:.3f})")

    return result


# å¤šè¨€èªéŸ³å£°ã®å‡¦ç†
def multilingual_transcription(audio_path):
    """å¤šè¨€èªéŸ³å£°ã®å‡¦ç†"""

    model = whisper.load_model("medium")  # å¤šè¨€èªã«ã¯ä¸­å‹ä»¥ä¸Šã‚’æ¨å¥¨

    # è¨€èªã‚’è‡ªå‹•æ¤œå‡º
    result = model.transcribe(
        audio_path,
        task="transcribe",
        language=None  # è‡ªå‹•æ¤œå‡º
    )

    detected_language = result["language"]
    print(f"æ¤œå‡ºã•ã‚ŒãŸè¨€èª: {detected_language}")
    print(f"æ–‡å­—èµ·ã“ã—: {result['text']}")

    # è‹±èªã«ç¿»è¨³
    translation = model.transcribe(
        audio_path,
        task="translate",  # è‹±èªã«ç¿»è¨³
        language=detected_language
    )

    print(f"è‹±è¨³: {translation['text']}")

    return result, translation


# å®Ÿè¡Œä¾‹
if __name__ == "__main__":
    print("Whisperä½¿ç”¨ä¾‹")
    print("=" * 50)

    # åŸºæœ¬çš„ãªä½¿ç”¨æ³•
    model, result = basic_whisper_usage()
    print("âœ“ åŸºæœ¬çš„ãªæ–‡å­—èµ·ã“ã—ãŒå®Œäº†ã—ã¾ã—ãŸ")

    # æ³¨: å®Ÿéš›ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã†å ´åˆã¯ä»¥ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã™
    # result = transcribe_audio_file("sample.mp3", model_size="base")
    # result = transcribe_with_timestamps("sample.mp3")
    # result, translation = multilingual_transcription("sample.mp3")
</code></pre>

        <h3>4.2 Whisperã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h3>
        <p>Whisperã¯Encoder-Decoderã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚Encoderã¯éŸ³éŸ¿ç‰¹å¾´é‡ã‚’å‡¦ç†ã—ã€Decoderã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚ã©ã¡ã‚‰ã‚‚Transformerãƒ™ãƒ¼ã‚¹ã§ã™ã€‚</p>

        <div class="mermaid">
graph LR
    A[éŸ³å£°å…¥åŠ›] --> B[ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ]
    B --> C[Encoder<br/>Transformer]
    C --> D[éŸ³éŸ¿è¡¨ç¾]
    D --> E[Decoder<br/>Transformer]
    E --> F[ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›]

    style A fill:#e1f5ff
    style F fill:#e1f5ff
    style C fill:#fff4e1
    style E fill:#fff4e1
        </div>

<pre><code class="language-python">import torch
import torch.nn as nn
from typing import Optional

class WhisperArchitecture(nn.Module):
    """
    Whisperã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦
    (å®Ÿéš›ã®Whisperã¯ã‚ˆã‚Šè¤‡é›‘ã§ã™ãŒã€ä¸»è¦ãªæ§‹é€ ã‚’ç¤ºã—ã¾ã™)
    """

    def __init__(self,
                 n_mels=80,
                 n_audio_ctx=1500,
                 n_audio_state=512,
                 n_audio_head=8,
                 n_audio_layer=6,
                 n_vocab=51865,
                 n_text_ctx=448,
                 n_text_state=512,
                 n_text_head=8,
                 n_text_layer=6):
        """
        Args:
            n_mels: ãƒ¡ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒãƒ³ã‚¯æ•°
            n_audio_ctx: éŸ³éŸ¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·
            n_audio_state: Encoderã®çŠ¶æ…‹æ¬¡å…ƒ
            n_audio_head: Encoderã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°
            n_audio_layer: Encoderã®å±¤æ•°
            n_vocab: èªå½™ã‚µã‚¤ã‚º
            n_text_ctx: ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·
            n_text_state: Decoderã®çŠ¶æ…‹æ¬¡å…ƒ
            n_text_head: Decoderã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°
            n_text_layer: Decoderã®å±¤æ•°
        """
        super().__init__()

        # Encoder: éŸ³éŸ¿ç‰¹å¾´é‡ã‚’å‡¦ç†
        self.encoder = AudioEncoder(
            n_mels=n_mels,
            n_ctx=n_audio_ctx,
            n_state=n_audio_state,
            n_head=n_audio_head,
            n_layer=n_audio_layer
        )

        # Decoder: ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
        self.decoder = TextDecoder(
            n_vocab=n_vocab,
            n_ctx=n_text_ctx,
            n_state=n_text_state,
            n_head=n_text_head,
            n_layer=n_text_layer
        )

    def forward(self, mel, tokens):
        """
        Args:
            mel: (batch, n_mels, time) ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ 
            tokens: (batch, seq_len) ãƒˆãƒ¼ã‚¯ãƒ³
        """
        # Encoder
        audio_features = self.encoder(mel)

        # Decoder
        logits = self.decoder(tokens, audio_features)

        return logits


class AudioEncoder(nn.Module):
    """Whisper Audio Encoder"""

    def __init__(self, n_mels, n_ctx, n_state, n_head, n_layer):
        super().__init__()

        # ç•³ã¿è¾¼ã¿å±¤ã§ç‰¹å¾´æŠ½å‡º
        self.conv1 = nn.Conv1d(n_mels, n_state, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)

        # Positional embedding
        self.positional_embedding = nn.Parameter(torch.empty(n_ctx, n_state))

        # Transformer layers
        self.blocks = nn.ModuleList([
            TransformerBlock(n_state, n_head) for _ in range(n_layer)
        ])

        self.ln_post = nn.LayerNorm(n_state)

    def forward(self, x):
        """
        Args:
            x: (batch, n_mels, time)
        Returns:
            (batch, time//2, n_state)
        """
        x = F.gelu(self.conv1(x))
        x = F.gelu(self.conv2(x))
        x = x.permute(0, 2, 1)  # (batch, time, n_state)

        # Positional embedding
        x = x + self.positional_embedding[:x.size(1)]

        # Transformer blocks
        for block in self.blocks:
            x = block(x)

        x = self.ln_post(x)
        return x


class TextDecoder(nn.Module):
    """Whisper Text Decoder"""

    def __init__(self, n_vocab, n_ctx, n_state, n_head, n_layer):
        super().__init__()

        # Token embedding
        self.token_embedding = nn.Embedding(n_vocab, n_state)

        # Positional embedding
        self.positional_embedding = nn.Parameter(torch.empty(n_ctx, n_state))

        # Transformer layers (with cross-attention)
        self.blocks = nn.ModuleList([
            DecoderBlock(n_state, n_head) for _ in range(n_layer)
        ])

        self.ln = nn.LayerNorm(n_state)

    def forward(self, tokens, audio_features):
        """
        Args:
            tokens: (batch, seq_len)
            audio_features: (batch, audio_len, n_state)
        Returns:
            (batch, seq_len, n_vocab)
        """
        x = self.token_embedding(tokens)
        x = x + self.positional_embedding[:x.size(1)]

        for block in self.blocks:
            x = block(x, audio_features)

        x = self.ln(x)

        # Weight tying: token embeddingã‚’å†åˆ©ç”¨
        logits = x @ self.token_embedding.weight.T

        return logits


class TransformerBlock(nn.Module):
    """Transformer Encoder Block"""

    def __init__(self, n_state, n_head):
        super().__init__()

        self.attn = nn.MultiheadAttention(n_state, n_head, batch_first=True)
        self.attn_ln = nn.LayerNorm(n_state)

        self.mlp = nn.Sequential(
            nn.Linear(n_state, n_state * 4),
            nn.GELU(),
            nn.Linear(n_state * 4, n_state)
        )
        self.mlp_ln = nn.LayerNorm(n_state)

    def forward(self, x):
        # Self-attention
        attn_out, _ = self.attn(x, x, x)
        x = self.attn_ln(x + attn_out)

        # MLP
        mlp_out = self.mlp(x)
        x = self.mlp_ln(x + mlp_out)

        return x


class DecoderBlock(nn.Module):
    """Transformer Decoder Block (with cross-attention)"""

    def __init__(self, n_state, n_head):
        super().__init__()

        # Self-attention
        self.self_attn = nn.MultiheadAttention(n_state, n_head, batch_first=True)
        self.self_attn_ln = nn.LayerNorm(n_state)

        # Cross-attention
        self.cross_attn = nn.MultiheadAttention(n_state, n_head, batch_first=True)
        self.cross_attn_ln = nn.LayerNorm(n_state)

        # MLP
        self.mlp = nn.Sequential(
            nn.Linear(n_state, n_state * 4),
            nn.GELU(),
            nn.Linear(n_state * 4, n_state)
        )
        self.mlp_ln = nn.LayerNorm(n_state)

    def forward(self, x, audio_features):
        # Self-attention (causal mask)
        attn_out, _ = self.self_attn(x, x, x, need_weights=False, is_causal=True)
        x = self.self_attn_ln(x + attn_out)

        # Cross-attention
        cross_out, _ = self.cross_attn(x, audio_features, audio_features)
        x = self.cross_attn_ln(x + cross_out)

        # MLP
        mlp_out = self.mlp(x)
        x = self.mlp_ln(x + mlp_out)

        return x


# ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ¦‚è¦è¡¨ç¤º
def show_architecture_info():
    """Whisperã®å„ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®æƒ…å ±"""

    models_info = {
        "tiny": {
            "parameters": "39M",
            "n_audio_layer": 4,
            "n_text_layer": 4,
            "n_state": 384,
            "n_head": 6
        },
        "base": {
            "parameters": "74M",
            "n_audio_layer": 6,
            "n_text_layer": 6,
            "n_state": 512,
            "n_head": 8
        },
        "small": {
            "parameters": "244M",
            "n_audio_layer": 12,
            "n_text_layer": 12,
            "n_state": 768,
            "n_head": 12
        },
        "medium": {
            "parameters": "769M",
            "n_audio_layer": 24,
            "n_text_layer": 24,
            "n_state": 1024,
            "n_head": 16
        },
        "large": {
            "parameters": "1550M",
            "n_audio_layer": 32,
            "n_text_layer": 32,
            "n_state": 1280,
            "n_head": 20
        }
    }

    print("Whisper ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºæ¯”è¼ƒ")
    print("=" * 70)
    print(f"{'Model':<10} {'Parameters':<12} {'Layers':<10} {'State':<10} {'Heads':<10}")
    print("=" * 70)

    for model, info in models_info.items():
        print(f"{model:<10} {info['parameters']:<12} "
              f"{info['n_audio_layer']}/{info['n_text_layer']:<10} "
              f"{info['n_state']:<10} {info['n_head']:<10}")

    print()
    print("æ¨å¥¨ç”¨é€”:")
    print("- tiny/base: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ã€ãƒªã‚½ãƒ¼ã‚¹åˆ¶ç´„ç’°å¢ƒ")
    print("- small: ãƒãƒ©ãƒ³ã‚¹å‹ã€ä¸€èˆ¬çš„ãªç”¨é€”")
    print("- medium/large: é«˜ç²¾åº¦ãŒå¿…è¦ãªå ´åˆã€å¤šè¨€èªå‡¦ç†")

show_architecture_info()
</code></pre>

        <h2>5. å®Ÿè·µçš„ãªASRã‚·ã‚¹ãƒ†ãƒ </h2>

        <h3>5.1 Whisperã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</h3>
        <p>Whisperã‚’ç‰¹å®šã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚„è¨€èªã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€ã•ã‚‰ã«é«˜ã„ç²¾åº¦ã‚’å®Ÿç¾ã§ãã¾ã™ã€‚</p>

<pre><code class="language-python">from transformers import WhisperProcessor, WhisperForConditionalGeneration
from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer
from datasets import load_dataset, Audio
import torch

def finetune_whisper_japanese():
    """
    æ—¥æœ¬èªéŸ³å£°èªè­˜ã®ãŸã‚ã®Whisperãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
    """

    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ã‚»ãƒƒã‚µã®ãƒ­ãƒ¼ãƒ‰
    model_name = "openai/whisper-small"
    processor = WhisperProcessor.from_pretrained(model_name, language="Japanese", task="transcribe")
    model = WhisperForConditionalGeneration.from_pretrained(model_name)

    # æ—¥æœ¬èªã«ç‰¹åŒ–ã—ãŸè¨­å®š
    model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(
        language="Japanese",
        task="transcribe"
    )
    model.config.suppress_tokens = []

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™(ä¾‹: Common Voice Japanese)
    # dataset = load_dataset("mozilla-foundation/common_voice_11_0", "ja", split="train[:100]")

    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(å®Ÿéš›ã«ã¯ä¸Šè¨˜ã®ã‚ˆã†ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨)
    def prepare_dataset(batch):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‰å‡¦ç†"""
        # éŸ³å£°ã‚’16kHzã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        audio = batch["audio"]

        # ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã«å¤‰æ›
        batch["input_features"] = processor(
            audio["array"],
            sampling_rate=audio["sampling_rate"],
            return_tensors="pt"
        ).input_features[0]

        # ãƒ©ãƒ™ãƒ«ã®æº–å‚™
        batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids

        return batch

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å‰å‡¦ç†ã‚’é©ç”¨
    # dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)

    # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼
    from dataclasses import dataclass
    from typing import Any, Dict, List, Union

    @dataclass
    class DataCollatorSpeechSeq2SeqWithPadding:
        """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼"""

        processor: Any

        def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
            # å…¥åŠ›ç‰¹å¾´é‡ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            input_features = [{"input_features": feature["input_features"]} for feature in features]
            batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

            # ãƒ©ãƒ™ãƒ«ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            label_features = [{"input_ids": feature["labels"]} for feature in features]
            labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’-100ã«ç½®ãæ›ãˆ(æå¤±è¨ˆç®—ã§ç„¡è¦–ã•ã‚Œã‚‹)
            labels = labels_batch["input_ids"].masked_fill(
                labels_batch.attention_mask.ne(1), -100
            )

            # bos tokenãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯å‰Šé™¤
            if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
                labels = labels[:, 1:]

            batch["labels"] = labels

            return batch

    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

    # è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    import evaluate

    metric = evaluate.load("wer")  # Word Error Rate

    def compute_metrics(pred):
        """è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—"""
        pred_ids = pred.predictions
        label_ids = pred.label_ids

        # -100ã‚’ pad_token_id ã«ç½®ãæ›ãˆ
        label_ids[label_ids == -100] = processor.tokenizer.pad_token_id

        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
        label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)

        # WERè¨ˆç®—
        wer = metric.compute(predictions=pred_str, references=label_str)

        return {"wer": wer}

    # è¨“ç·´è¨­å®š
    training_args = Seq2SeqTrainingArguments(
        output_dir="./whisper-japanese",
        per_device_train_batch_size=8,
        gradient_accumulation_steps=2,
        learning_rate=1e-5,
        warmup_steps=500,
        num_train_epochs=3,
        evaluation_strategy="steps",
        eval_steps=1000,
        save_steps=1000,
        logging_steps=100,
        generation_max_length=225,
        predict_with_generate=True,
        fp16=True,  # æ··åˆç²¾åº¦è¨“ç·´
        push_to_hub=False,
        report_to=["tensorboard"],
        load_best_model_at_end=True,
        metric_for_best_model="wer",
        greater_is_better=False,
    )

    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–
    # trainer = Seq2SeqTrainer(
    #     model=model,
    #     args=training_args,
    #     train_dataset=dataset["train"],
    #     eval_dataset=dataset["test"],
    #     data_collator=data_collator,
    #     compute_metrics=compute_metrics,
    #     tokenizer=processor.feature_extractor,
    # )

    # è¨“ç·´é–‹å§‹
    # trainer.train()

    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    # model.save_pretrained("./whisper-japanese-finetuned")
    # processor.save_pretrained("./whisper-japanese-finetuned")

    print("âœ“ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸ")
    print("å®Ÿéš›ã®è¨“ç·´ã«ã¯Common Voiceãªã©ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")

    return model, processor

# å®Ÿè¡Œä¾‹
model, processor = finetune_whisper_japanese()
</code></pre>

        <h3>5.2 ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°èªè­˜ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³</h3>
        <p>ãƒã‚¤ã‚¯ã‹ã‚‰ã®éŸ³å£°å…¥åŠ›ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§æ–‡å­—èµ·ã“ã—ã™ã‚‹ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">import pyaudio
import numpy as np
import whisper
import queue
import threading
from collections import deque

class RealtimeASR:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°èªè­˜ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, model_name="base", language="ja"):
        """
        Args:
            model_name: Whisperãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚º
            language: èªè­˜è¨€èª
        """
        # Whisperãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        print(f"Whisperãƒ¢ãƒ‡ãƒ« '{model_name}' ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        self.model = whisper.load_model(model_name)
        self.language = language

        # éŸ³å£°è¨­å®š
        self.RATE = 16000  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
        self.CHUNK = 1024  # ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º
        self.CHANNELS = 1  # ãƒ¢ãƒãƒ©ãƒ«
        self.FORMAT = pyaudio.paInt16

        # éŸ³å£°ãƒãƒƒãƒ•ã‚¡
        self.audio_queue = queue.Queue()
        self.audio_buffer = deque(maxlen=30)  # 30ç§’åˆ†ã®ãƒãƒƒãƒ•ã‚¡

        # PyAudioã®åˆæœŸåŒ–
        self.audio = pyaudio.PyAudio()

        # çŠ¶æ…‹ç®¡ç†
        self.is_recording = False
        self.transcription_thread = None

        print("âœ“ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ASRã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")

    def audio_callback(self, in_data, frame_count, time_info, status):
        """éŸ³å£°å…¥åŠ›ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        if self.is_recording:
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
            audio_data = np.frombuffer(in_data, dtype=np.int16)
            self.audio_queue.put(audio_data)

        return (in_data, pyaudio.paContinue)

    def start_recording(self):
        """éŒ²éŸ³é–‹å§‹"""
        self.is_recording = True

        # éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’é–‹ã
        self.stream = self.audio.open(
            format=self.FORMAT,
            channels=self.CHANNELS,
            rate=self.RATE,
            input=True,
            frames_per_buffer=self.CHUNK,
            stream_callback=self.audio_callback
        )

        self.stream.start_stream()

        # æ–‡å­—èµ·ã“ã—ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
        self.transcription_thread = threading.Thread(target=self.transcribe_loop)
        self.transcription_thread.start()

        print("ğŸ¤ éŒ²éŸ³ã‚’é–‹å§‹ã—ã¾ã—ãŸ...")

    def stop_recording(self):
        """éŒ²éŸ³åœæ­¢"""
        self.is_recording = False

        if hasattr(self, 'stream'):
            self.stream.stop_stream()
            self.stream.close()

        if self.transcription_thread:
            self.transcription_thread.join()

        print("â¹ï¸  éŒ²éŸ³ã‚’åœæ­¢ã—ã¾ã—ãŸ")

    def transcribe_loop(self):
        """æ–‡å­—èµ·ã“ã—ãƒ«ãƒ¼ãƒ—"""
        print("ğŸ“ æ–‡å­—èµ·ã“ã—ã‚’é–‹å§‹...")

        while self.is_recording:
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’åé›†(1ç§’åˆ†)
            audio_chunks = []
            for _ in range(int(self.RATE / self.CHUNK)):
                try:
                    chunk = self.audio_queue.get(timeout=0.1)
                    audio_chunks.append(chunk)
                except queue.Empty:
                    continue

            if not audio_chunks:
                continue

            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
            audio_data = np.concatenate(audio_chunks)
            self.audio_buffer.append(audio_data)

            # ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰éŸ³å£°ã‚’å–å¾—(5ç§’åˆ†)
            if len(self.audio_buffer) >= 5:
                # æœ€æ–°ã®5ç§’åˆ†ã‚’ä½¿ç”¨
                audio_segment = np.concatenate(list(self.audio_buffer)[-5:])

                # æ­£è¦åŒ–
                audio_segment = audio_segment.astype(np.float32) / 32768.0

                # æ–‡å­—èµ·ã“ã—
                try:
                    result = self.model.transcribe(
                        audio_segment,
                        language=self.language,
                        task="transcribe",
                        fp16=False,
                        temperature=0.0,
                        no_speech_threshold=0.6
                    )

                    text = result["text"].strip()
                    if text:
                        print(f"èªè­˜çµæœ: {text}")

                except Exception as e:
                    print(f"ã‚¨ãƒ©ãƒ¼: {e}")

    def __del__(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if hasattr(self, 'audio'):
            self.audio.terminate()


# ä½¿ç”¨ä¾‹
def realtime_asr_demo():
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ASRã®ãƒ‡ãƒ¢"""

    # ASRã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    asr = RealtimeASR(model_name="base", language="ja")

    try:
        # éŒ²éŸ³é–‹å§‹
        asr.start_recording()

        # 10ç§’é–“éŒ²éŸ³
        import time
        print("10ç§’é–“è©±ã—ã¦ãã ã•ã„...")
        time.sleep(10)

        # éŒ²éŸ³åœæ­¢
        asr.stop_recording()

    except KeyboardInterrupt:
        print("\nä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        asr.stop_recording()

    print("âœ“ ãƒ‡ãƒ¢ãŒå®Œäº†ã—ã¾ã—ãŸ")


# ãƒãƒƒãƒå‡¦ç†ç‰ˆ(ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰)
def batch_transcribe_with_speaker_diarization(audio_file):
    """
    è©±è€…åˆ†é›¢ã‚’å«ã‚€éŸ³å£°èªè­˜
    (pyannote.audioãªã©ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨)
    """
    import whisper

    # Whisperã§æ–‡å­—èµ·ã“ã—
    model = whisper.load_model("medium")
    result = model.transcribe(
        audio_file,
        language="ja",
        word_timestamps=True
    )

    # è©±è€…åˆ†é›¢(ãƒ€ãƒŸãƒ¼å®Ÿè£…)
    # å®Ÿéš›ã«ã¯pyannote.audioãªã©ã‚’ä½¿ç”¨
    print("=" * 50)
    print("æ–‡å­—èµ·ã“ã—çµæœ(è©±è€…ä»˜ã):")
    print("=" * 50)

    current_speaker = "Speaker 1"
    for i, segment in enumerate(result["segments"]):
        # ç°¡æ˜“çš„ãªè©±è€…åˆ‡ã‚Šæ›¿ãˆåˆ¤å®š(å®Ÿéš›ã«ã¯è©±è€…åˆ†é›¢ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨)
        if i > 0 and segment["start"] - result["segments"][i-1]["end"] > 2.0:
            current_speaker = "Speaker 2" if current_speaker == "Speaker 1" else "Speaker 1"

        start = segment["start"]
        end = segment["end"]
        text = segment["text"]

        print(f"[{current_speaker}] [{start:.2f}s - {end:.2f}s]")
        print(f"  {text}")
        print()

    return result


# æ³¨: å®Ÿéš›ã®å®Ÿè¡Œã«ã¯ pyaudio ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦
# pip install pyaudio
#
# macOSã®å ´åˆ:
# brew install portaudio
# pip install pyaudio

print("ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ASRã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ä¾‹ã‚’è¡¨ç¤ºã—ã¾ã—ãŸ")
print("å®Ÿè¡Œã«ã¯ 'pyaudio' ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™")
</code></pre>

        <h3>5.3 å®Œå…¨ãªASRã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³</h3>
        <p>Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æŒã¤å®Œå…¨ãªéŸ³å£°èªè­˜ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">import gradio as gr
import whisper
import numpy as np
from pathlib import Path

class ASRApplication:
    """Webãƒ™ãƒ¼ã‚¹ã®éŸ³å£°èªè­˜ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""

    def __init__(self):
        """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–"""
        self.models = {}
        self.current_model = None

        # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«
        self.available_models = {
            "tiny": "æœ€é€Ÿ(39M parameters)",
            "base": "é«˜é€Ÿ(74M parameters)",
            "small": "ãƒãƒ©ãƒ³ã‚¹å‹(244M parameters)",
            "medium": "é«˜ç²¾åº¦(769M parameters)",
            "large": "æœ€é«˜ç²¾åº¦(1550M parameters)"
        }

        # å¯¾å¿œè¨€èª
        self.languages = {
            "è‡ªå‹•æ¤œå‡º": None,
            "æ—¥æœ¬èª": "ja",
            "è‹±èª": "en",
            "ä¸­å›½èª": "zh",
            "éŸ“å›½èª": "ko",
            "ã‚¹ãƒšã‚¤ãƒ³èª": "es",
            "ãƒ•ãƒ©ãƒ³ã‚¹èª": "fr",
            "ãƒ‰ã‚¤ãƒ„èª": "de"
        }

    def load_model(self, model_name):
        """ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰(ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ã)"""
        if model_name not in self.models:
            print(f"ãƒ¢ãƒ‡ãƒ« '{model_name}' ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            self.models[model_name] = whisper.load_model(model_name)
            print(f"âœ“ ãƒ¢ãƒ‡ãƒ« '{model_name}' ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ")

        self.current_model = self.models[model_name]
        return self.current_model

    def transcribe_audio(self, audio_file, model_name, language, task,
                        include_timestamps, beam_size, temperature):
        """
        éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—

        Args:
            audio_file: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            model_name: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
            language: èªè­˜è¨€èª
            task: transcribe or translate
            include_timestamps: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å«ã‚ã‚‹ã‹
            beam_size: ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã®å¹…
            temperature: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦
        """
        if audio_file is None:
            return "éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", ""

        try:
            # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
            model = self.load_model(model_name)

            # æ–‡å­—èµ·ã“ã—
            result = model.transcribe(
                audio_file,
                language=self.languages.get(language),
                task=task,
                beam_size=beam_size,
                temperature=temperature,
                word_timestamps=include_timestamps
            )

            # åŸºæœ¬çš„ãªæ–‡å­—èµ·ã“ã—çµæœ
            transcription = result["text"]

            # è©³ç´°æƒ…å ±
            details = self._format_details(result, include_timestamps)

            return transcription, details

        except Exception as e:
            return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", ""

    def _format_details(self, result, include_timestamps):
        """è©³ç´°æƒ…å ±ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        details = []

        # æ¤œå‡ºè¨€èª
        if "language" in result:
            details.append(f"æ¤œå‡ºè¨€èª: {result['language']}")

        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±
        if include_timestamps and "segments" in result:
            details.append("\n" + "=" * 50)
            details.append("ã‚»ã‚°ãƒ¡ãƒ³ãƒˆè©³ç´°:")
            details.append("=" * 50)

            for i, segment in enumerate(result["segments"], 1):
                start = segment["start"]
                end = segment["end"]
                text = segment["text"]

                details.append(f"\n[{i}] {start:.2f}s - {end:.2f}s")
                details.append(f"    {text}")

                # å˜èªãƒ¬ãƒ™ãƒ«ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
                if "words" in segment:
                    details.append("    å˜èª:")
                    for word_info in segment["words"]:
                        word = word_info["word"]
                        w_start = word_info["start"]
                        w_end = word_info["end"]
                        details.append(f"      - {word:20s} [{w_start:.2f}s - {w_end:.2f}s]")

        return "\n".join(details)

    def create_interface(self):
        """Gradio ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ä½œæˆ"""

        with gr.Blocks(title="AIéŸ³å£°èªè­˜ã‚·ã‚¹ãƒ†ãƒ ") as interface:
            gr.Markdown(
                """
                # ğŸ™ï¸ AIéŸ³å£°èªè­˜ã‚·ã‚¹ãƒ†ãƒ 

                Whisperã‚’ä½¿ç”¨ã—ãŸé«˜ç²¾åº¦ãªéŸ³å£°èªè­˜ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚
                éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€ãƒã‚¤ã‚¯ã§éŒ²éŸ³ã—ã¦æ–‡å­—èµ·ã“ã—ã‚’è¡Œã„ã¾ã™ã€‚
                """
            )

            with gr.Row():
                with gr.Column(scale=1):
                    # å…¥åŠ›ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«
                    audio_input = gr.Audio(
                        sources=["upload", "microphone"],
                        type="filepath",
                        label="éŸ³å£°å…¥åŠ›"
                    )

                    model_selector = gr.Dropdown(
                        choices=list(self.available_models.keys()),
                        value="base",
                        label="ãƒ¢ãƒ‡ãƒ«é¸æŠ",
                        info="ç²¾åº¦ã¨é€Ÿåº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’é¸æŠ"
                    )

                    language_selector = gr.Dropdown(
                        choices=list(self.languages.keys()),
                        value="è‡ªå‹•æ¤œå‡º",
                        label="è¨€èª"
                    )

                    task_selector = gr.Radio(
                        choices=["transcribe", "translate"],
                        value="transcribe",
                        label="ã‚¿ã‚¹ã‚¯",
                        info="transcribe: åŒã˜è¨€èªã§æ–‡å­—èµ·ã“ã— / translate: è‹±èªã«ç¿»è¨³"
                    )

                    with gr.Accordion("è©³ç´°è¨­å®š", open=False):
                        include_timestamps = gr.Checkbox(
                            label="ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å«ã‚ã‚‹",
                            value=True
                        )

                        beam_size = gr.Slider(
                            minimum=1,
                            maximum=10,
                            value=5,
                            step=1,
                            label="ãƒ“ãƒ¼ãƒ ã‚µã‚¤ã‚º",
                            info="å¤§ãã„ã»ã©ç²¾åº¦å‘ä¸Šã€è¨ˆç®—æ™‚é–“å¢—åŠ "
                        )

                        temperature = gr.Slider(
                            minimum=0.0,
                            maximum=1.0,
                            value=0.0,
                            step=0.1,
                            label="æ¸©åº¦",
                            info="0: æ±ºå®šçš„ã€>0: ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚ã‚Š"
                        )

                    transcribe_btn = gr.Button("æ–‡å­—èµ·ã“ã—é–‹å§‹", variant="primary")

                with gr.Column(scale=2):
                    # å‡ºåŠ›
                    transcription_output = gr.Textbox(
                        label="æ–‡å­—èµ·ã“ã—çµæœ",
                        lines=5,
                        max_lines=10
                    )

                    details_output = gr.Textbox(
                        label="è©³ç´°æƒ…å ±",
                        lines=15,
                        max_lines=30
                    )

            # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©
            transcribe_btn.click(
                fn=self.transcribe_audio,
                inputs=[
                    audio_input,
                    model_selector,
                    language_selector,
                    task_selector,
                    include_timestamps,
                    beam_size,
                    temperature
                ],
                outputs=[transcription_output, details_output]
            )

            # ä½¿ç”¨ä¾‹
            gr.Markdown(
                """
                ## ä½¿ã„æ–¹

                1. **éŸ³å£°å…¥åŠ›**: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã§éŒ²éŸ³
                2. **ãƒ¢ãƒ‡ãƒ«é¸æŠ**: ç”¨é€”ã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’é¸æŠ
                   - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†: tiny/base
                   - ä¸€èˆ¬çš„ãªç”¨é€”: small
                   - é«˜ç²¾åº¦ãŒå¿…è¦: medium/large
                3. **è¨€èªé¸æŠ**: è‡ªå‹•æ¤œå‡ºã¾ãŸã¯ç‰¹å®šã®è¨€èªã‚’é¸æŠ
                4. **æ–‡å­—èµ·ã“ã—é–‹å§‹**: ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦å‡¦ç†é–‹å§‹

                ## ãƒ¢ãƒ‡ãƒ«æƒ…å ±

                | ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | é€Ÿåº¦ | ç²¾åº¦ | æ¨å¥¨ç”¨é€” |
                |--------|--------------|------|------|----------|
                | tiny   | 39M          | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜†â˜†â˜† | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç† |
                | base   | 74M          | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜†â˜† | é«˜é€Ÿå‡¦ç† |
                | small  | 244M         | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜…â˜† | ãƒãƒ©ãƒ³ã‚¹å‹ |
                | medium | 769M         | â˜…â˜…â˜†â˜†â˜† | â˜…â˜…â˜…â˜…â˜… | é«˜ç²¾åº¦ |
                | large  | 1550M        | â˜…â˜†â˜†â˜†â˜† | â˜…â˜…â˜…â˜…â˜… | æœ€é«˜ç²¾åº¦ |
                """
            )

        return interface

    def launch(self, share=False):
        """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®èµ·å‹•"""
        interface = self.create_interface()
        interface.launch(share=share)


# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œ
if __name__ == "__main__":
    app = ASRApplication()

    print("=" * 50)
    print("AIéŸ³å£°èªè­˜ã‚·ã‚¹ãƒ†ãƒ ã‚’èµ·å‹•ä¸­...")
    print("=" * 50)

    # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•
    # app.launch(share=False)

    # æ³¨: Gradioã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦
    # pip install gradio

    print("âœ“ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸ")
    print("å®Ÿè¡Œã«ã¯ 'gradio' ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™")
    print("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install gradio")
</code></pre>

        <h2>ç·´ç¿’å•é¡Œ</h2>

        <details>
            <summary><strong>å•é¡Œ1: CTCæå¤±ã®ç†è§£</strong></summary>
            <p><strong>å•é¡Œ</strong>: CTCãŒã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆæƒ…å ±ãªã—ã§å­¦ç¿’ã§ãã‚‹ç†ç”±ã‚’èª¬æ˜ã—ã€Blankãƒˆãƒ¼ã‚¯ãƒ³ã®å½¹å‰²ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

            <p><strong>è§£ç­”ä¾‹</strong>:</p>
            <p>CTCã¯å…¨ã¦ã®å¯èƒ½ãªã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ‘ã‚¹ã®ç¢ºç‡ã‚’å‘¨è¾ºåŒ–ã™ã‚‹ã“ã¨ã§ã€æ˜ç¤ºçš„ãªã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆæƒ…å ±ãªã—ã«å­¦ç¿’ã§ãã¾ã™ã€‚å…·ä½“çš„ã«ã¯:</p>
            <ul>
                <li><strong>å¤šå¯¾ä¸€ãƒãƒƒãƒ”ãƒ³ã‚°</strong>: è¤‡æ•°ã®éŸ³éŸ¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒåŒã˜æ–‡å­—ã«å¯¾å¿œå¯èƒ½</li>
                <li><strong>Blankãƒˆãƒ¼ã‚¯ãƒ³ã®å½¹å‰²</strong>:
                    <ul>
                        <li>æ–‡å­—ã¨æ–‡å­—ã®é–“ã®å¢ƒç•Œã‚’è¡¨ç¾</li>
                        <li>åŒã˜æ–‡å­—ã®é€£ç¶šã‚’åŒºåˆ¥(ä¾‹: "hello" â†’ "hhe-ll-oo")</li>
                        <li>å‡ºåŠ›ãŒãªã„ä½ç½®ã‚’è¡¨ç¾</li>
                    </ul>
                </li>
                <li><strong>Forward-Backwardã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </strong>: å…¨ã¦ã®ãƒ‘ã‚¹ã‚’åŠ¹ç‡çš„ã«è¨ˆç®—</li>
            </ul>
        </details>

        <details>
            <summary><strong>å•é¡Œ2: Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¨CTCã®é•ã„</strong></summary>
            <p><strong>å•é¡Œ</strong>: Attention-basedãƒ¢ãƒ‡ãƒ«ã¨CTCãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã®ä¸»ãªé•ã„ã‚’ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨å­¦ç¿’ã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

            <p><strong>è§£ç­”ä¾‹</strong>:</p>

            <table>
                <thead>
                    <tr>
                        <th>è¦³ç‚¹</th>
                        <th>CTC</th>
                        <th>Attention-based</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</td>
                        <td>Encoder + ç·šå½¢åˆ†é¡å™¨</td>
                        <td>Encoder + Attention + Decoder</td>
                    </tr>
                    <tr>
                        <td>ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ</td>
                        <td>Monotonic(å˜èª¿)</td>
                        <td>æŸ”è»Ÿ(Attentionã§æ±ºå®š)</td>
                    </tr>
                    <tr>
                        <td>è¨€èªãƒ¢ãƒ‡ãƒ«</td>
                        <td>æ¡ä»¶ä»˜ãç‹¬ç«‹(å¤–éƒ¨LMå¿…è¦)</td>
                        <td>Decoderã«çµ±åˆ</td>
                    </tr>
                    <tr>
                        <td>è¨ˆç®—ã‚³ã‚¹ãƒˆ</td>
                        <td>ä½ã„</td>
                        <td>é«˜ã„</td>
                    </tr>
                    <tr>
                        <td>é•·è·é›¢ä¾å­˜</td>
                        <td>è‹¦æ‰‹</td>
                        <td>å¾—æ„</td>
                    </tr>
                </tbody>
            </table>
        </details>

        <details>
            <summary><strong>å•é¡Œ3: RNN-Tã®å®Ÿè£…</strong></summary>
            <p><strong>å•é¡Œ</strong>: RNN-Transducerã®3ã¤ã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ(Encoder, Prediction Network, Joint Network)ã®å½¹å‰²ã‚’èª¬æ˜ã—ã€ç°¡å˜ãªå®Ÿè£…ã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚</p>

            <p><strong>è§£ç­”</strong>: æœ¬æ–‡ä¸­ã®RNN-Tã®å®Ÿè£…ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å½¹å‰²:</p>
            <ul>
                <li><strong>Encoder (Transcription Network)</strong>: éŸ³éŸ¿ç‰¹å¾´é‡ã‚’é«˜ãƒ¬ãƒ™ãƒ«è¡¨ç¾ã«å¤‰æ›</li>
                <li><strong>Prediction Network</strong>: è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æ©Ÿèƒ½ã€å‰ã®å‡ºåŠ›ã‹ã‚‰æ¬¡ã®äºˆæ¸¬ã‚’ç”Ÿæˆ</li>
                <li><strong>Joint Network</strong>: Encoderã¨Prediction Networkã®å‡ºåŠ›ã‚’çµåˆã—ã€æœ€çµ‚çš„ãªå‡ºåŠ›ç¢ºç‡ã‚’è¨ˆç®—</li>
            </ul>
        </details>

        <details>
            <summary><strong>å•é¡Œ4: Whisperã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</strong></summary>
            <p><strong>å•é¡Œ</strong>: Whisperã‚’ç‰¹å®šã®ãƒ‰ãƒ¡ã‚¤ãƒ³(ä¾‹: åŒ»ç™‚ã€æ³•å¾‹)ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹éš›ã®ä¸»ãªè€ƒæ…®ç‚¹ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

            <p><strong>è§£ç­”ä¾‹</strong>:</p>
            <ol>
                <li><strong>ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</strong>:
                    <ul>
                        <li>ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’åé›†</li>
                        <li>å°‚é–€ç”¨èªã®æ­£ç¢ºãªè»¢å†™</li>
                        <li>å¤šæ§˜ãªè©±è€…ã¨éŸ³éŸ¿æ¡ä»¶</li>
                    </ul>
                </li>
                <li><strong>èªå½™ã®æ‹¡å¼µ</strong>:
                    <ul>
                        <li>ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®ç”¨èªã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«è¿½åŠ </li>
                        <li>ç•¥èªã‚„å°‚é–€è¡¨è¨˜ã®å‡¦ç†</li>
                    </ul>
                </li>
                <li><strong>å­¦ç¿’ç‡ã¨æ­£å‰‡åŒ–</strong>:
                    <ul>
                        <li>ä½ã„å­¦ç¿’ç‡(1e-5ç¨‹åº¦)ã§æ…é‡ã«å­¦ç¿’</li>
                        <li>éå­¦ç¿’é˜²æ­¢ã®ãŸã‚ã®ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ</li>
                    </ul>
                </li>
                <li><strong>è©•ä¾¡</strong>:
                    <ul>
                        <li>ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§WERè©•ä¾¡</li>
                        <li>å°‚é–€ç”¨èªã®èªè­˜ç²¾åº¦ã‚’å€‹åˆ¥ã«è©•ä¾¡</li>
                    </ul>
                </li>
            </ol>
        </details>

        <details>
            <summary><strong>å•é¡Œ5: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ASRã®æœ€é©åŒ–</strong></summary>
            <p><strong>å•é¡Œ</strong>: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°èªè­˜ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã„ã¦ã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã¨ç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æœ€é©åŒ–ã™ã‚‹æ–¹æ³•ã‚’3ã¤æŒ™ã’ã¦ãã ã•ã„ã€‚</p>

            <p><strong>è§£ç­”ä¾‹</strong>:</p>
            <ol>
                <li><strong>ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®é¸æŠ</strong>:
                    <ul>
                        <li>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ã«ã¯è»½é‡ãƒ¢ãƒ‡ãƒ«(tiny/base)ã‚’ä½¿ç”¨</li>
                        <li>å¿…è¦ã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«è’¸ç•™ã§å°å‹åŒ–</li>
                    </ul>
                </li>
                <li><strong>ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†</strong>:
                    <ul>
                        <li>RNN-Tãªã©ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½¿ç”¨</li>
                        <li>ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã¨èªè­˜ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹èª¿æ•´</li>
                        <li>ãƒ«ãƒƒã‚¯ã‚¢ãƒ˜ãƒƒãƒ‰ã®åˆ¶é™</li>
                    </ul>
                </li>
                <li><strong>ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æœ€é©åŒ–</strong>:
                    <ul>
                        <li>GPU/TPUã®æ´»ç”¨</li>
                        <li>é‡å­åŒ–(INT8ãªã©)ã§æ¨è«–ã‚’é«˜é€ŸåŒ–</li>
                        <li>ãƒãƒƒãƒå‡¦ç†ã®æ´»ç”¨</li>
                    </ul>
                </li>
            </ol>
        </details>

        <h2>ã¾ã¨ã‚</h2>

        <p>æœ¬ç« ã§ã¯ã€æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹éŸ³å£°èªè­˜ã®ä¸»è¦æŠ€è¡“ã‚’å­¦ã³ã¾ã—ãŸ:</p>

        <ul>
            <li><strong>CTC</strong>: ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ•ãƒªãƒ¼ãªå­¦ç¿’ã‚’å¯èƒ½ã«ã—ã€ã‚·ãƒ³ãƒ—ãƒ«ã§åŠ¹ç‡çš„</li>
            <li><strong>Attention-based Models</strong>: æŸ”è»Ÿãªã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã¨æ–‡è„ˆæƒ…å ±ã®æ´»ç”¨</li>
            <li><strong>RNN-Transducer</strong>: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ASRã«æœ€é©ã€è¨€èªãƒ¢ãƒ‡ãƒ«çµ±åˆ</li>
            <li><strong>Whisper</strong>: å¤šè¨€èªå¯¾å¿œã€é«˜ã„ãƒ­ãƒã‚¹ãƒˆæ€§ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ€§èƒ½</li>
            <li><strong>å®Ÿè·µ</strong>: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ã€å®Œå…¨ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³</li>
        </ul>

        <p>ã“ã‚Œã‚‰ã®æŠ€è¡“ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€æ§˜ã€…ãªã‚·ãƒŠãƒªã‚ªã«å¯¾å¿œã—ãŸé«˜ç²¾åº¦ãªéŸ³å£°èªè­˜ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚æ¬¡ç« ã§ã¯ã€éŸ³å£°åˆæˆ(TTS)ã¨éŸ³å£°å¤‰æ›ã«ã¤ã„ã¦å­¦ã³ã¾ã™ã€‚</p>

        <h2>å‚è€ƒæ–‡çŒ®</h2>

        <ul>
            <li>Graves, A., et al. (2006). "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks"</li>
            <li>Chan, W., et al. (2016). "Listen, Attend and Spell"</li>
            <li>Vaswani, A., et al. (2017). "Attention Is All You Need"</li>
            <li>Graves, A. (2012). "Sequence Transduction with Recurrent Neural Networks"</li>
            <li>Radford, A., et al. (2022). "Robust Speech Recognition via Large-Scale Weak Supervision" (Whisper paper)</li>
            <li>Watanabe, S., et al. (2017). "Hybrid CTC/Attention Architecture for End-to-End Speech Recognition"</li>
            <li>PyTorch Documentation: <a href="https://pytorch.org/docs/stable/nn.html#ctcloss">nn.CTCLoss</a></li>
            <li>OpenAI Whisper: <a href="https://github.com/openai/whisper">GitHub Repository</a></li>
            <li>Hugging Face Transformers: <a href="https://huggingface.co/docs/transformers/model_doc/whisper">Whisper Documentation</a></li>
        </ul>

        <div class="navigation">
            <a href="chapter2-feature-extraction.html" class="nav-button">â† ç¬¬2ç« : éŸ³éŸ¿ç‰¹å¾´é‡æŠ½å‡º</a>
            <a href="index.html" class="nav-button">ç›®æ¬¡ã«æˆ»ã‚‹</a>
            <a href="chapter4-tts-voice-conversion.html" class="nav-button">ç¬¬4ç« : éŸ³å£°åˆæˆã¨éŸ³å£°å¤‰æ› â†’</a>
        </div>
    </div>

    <footer>
        <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</body>
</html>
