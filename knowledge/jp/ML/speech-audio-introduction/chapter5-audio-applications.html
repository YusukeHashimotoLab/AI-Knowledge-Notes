<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第5章:音声・音響アプリケーション - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>第5章:音声・音響アプリケーション</h1>
            <p class="subtitle">実世界への応用 - 話者認識・感情認識・音声強調・音楽情報処理</p>
            <div class="meta">
                <span class="meta-item">📖 読了時間: 30-35分</span>
                <span class="meta-item">📊 難易度: 上級</span>
                <span class="meta-item">💻 コード例: 8個</span>
                <span class="meta-item">📝 演習問題: 5問</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>学習目標</h2>
<p>この章を読むことで、以下を習得できます：</p>
<ul>
<li>✅ 話者認識と話者検証の違いを理解し、実装できる</li>
<li>✅ i-vectorとx-vectorによる話者埋め込みを使用できる</li>
<li>✅ 音声感情認識システムを構築できる</li>
<li>✅ 音声強調とノイズ除去手法を実装できる</li>
<li>✅ 音楽情報処理の基礎技術を理解する</li>
<li>✅ エンドツーエンドの音声AIアプリケーションを開発できる</li>
</ul>

<hr>

<h2>5.1 話者認識・検証</h2>

<h3>話者認識の概要</h3>
<p><strong>話者認識（Speaker Recognition）</strong>は、音声から話者を特定する技術です。主に以下の2つに分類されます：</p>

<table>
<thead>
<tr>
<th>タスク</th>
<th>説明</th>
<th>例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>話者識別</strong><br>(Speaker Identification)</td>
<td>複数の候補から話者を特定</td>
<td>「この音声は誰のものか？」</td>
</tr>
<tr>
<td><strong>話者検証</strong><br>(Speaker Verification)</td>
<td>話者が本人かどうかを確認</td>
<td>「この音声は山田さんか？」</td>
</tr>
</tbody>
</table>

<h3>話者認識のアプローチ</h3>

<div class="mermaid">
graph TD
    A[音声入力] --> B[特徴抽出]
    B --> C{手法選択}
    C --> D[i-vector]
    C --> E[x-vector]
    C --> F[Deep Speaker]
    D --> G[話者埋め込み]
    E --> G
    F --> G
    G --> H[分類/検証]
    H --> I[話者ID]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#e3f2fd
    style F fill:#e3f2fd
    style G fill:#e8f5e9
    style H fill:#fce4ec
    style I fill:#c8e6c9
</div>

<h3>実装例：基本的な話者認識</h3>

<pre><code class="language-python">import numpy as np
import librosa
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import warnings
warnings.filterwarnings('ignore')

# 話者の音声特徴を抽出する関数
def extract_speaker_features(audio_path, n_mfcc=20):
    """
    話者認識用の特徴量を抽出

    Parameters:
    -----------
    audio_path : str
        音声ファイルのパス
    n_mfcc : int
        MFCCの次元数

    Returns:
    --------
    features : np.ndarray
        統計的特徴量ベクトル
    """
    # 音声読み込み
    y, sr = librosa.load(audio_path, sr=16000)

    # MFCC抽出
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)

    # Delta MFCC (1次微分)
    mfcc_delta = librosa.feature.delta(mfcc)

    # Delta-Delta MFCC (2次微分)
    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)

    # 統計量を計算（平均と標準偏差）
    features = np.concatenate([
        np.mean(mfcc, axis=1),
        np.std(mfcc, axis=1),
        np.mean(mfcc_delta, axis=1),
        np.std(mfcc_delta, axis=1),
        np.mean(mfcc_delta2, axis=1),
        np.std(mfcc_delta2, axis=1)
    ])

    return features

# サンプルデータの生成（実際にはデータセットを使用）
def generate_sample_speaker_data(n_speakers=5, n_samples_per_speaker=20):
    """
    デモ用の話者データを生成
    """
    np.random.seed(42)
    X = []
    y = []

    for speaker_id in range(n_speakers):
        # 各話者に特有の特徴を持つデータを生成
        speaker_mean = np.random.randn(120) * 0.5 + speaker_id

        for _ in range(n_samples_per_speaker):
            # ノイズを加えてバリエーションを作成
            sample = speaker_mean + np.random.randn(120) * 0.3
            X.append(sample)
            y.append(speaker_id)

    return np.array(X), np.array(y)

# データ生成
X, y = generate_sample_speaker_data(n_speakers=5, n_samples_per_speaker=20)

# 訓練・テストデータ分割
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# 特徴量の標準化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# SVMで話者識別モデルを訓練
model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True)
model.fit(X_train_scaled, y_train)

# 評価
y_pred = model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)

print("=== 話者識別システム ===")
print(f"話者数: {len(np.unique(y))}")
print(f"訓練サンプル数: {len(X_train)}")
print(f"テストサンプル数: {len(X_test)}")
print(f"特徴量次元: {X.shape[1]}")
print(f"\n識別精度: {accuracy:.3f}")
print(f"\n詳細レポート:")
print(classification_report(y_test, y_pred,
                          target_names=[f'Speaker {i}' for i in range(5)]))

# 混同行列の可視化
from sklearn.metrics import confusion_matrix
import seaborn as sns

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=[f'S{i}' for i in range(5)],
            yticklabels=[f'S{i}' for i in range(5)])
plt.xlabel('予測話者')
plt.ylabel('真の話者')
plt.title('話者識別の混同行列', fontsize=14)
plt.tight_layout()
plt.show()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== 話者識別システム ===
話者数: 5
訓練サンプル数: 70
テストサンプル数: 30
特徴量次元: 120

識別精度: 0.967

詳細レポート:
              precision    recall  f1-score   support

   Speaker 0       1.00      1.00      1.00         6
   Speaker 1       1.00      0.83      0.91         6
   Speaker 2       0.86      1.00      0.92         6
   Speaker 3       1.00      1.00      1.00         6
   Speaker 4       1.00      1.00      1.00         6
</code></pre>

<h3>x-vectorによる話者埋め込み</h3>

<p><strong>x-vector</strong>は、深層ニューラルネットワークを使用して話者の特徴を固定長ベクトルに埋め込む手法です。</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class XVectorNetwork(nn.Module):
    """
    x-vector抽出ネットワーク

    アーキテクチャ:
    - TDNN (Time Delay Neural Network) layers
    - Statistics pooling
    - Embedding layers
    """
    def __init__(self, input_dim=40, embedding_dim=512):
        super(XVectorNetwork, self).__init__()

        # TDNN layers
        self.tdnn1 = nn.Conv1d(input_dim, 512, kernel_size=5, dilation=1)
        self.tdnn2 = nn.Conv1d(512, 512, kernel_size=3, dilation=2)
        self.tdnn3 = nn.Conv1d(512, 512, kernel_size=3, dilation=3)
        self.tdnn4 = nn.Conv1d(512, 512, kernel_size=1, dilation=1)
        self.tdnn5 = nn.Conv1d(512, 1500, kernel_size=1, dilation=1)

        # Statistics pooling後の次元: 1500 * 2 = 3000
        # Segment-level layers
        self.segment1 = nn.Linear(3000, embedding_dim)
        self.segment2 = nn.Linear(embedding_dim, embedding_dim)

        # Batch normalization
        self.bn1 = nn.BatchNorm1d(512)
        self.bn2 = nn.BatchNorm1d(512)
        self.bn3 = nn.BatchNorm1d(512)
        self.bn4 = nn.BatchNorm1d(512)
        self.bn5 = nn.BatchNorm1d(1500)

    def forward(self, x):
        """
        Forward pass

        Parameters:
        -----------
        x : torch.Tensor
            入力特徴量 (batch, features, time)

        Returns:
        --------
        embedding : torch.Tensor
            話者埋め込みベクトル (batch, embedding_dim)
        """
        # TDNN layers
        x = F.relu(self.bn1(self.tdnn1(x)))
        x = F.relu(self.bn2(self.tdnn2(x)))
        x = F.relu(self.bn3(self.tdnn3(x)))
        x = F.relu(self.bn4(self.tdnn4(x)))
        x = F.relu(self.bn5(self.tdnn5(x)))

        # Statistics pooling: mean + std
        mean = torch.mean(x, dim=2)
        std = torch.std(x, dim=2)
        stats = torch.cat([mean, std], dim=1)

        # Segment-level layers
        x = F.relu(self.segment1(stats))
        embedding = self.segment2(x)

        return embedding

# モデルの初期化
model = XVectorNetwork(input_dim=40, embedding_dim=512)
print("=== x-vector ネットワーク ===")
print(f"総パラメータ数: {sum(p.numel() for p in model.parameters()):,}")

# サンプル入力でテスト
batch_size = 4
n_features = 40
n_frames = 100

sample_input = torch.randn(batch_size, n_features, n_frames)
with torch.no_grad():
    embeddings = model(sample_input)

print(f"\n入力形状: {sample_input.shape}")
print(f"埋め込み形状: {embeddings.shape}")
print(f"埋め込みベクトルのサンプル:")
print(embeddings[0, :10])
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== x-vector ネットワーク ===
総パラメータ数: 5,358,336

入力形状: torch.Size([4, 40, 100])
埋め込み形状: torch.Size([4, 512])
埋め込みベクトルのサンプル:
tensor([-0.2156,  0.1834, -0.0923,  0.3421, -0.1567,  0.2891, -0.0456,  0.1234,
        -0.3012,  0.0789])
</code></pre>

<h3>話者検証システム</h3>

<pre><code class="language-python">from scipy.spatial.distance import cosine

class SpeakerVerification:
    """
    話者検証システム
    埋め込みベクトル間の類似度を計算して本人確認
    """
    def __init__(self, threshold=0.5):
        self.threshold = threshold
        self.enrolled_speakers = {}

    def enroll_speaker(self, speaker_id, embedding):
        """
        話者を登録

        Parameters:
        -----------
        speaker_id : str
            話者ID
        embedding : np.ndarray
            話者の埋め込みベクトル
        """
        self.enrolled_speakers[speaker_id] = embedding

    def verify(self, speaker_id, test_embedding):
        """
        話者を検証

        Parameters:
        -----------
        speaker_id : str
            検証する話者ID
        test_embedding : np.ndarray
            テスト音声の埋め込みベクトル

        Returns:
        --------
        is_verified : bool
            本人かどうか
        similarity : float
            類似度スコア
        """
        if speaker_id not in self.enrolled_speakers:
            raise ValueError(f"Speaker {speaker_id} is not enrolled")

        enrolled_embedding = self.enrolled_speakers[speaker_id]

        # コサイン類似度を計算（距離の補数）
        similarity = 1 - cosine(enrolled_embedding, test_embedding)

        is_verified = similarity > self.threshold

        return is_verified, similarity

# デモンストレーション
np.random.seed(42)

# 話者検証システムの初期化
verifier = SpeakerVerification(threshold=0.7)

# 話者を登録
speaker_a_embedding = np.random.randn(512)
speaker_b_embedding = np.random.randn(512)

verifier.enroll_speaker("Alice", speaker_a_embedding)
verifier.enroll_speaker("Bob", speaker_b_embedding)

print("=== 話者検証システム ===")
print(f"登録話者: {list(verifier.enrolled_speakers.keys())}")
print(f"閾値: {verifier.threshold}")

# テストケース1: Aliceの本人音声（類似度高い）
test_alice_genuine = speaker_a_embedding + np.random.randn(512) * 0.1
is_verified, similarity = verifier.verify("Alice", test_alice_genuine)
print(f"\nテスト1 - Alice（本人）:")
print(f"  検証結果: {'✓ 承認' if is_verified else '✗ 拒否'}")
print(f"  類似度: {similarity:.3f}")

# テストケース2: Aliceになりすまし（Bobの音声）
is_verified, similarity = verifier.verify("Alice", speaker_b_embedding)
print(f"\nテスト2 - Alice（なりすまし）:")
print(f"  検証結果: {'✓ 承認' if is_verified else '✗ 拒否'}")
print(f"  類似度: {similarity:.3f}")

# テストケース3: Bobの本人音声
test_bob_genuine = speaker_b_embedding + np.random.randn(512) * 0.1
is_verified, similarity = verifier.verify("Bob", test_bob_genuine)
print(f"\nテスト3 - Bob（本人）:")
print(f"  検証結果: {'✓ 承認' if is_verified else '✗ 拒否'}")
print(f"  類似度: {similarity:.3f}")
</code></pre>

<blockquote>
<p><strong>重要</strong>: 実際のシステムでは、複数の登録音声を平均化したり、より高度な類似度計算（PLDA: Probabilistic Linear Discriminant Analysis）を使用します。</p>
</blockquote>

<hr>

<h2>5.2 音声感情認識</h2>

<h3>音声感情認識とは</h3>
<p><strong>音声感情認識（Speech Emotion Recognition, SER）</strong>は、音声から話者の感情状態を推定する技術です。</p>

<h3>感情認識のための特徴量</h3>

<table>
<thead>
<tr>
<th>特徴量</th>
<th>説明</th>
<th>感情との関連</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>韻律特徴</strong></td>
<td>ピッチ、エネルギー、話速</td>
<td>怒り→高ピッチ、悲しみ→低エネルギー</td>
</tr>
<tr>
<td><strong>音響特徴</strong></td>
<td>MFCC、スペクトル</td>
<td>声質の変化を捉える</td>
</tr>
<tr>
<td><strong>時間特徴</strong></td>
<td>発話時間、ポーズ</td>
<td>緊張→早口、悲しみ→長いポーズ</td>
</tr>
</tbody>
</table>

<h3>主要な感情データセット</h3>

<table>
<thead>
<tr>
<th>データセット</th>
<th>説明</th>
<th>感情カテゴリ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>RAVDESS</strong></td>
<td>演技による感情音声</td>
<td>8感情（喜び、悲しみ、怒り、恐怖など）</td>
</tr>
<tr>
<td><strong>IEMOCAP</strong></td>
<td>対話形式の感情音声</td>
<td>5感情 + 次元モデル（覚醒度、好意度）</td>
</tr>
<tr>
<td><strong>EMO-DB</strong></td>
<td>ドイツ語の感情音声</td>
<td>7感情</td>
</tr>
</tbody>
</table>

<h3>実装例：感情認識システム</h3>

<pre><code class="language-python">import numpy as np
import librosa
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns

def extract_emotion_features(audio_path):
    """
    感情認識用の包括的な特徴量を抽出

    Returns:
    --------
    features : np.ndarray
        特徴量ベクトル
    """
    y, sr = librosa.load(audio_path, sr=22050)

    features = []

    # 1. MFCC（音響特徴）
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
    features.extend(np.mean(mfcc, axis=1))
    features.extend(np.std(mfcc, axis=1))

    # 2. クロマ特徴（音高）
    chroma = librosa.feature.chroma_stft(y=y, sr=sr)
    features.extend(np.mean(chroma, axis=1))
    features.extend(np.std(chroma, axis=1))

    # 3. メル・スペクトログラム
    mel = librosa.feature.melspectrogram(y=y, sr=sr)
    features.extend(np.mean(mel, axis=1))
    features.extend(np.std(mel, axis=1))

    # 4. スペクトル・コントラスト
    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)
    features.extend(np.mean(contrast, axis=1))
    features.extend(np.std(contrast, axis=1))

    # 5. トーナル・セントロイド（Tonnetz）
    tonnetz = librosa.feature.tonnetz(y=y, sr=sr)
    features.extend(np.mean(tonnetz, axis=1))
    features.extend(np.std(tonnetz, axis=1))

    # 6. ゼロ交差率（Zero Crossing Rate）
    zcr = librosa.feature.zero_crossing_rate(y)
    features.append(np.mean(zcr))
    features.append(np.std(zcr))

    # 7. RMSエネルギー
    rms = librosa.feature.rms(y=y)
    features.append(np.mean(rms))
    features.append(np.std(rms))

    # 8. ピッチ（基本周波数）
    pitches, magnitudes = librosa.piptrack(y=y, sr=sr)
    pitch_values = []
    for t in range(pitches.shape[1]):
        index = magnitudes[:, t].argmax()
        pitch = pitches[index, t]
        if pitch > 0:
            pitch_values.append(pitch)

    if len(pitch_values) > 0:
        features.append(np.mean(pitch_values))
        features.append(np.std(pitch_values))
    else:
        features.extend([0, 0])

    return np.array(features)

# サンプルデータ生成（実際にはRAVDESSなどを使用）
def generate_emotion_dataset(n_samples_per_emotion=50):
    """
    デモ用の感情データを生成
    """
    np.random.seed(42)

    emotions = ['neutral', 'happy', 'sad', 'angry', 'fear']
    n_features = 194  # 上記の特徴抽出関数と同じ次元数

    X = []
    y = []

    for emotion_id, emotion in enumerate(emotions):
        # 各感情に特有のパターンを持つデータを生成
        base_features = np.random.randn(n_features) + emotion_id * 2

        for _ in range(n_samples_per_emotion):
            # バリエーションを追加
            sample = base_features + np.random.randn(n_features) * 0.5
            X.append(sample)
            y.append(emotion_id)

    return np.array(X), np.array(y), emotions

# データ生成
X, y, emotion_labels = generate_emotion_dataset(n_samples_per_emotion=50)

# 訓練・テストデータ分割
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 特徴量の標準化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ランダムフォレストで感情分類
model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=20)
model.fit(X_train_scaled, y_train)

# 評価
y_pred = model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)

print("=== 音声感情認識システム ===")
print(f"感情カテゴリ: {emotion_labels}")
print(f"特徴量次元: {X.shape[1]}")
print(f"訓練サンプル数: {len(X_train)}")
print(f"テストサンプル数: {len(X_test)}")
print(f"\n分類精度: {accuracy:.3f}")
print(f"\n詳細レポート:")
print(classification_report(y_test, y_pred, target_names=emotion_labels))

# 混同行列の可視化
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd',
            xticklabels=emotion_labels,
            yticklabels=emotion_labels)
plt.xlabel('予測感情')
plt.ylabel('真の感情')
plt.title('感情認識の混同行列', fontsize=14)
plt.tight_layout()
plt.show()

# 特徴量の重要度
feature_importance = model.feature_importances_
plt.figure(figsize=(12, 6))
plt.bar(range(len(feature_importance)), feature_importance, alpha=0.7)
plt.xlabel('特徴量インデックス')
plt.ylabel('重要度')
plt.title('特徴量の重要度', fontsize=14)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<h3>深層学習による感情認識</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

class EmotionCNN(nn.Module):
    """
    感情認識用のCNNモデル
    スペクトログラムを入力として受け取る
    """
    def __init__(self, n_emotions=5):
        super(EmotionCNN, self).__init__()

        # Convolutional layers
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)

        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(0.3)

        # Fully connected layers
        self.fc1 = nn.Linear(128 * 16 * 16, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, n_emotions)

        self.bn1 = nn.BatchNorm2d(32)
        self.bn2 = nn.BatchNorm2d(64)
        self.bn3 = nn.BatchNorm2d(128)

    def forward(self, x):
        # Conv block 1
        x = self.pool(F.relu(self.bn1(self.conv1(x))))
        x = self.dropout(x)

        # Conv block 2
        x = self.pool(F.relu(self.bn2(self.conv2(x))))
        x = self.dropout(x)

        # Conv block 3
        x = self.pool(F.relu(self.bn3(self.conv3(x))))
        x = self.dropout(x)

        # Flatten
        x = x.view(x.size(0), -1)

        # FC layers
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)

        return x

# モデルの初期化
model = EmotionCNN(n_emotions=5)
print("=== 感情認識CNNモデル ===")
print(f"総パラメータ数: {sum(p.numel() for p in model.parameters()):,}")

# サンプル入力でテスト（スペクトログラム: 128x128）
sample_input = torch.randn(4, 1, 128, 128)
with torch.no_grad():
    output = model(sample_input)

print(f"\n入力形状: {sample_input.shape}")
print(f"出力形状: {output.shape}")
print(f"出力ロジット（サンプル）:")
print(output[0])

# 簡易訓練デモ
def train_emotion_model(model, X_train, y_train, epochs=10, batch_size=32):
    """
    感情認識モデルの訓練
    """
    # データをTensorに変換
    X_tensor = torch.FloatTensor(X_train).unsqueeze(1).unsqueeze(2)
    y_tensor = torch.LongTensor(y_train)

    # DataLoaderの作成
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # 損失関数とオプティマイザ
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # 訓練ループ
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch_X, batch_y in dataloader:
            # 前処理: データを適切な形状に変換
            batch_X_resized = F.interpolate(batch_X, size=(128, 128))

            optimizer.zero_grad()
            outputs = model(batch_X_resized)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        if (epoch + 1) % 2 == 0:
            print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

    return model

print("\n=== モデル訓練（デモ）===")
trained_model = train_emotion_model(model, X_train_scaled, y_train, epochs=5)
print("✓ 訓練完了")
</code></pre>

<hr>

<h2>5.3 音声強調・ノイズ除去</h2>

<h3>音声強調の目的</h3>
<p><strong>音声強調（Speech Enhancement）</strong>は、ノイズを含む音声から目的音声を抽出し、品質を向上させる技術です。</p>

<h3>主要な手法</h3>

<table>
<thead>
<tr>
<th>手法</th>
<th>原理</th>
<th>特徴</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>スペクトル減算</strong></td>
<td>ノイズスペクトルを推定して減算</td>
<td>シンプル、リアルタイム可</td>
</tr>
<tr>
<td><strong>ウィーナーフィルタ</strong></td>
<td>最小平均二乗誤差フィルタ</td>
<td>統計的に最適</td>
</tr>
<tr>
<td><strong>深層学習</strong></td>
<td>DNNでマスクを推定</td>
<td>高性能、学習データ必要</td>
</tr>
</tbody>
</table>

<h3>実装例：スペクトル減算</h3>

<pre><code class="language-python">import numpy as np
import librosa
import matplotlib.pyplot as plt
from scipy.signal import wiener

def spectral_subtraction(noisy_signal, sr, noise_estimate_duration=0.5):
    """
    スペクトル減算によるノイズ除去

    Parameters:
    -----------
    noisy_signal : np.ndarray
        ノイズを含む音声信号
    sr : int
        サンプリングレート
    noise_estimate_duration : float
        ノイズ推定に使用する冒頭の時間（秒）

    Returns:
    --------
    enhanced_signal : np.ndarray
        強調された音声信号
    """
    # STFT
    n_fft = 2048
    hop_length = 512

    D = librosa.stft(noisy_signal, n_fft=n_fft, hop_length=hop_length)
    magnitude = np.abs(D)
    phase = np.angle(D)

    # ノイズスペクトルの推定（冒頭部分を使用）
    noise_frames = int(noise_estimate_duration * sr / hop_length)
    noise_spectrum = np.mean(magnitude[:, :noise_frames], axis=1, keepdims=True)

    # スペクトル減算
    alpha = 2.0  # 減算係数
    enhanced_magnitude = magnitude - alpha * noise_spectrum

    # 負の値を0にクリップ
    enhanced_magnitude = np.maximum(enhanced_magnitude, 0)

    # 位相を元に戻して逆STFT
    enhanced_D = enhanced_magnitude * np.exp(1j * phase)
    enhanced_signal = librosa.istft(enhanced_D, hop_length=hop_length)

    return enhanced_signal

# サンプル音声の生成
sr = 22050
duration = 3.0
t = np.linspace(0, duration, int(sr * duration))

# クリーンな音声信号（正弦波の組み合わせ）
clean_signal = (
    np.sin(2 * np.pi * 440 * t) +  # A4音
    0.5 * np.sin(2 * np.pi * 880 * t)  # A5音
)

# ノイズを追加
noise = np.random.randn(len(clean_signal)) * 0.3
noisy_signal = clean_signal + noise

# スペクトル減算を適用
enhanced_signal = spectral_subtraction(noisy_signal, sr)

# SNRの計算
def calculate_snr(signal, noise):
    signal_power = np.mean(signal ** 2)
    noise_power = np.mean(noise ** 2)
    snr = 10 * np.log10(signal_power / noise_power)
    return snr

snr_before = calculate_snr(clean_signal, noisy_signal - clean_signal)
snr_after = calculate_snr(clean_signal, enhanced_signal[:len(clean_signal)] - clean_signal)

print("=== スペクトル減算によるノイズ除去 ===")
print(f"SNR（処理前）: {snr_before:.2f} dB")
print(f"SNR（処理後）: {snr_after:.2f} dB")
print(f"改善: {snr_after - snr_before:.2f} dB")

# 可視化
fig, axes = plt.subplots(3, 2, figsize=(15, 12))

# 時間領域の波形
axes[0, 0].plot(t[:1000], clean_signal[:1000], alpha=0.7)
axes[0, 0].set_title('クリーン信号', fontsize=12)
axes[0, 0].set_xlabel('時間 (秒)')
axes[0, 0].set_ylabel('振幅')
axes[0, 0].grid(True, alpha=0.3)

axes[1, 0].plot(t[:1000], noisy_signal[:1000], alpha=0.7, color='orange')
axes[1, 0].set_title('ノイズ付加信号', fontsize=12)
axes[1, 0].set_xlabel('時間 (秒)')
axes[1, 0].set_ylabel('振幅')
axes[1, 0].grid(True, alpha=0.3)

axes[2, 0].plot(t[:len(enhanced_signal)][:1000], enhanced_signal[:1000],
                alpha=0.7, color='green')
axes[2, 0].set_title('強調信号（スペクトル減算後）', fontsize=12)
axes[2, 0].set_xlabel('時間 (秒)')
axes[2, 0].set_ylabel('振幅')
axes[2, 0].grid(True, alpha=0.3)

# スペクトログラム
D_clean = librosa.stft(clean_signal)
D_noisy = librosa.stft(noisy_signal)
D_enhanced = librosa.stft(enhanced_signal)

axes[0, 1].imshow(librosa.amplitude_to_db(np.abs(D_clean), ref=np.max),
                  aspect='auto', origin='lower', cmap='viridis')
axes[0, 1].set_title('クリーン（スペクトログラム）', fontsize=12)
axes[0, 1].set_ylabel('周波数')

axes[1, 1].imshow(librosa.amplitude_to_db(np.abs(D_noisy), ref=np.max),
                  aspect='auto', origin='lower', cmap='viridis')
axes[1, 1].set_title('ノイズ付加（スペクトログラム）', fontsize=12)
axes[1, 1].set_ylabel('周波数')

axes[2, 1].imshow(librosa.amplitude_to_db(np.abs(D_enhanced), ref=np.max),
                  aspect='auto', origin='lower', cmap='viridis')
axes[2, 1].set_title('強調後（スペクトログラム）', fontsize=12)
axes[2, 1].set_xlabel('時間フレーム')
axes[2, 1].set_ylabel('周波数')

plt.tight_layout()
plt.show()
</code></pre>

<h3>noisereduceライブラリの使用</h3>

<pre><code class="language-python">import noisereduce as nr

# noisereduceを使用したノイズ除去
reduced_noise_signal = nr.reduce_noise(
    y=noisy_signal,
    sr=sr,
    stationary=True,
    prop_decrease=1.0
)

# SNR計算
snr_noisereduce = calculate_snr(clean_signal,
                                reduced_noise_signal[:len(clean_signal)] - clean_signal)

print("\n=== noisereduceライブラリ ===")
print(f"SNR（処理後）: {snr_noisereduce:.2f} dB")
print(f"改善: {snr_noisereduce - snr_before:.2f} dB")

# 比較可視化
plt.figure(figsize=(15, 8))

plt.subplot(4, 1, 1)
plt.plot(t[:1000], clean_signal[:1000])
plt.title('クリーン信号', fontsize=12)
plt.ylabel('振幅')
plt.grid(True, alpha=0.3)

plt.subplot(4, 1, 2)
plt.plot(t[:1000], noisy_signal[:1000], color='orange')
plt.title(f'ノイズ付加信号 (SNR: {snr_before:.1f} dB)', fontsize=12)
plt.ylabel('振幅')
plt.grid(True, alpha=0.3)

plt.subplot(4, 1, 3)
plt.plot(t[:len(enhanced_signal)][:1000], enhanced_signal[:1000], color='green')
plt.title(f'スペクトル減算 (SNR: {snr_after:.1f} dB)', fontsize=12)
plt.ylabel('振幅')
plt.grid(True, alpha=0.3)

plt.subplot(4, 1, 4)
plt.plot(t[:len(reduced_noise_signal)][:1000], reduced_noise_signal[:1000],
         color='red')
plt.title(f'noisereduce (SNR: {snr_noisereduce:.1f} dB)', fontsize=12)
plt.xlabel('時間 (秒)')
plt.ylabel('振幅')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<blockquote>
<p><strong>注意</strong>: noisereduceライブラリは<code>pip install noisereduce</code>でインストールできます。</p>
</blockquote>

<hr>

<h2>5.4 音楽情報処理</h2>

<h3>音楽情報処理（MIR）の概要</h3>
<p><strong>音楽情報処理（Music Information Retrieval, MIR）</strong>は、音楽信号から情報を抽出・分析する技術です。</p>

<h3>主要なタスク</h3>

<table>
<thead>
<tr>
<th>タスク</th>
<th>説明</th>
<th>応用例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ビートトラッキング</strong></td>
<td>リズムの拍を検出</td>
<td>自動DJ、ダンスゲーム</td>
</tr>
<tr>
<td><strong>コード認識</strong></td>
<td>和音進行の推定</td>
<td>自動採譜、音楽理論分析</td>
</tr>
<tr>
<td><strong>ジャンル分類</strong></td>
<td>音楽ジャンルの識別</td>
<td>音楽推薦、プレイリスト生成</td>
</tr>
<tr>
<td><strong>音源分離</strong></td>
<td>楽器ごとに分離</td>
<td>リミックス、カラオケ</td>
</tr>
</tbody>
</table>

<h3>実装例：ビートトラッキング</h3>

<pre><code class="language-python">import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np

def beat_tracking_demo():
    """
    ビートトラッキングのデモンストレーション
    """
    # サンプル音楽信号の生成（ドラムビート風）
    sr = 22050
    duration = 8.0
    t = np.linspace(0, duration, int(sr * duration))

    # 120 BPM (2 beats per second)
    bpm = 120
    beat_interval = 60.0 / bpm

    # ビート位置でキックドラムのような音を生成
    signal = np.zeros(len(t))
    for beat_time in np.arange(0, duration, beat_interval):
        beat_sample = int(beat_time * sr)
        if beat_sample < len(signal):
            # キックドラムの模擬（減衰する低周波）
            kick_duration = int(0.1 * sr)
            kick_t = np.linspace(0, 0.1, kick_duration)
            kick = np.sin(2 * np.pi * 80 * kick_t) * np.exp(-kick_t * 30)

            end_idx = min(beat_sample + kick_duration, len(signal))
            signal[beat_sample:end_idx] += kick[:end_idx - beat_sample]

    # ノイズを少し追加
    signal += np.random.randn(len(signal)) * 0.05

    # ビート検出
    tempo, beat_frames = librosa.beat.beat_track(y=signal, sr=sr)
    beat_times = librosa.frames_to_time(beat_frames, sr=sr)

    print("=== ビートトラッキング ===")
    print(f"推定テンポ: {tempo:.1f} BPM")
    print(f"検出されたビート数: {len(beat_times)}")
    print(f"ビート間隔: {np.mean(np.diff(beat_times)):.3f} 秒")

    # オンセット強度の計算
    onset_env = librosa.onset.onset_strength(y=signal, sr=sr)
    times = librosa.frames_to_time(np.arange(len(onset_env)), sr=sr)

    # 可視化
    fig, axes = plt.subplots(3, 1, figsize=(14, 10))

    # 波形とビート位置
    axes[0].plot(t, signal, alpha=0.6)
    axes[0].vlines(beat_times, -1, 1, color='r', alpha=0.8,
                   linestyle='--', label='検出されたビート')
    axes[0].set_xlabel('時間 (秒)')
    axes[0].set_ylabel('振幅')
    axes[0].set_title(f'音声波形とビート検出（推定テンポ: {tempo:.1f} BPM）', fontsize=12)
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # オンセット強度
    axes[1].plot(times, onset_env, alpha=0.7, color='green')
    axes[1].vlines(beat_times, 0, onset_env.max(), color='r',
                   alpha=0.8, linestyle='--')
    axes[1].set_xlabel('時間 (秒)')
    axes[1].set_ylabel('強度')
    axes[1].set_title('オンセット強度とビート位置', fontsize=12)
    axes[1].grid(True, alpha=0.3)

    # テンポグラム
    tempogram = librosa.feature.tempogram(y=signal, sr=sr)
    axes[2].imshow(tempogram, aspect='auto', origin='lower', cmap='magma')
    axes[2].set_xlabel('時間フレーム')
    axes[2].set_ylabel('テンポ (BPM)')
    axes[2].set_title('テンポグラム', fontsize=12)

    plt.tight_layout()
    plt.show()

    return signal, sr, tempo, beat_times

# 実行
signal, sr, tempo, beat_times = beat_tracking_demo()
</code></pre>

<h3>実装例：音楽ジャンル分類</h3>

<pre><code class="language-python">from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_val_score

def extract_music_features(audio, sr):
    """
    音楽ジャンル分類用の特徴量を抽出
    """
    features = []

    # 1. MFCCの統計量
    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20)
    features.extend(np.mean(mfcc, axis=1))
    features.extend(np.std(mfcc, axis=1))

    # 2. クロマ特徴
    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)
    features.extend(np.mean(chroma, axis=1))
    features.extend(np.std(chroma, axis=1))

    # 3. スペクトル特徴
    spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]
    features.append(np.mean(spectral_centroids))
    features.append(np.std(spectral_centroids))

    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)[0]
    features.append(np.mean(spectral_rolloff))
    features.append(np.std(spectral_rolloff))

    # 4. ゼロ交差率
    zcr = librosa.feature.zero_crossing_rate(audio)[0]
    features.append(np.mean(zcr))
    features.append(np.std(zcr))

    # 5. テンポ
    tempo, _ = librosa.beat.beat_track(y=audio, sr=sr)
    features.append(tempo)

    # 6. ハーモニック・パーカッシブ成分
    y_harmonic, y_percussive = librosa.effects.hpss(audio)
    harmonic_ratio = np.sum(y_harmonic**2) / (np.sum(audio**2) + 1e-6)
    features.append(harmonic_ratio)

    return np.array(features)

# ジャンル分類のデモ
def music_genre_classification():
    """
    音楽ジャンル分類のデモンストレーション
    """
    np.random.seed(42)

    # 仮想的なジャンルデータを生成
    genres = ['Classical', 'Jazz', 'Rock', 'Electronic', 'Hip-Hop']
    n_samples_per_genre = 30

    X = []
    y = []

    for genre_id, genre in enumerate(genres):
        # 各ジャンルに特徴的なパターンを生成
        base_features = np.random.randn(51) + genre_id * 1.5

        for _ in range(n_samples_per_genre):
            sample = base_features + np.random.randn(51) * 0.4
            X.append(sample)
            y.append(genre_id)

    X = np.array(X)
    y = np.array(y)

    # モデルの訓練と評価（交差検証）
    model = GradientBoostingClassifier(n_estimators=100, random_state=42)
    scores = cross_val_score(model, X, y, cv=5)

    print("\n=== 音楽ジャンル分類 ===")
    print(f"ジャンル: {genres}")
    print(f"サンプル数: {len(X)}")
    print(f"特徴量次元: {X.shape[1]}")
    print(f"\n交差検証精度: {scores.mean():.3f} (+/- {scores.std():.3f})")

    # モデルを全データで訓練
    model.fit(X, y)

    # 特徴量の重要度（上位10個）
    feature_importance = model.feature_importances_
    top_10_idx = np.argsort(feature_importance)[-10:]

    plt.figure(figsize=(10, 6))
    plt.barh(range(10), feature_importance[top_10_idx], alpha=0.7)
    plt.xlabel('重要度')
    plt.ylabel('特徴量インデックス')
    plt.title('重要な特徴量（上位10個）', fontsize=14)
    plt.yticks(range(10), top_10_idx)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

    return model, genres

model, genres = music_genre_classification()
</code></pre>

<hr>

<h2>5.5 エンドツーエンド音声AIアプリケーション</h2>

<h3>統合音声処理システム</h3>
<p>実世界のアプリケーションでは、複数の音声処理技術を組み合わせて使用します。</p>

<div class="mermaid">
graph LR
    A[音声入力] --> B[ノイズ除去]
    B --> C[話者検証]
    C --> D{本人?}
    D -->|Yes| E[感情認識]
    D -->|No| F[アクセス拒否]
    E --> G[音声認識]
    G --> H[応答生成]
    H --> I[音声合成]
    I --> J[出力]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#e8f5e9
    style F fill:#ffcdd2
    style G fill:#c8e6c9
    style H fill:#b2dfdb
    style I fill:#b2ebf2
    style J fill:#c5cae9
</div>

<h3>実装例：統合音声処理パイプライン</h3>

<pre><code class="language-python">import numpy as np
import librosa
from dataclasses import dataclass
from typing import Tuple, Optional

@dataclass
class AudioProcessingResult:
    """音声処理の結果を格納"""
    is_verified: bool
    speaker_similarity: float
    emotion: Optional[str]
    emotion_confidence: float
    enhanced_audio: np.ndarray
    processing_time: float

class IntegratedAudioPipeline:
    """
    統合音声処理パイプライン

    機能:
    1. ノイズ除去
    2. 話者検証
    3. 感情認識
    """
    def __init__(self, verification_threshold=0.7):
        self.verification_threshold = verification_threshold
        self.enrolled_speakers = {}

        # モデルの初期化（実際にはロード）
        self.emotion_labels = ['neutral', 'happy', 'sad', 'angry', 'fear']

    def preprocess_audio(self, audio, sr):
        """
        音声の前処理
        1. リサンプリング
        2. ノイズ除去
        """
        # 16kHzにリサンプリング
        if sr != 16000:
            audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)
            sr = 16000

        # ノイズ除去（簡易版）
        try:
            import noisereduce as nr
            audio_enhanced = nr.reduce_noise(y=audio, sr=sr, stationary=True)
        except:
            # noisereduceがない場合はそのまま
            audio_enhanced = audio

        return audio_enhanced, sr

    def extract_embedding(self, audio, sr):
        """
        話者埋め込みベクトルを抽出
        """
        # MFCCベースの簡易埋め込み
        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20)
        mfcc_delta = librosa.feature.delta(mfcc)

        embedding = np.concatenate([
            np.mean(mfcc, axis=1),
            np.std(mfcc, axis=1),
            np.mean(mfcc_delta, axis=1),
            np.std(mfcc_delta, axis=1)
        ])

        return embedding

    def verify_speaker(self, audio, sr, speaker_id):
        """
        話者検証
        """
        if speaker_id not in self.enrolled_speakers:
            return False, 0.0

        # 埋め込み抽出
        test_embedding = self.extract_embedding(audio, sr)
        enrolled_embedding = self.enrolled_speakers[speaker_id]

        # コサイン類似度
        from scipy.spatial.distance import cosine
        similarity = 1 - cosine(test_embedding, enrolled_embedding)

        is_verified = similarity > self.verification_threshold

        return is_verified, similarity

    def recognize_emotion(self, audio, sr):
        """
        感情認識
        """
        # 特徴抽出（簡易版）
        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
        chroma = librosa.feature.chroma_stft(y=audio, sr=sr)

        features = np.concatenate([
            np.mean(mfcc, axis=1),
            np.std(mfcc, axis=1),
            np.mean(chroma, axis=1)
        ])

        # 簡易的な感情分類（実際にはモデルを使用）
        # ここではランダムに選択
        emotion_idx = np.random.randint(0, len(self.emotion_labels))
        confidence = np.random.uniform(0.7, 0.95)

        return self.emotion_labels[emotion_idx], confidence

    def process(self, audio, sr, speaker_id=None):
        """
        統合処理パイプライン

        Parameters:
        -----------
        audio : np.ndarray
            入力音声
        sr : int
            サンプリングレート
        speaker_id : str, optional
            検証する話者ID

        Returns:
        --------
        result : AudioProcessingResult
            処理結果
        """
        import time
        start_time = time.time()

        # 1. 前処理（ノイズ除去）
        enhanced_audio, sr = self.preprocess_audio(audio, sr)

        # 2. 話者検証
        is_verified = True
        similarity = 1.0
        if speaker_id is not None:
            is_verified, similarity = self.verify_speaker(enhanced_audio, sr, speaker_id)

        # 3. 感情認識（検証が通った場合のみ）
        emotion = None
        emotion_confidence = 0.0
        if is_verified:
            emotion, emotion_confidence = self.recognize_emotion(enhanced_audio, sr)

        processing_time = time.time() - start_time

        result = AudioProcessingResult(
            is_verified=is_verified,
            speaker_similarity=similarity,
            emotion=emotion,
            emotion_confidence=emotion_confidence,
            enhanced_audio=enhanced_audio,
            processing_time=processing_time
        )

        return result

    def enroll_speaker(self, speaker_id, audio, sr):
        """
        話者を登録
        """
        audio_enhanced, sr = self.preprocess_audio(audio, sr)
        embedding = self.extract_embedding(audio_enhanced, sr)
        self.enrolled_speakers[speaker_id] = embedding
        print(f"✓ 話者 '{speaker_id}' を登録しました")

# パイプラインのデモンストレーション
print("=== 統合音声処理パイプライン ===\n")

# パイプラインの初期化
pipeline = IntegratedAudioPipeline(verification_threshold=0.7)

# サンプル音声の生成
sr = 16000
duration = 3.0
t = np.linspace(0, duration, int(sr * duration))

# 話者Aの音声
audio_speaker_a = np.sin(2 * np.pi * 300 * t) + 0.3 * np.random.randn(len(t))
# 話者Bの音声
audio_speaker_b = np.sin(2 * np.pi * 500 * t) + 0.3 * np.random.randn(len(t))

# 話者を登録
pipeline.enroll_speaker("Alice", audio_speaker_a, sr)
pipeline.enroll_speaker("Bob", audio_speaker_b, sr)

print(f"\n登録話者: {list(pipeline.enrolled_speakers.keys())}\n")

# テスト1: Aliceの本人音声
print("【テスト1】Alice（本人）の音声")
test_audio_alice = audio_speaker_a + 0.1 * np.random.randn(len(audio_speaker_a))
result = pipeline.process(test_audio_alice, sr, speaker_id="Alice")

print(f"  話者検証: {'✓ 承認' if result.is_verified else '✗ 拒否'}")
print(f"  類似度: {result.speaker_similarity:.3f}")
print(f"  感情: {result.emotion} (信頼度: {result.emotion_confidence:.2%})")
print(f"  処理時間: {result.processing_time*1000:.1f} ms")

# テスト2: Aliceになりすまし（Bobの音声）
print("\n【テスト2】Alice（なりすまし: Bob）の音声")
result = pipeline.process(audio_speaker_b, sr, speaker_id="Alice")

print(f"  話者検証: {'✓ 承認' if result.is_verified else '✗ 拒否'}")
print(f"  類似度: {result.speaker_similarity:.3f}")
print(f"  感情: {result.emotion if result.emotion else 'N/A'}")
print(f"  処理時間: {result.processing_time*1000:.1f} ms")

# テスト3: Bobの本人音声
print("\n【テスト3】Bob（本人）の音声")
test_audio_bob = audio_speaker_b + 0.1 * np.random.randn(len(audio_speaker_b))
result = pipeline.process(test_audio_bob, sr, speaker_id="Bob")

print(f"  話者検証: {'✓ 承認' if result.is_verified else '✗ 拒否'}")
print(f"  類似度: {result.speaker_similarity:.3f}")
print(f"  感情: {result.emotion} (信頼度: {result.emotion_confidence:.2%})")
print(f"  処理時間: {result.processing_time*1000:.1f} ms")

print("\n" + "="*50)
print("統合パイプライン処理完了")
print("="*50)
</code></pre>

<h3>リアルタイム処理の考慮事項</h3>

<table>
<thead>
<tr>
<th>要素</th>
<th>課題</th>
<th>対策</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>レイテンシ</strong></td>
<td>処理遅延が体感に影響</td>
<td>軽量モデル、フレーム単位処理</td>
</tr>
<tr>
<td><strong>メモリ</strong></td>
<td>組み込み機器では制約</td>
<td>量子化、プルーニング</td>
</tr>
<tr>
<td><strong>精度</strong></td>
<td>リアルタイムと精度のトレードオフ</td>
<td>適応的処理、段階的分析</td>
</tr>
</tbody>
</table>

<hr>

<h2>5.6 本章のまとめ</h2>

<h3>学んだこと</h3>

<ol>
<li><p><strong>話者認識・検証</strong></p>
<ul>
<li>話者識別と話者検証の違い</li>
<li>i-vector、x-vectorによる話者埋め込み</li>
<li>類似度計算による検証システム</li>
</ul></li>

<li><p><strong>音声感情認識</strong></p>
<ul>
<li>韻律・音響特徴による感情推定</li>
<li>RAVDESS、IEMOCAPなどのデータセット</li>
<li>CNN/LSTMによる深層学習アプローチ</li>
</ul></li>

<li><p><strong>音声強調・ノイズ除去</strong></p>
<ul>
<li>スペクトル減算、ウィーナーフィルタ</li>
<li>深層学習による強調</li>
<li>noisereduceライブラリの活用</li>
</ul></li>

<li><p><strong>音楽情報処理</strong></p>
<ul>
<li>ビートトラッキング、テンポ推定</li>
<li>コード認識、ジャンル分類</li>
<li>音楽的特徴量の抽出</li>
</ul></li>

<li><p><strong>統合システム</strong></p>
<ul>
<li>複数技術の組み合わせ</li>
<li>エンドツーエンドパイプライン</li>
<li>リアルタイム処理の最適化</li>
</ul></li>
</ol>

<h3>実世界への応用例</h3>

<table>
<thead>
<tr>
<th>分野</th>
<th>アプリケーション</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>セキュリティ</strong></td>
<td>音声認証、詐欺検出</td>
</tr>
<tr>
<td><strong>ヘルスケア</strong></td>
<td>感情モニタリング、診断支援</td>
</tr>
<tr>
<td><strong>コールセンター</strong></td>
<td>顧客感情分析、品質向上</td>
</tr>
<tr>
<td><strong>エンターテイメント</strong></td>
<td>音楽推薦、自動DJ、カラオケ</td>
</tr>
<tr>
<td><strong>通話品質</strong></td>
<td>ノイズキャンセリング、音声強調</td>
</tr>
</tbody>
</table>

<h3>さらに学ぶために</h3>

<ul>
<li><strong>データセット</strong>: VoxCeleb、LibriSpeech、GTZAN、MusicNet</li>
<li><strong>ライブラリ</strong>: pyannote.audio、speechbrain、essentia</li>
<li><strong>最新手法</strong>: WavLM、Conformer、U-Net for audio</li>
<li><strong>評価指標</strong>: EER（Equal Error Rate）、DER（Diarization Error Rate）</li>
</ul>

<hr>

<h2>演習問題</h2>

<h3>問題1（難易度：easy）</h3>
<p>話者識別（Speaker Identification）と話者検証（Speaker Verification）の違いを説明し、それぞれの応用例を挙げてください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>

<p><strong>話者識別（Speaker Identification）</strong>：</p>
<ul>
<li>定義: 複数の登録話者の中から、入力音声の話者が誰かを特定するタスク</li>
<li>問い: 「この音声は誰のものか？」</li>
<li>分類: N人の中から1人を選ぶN値分類問題</li>
<li>応用例:
  <ul>
  <li>会議の発言者認識（議事録の自動作成）</li>
  <li>テレビ番組での話者ラベリング</li>
  <li>音声アシスタントでのユーザー識別</li>
  </ul>
</li>
</ul>

<p><strong>話者検証（Speaker Verification）</strong>：</p>
<ul>
<li>定義: 入力音声が特定の話者本人のものかどうかを判定するタスク</li>
<li>問い: 「この音声は山田さん本人か？」</li>
<li>分類: Yes/Noの2値分類問題</li>
<li>応用例:
  <ul>
  <li>音声による本人認証（スマートフォンのロック解除）</li>
  <li>銀行の電話取引での本人確認</li>
  <li>セキュリティシステムへのアクセス制御</li>
  </ul>
</li>
</ul>

<p><strong>主な違い</strong>：</p>
<table>
<thead>
<tr>
<th>項目</th>
<th>話者識別</th>
<th>話者検証</th>
</tr>
</thead>
<tbody>
<tr>
<td>問題設定</td>
<td>N値分類</td>
<td>2値分類</td>
</tr>
<tr>
<td>出力</td>
<td>話者ID</td>
<td>本人/他人</td>
</tr>
<tr>
<td>登録話者</td>
<td>複数必要</td>
<td>1人のみでも可</td>
</tr>
<tr>
<td>難易度</td>
<td>話者数に依存</td>
<td>閾値設定が重要</td>
</tr>
</tbody>
</table>

</details>

<h3>問題2（難易度：medium）</h3>
<p>音声感情認識において、韻律特徴（ピッチ、エネルギー、話速）と各感情（喜び、悲しみ、怒り、恐怖）の関係を説明してください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>

<p><strong>感情と韻律特徴の関係</strong>：</p>

<table>
<thead>
<tr>
<th>感情</th>
<th>ピッチ</th>
<th>エネルギー</th>
<th>話速</th>
<th>その他の特徴</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>喜び</strong></td>
<td>高め、変動大</td>
<td>高め</td>
<td>速め</td>
<td>明瞭な発音、ピッチレンジが広い</td>
</tr>
<tr>
<td><strong>悲しみ</strong></td>
<td>低め、単調</td>
<td>低め</td>
<td>遅め</td>
<td>長いポーズ、エネルギー変動小</td>
</tr>
<tr>
<td><strong>怒り</strong></td>
<td>高め、強調</td>
<td>高め</td>
<td>速めor遅め</td>
<td>強いストレス、スペクトル帯域広い</td>
</tr>
<tr>
<td><strong>恐怖</strong></td>
<td>高め、不安定</td>
<td>中〜高</td>
<td>速め</td>
<td>声の震え、ピッチ変動大</td>
</tr>
<tr>
<td><strong>中立</strong></td>
<td>中程度、安定</td>
<td>中程度</td>
<td>通常</td>
<td>特徴的なパターンなし</td>
</tr>
</tbody>
</table>

<p><strong>詳細説明</strong>：</p>

<ol>
<li><p><strong>ピッチ（基本周波数）</strong>：</p>
<ul>
<li>高覚醒感情（喜び、怒り、恐怖）→ ピッチ高め</li>
<li>低覚醒感情（悲しみ）→ ピッチ低め</li>
<li>感情の強さとピッチ変動の大きさが相関</li>
</ul></li>

<li><p><strong>エネルギー（音量）</strong>：</p>
<ul>
<li>ポジティブ感情（喜び）、攻撃的感情（怒り）→ エネルギー高</li>
<li>ネガティブ消極的感情（悲しみ）→ エネルギー低</li>
<li>RMS（二乗平均平方根）で測定</li>
</ul></li>

<li><p><strong>話速（Speaking Rate）</strong>：</p>
<ul>
<li>興奮状態（喜び、恐怖）→ 速い</li>
<li>抑うつ状態（悲しみ）→ 遅い</li>
<li>怒りは個人差が大きい（速い/遅い両方）</li>
</ul></li>
</ol>

<p><strong>実装での注意点</strong>：</p>
<ul>
<li>個人差が大きいため、話者正規化が重要</li>
<li>文化的背景による表現の違いを考慮</li>
<li>複数の特徴量を組み合わせて使用</li>
<li>コンテキスト（会話の流れ）も重要な手がかり</li>
</ul>

</details>

<h3>問題3（難易度：medium）</h3>
<p>スペクトル減算（Spectral Subtraction）によるノイズ除去の原理を説明し、この手法の利点と欠点を述べてください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>

<p><strong>スペクトル減算の原理</strong>：</p>

<ol>
<li><p><strong>基本的な考え方</strong>：</p>
<ul>
<li>ノイズ付加音声 = クリーン音声 + ノイズ</li>
<li>周波数領域でノイズのスペクトルを推定し、減算する</li>
</ul></li>

<li><p><strong>処理ステップ</strong>：</p>
<ol>
<li>ノイズ付加音声をSTFT（短時間フーリエ変換）</li>
<li>無音部分からノイズスペクトルを推定</li>
<li>各周波数ビンでノイズスペクトルを減算</li>
<li>負の値を0にクリップ（ハーフウェーブ整流）</li>
<li>位相を元に戻して逆STFT</li>
</ol></li>
</ol>

<p><strong>数式表現</strong>：</p>
<p>$$
|\hat{S}(\omega, t)| = \max(|Y(\omega, t)| - \alpha |\hat{N}(\omega)|, \beta |Y(\omega, t)|)
$$</p>
<ul>
<li>$Y(\omega, t)$: ノイズ付加音声のスペクトル</li>
<li>$\hat{N}(\omega)$: 推定ノイズスペクトル</li>
<li>$\alpha$: 減算係数（通常1〜3）</li>
<li>$\beta$: スペクトルフロア（通常0.01〜0.1）</li>
</ul>

<p><strong>利点</strong>：</p>
<ul>
<li>✓ 実装がシンプル</li>
<li>✓ 計算コストが低い</li>
<li>✓ リアルタイム処理が可能</li>
<li>✓ 定常ノイズに対して効果的</li>
<li>✓ パラメータ調整が容易</li>
</ul>

<p><strong>欠点</strong>：</p>
<ul>
<li>✗ <strong>ミュージカルノイズ</strong>の発生
  <ul>
  <li>減算処理により残留ノイズが「キラキラ」した音になる</li>
  <li>聴感上、不快に感じることがある</li>
  </ul>
</li>
<li>✗ <strong>非定常ノイズに弱い</strong>
  <ul>
  <li>時間変動するノイズの推定が困難</li>
  <li>突発的なノイズには効果が限定的</li>
  </ul>
</li>
<li>✗ <strong>音声成分の歪み</strong>
  <ul>
  <li>過度な減算により音声品質が劣化</li>
  <li>特に低SNR環境では顕著</li>
  </ul>
</li>
<li>✗ <strong>ノイズ推定の精度依存</strong>
  <ul>
  <li>無音部分がない場合、推定が困難</li>
  <li>ノイズ特性が変化すると性能低下</li>
  </ul>
</li>
</ul>

<p><strong>改善手法</strong>：</p>
<ul>
<li>マルチバンドスペクトル減算: 周波数帯域ごとに減算係数を調整</li>
<li>非線形スペクトル減算: 過減算を防ぐ</li>
<li>後処理フィルタ: ミュージカルノイズの低減</li>
<li>適応的ノイズ推定: 音声区間を避けて更新</li>
</ul>

</details>

<h3>問題4（難易度：hard）</h3>
<p>x-vectorネットワークのアーキテクチャを説明し、従来のi-vectorと比較した利点を述べてください。また、Statistics Pooling層の役割を説明してください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>

<p><strong>x-vectorネットワークのアーキテクチャ</strong>：</p>

<ol>
<li><p><strong>全体構造</strong>：</p>
<ul>
<li>入力: 音声の特徴量系列（MFCC、フィルタバンクなど）</li>
<li>TDNN（Time Delay Neural Network）layers</li>
<li>Statistics Pooling layer</li>
<li>Segment-level fully connected layers</li>
<li>出力: 固定長の埋め込みベクトル（通常512次元）</li>
</ul></li>

<li><p><strong>TDNNレイヤー</strong>：</p>
<ul>
<li>時間軸方向に異なる遅延（dilation）を持つ1D畳み込み</li>
<li>異なる時間スケールの文脈を捉える</li>
<li>典型的な構成:
  <ul>
  <li>Layer 1: kernel=5, dilation=1</li>
  <li>Layer 2: kernel=3, dilation=2</li>
  <li>Layer 3: kernel=3, dilation=3</li>
  <li>Layer 4-5: kernel=1, dilation=1</li>
  </ul>
</li>
</ul></li>

<li><p><strong>Statistics Pooling層</strong>：</p>
<ul>
<li>可変長入力を固定長出力に変換する重要な層</li>
<li>時間軸方向の統計量を計算:
$$
\text{output} = [\mu, \sigma]
$$
  <ul>
  <li>$\mu = \frac{1}{T}\sum_{t=1}^{T} h_t$（平均）</li>
  <li>$\sigma = \sqrt{\frac{1}{T}\sum_{t=1}^{T} (h_t - \mu)^2}$（標準偏差）</li>
  </ul>
</li>
<li>入力: (batch, features, time)</li>
<li>出力: (batch, features * 2)</li>
</ul></li>

<li><p><strong>Segment-level layers</strong>：</p>
<ul>
<li>Statistics Pooling後の全結合層</li>
<li>話者埋め込みを生成</li>
<li>分類タスクで訓練、埋め込みを抽出</li>
</ul></li>
</ol>

<p><strong>i-vector vs x-vector の比較</strong>：</p>

<table>
<thead>
<tr>
<th>項目</th>
<th>i-vector</th>
<th>x-vector</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>手法</strong></td>
<td>統計的（GMM-UBM）</td>
<td>深層学習（DNN）</td>
</tr>
<tr>
<td><strong>特徴抽出</strong></td>
<td>Baum-Welch統計量</td>
<td>TDNN（畳み込み）</td>
</tr>
<tr>
<td><strong>訓練データ量</strong></td>
<td>少量でも可</td>
<td>大量必要</td>
</tr>
<tr>
<td><strong>計算コスト</strong></td>
<td>低い</td>
<td>高い（訓練時）</td>
</tr>
<tr>
<td><strong>性能</strong></td>
<td>中程度</td>
<td>高い</td>
</tr>
<tr>
<td><strong>短時間音声</strong></td>
<td>やや苦手</td>
<td>頑健</td>
</tr>
<tr>
<td><strong>ノイズ耐性</strong></td>
<td>中程度</td>
<td>高い</td>
</tr>
<tr>
<td><strong>実装難易度</strong></td>
<td>高い（UBM訓練）</td>
<td>中（フレームワーク利用）</td>
</tr>
</tbody>
</table>

<p><strong>x-vectorの利点</strong>：</p>

<ol>
<li><p><strong>高い識別性能</strong>：</p>
<ul>
<li>深層学習により複雑な話者特性を学習</li>
<li>大規模データで訓練すると大幅に性能向上</li>
</ul></li>

<li><p><strong>短時間音声への頑健性</strong>：</p>
<ul>
<li>2〜3秒の音声でも高精度</li>
<li>i-vectorは長時間音声（30秒以上）が望ましい</li>
</ul></li>

<li><p><strong>ノイズ耐性</strong>：</p>
<ul>
<li>訓練時のデータ拡張で頑健性向上</li>
<li>Statistics Poolingが時間変動を吸収</li>
</ul></li>

<li><p><strong>エンドツーエンド訓練</strong>：</p>
<ul>
<li>特徴抽出から分類まで同時最適化</li>
<li>i-vectorはUBM訓練が別途必要</li>
</ul></li>

<li><p><strong>転移学習が容易</strong>：</p>
<ul>
<li>事前訓練モデルをファインチューニング</li>
<li>少量データでも適応可能</li>
</ul></li>
</ol>

<p><strong>Statistics Poolingの役割</strong>：</p>

<ol>
<li><p><strong>可変長から固定長への変換</strong>：</p>
<ul>
<li>異なる長さの音声を同じ次元の埋め込みに変換</li>
<li>これにより分類器が一定の入力を受け取れる</li>
</ul></li>

<li><p><strong>時間不変性の獲得</strong>：</p>
<ul>
<li>平均と標準偏差は時間順序に依存しない</li>
<li>話者の特徴を時間軸で要約</li>
</ul></li>

<li><p><strong>2次統計量の活用</strong>：</p>
<ul>
<li>平均（1次）だけでなく標準偏差（2次）も使用</li>
<li>より豊かな話者表現が可能</li>
</ul></li>

<li><p><strong>i-vectorとの類似性</strong>：</p>
<ul>
<li>i-vectorも0次・1次統計量を使用</li>
<li>x-vectorは深層特徴の統計量を計算</li>
</ul></li>
</ol>

<p><strong>実装例（Statistics Pooling）</strong>：</p>
<pre><code class="language-python">import torch
import torch.nn as nn

class StatisticsPooling(nn.Module):
    def forward(self, x):
        # x: (batch, features, time)
        mean = torch.mean(x, dim=2)  # (batch, features)
        std = torch.std(x, dim=2)    # (batch, features)
        stats = torch.cat([mean, std], dim=1)  # (batch, features*2)
        return stats
</code></pre>

</details>

<h3>問題5（難易度：hard）</h3>
<p>統合音声処理パイプラインを設計する際の主要な考慮事項を挙げ、リアルタイム処理を実現するための最適化手法を説明してください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>

<p><strong>1. 主要な考慮事項</strong>：</p>

<h4>A. 機能的要件</h4>
<ul>
<li><strong>処理タスク</strong>:
  <ul>
  <li>ノイズ除去、話者認識、感情認識、音声認識など</li>
  <li>タスクの優先順位と依存関係</li>
  </ul>
</li>
<li><strong>精度要件</strong>:
  <ul>
  <li>アプリケーションごとの許容誤差</li>
  <li>セキュリティ vs ユーザビリティのバランス</li>
  </ul>
</li>
<li><strong>対応シナリオ</strong>:
  <ul>
  <li>静かな環境 vs 騒音環境</li>
  <li>クリアな音声 vs 品質劣化</li>
  </ul>
</li>
</ul>

<h4>B. 非機能的要件</h4>
<ul>
<li><strong>レイテンシ（遅延）</strong>:
  <ul>
  <li>リアルタイム: < 100ms（通話）</li>
  <li>準リアルタイム: < 500ms（アシスタント）</li>
  <li>バッチ: > 1s（分析）</li>
  </ul>
</li>
<li><strong>スループット</strong>:
  <ul>
  <li>同時処理可能なストリーム数</li>
  <li>CPU/GPU/メモリのリソース制約</li>
  </ul>
</li>
<li><strong>スケーラビリティ</strong>:
  <ul>
  <li>ユーザー数の増加への対応</li>
  <li>水平/垂直スケーリング</li>
  </ul>
</li>
<li><strong>信頼性</strong>:
  <ul>
  <li>エラーハンドリング</li>
  <li>フォールバックメカニズム</li>
  </ul>
</li>
</ul>

<h4>C. システム設計</h4>
<ul>
<li><strong>モジュール化</strong>:
  <ul>
  <li>各処理を独立したモジュールに</li>
  <li>再利用性と保守性の向上</li>
  </ul>
</li>
<li><strong>パイプライン構成</strong>:
  <ul>
  <li>直列 vs 並列処理</li>
  <li>条件分岐（例: 話者検証失敗時は以降スキップ）</li>
  </ul>
</li>
<li><strong>データフロー</strong>:
  <ul>
  <li>バッファ管理</li>
  <li>ストリーミング vs バッチ</li>
  </ul>
</li>
</ul>

<p><strong>2. リアルタイム処理の最適化手法</strong>：</p>

<h4>A. モデルレベルの最適化</h4>

<ol>
<li><p><strong>モデルの軽量化</strong>：</p>
<ul>
<li><strong>量子化（Quantization）</strong>:
<pre><code class="language-python">import torch

# FP32 → INT8
model_int8 = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)
# メモリ: 1/4、速度: 2-4倍
</code></pre>
</li>
<li><strong>プルーニング（Pruning）</strong>:
<pre><code class="language-python">import torch.nn.utils.prune as prune

# 重みの50%を削除
prune.l1_unstructured(module, name='weight', amount=0.5)
</code></pre>
</li>
<li><strong>知識蒸留（Knowledge Distillation）</strong>:
  <ul>
  <li>大規模モデルの知識を小規模モデルに転移</li>
  <li>精度を保ちつつサイズを削減</li>
  </ul>
</li>
</ul></li>

<li><p><strong>軽量アーキテクチャの選択</strong>：</p>
<ul>
<li><strong>MobileNet系</strong>: Depthwise Separable Convolution</li>
<li><strong>SqueezeNet</strong>: Fire Moduleによる圧縮</li>
<li><strong>EfficientNet</strong>: 精度とサイズのバランス</li>
</ul></li>

<li><p><strong>効率的な演算</strong>：</p>
<ul>
<li>畳み込みの最適化（Winograd、FFT）</li>
<li>行列演算のバッチ化</li>
<li>SIMD命令の活用</li>
</ul></li>
</ol>

<h4>B. システムレベルの最適化</h4>

<ol>
<li><p><strong>フレーム単位処理</strong>：</p>
<pre><code class="language-python">frame_length = 512  # 約23ms @ 22kHz
hop_length = 256    # 約12ms @ 22kHz

# ストリーミング処理
buffer = []
for frame in audio_stream:
    buffer.append(frame)
    if len(buffer) >= frame_length:
        process_frame(buffer[:frame_length])
        buffer = buffer[hop_length:]
</code></pre>
</li>

<li><p><strong>並列処理</strong>：</p>
<ul>
<li><strong>マルチスレッド</strong>:
<pre><code class="language-python">from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=4) as executor:
    futures = [
        executor.submit(noise_reduction, audio),
        executor.submit(feature_extraction, audio)
    ]
    results = [f.result() for f in futures]
</code></pre>
</li>
<li><strong>GPU活用</strong>:
<pre><code class="language-python"># バッチ処理でGPU効率を最大化
batch_audio = torch.stack(audio_list).cuda()
with torch.no_grad():
    embeddings = model(batch_audio)
</code></pre>
</li>
</ul></li>

<li><p><strong>キャッシング</strong>：</p>
<ul>
<li>話者埋め込みのキャッシュ</li>
<li>中間特徴量の再利用</li>
<li>モデルの事前ロード</li>
</ul></li>

<li><p><strong>適応的処理</strong>：</p>
<ul>
<li>信頼度に基づくスキップ:
<pre><code class="language-python">if speaker_confidence > 0.95:
    # 高信頼度なら詳細処理スキップ
    return quick_result
else:
    # 低信頼度なら詳細分析
    return detailed_analysis()
</code></pre>
</li>
<li>段階的処理（Early Exit）</li>
</ul></li>

<li><p><strong>メモリ管理</strong>：</p>
<ul>
<li>循環バッファの使用</li>
<li>オブジェクトプールパターン</li>
<li>明示的なメモリ解放</li>
</ul></li>
</ol>

<h4>C. アルゴリズムレベルの最適化</h4>

<ol>
<li><p><strong>オンライン処理</strong>：</p>
<ul>
<li>ストリーミングMFCC計算</li>
<li>オンラインノーマライゼーション</li>
<li>増分的統計量更新</li>
</ul></li>

<li><p><strong>近似アルゴリズム</strong>：</p>
<ul>
<li>FFTの近似（NFFT）</li>
<li>近似最近傍探索（ANN）</li>
<li>低ランク近似</li>
</ul></li>

<li><p><strong>特徴量の選択</strong>：</p>
<ul>
<li>計算コストの低い特徴量を優先</li>
<li>冗長な特徴量の削除</li>
<li>PCA/LDAによる次元削減</li>
</ul></li>
</ol>

<p><strong>3. 実装例: 最適化されたパイプライン</strong>：</p>

<pre><code class="language-python">import torch
import numpy as np
from queue import Queue
from threading import Thread

class OptimizedAudioPipeline:
    def __init__(self):
        # モデルの量子化
        self.model = torch.quantization.quantize_dynamic(
            load_model(), {torch.nn.Linear}, dtype=torch.qint8
        )
        self.model.eval()

        # キャッシュ
        self.speaker_cache = {}

        # ストリーム処理用バッファ
        self.audio_buffer = Queue(maxsize=100)

        # ワーカースレッド
        self.workers = [
            Thread(target=self._process_worker)
            for _ in range(4)
        ]
        for w in self.workers:
            w.start()

    def process_stream(self, audio_chunk):
        """ストリーミング処理"""
        # 非ブロッキングで追加
        if not self.audio_buffer.full():
            self.audio_buffer.put(audio_chunk)

    def _process_worker(self):
        """ワーカースレッドの処理"""
        while True:
            chunk = self.audio_buffer.get()

            # 1. 高速ノイズ除去
            clean_chunk = self._fast_denoise(chunk)

            # 2. 特徴抽出（GPU）
            with torch.no_grad():
                features = self._extract_features(clean_chunk)

            # 3. キャッシュチェック
            speaker_id = self._identify_speaker_cached(features)

            # 4. 結果の返却
            self._emit_result(speaker_id, features)

    def _fast_denoise(self, audio):
        """軽量なノイズ除去"""
        # スペクトル減算（FFT最小限）
        return spectral_subtract_fast(audio)

    def _identify_speaker_cached(self, features):
        """キャッシュを使った話者識別"""
        # 特徴量のハッシュ
        feat_hash = hash(features.tobytes())

        if feat_hash in self.speaker_cache:
            return self.speaker_cache[feat_hash]

        # 新規計算
        speaker_id = self.model(features)
        self.speaker_cache[feat_hash] = speaker_id

        return speaker_id

# 使用例
pipeline = OptimizedAudioPipeline()

# リアルタイム処理
for chunk in audio_stream:
    pipeline.process_stream(chunk)
</code></pre>

<p><strong>4. 性能指標とモニタリング</strong>：</p>

<ul>
<li><strong>レイテンシ</strong>: 入力から出力までの時間</li>
<li><strong>スループット</strong>: 単位時間あたりの処理数</li>
<li><strong>CPU/GPU使用率</strong>: リソース効率</li>
<li><strong>メモリ使用量</strong>: ピークとベースライン</li>
<li><strong>精度</strong>: 最適化による劣化の測定</li>
</ul>

<p><strong>まとめ</strong>：</p>
<p>リアルタイム処理の実現には、モデル・システム・アルゴリズムの各レベルでの最適化が必要です。特に以下が重要：</p>
<ol>
<li>軽量化（量子化、プルーニング）</li>
<li>並列処理（マルチスレッド、GPU）</li>
<li>ストリーミング処理（フレーム単位）</li>
<li>キャッシング（計算の再利用）</li>
<li>適応的処理（状況に応じた最適化）</li>
</ol>

</details>

<hr>

<h2>参考文献</h2>

<ol>
<li>Snyder, D., Garcia-Romero, D., Sell, G., Povey, D., & Khudanpur, S. (2018). <em>X-vectors: Robust DNN embeddings for speaker recognition</em>. ICASSP 2018.</li>
<li>Livingstone, S. R., & Russo, F. A. (2018). <em>The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)</em>. PLOS ONE.</li>
<li>Loizou, P. C. (2013). <em>Speech Enhancement: Theory and Practice</em> (2nd ed.). CRC Press.</li>
<li>Müller, M. (2015). <em>Fundamentals of Music Processing</em>. Springer.</li>
<li>Dehak, N., Kenny, P. J., Dehak, R., Dumouchel, P., & Ouellet, P. (2011). <em>Front-end factor analysis for speaker verification</em>. IEEE Transactions on Audio, Speech, and Language Processing.</li>
<li>Schuller, B., Steidl, S., & Batliner, A. (2009). <em>The INTERSPEECH 2009 emotion challenge</em>. INTERSPEECH 2009.</li>
<li>Boll, S. F. (1979). <em>Suppression of acoustic noise in speech using spectral subtraction</em>. IEEE Transactions on Acoustics, Speech, and Signal Processing.</li>
<li>Tzanetakis, G., & Cook, P. (2002). <em>Musical genre classification of audio signals</em>. IEEE Transactions on Speech and Audio Processing.</li>
</ol>

<div class="navigation">
    <a href="chapter4-asr-tts.html" class="nav-button">← 前の章: 音声認識・音声合成</a>
    <a href="index.html" class="nav-button">シリーズ目次に戻る →</a>
</div>

    </main>

    <footer>
        <p><strong>作成者</strong>: AI Terakoya Content Team</p>
        <p><strong>バージョン</strong>: 1.0 | <strong>作成日</strong>: 2025-10-21</p>
        <p><strong>ライセンス</strong>: Creative Commons BY 4.0</p>
        <p>© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>