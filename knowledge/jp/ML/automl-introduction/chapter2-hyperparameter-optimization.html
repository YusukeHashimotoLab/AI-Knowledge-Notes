<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬2ç« ï¼šãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ– - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/automl-introduction/chapter2-hyperparameter-optimization.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/automl-introduction/index.html">Automl</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 2</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬2ç« ï¼šãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</h1>
            <p class="subtitle">AutoMLã®æ ¸å¿ƒ - æœ€é©ãªè¨­å®šã‚’è‡ªå‹•æ¢ç´¢ã™ã‚‹</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 30-35åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 10å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã®åŸºç¤ã¨æ¢ç´¢æˆ¦ç•¥ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Optunaã‚’ä½¿ã£ãŸåŠ¹ç‡çš„ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Hyperoptã§ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’æ´»ç”¨ã§ãã‚‹</li>
<li>âœ… Ray Tuneã§åˆ†æ•£ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’å®Ÿè¡Œã§ãã‚‹</li>
<li>âœ… é«˜åº¦ãªHPOæ‰‹æ³•ï¼ˆASHAã€PBTã€Hyperbandï¼‰ã‚’ç†è§£ã—é©ç”¨ã§ãã‚‹</li>
<li>âœ… å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦æœ€é©åŒ–æˆ¦ç•¥ã‚’é¸æŠãƒ»å®Ÿè£…ã§ãã‚‹</li>
</ul>

<hr>

<h2>2.1 HPOã®åŸºç¤</h2>

<h3>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã¯</h3>
<p><strong>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆHyperparameterï¼‰</strong>ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’åˆ¶å¾¡ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã€å­¦ç¿’å‰ã«è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚</p>

<blockquote>
<p>ã€Œãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å­¦ç¿’ä¸­ã«æœ€é©åŒ–ã•ã‚Œã‚‹ãŒã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å­¦ç¿’å‰ã«äººé–“ãŒè¨­å®šã™ã‚‹ã€</p>
</blockquote>

<h3>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¾‹</h3>

<table>
<thead>
<tr>
<th>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </th>
<th>ä¸»è¦ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</th>
<th>å½±éŸ¿</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ</strong></td>
<td>n_estimators, max_depth, min_samples_split</td>
<td>æ€§èƒ½ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã€éå­¦ç¿’</td>
</tr>
<tr>
<td><strong>å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°</strong></td>
<td>learning_rate, n_estimators, max_depth</td>
<td>åæŸé€Ÿåº¦ã€æ€§èƒ½ã€éå­¦ç¿’</td>
</tr>
<tr>
<td><strong>SVM</strong></td>
<td>C, kernel, gamma</td>
<td>æ±ºå®šå¢ƒç•Œã€æ±åŒ–æ€§èƒ½</td>
</tr>
<tr>
<td><strong>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ</strong></td>
<td>learning_rate, batch_size, hidden_units</td>
<td>åæŸã€æ€§èƒ½ã€è¨ˆç®—åŠ¹ç‡</td>
</tr>
</tbody>
</table>

<h3>æ¢ç´¢ç©ºé–“ã®å®šç¾©</h3>

<p>æ¢ç´¢ç©ºé–“ã¯ã€å„ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå–ã‚Šã†ã‚‹å€¤ã®ç¯„å›²ã¨åˆ†å¸ƒã‚’å®šç¾©ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">import numpy as np

# æ¢ç´¢ç©ºé–“ã®ä¾‹
search_space = {
    # æ•´æ•°å‹ï¼šæœ¨ã®æ•°ï¼ˆ50ã‹ã‚‰500ã¾ã§ï¼‰
    'n_estimators': (50, 500),

    # æ•´æ•°å‹ï¼šæœ¨ã®æ·±ã•ï¼ˆ3ã‹ã‚‰20ã¾ã§ï¼‰
    'max_depth': (3, 20),

    # å®Ÿæ•°å‹ï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰ï¼šå­¦ç¿’ç‡
    'learning_rate': (1e-4, 1e-1, 'log'),

    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å‹ï¼šãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¿ã‚¤ãƒ—
    'boosting_type': ['gbdt', 'dart', 'goss'],

    # å®Ÿæ•°å‹ï¼ˆç·šå½¢ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰ï¼šæ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    'reg_alpha': (0.0, 10.0),
}

print("=== æ¢ç´¢ç©ºé–“ã®å®šç¾© ===")
for param, space in search_space.items():
    print(f"{param}: {space}")
</code></pre>

<h3>Grid Search vs Random Search</h3>

<h4>Grid Searchï¼ˆæ ¼å­æ¢ç´¢ï¼‰</h4>

<p>ã™ã¹ã¦ã®çµ„ã¿åˆã‚ã›ã‚’ç¶²ç¾…çš„ã«æ¢ç´¢ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import accuracy_score
import time

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
X, y = make_classification(n_samples=1000, n_features=20,
                          n_informative=15, n_redundant=5,
                          random_state=42)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Grid Searchã®æ¢ç´¢ç©ºé–“
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, 20],
    'min_samples_split': [2, 5, 10]
}

print("=== Grid Search ===")
print(f"æ¢ç´¢ã™ã‚‹çµ„ã¿åˆã‚ã›æ•°: {3 * 4 * 3} = 36é€šã‚Š")

# Grid Searchå®Ÿè¡Œ
start_time = time.time()
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=3,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)
grid_search.fit(X_train, y_train)
grid_time = time.time() - start_time

# çµæœ
best_params = grid_search.best_params_
best_score = grid_search.best_score_
test_score = accuracy_score(y_test, grid_search.predict(X_test))

print(f"\næœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {best_params}")
print(f"CVç²¾åº¦: {best_score:.4f}")
print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_score:.4f}")
print(f"å®Ÿè¡Œæ™‚é–“: {grid_time:.2f}ç§’")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Grid Search ===
æ¢ç´¢ã™ã‚‹çµ„ã¿åˆã‚ã›æ•°: 3 * 4 * 3 = 36é€šã‚Š
Fitting 3 folds for each of 36 candidates, totalling 108 fits

æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 200}
CVç²¾åº¦: 0.9162
ãƒ†ã‚¹ãƒˆç²¾åº¦: 0.9200
å®Ÿè¡Œæ™‚é–“: 12.34ç§’
</code></pre>

<h4>Random Searchï¼ˆãƒ©ãƒ³ãƒ€ãƒ æ¢ç´¢ï¼‰</h4>

<p>ãƒ©ãƒ³ãƒ€ãƒ ã«çµ„ã¿åˆã‚ã›ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦æ¢ç´¢ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# Random Searchã®æ¢ç´¢ç©ºé–“ï¼ˆåˆ†å¸ƒã§æŒ‡å®šï¼‰
param_distributions = {
    'n_estimators': randint(50, 300),
    'max_depth': randint(5, 25),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': uniform(0.3, 0.7)  # 0.3ã‹ã‚‰1.0ã®ç¯„å›²
}

print("\n=== Random Search ===")
print(f"ãƒ©ãƒ³ãƒ€ãƒ ã«è©¦è¡Œã™ã‚‹å›æ•°: 50å›")

# Random Searchå®Ÿè¡Œ
start_time = time.time()
random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_distributions,
    n_iter=50,  # 50å›ã®ãƒ©ãƒ³ãƒ€ãƒ è©¦è¡Œ
    cv=3,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42,
    verbose=1
)
random_search.fit(X_train, y_train)
random_time = time.time() - start_time

# çµæœ
best_params = random_search.best_params_
best_score = random_search.best_score_
test_score = accuracy_score(y_test, random_search.predict(X_test))

print(f"\næœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {best_params}")
print(f"CVç²¾åº¦: {best_score:.4f}")
print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_score:.4f}")
print(f"å®Ÿè¡Œæ™‚é–“: {random_time:.2f}ç§’")

# æ¯”è¼ƒ
print(f"\n=== Grid vs Random æ¯”è¼ƒ ===")
print(f"Grid Search: {grid_time:.2f}ç§’ã§ç²¾åº¦{test_score:.4f}")
print(f"Random Search: {random_time:.2f}ç§’ã§ç²¾åº¦{test_score:.4f}")
print(f"æ™‚é–“çŸ­ç¸®: {(1 - random_time/grid_time)*100:.1f}%")
</code></pre>

<h3>æ¢ç´¢æˆ¦ç•¥ã®åˆ†é¡</h3>

<div class="mermaid">
graph TD
    A[HPOæˆ¦ç•¥] --> B[å˜ç´”æ¢ç´¢]
    A --> C[é©å¿œçš„æ¢ç´¢]
    A --> D[å¤šæ®µéšæ¢ç´¢]

    B --> B1[Grid Search]
    B --> B2[Random Search]

    C --> C1[ãƒ™ã‚¤ã‚ºæœ€é©åŒ–]
    C --> C2[é€²åŒ–çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ]
    C --> C3[ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ]

    D --> D1[Hyperband]
    D --> D2[ASHA]
    D --> D3[PBT]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
</div>

<h3>Early Stoppingï¼ˆæ—©æœŸåœæ­¢ï¼‰</h3>

<p>å­¦ç¿’ä¸­ã«æ€§èƒ½ãŒæ”¹å–„ã—ãªã„å ´åˆã«å­¦ç¿’ã‚’æ—©æœŸçµ‚äº†ã—ã€è¨ˆç®—è³‡æºã‚’ç¯€ç´„ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">import lightgbm as lgb
from sklearn.metrics import accuracy_score

# LightGBMã§Early Stoppingã®ä¾‹
print("\n=== Early Stopping ãƒ‡ãƒ¢ ===")

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
train_data = lgb.Dataset(X_train, label=y_train)
valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
params = {
    'objective': 'binary',
    'metric': 'binary_logloss',
    'learning_rate': 0.05,
    'num_leaves': 31,
    'verbose': -1
}

# Early Stoppingã‚ã‚Š
print("\nEarly Stoppingã‚ã‚Š:")
model_es = lgb.train(
    params,
    train_data,
    num_boost_round=1000,
    valid_sets=[train_data, valid_data],
    valid_names=['train', 'valid'],
    callbacks=[
        lgb.early_stopping(stopping_rounds=50),
        lgb.log_evaluation(period=100)
    ]
)
print(f"å®Ÿéš›ã®å­¦ç¿’ãƒ©ã‚¦ãƒ³ãƒ‰æ•°: {model_es.best_iteration}")

# Early Stoppingãªã—
print("\nEarly Stoppingãªã—:")
model_no_es = lgb.train(
    params,
    train_data,
    num_boost_round=200,
    valid_sets=[train_data, valid_data],
    valid_names=['train', 'valid'],
    callbacks=[lgb.log_evaluation(period=100)]
)

# æ¯”è¼ƒ
pred_es = (model_es.predict(X_test) > 0.5).astype(int)
pred_no_es = (model_no_es.predict(X_test) > 0.5).astype(int)

print(f"\nç²¾åº¦ï¼ˆEarly Stoppingï¼‰: {accuracy_score(y_test, pred_es):.4f}")
print(f"ç²¾åº¦ï¼ˆ200ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰: {accuracy_score(y_test, pred_no_es):.4f}")
print(f"è¨ˆç®—æ™‚é–“å‰Šæ¸›: {(1 - model_es.best_iteration/200)*100:.1f}%")
</code></pre>

<hr>

<h2>2.2 Optuna</h2>

<h3>Optunaã®ç‰¹å¾´</h3>

<p><strong>Optuna</strong>ã¯ã€æ¬¡ä¸–ä»£ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚</p>

<table>
<thead>
<tr>
<th>ç‰¹å¾´</th>
<th>èª¬æ˜</th>
<th>åˆ©ç‚¹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Define-by-run API</strong></td>
<td>å‹•çš„ã«æ¢ç´¢ç©ºé–“ã‚’å®šç¾©</td>
<td>æŸ”è»Ÿã§ç›´æ„Ÿçš„ãªã‚³ãƒ¼ãƒ‰</td>
</tr>
<tr>
<td><strong>Pruning</strong></td>
<td>è¦‹è¾¼ã¿ã®ãªã„è©¦è¡Œã‚’æ—©æœŸçµ‚äº†</td>
<td>å¤§å¹…ãªæ™‚é–“çŸ­ç¸®</td>
</tr>
<tr>
<td><strong>ä¸¦åˆ—åŒ–</strong></td>
<td>è¤‡æ•°ã®è©¦è¡Œã‚’åŒæ™‚å®Ÿè¡Œ</td>
<td>é«˜é€ŸåŒ–</td>
</tr>
<tr>
<td><strong>å¯è¦–åŒ–</strong></td>
<td>æœ€é©åŒ–éç¨‹ã®è©³ç´°ãªå¯è¦–åŒ–</td>
<td>ç†è§£ã¨è¨ºæ–­ãŒå®¹æ˜“</td>
</tr>
</tbody>
</table>

<h3>Study and Trial</h3>

<p>Optunaã®åŸºæœ¬æ¦‚å¿µï¼š</p>

<ul>
<li><strong>Study</strong>: æœ€é©åŒ–ã‚¿ã‚¹ã‚¯å…¨ä½“ã‚’ç®¡ç†</li>
<li><strong>Trial</strong>: å€‹ã€…ã®è©¦è¡Œï¼ˆ1ã¤ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šï¼‰</li>
</ul>

<pre><code class="language-python">import optuna
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
import numpy as np

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X, y = make_classification(n_samples=1000, n_features=20,
                          n_informative=15, random_state=42)

# ç›®çš„é–¢æ•°ã®å®šç¾©
def objective(trial):
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ææ¡ˆ
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'max_depth': trial.suggest_int('max_depth', 3, 20),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
        'max_features': trial.suggest_float('max_features', 0.3, 1.0),
        'random_state': 42
    }

    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨è©•ä¾¡
    model = RandomForestClassifier(**params)
    score = cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()

    return score

# Studyã®ä½œæˆã¨æœ€é©åŒ–
print("=== Optuna åŸºæœ¬ä¾‹ ===")
study = optuna.create_study(
    direction='maximize',  # æœ€å¤§åŒ–
    study_name='random_forest_optimization'
)

# æœ€é©åŒ–å®Ÿè¡Œ
study.optimize(objective, n_trials=50, show_progress_bar=True)

# çµæœ
print(f"\næœ€é©ãªç²¾åº¦: {study.best_value:.4f}")
print(f"æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
for key, value in study.best_params.items():
    print(f"  {key}: {value}")

# çµ±è¨ˆæƒ…å ±
print(f"\nç·è©¦è¡Œå›æ•°: {len(study.trials)}")
print(f"å®Œäº†ã—ãŸè©¦è¡Œ: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}")
</code></pre>

<h3>Samplersï¼ˆã‚µãƒ³ãƒ—ãƒ©ãƒ¼ï¼‰</h3>

<p>Optunaã¯è¤‡æ•°ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚</p>

<h4>TPE (Tree-structured Parzen Estimator)</h4>

<p>ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã§ã€ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ä¸€ç¨®ã§ã™ã€‚</p>

<pre><code class="language-python">from optuna.samplers import TPESampler

# TPEã‚µãƒ³ãƒ—ãƒ©ãƒ¼
print("\n=== TPE Sampler ===")
study_tpe = optuna.create_study(
    direction='maximize',
    sampler=TPESampler(seed=42)
)
study_tpe.optimize(objective, n_trials=30)

print(f"TPEæœ€é©ç²¾åº¦: {study_tpe.best_value:.4f}")
</code></pre>

<h4>CMA-ES (Covariance Matrix Adaptation Evolution Strategy)</h4>

<p>é€²åŒ–æˆ¦ç•¥ã«åŸºã¥ãã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã§ã™ã€‚</p>

<pre><code class="language-python">from optuna.samplers import CmaEsSampler

# CMA-ESã‚µãƒ³ãƒ—ãƒ©ãƒ¼
print("\n=== CMA-ES Sampler ===")
study_cmaes = optuna.create_study(
    direction='maximize',
    sampler=CmaEsSampler(seed=42)
)
study_cmaes.optimize(objective, n_trials=30)

print(f"CMA-ESæœ€é©ç²¾åº¦: {study_cmaes.best_value:.4f}")
</code></pre>

<h3>Pruning Strategiesï¼ˆæåˆˆã‚Šæˆ¦ç•¥ï¼‰</h3>

<p>è¦‹è¾¼ã¿ã®ãªã„è©¦è¡Œã‚’æ—©æœŸã«æ‰“ã¡åˆ‡ã‚‹ã“ã¨ã§ã€è¨ˆç®—æ™‚é–“ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">from optuna.pruners import MedianPruner
import lightgbm as lgb
from sklearn.model_selection import train_test_split

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X, y = make_classification(n_samples=5000, n_features=50,
                          n_informative=30, random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Pruningã‚’ä½¿ã†ç›®çš„é–¢æ•°
def objective_with_pruning(trial):
    params = {
        'objective': 'binary',
        'metric': 'binary_logloss',
        'verbosity': -1,
        'boosting_type': 'gbdt',
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'num_leaves': trial.suggest_int('num_leaves', 20, 200),
        'max_depth': trial.suggest_int('max_depth', 3, 15),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
    }

    # LightGBMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)

    # Pruningã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, 'binary_logloss')

    # å­¦ç¿’
    model = lgb.train(
        params,
        train_data,
        num_boost_round=1000,
        valid_sets=[valid_data],
        valid_names=['valid'],
        callbacks=[pruning_callback, lgb.log_evaluation(period=0)]
    )

    # è©•ä¾¡
    preds = model.predict(X_valid)
    accuracy = accuracy_score(y_valid, (preds > 0.5).astype(int))

    return accuracy

# Prunerã‚’ä½¿ã£ãŸStudy
print("\n=== Pruning ãƒ‡ãƒ¢ ===")
study_pruning = optuna.create_study(
    direction='maximize',
    pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10)
)

study_pruning.optimize(objective_with_pruning, n_trials=30, timeout=60)

# çµ±è¨ˆ
n_complete = len([t for t in study_pruning.trials if t.state == optuna.trial.TrialState.COMPLETE])
n_pruned = len([t for t in study_pruning.trials if t.state == optuna.trial.TrialState.PRUNED])

print(f"\næœ€é©ç²¾åº¦: {study_pruning.best_value:.4f}")
print(f"å®Œäº†ã—ãŸè©¦è¡Œ: {n_complete}")
print(f"æåˆˆã‚Šã•ã‚ŒãŸè©¦è¡Œ: {n_pruned}")
print(f"æåˆˆã‚Šç‡: {n_pruned/(n_complete+n_pruned)*100:.1f}%")
</code></pre>

<h3>Complete Optuna Example</h3>

<p>Optunaã‚’ä½¿ã£ãŸå®Œå…¨ãªæœ€é©åŒ–ä¾‹ã§ã™ã€‚</p>

<pre><code class="language-python">import optuna
from optuna.visualization import (
    plot_optimization_history,
    plot_param_importances,
    plot_parallel_coordinate
)
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import GradientBoostingClassifier
import warnings
warnings.filterwarnings('ignore')

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
data = load_breast_cancer()
X, y = data.data, data.target

# ç›®çš„é–¢æ•°
def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 500),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),
        'random_state': 42
    }

    model = GradientBoostingClassifier(**params)
    score = cross_val_score(model, X, y, cv=5, scoring='accuracy', n_jobs=-1).mean()

    return score

# Studyã®ä½œæˆ
print("\n=== Complete Optuna Example ===")
study = optuna.create_study(
    direction='maximize',
    sampler=TPESampler(seed=42),
    pruner=MedianPruner(n_startup_trials=10, n_warmup_steps=5),
    study_name='breast_cancer_optimization'
)

# æœ€é©åŒ–å®Ÿè¡Œ
study.optimize(objective, n_trials=100, show_progress_bar=True)

# çµæœè¡¨ç¤º
print(f"\n=== æœ€é©åŒ–çµæœ ===")
print(f"æœ€é©ç²¾åº¦: {study.best_value:.4f}")
print(f"\næœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
for key, value in study.best_params.items():
    print(f"  {key}: {value}")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡è¦åº¦
print(f"\n=== ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡è¦åº¦ ===")
importances = optuna.importance.get_param_importances(study)
for param, importance in sorted(importances.items(), key=lambda x: x[1], reverse=True):
    print(f"  {param}: {importance:.4f}")

# å¯è¦–åŒ–ã¯å®Ÿéš›ã®ç’°å¢ƒã§è¡¨ç¤ºã•ã‚Œã¾ã™
# plot_optimization_history(study).show()
# plot_param_importances(study).show()
# plot_parallel_coordinate(study).show()

print("\nâœ“ æœ€é©åŒ–å®Œäº†")
</code></pre>

<hr>

<h2>2.3 Hyperopt</h2>

<h3>Tree-structured Parzen Estimator (TPE)</h3>

<p><strong>Hyperopt</strong>ã¯ã€TPEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç”¨ã„ãŸãƒ™ã‚¤ã‚ºæœ€é©åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚</p>

<pre><code class="language-python">from hyperopt import fmin, tpe, hp, Trials, STATUS_OK
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.datasets import make_classification
import numpy as np

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X, y = make_classification(n_samples=1000, n_features=20,
                          n_informative=15, random_state=42)

# æ¢ç´¢ç©ºé–“ã®å®šç¾©
space = {
    'n_estimators': hp.quniform('n_estimators', 50, 300, 1),
    'max_depth': hp.quniform('max_depth', 3, 20, 1),
    'min_samples_split': hp.quniform('min_samples_split', 2, 20, 1),
    'min_samples_leaf': hp.quniform('min_samples_leaf', 1, 10, 1),
    'max_features': hp.uniform('max_features', 0.3, 1.0)
}

# ç›®çš„é–¢æ•°ï¼ˆHyperoptã¯æœ€å°åŒ–ã™ã‚‹ãŸã‚ã€è² ã®ç²¾åº¦ã‚’è¿”ã™ï¼‰
def objective(params):
    # æ•´æ•°å‹ã«å¤‰æ›
    params['n_estimators'] = int(params['n_estimators'])
    params['max_depth'] = int(params['max_depth'])
    params['min_samples_split'] = int(params['min_samples_split'])
    params['min_samples_leaf'] = int(params['min_samples_leaf'])

    model = RandomForestClassifier(**params, random_state=42)
    score = cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()

    return {'loss': -score, 'status': STATUS_OK}

# æœ€é©åŒ–å®Ÿè¡Œ
print("=== Hyperopt TPE ===")
trials = Trials()
best = fmin(
    fn=objective,
    space=space,
    algo=tpe.suggest,
    max_evals=50,
    trials=trials,
    rstate=np.random.default_rng(42)
)

print(f"\næœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {best}")
print(f"æœ€é©ç²¾åº¦: {-min([trial['result']['loss'] for trial in trials.trials]):.4f}")

# æœ€é©åŒ–å±¥æ­´
losses = [trial['result']['loss'] for trial in trials.trials]
print(f"\nè©¦è¡Œå›æ•°: {len(trials.trials)}")
print(f"æœ€è‰¯ã‚¹ã‚³ã‚¢ã®æ¨ç§»:")
best_so_far = []
for i, loss in enumerate(losses):
    if i == 0:
        best_so_far.append(loss)
    else:
        best_so_far.append(min(best_so_far[-1], loss))
print(f"  é–‹å§‹: {-best_so_far[0]:.4f}")
print(f"  çµ‚äº†: {-best_so_far[-1]:.4f}")
print(f"  æ”¹å–„: {(-best_so_far[-1] + best_so_far[0]):.4f}")
</code></pre>

<h3>Search Space Definition</h3>

<p>Hyperoptã¯æŸ”è»Ÿãªæ¢ç´¢ç©ºé–“å®šç¾©ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚</p>

<pre><code class="language-python">from hyperopt import hp

# å„ç¨®åˆ†å¸ƒã®å®šç¾©
search_space_detailed = {
    # ä¸€æ§˜åˆ†å¸ƒï¼ˆé€£ç¶šå€¤ï¼‰
    'uniform_param': hp.uniform('uniform_param', 0.0, 1.0),

    # ä¸€æ§˜åˆ†å¸ƒï¼ˆé›¢æ•£å€¤ï¼‰
    'quniform_param': hp.quniform('quniform_param', 10, 100, 5),  # 10, 15, 20, ...

    # å¯¾æ•°ä¸€æ§˜åˆ†å¸ƒ
    'loguniform_param': hp.loguniform('loguniform_param', np.log(0.001), np.log(1.0)),

    # æ­£è¦åˆ†å¸ƒ
    'normal_param': hp.normal('normal_param', 0, 1),

    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«
    'choice_param': hp.choice('choice_param', ['option1', 'option2', 'option3']),

    # æ¡ä»¶ä»˜ãæ¢ç´¢ç©ºé–“
    'classifier_type': hp.choice('classifier_type', [
        {
            'type': 'random_forest',
            'n_estimators': hp.quniform('rf_n_estimators', 50, 300, 1),
            'max_depth': hp.quniform('rf_max_depth', 3, 20, 1)
        },
        {
            'type': 'gradient_boosting',
            'n_estimators': hp.quniform('gb_n_estimators', 50, 300, 1),
            'learning_rate': hp.loguniform('gb_learning_rate', np.log(0.01), np.log(0.3))
        }
    ])
}

print("=== Hyperopt æ¢ç´¢ç©ºé–“ã®ä¾‹ ===")
for key, value in search_space_detailed.items():
    print(f"{key}: {value}")
</code></pre>

<h3>Trials Database</h3>

<p>Trialsã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯ã€ã™ã¹ã¦ã®è©¦è¡Œå±¥æ­´ã‚’ä¿å­˜ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">from hyperopt import Trials
import pandas as pd

# Trialsæƒ…å ±ã®è©³ç´°åˆ†æ
print("\n=== Trials è©³ç´°åˆ†æ ===")

# DataFrameã«å¤‰æ›
trials_df = pd.DataFrame([
    {
        'trial_id': i,
        'loss': trial['result']['loss'],
        **{k: v[0] if isinstance(v, (list, np.ndarray)) else v
           for k, v in trial['misc']['vals'].items() if v}
    }
    for i, trial in enumerate(trials.trials)
])

print("\nTop 5 è©¦è¡Œ:")
print(trials_df.nsmallest(5, 'loss')[['trial_id', 'loss', 'n_estimators', 'max_depth']])

print("\nãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ±è¨ˆ:")
print(trials_df.describe())
</code></pre>

<h3>Hyperopt Integration</h3>

<p>Hyperoptã¨æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®çµ±åˆä¾‹ã§ã™ã€‚</p>

<pre><code class="language-python">from hyperopt import fmin, tpe, hp, Trials, STATUS_OK
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X, y = make_classification(n_samples=5000, n_features=50,
                          n_informative=30, random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# LightGBMç”¨ã®æ¢ç´¢ç©ºé–“
lgb_space = {
    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.3)),
    'num_leaves': hp.quniform('num_leaves', 20, 200, 1),
    'max_depth': hp.quniform('max_depth', 3, 15, 1),
    'min_child_samples': hp.quniform('min_child_samples', 5, 100, 1),
    'subsample': hp.uniform('subsample', 0.5, 1.0),
    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),
    'reg_alpha': hp.uniform('reg_alpha', 0.0, 10.0),
    'reg_lambda': hp.uniform('reg_lambda', 0.0, 10.0),
}

# ç›®çš„é–¢æ•°
def lgb_objective(params):
    # æ•´æ•°å‹ã«å¤‰æ›
    params['num_leaves'] = int(params['num_leaves'])
    params['max_depth'] = int(params['max_depth'])
    params['min_child_samples'] = int(params['min_child_samples'])

    # LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    lgb_params = {
        **params,
        'objective': 'binary',
        'metric': 'binary_logloss',
        'verbosity': -1,
        'boosting_type': 'gbdt',
    }

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)

    # å­¦ç¿’
    model = lgb.train(
        lgb_params,
        train_data,
        num_boost_round=1000,
        valid_sets=[valid_data],
        callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(period=0)]
    )

    # è©•ä¾¡
    preds = model.predict(X_valid)
    accuracy = accuracy_score(y_valid, (preds > 0.5).astype(int))

    return {'loss': -accuracy, 'status': STATUS_OK}

# æœ€é©åŒ–
print("\n=== Hyperopt + LightGBM çµ±åˆ ===")
lgb_trials = Trials()
best_lgb = fmin(
    fn=lgb_objective,
    space=lgb_space,
    algo=tpe.suggest,
    max_evals=50,
    trials=lgb_trials,
    rstate=np.random.default_rng(42)
)

print(f"\næœ€é©ç²¾åº¦: {-min([trial['result']['loss'] for trial in lgb_trials.trials]):.4f}")
print(f"æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
for key, value in best_lgb.items():
    print(f"  {key}: {value}")
</code></pre>

<hr>

<h2>2.4 Ray Tune</h2>

<h3>Tune API</h3>

<p><strong>Ray Tune</strong>ã¯ã€åˆ†æ•£ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã®ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚</p>

<pre><code class="language-python">from ray import tune
from ray.tune.schedulers import ASHAScheduler
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
import warnings
warnings.filterwarnings('ignore')

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X, y = make_classification(n_samples=1000, n_features=20,
                          n_informative=15, random_state=42)

# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–¢æ•°
def train_model(config):
    model = RandomForestClassifier(
        n_estimators=config['n_estimators'],
        max_depth=config['max_depth'],
        min_samples_split=config['min_samples_split'],
        random_state=42
    )

    score = cross_val_score(model, X, y, cv=3, scoring='accuracy').mean()

    # Ray Tuneã«çµæœã‚’å ±å‘Š
    tune.report(accuracy=score)

# æ¢ç´¢ç©ºé–“ã®å®šç¾©
config = {
    'n_estimators': tune.randint(50, 300),
    'max_depth': tune.randint(3, 20),
    'min_samples_split': tune.randint(2, 20)
}

# æœ€é©åŒ–å®Ÿè¡Œ
print("=== Ray Tune åŸºæœ¬ä¾‹ ===")
analysis = tune.run(
    train_model,
    config=config,
    num_samples=20,  # è©¦è¡Œå›æ•°
    resources_per_trial={'cpu': 1},
    verbose=1
)

# çµæœ
best_config = analysis.get_best_config(metric='accuracy', mode='max')
print(f"\næœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {best_config}")
print(f"æœ€é©ç²¾åº¦: {analysis.best_result['accuracy']:.4f}")
</code></pre>

<h3>Schedulersï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ï¼‰</h3>

<h4>ASHA (Async Successive Halving Algorithm)</h4>

<p>ASHAã¯ã€æ€§èƒ½ã®ä½ã„è©¦è¡Œã‚’æ—©æœŸã«æ‰“ã¡åˆ‡ã‚Šã€æœ‰æœ›ãªè©¦è¡Œã«ãƒªã‚½ãƒ¼ã‚¹ã‚’é›†ä¸­ã•ã›ã¾ã™ã€‚</p>

<pre><code class="language-python">from ray.tune.schedulers import ASHAScheduler
from ray import tune
import numpy as np

# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–¢æ•°ï¼ˆã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰
def train_with_iterations(config):
    # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã”ã¨ã«æ€§èƒ½ãŒå‘ä¸Š
    base_score = np.random.rand()

    for iteration in range(config['max_iterations']):
        # å­¦ç¿’æ›²ç·šã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        score = base_score + (1 - base_score) * (1 - np.exp(-iteration / 20))
        score += np.random.randn() * 0.01  # ãƒã‚¤ã‚º

        # å ±å‘Š
        tune.report(accuracy=score, iteration=iteration)

# ASHAã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
asha_scheduler = ASHAScheduler(
    time_attr='iteration',
    metric='accuracy',
    mode='max',
    max_t=100,  # æœ€å¤§ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    grace_period=10,  # æœ€ä½é™å®Ÿè¡Œã™ã‚‹ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    reduction_factor=3  # å‰Šæ¸›ç‡
)

# æ¢ç´¢ç©ºé–“
config_asha = {
    'learning_rate': tune.loguniform(1e-4, 1e-1),
    'batch_size': tune.choice([16, 32, 64, 128]),
    'max_iterations': 100
}

print("\n=== ASHA Scheduler ===")
analysis_asha = tune.run(
    train_with_iterations,
    config=config_asha,
    num_samples=30,
    scheduler=asha_scheduler,
    resources_per_trial={'cpu': 1},
    verbose=1
)

print(f"\næœ€é©ç²¾åº¦: {analysis_asha.best_result['accuracy']:.4f}")
print(f"å®Œäº†ã—ãŸè©¦è¡Œæ•°: {len(analysis_asha.trials)}")
</code></pre>

<h4>PBT (Population Based Training)</h4>

<p>PBTã¯ã€é›†å›£ãƒ™ãƒ¼ã‚¹ã®é€²åŒ–çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‹•çš„ã«èª¿æ•´ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">from ray.tune.schedulers import PopulationBasedTraining

# PBTã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
pbt_scheduler = PopulationBasedTraining(
    time_attr='iteration',
    metric='accuracy',
    mode='max',
    perturbation_interval=5,  # æ‘‚å‹•é–“éš”
    hyperparam_mutations={
        'learning_rate': lambda: np.random.uniform(1e-4, 1e-1),
        'batch_size': [16, 32, 64, 128]
    }
)

print("\n=== PBT Scheduler ===")
print("PBTã¯å‹•çš„ã«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã—ã¾ã™")
print("- æ€§èƒ½ã®è‰¯ã„ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã‚’ä»–ã®ãƒ¢ãƒ‡ãƒ«ã«ã‚³ãƒ”ãƒ¼")
print("- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å°ã•ãªå¤‰å‹•ã‚’åŠ ãˆã‚‹")
print("- é›†å›£å…¨ä½“ã§æœ€é©åŒ–ã‚’é€²ã‚ã‚‹")
</code></pre>

<h3>Integration with PyTorch/TensorFlow</h3>

<p>Ray Tuneã¯PyTorchã‚„TensorFlowã¨ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«çµ±åˆã§ãã¾ã™ã€‚</p>

<pre><code class="language-python">from ray import tune
from ray.tune.schedulers import ASHAScheduler
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
class SimpleNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
X_tensor = torch.FloatTensor(X)
y_tensor = torch.LongTensor(y)
dataset = TensorDataset(X_tensor, y_tensor)

# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–¢æ•°
def train_pytorch(config):
    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    model = SimpleNet(
        input_size=20,
        hidden_size=config['hidden_size'],
        output_size=2
    )

    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
    optimizer = optim.Adam(model.parameters(), lr=config['lr'])
    criterion = nn.CrossEntropyLoss()

    # DataLoader
    train_loader = DataLoader(
        dataset,
        batch_size=config['batch_size'],
        shuffle=True
    )

    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—
    for epoch in range(10):
        model.train()
        total_loss = 0
        correct = 0
        total = 0

        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

        accuracy = correct / total
        tune.report(accuracy=accuracy, loss=total_loss/len(train_loader))

# æ¢ç´¢ç©ºé–“
pytorch_config = {
    'hidden_size': tune.choice([32, 64, 128, 256]),
    'lr': tune.loguniform(1e-4, 1e-2),
    'batch_size': tune.choice([16, 32, 64])
}

print("\n=== Ray Tune + PyTorch çµ±åˆ ===")
analysis_pytorch = tune.run(
    train_pytorch,
    config=pytorch_config,
    num_samples=10,
    resources_per_trial={'cpu': 1},
    verbose=1
)

print(f"\næœ€é©ç²¾åº¦: {analysis_pytorch.best_result['accuracy']:.4f}")
print(f"æœ€é©è¨­å®š: {analysis_pytorch.get_best_config(metric='accuracy', mode='max')}")
</code></pre>

<h3>Distributed HPO</h3>

<p>Ray Tuneã¯è‡ªå‹•çš„ã«è¤‡æ•°ã®CPU/GPUã«å‡¦ç†ã‚’åˆ†æ•£ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">import ray

# RayåˆæœŸåŒ–ï¼ˆè¤‡æ•°CPUã‚’ä½¿ç”¨ï¼‰
ray.init(num_cpus=4, ignore_reinit_error=True)

# åˆ†æ•£å®Ÿè¡Œã®è¨­å®š
distributed_config = {
    'n_estimators': tune.randint(50, 300),
    'max_depth': tune.randint(3, 20),
    'min_samples_split': tune.randint(2, 20)
}

print("\n=== åˆ†æ•£ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ– ===")
print("4ã¤ã®CPUã‚³ã‚¢ã§ä¸¦åˆ—å®Ÿè¡Œ")

# ä¸¦åˆ—å®Ÿè¡Œ
analysis_distributed = tune.run(
    train_model,
    config=distributed_config,
    num_samples=40,
    resources_per_trial={'cpu': 1},  # 1è©¦è¡Œã‚ãŸã‚Š1CPU
    verbose=1
)

print(f"\næœ€é©ç²¾åº¦: {analysis_distributed.best_result['accuracy']:.4f}")
print(f"ç·è©¦è¡Œæ•°: {len(analysis_distributed.trials)}")

# ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
ray.shutdown()
</code></pre>

<hr>

<h2>2.5 é«˜åº¦ãªHPOæ‰‹æ³•</h2>

<h3>Bayesian Optimizationï¼ˆãƒ™ã‚¤ã‚ºæœ€é©åŒ–ï¼‰</h3>

<p>ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã¯ã€éå»ã®è©¦è¡Œçµæœã‚’æ´»ç”¨ã—ã¦æ¬¡ã®æ¢ç´¢ç‚¹ã‚’é¸æŠã—ã¾ã™ã€‚</p>

<pre><code class="language-python">from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern
from scipy.stats import norm
import numpy as np

class BayesianOptimizer:
    def __init__(self, bounds, n_init=5):
        self.bounds = bounds
        self.n_init = n_init
        self.X_obs = []
        self.y_obs = []
        self.gp = GaussianProcessRegressor(
            kernel=Matern(nu=2.5),
            alpha=1e-6,
            normalize_y=True,
            n_restarts_optimizer=5,
            random_state=42
        )

    def acquisition_function(self, X, xi=0.01):
        """Expected Improvement (EI)"""
        mu, sigma = self.gp.predict(X, return_std=True)

        if len(self.y_obs) == 0:
            return np.zeros_like(mu)

        mu_best = np.max(self.y_obs)

        with np.errstate(divide='warn'):
            imp = mu - mu_best - xi
            Z = imp / sigma
            ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)
            ei[sigma == 0.0] = 0.0

        return ei

    def propose_location(self):
        """æ¬¡ã®æ¢ç´¢ç‚¹ã‚’ææ¡ˆ"""
        if len(self.X_obs) < self.n_init:
            # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            return np.random.uniform(self.bounds[0], self.bounds[1])

        # Acquisition Functionã‚’æœ€å¤§åŒ–
        X_random = np.random.uniform(
            self.bounds[0], self.bounds[1], size=(1000, 1)
        )
        ei = self.acquisition_function(X_random)
        return X_random[np.argmax(ei)]

    def observe(self, X, y):
        """è¦³æ¸¬çµæœã‚’è¨˜éŒ²"""
        self.X_obs.append(X)
        self.y_obs.append(y)

        if len(self.X_obs) >= self.n_init:
            self.gp.fit(np.array(self.X_obs), np.array(self.y_obs))

# ãƒ†ã‚¹ãƒˆé–¢æ•°ï¼ˆæœ€é©åŒ–å¯¾è±¡ï¼‰
def test_function(x):
    """1æ¬¡å…ƒã®ãƒ†ã‚¹ãƒˆé–¢æ•°"""
    return -(x - 2) ** 2 + 5 + np.random.randn() * 0.1

# ãƒ™ã‚¤ã‚ºæœ€é©åŒ–å®Ÿè¡Œ
print("=== Bayesian Optimization ãƒ‡ãƒ¢ ===")
optimizer = BayesianOptimizer(bounds=(0, 5), n_init=3)

for i in range(20):
    # æ¬¡ã®æ¢ç´¢ç‚¹ã‚’ææ¡ˆ
    x_next = optimizer.propose_location()

    # è©•ä¾¡
    y_next = test_function(x_next[0])

    # è¦³æ¸¬ã‚’è¨˜éŒ²
    optimizer.observe(x_next, y_next)

    if i % 5 == 0:
        print(f"Iteration {i}: x={x_next[0]:.3f}, y={y_next:.3f}")

# çµæœ
best_idx = np.argmax(optimizer.y_obs)
print(f"\næœ€é©è§£: x={optimizer.X_obs[best_idx][0]:.3f}, y={optimizer.y_obs[best_idx]:.3f}")
print(f"çœŸã®æœ€é©å€¤: x=2.0, y=5.0")
</code></pre>

<h3>Population-based Training (PBT)</h3>

<p>PBTã¯ã€è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’åŒæ™‚ã«å­¦ç¿’ã•ã›ã€è‰¯ã„è¨­å®šã‚’å…±æœ‰ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">import numpy as np
from copy import deepcopy

class PBTOptimizer:
    def __init__(self, population_size=10, perturbation_factor=0.2):
        self.population_size = population_size
        self.perturbation_factor = perturbation_factor
        self.population = []

    def initialize_population(self, param_ranges):
        """é›†å›£ã®åˆæœŸåŒ–"""
        for _ in range(self.population_size):
            individual = {
                'params': {
                    key: np.random.uniform(low, high)
                    for key, (low, high) in param_ranges.items()
                },
                'score': 0.0,
                'history': []
            }
            self.population.append(individual)

    def exploit_and_explore(self, param_ranges):
        """Exploitï¼ˆè‰¯ã„è¨­å®šã‚’ã‚³ãƒ”ãƒ¼ï¼‰ã¨Exploreï¼ˆæ‘‚å‹•ï¼‰"""
        # æ€§èƒ½ã§ã‚½ãƒ¼ãƒˆ
        self.population.sort(key=lambda x: x['score'], reverse=True)

        # ä¸‹ä½20%ã‚’ä¸Šä½ã‹ã‚‰ã‚³ãƒ”ãƒ¼
        cutoff = int(0.2 * self.population_size)
        for i in range(self.population_size - cutoff, self.population_size):
            # ä¸Šä½ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠã—ã¦ã‚³ãƒ”ãƒ¼
            source = np.random.randint(0, cutoff)
            self.population[i]['params'] = deepcopy(
                self.population[source]['params']
            )

            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«æ‘‚å‹•ã‚’åŠ ãˆã‚‹ï¼ˆExploreï¼‰
            for key in self.population[i]['params']:
                low, high = param_ranges[key]
                current = self.population[i]['params'][key]

                # ãƒ©ãƒ³ãƒ€ãƒ ã«å¢—æ¸›
                factor = 1 + np.random.uniform(
                    -self.perturbation_factor,
                    self.perturbation_factor
                )
                new_value = current * factor

                # ç¯„å›²å†…ã«ã‚¯ãƒªãƒƒãƒ—
                self.population[i]['params'][key] = np.clip(
                    new_value, low, high
                )

    def step(self, eval_fn, param_ranges):
        """1ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ"""
        # å„å€‹ä½“ã‚’è©•ä¾¡
        for individual in self.population:
            score = eval_fn(individual['params'])
            individual['score'] = score
            individual['history'].append(score)

        # Exploit & Explore
        self.exploit_and_explore(param_ranges)

# è©•ä¾¡é–¢æ•°ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
def evaluate_params(params):
    """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è©•ä¾¡ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
    # æœ€é©å€¤: learning_rate=0.1, batch_size=32
    lr_score = 1 - abs(params['learning_rate'] - 0.1)
    bs_score = 1 - abs(params['batch_size'] - 32) / 64
    return (lr_score + bs_score) / 2 + np.random.randn() * 0.05

# PBTå®Ÿè¡Œ
print("\n=== Population-based Training ãƒ‡ãƒ¢ ===")
pbt = PBTOptimizer(population_size=10)
param_ranges = {
    'learning_rate': (0.001, 0.3),
    'batch_size': (16, 128)
}

pbt.initialize_population(param_ranges)

for step in range(20):
    pbt.step(evaluate_params, param_ranges)

    if step % 5 == 0:
        best = max(pbt.population, key=lambda x: x['score'])
        print(f"Step {step}: Best score={best['score']:.3f}, params={best['params']}")

# æœ€çµ‚çµæœ
best_individual = max(pbt.population, key=lambda x: x['score'])
print(f"\næœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {best_individual['params']}")
print(f"æœ€é©ã‚¹ã‚³ã‚¢: {best_individual['score']:.3f}")
</code></pre>

<h3>Hyperband</h3>

<p>Hyperbandã¯ã€æ§˜ã€…ãªäºˆç®—ï¼ˆã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°ï¼‰ã§å¤šæ•°ã®è¨­å®šã‚’è©¦ã™æ‰‹æ³•ã§ã™ã€‚</p>

<pre><code class="language-python">import numpy as np
import math

class HyperbandOptimizer:
    def __init__(self, max_iter=81, eta=3):
        self.max_iter = max_iter
        self.eta = eta
        self.logeta = lambda x: math.log(x) / math.log(self.eta)
        self.s_max = int(self.logeta(self.max_iter))
        self.B = (self.s_max + 1) * self.max_iter

    def run(self, get_config_fn, eval_fn):
        """Hyperbandå®Ÿè¡Œ"""
        results = []

        for s in reversed(range(self.s_max + 1)):
            n = int(math.ceil(self.B / self.max_iter / (s + 1) * self.eta ** s))
            r = self.max_iter * self.eta ** (-s)

            print(f"\nBracket s={s}: n={n} configs, r={r:.1f} iterations")

            # nå€‹ã®è¨­å®šã‚’ç”Ÿæˆ
            configs = [get_config_fn() for _ in range(n)]

            # Successive Halving
            for i in range(s + 1):
                n_i = n * self.eta ** (-i)
                r_i = r * self.eta ** i

                print(f"  Round {i}: {int(n_i)} configs, {int(r_i)} iterations each")

                # è©•ä¾¡
                scores = [eval_fn(config, int(r_i)) for config in configs]

                # çµæœã‚’è¨˜éŒ²
                for config, score in zip(configs, scores):
                    results.append({
                        'config': config,
                        'score': score,
                        'iterations': int(r_i)
                    })

                # ä¸Šä½ã‚’é¸æŠ
                if i < s:
                    indices = np.argsort(scores)[-int(n_i / self.eta):]
                    configs = [configs[i] for i in indices]

        return results

# è¨­å®šç”Ÿæˆé–¢æ•°
def get_random_config():
    return {
        'learning_rate': np.random.uniform(0.001, 0.3),
        'batch_size': np.random.choice([16, 32, 64, 128])
    }

# è©•ä¾¡é–¢æ•°ï¼ˆã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°ã«ä¾å­˜ï¼‰
def evaluate_config(config, iterations):
    # æœ€é©å€¤ã‹ã‚‰ã®è·é›¢ã§æ€§èƒ½ã‚’è¨ˆç®—
    lr_score = 1 - abs(config['learning_rate'] - 0.1)
    bs_score = 1 - abs(config['batch_size'] - 32) / 64
    base_score = (lr_score + bs_score) / 2

    # ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°ãŒå¤šã„ã»ã©æ€§èƒ½ãŒå‘ä¸Šï¼ˆå­¦ç¿’æ›²ç·šï¼‰
    improvement = 1 - np.exp(-iterations / 20)

    return base_score * improvement + np.random.randn() * 0.01

# Hyperbandå®Ÿè¡Œ
print("=== Hyperband ãƒ‡ãƒ¢ ===")
hyperband = HyperbandOptimizer(max_iter=81, eta=3)
results = hyperband.run(get_random_config, evaluate_config)

# æœ€è‰¯ã®çµæœ
best_result = max(results, key=lambda x: x['score'])
print(f"\n=== æœ€é©çµæœ ===")
print(f"è¨­å®š: {best_result['config']}")
print(f"ã‚¹ã‚³ã‚¢: {best_result['score']:.4f}")
print(f"ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°: {best_result['iterations']}")
print(f"\nç·è©•ä¾¡å›æ•°: {len(results)}")
</code></pre>

<h3>Multi-fidelity Optimization</h3>

<p>Multi-fidelityæœ€é©åŒ–ã¯ã€ä½ã‚³ã‚¹ãƒˆã®è¿‘ä¼¼è©•ä¾¡ã‚’æ´»ç”¨ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">import numpy as np

class MultiFidelityOptimizer:
    def __init__(self, fidelity_levels=[0.1, 0.3, 0.5, 1.0]):
        self.fidelity_levels = fidelity_levels
        self.evaluations = {level: [] for level in fidelity_levels}

    def evaluate_at_fidelity(self, config, fidelity, true_fn):
        """æŒ‡å®šã®fidelityã§è©•ä¾¡"""
        # ä½fidelityã¯è¨ˆç®—ãŒé€Ÿã„ãŒç²¾åº¦ãŒä½ã„
        # é«˜fidelityã¯è¨ˆç®—ãŒé…ã„ãŒç²¾åº¦ãŒé«˜ã„

        true_score = true_fn(config)
        noise = (1 - fidelity) * 0.2  # ä½fidelityã»ã©ãƒã‚¤ã‚ºãŒå¤§ãã„

        observed_score = true_score + np.random.randn() * noise

        return observed_score

    def optimize(self, param_ranges, true_fn, n_total_evals=100):
        """Multi-fidelityæœ€é©åŒ–"""
        # äºˆç®—é…åˆ†: ä½fidelityã§å¤šæ•°ã€é«˜fidelityã§å°‘æ•°
        eval_counts = {
            0.1: int(0.5 * n_total_evals),
            0.3: int(0.3 * n_total_evals),
            0.5: int(0.15 * n_total_evals),
            1.0: int(0.05 * n_total_evals)
        }

        all_configs = []

        # å„fidelityãƒ¬ãƒ™ãƒ«ã§è©•ä¾¡
        for fidelity in self.fidelity_levels:
            n_evals = eval_counts[fidelity]

            if fidelity == self.fidelity_levels[0]:
                # æœ€ä½fidelity: ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                configs = [
                    {key: np.random.uniform(low, high)
                     for key, (low, high) in param_ranges.items()}
                    for _ in range(n_evals)
                ]
            else:
                # å‰ã®fidelityã®ä¸Šä½ã‚’æ¬¡ã®fidelityã§è©•ä¾¡
                prev_results = sorted(
                    self.evaluations[self.fidelity_levels[self.fidelity_levels.index(fidelity) - 1]],
                    key=lambda x: x['score'],
                    reverse=True
                )
                configs = [r['config'] for r in prev_results[:n_evals]]

            # è©•ä¾¡
            for config in configs:
                score = self.evaluate_at_fidelity(config, fidelity, true_fn)
                self.evaluations[fidelity].append({
                    'config': config,
                    'score': score,
                    'fidelity': fidelity
                })
                all_configs.append((config, score, fidelity))

        # æœ€é«˜fidelityã§ã®æœ€è‰¯çµæœã‚’è¿”ã™
        best = max(
            self.evaluations[1.0],
            key=lambda x: x['score']
        )

        return best, all_configs

# çœŸã®ç›®çš„é–¢æ•°
def true_objective(config):
    lr_score = 1 - abs(config['learning_rate'] - 0.1)
    bs_score = 1 - abs(config['batch_size'] - 32) / 64
    return (lr_score + bs_score) / 2

# Multi-fidelityæœ€é©åŒ–å®Ÿè¡Œ
print("\n=== Multi-fidelity Optimization ãƒ‡ãƒ¢ ===")
mf_optimizer = MultiFidelityOptimizer()
param_ranges = {
    'learning_rate': (0.001, 0.3),
    'batch_size': (16, 128)
}

best, all_evals = mf_optimizer.optimize(param_ranges, true_objective, n_total_evals=100)

print(f"\næœ€é©è¨­å®š: {best['config']}")
print(f"æœ€é©ã‚¹ã‚³ã‚¢: {best['score']:.4f}")
print(f"\nFidelityãƒ¬ãƒ™ãƒ«åˆ¥ã®è©•ä¾¡æ•°:")
for fidelity in mf_optimizer.fidelity_levels:
    print(f"  Fidelity {fidelity}: {len(mf_optimizer.evaluations[fidelity])}å›")
</code></pre>

<hr>

<h2>2.6 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>HPOã®åŸºç¤</strong></p>
<ul>
<li>æ¢ç´¢ç©ºé–“ã®å®šç¾©ã¨åˆ†å¸ƒã®é¸æŠ</li>
<li>Grid Searchã¨Random Searchã®æ¯”è¼ƒ</li>
<li>Early Stoppingã«ã‚ˆã‚‹åŠ¹ç‡åŒ–</li>
</ul></li>

<li><p><strong>Optuna</strong></p>
<ul>
<li>Define-by-run APIã«ã‚ˆã‚‹æŸ”è»Ÿãªå®Ÿè£…</li>
<li>TPEã€CMA-ESãªã©ã®é«˜åº¦ãªã‚µãƒ³ãƒ—ãƒ©ãƒ¼</li>
<li>Pruningã«ã‚ˆã‚‹å¤§å¹…ãªæ™‚é–“çŸ­ç¸®</li>
<li>å¯è¦–åŒ–ã¨è¨ºæ–­æ©Ÿèƒ½</li>
</ul></li>

<li><p><strong>Hyperopt</strong></p>
<ul>
<li>TPEãƒ™ãƒ¼ã‚¹ã®ãƒ™ã‚¤ã‚ºæœ€é©åŒ–</li>
<li>æŸ”è»Ÿãªæ¢ç´¢ç©ºé–“å®šç¾©</li>
<li>Trialsã«ã‚ˆã‚‹è©³ç´°ãªå±¥æ­´ç®¡ç†</li>
</ul></li>

<li><p><strong>Ray Tune</strong></p>
<ul>
<li>åˆ†æ•£ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</li>
<li>ASHAã€PBTãªã©ã®é«˜åº¦ãªã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼</li>
<li>PyTorch/TensorFlowã¨ã®çµ±åˆ</li>
<li>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªä¸¦åˆ—å®Ÿè¡Œ</li>
</ul></li>

<li><p><strong>é«˜åº¦ãªHPOæ‰‹æ³•</strong></p>
<ul>
<li>Bayesian Optimization: éå»ã®è©¦è¡Œã‚’æ´»ç”¨</li>
<li>Population-based Training: å‹•çš„ãªè¨­å®šèª¿æ•´</li>
<li>Hyperband: å¤šæ§˜ãªäºˆç®—ã§ã®æ¢ç´¢</li>
<li>Multi-fidelity: ä½ã‚³ã‚¹ãƒˆè©•ä¾¡ã®æ´»ç”¨</li>
</ul></li>
</ol>

<h3>HPOãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®é¸æŠã‚¬ã‚¤ãƒ‰</h3>

<table>
<thead>
<tr>
<th>ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯</th>
<th>æœ€é©ãªç”¨é€”</th>
<th>é•·æ‰€</th>
<th>çŸ­æ‰€</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Optuna</strong></td>
<td>æ±ç”¨çš„ãªHPOã€ç ”ç©¶</td>
<td>æŸ”è»Ÿã€é«˜æ©Ÿèƒ½ã€å¯è¦–åŒ–</td>
<td>åˆ†æ•£ã¯é™å®šçš„</td>
</tr>
<tr>
<td><strong>Hyperopt</strong></td>
<td>ä¸­è¦æ¨¡HPOã€è¤‡é›‘ãªæ¢ç´¢ç©ºé–“</td>
<td>æˆç†Ÿã€å®‰å®š</td>
<td>ã‚„ã‚„å¤ã„è¨­è¨ˆ</td>
</tr>
<tr>
<td><strong>Ray Tune</strong></td>
<td>å¤§è¦æ¨¡åˆ†æ•£HPOã€DL</td>
<td>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã€çµ±åˆ</td>
<td>è¨­å®šãŒè¤‡é›‘</td>
</tr>
<tr>
<td><strong>scikit-learn</strong></td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ãªHPO</td>
<td>ç°¡å˜ã€æ¨™æº–çš„</td>
<td>æ©Ÿèƒ½ãŒé™å®šçš„</td>
</tr>
</tbody>
</table>

<h3>HPOæˆ¦ç•¥ã®é¸æŠåŸºæº–</h3>

<table>
<thead>
<tr>
<th>çŠ¶æ³</th>
<th>æ¨å¥¨æˆ¦ç•¥</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td>å°è¦æ¨¡æ¢ç´¢ï¼ˆ<10ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰</td>
<td>Grid Search</td>
<td>ç¶²ç¾…çš„ã§ç†è§£ã—ã‚„ã™ã„</td>
</tr>
<tr>
<td>ä¸­è¦æ¨¡æ¢ç´¢ï¼ˆ10-20ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰</td>
<td>Random Search, TPE</td>
<td>åŠ¹ç‡çš„ã§å®Ÿç”¨çš„</td>
</tr>
<tr>
<td>å¤§è¦æ¨¡æ¢ç´¢ï¼ˆ>20ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰</td>
<td>Bayesian Opt, ASHA</td>
<td>é«˜æ¬¡å…ƒã§ã‚‚åŠ¹ç‡çš„</td>
</tr>
<tr>
<td>é«˜ä¾¡ãªè©•ä¾¡é–¢æ•°</td>
<td>Bayesian Opt</td>
<td>å°‘ãªã„è©¦è¡Œã§æœ€é©åŒ–</td>
</tr>
<tr>
<td>å®‰ä¾¡ãªè©•ä¾¡é–¢æ•°</td>
<td>Random Search, Hyperband</td>
<td>å¤šæ•°ã®è©¦è¡ŒãŒå¯èƒ½</td>
</tr>
<tr>
<td>åˆ†æ•£ç’°å¢ƒã‚ã‚Š</td>
<td>Ray Tune + ASHA/PBT</td>
<td>ä¸¦åˆ—åŒ–ã§é«˜é€ŸåŒ–</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ç« ã¸</h3>

<p>ç¬¬3ç« ã§ã¯ã€<strong>Neural Architecture Searchï¼ˆNASï¼‰</strong>ã‚’å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>NASã®åŸºç¤ã¨å‹•æ©Ÿ</li>
<li>æ¤œç´¢ç©ºé–“ã®è¨­è¨ˆ</li>
<li>DARTSã€ENASã€NASNet</li>
<li>åŠ¹ç‡çš„ãªNASæ‰‹æ³•</li>
<li>å®Ÿè·µçš„ãªNASå®Ÿè£…</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>
<p>Grid Searchã¨Random Searchã®ä¸»ãªé•ã„ã‚’èª¬æ˜ã—ã€ãã‚Œãã‚Œã®é•·æ‰€ã¨çŸ­æ‰€ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>Grid Search</strong>ï¼š</p>
<ul>
<li>æ¢ç´¢æ–¹æ³•: ã™ã¹ã¦ã®çµ„ã¿åˆã‚ã›ã‚’ç¶²ç¾…çš„ã«è©¦ã™</li>
<li>é•·æ‰€: æ¢ç´¢ç©ºé–“ã‚’å®Œå…¨ã«èª¿æŸ»ã€å†ç¾æ€§ãŒé«˜ã„</li>
<li>çŸ­æ‰€: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå¢—ãˆã‚‹ã¨çµ„ã¿åˆã‚ã›çˆ†ç™ºã€è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„</li>
</ul>

<p><strong>Random Search</strong>ï¼š</p>
<ul>
<li>æ¢ç´¢æ–¹æ³•: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</li>
<li>é•·æ‰€: é«˜æ¬¡å…ƒã§ã‚‚åŠ¹ç‡çš„ã€é‡è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¦‹ã¤ã‘ã‚„ã™ã„</li>
<li>çŸ­æ‰€: æœ€é©è§£ã®ä¿è¨¼ãªã—ã€è©¦è¡Œå›æ•°ã®æ±ºå®šãŒé›£ã—ã„</li>
</ul>

<p><strong>ä½¿ã„åˆ†ã‘</strong>ï¼š</p>
<table>
<thead>
<tr>
<th>çŠ¶æ³</th>
<th>æ¨å¥¨</th>
</tr>
</thead>
<tbody>
<tr>
<td>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã„ï¼ˆ<5å€‹ï¼‰</td>
<td>Grid Search</td>
</tr>
<tr>
<td>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå¤šã„ï¼ˆ>5å€‹ï¼‰</td>
<td>Random Search</td>
</tr>
<tr>
<td>è¨ˆç®—è³‡æºãŒè±Šå¯Œ</td>
<td>Grid Search</td>
</tr>
<tr>
<td>è¨ˆç®—è³‡æºãŒé™å®šçš„</td>
<td>Random Search</td>
</tr>
</tbody>
</table>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>Optunaã‚’ä½¿ã£ã¦ã€LightGBMã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚Pruningã‚’æœ‰åŠ¹ã«ã—ã€å°‘ãªãã¨ã‚‚5ã¤ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import optuna
from optuna.pruners import MedianPruner
import lightgbm as lgb
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
data = load_breast_cancer()
X, y = data.data, data.target
X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ç›®çš„é–¢æ•°
def objective(trial):
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ææ¡ˆ
    params = {
        'objective': 'binary',
        'metric': 'binary_logloss',
        'verbosity': -1,
        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart']),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'num_leaves': trial.suggest_int('num_leaves', 20, 200),
        'max_depth': trial.suggest_int('max_depth', 3, 15),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),
    }

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)

    # Pruningã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    pruning_callback = optuna.integration.LightGBMPruningCallback(
        trial, 'binary_logloss'
    )

    # å­¦ç¿’
    model = lgb.train(
        params,
        train_data,
        num_boost_round=1000,
        valid_sets=[valid_data],
        valid_names=['valid'],
        callbacks=[pruning_callback, lgb.log_evaluation(period=0)]
    )

    # è©•ä¾¡
    preds = model.predict(X_valid)
    accuracy = accuracy_score(y_valid, (preds > 0.5).astype(int))

    return accuracy

# Studyä½œæˆ
study = optuna.create_study(
    direction='maximize',
    pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10)
)

# æœ€é©åŒ–å®Ÿè¡Œ
study.optimize(objective, n_trials=50, show_progress_bar=True)

# çµæœ
print(f"\n=== æœ€é©åŒ–çµæœ ===")
print(f"æœ€é©ç²¾åº¦: {study.best_value:.4f}")
print(f"\næœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
for key, value in study.best_params.items():
    print(f"  {key}: {value}")

# çµ±è¨ˆ
n_complete = len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])
n_pruned = len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])
print(f"\nå®Œäº†: {n_complete}, æåˆˆã‚Š: {n_pruned}, æåˆˆã‚Šç‡: {n_pruned/(n_complete+n_pruned)*100:.1f}%")
</code></pre>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ãŠã‘ã‚‹ã€ŒAcquisition Functionã€ã®å½¹å‰²ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€Expected Improvement (EI)ã¨Upper Confidence Bound (UCB)ã®é•ã„ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>Acquisition Functionã®å½¹å‰²</strong>ï¼š</p>
<ul>
<li>ç›®çš„: æ¬¡ã«è©•ä¾¡ã™ã¹ãç‚¹ã‚’æ±ºå®šã™ã‚‹</li>
<li>æ©Ÿèƒ½: æ¢ç´¢ï¼ˆExplorationï¼‰ã¨æ´»ç”¨ï¼ˆExploitationï¼‰ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹</li>
<li>å…¥åŠ›: ç¾åœ¨ã®ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«ï¼ˆäºˆæ¸¬å¹³å‡ã¨åˆ†æ•£ï¼‰</li>
<li>å‡ºåŠ›: å„å€™è£œç‚¹ã®ã€Œæœ‰ç”¨æ€§ã€ã‚¹ã‚³ã‚¢</li>
</ul>

<p><strong>Expected Improvement (EI)</strong>ï¼š</p>
<ul>
<li>å®šç¾©: ç¾åœ¨ã®æœ€è‰¯å€¤ã‹ã‚‰ã®æ”¹å–„ã®æœŸå¾…å€¤</li>
<li>æ•°å¼: $EI(x) = \mathbb{E}[\max(f(x) - f(x^+), 0)]$</li>
<li>ç‰¹å¾´: ç¢ºå®Ÿãªæ”¹å–„ã‚’é‡è¦–ã€ä¿å®ˆçš„</li>
<li>é•·æ‰€: å®‰å®šã—ãŸåæŸã€ç†è«–çš„ä¿è¨¼</li>
<li>çŸ­æ‰€: å±€æ‰€æœ€é©ã«é™¥ã‚Šã‚„ã™ã„</li>
</ul>

<p><strong>Upper Confidence Bound (UCB)</strong>ï¼š</p>
<ul>
<li>å®šç¾©: äºˆæ¸¬å¹³å‡ + ä¸ç¢ºå®Ÿæ€§ãƒœãƒ¼ãƒŠã‚¹</li>
<li>æ•°å¼: $UCB(x) = \mu(x) + \kappa \sigma(x)$</li>
<li>ç‰¹å¾´: ä¸ç¢ºå®Ÿæ€§ã®é«˜ã„é ˜åŸŸã‚’æ¢ç´¢</li>
<li>é•·æ‰€: æ¢ç´¢ã‚’ä¿ƒé€²ã€å¤§åŸŸçš„æœ€é©åŒ–</li>
<li>çŸ­æ‰€: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\kappa$ã®èª¿æ•´ãŒå¿…è¦</li>
</ul>

<p><strong>ä½¿ã„åˆ†ã‘</strong>ï¼š</p>
<table>
<thead>
<tr>
<th>çŠ¶æ³</th>
<th>æ¨å¥¨</th>
</tr>
</thead>
<tbody>
<tr>
<td>è©•ä¾¡ãŒé«˜ä¾¡ã€ç¢ºå®Ÿæ€§é‡è¦–</td>
<td>EI</td>
</tr>
<tr>
<td>æ¢ç´¢ã‚’é‡è¦–ã€å¤§åŸŸçš„æœ€é©åŒ–</td>
<td>UCB</td>
</tr>
<tr>
<td>ãƒã‚¤ã‚ºãŒå¤šã„</td>
<td>EI</td>
</tr>
<tr>
<td>æ»‘ã‚‰ã‹ãªç›®çš„é–¢æ•°</td>
<td>ã©ã¡ã‚‰ã§ã‚‚å¯</td>
</tr>
</tbody>
</table>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>ASHAã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®ä»•çµ„ã¿ã‚’èª¬æ˜ã—ã€é€šå¸¸ã®Random Searchã¨æ¯”è¼ƒã—ã¦ã€ãªãœåŠ¹ç‡çš„ãªã®ã‹ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>ASHAã®ä»•çµ„ã¿</strong>ï¼š</p>

<ol>
<li><p><strong>åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢</strong></p>
<ul>
<li>å¤šæ•°ã®è¨­å®šã‚’å°‘ãªã„ãƒªã‚½ãƒ¼ã‚¹ï¼ˆã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã§è©¦ã™</li>
<li>æ€§èƒ½ã®æ‚ªã„è¨­å®šã‚’æ—©æœŸã«æ‰“ã¡åˆ‡ã‚‹</li>
<li>æœ‰æœ›ãªè¨­å®šã«ãƒªã‚½ãƒ¼ã‚¹ã‚’é›†ä¸­</li>
</ul></li>

<li><p><strong>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </strong></p>
<ul>
<li>Rungï¼ˆæ®µéšï¼‰ã‚’è¨­å®š: ä¾‹ãˆã°[10, 30, 90, 270]ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</li>
<li>å„Rungã§ä¸Šä½1/Î·ï¼ˆä¾‹: Î·=3ãªã‚‰ä¸Šä½1/3ï¼‰ã®ã¿æ¬¡ã¸</li>
<li>æœ€çµ‚çš„ã«å°‘æ•°ã®è¨­å®šã®ã¿æœ€å¤§ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¾ã§å®Ÿè¡Œ</li>
</ul></li>

<li><p><strong>éåŒæœŸå®Ÿè¡Œ</strong></p>
<ul>
<li>å„è©¦è¡ŒãŒç‹¬ç«‹ã—ã¦é€²è¡Œ</li>
<li>Rungã«åˆ°é”ã—ãŸã‚‰æ˜‡æ ¼åˆ¤å®š</li>
<li>ãƒªã‚½ãƒ¼ã‚¹ã®åŠ¹ç‡çš„ãªæ´»ç”¨</li>
</ul></li>
</ol>

<p><strong>Random Searchã¨ã®æ¯”è¼ƒ</strong>ï¼š</p>

<table>
<thead>
<tr>
<th>å´é¢</th>
<th>Random Search</th>
<th>ASHA</th>
</tr>
</thead>
<tbody>
<tr>
<td>ãƒªã‚½ãƒ¼ã‚¹é…åˆ†</td>
<td>å…¨è©¦è¡Œã«å‡ç­‰</td>
<td>æœ‰æœ›ãªè©¦è¡Œã«é›†ä¸­</td>
</tr>
<tr>
<td>æ—©æœŸåœæ­¢</td>
<td>ãªã—</td>
<td>ã‚ã‚Šï¼ˆæ€§èƒ½ä¸è‰¯ã‚’æ‰“ã¡åˆ‡ã‚Šï¼‰</td>
</tr>
<tr>
<td>ä¸¦åˆ—åŒ–</td>
<td>ç°¡å˜</td>
<td>éåŒæœŸã§åŠ¹ç‡çš„</td>
</tr>
<tr>
<td>ç·è¨ˆç®—æ™‚é–“</td>
<td>N Ã— max_iter</td>
<td>â‰ˆ N Ã— min_iter + å°‘æ•° Ã— max_iter</td>
</tr>
</tbody>
</table>

<p><strong>åŠ¹ç‡æ€§ã®ç†ç”±</strong>ï¼š</p>

<ol>
<li><p><strong>ç„¡é§„ãªè¨ˆç®—ã®å‰Šæ¸›</strong></p>
<ul>
<li>æ˜ã‚‰ã‹ã«æ‚ªã„è¨­å®šã‚’æ—©æœŸã«æ‰“ã¡åˆ‡ã‚‹</li>
<li>æœ‰æœ›ãªè¨­å®šã®ã¿ãƒ•ãƒ«å­¦ç¿’</li>
</ul></li>

<li><p><strong>æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹</strong></p>
<ul>
<li>å¤šæ§˜ãªè¨­å®šã‚’è©¦ã™ï¼ˆæ¢ç´¢ï¼‰</li>
<li>è‰¯ã„è¨­å®šã«æ³¨åŠ›ï¼ˆæ´»ç”¨ï¼‰</li>
</ul></li>

<li><p><strong>ç†è«–çš„ä¿è¨¼</strong></p>
<ul>
<li>æœ€é©è¨­å®šã‚’è¦‹é€ƒã™ç¢ºç‡ãŒä½ã„</li>
<li>è¨ˆç®—é‡ãŒå¯¾æ•°çš„ã«å¢—åŠ </li>
</ul></li>
</ol>

<p><strong>å®Ÿä¾‹</strong>ï¼š</p>

<pre><code>Random Search: 81è©¦è¡Œ Ã— 100ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ = 8,100è¨ˆç®—å˜ä½

ASHA (Î·=3):
- Rung 0: 81è©¦è¡Œ Ã— 1ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ = 81
- Rung 1: 27è©¦è¡Œ Ã— 3ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ = 81
- Rung 2: 9è©¦è¡Œ Ã— 9ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ = 81
- Rung 3: 3è©¦è¡Œ Ã— 27ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ = 81
- Rung 4: 1è©¦è¡Œ Ã— 81ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ = 81
åˆè¨ˆ: 405è¨ˆç®—å˜ä½

å‰Šæ¸›ç‡: (8,100 - 405) / 8,100 = 95%
</code></pre>

</details>

<h3>å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>Population-based Training (PBT)ã¨ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ä¸»ãªé•ã„ã‚’èª¬æ˜ã—ã€ãã‚Œãã‚ŒãŒé©ã—ã¦ã„ã‚‹çŠ¶æ³ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>PBTã®ç‰¹å¾´</strong>ï¼š</p>
<ul>
<li>è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’åŒæ™‚ã«å­¦ç¿’</li>
<li>å­¦ç¿’ä¸­ã«å‹•çš„ã«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´</li>
<li>è‰¯ã„è¨­å®šã‚’ä»–ã®ãƒ¢ãƒ‡ãƒ«ã«ã‚³ãƒ”ãƒ¼ï¼ˆExploitï¼‰</li>
<li>ã‚³ãƒ”ãƒ¼ã—ãŸè¨­å®šã«æ‘‚å‹•ã‚’åŠ ãˆã‚‹ï¼ˆExploreï¼‰</li>
<li>é€²åŒ–çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆéºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«é¡ä¼¼ï¼‰</li>
</ul>

<p><strong>ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ç‰¹å¾´</strong>ï¼š</p>
<ul>
<li>1ã¤ãšã¤ãƒ¢ãƒ‡ãƒ«ã‚’é †æ¬¡å­¦ç¿’</li>
<li>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å­¦ç¿’å‰ã«å›ºå®š</li>
<li>éå»ã®è©¦è¡Œå±¥æ­´ã‹ã‚‰ã‚¬ã‚¦ã‚¹éç¨‹ã§ãƒ¢ãƒ‡ãƒ«åŒ–</li>
<li>æ¬¡ã®æœ€é©ãªæ¢ç´¢ç‚¹ã‚’ç¢ºç‡çš„ã«é¸æŠ</li>
<li>ç†è«–çš„ãªæœ€é©åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</li>
</ul>

<p><strong>ä¸»ãªé•ã„</strong>ï¼š</p>

<table>
<thead>
<tr>
<th>å´é¢</th>
<th>PBT</th>
<th>ãƒ™ã‚¤ã‚ºæœ€é©åŒ–</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ä¸¦åˆ—æ€§</strong></td>
<td>é«˜ã„ï¼ˆé›†å›£å…¨ä½“ã‚’åŒæ™‚å­¦ç¿’ï¼‰</td>
<td>é™å®šçš„ï¼ˆé †æ¬¡å®Ÿè¡ŒãŒåŸºæœ¬ï¼‰</td>
</tr>
<tr>
<td><strong>å‹•çš„èª¿æ•´</strong></td>
<td>ã‚ã‚Šï¼ˆå­¦ç¿’ä¸­ã«å¤‰æ›´ï¼‰</td>
<td>ãªã—ï¼ˆå­¦ç¿’å‰ã«å›ºå®šï¼‰</td>
</tr>
<tr>
<td><strong>è¨ˆç®—åŠ¹ç‡</strong></td>
<td>é«˜ã„ï¼ˆä¸¦åˆ—å®Ÿè¡Œï¼‰</td>
<td>ä¸­ç¨‹åº¦ï¼ˆè©¦è¡Œå›æ•°ã¯å°‘ãªã„ï¼‰</td>
</tr>
<tr>
<td><strong>ç†è«–çš„ä¿è¨¼</strong></td>
<td>å¼±ã„ï¼ˆãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ï¼‰</td>
<td>å¼·ã„ï¼ˆåæŸä¿è¨¼ã‚ã‚Šï¼‰</td>
</tr>
<tr>
<td><strong>å®Ÿè£…è¤‡é›‘åº¦</strong></td>
<td>é«˜ã„ï¼ˆé›†å›£ç®¡ç†ãŒå¿…è¦ï¼‰</td>
<td>ä¸­ç¨‹åº¦ï¼ˆæ—¢å­˜å®Ÿè£…åˆ©ç”¨å¯ï¼‰</td>
</tr>
<tr>
<td><strong>é©ç”¨ç¯„å›²</strong></td>
<td>é•·æ™‚é–“å­¦ç¿’ï¼ˆDLï¼‰</td>
<td>é«˜ä¾¡ãªè©•ä¾¡é–¢æ•°</td>
</tr>
</tbody>
</table>

<p><strong>é©ã—ã¦ã„ã‚‹çŠ¶æ³</strong>ï¼š</p>

<p><strong>PBTã‚’ä½¿ã†ã¹ãå ´åˆ</strong>ï¼š</p>
<ul>
<li>ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãªã©é•·æ™‚é–“ã®å­¦ç¿’</li>
<li>å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãªã©å‹•çš„ãªèª¿æ•´ãŒé‡è¦</li>
<li>ä¸¦åˆ—è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ãŒè±Šå¯Œ</li>
<li>æ¢ç´¢ç©ºé–“ãŒåºƒã„</li>
<li>ä¾‹: å¼·åŒ–å­¦ç¿’ã€å¤§è¦æ¨¡ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆå­¦ç¿’</li>
</ul>

<p><strong>ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’ä½¿ã†ã¹ãå ´åˆ</strong>ï¼š</p>
<ul>
<li>1å›ã®è©•ä¾¡ãŒéå¸¸ã«é«˜ä¾¡</li>
<li>è©•ä¾¡é–¢æ•°ãŒæ»‘ã‚‰ã‹</li>
<li>å°‘ãªã„è©¦è¡Œå›æ•°ã§æœ€é©åŒ–ã—ãŸã„</li>
<li>ç†è«–çš„ãªä¿è¨¼ãŒå¿…è¦</li>
<li>ä¾‹: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æœ€é©åŒ–ã€é«˜ä¾¡ãªå®Ÿé¨“è¨­å®š</li>
</ul>

<p><strong>ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</strong>ï¼š</p>
<ul>
<li>ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§åˆæœŸè¨­å®šã‚’æ¢ç´¢</li>
<li>è‰¯ã„è¨­å®šã®å‘¨è¾ºã‚’PBTã§ç´°ã‹ãèª¿æ•´</li>
<li>ä¸¡æ–¹ã®åˆ©ç‚¹ã‚’æ´»ç”¨</li>
</ul>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Bergstra, J., & Bengio, Y. (2012). <em>Random search for hyper-parameter optimization</em>. Journal of Machine Learning Research, 13(1), 281-305.</li>
<li>Akiba, T., Sano, S., Yanase, T., Ohta, T., & Koyama, M. (2019). <em>Optuna: A next-generation hyperparameter optimization framework</em>. KDD.</li>
<li>Bergstra, J., Yamins, D., & Cox, D. (2013). <em>Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</em>. ICML.</li>
<li>Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., & Talwalkar, A. (2017). <em>Hyperband: A novel bandit-based approach to hyperparameter optimization</em>. JMLR.</li>
<li>Jaderberg, M., Dalibard, V., Osindero, S., Czarnecki, W. M., Donahue, J., Razavi, A., ... & Kavukcuoglu, K. (2017). <em>Population based training of neural networks</em>. arXiv preprint arXiv:1711.09846.</li>
<li>Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., & De Freitas, N. (2015). <em>Taking the human out of the loop: A review of Bayesian optimization</em>. Proceedings of the IEEE, 104(1), 148-175.</li>
<li>Liaw, R., Liang, E., Nishihara, R., Moritz, P., Gonzalez, J. E., & Stoica, I. (2018). <em>Tune: A research platform for distributed model selection and training</em>. arXiv preprint arXiv:1807.05118.</li>
</ol>

<div class="navigation">
    <a href="chapter1-automl-overview.html" class="nav-button">â† å‰ã®ç« : AutoMLæ¦‚è¦</a>
    <a href="chapter3-neural-architecture-search.html" class="nav-button">æ¬¡ã®ç« : Neural Architecture Search â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
