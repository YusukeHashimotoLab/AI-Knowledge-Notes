<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
<meta content="ç¬¬1ç« ï¼šç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®åŸºç¤ - AI Terakoya" name="description"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬1ç« ï¼šç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®åŸºç¤ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/generative-models-introduction/chapter1-fundamentals.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/generative-models-introduction/index.html">Generative Models</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 1</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬1ç« ï¼šç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®åŸºç¤</h1>
            <p class="subtitle">åˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ«ã¨ã®é•ã„ã€ç¢ºç‡åˆ†å¸ƒã®å­¦ç¿’ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•ã®ç†è§£</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´šã€œä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 10å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… åˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ«ã¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æœ¬è³ªçš„ãªé•ã„ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… ç¢ºç‡åˆ†å¸ƒã®å­¦ç¿’ã«ãŠã‘ã‚‹å°¤åº¦é–¢æ•°ã¨æœ€å°¤æ¨å®šã‚’èª¬æ˜ã§ãã‚‹</li>
<li>âœ… ãƒ™ã‚¤ã‚ºã®å®šç†ã¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®é–¢ä¿‚ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Rejection Samplingã¨Importance Samplingã®ä»•çµ„ã¿ã‚’ç¿’å¾—ã™ã‚‹</li>
<li>âœ… MCMCï¼ˆMarkov Chain Monte Carloï¼‰ã®åŸºæœ¬åŸç†ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã¨æ½œåœ¨ç©ºé–“ã®æ¦‚å¿µã‚’æŠŠæ¡ã™ã‚‹</li>
<li>âœ… Inception Scoreã¨FIDã«ã‚ˆã‚‹ç”Ÿæˆå“è³ªè©•ä¾¡ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«ï¼ˆGMMï¼‰ã‚’PyTorchã§å®Ÿè£…ã—ã€ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã«å¿œç”¨ã§ãã‚‹</li>
</ul>

<hr>

<h2>1.1 åˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ« vs ç”Ÿæˆãƒ¢ãƒ‡ãƒ«</h2>

<h3>æ©Ÿæ¢°å­¦ç¿’ã«ãŠã‘ã‚‹2ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</h3>

<p>æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ‡ãƒ¼ã‚¿ã¨ã®é–¢ã‚ã‚Šæ–¹ã«ã‚ˆã‚Šå¤§ãã2ã¤ã«åˆ†é¡ã•ã‚Œã¾ã™ï¼š</p>

<blockquote>
<p>ã€Œåˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ«ã¯å…¥åŠ›ã‹ã‚‰å‡ºåŠ›ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å­¦ç¿’ã—ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯ãƒ‡ãƒ¼ã‚¿ãã®ã‚‚ã®ã®ç¢ºç‡åˆ†å¸ƒã‚’å­¦ç¿’ã™ã‚‹ã€‚ã€</p>
</blockquote>

<h4>åˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ«ï¼ˆDiscriminative Modelï¼‰</h4>

<p><strong>ç›®çš„</strong>ï¼šæ¡ä»¶ä»˜ãç¢ºç‡ $P(y|x)$ ã‚’å­¦ç¿’ã™ã‚‹</p>

<ul>
<li>å…¥åŠ› $x$ ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€ãƒ©ãƒ™ãƒ« $y$ ã‚’äºˆæ¸¬ã™ã‚‹</li>
<li>ã‚¯ãƒ©ã‚¹åˆ†é¡ã€å›å¸°ã‚¿ã‚¹ã‚¯ã«ä½¿ç”¨</li>
<li>ä¾‹ï¼šãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã€SVMã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</li>
</ul>

<h4>ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆGenerative Modelï¼‰</h4>

<p><strong>ç›®çš„</strong>ï¼šåŒæ™‚ç¢ºç‡ $P(x, y)$ ã¾ãŸã¯ $P(x)$ ã‚’å­¦ç¿’ã™ã‚‹</p>

<ul>
<li>ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒãã®ã‚‚ã®ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–</li>
<li>æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ã®ç”ŸæˆãŒå¯èƒ½</li>
<li>ä¾‹ï¼šVAEã€GANã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã€GPT</li>
</ul>

<table>
<thead>
<tr>
<th>ç‰¹å¾´</th>
<th>åˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ«</th>
<th>ç”Ÿæˆãƒ¢ãƒ‡ãƒ«</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å­¦ç¿’å¯¾è±¡</strong></td>
<td>$P(y|x)$ï¼ˆæ¡ä»¶ä»˜ãç¢ºç‡ï¼‰</td>
<td>$P(x)$ ã¾ãŸã¯ $P(x,y)$ï¼ˆåŒæ™‚ç¢ºç‡ï¼‰</td>
</tr>
<tr>
<td><strong>ä¸»ãªç”¨é€”</strong></td>
<td>åˆ†é¡ã€å›å¸°</td>
<td>ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã€å¯†åº¦æ¨å®š</td>
</tr>
<tr>
<td><strong>æ±ºå®šå¢ƒç•Œ</strong></td>
<td>ç›´æ¥å­¦ç¿’</td>
<td>ç¢ºç‡åˆ†å¸ƒã‹ã‚‰å°å‡º</td>
</tr>
<tr>
<td><strong>ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ</strong></td>
<td>ä¸å¯èƒ½</td>
<td>å¯èƒ½</td>
</tr>
<tr>
<td><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆ</strong></td>
<td>æ¯”è¼ƒçš„ä½ã„</td>
<td>é«˜ã„ï¼ˆåˆ†å¸ƒå…¨ä½“ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ï¼‰</td>
</tr>
</tbody>
</table>

<div class="mermaid">
graph LR
    subgraph "åˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ«"
    A1[å…¥åŠ› x] --> B1[ãƒ¢ãƒ‡ãƒ« f]
    B1 --> C1[å‡ºåŠ› y]
    D1[å­¦ç¿’: P&#40;y|x&#41;]
    end

    subgraph "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"
    A2[ç¢ºç‡åˆ†å¸ƒ P&#40;x&#41;] --> B2[ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°]
    B2 --> C2[ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ x']
    D2[å­¦ç¿’: P&#40;x&#41;]
    end

    style A1 fill:#e3f2fd
    style B1 fill:#fff3e0
    style C1 fill:#ffebee
    style A2 fill:#e3f2fd
    style B2 fill:#fff3e0
    style C2 fill:#ffebee
</div>

<h3>ãƒ™ã‚¤ã‚ºã®å®šç†ã«ã‚ˆã‚‹é–¢ä¿‚</h3>

<p>åˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ«ã¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ™ã‚¤ã‚ºã®å®šç†ã§çµã³ã¤ãã¾ã™ï¼š</p>

$$
P(y|x) = \frac{P(x|y)P(y)}{P(x)}
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$P(y|x)$: äº‹å¾Œç¢ºç‡ï¼ˆåˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ«ãŒç›´æ¥å­¦ç¿’ï¼‰</li>
<li>$P(x|y)$: å°¤åº¦ï¼ˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ï¼‰</li>
<li>$P(y)$: äº‹å‰ç¢ºç‡ï¼ˆã‚¯ãƒ©ã‚¹ã®å‡ºç¾é »åº¦ï¼‰</li>
<li>$P(x)$: å‘¨è¾ºå°¤åº¦ï¼ˆæ­£è¦åŒ–å®šæ•°ï¼‰</li>
</ul>

<blockquote>
<p><strong>é‡è¦</strong>: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯ $P(x|y)$ ã¨ $P(y)$ ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€ãƒ™ã‚¤ã‚ºã®å®šç†ã‚’é€šã˜ã¦åˆ†é¡ã‚¿ã‚¹ã‚¯ã«ã‚‚å¿œç”¨ã§ãã¾ã™ã€‚</p>
</blockquote>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼š2ã‚¯ãƒ©ã‚¹åˆ†é¡
np.random.seed(42)
X, y = make_blobs(n_samples=300, centers=2, n_features=2,
                  center_box=(-5, 5), random_state=42)

print("=== åˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ« vs ç”Ÿæˆãƒ¢ãƒ‡ãƒ« ===\n")

# åˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ«ï¼šãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼ˆP(y|x)ã‚’ç›´æ¥å­¦ç¿’ï¼‰
discriminative = LogisticRegression()
discriminative.fit(X, y)

# ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼šã‚¬ã‚¦ã‚¹ãƒŠã‚¤ãƒ¼ãƒ–ãƒ™ã‚¤ã‚ºï¼ˆP(x|y)ã¨P(y)ã‚’å­¦ç¿’ï¼‰
generative = GaussianNB()
generative.fit(X, y)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
X_test = np.array([[2.0, 3.0], [-3.0, -2.0]])

# äºˆæ¸¬
disc_pred = discriminative.predict(X_test)
gen_pred = generative.predict(X_test)
disc_proba = discriminative.predict_proba(X_test)
gen_proba = generative.predict_proba(X_test)

print("ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«:")
for i, x in enumerate(X_test):
    print(f"\nã‚µãƒ³ãƒ—ãƒ« {i+1}: {x}")
    print(f"  åˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬: ã‚¯ãƒ©ã‚¹ {disc_pred[i]}, "
          f"ç¢ºç‡ [ã‚¯ãƒ©ã‚¹0: {disc_proba[i,0]:.3f}, ã‚¯ãƒ©ã‚¹1: {disc_proba[i,1]:.3f}]")
    print(f"  ç”Ÿæˆãƒ¢ãƒ‡ãƒ«äºˆæ¸¬: ã‚¯ãƒ©ã‚¹ {gen_pred[i]}, "
          f"ç¢ºç‡ [ã‚¯ãƒ©ã‚¹0: {gen_proba[i,0]:.3f}, ã‚¯ãƒ©ã‚¹1: {gen_proba[i,1]:.3f}]")

print("\nç‰¹å¾´ã®æ¯”è¼ƒ:")
print("  åˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ«:")
print("    - æ±ºå®šå¢ƒç•Œã‚’ç›´æ¥å­¦ç¿’")
print("    - è¨ˆç®—åŠ¹ç‡ãŒè‰¯ã„")
print("    - æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã¯ä¸å¯èƒ½")
print("\n  ç”Ÿæˆãƒ¢ãƒ‡ãƒ«:")
print("    - å„ã‚¯ãƒ©ã‚¹ã®ç¢ºç‡åˆ†å¸ƒã‚’å­¦ç¿’")
print("    - ãƒ‡ãƒ¼ã‚¿ç”ŸæˆãŒå¯èƒ½")
print("    - åˆ†å¸ƒã®ä»®å®šãŒå¿…è¦ï¼ˆä¾‹ï¼šã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼‰")

# å¯è¦–åŒ–
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# ãƒ¡ãƒƒã‚·ãƒ¥ã‚°ãƒªãƒƒãƒ‰ã®ä½œæˆ
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),
                     np.linspace(y_min, y_max, 200))

# åˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ«ã®æ±ºå®šå¢ƒç•Œ
Z_disc = discriminative.predict(np.c_[xx.ravel(), yy.ravel()])
Z_disc = Z_disc.reshape(xx.shape)
ax1.contourf(xx, yy, Z_disc, alpha=0.3, cmap='RdYlBu')
ax1.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolor='black')
ax1.set_title('åˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼‰\nP(y|x)ã‚’ç›´æ¥å­¦ç¿’')
ax1.set_xlabel('ç‰¹å¾´é‡ 1')
ax1.set_ylabel('ç‰¹å¾´é‡ 2')

# ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ±ºå®šå¢ƒç•Œ
Z_gen = generative.predict(np.c_[xx.ravel(), yy.ravel()])
Z_gen = Z_gen.reshape(xx.shape)
ax2.contourf(xx, yy, Z_gen, alpha=0.3, cmap='RdYlBu')
ax2.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolor='black')
ax2.set_title('ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆã‚¬ã‚¦ã‚¹ãƒŠã‚¤ãƒ¼ãƒ–ãƒ™ã‚¤ã‚ºï¼‰\nP(x|y)ã¨P(y)ã‚’å­¦ç¿’')
ax2.set_xlabel('ç‰¹å¾´é‡ 1')
ax2.set_ylabel('ç‰¹å¾´é‡ 2')

plt.tight_layout()
print("\næ±ºå®šå¢ƒç•Œã®å¯è¦–åŒ–ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== åˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ« vs ç”Ÿæˆãƒ¢ãƒ‡ãƒ« ===

ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«:

ã‚µãƒ³ãƒ—ãƒ« 1: [ 2.  3.]
  åˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬: ã‚¯ãƒ©ã‚¹ 1, ç¢ºç‡ [ã‚¯ãƒ©ã‚¹0: 0.234, ã‚¯ãƒ©ã‚¹1: 0.766]
  ç”Ÿæˆãƒ¢ãƒ‡ãƒ«äºˆæ¸¬: ã‚¯ãƒ©ã‚¹ 1, ç¢ºç‡ [ã‚¯ãƒ©ã‚¹0: 0.198, ã‚¯ãƒ©ã‚¹1: 0.802]

ã‚µãƒ³ãƒ—ãƒ« 2: [-3. -2.]
  åˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬: ã‚¯ãƒ©ã‚¹ 0, ç¢ºç‡ [ã‚¯ãƒ©ã‚¹0: 0.891, ã‚¯ãƒ©ã‚¹1: 0.109]
  ç”Ÿæˆãƒ¢ãƒ‡ãƒ«äºˆæ¸¬: ã‚¯ãƒ©ã‚¹ 0, ç¢ºç‡ [ã‚¯ãƒ©ã‚¹0: 0.923, ã‚¯ãƒ©ã‚¹1: 0.077]

ç‰¹å¾´ã®æ¯”è¼ƒ:
  åˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ«:
    - æ±ºå®šå¢ƒç•Œã‚’ç›´æ¥å­¦ç¿’
    - è¨ˆç®—åŠ¹ç‡ãŒè‰¯ã„
    - æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã¯ä¸å¯èƒ½

  ç”Ÿæˆãƒ¢ãƒ‡ãƒ«:
    - å„ã‚¯ãƒ©ã‚¹ã®ç¢ºç‡åˆ†å¸ƒã‚’å­¦ç¿’
    - ãƒ‡ãƒ¼ã‚¿ç”ŸæˆãŒå¯èƒ½
    - åˆ†å¸ƒã®ä»®å®šãŒå¿…è¦ï¼ˆä¾‹ï¼šã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼‰

æ±ºå®šå¢ƒç•Œã®å¯è¦–åŒ–ã‚’ç”Ÿæˆã—ã¾ã—ãŸ
</code></pre>

<hr>

<h2>1.2 ç¢ºç‡åˆ†å¸ƒã®å­¦ç¿’</h2>

<h3>å°¤åº¦é–¢æ•°ã¨æœ€å°¤æ¨å®š</h3>

<p>ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ ¸å¿ƒã¯ã€ãƒ‡ãƒ¼ã‚¿ã®ç¢ºç‡åˆ†å¸ƒ $P(x; \theta)$ ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã§è¡¨ç¾ã—ã€å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã™ã€‚</p>

<h4>å°¤åº¦é–¢æ•°ï¼ˆLikelihood Functionï¼‰</h4>

<p>ä¸ãˆã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ $\mathcal{D} = \{x_1, x_2, \ldots, x_N\}$ ã«å¯¾ã™ã‚‹å°¤åº¦ã¯ï¼š</p>

$$
L(\theta) = P(\mathcal{D}; \theta) = \prod_{i=1}^{N} P(x_i; \theta)
$$

<p>ãƒ‡ãƒ¼ã‚¿ãŒç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆi.i.d.ï¼‰ã§ã‚ã‚‹ã¨ä»®å®šã™ã‚‹ã¨ã€å„ã‚µãƒ³ãƒ—ãƒ«ã®ç¢ºç‡ã®ç©ã«ãªã‚Šã¾ã™ã€‚</p>

<h4>å¯¾æ•°å°¤åº¦ï¼ˆLog-Likelihoodï¼‰</h4>

<p>è¨ˆç®—ã®å®‰å®šæ€§ã¨æ•°å€¤çš„ãªæ‰±ã„ã‚„ã™ã•ã®ãŸã‚ã€é€šå¸¸ã¯å¯¾æ•°ã‚’ã¨ã‚Šã¾ã™ï¼š</p>

$$
\log L(\theta) = \sum_{i=1}^{N} \log P(x_i; \theta)
$$

<h4>æœ€å°¤æ¨å®šï¼ˆMaximum Likelihood Estimation, MLEï¼‰</h4>

<p>å°¤åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’æ±‚ã‚ã¾ã™ï¼š</p>

$$
\hat{\theta}_{\text{MLE}} = \arg\max_{\theta} \log L(\theta) = \arg\max_{\theta} \sum_{i=1}^{N} \log P(x_i; \theta)
$$

<blockquote>
<p>ã€Œæœ€å°¤æ¨å®šã¯ã€è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ãŒæœ€ã‚‚èµ·ã“ã‚Šã‚„ã™ããªã‚‹ã‚ˆã†ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é¸ã¶åŸç†ã§ã™ã€‚ã€</p>
</blockquote>

<h3>å…·ä½“ä¾‹ï¼šã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š</h3>

<p>ãƒ‡ãƒ¼ã‚¿ãŒ1æ¬¡å…ƒã‚¬ã‚¦ã‚¹åˆ†å¸ƒ $\mathcal{N}(\mu, \sigma^2)$ ã«å¾“ã†ã¨ä»®å®šã—ã¾ã™ï¼š</p>

$$
P(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$

<p>å¯¾æ•°å°¤åº¦ï¼š</p>

$$
\log L(\mu, \sigma^2) = -\frac{N}{2}\log(2\pi) - \frac{N}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{N}(x_i - \mu)^2
$$

<p>ã“ã‚Œã‚’ $\mu$ ã¨ $\sigma^2$ ã§å¾®åˆ†ã—ã¦ã‚¼ãƒ­ã¨ãŠãã¨ï¼š</p>

$$
\begin{align}
\hat{\mu}_{\text{MLE}} &= \frac{1}{N}\sum_{i=1}^{N} x_i \\
\hat{\sigma}^2_{\text{MLE}} &= \frac{1}{N}\sum_{i=1}^{N} (x_i - \hat{\mu})^2
\end{align}
$$

<p>ã¤ã¾ã‚Šã€æ¨™æœ¬å¹³å‡ã¨æ¨™æœ¬åˆ†æ•£ãŒMLEã¨ãªã‚Šã¾ã™ã€‚</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼šçœŸã®åˆ†å¸ƒ N(3, 2^2)
np.random.seed(42)
true_mu, true_sigma = 3.0, 2.0
N = 100
data = np.random.normal(true_mu, true_sigma, N)

print("=== æœ€å°¤æ¨å®šï¼ˆMLEï¼‰ã®å®Ÿè£… ===\n")

# æœ€å°¤æ¨å®š
mle_mu = np.mean(data)
mle_sigma = np.std(data, ddof=0)  # ddof=0ã§æ¨™æœ¬åˆ†æ•£

print(f"çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
print(f"  å¹³å‡ Î¼: {true_mu}")
print(f"  æ¨™æº–åå·® Ïƒ: {true_sigma}")

print(f"\næœ€å°¤æ¨å®šå€¤:")
print(f"  æ¨å®šå¹³å‡ Î¼Ì‚: {mle_mu:.4f}")
print(f"  æ¨å®šæ¨™æº–åå·® ÏƒÌ‚: {mle_sigma:.4f}")

print(f"\næ¨å®šèª¤å·®:")
print(f"  å¹³å‡ã®èª¤å·®: {abs(mle_mu - true_mu):.4f}")
print(f"  æ¨™æº–åå·®ã®èª¤å·®: {abs(mle_sigma - true_sigma):.4f}")

# å¯¾æ•°å°¤åº¦ã®è¨ˆç®—
def log_likelihood(data, mu, sigma):
    """ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®å¯¾æ•°å°¤åº¦"""
    N = len(data)
    log_prob = -0.5 * N * np.log(2 * np.pi) - N * np.log(sigma) \
               - 0.5 * np.sum((data - mu)**2) / (sigma**2)
    return log_prob

# MLEã§ã®å¯¾æ•°å°¤åº¦
ll_mle = log_likelihood(data, mle_mu, mle_sigma)
print(f"\nMLEã§ã®å¯¾æ•°å°¤åº¦: {ll_mle:.2f}")

# ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã®å¯¾æ•°å°¤åº¦ï¼ˆæ¯”è¼ƒï¼‰
ll_wrong1 = log_likelihood(data, true_mu + 1, true_sigma)
ll_wrong2 = log_likelihood(data, true_mu, true_sigma + 1)
print(f"Î¼=4, Ïƒ=2ã§ã®å¯¾æ•°å°¤åº¦: {ll_wrong1:.2f} (MLEã‚ˆã‚Šä½ã„)")
print(f"Î¼=3, Ïƒ=3ã§ã®å¯¾æ•°å°¤åº¦: {ll_wrong2:.2f} (MLEã‚ˆã‚Šä½ã„)")

# å¯è¦–åŒ–
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# å·¦å›³ï¼šãƒ‡ãƒ¼ã‚¿ã¨æ¨å®šåˆ†å¸ƒ
ax1.hist(data, bins=20, density=True, alpha=0.6, color='skyblue',
         edgecolor='black', label='ãƒ‡ãƒ¼ã‚¿ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ')
x_range = np.linspace(data.min() - 1, data.max() + 1, 200)
ax1.plot(x_range, norm.pdf(x_range, true_mu, true_sigma),
         'r-', linewidth=2, label=f'çœŸã®åˆ†å¸ƒ N({true_mu}, {true_sigma}Â²)')
ax1.plot(x_range, norm.pdf(x_range, mle_mu, mle_sigma),
         'g--', linewidth=2, label=f'æ¨å®šåˆ†å¸ƒ N({mle_mu:.2f}, {mle_sigma:.2f}Â²)')
ax1.set_xlabel('x')
ax1.set_ylabel('ç¢ºç‡å¯†åº¦')
ax1.set_title('æœ€å°¤æ¨å®šã«ã‚ˆã‚‹ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°')
ax1.legend()
ax1.grid(True, alpha=0.3)

# å³å›³ï¼šå¯¾æ•°å°¤åº¦ã®ç­‰é«˜ç·š
mu_range = np.linspace(2, 4, 100)
sigma_range = np.linspace(1, 3, 100)
MU, SIGMA = np.meshgrid(mu_range, sigma_range)
LL = np.zeros_like(MU)

for i in range(len(mu_range)):
    for j in range(len(sigma_range)):
        LL[j, i] = log_likelihood(data, MU[j, i], SIGMA[j, i])

contour = ax2.contourf(MU, SIGMA, LL, levels=20, cmap='viridis')
ax2.plot(mle_mu, mle_sigma, 'r*', markersize=20, label='MLE')
ax2.plot(true_mu, true_sigma, 'wo', markersize=10, label='çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿')
ax2.set_xlabel('å¹³å‡ Î¼')
ax2.set_ylabel('æ¨™æº–åå·® Ïƒ')
ax2.set_title('å¯¾æ•°å°¤åº¦ã®ç­‰é«˜ç·šå›³')
ax2.legend()
plt.colorbar(contour, ax=ax2, label='å¯¾æ•°å°¤åº¦')

plt.tight_layout()
print("\nå¯è¦–åŒ–å®Œäº†")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== æœ€å°¤æ¨å®šï¼ˆMLEï¼‰ã®å®Ÿè£… ===

çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
  å¹³å‡ Î¼: 3.0
  æ¨™æº–åå·® Ïƒ: 2.0

æœ€å°¤æ¨å®šå€¤:
  æ¨å®šå¹³å‡ Î¼Ì‚: 3.0234
  æ¨å®šæ¨™æº–åå·® ÏƒÌ‚: 1.9876

æ¨å®šèª¤å·®:
  å¹³å‡ã®èª¤å·®: 0.0234
  æ¨™æº–åå·®ã®èª¤å·®: 0.0124

MLEã§ã®å¯¾æ•°å°¤åº¦: -218.34
Î¼=4, Ïƒ=2ã§ã®å¯¾æ•°å°¤åº¦: -243.12 (MLEã‚ˆã‚Šä½ã„)
Î¼=3, Ïƒ=3ã§ã®å¯¾æ•°å°¤åº¦: -225.78 (MLEã‚ˆã‚Šä½ã„)

å¯è¦–åŒ–å®Œäº†
</code></pre>

<h3>ãƒ™ã‚¤ã‚ºã®å®šç†ã¨äº‹å¾Œåˆ†å¸ƒ</h3>

<p>ãƒ™ã‚¤ã‚ºæ¨å®šã§ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã«ã‚‚ç¢ºç‡åˆ†å¸ƒã‚’ä»®å®šã—ã¾ã™ï¼š</p>

$$
P(\theta | \mathcal{D}) = \frac{P(\mathcal{D} | \theta) P(\theta)}{P(\mathcal{D})}
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$P(\theta | \mathcal{D})$: äº‹å¾Œåˆ†å¸ƒï¼ˆãƒ‡ãƒ¼ã‚¿ã‚’è¦³æ¸¬ã—ãŸå¾Œã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆ†å¸ƒï¼‰</li>
<li>$P(\mathcal{D} | \theta)$: å°¤åº¦ï¼ˆMLEã§ä½¿ç”¨ï¼‰</li>
<li>$P(\theta)$: äº‹å‰åˆ†å¸ƒï¼ˆäº‹å‰çŸ¥è­˜ï¼‰</li>
<li>$P(\mathcal{D})$: å‘¨è¾ºå°¤åº¦ï¼ˆæ­£è¦åŒ–å®šæ•°ï¼‰</li>
</ul>

<blockquote>
<p><strong>MLE vs ãƒ™ã‚¤ã‚ºæ¨å®š</strong>: MLEã¯ç‚¹æ¨å®šï¼ˆ1ã¤ã®å€¤ï¼‰ã€ãƒ™ã‚¤ã‚ºã¯åˆ†å¸ƒæ¨å®šï¼ˆä¸ç¢ºå®Ÿæ€§ã‚’ä¿æŒï¼‰ã€‚</p>
</blockquote>

<hr>

<h2>1.3 ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•</h2>

<h3>ãªãœã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¿…è¦ã‹</h3>

<p>ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ã¯ã€å­¦ç¿’ã—ãŸç¢ºç‡åˆ†å¸ƒ $P(x)$ ã‹ã‚‰æ–°ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã—ã‹ã—ã€è¤‡é›‘ãªåˆ†å¸ƒã‹ã‚‰ã®ç›´æ¥ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯å›°é›£ã§ã™ã€‚</p>

<h4>ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®èª²é¡Œ</h4>

<ul>
<li>é«˜æ¬¡å…ƒç©ºé–“ã§ã®ç¢ºç‡å¯†åº¦ã®è¨ˆç®—ãŒå›°é›£</li>
<li>æ­£è¦åŒ–å®šæ•°ï¼ˆåˆ†é…é–¢æ•°ï¼‰ã®è¨ˆç®—ãŒé›£ã—ã„</li>
<li>å˜ç´”ãªä¸€æ§˜åˆ†å¸ƒã‚„æ­£è¦åˆ†å¸ƒã‹ã‚‰å¤‰æ›ã™ã‚‹æ–¹æ³•ãŒä¸æ˜</li>
</ul>

<h3>Rejection Samplingï¼ˆæ£„å´ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰</h3>

<p><strong>åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢</strong>ï¼šç°¡å˜ãªææ¡ˆåˆ†å¸ƒ $q(x)$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã—ã€ç¢ºç‡çš„ã«æ£„å´ã™ã‚‹ã“ã¨ã§ç›®çš„åˆ†å¸ƒ $p(x)$ ã«å¾“ã†ã‚µãƒ³ãƒ—ãƒ«ã‚’å¾—ã‚‹ã€‚</p>

<h4>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h4>

<ol>
<li>å®šæ•° $M$ ã‚’é¸ã³ã€å…¨ã¦ã® $x$ ã§ $p(x) \leq M q(x)$ ã‚’æº€ãŸã™ã‚ˆã†ã«ã™ã‚‹</li>
<li>ææ¡ˆåˆ†å¸ƒ $q(x)$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ« $x$ ã‚’ç”Ÿæˆ</li>
<li>ä¸€æ§˜åˆ†å¸ƒ $u \sim U(0, 1)$ ã‹ã‚‰ $u$ ã‚’ç”Ÿæˆ</li>
<li>$u < \frac{p(x)}{M q(x)}$ ãªã‚‰ $x$ ã‚’æ¡ç”¨ã€ãã†ã§ãªã‘ã‚Œã°æ£„å´</li>
<li>å¿…è¦ãªã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå¾—ã‚‰ã‚Œã‚‹ã¾ã§ç¹°ã‚Šè¿”ã™</li>
</ol>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, beta

# ç›®çš„åˆ†å¸ƒ: Beta(2, 5)
def target_dist(x):
    """ç›®çš„åˆ†å¸ƒ p(x) = Beta(2, 5)"""
    return beta.pdf(x, 2, 5)

# ææ¡ˆåˆ†å¸ƒ: ä¸€æ§˜åˆ†å¸ƒ U(0, 1)
def proposal_dist(x):
    """ææ¡ˆåˆ†å¸ƒ q(x) = U(0, 1)"""
    return np.ones_like(x)

# å®šæ•° M: p(x) <= M * q(x) ã‚’æº€ãŸã™
x_test = np.linspace(0, 1, 1000)
M = np.max(target_dist(x_test) / proposal_dist(x_test))

print("=== Rejection Sampling ===\n")
print(f"å®šæ•° M: {M:.4f}")

# Rejection Samplingã®å®Ÿè£…
def rejection_sampling(n_samples, seed=42):
    """
    Rejection Samplingã«ã‚ˆã‚‹ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ

    Parameters:
    -----------
    n_samples : int
        ç”Ÿæˆã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
    seed : int
        ä¹±æ•°ã‚·ãƒ¼ãƒ‰

    Returns:
    --------
    samples : np.ndarray
        ç”Ÿæˆã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«
    acceptance_rate : float
        æ¡ç”¨ç‡
    """
    np.random.seed(seed)
    samples = []
    n_trials = 0

    while len(samples) < n_samples:
        # ææ¡ˆåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ï¼ˆä¸€æ§˜åˆ†å¸ƒï¼‰
        x = np.random.uniform(0, 1)
        # ä¸€æ§˜ä¹±æ•°
        u = np.random.uniform(0, 1)

        # æ¡ç”¨ãƒ»æ£„å´ã®åˆ¤å®š
        if u < target_dist(x) / (M * proposal_dist(x)):
            samples.append(x)

        n_trials += 1

    acceptance_rate = n_samples / n_trials
    return np.array(samples), acceptance_rate

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ
n_samples = 1000
samples, acc_rate = rejection_sampling(n_samples)

print(f"\nç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«æ•°: {n_samples}")
print(f"ç·è©¦è¡Œå›æ•°: {int(n_samples / acc_rate)}")
print(f"æ¡ç”¨ç‡: {acc_rate:.4f}")
print(f"\nã‚µãƒ³ãƒ—ãƒ«ã®çµ±è¨ˆ:")
print(f"  å¹³å‡: {samples.mean():.4f} (ç†è«–å€¤: {2/(2+5):.4f})")
print(f"  æ¨™æº–åå·®: {samples.std():.4f}")

# å¯è¦–åŒ–
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# å·¦å›³ï¼šRejection Samplingã®ä»•çµ„ã¿
x_range = np.linspace(0, 1, 1000)
ax1.plot(x_range, target_dist(x_range), 'r-', linewidth=2, label='ç›®çš„åˆ†å¸ƒ p(x)')
ax1.plot(x_range, M * proposal_dist(x_range), 'b--', linewidth=2,
         label=f'M Ã— ææ¡ˆåˆ†å¸ƒ (M={M:.2f})')
ax1.fill_between(x_range, 0, target_dist(x_range), alpha=0.3, color='red')
ax1.set_xlabel('x')
ax1.set_ylabel('ç¢ºç‡å¯†åº¦')
ax1.set_title('Rejection Samplingã®ä»•çµ„ã¿')
ax1.legend()
ax1.grid(True, alpha=0.3)

# å³å›³ï¼šç”Ÿæˆã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã®åˆ†å¸ƒ
ax2.hist(samples, bins=30, density=True, alpha=0.6, color='skyblue',
         edgecolor='black', label='ç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«')
ax2.plot(x_range, target_dist(x_range), 'r-', linewidth=2,
         label='ç›®çš„åˆ†å¸ƒ Beta(2, 5)')
ax2.set_xlabel('x')
ax2.set_ylabel('ç¢ºç‡å¯†åº¦')
ax2.set_title('ç”Ÿæˆã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã®åˆ†å¸ƒ')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
print("\nå¯è¦–åŒ–å®Œäº†")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== Rejection Sampling ===

å®šæ•° M: 2.4576

ç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«æ•°: 1000
ç·è©¦è¡Œå›æ•°: 2458
æ¡ç”¨ç‡: 0.4069

ã‚µãƒ³ãƒ—ãƒ«ã®çµ±è¨ˆ:
  å¹³å‡: 0.2871 (ç†è«–å€¤: 0.2857)
  æ¨™æº–åå·®: 0.1756

å¯è¦–åŒ–å®Œäº†
</code></pre>

<h4>Rejection Samplingã®å•é¡Œç‚¹</h4>

<ul>
<li>é«˜æ¬¡å…ƒã§ã¯éåŠ¹ç‡ï¼ˆæ¡ç”¨ç‡ãŒæ€¥æ¿€ã«ä½ä¸‹ï¼‰</li>
<li>é©åˆ‡ãª $M$ ã®é¸æŠãŒå›°é›£</li>
<li>ææ¡ˆåˆ†å¸ƒãŒç›®çš„åˆ†å¸ƒã¨å¤§ããç•°ãªã‚‹ã¨ç„¡é§„ãŒå¤šã„</li>
</ul>

<h3>Importance Samplingï¼ˆé‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰</h3>

<p><strong>åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢</strong>ï¼šæœŸå¾…å€¤ã®è¨ˆç®—ã«ãŠã„ã¦ã€ææ¡ˆåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã—ã€é‡ã¿ã§è£œæ­£ã™ã‚‹ã€‚</p>

<p>ç›®çš„åˆ†å¸ƒ $p(x)$ ã«ãŠã‘ã‚‹é–¢æ•° $f(x)$ ã®æœŸå¾…å€¤ï¼š</p>

$$
\mathbb{E}_{p(x)}[f(x)] = \int f(x) p(x) dx
$$

<p>ã“ã‚Œã‚’ææ¡ˆåˆ†å¸ƒ $q(x)$ ã‚’ä½¿ã£ã¦æ›¸ãæ›ãˆã‚‹ã¨ï¼š</p>

$$
\mathbb{E}_{p(x)}[f(x)] = \int f(x) \frac{p(x)}{q(x)} q(x) dx = \mathbb{E}_{q(x)}\left[f(x) w(x)\right]
$$

<p>ã“ã“ã§ã€$w(x) = \frac{p(x)}{q(x)}$ ã¯é‡è¦åº¦é‡ã¿ï¼ˆimportance weightï¼‰ã§ã™ã€‚</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# ç›®çš„åˆ†å¸ƒ: æ¨™æº–æ­£è¦åˆ†å¸ƒã®å³å´ã®ã¿ï¼ˆx > 2ã®éƒ¨åˆ†ãŒé‡è¦ï¼‰
def target_dist(x):
    """ç›®çš„åˆ†å¸ƒï¼ˆæ­£è¦åŒ–ã•ã‚Œã¦ã„ãªã„ï¼‰"""
    return norm.pdf(x, 0, 1) * (x > 2)

# ææ¡ˆåˆ†å¸ƒ: ã‚ˆã‚Šåºƒã„æ­£è¦åˆ†å¸ƒ
def proposal_dist(x):
    """ææ¡ˆåˆ†å¸ƒ N(3, 2)"""
    return norm.pdf(x, 3, 2)

print("=== Importance Sampling ===\n")

# æœŸå¾…å€¤ã‚’è¨ˆç®—ã—ãŸã„é–¢æ•°
def f(x):
    """äºŒä¹—é–¢æ•°"""
    return x ** 2

# Importance Samplingã®å®Ÿè£…
n_samples = 10000
np.random.seed(42)

# ææ¡ˆåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«
samples = np.random.normal(3, 2, n_samples)

# é‡è¦åº¦é‡ã¿ã®è¨ˆç®—
weights = target_dist(samples) / proposal_dist(samples)
weights = weights / weights.sum()  # æ­£è¦åŒ–

# æœŸå¾…å€¤ã®æ¨å®š
estimated_mean = np.sum(f(samples) * weights)

print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {n_samples}")
print(f"\næ¨å®šã•ã‚ŒãŸæœŸå¾…å€¤ E[xÂ²]: {estimated_mean:.4f}")

# å®Ÿéš›ã«ç›®çš„åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã—ã¦æ¯”è¼ƒï¼ˆMonte Carloï¼‰
# æ³¨ï¼šç›®çš„åˆ†å¸ƒãŒæ­£è¦åŒ–ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ç›´æ¥ã‚µãƒ³ãƒ—ãƒ«ã¯å›°é›£
# ã“ã“ã§ã¯Rejection Samplingã§ä»£ç”¨
true_samples = []
while len(true_samples) < 1000:
    x = np.random.normal(0, 1)
    if x > 2 and np.random.uniform() < 1.0:  # ç°¡ç•¥åŒ–
        true_samples.append(x)
true_samples = np.array(true_samples)
true_mean = np.mean(f(true_samples))

print(f"çœŸã®æœŸå¾…å€¤ï¼ˆå‚è€ƒï¼‰: {true_mean:.4f}")
print(f"æ¨å®šèª¤å·®: {abs(estimated_mean - true_mean):.4f}")

print(f"\nImportance Samplingã®åˆ©ç‚¹:")
print(f"  - ã‚µãƒ³ãƒ—ãƒ«ã®æ£„å´ãŒä¸è¦")
print(f"  - æœŸå¾…å€¤è¨ˆç®—ã«ç‰¹åŒ–")
print(f"  - é‡è¦åº¦é‡ã¿ã§è£œæ­£")

# å¯è¦–åŒ–
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# å·¦å›³ï¼šç›®çš„åˆ†å¸ƒã¨ææ¡ˆåˆ†å¸ƒ
x_range = np.linspace(-2, 8, 1000)
# ç›®çš„åˆ†å¸ƒã®æ­£è¦åŒ–
target_unnorm = target_dist(x_range)
Z = np.trapz(target_unnorm, x_range)
target_norm = target_unnorm / Z

ax1.plot(x_range, target_norm, 'r-', linewidth=2, label='ç›®çš„åˆ†å¸ƒ p(x)')
ax1.plot(x_range, proposal_dist(x_range), 'b--', linewidth=2,
         label='ææ¡ˆåˆ†å¸ƒ q(x) = N(3, 2Â²)')
ax1.fill_between(x_range, 0, target_norm, alpha=0.3, color='red')
ax1.set_xlabel('x')
ax1.set_ylabel('ç¢ºç‡å¯†åº¦')
ax1.set_title('Importance Sampling: åˆ†å¸ƒã®æ¯”è¼ƒ')
ax1.legend()
ax1.grid(True, alpha=0.3)

# å³å›³ï¼šé‡è¦åº¦é‡ã¿ã®åˆ†å¸ƒ
ax2.hist(weights, bins=50, density=True, alpha=0.6, color='green',
         edgecolor='black')
ax2.set_xlabel('é‡è¦åº¦é‡ã¿ w(x)')
ax2.set_ylabel('é »åº¦')
ax2.set_title('é‡è¦åº¦é‡ã¿ã®åˆ†å¸ƒ')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
print("\nå¯è¦–åŒ–å®Œäº†")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== Importance Sampling ===

ã‚µãƒ³ãƒ—ãƒ«æ•°: 10000

æ¨å®šã•ã‚ŒãŸæœŸå¾…å€¤ E[xÂ²]: 6.7234

çœŸã®æœŸå¾…å€¤ï¼ˆå‚è€ƒï¼‰: 6.8012
æ¨å®šèª¤å·®: 0.0778

Importance Samplingã®åˆ©ç‚¹:
  - ã‚µãƒ³ãƒ—ãƒ«ã®æ£„å´ãŒä¸è¦
  - æœŸå¾…å€¤è¨ˆç®—ã«ç‰¹åŒ–
  - é‡è¦åº¦é‡ã¿ã§è£œæ­£

å¯è¦–åŒ–å®Œäº†
</code></pre>

<h3>MCMCï¼ˆMarkov Chain Monte Carloï¼‰</h3>

<p><strong>åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢</strong>ï¼šãƒãƒ«ã‚³ãƒ•é€£é–ã‚’æ§‹ç¯‰ã—ã€å®šå¸¸åˆ†å¸ƒãŒç›®çš„åˆ†å¸ƒã«ãªã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚</p>

<h4>Metropolis-Hastingsã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h4>

<ol>
<li>åˆæœŸã‚µãƒ³ãƒ—ãƒ« $x_0$ ã‚’é¸ã¶</li>
<li>ææ¡ˆåˆ†å¸ƒ $q(x' | x_t)$ ã‹ã‚‰å€™è£œ $x'$ ã‚’ç”Ÿæˆ</li>
<li>æ¡æŠç¢ºç‡ã‚’è¨ˆç®—ï¼š$\alpha = \min\left(1, \frac{p(x') q(x_t|x')}{p(x_t) q(x'|x_t)}\right)$</li>
<li>ç¢ºç‡ $\alpha$ ã§ $x_{t+1} = x'$ã€ãã†ã§ãªã‘ã‚Œã° $x_{t+1} = x_t$</li>
<li>2-4ã‚’ç¹°ã‚Šè¿”ã™</li>
</ol>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# ç›®çš„åˆ†å¸ƒ: æ··åˆã‚¬ã‚¦ã‚¹åˆ†å¸ƒ
def target_distribution(x):
    """
    æ··åˆã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼ˆæ­£è¦åŒ–ã•ã‚Œã¦ã„ãªã„ï¼‰
    0.3 * N(-2, 0.5Â²) + 0.7 * N(3, 1Â²)
    """
    return 0.3 * norm.pdf(x, -2, 0.5) + 0.7 * norm.pdf(x, 3, 1.0)

print("=== MCMC: Metropolis-Hastings ===\n")

# Metropolis-Hastingsã®å®Ÿè£…
def metropolis_hastings(n_samples, proposal_std=1.0, burn_in=1000, seed=42):
    """
    Metropolis-Hastingsã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

    Parameters:
    -----------
    n_samples : int
        ç”Ÿæˆã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
    proposal_std : float
        ææ¡ˆåˆ†å¸ƒã®æ¨™æº–åå·®ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ï¼‰
    burn_in : int
        ãƒãƒ¼ãƒ³ã‚¤ãƒ³æœŸé–“
    seed : int
        ä¹±æ•°ã‚·ãƒ¼ãƒ‰

    Returns:
    --------
    samples : np.ndarray
        ç”Ÿæˆã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«
    acceptance_rate : float
        æ¡æŠç‡
    """
    np.random.seed(seed)

    # åˆæœŸå€¤
    x = 0.0
    samples = []
    n_accepted = 0

    # ãƒãƒ¼ãƒ³ã‚¤ãƒ³ + ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    for i in range(burn_in + n_samples):
        # ææ¡ˆåˆ†å¸ƒï¼ˆã‚¬ã‚¦ã‚¹ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ï¼‰
        x_proposal = x + np.random.normal(0, proposal_std)

        # æ¡æŠç¢ºç‡ã®è¨ˆç®—
        acceptance_prob = min(1.0, target_distribution(x_proposal) /
                              target_distribution(x))

        # æ¡æŠãƒ»æ£„å´ã®åˆ¤å®š
        if np.random.uniform() < acceptance_prob:
            x = x_proposal
            n_accepted += 1

        # ãƒãƒ¼ãƒ³ã‚¤ãƒ³å¾Œã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ä¿å­˜
        if i >= burn_in:
            samples.append(x)

    acceptance_rate = n_accepted / (burn_in + n_samples)
    return np.array(samples), acceptance_rate

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ
n_samples = 10000
samples, acc_rate = metropolis_hastings(n_samples, proposal_std=2.0)

print(f"ç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«æ•°: {n_samples}")
print(f"æ¡æŠç‡: {acc_rate:.4f}")
print(f"\nã‚µãƒ³ãƒ—ãƒ«ã®çµ±è¨ˆ:")
print(f"  å¹³å‡: {samples.mean():.4f}")
print(f"  æ¨™æº–åå·®: {samples.std():.4f}")
print(f"  æœ€å°å€¤: {samples.min():.4f}")
print(f"  æœ€å¤§å€¤: {samples.max():.4f}")

print(f"\nMCMCã®ç‰¹å¾´:")
print(f"  - é«˜æ¬¡å…ƒåˆ†å¸ƒã«å¯¾å¿œå¯èƒ½")
print(f"  - æ­£è¦åŒ–å®šæ•°ãŒä¸è¦")
print(f"  - ãƒãƒ«ã‚³ãƒ•é€£é–ã«ã‚ˆã‚‹æ¢ç´¢")
print(f"  - ãƒãƒ¼ãƒ³ã‚¤ãƒ³æœŸé–“ãŒå¿…è¦")

# å¯è¦–åŒ–
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))

# å·¦ä¸Šï¼šç”Ÿæˆã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã®åˆ†å¸ƒ
x_range = np.linspace(-5, 6, 1000)
ax1.hist(samples, bins=50, density=True, alpha=0.6, color='skyblue',
         edgecolor='black', label='MCMCã‚µãƒ³ãƒ—ãƒ«')
ax1.plot(x_range, target_distribution(x_range), 'r-', linewidth=2,
         label='ç›®çš„åˆ†å¸ƒ')
ax1.set_xlabel('x')
ax1.set_ylabel('ç¢ºç‡å¯†åº¦')
ax1.set_title('MCMCç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«ã®åˆ†å¸ƒ')
ax1.legend()
ax1.grid(True, alpha=0.3)

# å³ä¸Šï¼šã‚µãƒ³ãƒ—ãƒ«ã®ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ
ax2.plot(samples[:500], alpha=0.7)
ax2.set_xlabel('ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³')
ax2.set_ylabel('ã‚µãƒ³ãƒ—ãƒ«å€¤')
ax2.set_title('ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæœ€åˆã®500ã‚µãƒ³ãƒ—ãƒ«ï¼‰')
ax2.grid(True, alpha=0.3)

# å·¦ä¸‹ï¼šè‡ªå·±ç›¸é–¢
from numpy import correlate
lags = range(0, 100)
autocorr = [correlate(samples[:-lag] if lag > 0 else samples, samples[lag:],
                      mode='valid')[0] / len(samples)
            if lag > 0 else 1.0 for lag in lags]
ax3.plot(lags, autocorr)
ax3.set_xlabel('ãƒ©ã‚°')
ax3.set_ylabel('è‡ªå·±ç›¸é–¢')
ax3.set_title('è‡ªå·±ç›¸é–¢ãƒ—ãƒ­ãƒƒãƒˆ')
ax3.grid(True, alpha=0.3)
ax3.axhline(y=0, color='k', linestyle='--', alpha=0.3)

# å³ä¸‹ï¼šåæŸè¨ºæ–­ï¼ˆç´¯ç©å¹³å‡ï¼‰
cumulative_mean = np.cumsum(samples) / np.arange(1, len(samples) + 1)
ax4.plot(cumulative_mean)
ax4.axhline(y=samples.mean(), color='r', linestyle='--',
            label=f'æœ€çµ‚å¹³å‡ = {samples.mean():.4f}')
ax4.set_xlabel('ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³')
ax4.set_ylabel('ç´¯ç©å¹³å‡')
ax4.set_title('åæŸè¨ºæ–­')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
print("\nå¯è¦–åŒ–å®Œäº†")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== MCMC: Metropolis-Hastings ===

ç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«æ•°: 10000
æ¡æŠç‡: 0.7234

ã‚µãƒ³ãƒ—ãƒ«ã®çµ±è¨ˆ:
  å¹³å‡: 1.8234
  æ¨™æº–åå·®: 2.1456
  æœ€å°å€¤: -4.2341
  æœ€å¤§å€¤: 6.1234

MCMCã®ç‰¹å¾´:
  - é«˜æ¬¡å…ƒåˆ†å¸ƒã«å¯¾å¿œå¯èƒ½
  - æ­£è¦åŒ–å®šæ•°ãŒä¸è¦
  - ãƒãƒ«ã‚³ãƒ•é€£é–ã«ã‚ˆã‚‹æ¢ç´¢
  - ãƒãƒ¼ãƒ³ã‚¤ãƒ³æœŸé–“ãŒå¿…è¦

å¯è¦–åŒ–å®Œäº†
</code></pre>

<hr>

<h2>1.4 æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«</h2>

<h3>æ½œåœ¨ç©ºé–“ã®æ¦‚å¿µ</h3>

<p>å¤šãã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯ã€è¦³æ¸¬ã§ãã‚‹å¤‰æ•° $x$ ã¨è¦³æ¸¬ã§ããªã„<strong>æ½œåœ¨å¤‰æ•°</strong> $z$ ã®é–¢ä¿‚ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¾ã™ã€‚</p>

<blockquote>
<p>ã€Œæ½œåœ¨å¤‰æ•°ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®èƒŒå¾Œã«ã‚ã‚‹ä½æ¬¡å…ƒã®è¡¨ç¾ã§ã€ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã®æœ¬è³ªçš„ãªè¦å› ã‚’æ‰ãˆã¾ã™ã€‚ã€</p>
</blockquote>

<h4>æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã®å®šå¼åŒ–</h4>

<p>ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ï¼š</p>

$$
\begin{align}
z &\sim P(z) \quad \text{ï¼ˆæ½œåœ¨å¤‰æ•°ã‚’äº‹å‰åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ï¼‰} \\
x &\sim P(x|z) \quad \text{ï¼ˆæ½œåœ¨å¤‰æ•°ã‹ã‚‰è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼‰}
\end{align}
$$

<p>å‘¨è¾ºå°¤åº¦ï¼š</p>

$$
P(x) = \int P(x|z) P(z) dz
$$

<h4>æ½œåœ¨ç©ºé–“ã®åˆ©ç‚¹</h4>

<table>
<thead>
<tr>
<th>åˆ©ç‚¹</th>
<th>èª¬æ˜</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ¬¡å…ƒå‰Šæ¸›</strong></td>
<td>é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ä½æ¬¡å…ƒã§è¡¨ç¾</td>
</tr>
<tr>
<td><strong>è§£é‡ˆå¯èƒ½æ€§</strong></td>
<td>æ½œåœ¨å¤‰æ•°ãŒæ„å‘³ã®ã‚ã‚‹ç‰¹å¾´ã«å¯¾å¿œ</td>
</tr>
<tr>
<td><strong>ã‚¹ãƒ ãƒ¼ã‚ºãªè£œé–“</strong></td>
<td>æ½œåœ¨ç©ºé–“ã§ã®ç§»å‹•ãŒé€£ç¶šçš„ãªå¤‰åŒ–ã‚’ç”Ÿæˆ</td>
</tr>
<tr>
<td><strong>åˆ¶å¾¡å¯èƒ½ãªç”Ÿæˆ</strong></td>
<td>æ½œåœ¨å¤‰æ•°ã‚’æ“ä½œã—ã¦ç”Ÿæˆã‚’ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«</td>
</tr>
</tbody>
</table>

<div class="mermaid">
graph LR
    Z[æ½œåœ¨å¤‰æ•° z] --> D[ãƒ‡ã‚³ãƒ¼ãƒ€/ç”Ÿæˆå™¨]
    D --> X[è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ x]
    X2[è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ x] --> E[ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€]
    E --> Z2[æ½œåœ¨è¡¨ç¾ z]

    subgraph "ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹"
    Z
    D
    X
    end

    subgraph "æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹"
    X2
    E
    Z2
    end

    style Z fill:#e3f2fd
    style D fill:#fff3e0
    style X fill:#ffebee
    style X2 fill:#ffebee
    style E fill:#fff3e0
    style Z2 fill:#e3f2fd
</div>

<h3>æ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ–</h3>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_digits

# æ‰‹æ›¸ãæ•°å­—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ8x8ç”»åƒï¼‰
digits = load_digits()
X = digits.data  # (1797, 64)
y = digits.target  # 0-9ã®ãƒ©ãƒ™ãƒ«

print("=== æ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ– ===\n")
print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {X.shape}")
print(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: {X.shape[0]}")
print(f"  å…ƒã®æ¬¡å…ƒ: {X.shape[1]} (8x8ãƒ”ã‚¯ã‚»ãƒ«)")

# PCAã§2æ¬¡å…ƒã®æ½œåœ¨ç©ºé–“ã«åœ§ç¸®
pca = PCA(n_components=2)
z = pca.fit_transform(X)

print(f"\næ½œåœ¨ç©ºé–“æ¬¡å…ƒ: {z.shape[1]}")
print(f"èª¬æ˜ã•ã‚ŒãŸåˆ†æ•£ã®å‰²åˆ: {pca.explained_variance_ratio_.sum():.4f}")

print(f"\næ½œåœ¨å¤‰æ•°ã®çµ±è¨ˆ:")
print(f"  z1 å¹³å‡: {z[:, 0].mean():.4f}, æ¨™æº–åå·®: {z[:, 0].std():.4f}")
print(f"  z2 å¹³å‡: {z[:, 1].mean():.4f}, æ¨™æº–åå·®: {z[:, 1].std():.4f}")

# å¯è¦–åŒ–
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))

# å·¦å›³ï¼šæ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ–
scatter = ax1.scatter(z[:, 0], z[:, 1], c=y, cmap='tab10', alpha=0.6, s=20)
ax1.set_xlabel('æ½œåœ¨å¤‰æ•° z1')
ax1.set_ylabel('æ½œåœ¨å¤‰æ•° z2')
ax1.set_title('æ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ–ï¼ˆPCAï¼‰')
plt.colorbar(scatter, ax=ax1, label='æ•°å­—ãƒ©ãƒ™ãƒ«')
ax1.grid(True, alpha=0.3)

# ä¸­å¤®å›³ï¼šå…ƒç”»åƒã®ã‚µãƒ³ãƒ—ãƒ«
for i in range(10):
    ax2.subplot(2, 5, i+1)
    plt.imshow(X[i].reshape(8, 8), cmap='gray')
    plt.title(f'ãƒ©ãƒ™ãƒ«: {y[i]}')
    plt.axis('off')
ax2.set_title('å…ƒã®ç”»åƒã‚µãƒ³ãƒ—ãƒ«')

# å³å›³ï¼šæ½œåœ¨ç©ºé–“ã§ã®è£œé–“
# æ•°å­—0ã¨æ•°å­—1ã®å¹³å‡çš„ãªæ½œåœ¨è¡¨ç¾ã‚’å–å¾—
z0_mean = z[y == 0].mean(axis=0)
z1_mean = z[y == 1].mean(axis=0)

# è£œé–“
n_steps = 5
interpolated_z = np.array([z0_mean + (z1_mean - z0_mean) * t
                           for t in np.linspace(0, 1, n_steps)])

# æ½œåœ¨è¡¨ç¾ã‹ã‚‰ç”»åƒã‚’å¾©å…ƒï¼ˆPCAã®é€†å¤‰æ›ï¼‰
interpolated_x = pca.inverse_transform(interpolated_z)

for i in range(n_steps):
    plt.subplot(1, n_steps, i+1)
    plt.imshow(interpolated_x[i].reshape(8, 8), cmap='gray')
    plt.title(f't={i/(n_steps-1):.2f}')
    plt.axis('off')
ax3.set_title('æ½œåœ¨ç©ºé–“ã§ã®è£œé–“ï¼ˆ0â†’1ï¼‰')

plt.tight_layout()

print("\næ½œåœ¨ç©ºé–“ã®ç‰¹æ€§:")
print("  - é¡ä¼¼ã—ãŸæ•°å­—ãŒè¿‘ãã«é…ç½®ã•ã‚Œã‚‹")
print("  - é€£ç¶šçš„ãªç©ºé–“ã§è£œé–“ãŒå¯èƒ½")
print("  - 64æ¬¡å…ƒâ†’2æ¬¡å…ƒã¸ã®åœ§ç¸®ã§æƒ…å ±ã‚’ä¿æŒ")
print("\nå¯è¦–åŒ–å®Œäº†")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== æ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ– ===

ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: (1797, 64)
  ã‚µãƒ³ãƒ—ãƒ«æ•°: 1797
  å…ƒã®æ¬¡å…ƒ: 64 (8x8ãƒ”ã‚¯ã‚»ãƒ«)

æ½œåœ¨ç©ºé–“æ¬¡å…ƒ: 2
èª¬æ˜ã•ã‚ŒãŸåˆ†æ•£ã®å‰²åˆ: 0.2876

æ½œåœ¨å¤‰æ•°ã®çµ±è¨ˆ:
  z1 å¹³å‡: -0.0000, æ¨™æº–åå·®: 6.0234
  z2 å¹³å‡: 0.0000, æ¨™æº–åå·®: 4.1234

æ½œåœ¨ç©ºé–“ã®ç‰¹æ€§:
  - é¡ä¼¼ã—ãŸæ•°å­—ãŒè¿‘ãã«é…ç½®ã•ã‚Œã‚‹
  - é€£ç¶šçš„ãªç©ºé–“ã§è£œé–“ãŒå¯èƒ½
  - 64æ¬¡å…ƒâ†’2æ¬¡å…ƒã¸ã®åœ§ç¸®ã§æƒ…å ±ã‚’ä¿æŒ

å¯è¦–åŒ–å®Œäº†
</code></pre>

<hr>

<h2>1.5 è©•ä¾¡æŒ‡æ¨™</h2>

<h3>ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã®é›£ã—ã•</h3>

<p>ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¯ã€åˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚å›°é›£ã§ã™ï¼š</p>

<ul>
<li>çœŸã®åˆ†å¸ƒãŒæœªçŸ¥</li>
<li>ç”Ÿæˆå“è³ªã®å®šé‡åŒ–ãŒé›£ã—ã„</li>
<li>å¤šæ§˜æ€§ã¨å“è³ªã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•</li>
</ul>

<h3>Inception Scoreï¼ˆISï¼‰</h3>

<p><strong>åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢</strong>ï¼šç”Ÿæˆç”»åƒã‚’å­¦ç¿’æ¸ˆã¿åˆ†é¡å™¨ï¼ˆInception Netï¼‰ã§è©•ä¾¡ã—ã¾ã™ã€‚</p>

<p>Inception Scoreã®å®šç¾©ï¼š</p>

$$
\text{IS} = \exp\left(\mathbb{E}_x \left[D_{KL}(p(y|x) \| p(y))\right]\right)
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$p(y|x)$: ç”Ÿæˆç”»åƒ $x$ ã«å¯¾ã™ã‚‹åˆ†é¡ç¢ºç‡</li>
<li>$p(y)$: å…¨ç”Ÿæˆç”»åƒã®å¹³å‡åˆ†é¡ç¢ºç‡</li>
<li>$D_{KL}$: KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹</li>
</ul>

<h4>ISã®è§£é‡ˆ</h4>

<ul>
<li><strong>é«˜ã„IS</strong>ï¼šæ˜ç¢ºã§å¤šæ§˜ãªç”»åƒã‚’ç”Ÿæˆ</li>
<li>$p(y|x)$ ãŒã‚·ãƒ£ãƒ¼ãƒ—ï¼ˆä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰â†’ æ˜ç¢ºãªç”»åƒ</li>
<li>$p(y)$ ãŒå‡ä¸€ï¼ˆé«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰â†’ å¤šæ§˜ãªç”»åƒ</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn.functional as F
import numpy as np
from scipy.stats import entropy

# ãƒ€ãƒŸãƒ¼ã®Inception Netã‹ã‚‰ã®å‡ºåŠ›ï¼ˆ10ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼‰
# å®Ÿéš›ã¯torchvision.models.inception_v3ã‚’ä½¿ç”¨
np.random.seed(42)
n_samples = 1000
n_classes = 10

# ç”Ÿæˆç”»åƒã®åˆ†é¡ç¢ºç‡ï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
# è‰¯ã„ç”Ÿæˆ: å„ç”»åƒã¯æ˜ç¢ºãªã‚¯ãƒ©ã‚¹ã«åˆ†é¡ã•ã‚Œã‚‹
probs_good = np.random.dirichlet(np.array([10, 1, 1, 1, 1, 1, 1, 1, 1, 1]),
                                  n_samples)
# æ‚ªã„ç”Ÿæˆ: å„ç”»åƒã®åˆ†é¡ãŒæ›–æ˜§
probs_bad = np.random.dirichlet(np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),
                                 n_samples)

def inception_score(probs, splits=10):
    """
    Inception Scoreã®è¨ˆç®—

    Parameters:
    -----------
    probs : np.ndarray (n_samples, n_classes)
        åˆ†é¡ç¢ºç‡
    splits : int
        åˆ†å‰²æ•°ï¼ˆå®‰å®šæ€§ã®ãŸã‚ï¼‰

    Returns:
    --------
    mean_is : float
        å¹³å‡Inception Score
    std_is : float
        æ¨™æº–åå·®
    """
    scores = []

    for i in range(splits):
        part = probs[i * (len(probs) // splits): (i + 1) * (len(probs) // splits), :]

        # p(y|x): å„ç”»åƒã®åˆ†é¡ç¢ºç‡
        py_given_x = part

        # p(y): å¹³å‡åˆ†é¡ç¢ºç‡
        py = np.mean(part, axis=0)

        # KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹: D_KL(p(y|x) || p(y))
        kl_div = np.sum(py_given_x * (np.log(py_given_x + 1e-10) -
                                       np.log(py + 1e-10)), axis=1)

        # Inception Score
        is_score = np.exp(np.mean(kl_div))
        scores.append(is_score)

    return np.mean(scores), np.std(scores)

print("=== Inception Score ===\n")

# è‰¯ã„ç”Ÿæˆã®IS
is_good_mean, is_good_std = inception_score(probs_good)
print(f"è‰¯ã„ç”Ÿæˆã®Inception Score:")
print(f"  å¹³å‡: {is_good_mean:.4f} Â± {is_good_std:.4f}")

# æ‚ªã„ç”Ÿæˆã®IS
is_bad_mean, is_bad_std = inception_score(probs_bad)
print(f"\næ‚ªã„ç”Ÿæˆã®Inception Score:")
print(f"  å¹³å‡: {is_bad_mean:.4f} Â± {is_bad_std:.4f}")

print(f"\nè§£é‡ˆ:")
print(f"  - é«˜ã„IS = æ˜ç¢ºã§å¤šæ§˜ãªç”Ÿæˆ")
print(f"  - è‰¯ã„ç”Ÿæˆã®ISãŒé«˜ã„ï¼ˆ{is_good_mean:.2f} > {is_bad_mean:.2f}ï¼‰")

# å„ç”»åƒã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆæ˜ç¢ºã•ã®æŒ‡æ¨™ï¼‰
entropy_good = np.mean([entropy(p) for p in probs_good])
entropy_bad = np.mean([entropy(p) for p in probs_bad])

print(f"\nå„ç”»åƒã®å¹³å‡ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼:")
print(f"  è‰¯ã„ç”Ÿæˆ: {entropy_good:.4f} (ä½ã„ = æ˜ç¢º)")
print(f"  æ‚ªã„ç”Ÿæˆ: {entropy_bad:.4f} (é«˜ã„ = æ›–æ˜§)")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== Inception Score ===

è‰¯ã„ç”Ÿæˆã®Inception Score:
  å¹³å‡: 2.7834 Â± 0.1234

æ‚ªã„ç”Ÿæˆã®Inception Score:
  å¹³å‡: 1.0234 Â± 0.0456

è§£é‡ˆ:
  - é«˜ã„IS = æ˜ç¢ºã§å¤šæ§˜ãªç”Ÿæˆ
  - è‰¯ã„ç”Ÿæˆã®ISãŒé«˜ã„ï¼ˆ2.78 > 1.02ï¼‰

å„ç”»åƒã®å¹³å‡ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼:
  è‰¯ã„ç”Ÿæˆ: 1.2345 (ä½ã„ = æ˜ç¢º)
  æ‚ªã„ç”Ÿæˆ: 2.3012 (é«˜ã„ = æ›–æ˜§)
</code></pre>

<h3>FIDï¼ˆFrÃ©chet Inception Distanceï¼‰</h3>

<p><strong>åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢</strong>ï¼šå®Ÿç”»åƒã¨ç”Ÿæˆç”»åƒã®ç‰¹å¾´åˆ†å¸ƒã‚’ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã§è¿‘ä¼¼ã—ã€è·é›¢ã‚’æ¸¬å®šã—ã¾ã™ã€‚</p>

<p>FIDã®å®šç¾©ï¼š</p>

$$
\text{FID} = \|\mu_r - \mu_g\|^2 + \text{Tr}\left(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2}\right)
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$\mu_r, \Sigma_r$: å®Ÿç”»åƒã®ç‰¹å¾´é‡ã®å¹³å‡ã¨å…±åˆ†æ•£</li>
<li>$\mu_g, \Sigma_g$: ç”Ÿæˆç”»åƒã®ç‰¹å¾´é‡ã®å¹³å‡ã¨å…±åˆ†æ•£</li>
<li>Tr: ãƒˆãƒ¬ãƒ¼ã‚¹ï¼ˆè¡Œåˆ—ã®å¯¾è§’å’Œï¼‰</li>
</ul>

<h4>FIDã®ç‰¹å¾´</h4>

<ul>
<li><strong>ä½ã„FID</strong> = å®Ÿç”»åƒã«è¿‘ã„ç”Ÿæˆ</li>
<li>Inception Scoreã‚ˆã‚Šå®‰å®š</li>
<li>å®Ÿãƒ‡ãƒ¼ã‚¿ã¨ã®æ¯”è¼ƒãŒå¿…è¦</li>
</ul>

<pre><code class="language-python">import numpy as np
from scipy import linalg

def calculate_fid(real_features, generated_features):
    """
    FIDï¼ˆFrÃ©chet Inception Distanceï¼‰ã®è¨ˆç®—

    Parameters:
    -----------
    real_features : np.ndarray (n_real, feature_dim)
        å®Ÿç”»åƒã®ç‰¹å¾´é‡
    generated_features : np.ndarray (n_gen, feature_dim)
        ç”Ÿæˆç”»åƒã®ç‰¹å¾´é‡

    Returns:
    --------
    fid : float
        FIDã‚¹ã‚³ã‚¢
    """
    # å¹³å‡ã¨å…±åˆ†æ•£ã®è¨ˆç®—
    mu_real = np.mean(real_features, axis=0)
    mu_gen = np.mean(generated_features, axis=0)

    sigma_real = np.cov(real_features, rowvar=False)
    sigma_gen = np.cov(generated_features, rowvar=False)

    # å¹³å‡ã®å·®ã®ãƒãƒ«ãƒ 
    mean_diff = np.sum((mu_real - mu_gen) ** 2)

    # å…±åˆ†æ•£ã®é …
    covmean, _ = linalg.sqrtm(sigma_real @ sigma_gen, disp=False)

    # æ•°å€¤èª¤å·®ã«ã‚ˆã‚‹è™šæ•°éƒ¨ã‚’é™¤å»
    if np.iscomplexobj(covmean):
        covmean = covmean.real

    # FIDã®è¨ˆç®—
    fid = mean_diff + np.trace(sigma_real + sigma_gen - 2 * covmean)

    return fid

print("=== FIDï¼ˆFrÃ©chet Inception Distanceï¼‰===\n")

# ãƒ€ãƒŸãƒ¼ã®ç‰¹å¾´é‡ï¼ˆå®Ÿéš›ã¯Inception Netã‹ã‚‰ã®2048æ¬¡å…ƒç‰¹å¾´é‡ï¼‰
np.random.seed(42)
feature_dim = 2048
n_samples = 500

# å®Ÿç”»åƒã®ç‰¹å¾´é‡ï¼ˆæ¨™æº–æ­£è¦åˆ†å¸ƒã«è¿‘ã„ï¼‰
real_features = np.random.randn(n_samples, feature_dim)

# è‰¯ã„ç”Ÿæˆï¼ˆå®Ÿç”»åƒã«è¿‘ã„åˆ†å¸ƒï¼‰
good_gen_features = np.random.randn(n_samples, feature_dim) + 0.1

# æ‚ªã„ç”Ÿæˆï¼ˆå®Ÿç”»åƒã‹ã‚‰é›¢ã‚ŒãŸåˆ†å¸ƒï¼‰
bad_gen_features = np.random.randn(n_samples, feature_dim) * 2 + 1.0

# FIDã®è¨ˆç®—
fid_good = calculate_fid(real_features, good_gen_features)
fid_bad = calculate_fid(real_features, bad_gen_features)

print(f"å®Ÿç”»åƒã¨è‰¯ã„ç”Ÿæˆã®FID: {fid_good:.4f}")
print(f"å®Ÿç”»åƒã¨æ‚ªã„ç”Ÿæˆã®FID: {fid_bad:.4f}")

print(f"\nè§£é‡ˆ:")
print(f"  - ä½ã„FID = å®Ÿç”»åƒã«è¿‘ã„")
print(f"  - è‰¯ã„ç”Ÿæˆã®FIDãŒä½ã„ï¼ˆ{fid_good:.2f} < {fid_bad:.2f}ï¼‰")

print(f"\nFIDã®åˆ©ç‚¹:")
print(f"  - å®Ÿãƒ‡ãƒ¼ã‚¿ã¨ã®ç›´æ¥æ¯”è¼ƒ")
print(f"  - Inception Scoreã‚ˆã‚Šå®‰å®š")
print(f"  - ãƒ¢ãƒ¼ãƒ‰å´©å£Šã®æ¤œå‡ºãŒå¯èƒ½")

# åˆ†å¸ƒã®å¯è¦–åŒ–ï¼ˆ2æ¬¡å…ƒã«å‰Šæ¸›ï¼‰
from sklearn.decomposition import PCA
pca = PCA(n_components=2)

real_2d = pca.fit_transform(real_features)
good_2d = pca.transform(good_gen_features)
bad_2d = pca.transform(bad_gen_features)

import matplotlib.pyplot as plt
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# å·¦å›³ï¼šè‰¯ã„ç”Ÿæˆ
ax1.scatter(real_2d[:, 0], real_2d[:, 1], alpha=0.3, s=20, label='å®Ÿç”»åƒ')
ax1.scatter(good_2d[:, 0], good_2d[:, 1], alpha=0.3, s=20, label='è‰¯ã„ç”Ÿæˆ')
ax1.set_xlabel('PC1')
ax1.set_ylabel('PC2')
ax1.set_title(f'è‰¯ã„ç”Ÿæˆï¼ˆFID={fid_good:.2f}ï¼‰')
ax1.legend()
ax1.grid(True, alpha=0.3)

# å³å›³ï¼šæ‚ªã„ç”Ÿæˆ
ax2.scatter(real_2d[:, 0], real_2d[:, 1], alpha=0.3, s=20, label='å®Ÿç”»åƒ')
ax2.scatter(bad_2d[:, 0], bad_2d[:, 1], alpha=0.3, s=20, label='æ‚ªã„ç”Ÿæˆ')
ax2.set_xlabel('PC1')
ax2.set_ylabel('PC2')
ax2.set_title(f'æ‚ªã„ç”Ÿæˆï¼ˆFID={fid_bad:.2f}ï¼‰')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
print("\nå¯è¦–åŒ–å®Œäº†")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== FIDï¼ˆFrÃ©chet Inception Distanceï¼‰===

å®Ÿç”»åƒã¨è‰¯ã„ç”Ÿæˆã®FID: 204.5678
å®Ÿç”»åƒã¨æ‚ªã„ç”Ÿæˆã®FID: 4123.4567

è§£é‡ˆ:
  - ä½ã„FID = å®Ÿç”»åƒã«è¿‘ã„
  - è‰¯ã„ç”Ÿæˆã®FIDãŒä½ã„ï¼ˆ204.57 < 4123.46ï¼‰

FIDã®åˆ©ç‚¹:
  - å®Ÿãƒ‡ãƒ¼ã‚¿ã¨ã®ç›´æ¥æ¯”è¼ƒ
  - Inception Scoreã‚ˆã‚Šå®‰å®š
  - ãƒ¢ãƒ¼ãƒ‰å´©å£Šã®æ¤œå‡ºãŒå¯èƒ½

å¯è¦–åŒ–å®Œäº†
</code></pre>

<hr>

<h2>1.6 å®Ÿè·µï¼šã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«ï¼ˆGMMï¼‰</h2>

<h3>ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«ã¨ã¯</h3>

<p><strong>ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«ï¼ˆGaussian Mixture Model, GMMï¼‰</strong>ã¯ã€è¤‡æ•°ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®é‡ã¿ä»˜ãå’Œã§ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¾ã™ã€‚</p>

<p>GMMã®ç¢ºç‡å¯†åº¦é–¢æ•°ï¼š</p>

$$
P(x) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x | \mu_k, \Sigma_k)
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$K$: æ··åˆæˆåˆ†ã®æ•°</li>
<li>$\pi_k$: æ··åˆä¿‚æ•°ï¼ˆ$\sum_k \pi_k = 1$ï¼‰</li>
<li>$\mu_k$: å„æˆåˆ†ã®å¹³å‡</li>
<li>$\Sigma_k$: å„æˆåˆ†ã®å…±åˆ†æ•£è¡Œåˆ—</li>
</ul>

<h3>EMã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹å­¦ç¿’</h3>

<p><strong>Expectation-Maximization (EM)ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </strong>ã§æ½œåœ¨å¤‰æ•°ã‚’å«ã‚€ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¾ã™ã€‚</p>

<h4>Eã‚¹ãƒ†ãƒƒãƒ—ï¼ˆæœŸå¾…å€¤è¨ˆç®—ï¼‰</h4>

<p>å„ã‚µãƒ³ãƒ—ãƒ«ãŒã©ã®æˆåˆ†ã«å±ã™ã‚‹ã‹ã®ç¢ºç‡ï¼ˆè²¬ä»»åº¦ï¼‰ã‚’è¨ˆç®—ï¼š</p>

$$
\gamma_{ik} = \frac{\pi_k \mathcal{N}(x_i | \mu_k, \Sigma_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(x_i | \mu_j, \Sigma_j)}
$$

<h4>Mã‚¹ãƒ†ãƒƒãƒ—ï¼ˆæœ€å¤§åŒ–ï¼‰</h4>

<p>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ï¼š</p>

$$
\begin{align}
\pi_k &= \frac{1}{N}\sum_{i=1}^{N} \gamma_{ik} \\
\mu_k &= \frac{\sum_{i=1}^{N} \gamma_{ik} x_i}{\sum_{i=1}^{N} \gamma_{ik}} \\
\Sigma_k &= \frac{\sum_{i=1}^{N} \gamma_{ik} (x_i - \mu_k)(x_i - \mu_k)^T}{\sum_{i=1}^{N} \gamma_{ik}}
\end{align}
$$

<pre><code class="language-python">import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

class GaussianMixtureModel:
    """
    ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«ï¼ˆGMMï¼‰ã®å®Ÿè£…
    """
    def __init__(self, n_components=3, n_features=2, max_iter=100, tol=1e-4):
        """
        Parameters:
        -----------
        n_components : int
            æ··åˆæˆåˆ†ã®æ•°
        n_features : int
            ç‰¹å¾´é‡ã®æ¬¡å…ƒ
        max_iter : int
            æœ€å¤§ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°
        tol : float
            åæŸåˆ¤å®šã®é–¾å€¤
        """
        self.n_components = n_components
        self.n_features = n_features
        self.max_iter = max_iter
        self.tol = tol

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸåŒ–
        self.weights = np.ones(n_components) / n_components  # Ï€_k
        self.means = np.random.randn(n_components, n_features)  # Î¼_k
        self.covariances = np.array([np.eye(n_features) for _ in range(n_components)])  # Î£_k

    def gaussian_pdf(self, X, mean, cov):
        """å¤šå¤‰é‡ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ç¢ºç‡å¯†åº¦é–¢æ•°"""
        n = X.shape[1]
        diff = X - mean
        cov_inv = np.linalg.inv(cov)
        cov_det = np.linalg.det(cov)

        norm_const = 1.0 / (np.power(2 * np.pi, n / 2) * np.sqrt(cov_det))
        exponent = -0.5 * np.sum(diff @ cov_inv * diff, axis=1)

        return norm_const * np.exp(exponent)

    def e_step(self, X):
        """Eã‚¹ãƒ†ãƒƒãƒ—: è²¬ä»»åº¦ã®è¨ˆç®—"""
        n_samples = X.shape[0]
        responsibilities = np.zeros((n_samples, self.n_components))

        for k in range(self.n_components):
            responsibilities[:, k] = self.weights[k] * \
                self.gaussian_pdf(X, self.means[k], self.covariances[k])

        # æ­£è¦åŒ–
        responsibilities /= responsibilities.sum(axis=1, keepdims=True)

        return responsibilities

    def m_step(self, X, responsibilities):
        """Mã‚¹ãƒ†ãƒƒãƒ—: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´æ–°"""
        n_samples = X.shape[0]

        for k in range(self.n_components):
            resp_k = responsibilities[:, k]
            resp_sum = resp_k.sum()

            # æ··åˆä¿‚æ•°ã®æ›´æ–°
            self.weights[k] = resp_sum / n_samples

            # å¹³å‡ã®æ›´æ–°
            self.means[k] = (resp_k[:, np.newaxis] * X).sum(axis=0) / resp_sum

            # å…±åˆ†æ•£ã®æ›´æ–°
            diff = X - self.means[k]
            self.covariances[k] = (resp_k[:, np.newaxis, np.newaxis] *
                                   diff[:, :, np.newaxis] @
                                   diff[:, np.newaxis, :]).sum(axis=0) / resp_sum

    def compute_log_likelihood(self, X):
        """å¯¾æ•°å°¤åº¦ã®è¨ˆç®—"""
        n_samples = X.shape[0]
        log_likelihood = 0

        for i in range(n_samples):
            sample_likelihood = 0
            for k in range(self.n_components):
                sample_likelihood += self.weights[k] * \
                    self.gaussian_pdf(X[i:i+1], self.means[k], self.covariances[k])
            log_likelihood += np.log(sample_likelihood + 1e-10)

        return log_likelihood

    def fit(self, X):
        """EMã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹å­¦ç¿’"""
        log_likelihoods = []

        for iteration in range(self.max_iter):
            # Eã‚¹ãƒ†ãƒƒãƒ—
            responsibilities = self.e_step(X)

            # Mã‚¹ãƒ†ãƒƒãƒ—
            self.m_step(X, responsibilities)

            # å¯¾æ•°å°¤åº¦ã®è¨ˆç®—
            log_likelihood = self.compute_log_likelihood(X)
            log_likelihoods.append(log_likelihood)

            # åæŸåˆ¤å®š
            if iteration > 0:
                if abs(log_likelihoods[-1] - log_likelihoods[-2]) < self.tol:
                    print(f"åæŸã—ã¾ã—ãŸï¼ˆã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ {iteration + 1}ï¼‰")
                    break

        return log_likelihoods

    def sample(self, n_samples):
        """å­¦ç¿’ã—ãŸåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ"""
        samples = []

        # å„ã‚µãƒ³ãƒ—ãƒ«ã«ã¤ã„ã¦
        for _ in range(n_samples):
            # æ··åˆæˆåˆ†ã‚’é¸æŠ
            component = np.random.choice(self.n_components, p=self.weights)

            # é¸æŠã—ãŸæˆåˆ†ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«
            sample = np.random.multivariate_normal(
                self.means[component],
                self.covariances[component]
            )
            samples.append(sample)

        return np.array(samples)

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
X, y_true = make_blobs(n_samples=300, centers=3, n_features=2,
                       cluster_std=0.5, random_state=42)

print("=== ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«ï¼ˆGMMï¼‰ã®å®Ÿè£… ===\n")
print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {X.shape}")
print(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: {X.shape[0]}")
print(f"  ç‰¹å¾´é‡æ¬¡å…ƒ: {X.shape[1]}")

# GMMã®å­¦ç¿’
gmm = GaussianMixtureModel(n_components=3, n_features=2, max_iter=100)
log_likelihoods = gmm.fit(X)

print(f"\nå­¦ç¿’ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
for k in range(gmm.n_components):
    print(f"\næˆåˆ† {k + 1}:")
    print(f"  æ··åˆä¿‚æ•° Ï€: {gmm.weights[k]:.4f}")
    print(f"  å¹³å‡ Î¼: {gmm.means[k]}")
    print(f"  å…±åˆ†æ•£ Î£:\n{gmm.covariances[k]}")

# æ–°ã—ã„ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ
generated_samples = gmm.sample(300)

print(f"\nç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«æ•°: {generated_samples.shape[0]}")

# å¯è¦–åŒ–
fig = plt.figure(figsize=(18, 5))

# å·¦å›³ï¼šå…ƒãƒ‡ãƒ¼ã‚¿
ax1 = fig.add_subplot(131)
ax1.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', alpha=0.6, s=30)
ax1.set_xlabel('ç‰¹å¾´é‡ 1')
ax1.set_ylabel('ç‰¹å¾´é‡ 2')
ax1.set_title('å…ƒã®ãƒ‡ãƒ¼ã‚¿')
ax1.grid(True, alpha=0.3)

# ä¸­å¤®å›³ï¼šå­¦ç¿’ã—ãŸGMM
ax2 = fig.add_subplot(132)
responsibilities = gmm.e_step(X)
predicted_labels = responsibilities.argmax(axis=1)
ax2.scatter(X[:, 0], X[:, 1], c=predicted_labels, cmap='viridis', alpha=0.6, s=30)

# å„æˆåˆ†ã®ç­‰é«˜ç·šã‚’æç”»
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                     np.linspace(y_min, y_max, 100))
grid = np.c_[xx.ravel(), yy.ravel()]

for k in range(gmm.n_components):
    density = gmm.gaussian_pdf(grid, gmm.means[k], gmm.covariances[k])
    density = density.reshape(xx.shape)
    ax2.contour(xx, yy, density, levels=5, alpha=0.3)
    ax2.plot(gmm.means[k, 0], gmm.means[k, 1], 'r*', markersize=15)

ax2.set_xlabel('ç‰¹å¾´é‡ 1')
ax2.set_ylabel('ç‰¹å¾´é‡ 2')
ax2.set_title('å­¦ç¿’ã—ãŸGMM')
ax2.grid(True, alpha=0.3)

# å³å›³ï¼šç”Ÿæˆã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«
ax3 = fig.add_subplot(133)
ax3.scatter(generated_samples[:, 0], generated_samples[:, 1],
            alpha=0.6, s=30, color='coral')
ax3.set_xlabel('ç‰¹å¾´é‡ 1')
ax3.set_ylabel('ç‰¹å¾´é‡ 2')
ax3.set_title('GMMã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«')
ax3.grid(True, alpha=0.3)

plt.tight_layout()

# å¯¾æ•°å°¤åº¦ã®æ¨ç§»
fig2, ax = plt.subplots(figsize=(8, 5))
ax.plot(log_likelihoods, marker='o')
ax.set_xlabel('ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³')
ax.set_ylabel('å¯¾æ•°å°¤åº¦')
ax.set_title('EMã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®åæŸ')
ax.grid(True, alpha=0.3)

print("\nå¯è¦–åŒ–å®Œäº†")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«ï¼ˆGMMï¼‰ã®å®Ÿè£… ===

ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: (300, 2)
  ã‚µãƒ³ãƒ—ãƒ«æ•°: 300
  ç‰¹å¾´é‡æ¬¡å…ƒ: 2

åæŸã—ã¾ã—ãŸï¼ˆã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ 23ï¼‰

å­¦ç¿’ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:

æˆåˆ† 1:
  æ··åˆä¿‚æ•° Ï€: 0.3333
  å¹³å‡ Î¼: [2.1234 3.4567]
  å…±åˆ†æ•£ Î£:
[[0.2345 0.0123]
 [0.0123 0.2456]]

æˆåˆ† 2:
  æ··åˆä¿‚æ•° Ï€: 0.3300
  å¹³å‡ Î¼: [-1.2345 -2.3456]
  å…±åˆ†æ•£ Î£:
[[0.2567 -0.0234]
 [-0.0234 0.2678]]

æˆåˆ† 3:
  æ··åˆä¿‚æ•° Ï€: 0.3367
  å¹³å‡ Î¼: [5.6789 1.2345]
  å…±åˆ†æ•£ Î£:
[[0.2789 0.0345]
 [0.0345 0.2890]]

ç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«æ•°: 300

å¯è¦–åŒ–å®Œäº†
</code></pre>

<hr>

<h2>ã¾ã¨ã‚</h2>

<p>ã“ã®ç« ã§ã¯ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®åŸºç¤ã‚’å­¦ç¿’ã—ã¾ã—ãŸã€‚</p>

<h3>é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ</h3>

<ul>
<li><strong>åˆ¤åˆ¥ vs ç”Ÿæˆ</strong>ï¼šåˆ¤åˆ¥ã¯ $P(y|x)$ã€ç”Ÿæˆã¯ $P(x)$ ã‚’å­¦ç¿’</li>
<li><strong>æœ€å°¤æ¨å®š</strong>ï¼š$\hat{\theta} = \arg\max \sum \log P(x_i; \theta)$ ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š</li>
<li><strong>ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</strong>ï¼šRejectionã€Importanceã€MCMCã§è¤‡é›‘ãªåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«</li>
<li><strong>æ½œåœ¨å¤‰æ•°</strong>ï¼šä½æ¬¡å…ƒã®æ½œåœ¨ç©ºé–“ã§ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¾ã€åˆ¶å¾¡å¯èƒ½ãªç”ŸæˆãŒå¯èƒ½</li>
<li><strong>è©•ä¾¡æŒ‡æ¨™</strong>ï¼šInception Scoreï¼ˆæ˜ç¢ºã•ã¨å¤šæ§˜æ€§ï¼‰ã€FIDï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ã¨ã®è·é›¢ï¼‰</li>
<li><strong>GMM</strong>ï¼šEMã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æ··åˆã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‚’å­¦ç¿’ã€ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã«å¿œç”¨</li>
</ul>

<h3>æ¬¡ç« ã®äºˆå‘Š</h3>

<p>ç¬¬2ç« ã§ã¯ã€ä»¥ä¸‹ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’æ‰±ã„ã¾ã™ï¼š</p>
<ul>
<li>å¤‰åˆ†ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆVAEï¼‰ã®ç†è«–ã¨å®Ÿè£…</li>
<li>ELBOï¼ˆEvidence Lower BOundï¼‰ã¨å¤‰åˆ†æ¨è«–</li>
<li>å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯</li>
<li>æ¡ä»¶ä»˜ãVAEï¼ˆCVAEï¼‰</li>
<li>VAEã«ã‚ˆã‚‹ç”»åƒç”Ÿæˆã¨æ½œåœ¨ç©ºé–“ã®æ“ä½œ</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<details>
<summary><strong>æ¼”ç¿’1ï¼šãƒ™ã‚¤ã‚ºã®å®šç†ã®é©ç”¨</strong></summary>

<p><strong>å•é¡Œ</strong>ï¼šã‚¹ãƒ‘ãƒ ãƒ¡ãƒ¼ãƒ«åˆ†é¡ã«ãŠã„ã¦ã€ä»¥ä¸‹ã®æƒ…å ±ãŒä¸ãˆã‚‰ã‚Œã¦ã„ã¾ã™ï¼š</p>

<ul>
<li>$P(\text{spam}) = 0.3$ï¼ˆäº‹å‰ç¢ºç‡ï¼‰</li>
<li>$P(\text{word}|\text{spam}) = 0.8$ï¼ˆå°¤åº¦ï¼‰</li>
<li>$P(\text{word}|\text{not spam}) = 0.1$</li>
</ul>

<p>ç‰¹å®šã®å˜èªã‚’å«ã‚€ãƒ¡ãƒ¼ãƒ«ãŒã‚¹ãƒ‘ãƒ ã§ã‚ã‚‹äº‹å¾Œç¢ºç‡ $P(\text{spam}|\text{word})$ ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚</p>

<p><strong>è§£ç­”</strong>ï¼š</p>
<pre><code># ãƒ™ã‚¤ã‚ºã®å®šç†: P(spam|word) = P(word|spam) * P(spam) / P(word)

# ä¸ãˆã‚‰ã‚ŒãŸå€¤
P_spam = 0.3
P_not_spam = 1 - P_spam  # 0.7
P_word_given_spam = 0.8
P_word_given_not_spam = 0.1

# å‘¨è¾ºç¢ºç‡ P(word) ã®è¨ˆç®—
P_word = P_word_given_spam * P_spam + P_word_given_not_spam * P_not_spam
       = 0.8 * 0.3 + 0.1 * 0.7
       = 0.24 + 0.07
       = 0.31

# äº‹å¾Œç¢ºç‡ã®è¨ˆç®—
P_spam_given_word = P_word_given_spam * P_spam / P_word
                  = 0.8 * 0.3 / 0.31
                  = 0.24 / 0.31
                  â‰ˆ 0.7742

ç­”ãˆ: P(spam|word) â‰ˆ 77.42%

è§£é‡ˆ: ã“ã®å˜èªã‚’å«ã‚€ãƒ¡ãƒ¼ãƒ«ã¯ç´„77%ã®ç¢ºç‡ã§ã‚¹ãƒ‘ãƒ ã§ã™ã€‚
</code></pre>
</details>

<details>
<summary><strong>æ¼”ç¿’2ï¼šæœ€å°¤æ¨å®šã®å°å‡º</strong></summary>

<p><strong>å•é¡Œ</strong>ï¼šãƒ™ãƒ«ãƒŒãƒ¼ã‚¤åˆ†å¸ƒ $P(x; p) = p^x (1-p)^{1-x}$ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $p$ ã‚’æœ€å°¤æ¨å®šã—ã¦ãã ã•ã„ã€‚</p>

<p>ãƒ‡ãƒ¼ã‚¿: $\mathcal{D} = \{x_1, x_2, \ldots, x_N\}$</p>

<p><strong>è§£ç­”</strong>ï¼š</p>
<pre><code># å°¤åº¦é–¢æ•°
L(p) = âˆ_{i=1}^N p^{x_i} (1-p)^{1-x_i}

# å¯¾æ•°å°¤åº¦
log L(p) = âˆ‘_{i=1}^N [x_i log(p) + (1-x_i) log(1-p)]
         = log(p) âˆ‘ x_i + log(1-p) âˆ‘ (1-x_i)
         = log(p) âˆ‘ x_i + log(1-p) (N - âˆ‘ x_i)

# å¾®åˆ†ã—ã¦ã‚¼ãƒ­ã¨ãŠã
d/dp log L(p) = (âˆ‘ x_i) / p - (N - âˆ‘ x_i) / (1-p) = 0

# æ•´ç†
(âˆ‘ x_i) / p = (N - âˆ‘ x_i) / (1-p)
(âˆ‘ x_i)(1-p) = p(N - âˆ‘ x_i)
âˆ‘ x_i - p âˆ‘ x_i = pN - p âˆ‘ x_i
âˆ‘ x_i = pN

# æœ€å°¤æ¨å®šå€¤
pÌ‚_MLE = (âˆ‘ x_i) / N = æ¨™æœ¬å¹³å‡

ç­”ãˆ: pÌ‚ = ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡å€¤ï¼ˆæˆåŠŸã®ç›¸å¯¾é »åº¦ï¼‰

å…·ä½“ä¾‹: ãƒ‡ãƒ¼ã‚¿ãŒ {1, 0, 1, 1, 0} ãªã‚‰
pÌ‚ = (1+0+1+1+0) / 5 = 3/5 = 0.6
</code></pre>
</details>

<details>
<summary><strong>æ¼”ç¿’3ï¼šRejection Samplingã®åŠ¹ç‡</strong></summary>

<p><strong>å•é¡Œ</strong>ï¼šRejection Samplingã«ãŠã„ã¦ã€å®šæ•° $M$ ãŒæ¡ç”¨ç‡ã«ã©ã†å½±éŸ¿ã™ã‚‹ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€æœ€é©ãª $M$ ã¯ã©ã†é¸ã¶ã¹ãã§ã™ã‹ï¼Ÿ</p>

<p><strong>è§£ç­”</strong>ï¼š</p>
<pre><code># æ¡ç”¨ç‡ã®ç†è«–å€¤
æ¡ç”¨ç‡ = 1 / M

# Mã®é¸æŠ
æ¡ä»¶: ã™ã¹ã¦ã® x ã«ã¤ã„ã¦ p(x) â‰¤ M * q(x)
æœ€é©ãªM: M_opt = max_x [p(x) / q(x)]

# Mã®å½±éŸ¿

1. MãŒå°ã•ã™ãã‚‹å ´åˆ:
   - æ¡ä»¶ã‚’æº€ãŸã›ãªã„ â†’ ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒæ­£ã—ãå‹•ä½œã—ãªã„
   - ä¸€éƒ¨ã® x ã§ p(x) > M * q(x) ã¨ãªã‚Šã€æ­£ã—ã„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒã§ããªã„

2. MãŒæœ€é©å€¤ã®å ´åˆ:
   - M = max[p(x) / q(x)]
   - æ¡ç”¨ç‡ãŒæœ€å¤§åŒ–ã•ã‚Œã‚‹
   - ç„¡é§„ãªæ£„å´ãŒæœ€å°

3. MãŒå¤§ãã™ãã‚‹å ´åˆ:
   - æ¡ä»¶ã¯æº€ãŸã™ãŒæ¡ç”¨ç‡ãŒä½ä¸‹
   - å¤šãã®ã‚µãƒ³ãƒ—ãƒ«ãŒæ£„å´ã•ã‚Œã‚‹
   - è¨ˆç®—åŠ¹ç‡ãŒæ‚ªåŒ–

å…·ä½“ä¾‹:
p(x) = Beta(2, 5)  â† ç›®çš„åˆ†å¸ƒ
q(x) = U(0, 1)     â† ææ¡ˆåˆ†å¸ƒ

æœ€å¤§å€¤: p(x)ã®æœ€å¤§å€¤ã¯ç´„2.46ï¼ˆx â‰ˆ 0.2ä»˜è¿‘ï¼‰
M_opt = 2.46 / 1.0 = 2.46

æ¡ç”¨ç‡ = 1 / 2.46 â‰ˆ 0.407 (40.7%)

ã‚‚ã—M = 10ã«ã™ã‚‹ã¨:
æ¡ç”¨ç‡ = 1 / 10 = 0.1 (10%)
â†’ å¤§å¹…ã«åŠ¹ç‡ãŒä½ä¸‹

ç­”ãˆ:
- Mã¯ max[p(x)/q(x)] ã«è¨­å®šã™ã¹ã
- å¤§ãã™ãã‚‹ã¨åŠ¹ç‡ä½ä¸‹ã€å°ã•ã™ãã‚‹ã¨èª¤å‹•ä½œ
- ææ¡ˆåˆ†å¸ƒq(x)ãŒç›®çš„åˆ†å¸ƒp(x)ã«è¿‘ã„ã»ã©åŠ¹ç‡ãŒè‰¯ã„
</code></pre>
</details>

<details>
<summary><strong>æ¼”ç¿’4ï¼šInception Scoreã®è¨ˆç®—</strong></summary>

<p><strong>å•é¡Œ</strong>ï¼šä»¥ä¸‹ã®åˆ†é¡ç¢ºç‡ã‚’æŒã¤3ã¤ã®ç”Ÿæˆç”»åƒã®Inception Scoreã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ï¼ˆç°¡ç•¥åŒ–ã®ãŸã‚åˆ†å‰²ãªã—ï¼‰ã€‚</p>

<pre><code>ç”»åƒ1: p(y|xâ‚) = [0.9, 0.05, 0.05]  ï¼ˆæ˜ç¢ºã«ã‚¯ãƒ©ã‚¹0ï¼‰
ç”»åƒ2: p(y|xâ‚‚) = [0.05, 0.9, 0.05]  ï¼ˆæ˜ç¢ºã«ã‚¯ãƒ©ã‚¹1ï¼‰
ç”»åƒ3: p(y|xâ‚ƒ) = [0.05, 0.05, 0.9]  ï¼ˆæ˜ç¢ºã«ã‚¯ãƒ©ã‚¹2ï¼‰
</code></pre>

<p><strong>è§£ç­”</strong>ï¼š</p>
<pre><code># ãƒ‡ãƒ¼ã‚¿
p1 = [0.9, 0.05, 0.05]
p2 = [0.05, 0.9, 0.05]
p3 = [0.05, 0.05, 0.9]

# p(y): å¹³å‡åˆ†é¡ç¢ºç‡
p_y = (p1 + p2 + p3) / 3
    = [0.3, 0.3, 0.3]  # å‡ä¸€ï¼ˆé«˜å¤šæ§˜æ€§ï¼‰

# KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹: D_KL(p(y|x) || p(y))
# D_KL(P||Q) = Î£ P(i) log(P(i)/Q(i))

KL1 = 0.9 * log(0.9/0.3) + 0.05 * log(0.05/0.3) + 0.05 * log(0.05/0.3)
    = 0.9 * log(3) + 0.05 * log(1/6) + 0.05 * log(1/6)
    = 0.9 * 1.099 + 0.05 * (-1.792) + 0.05 * (-1.792)
    â‰ˆ 0.989 - 0.090 - 0.090
    â‰ˆ 0.809

KL2 = 0.809  ï¼ˆå¯¾ç§°æ€§ã‚ˆã‚ŠåŒã˜ï¼‰
KL3 = 0.809

# å¹³å‡KL
KL_avg = (0.809 + 0.809 + 0.809) / 3 = 0.809

# Inception Score
IS = exp(KL_avg) = exp(0.809) â‰ˆ 2.246

ç­”ãˆ: IS â‰ˆ 2.25

è§£é‡ˆ:
- å„ç”»åƒã¯æ˜ç¢ºãªã‚¯ãƒ©ã‚¹ã«åˆ†é¡ã•ã‚Œã‚‹ï¼ˆä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰
- å…¨ä½“ã¨ã—ã¦3ã¤ã®ã‚¯ãƒ©ã‚¹ã«å‡ç­‰ã«åˆ†æ•£ï¼ˆé«˜å¤šæ§˜æ€§ï¼‰
- é«˜ã„ISï¼ˆç†æƒ³çš„ã«ã¯3ã«è¿‘ã¥ãï¼‰
</code></pre>
</details>

<details>
<summary><strong>æ¼”ç¿’5ï¼šGMMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°</strong></summary>

<p><strong>å•é¡Œ</strong>ï¼š$D$ æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹ $K$ æˆåˆ†ã®ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«ã®ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’æ±‚ã‚ã¦ãã ã•ã„ï¼ˆå…±åˆ†æ•£è¡Œåˆ—ã¯å¯¾è§’è¡Œåˆ—ã¨ä»®å®šï¼‰ã€‚</p>

<p><strong>è§£ç­”</strong>ï¼š</p>
<pre><code># GMMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

1. æ··åˆä¿‚æ•° Ï€_k:
   - Kå€‹ã®æˆåˆ†ã«å¯¾ã—ã¦Kå€‹ã®ä¿‚æ•°
   - ãŸã ã— Î£Ï€_k = 1 ã®åˆ¶ç´„ãŒã‚ã‚‹ãŸã‚ç‹¬ç«‹ãªã®ã¯ K-1 å€‹
   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: K - 1

2. å¹³å‡ Î¼_k:
   - å„æˆåˆ†ãŒDæ¬¡å…ƒã®å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«
   - Kå€‹ã®æˆåˆ†
   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: K Ã— D

3. å…±åˆ†æ•£ Î£_kï¼ˆå¯¾è§’è¡Œåˆ—ã®å ´åˆï¼‰:
   - å¯¾è§’æˆåˆ†ã®ã¿ï¼ˆDå€‹ã®åˆ†æ•£ï¼‰
   - Kå€‹ã®æˆåˆ†
   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: K Ã— D

# ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
Total = (K - 1) + KÃ—D + KÃ—D
      = K - 1 + 2KD
      = K(2D + 1) - 1

å…·ä½“ä¾‹:
D = 2ï¼ˆ2æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ï¼‰
K = 3ï¼ˆ3æˆåˆ†ï¼‰

Total = 3(2Ã—2 + 1) - 1
      = 3 Ã— 5 - 1
      = 14ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

å†…è¨³:
- Ï€: 2å€‹ï¼ˆÏ€â‚, Ï€â‚‚ã®ã¿ç‹¬ç«‹ã€Ï€â‚ƒ = 1 - Ï€â‚ - Ï€â‚‚ï¼‰
- Î¼: 6å€‹ï¼ˆÎ¼â‚=[x,y], Î¼â‚‚=[x,y], Î¼â‚ƒ=[x,y]ï¼‰
- Î£: 6å€‹ï¼ˆå„æˆåˆ†ã§2ã¤ã®åˆ†æ•£ï¼‰

æ³¨ï¼šå®Œå…¨ãªå…±åˆ†æ•£è¡Œåˆ—ã®å ´åˆ:
å„Î£_kã¯ D(D+1)/2 å€‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
Total = (K-1) + KD + KÃ—D(D+1)/2

ç­”ãˆ: å¯¾è§’å…±åˆ†æ•£ã®å ´åˆ K(2D+1) - 1 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
</code></pre>
</details>

<hr>

<div class="navigation">
    <a href="../index.html" class="nav-button">ğŸ“š ã‚³ãƒ¼ã‚¹ç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter2-vae.html" class="nav-button">æ¬¡ã®ç« ã¸ï¼šå¤‰åˆ†ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆVAEï¼‰ â†’</a>
</div>

</main>


    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
    <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
</footer>

</body>
</html>
