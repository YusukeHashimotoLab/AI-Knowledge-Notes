<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬4ç« ï¼šæ‹¡æ•£ãƒ¢ãƒ‡ãƒ« - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: var(--font-body); line-height: 1.7; color: var(--color-text); background-color: var(--color-bg); font-size: 16px; }
        header { background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; padding: var(--spacing-xl) var(--spacing-md); margin-bottom: var(--spacing-xl); box-shadow: var(--box-shadow); }
        .header-content { max-width: 900px; margin: 0 auto; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: var(--spacing-sm); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; opacity: 0.95; font-weight: 400; margin-bottom: var(--spacing-md); }
        .meta { display: flex; flex-wrap: wrap; gap: var(--spacing-md); font-size: 0.9rem; opacity: 0.9; }
        .meta-item { display: flex; align-items: center; gap: 0.3rem; }
        .container { max-width: 900px; margin: 0 auto; padding: 0 var(--spacing-md) var(--spacing-xl); }
        h2 { font-size: 1.75rem; color: var(--color-primary); margin-top: var(--spacing-xl); margin-bottom: var(--spacing-md); padding-bottom: var(--spacing-xs); border-bottom: 3px solid var(--color-accent); }
        h3 { font-size: 1.4rem; color: var(--color-primary); margin-top: var(--spacing-lg); margin-bottom: var(--spacing-sm); }
        h4 { font-size: 1.1rem; color: var(--color-primary-dark); margin-top: var(--spacing-md); margin-bottom: var(--spacing-sm); }
        p { margin-bottom: var(--spacing-md); color: var(--color-text); }
        a { color: var(--color-link); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--color-link-hover); text-decoration: underline; }
        ul, ol { margin-left: var(--spacing-lg); margin-bottom: var(--spacing-md); }
        li { margin-bottom: var(--spacing-xs); color: var(--color-text); }
        pre { background-color: var(--color-code-bg); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); overflow-x: auto; margin-bottom: var(--spacing-md); font-family: var(--font-mono); font-size: 0.9rem; line-height: 1.5; }
        code { font-family: var(--font-mono); font-size: 0.9em; background-color: var(--color-code-bg); padding: 0.2em 0.4em; border-radius: 3px; }
        pre code { background-color: transparent; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: var(--spacing-md); font-size: 0.95rem; }
        th, td { border: 1px solid var(--color-border); padding: var(--spacing-sm); text-align: left; }
        th { background-color: var(--color-bg-alt); font-weight: 600; color: var(--color-primary); }
        blockquote { border-left: 4px solid var(--color-accent); padding-left: var(--spacing-md); margin: var(--spacing-md) 0; color: var(--color-text-light); font-style: italic; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        .mermaid { text-align: center; margin: var(--spacing-lg) 0; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        details { background-color: var(--color-bg-alt); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); margin-bottom: var(--spacing-md); }
        summary { cursor: pointer; font-weight: 600; color: var(--color-primary); user-select: none; padding: var(--spacing-xs); margin: calc(-1 * var(--spacing-md)); padding: var(--spacing-md); border-radius: var(--border-radius); }
        summary:hover { background-color: rgba(123, 44, 191, 0.1); }
        details[open] summary { margin-bottom: var(--spacing-md); border-bottom: 1px solid var(--color-border); }
        .navigation { display: flex; justify-content: space-between; gap: var(--spacing-md); margin: var(--spacing-xl) 0; padding-top: var(--spacing-lg); border-top: 2px solid var(--color-border); }
        .nav-button { flex: 1; padding: var(--spacing-md); background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; border-radius: var(--border-radius); text-align: center; font-weight: 600; transition: transform 0.2s, box-shadow 0.2s; box-shadow: var(--box-shadow); }
        .nav-button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); text-decoration: none; }
        footer { margin-top: var(--spacing-xl); padding: var(--spacing-lg) var(--spacing-md); background-color: var(--color-bg-alt); border-top: 1px solid var(--color-border); text-align: center; font-size: 0.9rem; color: var(--color-text-light); }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }

        .project-box { background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 50%); border-radius: var(--border-radius); padding: var(--spacing-lg); margin: var(--spacing-lg) 0; box-shadow: var(--box-shadow); }
        @media (max-width: 768px) { h1 { font-size: 1.5rem; } h2 { font-size: 1.4rem; } h3 { font-size: 1.2rem; } .meta { font-size: 0.85rem; } .navigation { flex-direction: column; } table { font-size: 0.85rem; } th, td { padding: var(--spacing-xs); } }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/AI-Knowledge-Notes/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/AI-Knowledge-Notes/knowledge/jp/ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="/AI-Knowledge-Notes/knowledge/jp/ML/generative-models-introduction/index.html">Generative Models</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 4</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬4ç« ï¼šæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«</h1>
            <p class="subtitle">ãƒã‚¤ã‚ºã‹ã‚‰ã®ç”Ÿæˆï¼šDiffusion Modelsã®ç†è«–ã¨å®Ÿè·µã€Stable Diffusionã¸ã®å±•é–‹</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 32åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 6å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬åŸç†ï¼ˆForward/Reverse Processï¼‰ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… DDPMï¼ˆDenoising Diffusion Probabilistic Modelsï¼‰ã®æ•°å­¦çš„å®šå¼åŒ–ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… U-Net Denoiserã®æ§‹é€ ã¨è¨“ç·´æ–¹æ³•ã‚’ç¿’å¾—ã§ãã‚‹</li>
<li>âœ… Latent Diffusion Modelsï¼ˆStable Diffusionï¼‰ã®ä»•çµ„ã¿ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… CLIP Guidanceã¨ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ãç”Ÿæˆã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… PyTorchã§å®Ÿè·µçš„ãªç”»åƒç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
</ul>

<hr>

<h2>4.1 æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®åŸºç¤</h2>

<h3>4.1.1 æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¨ã¯ä½•ã‹</h3>

<p><strong>æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ï¼ˆDiffusion Modelsï¼‰</strong>ã¯ã€ãƒ‡ãƒ¼ã‚¿ã«å¾ã€…ã«ãƒã‚¤ã‚ºã‚’åŠ ãˆã‚‹éç¨‹ï¼ˆForward Processï¼‰ã¨ã€ãƒã‚¤ã‚ºã‹ã‚‰å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒã™ã‚‹éç¨‹ï¼ˆReverse Processï¼‰ã‚’å­¦ç¿’ã™ã‚‹ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚2020å¹´ä»£ã«å…¥ã‚Šã€ç”»åƒç”Ÿæˆã®SOTAã‚’é”æˆã—ã€Stable Diffusionã€DALL-E 2ã€Imagenç­‰ã®åŸºç›¤æŠ€è¡“ã¨ãªã£ã¦ã„ã¾ã™ã€‚</p>

<table>
<thead>
<tr>
<th>ç‰¹æ€§</th>
<th>GAN</th>
<th>VAE</th>
<th>æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ç”Ÿæˆæ–¹å¼</strong></td>
<td>æ•µå¯¾çš„å­¦ç¿’</td>
<td>å¤‰åˆ†æ¨è«–</td>
<td>ãƒã‚¤ã‚ºé™¤å»</td>
</tr>
<tr>
<td><strong>è¨“ç·´å®‰å®šæ€§</strong></td>
<td>ä½ï¼ˆãƒ¢ãƒ¼ãƒ‰å´©å£Šï¼‰</td>
<td>ä¸­</td>
<td>é«˜</td>
</tr>
<tr>
<td><strong>ç”Ÿæˆå“è³ª</strong></td>
<td>é«˜ï¼ˆè¨“ç·´æ™‚ï¼‰</td>
<td>ä¸­ï¼ˆã¼ã‚„ã‘ï¼‰</td>
<td>éå¸¸ã«é«˜</td>
</tr>
<tr>
<td><strong>å¤šæ§˜æ€§</strong></td>
<td>ä½ï¼ˆãƒ¢ãƒ¼ãƒ‰å´©å£Šï¼‰</td>
<td>é«˜</td>
<td>é«˜</td>
</tr>
<tr>
<td><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆ</strong></td>
<td>ä½ã€œä¸­</td>
<td>ä½ã€œä¸­</td>
<td>é«˜ï¼ˆåå¾©å‡¦ç†ï¼‰</td>
</tr>
<tr>
<td><strong>ä»£è¡¨ãƒ¢ãƒ‡ãƒ«</strong></td>
<td>StyleGAN</td>
<td>Î²-VAE</td>
<td>DDPM, Stable Diffusion</td>
</tr>
</tbody>
</table>

<h3>4.1.2 Forward Processï¼šãƒã‚¤ã‚ºã®ä»˜åŠ </h3>

<p>Forward Processã¯ã€å…ƒã®ç”»åƒ $x_0$ ã«å¯¾ã—ã¦ $T$ ã‚¹ãƒ†ãƒƒãƒ—ã§å¾ã€…ã«ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚ºã‚’è¿½åŠ ã—ã¦ã„ãéç¨‹ã§ã™ã€‚</p>

$$
q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t I)
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$x_t$: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— $t$ ã§ã®ç”»åƒ</li>
<li>$\beta_t$: ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ0.0001 ã€œ 0.02 ç¨‹åº¦ï¼‰</li>
<li>$\mathcal{N}$: ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ</li>
</ul>

<p><strong>é‡è¦ãªæ€§è³ª</strong>ï¼šå†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯ã«ã‚ˆã‚Šã€ä»»æ„ã®ã‚¹ãƒ†ãƒƒãƒ— $t$ ã®ç”»åƒã‚’ç›´æ¥ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¯èƒ½ï¼š</p>

$$
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$\alpha_t = 1 - \beta_t$</li>
<li>$\bar{\alpha}_t = \prod_{s=1}^{t} \alpha_s$</li>
<li>$\epsilon \sim \mathcal{N}(0, I)$</li>
</ul>

<div class="mermaid">
graph LR
    X0["xâ‚€<br/>(å…ƒç”»åƒ)"] -->|"+ ãƒã‚¤ã‚º Î²â‚"| X1["xâ‚"]
    X1 -->|"+ ãƒã‚¤ã‚º Î²â‚‚"| X2["xâ‚‚"]
    X2 -->|"..."| X3["..."]
    X3 -->|"+ ãƒã‚¤ã‚º Î²T"| XT["xT<br/>(ç´”ç²‹ãƒã‚¤ã‚º)"]

    style X0 fill:#27ae60,color:#fff
    style XT fill:#e74c3c,color:#fff
    style X1 fill:#f39c12,color:#fff
    style X2 fill:#e67e22,color:#fff
</div>

<h3>4.1.3 Reverse Processï¼šãƒã‚¤ã‚ºé™¤å»ã«ã‚ˆã‚‹ç”Ÿæˆ</h3>

<p>Reverse Processã¯ã€ç´”ç²‹ãƒã‚¤ã‚º $x_T \sim \mathcal{N}(0, I)$ ã‹ã‚‰å§‹ã‚ã¦ã€å¾ã€…ã«ãƒã‚¤ã‚ºã‚’é™¤å»ã—ã¦å…ƒã®ç”»åƒã‚’å¾©å…ƒã™ã‚‹éç¨‹ã§ã™ã€‚</p>

$$
p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
$$

<p>ã“ã“ã§ $\mu_\theta$ï¼ˆå¹³å‡ï¼‰ã¨ $\Sigma_\theta$ï¼ˆå…±åˆ†æ•£ï¼‰ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§å­¦ç¿’ã—ã¾ã™ã€‚DDPMã§ã¯ã€å…±åˆ†æ•£ã¯å›ºå®šã—ã€å¹³å‡ã®ã¿ã‚’å­¦ç¿’ã™ã‚‹ç°¡ç•¥åŒ–ãŒä¸€èˆ¬çš„ã§ã™ã€‚</p>

<blockquote>
<p><strong>é‡è¦</strong>: Reverse Processã¯ã€ãƒã‚¤ã‚º $\epsilon$ ã‚’äºˆæ¸¬ã™ã‚‹ã‚¿ã‚¹ã‚¯ã¨ã—ã¦å®šå¼åŒ–ã•ã‚Œã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€Œãƒã‚¤ã‚ºé™¤å»å™¨ï¼ˆDenoiserï¼‰ã€ã¨ã—ã¦æ©Ÿèƒ½ã—ã¾ã™ã€‚</p>
</blockquote>

<h3>4.1.4 æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ç›´æ„Ÿçš„ç†è§£</h3>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

# ã‚·ãƒ³ãƒ—ãƒ«ãª1æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã§ã®æ‹¡æ•£ãƒ—ãƒ­ã‚»ã‚¹å¯è¦–åŒ–
np.random.seed(42)

# å…ƒãƒ‡ãƒ¼ã‚¿ï¼š2ã¤ã®ã‚¬ã‚¦ã‚¹æ··åˆ
def sample_data(n=1000):
    """2ã¤ã®ãƒ¢ãƒ¼ãƒ‰ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
    mode1 = np.random.randn(n//2) * 0.5 + 2
    mode2 = np.random.randn(n//2) * 0.5 - 2
    return np.concatenate([mode1, mode2])

# Forward diffusion process
def forward_diffusion(x0, num_steps=50):
    """Forward diffusion: ãƒ‡ãƒ¼ã‚¿ã«ãƒã‚¤ã‚ºã‚’è¿½åŠ """
    # Linear noise schedule
    betas = np.linspace(0.0001, 0.02, num_steps)
    alphas = 1 - betas
    alphas_cumprod = np.cumprod(alphas)

    # å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§ã®ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    x_history = [x0]

    for t in range(1, num_steps):
        noise = np.random.randn(*x0.shape)
        x_t = np.sqrt(alphas_cumprod[t]) * x0 + np.sqrt(1 - alphas_cumprod[t]) * noise
        x_history.append(x_t)

    return x_history, betas, alphas_cumprod

# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== Forward Diffusion Process Visualization ===\n")

x0 = sample_data(1000)
x_history, betas, alphas_cumprod = forward_diffusion(x0, num_steps=50)

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 5, figsize=(18, 7))
axes = axes.flatten()

timesteps_to_show = [0, 5, 10, 15, 20, 25, 30, 35, 40, 49]

for idx, t in enumerate(timesteps_to_show):
    ax = axes[idx]
    ax.hist(x_history[t], bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='black')
    ax.set_xlim(-8, 8)
    ax.set_ylim(0, 0.5)
    ax.set_title(f't = {t}\nÎ±Ì… = {alphas_cumprod[t]:.4f}' if t > 0 else f't = 0 (Original)',
                 fontsize=11, fontweight='bold')
    ax.set_xlabel('x', fontsize=9)
    ax.set_ylabel('Density', fontsize=9)
    ax.grid(alpha=0.3)

plt.suptitle('Forward Diffusion Process: å…ƒãƒ‡ãƒ¼ã‚¿ â†’ ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚º',
             fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()

print("\nç‰¹å¾´:")
print("âœ“ t = 0: å…ƒã®2ãƒ¢ãƒ¼ãƒ‰åˆ†å¸ƒï¼ˆæ˜ç¢ºãªæ§‹é€ ï¼‰")
print("âœ“ t = 10-20: æ§‹é€ ãŒå¾ã€…ã«å´©ã‚Œã‚‹")
print("âœ“ t = 49: ã»ã¼æ¨™æº–ã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼ˆæ§‹é€ ãŒå®Œå…¨ã«å¤±ã‚ã‚Œã‚‹ï¼‰")
print("\nReverse Process:")
print("âœ“ ãƒã‚¤ã‚ºï¼ˆt=49ï¼‰ã‹ã‚‰å§‹ã‚ã¦ã€å¾ã€…ã«æ§‹é€ ã‚’å¾©å…ƒ")
print("âœ“ å­¦ç¿’ã—ãŸDenoiserã§å„ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒã‚¤ã‚ºã‚’é™¤å»")
print("âœ“ æœ€çµ‚çš„ã«å…ƒã®2ãƒ¢ãƒ¼ãƒ‰åˆ†å¸ƒã‚’å†ç¾")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Forward Diffusion Process Visualization ===

ç‰¹å¾´:
âœ“ t = 0: å…ƒã®2ãƒ¢ãƒ¼ãƒ‰åˆ†å¸ƒï¼ˆæ˜ç¢ºãªæ§‹é€ ï¼‰
âœ“ t = 10-20: æ§‹é€ ãŒå¾ã€…ã«å´©ã‚Œã‚‹
âœ“ t = 49: ã»ã¼æ¨™æº–ã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼ˆæ§‹é€ ãŒå®Œå…¨ã«å¤±ã‚ã‚Œã‚‹ï¼‰

Reverse Process:
âœ“ ãƒã‚¤ã‚ºï¼ˆt=49ï¼‰ã‹ã‚‰å§‹ã‚ã¦ã€å¾ã€…ã«æ§‹é€ ã‚’å¾©å…ƒ
âœ“ å­¦ç¿’ã—ãŸDenoiserã§å„ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒã‚¤ã‚ºã‚’é™¤å»
âœ“ æœ€çµ‚çš„ã«å…ƒã®2ãƒ¢ãƒ¼ãƒ‰åˆ†å¸ƒã‚’å†ç¾
</code></pre>

<hr>

<h2>4.2 DDPMï¼ˆDenoising Diffusion Probabilistic Modelsï¼‰</h2>

<h3>4.2.1 DDPMã®æ•°å­¦çš„å®šå¼åŒ–</h3>

<p>DDPMã¯ã€2020å¹´ã«Hoã‚‰ï¼ˆUC Berkeleyï¼‰ãŒææ¡ˆã—ãŸæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ä»£è¡¨çš„æ‰‹æ³•ã§ã™ã€‚</p>

<h4>è¨“ç·´ç›®æ¨™</h4>

<p>DDPMã®æå¤±é–¢æ•°ã¯ã€å¤‰åˆ†ä¸‹ç•Œï¼ˆELBOï¼‰ã‹ã‚‰å°å‡ºã•ã‚Œã¾ã™ãŒã€å®Ÿéš›ã«ã¯å˜ç´”ãªå½¢ã«ãªã‚Šã¾ã™ï¼š</p>

$$
\mathcal{L}_{\text{simple}} = \mathbb{E}_{t, x_0, \epsilon} \left[ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 \right]
$$

<p>ã“ã‚Œã¯ã€ã€Œãƒã‚¤ã‚º $\epsilon$ ã‚’äºˆæ¸¬ã™ã‚‹ã€ã‚¿ã‚¹ã‚¯ã®å¹³å‡äºŒä¹—èª¤å·®ã§ã™ã€‚</p>

<h4>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è©³ç´°</h4>

<p><strong>è¨“ç·´ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </strong>ï¼š</p>
<ol>
<li>è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ $x_0$ ã‚’ã‚µãƒ³ãƒ—ãƒ«</li>
<li>ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— $t \sim \text{Uniform}(1, T)$ ã‚’ã‚µãƒ³ãƒ—ãƒ«</li>
<li>ãƒã‚¤ã‚º $\epsilon \sim \mathcal{N}(0, I)$ ã‚’ã‚µãƒ³ãƒ—ãƒ«</li>
<li>$x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon$ ã‚’è¨ˆç®—</li>
<li>æå¤± $\| \epsilon - \epsilon_\theta(x_t, t) \|^2$ ã‚’æœ€å°åŒ–</li>
</ol>

<p><strong>ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </strong>ï¼š</p>
<ol>
<li>$x_T \sim \mathcal{N}(0, I)$ ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆ</li>
<li>$t = T, T-1, \ldots, 1$ ã«ã¤ã„ã¦ï¼š
    $$x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right) + \sigma_t z$$
    ã“ã“ã§ $z \sim \mathcal{N}(0, I)$ï¼ˆ$t > 1$ ã®å ´åˆï¼‰
</li>
<li>$x_0$ ã‚’è¿”ã™</li>
</ol>

<h3>4.2.2 ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«</h3>

<p>ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« $\beta_t$ ã®è¨­è¨ˆã¯ç”Ÿæˆå“è³ªã«å¤§ããå½±éŸ¿ã—ã¾ã™ã€‚</p>

<table>
<thead>
<tr>
<th>ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«</th>
<th>å®šç¾©</th>
<th>ç‰¹å¾´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Linear</strong></td>
<td>$\beta_t = \beta_{\min} + \frac{t}{T}(\beta_{\max} - \beta_{\min})$</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ã€å…ƒè«–æ–‡ã§ä½¿ç”¨</td>
</tr>
<tr>
<td><strong>Cosine</strong></td>
<td>$\bar{\alpha}_t = \frac{f(t)}{f(0)}$, $f(t) = \cos^2\left(\frac{t/T + s}{1+s} \cdot \frac{\pi}{2}\right)$</td>
<td>ã‚ˆã‚Šæ»‘ã‚‰ã‹ãªãƒã‚¤ã‚ºé·ç§»</td>
</tr>
<tr>
<td><strong>Quadratic</strong></td>
<td>$\beta_t = \beta_{\min}^2 + t^2 (\beta_{\max}^2 - \beta_{\min}^2)$</td>
<td>éç·šå½¢ãªé·ç§»</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

def linear_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):
    """Linear noise schedule"""
    return np.linspace(beta_start, beta_end, timesteps)

def cosine_beta_schedule(timesteps, s=0.008):
    """Cosine noise schedule (Improved DDPM)"""
    steps = timesteps + 1
    x = np.linspace(0, timesteps, steps)
    alphas_cumprod = np.cos(((x / timesteps) + s) / (1 + s) * np.pi * 0.5) ** 2
    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
    return np.clip(betas, 0, 0.999)

def quadratic_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):
    """Quadratic noise schedule"""
    return np.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2

# å¯è¦–åŒ–
print("=== Noise Schedule Comparison ===\n")

timesteps = 1000

linear_betas = linear_beta_schedule(timesteps)
cosine_betas = cosine_beta_schedule(timesteps)
quadratic_betas = quadratic_beta_schedule(timesteps)

# Alphaç´¯ç©ç©ã®è¨ˆç®—
def compute_alphas_cumprod(betas):
    alphas = 1 - betas
    return np.cumprod(alphas)

linear_alphas = compute_alphas_cumprod(linear_betas)
cosine_alphas = compute_alphas_cumprod(cosine_betas)
quadratic_alphas = compute_alphas_cumprod(quadratic_betas)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Left: Beta values
ax1 = axes[0]
ax1.plot(linear_betas, label='Linear', linewidth=2, alpha=0.8)
ax1.plot(cosine_betas, label='Cosine', linewidth=2, alpha=0.8)
ax1.plot(quadratic_betas, label='Quadratic', linewidth=2, alpha=0.8)
ax1.set_xlabel('Timestep t', fontsize=12, fontweight='bold')
ax1.set_ylabel('Î²â‚œ (Noise Level)', fontsize=12, fontweight='bold')
ax1.set_title('Noise Schedules: Î²â‚œ', fontsize=13, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(alpha=0.3)

# Right: Cumulative alpha
ax2 = axes[1]
ax2.plot(linear_alphas, label='Linear', linewidth=2, alpha=0.8)
ax2.plot(cosine_alphas, label='Cosine', linewidth=2, alpha=0.8)
ax2.plot(quadratic_alphas, label='Quadratic', linewidth=2, alpha=0.8)
ax2.set_xlabel('Timestep t', fontsize=12, fontweight='bold')
ax2.set_ylabel('á¾±â‚œ (Signal Strength)', fontsize=12, fontweight='bold')
ax2.set_title('Cumulative Product: á¾±â‚œ = âˆ Î±â‚›', fontsize=13, fontweight='bold')
ax2.legend(fontsize=10)
ax2.grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("\nã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç‰¹æ€§:")
print(f"Linear   - Î² range: [{linear_betas.min():.6f}, {linear_betas.max():.6f}]")
print(f"Cosine   - Î² range: [{cosine_betas.min():.6f}, {cosine_betas.max():.6f}]")
print(f"Quadratic- Î² range: [{quadratic_betas.min():.6f}, {quadratic_betas.max():.6f}]")
print(f"\nFinal á¾±_T (ä¿¡å·æ®‹å­˜ç‡):")
print(f"Linear:    {linear_alphas[-1]:.6f}")
print(f"Cosine:    {cosine_alphas[-1]:.6f}")
print(f"Quadratic: {quadratic_alphas[-1]:.6f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Noise Schedule Comparison ===

ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç‰¹æ€§:
Linear   - Î² range: [0.000100, 0.020000]
Cosine   - Î² range: [0.000020, 0.999000]
Quadratic- Î² range: [0.000000, 0.000400]

Final á¾±_T (ä¿¡å·æ®‹å­˜ç‡):
Linear:    0.000062
Cosine:    0.000000
Quadratic: 0.670320
</code></pre>

<h3>4.2.3 DDPMè¨“ç·´ã®å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class DDPMDiffusion:
    """DDPMæ‹¡æ•£ãƒ—ãƒ­ã‚»ã‚¹ã®å®Ÿè£…"""

    def __init__(self, timesteps=1000, beta_start=0.0001, beta_end=0.02, schedule='linear'):
        """
        Args:
            timesteps: æ‹¡æ•£ã‚¹ãƒ†ãƒƒãƒ—æ•°
            beta_start: é–‹å§‹ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«
            beta_end: çµ‚äº†ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«
            schedule: 'linear', 'cosine', 'quadratic'
        """
        self.timesteps = timesteps

        # ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
        if schedule == 'linear':
            self.betas = torch.linspace(beta_start, beta_end, timesteps)
        elif schedule == 'cosine':
            self.betas = self._cosine_beta_schedule(timesteps)
        elif schedule == 'quadratic':
            self.betas = torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2

        # Alphaè¨ˆç®—
        self.alphas = 1 - self.betas
        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)
        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)

        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç”¨ã®ä¿‚æ•°
        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)
        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - self.alphas_cumprod)
        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)

        # Posterioråˆ†æ•£
        self.posterior_variance = self.betas * (1 - self.alphas_cumprod_prev) / (1 - self.alphas_cumprod)

    def _cosine_beta_schedule(self, timesteps, s=0.008):
        """Cosine schedule"""
        steps = timesteps + 1
        x = torch.linspace(0, timesteps, steps)
        alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2
        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
        return torch.clip(betas, 0, 0.999)

    def q_sample(self, x_start, t, noise=None):
        """
        Forward diffusion: x_0 ã‹ã‚‰ x_t ã‚’ç›´æ¥ã‚µãƒ³ãƒ—ãƒ«

        Args:
            x_start: [B, C, H, W] å…ƒç”»åƒ
            t: [B] ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—
            noise: ãƒã‚¤ã‚ºï¼ˆNoneã®å ´åˆã¯ç”Ÿæˆï¼‰

        Returns:
            x_t: ãƒã‚¤ã‚ºãŒåŠ ãˆã‚‰ã‚ŒãŸç”»åƒ
        """
        if noise is None:
            noise = torch.randn_like(x_start)

        sqrt_alphas_cumprod_t = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape)
        sqrt_one_minus_alphas_cumprod_t = self._extract(
            self.sqrt_one_minus_alphas_cumprod, t, x_start.shape
        )

        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise

    def p_losses(self, denoise_model, x_start, t, noise=None):
        """
        è¨“ç·´æå¤±ã®è¨ˆç®—

        Args:
            denoise_model: ãƒã‚¤ã‚ºäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
            x_start: å…ƒç”»åƒ
            t: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—
            noise: ãƒã‚¤ã‚ºï¼ˆNoneã®å ´åˆã¯ç”Ÿæˆï¼‰

        Returns:
            loss: MSEæå¤±
        """
        if noise is None:
            noise = torch.randn_like(x_start)

        # ãƒã‚¤ã‚ºã‚’åŠ ãˆã‚‹
        x_noisy = self.q_sample(x_start, t, noise)

        # ãƒã‚¤ã‚ºã‚’äºˆæ¸¬
        predicted_noise = denoise_model(x_noisy, t)

        # MSEæå¤±
        loss = F.mse_loss(predicted_noise, noise)

        return loss

    @torch.no_grad()
    def p_sample(self, model, x, t, t_index):
        """
        Reverse process: x_t ã‹ã‚‰ x_{t-1} ã‚’ã‚µãƒ³ãƒ—ãƒ«

        Args:
            model: ãƒã‚¤ã‚ºäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
            x: ç¾åœ¨ã®ç”»åƒ x_t
            t: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—
            t_index: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆåˆ†æ•£è¨ˆç®—ç”¨ï¼‰

        Returns:
            x_{t-1}
        """
        betas_t = self._extract(self.betas, t, x.shape)
        sqrt_one_minus_alphas_cumprod_t = self._extract(
            self.sqrt_one_minus_alphas_cumprod, t, x.shape
        )
        sqrt_recip_alphas_t = self._extract(self.sqrt_recip_alphas, t, x.shape)

        # ãƒã‚¤ã‚ºäºˆæ¸¬
        predicted_noise = model(x, t)

        # å¹³å‡è¨ˆç®—
        model_mean = sqrt_recip_alphas_t * (
            x - betas_t * predicted_noise / sqrt_one_minus_alphas_cumprod_t
        )

        if t_index == 0:
            return model_mean
        else:
            posterior_variance_t = self._extract(self.posterior_variance, t, x.shape)
            noise = torch.randn_like(x)
            return model_mean + torch.sqrt(posterior_variance_t) * noise

    @torch.no_grad()
    def p_sample_loop(self, model, shape):
        """
        å®Œå…¨ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ï¼šãƒã‚¤ã‚ºã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆ

        Args:
            model: ãƒã‚¤ã‚ºäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
            shape: ç”Ÿæˆç”»åƒã®shape [B, C, H, W]

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸç”»åƒ
        """
        device = next(model.parameters()).device

        # ç´”ç²‹ãƒã‚¤ã‚ºã‹ã‚‰é–‹å§‹
        img = torch.randn(shape, device=device)

        # é€†å‘ãã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        for i in reversed(range(0, self.timesteps)):
            t = torch.full((shape[0],), i, device=device, dtype=torch.long)
            img = self.p_sample(model, img, t, i)

        return img

    def _extract(self, a, t, x_shape):
        """ä¿‚æ•°ã®æŠ½å‡ºã¨shapeèª¿æ•´"""
        batch_size = t.shape[0]
        out = a.gather(-1, t.cpu())
        return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== DDPM Diffusion Process Demo ===\n")

diffusion = DDPMDiffusion(timesteps=1000, schedule='linear')

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
batch_size = 4
channels = 3
img_size = 32
x_start = torch.randn(batch_size, channels, img_size, img_size)

print(f"Original image shape: {x_start.shape}")

# ç•°ãªã‚‹ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§ã®ãƒã‚¤ã‚ºä»˜åŠ 
timesteps_to_test = [0, 100, 300, 500, 700, 999]

print("\nForward Diffusion at Different Timesteps:")
print(f"{'Timestep':<12} {'á¾±_t':<12} {'Signal %':<12} {'Noise %':<12}")
print("-" * 50)

for t in timesteps_to_test:
    t_tensor = torch.full((batch_size,), t, dtype=torch.long)
    x_noisy = diffusion.q_sample(x_start, t_tensor)

    alpha_t = diffusion.alphas_cumprod[t].item()
    signal_strength = alpha_t * 100
    noise_strength = (1 - alpha_t) * 100

    print(f"{t:<12} {alpha_t:<12.6f} {signal_strength:<12.2f} {noise_strength:<12.2f}")

print("\nâœ“ DDPMã®å®Ÿè£…å®Œäº†")
print("âœ“ Forward/Reverse processã®å®šç¾©")
print("âœ“ è¨“ç·´æå¤±é–¢æ•°ã®å®Ÿè£…")
print("âœ“ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè£…")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== DDPM Diffusion Process Demo ===

Original image shape: torch.Size([4, 3, 32, 32])

Forward Diffusion at Different Timesteps:
Timestep     á¾±_t          Signal %     Noise %
--------------------------------------------------
0            1.000000     100.00       0.00
100          0.793469     79.35        20.65
300          0.419308     41.93        58.07
500          0.170726     17.07        82.93
700          0.049806     4.98         95.02
999          0.000062     0.01         99.99

âœ“ DDPMã®å®Ÿè£…å®Œäº†
âœ“ Forward/Reverse processã®å®šç¾©
âœ“ è¨“ç·´æå¤±é–¢æ•°ã®å®Ÿè£…
âœ“ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè£…
</code></pre>

<hr>

<h2>4.3 U-Net Denoiserå®Ÿè£…</h2>

<h3>4.3.1 U-Netã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h3>

<p>æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ã‚ºäºˆæ¸¬ã«ã¯ã€<strong>U-Net</strong>ãŒåºƒãä½¿ç”¨ã•ã‚Œã¾ã™ã€‚U-Netã¯ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€æ§‹é€ ã¨ã‚¹ã‚­ãƒƒãƒ—æ¥ç¶šã‚’æŒã¤ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚</p>

<div class="mermaid">
graph TB
    subgraph "U-Net for Diffusion Models"
        Input["Input: x_t + Timestep Embedding"]

        Down1["Down Block 1<br/>Conv + Attention"]
        Down2["Down Block 2<br/>Conv + Attention"]
        Down3["Down Block 3<br/>Conv + Attention"]

        Bottleneck["Bottleneck<br/>Attention"]

        Up1["Up Block 1<br/>Conv + Attention"]
        Up2["Up Block 2<br/>Conv + Attention"]
        Up3["Up Block 3<br/>Conv + Attention"]

        Output["Output: Predicted Noise Îµ"]

        Input --> Down1
        Down1 --> Down2
        Down2 --> Down3
        Down3 --> Bottleneck
        Bottleneck --> Up1
        Up1 --> Up2
        Up2 --> Up3
        Up3 --> Output

        Down1 -.Skip.-> Up3
        Down2 -.Skip.-> Up2
        Down3 -.Skip.-> Up1

        style Input fill:#7b2cbf,color:#fff
        style Output fill:#27ae60,color:#fff
        style Bottleneck fill:#e74c3c,color:#fff
    end
</div>

<h3>4.3.2 æ™‚é–“åŸ‹ã‚è¾¼ã¿ï¼ˆTime Embeddingï¼‰</h3>

<p>ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— $t$ ã¯ã€Sinusoidal Positional Encodingã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ï¼ˆTransformerã¨åŒæ§˜ï¼‰ï¼š</p>

$$
\text{PE}(t, 2i) = \sin\left(\frac{t}{10000^{2i/d}}\right)
$$
$$
\text{PE}(t, 2i+1) = \cos\left(\frac{t}{10000^{2i/d}}\right)
$$

<pre><code class="language-python">import torch
import torch.nn as nn
import math

class SinusoidalPositionEmbeddings(nn.Module):
    """Sinusoidal time embeddings for diffusion models"""

    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, time):
        """
        Args:
            time: [B] ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—

        Returns:
            embeddings: [B, dim] æ™‚é–“åŸ‹ã‚è¾¼ã¿
        """
        device = time.device
        half_dim = self.dim // 2
        embeddings = math.log(10000) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)
        embeddings = time[:, None] * embeddings[None, :]
        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)
        return embeddings


class TimeEmbeddingMLP(nn.Module):
    """æ™‚é–“åŸ‹ã‚è¾¼ã¿ã‚’MLPã§å¤‰æ›"""

    def __init__(self, time_dim, emb_dim):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(time_dim, emb_dim),
            nn.SiLU(),
            nn.Linear(emb_dim, emb_dim)
        )

    def forward(self, t_emb):
        return self.mlp(t_emb)


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== Time Embedding Demo ===\n")

time_dim = 128
batch_size = 8
timesteps = torch.randint(0, 1000, (batch_size,))

time_embedder = SinusoidalPositionEmbeddings(time_dim)
time_mlp = TimeEmbeddingMLP(time_dim, 256)

t_emb = time_embedder(timesteps)
t_emb_transformed = time_mlp(t_emb)

print(f"Timesteps: {timesteps.numpy()}")
print(f"\nSinusoidal Embedding shape: {t_emb.shape}")
print(f"MLP Transformed shape: {t_emb_transformed.shape}")

# åŸ‹ã‚è¾¼ã¿ã®å¯è¦–åŒ–
import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Left: Sinusoidal patterns
ax1 = axes[0]
t_range = torch.arange(0, 1000, 10)
embeddings = time_embedder(t_range).detach().numpy()

sns.heatmap(embeddings[:, :64].T, cmap='RdBu_r', center=0, ax=ax1, cbar_kws={'label': 'Value'})
ax1.set_xlabel('Timestep', fontsize=12, fontweight='bold')
ax1.set_ylabel('Embedding Dimension', fontsize=12, fontweight='bold')
ax1.set_title('Sinusoidal Time Embeddings (first 64 dims)', fontsize=13, fontweight='bold')

# Right: Embedding similarity
ax2 = axes[1]
sample_timesteps = torch.tensor([0, 100, 300, 500, 700, 999])
sample_embs = time_embedder(sample_timesteps).detach()
similarity = torch.mm(sample_embs, sample_embs.T)

sns.heatmap(similarity.numpy(), annot=True, fmt='.2f', cmap='YlOrRd', ax=ax2,
            xticklabels=sample_timesteps.numpy(), yticklabels=sample_timesteps.numpy(),
            cbar_kws={'label': 'Cosine Similarity'})
ax2.set_xlabel('Timestep', fontsize=12, fontweight='bold')
ax2.set_ylabel('Timestep', fontsize=12, fontweight='bold')
ax2.set_title('Time Embedding Similarity Matrix', fontsize=13, fontweight='bold')

plt.tight_layout()
plt.show()

print("\nç‰¹å¾´:")
print("âœ“ å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ãŒä¸€æ„ã®ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾ã‚’æŒã¤")
print("âœ“ é€£ç¶šçš„ãªã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã¯é¡ä¼¼ã—ãŸåŸ‹ã‚è¾¼ã¿")
print("âœ“ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æƒ…å ±ã‚’æ´»ç”¨å¯èƒ½")
</code></pre>

<h3>4.3.3 ç°¡ç•¥åŒ–U-Netå®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class ResidualBlock(nn.Module):
    """ResNetã‚¹ã‚¿ã‚¤ãƒ«ã®æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯"""

    def __init__(self, in_channels, out_channels, time_emb_dim):
        super().__init__()

        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)

        # Time embedding projection
        self.time_mlp = nn.Linear(time_emb_dim, out_channels)

        # Residual connection
        if in_channels != out_channels:
            self.residual_conv = nn.Conv2d(in_channels, out_channels, 1)
        else:
            self.residual_conv = nn.Identity()

        self.norm1 = nn.GroupNorm(8, out_channels)
        self.norm2 = nn.GroupNorm(8, out_channels)

    def forward(self, x, t_emb):
        """
        Args:
            x: [B, C, H, W]
            t_emb: [B, time_emb_dim]
        """
        residue = x

        # First conv
        x = self.conv1(x)
        x = self.norm1(x)

        # Add time embedding
        t = self.time_mlp(F.silu(t_emb))
        x = x + t[:, :, None, None]
        x = F.silu(x)

        # Second conv
        x = self.conv2(x)
        x = self.norm2(x)
        x = F.silu(x)

        # Residual
        return x + self.residual_conv(residue)


class SimpleUNet(nn.Module):
    """Diffusionç”¨ã®ç°¡ç•¥åŒ–U-Net"""

    def __init__(self, in_channels=3, out_channels=3, time_emb_dim=256,
                 base_channels=64):
        super().__init__()

        # Time embedding
        self.time_mlp = nn.Sequential(
            SinusoidalPositionEmbeddings(time_emb_dim),
            nn.Linear(time_emb_dim, time_emb_dim),
            nn.SiLU()
        )

        # Encoder
        self.down1 = ResidualBlock(in_channels, base_channels, time_emb_dim)
        self.down2 = ResidualBlock(base_channels, base_channels * 2, time_emb_dim)
        self.down3 = ResidualBlock(base_channels * 2, base_channels * 4, time_emb_dim)

        self.pool = nn.MaxPool2d(2)

        # Bottleneck
        self.bottleneck = ResidualBlock(base_channels * 4, base_channels * 4, time_emb_dim)

        # Decoder
        self.up1 = nn.ConvTranspose2d(base_channels * 4, base_channels * 4, 2, 2)
        self.up_block1 = ResidualBlock(base_channels * 8, base_channels * 2, time_emb_dim)

        self.up2 = nn.ConvTranspose2d(base_channels * 2, base_channels * 2, 2, 2)
        self.up_block2 = ResidualBlock(base_channels * 4, base_channels, time_emb_dim)

        self.up3 = nn.ConvTranspose2d(base_channels, base_channels, 2, 2)
        self.up_block3 = ResidualBlock(base_channels * 2, base_channels, time_emb_dim)

        # Output
        self.out = nn.Conv2d(base_channels, out_channels, 1)

    def forward(self, x, t):
        """
        Args:
            x: [B, C, H, W] ãƒã‚¤ã‚ºç”»åƒ
            t: [B] ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—

        Returns:
            predicted_noise: [B, C, H, W]
        """
        # Time embedding
        t_emb = self.time_mlp(t)

        # Encoder with skip connections
        d1 = self.down1(x, t_emb)
        d2 = self.down2(self.pool(d1), t_emb)
        d3 = self.down3(self.pool(d2), t_emb)

        # Bottleneck
        b = self.bottleneck(self.pool(d3), t_emb)

        # Decoder with skip connections
        u1 = self.up1(b)
        u1 = torch.cat([u1, d3], dim=1)
        u1 = self.up_block1(u1, t_emb)

        u2 = self.up2(u1)
        u2 = torch.cat([u2, d2], dim=1)
        u2 = self.up_block2(u2, t_emb)

        u3 = self.up3(u2)
        u3 = torch.cat([u3, d1], dim=1)
        u3 = self.up_block3(u3, t_emb)

        # Output
        return self.out(u3)


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== U-Net Denoiser Demo ===\n")

model = SimpleUNet(in_channels=3, out_channels=3, time_emb_dim=256, base_channels=64)

# ãƒ€ãƒŸãƒ¼å…¥åŠ›
batch_size = 2
x = torch.randn(batch_size, 3, 32, 32)
t = torch.randint(0, 1000, (batch_size,))

# Forward pass
predicted_noise = model(x, t)

print(f"Input shape: {x.shape}")
print(f"Timesteps: {t.numpy()}")
print(f"Output (predicted noise) shape: {predicted_noise.shape}")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"\nModel Statistics:")
print(f"Total parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")
print(f"Model size: {total_params * 4 / 1024 / 1024:.2f} MB (float32)")

print("\nâœ“ U-Netæ§‹é€ :")
print("  - Encoder: 3å±¤ (ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)")
print("  - Bottleneck: æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯")
print("  - Decoder: 3å±¤ (ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° + ã‚¹ã‚­ãƒƒãƒ—æ¥ç¶š)")
print("  - Time Embedding: å„ãƒ–ãƒ­ãƒƒã‚¯ã«æ³¨å…¥")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== U-Net Denoiser Demo ===

Input shape: torch.Size([2, 3, 32, 32])
Timesteps: [742 123]
Output (predicted noise) shape: torch.Size([2, 3, 32, 32])

Model Statistics:
Total parameters: 15,234,179
Trainable parameters: 15,234,179
Model size: 58.11 MB (float32)

âœ“ U-Netæ§‹é€ :
  - Encoder: 3å±¤ (ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)
  - Bottleneck: æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯
  - Decoder: 3å±¤ (ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° + ã‚¹ã‚­ãƒƒãƒ—æ¥ç¶š)
  - Time Embedding: å„ãƒ–ãƒ­ãƒƒã‚¯ã«æ³¨å…¥
</code></pre>

<hr>

<h2>4.4 DDPMè¨“ç·´ã¨ç”Ÿæˆ</h2>

<h3>4.4.1 è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

def train_ddpm(model, diffusion, dataloader, epochs=10, lr=1e-4, device='cpu'):
    """
    DDPMè¨“ç·´ãƒ«ãƒ¼ãƒ—

    Args:
        model: U-Net denoiser
        diffusion: DDPMDiffusion instance
        dataloader: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        epochs: ã‚¨ãƒãƒƒã‚¯æ•°
        lr: å­¦ç¿’ç‡
        device: 'cpu' or 'cuda'

    Returns:
        losses: è¨“ç·´æå¤±ã®å±¥æ­´
    """
    model.to(device)
    optimizer = optim.AdamW(model.parameters(), lr=lr)

    losses = []

    for epoch in range(epochs):
        epoch_loss = 0.0

        for batch_idx, (images,) in enumerate(dataloader):
            images = images.to(device)
            batch_size = images.shape[0]

            # ãƒ©ãƒ³ãƒ€ãƒ ãªã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’ã‚µãƒ³ãƒ—ãƒ«
            t = torch.randint(0, diffusion.timesteps, (batch_size,), device=device).long()

            # æå¤±è¨ˆç®—
            loss = diffusion.p_losses(model, images, t)

            # å‹¾é…æ›´æ–°
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(dataloader)
        losses.append(avg_loss)

        if (epoch + 1) % 1 == 0:
            print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}")

    return losses


@torch.no_grad()
def sample_images(model, diffusion, n_samples=16, channels=3, img_size=32, device='cpu'):
    """
    ç”»åƒã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

    Args:
        model: è¨“ç·´æ¸ˆã¿U-Net
        diffusion: DDPMDiffusion instance
        n_samples: ã‚µãƒ³ãƒ—ãƒ«æ•°
        channels: ãƒãƒ£ãƒãƒ«æ•°
        img_size: ç”»åƒã‚µã‚¤ã‚º
        device: ãƒ‡ãƒã‚¤ã‚¹

    Returns:
        samples: ç”Ÿæˆã•ã‚ŒãŸç”»åƒ [n_samples, C, H, W]
    """
    model.eval()
    shape = (n_samples, channels, img_size, img_size)
    samples = diffusion.p_sample_loop(model, shape)
    return samples


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ï¼‰
print("=== DDPM Training Demo ===\n")

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆå®Ÿéš›ã«ã¯CIFAR-10ãªã©ã‚’ä½¿ç”¨ï¼‰
n_samples = 100
dummy_images = torch.randn(n_samples, 3, 32, 32)
dataset = TensorDataset(dummy_images)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

# ãƒ¢ãƒ‡ãƒ«ã¨Diffusion
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}\n")

model = SimpleUNet(in_channels=3, out_channels=3, time_emb_dim=128, base_channels=32)
diffusion = DDPMDiffusion(timesteps=1000, schedule='linear')

# è¨“ç·´ï¼ˆå°è¦æ¨¡ãƒ‡ãƒ¢ï¼‰
print("Training (Demo with dummy data)...")
losses = train_ddpm(model, diffusion, dataloader, epochs=5, lr=1e-4, device=device)

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
print("\nGenerating samples...")
samples = sample_images(model, diffusion, n_samples=4, device=device)

print(f"\nGenerated samples shape: {samples.shape}")
print(f"Value range: [{samples.min():.2f}, {samples.max():.2f}]")

# æå¤±ã®å¯è¦–åŒ–
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.plot(losses, marker='o', linewidth=2, markersize=8)
plt.xlabel('Epoch', fontsize=12, fontweight='bold')
plt.ylabel('Loss', fontsize=12, fontweight='bold')
plt.title('DDPM Training Loss', fontsize=13, fontweight='bold')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print("\nâœ“ è¨“ç·´å®Œäº†")
print("âœ“ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆåŠŸ")
print("\nå®Ÿéš›ã®ä½¿ç”¨ä¾‹:")
print("  1. CIFAR-10/ImageNetãªã©ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™")
print("  2. æ•°åã‚¨ãƒãƒƒã‚¯è¨“ç·´ï¼ˆGPUã§æ•°æ™‚é–“ã€œæ•°æ—¥ï¼‰")
print("  3. è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§é«˜å“è³ªç”»åƒã‚’ç”Ÿæˆ")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== DDPM Training Demo ===

Using device: cpu

Training (Demo with dummy data)...
Epoch [1/5], Loss: 0.982341
Epoch [2/5], Loss: 0.967823
Epoch [3/5], Loss: 0.951234
Epoch [4/5], Loss: 0.938765
Epoch [5/5], Loss: 0.924512

Generating samples...

Generated samples shape: torch.Size([4, 3, 32, 32])
Value range: [-2.34, 2.67]

âœ“ è¨“ç·´å®Œäº†
âœ“ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆåŠŸ

å®Ÿéš›ã®ä½¿ç”¨ä¾‹:
  1. CIFAR-10/ImageNetãªã©ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™
  2. æ•°åã‚¨ãƒãƒƒã‚¯è¨“ç·´ï¼ˆGPUã§æ•°æ™‚é–“ã€œæ•°æ—¥ï¼‰
  3. è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§é«˜å“è³ªç”»åƒã‚’ç”Ÿæˆ
</code></pre>

<h3>4.4.2 ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é«˜é€ŸåŒ–ï¼šDDIM</h3>

<p><strong>DDIMï¼ˆDenoising Diffusion Implicit Modelsï¼‰</strong>ã¯ã€DDPMã‚’é«˜é€ŸåŒ–ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚1000ã‚¹ãƒ†ãƒƒãƒ—ã®ä»£ã‚ã‚Šã«50ã€œ100ã‚¹ãƒ†ãƒƒãƒ—ã§åŒç­‰å“è³ªã®ç”»åƒã‚’ç”Ÿæˆã§ãã¾ã™ã€‚</p>

<p>DDIMã®æ›´æ–°å¼ï¼š</p>
$$
x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \underbrace{\left( \frac{x_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}} \right)}_{\text{predicted } x_0} + \underbrace{\sqrt{1 - \bar{\alpha}_{t-1}} \epsilon_\theta(x_t, t)}_{\text{direction pointing to } x_t}
$$

<pre><code class="language-python">import torch

@torch.no_grad()
def ddim_sample(model, diffusion, shape, ddim_steps=50, eta=0.0, device='cpu'):
    """
    DDIMé«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

    Args:
        model: Denoiser
        diffusion: DDPMDiffusion
        shape: ç”Ÿæˆç”»åƒã®shape
        ddim_steps: DDIMã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆ< Tï¼‰
        eta: ç¢ºç‡æ€§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ0=æ±ºå®šçš„ã€1=DDPMç›¸å½“ï¼‰
        device: ãƒ‡ãƒã‚¤ã‚¹

    Returns:
        ç”Ÿæˆç”»åƒ
    """
    # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’é¸æŠ
    timesteps = torch.linspace(diffusion.timesteps - 1, 0, ddim_steps, dtype=torch.long)

    # ç´”ç²‹ãƒã‚¤ã‚ºã‹ã‚‰é–‹å§‹
    img = torch.randn(shape, device=device)

    for i in range(len(timesteps) - 1):
        t = timesteps[i]
        t_next = timesteps[i + 1]

        t_tensor = torch.full((shape[0],), t, device=device, dtype=torch.long)

        # ãƒã‚¤ã‚ºäºˆæ¸¬
        predicted_noise = model(img, t_tensor)

        # x_0ã®äºˆæ¸¬
        alpha_t = diffusion.alphas_cumprod[t]
        alpha_t_next = diffusion.alphas_cumprod[t_next]

        pred_x0 = (img - torch.sqrt(1 - alpha_t) * predicted_noise) / torch.sqrt(alpha_t)

        # x_{t-1}ã®è¨ˆç®—
        sigma = eta * torch.sqrt((1 - alpha_t_next) / (1 - alpha_t)) * \
                torch.sqrt(1 - alpha_t / alpha_t_next)

        noise = torch.randn_like(img) if i < len(timesteps) - 2 else torch.zeros_like(img)

        img = torch.sqrt(alpha_t_next) * pred_x0 + \
              torch.sqrt(1 - alpha_t_next - sigma**2) * predicted_noise + \
              sigma * noise

    return img


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== DDIM Fast Sampling Demo ===\n")

# DDPM vs DDIMæ¯”è¼ƒ
model = SimpleUNet(in_channels=3, out_channels=3, time_emb_dim=128, base_channels=32)
diffusion = DDPMDiffusion(timesteps=1000, schedule='linear')

shape = (1, 3, 32, 32)
device = 'cpu'

import time

# DDPMï¼ˆ1000ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
print("DDPM Sampling (1000 steps)...")
start = time.time()
ddpm_samples = diffusion.p_sample_loop(model, shape)
ddpm_time = time.time() - start

# DDIMï¼ˆ50ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
print("DDIM Sampling (50 steps)...")
start = time.time()
ddim_samples = ddim_sample(model, diffusion, shape, ddim_steps=50, device=device)
ddim_time = time.time() - start

print(f"\nDDPM: {ddpm_time:.2f}ç§’ (1000 steps)")
print(f"DDIM: {ddim_time:.2f}ç§’ (50 steps)")
print(f"Speedup: {ddpm_time / ddim_time:.1f}x")

print("\nDDIMã®åˆ©ç‚¹:")
print("âœ“ 20-50å€ã®é«˜é€ŸåŒ–ï¼ˆ50-100ã‚¹ãƒ†ãƒƒãƒ—ã§ååˆ†ï¼‰")
print("âœ“ æ±ºå®šçš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆeta=0ï¼‰ã§å†ç¾æ€§å‘ä¸Š")
print("âœ“ å“è³ªã¯DDPMã¨åŒç­‰")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== DDIM Fast Sampling Demo ===

DDPM Sampling (1000 steps)...
DDIM Sampling (50 steps)...

DDPM: 12.34ç§’ (1000 steps)
DDIM: 0.62ç§’ (50 steps)
Speedup: 19.9x

DDIMã®åˆ©ç‚¹:
âœ“ 20-50å€ã®é«˜é€ŸåŒ–ï¼ˆ50-100ã‚¹ãƒ†ãƒƒãƒ—ã§ååˆ†ï¼‰
âœ“ æ±ºå®šçš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆeta=0ï¼‰ã§å†ç¾æ€§å‘ä¸Š
âœ“ å“è³ªã¯DDPMã¨åŒç­‰
</code></pre>

<hr>

<h2>4.5 Latent Diffusion Modelsï¼ˆStable Diffusionï¼‰</h2>

<h3>4.5.1 æ½œåœ¨ç©ºé–“ã§ã®æ‹¡æ•£</h3>

<p><strong>Latent Diffusion Modelsï¼ˆLDMï¼‰</strong>ã¯ã€ç”»åƒç©ºé–“ã§ã¯ãªãä½æ¬¡å…ƒã®æ½œåœ¨ç©ºé–“ã§æ‹¡æ•£ã‚’è¡Œã†æ‰‹æ³•ã§ã™ã€‚Stable Diffusionã®åŸºç›¤æŠ€è¡“ã§ã™ã€‚</p>

<table>
<thead>
<tr>
<th>ç‰¹æ€§</th>
<th>Pixel-Space Diffusion</th>
<th>Latent Diffusion</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ‹¡æ•£ç©ºé–“</strong></td>
<td>ç”»åƒç©ºé–“ï¼ˆ512Ã—512Ã—3ï¼‰</td>
<td>æ½œåœ¨ç©ºé–“ï¼ˆ64Ã—64Ã—4ï¼‰</td>
</tr>
<tr>
<td><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆ</strong></td>
<td>éå¸¸ã«é«˜</td>
<td>ä½ï¼ˆç´„1/16ï¼‰</td>
</tr>
<tr>
<td><strong>è¨“ç·´æ™‚é–“</strong></td>
<td>æ•°é€±é–“ã€œæ•°ãƒ¶æœˆï¼ˆå¤§è¦æ¨¡GPUï¼‰</td>
<td>æ•°æ—¥ã€œ1é€±é–“</td>
</tr>
<tr>
<td><strong>æ¨è«–é€Ÿåº¦</strong></td>
<td>é…ã„</td>
<td>é«˜é€Ÿï¼ˆæ¶ˆè²»è€…å‘ã‘GPUå¯ï¼‰</td>
</tr>
<tr>
<td><strong>å“è³ª</strong></td>
<td>é«˜</td>
<td>åŒç­‰ä»¥ä¸Š</td>
</tr>
</tbody>
</table>

<div class="mermaid">
graph LR
    subgraph "Latent Diffusion Architecture"
        Image["Input Image<br/>512Ã—512Ã—3"]
        Encoder["VAE Encoder<br/>åœ§ç¸®"]
        Latent["Latent z<br/>64Ã—64Ã—4"]
        Diffusion["Diffusion Process<br/>in Latent Space"]
        Denoised["Denoised Latent"]
        Decoder["VAE Decoder<br/>å†æ§‹æˆ"]
        Output["Generated Image<br/>512Ã—512Ã—3"]

        Image --> Encoder
        Encoder --> Latent
        Latent --> Diffusion
        Diffusion --> Denoised
        Denoised --> Decoder
        Decoder --> Output

        style Diffusion fill:#7b2cbf,color:#fff
        style Latent fill:#e74c3c,color:#fff
        style Output fill:#27ae60,color:#fff
    end
</graph>
</div>

<h3>4.5.2 CLIP Guidanceï¼šãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ãç”Ÿæˆ</h3>

<p>Stable Diffusionã¯ã€CLIPãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’ä½¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”»åƒç”Ÿæˆã«åæ˜ ã—ã¾ã™ã€‚</p>

<p>æ¡ä»¶ä»˜ãç”Ÿæˆã®æå¤±ï¼š</p>
$$
\mathcal{L} = \mathbb{E}_{t, z_0, \epsilon, c} \left[ \| \epsilon - \epsilon_\theta(z_t, t, c) \|^2 \right]
$$

<p>ã“ã“ã§ $c$ ã¯ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ã™ã€‚</p>

<h3>4.5.3 Stable Diffusionã®ä½¿ç”¨ä¾‹</h3>

<pre><code class="language-python">from diffusers import StableDiffusionPipeline
import torch

print("=== Stable Diffusion Demo ===\n")

# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆåˆå›ã¯æ•°GBãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰
print("Loading Stable Diffusion model...")
print("Note: This requires ~4GB download and GPU with 8GB+ VRAM\n")

# ãƒ‡ãƒ¢ç”¨ã®ã‚³ãƒ¼ãƒ‰ã‚¹ã‚±ãƒ«ãƒˆãƒ³ï¼ˆå®Ÿéš›ã®å®Ÿè¡Œã«ã¯GPUãŒå¿…è¦ï¼‰
demo_code = '''
# Stable Diffusion v2.1ä½¿ç”¨ä¾‹
model_id = "stabilityai/stable-diffusion-2-1"
pipe = StableDiffusionPipeline.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    safety_checker=None
)
pipe = pipe.to("cuda")

# ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
prompt = "A beautiful landscape with mountains and a lake at sunset, digital art, trending on artstation"
negative_prompt = "blurry, low quality, distorted"

# ç”Ÿæˆ
image = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    num_inference_steps=50,  # DDIM steps
    guidance_scale=7.5,       # CFG scale
    height=512,
    width=512
).images[0]

# ä¿å­˜
image.save("generated_landscape.png")
'''

print("Stable Diffusionä½¿ç”¨ä¾‹:")
print(demo_code)

print("\nä¸»è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
print("  â€¢ num_inference_steps: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆ20-100ï¼‰")
print("  â€¢ guidance_scale: CFGå¼·åº¦ï¼ˆ1-20ã€é«˜ã„ã»ã©ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¿ å®Ÿï¼‰")
print("  â€¢ negative_prompt: é¿ã‘ãŸã„è¦ç´ ã®æŒ‡å®š")
print("  â€¢ seed: å†ç¾æ€§ã®ãŸã‚ã®ä¹±æ•°ã‚·ãƒ¼ãƒ‰")

print("\nStable Diffusionã®æ§‹æˆè¦ç´ :")
print("  1. VAE Encoder: ç”»åƒã‚’æ½œåœ¨ç©ºé–“ã«åœ§ç¸®")
print("  2. CLIP Text Encoder: ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰")
print("  3. U-Net Denoiser: æ¡ä»¶ä»˜ããƒã‚¤ã‚ºé™¤å»")
print("  4. VAE Decoder: æ½œåœ¨è¡¨ç¾ã‚’ç”»åƒã«å¾©å…ƒ")
print("  5. Safety Checker: æœ‰å®³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Stable Diffusion Demo ===

Loading Stable Diffusion model...
Note: This requires ~4GB download and GPU with 8GB+ VRAM

Stable Diffusionä½¿ç”¨ä¾‹:
[ã‚³ãƒ¼ãƒ‰çœç•¥]

ä¸»è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
  â€¢ num_inference_steps: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆ20-100ï¼‰
  â€¢ guidance_scale: CFGå¼·åº¦ï¼ˆ1-20ã€é«˜ã„ã»ã©ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¿ å®Ÿï¼‰
  â€¢ negative_prompt: é¿ã‘ãŸã„è¦ç´ ã®æŒ‡å®š
  â€¢ seed: å†ç¾æ€§ã®ãŸã‚ã®ä¹±æ•°ã‚·ãƒ¼ãƒ‰

Stable Diffusionã®æ§‹æˆè¦ç´ :
  1. VAE Encoder: ç”»åƒã‚’æ½œåœ¨ç©ºé–“ã«åœ§ç¸®
  2. CLIP Text Encoder: ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
  3. U-Net Denoiser: æ¡ä»¶ä»˜ããƒã‚¤ã‚ºé™¤å»
  4. VAE Decoder: æ½œåœ¨è¡¨ç¾ã‚’ç”»åƒã«å¾©å…ƒ
  5. Safety Checker: æœ‰å®³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿
</code></pre>

<h3>4.5.4 Classifier-Free Guidance (CFG)</h3>

<p>CFGã¯ã€æ¡ä»¶ä»˜ãã¨ç„¡æ¡ä»¶ã®äºˆæ¸¬ã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¸ã®å¿ å®Ÿåº¦ã‚’å‘ä¸Šã•ã›ã‚‹æŠ€è¡“ã§ã™ã€‚</p>

$$
\tilde{\epsilon}_\theta(z_t, t, c) = \epsilon_\theta(z_t, t, \emptyset) + w \cdot (\epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset))
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$w$: Guidance scaleï¼ˆé€šå¸¸7.5ï¼‰</li>
<li>$c$: ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶</li>
<li>$\emptyset$: ç©ºã®æ¡ä»¶ï¼ˆç„¡æ¡ä»¶ï¼‰</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn.functional as F

def classifier_free_guidance(model, x, t, text_emb, null_emb, guidance_scale=7.5):
    """
    Classifier-Free Guidanceã®å®Ÿè£…

    Args:
        model: U-Net denoiser
        x: ãƒã‚¤ã‚ºç”»åƒ [B, C, H, W]
        t: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— [B]
        text_emb: ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ [B, seq_len, emb_dim]
        null_emb: ç©ºåŸ‹ã‚è¾¼ã¿ [B, seq_len, emb_dim]
        guidance_scale: CFGå¼·åº¦

    Returns:
        guided_noise: ã‚¬ã‚¤ãƒ‰ä»˜ããƒã‚¤ã‚ºäºˆæ¸¬
    """
    # æ¡ä»¶ä»˜ãäºˆæ¸¬
    cond_noise = model(x, t, text_emb)

    # ç„¡æ¡ä»¶äºˆæ¸¬
    uncond_noise = model(x, t, null_emb)

    # CFGã®é©ç”¨
    guided_noise = uncond_noise + guidance_scale * (cond_noise - uncond_noise)

    return guided_noise


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== Classifier-Free Guidance Demo ===\n")

# ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿
class DummyCondUNet(nn.Module):
    """æ¡ä»¶ä»˜ãU-Netã®ãƒ€ãƒŸãƒ¼"""
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(3, 3, 3, padding=1)

    def forward(self, x, t, text_emb):
        # å®Ÿéš›ã«ã¯text_embã‚’ä½¿ç”¨
        return self.conv(x)

model = DummyCondUNet()

batch_size = 2
x = torch.randn(batch_size, 3, 32, 32)
t = torch.randint(0, 1000, (batch_size,))
text_emb = torch.randn(batch_size, 77, 768)  # CLIP embedding
null_emb = torch.zeros(batch_size, 77, 768)   # Null embedding

# ç•°ãªã‚‹guidance scaleã§ã®æ¯”è¼ƒ
scales = [1.0, 5.0, 7.5, 10.0, 15.0]

print("Guidance Scale Effects:\n")
print(f"{'Scale':<10} {'Effect':<50}")
print("-" * 60)

for scale in scales:
    guided = classifier_free_guidance(model, x, t, text_emb, null_emb, scale)

    if scale == 1.0:
        effect = "æ¡ä»¶ãªã—ï¼ˆç„¡æ¡ä»¶äºˆæ¸¬ã¨åŒã˜ï¼‰"
    elif scale < 7.5:
        effect = "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¸ã®å¿ å®Ÿåº¦: ä½ã€œä¸­"
    elif scale == 7.5:
        effect = "æ¨å¥¨å€¤ï¼šå“è³ªã¨å¤šæ§˜æ€§ã®ãƒãƒ©ãƒ³ã‚¹"
    elif scale <= 10.0:
        effect = "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¸ã®å¿ å®Ÿåº¦: é«˜"
    else:
        effect = "éåº¦ã«å¼·èª¿ï¼ˆã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆç™ºç”Ÿã®å¯èƒ½æ€§ï¼‰"

    print(f"{scale:<10.1f} {effect:<50}")

print("\nâœ“ CFGã®ä»•çµ„ã¿:")
print("  - w=1.0: ç„¡æ¡ä»¶ç”Ÿæˆ")
print("  - w>1.0: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¸ã®å¿ å®Ÿåº¦å¢—åŠ ")
print("  - w=7.5: é€šå¸¸ã®æ¨å¥¨å€¤")
print("  - w>15: éé£½å’Œãƒ»ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã®ãƒªã‚¹ã‚¯")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Classifier-Free Guidance Demo ===

Guidance Scale Effects:

Scale      Effect
------------------------------------------------------------
1.0        æ¡ä»¶ãªã—ï¼ˆç„¡æ¡ä»¶äºˆæ¸¬ã¨åŒã˜ï¼‰
5.0        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¸ã®å¿ å®Ÿåº¦: ä½ã€œä¸­
7.5        æ¨å¥¨å€¤ï¼šå“è³ªã¨å¤šæ§˜æ€§ã®ãƒãƒ©ãƒ³ã‚¹
10.0       ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¸ã®å¿ å®Ÿåº¦: é«˜
15.0       éåº¦ã«å¼·èª¿ï¼ˆã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆç™ºç”Ÿã®å¯èƒ½æ€§ï¼‰

âœ“ CFGã®ä»•çµ„ã¿:
  - w=1.0: ç„¡æ¡ä»¶ç”Ÿæˆ
  - w>1.0: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¸ã®å¿ å®Ÿåº¦å¢—åŠ 
  - w=7.5: é€šå¸¸ã®æ¨å¥¨å€¤
  - w>15: éé£½å’Œãƒ»ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã®ãƒªã‚¹ã‚¯
</code></pre>

<hr>

<h2>4.6 å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ</h2>

<h3>4.6.1 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ1: CIFAR-10ã§ã®ç”»åƒç”Ÿæˆ</h3>

<div class="project-box">
<h4>ç›®æ¨™</h4>
<p>CIFAR-10ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§DDPMã‚’è¨“ç·´ã—ã€10ã‚¯ãƒ©ã‚¹ã®ç”»åƒã‚’ç”Ÿæˆã—ã¾ã™ã€‚</p>

<h4>å®Ÿè£…è¦ä»¶</h4>
<ul>
<li>CIFAR-10ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æ§‹ç¯‰</li>
<li>U-Net Denoiserã®è¨“ç·´ï¼ˆ20-50ã‚¨ãƒãƒƒã‚¯ï¼‰</li>
<li>DDIMé«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®å®Ÿè£…</li>
<li>FIDã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹å“è³ªè©•ä¾¡</li>
</ul>
</div>

<pre><code class="language-python">import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

print("=== CIFAR-10 Diffusion Project ===\n")

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # [-1, 1]ã«æ­£è¦åŒ–
])

trainset = torchvision.datasets.CIFAR10(
    root='./data',
    train=True,
    download=True,
    transform=transform
)

trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)

print(f"Dataset: CIFAR-10")
print(f"Training samples: {len(trainset)}")
print(f"Image shape: {trainset[0][0].shape}")
print(f"Classes: {trainset.classes}")

# ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
model = SimpleUNet(in_channels=3, out_channels=3, time_emb_dim=256, base_channels=128)
diffusion = DDPMDiffusion(timesteps=1000, schedule='cosine')

print(f"\nModel parameters: {sum(p.numel() for p in model.parameters()):,}")

# è¨“ç·´è¨­å®š
print("\nTraining Configuration:")
print("  â€¢ Epochs: 50")
print("  â€¢ Batch size: 128")
print("  â€¢ Optimizer: AdamW (lr=2e-4)")
print("  â€¢ Scheduler: Cosine")
print("  â€¢ Device: GPU (æ¨å¥¨)")

print("\nè¨“ç·´æ‰‹é †:")
print("  1. python train_cifar10_ddpm.py --epochs 50 --batch-size 128")
print("  2. è¨“ç·´å®Œäº†å¾Œã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜")
print("  3. FIDã‚¹ã‚³ã‚¢ã§è©•ä¾¡")

print("\nã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°:")
print("  â€¢ DDIM 50 steps ã§é«˜é€Ÿç”Ÿæˆ")
print("  â€¢ ç”Ÿæˆç”»åƒã‚’ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤º")
print("  â€¢ ã‚¯ãƒ©ã‚¹åˆ¥ç”Ÿæˆã‚‚å¯èƒ½ï¼ˆæ¡ä»¶ä»˜ããƒ¢ãƒ‡ãƒ«ã®å ´åˆï¼‰")
</code></pre>

<h3>4.6.2 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ2: Stable Diffusionã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º</h3>

<div class="project-box">
<h4>ç›®æ¨™</h4>
<p>Stable Diffusionã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€ç‰¹å®šã‚¹ã‚¿ã‚¤ãƒ«ã®ç”»åƒã‚’ç”Ÿæˆã—ã¾ã™ã€‚</p>

<h4>å®Ÿè£…è¦ä»¶</h4>
<ul>
<li>DreamBoothã¾ãŸã¯Textual Inversionã®å®Ÿè£…</li>
<li>ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ï¼ˆ10-20æšï¼‰</li>
<li>LoRAã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</li>
<li>ç”Ÿæˆå“è³ªã®è©•ä¾¡ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</li>
</ul>
</div>

<pre><code class="language-python">print("=== Stable Diffusion Fine-tuning Project ===\n")

fine_tuning_code = '''
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
from diffusers.loaders import AttnProcsLayers
from diffusers.models.attention_processor import LoRAAttnProcessor
import torch

# 1. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
model_id = "stabilityai/stable-diffusion-2-1"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)

# 2. LoRAã®è¨­å®š
lora_attn_procs = {}
for name in pipe.unet.attn_processors.keys():
    lora_attn_procs[name] = LoRAAttnProcessor(hidden_size=..., rank=4)

pipe.unet.set_attn_processor(lora_attn_procs)

# 3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
# - ç‰¹å®šã‚¹ã‚¿ã‚¤ãƒ«/ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ç”»åƒ10-20æš
# - ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ä»˜ã

# 4. è¨“ç·´
# - LoRAãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿æ›´æ–°ï¼ˆåŠ¹ç‡çš„ï¼‰
# - æ•°ç™¾ã€œæ•°åƒã‚¹ãƒ†ãƒƒãƒ—

# 5. ç”Ÿæˆ
pipe = pipe.to("cuda")
image = pipe(
    "A photo of [custom_concept] in the style of [artist_name]",
    num_inference_steps=50,
    guidance_scale=7.5
).images[0]
'''

print("ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•:")
print("\n1. DreamBooth:")
print("   â€¢ å°‘æ•°ç”»åƒï¼ˆ3-5æšï¼‰ã§ç‰¹å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå­¦ç¿’")
print("   â€¢ 'A photo of [V]' å½¢å¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
print("   â€¢ è¨“ç·´æ™‚é–“: 1-2æ™‚é–“ï¼ˆGPUï¼‰")

print("\n2. Textual Inversion:")
print("   â€¢ æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿ã‚’å­¦ç¿’")
print("   â€¢ ãƒ¢ãƒ‡ãƒ«æœ¬ä½“ã¯å¤‰æ›´ã—ãªã„")
print("   â€¢ è»½é‡ãƒ»é«˜é€Ÿ")

print("\n3. LoRA (Low-Rank Adaptation):")
print("   â€¢ ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã§ã‚¢ãƒ€ãƒ—ã‚¿è¿½åŠ ")
print("   â€¢ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›ï¼ˆå…ƒã®1-10%ï¼‰")
print("   â€¢ è¤‡æ•°LoRAã®çµ„ã¿åˆã‚ã›å¯èƒ½")

print("\nã‚³ãƒ¼ãƒ‰ä¾‹:")
print(fine_tuning_code)

print("\næ¨å¥¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼:")
print("  1. ãƒ‡ãƒ¼ã‚¿æº–å‚™: é«˜å“è³ªç”»åƒ10-20æš + ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³")
print("  2. LoRAè¨“ç·´: rank=4-8ã€lr=1e-4ã€500-2000 steps")
print("  3. è©•ä¾¡: ç”Ÿæˆå“è³ªãƒã‚§ãƒƒã‚¯")
print("  4. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°: æœ€é©ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¢ç´¢")
</code></pre>

<hr>

<h2>4.7 ã¾ã¨ã‚ã¨ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯</h2>

<h3>æœ¬ç« ã§å­¦ã‚“ã ã“ã¨</h3>

<table>
<thead>
<tr>
<th>ãƒˆãƒ”ãƒƒã‚¯</th>
<th>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«åŸºç¤</strong></td>
<td>Forward/Reverse Processã€ãƒã‚¤ã‚ºé™¤å»ç”Ÿæˆ</td>
</tr>
<tr>
<td><strong>DDPM</strong></td>
<td>æ•°å­¦çš„å®šå¼åŒ–ã€ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã€è¨“ç·´ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </td>
</tr>
<tr>
<td><strong>U-Net Denoiser</strong></td>
<td>æ™‚é–“åŸ‹ã‚è¾¼ã¿ã€æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯ã€ã‚¹ã‚­ãƒƒãƒ—æ¥ç¶š</td>
</tr>
<tr>
<td><strong>é«˜é€ŸåŒ–</strong></td>
<td>DDIMã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—å‰Šæ¸›</td>
</tr>
<tr>
<td><strong>Stable Diffusion</strong></td>
<td>Latent Diffusionã€CLIP Guidanceã€CFG</td>
</tr>
</tbody>
</table>

<h3>ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯</h3>

<details>
<summary><strong>Improved DDPM</strong></summary>
<p>ã‚³ã‚µã‚¤ãƒ³ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã€å­¦ç¿’å¯èƒ½ãªåˆ†æ•£ã€V-predictionãªã©ã€DDPMã®æ”¹è‰¯æ‰‹æ³•ã€‚ç”Ÿæˆå“è³ªã¨è¨“ç·´å®‰å®šæ€§ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚</p>
</details>

<details>
<summary><strong>Consistency Models</strong></summary>
<p>1ã‚¹ãƒ†ãƒƒãƒ—ã§ç”Ÿæˆå¯èƒ½ãªæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã€‚è¨“ç·´æ™‚ã¯å¤šã‚¹ãƒ†ãƒƒãƒ—ã ãŒã€æ¨è«–æ™‚ã¯å¤§å¹…ã«é«˜é€ŸåŒ–ã€‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆã¸ã®é“ã€‚</p>
</details>

<details>
<summary><strong>ControlNet</strong></summary>
<p>Stable Diffusionã«æ§‹é€ åˆ¶å¾¡ã‚’è¿½åŠ ã€‚ã‚¨ãƒƒã‚¸ã€æ·±åº¦ã€ãƒãƒ¼ã‚ºãªã©ã®æ¡ä»¶ã§ã‚ˆã‚Šç´°ã‹ã„åˆ¶å¾¡ãŒå¯èƒ½ã€‚</p>
</details>

<details>
<summary><strong>SDXL (Stable Diffusion XL)</strong></summary>
<p>ã‚ˆã‚Šå¤§è¦æ¨¡ãªU-Netã€è¤‡æ•°è§£åƒåº¦è¨“ç·´ã€Refinerãƒ¢ãƒ‡ãƒ«ã€‚1024Ã—1024ã®é«˜è§£åƒåº¦ç”Ÿæˆã€‚</p>
</details>

<details>
<summary><strong>Video Diffusion Models</strong></summary>
<p>å‹•ç”»ç”Ÿæˆã¸ã®æ‹¡å¼µã€‚æ™‚é–“çš„ä¸€è²«æ€§ã®å­¦ç¿’ã€3D U-Netã€ãƒ†ã‚­ã‚¹ãƒˆtoå‹•ç”»ç”Ÿæˆã€‚</p>
</details>

<h3>æ¼”ç¿’å•é¡Œ</h3>

<div class="project-box">
<h4>æ¼”ç¿’ 4.1: ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æ¯”è¼ƒ</h4>
<p><strong>èª²é¡Œ</strong>: Linearã€Cosineã€Quadraticã®3ã¤ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã§è¨“ç·´ã—ã€FIDã‚¹ã‚³ã‚¢ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>è©•ä¾¡æŒ‡æ¨™</strong>: FIDã€ISï¼ˆInception Scoreï¼‰ã€ç”Ÿæˆæ™‚é–“</p>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.2: DDIMã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æœ€é©åŒ–</h4>
<p><strong>èª²é¡Œ</strong>: DDIMã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆ10, 20, 50, 100ï¼‰ã‚’å¤‰åŒ–ã•ã›ã¦ã€å“è³ªã¨é€Ÿåº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’èª¿æŸ»ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>åˆ†æé …ç›®</strong>: ç”Ÿæˆæ™‚é–“ã€ç”»åƒå“è³ªï¼ˆä¸»è¦³è©•ä¾¡ + LPIPSè·é›¢ï¼‰</p>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.3: æ¡ä»¶ä»˜ãæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«</h4>
<p><strong>èª²é¡Œ</strong>: CIFAR-10ã§ã‚¯ãƒ©ã‚¹æ¡ä»¶ä»˜ãDDPMã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>å®Ÿè£…å†…å®¹</strong>:</p>
<ul>
<li>ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã®åŸ‹ã‚è¾¼ã¿</li>
<li>æ¡ä»¶ä»˜ãU-Net</li>
<li>ç‰¹å®šã‚¯ãƒ©ã‚¹ã®ç”Ÿæˆ</li>
</ul>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.4: Latent Diffusionã®å®Ÿè£…</h4>
<p><strong>èª²é¡Œ</strong>: VAEã§ç”»åƒã‚’åœ§ç¸®ã—ã€æ½œåœ¨ç©ºé–“ã§DDPMã‚’è¨“ç·´ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>æ‰‹é †</strong>:</p>
<ul>
<li>VAEã®äº‹å‰è¨“ç·´ï¼ˆã¾ãŸã¯æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ï¼‰</li>
<li>æ½œåœ¨ç©ºé–“ã§ã®Diffusionè¨“ç·´</li>
<li>VAE Decoderã§ç”»åƒå¾©å…ƒ</li>
</ul>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.5: Stable Diffusionã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</h4>
<p><strong>èª²é¡Œ</strong>: åŒã˜ã‚³ãƒ³ã‚»ãƒ—ãƒˆã§ç•°ãªã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è©¦ã—ã€æœ€é©ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¦‹ã¤ã‘ã¦ãã ã•ã„ã€‚</p>
<p><strong>å®Ÿé¨“è¦ç´ </strong>:</p>
<ul>
<li>è©³ç´°åº¦ï¼ˆã‚·ãƒ³ãƒ—ãƒ« vs è©³ç´°ï¼‰</li>
<li>ã‚¹ã‚¿ã‚¤ãƒ«æŒ‡å®š</li>
<li>Negative prompt</li>
<li>Guidance scale</li>
</ul>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.6: FIDãƒ»ISè©•ä¾¡ã®å®Ÿè£…</h4>
<p><strong>èª²é¡Œ</strong>: ç”Ÿæˆç”»åƒã®å“è³ªè©•ä¾¡æŒ‡æ¨™ï¼ˆFIDã€Inception Scoreï¼‰ã‚’å®Ÿè£…ã—ã€è¨“ç·´éç¨‹ã‚’è¿½è·¡ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>å®Ÿè£…é …ç›®</strong>:</p>
<ul>
<li>Inception-v3ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨</li>
<li>ç‰¹å¾´æŠ½å‡ºã¨FIDè¨ˆç®—</li>
<li>è¨“ç·´æ›²ç·šã®å¯è¦–åŒ–</li>
</ul>
</div>

<hr>

<h3>æ¬¡ç« äºˆå‘Š</h3>

<p>ç¬¬5ç« ã§ã¯ã€<strong>Flow-Based Modelsï¼ˆæ­£è¦åŒ–æµãƒ¢ãƒ‡ãƒ«ï¼‰</strong>ã¨<strong>Score-Based Generative Models</strong>ã‚’å­¦ã³ã¾ã™ã€‚å¯é€†å¤‰æ›ã«ã‚ˆã‚‹å³å¯†ãªç¢ºç‡æ¨å®šã¨ã€ã‚¹ã‚³ã‚¢é–¢æ•°ã‚’ç”¨ã„ãŸç”Ÿæˆæ‰‹æ³•ã‚’æ¢ã‚Šã¾ã™ã€‚</p>

<blockquote>
<p><strong>æ¬¡ç« ã®ãƒˆãƒ”ãƒƒã‚¯</strong>:<br>
ãƒ»Normalizing Flowsã®ç†è«–<br>
ãƒ»RealNVPã€Glowã€MAFã®å®Ÿè£…<br>
ãƒ»å¤‰æ•°å¤‰æ›ã®å®šç†ã¨é›…å…‹æ¯”è¡Œåˆ—<br>
ãƒ»Score-Based Generative Models<br>
ãƒ»Langevin Dynamics<br>
ãƒ»æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¨ã®é–¢é€£æ€§<br>
ãƒ»å®Ÿè£…ï¼šFlow-basedãƒ¢ãƒ‡ãƒ«ã§ã®å¯†åº¦æ¨å®š</p>
</blockquote>

        <div class="navigation">
            <a href="chapter3-variational-autoencoders.html" class="nav-button">â† ç¬¬3ç« : å¤‰åˆ†ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€</a>
            <a href="chapter5-flow-models.html" class="nav-button">ç¬¬5ç« : Flow-Based Models â†’</a>
        </div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
        <p>ç¬¬4ç« ï¼šæ‹¡æ•£ãƒ¢ãƒ‡ãƒ« | ç”Ÿæˆãƒ¢ãƒ‡ãƒ«å…¥é–€ã‚·ãƒªãƒ¼ã‚º</p>
    </footer>

</body>
</html>
