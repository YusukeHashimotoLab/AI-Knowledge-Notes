<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬2ç« ï¼šç”»åƒåˆ†é¡ã¨ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚° - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/computer-vision-introduction/chapter2-image-classification.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/computer-vision-introduction/index.html">Computer Vision</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 2</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬2ç« ï¼šç”»åƒåˆ†é¡ã¨ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°</h1>
            <p class="subtitle">CNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨è»¢ç§»å­¦ç¿’ã«ã‚ˆã‚‹é«˜ç²¾åº¦ãªç”»åƒåˆ†é¡ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 30-35åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 10å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… LeNetã€AlexNetã€VGGã€ResNetãªã©ã®ä¸»è¦CNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç‰¹å¾´ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Inceptionã¨MobileNetã®åŠ¹ç‡çš„ãªè¨­è¨ˆåŸç†ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>âœ… EfficientNetã®Compound Scalingã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… è»¢ç§»å­¦ç¿’ã¨Fine-tuningã®é•ã„ã¨ä½¿ã„åˆ†ã‘ã‚’ç¿’å¾—ã™ã‚‹</li>
<li>âœ… torchvision.modelsã‚’ä½¿ã£ãŸäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ´»ç”¨æ–¹æ³•ã‚’å­¦ã¶</li>
<li>âœ… Data Augmentationã«ã‚ˆã‚‹æ±åŒ–æ€§èƒ½ã®å‘ä¸Šæ‰‹æ³•ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Learning Rate Schedulingã€TTAã€Model Ensembleãªã©ã®è¨“ç·´ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’æ´»ç”¨ã§ãã‚‹</li>
<li>âœ… å®Ÿè·µçš„ãªç”»åƒåˆ†é¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å®Œæˆã•ã›ã‚‰ã‚Œã‚‹</li>
</ul>

<hr>

<h2>2.1 CNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é€²åŒ–</h2>

<h3>ç”»åƒåˆ†é¡ã®æ­´å²çš„ç™ºå±•</h3>

<p>ç”»åƒåˆ†é¡ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®æœ€ã‚‚åŸºæœ¬çš„ã‹ã¤é‡è¦ãªã‚¿ã‚¹ã‚¯ã®ä¸€ã¤ã§ã™ã€‚ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®ç™»å ´ã«ã‚ˆã‚Šã€ç”»åƒåˆ†é¡ã®ç²¾åº¦ã¯é£›èºçš„ã«å‘ä¸Šã—ã¾ã—ãŸã€‚</p>

<div class="mermaid">
graph LR
    A[LeNet-5<br/>1998<br/>MNIST] --> B[AlexNet<br/>2012<br/>ImageNet]
    B --> C[VGG<br/>2014<br/>æ·±ã•19å±¤]
    C --> D[GoogLeNet<br/>2014<br/>Inception]
    D --> E[ResNet<br/>2015<br/>æ®‹å·®æ¥ç¶š]
    E --> F[Inception-v4<br/>2016<br/>Hybrid]
    F --> G[MobileNet<br/>2017<br/>è»½é‡åŒ–]
    G --> H[EfficientNet<br/>2019<br/>æœ€é©åŒ–]
    H --> I[Vision Transformer<br/>2020+<br/>Attention]

    style A fill:#e1f5ff
    style B fill:#b3e5fc
    style C fill:#81d4fa
    style D fill:#4fc3f7
    style E fill:#29b6f6
    style F fill:#03a9f4
    style G fill:#039be5
    style H fill:#0288d1
    style I fill:#0277bd
</div>

<h3>LeNet-5 (1998): CNNã®åŸç‚¹</h3>

<p><strong>LeNet-5</strong>ã¯ã€Yann LeCunãŒé–‹ç™ºã—ãŸæ‰‹æ›¸ãæ•°å­—èªè­˜ã®ãŸã‚ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã€ç¾ä»£ã®CNNã®åŸºç¤ã¨ãªã‚Šã¾ã—ãŸã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class LeNet5(nn.Module):
    """LeNet-5: æ‰‹æ›¸ãæ•°å­—èªè­˜ã®ãŸã‚ã®å¤å…¸çš„CNN"""
    def __init__(self, num_classes=10):
        super(LeNet5, self).__init__()

        # ç‰¹å¾´æŠ½å‡ºå±¤
        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)    # 28Ã—28 â†’ 24Ã—24
        self.pool1 = nn.AvgPool2d(kernel_size=2)       # 24Ã—24 â†’ 12Ã—12
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)   # 12Ã—12 â†’ 8Ã—8
        self.pool2 = nn.AvgPool2d(kernel_size=2)       # 8Ã—8 â†’ 4Ã—4

        # åˆ†é¡å±¤
        self.fc1 = nn.Linear(16 * 4 * 4, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, num_classes)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool1(x)
        x = F.relu(self.conv2(x))
        x = self.pool2(x)

        x = x.view(x.size(0), -1)  # å¹³å¦åŒ–

        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)

        return x

# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã¨ãƒ†ã‚¹ãƒˆ
model = LeNet5(num_classes=10)
x = torch.randn(1, 1, 28, 28)
output = model(x)

print(f"LeNet-5")
print(f"å…¥åŠ›: {x.shape} â†’ å‡ºåŠ›: {output.shape}")
print(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}")
</code></pre>

<h3>AlexNet (2012): ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°é©å‘½</h3>

<p><strong>AlexNet</strong>ã¯ã€2012å¹´ã®ImageNet Large Scale Visual Recognition Challenge (ILSVRC)ã§å„ªå‹ã—ã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ–ãƒ¼ãƒ ã®ç«ä»˜ã‘å½¹ã¨ãªã‚Šã¾ã—ãŸã€‚</p>

<p>ä¸»ãªé©æ–°ï¼š</p>
<ul>
<li><strong>ReLUæ´»æ€§åŒ–é–¢æ•°</strong>: Sigmoidã‚ˆã‚Šé«˜é€Ÿãªå­¦ç¿’</li>
<li><strong>Dropout</strong>: éå­¦ç¿’ã®é˜²æ­¢</li>
<li><strong>Data Augmentation</strong>: æ±åŒ–æ€§èƒ½ã®å‘ä¸Š</li>
<li><strong>GPUä¸¦åˆ—å‡¦ç†</strong>: å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚’å¯èƒ½ã«</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn

class AlexNet(nn.Module):
    """AlexNet: ImageNet 2012å„ªå‹ãƒ¢ãƒ‡ãƒ«"""
    def __init__(self, num_classes=1000):
        super(AlexNet, self).__init__()

        # ç‰¹å¾´æŠ½å‡ºå±¤
        self.features = nn.Sequential(
            # Conv1: 96 filters, 11Ã—11, stride=4
            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),

            # Conv2: 256 filters, 5Ã—5
            nn.Conv2d(96, 256, kernel_size=5, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),

            # Conv3: 384 filters, 3Ã—3
            nn.Conv2d(256, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),

            # Conv4: 384 filters, 3Ã—3
            nn.Conv2d(384, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),

            # Conv5: 256 filters, 3Ã—3
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
        )

        # åˆ†é¡å±¤
        self.classifier = nn.Sequential(
            nn.Dropout(p=0.5),
            nn.Linear(256 * 6 * 6, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

# ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®ç¢ºèª
model = AlexNet(num_classes=1000)
total_params = sum(p.numel() for p in model.parameters())
print(f"\nAlexNet ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ç´„{total_params * 4 / (1024**2):.1f} MB")
</code></pre>

<h3>VGGNet (2014): ã‚·ãƒ³ãƒ—ãƒ«ã•ã®ç¾å­¦</h3>

<p><strong>VGGNet</strong>ã¯ã€3Ã—3ã®å°ã•ãªãƒ•ã‚£ãƒ«ã‚¿ã‚’ç¹°ã‚Šè¿”ã—ä½¿ã†ã‚·ãƒ³ãƒ—ãƒ«ãªè¨­è¨ˆã«ã‚ˆã‚Šã€æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ‰åŠ¹æ€§ã‚’ç¤ºã—ã¾ã—ãŸã€‚</p>

<p>è¨­è¨ˆåŸå‰‡ï¼š</p>
<ul>
<li><strong>3Ã—3ãƒ•ã‚£ãƒ«ã‚¿ã®ã¿</strong>ä½¿ç”¨ï¼ˆå°ã•ã„ãƒ•ã‚£ãƒ«ã‚¿ã‚’é‡ã­ã‚‹æ–¹ãŒåŠ¹ç‡çš„ï¼‰</li>
<li><strong>2Ã—2 Max Pooling</strong>ã§æ®µéšçš„ã«ã‚µã‚¤ã‚ºã‚’åŠæ¸›</li>
<li><strong>ãƒãƒ£ãƒ³ãƒãƒ«æ•°ã‚’å€å¢—</strong>ï¼ˆ64 â†’ 128 â†’ 256 â†’ 512ï¼‰</li>
</ul>

<blockquote>
<p>ãªãœ3Ã—3ãƒ•ã‚£ãƒ«ã‚¿ã‚’2å›é‡ã­ã‚‹ã®ã‹ï¼Ÿ<br>
å—å®¹é‡: 5Ã—5ã¨åŒã˜<br>
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 3Ã—3Ã—2 = 18 < 5Ã—5 = 25<br>
éç·šå½¢æ€§: ReLUãŒ2å› â†’ ã‚ˆã‚Šå¼·ã„è¡¨ç¾åŠ›</p>
</blockquote>

<pre><code class="language-python">import torch
import torch.nn as nn

class VGGBlock(nn.Module):
    """VGGã®åŸºæœ¬ãƒ–ãƒ­ãƒƒã‚¯: Conv â†’ ReLU ã‚’ç¹°ã‚Šè¿”ã™"""
    def __init__(self, in_channels, out_channels, num_convs):
        super(VGGBlock, self).__init__()

        layers = []
        for i in range(num_convs):
            layers.append(nn.Conv2d(
                in_channels if i == 0 else out_channels,
                out_channels,
                kernel_size=3,
                padding=1
            ))
            layers.append(nn.ReLU(inplace=True))

        self.block = nn.Sequential(*layers)

    def forward(self, x):
        return self.block(x)

class VGG16(nn.Module):
    """VGG-16: 16å±¤ã®æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""
    def __init__(self, num_classes=1000):
        super(VGG16, self).__init__()

        # ç‰¹å¾´æŠ½å‡ºéƒ¨åˆ†
        self.features = nn.Sequential(
            VGGBlock(3, 64, 2),      # Block 1
            nn.MaxPool2d(2, 2),

            VGGBlock(64, 128, 2),    # Block 2
            nn.MaxPool2d(2, 2),

            VGGBlock(128, 256, 3),   # Block 3
            nn.MaxPool2d(2, 2),

            VGGBlock(256, 512, 3),   # Block 4
            nn.MaxPool2d(2, 2),

            VGGBlock(512, 512, 3),   # Block 5
            nn.MaxPool2d(2, 2),
        )

        # åˆ†é¡éƒ¨åˆ†
        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

# VGG-16ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç¢ºèª
model = VGG16(num_classes=1000)
x = torch.randn(1, 3, 224, 224)

print("VGG-16 å„å±¤ã®å‡ºåŠ›ã‚µã‚¤ã‚º:")
for name, module in model.features.named_children():
    x = module(x)
    if isinstance(module, (VGGBlock, nn.MaxPool2d)):
        print(f"  {name}: {x.shape}")
</code></pre>

<h3>ResNet (2015): æ®‹å·®æ¥ç¶šã®é©å‘½</h3>

<p><strong>ResNet</strong>ã¯ã€<strong>Skip Connectionsï¼ˆæ®‹å·®æ¥ç¶šï¼‰</strong>ã‚’å°å…¥ã—ã€éå¸¸ã«æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆ100å±¤ä»¥ä¸Šï¼‰ã®å­¦ç¿’ã‚’å¯èƒ½ã«ã—ã¾ã—ãŸã€‚</p>

<p>å•é¡Œï¼šãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ·±ãã™ã‚‹ã¨å‹¾é…æ¶ˆå¤±å•é¡ŒãŒç™ºç”Ÿ<br>
è§£æ±ºï¼šResidual Blockã«ã‚ˆã‚‹æ’ç­‰å†™åƒã®å­¦ç¿’</p>

$$
\mathbf{y} = F(\mathbf{x}) + \mathbf{x}
$$

<p>ã“ã“ã§ã€$F(\mathbf{x})$ã¯æ®‹å·®é–¢æ•°ã€$\mathbf{x}$ã¯ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆæ¥ç¶šã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class ResidualBlock(nn.Module):
    """ResNetã®åŸºæœ¬ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆResidual Blockï¼‰"""
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()

        # Main path
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,
                               stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)

        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        # Shortcut path
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1,
                         stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        identity = x

        # Main path
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))

        # Shortcut connection
        out += self.shortcut(identity)
        out = F.relu(out)

        return out

class ResNet18(nn.Module):
    """ResNet-18: 18å±¤ã®æ®‹å·®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""
    def __init__(self, num_classes=1000):
        super(ResNet18, self).__init__()

        # åˆæœŸå±¤
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        # Residual blocks
        self.layer1 = self._make_layer(64, 64, num_blocks=2, stride=1)
        self.layer2 = self._make_layer(64, 128, num_blocks=2, stride=2)
        self.layer3 = self._make_layer(128, 256, num_blocks=2, stride=2)
        self.layer4 = self._make_layer(256, 512, num_blocks=2, stride=2)

        # åˆ†é¡å±¤
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)

    def _make_layer(self, in_channels, out_channels, num_blocks, stride):
        layers = []
        layers.append(ResidualBlock(in_channels, out_channels, stride))
        for _ in range(1, num_blocks):
            layers.append(ResidualBlock(out_channels, out_channels, 1))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        return x

# ResNet-18ã®æ§‹é€ ç¢ºèª
model = ResNet18(num_classes=1000)
print(f"ResNet-18 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}")

# Skip Connectionã®åŠ¹æœã‚’ç¢ºèª
x = torch.randn(1, 3, 224, 224)
output = model(x)
print(f"å…¥åŠ›: {x.shape} â†’ å‡ºåŠ›: {output.shape}")
</code></pre>

<h3>Inception (GoogLeNet, 2014): åŠ¹ç‡çš„ãªãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ç‰¹å¾´æŠ½å‡º</h3>

<p><strong>Inception</strong>ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€ç•°ãªã‚‹ã‚µã‚¤ã‚ºã®ãƒ•ã‚£ãƒ«ã‚¿ã‚’ä¸¦åˆ—ã«é©ç”¨ã—ã€ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãªç‰¹å¾´ã‚’åŠ¹ç‡çš„ã«æŠ½å‡ºã—ã¾ã™ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn

class InceptionModule(nn.Module):
    """Inception Module: è¤‡æ•°ã®ãƒ•ã‚£ãƒ«ã‚¿ã‚µã‚¤ã‚ºã‚’ä¸¦åˆ—å‡¦ç†"""
    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):
        super(InceptionModule, self).__init__()

        # 1x1 convolution branch
        self.branch1 = nn.Sequential(
            nn.Conv2d(in_channels, ch1x1, kernel_size=1),
            nn.ReLU(inplace=True)
        )

        # 1x1 â†’ 3x3 convolution branch
        self.branch2 = nn.Sequential(
            nn.Conv2d(in_channels, ch3x3red, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(ch3x3red, ch3x3, kernel_size=3, padding=1),
            nn.ReLU(inplace=True)
        )

        # 1x1 â†’ 5x5 convolution branch
        self.branch3 = nn.Sequential(
            nn.Conv2d(in_channels, ch5x5red, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(ch5x5red, ch5x5, kernel_size=5, padding=2),
            nn.ReLU(inplace=True)
        )

        # 3x3 pooling â†’ 1x1 convolution branch
        self.branch4 = nn.Sequential(
            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),
            nn.Conv2d(in_channels, pool_proj, kernel_size=1),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        branch1 = self.branch1(x)
        branch2 = self.branch2(x)
        branch3 = self.branch3(x)
        branch4 = self.branch4(x)

        # Concatenate along channel dimension
        outputs = torch.cat([branch1, branch2, branch3, branch4], dim=1)
        return outputs

# Inception Moduleã®ãƒ†ã‚¹ãƒˆ
x = torch.randn(1, 256, 28, 28)
inception = InceptionModule(256, ch1x1=64, ch3x3red=96, ch3x3=128,
                            ch5x5red=16, ch5x5=32, pool_proj=32)
output = inception(x)

print(f"Inception Module")
print(f"å…¥åŠ›: {x.shape}")
print(f"å‡ºåŠ›: {output.shape}")
print(f"å‡ºåŠ›ãƒãƒ£ãƒ³ãƒãƒ«æ•°: {64 + 128 + 32 + 32} = {output.size(1)}")
</code></pre>

<h3>MobileNet (2017): è»½é‡åŒ–ã¨ãƒ¢ãƒã‚¤ãƒ«æœ€é©åŒ–</h3>

<p><strong>MobileNet</strong>ã¯ã€<strong>Depthwise Separable Convolution</strong>ã‚’ä½¿ç”¨ã—ã€è¨ˆç®—é‡ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã¾ã™ã€‚</p>

<p>æ¨™æº–çš„ãªç•³ã¿è¾¼ã¿ï¼š$D_K \times D_K \times M \times N$ã®ã‚³ã‚¹ãƒˆ<br>
Depthwise Separable: $D_K \times D_K \times M + M \times N$ã®ã‚³ã‚¹ãƒˆ<br>
å‰Šæ¸›ç‡ï¼šç´„8ã€œ9å€</p>

<pre><code class="language-python">import torch
import torch.nn as nn

class DepthwiseSeparableConv(nn.Module):
    """Depthwise Separable Convolution"""
    def __init__(self, in_channels, out_channels, stride=1):
        super(DepthwiseSeparableConv, self).__init__()

        # Depthwise: å„ãƒãƒ£ãƒ³ãƒãƒ«ã”ã¨ã«ç•³ã¿è¾¼ã¿
        self.depthwise = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=3,
                     stride=stride, padding=1, groups=in_channels, bias=False),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True)
        )

        # Pointwise: 1x1ç•³ã¿è¾¼ã¿ã§ãƒãƒ£ãƒ³ãƒãƒ«æ–¹å‘ã®çµåˆ
        self.pointwise = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        x = self.depthwise(x)
        x = self.pointwise(x)
        return x

# è¨ˆç®—é‡ã®æ¯”è¼ƒ
def count_operations(in_channels, out_channels, kernel_size, input_size):
    # æ¨™æº–çš„ãªç•³ã¿è¾¼ã¿
    standard_ops = kernel_size * kernel_size * in_channels * out_channels * input_size * input_size

    # Depthwise Separable
    depthwise_ops = kernel_size * kernel_size * in_channels * input_size * input_size
    pointwise_ops = in_channels * out_channels * input_size * input_size
    separable_ops = depthwise_ops + pointwise_ops

    reduction = standard_ops / separable_ops

    return standard_ops, separable_ops, reduction

standard, separable, reduction = count_operations(128, 256, 3, 56)
print(f"è¨ˆç®—é‡ã®æ¯”è¼ƒï¼ˆ128â†’256ãƒãƒ£ãƒ³ãƒãƒ«ã€3Ã—3ãƒ•ã‚£ãƒ«ã‚¿ã€56Ã—56å…¥åŠ›ï¼‰:")
print(f"  æ¨™æº–çš„ãªç•³ã¿è¾¼ã¿: {standard:,} operations")
print(f"  Depthwise Separable: {separable:,} operations")
print(f"  å‰Šæ¸›ç‡: {reduction:.2f}x")
</code></pre>

<h3>EfficientNet (2019): æœ€é©ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°</h3>

<p><strong>EfficientNet</strong>ã¯ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ·±ã•ãƒ»å¹…ãƒ»è§£åƒåº¦ã‚’ãƒãƒ©ãƒ³ã‚¹ã‚ˆãã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹<strong>Compound Scaling</strong>ã‚’ææ¡ˆã—ã¾ã—ãŸã€‚</p>

<p>Compound Scaling:</p>
$$
\text{depth} = \alpha^\phi, \quad \text{width} = \beta^\phi, \quad \text{resolution} = \gamma^\phi
$$

<p>åˆ¶ç´„æ¡ä»¶: $\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2$, $\alpha \geq 1, \beta \geq 1, \gamma \geq 1$</p>

<table>
<thead>
<tr>
<th>ãƒ¢ãƒ‡ãƒ«</th>
<th>Top-1ç²¾åº¦</th>
<th>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°</th>
<th>FLOPs</th>
</tr>
</thead>
<tbody>
<tr>
<td>EfficientNet-B0</td>
<td>77.1%</td>
<td>5.3M</td>
<td>0.39B</td>
</tr>
<tr>
<td>EfficientNet-B1</td>
<td>79.1%</td>
<td>7.8M</td>
<td>0.70B</td>
</tr>
<tr>
<td>EfficientNet-B7</td>
<td>84.4%</td>
<td>66M</td>
<td>37B</td>
</tr>
<tr>
<td>ResNet-50</td>
<td>76.0%</td>
<td>26M</td>
<td>4.1B</td>
</tr>
</tbody>
</table>

<h3>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¯”è¼ƒã¾ã¨ã‚</h3>

<table>
<thead>
<tr>
<th>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</th>
<th>ä¸»ãªç‰¹å¾´</th>
<th>åˆ©ç‚¹</th>
<th>æ¬ ç‚¹</th>
</tr>
</thead>
<tbody>
<tr>
<td>LeNet-5</td>
<td>åŸºæœ¬çš„ãªCNN</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ã€ç†è§£ã—ã‚„ã™ã„</td>
<td>ç¾ä»£çš„ã‚¿ã‚¹ã‚¯ã«ã¯æ€§èƒ½ä¸è¶³</td>
</tr>
<tr>
<td>AlexNet</td>
<td>ReLUã€Dropout</td>
<td>å®Ÿç”¨çš„ãªæ€§èƒ½</td>
<td>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¤šã„</td>
</tr>
<tr>
<td>VGG</td>
<td>3Ã—3ãƒ•ã‚£ãƒ«ã‚¿ã®ç¹°ã‚Šè¿”ã—</td>
<td>æ§‹é€ ãŒå˜ç´”</td>
<td>éå¸¸ã«é‡ã„</td>
</tr>
<tr>
<td>ResNet</td>
<td>Skip Connections</td>
<td>æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒå¯èƒ½</td>
<td>ãƒ¡ãƒ¢ãƒªæ¶ˆè²»å¤§</td>
</tr>
<tr>
<td>Inception</td>
<td>ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ä¸¦åˆ—å‡¦ç†</td>
<td>åŠ¹ç‡çš„ãªç‰¹å¾´æŠ½å‡º</td>
<td>è¤‡é›‘ãªæ§‹é€ </td>
</tr>
<tr>
<td>MobileNet</td>
<td>Depthwise Separable Conv</td>
<td>è»½é‡ã€é«˜é€Ÿ</td>
<td>ç²¾åº¦ãŒã‚„ã‚„ä½ã„</td>
</tr>
<tr>
<td>EfficientNet</td>
<td>Compound Scaling</td>
<td>æœ€é«˜ã®åŠ¹ç‡æ€§</td>
<td>è¨“ç·´ã«æ™‚é–“ãŒã‹ã‹ã‚‹</td>
</tr>
</tbody>
</table>

<hr>

<h2>2.2 è»¢ç§»å­¦ç¿’ã¨Fine-tuning</h2>

<h3>è»¢ç§»å­¦ç¿’ã¨ã¯</h3>

<p><strong>è»¢ç§»å­¦ç¿’ï¼ˆTransfer Learningï¼‰</strong>ã¯ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆImageNetãªã©ï¼‰ã§äº‹å‰å­¦ç¿’ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ã€åˆ¥ã®ã‚¿ã‚¹ã‚¯ã«é©ç”¨ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<p>ãƒ¡ãƒªãƒƒãƒˆï¼š</p>
<ul>
<li><strong>å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§é«˜ç²¾åº¦</strong>: æ•°ç™¾ã€œæ•°åƒæšã®ç”»åƒã§å®Ÿç”¨çš„ãªæ€§èƒ½</li>
<li><strong>è¨“ç·´æ™‚é–“ã®çŸ­ç¸®</strong>: ã‚¼ãƒ­ã‹ã‚‰è¨“ç·´ã™ã‚‹ã‚ˆã‚Šå¤§å¹…ã«é«˜é€Ÿ</li>
<li><strong>æ±åŒ–æ€§èƒ½ã®å‘ä¸Š</strong>: äº‹å‰å­¦ç¿’ã§å¾—ãŸè±Šå¯Œãªç‰¹å¾´è¡¨ç¾ã‚’æ´»ç”¨</li>
</ul>

<div class="mermaid">
graph LR
    A[ImageNetã§<br/>äº‹å‰å­¦ç¿’] --> B[é‡ã¿ã‚’<br/>èª­ã¿è¾¼ã¿]
    B --> C{è»¢ç§»å­¦ç¿’ã®<br/>æˆ¦ç•¥}
    C --> D[Feature Extraction<br/>ç•³ã¿è¾¼ã¿å±¤ã‚’å‡çµ]
    C --> E[Fine-tuning<br/>å…¨ä½“ã¾ãŸã¯ä¸€éƒ¨ã‚’å†è¨“ç·´]

    D --> F[æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã§<br/>é«˜ç²¾åº¦]
    E --> F

    style A fill:#e1f5ff
    style C fill:#fff9c4
    style D fill:#c8e6c9
    style E fill:#ffccbc
    style F fill:#b3e5fc
</div>

<h3>Feature Extraction vs Fine-tuning</h3>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>ç•³ã¿è¾¼ã¿å±¤</th>
<th>åˆ†é¡å±¤</th>
<th>ãƒ‡ãƒ¼ã‚¿é‡</th>
<th>é¡ä¼¼åº¦</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Feature Extraction</strong></td>
<td>å‡çµ</td>
<td>è¨“ç·´</td>
<td>å°‘ï¼ˆ100ã€œ1000æšï¼‰</td>
<td>é«˜</td>
</tr>
<tr>
<td><strong>Fine-tuningï¼ˆå…¨å±¤ï¼‰</strong></td>
<td>è¨“ç·´</td>
<td>è¨“ç·´</td>
<td>å¤šï¼ˆ10000æšä»¥ä¸Šï¼‰</td>
<td>ä½</td>
</tr>
<tr>
<td><strong>Fine-tuningï¼ˆéƒ¨åˆ†ï¼‰</strong></td>
<td>ä¸Šå±¤ã®ã¿è¨“ç·´</td>
<td>è¨“ç·´</td>
<td>ä¸­ï¼ˆ1000ã€œ10000æšï¼‰</td>
<td>ä¸­</td>
</tr>
</tbody>
</table>

<h3>torchvision.modelsã«ã‚ˆã‚‹äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ´»ç”¨</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
from torchvision import models

# 1. äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
print("=== äº‹å‰å­¦ç¿’æ¸ˆã¿ResNet-18ã®ãƒ­ãƒ¼ãƒ‰ ===")
model = models.resnet18(pretrained=True)

# ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®ç¢ºèª
print(f"\nã‚ªãƒªã‚¸ãƒŠãƒ«ã®åˆ†é¡å±¤:")
print(model.fc)

# 2. Feature Extraction: ç•³ã¿è¾¼ã¿å±¤ã‚’å‡çµ
print("\n=== Feature Extractionï¼ˆç‰¹å¾´æŠ½å‡ºï¼‰ ===")

# ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‡çµ
for param in model.parameters():
    param.requires_grad = False

# æœ€å¾Œã®åˆ†é¡å±¤ã‚’ç½®ãæ›ãˆï¼ˆæ–°ã—ã„ã‚¿ã‚¹ã‚¯ç”¨ï¼‰
num_classes = 10  # CIFAR-10
model.fc = nn.Linear(model.fc.in_features, num_classes)

# è¨“ç·´å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¢ºèª
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())

print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
print(f"è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}")
print(f"å‡çµãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params - trainable_params:,}")

# 3. Fine-tuning: ä¸Šä½å±¤ã®ã¿è¨“ç·´
print("\n=== Fine-tuningï¼ˆéƒ¨åˆ†çš„ãªå†è¨“ç·´ï¼‰ ===")

# æ–°ã—ããƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
model_ft = models.resnet18(pretrained=True)

# ã™ã¹ã¦ã‚’ä¸€æ—¦å‡çµ
for param in model_ft.parameters():
    param.requires_grad = False

# æœ€å¾Œã®2ã¤ã®ãƒ–ãƒ­ãƒƒã‚¯ã¨åˆ†é¡å±¤ã‚’è§£å‡
for param in model_ft.layer4.parameters():
    param.requires_grad = True

model_ft.fc = nn.Linear(model_ft.fc.in_features, num_classes)

trainable_ft = sum(p.numel() for p in model_ft.parameters() if p.requires_grad)
print(f"Fine-tuningè¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_ft:,}")

# 4. å­¦ç¿’ç‡ã®è¨­å®šï¼ˆå±¤ã”ã¨ã«ç•°ãªã‚‹å­¦ç¿’ç‡ï¼‰
print("\n=== å±¤ã”ã¨ã®å­¦ç¿’ç‡è¨­å®š ===")
optimizer = torch.optim.Adam([
    {'params': model_ft.layer4.parameters(), 'lr': 1e-4},  # ä¸Šä½å±¤: å°ã•ã„å­¦ç¿’ç‡
    {'params': model_ft.fc.parameters(), 'lr': 1e-3}       # åˆ†é¡å±¤: å¤§ãã„å­¦ç¿’ç‡
])

print("å±¤ã”ã¨ã®å­¦ç¿’ç‡:")
for i, param_group in enumerate(optimizer.param_groups):
    print(f"  ã‚°ãƒ«ãƒ¼ãƒ— {i}: lr = {param_group['lr']}")
</code></pre>

<h3>å®Ÿè·µï¼šCIFAR-10ã§ã®è»¢ç§»å­¦ç¿’</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
transform_train = transforms.Compose([
    transforms.Resize(224),  # ResNetã®å…¥åŠ›ã‚µã‚¤ã‚ºã«åˆã‚ã›ã‚‹
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(224, padding=4),
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # ImageNetçµ±è¨ˆ
])

transform_test = transforms.Compose([
    transforms.Resize(224),
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])

# CIFAR-10ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
train_dataset = datasets.CIFAR10(root='./data', train=True,
                                 download=True, transform=transform_train)
test_dataset = datasets.CIFAR10(root='./data', train=False,
                                download=True, transform=transform_test)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)

# äº‹å‰å­¦ç¿’æ¸ˆã¿ResNet-18ã‚’ãƒ­ãƒ¼ãƒ‰
model = models.resnet18(pretrained=True)

# Feature Extraction: ç•³ã¿è¾¼ã¿å±¤ã‚’å‡çµ
for param in model.parameters():
    param.requires_grad = False

# åˆ†é¡å±¤ã‚’ç½®ãæ›ãˆ
num_classes = 10
model.fc = nn.Linear(model.fc.in_features, num_classes)
model = model.to(device)

# æå¤±é–¢æ•°ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)

# è¨“ç·´é–¢æ•°
def train_epoch(model, loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in loader:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    return running_loss / len(loader), 100. * correct / total

def test_epoch(model, loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    return running_loss / len(loader), 100. * correct / total

# è¨“ç·´å®Ÿè¡Œï¼ˆFeature Extractionï¼‰
print("\n=== Feature Extractionè¨“ç·´é–‹å§‹ ===")
num_epochs = 10

for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
    test_loss, test_acc = test_epoch(model, test_loader, criterion, device)

    if (epoch + 1) % 2 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}]")
        print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
        print(f"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%")

print("\nè¨“ç·´å®Œäº†ï¼")
</code></pre>

<h3>Fine-tuningæˆ¦ç•¥</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models

def create_finetuning_model(num_classes, freeze_layers='none'):
    """
    Fine-tuningãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ

    freeze_layers:
        - 'none': ã™ã¹ã¦è¨“ç·´
        - 'early': åˆæœŸå±¤ã®ã¿å‡çµ
        - 'most': æœ€çµ‚å±¤ã®ã¿è¨“ç·´
    """
    model = models.resnet18(pretrained=True)

    if freeze_layers == 'early':
        # åˆæœŸå±¤ï¼ˆlayer1, layer2ï¼‰ã‚’å‡çµ
        for param in model.conv1.parameters():
            param.requires_grad = False
        for param in model.bn1.parameters():
            param.requires_grad = False
        for param in model.layer1.parameters():
            param.requires_grad = False
        for param in model.layer2.parameters():
            param.requires_grad = False

        print("å‡çµ: conv1, bn1, layer1, layer2")
        print("è¨“ç·´: layer3, layer4, fc")

    elif freeze_layers == 'most':
        # ã»ã¨ã‚“ã©ã‚’å‡çµã€layer4ã¨fcã®ã¿è¨“ç·´
        for name, param in model.named_parameters():
            if 'layer4' not in name and 'fc' not in name:
                param.requires_grad = False

        print("å‡çµ: conv1ã€œlayer3")
        print("è¨“ç·´: layer4, fc")

    else:
        print("è¨“ç·´: ã™ã¹ã¦ã®å±¤")

    # åˆ†é¡å±¤ã‚’ç½®ãæ›ãˆ
    model.fc = nn.Linear(model.fc.in_features, num_classes)

    return model

# ç•°ãªã‚‹æˆ¦ç•¥ã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print("=== æˆ¦ç•¥1: ã™ã¹ã¦ã®å±¤ã‚’è¨“ç·´ ===")
model_all = create_finetuning_model(num_classes=10, freeze_layers='none')
trainable_all = sum(p.numel() for p in model_all.parameters() if p.requires_grad)
print(f"è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {trainable_all:,}\n")

print("=== æˆ¦ç•¥2: åˆæœŸå±¤ã‚’å‡çµ ===")
model_early = create_finetuning_model(num_classes=10, freeze_layers='early')
trainable_early = sum(p.numel() for p in model_early.parameters() if p.requires_grad)
print(f"è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {trainable_early:,}\n")

print("=== æˆ¦ç•¥3: æœ€çµ‚å±¤ã®ã¿è¨“ç·´ ===")
model_most = create_finetuning_model(num_classes=10, freeze_layers='most')
trainable_most = sum(p.numel() for p in model_most.parameters() if p.requires_grad)
print(f"è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {trainable_most:,}\n")

# å±¤ã”ã¨ã«ç•°ãªã‚‹å­¦ç¿’ç‡ã‚’è¨­å®š
def get_optimizer_with_layer_lr(model, base_lr=1e-3):
    """å±¤ã®æ·±ã•ã«å¿œã˜ã¦å­¦ç¿’ç‡ã‚’å¤‰ãˆã‚‹"""
    params = []

    # æµ…ã„å±¤: å°ã•ã„å­¦ç¿’ç‡
    params.append({'params': model.conv1.parameters(), 'lr': base_lr * 0.1})
    params.append({'params': model.layer1.parameters(), 'lr': base_lr * 0.2})
    params.append({'params': model.layer2.parameters(), 'lr': base_lr * 0.4})

    # æ·±ã„å±¤: å¤§ãã„å­¦ç¿’ç‡
    params.append({'params': model.layer3.parameters(), 'lr': base_lr * 0.7})
    params.append({'params': model.layer4.parameters(), 'lr': base_lr})

    # åˆ†é¡å±¤: æœ€ã‚‚å¤§ãã„å­¦ç¿’ç‡
    params.append({'params': model.fc.parameters(), 'lr': base_lr * 2})

    return optim.Adam(params)

optimizer = get_optimizer_with_layer_lr(model_all, base_lr=1e-3)
print("=== å±¤ã”ã¨ã®å­¦ç¿’ç‡ ===")
layer_names = ['conv1', 'layer1', 'layer2', 'layer3', 'layer4', 'fc']
for name, param_group in zip(layer_names, optimizer.param_groups):
    print(f"{name:10s}: lr = {param_group['lr']:.4f}")
</code></pre>

<hr>

<h2>2.3 Data Augmentationï¼ˆãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼‰</h2>

<h3>Data Augmentationã¨ã¯</h3>

<p><strong>Data Augmentation</strong>ã¯ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«æ§˜ã€…ãªå¤‰æ›ã‚’é©ç”¨ã—ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ‹¡å¼µã™ã‚‹æ‰‹æ³•ã§ã™ã€‚éå­¦ç¿’ã‚’é˜²ãã€æ±åŒ–æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚</p>

<h3>å¹¾ä½•å­¦çš„å¤‰æ›</h3>

<pre><code class="language-python">import torch
from torchvision import transforms
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np

# ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã®ç”Ÿæˆï¼ˆå®Ÿéš›ã«ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼‰
# ã“ã“ã§ã¯32Ã—32ã®ãƒ©ãƒ³ãƒ€ãƒ ç”»åƒã‚’ä½¿ç”¨
np.random.seed(42)
sample_image = Image.fromarray((np.random.rand(32, 32, 3) * 255).astype(np.uint8))

# å¹¾ä½•å­¦çš„å¤‰æ›ã®å®šç¾©
geometric_transforms = {
    'Original': transforms.ToTensor(),
    'Random Crop': transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.ToTensor()
    ]),
    'Horizontal Flip': transforms.Compose([
        transforms.RandomHorizontalFlip(p=1.0),
        transforms.ToTensor()
    ]),
    'Rotation': transforms.Compose([
        transforms.RandomRotation(degrees=15),
        transforms.ToTensor()
    ]),
    'Affine': transforms.Compose([
        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
        transforms.ToTensor()
    ]),
}

print("=== å¹¾ä½•å­¦çš„å¤‰æ›ã®ä¾‹ ===")
for name, transform in geometric_transforms.items():
    augmented = transform(sample_image)
    print(f"{name:20s}: {augmented.shape}")

# æ¨™æº–çš„ãªè¨“ç·´ç”¨Data Augmentation
standard_train_transform = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
])

print("\næ¨™æº–çš„ãªè¨“ç·´ç”¨å¤‰æ›:")
print(standard_train_transform)
</code></pre>

<h3>è‰²å¤‰æ›ï¼ˆColor Jitteringï¼‰</h3>

<pre><code class="language-python">from torchvision import transforms

# è‰²å¤‰æ›ã®ç¨®é¡
color_transforms = {
    'Color Jitter': transforms.ColorJitter(
        brightness=0.2,    # æ˜ã‚‹ã•ã‚’Â±20%å¤‰æ›´
        contrast=0.2,      # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆã‚’Â±20%å¤‰æ›´
        saturation=0.2,    # å½©åº¦ã‚’Â±20%å¤‰æ›´
        hue=0.1           # è‰²ç›¸ã‚’Â±10%å¤‰æ›´
    ),
    'Grayscale': transforms.RandomGrayscale(p=0.2),  # 20%ã®ç¢ºç‡ã§ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«åŒ–
    'Random Erasing': transforms.RandomErasing(
        p=0.5,             # 50%ã®ç¢ºç‡ã§é©ç”¨
        scale=(0.02, 0.33),  # æ¶ˆå»é ˜åŸŸã®ã‚µã‚¤ã‚º
        ratio=(0.3, 3.3)   # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”
    )
}

# å¼·åŠ›ãªData Augmentationï¼ˆCIFAR-10ç”¨ï¼‰
strong_augmentation = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    transforms.RandomErasing(p=0.5)
])

print("=== å¼·åŠ›ãªData Augmentation ===")
print(strong_augmentation)
</code></pre>

<h3>Mixup ã¨ CutMix</h3>

<p><strong>Mixup</strong>ã¨<strong>CutMix</strong>ã¯ã€2ã¤ã®ç”»åƒã‚’æ··ãœåˆã‚ã›ã¦æ–°ã—ã„è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹é«˜åº¦ãªæ‰‹æ³•ã§ã™ã€‚</p>

<pre><code class="language-python">import torch
import numpy as np

def mixup_data(x, y, alpha=1.0):
    """
    Mixup: 2ã¤ã®ç”»åƒã‚’ç·šå½¢è£œé–“

    Args:
        x: å…¥åŠ›ç”»åƒãƒãƒƒãƒ (B, C, H, W)
        y: ãƒ©ãƒ™ãƒ« (B,)
        alpha: Betaåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

    Returns:
        mixed_x, y_a, y_b, lam
    """
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size(0)
    index = torch.randperm(batch_size)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]

    return mixed_x, y_a, y_b, lam

def cutmix_data(x, y, alpha=1.0):
    """
    CutMix: 2ã¤ã®ç”»åƒã®ä¸€éƒ¨ã‚’åˆ‡ã‚Šè²¼ã‚Š

    Args:
        x: å…¥åŠ›ç”»åƒãƒãƒƒãƒ (B, C, H, W)
        y: ãƒ©ãƒ™ãƒ« (B,)
        alpha: Betaåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

    Returns:
        mixed_x, y_a, y_b, lam
    """
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size(0)
    index = torch.randperm(batch_size)

    # ãƒ©ãƒ³ãƒ€ãƒ ãªçŸ©å½¢é ˜åŸŸã‚’åˆ‡ã‚Šå–ã‚‹
    _, _, H, W = x.size()
    cut_rat = np.sqrt(1. - lam)
    cut_w = int(W * cut_rat)
    cut_h = int(H * cut_rat)

    # ä¸­å¿ƒåº§æ¨™ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ
    cx = np.random.randint(W)
    cy = np.random.randint(H)

    bbx1 = np.clip(cx - cut_w // 2, 0, W)
    bby1 = np.clip(cy - cut_h // 2, 0, H)
    bbx2 = np.clip(cx + cut_w // 2, 0, W)
    bby2 = np.clip(cy + cut_h // 2, 0, H)

    # ç”»åƒã‚’æ··ãœã‚‹
    mixed_x = x.clone()
    mixed_x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]

    # æ··åˆæ¯”ã‚’èª¿æ•´
    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))
    y_a, y_b = y, y[index]

    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """Mixupç”¨ã®æå¤±é–¢æ•°"""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

# ä½¿ç”¨ä¾‹
x = torch.randn(4, 3, 32, 32)  # 4æšã®ç”»åƒ
y = torch.tensor([0, 1, 2, 3])  # ãƒ©ãƒ™ãƒ«

# Mixup
mixed_x, y_a, y_b, lam = mixup_data(x, y, alpha=1.0)
print(f"Mixup:")
print(f"  å…ƒã®ç”»åƒ: {x.shape}")
print(f"  æ··åˆç”»åƒ: {mixed_x.shape}")
print(f"  æ··åˆæ¯” Î»: {lam:.3f}")
print(f"  ãƒ©ãƒ™ãƒ«A: {y_a.tolist()}, ãƒ©ãƒ™ãƒ«B: {y_b.tolist()}")

# CutMix
cutmix_x, y_a, y_b, lam = cutmix_data(x, y, alpha=1.0)
print(f"\nCutMix:")
print(f"  å…ƒã®ç”»åƒ: {x.shape}")
print(f"  æ··åˆç”»åƒ: {cutmix_x.shape}")
print(f"  æ··åˆæ¯” Î»: {lam:.3f}")
</code></pre>

<h3>albumentationsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æ´»ç”¨</h3>

<p><strong>albumentations</strong>ã¯ã€é«˜é€Ÿã§è±Šå¯ŒãªData Augmentationæ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚</p>

<pre><code class="language-python"># albumentationsã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install albumentations

import albumentations as A
from albumentations.pytorch import ToTensorV2
import cv2
import numpy as np

# albumentationsã«ã‚ˆã‚‹å¼·åŠ›ãªData Augmentation
album_transform = A.Compose([
    A.RandomCrop(height=32, width=32, p=1.0),
    A.HorizontalFlip(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),
    A.CoarseDropout(max_holes=1, max_height=16, max_width=16, p=0.5),
    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),
    A.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),
    ToTensorV2()
])

print("=== albumentations Data Augmentation ===")
print("å¤‰æ›ä¸€è¦§:")
for i, transform in enumerate(album_transform.transforms):
    print(f"  {i+1}. {transform.__class__.__name__}")

# PyTorchã®Datasetã¨çµ„ã¿åˆã‚ã›ã‚‹ä¾‹
class AlbumentationsDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, transform=None):
        self.dataset = dataset
        self.transform = transform

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        image, label = self.dataset[idx]

        # PIL Image â†’ numpy array
        image = np.array(image)

        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']

        return image, label

print("\nalbumentationsã¨PyTorchã®çµ±åˆ:")
print("AlbumentationsDatasetã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã—ã¦ã€")
print("torchvision.datasetsã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚")
</code></pre>

<hr>

<h2>2.4 è¨“ç·´ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</h2>

<h3>Learning Rate Scheduling</h3>

<p><strong>Learning Rate Scheduling</strong>ã¯ã€è¨“ç·´ã®é€²è¡Œã«å¿œã˜ã¦å­¦ç¿’ç‡ã‚’èª¿æ•´ã—ã€åæŸã‚’æ”¹å–„ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<pre><code class="language-python">import torch
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, ReduceLROnPlateau
import matplotlib.pyplot as plt

# ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«
model = torch.nn.Linear(10, 1)
optimizer = optim.SGD(model.parameters(), lr=0.1)

# 1. StepLR: å›ºå®šã‚¹ãƒ†ãƒƒãƒ—ã§å­¦ç¿’ç‡ã‚’æ¸›è¡°
scheduler_step = StepLR(optimizer, step_size=10, gamma=0.5)

# 2. CosineAnnealingLR: ã‚³ã‚µã‚¤ãƒ³é–¢æ•°ã§å­¦ç¿’ç‡ã‚’æ¸›è¡°
optimizer2 = optim.SGD(model.parameters(), lr=0.1)
scheduler_cosine = CosineAnnealingLR(optimizer2, T_max=50, eta_min=1e-5)

# 3. ReduceLROnPlateau: æ¤œè¨¼æå¤±ãŒæ”¹å–„ã—ãªã„å ´åˆã«æ¸›è¡°
optimizer3 = optim.SGD(model.parameters(), lr=0.1)
scheduler_plateau = ReduceLROnPlateau(optimizer3, mode='min', factor=0.5,
                                      patience=5, verbose=True)

# å­¦ç¿’ç‡ã®æ¨ç§»ã‚’å¯è¦–åŒ–
lrs_step = []
lrs_cosine = []

for epoch in range(50):
    # StepLR
    lrs_step.append(optimizer.param_groups[0]['lr'])
    scheduler_step.step()

    # CosineAnnealingLR
    lrs_cosine.append(optimizer2.param_groups[0]['lr'])
    scheduler_cosine.step()

print("=== Learning Rate Schedulingã®æ¯”è¼ƒ ===")
print(f"åˆæœŸå­¦ç¿’ç‡: {lrs_step[0]}")
print(f"StepLR (epoch 50): {lrs_step[-1]:.6f}")
print(f"CosineAnnealingLR (epoch 50): {lrs_cosine[-1]:.6f}")

# å®Ÿè·µçš„ãªä½¿ç”¨ä¾‹
def train_with_scheduler(model, train_loader, val_loader, epochs=50):
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-3)

    # Cosine Annealing with Warm Restarts
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=10, T_mult=2, eta_min=1e-6
    )

    for epoch in range(epochs):
        # è¨“ç·´ãƒ«ãƒ¼ãƒ—
        model.train()
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

        # å­¦ç¿’ç‡ã®æ›´æ–°
        scheduler.step()

        current_lr = optimizer.param_groups[0]['lr']
        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{epochs}], LR: {current_lr:.6f}")

print("\nå®Ÿè·µçš„ãªScheduler: CosineAnnealingWarmRestarts")
print("  T_0=10: æœ€åˆã®å†èµ·å‹•å‘¨æœŸ")
print("  T_mult=2: å‘¨æœŸã‚’2å€ãšã¤å¢—ã‚„ã™")
print("  eta_min=1e-6: æœ€å°å­¦ç¿’ç‡")
</code></pre>

<h3>Progressive Resizing</h3>

<p><strong>Progressive Resizing</strong>ã¯ã€è¨“ç·´ã®åˆæœŸã«å°ã•ã„ç”»åƒã§å­¦ç¿’ã—ã€å¾ã€…ã«å¤§ãã„ç”»åƒã«ç§»è¡Œã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<pre><code class="language-python">import torch
from torchvision import transforms, datasets
from torch.utils.data import DataLoader

class ProgressiveResizingTrainer:
    """Progressive Resizingã‚’å®Ÿè£…ã—ãŸè¨“ç·´ã‚¯ãƒ©ã‚¹"""

    def __init__(self, model, dataset_path, device):
        self.model = model
        self.dataset_path = dataset_path
        self.device = device

    def get_dataloader(self, image_size, batch_size):
        """æŒ‡å®šã‚µã‚¤ã‚ºã®DataLoaderã‚’ä½œæˆ"""
        transform = transforms.Compose([
            transforms.Resize(image_size),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
        ])

        dataset = datasets.ImageFolder(self.dataset_path, transform=transform)
        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        return loader

    def train_phase(self, image_size, epochs, lr):
        """ç‰¹å®šã®ç”»åƒã‚µã‚¤ã‚ºã§è¨“ç·´"""
        print(f"\n=== Training with image size: {image_size}Ã—{image_size} ===")

        loader = self.get_dataloader(image_size, batch_size=32)
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        criterion = torch.nn.CrossEntropyLoss()

        for epoch in range(epochs):
            self.model.train()
            for inputs, labels in loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                optimizer.zero_grad()
                outputs = self.model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

            if (epoch + 1) % 5 == 0:
                print(f"  Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}")

    def progressive_train(self):
        """Progressive Resizingã§è¨“ç·´"""
        # ãƒ•ã‚§ãƒ¼ã‚º1: 64Ã—64ã§é«˜é€Ÿã«å­¦ç¿’
        self.train_phase(image_size=64, epochs=10, lr=1e-3)

        # ãƒ•ã‚§ãƒ¼ã‚º2: 128Ã—128ã§ä¸­ç¨‹åº¦ã®å­¦ç¿’
        self.train_phase(image_size=128, epochs=10, lr=5e-4)

        # ãƒ•ã‚§ãƒ¼ã‚º3: 224Ã—224ã§æœ€çµ‚èª¿æ•´
        self.train_phase(image_size=224, epochs=20, lr=1e-4)

print("=== Progressive Resizingæˆ¦ç•¥ ===")
print("ãƒ•ã‚§ãƒ¼ã‚º1: 64Ã—64   (10 epochs, lr=1e-3)  - é«˜é€ŸãªåˆæœŸå­¦ç¿’")
print("ãƒ•ã‚§ãƒ¼ã‚º2: 128Ã—128 (10 epochs, lr=5e-4)  - ä¸­é–“çš„ãªèª¿æ•´")
print("ãƒ•ã‚§ãƒ¼ã‚º3: 224Ã—224 (20 epochs, lr=1e-4)  - é«˜è§£åƒåº¦ã§ã®æœ€çµ‚èª¿æ•´")
print("\nãƒ¡ãƒªãƒƒãƒˆ:")
print("  - è¨“ç·´æ™‚é–“ã®çŸ­ç¸®ï¼ˆåˆæœŸæ®µéšï¼‰")
print("  - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å‰Šæ¸›")
print("  - æ®µéšçš„ãªç²¾åº¦å‘ä¸Š")
</code></pre>

<h3>Test-Time Augmentation (TTA)</h3>

<p><strong>Test-Time Augmentation</strong>ã¯ã€ãƒ†ã‚¹ãƒˆæ™‚ã«è¤‡æ•°ã®æ‹¡å¼µç”»åƒã§äºˆæ¸¬ã—ã€ãã®å¹³å‡ã‚’å–ã‚‹ã“ã¨ã§ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
from torchvision import transforms

class TTAWrapper(nn.Module):
    """Test-Time Augmentationã‚’å®Ÿè£…ã—ãŸãƒ©ãƒƒãƒ‘ãƒ¼"""

    def __init__(self, model, num_augmentations=5):
        super(TTAWrapper, self).__init__()
        self.model = model
        self.num_augmentations = num_augmentations

        # TTAç”¨ã®å¤‰æ›
        self.tta_transforms = [
            transforms.Compose([]),  # ã‚ªãƒªã‚¸ãƒŠãƒ«
            transforms.Compose([transforms.RandomHorizontalFlip(p=1.0)]),  # å·¦å³åè»¢
            transforms.Compose([transforms.RandomRotation(degrees=5)]),    # 5åº¦å›è»¢
            transforms.Compose([transforms.RandomRotation(degrees=-5)]),   # -5åº¦å›è»¢
            transforms.Compose([
                transforms.ColorJitter(brightness=0.1, contrast=0.1)
            ]),  # è‰²èª¿æ•´
        ]

    def forward(self, x):
        """è¤‡æ•°ã®å¤‰æ›ã§äºˆæ¸¬ã—ã€å¹³å‡ã‚’å–ã‚‹"""
        predictions = []

        for transform in self.tta_transforms[:self.num_augmentations]:
            # ç”»åƒã‚’å¤‰æ›
            # æ³¨æ„: transformsã¯Tensorã«é©ç”¨ã§ããªã„ãŸã‚ã€å®Ÿéš›ã«ã¯ã‚«ã‚¹ã‚¿ãƒ å®Ÿè£…ãŒå¿…è¦
            augmented = x  # ç°¡ç•¥åŒ–ã®ãŸã‚ã€ã“ã“ã§ã¯ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ã¾ã¾

            # äºˆæ¸¬
            with torch.no_grad():
                pred = self.model(augmented)
                predictions.append(pred)

        # å¹³å‡ã‚’å–ã‚‹
        avg_prediction = torch.stack(predictions).mean(dim=0)
        return avg_prediction

# TTAã®ä½¿ç”¨ä¾‹
def predict_with_tta(model, image, num_augmentations=5):
    """
    TTAã‚’ä½¿ã£ãŸäºˆæ¸¬

    Args:
        model: è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        image: å…¥åŠ›ç”»åƒ (C, H, W)
        num_augmentations: æ‹¡å¼µã®æ•°

    Returns:
        averaged prediction
    """
    model.eval()
    predictions = []

    # ã‚ªãƒªã‚¸ãƒŠãƒ«
    with torch.no_grad():
        pred = model(image.unsqueeze(0))
        predictions.append(torch.softmax(pred, dim=1))

    # å·¦å³åè»¢
    flipped = torch.flip(image, dims=[2])
    with torch.no_grad():
        pred = model(flipped.unsqueeze(0))
        predictions.append(torch.softmax(pred, dim=1))

    # ä¸Šä¸‹åè»¢
    vflipped = torch.flip(image, dims=[1])
    with torch.no_grad():
        pred = model(vflipped.unsqueeze(0))
        predictions.append(torch.softmax(pred, dim=1))

    # å·¦å³+ä¸Šä¸‹åè»¢
    hvflipped = torch.flip(torch.flip(image, dims=[1]), dims=[2])
    with torch.no_grad():
        pred = model(hvflipped.unsqueeze(0))
        predictions.append(torch.softmax(pred, dim=1))

    # å¹³å‡ã‚’è¨ˆç®—
    avg_pred = torch.stack(predictions).mean(dim=0)

    return avg_pred

print("=== Test-Time Augmentation (TTA) ===")
print("å¤‰æ›ã®ç¨®é¡:")
print("  1. ã‚ªãƒªã‚¸ãƒŠãƒ«")
print("  2. å·¦å³åè»¢")
print("  3. ä¸Šä¸‹åè»¢")
print("  4. å·¦å³+ä¸Šä¸‹åè»¢")
print("\nç²¾åº¦å‘ä¸Šã®ç›®å®‰: 1-2%")
print("è¨ˆç®—ã‚³ã‚¹ãƒˆ: å¤‰æ›æ•°ã«æ¯”ä¾‹")
</code></pre>

<h3>Model Ensemble</h3>

<p><strong>Model Ensemble</strong>ã¯ã€è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ç²¾åº¦ã¨é ‘å¥æ€§ã‚’å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
from torchvision import models

class ModelEnsemble(nn.Module):
    """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«"""

    def __init__(self, models_list):
        super(ModelEnsemble, self).__init__()
        self.models = nn.ModuleList(models_list)

    def forward(self, x):
        """å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’å¹³å‡"""
        predictions = []
        for model in self.models:
            pred = model(x)
            predictions.append(torch.softmax(pred, dim=1))

        # å¹³å‡ã‚’å–ã‚‹
        avg_prediction = torch.stack(predictions).mean(dim=0)
        return avg_prediction

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®ä½œæˆä¾‹
def create_ensemble(num_classes=10, num_models=3):
    """ç•°ãªã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«"""
    models_list = []

    # ResNet-18
    model1 = models.resnet18(pretrained=False)
    model1.fc = nn.Linear(model1.fc.in_features, num_classes)
    models_list.append(model1)

    # ResNet-34
    model2 = models.resnet34(pretrained=False)
    model2.fc = nn.Linear(model2.fc.in_features, num_classes)
    models_list.append(model2)

    # MobileNetV2
    model3 = models.mobilenet_v2(pretrained=False)
    model3.classifier[1] = nn.Linear(model3.last_channel, num_classes)
    models_list.append(model3)

    ensemble = ModelEnsemble(models_list)
    return ensemble

# é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
class WeightedEnsemble(nn.Module):
    """é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«"""

    def __init__(self, models_list, weights=None):
        super(WeightedEnsemble, self).__init__()
        self.models = nn.ModuleList(models_list)

        if weights is None:
            weights = [1.0 / len(models_list)] * len(models_list)
        self.weights = torch.tensor(weights)

    def forward(self, x):
        """é‡ã¿ä»˜ãå¹³å‡"""
        predictions = []
        for model in self.models:
            pred = model(x)
            predictions.append(torch.softmax(pred, dim=1))

        # é‡ã¿ä»˜ãå¹³å‡
        predictions = torch.stack(predictions)
        weighted_pred = (predictions * self.weights.view(-1, 1, 1)).sum(dim=0)

        return weighted_pred

print("=== Model Ensemble ===")
print("æˆ¦ç•¥:")
print("  1. å˜ç´”å¹³å‡: ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’å‡ç­‰ã«å¹³å‡")
print("  2. é‡ã¿ä»˜ãå¹³å‡: æ€§èƒ½ã«å¿œã˜ãŸé‡ã¿ã§å¹³å‡")
print("  3. Voting: å¤šæ•°æ±ºã«ã‚ˆã‚‹åˆ†é¡")
print("\nã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®åŠ¹æœ:")
print("  - ç²¾åº¦å‘ä¸Š: 1-3%")
print("  - é ‘å¥æ€§å‘ä¸Š: å€‹ã€…ã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¨ãƒ©ãƒ¼ã‚’è£œå®Œ")
print("  - æ¨è«–ã‚³ã‚¹ãƒˆ: ãƒ¢ãƒ‡ãƒ«æ•°ã«æ¯”ä¾‹")

# ä½¿ç”¨ä¾‹
ensemble = create_ensemble(num_classes=10, num_models=3)
total_params = sum(p.numel() for p in ensemble.parameters())
print(f"\nã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
</code></pre>

<hr>

<h2>2.5 å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼šå®Œå…¨ãªç”»åƒåˆ†é¡ã‚·ã‚¹ãƒ†ãƒ </h2>

<h3>ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦</h3>

<p>å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æƒ³å®šã—ã€ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®ç”»åƒåˆ†é¡ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from PIL import Image
import os
from pathlib import Path

# ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
class CustomImageDataset(Dataset):
    """ã‚«ã‚¹ã‚¿ãƒ ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""

    def __init__(self, root_dir, transform=None):
        """
        Args:
            root_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
                      root_dir/
                        class1/
                          img1.jpg
                          img2.jpg
                        class2/
                          img1.jpg
        """
        self.root_dir = Path(root_dir)
        self.transform = transform
        self.classes = sorted([d.name for d in self.root_dir.iterdir() if d.is_dir()])
        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}

        # ã™ã¹ã¦ã®ç”»åƒãƒ‘ã‚¹ã¨ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
        self.samples = []
        for class_name in self.classes:
            class_dir = self.root_dir / class_name
            for img_path in class_dir.glob('*.jpg'):
                self.samples.append((img_path, self.class_to_idx[class_name]))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        image = Image.open(img_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        return image, label

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
def get_data_transforms(image_size=224):
    """è¨“ç·´ã¨ãƒ†ã‚¹ãƒˆç”¨ã®å¤‰æ›ã‚’å–å¾—"""
    train_transform = transforms.Compose([
        transforms.Resize((image_size, image_size)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    test_transform = transforms.Compose([
        transforms.Resize((image_size, image_size)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    return train_transform, test_transform

# ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
def build_model(num_classes, architecture='resnet18', pretrained=True):
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰

    Args:
        num_classes: ã‚¯ãƒ©ã‚¹æ•°
        architecture: 'resnet18', 'resnet50', 'efficientnet_b0'
        pretrained: äº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹
    """
    if architecture == 'resnet18':
        model = models.resnet18(pretrained=pretrained)
        model.fc = nn.Linear(model.fc.in_features, num_classes)

    elif architecture == 'resnet50':
        model = models.resnet50(pretrained=pretrained)
        model.fc = nn.Linear(model.fc.in_features, num_classes)

    elif architecture == 'efficientnet_b0':
        model = models.efficientnet_b0(pretrained=pretrained)
        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)

    return model

# è¨“ç·´ã‚¯ãƒ©ã‚¹
class ImageClassifier:
    """å®Œå…¨ãªç”»åƒåˆ†é¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(self, model, device):
        self.model = model.to(device)
        self.device = device
        self.history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

    def train_epoch(self, loader, criterion, optimizer):
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for inputs, labels in loader:
            inputs, labels = inputs.to(self.device), labels.to(self.device)

            optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

        epoch_loss = running_loss / len(loader)
        epoch_acc = 100. * correct / total

        return epoch_loss, epoch_acc

    def validate(self, loader, criterion):
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, labels in loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                outputs = self.model(inputs)
                loss = criterion(outputs, labels)

                running_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()

        epoch_loss = running_loss / len(loader)
        epoch_acc = 100. * correct / total

        return epoch_loss, epoch_acc

    def fit(self, train_loader, val_loader, epochs=50, lr=1e-3):
        """è¨“ç·´ã‚’å®Ÿè¡Œ"""
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.model.parameters(), lr=lr)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

        best_val_acc = 0

        for epoch in range(epochs):
            train_loss, train_acc = self.train_epoch(train_loader, criterion, optimizer)
            val_loss, val_acc = self.validate(val_loader, criterion)

            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_loss'].append(val_loss)
            self.history['val_acc'].append(val_acc)

            scheduler.step()

            if val_acc > best_val_acc:
                best_val_acc = val_acc
                self.save_model('best_model.pth')

            if (epoch + 1) % 5 == 0:
                print(f"Epoch [{epoch+1}/{epochs}]")
                print(f"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
                print(f"  Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%")

        print(f"\nè¨“ç·´å®Œäº†ï¼ãƒ™ã‚¹ãƒˆæ¤œè¨¼ç²¾åº¦: {best_val_acc:.2f}%")

    def save_model(self, path):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        torch.save(self.model.state_dict(), path)

    def load_model(self, path):
        """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        self.model.load_state_dict(torch.load(path))

# ä½¿ç”¨ä¾‹
print("=== å®Œå…¨ãªç”»åƒåˆ†é¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ===")
print("\nä½¿ç”¨æ–¹æ³•:")
print("1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ï¼ˆroot_dir/class1/, root_dir/class2/, ...ï¼‰")
print("2. ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰")
print("3. ImageClassifierã§è¨“ç·´")
print("4. ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜")
print("\nã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰:")
print("""
# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
train_transform, val_transform = get_data_transforms()
train_dataset = CustomImageDataset('data/train', transform=train_transform)
val_dataset = CustomImageDataset('data/val', transform=val_transform)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
model = build_model(num_classes=10, architecture='resnet18', pretrained=True)

# è¨“ç·´
classifier = ImageClassifier(model, device)
classifier.fit(train_loader, val_loader, epochs=50, lr=1e-3)
""")
</code></pre>

<h3>è©•ä¾¡ã¨å¯è¦–åŒ–</h3>

<pre><code class="language-python">import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

def evaluate_model(model, test_loader, device, class_names):
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’è©³ç´°ã«è©•ä¾¡

    Returns:
        accuracy, confusion matrix, classification report
    """
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.numpy())

    # ç²¾åº¦
    accuracy = 100. * np.sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels)

    # æ··åŒè¡Œåˆ—
    cm = confusion_matrix(all_labels, all_preds)

    # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
    report = classification_report(all_labels, all_preds, target_names=class_names)

    return accuracy, cm, report

def plot_confusion_matrix(cm, class_names):
    """æ··åŒè¡Œåˆ—ã‚’å¯è¦–åŒ–"""
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=150)
    print("æ··åŒè¡Œåˆ—ã‚’ä¿å­˜ã—ã¾ã—ãŸ: confusion_matrix.png")

def plot_training_history(history):
    """è¨“ç·´å±¥æ­´ã‚’å¯è¦–åŒ–"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # Loss
    axes[0].plot(history['train_loss'], label='Train Loss')
    axes[0].plot(history['val_loss'], label='Val Loss')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('Training and Validation Loss')
    axes[0].legend()
    axes[0].grid(True)

    # Accuracy
    axes[1].plot(history['train_acc'], label='Train Acc')
    axes[1].plot(history['val_acc'], label='Val Acc')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Accuracy (%)')
    axes[1].set_title('Training and Validation Accuracy')
    axes[1].legend()
    axes[1].grid(True)

    plt.tight_layout()
    plt.savefig('training_history.png', dpi=150)
    print("è¨“ç·´å±¥æ­´ã‚’ä¿å­˜ã—ã¾ã—ãŸ: training_history.png")

print("=== ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¨å¯è¦–åŒ– ===")
print("\nè©•ä¾¡æŒ‡æ¨™:")
print("  1. ç²¾åº¦ï¼ˆAccuracyï¼‰")
print("  2. æ··åŒè¡Œåˆ—ï¼ˆConfusion Matrixï¼‰")
print("  3. ã‚¯ãƒ©ã‚¹ã”ã¨ã® Precision, Recall, F1-score")
print("\nå¯è¦–åŒ–:")
print("  1. è¨“ç·´æ›²ç·šï¼ˆLoss, Accuracyï¼‰")
print("  2. æ··åŒè¡Œåˆ—ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—")
</code></pre>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<details>
<summary><strong>æ¼”ç¿’1: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ¯”è¼ƒ</strong></summary>

<p>ResNet-18ã€ResNet-50ã€EfficientNet-B0ã®3ã¤ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’CIFAR-10ã§æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€è¨“ç·´æ™‚é–“ã€ç²¾åº¦ã‚’è¨˜éŒ²ã—ã€ã©ã®ãƒ¢ãƒ‡ãƒ«ãŒæœ€ã‚‚åŠ¹ç‡çš„ã‹è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python"># TODO: 3ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…
# TODO: åŒã˜è¨“ç·´è¨­å®šã§æ€§èƒ½ã‚’æ¯”è¼ƒ
# TODO: çµæœã‚’è¡¨ã«ã¾ã¨ã‚ã‚‹

# ãƒ’ãƒ³ãƒˆ:
# - torchvision.modelsã‚’ä½¿ç”¨
# - è¨“ç·´æ™‚é–“ã‚’è¨ˆæ¸¬
# - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚‚è€ƒæ…®
</code></pre>

</details>

<details>
<summary><strong>æ¼”ç¿’2: è»¢ç§»å­¦ç¿’ã®åŠ¹æœ</strong></summary>

<p>äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚ãƒ‡ãƒ¼ã‚¿é‡ã‚’å¤‰ãˆã¦ï¼ˆ100æšã€500æšã€å…¨ãƒ‡ãƒ¼ã‚¿ï¼‰ã€è»¢ç§»å­¦ç¿’ã®åŠ¹æœãŒã©ã†å¤‰åŒ–ã™ã‚‹ã‹èª¿ã¹ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python"># TODO: pretrained=Trueã¨Falseã§æ¯”è¼ƒ
# TODO: ãƒ‡ãƒ¼ã‚¿é‡ã‚’å¤‰ãˆã¦å®Ÿé¨“
# TODO: è¨“ç·´æ›²ç·šã‚’æ¯”è¼ƒ

# è©•ä¾¡æŒ‡æ¨™:
# - æœ€çµ‚ç²¾åº¦
# - åæŸã¾ã§ã®ã‚¨ãƒãƒƒã‚¯æ•°
# - ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡æ€§
</code></pre>

</details>

<details>
<summary><strong>æ¼”ç¿’3: Data Augmentationã®åŠ¹æœ</strong></summary>

<p>ç•°ãªã‚‹Data Augmentationæˆ¦ç•¥ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ï¼š(1) å¤‰æ›ãªã—ã€(2) æ¨™æº–çš„ãªå¤‰æ›ã€(3) å¼·åŠ›ãªå¤‰æ›ï¼ˆMixup/CutMixå«ã‚€ï¼‰ã€‚éå­¦ç¿’ã¸ã®å½±éŸ¿ã‚’èª¿ã¹ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python"># TODO: 3ã¤ã®å¤‰æ›æˆ¦ç•¥ã‚’å®Ÿè£…
# TODO: è¨“ç·´èª¤å·®ã¨ãƒ†ã‚¹ãƒˆèª¤å·®ã®å·®ã‚’æ¯”è¼ƒ
# TODO: æœ€é©ãªå¤‰æ›ã®çµ„ã¿åˆã‚ã›ã‚’è¦‹ã¤ã‘ã‚‹

# ãƒ’ãƒ³ãƒˆ:
# - éå­¦ç¿’åº¦ = è¨“ç·´ç²¾åº¦ - ãƒ†ã‚¹ãƒˆç²¾åº¦
# - è¨“ç·´æ›²ç·šã‚’å¯è¦–åŒ–
</code></pre>

</details>

<details>
<summary><strong>æ¼”ç¿’4: Learning Rate Schedulingã®æœ€é©åŒ–</strong></summary>

<p>StepLRã€CosineAnnealingLRã€ReduceLROnPlateauã®3ã¤ã®Schedulerã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚ã©ã®SchedulerãŒæœ€ã‚‚åŠ¹æœçš„ã‹ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¿œã˜ã¦å¤‰ã‚ã‚‹ã‹èª¿ã¹ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python"># TODO: 3ã¤ã®Schedulerã‚’å®Ÿè£…
# TODO: å­¦ç¿’ç‡ã®æ¨ç§»ã‚’å¯è¦–åŒ–
# TODO: æœ€çµ‚ç²¾åº¦ã‚’æ¯”è¼ƒ

# è©•ä¾¡:
# - åæŸé€Ÿåº¦
# - æœ€çµ‚ç²¾åº¦
# - å®‰å®šæ€§
</code></pre>

</details>

<details>
<summary><strong>æ¼”ç¿’5: å®Œå…¨ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ§‹ç¯‰</strong></summary>

<p>ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å®Œå…¨ãªç”»åƒåˆ†é¡ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¦ãã ã•ã„ã€‚ãƒ‡ãƒ¼ã‚¿æº–å‚™ã€ãƒ¢ãƒ‡ãƒ«é¸æŠã€è¨“ç·´ã€è©•ä¾¡ã€æ¨è«–ã¾ã§ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python"># TODO: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
# TODO: æœ€é©ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é¸æŠ
# TODO: Data Augmentationã®è¨­è¨ˆ
# TODO: è¨“ç·´ã¨è©•ä¾¡
# TODO: æ¨è«–ç”¨ã®APIã‚’ä½œæˆ

# è¦ä»¶:
# 1. ãƒ†ã‚¹ãƒˆç²¾åº¦ > 90%
# 2. æ¨è«–æ™‚é–“ < 100ms/ç”»åƒ
# 3. ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º < 100MB
# 4. å¯è¦–åŒ–ã¨è©³ç´°ãªãƒ¬ãƒãƒ¼ãƒˆ
</code></pre>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ul>
<li>LeCun, Y., et al. (1998). "Gradient-based learning applied to document recognition." Proceedings of the IEEE.</li>
<li>Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). "ImageNet classification with deep convolutional neural networks." NeurIPS.</li>
<li>Simonyan, K., & Zisserman, A. (2014). "Very deep convolutional networks for large-scale image recognition." arXiv preprint.</li>
<li>He, K., et al. (2016). "Deep residual learning for image recognition." CVPR.</li>
<li>Szegedy, C., et al. (2015). "Going deeper with convolutions." CVPR.</li>
<li>Howard, A. G., et al. (2017). "MobileNets: Efficient convolutional neural networks for mobile vision applications." arXiv preprint.</li>
<li>Tan, M., & Le, Q. (2019). "EfficientNet: Rethinking model scaling for convolutional neural networks." ICML.</li>
<li>Yosinski, J., et al. (2014). "How transferable are features in deep neural networks?" NeurIPS.</li>
<li>Zhang, H., et al. (2018). "mixup: Beyond empirical risk minimization." ICLR.</li>
<li>Yun, S., et al. (2019). "CutMix: Regularization strategy to train strong classifiers with localizable features." ICCV.</li>
</ul>

<hr>

<h2>ã¾ã¨ã‚</h2>

<p>ã“ã®ç« ã§ã¯ã€ç”»åƒåˆ†é¡ã¨ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¤ã„ã¦å­¦ã³ã¾ã—ãŸã€‚</p>

<h3>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</h3>

<ul>
<li><strong>CNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é€²åŒ–</strong>: LeNet â†’ AlexNet â†’ VGG â†’ ResNet â†’ EfficientNet</li>
<li><strong>ResNetã®é©æ–°</strong>: Skip Connectionsã«ã‚ˆã‚‹æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å­¦ç¿’</li>
<li><strong>è»¢ç§»å­¦ç¿’</strong>: äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§é«˜ç²¾åº¦ã‚’å®Ÿç¾</li>
<li><strong>Feature Extraction vs Fine-tuning</strong>: ãƒ‡ãƒ¼ã‚¿é‡ã¨ã‚¿ã‚¹ã‚¯ã®é¡ä¼¼åº¦ã«å¿œã˜ãŸä½¿ã„åˆ†ã‘</li>
<li><strong>Data Augmentation</strong>: å¹¾ä½•å­¦çš„å¤‰æ›ã€è‰²å¤‰æ›ã€Mixup/CutMixã§æ±åŒ–æ€§èƒ½ã‚’å‘ä¸Š</li>
<li><strong>è¨“ç·´ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</strong>: Learning Rate Schedulingã€Progressive Resizingã€TTAã€Ensembleã§ç²¾åº¦ã‚’å‘ä¸Š</li>
<li><strong>å®Ÿè·µçš„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</strong>: ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‹ã‚‰è©•ä¾¡ã¾ã§ä¸€è²«ã—ãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</li>
</ul>

<h3>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>

<p>æ¬¡ç« ã§ã¯ã€<strong>ç‰©ä½“æ¤œå‡ºã¨ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³</strong>ã«ã¤ã„ã¦å­¦ã³ã¾ã™ã€‚ç”»åƒå†…ã®ç‰©ä½“ã®ä½ç½®ç‰¹å®šã‚„ãƒ”ã‚¯ã‚»ãƒ«å˜ä½ã®åˆ†é¡ãªã©ã€ã‚ˆã‚Šé«˜åº¦ãªã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã«æŒ‘æˆ¦ã—ã¾ã™ã€‚</p>

<div class="navigation">
    <a href="chapter1-fundamentals.html" class="nav-button">â† ç¬¬1ç« ï¼šã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®åŸºç¤</a>
    <a href="chapter3-object-detection.html" class="nav-button">ç¬¬3ç« ï¼šç‰©ä½“æ¤œå‡ºã¨ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ â†’</a>
</div>

</main>


    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
    <p>&copy; 2024 AI Terakoya. All rights reserved.</p>
</footer>

</body>
</html>
