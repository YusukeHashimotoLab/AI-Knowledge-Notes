<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
<meta content="ç¬¬5ç« ï¼šã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³å¿œç”¨ - AI Terakoya" name="description"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬5ç« ï¼šã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³å¿œç”¨ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/computer-vision-introduction/chapter5-cv-applications.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/computer-vision-introduction/index.html">Computer Vision</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 5</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬5ç« ï¼šã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³å¿œç”¨</h1>
            <p class="subtitle">å®Ÿè·µçš„ãªCVæŠ€è¡“ - é¡”èªè­˜ã‹ã‚‰ç”»åƒç”Ÿæˆã¾ã§</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 35-40åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šï½ä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 10å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… é¡”æ¤œå‡ºãƒ»èªè­˜ã®æœ€æ–°æŠ€è¡“ã‚’ç†è§£ã—å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… å§¿å‹¢æ¨å®šã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
<li>âœ… OCRæŠ€è¡“ã§æ–‡å­—èªè­˜ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… ç”»åƒç”Ÿæˆãƒ»ç·¨é›†æŠ€è¡“ã‚’å¿œç”¨ã§ãã‚‹</li>
<li>âœ… ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®CVã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹ç™ºã§ãã‚‹</li>
<li>âœ… ãƒ¢ãƒ‡ãƒ«ã‚’æœ€é©åŒ–ã—ãƒ‡ãƒ—ãƒ­ã‚¤ã§ãã‚‹</li>
</ul>

<hr>

<h2>5.1 é¡”èªè­˜ãƒ»æ¤œå‡º</h2>

<h3>é¡”æ¤œå‡ºæŠ€è¡“ã®é€²åŒ–</h3>

<p><strong>é¡”æ¤œå‡ºï¼ˆFace Detectionï¼‰</strong>ã¯ã€ç”»åƒã‹ã‚‰é¡”ã®ä½ç½®ã‚’ç‰¹å®šã™ã‚‹æŠ€è¡“ã§ã™ã€‚ç¾ä»£ã®ä¸»è¦ãªæ‰‹æ³•ï¼š</p>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>ç‰¹å¾´</th>
<th>ç²¾åº¦</th>
<th>é€Ÿåº¦</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Haar Cascade</strong></td>
<td>å¤å…¸çš„ã€è»½é‡</td>
<td>ä½</td>
<td>é«˜é€Ÿ</td>
</tr>
<tr>
<td><strong>HOG + SVM</strong></td>
<td>ç‰¹å¾´ãƒ™ãƒ¼ã‚¹</td>
<td>ä¸­</td>
<td>ä¸­é€Ÿ</td>
</tr>
<tr>
<td><strong>MTCNN</strong></td>
<td>å¤šæ®µéšCNN</td>
<td>é«˜</td>
<td>ä¸­é€Ÿ</td>
</tr>
<tr>
<td><strong>RetinaFace</strong></td>
<td>æœ€æ–°ã€ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ä»˜ã</td>
<td>æœ€é«˜</td>
<td>GPUå¿…é ˆ</td>
</tr>
</tbody>
</table>

<h3>MTCNNã«ã‚ˆã‚‹é¡”æ¤œå‡º</h3>

<pre><code class="language-python">import cv2
import numpy as np
import matplotlib.pyplot as plt
from mtcnn import MTCNN

# MTCNNãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
detector = MTCNN()

# ç”»åƒã®èª­ã¿è¾¼ã¿
image = cv2.imread('group_photo.jpg')
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# é¡”æ¤œå‡º
detections = detector.detect_faces(image_rgb)

print(f"=== æ¤œå‡ºçµæœ ===")
print(f"æ¤œå‡ºã•ã‚ŒãŸé¡”ã®æ•°: {len(detections)}")

# æ¤œå‡ºçµæœã®æç”»
image_with_boxes = image_rgb.copy()

for i, detection in enumerate(detections):
    x, y, width, height = detection['box']
    confidence = detection['confidence']

    # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹
    cv2.rectangle(image_with_boxes,
                  (x, y), (x + width, y + height),
                  (0, 255, 0), 2)

    # ä¿¡é ¼åº¦ã®è¡¨ç¤º
    cv2.putText(image_with_boxes,
                f'{confidence:.2f}',
                (x, y - 10),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.5, (0, 255, 0), 2)

    # ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ï¼ˆç›®ã€é¼»ã€å£ï¼‰
    keypoints = detection['keypoints']
    for name, point in keypoints.items():
        cv2.circle(image_with_boxes, point, 2, (255, 0, 0), -1)

    print(f"\né¡” {i+1}:")
    print(f"  ä½ç½®: ({x}, {y})")
    print(f"  ã‚µã‚¤ã‚º: {width} x {height}")
    print(f"  ä¿¡é ¼åº¦: {confidence:.3f}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

axes[0].imshow(image_rgb)
axes[0].set_title('å…ƒç”»åƒ', fontsize=14)
axes[0].axis('off')

axes[1].imshow(image_with_boxes)
axes[1].set_title(f'æ¤œå‡ºçµæœ ({len(detections)}å€‹ã®é¡”)', fontsize=14)
axes[1].axis('off')

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== æ¤œå‡ºçµæœ ===
æ¤œå‡ºã•ã‚ŒãŸé¡”ã®æ•°: 3

é¡” 1:
  ä½ç½®: (120, 80)
  ã‚µã‚¤ã‚º: 150 x 180
  ä¿¡é ¼åº¦: 0.998

é¡” 2:
  ä½ç½®: (350, 95)
  ã‚µã‚¤ã‚º: 145 x 175
  ä¿¡é ¼åº¦: 0.995

é¡” 3:
  ä½ç½®: (570, 110)
  ã‚µã‚¤ã‚º: 140 x 170
  ä¿¡é ¼åº¦: 0.992
</code></pre>

<h3>é¡”èªè­˜: DeepFaceãƒ©ã‚¤ãƒ–ãƒ©ãƒª</h3>

<p><strong>é¡”èªè­˜ï¼ˆFace Recognitionï¼‰</strong>ã¯ã€æ¤œå‡ºã•ã‚ŒãŸé¡”ãŒèª°ã§ã‚ã‚‹ã‹ã‚’è­˜åˆ¥ã™ã‚‹æŠ€è¡“ã§ã™ã€‚</p>

<pre><code class="language-python">from deepface import DeepFace
import cv2
import matplotlib.pyplot as plt

# é¡”èªè­˜ãƒ¢ãƒ‡ãƒ«: VGG-Face, Facenet, ArcFace, Dlib, OpenFace ãªã©
model_name = 'Facenet'

# é¡”ã®æ¯”è¼ƒ
def compare_faces(img1_path, img2_path, model='Facenet'):
    """2ã¤ã®é¡”ç”»åƒã‚’æ¯”è¼ƒ"""
    result = DeepFace.verify(
        img1_path=img1_path,
        img2_path=img2_path,
        model_name=model,
        enforce_detection=True
    )

    return result

# å®Ÿè¡Œä¾‹
result = compare_faces('person1_photo1.jpg', 'person1_photo2.jpg')

print("=== é¡”èªè­˜çµæœ ===")
print(f"åŒä¸€äººç‰©: {result['verified']}")
print(f"è·é›¢: {result['distance']:.4f}")
print(f"é–¾å€¤: {result['threshold']:.4f}")
print(f"ãƒ¢ãƒ‡ãƒ«: {result['model']}")

if result['verified']:
    print("\nâœ“ åŒä¸€äººç‰©ã¨åˆ¤å®šã•ã‚Œã¾ã—ãŸ")
else:
    print("\nâœ— åˆ¥äººã¨åˆ¤å®šã•ã‚Œã¾ã—ãŸ")

# é¡”ã®ç‰¹å¾´æŠ½å‡º
def extract_face_embedding(img_path, model='Facenet'):
    """é¡”ç”»åƒã‹ã‚‰ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º"""
    embedding = DeepFace.represent(
        img_path=img_path,
        model_name=model,
        enforce_detection=True
    )
    return np.array(embedding[0]['embedding'])

# ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®å–å¾—
embedding1 = extract_face_embedding('person1_photo1.jpg')
embedding2 = extract_face_embedding('person1_photo2.jpg')

print(f"\n=== ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ« ===")
print(f"æ¬¡å…ƒæ•°: {len(embedding1)}")
print(f"ãƒ™ã‚¯ãƒˆãƒ«1ã®ä¸€éƒ¨: {embedding1[:10]}")
print(f"\nã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦: {np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2)):.4f}")
</code></pre>

<h3>å®Œå…¨ãªé¡”èªè­˜ã‚·ã‚¹ãƒ†ãƒ </h3>

<pre><code class="language-python">import cv2
import numpy as np
from mtcnn import MTCNN
from deepface import DeepFace
import os
import pickle

class FaceRecognitionSystem:
    """é¡”èªè­˜ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, model_name='Facenet'):
        self.detector = MTCNN()
        self.model_name = model_name
        self.face_database = {}

    def register_face(self, name, image_path):
        """é¡”ã‚’ç™»éŒ²"""
        try:
            # ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º
            embedding = DeepFace.represent(
                img_path=image_path,
                model_name=self.model_name,
                enforce_detection=True
            )

            self.face_database[name] = np.array(embedding[0]['embedding'])
            print(f"âœ“ {name} ã‚’ç™»éŒ²ã—ã¾ã—ãŸ")

        except Exception as e:
            print(f"âœ— {name} ã®ç™»éŒ²ã«å¤±æ•—: {str(e)}")

    def recognize_face(self, image_path, threshold=0.6):
        """é¡”ã‚’èªè­˜"""
        try:
            # ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º
            embedding = DeepFace.represent(
                img_path=image_path,
                model_name=self.model_name,
                enforce_detection=True
            )

            query_embedding = np.array(embedding[0]['embedding'])

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®å…¨ã¦ã®é¡”ã¨æ¯”è¼ƒ
            min_distance = float('inf')
            best_match = None

            for name, db_embedding in self.face_database.items():
                # ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã‚’è¨ˆç®—
                distance = np.linalg.norm(query_embedding - db_embedding)

                if distance < min_distance:
                    min_distance = distance
                    best_match = name

            # é–¾å€¤åˆ¤å®š
            if min_distance < threshold:
                return best_match, min_distance
            else:
                return "Unknown", min_distance

        except Exception as e:
            return f"Error: {str(e)}", None

    def save_database(self, filepath):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜"""
        with open(filepath, 'wb') as f:
            pickle.dump(self.face_database, f)
        print(f"âœ“ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜: {filepath}")

    def load_database(self, filepath):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿"""
        with open(filepath, 'rb') as f:
            self.face_database = pickle.load(f)
        print(f"âœ“ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿: {filepath}")

# ã‚·ã‚¹ãƒ†ãƒ ã®ä½¿ç”¨ä¾‹
system = FaceRecognitionSystem(model_name='Facenet')

# é¡”ã‚’ç™»éŒ²
system.register_face("Alice", "alice_1.jpg")
system.register_face("Bob", "bob_1.jpg")
system.register_face("Carol", "carol_1.jpg")

# é¡”ã‚’èªè­˜
test_image = "unknown_person.jpg"
name, distance = system.recognize_face(test_image)

print(f"\n=== èªè­˜çµæœ ===")
print(f"èªè­˜ã•ã‚ŒãŸäººç‰©: {name}")
print(f"è·é›¢: {distance:.4f}")

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜
system.save_database("face_database.pkl")
</code></pre>

<hr>

<h2>5.2 å§¿å‹¢æ¨å®š</h2>

<h3>å§¿å‹¢æ¨å®šã¨ã¯</h3>

<p><strong>å§¿å‹¢æ¨å®šï¼ˆPose Estimationï¼‰</strong>ã¯ã€ç”»åƒã‹ã‚‰äººä½“ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆï¼ˆé–¢ç¯€ä½ç½®ï¼‰ã‚’æ¤œå‡ºã™ã‚‹æŠ€è¡“ã§ã™ã€‚</p>

<div class="mermaid">
graph TD
    A[å…¥åŠ›ç”»åƒ] --> B[å§¿å‹¢æ¨å®šãƒ¢ãƒ‡ãƒ«]
    B --> C[ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ¤œå‡º]
    C --> D[é¼»]
    C --> E[ç›®]
    C --> F[è‚©]
    C --> G[è‚˜]
    C --> H[æ‰‹é¦–]
    C --> I[è…°]
    C --> J[è†]
    C --> K[è¶³é¦–]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#e8f5e9
    style F fill:#e8f5e9
    style G fill:#e8f5e9
    style H fill:#e8f5e9
    style I fill:#e8f5e9
    style J fill:#e8f5e9
    style K fill:#e8f5e9
</div>

<h3>MediaPipe Poseã«ã‚ˆã‚‹å§¿å‹¢æ¨å®š</h3>

<pre><code class="language-python">import cv2
import mediapipe as mp
import numpy as np
import matplotlib.pyplot as plt

# MediaPipe Poseã®åˆæœŸåŒ–
mp_pose = mp.solutions.pose
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

# ç”»åƒã®èª­ã¿è¾¼ã¿
image = cv2.imread('person_standing.jpg')
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# å§¿å‹¢æ¨å®š
with mp_pose.Pose(
    static_image_mode=True,
    model_complexity=2,
    enable_segmentation=True,
    min_detection_confidence=0.5
) as pose:

    results = pose.process(image_rgb)

    if results.pose_landmarks:
        print("=== å§¿å‹¢æ¨å®šçµæœ ===")
        print(f"æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ•°: {len(results.pose_landmarks.landmark)}")

        # ä¸»è¦ãªã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®åº§æ¨™ã‚’è¡¨ç¤º
        landmarks = results.pose_landmarks.landmark
        h, w, _ = image.shape

        keypoints_of_interest = {
            0: 'é¼»',
            11: 'å·¦è‚©',
            12: 'å³è‚©',
            13: 'å·¦è‚˜',
            14: 'å³è‚˜',
            23: 'å·¦è…°',
            24: 'å³è…°',
            25: 'å·¦è†',
            26: 'å³è†'
        }

        for idx, name in keypoints_of_interest.items():
            landmark = landmarks[idx]
            x = int(landmark.x * w)
            y = int(landmark.y * h)
            visibility = landmark.visibility

            print(f"{name}: ({x}, {y}), å¯è¦–æ€§: {visibility:.3f}")

        # æç”»
        annotated_image = image_rgb.copy()
        mp_drawing.draw_landmarks(
            annotated_image,
            results.pose_landmarks,
            mp_pose.POSE_CONNECTIONS,
            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style()
        )

        # å¯è¦–åŒ–
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))

        axes[0].imshow(image_rgb)
        axes[0].set_title('å…ƒç”»åƒ', fontsize=14)
        axes[0].axis('off')

        axes[1].imshow(annotated_image)
        axes[1].set_title('å§¿å‹¢æ¨å®šçµæœ', fontsize=14)
        axes[1].axis('off')

        plt.tight_layout()
        plt.show()

    else:
        print("å§¿å‹¢ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
</code></pre>

<h3>å‹•ä½œèªè­˜: è§’åº¦è¨ˆç®—</h3>

<pre><code class="language-python">import numpy as np

def calculate_angle(point1, point2, point3):
    """3ç‚¹ã‹ã‚‰è§’åº¦ã‚’è¨ˆç®—ï¼ˆåº¦æ•°æ³•ï¼‰"""
    # ãƒ™ã‚¯ãƒˆãƒ«ã®è¨ˆç®—
    vector1 = np.array([point1[0] - point2[0], point1[1] - point2[1]])
    vector2 = np.array([point3[0] - point2[0], point3[1] - point2[1]])

    # è§’åº¦ã®è¨ˆç®—
    cosine_angle = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))
    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))

    return np.degrees(angle)

def detect_exercise(landmarks, image_shape):
    """é‹å‹•å‹•ä½œã‚’æ¤œå‡º"""
    h, w = image_shape[:2]

    # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®åº§æ¨™ã‚’å–å¾—
    left_shoulder = (int(landmarks[11].x * w), int(landmarks[11].y * h))
    left_elbow = (int(landmarks[13].x * w), int(landmarks[13].y * h))
    left_wrist = (int(landmarks[15].x * w), int(landmarks[15].y * h))

    left_hip = (int(landmarks[23].x * w), int(landmarks[23].y * h))
    left_knee = (int(landmarks[25].x * w), int(landmarks[25].y * h))
    left_ankle = (int(landmarks[27].x * w), int(landmarks[27].y * h))

    # è§’åº¦ã‚’è¨ˆç®—
    elbow_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)
    knee_angle = calculate_angle(left_hip, left_knee, left_ankle)
    hip_angle = calculate_angle(left_shoulder, left_hip, left_knee)

    print(f"\n=== é–¢ç¯€è§’åº¦ ===")
    print(f"è‚˜ã®è§’åº¦: {elbow_angle:.1f}Â°")
    print(f"è†ã®è§’åº¦: {knee_angle:.1f}Â°")
    print(f"è…°ã®è§’åº¦: {hip_angle:.1f}Â°")

    # å‹•ä½œåˆ¤å®š
    if knee_angle < 90:
        action = "ã‚¹ã‚¯ãƒ¯ãƒƒãƒˆï¼ˆä¸‹ï¼‰"
    elif knee_angle > 160:
        action = "ã‚¹ã‚¯ãƒ¯ãƒƒãƒˆï¼ˆä¸Šï¼‰ã¾ãŸã¯ç«‹ä½"
    else:
        action = "ä¸­é–“å§¿å‹¢"

    if 30 < elbow_angle < 90:
        arm_action = "è…•ç«‹ã¦ä¼ã›ï¼ˆä¸‹ï¼‰"
    elif elbow_angle > 160:
        arm_action = "è…•ç«‹ã¦ä¼ã›ï¼ˆä¸Šï¼‰"
    else:
        arm_action = "è…•æ›²ã’ä¸­"

    return {
        'angles': {
            'elbow': elbow_angle,
            'knee': knee_angle,
            'hip': hip_angle
        },
        'leg_action': action,
        'arm_action': arm_action
    }

# ä½¿ç”¨ä¾‹ï¼ˆå‰ã®ã‚³ãƒ¼ãƒ‰ã®ç¶šãï¼‰
if results.pose_landmarks:
    exercise_info = detect_exercise(results.pose_landmarks.landmark, image.shape)

    print(f"\n=== å‹•ä½œèªè­˜ ===")
    print(f"ä¸‹åŠèº«: {exercise_info['leg_action']}")
    print(f"ä¸ŠåŠèº«: {exercise_info['arm_action']}")
</code></pre>

<h3>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å§¿å‹¢æ¨å®š</h3>

<pre><code class="language-python">import cv2
import mediapipe as mp
import time

class PoseEstimator:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å§¿å‹¢æ¨å®š"""

    def __init__(self):
        self.mp_pose = mp.solutions.pose
        self.mp_drawing = mp.solutions.drawing_utils
        self.pose = self.mp_pose.Pose(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )

    def process_video(self, video_source=0):
        """ãƒ“ãƒ‡ã‚ªã‹ã‚‰å§¿å‹¢æ¨å®š"""
        cap = cv2.VideoCapture(video_source)

        fps_time = 0
        frame_count = 0

        while cap.isOpened():
            success, frame = cap.read()
            if not success:
                break

            # BGR to RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # å§¿å‹¢æ¨å®š
            results = self.pose.process(frame_rgb)

            # æç”»
            if results.pose_landmarks:
                self.mp_drawing.draw_landmarks(
                    frame,
                    results.pose_landmarks,
                    self.mp_pose.POSE_CONNECTIONS
                )

            # FPSè¨ˆç®—
            frame_count += 1
            if time.time() - fps_time > 1:
                fps = frame_count / (time.time() - fps_time)
                fps_time = time.time()
                frame_count = 0

                cv2.putText(frame, f'FPS: {fps:.1f}',
                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX,
                           1, (0, 255, 0), 2)

            cv2.imshow('Pose Estimation', frame)

            if cv2.waitKey(5) & 0xFF == ord('q'):
                break

        cap.release()
        cv2.destroyAllWindows()

# å®Ÿè¡Œ
estimator = PoseEstimator()
# Webã‚«ãƒ¡ãƒ©ã®å ´åˆ: estimator.process_video(0)
# å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ: estimator.process_video('video.mp4')
</code></pre>

<hr>

<h2>5.3 OCRï¼ˆå…‰å­¦æ–‡å­—èªè­˜ï¼‰</h2>

<h3>OCRæŠ€è¡“ã®æ¦‚è¦</h3>

<p><strong>OCRï¼ˆOptical Character Recognitionï¼‰</strong>ã¯ã€ç”»åƒã‹ã‚‰æ–‡å­—ã‚’èªè­˜ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹æŠ€è¡“ã§ã™ã€‚</p>

<table>
<thead>
<tr>
<th>ãƒ©ã‚¤ãƒ–ãƒ©ãƒª</th>
<th>ç‰¹å¾´</th>
<th>æ—¥æœ¬èªå¯¾å¿œ</th>
<th>ç²¾åº¦</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Tesseract</strong></td>
<td>ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã€å¤šè¨€èª</td>
<td>â—‹</td>
<td>ä¸­</td>
</tr>
<tr>
<td><strong>EasyOCR</strong></td>
<td>ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã€ç°¡å˜</td>
<td>â—</td>
<td>é«˜</td>
</tr>
<tr>
<td><strong>PaddleOCR</strong></td>
<td>é«˜é€Ÿã€é«˜ç²¾åº¦</td>
<td>â—</td>
<td>æœ€é«˜</td>
</tr>
</tbody>
</table>

<h3>EasyOCRã«ã‚ˆã‚‹æ–‡å­—èªè­˜</h3>

<pre><code class="language-python">import easyocr
import cv2
import matplotlib.pyplot as plt
import numpy as np

# EasyOCRãƒªãƒ¼ãƒ€ãƒ¼ã®åˆæœŸåŒ–ï¼ˆæ—¥æœ¬èªã¨è‹±èªï¼‰
reader = easyocr.Reader(['ja', 'en'], gpu=True)

# ç”»åƒã®èª­ã¿è¾¼ã¿
image_path = 'japanese_text.jpg'
image = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# OCRå®Ÿè¡Œ
results = reader.readtext(image_path)

print("=== OCRçµæœ ===")
print(f"æ¤œå‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸ: {len(results)}å€‹\n")

# çµæœã®æç”»
image_with_boxes = image_rgb.copy()

for i, (bbox, text, confidence) in enumerate(results):
    # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®é ‚ç‚¹
    top_left = tuple(map(int, bbox[0]))
    bottom_right = tuple(map(int, bbox[2]))

    # çŸ©å½¢ã‚’æç”»
    cv2.rectangle(image_with_boxes, top_left, bottom_right, (0, 255, 0), 2)

    # ãƒ†ã‚­ã‚¹ãƒˆã¨ä¿¡é ¼åº¦ã‚’è¡¨ç¤º
    print(f"é ˜åŸŸ {i+1}:")
    print(f"  ãƒ†ã‚­ã‚¹ãƒˆ: {text}")
    print(f"  ä¿¡é ¼åº¦: {confidence:.3f}")
    print(f"  ä½ç½®: {top_left} - {bottom_right}\n")

    # ç”»åƒä¸Šã«ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
    cv2.putText(image_with_boxes, text,
                (top_left[0], top_left[1] - 10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

axes[0].imshow(image_rgb)
axes[0].set_title('å…ƒç”»åƒ', fontsize=14)
axes[0].axis('off')

axes[1].imshow(image_with_boxes)
axes[1].set_title(f'OCRçµæœ ({len(results)}å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆ)', fontsize=14)
axes[1].axis('off')

plt.tight_layout()
plt.show()

# å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
full_text = '\n'.join([text for _, text, _ in results])
print("=== æŠ½å‡ºã•ã‚ŒãŸå…¨ãƒ†ã‚­ã‚¹ãƒˆ ===")
print(full_text)
</code></pre>

<h3>PaddleOCRã«ã‚ˆã‚‹é«˜ç²¾åº¦èªè­˜</h3>

<pre><code class="language-python">from paddleocr import PaddleOCR
import cv2
import matplotlib.pyplot as plt

# PaddleOCRã®åˆæœŸåŒ–ï¼ˆæ—¥æœ¬èªï¼‰
ocr = PaddleOCR(lang='japan', use_angle_cls=True, use_gpu=True)

# ç”»åƒã®èª­ã¿è¾¼ã¿
image_path = 'document.jpg'

# OCRå®Ÿè¡Œ
result = ocr.ocr(image_path, cls=True)

print("=== PaddleOCRçµæœ ===\n")

# çµæœã®å‡¦ç†
image = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
image_with_boxes = image_rgb.copy()

for line in result[0]:
    bbox = line[0]
    text = line[1][0]
    confidence = line[1][1]

    # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’æç”»
    points = np.array(bbox, dtype=np.int32)
    cv2.polylines(image_with_boxes, [points], True, (0, 255, 0), 2)

    print(f"ãƒ†ã‚­ã‚¹ãƒˆ: {text}")
    print(f"ä¿¡é ¼åº¦: {confidence:.3f}\n")

# å¯è¦–åŒ–
plt.figure(figsize=(12, 8))
plt.imshow(image_with_boxes)
plt.title('PaddleOCRçµæœ', fontsize=14)
plt.axis('off')
plt.tight_layout()
plt.show()
</code></pre>

<h3>ãƒ¬ã‚·ãƒ¼ãƒˆãƒ»é ˜åæ›¸ã®OCRå‡¦ç†</h3>

<pre><code class="language-python">import easyocr
import cv2
import re
from datetime import datetime

class ReceiptOCR:
    """ãƒ¬ã‚·ãƒ¼ãƒˆãƒ»é ˜åæ›¸ã®OCRå‡¦ç†"""

    def __init__(self, languages=['ja', 'en']):
        self.reader = easyocr.Reader(languages)

    def process_receipt(self, image_path):
        """ãƒ¬ã‚·ãƒ¼ãƒˆã‚’å‡¦ç†"""
        # OCRå®Ÿè¡Œ
        results = self.reader.readtext(image_path)

        # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’æŠ½å‡º
        texts = [text for _, text, _ in results]

        # æƒ…å ±ã‚’æŠ½å‡º
        receipt_info = {
            'store_name': self._extract_store_name(texts),
            'date': self._extract_date(texts),
            'total_amount': self._extract_total(texts),
            'items': self._extract_items(texts)
        }

        return receipt_info

    def _extract_store_name(self, texts):
        """åº—èˆ—åã‚’æŠ½å‡ºï¼ˆæœ€åˆã®è¡Œï¼‰"""
        return texts[0] if texts else "ä¸æ˜"

    def _extract_date(self, texts):
        """æ—¥ä»˜ã‚’æŠ½å‡º"""
        date_patterns = [
            r'\d{4}[/-]\d{1,2}[/-]\d{1,2}',
            r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥'
        ]

        for text in texts:
            for pattern in date_patterns:
                match = re.search(pattern, text)
                if match:
                    return match.group()
        return "ä¸æ˜"

    def _extract_total(self, texts):
        """åˆè¨ˆé‡‘é¡ã‚’æŠ½å‡º"""
        total_keywords = ['åˆè¨ˆ', 'è¨ˆ', 'TOTAL', 'Â¥']

        for i, text in enumerate(texts):
            for keyword in total_keywords:
                if keyword in text:
                    # é‡‘é¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
                    amount_match = re.search(r'(\d{1,3}(?:,\d{3})*|\d+)', texts[i])
                    if amount_match:
                        return int(amount_match.group().replace(',', ''))
        return 0

    def _extract_items(self, texts):
        """å•†å“ãƒªã‚¹ãƒˆã‚’æŠ½å‡º"""
        items = []

        # å•†å“è¡Œã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå•†å“å + é‡‘é¡ï¼‰
        for text in texts:
            # é‡‘é¡ã‚’å«ã‚€è¡Œã‚’æ¤œå‡º
            if re.search(r'\d{1,3}(?:,\d{3})*', text):
                items.append(text)

        return items[:10]  # æœ€å¤§10ä»¶

# ä½¿ç”¨ä¾‹
ocr_system = ReceiptOCR(languages=['ja', 'en'])
receipt_info = ocr_system.process_receipt('receipt.jpg')

print("=== ãƒ¬ã‚·ãƒ¼ãƒˆè§£æçµæœ ===")
print(f"åº—èˆ—å: {receipt_info['store_name']}")
print(f"æ—¥ä»˜: {receipt_info['date']}")
print(f"åˆè¨ˆé‡‘é¡: Â¥{receipt_info['total_amount']:,}")
print(f"\nå•†å“ãƒªã‚¹ãƒˆ:")
for i, item in enumerate(receipt_info['items'], 1):
    print(f"  {i}. {item}")
</code></pre>

<hr>

<h2>5.4 ç”»åƒç”Ÿæˆãƒ»ç·¨é›†</h2>

<h3>è¶…è§£åƒï¼ˆSuper-Resolutionï¼‰</h3>

<p><strong>è¶…è§£åƒ</strong>ã¯ã€ä½è§£åƒåº¦ç”»åƒã‚’é«˜è§£åƒåº¦åŒ–ã™ã‚‹æŠ€è¡“ã§ã™ã€‚</p>

<pre><code class="language-python">import cv2
import numpy as np
import matplotlib.pyplot as plt
from cv2 import dnn_superres

# è¶…è§£åƒãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
sr = dnn_superres.DnnSuperResImpl_create()

# ESPCN ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆ4å€æ‹¡å¤§ï¼‰
model_path = "ESPCN_x4.pb"
sr.readModel(model_path)
sr.setModel("espcn", 4)

# ä½è§£åƒåº¦ç”»åƒã®èª­ã¿è¾¼ã¿
low_res_image = cv2.imread('low_resolution.jpg')

print(f"=== è¶…è§£åƒå‡¦ç† ===")
print(f"å…ƒã®ç”»åƒã‚µã‚¤ã‚º: {low_res_image.shape[:2]}")

# è¶…è§£åƒå®Ÿè¡Œ
high_res_image = sr.upsample(low_res_image)

print(f"å‡¦ç†å¾Œã®ã‚µã‚¤ã‚º: {high_res_image.shape[:2]}")

# ãƒã‚¤ã‚­ãƒ¥ãƒ¼ãƒ“ãƒƒã‚¯è£œé–“ã¨ã®æ¯”è¼ƒ
bicubic_image = cv2.resize(low_res_image,
                           (low_res_image.shape[1] * 4, low_res_image.shape[0] * 4),
                           interpolation=cv2.INTER_CUBIC)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

axes[0].imshow(cv2.cvtColor(low_res_image, cv2.COLOR_BGR2RGB))
axes[0].set_title('å…ƒç”»åƒï¼ˆä½è§£åƒåº¦ï¼‰', fontsize=14)
axes[0].axis('off')

axes[1].imshow(cv2.cvtColor(bicubic_image, cv2.COLOR_BGR2RGB))
axes[1].set_title('ãƒã‚¤ã‚­ãƒ¥ãƒ¼ãƒ“ãƒƒã‚¯è£œé–“', fontsize=14)
axes[1].axis('off')

axes[2].imshow(cv2.cvtColor(high_res_image, cv2.COLOR_BGR2RGB))
axes[2].set_title('è¶…è§£åƒï¼ˆESPCNï¼‰', fontsize=14)
axes[2].axis('off')

plt.tight_layout()
plt.show()

# ç”»è³ªè©•ä¾¡
from skimage.metrics import peak_signal_noise_ratio as psnr
from skimage.metrics import structural_similarity as ssim

# å‚ç…§ç”»åƒãŒã‚ã‚‹å ´åˆ
if 'ground_truth.jpg' in os.listdir():
    gt_image = cv2.imread('ground_truth.jpg')

    psnr_bicubic = psnr(gt_image, bicubic_image)
    psnr_sr = psnr(gt_image, high_res_image)

    ssim_bicubic = ssim(gt_image, bicubic_image, multichannel=True)
    ssim_sr = ssim(gt_image, high_res_image, multichannel=True)

    print(f"\n=== ç”»è³ªè©•ä¾¡ ===")
    print(f"PSNR - ãƒã‚¤ã‚­ãƒ¥ãƒ¼ãƒ“ãƒƒã‚¯: {psnr_bicubic:.2f} dB")
    print(f"PSNR - è¶…è§£åƒ: {psnr_sr:.2f} dB")
    print(f"SSIM - ãƒã‚¤ã‚­ãƒ¥ãƒ¼ãƒ“ãƒƒã‚¯: {ssim_bicubic:.4f}")
    print(f"SSIM - è¶…è§£åƒ: {ssim_sr:.4f}")
</code></pre>

<h3>èƒŒæ™¯é™¤å»ï¼ˆBackground Removalï¼‰</h3>

<pre><code class="language-python">import cv2
import numpy as np
from rembg import remove
from PIL import Image
import matplotlib.pyplot as plt

# ç”»åƒã®èª­ã¿è¾¼ã¿
input_path = 'person.jpg'
input_image = Image.open(input_path)

print("=== èƒŒæ™¯é™¤å»å‡¦ç† ===")
print(f"å…ƒã®ç”»åƒã‚µã‚¤ã‚º: {input_image.size}")

# èƒŒæ™¯é™¤å»
output_image = remove(input_image)

print("âœ“ èƒŒæ™¯é™¤å»å®Œäº†")

# NumPyé…åˆ—ã«å¤‰æ›
input_array = np.array(input_image)
output_array = np.array(output_image)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# å…ƒç”»åƒ
axes[0].imshow(input_array)
axes[0].set_title('å…ƒç”»åƒ', fontsize=14)
axes[0].axis('off')

# èƒŒæ™¯é™¤å»å¾Œï¼ˆé€æ˜èƒŒæ™¯ï¼‰
axes[1].imshow(output_array)
axes[1].set_title('èƒŒæ™¯é™¤å»å¾Œ', fontsize=14)
axes[1].axis('off')

# æ–°ã—ã„èƒŒæ™¯ã¨åˆæˆ
# ç·‘è‰²ã®èƒŒæ™¯ã‚’ä½œæˆ
green_background = np.zeros_like(output_array)
green_background[:, :, 1] = 255  # ç·‘ãƒãƒ£ãƒ³ãƒãƒ«
green_background[:, :, 3] = 255  # ã‚¢ãƒ«ãƒ•ã‚¡ãƒãƒ£ãƒ³ãƒãƒ«

# ã‚¢ãƒ«ãƒ•ã‚¡ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°
alpha = output_array[:, :, 3:4] / 255.0
composited = (output_array[:, :, :3] * alpha +
              green_background[:, :, :3] * (1 - alpha)).astype(np.uint8)

axes[2].imshow(composited)
axes[2].set_title('æ–°ã—ã„èƒŒæ™¯ã¨åˆæˆ', fontsize=14)
axes[2].axis('off')

plt.tight_layout()
plt.show()

# ä¿å­˜
output_image.save('output_no_bg.png')
print("âœ“ çµæœã‚’ä¿å­˜: output_no_bg.png")
</code></pre>

<h3>ã‚¹ã‚¿ã‚¤ãƒ«è»¢é€ï¼ˆNeural Style Transferï¼‰</h3>

<pre><code class="language-python">import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

def load_image(image_path, max_dim=512):
    """ç”»åƒã‚’èª­ã¿è¾¼ã¿ã€å‰å‡¦ç†"""
    img = Image.open(image_path)
    img = img.convert('RGB')

    # ãƒªã‚µã‚¤ã‚º
    scale = max_dim / max(img.size)
    new_size = tuple([int(dim * scale) for dim in img.size])
    img = img.resize(new_size, Image.LANCZOS)

    # NumPyé…åˆ—ã«å¤‰æ›
    img = np.array(img)
    img = img[np.newaxis, :]

    return img

def style_transfer(content_path, style_path):
    """ã‚¹ã‚¿ã‚¤ãƒ«è»¢é€ã‚’å®Ÿè¡Œ"""
    print("=== Neural Style Transfer ===")

    # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    print("ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
    hub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')

    # ç”»åƒã®èª­ã¿è¾¼ã¿
    content_image = load_image(content_path)
    style_image = load_image(style_path)

    print(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”»åƒ: {content_image.shape}")
    print(f"ã‚¹ã‚¿ã‚¤ãƒ«ç”»åƒ: {style_image.shape}")

    # ã‚¹ã‚¿ã‚¤ãƒ«è»¢é€ã®å®Ÿè¡Œ
    print("ã‚¹ã‚¿ã‚¤ãƒ«è»¢é€ã‚’å®Ÿè¡Œä¸­...")
    stylized_image = hub_model(tf.constant(content_image), tf.constant(style_image))[0]

    return content_image[0], style_image[0], stylized_image.numpy()[0]

# å®Ÿè¡Œ
content_img, style_img, stylized_img = style_transfer(
    'content.jpg',  # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”»åƒ
    'style.jpg'     # ã‚¹ã‚¿ã‚¤ãƒ«ç”»åƒï¼ˆä¾‹ï¼šã‚´ãƒƒãƒ›ã®çµµç”»ï¼‰
)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

axes[0].imshow(content_img.astype(np.uint8))
axes[0].set_title('ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”»åƒ', fontsize=14)
axes[0].axis('off')

axes[1].imshow(style_img.astype(np.uint8))
axes[1].set_title('ã‚¹ã‚¿ã‚¤ãƒ«ç”»åƒ', fontsize=14)
axes[1].axis('off')

axes[2].imshow(stylized_img)
axes[2].set_title('ã‚¹ã‚¿ã‚¤ãƒ«è»¢é€çµæœ', fontsize=14)
axes[2].axis('off')

plt.tight_layout()
plt.show()

print("âœ“ ã‚¹ã‚¿ã‚¤ãƒ«è»¢é€å®Œäº†")
</code></pre>

<hr>

<h2>5.5 ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ</h2>

<h3>ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯CVã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³</h3>

<pre><code class="language-python">import cv2
import numpy as np
from mtcnn import MTCNN
import mediapipe as mp
import easyocr
from deepface import DeepFace

class MultiTaskCVSystem:
    """ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        # å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®åˆæœŸåŒ–
        self.face_detector = MTCNN()
        self.pose_estimator = mp.solutions.pose.Pose()
        self.ocr_reader = easyocr.Reader(['ja', 'en'])
        self.mp_drawing = mp.solutions.drawing_utils

        print("âœ“ ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯CVã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")

    def process_image(self, image_path, tasks=['face', 'pose', 'ocr']):
        """ç”»åƒã‚’å‡¦ç†"""
        # ç”»åƒã®èª­ã¿è¾¼ã¿
        image = cv2.imread(image_path)
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        results = {}

        # é¡”æ¤œå‡º
        if 'face' in tasks:
            print("é¡”æ¤œå‡ºã‚’å®Ÿè¡Œä¸­...")
            faces = self.face_detector.detect_faces(image_rgb)
            results['faces'] = faces
            print(f"  âœ“ {len(faces)}å€‹ã®é¡”ã‚’æ¤œå‡º")

        # å§¿å‹¢æ¨å®š
        if 'pose' in tasks:
            print("å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œä¸­...")
            pose_results = self.pose_estimator.process(image_rgb)
            results['pose'] = pose_results
            if pose_results.pose_landmarks:
                print(f"  âœ“ å§¿å‹¢ã‚’æ¤œå‡º")
            else:
                print(f"  âœ— å§¿å‹¢ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

        # OCR
        if 'ocr' in tasks:
            print("OCRã‚’å®Ÿè¡Œä¸­...")
            ocr_results = self.ocr_reader.readtext(image_path)
            results['ocr'] = ocr_results
            print(f"  âœ“ {len(ocr_results)}å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸã‚’æ¤œå‡º")

        return image_rgb, results

    def visualize_results(self, image, results):
        """çµæœã‚’å¯è¦–åŒ–"""
        output = image.copy()

        # é¡”æ¤œå‡ºçµæœã®æç”»
        if 'faces' in results:
            for face in results['faces']:
                x, y, w, h = face['box']
                cv2.rectangle(output, (x, y), (x+w, y+h), (0, 255, 0), 2)
                cv2.putText(output, 'Face', (x, y-10),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

        # å§¿å‹¢æ¨å®šçµæœã®æç”»
        if 'pose' in results and results['pose'].pose_landmarks:
            self.mp_drawing.draw_landmarks(
                output,
                results['pose'].pose_landmarks,
                mp.solutions.pose.POSE_CONNECTIONS
            )

        # OCRçµæœã®æç”»
        if 'ocr' in results:
            for bbox, text, conf in results['ocr']:
                top_left = tuple(map(int, bbox[0]))
                bottom_right = tuple(map(int, bbox[2]))
                cv2.rectangle(output, top_left, bottom_right, (255, 0, 0), 2)

        return output

# ä½¿ç”¨ä¾‹
system = MultiTaskCVSystem()

# ç”»åƒã‚’å‡¦ç†
image, results = system.process_image('test_image.jpg',
                                      tasks=['face', 'pose', 'ocr'])

# çµæœã‚’å¯è¦–åŒ–
output_image = system.visualize_results(image, results)

# è¡¨ç¤º
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(15, 6))

axes[0].imshow(image)
axes[0].set_title('å…ƒç”»åƒ', fontsize=14)
axes[0].axis('off')

axes[1].imshow(output_image)
axes[1].set_title('å‡¦ç†çµæœï¼ˆé¡”ãƒ»å§¿å‹¢ãƒ»ãƒ†ã‚­ã‚¹ãƒˆï¼‰', fontsize=14)
axes[1].axis('off')

plt.tight_layout()
plt.show()
</code></pre>

<h3>ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–ã¨ãƒ‡ãƒ—ãƒ­ã‚¤</h3>

<h4>ONNXã¸ã®å¤‰æ›</h4>

<pre><code class="language-python">import torch
import torch.onnx
import onnxruntime as ort
import numpy as np

# PyTorchãƒ¢ãƒ‡ãƒ«ã‚’ONNXã«å¤‰æ›
def convert_to_onnx(model, input_shape, output_path):
    """PyTorchãƒ¢ãƒ‡ãƒ«ã‚’ONNXã«å¤‰æ›"""
    print("=== ONNXå¤‰æ› ===")

    # ãƒ€ãƒŸãƒ¼å…¥åŠ›
    dummy_input = torch.randn(input_shape)

    # ONNXå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    torch.onnx.export(
        model,
        dummy_input,
        output_path,
        export_params=True,
        opset_version=11,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )

    print(f"âœ“ ONNXãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {output_path}")

# ONNXãƒ¢ãƒ‡ãƒ«ã®æ¨è«–
class ONNXInference:
    """ONNX Runtimeæ¨è«–"""

    def __init__(self, model_path):
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name
        self.output_name = self.session.get_outputs()[0].name

        print(f"âœ“ ONNXãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿: {model_path}")

    def predict(self, input_data):
        """æ¨è«–ã‚’å®Ÿè¡Œ"""
        outputs = self.session.run(
            [self.output_name],
            {self.input_name: input_data}
        )
        return outputs[0]

# é€Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
import time

def benchmark_model(model, input_data, num_iterations=100):
    """ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–é€Ÿåº¦ã‚’æ¸¬å®š"""
    print(f"\n=== ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ({num_iterations}å›) ===")

    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
    for _ in range(10):
        _ = model.predict(input_data)

    # æ¸¬å®š
    start_time = time.time()
    for _ in range(num_iterations):
        _ = model.predict(input_data)
    end_time = time.time()

    avg_time = (end_time - start_time) / num_iterations
    fps = 1.0 / avg_time

    print(f"å¹³å‡æ¨è«–æ™‚é–“: {avg_time*1000:.2f} ms")
    print(f"FPS: {fps:.1f}")

    return avg_time, fps

# ä½¿ç”¨ä¾‹
# PyTorchãƒ¢ãƒ‡ãƒ«ã‚’æƒ³å®š
# model = YourPyTorchModel()
# convert_to_onnx(model, (1, 3, 224, 224), 'model.onnx')

# ONNXæ¨è«–
onnx_model = ONNXInference('model.onnx')
test_input = np.random.randn(1, 3, 224, 224).astype(np.float32)
avg_time, fps = benchmark_model(onnx_model, test_input)
</code></pre>

<h3>ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹æœ€é©åŒ–</h3>

<pre><code class="language-python">import tensorflow as tf
import numpy as np

class ModelOptimizer:
    """ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ãƒ„ãƒ¼ãƒ«"""

    @staticmethod
    def quantize_model(model_path, output_path):
        """é‡å­åŒ–ï¼ˆINT8ï¼‰ã§ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’å‰Šæ¸›"""
        print("=== ãƒ¢ãƒ‡ãƒ«é‡å­åŒ– ===")

        # TFLiteã‚³ãƒ³ãƒãƒ¼ã‚¿ãƒ¼ã®ä½œæˆ
        converter = tf.lite.TFLiteConverter.from_saved_model(model_path)

        # é‡å­åŒ–è¨­å®š
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_types = [tf.int8]

        # å¤‰æ›
        tflite_model = converter.convert()

        # ä¿å­˜
        with open(output_path, 'wb') as f:
            f.write(tflite_model)

        # ã‚µã‚¤ã‚ºæ¯”è¼ƒ
        import os
        original_size = os.path.getsize(model_path) / (1024 * 1024)
        quantized_size = os.path.getsize(output_path) / (1024 * 1024)

        print(f"å…ƒã®ã‚µã‚¤ã‚º: {original_size:.2f} MB")
        print(f"é‡å­åŒ–å¾Œ: {quantized_size:.2f} MB")
        print(f"åœ§ç¸®ç‡: {(1 - quantized_size/original_size)*100:.1f}%")

        return output_path

    @staticmethod
    def prune_model(model, target_sparsity=0.5):
        """ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã§ä¸è¦ãªé‡ã¿ã‚’å‰Šé™¤"""
        import tensorflow_model_optimization as tfmot

        print(f"=== ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (ç›®æ¨™: {target_sparsity*100}%ç–) ===")

        # ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
        pruning_params = {
            'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(
                initial_sparsity=0.0,
                final_sparsity=target_sparsity,
                begin_step=0,
                end_step=1000
            )
        }

        # ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°é©ç”¨
        model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(
            model, **pruning_params
        )

        print("âœ“ ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã‚’é©ç”¨")

        return model_for_pruning

# ãƒ‡ãƒã‚¤ã‚¹åˆ¥ã®æœ€é©åŒ–æˆ¦ç•¥
optimization_strategies = {
    'Mobile': {
        'quantization': 'INT8',
        'target_size': '< 10 MB',
        'target_fps': '> 30',
        'framework': 'TFLite'
    },
    'Edge (Raspberry Pi)': {
        'quantization': 'INT8',
        'target_size': '< 50 MB',
        'target_fps': '> 10',
        'framework': 'TFLite or ONNX'
    },
    'Cloud': {
        'quantization': 'FP16 or FP32',
        'target_size': 'ä»»æ„',
        'target_fps': '> 100',
        'framework': 'TensorRT or ONNX'
    }
}

print("\n=== ãƒ‡ãƒã‚¤ã‚¹åˆ¥æœ€é©åŒ–æˆ¦ç•¥ ===")
for device, strategy in optimization_strategies.items():
    print(f"\n{device}:")
    for key, value in strategy.items():
        print(f"  {key}: {value}")
</code></pre>

<hr>

<h2>5.6 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>é¡”èªè­˜ãƒ»æ¤œå‡º</strong></p>
<ul>
<li>MTCNNã€RetinaFaceã«ã‚ˆã‚‹é«˜ç²¾åº¦ãªé¡”æ¤œå‡º</li>
<li>DeepFaceã‚’ä½¿ã£ãŸé¡”èªè­˜ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰</li>
<li>é¡”ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ç®¡ç†ã¨ç…§åˆ</li>
</ul></li>

<li><p><strong>å§¿å‹¢æ¨å®š</strong></p>
<ul>
<li>MediaPipe Poseã«ã‚ˆã‚‹ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ¤œå‡º</li>
<li>é–¢ç¯€è§’åº¦ã®è¨ˆç®—ã¨å‹•ä½œèªè­˜</li>
<li>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å§¿å‹¢æ¨å®šã®å®Ÿè£…</li>
</ul></li>

<li><p><strong>OCRæŠ€è¡“</strong></p>
<ul>
<li>EasyOCRã€PaddleOCRã«ã‚ˆã‚‹æ–‡å­—èªè­˜</li>
<li>æ—¥æœ¬èªã‚’å«ã‚€å¤šè¨€èªå¯¾å¿œ</li>
<li>ãƒ¬ã‚·ãƒ¼ãƒˆãƒ»æ–‡æ›¸ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æŠ½å‡º</li>
</ul></li>

<li><p><strong>ç”»åƒç”Ÿæˆãƒ»ç·¨é›†</strong></p>
<ul>
<li>è¶…è§£åƒã«ã‚ˆã‚‹ç”»è³ªå‘ä¸Š</li>
<li>èƒŒæ™¯é™¤å»ã¨ã‚¢ãƒ«ãƒ•ã‚¡ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°</li>
<li>Neural Style Transferã«ã‚ˆã‚‹èŠ¸è¡“çš„å¤‰æ›</li>
</ul></li>

<li><p><strong>ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰é–‹ç™º</strong></p>
<ul>
<li>ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯CVã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆ</li>
<li>ONNXå¤‰æ›ã¨ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–</li>
<li>ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤</li>
</ul></li>
</ol>

<h3>å®Ÿç”¨åŒ–ã®ãƒã‚¤ãƒ³ãƒˆ</h3>

<table>
<thead>
<tr>
<th>ã‚¿ã‚¹ã‚¯</th>
<th>æ¨å¥¨ãƒ¢ãƒ‡ãƒ«</th>
<th>æ³¨æ„ç‚¹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>é¡”æ¤œå‡º</strong></td>
<td>MTCNN, RetinaFace</td>
<td>ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ã€åŒæ„å–å¾—</td>
</tr>
<tr>
<td><strong>é¡”èªè­˜</strong></td>
<td>FaceNet, ArcFace</td>
<td>ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€èª¤èªè­˜ãƒªã‚¹ã‚¯</td>
</tr>
<tr>
<td><strong>å§¿å‹¢æ¨å®š</strong></td>
<td>MediaPipe, OpenPose</td>
<td>ç…§æ˜ãƒ»é®è”½ã¸ã®å¯¾å¿œ</td>
</tr>
<tr>
<td><strong>OCR</strong></td>
<td>PaddleOCR, EasyOCR</td>
<td>ãƒ•ã‚©ãƒ³ãƒˆãƒ»ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®å¤šæ§˜æ€§</td>
</tr>
<tr>
<td><strong>ç”»åƒç”Ÿæˆ</strong></td>
<td>GANs, Diffusion</td>
<td>å€«ç†çš„ä½¿ç”¨ã€è‘—ä½œæ¨©</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>

<p>ã•ã‚‰ã«å­¦ç¿’ã‚’é€²ã‚ã‚‹ãŸã‚ã«ï¼š</p>
<ul>
<li>3Då†æ§‹æˆã¨SLAMæŠ€è¡“</li>
<li>ãƒ“ãƒ‡ã‚ªè§£æã¨ç‰©ä½“è¿½è·¡</li>
<li>è‡ªå‹•é‹è»¢ã®ãŸã‚ã®CV</li>
<li>åŒ»ç™‚ç”»åƒè§£æ</li>
<li>ç”ŸæˆAIï¼ˆStable Diffusionã€DALL-Eï¼‰</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>MTCNNã¨RetinaFaceã®é•ã„ã‚’èª¬æ˜ã—ã€ãã‚Œãã‚Œã‚’ã©ã®ã‚ˆã†ãªå ´é¢ã§ä½¿ã†ã¹ãã‹è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>MTCNNï¼ˆMulti-task Cascaded Convolutional Networksï¼‰</strong>ï¼š</p>
<ul>
<li>æ§‹é€ : 3æ®µéšã®ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰CNNï¼ˆP-Net, R-Net, O-Netï¼‰</li>
<li>æ©Ÿèƒ½: é¡”æ¤œå‡º + 5ç‚¹ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ï¼ˆç›®ã€é¼»ã€å£è§’ï¼‰</li>
<li>é€Ÿåº¦: ä¸­é€Ÿï¼ˆCPUå¯ï¼‰</li>
<li>ç²¾åº¦: é«˜ï¼ˆç‰¹ã«å°ã•ã„é¡”ã«å¼·ã„ï¼‰</li>
</ul>

<p><strong>RetinaFace</strong>ï¼š</p>
<ul>
<li>æ§‹é€ : ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ†ãƒ¼ã‚¸æ¤œå‡ºå™¨ï¼ˆRetinaNetãƒ™ãƒ¼ã‚¹ï¼‰</li>
<li>æ©Ÿèƒ½: é¡”æ¤œå‡º + 5ç‚¹ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ + å¯†ãª3Dé¡”ãƒ¡ãƒƒã‚·ãƒ¥</li>
<li>é€Ÿåº¦: GPUå¿…é ˆã€ã‚„ã‚„é…ã„</li>
<li>ç²¾åº¦: æœ€é«˜ï¼ˆé®è”½ã‚„è§’åº¦å¤‰åŒ–ã«å¼·ã„ï¼‰</li>
</ul>

<p><strong>ä½¿ã„åˆ†ã‘</strong>ï¼š</p>

<table>
<thead>
<tr>
<th>å ´é¢</th>
<th>æ¨å¥¨</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ï¼ˆCPUï¼‰</td>
<td>MTCNN</td>
<td>è»½é‡ã§é«˜é€Ÿ</td>
</tr>
<tr>
<td>é«˜ç²¾åº¦ãŒå¿…é ˆ</td>
<td>RetinaFace</td>
<td>æœ€æ–°æŠ€è¡“ã€é ‘å¥æ€§é«˜</td>
</tr>
<tr>
<td>å°ã•ã„é¡”ã®æ¤œå‡º</td>
<td>MTCNN</td>
<td>ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«å¯¾å¿œ</td>
</tr>
<tr>
<td>é¡”ã®å‘ãæ¨å®šã‚‚å¿…è¦</td>
<td>RetinaFace</td>
<td>3Dæƒ…å ±ã‚’æä¾›</td>
</tr>
<tr>
<td>ãƒ¢ãƒã‚¤ãƒ«ãƒ‡ãƒã‚¤ã‚¹</td>
<td>MTCNN</td>
<td>è¨ˆç®—ã‚³ã‚¹ãƒˆä½</td>
</tr>
</tbody>
</table>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>MediaPipe Poseã‚’ä½¿ã£ã¦ã€ã‚¹ã‚¯ãƒ¯ãƒƒãƒˆã®ãƒ•ã‚©ãƒ¼ãƒ ãƒã‚§ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚è†ã®è§’åº¦ãŒ90åº¦ä»¥ä¸‹ã«ãªã£ãŸã‚‰ã‚«ã‚¦ãƒ³ãƒˆã—ã€æ­£ã—ã„ãƒ•ã‚©ãƒ¼ãƒ ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹æ©Ÿèƒ½ã‚’å«ã‚ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import cv2
import mediapipe as mp
import numpy as np

class SquatFormChecker:
    """ã‚¹ã‚¯ãƒ¯ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ãƒã‚§ãƒƒã‚«ãƒ¼"""

    def __init__(self):
        self.mp_pose = mp.solutions.pose
        self.mp_drawing = mp.solutions.drawing_utils
        self.pose = self.mp_pose.Pose(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )

        self.squat_count = 0
        self.is_down = False
        self.form_feedback = []

    def calculate_angle(self, p1, p2, p3):
        """3ç‚¹ã‹ã‚‰è§’åº¦ã‚’è¨ˆç®—"""
        v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])
        v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])

        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
        angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))
        return np.degrees(angle)

    def check_form(self, landmarks, h, w):
        """ãƒ•ã‚©ãƒ¼ãƒ ã‚’ãƒã‚§ãƒƒã‚¯"""
        # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—
        left_hip = (int(landmarks[23].x * w), int(landmarks[23].y * h))
        left_knee = (int(landmarks[25].x * w), int(landmarks[25].y * h))
        left_ankle = (int(landmarks[27].x * w), int(landmarks[27].y * h))

        left_shoulder = (int(landmarks[11].x * w), int(landmarks[11].y * h))

        # è†ã®è§’åº¦
        knee_angle = self.calculate_angle(left_hip, left_knee, left_ankle)

        # è…°-è‚©ã®è§’åº¦ï¼ˆå§¿å‹¢ãƒã‚§ãƒƒã‚¯ï¼‰
        hip_shoulder_vertical = abs(left_hip[0] - left_shoulder[0])

        # ãƒ•ã‚©ãƒ¼ãƒ åˆ¤å®š
        self.form_feedback = []

        # è†ã®è§’åº¦ãƒã‚§ãƒƒã‚¯
        if knee_angle < 90:
            self.form_feedback.append("âœ“ ååˆ†ãªæ·±ã•")
            if not self.is_down:
                self.is_down = True
        else:
            if self.is_down and knee_angle > 160:
                self.squat_count += 1
                self.is_down = False

        # å§¿å‹¢ãƒã‚§ãƒƒã‚¯
        if hip_shoulder_vertical > 50:
            self.form_feedback.append("âš  ä¸Šä½“ãŒå‰å‚¾ã—ã™ã")
        else:
            self.form_feedback.append("âœ“ ä¸Šä½“ã®å§¿å‹¢è‰¯å¥½")

        # è†ã®ä½ç½®ãƒã‚§ãƒƒã‚¯ï¼ˆã¤ã¾å…ˆã‚ˆã‚Šå‰ã«å‡ºã¦ã„ãªã„ã‹ï¼‰
        if left_knee[0] > left_ankle[0] + 20:
            self.form_feedback.append("âš  è†ãŒã¤ã¾å…ˆã‚ˆã‚Šå‰ã«å‡ºã¦ã„ã¾ã™")
        else:
            self.form_feedback.append("âœ“ è†ã®ä½ç½®è‰¯å¥½")

        return knee_angle

    def process_frame(self, frame):
        """ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†"""
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.pose.process(frame_rgb)

        if results.pose_landmarks:
            # æç”»
            self.mp_drawing.draw_landmarks(
                frame,
                results.pose_landmarks,
                self.mp_pose.POSE_CONNECTIONS
            )

            # ãƒ•ã‚©ãƒ¼ãƒ ãƒã‚§ãƒƒã‚¯
            h, w, _ = frame.shape
            knee_angle = self.check_form(
                results.pose_landmarks.landmark, h, w
            )

            # æƒ…å ±ã‚’è¡¨ç¤º
            cv2.putText(frame, f'Count: {self.squat_count}',
                       (10, 40), cv2.FONT_HERSHEY_SIMPLEX,
                       1, (0, 255, 0), 2)

            cv2.putText(frame, f'Knee Angle: {knee_angle:.1f}',
                       (10, 80), cv2.FONT_HERSHEY_SIMPLEX,
                       0.7, (255, 255, 0), 2)

            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è¡¨ç¤º
            y_offset = 120
            for feedback in self.form_feedback:
                cv2.putText(frame, feedback,
                           (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX,
                           0.6, (255, 255, 255), 2)
                y_offset += 30

        return frame

# ä½¿ç”¨ä¾‹
checker = SquatFormChecker()
cap = cv2.VideoCapture('squat_video.mp4')

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    frame = checker.process_frame(frame)
    cv2.imshow('Squat Form Checker', frame)

    if cv2.waitKey(5) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()

print(f"\n=== æœ€çµ‚çµæœ ===")
print(f"ç·ã‚¹ã‚¯ãƒ¯ãƒƒãƒˆå›æ•°: {checker.squat_count}")
</code></pre>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>è¤‡æ•°ã®OCRãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆTesseractã€EasyOCRã€PaddleOCRï¼‰ã‚’ä½¿ã£ã¦ã€åŒã˜ç”»åƒã‚’å‡¦ç†ã—ã€ç²¾åº¦ã¨é€Ÿåº¦ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚æ—¥æœ¬èªã‚’å«ã‚€ç”»åƒã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import time
import cv2
import pytesseract
import easyocr
from paddleocr import PaddleOCR
import matplotlib.pyplot as plt

class OCRComparison:
    """OCRãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æ¯”è¼ƒ"""

    def __init__(self, image_path):
        self.image_path = image_path
        self.image = cv2.imread(image_path)
        self.results = {}

    def test_tesseract(self):
        """Tesseract OCR"""
        print("=== Tesseract OCR ===")
        start = time.time()

        # æ—¥æœ¬èª+è‹±èªã®è¨­å®š
        text = pytesseract.image_to_string(
            self.image,
            lang='jpn+eng'
        )

        elapsed = time.time() - start

        self.results['Tesseract'] = {
            'text': text,
            'time': elapsed,
            'char_count': len(text)
        }

        print(f"å‡¦ç†æ™‚é–“: {elapsed:.3f}ç§’")
        print(f"æŠ½å‡ºæ–‡å­—æ•°: {len(text)}")
        print(f"ãƒ†ã‚­ã‚¹ãƒˆ:\n{text}\n")

    def test_easyocr(self):
        """EasyOCR"""
        print("=== EasyOCR ===")
        start = time.time()

        reader = easyocr.Reader(['ja', 'en'], gpu=True)
        results = reader.readtext(self.image_path)

        elapsed = time.time() - start

        text = '\n'.join([item[1] for item in results])

        self.results['EasyOCR'] = {
            'text': text,
            'time': elapsed,
            'char_count': len(text),
            'regions': len(results)
        }

        print(f"å‡¦ç†æ™‚é–“: {elapsed:.3f}ç§’")
        print(f"æ¤œå‡ºé ˜åŸŸ: {len(results)}")
        print(f"æŠ½å‡ºæ–‡å­—æ•°: {len(text)}")
        print(f"ãƒ†ã‚­ã‚¹ãƒˆ:\n{text}\n")

    def test_paddleocr(self):
        """PaddleOCR"""
        print("=== PaddleOCR ===")
        start = time.time()

        ocr = PaddleOCR(lang='japan', use_angle_cls=True)
        results = ocr.ocr(self.image_path, cls=True)

        elapsed = time.time() - start

        text = '\n'.join([line[1][0] for line in results[0]])

        self.results['PaddleOCR'] = {
            'text': text,
            'time': elapsed,
            'char_count': len(text),
            'regions': len(results[0])
        }

        print(f"å‡¦ç†æ™‚é–“: {elapsed:.3f}ç§’")
        print(f"æ¤œå‡ºé ˜åŸŸ: {len(results[0])}")
        print(f"æŠ½å‡ºæ–‡å­—æ•°: {len(text)}")
        print(f"ãƒ†ã‚­ã‚¹ãƒˆ:\n{text}\n")

    def compare_all(self):
        """ã™ã¹ã¦ã®OCRã‚’æ¯”è¼ƒ"""
        self.test_tesseract()
        self.test_easyocr()
        self.test_paddleocr()

        # æ¯”è¼ƒçµæœã‚’è¡¨ç¤º
        print("\n=== æ¯”è¼ƒçµæœ ===")
        print(f"{'ãƒ©ã‚¤ãƒ–ãƒ©ãƒª':<15} {'å‡¦ç†æ™‚é–“(ç§’)':<15} {'æ–‡å­—æ•°':<10} {'é ˜åŸŸæ•°':<10}")
        print("-" * 50)

        for name, result in self.results.items():
            regions = result.get('regions', 'N/A')
            print(f"{name:<15} {result['time']:<15.3f} {result['char_count']:<10} {regions:<10}")

        # ã‚°ãƒ©ãƒ•åŒ–
        self.visualize_comparison()

    def visualize_comparison(self):
        """æ¯”è¼ƒçµæœã‚’å¯è¦–åŒ–"""
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))

        # å‡¦ç†æ™‚é–“ã®æ¯”è¼ƒ
        names = list(self.results.keys())
        times = [self.results[name]['time'] for name in names]

        axes[0].bar(names, times, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        axes[0].set_ylabel('å‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰')
        axes[0].set_title('å‡¦ç†æ™‚é–“ã®æ¯”è¼ƒ', fontsize=14)
        axes[0].grid(True, alpha=0.3)

        # æŠ½å‡ºæ–‡å­—æ•°ã®æ¯”è¼ƒ
        char_counts = [self.results[name]['char_count'] for name in names]

        axes[1].bar(names, char_counts, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        axes[1].set_ylabel('æŠ½å‡ºæ–‡å­—æ•°')
        axes[1].set_title('æŠ½å‡ºæ–‡å­—æ•°ã®æ¯”è¼ƒ', fontsize=14)
        axes[1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

# å®Ÿè¡Œ
comparison = OCRComparison('japanese_document.jpg')
comparison.compare_all()
</code></pre>

<p><strong>æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== æ¯”è¼ƒçµæœ ===
ãƒ©ã‚¤ãƒ–ãƒ©ãƒª      å‡¦ç†æ™‚é–“(ç§’)    æ–‡å­—æ•°     é ˜åŸŸæ•°
--------------------------------------------------
Tesseract       2.341          156        N/A
EasyOCR         4.567          162        12
PaddleOCR       1.823          165        15

çµè«–:
- é€Ÿåº¦: PaddleOCR > Tesseract > EasyOCR
- ç²¾åº¦: PaddleOCR â‰ˆ EasyOCR > Tesseractï¼ˆæ—¥æœ¬èªï¼‰
- æ¨å¥¨: é«˜ç²¾åº¦ãŒå¿…è¦ãªã‚‰PaddleOCRã€ãƒãƒ©ãƒ³ã‚¹é‡è¦–ãªã‚‰Tesseract
</code></pre>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>PyTorchã§è¨“ç·´ã—ãŸCNNãƒ¢ãƒ‡ãƒ«ã‚’ONNXå½¢å¼ã«å¤‰æ›ã—ã€æ¨è«–é€Ÿåº¦ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚ã•ã‚‰ã«é‡å­åŒ–ã‚‚é©ç”¨ã—ã€ç²¾åº¦ã¨é€Ÿåº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.onnx
import onnxruntime as ort
import numpy as np
import time

# ã‚µãƒ³ãƒ—ãƒ«CNNãƒ¢ãƒ‡ãƒ«
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64 * 56 * 56, 128)
        self.fc2 = nn.Linear(128, 10)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = x.view(-1, 64 * 56 * 56)
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™
model = SimpleCNN()
model.eval()

dummy_input = torch.randn(1, 3, 224, 224)

print("=== PyTorchãƒ¢ãƒ‡ãƒ« ===")
print(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}")

# 1. PyTorchã§ã®æ¨è«–é€Ÿåº¦æ¸¬å®š
def benchmark_pytorch(model, input_data, iterations=100):
    """PyTorchãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    with torch.no_grad():
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for _ in range(10):
            _ = model(input_data)

        # æ¸¬å®š
        start = time.time()
        for _ in range(iterations):
            _ = model(input_data)
        elapsed = time.time() - start

    return elapsed / iterations

pytorch_time = benchmark_pytorch(model, dummy_input)
print(f"PyTorchæ¨è«–æ™‚é–“: {pytorch_time*1000:.2f} ms")

# 2. ONNXå¤‰æ›
onnx_path = 'model.onnx'
torch.onnx.export(
    model,
    dummy_input,
    onnx_path,
    export_params=True,
    opset_version=11,
    input_names=['input'],
    output_names=['output']
)
print(f"\nâœ“ ONNXãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {onnx_path}")

# 3. ONNX Runtimeæ¨è«–
ort_session = ort.InferenceSession(onnx_path)
input_name = ort_session.get_inputs()[0].name
output_name = ort_session.get_outputs()[0].name

def benchmark_onnx(session, input_data, iterations=100):
    """ONNX Runtimeã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
    for _ in range(10):
        _ = session.run([output_name], {input_name: input_data})

    # æ¸¬å®š
    start = time.time()
    for _ in range(iterations):
        _ = session.run([output_name], {input_name: input_data})
    elapsed = time.time() - start

    return elapsed / iterations

dummy_input_np = dummy_input.numpy()
onnx_time = benchmark_onnx(ort_session, dummy_input_np)
print(f"ONNXæ¨è«–æ™‚é–“: {onnx_time*1000:.2f} ms")

# 4. é‡å­åŒ–ï¼ˆDynamic Quantizationï¼‰
quantized_model = torch.quantization.quantize_dynamic(
    model,
    {nn.Linear, nn.Conv2d},
    dtype=torch.qint8
)

pytorch_quantized_time = benchmark_pytorch(quantized_model, dummy_input)
print(f"\né‡å­åŒ–PyTorchæ¨è«–æ™‚é–“: {pytorch_quantized_time*1000:.2f} ms")

# 5. ç²¾åº¦æ¯”è¼ƒ
test_input = torch.randn(10, 3, 224, 224)

with torch.no_grad():
    output_original = model(test_input)
    output_quantized = quantized_model(test_input)

# ç²¾åº¦ã®å·®
diff = torch.abs(output_original - output_quantized).mean()
print(f"\né‡å­åŒ–ã«ã‚ˆã‚‹å‡ºåŠ›ã®å·®: {diff:.6f}")

# 6. ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºæ¯”è¼ƒ
import os

def get_model_size(filepath):
    """ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’å–å¾—"""
    return os.path.getsize(filepath) / (1024 * 1024)

# PyTorchãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
torch.save(model.state_dict(), 'model_original.pth')
torch.save(quantized_model.state_dict(), 'model_quantized.pth')

original_size = get_model_size('model_original.pth')
quantized_size = get_model_size('model_quantized.pth')
onnx_size = get_model_size(onnx_path)

print(f"\n=== ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºæ¯”è¼ƒ ===")
print(f"ã‚ªãƒªã‚¸ãƒŠãƒ«: {original_size:.2f} MB")
print(f"é‡å­åŒ–: {quantized_size:.2f} MB ({quantized_size/original_size*100:.1f}%)")
print(f"ONNX: {onnx_size:.2f} MB")

# 7. ç·åˆæ¯”è¼ƒ
print("\n=== ç·åˆæ¯”è¼ƒ ===")
results = [
    ('PyTorch (FP32)', pytorch_time*1000, original_size, 1.0),
    ('PyTorch (INT8)', pytorch_quantized_time*1000, quantized_size, diff.item()),
    ('ONNX Runtime', onnx_time*1000, onnx_size, 0.0)
]

print(f"{'ãƒ¢ãƒ‡ãƒ«':<20} {'æ¨è«–æ™‚é–“(ms)':<15} {'ã‚µã‚¤ã‚º(MB)':<15} {'èª¤å·®':<10}")
print("-" * 65)
for name, latency, size, error in results:
    print(f"{name:<20} {latency:<15.2f} {size:<15.2f} {error:<10.6f}")

# ã‚°ãƒ©ãƒ•åŒ–
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# æ¨è«–æ™‚é–“
names = ['PyTorch\n(FP32)', 'PyTorch\n(INT8)', 'ONNX']
times = [r[1] for r in results]
axes[0].bar(names, times, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
axes[0].set_ylabel('æ¨è«–æ™‚é–“ï¼ˆmsï¼‰')
axes[0].set_title('æ¨è«–é€Ÿåº¦ã®æ¯”è¼ƒ', fontsize=14)
axes[0].grid(True, alpha=0.3)

# ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º
sizes = [r[2] for r in results]
axes[1].bar(names, sizes, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
axes[1].set_ylabel('ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºï¼ˆMBï¼‰')
axes[1].set_title('ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®æ¯”è¼ƒ', fontsize=14)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

</details>

<h3>å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯CVã‚·ã‚¹ãƒ†ãƒ ã‚’æ‹¡å¼µã—ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‹•ç”»ã‹ã‚‰ï¼ˆ1ï¼‰é¡”æ¤œå‡ºã€ï¼ˆ2ï¼‰å§¿å‹¢æ¨å®šã€ï¼ˆ3ï¼‰OCRã‚’åŒæ™‚ã«å®Ÿè¡Œã—ã€çµæœã‚’çµ±åˆã—ã¦è¡¨ç¤ºã™ã‚‹ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import cv2
import numpy as np
from mtcnn import MTCNN
import mediapipe as mp
import easyocr
import threading
import queue
import time

class RealtimeMultiTaskCV:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯CVã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        # å„ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.face_detector = MTCNN()
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.mp_drawing = mp.solutions.drawing_utils
        self.ocr_reader = easyocr.Reader(['ja', 'en'], gpu=True)

        # çµæœã‚’ä¿å­˜ã™ã‚‹ã‚­ãƒ¥ãƒ¼
        self.face_queue = queue.Queue(maxsize=1)
        self.pose_queue = queue.Queue(maxsize=1)
        self.ocr_queue = queue.Queue(maxsize=1)

        # æœ€æ–°ã®çµæœ
        self.latest_faces = []
        self.latest_pose = None
        self.latest_ocr = []

        # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚­ãƒƒãƒ—è¨­å®šï¼ˆé‡ã„å‡¦ç†ã‚’é–“å¼•ãï¼‰
        self.face_skip = 5
        self.ocr_skip = 30
        self.frame_count = 0

        print("âœ“ ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯CVã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–")

    def detect_faces_async(self, frame):
        """é¡”æ¤œå‡ºï¼ˆéåŒæœŸï¼‰"""
        def worker():
            try:
                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                faces = self.face_detector.detect_faces(rgb)

                if not self.face_queue.full():
                    self.face_queue.put(faces)
            except Exception as e:
                print(f"é¡”æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")

        thread = threading.Thread(target=worker)
        thread.daemon = True
        thread.start()

    def estimate_pose_async(self, frame):
        """å§¿å‹¢æ¨å®šï¼ˆéåŒæœŸï¼‰"""
        def worker():
            try:
                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = self.pose.process(rgb)

                if not self.pose_queue.full():
                    self.pose_queue.put(results)
            except Exception as e:
                print(f"å§¿å‹¢æ¨å®šã‚¨ãƒ©ãƒ¼: {e}")

        thread = threading.Thread(target=worker)
        thread.daemon = True
        thread.start()

    def detect_text_async(self, frame):
        """OCRï¼ˆéåŒæœŸï¼‰"""
        def worker():
            try:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆEasyOCRã®åˆ¶ç´„ï¼‰
                temp_path = 'temp_frame.jpg'
                cv2.imwrite(temp_path, frame)
                results = self.ocr_reader.readtext(temp_path)

                if not self.ocr_queue.full():
                    self.ocr_queue.put(results)
            except Exception as e:
                print(f"OCRã‚¨ãƒ©ãƒ¼: {e}")

        thread = threading.Thread(target=worker)
        thread.daemon = True
        thread.start()

    def update_results(self):
        """ã‚­ãƒ¥ãƒ¼ã‹ã‚‰æœ€æ–°ã®çµæœã‚’å–å¾—"""
        try:
            if not self.face_queue.empty():
                self.latest_faces = self.face_queue.get_nowait()
        except queue.Empty:
            pass

        try:
            if not self.pose_queue.empty():
                self.latest_pose = self.pose_queue.get_nowait()
        except queue.Empty:
            pass

        try:
            if not self.ocr_queue.empty():
                self.latest_ocr = self.ocr_queue.get_nowait()
        except queue.Empty:
            pass

    def draw_results(self, frame):
        """çµæœã‚’æç”»"""
        output = frame.copy()

        # é¡”æ¤œå‡ºçµæœ
        for face in self.latest_faces:
            x, y, w, h = face['box']
            confidence = face['confidence']

            cv2.rectangle(output, (x, y), (x+w, y+h), (0, 255, 0), 2)
            cv2.putText(output, f'Face: {confidence:.2f}',
                       (x, y-10), cv2.FONT_HERSHEY_SIMPLEX,
                       0.5, (0, 255, 0), 2)

            # ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯
            for point in face['keypoints'].values():
                cv2.circle(output, point, 2, (255, 0, 0), -1)

        # å§¿å‹¢æ¨å®šçµæœ
        if self.latest_pose and self.latest_pose.pose_landmarks:
            self.mp_drawing.draw_landmarks(
                output,
                self.latest_pose.pose_landmarks,
                self.mp_pose.POSE_CONNECTIONS
            )

        # OCRçµæœ
        for bbox, text, conf in self.latest_ocr:
            if conf > 0.5:
                top_left = tuple(map(int, bbox[0]))
                bottom_right = tuple(map(int, bbox[2]))

                cv2.rectangle(output, top_left, bottom_right, (255, 0, 0), 2)
                cv2.putText(output, text[:20],
                           (top_left[0], top_left[1]-10),
                           cv2.FONT_HERSHEY_SIMPLEX,
                           0.5, (255, 0, 0), 2)

        return output

    def add_info_panel(self, frame):
        """æƒ…å ±ãƒ‘ãƒãƒ«ã‚’è¿½åŠ """
        h, w = frame.shape[:2]

        # åŠé€æ˜ãƒ‘ãƒãƒ«
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (350, 150), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)

        # ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
        info = [
            f'Faces: {len(self.latest_faces)}',
            f'Pose: {"Detected" if self.latest_pose and self.latest_pose.pose_landmarks else "None"}',
            f'Text Regions: {len(self.latest_ocr)}',
            f'Frame: {self.frame_count}'
        ]

        y_offset = 40
        for line in info:
            cv2.putText(frame, line, (20, y_offset),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6,
                       (255, 255, 255), 2)
            y_offset += 30

        return frame

    def process_video(self, source=0):
        """ãƒ“ãƒ‡ã‚ªã‚’å‡¦ç†"""
        cap = cv2.VideoCapture(source)

        fps_time = time.time()
        fps = 0

        print("å‡¦ç†ã‚’é–‹å§‹ï¼ˆ'q'ã§çµ‚äº†ï¼‰")

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            self.frame_count += 1

            # é¡”æ¤œå‡ºï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            if self.frame_count % self.face_skip == 0:
                self.detect_faces_async(frame.copy())

            # å§¿å‹¢æ¨å®šï¼ˆæ¯ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰
            self.estimate_pose_async(frame.copy())

            # OCRï¼ˆå¤§å¹…ã«ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            if self.frame_count % self.ocr_skip == 0:
                self.detect_text_async(frame.copy())

            # çµæœã‚’æ›´æ–°
            self.update_results()

            # æç”»
            output = self.draw_results(frame)
            output = self.add_info_panel(output)

            # FPSè¨ˆç®—
            if time.time() - fps_time > 1:
                fps = self.frame_count / (time.time() - fps_time)
                fps_time = time.time()
                self.frame_count = 0

            cv2.putText(output, f'FPS: {fps:.1f}',
                       (w-150, 30), cv2.FONT_HERSHEY_SIMPLEX,
                       0.7, (0, 255, 255), 2)

            cv2.imshow('Multi-Task CV System', output)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

        cap.release()
        cv2.destroyAllWindows()

        print("\n=== å‡¦ç†å®Œäº† ===")
        print(f"ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {self.frame_count}")

# å®Ÿè¡Œ
system = RealtimeMultiTaskCV()

# Webã‚«ãƒ¡ãƒ©ã‹ã‚‰å‡¦ç†
system.process_video(0)

# ã¾ãŸã¯å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«
# system.process_video('video.mp4')
</code></pre>

<p><strong>ã‚·ã‚¹ãƒ†ãƒ ã®ç‰¹å¾´</strong>ï¼š</p>
<ul>
<li>éåŒæœŸå‡¦ç†ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§ã‚’ç¢ºä¿</li>
<li>é‡ã„å‡¦ç†ï¼ˆOCRï¼‰ã¯ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚­ãƒƒãƒ—ã§è»½é‡åŒ–</li>
<li>ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ã§ä¸¦åˆ—å®Ÿè¡Œ</li>
<li>çµ±åˆã•ã‚ŒãŸãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³</li>
</ul>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Zhang, K., et al. (2016). "Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks." <em>IEEE Signal Processing Letters</em>.</li>
<li>Deng, J., et al. (2020). "RetinaFace: Single-Shot Multi-Level Face Localisation in the Wild." <em>CVPR</em>.</li>
<li>Cao, Z., et al. (2017). "Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields." <em>CVPR</em>.</li>
<li>Bazarevsky, V., et al. (2020). "BlazePose: On-device Real-time Body Pose tracking." <em>arXiv preprint</em>.</li>
<li>Baek, J., et al. (2019). "Character Region Awareness for Text Detection." <em>CVPR</em>.</li>
<li>Gatys, L. A., et al. (2016). "Image Style Transfer Using Convolutional Neural Networks." <em>CVPR</em>.</li>
<li>Ledig, C., et al. (2017). "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network." <em>CVPR</em>.</li>
<li>Serengil, S. I., & Ozpinar, A. (2020). "LightFace: A Hybrid Deep Face Recognition Framework." <em>Innovations in Intelligent Systems and Applications Conference</em>.</li>
</ol>

<div class="navigation">
    <a href="chapter4-segmentation.html" class="nav-button">â† å‰ã®ç« : é«˜åº¦ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
