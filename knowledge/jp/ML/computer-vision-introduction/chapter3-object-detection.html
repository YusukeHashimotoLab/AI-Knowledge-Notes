<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬3ç« ï¼šç‰©ä½“æ¤œå‡º - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>
</head>

<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/AI-Knowledge-Notes/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/AI-Knowledge-Notes/knowledge/jp/ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="/AI-Knowledge-Notes/knowledge/jp/ML/computer-vision-introduction/index.html">Computer Vision</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬3ç« ï¼šç‰©ä½“æ¤œå‡º</h1>
            <p class="subtitle">Bounding Boxã€IoUã€NMSã€Two-Stage/One-Stageæ¤œå‡ºå™¨ã€YOLOã€å®Ÿè£…ã¨å¿œç”¨</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 35-40åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 6å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… Bounding Boxã®è¡¨ç¾æ–¹æ³•ã¨IoUã®è¨ˆç®—æ–¹æ³•ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Non-Maximum Suppression (NMS)ã®åŸç†ã¨å®Ÿè£…ã‚’ç¿’å¾—ã™ã‚‹</li>
<li>âœ… ç‰©ä½“æ¤œå‡ºã®è©•ä¾¡æŒ‡æ¨™ï¼ˆmAPã€Precision-Recallï¼‰ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Two-Stageæ¤œå‡ºå™¨ï¼ˆR-CNNã€Fast R-CNNã€Faster R-CNNï¼‰ã®ä»•çµ„ã¿ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>âœ… One-Stageæ¤œå‡ºå™¨ï¼ˆYOLOã€SSDã€RetinaNetï¼‰ã®ç‰¹å¾´ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… YOLOv8ã‚’ä½¿ã£ãŸå®Ÿè·µçš„ãªç‰©ä½“æ¤œå‡ºã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ç‰©ä½“æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã§ãã‚‹</li>
<li>âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œå‡ºã¨ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã®å¿œç”¨ã‚’å®Ÿè£…ã§ãã‚‹</li>
</ul>

<hr>

<h2>3.1 ç‰©ä½“æ¤œå‡ºã®åŸºç¤</h2>

<h3>ç‰©ä½“æ¤œå‡ºã¨ã¯</h3>

<p>ç‰©ä½“æ¤œå‡ºï¼ˆObject Detectionï¼‰ã¯ã€ç”»åƒå†…ã®è¤‡æ•°ã®ç‰©ä½“ã‚’æ¤œå‡ºã—ã€ãã‚Œãã‚Œã®ä½ç½®ï¼ˆBounding Boxï¼‰ã¨ã‚¯ãƒ©ã‚¹ã‚’äºˆæ¸¬ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚ç”»åƒåˆ†é¡ãŒã€Œç”»åƒå…¨ä½“ã«ä½•ãŒå†™ã£ã¦ã„ã‚‹ã‹ã€ã‚’ç­”ãˆã‚‹ã®ã«å¯¾ã—ã€ç‰©ä½“æ¤œå‡ºã¯ã€Œã©ã“ã«ä½•ãŒã‚ã‚‹ã‹ã€ã‚’ç­”ãˆã¾ã™ã€‚</p>

<blockquote>
<p><strong>ç‰©ä½“æ¤œå‡º = ä½ç½®ç‰¹å®šï¼ˆLocalizationï¼‰+ åˆ†é¡ï¼ˆClassificationï¼‰</strong></p>
<p>å…¥åŠ›ç”»åƒã‹ã‚‰ã€å„ç‰©ä½“ã® (x, y, width, height, class, confidence) ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚</p>
</blockquote>

<h4>Bounding Boxã®è¡¨ç¾æ–¹æ³•</h4>

<p>Bounding Boxï¼ˆãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ï¼‰ã¯ã€ç‰©ä½“ã‚’å›²ã‚€çŸ©å½¢é ˜åŸŸã§ã™ã€‚ä¸»ã«ä»¥ä¸‹ã®4ã¤ã®è¡¨ç¾æ–¹æ³•ãŒã‚ã‚Šã¾ã™ï¼š</p>

<table>
<thead>
<tr>
<th>è¡¨ç¾æ–¹æ³•</th>
<th>å½¢å¼</th>
<th>èª¬æ˜</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>XYXY</strong></td>
<td>(x1, y1, x2, y2)</td>
<td>å·¦ä¸Šåº§æ¨™ã¨å³ä¸‹åº§æ¨™</td>
</tr>
<tr>
<td><strong>XYWH</strong></td>
<td>(x, y, w, h)</td>
<td>å·¦ä¸Šåº§æ¨™ã¨å¹…ãƒ»é«˜ã•</td>
</tr>
<tr>
<td><strong>CXCYWH</strong></td>
<td>(cx, cy, w, h)</td>
<td>ä¸­å¿ƒåº§æ¨™ã¨å¹…ãƒ»é«˜ã•</td>
</tr>
<tr>
<td><strong>æ­£è¦åŒ–åº§æ¨™</strong></td>
<td>(0~1ã«æ­£è¦åŒ–)</td>
<td>ç”»åƒã‚µã‚¤ã‚ºã§æ­£è¦åŒ–ã—ãŸåº§æ¨™</td>
</tr>
</tbody>
</table>

<h3>IoU (Intersection over Union)</h3>

<p>IoUã¯ã€äºˆæ¸¬Bounding Boxã¨æ­£è§£Bounding Boxã®é‡ãªã‚Šåº¦åˆã„ã‚’æ¸¬ã‚‹æŒ‡æ¨™ã§ã™ã€‚ç‰©ä½“æ¤œå‡ºã«ãŠã„ã¦æœ€ã‚‚é‡è¦ãªè©•ä¾¡æŒ‡æ¨™ã®ä¸€ã¤ã§ã™ã€‚</p>

<blockquote>
<p>IoU = (äºˆæ¸¬ âˆ© æ­£è§£) / (äºˆæ¸¬ âˆª æ­£è§£) = é‡ãªã‚Šé¢ç© / çµåˆé¢ç©</p>
<p>IoU = 0ï¼ˆé‡ãªã‚Šãªã—ï¼‰ï½ 1ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰ã®ç¯„å›²ã‚’å–ã‚Šã¾ã™ã€‚</p>
</blockquote>

<details>
<summary>ã‚³ãƒ¼ãƒ‰ä¾‹1ï¼šIoUè¨ˆç®—ã®å®Ÿè£…</summary>

<pre><code class="language-python">import numpy as np

def calculate_iou(box1, box2):
    """
    IoU (Intersection over Union) ã‚’è¨ˆç®—

    Args:
        box1, box2: [x1, y1, x2, y2] å½¢å¼ã®Bounding Box

    Returns:
        float: IoUå€¤ (0~1)
    """
    # äº¤å·®é ˜åŸŸã®åº§æ¨™ã‚’è¨ˆç®—
    x1_inter = max(box1[0], box2[0])
    y1_inter = max(box1[1], box2[1])
    x2_inter = min(box1[2], box2[2])
    y2_inter = min(box1[3], box2[3])

    # äº¤å·®é ˜åŸŸã®é¢ç©
    inter_width = max(0, x2_inter - x1_inter)
    inter_height = max(0, y2_inter - y1_inter)
    intersection = inter_width * inter_height

    # å„Boxã®é¢ç©
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])

    # çµåˆé ˜åŸŸã®é¢ç©
    union = box1_area + box2_area - intersection

    # IoUè¨ˆç®—ï¼ˆã‚¼ãƒ­é™¤ç®—å›é¿ï¼‰
    iou = intersection / union if union > 0 else 0

    return iou

# ä½¿ç”¨ä¾‹
box_pred = [50, 50, 150, 150]   # äºˆæ¸¬Box
box_gt = [60, 60, 160, 160]     # æ­£è§£Box

iou = calculate_iou(box_pred, box_gt)
print(f"IoU: {iou:.4f}")

# è¤‡æ•°ã®Boxã«å¯¾ã—ã¦IoUã‚’è¨ˆç®—
boxes_pred = np.array([
    [50, 50, 150, 150],
    [100, 100, 200, 200],
    [30, 30, 130, 130]
])

boxes_gt = np.array([[60, 60, 160, 160]])

for i, box_pred in enumerate(boxes_pred):
    iou = calculate_iou(box_pred, boxes_gt[0])
    print(f"Box {i+1} IoU: {iou:.4f}")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹ï¼š</strong></p>
<pre><code>IoU: 0.6806
Box 1 IoU: 0.6806
Box 2 IoU: 0.2537
Box 3 IoU: 0.7347
</code></pre>

</details>

<h3>Non-Maximum Suppression (NMS)</h3>

<p>ç‰©ä½“æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã¯ã€åŒã˜ç‰©ä½“ã«å¯¾ã—ã¦è¤‡æ•°ã®Bounding Boxã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚NMSã¯ã€é‡è¤‡ã—ãŸæ¤œå‡ºã‚’é™¤å»ã—ã€æœ€ã‚‚ä¿¡é ¼åº¦ã®é«˜ã„Boxã®ã¿ã‚’æ®‹ã™æ‰‹æ³•ã§ã™ã€‚</p>

<h4>NMSã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h4>

<ol>
<li>ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã§Bounding Boxã‚’é™é †ã«ã‚½ãƒ¼ãƒˆ</li>
<li>æœ€ã‚‚ä¿¡é ¼åº¦ã®é«˜ã„Boxã‚’é¸æŠã—ã€å‡ºåŠ›ãƒªã‚¹ãƒˆã«è¿½åŠ </li>
<li>æ®‹ã‚Šã®Boxã®ã†ã¡ã€é¸æŠã—ãŸBoxã¨ã®IoUãŒé–¾å€¤ä»¥ä¸Šã®ã‚‚ã®ã‚’å‰Šé™¤</li>
<li>æ®‹ã‚Šã®Boxã«å¯¾ã—ã¦ã‚¹ãƒ†ãƒƒãƒ—2-3ã‚’ç¹°ã‚Šè¿”ã™</li>
</ol>

<details>
<summary>ã‚³ãƒ¼ãƒ‰ä¾‹2ï¼šNMSã®å®Ÿè£…</summary>

<pre><code class="language-python">import numpy as np

def non_max_suppression(boxes, scores, iou_threshold=0.5):
    """
    Non-Maximum Suppression (NMS) ã‚’å®Ÿè£…

    Args:
        boxes: numpy array, shape (N, 4) [x1, y1, x2, y2]
        scores: numpy array, shape (N,) ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
        iou_threshold: float, IoUé–¾å€¤

    Returns:
        list: æ®‹ã™ã¹ãBoxã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    """
    # BoxãŒç©ºã®å ´åˆ
    if len(boxes) == 0:
        return []

    # floatå‹ã«å¤‰æ›
    boxes = boxes.astype(np.float32)

    # å„Boxã®é¢ç©ã‚’è¨ˆç®—
    x1 = boxes[:, 0]
    y1 = boxes[:, 1]
    x2 = boxes[:, 2]
    y2 = boxes[:, 3]
    areas = (x2 - x1) * (y2 - y1)

    # ã‚¹ã‚³ã‚¢ã§é™é †ã«ã‚½ãƒ¼ãƒˆ
    order = scores.argsort()[::-1]

    keep = []

    while len(order) > 0:
        # æœ€ã‚‚ä¿¡é ¼åº¦ã®é«˜ã„Boxã‚’é¸æŠ
        idx = order[0]
        keep.append(idx)

        if len(order) == 1:
            break

        # æ®‹ã‚Šã®Boxã¨ã®IoUã‚’è¨ˆç®—
        xx1 = np.maximum(x1[idx], x1[order[1:]])
        yy1 = np.maximum(y1[idx], y1[order[1:]])
        xx2 = np.minimum(x2[idx], x2[order[1:]])
        yy2 = np.minimum(y2[idx], y2[order[1:]])

        w = np.maximum(0, xx2 - xx1)
        h = np.maximum(0, yy2 - yy1)

        intersection = w * h
        union = areas[idx] + areas[order[1:]] - intersection
        iou = intersection / union

        # IoUãŒé–¾å€¤æœªæº€ã®Boxã®ã¿æ®‹ã™
        inds = np.where(iou <= iou_threshold)[0]
        order = order[inds + 1]

    return keep

# ä½¿ç”¨ä¾‹
boxes = np.array([
    [50, 50, 150, 150],
    [55, 55, 155, 155],
    [60, 60, 160, 160],
    [200, 200, 300, 300],
    [205, 205, 305, 305]
])

scores = np.array([0.9, 0.85, 0.88, 0.95, 0.92])

keep_indices = non_max_suppression(boxes, scores, iou_threshold=0.5)

print(f"å…ƒã®Boxæ•°: {len(boxes)}")
print(f"NMSå¾Œã®Boxæ•°: {len(keep_indices)}")
print(f"æ®‹ã™Boxã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {keep_indices}")
print(f"\næ®‹ã£ãŸBoxes:")
for idx in keep_indices:
    print(f"  Box {idx}: {boxes[idx]}, Score: {scores[idx]:.2f}")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹ï¼š</strong></p>
<pre><code>å…ƒã®Boxæ•°: 5
NMSå¾Œã®Boxæ•°: 2
æ®‹ã™Boxã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: [3, 2]

æ®‹ã£ãŸBoxes:
  Box 3: [200 200 300 300], Score: 0.95
  Box 2: [ 60  60 160 160], Score: 0.88
</code></pre>

</details>

<h3>è©•ä¾¡æŒ‡æ¨™ï¼ˆmAPï¼‰</h3>

<p>ç‰©ä½“æ¤œå‡ºã®æ€§èƒ½è©•ä¾¡ã«ã¯ã€<strong>mAP (mean Average Precision)</strong> ãŒåºƒãä½¿ã‚ã‚Œã¾ã™ã€‚</p>

<h4>ä¸»è¦ãªè©•ä¾¡æŒ‡æ¨™</h4>

<ul>
<li><strong>Precisionï¼ˆé©åˆç‡ï¼‰</strong>ï¼šæ¤œå‡ºã—ãŸã‚‚ã®ã®ã†ã¡ã€æ­£è§£ã ã£ãŸå‰²åˆ</li>
<li><strong>Recallï¼ˆå†ç¾ç‡ï¼‰</strong>ï¼šæ­£è§£ã®ã†ã¡ã€æ¤œå‡ºã§ããŸå‰²åˆ</li>
<li><strong>AP (Average Precision)</strong>ï¼š1ã‚¯ãƒ©ã‚¹ã«å¯¾ã™ã‚‹Precision-Recallæ›²ç·šã®é¢ç©</li>
<li><strong>mAP (mean Average Precision)</strong>ï¼šå…¨ã‚¯ãƒ©ã‚¹ã®APã®å¹³å‡å€¤</li>
</ul>

<blockquote>
<p><strong>mAP@0.5</strong>ï¼šIoUé–¾å€¤0.5ã§ã®mAP</p>
<p><strong>mAP@[0.5:0.95]</strong>ï¼šIoUé–¾å€¤0.5ï½0.95ï¼ˆ0.05åˆ»ã¿ï¼‰ã®å¹³å‡mAPï¼ˆCOCOè©•ä¾¡ï¼‰</p>
</blockquote>

<hr>

<h2>3.2 Two-Stageæ¤œå‡ºå™¨</h2>

<h3>R-CNNãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®é€²åŒ–</h3>

<p>Two-Stageæ¤œå‡ºå™¨ã¯ã€ã€Œé ˜åŸŸææ¡ˆï¼ˆRegion Proposalï¼‰ã€ã¨ã€Œåˆ†é¡ãƒ»ä½ç½®èª¿æ•´ã€ã®2æ®µéšã§ç‰©ä½“æ¤œå‡ºã‚’è¡Œã„ã¾ã™ã€‚</p>

<h4>R-CNN (2014)</h4>

<ol>
<li><strong>Selective Search</strong>ã§ç´„2000å€‹ã®é ˜åŸŸå€™è£œã‚’æŠ½å‡º</li>
<li>å„é ˜åŸŸã‚’CNNã§ç‰¹å¾´æŠ½å‡ºï¼ˆAlexNetï¼‰</li>
<li>SVMã§åˆ†é¡ã€å›å¸°ã§ä½ç½®èª¿æ•´</li>
</ol>

<p><strong>å•é¡Œç‚¹</strong>ï¼š2000å›ã®CNNå‡¦ç†ãŒå¿…è¦ã§éå¸¸ã«é…ã„ï¼ˆ1ç”»åƒã«47ç§’ï¼‰</p>

<h4>Fast R-CNN (2015)</h4>

<ol>
<li>ç”»åƒå…¨ä½“ã‚’1å›ã ã‘CNNã§å‡¦ç†</li>
<li>ç‰¹å¾´ãƒãƒƒãƒ—ã‹ã‚‰é ˜åŸŸå€™è£œã‚’RoI Poolingã§æŠ½å‡º</li>
<li>å…¨çµåˆå±¤ã§åˆ†é¡ã¨ä½ç½®èª¿æ•´ã‚’åŒæ™‚å®Ÿè¡Œ</li>
</ol>

<p><strong>æ”¹å–„</strong>ï¼šR-CNNã®ç´„10å€é«˜é€ŸåŒ–ï¼ˆ1ç”»åƒã«2ç§’ï¼‰</p>

<h4>Faster R-CNN (2015)</h4>

<ol>
<li><strong>RPN (Region Proposal Network)</strong>ã§é ˜åŸŸå€™è£œã‚’ç”Ÿæˆ</li>
<li>RoI Poolingã§ç‰¹å¾´æŠ½å‡º</li>
<li>åˆ†é¡ã¨ä½ç½®èª¿æ•´</li>
</ol>

<p><strong>æ”¹å–„</strong>ï¼šSelective Searchã‚’ä¸è¦ã«ã—ã€å®Œå…¨ãªEnd-to-Endã®å­¦ç¿’ãŒå¯èƒ½ã«ï¼ˆ1ç”»åƒã«0.2ç§’ï¼‰</p>

<h3>Faster R-CNNã®å®Ÿè£…</h3>

<details>
<summary>ã‚³ãƒ¼ãƒ‰ä¾‹3ï¼šFaster R-CNNã§ã®ç‰©ä½“æ¤œå‡ºï¼ˆtorchvisionï¼‰</summary>

<pre><code class="language-python">import torch
import torchvision
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.transforms import functional as F
from PIL import Image, ImageDraw, ImageFont
import requests
from io import BytesIO

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# äº‹å‰å­¦ç¿’æ¸ˆã¿Faster R-CNNãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
model = fasterrcnn_resnet50_fpn(pretrained=True)
model = model.to(device)
model.eval()

# COCOã‚¯ãƒ©ã‚¹åï¼ˆ91ã‚¯ãƒ©ã‚¹ï¼‰
COCO_CLASSES = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',
    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',
    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',
    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',
    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',
    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',
    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',
    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
]

def detect_objects(image_path, threshold=0.5):
    """
    Faster R-CNNã§ç‰©ä½“æ¤œå‡ºã‚’å®Ÿè¡Œ

    Args:
        image_path: ç”»åƒãƒ‘ã‚¹ã¾ãŸã¯URL
        threshold: ä¿¡é ¼åº¦é–¾å€¤
    """
    # ç”»åƒèª­ã¿è¾¼ã¿
    if image_path.startswith('http'):
        response = requests.get(image_path)
        img = Image.open(BytesIO(response.content)).convert('RGB')
    else:
        img = Image.open(image_path).convert('RGB')

    # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
    img_tensor = F.to_tensor(img).to(device)

    # æ¨è«–
    with torch.no_grad():
        predictions = model([img_tensor])[0]

    # çµæœã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    keep = predictions['scores'] > threshold
    boxes = predictions['boxes'][keep].cpu().numpy()
    labels = predictions['labels'][keep].cpu().numpy()
    scores = predictions['scores'][keep].cpu().numpy()

    # çµæœã‚’æç”»
    draw = ImageDraw.Draw(img)

    for box, label, score in zip(boxes, labels, scores):
        x1, y1, x2, y2 = box
        class_name = COCO_CLASSES[label]

        # Bounding Boxæç”»
        draw.rectangle([x1, y1, x2, y2], outline='red', width=3)

        # ãƒ©ãƒ™ãƒ«ã¨ã‚¹ã‚³ã‚¢æç”»
        text = f"{class_name}: {score:.2f}"
        draw.text((x1, y1 - 15), text, fill='red')

    # çµæœã‚’è¡¨ç¤º
    print(f"æ¤œå‡ºã•ã‚ŒãŸç‰©ä½“æ•°: {len(boxes)}")
    for label, score in zip(labels, scores):
        print(f"  - {COCO_CLASSES[label]}: {score:.3f}")

    return img, boxes, labels, scores

# ä½¿ç”¨ä¾‹
image_url = "https://images.unsplash.com/photo-1544568100-847a948585b9?w=800"
result_img, boxes, labels, scores = detect_objects(image_url, threshold=0.7)

# ç”»åƒã‚’è¡¨ç¤ºï¼ˆJupyter Notebookã®å ´åˆï¼‰
# display(result_img)

# ç”»åƒã‚’ä¿å­˜
result_img.save('faster_rcnn_result.jpg')
print("çµæœã‚’ faster_rcnn_result.jpg ã«ä¿å­˜ã—ã¾ã—ãŸ")
</code></pre>

</details>

<h3>Feature Pyramid Networks (FPN)</h3>

<p>FPNã¯ã€ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã®ç‰¹å¾´ã‚’åŠ¹æœçš„ã«åˆ©ç”¨ã™ã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚ç•°ãªã‚‹ã‚µã‚¤ã‚ºã®ç‰©ä½“ã‚’æ¤œå‡ºã™ã‚‹ãŸã‚ã«ã€è¤‡æ•°ã®è§£åƒåº¦ã®ç‰¹å¾´ãƒãƒƒãƒ—ã‚’çµ„ã¿åˆã‚ã›ã¾ã™ã€‚</p>

<blockquote>
<p><strong>FPNã®ç‰¹å¾´</strong>ï¼š</p>
<ul>
<li>ãƒœãƒˆãƒ ã‚¢ãƒƒãƒ—ãƒ‘ã‚¹ï¼šé€šå¸¸ã®CNNã®é †ä¼æ’­</li>
<li>ãƒˆãƒƒãƒ—ãƒ€ã‚¦ãƒ³ãƒ‘ã‚¹ï¼šé«˜ãƒ¬ãƒ™ãƒ«ç‰¹å¾´ã‚’ä½è§£åƒåº¦ã‹ã‚‰é«˜è§£åƒåº¦ã¸ä¼æ’­</li>
<li>ãƒ©ãƒ†ãƒ©ãƒ«æ¥ç¶šï¼šå„ãƒ¬ãƒ™ãƒ«ã®ç‰¹å¾´ã‚’çµåˆ</li>
</ul>
</blockquote>

<hr>

<h2>3.3 One-Stageæ¤œå‡ºå™¨</h2>

<h3>YOLOãƒ•ã‚¡ãƒŸãƒªãƒ¼</h3>

<p><strong>YOLO (You Only Look Once)</strong> ã¯ã€ç”»åƒã‚’1å›è¦‹ã‚‹ã ã‘ã§ç‰©ä½“æ¤œå‡ºã‚’è¡Œã†é©æ–°çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã™ã€‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œå‡ºã‚’å®Ÿç¾ã—ã€Two-Stageæ¤œå‡ºå™¨ã‚ˆã‚Šã‚‚é«˜é€Ÿã§ã™ã€‚</p>

<h4>YOLOã®åŸºæœ¬åŸç†</h4>

<ol>
<li>ç”»åƒã‚’ã‚°ãƒªãƒƒãƒ‰ï¼ˆä¾‹ï¼š13Ã—13ï¼‰ã«åˆ†å‰²</li>
<li>å„ã‚°ãƒªãƒƒãƒ‰ã‚»ãƒ«ãŒBounding Boxã¨ä¿¡é ¼åº¦ã‚’äºˆæ¸¬</li>
<li>å„Boxã«å¯¾ã—ã¦ã‚¯ãƒ©ã‚¹ç¢ºç‡ã‚’äºˆæ¸¬</li>
<li>NMSã§é‡è¤‡ã‚’é™¤å»</li>
</ol>

<h4>YOLOã®é€²åŒ–</h4>

<table>
<thead>
<tr>
<th>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</th>
<th>å¹´</th>
<th>ä¸»ãªæ”¹è‰¯ç‚¹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>YOLOv1</strong></td>
<td>2016</td>
<td>One-Stageæ¤œå‡ºã®ææ¡ˆã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†</td>
</tr>
<tr>
<td><strong>YOLOv2</strong></td>
<td>2017</td>
<td>Batch Normalizationã€Anchor Boxå°å…¥</td>
</tr>
<tr>
<td><strong>YOLOv3</strong></td>
<td>2018</td>
<td>ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«äºˆæ¸¬ã€Darknet-53</td>
</tr>
<tr>
<td><strong>YOLOv4</strong></td>
<td>2020</td>
<td>CSPDarknet53ã€Mosaic augmentation</td>
</tr>
<tr>
<td><strong>YOLOv5</strong></td>
<td>2020</td>
<td>PyTorchå®Ÿè£…ã€ä½¿ã„ã‚„ã™ã•å‘ä¸Š</td>
</tr>
<tr>
<td><strong>YOLOv8</strong></td>
<td>2023</td>
<td>Anchor-freeã€æ”¹è‰¯ã•ã‚ŒãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</td>
</tr>
</tbody>
</table>

<details>
<summary>ã‚³ãƒ¼ãƒ‰ä¾‹4ï¼šYOLOv8ã§ã®ç‰©ä½“æ¤œå‡º</summary>

<pre><code class="language-python">from ultralytics import YOLO
from PIL import Image
import cv2
import numpy as np

# YOLOv8ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
# ã‚µã‚¤ã‚º: n (nano), s (small), m (medium), l (large), x (extra large)
model = YOLO('yolov8n.pt')  # nanoãƒ¢ãƒ‡ãƒ«ï¼ˆæœ€è»½é‡ï¼‰

def detect_with_yolo(image_path, conf_threshold=0.5):
    """
    YOLOv8ã§ç‰©ä½“æ¤œå‡ºã‚’å®Ÿè¡Œ

    Args:
        image_path: ç”»åƒãƒ‘ã‚¹ã¾ãŸã¯URL
        conf_threshold: ä¿¡é ¼åº¦é–¾å€¤
    """
    # æ¨è«–å®Ÿè¡Œ
    results = model(image_path, conf=conf_threshold)

    # çµæœã‚’å–å¾—
    result = results[0]

    # æ¤œå‡ºã•ã‚ŒãŸç‰©ä½“ã®æƒ…å ±ã‚’è¡¨ç¤º
    print(f"æ¤œå‡ºã•ã‚ŒãŸç‰©ä½“æ•°: {len(result.boxes)}")

    for box in result.boxes:
        # ã‚¯ãƒ©ã‚¹IDã€ä¿¡é ¼åº¦ã€åº§æ¨™ã‚’å–å¾—
        class_id = int(box.cls[0])
        confidence = float(box.conf[0])
        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()

        class_name = model.names[class_id]
        print(f"  - {class_name}: {confidence:.3f} at [{x1:.0f}, {y1:.0f}, {x2:.0f}, {y2:.0f}]")

    # çµæœç”»åƒã‚’å–å¾—ï¼ˆã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä»˜ãï¼‰
    annotated_img = result.plot()

    return annotated_img, result

# ä½¿ç”¨ä¾‹1ï¼šç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ¤œå‡º
image_path = "path/to/your/image.jpg"
annotated_img, result = detect_with_yolo(image_path, conf_threshold=0.5)

# çµæœã‚’ä¿å­˜
cv2.imwrite('yolov8_result.jpg', annotated_img)
print("çµæœã‚’ yolov8_result.jpg ã«ä¿å­˜ã—ã¾ã—ãŸ")

# ä½¿ç”¨ä¾‹2ï¼šãƒ“ãƒ‡ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯Webcamã‹ã‚‰æ¤œå‡º
def detect_video(source=0, conf_threshold=0.5):
    """
    ãƒ“ãƒ‡ã‚ªã¾ãŸã¯Webcamã‹ã‚‰ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œå‡º

    Args:
        source: ãƒ“ãƒ‡ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¾ãŸã¯0ï¼ˆWebcamï¼‰
        conf_threshold: ä¿¡é ¼åº¦é–¾å€¤
    """
    # ãƒ“ãƒ‡ã‚ªã‚¹ãƒˆãƒªãƒ¼ãƒ ã§æ¨è«–
    results = model(source, stream=True, conf=conf_threshold)

    for result in results:
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«å‡¦ç†
        annotated_frame = result.plot()

        # è¡¨ç¤º
        cv2.imshow('YOLOv8 Detection', annotated_frame)

        # 'q'ã‚­ãƒ¼ã§çµ‚äº†
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cv2.destroyAllWindows()

# Webcamã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œå‡ºï¼ˆã‚³ãƒ¡ãƒ³ãƒˆè§£é™¤ã—ã¦å®Ÿè¡Œï¼‰
# detect_video(source=0, conf_threshold=0.5)

# ãƒ“ãƒ‡ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ã§æ¤œå‡º
# detect_video(source='path/to/video.mp4', conf_threshold=0.5)
</code></pre>

</details>

<h3>SSD (Single Shot Detector)</h3>

<p>SSDã¯ã€YOLOã¨åŒæ§˜ã«One-Stageæ¤œå‡ºå™¨ã§ã™ãŒã€è¤‡æ•°ã®ã‚¹ã‚±ãƒ¼ãƒ«ã®ç‰¹å¾´ãƒãƒƒãƒ—ã‹ã‚‰æ¤œå‡ºã‚’è¡Œã„ã¾ã™ã€‚</p>

<h4>SSDã®ç‰¹å¾´</h4>

<ul>
<li><strong>ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ç‰¹å¾´ãƒãƒƒãƒ—</strong>ï¼šç•°ãªã‚‹è§£åƒåº¦ã®å±¤ã‹ã‚‰æ¤œå‡º</li>
<li><strong>ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒœãƒƒã‚¯ã‚¹</strong>ï¼šå„ä½ç½®ã§è¤‡æ•°ã®ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®Boxã‚’äºˆæ¸¬</li>
<li><strong>é«˜é€Ÿ</strong>ï¼šYOLOv1ã‚ˆã‚Šé«˜é€Ÿã§mAPã‚‚é«˜ã„</li>
</ul>

<h3>RetinaNet (Focal Loss)</h3>

<p>RetinaNetã¯ã€<strong>Focal Loss</strong>ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã€ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å•é¡Œã‚’è§£æ±ºã—ã¾ã—ãŸã€‚</p>

<h4>Focal Lossã¨ã¯</h4>

<blockquote>
<p>Focal Loss = -Î±(1-p_t)^Î³ log(p_t)</p>
<p>ç°¡å˜ãªä¾‹ï¼ˆèƒŒæ™¯ãªã©ï¼‰ã®æå¤±ã‚’å°ã•ãã—ã€é›£ã—ã„ä¾‹ã«é›†ä¸­ã—ã¦å­¦ç¿’ã—ã¾ã™ã€‚</p>
</blockquote>

<details>
<summary>ã‚³ãƒ¼ãƒ‰ä¾‹5ï¼šFocal Lossã®å®Ÿè£…</summary>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class FocalLoss(nn.Module):
    """
    Focal Loss for Object Detection

    Args:
        alpha: ã‚¯ãƒ©ã‚¹é‡ã¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.25ï¼‰
        gamma: ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2.0ï¼‰
    """
    def __init__(self, alpha=0.25, gamma=2.0):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, predictions, targets):
        """
        Args:
            predictions: (N, num_classes) äºˆæ¸¬ç¢ºç‡
            targets: (N,) æ­£è§£ãƒ©ãƒ™ãƒ«
        """
        # Cross Entropy Loss
        ce_loss = F.cross_entropy(predictions, targets, reduction='none')

        # p_tã‚’è¨ˆç®—ï¼ˆæ­£è§£ã‚¯ãƒ©ã‚¹ã®äºˆæ¸¬ç¢ºç‡ï¼‰
        p = torch.exp(-ce_loss)

        # Focal Loss
        focal_loss = self.alpha * (1 - p) ** self.gamma * ce_loss

        return focal_loss.mean()

# ä½¿ç”¨ä¾‹
num_classes = 91  # COCO
batch_size = 32

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
predictions = torch.randn(batch_size, num_classes)
targets = torch.randint(0, num_classes, (batch_size,))

# é€šå¸¸ã®Cross Entropy Loss
ce_loss = F.cross_entropy(predictions, targets)
print(f"Cross Entropy Loss: {ce_loss.item():.4f}")

# Focal Loss
focal_loss_fn = FocalLoss(alpha=0.25, gamma=2.0)
focal_loss = focal_loss_fn(predictions, targets)
print(f"Focal Loss: {focal_loss.item():.4f}")

# ç°¡å˜ãªä¾‹ vs é›£ã—ã„ä¾‹ã§ã®æå¤±æ¯”è¼ƒ
easy_predictions = torch.tensor([[10.0, 0.0, 0.0]])  # æ­£è§£ã‚¯ãƒ©ã‚¹0ã«é«˜ã„ç¢ºç‡
hard_predictions = torch.tensor([[1.0, 0.9, 0.8]])   # æ­£è§£ã‚¯ãƒ©ã‚¹0ã ãŒä½ã„ç¢ºç‡
targets_test = torch.tensor([0])

easy_loss = focal_loss_fn(easy_predictions, targets_test)
hard_loss = focal_loss_fn(hard_predictions, targets_test)

print(f"\nç°¡å˜ãªä¾‹ã®æå¤±: {easy_loss.item():.4f}")
print(f"é›£ã—ã„ä¾‹ã®æå¤±: {hard_loss.item():.4f}")
print(f"é›£ã—ã„ä¾‹ã®æå¤±ã¯ç°¡å˜ãªä¾‹ã® {hard_loss.item() / easy_loss.item():.1f} å€")
</code></pre>

</details>

<h3>EfficientDet</h3>

<p>EfficientDetã¯ã€EfficientNetã‚’ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã—ã€BiFPNï¼ˆBi-directional Feature Pyramid Networkï¼‰ã‚’ä½¿ç”¨ã—ãŸåŠ¹ç‡çš„ãªæ¤œå‡ºå™¨ã§ã™ã€‚</p>

<ul>
<li><strong>Compound Scaling</strong>ï¼šè§£åƒåº¦ã€æ·±ã•ã€å¹…ã‚’åŒæ™‚ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°</li>
<li><strong>BiFPN</strong>ï¼šåŒæ–¹å‘ã®ç‰¹å¾´èåˆ</li>
<li><strong>é«˜åŠ¹ç‡</strong>ï¼šYOLOv3ã‚„RetinaNetã‚ˆã‚Šã‚‚å°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§é«˜ç²¾åº¦</li>
</ul>

<hr>

<h2>3.4 å®Ÿè£…ã¨è¨“ç·´</h2>

<h3>COCOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</h3>

<p>COCO (Common Objects in Context) ã¯ã€ç‰©ä½“æ¤œå‡ºã®æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚</p>

<ul>
<li><strong>ç”»åƒæ•°</strong>ï¼š330Kæšï¼ˆtrain: 118K, val: 5K, test: 41Kï¼‰</li>
<li><strong>ã‚«ãƒ†ã‚´ãƒª</strong>ï¼š80ã‚¯ãƒ©ã‚¹ï¼ˆäººã€å‹•ç‰©ã€ä¹—ã‚Šç‰©ã€å®¶å…·ãªã©ï¼‰</li>
<li><strong>ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³</strong>ï¼šBounding Boxã€ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã€ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ</li>
</ul>

<details>
<summary>ã‚³ãƒ¼ãƒ‰ä¾‹6ï¼šPyTorch Object Detectionã®è¨“ç·´</summary>

<pre><code class="language-python">import torch
import torchvision
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torch.utils.data import DataLoader
import torchvision.transforms as T

# ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹
class CustomObjectDetectionDataset(torch.utils.data.Dataset):
    """
    ã‚«ã‚¹ã‚¿ãƒ ç‰©ä½“æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

    ç”»åƒã¨ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆboxes, labelsï¼‰ã‚’è¿”ã™
    """
    def __init__(self, image_paths, annotations, transforms=None):
        self.image_paths = image_paths
        self.annotations = annotations
        self.transforms = transforms

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        # ç”»åƒèª­ã¿è¾¼ã¿
        from PIL import Image
        img = Image.open(self.image_paths[idx]).convert("RGB")

        # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å–å¾—
        boxes = self.annotations[idx]['boxes']  # [[x1,y1,x2,y2], ...]
        labels = self.annotations[idx]['labels']  # [1, 2, 1, ...]

        # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)

        target = {
            'boxes': boxes,
            'labels': labels,
            'image_id': torch.tensor([idx])
        }

        if self.transforms:
            img = self.transforms(img)

        return img, target

def get_model(num_classes):
    """
    Faster R-CNNãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰

    Args:
        num_classes: ã‚¯ãƒ©ã‚¹æ•°ï¼ˆèƒŒæ™¯ + ç‰©ä½“ã‚¯ãƒ©ã‚¹ï¼‰
    """
    # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    model = fasterrcnn_resnet50_fpn(pretrained=True)

    # åˆ†é¡ãƒ˜ãƒƒãƒ‰ã‚’ç½®ãæ›ãˆ
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    return model

def train_one_epoch(model, optimizer, data_loader, device):
    """
    1ã‚¨ãƒãƒƒã‚¯ã®è¨“ç·´
    """
    model.train()
    total_loss = 0

    for images, targets in data_loader:
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        # é †ä¼æ’­
        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())

        # é€†ä¼æ’­
        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

        total_loss += losses.item()

    return total_loss / len(data_loader)

# è¨“ç·´è¨­å®š
num_classes = 3  # èƒŒæ™¯ + 2ã‚¯ãƒ©ã‚¹
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ãƒ¢ãƒ‡ãƒ«ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
model = get_model(num_classes)
model.to(device)

optimizer = torch.optim.SGD(
    model.parameters(),
    lr=0.005,
    momentum=0.9,
    weight_decay=0.0005
)

lr_scheduler = torch.optim.lr_scheduler.StepLR(
    optimizer,
    step_size=3,
    gamma=0.1
)

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
# å®Ÿéš›ã«ã¯ç”»åƒãƒ‘ã‚¹ã¨ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”¨æ„
image_paths = ['img1.jpg', 'img2.jpg', 'img3.jpg']
annotations = [
    {'boxes': [[10, 10, 50, 50]], 'labels': [1]},
    {'boxes': [[20, 20, 60, 60], [70, 70, 100, 100]], 'labels': [1, 2]},
    {'boxes': [[30, 30, 80, 80]], 'labels': [2]}
]

# transforms = T.Compose([T.ToTensor()])
# dataset = CustomObjectDetectionDataset(image_paths, annotations, transforms)
# data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))

# è¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆï¼‰
# num_epochs = 10
# for epoch in range(num_epochs):
#     train_loss = train_one_epoch(model, optimizer, data_loader, device)
#     lr_scheduler.step()
#     print(f"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}")

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜
# torch.save(model.state_dict(), 'object_detection_model.pth')

print("è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æº–å‚™å®Œäº†")
</code></pre>

</details>

<h3>ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®è¨“ç·´</h3>

<details>
<summary>ã‚³ãƒ¼ãƒ‰ä¾‹7ï¼šYOLOv8ã§ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨“ç·´</summary>

<pre><code class="language-python">from ultralytics import YOLO
import yaml
import os

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
dataset_yaml = """
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹
path: ./custom_dataset  # ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
train: images/train     # è¨“ç·´ç”»åƒï¼ˆpathã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ï¼‰
val: images/val         # æ¤œè¨¼ç”»åƒ

# ã‚¯ãƒ©ã‚¹å®šç¾©
names:
  0: cat
  1: dog
  2: bird
"""

# dataset.yamlã‚’ä¿å­˜
with open('custom_dataset.yaml', 'w') as f:
    f.write(dataset_yaml)

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®ä¾‹ï¼š
# custom_dataset/
# â”œâ”€â”€ images/
# â”‚   â”œâ”€â”€ train/
# â”‚   â”‚   â”œâ”€â”€ img1.jpg
# â”‚   â”‚   â”œâ”€â”€ img2.jpg
# â”‚   â”‚   â””â”€â”€ ...
# â”‚   â””â”€â”€ val/
# â”‚       â”œâ”€â”€ img1.jpg
# â”‚       â””â”€â”€ ...
# â””â”€â”€ labels/
#     â”œâ”€â”€ train/
#     â”‚   â”œâ”€â”€ img1.txt  # YOLOå½¢å¼ï¼ˆclass x_center y_center width heightï¼‰
#     â”‚   â”œâ”€â”€ img2.txt
#     â”‚   â””â”€â”€ ...
#     â””â”€â”€ val/
#         â”œâ”€â”€ img1.txt
#         â””â”€â”€ ...

# YOLOv8ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
model = YOLO('yolov8n.pt')  # äº‹å‰å­¦ç¿’æ¸ˆã¿weightsã‹ã‚‰é–‹å§‹

# è¨“ç·´å®Ÿè¡Œ
results = model.train(
    data='custom_dataset.yaml',
    epochs=100,
    imgsz=640,
    batch=16,
    name='custom_yolo',
    # ãã®ä»–ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    lr0=0.01,          # åˆæœŸå­¦ç¿’ç‡
    momentum=0.937,     # SGDãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ 
    weight_decay=0.0005,
    warmup_epochs=3,
    patience=50,        # Early stopping
    # Data Augmentation
    degrees=10.0,       # å›è»¢
    translate=0.1,      # å¹³è¡Œç§»å‹•
    scale=0.5,          # ã‚¹ã‚±ãƒ¼ãƒ«
    flipud=0.0,         # ä¸Šä¸‹åè»¢
    fliplr=0.5,         # å·¦å³åè»¢
    mosaic=1.0,         # Mosaic augmentation
)

# æ¤œè¨¼
metrics = model.val()
print(f"mAP50: {metrics.box.map50:.3f}")
print(f"mAP50-95: {metrics.box.map:.3f}")

# æ¨è«–
results = model('path/to/test/image.jpg')

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆè‡ªå‹•çš„ã«ä¿å­˜ã•ã‚Œã‚‹ãŒã€æ‰‹å‹•ã§ã‚‚å¯èƒ½ï¼‰
# model.save('custom_yolo_best.pt')

# ãƒ¢ãƒ‡ãƒ«ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆONNX, TensorRT, etc.ï¼‰
# model.export(format='onnx')

print("\nè¨“ç·´å®Œäº†ï¼")
print(f"Weights: runs/detect/custom_yolo/weights/best.pt")
print(f"Metrics: runs/detect/custom_yolo/results.csv")
</code></pre>

<p><strong>ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ï¼ˆYOLOï¼‰ï¼š</strong></p>
<pre><code># img1.txt ã®ä¾‹ï¼ˆå„è¡ŒãŒ1ã¤ã®ç‰©ä½“ï¼‰
0 0.5 0.5 0.3 0.2    # class=0, center=(0.5, 0.5), size=(0.3, 0.2)
1 0.7 0.3 0.2 0.15   # class=1, center=(0.7, 0.3), size=(0.2, 0.15)

# åº§æ¨™ã¯ç”»åƒã‚µã‚¤ã‚ºã§æ­£è¦åŒ–ï¼ˆ0~1ï¼‰
# class x_center y_center width height
</code></pre>

</details>

<hr>

<h2>3.5 å¿œç”¨ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</h2>

<h3>Anchor-Free Detection</h3>

<p>å¾“æ¥ã®æ¤œå‡ºå™¨ã¯Anchor Boxï¼ˆäº‹å‰å®šç¾©ã®Boxï¼‰ã«ä¾å­˜ã—ã¦ã„ã¾ã—ãŸãŒã€Anchor-Freeã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã“ã‚Œã‚’ä¸è¦ã«ã—ã¾ã™ã€‚</p>

<h4>ä¸»ãªAnchor-Freeæ‰‹æ³•</h4>

<ul>
<li><strong>FCOS (Fully Convolutional One-Stage)</strong>ï¼šå„ãƒ”ã‚¯ã‚»ãƒ«ã‹ã‚‰ç‰©ä½“ä¸­å¿ƒã¾ã§ã®è·é›¢ã‚’äºˆæ¸¬</li>
<li><strong>CenterNet</strong>ï¼šç‰©ä½“ã®ä¸­å¿ƒç‚¹ã‚’æ¤œå‡ºã—ã€ã‚µã‚¤ã‚ºã¨ä½ç½®ã‚’å›å¸°</li>
<li><strong>YOLOv8</strong>ï¼šAnchor-Freeã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¡ç”¨</li>
</ul>

<blockquote>
<p><strong>ãƒ¡ãƒªãƒƒãƒˆ</strong>ï¼šAnchorã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒä¸è¦ã€ã‚ˆã‚ŠæŸ”è»Ÿãªæ¤œå‡º</p>
</blockquote>

<h3>ç‰©ä½“è¿½è·¡ï¼ˆObject Trackingï¼‰</h3>

<p>ç‰©ä½“è¿½è·¡ã¯ã€ãƒ“ãƒ‡ã‚ªå†…ã®ç‰©ä½“ã‚’é€£ç¶šçš„ã«è¿½è·¡ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚æ¤œå‡ºå™¨ã¨çµ„ã¿åˆã‚ã›ã¦ä½¿ç”¨ã—ã¾ã™ã€‚</p>

<h4>SORT (Simple Online and Realtime Tracking)</h4>

<ol>
<li>ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«ç‰©ä½“æ¤œå‡º</li>
<li>Kalmanãƒ•ã‚£ãƒ«ã‚¿ã§æ¬¡ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä½ç½®ã‚’äºˆæ¸¬</li>
<li>Hungarian Algorithmã§æ¤œå‡ºã¨è¿½è·¡ã‚’ãƒãƒƒãƒãƒ³ã‚°</li>
</ol>

<h4>DeepSORT</h4>

<p>SORTã«å¤–è¦³ç‰¹å¾´ï¼ˆDeep featuresï¼‰ã‚’è¿½åŠ ã—ã€ã‚ˆã‚Šå …ç‰¢ãªè¿½è·¡ã‚’å®Ÿç¾ã€‚</p>

<details>
<summary>ã‚³ãƒ¼ãƒ‰ä¾‹8ï¼šYOLOv8 + ç‰©ä½“è¿½è·¡</summary>

<pre><code class="language-python">from ultralytics import YOLO
import cv2

# YOLOv8ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
model = YOLO('yolov8n.pt')

def track_objects_video(video_path, output_path='tracking_result.mp4'):
    """
    ãƒ“ãƒ‡ã‚ªã§ç‰©ä½“ã‚’æ¤œå‡ºãƒ»è¿½è·¡

    Args:
        video_path: å…¥åŠ›ãƒ“ãƒ‡ã‚ªãƒ‘ã‚¹
        output_path: å‡ºåŠ›ãƒ“ãƒ‡ã‚ªãƒ‘ã‚¹
    """
    # ãƒ“ãƒ‡ã‚ªã‚­ãƒ£ãƒ—ãƒãƒ£
    cap = cv2.VideoCapture(video_path)

    # å‡ºåŠ›è¨­å®š
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    frame_count = 0

    # è¿½è·¡ãƒ¢ãƒ¼ãƒ‰ã§æ¨è«–
    results = model.track(video_path, stream=True, persist=True, conf=0.5)

    for result in results:
        frame_count += 1

        # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä»˜ããƒ•ãƒ¬ãƒ¼ãƒ 
        annotated_frame = result.plot()

        # ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDã‚’è¡¨ç¤º
        if result.boxes.id is not None:
            for box, track_id in zip(result.boxes.xyxy, result.boxes.id):
                x1, y1, x2, y2 = box.cpu().numpy()
                track_id = int(track_id.cpu().numpy())

                # ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDæç”»
                cv2.putText(
                    annotated_frame,
                    f"ID: {track_id}",
                    (int(x1), int(y1) - 30),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.9,
                    (0, 255, 0),
                    2
                )

        # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ›¸ãè¾¼ã¿
        out.write(annotated_frame)

        # è¡¨ç¤º
        cv2.imshow('Object Tracking', annotated_frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    out.release()
    cv2.destroyAllWindows()

    print(f"å‡¦ç†å®Œäº†: {frame_count} ãƒ•ãƒ¬ãƒ¼ãƒ ")
    print(f"å‡ºåŠ›: {output_path}")

# ä½¿ç”¨ä¾‹
# track_objects_video('input_video.mp4', 'output_tracking.mp4')

# Webcamã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¿½è·¡
def track_webcam():
    """
    Webcamã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç‰©ä½“è¿½è·¡
    """
    results = model.track(source=0, stream=True, persist=True, conf=0.5)

    for result in results:
        annotated_frame = result.plot()
        cv2.imshow('Real-time Tracking', annotated_frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cv2.destroyAllWindows()

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¿½è·¡ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆè§£é™¤ã—ã¦å®Ÿè¡Œï¼‰
# track_webcam()

print("ç‰©ä½“è¿½è·¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æº–å‚™å®Œäº†")
</code></pre>

</details>

<h3>ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«æ¤œå‡º</h3>

<p>ç‰©ä½“ã®ã‚µã‚¤ã‚ºã¯ç”»åƒã«ã‚ˆã£ã¦å¤§ããç•°ãªã‚‹ãŸã‚ã€ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«æ¤œå‡ºãŒé‡è¦ã§ã™ã€‚</p>

<h4>ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</h4>

<ul>
<li><strong>Image Pyramid</strong>ï¼šç”»åƒã‚’è¤‡æ•°ã®ã‚¹ã‚±ãƒ¼ãƒ«ã§ãƒªã‚µã‚¤ã‚ºã—ã¦æ¤œå‡º</li>
<li><strong>Feature Pyramid</strong>ï¼šè¤‡æ•°ã®ç‰¹å¾´ãƒãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã§æ¤œå‡ºï¼ˆFPNï¼‰</li>
<li><strong>Multi-scale Training</strong>ï¼šè¨“ç·´æ™‚ã«ç•°ãªã‚‹ã‚µã‚¤ã‚ºã®å…¥åŠ›ã‚’ä½¿ç”¨</li>
</ul>

<h3>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æœ€é©åŒ–</h3>

<h4>é«˜é€ŸåŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</h4>

<ul>
<li><strong>ãƒ¢ãƒ‡ãƒ«ã®è»½é‡åŒ–</strong>ï¼šYOLOv8n, MobileNet-SSD</li>
<li><strong>é‡å­åŒ–</strong>ï¼šFP32 â†’ FP16 â†’ INT8</li>
<li><strong>TensorRT</strong>ï¼šNVIDIAã®æ¨è«–æœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³</li>
<li><strong>ONNX Runtime</strong>ï¼šã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ¨è«–</li>
<li><strong>è§£åƒåº¦èª¿æ•´</strong>ï¼šå…¥åŠ›ç”»åƒã‚µã‚¤ã‚ºã‚’å°ã•ãï¼ˆä¾‹ï¼š640â†’416ï¼‰</li>
</ul>

<blockquote>
<p><strong>é€Ÿåº¦ vs ç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•</strong>ï¼š</p>
<ul>
<li>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¦æ±‚ï¼šYOLOv8n/sï¼ˆ30+ FPSï¼‰</li>
<li>é«˜ç²¾åº¦è¦æ±‚ï¼šYOLOv8x, Faster R-CNN with FPN</li>
</ul>
</blockquote>

<hr>

<h2>ç·´ç¿’å•é¡Œ</h2>

<details>
<summary>æ¼”ç¿’1ï¼šIoUã¨NMSã®ç†è§£</summary>

<p><strong>å•é¡Œ</strong>ï¼šä»¥ä¸‹ã®Bounding Boxã‚»ãƒƒãƒˆã«å¯¾ã—ã¦IoUã‚’è¨ˆç®—ã—ã€NMSã‚’é©ç”¨ã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">boxes = np.array([
    [100, 100, 200, 200],
    [110, 110, 210, 210],
    [105, 105, 205, 205],
    [300, 300, 400, 400]
])
scores = np.array([0.9, 0.85, 0.95, 0.8])
</code></pre>

<ul>
<li>å„Boxã¨Box 0ã®IoUã‚’è¨ˆç®—</li>
<li>IoUé–¾å€¤0.5ã§NMSã‚’é©ç”¨</li>
<li>æ®‹ã‚‹Boxã‚’ç‰¹å®š</li>
</ul>

</details>

<details>
<summary>æ¼”ç¿’2ï¼šFaster R-CNNã§æ¤œå‡º</summary>

<p><strong>å•é¡Œ</strong>ï¼šäº‹å‰å­¦ç¿’æ¸ˆã¿Faster R-CNNã‚’ä½¿ã£ã¦ã€è¤‡æ•°ã®ç”»åƒã‹ã‚‰ç‰¹å®šã‚¯ãƒ©ã‚¹ï¼ˆä¾‹ï¼šperson, carï¼‰ã®ã¿ã‚’æ¤œå‡ºã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚</p>

<ul>
<li>è¤‡æ•°ç”»åƒã‚’èª­ã¿è¾¼ã¿</li>
<li>æŒ‡å®šã‚¯ãƒ©ã‚¹ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°</li>
<li>ä¿¡é ¼åº¦ãŒ0.7ä»¥ä¸Šã®ã‚‚ã®ã®ã¿è¡¨ç¤º</li>
<li>æ¤œå‡ºæ•°ã‚’é›†è¨ˆ</li>
</ul>

</details>

<details>
<summary>æ¼”ç¿’3ï¼šYOLOv8ã®ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºæ¯”è¼ƒ</summary>

<p><strong>å•é¡Œ</strong>ï¼šYOLOv8ã®ç•°ãªã‚‹ã‚µã‚¤ã‚ºï¼ˆn, s, m, lï¼‰ã§åŒã˜ç”»åƒã‚’æ¤œå‡ºã—ã€ç²¾åº¦ã¨é€Ÿåº¦ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>

<ul>
<li>å„ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–æ™‚é–“ã‚’æ¸¬å®š</li>
<li>æ¤œå‡ºã•ã‚ŒãŸç‰©ä½“æ•°ã‚’æ¯”è¼ƒ</li>
<li>ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒã‚’åˆ†æ</li>
<li>ã©ã®ãƒ¢ãƒ‡ãƒ«ãŒæœ€é©ã‹åˆ¤æ–­</li>
</ul>

</details>

<details>
<summary>æ¼”ç¿’4ï¼šã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™</summary>

<p><strong>å•é¡Œ</strong>ï¼šç‹¬è‡ªã®ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ10æšä»¥ä¸Šï¼‰ã‚’ç”¨æ„ã—ã€YOLOå½¢å¼ã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚</p>

<ul>
<li>LabelImgãªã©ã®ãƒ„ãƒ¼ãƒ«ã§ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ</li>
<li>YOLOå½¢å¼ï¼ˆclass x_center y_center width heightï¼‰ã«å¤‰æ›</li>
<li>train/valã«åˆ†å‰²ï¼ˆ80/20ï¼‰</li>
<li>dataset.yamlã‚’ä½œæˆ</li>
</ul>

</details>

<details>
<summary>æ¼”ç¿’5ï¼šç‰©ä½“è¿½è·¡ã®å®Ÿè£…</summary>

<p><strong>å•é¡Œ</strong>ï¼šãƒ“ãƒ‡ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯Webcamã‹ã‚‰ç‰©ä½“ã‚’è¿½è·¡ã—ã€å„ç‰©ä½“ã®è»Œè·¡ã‚’æç”»ã—ã¦ãã ã•ã„ã€‚</p>

<ul>
<li>YOLOv8ã®è¿½è·¡æ©Ÿèƒ½ã‚’ä½¿ç”¨</li>
<li>å„ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDã®è»Œè·¡ã‚’ä¿å­˜</li>
<li>è»Œè·¡ã‚’ç·šã§æç”»</li>
<li>ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã§IDãŒå®‰å®šã—ã¦ã„ã‚‹ã‹ç¢ºèª</li>
</ul>

</details>

<details>
<summary>æ¼”ç¿’6ï¼šãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œå‡ºã®æœ€é©åŒ–</summary>

<p><strong>å•é¡Œ</strong>ï¼šæ¤œå‡ºé€Ÿåº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹ãŸã‚ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’æœ€é©åŒ–ã—ã¦ãã ã•ã„ã€‚</p>

<ul>
<li>YOLOv8ã‚’ONNXå½¢å¼ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ</li>
<li>ç•°ãªã‚‹å…¥åŠ›è§£åƒåº¦ï¼ˆ320, 416, 640ï¼‰ã§FPSã‚’æ¸¬å®š</li>
<li>ä¿¡é ¼åº¦é–¾å€¤ã‚’èª¿æ•´ã—ã¦é€Ÿåº¦æ”¹å–„</li>
<li>é€Ÿåº¦ã¨ç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’åˆ†æ</li>
</ul>

</details>

<hr>

<h2>ã¾ã¨ã‚</h2>

<p>ã“ã®ç« ã§ã¯ã€ç‰©ä½“æ¤œå‡ºã®åŸºç¤ã‹ã‚‰å®Ÿè·µã¾ã§ã‚’å­¦ã³ã¾ã—ãŸï¼š</p>

<ul>
<li>âœ… <strong>ç‰©ä½“æ¤œå‡ºã®åŸºç¤</strong>ï¼šBounding Boxè¡¨ç¾ã€IoUã€NMSã€è©•ä¾¡æŒ‡æ¨™ï¼ˆmAPï¼‰</li>
<li>âœ… <strong>Two-Stageæ¤œå‡ºå™¨</strong>ï¼šR-CNNã€Fast R-CNNã€Faster R-CNNã€FPNã®é€²åŒ–ã¨ç‰¹å¾´</li>
<li>âœ… <strong>One-Stageæ¤œå‡ºå™¨</strong>ï¼šYOLOã€SSDã€RetinaNetã€EfficientDetã®åŸç†ã¨æ¯”è¼ƒ</li>
<li>âœ… <strong>å®Ÿè£…ã¨è¨“ç·´</strong>ï¼šPyTorchã€YOLOv8ã‚’ä½¿ã£ãŸå®Ÿè·µçš„ãªæ¤œå‡ºã¨ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨“ç·´</li>
<li>âœ… <strong>å¿œç”¨ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</strong>ï¼šAnchor-freeæ¤œå‡ºã€ç‰©ä½“è¿½è·¡ã€ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«æ¤œå‡ºã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æœ€é©åŒ–</li>
</ul>

<p>æ¬¡ç« ã§ã¯ã€<strong>ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³</strong>ã‚’å­¦ã³ã¾ã™ã€‚ãƒ”ã‚¯ã‚»ãƒ«ãƒ¬ãƒ™ãƒ«ã®åˆ†é¡ã€U-Netã€DeepLabã€Mask R-CNNãªã©ã€ã‚ˆã‚Šè©³ç´°ãªç”»åƒç†è§£ã®æ‰‹æ³•ã‚’ç†è§£ã—ã¦ã„ãã¾ã™ã€‚</p>

<blockquote>
<p><strong>é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ</strong>ï¼šç‰©ä½“æ¤œå‡ºã¯ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§ã¨ç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒé‡è¦ã§ã™ã€‚ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®è¦ä»¶ï¼ˆé€Ÿåº¦å„ªå…ˆ or ç²¾åº¦å„ªå…ˆï¼‰ã«å¿œã˜ã¦ã€é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é¸æŠã—ã¾ã—ã‚‡ã†ã€‚</p>
</blockquote>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ul>
<li>Girshick et al. (2014). "Rich feature hierarchies for accurate object detection and semantic segmentation" (R-CNN)</li>
<li>Girshick (2015). "Fast R-CNN"</li>
<li>Ren et al. (2015). "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"</li>
<li>Redmon et al. (2016). "You Only Look Once: Unified, Real-Time Object Detection" (YOLO)</li>
<li>Liu et al. (2016). "SSD: Single Shot MultiBox Detector"</li>
<li>Lin et al. (2017). "Focal Loss for Dense Object Detection" (RetinaNet)</li>
<li>Lin et al. (2017). "Feature Pyramid Networks for Object Detection"</li>
<li>Bochkovskiy et al. (2020). "YOLOv4: Optimal Speed and Accuracy of Object Detection"</li>
<li>Ultralytics YOLOv8: <a href="https://github.com/ultralytics/ultralytics">https://github.com/ultralytics/ultralytics</a></li>
<li>COCO Dataset: <a href="https://cocodataset.org/">https://cocodataset.org/</a></li>
</ul>

<div class="navigation">
    <a href="chapter2-cnn-architectures.html" class="nav-button">â† ç¬¬2ç« ï¼šCNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</a>
    <a href="chapter4-semantic-segmentation.html" class="nav-button">ç¬¬4ç« ï¼šã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
        <p>ã“ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ç›®çš„ã§ä½œæˆã•ã‚Œã¦ã„ã¾ã™ã€‚</p>
    </footer>
</body>
</html>
