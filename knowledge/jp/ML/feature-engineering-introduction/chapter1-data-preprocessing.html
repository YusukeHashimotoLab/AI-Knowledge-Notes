<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬1ç« ï¼šãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†åŸºç¤ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
        <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/wp/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/ML/feature-engineering-introduction/index.html">Feature Engineering</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 1</span>
        </div>
    </nav>

    <header>
        <div class="header-content">
            <h1>ç¬¬1ç« ï¼šãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†åŸºç¤</h1>
            <p class="subtitle">ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®åŸºç›¤ - ãƒ‡ãƒ¼ã‚¿ã®å“è³ªã‚’é«˜ã‚ã‚‹</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 20-25åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 12å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®é‡è¦æ€§ã¨å…¨ä½“åƒã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… æ¬ æå€¤ã®ç¨®é¡ã¨é©åˆ‡ãªå‡¦ç†æ–¹æ³•ã‚’é¸æŠã§ãã‚‹</li>
<li>âœ… å¤–ã‚Œå€¤ã‚’æ¤œå‡ºã—ã€é©åˆ‡ã«å¯¾å‡¦ã§ãã‚‹</li>
<li>âœ… ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¨æ­£è¦åŒ–ã®é•ã„ã‚’ç†è§£ã—ã€ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹</li>
<li>âœ… scikit-learnã§å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
<li>âœ… å®Ÿãƒ‡ãƒ¼ã‚¿ã§åŒ…æ‹¬çš„ãªå‰å‡¦ç†ã‚’å®Ÿè¡Œã§ãã‚‹</li>
</ul>

<hr>

<h2>1.1 ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®é‡è¦æ€§</h2>

<h3>ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¨ã¯</h3>
<p><strong>ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ï¼ˆData Preprocessingï¼‰</strong>ã¯ã€ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«é©ã—ãŸå½¢å¼ã«å¤‰æ›ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã§ã™ã€‚</p>

<blockquote>
<p>ã€ŒGarbage In, Garbage Outï¼ˆGIGOï¼‰ã€- ãƒ‡ãƒ¼ã‚¿ã®å“è³ªãŒã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æ±ºå®šã—ã¾ã™ã€‚</p>
</blockquote>

<h3>å‰å‡¦ç†ãŒå¿…è¦ãªç†ç”±</h3>

<table>
<thead>
<tr>
<th>å•é¡Œ</th>
<th>å½±éŸ¿</th>
<th>å¯¾å‡¦æ³•</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ¬ æå€¤</strong></td>
<td>å­¦ç¿’ã‚¨ãƒ©ãƒ¼ã€åã£ãŸäºˆæ¸¬</td>
<td>è£œå®Œã€å‰Šé™¤</td>
</tr>
<tr>
<td><strong>å¤–ã‚Œå€¤</strong></td>
<td>ãƒ¢ãƒ‡ãƒ«ã®æ­ªã¿ã€éå­¦ç¿’</td>
<td>æ¤œå‡ºã€å¤‰æ›ã€å‰Šé™¤</td>
</tr>
<tr>
<td><strong>ã‚¹ã‚±ãƒ¼ãƒ«å·®</strong></td>
<td>å­¦ç¿’ã®ä¸å®‰å®šåŒ–</td>
<td>æ­£è¦åŒ–ã€æ¨™æº–åŒ–</td>
</tr>
<tr>
<td><strong>ä¸è¦ãªç‰¹å¾´</strong></td>
<td>æ¬¡å…ƒã®å‘ªã„ã€éå­¦ç¿’</td>
<td>ç‰¹å¾´é¸æŠã€æ¬¡å…ƒå‰Šæ¸›</td>
</tr>
</tbody>
</table>

<h3>ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®å…¨ä½“åƒ</h3>

<div class="mermaid">
graph TD
    A[ç”Ÿãƒ‡ãƒ¼ã‚¿] --> B[æ¬ æå€¤å‡¦ç†]
    B --> C[å¤–ã‚Œå€¤å‡¦ç†]
    C --> D[ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ»æ­£è¦åŒ–]
    D --> E[ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°]
    E --> F[ç‰¹å¾´é¸æŠ]
    F --> G[å­¦ç¿’æº–å‚™å®Œäº†]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#e8f5e9
    style F fill:#fce4ec
    style G fill:#c8e6c9
</div>

<h3>å®Ÿä¾‹ï¼šå‰å‡¦ç†ã®åŠ¹æœ</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã®ç‰¹å¾´é‡ï¼‰
np.random.seed(42)
n_samples = 1000

# ç‰¹å¾´é‡1: å¹´é½¢ï¼ˆ20-60ï¼‰
X1 = np.random.uniform(20, 60, n_samples)

# ç‰¹å¾´é‡2: å¹´åï¼ˆ300-1000ä¸‡å††ï¼‰
X2 = np.random.uniform(300, 1000, n_samples)

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: å¹´é½¢ã¨å¹´åã«åŸºã¥ã
y = ((X1 > 40) & (X2 > 600)).astype(int)

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
X = pd.DataFrame({'age': X1, 'income': X2})

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# å‰å‡¦ç†ãªã—ã§ã®å­¦ç¿’
model_raw = LogisticRegression(random_state=42, max_iter=1000)
model_raw.fit(X_train, y_train)
acc_raw = accuracy_score(y_test, model_raw.predict(X_test))

# å‰å‡¦ç†ã‚ã‚Šã§ã®å­¦ç¿’
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model_scaled = LogisticRegression(random_state=42, max_iter=1000)
model_scaled.fit(X_train_scaled, y_train)
acc_scaled = accuracy_score(y_test, model_scaled.predict(X_test_scaled))

print("=== å‰å‡¦ç†ã®åŠ¹æœæ¯”è¼ƒ ===")
print(f"å‰å‡¦ç†ãªã—: ç²¾åº¦ = {acc_raw:.3f}")
print(f"å‰å‡¦ç†ã‚ã‚Š: ç²¾åº¦ = {acc_scaled:.3f}")
print(f"æ”¹å–„: {(acc_scaled - acc_raw) * 100:.1f}%")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== å‰å‡¦ç†ã®åŠ¹æœæ¯”è¼ƒ ===
å‰å‡¦ç†ãªã—: ç²¾åº¦ = 0.890
å‰å‡¦ç†ã‚ã‚Š: ç²¾åº¦ = 0.920
æ”¹å–„: 3.0%
</code></pre>

<blockquote>
<p><strong>é‡è¦</strong>: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ã‚ˆã‚Šã€åæŸé€Ÿåº¦ã¨ç²¾åº¦ãŒå‘ä¸Šã—ã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>1.2 æ¬ æå€¤å‡¦ç†</h2>

<h3>æ¬ æå€¤ã®ã‚¿ã‚¤ãƒ—</h3>

<p>æ¬ æå€¤ã¯ç™ºç”Ÿãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«ã‚ˆã‚Š3ç¨®é¡ã«åˆ†é¡ã•ã‚Œã¾ã™ï¼š</p>

<table>
<thead>
<tr>
<th>ã‚¿ã‚¤ãƒ—</th>
<th>èª¬æ˜</th>
<th>ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MCAR</strong><br>(Missing Completely At Random)</td>
<td>å®Œå…¨ã«ãƒ©ãƒ³ãƒ€ãƒ ã«æ¬ æ</td>
<td>æ©Ÿå™¨ã®æ•…éšœã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿æå¤±</td>
</tr>
<tr>
<td><strong>MAR</strong><br>(Missing At Random)</td>
<td>ä»–ã®å¤‰æ•°ã«ä¾å­˜ã—ã¦æ¬ æ</td>
<td>é«˜é½¢è€…ã»ã©å¹´åã‚’ç­”ãˆãªã„</td>
</tr>
<tr>
<td><strong>MNAR</strong><br>(Missing Not At Random)</td>
<td>æ¬ æå€¤è‡ªä½“ã«æ„å‘³ãŒã‚ã‚‹</td>
<td>ä½æ‰€å¾—è€…ãŒå¹´åã‚’è¨˜å…¥ã—ãªã„</td>
</tr>
</tbody>
</table>

<h3>æ¬ æå€¤ã®å¯è¦–åŒ–</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# æ¬ æå€¤ã‚’å«ã‚€ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
n = 100

df = pd.DataFrame({
    'age': np.random.randint(20, 70, n),
    'income': np.random.randint(300, 1200, n),
    'score': np.random.uniform(0, 100, n)
})

# æ„å›³çš„ã«æ¬ æå€¤ã‚’ä½œæˆ
# å¹´é½¢ã«10%ã®æ¬ æ
missing_age = np.random.choice(n, size=int(n * 0.1), replace=False)
df.loc[missing_age, 'age'] = np.nan

# å¹´åã«20%ã®æ¬ æï¼ˆå¹´é½¢ä¾å­˜ï¼‰
missing_income = df[df['age'] > 50].sample(frac=0.4).index
df.loc[missing_income, 'income'] = np.nan

# ã‚¹ã‚³ã‚¢ã«15%ã®æ¬ æ
missing_score = np.random.choice(n, size=int(n * 0.15), replace=False)
df.loc[missing_score, 'score'] = np.nan

# æ¬ æå€¤ã®ç¢ºèª
print("=== æ¬ æå€¤ã®çŠ¶æ³ ===")
print(df.isnull().sum())
print(f"\næ¬ æç‡:")
print((df.isnull().sum() / len(df) * 100).round(2))

# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis')
plt.title('æ¬ æå€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯è¦–åŒ–ï¼ˆé»„è‰² = æ¬ æï¼‰', fontsize=14)
plt.xlabel('ç‰¹å¾´é‡')
plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== æ¬ æå€¤ã®çŠ¶æ³ ===
age        10
income     16
score      15
dtype: int64

æ¬ æç‡:
age        10.0
income     16.0
score      15.0
dtype: float64
</code></pre>

<h3>å‰Šé™¤æ³•</h3>

<h4>è¡Œå‰Šé™¤ï¼ˆListwise Deletionï¼‰</h4>

<pre><code class="language-python"># æ¬ æå€¤ã‚’å«ã‚€è¡Œã‚’å‰Šé™¤
df_droprows = df.dropna()

print(f"å…ƒã®ãƒ‡ãƒ¼ã‚¿: {len(df)}è¡Œ")
print(f"å‰Šé™¤å¾Œ: {len(df_droprows)}è¡Œ")
print(f"å‰Šé™¤ã•ã‚ŒãŸè¡Œ: {len(df) - len(df_droprows)}è¡Œ ({(1 - len(df_droprows)/len(df))*100:.1f}%)")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>å…ƒã®ãƒ‡ãƒ¼ã‚¿: 100è¡Œ
å‰Šé™¤å¾Œ: 64è¡Œ
å‰Šé™¤ã•ã‚ŒãŸè¡Œ: 36è¡Œ (36.0%)
</code></pre>

<blockquote>
<p><strong>æ³¨æ„</strong>: ãƒ‡ãƒ¼ã‚¿é‡ãŒå¤§å¹…ã«æ¸›å°‘ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚</p>
</blockquote>

<h4>åˆ—å‰Šé™¤</h4>

<pre><code class="language-python"># æ¬ æç‡ãŒ30%ä»¥ä¸Šã®åˆ—ã‚’å‰Šé™¤
threshold = 0.3
df_dropcols = df.loc[:, df.isnull().mean() < threshold]

print(f"å…ƒã®ç‰¹å¾´é‡: {df.shape[1]}å€‹")
print(f"å‰Šé™¤å¾Œ: {df_dropcols.shape[1]}å€‹")
print(f"\nå‰Šé™¤ã•ã‚ŒãŸç‰¹å¾´é‡: {set(df.columns) - set(df_dropcols.columns)}")
</code></pre>

<h3>è£œå®Œæ³•ï¼ˆImputationï¼‰</h3>

<h4>å˜ç´”è£œå®Œ</h4>

<pre><code class="language-python">from sklearn.impute import SimpleImputer

# å¹³å‡å€¤è£œå®Œ
imputer_mean = SimpleImputer(strategy='mean')
df_mean = pd.DataFrame(
    imputer_mean.fit_transform(df),
    columns=df.columns
)

# ä¸­å¤®å€¤è£œå®Œ
imputer_median = SimpleImputer(strategy='median')
df_median = pd.DataFrame(
    imputer_median.fit_transform(df),
    columns=df.columns
)

# æœ€é »å€¤è£œå®Œ
imputer_mode = SimpleImputer(strategy='most_frequent')
df_mode = pd.DataFrame(
    imputer_mode.fit_transform(df),
    columns=df.columns
)

# å®šæ•°è£œå®Œ
imputer_constant = SimpleImputer(strategy='constant', fill_value=0)
df_constant = pd.DataFrame(
    imputer_constant.fit_transform(df),
    columns=df.columns
)

print("=== è£œå®Œæ–¹æ³•ã®æ¯”è¼ƒ ===\n")
print(f"å…ƒãƒ‡ãƒ¼ã‚¿ã®å¹´é½¢å¹³å‡: {df['age'].mean():.2f}")
print(f"å¹³å‡å€¤è£œå®Œå¾Œ: {df_mean['age'].mean():.2f}")
print(f"ä¸­å¤®å€¤è£œå®Œå¾Œ: {df_median['age'].median():.2f}")
print(f"æœ€é »å€¤è£œå®Œ: {df_mode['age'].mode()[0]:.2f}")
</code></pre>

<h4>Kè¿‘å‚æ³•è£œå®Œï¼ˆKNN Imputerï¼‰</h4>

<pre><code class="language-python">from sklearn.impute import KNNImputer

# KNNè£œå®Œï¼ˆk=5ï¼‰
knn_imputer = KNNImputer(n_neighbors=5)
df_knn = pd.DataFrame(
    knn_imputer.fit_transform(df),
    columns=df.columns
)

print("\n=== KNNè£œå®Œã®è©³ç´° ===")
print(f"æ¬ æå‰ã®å¹´é½¢å¹³å‡: {df['age'].mean():.2f}")
print(f"KNNè£œå®Œå¾Œã®å¹´é½¢å¹³å‡: {df_knn['age'].mean():.2f}")

# å¯è¦–åŒ–ï¼šè£œå®Œæ–¹æ³•ã®æ¯”è¼ƒ
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
methods = [
    ('å…ƒãƒ‡ãƒ¼ã‚¿', df),
    ('å¹³å‡å€¤è£œå®Œ', df_mean),
    ('ä¸­å¤®å€¤è£œå®Œ', df_median),
    ('æœ€é »å€¤è£œå®Œ', df_mode),
    ('å®šæ•°è£œå®Œ', df_constant),
    ('KNNè£œå®Œ', df_knn)
]

for ax, (name, data) in zip(axes.flat, methods):
    ax.scatter(data['age'], data['income'], alpha=0.6)
    ax.set_xlabel('å¹´é½¢')
    ax.set_ylabel('å¹´å')
    ax.set_title(name)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3>å¤šé‡ä»£å…¥æ³•ï¼ˆMultiple Imputationï¼‰</h3>

<pre><code class="language-python">from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# å¤šé‡ä»£å…¥æ³•ï¼ˆMICE: Multivariate Imputation by Chained Equationsï¼‰
mice_imputer = IterativeImputer(random_state=42, max_iter=10)
df_mice = pd.DataFrame(
    mice_imputer.fit_transform(df),
    columns=df.columns
)

print("=== å¤šé‡ä»£å…¥æ³•ï¼ˆMICEï¼‰===")
print(f"è£œå®Œå‰ã®æ¬ ææ•°: {df.isnull().sum().sum()}")
print(f"è£œå®Œå¾Œã®æ¬ ææ•°: {df_mice.isnull().sum().sum()}")
print(f"\nå„ç‰¹å¾´é‡ã®è£œå®Œå¾Œã®çµ±è¨ˆ:")
print(df_mice.describe())
</code></pre>

<h3>è£œå®Œæ–¹æ³•ã®é¸æŠã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³</h3>

<table>
<thead>
<tr>
<th>çŠ¶æ³</th>
<th>æ¨å¥¨æ‰‹æ³•</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td>æ¬ æç‡ < 5%</td>
<td>å‰Šé™¤æ³•</td>
<td>æƒ…å ±æå¤±ãŒå°‘ãªã„</td>
</tr>
<tr>
<td>æ•°å€¤ã€æ­£è¦åˆ†å¸ƒ</td>
<td>å¹³å‡å€¤è£œå®Œ</td>
<td>åˆ†å¸ƒã‚’ä¿æŒ</td>
</tr>
<tr>
<td>æ•°å€¤ã€å¤–ã‚Œå€¤ã‚ã‚Š</td>
<td>ä¸­å¤®å€¤è£œå®Œ</td>
<td>ãƒ­ãƒã‚¹ãƒˆ</td>
</tr>
<tr>
<td>ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«</td>
<td>æœ€é »å€¤è£œå®Œ</td>
<td>å¦¥å½“ãªæ¨å®š</td>
</tr>
<tr>
<td>ç‰¹å¾´é–“ã«ç›¸é–¢</td>
<td>KNNã€MICE</td>
<td>é–¢ä¿‚æ€§ã‚’åˆ©ç”¨</td>
</tr>
<tr>
<td>MNAR</td>
<td>ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜æ´»ç”¨</td>
<td>æ¬ æè‡ªä½“ãŒæƒ…å ±</td>
</tr>
</tbody>
</table>

<hr>

<h2>1.3 å¤–ã‚Œå€¤å‡¦ç†</h2>

<h3>å¤–ã‚Œå€¤ã¨ã¯</h3>

<p><strong>å¤–ã‚Œå€¤ï¼ˆOutlierï¼‰</strong>ã¯ã€ä»–ã®ãƒ‡ãƒ¼ã‚¿ã¨å¤§ããç•°ãªã‚‹å€¤ã§ã€æ¸¬å®šã‚¨ãƒ©ãƒ¼ã¾ãŸã¯çœŸã®ç•°å¸¸å€¤ã§ã™ã€‚</p>

<h3>å¤–ã‚Œå€¤æ¤œå‡ºæ–¹æ³•</h3>

<h4>1. IQRæ³•ï¼ˆå››åˆ†ä½ç¯„å›²ï¼‰</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆå¤–ã‚Œå€¤ã‚’å«ã‚€ï¼‰
np.random.seed(42)
data_normal = np.random.normal(50, 10, 95)
outliers = np.array([100, 105, 110, 0, -5])  # å¤–ã‚Œå€¤
data = np.concatenate([data_normal, outliers])

# IQRæ³•ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º
Q1 = np.percentile(data, 25)
Q3 = np.percentile(data, 75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers_iqr = (data < lower_bound) | (data > upper_bound)

print("=== IQRæ³•ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º ===")
print(f"Q1 (25%ç‚¹): {Q1:.2f}")
print(f"Q3 (75%ç‚¹): {Q3:.2f}")
print(f"IQR: {IQR:.2f}")
print(f"ä¸‹é™: {lower_bound:.2f}")
print(f"ä¸Šé™: {upper_bound:.2f}")
print(f"å¤–ã‚Œå€¤ã®æ•°: {outliers_iqr.sum()}")
print(f"å¤–ã‚Œå€¤: {data[outliers_iqr]}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ
axes[0].boxplot(data, vert=True)
axes[0].axhline(y=lower_bound, color='r', linestyle='--',
                label=f'ä¸‹é™: {lower_bound:.1f}')
axes[0].axhline(y=upper_bound, color='r', linestyle='--',
                label=f'ä¸Šé™: {upper_bound:.1f}')
axes[0].set_ylabel('å€¤')
axes[0].set_title('ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆï¼ˆIQRæ³•ï¼‰', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
axes[1].hist(data, bins=20, alpha=0.7, edgecolor='black')
axes[1].axvline(x=lower_bound, color='r', linestyle='--', linewidth=2)
axes[1].axvline(x=upper_bound, color='r', linestyle='--', linewidth=2)
axes[1].set_xlabel('å€¤')
axes[1].set_ylabel('é »åº¦')
axes[1].set_title('ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ', fontsize=14)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== IQRæ³•ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º ===
Q1 (25%ç‚¹): 43.26
Q3 (75%ç‚¹): 56.83
IQR: 13.57
ä¸‹é™: 22.90
ä¸Šé™: 77.19
å¤–ã‚Œå€¤ã®æ•°: 5
å¤–ã‚Œå€¤: [100. 105. 110.   0.  -5.]
</code></pre>

<h4>2. Z-scoreã«ã‚ˆã‚‹æ¤œå‡º</h4>

<pre><code class="language-python">from scipy import stats

# Z-scoreã®è¨ˆç®—
z_scores = np.abs(stats.zscore(data))
threshold = 3  # é€šå¸¸ã€3ã‚’é–¾å€¤ã¨ã™ã‚‹

outliers_zscore = z_scores > threshold

print("\n=== Z-scoreã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º ===")
print(f"é–¾å€¤: {threshold}")
print(f"å¤–ã‚Œå€¤ã®æ•°: {outliers_zscore.sum()}")
print(f"å¤–ã‚Œå€¤ã®Z-score: {z_scores[outliers_zscore]}")
print(f"å¤–ã‚Œå€¤: {data[outliers_zscore]}")

# å¯è¦–åŒ–
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(range(len(data)), data, c=outliers_zscore,
            cmap='coolwarm', s=50, alpha=0.7, edgecolors='black')
plt.axhline(y=data.mean() + 3*data.std(), color='r',
            linestyle='--', label='+3Ïƒ')
plt.axhline(y=data.mean() - 3*data.std(), color='r',
            linestyle='--', label='-3Ïƒ')
plt.xlabel('ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹')
plt.ylabel('å€¤')
plt.title('ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆï¼ˆèµ¤ = å¤–ã‚Œå€¤ï¼‰', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(range(len(z_scores)), z_scores,
            c=outliers_zscore, cmap='coolwarm', s=50,
            alpha=0.7, edgecolors='black')
plt.axhline(y=threshold, color='r', linestyle='--',
            label=f'é–¾å€¤: {threshold}')
plt.xlabel('ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹')
plt.ylabel('|Z-score|')
plt.title('Z-scoreã®åˆ†å¸ƒ', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h4>3. Isolation Forestã«ã‚ˆã‚‹æ¤œå‡º</h4>

<pre><code class="language-python">from sklearn.ensemble import IsolationForest

# ãƒ‡ãƒ¼ã‚¿ã‚’2æ¬¡å…ƒã«æ‹¡å¼µ
np.random.seed(42)
X = np.random.normal(50, 10, (95, 2))
X_outliers = np.array([[100, 100], [105, 105], [0, 0], [-5, -5], [110, 110]])
X_combined = np.vstack([X, X_outliers])

# Isolation Forest
iso_forest = IsolationForest(contamination=0.05, random_state=42)
outliers_iso = iso_forest.fit_predict(X_combined)
# -1: å¤–ã‚Œå€¤, 1: æ­£å¸¸å€¤

print("\n=== Isolation Forestã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º ===")
print(f"å¤–ã‚Œå€¤ã®æ•°: {(outliers_iso == -1).sum()}")
print(f"æ­£å¸¸å€¤ã®æ•°: {(outliers_iso == 1).sum()}")

# å¯è¦–åŒ–
plt.figure(figsize=(10, 8))
plt.scatter(X_combined[outliers_iso == 1, 0],
            X_combined[outliers_iso == 1, 1],
            c='blue', label='æ­£å¸¸å€¤', alpha=0.6, s=50, edgecolors='black')
plt.scatter(X_combined[outliers_iso == -1, 0],
            X_combined[outliers_iso == -1, 1],
            c='red', label='å¤–ã‚Œå€¤', alpha=0.8, s=100,
            edgecolors='black', marker='X')
plt.xlabel('ç‰¹å¾´é‡ 1')
plt.ylabel('ç‰¹å¾´é‡ 2')
plt.title('Isolation Forestã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

<h3>å¤–ã‚Œå€¤ã®æ‰±ã„æ–¹</h3>

<h4>1. å‰Šé™¤</h4>

<pre><code class="language-python"># å¤–ã‚Œå€¤ã‚’å‰Šé™¤
data_cleaned = data[~outliers_iqr]

print(f"å…ƒã®ãƒ‡ãƒ¼ã‚¿: {len(data)}å€‹")
print(f"å‰Šé™¤å¾Œ: {len(data_cleaned)}å€‹")
print(f"å¹³å‡ï¼ˆå‰Šé™¤å‰ï¼‰: {data.mean():.2f}")
print(f"å¹³å‡ï¼ˆå‰Šé™¤å¾Œï¼‰: {data_cleaned.mean():.2f}")
</code></pre>

<h4>2. å¤‰æ›ï¼ˆå¯¾æ•°å¤‰æ›ï¼‰</h4>

<pre><code class="language-python"># å¯¾æ•°å¤‰æ›ï¼ˆæ­£ã®å€¤ã®ã¿ï¼‰
data_positive = data[data > 0]
data_log = np.log1p(data_positive)  # log(1 + x) ã§0ã‚’å›é¿

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].hist(data_positive, bins=20, alpha=0.7, edgecolor='black')
axes[0].set_xlabel('å€¤')
axes[0].set_ylabel('é »åº¦')
axes[0].set_title('å…ƒã®ãƒ‡ãƒ¼ã‚¿', fontsize=14)
axes[0].grid(True, alpha=0.3)

axes[1].hist(data_log, bins=20, alpha=0.7, edgecolor='black', color='orange')
axes[1].set_xlabel('log(å€¤)')
axes[1].set_ylabel('é »åº¦')
axes[1].set_title('å¯¾æ•°å¤‰æ›å¾Œ', fontsize=14)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h4>3. ã‚­ãƒ£ãƒƒãƒ—å‡¦ç†ï¼ˆWinsorizationï¼‰</h4>

<pre><code class="language-python">from scipy.stats import mstats

# Winsorization: å¤–ã‚Œå€¤ã‚’ä¸Šä¸‹é™ã«ã‚­ãƒ£ãƒƒãƒ—
data_winsorized = mstats.winsorize(data, limits=[0.05, 0.05])

print("\n=== Winsorizationï¼ˆä¸Šä¸‹5%ã‚’ã‚­ãƒ£ãƒƒãƒ—ï¼‰===")
print(f"å…ƒã®ãƒ‡ãƒ¼ã‚¿ç¯„å›²: [{data.min():.2f}, {data.max():.2f}]")
print(f"å‡¦ç†å¾Œã®ç¯„å›²: [{data_winsorized.min():.2f}, {data_winsorized.max():.2f}]")

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].boxplot(data)
axes[0].set_ylabel('å€¤')
axes[0].set_title('å…ƒã®ãƒ‡ãƒ¼ã‚¿', fontsize=14)
axes[0].grid(True, alpha=0.3)

axes[1].boxplot(data_winsorized)
axes[1].set_ylabel('å€¤')
axes[1].set_title('Winsorizationå¾Œ', fontsize=14)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3>å¤–ã‚Œå€¤å‡¦ç†ã®é¸æŠã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³</h3>

<table>
<thead>
<tr>
<th>çŠ¶æ³</th>
<th>æ¨å¥¨æ‰‹æ³•</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td>æ˜ã‚‰ã‹ãªã‚¨ãƒ©ãƒ¼</td>
<td>å‰Šé™¤</td>
<td>ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Š</td>
</tr>
<tr>
<td>çœŸã®æ¥µç«¯ãªå€¤</td>
<td>ä¿æŒã¾ãŸã¯ã‚­ãƒ£ãƒƒãƒ—</td>
<td>æƒ…å ±ã‚’ç¶­æŒ</td>
</tr>
<tr>
<td>æ­ªã‚“ã åˆ†å¸ƒ</td>
<td>å¯¾æ•°å¤‰æ›</td>
<td>æ­£è¦åˆ†å¸ƒã«è¿‘ã¥ã‘ã‚‹</td>
</tr>
<tr>
<td>é ‘å¥æ€§ãŒå¿…è¦</td>
<td>Winsorization</td>
<td>å½±éŸ¿ã‚’æŠ‘ãˆã‚‹</td>
</tr>
<tr>
<td>å¤šæ¬¡å…ƒãƒ‡ãƒ¼ã‚¿</td>
<td>Isolation Forest</td>
<td>è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º</td>
</tr>
</tbody>
</table>

<hr>

<h2>1.4 ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¨æ­£è¦åŒ–</h2>

<h3>ãªãœã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒå¿…è¦ã‹</h3>

<p>ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒ«ãŒç•°ãªã‚‹ã¨ã€ä»¥ä¸‹ã®å•é¡ŒãŒç™ºç”Ÿã—ã¾ã™ï¼š</p>

<ul>
<li>è·é›¢ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆKNNã€SVMï¼‰ãŒå¤§ããªå€¤ã«æ”¯é…ã•ã‚Œã‚‹</li>
<li>å‹¾é…é™ä¸‹æ³•ã®åæŸãŒé…ããªã‚‹</li>
<li>æ­£å‰‡åŒ–ã®åŠ¹æœãŒä¸å‡ç­‰ã«ãªã‚‹</li>
</ul>

<h3>1. StandardScalerï¼ˆæ¨™æº–åŒ–ï¼‰</h3>

<p><strong>æ¨™æº–åŒ–ï¼ˆStandardizationï¼‰</strong>ã¯ã€å¹³å‡0ã€æ¨™æº–åå·®1ã«å¤‰æ›ã—ã¾ã™ã€‚</p>

<p>$$
z = \frac{x - \mu}{\sigma}
$$</p>

<ul>
<li>$\mu$: å¹³å‡</li>
<li>$\sigma$: æ¨™æº–åå·®</li>
</ul>

<pre><code class="language-python">from sklearn.preprocessing import StandardScaler
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
np.random.seed(42)
data = pd.DataFrame({
    'age': np.random.randint(20, 70, 100),
    'income': np.random.randint(300, 1500, 100),
    'score': np.random.uniform(0, 100, 100)
})

print("=== å…ƒã®ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆ ===")
print(data.describe())

# StandardScalerã®é©ç”¨
scaler = StandardScaler()
data_scaled = pd.DataFrame(
    scaler.fit_transform(data),
    columns=data.columns
)

print("\n=== æ¨™æº–åŒ–å¾Œã®ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆ ===")
print(data_scaled.describe())

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

for i, col in enumerate(data.columns):
    # å…ƒã®ãƒ‡ãƒ¼ã‚¿
    axes[0, i].hist(data[col], bins=20, alpha=0.7, edgecolor='black')
    axes[0, i].set_xlabel(col)
    axes[0, i].set_ylabel('é »åº¦')
    axes[0, i].set_title(f'{col} (å…ƒãƒ‡ãƒ¼ã‚¿)', fontsize=12)
    axes[0, i].grid(True, alpha=0.3)

    # æ¨™æº–åŒ–å¾Œ
    axes[1, i].hist(data_scaled[col], bins=20, alpha=0.7,
                    edgecolor='black', color='orange')
    axes[1, i].set_xlabel(col)
    axes[1, i].set_ylabel('é »åº¦')
    axes[1, i].set_title(f'{col} (æ¨™æº–åŒ–å¾Œ)', fontsize=12)
    axes[1, i].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3>2. MinMaxScalerï¼ˆæ­£è¦åŒ–ï¼‰</h3>

<p><strong>æ­£è¦åŒ–ï¼ˆNormalizationï¼‰</strong>ã¯ã€å€¤ã‚’æŒ‡å®šç¯„å›²ï¼ˆé€šå¸¸[0, 1]ï¼‰ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚</p>

<p>$$
x_{\text{norm}} = \frac{x - x_{\min}}{x_{\max} - x_{\min}}
$$</p>

<pre><code class="language-python">from sklearn.preprocessing import MinMaxScaler

# MinMaxScalerã®é©ç”¨
minmax_scaler = MinMaxScaler(feature_range=(0, 1))
data_minmax = pd.DataFrame(
    minmax_scaler.fit_transform(data),
    columns=data.columns
)

print("=== MinMaxScalerï¼ˆæ­£è¦åŒ–ï¼‰å¾Œã®çµ±è¨ˆ ===")
print(data_minmax.describe())

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for i, col in enumerate(data.columns):
    axes[i].hist(data_minmax[col], bins=20, alpha=0.7,
                 edgecolor='black', color='green')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('é »åº¦')
    axes[i].set_title(f'{col} (MinMax: [0,1])', fontsize=12)
    axes[i].set_xlim(-0.1, 1.1)
    axes[i].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3>3. RobustScalerï¼ˆãƒ­ãƒã‚¹ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰</h3>

<p><strong>RobustScaler</strong>ã¯ã€ä¸­å¤®å€¤ã¨IQRã‚’ä½¿ã„ã€å¤–ã‚Œå€¤ã«é ‘å¥ã§ã™ã€‚</p>

<p>$$
x_{\text{robust}} = \frac{x - \text{median}}{\text{IQR}}
$$</p>

<pre><code class="language-python">from sklearn.preprocessing import RobustScaler

# å¤–ã‚Œå€¤ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿
data_with_outliers = data.copy()
data_with_outliers.loc[0:5, 'income'] = [5000, 5500, 6000, 100, 50, 10000]

# RobustScalerã®é©ç”¨
robust_scaler = RobustScaler()
data_robust = pd.DataFrame(
    robust_scaler.fit_transform(data_with_outliers),
    columns=data.columns
)

# æ¯”è¼ƒ: StandardScaler vs RobustScaler
standard_scaler = StandardScaler()
data_standard = pd.DataFrame(
    standard_scaler.fit_transform(data_with_outliers),
    columns=data.columns
)

print("=== å¤–ã‚Œå€¤ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã§ã®æ¯”è¼ƒ ===")
print("\nStandardScaler:")
print(data_standard['income'].describe())
print("\nRobustScaler:")
print(data_robust['income'].describe())

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

axes[0].boxplot(data_with_outliers['income'])
axes[0].set_ylabel('income')
axes[0].set_title('å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆå¤–ã‚Œå€¤ã‚ã‚Šï¼‰', fontsize=12)
axes[0].grid(True, alpha=0.3)

axes[1].boxplot(data_standard['income'])
axes[1].set_ylabel('income (scaled)')
axes[1].set_title('StandardScaler', fontsize=12)
axes[1].grid(True, alpha=0.3)

axes[2].boxplot(data_robust['income'])
axes[2].set_ylabel('income (scaled)')
axes[2].set_title('RobustScalerï¼ˆå¤–ã‚Œå€¤ã«é ‘å¥ï¼‰', fontsize=12)
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®ä½¿ã„åˆ†ã‘ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³</h3>

<table>
<thead>
<tr>
<th>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼</th>
<th>ä½¿ç”¨å ´é¢</th>
<th>é•·æ‰€</th>
<th>çŸ­æ‰€</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>StandardScaler</strong></td>
<td>æ­£è¦åˆ†å¸ƒã€å¤–ã‚Œå€¤ãªã—</td>
<td>å¤šãã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æ¨™æº–</td>
<td>å¤–ã‚Œå€¤ã«æ•æ„Ÿ</td>
</tr>
<tr>
<td><strong>MinMaxScaler</strong></td>
<td>ç¯„å›²ãŒé‡è¦ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ</td>
<td>è§£é‡ˆã—ã‚„ã™ã„[0,1]</td>
<td>å¤–ã‚Œå€¤ã®å½±éŸ¿å¤§</td>
</tr>
<tr>
<td><strong>RobustScaler</strong></td>
<td>å¤–ã‚Œå€¤ã‚ã‚Š</td>
<td>å¤–ã‚Œå€¤ã«é ‘å¥</td>
<td>ç¯„å›²ãŒä¸å®š</td>
</tr>
</tbody>
</table>

<h3>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã”ã¨ã®æ¨å¥¨</h3>

<table>
<thead>
<tr>
<th>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </th>
<th>ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¿…è¦?</th>
<th>æ¨å¥¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼</th>
</tr>
</thead>
<tbody>
<tr>
<td>ç·šå½¢å›å¸°</td>
<td>æ¨å¥¨</td>
<td>StandardScaler</td>
</tr>
<tr>
<td>ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°</td>
<td>å¿…é ˆ</td>
<td>StandardScaler</td>
</tr>
<tr>
<td>SVM</td>
<td>å¿…é ˆ</td>
<td>StandardScaler</td>
</tr>
<tr>
<td>KNN</td>
<td>å¿…é ˆ</td>
<td>StandardScaler, MinMaxScaler</td>
</tr>
<tr>
<td>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</td>
<td>å¿…é ˆ</td>
<td>MinMaxScaler, StandardScaler</td>
</tr>
<tr>
<td>æ±ºå®šæœ¨</td>
<td>ä¸è¦</td>
<td>-</td>
</tr>
<tr>
<td>ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ</td>
<td>ä¸è¦</td>
<td>-</td>
</tr>
<tr>
<td>XGBoost</td>
<td>ä¸è¦</td>
<td>-</td>
</tr>
</tbody>
</table>

<hr>

<h2>1.5 å®Ÿè·µä¾‹ï¼šå®Œå…¨ãªå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h2>

<h3>ãƒ‡ãƒ¼ã‚¿æº–å‚™</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# å®Ÿä¸–ç•Œã‚’æ¨¡ã—ãŸãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
n = 1000

# ç‰¹å¾´é‡ã®ç”Ÿæˆ
df = pd.DataFrame({
    'age': np.random.randint(18, 80, n),
    'income': np.random.normal(500, 200, n),
    'credit_score': np.random.uniform(300, 850, n),
    'loan_amount': np.random.uniform(1000, 50000, n),
    'employment_years': np.random.randint(0, 40, n)
})

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼ˆãƒ­ãƒ¼ãƒ³æ‰¿èªï¼‰
df['approved'] = (
    (df['credit_score'] > 600) &
    (df['income'] > 400) &
    (df['age'] > 25)
).astype(int)

# æ„å›³çš„ã«ãƒ‡ãƒ¼ã‚¿å“è³ªã®å•é¡Œã‚’è¿½åŠ 
# 1. æ¬ æå€¤
missing_idx = np.random.choice(n, size=100, replace=False)
df.loc[missing_idx[:50], 'income'] = np.nan
df.loc[missing_idx[50:], 'credit_score'] = np.nan

# 2. å¤–ã‚Œå€¤
outlier_idx = np.random.choice(n, size=20, replace=False)
df.loc[outlier_idx, 'loan_amount'] = df.loc[outlier_idx, 'loan_amount'] * 10

print("=== ãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦ ===")
print(df.head(10))
print(f"\nå½¢çŠ¶: {df.shape}")
print(f"\næ¬ æå€¤:")
print(df.isnull().sum())
print(f"\nåŸºæœ¬çµ±è¨ˆ:")
print(df.describe())
</code></pre>

<h3>å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰</h3>

<pre><code class="language-python">from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, RobustScaler

# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
X = df.drop('approved', axis=1)
y = df['approved']

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®šç¾©
# æ•°å€¤ç‰¹å¾´é‡ã‚’2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†ã‘ã‚‹
sensitive_features = ['loan_amount']  # å¤–ã‚Œå€¤ã«æ•æ„Ÿ
regular_features = ['age', 'income', 'credit_score', 'employment_years']

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰
preprocessor = ColumnTransformer(
    transformers=[
        # é€šå¸¸ã®ç‰¹å¾´é‡: æ¬ æå€¤è£œå®Œ â†’ æ¨™æº–åŒ–
        ('regular', Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ]), regular_features),

        # å¤–ã‚Œå€¤ã«æ•æ„Ÿãªç‰¹å¾´é‡: æ¬ æå€¤è£œå®Œ â†’ ãƒ­ãƒã‚¹ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        ('sensitive', Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', RobustScaler())
        ]), sensitive_features)
    ]
)

# å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆå‰å‡¦ç† + ãƒ¢ãƒ‡ãƒ«ï¼‰
full_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])

print("=== ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹é€  ===")
print(full_pipeline)
</code></pre>

<h3>ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨è©•ä¾¡</h3>

<pre><code class="language-python"># ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ
full_pipeline.fit(X_train, y_train)

# äºˆæ¸¬
y_pred = full_pipeline.predict(X_test)

# è©•ä¾¡
accuracy = accuracy_score(y_test, y_pred)

print("\n=== ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ ===")
print(f"ç²¾åº¦: {accuracy:.3f}")
print(f"\nè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_test, y_pred,
                           target_names=['Rejected', 'Approved']))

# å‰å‡¦ç†ãªã—ã¨ã®æ¯”è¼ƒ
from sklearn.ensemble import RandomForestClassifier

# å‰å‡¦ç†ãªã—ï¼ˆæ¬ æå€¤ã‚’å˜ç´”ã«å‰Šé™¤ï¼‰
X_train_raw = X_train.dropna()
y_train_raw = y_train[X_train.dropna().index]
X_test_raw = X_test.fillna(X_test.median())

model_raw = RandomForestClassifier(n_estimators=100, random_state=42)
model_raw.fit(X_train_raw, y_train_raw)
y_pred_raw = model_raw.predict(X_test_raw)
accuracy_raw = accuracy_score(y_test, y_pred_raw)

print(f"\n=== ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ vs å‰å‡¦ç†ãªã— ===")
print(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚ã‚Š: {accuracy:.3f}")
print(f"å‰å‡¦ç†ãªã—: {accuracy_raw:.3f}")
print(f"æ”¹å–„: {(accuracy - accuracy_raw) * 100:.1f}%")
print(f"\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º:")
print(f"  ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: {len(X_train)}è¡Œ")
print(f"  å‰å‡¦ç†ãªã—: {len(X_train_raw)}è¡Œï¼ˆ{len(X_train) - len(X_train_raw)}è¡Œå‰Šé™¤ï¼‰")
</code></pre>

<h3>å‰å‡¦ç†ã®è©³ç´°åˆ†æ</h3>

<pre><code class="language-python"># å‰å‡¦ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®å–å¾—
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# ç‰¹å¾´é‡åã®å–å¾—
feature_names = regular_features + sensitive_features

print("\n=== å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ ===")
print(f"å½¢çŠ¶: {X_train_processed.shape}")
print(f"\nå‰å‡¦ç†å¾Œã®çµ±è¨ˆï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰:")
df_processed = pd.DataFrame(X_train_processed, columns=feature_names)
print(df_processed.describe())

# å¯è¦–åŒ–ï¼šå‰å‡¦ç†ã®åŠ¹æœ
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
features_to_plot = ['age', 'income', 'loan_amount']

for i, feature in enumerate(features_to_plot):
    # å‰å‡¦ç†å‰
    axes[0, i].hist(X_train[feature].dropna(), bins=30,
                    alpha=0.7, edgecolor='black')
    axes[0, i].set_xlabel(feature)
    axes[0, i].set_ylabel('é »åº¦')
    axes[0, i].set_title(f'{feature} (å‰å‡¦ç†å‰)', fontsize=12)
    axes[0, i].grid(True, alpha=0.3)

    # å‰å‡¦ç†å¾Œ
    feature_idx = feature_names.index(feature)
    axes[1, i].hist(X_train_processed[:, feature_idx], bins=30,
                    alpha=0.7, edgecolor='black', color='orange')
    axes[1, i].set_xlabel(feature)
    axes[1, i].set_ylabel('é »åº¦')
    axes[1, i].set_title(f'{feature} (å‰å‡¦ç†å¾Œ)', fontsize=12)
    axes[1, i].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä¿å­˜ã¨å†åˆ©ç”¨</h3>

<pre><code class="language-python">import joblib

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä¿å­˜
joblib.dump(full_pipeline, 'loan_approval_pipeline.pkl')
print("ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸ: loan_approval_pipeline.pkl")

# èª­ã¿è¾¼ã¿ã¨ä½¿ç”¨
loaded_pipeline = joblib.load('loan_approval_pipeline.pkl')

# æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬
new_data = pd.DataFrame({
    'age': [35, 22, 50],
    'income': [700, 300, np.nan],  # æ¬ æå€¤ã‚’å«ã‚€
    'credit_score': [750, 550, 800],
    'loan_amount': [25000, 5000, 100000],  # å¤–ã‚Œå€¤ã‚’å«ã‚€
    'employment_years': [10, 1, 25]
})

predictions = loaded_pipeline.predict(new_data)
probabilities = loaded_pipeline.predict_proba(new_data)

print("\n=== æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬ ===")
for i, (pred, prob) in enumerate(zip(predictions, probabilities)):
    print(f"\nã‚µãƒ³ãƒ—ãƒ« {i+1}:")
    print(f"  äºˆæ¸¬: {'æ‰¿èª' if pred == 1 else 'å´ä¸‹'}")
    print(f"  ç¢ºç‡: å´ä¸‹={prob[0]:.2%}, æ‰¿èª={prob[1]:.2%}")
</code></pre>

<hr>

<h2>1.6 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®é‡è¦æ€§</strong></p>
<ul>
<li>ãƒ‡ãƒ¼ã‚¿å“è³ªãŒãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’æ±ºå®šã™ã‚‹</li>
<li>å‰å‡¦ç†ã«ã‚ˆã‚Šç²¾åº¦ã¨ãƒ­ãƒã‚¹ãƒˆæ€§ãŒå‘ä¸Š</li>
</ul></li>

<li><p><strong>æ¬ æå€¤å‡¦ç†</strong></p>
<ul>
<li>MCARã€MARã€MNARã®3ã‚¿ã‚¤ãƒ—</li>
<li>å‰Šé™¤æ³•ã€å˜ç´”è£œå®Œã€KNNè£œå®Œã€å¤šé‡ä»£å…¥æ³•</li>
<li>çŠ¶æ³ã«å¿œã˜ãŸé©åˆ‡ãªæ‰‹æ³•ã®é¸æŠ</li>
</ul></li>

<li><p><strong>å¤–ã‚Œå€¤å‡¦ç†</strong></p>
<ul>
<li>IQRæ³•ã€Z-scoreã€Isolation Forestã«ã‚ˆã‚‹æ¤œå‡º</li>
<li>å‰Šé™¤ã€å¤‰æ›ã€ã‚­ãƒ£ãƒƒãƒ—å‡¦ç†ã§ã®å¯¾å‡¦</li>
<li>ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã¨ã®çµ„ã¿åˆã‚ã›</li>
</ul></li>

<li><p><strong>ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¨æ­£è¦åŒ–</strong></p>
<ul>
<li>StandardScaler: å¹³å‡0ã€æ¨™æº–åå·®1</li>
<li>MinMaxScaler: æŒ‡å®šç¯„å›²ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°</li>
<li>RobustScaler: å¤–ã‚Œå€¤ã«é ‘å¥</li>
<li>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã”ã¨ã®ä½¿ã„åˆ†ã‘</li>
</ul></li>

<li><p><strong>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰</strong></p>
<ul>
<li>å†ç¾æ€§ã®ã‚ã‚‹å‰å‡¦ç†ãƒ•ãƒ­ãƒ¼</li>
<li>è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆã§ã®ä¸€è²«æ€§</li>
<li>æœ¬ç•ªç’°å¢ƒã¸ã®å®¹æ˜“ãªãƒ‡ãƒ—ãƒ­ã‚¤</li>
</ul></li>
</ol>

<h3>å‰å‡¦ç†ã®åŸå‰‡</h3>

<table>
<thead>
<tr>
<th>åŸå‰‡</th>
<th>èª¬æ˜</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ‡ãƒ¼ã‚¿ç†è§£å„ªå…ˆ</strong></td>
<td>å¯è¦–åŒ–ã¨çµ±è¨ˆã§å•é¡Œã‚’æŠŠæ¡ã—ã¦ã‹ã‚‰å‡¦ç†</td>
</tr>
<tr>
<td><strong>ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜æ´»ç”¨</strong></td>
<td>æ¥­å‹™çŸ¥è­˜ã‚’å‰å‡¦ç†ã®åˆ¤æ–­ã«åæ˜ </td>
</tr>
<tr>
<td><strong>ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢</strong></td>
<td>è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§fitã—ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§transform</td>
</tr>
<tr>
<td><strong>å†ç¾æ€§ç¢ºä¿</strong></td>
<td>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å‡¦ç†ã‚’æ¨™æº–åŒ–</td>
</tr>
<tr>
<td><strong>æ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</strong></td>
<td>ä¸€åº¦ã«å¤šãã‚’å¤‰æ›´ã›ãšã€åŠ¹æœã‚’ç¢ºèª</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ç« ã¸</h3>

<p>ç¬¬2ç« ã§ã¯ã€<strong>ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°</strong>ã‚’å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>One-Hot Encoding</li>
<li>Label Encoding</li>
<li>Target Encoding</li>
<li>Frequency Encoding</li>
<li>é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã®æ‰±ã„</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>
<p>æ¬ æå€¤ã®3ã¤ã®ã‚¿ã‚¤ãƒ—ï¼ˆMCARã€MARã€MNARï¼‰ã‚’ãã‚Œãã‚Œèª¬æ˜ã—ã€å…·ä½“ä¾‹ã‚’æŒ™ã’ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<ol>
<li><p><strong>MCARï¼ˆMissing Completely At Randomï¼‰</strong></p>
<ul>
<li>èª¬æ˜: æ¬ æãŒå®Œå…¨ã«ãƒ©ãƒ³ãƒ€ãƒ ã§ã€ä»–ã®å¤‰æ•°ã¨ç„¡é–¢ä¿‚</li>
<li>ä¾‹: ã‚»ãƒ³ã‚µãƒ¼ã®æ•…éšœã«ã‚ˆã‚Šä¸€éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ãŒè¨˜éŒ²ã•ã‚Œãªã„</li>
</ul></li>

<li><p><strong>MARï¼ˆMissing At Randomï¼‰</strong></p>
<ul>
<li>èª¬æ˜: æ¬ æãŒè¦³æ¸¬ã•ã‚Œã¦ã„ã‚‹ä»–ã®å¤‰æ•°ã«ä¾å­˜</li>
<li>ä¾‹: é«˜é½¢è€…ã»ã©å¥åº·ãƒ‡ãƒ¼ã‚¿ã®è¨˜å…¥ç‡ãŒä½ã„ï¼ˆå¹´é½¢ã¯è¦³æ¸¬æ¸ˆã¿ï¼‰</li>
</ul></li>

<li><p><strong>MNARï¼ˆMissing Not At Randomï¼‰</strong></p>
<ul>
<li>èª¬æ˜: æ¬ æãŒæ¬ æå€¤ãã®ã‚‚ã®ã«ä¾å­˜</li>
<li>ä¾‹: ä½æ‰€å¾—è€…ãŒå¹´åã®è¨˜å…¥ã‚’é¿ã‘ã‚‹ï¼ˆå¹´åãã®ã‚‚ã®ãŒæ¬ æã®åŸå› ï¼‰</li>
</ul></li>
</ol>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã€IQRæ³•ã‚’ç”¨ã„ã¦å¤–ã‚Œå€¤ã‚’æ¤œå‡ºã—ã€ãã®æ•°ã‚’å ±å‘Šã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">data = np.array([12, 15, 14, 10, 8, 12, 15, 14, 100, 13, 12, 14, 15, -5, 11])
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import numpy as np

data = np.array([12, 15, 14, 10, 8, 12, 15, 14, 100, 13, 12, 14, 15, -5, 11])

# IQRæ³•
Q1 = np.percentile(data, 25)
Q3 = np.percentile(data, 75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = (data < lower_bound) | (data > upper_bound)

print("=== IQRæ³•ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º ===")
print(f"Q1: {Q1}")
print(f"Q3: {Q3}")
print(f"IQR: {IQR}")
print(f"ä¸‹é™: {lower_bound}")
print(f"ä¸Šé™: {upper_bound}")
print(f"\nå¤–ã‚Œå€¤ã®æ•°: {outliers.sum()}")
print(f"å¤–ã‚Œå€¤: {data[outliers]}")
print(f"æ­£å¸¸å€¤: {data[~outliers]}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== IQRæ³•ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º ===
Q1: 11.5
Q3: 14.5
IQR: 3.0
ä¸‹é™: 7.0
ä¸Šé™: 19.0

å¤–ã‚Œå€¤ã®æ•°: 2
å¤–ã‚Œå€¤: [100  -5]
æ­£å¸¸å€¤: [12 15 14 10  8 12 15 14 13 12 14 15 11]
</code></pre>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>StandardScalerã¨MinMaxScalerã®é•ã„ã‚’èª¬æ˜ã—ã€ãã‚Œãã‚Œã‚’ã©ã®ã‚ˆã†ãªå ´é¢ã§ä½¿ã†ã¹ãã‹è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>StandardScalerï¼ˆæ¨™æº–åŒ–ï¼‰</strong>ï¼š</p>
<ul>
<li>å¤‰æ›å¼: $z = \frac{x - \mu}{\sigma}$</li>
<li>çµæœ: å¹³å‡0ã€æ¨™æº–åå·®1</li>
<li>ç‰¹å¾´: ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒå½¢çŠ¶ã‚’ä¿æŒã€ç¯„å›²ã¯ä¸å®š</li>
</ul>

<p><strong>MinMaxScalerï¼ˆæ­£è¦åŒ–ï¼‰</strong>ï¼š</p>
<ul>
<li>å¤‰æ›å¼: $x_{\text{norm}} = \frac{x - x_{\min}}{x_{\max} - x_{\min}}$</li>
<li>çµæœ: æŒ‡å®šç¯„å›²ï¼ˆé€šå¸¸[0, 1]ï¼‰</li>
<li>ç‰¹å¾´: ç¯„å›²ãŒå›ºå®šã€å¤–ã‚Œå€¤ã®å½±éŸ¿å¤§</li>
</ul>

<p><strong>ä½¿ã„åˆ†ã‘</strong>ï¼š</p>

<table>
<thead>
<tr>
<th>å ´é¢</th>
<th>æ¨å¥¨</th>
</tr>
</thead>
<tbody>
<tr>
<td>æ­£è¦åˆ†å¸ƒã«è¿‘ã„ãƒ‡ãƒ¼ã‚¿</td>
<td>StandardScaler</td>
</tr>
<tr>
<td>ç¯„å›²ãŒé‡è¦ï¼ˆä¾‹: [0, 1]ãŒå¿…é ˆï¼‰</td>
<td>MinMaxScaler</td>
</tr>
<tr>
<td>å¤–ã‚Œå€¤ãŒå°‘ãªã„</td>
<td>ã©ã¡ã‚‰ã§ã‚‚å¯</td>
</tr>
<tr>
<td>å¤–ã‚Œå€¤ãŒå¤šã„</td>
<td>RobustScalerï¼ˆã¾ãŸã¯æ¨™æº–åŒ–ï¼‰</td>
</tr>
<tr>
<td>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</td>
<td>MinMaxScalerï¼ˆæ´»æ€§åŒ–é–¢æ•°ã®ç¯„å›²ã«åˆã‚ã›ã‚‹ï¼‰</td>
</tr>
<tr>
<td>ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã€SVM</td>
<td>StandardScaler</td>
</tr>
</tbody>
</table>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã€æ¬ æå€¤å‡¦ç†ã¨å¤–ã‚Œå€¤å‡¦ç†ã‚’å«ã‚€å®Œå…¨ãªå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import numpy as np
import pandas as pd

np.random.seed(42)
data = pd.DataFrame({
    'feature1': np.random.normal(50, 10, 100),
    'feature2': np.random.normal(100, 20, 100),
    'feature3': np.random.uniform(0, 1, 100)
})

# æ¬ æå€¤ã‚’è¿½åŠ 
data.loc[0:10, 'feature1'] = np.nan
data.loc[20:25, 'feature2'] = np.nan

# å¤–ã‚Œå€¤ã‚’è¿½åŠ 
data.loc[50, 'feature1'] = 200
data.loc[60, 'feature2'] = 500
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.impute import KNNImputer
from sklearn.compose import ColumnTransformer

np.random.seed(42)
data = pd.DataFrame({
    'feature1': np.random.normal(50, 10, 100),
    'feature2': np.random.normal(100, 20, 100),
    'feature3': np.random.uniform(0, 1, 100)
})

# æ¬ æå€¤ã‚’è¿½åŠ 
data.loc[0:10, 'feature1'] = np.nan
data.loc[20:25, 'feature2'] = np.nan

# å¤–ã‚Œå€¤ã‚’è¿½åŠ 
data.loc[50, 'feature1'] = 200
data.loc[60, 'feature2'] = 500

print("=== å‰å‡¦ç†å‰ã®ãƒ‡ãƒ¼ã‚¿ ===")
print(data.describe())
print(f"\næ¬ æå€¤:\n{data.isnull().sum()}")

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰
# feature1, feature2: æ¬ æå€¤ã‚ã‚Šã€å¤–ã‚Œå€¤ã‚ã‚Š â†’ KNNè£œå®Œ + RobustScaler
# feature3: å•é¡Œãªã— â†’ StandardScaler

preprocessor = ColumnTransformer(
    transformers=[
        ('features_with_issues', Pipeline([
            ('imputer', KNNImputer(n_neighbors=5)),
            ('scaler', RobustScaler())
        ]), ['feature1', 'feature2']),

        ('clean_features', Pipeline([
            ('scaler', StandardScaler())
        ]), ['feature3'])
    ]
)

# å‰å‡¦ç†ã®å®Ÿè¡Œ
data_processed = preprocessor.fit_transform(data)

print("\n=== å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ ===")
df_processed = pd.DataFrame(
    data_processed,
    columns=['feature1', 'feature2', 'feature3']
)
print(df_processed.describe())
print(f"\næ¬ æå€¤: {df_processed.isnull().sum().sum()}")

# å¯è¦–åŒ–
import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 3, figsize=(15, 10))

for i, col in enumerate(['feature1', 'feature2', 'feature3']):
    # å‰å‡¦ç†å‰
    axes[0, i].boxplot(data[col].dropna())
    axes[0, i].set_ylabel(col)
    axes[0, i].set_title(f'{col} (å‰å‡¦ç†å‰)', fontsize=12)
    axes[0, i].grid(True, alpha=0.3)

    # å‰å‡¦ç†å¾Œ
    axes[1, i].boxplot(df_processed[col])
    axes[1, i].set_ylabel(col)
    axes[1, i].set_title(f'{col} (å‰å‡¦ç†å¾Œ)', fontsize=12)
    axes[1, i].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nâœ“ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰å®Œäº†")
print("âœ“ æ¬ æå€¤è£œå®Œå®Œäº†ï¼ˆKNN, k=5ï¼‰")
print("âœ“ å¤–ã‚Œå€¤ã«é ‘å¥ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Œäº†ï¼ˆRobustScalerï¼‰")
</code></pre>

</details>

<h3>å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§åˆ¥ã€…ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’è¡Œã†ã¨ã€ãªãœãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãŒç™ºç”Ÿã™ã‚‹ã®ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚æ­£ã—ã„æ–¹æ³•ã‚‚ç¤ºã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãŒç™ºç”Ÿã™ã‚‹ç†ç”±</strong>ï¼š</p>

<p>ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§åˆ¥é€”ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’è¡Œã†ã¨ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆæƒ…å ±ï¼ˆå¹³å‡ã€æ¨™æº–åå·®ãªã©ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚ã“ã‚Œã¯ä»¥ä¸‹ã®å•é¡Œã‚’å¼•ãèµ·ã“ã—ã¾ã™ï¼š</p>

<ol>
<li><strong>æœªæ¥ã®æƒ…å ±ã‚’ä½¿ç”¨</strong>: æœ¬ç•ªç’°å¢ƒã§ã¯æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆæƒ…å ±ã¯äº‹å‰ã«ä¸æ˜</li>
<li><strong>è©•ä¾¡ã®æ­ªã¿</strong>: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ã‚’ä½¿ã†ãŸã‚ã€æ€§èƒ½ãŒéå¤§è©•ä¾¡ã•ã‚Œã‚‹</li>
<li><strong>å†ç¾æ€§ã®æ¬ å¦‚</strong>: å®Ÿéš›ã®ãƒ‡ãƒ—ãƒ­ã‚¤æ™‚ã«åŒã˜å¤‰æ›ãŒã§ããªã„</li>
</ol>

<p><strong>èª¤ã£ãŸæ–¹æ³•ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚ã‚Šï¼‰</strong>ï¼š</p>
<pre><code class="language-python"># âŒ é–“é•ã„
scaler_train = StandardScaler()
X_train_scaled = scaler_train.fit_transform(X_train)

scaler_test = StandardScaler()
X_test_scaled = scaler_test.fit_transform(X_test)  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§fit
</code></pre>

<p><strong>æ­£ã—ã„æ–¹æ³•</strong>ï¼š</p>
<pre><code class="language-python"># âœ… æ­£ã—ã„
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§fit
X_test_scaled = scaler.transform(X_test)  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆã§å¤‰æ›
</code></pre>

<p><strong>å®Ÿä¾‹ã§ç¢ºèª</strong>ï¼š</p>
<pre><code class="language-python">import numpy as np
from sklearn.preprocessing import StandardScaler

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
X_train = np.array([[1], [2], [3], [4], [5]])
X_test = np.array([[100], [200], [300]])

# èª¤ã£ãŸæ–¹æ³•
scaler_train = StandardScaler()
scaler_test = StandardScaler()
X_train_wrong = scaler_train.fit_transform(X_train)
X_test_wrong = scaler_test.fit_transform(X_test)

# æ­£ã—ã„æ–¹æ³•
scaler = StandardScaler()
X_train_correct = scaler.fit_transform(X_train)
X_test_correct = scaler.transform(X_test)

print("=== èª¤ã£ãŸæ–¹æ³•ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚ã‚Šï¼‰===")
print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡: {X_train_wrong.mean():.3f}")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å¹³å‡: {X_test_wrong.mean():.3f}")
print("â†’ ä¸¡æ–¹ã¨ã‚‚0ã«è¿‘ã„ï¼ˆç‹¬ç«‹ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰")

print("\n=== æ­£ã—ã„æ–¹æ³• ===")
print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡: {X_train_correct.mean():.3f}")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å¹³å‡: {X_test_correct.mean():.3f}")
print("â†’ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆã§å¤‰æ›")
print(f"\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å€¤: {X_test_correct.flatten()}")
print("â†’ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã¨æ¯”è¼ƒã—ã¦æ¥µç«¯ã«å¤§ãã„å€¤ï¼ˆæ­£ã—ãæ¤œå‡ºï¼‰")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== èª¤ã£ãŸæ–¹æ³•ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚ã‚Šï¼‰===
è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡: 0.000
ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å¹³å‡: 0.000
â†’ ä¸¡æ–¹ã¨ã‚‚0ã«è¿‘ã„ï¼ˆç‹¬ç«‹ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰

=== æ­£ã—ã„æ–¹æ³• ===
è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡: 0.000
ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å¹³å‡: 63.246
â†’ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆã§å¤‰æ›

ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å€¤: [63.25 126.49 189.74]
â†’ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã¨æ¯”è¼ƒã—ã¦æ¥µç«¯ã«å¤§ãã„å€¤ï¼ˆæ­£ã—ãæ¤œå‡ºï¼‰
</code></pre>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>GÃ©ron, A. (2019). <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em> (2nd ed.). O'Reilly Media.</li>
<li>Kuhn, M., & Johnson, K. (2019). <em>Feature Engineering and Selection: A Practical Approach for Predictive Models</em>. CRC Press.</li>
<li>Zheng, A., & Casari, A. (2018). <em>Feature Engineering for Machine Learning</em>. O'Reilly Media.</li>
<li>Little, R. J., & Rubin, D. B. (2019). <em>Statistical Analysis with Missing Data</em> (3rd ed.). Wiley.</li>
</ol>

<div class="navigation">
    <a href="index.html" class="nav-button">â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a>
    <a href="chapter2-categorical-encoding.html" class="nav-button">æ¬¡ã®ç« : ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>