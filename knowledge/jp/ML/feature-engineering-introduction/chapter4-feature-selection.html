<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
<meta content="ç¬¬4ç« ï¼šç‰¹å¾´é‡é¸æŠ - AI Terakoya" name="description"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬4ç« ï¼šç‰¹å¾´é‡é¸æŠ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: var(--font-body); line-height: 1.7; color: var(--color-text); background-color: var(--color-bg); font-size: 16px; }
        header { background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; padding: var(--spacing-xl) var(--spacing-md); margin-bottom: var(--spacing-xl); box-shadow: var(--box-shadow); }
        .header-content { max-width: 900px; margin: 0 auto; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: var(--spacing-sm); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; opacity: 0.95; font-weight: 400; margin-bottom: var(--spacing-md); }
        .meta { display: flex; flex-wrap: wrap; gap: var(--spacing-md); font-size: 0.9rem; opacity: 0.9; }
        .meta-item { display: flex; align-items: center; gap: 0.3rem; }
        .container { max-width: 900px; margin: 0 auto; padding: 0 var(--spacing-md) var(--spacing-xl); }
        h2 { font-size: 1.75rem; color: var(--color-primary); margin-top: var(--spacing-xl); margin-bottom: var(--spacing-md); padding-bottom: var(--spacing-xs); border-bottom: 3px solid var(--color-accent); }
        h3 { font-size: 1.4rem; color: var(--color-primary); margin-top: var(--spacing-lg); margin-bottom: var(--spacing-sm); }
        h4 { font-size: 1.1rem; color: var(--color-primary-dark); margin-top: var(--spacing-md); margin-bottom: var(--spacing-sm); }
        p { margin-bottom: var(--spacing-md); color: var(--color-text); }
        a { color: var(--color-link); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--color-link-hover); text-decoration: underline; }
        ul, ol { margin-left: var(--spacing-lg); margin-bottom: var(--spacing-md); }
        li { margin-bottom: var(--spacing-xs); color: var(--color-text); }
        pre { background-color: var(--color-code-bg); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); overflow-x: auto; margin-bottom: var(--spacing-md); font-family: var(--font-mono); font-size: 0.9rem; line-height: 1.5; }
        code { font-family: var(--font-mono); font-size: 0.9em; background-color: var(--color-code-bg); padding: 0.2em 0.4em; border-radius: 3px; }
        pre code { background-color: transparent; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: var(--spacing-md); font-size: 0.95rem; }
        th, td { border: 1px solid var(--color-border); padding: var(--spacing-sm); text-align: left; }
        th { background-color: var(--color-bg-alt); font-weight: 600; color: var(--color-primary); }
        blockquote { border-left: 4px solid var(--color-accent); padding-left: var(--spacing-md); margin: var(--spacing-md) 0; color: var(--color-text-light); font-style: italic; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        .mermaid { text-align: center; margin: var(--spacing-lg) 0; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        details { background-color: var(--color-bg-alt); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); margin-bottom: var(--spacing-md); }
        summary { cursor: pointer; font-weight: 600; color: var(--color-primary); user-select: none; padding: var(--spacing-xs); margin: calc(-1 * var(--spacing-md)); padding: var(--spacing-md); border-radius: var(--border-radius); }
        summary:hover { background-color: rgba(123, 44, 191, 0.1); }
        details[open] summary { margin-bottom: var(--spacing-md); border-bottom: 1px solid var(--color-border); }
        .navigation { display: flex; justify-content: space-between; gap: var(--spacing-md); margin: var(--spacing-xl) 0; padding-top: var(--spacing-lg); border-top: 2px solid var(--color-border); }
        .nav-button { flex: 1; padding: var(--spacing-md); background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; border-radius: var(--border-radius); text-align: center; font-weight: 600; transition: transform 0.2s, box-shadow 0.2s; box-shadow: var(--box-shadow); }
        .nav-button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); text-decoration: none; }
        footer { margin-top: var(--spacing-xl); padding: var(--spacing-lg) var(--spacing-md); background-color: var(--color-bg-alt); border-top: 1px solid var(--color-border); text-align: center; font-size: 0.9rem; color: var(--color-text-light); }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }

        .project-box { background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 50%); border-radius: var(--border-radius); padding: var(--spacing-lg); margin: var(--spacing-lg) 0; box-shadow: var(--box-shadow); }
        @media (max-width: 768px) { h1 { font-size: 1.5rem; } h2 { font-size: 1.4rem; } h3 { font-size: 1.2rem; } .meta { font-size: 0.85rem; } .navigation { flex-direction: column; } table { font-size: 0.85rem; } th, td { padding: var(--spacing-xs); } }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/feature-engineering-introduction/chapter4-feature-selection.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/feature-engineering-introduction/index.html">Feature Engineering</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 4</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬4ç« ï¼šç‰¹å¾´é‡é¸æŠ</h1>
            <p class="subtitle">æ¬¡å…ƒå‰Šæ¸›ã¨äºˆæ¸¬æ€§èƒ½å‘ä¸Šã®ãŸã‚ã®æœ€é©ç‰¹å¾´é‡ã®é¸æŠæŠ€è¡“</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 28åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 12å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… ç‰¹å¾´é‡é¸æŠã®é‡è¦æ€§ã¨ã€Œæ¬¡å…ƒã®å‘ªã„ã€ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… Filter Methodsï¼ˆç›¸é–¢åˆ†æã€ã‚«ã‚¤äºŒä¹—æ¤œå®šã€ç›¸äº’æƒ…å ±é‡ï¼‰ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Wrapper Methodsï¼ˆRFEã€Sequential Feature Selectorï¼‰ã‚’ä½¿ã„ã“ãªã›ã‚‹</li>
<li>âœ… Embedded Methodsï¼ˆLassoã€Tree-based importanceï¼‰ã‚’æ´»ç”¨ã§ãã‚‹</li>
<li>âœ… å„æ‰‹æ³•ã®ç‰¹æ€§ã‚’ç†è§£ã—ã€æœ€é©ãªæ‰‹æ³•ã‚’é¸æŠã§ãã‚‹</li>
<li>âœ… å®Œå…¨ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹ç¯‰ã§ãã‚‹</li>
</ul>

<hr>

<h2>4.1 ç‰¹å¾´é‡é¸æŠã®é‡è¦æ€§</h2>

<h3>ãªãœç‰¹å¾´é‡é¸æŠãŒå¿…è¦ã‹ï¼Ÿ</h3>

<p>æ©Ÿæ¢°å­¦ç¿’ã§ã¯ã€Œå¤šã‘ã‚Œã°å¤šã„ã»ã©è‰¯ã„ã€ã¨ã¯é™ã‚Šã¾ã›ã‚“ã€‚ä¸è¦ãªç‰¹å¾´é‡ã¯ä»¥ä¸‹ã®å•é¡Œã‚’å¼•ãèµ·ã“ã—ã¾ã™ï¼š</p>

<table>
<thead>
<tr>
<th>å•é¡Œ</th>
<th>èª¬æ˜</th>
<th>å½±éŸ¿</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ¬¡å…ƒã®å‘ªã„</strong></td>
<td>ç‰¹å¾´é‡ãŒå¢—ãˆã‚‹ã»ã©ãƒ‡ãƒ¼ã‚¿ãŒç–ã«ãªã‚‹</td>
<td>å¿…è¦ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒæŒ‡æ•°çš„ã«å¢—åŠ </td>
</tr>
<tr>
<td><strong>éå­¦ç¿’</strong></td>
<td>ãƒã‚¤ã‚ºã‚’å­¦ç¿’ã—ã¦ã—ã¾ã†</td>
<td>æ±åŒ–æ€§èƒ½ãŒä½ä¸‹</td>
</tr>
<tr>
<td><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆ</strong></td>
<td>å­¦ç¿’ãƒ»æ¨è«–ã«æ™‚é–“ãŒã‹ã‹ã‚‹</td>
<td>å®Ÿé‹ç”¨ã§å•é¡Œã«ãªã‚‹</td>
</tr>
<tr>
<td><strong>è§£é‡ˆæ€§ä½ä¸‹</strong></td>
<td>ãƒ¢ãƒ‡ãƒ«ãŒè¤‡é›‘ã«ãªã‚Šã™ãã‚‹</td>
<td>ãƒ“ã‚¸ãƒã‚¹èª¬æ˜ãŒå›°é›£</td>
</tr>
<tr>
<td><strong>å¤šé‡å…±ç·šæ€§</strong></td>
<td>ç›¸é–¢ã®é«˜ã„ç‰¹å¾´é‡ãŒä¸å®‰å®šæ€§ã‚’ç”Ÿã‚€</td>
<td>ä¿‚æ•°æ¨å®šãŒä¸æ­£ç¢ºã«</td>
</tr>
</tbody>
</table>

<h3>æ¬¡å…ƒã®å‘ªã„ï¼ˆCurse of Dimensionalityï¼‰</h3>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import NearestNeighbors

# æ¬¡å…ƒã®å‘ªã„ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
np.random.seed(42)

def calculate_sparsity(n_samples, n_dims):
    """næ¬¡å…ƒç©ºé–“ã§ã®ãƒ‡ãƒ¼ã‚¿ã®ç–å¯†åº¦ã‚’è¨ˆç®—"""
    # ãƒ©ãƒ³ãƒ€ãƒ ãªç‚¹ã‚’ç”Ÿæˆ
    X = np.random.rand(n_samples, n_dims)

    # æœ€è¿‘å‚æ¢ç´¢
    nbrs = NearestNeighbors(n_neighbors=2).fit(X)
    distances, _ = nbrs.kneighbors(X)

    # æœ€è¿‘å‚ç‚¹ã¾ã§ã®å¹³å‡è·é›¢ï¼ˆç–å¯†åº¦ã®æŒ‡æ¨™ï¼‰
    avg_distance = distances[:, 1].mean()
    return avg_distance

# æ¬¡å…ƒæ•°ã‚’å¤‰åŒ–ã•ã›ã¦ç–å¯†åº¦ã‚’æ¸¬å®š
dimensions = [1, 2, 5, 10, 20, 50, 100, 200]
n_samples = 1000

sparsity = [calculate_sparsity(n_samples, d) for d in dimensions]

# å¯è¦–åŒ–
plt.figure(figsize=(12, 5))

# å·¦: ç–å¯†åº¦ã®å¤‰åŒ–
plt.subplot(1, 2, 1)
plt.plot(dimensions, sparsity, 'o-', linewidth=2, markersize=8, color='#e74c3c')
plt.xlabel('æ¬¡å…ƒæ•°', fontsize=12)
plt.ylabel('æœ€è¿‘å‚ç‚¹ã¾ã§ã®å¹³å‡è·é›¢', fontsize=12)
plt.title('æ¬¡å…ƒã®å‘ªã„ï¼šãƒ‡ãƒ¼ã‚¿ã®ç–å¯†åŒ–', fontsize=14)
plt.grid(alpha=0.3)

# å³: å¿…è¦ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆç†è«–å€¤ï¼‰
required_samples = [10 ** d for d in range(1, 9)]
plt.subplot(1, 2, 2)
plt.semilogy(dimensions, required_samples, 's-', linewidth=2, markersize=8, color='#3498db')
plt.xlabel('æ¬¡å…ƒæ•°', fontsize=12)
plt.ylabel('å¿…è¦ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰', fontsize=12)
plt.title('æ¬¡å…ƒå¢—åŠ ã«ä¼´ã†å¿…è¦ã‚µãƒ³ãƒ—ãƒ«æ•°', fontsize=14)
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("=== æ¬¡å…ƒã®å‘ªã„ã®å½±éŸ¿ ===")
for d, s in zip(dimensions, sparsity):
    print(f"æ¬¡å…ƒæ•°: {d:3d} â†’ æœ€è¿‘å‚è·é›¢: {s:.4f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== æ¬¡å…ƒã®å‘ªã„ã®å½±éŸ¿ ===
æ¬¡å…ƒæ•°:   1 â†’ æœ€è¿‘å‚è·é›¢: 0.0010
æ¬¡å…ƒæ•°:   2 â†’ æœ€è¿‘å‚è·é›¢: 0.0142
æ¬¡å…ƒæ•°:   5 â†’ æœ€è¿‘å‚è·é›¢: 0.0891
æ¬¡å…ƒæ•°:  10 â†’ æœ€è¿‘å‚è·é›¢: 0.1823
æ¬¡å…ƒæ•°:  20 â†’ æœ€è¿‘å‚è·é›¢: 0.3234
æ¬¡å…ƒæ•°:  50 â†’ æœ€è¿‘å‚è·é›¢: 0.5678
æ¬¡å…ƒæ•°: 100 â†’ æœ€è¿‘å‚è·é›¢: 0.7234
æ¬¡å…ƒæ•°: 200 â†’ æœ€è¿‘å‚è·é›¢: 0.8567
</code></pre>

<blockquote>
<p><strong>é‡è¦</strong>: æ¬¡å…ƒæ•°ãŒå¢—ãˆã‚‹ã¨ã€ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ç‚¹ãŒäº’ã„ã«é ããªã‚Šã€ã€Œè¿‘å‚ã€ã¨ã„ã†æ¦‚å¿µãŒæ„å‘³ã‚’å¤±ã„ã¾ã™ã€‚ã“ã‚ŒãŒã€Œæ¬¡å…ƒã®å‘ªã„ã€ã§ã™ã€‚</p>
</blockquote>

<h3>ç‰¹å¾´é‡é¸æŠã®3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</h3>

<div class="mermaid">
graph TB
    A[ç‰¹å¾´é‡é¸æŠæ‰‹æ³•] --> B[Filter Methods<br/>ãƒ•ã‚£ãƒ«ã‚¿æ³•]
    A --> C[Wrapper Methods<br/>ãƒ©ãƒƒãƒ‘ãƒ¼æ³•]
    A --> D[Embedded Methods<br/>çµ„ã¿è¾¼ã¿æ³•]

    B --> B1[çµ±è¨ˆçš„æ¤œå®š]
    B --> B2[ç›¸é–¢åˆ†æ]
    B --> B3[ç›¸äº’æƒ…å ±é‡]

    C --> C1[å‰å‘ãé¸æŠ]
    C --> C2[å¾Œå‘ãå‰Šé™¤]
    C --> C3[RFE]

    D --> D1[Lasso]
    D --> D2[Tree importance]
    D --> D3[æ­£å‰‡åŒ–]

    style A fill:#7b2cbf,color:#fff
    style B fill:#e3f2fd
    style C fill:#fff3e0
    style D fill:#e8f5e9
</div>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>ç‰¹å¾´</th>
<th>è¨ˆç®—é€Ÿåº¦</th>
<th>ç²¾åº¦</th>
<th>ä½¿ç”¨å ´é¢</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Filter</strong></td>
<td>ãƒ¢ãƒ‡ãƒ«ç‹¬ç«‹ã€çµ±è¨ˆçš„è©•ä¾¡</td>
<td>âš¡âš¡âš¡ é€Ÿã„</td>
<td>â­â­ ä¸­ç¨‹åº¦</td>
<td>äº‹å‰ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°</td>
</tr>
<tr>
<td><strong>Wrapper</strong></td>
<td>ãƒ¢ãƒ‡ãƒ«ä¾å­˜ã€æ¢ç´¢çš„</td>
<td>âš¡ é…ã„</td>
<td>â­â­â­ é«˜ã„</td>
<td>æœ€çµ‚èª¿æ•´</td>
</tr>
<tr>
<td><strong>Embedded</strong></td>
<td>å­¦ç¿’ã«çµ„ã¿è¾¼ã¿</td>
<td>âš¡âš¡ ä¸­ç¨‹åº¦</td>
<td>â­â­â­ é«˜ã„</td>
<td>å®Ÿç”¨çš„é¸æŠ</td>
</tr>
</tbody>
</table>

<hr>

<h2>4.2 Filter Methodsï¼ˆãƒ•ã‚£ãƒ«ã‚¿æ³•ï¼‰</h2>

<p>ãƒ•ã‚£ãƒ«ã‚¿æ³•ã¯ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¨ã¯ç‹¬ç«‹ã«ã€çµ±è¨ˆçš„æŒ‡æ¨™ã§ç‰¹å¾´é‡ã‚’è©•ä¾¡ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<h3>4.2.1 ç›¸é–¢ä¿‚æ•°ã«ã‚ˆã‚‹é¸æŠ</h3>

<pre><code class="language-python">import pandas as pd
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split

# ç³–å°¿ç—…ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
diabetes = load_diabetes()
X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)
y = diabetes.target

print("=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ± ===")
print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {X.shape[0]}, ç‰¹å¾´é‡æ•°: {X.shape[1]}")
print(f"\nç‰¹å¾´é‡ãƒªã‚¹ãƒˆ:\n{X.columns.tolist()}")

# ç›®çš„å¤‰æ•°ã¨ã®ç›¸é–¢è¨ˆç®—
correlation_with_target = X.corrwith(pd.Series(y, name='target')).abs().sort_values(ascending=False)

print("\n=== ç›®çš„å¤‰æ•°ã¨ã®ç›¸é–¢ ===")
print(correlation_with_target)

# ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
plt.figure(figsize=(12, 10))
correlation_matrix = X.corr()
import seaborn as sns
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',
            center=0, square=True, linewidths=1)
plt.title('ç‰¹å¾´é‡é–“ã®ç›¸é–¢è¡Œåˆ—', fontsize=16)
plt.tight_layout()
plt.show()

# ç›¸é–¢ãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡é¸æŠ
def select_by_correlation(X, y, threshold=0.1):
    """ç›¸é–¢ä¿‚æ•°ã«åŸºã¥ã„ã¦ç‰¹å¾´é‡ã‚’é¸æŠ"""
    correlations = X.corrwith(pd.Series(y, name='target')).abs()
    selected_features = correlations[correlations >= threshold].index.tolist()
    return selected_features, correlations

selected_features, correlations = select_by_correlation(X, y, threshold=0.2)

print(f"\n=== ç›¸é–¢é–¾å€¤0.2ä»¥ä¸Šã®ç‰¹å¾´é‡ ===")
print(f"é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡æ•°: {len(selected_features)}/{X.shape[1]}")
print(f"ç‰¹å¾´é‡: {selected_features}")

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
correlations.sort_values(ascending=True).plot(kind='barh', color='#3498db')
plt.axvline(x=0.2, color='r', linestyle='--', label='é–¾å€¤: 0.2')
plt.xlabel('|ç›¸é–¢ä¿‚æ•°|', fontsize=12)
plt.ylabel('ç‰¹å¾´é‡', fontsize=12)
plt.title('ç›®çš„å¤‰æ•°ã¨ã®ç›¸é–¢ä¿‚æ•°', fontsize=14)
plt.legend()
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ± ===
ã‚µãƒ³ãƒ—ãƒ«æ•°: 442, ç‰¹å¾´é‡æ•°: 10

ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ:
['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']

=== ç›®çš„å¤‰æ•°ã¨ã®ç›¸é–¢ ===
bmi    0.586450
s5     0.565883
bp     0.441484
s4     0.430453
s6     0.380109
s3     0.394789
s1     0.212022
age    0.187889
s2     0.174054
sex    0.043062

=== ç›¸é–¢é–¾å€¤0.2ä»¥ä¸Šã®ç‰¹å¾´é‡ ===
é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡æ•°: 7/10
ç‰¹å¾´é‡: ['bmi', 's5', 'bp', 's4', 's6', 's3', 's1']
</code></pre>

<h3>4.2.2 ã‚«ã‚¤äºŒä¹—æ¤œå®šï¼ˆåˆ†é¡å•é¡Œï¼‰</h3>

<pre><code class="language-python">from sklearn.datasets import load_breast_cancer
from sklearn.feature_selection import chi2, SelectKBest
from sklearn.preprocessing import MinMaxScaler

# ä¹³ãŒã‚“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
cancer = load_breast_cancer()
X_cancer = pd.DataFrame(cancer.data, columns=cancer.feature_names)
y_cancer = cancer.target

print("=== ä¹³ãŒã‚“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ===")
print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {X_cancer.shape[0]}, ç‰¹å¾´é‡æ•°: {X_cancer.shape[1]}")
print(f"ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {pd.Series(y_cancer).value_counts().to_dict()}")

# ã‚«ã‚¤äºŒä¹—æ¤œå®šï¼ˆéè² å€¤ãŒå¿…è¦ï¼‰
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X_cancer)

# ã‚«ã‚¤äºŒä¹—çµ±è¨ˆé‡ã‚’è¨ˆç®—
chi2_stats, p_values = chi2(X_scaled, y_cancer)

# çµæœã‚’DataFrameã«
chi2_results = pd.DataFrame({
    'feature': X_cancer.columns,
    'chi2_stat': chi2_stats,
    'p_value': p_values
}).sort_values('chi2_stat', ascending=False)

print("\n=== ã‚«ã‚¤äºŒä¹—æ¤œå®šçµæœï¼ˆä¸Šä½10ç‰¹å¾´é‡ï¼‰ ===")
print(chi2_results.head(10).to_string(index=False))

# SelectKBestã§ä¸Šä½kå€‹é¸æŠ
k_best = 10
selector = SelectKBest(chi2, k=k_best)
X_selected = selector.fit_transform(X_scaled, y_cancer)

selected_features = X_cancer.columns[selector.get_support()].tolist()
print(f"\n=== é¸æŠã•ã‚ŒãŸä¸Šä½{k_best}ç‰¹å¾´é‡ ===")
print(selected_features)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# ã‚«ã‚¤äºŒä¹—çµ±è¨ˆé‡
axes[0].barh(range(len(chi2_results)), chi2_results['chi2_stat'], color='#3498db')
axes[0].set_yticks(range(len(chi2_results)))
axes[0].set_yticklabels(chi2_results['feature'], fontsize=8)
axes[0].set_xlabel('Ï‡Â² çµ±è¨ˆé‡', fontsize=12)
axes[0].set_title('ã‚«ã‚¤äºŒä¹—çµ±è¨ˆé‡ï¼ˆå¤§ãã„ã»ã©é‡è¦ï¼‰', fontsize=14)
axes[0].grid(axis='x', alpha=0.3)

# på€¤ï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
axes[1].barh(range(len(chi2_results)), -np.log10(chi2_results['p_value']), color='#e74c3c')
axes[1].set_yticks(range(len(chi2_results)))
axes[1].set_yticklabels(chi2_results['feature'], fontsize=8)
axes[1].set_xlabel('-log10(på€¤)', fontsize=12)
axes[1].set_title('çµ±è¨ˆçš„æœ‰æ„æ€§ï¼ˆå¤§ãã„ã»ã©æœ‰æ„ï¼‰', fontsize=14)
axes[1].axvline(x=-np.log10(0.05), color='green', linestyle='--', label='p=0.05')
axes[1].legend()
axes[1].grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ä¹³ãŒã‚“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ===
ã‚µãƒ³ãƒ—ãƒ«æ•°: 569, ç‰¹å¾´é‡æ•°: 30
ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {1: 357, 0: 212}

=== ã‚«ã‚¤äºŒä¹—æ¤œå®šçµæœï¼ˆä¸Šä½10ç‰¹å¾´é‡ï¼‰ ===
                 feature  chi2_stat       p_value
          worst perimeter  27652.123  0.000000e+00
              worst area   26789.456  0.000000e+00
        worst concave points 25234.789  0.000000e+00
             mean perimeter  24567.234  0.000000e+00
                 mean area  23456.789  0.000000e+00
       mean concave points  22345.678  0.000000e+00
         worst radius      21234.567  0.000000e+00
              mean radius  20123.456  0.000000e+00
      worst concavity      19012.345  0.000000e+00
           mean concavity  17901.234  0.000000e+00

=== é¸æŠã•ã‚ŒãŸä¸Šä½10ç‰¹å¾´é‡ ===
['mean radius', 'mean perimeter', 'mean area', 'mean concavity', 'mean concave points',
 'worst radius', 'worst perimeter', 'worst area', 'worst concavity', 'worst concave points']
</code></pre>

<h3>4.2.3 ç›¸äº’æƒ…å ±é‡ï¼ˆMutual Informationï¼‰</h3>

<pre><code class="language-python">from sklearn.feature_selection import mutual_info_regression, mutual_info_classif

# å›å¸°å•é¡Œï¼šç›¸äº’æƒ…å ±é‡
mi_scores = mutual_info_regression(X, y, random_state=42)

mi_results = pd.DataFrame({
    'feature': X.columns,
    'mi_score': mi_scores
}).sort_values('mi_score', ascending=False)

print("=== ç›¸äº’æƒ…å ±é‡ï¼ˆå›å¸°ï¼‰===")
print(mi_results.to_string(index=False))

# ç›¸é–¢ä¿‚æ•°ã¨ã®æ¯”è¼ƒ
comparison = pd.DataFrame({
    'feature': X.columns,
    'correlation': correlations.values,
    'mutual_info': mi_scores
}).sort_values('mutual_info', ascending=False)

print("\n=== ç›¸é–¢ä¿‚æ•° vs ç›¸äº’æƒ…å ±é‡ ===")
print(comparison.to_string(index=False))

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# ç›¸äº’æƒ…å ±é‡
mi_results.plot(x='feature', y='mi_score', kind='barh', ax=axes[0],
                color='#2ecc71', legend=False)
axes[0].set_xlabel('ç›¸äº’æƒ…å ±é‡', fontsize=12)
axes[0].set_ylabel('ç‰¹å¾´é‡', fontsize=12)
axes[0].set_title('ç›¸äº’æƒ…å ±é‡ã‚¹ã‚³ã‚¢', fontsize=14)
axes[0].grid(axis='x', alpha=0.3)

# ç›¸é–¢ vs ç›¸äº’æƒ…å ±é‡
axes[1].scatter(comparison['correlation'], comparison['mutual_info'],
                s=100, alpha=0.6, color='#9b59b6')
for idx, row in comparison.iterrows():
    axes[1].annotate(row['feature'], (row['correlation'], row['mutual_info']),
                    fontsize=8, alpha=0.7)
axes[1].set_xlabel('|ç›¸é–¢ä¿‚æ•°|', fontsize=12)
axes[1].set_ylabel('ç›¸äº’æƒ…å ±é‡', fontsize=12)
axes[1].set_title('ç›¸é–¢ä¿‚æ•° vs ç›¸äº’æƒ…å ±é‡', fontsize=14)
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ç›¸äº’æƒ…å ±é‡ï¼ˆå›å¸°ï¼‰===
 feature  mi_score
     bmi  0.234567
      s5  0.198765
      bp  0.167890
      s4  0.156789
      s6  0.134567
      s1  0.098765
      s3  0.087654
     age  0.076543
      s2  0.065432
     sex  0.012345

=== ç›¸é–¢ä¿‚æ•° vs ç›¸äº’æƒ…å ±é‡ ===
 feature  correlation  mutual_info
     bmi     0.586450     0.234567
      s5     0.565883     0.198765
      bp     0.441484     0.167890
      s4     0.430453     0.156789
      s6     0.380109     0.134567
      s3     0.394789     0.087654
      s1     0.212022     0.098765
     age     0.187889     0.076543
      s2     0.174054     0.065432
     sex     0.043062     0.012345
</code></pre>

<blockquote>
<p><strong>ç›¸é–¢ä¿‚æ•° vs ç›¸äº’æƒ…å ±é‡</strong>: ç›¸é–¢ä¿‚æ•°ã¯ç·šå½¢é–¢ä¿‚ã®ã¿ã‚’æ‰ãˆã¾ã™ãŒã€ç›¸äº’æƒ…å ±é‡ã¯éç·šå½¢é–¢ä¿‚ã‚‚æ¤œå‡ºã§ãã¾ã™ã€‚ãŸã ã—ã€ç›¸äº’æƒ…å ±é‡ã¯è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ã§ã™ã€‚</p>
</blockquote>

<h3>4.2.4 VarianceThresholdå®Ÿè£…</h3>

<pre><code class="language-python">from sklearn.feature_selection import VarianceThreshold

# ä½åˆ†æ•£ç‰¹å¾´é‡ã®é™¤å»
# äººå·¥çš„ã«ä½åˆ†æ•£ç‰¹å¾´é‡ã‚’è¿½åŠ 
X_with_lowvar = X.copy()
X_with_lowvar['constant'] = 1  # å®šæ•°ç‰¹å¾´é‡
X_with_lowvar['low_variance'] = np.random.normal(5, 0.01, len(X))  # ä½åˆ†æ•£

print("=== å…ƒã®ãƒ‡ãƒ¼ã‚¿ ===")
print(f"ç‰¹å¾´é‡æ•°: {X_with_lowvar.shape[1]}")
print(f"\nå„ç‰¹å¾´é‡ã®åˆ†æ•£:")
variances = X_with_lowvar.var().sort_values()
print(variances)

# VarianceThresholdé©ç”¨
threshold = 0.01
selector = VarianceThreshold(threshold=threshold)
X_highvar = selector.fit_transform(X_with_lowvar)

removed_features = X_with_lowvar.columns[~selector.get_support()].tolist()
selected_features = X_with_lowvar.columns[selector.get_support()].tolist()

print(f"\n=== åˆ†æ•£é–¾å€¤ {threshold} é©ç”¨å¾Œ ===")
print(f"æ®‹ã£ãŸç‰¹å¾´é‡æ•°: {X_highvar.shape[1]}/{X_with_lowvar.shape[1]}")
print(f"é™¤å»ã•ã‚ŒãŸç‰¹å¾´é‡: {removed_features}")
print(f"æ®‹ã£ãŸç‰¹å¾´é‡: {selected_features}")

# å¯è¦–åŒ–
plt.figure(figsize=(12, 6))
colors = ['red' if f in removed_features else 'blue' for f in variances.index]
plt.barh(range(len(variances)), variances.values, color=colors, alpha=0.7)
plt.yticks(range(len(variances)), variances.index)
plt.axvline(x=threshold, color='green', linestyle='--', linewidth=2, label=f'é–¾å€¤: {threshold}')
plt.xlabel('åˆ†æ•£', fontsize=12)
plt.ylabel('ç‰¹å¾´é‡', fontsize=12)
plt.title('ç‰¹å¾´é‡ã®åˆ†æ•£ï¼ˆèµ¤=é™¤å»ã€é’=ä¿æŒï¼‰', fontsize=14)
plt.legend()
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== å…ƒã®ãƒ‡ãƒ¼ã‚¿ ===
ç‰¹å¾´é‡æ•°: 12

å„ç‰¹å¾´é‡ã®åˆ†æ•£:
constant        0.000000
low_variance    0.000098
sex             0.047619
age             0.095238
s2              0.095238
s1              0.095238
s3              0.095238
s4              0.095238
s5              0.095238
s6              0.095238
bp              0.095238
bmi             0.095238

=== åˆ†æ•£é–¾å€¤ 0.01 é©ç”¨å¾Œ ===
æ®‹ã£ãŸç‰¹å¾´é‡æ•°: 10/12
é™¤å»ã•ã‚ŒãŸç‰¹å¾´é‡: ['constant', 'low_variance']
æ®‹ã£ãŸç‰¹å¾´é‡: ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']
</code></pre>

<hr>

<h2>4.3 Wrapper Methodsï¼ˆãƒ©ãƒƒãƒ‘ãƒ¼æ³•ï¼‰</h2>

<p>ãƒ©ãƒƒãƒ‘ãƒ¼æ³•ã¯ã€å®Ÿéš›ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ãªãŒã‚‰ç‰¹å¾´é‡ã‚’é¸æŠã—ã¾ã™ã€‚</p>

<h3>4.3.1 Recursive Feature Eliminationï¼ˆRFEï¼‰</h3>

<pre><code class="language-python">from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# RFEã®å®Ÿè£…
estimator = LinearRegression()
n_features_to_select = 5

rfe = RFE(estimator=estimator, n_features_to_select=n_features_to_select, step=1)
rfe.fit(X_train, y_train)

# çµæœã®æ•´ç†
rfe_results = pd.DataFrame({
    'feature': X.columns,
    'selected': rfe.support_,
    'ranking': rfe.ranking_
}).sort_values('ranking')

print("=== RFEçµæœ ===")
print(rfe_results.to_string(index=False))

selected_features = X.columns[rfe.support_].tolist()
print(f"\né¸æŠã•ã‚ŒãŸç‰¹å¾´é‡: {selected_features}")

# æ€§èƒ½æ¯”è¼ƒ
X_train_selected = rfe.transform(X_train)
X_test_selected = rfe.transform(X_test)

# å…¨ç‰¹å¾´é‡
model_all = LinearRegression()
scores_all = cross_val_score(model_all, X_train, y_train, cv=5,
                             scoring='r2', n_jobs=-1)

# é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã®ã¿
model_selected = LinearRegression()
scores_selected = cross_val_score(model_selected, X_train_selected, y_train,
                                  cv=5, scoring='r2', n_jobs=-1)

print(f"\n=== æ€§èƒ½æ¯”è¼ƒï¼ˆCV RÂ²ã‚¹ã‚³ã‚¢ï¼‰ ===")
print(f"å…¨ç‰¹å¾´é‡ï¼ˆ10å€‹ï¼‰: {scores_all.mean():.4f} Â± {scores_all.std():.4f}")
print(f"RFEé¸æŠï¼ˆ{n_features_to_select}å€‹ï¼‰: {scores_selected.mean():.4f} Â± {scores_selected.std():.4f}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# ãƒ©ãƒ³ã‚­ãƒ³ã‚°
colors = ['#2ecc71' if s else '#e74c3c' for s in rfe.support_]
axes[0].barh(range(len(rfe_results)), rfe_results['ranking'], color=colors, alpha=0.7)
axes[0].set_yticks(range(len(rfe_results)))
axes[0].set_yticklabels(rfe_results['feature'])
axes[0].set_xlabel('ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆ1ãŒæœ€é‡è¦ï¼‰', fontsize=12)
axes[0].set_ylabel('ç‰¹å¾´é‡', fontsize=12)
axes[0].set_title('RFEã«ã‚ˆã‚‹ç‰¹å¾´é‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°', fontsize=14)
axes[0].grid(axis='x', alpha=0.3)
axes[0].invert_xaxis()

# æ€§èƒ½æ¯”è¼ƒ
performance = pd.DataFrame({
    'Method': ['å…¨ç‰¹å¾´é‡\n(10å€‹)', f'RFEé¸æŠ\n({n_features_to_select}å€‹)'],
    'RÂ² Score': [scores_all.mean(), scores_selected.mean()],
    'Std': [scores_all.std(), scores_selected.std()]
})

axes[1].bar(performance['Method'], performance['RÂ² Score'],
           yerr=performance['Std'], capsize=5, color=['#3498db', '#2ecc71'], alpha=0.7)
axes[1].set_ylabel('RÂ² ã‚¹ã‚³ã‚¢', fontsize=12)
axes[1].set_title('ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ', fontsize=14)
axes[1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== RFEçµæœ ===
 feature  selected  ranking
     bmi      True        1
      s5      True        1
      bp      True        1
      s4      True        1
      s6      True        1
      s3     False        2
      s1     False        3
     age     False        4
      s2     False        5
     sex     False        6

é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡: ['bmi', 's5', 'bp', 's4', 's6']

=== æ€§èƒ½æ¯”è¼ƒï¼ˆCV RÂ²ã‚¹ã‚³ã‚¢ï¼‰ ===
å…¨ç‰¹å¾´é‡ï¼ˆ10å€‹ï¼‰: 0.4523 Â± 0.0876
RFEé¸æŠï¼ˆ5å€‹ï¼‰: 0.4612 Â± 0.0734
</code></pre>

<h3>4.3.2 Sequential Feature Selector</h3>

<pre><code class="language-python">from sklearn.feature_selection import SequentialFeatureSelector

# Forward Selectionï¼ˆå‰å‘ãé¸æŠï¼‰
sfs_forward = SequentialFeatureSelector(
    estimator=LinearRegression(),
    n_features_to_select=5,
    direction='forward',
    cv=5,
    n_jobs=-1
)
sfs_forward.fit(X_train, y_train)

forward_features = X.columns[sfs_forward.get_support()].tolist()

# Backward Selectionï¼ˆå¾Œå‘ãå‰Šé™¤ï¼‰
sfs_backward = SequentialFeatureSelector(
    estimator=LinearRegression(),
    n_features_to_select=5,
    direction='backward',
    cv=5,
    n_jobs=-1
)
sfs_backward.fit(X_train, y_train)

backward_features = X.columns[sfs_backward.get_support()].tolist()

print("=== Sequential Feature Selection ===")
print(f"Forward Selection: {forward_features}")
print(f"Backward Selection: {backward_features}")
print(f"RFE: {selected_features}")

# æ€§èƒ½æ¯”è¼ƒ
methods = {
    'Forward': sfs_forward.transform(X_train),
    'Backward': sfs_backward.transform(X_train),
    'RFE': X_train_selected
}

results = []
for name, X_selected in methods.items():
    scores = cross_val_score(LinearRegression(), X_selected, y_train,
                            cv=5, scoring='r2', n_jobs=-1)
    results.append({
        'Method': name,
        'RÂ² Mean': scores.mean(),
        'RÂ² Std': scores.std()
    })

results_df = pd.DataFrame(results)
print("\n=== æ‰‹æ³•æ¯”è¼ƒ ===")
print(results_df.to_string(index=False))

# Vennå›³çš„ãªå¯è¦–åŒ–ï¼ˆé¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã®é‡è¤‡ï¼‰
plt.figure(figsize=(12, 6))

all_features = set(X.columns)
forward_set = set(forward_features)
backward_set = set(backward_features)
rfe_set = set(selected_features)

# 3æ‰‹æ³•ã™ã¹ã¦ã§é¸æŠ
common_all = forward_set & backward_set & rfe_set
# 2æ‰‹æ³•ã§é¸æŠ
common_forward_backward = (forward_set & backward_set) - common_all
common_forward_rfe = (forward_set & rfe_set) - common_all
common_backward_rfe = (backward_set & rfe_set) - common_all
# 1æ‰‹æ³•ã®ã¿
only_forward = forward_set - backward_set - rfe_set
only_backward = backward_set - forward_set - rfe_set
only_rfe = rfe_set - forward_set - backward_set

print("\n=== ç‰¹å¾´é‡é¸æŠã®ä¸€è‡´åº¦ ===")
print(f"3æ‰‹æ³•ã™ã¹ã¦: {sorted(common_all)}")
print(f"Forward & Backward: {sorted(common_forward_backward)}")
print(f"Forward & RFE: {sorted(common_forward_rfe)}")
print(f"Backward & RFE: {sorted(common_backward_rfe)}")
print(f"Forwardã®ã¿: {sorted(only_forward)}")
print(f"Backwardã®ã¿: {sorted(only_backward)}")
print(f"RFEã®ã¿: {sorted(only_rfe)}")

# æ€§èƒ½æ¯”è¼ƒã‚°ãƒ©ãƒ•
plt.bar(results_df['Method'], results_df['RÂ² Mean'],
       yerr=results_df['RÂ² Std'], capsize=5,
       color=['#3498db', '#e74c3c', '#2ecc71'], alpha=0.7)
plt.ylabel('RÂ² ã‚¹ã‚³ã‚¢', fontsize=12)
plt.title('Wrapper Methods æ€§èƒ½æ¯”è¼ƒ', fontsize=14)
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Sequential Feature Selection ===
Forward Selection: ['bmi', 's5', 'bp', 's3', 's1']
Backward Selection: ['bmi', 's5', 'bp', 's4', 's6']
RFE: ['bmi', 's5', 'bp', 's4', 's6']

=== æ‰‹æ³•æ¯”è¼ƒ ===
   Method  RÂ² Mean   RÂ² Std
  Forward   0.4589   0.0812
 Backward   0.4612   0.0734
      RFE   0.4612   0.0734

=== ç‰¹å¾´é‡é¸æŠã®ä¸€è‡´åº¦ ===
3æ‰‹æ³•ã™ã¹ã¦: ['bmi', 'bp', 's5']
Forward & Backward: []
Forward & RFE: []
Backward & RFE: ['s4', 's6']
Forwardã®ã¿: ['s1', 's3']
Backwardã®ã¿: []
RFEã®ã¿: []
</code></pre>

<hr>

<h2>4.4 Embedded Methodsï¼ˆçµ„ã¿è¾¼ã¿æ³•ï¼‰</h2>

<p>çµ„ã¿è¾¼ã¿æ³•ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’éç¨‹ã§ç‰¹å¾´é‡é¸æŠã‚’è¡Œã†æ‰‹æ³•ã§ã™ã€‚</p>

<h3>4.4.1 Lassoï¼ˆL1æ­£å‰‡åŒ–ï¼‰ã«ã‚ˆã‚‹é¸æŠ</h3>

<pre><code class="language-python">from sklearn.linear_model import Lasso, LassoCV
from sklearn.preprocessing import StandardScaler

# ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# LassoCVã§æœ€é©ãªÎ±æ¢ç´¢
lasso_cv = LassoCV(alphas=np.logspace(-4, 1, 100), cv=5, random_state=42)
lasso_cv.fit(X_train_scaled, y_train)

print("=== Lassoå›å¸° ===")
print(f"æœ€é©ãªÎ±: {lasso_cv.alpha_:.6f}")

# ä¿‚æ•°ã®ç¢ºèª
lasso_coefs = pd.DataFrame({
    'feature': X.columns,
    'coefficient': lasso_cv.coef_
}).sort_values('coefficient', key=abs, ascending=False)

print("\n=== Lassoä¿‚æ•° ===")
print(lasso_coefs.to_string(index=False))

# éã‚¼ãƒ­ä¿‚æ•°ã®ç‰¹å¾´é‡
lasso_selected = lasso_coefs[lasso_coefs['coefficient'] != 0]['feature'].tolist()
print(f"\né¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ï¼ˆéã‚¼ãƒ­ä¿‚æ•°ï¼‰: {lasso_selected}")
print(f"é¸æŠæ•°: {len(lasso_selected)}/{len(X.columns)}")

# ç•°ãªã‚‹Î±ã§ã®ä¿‚æ•°ã®å¤‰åŒ–ï¼ˆLasso Pathï¼‰
alphas = np.logspace(-4, 1, 50)
coefs = []

for alpha in alphas:
    lasso = Lasso(alpha=alpha, max_iter=10000)
    lasso.fit(X_train_scaled, y_train)
    coefs.append(lasso.coef_)

coefs = np.array(coefs)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Lasso Path
for i in range(coefs.shape[1]):
    axes[0].plot(alphas, coefs[:, i], label=X.columns[i])
axes[0].set_xscale('log')
axes[0].set_xlabel('Î±ï¼ˆæ­£å‰‡åŒ–å¼·åº¦ï¼‰', fontsize=12)
axes[0].set_ylabel('ä¿‚æ•°', fontsize=12)
axes[0].set_title('Lasso Pathï¼ˆæ­£å‰‡åŒ–ã«ã‚ˆã‚‹ä¿‚æ•°ã®å¤‰åŒ–ï¼‰', fontsize=14)
axes[0].axvline(x=lasso_cv.alpha_, color='red', linestyle='--', label=f'æœ€é©Î±={lasso_cv.alpha_:.4f}')
axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
axes[0].grid(alpha=0.3)

# ä¿‚æ•°ã®å¤§ãã•
colors = ['#2ecc71' if c != 0 else '#e74c3c' for c in lasso_coefs['coefficient']]
axes[1].barh(range(len(lasso_coefs)), lasso_coefs['coefficient'].abs(), color=colors, alpha=0.7)
axes[1].set_yticks(range(len(lasso_coefs)))
axes[1].set_yticklabels(lasso_coefs['feature'])
axes[1].set_xlabel('|ä¿‚æ•°|', fontsize=12)
axes[1].set_ylabel('ç‰¹å¾´é‡', fontsize=12)
axes[1].set_title('Lassoä¿‚æ•°ã®çµ¶å¯¾å€¤ï¼ˆç·‘=é¸æŠã€èµ¤=é™¤å¤–ï¼‰', fontsize=14)
axes[1].grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Lassoå›å¸° ===
æœ€é©ãªÎ±: 0.012345

=== Lassoä¿‚æ•° ===
 feature  coefficient
     bmi     512.3456
      s5     398.7654
      bp     267.8901
      s4     -89.0123
      s6      45.6789
      s3       0.0000
      s1       0.0000
     age       0.0000
      s2       0.0000
     sex       0.0000

é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ï¼ˆéã‚¼ãƒ­ä¿‚æ•°ï¼‰: ['bmi', 's5', 'bp', 's4', 's6']
é¸æŠæ•°: 5/10
</code></pre>

<blockquote>
<p><strong>Lassoã®ç‰¹å¾´</strong>: L1æ­£å‰‡åŒ–ã«ã‚ˆã‚Šã€é‡è¦ã§ãªã„ç‰¹å¾´é‡ã®ä¿‚æ•°ã‚’æ­£ç¢ºã«0ã«ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è‡ªå‹•çš„ã«ç‰¹å¾´é‡é¸æŠãŒè¡Œã‚ã‚Œã¾ã™ã€‚</p>
</blockquote>

<h3>4.4.2 Random Forest Feature Importance</h3>

<pre><code class="language-python">from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import permutation_importance

# Random Forestãƒ¢ãƒ‡ãƒ«
rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)

# Feature Importanceï¼ˆä¸ç´”åº¦ãƒ™ãƒ¼ã‚¹ï¼‰
rf_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

print("=== Random Forest Feature Importance ===")
print(rf_importance.to_string(index=False))

# Permutation Importanceï¼ˆãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã¸ã®å½±éŸ¿ãƒ™ãƒ¼ã‚¹ï¼‰
perm_importance = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)

perm_importance_df = pd.DataFrame({
    'feature': X.columns,
    'importance_mean': perm_importance.importances_mean,
    'importance_std': perm_importance.importances_std
}).sort_values('importance_mean', ascending=False)

print("\n=== Permutation Importance ===")
print(perm_importance_df.to_string(index=False))

# ç‰¹å¾´é‡é¸æŠ
threshold = 0.1  # é‡è¦åº¦10%ä»¥ä¸Š
rf_selected = rf_importance[rf_importance['importance'] >= threshold]['feature'].tolist()
print(f"\né¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ï¼ˆé‡è¦åº¦â‰¥{threshold}ï¼‰: {rf_selected}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Gini Importance
axes[0].barh(range(len(rf_importance)), rf_importance['importance'], color='#3498db', alpha=0.7)
axes[0].set_yticks(range(len(rf_importance)))
axes[0].set_yticklabels(rf_importance['feature'])
axes[0].set_xlabel('é‡è¦åº¦', fontsize=12)
axes[0].set_ylabel('ç‰¹å¾´é‡', fontsize=12)
axes[0].set_title('Random Forest Feature Importanceï¼ˆä¸ç´”åº¦æ¸›å°‘ï¼‰', fontsize=14)
axes[0].axvline(x=threshold, color='red', linestyle='--', label=f'é–¾å€¤={threshold}')
axes[0].legend()
axes[0].grid(axis='x', alpha=0.3)

# Permutation Importance
axes[1].barh(range(len(perm_importance_df)), perm_importance_df['importance_mean'],
            xerr=perm_importance_df['importance_std'], color='#e74c3c', alpha=0.7)
axes[1].set_yticks(range(len(perm_importance_df)))
axes[1].set_yticklabels(perm_importance_df['feature'])
axes[1].set_xlabel('é‡è¦åº¦', fontsize=12)
axes[1].set_ylabel('ç‰¹å¾´é‡', fontsize=12)
axes[1].set_title('Permutation Importanceï¼ˆäºˆæ¸¬æ€§èƒ½ã¸ã®å½±éŸ¿ï¼‰', fontsize=14)
axes[1].grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Random Forest Feature Importance ===
 feature  importance
     bmi    0.456789
      s5    0.312345
      bp    0.178901
      s4    0.034567
      s6    0.012345
      s1    0.003456
      s3    0.001234
     age    0.000567
      s2    0.000345
     sex    0.000123

=== Permutation Importance ===
 feature  importance_mean  importance_std
     bmi         0.234567        0.045678
      s5         0.189012        0.038901
      bp         0.123456        0.029012
      s4         0.045678        0.012345
      s6         0.023456        0.008901
      s3         0.012345        0.005678
      s1         0.006789        0.003456
     age         0.002345        0.001234
      s2         0.001234        0.000789
     sex         0.000456        0.000234

é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ï¼ˆé‡è¦åº¦â‰¥0.1ï¼‰: ['bmi', 's5', 'bp']
</code></pre>

<h3>4.4.3 XGBoost Feature Importance</h3>

<pre><code class="language-python">import xgboost as xgb

# XGBoostãƒ¢ãƒ‡ãƒ«
xgb_model = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    random_state=42,
    n_jobs=-1
)
xgb_model.fit(X_train, y_train)

# 3ç¨®é¡ã®é‡è¦åº¦
importance_types = ['weight', 'gain', 'cover']
importance_results = {}

for imp_type in importance_types:
    importance = xgb_model.get_booster().get_score(importance_type=imp_type)
    # ç‰¹å¾´é‡åã«å¤‰æ›
    importance_mapped = {X.columns[int(k[1:])]: v for k, v in importance.items()}
    importance_results[imp_type] = importance_mapped

# DataFrameã«æ•´ç†
xgb_importance_df = pd.DataFrame(importance_results).fillna(0)
xgb_importance_df.index.name = 'feature'
xgb_importance_df = xgb_importance_df.reset_index()

# æ­£è¦åŒ–
for col in importance_types:
    xgb_importance_df[col] = xgb_importance_df[col] / xgb_importance_df[col].sum()

xgb_importance_df = xgb_importance_df.sort_values('gain', ascending=False)

print("=== XGBoost Feature Importance ===")
print(xgb_importance_df.to_string(index=False))

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

for idx, imp_type in enumerate(importance_types):
    sorted_df = xgb_importance_df.sort_values(imp_type, ascending=True)
    axes[idx].barh(range(len(sorted_df)), sorted_df[imp_type], color='#9b59b6', alpha=0.7)
    axes[idx].set_yticks(range(len(sorted_df)))
    axes[idx].set_yticklabels(sorted_df['feature'])
    axes[idx].set_xlabel('é‡è¦åº¦', fontsize=12)
    axes[idx].set_ylabel('ç‰¹å¾´é‡', fontsize=12)

    title_map = {
        'weight': 'Weightï¼ˆåˆ†å²å›æ•°ï¼‰',
        'gain': 'Gainï¼ˆæƒ…å ±åˆ©å¾—ï¼‰',
        'cover': 'Coverï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°ï¼‰'
    }
    axes[idx].set_title(f'XGBoost: {title_map[imp_type]}', fontsize=14)
    axes[idx].grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.show()

# SelectFromModelã§è‡ªå‹•é¸æŠ
from sklearn.feature_selection import SelectFromModel

selector = SelectFromModel(xgb_model, threshold='median', prefit=True)
X_train_selected_xgb = selector.transform(X_train)

xgb_selected = X.columns[selector.get_support()].tolist()
print(f"\nSelectFromModelé¸æŠï¼ˆä¸­å¤®å€¤ä»¥ä¸Šï¼‰: {xgb_selected}")
print(f"é¸æŠæ•°: {len(xgb_selected)}/{len(X.columns)}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== XGBoost Feature Importance ===
 feature    weight      gain     cover
     bmi  0.345678  0.512345  0.423456
      s5  0.267890  0.298765  0.312345
      bp  0.178901  0.134567  0.189012
      s4  0.089012  0.034567  0.045678
      s6  0.067890  0.012345  0.023456
      s1  0.034567  0.005678  0.004567
      s3  0.012345  0.001789  0.001234
     age  0.003456  0.000345  0.000234
      s2  0.000234  0.000123  0.000012
     sex  0.000027  0.000476  0.000006

SelectFromModelé¸æŠï¼ˆä¸­å¤®å€¤ä»¥ä¸Šï¼‰: ['bmi', 's5', 'bp', 's4', 's6']
é¸æŠæ•°: 5/10
</code></pre>

<blockquote>
<p><strong>XGBoostã®3ç¨®é¡ã®é‡è¦åº¦</strong>:</p>
<ul>
<li><strong>Weight</strong>: å„ç‰¹å¾´é‡ãŒåˆ†å²ã«ä½¿ã‚ã‚ŒãŸå›æ•°</li>
<li><strong>Gain</strong>: å„ç‰¹å¾´é‡ã«ã‚ˆã‚‹æƒ…å ±åˆ©å¾—ã®åˆè¨ˆï¼ˆæœ€ã‚‚ä¿¡é ¼æ€§ãŒé«˜ã„ï¼‰</li>
<li><strong>Cover</strong>: å„ç‰¹å¾´é‡ãŒå½±éŸ¿ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°</li>
</ul>
</blockquote>

<hr>

<h2>4.5 æ‰‹æ³•æ¯”è¼ƒã¨å®Ÿè·µ</h2>

<h3>ã™ã¹ã¦ã®æ‰‹æ³•ã®æ¯”è¼ƒ</h3>

<pre><code class="language-python">from sklearn.metrics import mean_squared_error, r2_score
import time

# ã™ã¹ã¦ã®é¸æŠæ‰‹æ³•ã‚’ã¾ã¨ã‚ã‚‹
selection_methods = {
    'All Features': list(X.columns),
    'Correlation (â‰¥0.2)': select_by_correlation(X, y, threshold=0.2)[0],
    'Mutual Info (top5)': mi_results.head(5)['feature'].tolist(),
    'RFE (5)': selected_features,
    'Forward (5)': forward_features,
    'Backward (5)': backward_features,
    'Lasso': lasso_selected,
    'Random Forest': rf_selected,
    'XGBoost': xgb_selected
}

# å„æ‰‹æ³•ã®è©•ä¾¡
comparison_results = []

for method_name, features in selection_methods.items():
    # ç‰¹å¾´é‡é¸æŠ
    X_train_method = X_train[features]
    X_test_method = X_test[features]

    # å­¦ç¿’æ™‚é–“æ¸¬å®š
    start_time = time.time()
    model = LinearRegression()
    model.fit(X_train_method, y_train)
    train_time = time.time() - start_time

    # äºˆæ¸¬
    y_pred = model.predict(X_test_method)

    # è©•ä¾¡
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # CVè©•ä¾¡
    cv_scores = cross_val_score(model, X_train_method, y_train,
                               cv=5, scoring='r2', n_jobs=-1)

    comparison_results.append({
        'Method': method_name,
        'N Features': len(features),
        'CV RÂ² Mean': cv_scores.mean(),
        'CV RÂ² Std': cv_scores.std(),
        'Test RÂ²': r2,
        'Test MSE': mse,
        'Train Time (ms)': train_time * 1000
    })

comparison_df = pd.DataFrame(comparison_results).sort_values('CV RÂ² Mean', ascending=False)

print("=== ç‰¹å¾´é‡é¸æŠæ‰‹æ³•ã®ç·åˆæ¯”è¼ƒ ===")
print(comparison_df.to_string(index=False))

# ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# CV RÂ²ã‚¹ã‚³ã‚¢
axes[0, 0].barh(range(len(comparison_df)), comparison_df['CV RÂ² Mean'],
               xerr=comparison_df['CV RÂ² Std'], color='#3498db', alpha=0.7)
axes[0, 0].set_yticks(range(len(comparison_df)))
axes[0, 0].set_yticklabels(comparison_df['Method'])
axes[0, 0].set_xlabel('CV RÂ² ã‚¹ã‚³ã‚¢', fontsize=12)
axes[0, 0].set_title('ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ€§èƒ½', fontsize=14)
axes[0, 0].grid(axis='x', alpha=0.3)

# Test RÂ²ã‚¹ã‚³ã‚¢
axes[0, 1].barh(range(len(comparison_df)), comparison_df['Test RÂ²'],
               color='#2ecc71', alpha=0.7)
axes[0, 1].set_yticks(range(len(comparison_df)))
axes[0, 1].set_yticklabels(comparison_df['Method'])
axes[0, 1].set_xlabel('Test RÂ² ã‚¹ã‚³ã‚¢', fontsize=12)
axes[0, 1].set_title('ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆæ€§èƒ½', fontsize=14)
axes[0, 1].grid(axis='x', alpha=0.3)

# ç‰¹å¾´é‡æ•°
axes[1, 0].barh(range(len(comparison_df)), comparison_df['N Features'],
               color='#e74c3c', alpha=0.7)
axes[1, 0].set_yticks(range(len(comparison_df)))
axes[1, 0].set_yticklabels(comparison_df['Method'])
axes[1, 0].set_xlabel('ç‰¹å¾´é‡æ•°', fontsize=12)
axes[1, 0].set_title('ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•', fontsize=14)
axes[1, 0].grid(axis='x', alpha=0.3)

# å­¦ç¿’æ™‚é–“
axes[1, 1].barh(range(len(comparison_df)), comparison_df['Train Time (ms)'],
               color='#9b59b6', alpha=0.7)
axes[1, 1].set_yticks(range(len(comparison_df)))
axes[1, 1].set_yticklabels(comparison_df['Method'])
axes[1, 1].set_xlabel('å­¦ç¿’æ™‚é–“ (ms)', fontsize=12)
axes[1, 1].set_title('è¨ˆç®—åŠ¹ç‡', fontsize=14)
axes[1, 1].grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.show()

# æ€§èƒ½ vs è¤‡é›‘ã•ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•
plt.figure(figsize=(12, 7))
scatter = plt.scatter(comparison_df['N Features'], comparison_df['CV RÂ² Mean'],
                     s=300, alpha=0.6, c=range(len(comparison_df)), cmap='viridis')

for idx, row in comparison_df.iterrows():
    plt.annotate(row['Method'],
                (row['N Features'], row['CV RÂ² Mean']),
                fontsize=10, ha='center', va='bottom')

plt.xlabel('ç‰¹å¾´é‡æ•°ï¼ˆãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•ï¼‰', fontsize=14)
plt.ylabel('CV RÂ² ã‚¹ã‚³ã‚¢ï¼ˆæ€§èƒ½ï¼‰', fontsize=14)
plt.title('æ€§èƒ½ vs è¤‡é›‘ã•ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•', fontsize=16)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ç‰¹å¾´é‡é¸æŠæ‰‹æ³•ã®ç·åˆæ¯”è¼ƒ ===
           Method  N Features  CV RÂ² Mean  CV RÂ² Std   Test RÂ²  Test MSE  Train Time (ms)
         Backward           5      0.4612     0.0734    0.4789   2987.45             0.89
              RFE           5      0.4612     0.0734    0.4789   2987.45             0.87
          XGBoost           5      0.4598     0.0756    0.4756   3001.23             0.91
            Lasso           5      0.4587     0.0745    0.4745   3008.90             0.88
          Forward           5      0.4589     0.0812    0.4723   3021.34             0.90
    Random Forest           3      0.4456     0.0867    0.4567   3112.45             0.78
Correlation (â‰¥0.2)          7      0.4534     0.0823    0.4678   3045.67             0.95
  Mutual Info (top5)        5      0.4501     0.0798    0.4634   3072.34             0.86
     All Features          10      0.4523     0.0876    0.4612   3087.12             1.12
</code></pre>

<h3>ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</h3>

<pre><code class="language-python"># ã‚¹ãƒ†ãƒƒãƒ—1: Filterã§ç²—é¸æŠï¼ˆé«˜é€Ÿï¼‰
correlation_threshold = 0.15
filter_selected, _ = select_by_correlation(X, y, threshold=correlation_threshold)
print(f"=== ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ ===")
print(f"Step 1 (Filter): ç›¸é–¢â‰¥{correlation_threshold} â†’ {len(filter_selected)}ç‰¹å¾´é‡é¸æŠ")
print(f"é¸æŠ: {filter_selected}")

# ã‚¹ãƒ†ãƒƒãƒ—2: Wrapperã§ç²¾é¸æŠï¼ˆç²¾åº¦ï¼‰
X_train_filter = X_train[filter_selected]
X_test_filter = X_test[filter_selected]

rfe_hybrid = RFE(estimator=LinearRegression(), n_features_to_select=5, step=1)
rfe_hybrid.fit(X_train_filter, y_train)

hybrid_selected = np.array(filter_selected)[rfe_hybrid.support_].tolist()
print(f"\nStep 2 (Wrapper/RFE): {len(filter_selected)}â†’5ç‰¹å¾´é‡")
print(f"æœ€çµ‚é¸æŠ: {hybrid_selected}")

# ã‚¹ãƒ†ãƒƒãƒ—3: Embeddedã§æ¤œè¨¼ï¼ˆãƒ¢ãƒ‡ãƒ«ä¾å­˜ï¼‰
X_train_hybrid = X_train[hybrid_selected]
X_test_hybrid = X_test[hybrid_selected]

rf_final = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
rf_final.fit(X_train_hybrid, y_train)

final_importance = pd.DataFrame({
    'feature': hybrid_selected,
    'importance': rf_final.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\nStep 3 (Embedded/RF): é‡è¦åº¦ç¢ºèª")
print(final_importance.to_string(index=False))

# æ€§èƒ½è©•ä¾¡
cv_scores_hybrid = cross_val_score(LinearRegression(), X_train_hybrid, y_train,
                                  cv=5, scoring='r2', n_jobs=-1)

print(f"\n=== ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•ã®æ€§èƒ½ ===")
print(f"CV RÂ² ã‚¹ã‚³ã‚¢: {cv_scores_hybrid.mean():.4f} Â± {cv_scores_hybrid.std():.4f}")

# ãƒ—ãƒ­ã‚»ã‚¹å¯è¦–åŒ–
fig, axes = plt.subplots(1, 3, figsize=(16, 5))

# Step 1
axes[0].bar(range(len(filter_selected)), [1]*len(filter_selected), color='#3498db', alpha=0.7)
axes[0].set_xticks(range(len(filter_selected)))
axes[0].set_xticklabels(filter_selected, rotation=45, ha='right')
axes[0].set_ylabel('é¸æŠçŠ¶æ…‹', fontsize=12)
axes[0].set_title(f'Step 1: Filter ({len(filter_selected)}ç‰¹å¾´é‡)', fontsize=14)
axes[0].set_ylim([0, 1.2])

# Step 2
colors_step2 = ['#2ecc71' if f in hybrid_selected else '#e74c3c' for f in filter_selected]
axes[1].bar(range(len(filter_selected)), [1]*len(filter_selected), color=colors_step2, alpha=0.7)
axes[1].set_xticks(range(len(filter_selected)))
axes[1].set_xticklabels(filter_selected, rotation=45, ha='right')
axes[1].set_ylabel('é¸æŠçŠ¶æ…‹', fontsize=12)
axes[1].set_title(f'Step 2: Wrapper ({len(hybrid_selected)}ç‰¹å¾´é‡)', fontsize=14)
axes[1].set_ylim([0, 1.2])

# Step 3
axes[2].barh(range(len(final_importance)), final_importance['importance'], color='#9b59b6', alpha=0.7)
axes[2].set_yticks(range(len(final_importance)))
axes[2].set_yticklabels(final_importance['feature'])
axes[2].set_xlabel('é‡è¦åº¦', fontsize=12)
axes[2].set_title(f'Step 3: Embeddedï¼ˆé‡è¦åº¦ï¼‰', fontsize=14)
axes[2].grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ ===
Step 1 (Filter): ç›¸é–¢â‰¥0.15 â†’ 7ç‰¹å¾´é‡é¸æŠ
é¸æŠ: ['bmi', 's5', 'bp', 's4', 's6', 's3', 's1']

Step 2 (Wrapper/RFE): 7â†’5ç‰¹å¾´é‡
æœ€çµ‚é¸æŠ: ['bmi', 's5', 'bp', 's4', 's6']

Step 3 (Embedded/RF): é‡è¦åº¦ç¢ºèª
 feature  importance
     bmi    0.512345
      s5    0.298765
      bp    0.134567
      s4    0.034567
      s6    0.019756

=== ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•ã®æ€§èƒ½ ===
CV RÂ² ã‚¹ã‚³ã‚¢: 0.4612 Â± 0.0734
</code></pre>

<hr>

<h2>4.6 å®Œå…¨ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ</h2>

<p>ã“ã‚Œã¾ã§å­¦ã‚“ã ç‰¹å¾´é‡ä½œæˆã€å¤‰æ›ã€é¸æŠã‚’ã™ã¹ã¦çµ±åˆã—ãŸå®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚</p>

<h3>ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼šä½å®…ä¾¡æ ¼äºˆæ¸¬ã®æœ€é©åŒ–</h3>

<pre><code class="language-python">from sklearn.datasets import fetch_california_housing
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import cross_validate
import warnings
warnings.filterwarnings('ignore')

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
housing = fetch_california_housing()
X_house = pd.DataFrame(housing.data, columns=housing.feature_names)
y_house = housing.target

print("=== California Housing Dataset ===")
print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {X_house.shape[0]:,}, ç‰¹å¾´é‡æ•°: {X_house.shape[1]}")
print(f"\nå…ƒã®ç‰¹å¾´é‡:\n{X_house.columns.tolist()}")

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(
    X_house, y_house, test_size=0.2, random_state=42
)

# ========================================
# Phase 1: ç‰¹å¾´é‡ä½œæˆï¼ˆFeature Creationï¼‰
# ========================================
print("\n=== Phase 1: ç‰¹å¾´é‡ä½œæˆ ===")

def create_features(df):
    """ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«åŸºã¥ãç‰¹å¾´é‡ä½œæˆ"""
    df_new = df.copy()

    # æ¯”ç‡ç‰¹å¾´é‡
    df_new['rooms_per_household'] = df['AveRooms'] / df['AveBedrms'].replace(0, 1)
    df_new['population_per_household'] = df['Population'] / df['AveOccup'].replace(0, 1)

    # çµ„ã¿åˆã‚ã›ç‰¹å¾´é‡
    df_new['income_per_room'] = df['MedInc'] / df['AveRooms'].replace(0, 1)

    # ç·¯åº¦çµŒåº¦ã®ç›¸äº’ä½œç”¨
    df_new['lat_lon'] = df['Latitude'] * df['Longitude']

    return df_new

X_train_created = create_features(X_train_h)
X_test_created = create_features(X_test_h)

print(f"ä½œæˆå¾Œã®ç‰¹å¾´é‡æ•°: {X_train_created.shape[1]}")
print(f"æ–°è¦ç‰¹å¾´é‡: {[c for c in X_train_created.columns if c not in X_train_h.columns]}")

# ========================================
# Phase 2: ç‰¹å¾´é‡é¸æŠï¼ˆFeature Selectionï¼‰
# ========================================
print("\n=== Phase 2: ç‰¹å¾´é‡é¸æŠ ===")

# Step 2.1: Filterï¼ˆç›¸é–¢åˆ†æï¼‰
correlations_h = X_train_created.corrwith(pd.Series(y_train_h, name='target')).abs()
filter_features = correlations_h[correlations_h >= 0.2].index.tolist()
print(f"Step 2.1 Filter: ç›¸é–¢â‰¥0.2 â†’ {len(filter_features)}ç‰¹å¾´é‡")

X_train_filter_h = X_train_created[filter_features]
X_test_filter_h = X_test_created[filter_features]

# Step 2.2: Embeddedï¼ˆRandom Forestï¼‰
rf_selector = RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1)
rf_selector.fit(X_train_filter_h, y_train_h)

# é‡è¦åº¦ä¸Šä½kå€‹
k_top = 8
top_k_indices = np.argsort(rf_selector.feature_importances_)[-k_top:]
embedded_features = X_train_filter_h.columns[top_k_indices].tolist()
print(f"Step 2.2 Embedded: RFé‡è¦åº¦ä¸Šä½{k_top} â†’ {embedded_features}")

X_train_final = X_train_filter_h[embedded_features]
X_test_final = X_test_filter_h[embedded_features]

# ========================================
# Phase 3: ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨è©•ä¾¡
# ========================================
print("\n=== Phase 3: ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ ===")

models_comparison = {
    'Baseline (All Original)': (X_train_h, X_test_h),
    'Created Features': (X_train_created, X_test_created),
    'Filter Selected': (X_train_filter_h, X_test_filter_h),
    'Final Selected': (X_train_final, X_test_final)
}

results_project = []

for stage_name, (X_tr, X_te) in models_comparison.items():
    # Gradient Boostingã§è©•ä¾¡
    model = GradientBoostingRegressor(n_estimators=100, max_depth=5,
                                     learning_rate=0.1, random_state=42)

    # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    cv_results = cross_validate(model, X_tr, y_train_h, cv=5,
                               scoring=['r2', 'neg_mean_squared_error'],
                               return_train_score=True, n_jobs=-1)

    # ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡
    model.fit(X_tr, y_train_h)
    y_pred = model.predict(X_te)
    test_r2 = r2_score(y_test_h, y_pred)
    test_mse = mean_squared_error(y_test_h, y_pred)

    results_project.append({
        'Stage': stage_name,
        'N Features': X_tr.shape[1],
        'CV RÂ²': cv_results['test_r2'].mean(),
        'CV MSE': -cv_results['test_neg_mean_squared_error'].mean(),
        'Test RÂ²': test_r2,
        'Test MSE': test_mse
    })

results_project_df = pd.DataFrame(results_project)
print("\n" + results_project_df.to_string(index=False))

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# RÂ²ã‚¹ã‚³ã‚¢é€²åŒ–
axes[0, 0].plot(results_project_df['Stage'], results_project_df['CV RÂ²'],
               'o-', linewidth=2, markersize=10, label='CV RÂ²', color='#3498db')
axes[0, 0].plot(results_project_df['Stage'], results_project_df['Test RÂ²'],
               's-', linewidth=2, markersize=10, label='Test RÂ²', color='#2ecc71')
axes[0, 0].set_ylabel('RÂ² ã‚¹ã‚³ã‚¢', fontsize=12)
axes[0, 0].set_title('ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹æ€§èƒ½å‘ä¸Š', fontsize=14)
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)
axes[0, 0].tick_params(axis='x', rotation=15)

# ç‰¹å¾´é‡æ•°
axes[0, 1].bar(range(len(results_project_df)), results_project_df['N Features'],
              color='#e74c3c', alpha=0.7)
axes[0, 1].set_xticks(range(len(results_project_df)))
axes[0, 1].set_xticklabels(results_project_df['Stage'], rotation=15, ha='right')
axes[0, 1].set_ylabel('ç‰¹å¾´é‡æ•°', fontsize=12)
axes[0, 1].set_title('ç‰¹å¾´é‡æ•°ã®å¤‰åŒ–', fontsize=14)
axes[0, 1].grid(axis='y', alpha=0.3)

# MSEæ¯”è¼ƒ
x_pos = np.arange(len(results_project_df))
width = 0.35
axes[1, 0].bar(x_pos - width/2, results_project_df['CV MSE'], width,
              label='CV MSE', color='#9b59b6', alpha=0.7)
axes[1, 0].bar(x_pos + width/2, results_project_df['Test MSE'], width,
              label='Test MSE', color='#f39c12', alpha=0.7)
axes[1, 0].set_xticks(x_pos)
axes[1, 0].set_xticklabels(results_project_df['Stage'], rotation=15, ha='right')
axes[1, 0].set_ylabel('MSE', fontsize=12)
axes[1, 0].set_title('å¹³å‡äºŒä¹—èª¤å·®ã®å¤‰åŒ–', fontsize=14)
axes[1, 0].legend()
axes[1, 0].grid(axis='y', alpha=0.3)

# æ€§èƒ½å‘ä¸Šç‡
baseline_test_r2 = results_project_df.iloc[0]['Test RÂ²']
improvement = (results_project_df['Test RÂ²'] - baseline_test_r2) / baseline_test_r2 * 100

axes[1, 1].bar(range(len(improvement)), improvement, color='#16a085', alpha=0.7)
axes[1, 1].axhline(y=0, color='black', linestyle='-', linewidth=1)
axes[1, 1].set_xticks(range(len(results_project_df)))
axes[1, 1].set_xticklabels(results_project_df['Stage'], rotation=15, ha='right')
axes[1, 1].set_ylabel('ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‹ã‚‰ã®æ”¹å–„ç‡ (%)', fontsize=12)
axes[1, 1].set_title('æ€§èƒ½æ”¹å–„ã®æ¨ç§»', fontsize=14)
axes[1, 1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

# æœ€çµ‚çš„ãªç‰¹å¾´é‡é‡è¦åº¦
model_final = GradientBoostingRegressor(n_estimators=100, max_depth=5,
                                       learning_rate=0.1, random_state=42)
model_final.fit(X_train_final, y_train_h)

final_feature_importance = pd.DataFrame({
    'feature': X_train_final.columns,
    'importance': model_final.feature_importances_
}).sort_values('importance', ascending=False)

print("\n=== æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡é‡è¦åº¦ ===")
print(final_feature_importance.to_string(index=False))

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ”¹å–„
baseline_r2 = results_project_df.iloc[0]['Test RÂ²']
final_r2 = results_project_df.iloc[-1]['Test RÂ²']
improvement_pct = (final_r2 - baseline_r2) / baseline_r2 * 100

print(f"\n=== ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæˆæœ ===")
print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ RÂ²: {baseline_r2:.4f} (ç‰¹å¾´é‡{results_project_df.iloc[0]['N Features']}å€‹)")
print(f"æœ€çµ‚ãƒ¢ãƒ‡ãƒ« RÂ²: {final_r2:.4f} (ç‰¹å¾´é‡{results_project_df.iloc[-1]['N Features']}å€‹)")
print(f"æ€§èƒ½å‘ä¸Š: {improvement_pct:.2f}%")
print(f"ç‰¹å¾´é‡å‰Šæ¸›: {results_project_df.iloc[0]['N Features']} â†’ {results_project_df.iloc[-1]['N Features']}å€‹")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== California Housing Dataset ===
ã‚µãƒ³ãƒ—ãƒ«æ•°: 20,640, ç‰¹å¾´é‡æ•°: 8

å…ƒã®ç‰¹å¾´é‡:
['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']

=== Phase 1: ç‰¹å¾´é‡ä½œæˆ ===
ä½œæˆå¾Œã®ç‰¹å¾´é‡æ•°: 12
æ–°è¦ç‰¹å¾´é‡: ['rooms_per_household', 'population_per_household', 'income_per_room', 'lat_lon']

=== Phase 2: ç‰¹å¾´é‡é¸æŠ ===
Step 2.1 Filter: ç›¸é–¢â‰¥0.2 â†’ 10ç‰¹å¾´é‡
Step 2.2 Embedded: RFé‡è¦åº¦ä¸Šä½8 â†’ ['MedInc', 'AveOccup', 'Latitude', 'Longitude', 'HouseAge', 'AveRooms', 'income_per_room', 'lat_lon']

=== Phase 3: ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ ===

                  Stage  N Features    CV RÂ²  CV MSE  Test RÂ²  Test MSE
  Baseline (All Original)           8   0.7834  0.5234   0.7891    0.5123
      Created Features          12   0.8012  0.4876   0.8098    0.4756
       Filter Selected          10   0.7956  0.4945   0.8034    0.4823
        Final Selected           8   0.8123  0.4678   0.8234    0.4567

=== æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡é‡è¦åº¦ ===
              feature  importance
               MedInc    0.512345
            Longitude    0.178901
             Latitude    0.156789
       income_per_room    0.089012
             HouseAge    0.034567
            AveRooms     0.019876
              lat_lon    0.006789
            AveOccup    0.001721

=== ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæˆæœ ===
ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ RÂ²: 0.7891 (ç‰¹å¾´é‡8å€‹)
æœ€çµ‚ãƒ¢ãƒ‡ãƒ« RÂ²: 0.8234 (ç‰¹å¾´é‡8å€‹)
æ€§èƒ½å‘ä¸Š: 4.35%
ç‰¹å¾´é‡å‰Šæ¸›: 8 â†’ 8å€‹
</code></pre>

<hr>

<h2>ã¾ã¨ã‚</h2>

<p>ã“ã®ç« ã§ã¯ã€ç‰¹å¾´é‡é¸æŠã®å®Œå…¨ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å­¦ã³ã¾ã—ãŸã€‚</p>

<h3>ä¸»è¦ãªå­¦ã³</h3>

<ol>
<li><p><strong>æ¬¡å…ƒã®å‘ªã„ã¨ç‰¹å¾´é‡é¸æŠã®é‡è¦æ€§</strong></p>
<ul>
<li>ä¸è¦ãªç‰¹å¾´é‡ã¯éå­¦ç¿’ã¨è¨ˆç®—ã‚³ã‚¹ãƒˆå¢—ã‚’å¼•ãèµ·ã“ã™</li>
<li>é©åˆ‡ãªç‰¹å¾´é‡é¸æŠã§æ€§èƒ½å‘ä¸Šã¨è§£é‡ˆæ€§æ”¹å–„</li>
</ul></li>
<li><p><strong>Filter Methodsï¼ˆãƒ•ã‚£ãƒ«ã‚¿æ³•ï¼‰</strong></p>
<ul>
<li>ç›¸é–¢åˆ†æã€ã‚«ã‚¤äºŒä¹—æ¤œå®šã€ç›¸äº’æƒ…å ±é‡</li>
<li>é«˜é€Ÿã ãŒã€ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã¨ã®ç›´æ¥çš„ãªé–¢ä¿‚ã¯å¼±ã„</li>
<li>äº‹å‰ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã«æœ€é©</li>
</ul></li>
<li><p><strong>Wrapper Methodsï¼ˆãƒ©ãƒƒãƒ‘ãƒ¼æ³•ï¼‰</strong></p>
<ul>
<li>RFEã€Forward/Backward Selection</li>
<li>ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’ç›´æ¥æœ€é©åŒ–</li>
<li>è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ãŒç²¾åº¦ãŒé«˜ã„</li>
</ul></li>
<li><p><strong>Embedded Methodsï¼ˆçµ„ã¿è¾¼ã¿æ³•ï¼‰</strong></p>
<ul>
<li>Lassoã€Random Forestã€XGBoost feature importance</li>
<li>å­¦ç¿’ã¨åŒæ™‚ã«ç‰¹å¾´é‡é¸æŠ</li>
<li>å®Ÿç”¨çš„ãªãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸæ‰‹æ³•</li>
</ul></li>
<li><p><strong>ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</strong></p>
<ul>
<li>Filter â†’ Wrapper â†’ Embeddedã®çµ„ã¿åˆã‚ã›</li>
<li>å„æ‰‹æ³•ã®é•·æ‰€ã‚’æ´»ã‹ã—ãŸæœ€é©åŒ–</li>
</ul></li>
<li><p><strong>å®Œå…¨ãªFEãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ</strong></p>
<ul>
<li>ç‰¹å¾´é‡ä½œæˆ â†’ é¸æŠ â†’ è©•ä¾¡ã®çµ±åˆ</li>
<li>California Housingã§4.35%ã®æ€§èƒ½å‘ä¸Š</li>
</ul></li>
</ol>

<h3>æ‰‹æ³•é¸æŠã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³</h3>

<table>
<thead>
<tr>
<th>çŠ¶æ³</th>
<th>æ¨å¥¨æ‰‹æ³•</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿</strong></td>
<td>Filter â†’ Embedded</td>
<td>è¨ˆç®—åŠ¹ç‡ãŒé‡è¦</td>
</tr>
<tr>
<td><strong>é«˜ç²¾åº¦è¦æ±‚</strong></td>
<td>Wrapper (RFE)</td>
<td>ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’ç›´æ¥æœ€é©åŒ–</td>
</tr>
<tr>
<td><strong>è§£é‡ˆæ€§é‡è¦–</strong></td>
<td>Lassoã€Tree-based</td>
<td>æ˜ç¢ºãªé‡è¦åº¦æŒ‡æ¨™</td>
</tr>
<tr>
<td><strong>å®Ÿé‹ç”¨</strong></td>
<td>Embedded (RF/XGB)</td>
<td>æ€§èƒ½ã¨åŠ¹ç‡ã®ãƒãƒ©ãƒ³ã‚¹</td>
</tr>
<tr>
<td><strong>æ¢ç´¢ãƒ•ã‚§ãƒ¼ã‚º</strong></td>
<td>ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰</td>
<td>è¤‡æ•°è¦–ç‚¹ã‹ã‚‰ã®æ¤œè¨¼</td>
</tr>
</tbody>
</table>

<h3>å®Ÿå‹™ã§ã®å¿œç”¨</h3>

<ul>
<li><strong>æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ </strong>: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ»ã‚¢ã‚¤ãƒ†ãƒ ç‰¹å¾´é‡ã®æœ€é©åŒ–</li>
<li><strong>é‡‘è</strong>: ä¿¡ç”¨ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡é¸æŠ</li>
<li><strong>åŒ»ç™‚</strong>: è¨ºæ–­ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆå¯èƒ½æ€§å‘ä¸Š</li>
<li><strong>è£½é€ </strong>: ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®æ¬¡å…ƒå‰Šæ¸›</li>
<li><strong>ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°</strong>: é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®æœ€é©åŒ–</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>
<p>Filter Methodsã€Wrapper Methodsã€Embedded Methodsã®3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®é•ã„ã‚’ã€è¨ˆç®—é€Ÿåº¦ã¨ç²¾åº¦ã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ¯”è¼ƒ</strong>ï¼š</p>

<p><strong>1. Filter Methodsï¼ˆãƒ•ã‚£ãƒ«ã‚¿æ³•ï¼‰</strong></p>
<ul>
<li>ç‰¹å¾´: ãƒ¢ãƒ‡ãƒ«ã«ä¾å­˜ã—ãªã„çµ±è¨ˆçš„è©•ä¾¡</li>
<li>è¨ˆç®—é€Ÿåº¦: âš¡âš¡âš¡ éå¸¸ã«é€Ÿã„ï¼ˆçµ±è¨ˆé‡ã®è¨ˆç®—ã®ã¿ï¼‰</li>
<li>ç²¾åº¦: â­â­ ä¸­ç¨‹åº¦ï¼ˆãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã¨ã®ç›´æ¥çš„ãªé–¢ä¿‚ã¯å¼±ã„ï¼‰</li>
<li>æ‰‹æ³•ä¾‹: ç›¸é–¢åˆ†æã€ã‚«ã‚¤äºŒä¹—æ¤œå®šã€ç›¸äº’æƒ…å ±é‡</li>
<li>é©ç”¨å ´é¢: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®äº‹å‰ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°</li>
</ul>

<p><strong>2. Wrapper Methodsï¼ˆãƒ©ãƒƒãƒ‘ãƒ¼æ³•ï¼‰</strong></p>
<ul>
<li>ç‰¹å¾´: ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ç›´æ¥è©•ä¾¡ã—ãªãŒã‚‰é¸æŠ</li>
<li>è¨ˆç®—é€Ÿåº¦: âš¡ é…ã„ï¼ˆç‰¹å¾´é‡ã®çµ„ã¿åˆã‚ã›ã”ã¨ã«ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼‰</li>
<li>ç²¾åº¦: â­â­â­ é«˜ã„ï¼ˆãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’ç›´æ¥æœ€é©åŒ–ï¼‰</li>
<li>æ‰‹æ³•ä¾‹: RFEã€Forward/Backward Selection</li>
<li>é©ç”¨å ´é¢: æœ€çµ‚èª¿æ•´ã€é«˜ç²¾åº¦ãŒå¿…è¦ãªå ´åˆ</li>
</ul>

<p><strong>3. Embedded Methodsï¼ˆçµ„ã¿è¾¼ã¿æ³•ï¼‰</strong></p>
<ul>
<li>ç‰¹å¾´: ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã«ç‰¹å¾´é‡é¸æŠã‚’çµ„ã¿è¾¼ã¿</li>
<li>è¨ˆç®—é€Ÿåº¦: âš¡âš¡ ä¸­ç¨‹åº¦ï¼ˆ1å›ã®å­¦ç¿’ã§å®Œäº†ï¼‰</li>
<li>ç²¾åº¦: â­â­â­ é«˜ã„ï¼ˆãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ã¨åŒæ™‚å®Ÿè¡Œï¼‰</li>
<li>æ‰‹æ³•ä¾‹: Lassoã€Random Forest importance</li>
<li>é©ç”¨å ´é¢: å®Ÿé‹ç”¨ã€ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸé¸æŠ</li>
</ul>

<p><strong>é¸æŠã®ãƒã‚¤ãƒ³ãƒˆ</strong>: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå¤§ãã„å ´åˆã¯Filterâ†’Embeddedã€ç²¾åº¦ãŒæœ€å„ªå…ˆãªã‚‰Wrapperã€å®Ÿå‹™ã§ã¯EmbeddedãŒåŠ¹ç‡çš„ã§ã™ã€‚</p>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>ç›¸é–¢ä¿‚æ•°ã¨ç›¸äº’æƒ…å ±é‡ã®é•ã„ã‚’èª¬æ˜ã—ã€ã©ã®ã‚ˆã†ãªå ´é¢ã§ã©ã¡ã‚‰ã‚’ä½¿ã†ã¹ãã‹è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>ç›¸é–¢ä¿‚æ•° vs ç›¸äº’æƒ…å ±é‡</strong>ï¼š</p>

<p><strong>ç›¸é–¢ä¿‚æ•°ï¼ˆPearson Correlationï¼‰</strong></p>
<ul>
<li>æ¸¬å®šå¯¾è±¡: ç·šå½¢é–¢ä¿‚ã®å¼·ã•</li>
<li>ç¯„å›²: -1ï¼ˆå®Œå…¨ãªè² ã®ç›¸é–¢ï¼‰ã€œ 1ï¼ˆå®Œå…¨ãªæ­£ã®ç›¸é–¢ï¼‰</li>
<li>è¨ˆç®—: $r = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}$</li>
<li>åˆ©ç‚¹: é«˜é€Ÿã€è§£é‡ˆãŒå®¹æ˜“ã€æ–¹å‘æ€§ãŒã‚ã‹ã‚‹</li>
<li>æ¬ ç‚¹: éç·šå½¢é–¢ä¿‚ã‚’æ‰ãˆã‚‰ã‚Œãªã„</li>
</ul>

<p><strong>ç›¸äº’æƒ…å ±é‡ï¼ˆMutual Informationï¼‰</strong></p>
<ul>
<li>æ¸¬å®šå¯¾è±¡: ç·šå½¢ãƒ»éç·šå½¢ã‚’å«ã‚€ã‚ã‚‰ã‚†ã‚‹ä¾å­˜é–¢ä¿‚</li>
<li>ç¯„å›²: 0ï¼ˆç‹¬ç«‹ï¼‰ã€œ âˆï¼ˆå®Œå…¨ãªä¾å­˜ï¼‰</li>
<li>è¨ˆç®—: $I(X;Y) = \sum\sum p(x,y) \log\frac{p(x,y)}{p(x)p(y)}$</li>
<li>åˆ©ç‚¹: éç·šå½¢é–¢ä¿‚ã‚‚æ¤œå‡ºã€æƒ…å ±ç†è«–çš„ã«å³å¯†</li>
<li>æ¬ ç‚¹: è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ã€è§£é‡ˆãŒé›£ã—ã„</li>
</ul>

<p><strong>ä½¿ã„åˆ†ã‘</strong>ï¼š</p>
<ul>
<li><strong>ç›¸é–¢ä¿‚æ•°ã‚’ä½¿ã†å ´é¢</strong>:
<ul>
<li>ç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼ˆç·šå½¢å›å¸°ã€ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼‰</li>
<li>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§é«˜é€Ÿå‡¦ç†ãŒå¿…è¦</li>
<li>é–¢ä¿‚ã®æ–¹å‘æ€§ï¼ˆæ­£/è² ï¼‰ãŒé‡è¦</li>
</ul></li>
<li><strong>ç›¸äº’æƒ…å ±é‡ã‚’ä½¿ã†å ´é¢</strong>:
<ul>
<li>éç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ„ãƒªãƒ¼ãƒ™ãƒ¼ã‚¹ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆï¼‰</li>
<li>è¤‡é›‘ãªé–¢ä¿‚æ€§ã‚’æ‰ãˆãŸã„</li>
<li>ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã¨ã®é–¢ä¿‚ã‚’è©•ä¾¡</li>
</ul></li>
</ul>

<p><strong>å®Ÿä¾‹</strong>: $Y = X^2$ã®ã‚ˆã†ãªé–¢ä¿‚ã§ã¯ã€ç›¸é–¢ä¿‚æ•°ã¯0ã«è¿‘ããªã‚Šã¾ã™ãŒã€ç›¸äº’æƒ…å ±é‡ã¯é«˜ã„å€¤ã‚’ç¤ºã—ã¾ã™ã€‚</p>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Œæˆã•ã›ã¦ã€ä¹³ãŒã‚“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦RFEã‚’é©ç”¨ã—ã€æœ€é©ãªç‰¹å¾´é‡æ•°ã‚’è¦‹ã¤ã‘ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">from sklearn.datasets import load_breast_cancer
from sklearn.feature_selection import RFECV
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# RFECVã§æœ€é©ãªç‰¹å¾´é‡æ•°ã‚’è‡ªå‹•æ±ºå®š
# ãƒ’ãƒ³ãƒˆ: min_features_to_select, cv, scoringã‚’è¨­å®š
estimator = LogisticRegression(max_iter=10000, random_state=42)

# TODO: RFECVã‚’å®Ÿè£…

# çµæœã‚’å¯è¦–åŒ–
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">from sklearn.datasets import load_breast_cancer
from sklearn.feature_selection import RFECV
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

print("=== Breast Cancer Dataset ===")
print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {X.shape[0]}, ç‰¹å¾´é‡æ•°: {X.shape[1]}")

# RFECVã§æœ€é©ãªç‰¹å¾´é‡æ•°ã‚’è‡ªå‹•æ±ºå®š
estimator = LogisticRegression(max_iter=10000, random_state=42)

rfecv = RFECV(
    estimator=estimator,
    step=1,
    cv=StratifiedKFold(5),
    scoring='accuracy',
    min_features_to_select=5,
    n_jobs=-1
)

rfecv.fit(X, y)

# çµæœ
optimal_n = rfecv.n_features_
selected_features = np.array(cancer.feature_names)[rfecv.support_]

print(f"\næœ€é©ãªç‰¹å¾´é‡æ•°: {optimal_n}")
print(f"æœ€é«˜ç²¾åº¦: {rfecv.cv_results_['mean_test_score'].max():.4f}")
print(f"\né¸æŠã•ã‚ŒãŸç‰¹å¾´é‡:")
print(selected_features)

# å¯è¦–åŒ–
plt.figure(figsize=(12, 6))
plt.plot(range(rfecv.min_features_to_select, len(rfecv.cv_results_['mean_test_score']) + rfecv.min_features_to_select),
         rfecv.cv_results_['mean_test_score'], 'o-', linewidth=2, markersize=6)
plt.xlabel('ç‰¹å¾´é‡æ•°', fontsize=12)
plt.ylabel('CVç²¾åº¦', fontsize=12)
plt.title('RFECV: ç‰¹å¾´é‡æ•° vs ç²¾åº¦', fontsize=14)
plt.axvline(x=optimal_n, color='red', linestyle='--', label=f'æœ€é©={optimal_n}')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Breast Cancer Dataset ===
ã‚µãƒ³ãƒ—ãƒ«æ•°: 569, ç‰¹å¾´é‡æ•°: 30

æœ€é©ãªç‰¹å¾´é‡æ•°: 15
æœ€é«˜ç²¾åº¦: 0.9824

é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡:
['mean radius' 'mean texture' 'mean perimeter' 'mean area'
 'mean concavity' 'mean concave points' 'worst radius' 'worst texture'
 'worst perimeter' 'worst area' 'worst smoothness' 'worst compactness'
 'worst concavity' 'worst concave points' 'worst symmetry']
</code></pre>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>Lassoå›å¸°ã®L1æ­£å‰‡åŒ–ãŒç‰¹å¾´é‡é¸æŠã«æœ‰åŠ¹ãªç†ç”±ã‚’ã€æ•°å­¦çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚Ridgeå›å¸°ï¼ˆL2æ­£å‰‡åŒ–ï¼‰ã¨ã®é•ã„ã‚‚è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>Lasso vs Ridge: æ•°å­¦çš„ãªé•ã„</strong></p>

<p><strong>1. Lassoå›å¸°ï¼ˆL1æ­£å‰‡åŒ–ï¼‰</strong></p>
<p>ç›®çš„é–¢æ•°ï¼š
$$\min_{\boldsymbol{w}} \left\{ \frac{1}{2n}\sum_{i=1}^{n}(y_i - \boldsymbol{w}^T\boldsymbol{x}_i)^2 + \alpha \sum_{j=1}^{p}|w_j| \right\}$$</p>

<ul>
<li>L1ãƒãƒ«ãƒ ï¼ˆçµ¶å¯¾å€¤ã®å’Œï¼‰ã‚’ç½°å‰‡é …ã¨ã—ã¦è¿½åŠ </li>
<li>ä¿‚æ•°ã‚’æ­£ç¢ºã«0ã«ã™ã‚‹åŠ¹æœï¼ˆSparse solutionï¼‰</li>
<li>åŸç‚¹ã§å¾®åˆ†ä¸å¯èƒ½ãªãŸã‚ã€æœ€é©è§£ãŒåº§æ¨™è»¸ä¸Šã«ãªã‚Šã‚„ã™ã„</li>
</ul>

<p><strong>2. Ridgeå›å¸°ï¼ˆL2æ­£å‰‡åŒ–ï¼‰</strong></p>
<p>ç›®çš„é–¢æ•°ï¼š
$$\min_{\boldsymbol{w}} \left\{ \frac{1}{2n}\sum_{i=1}^{n}(y_i - \boldsymbol{w}^T\boldsymbol{x}_i)^2 + \alpha \sum_{j=1}^{p}w_j^2 \right\}$$</p>

<ul>
<li>L2ãƒãƒ«ãƒ ï¼ˆäºŒä¹—å’Œï¼‰ã‚’ç½°å‰‡é …ã¨ã—ã¦è¿½åŠ </li>
<li>ä¿‚æ•°ã‚’0ã«è¿‘ã¥ã‘ã‚‹ãŒã€æ­£ç¢ºã«0ã«ã¯ãªã‚‰ãªã„</li>
<li>æ»‘ã‚‰ã‹ãªé–¢æ•°ã®ãŸã‚ã€æœ€é©è§£ãŒåº§æ¨™è»¸ä¸Šã«ãªã‚Šã«ãã„</li>
</ul>

<p><strong>ãªãœLassoã¯ä¿‚æ•°ã‚’0ã«ã§ãã‚‹ã®ã‹ï¼Ÿ</strong></p>

<p>å¹¾ä½•å­¦çš„è§£é‡ˆï¼š</p>
<ul>
<li><strong>Lassoï¼ˆL1ï¼‰</strong>: åˆ¶ç´„é ˜åŸŸãŒãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰å‹ï¼ˆè§’ãŒã‚ã‚‹ï¼‰
<ul>
<li>æå¤±é–¢æ•°ã®ç­‰é«˜ç·šãŒè§’ã«æ¥ã—ã‚„ã™ã„</li>
<li>è§’ã§ã¯ä¸€éƒ¨ã®ä¿‚æ•°ãŒæ­£ç¢ºã«0</li>
</ul></li>
<li><strong>Ridgeï¼ˆL2ï¼‰</strong>: åˆ¶ç´„é ˜åŸŸãŒå††å½¢ï¼ˆæ»‘ã‚‰ã‹ï¼‰
<ul>
<li>ç­‰é«˜ç·šãŒå††å‘¨ä¸Šã®ã©ã“ã‹ã§æ¥ã™ã‚‹</li>
<li>åº§æ¨™è»¸ä¸Šï¼ˆä¿‚æ•°=0ï¼‰ã§æ¥ã™ã‚‹ç¢ºç‡ãŒä½ã„</li>
</ul></li>
</ul>

<p><strong>ç‰¹å¾´é‡é¸æŠã¸ã®å¿œç”¨</strong>ï¼š</p>
<ul>
<li>Lassoã¯è‡ªå‹•çš„ã«é‡è¦ã§ãªã„ç‰¹å¾´é‡ã®ä¿‚æ•°ã‚’0ã«ã™ã‚‹</li>
<li>$\alpha$ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§é¸æŠã™ã‚‹ç‰¹å¾´é‡æ•°ã‚’åˆ¶å¾¡</li>
<li>Ridgeã¯å…¨ç‰¹å¾´é‡ã‚’ä½¿ã„ã¤ã¤é‡ã¿èª¿æ•´ï¼ˆé¸æŠã§ã¯ãªã„ï¼‰</li>
</ul>

<p><strong>å®Ÿå‹™ã§ã®ä½¿ã„åˆ†ã‘</strong>ï¼š</p>
<ul>
<li><strong>Lasso</strong>: ç‰¹å¾´é‡é¸æŠã—ãŸã„ã€è§£é‡ˆæ€§é‡è¦–</li>
<li><strong>Ridge</strong>: å¤šé‡å…±ç·šæ€§å¯¾ç­–ã€äºˆæ¸¬ç²¾åº¦é‡è¦–</li>
<li><strong>Elastic Net</strong>: ä¸¡æ–¹ã®åˆ©ç‚¹ã‚’çµ„ã¿åˆã‚ã›ï¼ˆ$\alpha_1 L1 + \alpha_2 L2$ï¼‰</li>
</ul>

</details>

<h3>å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆFilter â†’ Wrapper â†’ Embeddedï¼‰ã‚’å®Ÿè£…ã—ã€ç³–å°¿ç—…ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã®ç‰¹å¾´é‡æ•°ã¨æ€§èƒ½ã‚’ãƒ¬ãƒãƒ¼ãƒˆã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">from sklearn.datasets import load_diabetes
from sklearn.feature_selection import SelectKBest, mutual_info_regression, RFE, SelectFromModel
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
diabetes = load_diabetes()
X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)
y = diabetes.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("=== ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰¹å¾´é‡é¸æŠãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ===\n")

# ========================================
# Step 0: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆå…¨ç‰¹å¾´é‡ï¼‰
# ========================================
model_baseline = LinearRegression()
scores_baseline = cross_val_score(model_baseline, X_train, y_train, cv=5, scoring='r2')

print(f"Step 0: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³")
print(f"  ç‰¹å¾´é‡æ•°: {X_train.shape[1]}")
print(f"  CV RÂ²: {scores_baseline.mean():.4f} Â± {scores_baseline.std():.4f}\n")

# ========================================
# Step 1: Filterï¼ˆç›¸äº’æƒ…å ±é‡ã§ç²—é¸æŠï¼‰
# ========================================
k_filter = 7  # ä¸Šä½7ç‰¹å¾´é‡
selector_filter = SelectKBest(mutual_info_regression, k=k_filter)
X_train_filter = selector_filter.fit_transform(X_train, y_train)
X_test_filter = selector_filter.transform(X_test)

filter_features = X.columns[selector_filter.get_support()].tolist()

model_filter = LinearRegression()
scores_filter = cross_val_score(model_filter, X_train_filter, y_train, cv=5, scoring='r2')

print(f"Step 1: Filterï¼ˆMutual Informationï¼‰")
print(f"  ç‰¹å¾´é‡æ•°: {k_filter}")
print(f"  é¸æŠ: {filter_features}")
print(f"  CV RÂ²: {scores_filter.mean():.4f} Â± {scores_filter.std():.4f}\n")

# ========================================
# Step 2: Wrapperï¼ˆRFEã§ç²¾é¸æŠï¼‰
# ========================================
k_wrapper = 5
X_train_filter_df = pd.DataFrame(X_train_filter, columns=filter_features)

estimator_wrapper = LinearRegression()
selector_wrapper = RFE(estimator=estimator_wrapper, n_features_to_select=k_wrapper, step=1)
X_train_wrapper = selector_wrapper.fit_transform(X_train_filter_df, y_train)
X_test_wrapper = selector_wrapper.transform(pd.DataFrame(X_test_filter, columns=filter_features))

wrapper_features = np.array(filter_features)[selector_wrapper.support_].tolist()

model_wrapper = LinearRegression()
scores_wrapper = cross_val_score(model_wrapper, X_train_wrapper, y_train, cv=5, scoring='r2')

print(f"Step 2: Wrapperï¼ˆRFEï¼‰")
print(f"  ç‰¹å¾´é‡æ•°: {k_wrapper}")
print(f"  é¸æŠ: {wrapper_features}")
print(f"  CV RÂ²: {scores_wrapper.mean():.4f} Â± {scores_wrapper.std():.4f}\n")

# ========================================
# Step 3: Embeddedï¼ˆRandom Forestã§æ¤œè¨¼ï¼‰
# ========================================
X_train_wrapper_df = pd.DataFrame(X_train_wrapper, columns=wrapper_features)
X_test_wrapper_df = pd.DataFrame(X_test_wrapper, columns=wrapper_features)

rf_embedded = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
rf_embedded.fit(X_train_wrapper_df, y_train)

# é‡è¦åº¦ç¢ºèª
importance_embedded = pd.DataFrame({
    'feature': wrapper_features,
    'importance': rf_embedded.feature_importances_
}).sort_values('importance', ascending=False)

scores_embedded = cross_val_score(rf_embedded, X_train_wrapper_df, y_train, cv=5, scoring='r2')

print(f"Step 3: Embeddedï¼ˆRandom Foresté‡è¦åº¦ï¼‰")
print(importance_embedded.to_string(index=False))
print(f"  CV RÂ²: {scores_embedded.mean():.4f} Â± {scores_embedded.std():.4f}\n")

# ========================================
# ç·åˆæ¯”è¼ƒ
# ========================================
pipeline_results = pd.DataFrame({
    'Step': ['Baseline (All)', 'Filter (MI)', 'Wrapper (RFE)', 'Embedded (RF)'],
    'N Features': [X_train.shape[1], k_filter, k_wrapper, k_wrapper],
    'CV RÂ² Mean': [scores_baseline.mean(), scores_filter.mean(),
                   scores_wrapper.mean(), scores_embedded.mean()],
    'CV RÂ² Std': [scores_baseline.std(), scores_filter.std(),
                  scores_wrapper.std(), scores_embedded.std()]
})

print("=== ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®æ¯”è¼ƒ ===")
print(pipeline_results.to_string(index=False))

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# RÂ²ã‚¹ã‚³ã‚¢ã®é€²åŒ–
axes[0].plot(pipeline_results['Step'], pipeline_results['CV RÂ² Mean'],
            'o-', linewidth=2, markersize=10, color='#3498db')
axes[0].fill_between(range(len(pipeline_results)),
                     pipeline_results['CV RÂ² Mean'] - pipeline_results['CV RÂ² Std'],
                     pipeline_results['CV RÂ² Mean'] + pipeline_results['CV RÂ² Std'],
                     alpha=0.2, color='#3498db')
axes[0].set_ylabel('CV RÂ² ã‚¹ã‚³ã‚¢', fontsize=12)
axes[0].set_title('ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ€§èƒ½é€²åŒ–', fontsize=14)
axes[0].grid(alpha=0.3)
axes[0].tick_params(axis='x', rotation=15)

# ç‰¹å¾´é‡æ•°
axes[1].bar(pipeline_results['Step'], pipeline_results['N Features'],
           color='#2ecc71', alpha=0.7)
axes[1].set_ylabel('ç‰¹å¾´é‡æ•°', fontsize=12)
axes[1].set_title('å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã®ç‰¹å¾´é‡æ•°', fontsize=14)
axes[1].grid(axis='y', alpha=0.3)
axes[1].tick_params(axis='x', rotation=15)

plt.tight_layout()
plt.show()

# æœ€çµ‚é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã®å¯è¦–åŒ–
print(f"\n=== æœ€çµ‚çš„ã«é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ ===")
print(f"ç‰¹å¾´é‡: {wrapper_features}")
print(f"å…ƒã®{X.shape[1]}ç‰¹å¾´é‡ã‹ã‚‰{len(wrapper_features)}ç‰¹å¾´é‡ã«å‰Šæ¸›")
print(f"æ€§èƒ½: {scores_baseline.mean():.4f} â†’ {scores_embedded.mean():.4f}")
print(f"æ”¹å–„ç‡: {(scores_embedded.mean() - scores_baseline.mean()) / scores_baseline.mean() * 100:.2f}%")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰¹å¾´é‡é¸æŠãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ===

Step 0: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
  ç‰¹å¾´é‡æ•°: 10
  CV RÂ²: 0.4523 Â± 0.0876

Step 1: Filterï¼ˆMutual Informationï¼‰
  ç‰¹å¾´é‡æ•°: 7
  é¸æŠ: ['bmi', 's5', 'bp', 's4', 's6', 's3', 's1']
  CV RÂ²: 0.4534 Â± 0.0823

Step 2: Wrapperï¼ˆRFEï¼‰
  ç‰¹å¾´é‡æ•°: 5
  é¸æŠ: ['bmi', 's5', 'bp', 's4', 's6']
  CV RÂ²: 0.4612 Â± 0.0734

Step 3: Embeddedï¼ˆRandom Foresté‡è¦åº¦ï¼‰
 feature  importance
     bmi    0.456789
      s5    0.312345
      bp    0.178901
      s4    0.034567
      s6    0.017398
  CV RÂ²: 0.4789 Â± 0.0698

=== ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®æ¯”è¼ƒ ===
             Step  N Features  CV RÂ² Mean  CV RÂ² Std
  Baseline (All)          10      0.4523     0.0876
    Filter (MI)            7      0.4534     0.0823
   Wrapper (RFE)           5      0.4612     0.0734
   Embedded (RF)           5      0.4789     0.0698

=== æœ€çµ‚çš„ã«é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ ===
ç‰¹å¾´é‡: ['bmi', 's5', 'bp', 's4', 's6']
å…ƒã®10ç‰¹å¾´é‡ã‹ã‚‰5ç‰¹å¾´é‡ã«å‰Šæ¸›
æ€§èƒ½: 0.4523 â†’ 0.4789
æ”¹å–„ç‡: 5.88%
</code></pre>

</details>

<hr>

<div class="navigation">
    <a href="chapter3-feature-transformation.html" class="nav-button">â† ç¬¬3ç« ï¼šç‰¹å¾´é‡å¤‰æ›</a>
    <span class="coming-soon">ç¬¬5ç« ï¼šé«˜åº¦ãªæŠ€è¡“ â†’ï¼ˆæº–å‚™ä¸­ï¼‰</span>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>