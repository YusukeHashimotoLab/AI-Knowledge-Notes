<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Transformerå…¥é–€ã‚·ãƒªãƒ¼ã‚º - Attentionæ©Ÿæ§‹ã‹ã‚‰å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¾ã§å®Œå…¨ã‚¬ã‚¤ãƒ‰">
    <title>Transformerå…¥é–€ã‚·ãƒªãƒ¼ã‚º v1.0 - AI Terakoya</title>

    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header .container {
            padding: 0 1.5rem;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }

        .meta span {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
        }

        /* Typography */
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            color: var(--primary-color);
        }

        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.6rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1.2rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--link-hover);
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1.2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Code blocks */
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border: 1px solid var(--border-color);
        }

        pre code {
            background: none;
            padding: 0;
            font-size: 0.9rem;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
            overflow-x: auto;
            display: block;
        }

        thead {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        tbody {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        th, td {
            padding: 0.8rem;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        th {
            background: var(--primary-color);
            color: white;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background: #f9f9f9;
        }

        /* Blockquotes */
        blockquote {
            border-left: 4px solid var(--secondary-color);
            padding-left: 1.5rem;
            margin: 1.5rem 0;
            font-style: italic;
            color: #666;
        }

        /* Images */
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1rem 0;
        }

        /* Mermaid diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
            background: white;
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        /* Details/Summary (for exercises) */
        details {
            margin: 1rem 0;
            padding: 1rem;
            background: #f8f9fa;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--primary-color);
            padding: 0.5rem;
        }

        summary:hover {
            color: var(--secondary-color);
        }

        /* Footer */
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        /* Navigation buttons */
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }

        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            h1 {
                font-size: 1.6rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.85rem;
            }

            table {
                font-size: 0.9rem;
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        // Mermaid.js Converter - Converts markdown-style mermaid code blocks to renderable divs
        document.addEventListener('DOMContentLoaded', function() {
            // Find all code blocks with class="language-mermaid"
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                // Create a new div with mermaid class
                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                // Replace the pre element with the new div
                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            // Re-initialize mermaid after conversion
            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({ startOnLoad: true, theme: 'default' });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/AI-Knowledge-Notes/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/AI-Knowledge-Notes/knowledge/jp/ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="/AI-Knowledge-Notes/knowledge/jp/ML/transformer-introduction/index.html">Transformer</a>
        </div>
    </nav>

        <header>
        <div class="container">
            <h1>âš¡ Transformerå…¥é–€ã‚·ãƒªãƒ¼ã‚º v1.0</h1>
            <p style="font-size: 1.1rem; margin-top: 0.5rem; opacity: 0.95;">Attentionæ©Ÿæ§‹ã‹ã‚‰å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¾ã§</p>
            <div class="meta">
                <span>ğŸ“– ç·å­¦ç¿’æ™‚é–“: 120-150åˆ†</span>
                <span>ğŸ“Š ãƒ¬ãƒ™ãƒ«: ä¸­ç´šã€œä¸Šç´š</span>
            </div>
        </div>
    </header>

    <main class="container">
        <p><strong>ç¾ä»£NLPã®åŸºç›¤ã¨ãªã‚‹Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’åŸºç¤ã‹ã‚‰ä½“ç³»çš„ã«ãƒã‚¹ã‚¿ãƒ¼</strong></p>

        <h2 id="overview">ã‚·ãƒªãƒ¼ã‚ºæ¦‚è¦</h2>
        <p>ã“ã®ã‚·ãƒªãƒ¼ã‚ºã¯ã€Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’åŸºç¤ã‹ã‚‰æ®µéšçš„ã«å­¦ã¹ã‚‹å…¨5ç« æ§‹æˆã®å®Ÿè·µçš„æ•™è‚²ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã§ã™ã€‚</p>

        <p><strong>Transformer</strong>ã¯ã€è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNLPï¼‰ã«ãŠã‘ã‚‹æœ€ã‚‚é©å‘½çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã‚ã‚Šã€BERTãƒ»GPTãƒ»ChatGPTãªã©ç¾ä»£ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®åŸºç›¤æŠ€è¡“ã§ã™ã€‚Self-Attentionæ©Ÿæ§‹ã«ã‚ˆã‚‹ä¸¦åˆ—å‡¦ç†å¯èƒ½ãªç³»åˆ—ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€Multi-Head Attentionã«ã‚ˆã‚‹å¤šæ§˜ãªé–¢ä¿‚æ€§ã®å­¦ç¿’ã€Positional Encodingã«ã‚ˆã‚‹ä½ç½®æƒ…å ±ã®çµ„ã¿è¾¼ã¿ã€ãã—ã¦äº‹å‰å­¦ç¿’ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚‹è»¢ç§»å­¦ç¿’ã‚’ç¿’å¾—ã™ã‚‹ã“ã¨ã§ã€æœ€å…ˆç«¯ã®NLPã‚·ã‚¹ãƒ†ãƒ ã‚’ç†è§£ãƒ»æ§‹ç¯‰ã§ãã¾ã™ã€‚Self-Attentionã¨Multi-Headã®ä»•çµ„ã¿ã‹ã‚‰ã€Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€BERTãƒ»GPTã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¾ã§ã€ä½“ç³»çš„ãªçŸ¥è­˜ã‚’æä¾›ã—ã¾ã™ã€‚</p>

        <p><strong>ç‰¹å¾´:</strong></p>
        <ul>
            <li>âœ… <strong>åŸºç¤ã‹ã‚‰æœ€å…ˆç«¯ã¾ã§</strong>: Attentionæ©Ÿæ§‹ã‹ã‚‰GPT-4ã®ã‚ˆã†ãªå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã¾ã§ä½“ç³»çš„ã«å­¦ç¿’</li>
            <li>âœ… <strong>å®Ÿè£…é‡è¦–</strong>: 40å€‹ä»¥ä¸Šã®å®Ÿè¡Œå¯èƒ½ãªPyTorchã‚³ãƒ¼ãƒ‰ä¾‹ã€å®Ÿè·µçš„ãªãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</li>
            <li>âœ… <strong>ç›´æ„Ÿçš„ç†è§£</strong>: Attentionå¯è¦–åŒ–ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³è§£ã§å‹•ä½œåŸç†ã‚’ç†è§£</li>
            <li>âœ… <strong>Hugging Faceå®Œå…¨æº–æ‹ </strong>: æ¥­ç•Œæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ãŸæœ€æ–°ã®å®Ÿè£…æ‰‹æ³•</li>
            <li>âœ… <strong>å®Ÿç”¨çš„å¿œç”¨</strong>: æ„Ÿæƒ…åˆ†æãƒ»è³ªå•å¿œç­”ãƒ»ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãªã©å®Ÿè·µçš„ãªã‚¿ã‚¹ã‚¯ã¸ã®é©ç”¨</li>
        </ul>

        <p><strong>ç·å­¦ç¿’æ™‚é–“</strong>: 120-150åˆ†ï¼ˆã‚³ãƒ¼ãƒ‰å®Ÿè¡Œã¨æ¼”ç¿’ã‚’å«ã‚€ï¼‰</p>

        <h2 id="learning">å­¦ç¿’ã®é€²ã‚æ–¹</h2>

        <h3>æ¨å¥¨å­¦ç¿’é †åº</h3>

        <div class="mermaid">
graph TD
    A[ç¬¬1ç« : Self-Attentionã¨Multi-Head Attention] --> B[ç¬¬2ç« : Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£]
    B --> C[ç¬¬3ç« : äº‹å‰å­¦ç¿’ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°]
    C --> D[ç¬¬4ç« : BERTãƒ»GPT]
    D --> E[ç¬¬5ç« : å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
</div>

        <p><strong>åˆå­¦è€…ã®æ–¹ï¼ˆTransformerã‚’ã¾ã£ãŸãçŸ¥ã‚‰ãªã„ï¼‰:</strong><br>
        - ç¬¬1ç«  â†’ ç¬¬2ç«  â†’ ç¬¬3ç«  â†’ ç¬¬4ç«  â†’ ç¬¬5ç« ï¼ˆå…¨ç« æ¨å¥¨ï¼‰<br>
        - æ‰€è¦æ™‚é–“: 120-150åˆ†</p>

        <p><strong>ä¸­ç´šè€…ã®æ–¹ï¼ˆRNN/Attentionã®çµŒé¨“ã‚ã‚Šï¼‰:</strong><br>
        - ç¬¬2ç«  â†’ ç¬¬3ç«  â†’ ç¬¬4ç«  â†’ ç¬¬5ç« <br>
        - æ‰€è¦æ™‚é–“: 90-110åˆ†</p>

        <p><strong>ç‰¹å®šãƒˆãƒ”ãƒƒã‚¯ã®å¼·åŒ–:</strong><br>
        - Attentionæ©Ÿæ§‹: ç¬¬1ç« ï¼ˆé›†ä¸­å­¦ç¿’ï¼‰<br>
        - BERT/GPT: ç¬¬4ç« ï¼ˆé›†ä¸­å­¦ç¿’ï¼‰<br>
        - LLM/ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ç¬¬5ç« ï¼ˆé›†ä¸­å­¦ç¿’ï¼‰<br>
        - æ‰€è¦æ™‚é–“: 25-30åˆ†/ç« </p>

        <h2 id="chapters">å„ç« ã®è©³ç´°</h2>

        <h3><a href="./chapter1-self-attention.html">ç¬¬1ç« ï¼šSelf-Attentionã¨Multi-Head Attention</a></h3>
        <p><strong>é›£æ˜“åº¦</strong>: ä¸­ç´š<br>
        <strong>èª­äº†æ™‚é–“</strong>: 25-30åˆ†<br>
        <strong>ã‚³ãƒ¼ãƒ‰ä¾‹</strong>: 8å€‹</p>

        <h4>å­¦ç¿’å†…å®¹</h4>
        <ol>
            <li><strong>Attentionã®å¾©ç¿’</strong> - RNNã«ãŠã‘ã‚‹Attentionæ©Ÿæ§‹ã€ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ</li>
            <li><strong>Self-Attentionã®åŸç†</strong> - Queryãƒ»Keyãƒ»Valueã€å†…ç©ã«ã‚ˆã‚‹é¡ä¼¼åº¦è¨ˆç®—</li>
            <li><strong>Scaled Dot-Product Attention</strong> - ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€Softmaxã€é‡ã¿ä»˜ãå’Œ</li>
            <li><strong>Multi-Head Attention</strong> - è¤‡æ•°ã®Attentionãƒ˜ãƒƒãƒ‰ã€ä¸¦åˆ—å‡¦ç†</li>
            <li><strong>å¯è¦–åŒ–ã¨å®Ÿè£…</strong> - PyTorchã«ã‚ˆã‚‹å®Ÿè£…ã€Attentionãƒãƒƒãƒ—ã®å¯è¦–åŒ–</li>
        </ol>

        <h4>å­¦ç¿’ç›®æ¨™</h4>
        <ul>
            <li>âœ… Self-Attentionã®å‹•ä½œåŸç†ã‚’ç†è§£ã™ã‚‹</li>
            <li>âœ… Queryãƒ»Keyãƒ»Valueã®å½¹å‰²ã‚’èª¬æ˜ã§ãã‚‹</li>
            <li>âœ… Scaled Dot-Product Attentionã‚’è¨ˆç®—ã§ãã‚‹</li>
            <li>âœ… Multi-Head Attentionã®åˆ©ç‚¹ã‚’ç†è§£ã™ã‚‹</li>
            <li>âœ… PyTorchã§Self-Attentionã‚’å®Ÿè£…ã§ãã‚‹</li>
        </ul>

        <p><strong><a href="./chapter1-self-attention.html">ç¬¬1ç« ã‚’èª­ã‚€ â†’</a></strong></p>

        <hr>

        <h3><a href="./chapter2-transformer-architecture.html">ç¬¬2ç« ï¼šTransformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</a></h3>
        <p><strong>é›£æ˜“åº¦</strong>: ä¸­ç´šã€œä¸Šç´š<br>
        <strong>èª­äº†æ™‚é–“</strong>: 25-30åˆ†<br>
        <strong>ã‚³ãƒ¼ãƒ‰ä¾‹</strong>: 8å€‹</p>

        <h4>å­¦ç¿’å†…å®¹</h4>
        <ol>
            <li><strong>Encoder-Decoderã®å…¨ä½“æ§‹é€ </strong> - 6å±¤ã®ã‚¹ã‚¿ãƒƒã‚¯ã€æ®‹å·®æ¥ç¶š</li>
            <li><strong>Positional Encoding</strong> - ä½ç½®æƒ…å ±ã®åŸ‹ã‚è¾¼ã¿ã€sin/cosé–¢æ•°</li>
            <li><strong>Feed-Forward Network</strong> - ä½ç½®ã”ã¨ã®å…¨çµåˆå±¤</li>
            <li><strong>Layer Normalization</strong> - æ­£è¦åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã€å­¦ç¿’ã®å®‰å®šåŒ–</li>
            <li><strong>Masked Self-Attention</strong> - Decoderã«ãŠã‘ã‚‹æœªæ¥ã®æƒ…å ±ã®ãƒã‚¹ã‚¯</li>
        </ol>

        <h4>å­¦ç¿’ç›®æ¨™</h4>
        <ul>
            <li>âœ… Transformerã®å…¨ä½“æ§‹é€ ã‚’ç†è§£ã™ã‚‹</li>
            <li>âœ… Positional Encodingã®å½¹å‰²ã‚’èª¬æ˜ã§ãã‚‹</li>
            <li>âœ… æ®‹å·®æ¥ç¶šã¨Layer Normã®åŠ¹æœã‚’ç†è§£ã™ã‚‹</li>
            <li>âœ… Masked Self-Attentionã®å¿…è¦æ€§ã‚’èª¬æ˜ã§ãã‚‹</li>
            <li>âœ… PyTorchã§Transformerã‚’å®Ÿè£…ã§ãã‚‹</li>
        </ul>

        <p><strong><a href="./chapter2-transformer-architecture.html">ç¬¬2ç« ã‚’èª­ã‚€ â†’</a></strong></p>

        <hr>

        <h3><a href="./chapter3-pretraining-finetuning.html">ç¬¬3ç« ï¼šäº‹å‰å­¦ç¿’ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</a></h3>
        <p><strong>é›£æ˜“åº¦</strong>: ä¸­ç´šã€œä¸Šç´š<br>
        <strong>èª­äº†æ™‚é–“</strong>: 25-30åˆ†<br>
        <strong>ã‚³ãƒ¼ãƒ‰ä¾‹</strong>: 8å€‹</p>

        <h4>å­¦ç¿’å†…å®¹</h4>
        <ol>
            <li><strong>è»¢ç§»å­¦ç¿’ã®æ¦‚å¿µ</strong> - äº‹å‰å­¦ç¿’ã®é‡è¦æ€§ã€ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œ</li>
            <li><strong>äº‹å‰å­¦ç¿’ã‚¿ã‚¹ã‚¯</strong> - Masked Language Modelã€Next Sentence Prediction</li>
            <li><strong>ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥</strong> - å…¨å±¤/éƒ¨åˆ†å±¤ã®æ›´æ–°ã€å­¦ç¿’ç‡è¨­å®š</li>
            <li><strong>ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡æ€§</strong> - å°‘é‡ãƒ‡ãƒ¼ã‚¿ã§ã®é«˜æ€§èƒ½ã€Few-shot Learning</li>
            <li><strong>Hugging Face Transformers</strong> - å®Ÿè·µçš„ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ä½¿ã„æ–¹</li>
        </ol>

        <h4>å­¦ç¿’ç›®æ¨™</h4>
        <ul>
            <li>âœ… è»¢ç§»å­¦ç¿’ã®åˆ©ç‚¹ã‚’ç†è§£ã™ã‚‹</li>
            <li>âœ… äº‹å‰å­¦ç¿’ã‚¿ã‚¹ã‚¯ã®è¨­è¨ˆç†å¿µã‚’èª¬æ˜ã§ãã‚‹</li>
            <li>âœ… é©åˆ‡ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã‚’é¸æŠã§ãã‚‹</li>
            <li>âœ… Hugging Faceãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ãˆã‚‹</li>
            <li>âœ… ç‹¬è‡ªã‚¿ã‚¹ã‚¯ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã‚‹</li>
        </ul>

        <p><strong><a href="./chapter3-pretraining-finetuning.html">ç¬¬3ç« ã‚’èª­ã‚€ â†’</a></strong></p>

        <hr>

        <h3><a href="./chapter4-bert-gpt.html">ç¬¬4ç« ï¼šBERTãƒ»GPT</a></h3>
        <p><strong>é›£æ˜“åº¦</strong>: ä¸Šç´š<br>
        <strong>èª­äº†æ™‚é–“</strong>: 25-30åˆ†<br>
        <strong>ã‚³ãƒ¼ãƒ‰ä¾‹</strong>: 8å€‹</p>

        <h4>å­¦ç¿’å†…å®¹</h4>
        <ol>
            <li><strong>BERTã®æ§‹é€ </strong> - Encoder-onlyã€åŒæ–¹å‘ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ</li>
            <li><strong>BERTã®äº‹å‰å­¦ç¿’</strong> - Masked LMã€Next Sentence Prediction</li>
            <li><strong>GPTã®æ§‹é€ </strong> - Decoder-onlyã€è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«</li>
            <li><strong>GPTã®äº‹å‰å­¦ç¿’</strong> - è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€æ¬¡å˜èªäºˆæ¸¬</li>
            <li><strong>BERTã¨GPTã®æ¯”è¼ƒ</strong> - ã‚¿ã‚¹ã‚¯ç‰¹æ€§ã€ä½¿ã„åˆ†ã‘ã®åŸºæº–</li>
        </ol>

        <h4>å­¦ç¿’ç›®æ¨™</h4>
        <ul>
            <li>âœ… BERTã®åŒæ–¹å‘æ€§ã‚’ç†è§£ã™ã‚‹</li>
            <li>âœ… Masked LMã®å­¦ç¿’ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’èª¬æ˜ã§ãã‚‹</li>
            <li>âœ… GPTã®è‡ªå·±å›å¸°æ€§ã‚’ç†è§£ã™ã‚‹</li>
            <li>âœ… BERTã¨GPTã‚’é©åˆ‡ã«ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹</li>
            <li>âœ… æ„Ÿæƒ…åˆ†æãƒ»è³ªå•å¿œç­”ã‚’å®Ÿè£…ã§ãã‚‹</li>
        </ul>

        <p><strong><a href="./chapter4-bert-gpt.html">ç¬¬4ç« ã‚’èª­ã‚€ â†’</a></strong></p>

        <hr>

        <h3><a href="./chapter5-large-language-models.html">ç¬¬5ç« ï¼šå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«</a></h3>
        <p><strong>é›£æ˜“åº¦</strong>: ä¸Šç´š<br>
        <strong>èª­äº†æ™‚é–“</strong>: 30-35åˆ†<br>
        <strong>ã‚³ãƒ¼ãƒ‰ä¾‹</strong>: 8å€‹</p>

        <h4>å­¦ç¿’å†…å®¹</h4>
        <ol>
            <li><strong>ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡</strong> - ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã€ãƒ‡ãƒ¼ã‚¿é‡ã€è¨ˆç®—é‡ã®é–¢ä¿‚</li>
            <li><strong>GPT-3ãƒ»GPT-4</strong> - è¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã€Emergent Abilities</li>
            <li><strong>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</strong> - Few-shotã€Chain-of-Thought</li>
            <li><strong>In-Context Learning</strong> - ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸è¦ã®å­¦ç¿’</li>
            <li><strong>æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰</strong> - Instruction Tuningã€RLHFã€ChatGPT</li>
        </ol>

        <h4>å­¦ç¿’ç›®æ¨™</h4>
        <ul>
            <li>âœ… ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã‚’ç†è§£ã™ã‚‹</li>
            <li>âœ… Emergent Abilitiesã®æ¦‚å¿µã‚’èª¬æ˜ã§ãã‚‹</li>
            <li>âœ… åŠ¹æœçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­è¨ˆã§ãã‚‹</li>
            <li>âœ… In-Context Learningã‚’æ´»ç”¨ã§ãã‚‹</li>
            <li>âœ… æœ€æ–°ã®LLMãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ç†è§£ã™ã‚‹</li>
        </ul>

        <p><strong><a href="./chapter5-large-language-models.html">ç¬¬5ç« ã‚’èª­ã‚€ â†’</a></strong></p>

        <hr>

        <h2 id="outcomes">å…¨ä½“ã®å­¦ç¿’æˆæœ</h2>

        <p>ã“ã®ã‚·ãƒªãƒ¼ã‚ºã‚’å®Œäº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚¹ã‚­ãƒ«ã¨çŸ¥è­˜ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>

        <h3>çŸ¥è­˜ãƒ¬ãƒ™ãƒ«ï¼ˆUnderstandingï¼‰</h3>
        <ul>
            <li>âœ… Self-Attentionã¨Multi-Head Attentionã®ä»•çµ„ã¿ã‚’èª¬æ˜ã§ãã‚‹</li>
            <li>âœ… Transformerã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
            <li>âœ… äº‹å‰å­¦ç¿’ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®æˆ¦ç•¥ã‚’èª¬æ˜ã§ãã‚‹</li>
            <li>âœ… BERTã¨GPTã®é•ã„ã¨ä½¿ã„åˆ†ã‘ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
            <li>âœ… å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®åŸç†ã¨æ´»ç”¨æ³•ã‚’èª¬æ˜ã§ãã‚‹</li>
        </ul>

        <h3>å®Ÿè·µã‚¹ã‚­ãƒ«ï¼ˆDoingï¼‰</h3>
        <ul>
            <li>âœ… PyTorchã§Transformerã‚’å®Ÿè£…ã§ãã‚‹</li>
            <li>âœ… Hugging Face Transformersã‚’ä½¿ã£ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã‚‹</li>
            <li>âœ… BERTã§æ„Ÿæƒ…åˆ†æãƒ»è³ªå•å¿œç­”ã‚’å®Ÿè£…ã§ãã‚‹</li>
            <li>âœ… GPTã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’å®Ÿè£…ã§ãã‚‹</li>
            <li>âœ… åŠ¹æœçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­è¨ˆã§ãã‚‹</li>
        </ul>

        <h3>å¿œç”¨åŠ›ï¼ˆApplyingï¼‰</h3>
        <ul>
            <li>âœ… æ–°ã—ã„NLPã‚¿ã‚¹ã‚¯ã«é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã§ãã‚‹</li>
            <li>âœ… äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹ç‡çš„ã«æ´»ç”¨ã§ãã‚‹</li>
            <li>âœ… æœ€æ–°ã®LLMæŠ€è¡“ã‚’å®Ÿå‹™ã«é©ç”¨ã§ãã‚‹</li>
            <li>âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã§æ€§èƒ½ã‚’æœ€é©åŒ–ã§ãã‚‹</li>
        </ul>

        <hr>

        <h2 id="prerequisites">å‰æçŸ¥è­˜</h2>

        <p>ã“ã®ã‚·ãƒªãƒ¼ã‚ºã‚’åŠ¹æœçš„ã«å­¦ç¿’ã™ã‚‹ãŸã‚ã«ã€ä»¥ä¸‹ã®çŸ¥è­˜ãŒã‚ã‚‹ã“ã¨ãŒæœ›ã¾ã—ã„ã§ã™ï¼š</p>

        <h3>å¿…é ˆï¼ˆMust Haveï¼‰</h3>
        <ul>
            <li>âœ… <strong>PythonåŸºç¤</strong>: å¤‰æ•°ã€é–¢æ•°ã€ã‚¯ãƒ©ã‚¹ã€ãƒ«ãƒ¼ãƒ—ã€æ¡ä»¶åˆ†å²</li>
            <li>âœ… <strong>NumPyåŸºç¤</strong>: é…åˆ—æ“ä½œã€ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã€åŸºæœ¬çš„ãªæ•°å­¦é–¢æ•°</li>
            <li>âœ… <strong>æ·±å±¤å­¦ç¿’ã®åŸºç¤</strong>: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€èª¤å·®é€†ä¼æ’­ã€å‹¾é…é™ä¸‹æ³•</li>
            <li>âœ… <strong>PyTorchåŸºç¤</strong>: ãƒ†ãƒ³ã‚½ãƒ«æ“ä½œã€nn.Moduleã€Datasetã¨DataLoader</li>
            <li>âœ… <strong>ç·šå½¢ä»£æ•°ã®åŸºç¤</strong>: è¡Œåˆ—æ¼”ç®—ã€å†…ç©ã€å½¢çŠ¶å¤‰æ›</li>
        </ul>

        <h3>æ¨å¥¨ï¼ˆNice to Haveï¼‰</h3>
        <ul>
            <li>ğŸ’¡ <strong>RNN/LSTM</strong>: å†å¸°å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€Attentionæ©Ÿæ§‹</li>
            <li>ğŸ’¡ <strong>è‡ªç„¶è¨€èªå‡¦ç†ã®åŸºç¤</strong>: ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã€èªå½™ã€åŸ‹ã‚è¾¼ã¿</li>
            <li>ğŸ’¡ <strong>æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </strong>: Adamã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã€Warmup</li>
            <li>ğŸ’¡ <strong>GPUç’°å¢ƒ</strong>: CUDAã®åŸºæœ¬çš„ãªç†è§£</li>
        </ul>

        <p><strong>æ¨å¥¨ã•ã‚Œã‚‹å‰ã®å­¦ç¿’</strong>:</p>
        <ul>
            <li>ğŸ“š <a href="../deep-learning-basics/">æ·±å±¤å­¦ç¿’ã®åŸºç¤ã‚·ãƒªãƒ¼ã‚º</a> - ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åŸºæœ¬</li>
            <li>ğŸ“š <a href="../pytorch-introduction/">PyTorchå…¥é–€ã‚·ãƒªãƒ¼ã‚º</a> - PyTorchã®åŸºæœ¬æ“ä½œ</li>
            <li>ğŸ“š <a href="../rnn-introduction/">RNNå…¥é–€ã‚·ãƒªãƒ¼ã‚º</a> - å†å¸°å‹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨Attention</li>
        </ul>

        <hr>

        <h2 id="tech">ä½¿ç”¨æŠ€è¡“ã¨ãƒ„ãƒ¼ãƒ«</h2>

        <h3>ä¸»è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª</h3>
        <ul>
            <li><strong>PyTorch 2.0+</strong> - æ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯</li>
            <li><strong>transformers 4.30+</strong> - Hugging Face Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒª</li>
            <li><strong>tokenizers 0.13+</strong> - é«˜é€Ÿãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼</li>
            <li><strong>datasets 2.12+</strong> - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª</li>
            <li><strong>NumPy 1.24+</strong> - æ•°å€¤è¨ˆç®—</li>
            <li><strong>Matplotlib 3.7+</strong> - å¯è¦–åŒ–</li>
            <li><strong>scikit-learn 1.3+</strong> - ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¨è©•ä¾¡æŒ‡æ¨™</li>
        </ul>

        <h3>é–‹ç™ºç’°å¢ƒ</h3>
        <ul>
            <li><strong>Python 3.8+</strong> - ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èª</li>
            <li><strong>Jupyter Notebook / Lab</strong> - å¯¾è©±çš„é–‹ç™ºç’°å¢ƒ</li>
            <li><strong>Google Colab</strong> - GPUç’°å¢ƒï¼ˆç„¡æ–™ã§åˆ©ç”¨å¯èƒ½ï¼‰</li>
            <li><strong>CUDA 11.8+ / cuDNN</strong> - GPUé«˜é€ŸåŒ–ï¼ˆæ¨å¥¨ï¼‰</li>
        </ul>

        <h3>ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</h3>
        <ul>
            <li><strong>GLUE</strong> - è‡ªç„¶è¨€èªç†è§£ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯</li>
            <li><strong>SQuAD</strong> - è³ªå•å¿œç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</li>
            <li><strong>WikiText</strong> - è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</li>
            <li><strong>IMDb</strong> - æ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</li>
        </ul>

        <hr>

        <h2 id="start">ã•ã‚ã€å§‹ã‚ã¾ã—ã‚‡ã†ï¼</h2>
        <p>æº–å‚™ã¯ã§ãã¾ã—ãŸã‹ï¼Ÿ ç¬¬1ç« ã‹ã‚‰å§‹ã‚ã¦ã€Transformerã®æŠ€è¡“ã‚’ç¿’å¾—ã—ã¾ã—ã‚‡ã†ï¼</p>

        <p><strong><a href="./chapter1-self-attention.html">ç¬¬1ç« : Self-Attentionã¨Multi-Head Attention â†’</a></strong></p>

        <hr>

        <h2 id="next">æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h2>

        <p>ã“ã®ã‚·ãƒªãƒ¼ã‚ºã‚’å®Œäº†ã—ãŸå¾Œã€ä»¥ä¸‹ã®ãƒˆãƒ”ãƒƒã‚¯ã¸é€²ã‚€ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ï¼š</p>

        <h3>æ·±æ˜ã‚Šå­¦ç¿’</h3>
        <ul>
            <li>ğŸ“š <strong>Vision Transformer (ViT)</strong>: ç”»åƒå‡¦ç†ã¸ã®Transformeré©ç”¨</li>
            <li>ğŸ“š <strong>ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å­¦ç¿’</strong>: CLIPã€Flamingoã€GPT-4V</li>
            <li>ğŸ“š <strong>åŠ¹ç‡åŒ–æŠ€è¡“</strong>: ãƒ¢ãƒ‡ãƒ«åœ§ç¸®ã€è’¸ç•™ã€é‡å­åŒ–</li>
            <li>ğŸ“š <strong>å¼·åŒ–å­¦ç¿’ã¨ã®çµ±åˆ</strong>: RLHFã€Constitutional AI</li>
        </ul>

        <h3>é–¢é€£ã‚·ãƒªãƒ¼ã‚º</h3>
        <ul>
            <li>ğŸ¯ <a href="../nlp-advanced/">è‡ªç„¶è¨€èªå‡¦ç†å¿œç”¨</a> - æ„Ÿæƒ…åˆ†æã€è³ªå•å¿œç­”ã€è¦ç´„</li>
            <li>ğŸ¯ <a href="../llm-applications/">LLMå¿œç”¨é–‹ç™º</a> - RAGã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ãƒ„ãƒ¼ãƒ«åˆ©ç”¨</li>
            <li>ğŸ¯ <a href="../prompt-engineering/">ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</a> - å®Ÿè·µçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆ</li>
        </ul>

        <h3>å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ</h3>
        <ul>
            <li>ğŸš€ æ„Ÿæƒ…åˆ†æAPI - BERTã«ã‚ˆã‚‹ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ„Ÿæƒ…åˆ†æ</li>
            <li>ğŸš€ è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ  - æ–‡æ›¸æ¤œç´¢ã¨å›ç­”ç”Ÿæˆ</li>
            <li>ğŸš€ ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ - GPTãƒ™ãƒ¼ã‚¹ã®å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ </li>
            <li>ğŸš€ ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ãƒ„ãƒ¼ãƒ« - ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®è‡ªå‹•è¦ç´„</li>
        </ul>

        <hr>

        <p><strong>æ›´æ–°å±¥æ­´</strong></p>
        <ul>
            <li><strong>2025-10-21</strong>: v1.0 åˆç‰ˆå…¬é–‹</li>
        </ul>

        <hr>

        <p><strong>ã‚ãªãŸã®Transformerå­¦ç¿’ã®æ—…ã¯ã“ã“ã‹ã‚‰å§‹ã¾ã‚Šã¾ã™ï¼</strong></p>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <div class="container">
            <p>&copy; 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
