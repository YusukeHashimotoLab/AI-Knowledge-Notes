<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬1ç« ï¼šSelf-Attentionã¨Multi-Head Attention - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/transformer-introduction/index.html">Transformer</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 1</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬1ç« ï¼šSelf-Attentionã¨Multi-Head Attention</h1>
            <p class="subtitle">Transformerã®å¿ƒè‡“éƒ¨ - æ³¨æ„æ©Ÿæ§‹ã®é©å‘½çš„ãªãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ç†è§£ã™ã‚‹</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 30-35åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 12å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… RNNã®é™ç•Œã¨Attentionæ©Ÿæ§‹ã®å¿…è¦æ€§ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Queryã€Keyã€Valueã®æ¦‚å¿µã¨å½¹å‰²ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>âœ… Scaled Dot-Product Attentionã®æ•°å­¦çš„å®šç¾©ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Self-Attentionã®è¨ˆç®—ãƒ—ãƒ­ã‚»ã‚¹ã‚’è¿½è·¡ã§ãã‚‹</li>
<li>âœ… Multi-Head Attentionã®ä»•çµ„ã¿ã¨åˆ©ç‚¹ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Position Encodingã®é‡è¦æ€§ã¨å®Ÿè£…æ–¹æ³•ã‚’ç¿’å¾—ã™ã‚‹</li>
<li>âœ… PyTorchã§Self-Attentionã‚’å®Ÿè£…ã—ã€ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã«å¿œç”¨ã§ãã‚‹</li>
</ul>

<hr>

<h2>1.1 RNNã®é™ç•Œã¨Attentionã®å¾©ç¿’</h2>

<h3>RNNã®æ ¹æœ¬çš„ãªå•é¡Œ</h3>

<p><strong>Recurrent Neural Network (RNN)</strong>ã¯æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã«é©å‘½ã‚’ã‚‚ãŸã‚‰ã—ã¾ã—ãŸãŒã€ä»¥ä¸‹ã®æœ¬è³ªçš„ãªé™ç•ŒãŒã‚ã‚Šã¾ã™ï¼š</p>

<blockquote>
<p>ã€ŒRNNã¯éå»ã®æƒ…å ±ã‚’éš ã‚ŒçŠ¶æ…‹ã«åœ§ç¸®ã™ã‚‹ãŒã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§ã¯é‡è¦ãªæƒ…å ±ãŒå¤±ã‚ã‚Œã‚‹ã€‚ã¾ãŸã€é€æ¬¡çš„ãªå‡¦ç†ã«ã‚ˆã‚Šä¸¦åˆ—åŒ–ãŒå›°é›£ã§ã‚ã‚‹ã€‚ã€</p>
</blockquote>

<h4>RNNã®3ã¤ã®é™ç•Œ</h4>

<table>
<thead>
<tr>
<th>å•é¡Œç‚¹</th>
<th>èª¬æ˜</th>
<th>å½±éŸ¿</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>é•·æœŸä¾å­˜æ€§</strong></td>
<td>å‹¾é…æ¶ˆå¤±ã«ã‚ˆã‚Šé ã„éå»ã®æƒ…å ±ãŒå¤±ã‚ã‚Œã‚‹</td>
<td>é•·æ–‡ã®æ–‡è„ˆã‚’æ‰ãˆã‚‰ã‚Œãªã„</td>
</tr>
<tr>
<td><strong>é€æ¬¡å‡¦ç†</strong></td>
<td>æ™‚åˆ»t-1ã®è¨ˆç®—å®Œäº†å¾Œã«tã‚’è¨ˆç®—</td>
<td>ä¸¦åˆ—åŒ–ä¸å¯èƒ½ã€å­¦ç¿’ãŒé…ã„</td>
</tr>
<tr>
<td><strong>å›ºå®šé•·ãƒ™ã‚¯ãƒˆãƒ«</strong></td>
<td>å…¨æƒ…å ±ã‚’å˜ä¸€ã®éš ã‚ŒçŠ¶æ…‹ã«åœ§ç¸®</td>
<td>æƒ…å ±ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">import torch
import torch.nn as nn
import time

# RNNã®é€æ¬¡å‡¦ç†ã®å•é¡Œã‚’ç¤ºã™ãƒ‡ãƒ¢
class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(SimpleRNN, self).__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)

    def forward(self, x):
        output, hidden = self.rnn(x)
        return output

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
batch_size = 32
seq_length = 100
input_size = 512
hidden_size = 512

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿
rnn = SimpleRNN(input_size, hidden_size)
x = torch.randn(batch_size, seq_length, input_size)

print("=== RNNã®é€æ¬¡å‡¦ç†ã®å•é¡Œ ===\n")

# å‡¦ç†æ™‚é–“ã®æ¸¬å®š
start_time = time.time()
with torch.no_grad():
    output = rnn(x)
rnn_time = time.time() - start_time

print(f"å…¥åŠ›ã‚µã‚¤ã‚º: {x.shape}")
print(f"  [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, ç‰¹å¾´é‡] = [{batch_size}, {seq_length}, {input_size}]")
print(f"\nå‡¦ç†æ™‚é–“: {rnn_time*1000:.2f}ms")
print(f"\nå•é¡Œç‚¹:")
print(f"  1. æ™‚åˆ»0â†’1â†’2â†’...â†’99ã¨é€æ¬¡çš„ã«å‡¦ç†")
print(f"  2. å„æ™‚åˆ»ã¯å‰ã®æ™‚åˆ»ã®å®Œäº†ã‚’å¾…ã¤å¿…è¦ãŒã‚ã‚‹")
print(f"  3. GPUã®ä¸¦åˆ—å‡¦ç†èƒ½åŠ›ã‚’ååˆ†ã«æ´»ç”¨ã§ããªã„")
print(f"  4. ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ãŒå¢—ãˆã‚‹ã¨ç·šå½¢çš„ã«é…ããªã‚‹")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== RNNã®é€æ¬¡å‡¦ç†ã®å•é¡Œ ===

å…¥åŠ›ã‚µã‚¤ã‚º: torch.Size([32, 100, 512])
  [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, ç‰¹å¾´é‡] = [32, 100, 512]

å‡¦ç†æ™‚é–“: 45.23ms

å•é¡Œç‚¹:
  1. æ™‚åˆ»0â†’1â†’2â†’...â†’99ã¨é€æ¬¡çš„ã«å‡¦ç†
  2. å„æ™‚åˆ»ã¯å‰ã®æ™‚åˆ»ã®å®Œäº†ã‚’å¾…ã¤å¿…è¦ãŒã‚ã‚‹
  3. GPUã®ä¸¦åˆ—å‡¦ç†èƒ½åŠ›ã‚’ååˆ†ã«æ´»ç”¨ã§ããªã„
  4. ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ãŒå¢—ãˆã‚‹ã¨ç·šå½¢çš„ã«é…ããªã‚‹
</code></pre>

<h3>Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®ç™»å ´</h3>

<p><strong>Attentionæ©Ÿæ§‹</strong>ã¯ã€Seq2Seqãƒ¢ãƒ‡ãƒ«ã®æ”¹è‰¯ã¨ã—ã¦2014å¹´ã«ææ¡ˆã•ã‚Œã¾ã—ãŸï¼ˆBahdanau et al.ï¼‰ã€‚ãã®å¾Œã€2017å¹´ã®<strong>Transformer</strong>ï¼ˆVaswani et al.ï¼‰ã«ã‚ˆã‚Šã€RNNã‚’å®Œå…¨ã«ç½®ãæ›ãˆã‚‹é©å‘½ãŒèµ·ã“ã‚Šã¾ã—ãŸã€‚</p>

<h4>å¾“æ¥ã®Attentionã¨Self-Attentionã®é•ã„</h4>

<table>
<thead>
<tr>
<th>ç¨®é¡</th>
<th>ç”¨é€”</th>
<th>ç‰¹å¾´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Encoder-Decoder Attention</strong></td>
<td>Seq2Seqç¿»è¨³</td>
<td>DecoderãŒEncoderã®å…¨æ™‚åˆ»ã«æ³¨ç›®</td>
</tr>
<tr>
<td><strong>Self-Attention</strong></td>
<td>æ–‡è„ˆç†è§£</td>
<td>åŒä¸€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®å˜èªé–“ã®é–¢ä¿‚ã‚’å­¦ç¿’</td>
</tr>
<tr>
<td><strong>Multi-Head Attention</strong></td>
<td>Transformer</td>
<td>è¤‡æ•°ã®è¦–ç‚¹ã‹ã‚‰åŒæ™‚ã«æ³¨ç›®</td>
</tr>
</tbody>
</table>

<div class="mermaid">
graph LR
    subgraph "å¾“æ¥ã®Seq2Seq + Attention"
    A1[Encoder] --> B1[å›ºå®šé•·ãƒ™ã‚¯ãƒˆãƒ«]
    B1 --> C1[Decoder]
    A1 -.Attention.-> C1
    end

    subgraph "Self-Attentionï¼ˆTransformerï¼‰"
    A2[å…¨å˜èª] --> B2[ä¸¦åˆ—å‡¦ç†]
    B2 --> C2[æ–‡è„ˆè¡¨ç¾]
    B2 -.Self-Attention.-> B2
    end

    style A1 fill:#e3f2fd
    style B1 fill:#fff3e0
    style C1 fill:#ffebee
    style A2 fill:#e3f2fd
    style B2 fill:#fff3e0
    style C2 fill:#ffebee
</div>

<blockquote>
<p><strong>é‡è¦</strong>: Self-Attentionã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®å…¨ã¦ã®ä½ç½®ã‚’ä¸¦åˆ—ã«å‡¦ç†ã§ãã€ä»»æ„ã®è·é›¢ã®ä¾å­˜é–¢ä¿‚ã‚’ç›´æ¥æ‰ãˆã‚‰ã‚Œã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>1.2 Self-Attentionã®åŸºç¤</h2>

<h3>Queryã€Keyã€Valueã®æ¦‚å¿µ</h3>

<p>Self-Attentionã®æ ¸å¿ƒã¯ã€å„å˜èªã‚’<strong>Queryï¼ˆè³ªå•ï¼‰</strong>ã€<strong>Keyï¼ˆéµï¼‰</strong>ã€<strong>Valueï¼ˆå€¤ï¼‰</strong>ã®3ã¤ã®è¡¨ç¾ã«å¤‰æ›ã™ã‚‹ã“ã¨ã§ã™ã€‚</p>

<h4>ç›´æ„Ÿçš„ãªç†è§£</h4>

<p>æƒ…å ±æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã«ä¾‹ãˆã‚‹ã¨ï¼š</p>

<ul>
<li><strong>Queryï¼ˆQï¼‰</strong>: ã€Œä½•ã‚’æ¢ã—ã¦ã„ã‚‹ã‹ã€ï¼ˆæ¤œç´¢ã‚¯ã‚¨ãƒªï¼‰</li>
<li><strong>Keyï¼ˆKï¼‰</strong>: ã€Œä½•ã‚’æä¾›ã§ãã‚‹ã‹ã€ï¼ˆæ–‡æ›¸ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰</li>
<li><strong>Valueï¼ˆVï¼‰</strong>: ã€Œå®Ÿéš›ã®å†…å®¹ã€ï¼ˆæ–‡æ›¸ã®æœ¬æ–‡ï¼‰</li>
</ul>

<blockquote>
<p>ã€Œå„å˜èªã®QueryãŒã€ä»–ã®å…¨ã¦ã®å˜èªã®Keyã¨æ¯”è¼ƒã•ã‚Œã€é–¢é€£åº¦ï¼ˆAttentioné‡ã¿ï¼‰ãŒè¨ˆç®—ã•ã‚Œã‚‹ã€‚ãã®é‡ã¿ã§Valueã‚’é‡ã¿ä»˜ã‘å¹³å‡ã—ã€æ–°ã—ã„è¡¨ç¾ã‚’å¾—ã‚‹ã€‚ã€</p>
</blockquote>

<h4>å…·ä½“ä¾‹ï¼šæ–‡ç« å†…ã®å‚ç…§è§£æ±º</h4>

<p>æ–‡ç« : <strong>"The cat sat on the mat because it was comfortable"</strong></p>

<p>å˜èªã€Œitã€ã®Queryã¯ï¼š</p>
<ul>
<li>ã€Œtheã€ã®Key â†’ é–¢é€£åº¦: ä½</li>
<li>ã€Œcatã€ã®Key â†’ é–¢é€£åº¦: é«˜ï¼ˆä¸»èªï¼‰</li>
<li>ã€Œmatã€ã®Key â†’ é–¢é€£åº¦: ä¸­ï¼ˆå ´æ‰€ï¼‰</li>
<li>ã€Œcomfortableã€ã®Key â†’ é–¢é€£åº¦: ä½</li>
</ul>

<p>çµæœã¨ã—ã¦ã€ã€Œitã€ã®æ–°ã—ã„è¡¨ç¾ã¯ã€Œcatã€ã¨ã€Œmatã€ã®Valueã‚’ä¸»ã«åæ˜ ã—ã¾ã™ã€‚</p>

<h3>Scaled Dot-Product Attentionã®æ•°å¼</h3>

<p>Self-Attentionã®è¨ˆç®—ã¯ä»¥ä¸‹ã®æ•°å¼ã§å®šç¾©ã•ã‚Œã¾ã™ï¼š</p>

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$Q \in \mathbb{R}^{n \times d_k}$: Queryè¡Œåˆ—ï¼ˆnå€‹ã®å˜èªã€å„$d_k$æ¬¡å…ƒï¼‰</li>
<li>$K \in \mathbb{R}^{n \times d_k}$: Keyè¡Œåˆ—</li>
<li>$V \in \mathbb{R}^{n \times d_v}$: Valueè¡Œåˆ—</li>
<li>$d_k$: Key/Queryã®æ¬¡å…ƒ</li>
<li>$\sqrt{d_k}$: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ï¼ˆå‹¾é…ã®å®‰å®šåŒ–ï¼‰</li>
</ul>

<h4>è¨ˆç®—ã‚¹ãƒ†ãƒƒãƒ—ã®è©³ç´°</h4>

<p><strong>ã‚¹ãƒ†ãƒƒãƒ—1: ã‚¹ã‚³ã‚¢è¨ˆç®—</strong></p>

$$
S = QK^T \in \mathbb{R}^{n \times n}
$$

<p>å„è¦ç´  $S_{ij}$ ã¯ã€å˜èªiã®Queryã¨å˜èªjã®Keyã®å†…ç©ã§ã™ã€‚</p>

<p><strong>ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°</strong></p>

$$
S_{\text{scaled}} = \frac{S}{\sqrt{d_k}}
$$

<p>$d_k$ãŒå¤§ãã„ã¨ã‚¹ã‚³ã‚¢ã®åˆ†æ•£ãŒå¤§ãããªã‚Šã€softmaxã®å‹¾é…ãŒæ¶ˆå¤±ã—ã¾ã™ã€‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã§é˜²ãã¾ã™ã€‚</p>

<p><strong>ã‚¹ãƒ†ãƒƒãƒ—3: Attentioné‡ã¿ã®è¨ˆç®—</strong></p>

$$
A = \text{softmax}(S_{\text{scaled}}) \in \mathbb{R}^{n \times n}
$$

<p>å„è¡Œã¯ç¢ºç‡åˆ†å¸ƒï¼ˆåˆè¨ˆ=1ï¼‰ã§ã€å˜èªiãŒã©ã®å˜èªã«æ³¨ç›®ã™ã‚‹ã‹ã‚’è¡¨ã—ã¾ã™ã€‚</p>

<p><strong>ã‚¹ãƒ†ãƒƒãƒ—4: é‡ã¿ä»˜ãå’Œ</strong></p>

$$
\text{Output} = AV \in \mathbb{R}^{n \times d_v}
$$

<p>Attentioné‡ã¿ã§Valueã‚’åŠ é‡å¹³å‡ã—ã€æ–°ã—ã„è¡¨ç¾ã‚’å¾—ã¾ã™ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn.functional as F
import numpy as np

def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Scaled Dot-Product Attentionã®å®Ÿè£…

    Parameters:
    -----------
    Q : torch.Tensor (batch, n_queries, d_k)
        Queryè¡Œåˆ—
    K : torch.Tensor (batch, n_keys, d_k)
        Keyè¡Œåˆ—
    V : torch.Tensor (batch, n_values, d_v)
        Valueè¡Œåˆ—
    mask : torch.Tensor (optional)
        ãƒã‚¹ã‚¯ï¼ˆ0ã®ä½ç½®ã¯ç„¡è¦–ï¼‰

    Returns:
    --------
    output : torch.Tensor (batch, n_queries, d_v)
        Attentionå‡ºåŠ›
    attention_weights : torch.Tensor (batch, n_queries, n_keys)
        Attentioné‡ã¿
    """
    # ã‚¹ãƒ†ãƒƒãƒ—1: ã‚¹ã‚³ã‚¢è¨ˆç®— Q @ K^T
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch, n_q, n_k)

    # ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    scores = scores / np.sqrt(d_k)

    # ãƒã‚¹ã‚¯ã®é©ç”¨ï¼ˆå¿…è¦ãªå ´åˆï¼‰
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)

    # ã‚¹ãƒ†ãƒƒãƒ—3: Softmaxã§æ­£è¦åŒ–
    attention_weights = F.softmax(scores, dim=-1)  # (batch, n_q, n_k)

    # ã‚¹ãƒ†ãƒƒãƒ—4: Valueã¨ã®é‡ã¿ä»˜ãå’Œ
    output = torch.matmul(attention_weights, V)  # (batch, n_q, d_v)

    return output, attention_weights


# ãƒ‡ãƒ¢: ç°¡å˜ãªä¾‹
batch_size = 2
seq_length = 4
d_k = 8
d_v = 8

# ãƒ€ãƒŸãƒ¼ã®Q, K, V
Q = torch.randn(batch_size, seq_length, d_k)
K = torch.randn(batch_size, seq_length, d_k)
V = torch.randn(batch_size, seq_length, d_v)

# Attentionã®è¨ˆç®—
output, attn_weights = scaled_dot_product_attention(Q, K, V)

print("=== Scaled Dot-Product Attention ===\n")
print(f"å…¥åŠ›å½¢çŠ¶:")
print(f"  Q: {Q.shape}")
print(f"  K: {K.shape}")
print(f"  V: {V.shape}")

print(f"\nå‡ºåŠ›å½¢çŠ¶:")
print(f"  Output: {output.shape}")
print(f"  Attention Weights: {attn_weights.shape}")

print(f"\nAttentioné‡ã¿ã®æ€§è³ª:")
print(f"  å„è¡Œã®åˆè¨ˆï¼ˆç¢ºç‡åˆ†å¸ƒï¼‰: {attn_weights[0, 0, :].sum().item():.4f}")
print(f"  æœ€å°å€¤: {attn_weights.min().item():.4f}")
print(f"  æœ€å¤§å€¤: {attn_weights.max().item():.4f}")

# æœ€åˆã®ãƒãƒƒãƒã®æœ€åˆã®å˜èªã®Attentionåˆ†å¸ƒã‚’è¡¨ç¤º
print(f"\nå˜èª0ã®Attentionåˆ†å¸ƒ:")
print(f"  å˜èª0ã¸ã®æ³¨ç›®: {attn_weights[0, 0, 0].item():.4f}")
print(f"  å˜èª1ã¸ã®æ³¨ç›®: {attn_weights[0, 0, 1].item():.4f}")
print(f"  å˜èª2ã¸ã®æ³¨ç›®: {attn_weights[0, 0, 2].item():.4f}")
print(f"  å˜èª3ã¸ã®æ³¨ç›®: {attn_weights[0, 0, 3].item():.4f}")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== Scaled Dot-Product Attention ===

å…¥åŠ›å½¢çŠ¶:
  Q: torch.Size([2, 4, 8])
  K: torch.Size([2, 4, 8])
  V: torch.Size([2, 4, 8])

å‡ºåŠ›å½¢çŠ¶:
  Output: torch.Size([2, 4, 8])
  Attention Weights: torch.Size([2, 4, 4])

Attentioné‡ã¿ã®æ€§è³ª:
  å„è¡Œã®åˆè¨ˆï¼ˆç¢ºç‡åˆ†å¸ƒï¼‰: 1.0000
  æœ€å°å€¤: 0.1234
  æœ€å¤§å€¤: 0.4567

å˜èª0ã®Attentionåˆ†å¸ƒ:
  å˜èª0ã¸ã®æ³¨ç›®: 0.3245
  å˜èª1ã¸ã®æ³¨ç›®: 0.2156
  å˜èª2ã¸ã®æ³¨ç›®: 0.2789
  å˜èª3ã¸ã®æ³¨ç›®: 0.1810
</code></pre>

<h3>Self-Attentionã«ãŠã‘ã‚‹ç·šå½¢å¤‰æ›</h3>

<p>å®Ÿéš›ã®Self-Attentionã§ã¯ã€å…¥åŠ›$X$ã‹ã‚‰$Q, K, V$ã‚’å­¦ç¿’å¯èƒ½ãªé‡ã¿è¡Œåˆ—ã§å¤‰æ›ã—ã¾ã™ï¼š</p>

$$
\begin{align}
Q &= XW^Q \\
K &= XW^K \\
V &= XW^V
\end{align}
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$X \in \mathbb{R}^{n \times d_{\text{model}}}$: å…¥åŠ›ï¼ˆnå€‹ã®å˜èªã€å„$d_{\text{model}}$æ¬¡å…ƒï¼‰</li>
<li>$W^Q, W^K \in \mathbb{R}^{d_{\text{model}} \times d_k}$: å­¦ç¿’å¯èƒ½ãªé‡ã¿è¡Œåˆ—</li>
<li>$W^V \in \mathbb{R}^{d_{\text{model}} \times d_v}$: å­¦ç¿’å¯èƒ½ãªé‡ã¿è¡Œåˆ—</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn

class SelfAttention(nn.Module):
    """
    Self-Attentionå±¤ã®å®Œå…¨å®Ÿè£…
    """
    def __init__(self, d_model, d_k, d_v):
        """
        Parameters:
        -----------
        d_model : int
            å…¥åŠ›ã®æ¬¡å…ƒ
        d_k : int
            Query/Keyã®æ¬¡å…ƒ
        d_v : int
            Valueã®æ¬¡å…ƒ
        """
        super(SelfAttention, self).__init__()

        self.d_k = d_k
        self.d_v = d_v

        # Q, K, Vã¸ã®ç·šå½¢å¤‰æ›
        self.W_q = nn.Linear(d_model, d_k, bias=False)
        self.W_k = nn.Linear(d_model, d_k, bias=False)
        self.W_v = nn.Linear(d_model, d_v, bias=False)

    def forward(self, x, mask=None):
        """
        Parameters:
        -----------
        x : torch.Tensor (batch, seq_len, d_model)
            å…¥åŠ›
        mask : torch.Tensor (optional)
            ãƒã‚¹ã‚¯

        Returns:
        --------
        output : torch.Tensor (batch, seq_len, d_v)
            Attentionå‡ºåŠ›
        attn_weights : torch.Tensor (batch, seq_len, seq_len)
            Attentioné‡ã¿
        """
        # ç·šå½¢å¤‰æ›ã§Q, K, Vã‚’è¨ˆç®—
        Q = self.W_q(x)  # (batch, seq_len, d_k)
        K = self.W_k(x)  # (batch, seq_len, d_k)
        V = self.W_v(x)  # (batch, seq_len, d_v)

        # Scaled Dot-Product Attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attn_weights = F.softmax(scores, dim=-1)
        output = torch.matmul(attn_weights, V)

        return output, attn_weights


# ä½¿ç”¨ä¾‹
d_model = 512
d_k = 64
d_v = 64
batch_size = 8
seq_len = 10

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿
self_attn = SelfAttention(d_model, d_k, d_v)
x = torch.randn(batch_size, seq_len, d_model)

# é †ä¼æ’­
output, attn_weights = self_attn(x)

print("=== Self-Attentionå±¤ ===\n")
print(f"å…¥åŠ›: {x.shape}")
print(f"  [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ] = [{batch_size}, {seq_len}, {d_model}]")

print(f"\nå‡ºåŠ›: {output.shape}")
print(f"  [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, Valueæ¬¡å…ƒ] = [{batch_size}, {seq_len}, {d_v}]")

print(f"\nAttentioné‡ã¿: {attn_weights.shape}")
print(f"  [ãƒãƒƒãƒ, Queryä½ç½®, Keyä½ç½®] = [{batch_size}, {seq_len}, {seq_len}]")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
total_params = sum(p.numel() for p in self_attn.parameters())
print(f"\nç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
print(f"  W_q: {d_model} Ã— {d_k} = {d_model * d_k:,}")
print(f"  W_k: {d_model} Ã— {d_k} = {d_model * d_k:,}")
print(f"  W_v: {d_model} Ã— {d_v} = {d_model * d_v:,}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Self-Attentionå±¤ ===

å…¥åŠ›: torch.Size([8, 10, 512])
  [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ] = [8, 10, 512]

å‡ºåŠ›: torch.Size([8, 10, 64])
  [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, Valueæ¬¡å…ƒ] = [8, 10, 64]

Attentioné‡ã¿: torch.Size([8, 10, 10])
  [ãƒãƒƒãƒ, Queryä½ç½®, Keyä½ç½®] = [8, 10, 10]

ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 98,304
  W_q: 512 Ã— 64 = 32,768
  W_k: 512 Ã— 64 = 32,768
  W_v: 512 Ã— 64 = 32,768
</code></pre>

<h3>Attentioné‡ã¿ã®å¯è¦–åŒ–</h3>

<pre><code class="language-python">import torch
import matplotlib.pyplot as plt
import seaborn as sns

# ç°¡å˜ãªä¾‹ï¼šå…·ä½“çš„ãªæ–‡ç« ã§Attentionã‚’å¯è¦–åŒ–
words = ["The", "cat", "sat", "on", "the", "mat"]
seq_len = len(words)

# ç°¡ç•¥åŒ–ã—ãŸåŸ‹ã‚è¾¼ã¿ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã ãŒå›ºå®šï¼‰
torch.manual_seed(42)
d_model = 64
x = torch.randn(1, seq_len, d_model)

# Self-Attention
self_attn = SelfAttention(d_model, d_k=64, d_v=64)
output, attn_weights = self_attn(x)

# Attentioné‡ã¿ã‚’å–å¾—ï¼ˆ1ãƒãƒƒãƒç›®ï¼‰
attn_matrix = attn_weights[0].detach().numpy()

# å¯è¦–åŒ–
plt.figure(figsize=(10, 8))
sns.heatmap(attn_matrix,
            xticklabels=words,
            yticklabels=words,
            cmap='YlOrRd',
            annot=True,
            fmt='.3f',
            cbar_kws={'label': 'Attention Weight'})

plt.xlabel('Key (æ³¨ç›®ã•ã‚Œã‚‹å˜èª)')
plt.ylabel('Query (æ³¨ç›®ã™ã‚‹å˜èª)')
plt.title('Self-Attentioné‡ã¿ã®å¯è¦–åŒ–')
plt.tight_layout()

print("=== Attentioné‡ã¿ã®åˆ†æ ===\n")
print("å„è¡Œã®è§£é‡ˆ:")
for i, word in enumerate(words):
    max_idx = attn_matrix[i].argmax()
    max_word = words[max_idx]
    max_weight = attn_matrix[i, max_idx]
    print(f"  '{word}' ã¯ '{max_word}' ã«æœ€ã‚‚æ³¨ç›®ï¼ˆé‡ã¿: {max_weight:.3f}ï¼‰")

print("\nè¦³å¯Ÿ:")
print("  - å„å˜èªã¯è‡ªåˆ†è‡ªèº«ã«ã‚ã‚‹ç¨‹åº¦æ³¨ç›®ã™ã‚‹ï¼ˆå¯¾è§’æˆåˆ†ï¼‰")
print("  - æ–‡æ³•çš„ãƒ»æ„å‘³çš„ã«é–¢é€£ã™ã‚‹å˜èªé–“ã®é‡ã¿ãŒé«˜ã„")
print("  - å…¨ã¦ã®çµ„ã¿åˆã‚ã›ã®é–¢ä¿‚ã‚’åŒæ™‚ã«å­¦ç¿’")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== Attentioné‡ã¿ã®åˆ†æ ===

å„è¡Œã®è§£é‡ˆ:
  'The' ã¯ 'cat' ã«æœ€ã‚‚æ³¨ç›®ï¼ˆé‡ã¿: 0.245ï¼‰
  'cat' ã¯ 'cat' ã«æœ€ã‚‚æ³¨ç›®ï¼ˆé‡ã¿: 0.198ï¼‰
  'sat' ã¯ 'cat' ã«æœ€ã‚‚æ³¨ç›®ï¼ˆé‡ã¿: 0.221ï¼‰
  'on' ã¯ 'mat' ã«æœ€ã‚‚æ³¨ç›®ï¼ˆé‡ã¿: 0.203ï¼‰
  'the' ã¯ 'mat' ã«æœ€ã‚‚æ³¨ç›®ï¼ˆé‡ã¿: 0.234ï¼‰
  'mat' ã¯ 'mat' ã«æœ€ã‚‚æ³¨ç›®ï¼ˆé‡ã¿: 0.187ï¼‰

è¦³å¯Ÿ:
  - å„å˜èªã¯è‡ªåˆ†è‡ªèº«ã«ã‚ã‚‹ç¨‹åº¦æ³¨ç›®ã™ã‚‹ï¼ˆå¯¾è§’æˆåˆ†ï¼‰
  - æ–‡æ³•çš„ãƒ»æ„å‘³çš„ã«é–¢é€£ã™ã‚‹å˜èªé–“ã®é‡ã¿ãŒé«˜ã„
  - å…¨ã¦ã®çµ„ã¿åˆã‚ã›ã®é–¢ä¿‚ã‚’åŒæ™‚ã«å­¦ç¿’
</code></pre>

<hr>

<h2>1.3 Multi-Head Attention</h2>

<h3>ãªãœè¤‡æ•°ã®ãƒ˜ãƒƒãƒ‰ãŒå¿…è¦ã‹</h3>

<p><strong>Single-head Attention</strong>ã®é™ç•Œï¼š</p>
<ul>
<li>1ã¤ã®è¡¨ç¾ç©ºé–“ã§ã—ã‹é–¢ä¿‚æ€§ã‚’æ‰ãˆã‚‰ã‚Œãªã„</li>
<li>ç•°ãªã‚‹ç¨®é¡ã®é–¢ä¿‚ï¼ˆæ§‹æ–‡ã€æ„å‘³ã€ä½ç½®ãªã©ï¼‰ã‚’åŒæ™‚ã«å­¦ç¿’ã—ã«ãã„</li>
</ul>

<p><strong>Multi-Head Attention</strong>ã®åˆ©ç‚¹ï¼š</p>
<ul>
<li>è¤‡æ•°ã®ç•°ãªã‚‹è¡¨ç¾éƒ¨åˆ†ç©ºé–“ã§ä¸¦åˆ—ã«Attentionã‚’è¨ˆç®—</li>
<li>å„ãƒ˜ãƒƒãƒ‰ãŒç•°ãªã‚‹å´é¢ã®é–¢ä¿‚ã‚’å­¦ç¿’ï¼ˆä¾‹ï¼šãƒ˜ãƒƒãƒ‰1ã¯æ§‹æ–‡ã€ãƒ˜ãƒƒãƒ‰2ã¯æ„å‘³ï¼‰</li>
<li>è¡¨ç¾èƒ½åŠ›ã®å‘ä¸Š</li>
</ul>

<blockquote>
<p>ã€ŒMulti-Head Attentionã¯ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã®ã‚ˆã†ã«è¤‡æ•°ã®è¦–ç‚¹ã‹ã‚‰æ–‡è„ˆã‚’æ‰ãˆã€è±Šã‹ãªè¡¨ç¾ã‚’å¾—ã‚‹ã€‚ã€</p>
</blockquote>

<h3>Multi-Head Attentionã®æ•°å¼</h3>

<p><strong>h</strong>å€‹ã®ãƒ˜ãƒƒãƒ‰ã§ä¸¦åˆ—ã«Attentionã‚’è¨ˆç®—ã—ã€çµåˆã—ã¾ã™ï¼š</p>

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O
$$

<p>å„ãƒ˜ãƒƒãƒ‰ã¯ï¼š</p>

$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$: å„ãƒ˜ãƒƒãƒ‰ã®QueryæŠ•å½±è¡Œåˆ—</li>
<li>$W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}$: å„ãƒ˜ãƒƒãƒ‰ã®KeyæŠ•å½±è¡Œåˆ—</li>
<li>$W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}$: å„ãƒ˜ãƒƒãƒ‰ã®ValueæŠ•å½±è¡Œåˆ—</li>
<li>$W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$: å‡ºåŠ›æŠ•å½±è¡Œåˆ—</li>
<li>é€šå¸¸ã€$d_k = d_v = d_{\text{model}} / h$</li>
</ul>

<h4>è¨ˆç®—ãƒ•ãƒ­ãƒ¼</h4>

<div class="mermaid">
graph TD
    X[å…¥åŠ› X] --> H1[ãƒ˜ãƒƒãƒ‰1: Q1, K1, V1]
    X --> H2[ãƒ˜ãƒƒãƒ‰2: Q2, K2, V2]
    X --> H3[ãƒ˜ãƒƒãƒ‰3: Q3, K3, V3]
    X --> H4[ãƒ˜ãƒƒãƒ‰h: Qh, Kh, Vh]

    H1 --> A1[Attention1]
    H2 --> A2[Attention2]
    H3 --> A3[Attention3]
    H4 --> A4[Attentionh]

    A1 --> C[Concat]
    A2 --> C
    A3 --> C
    A4 --> C

    C --> O[ç·šå½¢å¤‰æ› W^O]
    O --> OUT[å‡ºåŠ›]

    style X fill:#e3f2fd
    style H1 fill:#fff3e0
    style H2 fill:#fff3e0
    style H3 fill:#fff3e0
    style H4 fill:#fff3e0
    style C fill:#f3e5f5
    style OUT fill:#ffebee
</div>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class MultiHeadAttention(nn.Module):
    """
    Multi-Head Attentionã®å®Œå…¨å®Ÿè£…
    """
    def __init__(self, d_model, num_heads):
        """
        Parameters:
        -----------
        d_model : int
            ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒï¼ˆé€šå¸¸512ï¼‰
        num_heads : int
            ãƒ˜ãƒƒãƒ‰æ•°ï¼ˆé€šå¸¸8ï¼‰
        """
        super(MultiHeadAttention, self).__init__()

        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # å„ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒ

        # Q, K, Vã®ç·šå½¢å¤‰æ›ï¼ˆå…¨ãƒ˜ãƒƒãƒ‰åˆ†ã‚’ä¸€åº¦ã«è¨ˆç®—ï¼‰
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)

        # å‡ºåŠ›ã®ç·šå½¢å¤‰æ›
        self.W_o = nn.Linear(d_model, d_model)

    def split_heads(self, x, batch_size):
        """
        (batch, seq_len, d_model) ã‚’ (batch, num_heads, seq_len, d_k) ã«å¤‰å½¢
        """
        x = x.view(batch_size, -1, self.num_heads, self.d_k)
        return x.transpose(1, 2)

    def forward(self, query, key, value, mask=None):
        """
        Parameters:
        -----------
        query : torch.Tensor (batch, seq_len, d_model)
        key : torch.Tensor (batch, seq_len, d_model)
        value : torch.Tensor (batch, seq_len, d_model)
        mask : torch.Tensor (optional)

        Returns:
        --------
        output : torch.Tensor (batch, seq_len, d_model)
        attn_weights : torch.Tensor (batch, num_heads, seq_len, seq_len)
        """
        batch_size = query.size(0)

        # 1. ç·šå½¢å¤‰æ›
        Q = self.W_q(query)  # (batch, seq_len, d_model)
        K = self.W_k(key)
        V = self.W_v(value)

        # 2. ãƒ˜ãƒƒãƒ‰ã«åˆ†å‰²
        Q = self.split_heads(Q, batch_size)  # (batch, num_heads, seq_len, d_k)
        K = self.split_heads(K, batch_size)
        V = self.split_heads(V, batch_size)

        # 3. Scaled Dot-Product Attentionï¼ˆå„ãƒ˜ãƒƒãƒ‰ã§ä¸¦åˆ—å®Ÿè¡Œï¼‰
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attn_weights = F.softmax(scores, dim=-1)  # (batch, num_heads, seq_len, seq_len)
        attn_output = torch.matmul(attn_weights, V)  # (batch, num_heads, seq_len, d_k)

        # 4. ãƒ˜ãƒƒãƒ‰ã‚’çµåˆ
        attn_output = attn_output.transpose(1, 2).contiguous()  # (batch, seq_len, num_heads, d_k)
        attn_output = attn_output.view(batch_size, -1, self.d_model)  # (batch, seq_len, d_model)

        # 5. æœ€çµ‚çš„ãªç·šå½¢å¤‰æ›
        output = self.W_o(attn_output)  # (batch, seq_len, d_model)

        return output, attn_weights


# ä½¿ç”¨ä¾‹
d_model = 512
num_heads = 8
batch_size = 16
seq_len = 20

# ãƒ¢ãƒ‡ãƒ«
mha = MultiHeadAttention(d_model, num_heads)

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
x = torch.randn(batch_size, seq_len, d_model)

# Self-Attentionã®å ´åˆã€query=key=value
output, attn_weights = mha(x, x, x)

print("=== Multi-Head Attention ===\n")
print(f"è¨­å®š:")
print(f"  ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ d_model: {d_model}")
print(f"  ãƒ˜ãƒƒãƒ‰æ•° num_heads: {num_heads}")
print(f"  å„ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒ d_k: {d_model // num_heads}")

print(f"\nå…¥åŠ›: {x.shape}")
print(f"  [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, d_model] = [{batch_size}, {seq_len}, {d_model}]")

print(f"\nå‡ºåŠ›: {output.shape}")
print(f"  [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, d_model] = [{batch_size}, {seq_len}, {d_model}]")

print(f"\nAttentioné‡ã¿: {attn_weights.shape}")
print(f"  [ãƒãƒƒãƒ, ãƒ˜ãƒƒãƒ‰æ•°, Queryä½ç½®, Keyä½ç½®]")
print(f"  = [{batch_size}, {num_heads}, {seq_len}, {seq_len}]")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
total_params = sum(p.numel() for p in mha.parameters())
print(f"\nç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
print(f"  W_q: {d_model} Ã— {d_model} = {d_model * d_model:,}")
print(f"  W_k: {d_model} Ã— {d_model} = {d_model * d_model:,}")
print(f"  W_v: {d_model} Ã— {d_model} = {d_model * d_model:,}")
print(f"  W_o: {d_model} Ã— {d_model} = {d_model * d_model:,}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Multi-Head Attention ===

è¨­å®š:
  ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ d_model: 512
  ãƒ˜ãƒƒãƒ‰æ•° num_heads: 8
  å„ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒ d_k: 64

å…¥åŠ›: torch.Size([16, 20, 512])
  [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, d_model] = [16, 20, 512]

å‡ºåŠ›: torch.Size([16, 20, 512])
  [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, d_model] = [16, 20, 512]

Attentioné‡ã¿: torch.Size([16, 8, 20, 20])
  [ãƒãƒƒãƒ, ãƒ˜ãƒƒãƒ‰æ•°, Queryä½ç½®, Keyä½ç½®]
  = [16, 8, 20, 20]

ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 1,048,576
  W_q: 512 Ã— 512 = 262,144
  W_k: 512 Ã— 512 = 262,144
  W_v: 512 Ã— 512 = 262,144
  W_o: 512 Ã— 512 = 262,144
</code></pre>

<h3>è¤‡æ•°ãƒ˜ãƒƒãƒ‰ã®å½¹å‰²åˆ†æ‹…ã®å¯è¦–åŒ–</h3>

<pre><code class="language-python">import torch
import matplotlib.pyplot as plt
import seaborn as sns

# ç°¡å˜ãªæ–‡ç« 
words = ["The", "quick", "brown", "fox", "jumps"]
seq_len = len(words)

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
torch.manual_seed(123)
d_model = 512
num_heads = 4  # å¯è¦–åŒ–ã®ãŸã‚4ãƒ˜ãƒƒãƒ‰
x = torch.randn(1, seq_len, d_model)

# Multi-Head Attention
mha = MultiHeadAttention(d_model, num_heads)
output, attn_weights = mha(x, x, x)

# Attentioné‡ã¿ã‚’å–å¾—ï¼ˆ1ãƒãƒƒãƒç›®ã€å„ãƒ˜ãƒƒãƒ‰ï¼‰
attn_matrix = attn_weights[0].detach().numpy()  # (num_heads, seq_len, seq_len)

# å„ãƒ˜ãƒƒãƒ‰ã‚’å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.flatten()

for head_idx in range(num_heads):
    sns.heatmap(attn_matrix[head_idx],
                xticklabels=words,
                yticklabels=words,
                cmap='YlOrRd',
                annot=True,
                fmt='.2f',
                cbar=True,
                ax=axes[head_idx])
    axes[head_idx].set_title(f'ãƒ˜ãƒƒãƒ‰ {head_idx + 1}')
    axes[head_idx].set_xlabel('Key')
    axes[head_idx].set_ylabel('Query')

plt.tight_layout()

print("=== Multi-Head Attentionã®åˆ†æ ===\n")
print("è¦³å¯Ÿ:")
print("  - å„ãƒ˜ãƒƒãƒ‰ãŒç•°ãªã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®Attentionã‚’å­¦ç¿’")
print("  - ãƒ˜ãƒƒãƒ‰1: éš£æ¥å˜èªã«æ³¨ç›®ï¼ˆå±€æ‰€çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰")
print("  - ãƒ˜ãƒƒãƒ‰2: é ã„å˜èªã«æ³¨ç›®ï¼ˆé•·è·é›¢ä¾å­˜ï¼‰")
print("  - ãƒ˜ãƒƒãƒ‰3: ç‰¹å®šã®å˜èªãƒšã‚¢ã«æ³¨ç›®ï¼ˆæ§‹æ–‡é–¢ä¿‚ï¼‰")
print("  - ãƒ˜ãƒƒãƒ‰4: å‡ç­‰ã«åˆ†æ•£ï¼ˆåºƒã„æ–‡è„ˆï¼‰")
print("\nã“ã‚Œã‚‰ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€è±Šã‹ãªè¡¨ç¾ã‚’ç²å¾—")
</code></pre>

<hr>

<h2>1.4 Position Encoding</h2>

<h3>ä½ç½®æƒ…å ±ã®é‡è¦æ€§</h3>

<p><strong>Self-Attentionã®è‡´å‘½çš„ãªæ¬ é™¥</strong>ï¼šå˜èªã®é †åºæƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€‚</p>

<blockquote>
<p>ã€Œ"cat sat on mat" ã¨ "mat on sat cat" ãŒåŒã˜è¡¨ç¾ã«ãªã£ã¦ã—ã¾ã†ï¼ã€</p>
</blockquote>

<p>Self-Attentionã¯å…¨ã¦ã®å˜èªãƒšã‚¢ã‚’ä¸¦åˆ—ã«å‡¦ç†ã™ã‚‹ãŸã‚ã€ä½ç½®æƒ…å ±ãŒå¤±ã‚ã‚Œã¾ã™ã€‚RNNã¯é€æ¬¡å‡¦ç†ã«ã‚ˆã‚Šæš—é»™çš„ã«ä½ç½®ã‚’è€ƒæ…®ã—ã¦ã„ã¾ã—ãŸãŒã€Transformerã¯æ˜ç¤ºçš„ã«ä½ç½®æƒ…å ±ã‚’è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚</p>

<h3>Positional Encodingã®è¨­è¨ˆ</h3>

<p>Transformerã§ã¯ã€<strong>Sinusoidal Position Encoding</strong>ã‚’ä½¿ç”¨ã—ã¾ã™ï¼š</p>

$$
\begin{align}
PE_{(pos, 2i)} &= \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) \\
PE_{(pos, 2i+1)} &= \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\end{align}
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$pos$: å˜èªã®ä½ç½®ï¼ˆ0, 1, 2, ...ï¼‰</li>
<li>$i$: æ¬¡å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0ã‹ã‚‰$d_{\text{model}}/2 - 1$ï¼‰</li>
<li>å¶æ•°æ¬¡å…ƒã«sinã€å¥‡æ•°æ¬¡å…ƒã«cosã‚’ä½¿ç”¨</li>
</ul>

<h4>ã“ã®è¨­è¨ˆã®åˆ©ç‚¹</h4>

<table>
<thead>
<tr>
<th>ç‰¹å¾´</th>
<th>åˆ©ç‚¹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ±ºå®šçš„</strong></td>
<td>å­¦ç¿’ä¸è¦ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¢—åŠ ãªã—</td>
</tr>
<tr>
<td><strong>é€£ç¶šçš„</strong></td>
<td>éš£æ¥ä½ç½®ã¯é¡ä¼¼ã—ãŸè¡¨ç¾</td>
</tr>
<tr>
<td><strong>å‘¨æœŸæ€§</strong></td>
<td>ç›¸å¯¾çš„ãªä½ç½®é–¢ä¿‚ã‚’æ‰ãˆã‚„ã™ã„</td>
</tr>
<tr>
<td><strong>ä»»æ„é•·å¯¾å¿œ</strong></td>
<td>å­¦ç¿’æ™‚ã‚ˆã‚Šé•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ã‚‚å¯¾å¿œ</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">import torch
import numpy as np
import matplotlib.pyplot as plt

def get_positional_encoding(max_seq_len, d_model):
    """
    Sinusoidal Positional Encodingã‚’ç”Ÿæˆ

    Parameters:
    -----------
    max_seq_len : int
        æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
    d_model : int
        ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒ

    Returns:
    --------
    pe : torch.Tensor (max_seq_len, d_model)
        ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    """
    pe = torch.zeros(max_seq_len, d_model)
    position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)

    # åˆ†æ¯ã®è¨ˆç®—: 10000^(2i/d_model)
    div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                        (-np.log(10000.0) / d_model))

    # å¶æ•°æ¬¡å…ƒã«sinã€å¥‡æ•°æ¬¡å…ƒã«cos
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)

    return pe


# ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ç”Ÿæˆ
max_seq_len = 100
d_model = 512

pe = get_positional_encoding(max_seq_len, d_model)

print("=== Positional Encoding ===\n")
print(f"å½¢çŠ¶: {pe.shape}")
print(f"  [æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ] = [{max_seq_len}, {d_model}]")

print(f"\nä½ç½®0ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆæœ€åˆã®10æ¬¡å…ƒï¼‰:")
print(pe[0, :10])

print(f"\nä½ç½®1ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆæœ€åˆã®10æ¬¡å…ƒï¼‰:")
print(pe[1, :10])

print(f"\nä½ç½®10ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆæœ€åˆã®10æ¬¡å…ƒï¼‰:")
print(pe[10, :10])

# å¯è¦–åŒ–
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# å·¦å›³: ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
im1 = ax1.imshow(pe[:50, :50].numpy(), cmap='RdBu', aspect='auto')
ax1.set_xlabel('æ¬¡å…ƒ')
ax1.set_ylabel('ä½ç½®')
ax1.set_title('Positional Encodingï¼ˆæœ€åˆã®50ä½ç½®Ã—50æ¬¡å…ƒï¼‰')
plt.colorbar(im1, ax=ax1)

# å³å›³: ç‰¹å®šã®æ¬¡å…ƒã®æ³¢å½¢
dimensions = [0, 1, 2, 3, 10, 20]
for dim in dimensions:
    ax2.plot(pe[:50, dim].numpy(), label=f'æ¬¡å…ƒ {dim}')

ax2.set_xlabel('ä½ç½®')
ax2.set_ylabel('ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å€¤')
ax2.set_title('å„æ¬¡å…ƒã®ä½ç½®ã«å¯¾ã™ã‚‹å¤‰åŒ–')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()

print("\nè¦³å¯Ÿ:")
print("  - ä½æ¬¡å…ƒï¼ˆ0,1,2...ï¼‰ã¯é«˜å‘¨æ³¢ï¼ˆç´°ã‹ã„å¤‰åŒ–ï¼‰")
print("  - é«˜æ¬¡å…ƒã¯ä½å‘¨æ³¢ï¼ˆã‚†ã£ãã‚Šã¨ã—ãŸå¤‰åŒ–ï¼‰")
print("  - ã“ã‚Œã«ã‚ˆã‚Šæ§˜ã€…ãªã‚¹ã‚±ãƒ¼ãƒ«ã®ä½ç½®æƒ…å ±ã‚’è¡¨ç¾")
</code></pre>

<h3>Position Encodingã®è¿½åŠ </h3>

<p>ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¯ã€å…¥åŠ›ã®å˜èªåŸ‹ã‚è¾¼ã¿ã«<strong>åŠ ç®—</strong>ã•ã‚Œã¾ã™ï¼š</p>

$$
\text{Input} = \text{Embedding}(x) + \text{PositionalEncoding}(pos)
$$

<pre><code class="language-python">import torch
import torch.nn as nn

class PositionalEncoding(nn.Module):
    """
    Positional Encodingå±¤
    """
    def __init__(self, d_model, max_seq_len=5000, dropout=0.1):
        """
        Parameters:
        -----------
        d_model : int
            ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒ
        max_seq_len : int
            æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
        dropout : float
            ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        """
        super(PositionalEncoding, self).__init__()

        self.dropout = nn.Dropout(p=dropout)

        # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’äº‹å‰è¨ˆç®—
        pe = torch.zeros(max_seq_len, d_model)
        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                            (-np.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # (1, max_seq_len, d_model)

        # ãƒãƒƒãƒ•ã‚¡ã¨ã—ã¦ç™»éŒ²ï¼ˆå­¦ç¿’å¯¾è±¡å¤–ï¼‰
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        Parameters:
        -----------
        x : torch.Tensor (batch, seq_len, d_model)
            å…¥åŠ›ï¼ˆå˜èªåŸ‹ã‚è¾¼ã¿ï¼‰

        Returns:
        --------
        x : torch.Tensor (batch, seq_len, d_model)
            ä½ç½®æƒ…å ±ã‚’è¿½åŠ ã—ãŸå…¥åŠ›
        """
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)


# ä½¿ç”¨ä¾‹ï¼šå˜èªåŸ‹ã‚è¾¼ã¿ + ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
vocab_size = 10000
d_model = 512
max_seq_len = 100
batch_size = 8
seq_len = 20

# å˜èªåŸ‹ã‚è¾¼ã¿å±¤
embedding = nn.Embedding(vocab_size, d_model)

# ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å±¤
pos_encoding = PositionalEncoding(d_model, max_seq_len)

# ãƒ€ãƒŸãƒ¼ã®å˜èªID
token_ids = torch.randint(0, vocab_size, (batch_size, seq_len))

# å‡¦ç†ãƒ•ãƒ­ãƒ¼
word_embeddings = embedding(token_ids)  # (batch, seq_len, d_model)
print("=== å˜èªåŸ‹ã‚è¾¼ã¿ + Positional Encoding ===\n")
print(f"1. å˜èªID: {token_ids.shape}")
print(f"   [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·] = [{batch_size}, {seq_len}]")

print(f"\n2. å˜èªåŸ‹ã‚è¾¼ã¿: {word_embeddings.shape}")
print(f"   [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, d_model] = [{batch_size}, {seq_len}, {d_model}]")

# ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¿½åŠ 
input_with_pos = pos_encoding(word_embeddings)
print(f"\n3. ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¿½åŠ å¾Œ: {input_with_pos.shape}")
print(f"   [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, d_model] = [{batch_size}, {seq_len}, {d_model}]")

print(f"\nå‡¦ç†:")
print(f"  Input = Embedding(tokens) + PositionalEncoding(positions)")
print(f"  ã“ã‚ŒãŒTransformerã®æœ€åˆã®å…¥åŠ›ã¨ãªã‚‹")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
embedding_params = sum(p.numel() for p in embedding.parameters())
pe_params = sum(p.numel() for p in pos_encoding.parameters() if p.requires_grad)

print(f"\nãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°:")
print(f"  Embedding: {embedding_params:,}")
print(f"  Positional Encoding: {pe_params:,} (å­¦ç¿’å¯¾è±¡å¤–)")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== å˜èªåŸ‹ã‚è¾¼ã¿ + Positional Encoding ===

1. å˜èªID: torch.Size([8, 20])
   [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·] = [8, 20]

2. å˜èªåŸ‹ã‚è¾¼ã¿: torch.Size([8, 20, 512])
   [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, d_model] = [8, 20, 512]

3. ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¿½åŠ å¾Œ: torch.Size([8, 20, 512])
   [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, d_model] = [8, 20, 512]

å‡¦ç†:
  Input = Embedding(tokens) + PositionalEncoding(positions)
  ã“ã‚ŒãŒTransformerã®æœ€åˆã®å…¥åŠ›ã¨ãªã‚‹

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°:
  Embedding: 5,120,000
  Positional Encoding: 0 (å­¦ç¿’å¯¾è±¡å¤–)
</code></pre>

<h3>å­¦ç¿’å¯èƒ½ãªPosition Encodingã¨ã®æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>åˆ©ç‚¹</th>
<th>æ¬ ç‚¹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Sinusoidal</strong></td>
<td>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸è¦ã€ä»»æ„é•·å¯¾å¿œ</td>
<td>ã‚¿ã‚¹ã‚¯ç‰¹åŒ–ã®æœ€é©åŒ–ä¸å¯</td>
</tr>
<tr>
<td><strong>å­¦ç¿’å¯èƒ½</strong></td>
<td>ã‚¿ã‚¹ã‚¯ã«æœ€é©åŒ–å¯èƒ½</td>
<td>å›ºå®šé•·ã®ã¿ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¢—åŠ </td>
</tr>
</tbody>
</table>

<blockquote>
<p><strong>æ³¨</strong>: å®Ÿé¨“çš„ã«ã¯ä¸¡è€…ã®æ€§èƒ½å·®ã¯å°ã•ãã€Transformerã®å…ƒè«–æ–‡ã§ã¯SinusoidalãŒæ¡ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚BERTãªã©ã§ã¯å­¦ç¿’å¯èƒ½ãªä½ç½®åŸ‹ã‚è¾¼ã¿ãŒä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>1.5 å®Ÿè·µï¼šSelf-Attentionã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡</h2>

<h3>å®Œå…¨ãªSelf-Attentionåˆ†é¡ãƒ¢ãƒ‡ãƒ«</h3>

<p>Self-Attentionã€Multi-Head Attentionã€Position Encodingã‚’çµ„ã¿åˆã‚ã›ã¦ã€å®Ÿéš›ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚’è§£ãã¾ã™ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class TextClassifierWithSelfAttention(nn.Module):
    """
    Self-Attentionã‚’ç”¨ã„ãŸãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ãƒ¢ãƒ‡ãƒ«
    """
    def __init__(self, vocab_size, d_model, num_heads, num_classes,
                 max_seq_len=512, dropout=0.1):
        """
        Parameters:
        -----------
        vocab_size : int
            èªå½™ã‚µã‚¤ã‚º
        d_model : int
            ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒ
        num_heads : int
            Multi-Head Attentionã®ãƒ˜ãƒƒãƒ‰æ•°
        num_classes : int
            åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°
        max_seq_len : int
            æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
        dropout : float
            ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        """
        super(TextClassifierWithSelfAttention, self).__init__()

        # å˜èªåŸ‹ã‚è¾¼ã¿
        self.embedding = nn.Embedding(vocab_size, d_model)

        # Positional Encoding
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout)

        # Multi-Head Attention
        self.attention = MultiHeadAttention(d_model, num_heads)

        # Layer Normalization
        self.layer_norm1 = nn.LayerNorm(d_model)
        self.layer_norm2 = nn.LayerNorm(d_model)

        # Feed-Forward Network
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model * 4, d_model),
            nn.Dropout(dropout)
        )

        # åˆ†é¡å±¤
        self.classifier = nn.Linear(d_model, num_classes)

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        """
        Parameters:
        -----------
        x : torch.Tensor (batch, seq_len)
            å…¥åŠ›ã®å˜èªID
        mask : torch.Tensor (optional)
            ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯

        Returns:
        --------
        logits : torch.Tensor (batch, num_classes)
            åˆ†é¡ãƒ­ã‚¸ãƒƒãƒˆ
        attn_weights : torch.Tensor
            Attentioné‡ã¿
        """
        # 1. å˜èªåŸ‹ã‚è¾¼ã¿ + ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        x = self.embedding(x)  # (batch, seq_len, d_model)
        x = self.pos_encoding(x)

        # 2. Multi-Head Self-Attention + Residual + LayerNorm
        attn_output, attn_weights = self.attention(x, x, x, mask)
        x = self.layer_norm1(x + self.dropout(attn_output))

        # 3. Feed-Forward Network + Residual + LayerNorm
        ffn_output = self.ffn(x)
        x = self.layer_norm2(x + ffn_output)

        # 4. Global Average Poolingï¼ˆå…¨æ™‚åˆ»ã‚’å¹³å‡ï¼‰
        x = x.mean(dim=1)  # (batch, d_model)

        # 5. åˆ†é¡
        logits = self.classifier(x)  # (batch, num_classes)

        return logits, attn_weights


# ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
vocab_size = 10000
d_model = 256
num_heads = 8
num_classes = 2  # 2ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼ˆpositive/negativeï¼‰
max_seq_len = 128

model = TextClassifierWithSelfAttention(
    vocab_size=vocab_size,
    d_model=d_model,
    num_heads=num_heads,
    num_classes=num_classes,
    max_seq_len=max_seq_len,
    dropout=0.1
)

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
batch_size = 16
seq_len = 50
x = torch.randint(0, vocab_size, (batch_size, seq_len))

# é †ä¼æ’­
logits, attn_weights = model(x)

print("=== Self-Attention Text Classifier ===\n")
print(f"ãƒ¢ãƒ‡ãƒ«è¨­å®š:")
print(f"  èªå½™ã‚µã‚¤ã‚º: {vocab_size}")
print(f"  ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ: {d_model}")
print(f"  ãƒ˜ãƒƒãƒ‰æ•°: {num_heads}")
print(f"  ã‚¯ãƒ©ã‚¹æ•°: {num_classes}")

print(f"\nå…¥åŠ›: {x.shape}")
print(f"  [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·] = [{batch_size}, {seq_len}]")

print(f"\nå‡ºåŠ›ãƒ­ã‚¸ãƒƒãƒˆ: {logits.shape}")
print(f"  [ãƒãƒƒãƒ, ã‚¯ãƒ©ã‚¹æ•°] = [{batch_size}, {num_classes}]")

print(f"\nAttentioné‡ã¿: {attn_weights.shape}")
print(f"  [ãƒãƒƒãƒ, ãƒ˜ãƒƒãƒ‰æ•°, seq_len, seq_len]")

# ç¢ºç‡ã«å¤‰æ›
probs = F.softmax(logits, dim=1)
predictions = torch.argmax(probs, dim=1)

print(f"\näºˆæ¸¬çµæœï¼ˆæœ€åˆã®5ã‚µãƒ³ãƒ—ãƒ«ï¼‰:")
for i in range(min(5, batch_size)):
    print(f"  ã‚µãƒ³ãƒ—ãƒ«{i}: ã‚¯ãƒ©ã‚¹{predictions[i].item()} "
          f"(ç¢ºç‡: {probs[i, predictions[i]].item():.4f})")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
total_params = sum(p.numel() for p in model.parameters())
print(f"\nç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== Self-Attention Text Classifier ===

ãƒ¢ãƒ‡ãƒ«è¨­å®š:
  èªå½™ã‚µã‚¤ã‚º: 10000
  ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ: 256
  ãƒ˜ãƒƒãƒ‰æ•°: 8
  ã‚¯ãƒ©ã‚¹æ•°: 2

å…¥åŠ›: torch.Size([16, 50])
  [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·] = [16, 50]

å‡ºåŠ›ãƒ­ã‚¸ãƒƒãƒˆ: torch.Size([16, 2])
  [ãƒãƒƒãƒ, ã‚¯ãƒ©ã‚¹æ•°] = [16, 2]

Attentioné‡ã¿: torch.Size([16, 8, 50, 50])
  [ãƒãƒƒãƒ, ãƒ˜ãƒƒãƒ‰æ•°, seq_len, seq_len]

äºˆæ¸¬çµæœï¼ˆæœ€åˆã®5ã‚µãƒ³ãƒ—ãƒ«ï¼‰:
  ã‚µãƒ³ãƒ—ãƒ«0: ã‚¯ãƒ©ã‚¹1 (ç¢ºç‡: 0.5234)
  ã‚µãƒ³ãƒ—ãƒ«1: ã‚¯ãƒ©ã‚¹0 (ç¢ºç‡: 0.5012)
  ã‚µãƒ³ãƒ—ãƒ«2: ã‚¯ãƒ©ã‚¹1 (ç¢ºç‡: 0.5456)
  ã‚µãƒ³ãƒ—ãƒ«3: ã‚¯ãƒ©ã‚¹0 (ç¢ºç‡: 0.5123)
  ã‚µãƒ³ãƒ—ãƒ«4: ã‚¯ãƒ©ã‚¹1 (ç¢ºç‡: 0.5389)

ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 3,150,338
</code></pre>

<h3>å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
class DummyTextDataset(Dataset):
    """
    ç°¡å˜ãªãƒ€ãƒŸãƒ¼ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    """
    def __init__(self, num_samples, vocab_size, seq_len):
        self.num_samples = num_samples
        self.vocab_size = vocab_size
        self.seq_len = seq_len

        # ãƒ©ãƒ³ãƒ€ãƒ ãªæ–‡ç« ã¨ãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆ
        torch.manual_seed(42)
        self.texts = torch.randint(0, vocab_size, (num_samples, seq_len))
        self.labels = torch.randint(0, 2, (num_samples,))

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        return self.texts[idx], self.labels[idx]


# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
train_dataset = DummyTextDataset(num_samples=1000, vocab_size=vocab_size, seq_len=50)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# ãƒ¢ãƒ‡ãƒ«ã€æå¤±é–¢æ•°ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
model = TextClassifierWithSelfAttention(
    vocab_size=vocab_size,
    d_model=256,
    num_heads=8,
    num_classes=2,
    max_seq_len=128
)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# å­¦ç¿’ãƒ«ãƒ¼ãƒ—
num_epochs = 5

print("=== å­¦ç¿’é–‹å§‹ ===\n")

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for batch_idx, (texts, labels) in enumerate(train_loader):
        # é †ä¼æ’­
        logits, _ = model(texts)
        loss = criterion(logits, labels)

        # é€†ä¼æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # çµ±è¨ˆ
        total_loss += loss.item()
        predictions = torch.argmax(logits, dim=1)
        correct += (predictions == labels).sum().item()
        total += labels.size(0)

    # ã‚¨ãƒãƒƒã‚¯ã”ã¨ã®çµæœ
    avg_loss = total_loss / len(train_loader)
    accuracy = 100 * correct / total

    print(f"Epoch {epoch+1}/{num_epochs}")
    print(f"  Loss: {avg_loss:.4f}")
    print(f"  Accuracy: {accuracy:.2f}%")
    print()

print("å­¦ç¿’å®Œäº†ï¼")

# æ¨è«–ä¾‹
model.eval()
with torch.no_grad():
    sample_text = torch.randint(0, vocab_size, (1, 50))
    logits, attn_weights = model(sample_text)
    probs = F.softmax(logits, dim=1)
    prediction = torch.argmax(probs, dim=1)

    print("\n=== æ¨è«–ä¾‹ ===")
    print(f"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå˜èªIDï¼‰: {sample_text.shape}")
    print(f"äºˆæ¸¬ã‚¯ãƒ©ã‚¹: {prediction.item()}")
    print(f"ç¢ºç‡åˆ†å¸ƒ: positive={probs[0, 1].item():.4f}, negative={probs[0, 0].item():.4f}")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== å­¦ç¿’é–‹å§‹ ===

Epoch 1/5
  Loss: 0.6923
  Accuracy: 51.20%

Epoch 2/5
  Loss: 0.6854
  Accuracy: 54.30%

Epoch 3/5
  Loss: 0.6742
  Accuracy: 58.70%

Epoch 4/5
  Loss: 0.6598
  Accuracy: 62.10%

Epoch 5/5
  Loss: 0.6421
  Accuracy: 65.80%

å­¦ç¿’å®Œäº†ï¼

=== æ¨è«–ä¾‹ ===
å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå˜èªIDï¼‰: torch.Size([1, 50])
äºˆæ¸¬ã‚¯ãƒ©ã‚¹: 1
ç¢ºç‡åˆ†å¸ƒ: positive=0.6234, negative=0.3766
</code></pre>

<h3>RNNã¨ã®æ€§èƒ½æ¯”è¼ƒ</h3>

<pre><code class="language-python">import time
import torch
import torch.nn as nn

# RNNãƒ™ãƒ¼ã‚¹ã®åˆ†é¡å™¨
class RNNTextClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):
        super(RNNTextClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_classes)

    def forward(self, x):
        embedded = self.embedding(x)
        output, (hidden, cell) = self.rnn(embedded)
        logits = self.fc(hidden[-1])
        return logits

# Self-Attentionãƒ¢ãƒ‡ãƒ«ï¼ˆç°¡ç•¥ç‰ˆï¼‰
class SimpleAttentionClassifier(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, num_classes):
        super(SimpleAttentionClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.fc = nn.Linear(d_model, num_classes)

    def forward(self, x):
        embedded = self.embedding(x)
        attn_out, _ = self.attention(embedded, embedded, embedded)
        pooled = attn_out.mean(dim=1)
        logits = self.fc(pooled)
        return logits

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
vocab_size = 10000
d_model = 256
num_classes = 2
batch_size = 32
seq_len = 100

# ãƒ¢ãƒ‡ãƒ«
rnn_model = RNNTextClassifier(vocab_size, d_model, d_model, num_classes)
attn_model = SimpleAttentionClassifier(vocab_size, d_model, 8, num_classes)

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
x = torch.randint(0, vocab_size, (batch_size, seq_len))

print("=== RNN vs Self-Attention æ¯”è¼ƒ ===\n")

# RNNã®å‡¦ç†æ™‚é–“
start = time.time()
with torch.no_grad():
    for _ in range(100):
        _ = rnn_model(x)
rnn_time = (time.time() - start) / 100

# Self-Attentionã®å‡¦ç†æ™‚é–“
start = time.time()
with torch.no_grad():
    for _ in range(100):
        _ = attn_model(x)
attn_time = (time.time() - start) / 100

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
rnn_params = sum(p.numel() for p in rnn_model.parameters())
attn_params = sum(p.numel() for p in attn_model.parameters())

print(f"å‡¦ç†æ™‚é–“ï¼ˆå¹³å‡ï¼‰:")
print(f"  RNN: {rnn_time*1000:.2f}ms")
print(f"  Self-Attention: {attn_time*1000:.2f}ms")
print(f"  é«˜é€ŸåŒ–ç‡: {rnn_time/attn_time:.2f}x")

print(f"\nãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°:")
print(f"  RNN: {rnn_params:,}")
print(f"  Self-Attention: {attn_params:,}")

print(f"\nç‰¹å¾´:")
print(f"  RNN:")
print(f"    âœ“ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå°‘ãªã„")
print(f"    âœ— é€æ¬¡å‡¦ç†ã§é…ã„")
print(f"    âœ— é•·æœŸä¾å­˜æ€§ãŒå¼±ã„")
print(f"\n  Self-Attention:")
print(f"    âœ“ ä¸¦åˆ—å‡¦ç†ã§é«˜é€Ÿ")
print(f"    âœ“ é•·è·é›¢ä¾å­˜æ€§ã‚’ç›´æ¥æ‰ãˆã‚‹")
print(f"    âœ— ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¤šã„ï¼ˆO(nÂ²)ã®ãƒ¡ãƒ¢ãƒªï¼‰")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== RNN vs Self-Attention æ¯”è¼ƒ ===

å‡¦ç†æ™‚é–“ï¼ˆå¹³å‡ï¼‰:
  RNN: 12.34ms
  Self-Attention: 8.76ms
  é«˜é€ŸåŒ–ç‡: 1.41x

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°:
  RNN: 2,826,498
  Self-Attention: 3,150,338

ç‰¹å¾´:
  RNN:
    âœ“ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå°‘ãªã„
    âœ— é€æ¬¡å‡¦ç†ã§é…ã„
    âœ— é•·æœŸä¾å­˜æ€§ãŒå¼±ã„

  Self-Attention:
    âœ“ ä¸¦åˆ—å‡¦ç†ã§é«˜é€Ÿ
    âœ“ é•·è·é›¢ä¾å­˜æ€§ã‚’ç›´æ¥æ‰ãˆã‚‹
    âœ— ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¤šã„ï¼ˆO(nÂ²)ã®ãƒ¡ãƒ¢ãƒªï¼‰
</code></pre>

<hr>

<h2>ã¾ã¨ã‚</h2>

<p>ã“ã®ç« ã§ã¯ã€Self-Attentionã¨Multi-Head Attentionã®åŸºç¤ã‚’å­¦ç¿’ã—ã¾ã—ãŸã€‚</p>

<h3>é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ</h3>

<ul>
<li><strong>RNNã®é™ç•Œ</strong>ï¼šé€æ¬¡å‡¦ç†ã€é•·æœŸä¾å­˜æ€§ã®å•é¡Œã€ä¸¦åˆ—åŒ–ä¸å¯</li>
<li><strong>Self-Attention</strong>ï¼šQueryã€Keyã€Valueã§å…¨å˜èªé–“ã®é–¢ä¿‚ã‚’ä¸¦åˆ—ã«è¨ˆç®—</li>
<li><strong>Scaled Dot-Product</strong>ï¼š$\text{Attention}(Q,K,V) = \text{softmax}(QK^T/\sqrt{d_k})V$</li>
<li><strong>Multi-Head Attention</strong>ï¼šè¤‡æ•°ã®è¦–ç‚¹ã‹ã‚‰æ–‡è„ˆã‚’æ‰ãˆã€è¡¨ç¾èƒ½åŠ›ã‚’å‘ä¸Š</li>
<li><strong>Position Encoding</strong>ï¼šå˜èªã®é †åºæƒ…å ±ã‚’æ˜ç¤ºçš„ã«è¿½åŠ </li>
<li><strong>ä¸¦åˆ—å‡¦ç†</strong>ï¼šRNNã‚ˆã‚Šé«˜é€Ÿã§ã€é•·è·é›¢ä¾å­˜æ€§ã‚’ç›´æ¥æ‰ãˆã‚‹</li>
</ul>

<h3>æ¬¡ç« ã®äºˆå‘Š</h3>

<p>ç¬¬2ç« ã§ã¯ã€ä»¥ä¸‹ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’æ‰±ã„ã¾ã™ï¼š</p>
<ul>
<li>Transformer Encoderã®å®Œå…¨ãªæ§‹é€ </li>
<li>Feed-Forward Networkã¨Layer Normalization</li>
<li>Residual Connectionã®å½¹å‰²</li>
<li>Transformer Decoderã¨ãƒã‚¹ã‚¯æ©Ÿæ§‹</li>
<li>å®Œå…¨ãªTransformerãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<details>
<summary><strong>æ¼”ç¿’1ï¼šAttentioné‡ã¿ã®æ‰‹è¨ˆç®—</strong></summary>

<p><strong>å•é¡Œ</strong>ï¼šä»¥ä¸‹ã®ç°¡ç•¥åŒ–ã•ã‚ŒãŸQueryã€Keyã€Valueã§self-attentionã‚’æ‰‹è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚</p>

<p>3å˜èªã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã€å„2æ¬¡å…ƒï¼š</p>
<pre><code>Q = [[1, 0], [0, 1], [1, 1]]
K = [[1, 0], [0, 1], [1, 1]]
V = [[2, 0], [0, 2], [1, 1]]
</code></pre>

<p>ã‚¹ãƒ†ãƒƒãƒ—ï¼š</p>
<ol>
<li>ã‚¹ã‚³ã‚¢è¡Œåˆ— $S = QK^T$ ã‚’è¨ˆç®—</li>
<li>ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆ$d_k=2$ï¼‰</li>
<li>Softmaxï¼ˆç°¡ç•¥åŒ–ã®ãŸã‚è¨ˆç®—ã—ã‚„ã™ã„å€¤ã§ï¼‰</li>
<li>å‡ºåŠ› $AV$ ã‚’è¨ˆç®—</li>
</ol>

<p><strong>è§£ç­”</strong>ï¼š</p>
<pre><code># ã‚¹ãƒ†ãƒƒãƒ—1: ã‚¹ã‚³ã‚¢è¨ˆç®— QK^T
Q = [[1, 0], [0, 1], [1, 1]]
K = [[1, 0], [0, 1], [1, 1]]

S = QK^T = [[1*1+0*0, 1*0+0*1, 1*1+0*1],
            [0*1+1*0, 0*0+1*1, 0*1+1*1],
            [1*1+1*0, 1*0+1*1, 1*1+1*1]]
         = [[1, 0, 1],
            [0, 1, 1],
            [1, 1, 2]]

# ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆd_k=2ãªã®ã§âˆš2ã§å‰²ã‚‹ï¼‰
S_scaled = [[1/âˆš2, 0, 1/âˆš2],
            [0, 1/âˆš2, 1/âˆš2],
            [1/âˆš2, 1/âˆš2, 2/âˆš2]]
         â‰ˆ [[0.71, 0, 0.71],
            [0, 0.71, 0.71],
            [0.71, 0.71, 1.41]]

# ã‚¹ãƒ†ãƒƒãƒ—3: Softmaxï¼ˆå„è¡Œï¼‰
# ç¬¬1è¡Œ: exp([0.71, 0, 0.71]) = [2.03, 1.00, 2.03]
# åˆè¨ˆ = 5.06 â†’ [0.40, 0.20, 0.40]

A â‰ˆ [[0.40, 0.20, 0.40],
     [0.20, 0.40, 0.40],
     [0.28, 0.28, 0.44]]

# ã‚¹ãƒ†ãƒƒãƒ—4: å‡ºåŠ› AV
V = [[2, 0], [0, 2], [1, 1]]

Output = AV
ç¬¬1å˜èª: 0.40*[2,0] + 0.20*[0,2] + 0.40*[1,1] = [1.2, 0.8]
ç¬¬2å˜èª: 0.20*[2,0] + 0.40*[0,2] + 0.40*[1,1] = [0.8, 1.2]
ç¬¬3å˜èª: 0.28*[2,0] + 0.28*[0,2] + 0.44*[1,1] = [1.0, 1.0]

ç­”ãˆ: Output â‰ˆ [[1.2, 0.8], [0.8, 1.2], [1.0, 1.0]]
</code></pre>
</details>

<details>
<summary><strong>æ¼”ç¿’2ï¼šMulti-Head Attentionã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°</strong></summary>

<p><strong>å•é¡Œ</strong>ï¼šä»¥ä¸‹ã®è¨­å®šã®Multi-Head Attentionã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚</p>

<ul>
<li>$d_{\text{model}} = 512$</li>
<li>$h = 8$ï¼ˆãƒ˜ãƒƒãƒ‰æ•°ï¼‰</li>
<li>$d_k = d_v = d_{\text{model}} / h = 64$</li>
</ul>

<p><strong>è§£ç­”</strong>ï¼š</p>
<pre><code># å„ãƒ˜ãƒƒãƒ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
# W^Q, W^K, W^V: å„ (d_model Ã— d_k) Ã— h ãƒ˜ãƒƒãƒ‰åˆ†

# å®Ÿè£…ã§ã¯ã€å…¨ãƒ˜ãƒƒãƒ‰åˆ†ã‚’1ã¤ã®è¡Œåˆ—ã§è¡¨ç¾
W_q: d_model Ã— d_model = 512 Ã— 512 = 262,144
W_k: d_model Ã— d_model = 512 Ã— 512 = 262,144
W_v: d_model Ã— d_model = 512 Ã— 512 = 262,144

# å‡ºåŠ›æŠ•å½±
W_o: d_model Ã— d_model = 512 Ã— 512 = 262,144

# åˆè¨ˆï¼ˆãƒã‚¤ã‚¢ã‚¹ãªã—ã®å ´åˆï¼‰
Total = 262,144 Ã— 4 = 1,048,576

ç­”ãˆ: 1,048,576 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
</code></pre>
</details>

<details>
<summary><strong>æ¼”ç¿’3ï¼šPosition Encodingã®å‘¨æœŸæ€§</strong></summary>

<p><strong>å•é¡Œ</strong>ï¼šSinusoidal Position Encodingã§ã€æ¬¡å…ƒ0ï¼ˆæœ€ã‚‚é«˜å‘¨æ³¢ï¼‰ã®å‘¨æœŸã‚’æ±‚ã‚ã¦ãã ã•ã„ã€‚</p>

<p>æ•°å¼ï¼š$PE_{(pos, 0)} = \sin(pos / 10000^0) = \sin(pos)$</p>

<p><strong>è§£ç­”</strong>ï¼š</p>
<pre><code># æ¬¡å…ƒ0ã®å¼
PE(pos, 0) = sin(pos)

# sinã®å‘¨æœŸã¯2Ï€
# pos ãŒ 2Ï€ å¢—ãˆã‚‹ã”ã¨ã«åŒã˜å€¤ã«æˆ»ã‚‹

å‘¨æœŸ = 2Ï€ â‰ˆ 6.28

# ã“ã‚Œã¯ä½ç½®6.28ã”ã¨ã«ç¹°ã‚Šè¿”ã™ã“ã¨ã‚’æ„å‘³ã™ã‚‹
# å®Ÿéš›ã®å˜èªä½ç½®ã¯æ•´æ•°ãªã®ã§ã€ç´„6å˜èªã”ã¨ã«ä¼¼ãŸå€¤

# æ¬¡å…ƒãŒé«˜ããªã‚‹ã»ã©å‘¨æœŸãŒé•·ããªã‚‹
# æ¬¡å…ƒi: å‘¨æœŸ = 2Ï€ Ã— 10000^(2i/d_model)

# d_model=512, æ¬¡å…ƒ256ï¼ˆæœ€ã‚‚ä½å‘¨æ³¢ï¼‰ã®å ´åˆ
å‘¨æœŸ_æœ€ä½ = 2Ï€ Ã— 10000^(512/512) = 2Ï€ Ã— 10000 â‰ˆ 62,832

ç­”ãˆ: æ¬¡å…ƒ0ã¯ç´„6ã€æ¬¡å…ƒ256ã¯ç´„62,832ã®å‘¨æœŸ
ã“ã‚Œã«ã‚ˆã‚Šæ§˜ã€…ãªã‚¹ã‚±ãƒ¼ãƒ«ã®ä½ç½®æƒ…å ±ã‚’è¡¨ç¾ã§ãã‚‹
</code></pre>
</details>

<details>
<summary><strong>æ¼”ç¿’4ï¼šMasked Self-Attentionã®å®Ÿè£…</strong></summary>

<p><strong>å•é¡Œ</strong>ï¼šDecoderç”¨ã®Masked Self-Attentionï¼ˆæœªæ¥ã®å˜èªã‚’è¦‹ãªã„ã‚ˆã†ã«ã™ã‚‹ï¼‰ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>

<p><strong>è§£ç­”ä¾‹</strong>ï¼š</p>
<pre><code class="language-python">import torch
import torch.nn.functional as F
import numpy as np

def create_causal_mask(seq_len):
    """
    å› æœãƒã‚¹ã‚¯ã‚’ç”Ÿæˆï¼ˆä¸Šä¸‰è§’è¡Œåˆ—ï¼‰

    Returns:
    --------
    mask : torch.Tensor (seq_len, seq_len)
        ä¸‹ä¸‰è§’ãŒ1ã€ä¸Šä¸‰è§’ãŒ0ã®ãƒã‚¹ã‚¯
    """
    mask = torch.tril(torch.ones(seq_len, seq_len))
    return mask

def masked_scaled_dot_product_attention(Q, K, V):
    """
    Masked Scaled Dot-Product Attention
    """
    seq_len = Q.size(1)
    d_k = Q.size(-1)

    # ã‚¹ã‚³ã‚¢è¨ˆç®—
    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)

    # å› æœãƒã‚¹ã‚¯ã®é©ç”¨
    mask = create_causal_mask(seq_len).to(Q.device)
    scores = scores.masked_fill(mask == 0, -1e9)

    # Softmax
    attn_weights = F.softmax(scores, dim=-1)

    # å‡ºåŠ›
    output = torch.matmul(attn_weights, V)

    return output, attn_weights

# ãƒ†ã‚¹ãƒˆ
Q = K = V = torch.randn(1, 5, 8)
output, attn = masked_scaled_dot_product_attention(Q, K, V)

print("Masked Attentioné‡ã¿:")
print(attn[0])
print("\nä¸‹ä¸‰è§’ã®ã¿éã‚¼ãƒ­ï¼ˆæœªæ¥ã‚’è¦‹ãªã„ï¼‰")
</code></pre>
</details>

<details>
<summary><strong>æ¼”ç¿’5ï¼šSelf-Attentionã®è¨ˆç®—é‡åˆ†æ</strong></summary>

<p><strong>å•é¡Œ</strong>ï¼šSelf-Attentionã®è¨ˆç®—é‡ã‚’åˆ†æã—ã€RNNã¨æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>

<p>ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•· $n$ã€ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ $d$ ã¨ã—ã¾ã™ã€‚</p>

<p><strong>è§£ç­”</strong>ï¼š</p>
<pre><code># Self-Attentionã®è¨ˆç®—é‡

1. Q, K, V ã®è¨ˆç®—: 3 Ã— (n Ã— d Ã— d) = O(ndÂ²)
   å„å˜èªã‚’dæ¬¡å…ƒã‹ã‚‰dæ¬¡å…ƒã¸ç·šå½¢å¤‰æ›

2. QK^T ã®è¨ˆç®—: n Ã— n Ã— d = O(nÂ²d)
   (nÃ—d) @ (dÃ—n) = (nÃ—n)

3. Softmax: O(nÂ²)
   nÃ—nè¡Œåˆ—ã®å„è¡Œ

4. Attention Ã— V: n Ã— n Ã— d = O(nÂ²d)
   (nÃ—n) @ (nÃ—d) = (nÃ—d)

åˆè¨ˆ: O(ndÂ² + nÂ²d)

# æ”¯é…çš„ãªé …ã¯
- n < d ã®å ´åˆ: O(ndÂ²)
- n > d ã®å ´åˆ: O(nÂ²d)

# RNNã®è¨ˆç®—é‡
å„æ™‚åˆ»ã§: d Ã— d ï¼ˆéš ã‚ŒçŠ¶æ…‹ã®æ›´æ–°ï¼‰
næ™‚åˆ»åˆ†: n Ã— dÂ² = O(ndÂ²)

# æ¯”è¼ƒ
Self-Attention: O(nÂ²d)ï¼ˆnãŒå¤§ãã„ã¨ãï¼‰
RNN: O(ndÂ²)ï¼ˆå¸¸ã«ï¼‰

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
Self-Attention: O(nÂ²)ï¼ˆAttentionè¡Œåˆ—ï¼‰
RNN: O(n)ï¼ˆå„æ™‚åˆ»ã®éš ã‚ŒçŠ¶æ…‹ï¼‰

ç­”ãˆ:
- Self-Attentionã¯é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§è¨ˆç®—é‡ãƒ»ãƒ¡ãƒ¢ãƒªãŒå¢—å¤§ï¼ˆnÂ²ï¼‰
- RNNã¯é€æ¬¡å‡¦ç†ãŒå¿…è¦ã§ä¸¦åˆ—åŒ–ä¸å¯
- çŸ­ã€œä¸­ç¨‹åº¦ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼ˆn < 512ç¨‹åº¦ï¼‰ã§ã¯
  Self-AttentionãŒä¸¦åˆ—å‡¦ç†ã§é«˜é€Ÿ
</code></pre>
</details>

<hr>

<div class="navigation">
    <a href="../index.html" class="nav-button">ğŸ“š ã‚³ãƒ¼ã‚¹ç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter2-transformer-architecture.html" class="nav-button">æ¬¡ã®ç« ã¸ï¼šTransformerå®Œå…¨æ§‹é€  â†’</a>
</div>

</main>


    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
    <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
</footer>

</body>
</html>
