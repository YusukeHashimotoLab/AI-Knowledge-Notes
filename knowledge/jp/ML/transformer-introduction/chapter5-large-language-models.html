<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
<meta content="ç¬¬5ç« ï¼šå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« (Large Language Models) - AI Terakoya" name="description"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬5ç« ï¼šå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« (Large Language Models) - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/transformer-introduction/chapter5-large-language-models.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/transformer-introduction/index.html">Transformer</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 5</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬5ç« ï¼šå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« (Large Language Models)</h1>
            <p class="subtitle">LLMã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã‹ã‚‰å®Ÿè·µçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã¾ã§</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 30-35åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 10å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã¨æ€§èƒ½ç‰¹æ€§ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… GPTã€LLaMAã€Claudeã€Geminiãªã©ä¸»è¦ãªLLMã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¯”è¼ƒã§ãã‚‹</li>
<li>âœ… Zero-shotã€Few-shotã€Chain-of-Thoughtãªã©ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æŠ€è¡“ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… In-Context Learningã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¨åŠ¹æœçš„ãªæ´»ç”¨æ–¹æ³•ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… RLHFã«ã‚ˆã‚‹äººé–“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æ´»ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«æ”¹å–„æ‰‹æ³•ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… å®Ÿç”¨çš„ãªLLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã—ã€APIã‚’çµ±åˆã§ãã‚‹</li>
</ul>

<hr>

<h2>5.1 LLMã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡</h2>

<h3>å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ã¯</h3>

<p><strong>å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLarge Language Model, LLMï¼‰</strong>ã¯ã€è†¨å¤§ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’ã•ã‚ŒãŸå·¨å¤§ãªTransformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚æ•°åå„„ã€œæ•°åƒå„„ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¡ã€æ§˜ã€…ãªè‡ªç„¶è¨€èªå‡¦ç†ã‚¿ã‚¹ã‚¯ã‚’é«˜ç²¾åº¦ã§å®Ÿè¡Œã§ãã¾ã™ã€‚</p>

<div class="mermaid">
graph TB
    A[è¨€èªãƒ¢ãƒ‡ãƒ«ã®é€²åŒ–] --> B[å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«<br/>~100M params<br/>2018-2019]
    A --> C[ä¸­è¦æ¨¡ãƒ¢ãƒ‡ãƒ«<br/>1B-10B params<br/>2019-2020]
    A --> D[å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«<br/>100B+ params<br/>2020-present]

    B --> B1[BERT Base<br/>110M]
    B --> B2[GPT-2<br/>117M-1.5B]

    C --> C1[GPT-3<br/>175B]
    C --> C2[T5<br/>11B]

    D --> D1[GPT-4<br/>~1.7Tæ¨å®š]
    D --> D2[Claude 3<br/>éå…¬é–‹]
    D --> D3[Gemini Ultra<br/>éå…¬é–‹]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#fff9c4
    style D fill:#c8e6c9
</div>

<h3>ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ï¼ˆScaling Lawsï¼‰</h3>

<p>2020å¹´ã«OpenAIãŒç™ºè¡¨ã—ãŸ<strong>ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡</strong>ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ãŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€ãƒ‡ãƒ¼ã‚¿é‡ã€è¨ˆç®—é‡ã«å¯¾ã—ã¦ã©ã®ã‚ˆã†ã«ã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹ã‹ã‚’å®šé‡åŒ–ã—ãŸã‚‚ã®ã§ã™ã€‚</p>

<h4>åŸºæœ¬çš„ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡</h4>

<p>ãƒ¢ãƒ‡ãƒ«ã®æå¤±ï¼ˆLossï¼‰$L$ã¯ä»¥ä¸‹ã®3ã¤ã®è¦å› ã§æ±ºã¾ã‚Šã¾ã™ï¼š</p>

<p>$$
L(N, D, C) = \left(\frac{N_c}{N}\right)^{\alpha_N} + \left(\frac{D_c}{D}\right)^{\alpha_D} + \left(\frac{C_c}{C}\right)^{\alpha_C}
$$</p>

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$N$: ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ï¼ˆéåŸ‹ã‚è¾¼ã¿å±¤ï¼‰</li>
<li>$D$: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°</li>
<li>$C$: è¨ˆç®—é‡ï¼ˆFLOPsï¼‰</li>
<li>$N_c, D_c, C_c$: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®šæ•°</li>
<li>$\alpha_N \approx 0.076, \alpha_D \approx 0.095, \alpha_C \approx 0.050$</li>
</ul>

<table>
<thead>
<tr>
<th>ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°è¦å› </th>
<th>å½±éŸ¿</th>
<th>å®Ÿç”¨çš„ãªæ„å‘³</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºï¼ˆNï¼‰</strong></td>
<td>10å€ã§æå¤±ãŒ~0.95å€</td>
<td>å¤§ãã„ãƒ¢ãƒ‡ãƒ«ã»ã©æ€§èƒ½å‘ä¸Š</td>
</tr>
<tr>
<td><strong>ãƒ‡ãƒ¼ã‚¿é‡ï¼ˆDï¼‰</strong></td>
<td>10å€ã§æå¤±ãŒ~0.93å€</td>
<td>ãƒ‡ãƒ¼ã‚¿ã®é‡è¦æ€§ãŒæœ€ã‚‚é«˜ã„</td>
</tr>
<tr>
<td><strong>è¨ˆç®—é‡ï¼ˆCï¼‰</strong></td>
<td>10å€ã§æå¤±ãŒ~0.97å€</td>
<td>åŠ¹ç‡çš„ãªè¨ˆç®—ãŒéµ</td>
</tr>
</tbody>
</table>

<blockquote>
<p><strong>Chinchillaè«–æ–‡ã®ç™ºè¦‹ï¼ˆ2022ï¼‰</strong>: DeepMindã®ç ”ç©¶ã«ã‚ˆã‚Šã€å¤šãã®LLMã¯ã€Œéå‰°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã€ã•ã‚Œã¦ãŠã‚Šã€åŒã˜è¨ˆç®—äºˆç®—ã§ã‚ˆã‚Šå°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‚’ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã™ã‚‹æ–¹ãŒåŠ¹ç‡çš„ã§ã‚ã‚‹ã“ã¨ãŒåˆ¤æ˜ã—ã¾ã—ãŸã€‚æœ€é©ãªæ¯”ç‡ã¯<strong>ãƒ‡ãƒ¼ã‚¿ãƒˆãƒ¼ã‚¯ãƒ³æ•° â‰ˆ 20 Ã— ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°</strong>ã§ã™ã€‚</p>
</blockquote>

<h3>Emergent Abilitiesï¼ˆå‰µç™ºèƒ½åŠ›ï¼‰</h3>

<p>LLMã¯ä¸€å®šè¦æ¨¡ã‚’è¶…ãˆã‚‹ã¨ã€æ˜ç¤ºçš„ã«è¨“ç·´ã•ã‚Œã¦ã„ãªã„èƒ½åŠ›ãŒ<strong>çªç„¶å‡ºç¾</strong>ã—ã¾ã™ã€‚ã“ã‚Œã‚’<strong>å‰µç™ºèƒ½åŠ›</strong>ã¨å‘¼ã³ã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®å¢—åŠ ] --> B[~1B parameters]
    B --> C[åŸºæœ¬çš„ãªæ–‡ç”Ÿæˆ]

    A --> D[~10B parameters]
    D --> E[Few-shot Learning<br/>ç°¡å˜ãªæ¨è«–]

    A --> F[~100B parameters]
    F --> G[Chain-of-Thought<br/>è¤‡é›‘ãªæ¨è«–<br/>æŒ‡ç¤ºè¿½å¾“]

    style A fill:#e3f2fd
    style G fill:#c8e6c9
</div>

<table>
<thead>
<tr>
<th>å‰µç™ºèƒ½åŠ›</th>
<th>å‡ºç¾è¦æ¨¡</th>
<th>èª¬æ˜</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>In-Context Learning</strong></td>
<td>~10B+</td>
<td>ä¾‹ç¤ºã‹ã‚‰ã‚¿ã‚¹ã‚¯ã‚’å­¦ç¿’</td>
</tr>
<tr>
<td><strong>Chain-of-Thought</strong></td>
<td>~100B+</td>
<td>æ®µéšçš„æ¨è«–ãŒå¯èƒ½</td>
</tr>
<tr>
<td><strong>æŒ‡ç¤ºè¿½å¾“ï¼ˆInstruction Followingï¼‰</strong></td>
<td>~10B+ï¼ˆRLHFå¾Œï¼‰</td>
<td>è‡ªç„¶è¨€èªæŒ‡ç¤ºã®ç†è§£</td>
</tr>
<tr>
<td><strong>å¤šè¨€èªèƒ½åŠ›</strong></td>
<td>~10B+</td>
<td>æœªå­¦ç¿’è¨€èªã¸ã®è»¢ç§»</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

def scaling_law_loss(N, D, C, N_c=8.8e13, D_c=5.4e13, C_c=1.3e13,
                     alpha_N=0.076, alpha_D=0.095, alpha_C=0.050):
    """
    ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã«åŸºã¥ã„ã¦æå¤±ã‚’è¨ˆç®—

    Args:
        N: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
        D: ãƒ‡ãƒ¼ã‚¿ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        C: è¨ˆç®—é‡ï¼ˆFLOPsï¼‰
        ãã®ä»–: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®šæ•°

    Returns:
        äºˆæ¸¬ã•ã‚Œã‚‹æå¤±å€¤
    """
    loss_N = (N_c / N) ** alpha_N
    loss_D = (D_c / D) ** alpha_D
    loss_C = (C_c / C) ** alpha_C
    return loss_N + loss_D + loss_C

# ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®å½±éŸ¿ã‚’å¯è¦–åŒ–
param_counts = np.logspace(6, 12, 50)  # 1M to 1T parameters
data_tokens = 1e12  # 1T tokenså›ºå®š
compute = 1e21  # å›ºå®š

losses = [scaling_law_loss(N, data_tokens, compute) for N in param_counts]

plt.figure(figsize=(12, 5))

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° vs æå¤±
plt.subplot(1, 2, 1)
plt.loglog(param_counts, losses, linewidth=2, color='#7b2cbf')
plt.xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°', fontsize=12)
plt.ylabel('æå¤±ï¼ˆLossï¼‰', fontsize=12)
plt.title('ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¨æ€§èƒ½', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)

# ä¸»è¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
models = {
    'GPT-2': (1.5e9, scaling_law_loss(1.5e9, data_tokens, compute)),
    'GPT-3': (175e9, scaling_law_loss(175e9, data_tokens, compute)),
    'GPT-4 (æ¨å®š)': (1.7e12, scaling_law_loss(1.7e12, data_tokens, compute)),
}

for name, (params, loss) in models.items():
    plt.scatter(params, loss, s=100, zorder=5)
    plt.annotate(name, (params, loss), xytext=(10, 10),
                textcoords='offset points', fontsize=9)

# ãƒ‡ãƒ¼ã‚¿é‡ã®å½±éŸ¿
plt.subplot(1, 2, 2)
data_amounts = np.logspace(9, 13, 50)
model_size = 100e9  # 100B parameterså›ºå®š

losses_data = [scaling_law_loss(model_size, D, compute) for D in data_amounts]
plt.loglog(data_amounts, losses_data, linewidth=2, color='#3182ce')
plt.xlabel('è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒˆãƒ¼ã‚¯ãƒ³æ•°', fontsize=12)
plt.ylabel('æå¤±ï¼ˆLossï¼‰', fontsize=12)
plt.title('ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡: ãƒ‡ãƒ¼ã‚¿é‡ã¨æ€§èƒ½', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)

# Chinchillaæœ€é©ç‚¹ã‚’ãƒãƒ¼ã‚¯
optimal_data = 20 * model_size  # 20x rule
optimal_loss = scaling_law_loss(model_size, optimal_data, compute)
plt.scatter(optimal_data, optimal_loss, s=150, c='red', marker='*',
           zorder=5, label='Chinchillaæœ€é©ç‚¹')
plt.legend()

plt.tight_layout()
plt.show()

# å®Ÿç”¨ä¾‹ï¼šå¿…è¦ãªè¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã®æ¨å®š
def estimate_training_cost(params, tokens, efficiency=0.5):
    """
    è¨“ç·´ã‚³ã‚¹ãƒˆã‚’æ¨å®šï¼ˆFLOPsï¼‰

    Args:
        params: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
        tokens: è¨“ç·´ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        efficiency: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢åŠ¹ç‡ï¼ˆ0-1ï¼‰

    Returns:
        å¿…è¦ãªFLOPsã€GPUæ™‚é–“ã®æ¨å®š
    """
    # 1ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šç´„6 Ã— params FLOPs
    total_flops = 6 * params * tokens

    # A100 GPU: ~312 TFLOPSï¼ˆFP16ï¼‰
    a100_flops = 312e12 * efficiency
    gpu_hours = total_flops / a100_flops / 3600

    return total_flops, gpu_hours

# GPT-3ã‚¯ãƒ©ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã®ã‚³ã‚¹ãƒˆæ¨å®š
params_gpt3 = 175e9
tokens_gpt3 = 300e9

total_flops, gpu_hours = estimate_training_cost(params_gpt3, tokens_gpt3)

print(f"\n=== GPT-3è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚³ã‚¹ãƒˆæ¨å®š ===")
print(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {params_gpt3/1e9:.1f}B")
print(f"è¨“ç·´ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {tokens_gpt3/1e9:.1f}B")
print(f"ç·è¨ˆç®—é‡: {total_flops:.2e} FLOPs")
print(f"A100 GPUæ™‚é–“: {gpu_hours:,.0f} æ™‚é–“ ({gpu_hours/24:,.0f} æ—¥)")
print(f"GPUæ•°ï¼ˆ30æ—¥ã§å®Œäº†ï¼‰: {int(np.ceil(gpu_hours / (24 * 30)))} å°")
</code></pre>

<hr>

<h2>5.2 ä»£è¡¨çš„ãªLLM</h2>

<h3>GPTã‚·ãƒªãƒ¼ã‚ºï¼ˆOpenAIï¼‰</h3>

<h4>GPT-3ï¼ˆ2020ï¼‰</h4>

<p><strong>GPT-3</strong>ï¼ˆGenerative Pre-trained Transformer 3ï¼‰ã¯ã€175Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤è‡ªå·±å›å¸°å‹è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã€Few-shot Learningã®æœ‰åŠ¹æ€§ã‚’å®Ÿè¨¼ã—ã¾ã—ãŸã€‚</p>

<table>
<thead>
<tr>
<th>ç‰¹å¾´</th>
<th>è©³ç´°</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°</strong></td>
<td>175Bï¼ˆæœ€å¤§ç‰ˆï¼‰</td>
</tr>
<tr>
<td><strong>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</strong></td>
<td>Decoder-only Transformerã€96å±¤ã€12,288æ¬¡å…ƒ</td>
</tr>
<tr>
<td><strong>è¨“ç·´ãƒ‡ãƒ¼ã‚¿</strong></td>
<td>Common Crawlã€WebTextã€Booksã€Wikipediaãªã© ~300B tokens</td>
</tr>
<tr>
<td><strong>ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·</strong></td>
<td>2,048 tokens</td>
</tr>
<tr>
<td><strong>é©æ–°æ€§</strong></td>
<td>Few-shot Learningã®å®Ÿè¨¼ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ™ãƒ¼ã‚¹ã®æ±ç”¨æ€§</td>
</tr>
</tbody>
</table>

<h4>GPT-4ï¼ˆ2023ï¼‰</h4>

<p><strong>GPT-4</strong>ã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ï¼ˆãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒï¼‰å¯¾å¿œã®æœ€å…ˆç«¯ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯éå…¬é–‹ã§ã™ãŒã€æ¨å®š1.7å…†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã•ã‚Œã¦ã„ã¾ã™ã€‚</p>

<table>
<thead>
<tr>
<th>èƒ½åŠ›</th>
<th>GPT-3.5</th>
<th>GPT-4</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ç±³å›½å¸æ³•è©¦é¨“</strong></td>
<td>ä¸‹ä½10%</td>
<td>ä¸Šä½10%</td>
</tr>
<tr>
<td><strong>æ•°å­¦ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯</strong></td>
<td>ä¸åˆæ ¼</td>
<td>ä¸Šä½500ä½ç›¸å½“</td>
</tr>
<tr>
<td><strong>ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°èƒ½åŠ›</strong></td>
<td>åŸºæœ¬çš„ãªå®Ÿè£…</td>
<td>è¤‡é›‘ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è¨­è¨ˆ</td>
</tr>
<tr>
<td><strong>ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«</strong></td>
<td>ãƒ†ã‚­ã‚¹ãƒˆã®ã¿</td>
<td>ãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒç†è§£</td>
</tr>
</tbody>
</table>

<h3>LLaMAã‚·ãƒªãƒ¼ã‚ºï¼ˆMetaï¼‰</h3>

<p><strong>LLaMA</strong>ï¼ˆLarge Language Model Meta AIï¼‰ã¯ã€MetaãŒã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§å…¬é–‹ã—ãŸåŠ¹ç‡çš„ãªLLMãƒ•ã‚¡ãƒŸãƒªãƒ¼ã§ã™ã€‚</p>

<h4>LLaMA 2ã®ç‰¹å¾´</h4>

<ul>
<li><strong>ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º</strong>: 7Bã€13Bã€70Bã®3ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³</li>
<li><strong>è¨“ç·´ãƒ‡ãƒ¼ã‚¿</strong>: 2å…†ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå…¬é–‹ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰</li>
<li><strong>ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·</strong>: 4,096 tokens</li>
<li><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: å•†ç”¨åˆ©ç”¨å¯èƒ½ï¼ˆæ¡ä»¶ä»˜ãï¼‰</li>
<li><strong>æœ€é©åŒ–</strong>: Chinchillaã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã«åŸºã¥ãåŠ¹ç‡çš„è¨­è¨ˆ</li>
</ul>

<div class="mermaid">
graph TB
    A[LLaMA 2ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£] --> B[Pre-normalization<br/>RMSNorm]
    A --> C[SwiGLU activation<br/>PaLMã‹ã‚‰æ¡ç”¨]
    A --> D[Rotary Positional<br/>Embedding RoPE]
    A --> E[Grouped Query<br/>Attention GQA]

    B --> F[è¨“ç·´å®‰å®šæ€§å‘ä¸Š]
    C --> G[æ€§èƒ½å‘ä¸Š]
    D --> H[é•·æ–‡å¯¾å¿œ]
    E --> I[æ¨è«–é«˜é€ŸåŒ–]

    style A fill:#e3f2fd
    style F fill:#c8e6c9
    style G fill:#c8e6c9
    style H fill:#c8e6c9
    style I fill:#c8e6c9
</div>

<h3>Claudeï¼ˆAnthropicï¼‰</h3>

<p><strong>Claude</strong>ã¯ã€AnthropicãŒé–‹ç™ºã—ãŸã€ŒConstitutional AIã€ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚‹å®‰å…¨æ€§é‡è¦–ã®LLMã§ã™ã€‚</p>

<table>
<thead>
<tr>
<th>ãƒ¢ãƒ‡ãƒ«</th>
<th>ç‰¹å¾´</th>
<th>ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Claude 3 Opus</strong></td>
<td>æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã€è¤‡é›‘ãªæ¨è«–</td>
<td>200K tokens</td>
</tr>
<tr>
<td><strong>Claude 3 Sonnet</strong></td>
<td>ãƒãƒ©ãƒ³ã‚¹å‹ã€é«˜é€Ÿå¿œç­”</td>
<td>200K tokens</td>
</tr>
<tr>
<td><strong>Claude 3 Haiku</strong></td>
<td>è»½é‡ãƒ»é«˜é€Ÿã€ã‚³ã‚¹ãƒˆåŠ¹ç‡</td>
<td>200K tokens</td>
</tr>
</tbody>
</table>

<blockquote>
<p><strong>Constitutional AI</strong>: äººé–“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼ˆRLHFï¼‰ã«åŠ ãˆã¦ã€AIãŒè‡ªå·±æ‰¹åˆ¤ãƒ»è‡ªå·±æ”¹å–„ã‚’è¡Œã†ã€Œæ†²æ³•ã€ã«åŸºã¥ã„ãŸè¨“ç·´æ‰‹æ³•ã€‚æœ‰å®³ãªå‡ºåŠ›ã‚’æ¸›ã‚‰ã—ã€ã‚ˆã‚Šå®‰å…¨ã§æœ‰ç›Šãªå¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚</p>
</blockquote>

<h3>Geminiï¼ˆGoogleï¼‰</h3>

<p><strong>Gemini</strong>ã¯ã€GoogleãŒé–‹ç™ºã—ãŸãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒã‚¤ãƒ†ã‚£ãƒ–ãªLLMã§ã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ã€å‹•ç”»ã‚’çµ±åˆçš„ã«å‡¦ç†ã—ã¾ã™ã€‚</p>

<ul>
<li><strong>Gemini Ultra</strong>: æœ€é«˜æ€§èƒ½ã€è¤‡é›‘ãªã‚¿ã‚¹ã‚¯å¯¾å¿œ</li>
<li><strong>Gemini Pro</strong>: æ±ç”¨çš„ãªç”¨é€”ã«æœ€é©åŒ–</li>
<li><strong>Gemini Nano</strong>: ãƒ‡ãƒã‚¤ã‚¹ä¸Šã§å‹•ä½œã™ã‚‹è»½é‡ç‰ˆ</li>
</ul>

<pre><code class="language-python"># ä¸»è¦LLMã®æ¯”è¼ƒå®Ÿè£…ä¾‹ï¼ˆAPIçµŒç”±ï¼‰
import os
from typing import List, Dict
import time

class LLMComparison:
    """
    è¤‡æ•°ã®LLM APIã‚’çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§æ¯”è¼ƒ
    """

    def __init__(self):
        """å„APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—"""
        self.openai_key = os.getenv('OPENAI_API_KEY')
        self.anthropic_key = os.getenv('ANTHROPIC_API_KEY')
        self.google_key = os.getenv('GOOGLE_API_KEY')

    def query_gpt4(self, prompt: str, max_tokens: int = 500) -> Dict:
        """GPT-4ã«ã‚¯ã‚¨ãƒªã‚’é€ä¿¡"""
        try:
            import openai
            openai.api_key = self.openai_key

            start_time = time.time()
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=0.7
            )
            latency = time.time() - start_time

            return {
                'model': 'GPT-4',
                'response': response.choices[0].message.content,
                'tokens': response.usage.total_tokens,
                'latency': latency
            }
        except Exception as e:
            return {'model': 'GPT-4', 'error': str(e)}

    def query_claude(self, prompt: str, max_tokens: int = 500) -> Dict:
        """Claude 3ã«ã‚¯ã‚¨ãƒªã‚’é€ä¿¡"""
        try:
            import anthropic
            client = anthropic.Anthropic(api_key=self.anthropic_key)

            start_time = time.time()
            message = client.messages.create(
                model="claude-3-opus-20240229",
                max_tokens=max_tokens,
                messages=[{"role": "user", "content": prompt}]
            )
            latency = time.time() - start_time

            return {
                'model': 'Claude 3 Opus',
                'response': message.content[0].text,
                'tokens': message.usage.input_tokens + message.usage.output_tokens,
                'latency': latency
            }
        except Exception as e:
            return {'model': 'Claude 3 Opus', 'error': str(e)}

    def query_gemini(self, prompt: str, max_tokens: int = 500) -> Dict:
        """Gemini Proã«ã‚¯ã‚¨ãƒªã‚’é€ä¿¡"""
        try:
            import google.generativeai as genai
            genai.configure(api_key=self.google_key)
            model = genai.GenerativeModel('gemini-pro')

            start_time = time.time()
            response = model.generate_content(prompt)
            latency = time.time() - start_time

            return {
                'model': 'Gemini Pro',
                'response': response.text,
                'tokens': 'N/A',  # Gemini APIã¯è©³ç´°ãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¿”ã•ãªã„
                'latency': latency
            }
        except Exception as e:
            return {'model': 'Gemini Pro', 'error': str(e)}

    def compare_all(self, prompt: str, max_tokens: int = 500) -> List[Dict]:
        """
        å…¨ãƒ¢ãƒ‡ãƒ«ã«åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€ä¿¡ã—ã¦æ¯”è¼ƒ

        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            max_tokens: æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°

        Returns:
            å„ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”çµæœãƒªã‚¹ãƒˆ
        """
        results = []

        print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}\n")
        print("=" * 80)

        # GPT-4
        print("GPT-4ã«ã‚¯ã‚¨ãƒªä¸­...")
        gpt4_result = self.query_gpt4(prompt, max_tokens)
        results.append(gpt4_result)
        self._print_result(gpt4_result)

        # Claude 3
        print("\nClaude 3ã«ã‚¯ã‚¨ãƒªä¸­...")
        claude_result = self.query_claude(prompt, max_tokens)
        results.append(claude_result)
        self._print_result(claude_result)

        # Gemini
        print("\nGemini Proã«ã‚¯ã‚¨ãƒªä¸­...")
        gemini_result = self.query_gemini(prompt, max_tokens)
        results.append(gemini_result)
        self._print_result(gemini_result)

        return results

    def _print_result(self, result: Dict):
        """çµæœã‚’æ•´å½¢ã—ã¦è¡¨ç¤º"""
        if 'error' in result:
            print(f"âŒ {result['model']}: ã‚¨ãƒ©ãƒ¼ - {result['error']}")
        else:
            print(f"âœ… {result['model']}:")
            print(f"   å¿œç­”: {result['response'][:200]}...")
            print(f"   ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {result['tokens']}")
            print(f"   ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: {result['latency']:.2f}ç§’")

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # æ³¨æ„: å®Ÿè¡Œã«ã¯å„APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™
    comparator = LLMComparison()

    # ç°¡å˜ãªæ¯”è¼ƒãƒ†ã‚¹ãƒˆ
    test_prompt = """
    ä»¥ä¸‹ã®å•é¡Œã‚’æ®µéšçš„ã«è§£ã„ã¦ãã ã•ã„ï¼š

    å•é¡Œ: ã‚ã‚‹ä¼šç¤¾ã®å£²ä¸ŠãŒæ¯å¹´20%ãšã¤å¢—åŠ ã—ã¦ã„ã¾ã™ã€‚
    ç¾åœ¨ã®å£²ä¸ŠãŒ1å„„å††ã®å ´åˆã€5å¹´å¾Œã®å£²ä¸Šã¯ã„ãã‚‰ã«ãªã‚Šã¾ã™ã‹ï¼Ÿ
    è¨ˆç®—éç¨‹ã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚
    """

    results = comparator.compare_all(test_prompt, max_tokens=300)

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
    print("\n" + "=" * 80)
    print("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ:")
    for result in results:
        if 'error' not in result:
            print(f"{result['model']:20} - ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: {result['latency']:.2f}ç§’")
</code></pre>

<hr>

<h2>5.3 Prompt Engineering</h2>

<h3>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã¨ã¯</h3>

<p><strong>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</strong>ã¯ã€LLMã‹ã‚‰æœ›ã¾ã—ã„å‡ºåŠ›ã‚’å¼•ãå‡ºã™ãŸã‚ã®å…¥åŠ›è¨­è¨ˆæŠ€è¡“ã§ã™ã€‚é©åˆ‡ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®å†è¨“ç·´ãªã—ã«æ€§èƒ½ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‰ã‚Œã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŠ€è¡“] --> B[Zero-shot]
    A --> C[Few-shot]
    A --> D[Chain-of-Thought]
    A --> E[Self-Consistency]

    B --> B1[æŒ‡ç¤ºã®ã¿ã§å®Ÿè¡Œ]
    C --> C1[ä¾‹ç¤ºã‹ã‚‰å­¦ç¿’]
    D --> D1[æ®µéšçš„æ¨è«–]
    E --> E1[è¤‡æ•°çµŒè·¯ã®çµ±åˆ]

    style A fill:#e3f2fd
    style D fill:#c8e6c9
</div>

<h3>Zero-shot Learning</h3>

<p><strong>Zero-shot Learning</strong>ã¯ã€ä¾‹ç¤ºãªã—ã«æŒ‡ç¤ºã®ã¿ã§ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã•ã›ã‚‹æ‰‹æ³•ã§ã™ã€‚å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®å‰µç™ºèƒ½åŠ›ã«ã‚ˆã‚Šå¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚</p>

<pre><code class="language-python">class ZeroShotPromptEngine:
    """
    Zero-shotãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¨­è¨ˆã¨å®Ÿè¡Œ
    """

    @staticmethod
    def sentiment_analysis(text: str) -> str:
        """æ„Ÿæƒ…åˆ†æã®Zero-shotãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        prompt = f"""
ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…ã‚’ã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã€Œãƒã‚¬ãƒ†ã‚£ãƒ–ã€ã€Œä¸­ç«‹ã€ã®ã„ãšã‚Œã‹ã«åˆ†é¡ã—ã¦ãã ã•ã„ã€‚
åˆ†é¡çµæœã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ: {text}

åˆ†é¡:"""
        return prompt

    @staticmethod
    def text_summarization(text: str, max_words: int = 50) -> str:
        """è¦ç´„ã®Zero-shotãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        prompt = f"""
ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’{max_words}èªä»¥å†…ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚
é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ:
{text}

è¦ç´„:"""
        return prompt

    @staticmethod
    def question_answering(context: str, question: str) -> str:
        """è³ªå•å¿œç­”ã®Zero-shotãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        prompt = f"""
ä»¥ä¸‹ã®æ–‡è„ˆã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚
æ–‡è„ˆã«æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œæƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

æ–‡è„ˆ:
{context}

è³ªå•: {question}

å›ç­”:"""
        return prompt

    @staticmethod
    def language_translation(text: str, target_lang: str = "è‹±èª") -> str:
        """ç¿»è¨³ã®Zero-shotãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        prompt = f"""
ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’{target_lang}ã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚
è‡ªç„¶ã§æ­£ç¢ºãªç¿»è¨³ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ: {text}

ç¿»è¨³:"""
        return prompt

# ä½¿ç”¨ä¾‹
engine = ZeroShotPromptEngine()

# æ„Ÿæƒ…åˆ†æ
text1 = "ã“ã®è£½å“ã¯æœŸå¾…ä»¥ä¸Šã®æ€§èƒ½ã§ã—ãŸã€‚è³¼å…¥ã—ã¦æœ¬å½“ã«è‰¯ã‹ã£ãŸã§ã™ã€‚"
prompt1 = engine.sentiment_analysis(text1)
print("=== Zero-shot æ„Ÿæƒ…åˆ†æ ===")
print(prompt1)
print()

# è¦ç´„
text2 = """
äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ã®ç™ºå±•ã«ã‚ˆã‚Šã€æ§˜ã€…ãªåˆ†é‡ã§é©æ–°ãŒèµ·ãã¦ã„ã¾ã™ã€‚
ç‰¹ã«è‡ªç„¶è¨€èªå‡¦ç†ã®åˆ†é‡ã§ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®ç™»å ´ã«ã‚ˆã‚Šã€
æ©Ÿæ¢°ç¿»è¨³ã€æ–‡ç« ç”Ÿæˆã€è³ªå•å¿œç­”ãªã©ã®ã‚¿ã‚¹ã‚¯ã§äººé–“ã«è¿‘ã„æ€§èƒ½ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚
ä»Šå¾Œã€AIã¯åŒ»ç™‚ã€æ•™è‚²ã€ãƒ“ã‚¸ãƒã‚¹ãªã©ã€ã•ã‚‰ã«å¤šãã®é ˜åŸŸã§æ´»ç”¨ã•ã‚Œã¦ã„ãã§ã—ã‚‡ã†ã€‚
"""
prompt2 = engine.text_summarization(text2, max_words=30)
print("=== Zero-shot è¦ç´„ ===")
print(prompt2)
</code></pre>

<h3>Few-shot Learning</h3>

<p><strong>Few-shot Learning</strong>ã¯ã€å°‘æ•°ã®ä¾‹ç¤ºï¼ˆé€šå¸¸1ã€œ10å€‹ï¼‰ã‚’æç¤ºã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã«ã‚¿ã‚¹ã‚¯ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã•ã›ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<pre><code class="language-python">class FewShotPromptEngine:
    """
    Few-shotãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¨­è¨ˆã¨å®Ÿè¡Œ
    """

    @staticmethod
    def sentiment_analysis(text: str, num_examples: int = 3) -> str:
        """æ„Ÿæƒ…åˆ†æã®Few-shotãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        # ä¾‹ç¤ºãƒ‡ãƒ¼ã‚¿
        examples = [
            ("ã“ã®æ˜ ç”»ã¯ç´ æ™´ã‚‰ã—ã‹ã£ãŸï¼æ„Ÿå‹•ã—ã¾ã—ãŸã€‚", "ãƒã‚¸ãƒ†ã‚£ãƒ–"),
            ("æ–™ç†ãŒå†·ã‚ã¦ã„ã¦ã€ã‚µãƒ¼ãƒ“ã‚¹ã‚‚æ‚ªã‹ã£ãŸã€‚", "ãƒã‚¬ãƒ†ã‚£ãƒ–"),
            ("æ™®é€šã®å•†å“ã§ã™ã€‚ç‰¹ã«è‰¯ãã‚‚æ‚ªãã‚‚ã‚ã‚Šã¾ã›ã‚“ã€‚", "ä¸­ç«‹"),
        ]

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        prompt = "ä»¥ä¸‹ã®ä¾‹ã‚’å‚è€ƒã«ã€ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…ã‚’åˆ†é¡ã—ã¦ãã ã•ã„ã€‚\n\n"

        for i, (example_text, label) in enumerate(examples[:num_examples], 1):
            prompt += f"ä¾‹{i}:\nãƒ†ã‚­ã‚¹ãƒˆ: {example_text}\nåˆ†é¡: {label}\n\n"

        prompt += f"ãƒ†ã‚­ã‚¹ãƒˆ: {text}\nåˆ†é¡:"
        return prompt

    @staticmethod
    def entity_extraction(text: str) -> str:
        """å›ºæœ‰è¡¨ç¾æŠ½å‡ºã®Few-shotãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        prompt = """
ä»¥ä¸‹ã®ä¾‹ã‚’å‚è€ƒã«ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰äººåã€çµ„ç¹”åã€å ´æ‰€ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ä¾‹1:
ãƒ†ã‚­ã‚¹ãƒˆ: ç”°ä¸­å¤ªéƒã•ã‚“ã¯æ±äº¬å¤§å­¦ã§æ©Ÿæ¢°å­¦ç¿’ã‚’ç ”ç©¶ã—ã¦ã„ã¾ã™ã€‚
æŠ½å‡º: äººå=ç”°ä¸­å¤ªéƒ, çµ„ç¹”å=æ±äº¬å¤§å­¦, å ´æ‰€=ãªã—

ä¾‹2:
ãƒ†ã‚­ã‚¹ãƒˆ: Appleã®CEOãƒ†ã‚£ãƒ ãƒ»ã‚¯ãƒƒã‚¯ãŒã‚·ãƒªã‚³ãƒ³ãƒãƒ¬ãƒ¼ã§è¬›æ¼”ã—ã¾ã—ãŸã€‚
æŠ½å‡º: äººå=ãƒ†ã‚£ãƒ ãƒ»ã‚¯ãƒƒã‚¯, çµ„ç¹”å=Apple, å ´æ‰€=ã‚·ãƒªã‚³ãƒ³ãƒãƒ¬ãƒ¼

ä¾‹3:
ãƒ†ã‚­ã‚¹ãƒˆ: Googleã¨MicrosoftãŒæ–°ã—ã„AIæŠ€è¡“ã‚’ç™ºè¡¨ã—ã¾ã—ãŸã€‚
æŠ½å‡º: äººå=ãªã—, çµ„ç¹”å=Google, Microsoft, å ´æ‰€=ãªã—

ãƒ†ã‚­ã‚¹ãƒˆ: {text}
æŠ½å‡º:"""
        return prompt.format(text=text)

    @staticmethod
    def code_generation(task_description: str) -> str:
        """ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã®Few-shotãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        prompt = """
ä»¥ä¸‹ã®ä¾‹ã‚’å‚è€ƒã«ã€ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹Pythoné–¢æ•°ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ä¾‹1:
ã‚¿ã‚¹ã‚¯: ãƒªã‚¹ãƒˆã®è¦ç´ ã‚’2å€ã«ã™ã‚‹
ã‚³ãƒ¼ãƒ‰:
def double_elements(lst):
    return [x * 2 for x in lst]

ä¾‹2:
ã‚¿ã‚¹ã‚¯: æ–‡å­—åˆ—ã‚’é€†é †ã«ã™ã‚‹
ã‚³ãƒ¼ãƒ‰:
def reverse_string(s):
    return s[::-1]

ä¾‹3:
ã‚¿ã‚¹ã‚¯: ãƒªã‚¹ãƒˆã®å¹³å‡å€¤ã‚’è¨ˆç®—ã™ã‚‹
ã‚³ãƒ¼ãƒ‰:
def calculate_average(lst):
    return sum(lst) / len(lst) if lst else 0

ã‚¿ã‚¹ã‚¯: {task}
ã‚³ãƒ¼ãƒ‰:"""
        return prompt.format(task=task_description)

    @staticmethod
    def analogical_reasoning(question: str) -> str:
        """é¡æ¨æ¨è«–ã®Few-shotãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        prompt = """
ä»¥ä¸‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç†è§£ã—ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

ä¾‹1: æ±äº¬:æ—¥æœ¬ = ãƒ‘ãƒª:?
ç­”ãˆ: ãƒ•ãƒ©ãƒ³ã‚¹
ç†ç”±: æ±äº¬ãŒæ—¥æœ¬ã®é¦–éƒ½ã§ã‚ã‚‹ã‚ˆã†ã«ã€ãƒ‘ãƒªã¯ãƒ•ãƒ©ãƒ³ã‚¹ã®é¦–éƒ½ã§ã™ã€‚

ä¾‹2: åŒ»è€…:ç—…é™¢ = æ•™å¸«:?
ç­”ãˆ: å­¦æ ¡
ç†ç”±: åŒ»è€…ãŒç—…é™¢ã§åƒãã‚ˆã†ã«ã€æ•™å¸«ã¯å­¦æ ¡ã§åƒãã¾ã™ã€‚

ä¾‹3: çŠ¬:å“ºä¹³é¡ = é·¹:?
ç­”ãˆ: é³¥é¡
ç†ç”±: çŠ¬ãŒå“ºä¹³é¡ã«å±ã™ã‚‹ã‚ˆã†ã«ã€é·¹ã¯é³¥é¡ã«å±ã—ã¾ã™ã€‚

è³ªå•: {question}
ç­”ãˆ:"""
        return prompt.format(question=question)

# ä½¿ç”¨ä¾‹
few_shot_engine = FewShotPromptEngine()

# Few-shotæ„Ÿæƒ…åˆ†æ
test_text = "æœŸå¾…ã—ã¦ã„ãŸã»ã©ã§ã¯ãªã‹ã£ãŸãŒã€ã¾ã‚ã¾ã‚ã§ã™ã€‚"
prompt = few_shot_engine.sentiment_analysis(test_text)
print("=== Few-shot æ„Ÿæƒ…åˆ†æ ===")
print(prompt)
print("\n" + "="*80 + "\n")

# Few-shotå›ºæœ‰è¡¨ç¾æŠ½å‡º
test_entity = "NHKã®è¨˜è€…ãŒãƒ‹ãƒ¥ãƒ¼ãƒ¨ãƒ¼ã‚¯ã§å±±ç”°èŠ±å­æ°ã«ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã—ãŸã€‚"
prompt = few_shot_engine.entity_extraction(test_entity)
print("=== Few-shot å›ºæœ‰è¡¨ç¾æŠ½å‡º ===")
print(prompt)
print("\n" + "="*80 + "\n")

# Few-shoté¡æ¨æ¨è«–
test_analogy = "æœ¬:è‘—è€… = æ˜ ç”»:?"
prompt = few_shot_engine.analogical_reasoning(test_analogy)
print("=== Few-shot é¡æ¨æ¨è«– ===")
print(prompt)
</code></pre>

<h3>Chain-of-Thought (CoT) Prompting</h3>

<p><strong>Chain-of-Thoughtï¼ˆæ€è€ƒã®é€£é–ï¼‰</strong>ã¯ã€ãƒ¢ãƒ‡ãƒ«ã«æ®µéšçš„ãªæ¨è«–éç¨‹ã‚’ç”Ÿæˆã•ã›ã‚‹ã“ã¨ã§ã€è¤‡é›‘ãªå•é¡Œã®ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<p>$$
\text{Accuracy}_{\text{CoT}} \approx \text{Accuracy}_{\text{standard}} + \Delta_{\text{reasoning}}
$$</p>

<p>ã“ã“ã§ã€$\Delta_{\text{reasoning}}$ã¯æ¨è«–ã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Šåˆ†ã§ã€è¤‡é›‘ãªå•é¡Œã»ã©å¤§ãããªã‚Šã¾ã™ã€‚</p>

<pre><code class="language-python">class ChainOfThoughtEngine:
    """
    Chain-of-Thought (CoT) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    """

    @staticmethod
    def math_problem_basic(problem: str) -> str:
        """æ•°å­¦å•é¡Œã®åŸºæœ¬çš„ãªCoT"""
        prompt = f"""
ä»¥ä¸‹ã®å•é¡Œã‚’æ®µéšçš„ã«è§£ã„ã¦ãã ã•ã„ã€‚
å„ã‚¹ãƒ†ãƒƒãƒ—ã§ä½•ã‚’è¨ˆç®—ã—ã¦ã„ã‚‹ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

å•é¡Œ: {problem}

è§£ç­”:
ã‚¹ãƒ†ãƒƒãƒ—1:"""
        return prompt

    @staticmethod
    def math_problem_with_examples(problem: str) -> str:
        """Few-shot CoTã®ä¾‹"""
        prompt = """
ä»¥ä¸‹ã®ä¾‹ã‚’å‚è€ƒã«ã€å•é¡Œã‚’æ®µéšçš„ã«è§£ã„ã¦ãã ã•ã„ã€‚

ä¾‹é¡Œ1:
å•é¡Œ: ã‚Šã‚“ã”ãŒ15å€‹ã€ã¿ã‹ã‚“ãŒ23å€‹ã‚ã‚Šã¾ã™ã€‚æœç‰©ã¯å…¨éƒ¨ã§ä½•å€‹ã§ã™ã‹ï¼Ÿ
è§£ç­”:
ã‚¹ãƒ†ãƒƒãƒ—1: ã‚Šã‚“ã”ã®æ•°ã‚’ç¢ºèª â†’ 15å€‹
ã‚¹ãƒ†ãƒƒãƒ—2: ã¿ã‹ã‚“ã®æ•°ã‚’ç¢ºèª â†’ 23å€‹
ã‚¹ãƒ†ãƒƒãƒ—3: åˆè¨ˆã‚’è¨ˆç®— â†’ 15 + 23 = 38å€‹
ç­”ãˆ: 38å€‹

ä¾‹é¡Œ2:
å•é¡Œ: 1å†Š500å††ã®æœ¬ã‚’3å†Šã¨ã€1æœ¬120å††ã®ãƒšãƒ³ã‚’5æœ¬è²·ã„ã¾ã—ãŸã€‚åˆè¨ˆé‡‘é¡ã¯ï¼Ÿ
è§£ç­”:
ã‚¹ãƒ†ãƒƒãƒ—1: æœ¬ã®åˆè¨ˆé‡‘é¡ã‚’è¨ˆç®— â†’ 500å†† Ã— 3å†Š = 1,500å††
ã‚¹ãƒ†ãƒƒãƒ—2: ãƒšãƒ³ã®åˆè¨ˆé‡‘é¡ã‚’è¨ˆç®— â†’ 120å†† Ã— 5æœ¬ = 600å††
ã‚¹ãƒ†ãƒƒãƒ—3: ç·åˆè¨ˆã‚’è¨ˆç®— â†’ 1,500å†† + 600å†† = 2,100å††
ç­”ãˆ: 2,100å††

å•é¡Œ: {problem}
è§£ç­”:
ã‚¹ãƒ†ãƒƒãƒ—1:"""
        return prompt.format(problem=problem)

    @staticmethod
    def logical_reasoning(scenario: str, question: str) -> str:
        """è«–ç†çš„æ¨è«–ã®CoT"""
        prompt = f"""
ä»¥ä¸‹ã®çŠ¶æ³ã‚’æ®µéšçš„ã«åˆ†æã—ã€è«–ç†çš„ã«çµè«–ã‚’å°ã„ã¦ãã ã•ã„ã€‚

çŠ¶æ³: {scenario}

è³ªå•: {question}

åˆ†æ:
è¦³å¯Ÿ1:"""
        return prompt

    @staticmethod
    def self_consistency_cot(problem: str, num_paths: int = 3) -> str:
        """
        Self-Consistency CoT: è¤‡æ•°ã®æ¨è«–çµŒè·¯ã‚’ç”Ÿæˆã—ã€
        æœ€ã‚‚ä¸€è²«æ€§ã®ã‚ã‚‹ç­”ãˆã‚’é¸æŠ
        """
        prompt = f"""
ä»¥ä¸‹ã®å•é¡Œã‚’{num_paths}ã¤ã®ç•°ãªã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§è§£ã„ã¦ãã ã•ã„ã€‚
å„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§æ®µéšçš„ã«æ¨è«–ã—ã€æœ€çµ‚çš„ã«æœ€ã‚‚ç¢ºå®Ÿãªç­”ãˆã‚’é¸ã‚“ã§ãã ã•ã„ã€‚

å•é¡Œ: {problem}

ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1:
"""
        return prompt

# å®Ÿè·µä¾‹ï¼šè¤‡é›‘ãªæ•°å­¦å•é¡Œ
cot_engine = ChainOfThoughtEngine()

problem1 = """
ã‚ã‚‹åº—èˆ—ã®å£²ä¸ŠãŒ1å¹´ç›®ã¯100ä¸‡å††ã§ã—ãŸã€‚
2å¹´ç›®ã¯å‰å¹´æ¯”20%å¢—ã€3å¹´ç›®ã¯å‰å¹´æ¯”15%æ¸›ã€4å¹´ç›®ã¯å‰å¹´æ¯”25%å¢—ã§ã—ãŸã€‚
4å¹´ç›®ã®å£²ä¸Šã¯ä½•ä¸‡å††ã§ã™ã‹ï¼Ÿ
"""

print("=== Chain-of-Thought: æ•°å­¦å•é¡Œ ===")
prompt1 = cot_engine.math_problem_with_examples(problem1)
print(prompt1)
print("\n" + "="*80 + "\n")

# è«–ç†çš„æ¨è«–ã®ä¾‹
scenario = """
ä¼šè­°å®¤Aã€Bã€CãŒã‚ã‚Šã¾ã™ã€‚
- ç”°ä¸­ã•ã‚“ã¯ä¼šè­°å®¤Aã«ã„ã¾ã›ã‚“
- ä½è—¤ã•ã‚“ã¯ä¼šè­°å®¤Bã«ã„ã¾ã™
- ä¼šè­°å®¤Cã«ã¯èª°ã‚‚ã„ã¾ã›ã‚“
- ç”°ä¸­ã•ã‚“ã¨ä½è—¤ã•ã‚“ä»¥å¤–ã«å±±ç”°ã•ã‚“ãŒã„ã¾ã™
"""

question = "å±±ç”°ã•ã‚“ã¯ã©ã®ä¼šè­°å®¤ã«ã„ã¾ã™ã‹ï¼Ÿ"

print("=== Chain-of-Thought: è«–ç†çš„æ¨è«– ===")
prompt2 = cot_engine.logical_reasoning(scenario, question)
print(prompt2)
print("\n" + "="*80 + "\n")

# Self-Consistency CoT
problem2 = """
è¢‹ã®ä¸­ã«èµ¤ã„ç‰ãŒ5å€‹ã€é’ã„ç‰ãŒ3å€‹å…¥ã£ã¦ã„ã¾ã™ã€‚
2å€‹ã®ç‰ã‚’åŒæ™‚ã«å–ã‚Šå‡ºã™ã¨ãã€ä¸¡æ–¹ã¨ã‚‚èµ¤ã„ç‰ã§ã‚ã‚‹ç¢ºç‡ã¯ï¼Ÿ
"""

print("=== Self-Consistency CoT ===")
prompt3 = cot_engine.self_consistency_cot(problem2, num_paths=3)
print(prompt3)
</code></pre>

<blockquote>
<p><strong>CoTã®åŠ¹æœ</strong>: Googleã®PaLMãƒ¢ãƒ‡ãƒ«ã§ã®å®Ÿé¨“ã§ã¯ã€æ¨™æº–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§34%ã ã£ãŸç®—æ•°å•é¡Œã®æ­£ç­”ç‡ãŒã€CoTãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§79%ã¾ã§å‘ä¸Šã—ã¾ã—ãŸã€‚ç‰¹ã«å¤šæ®µéšæ¨è«–ãŒå¿…è¦ãªå•é¡Œã§å¤§ããªæ”¹å–„ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚</p>
</blockquote>

<h3>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¨­è¨ˆ</h3>

<pre><code class="language-python">from typing import Dict, List, Optional
from dataclasses import dataclass

@dataclass
class PromptTemplate:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®æ§‹é€ åŒ–ç®¡ç†"""
    name: str
    instruction: str
    examples: Optional[List[Dict[str, str]]] = None
    output_format: Optional[str] = None

    def render(self, **kwargs) -> str:
        """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å®Ÿéš›ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ›"""
        prompt = f"{self.instruction}\n\n"

        # Few-shotä¾‹ã‚’è¿½åŠ 
        if self.examples:
            prompt += "ä¾‹:\n"
            for i, example in enumerate(self.examples, 1):
                prompt += f"\nä¾‹{i}:\n"
                for key, value in example.items():
                    prompt += f"{key}: {value}\n"

        # å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’è¿½åŠ 
        if self.output_format:
            prompt += f"\nå‡ºåŠ›å½¢å¼:\n{self.output_format}\n"

        # å¤‰æ•°ã‚’æŒ¿å…¥
        prompt += "\nå…¥åŠ›:\n"
        for key, value in kwargs.items():
            prompt += f"{key}: {value}\n"

        prompt += "\nå‡ºåŠ›:"
        return prompt

class PromptLibrary:
    """å†åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª"""

    @staticmethod
    def get_classification_template() -> PromptTemplate:
        """åˆ†é¡ã‚¿ã‚¹ã‚¯ç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ"""
        return PromptTemplate(
            name="classification",
            instruction="ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã—ã¦ãã ã•ã„ã€‚",
            examples=[
                {
                    "ãƒ†ã‚­ã‚¹ãƒˆ": "æ–°ã—ã„ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ãŒç™ºå£²ã•ã‚Œã¾ã—ãŸã€‚",
                    "ã‚«ãƒ†ã‚´ãƒª": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼"
                },
                {
                    "ãƒ†ã‚­ã‚¹ãƒˆ": "æ ªä¾¡ãŒæ€¥é¨°ã—ã¦ã„ã¾ã™ã€‚",
                    "ã‚«ãƒ†ã‚´ãƒª": "ãƒ“ã‚¸ãƒã‚¹"
                }
            ],
            output_format="ã‚«ãƒ†ã‚´ãƒªåã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚"
        )

    @staticmethod
    def get_extraction_template() -> PromptTemplate:
        """æƒ…å ±æŠ½å‡ºç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ"""
        return PromptTemplate(
            name="extraction",
            instruction="ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŒ‡å®šã•ã‚ŒãŸæƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚",
            output_format="JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„: {\"é …ç›®1\": \"å€¤1\", \"é …ç›®2\": \"å€¤2\"}"
        )

    @staticmethod
    def get_generation_template() -> PromptTemplate:
        """ç”Ÿæˆã‚¿ã‚¹ã‚¯ç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ"""
        return PromptTemplate(
            name="generation",
            instruction="ä»¥ä¸‹ã®æ¡ä»¶ã«åŸºã¥ã„ã¦ã€å‰µé€ çš„ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚",
            output_format="è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„æ–‡ç« ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
        )

    @staticmethod
    def get_reasoning_template() -> PromptTemplate:
        """æ¨è«–ã‚¿ã‚¹ã‚¯ç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ"""
        return PromptTemplate(
            name="reasoning",
            instruction="""
ä»¥ä¸‹ã®å•é¡Œã‚’æ®µéšçš„ã«åˆ†æã—ã€è«–ç†çš„ã«è§£æ±ºã—ã¦ãã ã•ã„ã€‚
å„ã‚¹ãƒ†ãƒƒãƒ—ã§æ¨è«–éç¨‹ã‚’æ˜ç¢ºã«ç¤ºã—ã¦ãã ã•ã„ã€‚
            """,
            output_format="""
ã‚¹ãƒ†ãƒƒãƒ—1: [æœ€åˆã®åˆ†æ]
ã‚¹ãƒ†ãƒƒãƒ—2: [æ¬¡ã®æ¨è«–]
...
çµè«–: [æœ€çµ‚çš„ãªç­”ãˆ]
            """
        )

# ä½¿ç”¨ä¾‹
library = PromptLibrary()

# åˆ†é¡ã‚¿ã‚¹ã‚¯
classification_template = library.get_classification_template()
prompt = classification_template.render(
    ãƒ†ã‚­ã‚¹ãƒˆ="äººå·¥çŸ¥èƒ½ã®ç ”ç©¶ãŒåŠ é€Ÿã—ã¦ã„ã¾ã™ã€‚",
    ã‚«ãƒ†ã‚´ãƒª="ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼, ãƒ“ã‚¸ãƒã‚¹, æ”¿æ²», ã‚¹ãƒãƒ¼ãƒ„, ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ¡ãƒ³ãƒˆ"
)
print("=== åˆ†é¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ===")
print(prompt)
print("\n" + "="*80 + "\n")

# æƒ…å ±æŠ½å‡ºã‚¿ã‚¹ã‚¯
extraction_template = library.get_extraction_template()
prompt = extraction_template.render(
    ãƒ†ã‚­ã‚¹ãƒˆ="ç”°ä¸­å¤ªéƒï¼ˆ35æ­³ï¼‰ã¯ABCæ ªå¼ä¼šç¤¾ã®CTOã§ã€æ±äº¬åœ¨ä½ã§ã™ã€‚",
    æŠ½å‡ºé …ç›®="åå‰, å¹´é½¢, ä¼šç¤¾å, å½¹è·, å±…ä½åœ°"
)
print("=== æŠ½å‡ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ===")
print(prompt)
print("\n" + "="*80 + "\n")

# æ¨è«–ã‚¿ã‚¹ã‚¯
reasoning_template = library.get_reasoning_template()
prompt = reasoning_template.render(
    å•é¡Œ="3ã¤ã®ç®±A, B, CãŒã‚ã‚Šã€1ã¤ã ã‘ã«å®ç‰©ãŒå…¥ã£ã¦ã„ã¾ã™ã€‚ç®±Aã«ã¯ã€Œå®ç‰©ã¯ã“ã“ã«ã‚ã‚‹ã€ã€ç®±Bã«ã¯ã€Œå®ç‰©ã¯Aã«ãªã„ã€ã€ç®±Cã«ã¯ã€Œå®ç‰©ã¯Bã«ã‚ã‚‹ã€ã¨æ›¸ã‹ã‚Œã¦ã„ã¾ã™ã€‚1ã¤ã ã‘ãŒçœŸå®Ÿã‚’è¿°ã¹ã¦ã„ã‚‹å ´åˆã€å®ç‰©ã¯ã©ã®ç®±ã«ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
)
print("=== æ¨è«–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ===")
print(prompt)
</code></pre>

<hr>

<h2>5.4 In-Context Learning</h2>

<h3>In-Context Learningã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ </h3>

<p><strong>In-Context Learning (ICL)</strong>ã¯ã€LLMãŒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã®ä¾‹ç¤ºã‹ã‚‰ç›´æ¥å­¦ç¿’ã—ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ãªã—ã«æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹èƒ½åŠ›ã§ã™ã€‚</p>

<div class="mermaid">
graph TB
    A[In-Context Learning] --> B[å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ]
    B --> C[ã‚¿ã‚¹ã‚¯æŒ‡ç¤º]
    B --> D[Few-shotä¾‹ç¤º]
    B --> E[ã‚¯ã‚¨ãƒª]

    C --> F[Transformerã®<br/>Self-Attention]
    D --> F
    E --> F

    F --> G[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†…ã§<br/>ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’]
    G --> H[å‡ºåŠ›ç”Ÿæˆ]

    style A fill:#e3f2fd
    style F fill:#fff3e0
    style G fill:#c8e6c9
</div>

<h4>ICLãŒãªãœæ©Ÿèƒ½ã™ã‚‹ã‹</h4>

<p>æœ€è¿‘ã®ç ”ç©¶ï¼ˆ2023ï¼‰ã«ã‚ˆã‚Šã€ICLã¯ä»¥ä¸‹ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã§å‹•ä½œã™ã‚‹ã“ã¨ãŒåˆ¤æ˜ã—ã¾ã—ãŸï¼š</p>

<ol>
<li><strong>æ½œåœ¨çš„ãªæ¦‚å¿µã®æ´»æ€§åŒ–</strong>: äº‹å‰å­¦ç¿’ã§ç²å¾—ã—ãŸçŸ¥è­˜ãŒä¾‹ç¤ºã«ã‚ˆã‚Šæ´»æ€§åŒ–ã•ã‚Œã‚‹</li>
<li><strong>ã‚¿ã‚¹ã‚¯ãƒ™ã‚¯ãƒˆãƒ«ã®å½¢æˆ</strong>: ä¾‹ç¤ºã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå†…éƒ¨è¡¨ç¾ã¨ã—ã¦ä¿æŒã•ã‚Œã‚‹</li>
<li><strong>ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼ãƒ™ãƒ¼ã‚¹ã®æ¨è«–</strong>: æ–°ã—ã„å…¥åŠ›ã‚’ä¾‹ç¤ºã¨ã®ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼ã§å‡¦ç†ã™ã‚‹</li>
</ol>

<p>$$
P(y|x, \text{examples}) \approx \sum_{i=1}^{k} \alpha_i \cdot P(y|x, \text{example}_i)
$$</p>

<p>ã“ã“ã§ã€$\alpha_i$ã¯å„ä¾‹ç¤ºã®é–¢é€£åº¦é‡ã¿ã§ã™ã€‚</p>

<pre><code class="language-python">import numpy as np
from typing import List, Tuple, Dict

class InContextLearningSimulator:
    """
    In-Context Learningã®å‹•ä½œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    ï¼ˆç°¡ç•¥åŒ–ã•ã‚ŒãŸãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®å†ç¾ï¼‰
    """

    def __init__(self, embedding_dim: int = 128):
        """
        Args:
            embedding_dim: åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°
        """
        self.embedding_dim = embedding_dim
        self.task_vector = None

    def create_example_embedding(self, input_text: str, output_text: str) -> np.ndarray:
        """
        å…¥åŠ›-å‡ºåŠ›ãƒšã‚¢ã‹ã‚‰ä¾‹ç¤ºåŸ‹ã‚è¾¼ã¿ã‚’ä½œæˆ
        ï¼ˆå®Ÿéš›ã«ã¯Transformerã§å‡¦ç†ã•ã‚Œã‚‹ãŒã€ã“ã“ã§ã¯ç°¡ç•¥åŒ–ï¼‰
        """
        # ç°¡ç•¥åŒ–: æ–‡å­—åˆ—ã‚’ãƒãƒƒã‚·ãƒ¥ã—ã¦åŸ‹ã‚è¾¼ã¿ã«å¤‰æ›
        input_hash = hash(input_text) % 10000
        output_hash = hash(output_text) % 10000

        np.random.seed(input_hash)
        input_emb = np.random.randn(self.embedding_dim)

        np.random.seed(output_hash)
        output_emb = np.random.randn(self.embedding_dim)

        # ã‚¿ã‚¹ã‚¯ãƒ™ã‚¯ãƒˆãƒ«: å‡ºåŠ› - å…¥åŠ›ã®å·®åˆ†
        task_emb = output_emb - input_emb
        return task_emb / (np.linalg.norm(task_emb) + 1e-8)

    def learn_from_examples(self, examples: List[Tuple[str, str]]) -> None:
        """
        Few-shotä¾‹ç¤ºã‹ã‚‰ã‚¿ã‚¹ã‚¯ãƒ™ã‚¯ãƒˆãƒ«ã‚’å­¦ç¿’

        Args:
            examples: [(å…¥åŠ›1, å‡ºåŠ›1), (å…¥åŠ›2, å‡ºåŠ›2), ...]
        """
        task_vectors = []

        for input_text, output_text in examples:
            task_vec = self.create_example_embedding(input_text, output_text)
            task_vectors.append(task_vec)

        # è¤‡æ•°ä¾‹ã®å¹³å‡ã¨ã—ã¦ã‚¿ã‚¹ã‚¯è¡¨ç¾ã‚’ç²å¾—
        self.task_vector = np.mean(task_vectors, axis=0)
        self.task_vector /= (np.linalg.norm(self.task_vector) + 1e-8)

    def predict(self, query: str, candidates: List[str]) -> Dict[str, float]:
        """
        å­¦ç¿’ã—ãŸã‚¿ã‚¹ã‚¯ãƒ™ã‚¯ãƒˆãƒ«ã«åŸºã¥ã„ã¦äºˆæ¸¬

        Args:
            query: å…¥åŠ›ã‚¯ã‚¨ãƒª
            candidates: å€™è£œã¨ãªã‚‹å‡ºåŠ›ãƒªã‚¹ãƒˆ

        Returns:
            å„å€™è£œã®ã‚¹ã‚³ã‚¢è¾æ›¸
        """
        if self.task_vector is None:
            raise ValueError("å…ˆã«learn_from_examples()ã‚’å‘¼ã³å‡ºã—ã¦ãã ã•ã„")

        # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿
        query_hash = hash(query) % 10000
        np.random.seed(query_hash)
        query_emb = np.random.randn(self.embedding_dim)
        query_emb /= (np.linalg.norm(query_emb) + 1e-8)

        # å„å€™è£œã®ã‚¹ã‚³ã‚¢è¨ˆç®—
        scores = {}
        for candidate in candidates:
            candidate_hash = hash(candidate) % 10000
            np.random.seed(candidate_hash)
            candidate_emb = np.random.randn(self.embedding_dim)
            candidate_emb /= (np.linalg.norm(candidate_emb) + 1e-8)

            # ã‚¿ã‚¹ã‚¯ãƒ™ã‚¯ãƒˆãƒ«ã¨ã®æ•´åˆæ€§
            predicted_output = query_emb + self.task_vector
            similarity = np.dot(predicted_output, candidate_emb)
            scores[candidate] = float(similarity)

        # ã‚¹ã‚³ã‚¢ã‚’æ­£è¦åŒ–
        total = sum(np.exp(s) for s in scores.values())
        scores = {k: np.exp(v)/total for k, v in scores.items()}

        return scores

# ä½¿ç”¨ä¾‹ï¼šæ„Ÿæƒ…åˆ†æã‚¿ã‚¹ã‚¯
print("=== In-Context Learning ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ ===\n")

simulator = InContextLearningSimulator(embedding_dim=128)

# Few-shotä¾‹ç¤º
examples = [
    ("ã“ã®æ˜ ç”»ã¯ç´ æ™´ã‚‰ã—ã‹ã£ãŸï¼", "ãƒã‚¸ãƒ†ã‚£ãƒ–"),
    ("æœ€æ‚ªã®ä½“é¨“ã§ã—ãŸã€‚äºŒåº¦ã¨è¡Œãã¾ã›ã‚“ã€‚", "ãƒã‚¬ãƒ†ã‚£ãƒ–"),
    ("æ™®é€šã§ã™ã€‚ç‰¹ã«å°è±¡ã«æ®‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", "ä¸­ç«‹"),
    ("æœŸå¾…ä»¥ä¸Šã®å“è³ªã§å¤§æº€è¶³ã§ã™ï¼", "ãƒã‚¸ãƒ†ã‚£ãƒ–"),
    ("ã²ã©ã„ã‚µãƒ¼ãƒ“ã‚¹ã«ãŒã£ã‹ã‚Šã—ã¾ã—ãŸã€‚", "ãƒã‚¬ãƒ†ã‚£ãƒ–"),
]

# ã‚¿ã‚¹ã‚¯ãƒ™ã‚¯ãƒˆãƒ«ã‚’å­¦ç¿’
simulator.learn_from_examples(examples)
print("âœ… Few-shotä¾‹ç¤ºã‹ã‚‰å­¦ç¿’å®Œäº†\n")

# æ–°ã—ã„å…¥åŠ›ã§äºˆæ¸¬
test_queries = [
    "ç´ æ™´ã‚‰ã—ã„è£½å“ã§ã™ã€‚ãŠã™ã™ã‚ã—ã¾ã™ã€‚",
    "æœŸå¾…å¤–ã‚Œã§ã—ãŸã€‚",
    "ã¾ã‚ã¾ã‚ã®å‡ºæ¥æ „ãˆã§ã™ã€‚"
]

candidates = ["ãƒã‚¸ãƒ†ã‚£ãƒ–", "ãƒã‚¬ãƒ†ã‚£ãƒ–", "ä¸­ç«‹"]

for query in test_queries:
    print(f"ã‚¯ã‚¨ãƒª: {query}")
    scores = simulator.predict(query, candidates)

    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)

    for label, score in sorted_scores:
        print(f"  {label}: {score:.4f} {'â˜…' * int(score * 10)}")
    print(f"  â†’ äºˆæ¸¬: {sorted_scores[0][0]}\n")
</code></pre>

<h3>ICLã®åŠ¹æœçš„ãªæ´»ç”¨</h3>

<h4>ä¾‹ç¤ºã®é¸æŠæˆ¦ç•¥</h4>

<table>
<thead>
<tr>
<th>æˆ¦ç•¥</th>
<th>æ–¹æ³•</th>
<th>åˆ©ç‚¹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ©ãƒ³ãƒ€ãƒ é¸æŠ</strong></td>
<td>è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã¶</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ã€ãƒã‚¤ã‚¢ã‚¹ãŒå°‘ãªã„</td>
</tr>
<tr>
<td><strong>é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹</strong></td>
<td>ã‚¯ã‚¨ãƒªã«é¡ä¼¼ã—ãŸä¾‹ã‚’é¸ã¶</td>
<td>ã‚¿ã‚¹ã‚¯æ€§èƒ½ãŒé«˜ã„</td>
</tr>
<tr>
<td><strong>å¤šæ§˜æ€§é‡è¦–</strong></td>
<td>å¤šæ§˜ãªä¾‹ã‚’å«ã‚ã‚‹</td>
<td>æ±åŒ–æ€§èƒ½ãŒå‘ä¸Š</td>
</tr>
<tr>
<td><strong>é›£æ˜“åº¦èª¿æ•´</strong></td>
<td>ç°¡å˜â†’é›£ã—ã„é †ã«ä¸¦ã¹ã‚‹</td>
<td>å­¦ç¿’åŠ¹ç‡ãŒè‰¯ã„</td>
</tr>
</tbody>
</table>

<hr>

<h2>5.5 RLHFï¼ˆReinforcement Learning from Human Feedbackï¼‰</h2>

<h3>RLHFã¨ã¯</h3>

<p><strong>RLHF</strong>ã¯ã€äººé–“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æ´»ç”¨ã—ã¦LLMã‚’æ”¹å–„ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚ChatGPTã‚„Claudeã€Geminiãªã©ã€ã»ã¼ã™ã¹ã¦ã®å•†ç”¨LLMã§æ¡ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚</p>

<div class="mermaid">
graph TB
    A[RLHF 3æ®µéšãƒ—ãƒ­ã‚»ã‚¹] --> B[Step 1<br/>äº‹å‰å­¦ç¿’æ¸ˆã¿LLM]

    B --> C[Step 2<br/>Reward Modelè¨“ç·´]
    C --> C1[äººé–“ã«ã‚ˆã‚‹<br/>å¿œç­”ã®è©•ä¾¡]
    C --> C2[å¥½ã¾ã—ã„/å¥½ã¾ã—ããªã„<br/>å¿œç­”ãƒšã‚¢ã‚’ä½œæˆ]
    C --> C3[Reward Modelå­¦ç¿’]

    C3 --> D[Step 3<br/>PPOã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’]
    D --> D1[LLMãŒå¿œç­”ç”Ÿæˆ]
    D --> D2[Reward ModelãŒ<br/>ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°]
    D --> D3[é«˜ã‚¹ã‚³ã‚¢å¿œç­”ã‚’å¼·åŒ–]

    D3 --> E[æœ€é©åŒ–ã•ã‚ŒãŸLLM]

    style A fill:#e3f2fd
    style C fill:#fff3e0
    style D fill:#fff9c4
    style E fill:#c8e6c9
</div>

<h3>RLHFã®3ã‚¹ãƒ†ãƒƒãƒ—</h3>

<h4>Step 1: äº‹å‰å­¦ç¿’ï¼ˆPre-trainingï¼‰</h4>

<p>å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹ã§Transformerã‚’è¨“ç·´ã—ã€è¨€èªã®åŸºæœ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã¾ã™ã€‚</p>

<p>$$
\mathcal{L}_{\text{pretrain}} = -\mathbb{E}_{x \sim \mathcal{D}} \left[\log P_{\theta}(x)\right]
$$</p>

<h4>Step 2: Reward Modelã®è¨“ç·´</h4>

<p>äººé–“ã®è©•ä¾¡è€…ãŒè¤‡æ•°ã®å¿œç­”ã‚’æ¯”è¼ƒã—ã€å¥½ã¾ã—ã•ã‚’ãƒ©ãƒ³ã‚¯ä»˜ã‘ã—ã¾ã™ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰<strong>Reward Model</strong>ã‚’è¨“ç·´ã—ã¾ã™ã€‚</p>

<p>$$
\mathcal{L}_{\text{reward}} = -\mathbb{E}_{(x, y_w, y_l)} \left[\log \sigma(r_{\phi}(x, y_w) - r_{\phi}(x, y_l))\right]
$$</p>

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$r_{\phi}$: Reward Modelï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\phi$ï¼‰</li>
<li>$y_w$: å¥½ã¾ã—ã„å¿œç­”ï¼ˆwinnerï¼‰</li>
<li>$y_l$: å¥½ã¾ã—ããªã„å¿œç­”ï¼ˆloserï¼‰</li>
<li>$\sigma$: ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°</li>
</ul>

<h4>Step 3: PPOã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’</h4>

<p><strong>PPOï¼ˆProximal Policy Optimizationï¼‰</strong>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§LLMã‚’æœ€é©åŒ–ã—ã¾ã™ã€‚KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹åˆ¶ç´„ã«ã‚ˆã‚Šã€å…ƒã®äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å¤§ããé€¸è„±ã—ãªã„ã‚ˆã†ã«ã—ã¾ã™ã€‚</p>

<p>$$
\mathcal{L}_{\text{RLHF}} = \mathbb{E}_{x, y} \left[r_{\phi}(x, y) - \beta \cdot D_{KL}(\pi_{\theta} || \pi_{\text{ref}})\right]
$$</p>

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$\pi_{\theta}$: æœ€é©åŒ–ä¸­ã®ãƒãƒªã‚·ãƒ¼ï¼ˆLLMï¼‰</li>
<li>$\pi_{\text{ref}}$: å‚ç…§ãƒãƒªã‚·ãƒ¼ï¼ˆå…ƒã®ãƒ¢ãƒ‡ãƒ«ï¼‰</li>
<li>$\beta$: KLåˆ¶ç´„ã®å¼·ã•</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical

class RewardModel(nn.Module):
    """
    RLHFç”¨ã®Reward Model
    """

    def __init__(self, input_dim: int = 768, hidden_dim: int = 256):
        """
        Args:
            input_dim: å…¥åŠ›åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒï¼ˆä¾‹: BERTåŸ‹ã‚è¾¼ã¿ï¼‰
            hidden_dim: éš ã‚Œå±¤ã®æ¬¡å…ƒ
        """
        super().__init__()

        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, 1)  # ã‚¹ã‚«ãƒ©ãƒ¼å ±é…¬ã‚’å‡ºåŠ›
        )

    def forward(self, embeddings):
        """
        Args:
            embeddings: [batch_size, seq_len, input_dim]

        Returns:
            rewards: [batch_size] ã‚¹ã‚«ãƒ©ãƒ¼å ±é…¬
        """
        # å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°
        pooled = embeddings.mean(dim=1)  # [batch_size, input_dim]
        rewards = self.network(pooled).squeeze(-1)  # [batch_size]
        return rewards

class RLHFTrainer:
    """
    RLHFè¨“ç·´ãƒ—ãƒ­ã‚»ã‚¹ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """

    def __init__(self, reward_model: RewardModel, beta: float = 0.1):
        """
        Args:
            reward_model: è¨“ç·´æ¸ˆã¿Reward Model
            beta: KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹åˆ¶ç´„ã®å¼·ã•
        """
        self.reward_model = reward_model
        self.beta = beta

    def compute_reward(self, response_embeddings: torch.Tensor) -> torch.Tensor:
        """
        å¿œç­”ã«å¯¾ã™ã‚‹å ±é…¬ã‚’è¨ˆç®—

        Args:
            response_embeddings: [batch_size, seq_len, embed_dim]

        Returns:
            rewards: [batch_size]
        """
        with torch.no_grad():
            rewards = self.reward_model(response_embeddings)
        return rewards

    def compute_kl_penalty(self,
                          current_logprobs: torch.Tensor,
                          reference_logprobs: torch.Tensor) -> torch.Tensor:
        """
        KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’è¨ˆç®—

        Args:
            current_logprobs: ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã®logç¢ºç‡
            reference_logprobs: å‚ç…§ãƒ¢ãƒ‡ãƒ«ã®logç¢ºç‡

        Returns:
            kl_penalty: KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹
        """
        kl = current_logprobs - reference_logprobs
        return kl.mean()

    def ppo_loss(self,
                 old_logprobs: torch.Tensor,
                 new_logprobs: torch.Tensor,
                 advantages: torch.Tensor,
                 epsilon: float = 0.2) -> torch.Tensor:
        """
        PPOï¼ˆProximal Policy Optimizationï¼‰æå¤±ã‚’è¨ˆç®—

        Args:
            old_logprobs: å¤ã„ãƒãƒªã‚·ãƒ¼ã®logç¢ºç‡
            new_logprobs: æ–°ã—ã„ãƒãƒªã‚·ãƒ¼ã®logç¢ºç‡
            advantages: ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ï¼ˆå ±é…¬ - ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
            epsilon: ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç¯„å›²

        Returns:
            ppo_loss: PPOæå¤±
        """
        # ç¢ºç‡æ¯”
        ratio = torch.exp(new_logprobs - old_logprobs)

        # ã‚¯ãƒªãƒƒãƒ—ã•ã‚ŒãŸç›®çš„é–¢æ•°
        clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)

        # PPOæå¤±ï¼ˆæœ€å°åŒ–ï¼‰
        loss = -torch.min(
            ratio * advantages,
            clipped_ratio * advantages
        ).mean()

        return loss

# ä½¿ç”¨ä¾‹
print("=== RLHF Reward Model ãƒ‡ãƒ¢ ===\n")

# Reward Modelã®åˆæœŸåŒ–
reward_model = RewardModel(input_dim=768, hidden_dim=256)

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿: 2ã¤ã®å¿œç­”ã®åŸ‹ã‚è¾¼ã¿
torch.manual_seed(42)
response1_emb = torch.randn(1, 20, 768)  # å¥½ã¾ã—ã„å¿œç­”
response2_emb = torch.randn(1, 20, 768)  # å¥½ã¾ã—ããªã„å¿œç­”

# å ±é…¬ã‚’è¨ˆç®—
reward1 = reward_model(response1_emb)
reward2 = reward_model(response2_emb)

print(f"å¿œç­”1ã®å ±é…¬: {reward1.item():.4f}")
print(f"å¿œç­”2ã®å ±é…¬: {reward2.item():.4f}")

# ãƒšã‚¢ãƒ¯ã‚¤ã‚ºæå¤±ã®è¨ˆç®—ï¼ˆè¨“ç·´æ™‚ï¼‰
pairwise_loss = -F.logsigmoid(reward1 - reward2).mean()
print(f"\nãƒšã‚¢ãƒ¯ã‚¤ã‚ºãƒ©ãƒ³ã‚­ãƒ³ã‚°æå¤±: {pairwise_loss.item():.4f}")

# RLHFãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–
trainer = RLHFTrainer(reward_model, beta=0.1)

# PPOæå¤±ã®è¨ˆç®—ä¾‹
old_logprobs = torch.randn(16)  # [batch_size]
new_logprobs = old_logprobs + torch.randn(16) * 0.1
advantages = torch.randn(16)

ppo_loss = trainer.ppo_loss(old_logprobs, new_logprobs, advantages)
print(f"\nPPOæå¤±: {ppo_loss.item():.4f}")

# KLãƒšãƒŠãƒ«ãƒ†ã‚£ã®è¨ˆç®—
kl_penalty = trainer.compute_kl_penalty(new_logprobs, old_logprobs)
print(f"KLãƒšãƒŠãƒ«ãƒ†ã‚£: {kl_penalty.item():.4f}")

# ç·åˆç›®çš„é–¢æ•°
total_loss = ppo_loss + trainer.beta * kl_penalty
print(f"\nç·åˆæå¤±ï¼ˆPPO + KLåˆ¶ç´„ï¼‰: {total_loss.item():.4f}")
</code></pre>

<h3>RLHFã®èª²é¡Œã¨æ”¹å–„</h3>

<table>
<thead>
<tr>
<th>èª²é¡Œ</th>
<th>èª¬æ˜</th>
<th>è§£æ±ºç­–</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å ±é…¬ãƒãƒƒã‚­ãƒ³ã‚°</strong></td>
<td>ãƒ¢ãƒ‡ãƒ«ãŒå ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹éæ„å›³çš„ãªæ–¹æ³•ã‚’è¦‹ã¤ã‘ã‚‹</td>
<td>å¤šæ§˜ãªè©•ä¾¡è€…ã€KLåˆ¶ç´„ã®èª¿æ•´</td>
</tr>
<tr>
<td><strong>è©•ä¾¡è€…ã®ãƒã‚¤ã‚¢ã‚¹</strong></td>
<td>äººé–“ã®è©•ä¾¡ã«ä¸€è²«æ€§ãŒãªã„</td>
<td>è¤‡æ•°è©•ä¾¡è€…ã®åˆæ„ã€ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³æ•´å‚™</td>
</tr>
<tr>
<td><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆ</strong></td>
<td>PPOè¨“ç·´ã¯è¨ˆç®—é‡ãŒå¤§ãã„</td>
<td>DPOï¼ˆDirect Preference Optimizationï¼‰ãªã©ã®ä»£æ›¿æ‰‹æ³•</td>
</tr>
<tr>
<td><strong>éåº¦ã®å®‰å…¨æ€§</strong></td>
<td>éå‰°ã«æ…é‡ãªå¿œç­”ã«ãªã‚‹</td>
<td>å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´</td>
</tr>
</tbody>
</table>

<hr>

<h2>5.6 å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ</h2>

<h3>ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ1: Few-shot ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚·ã‚¹ãƒ†ãƒ </h3>

<pre><code class="language-python">import openai
import os
from typing import List, Dict, Tuple
from collections import Counter

class FewShotClassifier:
    """
    Few-shot Learning ã«ã‚ˆã‚‹æ±ç”¨ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡å™¨
    """

    def __init__(self, api_key: str = None, model: str = "gpt-3.5-turbo"):
        """
        Args:
            api_key: OpenAI APIã‚­ãƒ¼ï¼ˆNoneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
            model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        """
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        self.model = model
        openai.api_key = self.api_key

    def create_few_shot_prompt(self,
                               examples: List[Tuple[str, str]],
                               query: str,
                               labels: List[str]) -> str:
        """
        Few-shotãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰

        Args:
            examples: [(ãƒ†ã‚­ã‚¹ãƒˆ1, ãƒ©ãƒ™ãƒ«1), (ãƒ†ã‚­ã‚¹ãƒˆ2, ãƒ©ãƒ™ãƒ«2), ...]
            query: åˆ†é¡å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
            labels: å¯èƒ½ãªãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆ

        Returns:
            æ§‹ç¯‰ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        prompt = "ä»¥ä¸‹ã¯æ–‡ç« ã‚’ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚\n\n"
        prompt += f"åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ†ã‚´ãƒª: {', '.join(labels)}\n\n"

        # Few-shotä¾‹ã‚’è¿½åŠ 
        for i, (text, label) in enumerate(examples, 1):
            prompt += f"ä¾‹{i}:\n"
            prompt += f"æ–‡ç« : {text}\n"
            prompt += f"ã‚«ãƒ†ã‚´ãƒª: {label}\n\n"

        # ã‚¯ã‚¨ãƒªã‚’è¿½åŠ 
        prompt += f"æ¬¡ã®æ–‡ç« ã‚’åˆ†é¡ã—ã¦ãã ã•ã„ï¼š\n"
        prompt += f"æ–‡ç« : {query}\n"
        prompt += f"ã‚«ãƒ†ã‚´ãƒª:"

        return prompt

    def classify(self,
                query: str,
                examples: List[Tuple[str, str]],
                labels: List[str],
                temperature: float = 0.3) -> Dict[str, any]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†é¡

        Args:
            query: åˆ†é¡å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
            examples: Few-shotä¾‹
            labels: å¯èƒ½ãªãƒ©ãƒ™ãƒ«
            temperature: ç”Ÿæˆã®å¤šæ§˜æ€§ï¼ˆ0-1ï¼‰

        Returns:
            åˆ†é¡çµæœè¾æ›¸
        """
        prompt = self.create_few_shot_prompt(examples, query, labels)

        try:
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=50
            )

            predicted_label = response.choices[0].message.content.strip()

            return {
                'query': query,
                'predicted_label': predicted_label,
                'prompt': prompt,
                'success': True
            }

        except Exception as e:
            return {
                'query': query,
                'error': str(e),
                'success': False
            }

    def batch_classify(self,
                      queries: List[str],
                      examples: List[Tuple[str, str]],
                      labels: List[str]) -> List[Dict]:
        """
        è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€æ‹¬åˆ†é¡

        Args:
            queries: åˆ†é¡å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
            examples: Few-shotä¾‹
            labels: å¯èƒ½ãªãƒ©ãƒ™ãƒ«

        Returns:
            åˆ†é¡çµæœã®ãƒªã‚¹ãƒˆ
        """
        results = []
        for query in queries:
            result = self.classify(query, examples, labels)
            results.append(result)

        return results

# ä½¿ç”¨ä¾‹: ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®åˆ†é¡
print("=== Few-shot ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ãƒ‡ãƒ¢ ===\n")

# æ³¨æ„: å®Ÿè¡Œã«ã¯OpenAI APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™
# classifier = FewShotClassifier()

# Few-shotè¨“ç·´ä¾‹
examples = [
    ("æ–°å‹ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ãŒç™ºå£²ã•ã‚Œã€äºˆç´„ãŒæ®ºåˆ°ã—ã¦ã„ã¾ã™ã€‚", "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼"),
    ("æ ªå¼å¸‚å ´ãŒæ€¥é¨°ã—ã€éå»æœ€é«˜å€¤ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚", "ãƒ“ã‚¸ãƒã‚¹"),
    ("ã‚µãƒƒã‚«ãƒ¼ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚«ãƒƒãƒ—ã§æ—¥æœ¬ä»£è¡¨ãŒå‹åˆ©ã—ã¾ã—ãŸã€‚", "ã‚¹ãƒãƒ¼ãƒ„"),
    ("æ–°ã—ã„æ˜ ç”»ãŒå…¬é–‹ã•ã‚Œã€èˆˆè¡Œåå…¥è¨˜éŒ²ã‚’æ¨¹ç«‹ã—ã¾ã—ãŸã€‚", "ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ¡ãƒ³ãƒˆ"),
    ("æ”¿åºœãŒæ–°ã—ã„çµŒæ¸ˆæ”¿ç­–ã‚’ç™ºè¡¨ã—ã¾ã—ãŸã€‚", "æ”¿æ²»"),
]

# ãƒ©ãƒ™ãƒ«
labels = ["ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼", "ãƒ“ã‚¸ãƒã‚¹", "ã‚¹ãƒãƒ¼ãƒ„", "ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ¡ãƒ³ãƒˆ", "æ”¿æ²»"]

# ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
test_queries = [
    "äººå·¥çŸ¥èƒ½ã®ç ”ç©¶é–‹ç™ºã«å·¨é¡ã®æŠ•è³‡ãŒè¡Œã‚ã‚Œã¦ã„ã¾ã™ã€‚",
    "ãƒ—ãƒ­é‡çƒã®å„ªå‹ãƒãƒ¼ãƒ ãŒæ±ºå®šã—ã¾ã—ãŸã€‚",
    "æ–°ä½œã‚²ãƒ¼ãƒ ãŒä¸–ç•Œçš„ã«ãƒ’ãƒƒãƒˆã—ã¦ã„ã¾ã™ã€‚",
]

# åˆ†é¡å®Ÿè¡Œï¼ˆãƒ‡ãƒ¢ç”¨ã®æ“¬ä¼¼ã‚³ãƒ¼ãƒ‰ï¼‰
print("Few-shotä¾‹:")
for text, label in examples:
    print(f"  [{label}] {text}")

print("\nåˆ†é¡å¯¾è±¡:")
for query in test_queries:
    print(f"  - {query}")

# å®Ÿéš›ã®APIå‘¼ã³å‡ºã—ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
# results = classifier.batch_classify(test_queries, examples, labels)
#
# print("\nçµæœ:")
# for result in results:
#     if result['success']:
#         print(f"âœ… [{result['predicted_label']}] {result['query']}")
#     else:
#         print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
</code></pre>

<h3>ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ2: Chain-of-Thought æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³</h3>

<pre><code class="language-python">class ChainOfThoughtReasoner:
    """
    Chain-of-Thoughtæ¨è«–ã‚’å®Ÿè£…ã—ãŸå•é¡Œè§£æ±ºã‚¨ãƒ³ã‚¸ãƒ³
    """

    def __init__(self, api_key: str = None, model: str = "gpt-4"):
        """
        Args:
            api_key: OpenAI APIã‚­ãƒ¼
            model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆCoTã«ã¯GPT-4æ¨å¥¨ï¼‰
        """
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        self.model = model
        openai.api_key = self.api_key

    def solve_math_problem(self, problem: str) -> Dict[str, any]:
        """
        æ•°å­¦å•é¡Œã‚’CoTã§è§£ã

        Args:
            problem: å•é¡Œæ–‡

        Returns:
            è§£ç­”ã¨æ¨è«–éç¨‹
        """
        prompt = f"""
ä»¥ä¸‹ã®æ•°å­¦å•é¡Œã‚’æ®µéšçš„ã«è§£ã„ã¦ãã ã•ã„ã€‚
å„ã‚¹ãƒ†ãƒƒãƒ—ã§ä½•ã‚’è¨ˆç®—ã—ã¦ã„ã‚‹ã‹æ˜ç¢ºã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

å•é¡Œ: {problem}

è§£ç­”æ‰‹é †:
ã‚¹ãƒ†ãƒƒãƒ—1:"""

        try:
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=500
            )

            reasoning = response.choices[0].message.content

            # æœ€çµ‚ç­”ãˆã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
            lines = reasoning.split('\n')
            answer_line = [l for l in lines if 'ç­”ãˆ' in l or 'è§£ç­”' in l]
            final_answer = answer_line[-1] if answer_line else "æŠ½å‡ºå¤±æ•—"

            return {
                'problem': problem,
                'reasoning': reasoning,
                'final_answer': final_answer,
                'success': True
            }

        except Exception as e:
            return {
                'problem': problem,
                'error': str(e),
                'success': False
            }

    def solve_logic_puzzle(self, puzzle: str, question: str) -> Dict[str, any]:
        """
        è«–ç†ãƒ‘ã‚ºãƒ«ã‚’CoTã§è§£ã

        Args:
            puzzle: ãƒ‘ã‚ºãƒ«ã®çŠ¶æ³èª¬æ˜
            question: è§£ãã¹ãè³ªå•

        Returns:
            è§£ç­”ã¨æ¨è«–éç¨‹
        """
        prompt = f"""
ä»¥ä¸‹ã®è«–ç†ãƒ‘ã‚ºãƒ«ã‚’æ®µéšçš„ã«åˆ†æã—ã¦è§£ã„ã¦ãã ã•ã„ã€‚

çŠ¶æ³:
{puzzle}

è³ªå•: {question}

åˆ†ææ‰‹é †:
è¦³å¯Ÿ1:"""

        try:
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=600
            )

            reasoning = response.choices[0].message.content

            return {
                'puzzle': puzzle,
                'question': question,
                'reasoning': reasoning,
                'success': True
            }

        except Exception as e:
            return {
                'puzzle': puzzle,
                'error': str(e),
                'success': False
            }

# ä½¿ç”¨ä¾‹
print("=== Chain-of-Thought æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ ãƒ‡ãƒ¢ ===\n")

# reasoner = ChainOfThoughtReasoner()

# æ•°å­¦å•é¡Œ
math_problem = """
ã‚ã‚‹å•†å“ã®å®šä¾¡ã¯10,000å††ã§ã™ã€‚
ã‚»ãƒ¼ãƒ«ã§20%ã‚ªãƒ•ã«ãªã‚Šã€ã•ã‚‰ã«ã‚¯ãƒ¼ãƒãƒ³ã§500å††å¼•ãã•ã‚Œã¾ã—ãŸã€‚
æœ€çµ‚çš„ãªæ”¯æ‰•é¡ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚
"""

print("æ•°å­¦å•é¡Œ:")
print(math_problem)
print("\næœŸå¾…ã•ã‚Œã‚‹æ¨è«–:")
print("ã‚¹ãƒ†ãƒƒãƒ—1: 20%ã‚ªãƒ•ã®é‡‘é¡ã‚’è¨ˆç®— â†’ 10,000 Ã— 0.2 = 2,000å††")
print("ã‚¹ãƒ†ãƒƒãƒ—2: ã‚»ãƒ¼ãƒ«å¾Œã®ä¾¡æ ¼ â†’ 10,000 - 2,000 = 8,000å††")
print("ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¯ãƒ¼ãƒãƒ³é©ç”¨ â†’ 8,000 - 500 = 7,500å††")
print("ç­”ãˆ: 7,500å††")

# result = reasoner.solve_math_problem(math_problem)
# if result['success']:
#     print("\nCoTæ¨è«–çµæœ:")
#     print(result['reasoning'])
</code></pre>

<h3>ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ3: çµ±åˆå‹LLMãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ </h3>

<pre><code class="language-python">from datetime import datetime
from typing import Optional

class LLMChatbot:
    """
    è¤‡æ•°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŠ€è¡“ã‚’çµ±åˆã—ãŸãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ
    """

    def __init__(self, api_key: str = None, model: str = "gpt-3.5-turbo"):
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        self.model = model
        self.conversation_history = []
        openai.api_key = self.api_key

    def set_system_prompt(self, persona: str, capabilities: List[str]):
        """
        ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­å®šï¼ˆãƒœãƒƒãƒˆã®ãƒšãƒ«ã‚½ãƒŠï¼‰

        Args:
            persona: ãƒœãƒƒãƒˆã®æ€§æ ¼ãƒ»å½¹å‰²
            capabilities: ãƒœãƒƒãƒˆã®èƒ½åŠ›ãƒªã‚¹ãƒˆ
        """
        system_prompt = f"""
ã‚ãªãŸã¯{persona}ã§ã™ã€‚

ã‚ãªãŸã®èƒ½åŠ›:
{chr(10).join('- ' + cap for cap in capabilities)}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®å¯¾è©±ã§ã¯ã€è¦ªåˆ‡ã§æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
ä¸ç¢ºå®Ÿãªæƒ…å ±ã«ã¤ã„ã¦ã¯ã€ãã®æ—¨ã‚’æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚
"""
        self.conversation_history = [
            {"role": "system", "content": system_prompt}
        ]

    def chat(self,
            user_message: str,
            use_cot: bool = False,
            temperature: float = 0.7) -> Dict[str, any]:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¿œç­”

        Args:
            user_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›
            use_cot: Chain-of-Thoughtã‚’ä½¿ç”¨ã™ã‚‹ã‹
            temperature: å¿œç­”ã®å¤šæ§˜æ€§

        Returns:
            å¿œç­”çµæœ
        """
        # CoTãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¿½åŠ 
        if use_cot:
            user_message = f"""
{user_message}

ä¸Šè¨˜ã®è³ªå•ã«ç­”ãˆã‚‹éš›ã¯ã€æ®µéšçš„ã«è€ƒãˆã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
"""

        # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
        self.conversation_history.append({
            "role": "user",
            "content": user_message
        })

        try:
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=self.conversation_history,
                temperature=temperature,
                max_tokens=500
            )

            assistant_message = response.choices[0].message.content

            # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã‚’å±¥æ­´ã«è¿½åŠ 
            self.conversation_history.append({
                "role": "assistant",
                "content": assistant_message
            })

            return {
                'user': user_message,
                'assistant': assistant_message,
                'timestamp': datetime.now().isoformat(),
                'success': True
            }

        except Exception as e:
            return {
                'user': user_message,
                'error': str(e),
                'success': False
            }

    def get_conversation_summary(self) -> str:
        """ä¼šè©±å±¥æ­´ã®è¦ç´„ã‚’å–å¾—"""
        if len(self.conversation_history) <= 1:
            return "ä¼šè©±ã¯ã¾ã é–‹å§‹ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"

        summary = "ä¼šè©±å±¥æ­´:\n"
        for i, msg in enumerate(self.conversation_history[1:], 1):  # systemãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—
            role = "ãƒ¦ãƒ¼ã‚¶ãƒ¼" if msg["role"] == "user" else "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ"
            content = msg["content"][:100] + "..." if len(msg["content"]) > 100 else msg["content"]
            summary += f"{i}. [{role}] {content}\n"

        return summary

    def clear_history(self, keep_system: bool = True):
        """
        ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢

        Args:
            keep_system: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ä¿æŒã™ã‚‹ã‹
        """
        if keep_system and self.conversation_history:
            self.conversation_history = [self.conversation_history[0]]
        else:
            self.conversation_history = []

# ä½¿ç”¨ä¾‹
print("=== çµ±åˆå‹LLMãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ ãƒ‡ãƒ¢ ===\n")

# ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®åˆæœŸåŒ–
# bot = LLMChatbot(model="gpt-3.5-turbo")

# ãƒšãƒ«ã‚½ãƒŠè¨­å®š
persona = "è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯ŒãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ"
capabilities = [
    "ä¸€èˆ¬çš„ãªè³ªå•ã¸ã®å›ç­”",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®ã‚µãƒãƒ¼ãƒˆ",
    "æ•°å­¦ãƒ»è«–ç†å•é¡Œã®è§£æ±º",
    "æ–‡ç« ã®è¦ç´„ãƒ»ç¿»è¨³",
]

# bot.set_system_prompt(persona, capabilities)

print(f"ãƒœãƒƒãƒˆãƒšãƒ«ã‚½ãƒŠ: {persona}\n")
print("ä¼šè©±ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³:\n")

# ä¼šè©±ä¾‹ï¼ˆãƒ‡ãƒ¢ï¼‰
demo_conversations = [
    ("ã“ã‚“ã«ã¡ã¯ï¼Pythonã§ãƒªã‚¹ãƒˆã‚’é€†é †ã«ã™ã‚‹æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚", False),
    ("ãƒªã‚¹ãƒˆã®è¦ç´ æ•°ãŒ100ä¸‡å€‹ã®å ´åˆã€æœ€ã‚‚åŠ¹ç‡çš„ãªæ–¹æ³•ã¯ä½•ã§ã™ã‹ï¼Ÿ", True),  # CoTä½¿ç”¨
]

for user_msg, use_cot in demo_conversations:
    print(f"ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_msg}")

    if use_cot:
        print("   (Chain-of-Thoughtæ¨è«–ã‚’ä½¿ç”¨)")

    # å®Ÿéš›ã®APIå‘¼ã³å‡ºã—ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
    # result = bot.chat(user_msg, use_cot=use_cot)
    # if result['success']:
    #     print(f"ğŸ¤– ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {result['assistant']}")
    # else:
    #     print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")

    print()

# print(bot.get_conversation_summary())
</code></pre>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<details>
<summary><strong>æ¼”ç¿’5.1: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã®ç†è§£</strong></summary>

<p><strong>å•é¡Œ</strong>: ä»¥ä¸‹ã®æ¡ä»¶ã§2ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹å ´åˆã€ã©ã¡ã‚‰ãŒã‚ˆã‚Šé«˜æ€§èƒ½ã«ãªã‚‹ã¨äºˆæƒ³ã•ã‚Œã¾ã™ã‹ï¼ŸChinchillaã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã«åŸºã¥ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<ul>
<li>ãƒ¢ãƒ‡ãƒ«A: 200Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€1å…†ãƒˆãƒ¼ã‚¯ãƒ³ã§è¨“ç·´</li>
<li>ãƒ¢ãƒ‡ãƒ«B: 70Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€4å…†ãƒˆãƒ¼ã‚¯ãƒ³ã§è¨“ç·´</li>
</ul>

<p><strong>ãƒ’ãƒ³ãƒˆ</strong>: Chinchillaæœ€é©æ¯”ç‡ã¯ã€Œãƒ‡ãƒ¼ã‚¿ãƒˆãƒ¼ã‚¯ãƒ³æ•° â‰ˆ 20 Ã— ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€ã§ã™ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>åˆ†æ</strong>:</p>

<ul>
<li>ãƒ¢ãƒ‡ãƒ«A: æœ€é©ãƒ‡ãƒ¼ã‚¿é‡ = 200B Ã— 20 = 4å…†ãƒˆãƒ¼ã‚¯ãƒ³ â†’ å®Ÿéš›ã¯1å…†ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆä¸è¶³ï¼‰</li>
<li>ãƒ¢ãƒ‡ãƒ«B: æœ€é©ãƒ‡ãƒ¼ã‚¿é‡ = 70B Ã— 20 = 1.4å…†ãƒˆãƒ¼ã‚¯ãƒ³ â†’ å®Ÿéš›ã¯4å…†ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆéå‰°ã ãŒè¨±å®¹ç¯„å›²ï¼‰</li>
</ul>

<p><strong>çµè«–</strong>: ãƒ¢ãƒ‡ãƒ«Bã®æ–¹ãŒé«˜æ€§èƒ½ã«ãªã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã€‚ãƒ¢ãƒ‡ãƒ«Aã¯ã€Œéå‰°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã€ã•ã‚Œã¦ãŠã‚Šã€ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã«ã‚ˆã‚Šæ€§èƒ½ãŒé ­æ‰“ã¡ã«ãªã‚Šã¾ã™ã€‚Chinchillaè«–æ–‡ãŒç¤ºã™ã‚ˆã†ã«ã€åŒã˜è¨ˆç®—äºˆç®—ãªã‚‰ã‚ˆã‚Šå°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‚’ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã™ã‚‹æ–¹ãŒåŠ¹ç‡çš„ã§ã™ã€‚</p>
</details>
</details>

<details>
<summary><strong>æ¼”ç¿’5.2: Few-shotãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¨­è¨ˆ</strong></summary>

<p><strong>å•é¡Œ</strong>: ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹åŠ¹æœçš„ãªFew-shotãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚</p>

<p><strong>ã‚¿ã‚¹ã‚¯</strong>: å•†å“ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰ã€Œè©•ä¾¡ã‚¹ã‚³ã‚¢ï¼ˆ1-5ï¼‰ã€ã¨ã€Œä¸»ãªç†ç”±ã€ã‚’æŠ½å‡ºã™ã‚‹</p>

<p><strong>è¦ä»¶</strong>:</p>
<ul>
<li>3ã¤ã®ä¾‹ç¤ºã‚’å«ã‚ã‚‹</li>
<li>å‡ºåŠ›å½¢å¼ã‚’æ˜ç¢ºã«æŒ‡å®šã™ã‚‹</li>
<li>ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ï¼ˆæ›–æ˜§ãªè©•ä¾¡ï¼‰ã‚‚è€ƒæ…®ã™ã‚‹</li>
</ul>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code>ä»¥ä¸‹ã®å•†å“ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰ã€Œè©•ä¾¡ã‚¹ã‚³ã‚¢ï¼ˆ1-5ï¼‰ã€ã¨ã€Œä¸»ãªç†ç”±ã€ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ä¾‹1:
ãƒ¬ãƒ“ãƒ¥ãƒ¼: ã“ã®æƒé™¤æ©Ÿã¯å¸å¼•åŠ›ãŒå¼·ãã€è»½é‡ã§ä½¿ã„ã‚„ã™ã„ã§ã™ã€‚ä¾¡æ ¼ã‚‚æ‰‹é ƒã§å¤§æº€è¶³ã§ã™ã€‚
å‡ºåŠ›: {"score": 5, "reason": "å¸å¼•åŠ›ã€è»½é‡æ€§ã€ã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹"}

ä¾‹2:
ãƒ¬ãƒ“ãƒ¥ãƒ¼: ãƒ‡ã‚¶ã‚¤ãƒ³ã¯è‰¯ã„ã®ã§ã™ãŒã€ãƒãƒƒãƒ†ãƒªãƒ¼ã®æŒã¡ãŒæ‚ªãã€ã™ãã«å……é›»ãŒå¿…è¦ã§ã™ã€‚
å‡ºåŠ›: {"score": 2, "reason": "ãƒãƒƒãƒ†ãƒªãƒ¼æŒç¶šæ™‚é–“ã®çŸ­ã•"}

ä¾‹3:
ãƒ¬ãƒ“ãƒ¥ãƒ¼: æ™®é€šã®å•†å“ã§ã™ã€‚ç‰¹ã«è‰¯ãã‚‚æ‚ªãã‚‚ã‚ã‚Šã¾ã›ã‚“ã€‚
å‡ºåŠ›: {"score": 3, "reason": "ç‰¹ç­†ã™ã¹ãç‰¹å¾´ãªã—"}

ãƒ¬ãƒ“ãƒ¥ãƒ¼: {å…¥åŠ›ãƒ¬ãƒ“ãƒ¥ãƒ¼}
å‡ºåŠ›:
</code></pre>

<p><strong>è¨­è¨ˆã®ãƒã‚¤ãƒ³ãƒˆ</strong>:</p>
<ul>
<li>ãƒã‚¸ãƒ†ã‚£ãƒ–ï¼ˆä¾‹1ï¼‰ã€ãƒã‚¬ãƒ†ã‚£ãƒ–ï¼ˆä¾‹2ï¼‰ã€ä¸­ç«‹ï¼ˆä¾‹3ï¼‰ã®ä¾‹ã‚’ãƒãƒ©ãƒ³ã‚¹è‰¯ãå«ã‚ã‚‹</li>
<li>JSONå½¢å¼ã§æ§‹é€ åŒ–å‡ºåŠ›ã‚’æŒ‡å®šã—ã€ãƒ‘ãƒ¼ã‚¹ã—ã‚„ã™ãã™ã‚‹</li>
<li>ã€Œç†ç”±ã€ã¯ç°¡æ½”ãªè¦ç´„ã¨ã—ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼å…¨æ–‡ã‚’ã‚³ãƒ”ãƒ¼ã—ãªã„ã‚ˆã†èª˜å°</li>
</ul>
</details>
</details>

<details>
<summary><strong>æ¼”ç¿’5.3: Chain-of-Thoughtæ¨è«–ã®å®Ÿè£…</strong></summary>

<p><strong>å•é¡Œ</strong>: ä»¥ä¸‹ã®è«–ç†ãƒ‘ã‚ºãƒ«ã‚’è§£ãCoTãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚</p>

<p><strong>ãƒ‘ã‚ºãƒ«</strong>:</p>
<blockquote>
3äººã®å®¹ç–‘è€…Aã€Bã€CãŒã„ã¾ã™ã€‚
<ul>
<li>Aã¯ã€ŒBãŒçŠ¯äººã ã€ã¨è¨€ã£ã¦ã„ã¾ã™</li>
<li>Bã¯ã€Œç§ã¯ç„¡å®Ÿã ã€ã¨è¨€ã£ã¦ã„ã¾ã™</li>
<li>Cã¯ã€ŒAãŒçŠ¯äººã ã€ã¨è¨€ã£ã¦ã„ã¾ã™</li>
</ul>
çŠ¯äººã¯1äººã§ã€ãã®äººã ã‘ãŒå˜˜ã‚’ã¤ã„ã¦ã„ã¾ã™ã€‚çŠ¯äººã¯èª°ã§ã™ã‹ï¼Ÿ
</blockquote>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code>ä»¥ä¸‹ã®è«–ç†ãƒ‘ã‚ºãƒ«ã‚’æ®µéšçš„ã«åˆ†æã—ã¦è§£ã„ã¦ãã ã•ã„ã€‚

ãƒ‘ã‚ºãƒ«:
3äººã®å®¹ç–‘è€…Aã€Bã€CãŒã„ã¾ã™ã€‚
- Aã¯ã€ŒBãŒçŠ¯äººã ã€ã¨è¨€ã£ã¦ã„ã¾ã™
- Bã¯ã€Œç§ã¯ç„¡å®Ÿã ã€ã¨è¨€ã£ã¦ã„ã¾ã™
- Cã¯ã€ŒAãŒçŠ¯äººã ã€ã¨è¨€ã£ã¦ã„ã¾ã™
çŠ¯äººã¯1äººã§ã€ãã®äººã ã‘ãŒå˜˜ã‚’ã¤ã„ã¦ã„ã¾ã™ã€‚çŠ¯äººã¯èª°ã§ã™ã‹ï¼Ÿ

æ®µéšçš„åˆ†æ:

ä»®å®š1: AãŒçŠ¯äººã®å ´åˆ
 - Aã®ç™ºè¨€ã€ŒBãŒçŠ¯äººã€ã¯å˜˜ â†’ âœ“ çŠ¯äººã¯å˜˜ã‚’ã¤ã
 - Bã®ç™ºè¨€ã€Œç§ã¯ç„¡å®Ÿã€ã¯çœŸå®Ÿ â†’ âœ“ Bã¨Cã¯çœŸå®Ÿã‚’è¨€ã†
 - Cã®ç™ºè¨€ã€ŒAãŒçŠ¯äººã€ã¯çœŸå®Ÿ â†’ âœ“ çŸ›ç›¾ãªã—
 çµè«–: AãŒçŠ¯äººã®å¯èƒ½æ€§ã‚ã‚Š

ä»®å®š2: BãŒçŠ¯äººã®å ´åˆ
 - Aã®ç™ºè¨€ã€ŒBãŒçŠ¯äººã€ã¯çœŸå®Ÿ â†’ âœ— çŸ›ç›¾ï¼ˆçŠ¯äººä»¥å¤–ã‚‚å˜˜ï¼Ÿï¼‰
 - Bã®ç™ºè¨€ã€Œç§ã¯ç„¡å®Ÿã€ã¯å˜˜ â†’ âœ“ çŠ¯äººã¯å˜˜ã‚’ã¤ã
 - Cã®ç™ºè¨€ã€ŒAãŒçŠ¯äººã€ã¯å˜˜ â†’ âœ— çŸ›ç›¾ï¼ˆ2äººãŒå˜˜ï¼Ÿï¼‰
 çµè«–: æ¡ä»¶ã«çŸ›ç›¾

ä»®å®š3: CãŒçŠ¯äººã®å ´åˆ
 - Aã®ç™ºè¨€ã€ŒBãŒçŠ¯äººã€ã¯å˜˜ â†’ âœ— çŸ›ç›¾ï¼ˆçŠ¯äººä»¥å¤–ã‚‚å˜˜ï¼Ÿï¼‰
 - Bã®ç™ºè¨€ã€Œç§ã¯ç„¡å®Ÿã€ã¯çœŸå®Ÿ â†’ âœ“
 - Cã®ç™ºè¨€ã€ŒAãŒçŠ¯äººã€ã¯å˜˜ â†’ âœ“ çŠ¯äººã¯å˜˜ã‚’ã¤ã
 çµè«–: æ¡ä»¶ã«çŸ›ç›¾

æœ€çµ‚çµè«–: AãŒçŠ¯äººã§ã™ã€‚
ç†ç”±: ä»®å®š1ã®ã¿ãŒã™ã¹ã¦ã®æ¡ä»¶ã‚’æº€ãŸã—ã¾ã™ã€‚
</code></pre>

<p><strong>CoTè¨­è¨ˆã®ãƒã‚¤ãƒ³ãƒˆ</strong>:</p>
<ul>
<li>ã™ã¹ã¦ã®å¯èƒ½æ€§ã‚’ä½“ç³»çš„ã«æ¤œè¨¼ã™ã‚‹</li>
<li>å„ä»®å®šã§çŸ›ç›¾ã®æœ‰ç„¡ã‚’æ˜ç¢ºã«ãƒã‚§ãƒƒã‚¯ã™ã‚‹</li>
<li>è¨˜å·ï¼ˆâœ“, âœ—ï¼‰ã§è¦–è¦šçš„ã«åˆ†ã‹ã‚Šã‚„ã™ãã™ã‚‹</li>
</ul>
</details>
</details>

<details>
<summary><strong>æ¼”ç¿’5.4: RLHFã®ç†è§£</strong></summary>

<p><strong>å•é¡Œ</strong>: RLHFã§ä½¿ç”¨ã•ã‚Œã‚‹KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹åˆ¶ç´„ $\beta \cdot D_{KL}(\pi_{\theta} || \pi_{\text{ref}})$ ã®å½¹å‰²ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€$\beta$ãŒå¤§ãã™ãã‚‹å ´åˆã¨å°ã•ã™ãã‚‹å ´åˆã®å•é¡Œç‚¹ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹åˆ¶ç´„ã®å½¹å‰²</strong>:</p>

<ol>
<li><strong>ãƒ¢ãƒ¼ãƒ‰ã®å´©å£Šé˜²æ­¢</strong>: æœ€é©åŒ–ä¸­ã®ãƒ¢ãƒ‡ãƒ« $\pi_{\theta}$ ãŒå‚ç…§ãƒ¢ãƒ‡ãƒ« $\pi_{\text{ref}}$ï¼ˆäº‹å‰å­¦ç¿’æ¸ˆã¿ï¼‰ã‹ã‚‰å¤§ããé€¸è„±ã—ãªã„ã‚ˆã†åˆ¶ç´„ã—ã¾ã™ã€‚</li>
<li><strong>è¨€èªèƒ½åŠ›ã®ä¿æŒ</strong>: å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹éç¨‹ã§ã€æ–‡æ³•ã‚„ä¸€è²«æ€§ãªã©ã®åŸºæœ¬çš„ãªè¨€èªèƒ½åŠ›ãŒå¤±ã‚ã‚Œã‚‹ã®ã‚’é˜²ãã¾ã™ã€‚</li>
<li><strong>å ±é…¬ãƒãƒƒã‚­ãƒ³ã‚°å›é¿</strong>: ãƒ¢ãƒ‡ãƒ«ãŒå ±é…¬ãƒ¢ãƒ‡ãƒ«ã®è„†å¼±æ€§ã‚’æ‚ªç”¨ã™ã‚‹æ¥µç«¯ãªæˆ¦ç•¥ã‚’å­¦ç¿’ã™ã‚‹ã®ã‚’é˜²ãã¾ã™ã€‚</li>
</ol>

<p><strong>$\beta$ãŒå¤§ãã™ãã‚‹å ´åˆ</strong>:</p>
<ul>
<li>å•é¡Œ: ãƒ¢ãƒ‡ãƒ«ãŒå‚ç…§ãƒ¢ãƒ‡ãƒ«ã«éåº¦ã«è¿‘ããªã‚Šã€RLHFã®åŠ¹æœãŒè–„ã‚Œã‚‹</li>
<li>çµæœ: äººé–“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãŒã»ã¨ã‚“ã©åæ˜ ã•ã‚Œãšã€æ”¹å–„ãŒè¦‹ã‚‰ã‚Œãªã„</li>
</ul>

<p><strong>$\beta$ãŒå°ã•ã™ãã‚‹å ´åˆ</strong>:</p>
<ul>
<li>å•é¡Œ: ãƒ¢ãƒ‡ãƒ«ãŒå‚ç…§ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å¤§ããé€¸è„±ã—ã€ä¸è‡ªç„¶ãªå‡ºåŠ›ã‚’ç”Ÿæˆ</li>
<li>çµæœ: æ–‡æ³•å´©å£Šã€æ„å‘³ä¸æ˜ãªå¿œç­”ã€å ±é…¬ãƒãƒƒã‚­ãƒ³ã‚°</li>
</ul>

<p><strong>å®Ÿç”¨çš„ãª$\beta$ã®é¸æŠ</strong>:</p>
<ul>
<li>ä¸€èˆ¬çš„ãªç¯„å›²: 0.01ã€œ0.1</li>
<li>èª¿æ•´æ–¹æ³•: æ¤œè¨¼ã‚»ãƒƒãƒˆã§ã®äººé–“è©•ä¾¡ã«åŸºã¥ã„ã¦æœ€é©å€¤ã‚’æ¢ç´¢</li>
</ul>
</details>
</details>

<details>
<summary><strong>æ¼”ç¿’5.5: LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­è¨ˆ</strong></summary>

<p><strong>å•é¡Œ</strong>: ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆç”¨ã®LLMãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®è¦ä»¶ã‚’æº€ãŸã™ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæˆ¦ç•¥ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚</p>

<p><strong>è¦ä»¶</strong>:</p>
<ul>
<li>ã‚ˆãã‚ã‚‹è³ªå•ï¼ˆFAQï¼‰ã«ã¯å³åº§ã«å›ç­”</li>
<li>è¤‡é›‘ãªå•é¡Œã«ã¯æ®µéšçš„ã«å¯¾å¿œ</li>
<li>ä¸ç¢ºå®Ÿãªå ´åˆã¯äººé–“ã®ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</li>
<li>ä¼šè©±å±¥æ­´ã‚’è€ƒæ…®ã—ãŸæ–‡è„ˆç†è§£</li>
</ul>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</strong>:</p>

<pre><code class="language-python">class CustomerSupportChatbot:
    """
    ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆç”¨LLMãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ
    """

    def __init__(self):
        self.faq_database = self.load_faq()
        self.conversation_history = []
        self.escalation_threshold = 0.3  # ä¿¡é ¼åº¦ã—ãã„å€¤

    def load_faq(self):
        """FAQãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰"""
        return {
            "é…é€ã«ã‹ã‹ã‚‹æ—¥æ•°": "é€šå¸¸3-5å–¶æ¥­æ—¥ã§ãŠå±Šã‘ã—ã¾ã™ã€‚",
            "è¿”å“ãƒãƒªã‚·ãƒ¼": "è³¼å…¥ã‹ã‚‰30æ—¥ä»¥å†…ã§ã‚ã‚Œã°è¿”å“å¯èƒ½ã§ã™ã€‚",
            "æ”¯æ‰•ã„æ–¹æ³•": "ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã€éŠ€è¡ŒæŒ¯è¾¼ã€ä»£å¼•ãã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚",
        }

    def check_faq(self, query: str) -> Optional[str]:
        """FAQã«ä¸€è‡´ã™ã‚‹è³ªå•ã‚’ãƒã‚§ãƒƒã‚¯"""
        # ç°¡ç•¥åŒ–: å®Ÿéš›ã¯åŸ‹ã‚è¾¼ã¿ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦æ¤œç´¢ã‚’ä½¿ç”¨
        for question, answer in self.faq_database.items():
            if question in query:
                return answer
        return None

    def classify_complexity(self, query: str) -> str:
        """å•ã„åˆã‚ã›ã®è¤‡é›‘åº¦ã‚’åˆ†é¡"""
        complexity_prompt = f"""
ä»¥ä¸‹ã®å•ã„åˆã‚ã›ã‚’ã€Œç°¡å˜ã€ã€Œä¸­ç¨‹åº¦ã€ã€Œè¤‡é›‘ã€ã«åˆ†é¡ã—ã¦ãã ã•ã„ã€‚

å•ã„åˆã‚ã›: {query}

åˆ†é¡:"""
        # LLMå‘¼ã³å‡ºã—ï¼ˆæ“¬ä¼¼ã‚³ãƒ¼ãƒ‰ï¼‰
        # complexity = call_llm(complexity_prompt)
        return "ä¸­ç¨‹åº¦"  # ãƒ‡ãƒ¢ç”¨

    def handle_query(self, query: str) -> Dict:
        """å•ã„åˆã‚ã›ã‚’å‡¦ç†"""
        # Step 1: FAQãƒã‚§ãƒƒã‚¯
        faq_answer = self.check_faq(query)
        if faq_answer:
            return {
                'type': 'faq',
                'answer': faq_answer,
                'confidence': 1.0
            }

        # Step 2: è¤‡é›‘åº¦è©•ä¾¡
        complexity = self.classify_complexity(query)

        # Step 3: è¤‡é›‘åº¦ã«å¿œã˜ãŸå‡¦ç†
        if complexity == "ç°¡å˜":
            return self.simple_response(query)
        elif complexity == "ä¸­ç¨‹åº¦":
            return self.cot_response(query)
        else:
            return self.escalate_to_human(query)

    def simple_response(self, query: str):
        """ã‚·ãƒ³ãƒ—ãƒ«ãªZero-shotå¿œç­”"""
        prompt = f"""
ã‚ãªãŸã¯ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®è³ªå•ã«ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚

è³ªå•: {query}

å›ç­”:"""
        # response = call_llm(prompt)
        return {'type': 'simple', 'answer': "å¿œç­”å†…å®¹"}

    def cot_response(self, query: str):
        """Chain-of-Thoughtã§æ®µéšçš„ã«å¯¾å¿œ"""
        prompt = f"""
ã‚ãªãŸã¯ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®å•é¡Œã‚’æ®µéšçš„ã«åˆ†æã—ã€è§£æ±ºç­–ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚

å•é¡Œ: {query}

åˆ†æ:
ã‚¹ãƒ†ãƒƒãƒ—1:"""
        # response = call_llm(prompt)
        return {'type': 'cot', 'answer': "æ®µéšçš„ãªå¿œç­”"}

    def escalate_to_human(self, query: str):
        """äººé–“ã®ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        return {
            'type': 'escalation',
            'message': "ã“ã®å•é¡Œã¯è¤‡é›‘ãªãŸã‚ã€å°‚é–€ã®ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ã«ãŠç¹‹ãã—ã¾ã™ã€‚",
            'query': query
        }
</code></pre>

<p><strong>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæˆ¦ç•¥</strong>:</p>

<ol>
<li><strong>ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ</strong>: ãƒœãƒƒãƒˆã®å½¹å‰²ã€ãƒˆãƒ¼ãƒ³ã€åˆ¶ç´„ã‚’æ˜ç¢ºã«å®šç¾©</li>
<li><strong>Few-shot FAQ</strong>: é¡ä¼¼è³ªå•ã®ä¾‹ã‚’æç¤ºã—ã¦ç²¾åº¦å‘ä¸Š</li>
<li><strong>CoT for Complex Issues</strong>: è¤‡é›‘ãªå•é¡Œã¯æ®µéšçš„ã«åˆ†æ</li>
<li><strong>Confidence Scoring</strong>: å¿œç­”ã®ä¿¡é ¼åº¦ã‚’è©•ä¾¡ã—ã€ä½ã„å ´åˆã¯ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</li>
</ol>

<p><strong>è©•ä¾¡æŒ‡æ¨™</strong>:</p>
<ul>
<li>FAQä¸€è‡´ç‡: 70%ä»¥ä¸Š</li>
<li>ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç‡: 15%ä»¥ä¸‹</li>
<li>é¡§å®¢æº€è¶³åº¦: 4.0/5.0ä»¥ä¸Š</li>
</ul>
</details>
</details>

<hr>

<h2>ã¾ã¨ã‚</h2>

<p>ã“ã®ç« ã§ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®æœ¬è³ªã¨å®Ÿè·µçš„ãªæ´»ç”¨æ–¹æ³•ã‚’å­¦ã³ã¾ã—ãŸï¼š</p>

<ul>
<li>âœ… <strong>ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡</strong>: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã€ãƒ‡ãƒ¼ã‚¿é‡ã€è¨ˆç®—é‡ã®é–¢ä¿‚ã‚’ç†è§£ã—ã€Chinchillaæœ€é©æ¯”ç‡ã‚’æŠŠæ¡ã—ã¾ã—ãŸ</li>
<li>âœ… <strong>ä¸»è¦LLM</strong>: GPTã€LLaMAã€Claudeã€Geminiãªã©ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨Differentiatorã‚’æ¯”è¼ƒã—ã¾ã—ãŸ</li>
<li>âœ… <strong>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</strong>: Zero-shotã€Few-shotã€Chain-of-Thoughtãªã©ã®æŠ€è¡“ã‚’å®Ÿè£…ã—ã¾ã—ãŸ</li>
<li>âœ… <strong>In-Context Learning</strong>: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ãªã—ã«æ–°ã‚¿ã‚¹ã‚¯ã‚’å­¦ç¿’ã™ã‚‹ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ç†è§£ã—ã¾ã—ãŸ</li>
<li>âœ… <strong>RLHF</strong>: äººé–“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æ´»ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«æ”¹å–„ãƒ—ãƒ­ã‚»ã‚¹ã‚’å­¦ã³ã¾ã—ãŸ</li>
<li>âœ… <strong>å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ</strong>: Few-shotåˆ†é¡ã€CoTæ¨è«–ã€çµ±åˆãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’æ§‹ç¯‰ã—ã¾ã—ãŸ</li>
</ul>

<blockquote>
<p><strong>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</strong>: LLMã®åŸºç¤ã‚’ç†è§£ã—ãŸã‚‰ã€æ¬¡ã¯ç‰¹å®šãƒ‰ãƒ¡ã‚¤ãƒ³ã¸ã®Fine-tuningã€RAGï¼ˆRetrieval-Augmented Generationï¼‰ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMãªã©ã€ã‚ˆã‚Šé«˜åº¦ãªãƒˆãƒ”ãƒƒã‚¯ã«é€²ã¿ã¾ã—ã‚‡ã†ã€‚ã¾ãŸã€è²¬ä»»ã‚ã‚‹AIé–‹ç™ºã®ãŸã‚ã®å€«ç†çš„é…æ…®ã¨ãƒã‚¤ã‚¢ã‚¹è»½æ¸›æŠ€è¡“ã‚‚é‡è¦ã§ã™ã€‚</p>
</blockquote>

    <section class="disclaimer">
<h3>å…è²¬äº‹é …</h3>
<ul>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹Code examplesã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
<li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
</ul>
</section>

</main>

    <nav class="navigation">
        <a href="chapter4-bert-gpt.html" class="nav-button">â† ç¬¬4ç« ï¼šBERTã¨GPT</a>
        <a href="../index.html" class="nav-button">ã‚³ãƒ¼ã‚¹ä¸€è¦§ã«æˆ»ã‚‹ â†’</a>
    </nav>

    <footer>
        <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
