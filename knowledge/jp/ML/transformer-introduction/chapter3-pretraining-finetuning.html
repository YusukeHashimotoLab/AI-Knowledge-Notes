<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬3ç« ï¼šäº‹å‰å­¦ç¿’ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: var(--font-body); line-height: 1.7; color: var(--color-text); background-color: var(--color-bg); font-size: 16px; }
        header { background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; padding: var(--spacing-xl) var(--spacing-md); margin-bottom: var(--spacing-xl); box-shadow: var(--box-shadow); }
        .header-content { max-width: 900px; margin: 0 auto; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: var(--spacing-sm); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; opacity: 0.95; font-weight: 400; margin-bottom: var(--spacing-md); }
        .meta { display: flex; flex-wrap: wrap; gap: var(--spacing-md); font-size: 0.9rem; opacity: 0.9; }
        .meta-item { display: flex; align-items: center; gap: 0.3rem; }
        .container { max-width: 900px; margin: 0 auto; padding: 0 var(--spacing-md) var(--spacing-xl); }
        h2 { font-size: 1.75rem; color: var(--color-primary); margin-top: var(--spacing-xl); margin-bottom: var(--spacing-md); padding-bottom: var(--spacing-xs); border-bottom: 3px solid var(--color-accent); }
        h3 { font-size: 1.4rem; color: var(--color-primary); margin-top: var(--spacing-lg); margin-bottom: var(--spacing-sm); }
        h4 { font-size: 1.1rem; color: var(--color-primary-dark); margin-top: var(--spacing-md); margin-bottom: var(--spacing-sm); }
        p { margin-bottom: var(--spacing-md); color: var(--color-text); }
        a { color: var(--color-link); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--color-link-hover); text-decoration: underline; }
        ul, ol { margin-left: var(--spacing-lg); margin-bottom: var(--spacing-md); }
        li { margin-bottom: var(--spacing-xs); color: var(--color-text); }
        pre { background-color: var(--color-code-bg); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); overflow-x: auto; margin-bottom: var(--spacing-md); font-family: var(--font-mono); font-size: 0.9rem; line-height: 1.5; }
        code { font-family: var(--font-mono); font-size: 0.9em; background-color: var(--color-code-bg); padding: 0.2em 0.4em; border-radius: 3px; }
        pre code { background-color: transparent; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: var(--spacing-md); font-size: 0.95rem; }
        th, td { border: 1px solid var(--color-border); padding: var(--spacing-sm); text-align: left; }
        th { background-color: var(--color-bg-alt); font-weight: 600; color: var(--color-primary); }
        blockquote { border-left: 4px solid var(--color-accent); padding-left: var(--spacing-md); margin: var(--spacing-md) 0; color: var(--color-text-light); font-style: italic; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        .mermaid { text-align: center; margin: var(--spacing-lg) 0; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        details { background-color: var(--color-bg-alt); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); margin-bottom: var(--spacing-md); }
        summary { cursor: pointer; font-weight: 600; color: var(--color-primary); user-select: none; padding: var(--spacing-xs); margin: calc(-1 * var(--spacing-md)); padding: var(--spacing-md); border-radius: var(--border-radius); }
        summary:hover { background-color: rgba(123, 44, 191, 0.1); }
        details[open] summary { margin-bottom: var(--spacing-md); border-bottom: 1px solid var(--color-border); }
        .navigation { display: flex; justify-content: space-between; gap: var(--spacing-md); margin: var(--spacing-xl) 0; padding-top: var(--spacing-lg); border-top: 2px solid var(--color-border); }
        .nav-button { flex: 1; padding: var(--spacing-md); background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; border-radius: var(--border-radius); text-align: center; font-weight: 600; transition: transform 0.2s, box-shadow 0.2s; box-shadow: var(--box-shadow); }
        .nav-button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); text-decoration: none; }
        footer { margin-top: var(--spacing-xl); padding: var(--spacing-lg) var(--spacing-md); background-color: var(--color-bg-alt); border-top: 1px solid var(--color-border); text-align: center; font-size: 0.9rem; color: var(--color-text-light); }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }

        @media (max-width: 768px) { h1 { font-size: 1.5rem; } h2 { font-size: 1.4rem; } h3 { font-size: 1.2rem; } .meta { font-size: 0.85rem; } .navigation { flex-direction: column; } table { font-size: 0.85rem; } th, td { padding: var(--spacing-xs); } }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/AI-Knowledge-Notes/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/AI-Knowledge-Notes/knowledge/jp/ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="/AI-Knowledge-Notes/knowledge/jp/ML/transformer-introduction/index.html">Transformer</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬3ç« ï¼šäº‹å‰å­¦ç¿’ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</h1>
            <p class="subtitle">Transfer Learningã§ã‚¿ã‚¹ã‚¯ç‰¹åŒ–å‹ãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹ç‡çš„ã«æ§‹ç¯‰ - MLMã‹ã‚‰LoRAã¾ã§</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… äº‹å‰å­¦ç¿’ã®é‡è¦æ€§ã¨Transfer Learningã®åŸç†ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Masked Language Modelingï¼ˆMLMï¼‰ã®ä»•çµ„ã¿ã¨å®Ÿè£…æ–¹æ³•ã‚’ç¿’å¾—ã™ã‚‹</li>
<li>âœ… Causal Language Modelingï¼ˆCLMï¼‰ã¨MLMã®é•ã„ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Hugging Face Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®åŸºæœ¬çš„ãªä½¿ã„æ–¹ã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹</li>
<li>âœ… å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®æ‰‹æ³•ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… LoRAï¼ˆLow-Rank Adaptationï¼‰ã®åŸç†ã¨åŠ¹ç‡æ€§ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… å®Ÿéš›ã®æ„Ÿæƒ…åˆ†æã‚¿ã‚¹ã‚¯ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã§ãã‚‹</li>
<li>âœ… åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã‚’é¸æŠã§ãã‚‹</li>
</ul>

<hr>

<h2>3.1 äº‹å‰å­¦ç¿’ã®é‡è¦æ€§</h2>

<h3>Transfer Learningã¨ã¯</h3>

<p><strong>Transfer Learningï¼ˆè»¢ç§»å­¦ç¿’ï¼‰</strong>ã¯ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ãŸæ±ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’ã€ç‰¹å®šã‚¿ã‚¹ã‚¯ã«é©å¿œã•ã›ã‚‹æŠ€è¡“ã§ã™ã€‚Transformerã®æˆåŠŸã¯ã“ã®æ‰‹æ³•ã«å¤§ããä¾å­˜ã—ã¦ã„ã¾ã™ã€‚</p>

<blockquote>
<p>ã€Œæ•°ç™¾GBã®ãƒ†ã‚­ã‚¹ãƒˆã§äº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ã€æ•°åƒã€œæ•°ä¸‡ã‚µãƒ³ãƒ—ãƒ«ã®ã‚¿ã‚¹ã‚¯å›ºæœ‰ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€é«˜æ€§èƒ½ãªã‚¿ã‚¹ã‚¯ç‰¹åŒ–å‹ãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹ç‡çš„ã«æ§‹ç¯‰ã§ãã‚‹ã€</p>
</blockquote>

<div class="mermaid">
graph LR
    A[å¤§è¦æ¨¡ãƒ†ã‚­ã‚¹ãƒˆ<br/>æ•°ç™¾GB] --> B[äº‹å‰å­¦ç¿’<br/>MLM/CLM]
    B --> C[æ±ç”¨ãƒ¢ãƒ‡ãƒ«<br/>BERT/GPT]
    C --> D1[ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°<br/>æ„Ÿæƒ…åˆ†æ]
    C --> D2[ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°<br/>è³ªå•å¿œç­”]
    C --> D3[ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°<br/>å›ºæœ‰è¡¨ç¾èªè­˜]
    D1 --> E1[ã‚¿ã‚¹ã‚¯ç‰¹åŒ–<br/>ãƒ¢ãƒ‡ãƒ«1]
    D2 --> E2[ã‚¿ã‚¹ã‚¯ç‰¹åŒ–<br/>ãƒ¢ãƒ‡ãƒ«2]
    D3 --> E3[ã‚¿ã‚¹ã‚¯ç‰¹åŒ–<br/>ãƒ¢ãƒ‡ãƒ«3]

    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D1 fill:#e8f5e9
    style D2 fill:#e8f5e9
    style D3 fill:#e8f5e9
</div>

<h3>å¾“æ¥æ‰‹æ³•ã¨ã®æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</th>
<th>è¨“ç·´ãƒ‡ãƒ¼ã‚¿é‡</th>
<th>è¨ˆç®—ã‚³ã‚¹ãƒˆ</th>
<th>æ€§èƒ½</th>
<th>æ±åŒ–æ€§èƒ½</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ã‚¹ã‚¯ãƒ©ãƒƒãƒå­¦ç¿’</strong></td>
<td>å¤§é‡ï¼ˆæ•°ç™¾ä¸‡ã€œï¼‰</td>
<td>éå¸¸ã«é«˜ã„</td>
<td>ãƒ‡ãƒ¼ã‚¿ä¾å­˜</td>
<td>ã‚¿ã‚¹ã‚¯ç‰¹åŒ–çš„</td>
</tr>
<tr>
<td><strong>ç‰¹å¾´æŠ½å‡ºã®ã¿</strong></td>
<td>ä¸­ç¨‹åº¦ï¼ˆæ•°åƒã€œï¼‰</td>
<td>ä½ã„</td>
<td>ä¸­ç¨‹åº¦</td>
<td>æ±ç”¨è¡¨ç¾</td>
</tr>
<tr>
<td><strong>ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</strong></td>
<td>å°‘é‡ï¼ˆæ•°ç™¾ã€œï¼‰</td>
<td>ä¸­ç¨‹åº¦</td>
<td>é«˜ã„</td>
<td>ä¸¡æ–¹ã‚’ç²å¾—</td>
</tr>
<tr>
<td><strong>LoRA/Adapter</strong></td>
<td>å°‘é‡ï¼ˆæ•°ç™¾ã€œï¼‰</td>
<td>éå¸¸ã«ä½ã„</td>
<td>é«˜ã„</td>
<td>åŠ¹ç‡çš„</td>
</tr>
</tbody>
</table>

<h3>äº‹å‰å­¦ç¿’ã®ãƒ¡ãƒªãƒƒãƒˆ</h3>

<ul>
<li><strong>è¨€èªçŸ¥è­˜ã®ç²å¾—</strong>ï¼šæ–‡æ³•ã€æ„å‘³ã€å¸¸è­˜ã‚’å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’</li>
<li><strong>å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§é«˜æ€§èƒ½</strong>ï¼šFew-shotå­¦ç¿’ãŒå¯èƒ½</li>
<li><strong>æ±åŒ–æ€§èƒ½ã®å‘ä¸Š</strong>ï¼šæœªçŸ¥ã®ã‚¿ã‚¹ã‚¯ã«ã‚‚å¯¾å¿œã—ã‚„ã™ã„</li>
<li><strong>é–‹ç™ºã‚³ã‚¹ãƒˆã®å‰Šæ¸›</strong>ï¼šã‚¹ã‚¯ãƒ©ãƒƒãƒå­¦ç¿’ã‚ˆã‚Šå¤§å¹…ã«åŠ¹ç‡çš„</li>
<li><strong>çŸ¥è­˜ã®å…±æœ‰</strong>ï¼šä¸€åº¦ã®äº‹å‰å­¦ç¿’ã§è¤‡æ•°ã‚¿ã‚¹ã‚¯ã«å¿œç”¨</li>
</ul>

<hr>

<h2>3.2 äº‹å‰å­¦ç¿’æˆ¦ç•¥</h2>

<h3>Masked Language Modelingï¼ˆMLMï¼‰</h3>

<p><strong>MLM</strong>ã¯BERTã§æ¡ç”¨ã•ã‚ŒãŸäº‹å‰å­¦ç¿’æ‰‹æ³•ã§ã€å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®ä¸€éƒ¨ï¼ˆé€šå¸¸15%ï¼‰ã‚’ãƒã‚¹ã‚¯ã—ã€ãã‚Œã‚‰ã‚’äºˆæ¸¬ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚</p>

<p>ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥ï¼š</p>
<ul>
<li><strong>80%</strong>ï¼š<code>[MASK]</code>ãƒˆãƒ¼ã‚¯ãƒ³ã«ç½®æ›</li>
<li><strong>10%</strong>ï¼šãƒ©ãƒ³ãƒ€ãƒ ãªãƒˆãƒ¼ã‚¯ãƒ³ã«ç½®æ›</li>
<li><strong>10%</strong>ï¼šå…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¾ã¾</li>
</ul>

<div class="mermaid">
graph TB
    subgraph Input["å…¥åŠ›æ–‡"]
        I1[The] --> I2[cat] --> I3[sat] --> I4[on] --> I5[the] --> I6[mat]
    end

    subgraph Masked["ãƒã‚¹ã‚¯å‡¦ç† (15%)"]
        M1[The] --> M2["[MASK]"] --> M3[sat] --> M4[on] --> M5[the] --> M6["[MASK]"]
    end

    subgraph BERT["BERT Encoder"]
        B1[Transformer] --> B2[Self-Attention] --> B3[Feed Forward]
    end

    subgraph Prediction["äºˆæ¸¬"]
        P1[The] --> P2[cat] --> P3[sat] --> P4[on] --> P5[the] --> P6[mat]
    end

    Input --> Masked
    Masked --> BERT
    BERT --> Prediction

    style M2 fill:#ffebee
    style M6 fill:#ffebee
    style P2 fill:#e8f5e9
    style P6 fill:#e8f5e9
</div>

<p>MLMã®æå¤±é–¢æ•°ï¼š</p>
<p>$$
\mathcal{L}_{\text{MLM}} = -\sum_{i \in \text{masked}} \log P(x_i | \mathbf{x}_{\setminus i})
$$</p>

<p>ã“ã“ã§ $\mathbf{x}_{\setminus i}$ ã¯ãƒˆãƒ¼ã‚¯ãƒ³ $i$ ã‚’é™¤ãæ–‡è„ˆã‚’è¡¨ã—ã¾ã™ã€‚</p>

<h3>Next Sentence Predictionï¼ˆNSPï¼‰</h3>

<p><strong>NSP</strong>ã¯BERTã®è£œåŠ©ã‚¿ã‚¹ã‚¯ã§ã€2ã¤ã®æ–‡ãŒé€£ç¶šã—ã¦ã„ã‚‹ã‹ã‚’åˆ¤å®šã—ã¾ã™ï¼ˆç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ã‚ã¾ã‚Šä½¿ã‚ã‚Œã¾ã›ã‚“ï¼‰ã€‚</p>

<table>
<thead>
<tr>
<th>Sentence A</th>
<th>Sentence B</th>
<th>ãƒ©ãƒ™ãƒ«</th>
</tr>
</thead>
<tbody>
<tr>
<td>The cat sat on the mat.</td>
<td>It was very comfortable.</td>
<td>IsNext (50%)</td>
</tr>
<tr>
<td>The cat sat on the mat.</td>
<td>I love pizza.</td>
<td>NotNext (50%)</td>
</tr>
</tbody>
</table>

<h3>Causal Language Modelingï¼ˆCLMï¼‰</h3>

<p><strong>CLM</strong>ã¯GPTã§æ¡ç”¨ã•ã‚ŒãŸæ‰‹æ³•ã§ã€å‰ã®å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã—ã¾ã™ï¼ˆè‡ªå·±å›å¸°çš„ï¼‰ã€‚</p>

<p>CLMã®æå¤±é–¢æ•°ï¼š</p>
<p>$$
\mathcal{L}_{\text{CLM}} = -\sum_{i=1}^{n} \log P(x_i | x_{1}, \ldots, x_{i-1})
$$</p>

<div class="mermaid">
graph LR
    A[The] --> B[cat]
    B --> C[sat]
    C --> D[on]
    D --> E[the]
    E --> F[mat]

    A -.äºˆæ¸¬.-> B
    B -.äºˆæ¸¬.-> C
    C -.äºˆæ¸¬.-> D
    D -.äºˆæ¸¬.-> E
    E -.äºˆæ¸¬.-> F

    style A fill:#e3f2fd
    style B fill:#e8f5e9
    style C fill:#e8f5e9
    style D fill:#e8f5e9
    style E fill:#e8f5e9
    style F fill:#e8f5e9
</div>

<h3>MLM vs CLM ã®æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>ç‰¹å¾´</th>
<th>MLMï¼ˆBERTå‹ï¼‰</th>
<th>CLMï¼ˆGPTå‹ï¼‰</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ–‡è„ˆ</strong></td>
<td>åŒæ–¹å‘ï¼ˆå‰å¾Œä¸¡æ–¹ï¼‰</td>
<td>å˜æ–¹å‘ï¼ˆå·¦ã‹ã‚‰å³ï¼‰</td>
</tr>
<tr>
<td><strong>å¾—æ„ã‚¿ã‚¹ã‚¯</strong></td>
<td>åˆ†é¡ã€æŠ½å‡ºã€ç†è§£</td>
<td>ç”Ÿæˆã€å¯¾è©±ã€ç¶šã</td>
</tr>
<tr>
<td><strong>Attention</strong></td>
<td>å…¨ãƒˆãƒ¼ã‚¯ãƒ³å‚ç…§å¯èƒ½</td>
<td>æœªæ¥ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚¹ã‚¯</td>
</tr>
<tr>
<td><strong>è¨“ç·´åŠ¹ç‡</strong></td>
<td>å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã§å­¦ç¿’</td>
<td>1ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤äºˆæ¸¬</td>
</tr>
<tr>
<td><strong>ä»£è¡¨ãƒ¢ãƒ‡ãƒ«</strong></td>
<td>BERT, RoBERTa</td>
<td>GPT-2, GPT-3, GPT-4</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.3 Hugging Face Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒª</h2>

<h3>ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æ¦‚è¦</h3>

<p><strong>Hugging Face Transformers</strong>ã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿Transformerãƒ¢ãƒ‡ãƒ«ã‚’ç°¡å˜ã«åˆ©ç”¨ã§ãã‚‹Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚</p>

<ul>
<li><strong>100,000+ã®ãƒ¢ãƒ‡ãƒ«</strong>ï¼šBERTã€GPTã€T5ã€LLaMAãªã©</li>
<li><strong>çµ±ä¸€API</strong>ï¼šAutoModelã€AutoTokenizerã§ä¸€è²«ã—ãŸä½¿ã„æ–¹</li>
<li><strong>Pipeline API</strong>ï¼š1è¡Œã§ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ</li>
<li><strong>ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£</strong>ï¼šModel Hubã€Datasetsã€Trainerã§é–‹ç™ºã‚’åŠ é€Ÿ</li>
</ul>

<h3>å®Ÿè£…ä¾‹1: Hugging FaceåŸºæœ¬æ“ä½œ</h3>

<pre><code class="language-python">import torch
from transformers import AutoTokenizer, AutoModel

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\n")

print("=== Hugging Face TransformersåŸºæœ¬æ“ä½œ ===\n")

# äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
model_name = "bert-base-uncased"
print(f"ãƒ¢ãƒ‡ãƒ«: {model_name}")

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name).to(device)

print(f"èªå½™ã‚µã‚¤ã‚º: {tokenizer.vocab_size:,}")
print(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}\n")

# ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
text = "The quick brown fox jumps over the lazy dog."
print(f"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ: {text}")

# ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆè©³ç´°è¡¨ç¤ºï¼‰
encoded = tokenizer(text, return_tensors='pt', padding=True, truncation=True)
tokens = tokenizer.tokenize(text)

print(f"\nãƒˆãƒ¼ã‚¯ãƒ³: {tokens}")
print(f"ãƒˆãƒ¼ã‚¯ãƒ³ID: {encoded['input_ids'][0].tolist()}")
print(f"Attention Mask: {encoded['attention_mask'][0].tolist()}\n")

# ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›
encoded = {k: v.to(device) for k, v in encoded.items()}
with torch.no_grad():
    outputs = model(**encoded)

# å‡ºåŠ›ã®ç¢ºèª
last_hidden_state = outputs.last_hidden_state  # [batch, seq_len, hidden_size]
pooler_output = outputs.pooler_output          # [batch, hidden_size]

print(f"Last Hidden Stateå½¢çŠ¶: {last_hidden_state.shape}")
print(f"Pooler Outputå½¢çŠ¶: {pooler_output.shape}")
print(f"Hidden Size: {model.config.hidden_size}")
print(f"Attention Heads: {model.config.num_attention_heads}")
print(f"Hidden Layers: {model.config.num_hidden_layers}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: cuda

=== Hugging Face TransformersåŸºæœ¬æ“ä½œ ===

ãƒ¢ãƒ‡ãƒ«: bert-base-uncased
èªå½™ã‚µã‚¤ã‚º: 30,522
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 109,482,240

å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ: The quick brown fox jumps over the lazy dog.

ãƒˆãƒ¼ã‚¯ãƒ³: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']
ãƒˆãƒ¼ã‚¯ãƒ³ID: [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 1012, 102]
Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]

Last Hidden Stateå½¢çŠ¶: torch.Size([1, 12, 768])
Pooler Outputå½¢çŠ¶: torch.Size([1, 768])
Hidden Size: 768
Attention Heads: 12
Hidden Layers: 12
</code></pre>

<h3>å®Ÿè£…ä¾‹2: Pipeline APIã§ç°¡å˜æ¨è«–</h3>

<pre><code class="language-python">from transformers import pipeline

print("\n=== Pipeline API ãƒ‡ãƒ¢ ===\n")

# æ„Ÿæƒ…åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
print("--- æ„Ÿæƒ…åˆ†æ ---")
sentiment_pipeline = pipeline("sentiment-analysis", device=0 if torch.cuda.is_available() else -1)

texts = [
    "I love this product! It's amazing!",
    "This is the worst experience ever.",
    "It's okay, nothing special."
]

for text in texts:
    result = sentiment_pipeline(text)[0]
    print(f"Text: {text}")
    print(f"  â†’ {result['label']}: {result['score']:.4f}\n")

# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
print("--- ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ ---")
generator = pipeline("text-generation", model="gpt2", device=0 if torch.cuda.is_available() else -1)

prompt = "Artificial intelligence will"
generated = generator(prompt, max_length=30, num_return_sequences=2)

print(f"Prompt: {prompt}")
for i, gen in enumerate(generated, 1):
    print(f"  Generated {i}: {gen['generated_text']}")

# å›ºæœ‰è¡¨ç¾èªè­˜
print("\n--- å›ºæœ‰è¡¨ç¾èªè­˜ ---")
ner_pipeline = pipeline("ner", aggregation_strategy="simple", device=0 if torch.cuda.is_available() else -1)

text_ner = "Apple Inc. was founded by Steve Jobs in Cupertino, California."
entities = ner_pipeline(text_ner)

print(f"Text: {text_ner}")
for entity in entities:
    print(f"  â†’ {entity['word']}: {entity['entity_group']} ({entity['score']:.4f})")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>
=== Pipeline API ãƒ‡ãƒ¢ ===

--- æ„Ÿæƒ…åˆ†æ ---
Text: I love this product! It's amazing!
  â†’ POSITIVE: 0.9998

Text: This is the worst experience ever.
  â†’ NEGATIVE: 0.9995

Text: It's okay, nothing special.
  â†’ NEUTRAL: 0.7234

--- ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ ---
Prompt: Artificial intelligence will
  Generated 1: Artificial intelligence will revolutionize the way we work and live in the coming decades.
  Generated 2: Artificial intelligence will transform industries from healthcare to transportation.

--- å›ºæœ‰è¡¨ç¾èªè­˜ ---
Text: Apple Inc. was founded by Steve Jobs in Cupertino, California.
  â†’ Apple Inc.: ORG (0.9987)
  â†’ Steve Jobs: PER (0.9995)
  â†’ Cupertino: LOC (0.9982)
  â†’ California: LOC (0.9991)
</code></pre>

<h3>å®Ÿè£…ä¾‹3: MLMäº‹å‰å­¦ç¿’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</h3>

<pre><code class="language-python">from transformers import BertForMaskedLM
import torch.nn.functional as F

print("\n=== Masked Language Modeling ãƒ‡ãƒ¢ ===\n")

# MLMç”¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
mlm_model = BertForMaskedLM.from_pretrained("bert-base-uncased").to(device)
mlm_model.eval()

# ãƒã‚¹ã‚¯ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
text_with_mask = "The capital of France is [MASK]."
print(f"å…¥åŠ›: {text_with_mask}\n")

# ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
inputs = tokenizer(text_with_mask, return_tensors='pt').to(device)
mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]

# äºˆæ¸¬
with torch.no_grad():
    outputs = mlm_model(**inputs)
    predictions = outputs.logits

# [MASK]ä½ç½®ã®äºˆæ¸¬
mask_token_logits = predictions[0, mask_token_index, :]
top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()

print("Top 5äºˆæ¸¬:")
for i, token_id in enumerate(top_5_tokens, 1):
    token = tokenizer.decode([token_id])
    prob = F.softmax(mask_token_logits, dim=1)[0, token_id].item()
    print(f"  {i}. {token}: {prob:.4f}")

# è¤‡æ•°ãƒã‚¹ã‚¯ã®ä¾‹
print("\n--- è¤‡æ•°ãƒã‚¹ã‚¯ ---")
text_multi_mask = "I love [MASK] learning and [MASK] intelligence."
print(f"å…¥åŠ›: {text_multi_mask}\n")

inputs_multi = tokenizer(text_multi_mask, return_tensors='pt').to(device)
mask_indices = torch.where(inputs_multi['input_ids'] == tokenizer.mask_token_id)[1]

with torch.no_grad():
    outputs_multi = mlm_model(**inputs_multi)
    predictions_multi = outputs_multi.logits

for idx, mask_pos in enumerate(mask_indices, 1):
    mask_logits = predictions_multi[0, mask_pos, :]
    top_token_id = torch.argmax(mask_logits).item()
    top_token = tokenizer.decode([top_token_id])
    prob = F.softmax(mask_logits, dim=0)[top_token_id].item()
    print(f"[MASK] {idx}: {top_token} ({prob:.4f})")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>
=== Masked Language Modeling ãƒ‡ãƒ¢ ===

å…¥åŠ›: The capital of France is [MASK].

Top 5äºˆæ¸¬:
  1. paris: 0.8234
  2. lyon: 0.0456
  3. france: 0.0234
  4. marseille: 0.0189
  5. unknown: 0.0067

--- è¤‡æ•°ãƒã‚¹ã‚¯ ---
å…¥åŠ›: I love [MASK] learning and [MASK] intelligence.

[MASK] 1: machine (0.7845)
[MASK] 2: artificial (0.8923)
</code></pre>

<hr>

<h2>3.4 ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•</h2>

<h3>å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</h3>

<p><strong>å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</strong>ã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å…¨ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚¿ã‚¹ã‚¯å›ºæœ‰ãƒ‡ãƒ¼ã‚¿ã§æ›´æ–°ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<div class="mermaid">
graph TB
    subgraph Pretrained["äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«"]
        P1[Embedding Layer] --> P2[Transformer Layer 1]
        P2 --> P3[Transformer Layer 2]
        P3 --> P4[...]
        P4 --> P5[Transformer Layer 12]
    end

    subgraph TaskHead["ã‚¿ã‚¹ã‚¯å›ºæœ‰ãƒ˜ãƒƒãƒ‰"]
        T1[åˆ†é¡ãƒ˜ãƒƒãƒ‰<br/>Dropout + Linear]
    end

    subgraph FineTuning["ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"]
        F1[å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°]
    end

    P5 --> T1
    P1 -.æ›´æ–°.-> F1
    P2 -.æ›´æ–°.-> F1
    P3 -.æ›´æ–°.-> F1
    P5 -.æ›´æ–°.-> F1
    T1 -.æ›´æ–°.-> F1

    style F1 fill:#e8f5e9
    style T1 fill:#fff3e0
</div>

<h3>å®Ÿè£…ä¾‹4: æ„Ÿæƒ…åˆ†æã¸ã®å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</h3>

<pre><code class="language-python">from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup
from torch.utils.data import DataLoader, Dataset
import numpy as np

print("\n=== å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ===\n")

# ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
class SentimentDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã«ã¯IMDbãªã©ã®å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ï¼‰
train_texts = [
    "This movie is fantastic! I loved every minute.",
    "Terrible film, waste of time and money.",
    "An absolute masterpiece of cinema.",
    "Boring and predictable plot.",
    "One of the best movies I've ever seen!",
    "Disappointing and poorly acted."
] * 100  # ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

train_labels = [1, 0, 1, 0, 1, 0] * 100  # 1: Positive, 0: Negative

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)

# ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
num_labels = 2  # Binary classification
model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=num_labels
).to(device)

print(f"ã‚¿ã‚¹ã‚¯: æ„Ÿæƒ…åˆ†æï¼ˆBinary Classificationï¼‰")
print(f"ãƒ©ãƒ™ãƒ«æ•°: {num_labels}")
print(f"è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(train_dataset)}")
print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}")
print(f"è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\n")

# ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)
num_epochs = 3
num_training_steps = num_epochs * len(train_loader)
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0.1 * num_training_steps,
    num_training_steps=num_training_steps
)

print("=== è¨“ç·´è¨­å®š ===")
print(f"ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶: AdamW")
print(f"å­¦ç¿’ç‡: 2e-5")
print(f"Weight Decay: 0.01")
print(f"ã‚¨ãƒãƒƒã‚¯æ•°: {num_epochs}")
print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: 8")
print(f"Warmup Steps: {int(0.1 * num_training_steps)}\n")

# è¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆç°¡ç•¥ç‰ˆï¼‰
print("=== è¨“ç·´é–‹å§‹ ===")
model.train()

for epoch in range(num_epochs):
    total_loss = 0
    correct_predictions = 0
    total_predictions = 0

    for batch_idx, batch in enumerate(train_loader):
        # GPUã«è»¢é€
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        # é †ä¼æ’­
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )

        loss = outputs.loss
        logits = outputs.logits

        # é€†ä¼æ’­
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        total_loss += loss.item()
        predictions = torch.argmax(logits, dim=1)
        correct_predictions += (predictions == labels).sum().item()
        total_predictions += labels.size(0)

        # 10ãƒãƒƒãƒã”ã¨ã«é€²æ—è¡¨ç¤º
        if (batch_idx + 1) % 10 == 0:
            avg_loss = total_loss / (batch_idx + 1)
            accuracy = correct_predictions / total_predictions
            print(f"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, "
                  f"Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}")

    epoch_loss = total_loss / len(train_loader)
    epoch_accuracy = correct_predictions / total_predictions
    print(f"\nEpoch {epoch+1} å®Œäº†: Loss = {epoch_loss:.4f}, Accuracy = {epoch_accuracy:.4f}\n")

print("è¨“ç·´å®Œäº†!")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>
=== å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ===

ã‚¿ã‚¹ã‚¯: æ„Ÿæƒ…åˆ†æï¼ˆBinary Classificationï¼‰
ãƒ©ãƒ™ãƒ«æ•°: 2
è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°: 600
ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 109,483,778
è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 109,483,778

=== è¨“ç·´è¨­å®š ===
ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶: AdamW
å­¦ç¿’ç‡: 2e-5
Weight Decay: 0.01
ã‚¨ãƒãƒƒã‚¯æ•°: 3
ãƒãƒƒãƒã‚µã‚¤ã‚º: 8
Warmup Steps: 22

=== è¨“ç·´é–‹å§‹ ===
Epoch 1/3, Batch 10/75, Loss: 0.6234, Accuracy: 0.6250
Epoch 1/3, Batch 20/75, Loss: 0.5123, Accuracy: 0.7375
Epoch 1/3, Batch 30/75, Loss: 0.3987, Accuracy: 0.8208
Epoch 1/3, Batch 40/75, Loss: 0.2876, Accuracy: 0.8813
Epoch 1/3, Batch 50/75, Loss: 0.2234, Accuracy: 0.9150
Epoch 1/3, Batch 60/75, Loss: 0.1823, Accuracy: 0.9354
Epoch 1/3, Batch 70/75, Loss: 0.1534, Accuracy: 0.9482

Epoch 1 å®Œäº†: Loss = 0.1423, Accuracy = 0.9517

Epoch 2/3, Batch 10/75, Loss: 0.0876, Accuracy: 0.9750
Epoch 2/3, Batch 20/75, Loss: 0.0723, Accuracy: 0.9813
...

Epoch 3 å®Œäº†: Loss = 0.0312, Accuracy = 0.9933

è¨“ç·´å®Œäº†!
</code></pre>

<h3>LoRAï¼ˆLow-Rank Adaptationï¼‰ã®åŸç†</h3>

<p><strong>LoRA</strong>ã¯ã€å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã§ã€é‡ã¿è¡Œåˆ—ã«ä½ãƒ©ãƒ³ã‚¯åˆ†è§£ã‚’é©ç”¨ã—ã¾ã™ã€‚</p>

<p>å…ƒã®é‡ã¿æ›´æ–°ï¼š</p>
<p>$$
W' = W + \Delta W
$$</p>

<p>LoRAã§ã¯ $\Delta W$ ã‚’ä½ãƒ©ãƒ³ã‚¯åˆ†è§£ï¼š</p>
<p>$$
\Delta W = BA
$$</p>
<p>ã“ã“ã§ $B \in \mathbb{R}^{d \times r}$ã€$A \in \mathbb{R}^{r \times k}$ã€$r \ll \min(d, k)$ ã§ã™ã€‚</p>

<div class="mermaid">
graph LR
    subgraph Original["å…ƒã®é‡ã¿ W"]
        O1[d Ã— k<br/>109M params]
    end

    subgraph LoRA["LoRAåˆ†è§£"]
        L1[B: d Ã— r] --> L2[A: r Ã— k]
    end

    subgraph Savings["ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›"]
        S1[r=8ã®å ´åˆ<br/>1%æœªæº€]
    end

    O1 -.å‡çµ.-> O1
    L1 --> S1
    L2 --> S1

    style O1 fill:#e0e0e0
    style L1 fill:#e8f5e9
    style L2 fill:#e8f5e9
    style S1 fill:#fff3e0
</div>

<p>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›ç‡ï¼š</p>
<p>$$
\text{å‰Šæ¸›ç‡} = \frac{r(d + k)}{d \times k} \times 100\%
$$</p>

<p>ä¾‹ï¼š$d=768$ã€$k=768$ã€$r=8$ ã®å ´åˆï¼š</p>
<p>$$
\text{å‰Šæ¸›ç‡} = \frac{8 \times (768 + 768)}{768 \times 768} \times 100\% = 2.08\%
$$</p>

<h3>å®Ÿè£…ä¾‹5: LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</h3>

<pre><code class="language-python">from peft import LoraConfig, get_peft_model, TaskType

print("\n=== LoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ===\n")

# æ–°ã—ã„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆLoRAç”¨ï¼‰
base_model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
).to(device)

# LoRAè¨­å®š
lora_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,  # Sequence Classification
    r=8,                          # LoRAãƒ©ãƒ³ã‚¯
    lora_alpha=16,                # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°
    lora_dropout=0.1,             # LoRAãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
    target_modules=["query", "value"],  # Attentionå±¤ã®Q, Vã«é©ç”¨
)

# LoRAãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
lora_model = get_peft_model(base_model, lora_config)
lora_model.print_trainable_parameters()

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¯”è¼ƒ
total_params = sum(p.numel() for p in lora_model.parameters())
trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)

print(f"\nç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
print(f"è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}")
print(f"è¨“ç·´å¯èƒ½æ¯”ç‡: {100 * trainable_params / total_params:.2f}%")
print(f"ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: ç´„{100 - 100 * trainable_params / total_params:.1f}%\n")

# LoRAã§è¨“ç·´ï¼ˆã‚³ãƒ¼ãƒ‰ã¯å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿FTã¨åŒã˜ï¼‰
print("LoRAè¨“ç·´ã®ç‰¹å¾´:")
print("  âœ“ è¨“ç·´é€Ÿåº¦: ç´„1.5ã€œ2å€é«˜é€Ÿ")
print("  âœ“ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ç´„50ã€œ70%å‰Šæ¸›")
print("  âœ“ æ€§èƒ½: å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿FTã¨åŒç­‰")
print("  âœ“ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: ä¿å­˜æ™‚ã«æ•°MBï¼ˆå…ƒãƒ¢ãƒ‡ãƒ«ã¯æ•°GBï¼‰")
print("  âœ“ ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯: è¤‡æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ã‚’åˆ‡ã‚Šæ›¿ãˆå¯èƒ½")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>
=== LoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ===

trainable params: 294,912 || all params: 109,778,690 || trainable%: 0.2687%

ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 109,778,690
è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 294,912
è¨“ç·´å¯èƒ½æ¯”ç‡: 0.27%
ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: ç´„99.7%

LoRAè¨“ç·´ã®ç‰¹å¾´:
  âœ“ è¨“ç·´é€Ÿåº¦: ç´„1.5ã€œ2å€é«˜é€Ÿ
  âœ“ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ç´„50ã€œ70%å‰Šæ¸›
  âœ“ æ€§èƒ½: å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿FTã¨åŒç­‰
  âœ“ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: ä¿å­˜æ™‚ã«æ•°MBï¼ˆå…ƒãƒ¢ãƒ‡ãƒ«ã¯æ•°GBï¼‰
  âœ“ ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯: è¤‡æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ã‚’åˆ‡ã‚Šæ›¿ãˆå¯èƒ½
</code></pre>

<h3>Adapter Layersã¨ã®æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</th>
<th>æ¨è«–é€Ÿåº¦</th>
<th>å®Ÿè£…é›£æ˜“åº¦</th>
<th>æ€§èƒ½</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿FT</strong></td>
<td>100%</td>
<td>æ¨™æº–</td>
<td>ç°¡å˜</td>
<td>æœ€é«˜</td>
</tr>
<tr>
<td><strong>Adapter Layers</strong></td>
<td>1ã€œ5%</td>
<td>ã‚„ã‚„é…ã„</td>
<td>ä¸­ç¨‹åº¦</td>
<td>é«˜ã„</td>
</tr>
<tr>
<td><strong>LoRA</strong></td>
<td>0.1ã€œ1%</td>
<td>æ¨™æº–</td>
<td>ç°¡å˜</td>
<td>é«˜ã„</td>
</tr>
<tr>
<td><strong>Prefix Tuning</strong></td>
<td>0.01ã€œ0.1%</td>
<td>æ¨™æº–</td>
<td>é›£ã—ã„</td>
<td>ä¸­ç¨‹åº¦</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.5 å®Ÿè·µï¼šæ„Ÿæƒ…åˆ†æã¸ã®å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h2>

<h3>å®Ÿè£…ä¾‹6: ãƒ‡ãƒ¼ã‚¿æº–å‚™ã¨ãƒˆãƒ¼ã‚¯ãƒ³åŒ–</h3>

<pre><code class="language-python">from datasets import load_dataset
from sklearn.model_selection import train_test_split

print("\n=== æ„Ÿæƒ…åˆ†æå®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ===\n")

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰ï¼ˆHugging Face Datasetsä½¿ç”¨ï¼‰
print("--- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ ---")

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆå®Ÿéš›ã«ã¯IMDbã€SST-2ãªã©ã‚’ä½¿ç”¨ï¼‰
sample_data = {
    'text': [
        "This movie exceeded all my expectations!",
        "Absolutely terrible, do not watch.",
        "A brilliant masterpiece of storytelling.",
        "Waste of time, boring from start to finish.",
        "Incredible performances by all actors!",
        "The worst film I've seen this year.",
        "Highly recommend, a must-see!",
        "Disappointing and uninspired."
    ] * 125,  # 1000ã‚µãƒ³ãƒ—ãƒ«ã«ã‚¹ã‚±ãƒ¼ãƒ«
    'label': [1, 0, 1, 0, 1, 0, 1, 0] * 125
}

# Train/Teståˆ†å‰²
train_texts, test_texts, train_labels, test_labels = train_test_split(
    sample_data['text'],
    sample_data['label'],
    test_size=0.2,
    random_state=42
)

print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_texts)}ã‚µãƒ³ãƒ—ãƒ«")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_texts)}ã‚µãƒ³ãƒ—ãƒ«")
print(f"ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ: {sum(train_labels)} Positive, {len(train_labels) - sum(train_labels)} Negative\n")

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, max_length=128)
test_dataset = SentimentDataset(test_texts, test_labels, tokenizer, max_length=128)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

print(f"è¨“ç·´ãƒãƒƒãƒæ•°: {len(train_loader)}")
print(f"ãƒ†ã‚¹ãƒˆãƒãƒƒãƒæ•°: {len(test_loader)}\n")

# ãƒˆãƒ¼ã‚¯ãƒ³çµ±è¨ˆ
sample_lengths = []
for text in train_texts[:100]:
    tokens = tokenizer.tokenize(text)
    sample_lengths.append(len(tokens))

print(f"å¹³å‡ãƒˆãƒ¼ã‚¯ãƒ³é•·: {np.mean(sample_lengths):.1f}")
print(f"æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·: {np.max(sample_lengths)}")
print(f"95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {np.percentile(sample_lengths, 95):.0f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>
=== æ„Ÿæƒ…åˆ†æå®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ===

--- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ ---
è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 800ã‚µãƒ³ãƒ—ãƒ«
ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: 200ã‚µãƒ³ãƒ—ãƒ«
ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ: 400 Positive, 400 Negative

è¨“ç·´ãƒãƒƒãƒæ•°: 50
ãƒ†ã‚¹ãƒˆãƒãƒƒãƒæ•°: 13

å¹³å‡ãƒˆãƒ¼ã‚¯ãƒ³é•·: 8.3
æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·: 12
95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: 11
</code></pre>

<h3>å®Ÿè£…ä¾‹7: ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨è©•ä¾¡</h3>

<pre><code class="language-python">from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix

print("\n=== ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨è©•ä¾¡ ===\n")

# ãƒ¢ãƒ‡ãƒ«ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–
model_ft = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2).to(device)
optimizer = AdamW(model_ft.parameters(), lr=2e-5, weight_decay=0.01)

# è¨“ç·´é–¢æ•°
def train_epoch(model, data_loader, optimizer):
    model.train()
    total_loss = 0
    predictions_list = []
    labels_list = []

    for batch in data_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)

        loss = outputs.loss
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += loss.item()
        predictions = torch.argmax(outputs.logits, dim=1)
        predictions_list.extend(predictions.cpu().numpy())
        labels_list.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(data_loader)
    accuracy = accuracy_score(labels_list, predictions_list)
    return avg_loss, accuracy

# è©•ä¾¡é–¢æ•°
def evaluate(model, data_loader):
    model.eval()
    predictions_list = []
    labels_list = []

    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            predictions = torch.argmax(outputs.logits, dim=1)

            predictions_list.extend(predictions.cpu().numpy())
            labels_list.extend(labels.cpu().numpy())

    accuracy = accuracy_score(labels_list, predictions_list)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels_list, predictions_list, average='binary'
    )

    return accuracy, precision, recall, f1, predictions_list, labels_list

# è¨“ç·´å®Ÿè¡Œ
print("--- è¨“ç·´é–‹å§‹ ---")
num_epochs = 3

for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(model_ft, train_loader, optimizer)
    test_acc, test_prec, test_rec, test_f1, _, _ = evaluate(model_ft, test_loader)

    print(f"Epoch {epoch+1}/{num_epochs}:")
    print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
    print(f"  Test Acc: {test_acc:.4f}, Precision: {test_prec:.4f}, Recall: {test_rec:.4f}, F1: {test_f1:.4f}\n")

# æœ€çµ‚è©•ä¾¡
print("--- æœ€çµ‚è©•ä¾¡ ---")
final_acc, final_prec, final_rec, final_f1, predictions, true_labels = evaluate(model_ft, test_loader)

print(f"Accuracy: {final_acc:.4f}")
print(f"Precision: {final_prec:.4f}")
print(f"Recall: {final_rec:.4f}")
print(f"F1-Score: {final_f1:.4f}\n")

# æ··åŒè¡Œåˆ—
cm = confusion_matrix(true_labels, predictions)
print("æ··åŒè¡Œåˆ—:")
print(f"              Predicted")
print(f"              Neg    Pos")
print(f"Actual Neg  [{cm[0,0]:4d}  {cm[0,1]:4d}]")
print(f"       Pos  [{cm[1,0]:4d}  {cm[1,1]:4d}]")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>
=== ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨è©•ä¾¡ ===

--- è¨“ç·´é–‹å§‹ ---
Epoch 1/3:
  Train Loss: 0.2134, Train Acc: 0.9125
  Test Acc: 0.9400, Precision: 0.9388, Recall: 0.9423, F1: 0.9405

Epoch 2/3:
  Train Loss: 0.0823, Train Acc: 0.9763
  Test Acc: 0.9600, Precision: 0.9608, Recall: 0.9615, F1: 0.9611

Epoch 3/3:
  Train Loss: 0.0412, Train Acc: 0.9900
  Test Acc: 0.9650, Precision: 0.9655, Recall: 0.9663, F1: 0.9659

--- æœ€çµ‚è©•ä¾¡ ---
Accuracy: 0.9650
Precision: 0.9655
Recall: 0.9663
F1-Score: 0.9659

æ··åŒè¡Œåˆ—:
              Predicted
              Neg    Pos
Actual Neg  [  97    3]
       Pos  [   4   96]
</code></pre>

<h3>å®Ÿè£…ä¾‹8: æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h3>

<pre><code class="language-python">import torch.nn.functional as F

print("\n=== æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ===\n")

def predict_sentiment(text, model, tokenizer, device):
    """
    å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…ã‚’äºˆæ¸¬

    Args:
        text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
        model: è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        device: ãƒ‡ãƒã‚¤ã‚¹

    Returns:
        label: äºˆæ¸¬ãƒ©ãƒ™ãƒ« (Positive/Negative)
        confidence: ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
    """
    model.eval()

    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    encoding = tokenizer(
        text,
        max_length=128,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )

    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    # æ¨è«–
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        probabilities = F.softmax(logits, dim=1)

    prediction = torch.argmax(probabilities, dim=1).item()
    confidence = probabilities[0, prediction].item()

    label = "Positive" if prediction == 1 else "Negative"
    return label, confidence

# ãƒ†ã‚¹ãƒˆæ–‡ç« 
test_sentences = [
    "This is the best movie I have ever seen!",
    "Absolutely horrible, a complete disaster.",
    "It was okay, nothing particularly special.",
    "Mind-blowing performance, highly recommend!",
    "Boring and predictable throughout.",
    "A true cinematic achievement!",
]

print("--- æ„Ÿæƒ…äºˆæ¸¬çµæœ ---\n")
for text in test_sentences:
    label, confidence = predict_sentiment(text, model_ft, tokenizer, device)
    print(f"Text: {text}")
    print(f"  â†’ Prediction: {label} (Confidence: {confidence:.4f})\n")

# ãƒãƒƒãƒæ¨è«–
print("--- ãƒãƒƒãƒæ¨è«–ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ ---")
import time

batch_texts = test_sentences * 100  # 600ã‚µãƒ³ãƒ—ãƒ«
start_time = time.time()

for text in batch_texts:
    _ = predict_sentiment(text, model_ft, tokenizer, device)

elapsed_time = time.time() - start_time
throughput = len(batch_texts) / elapsed_time

print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(batch_texts)}")
print(f"å‡¦ç†æ™‚é–“: {elapsed_time:.2f}ç§’")
print(f"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.1f}ã‚µãƒ³ãƒ—ãƒ«/ç§’")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>
=== æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ===

--- æ„Ÿæƒ…äºˆæ¸¬çµæœ ---

Text: This is the best movie I have ever seen!
  â†’ Prediction: Positive (Confidence: 0.9987)

Text: Absolutely horrible, a complete disaster.
  â†’ Prediction: Negative (Confidence: 0.9993)

Text: It was okay, nothing particularly special.
  â†’ Prediction: Negative (Confidence: 0.6234)

Text: Mind-blowing performance, highly recommend!
  â†’ Prediction: Positive (Confidence: 0.9978)

Text: Boring and predictable throughout.
  â†’ Prediction: Negative (Confidence: 0.9856)

Text: A true cinematic achievement!
  â†’ Prediction: Positive (Confidence: 0.9945)

--- ãƒãƒƒãƒæ¨è«–ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ ---
ã‚µãƒ³ãƒ—ãƒ«æ•°: 600
å‡¦ç†æ™‚é–“: 12.34ç§’
ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: 48.6ã‚µãƒ³ãƒ—ãƒ«/ç§’
</code></pre>

<hr>

<h2>ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h2>

<h3>å­¦ç¿’ç‡ã®é¸æŠ</h3>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>æ¨å¥¨å­¦ç¿’ç‡</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿FT</strong></td>
<td>1e-5 ã€œ 5e-5</td>
<td>äº‹å‰å­¦ç¿’æ¸ˆã¿ã®é‡ã¿ã‚’å¾®èª¿æ•´</td>
</tr>
<tr>
<td><strong>ã‚¿ã‚¹ã‚¯ãƒ˜ãƒƒãƒ‰ã®ã¿</strong></td>
<td>1e-4 ã€œ 1e-3</td>
<td>ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–å±¤ã¯é«˜ã„å­¦ç¿’ç‡</td>
</tr>
<tr>
<td><strong>LoRA</strong></td>
<td>1e-4 ã€œ 3e-4</td>
<td>é©å¿œå±¤ã®ã¿è¨“ç·´ã€ã‚„ã‚„é«˜ã‚</td>
</tr>
<tr>
<td><strong>Layer-wise LR</strong></td>
<td>ä¸‹å±¤: 1e-5ã€ä¸Šå±¤: 5e-5</td>
<td>å±¤ã”ã¨ã«ç•°ãªã‚‹å­¦ç¿’ç‡</td>
</tr>
</tbody>
</table>

<h3>ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µæˆ¦ç•¥</h3>

<ul>
<li><strong>Back Translation</strong>ï¼šä»–è¨€èªã«ç¿»è¨³ã—ã¦å†ç¿»è¨³</li>
<li><strong>Synonym Replacement</strong>ï¼šåŒç¾©èªã§å˜èªã‚’ç½®æ›</li>
<li><strong>Random Deletion/Insertion</strong>ï¼šå˜èªã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å‰Šé™¤ãƒ»æŒ¿å…¥</li>
<li><strong>Mixup</strong>ï¼šã‚µãƒ³ãƒ—ãƒ«é–“ã®ç·šå½¢è£œé–“</li>
<li><strong>Paraphrasing</strong>ï¼šè¨€ã„æ›ãˆãƒ¢ãƒ‡ãƒ«ã§æ–‡ã‚’å†æ§‹æˆ</li>
</ul>

<h3>éå­¦ç¿’ã®é˜²æ­¢</h3>

<div class="mermaid">
graph LR
    A[è¨“ç·´ãƒ‡ãƒ¼ã‚¿å°‘ãªã„] --> B[éå­¦ç¿’ãƒªã‚¹ã‚¯é«˜]
    B --> C1[Early Stopping]
    B --> C2[Dropoutå¢—åŠ ]
    B --> C3[Weight Decay]
    B --> C4[ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ]
    B --> C5[LoRA/Adapter]

    C1 --> D[æ±åŒ–æ€§èƒ½å‘ä¸Š]
    C2 --> D
    C3 --> D
    C4 --> D
    C5 --> D

    style B fill:#ffebee
    style D fill:#e8f5e9
</div>

<hr>

<h2>ã¾ã¨ã‚</h2>

<p>ã“ã®ç« ã§ã¯ã€Transformerã®äº‹å‰å­¦ç¿’ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å­¦ã³ã¾ã—ãŸï¼š</p>

<h3>é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ</h3>

<details>
<summary><strong>1. Transfer Learningã®å¨åŠ›</strong></summary>
<ul>
<li>å¤§è¦æ¨¡äº‹å‰å­¦ç¿’ã§æ±ç”¨è¨€èªçŸ¥è­˜ã‚’ç²å¾—</li>
<li>å°‘é‡ã®ã‚¿ã‚¹ã‚¯å›ºæœ‰ãƒ‡ãƒ¼ã‚¿ã§é«˜æ€§èƒ½ã‚’å®Ÿç¾</li>
<li>é–‹ç™ºã‚³ã‚¹ãƒˆã¨ãƒªã‚½ãƒ¼ã‚¹ã‚’å¤§å¹…å‰Šæ¸›</li>
<li>è¤‡æ•°ã‚¿ã‚¹ã‚¯ã¸ã®å¿œç”¨ãŒå®¹æ˜“</li>
</ul>
</details>

<details>
<summary><strong>2. äº‹å‰å­¦ç¿’æ‰‹æ³•</strong></summary>
<ul>
<li><strong>MLM</strong>ï¼šåŒæ–¹å‘æ–‡è„ˆã€åˆ†é¡ãƒ»æŠ½å‡ºã‚¿ã‚¹ã‚¯ã«å¼·ã„</li>
<li><strong>CLM</strong>ï¼šå˜æ–¹å‘ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«æœ€é©</li>
<li><strong>NSP</strong>ï¼šæ–‡é–“é–¢ä¿‚ã®ç†è§£ï¼ˆç¾åœ¨ã¯ä½¿ç”¨æ¸›ï¼‰</li>
<li>ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦æ‰‹æ³•ã‚’é¸æŠ</li>
</ul>
</details>

<details>
<summary><strong>3. Hugging Face Transformers</strong></summary>
<ul>
<li>AutoModel/AutoTokenizerã§çµ±ä¸€API</li>
<li>Pipeline APIã§1è¡Œæ¨è«–</li>
<li>100,000+ã®äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«</li>
<li>Trainer APIã§è¨“ç·´ã‚’ç°¡ç´ åŒ–</li>
</ul>
</details>

<details>
<summary><strong>4. åŠ¹ç‡çš„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</strong></summary>
<ul>
<li><strong>å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿FT</strong>ï¼šæœ€é«˜æ€§èƒ½ã€è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜</li>
<li><strong>LoRA</strong>ï¼š99%ä»¥ä¸Šã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›ã€æ€§èƒ½ç¶­æŒ</li>
<li><strong>Adapter</strong>ï¼šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¿½åŠ ã€æ¨è«–ã‚„ã‚„é…ã„</li>
<li>ã‚¿ã‚¹ã‚¯ã¨ãƒªã‚½ãƒ¼ã‚¹ã«å¿œã˜ã¦é¸æŠ</li>
</ul>
</details>

<h3>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>

<p>æ¬¡ç« ã§ã¯ã€Transformerã®å®Ÿéš›çš„ãªå¿œç”¨ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ï¼š</p>

<ul>
<li>è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰</li>
<li>ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</li>
<li>ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã¨ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆåˆ†é¡</li>
<li>å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®æ´»ç”¨</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<details>
<summary><strong>å•é¡Œ1: MLMã¨CLMã®é¸æŠ</strong></summary>
<p><strong>è³ªå•</strong>ï¼šä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦ã€MLMäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆBERTï¼‰ã¨CLMäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆGPTï¼‰ã®ã©ã¡ã‚‰ãŒé©åˆ‡ã‹ã€ç†ç”±ã¨å…±ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>
<ol>
<li>æ–‡ç« åˆ†é¡ï¼ˆæ„Ÿæƒ…åˆ†æï¼‰</li>
<li>å¯¾è©±ç”Ÿæˆï¼ˆãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆï¼‰</li>
<li>å›ºæœ‰è¡¨ç¾èªè­˜ï¼ˆNERï¼‰</li>
<li>è¦ç´„ç”Ÿæˆ</li>
</ol>

<p><strong>è§£ç­”ä¾‹</strong>ï¼š</p>

<p><strong>1. æ–‡ç« åˆ†é¡ï¼ˆæ„Ÿæƒ…åˆ†æï¼‰</strong></p>
<ul>
<li><strong>é©åˆ‡ï¼šBERTï¼ˆMLMï¼‰</strong></li>
<li>ç†ç”±ï¼šå…¨å˜èªã®æ–‡è„ˆã‚’åŒæ–¹å‘ã«è€ƒæ…®ã§ãã€æ–‡å…¨ä½“ã®æ„å‘³ç†è§£ãŒé‡è¦</li>
<li>[CLS]ãƒˆãƒ¼ã‚¯ãƒ³ã®è¡¨ç¾ã§æ–‡å…¨ä½“ã‚’è¡¨ç¾å¯èƒ½</li>
</ul>

<p><strong>2. å¯¾è©±ç”Ÿæˆï¼ˆãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆï¼‰</strong></p>
<ul>
<li><strong>é©åˆ‡ï¼šGPTï¼ˆCLMï¼‰</strong></li>
<li>ç†ç”±ï¼šè‡ªå·±å›å¸°çš„ã«æ¬¡ã®å˜èªã‚’äºˆæ¸¬ã™ã‚‹ç”Ÿæˆã‚¿ã‚¹ã‚¯</li>
<li>å·¦ã‹ã‚‰å³ã¸ã®é †æ¬¡ç”ŸæˆãŒè‡ªç„¶</li>
</ul>

<p><strong>3. å›ºæœ‰è¡¨ç¾èªè­˜ï¼ˆNERï¼‰</strong></p>
<ul>
<li><strong>é©åˆ‡ï¼šBERTï¼ˆMLMï¼‰</strong></li>
<li>ç†ç”±ï¼šå„ãƒˆãƒ¼ã‚¯ãƒ³ã®åˆ†é¡ã«å‰å¾Œã®æ–‡è„ˆãŒå¿…è¦</li>
<li>åŒæ–¹å‘Attentionã§æ–‡è„ˆæƒ…å ±ã‚’æœ€å¤§æ´»ç”¨</li>
</ul>

<p><strong>4. è¦ç´„ç”Ÿæˆ</strong></p>
<ul>
<li><strong>é©åˆ‡ï¼šGPTï¼ˆCLMï¼‰ã¾ãŸã¯T5ï¼ˆSeq2Seqï¼‰</strong></li>
<li>ç†ç”±ï¼šè¦ç´„æ–‡ã‚’ç”Ÿæˆã™ã‚‹ã‚¿ã‚¹ã‚¯</li>
<li>è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ãŒç”Ÿæˆã‚¿ã‚¹ã‚¯ã«æœ€é©</li>
<li>T5ã®ã‚ˆã†ãªEncoder-Decoderãƒ¢ãƒ‡ãƒ«ã‚‚å„ªç§€</li>
</ul>
</details>

<details>
<summary><strong>å•é¡Œ2: LoRAã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›è¨ˆç®—</strong></summary>
<p><strong>è³ªå•</strong>ï¼šBERT-baseãƒ¢ãƒ‡ãƒ«ï¼ˆhidden_size=768, 12å±¤ï¼‰ã®Attentionå±¤ï¼ˆQueryã€Keyã€Valueã€Outputï¼‰ã«LoRAã‚’é©ç”¨ã—ã¾ã™ã€‚ãƒ©ãƒ³ã‚¯r=16ã®å ´åˆã€è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚</p>

<p><strong>è§£ç­”ä¾‹</strong>ï¼š</p>

<p><strong>å…ƒã®é‡ã¿</strong>ï¼š</p>
<ul>
<li>å„Attentionå±¤ã«4ã¤ã®é‡ã¿è¡Œåˆ—ï¼ˆQ, K, V, Outputï¼‰</li>
<li>å„é‡ã¿: 768 Ã— 768 = 589,824ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
<li>1å±¤ã‚ãŸã‚Š: 4 Ã— 589,824 = 2,359,296ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
<li>12å±¤åˆè¨ˆ: 12 Ã— 2,359,296 = 28,311,552ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
</ul>

<p><strong>LoRAè¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆr=16ï¼‰</strong>ï¼š</p>
<ul>
<li>å„é‡ã¿ã«å¯¾ã—ã¦B (768Ã—16) + A (16Ã—768)</li>
<li>1ã¤ã®LoRA: 768Ã—16 + 16Ã—768 = 24,576ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
<li>4ã¤ã®é‡ã¿ï¼ˆQ, K, V, Outputï¼‰: 4 Ã— 24,576 = 98,304ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿/å±¤</li>
<li>12å±¤åˆè¨ˆ: 12 Ã— 98,304 = 1,179,648ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
</ul>

<p><strong>å‰Šæ¸›ç‡</strong>ï¼š</p>
<p>$$
\frac{1,179,648}{28,311,552} \times 100\% = 4.17\%
$$</p>

<p>ã¤ã¾ã‚Šã€Attentionå±¤ã ã‘ã§<strong>ç´„96%ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›</strong>ã‚’å®Ÿç¾ã§ãã¾ã™ã€‚</p>
</details>

<details>
<summary><strong>å•é¡Œ3: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã®é¸æŠ</strong></summary>
<p><strong>è³ªå•</strong>ï¼šä»¥ä¸‹ã®3ã¤ã®ã‚·ãƒŠãƒªã‚ªã§ã€æœ€é©ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã‚’é¸æŠã—ã€ç†ç”±ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<p><strong>ã‚·ãƒŠãƒªã‚ªA</strong>ï¼šè¨“ç·´ãƒ‡ãƒ¼ã‚¿100,000ã‚µãƒ³ãƒ—ãƒ«ã€GPU 1å°ï¼ˆ16GBï¼‰ã€3æ—¥é–“ã§è¨“ç·´å®Œäº†ãŒå¿…è¦</p>

<p><strong>ã‚·ãƒŠãƒªã‚ªB</strong>ï¼šè¨“ç·´ãƒ‡ãƒ¼ã‚¿500ã‚µãƒ³ãƒ—ãƒ«ã€GPU 1å°ï¼ˆ8GBï¼‰ã€éå­¦ç¿’ãŒæ‡¸å¿µã•ã‚Œã‚‹</p>

<p><strong>ã‚·ãƒŠãƒªã‚ªC</strong>ï¼š20å€‹ã®ã‚¿ã‚¹ã‚¯ã«åŒæ™‚å¯¾å¿œã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºåˆ¶ç´„ã‚ã‚Š</p>

<p><strong>è§£ç­”ä¾‹</strong>ï¼š</p>

<p><strong>ã‚·ãƒŠãƒªã‚ªA</strong>ï¼š</p>
<ul>
<li><strong>æ¨å¥¨ï¼šå…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</strong></li>
<li>ç†ç”±ï¼šãƒ‡ãƒ¼ã‚¿é‡ãŒååˆ†ã€æ™‚é–“çš„ä½™è£•ã‚ã‚Šã€æœ€é«˜æ€§èƒ½ã‚’è¿½æ±‚å¯èƒ½</li>
<li>16GB GPUã§å®Ÿè¡Œå¯èƒ½ã€3æ—¥ã§åæŸ</li>
</ul>

<p><strong>ã‚·ãƒŠãƒªã‚ªB</strong>ï¼š</p>
<ul>
<li><strong>æ¨å¥¨ï¼šLoRA + ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ</strong></li>
<li>ç†ç”±ï¼šå°‘é‡ãƒ‡ãƒ¼ã‚¿ã§ã¯éå­¦ç¿’ãƒªã‚¹ã‚¯ãŒé«˜ã„ã€LoRAã§è¨“ç·´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šæ¸›</li>
<li>8GB GPUã§ã‚‚å®Ÿè¡Œå¯èƒ½</li>
<li>ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã§å®Ÿè³ªçš„ãªãƒ‡ãƒ¼ã‚¿é‡ã‚’å¢—åŠ </li>
</ul>

<p><strong>ã‚·ãƒŠãƒªã‚ªC</strong>ï¼š</p>
<ul>
<li><strong>æ¨å¥¨ï¼šLoRAï¼ˆãƒãƒ«ãƒã‚¢ãƒ€ãƒ—ã‚¿ï¼‰</strong></li>
<li>ç†ç”±ï¼šãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«1ã¤ + 20å€‹ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ã§å¯¾å¿œ</li>
<li>å„ã‚¢ãƒ€ãƒ—ã‚¿ã¯æ•°MBã€ç·å®¹é‡ã‚’å¤§å¹…å‰Šæ¸›</li>
<li>ã‚¿ã‚¹ã‚¯åˆ‡ã‚Šæ›¿ãˆãŒå®¹æ˜“ã€æ¨è«–æ™‚ã«ã‚¢ãƒ€ãƒ—ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰</li>
</ul>
</details>

<details>
<summary><strong>å•é¡Œ4: äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å½±éŸ¿</strong></summary>
<p><strong>è³ªå•</strong>ï¼šäº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«åŒ»ç™‚æ–‡çŒ®ãŒå«ã¾ã‚Œã¦ã„ãªã„BERTãƒ¢ãƒ‡ãƒ«ã‚’ã€åŒ»ç™‚ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ç–¾æ‚£åˆ†é¡ã‚¿ã‚¹ã‚¯ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å ´åˆã€ã©ã®ã‚ˆã†ãªèª²é¡ŒãŒäºˆæƒ³ã•ã‚Œã¾ã™ã‹ï¼Ÿ3ã¤ä»¥ä¸ŠæŒ™ã’ã€å¯¾ç­–ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚</p>

<p><strong>è§£ç­”ä¾‹</strong>ï¼š</p>

<p><strong>èª²é¡Œ1: ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰èªå½™ã®ä¸è¶³</strong></p>
<ul>
<li>å•é¡Œï¼šåŒ»å­¦ç”¨èªï¼ˆ"ç³–å°¿ç—…"ã€"å¿ƒç­‹æ¢—å¡"ãªã©ï¼‰ãŒã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰åˆ†å‰²ã•ã‚Œã€é©åˆ‡ã«è¡¨ç¾ã•ã‚Œãªã„</li>
<li>å¯¾ç­–ï¼šåŒ»ç™‚ãƒ‰ãƒ¡ã‚¤ãƒ³ã§è¿½åŠ äº‹å‰å­¦ç¿’ï¼ˆDomain-Adaptive Pretrainingï¼‰ã‚’å®Ÿæ–½</li>
</ul>

<p><strong>èª²é¡Œ2: æ–‡è„ˆç†è§£ã®ãƒŸã‚¹ãƒãƒƒãƒ</strong></p>
<ul>
<li>å•é¡Œï¼šä¸€èˆ¬ãƒ†ã‚­ã‚¹ãƒˆã¨åŒ»ç™‚æ–‡çŒ®ã®æ–‡ä½“ãƒ»æ§‹é€ ãŒç•°ãªã‚‹</li>
<li>å¯¾ç­–ï¼šåŒ»ç™‚BERTãƒ¢ãƒ‡ãƒ«ï¼ˆBioBERTã€ClinicalBERTãªã©ï¼‰ã‚’ä½¿ç”¨</li>
</ul>

<p><strong>èª²é¡Œ3: å°‚é–€çŸ¥è­˜ã®æ¬ å¦‚</strong></p>
<ul>
<li>å•é¡Œï¼šç–¾æ‚£é–“ã®é–¢ä¿‚æ€§ã‚„åŒ»å­¦çš„å› æœé–¢ä¿‚ã‚’ç†è§£ã—ã¦ã„ãªã„</li>
<li>å¯¾ç­–ï¼šKnowledge-enhancedæ‰‹æ³•ã§åŒ»å­¦çŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚’çµ±åˆ</li>
</ul>

<p><strong>èª²é¡Œ4: æ€§èƒ½ã®é™ç•Œ</strong></p>
<ul>
<li>å•é¡Œï¼šæ±ç”¨BERTã§ã¯å°‚é–€ãƒ‰ãƒ¡ã‚¤ãƒ³ã§æ€§èƒ½ãŒä½ã„</li>
<li>å¯¾ç­–ï¼šå¤§é‡ã®åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€ã¾ãŸã¯ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨</li>
</ul>
</details>

<details>
<summary><strong>å•é¡Œ5: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</strong></summary>
<p><strong>è³ªå•</strong>ï¼šæ„Ÿæƒ…åˆ†æã‚¿ã‚¹ã‚¯ã§BERTã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹éš›ã€ä»¥ä¸‹ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ã‚’èª¬æ˜ã—ã€æ¨å¥¨å€¤ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚</p>
<ol>
<li>å­¦ç¿’ç‡ï¼ˆLearning Rateï¼‰</li>
<li>ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆBatch Sizeï¼‰</li>
<li>Warmupã‚¹ãƒ†ãƒƒãƒ—æ•°</li>
<li>Weight Decay</li>
<li>ã‚¨ãƒãƒƒã‚¯æ•°</li>
</ol>

<p><strong>è§£ç­”ä¾‹</strong>ï¼š</p>

<p><strong>1. å­¦ç¿’ç‡</strong></p>
<ul>
<li><strong>å½±éŸ¿</strong>ï¼šé«˜ã™ãã‚‹ã¨ç™ºæ•£ã€ä½ã™ãã‚‹ã¨åæŸé…ã„</li>
<li><strong>æ¨å¥¨å€¤</strong>ï¼š2e-5 ã€œ 5e-5ï¼ˆå…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿FTï¼‰ã€1e-4 ã€œ 3e-4ï¼ˆLoRAï¼‰</li>
<li><strong>èª¿æ•´æ³•</strong>ï¼šLearning Rate Schedulerã§æ®µéšçš„ã«æ¸›å°‘</li>
</ul>

<p><strong>2. ãƒãƒƒãƒã‚µã‚¤ã‚º</strong></p>
<ul>
<li><strong>å½±éŸ¿</strong>ï¼šå¤§ãã„ã¨å®‰å®šè¨“ç·´ã ãŒãƒ¡ãƒ¢ãƒªæ¶ˆè²»å¢—ã€å°ã•ã„ã¨ä¸å®‰å®š</li>
<li><strong>æ¨å¥¨å€¤</strong>ï¼š16ã€œ32ï¼ˆGPUãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦èª¿æ•´ï¼‰</li>
<li><strong>å·¥å¤«</strong>ï¼šGradient Accumulationã§å®Ÿè³ªçš„ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¢—åŠ </li>
</ul>

<p><strong>3. Warmupã‚¹ãƒ†ãƒƒãƒ—æ•°</strong></p>
<ul>
<li><strong>å½±éŸ¿</strong>ï¼šåˆæœŸã®æ€¥æ¿€ãªé‡ã¿å¤‰åŒ–ã‚’æŠ‘åˆ¶ã€å­¦ç¿’ã®å®‰å®šåŒ–</li>
<li><strong>æ¨å¥¨å€¤</strong>ï¼šå…¨è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—ã®10%ï¼ˆä¾‹ï¼š1000ã‚¹ãƒ†ãƒƒãƒ—ä¸­100ã‚¹ãƒ†ãƒƒãƒ—ï¼‰</li>
<li><strong>åŠ¹æœ</strong>ï¼šç‰¹ã«å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æœ‰åŠ¹</li>
</ul>

<p><strong>4. Weight Decay</strong></p>
<ul>
<li><strong>å½±éŸ¿</strong>ï¼šL2æ­£å‰‡åŒ–ã§éå­¦ç¿’ã‚’é˜²æ­¢</li>
<li><strong>æ¨å¥¨å€¤</strong>ï¼š0.01 ã€œ 0.1</li>
<li><strong>æ³¨æ„</strong>ï¼šLayerNormã‚„Biasã«ã¯é©ç”¨ã—ãªã„</li>
</ul>

<p><strong>5. ã‚¨ãƒãƒƒã‚¯æ•°</strong></p>
<ul>
<li><strong>å½±éŸ¿</strong>ï¼šå¤šã™ãã‚‹ã¨éå­¦ç¿’ã€å°‘ãªã„ã¨æœªå­¦ç¿’</li>
<li><strong>æ¨å¥¨å€¤</strong>ï¼š3ã€œ5ã‚¨ãƒãƒƒã‚¯ï¼ˆäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯å°‘ãªã‚ï¼‰</li>
<li><strong>å·¥å¤«</strong>ï¼šEarly Stoppingã§æ¤œè¨¼æå¤±ãŒä¸Šæ˜‡ã—ãŸã‚‰åœæ­¢</li>
</ul>

<p><strong>æœ€é©åŒ–ã®å„ªå…ˆé †ä½</strong>ï¼šå­¦ç¿’ç‡ > ãƒãƒƒãƒã‚µã‚¤ã‚º > Warmup > ã‚¨ãƒãƒƒã‚¯æ•° > Weight Decay</p>
</details>

<hr>

<div class="navigation">
    <a href="chapter2-architecture.html" class="nav-button">â† ç¬¬2ç« ï¼šTransformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</a>
    <a href="chapter4-applications.html" class="nav-button">ç¬¬4ç« ï¼šå®Ÿè·µçš„å¿œç”¨ â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
        <p>ML-A03: Transformerå…¥é–€ - äº‹å‰å­¦ç¿’ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§é«˜æ€§èƒ½NLPãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã‚ˆã†</p>
    </footer>

</body>
</html>
