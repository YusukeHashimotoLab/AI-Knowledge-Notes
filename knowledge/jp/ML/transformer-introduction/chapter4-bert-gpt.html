<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
<meta content="ç¬¬4ç« ï¼šBERTãƒ»GPT - AI Terakoya" name="description"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬4ç« ï¼šBERTãƒ»GPT - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: var(--font-body); line-height: 1.7; color: var(--color-text); background-color: var(--color-bg); font-size: 16px; }
        header { background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; padding: var(--spacing-xl) var(--spacing-md); margin-bottom: var(--spacing-xl); box-shadow: var(--box-shadow); }
        .header-content { max-width: 900px; margin: 0 auto; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: var(--spacing-sm); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; opacity: 0.95; font-weight: 400; margin-bottom: var(--spacing-md); }
        .meta { display: flex; flex-wrap: wrap; gap: var(--spacing-md); font-size: 0.9rem; opacity: 0.9; }
        .meta-item { display: flex; align-items: center; gap: 0.3rem; }
        .container { max-width: 900px; margin: 0 auto; padding: 0 var(--spacing-md) var(--spacing-xl); }
        h2 { font-size: 1.75rem; color: var(--color-primary); margin-top: var(--spacing-xl); margin-bottom: var(--spacing-md); padding-bottom: var(--spacing-xs); border-bottom: 3px solid var(--color-accent); }
        h3 { font-size: 1.4rem; color: var(--color-primary); margin-top: var(--spacing-lg); margin-bottom: var(--spacing-sm); }
        h4 { font-size: 1.1rem; color: var(--color-primary-dark); margin-top: var(--spacing-md); margin-bottom: var(--spacing-sm); }
        p { margin-bottom: var(--spacing-md); color: var(--color-text); }
        a { color: var(--color-link); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--color-link-hover); text-decoration: underline; }
        ul, ol { margin-left: var(--spacing-lg); margin-bottom: var(--spacing-md); }
        li { margin-bottom: var(--spacing-xs); color: var(--color-text); }
        pre { background-color: var(--color-code-bg); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); overflow-x: auto; margin-bottom: var(--spacing-md); font-family: var(--font-mono); font-size: 0.9rem; line-height: 1.5; }
        code { font-family: var(--font-mono); font-size: 0.9em; background-color: var(--color-code-bg); padding: 0.2em 0.4em; border-radius: 3px; }
        pre code { background-color: transparent; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: var(--spacing-md); font-size: 0.95rem; }
        th, td { border: 1px solid var(--color-border); padding: var(--spacing-sm); text-align: left; }
        th { background-color: var(--color-bg-alt); font-weight: 600; color: var(--color-primary); }
        blockquote { border-left: 4px solid var(--color-accent); padding-left: var(--spacing-md); margin: var(--spacing-md) 0; color: var(--color-text-light); font-style: italic; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        .mermaid { text-align: center; margin: var(--spacing-lg) 0; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        details { background-color: var(--color-bg-alt); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); margin-bottom: var(--spacing-md); }
        summary { cursor: pointer; font-weight: 600; color: var(--color-primary); user-select: none; padding: var(--spacing-xs); margin: calc(-1 * var(--spacing-md)); padding: var(--spacing-md); border-radius: var(--border-radius); }
        summary:hover { background-color: rgba(123, 44, 191, 0.1); }
        details[open] summary { margin-bottom: var(--spacing-md); border-bottom: 1px solid var(--color-border); }
        .navigation { display: flex; justify-content: space-between; gap: var(--spacing-md); margin: var(--spacing-xl) 0; padding-top: var(--spacing-lg); border-top: 2px solid var(--color-border); }
        .nav-button { flex: 1; padding: var(--spacing-md); background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; border-radius: var(--border-radius); text-align: center; font-weight: 600; transition: transform 0.2s, box-shadow 0.2s; box-shadow: var(--box-shadow); }
        .nav-button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); text-decoration: none; }
        footer { margin-top: var(--spacing-xl); padding: var(--spacing-lg) var(--spacing-md); background-color: var(--color-bg-alt); border-top: 1px solid var(--color-border); text-align: center; font-size: 0.9rem; color: var(--color-text-light); }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }

        .project-box { background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 50%); border-radius: var(--border-radius); padding: var(--spacing-lg); margin: var(--spacing-lg) 0; box-shadow: var(--box-shadow); }
        @media (max-width: 768px) { h1 { font-size: 1.5rem; } h2 { font-size: 1.4rem; } h3 { font-size: 1.2rem; } .meta { font-size: 0.85rem; } .navigation { flex-direction: column; } table { font-size: 0.85rem; } th, td { padding: var(--spacing-xs); } }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/transformer-introduction/chapter4-bert-gpt.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/transformer-introduction/index.html">Transformer</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 4</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬4ç« ï¼šBERTãƒ»GPT</h1>
            <p class="subtitle">äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®åŒç’§ï¼šåŒæ–¹å‘ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¨è‡ªå·±å›å¸°ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ç†è«–ã¨å®Ÿè·µ</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 28åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 9å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 6å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… BERTã®åŒæ–¹å‘ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨Masked Language Modelingã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… GPTã®è‡ªå·±å›å¸°ç”Ÿæˆã¨Causal Maskingã®ä»•çµ„ã¿ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… BERTãƒ»GPTã®äº‹å‰å­¦ç¿’ã‚¿ã‚¹ã‚¯ï¼ˆMLMã€NSPã€CLMï¼‰ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Hugging Face Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ä¸¡ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã§ãã‚‹</li>
<li>âœ… Fine-tuningã«ã‚ˆã‚‹ã‚¿ã‚¹ã‚¯ç‰¹åŒ–å‹ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ãŒã§ãã‚‹</li>
<li>âœ… BERTã¨GPTã®ä½¿ã„åˆ†ã‘ã¨é©ç”¨å ´é¢ã‚’åˆ¤æ–­ã§ãã‚‹</li>
<li>âœ… è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ã¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å®Œæˆã§ãã‚‹</li>
</ul>

<hr>

<h2>4.1 BERTã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h2>

<h3>4.1.1 BERTã®é©æ–°æ€§ã¨è¨­è¨ˆæ€æƒ³</h3>

<p><strong>BERT</strong>ï¼ˆBidirectional Encoder Representations from Transformersï¼‰ã¯ã€2018å¹´ã«GoogleãŒç™ºè¡¨ã—ãŸäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã€è‡ªç„¶è¨€èªå‡¦ç†ã«é©å‘½ã‚’ã‚‚ãŸã‚‰ã—ã¾ã—ãŸã€‚</p>

<table>
<thead>
<tr>
<th>ç‰¹æ€§</th>
<th>å¾“æ¥ãƒ¢ãƒ‡ãƒ«ï¼ˆELMoã€GPT-1ãªã©ï¼‰</th>
<th>BERT</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ–¹å‘æ€§</strong></td>
<td>å˜æ–¹å‘ï¼ˆå·¦â†’å³ï¼‰ã¾ãŸã¯æµ…ã„åŒæ–¹å‘</td>
<td>æ·±ã„åŒæ–¹å‘ï¼ˆå·¦å³ä¸¡æ–¹ã®æ–‡è„ˆã‚’åˆ©ç”¨ï¼‰</td>
</tr>
<tr>
<td><strong>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</strong></td>
<td>RNNã€LSTMã€æµ…ã„Transformer</td>
<td>Transformer Encoderã®ã¿ï¼ˆ12ã€œ24å±¤ï¼‰</td>
</tr>
<tr>
<td><strong>äº‹å‰å­¦ç¿’</strong></td>
<td>è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ï¼ˆæ¬¡å˜èªäºˆæ¸¬ï¼‰</td>
<td>Masked LM + Next Sentence Prediction</td>
</tr>
<tr>
<td><strong>ç”¨é€”</strong></td>
<td>ä¸»ã«ç”Ÿæˆã‚¿ã‚¹ã‚¯</td>
<td>åˆ†é¡ã€æŠ½å‡ºã€è³ªå•å¿œç­”ãªã©ç†è§£ã‚¿ã‚¹ã‚¯</td>
</tr>
<tr>
<td><strong>Fine-tuning</strong></td>
<td>è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ç‰¹åŒ–ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å¿…è¦</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ãªå‡ºåŠ›å±¤è¿½åŠ ã®ã¿</td>
</tr>
</tbody>
</table>

<h3>4.1.2 BERTã®åŒæ–¹å‘æ€§ã®å®Ÿç¾</h3>

<p>BERTã®æœ€å¤§ã®ç‰¹å¾´ã¯ã€<strong>åŒæ–¹å‘ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç†è§£</strong>ã§ã™ã€‚å¾“æ¥ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã¯å·¦ã‹ã‚‰å³ã¸é †æ¬¡å˜èªã‚’äºˆæ¸¬ã—ã¦ã„ã¾ã—ãŸãŒã€BERTã¯æ–‡å…¨ä½“ã‚’è¦‹æ¸¡ã—ã¦å„å˜èªã‚’ç†è§£ã—ã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    subgraph "å¾“æ¥ã®å˜æ–¹å‘ãƒ¢ãƒ‡ãƒ«ï¼ˆGPT-1ãªã©ï¼‰"
        A1[The] --> A2[cat]
        A2 --> A3[sat]
        A3 --> A4[on]
        A4 --> A5[mat]

        style A1 fill:#e74c3c,color:#fff
        style A2 fill:#e74c3c,color:#fff
        style A3 fill:#e74c3c,color:#fff
    end

    subgraph "BERTã®åŒæ–¹å‘ãƒ¢ãƒ‡ãƒ«"
        B1[The] <--> B2[cat]
        B2 <--> B3[sat]
        B3 <--> B4[on]
        B4 <--> B5[mat]

        style B2 fill:#27ae60,color:#fff
        style B3 fill:#27ae60,color:#fff
    end
</div>

<blockquote>
<p><strong>é‡è¦</strong>: BERTã¯æ–‡ä¸­ã®å˜èªã€Œcatã€ã‚’ç†è§£ã™ã‚‹éš›ã€ã€ŒTheã€ï¼ˆå·¦æ–‡è„ˆï¼‰ã¨ã€Œsat on matã€ï¼ˆå³æ–‡è„ˆï¼‰ã®ä¸¡æ–¹ã‚’åŒæ™‚ã«åˆ©ç”¨ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å˜èªã®æ„å‘³ã‚’æ­£ç¢ºã«æ‰ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚</p>
</blockquote>

<h3>4.1.3 BERTã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ§‹æˆ</h3>

<p>BERTã¯è¤‡æ•°ã®Transformer Encoderãƒ–ãƒ­ãƒƒã‚¯ã‚’ç©ã¿é‡ã­ãŸæ§‹é€ ã§ã™ï¼š</p>

<table>
<thead>
<tr>
<th>ãƒ¢ãƒ‡ãƒ«</th>
<th>å±¤æ•°ï¼ˆLï¼‰</th>
<th>éš ã‚Œå±¤ã‚µã‚¤ã‚ºï¼ˆHï¼‰</th>
<th>Attention Headsï¼ˆAï¼‰</th>
<th>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>BERT-Base</strong></td>
<td>12</td>
<td>768</td>
<td>12</td>
<td>110M</td>
</tr>
<tr>
<td><strong>BERT-Large</strong></td>
<td>24</td>
<td>1024</td>
<td>16</td>
<td>340M</td>
</tr>
</tbody>
</table>

<p>å„Transformer Encoderãƒ–ãƒ­ãƒƒã‚¯ã¯ã€ç¬¬2ç« ã§å­¦ã‚“ã Multi-Head Attentionã¨Feed-Forward Networkã§æ§‹æˆã•ã‚Œã¾ã™ï¼š</p>

$$
\text{EncoderBlock}(x) = \text{LayerNorm}(x + \text{FFN}(\text{LayerNorm}(x + \text{MultiHeadAttn}(x))))
$$

<h3>4.1.4 å…¥åŠ›è¡¨ç¾ï¼šToken + Segment + Position Embeddings</h3>

<p>BERTã®å…¥åŠ›ã¯3ç¨®é¡ã®Embeddingã®åˆè¨ˆã§ã™ï¼š</p>

<ol>
<li><strong>Token Embeddings</strong>: å˜èªï¼ˆã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ï¼‰ã®åŸ‹ã‚è¾¼ã¿è¡¨ç¾</li>
<li><strong>Segment Embeddings</strong>: æ–‡Aãƒ»æ–‡Bã‚’åŒºåˆ¥ï¼ˆNSPã‚¿ã‚¹ã‚¯ç”¨ï¼‰</li>
<li><strong>Position Embeddings</strong>: ä½ç½®æƒ…å ±ï¼ˆå­¦ç¿’å¯èƒ½ã€GPTã®Sinusoidalã¨ã¯ç•°ãªã‚‹ï¼‰</li>
</ol>

$$
\text{Input} = \text{TokenEmbed}(x) + \text{SegmentEmbed}(x) + \text{PositionEmbed}(x)
$$

<div class="mermaid">
graph TB
    subgraph "BERTå…¥åŠ›æ§‹æˆ"
        T1["[CLS] The cat sat [SEP] on mat [SEP]"]

        T2[Token Embeddings]
        T3[Segment Embeddings]
        T4[Position Embeddings]

        T5[Input to Transformer]

        T1 --> T2
        T1 --> T3
        T1 --> T4

        T2 --> T5
        T3 --> T5
        T4 --> T5

        style T5 fill:#7b2cbf,color:#fff
    end
</div>

<p><strong>ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³</strong>ï¼š</p>
<ul>
<li><code>[CLS]</code>: æ–‡å…¨ä½“ã®åˆ†é¡è¡¨ç¾ï¼ˆClassification tokenï¼‰</li>
<li><code>[SEP]</code>: æ–‡ã®åŒºåˆ‡ã‚Šï¼ˆSeparatorï¼‰</li>
<li><code>[MASK]</code>: Masked Language Modelingç”¨ã®ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³</li>
</ul>

<hr>

<h2>4.2 BERTã®äº‹å‰å­¦ç¿’ã‚¿ã‚¹ã‚¯</h2>

<h3>4.2.1 Masked Language Modeling (MLM)</h3>

<p>MLMã¯ã€å…¥åŠ›ã®ä¸€éƒ¨ã‚’ãƒã‚¹ã‚¯ã—ã¦ã€ãã®å˜èªã‚’äºˆæ¸¬ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚ŠåŒæ–¹å‘ã®æ–‡è„ˆã‚’å­¦ç¿’ã—ã¾ã™ã€‚</p>

<h4>MLMã®æ‰‹é †</h4>

<ol>
<li>å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®15%ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ</li>
<li>é¸æŠã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã—ã¦ï¼š
<ul>
<li>80%ã®ç¢ºç‡ã§<code>[MASK]</code>ãƒˆãƒ¼ã‚¯ãƒ³ã«ç½®æ›</li>
<li>10%ã®ç¢ºç‡ã§ãƒ©ãƒ³ãƒ€ãƒ ãªåˆ¥ã®å˜èªã«ç½®æ›</li>
<li>10%ã®ç¢ºç‡ã§å…ƒã®å˜èªã®ã¾ã¾ä¿æŒ</li>
</ul>
</li>
<li>ãƒã‚¹ã‚¯ã•ã‚ŒãŸä½ç½®ã®å…ƒã®å˜èªã‚’äºˆæ¸¬</li>
</ol>

<p><strong>ä¾‹</strong>ï¼š</p>
<pre><code>å…¥åŠ›: "The cat sat on the mat"
ãƒã‚¹ã‚¯å¾Œ: "The [MASK] sat on the mat"
ç›®æ¨™: "cat"ã‚’äºˆæ¸¬
</code></pre>

<h4>ãªãœ100%ãƒã‚¹ã‚¯ã—ãªã„ã®ã‹ï¼Ÿ</h4>

<p>Fine-tuningæ™‚ã«<code>[MASK]</code>ãƒˆãƒ¼ã‚¯ãƒ³ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚è¨“ç·´ã¨æœ¬ç•ªã®ã‚®ãƒ£ãƒƒãƒ—ã‚’æ¸›ã‚‰ã™ãŸã‚ã€ä¸€éƒ¨ã‚’ãƒ©ãƒ³ãƒ€ãƒ å˜èªã‚„å…ƒã®å˜èªã®ã¾ã¾ã«ã—ã¾ã™ã€‚</p>

<h3>4.2.2 Next Sentence Prediction (NSP)</h3>

<p>NSPã¯ã€2ã¤ã®æ–‡ãŒé€£ç¶šã—ã¦ã„ã‚‹ã‹ã‚’åˆ¤å®šã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚è³ªå•å¿œç­”ã‚„è‡ªç„¶è¨€èªæ¨è«–ã§æ–‡é–“é–¢ä¿‚ã®ç†è§£ãŒé‡è¦ã¨ãªã‚Šã¾ã™ã€‚</p>

<h4>NSPã®æ§‹æˆ</h4>

<ul>
<li><strong>IsNext</strong> (50%): å®Ÿéš›ã«é€£ç¶šã™ã‚‹æ–‡ãƒšã‚¢</li>
<li><strong>NotNext</strong> (50%): ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã°ã‚ŒãŸéé€£ç¶šæ–‡ãƒšã‚¢</li>
</ul>

<p><strong>ä¾‹</strong>ï¼š</p>
<pre><code>å…¥åŠ›A: "The cat sat on the mat."
å…¥åŠ›B (IsNext): "It was very comfortable."
å…¥åŠ›B (NotNext): "Paris is the capital of France."

BERTå…¥åŠ›: [CLS] The cat sat on the mat [SEP] It was very comfortable [SEP]
ç›®æ¨™: IsNext = True
</code></pre>

<h3>4.2.3 PyTorchã«ã‚ˆã‚‹MLMã®å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import random
import numpy as np

class MaskedLanguageModel:
    """BERT-style Masked Language Modelingå®Ÿè£…"""

    def __init__(self, vocab_size, mask_prob=0.15):
        self.vocab_size = vocab_size
        self.mask_prob = mask_prob

        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ID
        self.MASK_TOKEN_ID = vocab_size - 3
        self.CLS_TOKEN_ID = vocab_size - 2
        self.SEP_TOKEN_ID = vocab_size - 1

    def create_masked_lm_data(self, input_ids):
        """
        MLMç”¨ã®ãƒã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ

        Args:
            input_ids: [batch_size, seq_len] å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ID

        Returns:
            masked_input: ãƒã‚¹ã‚¯é©ç”¨å¾Œã®å…¥åŠ›
            labels: äºˆæ¸¬å¯¾è±¡ã®ãƒ©ãƒ™ãƒ«ï¼ˆãƒã‚¹ã‚¯ä½ç½®ã®ã¿æœ‰åŠ¹ã€ä»–ã¯-100ï¼‰
        """
        batch_size, seq_len = input_ids.shape

        # ãƒ©ãƒ™ãƒ«åˆæœŸåŒ–ï¼ˆ-100ã¯æå¤±è¨ˆç®—ã§ç„¡è¦–ã•ã‚Œã‚‹ï¼‰
        labels = torch.full_like(input_ids, -100)
        masked_input = input_ids.clone()

        for i in range(batch_size):
            # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å¤–ã—ã¦ãƒã‚¹ã‚¯å¯¾è±¡ã‚’é¸æŠ
            special_tokens_mask = (input_ids[i] == self.CLS_TOKEN_ID) | \
                                 (input_ids[i] == self.SEP_TOKEN_ID)

            # ãƒã‚¹ã‚¯å¯èƒ½ãªä½ç½®
            candidate_indices = torch.where(~special_tokens_mask)[0]

            # 15%ã‚’ãƒã‚¹ã‚¯å¯¾è±¡ã«é¸æŠ
            num_to_mask = max(1, int(len(candidate_indices) * self.mask_prob))
            mask_indices = candidate_indices[torch.randperm(len(candidate_indices))[:num_to_mask]]

            for idx in mask_indices:
                labels[i, idx] = input_ids[i, idx]  # å…ƒã®å˜èªã‚’ä¿å­˜

                rand = random.random()
                if rand < 0.8:
                    # 80%: [MASK]ãƒˆãƒ¼ã‚¯ãƒ³ã«ç½®æ›
                    masked_input[i, idx] = self.MASK_TOKEN_ID
                elif rand < 0.9:
                    # 10%: ãƒ©ãƒ³ãƒ€ãƒ ãªå˜èªã«ç½®æ›
                    random_token = random.randint(0, self.vocab_size - 4)
                    masked_input[i, idx] = random_token
                # 10%: å…ƒã®å˜èªã®ã¾ã¾ï¼ˆelseä¸è¦ï¼‰

        return masked_input, labels


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== Masked Language Modeling Demo ===\n")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
vocab_size = 1000
batch_size = 3
seq_len = 10

# ãƒ€ãƒŸãƒ¼å…¥åŠ›ç”Ÿæˆ
mlm = MaskedLanguageModel(vocab_size)
input_ids = torch.randint(0, vocab_size - 3, (batch_size, seq_len))

# [CLS]ã‚’å…ˆé ­ã€[SEP]ã‚’æœ«å°¾ã«è¿½åŠ 
input_ids[:, 0] = mlm.CLS_TOKEN_ID
input_ids[:, -1] = mlm.SEP_TOKEN_ID

print("Original Input IDs (Batch 0):")
print(input_ids[0].numpy())

# MLMãƒã‚¹ã‚¯é©ç”¨
masked_input, labels = mlm.create_masked_lm_data(input_ids)

print("\nMasked Input IDs (Batch 0):")
print(masked_input[0].numpy())

print("\nLabels (Batch 0, -100ã¯ç„¡è¦–):")
print(labels[0].numpy())

# ãƒã‚¹ã‚¯ä½ç½®ã‚’ç¢ºèª
mask_positions = torch.where(labels[0] != -100)[0]
print(f"\nMasked Positions: {mask_positions.numpy()}")
print(f"Number of masked tokens: {len(mask_positions)} / {seq_len-2} (excluding [CLS] and [SEP])")

for pos in mask_positions:
    original = input_ids[0, pos].item()
    masked = masked_input[0, pos].item()
    target = labels[0, pos].item()

    mask_type = "MASK" if masked == mlm.MASK_TOKEN_ID else \
                "RANDOM" if masked != original else \
                "UNCHANGED"

    print(f"  Position {pos}: Original={original}, Masked={masked} ({mask_type}), Target={target}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Masked Language Modeling Demo ===

Original Input IDs (Batch 0):
[998 453 721 892 156 334 667 289 445 999]

Masked Input IDs (Batch 0):
[998 997 721 542 156 997 667 289 445 999]

Labels (Batch 0, -100ã¯ç„¡è¦–):
[-100 453 -100 892 -100 334 -100 -100 -100 -100]

Masked Positions: [1 3 5]
Number of masked tokens: 3 / 8 (excluding [CLS] and [SEP])
  Position 1: Original=453, Masked=997 (MASK), Target=453
  Position 3: Original=892, Masked=542 (RANDOM), Target=892
  Position 5: Original=334, Masked=997 (MASK), Target=334
</code></pre>

<hr>

<h2>4.3 BERTã®ä½¿ç”¨ä¾‹</h2>

<h3>4.3.1 ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ï¼ˆSentiment Analysisï¼‰</h3>

<p>BERTã‚’ä½¿ã£ãŸæ„Ÿæƒ…åˆ†æã®å®Ÿè£…ä¾‹ã§ã™ã€‚<code>[CLS]</code>ãƒˆãƒ¼ã‚¯ãƒ³ã®å‡ºåŠ›ã‚’åˆ†é¡ã«ä½¿ç”¨ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">from transformers import BertTokenizer, BertForSequenceClassification
import torch

# ãƒ¢ãƒ‡ãƒ«ã¨Tokenizerã®èª­ã¿è¾¼ã¿
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2  # 2ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼ˆPositive/Negativeï¼‰
)

# æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
model.eval()

# ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
texts = [
    "I absolutely loved this movie! It was fantastic.",
    "This product is terrible and waste of money.",
    "The service was okay, nothing special."
]

print("=== BERT Sentiment Analysis Demo ===\n")

for text in texts:
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
    inputs = tokenizer(
        text,
        return_tensors='pt',
        padding=True,
        truncation=True,
        max_length=128
    )

    # æ¨è«–
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=1)
        predicted_class = torch.argmax(probs, dim=1).item()

    sentiment = "Positive" if predicted_class == 1 else "Negative"
    confidence = probs[0, predicted_class].item()

    print(f"Text: {text}")
    print(f"Sentiment: {sentiment} (Confidence: {confidence:.4f})")
    print(f"Probabilities: Negative={probs[0, 0]:.4f}, Positive={probs[0, 1]:.4f}\n")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== BERT Sentiment Analysis Demo ===

Text: I absolutely loved this movie! It was fantastic.
Sentiment: Positive (Confidence: 0.8234)
Probabilities: Negative=0.1766, Positive=0.8234

Text: This product is terrible and waste of money.
Sentiment: Negative (Confidence: 0.9102)
Probabilities: Negative=0.9102, Positive=0.0898

Text: The service was okay, nothing special.
Sentiment: Negative (Confidence: 0.5621)
Probabilities: Negative=0.5621, Positive=0.4379
</code></pre>

<h3>4.3.2 å›ºæœ‰è¡¨ç¾èªè­˜ï¼ˆNamed Entity Recognitionï¼‰</h3>

<p>BERTã‚’Token Classificationï¼ˆå„ãƒˆãƒ¼ã‚¯ãƒ³ã«ãƒ©ãƒ™ãƒ«ã‚’ä»˜ä¸ï¼‰ã«ä½¿ç”¨ã™ã‚‹ä¾‹ã§ã™ã€‚</p>

<pre><code class="language-python">from transformers import BertTokenizerFast, BertForTokenClassification
import torch

print("\n=== BERT Named Entity Recognition Demo ===\n")

# NERç”¨ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆäº‹å‰å­¦ç¿’æ¸ˆã¿ï¼‰
model_name = 'dbmdz/bert-large-cased-finetuned-conll03-english'
tokenizer = BertTokenizerFast.from_pretrained(model_name)
model = BertForTokenClassification.from_pretrained(model_name)

model.eval()

# ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°
label_list = [
    'O',       # Outside
    'B-MISC', 'I-MISC',  # Miscellaneous
    'B-PER', 'I-PER',    # Person
    'B-ORG', 'I-ORG',    # Organization
    'B-LOC', 'I-LOC'     # Location
]

# ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
text = "Apple Inc. was founded by Steve Jobs in Cupertino, California."

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆword_idsã‚’å–å¾—ã™ã‚‹ãŸã‚is_split_into_words=Falseã§ã‚‚å‡¦ç†ï¼‰
inputs = tokenizer(text, return_tensors='pt', truncation=True)

# æ¨è«–
with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.argmax(outputs.logits, dim=2)

# ãƒˆãƒ¼ã‚¯ãƒ³ã¨ãƒ©ãƒ™ãƒ«ã‚’è¡¨ç¤º
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
predicted_labels = [label_list[pred] for pred in predictions[0].numpy()]

print(f"Text: {text}\n")
print("Token-Level Predictions:")
print(f"{'Token':<15} {'Label':<10}")
print("-" * 25)

for token, label in zip(tokens, predicted_labels):
    if token not in ['[CLS]', '[SEP]', '[PAD]']:
        print(f"{token:<15} {label:<10}")

# ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º
print("\nExtracted Entities:")
current_entity = []
current_label = None

for token, label in zip(tokens, predicted_labels):
    if label.startswith('B-'):
        if current_entity:
            print(f"  {current_label}: {' '.join(current_entity)}")
        current_entity = [token]
        current_label = label[2:]
    elif label.startswith('I-') and current_label == label[2:]:
        current_entity.append(token)
    else:
        if current_entity:
            print(f"  {current_label}: {' '.join(current_entity)}")
        current_entity = []
        current_label = None

if current_entity:
    print(f"  {current_label}: {' '.join(current_entity)}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== BERT Named Entity Recognition Demo ===

Text: Apple Inc. was founded by Steve Jobs in Cupertino, California.

Token-Level Predictions:
Token           Label
-------------------------
Apple           B-ORG
Inc             I-ORG
.               O
was             O
founded         O
by              O
Steve           B-PER
Jobs            I-PER
in              O
Cup             B-LOC
##ert           I-LOC
##ino           I-LOC
,               O
California      B-LOC
.               O

Extracted Entities:
  ORG: Apple Inc
  PER: Steve Jobs
  LOC: Cup ##ert ##ino
  LOC: California
</code></pre>

<h3>4.3.3 è³ªå•å¿œç­”ï¼ˆQuestion Answeringï¼‰</h3>

<p>BERTã®ä»£è¡¨çš„ãªå¿œç”¨ä¾‹ã§ã‚ã‚‹SQuADå½¢å¼ã®è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚</p>

<pre><code class="language-python">from transformers import BertForQuestionAnswering, BertTokenizer
import torch

print("\n=== BERT Question Answering Demo ===\n")

# SQuADã§Fine-tunedã•ã‚ŒãŸBERTãƒ¢ãƒ‡ãƒ«
model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForQuestionAnswering.from_pretrained(model_name)

model.eval()

# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨è³ªå•
context = """
Transformers is a state-of-the-art natural language processing library developed by Hugging Face.
It provides thousands of pretrained models to perform tasks on texts such as classification,
information extraction, question answering, summarization, translation, and text generation.
The library supports PyTorch, TensorFlow, and JAX frameworks.
"""

questions = [
    "Who developed Transformers?",
    "What tasks can Transformers perform?",
    "Which frameworks does the library support?"
]

for question in questions:
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
    inputs = tokenizer(
        question,
        context,
        return_tensors='pt',
        truncation=True,
        max_length=384
    )

    # æ¨è«–
    with torch.no_grad():
        outputs = model(**inputs)
        start_logits = outputs.start_logits
        end_logits = outputs.end_logits

    # é–‹å§‹ãƒ»çµ‚äº†ä½ç½®ã®äºˆæ¸¬
    start_idx = torch.argmax(start_logits)
    end_idx = torch.argmax(end_logits)

    # å›ç­”ãƒˆãƒ¼ã‚¯ãƒ³ã®æŠ½å‡º
    answer_tokens = inputs['input_ids'][0][start_idx:end_idx+1]
    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)

    # ç¢ºä¿¡åº¦ã‚¹ã‚³ã‚¢
    start_score = start_logits[0, start_idx].item()
    end_score = end_logits[0, end_idx].item()
    confidence = (start_score + end_score) / 2

    print(f"Question: {question}")
    print(f"Answer: {answer}")
    print(f"Confidence Score: {confidence:.4f}\n")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== BERT Question Answering Demo ===

Question: Who developed Transformers?
Answer: Hugging Face
Confidence Score: 8.2341

Question: What tasks can Transformers perform?
Answer: classification, information extraction, question answering, summarization, translation, and text generation
Confidence Score: 7.9823

Question: Which frameworks does the library support?
Answer: PyTorch, TensorFlow, and JAX
Confidence Score: 9.1247
</code></pre>

<hr>

<h2>4.4 GPTã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h2>

<h3>4.4.1 GPTã®è¨­è¨ˆæ€æƒ³ï¼šè‡ªå·±å›å¸°è¨€èªãƒ¢ãƒ‡ãƒ«</h3>

<p><strong>GPT</strong>ï¼ˆGenerative Pre-trained Transformerï¼‰ã¯ã€OpenAIãŒé–‹ç™ºã—ãŸè‡ªå·±å›å¸°å‹ï¼ˆautoregressiveï¼‰è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚BERTã¨ã¯å¯¾ç…§çš„ã«ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«ç‰¹åŒ–ã—ã¦ã„ã¾ã™ã€‚</p>

<table>
<thead>
<tr>
<th>ç‰¹æ€§</th>
<th>BERT</th>
<th>GPT</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</strong></td>
<td>Transformer Encoder</td>
<td>Transformer Decoderï¼ˆCross-Attentionãªã—ï¼‰</td>
</tr>
<tr>
<td><strong>æ–¹å‘æ€§</strong></td>
<td>åŒæ–¹å‘ï¼ˆBidirectionalï¼‰</td>
<td>å˜æ–¹å‘ï¼ˆUnidirectionalã€å·¦â†’å³ï¼‰</td>
</tr>
<tr>
<td><strong>äº‹å‰å­¦ç¿’</strong></td>
<td>MLM + NSP</td>
<td>Causal Language Modelingï¼ˆæ¬¡å˜èªäºˆæ¸¬ï¼‰</td>
</tr>
<tr>
<td><strong>Attention Mask</strong></td>
<td>ãªã—ï¼ˆå…¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‚ç…§ï¼‰</td>
<td>Causal Maskï¼ˆæœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’éš ã™ï¼‰</td>
</tr>
<tr>
<td><strong>ä¸»ãªç”¨é€”</strong></td>
<td>åˆ†é¡ã€æŠ½å‡ºã€è³ªå•å¿œç­”</td>
<td>ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã€å¯¾è©±ã€è¦ç´„</td>
</tr>
<tr>
<td><strong>æ¨è«–æ–¹å¼</strong></td>
<td>ä¸¦åˆ—å‡¦ç†ï¼ˆå…¨ãƒˆãƒ¼ã‚¯ãƒ³åŒæ™‚ï¼‰</td>
<td>é€æ¬¡ç”Ÿæˆï¼ˆ1ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤ï¼‰</td>
</tr>
</tbody>
</table>

<h3>4.4.2 Causal Maskingï¼šæœªæ¥ã‚’è¦‹ãªã„Attention</h3>

<p>GPTã®æ ¸å¿ƒã¯<strong>Causal Attention Mask</strong>ã§ã™ã€‚å„ä½ç½®ã¯è‡ªåˆ†ã‚ˆã‚Šå‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã‚’å‚ç…§ã§ãã¾ã™ã€‚</p>

<p><strong>Causal Maskè¡Œåˆ—</strong>ï¼ˆ1=å‚ç…§å¯èƒ½ã€0=å‚ç…§ä¸å¯ï¼‰ï¼š</p>
$$
\text{CausalMask} = \begin{bmatrix}
1 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 \\
1 & 1 & 1 & 0 \\
1 & 1 & 1 & 1
\end{bmatrix}
$$

<p>Attentionè¨ˆç®—æ™‚ã«æœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¹ã‚³ã‚¢ã‚’$-\infty$ã«ã™ã‚‹ã“ã¨ã§ã€Softmaxå¾Œã«ç¢ºç‡0ã«ãªã‚Šã¾ã™ï¼š</p>

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + M\right) V
$$

<p>ã“ã“ã§ $M$ ã¯ Causal Maskè¡Œåˆ—ã§ã€ãƒã‚¹ã‚¯ä½ç½®ã¯$-\infty$ã§ã™ã€‚</p>

<h3>4.4.3 GPT-1/2/3ã®é€²åŒ–</h3>

<table>
<thead>
<tr>
<th>ãƒ¢ãƒ‡ãƒ«</th>
<th>ç™ºè¡¨å¹´</th>
<th>å±¤æ•°</th>
<th>éš ã‚Œå±¤ã‚µã‚¤ã‚º</th>
<th>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°</th>
<th>è¨“ç·´ãƒ‡ãƒ¼ã‚¿</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>GPT-1</strong></td>
<td>2018</td>
<td>12</td>
<td>768</td>
<td>117M</td>
<td>BooksCorpus (4.5GB)</td>
</tr>
<tr>
<td><strong>GPT-2</strong></td>
<td>2019</td>
<td>48</td>
<td>1600</td>
<td>1.5B</td>
<td>WebText (40GB)</td>
</tr>
<tr>
<td><strong>GPT-3</strong></td>
<td>2020</td>
<td>96</td>
<td>12288</td>
<td>175B</td>
<td>CommonCrawl (570GB)</td>
</tr>
<tr>
<td><strong>GPT-4</strong></td>
<td>2023</td>
<td>éå…¬é–‹</td>
<td>éå…¬é–‹</td>
<td>æ¨å®š1.7T</td>
<td>éå…¬é–‹ï¼ˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ï¼‰</td>
</tr>
</tbody>
</table>

<p><strong>ä¸»ãªé€²åŒ–ãƒã‚¤ãƒ³ãƒˆ</strong>ï¼š</p>
<ul>
<li><strong>ã‚¹ã‚±ãƒ¼ãƒ«æ‹¡å¤§</strong>: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®æŒ‡æ•°çš„å¢—åŠ </li>
<li><strong>Few-shot Learning</strong>: GPT-3ä»¥é™ã€ä¾‹ã‚’æ•°å€‹ç¤ºã™ã ã‘ã§æ–°ã‚¿ã‚¹ã‚¯ã«å¯¾å¿œ</li>
<li><strong>In-context Learning</strong>: Fine-tuningãªã—ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã¿ã§å­¦ç¿’</li>
<li><strong>Emergent Abilities</strong>: è¦æ¨¡æ‹¡å¤§ã§çªç„¶ç¾ã‚Œã‚‹èƒ½åŠ›ï¼ˆæ¨è«–ã€ç¿»è¨³ãªã©ï¼‰</li>
</ul>

<h3>4.4.4 Causal Attentionå®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

class CausalSelfAttention(nn.Module):
    """GPT-style Causal Self-Attentionå®Ÿè£…"""

    def __init__(self, embed_size, num_heads):
        super(CausalSelfAttention, self).__init__()
        assert embed_size % num_heads == 0

        self.embed_size = embed_size
        self.num_heads = num_heads
        self.head_dim = embed_size // num_heads

        # Q, K, Vã®ç·šå½¢å¤‰æ›
        self.query = nn.Linear(embed_size, embed_size)
        self.key = nn.Linear(embed_size, embed_size)
        self.value = nn.Linear(embed_size, embed_size)

        # å‡ºåŠ›å±¤
        self.proj = nn.Linear(embed_size, embed_size)

    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, embed_size]

        Returns:
            output: [batch, seq_len, embed_size]
            attention_weights: [batch, num_heads, seq_len, seq_len]
        """
        batch_size, seq_len, _ = x.shape

        # Q, K, Vè¨ˆç®—
        Q = self.query(x)
        K = self.key(x)
        V = self.value(x)

        # Multi-headç”¨ã«åˆ†å‰²: [batch, num_heads, seq_len, head_dim]
        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        # Scaled Dot-Product Attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)

        # Causal Maské©ç”¨ï¼ˆä¸Šä¸‰è§’ã‚’-infã«ã™ã‚‹ï¼‰
        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
        scores = scores.masked_fill(causal_mask, float('-inf'))

        # Softmax
        attention_weights = F.softmax(scores, dim=-1)

        # Valueã¨ã®é‡ã¿ä»˜ãå’Œ
        out = torch.matmul(attention_weights, V)

        # Headsã‚’çµåˆ
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_size)

        # æœ€çµ‚å°„å½±
        output = self.proj(out)

        return output, attention_weights


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== Causal Self-Attention Demo ===\n")

batch_size = 1
seq_len = 8
embed_size = 64
num_heads = 4

# ãƒ€ãƒŸãƒ¼å…¥åŠ›
x = torch.randn(batch_size, seq_len, embed_size)

# Causal Attentioné©ç”¨
causal_attn = CausalSelfAttention(embed_size, num_heads)
output, attn_weights = causal_attn(x)

print(f"Input shape: {x.shape}")
print(f"Output shape: {output.shape}")
print(f"Attention weights shape: {attn_weights.shape}")

# Causal Maskã®å¯è¦–åŒ–
sample_attn = attn_weights[0, 0].detach().numpy()  # 1st batch, 1st head

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# å·¦: Causal Attentioné‡ã¿
ax1 = axes[0]
sns.heatmap(sample_attn,
            cmap='YlOrRd',
            cbar_kws={'label': 'Attention Weight'},
            ax=ax1,
            annot=True,
            fmt='.3f',
            linewidths=0.5,
            xticklabels=[f't{i+1}' for i in range(seq_len)],
            yticklabels=[f't{i+1}' for i in range(seq_len)])

ax1.set_xlabel('Key Position', fontsize=12, fontweight='bold')
ax1.set_ylabel('Query Position', fontsize=12, fontweight='bold')
ax1.set_title('GPT Causal Attention Weights\n(ä¸‹ä¸‰è§’ã®ã¿æœ‰åŠ¹)', fontsize=13, fontweight='bold')

# å³: Causal Maskæ§‹é€ 
causal_mask_viz = np.tril(np.ones((seq_len, seq_len)))
ax2 = axes[1]
sns.heatmap(causal_mask_viz,
            cmap='RdYlGn',
            cbar_kws={'label': '1=å‚ç…§å¯èƒ½, 0=ãƒã‚¹ã‚¯'},
            ax=ax2,
            annot=True,
            fmt='.0f',
            linewidths=0.5,
            xticklabels=[f't{i+1}' for i in range(seq_len)],
            yticklabels=[f't{i+1}' for i in range(seq_len)])

ax2.set_xlabel('Key Position', fontsize=12, fontweight='bold')
ax2.set_ylabel('Query Position', fontsize=12, fontweight='bold')
ax2.set_title('Causal Mask Structure\n(æœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’éš ã™)', fontsize=13, fontweight='bold')

plt.tight_layout()
plt.show()

print("\nç‰¹å¾´:")
print("âœ“ å„ä½ç½®ã¯è‡ªåˆ†ã‚ˆã‚Šå‰ï¼ˆå·¦ï¼‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã‚’å‚ç…§")
print("âœ“ ä¸‹ä¸‰è§’è¡Œåˆ—ã®æ§‹é€ ï¼ˆä¸Šä¸‰è§’ã¯0ï¼‰")
print("âœ“ æœªæ¥ã®æƒ…å ±ã‚’ä½¿ã‚ãªã„ãŸã‚ã€é€æ¬¡ç”ŸæˆãŒå¯èƒ½")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Causal Self-Attention Demo ===

Input shape: torch.Size([1, 8, 64])
Output shape: torch.Size([1, 8, 64])
Attention weights shape: torch.Size([1, 4, 8, 8])

ç‰¹å¾´:
âœ“ å„ä½ç½®ã¯è‡ªåˆ†ã‚ˆã‚Šå‰ï¼ˆå·¦ï¼‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã‚’å‚ç…§
âœ“ ä¸‹ä¸‰è§’è¡Œåˆ—ã®æ§‹é€ ï¼ˆä¸Šä¸‰è§’ã¯0ï¼‰
âœ“ æœªæ¥ã®æƒ…å ±ã‚’ä½¿ã‚ãªã„ãŸã‚ã€é€æ¬¡ç”ŸæˆãŒå¯èƒ½
</code></pre>

<hr>

<h2>4.5 GPTã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ</h2>

<h3>4.5.1 è‡ªå·±å›å¸°ç”Ÿæˆã®ä»•çµ„ã¿</h3>

<p>GPTã¯1ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤é€æ¬¡çš„ã«ç”Ÿæˆã—ã¾ã™ï¼š</p>

<ol>
<li>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‚’ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›</li>
<li>æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºç‡åˆ†å¸ƒã‚’äºˆæ¸¬</li>
<li>ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã§æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠ</li>
<li>é¸æŠã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã«è¿½åŠ </li>
<li>ã‚¹ãƒ†ãƒƒãƒ—2ã€œ4ã‚’ç¹°ã‚Šè¿”ã—</li>
</ol>

<h3>4.5.2 ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥</h3>

<table>
<thead>
<tr>
<th>æˆ¦ç•¥</th>
<th>èª¬æ˜</th>
<th>ç‰¹å¾´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Greedy Decoding</strong></td>
<td>æœ€é«˜ç¢ºç‡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠ</td>
<td>æ±ºå®šçš„ã€ç¹°ã‚Šè¿”ã—ãŒå¤šã„</td>
</tr>
<tr>
<td><strong>Beam Search</strong></td>
<td>è¤‡æ•°å€™è£œã‚’ä¿æŒã—ã¦æ¢ç´¢</td>
<td>å“è³ªé«˜ã„ãŒå¤šæ§˜æ€§ä½ã„</td>
</tr>
<tr>
<td><strong>Temperature Sampling</strong></td>
<td>æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ç¢ºç‡ã‚’èª¿æ•´</td>
<td>Tâ†’0ã§æ±ºå®šçš„ã€Tâ†’âˆã§ãƒ©ãƒ³ãƒ€ãƒ </td>
</tr>
<tr>
<td><strong>Top-k Sampling</strong></td>
<td>ç¢ºç‡ä¸Šä½kå€‹ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</td>
<td>å¤šæ§˜æ€§ã¨å“è³ªã®ãƒãƒ©ãƒ³ã‚¹</td>
</tr>
<tr>
<td><strong>Top-p (Nucleus)</strong></td>
<td>ç´¯ç©ç¢ºç‡pä»¥ä¸Šã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</td>
<td>å‹•çš„ãªèªå½™ã‚µã‚¤ã‚ºèª¿æ•´</td>
</tr>
</tbody>
</table>

<h3>4.5.3 GPT-2ã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Ÿè£…</h3>

<pre><code class="language-python">from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

print("=== GPT-2 Text Generation Demo ===\n")

# GPT-2ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
model_name = 'gpt2'  # 124M parameters
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

model.eval()

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
prompt = "Artificial intelligence is transforming the world by"

print(f"Prompt: {prompt}\n")
print("=" * 80)

# ç•°ãªã‚‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã§ã®ç”Ÿæˆ
strategies = [
    {
        'name': 'Greedy Decoding',
        'params': {
            'do_sample': False,
            'max_length': 50
        }
    },
    {
        'name': 'Temperature Sampling (T=0.7)',
        'params': {
            'do_sample': True,
            'max_length': 50,
            'temperature': 0.7
        }
    },
    {
        'name': 'Top-k Sampling (k=50)',
        'params': {
            'do_sample': True,
            'max_length': 50,
            'top_k': 50,
            'temperature': 1.0
        }
    },
    {
        'name': 'Top-p Sampling (p=0.9)',
        'params': {
            'do_sample': True,
            'max_length': 50,
            'top_p': 0.9,
            'temperature': 1.0
        }
    }
]

for strategy in strategies:
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
    inputs = tokenizer(prompt, return_tensors='pt')

    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            inputs['input_ids'],
            **strategy['params'],
            pad_token_id=tokenizer.eos_token_id
        )

    # ãƒ‡ã‚³ãƒ¼ãƒ‰
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print(f"\n{strategy['name']}:")
    print(f"{generated_text}")
    print("-" * 80)
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== GPT-2 Text Generation Demo ===

Prompt: Artificial intelligence is transforming the world by

================================================================================

Greedy Decoding:
Artificial intelligence is transforming the world by making it easier for people to do things that they would otherwise have to do manually. The most common example is the use of AI to automate tasks such as scheduling, scheduling appointments, and scheduling meetings.

--------------------------------------------------------------------------------

Temperature Sampling (T=0.7):
Artificial intelligence is transforming the world by enabling machines to learn from experience and make decisions without human intervention. From self-driving cars to medical diagnosis systems, AI technologies are revolutionizing industries and improving our daily lives.

--------------------------------------------------------------------------------

Top-k Sampling (k=50):
Artificial intelligence is transforming the world by creating new possibilities in healthcare, education, and entertainment. AI systems can now analyze vast amounts of data, recognize patterns, and provide insights that were previously impossible to obtain.

--------------------------------------------------------------------------------

Top-p Sampling (p=0.9):
Artificial intelligence is transforming the world by automating complex tasks, enhancing decision-making processes, and opening doors to innovations we never thought possible. As AI continues to evolve, its impact on society will only grow stronger.

--------------------------------------------------------------------------------
</code></pre>

<h3>4.5.4 ã‚«ã‚¹ã‚¿ãƒ ç”Ÿæˆé–¢æ•°ã®å®Ÿè£…</h3>

<pre><code class="language-python">def generate_text_custom(model, tokenizer, prompt, max_length=50,
                        strategy='top_p', temperature=1.0, top_k=50, top_p=0.9):
    """
    ã‚«ã‚¹ã‚¿ãƒ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–¢æ•°

    Args:
        model: GPT-2ãƒ¢ãƒ‡ãƒ«
        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        max_length: æœ€å¤§ç”Ÿæˆé•·
        strategy: 'greedy', 'temperature', 'top_k', 'top_p'
        temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        top_k: Top-kã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®k
        top_p: Top-pã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®p

    Returns:
        ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ
    """
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
    input_ids = tokenizer.encode(prompt, return_tensors='pt')

    # ç”Ÿæˆãƒ«ãƒ¼ãƒ—
    for _ in range(max_length):
        with torch.no_grad():
            outputs = model(input_ids)
            logits = outputs.logits

        # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®logitsã‚’å–å¾—
        next_token_logits = logits[0, -1, :]

        # Temperatureé©ç”¨
        if temperature != 1.0:
            next_token_logits = next_token_logits / temperature

        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥
        if strategy == 'greedy':
            next_token_id = torch.argmax(next_token_logits).unsqueeze(0)

        elif strategy == 'temperature':
            probs = F.softmax(next_token_logits, dim=-1)
            next_token_id = torch.multinomial(probs, num_samples=1)

        elif strategy == 'top_k':
            # Top-kãƒã‚¹ã‚­ãƒ³ã‚°
            top_k_values, top_k_indices = torch.topk(next_token_logits, top_k)
            next_token_logits_filtered = torch.full_like(next_token_logits, float('-inf'))
            next_token_logits_filtered[top_k_indices] = top_k_values

            probs = F.softmax(next_token_logits_filtered, dim=-1)
            next_token_id = torch.multinomial(probs, num_samples=1)

        elif strategy == 'top_p':
            # Top-pãƒã‚¹ã‚­ãƒ³ã‚°
            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

            # ç´¯ç©ç¢ºç‡ãŒpã‚’è¶…ãˆã‚‹ä½ç½®ã‚’è¦‹ã¤ã‘ã‚‹
            sorted_indices_to_remove = cumulative_probs > top_p
            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
            sorted_indices_to_remove[0] = 0

            # ãƒã‚¹ã‚¯é©ç”¨
            next_token_logits_filtered = next_token_logits.clone()
            next_token_logits_filtered[sorted_indices[sorted_indices_to_remove]] = float('-inf')

            probs = F.softmax(next_token_logits_filtered, dim=-1)
            next_token_id = torch.multinomial(probs, num_samples=1)

        # å…¥åŠ›ã«è¿½åŠ 
        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=1)

        # EOSãƒˆãƒ¼ã‚¯ãƒ³ã§çµ‚äº†
        if next_token_id.item() == tokenizer.eos_token_id:
            break

    # ãƒ‡ã‚³ãƒ¼ãƒ‰
    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)
    return generated_text


# ã‚«ã‚¹ã‚¿ãƒ ç”Ÿæˆé–¢æ•°ã®ãƒ†ã‚¹ãƒˆ
print("\n=== Custom Generation Function Test ===\n")

prompt = "The future of machine learning is"
print(f"Prompt: {prompt}\n")

for strategy in ['greedy', 'temperature', 'top_k', 'top_p']:
    generated = generate_text_custom(
        model, tokenizer, prompt,
        max_length=30,
        strategy=strategy,
        temperature=0.8,
        top_k=40,
        top_p=0.9
    )
    print(f"{strategy.upper()}: {generated}\n")
</code></pre>

<hr>

<h2>4.6 BERT vs GPTï¼šæ¯”è¼ƒã¨ä½¿ã„åˆ†ã‘</h2>

<h3>4.6.1 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ¯”è¼ƒ</h3>

<div class="mermaid">
graph TB
    subgraph "BERT (Encoder-only)"
        B1[Input: æ–‡å…¨ä½“] --> B2[Token + Segment + Position Embeddings]
        B2 --> B3[Transformer Encoder Ã— 12]
        B3 --> B4[Bidirectional Attention]
        B4 --> B5["[CLS] for Classification<br/>All Tokens for Token-level"]

        style B4 fill:#27ae60,color:#fff
    end

    subgraph "GPT (Decoder-only)"
        G1[Input: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ] --> G2[Token + Position Embeddings]
        G2 --> G3[Transformer Decoder Ã— 12]
        G3 --> G4[Causal Attention]
        G4 --> G5[Next Token Prediction]
        G5 --> G6[Autoregressive Generation]

        style G4 fill:#e74c3c,color:#fff
    end
</div>

<h3>4.6.2 æ€§èƒ½æ¯”è¼ƒå®Ÿé¨“</h3>

<pre><code class="language-python">from transformers import BertModel, GPT2Model, BertTokenizer, GPT2Tokenizer
import torch
import time

print("=== BERT vs GPT Performance Comparison ===\n")

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased')

gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
gpt2_model = GPT2Model.from_pretrained('gpt2')

bert_model.eval()
gpt2_model.eval()

# ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
text = "Natural language processing is a fascinating field of artificial intelligence."

# BERTå‡¦ç†
bert_inputs = bert_tokenizer(text, return_tensors='pt', padding=True, truncation=True)
start_time = time.time()
with torch.no_grad():
    bert_outputs = bert_model(**bert_inputs)
bert_time = time.time() - start_time

# GPT-2å‡¦ç†
gpt2_inputs = gpt2_tokenizer(text, return_tensors='pt')
start_time = time.time()
with torch.no_grad():
    gpt2_outputs = gpt2_model(**gpt2_inputs)
gpt2_time = time.time() - start_time

# çµæœè¡¨ç¤º
print("Input Text:", text)
print(f"\nBERT:")
print(f"  Model: bert-base-uncased")
print(f"  Parameters: {sum(p.numel() for p in bert_model.parameters()):,}")
print(f"  Input shape: {bert_inputs['input_ids'].shape}")
print(f"  Output shape: {bert_outputs.last_hidden_state.shape}")
print(f"  Processing time: {bert_time*1000:.2f} ms")
print(f"  [CLS] embedding shape: {bert_outputs.pooler_output.shape}")

print(f"\nGPT-2:")
print(f"  Model: gpt2")
print(f"  Parameters: {sum(p.numel() for p in gpt2_model.parameters()):,}")
print(f"  Input shape: {gpt2_inputs['input_ids'].shape}")
print(f"  Output shape: {gpt2_outputs.last_hidden_state.shape}")
print(f"  Processing time: {gpt2_time*1000:.2f} ms")

# Attentionå¯è¦–åŒ–æ¯”è¼ƒ
print("\n" + "="*80)
print("Attention Pattern Comparison")
print("="*80)

# BERT: ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç›¸äº’å‚ç…§å¯èƒ½
print("\nBERT Attention Pattern:")
print("  âœ“ Bidirectional - ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‚ç…§")
print("  âœ“ ä¸¦åˆ—å‡¦ç†å¯èƒ½ - å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åŒæ™‚ã«å‡¦ç†")
print("  âœ“ ç”¨é€”: åˆ†é¡ã€NERã€QAã€æ–‡ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°")

# GPT: å·¦å´ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿å‚ç…§å¯èƒ½
print("\nGPT Attention Pattern:")
print("  âœ“ Unidirectional - å„ãƒˆãƒ¼ã‚¯ãƒ³ã¯å·¦å´ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿å‚ç…§")
print("  âœ“ é€æ¬¡ç”Ÿæˆ - 1ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤ç”Ÿæˆ")
print("  âœ“ ç”¨é€”: ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã€å¯¾è©±ã€è£œå®Œã€ç¿»è¨³")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== BERT vs GPT Performance Comparison ===

Input Text: Natural language processing is a fascinating field of artificial intelligence.

BERT:
  Model: bert-base-uncased
  Parameters: 109,482,240
  Input shape: torch.Size([1, 14])
  Output shape: torch.Size([1, 14, 768])
  Processing time: 45.23 ms
  [CLS] embedding shape: torch.Size([1, 768])

GPT-2:
  Model: gpt2
  Parameters: 124,439,808
  Input shape: torch.Size([1, 14])
  Output shape: torch.Size([1, 14, 768])
  Processing time: 38.67 ms

================================================================================
Attention Pattern Comparison
================================================================================

BERT Attention Pattern:
  âœ“ Bidirectional - ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‚ç…§
  âœ“ ä¸¦åˆ—å‡¦ç†å¯èƒ½ - å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åŒæ™‚ã«å‡¦ç†
  âœ“ ç”¨é€”: åˆ†é¡ã€NERã€QAã€æ–‡ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

GPT Attention Pattern:
  âœ“ Unidirectional - å„ãƒˆãƒ¼ã‚¯ãƒ³ã¯å·¦å´ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿å‚ç…§
  âœ“ é€æ¬¡ç”Ÿæˆ - 1ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤ç”Ÿæˆ
  âœ“ ç”¨é€”: ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã€å¯¾è©±ã€è£œå®Œã€ç¿»è¨³
</code></pre>

<h3>4.6.3 ä½¿ã„åˆ†ã‘ã‚¬ã‚¤ãƒ‰</h3>

<table>
<thead>
<tr>
<th>ã‚¿ã‚¹ã‚¯</th>
<th>æ¨å¥¨ãƒ¢ãƒ‡ãƒ«</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ„Ÿæƒ…åˆ†æ</strong></td>
<td>BERT</td>
<td>æ–‡å…¨ä½“ã®æ–‡è„ˆç†è§£ãŒå¿…è¦</td>
</tr>
<tr>
<td><strong>å›ºæœ‰è¡¨ç¾èªè­˜</strong></td>
<td>BERT</td>
<td>å„ãƒˆãƒ¼ã‚¯ãƒ³ã®åˆ†é¡ã€åŒæ–¹å‘æ–‡è„ˆãŒæœ‰åˆ©</td>
</tr>
<tr>
<td><strong>è³ªå•å¿œç­”</strong></td>
<td>BERT</td>
<td>æ–‡ç« ä¸­ã‹ã‚‰å›ç­”ç®‡æ‰€ã‚’ç‰¹å®š</td>
</tr>
<tr>
<td><strong>æ–‡æ›¸åˆ†é¡</strong></td>
<td>BERT</td>
<td>[CLS]ãƒˆãƒ¼ã‚¯ãƒ³ã§æ–‡å…¨ä½“ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰</td>
</tr>
<tr>
<td><strong>ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ</strong></td>
<td>GPT</td>
<td>è‡ªå·±å›å¸°ç”Ÿæˆã«ç‰¹åŒ–</td>
</tr>
<tr>
<td><strong>å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ </strong></td>
<td>GPT</td>
<td>å¿œç­”ç”ŸæˆãŒä¸»ã‚¿ã‚¹ã‚¯</td>
</tr>
<tr>
<td><strong>è¦ç´„</strong></td>
<td>GPTï¼ˆor BARTï¼‰</td>
<td>ç”Ÿæˆã‚¿ã‚¹ã‚¯ã€æŠ½è±¡çš„è¦ç´„</td>
</tr>
<tr>
<td><strong>ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ</strong></td>
<td>GPTï¼ˆCodexï¼‰</td>
<td>é€æ¬¡çš„ãªã‚³ãƒ¼ãƒ‰ç”Ÿæˆ</td>
</tr>
<tr>
<td><strong>ç¿»è¨³</strong></td>
<td>ä¸¡æ–¹å¯èƒ½</td>
<td>BERTâ†’Encoderã€GPTâ†’Decoderçš„ã«ä½¿ç”¨</td>
</tr>
</tbody>
</table>

<hr>

<h2>4.7 å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ</h2>

<h3>4.7.1 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ1: BERTã«ã‚ˆã‚‹è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ </h3>

<div class="project-box">
<h4>ç›®æ¨™</h4>
<p>SQuADå½¢å¼ã®è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ­£ç¢ºãªå›ç­”ã‚’æŠ½å‡ºã—ã¾ã™ã€‚</p>

<h4>å®Ÿè£…è¦ä»¶</h4>
<ul>
<li>Fine-tunedã•ã‚ŒãŸBERTãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨</li>
<li>è¤‡æ•°ã®è³ªå•ã«å¯¾ã™ã‚‹å›ç­”æŠ½å‡º</li>
<li>ç¢ºä¿¡åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ã¨è¡¨ç¤º</li>
<li>å›ç­”ã®å¦¥å½“æ€§æ¤œè¨¼</li>
</ul>
</div>

<pre><code class="language-python">from transformers import BertForQuestionAnswering, BertTokenizer
import torch

class QuestionAnsweringSystem:
    """BERTãƒ™ãƒ¼ã‚¹ã®è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, model_name='bert-large-uncased-whole-word-masking-finetuned-squad'):
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.model = BertForQuestionAnswering.from_pretrained(model_name)
        self.model.eval()

    def answer_question(self, question, context, return_confidence=True):
        """
        è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’æŠ½å‡º

        Args:
            question: è³ªå•æ–‡
            context: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå›ç­”å…ƒã®æ–‡ç« ï¼‰
            return_confidence: ç¢ºä¿¡åº¦ã‚’è¿”ã™ã‹ã©ã†ã‹

        Returns:
            answer: æŠ½å‡ºã•ã‚ŒãŸå›ç­”
            confidence: ç¢ºä¿¡åº¦ã‚¹ã‚³ã‚¢ï¼ˆreturn_confidence=Trueã®å ´åˆï¼‰
        """
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        inputs = self.tokenizer(
            question,
            context,
            return_tensors='pt',
            truncation=True,
            max_length=512,
            padding=True
        )

        # æ¨è«–
        with torch.no_grad():
            outputs = self.model(**inputs)

        # é–‹å§‹ãƒ»çµ‚äº†ä½ç½®ã®äºˆæ¸¬
        start_logits = outputs.start_logits
        end_logits = outputs.end_logits

        start_idx = torch.argmax(start_logits)
        end_idx = torch.argmax(end_logits)

        # å›ç­”æŠ½å‡º
        answer_tokens = inputs['input_ids'][0][start_idx:end_idx+1]
        answer = self.tokenizer.decode(answer_tokens, skip_special_tokens=True)

        if return_confidence:
            # ç¢ºä¿¡åº¦è¨ˆç®—
            start_score = torch.softmax(start_logits, dim=1)[0, start_idx].item()
            end_score = torch.softmax(end_logits, dim=1)[0, end_idx].item()
            confidence = (start_score + end_score) / 2

            return answer, confidence
        else:
            return answer

    def batch_answer(self, qa_pairs):
        """
        è¤‡æ•°ã®è³ªå•ã«ä¸€æ‹¬ã§å›ç­”

        Args:
            qa_pairs: [(question, context), ...] ã®ãƒªã‚¹ãƒˆ

        Returns:
            results: [(answer, confidence), ...] ã®ãƒªã‚¹ãƒˆ
        """
        results = []
        for question, context in qa_pairs:
            answer, confidence = self.answer_question(question, context)
            results.append((answer, confidence))
        return results


# ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ
print("=== Question Answering System Demo ===\n")

qa_system = QuestionAnsweringSystem()

# ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
context = """
The Transformer architecture was introduced in the paper "Attention is All You Need"
by Vaswani et al. in 2017. It relies entirely on self-attention mechanisms to compute
representations of input and output sequences without using recurrent or convolutional layers.
The model achieved state-of-the-art results on machine translation tasks and has since become
the foundation for models like BERT and GPT. The architecture consists of an encoder and a decoder,
each composed of multiple identical layers. Each layer has two sub-layers: a multi-head self-attention
mechanism and a position-wise fully connected feed-forward network.
"""

questions = [
    "When was the Transformer introduced?",
    "Who introduced the Transformer?",
    "What does the Transformer rely on?",
    "What are the two main components of the Transformer?",
    "What models are based on the Transformer?"
]

print("Context:")
print(context)
print("\n" + "="*80 + "\n")

for i, question in enumerate(questions, 1):
    answer, confidence = qa_system.answer_question(question, context)

    print(f"Q{i}: {question}")
    print(f"A{i}: {answer}")
    print(f"Confidence: {confidence:.4f}")
    print()

# ãƒãƒƒãƒå‡¦ç†ã®ãƒ‡ãƒ¢
print("="*80)
print("\nBatch Processing Demo:")
print("="*80 + "\n")

qa_pairs = [(q, context) for q in questions]
results = qa_system.batch_answer(qa_pairs)

for (question, _), (answer, conf) in zip(qa_pairs, results):
    print(f"Q: {question}")
    print(f"A: {answer} (Conf: {conf:.4f})\n")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Question Answering System Demo ===

Context:
The Transformer architecture was introduced in the paper "Attention is All You Need"
by Vaswani et al. in 2017. It relies entirely on self-attention mechanisms to compute
representations of input and output sequences without using recurrent or convolutional layers.
The model achieved state-of-the-art results on machine translation tasks and has since become
the foundation for models like BERT and GPT. The architecture consists of an encoder and a decoder,
each composed of multiple identical layers. Each layer has two sub-layers: a multi-head self-attention
mechanism and a position-wise fully connected feed-forward network.

================================================================================

Q1: When was the Transformer introduced?
A1: 2017
Confidence: 0.9523

Q2: Who introduced the Transformer?
A2: Vaswani et al.
Confidence: 0.8876

Q3: What does the Transformer rely on?
A3: self-attention mechanisms
Confidence: 0.9234

Q4: What are the two main components of the Transformer?
A4: an encoder and a decoder
Confidence: 0.8912

Q5: What models are based on the Transformer?
A5: BERT and GPT
Confidence: 0.9101
</code></pre>

<h3>4.7.2 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ2: GPTã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¢ãƒ—ãƒª</h3>

<div class="project-box">
<h4>ç›®æ¨™</h4>
<p>ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã€æ§˜ã€…ãªç”Ÿæˆæˆ¦ç•¥ã‚’è©¦ã—ã¾ã™ã€‚</p>

<h4>å®Ÿè£…è¦ä»¶</h4>
<ul>
<li>è¤‡æ•°ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã®ã‚µãƒãƒ¼ãƒˆ</li>
<li>ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´æ©Ÿèƒ½</li>
<li>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å®Ÿè·µ</li>
<li>ç”Ÿæˆå“è³ªã®è©•ä¾¡</li>
</ul>
</div>

<pre><code class="language-python">from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

class TextGenerator:
    """GPT-2ãƒ™ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, model_name='gpt2-medium'):
        """
        Args:
            model_name: 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'
        """
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        self.model = GPT2LMHeadModel.from_pretrained(model_name)
        self.model.eval()

        # PADãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
        self.tokenizer.pad_token = self.tokenizer.eos_token

    def generate(self, prompt, max_length=100, strategy='top_p',
                num_return_sequences=1, **kwargs):
        """
        ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ

        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            max_length: æœ€å¤§ç”Ÿæˆé•·
            strategy: 'greedy', 'beam', 'temperature', 'top_k', 'top_p'
            num_return_sequences: ç”Ÿæˆã™ã‚‹å€™è£œæ•°
            **kwargs: æˆ¦ç•¥å›ºæœ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        inputs = self.tokenizer(prompt, return_tensors='pt')

        # æˆ¦ç•¥ã«å¿œã˜ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        gen_params = {
            'max_length': max_length,
            'num_return_sequences': num_return_sequences,
            'pad_token_id': self.tokenizer.eos_token_id,
            'early_stopping': True
        }

        if strategy == 'greedy':
            gen_params['do_sample'] = False

        elif strategy == 'beam':
            gen_params['num_beams'] = kwargs.get('num_beams', 5)
            gen_params['do_sample'] = False

        elif strategy == 'temperature':
            gen_params['do_sample'] = True
            gen_params['temperature'] = kwargs.get('temperature', 0.7)

        elif strategy == 'top_k':
            gen_params['do_sample'] = True
            gen_params['top_k'] = kwargs.get('top_k', 50)
            gen_params['temperature'] = kwargs.get('temperature', 1.0)

        elif strategy == 'top_p':
            gen_params['do_sample'] = True
            gen_params['top_p'] = kwargs.get('top_p', 0.9)
            gen_params['temperature'] = kwargs.get('temperature', 1.0)

        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(inputs['input_ids'], **gen_params)

        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        generated_texts = [
            self.tokenizer.decode(output, skip_special_tokens=True)
            for output in outputs
        ]

        return generated_texts

    def interactive_generation(self):
        """å¯¾è©±çš„ãªç”Ÿæˆã‚»ãƒƒã‚·ãƒ§ãƒ³"""
        print("=== Interactive Text Generation ===")
        print("Type 'quit' to exit\n")

        while True:
            prompt = input("Prompt: ")
            if prompt.lower() == 'quit':
                break

            # ç”Ÿæˆè¨­å®š
            print("\nGeneration Settings:")
            strategy = input("Strategy (greedy/beam/temperature/top_k/top_p) [top_p]: ") or 'top_p'
            max_length = int(input("Max length [100]: ") or 100)

            # ç”Ÿæˆ
            outputs = self.generate(prompt, max_length=max_length, strategy=strategy)

            print("\n--- Generated Text ---")
            print(outputs[0])
            print("-" * 80 + "\n")


# ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ
print("=== Text Generation System Demo ===\n")

generator = TextGenerator(model_name='gpt2')

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
prompts = [
    "In the future of artificial intelligence,",
    "The most important breakthrough in deep learning was",
    "Once upon a time in a distant galaxy,"
]

print("Comparing Different Generation Strategies:\n")
print("="*80 + "\n")

for prompt in prompts:
    print(f"Prompt: {prompt}\n")

    strategies = [
        ('greedy', {}),
        ('top_k', {'top_k': 50, 'temperature': 0.8}),
        ('top_p', {'top_p': 0.9, 'temperature': 0.8})
    ]

    for strategy, params in strategies:
        outputs = generator.generate(
            prompt,
            max_length=60,
            strategy=strategy,
            num_return_sequences=1,
            **params
        )

        print(f"{strategy.upper()}:")
        print(f"{outputs[0]}\n")

    print("="*80 + "\n")

# è¤‡æ•°å€™è£œç”Ÿæˆã®ãƒ‡ãƒ¢
print("\nMultiple Candidates Generation:")
print("="*80 + "\n")

prompt = "The key to successful machine learning is"
outputs = generator.generate(
    prompt,
    max_length=50,
    strategy='top_p',
    num_return_sequences=3,
    top_p=0.9,
    temperature=0.9
)

for i, output in enumerate(outputs, 1):
    print(f"Candidate {i}:")
    print(output)
    print()
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== Text Generation System Demo ===

Comparing Different Generation Strategies:

================================================================================

Prompt: In the future of artificial intelligence,

GREEDY:
In the future of artificial intelligence, we will be able to create a new kind of AI that can do things that we have never done before. We will be able to build systems that can learn from data and make decisions based on that data.

TOP_K:
In the future of artificial intelligence, machines will become increasingly capable of understanding human language, emotions, and intentions. This will revolutionize how we interact with technology and open new possibilities in healthcare, education, and entertainment.

TOP_P:
In the future of artificial intelligence, we can expect to see breakthroughs in areas such as natural language understanding, computer vision, and autonomous decision-making. These advances will transform industries and create opportunities we haven't yet imagined.

================================================================================
</code></pre>

<hr>

<h2>4.8 ã¾ã¨ã‚ã¨ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯</h2>

<h3>æœ¬ç« ã§å­¦ã‚“ã ã“ã¨</h3>

<table>
<thead>
<tr>
<th>ãƒˆãƒ”ãƒƒã‚¯</th>
<th>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>BERT</strong></td>
<td>åŒæ–¹å‘ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã€MLM+NSPã€ç†è§£ã‚¿ã‚¹ã‚¯ã«æœ€é©</td>
</tr>
<tr>
<td><strong>GPT</strong></td>
<td>è‡ªå·±å›å¸°ç”Ÿæˆã€Causal Maskingã€ç”Ÿæˆã‚¿ã‚¹ã‚¯ã«æœ€é©</td>
</tr>
<tr>
<td><strong>äº‹å‰å­¦ç¿’</strong></td>
<td>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã€Fine-tuningã§ç‰¹åŒ–</td>
</tr>
<tr>
<td><strong>ä½¿ã„åˆ†ã‘</strong></td>
<td>åˆ†é¡ãƒ»æŠ½å‡ºã¯BERTã€ç”Ÿæˆã¯GPT</td>
</tr>
<tr>
<td><strong>å®Ÿè·µæ‰‹æ³•</strong></td>
<td>Hugging Faceã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã€QAã‚·ã‚¹ãƒ†ãƒ </td>
</tr>
</tbody>
</table>

<h3>ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯</h3>

<details>
<summary><strong>RoBERTaï¼šBERTã®æ”¹è‰¯ç‰ˆ</strong></summary>
<p>Facebookã«ã‚ˆã‚‹BERTã®æ”¹è‰¯ç‰ˆã€‚NSPã‚¿ã‚¹ã‚¯ã‚’å‰Šé™¤ã—ã€å‹•çš„ãƒã‚¹ã‚­ãƒ³ã‚°ã€ã‚ˆã‚Šå¤§è¦æ¨¡ãªè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã€é•·ã„è¨“ç·´æ™‚é–“ã‚’æ¡ç”¨ã—ã¦æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã¾ã—ãŸã€‚</p>
</details>

<details>
<summary><strong>ALBERTï¼šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡åŒ–</strong></summary>
<p>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å…±æœ‰ã¨FactoråŒ–ã«ã‚ˆã‚Šã€BERTã¨åŒç­‰ã®æ€§èƒ½ã‚’å°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å®Ÿç¾ã€‚å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚’åŠ¹ç‡åŒ–ã—ã¾ã™ã€‚</p>
</details>

<details>
<summary><strong>GPT-3.5/4ï¼šInstructGPTãƒ»ChatGPT</strong></summary>
<p>Instruction Tuningã¨RLHFï¼ˆäººé–“ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰ã®å¼·åŒ–å­¦ç¿’ï¼‰ã«ã‚ˆã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã«å¾“ã†èƒ½åŠ›ã‚’å¤§å¹…ã«å‘ä¸Šã€‚å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã®ä¸»æµã«ã€‚</p>
</details>

<details>
<summary><strong>Prompt Engineering</strong></summary>
<p>ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æœ€å¤§åŒ–ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆæŠ€è¡“ã€‚Few-shot examplesã€Chain-of-Thought promptingã€Role promptingãªã©ã€‚</p>
</details>

<details>
<summary><strong>PEFT (Parameter-Efficient Fine-Tuning)</strong></summary>
<p>LoRAã€Adapterã€Prefix Tuningãªã©ã€å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã›ãšã«åŠ¹ç‡çš„ã«Fine-tuningã™ã‚‹æ‰‹æ³•ã€‚å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«æ™‚ä»£ã®å¿…é ˆæŠ€è¡“ã€‚</p>
</details>

<h3>æ¼”ç¿’å•é¡Œ</h3>

<div class="project-box">
<h4>æ¼”ç¿’ 4.1: BERT Fine-tuningã«ã‚ˆã‚‹æ„Ÿæƒ…åˆ†æ</h4>
<p><strong>èª²é¡Œ</strong>: IMDBãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§BERTã‚’Fine-tuningã—ã€æ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>è¦ä»¶</strong>:</p>
<ul>
<li>ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³</li>
<li>BERT-Baseãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨åˆ†é¡å±¤ã®è¿½åŠ </li>
<li>è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®å®Ÿè£…</li>
<li>ç²¾åº¦ãƒ»F1ã‚¹ã‚³ã‚¢ã®è©•ä¾¡</li>
</ul>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.2: GPT-2ã«ã‚ˆã‚‹å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ </h4>
<p><strong>èª²é¡Œ</strong>: GPT-2ã‚’ä½¿ã£ãŸç°¡å˜ãªå¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>è¦ä»¶</strong>:</p>
<ul>
<li>ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®å—ä»˜ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†</li>
<li>å¿œç­”ç”Ÿæˆï¼ˆè¤‡æ•°ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ï¼‰</li>
<li>ä¼šè©±å±¥æ­´ã®ä¿æŒã¨åæ˜ </li>
<li>å¯¾è©±ã®è‡ªç„¶æ€§è©•ä¾¡</li>
</ul>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.3: BERT vs GPTæ€§èƒ½æ¯”è¼ƒ</h4>
<p><strong>èª²é¡Œ</strong>: åŒã˜ã‚¿ã‚¹ã‚¯ã§BERTã¨GPTã®æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>ã‚¿ã‚¹ã‚¯</strong>: æ–‡æ›¸åˆ†é¡ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ï¼‰</p>
<p><strong>æ¯”è¼ƒé …ç›®</strong>:</p>
<ul>
<li>ç²¾åº¦ã€F1ã‚¹ã‚³ã‚¢</li>
<li>è¨“ç·´æ™‚é–“</li>
<li>æ¨è«–é€Ÿåº¦</li>
<li>ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡</li>
</ul>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.4: Masked Language Modelingã®å®Ÿè£…</h4>
<p><strong>èª²é¡Œ</strong>: å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§MLMã‚’å®Ÿè£…ã—ã€BERTã®äº‹å‰å­¦ç¿’ã‚’å†ç¾ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>å®Ÿè£…å†…å®¹</strong>:</p>
<ul>
<li>ãƒã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯</li>
<li>MLMæå¤±é–¢æ•°</li>
<li>è¨“ç·´ãƒ«ãƒ¼ãƒ—</li>
<li>ãƒã‚¹ã‚¯äºˆæ¸¬ç²¾åº¦ã®è©•ä¾¡</li>
</ul>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.5: å¤šè¨€èªBERTï¼ˆmBERTï¼‰ã®æ´»ç”¨</h4>
<p><strong>èª²é¡Œ</strong>: å¤šè¨€èªBERTã‚’ä½¿ã£ã¦ã€è¤‡æ•°è¨€èªã§ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>è¨€èª</strong>: è‹±èªã€æ—¥æœ¬èªã€ä¸­å›½èª</p>
<p><strong>ã‚¿ã‚¹ã‚¯</strong>: ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡</p>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.6: GPTã«ã‚ˆã‚‹ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ</h4>
<p><strong>èª²é¡Œ</strong>: GPT-2ã‚’ä½¿ã£ã¦ã€è‡ªç„¶è¨€èªã®æŒ‡ç¤ºã‹ã‚‰Pythonã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>è¦ä»¶</strong>:</p>
<ul>
<li>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®è¨­è¨ˆ</li>
<li>ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã¨æ§‹æ–‡æ¤œè¨¼</li>
<li>ç”Ÿæˆå“è³ªã®è©•ä¾¡</li>
</ul>
</div>

<hr>

<h3>æ¬¡ç« äºˆå‘Š</h3>

<p>ç¬¬5ç« ã§ã¯ã€<strong>Vision Transformer (ViT)</strong>ã‚’å­¦ã³ã¾ã™ã€‚Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’Computer Visionã«é©ç”¨ã—ã€ç”»åƒã‚’ã€Œãƒˆãƒ¼ã‚¯ãƒ³ã€ã¨ã—ã¦æ‰±ã†é©æ–°çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¢ã‚Šã¾ã™ã€‚</p>

<blockquote>
<p><strong>æ¬¡ç« ã®ãƒˆãƒ”ãƒƒã‚¯</strong>:<br>
ãƒ»Vision Transformerã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£<br>
ãƒ»ç”»åƒãƒ‘ãƒƒãƒã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–<br>
ãƒ»Position Embeddingsã®2Dæ‹¡å¼µ<br>
ãƒ»CNNã¨ã®æ€§èƒ½æ¯”è¼ƒ<br>
ãƒ»Pre-trainingæˆ¦ç•¥ï¼ˆImageNet-21kï¼‰<br>
ãƒ»å®Ÿè£…ï¼šViTã«ã‚ˆã‚‹ç”»åƒåˆ†é¡<br>
ãƒ»å¿œç”¨ï¼šObject Detectionã€Segmentation</p>
</blockquote>

        <div class="navigation">
            <a href="chapter3-pretraining-finetuning.html" class="nav-button">â† ç¬¬3ç« : ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯</a>
            <a href="chapter5-large-language-models.html" class="nav-button">ç¬¬5ç« : Vision Transformer â†’</a>
        </div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
        <p>ç¬¬4ç« ï¼šBERTãƒ»GPT | Transformerå…¥é–€ã‚·ãƒªãƒ¼ã‚º</p>
    </footer>

</body>
</html>
