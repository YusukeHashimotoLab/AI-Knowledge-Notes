<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬4ç« ï¼šé«˜åº¦ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ€è¡“ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: var(--font-body); line-height: 1.7; color: var(--color-text); background-color: var(--color-bg); font-size: 16px; }
        header { background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; padding: var(--spacing-xl) var(--spacing-md); margin-bottom: var(--spacing-xl); box-shadow: var(--box-shadow); }
        .header-content { max-width: 900px; margin: 0 auto; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: var(--spacing-sm); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; opacity: 0.95; font-weight: 400; margin-bottom: var(--spacing-md); }
        .meta { display: flex; flex-wrap: wrap; gap: var(--spacing-md); font-size: 0.9rem; opacity: 0.9; }
        .meta-item { display: flex; align-items: center; gap: 0.3rem; }
        .container { max-width: 900px; margin: 0 auto; padding: 0 var(--spacing-md) var(--spacing-xl); }
        h2 { font-size: 1.75rem; color: var(--color-primary); margin-top: var(--spacing-xl); margin-bottom: var(--spacing-md); padding-bottom: var(--spacing-xs); border-bottom: 3px solid var(--color-accent); }
        h3 { font-size: 1.4rem; color: var(--color-primary); margin-top: var(--spacing-lg); margin-bottom: var(--spacing-sm); }
        h4 { font-size: 1.1rem; color: var(--color-primary-dark); margin-top: var(--spacing-md); margin-bottom: var(--spacing-sm); }
        p { margin-bottom: var(--spacing-md); color: var(--color-text); }
        a { color: var(--color-link); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--color-link-hover); text-decoration: underline; }
        ul, ol { margin-left: var(--spacing-lg); margin-bottom: var(--spacing-md); }
        li { margin-bottom: var(--spacing-xs); color: var(--color-text); }
        pre { background-color: var(--color-code-bg); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); overflow-x: auto; margin-bottom: var(--spacing-md); font-family: var(--font-mono); font-size: 0.9rem; line-height: 1.5; }
        code { font-family: var(--font-mono); font-size: 0.9em; background-color: var(--color-code-bg); padding: 0.2em 0.4em; border-radius: 3px; }
        pre code { background-color: transparent; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: var(--spacing-md); font-size: 0.95rem; }
        th, td { border: 1px solid var(--color-border); padding: var(--spacing-sm); text-align: left; }
        th { background-color: var(--color-bg-alt); font-weight: 600; color: var(--color-primary); }
        blockquote { border-left: 4px solid var(--color-accent); padding-left: var(--spacing-md); margin: var(--spacing-md) 0; color: var(--color-text-light); font-style: italic; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        .mermaid { text-align: center; margin: var(--spacing-lg) 0; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        details { background-color: var(--color-bg-alt); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); margin-bottom: var(--spacing-md); }
        summary { cursor: pointer; font-weight: 600; color: var(--color-primary); user-select: none; padding: var(--spacing-xs); margin: calc(-1 * var(--spacing-md)); padding: var(--spacing-md); border-radius: var(--border-radius); }
        summary:hover { background-color: rgba(123, 44, 191, 0.1); }
        details[open] summary { margin-bottom: var(--spacing-md); border-bottom: 1px solid var(--color-border); }
        .navigation { display: flex; justify-content: space-between; gap: var(--spacing-md); margin: var(--spacing-xl) 0; padding-top: var(--spacing-lg); border-top: 2px solid var(--color-border); }
        .nav-button { flex: 1; padding: var(--spacing-md); background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; border-radius: var(--border-radius); text-align: center; font-weight: 600; transition: transform 0.2s, box-shadow 0.2s; box-shadow: var(--box-shadow); }
        .nav-button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); text-decoration: none; }
        footer { margin-top: var(--spacing-xl); padding: var(--spacing-lg) var(--spacing-md); background-color: var(--color-bg-alt); border-top: 1px solid var(--color-border); text-align: center; font-size: 0.9rem; color: var(--color-text-light); }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }

        .project-box { background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 50%); border-radius: var(--border-radius); padding: var(--spacing-lg); margin: var(--spacing-lg) 0; box-shadow: var(--box-shadow); }
        @media (max-width: 768px) { h1 { font-size: 1.5rem; } h2 { font-size: 1.4rem; } h3 { font-size: 1.2rem; } .meta { font-size: 0.85rem; } .navigation { flex-direction: column; } table { font-size: 0.85rem; } th, td { padding: var(--spacing-xs); } }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬4ç« ï¼šé«˜åº¦ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ€è¡“</h1>
            <p class="subtitle">ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã€ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã€Kaggleæˆ¦ç•¥ã«ã‚ˆã‚‹å®Ÿè·µçš„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ§‹ç¯‰</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 32åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 4å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ï¼ˆStackingï¼‰ã®å¤šå±¤æ§‹é€ ã‚’ç†è§£ã—å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆBlendingï¼‰ã§æœ€é©ãªé‡ã¿ä»˜ã‘ã‚’è¨­è¨ˆã§ãã‚‹</li>
<li>âœ… Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã§ã®é«˜åº¦ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æˆ¦ç•¥ã‚’ç¿’å¾—ã§ãã‚‹</li>
<li>âœ… ãƒ¢ãƒ‡ãƒ«å¤šæ§˜æ€§ã¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’æ´»ç”¨ã§ãã‚‹</li>
<li>âœ… å®Ÿè·µçš„ãªã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ç”¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
<li>âœ… æœ¬ç•ªç’°å¢ƒã§ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‹ç”¨ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’ç†è§£ã§ãã‚‹</li>
</ul>

<hr>

<h2>4.1 ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ï¼ˆStackingï¼‰ã®å®Ÿè·µ</h2>

<h3>ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã®æ¦‚å¿µ</h3>

<p>ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã¯ã€è¤‡æ•°ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬çµæœã‚’ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚Votingã¨ç•°ãªã‚Šã€ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ãŒæœ€é©ãªçµ„ã¿åˆã‚ã›æ–¹ã‚’è‡ªå‹•çš„ã«å­¦ç¿’ã—ã¾ã™ã€‚</p>

<div class="mermaid">
graph TB
    A[è¨“ç·´ãƒ‡ãƒ¼ã‚¿] --> B1[Random Forest]
    A --> B2[XGBoost]
    A --> B3[LightGBM]
    A --> B4[Neural Network]

    B1 --> C[äº¤å·®æ¤œè¨¼äºˆæ¸¬]
    B2 --> C
    B3 --> C
    B4 --> C

    C --> D[ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«<br/>Logistic Regression]
    D --> E[æœ€çµ‚äºˆæ¸¬]

    style A fill:#e3f2fd
    style E fill:#c8e6c9
    style D fill:#fff3e0
</div>

<h3>åŸºæœ¬çš„ãªã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã®å®Ÿè£…</h3>

<pre><code class="language-python">import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_predict, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, roc_auc_score
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
X, y = make_classification(n_samples=2000, n_features=20, n_informative=15,
                          n_redundant=5, random_state=42)

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆã«åˆ†å‰²
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
base_models = [
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('et', ExtraTreesClassifier(n_estimators=100, random_state=42)),
    ('xgb', XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')),
    ('lgb', LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)),
    ('knn', KNeighborsClassifier(n_neighbors=5))
]

# ãƒ¡ã‚¿ç‰¹å¾´é‡ã®ä½œæˆï¼ˆäº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹äºˆæ¸¬ï¼‰
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
meta_features_train = np.zeros((X_train.shape[0], len(base_models)))
meta_features_test = np.zeros((X_test.shape[0], len(base_models)))

for i, (name, model) in enumerate(base_models):
    print(f"Training {name}...")

    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®äº¤å·®æ¤œè¨¼äºˆæ¸¬ï¼ˆout-of-fold predictionsï¼‰
    meta_features_train[:, i] = cross_val_predict(
        model, X_train, y_train, cv=cv, method='predict_proba'
    )[:, 1]

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬
    model.fit(X_train, y_train)
    meta_features_test[:, i] = model.predict_proba(X_test)[:, 1]

# ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
meta_model = LogisticRegression(random_state=42)
meta_model.fit(meta_features_train, y_train)

# æœ€çµ‚äºˆæ¸¬
y_pred = meta_model.predict(meta_features_test)
y_pred_proba = meta_model.predict_proba(meta_features_test)[:, 1]

print(f"\n=== Stacking Results ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")

# ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ï¼ˆå„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®é‡è¦åº¦ï¼‰
print(f"\nMeta-model coefficients:")
for (name, _), coef in zip(base_models, meta_model.coef_[0]):
    print(f"  {name}: {coef:.4f}")
</code></pre>

<h3>mlxtendã‚’ä½¿ã£ãŸç°¡æ½”ãªå®Ÿè£…</h3>

<pre><code class="language-python">from mlxtend.classifier import StackingCVClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
clf1 = RandomForestClassifier(n_estimators=100, random_state=42)
clf2 = GradientBoostingClassifier(n_estimators=100, random_state=42)
clf3 = SVC(probability=True, random_state=42)

# ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«
meta_clf = LogisticRegression()

# StackingCVClassifierï¼ˆè‡ªå‹•çš„ã«CVäºˆæ¸¬ã‚’è¡Œã†ï¼‰
stacking = StackingCVClassifier(
    classifiers=[clf1, clf2, clf3],
    meta_classifier=meta_clf,
    cv=5,
    use_probas=True,  # ç¢ºç‡ã‚’ä½¿ç”¨
    random_state=42
)

# è¨“ç·´ã¨è©•ä¾¡
stacking.fit(X_train, y_train)
y_pred = stacking.predict(X_test)

print(f"Stacking Accuracy: {accuracy_score(y_test, y_pred):.4f}")

# å„ãƒ¢ãƒ‡ãƒ«ã®å€‹åˆ¥æ€§èƒ½ã¨æ¯”è¼ƒ
for name, clf in [('RF', clf1), ('GB', clf2), ('SVC', clf3)]:
    clf.fit(X_train, y_train)
    acc = accuracy_score(y_test, clf.predict(X_test))
    print(f"{name} Accuracy: {acc:.4f}")
</code></pre>

<h3>å¤šå±¤ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ï¼ˆMulti-level Stackingï¼‰</h3>

<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.neural_network import MLPClassifier

# ãƒ¬ãƒ™ãƒ«1ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
level1_models = [
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('et', ExtraTreesClassifier(n_estimators=100, random_state=42)),
    ('xgb', XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')),
    ('lgb', LGBMClassifier(n_estimators=100, random_state=42, verbose=-1))
]

# ãƒ¬ãƒ™ãƒ«2ã®ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«
level2_models = [
    ('lr', LogisticRegression(random_state=42)),
    ('ridge', RidgeClassifier(random_state=42)),
]

# ãƒ¬ãƒ™ãƒ«3ã®æœ€çµ‚ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«
level3_model = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, random_state=42)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# ãƒ¬ãƒ™ãƒ«1: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬
meta_l1_train = np.zeros((X_train.shape[0], len(level1_models)))
meta_l1_test = np.zeros((X_test.shape[0], len(level1_models)))

for i, (name, model) in enumerate(level1_models):
    print(f"Level 1 - Training {name}...")
    meta_l1_train[:, i] = cross_val_predict(
        model, X_train, y_train, cv=cv, method='predict_proba'
    )[:, 1]
    model.fit(X_train, y_train)
    meta_l1_test[:, i] = model.predict_proba(X_test)[:, 1]

# ãƒ¬ãƒ™ãƒ«2: ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬
meta_l2_train = np.zeros((X_train.shape[0], len(level2_models)))
meta_l2_test = np.zeros((X_test.shape[0], len(level2_models)))

for i, (name, model) in enumerate(level2_models):
    print(f"Level 2 - Training {name}...")
    meta_l2_train[:, i] = cross_val_predict(
        model, meta_l1_train, y_train, cv=cv, method='decision_function'
    )
    model.fit(meta_l1_train, y_train)
    meta_l2_test[:, i] = model.decision_function(meta_l1_test)

# ãƒ¬ãƒ™ãƒ«3: æœ€çµ‚äºˆæ¸¬
print("Level 3 - Training final meta-model...")
level3_model.fit(meta_l2_train, y_train)
y_pred = level3_model.predict(meta_l2_test)
y_pred_proba = level3_model.predict_proba(meta_l2_test)[:, 1]

print(f"\n=== Multi-level Stacking Results ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")
</code></pre>

<hr>

<h2>4.2 ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆBlendingï¼‰</h2>

<h3>ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã¨ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã®é•ã„</h3>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°</th>
<th>ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ‡ãƒ¼ã‚¿åˆ†å‰²</strong></td>
<td>äº¤å·®æ¤œè¨¼ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰</td>
<td>ãƒ›ãƒ¼ãƒ«ãƒ‰ã‚¢ã‚¦ãƒˆï¼ˆä¸€éƒ¨ã‚’ãƒ¡ã‚¿è¨“ç·´ç”¨ï¼‰</td>
</tr>
<tr>
<td><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆ</strong></td>
<td>é«˜ã„ï¼ˆCVåˆ†ã®ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼‰</td>
<td>ä½ã„ï¼ˆ1å›ã®ã¿ï¼‰</td>
</tr>
<tr>
<td><strong>ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡</strong></td>
<td>é«˜ã„ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿æ´»ç”¨ï¼‰</td>
<td>ã‚„ã‚„ä½ã„ï¼ˆåˆ†å‰²ãŒå¿…è¦ï¼‰</td>
</tr>
<tr>
<td><strong>éå­¦ç¿’ãƒªã‚¹ã‚¯</strong></td>
<td>ä½ã„ï¼ˆCVã«ã‚ˆã‚‹æ­£å‰‡åŒ–ï¼‰</td>
<td>ã‚„ã‚„é«˜ã„ï¼ˆåˆ†å‰²æ¬¡ç¬¬ï¼‰</td>
</tr>
<tr>
<td><strong>å®Ÿè£…ã®ç°¡æ½”æ€§</strong></td>
<td>ã‚„ã‚„è¤‡é›‘</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«</td>
</tr>
</tbody>
</table>

<h3>é‡ã¿ä»˜ãå¹³å‡ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°</h3>

<pre><code class="language-python">from scipy.optimize import minimize
from sklearn.metrics import log_loss

# ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´/ãƒ–ãƒ¬ãƒ³ãƒ‰/ãƒ†ã‚¹ãƒˆã«åˆ†å‰²
X_train_base, X_blend, y_train_base, y_blend = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42
)

# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨äºˆæ¸¬
base_models = [
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('xgb', XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')),
    ('lgb', LGBMClassifier(n_estimators=100, random_state=42, verbose=-1))
]

blend_train = np.zeros((X_blend.shape[0], len(base_models)))
blend_test = np.zeros((X_test.shape[0], len(base_models)))

for i, (name, model) in enumerate(base_models):
    print(f"Training {name}...")
    model.fit(X_train_base, y_train_base)
    blend_train[:, i] = model.predict_proba(X_blend)[:, 1]
    blend_test[:, i] = model.predict_proba(X_test)[:, 1]

# æœ€é©ãªé‡ã¿ã‚’æ¢ç´¢ï¼ˆlog_lossã‚’æœ€å°åŒ–ï¼‰
def blend_loss(weights):
    """é‡ã¿ä»˜ãå¹³å‡ã®log_loss"""
    weights = weights / weights.sum()  # æ­£è¦åŒ–
    blended = np.dot(blend_train, weights)
    return log_loss(y_blend, blended)

# åˆ¶ç´„ï¼šé‡ã¿ã®åˆè¨ˆ=1ã€å„é‡ã¿â‰¥0
constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}
bounds = [(0, 1) for _ in range(len(base_models))]
initial_weights = np.ones(len(base_models)) / len(base_models)

# æœ€é©åŒ–
result = minimize(blend_loss, initial_weights, method='SLSQP',
                 bounds=bounds, constraints=constraints)

optimal_weights = result.x / result.x.sum()

print(f"\n=== Optimal Blending Weights ===")
for (name, _), weight in zip(base_models, optimal_weights):
    print(f"{name}: {weight:.4f}")

# æœ€çµ‚äºˆæ¸¬
y_pred_proba = np.dot(blend_test, optimal_weights)
y_pred = (y_pred_proba > 0.5).astype(int)

print(f"\nBlending Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"Blending AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")
</code></pre>

<h3>ãƒ©ãƒ³ã‚¯å¹³å‡ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°</h3>

<pre><code class="language-python">from scipy.stats import rankdata

# å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’ãƒ©ãƒ³ã‚¯ã«å¤‰æ›
rank_train = np.zeros_like(blend_train)
rank_test = np.zeros_like(blend_test)

for i in range(len(base_models)):
    rank_train[:, i] = rankdata(blend_train[:, i]) / len(blend_train)
    rank_test[:, i] = rankdata(blend_test[:, i]) / len(blend_test)

# ãƒ©ãƒ³ã‚¯ã®å¹³å‡
rank_avg_train = rank_train.mean(axis=1)
rank_avg_test = rank_test.mean(axis=1)

# è©•ä¾¡
y_pred = (rank_avg_test > 0.5).astype(int)

print(f"\n=== Rank Averaging Results ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"AUC: {roc_auc_score(y_test, rank_avg_test):.4f}")

# å„ãƒ¢ãƒ‡ãƒ«ã®å€‹åˆ¥æ€§èƒ½ã¨æ¯”è¼ƒ
print(f"\n=== Individual Model Performance ===")
for i, (name, _) in enumerate(base_models):
    pred = (blend_test[:, i] > 0.5).astype(int)
    print(f"{name} - Accuracy: {accuracy_score(y_test, pred):.4f}, "
          f"AUC: {roc_auc_score(y_test, blend_test[:, i]):.4f}")
</code></pre>

<hr>

<h2>4.3 Kaggleã§ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æˆ¦ç•¥</h2>

<h3>ãƒ¢ãƒ‡ãƒ«å¤šæ§˜æ€§ã®ç¢ºä¿</h3>

<p>é«˜æ€§èƒ½ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã«ã¯ã€ç•°ãªã‚‹ç‰¹æ€§ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›ãŒä¸å¯æ¬ ã§ã™ï¼š</p>

<table>
<thead>
<tr>
<th>å¤šæ§˜æ€§ã®æºæ³‰</th>
<th>å®Ÿè£…ä¾‹</th>
<th>åŠ¹æœ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </strong></td>
<td>ãƒ„ãƒªãƒ¼ç³»ã€ç·šå½¢ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ</td>
<td>ç•°ãªã‚‹æ±ºå®šå¢ƒç•Œ</td>
</tr>
<tr>
<td><strong>ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ</strong></td>
<td>ç•°ãªã‚‹ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</td>
<td>ç•°ãªã‚‹æƒ…å ±æº</td>
</tr>
<tr>
<td><strong>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</strong></td>
<td>æ·±ã•ã€å­¦ç¿’ç‡ã€æ­£å‰‡åŒ–ã®é•ã„</td>
<td>ãƒã‚¤ã‚¢ã‚¹ãƒ»ãƒãƒªã‚¢ãƒ³ã‚¹ã®èª¿æ•´</td>
</tr>
<tr>
<td><strong>ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</strong></td>
<td>ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã€CV fold</td>
<td>ãƒ‡ãƒ¼ã‚¿ã®ç•°ãªã‚‹è¦–ç‚¹</td>
</tr>
<tr>
<td><strong>ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰</strong></td>
<td>ç•°ãªã‚‹åˆæœŸåŒ–</td>
<td>å±€æ‰€è§£ã®å¤šæ§˜æ€§</td>
</tr>
</tbody>
</table>

<h3>ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã®å¤šæ§˜åŒ–</h3>

<pre><code class="language-python">import pandas as pd
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.decomposition import PCA

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã®Kaggleã§ã¯ã‚³ãƒ³ãƒšã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
df = pd.DataFrame(X_train, columns=[f'feat_{i}' for i in range(X_train.shape[1])])
df['target'] = y_train

# ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ1: å…ƒã®ç‰¹å¾´é‡ + çµ±è¨ˆé‡
def create_feature_set1(X):
    features = pd.DataFrame(X)
    features['mean'] = X.mean(axis=1)
    features['std'] = X.std(axis=1)
    features['max'] = X.max(axis=1)
    features['min'] = X.min(axis=1)
    return features.values

# ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ2: å¤šé …å¼ç‰¹å¾´é‡
def create_feature_set2(X):
    poly = PolynomialFeatures(degree=2, include_bias=False)
    return poly.fit_transform(X[:, :5])  # è¨ˆç®—é‡å‰Šæ¸›ã®ãŸã‚æœ€åˆã®5ç‰¹å¾´é‡ã®ã¿

# ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ3: PCAç‰¹å¾´é‡
def create_feature_set3(X):
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    pca = PCA(n_components=10)
    return pca.fit_transform(X_scaled)

# å„ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
feature_sets = [
    ('original', lambda X: X),
    ('statistics', create_feature_set1),
    ('polynomial', create_feature_set2),
    ('pca', create_feature_set3)
]

ensemble_predictions = []

for name, feature_func in feature_sets:
    print(f"\n=== Training with {name} features ===")

    X_train_feat = feature_func(X_train)
    X_test_feat = feature_func(X_test)

    # XGBoostã§è¨“ç·´
    model = XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')
    model.fit(X_train_feat, y_train)

    pred_proba = model.predict_proba(X_test_feat)[:, 1]
    ensemble_predictions.append(pred_proba)

    print(f"AUC: {roc_auc_score(y_test, pred_proba):.4f}")

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ï¼ˆå˜ç´”å¹³å‡ï¼‰
ensemble_avg = np.mean(ensemble_predictions, axis=0)
print(f"\n=== Ensemble with Diverse Features ===")
print(f"Ensemble AUC: {roc_auc_score(y_test, ensemble_avg):.4f}")
</code></pre>

<h3>Kaggleæˆ¦ç•¥ã®ãƒ•ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h3>

<pre><code class="language-python">from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, roc_auc_score
import warnings
warnings.filterwarnings('ignore')

class KaggleEnsemble:
    """Kaggleã‚³ãƒ³ãƒšç”¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(self, n_folds=5, random_state=42):
        self.n_folds = n_folds
        self.random_state = random_state
        self.base_models = []
        self.meta_model = None

    def add_base_model(self, name, model):
        """ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ """
        self.base_models.append((name, model))

    def set_meta_model(self, model):
        """ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š"""
        self.meta_model = model

    def fit(self, X, y):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
        cv = StratifiedKFold(n_splits=self.n_folds, shuffle=True,
                            random_state=self.random_state)

        # ãƒ¡ã‚¿ç‰¹å¾´é‡ã®åˆæœŸåŒ–
        self.meta_features_ = np.zeros((X.shape[0], len(self.base_models)))

        # å„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§OOFäºˆæ¸¬ã‚’ç”Ÿæˆ
        for i, (name, model) in enumerate(self.base_models):
            print(f"Training {name}...")
            oof_predictions = np.zeros(X.shape[0])

            for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):
                X_train_fold, X_val_fold = X[train_idx], X[val_idx]
                y_train_fold, y_val_fold = y[train_idx], y[val_idx]

                model.fit(X_train_fold, y_train_fold)
                oof_predictions[val_idx] = model.predict_proba(X_val_fold)[:, 1]

            self.meta_features_[:, i] = oof_predictions

            # å…¨ãƒ‡ãƒ¼ã‚¿ã§å†è¨“ç·´
            model.fit(X, y)

            # OOFã‚¹ã‚³ã‚¢
            auc = roc_auc_score(y, oof_predictions)
            print(f"  {name} OOF AUC: {auc:.4f}")

        # ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
        print("Training meta-model...")
        self.meta_model.fit(self.meta_features_, y)

        return self

    def predict_proba(self, X):
        """ç¢ºç‡äºˆæ¸¬"""
        # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬
        base_predictions = np.zeros((X.shape[0], len(self.base_models)))
        for i, (name, model) in enumerate(self.base_models):
            base_predictions[:, i] = model.predict_proba(X)[:, 1]

        # ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã§æœ€çµ‚äºˆæ¸¬
        return self.meta_model.predict_proba(base_predictions)

    def predict(self, X):
        """ã‚¯ãƒ©ã‚¹äºˆæ¸¬"""
        return self.meta_model.predict(
            np.column_stack([model.predict_proba(X)[:, 1]
                           for _, model in self.base_models])
        )

# ä½¿ç”¨ä¾‹
ensemble = KaggleEnsemble(n_folds=5, random_state=42)

# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®è¿½åŠ ï¼ˆå¤šæ§˜æ€§ã‚’ç¢ºä¿ï¼‰
ensemble.add_base_model('rf', RandomForestClassifier(
    n_estimators=200, max_depth=10, random_state=42
))
ensemble.add_base_model('xgb', XGBClassifier(
    n_estimators=200, max_depth=6, learning_rate=0.1,
    random_state=42, eval_metric='logloss'
))
ensemble.add_base_model('lgb', LGBMClassifier(
    n_estimators=200, max_depth=8, learning_rate=0.05,
    random_state=42, verbose=-1
))

# ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š
ensemble.set_meta_model(LogisticRegression(random_state=42))

# è¨“ç·´
print("=== Training Kaggle Ensemble ===\n")
ensemble.fit(X_train, y_train)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
y_pred_proba = ensemble.predict_proba(X_test)[:, 1]
y_pred = ensemble.predict(X_test)

print(f"\n=== Final Ensemble Performance ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")
</code></pre>

<hr>

<h2>4.4 å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼šã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ç”¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h2>

<div class="project-box">
<h3>ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦</h3>
<p>äºŒå€¤åˆ†é¡ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’æƒ³å®šã—ãŸã€å®Œå…¨ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚</p>

<h4>è¦ä»¶</h4>
<ul>
<li>è¤‡æ•°ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æ‰‹æ³•</li>
<li>ç•°ãªã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆ5ç¨®é¡ä»¥ä¸Šï¼‰</li>
<li>2å±¤ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°æ§‹é€ </li>
<li>äº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«é¸æŠ</li>
<li>äºˆæ¸¬çµæœã®æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ</li>
</ul>
</div>

<pre><code class="language-python">from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.neural_network import MLPClassifier
import pandas as pd

class CompetitionPipeline:
    """ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ç”¨å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(self, n_folds=5):
        self.n_folds = n_folds
        self.level1_models = self._create_level1_models()
        self.level2_models = self._create_level2_models()
        self.final_model = LogisticRegression(C=0.1, random_state=42)

    def _create_level1_models(self):
        """ãƒ¬ãƒ™ãƒ«1ï¼šå¤šæ§˜ãªãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«"""
        return [
            ('rf', RandomForestClassifier(
                n_estimators=300, max_depth=12, min_samples_split=5,
                random_state=42, n_jobs=-1
            )),
            ('et', ExtraTreesClassifier(
                n_estimators=300, max_depth=12, min_samples_split=5,
                random_state=43, n_jobs=-1
            )),
            ('xgb1', XGBClassifier(
                n_estimators=300, max_depth=6, learning_rate=0.05,
                subsample=0.8, colsample_bytree=0.8,
                random_state=42, eval_metric='logloss', n_jobs=-1
            )),
            ('xgb2', XGBClassifier(
                n_estimators=300, max_depth=8, learning_rate=0.03,
                subsample=0.7, colsample_bytree=0.7,
                random_state=43, eval_metric='logloss', n_jobs=-1
            )),
            ('lgb1', LGBMClassifier(
                n_estimators=300, max_depth=8, learning_rate=0.05,
                subsample=0.8, colsample_bytree=0.8,
                random_state=42, verbose=-1, n_jobs=-1
            )),
            ('lgb2', LGBMClassifier(
                n_estimators=300, max_depth=10, learning_rate=0.03,
                subsample=0.7, colsample_bytree=0.7,
                random_state=43, verbose=-1, n_jobs=-1
            ))
        ]

    def _create_level2_models(self):
        """ãƒ¬ãƒ™ãƒ«2ï¼šãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«"""
        return [
            ('lr', LogisticRegression(C=1.0, random_state=42)),
            ('gb', GradientBoostingClassifier(
                n_estimators=100, max_depth=3, learning_rate=0.1,
                random_state=42
            )),
            ('mlp', MLPClassifier(
                hidden_layer_sizes=(50, 20), max_iter=1000,
                random_state=42, early_stopping=True
            ))
        ]

    def fit(self, X, y, X_test=None):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®è¨“ç·´"""
        cv = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=42)

        # ãƒ¬ãƒ™ãƒ«1ã®è¨“ç·´ã¨OOFäºˆæ¸¬
        print("=" * 60)
        print("LEVEL 1: Training Base Models")
        print("=" * 60)

        oof_l1 = np.zeros((X.shape[0], len(self.level1_models)))
        test_l1 = np.zeros((X_test.shape[0] if X_test is not None else 0,
                          len(self.level1_models)))

        for i, (name, model) in enumerate(self.level1_models):
            print(f"\nModel {i+1}/{len(self.level1_models)}: {name}")
            oof_preds = np.zeros(X.shape[0])
            test_preds = np.zeros(X_test.shape[0] if X_test is not None else 0)

            for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]

                model.fit(X_train, y_train)
                oof_preds[val_idx] = model.predict_proba(X_val)[:, 1]

                if X_test is not None:
                    test_preds += model.predict_proba(X_test)[:, 1] / self.n_folds

                fold_auc = roc_auc_score(y_val, model.predict_proba(X_val)[:, 1])
                print(f"  Fold {fold+1}: AUC = {fold_auc:.4f}")

            oof_l1[:, i] = oof_preds
            if X_test is not None:
                test_l1[:, i] = test_preds

            oof_auc = roc_auc_score(y, oof_preds)
            print(f"  OOF AUC: {oof_auc:.4f}")

        # ãƒ¬ãƒ™ãƒ«2ã®è¨“ç·´
        print("\n" + "=" * 60)
        print("LEVEL 2: Training Meta Models")
        print("=" * 60)

        oof_l2 = np.zeros((X.shape[0], len(self.level2_models)))
        test_l2 = np.zeros((X_test.shape[0] if X_test is not None else 0,
                          len(self.level2_models)))

        for i, (name, model) in enumerate(self.level2_models):
            print(f"\nMeta Model {i+1}/{len(self.level2_models)}: {name}")
            oof_preds = np.zeros(X.shape[0])
            test_preds = np.zeros(X_test.shape[0] if X_test is not None else 0)

            for fold, (train_idx, val_idx) in enumerate(cv.split(oof_l1, y)):
                X_train, X_val = oof_l1[train_idx], oof_l1[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]

                model.fit(X_train, y_train)
                oof_preds[val_idx] = model.predict_proba(X_val)[:, 1]

                if X_test is not None:
                    test_preds += model.predict_proba(test_l1)[:, 1] / self.n_folds

            oof_l2[:, i] = oof_preds
            if X_test is not None:
                test_l2[:, i] = test_preds

            oof_auc = roc_auc_score(y, oof_preds)
            print(f"  OOF AUC: {oof_auc:.4f}")

        # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
        print("\n" + "=" * 60)
        print("FINAL: Training Ensemble Model")
        print("=" * 60)

        self.final_model.fit(oof_l2, y)
        oof_final = self.final_model.predict_proba(oof_l2)[:, 1]
        final_auc = roc_auc_score(y, oof_final)

        print(f"\nFinal Ensemble OOF AUC: {final_auc:.4f}")

        if X_test is not None:
            self.test_predictions_ = self.final_model.predict_proba(test_l2)[:, 1]

        return self

    def predict_proba(self, X):
        """ç¢ºç‡äºˆæ¸¬ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰"""
        return self.test_predictions_

# å®Ÿè¡Œä¾‹
print("Initializing Competition Pipeline...")
pipeline = CompetitionPipeline(n_folds=5)

print("\nTraining pipeline...")
pipeline.fit(X_train, y_train, X_test)

# æœ€çµ‚äºˆæ¸¬
final_predictions = pipeline.predict_proba(X_test)
final_auc = roc_auc_score(y_test, final_predictions)

print("\n" + "=" * 60)
print(f"FINAL TEST AUC: {final_auc:.4f}")
print("=" * 60)

# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆï¼ˆå®Ÿéš›ã®Kaggleã§ã¯é©åˆ‡ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›´ï¼‰
submission = pd.DataFrame({
    'id': range(len(final_predictions)),
    'prediction': final_predictions
})
print("\nSubmission file preview:")
print(submission.head())
</code></pre>

<hr>

<h2>4.5 ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°</h2>

<h3>ã‚ˆãã‚ã‚‹å¤±æ•—ã¨ãã®å¯¾ç­–</h3>

<table>
<thead>
<tr>
<th>å•é¡Œ</th>
<th>åŸå› </th>
<th>å¯¾ç­–</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>OOFäºˆæ¸¬ãŒè¨“ç·´ã«ä½¿ã‚ã‚Œã‚‹</strong></td>
<td>ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã§éå­¦ç¿’</td>
<td>å¿…ãšäº¤å·®æ¤œè¨¼ã§OOFäºˆæ¸¬ã‚’ç”Ÿæˆ</td>
</tr>
<tr>
<td><strong>é¡ä¼¼ãƒ¢ãƒ‡ãƒ«ã®ã¿</strong></td>
<td>å¤šæ§˜æ€§ä¸è¶³</td>
<td>ç•°ãªã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒ»ç‰¹å¾´é‡ã‚’ä½¿ç”¨</td>
</tr>
<tr>
<td><strong>ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãŒå€‹åˆ¥ã‚ˆã‚Šæ‚ªã„</strong></td>
<td>å¼±ã„ãƒ¢ãƒ‡ãƒ«ãŒæ‚ªå½±éŸ¿</td>
<td>ãƒ¢ãƒ‡ãƒ«é¸æŠãƒ»é‡ã¿ä»˜ã‘ã‚’å°å…¥</td>
</tr>
<tr>
<td><strong>éåº¦ã«è¤‡é›‘ãªæ§‹é€ </strong></td>
<td>ã‚ªãƒ¼ãƒãƒ¼ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ãªæ§‹é€ ã‹ã‚‰å§‹ã‚ã‚‹</td>
</tr>
<tr>
<td><strong>è¨ˆç®—æ™‚é–“ãŒé•·ã™ãã‚‹</strong></td>
<td>éåŠ¹ç‡ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</td>
<td>ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ»ä¸¦åˆ—åŒ–ã‚’æ¤œè¨</td>
</tr>
</tbody>
</table>

<h3>æœ¬ç•ªç’°å¢ƒã§ã®é‹ç”¨Tips</h3>

<details>
<summary>ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†</summary>

<ul>
<li>å„ãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã‚’ä»˜ä¸</li>
<li>è¨“ç·´æ™‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨˜éŒ²</li>
<li>OOFã‚¹ã‚³ã‚¢ã¨å®Ÿé‹ç”¨ã‚¹ã‚³ã‚¢ã‚’è¿½è·¡</li>
<li>ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€ç·’ã«ä¿å­˜</li>
</ul>

<pre><code class="language-python">import pickle
import json
from datetime import datetime

# ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
model_metadata = {
    'version': '1.0.0',
    'timestamp': datetime.now().isoformat(),
    'oof_auc': 0.8542,
    'base_models': ['rf', 'xgb', 'lgb'],
    'hyperparameters': {
        'n_folds': 5,
        'random_state': 42
    }
}

with open('ensemble_model.pkl', 'wb') as f:
    pickle.dump(ensemble, f)

with open('ensemble_metadata.json', 'w') as f:
    json.dump(model_metadata, f, indent=2)
</code></pre>
</details>

<details>
<summary>æ¨è«–é€Ÿåº¦ã®æœ€é©åŒ–</summary>

<ul>
<li>ä¸è¦ãªãƒ¢ãƒ‡ãƒ«ã‚’å‰Šé™¤ï¼ˆè²¢çŒ®åº¦ãŒä½ã„ã‚‚ã®ï¼‰</li>
<li>ãƒ¢ãƒ‡ãƒ«æ•°ã‚’æ¸›ã‚‰ã™ï¼ˆTop-Kãƒ¢ãƒ‡ãƒ«ã®ã¿ï¼‰</li>
<li>è»½é‡ãªãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼‰</li>
<li>äºˆæ¸¬ã‚’äº‹å‰è¨ˆç®—ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŒ–</li>
</ul>
</details>

<details>
<summary>ãƒ‡ãƒãƒƒã‚°æ‰‹æ³•</summary>

<ul>
<li>å„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å€‹åˆ¥æ€§èƒ½ã‚’ç¢ºèª</li>
<li>ãƒ¢ãƒ‡ãƒ«é–“ã®ç›¸é–¢ã‚’åˆ†æï¼ˆé«˜ç›¸é–¢ãªã‚‰å¤šæ§˜æ€§ä¸è¶³ï¼‰</li>
<li>ãƒ¡ã‚¿ç‰¹å¾´é‡ã®åˆ†å¸ƒã‚’å¯è¦–åŒ–</li>
<li>CV foldã”ã¨ã®ã‚¹ã‚³ã‚¢å¤‰å‹•ã‚’ç¢ºèª</li>
</ul>

<pre><code class="language-python"># ãƒ¢ãƒ‡ãƒ«é–“ã®ç›¸é–¢ã‚’ç¢ºèª
import seaborn as sns
import matplotlib.pyplot as plt

correlation_matrix = np.corrcoef(meta_features_train.T)

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt='.3f',
            xticklabels=[name for name, _ in base_models],
            yticklabels=[name for name, _ in base_models],
            cmap='coolwarm', center=0)
plt.title('Base Model Predictions Correlation')
plt.tight_layout()
plt.show()

# é«˜ç›¸é–¢ï¼ˆ>0.95ï¼‰ã®ãƒšã‚¢ã‚’è¡¨ç¤º
print("High correlation pairs (>0.95):")
for i in range(len(base_models)):
    for j in range(i+1, len(base_models)):
        if correlation_matrix[i, j] > 0.95:
            print(f"{base_models[i][0]} - {base_models[j][0]}: {correlation_matrix[i, j]:.3f}")
</code></pre>
</details>

<h3>ã¾ã¨ã‚</h3>

<blockquote>
<p><strong>ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®æˆåŠŸã®éµ</strong></p>
<ul>
<li><strong>å¤šæ§˜æ€§</strong>: ç•°ãªã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒ»ç‰¹å¾´é‡ãƒ»ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’çµ„ã¿åˆã‚ã›ã‚‹</li>
<li><strong>é©åˆ‡ãªæ¤œè¨¼</strong>: äº¤å·®æ¤œè¨¼ã§OOFäºˆæ¸¬ã‚’ç”Ÿæˆã—ã€ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’é˜²ã</li>
<li><strong>æ®µéšçš„æ§‹ç¯‰</strong>: ã‚·ãƒ³ãƒ—ãƒ«ãªæ§‹é€ ã‹ã‚‰å§‹ã‚ã€å¿…è¦ã«å¿œã˜ã¦è¤‡é›‘åŒ–ã™ã‚‹</li>
<li><strong>æ€§èƒ½ç›£è¦–</strong>: å„ãƒ¢ãƒ‡ãƒ«ã®è²¢çŒ®åº¦ã‚’è¿½è·¡ã—ã€ä¸è¦ãªãƒ¢ãƒ‡ãƒ«ã‚’å‰Šé™¤ã™ã‚‹</li>
<li><strong>å®Ÿé‹ç”¨ã‚’æ„è­˜</strong>: è¨ˆç®—ã‚³ã‚¹ãƒˆã¨äºˆæ¸¬ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹</li>
</ul>
</blockquote>

<hr>

<h2>ç·´ç¿’å•é¡Œ</h2>

<details>
<summary>å•é¡Œ1ï¼š2å±¤ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã®å®Ÿè£…ï¼ˆé›£æ˜“åº¦ï¼šä¸­ï¼‰</summary>

<p>ä»¥ä¸‹ã®è¦ä»¶ã§2å±¤ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š</p>
<ul>
<li>ãƒ¬ãƒ™ãƒ«1: Random Forestã€XGBoostã€LightGBM</li>
<li>ãƒ¬ãƒ™ãƒ«2: Logistic Regression</li>
<li>5-foldäº¤å·®æ¤œè¨¼ã§OOFäºˆæ¸¬ã‚’ç”Ÿæˆ</li>
<li>å„ãƒ¬ãƒ™ãƒ«ã®OOF AUCã‚’è¡¨ç¤º</li>
</ul>

<pre><code class="language-python"># ãƒ’ãƒ³ãƒˆ
from sklearn.model_selection import cross_val_predict

# ãƒ¬ãƒ™ãƒ«1ã®OOFäºˆæ¸¬ã‚’ç”Ÿæˆ
# meta_features_l1 = ...

# ãƒ¬ãƒ™ãƒ«2ã®OOFäºˆæ¸¬ã‚’ç”Ÿæˆ
# meta_features_l2 = ...
</code></pre>
</details>

<details>
<summary>å•é¡Œ2ï¼šæœ€é©ãªãƒ–ãƒ¬ãƒ³ãƒ‰é‡ã¿ã®æ¢ç´¢ï¼ˆé›£æ˜“åº¦ï¼šä¸­ï¼‰</summary>

<p>3ã¤ã®ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã«å¯¾ã—ã¦ã€AUCã‚’æœ€å¤§åŒ–ã™ã‚‹æœ€é©ãªé‡ã¿ã‚’æ¢ç´¢ã—ã¦ãã ã•ã„ã€‚scipy.optimize.minimizeã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚</p>

<pre><code class="language-python"># ãƒ’ãƒ³ãƒˆ
from scipy.optimize import minimize

def objective(weights):
    # é‡ã¿ä»˜ãå¹³å‡ã®äºˆæ¸¬
    blended = ...
    # AUCã‚’æœ€å¤§åŒ– â†’ è² ã®AUCã‚’æœ€å°åŒ–
    return -roc_auc_score(y_true, blended)
</code></pre>
</details>

<details>
<summary>å•é¡Œ3ï¼šãƒ¢ãƒ‡ãƒ«å¤šæ§˜æ€§ã®åˆ†æï¼ˆé›£æ˜“åº¦ï¼šä½ï¼‰</summary>

<p>5ã¤ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬å€¤ã®ç›¸é–¢è¡Œåˆ—ã‚’è¨ˆç®—ã—ã€ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–ã—ã¦ãã ã•ã„ã€‚ç›¸é–¢ãŒ0.95ä»¥ä¸Šã®ãƒšã‚¢ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚</p>
</details>

<details>
<summary>å•é¡Œ4ï¼šã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ç”¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆé›£æ˜“åº¦ï¼šé«˜ï¼‰</summary>

<p>ä»¥ä¸‹ã‚’å«ã‚€å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ã¦ãã ã•ã„ï¼š</p>
<ul>
<li>æœ€ä½5ç¨®é¡ã®ç•°ãªã‚‹ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«</li>
<li>2ç¨®é¡ã®ç‰¹å¾´é‡ã‚»ãƒƒãƒˆï¼ˆå…ƒã®ç‰¹å¾´é‡ã¨åŠ å·¥å¾Œã®ç‰¹å¾´é‡ï¼‰</li>
<li>ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã¾ãŸã¯ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°</li>
<li>äº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹OOFã‚¹ã‚³ã‚¢è¨ˆç®—</li>
<li>æå‡ºç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆï¼ˆCSVå½¢å¼ï¼‰</li>
</ul>
</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®ãƒ»ãƒªã‚½ãƒ¼ã‚¹</h2>

<h3>è«–æ–‡ãƒ»æ›¸ç±</h3>
<ul>
<li>Wolpert, D. H. (1992). "Stacked generalization." Neural Networks, 5(2), 241-259.</li>
<li>Breiman, L. (1996). "Stacked regressions." Machine Learning, 24(1), 49-64.</li>
<li>Kaggle Ensembling Guide: <a href="https://mlwave.com/kaggle-ensembling-guide/">https://mlwave.com/kaggle-ensembling-guide/</a></li>
</ul>

<h3>å®Ÿè£…ãƒªã‚½ãƒ¼ã‚¹</h3>
<ul>
<li><a href="http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/">mlxtend.classifier.StackingCVClassifier</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html">sklearn.ensemble.StackingClassifier</a></li>
<li><a href="https://github.com/vecxoz/vecstack">vecstack: Stacking library</a></li>
</ul>

<h3>Kaggleã‚«ãƒ¼ãƒãƒ«</h3>
<ul>
<li><a href="https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python">Introduction to Ensembling/Stacking in Python</a></li>
<li><a href="https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard">Stacked Regressions: Top 4% on Leaderboard</a></li>
</ul>

<hr>

<div class="navigation">
    <a href="chapter3-voting-averaging.html" class="nav-button">â† ç¬¬3ç« ï¼šæŠ•ç¥¨ã¨ã‚¢ãƒ™ãƒ¬ãƒ¼ã‚¸ãƒ³ã‚°</a>
    <a href="../index.html" class="nav-button">ã‚³ãƒ¼ã‚¹ç›®æ¬¡ã¸ â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p>&copy; 2024 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
