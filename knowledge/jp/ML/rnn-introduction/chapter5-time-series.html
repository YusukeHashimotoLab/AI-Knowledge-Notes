<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬5ç« ï¼šæ™‚ç³»åˆ—äºˆæ¸¬å…¥é–€ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
        <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/wp/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/ML/rnn-introduction/index.html">Rnn</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 5</span>
        </div>
    </nav>

    <header>
        <div class="header-content">
            <h1>ç¬¬5ç« ï¼šæ™‚ç³»åˆ—äºˆæ¸¬å…¥é–€</h1>
            <p class="subtitle">RNNã«ã‚ˆã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®è§£æã¨æœªæ¥äºˆæ¸¬ - æ ªä¾¡ãƒ»æ°—è±¡ãƒ»éœ€è¦äºˆæ¸¬</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 4å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã‚’ç†è§£ã—ã€é©åˆ‡ãªå‰å‡¦ç†ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Windowï¼ˆã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼‰ã®æ¦‚å¿µã‚’ç†è§£ã—ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã§ãã‚‹</li>
<li>âœ… LSTMã¨GRUã‚’ç”¨ã„ãŸå˜å¤‰é‡ãƒ»å¤šå¤‰é‡æ™‚ç³»åˆ—äºˆæ¸¬ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Seq2Seqãƒ¢ãƒ‡ãƒ«ã§è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—å…ˆã®äºˆæ¸¬ã‚’è¡Œãˆã‚‹</li>
<li>âœ… MAEã€RMSEã€MAPEãªã©ã®è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—ã—è§£é‡ˆã§ãã‚‹</li>
<li>âœ… å®Ÿè·µçš„ãªæ ªä¾¡äºˆæ¸¬ãƒ»æ°—è±¡äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
</ul>

<hr>

<h2>5.1 æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´</h2>

<h3>æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã¨ã¯</h3>

<p><strong>æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼ˆTime Series Dataï¼‰</strong>ã¯ã€æ™‚é–“è»¸ã«æ²¿ã£ã¦è¨˜éŒ²ã•ã‚ŒãŸä¸€é€£ã®è¦³æ¸¬å€¤ã§ã™ã€‚éå»ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æœªæ¥ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ãŒä¸»ãªç›®çš„ã¨ãªã‚Šã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ç¨®é¡] --> B[å˜å¤‰é‡<br/>Univariate]
    A --> C[å¤šå¤‰é‡<br/>Multivariate]

    B --> B1["1ã¤ã®å¤‰æ•°ã®ã¿<br/>ä¾‹: æ ªä¾¡ã®çµ‚å€¤"]
    C --> C1["è¤‡æ•°ã®å¤‰æ•°<br/>ä¾‹: æ ªä¾¡+å‡ºæ¥é«˜+æŒ‡æ¨™"]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#e8f5e9
</div>

<table>
<thead>
<tr>
<th>ç‰¹å¾´</th>
<th>èª¬æ˜</th>
<th>å¿œç”¨ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆTrendï¼‰</strong></td>
<td>é•·æœŸçš„ãªå¢—åŠ ãƒ»æ¸›å°‘å‚¾å‘</td>
<td>çµŒæ¸ˆæˆé•·ã€äººå£å¢—åŠ </td>
</tr>
<tr>
<td><strong>å­£ç¯€æ€§ï¼ˆSeasonalityï¼‰</strong></td>
<td>å‘¨æœŸçš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ—¥æ¬¡ã€æœˆæ¬¡ã€å¹´æ¬¡ï¼‰</td>
<td>æ°—æ¸©ã®å­£ç¯€å¤‰å‹•ã€å°å£²ã®ç¹å¿™æœŸ</td>
</tr>
<tr>
<td><strong>å‘¨æœŸæ€§ï¼ˆCyclicityï¼‰</strong></td>
<td>ä¸è¦å‰‡ãªå‘¨æœŸã®ãƒ‘ã‚¿ãƒ¼ãƒ³</td>
<td>æ™¯æ°—å¾ªç’°ã€ãƒ“ã‚¸ãƒã‚¹ã‚µã‚¤ã‚¯ãƒ«</td>
</tr>
<tr>
<td><strong>ãƒã‚¤ã‚ºï¼ˆNoiseï¼‰</strong></td>
<td>ãƒ©ãƒ³ãƒ€ãƒ ãªå¤‰å‹•</td>
<td>æ¸¬å®šèª¤å·®ã€äºˆæ¸¬ä¸å¯èƒ½ãªäº‹è±¡</td>
</tr>
</tbody>
</table>

<h3>æ™‚ç³»åˆ—äºˆæ¸¬ã®èª²é¡Œ</h3>

<h4>ãƒ‡ãƒ¼ã‚¿ã®ä¾å­˜æ€§</h4>

<p>æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã¯æ™‚é–“çš„ãª<strong>è‡ªå·±ç›¸é–¢ï¼ˆAutocorrelationï¼‰</strong>ã‚’æŒã¡ã¾ã™ã€‚éå»ã®å€¤ãŒæœªæ¥ã®å€¤ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã¯ç‹¬ç«‹ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</p>

<p>$$
\text{Autocorrelation}(k) = \frac{\sum_{t=1}^{N-k}(x_t - \bar{x})(x_{t+k} - \bar{x})}{\sum_{t=1}^{N}(x_t - \bar{x})^2}
$$</p>

<h4>éå®šå¸¸æ€§ï¼ˆNon-Stationarityï¼‰</h4>

<p>å¤šãã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã¯çµ±è¨ˆçš„æ€§è³ªï¼ˆå¹³å‡ã€åˆ†æ•£ï¼‰ãŒæ™‚é–“ã¨å…±ã«å¤‰åŒ–ã™ã‚‹<strong>éå®šå¸¸</strong>ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã¯é€šå¸¸ã€å®šå¸¸æ€§ã‚’ä»®å®šã™ã‚‹ãŸã‚ã€å‰å‡¦ç†ãŒå¿…è¦ã§ã™ã€‚</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import adfuller

def check_stationarity(timeseries, title='Time Series'):
    """
    æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å®šå¸¸æ€§ã‚’ãƒã‚§ãƒƒã‚¯

    Args:
        timeseries: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼ˆpandas Seriesï¼‰
        title: ã‚°ãƒ©ãƒ•ã®ã‚¿ã‚¤ãƒˆãƒ«
    """
    # ãƒ­ãƒ¼ãƒªãƒ³ã‚°çµ±è¨ˆé‡ã®è¨ˆç®—
    rolling_mean = timeseries.rolling(window=12).mean()
    rolling_std = timeseries.rolling(window=12).std()

    # å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(12, 6))
    ax.plot(timeseries, color='blue', label='Original')
    ax.plot(rolling_mean, color='red', label='Rolling Mean (12 periods)')
    ax.plot(rolling_std, color='black', label='Rolling Std (12 periods)')
    ax.legend(loc='best')
    ax.set_title(f'{title} - Rolling Mean & Standard Deviation')
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

    # Augmented Dickey-Fulleræ¤œå®šï¼ˆå®šå¸¸æ€§ã®çµ±è¨ˆçš„æ¤œå®šï¼‰
    print(f'\n{title} - ADF Test Results:')
    adf_result = adfuller(timeseries.dropna(), autolag='AIC')

    print(f'ADF Statistic: {adf_result[0]:.6f}')
    print(f'p-value: {adf_result[1]:.6f}')
    print(f'Critical Values:')
    for key, value in adf_result[4].items():
        print(f'  {key}: {value:.3f}')

    # åˆ¤å®š
    if adf_result[1] <= 0.05:
        print('çµè«–: ãƒ‡ãƒ¼ã‚¿ã¯å®šå¸¸ï¼ˆstationaryï¼‰ã§ã™ï¼ˆp-value â‰¤ 0.05ï¼‰')
    else:
        print('çµè«–: ãƒ‡ãƒ¼ã‚¿ã¯éå®šå¸¸ï¼ˆnon-stationaryï¼‰ã§ã™ï¼ˆp-value > 0.05ï¼‰')
        print('      â†’ å·®åˆ†å¤‰æ›ã‚„å¯¾æ•°å¤‰æ›ã‚’æ¤œè¨ã—ã¦ãã ã•ã„')

# ä½¿ç”¨ä¾‹: ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ï¼ˆéå®šå¸¸ï¼‰
np.random.seed(42)
random_walk = np.cumsum(np.random.randn(200))
ts_nonstationary = pd.Series(random_walk, index=pd.date_range('2020-01-01', periods=200))

# å®šå¸¸ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ›ãƒ¯ã‚¤ãƒˆãƒã‚¤ã‚ºï¼‰
ts_stationary = pd.Series(np.random.randn(200), index=pd.date_range('2020-01-01', periods=200))

# æ¤œå®šå®Ÿè¡Œ
check_stationarity(ts_nonstationary, title='Non-Stationary Data (Random Walk)')
check_stationarity(ts_stationary, title='Stationary Data (White Noise)')
</code></pre>

<blockquote>
<p><strong>å®šå¸¸åŒ–ã®æ‰‹æ³•</strong>:</p>
<ul>
<li><strong>å·®åˆ†å¤‰æ›ï¼ˆDifferencingï¼‰</strong>: $x'_t = x_t - x_{t-1}$ ã§ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’é™¤å»</li>
<li><strong>å¯¾æ•°å¤‰æ›ï¼ˆLog Transformï¼‰</strong>: $x'_t = \log(x_t)$ ã§åˆ†æ•£ã‚’å®‰å®šåŒ–</li>
<li><strong>ç§»å‹•å¹³å‡ï¼ˆMoving Averageï¼‰</strong>: å¹³æ»‘åŒ–ã«ã‚ˆã‚Šå­£ç¯€æ€§ã‚’é™¤å»</li>
</ul>
</blockquote>

<hr>

<h2>5.2 ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¨Windowä½œæˆ</h2>

<h3>æ­£è¦åŒ–ï¼ˆNormalizationï¼‰</h3>

<p>æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã§ã¯ã€<strong>Min-Maxã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°</strong>ã‚„<strong>æ¨™æº–åŒ–</strong>ãŒä¸€èˆ¬çš„ã§ã™ã€‚é‡è¦ãªã®ã¯ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆé‡ã®ã¿ã‚’ä½¿ç”¨ã—ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ãƒªãƒ¼ã‚¯ã•ã›ãªã„ã“ã¨ã§ã™ã€‚</p>

<pre><code class="language-python">import numpy as np
import torch
from sklearn.preprocessing import MinMaxScaler, StandardScaler

class TimeSeriesNormalizer:
    """
    æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–ã‚¯ãƒ©ã‚¹
    """

    def __init__(self, method='minmax'):
        """
        Args:
            method: 'minmax' ã¾ãŸã¯ 'standard'
        """
        self.method = method

        if method == 'minmax':
            self.scaler = MinMaxScaler(feature_range=(0, 1))
        elif method == 'standard':
            self.scaler = StandardScaler()
        else:
            raise ValueError("method must be 'minmax' or 'standard'")

    def fit_transform(self, data):
        """
        è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã—ã€å¤‰æ›

        Args:
            data: numpy array of shape [N, features]

        Returns:
            normalized_data: æ­£è¦åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        """
        # 2æ¬¡å…ƒé…åˆ—ã«å¤‰æ›ï¼ˆå¿…è¦ãªå ´åˆï¼‰
        if len(data.shape) == 1:
            data = data.reshape(-1, 1)

        normalized_data = self.scaler.fit_transform(data)

        return normalized_data

    def transform(self, data):
        """
        å­¦ç¿’æ¸ˆã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¤‰æ›ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰

        Args:
            data: numpy array of shape [N, features]

        Returns:
            normalized_data: æ­£è¦åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        """
        if len(data.shape) == 1:
            data = data.reshape(-1, 1)

        normalized_data = self.scaler.transform(data)

        return normalized_data

    def inverse_transform(self, data):
        """
        æ­£è¦åŒ–ã‚’å…ƒã«æˆ»ã™ï¼ˆäºˆæ¸¬å€¤ã®å¾©å…ƒç”¨ï¼‰

        Args:
            data: æ­£è¦åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿

        Returns:
            original_scale_data: å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã—ãŸãƒ‡ãƒ¼ã‚¿
        """
        if len(data.shape) == 1:
            data = data.reshape(-1, 1)

        original_scale_data = self.scaler.inverse_transform(data)

        return original_scale_data

# ä½¿ç”¨ä¾‹
np.random.seed(42)
stock_prices = np.cumsum(np.random.randn(1000)) + 100  # ç–‘ä¼¼æ ªä¾¡ãƒ‡ãƒ¼ã‚¿

# Min-Maxæ­£è¦åŒ–
normalizer_minmax = TimeSeriesNormalizer(method='minmax')
normalized_minmax = normalizer_minmax.fit_transform(stock_prices)

print("Min-Maxæ­£è¦åŒ–:")
print(f"  å…ƒã®ãƒ‡ãƒ¼ã‚¿ç¯„å›²: [{stock_prices.min():.2f}, {stock_prices.max():.2f}]")
print(f"  æ­£è¦åŒ–å¾Œã®ç¯„å›²: [{normalized_minmax.min():.2f}, {normalized_minmax.max():.2f}]")

# æ¨™æº–åŒ–
normalizer_std = TimeSeriesNormalizer(method='standard')
normalized_std = normalizer_std.fit_transform(stock_prices)

print("\næ¨™æº–åŒ–:")
print(f"  å…ƒã®ãƒ‡ãƒ¼ã‚¿: å¹³å‡={stock_prices.mean():.2f}, æ¨™æº–åå·®={stock_prices.std():.2f}")
print(f"  æ­£è¦åŒ–å¾Œ: å¹³å‡={normalized_std.mean():.4f}, æ¨™æº–åå·®={normalized_std.std():.4f}")
</code></pre>

<h3>ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆSliding Windowï¼‰</h3>

<p><strong>ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦</strong>ã¯ã€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼ˆéå»ã®ãƒ‡ãƒ¼ã‚¿ï¼‰ã¨ç›®æ¨™å€¤ï¼ˆæœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ï¼‰ã®ãƒšã‚¢ã«åˆ†å‰²ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[å…ƒã®æ™‚ç³»åˆ—<br/>x1, x2, ..., xN] --> B[Window 1<br/>å…¥åŠ›: x1~x10<br/>ç›®æ¨™: x11]
    A --> C[Window 2<br/>å…¥åŠ›: x2~x11<br/>ç›®æ¨™: x12]
    A --> D[Window 3<br/>å…¥åŠ›: x3~x12<br/>ç›®æ¨™: x13]
    A --> E[...]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#fff3e0
    style D fill:#fff3e0
</div>

<pre><code class="language-python">def create_sliding_windows(data, window_size, horizon=1, stride=1):
    """
    ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ

    Args:
        data: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼ˆnumpy arrayï¼‰[N, features]
        window_size: å…¥åŠ›ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºï¼ˆéå»ä½•ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¦‹ã‚‹ã‹ï¼‰
        horizon: äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ï¼ˆä½•ã‚¹ãƒ†ãƒƒãƒ—å…ˆã‚’äºˆæ¸¬ã™ã‚‹ã‹ï¼‰
        stride: ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ã‚¹ãƒ©ã‚¤ãƒ‰å¹…

    Returns:
        X: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ [num_windows, window_size, features]
        y: ç›®æ¨™ãƒ‡ãƒ¼ã‚¿ [num_windows, horizon, features]
    """
    X, y = [], []

    for i in range(0, len(data) - window_size - horizon + 1, stride):
        # å…¥åŠ›ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
        X.append(data[i : i + window_size])

        # ç›®æ¨™å€¤ï¼ˆhorizon ã‚¹ãƒ†ãƒƒãƒ—å…ˆï¼‰
        y.append(data[i + window_size : i + window_size + horizon])

    X = np.array(X)
    y = np.array(y)

    return X, y

# ä½¿ç”¨ä¾‹
# ç–‘ä¼¼æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ï¼ˆæ­£è¦åŒ–æ¸ˆã¿ï¼‰
data = normalized_minmax.reshape(-1, 1)  # [1000, 1]

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
WINDOW_SIZE = 60   # éå»60æ—¥åˆ†ã‚’è¦‹ã‚‹
HORIZON = 1        # 1æ—¥å…ˆã‚’äºˆæ¸¬
STRIDE = 1         # 1æ—¥ã”ã¨ã«ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ã‚¹ãƒ©ã‚¤ãƒ‰

# ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ä½œæˆ
X, y = create_sliding_windows(data, window_size=WINDOW_SIZE, horizon=HORIZON, stride=STRIDE)

print(f"ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†:")
print(f"  å…¥åŠ› X ã®å½¢çŠ¶: {X.shape}")  # [num_windows, 60, 1]
print(f"  ç›®æ¨™ y ã®å½¢çŠ¶: {y.shape}")  # [num_windows, 1, 1]
print(f"  ç·ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ•°: {len(X)}")

# 1ã¤ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’å¯è¦–åŒ–
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(12, 5))
sample_idx = 100

# å…¥åŠ›ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆéå»60æ—¥ï¼‰
ax.plot(range(WINDOW_SIZE), X[sample_idx, :, 0], marker='o', label='Input Window (Past 60 days)')

# ç›®æ¨™å€¤ï¼ˆ1æ—¥å…ˆï¼‰
ax.scatter([WINDOW_SIZE], y[sample_idx, 0, 0], color='red', s=100, zorder=5, label='Target (Next day)', marker='*')

ax.set_xlabel('Time Steps')
ax.set_ylabel('Normalized Price')
ax.set_title('Sliding Window Example')
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<h3>Train/Validation/Teståˆ†å‰²</h3>

<p>æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã¯ã€<strong>æ™‚é–“é †åºã‚’ä¿æŒã—ãŸåˆ†å‰²</strong>ãŒå¿…é ˆã§ã™ã€‚ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã¯ä½¿ç”¨ã—ã¾ã›ã‚“ã€‚</p>

<pre><code class="language-python">def train_val_test_split(X, y, train_ratio=0.7, val_ratio=0.15):
    """
    æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®Train/Val/Teståˆ†å‰²ï¼ˆæ™‚é–“é †åºã‚’ä¿æŒï¼‰

    Args:
        X: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ [num_samples, window_size, features]
        y: ç›®æ¨™ãƒ‡ãƒ¼ã‚¿ [num_samples, horizon, features]
        train_ratio: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ
        val_ratio: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ

    Returns:
        X_train, X_val, X_test, y_train, y_val, y_test
    """
    num_samples = len(X)

    # åˆ†å‰²ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    train_end = int(num_samples * train_ratio)
    val_end = int(num_samples * (train_ratio + val_ratio))

    # åˆ†å‰²
    X_train, y_train = X[:train_end], y[:train_end]
    X_val, y_val = X[train_end:val_end], y[train_end:val_end]
    X_test, y_test = X[val_end:], y[val_end:]

    print(f"ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†:")
    print(f"  Train: {len(X_train)} samples ({train_ratio*100:.0f}%)")
    print(f"  Val:   {len(X_val)} samples ({val_ratio*100:.0f}%)")
    print(f"  Test:  {len(X_test)} samples ({(1-train_ratio-val_ratio)*100:.0f}%)")

    return X_train, X_val, X_test, y_train, y_val, y_test

# åˆ†å‰²å®Ÿè¡Œ
X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X, y, train_ratio=0.7, val_ratio=0.15)

# PyTorch Tensorã«å¤‰æ›
X_train_tensor = torch.FloatTensor(X_train)
y_train_tensor = torch.FloatTensor(y_train)
X_val_tensor = torch.FloatTensor(X_val)
y_val_tensor = torch.FloatTensor(y_val)
X_test_tensor = torch.FloatTensor(X_test)
y_test_tensor = torch.FloatTensor(y_test)

print(f"\nTensorå½¢çŠ¶:")
print(f"  X_train: {X_train_tensor.shape}")
print(f"  y_train: {y_train_tensor.shape}")
</code></pre>

<blockquote>
<p><strong>é‡è¦ãªæ³¨æ„ç‚¹</strong>:</p>
<ul>
<li>æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã¯å¿…ãšæ™‚é–“é †ã«åˆ†å‰²ï¼ˆTrain â†’ Val â†’ Test ã®é †ï¼‰</li>
<li>æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‹ã‚‰è¨ˆç®—</li>
<li>ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯æœ€å¾Œã®è©•ä¾¡ã¾ã§ä¸€åˆ‡ä½¿ç”¨ã—ãªã„</li>
</ul>
</blockquote>

<hr>

<h2>5.3 å˜å¤‰é‡æ™‚ç³»åˆ—äºˆæ¸¬</h2>

<h3>LSTMã«ã‚ˆã‚‹æ ªä¾¡äºˆæ¸¬</h3>

<p><strong>å˜å¤‰é‡æ™‚ç³»åˆ—äºˆæ¸¬</strong>ã¯ã€1ã¤ã®å¤‰æ•°ï¼ˆä¾‹: æ ªä¾¡ã®çµ‚å€¤ï¼‰ã®ã¿ã‚’ä½¿ç”¨ã—ã¦æœªæ¥ã‚’äºˆæ¸¬ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

class LSTMForecaster(nn.Module):
    """
    å˜å¤‰é‡æ™‚ç³»åˆ—äºˆæ¸¬ç”¨LSTMãƒ¢ãƒ‡ãƒ«
    """

    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):
        """
        Args:
            input_size: å…¥åŠ›ç‰¹å¾´æ•°ï¼ˆå˜å¤‰é‡ãªã‚‰1ï¼‰
            hidden_size: LSTMéš ã‚Œå±¤ã®ã‚µã‚¤ã‚º
            num_layers: LSTMãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
            dropout: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        """
        super(LSTMForecaster, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTMå±¤
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )

        # å…¨çµåˆå±¤ï¼ˆå‡ºåŠ›ï¼‰
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        """
        Args:
            x: [batch_size, seq_len, input_size]

        Returns:
            out: [batch_size, 1] - 1ã‚¹ãƒ†ãƒƒãƒ—å…ˆã®äºˆæ¸¬
        """
        # LSTMé †ä¼æ’­
        lstm_out, (h_n, c_n) = self.lstm(x)

        # æœ€å¾Œã®éš ã‚ŒçŠ¶æ…‹ã‚’ä½¿ç”¨
        out = self.fc(lstm_out[:, -1, :])  # [batch_size, 1]

        return out

# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
model = LSTMForecaster(input_size=1, hidden_size=64, num_layers=2, dropout=0.2)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

print("LSTMäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«:")
print(model)
print(f"\nç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}")

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ
train_dataset = TensorDataset(X_train_tensor, y_train_tensor.squeeze(-1))
val_dataset = TensorDataset(X_val_tensor, y_val_tensor.squeeze(-1))

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)  # æ™‚ç³»åˆ—ã¯é€šå¸¸ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ãªã„
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# æå¤±é–¢æ•°ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

def train_epoch(model, data_loader, criterion, optimizer, device):
    """
    1ã‚¨ãƒãƒƒã‚¯ã®è¨“ç·´
    """
    model.train()
    total_loss = 0

    for batch_X, batch_y in data_loader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)

        # é †ä¼æ’­
        predictions = model(batch_X).squeeze()
        loss = criterion(predictions, batch_y)

        # é€†ä¼æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(data_loader)
    return avg_loss

def validate(model, data_loader, criterion, device):
    """
    æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡
    """
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for batch_X, batch_y in data_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)

            predictions = model(batch_X).squeeze()
            loss = criterion(predictions, batch_y)

            total_loss += loss.item()

    avg_loss = total_loss / len(data_loader)
    return avg_loss

# è¨“ç·´ãƒ«ãƒ¼ãƒ—
num_epochs = 50
train_losses = []
val_losses = []

print("\nè¨“ç·´é–‹å§‹:")
for epoch in range(1, num_epochs + 1):
    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
    val_loss = validate(model, val_loader, criterion, device)

    train_losses.append(train_loss)
    val_losses.append(val_loss)

    if epoch % 10 == 0:
        print(f'Epoch {epoch}/{num_epochs} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')

print("\nè¨“ç·´å®Œäº†!")

# æå¤±ã®å¯è¦–åŒ–
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.title('Training History')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<h3>äºˆæ¸¬ã¨å¯è¦–åŒ–</h3>

<pre><code class="language-python">def forecast_and_visualize(model, X_test, y_test, normalizer, device, num_samples=100):
    """
    ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬ã‚’å®Ÿè¡Œã—ã€çµæœã‚’å¯è¦–åŒ–

    Args:
        model: è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        X_test: ãƒ†ã‚¹ãƒˆå…¥åŠ› [num_samples, seq_len, 1]
        y_test: ãƒ†ã‚¹ãƒˆç›®æ¨™ [num_samples, 1, 1]
        normalizer: TimeSeriesNormalizerï¼ˆé€†å¤‰æ›ç”¨ï¼‰
        device: ãƒ‡ãƒã‚¤ã‚¹
        num_samples: å¯è¦–åŒ–ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
    """
    model.eval()

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’Tensorã«å¤‰æ›
    X_test_tensor = torch.FloatTensor(X_test[:num_samples]).to(device)

    # äºˆæ¸¬
    with torch.no_grad():
        predictions = model(X_test_tensor).cpu().numpy()

    # æ­£è¦åŒ–ã‚’å…ƒã«æˆ»ã™
    y_true = normalizer.inverse_transform(y_test[:num_samples].squeeze())
    y_pred = normalizer.inverse_transform(predictions)

    # å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(14, 6))

    time_steps = np.arange(num_samples)
    ax.plot(time_steps, y_true, label='Actual', color='blue', linewidth=2)
    ax.plot(time_steps, y_pred, label='Predicted', color='red', linestyle='--', linewidth=2, alpha=0.7)

    ax.set_xlabel('Time Steps')
    ax.set_ylabel('Price')
    ax.set_title('Stock Price Prediction - LSTM Forecasting')
    ax.legend()
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

    # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
    from sklearn.metrics import mean_absolute_error, mean_squared_error

    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100

    print(f"\nè©•ä¾¡æŒ‡æ¨™:")
    print(f"  MAE (Mean Absolute Error): {mae:.4f}")
    print(f"  RMSE (Root Mean Squared Error): {rmse:.4f}")
    print(f"  MAPE (Mean Absolute Percentage Error): {mape:.2f}%")

    return y_true, y_pred

# äºˆæ¸¬å®Ÿè¡Œ
y_true, y_pred = forecast_and_visualize(
    model, X_test, y_test, normalizer_minmax, device, num_samples=100
)
</code></pre>

<hr>

<h2>5.4 å¤šå¤‰é‡æ™‚ç³»åˆ—äºˆæ¸¬</h2>

<h3>è¤‡æ•°ç‰¹å¾´é‡ã‚’ç”¨ã„ãŸäºˆæ¸¬</h3>

<p><strong>å¤šå¤‰é‡æ™‚ç³»åˆ—äºˆæ¸¬</strong>ã¯ã€è¤‡æ•°ã®é–¢é€£ã™ã‚‹å¤‰æ•°ã‚’åŒæ™‚ã«è€ƒæ…®ã—ã¦äºˆæ¸¬ã‚’è¡Œã„ã¾ã™ã€‚ä¾‹ãˆã°ã€æ ªä¾¡äºˆæ¸¬ã§ã¯çµ‚å€¤ã ã‘ã§ãªãã€å‡ºæ¥é«˜ã€ç§»å‹•å¹³å‡ã€RSIãªã©ã®æŒ‡æ¨™ã‚‚ä½¿ç”¨ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">import pandas as pd
import numpy as np

def create_multivariate_features(prices, window=20):
    """
    å¤šå¤‰é‡æ™‚ç³»åˆ—ã®ç‰¹å¾´é‡ã‚’ä½œæˆ

    Args:
        prices: æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ï¼ˆnumpy arrayï¼‰
        window: ç§»å‹•å¹³å‡ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º

    Returns:
        features_df: ç‰¹å¾´é‡DataFrame
    """
    df = pd.DataFrame({'price': prices})

    # ç§»å‹•å¹³å‡ï¼ˆMoving Averageï¼‰
    df['ma_5'] = df['price'].rolling(window=5).mean()
    df['ma_20'] = df['price'].rolling(window=20).mean()

    # æ¨™æº–åå·®ï¼ˆVolatilityï¼‰
    df['std_20'] = df['price'].rolling(window=20).std()

    # å¤‰åŒ–ç‡ï¼ˆReturnï¼‰
    df['return'] = df['price'].pct_change()

    # RSIï¼ˆRelative Strength Indexï¼‰
    delta = df['price'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['rsi'] = 100 - (100 / (1 + rs))

    # æ¬ æå€¤ã‚’å‰æ–¹åŸ‹ã‚
    df = df.fillna(method='ffill').fillna(method='bfill')

    return df

# ç–‘ä¼¼æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã§ç‰¹å¾´é‡ä½œæˆ
stock_prices = np.cumsum(np.random.randn(1000)) + 100
features_df = create_multivariate_features(stock_prices)

print("å¤šå¤‰é‡ç‰¹å¾´é‡:")
print(features_df.head(30))
print(f"\nç‰¹å¾´é‡æ•°: {features_df.shape[1]}")

# æ­£è¦åŒ–
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
features_normalized = scaler.fit_transform(features_df.values)

# ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ä½œæˆï¼ˆå¤šå¤‰é‡ç‰ˆï¼‰
WINDOW_SIZE = 60
HORIZON = 1

X_multi, y_multi = create_sliding_windows(
    features_normalized,
    window_size=WINDOW_SIZE,
    horizon=HORIZON
)

print(f"\nå¤šå¤‰é‡ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦:")
print(f"  Xå½¢çŠ¶: {X_multi.shape}")  # [num_samples, 60, 6] - 6ç‰¹å¾´é‡
print(f"  yå½¢çŠ¶: {y_multi.shape}")  # [num_samples, 1, 6]

# ç›®æ¨™ã¯ä¾¡æ ¼ï¼ˆæœ€åˆã®åˆ—ï¼‰ã®ã¿ã‚’ä½¿ç”¨
y_multi_price = y_multi[:, :, 0:1]  # [num_samples, 1, 1]

print(f"  yï¼ˆä¾¡æ ¼ã®ã¿ï¼‰å½¢çŠ¶: {y_multi_price.shape}")
</code></pre>

<h3>å¤šå¤‰é‡LSTMäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«</h3>

<pre><code class="language-python">class MultivariateLSTM(nn.Module):
    """
    å¤šå¤‰é‡æ™‚ç³»åˆ—äºˆæ¸¬ç”¨LSTMãƒ¢ãƒ‡ãƒ«
    """

    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2):
        """
        Args:
            input_size: å…¥åŠ›ç‰¹å¾´æ•°ï¼ˆè¤‡æ•°ï¼‰
            hidden_size: LSTMéš ã‚Œå±¤ã®ã‚µã‚¤ã‚º
            num_layers: LSTMãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
            dropout: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        """
        super(MultivariateLSTM, self).__init__()

        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )

        # å‡ºåŠ›å±¤
        self.fc = nn.Linear(hidden_size, 1)  # 1å¤‰æ•°ï¼ˆä¾¡æ ¼ï¼‰ã‚’äºˆæ¸¬

    def forward(self, x):
        """
        Args:
            x: [batch_size, seq_len, input_size]

        Returns:
            out: [batch_size, 1]
        """
        lstm_out, _ = self.lstm(x)
        out = self.fc(lstm_out[:, -1, :])

        return out

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train_m, X_val_m, X_test_m, y_train_m, y_val_m, y_test_m = train_val_test_split(
    X_multi, y_multi_price, train_ratio=0.7, val_ratio=0.15
)

# Tensorã«å¤‰æ›
X_train_m = torch.FloatTensor(X_train_m)
y_train_m = torch.FloatTensor(y_train_m).squeeze()
X_val_m = torch.FloatTensor(X_val_m)
y_val_m = torch.FloatTensor(y_val_m).squeeze()
X_test_m = torch.FloatTensor(X_test_m)
y_test_m = torch.FloatTensor(y_test_m).squeeze()

# ãƒ¢ãƒ‡ãƒ«ä½œæˆ
input_size = X_multi.shape[2]  # ç‰¹å¾´é‡æ•°ï¼ˆ6ï¼‰
model_multi = MultivariateLSTM(input_size=input_size, hidden_size=128, num_layers=3, dropout=0.3)
model_multi.to(device)

print(f"\nå¤šå¤‰é‡LSTMãƒ¢ãƒ‡ãƒ«:")
print(f"  å…¥åŠ›ç‰¹å¾´æ•°: {input_size}")
print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model_multi.parameters()):,}")

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
train_loader_m = DataLoader(TensorDataset(X_train_m, y_train_m), batch_size=64, shuffle=False)
val_loader_m = DataLoader(TensorDataset(X_val_m, y_val_m), batch_size=64, shuffle=False)

# è¨“ç·´
criterion = nn.MSELoss()
optimizer = optim.Adam(model_multi.parameters(), lr=0.001)

print("\nå¤šå¤‰é‡ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹:")
for epoch in range(1, 51):
    train_loss = train_epoch(model_multi, train_loader_m, criterion, optimizer, device)
    val_loss = validate(model_multi, val_loader_m, criterion, device)

    if epoch % 10 == 0:
        print(f'Epoch {epoch}/50 - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')

print("è¨“ç·´å®Œäº†!")
</code></pre>

<blockquote>
<p><strong>å¤šå¤‰é‡äºˆæ¸¬ã®åˆ©ç‚¹</strong>:</p>
<ul>
<li>è¤‡æ•°ã®é–¢é€£æƒ…å ±ã‚’æ´»ç”¨ã—ã€äºˆæ¸¬ç²¾åº¦ã‚’å‘ä¸Š</li>
<li>å¸‚å ´æŒ‡æ¨™ã€çµŒæ¸ˆæŒ‡æ¨™ãªã©å¤–éƒ¨æƒ…å ±ã‚’çµ±åˆå¯èƒ½</li>
<li>ãƒ¢ãƒ‡ãƒ«ãŒå¤‰æ•°é–“ã®ç›¸äº’ä½œç”¨ã‚’å­¦ç¿’</li>
</ul>
</blockquote>

<hr>

<h2>5.5 Seq2Seqã«ã‚ˆã‚‹å¤šã‚¹ãƒ†ãƒƒãƒ—äºˆæ¸¬</h2>

<h3>Sequence-to-Sequenceäºˆæ¸¬</h3>

<p><strong>Seq2Seqï¼ˆSequence-to-Sequenceï¼‰</strong>ãƒ¢ãƒ‡ãƒ«ã¯ã€è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—å…ˆã®æ™‚ç³»åˆ—ã‚’ä¸€åº¦ã«äºˆæ¸¬ã™ã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[Encoder<br/>éå»60æ—¥] --> B[Context Vector<br/>éš ã‚ŒçŠ¶æ…‹]
    B --> C[Decoder<br/>æœªæ¥7æ—¥äºˆæ¸¬]

    A1[x1, x2, ..., x60] --> A
    C --> C1[y1, y2, ..., y7]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#e8f5e9
</div>

<pre><code class="language-python">class Seq2SeqLSTM(nn.Module):
    """
    Seq2Seqæ™‚ç³»åˆ—äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ï¼ˆEncoder-Decoderï¼‰
    """

    def __init__(self, input_size=1, hidden_size=128, num_layers=2, output_length=7, dropout=0.2):
        """
        Args:
            input_size: å…¥åŠ›ç‰¹å¾´æ•°
            hidden_size: LSTMéš ã‚Œå±¤ã‚µã‚¤ã‚º
            num_layers: LSTMãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
            output_length: å‡ºåŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ï¼ˆä½•ã‚¹ãƒ†ãƒƒãƒ—å…ˆã¾ã§äºˆæ¸¬ã™ã‚‹ã‹ï¼‰
            dropout: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        """
        super(Seq2SeqLSTM, self).__init__()

        self.output_length = output_length
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Encoder LSTM
        self.encoder = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )

        # Decoder LSTM
        self.decoder = nn.LSTM(
            input_size=1,  # Decoderã®å…¥åŠ›ã¯å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã®äºˆæ¸¬å€¤
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )

        # å‡ºåŠ›å±¤
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x, target_len=None):
        """
        Args:
            x: [batch_size, seq_len, input_size]
            target_len: ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®å‡ºåŠ›é•·ï¼ˆæ¨è«–æ™‚ã«æŒ‡å®šï¼‰

        Returns:
            outputs: [batch_size, output_length, 1]
        """
        batch_size = x.size(0)

        if target_len is None:
            target_len = self.output_length

        # Encoderã§éå»ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        _, (hidden, cell) = self.encoder(x)

        # Decoderã®åˆæœŸå…¥åŠ›ï¼ˆæœ€å¾Œã®å…¥åŠ›å€¤ï¼‰
        decoder_input = x[:, -1, 0:1].unsqueeze(1)  # [batch_size, 1, 1]

        outputs = []

        # Decoderã§æœªæ¥ã‚’ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«äºˆæ¸¬
        for t in range(target_len):
            decoder_output, (hidden, cell) = self.decoder(decoder_input, (hidden, cell))

            # äºˆæ¸¬å€¤
            prediction = self.fc(decoder_output)  # [batch_size, 1, 1]
            outputs.append(prediction)

            # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®å…¥åŠ›ã¯å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã®äºˆæ¸¬å€¤
            decoder_input = prediction

        # å‡ºåŠ›ã‚’çµåˆ
        outputs = torch.cat(outputs, dim=1)  # [batch_size, target_len, 1]

        return outputs

# Seq2Seqç”¨ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ä½œæˆï¼ˆè¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—å…ˆã‚’äºˆæ¸¬ï¼‰
WINDOW_SIZE = 60
HORIZON = 7  # 7ã‚¹ãƒ†ãƒƒãƒ—å…ˆã¾ã§äºˆæ¸¬

X_seq2seq, y_seq2seq = create_sliding_windows(
    data,
    window_size=WINDOW_SIZE,
    horizon=HORIZON,
    stride=1
)

print(f"Seq2Seqãƒ‡ãƒ¼ã‚¿:")
print(f"  Xå½¢çŠ¶: {X_seq2seq.shape}")  # [num_samples, 60, 1]
print(f"  yå½¢çŠ¶: {y_seq2seq.shape}")  # [num_samples, 7, 1]

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train_s2s, X_val_s2s, X_test_s2s, y_train_s2s, y_val_s2s, y_test_s2s = train_val_test_split(
    X_seq2seq, y_seq2seq, train_ratio=0.7, val_ratio=0.15
)

# Tensorã«å¤‰æ›
X_train_s2s = torch.FloatTensor(X_train_s2s)
y_train_s2s = torch.FloatTensor(y_train_s2s)
X_val_s2s = torch.FloatTensor(X_val_s2s)
y_val_s2s = torch.FloatTensor(y_val_s2s)
X_test_s2s = torch.FloatTensor(X_test_s2s)
y_test_s2s = torch.FloatTensor(y_test_s2s)

# ãƒ¢ãƒ‡ãƒ«ä½œæˆ
model_seq2seq = Seq2SeqLSTM(
    input_size=1,
    hidden_size=128,
    num_layers=2,
    output_length=HORIZON,
    dropout=0.2
)
model_seq2seq.to(device)

print(f"\nSeq2Seqãƒ¢ãƒ‡ãƒ«:")
print(f"  å‡ºåŠ›é•·: {HORIZON} ã‚¹ãƒ†ãƒƒãƒ—")
print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model_seq2seq.parameters()):,}")

# è¨“ç·´é–¢æ•°ï¼ˆSeq2Seqç”¨ï¼‰
def train_seq2seq(model, X, y, criterion, optimizer, device, epochs=30, batch_size=32):
    """
    Seq2Seqãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
    """
    dataset = TensorDataset(X, y)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

    train_losses = []

    for epoch in range(1, epochs + 1):
        model.train()
        total_loss = 0

        for batch_X, batch_y in loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)

            # é †ä¼æ’­
            predictions = model(batch_X)
            loss = criterion(predictions, batch_y)

            # é€†ä¼æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(loader)
        train_losses.append(avg_loss)

        if epoch % 10 == 0:
            print(f'Epoch {epoch}/{epochs} - Loss: {avg_loss:.6f}')

    return train_losses

# è¨“ç·´å®Ÿè¡Œ
criterion = nn.MSELoss()
optimizer = optim.Adam(model_seq2seq.parameters(), lr=0.001)

print("\nSeq2Seqè¨“ç·´é–‹å§‹:")
train_losses = train_seq2seq(
    model_seq2seq, X_train_s2s, y_train_s2s,
    criterion, optimizer, device, epochs=50, batch_size=32
)

print("è¨“ç·´å®Œäº†!")
</code></pre>

<h3>å¤šã‚¹ãƒ†ãƒƒãƒ—äºˆæ¸¬ã®å¯è¦–åŒ–</h3>

<pre><code class="language-python">def visualize_multistep_forecast(model, X_test, y_test, normalizer, device, sample_idx=0):
    """
    å¤šã‚¹ãƒ†ãƒƒãƒ—äºˆæ¸¬ã®å¯è¦–åŒ–

    Args:
        model: Seq2Seqãƒ¢ãƒ‡ãƒ«
        X_test: ãƒ†ã‚¹ãƒˆå…¥åŠ›
        y_test: ãƒ†ã‚¹ãƒˆç›®æ¨™
        normalizer: æ­£è¦åŒ–å™¨
        device: ãƒ‡ãƒã‚¤ã‚¹
        sample_idx: å¯è¦–åŒ–ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    """
    model.eval()

    # 1ã¤ã®ã‚µãƒ³ãƒ—ãƒ«ã§äºˆæ¸¬
    sample_input = torch.FloatTensor(X_test[sample_idx:sample_idx+1]).to(device)

    with torch.no_grad():
        prediction = model(sample_input).cpu().numpy()

    # æ­£è¦åŒ–ã‚’å…ƒã«æˆ»ã™
    y_true = normalizer.inverse_transform(y_test[sample_idx].squeeze())
    y_pred = normalizer.inverse_transform(prediction.squeeze())

    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚‚å…ƒã«æˆ»ã™
    x_input = normalizer.inverse_transform(X_test[sample_idx].squeeze())

    # å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(14, 6))

    # éå»ãƒ‡ãƒ¼ã‚¿ï¼ˆå…¥åŠ›ï¼‰
    past_steps = np.arange(len(x_input))
    ax.plot(past_steps, x_input, label='Past (Input)', color='gray', linewidth=2)

    # æœªæ¥ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã®å€¤ã¨äºˆæ¸¬å€¤ï¼‰
    future_steps = np.arange(len(x_input), len(x_input) + len(y_true))
    ax.plot(future_steps, y_true, label='Actual Future', color='blue', linewidth=2, marker='o')
    ax.plot(future_steps, y_pred, label='Predicted Future', color='red', linewidth=2, linestyle='--', marker='x')

    # å¢ƒç•Œç·š
    ax.axvline(x=len(x_input)-1, color='black', linestyle=':', linewidth=1.5, alpha=0.7)
    ax.text(len(x_input)-1, ax.get_ylim()[1]*0.95, 'Prediction Start',
            verticalalignment='top', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))

    ax.set_xlabel('Time Steps')
    ax.set_ylabel('Price')
    ax.set_title(f'Multi-Step Forecasting (7 steps ahead) - Seq2Seq')
    ax.legend()
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

    # è©•ä¾¡æŒ‡æ¨™
    from sklearn.metrics import mean_absolute_error, mean_squared_error

    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))

    print(f"\nå¤šã‚¹ãƒ†ãƒƒãƒ—äºˆæ¸¬ è©•ä¾¡æŒ‡æ¨™:")
    print(f"  MAE: {mae:.4f}")
    print(f"  RMSE: {rmse:.4f}")

# å¯è¦–åŒ–å®Ÿè¡Œ
visualize_multistep_forecast(
    model_seq2seq, X_test_s2s, y_test_s2s, normalizer_minmax, device, sample_idx=10
)
</code></pre>

<hr>

<h2>5.6 è©•ä¾¡æŒ‡æ¨™</h2>

<h3>æ™‚ç³»åˆ—äºˆæ¸¬ã®è©•ä¾¡æŒ‡æ¨™</h3>

<p>æ™‚ç³»åˆ—äºˆæ¸¬ã§ã¯ã€ä»¥ä¸‹ã®æŒ‡æ¨™ãŒä¸€èˆ¬çš„ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚</p>

<table>
<thead>
<tr>
<th>æŒ‡æ¨™</th>
<th>å¼</th>
<th>ç‰¹å¾´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MAE</strong></td>
<td>$\frac{1}{N}\sum_{i=1}^{N}|y_i - \hat{y}_i|$</td>
<td>å¹³å‡çµ¶å¯¾èª¤å·®ï¼ˆå¤–ã‚Œå€¤ã«é ‘å¥ï¼‰</td>
</tr>
<tr>
<td><strong>RMSE</strong></td>
<td>$\sqrt{\frac{1}{N}\sum_{i=1}^{N}(y_i - \hat{y}_i)^2}$</td>
<td>äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·®ï¼ˆå¤§ããªèª¤å·®ã‚’é‡è¦–ï¼‰</td>
</tr>
<tr>
<td><strong>MAPE</strong></td>
<td>$\frac{100}{N}\sum_{i=1}^{N}\left|\frac{y_i - \hat{y}_i}{y_i}\right|$</td>
<td>å¹³å‡çµ¶å¯¾ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆèª¤å·®ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«éä¾å­˜ï¼‰</td>
</tr>
<tr>
<td><strong>RÂ²</strong></td>
<td>$1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}$</td>
<td>æ±ºå®šä¿‚æ•°ï¼ˆèª¬æ˜åŠ›ã®æŒ‡æ¨™ï¼‰</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

class TimeSeriesMetrics:
    """
    æ™‚ç³»åˆ—äºˆæ¸¬ã®è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—ã™ã‚‹ã‚¯ãƒ©ã‚¹
    """

    @staticmethod
    def mae(y_true, y_pred):
        """Mean Absolute Error"""
        return mean_absolute_error(y_true, y_pred)

    @staticmethod
    def rmse(y_true, y_pred):
        """Root Mean Squared Error"""
        return np.sqrt(mean_squared_error(y_true, y_pred))

    @staticmethod
    def mape(y_true, y_pred):
        """
        Mean Absolute Percentage Error

        Note: y_trueã«0ãŒå«ã¾ã‚Œã‚‹å ´åˆã¯è¨ˆç®—ã§ããªã„
        """
        # ã‚¼ãƒ­ã‚’å«ã‚€å ´åˆã¯é™¤å¤–
        mask = y_true != 0
        return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100

    @staticmethod
    def r2(y_true, y_pred):
        """RÂ² Score (Coefficient of Determination)"""
        return r2_score(y_true, y_pred)

    @staticmethod
    def mse(y_true, y_pred):
        """Mean Squared Error"""
        return mean_squared_error(y_true, y_pred)

    @staticmethod
    def smape(y_true, y_pred):
        """
        Symmetric Mean Absolute Percentage Error

        sMAPEã¯åˆ†æ¯ãŒã‚¼ãƒ­ã«ãªã‚Šã«ãã„æ”¹è‰¯ç‰ˆMAPE
        """
        numerator = np.abs(y_true - y_pred)
        denominator = (np.abs(y_true) + np.abs(y_pred)) / 2
        return np.mean(numerator / denominator) * 100

    @classmethod
    def calculate_all(cls, y_true, y_pred):
        """
        å…¨ã¦ã®è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—

        Args:
            y_true: å®Ÿéš›ã®å€¤
            y_pred: äºˆæ¸¬å€¤

        Returns:
            metrics: è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
        """
        metrics = {
            'MAE': cls.mae(y_true, y_pred),
            'RMSE': cls.rmse(y_true, y_pred),
            'MSE': cls.mse(y_true, y_pred),
            'MAPE': cls.mape(y_true, y_pred),
            'sMAPE': cls.smape(y_true, y_pred),
            'RÂ²': cls.r2(y_true, y_pred)
        }

        return metrics

    @classmethod
    def print_metrics(cls, y_true, y_pred, title="Evaluation Metrics"):
        """
        è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã¦è¡¨ç¤º
        """
        metrics = cls.calculate_all(y_true, y_pred)

        print(f"\n{title}:")
        print("=" * 50)
        for name, value in metrics.items():
            if name == 'RÂ²':
                print(f"  {name:10s}: {value:.4f}")
            elif 'MAPE' in name or 'sMAPE' in name:
                print(f"  {name:10s}: {value:.2f}%")
            else:
                print(f"  {name:10s}: {value:.4f}")
        print("=" * 50)

# ä½¿ç”¨ä¾‹
# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
y_true_sample = np.array([100, 105, 110, 108, 115, 120, 118])
y_pred_sample = np.array([98, 107, 109, 110, 113, 122, 117])

TimeSeriesMetrics.print_metrics(y_true_sample, y_pred_sample, title="Sample Forecast Evaluation")
</code></pre>

<blockquote>
<p><strong>æŒ‡æ¨™ã®é¸ã³æ–¹</strong>:</p>
<ul>
<li><strong>MAE</strong>: èª¤å·®ã®å¤§ãã•ã‚’ç›´æ„Ÿçš„ã«ç†è§£ã—ãŸã„å ´åˆ</li>
<li><strong>RMSE</strong>: å¤§ããªèª¤å·®ã‚’é‡è¦–ã—ãŸã„å ´åˆï¼ˆå¤–ã‚Œå€¤ã«æ•æ„Ÿï¼‰</li>
<li><strong>MAPE</strong>: ç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒã—ãŸã„å ´åˆ</li>
<li><strong>RÂ²</strong>: ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜åŠ›ã‚’è©•ä¾¡ã—ãŸã„å ´åˆ</li>
</ul>
</blockquote>

<hr>

<h2>5.7 å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼šæ ªä¾¡äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ </h2>

<h3>å®Œå…¨ãªæ ªä¾¡äºˆæ¸¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h3>

<p>å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ãŸæ ªä¾¡äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ä¾‹ã§ã™ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

class StockPriceForecastingPipeline:
    """
    æ ªä¾¡äºˆæ¸¬ã®å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    """

    def __init__(self, window_size=60, horizon=1, hidden_size=128, num_layers=2):
        """
        Args:
            window_size: å…¥åŠ›ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºï¼ˆéå»ä½•æ—¥è¦‹ã‚‹ã‹ï¼‰
            horizon: äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ï¼ˆä½•æ—¥å…ˆã‚’äºˆæ¸¬ã™ã‚‹ã‹ï¼‰
            hidden_size: LSTMéš ã‚Œå±¤ã‚µã‚¤ã‚º
            num_layers: LSTMãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
        """
        self.window_size = window_size
        self.horizon = horizon
        self.scaler = MinMaxScaler()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        self.model = None
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        print(f"æ ªä¾¡äºˆæ¸¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–:")
        print(f"  ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º: {window_size}æ—¥")
        print(f"  äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚ºãƒ³: {horizon}æ—¥")
        print(f"  ãƒ‡ãƒã‚¤ã‚¹: {self.device}")

    def load_data(self, prices):
        """
        ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†

        Args:
            prices: æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ï¼ˆnumpy array or listï¼‰

        Returns:
            X_train, X_val, X_test, y_train, y_val, y_test
        """
        # æ­£è¦åŒ–
        prices = np.array(prices).reshape(-1, 1)
        normalized_data = self.scaler.fit_transform(prices)

        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ä½œæˆ
        X, y = create_sliding_windows(
            normalized_data,
            window_size=self.window_size,
            horizon=self.horizon
        )

        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(
            X, y, train_ratio=0.7, val_ratio=0.15
        )

        # Tensorã«å¤‰æ›
        self.X_train = torch.FloatTensor(X_train)
        self.y_train = torch.FloatTensor(y_train).squeeze()
        self.X_val = torch.FloatTensor(X_val)
        self.y_val = torch.FloatTensor(y_val).squeeze()
        self.X_test = torch.FloatTensor(X_test)
        self.y_test = torch.FloatTensor(y_test).squeeze()

        print(f"\nãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†:")
        print(f"  è¨“ç·´: {len(self.X_train)} samples")
        print(f"  æ¤œè¨¼: {len(self.X_val)} samples")
        print(f"  ãƒ†ã‚¹ãƒˆ: {len(self.X_test)} samples")

        return self.X_train, self.X_val, self.X_test, self.y_train, self.y_val, self.y_test

    def build_model(self, input_size=1):
        """
        LSTMãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
        """
        self.model = LSTMForecaster(
            input_size=input_size,
            hidden_size=self.hidden_size,
            num_layers=self.num_layers,
            dropout=0.2
        )
        self.model.to(self.device)

        print(f"\nãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰å®Œäº†:")
        print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in self.model.parameters()):,}")

    def train(self, epochs=50, batch_size=32, learning_rate=0.001):
        """
        ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´

        Args:
            epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            learning_rate: å­¦ç¿’ç‡

        Returns:
            train_losses, val_losses: è¨“ç·´ãƒ»æ¤œè¨¼æå¤±ã®å±¥æ­´
        """
        from torch.utils.data import TensorDataset, DataLoader

        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        train_loader = DataLoader(
            TensorDataset(self.X_train, self.y_train),
            batch_size=batch_size,
            shuffle=False
        )
        val_loader = DataLoader(
            TensorDataset(self.X_val, self.y_val),
            batch_size=batch_size,
            shuffle=False
        )

        # æå¤±é–¢æ•°ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)

        train_losses = []
        val_losses = []

        print(f"\nè¨“ç·´é–‹å§‹:")
        for epoch in range(1, epochs + 1):
            # è¨“ç·´
            train_loss = train_epoch(self.model, train_loader, criterion, optimizer, self.device)
            val_loss = validate(self.model, val_loader, criterion, self.device)

            train_losses.append(train_loss)
            val_losses.append(val_loss)

            if epoch % 10 == 0:
                print(f'Epoch {epoch}/{epochs} - Train: {train_loss:.6f}, Val: {val_loss:.6f}')

        print("è¨“ç·´å®Œäº†!")

        return train_losses, val_losses

    def predict(self, X):
        """
        äºˆæ¸¬ã‚’å®Ÿè¡Œ

        Args:
            X: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆTensor or numpyï¼‰

        Returns:
            predictions: äºˆæ¸¬å€¤ï¼ˆå…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        """
        self.model.eval()

        if isinstance(X, np.ndarray):
            X = torch.FloatTensor(X)

        X = X.to(self.device)

        with torch.no_grad():
            predictions = self.model(X).cpu().numpy()

        # æ­£è¦åŒ–ã‚’å…ƒã«æˆ»ã™
        predictions = self.scaler.inverse_transform(predictions.reshape(-1, 1))

        return predictions.flatten()

    def evaluate(self, X_test=None, y_test=None):
        """
        ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡

        Returns:
            metrics: è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
        """
        if X_test is None:
            X_test = self.X_test
            y_test = self.y_test

        # äºˆæ¸¬
        y_pred = self.predict(X_test)

        # å®Ÿéš›ã®å€¤ã‚’å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™
        y_true = self.scaler.inverse_transform(y_test.numpy().reshape(-1, 1)).flatten()

        # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
        metrics = TimeSeriesMetrics.calculate_all(y_true, y_pred)

        TimeSeriesMetrics.print_metrics(y_true, y_pred, title="Test Set Evaluation")

        return metrics, y_true, y_pred

    def visualize_predictions(self, num_samples=100):
        """
        äºˆæ¸¬çµæœã®å¯è¦–åŒ–
        """
        metrics, y_true, y_pred = self.evaluate()

        # æœ€åˆã®num_samplesã®ã¿å¯è¦–åŒ–
        y_true = y_true[:num_samples]
        y_pred = y_pred[:num_samples]

        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))

        # äºˆæ¸¬ vs å®Ÿéš›
        time_steps = np.arange(len(y_true))
        ax1.plot(time_steps, y_true, label='Actual', color='blue', linewidth=2)
        ax1.plot(time_steps, y_pred, label='Predicted', color='red', linestyle='--', linewidth=2, alpha=0.7)
        ax1.set_xlabel('Time Steps')
        ax1.set_ylabel('Stock Price')
        ax1.set_title('Stock Price Prediction - Actual vs Predicted')
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # èª¤å·®
        errors = y_true - y_pred
        ax2.plot(time_steps, errors, color='purple', linewidth=1.5)
        ax2.axhline(y=0, color='black', linestyle='--', linewidth=1)
        ax2.fill_between(time_steps, 0, errors, alpha=0.3, color='purple')
        ax2.set_xlabel('Time Steps')
        ax2.set_ylabel('Prediction Error')
        ax2.set_title('Prediction Error Over Time')
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    def save_model(self, path='stock_forecaster.pth'):
        """ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        torch.save(self.model.state_dict(), path)
        print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {path}")

    def load_model(self, path='stock_forecaster.pth'):
        """ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        self.model.load_state_dict(torch.load(path))
        self.model.to(self.device)
        print(f"ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿: {path}")

# ä½¿ç”¨ä¾‹
if __name__ == '__main__':
    # ç–‘ä¼¼æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå®Ÿéš›ã«ã¯yfinanceãªã©ã§å–å¾—ï¼‰
    np.random.seed(42)
    stock_prices = np.cumsum(np.random.randn(1500)) + 100

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–
    pipeline = StockPriceForecastingPipeline(
        window_size=60,
        horizon=1,
        hidden_size=128,
        num_layers=2
    )

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    pipeline.load_data(stock_prices)

    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    pipeline.build_model(input_size=1)

    # è¨“ç·´
    train_losses, val_losses = pipeline.train(epochs=50, batch_size=32, learning_rate=0.001)

    # è©•ä¾¡ã¨å¯è¦–åŒ–
    pipeline.visualize_predictions(num_samples=100)

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    # pipeline.save_model('stock_forecaster.pth')
</code></pre>

<hr>

<h2>5.8 å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼šæ°—è±¡äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ </h2>

<h3>å¤šå¤‰é‡æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬</h3>

<p>æ°—æ¸©ã€æ¹¿åº¦ã€æ°—åœ§ãªã©ã®è¤‡æ•°ã®æ°—è±¡å¤‰æ•°ã‚’ä½¿ã£ãŸäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import torch
import torch.nn as nn

class WeatherForecastingSystem:
    """
    å¤šå¤‰é‡æ°—è±¡äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
    """

    def __init__(self, window_size=24, horizon=6, hidden_size=128):
        """
        Args:
            window_size: å…¥åŠ›ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºï¼ˆéå»24æ™‚é–“ï¼‰
            horizon: äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ï¼ˆ6æ™‚é–“å…ˆï¼‰
            hidden_size: LSTMéš ã‚Œå±¤ã‚µã‚¤ã‚º
        """
        self.window_size = window_size
        self.horizon = horizon
        self.hidden_size = hidden_size
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        self.scaler = MinMaxScaler()
        self.model = None

        print(f"æ°—è±¡äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–:")
        print(f"  å…¥åŠ›: éå»{window_size}æ™‚é–“")
        print(f"  å‡ºåŠ›: {horizon}æ™‚é–“å…ˆã®æ°—æ¸©äºˆæ¸¬")

    def create_synthetic_weather_data(self, num_hours=2000):
        """
        ç–‘ä¼¼æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆå®Ÿéš›ã«ã¯APIã‹ã‚‰å–å¾—ï¼‰

        Returns:
            weather_df: æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã®DataFrame
        """
        np.random.seed(42)

        # æ™‚é–“è»¸
        time_index = pd.date_range('2023-01-01', periods=num_hours, freq='H')

        # åŸºæœ¬ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆå­£ç¯€æ€§ã‚’æŒã¤æ°—æ¸©ï¼‰
        season = 15 * np.sin(2 * np.pi * np.arange(num_hours) / (24 * 365))
        daily = 5 * np.sin(2 * np.pi * np.arange(num_hours) / 24)

        # å„æ°—è±¡å¤‰æ•°
        temperature = 15 + season + daily + np.random.randn(num_hours) * 2
        humidity = 60 + 20 * np.sin(2 * np.pi * np.arange(num_hours) / 24) + np.random.randn(num_hours) * 5
        pressure = 1013 + np.random.randn(num_hours) * 3
        wind_speed = 5 + np.abs(np.random.randn(num_hours) * 2)

        # DataFrameä½œæˆ
        weather_df = pd.DataFrame({
            'temperature': temperature,
            'humidity': humidity,
            'pressure': pressure,
            'wind_speed': wind_speed
        }, index=time_index)

        return weather_df

    def prepare_data(self, weather_df):
        """
        ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ä½œæˆ

        Args:
            weather_df: æ°—è±¡ãƒ‡ãƒ¼ã‚¿DataFrame

        Returns:
            X_train, X_val, X_test, y_train, y_val, y_test
        """
        # æ­£è¦åŒ–
        data_normalized = self.scaler.fit_transform(weather_df.values)

        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ä½œæˆ
        X, y = create_sliding_windows(
            data_normalized,
            window_size=self.window_size,
            horizon=self.horizon
        )

        # ç›®æ¨™ã¯æ°—æ¸©ï¼ˆæœ€åˆã®åˆ—ï¼‰ã®ã¿
        y_temperature = y[:, :, 0]  # [num_samples, horizon]

        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(
            X, y_temperature, train_ratio=0.7, val_ratio=0.15
        )

        # Tensorã«å¤‰æ›
        self.X_train = torch.FloatTensor(X_train)
        self.y_train = torch.FloatTensor(y_train)
        self.X_val = torch.FloatTensor(X_val)
        self.y_val = torch.FloatTensor(y_val)
        self.X_test = torch.FloatTensor(X_test)
        self.y_test = torch.FloatTensor(y_test)

        print(f"\nãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†:")
        print(f"  ç‰¹å¾´é‡æ•°: {X.shape[2]}")
        print(f"  è¨“ç·´: {len(self.X_train)} samples")
        print(f"  ãƒ†ã‚¹ãƒˆ: {len(self.X_test)} samples")

        return self.X_train, self.X_val, self.X_test, self.y_train, self.y_val, self.y_test

    def build_model(self, input_size):
        """
        Seq2Seqãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
        """
        self.model = Seq2SeqLSTM(
            input_size=input_size,
            hidden_size=self.hidden_size,
            num_layers=2,
            output_length=self.horizon,
            dropout=0.2
        )
        self.model.to(self.device)

        print(f"\nSeq2Seqãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰å®Œäº†:")
        print(f"  å…¥åŠ›ç‰¹å¾´æ•°: {input_size}")
        print(f"  å‡ºåŠ›é•·: {self.horizon}æ™‚é–“")
        print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in self.model.parameters()):,}")

    def train(self, epochs=30, batch_size=32, learning_rate=0.001):
        """
        ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
        """
        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)

        print(f"\nè¨“ç·´é–‹å§‹:")
        losses = train_seq2seq(
            self.model, self.X_train, self.y_train,
            criterion, optimizer, self.device,
            epochs=epochs, batch_size=batch_size
        )

        return losses

    def predict_weather(self, X):
        """
        æ°—è±¡äºˆæ¸¬

        Args:
            X: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆéå»ã®æ°—è±¡ãƒ‡ãƒ¼ã‚¿ï¼‰

        Returns:
            predictions: æ°—æ¸©ã®äºˆæ¸¬å€¤ï¼ˆå…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        """
        self.model.eval()

        if isinstance(X, np.ndarray):
            X = torch.FloatTensor(X)

        X = X.to(self.device)

        with torch.no_grad():
            predictions = self.model(X).cpu().numpy()

        # æ°—æ¸©ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å…ƒã«æˆ»ã™ï¼ˆæœ€åˆã®ç‰¹å¾´é‡ï¼‰
        # scalerã®é€†å¤‰æ›ã¯å…¨ç‰¹å¾´é‡å¿…è¦ãªã®ã§ã€ãƒ€ãƒŸãƒ¼ã‚’è¿½åŠ 
        num_features = self.scaler.n_features_in_

        # äºˆæ¸¬å€¤ã‚’æ‹¡å¼µ
        predictions_expanded = np.zeros((predictions.shape[0], predictions.shape[1], num_features))
        predictions_expanded[:, :, 0] = predictions.squeeze()

        # é€†å¤‰æ›
        predictions_original = self.scaler.inverse_transform(
            predictions_expanded.reshape(-1, num_features)
        )[:, 0].reshape(predictions.shape[0], predictions.shape[1])

        return predictions_original

    def visualize_forecast(self, sample_idx=0):
        """
        äºˆæ¸¬çµæœã®å¯è¦–åŒ–
        """
        # äºˆæ¸¬
        y_pred = self.predict_weather(self.X_test[sample_idx:sample_idx+1])

        # å®Ÿéš›ã®å€¤ã‚’å…ƒã«æˆ»ã™
        y_true_expanded = np.zeros((self.horizon, self.scaler.n_features_in_))
        y_true_expanded[:, 0] = self.y_test[sample_idx].numpy()
        y_true = self.scaler.inverse_transform(y_true_expanded)[:, 0]

        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚‚å…ƒã«æˆ»ã™
        X_input = self.scaler.inverse_transform(
            self.X_test[sample_idx].numpy()
        )[:, 0]  # æ°—æ¸©ã®ã¿

        # å¯è¦–åŒ–
        fig, ax = plt.subplots(figsize=(14, 6))

        # éå»ãƒ‡ãƒ¼ã‚¿
        past_hours = np.arange(len(X_input))
        ax.plot(past_hours, X_input, label='Past Temperature (24h)', color='gray', linewidth=2, marker='o')

        # æœªæ¥äºˆæ¸¬
        future_hours = np.arange(len(X_input), len(X_input) + self.horizon)
        ax.plot(future_hours, y_true, label='Actual Temperature (6h)', color='blue', linewidth=2, marker='o')
        ax.plot(future_hours, y_pred.flatten(), label='Predicted Temperature (6h)',
                color='red', linewidth=2, linestyle='--', marker='x')

        # å¢ƒç•Œç·š
        ax.axvline(x=len(X_input)-1, color='black', linestyle=':', linewidth=1.5, alpha=0.7)

        ax.set_xlabel('Hours')
        ax.set_ylabel('Temperature (Â°C)')
        ax.set_title('Weather Forecasting - Temperature Prediction (6 hours ahead)')
        ax.legend()
        ax.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

        # è©•ä¾¡æŒ‡æ¨™
        TimeSeriesMetrics.print_metrics(y_true, y_pred.flatten(), title="Weather Forecast Evaluation")

# ä½¿ç”¨ä¾‹
if __name__ == '__main__':
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    weather_system = WeatherForecastingSystem(window_size=24, horizon=6, hidden_size=128)

    # ç–‘ä¼¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    weather_data = weather_system.create_synthetic_weather_data(num_hours=2000)
    print("\næ°—è±¡ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:")
    print(weather_data.head(24))

    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    X_train, X_val, X_test, y_train, y_val, y_test = weather_system.prepare_data(weather_data)

    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    weather_system.build_model(input_size=weather_data.shape[1])

    # è¨“ç·´
    losses = weather_system.train(epochs=30, batch_size=32)

    # äºˆæ¸¬ã¨å¯è¦–åŒ–
    weather_system.visualize_forecast(sample_idx=10)
</code></pre>

<hr>

<h2>æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®åŸºç¤</strong></p>
<ul>
<li>ãƒˆãƒ¬ãƒ³ãƒ‰ã€å­£ç¯€æ€§ã€å‘¨æœŸæ€§ã€ãƒã‚¤ã‚ºã®ç†è§£</li>
<li>å®šå¸¸æ€§ã®æ¤œå®šã¨å¤‰æ›æ‰‹æ³•</li>
<li>è‡ªå·±ç›¸é–¢ã¨æ™‚é–“ä¾å­˜æ€§</li>
</ul></li>
<li><p><strong>ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†</strong></p>
<ul>
<li>æ­£è¦åŒ–ã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®é‡è¦æ€§</li>
<li>ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ä½œæˆ</li>
<li>æ™‚é–“é †åºã‚’ä¿æŒã—ãŸãƒ‡ãƒ¼ã‚¿åˆ†å‰²</li>
</ul></li>
<li><p><strong>äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«</strong></p>
<ul>
<li>å˜å¤‰é‡LSTMäºˆæ¸¬ï¼ˆæ ªä¾¡ãªã©ï¼‰</li>
<li>å¤šå¤‰é‡LSTMäºˆæ¸¬ï¼ˆè¤‡æ•°ç‰¹å¾´é‡ï¼‰</li>
<li>Seq2Seqå¤šã‚¹ãƒ†ãƒƒãƒ—äºˆæ¸¬</li>
</ul></li>
<li><p><strong>è©•ä¾¡ã¨å®Ÿè·µ</strong></p>
<ul>
<li>MAEã€RMSEã€MAPEã€RÂ²ã®è¨ˆç®—ã¨è§£é‡ˆ</li>
<li>å®Ÿè·µçš„ãªæ ªä¾¡äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ </li>
<li>å¤šå¤‰é‡æ°—è±¡äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ </li>
</ul></li>
</ol>

<h3>æ™‚ç³»åˆ—äºˆæ¸¬ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>æ¨å¥¨æ‰‹æ³•</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ‡ãƒ¼ã‚¿åˆ†å‰²</strong></td>
<td>æ™‚é–“é †åºã‚’ä¿æŒ</td>
<td>æœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ã§éå»ã‚’äºˆæ¸¬ã™ã‚‹äº‹æ…‹ã‚’é˜²ã</td>
</tr>
<tr>
<td><strong>æ­£è¦åŒ–</strong></td>
<td>è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§å­¦ç¿’</td>
<td>ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¸ã®ãƒªãƒ¼ã‚¯ã‚’é˜²æ­¢</td>
</tr>
<tr>
<td><strong>ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º</strong></td>
<td>ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦èª¿æ•´ï¼ˆ1é€±é–“ã€œ3ãƒ¶æœˆï¼‰</td>
<td>çŸ­ã™ãã‚‹ã¨æ–‡è„ˆä¸è¶³ã€é•·ã™ãã‚‹ã¨éå­¦ç¿’</td>
</tr>
<tr>
<td><strong>è©•ä¾¡æŒ‡æ¨™</strong></td>
<td>è¤‡æ•°ã®æŒ‡æ¨™ã‚’ä½µç”¨ï¼ˆMAE + RMSE + MAPEï¼‰</td>
<td>å¤šè§’çš„ã«ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡</td>
</tr>
<tr>
<td><strong>ãƒ¢ãƒ‡ãƒ«é¸æŠ</strong></td>
<td>LSTM/GRU â†’ Transformerï¼ˆé•·æœŸä¾å­˜ï¼‰</td>
<td>ã‚¿ã‚¹ã‚¯ã®è¤‡é›‘ã•ã«å¿œã˜ã¦é¸æŠ</td>
</tr>
</tbody>
</table>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>ä»¥ä¸‹ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã€ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆwindow_size=5, horizon=2ï¼‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚</p>
<pre><code>data = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
</code></pre>
<p>ä½œæˆã•ã‚Œã‚‹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®æ•°ã¨ã€æœ€åˆã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®X, yã‚’ç­”ãˆã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import numpy as np

data = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100]).reshape(-1, 1)

def create_sliding_windows(data, window_size, horizon):
    X, y = [], []
    for i in range(len(data) - window_size - horizon + 1):
        X.append(data[i : i + window_size])
        y.append(data[i + window_size : i + window_size + horizon])
    return np.array(X), np.array(y)

X, y = create_sliding_windows(data, window_size=5, horizon=2)

print(f"ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ•°: {len(X)}")
print(f"æœ€åˆã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦:")
print(f"  X (å…¥åŠ›): {X[0].flatten()}")
print(f"  y (ç›®æ¨™): {y[0].flatten()}")

# å‡ºåŠ›:
# ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ•°: 4
# æœ€åˆã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦:
#   X (å…¥åŠ›): [10 20 30 40 50]
#   y (ç›®æ¨™): [60 70]
</code></pre>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>å˜å¤‰é‡LSTMãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã€sinæ³¢ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚sinæ³¢ã¯ <code>np.sin(np.linspace(0, 100, 1000))</code> ã§ç”Ÿæˆã—ã€éå»30ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰æ¬¡ã®1ãƒã‚¤ãƒ³ãƒˆã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

<ul>
<li>ãƒ‡ãƒ¼ã‚¿ã‚’Min-Maxã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°</li>
<li>window_size=30, horizon=1ã§ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ä½œæˆ</li>
<li>LSTMForecasterãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨</li>
<li>MSEæå¤±ã§è¨“ç·´ï¼ˆ50ã‚¨ãƒãƒƒã‚¯ï¼‰</li>
</ul>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>ä»¥ä¸‹ã®äºˆæ¸¬çµæœã«å¯¾ã—ã¦ã€MAEã€RMSEã€MAPEã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚</p>
<pre><code>y_true = [100, 110, 105, 115, 120]
y_pred = [98, 112, 107, 113, 122]
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error

y_true = np.array([100, 110, 105, 115, 120])
y_pred = np.array([98, 112, 107, 113, 122])

mae = mean_absolute_error(y_true, y_pred)
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100

print(f"MAE: {mae:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"MAPE: {mape:.2f}%")

# å‡ºåŠ›:
# MAE: 2.0000
# RMSE: 2.2361
# MAPE: 1.86%
</code></pre>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>Seq2Seqãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã€7æ—¥å…ˆã¾ã§ã®æ ªä¾¡ã‚’äºˆæ¸¬ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚éå»60æ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’å…¥åŠ›ã¨ã—ã€æœªæ¥7æ—¥ã‚’å‡ºåŠ›ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€çµæœã‚’å¯è¦–åŒ–ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

<ul>
<li>Seq2SeqLSTMãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆoutput_length=7ï¼‰</li>
<li>window_size=60, horizon=7ã§ãƒ‡ãƒ¼ã‚¿ä½œæˆ</li>
<li>è¨“ç·´å¾Œã€1ã¤ã®ã‚µãƒ³ãƒ—ãƒ«ã§äºˆæ¸¬ã‚’å¯è¦–åŒ–</li>
<li>éå»60æ—¥ã¨æœªæ¥7æ—¥ã‚’1ã¤ã®ã‚°ãƒ©ãƒ•ã«è¡¨ç¤º</li>
</ul>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Box, G. E., Jenkins, G. M., &amp; Reinsel, G. C. (2015). <em>Time Series Analysis: Forecasting and Control</em>. Wiley.</li>
<li>Hochreiter, S., &amp; Schmidhuber, J. (1997). "Long short-term memory." <em>Neural Computation</em>, 9(8), 1735-1780.</li>
<li>Sutskever, I., Vinyals, O., &amp; Le, Q. V. (2014). "Sequence to sequence learning with neural networks." <em>NeurIPS</em>.</li>
<li>Cho, K., et al. (2014). "Learning phrase representations using RNN encoder-decoder for statistical machine translation." <em>EMNLP</em>.</li>
<li>Hyndman, R. J., &amp; Athanasopoulos, G. (2018). <em>Forecasting: Principles and Practice</em>. OTexts.</li>
<li>Vaswani, A., et al. (2017). "Attention is all you need." <em>NeurIPS</em>. (Transformer)</li>
<li>Lim, B., et al. (2021). "Temporal Fusion Transformers for interpretable multi-horizon time series forecasting." <em>International Journal of Forecasting</em>.</li>
</ol>

<div class="navigation">
    <a href="chapter4-attention-mechanism.html" class="nav-button">â† å‰ã®ç« : æ³¨æ„æ©Ÿæ§‹å…¥é–€</a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a>
    <a href="../index.html" class="nav-button">æ©Ÿæ¢°å­¦ç¿’ãƒˆãƒƒãƒ— â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
