<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬4ç« ï¼šAttentionæ©Ÿæ§‹ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: var(--font-body); line-height: 1.7; color: var(--color-text); background-color: var(--color-bg); font-size: 16px; }
        header { background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; padding: var(--spacing-xl) var(--spacing-md); margin-bottom: var(--spacing-xl); box-shadow: var(--box-shadow); }
        .header-content { max-width: 900px; margin: 0 auto; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: var(--spacing-sm); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; opacity: 0.95; font-weight: 400; margin-bottom: var(--spacing-md); }
        .meta { display: flex; flex-wrap: wrap; gap: var(--spacing-md); font-size: 0.9rem; opacity: 0.9; }
        .meta-item { display: flex; align-items: center; gap: 0.3rem; }
        .container { max-width: 900px; margin: 0 auto; padding: 0 var(--spacing-md) var(--spacing-xl); }
        h2 { font-size: 1.75rem; color: var(--color-primary); margin-top: var(--spacing-xl); margin-bottom: var(--spacing-md); padding-bottom: var(--spacing-xs); border-bottom: 3px solid var(--color-accent); }
        h3 { font-size: 1.4rem; color: var(--color-primary); margin-top: var(--spacing-lg); margin-bottom: var(--spacing-sm); }
        h4 { font-size: 1.1rem; color: var(--color-primary-dark); margin-top: var(--spacing-md); margin-bottom: var(--spacing-sm); }
        p { margin-bottom: var(--spacing-md); color: var(--color-text); }
        a { color: var(--color-link); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--color-link-hover); text-decoration: underline; }
        ul, ol { margin-left: var(--spacing-lg); margin-bottom: var(--spacing-md); }
        li { margin-bottom: var(--spacing-xs); color: var(--color-text); }
        pre { background-color: var(--color-code-bg); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); overflow-x: auto; margin-bottom: var(--spacing-md); font-family: var(--font-mono); font-size: 0.9rem; line-height: 1.5; }
        code { font-family: var(--font-mono); font-size: 0.9em; background-color: var(--color-code-bg); padding: 0.2em 0.4em; border-radius: 3px; }
        pre code { background-color: transparent; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: var(--spacing-md); font-size: 0.95rem; }
        th, td { border: 1px solid var(--color-border); padding: var(--spacing-sm); text-align: left; }
        th { background-color: var(--color-bg-alt); font-weight: 600; color: var(--color-primary); }
        blockquote { border-left: 4px solid var(--color-accent); padding-left: var(--spacing-md); margin: var(--spacing-md) 0; color: var(--color-text-light); font-style: italic; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        .mermaid { text-align: center; margin: var(--spacing-lg) 0; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        details { background-color: var(--color-bg-alt); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); margin-bottom: var(--spacing-md); }
        summary { cursor: pointer; font-weight: 600; color: var(--color-primary); user-select: none; padding: var(--spacing-xs); margin: calc(-1 * var(--spacing-md)); padding: var(--spacing-md); border-radius: var(--border-radius); }
        summary:hover { background-color: rgba(123, 44, 191, 0.1); }
        details[open] summary { margin-bottom: var(--spacing-md); border-bottom: 1px solid var(--color-border); }
        .navigation { display: flex; justify-content: space-between; gap: var(--spacing-md); margin: var(--spacing-xl) 0; padding-top: var(--spacing-lg); border-top: 2px solid var(--color-border); }
        .nav-button { flex: 1; padding: var(--spacing-md); background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; border-radius: var(--border-radius); text-align: center; font-weight: 600; transition: transform 0.2s, box-shadow 0.2s; box-shadow: var(--box-shadow); }
        .nav-button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); text-decoration: none; }
        footer { margin-top: var(--spacing-xl); padding: var(--spacing-lg) var(--spacing-md); background-color: var(--color-bg-alt); border-top: 1px solid var(--color-border); text-align: center; font-size: 0.9rem; color: var(--color-text-light); }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }

        .project-box { background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 50%); border-radius: var(--border-radius); padding: var(--spacing-lg); margin: var(--spacing-lg) 0; box-shadow: var(--box-shadow); }
        @media (max-width: 768px) { h1 { font-size: 1.5rem; } h2 { font-size: 1.4rem; } h3 { font-size: 1.2rem; } .meta { font-size: 0.85rem; } .navigation { flex-direction: column; } table { font-size: 0.85rem; } th, td { padding: var(--spacing-xs); } }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/rnn-introduction/index.html">Rnn</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 4</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬4ç« ï¼šAttentionæ©Ÿæ§‹</h1>
            <p class="subtitle">ç³»åˆ—å¤‰æ›ã‚¿ã‚¹ã‚¯ã‚’é©æ–°ã—ãŸAttentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®ç†è«–ã¨å®Ÿè£…</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 23åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 6å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… Attentionæ©Ÿæ§‹ã®å¿…è¦æ€§ã¨ç†è«–çš„èƒŒæ™¯ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… Bahdanau Attentionï¼ˆAdditive Attentionï¼‰ã®ä»•çµ„ã¿ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Luong Attentionï¼ˆMultiplicative Attentionï¼‰ã®æ•°å­¦çš„å®šç¾©ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… Attentioné‡ã¿ã®å¯è¦–åŒ–ã§ãƒ¢ãƒ‡ãƒ«ã®æŒ™å‹•ã‚’è§£é‡ˆã§ãã‚‹</li>
<li>âœ… Seq2Seqãƒ¢ãƒ‡ãƒ«ã«Attentionã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã‚‹</li>
<li>âœ… Self-Attentionã®åŸºç¤æ¦‚å¿µã‚’ç†è§£ã§ãã‚‹ï¼ˆTransformeræº–å‚™ï¼‰</li>
<li>âœ… Attentionãƒ™ãƒ¼ã‚¹ã®NMTã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
</ul>

<hr>

<h2>4.1 Attentionã®å¿…è¦æ€§</h2>

<h3>Context Vectorã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯å•é¡Œ</h3>

<p>ç¬¬3ç« ã§å­¦ã‚“ã Encoder-Decoderãƒ¢ãƒ‡ãƒ«ã§ã¯ã€å…¥åŠ›ç³»åˆ—å…¨ä½“ã‚’å›ºå®šé•·ã®Context Vectorï¼ˆæ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã«åœ§ç¸®ã—ã¦ã„ã¾ã—ãŸã€‚ã“ã®è¨­è¨ˆã«ã¯ä»¥ä¸‹ã®æ·±åˆ»ãªå•é¡ŒãŒã‚ã‚Šã¾ã™ï¼š</p>

<table>
<thead>
<tr>
<th>å•é¡Œ</th>
<th>è©³ç´°</th>
<th>å½±éŸ¿</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯</strong></td>
<td>é•·ã„ç³»åˆ—ã‚’å›ºå®šé•·ãƒ™ã‚¯ãƒˆãƒ«ã«åœ§ç¸®</td>
<td>æƒ…å ±æå¤±ã€é•·æ–‡ã§ã®æ€§èƒ½ä½ä¸‹</td>
</tr>
<tr>
<td><strong>é•·è·é›¢ä¾å­˜ã®å›°é›£</strong></td>
<td>ç³»åˆ—ã®å§‹ã‚ã¨çµ‚ã‚ã‚Šã®é–¢é€£ä»˜ã‘ãŒå›°é›£</td>
<td>é•·æ–‡ç¿»è¨³ã®ç²¾åº¦ä½ä¸‹</td>
</tr>
<tr>
<td><strong>ä¸€æ§˜ãªé‡è¦åº¦</strong></td>
<td>å…¨å˜èªã‚’åŒã˜é‡ã¿ã§æ‰±ã†</td>
<td>é‡è¦ãªå˜èªã‚’å¼·èª¿ã§ããªã„</td>
</tr>
<tr>
<td><strong>è§£é‡ˆæ€§ã®æ¬ å¦‚</strong></td>
<td>ãƒ¢ãƒ‡ãƒ«ã®åˆ¤æ–­æ ¹æ‹ ãŒä¸æ˜</td>
<td>ãƒ‡ãƒãƒƒã‚°ã‚„æ”¹å–„ãŒå›°é›£</td>
</tr>
</tbody>
</table>

<h3>Attentionã«ã‚ˆã‚‹è§£æ±ºç­–</h3>

<p>Attentionæ©Ÿæ§‹ã¯ã€DecoderãŒå„æ™‚åˆ»ã§ã€Œå…¥åŠ›ç³»åˆ—ã®ã©ã®éƒ¨åˆ†ã«æ³¨ç›®ã™ã¹ãã‹ã€ã‚’å‹•çš„ã«å­¦ç¿’ã—ã¾ã™ã€‚å›ºå®šé•·ã®Context Vectorã§ã¯ãªãã€å„æ™‚åˆ»ã§ç•°ãªã‚‹Context Vectorã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã§ä¸Šè¨˜ã®å•é¡Œã‚’è§£æ±ºã—ã¾ã™ã€‚</p>

<div class="mermaid">
graph TB
    subgraph "å¾“æ¥ã®Seq2Seqï¼ˆå›ºå®šContextï¼‰"
        A1[å…¥åŠ›: I love AI] --> E1[Encoder]
        E1 --> C1[å›ºå®šContext Vector]
        C1 --> D1[Decoder]
        D1 --> O1[å‡ºåŠ›: ç§ã¯AIãŒå¥½ã]

        style C1 fill:#e74c3c,color:#fff
    end

    subgraph "Attentionä»˜ãSeq2Seqï¼ˆå‹•çš„Contextï¼‰"
        A2[å…¥åŠ›: I love AI] --> E2[Encoder]
        E2 --> H1[hidden states]
        H1 --> ATT[Attentionæ©Ÿæ§‹]
        D2[Decoder state] --> ATT
        ATT --> C2[å‹•çš„Context t=1]
        ATT --> C3[å‹•çš„Context t=2]
        ATT --> C4[å‹•çš„Context t=3]
        C2 --> D2
        C3 --> D2
        C4 --> D2
        D2 --> O2[å‡ºåŠ›: ç§ã¯AIãŒå¥½ã]

        style ATT fill:#27ae60,color:#fff
    end
</div>

<blockquote>
<p><strong>é‡è¦</strong>: Attentionã¯ã€Œã©ã“ã‚’è¦‹ã‚‹ã¹ãã‹ã€ã‚’å­¦ç¿’ã™ã‚‹æ©Ÿæ§‹ã§ã™ã€‚äººé–“ãŒæ–‡ç« ã‚’èª­ã‚€ã¨ãã«é‡è¦ãªéƒ¨åˆ†ã«æ³¨æ„ã‚’æ‰•ã†ã®ã¨åŒã˜ã‚ˆã†ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚‚å…¥åŠ›ç³»åˆ—ã®é‡è¦ãªéƒ¨åˆ†ã«é‡ã¿ã‚’ç½®ãã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>4.2 Bahdanau Attentionï¼ˆAdditive Attentionï¼‰</h2>

<h3>4.2.1 åŸºæœ¬æ¦‚å¿µã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h3>

<p>Bahdanau Attentionï¼ˆ2014å¹´ææ¡ˆï¼‰ã¯ã€æœ€åˆã«åºƒãæ¡ç”¨ã•ã‚ŒãŸAttentionæ©Ÿæ§‹ã§ã™ã€‚Additive Attentionã¨ã‚‚å‘¼ã°ã‚Œã€Encoderã¨Decoderã®éš ã‚ŒçŠ¶æ…‹ã‚’åŠ ç®—çš„ã«çµ„ã¿åˆã‚ã›ã¾ã™ã€‚</p>

<h4>æ•°å­¦çš„å®šç¾©</h4>

<p>æ™‚åˆ» $t$ ã«ãŠã‘ã‚‹Attentioné‡ã¿ã®è¨ˆç®—ï¼š</p>

<p><strong>ã‚¹ãƒ†ãƒƒãƒ—1: Alignment Scoreï¼ˆã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ï¼‰</strong></p>
$$
e_{ti} = v_a^\top \tanh(W_a s_{t-1} + U_a h_i)
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$s_{t-1}$: Decoderã®å‰æ™‚åˆ»ã®éš ã‚ŒçŠ¶æ…‹ï¼ˆQueryï¼‰</li>
<li>$h_i$: Encoderã® $i$ ç•ªç›®ã®éš ã‚ŒçŠ¶æ…‹ï¼ˆKeyï¼‰</li>
<li>$W_a, U_a$: å­¦ç¿’å¯èƒ½ãªé‡ã¿è¡Œåˆ—</li>
<li>$v_a$: å­¦ç¿’å¯èƒ½ãªé‡ã¿ãƒ™ã‚¯ãƒˆãƒ«</li>
</ul>

<p><strong>ã‚¹ãƒ†ãƒƒãƒ—2: Attention Weightï¼ˆæ³¨æ„é‡ã¿ï¼‰</strong></p>
$$
\alpha_{ti} = \frac{\exp(e_{ti})}{\sum_{j=1}^{T_x} \exp(e_{tj})}
$$

<p>Softmaxé–¢æ•°ã«ã‚ˆã‚Šã€å…¨æ™‚åˆ»ã®é‡ã¿ã®åˆè¨ˆãŒ1ã«ãªã‚‹ã‚ˆã†ã«æ­£è¦åŒ–ã•ã‚Œã¾ã™ã€‚</p>

<p><strong>ã‚¹ãƒ†ãƒƒãƒ—3: Context Vectorï¼ˆæ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰</strong></p>
$$
c_t = \sum_{i=1}^{T_x} \alpha_{ti} h_i
$$

<p>Attentioné‡ã¿ã‚’ä½¿ã£ã¦Encoderéš ã‚ŒçŠ¶æ…‹ã®é‡ã¿ä»˜ãå’Œã‚’è¨ˆç®—ã—ã¾ã™ã€‚</p>

<h3>4.2.2 PyTorchã«ã‚ˆã‚‹å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt

class BahdanauAttention(nn.Module):
    """Bahdanau Attentionï¼ˆAdditive Attentionï¼‰ã®å®Ÿè£…"""

    def __init__(self, hidden_size):
        super(BahdanauAttention, self).__init__()
        self.hidden_size = hidden_size

        # Attentionç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.W_a = nn.Linear(hidden_size, hidden_size, bias=False)  # Decoderç”¨
        self.U_a = nn.Linear(hidden_size, hidden_size, bias=False)  # Encoderç”¨
        self.v_a = nn.Linear(hidden_size, 1, bias=False)            # ã‚¹ã‚«ãƒ©ãƒ¼å¤‰æ›

    def forward(self, decoder_hidden, encoder_outputs):
        """
        Args:
            decoder_hidden: Decoderã®éš ã‚ŒçŠ¶æ…‹ [batch, hidden_size]
            encoder_outputs: Encoderã®å…¨éš ã‚ŒçŠ¶æ…‹ [batch, seq_len, hidden_size]

        Returns:
            context: Context vector [batch, hidden_size]
            attention_weights: Attentioné‡ã¿ [batch, seq_len]
        """
        batch_size = encoder_outputs.size(0)
        seq_len = encoder_outputs.size(1)

        # Decoder hiddenã‚’å„æ™‚åˆ»ã§ç¹°ã‚Šè¿”ã— [batch, seq_len, hidden_size]
        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)

        # Alignment scoreè¨ˆç®—: e_ti = v_a^T * tanh(W_a * s_t + U_a * h_i)
        energy = torch.tanh(self.W_a(decoder_hidden) + self.U_a(encoder_outputs))
        alignment_scores = self.v_a(energy).squeeze(-1)  # [batch, seq_len]

        # Softmaxã§æ­£è¦åŒ–ã—ã¦Attention weightå–å¾—
        attention_weights = F.softmax(alignment_scores, dim=1)  # [batch, seq_len]

        # Context vectorè¨ˆç®—: c_t = Î£ Î±_ti * h_i
        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)
        context = context.squeeze(1)  # [batch, hidden_size]

        return context, attention_weights


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== Bahdanau Attention ãƒ‡ãƒ¢ ===\n")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
batch_size = 2
seq_len = 5
hidden_size = 8

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
encoder_outputs = torch.randn(batch_size, seq_len, hidden_size)
decoder_hidden = torch.randn(batch_size, hidden_size)

# Attentioné©ç”¨
attention = BahdanauAttention(hidden_size)
context, weights = attention(decoder_hidden, encoder_outputs)

print(f"Encoder outputs shape: {encoder_outputs.shape}")
print(f"Decoder hidden shape: {decoder_hidden.shape}")
print(f"Context vector shape: {context.shape}")
print(f"Attention weights shape: {weights.shape}")
print(f"\nAttention weights (batch 0):")
print(weights[0].detach().numpy())
print(f"åˆè¨ˆ: {weights[0].sum().item():.4f} (Should be 1.0)")

# Attentioné‡ã¿ã®å¯è¦–åŒ–
fig, ax = plt.subplots(1, 1, figsize=(10, 4))

# 1ãƒãƒƒãƒç›®ã®Attentioné‡ã¿ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤º
weights_np = weights[0].detach().numpy()
positions = np.arange(seq_len)

ax.bar(positions, weights_np, color='#7b2cbf', alpha=0.7, edgecolor='black')
ax.set_xlabel('Encoder Position', fontsize=12, fontweight='bold')
ax.set_ylabel('Attention Weight', fontsize=12, fontweight='bold')
ax.set_title('Bahdanau Attention Weights Distribution', fontsize=14, fontweight='bold')
ax.set_xticks(positions)
ax.set_xticklabels([f't={i+1}' for i in positions])
ax.grid(axis='y', alpha=0.3)

# å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
for i, v in enumerate(weights_np):
    ax.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Bahdanau Attention ãƒ‡ãƒ¢ ===

Encoder outputs shape: torch.Size([2, 5, 8])
Decoder hidden shape: torch.Size([2, 8])
Context vector shape: torch.Size([2, 8])
Attention weights shape: torch.Size([2, 5])

Attention weights (batch 0):
[0.178 0.245 0.198 0.156 0.223]
åˆè¨ˆ: 1.0000 (Should be 1.0)
</code></pre>

<hr>

<h2>4.3 Luong Attentionï¼ˆMultiplicative Attentionï¼‰</h2>

<h3>4.3.1 Bahdanauã¨Luongã®é•ã„</h3>

<p>Luong Attentionï¼ˆ2015å¹´ææ¡ˆï¼‰ã¯ã€Bahdanauã¨ç•°ãªã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§Alignment Scoreã‚’è¨ˆç®—ã—ã¾ã™ã€‚</p>

<table>
<thead>
<tr>
<th>ç‰¹æ€§</th>
<th>Bahdanau Attention</th>
<th>Luong Attention</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ææ¡ˆå¹´</strong></td>
<td>2014å¹´</td>
<td>2015å¹´</td>
</tr>
<tr>
<td><strong>åˆ¥å</strong></td>
<td>Additive Attention</td>
<td>Multiplicative Attention</td>
</tr>
<tr>
<td><strong>Scoreè¨ˆç®—</strong></td>
<td>$v_a^\top \tanh(W_a s_t + U_a h_i)$</td>
<td>$s_t^\top W_a h_i$ (general)</td>
</tr>
<tr>
<td><strong>DecoderçŠ¶æ…‹</strong></td>
<td>å‰æ™‚åˆ»ã®çŠ¶æ…‹ $s_{t-1}$ ã‚’ä½¿ç”¨</td>
<td>ç¾åœ¨ã®çŠ¶æ…‹ $s_t$ ã‚’ä½¿ç”¨</td>
</tr>
<tr>
<td><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆ</strong></td>
<td>ã‚„ã‚„é«˜ã„ï¼ˆtanhæ¼”ç®—ï¼‰</td>
<td>ä½ã„ï¼ˆå†…ç©ã®ã¿ï¼‰</td>
</tr>
<tr>
<td><strong>æ€§èƒ½</strong></td>
<td>å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§å„ªä½</td>
<td>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§å„ªä½</td>
</tr>
</tbody>
</table>

<h3>4.3.2 Luong Attentionã®3ã¤ã®ã‚¹ã‚³ã‚¢é–¢æ•°</h3>

<p>Luongã¯3ç¨®é¡ã®Alignment Scoreè¨ˆç®—æ–¹æ³•ã‚’ææ¡ˆã—ã¾ã—ãŸï¼š</p>

<p><strong>1. Dotï¼ˆå†…ç©ï¼‰</strong></p>
$$
\text{score}(s_t, h_i) = s_t^\top h_i
$$

<p><strong>2. Generalï¼ˆä¸€èˆ¬åŒ–å†…ç©ï¼‰</strong></p>
$$
\text{score}(s_t, h_i) = s_t^\top W_a h_i
$$

<p><strong>3. Concatï¼ˆé€£çµï¼‰</strong></p>
$$
\text{score}(s_t, h_i) = v_a^\top \tanh(W_a [s_t; h_i])
$$

<h3>4.3.3 å®Ÿè£…ã¨Bahdanauã¨ã®æ¯”è¼ƒ</h3>

<pre><code class="language-python">class LuongAttention(nn.Module):
    """Luong Attentionï¼ˆMultiplicative Attentionï¼‰ã®å®Ÿè£…"""

    def __init__(self, hidden_size, score_type='general'):
        super(LuongAttention, self).__init__()
        self.hidden_size = hidden_size
        self.score_type = score_type

        if score_type == 'general':
            self.W_a = nn.Linear(hidden_size, hidden_size, bias=False)
        elif score_type == 'concat':
            self.W_a = nn.Linear(hidden_size * 2, hidden_size, bias=False)
            self.v_a = nn.Linear(hidden_size, 1, bias=False)
        elif score_type == 'dot':
            pass  # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸è¦
        else:
            raise ValueError(f"Unknown score_type: {score_type}")

    def forward(self, decoder_hidden, encoder_outputs):
        """
        Args:
            decoder_hidden: [batch, hidden_size]
            encoder_outputs: [batch, seq_len, hidden_size]

        Returns:
            context: [batch, hidden_size]
            attention_weights: [batch, seq_len]
        """
        batch_size = encoder_outputs.size(0)
        seq_len = encoder_outputs.size(1)

        if self.score_type == 'dot':
            # s_t^T * h_i
            alignment_scores = torch.bmm(
                encoder_outputs,  # [batch, seq_len, hidden]
                decoder_hidden.unsqueeze(2)  # [batch, hidden, 1]
            ).squeeze(2)  # [batch, seq_len]

        elif self.score_type == 'general':
            # s_t^T * W_a * h_i
            transformed = self.W_a(encoder_outputs)  # [batch, seq_len, hidden]
            alignment_scores = torch.bmm(
                transformed,
                decoder_hidden.unsqueeze(2)
            ).squeeze(2)

        elif self.score_type == 'concat':
            # v_a^T * tanh(W_a * [s_t; h_i])
            decoder_repeated = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)
            concat = torch.cat([decoder_repeated, encoder_outputs], dim=2)
            energy = torch.tanh(self.W_a(concat))
            alignment_scores = self.v_a(energy).squeeze(-1)

        # Softmaxã§æ­£è¦åŒ–
        attention_weights = F.softmax(alignment_scores, dim=1)

        # Context vectorè¨ˆç®—
        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)
        context = context.squeeze(1)

        return context, attention_weights


# 3ã¤ã®ã‚¹ã‚³ã‚¢é–¢æ•°ã®æ¯”è¼ƒ
print("\n=== Luong Attention 3ã¤ã®ã‚¹ã‚³ã‚¢é–¢æ•°ã®æ¯”è¼ƒ ===\n")

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
batch_size = 1
seq_len = 6
hidden_size = 4
encoder_outputs = torch.randn(batch_size, seq_len, hidden_size)
decoder_hidden = torch.randn(batch_size, hidden_size)

score_types = ['dot', 'general', 'concat']
results = {}

fig, axes = plt.subplots(1, 3, figsize=(15, 4))

for idx, score_type in enumerate(score_types):
    attention = LuongAttention(hidden_size, score_type=score_type)
    context, weights = attention(decoder_hidden, encoder_outputs)
    results[score_type] = weights[0].detach().numpy()

    print(f"{score_type.upper()} score:")
    print(f"  Weights: {results[score_type]}")
    print(f"  Sum: {results[score_type].sum():.4f}\n")

    # å¯è¦–åŒ–
    ax = axes[idx]
    positions = np.arange(seq_len)
    ax.bar(positions, results[score_type], color='#7b2cbf', alpha=0.7, edgecolor='black')
    ax.set_title(f'{score_type.upper()} Score', fontsize=12, fontweight='bold')
    ax.set_xlabel('Encoder Position', fontsize=10)
    ax.set_ylabel('Attention Weight', fontsize=10)
    ax.set_xticks(positions)
    ax.set_xticklabels([f't={i+1}' for i in positions], fontsize=9)
    ax.set_ylim([0, max(results[score_type]) * 1.2])
    ax.grid(axis='y', alpha=0.3)

    # å€¤è¡¨ç¤º
    for i, v in enumerate(results[score_type]):
        ax.text(i, v + 0.005, f'{v:.3f}', ha='center', va='bottom', fontsize=8)

plt.suptitle('Luong Attention: Score Function Comparison',
             fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()

# Bahdanauã¨Luongã®æ¯”è¼ƒ
print("\n=== Bahdanau vs Luong Attention ===\n")

bahdanau_attn = BahdanauAttention(hidden_size)
luong_attn = LuongAttention(hidden_size, score_type='general')

_, bahdanau_weights = bahdanau_attn(decoder_hidden, encoder_outputs)
_, luong_weights = luong_attn(decoder_hidden, encoder_outputs)

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Bahdanau
ax1 = axes[0]
positions = np.arange(seq_len)
bahdanau_np = bahdanau_weights[0].detach().numpy()
ax1.bar(positions, bahdanau_np, color='#e74c3c', alpha=0.7, edgecolor='black')
ax1.set_title('Bahdanau Attention\n(Additive)', fontsize=12, fontweight='bold')
ax1.set_xlabel('Encoder Position', fontsize=10)
ax1.set_ylabel('Attention Weight', fontsize=10)
ax1.set_xticks(positions)
ax1.set_ylim([0, 0.3])
ax1.grid(axis='y', alpha=0.3)

# Luong
ax2 = axes[1]
luong_np = luong_weights[0].detach().numpy()
ax2.bar(positions, luong_np, color='#27ae60', alpha=0.7, edgecolor='black')
ax2.set_title('Luong Attention\n(Multiplicative - General)', fontsize=12, fontweight='bold')
ax2.set_xlabel('Encoder Position', fontsize=10)
ax2.set_ylabel('Attention Weight', fontsize=10)
ax2.set_xticks(positions)
ax2.set_ylim([0, 0.3])
ax2.grid(axis='y', alpha=0.3)

plt.suptitle('Bahdanau vs Luong: Attention Weight Comparison',
             fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Luong Attention 3ã¤ã®ã‚¹ã‚³ã‚¢é–¢æ•°ã®æ¯”è¼ƒ ===

DOT score:
  Weights: [0.142 0.189 0.165 0.178 0.154 0.172]
  Sum: 1.0000

GENERAL score:
  Weights: [0.158 0.172 0.169 0.165 0.161 0.175]
  Sum: 1.0000

CONCAT score:
  Weights: [0.167 0.171 0.164 0.169 0.162 0.167]
  Sum: 1.0000
</code></pre>

<hr>

<h2>4.4 Attentioné‡ã¿ã®å¯è¦–åŒ–ã¨è§£é‡ˆ</h2>

<h3>4.4.1 ç¿»è¨³ã‚¿ã‚¹ã‚¯ã§ã®Attentionå¯è¦–åŒ–</h3>

<p>Attentionã®æœ€å¤§ã®åˆ©ç‚¹ã®1ã¤ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒã©ã®å…¥åŠ›å˜èªã«æ³¨ç›®ã—ã¦å‡ºåŠ›ã‚’ç”Ÿæˆã—ãŸã‹ã‚’è¦–è¦šçš„ã«ç¢ºèªã§ãã‚‹ã“ã¨ã§ã™ã€‚</p>

<pre><code class="language-python">import matplotlib.pyplot as plt
import seaborn as sns

def visualize_attention(attention_weights, source_tokens, target_tokens,
                       title='Attention Visualization'):
    """
    Attentioné‡ã¿ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–

    Args:
        attention_weights: [target_len, source_len] ã®Attentioné‡ã¿è¡Œåˆ—
        source_tokens: å…¥åŠ›å˜èªãƒªã‚¹ãƒˆ
        target_tokens: å‡ºåŠ›å˜èªãƒªã‚¹ãƒˆ
        title: ã‚°ãƒ©ãƒ•ã®ã‚¿ã‚¤ãƒˆãƒ«
    """
    fig, ax = plt.subplots(figsize=(10, 8))

    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æç”»
    sns.heatmap(attention_weights,
                xticklabels=source_tokens,
                yticklabels=target_tokens,
                cmap='YlOrRd',
                cbar_kws={'label': 'Attention Weight'},
                ax=ax,
                annot=True,  # å€¤ã‚’è¡¨ç¤º
                fmt='.3f',   # å°æ•°ç‚¹3æ¡
                linewidths=0.5,
                linecolor='gray')

    ax.set_xlabel('Source (Input)', fontsize=12, fontweight='bold')
    ax.set_ylabel('Target (Output)', fontsize=12, fontweight='bold')
    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)

    plt.tight_layout()
    plt.show()


# æ©Ÿæ¢°ç¿»è¨³ã®ä¾‹ï¼ˆè‹±èªâ†’æ—¥æœ¬èªï¼‰
print("=== Attention Visualization Example: Machine Translation ===\n")

# ä¾‹: "I love deep learning" â†’ "ç§ã¯æ·±å±¤å­¦ç¿’ãŒå¥½ãã§ã™"
source_tokens = ['I', 'love', 'deep', 'learning', '<EOS>']
target_tokens = ['ç§ã¯', 'æ·±å±¤', 'å­¦ç¿’ãŒ', 'å¥½ãã§ã™', '<EOS>']

# æ¨¡æ“¬çš„ãªAttentioné‡ã¿è¡Œåˆ—ï¼ˆå®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å–å¾—ã™ã‚‹å€¤ï¼‰
# å„è¡ŒãŒå„å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆæ™‚ã®Attentionåˆ†å¸ƒã‚’è¡¨ã™
attention_matrix = np.array([
    [0.82, 0.05, 0.03, 0.05, 0.05],  # "ç§ã¯" ã‚’ç”Ÿæˆæ™‚ â†’ "I" ã«å¼·ãæ³¨ç›®
    [0.05, 0.08, 0.70, 0.12, 0.05],  # "æ·±å±¤" ã‚’ç”Ÿæˆæ™‚ â†’ "deep" ã«å¼·ãæ³¨ç›®
    [0.03, 0.05, 0.15, 0.72, 0.05],  # "å­¦ç¿’ãŒ" ã‚’ç”Ÿæˆæ™‚ â†’ "learning" ã«å¼·ãæ³¨ç›®
    [0.05, 0.75, 0.08, 0.07, 0.05],  # "å¥½ãã§ã™" ã‚’ç”Ÿæˆæ™‚ â†’ "love" ã«å¼·ãæ³¨ç›®
    [0.05, 0.05, 0.05, 0.05, 0.80],  # "<EOS>" ã‚’ç”Ÿæˆæ™‚ â†’ "<EOS>" ã«å¼·ãæ³¨ç›®
])

visualize_attention(attention_matrix, source_tokens, target_tokens,
                   title='Attention Weights: English â†’ Japanese Translation')

# ã‚ˆã‚Šè¤‡é›‘ãªä¾‹
print("\n=== é•·æ–‡ç¿»è¨³ã§ã®Attentionãƒ‘ã‚¿ãƒ¼ãƒ³ ===\n")

source_long = ['The', 'cat', 'sat', 'on', 'the', 'mat', '<EOS>']
target_long = ['ãã®', 'çŒ«ãŒ', 'ãƒãƒƒãƒˆã®', 'ä¸Šã«', 'åº§ã£ãŸ', '<EOS>']

# èªé †ãŒç•°ãªã‚‹å ´åˆã®Attention
attention_long = np.array([
    [0.65, 0.10, 0.05, 0.05, 0.10, 0.03, 0.02],  # "ãã®" â†’ "The"
    [0.10, 0.70, 0.05, 0.05, 0.05, 0.03, 0.02],  # "çŒ«ãŒ" â†’ "cat"
    [0.05, 0.05, 0.05, 0.05, 0.10, 0.68, 0.02],  # "ãƒãƒƒãƒˆã®" â†’ "mat"
    [0.05, 0.05, 0.10, 0.75, 0.02, 0.02, 0.01],  # "ä¸Šã«" â†’ "on"
    [0.05, 0.10, 0.70, 0.05, 0.05, 0.03, 0.02],  # "åº§ã£ãŸ" â†’ "sat"
    [0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.88],  # "<EOS>" â†’ "<EOS>"
])

visualize_attention(attention_long, source_long, target_long,
                   title='Attention Weights: Word Order Reordering (ENâ†’JA)')

print("å¯è¦–åŒ–ã‹ã‚‰è¦³å¯Ÿã§ãã‚‹ãƒã‚¤ãƒ³ãƒˆ:")
print("âœ“ å¯¾è§’ç·šã«è¿‘ã„åˆ†å¸ƒ: èªé †ãŒè¿‘ã„è¨€èªå¯¾ï¼ˆè‹±ç‹¬ãªã©ï¼‰")
print("âœ“ éå¯¾è§’ç·šã®åˆ†å¸ƒ: èªé †ãŒç•°ãªã‚‹è¨€èªå¯¾ï¼ˆè‹±æ—¥ãªã©ï¼‰")
print("âœ“ è¤‡æ•°å˜èªã¸ã®åˆ†æ•£: 1å¯¾å¤šã€å¤šå¯¾1ã®å˜èªå¯¾å¿œ")
print("âœ“ EOSè¨˜å·ã¸ã®é›†ä¸­: æ–‡æœ«å‡¦ç†ã®æ˜ç¢ºåŒ–")
</code></pre>

<h3>4.4.2 Attentioné‡ã¿ã®çµ±è¨ˆåˆ†æ</h3>

<pre><code class="language-python">def analyze_attention_statistics(attention_weights):
    """Attentioné‡ã¿ã®çµ±è¨ˆçš„ç‰¹æ€§ã‚’åˆ†æ"""

    print("=== Attention Statistics Analysis ===\n")

    # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ï¼ˆé›†ä¸­åº¦ã®æŒ‡æ¨™ï¼‰
    epsilon = 1e-10
    entropy = -np.sum(attention_weights * np.log(attention_weights + epsilon), axis=1)

    # æœ€å¤§é‡ã¿
    max_weights = np.max(attention_weights, axis=1)

    # æœ‰åŠ¹ãªæ³¨ç›®ä½ç½®æ•°ï¼ˆé‡ã¿ > é–¾å€¤ï¼‰
    threshold = 0.1
    effective_positions = np.sum(attention_weights > threshold, axis=1)

    print(f"å„æ™‚åˆ»ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: {entropy}")
    print(f"  å¹³å‡: {entropy.mean():.4f}, æ¨™æº–åå·®: {entropy.std():.4f}")
    print(f"\nå„æ™‚åˆ»ã®æœ€å¤§é‡ã¿: {max_weights}")
    print(f"  å¹³å‡: {max_weights.mean():.4f}, æ¨™æº–åå·®: {max_weights.std():.4f}")
    print(f"\næœ‰åŠ¹æ³¨ç›®ä½ç½®æ•°ï¼ˆé‡ã¿ > {threshold}ï¼‰: {effective_positions}")
    print(f"  å¹³å‡: {effective_positions.mean():.2f}")

    # å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
    axes[0].plot(entropy, marker='o', color='#7b2cbf', linewidth=2, markersize=8)
    axes[0].set_xlabel('Target Position', fontsize=11, fontweight='bold')
    axes[0].set_ylabel('Entropy', fontsize=11, fontweight='bold')
    axes[0].set_title('Attention Concentration\n(Lower = More Focused)',
                     fontsize=12, fontweight='bold')
    axes[0].grid(alpha=0.3)

    # æœ€å¤§é‡ã¿
    axes[1].plot(max_weights, marker='s', color='#e74c3c', linewidth=2, markersize=8)
    axes[1].set_xlabel('Target Position', fontsize=11, fontweight='bold')
    axes[1].set_ylabel('Max Weight', fontsize=11, fontweight='bold')
    axes[1].set_title('Maximum Attention Weight\n(Higher = Strong Focus)',
                     fontsize=12, fontweight='bold')
    axes[1].grid(alpha=0.3)

    # æœ‰åŠ¹ä½ç½®æ•°
    axes[2].bar(range(len(effective_positions)), effective_positions,
               color='#27ae60', alpha=0.7, edgecolor='black')
    axes[2].set_xlabel('Target Position', fontsize=11, fontweight='bold')
    axes[2].set_ylabel('Num. Effective Positions', fontsize=11, fontweight='bold')
    axes[2].set_title(f'Attention Spread\n(Threshold = {threshold})',
                     fontsize=12, fontweight='bold')
    axes[2].grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.show()

# åˆ†æå®Ÿè¡Œ
analyze_attention_statistics(attention_long)
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Attention Statistics Analysis ===

å„æ™‚åˆ»ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: [1.086 0.897 0.935 0.735 0.901 0.412]
  å¹³å‡: 0.8277, æ¨™æº–åå·®: 0.2194

å„æ™‚åˆ»ã®æœ€å¤§é‡ã¿: [0.65 0.70 0.68 0.75 0.70 0.88]
  å¹³å‡: 0.7267, æ¨™æº–åå·®: 0.0737

æœ‰åŠ¹æ³¨ç›®ä½ç½®æ•°ï¼ˆé‡ã¿ > 0.1ï¼‰: [4 3 2 2 3 1]
  å¹³å‡: 2.50
</code></pre>

<blockquote>
<p><strong>è§£é‡ˆã‚¬ã‚¤ãƒ‰</strong>:<br>
ãƒ»<strong>ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼</strong>: ç‰¹å®šä½ç½®ã¸ã®å¼·ã„é›†ä¸­ï¼ˆæ–‡æœ«ã®EOSç”Ÿæˆæ™‚ãªã©ï¼‰<br>
ãƒ»<strong>é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼</strong>: è¤‡æ•°ä½ç½®ã¸ã®åˆ†æ•£ï¼ˆè¤‡åˆèªã‚„å¥ã®å‡¦ç†æ™‚ãªã©ï¼‰<br>
ãƒ»<strong>é«˜æœ€å¤§é‡ã¿</strong>: æ˜ç¢ºãªå¯¾å¿œé–¢ä¿‚ï¼ˆ1å¯¾1ã®å˜èªç¿»è¨³ãªã©ï¼‰<br>
ãƒ»<strong>æœ‰åŠ¹ä½ç½®æ•°</strong>: æ–‡è„ˆæƒ…å ±ã®åºƒãŒã‚Šï¼ˆå¤šã„ã»ã©åºƒç¯„ãªæ–‡è„ˆåˆ©ç”¨ï¼‰</p>
</blockquote>

<hr>

<h2>4.5 Attentionä»˜ãSeq2Seqãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…</h2>

<h3>4.5.1 å®Œå…¨ãªEncoder-Decoder with Attention</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
import random

class AttentionEncoder(nn.Module):
    """Attentionç”¨ã®Encoderï¼ˆéš ã‚ŒçŠ¶æ…‹ã‚’å…¨ã¦ä¿å­˜ï¼‰"""

    def __init__(self, input_size, hidden_size, num_layers=1):
        super(AttentionEncoder, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.embedding = nn.Embedding(input_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size, num_layers,
                         batch_first=True)

    def forward(self, x):
        """
        Args:
            x: [batch, seq_len]
        Returns:
            outputs: [batch, seq_len, hidden_size]
            hidden: [num_layers, batch, hidden_size]
        """
        embedded = self.embedding(x)  # [batch, seq_len, hidden_size]
        outputs, hidden = self.gru(embedded)
        return outputs, hidden


class AttentionDecoder(nn.Module):
    """Attentionæ©Ÿæ§‹ã‚’æŒã¤Decoder"""

    def __init__(self, output_size, hidden_size, attention_type='bahdanau', num_layers=1):
        super(AttentionDecoder, self).__init__()
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.num_layers = num_layers

        self.embedding = nn.Embedding(output_size, hidden_size)

        # Attentionæ©Ÿæ§‹
        if attention_type == 'bahdanau':
            self.attention = BahdanauAttention(hidden_size)
        elif attention_type == 'luong':
            self.attention = LuongAttention(hidden_size, score_type='general')
        else:
            raise ValueError(f"Unknown attention type: {attention_type}")

        # GRUå…¥åŠ› = embedding + context
        self.gru = nn.GRU(hidden_size * 2, hidden_size, num_layers,
                         batch_first=True)

        # å‡ºåŠ›å±¤
        self.out = nn.Linear(hidden_size, output_size)

    def forward(self, input_token, hidden, encoder_outputs):
        """
        Args:
            input_token: [batch, 1]
            hidden: [num_layers, batch, hidden_size]
            encoder_outputs: [batch, source_len, hidden_size]
        Returns:
            output: [batch, output_size]
            hidden: [num_layers, batch, hidden_size]
            attention_weights: [batch, source_len]
        """
        # Embedding
        embedded = self.embedding(input_token)  # [batch, 1, hidden_size]

        # Attentionè¨ˆç®—ï¼ˆæœ€å¾Œã®å±¤ã®hiddenã‚’ä½¿ç”¨ï¼‰
        query = hidden[-1]  # [batch, hidden_size]
        context, attention_weights = self.attention(query, encoder_outputs)

        # Context vectorã¨embeddingã‚’çµåˆ
        context = context.unsqueeze(1)  # [batch, 1, hidden_size]
        gru_input = torch.cat([embedded, context], dim=2)  # [batch, 1, hidden*2]

        # GRU
        gru_output, hidden = self.gru(gru_input, hidden)

        # å‡ºåŠ›
        output = self.out(gru_output.squeeze(1))  # [batch, output_size]

        return output, hidden, attention_weights


class Seq2SeqWithAttention(nn.Module):
    """Attentionä»˜ãSeq2Seqãƒ¢ãƒ‡ãƒ«"""

    def __init__(self, encoder, decoder, device):
        super(Seq2SeqWithAttention, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, source, target, teacher_forcing_ratio=0.5):
        """
        Args:
            source: [batch, source_len]
            target: [batch, target_len]
            teacher_forcing_ratio: Teacher forcingã‚’ä½¿ã†ç¢ºç‡
        Returns:
            outputs: [batch, target_len, output_size]
            all_attention_weights: [batch, target_len, source_len]
        """
        batch_size = source.size(0)
        target_len = target.size(1)
        target_vocab_size = self.decoder.output_size

        # å‡ºåŠ›ã¨Attentioné‡ã¿ã‚’ä¿å­˜
        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)
        all_attention_weights = torch.zeros(batch_size, target_len, source.size(1)).to(self.device)

        # Encoder
        encoder_outputs, hidden = self.encoder(source)

        # DecoderåˆæœŸå…¥åŠ›ï¼ˆ<SOS>ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
        decoder_input = target[:, 0].unsqueeze(1)  # [batch, 1]

        # Decoderå„æ™‚åˆ»
        for t in range(1, target_len):
            output, hidden, attention_weights = self.decoder(
                decoder_input, hidden, encoder_outputs
            )

            outputs[:, t, :] = output
            all_attention_weights[:, t, :] = attention_weights

            # Teacher forcing
            use_teacher_forcing = random.random() < teacher_forcing_ratio
            if use_teacher_forcing:
                decoder_input = target[:, t].unsqueeze(1)
            else:
                decoder_input = output.argmax(1).unsqueeze(1)

        return outputs, all_attention_weights


# ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã¨ãƒ†ã‚¹ãƒˆ
print("=== Seq2Seq with Attention Model Test ===\n")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
INPUT_VOCAB = 100
OUTPUT_VOCAB = 100
HIDDEN_SIZE = 128
BATCH_SIZE = 4
SOURCE_LEN = 10
TARGET_LEN = 12

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
encoder = AttentionEncoder(INPUT_VOCAB, HIDDEN_SIZE).to(device)
decoder = AttentionDecoder(OUTPUT_VOCAB, HIDDEN_SIZE, attention_type='bahdanau').to(device)
model = Seq2SeqWithAttention(encoder, decoder, device).to(device)

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
source = torch.randint(0, INPUT_VOCAB, (BATCH_SIZE, SOURCE_LEN)).to(device)
target = torch.randint(0, OUTPUT_VOCAB, (BATCH_SIZE, TARGET_LEN)).to(device)

# Forward pass
outputs, attention_weights = model(source, target)

print(f"Input shape: {source.shape}")
print(f"Target shape: {target.shape}")
print(f"Output shape: {outputs.shape}")
print(f"Attention weights shape: {attention_weights.shape}")
print(f"\nãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}")

# 1ã‚µãƒ³ãƒ—ãƒ«ã®Attentioné‡ã¿ã‚’å¯è¦–åŒ–
sample_attention = attention_weights[0].detach().cpu().numpy()
print(f"\nSample attention weights (target_len={TARGET_LEN}, source_len={SOURCE_LEN}):")
print(f"Shape: {sample_attention.shape}")
print(f"å„æ™‚åˆ»ã®åˆè¨ˆ: {sample_attention.sum(axis=1)}")  # å…¨ã¦1.0ã«ãªã‚‹ã¯ãš
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Seq2Seq with Attention Model Test ===

Input shape: torch.Size([4, 10])
Target shape: torch.Size([4, 12])
Output shape: torch.Size([4, 12, 100])
Attention weights shape: torch.Size([4, 12, 10])

ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 49,700

Sample attention weights (target_len=12, source_len=10):
Shape: (12, 10)
å„æ™‚åˆ»ã®åˆè¨ˆ: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
</code></pre>

<h3>4.5.2 è¨“ç·´ã¨è©•ä¾¡ã®å®Ÿè£…</h3>

<pre><code class="language-python">def train_attention_model(model, train_loader, optimizer, criterion, device, clip=1.0):
    """Attentionä»˜ãSeq2Seqã®è¨“ç·´"""
    model.train()
    epoch_loss = 0

    for batch_idx, (source, target) in enumerate(train_loader):
        source, target = source.to(device), target.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs, _ = model(source, target, teacher_forcing_ratio=0.5)

        # Lossè¨ˆç®—ï¼ˆ<SOS>ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤ãï¼‰
        output_dim = outputs.shape[-1]
        outputs_flat = outputs[:, 1:].reshape(-1, output_dim)
        target_flat = target[:, 1:].reshape(-1)

        loss = criterion(outputs_flat, target_flat)

        # Backward pass
        loss.backward()

        # Gradient clippingï¼ˆå‹¾é…çˆ†ç™ºé˜²æ­¢ï¼‰
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)

        optimizer.step()

        epoch_loss += loss.item()

    return epoch_loss / len(train_loader)


def evaluate_attention_model(model, test_loader, criterion, device):
    """Attentionä»˜ãSeq2Seqã®è©•ä¾¡"""
    model.eval()
    epoch_loss = 0

    with torch.no_grad():
        for source, target in test_loader:
            source, target = source.to(device), target.to(device)

            # Teacher forcingãªã—ã§è©•ä¾¡
            outputs, _ = model(source, target, teacher_forcing_ratio=0.0)

            output_dim = outputs.shape[-1]
            outputs_flat = outputs[:, 1:].reshape(-1, output_dim)
            target_flat = target[:, 1:].reshape(-1)

            loss = criterion(outputs_flat, target_flat)
            epoch_loss += loss.item()

    return epoch_loss / len(test_loader)


# ç°¡æ˜“ãƒ‡ãƒ¢
print("\n=== Training Demo (Simplified) ===\n")

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
class DummyDataset(torch.utils.data.Dataset):
    def __init__(self, num_samples, source_len, target_len, vocab_size):
        self.num_samples = num_samples
        self.source_len = source_len
        self.target_len = target_len
        self.vocab_size = vocab_size

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        source = torch.randint(1, self.vocab_size, (self.source_len,))
        target = torch.randint(1, self.vocab_size, (self.target_len,))
        return source, target

train_dataset = DummyDataset(100, SOURCE_LEN, TARGET_LEN, OUTPUT_VOCAB)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)

# è¨“ç·´è¨­å®š
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss(ignore_index=0)  # Paddingç„¡è¦–

# æ•°ã‚¨ãƒãƒƒã‚¯è¨“ç·´
num_epochs = 3
for epoch in range(num_epochs):
    train_loss = train_attention_model(model, train_loader, optimizer, criterion, device)
    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}")

print("\nè¨“ç·´å®Œäº†ï¼")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Training Demo (Simplified) ===

Epoch [1/3], Train Loss: 4.5342
Epoch [2/3], Train Loss: 4.4187
Epoch [3/3], Train Loss: 4.3256

è¨“ç·´å®Œäº†ï¼
</code></pre>

<hr>

<h2>4.6 Self-Attentionå…¥é–€ï¼ˆTransformeræº–å‚™ï¼‰</h2>

<h3>4.6.1 Self-Attentionã¨ã¯</h3>

<p>ã“ã‚Œã¾ã§ã®Attentionã¯ã€Encoderã¨Decoderé–“ã®é–¢ä¿‚ã‚’å­¦ç¿’ã—ã¦ã„ã¾ã—ãŸã€‚<strong>Self-Attention</strong>ã¯ã€åŒã˜ç³»åˆ—å†…ã®è¦ç´ é–“ã®é–¢ä¿‚ã‚’å­¦ç¿’ã™ã‚‹æ©Ÿæ§‹ã§ã™ã€‚</p>

<table>
<thead>
<tr>
<th>ç‰¹æ€§</th>
<th>Encoder-Decoder Attention</th>
<th>Self-Attention</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ³¨ç›®å¯¾è±¡</strong></td>
<td>ç•°ãªã‚‹ç³»åˆ—é–“ï¼ˆSourceâ†’Targetï¼‰</td>
<td>åŒã˜ç³»åˆ—å†…ï¼ˆSelfâ†’Selfï¼‰</td>
</tr>
<tr>
<td><strong>ç”¨é€”</strong></td>
<td>ç¿»è¨³ã€è¦ç´„ãªã©ã®å¤‰æ›ã‚¿ã‚¹ã‚¯</td>
<td>æ–‡è„ˆç†è§£ã€ç‰¹å¾´æŠ½å‡º</td>
</tr>
<tr>
<td><strong>Query</strong></td>
<td>DecoderçŠ¶æ…‹</td>
<td>ç³»åˆ—å†…ã®å„è¦ç´ </td>
</tr>
<tr>
<td><strong>Key/Value</strong></td>
<td>EncoderçŠ¶æ…‹</td>
<td>ç³»åˆ—å†…ã®å…¨è¦ç´ </td>
</tr>
<tr>
<td><strong>ä»£è¡¨ä¾‹</strong></td>
<td>Bahdanau/Luong Attention</td>
<td>Transformerï¼ˆBERT, GPTï¼‰</td>
</tr>
</tbody>
</table>

<h3>4.6.2 Query, Key, Valueã®æ¦‚å¿µ</h3>

<p>Self-Attentionã¯ã€å„å˜èªã‚’3ã¤ã®ç•°ãªã‚‹å½¹å‰²ã§è¡¨ç¾ã—ã¾ã™ï¼š</p>

<ul>
<li><strong>Queryï¼ˆQï¼‰</strong>: ã€Œç§ã¯ä½•ã‚’æ¢ã—ã¦ã„ã‚‹ã‹ï¼Ÿã€- æ³¨ç›®ã™ã‚‹å´</li>
<li><strong>Keyï¼ˆKï¼‰</strong>: ã€Œç§ã¯ä½•ã‚’æä¾›ã§ãã‚‹ã‹ï¼Ÿã€- æ³¨ç›®ã•ã‚Œã‚‹å´</li>
<li><strong>Valueï¼ˆVï¼‰</strong>: ã€Œç§ã®å®Ÿéš›ã®å†…å®¹ã¯ä½•ã‹ï¼Ÿã€- å–å¾—ã•ã‚Œã‚‹æƒ…å ±</li>
</ul>

<p>Attentionè¨ˆç®—ã®æ•°å¼ï¼š</p>
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
$$

<p>ã“ã“ã§ $d_k$ ã¯Keyã®æ¬¡å…ƒæ•°ã§ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚</p>

<h3>4.6.3 Self-Attentionã®å®Ÿè£…</h3>

<pre><code class="language-python">class SelfAttention(nn.Module):
    """Self-Attentionæ©Ÿæ§‹ã®å®Ÿè£…ï¼ˆScaled Dot-Product Attentionï¼‰"""

    def __init__(self, embed_size, heads=1):
        super(SelfAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads

        assert self.head_dim * heads == embed_size, "Embed size must be divisible by heads"

        # Query, Key, Valueã®ç·šå½¢å¤‰æ›
        self.query = nn.Linear(embed_size, embed_size, bias=False)
        self.key = nn.Linear(embed_size, embed_size, bias=False)
        self.value = nn.Linear(embed_size, embed_size, bias=False)

        # å‡ºåŠ›ã®ç·šå½¢å¤‰æ›
        self.fc_out = nn.Linear(embed_size, embed_size)

    def forward(self, x, mask=None):
        """
        Args:
            x: [batch, seq_len, embed_size]
            mask: [batch, seq_len, seq_len] (Optional)
        Returns:
            output: [batch, seq_len, embed_size]
            attention_weights: [batch, heads, seq_len, seq_len]
        """
        batch_size = x.size(0)
        seq_len = x.size(1)

        # Q, K, Vã®ç”Ÿæˆ
        Q = self.query(x)  # [batch, seq_len, embed_size]
        K = self.key(x)
        V = self.value(x)

        # Multi-headç”¨ã«åˆ†å‰²: [batch, seq_len, heads, head_dim]
        Q = Q.view(batch_size, seq_len, self.heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.heads, self.head_dim).transpose(1, 2)
        # â†’ [batch, heads, seq_len, head_dim]

        # Scaled Dot-Product Attention
        # Q @ K^T: [batch, heads, seq_len, seq_len]
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)

        # MaskingãŒã‚ã‚‹å ´åˆé©ç”¨
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))

        # Softmaxã§æ­£è¦åŒ–
        attention_weights = F.softmax(scores, dim=-1)

        # Valueã¨ã®é‡ã¿ä»˜ãå’Œ
        out = torch.matmul(attention_weights, V)  # [batch, heads, seq_len, head_dim]

        # Headsã‚’çµåˆ
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_size)

        # æœ€çµ‚çš„ãªç·šå½¢å¤‰æ›
        output = self.fc_out(out)

        return output, attention_weights


# Self-Attention ãƒ‡ãƒ¢
print("=== Self-Attention Demo ===\n")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
batch_size = 2
seq_len = 6
embed_size = 8
num_heads = 2

# ãƒ€ãƒŸãƒ¼å…¥åŠ›ï¼ˆæ–‡ç« ã®åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã‚’æƒ³å®šï¼‰
x = torch.randn(batch_size, seq_len, embed_size)

# Self-Attentioné©ç”¨
self_attn = SelfAttention(embed_size, heads=num_heads)
output, attn_weights = self_attn(x)

print(f"Input shape: {x.shape}")
print(f"Output shape: {output.shape}")
print(f"Attention weights shape: {attn_weights.shape}")
print(f"  â†’ [batch, heads, seq_len, seq_len]")

# 1ã¤ã®ãƒ˜ãƒƒãƒ‰ã®Attentioné‡ã¿ã‚’å¯è¦–åŒ–
sample_attn = attn_weights[0, 0].detach().numpy()  # 1st batch, 1st head

fig, ax = plt.subplots(figsize=(8, 7))
sns.heatmap(sample_attn,
            cmap='viridis',
            cbar_kws={'label': 'Attention Weight'},
            ax=ax,
            annot=True,
            fmt='.3f',
            linewidths=0.5,
            xticklabels=[f'Pos{i+1}' for i in range(seq_len)],
            yticklabels=[f'Pos{i+1}' for i in range(seq_len)])

ax.set_xlabel('Key Position', fontsize=12, fontweight='bold')
ax.set_ylabel('Query Position', fontsize=12, fontweight='bold')
ax.set_title('Self-Attention Weights Visualization\n(Head 1)',
            fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

print("\nå„ä½ç½®ãŒä»–ã®ä½ç½®ã«ã©ã‚Œã ã‘æ³¨ç›®ã—ã¦ã„ã‚‹ã‹ã‚’è¡¨ã™è¡Œåˆ—")
print("å¯¾è§’æˆåˆ†: è‡ªåˆ†è‡ªèº«ã¸ã®æ³¨ç›®åº¦")
print("éå¯¾è§’æˆåˆ†: ä»–ã®ä½ç½®ã¸ã®æ³¨ç›®åº¦")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Self-Attention Demo ===

Input shape: torch.Size([2, 6, 8])
Output shape: torch.Size([2, 6, 8])
Attention weights shape: torch.Size([2, 2, 6, 6])
  â†’ [batch, heads, seq_len, seq_len]

å„ä½ç½®ãŒä»–ã®ä½ç½®ã«ã©ã‚Œã ã‘æ³¨ç›®ã—ã¦ã„ã‚‹ã‹ã‚’è¡¨ã™è¡Œåˆ—
å¯¾è§’æˆåˆ†: è‡ªåˆ†è‡ªèº«ã¸ã®æ³¨ç›®åº¦
éå¯¾è§’æˆåˆ†: ä»–ã®ä½ç½®ã¸ã®æ³¨ç›®åº¦
</code></pre>

<h3>4.6.4 Self-Attentionã®å¿œç”¨ä¾‹</h3>

<pre><code class="language-python">def demonstrate_self_attention_patterns():
    """Self-Attentionã®å…¸å‹çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å¯è¦–åŒ–"""

    print("\n=== Self-Attentionå…¸å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ ===\n")

    seq_len = 8
    tokens = ['The', 'cat', 'sat', 'on', 'the', 'mat', 'today', '.']

    fig, axes = plt.subplots(2, 2, figsize=(14, 12))

    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: å±€æ‰€çš„æ³¨æ„ï¼ˆè¿‘éš£ã®å˜èªã«æ³¨ç›®ï¼‰
    local_attn = np.zeros((seq_len, seq_len))
    for i in range(seq_len):
        for j in range(seq_len):
            distance = abs(i - j)
            local_attn[i, j] = np.exp(-distance / 2)
    local_attn = local_attn / local_attn.sum(axis=1, keepdims=True)

    sns.heatmap(local_attn, ax=axes[0, 0], cmap='Reds',
                xticklabels=tokens, yticklabels=tokens,
                cbar_kws={'label': 'Weight'}, annot=True, fmt='.2f', linewidths=0.5)
    axes[0, 0].set_title('Pattern 1: Local Attention\n(è¿‘éš£å˜èªã¸ã®æ³¨ç›®)',
                        fontsize=12, fontweight='bold')

    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚°ãƒ­ãƒ¼ãƒãƒ«æ³¨æ„ï¼ˆç‰¹å®šã®é‡è¦å˜èªã«æ³¨ç›®ï¼‰
    global_attn = np.ones((seq_len, seq_len)) * 0.05
    global_attn[:, 1] = 0.4  # "cat"ã«å¼·ãæ³¨ç›®
    global_attn[:, 5] = 0.3  # "mat"ã«ã‚‚æ³¨ç›®
    global_attn = global_attn / global_attn.sum(axis=1, keepdims=True)

    sns.heatmap(global_attn, ax=axes[0, 1], cmap='Blues',
                xticklabels=tokens, yticklabels=tokens,
                cbar_kws={'label': 'Weight'}, annot=True, fmt='.2f', linewidths=0.5)
    axes[0, 1].set_title('Pattern 2: Global Attention\n(é‡è¦å˜èªã¸ã®é›†ä¸­)',
                        fontsize=12, fontweight='bold')

    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: æ§‹æ–‡æ§‹é€ ï¼ˆä¸»èª-å‹•è©-ç›®çš„èªã®é–¢ä¿‚ï¼‰
    syntax_attn = np.eye(seq_len) * 0.3
    syntax_attn[1, 2] = 0.4  # cat â†’ sat
    syntax_attn[2, 1] = 0.4  # sat â†’ cat
    syntax_attn[2, 5] = 0.3  # sat â†’ mat
    syntax_attn[5, 3] = 0.3  # mat â†’ on
    syntax_attn = syntax_attn / syntax_attn.sum(axis=1, keepdims=True)

    sns.heatmap(syntax_attn, ax=axes[1, 0], cmap='Greens',
                xticklabels=tokens, yticklabels=tokens,
                cbar_kws={'label': 'Weight'}, annot=True, fmt='.2f', linewidths=0.5)
    axes[1, 0].set_title('Pattern 3: Syntactic Attention\n(æ§‹æ–‡æ§‹é€ ã®é–¢ä¿‚)',
                        fontsize=12, fontweight='bold')

    # ãƒ‘ã‚¿ãƒ¼ãƒ³4: ä½ç½®çš„æ³¨æ„ï¼ˆæ–‡ã®å‰åŠ/å¾ŒåŠã¸ã®æ³¨ç›®ï¼‰
    position_attn = np.zeros((seq_len, seq_len))
    for i in range(seq_len):
        if i < seq_len // 2:
            position_attn[i, :seq_len//2] = 1.0  # å‰åŠã¯å‰åŠã«æ³¨ç›®
        else:
            position_attn[i, seq_len//2:] = 1.0  # å¾ŒåŠã¯å¾ŒåŠã«æ³¨ç›®
    position_attn = position_attn / position_attn.sum(axis=1, keepdims=True)

    sns.heatmap(position_attn, ax=axes[1, 1], cmap='Purples',
                xticklabels=tokens, yticklabels=tokens,
                cbar_kws={'label': 'Weight'}, annot=True, fmt='.2f', linewidths=0.5)
    axes[1, 1].set_title('Pattern 4: Positional Attention\n(ä½ç½®çš„ãªæ³¨ç›®ãƒ‘ã‚¿ãƒ¼ãƒ³)',
                        fontsize=12, fontweight='bold')

    plt.suptitle('Self-Attention: å…¸å‹çš„ãªæ³¨ç›®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯è¦–åŒ–',
                fontsize=16, fontweight='bold', y=1.00)
    plt.tight_layout()
    plt.show()

    print("å®Ÿéš›ã®Transformerãƒ¢ãƒ‡ãƒ«ã¯ã€ã“ã‚Œã‚‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è‡ªå‹•çš„ã«å­¦ç¿’ã—ã¾ã™")
    print("ç•°ãªã‚‹Attention HeadãŒç•°ãªã‚‹è¨€èªçš„ç‰¹å¾´ã‚’æ‰ãˆã¾ã™ï¼š")
    print("  Head 1: æ§‹æ–‡é–¢ä¿‚ï¼ˆä¸»èª-è¿°èªãªã©ï¼‰")
    print("  Head 2: æ„å‘³çš„é¡ä¼¼æ€§ï¼ˆé–¢é€£ã™ã‚‹å˜èªï¼‰")
    print("  Head 3: ä½ç½®æƒ…å ±ï¼ˆè¿‘ã„å˜èªã€é ã„å˜èªï¼‰")
    print("  ...ãªã©")

demonstrate_self_attention_patterns()
</code></pre>

<hr>

<h2>4.7 ã¾ã¨ã‚ã¨ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯</h2>

<h3>æœ¬ç« ã§å­¦ã‚“ã ã“ã¨</h3>

<table>
<thead>
<tr>
<th>ãƒˆãƒ”ãƒƒã‚¯</th>
<th>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Attentionã®å¿…è¦æ€§</strong></td>
<td>å›ºå®šé•·Context Vectorã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯è§£æ¶ˆ</td>
</tr>
<tr>
<td><strong>Bahdanau Attention</strong></td>
<td>åŠ ç®—çš„ã‚¹ã‚³ã‚¢è¨ˆç®—ã€åˆæœŸã®Attentionæ©Ÿæ§‹</td>
</tr>
<tr>
<td><strong>Luong Attention</strong></td>
<td>ä¹—ç®—çš„ã‚¹ã‚³ã‚¢è¨ˆç®—ã€è¨ˆç®—åŠ¹ç‡ã®å‘ä¸Š</td>
</tr>
<tr>
<td><strong>Attentionå¯è¦–åŒ–</strong></td>
<td>ãƒ¢ãƒ‡ãƒ«è§£é‡ˆæ€§ã®å‘ä¸Šã€ãƒ‡ãƒãƒƒã‚°æ”¯æ´</td>
</tr>
<tr>
<td><strong>Self-Attention</strong></td>
<td>ç³»åˆ—å†…é–¢ä¿‚ã®å­¦ç¿’ã€Transformerã®åŸºç¤</td>
</tr>
<tr>
<td><strong>å®Ÿè£…ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</strong></td>
<td>Teacher Forcingã€Gradient Clipping</td>
</tr>
</tbody>
</table>

<h3>ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯</h3>

<details>
<summary><strong>Multi-Head Attention</strong></summary>
<p>è¤‡æ•°ã®Attention Headã‚’ä¸¦åˆ—ã«ä½¿ç”¨ã—ã€ç•°ãªã‚‹è¡¨ç¾éƒ¨åˆ†ç©ºé–“ã‹ã‚‰æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚Transformerã®ä¸­æ ¸çš„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã€æ¬¡ç« ã§è©³ã—ãå­¦ã³ã¾ã™ã€‚</p>
<pre><code>MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O
where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
</code></pre>
</details>

<details>
<summary><strong>Sparse Attention</strong></summary>
<p>è¨ˆç®—é‡ã‚’å‰Šæ¸›ã™ã‚‹ãŸã‚ã€å…¨ã¦ã®ä½ç½®ã«æ³¨ç›®ã™ã‚‹ä»£ã‚ã‚Šã«ã€ç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå±€æ‰€ã€ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ï¼‰ã®ã¿ã«æ³¨ç›®ã—ã¾ã™ã€‚Longformerã€BigBirdãªã©ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚</p>
</details>

<details>
<summary><strong>Cross-Attention</strong></summary>
<p>2ã¤ã®ç•°ãªã‚‹ç³»åˆ—é–“ã®Attentionã§ã€ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆï¼ˆç”»åƒâ†’ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‚„ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å­¦ç¿’ã§æ´»ç”¨ã•ã‚Œã¾ã™ã€‚</p>
</details>

<details>
<summary><strong>Attention Interpretability</strong></summary>
<p>AttentionãŒå®Ÿéš›ã«ã€Œè§£é‡ˆå¯èƒ½ã€ã‹ã©ã†ã‹ã¯è­°è«–ãŒã‚ã‚Šã¾ã™ã€‚æœ€è¿‘ã®ç ”ç©¶ã§ã¯ã€Attentionã¯å¿…ãšã—ã‚‚ãƒ¢ãƒ‡ãƒ«ã®åˆ¤æ–­æ ¹æ‹ ã‚’æ­£ç¢ºã«åæ˜ ã—ãªã„ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚</p>
</details>

<h3>æ¼”ç¿’å•é¡Œ</h3>

<div class="project-box">
<h4>æ¼”ç¿’ 4.1: Attentionæ©Ÿæ§‹ã®æ¯”è¼ƒå®Ÿé¨“</h4>
<p><strong>èª²é¡Œ</strong>: Bahdanau Attentionã¨Luong Attentionã‚’åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¨“ç·´ã—ã€æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>è©•ä¾¡é …ç›®</strong>:</p>
<ul>
<li>ç¿»è¨³ç²¾åº¦ï¼ˆBLEU scoreï¼‰</li>
<li>è¨“ç·´æ™‚é–“</li>
<li>ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡</li>
<li>Attentioné‡ã¿ã®åˆ†å¸ƒã®é•ã„</li>
</ul>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.2: Attentionå¯è¦–åŒ–ãƒ„ãƒ¼ãƒ«ã®é–‹ç™º</h4>
<p><strong>èª²é¡Œ</strong>: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªAttentionå¯è¦–åŒ–ãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>æ©Ÿèƒ½è¦ä»¶</strong>:</p>
<ul>
<li>ä»»æ„ã®æ–‡ã«å¯¾ã™ã‚‹Attentioné‡ã¿è¡¨ç¤º</li>
<li>è¤‡æ•°ã®ãƒ˜ãƒƒãƒ‰ã®æ¯”è¼ƒè¡¨ç¤º</li>
<li>å±¤ã”ã¨ã®Attentionãƒ‘ã‚¿ãƒ¼ãƒ³è¡¨ç¤º</li>
<li>çµ±è¨ˆæƒ…å ±ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€é›†ä¸­åº¦ï¼‰ã®è¨ˆç®—</li>
</ul>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.3: Self-Attentionã«ã‚ˆã‚‹æ–‡åˆ†é¡</h4>
<p><strong>èª²é¡Œ</strong>: Self-Attentionã‚’ä½¿ã£ãŸæ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>ãƒ‡ãƒ¼ã‚¿</strong>: IMDBãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</p>
<p><strong>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</strong>: Embedding â†’ Self-Attention â†’ Pooling â†’ FC</p>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.4: é•·æ–‡ç¿»è¨³ã§ã®Attentionåˆ†æ</h4>
<p><strong>èª²é¡Œ</strong>: ç³»åˆ—é•·ã‚’å¤‰ãˆã¦ã€Attentionã®æŒ™å‹•ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>å®Ÿé¨“</strong>:</p>
<ul>
<li>çŸ­æ–‡ï¼ˆ5-10å˜èªï¼‰</li>
<li>ä¸­æ–‡ï¼ˆ20-30å˜èªï¼‰</li>
<li>é•·æ–‡ï¼ˆ50-100å˜èªï¼‰</li>
</ul>
<p>å„ã‚±ãƒ¼ã‚¹ã§ã®Attentionåˆ†å¸ƒã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¨ç¿»è¨³ç²¾åº¦ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.5: Attention Dropout</h4>
<p><strong>èª²é¡Œ</strong>: Attentioné‡ã¿ã«Dropoutã‚’é©ç”¨ã™ã‚‹å®Ÿè£…ã‚’è¿½åŠ ã—ã€æ±åŒ–æ€§èƒ½ã¸ã®å½±éŸ¿ã‚’èª¿æŸ»ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>æ¯”è¼ƒ</strong>: Dropoutç‡ 0%, 10%, 20%, 30%ã§ã®æ€§èƒ½æ¯”è¼ƒ</p>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.6: Multi-Head Attentionã®ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—</h4>
<p><strong>èª²é¡Œ</strong>: Single-head Self-Attentionã‚’Multi-headç‰ˆã«æ‹¡å¼µã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>å®Ÿè£…</strong>:</p>
<ul>
<li>è¤‡æ•°ã®Attention Headã®ä¸¦åˆ—è¨ˆç®—</li>
<li>å„Headã®å‡ºåŠ›ã®çµåˆ</li>
<li>å„HeadãŒå­¦ç¿’ã™ã‚‹ç‰¹å¾´ã®å¯è¦–åŒ–</li>
</ul>
</div>

<hr>

<h3>æ¬¡ç« äºˆå‘Š</h3>

<p>ç¬¬5ç« ã§ã¯ã€Attentionæ©Ÿæ§‹ã‚’ç™ºå±•ã•ã›ãŸ<strong>Transformer</strong>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å­¦ã³ã¾ã™ã€‚Transformerã¯RNNã‚’å®Œå…¨ã«æ’é™¤ã—ã€Self-Attentionã¨Position Encodingã®ã¿ã§ç³»åˆ—å‡¦ç†ã‚’å®Ÿç¾ã—ã¾ã™ã€‚BERTã€GPTã€T5ãªã©ã®æœ€å…ˆç«¯ãƒ¢ãƒ‡ãƒ«ã®åŸºç›¤æŠ€è¡“ã§ã™ã€‚</p>

<blockquote>
<p><strong>æ¬¡ç« ã®ãƒˆãƒ”ãƒƒã‚¯</strong>:<br>
ãƒ»Transformerã®å…¨ä½“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£<br>
ãƒ»Multi-Head Attention<br>
ãƒ»Position Encoding<br>
ãƒ»Feed-Forward Network<br>
ãƒ»Layer Normalizationã¨Residual Connection<br>
ãƒ»Encoderã¨Decoderã®è©³ç´°<br>
ãƒ»å®Ÿè£…ï¼šãƒŸãƒ‹Transformerã«ã‚ˆã‚‹æ©Ÿæ¢°ç¿»è¨³</p>
</blockquote>

        <div class="navigation">
            <a href="chapter3-seq2seq.html" class="nav-button">â† ç¬¬3ç« : Seq2Seqãƒ¢ãƒ‡ãƒ«</a>
            <a href="chapter5-transformer.html" class="nav-button">ç¬¬5ç« : Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ â†’</a>
        </div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
        <p>ç¬¬4ç« ï¼šAttentionæ©Ÿæ§‹ | RNNå…¥é–€ã‚·ãƒªãƒ¼ã‚º</p>
    </footer>

</body>
</html>
