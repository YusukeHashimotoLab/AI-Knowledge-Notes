<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬2ç« ï¼šLSTMãƒ»GRU - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬2ç« ï¼šLSTMãƒ»GRUï¼ˆLong Short-Term Memory and Gated Recurrent Unitï¼‰</h1>
            <p class="subtitle">é•·æœŸä¾å­˜é–¢ä¿‚ã‚’æ‰±ã†ã‚²ãƒ¼ãƒˆä»˜ãRNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç†è«–ã¨å®Ÿè£…</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… Vanilla RNNã®é™ç•Œï¼ˆå‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™ºå•é¡Œï¼‰ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… LSTMã®ã‚»ãƒ«ã‚¹ãƒ†ãƒ¼ãƒˆã¨ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹ï¼ˆå¿˜å´ãƒ»å…¥åŠ›ãƒ»å‡ºåŠ›ã‚²ãƒ¼ãƒˆï¼‰ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>âœ… GRUã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨LSTMã¨ã®é•ã„ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… PyTorchã§LSTMãƒ»GRUã‚’å®Ÿè£…ã—ã€å®Ÿéš›ã®å•é¡Œã«é©ç”¨ã§ãã‚‹</li>
<li>âœ… åŒæ–¹å‘RNNï¼ˆBidirectional RNNï¼‰ã®ä»•çµ„ã¿ã¨åˆ©ç‚¹ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… IMDbæ„Ÿæƒ…åˆ†æã‚¿ã‚¹ã‚¯ã§å®Ÿè·µçš„ãªLSTMãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
</ul>

<hr>

<h2>2.1 Vanilla RNNã®é™ç•Œ</h2>

<h3>å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™ºå•é¡Œ</h3>

<p>ç¬¬1ç« ã§å­¦ã‚“ã æ¨™æº–çš„ãªRNNï¼ˆVanilla RNNï¼‰ã¯ã€ç†è«–ä¸Šã¯ä»»æ„ã®é•·ã•ã®ç³»åˆ—ã‚’æ‰±ãˆã¾ã™ãŒã€å®Ÿéš›ã«ã¯<strong>é•·æœŸä¾å­˜é–¢ä¿‚</strong>ã®å­¦ç¿’ãŒå›°é›£ã§ã™ã€‚</p>

<p>RNNã®BPTTï¼ˆBackpropagation Through Timeï¼‰ã§ã¯ã€æ™‚åˆ»$t$ã§ã®å‹¾é…ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«æ™‚é–“ã‚’ã•ã‹ã®ã¼ã£ã¦ä¼æ’­ã—ã¾ã™ï¼š</p>

$$
\frac{\partial L}{\partial h_1} = \frac{\partial L}{\partial h_T} \prod_{t=2}^{T} \frac{\partial h_t}{\partial h_{t-1}}
$$

<p>ã“ã®ç©ã®é …ãŒå•é¡Œã§ã™ï¼š</p>

<ul>
<li><strong>å‹¾é…æ¶ˆå¤±</strong>ï¼š$\left\|\frac{\partial h_t}{\partial h_{t-1}}\right\| < 1$ ã®å ´åˆã€æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ãŒå¢—ãˆã‚‹ã¨å‹¾é…ãŒæŒ‡æ•°çš„ã«æ¸›å°‘</li>
<li><strong>å‹¾é…çˆ†ç™º</strong>ï¼š$\left\|\frac{\partial h_t}{\partial h_{t-1}}\right\| > 1$ ã®å ´åˆã€å‹¾é…ãŒæŒ‡æ•°çš„ã«å¢—å¤§</li>
</ul>

<blockquote>
<p>ã€Œå‹¾é…æ¶ˆå¤±ã«ã‚ˆã‚Šã€Vanilla RNNã¯10ã‚¹ãƒ†ãƒƒãƒ—ä»¥ä¸Šã®é•·æœŸä¾å­˜é–¢ä¿‚ã‚’ã»ã¨ã‚“ã©å­¦ç¿’ã§ãã¾ã›ã‚“ã€</p>
</blockquote>

<h3>å•é¡Œã®å¯è¦–åŒ–</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

# Vanilla RNNã§å‹¾é…ã®å¤§ãã•ã‚’è¦³å¯Ÿ
class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(SimpleRNN, self).__init__()
        self.hidden_size = hidden_size
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)

    def forward(self, x):
        output, hidden = self.rnn(x)
        return output, hidden

# ãƒ¢ãƒ‡ãƒ«ä½œæˆ
input_size = 10
hidden_size = 20
sequence_length = 50

model = SimpleRNN(input_size, hidden_size)

# ãƒ©ãƒ³ãƒ€ãƒ ãªå…¥åŠ›
x = torch.randn(1, sequence_length, input_size, requires_grad=True)

# Forward pass
output, hidden = model(x)
loss = output.sum()

# Backward pass
loss.backward()

# å„æ™‚åˆ»ã®å‹¾é…ãƒãƒ«ãƒ ã‚’è¨ˆç®—
gradients = []
for t in range(sequence_length):
    grad = x.grad[0, t, :].norm().item()
    gradients.append(grad)

print("=== Vanilla RNNã®å‹¾é…ä¼æ’­ ===")
print(f"åˆæœŸæ™‚åˆ»ã®å‹¾é…ãƒãƒ«ãƒ : {gradients[0]:.6f}")
print(f"ä¸­é–“æ™‚åˆ»ã®å‹¾é…ãƒãƒ«ãƒ : {gradients[25]:.6f}")
print(f"æœ€çµ‚æ™‚åˆ»ã®å‹¾é…ãƒãƒ«ãƒ : {gradients[49]:.6f}")
print(f"\nå‹¾é…ã®æ¸›è¡°ç‡: {gradients[0] / gradients[49]:.2f}å€")
print("â†’ éå»ã«ã•ã‹ã®ã¼ã‚‹ã»ã©å‹¾é…ãŒå°ã•ããªã‚‹ï¼ˆå‹¾é…æ¶ˆå¤±ï¼‰")
</code></pre>

<h3>å…·ä½“ä¾‹ï¼šé•·æœŸä¾å­˜ã‚¿ã‚¹ã‚¯</h3>

<pre><code class="language-python">import torch
import torch.nn as nn

# ã‚¿ã‚¹ã‚¯: ç³»åˆ—ã®æœ€åˆã®è¦ç´ ã‚’æœ€å¾Œã«äºˆæ¸¬ã™ã‚‹
def create_long_dependency_task(batch_size=32, seq_length=50):
    """
    æœ€åˆã®æ™‚åˆ»ã«é‡è¦ãªæƒ…å ±ãŒã‚ã‚Šã€æœ€å¾Œã§ãã‚Œã‚’ä½¿ã†å¿…è¦ãŒã‚ã‚‹ã‚¿ã‚¹ã‚¯
    ä¾‹: [5, 0, 0, ..., 0] â†’ æœ€å¾Œã«5ã‚’äºˆæ¸¬
    """
    x = torch.zeros(batch_size, seq_length, 10)
    targets = torch.randint(0, 10, (batch_size,))

    # æœ€åˆã®æ™‚åˆ»ã«æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    for i in range(batch_size):
        x[i, 0, targets[i]] = 1.0

    return x, targets

# Vanilla RNNã§å­¦ç¿’
class VanillaRNNClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(VanillaRNNClassifier, self).__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        output, hidden = self.rnn(x)
        # æœ€å¾Œã®æ™‚åˆ»ã®å‡ºåŠ›ã‚’ä½¿ç”¨
        logits = self.fc(output[:, -1, :])
        return logits

# å®Ÿé¨“
model = VanillaRNNClassifier(input_size=10, hidden_size=32, num_classes=10)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# è¨“ç·´
num_epochs = 100
for epoch in range(num_epochs):
    x, targets = create_long_dependency_task(batch_size=32, seq_length=50)

    optimizer.zero_grad()
    logits = model(x)
    loss = criterion(logits, targets)
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 20 == 0:
        with torch.no_grad():
            x_test, targets_test = create_long_dependency_task(batch_size=100, seq_length=50)
            logits_test = model(x_test)
            _, predicted = logits_test.max(1)
            accuracy = (predicted == targets_test).float().mean().item()
            print(f"Epoch {epoch+1}: ç²¾åº¦ = {accuracy*100:.2f}%")

print("\nâ†’ Vanilla RNNã¯é•·æœŸä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã§ããšã€ç²¾åº¦ãŒä½ã„ï¼ˆãƒ©ãƒ³ãƒ€ãƒ äºˆæ¸¬ã¨åŒç¨‹åº¦ï¼‰")
</code></pre>

<h3>è§£æ±ºç­–ï¼šã‚²ãƒ¼ãƒˆæ©Ÿæ§‹</h3>

<p>ã“ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€<strong>LSTMï¼ˆLong Short-Term Memoryï¼‰</strong>ã¨<strong>GRUï¼ˆGated Recurrent Unitï¼‰</strong>ãŒææ¡ˆã•ã‚Œã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã¯<strong>ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹</strong>ã«ã‚ˆã‚Šã€æƒ…å ±ã®æµã‚Œã‚’åˆ¶å¾¡ã—ã€é•·æœŸä¾å­˜é–¢ä¿‚ã‚’åŠ¹æœçš„ã«å­¦ç¿’ã§ãã¾ã™ã€‚</p>

<hr>

<h2>2.2 LSTM (Long Short-Term Memory)</h2>

<h3>LSTMã®æ¦‚è¦</h3>

<p><strong>LSTM</strong>ã¯ã€1997å¹´ã«Hochreiterã¨Schmidhuberã«ã‚ˆã£ã¦ææ¡ˆã•ã‚ŒãŸã€ã‚²ãƒ¼ãƒˆä»˜ãRNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚Vanilla RNNã®éš ã‚ŒçŠ¶æ…‹ã«åŠ ãˆã¦ã€<strong>ã‚»ãƒ«ã‚¹ãƒ†ãƒ¼ãƒˆï¼ˆCell Stateï¼‰</strong>ã¨ã„ã†é•·æœŸè¨˜æ†¶ã‚’æŒã¤ã“ã¨ãŒç‰¹å¾´ã§ã™ã€‚</p>

<div class="mermaid">
graph LR
    A["å…¥åŠ› x_t"] --> B["LSTM Cell"]
    C["å‰ã®éš ã‚ŒçŠ¶æ…‹ h_{t-1}"] --> B
    D["å‰ã®ã‚»ãƒ«ã‚¹ãƒ†ãƒ¼ãƒˆ c_{t-1}"] --> B

    B --> E["å‡ºåŠ› h_t"]
    B --> F["æ–°ã—ã„ã‚»ãƒ«ã‚¹ãƒ†ãƒ¼ãƒˆ c_t"]

    style A fill:#e1f5ff
    style B fill:#b3e5fc
    style D fill:#fff9c4
    style E fill:#4fc3f7
    style F fill:#ffeb3b
</div>

<h3>LSTMã®4ã¤ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</h3>

<p>LSTMã‚»ãƒ«ã¯ã€ä»¥ä¸‹ã®4ã¤ã®è¦ç´ ã§æ§‹æˆã•ã‚Œã¾ã™ï¼š</p>

<table>
<thead>
<tr>
<th>ã‚²ãƒ¼ãƒˆ</th>
<th>å½¹å‰²</th>
<th>æ•°å¼</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å¿˜å´ã‚²ãƒ¼ãƒˆ</strong><br/>(Forget Gate)</td>
<td>éå»ã®è¨˜æ†¶ã‚’ã©ã‚Œã ã‘ä¿æŒã™ã‚‹ã‹</td>
<td>$f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)$</td>
</tr>
<tr>
<td><strong>å…¥åŠ›ã‚²ãƒ¼ãƒˆ</strong><br/>(Input Gate)</td>
<td>æ–°ã—ã„æƒ…å ±ã‚’ã©ã‚Œã ã‘è¿½åŠ ã™ã‚‹ã‹</td>
<td>$i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)$</td>
</tr>
<tr>
<td><strong>å€™è£œå€¤</strong><br/>(Candidate Cell)</td>
<td>è¿½åŠ ã™ã‚‹æ–°ã—ã„æƒ…å ±ã®å†…å®¹</td>
<td>$\tilde{C}_t = \tanh(W_C [h_{t-1}, x_t] + b_C)$</td>
</tr>
<tr>
<td><strong>å‡ºåŠ›ã‚²ãƒ¼ãƒˆ</strong><br/>(Output Gate)</td>
<td>ã‚»ãƒ«ã‚¹ãƒ†ãƒ¼ãƒˆã‹ã‚‰ã©ã‚Œã ã‘å‡ºåŠ›ã™ã‚‹ã‹</td>
<td>$o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)$</td>
</tr>
</tbody>
</table>

<h3>LSTMã®æ•°å­¦çš„å®šç¾©</h3>

<p>LSTMã®å®Œå…¨ãªæ›´æ–°å¼ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š</p>

$$
\begin{align}
f_t &= \sigma(W_f [h_{t-1}, x_t] + b_f) \quad &\text{(å¿˜å´ã‚²ãƒ¼ãƒˆ)} \\
i_t &= \sigma(W_i [h_{t-1}, x_t] + b_i) \quad &\text{(å…¥åŠ›ã‚²ãƒ¼ãƒˆ)} \\
\tilde{C}_t &= \tanh(W_C [h_{t-1}, x_t] + b_C) \quad &\text{(å€™è£œå€¤)} \\
C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \quad &\text{(ã‚»ãƒ«ã‚¹ãƒ†ãƒ¼ãƒˆæ›´æ–°)} \\
o_t &= \sigma(W_o [h_{t-1}, x_t] + b_o) \quad &\text{(å‡ºåŠ›ã‚²ãƒ¼ãƒˆ)} \\
h_t &= o_t \odot \tanh(C_t) \quad &\text{(éš ã‚ŒçŠ¶æ…‹æ›´æ–°)}
\end{align}
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$\sigma$ï¼šã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ï¼ˆ0ã€œ1ã®å€¤ã‚’å‡ºåŠ›ã€ã‚²ãƒ¼ãƒˆã®åˆ¶å¾¡ã«ä½¿ç”¨ï¼‰</li>
<li>$\odot$ï¼šè¦ç´ ã”ã¨ã®ç©ï¼ˆHadamardç©ï¼‰</li>
<li>$[h_{t-1}, x_t]$ï¼šãƒ™ã‚¯ãƒˆãƒ«ã®é€£çµ</li>
<li>$W_*, b_*$ï¼šå­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
</ul>

<h3>ã‚»ãƒ«ã‚¹ãƒ†ãƒ¼ãƒˆã®å½¹å‰²</h3>

<p>ã‚»ãƒ«ã‚¹ãƒ†ãƒ¼ãƒˆ$C_t$ã¯ã€æƒ…å ±ã®ãƒã‚¤ã‚¦ã‚§ã‚¤ã¨ã—ã¦æ©Ÿèƒ½ã—ã¾ã™ï¼š</p>

$$
C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
$$

<ul>
<li>$f_t \odot C_{t-1}$ï¼šéå»ã®è¨˜æ†¶ã‚’é¸æŠçš„ã«ä¿æŒï¼ˆå¿˜å´ã‚²ãƒ¼ãƒˆã§åˆ¶å¾¡ï¼‰</li>
<li>$i_t \odot \tilde{C}_t$ï¼šæ–°ã—ã„æƒ…å ±ã‚’é¸æŠçš„ã«è¿½åŠ ï¼ˆå…¥åŠ›ã‚²ãƒ¼ãƒˆã§åˆ¶å¾¡ï¼‰</li>
</ul>

<blockquote>
<p>ã€Œå‹¾é…ã¯ã‚»ãƒ«ã‚¹ãƒ†ãƒ¼ãƒˆã‚’é€šã˜ã¦ç›´æ¥æµã‚Œã‚‹ãŸã‚ã€å‹¾é…æ¶ˆå¤±ãŒèµ·ã“ã‚Šã«ãã„ï¼ã€</p>
</blockquote>

<h3>LSTMã®æ‰‹å‹•å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class LSTMCell(nn.Module):
    """LSTMã‚»ãƒ«ã®æ‰‹å‹•å®Ÿè£…ï¼ˆæ•™è‚²ç›®çš„ï¼‰"""
    def __init__(self, input_size, hidden_size):
        super(LSTMCell, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size

        # å¿˜å´ã‚²ãƒ¼ãƒˆ
        self.W_f = nn.Linear(input_size + hidden_size, hidden_size)
        # å…¥åŠ›ã‚²ãƒ¼ãƒˆ
        self.W_i = nn.Linear(input_size + hidden_size, hidden_size)
        # å€™è£œå€¤
        self.W_C = nn.Linear(input_size + hidden_size, hidden_size)
        # å‡ºåŠ›ã‚²ãƒ¼ãƒˆ
        self.W_o = nn.Linear(input_size + hidden_size, hidden_size)

    def forward(self, x_t, h_prev, C_prev):
        """
        x_t: (batch_size, input_size) - ç¾åœ¨ã®å…¥åŠ›
        h_prev: (batch_size, hidden_size) - å‰ã®éš ã‚ŒçŠ¶æ…‹
        C_prev: (batch_size, hidden_size) - å‰ã®ã‚»ãƒ«ã‚¹ãƒ†ãƒ¼ãƒˆ
        """
        # å…¥åŠ›ã¨éš ã‚ŒçŠ¶æ…‹ã‚’é€£çµ
        combined = torch.cat([h_prev, x_t], dim=1)

        # å¿˜å´ã‚²ãƒ¼ãƒˆ: ã©ã®æƒ…å ±ã‚’å¿˜ã‚Œã‚‹ã‹
        f_t = torch.sigmoid(self.W_f(combined))

        # å…¥åŠ›ã‚²ãƒ¼ãƒˆ: ã©ã®æƒ…å ±ã‚’è¿½åŠ ã™ã‚‹ã‹
        i_t = torch.sigmoid(self.W_i(combined))

        # å€™è£œå€¤: è¿½åŠ ã™ã‚‹æƒ…å ±ã®å†…å®¹
        C_tilde = torch.tanh(self.W_C(combined))

        # ã‚»ãƒ«ã‚¹ãƒ†ãƒ¼ãƒˆæ›´æ–°
        C_t = f_t * C_prev + i_t * C_tilde

        # å‡ºåŠ›ã‚²ãƒ¼ãƒˆ: ã©ã®æƒ…å ±ã‚’å‡ºåŠ›ã™ã‚‹ã‹
        o_t = torch.sigmoid(self.W_o(combined))

        # éš ã‚ŒçŠ¶æ…‹æ›´æ–°
        h_t = o_t * torch.tanh(C_t)

        return h_t, C_t


class ManualLSTM(nn.Module):
    """è¤‡æ•°æ™‚åˆ»ã®LSTMå‡¦ç†"""
    def __init__(self, input_size, hidden_size):
        super(ManualLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.cell = LSTMCell(input_size, hidden_size)

    def forward(self, x, init_states=None):
        """
        x: (batch_size, seq_length, input_size)
        """
        batch_size, seq_length, _ = x.size()

        # åˆæœŸçŠ¶æ…‹
        if init_states is None:
            h_t = torch.zeros(batch_size, self.hidden_size).to(x.device)
            C_t = torch.zeros(batch_size, self.hidden_size).to(x.device)
        else:
            h_t, C_t = init_states

        # å„æ™‚åˆ»ã®å‡ºåŠ›ã‚’ä¿å­˜
        outputs = []

        # ç³»åˆ—ã‚’æ™‚åˆ»ã”ã¨ã«å‡¦ç†
        for t in range(seq_length):
            h_t, C_t = self.cell(x[:, t, :], h_t, C_t)
            outputs.append(h_t.unsqueeze(1))

        # å‡ºåŠ›ã‚’é€£çµ
        outputs = torch.cat(outputs, dim=1)

        return outputs, (h_t, C_t)


# å‹•ä½œç¢ºèª
batch_size = 4
seq_length = 10
input_size = 8
hidden_size = 16

model = ManualLSTM(input_size, hidden_size)
x = torch.randn(batch_size, seq_length, input_size)

outputs, (h_final, C_final) = model(x)

print("=== æ‰‹å‹•å®Ÿè£…LSTMã®å‹•ä½œç¢ºèª ===")
print(f"å…¥åŠ›ã‚µã‚¤ã‚º: {x.shape}")
print(f"å‡ºåŠ›ã‚µã‚¤ã‚º: {outputs.shape}")
print(f"æœ€çµ‚éš ã‚ŒçŠ¶æ…‹: {h_final.shape}")
print(f"æœ€çµ‚ã‚»ãƒ«ã‚¹ãƒ†ãƒ¼ãƒˆ: {C_final.shape}")
</code></pre>

<h3>PyTorchã®nn.LSTMã‚’ä½¿ã†</h3>

<p>å®Ÿéš›ã®é–‹ç™ºã§ã¯ã€PyTorchã®æœ€é©åŒ–ã•ã‚ŒãŸ<code>nn.LSTM</code>ã‚’ä½¿ç”¨ã—ã¾ã™ï¼š</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# PyTorchã®LSTM
lstm = nn.LSTM(
    input_size=10,      # å…¥åŠ›ã®æ¬¡å…ƒæ•°
    hidden_size=20,     # éš ã‚ŒçŠ¶æ…‹ã®æ¬¡å…ƒæ•°
    num_layers=2,       # LSTMã®å±¤æ•°
    batch_first=True,   # (batch, seq, feature)ã®é †åº
    dropout=0.2,        # å±¤é–“ã®Dropout
    bidirectional=False # åŒæ–¹å‘ã‹ã©ã†ã‹
)

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
batch_size = 32
seq_length = 15
input_size = 10

x = torch.randn(batch_size, seq_length, input_size)

# Forward pass
output, (h_n, c_n) = lstm(x)

print("=== PyTorch nn.LSTMã®ä½¿ç”¨ ===")
print(f"å…¥åŠ›: {x.shape}")
print(f"å‡ºåŠ›: {output.shape}  # (batch, seq, hidden_size)")
print(f"æœ€çµ‚éš ã‚ŒçŠ¶æ…‹: {h_n.shape}  # (num_layers, batch, hidden_size)")
print(f"æœ€çµ‚ã‚»ãƒ«ã‚¹ãƒ†ãƒ¼ãƒˆ: {c_n.shape}  # (num_layers, batch, hidden_size)")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®ç¢ºèª
total_params = sum(p.numel() for p in lstm.parameters())
print(f"\nç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
print("â†’ å„å±¤ã«4ã¤ã®ã‚²ãƒ¼ãƒˆï¼ˆf, i, C, oï¼‰ãŒã‚ã‚‹ãŸã‚ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¤šã„")
</code></pre>

<h3>LSTMã¨é•·æœŸä¾å­˜é–¢ä¿‚</h3>

<pre><code class="language-python">import torch
import torch.nn as nn

# å…ˆã»ã©ã®é•·æœŸä¾å­˜ã‚¿ã‚¹ã‚¯ã‚’LSTMã§è§£ã
class LSTMClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(LSTMClassifier, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        output, (h_n, c_n) = self.lstm(x)
        # æœ€å¾Œã®æ™‚åˆ»ã®å‡ºåŠ›ã‚’ä½¿ç”¨
        logits = self.fc(output[:, -1, :])
        return logits

# ã‚¿ã‚¹ã‚¯ä½œæˆé–¢æ•°ï¼ˆå‰ã¨åŒã˜ï¼‰
def create_long_dependency_task(batch_size=32, seq_length=50):
    x = torch.zeros(batch_size, seq_length, 10)
    targets = torch.randint(0, 10, (batch_size,))
    for i in range(batch_size):
        x[i, 0, targets[i]] = 1.0
    return x, targets

# LSTMã§è¨“ç·´
model = LSTMClassifier(input_size=10, hidden_size=32, num_classes=10)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

print("=== LSTMã§é•·æœŸä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ ===")
num_epochs = 100
for epoch in range(num_epochs):
    x, targets = create_long_dependency_task(batch_size=32, seq_length=50)

    optimizer.zero_grad()
    logits = model(x)
    loss = criterion(logits, targets)
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 20 == 0:
        with torch.no_grad():
            x_test, targets_test = create_long_dependency_task(batch_size=100, seq_length=50)
            logits_test = model(x_test)
            _, predicted = logits_test.max(1)
            accuracy = (predicted == targets_test).float().mean().item()
            print(f"Epoch {epoch+1}: ç²¾åº¦ = {accuracy*100:.2f}%")

print("\nâ†’ LSTMã¯é•·æœŸä¾å­˜é–¢ä¿‚ã‚’åŠ¹æœçš„ã«å­¦ç¿’ã§ãã€é«˜ç²¾åº¦ã‚’é”æˆï¼")
</code></pre>

<hr>

<h2>2.3 GRU (Gated Recurrent Unit)</h2>

<h3>GRUã®æ¦‚è¦</h3>

<p><strong>GRUï¼ˆGated Recurrent Unitï¼‰</strong>ã¯ã€2014å¹´ã«Choã‚‰ã«ã‚ˆã£ã¦ææ¡ˆã•ã‚ŒãŸã€LSTMã‚’ç°¡ç•¥åŒ–ã—ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚LSTMã‚ˆã‚Šå°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã€åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ãŒå¤šã„ã§ã™ã€‚</p>

<h3>LSTMã¨GRUã®é•ã„</h3>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>LSTM</th>
<th>GRU</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ã‚²ãƒ¼ãƒˆæ•°</strong></td>
<td>3ã¤ï¼ˆå¿˜å´ã€å…¥åŠ›ã€å‡ºåŠ›ï¼‰</td>
<td>2ã¤ï¼ˆãƒªã‚»ãƒƒãƒˆã€æ›´æ–°ï¼‰</td>
</tr>
<tr>
<td><strong>çŠ¶æ…‹</strong></td>
<td>éš ã‚ŒçŠ¶æ…‹$h_t$ã¨ã‚»ãƒ«ã‚¹ãƒ†ãƒ¼ãƒˆ$C_t$</td>
<td>éš ã‚ŒçŠ¶æ…‹$h_t$ã®ã¿</td>
</tr>
<tr>
<td><strong>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°</strong></td>
<td>å¤šã„</td>
<td>å°‘ãªã„ï¼ˆLSTMã®ç´„75%ï¼‰</td>
</tr>
<tr>
<td><strong>è¨ˆç®—é€Ÿåº¦</strong></td>
<td>ã‚„ã‚„é…ã„</td>
<td>ã‚„ã‚„é€Ÿã„</td>
</tr>
<tr>
<td><strong>æ€§èƒ½</strong></td>
<td>ã‚¿ã‚¹ã‚¯ã«ã‚ˆã‚‹</td>
<td>ã‚¿ã‚¹ã‚¯ã«ã‚ˆã‚‹ï¼ˆçŸ­ã„ç³»åˆ—ã§ã¯æœ‰åˆ©ï¼‰</td>
</tr>
</tbody>
</table>

<h3>GRUã®æ•°å­¦çš„å®šç¾©</h3>

<p>GRUã®æ›´æ–°å¼ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š</p>

$$
\begin{align}
r_t &= \sigma(W_r [h_{t-1}, x_t] + b_r) \quad &\text{(ãƒªã‚»ãƒƒãƒˆã‚²ãƒ¼ãƒˆ)} \\
z_t &= \sigma(W_z [h_{t-1}, x_t] + b_z) \quad &\text{(æ›´æ–°ã‚²ãƒ¼ãƒˆ)} \\
\tilde{h}_t &= \tanh(W_h [r_t \odot h_{t-1}, x_t] + b_h) \quad &\text{(å€™è£œéš ã‚ŒçŠ¶æ…‹)} \\
h_t &= (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t \quad &\text{(éš ã‚ŒçŠ¶æ…‹æ›´æ–°)}
\end{align}
$$

<p>å„ã‚²ãƒ¼ãƒˆã®å½¹å‰²ï¼š</p>
<ul>
<li><strong>ãƒªã‚»ãƒƒãƒˆã‚²ãƒ¼ãƒˆ$r_t$</strong>ï¼šéå»ã®æƒ…å ±ã‚’ã©ã‚Œã ã‘ç„¡è¦–ã™ã‚‹ã‹ï¼ˆ0ã«è¿‘ã„ã¨éå»ã‚’ç„¡è¦–ï¼‰</li>
<li><strong>æ›´æ–°ã‚²ãƒ¼ãƒˆ$z_t$</strong>ï¼šéå»ã¨ç¾åœ¨ã®æƒ…å ±ã‚’ã©ã®ç¨‹åº¦æ··ãœã‚‹ã‹ï¼ˆ0ã«è¿‘ã„ã¨éå»ã‚’ä¿æŒã€1ã«è¿‘ã„ã¨æ–°æƒ…å ±ã‚’æ¡ç”¨ï¼‰</li>
</ul>

<blockquote>
<p>ã€ŒGRUã¯æ›´æ–°ã‚²ãƒ¼ãƒˆ$z_t$ã§ã€LSTMã®å¿˜å´ã‚²ãƒ¼ãƒˆã¨å…¥åŠ›ã‚²ãƒ¼ãƒˆã‚’çµ±åˆã—ã¦ã„ã¾ã™ã€</p>
</blockquote>

<h3>GRUã®æ‰‹å‹•å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn

class GRUCell(nn.Module):
    """GRUã‚»ãƒ«ã®æ‰‹å‹•å®Ÿè£…"""
    def __init__(self, input_size, hidden_size):
        super(GRUCell, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size

        # ãƒªã‚»ãƒƒãƒˆã‚²ãƒ¼ãƒˆ
        self.W_r = nn.Linear(input_size + hidden_size, hidden_size)
        # æ›´æ–°ã‚²ãƒ¼ãƒˆ
        self.W_z = nn.Linear(input_size + hidden_size, hidden_size)
        # å€™è£œéš ã‚ŒçŠ¶æ…‹
        self.W_h = nn.Linear(input_size + hidden_size, hidden_size)

    def forward(self, x_t, h_prev):
        """
        x_t: (batch_size, input_size)
        h_prev: (batch_size, hidden_size)
        """
        # å…¥åŠ›ã¨éš ã‚ŒçŠ¶æ…‹ã‚’é€£çµ
        combined = torch.cat([h_prev, x_t], dim=1)

        # ãƒªã‚»ãƒƒãƒˆã‚²ãƒ¼ãƒˆ
        r_t = torch.sigmoid(self.W_r(combined))

        # æ›´æ–°ã‚²ãƒ¼ãƒˆ
        z_t = torch.sigmoid(self.W_z(combined))

        # å€™è£œéš ã‚ŒçŠ¶æ…‹ï¼ˆãƒªã‚»ãƒƒãƒˆã‚²ãƒ¼ãƒˆã§éå»ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
        combined_reset = torch.cat([r_t * h_prev, x_t], dim=1)
        h_tilde = torch.tanh(self.W_h(combined_reset))

        # éš ã‚ŒçŠ¶æ…‹æ›´æ–°ï¼ˆæ›´æ–°ã‚²ãƒ¼ãƒˆã§éå»ã¨ç¾åœ¨ã‚’æ··åˆï¼‰
        h_t = (1 - z_t) * h_prev + z_t * h_tilde

        return h_t


class ManualGRU(nn.Module):
    """è¤‡æ•°æ™‚åˆ»ã®GRUå‡¦ç†"""
    def __init__(self, input_size, hidden_size):
        super(ManualGRU, self).__init__()
        self.hidden_size = hidden_size
        self.cell = GRUCell(input_size, hidden_size)

    def forward(self, x, init_state=None):
        batch_size, seq_length, _ = x.size()

        # åˆæœŸçŠ¶æ…‹
        if init_state is None:
            h_t = torch.zeros(batch_size, self.hidden_size).to(x.device)
        else:
            h_t = init_state

        outputs = []

        for t in range(seq_length):
            h_t = self.cell(x[:, t, :], h_t)
            outputs.append(h_t.unsqueeze(1))

        outputs = torch.cat(outputs, dim=1)
        return outputs, h_t


# å‹•ä½œç¢ºèª
model = ManualGRU(input_size=8, hidden_size=16)
x = torch.randn(4, 10, 8)

outputs, h_final = model(x)

print("=== æ‰‹å‹•å®Ÿè£…GRUã®å‹•ä½œç¢ºèª ===")
print(f"å…¥åŠ›: {x.shape}")
print(f"å‡ºåŠ›: {outputs.shape}")
print(f"æœ€çµ‚éš ã‚ŒçŠ¶æ…‹: {h_final.shape}")
print("â†’ GRUã¯ã‚»ãƒ«ã‚¹ãƒ†ãƒ¼ãƒˆãŒãªãã€éš ã‚ŒçŠ¶æ…‹ã®ã¿")
</code></pre>

<h3>PyTorchã®nn.GRUã‚’ä½¿ã†</h3>

<pre><code class="language-python">import torch
import torch.nn as nn

# PyTorchã®GRU
gru = nn.GRU(
    input_size=10,
    hidden_size=20,
    num_layers=2,
    batch_first=True,
    dropout=0.2,
    bidirectional=False
)

x = torch.randn(32, 15, 10)
output, h_n = gru(x)

print("=== PyTorch nn.GRUã®ä½¿ç”¨ ===")
print(f"å…¥åŠ›: {x.shape}")
print(f"å‡ºåŠ›: {output.shape}")
print(f"æœ€çµ‚éš ã‚ŒçŠ¶æ…‹: {h_n.shape}")

# LSTMã¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°æ¯”è¼ƒ
lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True)
gru_params = sum(p.numel() for p in gru.parameters())
lstm_params = sum(p.numel() for p in lstm.parameters())

print(f"\nGRU ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {gru_params:,}")
print(f"LSTM ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {lstm_params:,}")
print(f"å·®: {lstm_params - gru_params:,} (GRUã®æ–¹ãŒ {(lstm_params/gru_params - 1)*100:.1f}% å°‘ãªã„)")
</code></pre>

<h3>LSTMã¨GRUã®æ€§èƒ½æ¯”è¼ƒ</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import time

class SequenceClassifier(nn.Module):
    """æ±ç”¨ç³»åˆ—åˆ†é¡å™¨"""
    def __init__(self, input_size, hidden_size, num_classes, rnn_type='lstm'):
        super(SequenceClassifier, self).__init__()

        if rnn_type == 'lstm':
            self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)
        elif rnn_type == 'gru':
            self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)
        else:
            raise ValueError("rnn_type must be 'lstm' or 'gru'")

        self.fc = nn.Linear(hidden_size, num_classes)
        self.rnn_type = rnn_type

    def forward(self, x):
        if self.rnn_type == 'lstm':
            output, (h_n, c_n) = self.rnn(x)
        else:
            output, h_n = self.rnn(x)

        logits = self.fc(output[:, -1, :])
        return logits

# æ¯”è¼ƒå®Ÿé¨“
def compare_models(seq_length=50):
    print(f"\n=== ç³»åˆ—é•·={seq_length}ã§ã®æ¯”è¼ƒ ===")

    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    lstm_model = SequenceClassifier(10, 32, 10, rnn_type='lstm')
    gru_model = SequenceClassifier(10, 32, 10, rnn_type='gru')

    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    x, targets = create_long_dependency_task(batch_size=32, seq_length=seq_length)

    criterion = nn.CrossEntropyLoss()

    # LSTMè¨“ç·´
    optimizer_lstm = torch.optim.Adam(lstm_model.parameters(), lr=0.001)
    start = time.time()
    for _ in range(50):
        optimizer_lstm.zero_grad()
        logits = lstm_model(x)
        loss = criterion(logits, targets)
        loss.backward()
        optimizer_lstm.step()
    lstm_time = time.time() - start

    # GRUè¨“ç·´
    optimizer_gru = torch.optim.Adam(gru_model.parameters(), lr=0.001)
    start = time.time()
    for _ in range(50):
        optimizer_gru.zero_grad()
        logits = gru_model(x)
        loss = criterion(logits, targets)
        loss.backward()
        optimizer_gru.step()
    gru_time = time.time() - start

    # ç²¾åº¦è©•ä¾¡
    x_test, targets_test = create_long_dependency_task(batch_size=100, seq_length=seq_length)

    with torch.no_grad():
        logits_lstm = lstm_model(x_test)
        logits_gru = gru_model(x_test)

        _, pred_lstm = logits_lstm.max(1)
        _, pred_gru = logits_gru.max(1)

        acc_lstm = (pred_lstm == targets_test).float().mean().item()
        acc_gru = (pred_gru == targets_test).float().mean().item()

    print(f"LSTM - ç²¾åº¦: {acc_lstm*100:.2f}%, è¨“ç·´æ™‚é–“: {lstm_time:.2f}ç§’")
    print(f"GRU  - ç²¾åº¦: {acc_gru*100:.2f}%, è¨“ç·´æ™‚é–“: {gru_time:.2f}ç§’")

# ç•°ãªã‚‹ç³»åˆ—é•·ã§æ¯”è¼ƒ
compare_models(seq_length=20)
compare_models(seq_length=50)
compare_models(seq_length=100)

print("\nâ†’ çŸ­ã„ç³»åˆ—ã§ã¯GRUãŒåŠ¹ç‡çš„ã€é•·ã„ç³»åˆ—ã§ã¯LSTMãŒæœ‰åˆ©ãªå‚¾å‘")
</code></pre>

<hr>

<h2>2.4 åŒæ–¹å‘RNN (Bidirectional RNN)</h2>

<h3>åŒæ–¹å‘RNNã¨ã¯</h3>

<p><strong>åŒæ–¹å‘RNNï¼ˆBidirectional RNNï¼‰</strong>ã¯ã€ç³»åˆ—ã‚’å‰ã‹ã‚‰å¾Œã‚ï¼ˆé †æ–¹å‘ï¼‰ã¨å¾Œã‚ã‹ã‚‰å‰ï¼ˆé€†æ–¹å‘ï¼‰ã®ä¸¡æ–¹å‘ã‹ã‚‰å‡¦ç†ã—ã€ä¸¡æ–¹ã®æƒ…å ±ã‚’çµ±åˆã—ã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    A["x_1"] --> B["é †æ–¹å‘<br/>â†’"]
    B --> C["x_2"]
    C --> D["é †æ–¹å‘<br/>â†’"]
    D --> E["x_3"]

    E --> F["é€†æ–¹å‘<br/>â†"]
    F --> C
    C --> G["é€†æ–¹å‘<br/>â†"]
    G --> A

    B --> H["h_1"]
    D --> I["h_2"]
    F --> J["h_3 (é€†)"]
    G --> K["h_2 (é€†)"]

    style B fill:#b3e5fc
    style D fill:#b3e5fc
    style F fill:#ffab91
    style G fill:#ffab91
</div>

<h3>åŒæ–¹å‘RNNã®åˆ©ç‚¹</h3>

<ul>
<li><strong>æ–‡è„ˆã®å®Œå…¨ãªæŠŠæ¡</strong>ï¼šå„ä½ç½®ã§ã€å‰å¾Œä¸¡æ–¹ã®æ–‡è„ˆã‚’è€ƒæ…®ã§ãã‚‹</li>
<li><strong>å“è©ã‚¿ã‚°ä»˜ã‘</strong>ï¼šå˜èªã®å‰å¾Œã‚’è¦‹ã¦å“è©ã‚’æ±ºå®š</li>
<li><strong>æ„Ÿæƒ…åˆ†æ</strong>ï¼šæ–‡å…¨ä½“ã‚’è¦‹ã¦æ„Ÿæƒ…ã‚’åˆ¤æ–­</li>
<li><strong>æ©Ÿæ¢°ç¿»è¨³</strong>ï¼šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¨ã—ã¦ä½¿ç”¨</li>
</ul>

<blockquote>
<p>ã€ŒåŒæ–¹å‘RNNã¯ã€æ™‚åˆ»$t$ã§ã®å‡ºåŠ›ãŒæœªæ¥ã®æƒ…å ±ã«ã‚‚ä¾å­˜ã™ã‚‹ãŸã‚ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ã«ã¯ä½¿ãˆã¾ã›ã‚“ã€‚ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å‡¦ç†ï¼ˆå…¨ç³»åˆ—ãŒåˆ©ç”¨å¯èƒ½ï¼‰ã«é©ã—ã¦ã„ã¾ã™ã€‚ã€</p>
</blockquote>

<h3>åŒæ–¹å‘LSTMã®å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn

# PyTorchã§ã¯ bidirectional=True ã‚’æŒ‡å®šã™ã‚‹ã ã‘
class BidirectionalLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(BidirectionalLSTM, self).__init__()

        self.lstm = nn.LSTM(
            input_size,
            hidden_size,
            batch_first=True,
            bidirectional=True  # åŒæ–¹å‘ã‚’æœ‰åŠ¹åŒ–
        )

        # åŒæ–¹å‘ãªã®ã§ hidden_size * 2
        self.fc = nn.Linear(hidden_size * 2, num_classes)

    def forward(self, x):
        # output: (batch, seq, hidden_size * 2)
        output, (h_n, c_n) = self.lstm(x)

        # æœ€å¾Œã®æ™‚åˆ»ã®å‡ºåŠ›ã‚’ä½¿ç”¨
        logits = self.fc(output[:, -1, :])
        return logits

# å‹•ä½œç¢ºèª
model = BidirectionalLSTM(input_size=10, hidden_size=32, num_classes=10)
x = torch.randn(4, 15, 10)

logits = model(x)

print("=== åŒæ–¹å‘LSTMã®å‹•ä½œç¢ºèª ===")
print(f"å…¥åŠ›: {x.shape}")
print(f"å‡ºåŠ›: {logits.shape}")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®æ¯”è¼ƒ
uni_lstm = nn.LSTM(10, 32, batch_first=True, bidirectional=False)
bi_lstm = nn.LSTM(10, 32, batch_first=True, bidirectional=True)

uni_params = sum(p.numel() for p in uni_lstm.parameters())
bi_params = sum(p.numel() for p in bi_lstm.parameters())

print(f"\nå˜æ–¹å‘LSTM: {uni_params:,} ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
print(f"åŒæ–¹å‘LSTM: {bi_params:,} ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
print(f"â†’ åŒæ–¹å‘ã¯ç´„2å€ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°")
</code></pre>

<h3>åŒæ–¹å‘vså˜æ–¹å‘ã®æ€§èƒ½æ¯”è¼ƒ</h3>

<pre><code class="language-python">import torch
import torch.nn as nn

class DirectionalClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes, bidirectional=False):
        super(DirectionalClassifier, self).__init__()

        self.lstm = nn.LSTM(
            input_size,
            hidden_size,
            batch_first=True,
            bidirectional=bidirectional
        )

        fc_input_size = hidden_size * 2 if bidirectional else hidden_size
        self.fc = nn.Linear(fc_input_size, num_classes)

    def forward(self, x):
        output, _ = self.lstm(x)
        logits = self.fc(output[:, -1, :])
        return logits

# æ¯”è¼ƒå®Ÿé¨“
def compare_directionality():
    print("\n=== å˜æ–¹å‘ vs åŒæ–¹å‘ã®æ¯”è¼ƒ ===")

    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    uni_model = DirectionalClassifier(10, 32, 10, bidirectional=False)
    bi_model = DirectionalClassifier(10, 32, 10, bidirectional=True)

    criterion = nn.CrossEntropyLoss()

    # è¨“ç·´
    for model, name in [(uni_model, "å˜æ–¹å‘"), (bi_model, "åŒæ–¹å‘")]:
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

        for epoch in range(100):
            x, targets = create_long_dependency_task(batch_size=32, seq_length=50)

            optimizer.zero_grad()
            logits = model(x)
            loss = criterion(logits, targets)
            loss.backward()
            optimizer.step()

        # è©•ä¾¡
        x_test, targets_test = create_long_dependency_task(batch_size=100, seq_length=50)
        with torch.no_grad():
            logits_test = model(x_test)
            _, predicted = logits_test.max(1)
            accuracy = (predicted == targets_test).float().mean().item()

        print(f"{name}LSTM - ç²¾åº¦: {accuracy*100:.2f}%")

compare_directionality()
print("\nâ†’ ã“ã®ã‚¿ã‚¹ã‚¯ã§ã¯æƒ…å ±ãŒæœ€åˆã«ã‚ã‚‹ãŸã‚ã€åŒæ–¹å‘ã®å„ªä½æ€§ã¯å°ã•ã„")
print("  å“è©ã‚¿ã‚°ä»˜ã‘ãªã©ã€å‰å¾Œæ–‡è„ˆãŒé‡è¦ãªã‚¿ã‚¹ã‚¯ã§ã¯åŒæ–¹å‘ãŒæœ‰åˆ©")
</code></pre>

<hr>

<h2>2.5 å®Ÿè·µï¼šIMDbæ„Ÿæƒ…åˆ†æ</h2>

<h3>IMDbãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</h3>

<p><strong>IMDbï¼ˆInternet Movie Databaseï¼‰</strong>ã¯ã€æ˜ ç”»ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®æ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ï¼š</p>

<ul>
<li>50,000ä»¶ã®æ˜ ç”»ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆè¨“ç·´25,000ä»¶ã€ãƒ†ã‚¹ãƒˆ25,000ä»¶ï¼‰</li>
<li>2ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼šè‚¯å®šçš„ï¼ˆPositiveï¼‰ã€å¦å®šçš„ï¼ˆNegativeï¼‰</li>
<li>å„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯è‹±èªã®ãƒ†ã‚­ã‚¹ãƒˆ</li>
</ul>

<h3>ãƒ‡ãƒ¼ã‚¿æº–å‚™</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchtext.datasets import IMDB
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from collections import Counter

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
tokenizer = get_tokenizer('basic_english')

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
train_iter = IMDB(split='train')

# èªå½™æ§‹ç¯‰
def yield_tokens(data_iter):
    for _, text in data_iter:
        yield tokenizer(text)

# èªå½™ã‚’æ§‹ç¯‰ï¼ˆé »åº¦ä¸Šä½10,000èªï¼‰
vocab = build_vocab_from_iterator(
    yield_tokens(IMDB(split='train')),
    specials=['<unk>', '<pad>'],
    max_tokens=10000
)
vocab.set_default_index(vocab['<unk>'])

print("=== IMDbãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ ===")
print(f"èªå½™ã‚µã‚¤ã‚º: {len(vocab)}")
print(f"<pad>ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {vocab['<pad>']}")
print(f"<unk>ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {vocab['<unk>']}")

# ã‚µãƒ³ãƒ—ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
sample_text = "This movie is great!"
tokens = tokenizer(sample_text)
indices = [vocab[token] for token in tokens]
print(f"\nã‚µãƒ³ãƒ—ãƒ«: '{sample_text}'")
print(f"ãƒˆãƒ¼ã‚¯ãƒ³: {tokens}")
print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {indices}")
</code></pre>

<h3>ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹</h3>

<pre><code class="language-python">import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence

class IMDbDataset(Dataset):
    def __init__(self, split='train'):
        self.data = list(IMDB(split=split))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        label, text = self.data[idx]

        # ãƒ©ãƒ™ãƒ«ã‚’æ•°å€¤ã«å¤‰æ›ï¼ˆneg=0, pos=1ï¼‰
        label = 1 if label == 'pos' else 0

        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›
        tokens = tokenizer(text)
        indices = [vocab[token] for token in tokens]

        return torch.tensor(indices), torch.tensor(label)

def collate_batch(batch):
    """
    ãƒãƒƒãƒå†…ã®ç³»åˆ—ã‚’åŒã˜é•·ã•ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    """
    texts, labels = zip(*batch)

    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    texts_padded = pad_sequence(texts, batch_first=True, padding_value=vocab['<pad>'])
    labels = torch.stack(labels)

    return texts_padded, labels

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
train_dataset = IMDbDataset(split='train')
test_dataset = IMDbDataset(split='test')

train_loader = DataLoader(
    train_dataset,
    batch_size=64,
    shuffle=True,
    collate_fn=collate_batch
)

test_loader = DataLoader(
    test_dataset,
    batch_size=64,
    shuffle=False,
    collate_fn=collate_batch
)

print("\n=== ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ç¢ºèª ===")
texts, labels = next(iter(train_loader))
print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {texts.shape[0]}")
print(f"ç³»åˆ—é•·ï¼ˆæœ€é•·ï¼‰: {texts.shape[1]}")
print(f"ãƒ©ãƒ™ãƒ«: {labels[:5]}")
</code></pre>

<h3>LSTMæ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«</h3>

<pre><code class="language-python">import torch
import torch.nn as nn

class LSTMSentimentClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, num_layers=2, dropout=0.5):
        super(LSTMSentimentClassifier, self).__init__()

        # åŸ‹ã‚è¾¼ã¿å±¤
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab['<pad>'])

        # LSTMå±¤
        self.lstm = nn.LSTM(
            embedding_dim,
            hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )

        # åˆ†é¡å±¤
        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # åŒæ–¹å‘ãªã®ã§ *2

        # Dropout
        self.dropout = nn.Dropout(dropout)

    def forward(self, text):
        # text: (batch, seq_len)

        # åŸ‹ã‚è¾¼ã¿: (batch, seq_len, embedding_dim)
        embedded = self.dropout(self.embedding(text))

        # LSTM: output (batch, seq_len, hidden_dim * 2)
        output, (hidden, cell) = self.lstm(embedded)

        # æœ€å¾Œã®æ™‚åˆ»ã®å‡ºåŠ›ã‚’ä½¿ç”¨
        # ã¾ãŸã¯ã€é †æ–¹å‘ã¨é€†æ–¹å‘ã®æœ€çµ‚éš ã‚ŒçŠ¶æ…‹ã‚’é€£çµ
        # hidden: (num_layers * 2, batch, hidden_dim)

        # æœ€çµ‚å±¤ã®é †æ–¹å‘ã¨é€†æ–¹å‘ã‚’é€£çµ
        hidden_forward = hidden[-2, :, :]
        hidden_backward = hidden[-1, :, :]
        hidden_concat = torch.cat([hidden_forward, hidden_backward], dim=1)

        # Dropout + åˆ†é¡
        hidden_concat = self.dropout(hidden_concat)
        logits = self.fc(hidden_concat)

        return logits

# ãƒ¢ãƒ‡ãƒ«ä½œæˆ
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\nä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

model = LSTMSentimentClassifier(
    vocab_size=len(vocab),
    embedding_dim=100,
    hidden_dim=256,
    num_classes=2,
    num_layers=2,
    dropout=0.5
).to(device)

print(model)

total_params = sum(p.numel() for p in model.parameters())
print(f"\nç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
</code></pre>

<h3>è¨“ç·´ãƒ«ãƒ¼ãƒ—</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim

# æå¤±é–¢æ•°ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

def train_epoch(model, loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for texts, labels in loader:
        texts, labels = texts.to(device), labels.to(device)

        optimizer.zero_grad()
        logits = model(texts)
        loss = criterion(logits, labels)

        loss.backward()
        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå‹¾é…çˆ†ç™ºã‚’é˜²ãï¼‰
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
        optimizer.step()

        running_loss += loss.item()
        _, predicted = logits.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    epoch_loss = running_loss / len(loader)
    epoch_acc = 100. * correct / total
    return epoch_loss, epoch_acc

def test_epoch(model, loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for texts, labels in loader:
            texts, labels = texts.to(device), labels.to(device)

            logits = model(texts)
            loss = criterion(logits, labels)

            running_loss += loss.item()
            _, predicted = logits.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    epoch_loss = running_loss / len(loader)
    epoch_acc = 100. * correct / total
    return epoch_loss, epoch_acc

# è¨“ç·´å®Ÿè¡Œ
num_epochs = 5
best_acc = 0

print("\n=== è¨“ç·´é–‹å§‹ ===")
for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
    test_loss, test_acc = test_epoch(model, test_loader, criterion, device)

    print(f"Epoch [{epoch+1}/{num_epochs}]")
    print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
    print(f"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%")

    if test_acc > best_acc:
        best_acc = test_acc
        torch.save(model.state_dict(), 'best_imdb_lstm.pth')

print(f"\nè¨“ç·´å®Œäº†ï¼ãƒ™ã‚¹ãƒˆç²¾åº¦: {best_acc:.2f}%")
</code></pre>

<h3>æ¨è«–ã¨è§£é‡ˆ</h3>

<pre><code class="language-python">import torch

def predict_sentiment(model, text, vocab, tokenizer, device):
    """å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…ã‚’äºˆæ¸¬"""
    model.eval()

    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    tokens = tokenizer(text)
    indices = [vocab[token] for token in tokens]

    # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
    text_tensor = torch.tensor(indices).unsqueeze(0).to(device)  # (1, seq_len)

    # äºˆæ¸¬
    with torch.no_grad():
        logits = model(text_tensor)
        probs = torch.softmax(logits, dim=1)
        pred = logits.argmax(1).item()

    sentiment = "Positive" if pred == 1 else "Negative"
    confidence = probs[0, pred].item()

    return sentiment, confidence

# ãƒ†ã‚¹ãƒˆ
test_reviews = [
    "This movie is absolutely amazing! I loved every moment.",
    "Terrible film. Waste of time and money.",
    "It was okay, nothing special but not bad either.",
    "One of the best movies I've ever seen!",
    "Boring and predictable. Would not recommend."
]

print("\n=== æ„Ÿæƒ…åˆ†æã®äºˆæ¸¬çµæœ ===")
for review in test_reviews:
    sentiment, confidence = predict_sentiment(model, review, vocab, tokenizer, device)
    print(f"\nãƒ¬ãƒ“ãƒ¥ãƒ¼: {review}")
    print(f"äºˆæ¸¬: {sentiment} (ä¿¡é ¼åº¦: {confidence*100:.2f}%)")
</code></pre>

<hr>

<h2>2.6 LSTMã¨GRUã®ä½¿ã„åˆ†ã‘ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³</h2>

<h3>é¸æŠåŸºæº–</h3>

<table>
<thead>
<tr>
<th>çŠ¶æ³</th>
<th>æ¨å¥¨</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td>é•·ã„ç³»åˆ—ï¼ˆ>100ï¼‰</td>
<td>LSTM</td>
<td>ã‚»ãƒ«ã‚¹ãƒ†ãƒ¼ãƒˆã§é•·æœŸè¨˜æ†¶ã‚’ä¿æŒ</td>
</tr>
<tr>
<td>çŸ­ã„ç³»åˆ—ï¼ˆ<50ï¼‰</td>
<td>GRU</td>
<td>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå°‘ãªãåŠ¹ç‡çš„</td>
</tr>
<tr>
<td>è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹åˆ¶ç´„</td>
<td>GRU</td>
<td>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒç´„25%å°‘ãªã„</td>
</tr>
<tr>
<td>é«˜ç²¾åº¦ãŒå¿…é ˆ</td>
<td>LSTM</td>
<td>è¡¨ç¾åŠ›ãŒé«˜ã„</td>
</tr>
<tr>
<td>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†</td>
<td>GRU</td>
<td>è¨ˆç®—ãŒé«˜é€Ÿ</td>
</tr>
<tr>
<td>å‰å¾Œæ–‡è„ˆãŒå¿…è¦</td>
<td>åŒæ–¹å‘LSTM/GRU</td>
<td>ä¸¡æ–¹å‘ã®æƒ…å ±ã‚’æ´»ç”¨</td>
</tr>
<tr>
<td>ä¸æ˜ãªå ´åˆ</td>
<td>ä¸¡æ–¹è©¦ã™</td>
<td>ã‚¿ã‚¹ã‚¯ä¾å­˜æ€§ãŒé«˜ã„</td>
</tr>
</tbody>
</table>

<h3>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é¸ã³æ–¹</h3>

<ul>
<li><strong>éš ã‚Œå±¤ã®ã‚µã‚¤ã‚º</strong>ï¼š64ã€œ512ï¼ˆã‚¿ã‚¹ã‚¯ã®è¤‡é›‘ã•ã«å¿œã˜ã¦ï¼‰</li>
<li><strong>å±¤æ•°</strong>ï¼š1ã€œ3å±¤ï¼ˆæ·±ã™ãã‚‹ã¨éå­¦ç¿’ã®ãƒªã‚¹ã‚¯ï¼‰</li>
<li><strong>Dropout</strong>ï¼š0.2ã€œ0.5ï¼ˆéå­¦ç¿’ã‚’é˜²ãï¼‰</li>
<li><strong>åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ</strong>ï¼š50ã€œ300ï¼ˆèªå½™ã‚µã‚¤ã‚ºã«å¿œã˜ã¦ï¼‰</li>
<li><strong>å­¦ç¿’ç‡</strong>ï¼š0.0001ã€œ0.001ï¼ˆAdamãŒæ¨å¥¨ï¼‰</li>
<li><strong>ãƒãƒƒãƒã‚µã‚¤ã‚º</strong>ï¼š32ã€œ128ï¼ˆãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦ï¼‰</li>
</ul>

<h3>å®Ÿè£…ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>

<pre><code class="language-python">import torch
import torch.nn as nn

class BestPracticeLSTM(nn.Module):
    """ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’çµ„ã¿è¾¼ã‚“ã LSTMãƒ¢ãƒ‡ãƒ«"""
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):
        super(BestPracticeLSTM, self).__init__()

        # 1. Embeddingå±¤ã«padding_idxã‚’æŒ‡å®š
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)

        # 2. åŒæ–¹å‘LSTM
        self.lstm = nn.LSTM(
            embedding_dim,
            hidden_dim,
            num_layers=2,
            batch_first=True,
            dropout=0.3,  # å±¤é–“Dropout
            bidirectional=True
        )

        # 3. Batch Normalizationï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        self.batch_norm = nn.BatchNorm1d(hidden_dim * 2)

        # 4. Dropout
        self.dropout = nn.Dropout(0.5)

        # 5. åˆ†é¡å±¤
        self.fc = nn.Linear(hidden_dim * 2, num_classes)

    def forward(self, x):
        embedded = self.dropout(self.embedding(x))
        output, (hidden, cell) = self.lstm(embedded)

        # é †æ–¹å‘ã¨é€†æ–¹å‘ã®æœ€çµ‚éš ã‚ŒçŠ¶æ…‹ã‚’é€£çµ
        hidden_concat = torch.cat([hidden[-2], hidden[-1]], dim=1)

        # Batch Normï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        hidden_concat = self.batch_norm(hidden_concat)

        # Dropout + åˆ†é¡
        hidden_concat = self.dropout(hidden_concat)
        logits = self.fc(hidden_concat)

        return logits

# è¨“ç·´æ™‚ã®æ³¨æ„ç‚¹
def train_with_best_practices(model, train_loader, num_epochs=10):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=2
    )

    for epoch in range(num_epochs):
        model.train()
        for texts, labels in train_loader:
            optimizer.zero_grad()
            logits = model(texts)
            loss = criterion(logits, labels)
            loss.backward()

            # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå¿…é ˆï¼‰
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)

            optimizer.step()

        # æ¤œè¨¼ãƒ­ã‚¹ã§å­¦ç¿’ç‡ã‚’èª¿æ•´
        val_loss = evaluate(model, val_loader)
        scheduler.step(val_loss)

print("=== ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ ===")
print("1. padding_idxã‚’æŒ‡å®šã—ã¦<pad>ã‚’å­¦ç¿’å¯¾è±¡å¤–ã«")
print("2. åŒæ–¹å‘LSTMã§æ–‡è„ˆã‚’å®Œå…¨æŠŠæ¡")
print("3. Dropoutã§éå­¦ç¿’ã‚’é˜²æ­¢")
print("4. å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã§å‹¾é…çˆ†ç™ºã‚’é˜²æ­¢")
print("5. å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã§æœ€é©åŒ–ã‚’æ”¹å–„")
</code></pre>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<details>
<summary><strong>æ¼”ç¿’1ï¼šLSTMã®ã‚²ãƒ¼ãƒˆå‹•ä½œã‚’è¦³å¯Ÿ</strong></summary>

<p>LSTMã®å„ã‚²ãƒ¼ãƒˆï¼ˆå¿˜å´ã€å…¥åŠ›ã€å‡ºåŠ›ï¼‰ã®å€¤ã‚’å¯è¦–åŒ–ã—ã€ã©ã®ã‚ˆã†ã«æƒ…å ±ã‚’åˆ¶å¾¡ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# TODO: LSTMCellã‚’ä¿®æ­£ã—ã¦ã€å„ã‚²ãƒ¼ãƒˆã®å€¤ã‚’è¿”ã™ã‚ˆã†ã«ã™ã‚‹
# TODO: ç°¡å˜ãªç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§å„ã‚²ãƒ¼ãƒˆã®å€¤ã‚’æ™‚ç³»åˆ—ã«ãƒ—ãƒ­ãƒƒãƒˆ
# ãƒ’ãƒ³ãƒˆ: f_t, i_t, o_t ã®å€¤ã‚’è¨˜éŒ²ã—ã€matplotlib ã§ã‚°ãƒ©ãƒ•åŒ–
</code></pre>

</details>

<details>
<summary><strong>æ¼”ç¿’2ï¼šGRUã¨LSTMã®åæŸé€Ÿåº¦æ¯”è¼ƒ</strong></summary>

<p>åŒã˜ã‚¿ã‚¹ã‚¯ã§GRUã¨LSTMã‚’è¨“ç·´ã—ã€è¨“ç·´æ›²ç·šï¼ˆæå¤±ã¨ç²¾åº¦ï¼‰ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import matplotlib.pyplot as plt

# TODO: GRUã¨LSTMãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
# TODO: åŒã˜ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã—ã€å„ã‚¨ãƒãƒƒã‚¯ã®æå¤±ã¨ç²¾åº¦ã‚’è¨˜éŒ²
# TODO: è¨“ç·´æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆ
# è©•ä¾¡æŒ‡æ¨™: åæŸé€Ÿåº¦ã€æœ€çµ‚ç²¾åº¦ã€è¨“ç·´æ™‚é–“
</code></pre>

</details>

<details>
<summary><strong>æ¼”ç¿’3ï¼šåŒæ–¹å‘RNNã®åŠ¹æœã‚’æ¤œè¨¼</strong></summary>

<p>å“è©ã‚¿ã‚°ä»˜ã‘ã‚¿ã‚¹ã‚¯ã§ã€å˜æ–¹å‘ã¨åŒæ–¹å‘RNNã®æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# TODO: å˜æ–¹å‘ã¨åŒæ–¹å‘ã®LSTMãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…
# TODO: å“è©ã‚¿ã‚°ä»˜ã‘ã‚¿ã‚¹ã‚¯ï¼ˆå„å˜èªã®å“è©ã‚’äºˆæ¸¬ï¼‰ã§æ€§èƒ½æ¯”è¼ƒ
# ãƒ’ãƒ³ãƒˆ: torchtext.datasets ã®UD_English ãªã©ã‚’ä½¿ç”¨
# å‰å¾Œã®æ–‡è„ˆãŒé‡è¦ãªã‚¿ã‚¹ã‚¯ã§åŒæ–¹å‘ã®å„ªä½æ€§ã‚’ç¢ºèª
</code></pre>

</details>

<details>
<summary><strong>æ¼”ç¿’4ï¼šç³»åˆ—é•·ã¨æ€§èƒ½ã®é–¢ä¿‚</strong></summary>

<p>ç•°ãªã‚‹ç³»åˆ—é•·ï¼ˆ10, 50, 100, 200ï¼‰ã§LSTMã¨GRUã®æ€§èƒ½ã‚’æ¯”è¼ƒã—ã€ã©ã¡ã‚‰ãŒé•·æœŸä¾å­˜ã«å¼·ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# TODO: ç³»åˆ—é•·ã‚’å¤‰ãˆã¦é•·æœŸä¾å­˜ã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆ
# TODO: LSTMã¨GRUã§ç²¾åº¦ã‚’æ¯”è¼ƒ
# TODO: ç³»åˆ—é•· vs ç²¾åº¦ã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
# ã©ã®ç³»åˆ—é•·ã‹ã‚‰æ€§èƒ½å·®ãŒé¡•è‘—ã«ãªã‚‹ã‹åˆ†æ
</code></pre>

</details>

<details>
<summary><strong>æ¼”ç¿’5ï¼šIMDbæ„Ÿæƒ…åˆ†æã®æ”¹å–„</strong></summary>

<p>åŸºæœ¬ã®LSTMãƒ¢ãƒ‡ãƒ«ã‚’æ”¹å–„ã—ã€ãƒ†ã‚¹ãƒˆç²¾åº¦ã‚’å‘ä¸Šã•ã›ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# TODO: ä»¥ä¸‹ã®æ‰‹æ³•ã‚’è©¦ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’æ”¹å–„
# 1. äº‹å‰å­¦ç¿’æ¸ˆã¿åŸ‹ã‚è¾¼ã¿ï¼ˆGloVe, Word2Vecï¼‰ã‚’ä½¿ç”¨
# 2. Attentionæ©Ÿæ§‹ã‚’è¿½åŠ 
# 3. å±¤æ•°ã‚„hidden_sizeã‚’èª¿æ•´
# 4. Data Augmentationï¼ˆãƒãƒƒã‚¯ãƒˆãƒ©ãƒ³ã‚¹ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç­‰ï¼‰
# 5. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’

# ç›®æ¨™: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‹ã‚‰+2%ä»¥ä¸Šã®ç²¾åº¦å‘ä¸Š
</code></pre>

</details>

<hr>

<h2>ã¾ã¨ã‚</h2>

<p>ã“ã®ç« ã§ã¯ã€LSTMãƒ»GRUã¨ãã®å¿œç”¨ã«ã¤ã„ã¦å­¦ã³ã¾ã—ãŸã€‚</p>

<h3>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</h3>

<ul>
<li><strong>Vanilla RNNã®é™ç•Œ</strong>ï¼šå‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™ºã«ã‚ˆã‚Šé•·æœŸä¾å­˜é–¢ä¿‚ã®å­¦ç¿’ãŒå›°é›£</li>
<li><strong>LSTM</strong>ï¼šã‚»ãƒ«ã‚¹ãƒ†ãƒ¼ãƒˆã¨ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹ï¼ˆå¿˜å´ãƒ»å…¥åŠ›ãƒ»å‡ºåŠ›ï¼‰ã§é•·æœŸè¨˜æ†¶ã‚’å®Ÿç¾</li>
<li><strong>GRU</strong>ï¼šLSTMã‚’ç°¡ç•¥åŒ–ã€2ã¤ã®ã‚²ãƒ¼ãƒˆï¼ˆãƒªã‚»ãƒƒãƒˆãƒ»æ›´æ–°ï¼‰ã§åŠ¹ç‡çš„ã«å‹•ä½œ</li>
<li><strong>LSTMã¨GRUã®é•ã„</strong>ï¼šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€è¨ˆç®—é€Ÿåº¦ã€æ€§èƒ½ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•</li>
<li><strong>åŒæ–¹å‘RNN</strong>ï¼šå‰å¾Œä¸¡æ–¹å‘ã‹ã‚‰å‡¦ç†ã—ã€æ–‡è„ˆã‚’å®Œå…¨ã«æŠŠæ¡</li>
<li><strong>å®Ÿè·µ</strong>ï¼šIMDbæ„Ÿæƒ…åˆ†æã§å®Ÿéš›ã®NLPã‚¿ã‚¹ã‚¯ã«é©ç”¨</li>
<li><strong>ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</strong>ï¼šå‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã€Dropoutã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°</li>
</ul>

<h3>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>

<p>æ¬¡ç« ã§ã¯ã€<strong>Sequence-to-Sequenceï¼ˆSeq2Seqï¼‰</strong>ã¨<strong>Attentionæ©Ÿæ§‹</strong>ã«ã¤ã„ã¦å­¦ã³ã¾ã™ã€‚æ©Ÿæ¢°ç¿»è¨³ã‚„è¦ç´„ãªã©ã®ç³»åˆ—å¤‰æ›ã‚¿ã‚¹ã‚¯ã«å¿…é ˆã®æŠ€è¡“ã‚’ç¿’å¾—ã—ã¾ã™ã€‚</p>

<div class="navigation">
    <a href="chapter1-rnn-basics.html" class="nav-button">â† ç¬¬1ç« ï¼šRNNã®åŸºç¤</a>
    <a href="chapter3-seq2seq-attention.html" class="nav-button">ç¬¬3ç« ï¼šSeq2Seqã¨Attention â†’</a>
</div>

</main>


    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
    <p>&copy; 2024 AI Terakoya. All rights reserved.</p>
</footer>

</body>
</html>
