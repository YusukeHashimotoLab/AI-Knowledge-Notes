<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬1ç« ï¼šRNNã®åŸºç¤ã¨é †ä¼æ’­ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/rnn-introduction/chapter1-rnn-basics.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/rnn-introduction/index.html">Rnn</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 1</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬1ç« ï¼šRNNã®åŸºç¤ã¨é †ä¼æ’­</h1>
            <p class="subtitle">æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®é©å‘½ - å†å¸°å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åŸºæœ¬åŸç†ã‚’ç†è§£ã™ã‚‹</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 20-25åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´šã€œä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã¨RNNã®å¿…è¦æ€§ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… RNNã®åŸºæœ¬æ§‹é€ ã¨éš ã‚ŒçŠ¶æ…‹ã®æ¦‚å¿µã‚’èª¬æ˜ã§ãã‚‹</li>
<li>âœ… é †ä¼æ’­ã®æ•°å­¦çš„å®šç¾©ã¨è¨ˆç®—ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç¿’å¾—ã™ã‚‹</li>
<li>âœ… Backpropagation Through Time (BPTT) ã®åŸç†ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™ºå•é¡Œã®åŸå› ã¨å¯¾ç­–ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>âœ… PyTorchã§RNNã‚’å®Ÿè£…ã—ã€æ–‡å­—ãƒ¬ãƒ™ãƒ«è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
</ul>

<hr>

<h2>1.1 ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã¨ã¯</h2>

<h3>å¾“æ¥ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é™ç•Œ</h3>

<p><strong>å¾“æ¥ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</strong>ã¯ã€å›ºå®šé•·ã®å…¥åŠ›ã‚’å—ã‘å–ã‚Šã€å›ºå®šé•·ã®å‡ºåŠ›ã‚’è¿”ã—ã¾ã™ã€‚ã—ã‹ã—ã€å¤šãã®å®Ÿä¸–ç•Œã®ãƒ‡ãƒ¼ã‚¿ã¯<strong>å¯å¤‰é•·ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹</strong>ã§ã™ã€‚</p>

<blockquote>
<p>ã€Œã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã¯æ™‚é–“çš„ã¾ãŸã¯ç©ºé–“çš„ãªé †åºã‚’æŒã¤ã€‚éå»ã®æƒ…å ±ã‚’è¨˜æ†¶ã—ã€æœªæ¥ã‚’äºˆæ¸¬ã™ã‚‹èƒ½åŠ›ãŒå¿…è¦ã§ã‚ã‚‹ã€‚ã€</p>
</blockquote>

<h4>ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ä¾‹</h4>

<table>
<thead>
<tr>
<th>ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—</th>
<th>å…·ä½“ä¾‹</th>
<th>ç‰¹å¾´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>è‡ªç„¶è¨€èª</strong></td>
<td>æ–‡ç« ã€ä¼šè©±ã€ç¿»è¨³</td>
<td>å˜èªã®é †åºãŒæ„å‘³ã‚’æ±ºå®š</td>
</tr>
<tr>
<td><strong>éŸ³å£°</strong></td>
<td>éŸ³å£°èªè­˜ã€éŸ³æ¥½ç”Ÿæˆ</td>
<td>æ™‚é–“çš„ãªé€£ç¶šæ€§ã‚’æŒã¤</td>
</tr>
<tr>
<td><strong>æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿</strong></td>
<td>æ ªä¾¡ã€æ°—æ¸©ã€ã‚»ãƒ³ã‚µãƒ¼å€¤</td>
<td>éå»ã®å€¤ãŒæœªæ¥ã«å½±éŸ¿</td>
</tr>
<tr>
<td><strong>å‹•ç”»</strong></td>
<td>è¡Œå‹•èªè­˜ã€å‹•ç”»ç”Ÿæˆ</td>
<td>ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®æ™‚é–“çš„ä¾å­˜æ€§</td>
</tr>
<tr>
<td><strong>DNAé…åˆ—</strong></td>
<td>éºä¼å­è§£æã€ã‚¿ãƒ³ãƒ‘ã‚¯è³ªäºˆæ¸¬</td>
<td>å¡©åŸºé…åˆ—ã®é †åºãŒæ©Ÿèƒ½ã‚’æ±ºå®š</td>
</tr>
</tbody>
</table>

<h4>å•é¡Œï¼šãªãœãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã¯ä¸ååˆ†ã‹</h4>

<pre><code class="language-python">import numpy as np

# ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ä¾‹ï¼šç°¡å˜ãªæ–‡ç« 
sentence1 = ["I", "love", "machine", "learning"]
sentence2 = ["machine", "learning", "I", "love"]

print("æ–‡ç« 1:", " ".join(sentence1))
print("æ–‡ç« 2:", " ".join(sentence2))
print("\nåŒã˜å˜èªã‚’å«ã‚€ãŒã€æ„å‘³ã¯ç•°ãªã‚‹")

# ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å•é¡Œç‚¹
# 1. å›ºå®šé•·å…¥åŠ›ãŒå¿…è¦
# 2. å˜èªã®é †åºæƒ…å ±ãŒå¤±ã‚ã‚Œã‚‹ï¼ˆBag-of-Wordsçš„ãªæ‰±ã„ï¼‰

# ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é•·ã•ãŒç•°ãªã‚‹ä¾‹
sequences = [
    ["Hello"],
    ["How", "are", "you"],
    ["The", "quick", "brown", "fox", "jumps"]
]

print("\n=== å¯å¤‰é•·ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å•é¡Œ ===")
for i, seq in enumerate(sequences, 1):
    print(f"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹{i}: é•·ã•={len(seq)}, å†…å®¹={seq}")

print("\nãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã¯ã€ã“ã‚Œã‚‰ã‚’çµ±ä¸€çš„ã«æ‰±ãˆãªã„")
print("â†’ RNNãŒå¿…è¦ï¼")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>æ–‡ç« 1: I love machine learning
æ–‡ç« 2: machine learning I love

åŒã˜å˜èªã‚’å«ã‚€ãŒã€æ„å‘³ã¯ç•°ãªã‚‹

=== å¯å¤‰é•·ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å•é¡Œ ===
ã‚·ãƒ¼ã‚±ãƒ³ã‚¹1: é•·ã•=1, å†…å®¹=['Hello']
ã‚·ãƒ¼ã‚±ãƒ³ã‚¹2: é•·ã•=3, å†…å®¹=['How', 'are', 'you']
ã‚·ãƒ¼ã‚±ãƒ³ã‚¹3: é•·ã•=5, å†…å®¹=['The', 'quick', 'brown', 'fox', 'jumps']

ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã¯ã€ã“ã‚Œã‚‰ã‚’çµ±ä¸€çš„ã«æ‰±ãˆãªã„
â†’ RNNãŒå¿…è¦ï¼
</code></pre>

<h3>RNNã®ã‚¿ã‚¹ã‚¯åˆ†é¡</h3>

<p>RNNã¯å…¥å‡ºåŠ›ã®å½¢å¼ã«ã‚ˆã‚Šã€ä»¥ä¸‹ã®ã‚ˆã†ã«åˆ†é¡ã•ã‚Œã¾ã™ï¼š</p>

<div class="mermaid">
graph TD
    subgraph "One-to-Manyï¼ˆç³»åˆ—ç”Ÿæˆï¼‰"
    A1[1ã¤ã®å…¥åŠ›] --> B1[è¤‡æ•°ã®å‡ºåŠ›]
    end

    subgraph "Many-to-Oneï¼ˆç³»åˆ—åˆ†é¡ï¼‰"
    A2[è¤‡æ•°ã®å…¥åŠ›] --> B2[1ã¤ã®å‡ºåŠ›]
    end

    subgraph "Many-to-Manyï¼ˆç³»åˆ—å¤‰æ›ï¼‰"
    A3[è¤‡æ•°ã®å…¥åŠ›] --> B3[è¤‡æ•°ã®å‡ºåŠ›]
    end

    subgraph "Many-to-Manyï¼ˆåŒæœŸï¼‰"
    A4[è¤‡æ•°ã®å…¥åŠ›] --> B4[å„ã‚¹ãƒ†ãƒƒãƒ—ã§å‡ºåŠ›]
    end

    style A1 fill:#e3f2fd
    style B1 fill:#ffebee
    style A2 fill:#e3f2fd
    style B2 fill:#ffebee
    style A3 fill:#e3f2fd
    style B3 fill:#ffebee
    style A4 fill:#e3f2fd
    style B4 fill:#ffebee
</div>

<table>
<thead>
<tr>
<th>ã‚¿ã‚¤ãƒ—</th>
<th>å…¥åŠ›â†’å‡ºåŠ›</th>
<th>å¿œç”¨ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>One-to-Many</strong></td>
<td>1 â†’ N</td>
<td>ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆã€éŸ³æ¥½ç”Ÿæˆ</td>
</tr>
<tr>
<td><strong>Many-to-One</strong></td>
<td>N â†’ 1</td>
<td>æ„Ÿæƒ…åˆ†æã€æ–‡æ›¸åˆ†é¡</td>
</tr>
<tr>
<td><strong>Many-to-Manyï¼ˆéåŒæœŸï¼‰</strong></td>
<td>N â†’ M</td>
<td>æ©Ÿæ¢°ç¿»è¨³ã€æ–‡ç« è¦ç´„</td>
</tr>
<tr>
<td><strong>Many-to-Manyï¼ˆåŒæœŸï¼‰</strong></td>
<td>N â†’ N</td>
<td>å“è©ã‚¿ã‚°ä»˜ã‘ã€å‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†é¡</td>
</tr>
</tbody>
</table>

<hr>

<h2>1.2 RNNã®åŸºæœ¬æ§‹é€ </h2>

<h3>éš ã‚ŒçŠ¶æ…‹ã®æ¦‚å¿µ</h3>

<p><strong>RNNï¼ˆRecurrent Neural Networkï¼‰</strong>ã®æ ¸å¿ƒã¯ã€<strong>éš ã‚ŒçŠ¶æ…‹ï¼ˆHidden Stateï¼‰</strong>ã‚’ç”¨ã„ã¦éå»ã®æƒ…å ±ã‚’è¨˜æ†¶ã™ã‚‹ä»•çµ„ã¿ã§ã™ã€‚</p>

<h4>RNNã®åŸºæœ¬æ–¹ç¨‹å¼</h4>

<p>æ™‚åˆ» $t$ ã«ãŠã‘ã‚‹éš ã‚ŒçŠ¶æ…‹ $h_t$ ã¨å‡ºåŠ› $y_t$ ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã¾ã™ï¼š</p>

$$
\begin{align}
h_t &= \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h) \\
y_t &= W_{hy} h_t + b_y
\end{align}
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$x_t$: æ™‚åˆ» $t$ ã®å…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ«</li>
<li>$h_t$: æ™‚åˆ» $t$ ã®éš ã‚ŒçŠ¶æ…‹ï¼ˆHidden Stateï¼‰</li>
<li>$h_{t-1}$: æ™‚åˆ» $t-1$ ã®éš ã‚ŒçŠ¶æ…‹ï¼ˆå‰ã®æ™‚åˆ»ã®è¨˜æ†¶ï¼‰</li>
<li>$y_t$: æ™‚åˆ» $t$ ã®å‡ºåŠ›</li>
<li>$W_{xh}$: å…¥åŠ›ã‹ã‚‰éš ã‚ŒçŠ¶æ…‹ã¸ã®é‡ã¿è¡Œåˆ—</li>
<li>$W_{hh}$: éš ã‚ŒçŠ¶æ…‹ã‹ã‚‰éš ã‚ŒçŠ¶æ…‹ã¸ã®é‡ã¿è¡Œåˆ—ï¼ˆå†å¸°çš„ãªæ¥ç¶šï¼‰</li>
<li>$W_{hy}$: éš ã‚ŒçŠ¶æ…‹ã‹ã‚‰å‡ºåŠ›ã¸ã®é‡ã¿è¡Œåˆ—</li>
<li>$b_h, b_y$: ãƒã‚¤ã‚¢ã‚¹é …</li>
</ul>

<h3>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å…±æœ‰</h3>

<p>RNNã®é‡è¦ãªç‰¹å¾´ã¯ã€<strong>å…¨ã¦ã®æ™‚åˆ»ã§åŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å…±æœ‰</strong>ã™ã‚‹ã“ã¨ã§ã™ã€‚</p>

<blockquote>
<p><strong>é‡è¦</strong>: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å…±æœ‰ã«ã‚ˆã‚Šã€ä»»æ„ã®é•·ã•ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æ‰±ãˆã‚‹ã¨åŒæ™‚ã«ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã¾ã™ã€‚</p>
</blockquote>

<table>
<thead>
<tr>
<th>æ¯”è¼ƒé …ç›®</th>
<th>ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰NN</th>
<th>RNN</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°</strong></td>
<td>å±¤ã”ã¨ã«ç‹¬ç«‹</td>
<td>å…¨æ™‚åˆ»ã§å…±æœ‰</td>
</tr>
<tr>
<td><strong>å…¥åŠ›é•·</strong></td>
<td>å›ºå®š</td>
<td>å¯å¤‰</td>
</tr>
<tr>
<td><strong>è¨˜æ†¶æ©Ÿæ§‹</strong></td>
<td>ãªã—</td>
<td>éš ã‚ŒçŠ¶æ…‹</td>
</tr>
<tr>
<td><strong>è¨ˆç®—ã‚°ãƒ©ãƒ•</strong></td>
<td>éå·¡å›</td>
<td>å·¡å›ï¼ˆå†å¸°çš„ï¼‰</td>
</tr>
</tbody>
</table>

<h3>å±•é–‹å›³ï¼ˆUnrollingï¼‰</h3>

<p>RNNã®è¨ˆç®—ã‚’ç†è§£ã™ã‚‹ãŸã‚ã€æ™‚é–“æ–¹å‘ã«<strong>å±•é–‹ï¼ˆUnrollï¼‰</strong>ã—ã¦å¯è¦–åŒ–ã—ã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    X0[x_0] --> H0[h_0]
    H0 --> Y0[y_0]
    H0 --> H1[h_1]
    X1[x_1] --> H1
    H1 --> Y1[y_1]
    H1 --> H2[h_2]
    X2[x_2] --> H2
    H2 --> Y2[y_2]
    H2 --> H3[h_3]
    X3[x_3] --> H3
    H3 --> Y3[y_3]

    style X0 fill:#e3f2fd
    style X1 fill:#e3f2fd
    style X2 fill:#e3f2fd
    style X3 fill:#e3f2fd
    style H0 fill:#fff3e0
    style H1 fill:#fff3e0
    style H2 fill:#fff3e0
    style H3 fill:#fff3e0
    style Y0 fill:#ffebee
    style Y1 fill:#ffebee
    style Y2 fill:#ffebee
    style Y3 fill:#ffebee
</div>

<pre><code class="language-python">import numpy as np

# RNNã®æ‰‹å‹•å®Ÿè£…ï¼ˆç°¡ç•¥ç‰ˆï¼‰
class SimpleRNN:
    def __init__(self, input_size, hidden_size, output_size):
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸåŒ–ï¼ˆXavierã®åˆæœŸåŒ–ã‚’ç°¡ç•¥åŒ–ï¼‰
        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01
        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01
        self.Why = np.random.randn(output_size, hidden_size) * 0.01
        self.bh = np.zeros((hidden_size, 1))
        self.by = np.zeros((output_size, 1))

        self.hidden_size = hidden_size

    def forward(self, inputs):
        """
        é †ä¼æ’­ã‚’å®Ÿè¡Œ

        Parameters:
        -----------
        inputs : list of np.array
            å„æ™‚åˆ»ã®å…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆ [x_0, x_1, ..., x_T]

        Returns:
        --------
        outputs : list of np.array
            å„æ™‚åˆ»ã®å‡ºåŠ›
        hidden_states : list of np.array
            å„æ™‚åˆ»ã®éš ã‚ŒçŠ¶æ…‹
        """
        h = np.zeros((self.hidden_size, 1))  # åˆæœŸéš ã‚ŒçŠ¶æ…‹
        hidden_states = []
        outputs = []

        for x in inputs:
            # éš ã‚ŒçŠ¶æ…‹ã®æ›´æ–°: h_t = tanh(Wxh @ x_t + Whh @ h_{t-1} + bh)
            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)
            # å‡ºåŠ›: y_t = Why @ h_t + by
            y = np.dot(self.Why, h) + self.by

            hidden_states.append(h)
            outputs.append(y)

        return outputs, hidden_states

# ä½¿ç”¨ä¾‹
input_size = 3
hidden_size = 5
output_size = 2
sequence_length = 4

rnn = SimpleRNN(input_size, hidden_size, output_size)

# ãƒ€ãƒŸãƒ¼ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¥åŠ›
inputs = [np.random.randn(input_size, 1) for _ in range(sequence_length)]

# é †ä¼æ’­
outputs, hidden_states = rnn.forward(inputs)

print("=== SimpleRNN ã®å‹•ä½œç¢ºèª ===\n")
print(f"å…¥åŠ›ã‚µã‚¤ã‚º: {input_size}")
print(f"éš ã‚ŒçŠ¶æ…‹ã‚µã‚¤ã‚º: {hidden_size}")
print(f"å‡ºåŠ›ã‚µã‚¤ã‚º: {output_size}")
print(f"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {sequence_length}\n")

print("å„æ™‚åˆ»ã®éš ã‚ŒçŠ¶æ…‹ã®å½¢çŠ¶:")
for t, h in enumerate(hidden_states):
    print(f"  h_{t}: {h.shape}")

print("\nå„æ™‚åˆ»ã®å‡ºåŠ›ã®å½¢çŠ¶:")
for t, y in enumerate(outputs):
    print(f"  y_{t}: {y.shape}")

print("\nãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
print(f"  Wxh (å…¥åŠ›â†’éš ã‚Œ): {rnn.Wxh.shape}")
print(f"  Whh (éš ã‚Œâ†’éš ã‚Œ): {rnn.Whh.shape}")
print(f"  Why (éš ã‚Œâ†’å‡ºåŠ›): {rnn.Why.shape}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== SimpleRNN ã®å‹•ä½œç¢ºèª ===

å…¥åŠ›ã‚µã‚¤ã‚º: 3
éš ã‚ŒçŠ¶æ…‹ã‚µã‚¤ã‚º: 5
å‡ºåŠ›ã‚µã‚¤ã‚º: 2
ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: 4

å„æ™‚åˆ»ã®éš ã‚ŒçŠ¶æ…‹ã®å½¢çŠ¶:
  h_0: (5, 1)
  h_1: (5, 1)
  h_2: (5, 1)
  h_3: (5, 1)

å„æ™‚åˆ»ã®å‡ºåŠ›ã®å½¢çŠ¶:
  y_0: (2, 1)
  y_1: (2, 1)
  y_2: (2, 1)
  y_3: (2, 1)

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
  Wxh (å…¥åŠ›â†’éš ã‚Œ): (5, 3)
  Whh (éš ã‚Œâ†’éš ã‚Œ): (5, 5)
  Why (éš ã‚Œâ†’å‡ºåŠ›): (2, 5)
</code></pre>

<hr>

<h2>1.3 é †ä¼æ’­ã®æ•°å­¦çš„å®šç¾©</h2>

<h3>è©³ç´°ãªè¨ˆç®—ãƒ—ãƒ­ã‚»ã‚¹</h3>

<p>RNNã®é †ä¼æ’­ã‚’æ®µéšçš„ã«è¦‹ã¦ã„ãã¾ã—ã‚‡ã†ã€‚ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•· $T$ ã®å…¥åŠ› $(x_1, x_2, \ldots, x_T)$ ã‚’è€ƒãˆã¾ã™ã€‚</p>

<h4>ã‚¹ãƒ†ãƒƒãƒ—1: åˆæœŸéš ã‚ŒçŠ¶æ…‹</h4>

$$
h_0 = \mathbf{0} \quad \text{ã¾ãŸã¯} \quad h_0 \sim \mathcal{N}(0, \sigma^2)
$$

<p>é€šå¸¸ã€åˆæœŸéš ã‚ŒçŠ¶æ…‹ã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã§åˆæœŸåŒ–ã•ã‚Œã¾ã™ã€‚</p>

<h4>ã‚¹ãƒ†ãƒƒãƒ—2: å„æ™‚åˆ»ã®éš ã‚ŒçŠ¶æ…‹ã®æ›´æ–°</h4>

<p>æ™‚åˆ» $t = 1, 2, \ldots, T$ ã«ã¤ã„ã¦ï¼š</p>

$$
\begin{align}
a_t &= W_{xh} x_t + W_{hh} h_{t-1} + b_h \quad \text{ï¼ˆç·šå½¢å¤‰æ›ï¼‰} \\
h_t &= \tanh(a_t) \quad \text{ï¼ˆæ´»æ€§åŒ–é–¢æ•°ï¼‰}
\end{align}
$$

<h4>ã‚¹ãƒ†ãƒƒãƒ—3: å‡ºåŠ›ã®è¨ˆç®—</h4>

$$
y_t = W_{hy} h_t + b_y
$$

<p>åˆ†é¡ã‚¿ã‚¹ã‚¯ã®å ´åˆã€ã•ã‚‰ã«ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã‚’é©ç”¨ï¼š</p>

$$
\hat{y}_t = \text{softmax}(y_t) = \frac{\exp(y_t)}{\sum_j \exp(y_{t,j})}
$$

<h3>å…·ä½“çš„ãªæ•°å€¤ä¾‹</h3>

<pre><code class="language-python">import numpy as np

# å°ã•ãªä¾‹ã§è¨ˆç®—ã‚’è¿½è·¡
np.random.seed(42)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®šï¼ˆç°¡å˜ã®ãŸã‚å°ã•ãªã‚µã‚¤ã‚ºï¼‰
input_size = 2
hidden_size = 3
output_size = 1

# é‡ã¿ã®åˆæœŸåŒ–ï¼ˆå›ºå®šå€¤ã§ç¢ºèªã—ã‚„ã™ãï¼‰
Wxh = np.array([[0.1, 0.2],
                [0.3, 0.4],
                [0.5, 0.6]])  # (3, 2)

Whh = np.array([[0.1, 0.2, 0.3],
                [0.4, 0.5, 0.6],
                [0.7, 0.8, 0.9]])  # (3, 3)

Why = np.array([[0.2, 0.4, 0.6]])  # (1, 3)

bh = np.zeros((3, 1))
by = np.zeros((1, 1))

# ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¥åŠ›ï¼ˆ3æ™‚åˆ»ï¼‰
x1 = np.array([[1.0], [0.5]])
x2 = np.array([[0.8], [0.3]])
x3 = np.array([[0.6], [0.9]])

print("=== RNN é †ä¼æ’­ã®è©³ç´°è¨ˆç®— ===\n")

# åˆæœŸéš ã‚ŒçŠ¶æ…‹
h0 = np.zeros((3, 1))
print("åˆæœŸéš ã‚ŒçŠ¶æ…‹ h_0:")
print(h0.T)

# æ™‚åˆ» t=1
print("\n--- æ™‚åˆ» t=1 ---")
print(f"å…¥åŠ› x_1: {x1.T}")
a1 = np.dot(Wxh, x1) + np.dot(Whh, h0) + bh
print(f"ç·šå½¢å¤‰æ› a_1 = Wxh @ x_1 + Whh @ h_0 + bh:")
print(a1.T)
h1 = np.tanh(a1)
print(f"éš ã‚ŒçŠ¶æ…‹ h_1 = tanh(a_1):")
print(h1.T)
y1 = np.dot(Why, h1) + by
print(f"å‡ºåŠ› y_1 = Why @ h_1 + by:")
print(y1.T)

# æ™‚åˆ» t=2
print("\n--- æ™‚åˆ» t=2 ---")
print(f"å…¥åŠ› x_2: {x2.T}")
a2 = np.dot(Wxh, x2) + np.dot(Whh, h1) + bh
print(f"ç·šå½¢å¤‰æ› a_2 = Wxh @ x_2 + Whh @ h_1 + bh:")
print(a2.T)
h2 = np.tanh(a2)
print(f"éš ã‚ŒçŠ¶æ…‹ h_2 = tanh(a_2):")
print(h2.T)
y2 = np.dot(Why, h2) + by
print(f"å‡ºåŠ› y_2 = Why @ h_2 + by:")
print(y2.T)

# æ™‚åˆ» t=3
print("\n--- æ™‚åˆ» t=3 ---")
print(f"å…¥åŠ› x_3: {x3.T}")
a3 = np.dot(Wxh, x3) + np.dot(Whh, h2) + bh
print(f"ç·šå½¢å¤‰æ› a_3 = Wxh @ x_3 + Whh @ h_2 + bh:")
print(a3.T)
h3 = np.tanh(a3)
print(f"éš ã‚ŒçŠ¶æ…‹ h_3 = tanh(a_3):")
print(h3.T)
y3 = np.dot(Why, h3) + by
print(f"å‡ºåŠ› y_3 = Why @ h_3 + by:")
print(y3.T)

print("\n=== ã¾ã¨ã‚ ===")
print("éš ã‚ŒçŠ¶æ…‹ãŒæ™‚é–“ã¨ã¨ã‚‚ã«æ›´æ–°ã•ã‚Œã€éå»ã®æƒ…å ±ã‚’ä¿æŒã—ã¦ã„ã‚‹ã“ã¨ãŒç¢ºèªã§ãã¾ã™")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== RNN é †ä¼æ’­ã®è©³ç´°è¨ˆç®— ===

åˆæœŸéš ã‚ŒçŠ¶æ…‹ h_0:
[[0. 0. 0.]]

--- æ™‚åˆ» t=1 ---
å…¥åŠ› x_1: [[1.  0.5]]
ç·šå½¢å¤‰æ› a_1 = Wxh @ x_1 + Whh @ h_0 + bh:
[[0.2 0.5 0.8]]
éš ã‚ŒçŠ¶æ…‹ h_1 = tanh(a_1):
[[0.19737532 0.46211716 0.66403677]]
å‡ºåŠ› y_1 = Why @ h_1 + by:
[[0.62507946]]

--- æ™‚åˆ» t=2 ---
å…¥åŠ› x_2: [[0.8 0.3]]
ç·šå½¢å¤‰æ› a_2 = Wxh @ x_2 + Whh @ h_1 + bh:
[[0.29047307 0.65308434 1.00569561]]
éš ã‚ŒçŠ¶æ…‹ h_2 = tanh(a_2):
[[0.28267734 0.57345841 0.76354129]]
å‡ºåŠ› y_2 = Why @ h_2 + by:
[[0.74487427]]

--- æ™‚åˆ» t=3 ---
å…¥åŠ› x_3: [[0.6 0.9]]
ç·šå½¢å¤‰æ› a_3 = Wxh @ x_3 + Whh @ h_2 + bh:
[[0.35687144 0.8098169  1.25276236]]
éš ã‚ŒçŠ¶æ…‹ h_3 = tanh(a_3):
[[0.34242503 0.66919951 0.84956376]]
å‡ºåŠ› y_3 = Why @ h_3 + by:
[[0.84642439]]

=== ã¾ã¨ã‚ ===
éš ã‚ŒçŠ¶æ…‹ãŒæ™‚é–“ã¨ã¨ã‚‚ã«æ›´æ–°ã•ã‚Œã€éå»ã®æƒ…å ±ã‚’ä¿æŒã—ã¦ã„ã‚‹ã“ã¨ãŒç¢ºèªã§ãã¾ã™
</code></pre>

<hr>

<h2>1.4 Backpropagation Through Time (BPTT)</h2>

<h3>BPTTã®åŸºæœ¬åŸç†</h3>

<p><strong>Backpropagation Through Time (BPTT)</strong>ã¯ã€RNNã®å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚å±•é–‹ã•ã‚ŒãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«å¯¾ã—ã¦ã€é€šå¸¸ã®èª¤å·®é€†ä¼æ’­æ³•ã‚’é©ç”¨ã—ã¾ã™ã€‚</p>

<h4>æå¤±é–¢æ•°</h4>

<p>ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã®æå¤±ã¯ã€å„æ™‚åˆ»ã®æå¤±ã®åˆè¨ˆï¼š</p>

$$
L = \sum_{t=1}^{T} L_t = \sum_{t=1}^{T} \mathcal{L}(y_t, \hat{y}_t)
$$

<h4>å‹¾é…ã®è¨ˆç®—</h4>

<p>æ™‚åˆ» $t$ ã«ãŠã‘ã‚‹éš ã‚ŒçŠ¶æ…‹ $h_t$ ã«é–¢ã™ã‚‹å‹¾é…ã¯ã€æœªæ¥ã®å…¨ã¦ã®æ™‚åˆ»ã‹ã‚‰ã®å¯„ä¸ã‚’å«ã¿ã¾ã™ï¼š</p>

$$
\frac{\partial L}{\partial h_t} = \frac{\partial L_t}{\partial h_t} + \frac{\partial L_{t+1}}{\partial h_t}
$$

<p>ã“ã‚Œã‚’å†å¸°çš„ã«å±•é–‹ã™ã‚‹ã¨ï¼š</p>

$$
\frac{\partial L}{\partial h_t} = \sum_{k=t}^{T} \frac{\partial L_k}{\partial h_k} \prod_{j=t+1}^{k} \frac{\partial h_j}{\partial h_{j-1}}
$$

<h3>å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™ºå•é¡Œ</h3>

<p>BPTTã®æœ€å¤§ã®èª²é¡Œã¯ã€<strong>å‹¾é…æ¶ˆå¤±ï¼ˆVanishing Gradientï¼‰</strong>ã¨<strong>å‹¾é…çˆ†ç™ºï¼ˆExploding Gradientï¼‰</strong>ã§ã™ã€‚</p>

<blockquote>
<p><strong>å•é¡Œã®æœ¬è³ª</strong>: é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§ã¯ã€å‹¾é…ãŒæ™‚é–“æ–¹å‘ã«é€†ä¼æ’­ã™ã‚‹éš›ã«ã€æŒ‡æ•°çš„ã«æ¸›è¡°ã¾ãŸã¯å¢—å¤§ã™ã‚‹ã€‚</p>
</blockquote>

<h4>æ•°å­¦çš„ãªèª¬æ˜</h4>

<p>éš ã‚ŒçŠ¶æ…‹ã®å‹¾é…ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«é€£é–å¾‹ã§è¨ˆç®—ã•ã‚Œã¾ã™ï¼š</p>

$$
\frac{\partial h_t}{\partial h_{t-1}} = W_{hh}^T \cdot \text{diag}(\tanh'(a_{t-1}))
$$

<p>$k$ æ™‚åˆ»é¡ã‚‹ã¨ï¼š</p>

$$
\frac{\partial h_t}{\partial h_{t-k}} = \prod_{j=0}^{k-1} \frac{\partial h_{t-j}}{\partial h_{t-j-1}}
$$

<p>ã“ã®ç©ãŒï¼š</p>
<ul>
<li><strong>å‹¾é…æ¶ˆå¤±</strong>: $\|W_{hh}\| < 1$ ã‹ã¤ $|\tanh'(x)| < 1$ ã®å ´åˆã€ç©ãŒ0ã«è¿‘ã¥ã</li>
<li><strong>å‹¾é…çˆ†ç™º</strong>: $\|W_{hh}\| > 1$ ã®å ´åˆã€ç©ãŒç™ºæ•£</li>
</ul>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™ºã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
def simulate_gradient_flow(W_norm, sequence_length=50):
    """
    å‹¾é…ã®ä¼æ’­ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ

    Parameters:
    -----------
    W_norm : float
        é‡ã¿è¡Œåˆ—ã®ãƒãƒ«ãƒ ï¼ˆç°¡ç•¥åŒ–ã—ã¦1æ¬¡å…ƒã§è€ƒãˆã‚‹ï¼‰
    sequence_length : int
        ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é•·ã•

    Returns:
    --------
    gradients : np.array
        å„æ™‚åˆ»ã®å‹¾é…ã®å¤§ãã•
    """
    # tanh ã®å¾®åˆ†ã®å¹³å‡çš„ãªå€¤ï¼ˆç´„0.4ç¨‹åº¦ï¼‰
    tanh_derivative = 0.4

    # å‹¾é…ã®åˆæœŸå€¤
    gradient = 1.0
    gradients = [gradient]

    # æ™‚é–“ã‚’é¡ã£ã¦å‹¾é…ã‚’è¨ˆç®—
    for t in range(sequence_length - 1):
        gradient *= W_norm * tanh_derivative
        gradients.append(gradient)

    return np.array(gradients[::-1])  # æ™‚é–“é †ã«ä¸¦ã³æ›¿ãˆ

# ç•°ãªã‚‹ãƒãƒ«ãƒ ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
sequence_length = 50
W_norms = [0.5, 1.0, 2.0, 4.0]
colors = ['blue', 'green', 'orange', 'red']
labels = ['W_norm=0.5 (æ¶ˆå¤±)', 'W_norm=1.0 (å®‰å®š)', 'W_norm=2.0 (çˆ†ç™º)', 'W_norm=4.0 (çˆ†ç™º)']

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

for W_norm, color, label in zip(W_norms, colors, labels):
    gradients = simulate_gradient_flow(W_norm, sequence_length)

    # ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒ«
    ax1.plot(gradients, color=color, label=label, linewidth=2)

    # å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«
    ax2.semilogy(gradients, color=color, label=label, linewidth=2)

ax1.set_xlabel('æ™‚åˆ»ï¼ˆéå» â† ç¾åœ¨ï¼‰')
ax1.set_ylabel('å‹¾é…ã®å¤§ãã•')
ax1.set_title('å‹¾é…ã®ä¼æ’­ï¼ˆç·šå½¢ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰')
ax1.legend()
ax1.grid(True, alpha=0.3)

ax2.set_xlabel('æ™‚åˆ»ï¼ˆéå» â† ç¾åœ¨ï¼‰')
ax2.set_ylabel('å‹¾é…ã®å¤§ãã•ï¼ˆå¯¾æ•°ï¼‰')
ax2.set_title('å‹¾é…ã®ä¼æ’­ï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
print("å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™ºã®å¯è¦–åŒ–ã‚’è¡¨ç¤ºã—ã¾ã—ãŸ")

# æ•°å€¤çš„ãªåˆ†æ
print("\n=== å‹¾é…ã®æ¸›è¡°ãƒ»å¢—å¤§ã®åˆ†æ ===\n")
for W_norm in W_norms:
    gradients = simulate_gradient_flow(W_norm, sequence_length)
    print(f"W_norm={W_norm}:")
    print(f"  åˆæœŸå‹¾é…: {gradients[-1]:.6f}")
    print(f"  50æ™‚åˆ»å‰ã®å‹¾é…: {gradients[0]:.6e}")
    print(f"  æ¸›è¡°ç‡: {gradients[0]/gradients[-1]:.6e}\n")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™ºã®å¯è¦–åŒ–ã‚’è¡¨ç¤ºã—ã¾ã—ãŸ

=== å‹¾é…ã®æ¸›è¡°ãƒ»å¢—å¤§ã®åˆ†æ ===

W_norm=0.5:
  åˆæœŸå‹¾é…: 1.000000
  50æ™‚åˆ»å‰ã®å‹¾é…: 7.105427e-15
  æ¸›è¡°ç‡: 7.105427e-15

W_norm=1.0:
  åˆæœŸå‹¾é…: 1.000000
  50æ™‚åˆ»å‰ã®å‹¾é…: 1.125899e-12
  æ¸›è¡°ç‡: 1.125899e-12

W_norm=2.0:
  åˆæœŸå‹¾é…: 1.000000
  50æ™‚åˆ»å‰ã®å‹¾é…: 1.152922e+03
  æ¸›è¡°ç‡: 1.152922e+03

W_norm=4.0:
  åˆæœŸå‹¾é…: 1.000000
  50æ™‚åˆ»å‰ã®å‹¾é…: 1.329228e+12
  æ¸›è¡°ç‡: 1.329228e+12
</code></pre>

<h3>å‹¾é…çˆ†ç™ºã¸ã®å¯¾ç­–ï¼šå‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°</h3>

<p><strong>å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆGradient Clippingï¼‰</strong>ã¯ã€å‹¾é…ã®ãƒãƒ«ãƒ ãŒé–¾å€¤ã‚’è¶…ãˆãŸå ´åˆã€å‹¾é…ã‚’ã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

$$
\mathbf{g} \leftarrow \begin{cases}
\mathbf{g} & \text{if } \|\mathbf{g}\| \leq \theta \\
\theta \frac{\mathbf{g}}{\|\mathbf{g}\|} & \text{if } \|\mathbf{g}\| > \theta
\end{cases}
$$

<pre><code class="language-python">import torch

def gradient_clipping_example():
    """å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®å®Ÿè£…ä¾‹"""
    # ãƒ€ãƒŸãƒ¼ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‹¾é…
    params = [
        torch.randn(10, 10, requires_grad=True),
        torch.randn(5, 10, requires_grad=True)
    ]

    # å¤§ããªå‹¾é…ã‚’è¨­å®šï¼ˆçˆ†ç™ºã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
    params[0].grad = torch.randn(10, 10) * 100  # å¤§ããªå‹¾é…
    params[1].grad = torch.randn(5, 10) * 100

    # ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å‰ã®ãƒãƒ«ãƒ 
    total_norm_before = 0
    for p in params:
        if p.grad is not None:
            total_norm_before += p.grad.data.norm(2).item() ** 2
    total_norm_before = total_norm_before ** 0.5

    # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
    max_norm = 1.0
    torch.nn.utils.clip_grad_norm_(params, max_norm)

    # ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å¾Œã®ãƒãƒ«ãƒ 
    total_norm_after = 0
    for p in params:
        if p.grad is not None:
            total_norm_after += p.grad.data.norm(2).item() ** 2
    total_norm_after = total_norm_after ** 0.5

    print("=== å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®åŠ¹æœ ===\n")
    print(f"ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å‰ã®å‹¾é…ãƒãƒ«ãƒ : {total_norm_before:.4f}")
    print(f"ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å¾Œã®å‹¾é…ãƒãƒ«ãƒ : {total_norm_after:.4f}")
    print(f"é–¾å€¤: {max_norm}")
    print(f"\nå‹¾é…ãŒé–¾å€¤ä»¥ä¸‹ã«åˆ¶é™ã•ã‚Œã¾ã—ãŸï¼")

gradient_clipping_example()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®åŠ¹æœ ===

ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å‰ã®å‹¾é…ãƒãƒ«ãƒ : 163.4521
ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å¾Œã®å‹¾é…ãƒãƒ«ãƒ : 1.0000
é–¾å€¤: 1.0

å‹¾é…ãŒé–¾å€¤ä»¥ä¸‹ã«åˆ¶é™ã•ã‚Œã¾ã—ãŸï¼
</code></pre>

<h3>å‹¾é…æ¶ˆå¤±ã¸ã®å¯¾ç­–ï¼šã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ”¹å–„</h3>

<p>å‹¾é…æ¶ˆå¤±å•é¡Œã®æ ¹æœ¬çš„ãªè§£æ±ºç­–ã¨ã—ã¦ã€ä»¥ä¸‹ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã™ï¼š</p>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>ä¸»ãªç‰¹å¾´</th>
<th>åŠ¹æœ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>LSTM</strong></td>
<td>ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹ã§æƒ…å ±ã®æµã‚Œã‚’åˆ¶å¾¡</td>
<td>é•·æœŸä¾å­˜æ€§ã‚’å­¦ç¿’å¯èƒ½</td>
</tr>
<tr>
<td><strong>GRU</strong></td>
<td>LSTMã®ç°¡ç•¥ç‰ˆã€2ã¤ã®ã‚²ãƒ¼ãƒˆ</td>
<td>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›ã€é«˜é€Ÿ</td>
</tr>
<tr>
<td><strong>Residual Connection</strong></td>
<td>ã‚¹ã‚­ãƒƒãƒ—æ¥ç¶šã§å‹¾é…ã‚’ç›´æ¥ä¼æ’­</td>
<td>æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å­¦ç¿’</td>
</tr>
<tr>
<td><strong>Layer Normalization</strong></td>
<td>å±¤ã”ã¨ã«æ­£è¦åŒ–</td>
<td>å­¦ç¿’ã®å®‰å®šåŒ–</td>
</tr>
</tbody>
</table>

<blockquote>
<p><strong>æ³¨</strong>: LSTM ã¨ GRU ã«ã¤ã„ã¦ã¯ã€æ¬¡ç« ã§è©³ã—ãè§£èª¬ã—ã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>1.5 PyTorchã§ã®RNNå®Ÿè£…</h2>

<h3>nn.RNNã®åŸºæœ¬çš„ãªä½¿ã„æ–¹</h3>

<p>PyTorchã§ã¯<code>torch.nn.RNN</code>ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã—ã¦RNNå±¤ã‚’å®šç¾©ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# RNNã®åŸºæœ¬æ§‹æ–‡
rnn = nn.RNN(
    input_size=10,      # å…¥åŠ›ã®ç‰¹å¾´é‡æ¬¡å…ƒ
    hidden_size=20,     # éš ã‚ŒçŠ¶æ…‹ã®æ¬¡å…ƒ
    num_layers=2,       # RNNå±¤ã®æ•°
    nonlinearity='tanh', # æ´»æ€§åŒ–é–¢æ•°ï¼ˆ'tanh' ã¾ãŸã¯ 'relu'ï¼‰
    batch_first=True,   # (batch, seq, feature) ã®é †åº
    dropout=0.0,        # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡ï¼ˆå±¤é–“ï¼‰
    bidirectional=False # åŒæ–¹å‘RNN
)

# ãƒ€ãƒŸãƒ¼å…¥åŠ›ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚º3ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·5ã€ç‰¹å¾´é‡10ï¼‰
x = torch.randn(3, 5, 10)

# åˆæœŸéš ã‚ŒçŠ¶æ…‹ï¼ˆnum_layers, batch, hidden_sizeï¼‰
h0 = torch.zeros(2, 3, 20)

# é †ä¼æ’­
output, hn = rnn(x, h0)

print("=== PyTorch RNN ã®å‹•ä½œç¢ºèª ===\n")
print(f"å…¥åŠ›ã‚µã‚¤ã‚º: {x.shape}")
print(f"  [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, ç‰¹å¾´é‡] = [{x.shape[0]}, {x.shape[1]}, {x.shape[2]}]")
print(f"\nåˆæœŸéš ã‚ŒçŠ¶æ…‹: {h0.shape}")
print(f"  [å±¤æ•°, ãƒãƒƒãƒ, éš ã‚ŒçŠ¶æ…‹] = [{h0.shape[0]}, {h0.shape[1]}, {h0.shape[2]}]")
print(f"\nå‡ºåŠ›ã‚µã‚¤ã‚º: {output.shape}")
print(f"  [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, éš ã‚ŒçŠ¶æ…‹] = [{output.shape[0]}, {output.shape[1]}, {output.shape[2]}]")
print(f"\næœ€çµ‚éš ã‚ŒçŠ¶æ…‹: {hn.shape}")
print(f"  [å±¤æ•°, ãƒãƒƒãƒ, éš ã‚ŒçŠ¶æ…‹] = [{hn.shape[0]}, {hn.shape[1]}, {hn.shape[2]}]")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®è¨ˆç®—
total_params = sum(p.numel() for p in rnn.parameters())
print(f"\nç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== PyTorch RNN ã®å‹•ä½œç¢ºèª ===

å…¥åŠ›ã‚µã‚¤ã‚º: torch.Size([3, 5, 10])
  [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, ç‰¹å¾´é‡] = [3, 5, 10]

åˆæœŸéš ã‚ŒçŠ¶æ…‹: torch.Size([2, 3, 20])
  [å±¤æ•°, ãƒãƒƒãƒ, éš ã‚ŒçŠ¶æ…‹] = [2, 3, 20]

å‡ºåŠ›ã‚µã‚¤ã‚º: torch.Size([3, 5, 20])
  [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, éš ã‚ŒçŠ¶æ…‹] = [3, 5, 20]

æœ€çµ‚éš ã‚ŒçŠ¶æ…‹: torch.Size([2, 3, 20])
  [å±¤æ•°, ãƒãƒƒãƒ, éš ã‚ŒçŠ¶æ…‹] = [2, 3, 20]

ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 1,240
</code></pre>

<h3>nn.RNNCell vs nn.RNN</h3>

<p>PyTorchã«ã¯2ã¤ã®RNNå®Ÿè£…ãŒã‚ã‚Šã¾ã™ï¼š</p>

<table>
<thead>
<tr>
<th>ã‚¯ãƒ©ã‚¹</th>
<th>ç‰¹å¾´</th>
<th>ç”¨é€”</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>nn.RNN</strong></td>
<td>ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã‚’ä¸€åº¦ã«å‡¦ç†</td>
<td>æ¨™æº–çš„ãªç”¨é€”ã€åŠ¹ç‡çš„</td>
</tr>
<tr>
<td><strong>nn.RNNCell</strong></td>
<td>1æ™‚åˆ»ãšã¤æ‰‹å‹•ã§å‡¦ç†</td>
<td>ã‚«ã‚¹ã‚¿ãƒ åˆ¶å¾¡ãŒå¿…è¦ãªå ´åˆ</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">import torch
import torch.nn as nn

# nn.RNNCellã®ä½¿ç”¨ä¾‹
input_size = 5
hidden_size = 10
batch_size = 3
sequence_length = 4

rnn_cell = nn.RNNCell(input_size, hidden_size)

# ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¥åŠ›
x_sequence = torch.randn(batch_size, sequence_length, input_size)

# åˆæœŸéš ã‚ŒçŠ¶æ…‹
h = torch.zeros(batch_size, hidden_size)

print("=== nn.RNNCell ã‚’ä½¿ã£ãŸæ‰‹å‹•ãƒ«ãƒ¼ãƒ— ===\n")

# å„æ™‚åˆ»ã‚’æ‰‹å‹•ã§ãƒ«ãƒ¼ãƒ—
outputs = []
for t in range(sequence_length):
    x_t = x_sequence[:, t, :]  # æ™‚åˆ»tã®å…¥åŠ›
    h = rnn_cell(x_t, h)       # éš ã‚ŒçŠ¶æ…‹ã®æ›´æ–°
    outputs.append(h)
    print(f"æ™‚åˆ» t={t}: å…¥åŠ› {x_t.shape} â†’ éš ã‚ŒçŠ¶æ…‹ {h.shape}")

# å‡ºåŠ›ã‚’ã‚¹ã‚¿ãƒƒã‚¯
output = torch.stack(outputs, dim=1)

print(f"\nå…¨æ™‚åˆ»ã®å‡ºåŠ›: {output.shape}")
print(f"  [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, éš ã‚ŒçŠ¶æ…‹] = [{output.shape[0]}, {output.shape[1]}, {output.shape[2]}]")

# nn.RNNã¨ã®æ¯”è¼ƒ
rnn = nn.RNN(input_size, hidden_size, batch_first=True)
output_rnn, hn_rnn = rnn(x_sequence)

print(f"\nnn.RNN ã®å‡ºåŠ›: {output_rnn.shape}")
print("â†’ nn.RNNCell ã®ãƒ«ãƒ¼ãƒ—å‡¦ç†ã¨åŒã˜çµæœã‚’ä¸€åº¦ã«è¨ˆç®—")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== nn.RNNCell ã‚’ä½¿ã£ãŸæ‰‹å‹•ãƒ«ãƒ¼ãƒ— ===

æ™‚åˆ» t=0: å…¥åŠ› torch.Size([3, 5]) â†’ éš ã‚ŒçŠ¶æ…‹ torch.Size([3, 10])
æ™‚åˆ» t=1: å…¥åŠ› torch.Size([3, 5]) â†’ éš ã‚ŒçŠ¶æ…‹ torch.Size([3, 10])
æ™‚åˆ» t=2: å…¥åŠ› torch.Size([3, 5]) â†’ éš ã‚ŒçŠ¶æ…‹ torch.Size([3, 10])
æ™‚åˆ» t=3: å…¥åŠ› torch.Size([3, 5]) â†’ éš ã‚ŒçŠ¶æ…‹ torch.Size([3, 10])

å…¨æ™‚åˆ»ã®å‡ºåŠ›: torch.Size([3, 4, 10])
  [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, éš ã‚ŒçŠ¶æ…‹] = [3, 4, 10]

nn.RNN ã®å‡ºåŠ›: torch.Size([3, 4, 10])
â†’ nn.RNNCell ã®ãƒ«ãƒ¼ãƒ—å‡¦ç†ã¨åŒã˜çµæœã‚’ä¸€åº¦ã«è¨ˆç®—
</code></pre>

<h3>Many-to-Oneã‚¿ã‚¹ã‚¯ï¼šæ„Ÿæƒ…åˆ†æ</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class SentimentRNN(nn.Module):
    """
    Many-to-One RNNï¼šæ–‡ç« å…¨ä½“ã‹ã‚‰æ„Ÿæƒ…ã‚’åˆ†é¡
    """
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super(SentimentRNN, self).__init__()

        # å˜èªåŸ‹ã‚è¾¼ã¿å±¤
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # RNNå±¤
        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)

        # å‡ºåŠ›å±¤
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        # x: (batch, seq_len)

        # å˜èªåŸ‹ã‚è¾¼ã¿
        embedded = self.embedding(x)  # (batch, seq_len, embedding_dim)

        # RNN
        output, hidden = self.rnn(embedded)
        # output: (batch, seq_len, hidden_dim)
        # hidden: (1, batch, hidden_dim)

        # æœ€å¾Œã®éš ã‚ŒçŠ¶æ…‹ã®ã¿ä½¿ç”¨ï¼ˆMany-to-Oneï¼‰
        last_hidden = hidden.squeeze(0)  # (batch, hidden_dim)

        # åˆ†é¡
        logits = self.fc(last_hidden)  # (batch, output_dim)

        return logits

# ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
vocab_size = 5000      # èªå½™ã‚µã‚¤ã‚º
embedding_dim = 100    # å˜èªåŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
hidden_dim = 128       # éš ã‚ŒçŠ¶æ…‹æ¬¡å…ƒ
output_dim = 2         # 2ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼ˆpositive/negativeï¼‰

model = SentimentRNN(vocab_size, embedding_dim, hidden_dim, output_dim)

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚º4ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·10ï¼‰
x = torch.randint(0, vocab_size, (4, 10))
logits = model(x)

print("=== Sentiment RNNï¼ˆMany-to-Oneï¼‰===\n")
print(f"å…¥åŠ›ï¼ˆå˜èªIDï¼‰: {x.shape}")
print(f"  [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·] = [{x.shape[0]}, {x.shape[1]}]")
print(f"\nå‡ºåŠ›ï¼ˆãƒ­ã‚¸ãƒƒãƒˆï¼‰: {logits.shape}")
print(f"  [ãƒãƒƒãƒ, ã‚¯ãƒ©ã‚¹æ•°] = [{logits.shape[0]}, {logits.shape[1]}]")

# ç¢ºç‡ã«å¤‰æ›
probs = F.softmax(logits, dim=1)
print(f"\nç¢ºç‡åˆ†å¸ƒ:")
print(probs)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
total_params = sum(p.numel() for p in model.parameters())
print(f"\nç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
print(f"  å†…è¨³:")
print(f"    Embedding: {vocab_size * embedding_dim:,}")
print(f"    RNN: {(embedding_dim * hidden_dim + hidden_dim * hidden_dim + 2 * hidden_dim):,}")
print(f"    FC: {(hidden_dim * output_dim + output_dim):,}")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== Sentiment RNNï¼ˆMany-to-Oneï¼‰===

å…¥åŠ›ï¼ˆå˜èªIDï¼‰: torch.Size([4, 10])
  [ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·] = [4, 10]

å‡ºåŠ›ï¼ˆãƒ­ã‚¸ãƒƒãƒˆï¼‰: torch.Size([4, 2])
  [ãƒãƒƒãƒ, ã‚¯ãƒ©ã‚¹æ•°] = [4, 2]

ç¢ºç‡åˆ†å¸ƒ:
tensor([[0.5234, 0.4766],
        [0.4892, 0.5108],
        [0.5123, 0.4877],
        [0.4956, 0.5044]], grad_fn=<SoftmaxBackward0>)

ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 529,410
  å†…è¨³:
    Embedding: 500,000
    RNN: 29,152
    FC: 258
</code></pre>

<hr>

<h2>1.6 å®Ÿè·µï¼šæ–‡å­—ãƒ¬ãƒ™ãƒ«è¨€èªãƒ¢ãƒ‡ãƒ«</h2>

<h3>æ–‡å­—ãƒ¬ãƒ™ãƒ«RNNã¨ã¯</h3>

<p><strong>æ–‡å­—ãƒ¬ãƒ™ãƒ«è¨€èªãƒ¢ãƒ‡ãƒ«</strong>ã¯ã€æ–‡å­—å˜ä½ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’å­¦ç¿’ã—ã€æ¬¡ã®æ–‡å­—ã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</p>

<ul>
<li>å…¥åŠ›ï¼šã“ã‚Œã¾ã§ã®æ–‡å­—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹</li>
<li>å‡ºåŠ›ï¼šæ¬¡ã«æ¥ã‚‹æ–‡å­—ã®ç¢ºç‡åˆ†å¸ƒ</li>
<li>å­¦ç¿’å¾Œï¼šæ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆå¯èƒ½</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚·ã‚§ã‚¤ã‚¯ã‚¹ãƒ”ã‚¢é¢¨ï¼‰
text = """To be or not to be, that is the question.
Whether 'tis nobler in the mind to suffer
The slings and arrows of outrageous fortune,
Or to take arms against a sea of troubles"""

# æ–‡å­—ã®é›†åˆã‚’ä½œæˆ
chars = sorted(list(set(text)))
char_to_idx = {ch: i for i, ch in enumerate(chars)}
idx_to_char = {i: ch for i, ch in enumerate(chars)}

vocab_size = len(chars)

print("=== æ–‡å­—ãƒ¬ãƒ™ãƒ«è¨€èªãƒ¢ãƒ‡ãƒ« ===\n")
print(f"ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•: {len(text)} æ–‡å­—")
print(f"èªå½™ã‚µã‚¤ã‚º: {vocab_size} æ–‡å­—")
print(f"æ–‡å­—é›†åˆ: {''.join(chars)}")

# ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•°å€¤ã«å¤‰æ›
encoded_text = [char_to_idx[ch] for ch in text]

print(f"\nå…ƒã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæœ€åˆã®50æ–‡å­—ï¼‰:")
print(text[:50])
print(f"\nã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ¸ˆã¿:")
print(encoded_text[:50])

# RNNè¨€èªãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
class CharRNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(CharRNN, self).__init__()
        self.hidden_dim = hidden_dim

        # æ–‡å­—åŸ‹ã‚è¾¼ã¿
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # RNNå±¤
        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)

        # å‡ºåŠ›å±¤
        self.fc = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x, hidden=None):
        # x: (batch, seq_len)
        embedded = self.embedding(x)  # (batch, seq_len, embedding_dim)
        output, hidden = self.rnn(embedded, hidden)  # (batch, seq_len, hidden_dim)
        logits = self.fc(output)  # (batch, seq_len, vocab_size)
        return logits, hidden

    def init_hidden(self, batch_size):
        return torch.zeros(1, batch_size, self.hidden_dim)

# ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
embedding_dim = 32
hidden_dim = 64
model = CharRNN(vocab_size, embedding_dim, hidden_dim)

print(f"\n=== CharRNN ãƒ¢ãƒ‡ãƒ« ===")
print(f"åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {embedding_dim}")
print(f"éš ã‚ŒçŠ¶æ…‹æ¬¡å…ƒ: {hidden_dim}")
total_params = sum(p.numel() for p in model.parameters())
print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== æ–‡å­—ãƒ¬ãƒ™ãƒ«è¨€èªãƒ¢ãƒ‡ãƒ« ===

ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•: 179 æ–‡å­—
èªå½™ã‚µã‚¤ã‚º: 36 æ–‡å­—
æ–‡å­—é›†åˆ:  ',.Tabdefghilmnopqrstuwy

å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæœ€åˆã®50æ–‡å­—ï¼‰:
To be or not to be, that is the question.
Whethe

ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ¸ˆã¿:
[7, 22, 0, 13, 14, 0, 22, 23, 0, 21, 22, 25, 0, 25, 22, 0, 13, 14, 2, 0, 25, 17, 10, 25, 0, 18, 24, 0, 25, 17, 14, 0, 23, 26, 14, 24, 25, 18, 22, 21, 3, 1, 8, 17, 14, 25, 17, 14, 23, 0]

=== CharRNN ãƒ¢ãƒ‡ãƒ« ===
åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: 32
éš ã‚ŒçŠ¶æ…‹æ¬¡å…ƒ: 64
ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 8,468
</code></pre>

<h3>å­¦ç¿’ã¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ</h3>

<pre><code class="language-python">def create_sequences(encoded_text, seq_length):
    """
    å­¦ç¿’ç”¨ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä½œæˆ
    """
    X, y = [], []
    for i in range(len(encoded_text) - seq_length):
        X.append(encoded_text[i:i+seq_length])
        y.append(encoded_text[i+1:i+seq_length+1])
    return torch.LongTensor(X), torch.LongTensor(y)

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
seq_length = 25
X, y = create_sequences(encoded_text, seq_length)

print(f"=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ===")
print(f"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {len(X)}")
print(f"å…¥åŠ›ã‚µã‚¤ã‚º: {X.shape}")
print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚µã‚¤ã‚º: {y.shape}")

# å­¦ç¿’è¨­å®š
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# ç°¡ç•¥ç‰ˆã®å­¦ç¿’ãƒ«ãƒ¼ãƒ—
num_epochs = 100
batch_size = 32

print(f"\n=== å­¦ç¿’é–‹å§‹ ===")
model.train()

for epoch in range(num_epochs):
    total_loss = 0
    hidden = None

    for i in range(0, len(X), batch_size):
        batch_X = X[i:i+batch_size]
        batch_y = y[i:i+batch_size]

        # é †ä¼æ’­
        optimizer.zero_grad()
        logits, hidden = model(batch_X, hidden)

        # æå¤±è¨ˆç®—ï¼ˆreshapeãŒå¿…è¦ï¼‰
        loss = criterion(logits.view(-1, vocab_size), batch_y.view(-1))

        # é€†ä¼æ’­
        loss.backward()

        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)

        optimizer.step()

        # éš ã‚ŒçŠ¶æ…‹ã®detachï¼ˆBPTTã®åˆ‡ã‚Šæ¨ã¦ï¼‰
        hidden = hidden.detach()

        total_loss += loss.item()

    if (epoch + 1) % 20 == 0:
        avg_loss = total_loss / (len(X) // batch_size)
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")

print("\nå­¦ç¿’å®Œäº†ï¼")

# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
def generate_text(model, start_str, length=100, temperature=1.0):
    """
    å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ

    Parameters:
    -----------
    model : CharRNN
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
    start_str : str
        é–‹å§‹æ–‡å­—åˆ—
    length : int
        ç”Ÿæˆã™ã‚‹æ–‡å­—æ•°
    temperature : float
        æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé«˜ã„ã»ã©ãƒ©ãƒ³ãƒ€ãƒ ï¼‰
    """
    model.eval()

    # é–‹å§‹æ–‡å­—åˆ—ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    chars_encoded = [char_to_idx[ch] for ch in start_str]
    input_seq = torch.LongTensor(chars_encoded).unsqueeze(0)

    hidden = None
    generated = start_str

    with torch.no_grad():
        for _ in range(length):
            # äºˆæ¸¬
            logits, hidden = model(input_seq, hidden)

            # æœ€å¾Œã®æ™‚åˆ»ã®å‡ºåŠ›
            logits = logits[0, -1, :] / temperature
            probs = torch.softmax(logits, dim=0)

            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            next_char_idx = torch.multinomial(probs, 1).item()
            next_char = idx_to_char[next_char_idx]

            generated += next_char

            # æ¬¡ã®å…¥åŠ›
            input_seq = torch.LongTensor([[next_char_idx]])

    return generated

# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®å®Ÿè¡Œ
print("\n=== ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ ===\n")
start_str = "To be"
generated_text = generate_text(model, start_str, length=100, temperature=0.8)
print(f"é–‹å§‹æ–‡å­—åˆ—: '{start_str}'")
print(f"\nç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ:")
print(generated_text)
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ===
ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: 154
å…¥åŠ›ã‚µã‚¤ã‚º: torch.Size([154, 25])
ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚µã‚¤ã‚º: torch.Size([154, 25])

=== å­¦ç¿’é–‹å§‹ ===
Epoch 20/100, Loss: 2.1234
Epoch 40/100, Loss: 1.8765
Epoch 60/100, Loss: 1.5432
Epoch 80/100, Loss: 1.2987
Epoch 100/100, Loss: 1.0654

å­¦ç¿’å®Œäº†ï¼

=== ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ ===

é–‹å§‹æ–‡å­—åˆ—: 'To be'

ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ:
To be or not the question.
Whether 'tis nobler in the mind to suffer
The slings and arrows of out
</code></pre>

<blockquote>
<p><strong>æ³¨</strong>: å®Ÿéš›ã®å‡ºåŠ›ã¯å­¦ç¿’ã®ä¹±æ•°æ€§ã«ã‚ˆã‚Šç•°ãªã‚Šã¾ã™ã€‚ã‚ˆã‚Šé•·ã„ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚¨ãƒãƒƒã‚¯æ•°ã§ã‚ˆã‚Šè‰¯ã„çµæœãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>ã¾ã¨ã‚</h2>

<p>ã“ã®ç« ã§ã¯ã€RNNã®åŸºç¤ã¨é †ä¼æ’­ã«ã¤ã„ã¦å­¦ç¿’ã—ã¾ã—ãŸã€‚</p>

<h3>é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ</h3>

<ul>
<li><strong>ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿</strong>ã¯æ™‚é–“çš„ãªé †åºã‚’æŒã¡ã€å¾“æ¥ã®NNã§ã¯æ‰±ã„ã«ãã„</li>
<li><strong>éš ã‚ŒçŠ¶æ…‹</strong>ã«ã‚ˆã‚Šã€RNNã¯éå»ã®æƒ…å ±ã‚’è¨˜æ†¶ã§ãã‚‹</li>
<li><strong>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å…±æœ‰</strong>ã§ä»»æ„é•·ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’çµ±ä¸€çš„ã«å‡¦ç†</li>
<li><strong>BPTT</strong>ã«ã‚ˆã‚Šå­¦ç¿’ã™ã‚‹ãŒã€å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™ºãŒèª²é¡Œ</li>
<li><strong>å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°</strong>ã§å‹¾é…çˆ†ç™ºã‚’æŠ‘åˆ¶</li>
<li><strong>PyTorch</strong>ã®nn.RNNã§ç°¡å˜ã«å®Ÿè£…å¯èƒ½</li>
</ul>

<h3>æ¬¡ç« ã®äºˆå‘Š</h3>

<p>ç¬¬2ç« ã§ã¯ã€ä»¥ä¸‹ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’æ‰±ã„ã¾ã™ï¼š</p>
<ul>
<li>LSTMï¼ˆLong Short-Term Memoryï¼‰ã®ä»•çµ„ã¿</li>
<li>GRUï¼ˆGated Recurrent Unitï¼‰ã®æ§‹é€ </li>
<li>åŒæ–¹å‘RNNï¼ˆBidirectional RNNï¼‰</li>
<li>Seq2Seqãƒ¢ãƒ‡ãƒ«ã¨æ³¨æ„æ©Ÿæ§‹ï¼ˆAttentionï¼‰</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<details>
<summary><strong>æ¼”ç¿’1ï¼šéš ã‚ŒçŠ¶æ…‹ã®ã‚µã‚¤ã‚ºè¨ˆç®—</strong></summary>

<p><strong>å•é¡Œ</strong>ï¼šä»¥ä¸‹ã®RNNã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚</p>

<ul>
<li>å…¥åŠ›ã‚µã‚¤ã‚º: 50</li>
<li>éš ã‚ŒçŠ¶æ…‹ã‚µã‚¤ã‚º: 128</li>
<li>å‡ºåŠ›ã‚µã‚¤ã‚º: 10</li>
</ul>

<p><strong>è§£ç­”</strong>ï¼š</p>
<pre><code># Wxh: å…¥åŠ›â†’éš ã‚Œ
Wxh_params = 50 * 128 = 6,400

# Whh: éš ã‚Œâ†’éš ã‚Œ
Whh_params = 128 * 128 = 16,384

# Why: éš ã‚Œâ†’å‡ºåŠ›
Why_params = 128 * 10 = 1,280

# ãƒã‚¤ã‚¢ã‚¹é …
bh_params = 128
by_params = 10

# åˆè¨ˆ
total = 6,400 + 16,384 + 1,280 + 128 + 10 = 24,202

ç­”ãˆ: 24,202 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
</code></pre>
</details>

<details>
<summary><strong>æ¼”ç¿’2ï¼šã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡</strong></summary>

<p><strong>å•é¡Œ</strong>ï¼šãƒãƒƒãƒã‚µã‚¤ã‚º32ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·100ã€éš ã‚ŒçŠ¶æ…‹ã‚µã‚¤ã‚º256ã®RNNã§ã€é †ä¼æ’­æ™‚ã«å¿…è¦ãªéš ã‚ŒçŠ¶æ…‹ã®ç·ãƒ¡ãƒ¢ãƒªé‡ï¼ˆè¦ç´ æ•°ï¼‰ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚</p>

<p><strong>è§£ç­”</strong>ï¼š</p>
<pre><code># å„æ™‚åˆ»ã®éš ã‚ŒçŠ¶æ…‹: (batch_size, hidden_size)
# ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·åˆ†ä¿å­˜ã™ã‚‹å¿…è¦ãŒã‚ã‚‹

memory_elements = batch_size * seq_length * hidden_size
                = 32 * 100 * 256
                = 819,200 è¦ç´ 

# float32ã®å ´åˆï¼ˆ4ãƒã‚¤ãƒˆï¼‰
memory_bytes = 819,200 * 4 = 3,276,800 ãƒã‚¤ãƒˆ â‰ˆ 3.2 MB

ç­”ãˆ: ç´„3.2 MBï¼ˆé€†ä¼æ’­æ™‚ã¯ã•ã‚‰ã«å¿…è¦ï¼‰
</code></pre>
</details>

<details>
<summary><strong>æ¼”ç¿’3ï¼šMany-to-Many RNNã®å®Ÿè£…</strong></summary>

<p><strong>å•é¡Œ</strong>ï¼šå“è©ã‚¿ã‚°ä»˜ã‘ã‚¿ã‚¹ã‚¯ï¼ˆå„å˜èªã«å“è©ãƒ©ãƒ™ãƒ«ã‚’ä»˜ä¸ï¼‰ã®ãŸã‚ã®Many-to-Many RNNã‚’PyTorchã§å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>

<p><strong>è§£ç­”ä¾‹</strong>ï¼š</p>
<pre><code class="language-python">import torch
import torch.nn as nn

class POSTaggingRNN(nn.Module):
    """
    Many-to-Many RNNï¼šå„å˜èªã«å“è©ã‚¿ã‚°ã‚’äºˆæ¸¬
    """
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_tags):
        super(POSTaggingRNN, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_tags)

    def forward(self, x):
        # x: (batch, seq_len)
        embedded = self.embedding(x)  # (batch, seq_len, embedding_dim)
        output, _ = self.rnn(embedded)  # (batch, seq_len, hidden_dim)
        logits = self.fc(output)  # (batch, seq_len, num_tags)
        return logits

# ä½¿ç”¨ä¾‹
vocab_size = 5000
embedding_dim = 100
hidden_dim = 128
num_tags = 45  # Penn Treebank ã®å“è©ã‚¿ã‚°æ•°

model = POSTaggingRNN(vocab_size, embedding_dim, hidden_dim, num_tags)

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
x = torch.randint(0, vocab_size, (8, 20))  # batch=8, seq_len=20
logits = model(x)

print(f"å…¥åŠ›: {x.shape}")
print(f"å‡ºåŠ›: {logits.shape}")  # (8, 20, 45)
print("å„æ™‚åˆ»ã®å„å˜èªã«å¯¾ã—ã¦å“è©ã‚¿ã‚°ã‚’äºˆæ¸¬")
</code></pre>
</details>

<details>
<summary><strong>æ¼”ç¿’4ï¼šå‹¾é…æ¶ˆå¤±ã®å®Ÿé¨“</strong></summary>

<p><strong>å•é¡Œ</strong>ï¼šç•°ãªã‚‹é‡ã¿åˆæœŸåŒ–ã§RNNã‚’å­¦ç¿’ã•ã›ã€å‹¾é…æ¶ˆå¤±ã®å½±éŸ¿ã‚’è¦³å¯Ÿã—ã¦ãã ã•ã„ã€‚</p>

<p><strong>è§£ç­”ä¾‹</strong>ï¼š</p>
<pre><code class="language-python">import torch
import torch.nn as nn
import matplotlib.pyplot as plt

def test_gradient_vanishing(init_scale):
    """
    é‡ã¿åˆæœŸåŒ–ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å¤‰ãˆã¦å‹¾é…ã‚’è¦³å¯Ÿ
    """
    rnn = nn.RNN(10, 20, batch_first=True)

    # é‡ã¿ã‚’æ‰‹å‹•ã§åˆæœŸåŒ–
    for name, param in rnn.named_parameters():
        if 'weight' in name:
            nn.init.uniform_(param, -init_scale, init_scale)

    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆé•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼‰
    x = torch.randn(1, 50, 10)
    target = torch.randn(1, 50, 20)

    # é †ä¼æ’­
    output, _ = rnn(x)
    loss = ((output - target) ** 2).mean()

    # é€†ä¼æ’­
    loss.backward()

    # å‹¾é…ã®ãƒãƒ«ãƒ ã‚’è¨ˆç®—
    grad_norms = []
    for name, param in rnn.named_parameters():
        if param.grad is not None:
            grad_norms.append(param.grad.norm().item())

    return grad_norms

# ç•°ãªã‚‹åˆæœŸåŒ–ã‚¹ã‚±ãƒ¼ãƒ«ã§å®Ÿé¨“
scales = [0.01, 0.1, 0.5, 1.0, 2.0]
results = {scale: test_gradient_vanishing(scale) for scale in scales}

print("=== å‹¾é…ãƒãƒ«ãƒ ã®æ¯”è¼ƒ ===")
for scale, norms in results.items():
    avg_norm = sum(norms) / len(norms)
    print(f"åˆæœŸåŒ–ã‚¹ã‚±ãƒ¼ãƒ« {scale}: å¹³å‡å‹¾é…ãƒãƒ«ãƒ  = {avg_norm:.6f}")

print("\nã‚¹ã‚±ãƒ¼ãƒ«ãŒå°ã•ã™ãã‚‹ã¨å‹¾é…æ¶ˆå¤±ã€å¤§ãã™ãã‚‹ã¨å‹¾é…çˆ†ç™ºãŒç™ºç”Ÿ")
</code></pre>
</details>

<details>
<summary><strong>æ¼”ç¿’5ï¼šåŒæ–¹å‘RNNã®ç†è§£</strong></summary>

<p><strong>å•é¡Œ</strong>ï¼šåŒæ–¹å‘RNNï¼ˆBidirectional RNNï¼‰ã‚’å®Ÿè£…ã—ã€é †æ–¹å‘ã®ã¿ã®RNNã¨ã®é•ã„ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<p><strong>è§£ç­”ä¾‹</strong>ï¼š</p>
<pre><code class="language-python">import torch
import torch.nn as nn

# åŒæ–¹å‘RNN
bi_rnn = nn.RNN(10, 20, batch_first=True, bidirectional=True)

# é †æ–¹å‘ã®ã¿ã®RNN
uni_rnn = nn.RNN(10, 20, batch_first=True, bidirectional=False)

# ãƒ€ãƒŸãƒ¼å…¥åŠ›
x = torch.randn(3, 5, 10)  # batch=3, seq_len=5, features=10

# é †ä¼æ’­
bi_output, bi_hidden = bi_rnn(x)
uni_output, uni_hidden = uni_rnn(x)

print("=== åŒæ–¹å‘RNN vs å˜æ–¹å‘RNN ===\n")

print(f"å…¥åŠ›ã‚µã‚¤ã‚º: {x.shape}")

print(f"\nåŒæ–¹å‘RNN:")
print(f"  å‡ºåŠ›: {bi_output.shape}")  # (3, 5, 40) â† 20*2
print(f"  éš ã‚ŒçŠ¶æ…‹: {bi_hidden.shape}")  # (2, 3, 20) â† 2æ–¹å‘

print(f"\nå˜æ–¹å‘RNN:")
print(f"  å‡ºåŠ›: {uni_output.shape}")  # (3, 5, 20)
print(f"  éš ã‚ŒçŠ¶æ…‹: {uni_hidden.shape}")  # (1, 3, 20)

print("\nåŒæ–¹å‘RNNã®ç‰¹å¾´:")
print("  âœ“ é †æ–¹å‘ã¨é€†æ–¹å‘ã®ä¸¡æ–¹ã®æ–‡è„ˆã‚’æ‰ãˆã‚‹")
print("  âœ“ å‡ºåŠ›æ¬¡å…ƒã¯2å€ã«ãªã‚‹ï¼ˆforward + backwardï¼‰")
print("  âœ“ æœªæ¥ã®æƒ…å ±ã‚‚åˆ©ç”¨ã§ãã‚‹ãŸã‚ã€ç²¾åº¦å‘ä¸Š")
print("  âœ— ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ã«ã¯ä¸å‘ãï¼ˆå…¨ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒå¿…è¦ï¼‰")
</code></pre>
</details>

<hr>

<div class="navigation">
    <a href="../index.html" class="nav-button">ğŸ“š ã‚³ãƒ¼ã‚¹ç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter2-lstm-gru.html" class="nav-button">æ¬¡ã®ç« ã¸ï¼šLSTMã¨GRU â†’</a>
</div>

</main>


    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
    <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
</footer>

</body>
</html>
