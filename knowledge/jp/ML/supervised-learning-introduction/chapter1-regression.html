<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第1章：回帰問題の基礎 - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>第1章：回帰問題の基礎</h1>
            <p class="subtitle">連続値予測の理論と実装 - 線形回帰から正則化まで</p>
            <div class="meta">
                <span class="meta-item">📖 読了時間: 20-25分</span>
                <span class="meta-item">📊 難易度: 初級</span>
                <span class="meta-item">💻 コード例: 12個</span>
                <span class="meta-item">📝 演習問題: 5問</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>学習目標</h2>
<p>この章を読むことで、以下を習得できます：</p>
<ul>
<li>✅ 回帰問題の定義と応用例を理解する</li>
<li>✅ 線形回帰の数学的背景を説明できる</li>
<li>✅ 最小二乗法と勾配降下法を実装できる</li>
<li>✅ 多項式回帰で非線形関係をモデル化できる</li>
<li>✅ 正則化（Ridge, Lasso, Elastic Net）を適用できる</li>
<li>✅ R²、RMSE、MAEで回帰モデルを評価できる</li>
</ul>

<hr>

<h2>1.1 回帰問題とは</h2>

<h3>定義</h3>
<p><strong>回帰問題（Regression）</strong>は、入力変数から<strong>連続値</strong>の出力を予測する教師あり学習のタスクです。</p>

<blockquote>
<p>「特徴量 $X$ から目的変数 $y$ を予測する関数 $f: X \rightarrow y$ を学習する」</p>
</blockquote>

<h3>回帰 vs 分類</h3>

<table>
<thead>
<tr>
<th>タスク</th>
<th>出力</th>
<th>例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>回帰</strong></td>
<td>連続値（数値）</td>
<td>住宅価格予測、気温予測、売上予測</td>
</tr>
<tr>
<td><strong>分類</strong></td>
<td>離散値（カテゴリ）</td>
<td>画像分類、スパム判定、疾病診断</td>
</tr>
</tbody>
</table>

<h3>実世界の応用例</h3>

<div class="mermaid">
graph LR
    A[回帰問題の応用] --> B[金融: 株価予測]
    A --> C[不動産: 住宅価格予測]
    A --> D[製造: 需要予測]
    A --> E[医療: 患者滞在期間予測]
    A --> F[マーケティング: 売上予測]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#fff3e0
    style D fill:#fff3e0
    style E fill:#fff3e0
    style F fill:#fff3e0
</div>

<hr>

<h2>1.2 線形回帰の理論</h2>

<h3>単回帰モデル</h3>

<p><strong>単回帰（Simple Linear Regression）</strong>は、1つの特徴量から予測を行います。</p>

<p>$$
y = w_0 + w_1 x + \epsilon
$$</p>

<ul>
<li>$y$: 目的変数（予測したい値）</li>
<li>$x$: 説明変数（特徴量）</li>
<li>$w_0$: 切片（intercept, bias）</li>
<li>$w_1$: 傾き（slope, weight）</li>
<li>$\epsilon$: 誤差項</li>
</ul>

<h3>重回帰モデル</h3>

<p><strong>重回帰（Multiple Linear Regression）</strong>は、複数の特徴量を使用します。</p>

<p>$$
y = w_0 + w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + \epsilon
$$</p>

<p>行列表記：</p>

<p>$$
\mathbf{y} = \mathbf{X}\mathbf{w} + \epsilon
$$</p>

<ul>
<li>$\mathbf{y}$: 目的変数ベクトル（shape: $m \times 1$）</li>
<li>$\mathbf{X}$: 特徴量行列（shape: $m \times (n+1)$）</li>
<li>$\mathbf{w}$: 重みベクトル（shape: $(n+1) \times 1$）</li>
<li>$m$: サンプル数、$n$: 特徴量数</li>
</ul>

<h3>損失関数（Loss Function）</h3>

<p><strong>平均二乗誤差（Mean Squared Error, MSE）</strong>を最小化します：</p>

<p>$$
J(\mathbf{w}) = \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2 = \frac{1}{m} ||\mathbf{y} - \mathbf{X}\mathbf{w}||^2
$$</p>

<ul>
<li>$y^{(i)}$: 実際の値</li>
<li>$\hat{y}^{(i)} = \mathbf{w}^T \mathbf{x}^{(i)}$: 予測値</li>
</ul>

<hr>

<h2>1.3 最小二乗法（Ordinary Least Squares）</h2>

<h3>解析解</h3>

<p>MSEを最小化する重み $\mathbf{w}$ は、解析的に求められます：</p>

<p>$$
\mathbf{w}^* = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
$$</p>

<p>これを<strong>正規方程式（Normal Equation）</strong>と呼びます。</p>

<h3>実装例：単回帰</h3>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# データ生成
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# バイアス項を追加
X_b = np.c_[np.ones((100, 1)), X]  # shape: (100, 2)

# 正規方程式で重みを計算
w_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y

print("学習した重み:")
print(f"w0 (切片): {w_best[0][0]:.4f}")
print(f"w1 (傾き): {w_best[1][0]:.4f}")

# 予測
X_new = np.array([[0], [2]])
X_new_b = np.c_[np.ones((2, 1)), X_new]
y_predict = X_new_b @ w_best

# 可視化
plt.figure(figsize=(10, 6))
plt.scatter(X, y, alpha=0.6, label='データ')
plt.plot(X_new, y_predict, 'r-', linewidth=2, label='予測直線')
plt.xlabel('X', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('線形回帰 - 最小二乗法', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>学習した重み:
w0 (切片): 4.2153
w1 (傾き): 2.7702
</code></pre>

<h3>scikit-learnによる実装</h3>

<pre><code class="language-python">from sklearn.linear_model import LinearRegression

# モデル構築
model = LinearRegression()
model.fit(X, y)

print("\nscikit-learn:")
print(f"切片: {model.intercept_[0]:.4f}")
print(f"傾き: {model.coef_[0][0]:.4f}")

# 予測
y_pred = model.predict(X_new)
print(f"\n予測値: {y_pred.flatten()}")
</code></pre>

<hr>

<h2>1.4 勾配降下法（Gradient Descent）</h2>

<h3>原理</h3>

<p>損失関数の勾配を計算し、勾配の逆方向に重みを更新します。</p>

<div class="mermaid">
graph LR
    A[初期重み w] --> B[勾配計算 ∇J]
    B --> C[重み更新 w := w - α∇J]
    C --> D{収束?}
    D -->|No| B
    D -->|Yes| E[最適重み w*]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#ffe0b2
    style E fill:#e8f5e9
</div>

<h3>更新式</h3>

<p>$$
\mathbf{w} := \mathbf{w} - \alpha \nabla_{\mathbf{w}} J(\mathbf{w})
$$</p>

<p>勾配：</p>

<p>$$
\nabla_{\mathbf{w}} J(\mathbf{w}) = \frac{2}{m} \mathbf{X}^T (\mathbf{X}\mathbf{w} - \mathbf{y})
$$</p>

<ul>
<li>$\alpha$: 学習率（learning rate）</li>
</ul>

<h3>実装例</h3>

<pre><code class="language-python">def gradient_descent(X, y, alpha=0.01, n_iterations=1000):
    """
    勾配降下法で線形回帰を学習

    Args:
        X: 特徴量行列 (バイアス項含む)
        y: 目的変数
        alpha: 学習率
        n_iterations: イテレーション数

    Returns:
        w: 学習した重み
        history: 損失関数の履歴
    """
    m = len(y)
    w = np.random.randn(X.shape[1], 1)  # 重みの初期化
    history = []

    for i in range(n_iterations):
        # 予測
        y_pred = X @ w

        # 損失計算
        loss = (1 / m) * np.sum((y_pred - y) ** 2)
        history.append(loss)

        # 勾配計算
        gradients = (2 / m) * X.T @ (y_pred - y)

        # 重み更新
        w = w - alpha * gradients

        if i % 100 == 0:
            print(f"Iteration {i}: Loss = {loss:.4f}")

    return w, history

# 実行
w_gd, loss_history = gradient_descent(X_b, y, alpha=0.1, n_iterations=1000)

print("\n勾配降下法で学習した重み:")
print(f"w0 (切片): {w_gd[0][0]:.4f}")
print(f"w1 (傾き): {w_gd[1][0]:.4f}")

# 損失関数の推移を可視化
plt.figure(figsize=(10, 6))
plt.plot(loss_history, linewidth=2)
plt.xlabel('Iteration', fontsize=12)
plt.ylabel('MSE Loss', fontsize=12)
plt.title('勾配降下法の収束過程', fontsize=14)
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>Iteration 0: Loss = 6.8421
Iteration 100: Loss = 0.8752
Iteration 200: Loss = 0.8284
Iteration 300: Loss = 0.8243
Iteration 400: Loss = 0.8236
Iteration 500: Loss = 0.8235
Iteration 600: Loss = 0.8235
Iteration 700: Loss = 0.8235
Iteration 800: Loss = 0.8235
Iteration 900: Loss = 0.8235

勾配降下法で学習した重み:
w0 (切片): 4.2152
w1 (傾き): 2.7703
</code></pre>

<h3>学習率の重要性</h3>

<pre><code class="language-python"># 異なる学習率での比較
learning_rates = [0.001, 0.01, 0.1, 0.5]

plt.figure(figsize=(12, 8))
for i, alpha in enumerate(learning_rates):
    w, history = gradient_descent(X_b, y, alpha=alpha, n_iterations=100)
    plt.subplot(2, 2, i+1)
    plt.plot(history, linewidth=2)
    plt.title(f'学習率 α = {alpha}', fontsize=12)
    plt.xlabel('Iteration')
    plt.ylabel('MSE Loss')
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<hr>

<h2>1.5 多項式回帰（Polynomial Regression）</h2>

<h3>概要</h3>

<p>線形回帰では表現できない<strong>非線形関係</strong>をモデル化します。</p>

<p>$$
y = w_0 + w_1 x + w_2 x^2 + \cdots + w_d x^d
$$</p>

<p>特徴量を変換することで、線形回帰の枠組みを使用できます。</p>

<h3>実装例</h3>

<pre><code class="language-python">from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline

# 非線形データ生成
np.random.seed(42)
X = 6 * np.random.rand(100, 1) - 3
y = 0.5 * X**2 + X + 2 + np.random.randn(100, 1)

# 多項式回帰（次数2）
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)

model = LinearRegression()
model.fit(X_poly, y)

print("多項式回帰の係数:")
print(f"w1 (x): {model.coef_[0][0]:.4f}")
print(f"w2 (x²): {model.coef_[0][1]:.4f}")
print(f"切片: {model.intercept_[0]:.4f}")

# 予測と可視化
X_test = np.linspace(-3, 3, 100).reshape(-1, 1)
X_test_poly = poly_features.transform(X_test)
y_pred = model.predict(X_test_poly)

plt.figure(figsize=(10, 6))
plt.scatter(X, y, alpha=0.6, label='データ')
plt.plot(X_test, y_pred, 'r-', linewidth=2, label='多項式回帰 (次数2)')
plt.xlabel('X', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('多項式回帰', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

<h3>過学習の危険性</h3>

<pre><code class="language-python"># 異なる次数での比較
degrees = [1, 2, 5, 10]

plt.figure(figsize=(14, 10))
for i, degree in enumerate(degrees):
    poly_features = PolynomialFeatures(degree=degree, include_bias=False)
    X_poly = poly_features.fit_transform(X)

    model = LinearRegression()
    model.fit(X_poly, y)

    X_test_poly = poly_features.transform(X_test)
    y_pred = model.predict(X_test_poly)

    plt.subplot(2, 2, i+1)
    plt.scatter(X, y, alpha=0.6, label='データ')
    plt.plot(X_test, y_pred, 'r-', linewidth=2, label=f'次数 {degree}')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title(f'多項式回帰（次数 {degree}）', fontsize=12)
    plt.ylim(-5, 15)
    plt.legend()
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<blockquote>
<p><strong>注意</strong>: 次数が高すぎると過学習（overfitting）が発生します。次数10ではデータに過剰適合し、汎化性能が低下します。</p>
</blockquote>

<hr>

<h2>1.6 正則化（Regularization）</h2>

<h3>概要</h3>

<p>過学習を防ぐため、損失関数に<strong>ペナルティ項</strong>を追加します。</p>

<div class="mermaid">
graph TD
    A[正則化手法] --> B[Ridge L2正則化]
    A --> C[Lasso L1正則化]
    A --> D[Elastic Net L1+L2]

    B --> B1[重みの大きさを抑制]
    C --> C1[重みの一部をゼロに]
    D --> D1[両方のバランス]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
</div>

<h3>Ridge回帰（L2正則化）</h3>

<p>$$
J(\mathbf{w}) = \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2 + \alpha \sum_{j=1}^{n} w_j^2
$$</p>

<ul>
<li>$\alpha$: 正則化パラメータ</li>
<li>重みの二乗和にペナルティ</li>
</ul>

<pre><code class="language-python">from sklearn.linear_model import Ridge

# Ridge回帰
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_poly, y)

print("Ridge回帰の係数:")
print(f"重み: {ridge_model.coef_[0]}")
print(f"切片: {ridge_model.intercept_[0]:.4f}")
</code></pre>

<h3>Lasso回帰（L1正則化）</h3>

<p>$$
J(\mathbf{w}) = \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2 + \alpha \sum_{j=1}^{n} |w_j|
$$</p>

<ul>
<li>重みの絶対値和にペナルティ</li>
<li><strong>スパース性</strong>: 重要でない特徴量の重みをゼロにする</li>
</ul>

<pre><code class="language-python">from sklearn.linear_model import Lasso

# Lasso回帰
lasso_model = Lasso(alpha=0.1)
lasso_model.fit(X_poly, y)

print("\nLasso回帰の係数:")
print(f"重み: {lasso_model.coef_}")
print(f"切片: {lasso_model.intercept_:.4f}")
print(f"ゼロの重み数: {np.sum(lasso_model.coef_ == 0)}")
</code></pre>

<h3>Elastic Net（L1 + L2正則化）</h3>

<p>$$
J(\mathbf{w}) = \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2 + \alpha \rho \sum_{j=1}^{n} |w_j| + \frac{\alpha(1-\rho)}{2} \sum_{j=1}^{n} w_j^2
$$</p>

<ul>
<li>$\rho$: L1とL2のバランス（0〜1）</li>
</ul>

<pre><code class="language-python">from sklearn.linear_model import ElasticNet

# Elastic Net
elastic_model = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic_model.fit(X_poly, y)

print("\nElastic Net回帰の係数:")
print(f"重み: {elastic_model.coef_}")
print(f"切片: {elastic_model.intercept_:.4f}")
</code></pre>

<h3>正則化パラメータの比較</h3>

<pre><code class="language-python"># 異なるalphaでの比較
alphas = np.logspace(-3, 2, 100)
ridge_coefs = []
lasso_coefs = []

for alpha in alphas:
    ridge = Ridge(alpha=alpha)
    ridge.fit(X_poly, y)
    ridge_coefs.append(ridge.coef_[0])

    lasso = Lasso(alpha=alpha, max_iter=10000)
    lasso.fit(X_poly, y)
    lasso_coefs.append(lasso.coef_)

ridge_coefs = np.array(ridge_coefs)
lasso_coefs = np.array(lasso_coefs)

plt.figure(figsize=(14, 6))

# Ridge
plt.subplot(1, 2, 1)
for i in range(X_poly.shape[1]):
    plt.plot(alphas, ridge_coefs[:, i], label=f'w{i+1}')
plt.xscale('log')
plt.xlabel('Alpha (正則化強度)', fontsize=12)
plt.ylabel('係数の大きさ', fontsize=12)
plt.title('Ridge回帰: 正則化パラメータの影響', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

# Lasso
plt.subplot(1, 2, 2)
for i in range(X_poly.shape[1]):
    plt.plot(alphas, lasso_coefs[:, i], label=f'w{i+1}')
plt.xscale('log')
plt.xlabel('Alpha (正則化強度)', fontsize=12)
plt.ylabel('係数の大きさ', fontsize=12)
plt.title('Lasso回帰: 正則化パラメータの影響', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<hr>

<h2>1.7 回帰モデルの評価</h2>

<h3>評価指標</h3>

<table>
<thead>
<tr>
<th>指標</th>
<th>数式</th>
<th>説明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>平均絶対誤差</strong><br>(MAE)</td>
<td>$\frac{1}{m}\sum|y_i - \hat{y}_i|$</td>
<td>予測誤差の平均（外れ値に頑健）</td>
</tr>
<tr>
<td><strong>平均二乗誤差</strong><br>(MSE)</td>
<td>$\frac{1}{m}\sum(y_i - \hat{y}_i)^2$</td>
<td>予測誤差の二乗平均（外れ値に敏感）</td>
</tr>
<tr>
<td><strong>平均二乗平方根誤差</strong><br>(RMSE)</td>
<td>$\sqrt{\frac{1}{m}\sum(y_i - \hat{y}_i)^2}$</td>
<td>MSEの平方根（元の単位）</td>
</tr>
<tr>
<td><strong>決定係数</strong><br>(R²)</td>
<td>$1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}$</td>
<td>モデルの説明力（0〜1、高いほど良い）</td>
</tr>
</tbody>
</table>

<h3>実装例</h3>

<pre><code class="language-python">from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# データ分割
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_poly, y, test_size=0.2, random_state=42
)

# モデル学習
model = LinearRegression()
model.fit(X_train, y_train)

# 予測
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# 評価
print("=== 訓練データ ===")
print(f"MAE:  {mean_absolute_error(y_train, y_train_pred):.4f}")
print(f"MSE:  {mean_squared_error(y_train, y_train_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_train, y_train_pred)):.4f}")
print(f"R²:   {r2_score(y_train, y_train_pred):.4f}")

print("\n=== テストデータ ===")
print(f"MAE:  {mean_absolute_error(y_test, y_test_pred):.4f}")
print(f"MSE:  {mean_squared_error(y_test, y_test_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred)):.4f}")
print(f"R²:   {r2_score(y_test, y_test_pred):.4f}")

# 残差プロット
residuals = y_test - y_test_pred

plt.figure(figsize=(14, 6))

# 予測 vs 実際
plt.subplot(1, 2, 1)
plt.scatter(y_test, y_test_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)
plt.xlabel('実際の値', fontsize=12)
plt.ylabel('予測値', fontsize=12)
plt.title('予測 vs 実際', fontsize=14)
plt.grid(True, alpha=0.3)

# 残差プロット
plt.subplot(1, 2, 2)
plt.scatter(y_test_pred, residuals, alpha=0.6)
plt.axhline(y=0, color='r', linestyle='--', linewidth=2)
plt.xlabel('予測値', fontsize=12)
plt.ylabel('残差', fontsize=12)
plt.title('残差プロット', fontsize=14)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== 訓練データ ===
MAE:  0.7234
MSE:  0.8456
RMSE: 0.9196
R²:   0.9145

=== テストデータ ===
MAE:  0.7891
MSE:  0.9234
RMSE: 0.9609
R²:   0.9023
</code></pre>

<hr>

<h2>1.8 本章のまとめ</h2>

<h3>学んだこと</h3>

<ol>
<li><p><strong>回帰問題の定義</strong></p>
<ul>
<li>連続値の予測タスク</li>
<li>実世界の応用例（価格予測、需要予測など）</li>
</ul></li>
<li><p><strong>線形回帰</strong></p>
<ul>
<li>最小二乗法による解析解</li>
<li>勾配降下法による数値解</li>
</ul></li>
<li><p><strong>多項式回帰</strong></p>
<ul>
<li>非線形関係のモデル化</li>
<li>過学習の危険性</li>
</ul></li>
<li><p><strong>正則化</strong></p>
<ul>
<li>Ridge（L2）: 重みの大きさを抑制</li>
<li>Lasso（L1）: スパース性の導入</li>
<li>Elastic Net: 両方のバランス</li>
</ul></li>
<li><p><strong>評価指標</strong></p>
<ul>
<li>MAE、MSE、RMSE、R²</li>
<li>残差分析の重要性</li>
</ul></li>
</ol>

<h3>次の章へ</h3>

<p>第2章では、<strong>分類問題の基礎</strong>を学びます：</p>
<ul>
<li>ロジスティック回帰</li>
<li>決定木</li>
<li>k-NN、SVM</li>
<li>評価指標（精度、再現率、F1スコア）</li>
</ul>

<hr>

<h2>演習問題</h2>

<h3>問題1（難易度：easy）</h3>
<p>回帰問題と分類問題の違いを3つ挙げてください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>
<ol>
<li><strong>出力の種類</strong>: 回帰は連続値、分類は離散値（カテゴリ）</li>
<li><strong>損失関数</strong>: 回帰はMSE、分類は交差エントロピー</li>
<li><strong>評価指標</strong>: 回帰はRMSE/R²、分類は精度/F1スコア</li>
</ol>

</details>

<h3>問題2（難易度：medium）</h3>
<p>以下のデータで線形回帰を実装し、重みとバイアスを求めてください。</p>

<pre><code class="language-python">X = np.array([[1], [2], [3], [4], [5]])
y = np.array([[2], [4], [5], [4], [5]])
</code></pre>

<details>
<summary>解答例</summary>

<pre><code class="language-python">import numpy as np

X = np.array([[1], [2], [3], [4], [5]])
y = np.array([[2], [4], [5], [4], [5]])

# バイアス項を追加
X_b = np.c_[np.ones((5, 1)), X]

# 正規方程式
w = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y

print(f"切片 w0: {w[0][0]:.4f}")
print(f"傾き w1: {w[1][0]:.4f}")

# 予測
y_pred = X_b @ w
print(f"\n予測値: {y_pred.flatten()}")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>切片 w0: 2.2000
傾き w1: 0.6000

予測値: [2.8 3.4 4.  4.6 5.2]
</code></pre>

</details>

<h3>問題3（難易度：medium）</h3>
<p>学習率が大きすぎる場合、勾配降下法でどのような問題が発生しますか？</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>
<ul>
<li><strong>発散（divergence）</strong>: 損失関数が最小値を通り越して発散する</li>
<li><strong>振動</strong>: 最小値の周りで振動し続ける</li>
<li><strong>収束しない</strong>: 最適解に到達できない</li>
</ul>

<p><strong>対策</strong>：</p>
<ul>
<li>学習率を小さくする（例: 0.1 → 0.01）</li>
<li>学習率スケジューリングを使用する</li>
<li>適応的最適化手法（Adam、RMSprop）を使用する</li>
</ul>

</details>

<h3>問題4（難易度：hard）</h3>
<p>Ridge回帰とLasso回帰の違いを説明し、どのような場合にそれぞれを使うべきか述べてください。</p>

<details>
<summary>解答例</summary>

<p><strong>Ridge回帰（L2正則化）</strong>：</p>
<ul>
<li>重みの二乗和にペナルティ</li>
<li>重みを小さくするが、ゼロにはしない</li>
<li><strong>使用場面</strong>: 多重共線性がある場合、すべての特徴量が重要な場合</li>
</ul>

<p><strong>Lasso回帰（L1正則化）</strong>：</p>
<ul>
<li>重みの絶対値和にペナルティ</li>
<li>重要でない特徴量の重みをゼロにする（スパース性）</li>
<li><strong>使用場面</strong>: 特徴量選択が必要な場合、解釈性を高めたい場合</li>
</ul>

<p><strong>選択基準</strong>：</p>
<table>
<thead>
<tr>
<th>状況</th>
<th>推奨手法</th>
</tr>
</thead>
<tbody>
<tr>
<td>特徴量が多く、重要度が不明</td>
<td>Lasso</td>
</tr>
<tr>
<td>多重共線性がある</td>
<td>Ridge</td>
</tr>
<tr>
<td>特徴量選択が必要</td>
<td>Lasso</td>
</tr>
<tr>
<td>すべての特徴量を使いたい</td>
<td>Ridge</td>
</tr>
<tr>
<td>どちらか不明</td>
<td>Elastic Net</td>
</tr>
</tbody>
</table>

</details>

<h3>問題5（難易度：hard）</h3>
<p>以下のコードを完成させ、交差検証でRidge回帰の最適なalphaを見つけてください。</p>

<pre><code class="language-python">from sklearn.model_selection import cross_val_score
from sklearn.linear_model import Ridge

# データ生成（省略）
alphas = np.logspace(-3, 3, 50)

# ここに実装
</code></pre>

<details>
<summary>解答例</summary>

<pre><code class="language-python">from sklearn.model_selection import cross_val_score
from sklearn.linear_model import Ridge
import numpy as np
import matplotlib.pyplot as plt

# データ生成
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=5, include_bias=False)
X_poly = poly.fit_transform(X)

alphas = np.logspace(-3, 3, 50)
scores = []

for alpha in alphas:
    ridge = Ridge(alpha=alpha)
    # 5分割交差検証
    cv_scores = cross_val_score(ridge, X_poly, y.ravel(),
                                 cv=5, scoring='neg_mean_squared_error')
    scores.append(-cv_scores.mean())  # 負のMSEを正に変換

# 最適なalphaを見つける
best_alpha = alphas[np.argmin(scores)]
best_score = np.min(scores)

print(f"最適なalpha: {best_alpha:.4f}")
print(f"最小MSE: {best_score:.4f}")

# 可視化
plt.figure(figsize=(10, 6))
plt.plot(alphas, scores, linewidth=2)
plt.axvline(best_alpha, color='r', linestyle='--',
            label=f'最適alpha = {best_alpha:.4f}')
plt.xscale('log')
plt.xlabel('Alpha', fontsize=12)
plt.ylabel('MSE (交差検証)', fontsize=12)
plt.title('Ridge回帰: 最適なalphaの探索', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>最適なalpha: 2.1544
最小MSE: 1.0234
</code></pre>

</details>

<hr>

<h2>参考文献</h2>

<ol>
<li>Bishop, C. M. (2006). <em>Pattern Recognition and Machine Learning</em>. Springer.</li>
<li>Hastie, T., Tibshirani, R., & Friedman, J. (2009). <em>The Elements of Statistical Learning</em>. Springer.</li>
<li>Géron, A. (2019). <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em>. O'Reilly Media.</li>
</ol>

<div class="navigation">
    <a href="index.html" class="nav-button">← シリーズ目次</a>
    <a href="chapter2-classification.html" class="nav-button">次の章: 分類問題の基礎 →</a>
</div>

    </main>

    <footer>
        <p><strong>作成者</strong>: AI Terakoya Content Team</p>
        <p><strong>バージョン</strong>: 1.0 | <strong>作成日</strong>: 2025-10-20</p>
        <p><strong>ライセンス</strong>: Creative Commons BY 4.0</p>
        <p>© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
