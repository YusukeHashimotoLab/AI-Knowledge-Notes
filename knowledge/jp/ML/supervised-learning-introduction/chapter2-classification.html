<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第2章：分類問題の基礎 - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>第2章：分類問題の基礎</h1>
            <p class="subtitle">カテゴリ予測の理論と実装 - ロジスティック回帰から決定木・SVMまで</p>
            <div class="meta">
                <span class="meta-item">📖 読了時間: 25-30分</span>
                <span class="meta-item">📊 難易度: 初級〜中級</span>
                <span class="meta-item">💻 コード例: 12個</span>
                <span class="meta-item">📝 演習問題: 5問</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>学習目標</h2>
<p>この章を読むことで、以下を習得できます：</p>
<ul>
<li>✅ 分類問題の定義と応用例を理解する</li>
<li>✅ ロジスティック回帰の理論と実装ができる</li>
<li>✅ シグモイド関数と確率解釈を説明できる</li>
<li>✅ 決定木の仕組みと実装ができる</li>
<li>✅ k-NN、SVMを適用できる</li>
<li>✅ 混同行列、精度、再現率、F1スコアで評価できる</li>
<li>✅ ROC曲線とAUCを理解し使いこなせる</li>
</ul>

<hr>

<h2>2.1 分類問題とは</h2>

<h3>定義</h3>
<p><strong>分類問題（Classification）</strong>は、入力変数から<strong>離散値（カテゴリ）</strong>の出力を予測する教師あり学習のタスクです。</p>

<blockquote>
<p>「特徴量 $X$ から離散的なクラスラベル $y \in \{1, 2, ..., K\}$ を予測する関数 $f: X \rightarrow y$ を学習する」</p>
</blockquote>

<h3>分類のタイプ</h3>

<table>
<thead>
<tr>
<th>タイプ</th>
<th>クラス数</th>
<th>例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>二値分類</strong></td>
<td>2クラス</td>
<td>スパム判定、疾病診断、顧客離反予測</td>
</tr>
<tr>
<td><strong>多クラス分類</strong></td>
<td>3+クラス</td>
<td>手書き数字認識、画像分類、感情分析</td>
</tr>
<tr>
<td><strong>多ラベル分類</strong></td>
<td>複数ラベル</td>
<td>タグ付け、遺伝子機能予測</td>
</tr>
</tbody>
</table>

<h3>実世界の応用例</h3>

<div class="mermaid">
graph LR
    A[分類問題の応用] --> B[医療: 疾病診断]
    A --> C[金融: クレジット審査]
    A --> D[マーケティング: 顧客セグメント]
    A --> E[セキュリティ: 不正検知]
    A --> F[画像: 物体認識]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#fff3e0
    style D fill:#fff3e0
    style E fill:#fff3e0
    style F fill:#fff3e0
</div>

<hr>

<h2>2.2 ロジスティック回帰（Logistic Regression）</h2>

<h3>概要</h3>

<p><strong>ロジスティック回帰</strong>は、二値分類に使われる線形モデルです。線形回帰にシグモイド関数を適用して確率を出力します。</p>

<h3>シグモイド関数</h3>

<p>$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$</p>

<p>特徴：</p>
<ul>
<li>出力範囲: $[0, 1]$（確率として解釈可能）</li>
<li>$z = 0$ で $\sigma(z) = 0.5$</li>
<li>滑らかな S字カーブ</li>
</ul>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# シグモイド関数
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# 可視化
z = np.linspace(-10, 10, 100)
y = sigmoid(z)

plt.figure(figsize=(10, 6))
plt.plot(z, y, linewidth=2, label='σ(z)')
plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='閾値 0.5')
plt.axvline(x=0, color='g', linestyle='--', alpha=0.5)
plt.xlabel('z = w^T x', fontsize=12)
plt.ylabel('σ(z)', fontsize=12)
plt.title('シグモイド関数', fontsize=14)
plt.grid(True, alpha=0.3)
plt.legend()
plt.show()
</code></pre>

<h3>モデルの定義</h3>

<p>$$
P(y=1 | \mathbf{x}) = \sigma(\mathbf{w}^T \mathbf{x}) = \frac{1}{1 + e^{-\mathbf{w}^T \mathbf{x}}}
$$</p>

<p>予測：</p>

<p>$$
\hat{y} = \begin{cases}
1 & \text{if } P(y=1 | \mathbf{x}) \geq 0.5 \\
0 & \text{otherwise}
\end{cases}
$$</p>

<h3>損失関数：交差エントロピー</h3>

<p>$$
J(\mathbf{w}) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right]
$$</p>

<h3>実装例</h3>

<pre><code class="language-python">from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# データ生成
X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0,
                          n_informative=2, random_state=42, n_clusters_per_class=1)

# データ分割
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ロジスティック回帰
model = LogisticRegression()
model.fit(X_train, y_train)

# 予測
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)

print("=== ロジスティック回帰 ===")
print(f"精度: {accuracy_score(y_test, y_pred):.4f}")
print(f"\n重み: {model.coef_[0]}")
print(f"切片: {model.intercept_[0]:.4f}")

# 決定境界の可視化
def plot_decision_boundary(model, X, y):
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                        np.arange(y_min, y_max, h))

    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(10, 6))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')
    plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', marker='o',
                edgecolors='k', s=80, label='クラス 0')
    plt.scatter(X[y==1, 0], X[y==1, 1], c='red', marker='s',
                edgecolors='k', s=80, label='クラス 1')
    plt.xlabel('特徴量 1', fontsize=12)
    plt.ylabel('特徴量 2', fontsize=12)
    plt.title('ロジスティック回帰の決定境界', fontsize=14)
    plt.legend()
    plt.show()

plot_decision_boundary(model, X_test, y_test)
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== ロジスティック回帰 ===
精度: 0.9550

重み: [2.14532851 1.87653214]
切片: -0.2341
</code></pre>

<hr>

<h2>2.3 決定木（Decision Tree）</h2>

<h3>概要</h3>

<p><strong>決定木</strong>は、if-then-elseルールの階層構造で分類を行います。特徴量を基準に再帰的にデータを分割します。</p>

<div class="mermaid">
graph TD
    A[特徴量1 <= 0.5] -->|Yes| B[特徴量2 <= 1.2]
    A -->|No| C[クラス 1]
    B -->|Yes| D[クラス 0]
    B -->|No| E[クラス 1]

    style A fill:#fff3e0
    style B fill:#fff3e0
    style C fill:#e8f5e9
    style D fill:#e3f2fd
    style E fill:#e8f5e9
</div>

<h3>分割基準</h3>

<p><strong>1. ジニ不純度（Gini Impurity）</strong>：</p>

<p>$$
\text{Gini}(S) = 1 - \sum_{i=1}^{K} p_i^2
$$</p>

<ul>
<li>$p_i$: クラス $i$ の割合</li>
<li>値が小さいほど純粋（一つのクラスに偏っている）</li>
</ul>

<p><strong>2. エントロピー（Entropy）</strong>：</p>

<p>$$
\text{Entropy}(S) = -\sum_{i=1}^{K} p_i \log_2(p_i)
$$</p>

<h3>実装例</h3>

<pre><code class="language-python">from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree

# 決定木モデル
dt_model = DecisionTreeClassifier(max_depth=3, random_state=42)
dt_model.fit(X_train, y_train)

# 予測
y_pred_dt = dt_model.predict(X_test)

print("=== 決定木 ===")
print(f"精度: {accuracy_score(y_test, y_pred_dt):.4f}")

# 決定木の可視化
plt.figure(figsize=(16, 10))
plot_tree(dt_model, filled=True, feature_names=['特徴量1', '特徴量2'],
          class_names=['クラス0', 'クラス1'], fontsize=10)
plt.title('決定木の構造', fontsize=16)
plt.show()

# 決定境界
plot_decision_boundary(dt_model, X_test, y_test)
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== 決定木 ===
精度: 0.9450
</code></pre>

<h3>特徴量重要度</h3>

<pre><code class="language-python"># 特徴量重要度
importances = dt_model.feature_importances_

plt.figure(figsize=(8, 6))
plt.bar(['特徴量1', '特徴量2'], importances, color=['#3498db', '#e74c3c'])
plt.ylabel('重要度', fontsize=12)
plt.title('特徴量重要度', fontsize=14)
plt.grid(axis='y', alpha=0.3)
plt.show()

print(f"\n特徴量1の重要度: {importances[0]:.4f}")
print(f"特徴量2の重要度: {importances[1]:.4f}")
</code></pre>

<hr>

<h2>2.4 k-近傍法（k-Nearest Neighbors, k-NN）</h2>

<h3>概要</h3>

<p><strong>k-NN</strong>は、最も近い $k$ 個の訓練データの多数決で分類します。</p>

<h3>アルゴリズム</h3>

<ol>
<li>テストデータ $\mathbf{x}$ に対して、訓練データとの距離を計算</li>
<li>最も近い $k$ 個のデータを選択</li>
<li>多数決で最も多いクラスを予測</li>
</ol>

<h3>距離の種類</h3>

<table>
<thead>
<tr>
<th>距離</th>
<th>数式</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ユークリッド距離</strong></td>
<td>$\sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}$</td>
</tr>
<tr>
<td><strong>マンハッタン距離</strong></td>
<td>$\sum_{i=1}^{n} |x_i - y_i|$</td>
</tr>
<tr>
<td><strong>ミンコフスキー距離</strong></td>
<td>$\left(\sum_{i=1}^{n} |x_i - y_i|^p\right)^{1/p}$</td>
</tr>
</tbody>
</table>

<h3>実装例</h3>

<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier

# k-NNモデル
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)

# 予測
y_pred_knn = knn_model.predict(X_test)

print("=== k-NN (k=5) ===")
print(f"精度: {accuracy_score(y_test, y_pred_knn):.4f}")

# 決定境界
plot_decision_boundary(knn_model, X_test, y_test)
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== k-NN (k=5) ===
精度: 0.9400
</code></pre>

<h3>kの選択</h3>

<pre><code class="language-python"># 異なるkでの精度比較
k_range = range(1, 31)
train_scores = []
test_scores = []

for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)

    train_scores.append(knn.score(X_train, y_train))
    test_scores.append(knn.score(X_test, y_test))

plt.figure(figsize=(10, 6))
plt.plot(k_range, train_scores, 'o-', label='訓練データ', linewidth=2)
plt.plot(k_range, test_scores, 's-', label='テストデータ', linewidth=2)
plt.xlabel('k (近傍数)', fontsize=12)
plt.ylabel('精度', fontsize=12)
plt.title('k-NN: kの値と精度の関係', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

best_k = k_range[np.argmax(test_scores)]
print(f"\n最適なk: {best_k}")
print(f"最高精度: {max(test_scores):.4f}")
</code></pre>

<hr>

<h2>2.5 サポートベクターマシン（SVM）</h2>

<h3>概要</h3>

<p><strong>SVM</strong>は、マージンを最大化する決定境界を見つけます。</p>

<div class="mermaid">
graph LR
    A[SVM] --> B[線形SVM]
    A --> C[非線形SVM カーネル法]

    B --> B1[線形分離可能なデータ]
    C --> C1[RBFカーネル]
    C --> C2[多項式カーネル]
    C --> C3[シグモイドカーネル]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
</div>

<h3>マージン最大化</h3>

<p>$$
\text{maximize} \quad \frac{2}{||\mathbf{w}||} \quad \text{subject to} \quad y^{(i)}(\mathbf{w}^T \mathbf{x}^{(i)} + b) \geq 1
$$</p>

<h3>カーネルトリック</h3>

<p><strong>RBF（ガウシアン）カーネル</strong>：</p>

<p>$$
K(\mathbf{x}, \mathbf{x}') = \exp\left(-\frac{||\mathbf{x} - \mathbf{x}'||^2}{2\sigma^2}\right)
$$</p>

<h3>実装例</h3>

<pre><code class="language-python">from sklearn.svm import SVC

# 線形SVM
svm_linear = SVC(kernel='linear')
svm_linear.fit(X_train, y_train)

# RBF SVM
svm_rbf = SVC(kernel='rbf', gamma='auto')
svm_rbf.fit(X_train, y_train)

print("=== SVM (線形カーネル) ===")
print(f"精度: {svm_linear.score(X_test, y_test):.4f}")

print("\n=== SVM (RBFカーネル) ===")
print(f"精度: {svm_rbf.score(X_test, y_test):.4f}")

# 決定境界の比較
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

for ax, model, title in zip(axes, [svm_linear, svm_rbf],
                            ['線形SVM', 'RBF SVM']):
    h = 0.02
    x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1
    y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                        np.arange(y_min, y_max, h))

    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')
    ax.scatter(X_test[y_test==0, 0], X_test[y_test==0, 1],
              c='blue', marker='o', edgecolors='k', s=80, label='クラス 0')
    ax.scatter(X_test[y_test==1, 0], X_test[y_test==1, 1],
              c='red', marker='s', edgecolors='k', s=80, label='クラス 1')
    ax.set_xlabel('特徴量 1')
    ax.set_ylabel('特徴量 2')
    ax.set_title(title, fontsize=14)
    ax.legend()

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== SVM (線形カーネル) ===
精度: 0.9550

=== SVM (RBFカーネル) ===
精度: 0.9650
</code></pre>

<hr>

<h2>2.6 分類モデルの評価</h2>

<h3>混同行列（Confusion Matrix）</h3>

<table>
<thead>
<tr>
<th></th>
<th>予測: 陽性</th>
<th>予測: 陰性</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>実際: 陽性</strong></td>
<td>TP (真陽性)</td>
<td>FN (偽陰性)</td>
</tr>
<tr>
<td><strong>実際: 陰性</strong></td>
<td>FP (偽陽性)</td>
<td>TN (真陰性)</td>
</tr>
</tbody>
</table>

<h3>評価指標</h3>

<table>
<thead>
<tr>
<th>指標</th>
<th>数式</th>
<th>意味</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>精度</strong><br>(Accuracy)</td>
<td>$\frac{TP + TN}{TP + TN + FP + FN}$</td>
<td>全体の正解率</td>
</tr>
<tr>
<td><strong>適合率</strong><br>(Precision)</td>
<td>$\frac{TP}{TP + FP}$</td>
<td>陽性予測の正確さ</td>
</tr>
<tr>
<td><strong>再現率</strong><br>(Recall)</td>
<td>$\frac{TP}{TP + FN}$</td>
<td>実際の陽性を捕捉する率</td>
</tr>
<tr>
<td><strong>F1スコア</strong></td>
<td>$2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$</td>
<td>PrecisionとRecallの調和平均</td>
</tr>
</tbody>
</table>

<h3>実装例</h3>

<pre><code class="language-python">from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

# 混同行列
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['クラス 0', 'クラス 1'],
            yticklabels=['クラス 0', 'クラス 1'])
plt.xlabel('予測', fontsize=12)
plt.ylabel('実際', fontsize=12)
plt.title('混同行列', fontsize=14)
plt.show()

# 詳細な評価レポート
print("\n=== 分類レポート ===")
print(classification_report(y_test, y_pred,
                          target_names=['クラス 0', 'クラス 1']))
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== 分類レポート ===
              precision    recall  f1-score   support

    クラス 0       0.96      0.95      0.95        99
    クラス 1       0.95      0.96      0.96       101

    accuracy                           0.96       200
   macro avg       0.96      0.96      0.96       200
weighted avg       0.96      0.96      0.96       200
</code></pre>

<h3>ROC曲線とAUC</h3>

<p><strong>ROC（Receiver Operating Characteristic）曲線</strong>は、閾値を変えたときのTPR（真陽性率）とFPR（偽陽性率）の関係を示します。</p>

<p>$$
\text{TPR} = \frac{TP}{TP + FN}, \quad \text{FPR} = \frac{FP}{FP + TN}
$$</p>

<pre><code class="language-python">from sklearn.metrics import roc_curve, roc_auc_score

# ROC曲線の計算
fpr, tpr, thresholds = roc_curve(y_test, y_proba[:, 1])
auc = roc_auc_score(y_test, y_proba[:, 1])

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, linewidth=2, label=f'ROC曲線 (AUC = {auc:.4f})')
plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='ランダム (AUC = 0.5)')
plt.xlabel('偽陽性率 (FPR)', fontsize=12)
plt.ylabel('真陽性率 (TPR)', fontsize=12)
plt.title('ROC曲線', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)
plt.show()

print(f"AUC: {auc:.4f}")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>AUC: 0.9876
</code></pre>

<blockquote>
<p><strong>AUC（Area Under the Curve）</strong>: ROC曲線の下の面積。1に近いほど良いモデル。</p>
</blockquote>

<hr>

<h2>2.7 本章のまとめ</h2>

<h3>学んだこと</h3>

<ol>
<li><p><strong>分類問題の定義</strong></p>
<ul>
<li>離散値（カテゴリ）の予測タスク</li>
<li>二値分類、多クラス分類、多ラベル分類</li>
</ul></li>
<li><p><strong>ロジスティック回帰</strong></p>
<ul>
<li>シグモイド関数による確率出力</li>
<li>交差エントロピー損失</li>
</ul></li>
<li><p><strong>決定木</strong></p>
<ul>
<li>if-then-elseルールの階層構造</li>
<li>ジニ不純度、エントロピー</li>
<li>特徴量重要度</li>
</ul></li>
<li><p><strong>k-NN</strong></p>
<ul>
<li>最近傍の多数決</li>
<li>kの選択の重要性</li>
</ul></li>
<li><p><strong>SVM</strong></p>
<ul>
<li>マージン最大化</li>
<li>カーネルトリック</li>
</ul></li>
<li><p><strong>評価指標</strong></p>
<ul>
<li>混同行列、精度、適合率、再現率、F1スコア</li>
<li>ROC曲線とAUC</li>
</ul></li>
</ol>

<h3>次の章へ</h3>

<p>第3章では、<strong>アンサンブル手法</strong>を学びます：</p>
<ul>
<li>Baggingの原理</li>
<li>Random Forest</li>
<li>Boosting（Gradient Boosting、XGBoost、LightGBM、CatBoost）</li>
</ul>

<hr>

<h2>演習問題</h2>

<h3>問題1（難易度：easy）</h3>
<p>精度（Accuracy）が高くても不適切な場合があります。どのような状況か説明してください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>
<p><strong>不均衡データ（Imbalanced Data）</strong>の場合、精度は不適切です。</p>

<p><strong>例</strong>：</p>
<ul>
<li>がん診断データ: 陽性1%、陰性99%</li>
<li>すべて「陰性」と予測すると精度99%だが、意味がない</li>
<li>陽性を見逃すと重大な結果に</li>
</ul>

<p><strong>適切な指標</strong>：</p>
<ul>
<li>再現率（Recall）: 陽性を見逃さない</li>
<li>F1スコア: PrecisionとRecallのバランス</li>
<li>AUC: 閾値に依存しない評価</li>
</ul>

</details>

<h3>問題2（難易度：medium）</h3>
<p>以下の混同行列から、精度、適合率、再現率、F1スコアを計算してください。</p>

<table>
<thead>
<tr>
<th></th>
<th>予測: 陽性</th>
<th>予測: 陰性</th>
</tr>
</thead>
<tbody>
<tr>
<td>実際: 陽性</td>
<td>80</td>
<td>20</td>
</tr>
<tr>
<td>実際: 陰性</td>
<td>10</td>
<td>90</td>
</tr>
</tbody>
</table>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>

<pre><code>TP = 80, FN = 20, FP = 10, TN = 90

精度 (Accuracy) = (TP + TN) / (TP + TN + FP + FN)
                = (80 + 90) / (80 + 90 + 10 + 20)
                = 170 / 200 = 0.85 = 85%

適合率 (Precision) = TP / (TP + FP)
                   = 80 / (80 + 10)
                   = 80 / 90 = 0.8889 = 88.89%

再現率 (Recall) = TP / (TP + FN)
                = 80 / (80 + 20)
                = 80 / 100 = 0.80 = 80%

F1スコア = 2 * (Precision * Recall) / (Precision + Recall)
        = 2 * (0.8889 * 0.80) / (0.8889 + 0.80)
        = 2 * 0.7111 / 1.6889
        = 0.8421 = 84.21%
</code></pre>

</details>

<h3>問題3（難易度：medium）</h3>
<p>k-NNで最適なkを選ぶ際、kが小さすぎる場合と大きすぎる場合の問題を説明してください。</p>

<details>
<summary>解答例</summary>

<p><strong>kが小さすぎる場合（例: k=1）</strong>：</p>
<ul>
<li><strong>過学習（Overfitting）</strong>: ノイズに敏感</li>
<li>訓練データの精度は高いが、テストデータの精度は低い</li>
<li>決定境界が複雑でギザギザ</li>
</ul>

<p><strong>kが大きすぎる場合（例: k=全データ数）</strong>：</p>
<ul>
<li><strong>過度な単純化</strong>: すべて多数派クラスに分類</li>
<li>決定境界が単純すぎる</li>
<li>訓練・テスト両方の精度が低い</li>
</ul>

<p><strong>最適なk</strong>：</p>
<ul>
<li>交差検証で選択</li>
<li>通常 $\sqrt{N}$ 付近（$N$はデータ数）</li>
<li>奇数を選ぶ（二値分類の同票を避ける）</li>
</ul>

</details>

<h3>問題4（難易度：hard）</h3>
<p>SVMのカーネルトリックを使う意義を、計算量の観点から説明してください。</p>

<details>
<summary>解答例</summary>

<p><strong>カーネルトリックの意義</strong>：</p>

<p><strong>問題</strong>：非線形分離可能なデータを分類するには、高次元空間に変換する必要がある。</p>

<p><strong>直接的な方法</strong>：</p>
<ul>
<li>特徴量を明示的に高次元に変換: $\phi(\mathbf{x})$</li>
<li>計算量: $O(d^2)$ または $O(d^3)$（$d$は次元）</li>
<li>次元が高いと計算不可能</li>
</ul>

<p><strong>カーネルトリック</strong>：</p>
<ul>
<li>内積 $\langle \phi(\mathbf{x}), \phi(\mathbf{x}') \rangle$ をカーネル関数 $K(\mathbf{x}, \mathbf{x}')$ で直接計算</li>
<li>高次元変換を明示的に行わない</li>
<li>計算量: $O(d)$（元の次元のまま）</li>
</ul>

<p><strong>例（RBFカーネル）</strong>：</p>
<ul>
<li>無限次元への変換を $O(d)$ で計算</li>
<li>$K(\mathbf{x}, \mathbf{x}') = \exp(-\gamma ||\mathbf{x} - \mathbf{x}'||^2)$</li>
</ul>

<p><strong>結論</strong>：カーネルトリックにより、高次元での計算を低次元で効率的に実行可能。</p>

</details>

<h3>問題5（難易度：hard）</h3>
<p>ロジスティック回帰を実装し、irisデータセットの二値分類（setosa vs versicolor）を行ってください。</p>

<details>
<summary>解答例</summary>

<pre><code class="language-python">import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# データ読み込み
iris = load_iris()
X = iris.data[iris.target != 2]  # setosa (0) と versicolor (1)のみ
y = iris.target[iris.target != 2]

# 最初の2つの特徴量のみ使用（可視化のため）
X = X[:, :2]

# データ分割
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 標準化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ロジスティック回帰
model = LogisticRegression()
model.fit(X_train_scaled, y_train)

# 予測
y_pred = model.predict(X_test_scaled)
y_proba = model.predict_proba(X_test_scaled)

# 評価
print("=== 分類レポート ===")
print(classification_report(y_test, y_pred,
                          target_names=['setosa', 'versicolor']))

# 混同行列
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['setosa', 'versicolor'],
            yticklabels=['setosa', 'versicolor'])
plt.xlabel('予測')
plt.ylabel('実際')
plt.title('混同行列')
plt.show()

# 決定境界の可視化
h = 0.02
x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1
y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                    np.arange(y_min, y_max, h))

Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.figure(figsize=(10, 6))
plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')
plt.scatter(X_train_scaled[y_train==0, 0], X_train_scaled[y_train==0, 1],
           c='blue', marker='o', edgecolors='k', s=80, label='setosa')
plt.scatter(X_train_scaled[y_train==1, 0], X_train_scaled[y_train==1, 1],
           c='red', marker='s', edgecolors='k', s=80, label='versicolor')
plt.xlabel('Sepal length (標準化)')
plt.ylabel('Sepal width (標準化)')
plt.title('ロジスティック回帰の決定境界')
plt.legend()
plt.show()

print(f"\n精度: {model.score(X_test_scaled, y_test):.4f}")
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== 分類レポート ===
              precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        10
  versicolor       1.00      1.00      1.00        10

    accuracy                           1.00        20
   macro avg       1.00      1.00      1.00        20
weighted avg       1.00      1.00      1.00        20

精度: 1.0000
</code></pre>

</details>

<hr>

<h2>参考文献</h2>

<ol>
<li>Hastie, T., Tibshirani, R., & Friedman, J. (2009). <em>The Elements of Statistical Learning</em>. Springer.</li>
<li>Murphy, K. P. (2012). <em>Machine Learning: A Probabilistic Perspective</em>. MIT Press.</li>
<li>James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). <em>An Introduction to Statistical Learning</em>. Springer.</li>
</ol>

<div class="navigation">
    <a href="chapter1-regression.html" class="nav-button">← 前の章: 回帰問題の基礎</a>
    <a href="chapter3-ensemble.html" class="nav-button">次の章: アンサンブル手法 →</a>
</div>

    </main>

    <footer>
        <p><strong>作成者</strong>: AI Terakoya Content Team</p>
        <p><strong>バージョン</strong>: 1.0 | <strong>作成日</strong>: 2025-10-20</p>
        <p><strong>ライセンス</strong>: Creative Commons BY 4.0</p>
        <p>© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
