<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
<meta content="ç¬¬2ç« ï¼šQå­¦ç¿’ã¨SARSA - AI Terakoya" name="description"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬2ç« ï¼šQå­¦ç¿’ã¨SARSA - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/reinforcement-learning-introduction/chapter2-q-learning-sarsa.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/reinforcement-learning-introduction/index.html">Reinforcement Learning</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 2</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬2ç« ï¼šQå­¦ç¿’ã¨SARSA</h1>
            <p class="subtitle">æ™‚é–“å·®åˆ†å­¦ç¿’ã«ã‚ˆã‚‹ä¾¡å€¤é–¢æ•°ã®æ¨å®šã¨è¡Œå‹•æ–¹ç­–ã®æœ€é©åŒ–</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´šã€œä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… æ™‚é–“å·®åˆ†ï¼ˆTDï¼‰å­¦ç¿’ã®åŸºæœ¬åŸç†ã¨ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã¨ã®é•ã„ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Qå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆã‚ªãƒ•ãƒãƒªã‚·ãƒ¼å‹ï¼‰ã®ä»•çµ„ã¿ã¨æ›´æ–°å¼ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>âœ… SARSAã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆã‚ªãƒ³ãƒãƒªã‚·ãƒ¼å‹ï¼‰ã®ç‰¹å¾´ã¨é©ç”¨å ´é¢ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Îµ-greedyæ–¹ç­–ã«ã‚ˆã‚‹æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ãŒã§ãã‚‹</li>
<li>âœ… å­¦ç¿’ç‡ã¨å‰²å¼•ç‡ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>âœ… OpenAI Gymã®Taxi-v3ã‚„Cliff Walkingç’°å¢ƒã§å®Ÿè£…ã§ãã‚‹</li>
</ul>

<hr>

<h2>2.1 æ™‚é–“å·®åˆ†ï¼ˆTDï¼‰å­¦ç¿’ã®åŸºç¤</h2>

<h3>ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã®èª²é¡Œ</h3>

<p>ç¬¬1ç« ã§å­¦ã‚“ã ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã¯ã€<strong>ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ã¾ã§å¾…ã¤å¿…è¦ãŒã‚ã‚‹</strong>ã¨ã„ã†åˆ¶ç´„ãŒã‚ã‚Šã¾ã—ãŸï¼š</p>

$$
V(s_t) \leftarrow V(s_t) + \alpha [G_t - V(s_t)]
$$

<p>ã“ã“ã§ $G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \cdots$ ã¯å®Ÿéš›ã®åç›Šï¼ˆãƒªã‚¿ãƒ¼ãƒ³ï¼‰ã§ã™ã€‚</p>

<h3>æ™‚é–“å·®åˆ†å­¦ç¿’ã®åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢</h3>

<p><strong>æ™‚é–“å·®åˆ†ï¼ˆTemporal Difference: TDï¼‰å­¦ç¿’</strong>ã¯ã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ã‚’å¾…ãŸãšã«ã€<strong>1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ä¾¡å€¤é–¢æ•°ã‚’æ›´æ–°</strong>ã—ã¾ã™ï¼š</p>

$$
V(s_t) \leftarrow V(s_t) + \alpha [r_{t+1} + \gamma V(s_{t+1}) - V(s_t)]
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$r_{t+1} + \gamma V(s_{t+1})$ï¼š<strong>TDç›®æ¨™</strong>ï¼ˆTD targetï¼‰</li>
<li>$\delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)$ï¼š<strong>TDèª¤å·®</strong>ï¼ˆTD errorï¼‰</li>
<li>$\alpha$ï¼šå­¦ç¿’ç‡ï¼ˆlearning rateï¼‰</li>
</ul>

<div class="mermaid">
graph LR
    S1["çŠ¶æ…‹ s_t"] --> A["è¡Œå‹• a_t"]
    A --> S2["çŠ¶æ…‹ s_t+1"]
    S2 --> R["å ±é…¬ r_t+1"]
    R --> Update["V(s_t) æ›´æ–°"]
    S2 --> Update

    style S1 fill:#b3e5fc
    style S2 fill:#c5e1a5
    style R fill:#fff9c4
    style Update fill:#ffab91
</div>

<h3>TD(0)ã®å®Ÿè£…</h3>

<pre><code class="language-python">import numpy as np
import gym

def td_0_prediction(env, policy, num_episodes=1000, alpha=0.1, gamma=0.99):
    """
    TD(0)ã«ã‚ˆã‚‹çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ã®æ¨å®š

    Args:
        env: ç’°å¢ƒ
        policy: æ–¹ç­– (çŠ¶æ…‹ -> è¡Œå‹•ç¢ºç‡åˆ†å¸ƒ)
        num_episodes: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°
        alpha: å­¦ç¿’ç‡
        gamma: å‰²å¼•ç‡

    Returns:
        V: çŠ¶æ…‹ä¾¡å€¤é–¢æ•°
    """
    # çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ã®åˆæœŸåŒ–
    V = np.zeros(env.observation_space.n)

    for episode in range(num_episodes):
        state, _ = env.reset()

        while True:
            # æ–¹ç­–ã«å¾“ã£ã¦è¡Œå‹•é¸æŠ
            action = np.random.choice(env.action_space.n, p=policy[state])

            # ç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            # TD(0)æ›´æ–°
            td_target = reward + gamma * V[next_state]
            td_error = td_target - V[state]
            V[state] = V[state] + alpha * td_error

            if done:
                break

            state = next_state

    return V


# ä½¿ç”¨ä¾‹ï¼šFrozenLakeç’°å¢ƒ
print("=== TD(0)ã«ã‚ˆã‚‹ä¾¡å€¤é–¢æ•°æ¨å®š ===")

env = gym.make('FrozenLake-v1', is_slippery=False)

# ãƒ©ãƒ³ãƒ€ãƒ æ–¹ç­–
policy = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n

# TD(0)å®Ÿè¡Œ
V = td_0_prediction(env, policy, num_episodes=1000, alpha=0.1, gamma=0.99)

print(f"çŠ¶æ…‹ä¾¡å€¤é–¢æ•°:\n{V.reshape(4, 4)}")
env.close()
</code></pre>

<h3>ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã¨TDå­¦ç¿’ã®æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•</th>
<th>TDå­¦ç¿’</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ›´æ–°ã‚¿ã‚¤ãƒŸãƒ³ã‚°</strong></td>
<td>ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å¾Œ</td>
<td>å„ã‚¹ãƒ†ãƒƒãƒ—å¾Œ</td>
</tr>
<tr>
<td><strong>åç›Šã®è¨ˆç®—</strong></td>
<td>å®Ÿéš›ã®åç›Š $G_t$</td>
<td>æ¨å®šåç›Š $r + \gamma V(s')$</td>
</tr>
<tr>
<td><strong>ãƒã‚¤ã‚¢ã‚¹</strong></td>
<td>ãªã—ï¼ˆä¸åæ¨å®šï¼‰</td>
<td>ã‚ã‚Šï¼ˆåˆæœŸå€¤ã«ä¾å­˜ï¼‰</td>
</tr>
<tr>
<td><strong>åˆ†æ•£</strong></td>
<td>é«˜ã„</td>
<td>ä½ã„</td>
</tr>
<tr>
<td><strong>ç¶™ç¶šã‚¿ã‚¹ã‚¯</strong></td>
<td>é©ç”¨ä¸å¯</td>
<td>é©ç”¨å¯èƒ½</td>
</tr>
<tr>
<td><strong>åæŸé€Ÿåº¦</strong></td>
<td>é…ã„</td>
<td>é€Ÿã„</td>
</tr>
</tbody>
</table>

<blockquote>
<p>ã€ŒTDå­¦ç¿’ã¯ã€ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ï¼ˆè‡ªå·±ã®æ¨å®šå€¤ã‚’ä½¿ã£ã¦æ›´æ–°ï¼‰ã«ã‚ˆã‚Šã€åŠ¹ç‡çš„ãªå­¦ç¿’ã‚’å®Ÿç¾ã—ã¾ã™ã€</p>
</blockquote>

<hr>

<h2>2.2 Qå­¦ç¿’ï¼ˆQ-Learningï¼‰</h2>

<h3>è¡Œå‹•ä¾¡å€¤é–¢æ•°Q(s, a)</h3>

<p>çŠ¶æ…‹ä¾¡å€¤é–¢æ•° $V(s)$ ã®ä»£ã‚ã‚Šã«ã€<strong>è¡Œå‹•ä¾¡å€¤é–¢æ•°</strong> $Q(s, a)$ ã‚’å­¦ç¿’ã—ã¾ã™ï¼š</p>

$$
Q(s, a) = \mathbb{E}[G_t | S_t = s, A_t = a]
$$

<p>ã“ã‚Œã¯ã€ŒçŠ¶æ…‹ $s$ ã§è¡Œå‹• $a$ ã‚’å–ã£ãŸå¾Œã®æœŸå¾…åç›Šã€ã‚’è¡¨ã—ã¾ã™ã€‚</p>

<h3>Qå­¦ç¿’ã®æ›´æ–°å¼</h3>

<p><strong>Qå­¦ç¿’</strong>ã¯ã€TDå­¦ç¿’ã‚’è¡Œå‹•ä¾¡å€¤é–¢æ•°ã«é©ç”¨ã—ãŸã‚‚ã®ã§ã™ï¼š</p>

$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right]
$$

<p>é‡è¦ãªãƒã‚¤ãƒ³ãƒˆï¼š</p>
<ul>
<li>$\max_{a'} Q(s_{t+1}, a')$ï¼šæ¬¡çŠ¶æ…‹ã§ã®<strong>æœ€è‰¯ã®è¡Œå‹•</strong>ã®ä¾¡å€¤ã‚’ä½¿ã†</li>
<li><strong>ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼å‹</strong>ï¼šå®Ÿéš›ã®è¡Œå‹•ã¨æ›´æ–°ã«ä½¿ã†è¡Œå‹•ãŒç•°ãªã‚‹</li>
<li>æœ€é©æ–¹ç­–ã‚’ç›´æ¥å­¦ç¿’ã§ãã‚‹</li>
</ul>

<div class="mermaid">
graph TB
    Start["çŠ¶æ…‹ s, è¡Œå‹• a"] --> Execute["ç’°å¢ƒå®Ÿè¡Œ"]
    Execute --> Observe["s', r è¦³æ¸¬"]
    Observe --> MaxQ["max_a' Q(s', a')"]
    MaxQ --> Target["TDç›®æ¨™ = r + Î³ max Q(s', a')"]
    Target --> Update["Q(s,a) æ›´æ–°"]
    Update --> Next["æ¬¡ã‚¹ãƒ†ãƒƒãƒ—"]

    style Start fill:#b3e5fc
    style MaxQ fill:#fff59d
    style Update fill:#ffab91
</div>

<h3>Qå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè£…</h3>

<pre><code class="language-python">import numpy as np
import gym

class QLearningAgent:
    """Qå­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""

    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):
        """
        Args:
            n_states: çŠ¶æ…‹æ•°
            n_actions: è¡Œå‹•æ•°
            alpha: å­¦ç¿’ç‡
            gamma: å‰²å¼•ç‡
            epsilon: Îµ-greedy ã®Îµ
        """
        self.Q = np.zeros((n_states, n_actions))
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.n_actions = n_actions

    def select_action(self, state):
        """Îµ-greedyæ–¹ç­–ã§è¡Œå‹•é¸æŠ"""
        if np.random.rand() < self.epsilon:
            # ãƒ©ãƒ³ãƒ€ãƒ è¡Œå‹•ï¼ˆæ¢ç´¢ï¼‰
            return np.random.randint(self.n_actions)
        else:
            # æœ€è‰¯è¡Œå‹•ï¼ˆæ´»ç”¨ï¼‰
            return np.argmax(self.Q[state])

    def update(self, state, action, reward, next_state, done):
        """Qå€¤ã®æ›´æ–°"""
        if done:
            # çµ‚ç«¯çŠ¶æ…‹
            td_target = reward
        else:
            # Qå­¦ç¿’ã®æ›´æ–°å¼
            td_target = reward + self.gamma * np.max(self.Q[next_state])

        td_error = td_target - self.Q[state, action]
        self.Q[state, action] += self.alpha * td_error


def train_q_learning(env, agent, num_episodes=1000):
    """Qå­¦ç¿’ã®è¨“ç·´"""
    episode_rewards = []

    for episode in range(num_episodes):
        state, _ = env.reset()
        total_reward = 0

        while True:
            # è¡Œå‹•é¸æŠ
            action = agent.select_action(state)

            # ç’°å¢ƒå®Ÿè¡Œ
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            # Qå€¤æ›´æ–°
            agent.update(state, action, reward, next_state, done)

            total_reward += reward

            if done:
                break

            state = next_state

        episode_rewards.append(total_reward)

        # é€²æ—è¡¨ç¤º
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(episode_rewards[-100:])
            print(f"Episode {episode + 1}, Avg Reward: {avg_reward:.2f}")

    return episode_rewards


# ä½¿ç”¨ä¾‹ï¼šFrozenLake
print("\n=== Qå­¦ç¿’ã®è¨“ç·´ ===")

env = gym.make('FrozenLake-v1', is_slippery=False)

agent = QLearningAgent(
    n_states=env.observation_space.n,
    n_actions=env.action_space.n,
    alpha=0.1,
    gamma=0.99,
    epsilon=0.1
)

rewards = train_q_learning(env, agent, num_episodes=1000)

print(f"\nå­¦ç¿’æ¸ˆã¿Qè¡¨ï¼ˆä¸€éƒ¨ï¼‰:")
print(agent.Q[:16].reshape(4, 4, -1)[:, :, 0])  # è¡Œå‹•0ã®Qå€¤
env.close()
</code></pre>

<h3>Qè¡¨ï¼ˆQ-Tableï¼‰ã®å¯è¦–åŒ–</h3>

<pre><code class="language-python">import matplotlib.pyplot as plt
import seaborn as sns

def visualize_q_table(Q, env_shape=(4, 4)):
    """Qè¡¨ã‚’å¯è¦–åŒ–"""
    n_states = Q.shape[0]
    n_actions = Q.shape[1]

    fig, axes = plt.subplots(1, n_actions, figsize=(16, 4))

    action_names = ['LEFT', 'DOWN', 'RIGHT', 'UP']

    for action in range(n_actions):
        Q_action = Q[:, action].reshape(env_shape)

        sns.heatmap(Q_action, annot=True, fmt='.2f', cmap='YlOrRd',
                   ax=axes[action], cbar=True, square=True)
        axes[action].set_title(f'Qå€¤: {action_names[action]}')
        axes[action].set_xlabel('åˆ—')
        axes[action].set_ylabel('è¡Œ')

    plt.tight_layout()
    plt.savefig('q_table_visualization.png', dpi=150, bbox_inches='tight')
    print("Qè¡¨ã‚’ä¿å­˜: q_table_visualization.png")
    plt.close()


# Qè¡¨ã®å¯è¦–åŒ–
visualize_q_table(agent.Q)
</code></pre>

<h3>å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–</h3>

<pre><code class="language-python">import matplotlib.pyplot as plt
import numpy as np

def plot_learning_curve(rewards, window=100):
    """å­¦ç¿’æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
    # ç§»å‹•å¹³å‡ã‚’è¨ˆç®—
    smoothed_rewards = np.convolve(rewards, np.ones(window)/window, mode='valid')

    plt.figure(figsize=(12, 5))

    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å ±é…¬
    plt.subplot(1, 2, 1)
    plt.plot(rewards, alpha=0.3, label='ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å ±é…¬')
    plt.plot(range(window-1, len(rewards)), smoothed_rewards,
             linewidth=2, label=f'{window}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç§»å‹•å¹³å‡')
    plt.xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰')
    plt.ylabel('å ±é…¬')
    plt.title('Qå­¦ç¿’ã®å­¦ç¿’æ›²ç·š')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # ç´¯ç©å ±é…¬
    plt.subplot(1, 2, 2)
    cumulative_rewards = np.cumsum(rewards)
    plt.plot(cumulative_rewards, linewidth=2, color='green')
    plt.xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰')
    plt.ylabel('ç´¯ç©å ±é…¬')
    plt.title('ç´¯ç©å ±é…¬')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('q_learning_curve.png', dpi=150, bbox_inches='tight')
    print("å­¦ç¿’æ›²ç·šã‚’ä¿å­˜: q_learning_curve.png")
    plt.close()


plot_learning_curve(rewards)
</code></pre>

<hr>

<h2>2.3 SARSAï¼ˆState-Action-Reward-State-Actionï¼‰</h2>

<h3>SARSAã®åŸºæœ¬åŸç†</h3>

<p><strong>SARSA</strong>ã¯ã€Qå­¦ç¿’ã®<strong>ã‚ªãƒ³ãƒãƒªã‚·ãƒ¼ç‰ˆ</strong>ã§ã™ã€‚å®Ÿéš›ã«å–ã‚‹è¡Œå‹•ã‚’ä½¿ã£ã¦æ›´æ–°ã—ã¾ã™ï¼š</p>

$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
$$

<p>é‡è¦ãªé•ã„ï¼š</p>
<ul>
<li>Qå­¦ç¿’ï¼š$\max_{a'} Q(s_{t+1}, a')$ ã‚’ä½¿ã†ï¼ˆæœ€è‰¯ã®è¡Œå‹•ï¼‰</li>
<li>SARSAï¼š$Q(s_{t+1}, a_{t+1})$ ã‚’ä½¿ã†ï¼ˆå®Ÿéš›ã«å–ã‚‹è¡Œå‹•ï¼‰</li>
</ul>

<div class="mermaid">
graph LR
    S1["S_t"] --> A1["A_t"]
    A1 --> R["R_t+1"]
    R --> S2["S_t+1"]
    S2 --> A2["A_t+1"]
    A2 --> Update["Q(S_t, A_t) æ›´æ–°"]

    style S1 fill:#b3e5fc
    style A1 fill:#c5e1a5
    style R fill:#fff9c4
    style S2 fill:#b3e5fc
    style A2 fill:#c5e1a5
    style Update fill:#ffab91
</div>

<h3>Qå­¦ç¿’ã¨SARSAã®æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>Qå­¦ç¿’</th>
<th>SARSA</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å­¦ç¿’ã‚¿ã‚¤ãƒ—</strong></td>
<td>ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼</td>
<td>ã‚ªãƒ³ãƒãƒªã‚·ãƒ¼</td>
</tr>
<tr>
<td><strong>æ›´æ–°å¼</strong></td>
<td>$r + \gamma \max_a Q(s', a)$</td>
<td>$r + \gamma Q(s', a')$</td>
</tr>
<tr>
<td><strong>æ¢ç´¢ã®å½±éŸ¿</strong></td>
<td>å­¦ç¿’ã«å½±éŸ¿ã—ãªã„</td>
<td>å­¦ç¿’ã«å½±éŸ¿ã™ã‚‹</td>
</tr>
<tr>
<td><strong>åæŸå…ˆ</strong></td>
<td>æœ€é©æ–¹ç­–</td>
<td>ç¾åœ¨ã®æ–¹ç­–ã®ä¾¡å€¤</td>
</tr>
<tr>
<td><strong>å®‰å…¨æ€§</strong></td>
<td>ãƒªã‚¹ã‚¯ã‚’è€ƒæ…®ã—ãªã„</td>
<td>ãƒªã‚¹ã‚¯ã‚’è€ƒæ…®</td>
</tr>
<tr>
<td><strong>é©ç”¨å ´é¢</strong></td>
<td>ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒ</td>
<td>å®Ÿç’°å¢ƒã§ã®å­¦ç¿’</td>
</tr>
</tbody>
</table>

<h3>SARSAã®å®Ÿè£…</h3>

<pre><code class="language-python">import numpy as np
import gym

class SARSAAgent:
    """SARSAã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""

    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):
        """
        Args:
            n_states: çŠ¶æ…‹æ•°
            n_actions: è¡Œå‹•æ•°
            alpha: å­¦ç¿’ç‡
            gamma: å‰²å¼•ç‡
            epsilon: Îµ-greedy ã®Îµ
        """
        self.Q = np.zeros((n_states, n_actions))
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.n_actions = n_actions

    def select_action(self, state):
        """Îµ-greedyæ–¹ç­–ã§è¡Œå‹•é¸æŠ"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.n_actions)
        else:
            return np.argmax(self.Q[state])

    def update(self, state, action, reward, next_state, next_action, done):
        """Qå€¤ã®æ›´æ–°ï¼ˆSARSAï¼‰"""
        if done:
            td_target = reward
        else:
            # SARSAã®æ›´æ–°å¼ï¼ˆæ¬¡ã«å®Ÿéš›ã«å–ã‚‹è¡Œå‹•ã‚’ä½¿ç”¨ï¼‰
            td_target = reward + self.gamma * self.Q[next_state, next_action]

        td_error = td_target - self.Q[state, action]
        self.Q[state, action] += self.alpha * td_error


def train_sarsa(env, agent, num_episodes=1000):
    """SARSAã®è¨“ç·´"""
    episode_rewards = []

    for episode in range(num_episodes):
        state, _ = env.reset()
        action = agent.select_action(state)  # åˆæœŸè¡Œå‹•é¸æŠ
        total_reward = 0

        while True:
            # ç’°å¢ƒå®Ÿè¡Œ
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            if not done:
                # æ¬¡ã®è¡Œå‹•ã‚’é¸æŠï¼ˆSARSAã®ç‰¹å¾´ï¼‰
                next_action = agent.select_action(next_state)
            else:
                next_action = None

            # Qå€¤æ›´æ–°
            agent.update(state, action, reward, next_state, next_action, done)

            total_reward += reward

            if done:
                break

            state = next_state
            action = next_action  # æ¬¡ã®è¡Œå‹•ã«ç§»è¡Œ

        episode_rewards.append(total_reward)

        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(episode_rewards[-100:])
            print(f"Episode {episode + 1}, Avg Reward: {avg_reward:.2f}")

    return episode_rewards


# ä½¿ç”¨ä¾‹
print("\n=== SARSAã®è¨“ç·´ ===")

env = gym.make('FrozenLake-v1', is_slippery=False)

sarsa_agent = SARSAAgent(
    n_states=env.observation_space.n,
    n_actions=env.action_space.n,
    alpha=0.1,
    gamma=0.99,
    epsilon=0.1
)

sarsa_rewards = train_sarsa(env, sarsa_agent, num_episodes=1000)

print(f"\nå­¦ç¿’æ¸ˆã¿Qè¡¨ï¼ˆSARSAï¼‰:")
print(sarsa_agent.Q[:16].reshape(4, 4, -1)[:, :, 0])
env.close()
</code></pre>

<hr>

<h2>2.4 Îµ-greedyæ¢ç´¢æˆ¦ç•¥</h2>

<h3>æ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•</h3>

<p>å¼·åŒ–å­¦ç¿’ã§ã¯ã€<strong>æ¢ç´¢ï¼ˆExplorationï¼‰</strong>ã¨<strong>æ´»ç”¨ï¼ˆExploitationï¼‰</strong>ã®ãƒãƒ©ãƒ³ã‚¹ãŒé‡è¦ã§ã™ï¼š</p>

<ul>
<li><strong>æ¢ç´¢</strong>ï¼šæ–°ã—ã„çŠ¶æ…‹ãƒ»è¡Œå‹•ã‚’è©¦ã—ã¦ç’°å¢ƒã‚’ç†è§£ã™ã‚‹</li>
<li><strong>æ´»ç”¨</strong>ï¼šç¾åœ¨ã®çŸ¥è­˜ã§æœ€è‰¯ã®è¡Œå‹•ã‚’é¸æŠã™ã‚‹</li>
</ul>

<h3>Îµ-greedyæ–¹ç­–</h3>

<p>æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªæ¢ç´¢æˆ¦ç•¥ï¼š</p>

$$
a = \begin{cases}
\text{random action} & \text{ç¢ºç‡ } \epsilon \\
\arg\max_a Q(s, a) & \text{ç¢ºç‡ } 1 - \epsilon
\end{cases}
$$

<h3>Îµã®æ¸›è¡°ï¼ˆEpsilon Decayï¼‰</h3>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

class EpsilonGreedy:
    """Îµ-greedyæ–¹ç­–ï¼ˆæ¸›è¡°æ©Ÿèƒ½ä»˜ãï¼‰"""

    def __init__(self, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):
        """
        Args:
            epsilon_start: åˆæœŸÎµ
            epsilon_end: æœ€å°Îµ
            epsilon_decay: æ¸›è¡°ç‡
        """
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay

    def select_action(self, Q, state, n_actions):
        """è¡Œå‹•é¸æŠ"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(n_actions)
        else:
            return np.argmax(Q[state])

    def decay(self):
        """Îµã‚’æ¸›è¡°"""
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)


# Îµã®æ¸›è¡°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å¯è¦–åŒ–
print("\n=== Îµæ¸›è¡°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯è¦–åŒ– ===")

fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# ç•°ãªã‚‹æ¸›è¡°ç‡
decay_rates = [0.99, 0.995, 0.999]

for i, decay_rate in enumerate(decay_rates):
    epsilon_greedy = EpsilonGreedy(epsilon_start=1.0, epsilon_end=0.01,
                                   epsilon_decay=decay_rate)
    epsilons = [epsilon_greedy.epsilon]

    for _ in range(1000):
        epsilon_greedy.decay()
        epsilons.append(epsilon_greedy.epsilon)

    axes[i].plot(epsilons, linewidth=2)
    axes[i].set_xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰')
    axes[i].set_ylabel('Îµ')
    axes[i].set_title(f'æ¸›è¡°ç‡ = {decay_rate}')
    axes[i].grid(True, alpha=0.3)
    axes[i].set_ylim([0, 1.1])

plt.tight_layout()
plt.savefig('epsilon_decay.png', dpi=150, bbox_inches='tight')
print("Îµæ¸›è¡°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¿å­˜: epsilon_decay.png")
plt.close()
</code></pre>

<h3>ä»–ã®æ¢ç´¢æˆ¦ç•¥</h3>

<h4>ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ï¼ˆBoltzmannï¼‰æ¢ç´¢</h4>

<p>è¡Œå‹•ã®ä¾¡å€¤ã«åŸºã¥ã„ã¦ç¢ºç‡çš„ã«é¸æŠï¼š</p>

$$
P(a | s) = \frac{\exp(Q(s,a) / \tau)}{\sum_{a'} \exp(Q(s,a') / \tau)}
$$

<p>$\tau$ ã¯æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé«˜ã„ã»ã©ãƒ©ãƒ³ãƒ€ãƒ ï¼‰</p>

<h4>Upper Confidence Bound (UCB)</h4>

<p>ä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®ã—ãŸæ¢ç´¢ï¼š</p>

$$
a = \arg\max_a \left[ Q(s,a) + c \sqrt{\frac{\ln t}{N(s,a)}} \right]
$$

<p>$N(s,a)$ ã¯è¡Œå‹• $a$ ã®é¸æŠå›æ•°ã€$c$ ã¯æ¢ç´¢ä¿‚æ•°</p>

<hr>

<h2>2.5 ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿</h2>

<h3>å­¦ç¿’ç‡ï¼ˆLearning Rateï¼‰Î±</h3>

<p>å­¦ç¿’ç‡ $\alpha$ ã¯æ›´æ–°ã®å¼·ã•ã‚’åˆ¶å¾¡ã—ã¾ã™ï¼š</p>

<ul>
<li><strong>å¤§ãã„Î±ï¼ˆä¾‹ï¼š0.5ï¼‰</strong>ï¼šé€Ÿãå­¦ç¿’ã™ã‚‹ãŒä¸å®‰å®š</li>
<li><strong>å°ã•ã„Î±ï¼ˆä¾‹ï¼š0.01ï¼‰</strong>ï¼šå®‰å®šã ãŒåæŸãŒé…ã„</li>
<li><strong>æ¨å¥¨å€¤</strong>ï¼š0.1 ã€œ 0.3</li>
</ul>

<h3>å‰²å¼•ç‡ï¼ˆDiscount Factorï¼‰Î³</h3>

<p>å‰²å¼•ç‡ $\gamma$ ã¯å°†æ¥ã®å ±é…¬ã®é‡è¦åº¦ã‚’æ±ºå®šï¼š</p>

<ul>
<li><strong>Î³ = 0</strong>ï¼šå³åº§ã®å ±é…¬ã®ã¿è€ƒæ…®ï¼ˆè¿‘è¦–çœ¼çš„ï¼‰</li>
<li><strong>Î³ â†’ 1</strong>ï¼šé ã„å°†æ¥ã¾ã§è€ƒæ…®ï¼ˆå…ˆè¦‹çš„ï¼‰</li>
<li><strong>æ¨å¥¨å€¤</strong>ï¼š0.95 ã€œ 0.99</li>
</ul>

<h3>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æŸ»ã®å®Ÿè£…</h3>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
import gym

def hyperparameter_search(env_name, param_name, param_values, num_episodes=500):
    """ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ã‚’èª¿æŸ»"""
    results = {}

    for value in param_values:
        print(f"\n{param_name} = {value} ã§è¨“ç·´ä¸­...")

        env = gym.make(env_name)

        if param_name == 'alpha':
            agent = QLearningAgent(env.observation_space.n, env.action_space.n,
                                  alpha=value, gamma=0.99, epsilon=0.1)
        elif param_name == 'gamma':
            agent = QLearningAgent(env.observation_space.n, env.action_space.n,
                                  alpha=0.1, gamma=value, epsilon=0.1)
        elif param_name == 'epsilon':
            agent = QLearningAgent(env.observation_space.n, env.action_space.n,
                                  alpha=0.1, gamma=0.99, epsilon=value)

        rewards = train_q_learning(env, agent, num_episodes=num_episodes)
        results[value] = rewards
        env.close()

    return results


# å­¦ç¿’ç‡ã®å½±éŸ¿èª¿æŸ»
print("=== å­¦ç¿’ç‡Î±ã®å½±éŸ¿èª¿æŸ» ===")

alpha_values = [0.01, 0.05, 0.1, 0.3, 0.5]
alpha_results = hyperparameter_search('FrozenLake-v1', 'alpha', alpha_values)

# å¯è¦–åŒ–
plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
for alpha, rewards in alpha_results.items():
    smoothed = np.convolve(rewards, np.ones(50)/50, mode='valid')
    plt.plot(smoothed, label=f'Î± = {alpha}', linewidth=2)

plt.xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰')
plt.ylabel('å¹³å‡å ±é…¬')
plt.title('å­¦ç¿’ç‡Î±ã®å½±éŸ¿')
plt.legend()
plt.grid(True, alpha=0.3)

# å‰²å¼•ç‡ã®å½±éŸ¿èª¿æŸ»
gamma_values = [0.5, 0.9, 0.95, 0.99, 0.999]
gamma_results = hyperparameter_search('FrozenLake-v1', 'gamma', gamma_values)

plt.subplot(1, 2, 2)
for gamma, rewards in gamma_results.items():
    smoothed = np.convolve(rewards, np.ones(50)/50, mode='valid')
    plt.plot(smoothed, label=f'Î³ = {gamma}', linewidth=2)

plt.xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰')
plt.ylabel('å¹³å‡å ±é…¬')
plt.title('å‰²å¼•ç‡Î³ã®å½±éŸ¿')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('hyperparameter_impact.png', dpi=150, bbox_inches='tight')
print("\nãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ã‚’ä¿å­˜: hyperparameter_impact.png")
plt.close()
</code></pre>

<hr>

<h2>2.6 å®Ÿè·µï¼šTaxi-v3ç’°å¢ƒ</h2>

<h3>Taxi-v3ç’°å¢ƒã®æ¦‚è¦</h3>

<p><strong>Taxi-v3</strong>ã¯ã€ã‚¿ã‚¯ã‚·ãƒ¼ãŒä¹—å®¢ã‚’ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ã¦ç›®çš„åœ°ã¾ã§é€ã‚‹ç’°å¢ƒã§ã™ï¼š</p>

<ul>
<li><strong>çŠ¶æ…‹ç©ºé–“</strong>ï¼š500çŠ¶æ…‹ï¼ˆ5Ã—5ã‚°ãƒªãƒƒãƒ‰ Ã— 5ä¹—å®¢ä½ç½® Ã— 4ç›®çš„åœ°ï¼‰</li>
<li><strong>è¡Œå‹•ç©ºé–“</strong>ï¼š6è¡Œå‹•ï¼ˆä¸Šä¸‹å·¦å³ã€ä¹—å®¢ã®ä¹—é™ï¼‰</li>
<li><strong>å ±é…¬</strong>ï¼šæ­£ã—ã„ç›®çš„åœ°ã«åˆ°ç€ +20ã€1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ -1ã€ä¸æ­£ãªä¹—é™ -10</li>
</ul>

<h3>Taxi-v3ã§ã®Qå­¦ç¿’</h3>

<pre><code class="language-python">import numpy as np
import gym
import matplotlib.pyplot as plt

# Taxi-v3ç’°å¢ƒ
print("=== Taxi-v3ç’°å¢ƒã§ã®Qå­¦ç¿’ ===")

env = gym.make('Taxi-v3', render_mode=None)

print(f"çŠ¶æ…‹ç©ºé–“: {env.observation_space.n}")
print(f"è¡Œå‹•ç©ºé–“: {env.action_space.n}")
print(f"è¡Œå‹•: {['å—', 'åŒ—', 'æ±', 'è¥¿', 'ä¹—è»Š', 'é™è»Š']}")

# Qå­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
taxi_agent = QLearningAgent(
    n_states=env.observation_space.n,
    n_actions=env.action_space.n,
    alpha=0.1,
    gamma=0.99,
    epsilon=0.1
)

# è¨“ç·´
taxi_rewards = train_q_learning(env, taxi_agent, num_episodes=5000)

# å­¦ç¿’æ›²ç·š
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
smoothed = np.convolve(taxi_rewards, np.ones(100)/100, mode='valid')
plt.plot(smoothed, linewidth=2, color='blue')
plt.xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰')
plt.ylabel('å¹³å‡å ±é…¬')
plt.title('Taxi-v3 Qå­¦ç¿’ã®å­¦ç¿’æ›²ç·š')
plt.grid(True, alpha=0.3)

# æˆåŠŸç‡ã®è¨ˆç®—
success_rate = []
window = 100
for i in range(len(taxi_rewards) - window):
    success = np.sum(np.array(taxi_rewards[i:i+window]) > 0) / window
    success_rate.append(success)

plt.subplot(1, 2, 2)
plt.plot(success_rate, linewidth=2, color='green')
plt.xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰')
plt.ylabel('æˆåŠŸç‡')
plt.title('ã‚¿ã‚¹ã‚¯æˆåŠŸç‡ï¼ˆ100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç§»å‹•å¹³å‡ï¼‰')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('taxi_training.png', dpi=150, bbox_inches='tight')
print("Taxiè¨“ç·´çµæœã‚’ä¿å­˜: taxi_training.png")
plt.close()

env.close()
</code></pre>

<h3>å­¦ç¿’æ¸ˆã¿ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è©•ä¾¡</h3>

<pre><code class="language-python">def evaluate_agent(env, agent, num_episodes=100, render=False):
    """å­¦ç¿’æ¸ˆã¿ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è©•ä¾¡"""
    total_rewards = []
    total_steps = []

    for episode in range(num_episodes):
        state, _ = env.reset()
        episode_reward = 0
        steps = 0

        while steps < 200:  # æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°
            # æœ€è‰¯ã®è¡Œå‹•ã‚’é¸æŠï¼ˆæ¢ç´¢ãªã—ï¼‰
            action = np.argmax(agent.Q[state])

            state, reward, terminated, truncated, _ = env.step(action)
            episode_reward += reward
            steps += 1

            if terminated or truncated:
                break

        total_rewards.append(episode_reward)
        total_steps.append(steps)

    return total_rewards, total_steps


# è©•ä¾¡
print("\n=== å­¦ç¿’æ¸ˆã¿ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è©•ä¾¡ ===")

env = gym.make('Taxi-v3', render_mode=None)
eval_rewards, eval_steps = evaluate_agent(env, taxi_agent, num_episodes=100)

print(f"å¹³å‡å ±é…¬: {np.mean(eval_rewards):.2f} Â± {np.std(eval_rewards):.2f}")
print(f"å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ•°: {np.mean(eval_steps):.2f} Â± {np.std(eval_steps):.2f}")
print(f"æˆåŠŸç‡: {np.sum(np.array(eval_rewards) > 0) / len(eval_rewards) * 100:.1f}%")

env.close()
</code></pre>

<hr>

<h2>2.7 å®Ÿè·µï¼šCliff Walkingç’°å¢ƒ</h2>

<h3>Cliff Walkingç’°å¢ƒã®å®šç¾©</h3>

<p><strong>Cliff Walking</strong>ã¯ã€å´–ã‚’é¿ã‘ã¦ã‚´ãƒ¼ãƒ«ã«åˆ°é”ã™ã‚‹ç’°å¢ƒã§ã™ã€‚Qå­¦ç¿’ã¨SARSAã®é•ã„ã‚’æ˜ç¢ºã«ç¤ºã™ä¾‹ã§ã™ï¼š</p>

<ul>
<li><strong>4Ã—12ã‚°ãƒªãƒƒãƒ‰</strong>ï¼šå·¦ä¸‹ãŒã‚¹ã‚¿ãƒ¼ãƒˆã€å³ä¸‹ãŒã‚´ãƒ¼ãƒ«</li>
<li><strong>å´–ã‚¨ãƒªã‚¢</strong>ï¼šä¸‹ç«¯ã®ä¸­å¤®éƒ¨åˆ†ï¼ˆè¸ã‚€ã¨ -100 ã®ç½°å‰‡ï¼‰</li>
<li><strong>å ±é…¬</strong>ï¼šå„ã‚¹ãƒ†ãƒƒãƒ— -1ã€å´– -100ã€ã‚´ãƒ¼ãƒ« 0</li>
</ul>

<h3>Cliff Walkingç’°å¢ƒã®å®Ÿè£…</h3>

<pre><code class="language-python">import numpy as np
import gym

# Cliff Walkingç’°å¢ƒ
print("=== Cliff Walkingç’°å¢ƒ ===")

env = gym.make('CliffWalking-v0')

print(f"çŠ¶æ…‹ç©ºé–“: {env.observation_space.n}")
print(f"è¡Œå‹•ç©ºé–“: {env.action_space.n}")
print(f"ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º: 4Ã—12")

# Qå­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
cliff_q_agent = QLearningAgent(
    n_states=env.observation_space.n,
    n_actions=env.action_space.n,
    alpha=0.5,
    gamma=0.99,
    epsilon=0.1
)

# SARSAã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
cliff_sarsa_agent = SARSAAgent(
    n_states=env.observation_space.n,
    n_actions=env.action_space.n,
    alpha=0.5,
    gamma=0.99,
    epsilon=0.1
)

# è¨“ç·´
print("\nQå­¦ç¿’ã§è¨“ç·´ä¸­...")
q_rewards = train_q_learning(env, cliff_q_agent, num_episodes=500)

env = gym.make('CliffWalking-v0')
print("\nSARSAã§è¨“ç·´ä¸­...")
sarsa_rewards = train_sarsa(env, cliff_sarsa_agent, num_episodes=500)

# æ¯”è¼ƒå¯è¦–åŒ–
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
smoothed_q = np.convolve(q_rewards, np.ones(10)/10, mode='valid')
smoothed_sarsa = np.convolve(sarsa_rewards, np.ones(10)/10, mode='valid')

plt.plot(smoothed_q, label='Qå­¦ç¿’', linewidth=2)
plt.plot(smoothed_sarsa, label='SARSA', linewidth=2)
plt.xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰')
plt.ylabel('å ±é…¬')
plt.title('Cliff Walking: Qå­¦ç¿’ vs SARSA')
plt.legend()
plt.grid(True, alpha=0.3)

# æ–¹ç­–ã®å¯è¦–åŒ–ï¼ˆçŸ¢å°ã§è¡¨ç¤ºï¼‰
plt.subplot(1, 2, 2)

def visualize_policy(Q, shape=(4, 12)):
    """å­¦ç¿’ã—ãŸæ–¹ç­–ã‚’å¯è¦–åŒ–"""
    policy = np.argmax(Q, axis=1)
    policy_grid = policy.reshape(shape)

    # çŸ¢å°ã®æ–¹å‘
    arrows = {0: 'â†‘', 1: 'â†’', 2: 'â†“', 3: 'â†'}

    fig, ax = plt.subplots(figsize=(12, 4))
    ax.imshow(np.zeros(shape), cmap='Blues', alpha=0.3)

    for i in range(shape[0]):
        for j in range(shape[1]):
            state = i * shape[1] + j
            action = policy[state]

            # å´–ã‚¨ãƒªã‚¢ã‚’èµ¤ãè¡¨ç¤º
            if i == 3 and 1 <= j <= 10:
                ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1,
                                          fill=True, color='red', alpha=0.3))

            # ã‚´ãƒ¼ãƒ«
            if i == 3 and j == 11:
                ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1,
                                          fill=True, color='green', alpha=0.3))

            # çŸ¢å°
            ax.text(j, i, arrows[action], ha='center', va='center',
                   fontsize=16, fontweight='bold')

    ax.set_xlim(-0.5, shape[1]-0.5)
    ax.set_ylim(shape[0]-0.5, -0.5)
    ax.set_xticks(range(shape[1]))
    ax.set_yticks(range(shape[0]))
    ax.grid(True)
    ax.set_title('å­¦ç¿’ã—ãŸæ–¹ç­–ï¼ˆQå­¦ç¿’ï¼‰')


visualize_policy(cliff_q_agent.Q)

plt.tight_layout()
plt.savefig('cliff_walking_comparison.png', dpi=150, bbox_inches='tight')
print("\nCliff Walkingæ¯”è¼ƒã‚’ä¿å­˜: cliff_walking_comparison.png")
plt.close()

env.close()
</code></pre>

<h3>Qå­¦ç¿’ã¨SARSAã®çµŒè·¯ã®é•ã„</h3>

<blockquote>
<p><strong>é‡è¦ãªè¦³å¯Ÿ</strong>ï¼šCliff Walkingã§ã¯ã€Qå­¦ç¿’ã¯<strong>æœ€çŸ­çµŒè·¯ï¼ˆå´–ã®è¿‘ãï¼‰</strong>ã‚’å­¦ç¿’ã—ã¾ã™ãŒã€SARSAã¯<strong>å®‰å…¨ãªçµŒè·¯ï¼ˆå´–ã‹ã‚‰é›¢ã‚Œã‚‹ï¼‰</strong>ã‚’å­¦ç¿’ã—ã¾ã™ã€‚ã“ã‚Œã¯Îµ-greedyæ¢ç´¢ã«ã‚ˆã‚‹å¶ç™ºçš„ãªå´–ã¸ã®è»¢è½ã‚’SARSAãŒå­¦ç¿’ã«åæ˜ ã™ã‚‹ãŸã‚ã§ã™ã€‚</p>
</blockquote>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<details>
<summary><strong>æ¼”ç¿’1ï¼šQå­¦ç¿’ã¨SARSAã®åæŸé€Ÿåº¦æ¯”è¼ƒ</strong></summary>

<p>FrozenLakeç’°å¢ƒã§Qå­¦ç¿’ã¨SARSAã‚’åŒã˜ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§è¨“ç·´ã—ã€åæŸé€Ÿåº¦ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import gym
import numpy as np

# TODO: Qå­¦ç¿’ã¨SARSAã‚’åŒã˜è¨­å®šã§è¨“ç·´
# TODO: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ã®å ±é…¬ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
# TODO: åæŸã«å¿…è¦ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã‚’æ¯”è¼ƒ
# æœŸå¾…: ç’°å¢ƒã«ã‚ˆã£ã¦åæŸé€Ÿåº¦ãŒç•°ãªã‚‹
</code></pre>

</details>

<details>
<summary><strong>æ¼”ç¿’2ï¼šÎµæ¸›è¡°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æœ€é©åŒ–</strong></summary>

<p>ç•°ãªã‚‹Îµæ¸›è¡°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆç·šå½¢æ¸›è¡°ã€æŒ‡æ•°æ¸›è¡°ã€ã‚¹ãƒ†ãƒƒãƒ—æ¸›è¡°ï¼‰ã‚’å®Ÿè£…ã—ã€Taxi-v3ã§ã®æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import numpy as np

# TODO: 3ç¨®é¡ã®Îµæ¸›è¡°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å®Ÿè£…
# TODO: Taxi-v3ã§å„ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è©•ä¾¡
# TODO: å­¦ç¿’æ›²ç·šã¨æœ€çµ‚æ€§èƒ½ã‚’æ¯”è¼ƒ
# ãƒ’ãƒ³ãƒˆ: åˆæœŸã¯æ¢ç´¢é‡è¦–ã€å¾ŒåŠã¯æ´»ç”¨é‡è¦–
</code></pre>

</details>

<details>
<summary><strong>æ¼”ç¿’3ï¼šDouble Q-Learning ã®å®Ÿè£…</strong></summary>

<p>éå¤§è©•ä¾¡ã‚’é˜²ãDouble Q-Learningã‚’å®Ÿè£…ã—ã€é€šå¸¸ã®Qå­¦ç¿’ã¨æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import numpy as np

# TODO: 2ã¤ã®Qè¡¨ã‚’ä½¿ã†Double Q-Learningã‚’å®Ÿè£…
# TODO: FrozenLakeç’°å¢ƒã§è¨“ç·´
# TODO: Qå€¤ã®æ¨å®šèª¤å·®ã‚’é€šå¸¸ã®Qå­¦ç¿’ã¨æ¯”è¼ƒ
# ç†è«–: Doubleã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯éå¤§è©•ä¾¡ãƒã‚¤ã‚¢ã‚¹ã‚’è»½æ¸›
</code></pre>

</details>

<details>
<summary><strong>æ¼”ç¿’4ï¼šå­¦ç¿’ç‡ã®é©å¿œçš„èª¿æ•´</strong></summary>

<p>è¨ªå•å›æ•°ã«å¿œã˜ã¦å­¦ç¿’ç‡ã‚’èª¿æ•´ã™ã‚‹é©å¿œçš„å­¦ç¿’ç‡ã‚’å®Ÿè£…ã—ã€å›ºå®šå­¦ç¿’ç‡ã¨æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import numpy as np

# TODO: Î±(s,a) = 1 / (1 + N(s,a)) ã®é©å¿œçš„å­¦ç¿’ç‡ã‚’å®Ÿè£…
# TODO: å›ºå®šå­¦ç¿’ç‡ã¨æ€§èƒ½ã‚’æ¯”è¼ƒ
# TODO: å„çŠ¶æ…‹ã§ã®è¨ªå•å›æ•°ã‚’å¯è¦–åŒ–
# æœŸå¾…: é©å¿œçš„å­¦ç¿’ç‡ã§åæŸãŒå®‰å®šã™ã‚‹
</code></pre>

</details>

<details>
<summary><strong>æ¼”ç¿’5ï¼šç‹¬è‡ªç’°å¢ƒã§ã®å®Ÿé¨“</strong></summary>

<p>OpenAI Gymã®åˆ¥ã®ç’°å¢ƒï¼ˆCartPole-v1ã€MountainCar-v0ãªã©ï¼‰ã§çŠ¶æ…‹ã®é›¢æ•£åŒ–ã‚’è¡Œã„ã€Qå­¦ç¿’ã‚’é©ç”¨ã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import gym
import numpy as np

# TODO: é€£ç¶šçŠ¶æ…‹ç©ºé–“ã‚’é›¢æ•£åŒ–ã™ã‚‹é–¢æ•°ã‚’å®Ÿè£…
# TODO: é›¢æ•£åŒ–ã—ãŸCartPoleç’°å¢ƒã§Qå­¦ç¿’
# TODO: é›¢æ•£åŒ–ã®ç²’åº¦ã¨æ€§èƒ½ã®é–¢ä¿‚ã‚’èª¿æŸ»
# èª²é¡Œ: é€£ç¶šç©ºé–“ã®é©åˆ‡ãªé›¢æ•£åŒ–ãŒé‡è¦
</code></pre>

</details>

<hr>

<h2>ã¾ã¨ã‚</h2>

<p>ã“ã®ç« ã§ã¯ã€æ™‚é–“å·®åˆ†å­¦ç¿’ã«åŸºã¥ãQå­¦ç¿’ã¨SARSAã‚’å­¦ã³ã¾ã—ãŸã€‚</p>

<h3>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</h3>

<ul>
<li><strong>TDå­¦ç¿’</strong>ï¼šã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ã‚’å¾…ãŸãšã«ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«æ›´æ–°</li>
<li><strong>Qå­¦ç¿’</strong>ï¼šã‚ªãƒ•ãƒãƒªã‚·ãƒ¼å‹ã€æœ€é©æ–¹ç­–ã‚’ç›´æ¥å­¦ç¿’</li>
<li><strong>SARSA</strong>ï¼šã‚ªãƒ³ãƒãƒªã‚·ãƒ¼å‹ã€æ¢ç´¢ã®å½±éŸ¿ã‚’è€ƒæ…®ã—ãŸå­¦ç¿’</li>
<li><strong>Îµ-greedy</strong>ï¼šæ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹ã‚’åˆ¶å¾¡ã™ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªæ–¹ç­–</li>
<li><strong>å­¦ç¿’ç‡Î±</strong>ï¼šæ›´æ–°ã®å¼·ã•ã‚’åˆ¶å¾¡ï¼ˆ0.1ã€œ0.3ãŒæ¨å¥¨ï¼‰</li>
<li><strong>å‰²å¼•ç‡Î³</strong>ï¼šå°†æ¥ã®å ±é…¬ã®é‡è¦åº¦ï¼ˆ0.95ã€œ0.99ãŒæ¨å¥¨ï¼‰</li>
<li><strong>Qè¡¨</strong>ï¼šçŠ¶æ…‹-è¡Œå‹•ãƒšã‚¢ã”ã¨ã®ä¾¡å€¤ã‚’ä¿å­˜</li>
<li><strong>é©ç”¨å ´é¢</strong>ï¼šé›¢æ•£çŠ¶æ…‹ãƒ»è¡Œå‹•ç©ºé–“ã®ã‚¿ã‚¹ã‚¯</li>
</ul>

<h3>Qå­¦ç¿’ã¨SARSAã®ä½¿ã„åˆ†ã‘</h3>

<table>
<thead>
<tr>
<th>çŠ¶æ³</th>
<th>æ¨å¥¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒ</strong></td>
<td>Qå­¦ç¿’</td>
<td>æœ€é©æ–¹ç­–ã‚’åŠ¹ç‡çš„ã«å­¦ç¿’</td>
</tr>
<tr>
<td><strong>å®Ÿç’°å¢ƒãƒ»ãƒ­ãƒœãƒƒãƒˆ</strong></td>
<td>SARSA</td>
<td>å®‰å…¨ãªæ–¹ç­–ã‚’å­¦ç¿’</td>
</tr>
<tr>
<td><strong>å±é™ºãªçŠ¶æ…‹ãŒã‚ã‚‹</strong></td>
<td>SARSA</td>
<td>ãƒªã‚¹ã‚¯å›é¿ã®å‚¾å‘</td>
</tr>
<tr>
<td><strong>é«˜é€ŸãªåæŸãŒå¿…è¦</strong></td>
<td>Qå­¦ç¿’</td>
<td>ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ã§æŸ”è»Ÿ</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>

<p>æ¬¡ç« ã§ã¯ã€<strong>Deep Q-Network (DQN)</strong>ã«ã¤ã„ã¦å­¦ã³ã¾ã™ã€‚Qè¡¨ã§ã¯æ‰±ãˆãªã„å¤§è¦æ¨¡ãƒ»é€£ç¶šçŠ¶æ…‹ç©ºé–“ã«å¯¾ã—ã¦ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’è¿‘ä¼¼ã™ã‚‹æ‰‹æ³•ã€Experience Replayã€Target Networkã€Atariã‚²ãƒ¼ãƒ ã§ã®å¿œç”¨ãªã©ã‚’ç¿’å¾—ã—ã¾ã™ã€‚</p>

<div class="navigation">
    <a href="chapter1-reinforcement-learning-basics.html" class="nav-button">â† ç¬¬1ç« ï¼šå¼·åŒ–å­¦ç¿’ã®åŸºç¤</a>
    <a href="chapter3-dqn.html" class="nav-button">ç¬¬3ç« ï¼šDeep Q-Network â†’</a>
</div>

</main>


    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
    <p>&copy; 2024 AI Terakoya. All rights reserved.</p>
</footer>

</body>
</html>
