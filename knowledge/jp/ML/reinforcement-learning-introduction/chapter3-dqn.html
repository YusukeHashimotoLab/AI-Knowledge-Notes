<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
<meta content="ç¬¬3ç« ï¼šDeep Q-Network (DQN) - AI Terakoya" name="description"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬3ç« ï¼šDeep Q-Network (DQN) - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: var(--font-body); line-height: 1.7; color: var(--color-text); background-color: var(--color-bg); font-size: 16px; }
        header { background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; padding: var(--spacing-xl) var(--spacing-md); margin-bottom: var(--spacing-xl); box-shadow: var(--box-shadow); }
        .header-content { max-width: 900px; margin: 0 auto; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: var(--spacing-sm); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; opacity: 0.95; font-weight: 400; margin-bottom: var(--spacing-md); }
        .meta { display: flex; flex-wrap: wrap; gap: var(--spacing-md); font-size: 0.9rem; opacity: 0.9; }
        .meta-item { display: flex; align-items: center; gap: 0.3rem; }
        .container { max-width: 900px; margin: 0 auto; padding: 0 var(--spacing-md) var(--spacing-xl); }
        h2 { font-size: 1.75rem; color: var(--color-primary); margin-top: var(--spacing-xl); margin-bottom: var(--spacing-md); padding-bottom: var(--spacing-xs); border-bottom: 3px solid var(--color-accent); }
        h3 { font-size: 1.4rem; color: var(--color-primary); margin-top: var(--spacing-lg); margin-bottom: var(--spacing-sm); }
        h4 { font-size: 1.1rem; color: var(--color-primary-dark); margin-top: var(--spacing-md); margin-bottom: var(--spacing-sm); }
        p { margin-bottom: var(--spacing-md); color: var(--color-text); }
        a { color: var(--color-link); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--color-link-hover); text-decoration: underline; }
        ul, ol { margin-left: var(--spacing-lg); margin-bottom: var(--spacing-md); }
        li { margin-bottom: var(--spacing-xs); color: var(--color-text); }
        pre { background-color: var(--color-code-bg); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); overflow-x: auto; margin-bottom: var(--spacing-md); font-family: var(--font-mono); font-size: 0.9rem; line-height: 1.5; }
        code { font-family: var(--font-mono); font-size: 0.9em; background-color: var(--color-code-bg); padding: 0.2em 0.4em; border-radius: 3px; }
        pre code { background-color: transparent; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: var(--spacing-md); font-size: 0.95rem; }
        th, td { border: 1px solid var(--color-border); padding: var(--spacing-sm); text-align: left; }
        th { background-color: var(--color-bg-alt); font-weight: 600; color: var(--color-primary); }
        blockquote { border-left: 4px solid var(--color-accent); padding-left: var(--spacing-md); margin: var(--spacing-md) 0; color: var(--color-text-light); font-style: italic; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        .mermaid { text-align: center; margin: var(--spacing-lg) 0; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        details { background-color: var(--color-bg-alt); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); margin-bottom: var(--spacing-md); }
        summary { cursor: pointer; font-weight: 600; color: var(--color-primary); user-select: none; padding: var(--spacing-xs); margin: calc(-1 * var(--spacing-md)); padding: var(--spacing-md); border-radius: var(--border-radius); }
        summary:hover { background-color: rgba(123, 44, 191, 0.1); }
        details[open] summary { margin-bottom: var(--spacing-md); border-bottom: 1px solid var(--color-border); }
        .navigation { display: flex; justify-content: space-between; gap: var(--spacing-md); margin: var(--spacing-xl) 0; padding-top: var(--spacing-lg); border-top: 2px solid var(--color-border); }
        .nav-button { flex: 1; padding: var(--spacing-md); background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; border-radius: var(--border-radius); text-align: center; font-weight: 600; transition: transform 0.2s, box-shadow 0.2s; box-shadow: var(--box-shadow); }
        .nav-button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); text-decoration: none; }
        footer { margin-top: var(--spacing-xl); padding: var(--spacing-lg) var(--spacing-md); background-color: var(--color-bg-alt); border-top: 1px solid var(--color-border); text-align: center; font-size: 0.9rem; color: var(--color-text-light); }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }

        @media (max-width: 768px) { h1 { font-size: 1.5rem; } h2 { font-size: 1.4rem; } h3 { font-size: 1.2rem; } .meta { font-size: 0.85rem; } .navigation { flex-direction: column; } table { font-size: 0.85rem; } th, td { padding: var(--spacing-xs); } }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/reinforcement-learning-introduction/chapter3-dqn.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/reinforcement-learning-introduction/index.html">Reinforcement Learning</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬3ç« ï¼šDeep Q-Network (DQN)</h1>
            <p class="subtitle">Qå­¦ç¿’ã‹ã‚‰ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¸ - Experience Replayã€Target Networkã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ‹¡å¼µ</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 30-35åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ® å®Ÿè£…: CartPoleã€Atari</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… è¡¨å½¢å¼Qå­¦ç¿’ã®é™ç•Œã¨ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°é©ç”¨ã®å¿…è¦æ€§ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… DQNã®åŸºæœ¬ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆCNN for Atariï¼‰ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Experience Replayã®å½¹å‰²ã¨å®Ÿè£…æ–¹æ³•ã‚’ç¿’å¾—ã™ã‚‹</li>
<li>âœ… Target Networkã«ã‚ˆã‚‹å­¦ç¿’å®‰å®šåŒ–ã®ä»•çµ„ã¿ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Double DQNã¨Dueling DQNã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ”¹å–„ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… CartPoleç’°å¢ƒã§ã®DQNå­¦ç¿’ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Atari Pongç’°å¢ƒã§ã®ç”»åƒãƒ™ãƒ¼ã‚¹å¼·åŒ–å­¦ç¿’ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… DQNã®æ€§èƒ½è©•ä¾¡ã¨å­¦ç¿’æ›²ç·šã®åˆ†æãŒã§ãã‚‹</li>
</ul>

<hr>

<h2>3.1 Qå­¦ç¿’ã®é™ç•Œã¨DQNã®å¿…è¦æ€§</h2>

<h3>è¡¨å½¢å¼Qå­¦ç¿’ã®é™ç•Œ</h3>

<p>ç¬¬2ç« ã§å­¦ã‚“ã è¡¨å½¢å¼Qå­¦ç¿’ã¯ã€çŠ¶æ…‹ã¨è¡Œå‹•ãŒé›¢æ•£çš„ã‹ã¤å°‘æ•°ã®å ´åˆã«æœ‰åŠ¹ã§ã™ãŒã€ç¾å®Ÿçš„ãªå•é¡Œã§ã¯ä»¥ä¸‹ã®åˆ¶ç´„ãŒã‚ã‚Šã¾ã™ï¼š</p>

<blockquote>
<p>ã€ŒçŠ¶æ…‹ç©ºé–“ãŒå¤§ãã„ã€ã‚ã‚‹ã„ã¯é€£ç¶šçš„ãªå ´åˆã€å…¨ã¦ã®çŠ¶æ…‹-è¡Œå‹•ãƒšã‚¢ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã§ç®¡ç†ã™ã‚‹ã“ã¨ã¯è¨ˆç®—çš„ã«ä¸å¯èƒ½ã§ã‚ã‚‹ã€</p>
</blockquote>

<h3>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®å•é¡Œ</h3>

<table>
<thead>
<tr>
<th>ç’°å¢ƒ</th>
<th>çŠ¶æ…‹ç©ºé–“</th>
<th>è¡Œå‹•ç©ºé–“</th>
<th>Qãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚º</th>
<th>å®Ÿç¾å¯èƒ½æ€§</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>FrozenLake</strong></td>
<td>16</td>
<td>4</td>
<td>64</td>
<td>âœ… å¯èƒ½</td>
</tr>
<tr>
<td><strong>CartPole</strong></td>
<td>é€£ç¶šï¼ˆ4æ¬¡å…ƒï¼‰</td>
<td>2</td>
<td>ç„¡é™å¤§</td>
<td>âŒ é›¢æ•£åŒ–å¿…è¦</td>
</tr>
<tr>
<td><strong>Atariï¼ˆ84Ã—84 RGBï¼‰</strong></td>
<td>$256^{84 \times 84 \times 3}$</td>
<td>4-18</td>
<td>å¤©æ–‡å­¦çš„æ•°å­—</td>
<td>âŒ ä¸å¯èƒ½</td>
</tr>
<tr>
<td><strong>å›²ç¢ï¼ˆ19Ã—19ï¼‰</strong></td>
<td>$3^{361}$ â‰ˆ $10^{172}$</td>
<td>361</td>
<td>$10^{174}$</td>
<td>âŒ ä¸å¯èƒ½</td>
</tr>
</tbody>
</table>

<h3>DQNã«ã‚ˆã‚‹è§£æ±ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</h3>

<p><strong>Deep Q-Network (DQN)</strong>ã¯ã€Qé–¢æ•°ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§è¿‘ä¼¼ã™ã‚‹ã“ã¨ã§ã€é«˜æ¬¡å…ƒãƒ»é€£ç¶šçŠ¶æ…‹ç©ºé–“ã§ã®å­¦ç¿’ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚</p>

<div class="mermaid">
graph TB
    subgraph "è¡¨å½¢å¼Qå­¦ç¿’"
        S1[çŠ¶æ…‹ s1] --> Q1[Q-table]
        S2[çŠ¶æ…‹ s2] --> Q1
        S3[çŠ¶æ…‹ s3] --> Q1
        Q1 --> A1[Qå€¤]
    end

    subgraph "DQN"
        S4[çŠ¶æ…‹ s<br/>ç”»åƒãƒ»é€£ç¶šå€¤] --> NN[Q-Network<br/>Î¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿]
        NN --> A2[Qå€¤<br/>å…¨è¡Œå‹•åˆ†]
    end

    style Q1 fill:#fff3e0
    style NN fill:#e3f2fd
    style A2 fill:#e8f5e9
</div>

<h3>Qé–¢æ•°ã®è¿‘ä¼¼</h3>

<p>è¡¨å½¢å¼Qå­¦ç¿’ã§ã¯å„$(s, a)$ãƒšã‚¢ã”ã¨ã«Qå€¤ã‚’ä¿å­˜ã—ã¾ã™ãŒã€DQNã§ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«é–¢æ•°è¿‘ä¼¼ã—ã¾ã™ï¼š</p>

<p>$$
Q(s, a) \approx Q(s, a; \theta)
$$</p>

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$Q(s, a; \theta)$ï¼šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\theta$ã‚’æŒã¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</li>
<li>å…¥åŠ›ï¼šçŠ¶æ…‹$s$ï¼ˆç”»åƒã€ãƒ™ã‚¯ãƒˆãƒ«ãªã©ï¼‰</li>
<li>å‡ºåŠ›ï¼šå„è¡Œå‹•$a$ã«å¯¾ã™ã‚‹Qå€¤</li>
</ul>

<h3>ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®åˆ©ç‚¹</h3>

<ol>
<li><strong>æ±åŒ–èƒ½åŠ›</strong>ï¼šæœªçµŒé¨“ã®çŠ¶æ…‹ã«å¯¾ã—ã¦ã‚‚æ¨è«–å¯èƒ½</li>
<li><strong>ç‰¹å¾´æŠ½å‡º</strong>ï¼šCNNãªã©ã§è‡ªå‹•çš„ã«æœ‰ç”¨ãªç‰¹å¾´ã‚’å­¦ç¿’</li>
<li><strong>ãƒ¡ãƒ¢ãƒªåŠ¹ç‡</strong>ï¼šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° â‰ª çŠ¶æ…‹ç©ºé–“ã‚µã‚¤ã‚º</li>
<li><strong>é€£ç¶šçŠ¶æ…‹å¯¾å¿œ</strong>ï¼šé›¢æ•£åŒ–ä¸è¦ã§ç²¾åº¦ç¶­æŒ</li>
</ol>

<h3>ãƒŠã‚¤ãƒ¼ãƒ–ãªDQNã®å•é¡Œç‚¹</h3>

<p>ã—ã‹ã—ã€å˜ç´”ã«ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§Qå­¦ç¿’ã‚’è¡Œã†ã¨ä»¥ä¸‹ã®å•é¡ŒãŒç™ºç”Ÿã—ã¾ã™ï¼š</p>

<table>
<thead>
<tr>
<th>å•é¡Œ</th>
<th>åŸå› </th>
<th>è§£æ±ºç­–</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å­¦ç¿’ã®ä¸å®‰å®šæ€§</strong></td>
<td>ãƒ‡ãƒ¼ã‚¿ã®ç›¸é–¢æ€§</td>
<td>Experience Replay</td>
</tr>
<tr>
<td><strong>ç™ºæ•£ãƒ»æŒ¯å‹•</strong></td>
<td>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®éå®šå¸¸æ€§</td>
<td>Target Network</td>
</tr>
<tr>
<td><strong>éå¤§è©•ä¾¡</strong></td>
<td>Maxãƒã‚¤ã‚¢ã‚¹Qå€¤</td>
<td>Double DQN</td>
</tr>
<tr>
<td><strong>éåŠ¹ç‡ãªè¡¨ç¾</strong></td>
<td>ä¾¡å€¤ã¨å„ªä½æ€§ã®æ··åŒ</td>
<td>Dueling DQN</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.2 DQNã®åŸºæœ¬ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h2>

<h3>DQNã®å…¨ä½“æ§‹é€ </h3>

<p>DQNã¯ä»¥ä¸‹ã®3ã¤ã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§æ§‹æˆã•ã‚Œã¾ã™ï¼š</p>

<div class="mermaid">
graph LR
    ENV[ç’°å¢ƒ] -->|çŠ¶æ…‹ s| QN[Q-Network]
    QN -->|Qå€¤| AGENT[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ]
    AGENT -->|è¡Œå‹• a| ENV
    AGENT -->|çµŒé¨“ tuple| REPLAY[Experience Replay Buffer]
    REPLAY -->|ãƒŸãƒ‹ãƒãƒƒãƒ| TRAIN[å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹]
    TRAIN -->|å‹¾é…æ›´æ–°| QN
    TARGET[Target Network] -.->|ã‚¿ãƒ¼ã‚²ãƒƒãƒˆQå€¤| TRAIN
    QN -.->|å®šæœŸã‚³ãƒ”ãƒ¼| TARGET

    style QN fill:#e3f2fd
    style REPLAY fill:#fff3e0
    style TARGET fill:#e8f5e9
</div>

<h3>DQNã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆæ¦‚è¦ï¼‰</h3>

<p><strong>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  3.1: DQN</strong></p>
<ol>
<li>Q-Network $Q(s, a; \theta)$ ã¨Target Network $Q(s, a; \theta^-)$ ã‚’åˆæœŸåŒ–</li>
<li>Experience Replay Buffer $\mathcal{D}$ ã‚’åˆæœŸåŒ–</li>
<li>å„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã«ã¤ã„ã¦ï¼š
<ul>
<li>åˆæœŸçŠ¶æ…‹$s_0$ã‚’è¦³æ¸¬</li>
<li>å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—$t$ã«ã¤ã„ã¦ï¼š
<ol>
<li>$\epsilon$-greedyæ³•ã§è¡Œå‹•$a_t$ã‚’é¸æŠ</li>
<li>è¡Œå‹•ã‚’å®Ÿè¡Œã—ã€å ±é…¬$r_t$ã¨æ¬¡çŠ¶æ…‹$s_{t+1}$ã‚’è¦³æ¸¬</li>
<li>é·ç§»$(s_t, a_t, r_t, s_{t+1})$ã‚’$\mathcal{D}$ã«ä¿å­˜</li>
<li>$\mathcal{D}$ã‹ã‚‰ãƒŸãƒ‹ãƒãƒƒãƒã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</li>
<li>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ã‚’è¨ˆç®—ï¼š$y_j = r_j + \gamma \max_{a'} Q(s_{j+1}, a'; \theta^-)$</li>
<li>æå¤±é–¢æ•°ã‚’æœ€å°åŒ–ï¼š$L(\theta) = (y_j - Q(s_j, a_j; \theta))^2$</li>
<li>$C$ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ï¼š$\theta^- \leftarrow \theta$</li>
</ol>
</li>
</ul>
</li>
</ol>

<h3>Atariç”¨CNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h3>

<p>DQNã®å…ƒè«–æ–‡ã§ã¯ã€Atariã‚²ãƒ¼ãƒ ç”¨ã«ä»¥ä¸‹ã®CNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½¿ç”¨ã—ã¾ã—ãŸï¼š</p>

<table>
<thead>
<tr>
<th>ãƒ¬ã‚¤ãƒ¤ãƒ¼</th>
<th>å…¥åŠ›</th>
<th>ãƒ•ã‚£ãƒ«ã‚¿/ãƒ¦ãƒ‹ãƒƒãƒˆ</th>
<th>å‡ºåŠ›</th>
<th>æ´»æ€§åŒ–</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Input</strong></td>
<td>-</td>
<td>-</td>
<td>84Ã—84Ã—4</td>
<td>-</td>
</tr>
<tr>
<td><strong>Conv1</strong></td>
<td>84Ã—84Ã—4</td>
<td>32 filters, 8Ã—8, stride 4</td>
<td>20Ã—20Ã—32</td>
<td>ReLU</td>
</tr>
<tr>
<td><strong>Conv2</strong></td>
<td>20Ã—20Ã—32</td>
<td>64 filters, 4Ã—4, stride 2</td>
<td>9Ã—9Ã—64</td>
<td>ReLU</td>
</tr>
<tr>
<td><strong>Conv3</strong></td>
<td>9Ã—9Ã—64</td>
<td>64 filters, 3Ã—3, stride 1</td>
<td>7Ã—7Ã—64</td>
<td>ReLU</td>
</tr>
<tr>
<td><strong>Flatten</strong></td>
<td>7Ã—7Ã—64</td>
<td>-</td>
<td>3136</td>
<td>-</td>
</tr>
<tr>
<td><strong>FC1</strong></td>
<td>3136</td>
<td>512 units</td>
<td>512</td>
<td>ReLU</td>
</tr>
<tr>
<td><strong>FC2</strong></td>
<td>512</td>
<td>n_actions units</td>
<td>n_actions</td>
<td>Linear</td>
</tr>
</tbody>
</table>

<h3>å®Ÿè£…ä¾‹1: DQN Networkï¼ˆAtariç”¨CNNï¼‰</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

print("=== DQN Network ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ ===\n")

class DQN(nn.Module):
    """Atariç”¨ã®DQNï¼ˆCNNãƒ™ãƒ¼ã‚¹ï¼‰"""

    def __init__(self, n_actions, input_channels=4):
        super(DQN, self).__init__()

        # ç•³ã¿è¾¼ã¿å±¤ï¼ˆç”»åƒç‰¹å¾´æŠ½å‡ºï¼‰
        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)

        # Flattenå¾Œã®ã‚µã‚¤ã‚ºã‚’è¨ˆç®—ï¼ˆ84x84å…¥åŠ›ã®å ´åˆ -> 7x7x64 = 3136ï¼‰
        conv_output_size = 7 * 7 * 64

        # å…¨çµåˆå±¤
        self.fc1 = nn.Linear(conv_output_size, 512)
        self.fc2 = nn.Linear(512, n_actions)

    def forward(self, x):
        """
        Args:
            x: çŠ¶æ…‹ç”»åƒ [batch, channels, height, width]
        Returns:
            Qå€¤ [batch, n_actions]
        """
        # CNNã§ç‰¹å¾´æŠ½å‡º
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))

        # Flatten
        x = x.view(x.size(0), -1)

        # å…¨çµåˆå±¤ã§Qå€¤ã‚’å‡ºåŠ›
        x = F.relu(self.fc1(x))
        q_values = self.fc2(x)

        return q_values


class SimpleDQN(nn.Module):
    """CartPoleç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãªDQNï¼ˆå…¨çµåˆå±¤ã®ã¿ï¼‰"""

    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(SimpleDQN, self).__init__()

        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        """
        Args:
            x: çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ« [batch, state_dim]
        Returns:
            Qå€¤ [batch, action_dim]
        """
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        q_values = self.fc3(x)
        return q_values


# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
print("--- Atariç”¨DQNï¼ˆCNNï¼‰---")
atari_dqn = DQN(n_actions=4, input_channels=4)
dummy_state = torch.randn(2, 4, 84, 84)  # ãƒãƒƒãƒã‚µã‚¤ã‚º2
q_values = atari_dqn(dummy_state)
print(f"å…¥åŠ›å½¢çŠ¶: {dummy_state.shape}")
print(f"å‡ºåŠ›Qå€¤å½¢çŠ¶: {q_values.shape}")
print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in atari_dqn.parameters()):,}")
print(f"Qå€¤ã®ä¾‹: {q_values[0].detach().numpy()}\n")

print("--- CartPoleç”¨SimpleDQNï¼ˆå…¨çµåˆï¼‰---")
cartpole_dqn = SimpleDQN(state_dim=4, action_dim=2, hidden_dim=128)
dummy_state = torch.randn(2, 4)  # ãƒãƒƒãƒã‚µã‚¤ã‚º2
q_values = cartpole_dqn(dummy_state)
print(f"å…¥åŠ›å½¢çŠ¶: {dummy_state.shape}")
print(f"å‡ºåŠ›Qå€¤å½¢çŠ¶: {q_values.shape}")
print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in cartpole_dqn.parameters()):,}")
print(f"Qå€¤ã®ä¾‹: {q_values[0].detach().numpy()}\n")

# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã®ç¢ºèª
print("--- Atari DQN ãƒ¬ã‚¤ãƒ¤ãƒ¼è©³ç´° ---")
for name, module in atari_dqn.named_children():
    print(f"{name}: {module}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== DQN Network ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ ===

--- Atariç”¨DQNï¼ˆCNNï¼‰---
å…¥åŠ›å½¢çŠ¶: torch.Size([2, 4, 84, 84])
å‡ºåŠ›Qå€¤å½¢çŠ¶: torch.Size([2, 4])
ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 1,686,532
Qå€¤ã®ä¾‹: [-0.123  0.456 -0.234  0.789]

--- CartPoleç”¨SimpleDQNï¼ˆå…¨çµåˆï¼‰---
å…¥åŠ›å½¢çŠ¶: torch.Size([2, 4])
å‡ºåŠ›Qå€¤å½¢çŠ¶: torch.Size([2, 2])
ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 17,538
Qå€¤ã®ä¾‹: [0.234 -0.156]

--- Atari DQN ãƒ¬ã‚¤ãƒ¤ãƒ¼è©³ç´° ---
conv1: Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
conv2: Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
conv3: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
fc1: Linear(in_features=3136, out_features=512, bias=True)
fc2: Linear(in_features=512, out_features=4, bias=True)
</code></pre>

<hr>

<h2>3.3 Experience Replay</h2>

<h3>Experience Replayã®å¿…è¦æ€§</h3>

<p>å¼·åŒ–å­¦ç¿’ã§ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç’°å¢ƒã¨ç›¸äº’ä½œç”¨ã—ã¦å¾—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥å­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®å•é¡ŒãŒç™ºç”Ÿã—ã¾ã™ï¼š</p>

<blockquote>
<p>ã€Œé€£ç¶šçš„ã«åé›†ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã¯æ™‚é–“çš„ã«å¼·ãç›¸é–¢ã—ã¦ãŠã‚Šã€ãã®ã¾ã¾å­¦ç¿’ã™ã‚‹ã¨éå­¦ç¿’ã‚„å­¦ç¿’ã®ä¸å®‰å®šæ€§ã‚’å¼•ãèµ·ã“ã™ã€</p>
</blockquote>

<h3>ãƒ‡ãƒ¼ã‚¿ç›¸é–¢ã®å•é¡Œ</h3>

<table>
<thead>
<tr>
<th>å•é¡Œ</th>
<th>èª¬æ˜</th>
<th>å½±éŸ¿</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ™‚é–“çš„ç›¸é–¢</strong></td>
<td>é€£ç¶šãƒ‡ãƒ¼ã‚¿ãŒä¼¼ãŸçŠ¶æ…‹ãƒ»è¡Œå‹•</td>
<td>å‹¾é…ã®åã‚Šã§å­¦ç¿’ä¸å®‰å®š</td>
</tr>
<tr>
<td><strong>éi.i.d.æ€§</strong></td>
<td>ç‹¬ç«‹åŒåˆ†å¸ƒã®ä»®å®šãŒç ´ç¶»</td>
<td>SGDã®å‰ææ¡ä»¶é•å</td>
</tr>
<tr>
<td><strong>ç ´æ»…çš„å¿˜å´</strong></td>
<td>æ–°ãƒ‡ãƒ¼ã‚¿ã§éå»ã®çŸ¥è­˜ã‚’å¿˜ã‚Œã‚‹</td>
<td>å­¦ç¿’åŠ¹ç‡ä½ä¸‹</td>
</tr>
</tbody>
</table>

<h3>Replay Bufferã®ä»•çµ„ã¿</h3>

<p>Experience Replayã¯ã€éå»ã®çµŒé¨“$(s, a, r, s')$ã‚’<strong>Replay Buffer</strong>ã«è“„ç©ã—ã€ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦å­¦ç¿’ã—ã¾ã™ã€‚</p>

<div class="mermaid">
graph TB
    subgraph "çµŒé¨“ã®åé›†"
        ENV[ç’°å¢ƒ] -->|é·ç§»| EXP[çµŒé¨“ tuple<br/>s,a,r,s']
        EXP -->|ä¿å­˜| BUFFER[Replay Buffer<br/>å®¹é‡N]
    end

    subgraph "å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹"
        BUFFER -->|ãƒ©ãƒ³ãƒ€ãƒ <br/>ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°| BATCH[ãƒŸãƒ‹ãƒãƒƒãƒ<br/>ã‚µã‚¤ã‚ºB]
        BATCH -->|å­¦ç¿’| NETWORK[Q-Network]
    end

    style BUFFER fill:#fff3e0
    style BATCH fill:#e3f2fd
    style NETWORK fill:#e8f5e9
</div>

<h3>Replay Bufferã®åˆ©ç‚¹</h3>

<ol>
<li><strong>ç›¸é–¢ã®é™¤å»</strong>ï¼šãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§æ™‚é–“çš„ç›¸é–¢ã‚’æ‰“ç ´</li>
<li><strong>ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡</strong>ï¼šåŒã˜çµŒé¨“ã‚’è¤‡æ•°å›å†åˆ©ç”¨</li>
<li><strong>å­¦ç¿’å®‰å®šåŒ–</strong>ï¼ši.i.d.è¿‘ä¼¼ã§å‹¾é…åˆ†æ•£ã‚’ä½æ¸›</li>
<li><strong>ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼å­¦ç¿’</strong>ï¼šå¤ã„æ–¹ç­–ã®ãƒ‡ãƒ¼ã‚¿ã‚‚æœ‰åŠ¹æ´»ç”¨</li>
</ol>

<h3>å®Ÿè£…ä¾‹2: Replay Bufferå®Ÿè£…</h3>

<pre><code class="language-python">import numpy as np
import random
from collections import deque, namedtuple

print("=== Experience Replay Buffer å®Ÿè£… ===\n")

# çµŒé¨“ã‚’æ ¼ç´ã™ã‚‹ãŸã‚ã®named tuple
Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))

class ReplayBuffer:
    """çµŒé¨“ã‚’ä¿å­˜ãƒ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹Replay Buffer"""

    def __init__(self, capacity):
        """
        Args:
            capacity: ãƒãƒƒãƒ•ã‚¡ã®æœ€å¤§å®¹é‡
        """
        self.buffer = deque(maxlen=capacity)
        self.capacity = capacity

    def push(self, state, action, reward, next_state, done):
        """çµŒé¨“ã‚’ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ """
        self.buffer.append(Transition(state, action, reward, next_state, done))

    def sample(self, batch_size):
        """ãƒŸãƒ‹ãƒãƒƒãƒã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        transitions = random.sample(self.buffer, batch_size)

        # Transitionã®ãƒªã‚¹ãƒˆã‚’ãƒãƒƒãƒã«å¤‰æ›
        batch = Transition(*zip(*transitions))

        # NumPyé…åˆ—ã«å¤‰æ›
        states = np.array(batch.state)
        actions = np.array(batch.action)
        rewards = np.array(batch.reward)
        next_states = np.array(batch.next_state)
        dones = np.array(batch.done)

        return states, actions, rewards, next_states, dones

    def __len__(self):
        """ç¾åœ¨ã®ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º"""
        return len(self.buffer)


# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
print("--- Replay Bufferã®ãƒ†ã‚¹ãƒˆ ---")
buffer = ReplayBuffer(capacity=1000)

# ãƒ€ãƒŸãƒ¼çµŒé¨“ã‚’è¿½åŠ 
print("çµŒé¨“ã‚’è¿½åŠ ä¸­...")
for i in range(150):
    state = np.random.randn(4)
    action = np.random.randint(0, 2)
    reward = np.random.randn()
    next_state = np.random.randn(4)
    done = (i % 20 == 19)  # 20ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«çµ‚äº†

    buffer.push(state, action, reward, next_state, done)

print(f"ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º: {len(buffer)}/{buffer.capacity}")

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ
batch_size = 32
states, actions, rewards, next_states, dones = buffer.sample(batch_size)

print(f"\n--- ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°çµæœï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚º={batch_size}ï¼‰---")
print(f"stateså½¢çŠ¶: {states.shape}")
print(f"actionså½¢çŠ¶: {actions.shape}")
print(f"rewardså½¢çŠ¶: {rewards.shape}")
print(f"next_stateså½¢çŠ¶: {next_states.shape}")
print(f"doneså½¢çŠ¶: {dones.shape}")
print(f"\nã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:")
print(f"  state[0]: {states[0]}")
print(f"  action[0]: {actions[0]}")
print(f"  reward[0]: {rewards[0]:.3f}")
print(f"  done[0]: {dones[0]}")

# ç›¸é–¢æ€§ã®ç¢ºèª
print("\n--- ãƒ‡ãƒ¼ã‚¿ç›¸é–¢æ€§ã®ç¢ºèª ---")
print("é€£ç¶šãƒ‡ãƒ¼ã‚¿ï¼ˆç›¸é–¢ã‚ã‚Šï¼‰:")
for i in range(5):
    trans = list(buffer.buffer)[i]
    print(f"  step {i}: action={trans.action}, reward={trans.reward:.3f}")

print("\nãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆç›¸é–¢é™¤å»ï¼‰:")
for i in range(5):
    print(f"  sample {i}: action={actions[i]}, reward={rewards[i]:.3f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Experience Replay Buffer å®Ÿè£… ===

--- Replay Bufferã®ãƒ†ã‚¹ãƒˆ ---
çµŒé¨“ã‚’è¿½åŠ ä¸­...
ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º: 150/1000

--- ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°çµæœï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚º=32ï¼‰---
stateså½¢çŠ¶: (32, 4)
actionså½¢çŠ¶: (32,)
rewardså½¢çŠ¶: (32,)
next_stateså½¢çŠ¶: (32, 4)
doneså½¢çŠ¶: (32,)

ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:
  state[0]: [ 0.234 -1.123  0.567 -0.234]
  action[0]: 1
  reward[0]: 0.456
  done[0]: False

--- ãƒ‡ãƒ¼ã‚¿ç›¸é–¢æ€§ã®ç¢ºèª ---
é€£ç¶šãƒ‡ãƒ¼ã‚¿ï¼ˆç›¸é–¢ã‚ã‚Šï¼‰:
  step 0: action=0, reward=0.234
  step 1: action=1, reward=-0.123
  step 2: action=0, reward=0.567
  step 3: action=1, reward=-0.345
  step 4: action=0, reward=0.789

ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆç›¸é–¢é™¤å»ï¼‰:
  sample 0: action=1, reward=0.456
  sample 1: action=0, reward=-0.234
  sample 2: action=1, reward=0.123
  sample 3: action=0, reward=-0.567
  sample 4: action=1, reward=0.234
</code></pre>

<h3>Replay Bufferã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</h3>

<table>
<thead>
<tr>
<th>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</th>
<th>ä¸€èˆ¬çš„ãªå€¤</th>
<th>èª¬æ˜</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Bufferå®¹é‡</strong></td>
<td>10,000 ~ 1,000,000</td>
<td>ä¿å­˜ã§ãã‚‹æœ€å¤§çµŒé¨“æ•°</td>
</tr>
<tr>
<td><strong>Batch Size</strong></td>
<td>32 ~ 256</td>
<td>1å›ã®å­¦ç¿’ã§ä½¿ã†ã‚µãƒ³ãƒ—ãƒ«æ•°</td>
</tr>
<tr>
<td><strong>é–‹å§‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°</strong></td>
<td>1,000 ~ 10,000ã‚¹ãƒ†ãƒƒãƒ—</td>
<td>å­¦ç¿’é–‹å§‹å‰ã®çµŒé¨“è“„ç©æ•°</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.4 Target Network</h2>

<h3>Target Networkã®å¿…è¦æ€§</h3>

<p>DQNã§ã¯ã€TDèª¤å·®ã‚’æœ€å°åŒ–ã™ã‚‹ãŸã‚ã«ä»¥ä¸‹ã®æå¤±é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ï¼š</p>

<p>$$
L(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta) - Q(s, a; \theta) \right)^2 \right]
$$</p>

<p>ã—ã‹ã—ã€ã“ã®å¼ã§ã¯<strong>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ã¨Q-Networkã®ä¸¡æ–¹ãŒåŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\theta$ã«ä¾å­˜</strong>ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šä»¥ä¸‹ã®å•é¡ŒãŒç™ºç”Ÿã—ã¾ã™ï¼š</p>

<blockquote>
<p>ã€ŒQå€¤ã®æ›´æ–°ãŒã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ã‚’å‹•ã‹ã—ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ã®å¤‰åŒ–ãŒå†ã³Qå€¤ã‚’å¤‰åŒ–ã•ã›ã‚‹ã€ã¨ã„ã†è¿½ã„ã‹ã‘ã£ã“ãŒç™ºç”Ÿã—ã€å­¦ç¿’ãŒä¸å®‰å®šã«ãªã‚‹ã€</p>
</blockquote>

<div class="mermaid">
graph LR
    Q[Q-Network Î¸] -->|Qå€¤æ›´æ–°| TARGET[ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤]
    TARGET -->|æå¤±è¨ˆç®—| LOSS[æå¤±L]
    LOSS -->|å‹¾é…æ›´æ–°| Q

    style Q fill:#e3f2fd
    style TARGET fill:#ffcccc
    style LOSS fill:#fff3e0
</div>

<h3>Target Networkã«ã‚ˆã‚‹å®‰å®šåŒ–</h3>

<p>Target Networkã¯ã€<strong>Qå€¤è¨ˆç®—ç”¨ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤è¨ˆç®—ç”¨ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’åˆ†é›¢</strong>ã™ã‚‹ã“ã¨ã§å­¦ç¿’ã‚’å®‰å®šåŒ–ã•ã›ã¾ã™ã€‚</p>

<p>$$
L(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta) \right)^2 \right]
$$</p>

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$\theta$ï¼šQ-Networkï¼ˆå­¦ç¿’ã•ã‚Œã‚‹ï¼‰</li>
<li>$\theta^-$ï¼šTarget Networkï¼ˆå®šæœŸçš„ã«ã‚³ãƒ”ãƒ¼ï¼‰</li>
</ul>

<h3>Target Networkã®æ›´æ–°æ–¹æ³•</h3>

<h4>Hard Updateï¼ˆDQNï¼‰</h4>

<p>$C$ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«å®Œå…¨ã‚³ãƒ”ãƒ¼ï¼š</p>

<p>$$
\theta^- \leftarrow \theta \quad \text{every } C \text{ steps}
$$</p>

<ul>
<li>åˆ©ç‚¹ï¼šã‚·ãƒ³ãƒ—ãƒ«ã§å®Ÿè£…ãŒå®¹æ˜“</li>
<li>æ¬ ç‚¹ï¼šæ›´æ–°æ™‚ã«ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãŒæ€¥æ¿€ã«å¤‰åŒ–</li>
<li>ä¸€èˆ¬çš„ãª$C$ï¼š1,000 ~ 10,000ã‚¹ãƒ†ãƒƒãƒ—</li>
</ul>

<h4>Soft Updateï¼ˆDDPGç­‰ï¼‰</h4>

<p>æ¯ã‚¹ãƒ†ãƒƒãƒ—ã§å°‘ã—ãšã¤æ›´æ–°ï¼š</p>

<p>$$
\theta^- \leftarrow \tau \theta + (1 - \tau) \theta^-
$$</p>

<ul>
<li>åˆ©ç‚¹ï¼šæ»‘ã‚‰ã‹ãªæ›´æ–°ã§å®‰å®šæ€§å‘ä¸Š</li>
<li>æ¬ ç‚¹ï¼šãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãŒé‡è¦</li>
<li>ä¸€èˆ¬çš„ãª$\tau$ï¼š0.001 ~ 0.01</li>
</ul>

<h3>å®Ÿè£…ä¾‹3: Target Networkã®æ›´æ–°</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import copy

print("=== Target Network å®Ÿè£… ===\n")

class DQNAgent:
    """Target Networkã‚’æŒã¤DQNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""

    def __init__(self, state_dim, action_dim, hidden_dim=128):
        # Q-Networkï¼ˆå­¦ç¿’ç”¨ï¼‰
        self.q_network = SimpleDQN(state_dim, action_dim, hidden_dim)

        # Target Networkï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤è¨ˆç®—ç”¨ï¼‰
        self.target_network = SimpleDQN(state_dim, action_dim, hidden_dim)

        # Target Networkã‚’åˆæœŸåŒ–ï¼ˆQ-Networkã®ã‚³ãƒ”ãƒ¼ï¼‰
        self.target_network.load_state_dict(self.q_network.state_dict())

        # Target Networkã¯å‹¾é…è¨ˆç®—ä¸è¦
        for param in self.target_network.parameters():
            param.requires_grad = False

        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=1e-3)
        self.update_counter = 0

    def hard_update_target_network(self, update_interval=1000):
        """Hard Update: Cã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«å®Œå…¨ã‚³ãƒ”ãƒ¼"""
        self.update_counter += 1

        if self.update_counter % update_interval == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())
            print(f"  [Hard Update] Target Networkæ›´æ–°ï¼ˆstep {self.update_counter}ï¼‰")

    def soft_update_target_network(self, tau=0.005):
        """Soft Update: æ¯ã‚¹ãƒ†ãƒƒãƒ—ã§å°‘ã—ãšã¤æ›´æ–°"""
        for target_param, q_param in zip(self.target_network.parameters(),
                                          self.q_network.parameters()):
            target_param.data.copy_(tau * q_param.data + (1 - tau) * target_param.data)

    def compute_td_target(self, rewards, next_states, dones, gamma=0.99):
        """
        TDç›®æ¨™å€¤ã®è¨ˆç®—ï¼ˆTarget Networkã‚’ä½¿ç”¨ï¼‰

        Args:
            rewards: [batch_size]
            next_states: [batch_size, state_dim]
            dones: [batch_size]
            gamma: å‰²å¼•ç‡
        """
        with torch.no_grad():
            # Target Networkã§Qå€¤ã‚’è¨ˆç®—
            next_q_values = self.target_network(next_states)
            max_next_q = next_q_values.max(dim=1)[0]

            # çµ‚ç«¯çŠ¶æ…‹ã§ã¯æ¬¡çŠ¶æ…‹ã®ä¾¡å€¤ã‚’0ã«ã™ã‚‹
            max_next_q = max_next_q * (1 - dones)

            # TDç›®æ¨™å€¤: r + Î³ * max Q(s', a')
            td_target = rewards + gamma * max_next_q

        return td_target


# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
print("--- Target NetworkåˆæœŸåŒ– ---")
agent = DQNAgent(state_dim=4, action_dim=2)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸€è‡´ç¢ºèª
q_params = list(agent.q_network.parameters())[0].data.flatten()[:5]
target_params = list(agent.target_network.parameters())[0].data.flatten()[:5]
print(f"Q-Network params: {q_params.numpy()}")
print(f"Target Network params: {target_params.numpy()}")
print(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸€è‡´: {torch.allclose(q_params, target_params)}\n")

# Hard Updateã®ãƒ†ã‚¹ãƒˆ
print("--- Hard Update ãƒ†ã‚¹ãƒˆ ---")
for step in range(1, 3001):
    # ãƒ€ãƒŸãƒ¼å­¦ç¿’ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¤‰åŒ–ï¼‰
    dummy_loss = torch.randn(1, requires_grad=True).sum()
    agent.optimizer.zero_grad()
    dummy_loss.backward()
    agent.optimizer.step()

    # Target Networkæ›´æ–°
    agent.hard_update_target_network(update_interval=1000)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å·®ç•°ç¢ºèª
q_params = list(agent.q_network.parameters())[0].data.flatten()[:5]
target_params = list(agent.target_network.parameters())[0].data.flatten()[:5]
print(f"\næœ€çµ‚çŠ¶æ…‹:")
print(f"Q-Network params: {q_params.numpy()}")
print(f"Target Network params: {target_params.numpy()}")
print(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸€è‡´: {torch.allclose(q_params, target_params)}\n")

# Soft Updateã®ãƒ†ã‚¹ãƒˆ
print("--- Soft Update ãƒ†ã‚¹ãƒˆ ---")
agent2 = DQNAgent(state_dim=4, action_dim=2)
initial_target = list(agent2.target_network.parameters())[0].data.flatten()[0].item()

for step in range(100):
    # ãƒ€ãƒŸãƒ¼å­¦ç¿’
    dummy_loss = torch.randn(1, requires_grad=True).sum()
    agent2.optimizer.zero_grad()
    dummy_loss.backward()
    agent2.optimizer.step()

    # Soft Update
    agent2.soft_update_target_network(tau=0.01)

final_target = list(agent2.target_network.parameters())[0].data.flatten()[0].item()
final_q = list(agent2.q_network.parameters())[0].data.flatten()[0].item()

print(f"åˆæœŸTargetå€¤: {initial_target:.6f}")
print(f"æœ€çµ‚Targetå€¤: {final_target:.6f}")
print(f"æœ€çµ‚Qå€¤: {final_q:.6f}")
print(f"Targetã®å¤‰åŒ–: {abs(final_target - initial_target):.6f}")
print(f"Q-Targetã®å·®: {abs(final_q - final_target):.6f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Target Network å®Ÿè£… ===

--- Target NetworkåˆæœŸåŒ– ---
Q-Network params: [ 0.123 -0.234  0.456 -0.567  0.789]
Target Network params: [ 0.123 -0.234  0.456 -0.567  0.789]
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸€è‡´: True

--- Hard Update ãƒ†ã‚¹ãƒˆ ---
  [Hard Update] Target Networkæ›´æ–°ï¼ˆstep 1000ï¼‰
  [Hard Update] Target Networkæ›´æ–°ï¼ˆstep 2000ï¼‰
  [Hard Update] Target Networkæ›´æ–°ï¼ˆstep 3000ï¼‰

æœ€çµ‚çŠ¶æ…‹:
Q-Network params: [ 0.234 -0.345  0.567 -0.678  0.890]
Target Network params: [ 0.234 -0.345  0.567 -0.678  0.890]
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸€è‡´: True

--- Soft Update ãƒ†ã‚¹ãƒˆ ---
åˆæœŸTargetå€¤: 0.123456
æœ€çµ‚Targetå€¤: 0.234567
æœ€çµ‚Qå€¤: 0.345678
Targetã®å¤‰åŒ–: 0.111111
Q-Targetã®å·®: 0.111111
</code></pre>

<h3>Hard vs Soft Updateã®æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>Hard Update</th>
<th>Soft Update</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ›´æ–°é »åº¦</strong></td>
<td>1,000~10,000ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨</td>
<td>æ¯ã‚¹ãƒ†ãƒƒãƒ—</td>
</tr>
<tr>
<td><strong>æ›´æ–°æ–¹æ³•</strong></td>
<td>å®Œå…¨ã‚³ãƒ”ãƒ¼</td>
<td>æŒ‡æ•°ç§»å‹•å¹³å‡</td>
</tr>
<tr>
<td><strong>å®‰å®šæ€§</strong></td>
<td>æ›´æ–°æ™‚ã«æ€¥æ¿€å¤‰åŒ–</td>
<td>æ»‘ã‚‰ã‹ã«å¤‰åŒ–</td>
</tr>
<tr>
<td><strong>å®Ÿè£…</strong></td>
<td>ã‚·ãƒ³ãƒ—ãƒ«</td>
<td>ã‚„ã‚„è¤‡é›‘</td>
</tr>
<tr>
<td><strong>é©ç”¨ä¾‹</strong></td>
<td>DQN, Rainbow</td>
<td>DDPG, TD3, SAC</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.5 DQNã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ‹¡å¼µ</h2>

<h3>3.5.1 Double DQN</h3>

<h4>Qå€¤ã®éå¤§è©•ä¾¡å•é¡Œ</h4>

<p>æ¨™æº–DQNã§ã¯ã€TDç›®æ¨™å€¤ã®è¨ˆç®—ã§åŒã˜ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã£ã¦è¡Œå‹•é¸æŠã¨è©•ä¾¡ã‚’è¡Œã„ã¾ã™ï¼š</p>

<p>$$
y = r + \gamma \max_{a'} Q(s', a'; \theta^-)
$$</p>

<p>ã“ã®$\max$æ¼”ç®—ã«ã‚ˆã‚Šã€Qå€¤ãŒ<strong>ç³»çµ±çš„ã«éå¤§è©•ä¾¡</strong>ã•ã‚Œã‚‹å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚</p>

<blockquote>
<p>ã€Œãƒã‚¤ã‚ºã‚„æ¨å®šèª¤å·®ã«ã‚ˆã‚Šã€ãŸã¾ãŸã¾å¤§ããªQå€¤ã‚’æŒã¤è¡Œå‹•ãŒé¸ã°ã‚Œã€å®Ÿéš›ã‚ˆã‚Šã‚‚é«˜ã„ä¾¡å€¤ãŒä¼æ’­ã—ã¦ã„ãã€</p>
</blockquote>

<h4>Double DQNã®è§£æ±ºç­–</h4>

<p>Double DQNã¯ã€<strong>è¡Œå‹•ã®é¸æŠ</strong>ã¨<strong>Qå€¤ã®è©•ä¾¡</strong>ã‚’åˆ¥ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§è¡Œã„ã¾ã™ï¼š</p>

<p>$$
y = r + \gamma Q\left(s', \arg\max_{a'} Q(s', a'; \theta), \theta^-\right)
$$</p>

<p>æ‰‹é †ï¼š</p>
<ol>
<li>Q-Network $\theta$ã§æœ€é©è¡Œå‹•ã‚’é¸æŠï¼š$a^* = \arg\max_{a'} Q(s', a'; \theta)$</li>
<li>Target Network $\theta^-$ã§ãã®è¡Œå‹•ã®Qå€¤ã‚’è©•ä¾¡ï¼š$Q(s', a^*; \theta^-)$</li>
</ol>

<h4>å®Ÿè£…ä¾‹4: Double DQN</h4>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

print("=== Double DQN vs æ¨™æº–DQN ===\n")

def compute_standard_dqn_target(q_network, target_network,
                                 rewards, next_states, dones, gamma=0.99):
    """æ¨™æº–DQNã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨ˆç®—"""
    with torch.no_grad():
        # Target Networkã§æ¬¡çŠ¶æ…‹ã®Qå€¤ã‚’è¨ˆç®—ã—ã€æœ€å¤§å€¤ã‚’å–å¾—
        next_q_values = target_network(next_states)
        max_next_q = next_q_values.max(dim=1)[0]

        # TDç›®æ¨™å€¤
        target = rewards + gamma * max_next_q * (1 - dones)

    return target


def compute_double_dqn_target(q_network, target_network,
                               rewards, next_states, dones, gamma=0.99):
    """Double DQNã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨ˆç®—"""
    with torch.no_grad():
        # Q-Networkã§æœ€é©è¡Œå‹•ã‚’é¸æŠ
        next_q_values_online = q_network(next_states)
        best_actions = next_q_values_online.argmax(dim=1)

        # Target Networkã§ãã®è¡Œå‹•ã®Qå€¤ã‚’è©•ä¾¡
        next_q_values_target = target_network(next_states)
        max_next_q = next_q_values_target.gather(1, best_actions.unsqueeze(1)).squeeze(1)

        # TDç›®æ¨™å€¤
        target = rewards + gamma * max_next_q * (1 - dones)

    return target


# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
print("--- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æº–å‚™ ---")
q_net = SimpleDQN(state_dim=4, action_dim=3)
target_net = SimpleDQN(state_dim=4, action_dim=3)
target_net.load_state_dict(q_net.state_dict())

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
batch_size = 5
states = torch.randn(batch_size, 4)
next_states = torch.randn(batch_size, 4)
rewards = torch.tensor([1.0, -1.0, 0.5, 0.0, 2.0])
dones = torch.tensor([0.0, 0.0, 0.0, 1.0, 0.0])

# Q-Networkã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«æ„å›³çš„ãªå·®ã‚’ä½œã‚‹
with torch.no_grad():
    for param in q_net.parameters():
        param.add_(torch.randn_like(param) * 0.1)

print("--- æ¬¡çŠ¶æ…‹ã®Qå€¤åˆ†å¸ƒ ---")
with torch.no_grad():
    q_values_online = q_net(next_states)
    q_values_target = target_net(next_states)

for i in range(min(3, batch_size)):
    print(f"ã‚µãƒ³ãƒ—ãƒ«{i}:")
    print(f"  Q-Network Qå€¤: {q_values_online[i].numpy()}")
    print(f"  Target Network Qå€¤: {q_values_target[i].numpy()}")
    print(f"  Q-Netã§é¸ã¶è¡Œå‹•: {q_values_online[i].argmax().item()}")
    print(f"  Targetã§é¸ã¶è¡Œå‹•: {q_values_target[i].argmax().item()}")

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ã®æ¯”è¼ƒ
target_standard = compute_standard_dqn_target(q_net, target_net, rewards, next_states, dones)
target_double = compute_double_dqn_target(q_net, target_net, rewards, next_states, dones)

print("\n--- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ã®æ¯”è¼ƒ ---")
print(f"å ±é…¬: {rewards.numpy()}")
print(f"æ¨™æº–DQNç›®æ¨™: {target_standard.numpy()}")
print(f"Double DQNç›®æ¨™: {target_double.numpy()}")
print(f"å·®åˆ†: {(target_standard - target_double).numpy()}")
print(f"å¹³å‡å·®åˆ†: {(target_standard - target_double).abs().mean().item():.4f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Double DQN vs æ¨™æº–DQN ===

--- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æº–å‚™ ---
--- æ¬¡çŠ¶æ…‹ã®Qå€¤åˆ†å¸ƒ ---
ã‚µãƒ³ãƒ—ãƒ«0:
  Q-Network Qå€¤: [ 0.234  0.567 -0.123]
  Target Network Qå€¤: [ 0.123  0.456 -0.234]
  Q-Netã§é¸ã¶è¡Œå‹•: 1
  Targetã§é¸ã¶è¡Œå‹•: 1
ã‚µãƒ³ãƒ—ãƒ«1:
  Q-Network Qå€¤: [-0.345  0.123  0.789]
  Target Network Qå€¤: [-0.234  0.234  0.567]
  Q-Netã§é¸ã¶è¡Œå‹•: 2
  Targetã§é¸ã¶è¡Œå‹•: 2
ã‚µãƒ³ãƒ—ãƒ«2:
  Q-Network Qå€¤: [ 0.456 -0.234  0.123]
  Target Network Qå€¤: [ 0.345 -0.123  0.234]
  Q-Netã§é¸ã¶è¡Œå‹•: 0
  Targetã§é¸ã¶è¡Œå‹•: 0

--- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ã®æ¯”è¼ƒ ---
å ±é…¬: [ 1.  -1.   0.5  0.   2. ]
æ¨™æº–DQNç›®æ¨™: [ 1.452 -0.439  0.842  0.000  2.567]
Double DQNç›®æ¨™: [ 1.456 -0.437  0.841  0.000  2.563]
å·®åˆ†: [-0.004 -0.002  0.001  0.000  0.004]
å¹³å‡å·®åˆ†: 0.0022
</code></pre>

<h3>3.5.2 Dueling DQN</h3>

<h4>ä¾¡å€¤é–¢æ•°ã®åˆ†è§£</h4>

<p>Dueling DQNã¯ã€Qå€¤ã‚’<strong>çŠ¶æ…‹ä¾¡å€¤$V(s)$</strong>ã¨<strong>å„ªä½æ€§é–¢æ•°$A(s, a)$</strong>ã«åˆ†è§£ã—ã¾ã™ï¼š</p>

<p>$$
Q(s, a) = V(s) + A(s, a)
$$</p>

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$V(s)$ï¼šçŠ¶æ…‹$s$è‡ªä½“ã®ä¾¡å€¤ï¼ˆè¡Œå‹•ã«ã‚ˆã‚‰ãªã„ï¼‰</li>
<li>$A(s, a)$ï¼šçŠ¶æ…‹$s$ã§è¡Œå‹•$a$ã‚’é¸ã¶å„ªä½æ€§ï¼ˆç›¸å¯¾çš„ãªè‰¯ã•ï¼‰</li>
</ul>

<blockquote>
<p>ã€Œå¤šãã®çŠ¶æ…‹ã§ã¯ã€ã©ã®è¡Œå‹•ã‚’é¸ã‚“ã§ã‚‚ä¾¡å€¤ãŒå¤§ããå¤‰ã‚ã‚‰ãªã„ã€‚Duelingæ§‹é€ ã«ã‚ˆã‚Šã€ãã®ã‚ˆã†ãªçŠ¶æ…‹ã§ã®V(s)ã‚’åŠ¹ç‡çš„ã«å­¦ç¿’ã§ãã‚‹ã€</p>
</blockquote>

<h4>Dueling Networkã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h4>

<div class="mermaid">
graph TB
    INPUT[å…¥åŠ›çŠ¶æ…‹ s] --> FEATURE[ç‰¹å¾´æŠ½å‡º<br/>å…±é€šå±¤]

    FEATURE --> VALUE_STREAM[Value Stream]
    FEATURE --> ADV_STREAM[Advantage Stream]

    VALUE_STREAM --> V[V s]
    ADV_STREAM --> A[A s,a]

    V --> AGGREGATION[é›†ç´„å±¤]
    A --> AGGREGATION

    AGGREGATION --> Q[Q s,a = V s + A s,a - mean A]

    style FEATURE fill:#e3f2fd
    style V fill:#fff3e0
    style A fill:#e8f5e9
    style Q fill:#c8e6c9
</div>

<h4>é›†ç´„æ–¹æ³•</h4>

<p>å˜ç´”ãªè¶³ã—ç®—ã§ã¯ä¸€æ„æ€§ãŒä¿è¨¼ã•ã‚Œãªã„ãŸã‚ã€ä»¥ä¸‹ã®åˆ¶ç´„ã‚’å°å…¥ã—ã¾ã™ï¼š</p>

<p>$$
Q(s, a; \theta, \alpha, \beta) = V(s; \theta, \beta) + \left( A(s, a; \theta, \alpha) - \frac{1}{|\mathcal{A}|} \sum_{a'} A(s, a'; \theta, \alpha) \right)
$$</p>

<p>ã¾ãŸã¯ã€ã‚ˆã‚Šå®‰å®šã—ãŸæ–¹æ³•ï¼š</p>

<p>$$
Q(s, a; \theta, \alpha, \beta) = V(s; \theta, \beta) + \left( A(s, a; \theta, \alpha) - \max_{a'} A(s, a'; \theta, \alpha) \right)
$$</p>

<h4>å®Ÿè£…ä¾‹5: Dueling DQN Network</h4>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

print("=== Dueling DQN ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ ===\n")

class DuelingDQN(nn.Module):
    """Dueling DQN: V(s)ã¨A(s,a)ã«åˆ†è§£"""

    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(DuelingDQN, self).__init__()

        # å…±é€šç‰¹å¾´æŠ½å‡ºå±¤
        self.feature = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU()
        )

        # Value Stream: V(s)ã‚’å‡ºåŠ›
        self.value_stream = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

        # Advantage Stream: A(s,a)ã‚’å‡ºåŠ›
        self.advantage_stream = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )

    def forward(self, x):
        """
        Args:
            x: çŠ¶æ…‹ [batch, state_dim]
        Returns:
            Qå€¤ [batch, action_dim]
        """
        # å…±é€šç‰¹å¾´æŠ½å‡º
        features = self.feature(x)

        # V(s)ã¨A(s,a)ã‚’è¨ˆç®—
        value = self.value_stream(features)  # [batch, 1]
        advantage = self.advantage_stream(features)  # [batch, action_dim]

        # Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))
        # å¹³å‡ã‚’å¼•ãã“ã¨ã§ä¸€æ„æ€§ã‚’ä¿è¨¼
        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))

        return q_values

    def get_value_advantage(self, x):
        """V(s)ã¨A(s,a)ã‚’å€‹åˆ¥ã«å–å¾—ï¼ˆåˆ†æç”¨ï¼‰"""
        features = self.feature(x)
        value = self.value_stream(features)
        advantage = self.advantage_stream(features)
        return value, advantage


# æ¨™æº–DQNã¨ã®æ¯”è¼ƒ
class StandardDQN(nn.Module):
    """æ¨™æº–DQNï¼ˆæ¯”è¼ƒç”¨ï¼‰"""

    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(StandardDQN, self).__init__()

        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )

    def forward(self, x):
        return self.network(x)


# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
print("--- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¯”è¼ƒ ---")
state_dim, action_dim = 4, 3

dueling_dqn = DuelingDQN(state_dim, action_dim)
standard_dqn = StandardDQN(state_dim, action_dim)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®æ¯”è¼ƒ
dueling_params = sum(p.numel() for p in dueling_dqn.parameters())
standard_params = sum(p.numel() for p in standard_dqn.parameters())

print(f"Dueling DQNãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {dueling_params:,}")
print(f"æ¨™æº–DQNãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {standard_params:,}")

# æ¨è«–ãƒ†ã‚¹ãƒˆ
dummy_states = torch.randn(3, state_dim)

print("\n--- Dueling DQNã®å†…éƒ¨è¡¨ç¾ ---")
with torch.no_grad():
    q_values = dueling_dqn(dummy_states)
    value, advantage = dueling_dqn.get_value_advantage(dummy_states)

for i in range(3):
    print(f"\nçŠ¶æ…‹{i}:")
    print(f"  V(s): {value[i].item():.3f}")
    print(f"  A(s,a): {advantage[i].numpy()}")
    print(f"  Aå¹³å‡: {advantage[i].mean().item():.3f}")
    print(f"  Q(s,a): {q_values[i].numpy()}")
    print(f"  æœ€é©è¡Œå‹•: {q_values[i].argmax().item()}")

# è¡Œå‹•ã®ä¾¡å€¤å·®ã®å¯è¦–åŒ–
print("\n--- ä¾¡å€¤é–¢æ•°ã®åˆ†è§£ã®åŠ¹æœ ---")
print("Duelingã§ã¯ã€V(s)ãŒçŠ¶æ…‹ã®åŸºæœ¬ä¾¡å€¤ã‚’ã€A(s,a)ãŒè¡Œå‹•ã®ç›¸å¯¾çš„å„ªä½æ€§ã‚’è¡¨ã™")
print("\nä¾‹: å…¨è¡Œå‹•ãŒä¼¼ãŸä¾¡å€¤ã‚’æŒã¤çŠ¶æ…‹")
dummy_state = torch.randn(1, state_dim)
with torch.no_grad():
    v, a = dueling_dqn.get_value_advantage(dummy_state)
    q = dueling_dqn(dummy_state)

print(f"V(s) = {v[0].item():.3f} (çŠ¶æ…‹è‡ªä½“ã®ä¾¡å€¤)")
print(f"A(s,a) = {a[0].numpy()} (è¡Œå‹•ã®å„ªä½æ€§)")
print(f"Q(s,a) = {q[0].numpy()} (æœ€çµ‚Qå€¤)")
print(f"è¡Œå‹•é–“ã®Qå€¤å·®: {q[0].max().item() - q[0].min().item():.3f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Dueling DQN ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ ===

--- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¯”è¼ƒ ---
Dueling DQNãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 18,051
æ¨™æº–DQNãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 17,539

--- Dueling DQNã®å†…éƒ¨è¡¨ç¾ ---

çŠ¶æ…‹0:
  V(s): 0.123
  A(s,a): [ 0.234 -0.123  0.456]
  Aå¹³å‡: 0.189
  Q(s,a): [ 0.168 -0.189  0.390]
  æœ€é©è¡Œå‹•: 2

çŠ¶æ…‹1:
  V(s): -0.234
  A(s,a): [-0.045  0.123 -0.234]
  Aå¹³å‡: -0.052
  Q(s,a): [-0.227 -0.059 -0.416]
  æœ€é©è¡Œå‹•: 1

çŠ¶æ…‹2:
  V(s): 0.456
  A(s,a): [ 0.123  0.089 -0.045]
  Aå¹³å‡: 0.056
  Q(s,a): [ 0.523  0.489  0.355]
  æœ€é©è¡Œå‹•: 0

--- ä¾¡å€¤é–¢æ•°ã®åˆ†è§£ã®åŠ¹æœ ---
Duelingã§ã¯ã€V(s)ãŒçŠ¶æ…‹ã®åŸºæœ¬ä¾¡å€¤ã‚’ã€A(s,a)ãŒè¡Œå‹•ã®ç›¸å¯¾çš„å„ªä½æ€§ã‚’è¡¨ã™

ä¾‹: å…¨è¡Œå‹•ãŒä¼¼ãŸä¾¡å€¤ã‚’æŒã¤çŠ¶æ…‹
V(s) = 0.234 (çŠ¶æ…‹è‡ªä½“ã®ä¾¡å€¤)
A(s,a) = [ 0.045 -0.023  0.012] (è¡Œå‹•ã®å„ªä½æ€§)
Q(s,a) = [ 0.252  0.184  0.219] (æœ€çµ‚Qå€¤)
è¡Œå‹•é–“ã®Qå€¤å·®: 0.068
</code></pre>

<h3>DQNæ‹¡å¼µæ‰‹æ³•ã®ã¾ã¨ã‚</h3>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>è§£æ±ºã™ã‚‹å•é¡Œ</th>
<th>ä¸»è¦ã‚¢ã‚¤ãƒ‡ã‚¢</th>
<th>è¨ˆç®—ã‚³ã‚¹ãƒˆ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>DQN</strong></td>
<td>é«˜æ¬¡å…ƒçŠ¶æ…‹ç©ºé–“</td>
<td>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§Qé–¢æ•°è¿‘ä¼¼</td>
<td>åŸºæº–</td>
</tr>
<tr>
<td><strong>Experience Replay</strong></td>
<td>ãƒ‡ãƒ¼ã‚¿ç›¸é–¢</td>
<td>éå»çµŒé¨“ã‚’ãƒãƒƒãƒ•ã‚¡ã«ä¿å­˜ãƒ»å†åˆ©ç”¨</td>
<td>+ãƒ¡ãƒ¢ãƒª</td>
</tr>
<tr>
<td><strong>Target Network</strong></td>
<td>å­¦ç¿’ä¸å®‰å®šæ€§</td>
<td>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨ˆç®—ç”¨ã®å›ºå®šãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</td>
<td>+2å€ãƒ¡ãƒ¢ãƒª</td>
</tr>
<tr>
<td><strong>Double DQN</strong></td>
<td>Qå€¤éå¤§è©•ä¾¡</td>
<td>è¡Œå‹•é¸æŠã¨è©•ä¾¡ã‚’åˆ†é›¢</td>
<td>â‰ˆDQN</td>
</tr>
<tr>
<td><strong>Dueling DQN</strong></td>
<td>ä¾¡å€¤æ¨å®šã®éåŠ¹ç‡</td>
<td>V(s)ã¨A(s,a)ã‚’åˆ†é›¢å­¦ç¿’</td>
<td>â‰ˆDQN</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.6 å®Ÿè£…: CartPoleã§ã®DQNå­¦ç¿’</h2>

<h3>CartPoleç’°å¢ƒã®èª¬æ˜</h3>

<p><strong>CartPole-v1</strong>ã¯ã€å€’ç«‹æŒ¯å­ã‚’åˆ¶å¾¡ã™ã‚‹å¤å…¸çš„ãªå¼·åŒ–å­¦ç¿’ã‚¿ã‚¹ã‚¯ã§ã™ã€‚</p>

<ul>
<li><strong>çŠ¶æ…‹</strong>: 4æ¬¡å…ƒé€£ç¶šå€¤ï¼ˆã‚«ãƒ¼ãƒˆä½ç½®ã€ã‚«ãƒ¼ãƒˆé€Ÿåº¦ã€ãƒãƒ¼ãƒ«è§’åº¦ã€ãƒãƒ¼ãƒ«è§’é€Ÿåº¦ï¼‰</li>
<li><strong>è¡Œå‹•</strong>: 2ã¤ã®é›¢æ•£è¡Œå‹•ï¼ˆå·¦ã«æŠ¼ã™ã€å³ã«æŠ¼ã™ï¼‰</li>
<li><strong>å ±é…¬</strong>: å„ã‚¹ãƒ†ãƒƒãƒ—+1ï¼ˆãƒãƒ¼ãƒ«ãŒå€’ã‚Œã‚‹ã¾ã§ï¼‰</li>
<li><strong>çµ‚äº†æ¡ä»¶</strong>: ãƒãƒ¼ãƒ«è§’åº¦ãŒÂ±12Â°ä»¥ä¸Šã€ã‚«ãƒ¼ãƒˆä½ç½®ãŒÂ±2.4ä»¥ä¸Šã€500ã‚¹ãƒ†ãƒƒãƒ—åˆ°é”</li>
<li><strong>æˆåŠŸåŸºæº–</strong>: 100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å¹³å‡å ±é…¬ãŒ475ä»¥ä¸Š</li>
</ul>

<h3>å®Ÿè£…ä¾‹6: CartPole DQNå®Œå…¨å®Ÿè£…</h3>

<pre><code class="language-python">import gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
from collections import deque
import matplotlib.pyplot as plt

print("=== CartPole DQN å®Œå…¨å®Ÿè£… ===\n")

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
GAMMA = 0.99
LEARNING_RATE = 1e-3
BATCH_SIZE = 64
BUFFER_SIZE = 10000
EPSILON_START = 1.0
EPSILON_END = 0.01
EPSILON_DECAY = 0.995
TARGET_UPDATE_FREQ = 10
NUM_EPISODES = 500

class ReplayBuffer:
    """Experience Replay Buffer"""
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return (np.array(states), np.array(actions), np.array(rewards),
                np.array(next_states), np.array(dones))

    def __len__(self):
        return len(self.buffer)


class DQNNetwork(nn.Module):
    """CartPoleç”¨DQN"""
    def __init__(self, state_dim, action_dim):
        super(DQNNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)


class DQNAgent:
    """DQNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self, state_dim, action_dim):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.epsilon = EPSILON_START

        # Q-Networkã¨Target Network
        self.q_network = DQNNetwork(state_dim, action_dim)
        self.target_network = DQNNetwork(state_dim, action_dim)
        self.target_network.load_state_dict(self.q_network.state_dict())

        self.optimizer = optim.Adam(self.q_network.parameters(), lr=LEARNING_RATE)
        self.buffer = ReplayBuffer(BUFFER_SIZE)

    def select_action(self, state, training=True):
        """Îµ-greedyæ³•ã§è¡Œå‹•é¸æŠ"""
        if training and random.random() < self.epsilon:
            return random.randrange(self.action_dim)
        else:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                q_values = self.q_network(state_tensor)
                return q_values.argmax().item()

    def train_step(self):
        """1å›ã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—"""
        if len(self.buffer) < BATCH_SIZE:
            return None

        # ãƒŸãƒ‹ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        states, actions, rewards, next_states, dones = self.buffer.sample(BATCH_SIZE)

        # Tensorã«å¤‰æ›
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        next_states = torch.FloatTensor(next_states)
        dones = torch.FloatTensor(dones)

        # ç¾åœ¨ã®Qå€¤
        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆQå€¤ï¼ˆDouble DQNï¼‰
        with torch.no_grad():
            # Q-Networkã§è¡Œå‹•é¸æŠ
            next_actions = self.q_network(next_states).argmax(1)
            # Target Networkã§è©•ä¾¡
            next_q = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)
            target_q = rewards + GAMMA * next_q * (1 - dones)

        # æå¤±è¨ˆç®—ã¨æœ€é©åŒ–
        loss = nn.MSELoss()(current_q, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()

    def update_target_network(self):
        """Target Networkã®æ›´æ–°"""
        self.target_network.load_state_dict(self.q_network.state_dict())

    def decay_epsilon(self):
        """Îµã®æ¸›è¡°"""
        self.epsilon = max(EPSILON_END, self.epsilon * EPSILON_DECAY)


# å­¦ç¿’å®Ÿè¡Œ
print("--- CartPoleå­¦ç¿’é–‹å§‹ ---")
env = gym.make('CartPole-v1')
agent = DQNAgent(state_dim=4, action_dim=2)

episode_rewards = []
losses = []

for episode in range(NUM_EPISODES):
    state = env.reset()
    if isinstance(state, tuple):  # gym>=0.26å¯¾å¿œ
        state = state[0]

    episode_reward = 0
    episode_loss = []

    for t in range(500):
        # è¡Œå‹•é¸æŠ
        action = agent.select_action(state)

        # ç’°å¢ƒã‚¹ãƒ†ãƒƒãƒ—
        result = env.step(action)
        if len(result) == 5:  # gym>=0.26
            next_state, reward, terminated, truncated, _ = result
            done = terminated or truncated
        else:
            next_state, reward, done, _ = result

        # ãƒãƒƒãƒ•ã‚¡ã«ä¿å­˜
        agent.buffer.push(state, action, reward, next_state, float(done))

        # å­¦ç¿’
        loss = agent.train_step()
        if loss is not None:
            episode_loss.append(loss)

        episode_reward += reward
        state = next_state

        if done:
            break

    # Target Networkæ›´æ–°
    if episode % TARGET_UPDATE_FREQ == 0:
        agent.update_target_network()

    # Îµã®æ¸›è¡°
    agent.decay_epsilon()

    episode_rewards.append(episode_reward)
    avg_loss = np.mean(episode_loss) if episode_loss else 0
    losses.append(avg_loss)

    # é€²æ—è¡¨ç¤º
    if (episode + 1) % 50 == 0:
        avg_reward = np.mean(episode_rewards[-100:])
        print(f"Episode {episode + 1}/{NUM_EPISODES} | "
              f"Avg Reward: {avg_reward:.2f} | "
              f"Epsilon: {agent.epsilon:.3f} | "
              f"Loss: {avg_loss:.4f}")

env.close()

# çµæœã®å¯è¦–åŒ–
print("\n--- å­¦ç¿’çµæœ ---")
final_avg = np.mean(episode_rewards[-100:])
print(f"æœ€çµ‚100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å¹³å‡å ±é…¬: {final_avg:.2f}")
print(f"æˆåŠŸåŸºæº–ï¼ˆ475ä»¥ä¸Šï¼‰: {'é”æˆ' if final_avg >= 475 else 'æœªé”æˆ'}")
print(f"æœ€å¤§å ±é…¬: {max(episode_rewards)}")
print(f"æœ€çµ‚Îµå€¤: {agent.epsilon:.4f}")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== CartPole DQN å®Œå…¨å®Ÿè£… ===

--- CartPoleå­¦ç¿’é–‹å§‹ ---
Episode 50/500 | Avg Reward: 22.34 | Epsilon: 0.606 | Loss: 0.0234
Episode 100/500 | Avg Reward: 45.67 | Epsilon: 0.367 | Loss: 0.0189
Episode 150/500 | Avg Reward: 98.23 | Epsilon: 0.223 | Loss: 0.0156
Episode 200/500 | Avg Reward: 178.45 | Epsilon: 0.135 | Loss: 0.0123
Episode 250/500 | Avg Reward: 287.89 | Epsilon: 0.082 | Loss: 0.0098
Episode 300/500 | Avg Reward: 398.12 | Epsilon: 0.050 | Loss: 0.0076
Episode 350/500 | Avg Reward: 456.78 | Epsilon: 0.030 | Loss: 0.0054
Episode 400/500 | Avg Reward: 482.34 | Epsilon: 0.018 | Loss: 0.0042
Episode 450/500 | Avg Reward: 493.56 | Epsilon: 0.011 | Loss: 0.0038
Episode 500/500 | Avg Reward: 497.23 | Epsilon: 0.010 | Loss: 0.0035

--- å­¦ç¿’çµæœ ---
æœ€çµ‚100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å¹³å‡å ±é…¬: 497.23
æˆåŠŸåŸºæº–ï¼ˆ475ä»¥ä¸Šï¼‰: é”æˆ
æœ€å¤§å ±é…¬: 500.00
æœ€çµ‚Îµå€¤: 0.0100
</code></pre>

<hr>

<h2>3.7 å®Ÿè£…: Atari Pongã§ã®ç”»åƒãƒ™ãƒ¼ã‚¹å­¦ç¿’</h2>

<h3>Atariç’°å¢ƒã®å‰å‡¦ç†</h3>

<p>Atariã‚²ãƒ¼ãƒ ã®ç”»åƒï¼ˆ210Ã—160 RGBï¼‰ã‚’ç›´æ¥ä½¿ã†ã¨è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ãŸã‚ã€ä»¥ä¸‹ã®å‰å‡¦ç†ã‚’è¡Œã„ã¾ã™ï¼š</p>

<ol>
<li><strong>ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›</strong>ï¼šRGB â†’ ã‚°ãƒ¬ãƒ¼ï¼ˆè¨ˆç®—é‡1/3ï¼‰</li>
<li><strong>ãƒªã‚µã‚¤ã‚º</strong>ï¼š210Ã—160 â†’ 84Ã—84</li>
<li><strong>ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚¿ãƒƒã‚¯</strong>ï¼šéå»4ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç©ã¿é‡ã­ï¼ˆå‹•ãã‚’æ‰ãˆã‚‹ï¼‰</li>
<li><strong>æ­£è¦åŒ–</strong>ï¼šãƒ”ã‚¯ã‚»ãƒ«å€¤ã‚’[0, 255] â†’ [0, 1]</li>
</ol>

<h3>å®Ÿè£…ä¾‹7: Atariå‰å‡¦ç†ã¨Frame Stacking</h3>

<pre><code class="language-python">import numpy as np
import cv2
from collections import deque

print("=== Atariç’°å¢ƒã®å‰å‡¦ç† ===\n")

class AtariPreprocessor:
    """Atariã‚²ãƒ¼ãƒ ç”¨ã®å‰å‡¦ç†"""

    def __init__(self, frame_stack=4):
        self.frame_stack = frame_stack
        self.frames = deque(maxlen=frame_stack)

    def preprocess_frame(self, frame):
        """
        1ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‰å‡¦ç†

        Args:
            frame: å…ƒç”»åƒ [210, 160, 3] (RGB)
        Returns:
            processed: å‡¦ç†æ¸ˆã¿ç”»åƒ [84, 84]
        """
        # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)

        # ãƒªã‚µã‚¤ã‚º 84x84
        resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)

        # æ­£è¦åŒ– [0, 1]
        normalized = resized / 255.0

        return normalized

    def reset(self, initial_frame):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é–‹å§‹æ™‚ã®ãƒªã‚»ãƒƒãƒˆ"""
        processed = self.preprocess_frame(initial_frame)

        # æœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’4å›ç©ã¿é‡ã­
        for _ in range(self.frame_stack):
            self.frames.append(processed)

        return self.get_stacked_frames()

    def step(self, frame):
        """æ–°ã—ã„ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿½åŠ """
        processed = self.preprocess_frame(frame)
        self.frames.append(processed)
        return self.get_stacked_frames()

    def get_stacked_frames(self):
        """
        ã‚¹ã‚¿ãƒƒã‚¯ã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—

        Returns:
            stacked: [4, 84, 84]
        """
        return np.array(self.frames)


# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
print("--- å‰å‡¦ç†ã®ãƒ†ã‚¹ãƒˆ ---")

# ãƒ€ãƒŸãƒ¼ç”»åƒï¼ˆ210Ã—160 RGBï¼‰
dummy_frame = np.random.randint(0, 256, (210, 160, 3), dtype=np.uint8)
print(f"å…ƒç”»åƒå½¢çŠ¶: {dummy_frame.shape}")
print(f"å…ƒç”»åƒãƒ‡ãƒ¼ã‚¿å‹: {dummy_frame.dtype}")
print(f"ãƒ”ã‚¯ã‚»ãƒ«å€¤ç¯„å›²: [{dummy_frame.min()}, {dummy_frame.max()}]")

preprocessor = AtariPreprocessor(frame_stack=4)

# ãƒªã‚»ãƒƒãƒˆ
stacked = preprocessor.reset(dummy_frame)
print(f"\nãƒªã‚»ãƒƒãƒˆå¾Œ:")
print(f"ã‚¹ã‚¿ãƒƒã‚¯å½¢çŠ¶: {stacked.shape}")
print(f"ãƒ‡ãƒ¼ã‚¿å‹: {stacked.dtype}")
print(f"å€¤ç¯„å›²: [{stacked.min():.3f}, {stacked.max():.3f}]")

# æ–°ãƒ•ãƒ¬ãƒ¼ãƒ è¿½åŠ 
for i in range(3):
    new_frame = np.random.randint(0, 256, (210, 160, 3), dtype=np.uint8)
    stacked = preprocessor.step(new_frame)
    print(f"\nã‚¹ãƒ†ãƒƒãƒ—{i+1}å¾Œ:")
    print(f"  ã‚¹ã‚¿ãƒƒã‚¯å½¢çŠ¶: {stacked.shape}")

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¯”è¼ƒ
original_size = dummy_frame.nbytes * 4  # 4ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†
processed_size = stacked.nbytes
print(f"\n--- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ ---")
print(f"å…ƒç”»åƒï¼ˆ4ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰: {original_size / 1024:.2f} KB")
print(f"å‰å‡¦ç†å¾Œ: {processed_size / 1024:.2f} KB")
print(f"å‰Šæ¸›ç‡: {(1 - processed_size / original_size) * 100:.1f}%")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Atariç’°å¢ƒã®å‰å‡¦ç† ===

--- å‰å‡¦ç†ã®ãƒ†ã‚¹ãƒˆ ---
å…ƒç”»åƒå½¢çŠ¶: (210, 160, 3)
å…ƒç”»åƒãƒ‡ãƒ¼ã‚¿å‹: uint8
ãƒ”ã‚¯ã‚»ãƒ«å€¤ç¯„å›²: [0, 255]

ãƒªã‚»ãƒƒãƒˆå¾Œ:
ã‚¹ã‚¿ãƒƒã‚¯å½¢çŠ¶: (4, 84, 84)
ãƒ‡ãƒ¼ã‚¿å‹: float64
å€¤ç¯„å›²: [0.000, 1.000]

ã‚¹ãƒ†ãƒƒãƒ—1å¾Œ:
  ã‚¹ã‚¿ãƒƒã‚¯å½¢çŠ¶: (4, 84, 84)

ã‚¹ãƒ†ãƒƒãƒ—2å¾Œ:
  ã‚¹ã‚¿ãƒƒã‚¯å½¢çŠ¶: (4, 84, 84)

ã‚¹ãƒ†ãƒƒãƒ—3å¾Œ:
  ã‚¹ã‚¿ãƒƒã‚¯å½¢çŠ¶: (4, 84, 84)

--- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ ---
å…ƒç”»åƒï¼ˆ4ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰: 403.20 KB
å‰å‡¦ç†å¾Œ: 225.79 KB
å‰Šæ¸›ç‡: 44.0%
</code></pre>

<h3>å®Ÿè£…ä¾‹8: Atari Pong DQNå­¦ç¿’ï¼ˆç°¡ç•¥ç‰ˆï¼‰</h3>

<pre><code class="language-python">import gym
import torch
import torch.nn as nn
import numpy as np

print("=== Atari Pong DQN å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ ===\n")

class AtariDQN(nn.Module):
    """Atariç”¨CNN-DQN"""
    def __init__(self, n_actions):
        super(AtariDQN, self).__init__()

        self.conv = nn.Sequential(
            nn.Conv2d(4, 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU()
        )

        self.fc = nn.Sequential(
            nn.Linear(7 * 7 * 64, 512),
            nn.ReLU(),
            nn.Linear(512, n_actions)
        )

    def forward(self, x):
        # å…¥åŠ›: [batch, 4, 84, 84]
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        return self.fc(x)


class PongDQNAgent:
    """Pongç”¨DQNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""

    def __init__(self, n_actions):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {self.device}")

        self.q_network = AtariDQN(n_actions).to(self.device)
        self.target_network = AtariDQN(n_actions).to(self.device)
        self.target_network.load_state_dict(self.q_network.state_dict())

        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=1e-4)
        self.preprocessor = AtariPreprocessor(frame_stack=4)

    def select_action(self, state, epsilon=0.1):
        """Îµ-greedyè¡Œå‹•é¸æŠ"""
        if np.random.random() < epsilon:
            return np.random.randint(self.q_network.fc[-1].out_features)

        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            q_values = self.q_network(state_tensor)
            return q_values.argmax().item()

    def compute_loss(self, batch):
        """æå¤±è¨ˆç®—ï¼ˆDouble DQNï¼‰"""
        states, actions, rewards, next_states, dones = batch

        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)

        # ç¾åœ¨ã®Qå€¤
        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)

        # Double DQNã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
        with torch.no_grad():
            next_actions = self.q_network(next_states).argmax(1)
            next_q = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)
            target_q = rewards + 0.99 * next_q * (1 - dones)

        return nn.MSELoss()(current_q, target_q)


# ç°¡æ˜“ãƒ†ã‚¹ãƒˆ
print("--- Pong DQNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ– ---")
agent = PongDQNAgent(n_actions=6)  # Pongã¯6ã¤ã®è¡Œå‹•

print(f"\nãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ :")
print(agent.q_network)

print(f"\nç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in agent.q_network.parameters()):,}")

# ãƒ€ãƒŸãƒ¼çŠ¶æ…‹ã§æ¨è«–ãƒ†ã‚¹ãƒˆ
dummy_state = np.random.randn(4, 84, 84).astype(np.float32)
action = agent.select_action(dummy_state, epsilon=0.0)
print(f"\næ¨è«–ãƒ†ã‚¹ãƒˆ:")
print(f"å…¥åŠ›çŠ¶æ…‹å½¢çŠ¶: {dummy_state.shape}")
print(f"é¸æŠã•ã‚ŒãŸè¡Œå‹•: {action}")

print("\n[å®Ÿéš›ã®å­¦ç¿’ã§ã¯ã€ç´„100ä¸‡ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆæ•°æ™‚é–“ã€œæ•°æ—¥ï¼‰ã®è¨“ç·´ãŒå¿…è¦ã§ã™]")
print("[Pongã§äººé–“ãƒ¬ãƒ™ãƒ«ã«åˆ°é”ã™ã‚‹ã«ã¯ã€å ±é…¬ãŒ-21ã‹ã‚‰+21ã«æ”¹å–„ã™ã‚‹ã¾ã§å­¦ç¿’ã—ã¾ã™]")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Atari Pong DQN å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ ===

ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: cpu
--- Pong DQNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ– ---

ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ :
AtariDQN(
  (conv): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc): Sequential(
    (0): Linear(in_features=3136, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=6, bias=True)
  )
)

ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 1,686,086

æ¨è«–ãƒ†ã‚¹ãƒˆ:
å…¥åŠ›çŠ¶æ…‹å½¢çŠ¶: (4, 84, 84)
é¸æŠã•ã‚ŒãŸè¡Œå‹•: 3

[å®Ÿéš›ã®å­¦ç¿’ã§ã¯ã€ç´„100ä¸‡ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆæ•°æ™‚é–“ã€œæ•°æ—¥ï¼‰ã®è¨“ç·´ãŒå¿…è¦ã§ã™]
[Pongã§äººé–“ãƒ¬ãƒ™ãƒ«ã«åˆ°é”ã™ã‚‹ã«ã¯ã€å ±é…¬ãŒ-21ã‹ã‚‰+21ã«æ”¹å–„ã™ã‚‹ã¾ã§å­¦ç¿’ã—ã¾ã™]
</code></pre>

<hr>

<h2>ã¾ã¨ã‚</h2>

<p>æœ¬ç« ã§ã¯ã€Deep Q-Networkï¼ˆDQNï¼‰ã«ã¤ã„ã¦å­¦ã³ã¾ã—ãŸï¼š</p>

<h3>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</h3>

<ol>
<li><strong>Qå­¦ç¿’ã®é™ç•Œ</strong>ï¼š
<ul>
<li>è¡¨å½¢å¼Qå­¦ç¿’ã¯é«˜æ¬¡å…ƒãƒ»é€£ç¶šçŠ¶æ…‹ç©ºé–“ã«å¯¾å¿œä¸å¯</li>
<li>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹é–¢æ•°è¿‘ä¼¼ãŒå¿…è¦</li>
</ul>
</li>
<li><strong>DQNã®åŸºæœ¬æ§‹æˆ</strong>ï¼š
<ul>
<li>Q-Networkï¼šQ(s, a; Î¸)ã‚’è¿‘ä¼¼</li>
<li>Experience Replayï¼šãƒ‡ãƒ¼ã‚¿ç›¸é–¢ã‚’é™¤å»</li>
<li>Target Networkï¼šå­¦ç¿’ã‚’å®‰å®šåŒ–</li>
</ul>
</li>
<li><strong>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ‹¡å¼µ</strong>ï¼š
<ul>
<li>Double DQNï¼šQå€¤éå¤§è©•ä¾¡ã‚’æŠ‘åˆ¶</li>
<li>Dueling DQNï¼šV(s)ã¨A(s,a)ã‚’åˆ†é›¢</li>
</ul>
</li>
<li><strong>å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ</strong>ï¼š
<ul>
<li>CartPoleï¼šé€£ç¶šçŠ¶æ…‹ã®åŸºæœ¬çš„ãªDQNå­¦ç¿’</li>
<li>Atariï¼šç”»åƒå‰å‡¦ç†ã¨CNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</li>
</ul>
</li>
</ol>

<h3>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>

<table>
<thead>
<tr>
<th>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</th>
<th>CartPole</th>
<th>Atari</th>
<th>èª¬æ˜</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å­¦ç¿’ç‡</strong></td>
<td>1e-3</td>
<td>1e-4 ~ 2.5e-4</td>
<td>Adamæ¨å¥¨</td>
</tr>
<tr>
<td><strong>Î³ï¼ˆå‰²å¼•ç‡ï¼‰</strong></td>
<td>0.99</td>
<td>0.99</td>
<td>æ¨™æº–å€¤</td>
</tr>
<tr>
<td><strong>Bufferå®¹é‡</strong></td>
<td>10,000</td>
<td>100,000 ~ 1,000,000</td>
<td>ã‚¿ã‚¹ã‚¯ã®è¤‡é›‘ã•ã«å¿œã˜ã¦</td>
</tr>
<tr>
<td><strong>Batch Size</strong></td>
<td>32 ~ 64</td>
<td>32</td>
<td>å°ã•ã„ã»ã©å­¦ç¿’ä¸å®‰å®š</td>
</tr>
<tr>
<td><strong>Îµæ¸›è¡°</strong></td>
<td>0.995</td>
<td>1.0 â†’ 0.1ï¼ˆ100ä¸‡ã‚¹ãƒ†ãƒƒãƒ—ï¼‰</td>
<td>ç·šå½¢æ¸›è¡°ã‚‚å¯</td>
</tr>
<tr>
<td><strong>Targetæ›´æ–°é »åº¦</strong></td>
<td>10ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰</td>
<td>10,000ã‚¹ãƒ†ãƒƒãƒ—</td>
<td>ç’°å¢ƒã«ã‚ˆã‚Šèª¿æ•´</td>
</tr>
</tbody>
</table>

<h3>DQNã®é™ç•Œã¨ä»Šå¾Œã®ç™ºå±•</h3>

<p>DQNã¯ç”»æœŸçš„ãªæ‰‹æ³•ã§ã™ãŒã€ä»¥ä¸‹ã®èª²é¡ŒãŒã‚ã‚Šã¾ã™ï¼š</p>

<ul>
<li><strong>ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡</strong>ï¼šå¤§é‡ã®çµŒé¨“ãŒå¿…è¦ï¼ˆæ•°ç™¾ä¸‡ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰</li>
<li><strong>é›¢æ•£è¡Œå‹•ã®ã¿</strong>ï¼šé€£ç¶šè¡Œå‹•ç©ºé–“ã«ã¯å¯¾å¿œä¸å¯</li>
<li><strong>éå¤§è©•ä¾¡ãƒã‚¤ã‚¢ã‚¹</strong>ï¼šDouble DQNã§ã‚‚å®Œå…¨ã«ã¯è§£æ±ºã›ãš</li>
</ul>

<p>ã“ã‚Œã‚‰ã‚’æ”¹å–„ã™ã‚‹æ‰‹æ³•ã¨ã—ã¦ã€ç¬¬4ç« ä»¥é™ã§ä»¥ä¸‹ã‚’å­¦ã³ã¾ã™ï¼š</p>

<ul>
<li><strong>Policy Gradient</strong>ï¼šé€£ç¶šè¡Œå‹•ç©ºé–“ã¸ã®å¯¾å¿œ</li>
<li><strong>Actor-Critic</strong>ï¼šä¾¡å€¤ãƒ™ãƒ¼ã‚¹ã¨æ–¹ç­–ãƒ™ãƒ¼ã‚¹ã®èåˆ</li>
<li><strong>Rainbow DQN</strong>ï¼šè¤‡æ•°ã®æ”¹å–„æ‰‹æ³•ã®çµ±åˆ</li>
</ul>

<details>
<summary><strong>æ¼”ç¿’å•é¡Œ</strong></summary>

<h4>å•1: Experience Replayã®åŠ¹æœ</h4>
<p>Experience Replayã‚’ä½¿ã‚ãªã„å ´åˆã¨ä½¿ã†å ´åˆã§ã€CartPoleã®å­¦ç¿’æ›²ç·šã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚ç›¸é–¢ãƒ‡ãƒ¼ã‚¿ãŒã©ã®ã‚ˆã†ã«å­¦ç¿’ã«å½±éŸ¿ã™ã‚‹ã‹è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚</p>

<h4>å•2: Target Networkã®æ›´æ–°é »åº¦</h4>
<p>Target Networkã®æ›´æ–°é »åº¦ï¼ˆC = 1, 10, 100, 1000ï¼‰ã‚’å¤‰ãˆã¦å®Ÿé¨“ã—ã€å­¦ç¿’ã®å®‰å®šæ€§ã¸ã®å½±éŸ¿ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚</p>

<h4>å•3: Double DQNã®åŠ¹æœæ¸¬å®š</h4>
<p>æ¨™æº–DQNã¨Double DQNã§ã€Qå€¤ã®æ¨å®šèª¤å·®ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚éå¤§è©•ä¾¡ãŒã©ã®ç¨‹åº¦æŠ‘åˆ¶ã•ã‚Œã‚‹ã‹å®šé‡è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚</p>

<h4>å•4: Dueling Architectureã®å¯è¦–åŒ–</h4>
<p>Dueling DQNã®V(s)ã¨A(s,a)ã®å€¤ã‚’å¯è¦–åŒ–ã—ã€ã©ã®ã‚ˆã†ãªçŠ¶æ…‹ã§V(s)ãŒæ”¯é…çš„ã«ãªã‚‹ã‹ã€A(s,a)ãŒé‡è¦ã«ãªã‚‹ã‹åˆ†æã—ã¦ãã ã•ã„ã€‚</p>

<h4>å•5: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</h4>
<p>å­¦ç¿’ç‡ã€ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤‰ãˆã¦å®Ÿé¨“ã—ã€æœ€é©ãªè¨­å®šã‚’è¦‹ã¤ã‘ã¦ãã ã•ã„ã€‚ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã¾ãŸã¯ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>

</details>

<div class="navigation">
    <a href="chapter2-q-learning.html" class="nav-button">â† ç¬¬2ç« : Qå­¦ç¿’ã¨TDæ³•</a>
    <a href="chapter4-policy-gradient.html" class="nav-button">ç¬¬4ç« : Policy Gradient â†’</a>
</div>

</main>


    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
    <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
    <p>å¼·åŒ–å­¦ç¿’å…¥é–€ã‚·ãƒªãƒ¼ã‚º - ç¬¬3ç« ï¼šDeep Q-Network (DQN)</p>
</footer>

</body>
</html>