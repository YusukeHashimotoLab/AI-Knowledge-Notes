<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬1ç« ï¼šå¼·åŒ–å­¦ç¿’ã®åŸºç¤ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
        <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/wp/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/ML/reinforcement-learning-introduction/index.html">Reinforcement Learning</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 1</span>
        </div>
    </nav>

    <header>
        <div class="header-content">
            <h1>ç¬¬1ç« ï¼šå¼·åŒ–å­¦ç¿’ã®åŸºç¤</h1>
            <p class="subtitle">å¼·åŒ–å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µã€ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ã€ä¾¡å€¤é–¢æ•°ã¨ãƒ™ãƒ«ãƒãƒ³æ–¹ç¨‹å¼ã€åŸºæœ¬ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç†è§£</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´šã€œä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 6å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… å¼·åŒ–å­¦ç¿’ã¨æ•™å¸«ã‚ã‚Šå­¦ç¿’ãƒ»æ•™å¸«ãªã—å­¦ç¿’ã®é•ã„ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼ˆMDPï¼‰ã®åŸºæœ¬æ¦‚å¿µï¼ˆçŠ¶æ…‹ã€è¡Œå‹•ã€å ±é…¬ã€é·ç§»ï¼‰ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>âœ… ãƒ™ãƒ«ãƒãƒ³æ–¹ç¨‹å¼ã®æ„å‘³ã¨å½¹å‰²ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… ä¾¡å€¤é–¢æ•°ï¼ˆVï¼‰ã¨è¡Œå‹•ä¾¡å€¤é–¢æ•°ï¼ˆQï¼‰ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>âœ… æ–¹ç­–ï¼ˆPolicyï¼‰ã®æ¦‚å¿µã¨æœ€é©æ–¹ç­–ã®å®šç¾©ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… æ¢ç´¢ï¼ˆExplorationï¼‰ã¨æ´»ç”¨ï¼ˆExploitationï¼‰ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… ä¾¡å€¤åå¾©æ³•ï¼ˆValue Iterationï¼‰ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… æ–¹ç­–åå¾©æ³•ï¼ˆPolicy Iterationï¼‰ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã®åŸºæœ¬åŸç†ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… TDå­¦ç¿’ï¼ˆTemporal Differenceï¼‰ã®ä»•çµ„ã¿ã‚’ç†è§£ã—å®Ÿè£…ã§ãã‚‹</li>
</ul>

<hr>

<h2>1.1 å¼·åŒ–å­¦ç¿’ã¨ã¯ä½•ã‹</h2>

<h3>å¼·åŒ–å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µ</h3>

<p>å¼·åŒ–å­¦ç¿’ï¼ˆReinforcement Learningï¼‰ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç’°å¢ƒã¨ç›¸äº’ä½œç”¨ã—ãªãŒã‚‰ã€è©¦è¡ŒéŒ¯èª¤ã‚’é€šã˜ã¦æœ€é©ãªè¡Œå‹•ã‚’å­¦ç¿’ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’ã®ä¸€åˆ†é‡ã§ã™ã€‚ã‚²ãƒ¼ãƒ AIã€ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡ã€è‡ªå‹•é‹è»¢ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ãªã©ã€å¹…åºƒã„åˆ†é‡ã§å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚</p>

<blockquote>
<p>ã€Œå¼·åŒ–å­¦ç¿’ã¯ã€å ±é…¬ä¿¡å·ã‚’æœ€å¤§åŒ–ã™ã‚‹ãŸã‚ã«ã€ã©ã®ã‚ˆã†ãªè¡Œå‹•ã‚’å–ã‚‹ã¹ãã‹ã‚’å­¦ç¿’ã™ã‚‹å•é¡Œã§ã‚ã‚‹ã€‚ã€</p>
</blockquote>

<h4>å¼·åŒ–å­¦ç¿’ã®æ§‹æˆè¦ç´ </h4>

<ul>
<li><strong>ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆAgentï¼‰</strong>ï¼šå­¦ç¿’ã—è¡Œå‹•ã‚’æ±ºå®šã™ã‚‹ä¸»ä½“</li>
<li><strong>ç’°å¢ƒï¼ˆEnvironmentï¼‰</strong>ï¼šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç›¸äº’ä½œç”¨ã™ã‚‹å¯¾è±¡</li>
<li><strong>çŠ¶æ…‹ï¼ˆStateï¼‰</strong>ï¼šç’°å¢ƒã®ç¾åœ¨ã®çŠ¶æ³ã‚’è¡¨ã™æƒ…å ±</li>
<li><strong>è¡Œå‹•ï¼ˆActionï¼‰</strong>ï¼šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒé¸æŠã§ãã‚‹å‹•ä½œ</li>
<li><strong>å ±é…¬ï¼ˆRewardï¼‰</strong>ï¼šè¡Œå‹•ã®è‰¯ã—æ‚ªã—ã‚’ç¤ºã™å³æ™‚çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯</li>
<li><strong>æ–¹ç­–ï¼ˆPolicyï¼‰</strong>ï¼šçŠ¶æ…‹ã‹ã‚‰è¡Œå‹•ã¸ã®å†™åƒï¼ˆæ„æ€æ±ºå®šãƒ«ãƒ¼ãƒ«ï¼‰</li>
</ul>

<div class="mermaid">
graph LR
    A[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ] -->|è¡Œå‹• At| B[ç’°å¢ƒ]
    B -->|çŠ¶æ…‹ St+1| A
    B -->|å ±é…¬ Rt+1| A

    style A fill:#e3f2fd
    style B fill:#fff3e0
</div>

<h3>å¼·åŒ–å­¦ç¿’ vs æ•™å¸«ã‚ã‚Šå­¦ç¿’ vs æ•™å¸«ãªã—å­¦ç¿’</h3>

<table>
<thead>
<tr>
<th>å­¦ç¿’æ–¹æ³•</th>
<th>ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´</th>
<th>å­¦ç¿’ã®ç›®çš„</th>
<th>å…·ä½“ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ•™å¸«ã‚ã‚Šå­¦ç¿’</strong></td>
<td>å…¥åŠ›ã¨æ­£è§£ãƒ©ãƒ™ãƒ«ã®ãƒšã‚¢</td>
<td>æ­£è§£ã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰</td>
<td>ç”»åƒåˆ†é¡ã€éŸ³å£°èªè­˜</td>
</tr>
<tr>
<td><strong>æ•™å¸«ãªã—å­¦ç¿’</strong></td>
<td>ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿</td>
<td>ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã‚„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç™ºè¦‹</td>
<td>ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã€æ¬¡å…ƒå‰Šæ¸›</td>
</tr>
<tr>
<td><strong>å¼·åŒ–å­¦ç¿’</strong></td>
<td>è¡Œå‹•ã¨å ±é…¬ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯</td>
<td>ç´¯ç©å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹æ–¹ç­–ã‚’å­¦ç¿’</td>
<td>ã‚²ãƒ¼ãƒ AIã€ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡</td>
</tr>
</tbody>
</table>

<h4>å¼·åŒ–å­¦ç¿’ã®ç‰¹å¾´</h4>

<ol>
<li><strong>è©¦è¡ŒéŒ¯èª¤ã«ã‚ˆã‚‹å­¦ç¿’</strong>ï¼šæ­£è§£ã¯ä¸ãˆã‚‰ã‚Œãšã€å ±é…¬ä¿¡å·ã‹ã‚‰å­¦ç¿’</li>
<li><strong>é…å»¶å ±é…¬</strong>ï¼šè¡Œå‹•ã®çµæœãŒå³åº§ã«åˆ†ã‹ã‚‰ãªã„ã“ã¨ãŒå¤šã„</li>
<li><strong>æ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•</strong>ï¼šæ–°ã—ã„è¡Œå‹•ã‚’è©¦ã™ã‹ã€æ—¢çŸ¥ã®è‰¯ã„è¡Œå‹•ã‚’å–ã‚‹ã‹</li>
<li><strong>é€æ¬¡çš„æ„æ€æ±ºå®š</strong>ï¼šéå»ã®è¡Œå‹•ãŒæœªæ¥ã®çŠ¶æ…‹ã«å½±éŸ¿ã™ã‚‹</li>
</ol>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

print("=== å¼·åŒ–å­¦ç¿’ vs æ•™å¸«ã‚ã‚Šå­¦ç¿’ã®æ¯”è¼ƒ ===\n")

# æ•™å¸«ã‚ã‚Šå­¦ç¿’ã®ä¾‹ï¼šå˜ç´”ãªå›å¸°
print("ã€æ•™å¸«ã‚ã‚Šå­¦ç¿’ã€‘")
print("ç›®çš„: ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é–¢æ•°ã‚’å­¦ç¿’ã™ã‚‹")
X_train = np.array([1, 2, 3, 4, 5])
y_train = np.array([2, 4, 6, 8, 10])  # y = 2x ã®é–¢ä¿‚

# æœ€å°äºŒä¹—æ³•ã§å­¦ç¿’
slope = np.sum((X_train - X_train.mean()) * (y_train - y_train.mean())) / \
        np.sum((X_train - X_train.mean())**2)
intercept = y_train.mean() - slope * X_train.mean()

print(f"å­¦ç¿’çµæœ: y = {slope:.2f}x + {intercept:.2f}")
print("ç‰¹å¾´: æ­£è§£ï¼ˆy_trainï¼‰ãŒæ˜ç¤ºçš„ã«ä¸ãˆã‚‰ã‚Œã‚‹\n")

# å¼·åŒ–å­¦ç¿’ã®ä¾‹ï¼šç°¡å˜ãªãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œ
print("ã€å¼·åŒ–å­¦ç¿’ã€‘")
print("ç›®çš„: å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹è¡Œå‹•ã‚’å­¦ç¿’ã™ã‚‹")

class SimpleBandit:
    """3æœ¬è…•ã®ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œ"""
    def __init__(self):
        # å„è…•ã®çœŸã®æœŸå¾…å ±é…¬ï¼ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã¯æœªçŸ¥ï¼‰
        self.true_values = np.array([0.3, 0.5, 0.7])

    def pull(self, action):
        """è…•ã‚’å¼•ã„ã¦å ±é…¬ã‚’å¾—ã‚‹"""
        # ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤åˆ†å¸ƒã‹ã‚‰å ±é…¬ã‚’ç”Ÿæˆ
        reward = 1 if np.random.rand() < self.true_values[action] else 0
        return reward

# Îµ-greedy ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§å­¦ç¿’
bandit = SimpleBandit()
n_arms = 3
n_steps = 1000
epsilon = 0.1

Q = np.zeros(n_arms)  # å„è…•ã®æ¨å®šä¾¡å€¤
N = np.zeros(n_arms)  # å„è…•ã‚’å¼•ã„ãŸå›æ•°
rewards = []

for step in range(n_steps):
    # Îµ-greedy æ–¹ç­–ã§è¡Œå‹•é¸æŠ
    if np.random.rand() < epsilon:
        action = np.random.randint(n_arms)  # æ¢ç´¢
    else:
        action = np.argmax(Q)  # æ´»ç”¨

    # å ±é…¬ã‚’å¾—ã‚‹
    reward = bandit.pull(action)
    rewards.append(reward)

    # Qå€¤ã‚’æ›´æ–°
    N[action] += 1
    Q[action] += (reward - Q[action]) / N[action]

print(f"çœŸã®æœŸå¾…å ±é…¬: {bandit.true_values}")
print(f"å­¦ç¿’ã—ãŸæ¨å®šå€¤: {Q}")
print(f"å¹³å‡å ±é…¬: {np.mean(rewards):.3f}")
print("ç‰¹å¾´: æ­£è§£ã¯ä¸æ˜ã€å ±é…¬ä¿¡å·ã‹ã‚‰è©¦è¡ŒéŒ¯èª¤ã§å­¦ç¿’\n")

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# å·¦ï¼šæ•™å¸«ã‚ã‚Šå­¦ç¿’
axes[0].scatter(X_train, y_train, s=100, alpha=0.6, label='è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆæ­£è§£ã‚ã‚Šï¼‰')
X_test = np.linspace(0, 6, 100)
y_pred = slope * X_test + intercept
axes[0].plot(X_test, y_pred, 'r-', linewidth=2, label=f'å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«')
axes[0].set_xlabel('å…¥åŠ› X', fontsize=12)
axes[0].set_ylabel('å‡ºåŠ› y', fontsize=12)
axes[0].set_title('æ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼šæ­£è§£ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’', fontsize=14, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# å³ï¼šå¼·åŒ–å­¦ç¿’
window = 50
cumulative_rewards = np.convolve(rewards, np.ones(window)/window, mode='valid')
axes[1].plot(cumulative_rewards, linewidth=2)
axes[1].axhline(y=max(bandit.true_values), color='r', linestyle='--',
                linewidth=2, label=f'æœ€é©å ±é…¬ ({max(bandit.true_values):.1f})')
axes[1].set_xlabel('ã‚¹ãƒ†ãƒƒãƒ—æ•°', fontsize=12)
axes[1].set_ylabel('å¹³å‡å ±é…¬ï¼ˆç§»å‹•å¹³å‡ï¼‰', fontsize=12)
axes[1].set_title('å¼·åŒ–å­¦ç¿’ï¼šè©¦è¡ŒéŒ¯èª¤ã§æœ€é©è¡Œå‹•ã‚’å­¦ç¿’', fontsize=14, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('rl_vs_supervised.png', dpi=150, bbox_inches='tight')
print("å¯è¦–åŒ–ã‚’ 'rl_vs_supervised.png' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
</code></pre>

<hr>

<h2>1.2 ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼ˆMDPï¼‰</h2>

<h3>MDPã®å®šç¾©</h3>

<p>ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼ˆMarkov Decision Processï¼‰ã¯ã€å¼·åŒ–å­¦ç¿’ã®æ•°å­¦çš„åŸºç›¤ã‚’æä¾›ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚MDPã¯5ã¤çµ„ $(S, A, P, R, \gamma)$ ã§å®šç¾©ã•ã‚Œã¾ã™ï¼š</p>

<ul>
<li>$S$ï¼šçŠ¶æ…‹ç©ºé–“ï¼ˆState spaceï¼‰</li>
<li>$A$ï¼šè¡Œå‹•ç©ºé–“ï¼ˆAction spaceï¼‰</li>
<li>$P$ï¼šçŠ¶æ…‹é·ç§»ç¢ºç‡ $P(s'|s, a)$</li>
<li>$R$ï¼šå ±é…¬é–¢æ•° $R(s, a, s')$</li>
<li>$\gamma$ï¼šå‰²å¼•ç‡ï¼ˆDiscount factorï¼‰$\in [0, 1]$</li>
</ul>

<blockquote>
<p>ã€Œãƒãƒ«ã‚³ãƒ•æ€§ï¼šæ¬¡ã®çŠ¶æ…‹ã¯ç¾åœ¨ã®çŠ¶æ…‹ã¨è¡Œå‹•ã®ã¿ã«ä¾å­˜ã—ã€éå»ã®å±¥æ­´ã«ã¯ä¾å­˜ã—ãªã„ã€‚ã€</p>
</blockquote>

<h4>ãƒãƒ«ã‚³ãƒ•æ€§ã®æ•°å­¦çš„è¡¨ç¾</h4>

<p>çŠ¶æ…‹ $s$ ãŒãƒãƒ«ã‚³ãƒ•æ€§ã‚’æº€ãŸã™ã¨ãï¼š</p>

$$
P(S_{t+1}|S_t, A_t, S_{t-1}, A_{t-1}, \ldots, S_0, A_0) = P(S_{t+1}|S_t, A_t)
$$

<h3>çŠ¶æ…‹ã€è¡Œå‹•ã€å ±é…¬ã€é·ç§»</h3>

<h4>1. çŠ¶æ…‹ï¼ˆStateï¼‰</h4>

<p>çŠ¶æ…‹ã¯ç’°å¢ƒã®ç¾åœ¨ã®çŠ¶æ³ã‚’è¡¨ã™æƒ…å ±ã§ã™ï¼š</p>

<ul>
<li><strong>å®Œå…¨è¦³æ¸¬</strong>ï¼šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç’°å¢ƒã®å…¨æƒ…å ±ã‚’è¦³æ¸¬ã§ãã‚‹ï¼ˆä¾‹ï¼šãƒã‚§ã‚¹ï¼‰</li>
<li><strong>éƒ¨åˆ†è¦³æ¸¬</strong>ï¼šä¸€éƒ¨ã®æƒ…å ±ã®ã¿è¦³æ¸¬å¯èƒ½ï¼ˆä¾‹ï¼šãƒãƒ¼ã‚«ãƒ¼ï¼‰</li>
</ul>

<h4>2. è¡Œå‹•ï¼ˆActionï¼‰</h4>

<ul>
<li><strong>é›¢æ•£è¡Œå‹•ç©ºé–“</strong>ï¼šæœ‰é™å€‹ã®è¡Œå‹•ï¼ˆä¾‹ï¼šä¸Šä¸‹å·¦å³ã®ç§»å‹•ï¼‰</li>
<li><strong>é€£ç¶šè¡Œå‹•ç©ºé–“</strong>ï¼šå®Ÿæ•°å€¤ã®è¡Œå‹•ï¼ˆä¾‹ï¼šãƒ­ãƒœãƒƒãƒˆã®é–¢ç¯€è§’åº¦ï¼‰</li>
</ul>

<h4>3. å ±é…¬ï¼ˆRewardï¼‰</h4>

<p>å ±é…¬ã¯ã€æ™‚åˆ» $t$ ã§ã®è¡Œå‹•ã®è‰¯ã•ã‚’ç¤ºã™ã‚¹ã‚«ãƒ©ãƒ¼å€¤ $r_t$ ã§ã™ã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç›®æ¨™ã¯ç´¯ç©å ±é…¬ï¼ˆãƒªã‚¿ãƒ¼ãƒ³ï¼‰ã®æœŸå¾…å€¤ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã§ã™ï¼š</p>

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

<h4>4. å‰²å¼•ç‡ $\gamma$</h4>

<ul>
<li>$\gamma = 0$ï¼šå³æ™‚å ±é…¬ã®ã¿ã‚’è€ƒæ…®ï¼ˆè¿‘è¦–çœ¼çš„ï¼‰</li>
<li>$\gamma = 1$ï¼šå…¨ã¦ã®å°†æ¥å ±é…¬ã‚’ç­‰ã—ãé‡è¦–</li>
<li>$0 < \gamma < 1$ï¼šå°†æ¥ã®å ±é…¬ã‚’å‰²ã‚Šå¼•ãï¼ˆä¸€èˆ¬çš„ã«ã¯0.9ã€œ0.99ï¼‰</li>
</ul>

<div class="mermaid">
graph TD
    S0[çŠ¶æ…‹ S0] -->|è¡Œå‹• a0| S1[çŠ¶æ…‹ S1]
    S1 -->|å ±é…¬ r1| S0
    S1 -->|è¡Œå‹• a1| S2[çŠ¶æ…‹ S2]
    S2 -->|å ±é…¬ r2| S1
    S2 -->|è¡Œå‹• a2| S3[çŠ¶æ…‹ S3]
    S3 -->|å ±é…¬ r3| S2

    style S0 fill:#e3f2fd
    style S1 fill:#fff3e0
    style S2 fill:#e8f5e9
    style S3 fill:#fce4ec
</div>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

print("=== ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼ˆMDPï¼‰ã®åŸºæœ¬ ===\n")

class SimpleMDP:
    """
    ç°¡å˜ãªMDPã®ä¾‹ï¼š3x3ã®ã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰

    çŠ¶æ…‹: (x, y) åº§æ¨™
    è¡Œå‹•: ä¸Šä¸‹å·¦å³ã®ç§»å‹•ï¼ˆ0:ä¸Š, 1:å³, 2:ä¸‹, 3:å·¦ï¼‰
    å ±é…¬: ã‚´ãƒ¼ãƒ«åˆ°é”ã§+1ã€ãã®ä»–0
    """
    def __init__(self):
        self.grid_size = 3
        self.start_state = (0, 0)
        self.goal_state = (2, 2)
        self.actions = [(-1, 0), (0, 1), (1, 0), (0, -1)]  # ä¸Šå³ä¸‹å·¦
        self.action_names = ['ä¸Š', 'å³', 'ä¸‹', 'å·¦']

    def is_valid_state(self, state):
        """çŠ¶æ…‹ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯"""
        x, y = state
        return 0 <= x < self.grid_size and 0 <= y < self.grid_size

    def step(self, state, action):
        """
        çŠ¶æ…‹é·ç§»é–¢æ•°

        Returns:
        --------
        next_state : tuple
        reward : float
        done : bool
        """
        if state == self.goal_state:
            return state, 0, True

        # æ¬¡ã®çŠ¶æ…‹ã‚’è¨ˆç®—
        dx, dy = self.actions[action]
        next_state = (state[0] + dx, state[1] + dy)

        # å£ã«ã¶ã¤ã‹ã‚‹å ´åˆã¯å…ƒã®çŠ¶æ…‹ã«ç•™ã¾ã‚‹
        if not self.is_valid_state(next_state):
            next_state = state

        # å ±é…¬ã‚’è¨ˆç®—
        reward = 1.0 if next_state == self.goal_state else 0.0
        done = (next_state == self.goal_state)

        return next_state, reward, done

    def get_all_states(self):
        """å…¨ã¦ã®çŠ¶æ…‹ã‚’å–å¾—"""
        return [(x, y) for x in range(self.grid_size)
                for y in range(self.grid_size)]

# MDPã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
mdp = SimpleMDP()

print("ã€MDP ã®æ§‹æˆè¦ç´ ã€‘")
print(f"çŠ¶æ…‹ç©ºé–“ S: {mdp.get_all_states()}")
print(f"è¡Œå‹•ç©ºé–“ A: {mdp.action_names}")
print(f"é–‹å§‹çŠ¶æ…‹: {mdp.start_state}")
print(f"ã‚´ãƒ¼ãƒ«çŠ¶æ…‹: {mdp.goal_state}\n")

# ãƒãƒ«ã‚³ãƒ•æ€§ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("ã€ãƒãƒ«ã‚³ãƒ•æ€§ã®æ¤œè¨¼ã€‘")
current_state = (1, 1)
action = 1  # å³

print(f"ç¾åœ¨ã®çŠ¶æ…‹: {current_state}")
print(f"é¸æŠã—ãŸè¡Œå‹•: {mdp.action_names[action]}")

# çŠ¶æ…‹é·ç§»ã‚’å®Ÿè¡Œ
next_state, reward, done = mdp.step(current_state, action)
print(f"æ¬¡ã®çŠ¶æ…‹: {next_state}")
print(f"å ±é…¬: {reward}")
print(f"çµ‚äº†: {done}")
print("\nâ†’ æ¬¡ã®çŠ¶æ…‹ã¯ç¾åœ¨ã®çŠ¶æ…‹ã¨è¡Œå‹•ã®ã¿ã§æ±ºã¾ã‚‹ï¼ˆãƒãƒ«ã‚³ãƒ•æ€§ï¼‰\n")

# å‰²å¼•ç‡ã®å½±éŸ¿ã‚’å¯è¦–åŒ–
print("ã€å‰²å¼•ç‡ Î³ ã®å½±éŸ¿ã€‘")
gammas = [0.0, 0.5, 0.9, 0.99]
rewards = np.array([1, 1, 1, 1, 1])  # 5ã‚¹ãƒ†ãƒƒãƒ—é€£ç¶šã§å ±é…¬1

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# å·¦ï¼šç´¯ç©å ±é…¬ã®è¨ˆç®—
axes[0].set_title('å‰²å¼•ç‡ã«ã‚ˆã‚‹ç´¯ç©å ±é…¬ã®é•ã„', fontsize=14, fontweight='bold')
for gamma in gammas:
    discounted_rewards = [gamma**i * r for i, r in enumerate(rewards)]
    cumulative = np.cumsum(discounted_rewards)
    axes[0].plot(cumulative, marker='o', label=f'Î³={gamma}', linewidth=2)

axes[0].set_xlabel('ã‚¹ãƒ†ãƒƒãƒ—æ•°', fontsize=12)
axes[0].set_ylabel('ç´¯ç©å ±é…¬', fontsize=12)
axes[0].legend()
axes[0].grid(alpha=0.3)

# å³ï¼šå„ã‚¹ãƒ†ãƒƒãƒ—ã§ã®é‡ã¿
axes[1].set_title('å„ã‚¹ãƒ†ãƒƒãƒ—ã®å ±é…¬ã«å¯¾ã™ã‚‹é‡ã¿', fontsize=14, fontweight='bold')
steps = np.arange(10)
for gamma in gammas:
    weights = [gamma**i for i in steps]
    axes[1].plot(steps, weights, marker='o', label=f'Î³={gamma}', linewidth=2)

axes[1].set_xlabel('å°†æ¥ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°', fontsize=12)
axes[1].set_ylabel('é‡ã¿ (Î³^k)', fontsize=12)
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('mdp_basics.png', dpi=150, bbox_inches='tight')
print("å¯è¦–åŒ–ã‚’ 'mdp_basics.png' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
</code></pre>

<hr>

<h2>1.3 ãƒ™ãƒ«ãƒãƒ³æ–¹ç¨‹å¼ã¨ä¾¡å€¤é–¢æ•°</h2>

<h3>ä¾¡å€¤é–¢æ•°ï¼ˆValue Functionï¼‰</h3>

<p>ä¾¡å€¤é–¢æ•°ã¯ã€ã‚ã‚‹çŠ¶æ…‹ã¾ãŸã¯çŠ¶æ…‹-è¡Œå‹•ãƒšã‚¢ãŒã©ã‚Œã ã‘è‰¯ã„ã‹ã‚’è©•ä¾¡ã™ã‚‹é–¢æ•°ã§ã™ã€‚</p>

<h4>çŠ¶æ…‹ä¾¡å€¤é–¢æ•° $V^\pi(s)$</h4>

<p>æ–¹ç­– $\pi$ ã«å¾“ã£ãŸã¨ãã®ã€çŠ¶æ…‹ $s$ ã‹ã‚‰ã®æœŸå¾…ç´¯ç©å ±é…¬ï¼š</p>

$$
V^\pi(s) = \mathbb{E}_\pi[G_t | S_t = s] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s\right]
$$

<h4>è¡Œå‹•ä¾¡å€¤é–¢æ•° $Q^\pi(s, a)$</h4>

<p>çŠ¶æ…‹ $s$ ã§è¡Œå‹• $a$ ã‚’å–ã‚Šã€ãã®å¾Œæ–¹ç­– $\pi$ ã«å¾“ã£ãŸã¨ãã®æœŸå¾…ç´¯ç©å ±é…¬ï¼š</p>

$$
Q^\pi(s, a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s, A_t = a\right]
$$

<h4>V ã¨ Q ã®é–¢ä¿‚</h4>

$$
V^\pi(s) = \sum_{a} \pi(a|s) Q^\pi(s, a)
$$

<h3>ãƒ™ãƒ«ãƒãƒ³æ–¹ç¨‹å¼</h3>

<p>ãƒ™ãƒ«ãƒãƒ³æ–¹ç¨‹å¼ã¯ã€ä¾¡å€¤é–¢æ•°ã‚’å†å¸°çš„ã«å®šç¾©ã—ã¾ã™ã€‚ã“ã‚Œã¯å‹•çš„è¨ˆç”»æ³•ã®åŸºç¤ã¨ãªã‚‹é‡è¦ãªæ–¹ç¨‹å¼ã§ã™ã€‚</p>

<h4>ãƒ™ãƒ«ãƒãƒ³æœŸå¾…æ–¹ç¨‹å¼ï¼ˆBellman Expectation Equationï¼‰</h4>

<p>çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ï¼š</p>

$$
V^\pi(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s,a) \left[R(s,a,s') + \gamma V^\pi(s')\right]
$$

<p>è¡Œå‹•ä¾¡å€¤é–¢æ•°ï¼š</p>

$$
Q^\pi(s, a) = \sum_{s'} P(s'|s,a) \left[R(s,a,s') + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s', a')\right]
$$

<h4>ãƒ™ãƒ«ãƒãƒ³æœ€é©æ–¹ç¨‹å¼ï¼ˆBellman Optimality Equationï¼‰</h4>

<p>æœ€é©çŠ¶æ…‹ä¾¡å€¤é–¢æ•° $V^*(s)$ï¼š</p>

$$
V^*(s) = \max_{a} \sum_{s'} P(s'|s,a) \left[R(s,a,s') + \gamma V^*(s')\right]
$$

<p>æœ€é©è¡Œå‹•ä¾¡å€¤é–¢æ•° $Q^*(s, a)$ï¼š</p>

$$
Q^*(s, a) = \sum_{s'} P(s'|s,a) \left[R(s,a,s') + \gamma \max_{a'} Q^*(s', a')\right]
$$

<blockquote>
<p>ã€Œãƒ™ãƒ«ãƒãƒ³æ–¹ç¨‹å¼ã®ç›´æ„Ÿï¼šç¾åœ¨ã®ä¾¡å€¤ = å³æ™‚å ±é…¬ + å°†æ¥ã®ä¾¡å€¤ã®å‰²å¼•ã€</p>
</blockquote>

<h3>æ–¹ç­–ï¼ˆPolicyï¼‰</h3>

<p>æ–¹ç­– $\pi$ ã¯ã€çŠ¶æ…‹ã‹ã‚‰è¡Œå‹•ã¸ã®å†™åƒã§ã™ï¼š</p>

<ul>
<li><strong>æ±ºå®šçš„æ–¹ç­–</strong>ï¼š$a = \pi(s)$ï¼ˆå„çŠ¶æ…‹ã§1ã¤ã®è¡Œå‹•ã‚’æ±ºå®šï¼‰</li>
<li><strong>ç¢ºç‡çš„æ–¹ç­–</strong>ï¼š$\pi(a|s)$ï¼ˆå„çŠ¶æ…‹ã§è¡Œå‹•ã®ç¢ºç‡åˆ†å¸ƒï¼‰</li>
</ul>

<h4>æœ€é©æ–¹ç­– $\pi^*$</h4>

<p>å…¨ã¦ã®çŠ¶æ…‹ã§æœ€å¤§ã®ä¾¡å€¤ã‚’é”æˆã™ã‚‹æ–¹ç­–ï¼š</p>

$$
\pi^*(s) = \arg\max_{a} Q^*(s, a)
$$

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

print("=== ãƒ™ãƒ«ãƒãƒ³æ–¹ç¨‹å¼ã¨ä¾¡å€¤é–¢æ•° ===\n")

class GridWorld:
    """
    4x4 ã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ MDP
    """
    def __init__(self, grid_size=4):
        self.size = grid_size
        self.n_states = grid_size * grid_size
        self.n_actions = 4  # ä¸Šå³ä¸‹å·¦
        self.actions = [(-1, 0), (0, 1), (1, 0), (0, -1)]
        self.action_names = ['â†‘', 'â†’', 'â†“', 'â†']

        # çµ‚ç«¯çŠ¶æ…‹ï¼ˆå·¦ä¸Šã¨å³ä¸‹ï¼‰
        self.terminal_states = [(0, 0), (grid_size-1, grid_size-1)]

    def state_to_index(self, state):
        """(x, y) ã‚’ 1æ¬¡å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›"""
        return state[0] * self.size + state[1]

    def index_to_state(self, index):
        """1æ¬¡å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ (x, y) ã«å¤‰æ›"""
        return (index // self.size, index % self.size)

    def is_terminal(self, state):
        """çµ‚ç«¯çŠ¶æ…‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return state in self.terminal_states

    def get_next_state(self, state, action):
        """æ¬¡ã®çŠ¶æ…‹ã‚’å–å¾—"""
        if self.is_terminal(state):
            return state

        dx, dy = self.actions[action]
        next_state = (state[0] + dx, state[1] + dy)

        # ã‚°ãƒªãƒƒãƒ‰ã®ç¯„å›²å†…ã‹ãƒã‚§ãƒƒã‚¯
        if (0 <= next_state[0] < self.size and
            0 <= next_state[1] < self.size):
            return next_state
        else:
            return state  # å£ã«ã¶ã¤ã‹ã‚‹å ´åˆã¯ç§»å‹•ã—ãªã„

    def get_reward(self, state, action, next_state):
        """å ±é…¬ã‚’å–å¾—"""
        if self.is_terminal(state):
            return 0
        return -1  # å„ã‚¹ãƒ†ãƒƒãƒ—ã§-1ã®å ±é…¬ï¼ˆæœ€çŸ­çµŒè·¯ã‚’è¦‹ã¤ã‘ã‚‹å‹•æ©Ÿä»˜ã‘ï¼‰

def policy_evaluation(env, policy, gamma=0.9, theta=1e-6):
    """
    æ–¹ç­–è©•ä¾¡ï¼šä¸ãˆã‚‰ã‚ŒãŸæ–¹ç­–ã®ä¾¡å€¤é–¢æ•°ã‚’è¨ˆç®—

    Parameters:
    -----------
    env : GridWorld
    policy : ndarray
        å„çŠ¶æ…‹ã§ã®è¡Œå‹•ç¢ºç‡åˆ†å¸ƒ [n_states, n_actions]
    gamma : float
        å‰²å¼•ç‡
    theta : float
        åæŸåˆ¤å®šã®é–¾å€¤

    Returns:
    --------
    V : ndarray
        çŠ¶æ…‹ä¾¡å€¤é–¢æ•°
    """
    V = np.zeros(env.n_states)

    iteration = 0
    while True:
        delta = 0
        V_old = V.copy()

        for s_idx in range(env.n_states):
            state = env.index_to_state(s_idx)

            if env.is_terminal(state):
                continue

            v = 0
            # ãƒ™ãƒ«ãƒãƒ³æœŸå¾…æ–¹ç¨‹å¼
            for action in range(env.n_actions):
                next_state = env.get_next_state(state, action)
                reward = env.get_reward(state, action, next_state)
                next_s_idx = env.state_to_index(next_state)

                # V(s) = Î£ Ï€(a|s) [R + Î³V(s')]
                v += policy[s_idx, action] * (reward + gamma * V_old[next_s_idx])

            V[s_idx] = v
            delta = max(delta, abs(V[s_idx] - V_old[s_idx]))

        iteration += 1
        if delta < theta:
            break

    print(f"æ–¹ç­–è©•ä¾¡ãŒ {iteration} å›ã®åå¾©ã§åæŸã—ã¾ã—ãŸ")
    return V

# ã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ã®ä½œæˆ
env = GridWorld(grid_size=4)

# ãƒ©ãƒ³ãƒ€ãƒ æ–¹ç­–ï¼ˆå„è¡Œå‹•ã‚’ç­‰ç¢ºç‡ã§é¸æŠï¼‰
random_policy = np.ones((env.n_states, env.n_actions)) / env.n_actions

print("ã€ãƒ©ãƒ³ãƒ€ãƒ æ–¹ç­–ã®è©•ä¾¡ã€‘")
V_random = policy_evaluation(env, random_policy, gamma=0.9)

# ä¾¡å€¤é–¢æ•°ã‚’2æ¬¡å…ƒã‚°ãƒªãƒƒãƒ‰ã§è¡¨ç¤º
V_grid = V_random.reshape((env.size, env.size))
print("\nçŠ¶æ…‹ä¾¡å€¤é–¢æ•° V(s):")
print(V_grid)
print()

# æœ€é©ãªæ–¹ç­–ï¼ˆè²ªæ¬²æ–¹ç­–ï¼‰ã‚’è¨ˆç®—
def compute_greedy_policy(env, V, gamma=0.9):
    """
    ä¾¡å€¤é–¢æ•°ã‹ã‚‰è²ªæ¬²æ–¹ç­–ã‚’è¨ˆç®—
    """
    policy = np.zeros((env.n_states, env.n_actions))

    for s_idx in range(env.n_states):
        state = env.index_to_state(s_idx)

        if env.is_terminal(state):
            policy[s_idx] = 1.0 / env.n_actions  # çµ‚ç«¯çŠ¶æ…‹ã§ã¯å‡ç­‰
            continue

        # å„è¡Œå‹•ã®Qå€¤ã‚’è¨ˆç®—
        q_values = np.zeros(env.n_actions)
        for action in range(env.n_actions):
            next_state = env.get_next_state(state, action)
            reward = env.get_reward(state, action, next_state)
            next_s_idx = env.state_to_index(next_state)
            q_values[action] = reward + gamma * V[next_s_idx]

        # æœ€å¤§Qå€¤ã‚’æŒã¤è¡Œå‹•ã‚’é¸æŠ
        best_action = np.argmax(q_values)
        policy[s_idx, best_action] = 1.0

    return policy

greedy_policy = compute_greedy_policy(env, V_random, gamma=0.9)

# æ–¹ç­–ã®å¯è¦–åŒ–
def visualize_policy(env, policy):
    """æ–¹ç­–ã‚’çŸ¢å°ã§å¯è¦–åŒ–"""
    policy_grid = np.zeros((env.size, env.size), dtype=object)

    for s_idx in range(env.n_states):
        state = env.index_to_state(s_idx)
        if env.is_terminal(state):
            policy_grid[state] = 'T'
        else:
            action = np.argmax(policy[s_idx])
            policy_grid[state] = env.action_names[action]

    return policy_grid

print("ã€è²ªæ¬²æ–¹ç­–ï¼ˆãƒ©ãƒ³ãƒ€ãƒ æ–¹ç­–ã‹ã‚‰å°å‡ºï¼‰ã€‘")
policy_grid = visualize_policy(env, greedy_policy)
print(policy_grid)
print()

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# å·¦ï¼šçŠ¶æ…‹ä¾¡å€¤é–¢æ•°
im1 = axes[0].imshow(V_grid, cmap='RdYlGn', interpolation='nearest')
axes[0].set_title('çŠ¶æ…‹ä¾¡å€¤é–¢æ•° V(s)\nï¼ˆãƒ©ãƒ³ãƒ€ãƒ æ–¹ç­–ï¼‰',
                  fontsize=14, fontweight='bold')
for i in range(env.size):
    for j in range(env.size):
        text = axes[0].text(j, i, f'{V_grid[i, j]:.1f}',
                           ha="center", va="center", color="black", fontsize=11)
axes[0].set_xticks(range(env.size))
axes[0].set_yticks(range(env.size))
plt.colorbar(im1, ax=axes[0])

# å³ï¼šè²ªæ¬²æ–¹ç­–
policy_display = np.zeros((env.size, env.size))
axes[1].imshow(policy_display, cmap='Blues', alpha=0.3)
axes[1].set_title('è²ªæ¬²æ–¹ç­–\nï¼ˆV(s)ã‹ã‚‰å°å‡ºï¼‰',
                  fontsize=14, fontweight='bold')

for i in range(env.size):
    for j in range(env.size):
        state = (i, j)
        if env.is_terminal(state):
            axes[1].text(j, i, 'GOAL', ha="center", va="center",
                        fontsize=12, fontweight='bold', color='red')
        else:
            s_idx = env.state_to_index(state)
            action = np.argmax(greedy_policy[s_idx])
            arrow = env.action_names[action]
            axes[1].text(j, i, arrow, ha="center", va="center",
                        fontsize=20, fontweight='bold')

axes[1].set_xticks(range(env.size))
axes[1].set_yticks(range(env.size))
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('value_function_bellman.png', dpi=150, bbox_inches='tight')
print("å¯è¦–åŒ–ã‚’ 'value_function_bellman.png' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
</code></pre>

<hr>

<h2>1.4 ä¾¡å€¤åå¾©æ³•ã¨æ–¹ç­–åå¾©æ³•</h2>

<h3>ä¾¡å€¤åå¾©æ³•ï¼ˆValue Iterationï¼‰</h3>

<p>ä¾¡å€¤åå¾©æ³•ã¯ã€ãƒ™ãƒ«ãƒãƒ³æœ€é©æ–¹ç¨‹å¼ã‚’åå¾©çš„ã«é©ç”¨ã—ã¦æœ€é©ä¾¡å€¤é–¢æ•°ã‚’æ±‚ã‚ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚</p>

<h4>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h4>

<ol>
<li>$V(s)$ ã‚’ä»»æ„ã®å€¤ï¼ˆé€šå¸¸ã¯0ï¼‰ã§åˆæœŸåŒ–</li>
<li>å„çŠ¶æ…‹ $s$ ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã‚’åå¾©ï¼š
$$
V_{k+1}(s) = \max_{a} \sum_{s'} P(s'|s,a) \left[R(s,a,s') + \gamma V_k(s')\right]
$$
</li>
<li>$V_k$ ãŒåæŸã™ã‚‹ã¾ã§ç¹°ã‚Šè¿”ã™</li>
<li>æœ€é©æ–¹ç­–ã‚’æŠ½å‡ºï¼š$\pi^*(s) = \arg\max_{a} Q^*(s, a)$</li>
</ol>

<h3>æ–¹ç­–åå¾©æ³•ï¼ˆPolicy Iterationï¼‰</h3>

<p>æ–¹ç­–åå¾©æ³•ã¯ã€æ–¹ç­–è©•ä¾¡ã¨æ–¹ç­–æ”¹å–„ã‚’äº¤äº’ã«è¡Œã„ã¾ã™ã€‚</p>

<h4>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h4>

<ol>
<li><strong>åˆæœŸåŒ–</strong>ï¼šä»»æ„ã®æ–¹ç­– $\pi$ ã‚’é¸ã¶</li>
<li><strong>æ–¹ç­–è©•ä¾¡</strong>ï¼š$V^\pi$ ã‚’è¨ˆç®—</li>
<li><strong>æ–¹ç­–æ”¹å–„</strong>ï¼š$\pi' = \text{greedy}(V^\pi)$</li>
<li>$\pi' = \pi$ ãªã‚‰çµ‚äº†ã€ãã†ã§ãªã‘ã‚Œã° $\pi = \pi'$ ã¨ã—ã¦2ã¸</li>
</ol>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>ç‰¹å¾´</th>
<th>åæŸé€Ÿåº¦</th>
<th>é©ç”¨å ´é¢</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ä¾¡å€¤åå¾©æ³•</strong></td>
<td>ä¾¡å€¤é–¢æ•°ã‚’ç›´æ¥æœ€é©åŒ–</td>
<td>é…ã„</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…ãŒå¿…è¦</td>
</tr>
<tr>
<td><strong>æ–¹ç­–åå¾©æ³•</strong></td>
<td>æ–¹ç­–ã‚’ç¹°ã‚Šè¿”ã—æ”¹å–„</td>
<td>é€Ÿã„</td>
<td>æ–¹ç­–ãŒé‡è¦ãªå ´åˆ</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

print("=== ä¾¡å€¤åå¾©æ³•ã¨æ–¹ç­–åå¾©æ³•ã®å®Ÿè£… ===\n")

class GridWorldEnv:
    """ã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ç’°å¢ƒ"""
    def __init__(self, size=4):
        self.size = size
        self.n_states = size * size
        self.n_actions = 4
        self.actions = [(-1, 0), (0, 1), (1, 0), (0, -1)]  # ä¸Šå³ä¸‹å·¦
        self.terminal_states = [(0, 0), (size-1, size-1)]

    def state_to_index(self, state):
        return state[0] * self.size + state[1]

    def index_to_state(self, index):
        return (index // self.size, index % self.size)

    def is_terminal(self, state):
        return state in self.terminal_states

    def step(self, state, action):
        if self.is_terminal(state):
            return state, 0

        dx, dy = self.actions[action]
        next_state = (state[0] + dx, state[1] + dy)

        if (0 <= next_state[0] < self.size and
            0 <= next_state[1] < self.size):
            return next_state, -1
        else:
            return state, -1  # å£ã«ã¶ã¤ã‹ã£ã¦ã‚‚å ±é…¬-1

def value_iteration(env, gamma=0.9, theta=1e-6):
    """
    ä¾¡å€¤åå¾©æ³•

    Returns:
    --------
    V : ndarray
        æœ€é©çŠ¶æ…‹ä¾¡å€¤é–¢æ•°
    policy : ndarray
        æœ€é©æ–¹ç­–
    iterations : int
        åæŸã¾ã§ã®åå¾©å›æ•°
    """
    V = np.zeros(env.n_states)
    policy = np.zeros(env.n_states, dtype=int)

    iteration = 0
    while True:
        delta = 0
        V_old = V.copy()

        for s_idx in range(env.n_states):
            state = env.index_to_state(s_idx)

            if env.is_terminal(state):
                continue

            # å„è¡Œå‹•ã®Qå€¤ã‚’è¨ˆç®—
            q_values = np.zeros(env.n_actions)
            for action in range(env.n_actions):
                next_state, reward = env.step(state, action)
                next_s_idx = env.state_to_index(next_state)
                q_values[action] = reward + gamma * V_old[next_s_idx]

            # ãƒ™ãƒ«ãƒãƒ³æœ€é©æ–¹ç¨‹å¼ï¼šV(s) = max_a Q(s, a)
            V[s_idx] = np.max(q_values)
            policy[s_idx] = np.argmax(q_values)

            delta = max(delta, abs(V[s_idx] - V_old[s_idx]))

        iteration += 1
        if delta < theta:
            break

    return V, policy, iteration

def policy_iteration(env, gamma=0.9, theta=1e-6):
    """
    æ–¹ç­–åå¾©æ³•

    Returns:
    --------
    V : ndarray
        æœ€é©çŠ¶æ…‹ä¾¡å€¤é–¢æ•°
    policy : ndarray
        æœ€é©æ–¹ç­–
    iterations : int
        æ–¹ç­–æ”¹å–„ã®å›æ•°
    """
    # ãƒ©ãƒ³ãƒ€ãƒ æ–¹ç­–ã§åˆæœŸåŒ–
    policy = np.random.randint(0, env.n_actions, size=env.n_states)

    iteration = 0
    while True:
        # 1. æ–¹ç­–è©•ä¾¡
        V = np.zeros(env.n_states)
        while True:
            delta = 0
            V_old = V.copy()

            for s_idx in range(env.n_states):
                state = env.index_to_state(s_idx)

                if env.is_terminal(state):
                    continue

                action = policy[s_idx]
                next_state, reward = env.step(state, action)
                next_s_idx = env.state_to_index(next_state)

                V[s_idx] = reward + gamma * V_old[next_s_idx]
                delta = max(delta, abs(V[s_idx] - V_old[s_idx]))

            if delta < theta:
                break

        # 2. æ–¹ç­–æ”¹å–„
        policy_stable = True
        for s_idx in range(env.n_states):
            state = env.index_to_state(s_idx)

            if env.is_terminal(state):
                continue

            old_action = policy[s_idx]

            # è²ªæ¬²æ–¹ç­–ã‚’è¨ˆç®—
            q_values = np.zeros(env.n_actions)
            for action in range(env.n_actions):
                next_state, reward = env.step(state, action)
                next_s_idx = env.state_to_index(next_state)
                q_values[action] = reward + gamma * V[next_s_idx]

            policy[s_idx] = np.argmax(q_values)

            if old_action != policy[s_idx]:
                policy_stable = False

        iteration += 1
        if policy_stable:
            break

    return V, policy, iteration

# ç’°å¢ƒã®ä½œæˆ
env = GridWorldEnv(size=4)

# ä¾¡å€¤åå¾©æ³•ã®å®Ÿè¡Œ
print("ã€ä¾¡å€¤åå¾©æ³•ã€‘")
V_vi, policy_vi, iter_vi = value_iteration(env, gamma=0.9)
print(f"åæŸã¾ã§ã®åå¾©å›æ•°: {iter_vi}")
print(f"\næœ€é©çŠ¶æ…‹ä¾¡å€¤é–¢æ•°:")
print(V_vi.reshape(env.size, env.size))
print()

# æ–¹ç­–åå¾©æ³•ã®å®Ÿè¡Œ
print("ã€æ–¹ç­–åå¾©æ³•ã€‘")
V_pi, policy_pi, iter_pi = policy_iteration(env, gamma=0.9)
print(f"æ–¹ç­–æ”¹å–„ã®å›æ•°: {iter_pi}")
print(f"\næœ€é©çŠ¶æ…‹ä¾¡å€¤é–¢æ•°:")
print(V_pi.reshape(env.size, env.size))
print()

# æ–¹ç­–ã®å¯è¦–åŒ–
action_symbols = ['â†‘', 'â†’', 'â†“', 'â†']

def visualize_policy_grid(env, policy):
    policy_grid = np.zeros((env.size, env.size), dtype=object)
    for s_idx in range(env.n_states):
        state = env.index_to_state(s_idx)
        if env.is_terminal(state):
            policy_grid[state] = 'G'
        else:
            policy_grid[state] = action_symbols[policy[s_idx]]
    return policy_grid

print("ã€æœ€é©æ–¹ç­–ï¼ˆä¾¡å€¤åå¾©æ³•ï¼‰ã€‘")
policy_grid_vi = visualize_policy_grid(env, policy_vi)
print(policy_grid_vi)
print()

print("ã€æœ€é©æ–¹ç­–ï¼ˆæ–¹ç­–åå¾©æ³•ï¼‰ã€‘")
policy_grid_pi = visualize_policy_grid(env, policy_pi)
print(policy_grid_pi)
print()

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(12, 12))

# ä¾¡å€¤åå¾©æ³•ã®çµæœ
V_grid_vi = V_vi.reshape(env.size, env.size)
im1 = axes[0, 0].imshow(V_grid_vi, cmap='RdYlGn', interpolation='nearest')
axes[0, 0].set_title('ä¾¡å€¤åå¾©æ³•ï¼šæœ€é©ä¾¡å€¤é–¢æ•°', fontsize=12, fontweight='bold')
for i in range(env.size):
    for j in range(env.size):
        axes[0, 0].text(j, i, f'{V_grid_vi[i, j]:.1f}',
                       ha="center", va="center", color="black", fontsize=10)
plt.colorbar(im1, ax=axes[0, 0])

# ä¾¡å€¤åå¾©æ³•ã®æ–¹ç­–
axes[0, 1].imshow(np.zeros((env.size, env.size)), cmap='Blues', alpha=0.3)
axes[0, 1].set_title(f'ä¾¡å€¤åå¾©æ³•ï¼šæœ€é©æ–¹ç­–\n(åå¾©å›æ•°: {iter_vi})',
                     fontsize=12, fontweight='bold')
for i in range(env.size):
    for j in range(env.size):
        if env.is_terminal((i, j)):
            axes[0, 1].text(j, i, 'GOAL', ha="center", va="center",
                           fontsize=10, fontweight='bold', color='red')
        else:
            s_idx = env.state_to_index((i, j))
            axes[0, 1].text(j, i, action_symbols[policy_vi[s_idx]],
                           ha="center", va="center", fontsize=16)
axes[0, 1].grid(True, alpha=0.3)

# æ–¹ç­–åå¾©æ³•ã®çµæœ
V_grid_pi = V_pi.reshape(env.size, env.size)
im2 = axes[1, 0].imshow(V_grid_pi, cmap='RdYlGn', interpolation='nearest')
axes[1, 0].set_title('æ–¹ç­–åå¾©æ³•ï¼šæœ€é©ä¾¡å€¤é–¢æ•°', fontsize=12, fontweight='bold')
for i in range(env.size):
    for j in range(env.size):
        axes[1, 0].text(j, i, f'{V_grid_pi[i, j]:.1f}',
                       ha="center", va="center", color="black", fontsize=10)
plt.colorbar(im2, ax=axes[1, 0])

# æ–¹ç­–åå¾©æ³•ã®æ–¹ç­–
axes[1, 1].imshow(np.zeros((env.size, env.size)), cmap='Blues', alpha=0.3)
axes[1, 1].set_title(f'æ–¹ç­–åå¾©æ³•ï¼šæœ€é©æ–¹ç­–\n(æ–¹ç­–æ”¹å–„å›æ•°: {iter_pi})',
                     fontsize=12, fontweight='bold')
for i in range(env.size):
    for j in range(env.size):
        if env.is_terminal((i, j)):
            axes[1, 1].text(j, i, 'GOAL', ha="center", va="center",
                           fontsize=10, fontweight='bold', color='red')
        else:
            s_idx = env.state_to_index((i, j))
            axes[1, 1].text(j, i, action_symbols[policy_pi[s_idx]],
                           ha="center", va="center", fontsize=16)
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('value_policy_iteration.png', dpi=150, bbox_inches='tight')
print("å¯è¦–åŒ–ã‚’ 'value_policy_iteration.png' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
</code></pre>

<hr>

<h2>1.5 æ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•</h2>

<h3>æ¢ç´¢ï¼ˆExplorationï¼‰vs æ´»ç”¨ï¼ˆExploitationï¼‰</h3>

<p>å¼·åŒ–å­¦ç¿’ã«ãŠã‘ã‚‹æœ€ã‚‚é‡è¦ãªèª²é¡Œã®ä¸€ã¤ãŒã€æ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã§ã™ã€‚</p>

<blockquote>
<p>ã€Œæ¢ç´¢ï¼šæ–°ã—ã„è¡Œå‹•ã‚’è©¦ã—ã¦ã€ã‚ˆã‚Šè‰¯ã„é¸æŠè‚¢ã‚’è¦‹ã¤ã‘ã‚‹ã€<br>
ã€Œæ´»ç”¨ï¼šæ—¢çŸ¥ã®æœ€è‰¯ã®è¡Œå‹•ã‚’é¸æŠã—ã¦ã€å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹ã€</p>
</blockquote>

<h4>å¤šè…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œ</h4>

<p>$K$ æœ¬ã®è…•ã‚’æŒã¤ã‚¹ãƒ­ãƒƒãƒˆãƒã‚·ãƒ³ï¼ˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆï¼‰ãŒã‚ã‚Šã€å„è…• $i$ ã¯ç•°ãªã‚‹æœŸå¾…å ±é…¬ $\mu_i$ ã‚’æŒã¡ã¾ã™ã€‚ç›®æ¨™ã¯ã€ç´¯ç©å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã§ã™ã€‚</p>

<h3>æ¢ç´¢æˆ¦ç•¥</h3>

<table>
<thead>
<tr>
<th>æˆ¦ç•¥</th>
<th>èª¬æ˜</th>
<th>ç‰¹å¾´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Îµ-greedy</strong></td>
<td>ç¢ºç‡ $\epsilon$ ã§ãƒ©ãƒ³ãƒ€ãƒ è¡Œå‹•</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ã€èª¿æ•´ãŒå®¹æ˜“</td>
</tr>
<tr>
<td><strong>Softmax</strong></td>
<td>Qå€¤ã«åŸºã¥ãç¢ºç‡çš„é¸æŠ</td>
<td>æ»‘ã‚‰ã‹ãªæ¢ç´¢</td>
</tr>
<tr>
<td><strong>UCB</strong></td>
<td>ä¸Šå´ä¿¡é ¼é™ç•Œã‚’ä½¿ç”¨</td>
<td>ç†è«–çš„ä¿è¨¼ã‚ã‚Š</td>
</tr>
<tr>
<td><strong>Thompson Sampling</strong></td>
<td>ãƒ™ã‚¤ã‚ºæ¨å®šã«åŸºã¥ã</td>
<td>åŠ¹ç‡çš„ãªæ¢ç´¢</td>
</tr>
</tbody>
</table>

<h4>Îµ-greedy æ–¹ç­–</h4>

$$
a_t = \begin{cases}
\arg\max_a Q(a) & \text{ç¢ºç‡ } 1-\epsilon \\
\text{random action} & \text{ç¢ºç‡ } \epsilon
\end{cases}
$$

<h4>Softmaxï¼ˆãƒœãƒ«ãƒ„ãƒãƒ³ï¼‰æ–¹ç­–</h4>

$$
P(a) = \frac{\exp(Q(a) / \tau)}{\sum_{a'} \exp(Q(a') / \tau)}
$$

ã“ã“ã§ $\tau$ ã¯æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé«˜ã„ã»ã©æ¢ç´¢çš„ï¼‰ã€‚

<h4>UCBï¼ˆUpper Confidence Boundï¼‰</h4>

$$
a_t = \arg\max_a \left[Q(a) + c\sqrt{\frac{\ln t}{N(a)}}\right]
$$

ã“ã“ã§ $N(a)$ ã¯è¡Œå‹• $a$ ã‚’é¸ã‚“ã å›æ•°ã€$c$ ã¯æ¢ç´¢åº¦åˆã„ã‚’åˆ¶å¾¡ã€‚

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

print("=== æ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ• ===\n")

class MultiArmedBandit:
    """å¤šè…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œ"""
    def __init__(self, n_arms=10, seed=42):
        np.random.seed(seed)
        self.n_arms = n_arms
        # å„è…•ã®çœŸã®æœŸå¾…å ±é…¬ï¼ˆæ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰ç”Ÿæˆï¼‰
        self.true_values = np.random.randn(n_arms)
        self.optimal_arm = np.argmax(self.true_values)

    def pull(self, arm):
        """è…•ã‚’å¼•ã„ã¦å ±é…¬ã‚’å¾—ã‚‹ï¼ˆæœŸå¾…å€¤ + ãƒã‚¤ã‚ºï¼‰"""
        reward = self.true_values[arm] + np.random.randn()
        return reward

class EpsilonGreedy:
    """Îµ-greedy ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ """
    def __init__(self, n_arms, epsilon=0.1):
        self.n_arms = n_arms
        self.epsilon = epsilon
        self.Q = np.zeros(n_arms)  # æ¨å®šä¾¡å€¤
        self.N = np.zeros(n_arms)  # é¸æŠå›æ•°

    def select_action(self):
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.n_arms)  # æ¢ç´¢
        else:
            return np.argmax(self.Q)  # æ´»ç”¨

    def update(self, action, reward):
        self.N[action] += 1
        # å¢—åˆ†æ›´æ–°ï¼šQ(a) â† Q(a) + Î±[R - Q(a)]
        self.Q[action] += (reward - self.Q[action]) / self.N[action]

class UCB:
    """UCBï¼ˆUpper Confidence Boundï¼‰ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ """
    def __init__(self, n_arms, c=2.0):
        self.n_arms = n_arms
        self.c = c
        self.Q = np.zeros(n_arms)
        self.N = np.zeros(n_arms)
        self.t = 0

    def select_action(self):
        self.t += 1

        # å…¨ã¦ã®è…•ã‚’å°‘ãªãã¨ã‚‚1å›ã¯é¸æŠ
        if np.min(self.N) == 0:
            return np.argmin(self.N)

        # UCB ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        ucb_values = self.Q + self.c * np.sqrt(np.log(self.t) / self.N)
        return np.argmax(ucb_values)

    def update(self, action, reward):
        self.N[action] += 1
        self.Q[action] += (reward - self.Q[action]) / self.N[action]

class Softmax:
    """Softmaxï¼ˆãƒœãƒ«ãƒ„ãƒãƒ³ï¼‰æ–¹ç­–"""
    def __init__(self, n_arms, tau=1.0):
        self.n_arms = n_arms
        self.tau = tau  # æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.Q = np.zeros(n_arms)
        self.N = np.zeros(n_arms)

    def select_action(self):
        # Softmax ç¢ºç‡ã‚’è¨ˆç®—
        exp_values = np.exp(self.Q / self.tau)
        probs = exp_values / np.sum(exp_values)
        return np.random.choice(self.n_arms, p=probs)

    def update(self, action, reward):
        self.N[action] += 1
        self.Q[action] += (reward - self.Q[action]) / self.N[action]

def run_experiment(bandit, agent, n_steps=1000):
    """å®Ÿé¨“ã‚’å®Ÿè¡Œ"""
    rewards = np.zeros(n_steps)
    optimal_actions = np.zeros(n_steps)

    for step in range(n_steps):
        action = agent.select_action()
        reward = bandit.pull(action)
        agent.update(action, reward)

        rewards[step] = reward
        optimal_actions[step] = (action == bandit.optimal_arm)

    return rewards, optimal_actions

# ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®ä½œæˆ
bandit = MultiArmedBandit(n_arms=10, seed=42)

print("ã€çœŸã®æœŸå¾…å ±é…¬ã€‘")
for i, value in enumerate(bandit.true_values):
    marker = " â† æœ€é©" if i == bandit.optimal_arm else ""
    print(f"è…• {i}: {value:.3f}{marker}")
print()

# è¤‡æ•°ã®æˆ¦ç•¥ã‚’æ¯”è¼ƒ
n_runs = 100
n_steps = 1000

strategies = {
    'Îµ-greedy (Îµ=0.01)': lambda: EpsilonGreedy(10, epsilon=0.01),
    'Îµ-greedy (Îµ=0.1)': lambda: EpsilonGreedy(10, epsilon=0.1),
    'UCB (c=2)': lambda: UCB(10, c=2.0),
    'Softmax (Ï„=1)': lambda: Softmax(10, tau=1.0),
}

results = {}

for name, agent_fn in strategies.items():
    print(f"å®Ÿè¡Œä¸­: {name}")
    all_rewards = np.zeros((n_runs, n_steps))
    all_optimal = np.zeros((n_runs, n_steps))

    for run in range(n_runs):
        bandit_run = MultiArmedBandit(n_arms=10, seed=run)
        agent = agent_fn()
        rewards, optimal = run_experiment(bandit_run, agent, n_steps)
        all_rewards[run] = rewards
        all_optimal[run] = optimal

    results[name] = {
        'rewards': all_rewards.mean(axis=0),
        'optimal': all_optimal.mean(axis=0)
    }

print("\nå®Ÿé¨“å®Œäº†\n")

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# å·¦ï¼šå¹³å‡å ±é…¬
axes[0].set_title('å¹³å‡å ±é…¬ã®æ¨ç§»', fontsize=14, fontweight='bold')
for name, data in results.items():
    axes[0].plot(data['rewards'], label=name, linewidth=2, alpha=0.8)

axes[0].axhline(y=max(bandit.true_values), color='r', linestyle='--',
                linewidth=2, label='æœ€é©å ±é…¬', alpha=0.5)
axes[0].set_xlabel('ã‚¹ãƒ†ãƒƒãƒ—æ•°', fontsize=12)
axes[0].set_ylabel('å¹³å‡å ±é…¬', fontsize=12)
axes[0].legend(loc='lower right')
axes[0].grid(alpha=0.3)

# å³ï¼šæœ€é©è¡Œå‹•ã®é¸æŠç‡
axes[1].set_title('æœ€é©è¡Œå‹•ã®é¸æŠç‡', fontsize=14, fontweight='bold')
for name, data in results.items():
    axes[1].plot(data['optimal'], label=name, linewidth=2, alpha=0.8)

axes[1].set_xlabel('ã‚¹ãƒ†ãƒƒãƒ—æ•°', fontsize=12)
axes[1].set_ylabel('æœ€é©è¡Œå‹•ã‚’é¸ã¶ç¢ºç‡', fontsize=12)
axes[1].legend(loc='lower right')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('exploration_exploitation.png', dpi=150, bbox_inches='tight')
print("å¯è¦–åŒ–ã‚’ 'exploration_exploitation.png' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
</code></pre>

<hr>

<h2>1.6 ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã¨TDå­¦ç¿’</h2>

<h3>ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ï¼ˆMonte Carlo Methodsï¼‰</h3>

<p>ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã¯ã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å…¨ä½“ã‚’çµŒé¨“ã—ã¦ã‹ã‚‰ä¾¡å€¤é–¢æ•°ã‚’æ›´æ–°ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ãƒ•ãƒªãƒ¼ã§ã€ç’°å¢ƒã®ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’çŸ¥ã‚‰ãªãã¦ã‚‚å­¦ç¿’ã§ãã¾ã™ã€‚</p>

<h4>First-Visit MC</h4>

<p>çŠ¶æ…‹ $s$ ã«æœ€åˆã«è¨ªã‚ŒãŸã¨ãã®ãƒªã‚¿ãƒ¼ãƒ³ $G_t$ ã‚’ä½¿ã£ã¦æ›´æ–°ï¼š</p>

$$
V(s) \leftarrow V(s) + \alpha [G_t - V(s)]
$$

<h4>Every-Visit MC</h4>

<p>çŠ¶æ…‹ $s$ ã«è¨ªã‚Œã‚‹å…¨ã¦ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ãƒªã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ã€‚</p>

<h3>TDå­¦ç¿’ï¼ˆTemporal Difference Learningï¼‰</h3>

<p>TDå­¦ç¿’ã¯ã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®çµ‚äº†ã‚’å¾…ãŸãšã«ã€1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ä¾¡å€¤é–¢æ•°ã‚’æ›´æ–°ã—ã¾ã™ã€‚</p>

<h4>TD(0) ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h4>

$$
V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]
$$

ã“ã“ã§ $R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$ ã‚’ <strong>TDèª¤å·®</strong> ã¨å‘¼ã³ã¾ã™ã€‚

<h3>MC vs TD ã®æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>ç‰¹å¾´</th>
<th>ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•</th>
<th>TDå­¦ç¿’</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ›´æ–°ã‚¿ã‚¤ãƒŸãƒ³ã‚°</strong></td>
<td>ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å¾Œ</td>
<td>å„ã‚¹ãƒ†ãƒƒãƒ—</td>
</tr>
<tr>
<td><strong>å¿…è¦ãªæƒ…å ±</strong></td>
<td>å®Ÿéš›ã®ãƒªã‚¿ãƒ¼ãƒ³ $G_t$</td>
<td>æ¬¡çŠ¶æ…‹ã®æ¨å®šå€¤ $V(S_{t+1})$</td>
</tr>
<tr>
<td><strong>ãƒã‚¤ã‚¢ã‚¹</strong></td>
<td>ç„¡ãƒã‚¤ã‚¢ã‚¹</td>
<td>ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Šï¼ˆæ¨å®šå€¤ã‚’ä½¿ç”¨ï¼‰</td>
</tr>
<tr>
<td><strong>åˆ†æ•£</strong></td>
<td>é«˜åˆ†æ•£</td>
<td>ä½åˆ†æ•£</td>
</tr>
<tr>
<td><strong>åæŸé€Ÿåº¦</strong></td>
<td>é…ã„</td>
<td>é€Ÿã„</td>
</tr>
<tr>
<td><strong>é©ç”¨</strong></td>
<td>ã‚¨ãƒ”ã‚½ãƒ‡ã‚£ãƒƒã‚¯ã‚¿ã‚¹ã‚¯ã®ã¿</td>
<td>ç¶™ç¶šã‚¿ã‚¹ã‚¯ã‚‚å¯èƒ½</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

print("=== ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã¨TDå­¦ç¿’ ===\n")

class RandomWalk:
    """
    1æ¬¡å…ƒãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ç’°å¢ƒ

    çŠ¶æ…‹: [0, 1, 2, 3, 4, 5, 6]
    - çŠ¶æ…‹0: å·¦ç«¯ï¼ˆçµ‚ç«¯ã€å ±é…¬0ï¼‰
    - çŠ¶æ…‹1-5: é€šå¸¸çŠ¶æ…‹
    - çŠ¶æ…‹6: å³ç«¯ï¼ˆçµ‚ç«¯ã€å ±é…¬+1ï¼‰
    """
    def __init__(self):
        self.n_states = 7
        self.start_state = 3  # ä¸­å¤®ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆ

    def reset(self):
        return self.start_state

    def step(self, state):
        """ãƒ©ãƒ³ãƒ€ãƒ ã«å·¦å³ã«ç§»å‹•"""
        if state == 0 or state == 6:
            return state, 0, True  # çµ‚ç«¯çŠ¶æ…‹

        # 50%ã®ç¢ºç‡ã§å·¦å³ã«ç§»å‹•
        if np.random.rand() < 0.5:
            next_state = state - 1
        else:
            next_state = state + 1

        # å ±é…¬ã¨çµ‚äº†åˆ¤å®š
        if next_state == 6:
            return next_state, 1.0, True
        elif next_state == 0:
            return next_state, 0.0, True
        else:
            return next_state, 0.0, False

def monte_carlo_evaluation(env, n_episodes=1000, alpha=0.1):
    """
    ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã«ã‚ˆã‚‹ä¾¡å€¤é–¢æ•°ã®æ¨å®š
    """
    V = np.zeros(env.n_states)
    V[6] = 1.0  # å³ç«¯ã®çœŸã®ä¾¡å€¤

    for episode in range(n_episodes):
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ç”Ÿæˆ
        states = []
        rewards = []

        state = env.reset()
        states.append(state)

        while True:
            next_state, reward, done = env.step(state)
            rewards.append(reward)

            if done:
                break

            states.append(next_state)
            state = next_state

        # ãƒªã‚¿ãƒ¼ãƒ³ã®è¨ˆç®—ï¼ˆå¾Œã‚ã‹ã‚‰å‰ã¸ï¼‰
        G = 0
        visited = set()

        for t in range(len(states) - 1, -1, -1):
            G = rewards[t] + G  # Î³=1 ã‚’ä»®å®š
            s = states[t]

            # First-Visit MC
            if s not in visited:
                visited.add(s)
                # å¢—åˆ†æ›´æ–°
                V[s] = V[s] + alpha * (G - V[s])

    return V

def td_learning(env, n_episodes=1000, alpha=0.1, gamma=1.0):
    """
    TD(0) ã«ã‚ˆã‚‹ä¾¡å€¤é–¢æ•°ã®æ¨å®š
    """
    V = np.zeros(env.n_states)
    V[6] = 1.0  # å³ç«¯ã®çœŸã®ä¾¡å€¤

    for episode in range(n_episodes):
        state = env.reset()

        while True:
            next_state, reward, done = env.step(state)

            # TD(0) æ›´æ–°
            td_target = reward + gamma * V[next_state]
            td_error = td_target - V[state]
            V[state] = V[state] + alpha * td_error

            if done:
                break

            state = next_state

    return V

# çœŸã®ä¾¡å€¤é–¢æ•°ï¼ˆè§£æçš„ã«è¨ˆç®—å¯èƒ½ï¼‰
true_values = np.array([0, 1/6, 2/6, 3/6, 4/6, 5/6, 1])

# ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ç’°å¢ƒ
env = RandomWalk()

print("ã€çœŸã®ä¾¡å€¤é–¢æ•°ã€‘")
print("çŠ¶æ…‹:", list(range(7)))
print("ä¾¡å€¤:", [f"{v:.3f}" for v in true_values])
print()

# ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã®å®Ÿè¡Œ
print("ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã‚’å®Ÿè¡Œä¸­...")
V_mc = monte_carlo_evaluation(env, n_episodes=5000, alpha=0.01)

print("ã€ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã«ã‚ˆã‚‹æ¨å®šã€‘")
print("çŠ¶æ…‹:", list(range(7)))
print("ä¾¡å€¤:", [f"{v:.3f}" for v in V_mc])
print()

# TDå­¦ç¿’ã®å®Ÿè¡Œ
print("TDå­¦ç¿’ã‚’å®Ÿè¡Œä¸­...")
V_td = td_learning(env, n_episodes=5000, alpha=0.01)

print("ã€TDå­¦ç¿’ã«ã‚ˆã‚‹æ¨å®šã€‘")
print("çŠ¶æ…‹:", list(range(7)))
print("ä¾¡å€¤:", [f"{v:.3f}" for v in V_td])
print()

# å­¦ç¿’æ›²ç·šã®æ¯”è¼ƒ
def evaluate_learning_curve(env, method, n_runs=20, episode_checkpoints=None):
    """å­¦ç¿’æ›²ç·šã‚’è©•ä¾¡"""
    if episode_checkpoints is None:
        episode_checkpoints = [0, 1, 10, 100, 500, 1000, 2000, 5000]

    errors = {ep: [] for ep in episode_checkpoints}

    for run in range(n_runs):
        for n_ep in episode_checkpoints:
            if n_ep == 0:
                V = np.zeros(7)
                V[6] = 1.0
            else:
                if method == 'MC':
                    V = monte_carlo_evaluation(env, n_episodes=n_ep, alpha=0.01)
                else:  # TD
                    V = td_learning(env, n_episodes=n_ep, alpha=0.01)

            # RMSEã‚’è¨ˆç®—
            rmse = np.sqrt(np.mean((V - true_values)**2))
            errors[n_ep].append(rmse)

    # å¹³å‡ã‚’è¨ˆç®—
    avg_errors = {ep: np.mean(errors[ep]) for ep in episode_checkpoints}
    return avg_errors

print("å­¦ç¿’æ›²ç·šã‚’è©•ä¾¡ä¸­ï¼ˆã“ã‚Œã«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰...")
episode_checkpoints = [0, 1, 10, 100, 500, 1000, 2000, 5000]
mc_errors = evaluate_learning_curve(env, 'MC', n_runs=10,
                                    episode_checkpoints=episode_checkpoints)
td_errors = evaluate_learning_curve(env, 'TD', n_runs=10,
                                    episode_checkpoints=episode_checkpoints)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# å·¦ï¼šä¾¡å€¤é–¢æ•°ã®æ¯”è¼ƒ
states = np.arange(7)
axes[0].plot(states, true_values, 'k-', linewidth=3, marker='o',
             markersize=8, label='çœŸã®ä¾¡å€¤', alpha=0.7)
axes[0].plot(states, V_mc, 'b--', linewidth=2, marker='s',
             markersize=6, label='MC (5000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)', alpha=0.8)
axes[0].plot(states, V_td, 'r-.', linewidth=2, marker='^',
             markersize=6, label='TD (5000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)', alpha=0.8)

axes[0].set_xlabel('çŠ¶æ…‹', fontsize=12)
axes[0].set_ylabel('æ¨å®šä¾¡å€¤', fontsize=12)
axes[0].set_title('ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ï¼šä¾¡å€¤é–¢æ•°ã®æ¨å®š', fontsize=14, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)
axes[0].set_xticks(states)

# å³ï¼šå­¦ç¿’æ›²ç·š
episodes = list(mc_errors.keys())
mc_rmse = [mc_errors[ep] for ep in episodes]
td_rmse = [td_errors[ep] for ep in episodes]

axes[1].plot(episodes, mc_rmse, 'b-', linewidth=2, marker='s',
             markersize=6, label='ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•', alpha=0.8)
axes[1].plot(episodes, td_rmse, 'r-', linewidth=2, marker='^',
             markersize=6, label='TDå­¦ç¿’', alpha=0.8)

axes[1].set_xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°', fontsize=12)
axes[1].set_ylabel('RMSEï¼ˆå¹³å‡äºŒä¹—èª¤å·®ï¼‰', fontsize=12)
axes[1].set_title('å­¦ç¿’é€Ÿåº¦ã®æ¯”è¼ƒ', fontsize=14, fontweight='bold')
axes[1].set_xscale('log')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('mc_vs_td.png', dpi=150, bbox_inches='tight')
print("å¯è¦–åŒ–ã‚’ 'mc_vs_td.png' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
</code></pre>

<hr>

<h2>1.7 å®Ÿè·µï¼šGrid World ã§ã®å¼·åŒ–å­¦ç¿’</h2>

<h3>Grid World ç’°å¢ƒã®å®Ÿè£…</h3>

<p>ã“ã“ã§ã¯ã€ã‚ˆã‚Šè¤‡é›‘ãª Grid World ç’°å¢ƒã‚’æ§‹ç¯‰ã—ã€å­¦ã‚“ã ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’çµ±åˆçš„ã«é©ç”¨ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle

print("=== Grid World ã§ã®å¼·åŒ–å­¦ç¿’å®Ÿè·µ ===\n")

class GridWorldEnv:
    """
    ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãª Grid World ç’°å¢ƒ

    - å£ã€ã‚´ãƒ¼ãƒ«ã€ç©´ï¼ˆè½ã¨ã—ç©´ï¼‰ã‚’é…ç½®å¯èƒ½
    - ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ä¸Šä¸‹å·¦å³ã«ç§»å‹•
    - ç¢ºç‡çš„ãªé·ç§»ï¼ˆæ„å›³ã—ãŸæ–¹å‘ã«é€²ã¾ãªã„å¯èƒ½æ€§ï¼‰
    """
    def __init__(self, size=5, slip_prob=0.1):
        self.size = size
        self.slip_prob = slip_prob  # ã‚¹ãƒªãƒƒãƒ—ç¢ºç‡

        # ã‚°ãƒªãƒƒãƒ‰ã®è¨­å®š
        self.grid = np.zeros((size, size), dtype=int)
        # 0: é€šå¸¸, 1: å£, 2: ã‚´ãƒ¼ãƒ«, 3: ç©´

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ç’°å¢ƒè¨­å®š
        self.grid[1, 1] = 1  # å£
        self.grid[1, 2] = 1  # å£
        self.grid[2, 3] = 3  # ç©´
        self.grid[4, 4] = 2  # ã‚´ãƒ¼ãƒ«

        self.start_pos = (0, 0)
        self.current_pos = self.start_pos

        self.actions = [(-1, 0), (0, 1), (1, 0), (0, -1)]  # ä¸Šå³ä¸‹å·¦
        self.action_names = ['â†‘', 'â†’', 'â†“', 'â†']
        self.n_actions = 4

    def reset(self):
        """ç’°å¢ƒã‚’ãƒªã‚»ãƒƒãƒˆ"""
        self.current_pos = self.start_pos
        return self.current_pos

    def is_valid_pos(self, pos):
        """ä½ç½®ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯"""
        x, y = pos
        if not (0 <= x < self.size and 0 <= y < self.size):
            return False
        if self.grid[x, y] == 1:  # å£
            return False
        return True

    def step(self, action):
        """
        è¡Œå‹•ã‚’å®Ÿè¡Œ

        Returns:
        --------
        next_pos : tuple
        reward : float
        done : bool
        """
        # ã‚¹ãƒªãƒƒãƒ—ã®å‡¦ç†
        if np.random.rand() < self.slip_prob:
            action = np.random.randint(self.n_actions)

        dx, dy = self.actions[action]
        next_pos = (self.current_pos[0] + dx, self.current_pos[1] + dy)

        # ç„¡åŠ¹ãªç§»å‹•ã®å ´åˆã¯å…ƒã®ä½ç½®ã«ç•™ã¾ã‚‹
        if not self.is_valid_pos(next_pos):
            next_pos = self.current_pos

        # å ±é…¬ã¨çµ‚äº†åˆ¤å®š
        cell_type = self.grid[next_pos]
        if cell_type == 2:  # ã‚´ãƒ¼ãƒ«
            reward = 10.0
            done = True
        elif cell_type == 3:  # ç©´
            reward = -10.0
            done = True
        else:
            reward = -0.1  # å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã®å°ã•ãªãƒšãƒŠãƒ«ãƒ†ã‚£
            done = False

        self.current_pos = next_pos
        return next_pos, reward, done

    def render(self, policy=None, values=None):
        """ç’°å¢ƒã‚’å¯è¦–åŒ–"""
        fig, ax = plt.subplots(figsize=(8, 8))

        # ã‚°ãƒªãƒƒãƒ‰ã®æç”»
        for i in range(self.size):
            for j in range(self.size):
                cell_type = self.grid[i, j]

                if cell_type == 1:  # å£
                    color = 'gray'
                    ax.add_patch(Rectangle((j, self.size-1-i), 1, 1,
                                          facecolor=color))
                elif cell_type == 2:  # ã‚´ãƒ¼ãƒ«
                    color = 'gold'
                    ax.add_patch(Rectangle((j, self.size-1-i), 1, 1,
                                          facecolor=color))
                    ax.text(j+0.5, self.size-1-i+0.5, 'GOAL',
                           ha='center', va='center', fontsize=10,
                           fontweight='bold', color='red')
                elif cell_type == 3:  # ç©´
                    color = 'black'
                    ax.add_patch(Rectangle((j, self.size-1-i), 1, 1,
                                          facecolor=color))
                    ax.text(j+0.5, self.size-1-i+0.5, 'HOLE',
                           ha='center', va='center', fontsize=10,
                           fontweight='bold', color='white')
                else:  # é€šå¸¸
                    # ä¾¡å€¤é–¢æ•°ã®è¡¨ç¤º
                    if values is not None:
                        value = values[i, j]
                        norm_value = (value - values.min()) / \
                                    (values.max() - values.min() + 1e-8)
                        color = plt.cm.RdYlGn(norm_value)
                        ax.add_patch(Rectangle((j, self.size-1-i), 1, 1,
                                              facecolor=color, alpha=0.6))
                        ax.text(j+0.5, self.size-1-i+0.7, f'{value:.1f}',
                               ha='center', va='center', fontsize=8)

                    # æ–¹ç­–ã®è¡¨ç¤º
                    if policy is not None and values is not None:
                        arrow = policy[i, j]
                        ax.text(j+0.5, self.size-1-i+0.3, arrow,
                               ha='center', va='center', fontsize=14,
                               fontweight='bold')

        # ã‚¹ã‚¿ãƒ¼ãƒˆä½ç½®ã‚’ãƒãƒ¼ã‚¯
        ax.plot(self.start_pos[1]+0.5, self.size-1-self.start_pos[0]+0.5,
               'go', markersize=15, label='Start')

        ax.set_xlim(0, self.size)
        ax.set_ylim(0, self.size)
        ax.set_aspect('equal')
        ax.set_xticks(range(self.size+1))
        ax.set_yticks(range(self.size+1))
        ax.grid(True)
        ax.legend()

        return fig

class QLearningAgent:
    """Qå­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self, env, alpha=0.1, gamma=0.95, epsilon=0.1):
        self.env = env
        self.alpha = alpha  # å­¦ç¿’ç‡
        self.gamma = gamma  # å‰²å¼•ç‡
        self.epsilon = epsilon  # æ¢ç´¢ç‡

        # Q ãƒ†ãƒ¼ãƒ–ãƒ«
        self.Q = np.zeros((env.size, env.size, env.n_actions))

    def select_action(self, state):
        """Îµ-greedy æ–¹ç­–ã§è¡Œå‹•é¸æŠ"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.env.n_actions)
        else:
            x, y = state
            return np.argmax(self.Q[x, y])

    def update(self, state, action, reward, next_state, done):
        """Qå€¤ã‚’æ›´æ–°"""
        x, y = state
        nx, ny = next_state

        if done:
            td_target = reward
        else:
            td_target = reward + self.gamma * np.max(self.Q[nx, ny])

        td_error = td_target - self.Q[x, y, action]
        self.Q[x, y, action] += self.alpha * td_error

    def get_policy(self):
        """ç¾åœ¨ã®æ–¹ç­–ã‚’å–å¾—"""
        policy = np.zeros((self.env.size, self.env.size), dtype=object)
        values = np.zeros((self.env.size, self.env.size))

        for i in range(self.env.size):
            for j in range(self.env.size):
                if self.env.grid[i, j] == 1:  # å£
                    policy[i, j] = 'â– '
                    values[i, j] = 0
                elif self.env.grid[i, j] == 2:  # ã‚´ãƒ¼ãƒ«
                    policy[i, j] = 'G'
                    values[i, j] = 10
                elif self.env.grid[i, j] == 3:  # ç©´
                    policy[i, j] = 'H'
                    values[i, j] = -10
                else:
                    best_action = np.argmax(self.Q[i, j])
                    policy[i, j] = self.env.action_names[best_action]
                    values[i, j] = np.max(self.Q[i, j])

        return policy, values

    def train(self, n_episodes=1000):
        """å­¦ç¿’ã‚’å®Ÿè¡Œ"""
        episode_rewards = []
        episode_lengths = []

        for episode in range(n_episodes):
            state = self.env.reset()
            total_reward = 0
            steps = 0

            while steps < 100:  # æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°
                action = self.select_action(state)
                next_state, reward, done = self.env.step(action)
                self.update(state, action, reward, next_state, done)

                total_reward += reward
                steps += 1
                state = next_state

                if done:
                    break

            episode_rewards.append(total_reward)
            episode_lengths.append(steps)

            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                avg_length = np.mean(episode_lengths[-100:])
                print(f"ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {episode+1}: "
                      f"å¹³å‡å ±é…¬={avg_reward:.2f}, å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ•°={avg_length:.1f}")

        return episode_rewards, episode_lengths

# Grid World ç’°å¢ƒã®ä½œæˆ
env = GridWorldEnv(size=5, slip_prob=0.1)

print("ã€Grid World ç’°å¢ƒã€‘")
print("ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º: 5x5")
print("ã‚¹ãƒªãƒƒãƒ—ç¢ºç‡: 0.1")
print("ã‚´ãƒ¼ãƒ«: (4, 4) â†’ å ±é…¬ +10")
print("ç©´: (2, 3) â†’ å ±é…¬ -10")
print("å„ã‚¹ãƒ†ãƒƒãƒ—: å ±é…¬ -0.1\n")

# Qå­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä½œæˆã¨è¨“ç·´
agent = QLearningAgent(env, alpha=0.1, gamma=0.95, epsilon=0.1)

print("ã€Qå­¦ç¿’ã«ã‚ˆã‚‹è¨“ç·´ã€‘")
rewards, lengths = agent.train(n_episodes=500)

print("\nè¨“ç·´å®Œäº†\n")

# å­¦ç¿’ã—ãŸæ–¹ç­–ã®å–å¾—
policy, values = agent.get_policy()

print("ã€å­¦ç¿’ã—ãŸæ–¹ç­–ã€‘")
print(policy)
print()

# å¯è¦–åŒ–
fig1 = env.render(policy=policy, values=values)
plt.title('Qå­¦ç¿’ã§å­¦ç¿’ã—ãŸæ–¹ç­–ã¨ä¾¡å€¤é–¢æ•°', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('gridworld_qlearning.png', dpi=150, bbox_inches='tight')
print("æ–¹ç­–ã®å¯è¦–åŒ–ã‚’ 'gridworld_qlearning.png' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

# å­¦ç¿’æ›²ç·š
fig2, axes = plt.subplots(1, 2, figsize=(14, 5))

# å·¦ï¼šå ±é…¬ã®æ¨ç§»
window = 20
smoothed_rewards = np.convolve(rewards, np.ones(window)/window, mode='valid')
axes[0].plot(smoothed_rewards, linewidth=2)
axes[0].set_xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°', fontsize=12)
axes[0].set_ylabel('å¹³å‡å ±é…¬ï¼ˆç§»å‹•å¹³å‡ï¼‰', fontsize=12)
axes[0].set_title('å ±é…¬ã®æ¨ç§»', fontsize=14, fontweight='bold')
axes[0].grid(alpha=0.3)

# å³ï¼šã‚¹ãƒ†ãƒƒãƒ—æ•°ã®æ¨ç§»
smoothed_lengths = np.convolve(lengths, np.ones(window)/window, mode='valid')
axes[1].plot(smoothed_lengths, linewidth=2, color='orange')
axes[1].set_xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°', fontsize=12)
axes[1].set_ylabel('å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆç§»å‹•å¹³å‡ï¼‰', fontsize=12)
axes[1].set_title('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·ã®æ¨ç§»', fontsize=14, fontweight='bold')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('gridworld_learning_curves.png', dpi=150, bbox_inches='tight')
print("å­¦ç¿’æ›²ç·šã‚’ 'gridworld_learning_curves.png' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
</code></pre>

<hr>

<h2>ã¾ã¨ã‚</h2>

<p>ã“ã®ç« ã§ã¯ã€å¼·åŒ–å­¦ç¿’ã®åŸºç¤ã‚’å­¦ã³ã¾ã—ãŸï¼š</p>

<ul>
<li><strong>å¼·åŒ–å­¦ç¿’ã®å®šç¾©</strong>ï¼šè©¦è¡ŒéŒ¯èª¤ã‚’é€šã˜ã¦æœ€é©ãªè¡Œå‹•ã‚’å­¦ç¿’ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯</li>
<li><strong>MDP</strong>ï¼šçŠ¶æ…‹ã€è¡Œå‹•ã€å ±é…¬ã€é·ç§»ç¢ºç‡ã€å‰²å¼•ç‡ã«ã‚ˆã‚‹å®šå¼åŒ–</li>
<li><strong>ãƒ™ãƒ«ãƒãƒ³æ–¹ç¨‹å¼</strong>ï¼šä¾¡å€¤é–¢æ•°ã®å†å¸°çš„å®šç¾©ã¨å‹•çš„è¨ˆç”»æ³•ã®åŸºç¤</li>
<li><strong>ä¾¡å€¤åå¾©æ³•ãƒ»æ–¹ç­–åå¾©æ³•</strong>ï¼šæœ€é©æ–¹ç­–ã‚’æ±‚ã‚ã‚‹å‹•çš„è¨ˆç”»æ³•ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </li>
<li><strong>æ¢ç´¢ã¨æ´»ç”¨</strong>ï¼šÎµ-greedyã€UCBã€Softmax ãªã©ã®æˆ¦ç•¥</li>
<li><strong>ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã¨TDå­¦ç¿’</strong>ï¼šãƒ¢ãƒ‡ãƒ«ãƒ•ãƒªãƒ¼ãªå­¦ç¿’æ‰‹æ³•</li>
<li><strong>å®Ÿè·µ</strong>ï¼šGrid World ã§ã® Qå­¦ç¿’ã®å®Ÿè£…</li>
</ul>

<p>æ¬¡ç« ã§ã¯ã€ã‚ˆã‚Šé«˜åº¦ãªå¼·åŒ–å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆSARSAã€Qå­¦ç¿’ã€DQNï¼‰ã«ã¤ã„ã¦å­¦ã³ã¾ã™ã€‚</p>

<details>
<summary>æ¼”ç¿’å•é¡Œ</summary>

<h4>å•é¡Œ1ï¼šMDPã®ç†è§£</h4>
<p>å‰²å¼•ç‡ $\gamma$ ãŒ 0 ã«è¿‘ã„å ´åˆã¨ 1 ã«è¿‘ã„å ´åˆã§ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•ãŒã©ã®ã‚ˆã†ã«å¤‰ã‚ã‚‹ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<h4>å•é¡Œ2ï¼šãƒ™ãƒ«ãƒãƒ³æ–¹ç¨‹å¼</h4>
<p>çŠ¶æ…‹ä¾¡å€¤é–¢æ•° $V^\pi(s)$ ã¨è¡Œå‹•ä¾¡å€¤é–¢æ•° $Q^\pi(s, a)$ ã®é–¢ä¿‚å¼ã‚’å°å‡ºã—ã¦ãã ã•ã„ã€‚</p>

<h4>å•é¡Œ3ï¼šæ¢ç´¢æˆ¦ç•¥</h4>
<p>Îµ-greedy æ–¹ç­–ã«ãŠã„ã¦ã€$\epsilon = 0$ ã¨ $\epsilon = 1$ ã®æ¥µç«¯ãªå ´åˆã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã©ã®ã‚ˆã†ã«æŒ¯ã‚‹èˆã„ã¾ã™ã‹ï¼Ÿ</p>

<h4>å•é¡Œ4ï¼šMCã¨TD</h4>
<p>ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã¨TDå­¦ç¿’ã®é•ã„ã‚’ã€ãƒã‚¤ã‚¢ã‚¹ã¨åˆ†æ•£ã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<h4>å•é¡Œ5ï¼šä¾¡å€¤åå¾©æ³•</h4>
<p>æä¾›ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã‚’ä¿®æ­£ã—ã¦ã€3x3ã®ã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ã§ä¾¡å€¤åå¾©æ³•ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚</p>

<h4>å•é¡Œ6ï¼šÎµ-greedy ã®å®Ÿè£…</h4>
<p>å¤šè…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã§ã€$\epsilon$ ã‚’æ™‚é–“ã¨ã¨ã‚‚ã«æ¸›è¡°ã•ã›ã‚‹ Îµ-decay æˆ¦ç•¥ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼š$\epsilon_t = \epsilon_0 / (1 + t)$ï¼‰ã€‚</p>

</details>

<div class="navigation">
    <a href="../index.html" class="nav-button">ã‚³ãƒ¼ã‚¹ä¸€è¦§ã«æˆ»ã‚‹</a>
    <a href="chapter2-value-based-methods.html" class="nav-button">æ¬¡ã®ç« ã¸ï¼šä¾¡å€¤ãƒ™ãƒ¼ã‚¹æ‰‹æ³•</a>
</div>

</main>


    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
    <p>&copy; 2024 AI Terakoya. All rights reserved.</p>
</footer>

</body>
</html>
