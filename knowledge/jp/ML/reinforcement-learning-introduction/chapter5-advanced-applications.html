<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬5ç« ï¼šé«˜åº¦ãªRLæ‰‹æ³•ã¨å¿œç”¨ (Advanced RL Methods and Applications) - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬5ç« ï¼šé«˜åº¦ãªRLæ‰‹æ³•ã¨å¿œç”¨ (Advanced RL Methods and Applications)</h1>
            <p class="subtitle">æœ€æ–°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‹ã‚‰å®Ÿä¸–ç•Œå¿œç”¨ã¾ã§</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 7å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… A3Cã®ä¸¦åˆ—å­¦ç¿’ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ç†è§£ã—ã€æ¦‚å¿µå®Ÿè£…ãŒã§ãã‚‹</li>
<li>âœ… SACã®ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ»ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯æ§‹é€ ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¼·åŒ–å­¦ç¿’ã®åŸºæœ¬ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹å¼·åŒ–å­¦ç¿’ã®åŸç†ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã€ã‚²ãƒ¼ãƒ AIã€ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¸ã®å¿œç”¨ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Stable-Baselines3ã‚’ä½¿ã£ãŸå®Ÿè·µçš„ãªRLãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹ç¯‰ã§ãã‚‹</li>
<li>âœ… å®Ÿä¸–ç•Œã¸ã®é©ç”¨ã«ãŠã‘ã‚‹èª²é¡Œã¨è§£æ±ºç­–ã‚’ç†è§£ã§ãã‚‹</li>
</ul>

<hr>

<h2>5.1 A3C (Asynchronous Advantage Actor-Critic)</h2>

<h3>A3Cã®æ¦‚è¦</h3>

<p><strong>A3C (Asynchronous Advantage Actor-Critic)</strong>ã¯ã€DeepMindãŒ2016å¹´ã«ææ¡ˆã—ãŸä¸¦åˆ—å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚è¤‡æ•°ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒéåŒæœŸçš„ã«ç’°å¢ƒã¨ç›¸äº’ä½œç”¨ã—ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ›´æ–°ã™ã‚‹ã“ã¨ã§ã€é«˜é€Ÿã‹ã¤å®‰å®šã—ãŸå­¦ç¿’ã‚’å®Ÿç¾ã—ã¾ã™ã€‚</p>

<div class="mermaid">
graph TB
    GN[Global Network<br/>å…±æœ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ Î¸]

    W1[Worker 1<br/>ç’°å¢ƒã‚³ãƒ”ãƒ¼ 1]
    W2[Worker 2<br/>ç’°å¢ƒã‚³ãƒ”ãƒ¼ 2]
    W3[Worker 3<br/>ç’°å¢ƒã‚³ãƒ”ãƒ¼ 3]
    Wn[Worker N<br/>ç’°å¢ƒã‚³ãƒ”ãƒ¼ N]

    W1 -->|å‹¾é…æ›´æ–°| GN
    W2 -->|å‹¾é…æ›´æ–°| GN
    W3 -->|å‹¾é…æ›´æ–°| GN
    Wn -->|å‹¾é…æ›´æ–°| GN

    GN -->|ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒæœŸ| W1
    GN -->|ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒæœŸ| W2
    GN -->|ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒæœŸ| W3
    GN -->|ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒæœŸ| Wn

    style GN fill:#e3f2fd
    style W1 fill:#c8e6c9
    style W2 fill:#c8e6c9
    style W3 fill:#c8e6c9
    style Wn fill:#c8e6c9
</div>

<h4>A3Cã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</h4>

<table>
<thead>
<tr>
<th>ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</th>
<th>èª¬æ˜</th>
<th>ç‰¹å¾´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>éåŒæœŸæ›´æ–°</strong></td>
<td>å„ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒç‹¬ç«‹ã—ã¦å­¦ç¿’</td>
<td>Experience Replayä¸è¦ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„</td>
</tr>
<tr>
<td><strong>Advantageé–¢æ•°</strong></td>
<td>$A(s, a) = Q(s, a) - V(s)$</td>
<td>åˆ†æ•£æ¸›å°‘ã€å®‰å®šã—ãŸå­¦ç¿’</td>
</tr>
<tr>
<td><strong>ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–</strong></td>
<td>æ¢ç´¢ã‚’ä¿ƒé€²</td>
<td>æ—©æœŸåæŸã‚’é˜²ã</td>
</tr>
<tr>
<td><strong>ä¸¦åˆ—å®Ÿè¡Œ</strong></td>
<td>è¤‡æ•°ç’°å¢ƒã§åŒæ™‚å­¦ç¿’</td>
<td>å­¦ç¿’é€Ÿåº¦å‘ä¸Šã€å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿</td>
</tr>
</tbody>
</table>

<h3>A3Cã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h3>

<p>å„ãƒ¯ãƒ¼ã‚«ãƒ¼ã¯ä»¥ä¸‹ã®æ‰‹é †ã‚’ç¹°ã‚Šè¿”ã—ã¾ã™ï¼š</p>

<ol>
<li><strong>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒæœŸ</strong>: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ $\theta' \leftarrow \theta$</li>
<li><strong>çµŒé¨“åé›†</strong>: $t_{\text{max}}$ ã‚¹ãƒ†ãƒƒãƒ—ã¾ãŸã¯çµ‚ç«¯ã¾ã§ $(s_t, a_t, r_t)$ ã‚’åé›†</li>
<li><strong>ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—</strong>: $n$ã‚¹ãƒ†ãƒƒãƒ—ãƒªã‚¿ãƒ¼ãƒ³ $R_t = \sum_{i=0}^{n-1} \gamma^i r_{t+i} + \gamma^n V(s_{t+n})$</li>
<li><strong>å‹¾é…è¨ˆç®—</strong>: ã‚¢ã‚¯ã‚¿ãƒ¼ã¨ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã®æå¤±ã‚’è¨ˆç®—</li>
<li><strong>éåŒæœŸæ›´æ–°</strong>: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ›´æ–°</li>
</ol>

<p>æå¤±é–¢æ•°ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š</p>

$$
\mathcal{L}_{\text{actor}} = -\log \pi(a_t | s_t; \theta) A_t - \beta H(\pi(\cdot | s_t; \theta))
$$

$$
\mathcal{L}_{\text{critic}} = (R_t - V(s_t; \theta))^2
$$

<p>ã“ã“ã§ã€$H(\pi)$ã¯ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€$\beta$ã¯ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ä¿‚æ•°ã€$A_t = R_t - V(s_t)$ã¯Advantageæ¨å®šå€¤ã§ã™ã€‚</p>

<h3>A3Cã®æ¦‚å¿µå®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.multiprocessing as mp
from torch.distributions import Categorical
import gymnasium as gym
import numpy as np

class A3CNetwork(nn.Module):
    """
    A3Cç”¨ã®ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ»ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯å…±æœ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

    Architecture:
    - å…±æœ‰å±¤: ç‰¹å¾´æŠ½å‡º
    - ã‚¢ã‚¯ã‚¿ãƒ¼å‡ºåŠ›: è¡Œå‹•ç¢ºç‡åˆ†å¸ƒ
    - ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯å‡ºåŠ›: çŠ¶æ…‹ä¾¡å€¤é–¢æ•°
    """

    def __init__(self, state_dim, action_dim, hidden_dim=128):
        """
        Args:
            state_dim: çŠ¶æ…‹ç©ºé–“ã®æ¬¡å…ƒ
            action_dim: è¡Œå‹•ç©ºé–“ã®æ¬¡å…ƒ
            hidden_dim: éš ã‚Œå±¤ã®æ¬¡å…ƒ
        """
        super(A3CNetwork, self).__init__()

        # å…±æœ‰ç‰¹å¾´æŠ½å‡ºå±¤
        self.shared_fc1 = nn.Linear(state_dim, hidden_dim)
        self.shared_fc2 = nn.Linear(hidden_dim, hidden_dim)

        # ã‚¢ã‚¯ã‚¿ãƒ¼å‡ºåŠ›ï¼ˆè¡Œå‹•ç¢ºç‡ï¼‰
        self.actor_head = nn.Linear(hidden_dim, action_dim)

        # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯å‡ºåŠ›ï¼ˆçŠ¶æ…‹ä¾¡å€¤ï¼‰
        self.critic_head = nn.Linear(hidden_dim, 1)

    def forward(self, state):
        """
        å‰å‘ãè¨ˆç®—

        Args:
            state: çŠ¶æ…‹ (batch_size, state_dim)

        Returns:
            action_probs: è¡Œå‹•ç¢ºç‡åˆ†å¸ƒ (batch_size, action_dim)
            state_value: çŠ¶æ…‹ä¾¡å€¤ (batch_size, 1)
        """
        # å…±æœ‰å±¤
        x = F.relu(self.shared_fc1(state))
        x = F.relu(self.shared_fc2(x))

        # ã‚¢ã‚¯ã‚¿ãƒ¼å‡ºåŠ›
        action_logits = self.actor_head(x)
        action_probs = F.softmax(action_logits, dim=-1)

        # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯å‡ºåŠ›
        state_value = self.critic_head(x)

        return action_probs, state_value


class A3CWorker:
    """
    A3Cãƒ¯ãƒ¼ã‚«ãƒ¼: ç‹¬ç«‹ã—ãŸç’°å¢ƒã§å­¦ç¿’ã—ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ›´æ–°

    Features:
    - éåŒæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
    - n-step returnsè¨ˆç®—
    - ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–
    """

    def __init__(self, worker_id, global_network, optimizer,
                 env_name='CartPole-v1', gamma=0.99,
                 max_steps=20, entropy_coef=0.01):
        """
        Args:
            worker_id: ãƒ¯ãƒ¼ã‚«ãƒ¼ID
            global_network: å…±æœ‰ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
            optimizer: å…±æœ‰ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
            env_name: ç’°å¢ƒå
            gamma: å‰²å¼•ç‡
            max_steps: n-stepãƒªã‚¿ãƒ¼ãƒ³ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°
            entropy_coef: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ä¿‚æ•°
        """
        self.worker_id = worker_id
        self.env = gym.make(env_name)
        self.global_network = global_network
        self.optimizer = optimizer
        self.gamma = gamma
        self.max_steps = max_steps
        self.entropy_coef = entropy_coef

        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ã¨åŒã˜æ§‹é€ ï¼‰
        state_dim = self.env.observation_space.shape[0]
        action_dim = self.env.action_space.n
        self.local_network = A3CNetwork(state_dim, action_dim)

    def compute_returns(self, rewards, next_value, dones):
        """
        n-stepãƒªã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—

        Args:
            rewards: å ±é…¬ãƒªã‚¹ãƒˆ
            next_value: æœ€å¾Œã®çŠ¶æ…‹ã®ä¾¡å€¤æ¨å®š
            dones: çµ‚ç«¯ãƒ•ãƒ©ã‚°ãƒªã‚¹ãƒˆ

        Returns:
            returns: å„ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒªã‚¿ãƒ¼ãƒ³
        """
        returns = []
        R = next_value

        # é€†é †ã§è¨ˆç®—
        for r, done in zip(reversed(rewards), reversed(dones)):
            R = r + self.gamma * R * (1 - done)
            returns.insert(0, R)

        return returns

    def train_step(self):
        """
        1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ†ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

        Returns:
            total_reward: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆè¨ˆå ±é…¬
        """
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒæœŸ
        self.local_network.load_state_dict(self.global_network.state_dict())

        state, _ = self.env.reset()
        done = False

        states, actions, rewards, dones, values = [], [], [], [], []
        episode_reward = 0

        while not done:
            # è¡Œå‹•é¸æŠ
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            action_probs, value = self.local_network(state_tensor)

            dist = Categorical(action_probs)
            action = dist.sample()

            # ç’°å¢ƒã‚¹ãƒ†ãƒƒãƒ—
            next_state, reward, terminated, truncated, _ = self.env.step(action.item())
            done = terminated or truncated

            # çµŒé¨“ã‚’ä¿å­˜
            states.append(state)
            actions.append(action)
            rewards.append(reward)
            dones.append(done)
            values.append(value)

            episode_reward += reward
            state = next_state

            # max_stepsã”ã¨ã¾ãŸã¯çµ‚ç«¯ã§æ›´æ–°
            if len(states) >= self.max_steps or done:
                # æ¬¡çŠ¶æ…‹ã®ä¾¡å€¤æ¨å®š
                if done:
                    next_value = 0
                else:
                    next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)
                    _, next_value = self.local_network(next_state_tensor)
                    next_value = next_value.item()

                # ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—
                returns = self.compute_returns(rewards, next_value, dones)

                # æå¤±è¨ˆç®—
                self._update_global_network(states, actions, returns, values)

                # ãƒãƒƒãƒ•ã‚¡ã‚¯ãƒªã‚¢
                states, actions, rewards, dones, values = [], [], [], [], []

        return episode_reward

    def _update_global_network(self, states, actions, returns, values):
        """
        ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ›´æ–°

        Args:
            states: çŠ¶æ…‹ãƒªã‚¹ãƒˆ
            actions: è¡Œå‹•ãƒªã‚¹ãƒˆ
            returns: ãƒªã‚¿ãƒ¼ãƒ³ãƒªã‚¹ãƒˆ
            values: ä¾¡å€¤æ¨å®šãƒªã‚¹ãƒˆ
        """
        states_tensor = torch.FloatTensor(states)
        actions_tensor = torch.LongTensor(actions)
        returns_tensor = torch.FloatTensor(returns)

        # å†è¨ˆç®—
        action_probs, state_values = self.local_network(states_tensor)
        state_values = state_values.squeeze()

        # Advantageè¨ˆç®—
        advantages = returns_tensor - state_values.detach()

        # ã‚¢ã‚¯ã‚¿ãƒ¼æå¤±ï¼ˆPolicy Gradient + Entropyï¼‰
        dist = Categorical(action_probs)
        log_probs = dist.log_prob(actions_tensor)
        entropy = dist.entropy().mean()
        actor_loss = -(log_probs * advantages).mean() - self.entropy_coef * entropy

        # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯æå¤±ï¼ˆMSEï¼‰
        critic_loss = F.mse_loss(state_values, returns_tensor)

        # åˆè¨ˆæå¤±
        total_loss = actor_loss + critic_loss

        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°
        self.optimizer.zero_grad()
        total_loss.backward()

        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
        torch.nn.utils.clip_grad_norm_(self.local_network.parameters(), 40)

        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«å‹¾é…ã‚’è»¢é€
        for local_param, global_param in zip(
            self.local_network.parameters(),
            self.global_network.parameters()
        ):
            if global_param.grad is not None:
                return  # ä»–ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒæ›´æ–°ä¸­
            global_param._grad = local_param.grad

        self.optimizer.step()


def worker_process(worker_id, global_network, optimizer, num_episodes=100):
    """
    ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹é–¢æ•°ï¼ˆä¸¦åˆ—å®Ÿè¡Œç”¨ï¼‰

    Args:
        worker_id: ãƒ¯ãƒ¼ã‚«ãƒ¼ID
        global_network: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        optimizer: å…±æœ‰ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        num_episodes: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°
    """
    worker = A3CWorker(worker_id, global_network, optimizer)

    for episode in range(num_episodes):
        reward = worker.train_step()
        if episode % 10 == 0:
            print(f"Worker {worker_id} - Episode {episode}, Reward: {reward:.2f}")


# A3Cè¨“ç·´ä¾‹ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒ—ãƒ­ã‚»ã‚¹ç‰ˆ - æ¦‚å¿µå®Ÿè¨¼ç”¨ï¼‰
def train_a3c_simple():
    """
    A3Cè¨“ç·´ã®ç°¡æ˜“ç‰ˆï¼ˆä¸¦åˆ—å‡¦ç†ãªã—ï¼‰
    å®Ÿéš›ã®A3Cã¯ multiprocessing ã‚’ä½¿ç”¨
    """
    env = gym.make('CartPole-v1')
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    global_network = A3CNetwork(state_dim, action_dim)
    global_network.share_memory()  # ãƒ—ãƒ­ã‚»ã‚¹é–“å…±æœ‰ç”¨

    optimizer = torch.optim.Adam(global_network.parameters(), lr=0.0001)

    # å˜ä¸€ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¾‹
    worker = A3CWorker(0, global_network, optimizer)

    rewards = []
    for episode in range(100):
        reward = worker.train_step()
        rewards.append(reward)

        if episode % 10 == 0:
            avg_reward = np.mean(rewards[-10:])
            print(f"Episode {episode}, Avg Reward: {avg_reward:.2f}")

    return global_network, rewards


# å®Ÿè¡Œä¾‹
if __name__ == "__main__":
    print("A3C Training (Simple Version)")
    print("=" * 50)
    model, rewards = train_a3c_simple()
    print(f"Training completed. Final avg reward: {np.mean(rewards[-10:]):.2f}")
</code></pre>

<blockquote>
<p><strong>A3Cã®å®Ÿè£…ãƒã‚¤ãƒ³ãƒˆ</strong>: å®Œå…¨ãªä¸¦åˆ—ç‰ˆã¯Pythonã®<code>multiprocessing</code>ã‚’ä½¿ç”¨ã—ã¾ã™ãŒã€ä¸Šè¨˜ã¯æ¦‚å¿µã‚’ç¤ºã™ç°¡æ˜“ç‰ˆã§ã™ã€‚å®Ÿéš›ã®A3Cã§ã¯ã€è¤‡æ•°ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ãŒåŒæ™‚ã«ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ›´æ–°ã—ã¾ã™ã€‚ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ã«ã‚ˆã‚Šæ¢ç´¢ãŒä¿ƒé€²ã•ã‚Œã€å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚Šå­¦ç¿’ãŒå®‰å®šã—ã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>5.2 SAC (Soft Actor-Critic)</h2>

<h3>SACã®æ¦‚è¦</h3>

<p><strong>SAC (Soft Actor-Critic)</strong>ã¯ã€æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å¼·åŒ–å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«åŸºã¥ãã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚å ±é…¬æœ€å¤§åŒ–ã¨æ¢ç´¢ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è‡ªå‹•çš„ã«èª¿æ•´ã—ã€é€£ç¶šè¡Œå‹•ç©ºé–“ã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    S[çŠ¶æ…‹ s] --> A[Actor Ï€<br/>ç¢ºç‡çš„æ–¹ç­–]
    S --> Q1[Q-Network 1<br/>Qâ‚s,a]
    S --> Q2[Q-Network 2<br/>Qâ‚‚s,a]
    S --> V[Value Network<br/>Vs]

    A --> |è¡Œå‹• a| E[ç’°å¢ƒ]
    Q1 --> |æœ€å°å€¤| MIN[min Q]
    Q2 --> |æœ€å°å€¤| MIN

    E --> |å ±é…¬ + ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼| R[æœ€å¤§åŒ–ç›®æ¨™]
    MIN --> R
    V --> R

    style A fill:#e3f2fd
    style Q1 fill:#fff9c4
    style Q2 fill:#fff9c4
    style V fill:#c8e6c9
    style R fill:#ffccbc
</div>

<h4>SACã®ä¸»è¦ç‰¹å¾´</h4>

<table>
<thead>
<tr>
<th>ç‰¹å¾´</th>
<th>èª¬æ˜</th>
<th>åˆ©ç‚¹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç›®æ¨™</strong></td>
<td>å ±é…¬ + ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’æœ€å¤§åŒ–</td>
<td>è‡ªå‹•æ¢ç´¢ã€ãƒ­ãƒã‚¹ãƒˆãªæ–¹ç­–</td>
</tr>
<tr>
<td><strong>Double Q-Learning</strong></td>
<td>2ã¤ã®Q-Networkã§éå¤§æ¨å®šã‚’é˜²ã</td>
<td>å®‰å®šã—ãŸå­¦ç¿’</td>
</tr>
<tr>
<td><strong>Off-Policy</strong></td>
<td>Experience Replayã‚’ä½¿ç”¨</td>
<td>ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ãŒé«˜ã„</td>
</tr>
<tr>
<td><strong>è‡ªå‹•æ¸©åº¦èª¿æ•´</strong></td>
<td>ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ä¿‚æ•°Î±ã‚’å­¦ç¿’</td>
<td>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ä¸è¦</td>
</tr>
</tbody>
</table>

<h3>SACã®ç›®çš„é–¢æ•°</h3>

<p>SACã¯ä»¥ä¸‹ã®æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç›®çš„ã‚’æœ€é©åŒ–ã—ã¾ã™ï¼š</p>

$$
J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{\infty} \gamma^t (r(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot | s_t))) \right]
$$

<p>ã“ã“ã§ã€$\mathcal{H}(\pi)$ã¯æ–¹ç­–ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€$\alpha$ã¯æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã™ã€‚</p>

<p><strong>ã‚¢ã‚¯ã‚¿ãƒ¼æ›´æ–°</strong>ï¼ˆæ–¹ç­–æ”¹å–„ï¼‰ï¼š</p>

$$
\mathcal{L}_{\pi}(\theta) = \mathbb{E}_{s_t \sim \mathcal{D}} \left[ \mathbb{E}_{a_t \sim \pi_\theta} [\alpha \log \pi_\theta(a_t | s_t) - Q(s_t, a_t)] \right]
$$

<p><strong>ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯æ›´æ–°</strong>ï¼ˆBellmanèª¤å·®æœ€å°åŒ–ï¼‰ï¼š</p>

$$
\mathcal{L}_Q(\phi) = \mathbb{E}_{(s, a, r, s') \sim \mathcal{D}} \left[ (Q_\phi(s, a) - (r + \gamma V(s')))^2 \right]
$$

<p>ã“ã“ã§ã€$V(s') = \mathbb{E}_{a' \sim \pi}[Q(s', a') - \alpha \log \pi(a' | s')]$ã§ã™ã€‚</p>

<h3>SACã®å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal
import numpy as np
from collections import deque
import random

class GaussianPolicy(nn.Module):
    """
    SACç”¨ã®ã‚¬ã‚¦ã‚¹æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

    Architecture:
    - çŠ¶æ…‹ã‚’å…¥åŠ›
    - å¹³å‡Î¼ã¨æ¨™æº–åå·®Ïƒã‚’å‡ºåŠ›
    - Reparameterization Trickã§å¾®åˆ†å¯èƒ½ãªè¡Œå‹•ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    """

    def __init__(self, state_dim, action_dim, hidden_dim=256,
                 log_std_min=-20, log_std_max=2):
        """
        Args:
            state_dim: çŠ¶æ…‹ç©ºé–“ã®æ¬¡å…ƒ
            action_dim: è¡Œå‹•ç©ºé–“ã®æ¬¡å…ƒ
            hidden_dim: éš ã‚Œå±¤ã®æ¬¡å…ƒ
            log_std_min: logæ¨™æº–åå·®ã®æœ€å°å€¤
            log_std_max: logæ¨™æº–åå·®ã®æœ€å¤§å€¤
        """
        super(GaussianPolicy, self).__init__()

        self.log_std_min = log_std_min
        self.log_std_max = log_std_max

        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)

        self.mean = nn.Linear(hidden_dim, action_dim)
        self.log_std = nn.Linear(hidden_dim, action_dim)

    def forward(self, state):
        """
        å‰å‘ãè¨ˆç®—

        Args:
            state: çŠ¶æ…‹ (batch_size, state_dim)

        Returns:
            mean: è¡Œå‹•åˆ†å¸ƒã®å¹³å‡
            log_std: è¡Œå‹•åˆ†å¸ƒã®logæ¨™æº–åå·®
        """
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))

        mean = self.mean(x)
        log_std = self.log_std(x)
        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)

        return mean, log_std

    def sample(self, state):
        """
        Reparameterization Trickã«ã‚ˆã‚‹è¡Œå‹•ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

        Args:
            state: çŠ¶æ…‹

        Returns:
            action: ã‚µãƒ³ãƒ—ãƒ«ã•ã‚ŒãŸè¡Œå‹•ï¼ˆtanh squashingé©ç”¨å¾Œï¼‰
            log_prob: è¡Œå‹•ã®å¯¾æ•°ç¢ºç‡
        """
        mean, log_std = self.forward(state)
        std = log_std.exp()

        # ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        normal = Normal(mean, std)
        x_t = normal.rsample()  # Reparameterization trick

        # tanh squashingã§[-1, 1]ã«åˆ¶é™
        action = torch.tanh(x_t)

        # å¯¾æ•°ç¢ºç‡ï¼ˆtanhå¤‰æ›ã®è£œæ­£å«ã‚€ï¼‰
        log_prob = normal.log_prob(x_t)
        log_prob -= torch.log(1 - action.pow(2) + 1e-6)
        log_prob = log_prob.sum(1, keepdim=True)

        return action, log_prob


class QNetwork(nn.Module):
    """
    SACç”¨ã®Q-Networkï¼ˆçŠ¶æ…‹-è¡Œå‹•ä¾¡å€¤é–¢æ•°ï¼‰
    """

    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(QNetwork, self).__init__()

        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)

    def forward(self, state, action):
        """
        Qå€¤ã‚’è¨ˆç®—

        Args:
            state: çŠ¶æ…‹ (batch_size, state_dim)
            action: è¡Œå‹• (batch_size, action_dim)

        Returns:
            q_value: Qå€¤ (batch_size, 1)
        """
        x = torch.cat([state, action], dim=1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        q_value = self.fc3(x)
        return q_value


class ReplayBuffer:
    """Experience Replay Buffer"""

    def __init__(self, capacity=100000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = zip(*batch)
        return (
            np.array(state),
            np.array(action),
            np.array(reward).reshape(-1, 1),
            np.array(next_state),
            np.array(done).reshape(-1, 1)
        )

    def __len__(self):
        return len(self.buffer)


class SAC:
    """
    Soft Actor-Critic Implementation

    Features:
    - Maximum entropy reinforcement learning
    - Double Q-learning for stability
    - Automatic temperature tuning
    - Off-policy learning with replay buffer
    """

    def __init__(self, state_dim, action_dim,
                 lr=3e-4, gamma=0.99, tau=0.005, alpha=0.2,
                 automatic_entropy_tuning=True):
        """
        Args:
            state_dim: çŠ¶æ…‹ç©ºé–“ã®æ¬¡å…ƒ
            action_dim: è¡Œå‹•ç©ºé–“ã®æ¬¡å…ƒ
            lr: å­¦ç¿’ç‡
            gamma: å‰²å¼•ç‡
            tau: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°ç‡
            alpha: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ä¿‚æ•°ï¼ˆautomatic_entropy_tuning=Falseã®å ´åˆï¼‰
            automatic_entropy_tuning: è‡ªå‹•æ¸©åº¦èª¿æ•´ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        """
        self.gamma = gamma
        self.tau = tau
        self.alpha = alpha

        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–
        self.policy = GaussianPolicy(state_dim, action_dim)

        self.q_net1 = QNetwork(state_dim, action_dim)
        self.q_net2 = QNetwork(state_dim, action_dim)

        self.target_q_net1 = QNetwork(state_dim, action_dim)
        self.target_q_net2 = QNetwork(state_dim, action_dim)

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼
        self.target_q_net1.load_state_dict(self.q_net1.state_dict())
        self.target_q_net2.load_state_dict(self.q_net2.state_dict())

        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)
        self.q1_optimizer = torch.optim.Adam(self.q_net1.parameters(), lr=lr)
        self.q2_optimizer = torch.optim.Adam(self.q_net2.parameters(), lr=lr)

        # è‡ªå‹•æ¸©åº¦èª¿æ•´
        self.automatic_entropy_tuning = automatic_entropy_tuning
        if automatic_entropy_tuning:
            self.target_entropy = -action_dim
            self.log_alpha = torch.zeros(1, requires_grad=True)
            self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=lr)

        self.replay_buffer = ReplayBuffer()

    def select_action(self, state, evaluate=False):
        """
        è¡Œå‹•é¸æŠ

        Args:
            state: çŠ¶æ…‹
            evaluate: è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ï¼ˆæ±ºå®šçš„è¡Œå‹•ï¼‰

        Returns:
            action: é¸æŠã•ã‚ŒãŸè¡Œå‹•
        """
        state = torch.FloatTensor(state).unsqueeze(0)

        if evaluate:
            with torch.no_grad():
                mean, _ = self.policy(state)
                action = torch.tanh(mean)
        else:
            with torch.no_grad():
                action, _ = self.policy.sample(state)

        return action.cpu().numpy()[0]

    def update(self, batch_size=256):
        """
        SACæ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—

        Args:
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        """
        if len(self.replay_buffer) < batch_size:
            return

        # ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)

        state = torch.FloatTensor(state)
        action = torch.FloatTensor(action)
        reward = torch.FloatTensor(reward)
        next_state = torch.FloatTensor(next_state)
        done = torch.FloatTensor(done)

        # --- Q-Networkæ›´æ–° ---
        with torch.no_grad():
            next_action, next_log_prob = self.policy.sample(next_state)

            # Double Q-learning: æœ€å°å€¤ã‚’ä½¿ç”¨
            target_q1 = self.target_q_net1(next_state, next_action)
            target_q2 = self.target_q_net2(next_state, next_action)
            target_q = torch.min(target_q1, target_q2)

            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é …ã‚’å«ã‚€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            target_value = reward + (1 - done) * self.gamma * (
                target_q - self.alpha * next_log_prob
            )

        # Q1æå¤±
        q1_value = self.q_net1(state, action)
        q1_loss = F.mse_loss(q1_value, target_value)

        # Q2æå¤±
        q2_value = self.q_net2(state, action)
        q2_loss = F.mse_loss(q2_value, target_value)

        # Q-Networkæ›´æ–°
        self.q1_optimizer.zero_grad()
        q1_loss.backward()
        self.q1_optimizer.step()

        self.q2_optimizer.zero_grad()
        q2_loss.backward()
        self.q2_optimizer.step()

        # --- Policyæ›´æ–° ---
        new_action, log_prob = self.policy.sample(state)

        q1_new = self.q_net1(state, new_action)
        q2_new = self.q_net2(state, new_action)
        q_new = torch.min(q1_new, q2_new)

        policy_loss = (self.alpha * log_prob - q_new).mean()

        self.policy_optimizer.zero_grad()
        policy_loss.backward()
        self.policy_optimizer.step()

        # --- æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ï¼ˆè‡ªå‹•èª¿æ•´ï¼‰ ---
        if self.automatic_entropy_tuning:
            alpha_loss = -(self.log_alpha * (log_prob + self.target_entropy).detach()).mean()

            self.alpha_optimizer.zero_grad()
            alpha_loss.backward()
            self.alpha_optimizer.step()

            self.alpha = self.log_alpha.exp().item()

        # --- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚½ãƒ•ãƒˆæ›´æ–° ---
        self._soft_update(self.q_net1, self.target_q_net1)
        self._soft_update(self.q_net2, self.target_q_net2)

    def _soft_update(self, source, target):
        """
        ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚½ãƒ•ãƒˆæ›´æ–°
        Î¸_target = Ï„ * Î¸_source + (1 - Ï„) * Î¸_target
        """
        for target_param, source_param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(
                self.tau * source_param.data + (1 - self.tau) * target_param.data
            )


# SACè¨“ç·´ä¾‹
def train_sac():
    """SACè¨“ç·´ã®å®Ÿè¡Œä¾‹ï¼ˆPendulumç’°å¢ƒï¼‰"""
    import gymnasium as gym

    env = gym.make('Pendulum-v1')
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]

    agent = SAC(state_dim, action_dim)

    num_episodes = 100
    max_steps = 200

    for episode in range(num_episodes):
        state, _ = env.reset()
        episode_reward = 0

        for step in range(max_steps):
            # è¡Œå‹•é¸æŠ
            action = agent.select_action(state)

            # ç’°å¢ƒã‚¹ãƒ†ãƒƒãƒ—
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            # ãƒãƒƒãƒ•ã‚¡ã«ä¿å­˜
            agent.replay_buffer.push(state, action, reward, next_state, done)

            # æ›´æ–°
            agent.update()

            episode_reward += reward
            state = next_state

            if done:
                break

        if episode % 10 == 0:
            print(f"Episode {episode}, Reward: {episode_reward:.2f}, Alpha: {agent.alpha:.3f}")

    return agent


if __name__ == "__main__":
    print("SAC Training on Pendulum-v1")
    print("=" * 50)
    agent = train_sac()
    print("Training completed!")
</code></pre>

<blockquote>
<p><strong>SACã®å®Ÿè£…ãƒã‚¤ãƒ³ãƒˆ</strong>: Reparameterization Trickã«ã‚ˆã‚Šæ–¹ç­–ãŒå¾®åˆ†å¯èƒ½ã«ãªã‚Šã€åŠ¹ç‡çš„ãªå‹¾é…ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ãŒå¯èƒ½ã§ã™ã€‚Double Q-learningã§éå¤§æ¨å®šã‚’é˜²ãã€è‡ªå‹•æ¸©åº¦èª¿æ•´ã«ã‚ˆã‚Šæ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹ãŒè‡ªå‹•çš„ã«æœ€é©åŒ–ã•ã‚Œã¾ã™ã€‚tanh squashingã«ã‚ˆã‚Šè¡Œå‹•ãŒæœ‰ç•Œç¯„å›²ã«åˆ¶é™ã•ã‚Œã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>5.3 ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¼·åŒ–å­¦ç¿’ (Multi-Agent RL)</h2>

<h3>ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¼·åŒ–å­¦ç¿’ã®åŸºæœ¬</h3>

<p><strong>ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¼·åŒ–å­¦ç¿’ (MARL)</strong>ã§ã¯ã€è¤‡æ•°ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒåŒã˜ç’°å¢ƒã§åŒæ™‚ã«å­¦ç¿’ãƒ»è¡Œå‹•ã—ã¾ã™ã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®ç›¸äº’ä½œç”¨ã«ã‚ˆã‚Šã€ã‚·ãƒ³ã‚°ãƒ«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆRLã¨ã¯ç•°ãªã‚‹èª²é¡ŒãŒç”Ÿã˜ã¾ã™ã€‚</p>

<div class="mermaid">
graph TB
    ENV[ç’°å¢ƒ Environment]

    A1[Agent 1<br/>æ–¹ç­– Ï€â‚]
    A2[Agent 2<br/>æ–¹ç­– Ï€â‚‚]
    A3[Agent 3<br/>æ–¹ç­– Ï€â‚ƒ]

    A1 --> |è¡Œå‹• aâ‚| ENV
    A2 --> |è¡Œå‹• aâ‚‚| ENV
    A3 --> |è¡Œå‹• aâ‚ƒ| ENV

    ENV --> |è¦³æ¸¬ oâ‚, å ±é…¬ râ‚| A1
    ENV --> |è¦³æ¸¬ oâ‚‚, å ±é…¬ râ‚‚| A2
    ENV --> |è¦³æ¸¬ oâ‚ƒ, å ±é…¬ râ‚ƒ| A3

    A1 -.-> |è¦³æ¸¬ãƒ»é€šä¿¡| A2
    A2 -.-> |è¦³æ¸¬ãƒ»é€šä¿¡| A3
    A3 -.-> |è¦³æ¸¬ãƒ»é€šä¿¡| A1

    style ENV fill:#e3f2fd
    style A1 fill:#c8e6c9
    style A2 fill:#fff9c4
    style A3 fill:#ffccbc
</div>

<h4>MARLã®ä¸»è¦ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ </h4>

<table>
<thead>
<tr>
<th>ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ </th>
<th>èª¬æ˜</th>
<th>ç”¨é€”</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Cooperativeï¼ˆå”èª¿ï¼‰</strong></td>
<td>å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå…±é€šç›®æ¨™ã‚’å…±æœ‰</td>
<td>ãƒãƒ¼ãƒ ã‚¹ãƒãƒ¼ãƒ„ã€å”èª¿ãƒ­ãƒœãƒƒãƒˆ</td>
</tr>
<tr>
<td><strong>Competitiveï¼ˆç«¶äº‰ï¼‰</strong></td>
<td>ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã§ã‚¼ãƒ­ã‚µãƒ </td>
<td>ã‚²ãƒ¼ãƒ AIã€å¯¾æˆ¦å‹ã‚¿ã‚¹ã‚¯</td>
</tr>
<tr>
<td><strong>Mixedï¼ˆæ··åˆï¼‰</strong></td>
<td>å”èª¿ã¨ç«¶äº‰ã®ä¸¡æ–¹ãŒå­˜åœ¨</td>
<td>çµŒæ¸ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€äº¤æ¸‰</td>
</tr>
</tbody>
</table>

<h4>MARLã®èª²é¡Œ</h4>

<ul>
<li><strong>éå®šå¸¸æ€§</strong>: ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å­¦ç¿’ã«ã‚ˆã‚Šç’°å¢ƒãŒå‹•çš„ã«å¤‰åŒ–</li>
<li><strong>ä¿¡ç”¨å‰²å½“</strong>: å ±é…¬ã‚’å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«é©åˆ‡ã«å‰²ã‚Šå½“ã¦ã‚‹</li>
<li><strong>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£</strong>: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°ã®å¢—åŠ ã«ä¼´ã†è¨ˆç®—é‡å¢—å¤§</li>
<li><strong>é€šä¿¡</strong>: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®åŠ¹æœçš„ãªæƒ…å ±å…±æœ‰</li>
</ul>

<h3>ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç’°å¢ƒã®å®Ÿè£…</h3>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Circle
import gymnasium as gym
from gymnasium import spaces

class SimpleMultiAgentEnv(gym.Env):
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç’°å¢ƒ

    Task: è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç›®æ¨™åœ°ç‚¹ã«åˆ°é”
    - ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯2Dç©ºé–“ã‚’ç§»å‹•
    - ç›®æ¨™åœ°ç‚¹ã«è¿‘ã¥ãã¨æ­£ã®å ±é…¬
    - ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåŒå£«ãŒè¿‘ã™ãã‚‹ã¨è² ã®å ±é…¬ï¼ˆè¡çªå›é¿ï¼‰
    - å”èª¿çš„ã‚¿ã‚¹ã‚¯ï¼ˆå…±é€šå ±é…¬ï¼‰
    """

    def __init__(self, n_agents=3, grid_size=10, max_steps=50):
        """
        Args:
            n_agents: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°
            grid_size: ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º
            max_steps: æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°
        """
        super(SimpleMultiAgentEnv, self).__init__()

        self.n_agents = n_agents
        self.grid_size = grid_size
        self.max_steps = max_steps

        # è¡Œå‹•ç©ºé–“: ä¸Šä¸‹å·¦å³ã®4æ–¹å‘
        self.action_space = spaces.Discrete(4)

        # è¦³æ¸¬ç©ºé–“: [è‡ªåˆ†ã®x, y, ç›®æ¨™ã¾ã§ã®xè·é›¢, yè·é›¢, ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã®ç›¸å¯¾ä½ç½®...]
        obs_dim = 2 + 2 + (n_agents - 1) * 2
        self.observation_space = spaces.Box(
            low=-grid_size, high=grid_size,
            shape=(obs_dim,), dtype=np.float32
        )

        self.agent_positions = None
        self.goal_position = None
        self.current_step = 0

    def reset(self, seed=None):
        """ç’°å¢ƒãƒªã‚»ãƒƒãƒˆ"""
        super().reset(seed=seed)

        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ãƒ©ãƒ³ãƒ€ãƒ é…ç½®
        self.agent_positions = np.random.rand(self.n_agents, 2) * self.grid_size

        # ç›®æ¨™ã‚’ãƒ©ãƒ³ãƒ€ãƒ é…ç½®
        self.goal_position = np.random.rand(2) * self.grid_size

        self.current_step = 0

        return self._get_observations(), {}

    def step(self, actions):
        """
        ç’°å¢ƒã‚¹ãƒ†ãƒƒãƒ—

        Args:
            actions: å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•ãƒªã‚¹ãƒˆ

        Returns:
            observations: å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¦³æ¸¬
            rewards: å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å ±é…¬
            terminated: çµ‚äº†ãƒ•ãƒ©ã‚°
            truncated: æ‰“ã¡åˆ‡ã‚Šãƒ•ãƒ©ã‚°
            info: è¿½åŠ æƒ…å ±
        """
        # è¡Œå‹•ã‚’é©ç”¨ï¼ˆä¸Šä¸‹å·¦å³ç§»å‹•ï¼‰
        for i, action in enumerate(actions):
            if action == 0:  # ä¸Š
                self.agent_positions[i, 1] = min(self.grid_size, self.agent_positions[i, 1] + 0.5)
            elif action == 1:  # ä¸‹
                self.agent_positions[i, 1] = max(0, self.agent_positions[i, 1] - 0.5)
            elif action == 2:  # å³
                self.agent_positions[i, 0] = min(self.grid_size, self.agent_positions[i, 0] + 0.5)
            elif action == 3:  # å·¦
                self.agent_positions[i, 0] = max(0, self.agent_positions[i, 0] - 0.5)

        # å ±é…¬è¨ˆç®—
        rewards = self._compute_rewards()

        # çµ‚äº†åˆ¤å®š
        self.current_step += 1
        terminated = self._is_done()
        truncated = self.current_step >= self.max_steps

        observations = self._get_observations()

        return observations, rewards, terminated, truncated, {}

    def _get_observations(self):
        """å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¦³æ¸¬ã‚’å–å¾—"""
        observations = []

        for i in range(self.n_agents):
            obs = []

            # è‡ªåˆ†ã®ä½ç½®
            obs.extend(self.agent_positions[i])

            # ç›®æ¨™ã¾ã§ã®è·é›¢
            obs.extend(self.goal_position - self.agent_positions[i])

            # ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã®ç›¸å¯¾ä½ç½®
            for j in range(self.n_agents):
                if i != j:
                    obs.extend(self.agent_positions[j] - self.agent_positions[i])

            observations.append(np.array(obs, dtype=np.float32))

        return observations

    def _compute_rewards(self):
        """å ±é…¬è¨ˆç®—"""
        rewards = []

        for i in range(self.n_agents):
            reward = 0

            # ç›®æ¨™ã¸ã®è·é›¢ã«åŸºã¥ãå ±é…¬
            dist_to_goal = np.linalg.norm(self.agent_positions[i] - self.goal_position)
            reward -= dist_to_goal * 0.1

            # ç›®æ¨™åˆ°é”ãƒœãƒ¼ãƒŠã‚¹
            if dist_to_goal < 0.5:
                reward += 10.0

            # è¡çªå›é¿ãƒšãƒŠãƒ«ãƒ†ã‚£
            for j in range(self.n_agents):
                if i != j:
                    dist_to_agent = np.linalg.norm(
                        self.agent_positions[i] - self.agent_positions[j]
                    )
                    if dist_to_agent < 1.0:
                        reward -= 2.0

            rewards.append(reward)

        return rewards

    def _is_done(self):
        """å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç›®æ¨™ã«åˆ°é”ã—ãŸã‹"""
        for i in range(self.n_agents):
            dist = np.linalg.norm(self.agent_positions[i] - self.goal_position)
            if dist >= 0.5:
                return False
        return True

    def render(self):
        """ç’°å¢ƒã®å¯è¦–åŒ–"""
        plt.figure(figsize=(8, 8))
        plt.xlim(0, self.grid_size)
        plt.ylim(0, self.grid_size)

        # ç›®æ¨™ã‚’æç”»
        goal_circle = Circle(self.goal_position, 0.5, color='gold', alpha=0.6, label='Goal')
        plt.gca().add_patch(goal_circle)

        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æç”»
        colors = ['blue', 'red', 'green', 'purple', 'orange']
        for i in range(self.n_agents):
            agent_circle = Circle(
                self.agent_positions[i], 0.3,
                color=colors[i % len(colors)],
                alpha=0.8,
                label=f'Agent {i+1}'
            )
            plt.gca().add_patch(agent_circle)

        plt.legend()
        plt.title(f'Multi-Agent Environment (Step {self.current_step})')
        plt.grid(True, alpha=0.3)
        plt.show()


class IndependentQLearning:
    """
    Independent Q-Learning for MARL
    å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç‹¬ç«‹ã—ã¦Qå­¦ç¿’ã‚’å®Ÿè¡Œ
    """

    def __init__(self, n_agents, state_dim, n_actions,
                 lr=0.1, gamma=0.99, epsilon=0.1):
        """
        Args:
            n_agents: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°
            state_dim: çŠ¶æ…‹ç©ºé–“ã®æ¬¡å…ƒ
            n_actions: è¡Œå‹•æ•°
            lr: å­¦ç¿’ç‡
            gamma: å‰²å¼•ç‡
            epsilon: Îµ-greedyæ¢ç´¢ç‡
        """
        self.n_agents = n_agents
        self.n_actions = n_actions
        self.lr = lr
        self.gamma = gamma
        self.epsilon = epsilon

        # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ã®Q-tableï¼ˆç°¡æ˜“ç‰ˆ: é›¢æ•£åŒ–ï¼‰
        # å®Ÿéš›ã¯é–¢æ•°è¿‘ä¼¼ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆï¼‰ã‚’ä½¿ç”¨
        self.q_tables = [
            np.zeros((100, n_actions)) for _ in range(n_agents)
        ]

    def select_actions(self, observations):
        """Îµ-greedyè¡Œå‹•é¸æŠ"""
        actions = []

        for i in range(self.n_agents):
            if np.random.rand() < self.epsilon:
                action = np.random.randint(self.n_actions)
            else:
                # è¦³æ¸¬ã‚’é›¢æ•£åŒ–ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                state_idx = self._discretize_state(observations[i])
                action = np.argmax(self.q_tables[i][state_idx])

            actions.append(action)

        return actions

    def update(self, observations, actions, rewards, next_observations, done):
        """Qå€¤æ›´æ–°"""
        for i in range(self.n_agents):
            state_idx = self._discretize_state(observations[i])
            next_state_idx = self._discretize_state(next_observations[i])

            # Qå­¦ç¿’æ›´æ–°
            target = rewards[i]
            if not done:
                target += self.gamma * np.max(self.q_tables[i][next_state_idx])

            self.q_tables[i][state_idx, actions[i]] += self.lr * (
                target - self.q_tables[i][state_idx, actions[i]]
            )

    def _discretize_state(self, observation):
        """è¦³æ¸¬ã‚’é›¢æ•£åŒ–ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        # å®Ÿéš›ã¯çŠ¶æ…‹ã‚’ãƒãƒƒã‚·ãƒ¥åŒ–ã¾ãŸã¯é–¢æ•°è¿‘ä¼¼ã‚’ä½¿ç”¨
        return int(np.sum(np.abs(observation)) * 10) % 100


# MARLè¨“ç·´ä¾‹
def train_marl():
    """ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç’°å¢ƒã§ã®è¨“ç·´"""
    env = SimpleMultiAgentEnv(n_agents=3, grid_size=10)
    agent_controller = IndependentQLearning(
        n_agents=3,
        state_dim=env.observation_space.shape[0],
        n_actions=4
    )

    num_episodes = 100

    for episode in range(num_episodes):
        observations, _ = env.reset()
        done = False
        episode_reward = 0

        while not done:
            # è¡Œå‹•é¸æŠ
            actions = agent_controller.select_actions(observations)

            # ç’°å¢ƒã‚¹ãƒ†ãƒƒãƒ—
            next_observations, rewards, terminated, truncated, _ = env.step(actions)
            done = terminated or truncated

            # æ›´æ–°
            agent_controller.update(observations, actions, rewards, next_observations, done)

            episode_reward += sum(rewards)
            observations = next_observations

        if episode % 10 == 0:
            print(f"Episode {episode}, Total Reward: {episode_reward:.2f}")

    # æœ€çµ‚ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å¯è¦–åŒ–
    observations, _ = env.reset()
    env.render()

    return agent_controller


if __name__ == "__main__":
    print("Multi-Agent RL Training")
    print("=" * 50)
    controller = train_marl()
    print("Training completed!")
</code></pre>

<blockquote>
<p><strong>MARLã®å®Ÿè£…ãƒã‚¤ãƒ³ãƒˆ</strong>: Independent Q-Learningã¯æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªMARLã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã€å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç‹¬ç«‹ã—ã¦å­¦ç¿’ã—ã¾ã™ã€‚ã‚ˆã‚Šé«˜åº¦ãªæ‰‹æ³•ã«ã¯QMIXï¼ˆä¸­å¤®é›†æ¨©çš„è¨“ç·´ãƒ»åˆ†æ•£å®Ÿè¡Œï¼‰ã€MADDPGï¼ˆMulti-Agent DDPGï¼‰ãªã©ãŒã‚ã‚Šã¾ã™ã€‚å”èª¿ã‚¿ã‚¹ã‚¯ã§ã¯å ±é…¬å…±æœ‰ãŒæœ‰åŠ¹ã§ã€é€šä¿¡æ©Ÿæ§‹ã‚’å°å…¥ã™ã‚‹ã¨æ€§èƒ½ãŒå‘ä¸Šã—ã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>5.4 ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹å¼·åŒ–å­¦ç¿’ (Model-Based RL)</h2>

<h3>ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹RLã®æ¦‚è¦</h3>

<p><strong>ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹å¼·åŒ–å­¦ç¿’</strong>ã¯ã€ç’°å¢ƒã®ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆé·ç§»é–¢æ•°ã¨å ±é…¬é–¢æ•°ï¼‰ã‚’å­¦ç¿’ã—ã€ãã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦æ–¹ç­–ã‚’æœ€é©åŒ–ã—ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ãƒ•ãƒªãƒ¼æ‰‹æ³•ã¨æ¯”ã¹ã¦ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ãŒé«˜ã„ã®ãŒç‰¹å¾´ã§ã™ã€‚</p>

<div class="mermaid">
graph LR
    ENV[å®Ÿç’°å¢ƒ] --> |çµŒé¨“ s,a,r,s'| MD[ãƒ¢ãƒ‡ãƒ«å­¦ç¿’<br/>PÌ‚s'|s,a, RÌ‚s,a]
    MD --> |å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«| PLAN[ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°<br/>ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³]
    PLAN --> |æ–¹ç­–æ”¹å–„| POL[æ–¹ç­– Ï€]
    POL --> |è¡Œå‹• a| ENV

    PLAN -.-> |æƒ³åƒä¸Šã®çµŒé¨“| MB[ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹æ›´æ–°]
    ENV -.-> |å®ŸçµŒé¨“| MF[ãƒ¢ãƒ‡ãƒ«ãƒ•ãƒªãƒ¼æ›´æ–°]

    MB --> POL
    MF --> POL

    style ENV fill:#e3f2fd
    style MD fill:#fff9c4
    style PLAN fill:#c8e6c9
    style POL fill:#ffccbc
</div>

<h4>ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ vs ãƒ¢ãƒ‡ãƒ«ãƒ•ãƒªãƒ¼</h4>

<table>
<thead>
<tr>
<th>å´é¢</th>
<th>ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹</th>
<th>ãƒ¢ãƒ‡ãƒ«ãƒ•ãƒªãƒ¼</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡</strong></td>
<td>é«˜ã„ï¼ˆãƒ¢ãƒ‡ãƒ«ã§è£œå®Œï¼‰</td>
<td>ä½ã„ï¼ˆå¤šãã®çµŒé¨“ãŒå¿…è¦ï¼‰</td>
</tr>
<tr>
<td><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆ</strong></td>
<td>é«˜ã„ï¼ˆãƒ¢ãƒ‡ãƒ«å­¦ç¿’+ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ï¼‰</td>
<td>ä½ã„ï¼ˆç›´æ¥æ–¹ç­–å­¦ç¿’ï¼‰</td>
</tr>
<tr>
<td><strong>é©ç”¨é›£æ˜“åº¦</strong></td>
<td>é›£ã—ã„ï¼ˆãƒ¢ãƒ‡ãƒ«èª¤å·®ã®å½±éŸ¿ï¼‰</td>
<td>å®¹æ˜“ï¼ˆç›´æ¥å­¦ç¿’ï¼‰</td>
</tr>
<tr>
<td><strong>è§£é‡ˆæ€§</strong></td>
<td>é«˜ã„ï¼ˆãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬å¯èƒ½ï¼‰</td>
<td>ä½ã„ï¼ˆãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ï¼‰</td>
</tr>
</tbody>
</table>

<h4>ä¸»è¦ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</h4>

<ul>
<li><strong>Dyna-Q</strong>: å®ŸçµŒé¨“ã¨ãƒ¢ãƒ‡ãƒ«çµŒé¨“ã‚’çµ„ã¿åˆã‚ã›</li>
<li><strong>PETS</strong>: ç¢ºç‡çš„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã§ä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®</li>
<li><strong>MBPO</strong>: ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹æ–¹ç­–æœ€é©åŒ–</li>
<li><strong>MuZero</strong>: ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨MCTSã®çµ±åˆ</li>
</ul>

<p>ç’°å¢ƒãƒ¢ãƒ‡ãƒ«ã¯ä»¥ä¸‹ã‚’å­¦ç¿’ã—ã¾ã™ï¼š</p>

$$
\hat{P}(s' | s, a) \approx P(s' | s, a)
$$

$$
\hat{R}(s, a) \approx R(s, a)
$$

<p>å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã€å¤šãã®ä»®æƒ³çµŒé¨“ã‚’ç”Ÿæˆã—ã¾ã™ã€‚</p>

<blockquote>
<p><strong>ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹RLã®ãƒã‚¤ãƒ³ãƒˆ</strong>: ãƒ¢ãƒ‡ãƒ«èª¤å·®ãŒç´¯ç©ã™ã‚‹ã¨æ€§èƒ½ãŒæ‚ªåŒ–ã™ã‚‹ãŸã‚ã€ä¸ç¢ºå®Ÿæ€§æ¨å®šã¨ãƒ¢ãƒ‡ãƒ«ã®é©åˆ‡ãªä½¿ç”¨ãŒé‡è¦ã§ã™ã€‚å®Ÿç’°å¢ƒã¨ãƒ¢ãƒ‡ãƒ«ç’°å¢ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ©ãƒ³ã‚¹è‰¯ãä½¿ã†ã“ã¨ã§ã€ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ã¨æ€§èƒ½ã‚’ä¸¡ç«‹ã§ãã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>5.5 å®Ÿä¸–ç•Œå¿œç”¨</h2>

<h3>5.5.1 ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ (Robotics)</h3>

<p>å¼·åŒ–å­¦ç¿’ã¯ãƒ­ãƒœãƒƒãƒˆã®åˆ¶å¾¡ã€æ“ä½œã€ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã«åºƒãå¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚</p>

<h4>ä¸»è¦å¿œç”¨åˆ†é‡</h4>

<ul>
<li><strong>ãƒ­ãƒœãƒƒãƒˆã‚¢ãƒ¼ãƒ åˆ¶å¾¡</strong>: ç‰©ä½“æŠŠæŒã€çµ„ã¿ç«‹ã¦ä½œæ¥­</li>
<li><strong>æ­©è¡Œãƒ­ãƒœãƒƒãƒˆ</strong>: äºŒè¶³æ­©è¡Œã€å››è¶³æ­©è¡Œã®å­¦ç¿’</li>
<li><strong>è‡ªå¾‹ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³</strong>: éšœå®³ç‰©å›é¿ã€çµŒè·¯è¨ˆç”»</li>
<li><strong>Sim-to-Realè»¢ç§»</strong>: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§å­¦ç¿’â†’å®Ÿæ©Ÿã¸è»¢ç§»</li>
</ul>

<h3>5.5.2 ã‚²ãƒ¼ãƒ AI (Game AI)</h3>

<p>å¼·åŒ–å­¦ç¿’ã¯è¤‡é›‘ãªã‚²ãƒ¼ãƒ ã§äººé–“ãƒ¬ãƒ™ãƒ«ä»¥ä¸Šã®æ€§èƒ½ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚</p>

<h4>ä»£è¡¨çš„ãªæˆåŠŸä¾‹</h4>

<table>
<thead>
<tr>
<th>ã‚·ã‚¹ãƒ†ãƒ </th>
<th>ã‚²ãƒ¼ãƒ </th>
<th>æ‰‹æ³•</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>AlphaGo</strong></td>
<td>å›²ç¢</td>
<td>MCTS + Deep RL</td>
</tr>
<tr>
<td><strong>AlphaStar</strong></td>
<td>StarCraft II</td>
<td>Multi-agent RL</td>
</tr>
<tr>
<td><strong>OpenAI Five</strong></td>
<td>Dota 2</td>
<td>PPO + å¤§è¦æ¨¡åˆ†æ•£å­¦ç¿’</td>
</tr>
<tr>
<td><strong>MuZero</strong></td>
<td>ãƒã‚§ã‚¹ã€å°†æ£‹ã€Atari</td>
<td>Model-based RL + MCTS</td>
</tr>
</tbody>
</table>

<h3>5.5.3 é‡‘èãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° (Trading)</h3>

<p>å¼·åŒ–å­¦ç¿’ã¯è‡ªå‹•ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæœ€é©åŒ–ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import deque

class TradingEnvironment:
    """
    æ ªå¼ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç’°å¢ƒ

    Features:
    - éå»ã®ä¾¡æ ¼å±¥æ­´ã‹ã‚‰è¡Œå‹•æ±ºå®š
    - å–å¼•ã‚³ã‚¹ãƒˆã‚’è€ƒæ…®
    - ä¿æœ‰ãƒã‚¸ã‚·ãƒ§ãƒ³ç®¡ç†
    """

    def __init__(self, price_data, initial_balance=10000,
                 transaction_cost=0.001, window_size=20):
        """
        Args:
            price_data: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ (DataFrame)
            initial_balance: åˆæœŸè³‡é‡‘
            transaction_cost: å–å¼•ã‚³ã‚¹ãƒˆï¼ˆç‰‡é“ï¼‰
            window_size: è¦³æ¸¬ã™ã‚‹éå»ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
        """
        self.price_data = price_data
        self.initial_balance = initial_balance
        self.transaction_cost = transaction_cost
        self.window_size = window_size

        self.reset()

    def reset(self):
        """ç’°å¢ƒãƒªã‚»ãƒƒãƒˆ"""
        self.current_step = self.window_size
        self.balance = self.initial_balance
        self.shares_held = 0
        self.net_worth = self.initial_balance
        self.max_net_worth = self.initial_balance

        return self._get_observation()

    def _get_observation(self):
        """
        è¦³æ¸¬å–å¾—

        Returns:
            observation: [ä¾¡æ ¼å±¥æ­´, ä¿æœ‰æ ªæ•°, æ®‹é«˜] ã®æ­£è¦åŒ–ç‰ˆ
        """
        # éå»window_sizeã‚¹ãƒ†ãƒƒãƒ—ã®ä¾¡æ ¼å¤‰åŒ–ç‡
        window_data = self.price_data.iloc[
            self.current_step - self.window_size:self.current_step
        ]['Close'].pct_change().fillna(0).values

        # ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªçŠ¶æ…‹
        portfolio_state = np.array([
            self.shares_held / 100,  # æ­£è¦åŒ–
            self.balance / self.initial_balance  # æ­£è¦åŒ–
        ])

        observation = np.concatenate([window_data, portfolio_state])
        return observation

    def step(self, action):
        """
        ç’°å¢ƒã‚¹ãƒ†ãƒƒãƒ—

        Args:
            action: 0=Hold, 1=Buy, 2=Sell

        Returns:
            observation: æ¬¡çŠ¶æ…‹
            reward: å ±é…¬
            done: çµ‚äº†ãƒ•ãƒ©ã‚°
            info: è¿½åŠ æƒ…å ±
        """
        current_price = self.price_data.iloc[self.current_step]['Close']

        # è¡Œå‹•å®Ÿè¡Œ
        if action == 1:  # Buy
            shares_to_buy = self.balance // current_price
            cost = shares_to_buy * current_price * (1 + self.transaction_cost)

            if cost <= self.balance:
                self.shares_held += shares_to_buy
                self.balance -= cost

        elif action == 2:  # Sell
            if self.shares_held > 0:
                proceeds = self.shares_held * current_price * (1 - self.transaction_cost)
                self.balance += proceeds
                self.shares_held = 0

        # ã‚¹ãƒ†ãƒƒãƒ—é€²è¡Œ
        self.current_step += 1

        # ç´”è³‡ç”£è¨ˆç®—
        self.net_worth = self.balance + self.shares_held * current_price
        self.max_net_worth = max(self.max_net_worth, self.net_worth)

        # å ±é…¬: ç´”è³‡ç”£ã®å¤‰åŒ–ç‡
        reward = (self.net_worth - self.initial_balance) / self.initial_balance

        # çµ‚äº†åˆ¤å®š
        done = self.current_step >= len(self.price_data) - 1

        observation = self._get_observation()
        info = {
            'net_worth': self.net_worth,
            'shares_held': self.shares_held,
            'balance': self.balance
        }

        return observation, reward, done, info


class DQNTrader:
    """
    DQNãƒ™ãƒ¼ã‚¹ã®ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
    """

    def __init__(self, state_dim, n_actions=3, lr=0.001, gamma=0.95):
        import torch
        import torch.nn as nn

        self.state_dim = state_dim
        self.n_actions = n_actions
        self.gamma = gamma

        # Q-Network
        self.q_network = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, n_actions)
        )

        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=lr)
        self.memory = deque(maxlen=2000)
        self.epsilon = 1.0
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01

    def select_action(self, state, training=True):
        """Îµ-greedyè¡Œå‹•é¸æŠ"""
        import torch

        if training and np.random.rand() < self.epsilon:
            return np.random.randint(self.n_actions)

        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            q_values = self.q_network(state_tensor)
            return q_values.argmax().item()

    def train(self, batch_size=32):
        """DQNæ›´æ–°"""
        import torch
        import torch.nn.functional as F

        if len(self.memory) < batch_size:
            return

        # ãƒŸãƒ‹ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        batch = np.array(self.memory, dtype=object)
        indices = np.random.choice(len(batch), batch_size, replace=False)
        samples = batch[indices]

        states = torch.FloatTensor(np.vstack([s[0] for s in samples]))
        actions = torch.LongTensor([s[1] for s in samples])
        rewards = torch.FloatTensor([s[2] for s in samples])
        next_states = torch.FloatTensor(np.vstack([s[3] for s in samples]))
        dones = torch.FloatTensor([s[4] for s in samples])

        # Qå€¤è¨ˆç®—
        current_q = self.q_network(states).gather(1, actions.unsqueeze(1))

        with torch.no_grad():
            max_next_q = self.q_network(next_states).max(1)[0]
            target_q = rewards + (1 - dones) * self.gamma * max_next_q

        # æå¤±è¨ˆç®—ã¨æ›´æ–°
        loss = F.mse_loss(current_q.squeeze(), target_q)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # Îµæ¸›è¡°
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)


# ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒãƒˆè¨“ç·´ä¾‹
def train_trading_bot():
    """
    æ ªå¼ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒãƒˆã®è¨“ç·´
    ï¼ˆãƒ‡ãƒ¢ç”¨: ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰
    """
    # ãƒ‡ãƒ¢ç”¨ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    dates = pd.date_range('2020-01-01', periods=500)
    prices = 100 * np.exp(np.cumsum(np.random.randn(500) * 0.02))
    price_data = pd.DataFrame({'Close': prices}, index=dates)

    # ç’°å¢ƒã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–
    env = TradingEnvironment(price_data, window_size=20)
    obs = env.reset()
    agent = DQNTrader(state_dim=len(obs))

    num_episodes = 50

    for episode in range(num_episodes):
        state = env.reset()
        total_reward = 0
        done = False

        while not done:
            # è¡Œå‹•é¸æŠ
            action = agent.select_action(state, training=True)

            # ç’°å¢ƒã‚¹ãƒ†ãƒƒãƒ—
            next_state, reward, done, info = env.step(action)

            # çµŒé¨“ä¿å­˜
            agent.memory.append((state, action, reward, next_state, done))

            # è¨“ç·´
            agent.train(batch_size=32)

            total_reward += reward
            state = next_state

        if episode % 10 == 0:
            print(f"Episode {episode}, Total Reward: {total_reward:.4f}, "
                  f"Final Net Worth: ${info['net_worth']:.2f}, "
                  f"Epsilon: {agent.epsilon:.3f}")

    # æœ€çµ‚è©•ä¾¡
    state = env.reset()
    done = False
    actions_taken = []
    net_worths = []

    while not done:
        action = agent.select_action(state, training=False)
        actions_taken.append(action)
        state, reward, done, info = env.step(action)
        net_worths.append(info['net_worth'])

    # å¯è¦–åŒ–
    plt.figure(figsize=(14, 6))

    plt.subplot(1, 2, 1)
    plt.plot(price_data.index[-len(net_worths):],
             price_data['Close'].iloc[-len(net_worths):],
             label='Stock Price', alpha=0.7)
    plt.title('Stock Price Over Time')
    plt.xlabel('Date')
    plt.ylabel('Price ($)')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 2, 2)
    plt.plot(net_worths, label='Portfolio Net Worth', color='green')
    plt.axhline(y=env.initial_balance, color='r', linestyle='--',
                label='Initial Balance', alpha=0.7)
    plt.title('Portfolio Performance')
    plt.xlabel('Time Step')
    plt.ylabel('Net Worth ($)')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('trading_bot_performance.png', dpi=150, bbox_inches='tight')
    print("Performance chart saved as 'trading_bot_performance.png'")

    final_return = (net_worths[-1] - env.initial_balance) / env.initial_balance * 100
    print(f"\nFinal Return: {final_return:.2f}%")

    return agent, env


if __name__ == "__main__":
    print("RL Trading Bot Training")
    print("=" * 50)
    agent, env = train_trading_bot()
    print("Training completed!")
</code></pre>

<blockquote>
<p><strong>ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¸ã®å¿œç”¨ãƒã‚¤ãƒ³ãƒˆ</strong>: å–å¼•ã‚³ã‚¹ãƒˆã€ã‚¹ãƒªãƒƒãƒšãƒ¼ã‚¸ã€å¸‚å ´ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚’è€ƒæ…®ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚éå»ãƒ‡ãƒ¼ã‚¿ã§ã®éå­¦ç¿’ã‚’é¿ã‘ã‚‹ãŸã‚ã€è¤‡æ•°ã®æ™‚æœŸã§ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚’è¡Œã„ã¾ã™ã€‚å®Ÿéš›ã®é‹ç”¨ã§ã¯ã€ãƒªã‚¹ã‚¯ç®¡ç†ï¼ˆãƒã‚¸ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºåˆ¶é™ã€ã‚¹ãƒˆãƒƒãƒ—ãƒ­ã‚¹ï¼‰ã‚’çµ„ã¿è¾¼ã‚€å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>5.6 Stable-Baselines3ã«ã‚ˆã‚‹å®Ÿè·µ</h2>

<h3>Stable-Baselines3ã®æ¦‚è¦</h3>

<p><strong>Stable-Baselines3 (SB3)</strong>ã¯ã€ä¿¡é ¼æ€§ã®é«˜ã„RLå®Ÿè£…ã‚’æä¾›ã™ã‚‹Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚æœ€æ–°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè£…ãŒå……å®Ÿã—ã¦ãŠã‚Šã€å®Ÿè·µçš„ãªRLãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«æœ€é©ã§ã™ã€‚</p>

<h4>SB3ã®ä¸»è¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h4>

<table>
<thead>
<tr>
<th>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </th>
<th>ã‚¿ã‚¤ãƒ—</th>
<th>é©ç”¨å ´é¢</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>PPO</strong></td>
<td>On-policy, Actor-Critic</td>
<td>æ±ç”¨æ€§ãŒé«˜ã„ã€å®‰å®š</td>
</tr>
<tr>
<td><strong>A2C</strong></td>
<td>On-policy, Actor-Critic</td>
<td>é«˜é€Ÿå­¦ç¿’ã€ä¸¦åˆ—åŒ–</td>
</tr>
<tr>
<td><strong>SAC</strong></td>
<td>Off-policy, Max-Entropy</td>
<td>é€£ç¶šè¡Œå‹•ã€ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡</td>
</tr>
<tr>
<td><strong>TD3</strong></td>
<td>Off-policy, DDPGæ”¹è‰¯</td>
<td>é€£ç¶šè¡Œå‹•ã€å®‰å®šæ€§</td>
</tr>
<tr>
<td><strong>DQN</strong></td>
<td>Off-policy, Value-based</td>
<td>é›¢æ•£è¡Œå‹•</td>
</tr>
</tbody>
</table>

<h3>Stable-Baselines3ã®å®Ÿè·µä¾‹</h3>

<pre><code class="language-python">"""
Stable-Baselines3ã‚’ä½¿ã£ãŸå®Ÿè·µçš„ãªRLè¨“ç·´
"""

# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
# !pip install stable-baselines3[extra]

import gymnasium as gym
from stable_baselines3 import PPO, SAC, DQN
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from stable_baselines3.common.monitor import Monitor
import numpy as np
import matplotlib.pyplot as plt

# === Example 1: PPO for CartPole ===
def train_ppo_cartpole():
    """
    PPOã§CartPoleç’°å¢ƒã‚’è¨“ç·´

    Features:
    - Vectorized environment for parallel training
    - Evaluation callback for monitoring
    - Model checkpointing
    """
    print("Training PPO on CartPole-v1")
    print("=" * 50)

    # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ç’°å¢ƒï¼ˆä¸¦åˆ—è¨“ç·´ï¼‰
    env = make_vec_env('CartPole-v1', n_envs=4)

    # è©•ä¾¡ç”¨ç’°å¢ƒ
    eval_env = gym.make('CartPole-v1')
    eval_env = Monitor(eval_env)

    # PPOãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    model = PPO(
        'MlpPolicy',           # Multi-Layer Perceptron policy
        env,
        learning_rate=3e-4,
        n_steps=2048,          # ã‚¹ãƒ†ãƒƒãƒ—æ•°/æ›´æ–°
        batch_size=64,
        n_epochs=10,           # æ›´æ–°ã‚¨ãƒãƒƒã‚¯æ•°
        gamma=0.99,
        gae_lambda=0.95,       # GAE parameter
        clip_range=0.2,        # PPO clipping
        verbose=1,
        tensorboard_log="./ppo_cartpole_tensorboard/"
    )

    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path='./logs/best_model',
        log_path='./logs/',
        eval_freq=10000,
        deterministic=True,
        render=False
    )

    checkpoint_callback = CheckpointCallback(
        save_freq=10000,
        save_path='./logs/checkpoints/',
        name_prefix='ppo_cartpole'
    )

    # è¨“ç·´å®Ÿè¡Œ
    model.learn(
        total_timesteps=100000,
        callback=[eval_callback, checkpoint_callback]
    )

    # è©•ä¾¡
    mean_reward, std_reward = evaluate_policy(
        model, eval_env, n_eval_episodes=10
    )
    print(f"\nEvaluation: Mean Reward = {mean_reward:.2f} +/- {std_reward:.2f}")

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model.save("ppo_cartpole_final")

    return model


# === Example 2: SAC for Continuous Control ===
def train_sac_pendulum():
    """
    SACã§Pendulumç’°å¢ƒã‚’è¨“ç·´ï¼ˆé€£ç¶šè¡Œå‹•ç©ºé–“ï¼‰

    Features:
    - Maximum entropy RL
    - Off-policy learning
    - Automatic temperature tuning
    """
    print("\nTraining SAC on Pendulum-v1")
    print("=" * 50)

    # ç’°å¢ƒä½œæˆ
    env = gym.make('Pendulum-v1')

    # SACãƒ¢ãƒ‡ãƒ«
    model = SAC(
        'MlpPolicy',
        env,
        learning_rate=3e-4,
        buffer_size=100000,
        learning_starts=1000,
        batch_size=256,
        tau=0.005,             # Soft update coefficient
        gamma=0.99,
        train_freq=1,
        gradient_steps=1,
        ent_coef='auto',       # Automatic entropy tuning
        verbose=1,
        tensorboard_log="./sac_pendulum_tensorboard/"
    )

    # è¨“ç·´
    model.learn(total_timesteps=50000)

    # è©•ä¾¡
    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)
    print(f"Evaluation: Mean Reward = {mean_reward:.2f} +/- {std_reward:.2f}")

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model.save("sac_pendulum_final")

    return model


# === Example 3: Custom Environment with SB3 ===
class CustomGridWorld(gym.Env):
    """
    ã‚«ã‚¹ã‚¿ãƒ ã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ç’°å¢ƒ
    SB3äº’æ›ã®Gymç’°å¢ƒ
    """

    def __init__(self, grid_size=5):
        super(CustomGridWorld, self).__init__()

        self.grid_size = grid_size
        self.agent_pos = [0, 0]
        self.goal_pos = [grid_size - 1, grid_size - 1]

        # è¡Œå‹•ç©ºé–“: ä¸Šä¸‹å·¦å³
        self.action_space = gym.spaces.Discrete(4)

        # è¦³æ¸¬ç©ºé–“: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½ç½®ï¼ˆæ­£è¦åŒ–ï¼‰
        self.observation_space = gym.spaces.Box(
            low=0, high=1, shape=(2,), dtype=np.float32
        )

    def reset(self, seed=None):
        super().reset(seed=seed)
        self.agent_pos = [0, 0]
        return self._get_obs(), {}

    def _get_obs(self):
        return np.array(self.agent_pos, dtype=np.float32) / self.grid_size

    def step(self, action):
        # è¡Œå‹•å®Ÿè¡Œ
        if action == 0 and self.agent_pos[1] < self.grid_size - 1:  # Up
            self.agent_pos[1] += 1
        elif action == 1 and self.agent_pos[1] > 0:  # Down
            self.agent_pos[1] -= 1
        elif action == 2 and self.agent_pos[0] < self.grid_size - 1:  # Right
            self.agent_pos[0] += 1
        elif action == 3 and self.agent_pos[0] > 0:  # Left
            self.agent_pos[0] -= 1

        # å ±é…¬è¨ˆç®—
        if self.agent_pos == self.goal_pos:
            reward = 1.0
            done = True
        else:
            reward = -0.01
            done = False

        return self._get_obs(), reward, done, False, {}


def train_custom_env():
    """ã‚«ã‚¹ã‚¿ãƒ ç’°å¢ƒã§DQNã‚’è¨“ç·´"""
    print("\nTraining DQN on Custom GridWorld")
    print("=" * 50)

    # ã‚«ã‚¹ã‚¿ãƒ ç’°å¢ƒ
    env = CustomGridWorld(grid_size=5)

    # DQNãƒ¢ãƒ‡ãƒ«
    model = DQN(
        'MlpPolicy',
        env,
        learning_rate=1e-3,
        buffer_size=10000,
        learning_starts=1000,
        batch_size=32,
        gamma=0.99,
        exploration_fraction=0.1,
        exploration_final_eps=0.02,
        verbose=1
    )

    # è¨“ç·´
    model.learn(total_timesteps=50000)

    # è©•ä¾¡
    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)
    print(f"Evaluation: Mean Reward = {mean_reward:.2f} +/- {std_reward:.2f}")

    return model


# === Example 4: Loading and Using Trained Model ===
def use_trained_model():
    """è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨ä½¿ç”¨"""
    print("\nUsing Trained Model")
    print("=" * 50)

    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    model = PPO.load("ppo_cartpole_final")

    # ç’°å¢ƒã§å®Ÿè¡Œ
    env = gym.make('CartPole-v1', render_mode='rgb_array')

    obs, _ = env.reset()
    total_reward = 0

    for _ in range(500):
        # æ±ºå®šçš„è¡Œå‹•é¸æŠ
        action, _states = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, info = env.step(action)
        total_reward += reward

        if terminated or truncated:
            break

    print(f"Episode reward: {total_reward}")
    env.close()


# === Example 5: Hyperparameter Tuning with Optuna ===
def hyperparameter_tuning():
    """
    Optunaã‚’ä½¿ã£ãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
    ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³: optunaã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¿…è¦ï¼‰
    """
    try:
        from stable_baselines3.common.env_util import make_vec_env
        import optuna
        from optuna.pruners import MedianPruner
        from optuna.samplers import TPESampler

        def objective(trial):
            """Optunaç›®çš„é–¢æ•°"""
            # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ææ¡ˆ
            lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)
            gamma = trial.suggest_uniform('gamma', 0.9, 0.9999)
            clip_range = trial.suggest_uniform('clip_range', 0.1, 0.4)

            # ç’°å¢ƒã¨ãƒ¢ãƒ‡ãƒ«
            env = make_vec_env('CartPole-v1', n_envs=4)
            model = PPO(
                'MlpPolicy', env,
                learning_rate=lr,
                gamma=gamma,
                clip_range=clip_range,
                verbose=0
            )

            # è¨“ç·´
            model.learn(total_timesteps=20000)

            # è©•ä¾¡
            eval_env = gym.make('CartPole-v1')
            mean_reward, _ = evaluate_policy(model, eval_env, n_eval_episodes=5)

            return mean_reward

        # Optuna study
        study = optuna.create_study(
            direction='maximize',
            sampler=TPESampler(),
            pruner=MedianPruner()
        )

        study.optimize(objective, n_trials=20, timeout=600)

        print("\nBest hyperparameters:")
        print(study.best_params)
        print(f"Best value: {study.best_value:.2f}")

    except ImportError:
        print("Optuna not installed. Skipping hyperparameter tuning.")
        print("Install with: pip install optuna")


# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
if __name__ == "__main__":
    print("Stable-Baselines3 Practical Examples")
    print("=" * 50)

    # Example 1: PPO
    ppo_model = train_ppo_cartpole()

    # Example 2: SAC
    sac_model = train_sac_pendulum()

    # Example 3: Custom Environment
    custom_model = train_custom_env()

    # Example 4: Using trained model
    use_trained_model()

    # Example 5: Hyperparameter tuning (optional)
    # hyperparameter_tuning()

    print("\n" + "=" * 50)
    print("All examples completed!")
    print("Tensorboard logs saved. View with:")
    print("  tensorboard --logdir ./ppo_cartpole_tensorboard/")
</code></pre>

<blockquote>
<p><strong>SB3ã®å®Ÿè·µãƒã‚¤ãƒ³ãƒˆ</strong>: ãƒ™ã‚¯ãƒˆãƒ«åŒ–ç’°å¢ƒã«ã‚ˆã‚Šè¨“ç·´ãŒé«˜é€ŸåŒ–ã•ã‚Œã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã«ã‚ˆã‚Šè¨“ç·´ä¸­ã®ç›£è¦–ãƒ»è©•ä¾¡ãŒå®¹æ˜“ã«ãªã‚Šã¾ã™ã€‚TensorBoardãƒ­ã‚°ã§å­¦ç¿’æ›²ç·šã‚’å¯è¦–åŒ–ã§ãã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯OptunaãŒæœ‰åŠ¹ã§ã™ã€‚ã‚«ã‚¹ã‚¿ãƒ ç’°å¢ƒã¯Gym APIã«æº–æ‹ ã™ã‚Œã°ç°¡å˜ã«çµ±åˆã§ãã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>5.7 å®Ÿä¸–ç•Œé©ç”¨ã®èª²é¡Œã¨è§£æ±ºç­–</h2>

<h3>ä¸»è¦ãªèª²é¡Œ</h3>

<table>
<thead>
<tr>
<th>èª²é¡Œ</th>
<th>èª¬æ˜</th>
<th>è§£æ±ºç­–</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡</strong></td>
<td>å®Ÿç’°å¢ƒã§ã®å­¦ç¿’ã¯æ™‚é–“ãƒ»ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹</td>
<td>ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®äº‹å‰å­¦ç¿’ã€ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹RLã€è»¢ç§»å­¦ç¿’</td>
</tr>
<tr>
<td><strong>å®‰å…¨æ€§</strong></td>
<td>å­¦ç¿’ä¸­ã®å¤±æ•—ãŒå±é™º</td>
<td>Safe RLã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®æ¤œè¨¼ã€äººé–“ã®ç›£è¦–</td>
</tr>
<tr>
<td><strong>Sim-to-Real Gap</strong></td>
<td>ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨å®Ÿç’°å¢ƒã®å·®</td>
<td>Domain Randomizationã€ç¾å®Ÿæ€§ã®é«˜ã„ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿</td>
</tr>
<tr>
<td><strong>éƒ¨åˆ†è¦³æ¸¬</strong></td>
<td>å®Œå…¨ãªçŠ¶æ…‹ãŒè¦³æ¸¬ã§ããªã„</td>
<td>LSTM/Transformerã€ä¿¡å¿µçŠ¶æ…‹ã®ä½¿ç”¨</td>
</tr>
<tr>
<td><strong>å ±é…¬è¨­è¨ˆ</strong></td>
<td>é©åˆ‡ãªå ±é…¬é–¢æ•°ã®è¨­è¨ˆãŒé›£ã—ã„</td>
<td>é€†å¼·åŒ–å­¦ç¿’ã€æ¨¡å€£å­¦ç¿’ã€å ±é…¬ã‚·ã‚§ãƒ¼ãƒ”ãƒ³ã‚°</td>
</tr>
<tr>
<td><strong>æ±åŒ–æ€§èƒ½</strong></td>
<td>è¨“ç·´ç’°å¢ƒå¤–ã§ã®æ€§èƒ½ä½ä¸‹</td>
<td>å¤šæ§˜ãªè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã€Meta-RLã€ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œ</td>
</tr>
</tbody>
</table>

<h3>ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>

<ol>
<li><strong>æ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</strong>: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ â†’ Sim-to-Real â†’ å®Ÿç’°å¢ƒ</li>
<li><strong>ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼</strong>: è¤‡æ•°ã®è©•ä¾¡æŒ‡æ¨™ã€ç•°ãªã‚‹ç’°å¢ƒè¨­å®šã§ãƒ†ã‚¹ãƒˆ</li>
<li><strong>äººé–“ã®çŸ¥è­˜æ´»ç”¨</strong>: æ¨¡å€£å­¦ç¿’ã€äº‹å‰å­¦ç¿’ã€å ±é…¬ã‚·ã‚§ãƒ¼ãƒ”ãƒ³ã‚°</li>
<li><strong>å®‰å…¨æ€§ã®ç¢ºä¿</strong>: åˆ¶ç´„ä»˜ãRLã€ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•æ©Ÿæ§‹</li>
<li><strong>ç¶™ç¶šçš„å­¦ç¿’</strong>: ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã€é©å¿œçš„æ–¹ç­–</li>
</ol>

<hr>

<h2>ã¾ã¨ã‚</h2>

<p>ã“ã®ç« ã§ã¯ã€ä»¥ä¸‹ã®é«˜åº¦ãªRLæ‰‹æ³•ã¨å¿œç”¨ã‚’å­¦ã³ã¾ã—ãŸï¼š</p>

<ul>
<li>âœ… <strong>A3C</strong>: éåŒæœŸä¸¦åˆ—å­¦ç¿’ã«ã‚ˆã‚‹é«˜é€ŸåŒ–</li>
<li>âœ… <strong>SAC</strong>: æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹å®‰å®šã—ãŸé€£ç¶šåˆ¶å¾¡</li>
<li>âœ… <strong>ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆRL</strong>: è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å”èª¿ãƒ»ç«¶äº‰</li>
<li>âœ… <strong>ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹RL</strong>: ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ã®å‘ä¸Š</li>
<li>âœ… <strong>å®Ÿä¸–ç•Œå¿œç”¨</strong>: ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã€ã‚²ãƒ¼ãƒ AIã€é‡‘èãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°</li>
<li>âœ… <strong>Stable-Baselines3</strong>: å®Ÿè·µçš„ãªRLé–‹ç™ºãƒ„ãƒ¼ãƒ«</li>
<li>âœ… <strong>å®Ÿè£…èª²é¡Œ</strong>: å®‰å…¨æ€§ã€æ±åŒ–æ€§èƒ½ã€Sim-to-Realè»¢ç§»</li>
</ul>

<h3>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>

<ol>
<li><strong>å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ</strong>: Stable-Baselines3ã§ç‹¬è‡ªã®ç’°å¢ƒã‚’ä½œæˆ</li>
<li><strong>è«–æ–‡èª­è§£</strong>: æœ€æ–°ã®RLè«–æ–‡ã‚’èª­ã¿ã€å®Ÿè£…ã—ã¦ã¿ã‚‹</li>
<li><strong>ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³</strong>: Kaggle RLã‚³ãƒ³ãƒšã€OpenAI Gym Leaderboard</li>
<li><strong>å¿œç”¨åˆ†é‡æ¢ç´¢</strong>: è‡ªå‹•é‹è»¢ã€ãƒ˜ãƒ«ã‚¹ã‚±ã‚¢ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼ç®¡ç†ãªã©</li>
</ol>

<h3>å‚è€ƒãƒªã‚½ãƒ¼ã‚¹</h3>

<ul>
<li><a href="https://stable-baselines3.readthedocs.io/">Stable-Baselines3 Documentation</a></li>
<li><a href="https://spinningup.openai.com/">OpenAI Spinning Up in Deep RL</a></li>
<li><a href="http://www.incompleteideas.net/book/the-book-2nd.html">Sutton & Barto: Reinforcement Learning Book</a></li>
<li><a href="https://arxiv.org/abs/1602.01783">A3C Paper (Mnih et al., 2016)</a></li>
<li><a href="https://arxiv.org/abs/1801.01290">SAC Paper (Haarnoja et al., 2018)</a></li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<details>
<summary><strong>æ¼”ç¿’5.1: A3Cã®ä¸¦åˆ—åŒ–å®Ÿè£…</strong></summary>
<p><strong>å•é¡Œ</strong>: Pythonã®<code>multiprocessing</code>ã‚’ä½¿ã£ã¦ã€å®Œå…¨ãªä¸¦åˆ—A3Cã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>ãƒ’ãƒ³ãƒˆ</strong>:</p>
<ul>
<li><code>torch.multiprocessing</code>ã‚’ä½¿ç”¨</li>
<li>ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯<code>share_memory()</code>ã§å…±æœ‰</li>
<li>å„ãƒ¯ãƒ¼ã‚«ãƒ¼ã¯ç‹¬ç«‹ã—ãŸãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œ</li>
<li>ãƒ­ãƒƒã‚¯ã‚’ä½¿ã£ãŸåŒæœŸã«æ³¨æ„</li>
</ul>
</details>

<details>
<summary><strong>æ¼”ç¿’5.2: SACã®æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†æ</strong></summary>
<p><strong>å•é¡Œ</strong>: SACã®æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\alpha$ã‚’å›ºå®šå€¤ã¨è‡ªå‹•èª¿æ•´ã§æ¯”è¼ƒã—ã€å­¦ç¿’æ›²ç·šã¨æœ€çµ‚æ€§èƒ½ã®é•ã„ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>ãƒ’ãƒ³ãƒˆ</strong>:</p>
<ul>
<li>$\alpha \in \{0.05, 0.1, 0.2, \text{auto}\}$ã§å®Ÿé¨“</li>
<li>ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®æ¨ç§»ã‚’è¨˜éŒ²</li>
<li>æ¢ç´¢ã®å¤šæ§˜æ€§ã‚’åˆ†æ</li>
</ul>
</details>

<details>
<summary><strong>æ¼”ç¿’5.3: ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå”èª¿ã‚¿ã‚¹ã‚¯</strong></summary>
<p><strong>å•é¡Œ</strong>: 3ã¤ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå”èª¿ã—ã¦ç›®æ¨™ã‚’é‹ã¶ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚1ã¤ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã ã‘ã§ã¯é‹ã¹ãªã„é‡ã„ç‰©ä½“ã‚’ã€è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§å”åŠ›ã—ã¦ç›®æ¨™åœ°ç‚¹ã¾ã§é‹ã³ã¾ã™ã€‚</p>
<p><strong>ãƒ’ãƒ³ãƒˆ</strong>:</p>
<ul>
<li>è¿‘æ¥ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°ã«å¿œã˜ã¦é‹æ¬å¯èƒ½ã‹åˆ¤å®š</li>
<li>å…±æœ‰å ±é…¬ã§å”èª¿ã‚’ä¿ƒé€²</li>
<li>é€šä¿¡æ©Ÿæ§‹ã‚’å°å…¥ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰</li>
</ul>
</details>

<details>
<summary><strong>æ¼”ç¿’5.4: ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒãƒˆã®æ”¹è‰¯</strong></summary>
<p><strong>å•é¡Œ</strong>: æä¾›ã•ã‚ŒãŸãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒãƒˆã«ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ï¼š</p>
<ul>
<li>è¤‡æ•°éŠ˜æŸ„ã®ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªç®¡ç†</li>
<li>ãƒªã‚¹ã‚¯ç®¡ç†ï¼ˆæœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³åˆ¶é™ï¼‰</li>
<li>ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ï¼ˆç§»å‹•å¹³å‡ã€RSIãªã©ï¼‰ã®è¿½åŠ </li>
</ul>
<p><strong>ãƒ’ãƒ³ãƒˆ</strong>:</p>
<ul>
<li>è¦³æ¸¬ç©ºé–“ã«ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚’è¿½åŠ </li>
<li>å ±é…¬é–¢æ•°ã«ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ªã‚’çµ„ã¿è¾¼ã‚€</li>
<li>è¡Œå‹•ç©ºé–“ã‚’æ‹¡å¼µï¼ˆè¤‡æ•°éŠ˜æŸ„ã®å£²è²·ï¼‰</li>
</ul>
</details>

<details>
<summary><strong>æ¼”ç¿’5.5: Stable-Baselines3ã§ã®ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯</strong></summary>
<p><strong>å•é¡Œ</strong>: Stable-Baselines3ã®ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½œæˆã—ã€è¨“ç·´ä¸­ã«ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š</p>
<ul>
<li>ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ã®å ±é…¬ã‚’ãƒ­ã‚°</li>
<li>æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•ä¿å­˜</li>
<li>è¨“ç·´ã®æ—©æœŸåœæ­¢ï¼ˆç›®æ¨™æ€§èƒ½é”æˆæ™‚ï¼‰</li>
</ul>
<p><strong>ãƒ’ãƒ³ãƒˆ</strong>:</p>
<ul>
<li><code>BaseCallback</code>ã‚’ç¶™æ‰¿</li>
<li><code>_on_step()</code>ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰</li>
<li><code>self.locals</code>ã§è¨“ç·´æƒ…å ±ã«ã‚¢ã‚¯ã‚»ã‚¹</li>
</ul>
</details>

<hr>

<div class="navigation">
    <a href="chapter4-policy-gradient.html" class="nav-button">â† ç¬¬4ç« : æ–¹ç­–å‹¾é…æ³•</a>
    <a href="../index.html" class="nav-button">ã‚³ãƒ¼ã‚¹ä¸€è¦§ã¸</a>
</div>

</main>

<footer>
    <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
</footer>

</body>
</html>
