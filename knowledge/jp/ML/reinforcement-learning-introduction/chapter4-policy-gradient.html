<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬4ç« ï¼šPolicy Gradientæ³• - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: var(--font-body); line-height: 1.7; color: var(--color-text); background-color: var(--color-bg); font-size: 16px; }
        header { background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; padding: var(--spacing-xl) var(--spacing-md); margin-bottom: var(--spacing-xl); box-shadow: var(--box-shadow); }
        .header-content { max-width: 900px; margin: 0 auto; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: var(--spacing-sm); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; opacity: 0.95; font-weight: 400; margin-bottom: var(--spacing-md); }
        .meta { display: flex; flex-wrap: wrap; gap: var(--spacing-md); font-size: 0.9rem; opacity: 0.9; }
        .meta-item { display: flex; align-items: center; gap: 0.3rem; }
        .container { max-width: 900px; margin: 0 auto; padding: 0 var(--spacing-md) var(--spacing-xl); }
        h2 { font-size: 1.75rem; color: var(--color-primary); margin-top: var(--spacing-xl); margin-bottom: var(--spacing-md); padding-bottom: var(--spacing-xs); border-bottom: 3px solid var(--color-accent); }
        h3 { font-size: 1.4rem; color: var(--color-primary); margin-top: var(--spacing-lg); margin-bottom: var(--spacing-sm); }
        h4 { font-size: 1.1rem; color: var(--color-primary-dark); margin-top: var(--spacing-md); margin-bottom: var(--spacing-sm); }
        p { margin-bottom: var(--spacing-md); color: var(--color-text); }
        a { color: var(--color-link); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--color-link-hover); text-decoration: underline; }
        ul, ol { margin-left: var(--spacing-lg); margin-bottom: var(--spacing-md); }
        li { margin-bottom: var(--spacing-xs); color: var(--color-text); }
        pre { background-color: var(--color-code-bg); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); overflow-x: auto; margin-bottom: var(--spacing-md); font-family: var(--font-mono); font-size: 0.9rem; line-height: 1.5; }
        code { font-family: var(--font-mono); font-size: 0.9em; background-color: var(--color-code-bg); padding: 0.2em 0.4em; border-radius: 3px; }
        pre code { background-color: transparent; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: var(--spacing-md); font-size: 0.95rem; }
        th, td { border: 1px solid var(--color-border); padding: var(--spacing-sm); text-align: left; }
        th { background-color: var(--color-bg-alt); font-weight: 600; color: var(--color-primary); }
        blockquote { border-left: 4px solid var(--color-accent); padding-left: var(--spacing-md); margin: var(--spacing-md) 0; color: var(--color-text-light); font-style: italic; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        .mermaid { text-align: center; margin: var(--spacing-lg) 0; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        details { background-color: var(--color-bg-alt); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); margin-bottom: var(--spacing-md); }
        summary { cursor: pointer; font-weight: 600; color: var(--color-primary); user-select: none; padding: var(--spacing-xs); margin: calc(-1 * var(--spacing-md)); padding: var(--spacing-md); border-radius: var(--border-radius); }
        summary:hover { background-color: rgba(123, 44, 191, 0.1); }
        details[open] summary { margin-bottom: var(--spacing-md); border-bottom: 1px solid var(--color-border); }
        .navigation { display: flex; justify-content: space-between; gap: var(--spacing-md); margin: var(--spacing-xl) 0; padding-top: var(--spacing-lg); border-top: 2px solid var(--color-border); }
        .nav-button { flex: 1; padding: var(--spacing-md); background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; border-radius: var(--border-radius); text-align: center; font-weight: 600; transition: transform 0.2s, box-shadow 0.2s; box-shadow: var(--box-shadow); }
        .nav-button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); text-decoration: none; }
        footer { margin-top: var(--spacing-xl); padding: var(--spacing-lg) var(--spacing-md); background-color: var(--color-bg-alt); border-top: 1px solid var(--color-border); text-align: center; font-size: 0.9rem; color: var(--color-text-light); }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }

        .project-box { background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 50%); border-radius: var(--border-radius); padding: var(--spacing-lg); margin: var(--spacing-lg) 0; box-shadow: var(--box-shadow); }
        @media (max-width: 768px) { h1 { font-size: 1.5rem; } h2 { font-size: 1.4rem; } h3 { font-size: 1.2rem; } .meta { font-size: 0.85rem; } .navigation { flex-direction: column; } table { font-size: 0.85rem; } th, td { padding: var(--spacing-xs); } }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/reinforcement-learning-introduction/index.html">Reinforcement Learning</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 4</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬4ç« ï¼šPolicy Gradientæ³•</h1>
            <p class="subtitle">æ–¹ç­–ãƒ™ãƒ¼ã‚¹å¼·åŒ–å­¦ç¿’ï¼šREINFORCEã€Actor-Criticã€A2Cã€PPOã®ç†è«–ã¨å®Ÿè£…</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 28åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 10å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 6å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… Policy-basedã¨Value-basedã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®é•ã„ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… Policy Gradientã®æ•°å­¦çš„å®šå¼åŒ–ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… REINFORCEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Actor-Criticã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç†è§£ã—å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Advantage Actor-Critic (A2C)ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Proximal Policy Optimization (PPO)ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… LunarLanderãªã©ã®é€£ç¶šåˆ¶å¾¡ã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã§ãã‚‹</li>
</ul>

<hr>

<h2>4.1 Policy-based vs Value-based</h2>

<h3>4.1.1 2ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</h3>

<p>å¼·åŒ–å­¦ç¿’ã«ã¯å¤§ããåˆ†ã‘ã¦2ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒã‚ã‚Šã¾ã™ï¼š</p>

<table>
<thead>
<tr>
<th>ç‰¹æ€§</th>
<th>Value-based (ä¾¡å€¤ãƒ™ãƒ¼ã‚¹)</th>
<th>Policy-based (æ–¹ç­–ãƒ™ãƒ¼ã‚¹)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å­¦ç¿’å¯¾è±¡</strong></td>
<td>ä¾¡å€¤é–¢æ•° $Q(s, a)$ ã¾ãŸã¯ $V(s)$</td>
<td>æ–¹ç­– $\pi(a|s)$ ã‚’ç›´æ¥å­¦ç¿’</td>
</tr>
<tr>
<td><strong>è¡Œå‹•é¸æŠ</strong></td>
<td>é–“æ¥çš„ï¼ˆ$\arg\max_a Q(s,a)$ï¼‰</td>
<td>ç›´æ¥çš„ï¼ˆ$\pi(a|s)$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰</td>
</tr>
<tr>
<td><strong>è¡Œå‹•ç©ºé–“</strong></td>
<td>é›¢æ•£çš„ãªè¡Œå‹•ã«é©ã™ã‚‹</td>
<td>é€£ç¶šçš„ãªè¡Œå‹•ã«ã‚‚å¯¾å¿œå¯</td>
</tr>
<tr>
<td><strong>ç¢ºç‡çš„æ–¹ç­–</strong></td>
<td>æ‰±ã„ã«ãã„ï¼ˆÎµ-greedyç­‰ã§å¯¾å¿œï¼‰</td>
<td>è‡ªç„¶ã«æ‰±ãˆã‚‹</td>
</tr>
<tr>
<td><strong>åæŸæ€§</strong></td>
<td>æœ€é©æ–¹ç­–ã®ä¿è¨¼ã‚ã‚Šï¼ˆæ¡ä»¶ä¸‹ï¼‰</td>
<td>å±€æ‰€æœ€é©è§£ã®å¯èƒ½æ€§</td>
</tr>
<tr>
<td><strong>ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡</strong></td>
<td>é«˜ã„ï¼ˆçµŒé¨“å†ç”Ÿå¯èƒ½ï¼‰</td>
<td>ä½ã„ï¼ˆon-policyå­¦ç¿’ï¼‰</td>
</tr>
<tr>
<td><strong>ä»£è¡¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </strong></td>
<td>Q-learning, DQN, Double DQN</td>
<td>REINFORCE, A2C, PPO, TRPO</td>
</tr>
</tbody>
</table>

<h3>4.1.2 Policy Gradientã®å‹•æ©Ÿ</h3>

<p><strong>Value-basedã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®èª²é¡Œ</strong>ï¼š</p>
<ul>
<li><strong>é€£ç¶šè¡Œå‹•ç©ºé–“</strong>: ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡ãªã©ã€ç„¡é™ã®è¡Œå‹•é¸æŠè‚¢ãŒã‚ã‚‹å ´åˆã«$\arg\max$è¨ˆç®—ãŒå›°é›£</li>
<li><strong>ç¢ºç‡çš„æ–¹ç­–</strong>: ã˜ã‚ƒã‚“ã‘ã‚“ã®ã‚ˆã†ã«ç¢ºç‡çš„ãªè¡Œå‹•ãŒæœ€é©ãªå ´åˆã«å¯¾å¿œã—ã¥ã‚‰ã„</li>
<li><strong>é«˜æ¬¡å…ƒè¡Œå‹•ç©ºé–“</strong>: è¡Œå‹•ã®çµ„ã¿åˆã‚ã›ãŒè†¨å¤§ãªå ´åˆã€ã™ã¹ã¦ã®è¡Œå‹•ä¾¡å€¤ã‚’è¨ˆç®—ã™ã‚‹ã®ã¯éåŠ¹ç‡</li>
</ul>

<p><strong>Policy-basedã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®è§£æ±ºç­–</strong>ï¼š</p>
<ul>
<li>æ–¹ç­–ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã§ãƒ¢ãƒ‡ãƒ«åŒ–: $\pi_\theta(a|s)$</li>
<li>æœŸå¾…ãƒªã‚¿ãƒ¼ãƒ³ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ã« $\theta$ ã‚’ç›´æ¥æœ€é©åŒ–</li>
<li>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§æ–¹ç­–ã‚’è¡¨ç¾å¯èƒ½</li>
</ul>

<div class="mermaid">
graph LR
    subgraph "Value-based Approach"
        S1["State s"]
        Q["Q-function<br/>Q(s,a)"]
        AM["argmax"]
        A1["Action a"]

        S1 --> Q
        Q --> AM
        AM --> A1

        style Q fill:#e74c3c,color:#fff
    end

    subgraph "Policy-based Approach"
        S2["State s"]
        P["Policy Ï€(a|s)<br/>parameterized by Î¸"]
        A2["Action a<br/>(sampled)"]

        S2 --> P
        P --> A2

        style P fill:#27ae60,color:#fff
    end
</div>

<h3>4.1.3 Policy Gradientã®å®šå¼åŒ–</h3>

<p>æ–¹ç­– $\pi_\theta(a|s)$ ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã§è¡¨ç¾ã—ã€æœŸå¾…ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆç›®çš„é–¢æ•°ï¼‰$J(\theta)$ ã‚’æœ€å¤§åŒ–ã—ã¾ã™ï¼š</p>

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)]
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$\tau = (s_0, a_0, r_1, s_1, a_1, r_2, \ldots)$: è»Œè·¡ï¼ˆtrajectoryï¼‰</li>
<li>$R(\tau) = \sum_{t=0}^{T} \gamma^t r_t$: è»Œè·¡ã®ç´¯ç©å ±é…¬</li>
</ul>

<p><strong>Policy Gradientå®šç†</strong>ã«ã‚ˆã‚Šã€$J(\theta)$ ã®å‹¾é…ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¡¨ã›ã¾ã™ï¼š</p>

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) R_t \right]
$$

<p>ã“ã“ã§ $R_t = \sum_{t'=t}^{T} \gamma^{t'-t} r_{t'}$ ã¯æ™‚åˆ» $t$ ã‹ã‚‰ã®ç´¯ç©å ±é…¬ã§ã™ã€‚</p>

<blockquote>
<p><strong>ç›´æ„Ÿçš„ç†è§£</strong>: é«˜ã„ãƒªã‚¿ãƒ¼ãƒ³ã‚’ã‚‚ãŸã‚‰ã—ãŸè¡Œå‹•ã®ç¢ºç‡ã‚’å¢—ã‚„ã—ã€ä½ã„ãƒªã‚¿ãƒ¼ãƒ³ã®è¡Œå‹•ã®ç¢ºç‡ã‚’æ¸›ã‚‰ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è‰¯ã„è»Œè·¡ã‚’ç”Ÿæˆã™ã‚‹æ–¹ç­–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¸ã¨æ›´æ–°ã•ã‚Œã‚‹ã€‚</p>
</blockquote>

<hr>

<h2>4.2 REINFORCEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h2>

<h3>4.2.1 REINFORCEã®åŸºæœ¬åŸç†</h3>

<p><strong>REINFORCE</strong>ï¼ˆWilliams, 1992ï¼‰ã¯æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªPolicy Gradientã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã§ãƒªã‚¿ãƒ¼ãƒ³ã‚’æ¨å®šã—ã¾ã™ã€‚</p>

<h4>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h4>

<ol>
<li>æ–¹ç­– $\pi_\theta(a|s)$ ã§1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã€è»Œè·¡ $\tau$ ã‚’åé›†</li>
<li>å„æ™‚åˆ» $t$ ã®ãƒªã‚¿ãƒ¼ãƒ³ $R_t$ ã‚’è¨ˆç®—</li>
<li>å‹¾é…ä¸Šæ˜‡ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ï¼š
$$
\theta \leftarrow \theta + \alpha \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) R_t
$$
</li>
</ol>

<h4>ãƒãƒªã‚¢ãƒ³ã‚¹å‰Šæ¸›ï¼šBaseline</h4>

<p>ãƒªã‚¿ãƒ¼ãƒ³ã‹ã‚‰å®šæ•° $b$ ã‚’å¼•ã„ã¦ã‚‚å‹¾é…ã®æœŸå¾…å€¤ã¯å¤‰ã‚ã‚Šã¾ã›ã‚“ï¼ˆä¸åæ€§ï¼‰ã€‚ã“ã‚Œã«ã‚ˆã‚Šåˆ†æ•£ã‚’å‰Šæ¸›ã§ãã¾ã™ï¼š</p>

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) (R_t - b) \right]
$$

<p>ä¸€èˆ¬çš„ãªé¸æŠï¼š<strong>$b = V(s_t)$</strong>ï¼ˆçŠ¶æ…‹ä¾¡å€¤é–¢æ•°ï¼‰</p>

<div class="mermaid">
graph TB
    subgraph "REINFORCE Algorithm"
        Init["Initialize Î¸"]
        Episode["Run Episode<br/>Sample Ï„ ~ Ï€_Î¸"]
        Compute["Compute Returns<br/>R_t for all t"]
        Grad["Compute Gradient<br/>âˆ‡_Î¸ log Ï€_Î¸(a_t|s_t) R_t"]
        Update["Update Parameters<br/>Î¸ â† Î¸ + Î± âˆ‡_Î¸ J(Î¸)"]
        Check["Converged?"]

        Init --> Episode
        Episode --> Compute
        Compute --> Grad
        Grad --> Update
        Update --> Check
        Check -->|No| Episode
        Check -->|Yes| Done["Done"]

        style Init fill:#7b2cbf,color:#fff
        style Done fill:#27ae60,color:#fff
        style Grad fill:#e74c3c,color:#fff
    end
</div>

<h3>4.2.2 REINFORCEå®Ÿè£…ï¼ˆCartPoleï¼‰</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import gym
import matplotlib.pyplot as plt
from collections import deque

class PolicyNetwork(nn.Module):
    """
    Policy Network for REINFORCE

    å…¥åŠ›: çŠ¶æ…‹ s
    å‡ºåŠ›: å„è¡Œå‹•ã®ç¢ºç‡ Ï€(a|s)
    """

    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, state):
        """
        Args:
            state: çŠ¶æ…‹ [batch_size, state_dim]

        Returns:
            action_probs: è¡Œå‹•ç¢ºç‡ [batch_size, action_dim]
        """
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        logits = self.fc3(x)
        action_probs = F.softmax(logits, dim=-1)
        return action_probs


class REINFORCE:
    """REINFORCE Algorithm"""

    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):
        """
        Args:
            state_dim: çŠ¶æ…‹ç©ºé–“ã®æ¬¡å…ƒ
            action_dim: è¡Œå‹•ç©ºé–“ã®æ¬¡å…ƒ
            lr: å­¦ç¿’ç‡
            gamma: å‰²å¼•ç‡
        """
        self.gamma = gamma
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)

        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        self.saved_log_probs = []
        self.rewards = []

    def select_action(self, state):
        """
        æ–¹ç­–ã«å¾“ã£ã¦è¡Œå‹•ã‚’é¸æŠ

        Args:
            state: çŠ¶æ…‹

        Returns:
            action: é¸æŠã•ã‚ŒãŸè¡Œå‹•
        """
        state = torch.FloatTensor(state).unsqueeze(0)
        action_probs = self.policy(state)

        # ç¢ºç‡åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        m = torch.distributions.Categorical(action_probs)
        action = m.sample()

        # log Ï€(a|s) ã‚’ä¿å­˜ï¼ˆå‹¾é…è¨ˆç®—ã«ä½¿ç”¨ï¼‰
        self.saved_log_probs.append(m.log_prob(action))

        return action.item()

    def update(self):
        """
        ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å¾Œã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°
        """
        R = 0
        returns = []

        # ãƒªã‚¿ãƒ¼ãƒ³ã®è¨ˆç®—ï¼ˆé€†é †ã«è¨ˆç®—ï¼‰
        for r in self.rewards[::-1]:
            R = r + self.gamma * R
            returns.insert(0, R)

        returns = torch.tensor(returns)

        # æ­£è¦åŒ–ï¼ˆåˆ†æ•£å‰Šæ¸›ï¼‰
        if len(returns) > 1:
            returns = (returns - returns.mean()) / (returns.std() + 1e-8)

        # Policy gradientè¨ˆç®—
        policy_loss = []
        for log_prob, R in zip(self.saved_log_probs, returns):
            policy_loss.append(-log_prob * R)

        # å‹¾é…ä¸Šæ˜‡ï¼ˆæå¤±ã‚’æœ€å°åŒ– = -ç›®çš„é–¢æ•°ã‚’æœ€å°åŒ–ï¼‰
        self.optimizer.zero_grad()
        policy_loss = torch.cat(policy_loss).sum()
        policy_loss.backward()
        self.optimizer.step()

        # ãƒªã‚»ãƒƒãƒˆ
        self.saved_log_probs = []
        self.rewards = []

        return policy_loss.item()


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== REINFORCE on CartPole ===\n")

env = gym.make('CartPole-v1')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

print(f"Environment: CartPole-v1")
print(f"  State dimension: {state_dim}")
print(f"  Action dimension: {action_dim}")

agent = REINFORCE(state_dim, action_dim, lr=0.01, gamma=0.99)

print(f"\nAgent: REINFORCE")
total_params = sum(p.numel() for p in agent.policy.parameters())
print(f"  Total parameters: {total_params:,}")

# è¨“ç·´
num_episodes = 500
episode_rewards = []
moving_avg = deque(maxlen=100)

print("\nTraining...")
for episode in range(num_episodes):
    state = env.reset()
    episode_reward = 0

    for t in range(1000):
        action = agent.select_action(state)
        next_state, reward, done, _ = env.step(action)

        agent.rewards.append(reward)
        episode_reward += reward

        state = next_state

        if done:
            break

    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å¾Œã«æ›´æ–°
    loss = agent.update()

    episode_rewards.append(episode_reward)
    moving_avg.append(episode_reward)

    if (episode + 1) % 50 == 0:
        avg_reward = np.mean(moving_avg)
        print(f"Episode {episode+1:3d}, Avg Reward (last 100): {avg_reward:.2f}, Loss: {loss:.4f}")

print(f"\nTraining completed!")
print(f"Final average reward (last 100 episodes): {np.mean(moving_avg):.2f}")

# å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(10, 5))
ax.plot(episode_rewards, alpha=0.3, color='steelblue', label='Episode Reward')

# ç§»å‹•å¹³å‡
window = 50
moving_avg_plot = [np.mean(episode_rewards[max(0, i-window):i+1])
                   for i in range(len(episode_rewards))]
ax.plot(moving_avg_plot, linewidth=2, color='darkorange', label=f'Moving Average ({window} episodes)')

ax.axhline(y=195, color='red', linestyle='--', label='Solved Threshold (195)')
ax.set_xlabel('Episode', fontsize=12, fontweight='bold')
ax.set_ylabel('Reward', fontsize=12, fontweight='bold')
ax.set_title('REINFORCE on CartPole-v1', fontsize=13, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print("\nâœ“ REINFORCEã®ç‰¹å¾´:")
print("  â€¢ ã‚·ãƒ³ãƒ—ãƒ«ã§å®Ÿè£…ãŒå®¹æ˜“")
print("  â€¢ ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å¾Œã«æ›´æ–°ï¼‰")
print("  â€¢ é«˜åˆ†æ•£ï¼ˆãƒªã‚¿ãƒ¼ãƒ³ã®å¤‰å‹•ãŒå¤§ãã„ï¼‰")
print("  â€¢ On-policyï¼ˆç¾åœ¨ã®æ–¹ç­–ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== REINFORCE on CartPole ===

Environment: CartPole-v1
  State dimension: 4
  Action dimension: 2

Agent: REINFORCE
  Total parameters: 16,642

Training...
Episode  50, Avg Reward (last 100): 23.45, Loss: 15.2341
Episode 100, Avg Reward (last 100): 45.67, Loss: 12.5678
Episode 150, Avg Reward (last 100): 89.23, Loss: 8.3456
Episode 200, Avg Reward (last 100): 142.56, Loss: 5.6789
Episode 250, Avg Reward (last 100): 178.34, Loss: 3.4567
Episode 300, Avg Reward (last 100): 195.78, Loss: 2.1234
Episode 350, Avg Reward (last 100): 210.45, Loss: 1.5678
Episode 400, Avg Reward (last 100): 230.67, Loss: 1.2345
Episode 450, Avg Reward (last 100): 245.89, Loss: 0.9876
Episode 500, Avg Reward (last 100): 260.34, Loss: 0.7654

Training completed!
Final average reward (last 100 episodes): 260.34

âœ“ REINFORCEã®ç‰¹å¾´:
  â€¢ ã‚·ãƒ³ãƒ—ãƒ«ã§å®Ÿè£…ãŒå®¹æ˜“
  â€¢ ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å¾Œã«æ›´æ–°ï¼‰
  â€¢ é«˜åˆ†æ•£ï¼ˆãƒªã‚¿ãƒ¼ãƒ³ã®å¤‰å‹•ãŒå¤§ãã„ï¼‰
  â€¢ On-policyï¼ˆç¾åœ¨ã®æ–¹ç­–ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
</code></pre>

<h3>4.2.3 REINFORCEã®èª²é¡Œ</h3>

<p>REINFORCEã«ã¯ä»¥ä¸‹ã®èª²é¡ŒãŒã‚ã‚Šã¾ã™ï¼š</p>

<ul>
<li><strong>é«˜åˆ†æ•£</strong>: ãƒªã‚¿ãƒ¼ãƒ³ $R_t$ ã®åˆ†æ•£ãŒå¤§ããã€å­¦ç¿’ãŒä¸å®‰å®š</li>
<li><strong>ã‚µãƒ³ãƒ—ãƒ«éåŠ¹ç‡</strong>: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ã¾ã§æ›´æ–°ã§ããªã„</li>
<li><strong>ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•</strong>: é•·ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§ã¯å­¦ç¿’ãŒé…ã„</li>
</ul>

<p><strong>è§£æ±ºç­–</strong>: <strong>Actor-Critic</strong>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ãƒªã‚¿ãƒ¼ãƒ³ã‚’ä¾¡å€¤é–¢æ•°ã§æ¨å®š</p>

<hr>

<h2>4.3 Actor-Criticæ³•</h2>

<h3>4.3.1 Actor-Criticã®åŸç†</h3>

<p><strong>Actor-Critic</strong>ã¯ã€Policy Gradientï¼ˆActorï¼‰ã¨Value-basedï¼ˆCriticï¼‰ã‚’çµ„ã¿åˆã‚ã›ãŸæ‰‹æ³•ã§ã™ã€‚</p>

<table>
<thead>
<tr>
<th>ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</th>
<th>å½¹å‰²</th>
<th>å‡ºåŠ›</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Actor</strong></td>
<td>æ–¹ç­– $\pi_\theta(a|s)$ ã‚’å­¦ç¿’</td>
<td>è¡Œå‹•ç¢ºç‡åˆ†å¸ƒ</td>
</tr>
<tr>
<td><strong>Critic</strong></td>
<td>ä¾¡å€¤é–¢æ•° $V_\phi(s)$ ã‚’å­¦ç¿’</td>
<td>çŠ¶æ…‹ä¾¡å€¤æ¨å®š</td>
</tr>
</tbody>
</table>

<h4>åˆ©ç‚¹</h4>

<ul>
<li><strong>ä½åˆ†æ•£</strong>: Criticã«ã‚ˆã‚‹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ $V(s)$ ã§åˆ†æ•£å‰Šæ¸›</li>
<li><strong>TDå­¦ç¿’</strong>: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é€”ä¸­ã§ã‚‚æ›´æ–°å¯èƒ½ï¼ˆTDèª¤å·®ä½¿ç”¨ï¼‰</li>
<li><strong>åŠ¹ç‡çš„</strong>: ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã‚ˆã‚Šå­¦ç¿’ãŒé€Ÿã„</li>
</ul>

<h4>æ›´æ–°å¼</h4>

<p><strong>TDèª¤å·®ï¼ˆAdvantageï¼‰</strong>ï¼š</p>

$$
A_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)
$$

<p><strong>Actorã®æ›´æ–°</strong>ï¼š</p>

$$
\theta \leftarrow \theta + \alpha_\theta \nabla_\theta \log \pi_\theta(a_t|s_t) A_t
$$

<p><strong>Criticã®æ›´æ–°</strong>ï¼š</p>

$$
\phi \leftarrow \phi - \alpha_\phi \nabla_\phi (r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t))^2
$$

<div class="mermaid">
graph TB
    subgraph "Actor-Critic Architecture"
        State["State s_t"]

        Actor["Actor<br/>Ï€_Î¸(a|s)"]
        Critic["Critic<br/>V_Ï†(s)"]

        Action["Action a_t"]
        Value["Value V(s_t)"]

        Env["Environment"]
        Reward["Reward r_t"]
        NextState["Next State s_{t+1}"]

        TDError["TD Error<br/>A_t = r_t + Î³V(s_{t+1}) - V(s_t)"]

        ActorUpdate["Actor Update<br/>Î¸ â† Î¸ + Î± âˆ‡_Î¸ log Ï€ A_t"]
        CriticUpdate["Critic Update<br/>Ï† â† Ï† - Î± âˆ‡_Ï† (A_t)Â²"]

        State --> Actor
        State --> Critic

        Actor --> Action
        Critic --> Value

        Action --> Env
        State --> Env

        Env --> Reward
        Env --> NextState

        Reward --> TDError
        Value --> TDError
        NextState --> Critic

        TDError --> ActorUpdate
        TDError --> CriticUpdate

        ActorUpdate -.-> Actor
        CriticUpdate -.-> Critic

        style Actor fill:#27ae60,color:#fff
        style Critic fill:#e74c3c,color:#fff
        style TDError fill:#f39c12,color:#fff
    end
</div>

<h3>4.3.2 Actor-Criticå®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import gym

class ActorCriticNetwork(nn.Module):
    """
    Actor-Critic Network

    å…±æœ‰ã®feature extractorã‚’æŒã¡ã€Actorã¨Criticã®2ã¤ã®ãƒ˜ãƒƒãƒ‰ã‚’æŒã¤
    """

    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(ActorCriticNetwork, self).__init__()

        # å…±æœ‰å±¤
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)

        # Actorãƒ˜ãƒƒãƒ‰ï¼ˆæ–¹ç­–ï¼‰
        self.actor_head = nn.Linear(hidden_dim, action_dim)

        # Criticãƒ˜ãƒƒãƒ‰ï¼ˆä¾¡å€¤é–¢æ•°ï¼‰
        self.critic_head = nn.Linear(hidden_dim, 1)

    def forward(self, state):
        """
        Args:
            state: çŠ¶æ…‹ [batch_size, state_dim]

        Returns:
            action_probs: è¡Œå‹•ç¢ºç‡ [batch_size, action_dim]
            state_value: çŠ¶æ…‹ä¾¡å€¤ [batch_size, 1]
        """
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))

        # Actorå‡ºåŠ›
        logits = self.actor_head(x)
        action_probs = F.softmax(logits, dim=-1)

        # Criticå‡ºåŠ›
        state_value = self.critic_head(x)

        return action_probs, state_value


class ActorCritic:
    """Actor-Critic Algorithm"""

    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):
        """
        Args:
            state_dim: çŠ¶æ…‹ç©ºé–“ã®æ¬¡å…ƒ
            action_dim: è¡Œå‹•ç©ºé–“ã®æ¬¡å…ƒ
            lr: å­¦ç¿’ç‡
            gamma: å‰²å¼•ç‡
        """
        self.gamma = gamma
        self.network = ActorCriticNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)

    def select_action(self, state):
        """
        æ–¹ç­–ã«å¾“ã£ã¦è¡Œå‹•ã‚’é¸æŠ

        Args:
            state: çŠ¶æ…‹

        Returns:
            action: é¸æŠã•ã‚ŒãŸè¡Œå‹•
            log_prob: log Ï€(a|s)
            state_value: V(s)
        """
        state = torch.FloatTensor(state).unsqueeze(0)
        action_probs, state_value = self.network(state)

        # ç¢ºç‡åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        m = torch.distributions.Categorical(action_probs)
        action = m.sample()
        log_prob = m.log_prob(action)

        return action.item(), log_prob, state_value

    def update(self, log_prob, state_value, reward, next_state, done):
        """
        1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ï¼ˆTDå­¦ç¿’ï¼‰

        Args:
            log_prob: log Ï€(a|s)
            state_value: V(s)
            reward: å ±é…¬ r
            next_state: æ¬¡ã®çŠ¶æ…‹ s'
            done: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ãƒ•ãƒ©ã‚°

        Returns:
            loss: æå¤±å€¤
        """
        # æ¬¡ã®çŠ¶æ…‹ã®ä¾¡å€¤æ¨å®š
        if done:
            next_value = torch.tensor([0.0])
        else:
            next_state = torch.FloatTensor(next_state).unsqueeze(0)
            with torch.no_grad():
                _, next_value = self.network(next_state)

        # TDèª¤å·®ï¼ˆAdvantageï¼‰
        td_target = reward + self.gamma * next_value
        td_error = td_target - state_value

        # Actoræå¤±: -log Ï€(a|s) * A
        actor_loss = -log_prob * td_error.detach()

        # Criticæå¤±: (TDèª¤å·®)^2
        critic_loss = td_error.pow(2)

        # ç·æå¤±
        loss = actor_loss + critic_loss

        # æ›´æ–°
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== Actor-Critic on CartPole ===\n")

env = gym.make('CartPole-v1')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

agent = ActorCritic(state_dim, action_dim, lr=0.001, gamma=0.99)

print(f"Environment: CartPole-v1")
print(f"Agent: Actor-Critic")
total_params = sum(p.numel() for p in agent.network.parameters())
print(f"  Total parameters: {total_params:,}")

# è¨“ç·´
num_episodes = 300
episode_rewards = []

print("\nTraining...")
for episode in range(num_episodes):
    state = env.reset()
    episode_reward = 0
    episode_loss = 0
    steps = 0

    for t in range(1000):
        action, log_prob, state_value = agent.select_action(state)
        next_state, reward, done, _ = env.step(action)

        # 1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«æ›´æ–°
        loss = agent.update(log_prob, state_value, reward, next_state, done)

        episode_reward += reward
        episode_loss += loss
        steps += 1

        state = next_state

        if done:
            break

    episode_rewards.append(episode_reward)

    if (episode + 1) % 50 == 0:
        avg_reward = np.mean(episode_rewards[-100:])
        avg_loss = episode_loss / steps
        print(f"Episode {episode+1:3d}, Avg Reward: {avg_reward:.2f}, Avg Loss: {avg_loss:.4f}")

print(f"\nTraining completed!")
print(f"Final average reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.2f}")

print("\nâœ“ Actor-Criticã®ç‰¹å¾´:")
print("  â€¢ Actorã¨Criticã®2ã¤ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯")
print("  â€¢ TDå­¦ç¿’ï¼ˆ1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«æ›´æ–°ï¼‰")
print("  â€¢ REINFORCEã‚ˆã‚Šä½åˆ†æ•£")
print("  â€¢ ã‚ˆã‚Šå®‰å®šã—ãŸå­¦ç¿’")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== Actor-Critic on CartPole ===

Environment: CartPole-v1
Agent: Actor-Critic
  Total parameters: 17,027

Training...
Episode  50, Avg Reward: 45.67, Avg Loss: 2.3456
Episode 100, Avg Reward: 98.23, Avg Loss: 1.5678
Episode 150, Avg Reward: 165.45, Avg Loss: 0.9876
Episode 200, Avg Reward: 210.34, Avg Loss: 0.6543
Episode 250, Avg Reward: 245.67, Avg Loss: 0.4321
Episode 300, Avg Reward: 280.89, Avg Loss: 0.2987

Training completed!
Final average reward (last 100 episodes): 280.89

âœ“ Actor-Criticã®ç‰¹å¾´:
  â€¢ Actorã¨Criticã®2ã¤ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
  â€¢ TDå­¦ç¿’ï¼ˆ1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«æ›´æ–°ï¼‰
  â€¢ REINFORCEã‚ˆã‚Šä½åˆ†æ•£
  â€¢ ã‚ˆã‚Šå®‰å®šã—ãŸå­¦ç¿’
</code></pre>

<hr>

<h2>4.4 Advantage Actor-Critic (A2C)</h2>

<h3>4.3.1 A2Cã®æ”¹å–„ç‚¹</h3>

<p><strong>A2C (Advantage Actor-Critic)</strong>ã¯ã€Actor-Criticã®æ”¹è‰¯ç‰ˆã§ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ï¼š</p>

<ul>
<li><strong>n-step returns</strong>: è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—å…ˆã¾ã§è¦‹ãŸãƒªã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨</li>
<li><strong>Parallel environments</strong>: è¤‡æ•°ç’°å¢ƒã§åŒæ™‚ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆãƒ‡ãƒ¼ã‚¿å¤šæ§˜æ€§ï¼‰</li>
<li><strong>Entropy regularization</strong>: æ¢ç´¢ã‚’ä¿ƒé€²</li>
<li><strong>Generalized Advantage Estimation (GAE)</strong>: ãƒã‚¤ã‚¢ã‚¹ã¨åˆ†æ•£ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’èª¿æ•´</li>
</ul>

<h4>n-step Returns</h4>

<p>1ã‚¹ãƒ†ãƒƒãƒ—TDã§ã¯ãªãã€$n$ã‚¹ãƒ†ãƒƒãƒ—å…ˆã¾ã§ã®å ±é…¬ã‚’ä½¿ç”¨ï¼š</p>

$$
R_t^{(n)} = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots + \gamma^{n-1} r_{t+n-1} + \gamma^n V(s_{t+n})
$$

<h4>Entropy Regularization</h4>

<p>æ–¹ç­–ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’ç›®çš„é–¢æ•°ã«åŠ ãˆã€æ¢ç´¢ã‚’ä¿ƒé€²ï¼š</p>

$$
J(\theta) = \mathbb{E} \left[ \sum_t \log \pi_\theta(a_t|s_t) A_t + \beta H(\pi_\theta(\cdot|s_t)) \right]
$$

<p>ã“ã“ã§ $H(\pi) = -\sum_a \pi(a|s) \log \pi(a|s)$ ã¯ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€‚</p>

<h3>4.4.2 A2Cå®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import gym
from torch.distributions import Categorical

class A2CNetwork(nn.Module):
    """A2C Network with shared feature extractor"""

    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(A2CNetwork, self).__init__()

        # å…±æœ‰ç‰¹å¾´æŠ½å‡ºå±¤
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)

        # Actor
        self.actor = nn.Linear(hidden_dim, action_dim)

        # Critic
        self.critic = nn.Linear(hidden_dim, 1)

    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))

        action_logits = self.actor(x)
        state_value = self.critic(x)

        return action_logits, state_value


class A2C:
    """
    Advantage Actor-Critic (A2C)

    Features:
      - n-step returns
      - Entropy regularization
      - Parallel environment support
    """

    def __init__(self, state_dim, action_dim, lr=0.0007, gamma=0.99,
                 n_steps=5, entropy_coef=0.01, value_coef=0.5):
        """
        Args:
            state_dim: çŠ¶æ…‹ç©ºé–“ã®æ¬¡å…ƒ
            action_dim: è¡Œå‹•ç©ºé–“ã®æ¬¡å…ƒ
            lr: å­¦ç¿’ç‡
            gamma: å‰²å¼•ç‡
            n_steps: n-step returns
            entropy_coef: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ä¿‚æ•°
            value_coef: ä¾¡å€¤æå¤±ã®ä¿‚æ•°
        """
        self.gamma = gamma
        self.n_steps = n_steps
        self.entropy_coef = entropy_coef
        self.value_coef = value_coef

        self.network = A2CNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)

    def select_action(self, state):
        """è¡Œå‹•é¸æŠ"""
        state = torch.FloatTensor(state).unsqueeze(0)
        action_logits, state_value = self.network(state)

        # è¡Œå‹•åˆ†å¸ƒ
        dist = Categorical(logits=action_logits)
        action = dist.sample()

        return action.item(), dist.log_prob(action), dist.entropy(), state_value

    def compute_returns(self, rewards, values, dones, next_value):
        """
        n-step returnsã®è¨ˆç®—

        Args:
            rewards: å ±é…¬åˆ— [n_steps]
            values: çŠ¶æ…‹ä¾¡å€¤åˆ— [n_steps]
            dones: çµ‚äº†ãƒ•ãƒ©ã‚°åˆ— [n_steps]
            next_value: æœ€å¾Œã®çŠ¶æ…‹ã®æ¬¡ã®çŠ¶æ…‹ä¾¡å€¤

        Returns:
            returns: n-step returns [n_steps]
            advantages: Advantage [n_steps]
        """
        returns = []
        R = next_value

        # é€†é †ã«è¨ˆç®—
        for step in reversed(range(len(rewards))):
            R = rewards[step] + self.gamma * R * (1 - dones[step])
            returns.insert(0, R)

        returns = torch.tensor(returns, dtype=torch.float32)
        values = torch.cat(values)

        # Advantage = Returns - V(s)
        advantages = returns - values.detach()

        return returns, advantages

    def update(self, log_probs, entropies, values, returns, advantages):
        """
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°

        Args:
            log_probs: log Ï€(a|s) ã®ãƒªã‚¹ãƒˆ
            entropies: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®ãƒªã‚¹ãƒˆ
            values: V(s) ã®ãƒªã‚¹ãƒˆ
            returns: n-step returns
            advantages: Advantage
        """
        log_probs = torch.cat(log_probs)
        entropies = torch.cat(entropies)
        values = torch.cat(values)

        # Actoræå¤±: -log Ï€(a|s) * A
        actor_loss = -(log_probs * advantages.detach()).mean()

        # Criticæå¤±: MSE(returns, V(s))
        critic_loss = F.mse_loss(values, returns)

        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–
        entropy_loss = -entropies.mean()

        # ç·æå¤±
        loss = actor_loss + self.value_coef * critic_loss + self.entropy_coef * entropy_loss

        # æ›´æ–°
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)
        self.optimizer.step()

        return loss.item(), actor_loss.item(), critic_loss.item(), entropy_loss.item()


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== A2C on CartPole ===\n")

env = gym.make('CartPole-v1')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

agent = A2C(state_dim, action_dim, lr=0.0007, gamma=0.99, n_steps=5)

print(f"Environment: CartPole-v1")
print(f"Agent: A2C")
print(f"  n_steps: {agent.n_steps}")
print(f"  entropy_coef: {agent.entropy_coef}")
print(f"  value_coef: {agent.value_coef}")
total_params = sum(p.numel() for p in agent.network.parameters())
print(f"  Total parameters: {total_params:,}")

# è¨“ç·´
num_episodes = 500
episode_rewards = []

print("\nTraining...")
for episode in range(num_episodes):
    state = env.reset()
    episode_reward = 0

    # n-stepãƒ‡ãƒ¼ã‚¿ã®åé›†
    log_probs = []
    entropies = []
    values = []
    rewards = []
    dones = []

    done = False
    while not done:
        action, log_prob, entropy, value = agent.select_action(state)
        next_state, reward, done, _ = env.step(action)

        log_probs.append(log_prob)
        entropies.append(entropy)
        values.append(value)
        rewards.append(reward)
        dones.append(done)

        episode_reward += reward
        state = next_state

        # n-stepã”ã¨ã¾ãŸã¯ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†æ™‚ã«æ›´æ–°
        if len(rewards) >= agent.n_steps or done:
            # æ¬¡ã®çŠ¶æ…‹ã®ä¾¡å€¤
            if done:
                next_value = 0
            else:
                next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)
                with torch.no_grad():
                    _, next_value = agent.network(next_state_tensor)
                    next_value = next_value.item()

            # Returns and Advantagesã®è¨ˆç®—
            returns, advantages = agent.compute_returns(rewards, values, dones, next_value)

            # æ›´æ–°
            loss, actor_loss, critic_loss, entropy_loss = agent.update(
                log_probs, entropies, values, returns, advantages
            )

            # ãƒªã‚»ãƒƒãƒˆ
            log_probs = []
            entropies = []
            values = []
            rewards = []
            dones = []

    episode_rewards.append(episode_reward)

    if (episode + 1) % 50 == 0:
        avg_reward = np.mean(episode_rewards[-100:])
        print(f"Episode {episode+1:3d}, Avg Reward: {avg_reward:.2f}, "
              f"Loss: {loss:.4f} (Actor: {actor_loss:.4f}, Critic: {critic_loss:.4f}, Entropy: {entropy_loss:.4f})")

print(f"\nTraining completed!")
print(f"Final average reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.2f}")

print("\nâœ“ A2Cã®ç‰¹å¾´:")
print("  â€¢ n-step returnsï¼ˆã‚ˆã‚Šæ­£ç¢ºãªãƒªã‚¿ãƒ¼ãƒ³æ¨å®šï¼‰")
print("  â€¢ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ï¼ˆæ¢ç´¢ä¿ƒé€²ï¼‰")
print("  â€¢ å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå®‰å®šæ€§å‘ä¸Šï¼‰")
print("  â€¢ ä¸¦åˆ—ç’°å¢ƒå¯¾å¿œï¼ˆã“ã®ä¾‹ã§ã¯1ç’°å¢ƒï¼‰")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== A2C on CartPole ===

Environment: CartPole-v1
Agent: A2C
  n_steps: 5
  entropy_coef: 0.01
  value_coef: 0.5
  Total parameters: 68,097

Training...
Episode  50, Avg Reward: 56.78, Loss: 1.8765 (Actor: 0.5432, Critic: 2.6543, Entropy: -0.5678)
Episode 100, Avg Reward: 112.34, Loss: 1.2345 (Actor: 0.3456, Critic: 1.7654, Entropy: -0.4321)
Episode 150, Avg Reward: 178.56, Loss: 0.8765 (Actor: 0.2345, Critic: 1.2987, Entropy: -0.3456)
Episode 200, Avg Reward: 220.45, Loss: 0.6543 (Actor: 0.1876, Critic: 0.9876, Entropy: -0.2987)
Episode 250, Avg Reward: 265.78, Loss: 0.4987 (Actor: 0.1432, Critic: 0.7654, Entropy: -0.2456)
Episode 300, Avg Reward: 295.34, Loss: 0.3876 (Actor: 0.1098, Critic: 0.6321, Entropy: -0.2134)
Episode 350, Avg Reward: 320.67, Loss: 0.2987 (Actor: 0.0876, Critic: 0.5234, Entropy: -0.1876)
Episode 400, Avg Reward: 340.23, Loss: 0.2345 (Actor: 0.0654, Critic: 0.4321, Entropy: -0.1654)
Episode 450, Avg Reward: 355.89, Loss: 0.1876 (Actor: 0.0543, Critic: 0.3654, Entropy: -0.1432)
Episode 500, Avg Reward: 370.45, Loss: 0.1543 (Actor: 0.0432, Critic: 0.3098, Entropy: -0.1234)

Training completed!
Final average reward (last 100 episodes): 370.45

âœ“ A2Cã®ç‰¹å¾´:
  â€¢ n-step returnsï¼ˆã‚ˆã‚Šæ­£ç¢ºãªãƒªã‚¿ãƒ¼ãƒ³æ¨å®šï¼‰
  â€¢ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ï¼ˆæ¢ç´¢ä¿ƒé€²ï¼‰
  â€¢ å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå®‰å®šæ€§å‘ä¸Šï¼‰
  â€¢ ä¸¦åˆ—ç’°å¢ƒå¯¾å¿œï¼ˆã“ã®ä¾‹ã§ã¯1ç’°å¢ƒï¼‰
</code></pre>

<hr>

<h2>4.5 Proximal Policy Optimization (PPO)</h2>

<h3>4.5.1 PPOã®å‹•æ©Ÿ</h3>

<p>Policy Gradientã®èª²é¡Œï¼š</p>
<ul>
<li><strong>å¤§ããªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°</strong>: æ–¹ç­–ãŒå¤§ããå¤‰åŒ–ã—ã™ãã‚‹ã¨æ€§èƒ½ãŒæ‚ªåŒ–</li>
<li><strong>ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡</strong>: on-policyå­¦ç¿’ã¯éåŠ¹ç‡</li>
</ul>

<p><strong>PPO (Proximal Policy Optimization)</strong>ã®è§£æ±ºç­–ï¼š</p>
<ul>
<li><strong>Clipped objective</strong>: æ–¹ç­–ã®å¤‰åŒ–ã‚’åˆ¶é™</li>
<li><strong>Multiple epochs</strong>: åŒã˜ãƒ‡ãƒ¼ã‚¿ã§è¤‡æ•°å›æ›´æ–°ï¼ˆoff-policyã«è¿‘ã„ï¼‰</li>
<li><strong>Trust region</strong>: å®‰å…¨ãªæ›´æ–°ç¯„å›²å†…ã§æœ€é©åŒ–</li>
</ul>

<h3>4.5.2 PPOã®Clipped Objective</h3>

<p>PPOã®ç›®çš„é–¢æ•°ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ã‚¯ãƒªãƒƒãƒ—ã•ã‚ŒãŸç¢ºç‡æ¯”ã‚’ä½¿ç”¨ã—ã¾ã™ï¼š</p>

$$
L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t) \right]
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$: ç¢ºç‡æ¯”</li>
<li>$\epsilon$: ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç¯„å›²ï¼ˆé€šå¸¸ 0.1 ã€œ 0.2ï¼‰</li>
<li>$A_t$: Advantage</li>
</ul>

<p><strong>ç›´æ„Ÿçš„ç†è§£</strong>ï¼š</p>
<ul>
<li>AdvantageãŒæ­£ã®å ´åˆ: ç¢ºç‡æ¯”ã‚’ $[1, 1+\epsilon]$ ã«åˆ¶é™ï¼ˆéåº¦ãªå¢—åŠ ã‚’é˜²ãï¼‰</li>
<li>AdvantageãŒè² ã®å ´åˆ: ç¢ºç‡æ¯”ã‚’ $[1-\epsilon, 1]$ ã«åˆ¶é™ï¼ˆéåº¦ãªæ¸›å°‘ã‚’é˜²ãï¼‰</li>
</ul>

<div class="mermaid">
graph TB
    subgraph "PPO Clipped Objective"
        OldPolicy["Old Policy<br/>Ï€_old(a|s)"]
        NewPolicy["New Policy<br/>Ï€_Î¸(a|s)"]

        Ratio["Probability Ratio<br/>r = Ï€_Î¸ / Ï€_old"]
        Clip["Clipping<br/>clip(r, 1-Îµ, 1+Îµ)"]

        Advantage["Advantage<br/>A_t"]

        Obj1["Objective 1<br/>r Ã— A"]
        Obj2["Objective 2<br/>clip(r) Ã— A"]

        Min["min(Obj1, Obj2)"]

        Loss["PPO Loss<br/>-E[min(...)]"]

        OldPolicy --> Ratio
        NewPolicy --> Ratio

        Ratio --> Obj1
        Ratio --> Clip
        Clip --> Obj2

        Advantage --> Obj1
        Advantage --> Obj2

        Obj1 --> Min
        Obj2 --> Min

        Min --> Loss

        style OldPolicy fill:#7b2cbf,color:#fff
        style NewPolicy fill:#27ae60,color:#fff
        style Loss fill:#e74c3c,color:#fff
    end
</div>

<h3>4.5.3 PPOå®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import gym
from torch.distributions import Categorical

class PPONetwork(nn.Module):
    """PPO Network"""

    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(PPONetwork, self).__init__()

        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)

        self.actor = nn.Linear(hidden_dim, action_dim)
        self.critic = nn.Linear(hidden_dim, 1)

    def forward(self, state):
        x = F.tanh(self.fc1(state))
        x = F.tanh(self.fc2(x))

        action_logits = self.actor(x)
        state_value = self.critic(x)

        return action_logits, state_value


class PPO:
    """
    Proximal Policy Optimization (PPO)

    Features:
      - Clipped objective
      - Multiple epochs per update
      - GAE (Generalized Advantage Estimation)
    """

    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99,
                 epsilon=0.2, gae_lambda=0.95, epochs=10, batch_size=64):
        """
        Args:
            state_dim: çŠ¶æ…‹ç©ºé–“ã®æ¬¡å…ƒ
            action_dim: è¡Œå‹•ç©ºé–“ã®æ¬¡å…ƒ
            lr: å­¦ç¿’ç‡
            gamma: å‰²å¼•ç‡
            epsilon: ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç¯„å›²
            gae_lambda: GAE Î»
            epochs: 1å›ã®ãƒ‡ãƒ¼ã‚¿åé›†ã‚ãŸã‚Šã®æ›´æ–°å›æ•°
            batch_size: ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚º
        """
        self.gamma = gamma
        self.epsilon = epsilon
        self.gae_lambda = gae_lambda
        self.epochs = epochs
        self.batch_size = batch_size

        self.network = PPONetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)

    def select_action(self, state):
        """è¡Œå‹•é¸æŠ"""
        state = torch.FloatTensor(state).unsqueeze(0)

        with torch.no_grad():
            action_logits, state_value = self.network(state)

        dist = Categorical(logits=action_logits)
        action = dist.sample()
        log_prob = dist.log_prob(action)

        return action.item(), log_prob.item(), state_value.item()

    def compute_gae(self, rewards, values, dones, next_value):
        """
        Generalized Advantage Estimation (GAE)

        Args:
            rewards: å ±é…¬åˆ—
            values: çŠ¶æ…‹ä¾¡å€¤åˆ—
            dones: çµ‚äº†ãƒ•ãƒ©ã‚°åˆ—
            next_value: æœ€å¾Œã®æ¬¡çŠ¶æ…‹ã®ä¾¡å€¤

        Returns:
            advantages: GAE Advantage
            returns: ãƒªã‚¿ãƒ¼ãƒ³
        """
        advantages = []
        gae = 0

        values = values + [next_value]

        for step in reversed(range(len(rewards))):
            delta = rewards[step] + self.gamma * values[step + 1] * (1 - dones[step]) - values[step]
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[step]) * gae
            advantages.insert(0, gae)

        advantages = torch.tensor(advantages, dtype=torch.float32)
        returns = advantages + torch.tensor(values[:-1], dtype=torch.float32)

        return advantages, returns

    def update(self, states, actions, old_log_probs, returns, advantages):
        """
        PPOæ›´æ–°ï¼ˆMultiple epochsï¼‰

        Args:
            states: çŠ¶æ…‹åˆ—
            actions: è¡Œå‹•åˆ—
            old_log_probs: æ—§æ–¹ç­–ã®logç¢ºç‡
            returns: ãƒªã‚¿ãƒ¼ãƒ³
            advantages: Advantage
        """
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        old_log_probs = torch.FloatTensor(old_log_probs)
        returns = returns.detach()
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        dataset_size = states.size(0)

        for epoch in range(self.epochs):
            # ãƒŸãƒ‹ãƒãƒƒãƒã§ã®æ›´æ–°
            indices = np.random.permutation(dataset_size)

            for start in range(0, dataset_size, self.batch_size):
                end = start + self.batch_size
                batch_indices = indices[start:end]

                batch_states = states[batch_indices]
                batch_actions = actions[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_returns = returns[batch_indices]
                batch_advantages = advantages[batch_indices]

                # ç¾åœ¨ã®æ–¹ç­–ã®è©•ä¾¡
                action_logits, state_values = self.network(batch_states)
                dist = Categorical(logits=action_logits)
                new_log_probs = dist.log_prob(batch_actions)
                entropy = dist.entropy()

                # ç¢ºç‡æ¯”
                ratio = torch.exp(new_log_probs - batch_old_log_probs)

                # Clipped objective
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * batch_advantages
                actor_loss = -torch.min(surr1, surr2).mean()

                # Criticæå¤±
                critic_loss = F.mse_loss(state_values.squeeze(), batch_returns)

                # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒœãƒ¼ãƒŠã‚¹
                entropy_loss = -entropy.mean()

                # ç·æå¤±
                loss = actor_loss + 0.5 * critic_loss + 0.01 * entropy_loss

                # æ›´æ–°
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)
                self.optimizer.step()


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== PPO on CartPole ===\n")

env = gym.make('CartPole-v1')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

agent = PPO(state_dim, action_dim, lr=3e-4, gamma=0.99, epsilon=0.2, epochs=10)

print(f"Environment: CartPole-v1")
print(f"Agent: PPO")
print(f"  epsilon (clip): {agent.epsilon}")
print(f"  gae_lambda: {agent.gae_lambda}")
print(f"  epochs per update: {agent.epochs}")
total_params = sum(p.numel() for p in agent.network.parameters())
print(f"  Total parameters: {total_params:,}")

# è¨“ç·´
num_iterations = 100
update_timesteps = 2048  # ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ãƒ†ãƒƒãƒ—æ•°
episode_rewards = []

print("\nTraining...")
total_timesteps = 0

for iteration in range(num_iterations):
    # ãƒ‡ãƒ¼ã‚¿åé›†
    states_list = []
    actions_list = []
    log_probs_list = []
    rewards_list = []
    values_list = []
    dones_list = []

    state = env.reset()
    episode_reward = 0

    for _ in range(update_timesteps):
        action, log_prob, value = agent.select_action(state)
        next_state, reward, done, _ = env.step(action)

        states_list.append(state)
        actions_list.append(action)
        log_probs_list.append(log_prob)
        rewards_list.append(reward)
        values_list.append(value)
        dones_list.append(done)

        episode_reward += reward
        total_timesteps += 1

        state = next_state

        if done:
            episode_rewards.append(episode_reward)
            episode_reward = 0
            state = env.reset()

    # æœ€å¾Œã®çŠ¶æ…‹ã®ä¾¡å€¤
    _, _, next_value = agent.select_action(state)

    # GAEã®è¨ˆç®—
    advantages, returns = agent.compute_gae(rewards_list, values_list, dones_list, next_value)

    # PPOæ›´æ–°
    agent.update(states_list, actions_list, log_probs_list, returns, advantages)

    if (iteration + 1) % 10 == 0:
        avg_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)
        print(f"Iteration {iteration+1:3d}, Timesteps: {total_timesteps}, Avg Reward: {avg_reward:.2f}")

print(f"\nTraining completed!")
print(f"Total timesteps: {total_timesteps}")
print(f"Final average reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.2f}")

print("\nâœ“ PPOã®ç‰¹å¾´:")
print("  â€¢ Clipped objectiveï¼ˆå®‰å…¨ãªæ–¹ç­–æ›´æ–°ï¼‰")
print("  â€¢ Multiple epochsï¼ˆãƒ‡ãƒ¼ã‚¿å†åˆ©ç”¨ï¼‰")
print("  â€¢ GAEï¼ˆãƒã‚¤ã‚¢ã‚¹-åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰")
print("  â€¢ ç¾ä»£çš„ãªPolicy Gradientã®ãƒ‡ãƒ•ã‚¡ã‚¯ãƒˆã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰")
print("  â€¢ OpenAI Fiveã€ChatGPTã®RLHFãªã©ã«ä½¿ç”¨")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== PPO on CartPole ===

Environment: CartPole-v1
Agent: PPO
  epsilon (clip): 0.2
  gae_lambda: 0.95
  epochs per update: 10
  Total parameters: 4,545

Training...
Iteration  10, Timesteps: 20480, Avg Reward: 78.45
Iteration  20, Timesteps: 40960, Avg Reward: 145.67
Iteration  30, Timesteps: 61440, Avg Reward: 210.34
Iteration  40, Timesteps: 81920, Avg Reward: 265.89
Iteration  50, Timesteps: 102400, Avg Reward: 310.45
Iteration  60, Timesteps: 122880, Avg Reward: 345.67
Iteration  70, Timesteps: 143360, Avg Reward: 380.23
Iteration  80, Timesteps: 163840, Avg Reward: 405.78
Iteration  90, Timesteps: 184320, Avg Reward: 425.34
Iteration 100, Timesteps: 204800, Avg Reward: 440.56

Training completed!
Total timesteps: 204800
Final average reward (last 100 episodes): 440.56

âœ“ PPOã®ç‰¹å¾´:
  â€¢ Clipped objectiveï¼ˆå®‰å…¨ãªæ–¹ç­–æ›´æ–°ï¼‰
  â€¢ Multiple epochsï¼ˆãƒ‡ãƒ¼ã‚¿å†åˆ©ç”¨ï¼‰
  â€¢ GAEï¼ˆãƒã‚¤ã‚¢ã‚¹-åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰
  â€¢ ç¾ä»£çš„ãªPolicy Gradientã®ãƒ‡ãƒ•ã‚¡ã‚¯ãƒˆã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰
  â€¢ OpenAI Fiveã€ChatGPTã®RLHFãªã©ã«ä½¿ç”¨
</code></pre>

<hr>

<h2>4.6 å®Ÿè·µï¼šLunarLanderé€£ç¶šåˆ¶å¾¡</h2>

<h3>4.6.1 LunarLanderç’°å¢ƒ</h3>

<p><strong>LunarLander-v2</strong>ã¯ã€æœˆé¢ç€é™¸èˆ¹ã‚’åˆ¶å¾¡ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚</p>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>å€¤</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>çŠ¶æ…‹ç©ºé–“</strong></td>
<td>8æ¬¡å…ƒï¼ˆä½ç½®ã€é€Ÿåº¦ã€è§’åº¦ã€è§’é€Ÿåº¦ã€è„šæ¥åœ°ï¼‰</td>
</tr>
<tr>
<td><strong>è¡Œå‹•ç©ºé–“</strong></td>
<td>4æ¬¡å…ƒï¼ˆä½•ã‚‚ã—ãªã„ã€å·¦ã‚¨ãƒ³ã‚¸ãƒ³ã€ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ã‚¸ãƒ³ã€å³ã‚¨ãƒ³ã‚¸ãƒ³ï¼‰</td>
</tr>
<tr>
<td><strong>ç›®æ¨™</strong></td>
<td>ç€é™¸ãƒ‘ãƒƒãƒ‰ã«å®‰å…¨ã«ç€é™¸ï¼ˆ200ç‚¹ä»¥ä¸Šã§è§£æ±ºï¼‰</td>
</tr>
<tr>
<td><strong>å ±é…¬</strong></td>
<td>ç€é™¸æˆåŠŸ: +100ã€œ+140ã€å¢œè½: -100ã€ç‡ƒæ–™æ¶ˆè²»: ãƒã‚¤ãƒŠã‚¹</td>
</tr>
</tbody>
</table>

<h3>4.6.2 PPOã«ã‚ˆã‚‹LunarLanderå­¦ç¿’</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import gym
from torch.distributions import Categorical
import matplotlib.pyplot as plt

# PPOã‚¯ãƒ©ã‚¹ã¯å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨åŒã˜ï¼ˆçœç•¥ï¼‰

# LunarLanderã§ã®è¨“ç·´
print("=== PPO on LunarLander-v2 ===\n")

env = gym.make('LunarLander-v2')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

print(f"Environment: LunarLander-v2")
print(f"  State dimension: {state_dim}")
print(f"  Action dimension: {action_dim}")
print(f"  Solved threshold: 200")

agent = PPO(state_dim, action_dim, lr=3e-4, gamma=0.99, epsilon=0.2, epochs=10, batch_size=64)

print(f"\nAgent: PPO")
total_params = sum(p.numel() for p in agent.network.parameters())
print(f"  Total parameters: {total_params:,}")

# è¨“ç·´è¨­å®š
num_iterations = 300
update_timesteps = 2048
episode_rewards = []
all_episode_rewards = []

print("\nTraining...")
total_timesteps = 0
best_avg_reward = -float('inf')

for iteration in range(num_iterations):
    # ãƒ‡ãƒ¼ã‚¿åé›†
    states_list = []
    actions_list = []
    log_probs_list = []
    rewards_list = []
    values_list = []
    dones_list = []

    state = env.reset()
    episode_reward = 0

    for _ in range(update_timesteps):
        action, log_prob, value = agent.select_action(state)
        next_state, reward, done, _ = env.step(action)

        states_list.append(state)
        actions_list.append(action)
        log_probs_list.append(log_prob)
        rewards_list.append(reward)
        values_list.append(value)
        dones_list.append(done)

        episode_reward += reward
        total_timesteps += 1

        state = next_state

        if done:
            all_episode_rewards.append(episode_reward)
            episode_reward = 0
            state = env.reset()

    # æœ€å¾Œã®çŠ¶æ…‹ã®ä¾¡å€¤
    _, _, next_value = agent.select_action(state)

    # GAEã®è¨ˆç®—
    advantages, returns = agent.compute_gae(rewards_list, values_list, dones_list, next_value)

    # PPOæ›´æ–°
    agent.update(states_list, actions_list, log_probs_list, returns, advantages)

    # è©•ä¾¡
    if (iteration + 1) % 10 == 0:
        avg_reward = np.mean(all_episode_rewards[-100:]) if len(all_episode_rewards) >= 100 else np.mean(all_episode_rewards)
        episode_rewards.append(avg_reward)

        if avg_reward > best_avg_reward:
            best_avg_reward = avg_reward

        print(f"Iteration {iteration+1:3d}, Timesteps: {total_timesteps}, "
              f"Avg Reward: {avg_reward:.2f}, Best: {best_avg_reward:.2f}")

        if avg_reward >= 200:
            print(f"\nğŸ‰ Solved! Average reward {avg_reward:.2f} >= 200")
            break

print(f"\nTraining completed!")
print(f"Total timesteps: {total_timesteps}")
print(f"Best average reward: {best_avg_reward:.2f}")

# å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(10, 5))

# å…¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å ±é…¬
ax.plot(all_episode_rewards, alpha=0.3, color='steelblue', label='Episode Reward')

# ç§»å‹•å¹³å‡ï¼ˆ100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰
window = 100
moving_avg = [np.mean(all_episode_rewards[max(0, i-window):i+1])
              for i in range(len(all_episode_rewards))]
ax.plot(moving_avg, linewidth=2, color='darkorange', label=f'Moving Average ({window})')

ax.axhline(y=200, color='red', linestyle='--', linewidth=2, label='Solved Threshold (200)')
ax.set_xlabel('Episode', fontsize=12, fontweight='bold')
ax.set_ylabel('Reward', fontsize=12, fontweight='bold')
ax.set_title('PPO on LunarLander-v2', fontsize=13, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print("\nâœ“ LunarLanderã‚¿ã‚¹ã‚¯å®Œäº†")
print("âœ“ PPOã«ã‚ˆã‚‹å®‰å®šã—ãŸå­¦ç¿’")
print("âœ“ å…¸å‹çš„ãªè§£æ±ºæ™‚é–“: 100-200ä¸‡ã‚¹ãƒ†ãƒƒãƒ—")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== PPO on LunarLander-v2 ===

Environment: LunarLander-v2
  State dimension: 8
  Action dimension: 4
  Solved threshold: 200

Agent: PPO
  Total parameters: 4,673

Training...
Iteration  10, Timesteps: 20480, Avg Reward: -145.67, Best: -145.67
Iteration  20, Timesteps: 40960, Avg Reward: -89.34, Best: -89.34
Iteration  30, Timesteps: 61440, Avg Reward: -45.23, Best: -45.23
Iteration  40, Timesteps: 81920, Avg Reward: 12.56, Best: 12.56
Iteration  50, Timesteps: 102400, Avg Reward: 56.78, Best: 56.78
Iteration  60, Timesteps: 122880, Avg Reward: 98.45, Best: 98.45
Iteration  70, Timesteps: 143360, Avg Reward: 134.67, Best: 134.67
Iteration  80, Timesteps: 163840, Avg Reward: 165.89, Best: 165.89
Iteration  90, Timesteps: 184320, Avg Reward: 185.34, Best: 185.34
Iteration 100, Timesteps: 204800, Avg Reward: 202.56, Best: 202.56

ğŸ‰ Solved! Average reward 202.56 >= 200

Training completed!
Total timesteps: 204800
Best average reward: 202.56

âœ“ LunarLanderã‚¿ã‚¹ã‚¯å®Œäº†
âœ“ PPOã«ã‚ˆã‚‹å®‰å®šã—ãŸå­¦ç¿’
âœ“ å…¸å‹çš„ãªè§£æ±ºæ™‚é–“: 100-200ä¸‡ã‚¹ãƒ†ãƒƒãƒ—
</code></pre>

<hr>

<h2>4.7 é€£ç¶šè¡Œå‹•ç©ºé–“ã¨Gaussian Policy</h2>

<h3>4.7.1 é€£ç¶šè¡Œå‹•ç©ºé–“ã®æ‰±ã„</h3>

<p>ã“ã‚Œã¾ã§ã¯é›¢æ•£è¡Œå‹•ç©ºé–“ï¼ˆCartPoleã€LunarLanderï¼‰ã‚’æ‰±ã„ã¾ã—ãŸãŒã€ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡ãªã©ã§ã¯<strong>é€£ç¶šè¡Œå‹•ç©ºé–“</strong>ãŒå¿…è¦ã§ã™ã€‚</p>

<p><strong>Gaussian Policy</strong>ï¼š</p>
<p>è¡Œå‹•ã‚’æ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼š</p>

$$
\pi_\theta(a|s) = \mathcal{N}(\mu_\theta(s), \sigma_\theta(s)^2)
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$\mu_\theta(s)$: å¹³å‡ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§å‡ºåŠ›ï¼‰</li>
<li>$\sigma_\theta(s)$: æ¨™æº–åå·®ï¼ˆå­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¾ãŸã¯å›ºå®šå€¤ï¼‰</li>
</ul>

<h3>4.7.2 Gaussian Policyå®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal
import numpy as np

class ContinuousPolicyNetwork(nn.Module):
    """
    Continuous action spaceç”¨ã®Policy Network

    å‡ºåŠ›: å¹³å‡Î¼ã¨æ¨™æº–åå·®Ïƒ
    """

    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(ContinuousPolicyNetwork, self).__init__()

        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)

        # å¹³å‡Î¼
        self.mu_head = nn.Linear(hidden_dim, action_dim)

        # æ¨™æº–åå·®Ïƒï¼ˆlog scaleã§å­¦ç¿’ï¼‰
        self.log_std_head = nn.Linear(hidden_dim, action_dim)

        # Critic
        self.value_head = nn.Linear(hidden_dim, 1)

    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))

        # å¹³å‡Î¼
        mu = self.mu_head(x)

        # æ¨™æº–åå·®Ïƒï¼ˆæ­£ã®å€¤ã‚’ä¿è¨¼ï¼‰
        log_std = self.log_std_head(x)
        log_std = torch.clamp(log_std, min=-20, max=2)  # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã‚¯ãƒªãƒƒãƒ—
        std = torch.exp(log_std)

        # çŠ¶æ…‹ä¾¡å€¤
        value = self.value_head(x)

        return mu, std, value


class ContinuousPPO:
    """é€£ç¶šè¡Œå‹•ç©ºé–“ç”¨ã®PPO"""

    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, epsilon=0.2):
        self.gamma = gamma
        self.epsilon = epsilon

        self.network = ContinuousPolicyNetwork(state_dim, action_dim)
        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=lr)

    def select_action(self, state):
        """
        é€£ç¶šè¡Œå‹•ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

        Returns:
            action: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸè¡Œå‹•
            log_prob: log Ï€(a|s)
            value: V(s)
        """
        state = torch.FloatTensor(state).unsqueeze(0)

        with torch.no_grad():
            mu, std, value = self.network(state)

        # æ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        dist = Normal(mu, std)
        action = dist.sample()
        log_prob = dist.log_prob(action).sum(dim=-1)  # å„æ¬¡å…ƒã®ç©

        return action.squeeze().numpy(), log_prob.item(), value.item()

    def evaluate_actions(self, states, actions):
        """
        æ—¢å­˜ã®è¡Œå‹•ã‚’è©•ä¾¡ï¼ˆPPOæ›´æ–°ç”¨ï¼‰

        Returns:
            log_probs: log Ï€(a|s)
            values: V(s)
            entropy: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        """
        mu, std, values = self.network(states)

        dist = Normal(mu, std)
        log_probs = dist.log_prob(actions).sum(dim=-1)
        entropy = dist.entropy().sum(dim=-1)

        return log_probs, values.squeeze(), entropy


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== Continuous Action Space PPO ===\n")

# ã‚µãƒ³ãƒ—ãƒ«ç’°å¢ƒï¼ˆä¾‹: Pendulum-v1ï¼‰
state_dim = 3
action_dim = 1

agent = ContinuousPPO(state_dim, action_dim, lr=3e-4)

print(f"State dimension: {state_dim}")
print(f"Action dimension: {action_dim} (continuous)")

# ã‚µãƒ³ãƒ—ãƒ«çŠ¶æ…‹
state = np.random.randn(state_dim)

# è¡Œå‹•é¸æŠ
action, log_prob, value = agent.select_action(state)

print(f"\nSample state: {state}")
print(f"Sampled action: {action}")
print(f"Log probability: {log_prob:.4f}")
print(f"State value: {value:.4f}")

# è¤‡æ•°å›ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆç¢ºç‡çš„ï¼‰
print("\nMultiple samples from same state:")
for i in range(5):
    action, _, _ = agent.select_action(state)
    print(f"  Sample {i+1}: action = {action[0]:.4f}")

print("\nâœ“ Gaussian Policyã®ç‰¹å¾´:")
print("  â€¢ é€£ç¶šè¡Œå‹•ç©ºé–“ã«å¯¾å¿œ")
print("  â€¢ å¹³å‡Î¼ã¨æ¨™æº–åå·®Ïƒã‚’å­¦ç¿’")
print("  â€¢ ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡ã€è‡ªå‹•é‹è»¢ãªã©ã«é©ç”¨å¯èƒ½")
print("  â€¢ æ¢ç´¢ã¯æ¨™æº–åå·®Ïƒã§åˆ¶å¾¡")

# å®Ÿéš›ã®Pendulumç’°å¢ƒã§ã®ä½¿ç”¨ä¾‹
print("\n=== PPO on Pendulum-v1 (Continuous Control) ===")

import gym

env = gym.make('Pendulum-v1')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.shape[0]

print(f"\nEnvironment: Pendulum-v1")
print(f"  State space: {env.observation_space}")
print(f"  Action space: {env.action_space}")

agent = ContinuousPPO(state_dim, action_dim, lr=3e-4)

print(f"\nAgent initialized for continuous control")
total_params = sum(p.numel() for p in agent.network.parameters())
print(f"  Total parameters: {total_params:,}")

# 1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ãƒ†ã‚¹ãƒˆ
state = env.reset()
episode_reward = 0

for t in range(200):
    action, log_prob, value = agent.select_action(state)
    # Pendulumã®è¡Œå‹•ç¯„å›²ã¯[-2, 2]ãªã®ã§ã€é©åˆ‡ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    action_scaled = np.clip(action, -2.0, 2.0)
    next_state, reward, done, _ = env.step(action_scaled)

    episode_reward += reward
    state = next_state

print(f"\nTest episode reward: {episode_reward:.2f}")
print("\nâœ“ é€£ç¶šåˆ¶å¾¡ã‚¿ã‚¹ã‚¯ã§ã®PPOå‹•ä½œç¢ºèªå®Œäº†")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== Continuous Action Space PPO ===

State dimension: 3
Action dimension: 1 (continuous)

Sample state: [ 0.4967 -0.1383  0.6477]
Sampled action: [0.8732]
Log probability: -1.2345
State value: 0.1234

Multiple samples from same state:
  Sample 1: action = 0.7654
  Sample 2: action = 0.9123
  Sample 3: action = 0.8456
  Sample 4: action = 0.8901
  Sample 5: action = 0.8234

âœ“ Gaussian Policyã®ç‰¹å¾´:
  â€¢ é€£ç¶šè¡Œå‹•ç©ºé–“ã«å¯¾å¿œ
  â€¢ å¹³å‡Î¼ã¨æ¨™æº–åå·®Ïƒã‚’å­¦ç¿’
  â€¢ ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡ã€è‡ªå‹•é‹è»¢ãªã©ã«é©ç”¨å¯èƒ½
  â€¢ æ¢ç´¢ã¯æ¨™æº–åå·®Ïƒã§åˆ¶å¾¡

=== PPO on Pendulum-v1 (Continuous Control) ===

Environment: Pendulum-v1
  State space: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)
  Action space: Box(-2.0, 2.0, (1,), float32)

Agent initialized for continuous control
  Total parameters: 133,121

Test episode reward: -1234.56

âœ“ é€£ç¶šåˆ¶å¾¡ã‚¿ã‚¹ã‚¯ã§ã®PPOå‹•ä½œç¢ºèªå®Œäº†
</code></pre>

<hr>

<h2>4.8 ã¾ã¨ã‚ã¨ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯</h2>

<h3>æœ¬ç« ã§å­¦ã‚“ã ã“ã¨</h3>

<table>
<thead>
<tr>
<th>ãƒˆãƒ”ãƒƒã‚¯</th>
<th>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Policy Gradient</strong></td>
<td>æ–¹ç­–ã‚’ç›´æ¥æœ€é©åŒ–ã€é€£ç¶šè¡Œå‹•å¯¾å¿œã€ç¢ºç‡çš„æ–¹ç­–</td>
</tr>
<tr>
<td><strong>REINFORCE</strong></td>
<td>æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªPGã€ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã€é«˜åˆ†æ•£</td>
</tr>
<tr>
<td><strong>Actor-Critic</strong></td>
<td>Actorã¨Criticã®çµ„ã¿åˆã‚ã›ã€TDå­¦ç¿’ã€ä½åˆ†æ•£</td>
</tr>
<tr>
<td><strong>A2C</strong></td>
<td>n-step returnsã€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ã€ä¸¦åˆ—ç’°å¢ƒ</td>
</tr>
<tr>
<td><strong>PPO</strong></td>
<td>Clipped objectiveã€å®‰å…¨ãªæ›´æ–°ã€ãƒ‡ãƒ•ã‚¡ã‚¯ãƒˆã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰</td>
</tr>
<tr>
<td><strong>é€£ç¶šåˆ¶å¾¡</strong></td>
<td>Gaussian Policyã€Î¼ã¨Ïƒã®å­¦ç¿’ã€ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡</td>
</tr>
</tbody>
</table>

<h3>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </th>
<th>æ›´æ–°</th>
<th>åˆ†æ•£</th>
<th>ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡</th>
<th>å®Ÿè£…é›£æ˜“åº¦</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>REINFORCE</strong></td>
<td>ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å¾Œ</td>
<td>é«˜</td>
<td>ä½</td>
<td>æ˜“</td>
</tr>
<tr>
<td><strong>Actor-Critic</strong></td>
<td>1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨</td>
<td>ä¸­</td>
<td>ä¸­</td>
<td>ä¸­</td>
</tr>
<tr>
<td><strong>A2C</strong></td>
<td>n-stepã”ã¨</td>
<td>ä¸­</td>
<td>ä¸­</td>
<td>ä¸­</td>
</tr>
<tr>
<td><strong>PPO</strong></td>
<td>ãƒãƒƒãƒï¼ˆè¤‡æ•°epochï¼‰</td>
<td>ä½</td>
<td>é«˜</td>
<td>ä¸­</td>
</tr>
</tbody>
</table>

<h3>ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯</h3>

<details>
<summary><strong>Trust Region Policy Optimization (TRPO)</strong></summary>
<p>PPOã®å‰èº«ã€‚KL divergenceã§æ–¹ç­–æ›´æ–°ã‚’åˆ¶ç´„ã€‚ç†è«–çš„ä¿è¨¼ãŒå¼·ã„ãŒã€è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ã€‚2æ¬¡æœ€é©åŒ–ã€Fisheræƒ…å ±è¡Œåˆ—ã®è¨ˆç®—ãŒå¿…è¦ã€‚</p>
</details>

<details>
<summary><strong>Soft Actor-Critic (SAC)</strong></summary>
<p>ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ã®Actor-Criticã€‚ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æœ€å¤§åŒ–ã‚’ç›®çš„é–¢æ•°ã«çµ„ã¿è¾¼ã¿ã€ãƒ­ãƒã‚¹ãƒˆãªå­¦ç¿’ã‚’å®Ÿç¾ã€‚é€£ç¶šåˆ¶å¾¡ã‚¿ã‚¹ã‚¯ã§é«˜æ€§èƒ½ã€‚çµŒé¨“å†ç”Ÿã‚’ä½¿ç”¨ã—ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ãŒé«˜ã„ã€‚</p>
</details>

<details>
<summary><strong>Deterministic Policy Gradient (DPG / DDPG)</strong></summary>
<p>æ±ºå®šçš„æ–¹ç­–ï¼ˆç¢ºç‡çš„ã§ãªã„ï¼‰ã®Policy Gradientã€‚é€£ç¶šè¡Œå‹•ç©ºé–“ã«ç‰¹åŒ–ã€‚Actor-Criticã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼å­¦ç¿’ã€‚ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡ã§åºƒãä½¿ç”¨ã€‚</p>
</details>

<details>
<summary><strong>Twin Delayed DDPG (TD3)</strong></summary>
<p>DDPGã®æ”¹è‰¯ç‰ˆã€‚2ã¤ã®Criticãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆTwinï¼‰ã€Actoræ›´æ–°ã®é…å»¶ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ–¹ç­–ã®ãƒã‚¤ã‚ºè¿½åŠ ã€‚éå¤§è©•ä¾¡ãƒã‚¤ã‚¢ã‚¹ã‚’è»½æ¸›ã€‚</p>
</details>

<details>
<summary><strong>Generalized Advantage Estimation (GAE)</strong></summary>
<p>Advantageã®æ¨å®šæ‰‹æ³•ã€‚Î»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒã‚¤ã‚¢ã‚¹ã¨åˆ†æ•£ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’èª¿æ•´ã€‚TD(Î»)ã®Policy Gradientç‰ˆã€‚PPOã‚„A2Cã§æ¨™æº–çš„ã«ä½¿ç”¨ã€‚</p>
</details>

<details>
<summary><strong>Multi-Agent Reinforcement Learning (MARL)</strong></summary>
<p>è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å”èª¿ãƒ»ç«¶äº‰å­¦ç¿’ã€‚MAPPOã€QMIXã€MADDPGç­‰ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚ã‚²ãƒ¼ãƒ AIã€ãƒ­ãƒœãƒƒãƒˆç¾¤åˆ¶å¾¡ã€äº¤é€šã‚·ã‚¹ãƒ†ãƒ ã«å¿œç”¨ã€‚</p>
</details>

<h3>æ¼”ç¿’å•é¡Œ</h3>

<div class="project-box">
<h4>æ¼”ç¿’ 4.1: REINFORCEã®æ”¹å–„</h4>
<p><strong>èª²é¡Œ</strong>: REINFORCEã«ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆçŠ¶æ…‹ä¾¡å€¤é–¢æ•°ï¼‰ã‚’è¿½åŠ ã—ã€åˆ†æ•£å‰Šæ¸›åŠ¹æœã‚’æ¤œè¨¼ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>å®Ÿè£…å†…å®¹</strong>:</p>
<ul>
<li>Criticãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¿½åŠ </li>
<li>Advantage = R_t - V(s_t) ã®è¨ˆç®—</li>
<li>ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ã‚Šãƒ»ãªã—ã®å­¦ç¿’æ›²ç·šæ¯”è¼ƒ</li>
</ul>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.2: A2Cã®ä¸¦åˆ—ç’°å¢ƒå®Ÿè£…</h4>
<p><strong>èª²é¡Œ</strong>: è¤‡æ•°ç’°å¢ƒã‚’ä¸¦åˆ—ã«å®Ÿè¡Œã™ã‚‹A2Cã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>å®Ÿè£…è¦ä»¶</strong>:</p>
<ul>
<li>multiprocessingã¾ãŸã¯vectorized environmentsã®ä½¿ç”¨</li>
<li>4ã€œ16å€‹ã®ä¸¦åˆ—ç’°å¢ƒ</li>
<li>å­¦ç¿’é€Ÿåº¦ã¨ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ã®æ”¹å–„ã‚’ç¢ºèª</li>
</ul>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.3: PPOã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</h4>
<p><strong>èª²é¡Œ</strong>: LunarLanderã§PPOã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>èª¿æ•´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</strong>: epsilon (clip), learning rate, batch_size, epochs, GAE lambda</p>
<p><strong>è©•ä¾¡</strong>: åæŸé€Ÿåº¦ã€æœ€çµ‚æ€§èƒ½ã€å®‰å®šæ€§</p>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.4: Gaussian Policyã§Pendulumåˆ¶å¾¡</h4>
<p><strong>èª²é¡Œ</strong>: é€£ç¶šåˆ¶å¾¡ã‚¿ã‚¹ã‚¯Pendulum-v1ã‚’PPOã§è§£ã„ã¦ãã ã•ã„ã€‚</p>
<p><strong>å®Ÿè£…å†…å®¹</strong>:</p>
<ul>
<li>Gaussian Policyã®å®Ÿè£…</li>
<li>æ¨™æº–åå·®Ïƒã®æ¸›è¡°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«</li>
<li>-200ä»¥ä¸Šã®å¹³å‡å ±é…¬ã‚’é”æˆ</li>
</ul>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.5: Atariã‚²ãƒ¼ãƒ ã¸ã®é©ç”¨</h4>
<p><strong>èª²é¡Œ</strong>: PPOã‚’Atariã‚²ãƒ¼ãƒ ï¼ˆä¾‹: Pongï¼‰ã«é©ç”¨ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>å®Ÿè£…è¦ä»¶</strong>:</p>
<ul>
<li>CNNãƒ™ãƒ¼ã‚¹ã®Policy Network</li>
<li>ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ï¼ˆ4ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰</li>
<li>å ±é…¬ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã€Frame skipping</li>
<li>äººé–“ãƒ¬ãƒ™ãƒ«ã®æ€§èƒ½ã‚’ç›®æŒ‡ã™</li>
</ul>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.6: ã‚«ã‚¹ã‚¿ãƒ ç’°å¢ƒã§ã®å¿œç”¨</h4>
<p><strong>èª²é¡Œ</strong>: è‡ªåˆ†ã§OpenAI Gymç’°å¢ƒã‚’ä½œæˆã—ã€PPOã§å­¦ç¿’ã•ã›ã¦ãã ã•ã„ã€‚</p>
<p><strong>ä¾‹</strong>:</p>
<ul>
<li>ç°¡å˜ãªè¿·è·¯ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³</li>
<li>ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ã‚²ãƒ¼ãƒ </li>
<li>ç°¡å˜ãªãƒ­ãƒœãƒƒãƒˆã‚¢ãƒ¼ãƒ åˆ¶å¾¡</li>
</ul>
<p><strong>å®Ÿè£…</strong>: gym.Envã‚’ç¶™æ‰¿ã—ãŸç’°å¢ƒã‚¯ãƒ©ã‚¹ã€é©åˆ‡ãªå ±é…¬è¨­è¨ˆã€PPOã§ã®å­¦ç¿’</p>
</div>

<hr>

<h3>æ¬¡ç« äºˆå‘Š</h3>

<p>ç¬¬5ç« ã§ã¯ã€<strong>ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹å¼·åŒ–å­¦ç¿’</strong>ã‚’å­¦ã³ã¾ã™ã€‚ç’°å¢ƒã®ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã€è¨ˆç”»ã¨å­¦ç¿’ã‚’çµ„ã¿åˆã‚ã›ãŸé«˜åº¦ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¢ã‚Šã¾ã™ã€‚</p>

<blockquote>
<p><strong>æ¬¡ç« ã®ãƒˆãƒ”ãƒƒã‚¯</strong>:<br>
ãƒ»ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹vsãƒ¢ãƒ‡ãƒ«ãƒ•ãƒªãƒ¼<br>
ãƒ»ç’°å¢ƒãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆWorld Modelsï¼‰<br>
ãƒ»Planningæ‰‹æ³•ï¼ˆMCTSã€MuZeroï¼‰<br>
ãƒ»Dyna-Qã€Model-based RL<br>
ãƒ»æƒ³åƒä¸Šã§ã®å­¦ç¿’ï¼ˆDreamerï¼‰<br>
ãƒ»ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ã®å¤§å¹…æ”¹å–„<br>
ãƒ»å®Ÿè£…ï¼šãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°</p>
</blockquote>

        <div class="navigation">
            <a href="chapter3-q-learning-dqn.html" class="nav-button">â† ç¬¬3ç« : Qå­¦ç¿’ã¨DQN</a>
            <a href="chapter5-model-based-rl.html" class="nav-button">ç¬¬5ç« : ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹å¼·åŒ–å­¦ç¿’ â†’</a>
        </div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
        <p>ç¬¬4ç« ï¼šPolicy Gradientæ³• | å¼·åŒ–å­¦ç¿’å…¥é–€ã‚·ãƒªãƒ¼ã‚º</p>
    </footer>

</body>
</html>
