---
title: ç¬¬4ç« ï¼šPolicy Gradientæ³•
chapter_title: ç¬¬4ç« ï¼šPolicy Gradientæ³•
subtitle: æ–¹ç­–ãƒ™ãƒ¼ã‚¹å¼·åŒ–å­¦ç¿’ï¼šREINFORCEã€Actor-Criticã€A2Cã€PPOã®ç†è«–ã¨å®Ÿè£…
reading_time: 28åˆ†
difficulty: ä¸­ç´šã€œä¸Šç´š
code_examples: 10
exercises: 6
---

## å­¦ç¿’ç›®æ¨™

ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š

  * âœ… Policy-basedã¨Value-basedã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®é•ã„ã‚’ç†è§£ã§ãã‚‹
  * âœ… Policy Gradientã®æ•°å­¦çš„å®šå¼åŒ–ã‚’ç†è§£ã§ãã‚‹
  * âœ… REINFORCEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿè£…ã§ãã‚‹
  * âœ… Actor-Criticã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç†è§£ã—å®Ÿè£…ã§ãã‚‹
  * âœ… Advantage Actor-Critic (A2C)ã‚’å®Ÿè£…ã§ãã‚‹
  * âœ… Proximal Policy Optimization (PPO)ã‚’ç†è§£ã§ãã‚‹
  * âœ… LunarLanderãªã©ã®é€£ç¶šåˆ¶å¾¡ã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã§ãã‚‹

* * *

## 4.1 Policy-based vs Value-based

### 4.1.1 2ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

å¼·åŒ–å­¦ç¿’ã«ã¯å¤§ããåˆ†ã‘ã¦2ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒã‚ã‚Šã¾ã™ï¼š

ç‰¹æ€§ | Value-based (ä¾¡å€¤ãƒ™ãƒ¼ã‚¹) | Policy-based (æ–¹ç­–ãƒ™ãƒ¼ã‚¹)  
---|---|---  
**å­¦ç¿’å¯¾è±¡** | ä¾¡å€¤é–¢æ•° $Q(s, a)$ ã¾ãŸã¯ $V(s)$ | æ–¹ç­– $\pi(a|s)$ ã‚’ç›´æ¥å­¦ç¿’  
**è¡Œå‹•é¸æŠ** | é–“æ¥çš„ï¼ˆ$\arg\max_a Q(s,a)$ï¼‰ | ç›´æ¥çš„ï¼ˆ$\pi(a|s)$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰  
**è¡Œå‹•ç©ºé–“** | é›¢æ•£çš„ãªè¡Œå‹•ã«é©ã™ã‚‹ | é€£ç¶šçš„ãªè¡Œå‹•ã«ã‚‚å¯¾å¿œå¯  
**ç¢ºç‡çš„æ–¹ç­–** | æ‰±ã„ã«ãã„ï¼ˆÎµ-greedyç­‰ã§å¯¾å¿œï¼‰ | è‡ªç„¶ã«æ‰±ãˆã‚‹  
**åæŸæ€§** | æœ€é©æ–¹ç­–ã®ä¿è¨¼ã‚ã‚Šï¼ˆæ¡ä»¶ä¸‹ï¼‰ | å±€æ‰€æœ€é©è§£ã®å¯èƒ½æ€§  
**ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡** | é«˜ã„ï¼ˆçµŒé¨“å†ç”Ÿå¯èƒ½ï¼‰ | ä½ã„ï¼ˆon-policyå­¦ç¿’ï¼‰  
**ä»£è¡¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ** | Q-learning, DQN, Double DQN | REINFORCE, A2C, PPO, TRPO  
  
### 4.1.2 Policy Gradientã®å‹•æ©Ÿ

**Value-basedã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®èª²é¡Œ** ï¼š

  * **é€£ç¶šè¡Œå‹•ç©ºé–“** : ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡ãªã©ã€ç„¡é™ã®è¡Œå‹•é¸æŠè‚¢ãŒã‚ã‚‹å ´åˆã«$\arg\max$è¨ˆç®—ãŒå›°é›£
  * **ç¢ºç‡çš„æ–¹ç­–** : ã˜ã‚ƒã‚“ã‘ã‚“ã®ã‚ˆã†ã«ç¢ºç‡çš„ãªè¡Œå‹•ãŒæœ€é©ãªå ´åˆã«å¯¾å¿œã—ã¥ã‚‰ã„
  * **é«˜æ¬¡å…ƒè¡Œå‹•ç©ºé–“** : è¡Œå‹•ã®çµ„ã¿åˆã‚ã›ãŒè†¨å¤§ãªå ´åˆã€ã™ã¹ã¦ã®è¡Œå‹•ä¾¡å€¤ã‚’è¨ˆç®—ã™ã‚‹ã®ã¯éåŠ¹ç‡

**Policy-basedã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®è§£æ±ºç­–** ï¼š

  * æ–¹ç­–ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã§ãƒ¢ãƒ‡ãƒ«åŒ–: $\pi_\theta(a|s)$
  * æœŸå¾…ãƒªã‚¿ãƒ¼ãƒ³ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ã« $\theta$ ã‚’ç›´æ¥æœ€é©åŒ–
  * ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§æ–¹ç­–ã‚’è¡¨ç¾å¯èƒ½

    
    
    ```mermaid
    graph LR
        subgraph "Value-based Approach"
            S1["State s"]
            Q["Q-functionQ(s,a)"]
            AM["argmax"]
            A1["Action a"]
    
            S1 --> Q
            Q --> AM
            AM --> A1
    
            style Q fill:#e74c3c,color:#fff
        end
    
        subgraph "Policy-based Approach"
            S2["State s"]
            P["Policy Ï€(a|s)parameterized by Î¸"]
            A2["Action a(sampled)"]
    
            S2 --> P
            P --> A2
    
            style P fill:#27ae60,color:#fff
        end
    ```

### 4.1.3 Policy Gradientã®å®šå¼åŒ–

æ–¹ç­– $\pi_\theta(a|s)$ ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã§è¡¨ç¾ã—ã€æœŸå¾…ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆç›®çš„é–¢æ•°ï¼‰$J(\theta)$ ã‚’æœ€å¤§åŒ–ã—ã¾ã™ï¼š

$$ J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)] $$ 

ã“ã“ã§ï¼š

  * $\tau = (s_0, a_0, r_1, s_1, a_1, r_2, \ldots)$: è»Œè·¡ï¼ˆtrajectoryï¼‰
  * $R(\tau) = \sum_{t=0}^{T} \gamma^t r_t$: è»Œè·¡ã®ç´¯ç©å ±é…¬

**Policy Gradientå®šç†** ã«ã‚ˆã‚Šã€$J(\theta)$ ã®å‹¾é…ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¡¨ã›ã¾ã™ï¼š

$$ \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) R_t \right] $$ 

ã“ã“ã§ $R_t = \sum_{t'=t}^{T} \gamma^{t'-t} r_{t'}$ ã¯æ™‚åˆ» $t$ ã‹ã‚‰ã®ç´¯ç©å ±é…¬ã§ã™ã€‚

> **ç›´æ„Ÿçš„ç†è§£** : é«˜ã„ãƒªã‚¿ãƒ¼ãƒ³ã‚’ã‚‚ãŸã‚‰ã—ãŸè¡Œå‹•ã®ç¢ºç‡ã‚’å¢—ã‚„ã—ã€ä½ã„ãƒªã‚¿ãƒ¼ãƒ³ã®è¡Œå‹•ã®ç¢ºç‡ã‚’æ¸›ã‚‰ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è‰¯ã„è»Œè·¡ã‚’ç”Ÿæˆã™ã‚‹æ–¹ç­–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¸ã¨æ›´æ–°ã•ã‚Œã‚‹ã€‚

* * *

## 4.2 REINFORCEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

### 4.2.1 REINFORCEã®åŸºæœ¬åŸç†

**REINFORCE** ï¼ˆWilliams, 1992ï¼‰ã¯æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªPolicy Gradientã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã§ãƒªã‚¿ãƒ¼ãƒ³ã‚’æ¨å®šã—ã¾ã™ã€‚

#### ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

  1. æ–¹ç­– $\pi_\theta(a|s)$ ã§1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã€è»Œè·¡ $\tau$ ã‚’åé›†
  2. å„æ™‚åˆ» $t$ ã®ãƒªã‚¿ãƒ¼ãƒ³ $R_t$ ã‚’è¨ˆç®—
  3. å‹¾é…ä¸Šæ˜‡ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ï¼š $$ \theta \leftarrow \theta + \alpha \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) R_t $$ 

#### ãƒãƒªã‚¢ãƒ³ã‚¹å‰Šæ¸›ï¼šBaseline

ãƒªã‚¿ãƒ¼ãƒ³ã‹ã‚‰å®šæ•° $b$ ã‚’å¼•ã„ã¦ã‚‚å‹¾é…ã®æœŸå¾…å€¤ã¯å¤‰ã‚ã‚Šã¾ã›ã‚“ï¼ˆä¸åæ€§ï¼‰ã€‚ã“ã‚Œã«ã‚ˆã‚Šåˆ†æ•£ã‚’å‰Šæ¸›ã§ãã¾ã™ï¼š

$$ \nabla_\theta J(\theta) = \mathbb{E}_{\tau} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) (R_t - b) \right] $$ 

ä¸€èˆ¬çš„ãªé¸æŠï¼š**$b = V(s_t)$** ï¼ˆçŠ¶æ…‹ä¾¡å€¤é–¢æ•°ï¼‰
    
    
    ```mermaid
    graph TB
        subgraph "REINFORCE Algorithm"
            Init["Initialize Î¸"]
            Episode["Run EpisodeSample Ï„ ~ Ï€_Î¸"]
            Compute["Compute ReturnsR_t for all t"]
            Grad["Compute Gradientâˆ‡_Î¸ log Ï€_Î¸(a_t|s_t) R_t"]
            Update["Update ParametersÎ¸ â† Î¸ + Î± âˆ‡_Î¸ J(Î¸)"]
            Check["Converged?"]
    
            Init --> Episode
            Episode --> Compute
            Compute --> Grad
            Grad --> Update
            Update --> Check
            Check -->|No| Episode
            Check -->|Yes| Done["Done"]
    
            style Init fill:#7b2cbf,color:#fff
            style Done fill:#27ae60,color:#fff
            style Grad fill:#e74c3c,color:#fff
        end
    ```

### 4.2.2 REINFORCEå®Ÿè£…ï¼ˆCartPoleï¼‰
    
    
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    import numpy as np
    import gym
    import matplotlib.pyplot as plt
    from collections import deque
    
    class PolicyNetwork(nn.Module):
        """
        Policy Network for REINFORCE
    
        å…¥åŠ›: çŠ¶æ…‹ s
        å‡ºåŠ›: å„è¡Œå‹•ã®ç¢ºç‡ Ï€(a|s)
        """
    
        def __init__(self, state_dim, action_dim, hidden_dim=128):
            super(PolicyNetwork, self).__init__()
            self.fc1 = nn.Linear(state_dim, hidden_dim)
            self.fc2 = nn.Linear(hidden_dim, hidden_dim)
            self.fc3 = nn.Linear(hidden_dim, action_dim)
    
        def forward(self, state):
            """
            Args:
                state: çŠ¶æ…‹ [batch_size, state_dim]
    
            Returns:
                action_probs: è¡Œå‹•ç¢ºç‡ [batch_size, action_dim]
            """
            x = F.relu(self.fc1(state))
            x = F.relu(self.fc2(x))
            logits = self.fc3(x)
            action_probs = F.softmax(logits, dim=-1)
            return action_probs
    
    
    class REINFORCE:
        """REINFORCE Algorithm"""
    
        def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):
            """
            Args:
                state_dim: çŠ¶æ…‹ç©ºé–“ã®æ¬¡å…ƒ
                action_dim: è¡Œå‹•ç©ºé–“ã®æ¬¡å…ƒ
                lr: å­¦ç¿’ç‡
                gamma: å‰²å¼•ç‡
            """
            self.gamma = gamma
            self.policy = PolicyNetwork(state_dim, action_dim)
            self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)
    
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
            self.saved_log_probs = []
            self.rewards = []
    
        def select_action(self, state):
            """
            æ–¹ç­–ã«å¾“ã£ã¦è¡Œå‹•ã‚’é¸æŠ
    
            Args:
                state: çŠ¶æ…‹
    
            Returns:
                action: é¸æŠã•ã‚ŒãŸè¡Œå‹•
            """
            state = torch.FloatTensor(state).unsqueeze(0)
            action_probs = self.policy(state)
    
            # ç¢ºç‡åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            m = torch.distributions.Categorical(action_probs)
            action = m.sample()
    
            # log Ï€(a|s) ã‚’ä¿å­˜ï¼ˆå‹¾é…è¨ˆç®—ã«ä½¿ç”¨ï¼‰
            self.saved_log_probs.append(m.log_prob(action))
    
            return action.item()
    
        def update(self):
            """
            ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å¾Œã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°
            """
            R = 0
            returns = []
    
            # ãƒªã‚¿ãƒ¼ãƒ³ã®è¨ˆç®—ï¼ˆé€†é †ã«è¨ˆç®—ï¼‰
            for r in self.rewards[::-1]:
                R = r + self.gamma * R
                returns.insert(0, R)
    
            returns = torch.tensor(returns)
    
            # æ­£è¦åŒ–ï¼ˆåˆ†æ•£å‰Šæ¸›ï¼‰
            if len(returns) > 1:
                returns = (returns - returns.mean()) / (returns.std() + 1e-8)
    
            # Policy gradientè¨ˆç®—
            policy_loss = []
            for log_prob, R in zip(self.saved_log_probs, returns):
                policy_loss.append(-log_prob * R)
    
            # å‹¾é…ä¸Šæ˜‡ï¼ˆæå¤±ã‚’æœ€å°åŒ– = -ç›®çš„é–¢æ•°ã‚’æœ€å°åŒ–ï¼‰
            self.optimizer.zero_grad()
            policy_loss = torch.cat(policy_loss).sum()
            policy_loss.backward()
            self.optimizer.step()
    
            # ãƒªã‚»ãƒƒãƒˆ
            self.saved_log_probs = []
            self.rewards = []
    
            return policy_loss.item()
    
    
    # ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    print("=== REINFORCE on CartPole ===\n")
    
    env = gym.make('CartPole-v1')
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    
    print(f"Environment: CartPole-v1")
    print(f"  State dimension: {state_dim}")
    print(f"  Action dimension: {action_dim}")
    
    agent = REINFORCE(state_dim, action_dim, lr=0.01, gamma=0.99)
    
    print(f"\nAgent: REINFORCE")
    total_params = sum(p.numel() for p in agent.policy.parameters())
    print(f"  Total parameters: {total_params:,}")
    
    # è¨“ç·´
    num_episodes = 500
    episode_rewards = []
    moving_avg = deque(maxlen=100)
    
    print("\nTraining...")
    for episode in range(num_episodes):
        state = env.reset()
        episode_reward = 0
    
        for t in range(1000):
            action = agent.select_action(state)
            next_state, reward, done, _ = env.step(action)
    
            agent.rewards.append(reward)
            episode_reward += reward
    
            state = next_state
    
            if done:
                break
    
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å¾Œã«æ›´æ–°
        loss = agent.update()
    
        episode_rewards.append(episode_reward)
        moving_avg.append(episode_reward)
    
        if (episode + 1) % 50 == 0:
            avg_reward = np.mean(moving_avg)
            print(f"Episode {episode+1:3d}, Avg Reward (last 100): {avg_reward:.2f}, Loss: {loss:.4f}")
    
    print(f"\nTraining completed!")
    print(f"Final average reward (last 100 episodes): {np.mean(moving_avg):.2f}")
    
    # å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.plot(episode_rewards, alpha=0.3, color='steelblue', label='Episode Reward')
    
    # ç§»å‹•å¹³å‡
    window = 50
    moving_avg_plot = [np.mean(episode_rewards[max(0, i-window):i+1])
                       for i in range(len(episode_rewards))]
    ax.plot(moving_avg_plot, linewidth=2, color='darkorange', label=f'Moving Average ({window} episodes)')
    
    ax.axhline(y=195, color='red', linestyle='--', label='Solved Threshold (195)')
    ax.set_xlabel('Episode', fontsize=12, fontweight='bold')
    ax.set_ylabel('Reward', fontsize=12, fontweight='bold')
    ax.set_title('REINFORCE on CartPole-v1', fontsize=13, fontweight='bold')
    ax.legend()
    ax.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    print("\nâœ“ REINFORCEã®ç‰¹å¾´:")
    print("  â€¢ ã‚·ãƒ³ãƒ—ãƒ«ã§å®Ÿè£…ãŒå®¹æ˜“")
    print("  â€¢ ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å¾Œã«æ›´æ–°ï¼‰")
    print("  â€¢ é«˜åˆ†æ•£ï¼ˆãƒªã‚¿ãƒ¼ãƒ³ã®å¤‰å‹•ãŒå¤§ãã„ï¼‰")
    print("  â€¢ On-policyï¼ˆç¾åœ¨ã®æ–¹ç­–ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰")
    

**å‡ºåŠ›ä¾‹** ï¼š
    
    
    === REINFORCE on CartPole ===
    
    Environment: CartPole-v1
      State dimension: 4
      Action dimension: 2
    
    Agent: REINFORCE
      Total parameters: 16,642
    
    Training...
    Episode  50, Avg Reward (last 100): 23.45, Loss: 15.2341
    Episode 100, Avg Reward (last 100): 45.67, Loss: 12.5678
    Episode 150, Avg Reward (last 100): 89.23, Loss: 8.3456
    Episode 200, Avg Reward (last 100): 142.56, Loss: 5.6789
    Episode 250, Avg Reward (last 100): 178.34, Loss: 3.4567
    Episode 300, Avg Reward (last 100): 195.78, Loss: 2.1234
    Episode 350, Avg Reward (last 100): 210.45, Loss: 1.5678
    Episode 400, Avg Reward (last 100): 230.67, Loss: 1.2345
    Episode 450, Avg Reward (last 100): 245.89, Loss: 0.9876
    Episode 500, Avg Reward (last 100): 260.34, Loss: 0.7654
    
    Training completed!
    Final average reward (last 100 episodes): 260.34
    
    âœ“ REINFORCEã®ç‰¹å¾´:
      â€¢ ã‚·ãƒ³ãƒ—ãƒ«ã§å®Ÿè£…ãŒå®¹æ˜“
      â€¢ ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å¾Œã«æ›´æ–°ï¼‰
      â€¢ é«˜åˆ†æ•£ï¼ˆãƒªã‚¿ãƒ¼ãƒ³ã®å¤‰å‹•ãŒå¤§ãã„ï¼‰
      â€¢ On-policyï¼ˆç¾åœ¨ã®æ–¹ç­–ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
    

### 4.2.3 REINFORCEã®èª²é¡Œ

REINFORCEã«ã¯ä»¥ä¸‹ã®èª²é¡ŒãŒã‚ã‚Šã¾ã™ï¼š

  * **é«˜åˆ†æ•£** : ãƒªã‚¿ãƒ¼ãƒ³ $R_t$ ã®åˆ†æ•£ãŒå¤§ããã€å­¦ç¿’ãŒä¸å®‰å®š
  * **ã‚µãƒ³ãƒ—ãƒ«éåŠ¹ç‡** : ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ã¾ã§æ›´æ–°ã§ããªã„
  * **ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•** : é•·ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§ã¯å­¦ç¿’ãŒé…ã„

**è§£æ±ºç­–** : **Actor-Critic** ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ãƒªã‚¿ãƒ¼ãƒ³ã‚’ä¾¡å€¤é–¢æ•°ã§æ¨å®š

* * *

## 4.3 Actor-Criticæ³•

### 4.3.1 Actor-Criticã®åŸç†

**Actor-Critic** ã¯ã€Policy Gradientï¼ˆActorï¼‰ã¨Value-basedï¼ˆCriticï¼‰ã‚’çµ„ã¿åˆã‚ã›ãŸæ‰‹æ³•ã§ã™ã€‚

ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | å½¹å‰² | å‡ºåŠ›  
---|---|---  
**Actor** | æ–¹ç­– $\pi_\theta(a|s)$ ã‚’å­¦ç¿’ | è¡Œå‹•ç¢ºç‡åˆ†å¸ƒ  
**Critic** | ä¾¡å€¤é–¢æ•° $V_\phi(s)$ ã‚’å­¦ç¿’ | çŠ¶æ…‹ä¾¡å€¤æ¨å®š  
  
#### åˆ©ç‚¹

  * **ä½åˆ†æ•£** : Criticã«ã‚ˆã‚‹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ $V(s)$ ã§åˆ†æ•£å‰Šæ¸›
  * **TDå­¦ç¿’** : ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é€”ä¸­ã§ã‚‚æ›´æ–°å¯èƒ½ï¼ˆTDèª¤å·®ä½¿ç”¨ï¼‰
  * **åŠ¹ç‡çš„** : ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã‚ˆã‚Šå­¦ç¿’ãŒé€Ÿã„

#### æ›´æ–°å¼

**TDèª¤å·®ï¼ˆAdvantageï¼‰** ï¼š

$$ A_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t) $$ 

**Actorã®æ›´æ–°** ï¼š

$$ \theta \leftarrow \theta + \alpha_\theta \nabla_\theta \log \pi_\theta(a_t|s_t) A_t $$ 

**Criticã®æ›´æ–°** ï¼š

$$ \phi \leftarrow \phi - \alpha_\phi \nabla_\phi (r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t))^2 $$ 
    
    
    ```mermaid
    graph TB
        subgraph "Actor-Critic Architecture"
            State["State s_t"]
    
            Actor["ActorÏ€_Î¸(a|s)"]
            Critic["CriticV_Ï†(s)"]
    
            Action["Action a_t"]
            Value["Value V(s_t)"]
    
            Env["Environment"]
            Reward["Reward r_t"]
            NextState["Next State s_{t+1}"]
    
            TDError["TD ErrorA_t = r_t + Î³V(s_{t+1}) - V(s_t)"]
    
            ActorUpdate["Actor UpdateÎ¸ â† Î¸ + Î± âˆ‡_Î¸ log Ï€ A_t"]
            CriticUpdate["Critic UpdateÏ† â† Ï† - Î± âˆ‡_Ï† (A_t)Â²"]
    
            State --> Actor
            State --> Critic
    
            Actor --> Action
            Critic --> Value
    
            Action --> Env
            State --> Env
    
            Env --> Reward
            Env --> NextState
    
            Reward --> TDError
            Value --> TDError
            NextState --> Critic
    
            TDError --> ActorUpdate
            TDError --> CriticUpdate
    
            ActorUpdate -.-> Actor
            CriticUpdate -.-> Critic
    
            style Actor fill:#27ae60,color:#fff
            style Critic fill:#e74c3c,color:#fff
            style TDError fill:#f39c12,color:#fff
        end
    ```

### 4.3.2 Actor-Criticå®Ÿè£…
    
    
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    import numpy as np
    import gym
    
    class ActorCriticNetwork(nn.Module):
        """
        Actor-Critic Network
    
        å…±æœ‰ã®feature extractorã‚’æŒã¡ã€Actorã¨Criticã®2ã¤ã®ãƒ˜ãƒƒãƒ‰ã‚’æŒã¤
        """
    
        def __init__(self, state_dim, action_dim, hidden_dim=128):
            super(ActorCriticNetwork, self).__init__()
    
            # å…±æœ‰å±¤
            self.fc1 = nn.Linear(state_dim, hidden_dim)
            self.fc2 = nn.Linear(hidden_dim, hidden_dim)
    
            # Actorãƒ˜ãƒƒãƒ‰ï¼ˆæ–¹ç­–ï¼‰
            self.actor_head = nn.Linear(hidden_dim, action_dim)
    
            # Criticãƒ˜ãƒƒãƒ‰ï¼ˆä¾¡å€¤é–¢æ•°ï¼‰
            self.critic_head = nn.Linear(hidden_dim, 1)
    
        def forward(self, state):
            """
            Args:
                state: çŠ¶æ…‹ [batch_size, state_dim]
    
            Returns:
                action_probs: è¡Œå‹•ç¢ºç‡ [batch_size, action_dim]
                state_value: çŠ¶æ…‹ä¾¡å€¤ [batch_size, 1]
            """
            x = F.relu(self.fc1(state))
            x = F.relu(self.fc2(x))
    
            # Actorå‡ºåŠ›
            logits = self.actor_head(x)
            action_probs = F.softmax(logits, dim=-1)
    
            # Criticå‡ºåŠ›
            state_value = self.critic_head(x)
    
            return action_probs, state_value
    
    
    class ActorCritic:
        """Actor-Critic Algorithm"""
    
        def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):
            """
            Args:
                state_dim: çŠ¶æ…‹ç©ºé–“ã®æ¬¡å…ƒ
                action_dim: è¡Œå‹•ç©ºé–“ã®æ¬¡å…ƒ
                lr: å­¦ç¿’ç‡
                gamma: å‰²å¼•ç‡
            """
            self.gamma = gamma
            self.network = ActorCriticNetwork(state_dim, action_dim)
            self.optimizer = optim.Adam(self.network.parameters(), lr=lr)
    
        def select_action(self, state):
            """
            æ–¹ç­–ã«å¾“ã£ã¦è¡Œå‹•ã‚’é¸æŠ
    
            Args:
                state: çŠ¶æ…‹
    
            Returns:
                action: é¸æŠã•ã‚ŒãŸè¡Œå‹•
                log_prob: log Ï€(a|s)
                state_value: V(s)
            """
            state = torch.FloatTensor(state).unsqueeze(0)
            action_probs, state_value = self.network(state)
    
            # ç¢ºç‡åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            m = torch.distributions.Categorical(action_probs)
            action = m.sample()
            log_prob = m.log_prob(action)
    
            return action.item(), log_prob, state_value
    
        def update(self, log_prob, state_value, reward, next_state, done):
            """
            1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ï¼ˆTDå­¦ç¿’ï¼‰
    
            Args:
                log_prob: log Ï€(a|s)
                state_value: V(s)
                reward: å ±é…¬ r
                next_state: æ¬¡ã®çŠ¶æ…‹ s'
                done: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ãƒ•ãƒ©ã‚°
    
            Returns:
                loss: æå¤±å€¤
            """
            # æ¬¡ã®çŠ¶æ…‹ã®ä¾¡å€¤æ¨å®š
            if done:
                next_value = torch.tensor([0.0])
            else:
                next_state = torch.FloatTensor(next_state).unsqueeze(0)
                with torch.no_grad():
                    _, next_value = self.network(next_state)
    
            # TDèª¤å·®ï¼ˆAdvantageï¼‰
            td_target = reward + self.gamma * next_value
            td_error = td_target - state_value
    
            # Actoræå¤±: -log Ï€(a|s) * A
            actor_loss = -log_prob * td_error.detach()
    
            # Criticæå¤±: (TDèª¤å·®)^2
            critic_loss = td_error.pow(2)
    
            # ç·æå¤±
            loss = actor_loss + critic_loss
    
            # æ›´æ–°
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
    
            return loss.item()
    
    
    # ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    print("=== Actor-Critic on CartPole ===\n")
    
    env = gym.make('CartPole-v1')
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    
    agent = ActorCritic(state_dim, action_dim, lr=0.001, gamma=0.99)
    
    print(f"Environment: CartPole-v1")
    print(f"Agent: Actor-Critic")
    total_params = sum(p.numel() for p in agent.network.parameters())
    print(f"  Total parameters: {total_params:,}")
    
    # è¨“ç·´
    num_episodes = 300
    episode_rewards = []
    
    print("\nTraining...")
    for episode in range(num_episodes):
        state = env.reset()
        episode_reward = 0
        episode_loss = 0
        steps = 0
    
        for t in range(1000):
            action, log_prob, state_value = agent.select_action(state)
            next_state, reward, done, _ = env.step(action)
    
            # 1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«æ›´æ–°
            loss = agent.update(log_prob, state_value, reward, next_state, done)
    
            episode_reward += reward
            episode_loss += loss
            steps += 1
    
            state = next_state
    
            if done:
                break
    
        episode_rewards.append(episode_reward)
    
        if (episode + 1) % 50 == 0:
            avg_reward = np.mean(episode_rewards[-100:])
            avg_loss = episode_loss / steps
            print(f"Episode {episode+1:3d}, Avg Reward: {avg_reward:.2f}, Avg Loss: {avg_loss:.4f}")
    
    print(f"\nTraining completed!")
    print(f"Final average reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.2f}")
    
    print("\nâœ“ Actor-Criticã®ç‰¹å¾´:")
    print("  â€¢ Actorã¨Criticã®2ã¤ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯")
    print("  â€¢ TDå­¦ç¿’ï¼ˆ1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«æ›´æ–°ï¼‰")
    print("  â€¢ REINFORCEã‚ˆã‚Šä½åˆ†æ•£")
    print("  â€¢ ã‚ˆã‚Šå®‰å®šã—ãŸå­¦ç¿’")
    

**å‡ºåŠ›ä¾‹** ï¼š
    
    
    === Actor-Critic on CartPole ===
    
    Environment: CartPole-v1
    Agent: Actor-Critic
      Total parameters: 17,027
    
    Training...
    Episode  50, Avg Reward: 45.67, Avg Loss: 2.3456
    Episode 100, Avg Reward: 98.23, Avg Loss: 1.5678
    Episode 150, Avg Reward: 165.45, Avg Loss: 0.9876
    Episode 200, Avg Reward: 210.34, Avg Loss: 0.6543
    Episode 250, Avg Reward: 245.67, Avg Loss: 0.4321
    Episode 300, Avg Reward: 280.89, Avg Loss: 0.2987
    
    Training completed!
    Final average reward (last 100 episodes): 280.89
    
    âœ“ Actor-Criticã®ç‰¹å¾´:
      â€¢ Actorã¨Criticã®2ã¤ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
      â€¢ TDå­¦ç¿’ï¼ˆ1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«æ›´æ–°ï¼‰
      â€¢ REINFORCEã‚ˆã‚Šä½åˆ†æ•£
      â€¢ ã‚ˆã‚Šå®‰å®šã—ãŸå­¦ç¿’
    

* * *

## 4.4 Advantage Actor-Critic (A2C)

### 4.3.1 A2Cã®æ”¹å–„ç‚¹

**A2C (Advantage Actor-Critic)** ã¯ã€Actor-Criticã®æ”¹è‰¯ç‰ˆã§ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ï¼š

  * **n-step returns** : è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—å…ˆã¾ã§è¦‹ãŸãƒªã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨
  * **Parallel environments** : è¤‡æ•°ç’°å¢ƒã§åŒæ™‚ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆãƒ‡ãƒ¼ã‚¿å¤šæ§˜æ€§ï¼‰
  * **Entropy regularization** : æ¢ç´¢ã‚’ä¿ƒé€²
  * **Generalized Advantage Estimation (GAE)** : ãƒã‚¤ã‚¢ã‚¹ã¨åˆ†æ•£ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’èª¿æ•´

#### n-step Returns

1ã‚¹ãƒ†ãƒƒãƒ—TDã§ã¯ãªãã€$n$ã‚¹ãƒ†ãƒƒãƒ—å…ˆã¾ã§ã®å ±é…¬ã‚’ä½¿ç”¨ï¼š

$$ R_t^{(n)} = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots + \gamma^{n-1} r_{t+n-1} + \gamma^n V(s_{t+n}) $$ 

#### Entropy Regularization

æ–¹ç­–ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’ç›®çš„é–¢æ•°ã«åŠ ãˆã€æ¢ç´¢ã‚’ä¿ƒé€²ï¼š

$$ J(\theta) = \mathbb{E} \left[ \sum_t \log \pi_\theta(a_t|s_t) A_t + \beta H(\pi_\theta(\cdot|s_t)) \right] $$ 

ã“ã“ã§ $H(\pi) = -\sum_a \pi(a|s) \log \pi(a|s)$ ã¯ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€‚

### 4.4.2 A2Cå®Ÿè£…
    
    
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    import numpy as np
    import gym
    from torch.distributions import Categorical
    
    class A2CNetwork(nn.Module):
        """A2C Network with shared feature extractor"""
    
        def __init__(self, state_dim, action_dim, hidden_dim=256):
            super(A2CNetwork, self).__init__()
    
            # å…±æœ‰ç‰¹å¾´æŠ½å‡ºå±¤
            self.fc1 = nn.Linear(state_dim, hidden_dim)
            self.fc2 = nn.Linear(hidden_dim, hidden_dim)
    
            # Actor
            self.actor = nn.Linear(hidden_dim, action_dim)
    
            # Critic
            self.critic = nn.Linear(hidden_dim, 1)
    
        def forward(self, state):
            x = F.relu(self.fc1(state))
            x = F.relu(self.fc2(x))
    
            action_logits = self.actor(x)
            state_value = self.critic(x)
    
            return action_logits, state_value
    
    
    class A2C:
        """
        Advantage Actor-Critic (A2C)
    
        Features:
          - n-step returns
          - Entropy regularization
          - Parallel environment support
        """
    
        def __init__(self, state_dim, action_dim, lr=0.0007, gamma=0.99,
                     n_steps=5, entropy_coef=0.01, value_coef=0.5):
            """
            Args:
                state_dim: çŠ¶æ…‹ç©ºé–“ã®æ¬¡å…ƒ
                action_dim: è¡Œå‹•ç©ºé–“ã®æ¬¡å…ƒ
                lr: å­¦ç¿’ç‡
                gamma: å‰²å¼•ç‡
                n_steps: n-step returns
                entropy_coef: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ä¿‚æ•°
                value_coef: ä¾¡å€¤æå¤±ã®ä¿‚æ•°
            """
            self.gamma = gamma
            self.n_steps = n_steps
            self.entropy_coef = entropy_coef
            self.value_coef = value_coef
    
            self.network = A2CNetwork(state_dim, action_dim)
            self.optimizer = optim.Adam(self.network.parameters(), lr=lr)
    
        def select_action(self, state):
            """è¡Œå‹•é¸æŠ"""
            state = torch.FloatTensor(state).unsqueeze(0)
            action_logits, state_value = self.network(state)
    
            # è¡Œå‹•åˆ†å¸ƒ
            dist = Categorical(logits=action_logits)
            action = dist.sample()
    
            return action.item(), dist.log_prob(action), dist.entropy(), state_value
    
        def compute_returns(self, rewards, values, dones, next_value):
            """
            n-step returnsã®è¨ˆç®—
    
            Args:
                rewards: å ±é…¬åˆ— [n_steps]
                values: çŠ¶æ…‹ä¾¡å€¤åˆ— [n_steps]
                dones: çµ‚äº†ãƒ•ãƒ©ã‚°åˆ— [n_steps]
                next_value: æœ€å¾Œã®çŠ¶æ…‹ã®æ¬¡ã®çŠ¶æ…‹ä¾¡å€¤
    
            Returns:
                returns: n-step returns [n_steps]
                advantages: Advantage [n_steps]
            """
            returns = []
            R = next_value
    
            # é€†é †ã«è¨ˆç®—
            for step in reversed(range(len(rewards))):
                R = rewards[step] + self.gamma * R * (1 - dones[step])
                returns.insert(0, R)
    
            returns = torch.tensor(returns, dtype=torch.float32)
            values = torch.cat(values)
    
            # Advantage = Returns - V(s)
            advantages = returns - values.detach()
    
            return returns, advantages
    
        def update(self, log_probs, entropies, values, returns, advantages):
            """
            ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
    
            Args:
                log_probs: log Ï€(a|s) ã®ãƒªã‚¹ãƒˆ
                entropies: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®ãƒªã‚¹ãƒˆ
                values: V(s) ã®ãƒªã‚¹ãƒˆ
                returns: n-step returns
                advantages: Advantage
            """
            log_probs = torch.cat(log_probs)
            entropies = torch.cat(entropies)
            values = torch.cat(values)
    
            # Actoræå¤±: -log Ï€(a|s) * A
            actor_loss = -(log_probs * advantages.detach()).mean()
    
            # Criticæå¤±: MSE(returns, V(s))
            critic_loss = F.mse_loss(values, returns)
    
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–
            entropy_loss = -entropies.mean()
    
            # ç·æå¤±
            loss = actor_loss + self.value_coef * critic_loss + self.entropy_coef * entropy_loss
    
            # æ›´æ–°
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)
            self.optimizer.step()
    
            return loss.item(), actor_loss.item(), critic_loss.item(), entropy_loss.item()
    
    
    # ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    print("=== A2C on CartPole ===\n")
    
    env = gym.make('CartPole-v1')
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    
    agent = A2C(state_dim, action_dim, lr=0.0007, gamma=0.99, n_steps=5)
    
    print(f"Environment: CartPole-v1")
    print(f"Agent: A2C")
    print(f"  n_steps: {agent.n_steps}")
    print(f"  entropy_coef: {agent.entropy_coef}")
    print(f"  value_coef: {agent.value_coef}")
    total_params = sum(p.numel() for p in agent.network.parameters())
    print(f"  Total parameters: {total_params:,}")
    
    # è¨“ç·´
    num_episodes = 500
    episode_rewards = []
    
    print("\nTraining...")
    for episode in range(num_episodes):
        state = env.reset()
        episode_reward = 0
    
        # n-stepãƒ‡ãƒ¼ã‚¿ã®åé›†
        log_probs = []
        entropies = []
        values = []
        rewards = []
        dones = []
    
        done = False
        while not done:
            action, log_prob, entropy, value = agent.select_action(state)
            next_state, reward, done, _ = env.step(action)
    
            log_probs.append(log_prob)
            entropies.append(entropy)
            values.append(value)
            rewards.append(reward)
            dones.append(done)
    
            episode_reward += reward
            state = next_state
    
            # n-stepã”ã¨ã¾ãŸã¯ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†æ™‚ã«æ›´æ–°
            if len(rewards) >= agent.n_steps or done:
                # æ¬¡ã®çŠ¶æ…‹ã®ä¾¡å€¤
                if done:
                    next_value = 0
                else:
                    next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)
                    with torch.no_grad():
                        _, next_value = agent.network(next_state_tensor)
                        next_value = next_value.item()
    
                # Returns and Advantagesã®è¨ˆç®—
                returns, advantages = agent.compute_returns(rewards, values, dones, next_value)
    
                # æ›´æ–°
                loss, actor_loss, critic_loss, entropy_loss = agent.update(
                    log_probs, entropies, values, returns, advantages
                )
    
                # ãƒªã‚»ãƒƒãƒˆ
                log_probs = []
                entropies = []
                values = []
                rewards = []
                dones = []
    
        episode_rewards.append(episode_reward)
    
        if (episode + 1) % 50 == 0:
            avg_reward = np.mean(episode_rewards[-100:])
            print(f"Episode {episode+1:3d}, Avg Reward: {avg_reward:.2f}, "
                  f"Loss: {loss:.4f} (Actor: {actor_loss:.4f}, Critic: {critic_loss:.4f}, Entropy: {entropy_loss:.4f})")
    
    print(f"\nTraining completed!")
    print(f"Final average reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.2f}")
    
    print("\nâœ“ A2Cã®ç‰¹å¾´:")
    print("  â€¢ n-step returnsï¼ˆã‚ˆã‚Šæ­£ç¢ºãªãƒªã‚¿ãƒ¼ãƒ³æ¨å®šï¼‰")
    print("  â€¢ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ï¼ˆæ¢ç´¢ä¿ƒé€²ï¼‰")
    print("  â€¢ å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå®‰å®šæ€§å‘ä¸Šï¼‰")
    print("  â€¢ ä¸¦åˆ—ç’°å¢ƒå¯¾å¿œï¼ˆã“ã®ä¾‹ã§ã¯1ç’°å¢ƒï¼‰")
    

**å‡ºåŠ›ä¾‹** ï¼š
    
    
    === A2C on CartPole ===
    
    Environment: CartPole-v1
    Agent: A2C
      n_steps: 5
      entropy_coef: 0.01
      value_coef: 0.5
      Total parameters: 68,097
    
    Training...
    Episode  50, Avg Reward: 56.78, Loss: 1.8765 (Actor: 0.5432, Critic: 2.6543, Entropy: -0.5678)
    Episode 100, Avg Reward: 112.34, Loss: 1.2345 (Actor: 0.3456, Critic: 1.7654, Entropy: -0.4321)
    Episode 150, Avg Reward: 178.56, Loss: 0.8765 (Actor: 0.2345, Critic: 1.2987, Entropy: -0.3456)
    Episode 200, Avg Reward: 220.45, Loss: 0.6543 (Actor: 0.1876, Critic: 0.9876, Entropy: -0.2987)
    Episode 250, Avg Reward: 265.78, Loss: 0.4987 (Actor: 0.1432, Critic: 0.7654, Entropy: -0.2456)
    Episode 300, Avg Reward: 295.34, Loss: 0.3876 (Actor: 0.1098, Critic: 0.6321, Entropy: -0.2134)
    Episode 350, Avg Reward: 320.67, Loss: 0.2987 (Actor: 0.0876, Critic: 0.5234, Entropy: -0.1876)
    Episode 400, Avg Reward: 340.23, Loss: 0.2345 (Actor: 0.0654, Critic: 0.4321, Entropy: -0.1654)
    Episode 450, Avg Reward: 355.89, Loss: 0.1876 (Actor: 0.0543, Critic: 0.3654, Entropy: -0.1432)
    Episode 500, Avg Reward: 370.45, Loss: 0.1543 (Actor: 0.0432, Critic: 0.3098, Entropy: -0.1234)
    
    Training completed!
    Final average reward (last 100 episodes): 370.45
    
    âœ“ A2Cã®ç‰¹å¾´:
      â€¢ n-step returnsï¼ˆã‚ˆã‚Šæ­£ç¢ºãªãƒªã‚¿ãƒ¼ãƒ³æ¨å®šï¼‰
      â€¢ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ï¼ˆæ¢ç´¢ä¿ƒé€²ï¼‰
      â€¢ å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå®‰å®šæ€§å‘ä¸Šï¼‰
      â€¢ ä¸¦åˆ—ç’°å¢ƒå¯¾å¿œï¼ˆã“ã®ä¾‹ã§ã¯1ç’°å¢ƒï¼‰
    

* * *

## 4.5 Proximal Policy Optimization (PPO)

### 4.5.1 PPOã®å‹•æ©Ÿ

Policy Gradientã®èª²é¡Œï¼š

  * **å¤§ããªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°** : æ–¹ç­–ãŒå¤§ããå¤‰åŒ–ã—ã™ãã‚‹ã¨æ€§èƒ½ãŒæ‚ªåŒ–
  * **ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡** : on-policyå­¦ç¿’ã¯éåŠ¹ç‡

**PPO (Proximal Policy Optimization)** ã®è§£æ±ºç­–ï¼š

  * **Clipped objective** : æ–¹ç­–ã®å¤‰åŒ–ã‚’åˆ¶é™
  * **Multiple epochs** : åŒã˜ãƒ‡ãƒ¼ã‚¿ã§è¤‡æ•°å›æ›´æ–°ï¼ˆoff-policyã«è¿‘ã„ï¼‰
  * **Trust region** : å®‰å…¨ãªæ›´æ–°ç¯„å›²å†…ã§æœ€é©åŒ–

### 4.5.2 PPOã®Clipped Objective

PPOã®ç›®çš„é–¢æ•°ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ã‚¯ãƒªãƒƒãƒ—ã•ã‚ŒãŸç¢ºç‡æ¯”ã‚’ä½¿ç”¨ã—ã¾ã™ï¼š

$$ L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t) \right] $$ 

ã“ã“ã§ï¼š

  * $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$: ç¢ºç‡æ¯”
  * $\epsilon$: ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç¯„å›²ï¼ˆé€šå¸¸ 0.1 ã€œ 0.2ï¼‰
  * $A_t$: Advantage

**ç›´æ„Ÿçš„ç†è§£** ï¼š

  * AdvantageãŒæ­£ã®å ´åˆ: ç¢ºç‡æ¯”ã‚’ $[1, 1+\epsilon]$ ã«åˆ¶é™ï¼ˆéåº¦ãªå¢—åŠ ã‚’é˜²ãï¼‰
  * AdvantageãŒè² ã®å ´åˆ: ç¢ºç‡æ¯”ã‚’ $[1-\epsilon, 1]$ ã«åˆ¶é™ï¼ˆéåº¦ãªæ¸›å°‘ã‚’é˜²ãï¼‰

    
    
    ```mermaid
    graph TB
        subgraph "PPO Clipped Objective"
            OldPolicy["Old PolicyÏ€_old(a|s)"]
            NewPolicy["New PolicyÏ€_Î¸(a|s)"]
    
            Ratio["Probability Ratior = Ï€_Î¸ / Ï€_old"]
            Clip["Clippingclip(r, 1-Îµ, 1+Îµ)"]
    
            Advantage["AdvantageA_t"]
    
            Obj1["Objective 1r Ã— A"]
            Obj2["Objective 2clip(r) Ã— A"]
    
            Min["min(Obj1, Obj2)"]
    
            Loss["PPO Loss-E[min(...)]"]
    
            OldPolicy --> Ratio
            NewPolicy --> Ratio
    
            Ratio --> Obj1
            Ratio --> Clip
            Clip --> Obj2
    
            Advantage --> Obj1
            Advantage --> Obj2
    
            Obj1 --> Min
            Obj2 --> Min
    
            Min --> Loss
    
            style OldPolicy fill:#7b2cbf,color:#fff
            style NewPolicy fill:#27ae60,color:#fff
            style Loss fill:#e74c3c,color:#fff
        end
    ```

### 4.5.3 PPOå®Ÿè£…
    
    
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    import numpy as np
    import gym
    from torch.distributions import Categorical
    
    class PPONetwork(nn.Module):
        """PPO Network"""
    
        def __init__(self, state_dim, action_dim, hidden_dim=64):
            super(PPONetwork, self).__init__()
    
            self.fc1 = nn.Linear(state_dim, hidden_dim)
            self.fc2 = nn.Linear(hidden_dim, hidden_dim)
    
            self.actor = nn.Linear(hidden_dim, action_dim)
            self.critic = nn.Linear(hidden_dim, 1)
    
        def forward(self, state):
            x = F.tanh(self.fc1(state))
            x = F.tanh(self.fc2(x))
    
            action_logits = self.actor(x)
            state_value = self.critic(x)
    
            return action_logits, state_value
    
    
    class PPO:
        """
        Proximal Policy Optimization (PPO)
    
        Features:
          - Clipped objective
          - Multiple epochs per update
          - GAE (Generalized Advantage Estimation)
        """
    
        def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99,
                     epsilon=0.2, gae_lambda=0.95, epochs=10, batch_size=64):
            """
            Args:
                state_dim: çŠ¶æ…‹ç©ºé–“ã®æ¬¡å…ƒ
                action_dim: è¡Œå‹•ç©ºé–“ã®æ¬¡å…ƒ
                lr: å­¦ç¿’ç‡
                gamma: å‰²å¼•ç‡
                epsilon: ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç¯„å›²
                gae_lambda: GAE Î»
                epochs: 1å›ã®ãƒ‡ãƒ¼ã‚¿åé›†ã‚ãŸã‚Šã®æ›´æ–°å›æ•°
                batch_size: ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚º
            """
            self.gamma = gamma
            self.epsilon = epsilon
            self.gae_lambda = gae_lambda
            self.epochs = epochs
            self.batch_size = batch_size
    
            self.network = PPONetwork(state_dim, action_dim)
            self.optimizer = optim.Adam(self.network.parameters(), lr=lr)
    
        def select_action(self, state):
            """è¡Œå‹•é¸æŠ"""
            state = torch.FloatTensor(state).unsqueeze(0)
    
            with torch.no_grad():
                action_logits, state_value = self.network(state)
    
            dist = Categorical(logits=action_logits)
            action = dist.sample()
            log_prob = dist.log_prob(action)
    
            return action.item(), log_prob.item(), state_value.item()
    
        def compute_gae(self, rewards, values, dones, next_value):
            """
            Generalized Advantage Estimation (GAE)
    
            Args:
                rewards: å ±é…¬åˆ—
                values: çŠ¶æ…‹ä¾¡å€¤åˆ—
                dones: çµ‚äº†ãƒ•ãƒ©ã‚°åˆ—
                next_value: æœ€å¾Œã®æ¬¡çŠ¶æ…‹ã®ä¾¡å€¤
    
            Returns:
                advantages: GAE Advantage
                returns: ãƒªã‚¿ãƒ¼ãƒ³
            """
            advantages = []
            gae = 0
    
            values = values + [next_value]
    
            for step in reversed(range(len(rewards))):
                delta = rewards[step] + self.gamma * values[step + 1] * (1 - dones[step]) - values[step]
                gae = delta + self.gamma * self.gae_lambda * (1 - dones[step]) * gae
                advantages.insert(0, gae)
    
            advantages = torch.tensor(advantages, dtype=torch.float32)
            returns = advantages + torch.tensor(values[:-1], dtype=torch.float32)
    
            return advantages, returns
    
        def update(self, states, actions, old_log_probs, returns, advantages):
            """
            PPOæ›´æ–°ï¼ˆMultiple epochsï¼‰
    
            Args:
                states: çŠ¶æ…‹åˆ—
                actions: è¡Œå‹•åˆ—
                old_log_probs: æ—§æ–¹ç­–ã®logç¢ºç‡
                returns: ãƒªã‚¿ãƒ¼ãƒ³
                advantages: Advantage
            """
            states = torch.FloatTensor(states)
            actions = torch.LongTensor(actions)
            old_log_probs = torch.FloatTensor(old_log_probs)
            returns = returns.detach()
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
    
            dataset_size = states.size(0)
    
            for epoch in range(self.epochs):
                # ãƒŸãƒ‹ãƒãƒƒãƒã§ã®æ›´æ–°
                indices = np.random.permutation(dataset_size)
    
                for start in range(0, dataset_size, self.batch_size):
                    end = start + self.batch_size
                    batch_indices = indices[start:end]
    
                    batch_states = states[batch_indices]
                    batch_actions = actions[batch_indices]
                    batch_old_log_probs = old_log_probs[batch_indices]
                    batch_returns = returns[batch_indices]
                    batch_advantages = advantages[batch_indices]
    
                    # ç¾åœ¨ã®æ–¹ç­–ã®è©•ä¾¡
                    action_logits, state_values = self.network(batch_states)
                    dist = Categorical(logits=action_logits)
                    new_log_probs = dist.log_prob(batch_actions)
                    entropy = dist.entropy()
    
                    # ç¢ºç‡æ¯”
                    ratio = torch.exp(new_log_probs - batch_old_log_probs)
    
                    # Clipped objective
                    surr1 = ratio * batch_advantages
                    surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * batch_advantages
                    actor_loss = -torch.min(surr1, surr2).mean()
    
                    # Criticæå¤±
                    critic_loss = F.mse_loss(state_values.squeeze(), batch_returns)
    
                    # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒœãƒ¼ãƒŠã‚¹
                    entropy_loss = -entropy.mean()
    
                    # ç·æå¤±
                    loss = actor_loss + 0.5 * critic_loss + 0.01 * entropy_loss
    
                    # æ›´æ–°
                    self.optimizer.zero_grad()
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)
                    self.optimizer.step()
    
    
    # ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    print("=== PPO on CartPole ===\n")
    
    env = gym.make('CartPole-v1')
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    
    agent = PPO(state_dim, action_dim, lr=3e-4, gamma=0.99, epsilon=0.2, epochs=10)
    
    print(f"Environment: CartPole-v1")
    print(f"Agent: PPO")
    print(f"  epsilon (clip): {agent.epsilon}")
    print(f"  gae_lambda: {agent.gae_lambda}")
    print(f"  epochs per update: {agent.epochs}")
    total_params = sum(p.numel() for p in agent.network.parameters())
    print(f"  Total parameters: {total_params:,}")
    
    # è¨“ç·´
    num_iterations = 100
    update_timesteps = 2048  # ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ãƒ†ãƒƒãƒ—æ•°
    episode_rewards = []
    
    print("\nTraining...")
    total_timesteps = 0
    
    for iteration in range(num_iterations):
        # ãƒ‡ãƒ¼ã‚¿åé›†
        states_list = []
        actions_list = []
        log_probs_list = []
        rewards_list = []
        values_list = []
        dones_list = []
    
        state = env.reset()
        episode_reward = 0
    
        for _ in range(update_timesteps):
            action, log_prob, value = agent.select_action(state)
            next_state, reward, done, _ = env.step(action)
    
            states_list.append(state)
            actions_list.append(action)
            log_probs_list.append(log_prob)
            rewards_list.append(reward)
            values_list.append(value)
            dones_list.append(done)
    
            episode_reward += reward
            total_timesteps += 1
    
            state = next_state
    
            if done:
                episode_rewards.append(episode_reward)
                episode_reward = 0
                state = env.reset()
    
        # æœ€å¾Œã®çŠ¶æ…‹ã®ä¾¡å€¤
        _, _, next_value = agent.select_action(state)
    
        # GAEã®è¨ˆç®—
        advantages, returns = agent.compute_gae(rewards_list, values_list, dones_list, next_value)
    
        # PPOæ›´æ–°
        agent.update(states_list, actions_list, log_probs_list, returns, advantages)
    
        if (iteration + 1) % 10 == 0:
            avg_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)
            print(f"Iteration {iteration+1:3d}, Timesteps: {total_timesteps}, Avg Reward: {avg_reward:.2f}")
    
    print(f"\nTraining completed!")
    print(f"Total timesteps: {total_timesteps}")
    print(f"Final average reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.2f}")
    
    print("\nâœ“ PPOã®ç‰¹å¾´:")
    print("  â€¢ Clipped objectiveï¼ˆå®‰å…¨ãªæ–¹ç­–æ›´æ–°ï¼‰")
    print("  â€¢ Multiple epochsï¼ˆãƒ‡ãƒ¼ã‚¿å†åˆ©ç”¨ï¼‰")
    print("  â€¢ GAEï¼ˆãƒã‚¤ã‚¢ã‚¹-åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰")
    print("  â€¢ ç¾ä»£çš„ãªPolicy Gradientã®ãƒ‡ãƒ•ã‚¡ã‚¯ãƒˆã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰")
    print("  â€¢ OpenAI Fiveã€ChatGPTã®RLHFãªã©ã«ä½¿ç”¨")
    

**å‡ºåŠ›ä¾‹** ï¼š
    
    
    === PPO on CartPole ===
    
    Environment: CartPole-v1
    Agent: PPO
      epsilon (clip): 0.2
      gae_lambda: 0.95
      epochs per update: 10
      Total parameters: 4,545
    
    Training...
    Iteration  10, Timesteps: 20480, Avg Reward: 78.45
    Iteration  20, Timesteps: 40960, Avg Reward: 145.67
    Iteration  30, Timesteps: 61440, Avg Reward: 210.34
    Iteration  40, Timesteps: 81920, Avg Reward: 265.89
    Iteration  50, Timesteps: 102400, Avg Reward: 310.45
    Iteration  60, Timesteps: 122880, Avg Reward: 345.67
    Iteration  70, Timesteps: 143360, Avg Reward: 380.23
    Iteration  80, Timesteps: 163840, Avg Reward: 405.78
    Iteration  90, Timesteps: 184320, Avg Reward: 425.34
    Iteration 100, Timesteps: 204800, Avg Reward: 440.56
    
    Training completed!
    Total timesteps: 204800
    Final average reward (last 100 episodes): 440.56
    
    âœ“ PPOã®ç‰¹å¾´:
      â€¢ Clipped objectiveï¼ˆå®‰å…¨ãªæ–¹ç­–æ›´æ–°ï¼‰
      â€¢ Multiple epochsï¼ˆãƒ‡ãƒ¼ã‚¿å†åˆ©ç”¨ï¼‰
      â€¢ GAEï¼ˆãƒã‚¤ã‚¢ã‚¹-åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰
      â€¢ ç¾ä»£çš„ãªPolicy Gradientã®ãƒ‡ãƒ•ã‚¡ã‚¯ãƒˆã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰
      â€¢ OpenAI Fiveã€ChatGPTã®RLHFãªã©ã«ä½¿ç”¨
    

* * *

## 4.6 å®Ÿè·µï¼šLunarLanderé€£ç¶šåˆ¶å¾¡

### 4.6.1 LunarLanderç’°å¢ƒ

**LunarLander-v2** ã¯ã€æœˆé¢ç€é™¸èˆ¹ã‚’åˆ¶å¾¡ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚

é …ç›® | å€¤  
---|---  
**çŠ¶æ…‹ç©ºé–“** | 8æ¬¡å…ƒï¼ˆä½ç½®ã€é€Ÿåº¦ã€è§’åº¦ã€è§’é€Ÿåº¦ã€è„šæ¥åœ°ï¼‰  
**è¡Œå‹•ç©ºé–“** | 4æ¬¡å…ƒï¼ˆä½•ã‚‚ã—ãªã„ã€å·¦ã‚¨ãƒ³ã‚¸ãƒ³ã€ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ã‚¸ãƒ³ã€å³ã‚¨ãƒ³ã‚¸ãƒ³ï¼‰  
**ç›®æ¨™** | ç€é™¸ãƒ‘ãƒƒãƒ‰ã«å®‰å…¨ã«ç€é™¸ï¼ˆ200ç‚¹ä»¥ä¸Šã§è§£æ±ºï¼‰  
**å ±é…¬** | ç€é™¸æˆåŠŸ: +100ã€œ+140ã€å¢œè½: -100ã€ç‡ƒæ–™æ¶ˆè²»: ãƒã‚¤ãƒŠã‚¹  
  
### 4.6.2 PPOã«ã‚ˆã‚‹LunarLanderå­¦ç¿’
    
    
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    import numpy as np
    import gym
    from torch.distributions import Categorical
    import matplotlib.pyplot as plt
    
    # PPOã‚¯ãƒ©ã‚¹ã¯å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨åŒã˜ï¼ˆçœç•¥ï¼‰
    
    # LunarLanderã§ã®è¨“ç·´
    print("=== PPO on LunarLander-v2 ===\n")
    
    env = gym.make('LunarLander-v2')
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    
    print(f"Environment: LunarLander-v2")
    print(f"  State dimension: {state_dim}")
    print(f"  Action dimension: {action_dim}")
    print(f"  Solved threshold: 200")
    
    agent = PPO(state_dim, action_dim, lr=3e-4, gamma=0.99, epsilon=0.2, epochs=10, batch_size=64)
    
    print(f"\nAgent: PPO")
    total_params = sum(p.numel() for p in agent.network.parameters())
    print(f"  Total parameters: {total_params:,}")
    
    # è¨“ç·´è¨­å®š
    num_iterations = 300
    update_timesteps = 2048
    episode_rewards = []
    all_episode_rewards = []
    
    print("\nTraining...")
    total_timesteps = 0
    best_avg_reward = -float('inf')
    
    for iteration in range(num_iterations):
        # ãƒ‡ãƒ¼ã‚¿åé›†
        states_list = []
        actions_list = []
        log_probs_list = []
        rewards_list = []
        values_list = []
        dones_list = []
    
        state = env.reset()
        episode_reward = 0
    
        for _ in range(update_timesteps):
            action, log_prob, value = agent.select_action(state)
            next_state, reward, done, _ = env.step(action)
    
            states_list.append(state)
            actions_list.append(action)
            log_probs_list.append(log_prob)
            rewards_list.append(reward)
            values_list.append(value)
            dones_list.append(done)
    
            episode_reward += reward
            total_timesteps += 1
    
            state = next_state
    
            if done:
                all_episode_rewards.append(episode_reward)
                episode_reward = 0
                state = env.reset()
    
        # æœ€å¾Œã®çŠ¶æ…‹ã®ä¾¡å€¤
        _, _, next_value = agent.select_action(state)
    
        # GAEã®è¨ˆç®—
        advantages, returns = agent.compute_gae(rewards_list, values_list, dones_list, next_value)
    
        # PPOæ›´æ–°
        agent.update(states_list, actions_list, log_probs_list, returns, advantages)
    
        # è©•ä¾¡
        if (iteration + 1) % 10 == 0:
            avg_reward = np.mean(all_episode_rewards[-100:]) if len(all_episode_rewards) >= 100 else np.mean(all_episode_rewards)
            episode_rewards.append(avg_reward)
    
            if avg_reward > best_avg_reward:
                best_avg_reward = avg_reward
    
            print(f"Iteration {iteration+1:3d}, Timesteps: {total_timesteps}, "
                  f"Avg Reward: {avg_reward:.2f}, Best: {best_avg_reward:.2f}")
    
            if avg_reward >= 200:
                print(f"\nğŸ‰ Solved! Average reward {avg_reward:.2f} >= 200")
                break
    
    print(f"\nTraining completed!")
    print(f"Total timesteps: {total_timesteps}")
    print(f"Best average reward: {best_avg_reward:.2f}")
    
    # å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(10, 5))
    
    # å…¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å ±é…¬
    ax.plot(all_episode_rewards, alpha=0.3, color='steelblue', label='Episode Reward')
    
    # ç§»å‹•å¹³å‡ï¼ˆ100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰
    window = 100
    moving_avg = [np.mean(all_episode_rewards[max(0, i-window):i+1])
                  for i in range(len(all_episode_rewards))]
    ax.plot(moving_avg, linewidth=2, color='darkorange', label=f'Moving Average ({window})')
    
    ax.axhline(y=200, color='red', linestyle='--', linewidth=2, label='Solved Threshold (200)')
    ax.set_xlabel('Episode', fontsize=12, fontweight='bold')
    ax.set_ylabel('Reward', fontsize=12, fontweight='bold')
    ax.set_title('PPO on LunarLander-v2', fontsize=13, fontweight='bold')
    ax.legend()
    ax.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    print("\nâœ“ LunarLanderã‚¿ã‚¹ã‚¯å®Œäº†")
    print("âœ“ PPOã«ã‚ˆã‚‹å®‰å®šã—ãŸå­¦ç¿’")
    print("âœ“ å…¸å‹çš„ãªè§£æ±ºæ™‚é–“: 100-200ä¸‡ã‚¹ãƒ†ãƒƒãƒ—")
    

**å‡ºåŠ›ä¾‹** ï¼š
    
    
    === PPO on LunarLander-v2 ===
    
    Environment: LunarLander-v2
      State dimension: 8
      Action dimension: 4
      Solved threshold: 200
    
    Agent: PPO
      Total parameters: 4,673
    
    Training...
    Iteration  10, Timesteps: 20480, Avg Reward: -145.67, Best: -145.67
    Iteration  20, Timesteps: 40960, Avg Reward: -89.34, Best: -89.34
    Iteration  30, Timesteps: 61440, Avg Reward: -45.23, Best: -45.23
    Iteration  40, Timesteps: 81920, Avg Reward: 12.56, Best: 12.56
    Iteration  50, Timesteps: 102400, Avg Reward: 56.78, Best: 56.78
    Iteration  60, Timesteps: 122880, Avg Reward: 98.45, Best: 98.45
    Iteration  70, Timesteps: 143360, Avg Reward: 134.67, Best: 134.67
    Iteration  80, Timesteps: 163840, Avg Reward: 165.89, Best: 165.89
    Iteration  90, Timesteps: 184320, Avg Reward: 185.34, Best: 185.34
    Iteration 100, Timesteps: 204800, Avg Reward: 202.56, Best: 202.56
    
    ğŸ‰ Solved! Average reward 202.56 >= 200
    
    Training completed!
    Total timesteps: 204800
    Best average reward: 202.56
    
    âœ“ LunarLanderã‚¿ã‚¹ã‚¯å®Œäº†
    âœ“ PPOã«ã‚ˆã‚‹å®‰å®šã—ãŸå­¦ç¿’
    âœ“ å…¸å‹çš„ãªè§£æ±ºæ™‚é–“: 100-200ä¸‡ã‚¹ãƒ†ãƒƒãƒ—
    

* * *

## 4.7 é€£ç¶šè¡Œå‹•ç©ºé–“ã¨Gaussian Policy

### 4.7.1 é€£ç¶šè¡Œå‹•ç©ºé–“ã®æ‰±ã„

ã“ã‚Œã¾ã§ã¯é›¢æ•£è¡Œå‹•ç©ºé–“ï¼ˆCartPoleã€LunarLanderï¼‰ã‚’æ‰±ã„ã¾ã—ãŸãŒã€ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡ãªã©ã§ã¯**é€£ç¶šè¡Œå‹•ç©ºé–“** ãŒå¿…è¦ã§ã™ã€‚

**Gaussian Policy** ï¼š

è¡Œå‹•ã‚’æ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼š

$$ \pi_\theta(a|s) = \mathcal{N}(\mu_\theta(s), \sigma_\theta(s)^2) $$ 

ã“ã“ã§ï¼š

  * $\mu_\theta(s)$: å¹³å‡ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§å‡ºåŠ›ï¼‰
  * $\sigma_\theta(s)$: æ¨™æº–åå·®ï¼ˆå­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¾ãŸã¯å›ºå®šå€¤ï¼‰

### 4.7.2 Gaussian Policyå®Ÿè£…
    
    
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.distributions import Normal
    import numpy as np
    
    class ContinuousPolicyNetwork(nn.Module):
        """
        Continuous action spaceç”¨ã®Policy Network
    
        å‡ºåŠ›: å¹³å‡Î¼ã¨æ¨™æº–åå·®Ïƒ
        """
    
        def __init__(self, state_dim, action_dim, hidden_dim=256):
            super(ContinuousPolicyNetwork, self).__init__()
    
            self.fc1 = nn.Linear(state_dim, hidden_dim)
            self.fc2 = nn.Linear(hidden_dim, hidden_dim)
    
            # å¹³å‡Î¼
            self.mu_head = nn.Linear(hidden_dim, action_dim)
    
            # æ¨™æº–åå·®Ïƒï¼ˆlog scaleã§å­¦ç¿’ï¼‰
            self.log_std_head = nn.Linear(hidden_dim, action_dim)
    
            # Critic
            self.value_head = nn.Linear(hidden_dim, 1)
    
        def forward(self, state):
            x = F.relu(self.fc1(state))
            x = F.relu(self.fc2(x))
    
            # å¹³å‡Î¼
            mu = self.mu_head(x)
    
            # æ¨™æº–åå·®Ïƒï¼ˆæ­£ã®å€¤ã‚’ä¿è¨¼ï¼‰
            log_std = self.log_std_head(x)
            log_std = torch.clamp(log_std, min=-20, max=2)  # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã‚¯ãƒªãƒƒãƒ—
            std = torch.exp(log_std)
    
            # çŠ¶æ…‹ä¾¡å€¤
            value = self.value_head(x)
    
            return mu, std, value
    
    
    class ContinuousPPO:
        """é€£ç¶šè¡Œå‹•ç©ºé–“ç”¨ã®PPO"""
    
        def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, epsilon=0.2):
            self.gamma = gamma
            self.epsilon = epsilon
    
            self.network = ContinuousPolicyNetwork(state_dim, action_dim)
            self.optimizer = torch.optim.Adam(self.network.parameters(), lr=lr)
    
        def select_action(self, state):
            """
            é€£ç¶šè¡Œå‹•ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    
            Returns:
                action: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸè¡Œå‹•
                log_prob: log Ï€(a|s)
                value: V(s)
            """
            state = torch.FloatTensor(state).unsqueeze(0)
    
            with torch.no_grad():
                mu, std, value = self.network(state)
    
            # æ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            dist = Normal(mu, std)
            action = dist.sample()
            log_prob = dist.log_prob(action).sum(dim=-1)  # å„æ¬¡å…ƒã®ç©
    
            return action.squeeze().numpy(), log_prob.item(), value.item()
    
        def evaluate_actions(self, states, actions):
            """
            æ—¢å­˜ã®è¡Œå‹•ã‚’è©•ä¾¡ï¼ˆPPOæ›´æ–°ç”¨ï¼‰
    
            Returns:
                log_probs: log Ï€(a|s)
                values: V(s)
                entropy: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
            """
            mu, std, values = self.network(states)
    
            dist = Normal(mu, std)
            log_probs = dist.log_prob(actions).sum(dim=-1)
            entropy = dist.entropy().sum(dim=-1)
    
            return log_probs, values.squeeze(), entropy
    
    
    # ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    print("=== Continuous Action Space PPO ===\n")
    
    # ã‚µãƒ³ãƒ—ãƒ«ç’°å¢ƒï¼ˆä¾‹: Pendulum-v1ï¼‰
    state_dim = 3
    action_dim = 1
    
    agent = ContinuousPPO(state_dim, action_dim, lr=3e-4)
    
    print(f"State dimension: {state_dim}")
    print(f"Action dimension: {action_dim} (continuous)")
    
    # ã‚µãƒ³ãƒ—ãƒ«çŠ¶æ…‹
    state = np.random.randn(state_dim)
    
    # è¡Œå‹•é¸æŠ
    action, log_prob, value = agent.select_action(state)
    
    print(f"\nSample state: {state}")
    print(f"Sampled action: {action}")
    print(f"Log probability: {log_prob:.4f}")
    print(f"State value: {value:.4f}")
    
    # è¤‡æ•°å›ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆç¢ºç‡çš„ï¼‰
    print("\nMultiple samples from same state:")
    for i in range(5):
        action, _, _ = agent.select_action(state)
        print(f"  Sample {i+1}: action = {action[0]:.4f}")
    
    print("\nâœ“ Gaussian Policyã®ç‰¹å¾´:")
    print("  â€¢ é€£ç¶šè¡Œå‹•ç©ºé–“ã«å¯¾å¿œ")
    print("  â€¢ å¹³å‡Î¼ã¨æ¨™æº–åå·®Ïƒã‚’å­¦ç¿’")
    print("  â€¢ ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡ã€è‡ªå‹•é‹è»¢ãªã©ã«é©ç”¨å¯èƒ½")
    print("  â€¢ æ¢ç´¢ã¯æ¨™æº–åå·®Ïƒã§åˆ¶å¾¡")
    
    # å®Ÿéš›ã®Pendulumç’°å¢ƒã§ã®ä½¿ç”¨ä¾‹
    print("\n=== PPO on Pendulum-v1 (Continuous Control) ===")
    
    import gym
    
    env = gym.make('Pendulum-v1')
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    
    print(f"\nEnvironment: Pendulum-v1")
    print(f"  State space: {env.observation_space}")
    print(f"  Action space: {env.action_space}")
    
    agent = ContinuousPPO(state_dim, action_dim, lr=3e-4)
    
    print(f"\nAgent initialized for continuous control")
    total_params = sum(p.numel() for p in agent.network.parameters())
    print(f"  Total parameters: {total_params:,}")
    
    # 1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ãƒ†ã‚¹ãƒˆ
    state = env.reset()
    episode_reward = 0
    
    for t in range(200):
        action, log_prob, value = agent.select_action(state)
        # Pendulumã®è¡Œå‹•ç¯„å›²ã¯[-2, 2]ãªã®ã§ã€é©åˆ‡ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        action_scaled = np.clip(action, -2.0, 2.0)
        next_state, reward, done, _ = env.step(action_scaled)
    
        episode_reward += reward
        state = next_state
    
    print(f"\nTest episode reward: {episode_reward:.2f}")
    print("\nâœ“ é€£ç¶šåˆ¶å¾¡ã‚¿ã‚¹ã‚¯ã§ã®PPOå‹•ä½œç¢ºèªå®Œäº†")
    

**å‡ºåŠ›ä¾‹** ï¼š
    
    
    === Continuous Action Space PPO ===
    
    State dimension: 3
    Action dimension: 1 (continuous)
    
    Sample state: [ 0.4967 -0.1383  0.6477]
    Sampled action: [0.8732]
    Log probability: -1.2345
    State value: 0.1234
    
    Multiple samples from same state:
      Sample 1: action = 0.7654
      Sample 2: action = 0.9123
      Sample 3: action = 0.8456
      Sample 4: action = 0.8901
      Sample 5: action = 0.8234
    
    âœ“ Gaussian Policyã®ç‰¹å¾´:
      â€¢ é€£ç¶šè¡Œå‹•ç©ºé–“ã«å¯¾å¿œ
      â€¢ å¹³å‡Î¼ã¨æ¨™æº–åå·®Ïƒã‚’å­¦ç¿’
      â€¢ ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡ã€è‡ªå‹•é‹è»¢ãªã©ã«é©ç”¨å¯èƒ½
      â€¢ æ¢ç´¢ã¯æ¨™æº–åå·®Ïƒã§åˆ¶å¾¡
    
    === PPO on Pendulum-v1 (Continuous Control) ===
    
    Environment: Pendulum-v1
      State space: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)
      Action space: Box(-2.0, 2.0, (1,), float32)
    
    Agent initialized for continuous control
      Total parameters: 133,121
    
    Test episode reward: -1234.56
    
    âœ“ é€£ç¶šåˆ¶å¾¡ã‚¿ã‚¹ã‚¯ã§ã®PPOå‹•ä½œç¢ºèªå®Œäº†
    

* * *

## 4.8 ã¾ã¨ã‚ã¨ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯

### æœ¬ç« ã§å­¦ã‚“ã ã“ã¨

ãƒˆãƒ”ãƒƒã‚¯ | é‡è¦ãƒã‚¤ãƒ³ãƒˆ  
---|---  
**Policy Gradient** | æ–¹ç­–ã‚’ç›´æ¥æœ€é©åŒ–ã€é€£ç¶šè¡Œå‹•å¯¾å¿œã€ç¢ºç‡çš„æ–¹ç­–  
**REINFORCE** | æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªPGã€ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã€é«˜åˆ†æ•£  
**Actor-Critic** | Actorã¨Criticã®çµ„ã¿åˆã‚ã›ã€TDå­¦ç¿’ã€ä½åˆ†æ•£  
**A2C** | n-step returnsã€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ã€ä¸¦åˆ—ç’°å¢ƒ  
**PPO** | Clipped objectiveã€å®‰å…¨ãªæ›´æ–°ã€ãƒ‡ãƒ•ã‚¡ã‚¯ãƒˆã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰  
**é€£ç¶šåˆ¶å¾¡** | Gaussian Policyã€Î¼ã¨Ïƒã®å­¦ç¿’ã€ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡  
  
### ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ¯”è¼ƒ

ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  | æ›´æ–° | åˆ†æ•£ | ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ | å®Ÿè£…é›£æ˜“åº¦  
---|---|---|---|---  
**REINFORCE** | ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å¾Œ | é«˜ | ä½ | æ˜“  
**Actor-Critic** | 1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ | ä¸­ | ä¸­ | ä¸­  
**A2C** | n-stepã”ã¨ | ä¸­ | ä¸­ | ä¸­  
**PPO** | ãƒãƒƒãƒï¼ˆè¤‡æ•°epochï¼‰ | ä½ | é«˜ | ä¸­  
  
### ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯

**Trust Region Policy Optimization (TRPO)**

PPOã®å‰èº«ã€‚KL divergenceã§æ–¹ç­–æ›´æ–°ã‚’åˆ¶ç´„ã€‚ç†è«–çš„ä¿è¨¼ãŒå¼·ã„ãŒã€è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ã€‚2æ¬¡æœ€é©åŒ–ã€Fisheræƒ…å ±è¡Œåˆ—ã®è¨ˆç®—ãŒå¿…è¦ã€‚

**Soft Actor-Critic (SAC)**

ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ã®Actor-Criticã€‚ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æœ€å¤§åŒ–ã‚’ç›®çš„é–¢æ•°ã«çµ„ã¿è¾¼ã¿ã€ãƒ­ãƒã‚¹ãƒˆãªå­¦ç¿’ã‚’å®Ÿç¾ã€‚é€£ç¶šåˆ¶å¾¡ã‚¿ã‚¹ã‚¯ã§é«˜æ€§èƒ½ã€‚çµŒé¨“å†ç”Ÿã‚’ä½¿ç”¨ã—ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ãŒé«˜ã„ã€‚

**Deterministic Policy Gradient (DPG / DDPG)**

æ±ºå®šçš„æ–¹ç­–ï¼ˆç¢ºç‡çš„ã§ãªã„ï¼‰ã®Policy Gradientã€‚é€£ç¶šè¡Œå‹•ç©ºé–“ã«ç‰¹åŒ–ã€‚Actor-Criticã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼å­¦ç¿’ã€‚ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡ã§åºƒãä½¿ç”¨ã€‚

**Twin Delayed DDPG (TD3)**

DDPGã®æ”¹è‰¯ç‰ˆã€‚2ã¤ã®Criticãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆTwinï¼‰ã€Actoræ›´æ–°ã®é…å»¶ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ–¹ç­–ã®ãƒã‚¤ã‚ºè¿½åŠ ã€‚éå¤§è©•ä¾¡ãƒã‚¤ã‚¢ã‚¹ã‚’è»½æ¸›ã€‚

**Generalized Advantage Estimation (GAE)**

Advantageã®æ¨å®šæ‰‹æ³•ã€‚Î»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒã‚¤ã‚¢ã‚¹ã¨åˆ†æ•£ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’èª¿æ•´ã€‚TD(Î»)ã®Policy Gradientç‰ˆã€‚PPOã‚„A2Cã§æ¨™æº–çš„ã«ä½¿ç”¨ã€‚

**Multi-Agent Reinforcement Learning (MARL)**

è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å”èª¿ãƒ»ç«¶äº‰å­¦ç¿’ã€‚MAPPOã€QMIXã€MADDPGç­‰ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚ã‚²ãƒ¼ãƒ AIã€ãƒ­ãƒœãƒƒãƒˆç¾¤åˆ¶å¾¡ã€äº¤é€šã‚·ã‚¹ãƒ†ãƒ ã«å¿œç”¨ã€‚

### æ¼”ç¿’å•é¡Œ

#### æ¼”ç¿’ 4.1: REINFORCEã®æ”¹å–„

**èª²é¡Œ** : REINFORCEã«ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆçŠ¶æ…‹ä¾¡å€¤é–¢æ•°ï¼‰ã‚’è¿½åŠ ã—ã€åˆ†æ•£å‰Šæ¸›åŠ¹æœã‚’æ¤œè¨¼ã—ã¦ãã ã•ã„ã€‚

**å®Ÿè£…å†…å®¹** :

  * Criticãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¿½åŠ 
  * Advantage = R_t - V(s_t) ã®è¨ˆç®—
  * ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ã‚Šãƒ»ãªã—ã®å­¦ç¿’æ›²ç·šæ¯”è¼ƒ

#### æ¼”ç¿’ 4.2: A2Cã®ä¸¦åˆ—ç’°å¢ƒå®Ÿè£…

**èª²é¡Œ** : è¤‡æ•°ç’°å¢ƒã‚’ä¸¦åˆ—ã«å®Ÿè¡Œã™ã‚‹A2Cã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚

**å®Ÿè£…è¦ä»¶** :

  * multiprocessingã¾ãŸã¯vectorized environmentsã®ä½¿ç”¨
  * 4ã€œ16å€‹ã®ä¸¦åˆ—ç’°å¢ƒ
  * å­¦ç¿’é€Ÿåº¦ã¨ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ã®æ”¹å–„ã‚’ç¢ºèª

#### æ¼”ç¿’ 4.3: PPOã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

**èª²é¡Œ** : LunarLanderã§PPOã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã—ã¦ãã ã•ã„ã€‚

**èª¿æ•´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿** : epsilon (clip), learning rate, batch_size, epochs, GAE lambda

**è©•ä¾¡** : åæŸé€Ÿåº¦ã€æœ€çµ‚æ€§èƒ½ã€å®‰å®šæ€§

#### æ¼”ç¿’ 4.4: Gaussian Policyã§Pendulumåˆ¶å¾¡

**èª²é¡Œ** : é€£ç¶šåˆ¶å¾¡ã‚¿ã‚¹ã‚¯Pendulum-v1ã‚’PPOã§è§£ã„ã¦ãã ã•ã„ã€‚

**å®Ÿè£…å†…å®¹** :

  * Gaussian Policyã®å®Ÿè£…
  * æ¨™æº–åå·®Ïƒã®æ¸›è¡°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
  * -200ä»¥ä¸Šã®å¹³å‡å ±é…¬ã‚’é”æˆ

#### æ¼”ç¿’ 4.5: Atariã‚²ãƒ¼ãƒ ã¸ã®é©ç”¨

**èª²é¡Œ** : PPOã‚’Atariã‚²ãƒ¼ãƒ ï¼ˆä¾‹: Pongï¼‰ã«é©ç”¨ã—ã¦ãã ã•ã„ã€‚

**å®Ÿè£…è¦ä»¶** :

  * CNNãƒ™ãƒ¼ã‚¹ã®Policy Network
  * ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ï¼ˆ4ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰
  * å ±é…¬ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã€Frame skipping
  * äººé–“ãƒ¬ãƒ™ãƒ«ã®æ€§èƒ½ã‚’ç›®æŒ‡ã™

#### æ¼”ç¿’ 4.6: ã‚«ã‚¹ã‚¿ãƒ ç’°å¢ƒã§ã®å¿œç”¨

**èª²é¡Œ** : è‡ªåˆ†ã§OpenAI Gymç’°å¢ƒã‚’ä½œæˆã—ã€PPOã§å­¦ç¿’ã•ã›ã¦ãã ã•ã„ã€‚

**ä¾‹** :

  * ç°¡å˜ãªè¿·è·¯ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
  * ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ã‚²ãƒ¼ãƒ 
  * ç°¡å˜ãªãƒ­ãƒœãƒƒãƒˆã‚¢ãƒ¼ãƒ åˆ¶å¾¡

**å®Ÿè£…** : gym.Envã‚’ç¶™æ‰¿ã—ãŸç’°å¢ƒã‚¯ãƒ©ã‚¹ã€é©åˆ‡ãªå ±é…¬è¨­è¨ˆã€PPOã§ã®å­¦ç¿’

* * *

### æ¬¡ç« äºˆå‘Š

ç¬¬5ç« ã§ã¯ã€**ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹å¼·åŒ–å­¦ç¿’** ã‚’å­¦ã³ã¾ã™ã€‚ç’°å¢ƒã®ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã€è¨ˆç”»ã¨å­¦ç¿’ã‚’çµ„ã¿åˆã‚ã›ãŸé«˜åº¦ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¢ã‚Šã¾ã™ã€‚

> **æ¬¡ç« ã®ãƒˆãƒ”ãƒƒã‚¯** :  
>  ãƒ»ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹vsãƒ¢ãƒ‡ãƒ«ãƒ•ãƒªãƒ¼  
>  ãƒ»ç’°å¢ƒãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆWorld Modelsï¼‰  
>  ãƒ»Planningæ‰‹æ³•ï¼ˆMCTSã€MuZeroï¼‰  
>  ãƒ»Dyna-Qã€Model-based RL  
>  ãƒ»æƒ³åƒä¸Šã§ã®å­¦ç¿’ï¼ˆDreamerï¼‰  
>  ãƒ»ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ã®å¤§å¹…æ”¹å–„  
>  ãƒ»å®Ÿè£…ï¼šãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°
