<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬3ç« ï¼šãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: var(--font-body); line-height: 1.7; color: var(--color-text); background-color: var(--color-bg); font-size: 16px; }
        header { background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; padding: var(--spacing-xl) var(--spacing-md); margin-bottom: var(--spacing-xl); box-shadow: var(--box-shadow); }
        .header-content { max-width: 900px; margin: 0 auto; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: var(--spacing-sm); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; opacity: 0.95; font-weight: 400; margin-bottom: var(--spacing-md); }
        .meta { display: flex; flex-wrap: wrap; gap: var(--spacing-md); font-size: 0.9rem; opacity: 0.9; }
        .meta-item { display: flex; align-items: center; gap: 0.3rem; }
        .container { max-width: 900px; margin: 0 auto; padding: 0 var(--spacing-md) var(--spacing-xl); }
        h2 { font-size: 1.75rem; color: var(--color-primary); margin-top: var(--spacing-xl); margin-bottom: var(--spacing-md); padding-bottom: var(--spacing-xs); border-bottom: 3px solid var(--color-accent); }
        h3 { font-size: 1.4rem; color: var(--color-primary); margin-top: var(--spacing-lg); margin-bottom: var(--spacing-sm); }
        h4 { font-size: 1.1rem; color: var(--color-primary-dark); margin-top: var(--spacing-md); margin-bottom: var(--spacing-sm); }
        p { margin-bottom: var(--spacing-md); color: var(--color-text); }
        a { color: var(--color-link); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--color-link-hover); text-decoration: underline; }
        ul, ol { margin-left: var(--spacing-lg); margin-bottom: var(--spacing-md); }
        li { margin-bottom: var(--spacing-xs); color: var(--color-text); }
        pre { background-color: var(--color-code-bg); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); overflow-x: auto; margin-bottom: var(--spacing-md); font-family: var(--font-mono); font-size: 0.9rem; line-height: 1.5; }
        code { font-family: var(--font-mono); font-size: 0.9em; background-color: var(--color-code-bg); padding: 0.2em 0.4em; border-radius: 3px; }
        pre code { background-color: transparent; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: var(--spacing-md); font-size: 0.95rem; }
        th, td { border: 1px solid var(--color-border); padding: var(--spacing-sm); text-align: left; }
        th { background-color: var(--color-bg-alt); font-weight: 600; color: var(--color-primary); }
        blockquote { border-left: 4px solid var(--color-accent); padding-left: var(--spacing-md); margin: var(--spacing-md) 0; color: var(--color-text-light); font-style: italic; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        .mermaid { text-align: center; margin: var(--spacing-lg) 0; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        details { background-color: var(--color-bg-alt); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); margin-bottom: var(--spacing-md); }
        summary { cursor: pointer; font-weight: 600; color: var(--color-primary); user-select: none; padding: var(--spacing-xs); margin: calc(-1 * var(--spacing-md)); padding: var(--spacing-md); border-radius: var(--border-radius); }
        summary:hover { background-color: rgba(123, 44, 191, 0.1); }
        details[open] summary { margin-bottom: var(--spacing-md); border-bottom: 1px solid var(--color-border); }
        .navigation { display: flex; justify-content: space-between; gap: var(--spacing-md); margin: var(--spacing-xl) 0; padding-top: var(--spacing-lg); border-top: 2px solid var(--color-border); }
        .nav-button { flex: 1; padding: var(--spacing-md); background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; border-radius: var(--border-radius); text-align: center; font-weight: 600; transition: transform 0.2s, box-shadow 0.2s; box-shadow: var(--box-shadow); }
        .nav-button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); text-decoration: none; }
        footer { margin-top: var(--spacing-xl); padding: var(--spacing-lg) var(--spacing-md); background-color: var(--color-bg-alt); border-top: 1px solid var(--color-border); text-align: center; font-size: 0.9rem; color: var(--color-text-light); }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }

        @media (max-width: 768px) { h1 { font-size: 1.5rem; } h2 { font-size: 1.4rem; } h3 { font-size: 1.2rem; } .meta { font-size: 0.85rem; } .navigation { flex-direction: column; } table { font-size: 0.85rem; } th, td { padding: var(--spacing-xs); } }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/model-evaluation-introduction/chapter3-hyperparameter-tuning.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/model-evaluation-introduction/index.html">Model Evaluation</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬3ç« ï¼šãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</h1>
            <p class="subtitle">ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’æœ€å¤§åŒ–ã™ã‚‹ - Grid Searchã€Random Searchã€Bayesian Optimizationã®å®Ÿè·µ</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 12å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é•ã„ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Grid Searchã¨Random Searchã‚’å®Ÿè£…ã—ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹</li>
<li>âœ… Bayesian Optimizationï¼ˆOptunaï¼‰ã§åŠ¹ç‡çš„ã«æœ€é©åŒ–ã§ãã‚‹</li>
<li>âœ… æ—©æœŸåœæ­¢ã¨Pruningã‚’æ´»ç”¨ã§ãã‚‹</li>
<li>âœ… ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é‡è¦åº¦ã‚’åˆ†æã§ãã‚‹</li>
<li>âœ… XGBoost/LightGBMã®å®Ÿè·µçš„ãªãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã‚’ç¿’å¾—ã™ã‚‹</li>
<li>âœ… è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’è€ƒæ…®ã—ãŸæœ€é©åŒ–æˆ¦ç•¥ã‚’ç«‹ã¦ã‚‰ã‚Œã‚‹</li>
</ul>

<hr>

<h2>3.1 ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã¯</h2>

<h3>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é•ã„</h3>
<p>æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«ã¯2ç¨®é¡ã®èª¿æ•´å¯èƒ½ãªå€¤ãŒã‚ã‚Šã¾ã™ï¼š</p>

<blockquote>
<p>ã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å­¦ç¿’ã«ã‚ˆã£ã¦è‡ªå‹•çš„ã«æœ€é©åŒ–ã•ã‚Œã‚‹ã€‚ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯äººé–“ãŒäº‹å‰ã«è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€</p>
</blockquote>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</th>
<th>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å®šç¾©</strong></td>
<td>å­¦ç¿’éç¨‹ã§æœ€é©åŒ–ã•ã‚Œã‚‹å€¤</td>
<td>å­¦ç¿’å‰ã«è¨­å®šã™ã‚‹å€¤</td>
</tr>
<tr>
<td><strong>ä¾‹ï¼ˆç·šå½¢å›å¸°ï¼‰</strong></td>
<td>é‡ã¿ $w$ã€ãƒã‚¤ã‚¢ã‚¹ $b$</td>
<td>æ­£å‰‡åŒ–ä¿‚æ•° $\alpha$</td>
</tr>
<tr>
<td><strong>ä¾‹ï¼ˆæ±ºå®šæœ¨ï¼‰</strong></td>
<td>åˆ†å‰²ç‚¹ã®é–¾å€¤</td>
<td>æœ€å¤§æ·±ã•ã€æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°</td>
</tr>
<tr>
<td><strong>ä¾‹ï¼ˆNNï¼‰</strong></td>
<td>å„å±¤ã®é‡ã¿è¡Œåˆ—</td>
<td>å­¦ç¿’ç‡ã€å±¤æ•°ã€ãƒ¦ãƒ‹ãƒƒãƒˆæ•°</td>
</tr>
<tr>
<td><strong>æœ€é©åŒ–æ–¹æ³•</strong></td>
<td>å‹¾é…é™ä¸‹æ³•ãªã©</td>
<td>æ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆæœ¬ç« ã®ãƒ†ãƒ¼ãƒï¼‰</td>
</tr>
</tbody>
</table>

<h3>ä¸»è¦ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¾‹</h3>

<div class="mermaid">
graph TD
    A[ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿] --> B[ãƒ¢ãƒ‡ãƒ«æ§‹é€ ]
    A --> C[å­¦ç¿’åˆ¶å¾¡]
    A --> D[æ­£å‰‡åŒ–]

    B --> B1[æ±ºå®šæœ¨: max_depth<br/>NN: layers, units]
    B --> B2[SVM: kernel<br/>Random Forest: n_estimators]

    C --> C1[å­¦ç¿’ç‡<br/>learning_rate]
    C --> C2[ãƒãƒƒãƒã‚µã‚¤ã‚º<br/>ã‚¨ãƒãƒƒã‚¯æ•°]

    D --> D1[L1/L2æ­£å‰‡åŒ–<br/>alpha, lambda]
    D --> D2[Dropoutç‡<br/>Early Stopping]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
</div>

<h3>ãªãœãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒé‡è¦ã‹</h3>

<ul>
<li><strong>æ€§èƒ½å‘ä¸Š</strong>: é©åˆ‡ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ç²¾åº¦ãŒ10-20%å‘ä¸Šã™ã‚‹ã“ã¨ã‚‚</li>
<li><strong>éå­¦ç¿’ã®é˜²æ­¢</strong>: æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æ±åŒ–æ€§èƒ½ã‚’æ”¹å–„</li>
<li><strong>è¨ˆç®—åŠ¹ç‡</strong>: æœ€é©ãªå­¦ç¿’ç‡ã§å­¦ç¿’æ™‚é–“ã‚’çŸ­ç¸®</li>
<li><strong>ãƒ¢ãƒ‡ãƒ«ã®å®‰å®šæ€§</strong>: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šã§äºˆæ¸¬ã®ä¿¡é ¼æ€§ãŒå‘ä¸Š</li>
</ul>

<hr>

<h2>3.2 Grid Searchï¼ˆã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒï¼‰</h2>

<h3>æ¦‚è¦</h3>

<p><strong>Grid Search</strong>ã¯ã€æŒ‡å®šã—ãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å…¨çµ„åˆã›ã‚’ç¶²ç¾…çš„ã«æ¢ç´¢ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<p>æ¢ç´¢ã™ã‚‹çµ„åˆã›æ•°ï¼š</p>
<p>$$
N_{\text{total}} = \prod_{i=1}^{k} N_i
$$</p>
<p>ã“ã“ã§ã€$k$ ã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã€$N_i$ ã¯å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å€™è£œæ•°ã§ã™ã€‚</p>

<h3>Grid Searchã®æµã‚Œ</h3>

<div class="mermaid">
graph LR
    A[ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“<br/>å®šç¾©] --> B[ã‚°ãƒªãƒƒãƒ‰ç”Ÿæˆ<br/>å…¨çµ„åˆã›]
    B --> C[äº¤å·®æ¤œè¨¼<br/>å„çµ„åˆã›ã‚’è©•ä¾¡]
    C --> D[æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿<br/>é¸æŠ]
    D --> E[å†å­¦ç¿’<br/>å…¨ãƒ‡ãƒ¼ã‚¿ã§]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#ffe0b2
</div>

<h3>å®Ÿè£…ä¾‹: GridSearchCVã®åŸºæœ¬</h3>

<pre><code class="language-python">import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import time

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
data = load_breast_cancer()
X, y = data.data, data.target

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("=== Grid Search: Random Forest ===\n")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰å®šç¾©
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# ç·çµ„åˆã›æ•°
total_combinations = (
    len(param_grid['n_estimators']) *
    len(param_grid['max_depth']) *
    len(param_grid['min_samples_split']) *
    len(param_grid['min_samples_leaf'])
)
print(f"æ¢ç´¢ã™ã‚‹çµ„åˆã›æ•°: {total_combinations}")
print(f"äº¤å·®æ¤œè¨¼æŠ˜æ•°: 5")
print(f"ç·ãƒ•ã‚£ãƒƒãƒˆå›æ•°: {total_combinations * 5}\n")

# Grid Searchå®Ÿè¡Œ
start_time = time.time()

grid_search = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42, n_jobs=-1),
    param_grid=param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1,
    return_train_score=True
)

grid_search.fit(X_train_scaled, y_train)

elapsed_time = time.time() - start_time

print(f"\næ¢ç´¢æ™‚é–“: {elapsed_time:.2f}ç§’\n")

# çµæœè¡¨ç¤º
print("=== æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===")
for param, value in grid_search.best_params_.items():
    print(f"{param}: {value}")

print(f"\næœ€è‰¯CVç²¾åº¦: {grid_search.best_score_:.4f}")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç²¾åº¦: {grid_search.score(X_test_scaled, y_test):.4f}")

# Top 5 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¡¨ç¤º
print("\n=== Top 5 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ===")
results = grid_search.cv_results_
indices = np.argsort(results['mean_test_score'])[::-1][:5]

for i, idx in enumerate(indices, 1):
    print(f"\n{i}ä½: CVç²¾åº¦={results['mean_test_score'][idx]:.4f} "
          f"(Â±{results['std_test_score'][idx]:.4f})")
    print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {results['params'][idx]}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Grid Search: Random Forest ===

æ¢ç´¢ã™ã‚‹çµ„åˆã›æ•°: 144
äº¤å·®æ¤œè¨¼æŠ˜æ•°: 5
ç·ãƒ•ã‚£ãƒƒãƒˆå›æ•°: 720

Fitting 5 folds for each of 144 candidates, totalling 720 fits

æ¢ç´¢æ™‚é–“: 28.45ç§’

=== æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
n_estimators: 200
max_depth: 15
min_samples_split: 2
min_samples_leaf: 1

æœ€è‰¯CVç²¾åº¦: 0.9736
ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç²¾åº¦: 0.9737

=== Top 5 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ===

1ä½: CVç²¾åº¦=0.9736 (Â±0.0124)
  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {'n_estimators': 200, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 1}

2ä½: CVç²¾åº¦=0.9714 (Â±0.0145)
  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1}

3ä½: CVç²¾åº¦=0.9714 (Â±0.0167)
  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1}

4ä½: CVç²¾åº¦=0.9692 (Â±0.0156)
  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}

5ä½: CVç²¾åº¦=0.9692 (Â±0.0189)
  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {'n_estimators': 100, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 1}
</code></pre>

<h3>Grid Searchã®ç‰¹å¾´</h3>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>ãƒ¡ãƒªãƒƒãƒˆ</th>
<th>ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ¢ç´¢ã®å®Œå…¨æ€§</strong></td>
<td>æŒ‡å®šç¯„å›²å†…ã§æœ€è‰¯ã®çµ„åˆã›ã‚’ç¢ºå®Ÿã«ç™ºè¦‹</td>
<td>æ¬¡å…ƒãŒå¢—ãˆã‚‹ã¨çµ„åˆã›çˆ†ç™º</td>
</tr>
<tr>
<td><strong>å®Ÿè£…ã®ç°¡å˜ã•</strong></td>
<td>ã‚³ãƒ¼ãƒ‰ãŒã‚·ãƒ³ãƒ—ãƒ«ã§ç†è§£ã—ã‚„ã™ã„</td>
<td>-</td>
</tr>
<tr>
<td><strong>ä¸¦åˆ—åŒ–</strong></td>
<td>å„çµ„åˆã›ã‚’ç‹¬ç«‹ã«è©•ä¾¡å¯èƒ½</td>
<td>-</td>
</tr>
<tr>
<td><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆ</strong></td>
<td>-</td>
<td>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå¤šã„ã¨éç¾å®Ÿçš„</td>
</tr>
<tr>
<td><strong>æ¢ç´¢åŠ¹ç‡</strong></td>
<td>-</td>
<td>ç„¡é§„ãªæ¢ç´¢ãŒå¤šã„</td>
</tr>
</tbody>
</table>

<h3>è¨ˆç®—ã‚³ã‚¹ãƒˆã®å•é¡Œ</h3>

<p>ä¾‹ï¼š5ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãã‚Œãã‚Œ10å€‹ã®å€™è£œå€¤ã€5-fold CVã®å ´åˆï¼š</p>
<p>$$
N_{\text{fits}} = 10^5 \times 5 = 500,000 \text{ ãƒ•ã‚£ãƒƒãƒˆ}
$$</p>

<p>1ãƒ•ã‚£ãƒƒãƒˆã«1ç§’ã‹ã‹ã‚‹å ´åˆã€ç´„139æ™‚é–“ï¼ˆ5.8æ—¥ï¼‰å¿…è¦ã«ãªã‚Šã¾ã™ã€‚</p>

<hr>

<h2>3.3 Random Searchï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒï¼‰</h2>

<h3>æ¦‚è¦</h3>

<p><strong>Random Search</strong>ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦æ¢ç´¢ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<blockquote>
<p>ã€ŒBergstra & Bengio (2012)ã®ç ”ç©¶ã«ã‚ˆã‚Œã°ã€Random Searchã¯Grid Searchã‚ˆã‚Šã‚‚åŠ¹ç‡çš„ã«è‰¯ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç™ºè¦‹ã§ãã‚‹ã“ã¨ãŒå¤šã„ã€</p>
</blockquote>

<h3>ãªãœRandom SearchãŒåŠ¹ç‡çš„ã‹</h3>

<p>å¤šãã®å ´åˆã€ä¸€éƒ¨ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ãŒæ€§èƒ½ã«å¤§ããå½±éŸ¿ã—ã¾ã™ã€‚Random Searchã¯é‡è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’ã‚ˆã‚Šåºƒãã‚«ãƒãƒ¼ã§ãã¾ã™ã€‚</p>

<div class="mermaid">
graph TD
    A[Grid Search<br/>9å›æ¢ç´¢] --> B[2ã¤ã®é‡è¦<br/>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿]
    A --> C[å„3ç‚¹ãšã¤<br/>æ¢ç´¢]

    D[Random Search<br/>9å›æ¢ç´¢] --> E[2ã¤ã®é‡è¦<br/>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿]
    D --> F[å„9ç‚¹ã‚’<br/>æ¢ç´¢å¯èƒ½]

    style A fill:#ffcdd2
    style D fill:#c8e6c9
    style B fill:#fff3e0
    style E fill:#fff3e0
</div>

<h3>å®Ÿè£…ä¾‹: RandomizedSearchCVã®åŸºæœ¬</h3>

<pre><code class="language-python">from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

print("=== Random Search: Random Forest ===\n")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†å¸ƒå®šç¾©
param_distributions = {
    'n_estimators': randint(50, 300),        # 50-299ã®æ•´æ•°
    'max_depth': [5, 10, 15, 20, None],      # é›¢æ•£å€¤
    'min_samples_split': randint(2, 20),     # 2-19ã®æ•´æ•°
    'min_samples_leaf': randint(1, 10),      # 1-9ã®æ•´æ•°
    'max_features': uniform(0.1, 0.9)        # 0.1-1.0ã®é€£ç¶šå€¤
}

# Random Searchå®Ÿè¡Œï¼ˆ144å›ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° - Grid Searchã¨åŒã˜å›æ•°ï¼‰
start_time = time.time()

random_search = RandomizedSearchCV(
    estimator=RandomForestClassifier(random_state=42, n_jobs=-1),
    param_distributions=param_distributions,
    n_iter=144,  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å›æ•°
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1,
    random_state=42,
    return_train_score=True
)

random_search.fit(X_train_scaled, y_train)

elapsed_time = time.time() - start_time

print(f"\næ¢ç´¢æ™‚é–“: {elapsed_time:.2f}ç§’\n")

# çµæœè¡¨ç¤º
print("=== æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===")
for param, value in random_search.best_params_.items():
    print(f"{param}: {value}")

print(f"\næœ€è‰¯CVç²¾åº¦: {random_search.best_score_:.4f}")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç²¾åº¦: {random_search.score(X_test_scaled, y_test):.4f}")

# Grid Searchã¨Random Searchã®æ¯”è¼ƒ
print("\n=== Grid Search vs Random Search ===")
print(f"Grid Search   - CVç²¾åº¦: {grid_search.best_score_:.4f}, æ™‚é–“: {28.45:.2f}ç§’")
print(f"Random Search - CVç²¾åº¦: {random_search.best_score_:.4f}, æ™‚é–“: {elapsed_time:.2f}ç§’")

# æ¢ç´¢éç¨‹ã‚’å¯è¦–åŒ–
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Grid Searchã®çµæœ
grid_scores = grid_search.cv_results_['mean_test_score']
axes[0].hist(grid_scores, bins=30, edgecolor='black', alpha=0.7, color='steelblue')
axes[0].axvline(grid_search.best_score_, color='red', linestyle='--', linewidth=2,
                label=f'Best: {grid_search.best_score_:.4f}')
axes[0].set_xlabel('CVç²¾åº¦', fontsize=12)
axes[0].set_ylabel('é »åº¦', fontsize=12)
axes[0].set_title('Grid Search: ã‚¹ã‚³ã‚¢åˆ†å¸ƒ', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Random Searchã®çµæœ
random_scores = random_search.cv_results_['mean_test_score']
axes[1].hist(random_scores, bins=30, edgecolor='black', alpha=0.7, color='green')
axes[1].axvline(random_search.best_score_, color='red', linestyle='--', linewidth=2,
                label=f'Best: {random_search.best_score_:.4f}')
axes[1].set_xlabel('CVç²¾åº¦', fontsize=12)
axes[1].set_ylabel('é »åº¦', fontsize=12)
axes[1].set_title('Random Search: ã‚¹ã‚³ã‚¢åˆ†å¸ƒ', fontsize=14)
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Random Search: Random Forest ===

Fitting 5 folds for each of 144 candidates, totalling 720 fits

æ¢ç´¢æ™‚é–“: 26.78ç§’

=== æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
n_estimators: 267
max_depth: 20
min_samples_split: 2
min_samples_leaf: 1
max_features: 0.7845

æœ€è‰¯CVç²¾åº¦: 0.9758
ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç²¾åº¦: 0.9825

=== Grid Search vs Random Search ===
Grid Search   - CVç²¾åº¦: 0.9736, æ™‚é–“: 28.45ç§’
Random Search - CVç²¾åº¦: 0.9758, æ™‚é–“: 26.78ç§’
</code></pre>

<h3>Random Searchã®åˆ©ç‚¹</h3>

<ul>
<li><strong>æ¢ç´¢åŠ¹ç‡</strong>: é‡è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’ã‚ˆã‚Šåºƒãã‚«ãƒãƒ¼</li>
<li><strong>æŸ”è»Ÿæ€§</strong>: é€£ç¶šå€¤ã®åˆ†å¸ƒã‚’ç›´æ¥æŒ‡å®šå¯èƒ½</li>
<li><strong>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£</strong>: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å›æ•°ã§è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’åˆ¶å¾¡</li>
<li><strong>æ–°ã—ã„ç™ºè¦‹</strong>: Grid Searchã§ã¯è©¦ã•ãªã„çµ„åˆã›ã‚’ç™ºè¦‹</li>
</ul>

<h3>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†å¸ƒã®é¸ã³æ–¹</h3>

<table>
<thead>
<tr>
<th>åˆ†å¸ƒ</th>
<th>ä½¿ç”¨å ´é¢</th>
<th>ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>randint(low, high)</code></td>
<td>æ•´æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</td>
<td><code>n_estimators</code>, <code>max_depth</code></td>
</tr>
<tr>
<td><code>uniform(low, high)</code></td>
<td>é€£ç¶šå€¤ï¼ˆç·šå½¢ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰</td>
<td><code>max_features</code>, <code>subsample</code></td>
</tr>
<tr>
<td><code>loguniform(low, high)</code></td>
<td>é€£ç¶šå€¤ï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰</td>
<td><code>learning_rate</code>, <code>alpha</code></td>
</tr>
<tr>
<td>ãƒªã‚¹ãƒˆ</td>
<td>é›¢æ•£çš„ãªé¸æŠè‚¢</td>
<td><code>kernel=['rbf', 'poly']</code></td>
</tr>
</tbody>
</table>

<hr>

<h2>3.4 Bayesian Optimizationï¼ˆãƒ™ã‚¤ã‚ºæœ€é©åŒ–ï¼‰</h2>

<h3>æ¦‚è¦</h3>

<p><strong>Bayesian Optimization</strong>ã¯ã€éå»ã®è©•ä¾¡çµæœã‚’æ´»ç”¨ã—ã¦ã€æ¬¡ã«è©¦ã™ã¹ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è³¢ãé¸æŠã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<blockquote>
<p>ã€ŒRandom Searchã¯éå»ã®æƒ…å ±ã‚’ä½¿ã‚ãªã„ã€‚Bayesian Optimizationã¯å­¦ç¿’ã—ãªãŒã‚‰æ¢ç´¢ã™ã‚‹ã€</p>
</blockquote>

<h3>åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢</h3>

<div class="mermaid">
graph LR
    A[åˆæœŸæ¢ç´¢<br/>ãƒ©ãƒ³ãƒ€ãƒ ] --> B[ä»£ç†ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰<br/>Surrogate Model]
    B --> C[ç²å¾—é–¢æ•°<br/>Acquisition]
    C --> D[æ¬¡ã®å€™è£œé¸æŠ<br/>æœ‰æœ›ãªé ˜åŸŸ]
    D --> E[è©•ä¾¡<br/>å®Ÿéš›ã«å­¦ç¿’]
    E --> B

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#ffe0b2
</div>

<h3>ä¸»è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</h3>

<ol>
<li><strong>ä»£ç†ãƒ¢ãƒ‡ãƒ«ï¼ˆSurrogate Modelï¼‰</strong>: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨æ€§èƒ½ã®é–¢ä¿‚ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–
<ul>
<li>ã‚¬ã‚¦ã‚¹éç¨‹ï¼ˆGaussian Processï¼‰</li>
<li>TPEï¼ˆTree-structured Parzen Estimatorï¼‰â† Optunaã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ</li>
</ul>
</li>
<li><strong>ç²å¾—é–¢æ•°ï¼ˆAcquisition Functionï¼‰</strong>: æ¬¡ã«è©•ä¾¡ã™ã¹ãç‚¹ã‚’æ±ºå®š
<ul>
<li>EIï¼ˆExpected Improvementï¼‰: æ”¹å–„ã®æœŸå¾…å€¤</li>
<li>UCBï¼ˆUpper Confidence Boundï¼‰: ä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®</li>
<li>PIï¼ˆProbability of Improvementï¼‰: æ”¹å–„ç¢ºç‡</li>
</ul>
</li>
</ol>

<h3>å®Ÿè£…ä¾‹: Optunaã«ã‚ˆã‚‹ãƒ™ã‚¤ã‚ºæœ€é©åŒ–</h3>

<pre><code class="language-python">import optuna
from optuna.samplers import TPESampler

print("=== Bayesian Optimization with Optuna ===\n")

# ç›®çš„é–¢æ•°ã®å®šç¾©
def objective(trial):
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ææ¡ˆ
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'max_depth': trial.suggest_int('max_depth', 5, 30),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
        'max_features': trial.suggest_float('max_features', 0.1, 1.0),
        'random_state': 42,
        'n_jobs': -1
    }

    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã¨è©•ä¾¡
    model = RandomForestClassifier(**params)
    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')

    return scores.mean()

# Optuna Studyä½œæˆ
study = optuna.create_study(
    direction='maximize',
    sampler=TPESampler(seed=42)
)

# æœ€é©åŒ–å®Ÿè¡Œ
start_time = time.time()
study.optimize(objective, n_trials=100, show_progress_bar=True)
elapsed_time = time.time() - start_time

print(f"\næ¢ç´¢æ™‚é–“: {elapsed_time:.2f}ç§’\n")

# çµæœè¡¨ç¤º
print("=== æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===")
for param, value in study.best_params.items():
    print(f"{param}: {value}")

print(f"\næœ€è‰¯CVç²¾åº¦: {study.best_value:.4f}")

# æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è©•ä¾¡
best_model = RandomForestClassifier(**study.best_params, random_state=42, n_jobs=-1)
best_model.fit(X_train_scaled, y_train)
test_score = best_model.score(X_test_scaled, y_test)
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç²¾åº¦: {test_score:.4f}")

# 3æ‰‹æ³•ã®æ¯”è¼ƒ
print("\n=== 3æ‰‹æ³•ã®æ¯”è¼ƒ ===")
print(f"Grid Search       - CVç²¾åº¦: {grid_search.best_score_:.4f}, Trialæ•°: 144, æ™‚é–“: 28.45ç§’")
print(f"Random Search     - CVç²¾åº¦: {random_search.best_score_:.4f}, Trialæ•°: 144, æ™‚é–“: 26.78ç§’")
print(f"Bayesian Opt.     - CVç²¾åº¦: {study.best_value:.4f}, Trialæ•°: 100, æ™‚é–“: {elapsed_time:.2f}ç§’")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Bayesian Optimization with Optuna ===

[I 2025-10-21 14:32:10,123] A new study created in memory with name: no-name-1
[I 2025-10-21 14:32:12,456] Trial 0 finished with value: 0.9648 and parameters: {'n_estimators': 189, 'max_depth': 18, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 0.6234}
...
[I 2025-10-21 14:33:45,789] Trial 99 finished with value: 0.9780 and parameters: {'n_estimators': 245, 'max_depth': 22, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 0.8123}

æ¢ç´¢æ™‚é–“: 18.92ç§’

=== æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
n_estimators: 245
max_depth: 22
min_samples_split: 2
min_samples_leaf: 1
max_features: 0.8123

æœ€è‰¯CVç²¾åº¦: 0.9780
ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç²¾åº¦: 0.9825

=== 3æ‰‹æ³•ã®æ¯”è¼ƒ ===
Grid Search       - CVç²¾åº¦: 0.9736, Trialæ•°: 144, æ™‚é–“: 28.45ç§’
Random Search     - CVç²¾åº¦: 0.9758, Trialæ•°: 144, æ™‚é–“: 26.78ç§’
Bayesian Opt.     - CVç²¾åº¦: 0.9780, Trialæ•°: 100, æ™‚é–“: 18.92ç§’
</code></pre>

<h3>Optunaã®æœ€é©åŒ–éç¨‹ã‚’å¯è¦–åŒ–</h3>

<pre><code class="language-python">import plotly.io as pio
pio.renderers.default = 'browser'

# æœ€é©åŒ–å±¥æ­´
fig1 = optuna.visualization.plot_optimization_history(study)
fig1.update_layout(title='æœ€é©åŒ–å±¥æ­´: è©¦è¡Œã”ã¨ã®CVç²¾åº¦', width=900, height=500)
fig1.show()

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡è¦åº¦
fig2 = optuna.visualization.plot_param_importances(study)
fig2.update_layout(title='ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡è¦åº¦', width=900, height=500)
fig2.show()

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é–“ã®é–¢ä¿‚
fig3 = optuna.visualization.plot_contour(study, params=['n_estimators', 'max_depth'])
fig3.update_layout(title='ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é–“ã®ç›¸äº’ä½œç”¨', width=900, height=500)
fig3.show()

# ã‚¹ãƒ©ã‚¤ã‚¹ãƒ—ãƒ­ãƒƒãƒˆï¼ˆå„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ï¼‰
fig4 = optuna.visualization.plot_slice(study)
fig4.update_layout(title='å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿', width=900, height=600)
fig4.show()
</code></pre>

<h3>Bayesian Optimizationã®ç‰¹å¾´</h3>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>Grid/Random Search</th>
<th>Bayesian Optimization</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ¢ç´¢æˆ¦ç•¥</strong></td>
<td>éå»ã®æƒ…å ±ã‚’ä½¿ã‚ãªã„</td>
<td>éå»ã®çµæœã‹ã‚‰å­¦ç¿’</td>
</tr>
<tr>
<td><strong>åŠ¹ç‡æ€§</strong></td>
<td>ç„¡é§„ãªæ¢ç´¢ãŒå¤šã„</td>
<td>æœ‰æœ›ãªé ˜åŸŸã‚’é›†ä¸­æ¢ç´¢</td>
</tr>
<tr>
<td><strong>è©¦è¡Œå›æ•°</strong></td>
<td>å¤šãã®è©¦è¡ŒãŒå¿…è¦</td>
<td>å°‘ãªã„è©¦è¡Œã§é«˜ç²¾åº¦</td>
</tr>
<tr>
<td><strong>å®Ÿè£…ã®è¤‡é›‘ã•</strong></td>
<td>ã‚·ãƒ³ãƒ—ãƒ«</td>
<td>ã‚„ã‚„è¤‡é›‘ï¼ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨ã§ç°¡å˜ï¼‰</td>
</tr>
<tr>
<td><strong>ä¸¦åˆ—åŒ–</strong></td>
<td>å®Œå…¨ã«ç‹¬ç«‹</td>
<td>åˆ¶é™ã‚ã‚Šï¼ˆæœ€è¿‘ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯å¯¾å¿œï¼‰</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.5 æ—©æœŸåœæ­¢ã¨Pruning</h2>

<h3>æ—©æœŸåœæ­¢ï¼ˆEarly Stoppingï¼‰</h3>

<p><strong>æ—©æœŸåœæ­¢</strong>ã¯ã€å­¦ç¿’éç¨‹ã§ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ãŒæ”¹å–„ã—ãªããªã£ãŸã‚‰å­¦ç¿’ã‚’æ‰“ã¡åˆ‡ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<h3>Pruningï¼ˆæåˆˆã‚Šï¼‰</h3>

<p><strong>Pruning</strong>ã¯ã€Optunaã®æ©Ÿèƒ½ã§ã€è¦‹è¾¼ã¿ã®ãªã„è©¦è¡Œã‚’é€”ä¸­ã§æ‰“ã¡åˆ‡ã‚‹ã“ã¨ã§æ¢ç´¢ã‚’é«˜é€ŸåŒ–ã—ã¾ã™ã€‚</p>

<div class="mermaid">
graph TD
    A[Trialé–‹å§‹] --> B[Epoch 1è©•ä¾¡]
    B --> C{æœ‰æœ›?}
    C -->|Yes| D[Epoch 2è©•ä¾¡]
    C -->|No| E[Pruning<br/>è©¦è¡Œä¸­æ­¢]
    D --> F{æœ‰æœ›?}
    F -->|Yes| G[ç¶™ç¶š...]
    F -->|No| E
    G --> H[å®Œäº†]

    style A fill:#e3f2fd
    style E fill:#ffcdd2
    style H fill:#c8e6c9
</graph>
</div>

<h3>å®Ÿè£…ä¾‹: Optunaã§ã®æ—©æœŸåœæ­¢ã¨Pruning</h3>

<pre><code class="language-python">from xgboost import XGBClassifier
from optuna.pruners import MedianPruner

print("=== Optuna with Pruning: XGBoost ===\n")

# Pruningå¯¾å¿œã®ç›®çš„é–¢æ•°
def objective_with_pruning(trial):
    params = {
        'n_estimators': 1000,  # å¤§ããªå€¤ã«è¨­å®š
        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'gamma': trial.suggest_float('gamma', 0, 5),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10, log=True),
        'random_state': 42,
        'n_jobs': -1,
        'eval_metric': 'logloss'
    }

    # æ—©æœŸåœæ­¢ã®è¨­å®š
    model = XGBClassifier(**params)

    # è¨“ç·´ãƒ»ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³åˆ†å‰²
    X_tr, X_val, y_tr, y_val = train_test_split(
        X_train_scaled, y_train, test_size=0.2, random_state=42
    )

    # æ—©æœŸåœæ­¢ä»˜ãã§å­¦ç¿’
    model.fit(
        X_tr, y_tr,
        eval_set=[(X_val, y_val)],
        early_stopping_rounds=50,
        verbose=False
    )

    # Pruningã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆä¸­é–“å€¤ã‚’å ±å‘Šï¼‰
    for step in range(0, model.best_iteration, 50):
        intermediate_score = model.score(X_val, y_val)
        trial.report(intermediate_score, step)

        # Pruningãƒã‚§ãƒƒã‚¯
        if trial.should_prune():
            raise optuna.TrialPruned()

    # æœ€çµ‚ã‚¹ã‚³ã‚¢
    return model.score(X_val, y_val)

# MedianPrunerã‚’ä½¿ç”¨
pruner = MedianPruner(
    n_startup_trials=10,      # æœ€åˆã®10è©¦è¡Œã¯Pruningã—ãªã„
    n_warmup_steps=0,         # å³åº§ã«Pruningåˆ¤å®šé–‹å§‹
    interval_steps=1          # æ¯ã‚¹ãƒ†ãƒƒãƒ—ã§Pruningåˆ¤å®š
)

study_with_pruning = optuna.create_study(
    direction='maximize',
    sampler=TPESampler(seed=42),
    pruner=pruner
)

# æœ€é©åŒ–å®Ÿè¡Œ
start_time = time.time()
study_with_pruning.optimize(objective_with_pruning, n_trials=100, show_progress_bar=True)
elapsed_time = time.time() - start_time

print(f"\næ¢ç´¢æ™‚é–“: {elapsed_time:.2f}ç§’")
print(f"å®Œäº†ã—ãŸè©¦è¡Œæ•°: {len([t for t in study_with_pruning.trials if t.state == optuna.trial.TrialState.COMPLETE])}")
print(f"Pruningã•ã‚ŒãŸè©¦è¡Œæ•°: {len([t for t in study_with_pruning.trials if t.state == optuna.trial.TrialState.PRUNED])}")

# æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
print("\n=== æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===")
for param, value in study_with_pruning.best_params.items():
    print(f"{param}: {value}")

print(f"\næœ€è‰¯ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç²¾åº¦: {study_with_pruning.best_value:.4f}")

# Pruningãªã—ã¨ã®æ¯”è¼ƒ
study_without_pruning = optuna.create_study(
    direction='maximize',
    sampler=TPESampler(seed=42)
)

start_time = time.time()
study_without_pruning.optimize(objective_with_pruning, n_trials=100, show_progress_bar=True)
elapsed_time_no_pruning = time.time() - start_time

print("\n=== Pruningã®åŠ¹æœ ===")
print(f"Pruningã‚ã‚Š: {elapsed_time:.2f}ç§’")
print(f"Pruningãªã—: {elapsed_time_no_pruning:.2f}ç§’")
print(f"é«˜é€ŸåŒ–ç‡: {elapsed_time_no_pruning / elapsed_time:.2f}x")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Optuna with Pruning: XGBoost ===

[I 2025-10-21 14:35:12,345] Trial 0 finished with value: 0.9670 and parameters: {...}
...
[I 2025-10-21 14:36:45,678] Trial 23 pruned.
...
[I 2025-10-21 14:38:20,123] Trial 99 finished with value: 0.9835 and parameters: {...}

æ¢ç´¢æ™‚é–“: 187.45ç§’
å®Œäº†ã—ãŸè©¦è¡Œæ•°: 68
Pruningã•ã‚ŒãŸè©¦è¡Œæ•°: 32

=== æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
learning_rate: 0.0523
max_depth: 6
min_child_weight: 2
subsample: 0.8234
colsample_bytree: 0.7654
gamma: 1.234
reg_alpha: 0.0123
reg_lambda: 2.345

æœ€è‰¯ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç²¾åº¦: 0.9835

=== Pruningã®åŠ¹æœ ===
Pruningã‚ã‚Š: 187.45ç§’
Pruningãªã—: 314.23ç§’
é«˜é€ŸåŒ–ç‡: 1.68x
</code></pre>

<h3>Pruningã®ç¨®é¡</h3>

<table>
<thead>
<tr>
<th>Pruner</th>
<th>åˆ¤å®šåŸºæº–</th>
<th>é©ç”¨å ´é¢</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>MedianPruner</code></td>
<td>ä¸­å¤®å€¤ã‚ˆã‚Šæ‚ªã„è©¦è¡Œã‚’æ‰“ã¡åˆ‡ã‚Š</td>
<td>æ±ç”¨çš„ã€ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„</td>
</tr>
<tr>
<td><code>PercentilePruner</code></td>
<td>ä¸‹ä½X%ã®è©¦è¡Œã‚’æ‰“ã¡åˆ‡ã‚Š</td>
<td>ã‚ˆã‚Šç©æ¥µçš„ãªæåˆˆã‚Š</td>
</tr>
<tr>
<td><code>SuccessiveHalvingPruner</code></td>
<td>æ®µéšçš„ã«å€™è£œã‚’çµã‚Šè¾¼ã‚€</td>
<td>ä¸¦åˆ—æœ€é©åŒ–ã«é©ã—ã¦ã„ã‚‹</td>
</tr>
<tr>
<td><code>HyperbandPruner</code></td>
<td>Successive Halvingã®æ”¹è‰¯ç‰ˆ</td>
<td>æœ€å…ˆç«¯ã®æ‰‹æ³•</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.6 ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡è¦åº¦åˆ†æ</h2>

<h3>æ¦‚è¦</h3>

<p>ã©ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ€§èƒ½ã«æœ€ã‚‚å½±éŸ¿ã™ã‚‹ã‹ã‚’ç†è§£ã™ã‚‹ã“ã¨ã¯ã€åŠ¹ç‡çš„ãªãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä¸å¯æ¬ ã§ã™ã€‚</p>

<h3>å®Ÿè£…ä¾‹: Optunaã§ã®é‡è¦åº¦åˆ†æ</h3>

<pre><code class="language-python">import matplotlib.pyplot as plt
import pandas as pd

print("=== ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡è¦åº¦åˆ†æ ===\n")

# é‡è¦åº¦ã‚’è¨ˆç®—
importance = optuna.importance.get_param_importances(study_with_pruning)

print("=== é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° ===")
for param, imp in importance.items():
    print(f"{param}: {imp:.4f}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. é‡è¦åº¦ãƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ
params = list(importance.keys())
values = list(importance.values())
colors = plt.cm.viridis(np.linspace(0, 1, len(params)))

axes[0, 0].barh(params, values, color=colors)
axes[0, 0].set_xlabel('é‡è¦åº¦', fontsize=12)
axes[0, 0].set_title('ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡è¦åº¦', fontsize=14)
axes[0, 0].grid(True, alpha=0.3, axis='x')

# 2. æœ€é©åŒ–å±¥æ­´ï¼ˆç´¯ç©æœ€è‰¯å€¤ï¼‰
trials_df = study_with_pruning.trials_dataframe()
trials_df = trials_df[trials_df['state'] == 'COMPLETE']
trials_df['best_value_so_far'] = trials_df['value'].cummax()

axes[0, 1].plot(trials_df['number'], trials_df['value'], 'o', alpha=0.3,
                label='å„è©¦è¡Œ', color='steelblue')
axes[0, 1].plot(trials_df['number'], trials_df['best_value_so_far'],
                linewidth=2, label='ç´¯ç©æœ€è‰¯å€¤', color='red')
axes[0, 1].set_xlabel('Trialæ•°', fontsize=12)
axes[0, 1].set_ylabel('Accuracy', fontsize=12)
axes[0, 1].set_title('æœ€é©åŒ–å±¥æ­´', fontsize=14)
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# 3. æœ€ã‚‚é‡è¦ãª2ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é–¢ä¿‚
top_params = list(importance.keys())[:2]
param1, param2 = top_params[0], top_params[1]

x_data = [t.params[param1] for t in study_with_pruning.trials if t.state == optuna.trial.TrialState.COMPLETE]
y_data = [t.params[param2] for t in study_with_pruning.trials if t.state == optuna.trial.TrialState.COMPLETE]
colors_data = [t.value for t in study_with_pruning.trials if t.state == optuna.trial.TrialState.COMPLETE]

scatter = axes[1, 0].scatter(x_data, y_data, c=colors_data, cmap='viridis', s=50, alpha=0.6)
axes[1, 0].set_xlabel(param1, fontsize=12)
axes[1, 0].set_ylabel(param2, fontsize=12)
axes[1, 0].set_title(f'{param1} vs {param2}', fontsize=14)
axes[1, 0].grid(True, alpha=0.3)
plt.colorbar(scatter, ax=axes[1, 0], label='Accuracy')

# 4. å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆ†å¸ƒã¨æ€§èƒ½
param_to_plot = top_params[0]
param_values = [t.params[param_to_plot] for t in study_with_pruning.trials if t.state == optuna.trial.TrialState.COMPLETE]
param_scores = [t.value for t in study_with_pruning.trials if t.state == optuna.trial.TrialState.COMPLETE]

axes[1, 1].scatter(param_values, param_scores, alpha=0.6, s=50, color='steelblue')
axes[1, 1].set_xlabel(param_to_plot, fontsize=12)
axes[1, 1].set_ylabel('Accuracy', fontsize=12)
axes[1, 1].set_title(f'{param_to_plot}ã®å½±éŸ¿', fontsize=14)
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é–“ã®ç›¸é–¢åˆ†æ
print("\n=== ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é–“ã®ç›¸é–¢ ===")
params_df = trials_df[[c for c in trials_df.columns if c.startswith('params_')]].copy()
params_df.columns = [c.replace('params_', '') for c in params_df.columns]
correlation = params_df.corr()

print("\nç›¸é–¢ä¿‚æ•°ãŒé«˜ã„çµ„åˆã›ï¼ˆ|r| > 0.3ï¼‰:")
for i in range(len(correlation.columns)):
    for j in range(i+1, len(correlation.columns)):
        corr_value = correlation.iloc[i, j]
        if abs(corr_value) > 0.3:
            print(f"{correlation.columns[i]} - {correlation.columns[j]}: {corr_value:.3f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡è¦åº¦åˆ†æ ===

=== é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° ===
learning_rate: 0.3456
max_depth: 0.2123
subsample: 0.1876
reg_lambda: 0.1234
colsample_bytree: 0.0987
min_child_weight: 0.0654
gamma: 0.0432
reg_alpha: 0.0238

=== ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é–“ã®ç›¸é–¢ ===

ç›¸é–¢ä¿‚æ•°ãŒé«˜ã„çµ„åˆã›ï¼ˆ|r| > 0.3ï¼‰:
learning_rate - max_depth: -0.342
subsample - colsample_bytree: 0.387
reg_alpha - reg_lambda: 0.456
</code></pre>

<h3>é‡è¦åº¦åˆ†æã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹çŸ¥è¦‹</h3>

<ul>
<li><strong>å„ªå…ˆé †ä½ä»˜ã‘</strong>: é‡è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é‡ç‚¹çš„ã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</li>
<li><strong>æ¢ç´¢ç¯„å›²ã®èª¿æ•´</strong>: é‡è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ç´°ã‹ãæ¢ç´¢</li>
<li><strong>å›ºå®šå€¤ã®æ±ºå®š</strong>: é‡è¦åº¦ã®ä½ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§å›ºå®š</li>
<li><strong>ç›¸äº’ä½œç”¨ã®ç†è§£</strong>: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é–“ã®ä¾å­˜é–¢ä¿‚ã‚’æŠŠæ¡</li>
</ul>

<hr>

<h2>3.7 å®Ÿè·µï¼šXGBoost/LightGBMã®å®Œå…¨ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</h2>

<h3>æ®µéšçš„ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥</h3>

<p>å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ç³»ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¤šãè¤‡é›‘ã§ã™ã€‚ä»¥ä¸‹ã®æ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒåŠ¹æœçš„ã§ã™ï¼š</p>

<div class="mermaid">
graph LR
    A[Stage 1<br/>æœ¨æ§‹é€ ] --> B[Stage 2<br/>æ­£å‰‡åŒ–]
    B --> C[Stage 3<br/>å­¦ç¿’ç‡]
    C --> D[Stage 4<br/>ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°]
    D --> E[æœ€çµ‚èª¿æ•´<br/>Fine-tuning]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#ffe0b2
</div>

<h3>å®Ÿè£…ä¾‹: XGBoostã®æ®µéšçš„ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</h3>

<pre><code class="language-python">import xgboost as xgb
from sklearn.metrics import accuracy_score, roc_auc_score

print("=== XGBoost æ®µéšçš„ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ===\n")

# ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆDMatrixå½¢å¼ï¼‰
dtrain = xgb.DMatrix(X_train_scaled, label=y_train)
dtest = xgb.DMatrix(X_test_scaled, label=y_test)

# Stage 1: æœ¨æ§‹é€ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
def objective_stage1(trial):
    params = {
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'learning_rate': 0.1,  # å›ºå®š
        'n_estimators': 100,
        'random_state': 42
    }

    model = XGBClassifier(**params)
    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')
    return scores.mean()

study_stage1 = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))
study_stage1.optimize(objective_stage1, n_trials=30, show_progress_bar=False)

print("Stage 1: æœ¨æ§‹é€ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
print(f"æœ€è‰¯AUC: {study_stage1.best_value:.4f}")
print(f"æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {study_stage1.best_params}\n")

# Stage 2: æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆStage 1ã®çµæœã‚’ä½¿ç”¨ï¼‰
def objective_stage2(trial):
    params = {
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'max_depth': study_stage1.best_params['max_depth'],
        'min_child_weight': study_stage1.best_params['min_child_weight'],
        'gamma': trial.suggest_float('gamma', 0, 5),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10, log=True),
        'learning_rate': 0.1,
        'n_estimators': 100,
        'random_state': 42
    }

    model = XGBClassifier(**params)
    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')
    return scores.mean()

study_stage2 = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))
study_stage2.optimize(objective_stage2, n_trials=30, show_progress_bar=False)

print("Stage 2: æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
print(f"æœ€è‰¯AUC: {study_stage2.best_value:.4f}")
print(f"æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {study_stage2.best_params}\n")

# Stage 3: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
def objective_stage3(trial):
    params = {
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'max_depth': study_stage1.best_params['max_depth'],
        'min_child_weight': study_stage1.best_params['min_child_weight'],
        'gamma': study_stage2.best_params['gamma'],
        'reg_alpha': study_stage2.best_params['reg_alpha'],
        'reg_lambda': study_stage2.best_params['reg_lambda'],
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'learning_rate': 0.1,
        'n_estimators': 100,
        'random_state': 42
    }

    model = XGBClassifier(**params)
    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')
    return scores.mean()

study_stage3 = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))
study_stage3.optimize(objective_stage3, n_trials=30, show_progress_bar=False)

print("Stage 3: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
print(f"æœ€è‰¯AUC: {study_stage3.best_value:.4f}")
print(f"æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {study_stage3.best_params}\n")

# Stage 4: å­¦ç¿’ç‡ã¨æœ¨ã®æ•°ï¼ˆæœ€çµ‚èª¿æ•´ï¼‰
def objective_stage4(trial):
    params = {
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'max_depth': study_stage1.best_params['max_depth'],
        'min_child_weight': study_stage1.best_params['min_child_weight'],
        'gamma': study_stage2.best_params['gamma'],
        'reg_alpha': study_stage2.best_params['reg_alpha'],
        'reg_lambda': study_stage2.best_params['reg_lambda'],
        'subsample': study_stage3.best_params['subsample'],
        'colsample_bytree': study_stage3.best_params['colsample_bytree'],
        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),
        'n_estimators': 1000,  # å¤§ãã‚ã«è¨­å®š
        'random_state': 42
    }

    # æ—©æœŸåœæ­¢ã‚’ä½¿ç”¨
    X_tr, X_val, y_tr, y_val = train_test_split(
        X_train_scaled, y_train, test_size=0.2, random_state=42
    )

    model = XGBClassifier(**params)
    model.fit(
        X_tr, y_tr,
        eval_set=[(X_val, y_val)],
        early_stopping_rounds=50,
        verbose=False
    )

    pred_proba = model.predict_proba(X_val)[:, 1]
    return roc_auc_score(y_val, pred_proba)

study_stage4 = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))
study_stage4.optimize(objective_stage4, n_trials=30, show_progress_bar=False)

print("Stage 4: å­¦ç¿’ç‡ã®æœ€é©åŒ–")
print(f"æœ€è‰¯AUC: {study_stage4.best_value:.4f}")
print(f"æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {study_stage4.best_params}\n")

# æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
final_params = {
    'objective': 'binary:logistic',
    'eval_metric': 'auc',
    'max_depth': study_stage1.best_params['max_depth'],
    'min_child_weight': study_stage1.best_params['min_child_weight'],
    'gamma': study_stage2.best_params['gamma'],
    'reg_alpha': study_stage2.best_params['reg_alpha'],
    'reg_lambda': study_stage2.best_params['reg_lambda'],
    'subsample': study_stage3.best_params['subsample'],
    'colsample_bytree': study_stage3.best_params['colsample_bytree'],
    'learning_rate': study_stage4.best_params['learning_rate'],
    'n_estimators': 1000,
    'random_state': 42
}

print("=== æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===")
for param, value in final_params.items():
    if param not in ['objective', 'eval_metric', 'random_state']:
        print(f"{param}: {value}")

# æœ€çµ‚è©•ä¾¡
final_model = XGBClassifier(**final_params)
final_model.fit(
    X_train_scaled, y_train,
    eval_set=[(X_test_scaled, y_test)],
    early_stopping_rounds=50,
    verbose=False
)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡
y_pred = final_model.predict(X_test_scaled)
y_pred_proba = final_model.predict_proba(X_test_scaled)[:, 1]

print(f"\n=== æœ€çµ‚æ€§èƒ½ ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")
print(f"ä½¿ç”¨ã—ãŸæœ¨ã®æ•°: {final_model.best_iteration}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== XGBoost æ®µéšçš„ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ===

Stage 1: æœ¨æ§‹é€ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
æœ€è‰¯AUC: 0.9889
æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {'max_depth': 5, 'min_child_weight': 2}

Stage 2: æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
æœ€è‰¯AUC: 0.9912
æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {'gamma': 0.234, 'reg_alpha': 0.0456, 'reg_lambda': 1.234}

Stage 3: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
æœ€è‰¯AUC: 0.9934
æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {'subsample': 0.8765, 'colsample_bytree': 0.7654}

Stage 4: å­¦ç¿’ç‡ã®æœ€é©åŒ–
æœ€è‰¯AUC: 0.9956
æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {'learning_rate': 0.0234}

=== æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
max_depth: 5
min_child_weight: 2
gamma: 0.234
reg_alpha: 0.0456
reg_lambda: 1.234
subsample: 0.8765
colsample_bytree: 0.7654
learning_rate: 0.0234
n_estimators: 1000

=== æœ€çµ‚æ€§èƒ½ ===
Accuracy: 0.9825
AUC: 0.9956
ä½¿ç”¨ã—ãŸæœ¨ã®æ•°: 342
</code></pre>

<h3>LightGBMã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¾‹</h3>

<pre><code class="language-python">import lightgbm as lgb

print("=== LightGBM ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ===\n")

# LightGBMç‰¹æœ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å«ã‚€æœ€é©åŒ–
def objective_lgb(trial):
    params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart', 'goss']),
        'num_leaves': trial.suggest_int('num_leaves', 20, 150),
        'max_depth': trial.suggest_int('max_depth', 3, 12),
        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10, log=True),
        'n_estimators': 1000,
        'random_state': 42,
        'n_jobs': -1,
        'verbose': -1
    }

    # DART/GOSSã®å ´åˆã®è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    if params['boosting_type'] == 'dart':
        params['drop_rate'] = trial.suggest_float('drop_rate', 0.0, 0.5)
    elif params['boosting_type'] == 'goss':
        params['top_rate'] = trial.suggest_float('top_rate', 0.0, 0.5)
        params['other_rate'] = trial.suggest_float('other_rate', 0.0, 0.5)

    # æ—©æœŸåœæ­¢ä»˜ãå­¦ç¿’
    X_tr, X_val, y_tr, y_val = train_test_split(
        X_train_scaled, y_train, test_size=0.2, random_state=42
    )

    model = lgb.LGBMClassifier(**params)
    model.fit(
        X_tr, y_tr,
        eval_set=[(X_val, y_val)],
        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]
    )

    pred_proba = model.predict_proba(X_val)[:, 1]
    return roc_auc_score(y_val, pred_proba)

# Optunaæœ€é©åŒ–
study_lgb = optuna.create_study(
    direction='maximize',
    sampler=TPESampler(seed=42),
    pruner=MedianPruner(n_startup_trials=10)
)

study_lgb.optimize(objective_lgb, n_trials=100, show_progress_bar=True)

print(f"\næœ€è‰¯AUC: {study_lgb.best_value:.4f}")
print("\n=== æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===")
for param, value in study_lgb.best_params.items():
    print(f"{param}: {value}")

# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è©•ä¾¡
best_lgb = lgb.LGBMClassifier(**study_lgb.best_params, n_estimators=1000,
                              random_state=42, n_jobs=-1, verbose=-1)
best_lgb.fit(
    X_train_scaled, y_train,
    eval_set=[(X_test_scaled, y_test)],
    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]
)

y_pred_lgb = best_lgb.predict(X_test_scaled)
y_pred_proba_lgb = best_lgb.predict_proba(X_test_scaled)[:, 1]

print(f"\n=== ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ€§èƒ½ ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred_lgb):.4f}")
print(f"AUC: {roc_auc_score(y_test, y_pred_proba_lgb):.4f}")

# XGBoostã¨LightGBMã®æ¯”è¼ƒ
print(f"\n=== XGBoost vs LightGBM ===")
print(f"XGBoost  - AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")
print(f"LightGBM - AUC: {roc_auc_score(y_test, y_pred_proba_lgb):.4f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== LightGBM ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ===

[I 2025-10-21 15:12:34,567] A new study created in memory with name: no-name-4
...
[I 2025-10-21 15:18:45,123] Trial 99 finished with value: 0.9968 and parameters: {...}

æœ€è‰¯AUC: 0.9968

=== æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
boosting_type: gbdt
num_leaves: 52
max_depth: 8
learning_rate: 0.0187
min_child_samples: 12
subsample: 0.8234
colsample_bytree: 0.8765
reg_alpha: 0.0234
reg_lambda: 2.345

=== ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ€§èƒ½ ===
Accuracy: 0.9912
AUC: 0.9968

=== XGBoost vs LightGBM ===
XGBoost  - AUC: 0.9956
LightGBM - AUC: 0.9968
</code></pre>

<h3>ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>

<table>
<thead>
<tr>
<th>åŸå‰‡</th>
<th>èª¬æ˜</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</strong></td>
<td>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ®µéšçš„ã«æœ€é©åŒ–</td>
<td>æ¢ç´¢ç©ºé–“ã‚’å‰Šæ¸›ã€è§£é‡ˆæ€§å‘ä¸Š</td>
</tr>
<tr>
<td><strong>æ—©æœŸåœæ­¢ã®æ´»ç”¨</strong></td>
<td>éå­¦ç¿’ã‚’é˜²ãè¨ˆç®—æ™‚é–“ã‚’çŸ­ç¸®</td>
<td>åŠ¹ç‡çš„ã§æ±åŒ–æ€§èƒ½ãŒå‘ä¸Š</td>
</tr>
<tr>
<td><strong>å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«æ¢ç´¢</strong></td>
<td>å­¦ç¿’ç‡ãªã©ã¯å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§</td>
<td>åºƒç¯„å›²ã‚’åŠ¹ç‡çš„ã«ã‚«ãƒãƒ¼</td>
</tr>
<tr>
<td><strong>é‡è¦åº¦ã‚’ç¢ºèª</strong></td>
<td>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡è¦åº¦ã‚’åˆ†æ</td>
<td>æ¬¡å›ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«æ´»ã‹ã™</td>
</tr>
<tr>
<td><strong>äº¤å·®æ¤œè¨¼ã‚’ä½¿ç”¨</strong></td>
<td>éå­¦ç¿’ã‚’é˜²ã</td>
<td>æ±åŒ–æ€§èƒ½ã®æ­£ç¢ºãªè©•ä¾¡</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.8 ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã®ã¾ã¨ã‚</h2>

<h3>æ‰‹æ³•ã®é¸ã³æ–¹</h3>

<table>
<thead>
<tr>
<th>çŠ¶æ³</th>
<th>æ¨å¥¨æ‰‹æ³•</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã„ï¼ˆâ‰¤3ï¼‰</strong></td>
<td>Grid Search</td>
<td>å®Œå…¨æ¢ç´¢ãŒç¾å®Ÿçš„</td>
</tr>
<tr>
<td><strong>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒä¸­ç¨‹åº¦ï¼ˆ4-6ï¼‰</strong></td>
<td>Random Search</td>
<td>ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„</td>
</tr>
<tr>
<td><strong>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå¤šã„ï¼ˆâ‰¥7ï¼‰</strong></td>
<td>Bayesian Opt.</td>
<td>åŠ¹ç‡çš„ã«æ¢ç´¢</td>
</tr>
<tr>
<td><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„</strong></td>
<td>Bayesian Opt. + Pruning</td>
<td>ç„¡é§„ãªè¨ˆç®—ã‚’å‰Šæ¸›</td>
</tr>
<tr>
<td><strong>åˆå¿ƒè€…ãƒ»æ¢ç´¢çš„åˆ†æ</strong></td>
<td>Random Search</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ã§ç†è§£ã—ã‚„ã™ã„</td>
</tr>
<tr>
<td><strong>æœ¬ç•ªç’°å¢ƒãƒ»ç«¶æŠ€</strong></td>
<td>Bayesian Opt.</td>
<td>æœ€é«˜æ€§èƒ½ã‚’è¿½æ±‚</td>
</tr>
</tbody>
</table>

<h3>è¨ˆç®—ã‚³ã‚¹ãƒˆã®è€ƒæ…®</h3>

<p>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ç·è¨ˆç®—æ™‚é–“ï¼š</p>
<p>$$
T_{\text{total}} = N_{\text{trials}} \times N_{\text{folds}} \times T_{\text{fit}}
$$</p>

<p>ä¾‹ï¼š100è©¦è¡Œã€5-fold CVã€1ãƒ•ã‚£ãƒƒãƒˆ10ç§’ã®å ´åˆï¼š</p>
<p>$$
T_{\text{total}} = 100 \times 5 \times 10 = 5000 \text{ç§’} \approx 83 \text{åˆ†}
$$</p>

<h3>å®Ÿè·µçš„ãªãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹é †</h3>

<div class="mermaid">
graph TD
    A[1. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºç«‹<br/>ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿] --> B[2. ç²—ã„æ¢ç´¢<br/>Random Search]
    B --> C[3. é‡è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç‰¹å®š<br/>é‡è¦åº¦åˆ†æ]
    C --> D[4. é›†ä¸­æ¢ç´¢<br/>Bayesian Opt.]
    D --> E[5. æœ€çµ‚èª¿æ•´<br/>Fine-tuning]
    E --> F[6. æ¤œè¨¼<br/>Hold-out Test]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#ffe0b2
    style F fill:#ffebee
</div>

<h3>ã‚ˆãã‚ã‚‹å¤±æ•—ã¨ãã®å¯¾ç­–</h3>

<table>
<thead>
<tr>
<th>å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³</th>
<th>åŸå› </th>
<th>å¯¾ç­–</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>éå­¦ç¿’</strong></td>
<td>è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</td>
<td>äº¤å·®æ¤œè¨¼ã‚’å¿…ãšä½¿ç”¨</td>
</tr>
<tr>
<td><strong>ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯</strong></td>
<td>ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®é †åºèª¤ã‚Š</td>
<td>Pipelineã‚’ä½¿ç”¨</td>
</tr>
<tr>
<td><strong>æ¢ç´¢ç¯„å›²ãŒç‹­ã„</strong></td>
<td>æœ€è‰¯å€¤ãŒå¢ƒç•Œä»˜è¿‘</td>
<td>ç¯„å›²ã‚’åºƒã’ã¦å†æ¢ç´¢</td>
</tr>
<tr>
<td><strong>è¨ˆç®—æ™‚é–“éå¤š</strong></td>
<td>æ¢ç´¢ç©ºé–“ãŒåºƒã™ã</td>
<td>æ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€Pruningä½¿ç”¨</td>
</tr>
<tr>
<td><strong>å†ç¾æ€§ãŒãªã„</strong></td>
<td>random_stateæœªè¨­å®š</td>
<td>å¿…ãšã‚·ãƒ¼ãƒ‰ã‚’å›ºå®š</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ç« ã¸</h3>

<p>ç¬¬4ç« ã§ã¯ã€<strong>äº¤å·®æ¤œè¨¼ã®å®Ÿè·µ</strong>ã‚’å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>K-Foldã€Stratifiedã€TimeSeriesSplitã®ä½¿ã„åˆ†ã‘</li>
<li>Nested Cross-Validationã«ã‚ˆã‚‹æ­£ç¢ºãªæ€§èƒ½è©•ä¾¡</li>
<li>å®Ÿè·µçš„ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>
<p>Grid Searchã¨Random Searchã®é•ã„ã‚’3ã¤æŒ™ã’ã€ãã‚Œãã‚Œã©ã®ã‚ˆã†ãªå ´é¢ã§ä½¿ã†ã¹ãã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>Grid Search</strong>ï¼š</p>
<ul>
<li><strong>æ¢ç´¢æ–¹æ³•</strong>: å…¨çµ„åˆã›ã‚’ç¶²ç¾…çš„ã«æ¢ç´¢</li>
<li><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆ</strong>: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã«å¯¾ã—ã¦æŒ‡æ•°çš„ã«å¢—åŠ </li>
<li><strong>æ¢ç´¢ã®ç¢ºå®Ÿæ€§</strong>: æŒ‡å®šç¯„å›²å†…ã§æœ€è‰¯ã®çµ„åˆã›ã‚’å¿…ãšç™ºè¦‹</li>
<li><strong>ä½¿ç”¨å ´é¢</strong>: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã„ï¼ˆ2-3å€‹ï¼‰ã€è¨ˆç®—æ™‚é–“ã«ä½™è£•ãŒã‚ã‚‹å ´åˆ</li>
</ul>

<p><strong>Random Search</strong>ï¼š</p>
<ul>
<li><strong>æ¢ç´¢æ–¹æ³•</strong>: ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</li>
<li><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆ</strong>: è©¦è¡Œå›æ•°ã§åˆ¶å¾¡å¯èƒ½ï¼ˆç·šå½¢çš„ï¼‰</li>
<li><strong>æ¢ç´¢ã®åŠ¹ç‡</strong>: é‡è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’åºƒãã‚«ãƒãƒ¼</li>
<li><strong>ä½¿ç”¨å ´é¢</strong>: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå¤šã„ã€é€£ç¶šå€¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¤šã„å ´åˆ</li>
</ul>

<p><strong>3ã¤ã®ä¸»ãªé•ã„</strong>ï¼š</p>
<ol>
<li><strong>æ¢ç´¢æˆ¦ç•¥</strong>: Grid Searchã¯æ±ºå®šè«–çš„ã€Random Searchã¯ç¢ºç‡çš„</li>
<li><strong>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£</strong>: Random Searchã¯é«˜æ¬¡å…ƒã§ã‚‚åŠ¹ç‡çš„</li>
<li><strong>ç™ºè¦‹ã®å¤šæ§˜æ€§</strong>: Random Searchã¯äºˆæƒ³å¤–ã®è‰¯ã„çµ„åˆã›ã‚’ç™ºè¦‹ã—ã‚„ã™ã„</li>
</ol>

<p><strong>ä½¿ã„åˆ†ã‘</strong>ï¼š</p>
<ul>
<li>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå°‘ãªãã€å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ã‚’è©³ç´°ã«è¦‹ãŸã„ â†’ Grid Search</li>
<li>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå¤šã„ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’æŠ‘ãˆãŸã„ â†’ Random Search</li>
<li>æœ€é«˜æ€§èƒ½ã‚’è¿½æ±‚ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå¤šã„ â†’ Bayesian Optimization</li>
</ul>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã«ã¯å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ä½•ãŒé–“é•ã£ã¦ã„ã‚‹ã‹æŒ‡æ‘˜ã—ã€æ­£ã—ã„ã‚³ãƒ¼ãƒ‰ã«ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler

# ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)

# Grid Search
param_grid = {'n_estimators': [50, 100], 'max_depth': [5, 10]}
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
grid_search.fit(X_train, y_train)

print(f"Best score: {grid_search.best_score_}")
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>å•é¡Œç‚¹</strong>ï¼š</p>
<ol>
<li><strong>ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯</strong>: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å…¨ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å®Ÿè¡Œã—ã¦ã‹ã‚‰åˆ†å‰²ã—ã¦ã„ã‚‹</li>
<li><strong>äº¤å·®æ¤œè¨¼ã§ã®ãƒªãƒ¼ã‚¯</strong>: GridSearchCVå†…ã§ã‚‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒé©åˆ‡ã«è¡Œã‚ã‚Œã¦ã„ãªã„</li>
<li><strong>å†ç¾æ€§ãªã—</strong>: random_stateãŒè¨­å®šã•ã‚Œã¦ã„ãªã„</li>
</ol>

<p><strong>æ­£ã—ã„ã‚³ãƒ¼ãƒ‰</strong>ï¼š</p>

<pre><code class="language-python">from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰ï¼‰
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Pipelineã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¨ãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆ
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier(random_state=42))
])

# Grid Search
param_grid = {
    'classifier__n_estimators': [50, 100],
    'classifier__max_depth': [5, 10]
}

grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42
)

grid_search.fit(X_train, y_train)

print(f"Best CV score: {grid_search.best_score_:.4f}")
print(f"Test score: {grid_search.score(X_test, y_test):.4f}")
print(f"Best params: {grid_search.best_params_}")
</code></pre>

<p><strong>ä¿®æ­£å†…å®¹ã®èª¬æ˜</strong>ï¼š</p>
<ul>
<li><strong>Pipelineä½¿ç”¨</strong>: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¨ãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã—ã€äº¤å·®æ¤œè¨¼å†…ã§æ­£ã—ãã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°</li>
<li><strong>åˆ†å‰²é †åº</strong>: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰ã«ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²</li>
<li><strong>random_state</strong>: å†ç¾æ€§ã®ãŸã‚ã«å…¨ã¦ã®ç®‡æ‰€ã§è¨­å®š</li>
<li><strong>stratify</strong>: ã‚¯ãƒ©ã‚¹æ¯”ç‡ã‚’ä¿æŒ</li>
<li><strong>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å</strong>: Pipelineå†…ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯<code>classifier__</code>æ¥é ­è¾ãŒå¿…è¦</li>
</ul>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>Bayesian OptimizationãŒåŠ¹ç‡çš„ãªç†ç”±ã‚’ã€ä»£ç†ãƒ¢ãƒ‡ãƒ«ã¨ç²å¾—é–¢æ•°ã®æ¦‚å¿µã‚’ç”¨ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p>Bayesian Optimizationã¯ã€ä»¥ä¸‹ã®2ã¤ã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«ã‚ˆã‚ŠåŠ¹ç‡çš„ãªæ¢ç´¢ã‚’å®Ÿç¾ã—ã¾ã™ï¼š</p>

<p><strong>1. ä»£ç†ãƒ¢ãƒ‡ãƒ«ï¼ˆSurrogate Modelï¼‰</strong></p>
<ul>
<li><strong>å½¹å‰²</strong>: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨æ€§èƒ½ã®é–¢ä¿‚ã‚’ç¢ºç‡çš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–</li>
<li><strong>æ‰‹æ³•</strong>: ã‚¬ã‚¦ã‚¹éç¨‹ï¼ˆGPï¼‰ã‚„TPEï¼ˆTree-structured Parzen Estimatorï¼‰</li>
<li><strong>åˆ©ç‚¹</strong>:
  <ul>
  <li>éå»ã®è©•ä¾¡çµæœã‹ã‚‰æ€§èƒ½ã‚’äºˆæ¸¬</li>
  <li>äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ã‚‚åŒæ™‚ã«æ¨å®š</li>
  <li>å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆè¨ˆç®—ã‚³ã‚¹ãƒˆå¤§ï¼‰ã‚’ä»£ç†ï¼ˆè¨ˆç®—ã‚³ã‚¹ãƒˆå°ï¼‰</li>
  </ul>
</li>
</ul>

<p><strong>2. ç²å¾—é–¢æ•°ï¼ˆAcquisition Functionï¼‰</strong></p>
<ul>
<li><strong>å½¹å‰²</strong>: æ¬¡ã«è©•ä¾¡ã™ã¹ããƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é¸æŠ</li>
<li><strong>ä¸»ãªæ‰‹æ³•</strong>:
  <ul>
  <li><strong>EIï¼ˆExpected Improvementï¼‰</strong>: ç¾åœ¨ã®æœ€è‰¯å€¤ã‹ã‚‰ã®æ”¹å–„æœŸå¾…å€¤ã‚’æœ€å¤§åŒ–</li>
  <li><strong>UCBï¼ˆUpper Confidence Boundï¼‰</strong>: äºˆæ¸¬å€¤ã¨ä¸ç¢ºå®Ÿæ€§ã®ãƒãƒ©ãƒ³ã‚¹</li>
  <li><strong>PIï¼ˆProbability of Improvementï¼‰</strong>: æ”¹å–„ã™ã‚‹ç¢ºç‡ã‚’æœ€å¤§åŒ–</li>
  </ul>
</li>
<li><strong>åˆ©ç‚¹</strong>: æ¢ç´¢ï¼ˆExplorationï¼‰ã¨æ´»ç”¨ï¼ˆExploitationï¼‰ã®ãƒãƒ©ãƒ³ã‚¹</li>
</ul>

<p><strong>åŠ¹ç‡æ€§ã®ç†ç”±</strong>ï¼š</p>
<ol>
<li><strong>æƒ…å ±ã®è“„ç©</strong>: å„è©¦è¡Œã‹ã‚‰å­¦ç¿’ã—ã€æ¬¡ã®è©¦è¡Œã«æ´»ã‹ã™</li>
<li><strong>è³¢ã„é¸æŠ</strong>: æœ‰æœ›ãªé ˜åŸŸã‚’å„ªå…ˆçš„ã«æ¢ç´¢</li>
<li><strong>ç„¡é§„ã®å‰Šæ¸›</strong>: æ€§èƒ½ãŒä½ãã†ãªé ˜åŸŸã¯é¿ã‘ã‚‹</li>
<li><strong>ä¸ç¢ºå®Ÿæ€§ã®æ´»ç”¨</strong>: æœªæ¢ç´¢é ˜åŸŸã‚‚é©åˆ‡ã«è©•ä¾¡</li>
</ol>

<p><strong>Random Searchã¨ã®æ¯”è¼ƒ</strong>ï¼š</p>
<ul>
<li>Random Search: éå»ã®æƒ…å ±ã‚’ç„¡è¦–ã—ã¦ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</li>
<li>Bayesian Opt.: éå»ã®æƒ…å ±ã‹ã‚‰å­¦ç¿’ã—ã€è³¢ãæ¬¡ã®å€™è£œã‚’é¸æŠ</li>
<li>çµæœ: åŒã˜è©¦è¡Œå›æ•°ã§Bayesian Opt.ã®æ–¹ãŒé«˜ç²¾åº¦ã‚’é”æˆã—ã‚„ã™ã„</li>
</ul>

<p><strong>å®Ÿè·µä¾‹</strong>ï¼š</p>
<pre><code>è©¦è¡Œ 1-10: ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸæ¢ç´¢
è©¦è¡Œ 11: ä»£ç†ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã€Œå­¦ç¿’ç‡=0.05, æ·±ã•=8ã€ãŒæœ‰æœ›ã¨äºˆæ¸¬
è©¦è¡Œ 12: ã€Œå­¦ç¿’ç‡=0.05ã€ä»˜è¿‘ã‚’é›†ä¸­æ¢ç´¢
è©¦è¡Œ 13: ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„é ˜åŸŸã‚‚è©¦ã™ï¼ˆæ¢ç´¢ï¼‰
...
</code></pre>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>XGBoostã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦ã€ãªãœæ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆStage 1: æœ¨æ§‹é€  â†’ Stage 2: æ­£å‰‡åŒ– â†’ ...ï¼‰ãŒæ¨å¥¨ã•ã‚Œã‚‹ã®ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ä¸€åº¦ã«å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ã®é•ã„ã‚‚è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>æ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒæ¨å¥¨ã•ã‚Œã‚‹ç†ç”±</strong>ï¼š</p>

<ol>
<li><strong>æ¢ç´¢ç©ºé–“ã®å‰Šæ¸›</strong>
<ul>
<li>XGBoostã«ã¯10å€‹ä»¥ä¸Šã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå­˜åœ¨</li>
<li>ä¸€åº¦ã«å…¨ã¦æ¢ç´¢ã™ã‚‹ã¨çµ„åˆã›çˆ†ç™ºï¼ˆä¾‹ï¼šå„10å€™è£œã§$10^{10}$é€šã‚Šï¼‰</li>
<li>æ®µéšçš„ã«æ¢ç´¢ã™ã‚‹ã“ã¨ã§ã€å„æ®µéšã®æ¢ç´¢ç©ºé–“ã‚’å¤§å¹…ã«å‰Šæ¸›</li>
</ul>
</li>

<li><strong>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é–“ã®ä¾å­˜é–¢ä¿‚</strong>
<ul>
<li>ä¸€éƒ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©å€¤ã«å½±éŸ¿</li>
<li>ä¾‹ï¼š<code>max_depth</code>ãŒæ±ºã¾ã‚‹ã¨ã€é©åˆ‡ãª<code>learning_rate</code>ã®ç¯„å›²ãŒå¤‰ã‚ã‚‹</li>
<li>æ®µéšçš„ã«å›ºå®šã—ã¦ã„ãã“ã¨ã§ã€ä¾å­˜é–¢ä¿‚ã‚’è€ƒæ…®ã—ãŸæœ€é©åŒ–ãŒå¯èƒ½</li>
</ul>
</li>

<li><strong>è§£é‡ˆæ€§ã¨ç†è§£</strong>
<ul>
<li>å„æ®µéšã§ã©ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã©ã†æ€§èƒ½ã«å½±éŸ¿ã™ã‚‹ã‹ç†è§£ã§ãã‚‹</li>
<li>å•é¡Œã‚„ãƒ‡ãƒ¼ã‚¿ã«å¿œã˜ãŸèª¿æ•´ãŒã—ã‚„ã™ã„</li>
<li>ãƒ‡ãƒãƒƒã‚°ã‚„ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãŒå®¹æ˜“</li>
</ul>
</li>

<li><strong>è¨ˆç®—åŠ¹ç‡</strong>
<ul>
<li>åˆæœŸæ®µéšã§ç²—ã„è¨­å®šã‚’æ±ºå®šã—ã€å¾ŒåŠã§å¾®èª¿æ•´</li>
<li>ç„¡é§„ãªçµ„åˆã›ã®è©•ä¾¡ã‚’é¿ã‘ã‚‰ã‚Œã‚‹</li>
<li>Pruningã¨çµ„ã¿åˆã‚ã›ã¦ã•ã‚‰ã«åŠ¹ç‡åŒ–</li>
</ul>
</li>
</ol>

<p><strong>æ¨å¥¨ã•ã‚Œã‚‹æ®µéš</strong>ï¼š</p>

<pre><code>Stage 1: æœ¨æ§‹é€ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
  - max_depth, min_child_weight
  â†’ ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬çš„ãªè¤‡é›‘ã•ã‚’æ±ºå®š

Stage 2: æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
  - gamma, reg_alpha, reg_lambda
  â†’ éå­¦ç¿’ã‚’é˜²ã

Stage 3: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
  - subsample, colsample_bytree
  â†’ ã•ã‚‰ãªã‚‹æ­£å‰‡åŒ–ã¨å¤šæ§˜æ€§

Stage 4: å­¦ç¿’ç‡ã¨æœ¨ã®æ•°
  - learning_rate, n_estimators (with early stopping)
  â†’ æœ€çµ‚çš„ãªæ€§èƒ½èª¿æ•´
</code></pre>

<p><strong>ä¸€åº¦ã«å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã™ã‚‹å ´åˆã®å•é¡Œ</strong>ï¼š</p>

<table>
<tr>
<th>é …ç›®</th>
<th>ä¸€åº¦ã«æœ€é©åŒ–</th>
<th>æ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</th>
</tr>
<tr>
<td><strong>æ¢ç´¢ç©ºé–“</strong></td>
<td>è†¨å¤§ï¼ˆ$10^{10}$é€šã‚Šä»¥ä¸Šï¼‰</td>
<td>å„æ®µéšã§ç®¡ç†å¯èƒ½ï¼ˆæ•°ç™¾é€šã‚Šï¼‰</td>
</tr>
<tr>
<td><strong>è¨ˆç®—æ™‚é–“</strong></td>
<td>éç¾å®Ÿçš„ï¼ˆæ•°æ—¥ã€œæ•°é€±é–“ï¼‰</td>
<td>ç¾å®Ÿçš„ï¼ˆæ•°æ™‚é–“ï¼‰</td>
</tr>
<tr>
<td><strong>æœ€é©åŒ–ã®è³ª</strong></td>
<td>å±€æ‰€æœ€é©ã«é™¥ã‚Šã‚„ã™ã„</td>
<td>ã‚ˆã‚Šè‰¯ã„æœ€é©è§£ã‚’ç™ºè¦‹</td>
</tr>
<tr>
<td><strong>è§£é‡ˆæ€§</strong></td>
<td>ä½ã„ï¼ˆãªãœãã®å€¤ã‹ä¸æ˜ï¼‰</td>
<td>é«˜ã„ï¼ˆå„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½¹å‰²ãŒæ˜ç¢ºï¼‰</td>
</tr>
<tr>
<td><strong>å†åˆ©ç”¨æ€§</strong></td>
<td>ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¿œç”¨å›°é›£</td>
<td>çŸ¥è¦‹ã‚’ä»–ã«å¿œç”¨å¯èƒ½</td>
</tr>
</table>

<p><strong>å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹</strong>ï¼š</p>
<ul>
<li>æœ€åˆã¯æ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§å¤§ã¾ã‹ãªæœ€é©å€¤ã‚’è¦‹ã¤ã‘ã‚‹</li>
<li>æœ€å¾Œã«é‡è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°å€‹ã‚’ä¸€åº¦ã«å¾®èª¿æ•´ã™ã‚‹ã“ã¨ã¯æœ‰åŠ¹</li>
<li>Bayesian Optimizationã‚’ä½¿ãˆã°ã€ã‚ã‚‹ç¨‹åº¦å¤šãã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åŒæ™‚ã«æ‰±ãˆã‚‹</li>
<li>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡è¦åº¦åˆ†æã§ã€æœ¬å½“ã«é‡è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«æ³¨åŠ›</li>
</ul>

<p><strong>çµè«–</strong>ï¼š</p>
<p>æ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€è¨ˆç®—åŠ¹ç‡ã€æœ€é©åŒ–ã®è³ªã€è§£é‡ˆæ€§ã®ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ãã€å®Ÿè·µçš„ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«æœ€é©ã§ã™ã€‚</p>

</details>

<h3>å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>ã‚ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã€Grid Searchã«ã‚ˆã‚‹5-fold CVã§æœ€è‰¯ç²¾åº¦0.9500ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§0.8800ã¨ã„ã†çµæœãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã€‚ã“ã®çµæœã‹ã‚‰èª­ã¿å–ã‚Œã‚‹å•é¡Œã¨ã€æ”¹å–„ç­–ã‚’3ã¤ä»¥ä¸Šææ¡ˆã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>å•é¡Œã®è¨ºæ–­</strong>ï¼š</p>

<p>CVç²¾åº¦ï¼ˆ0.9500ï¼‰ã¨ãƒ†ã‚¹ãƒˆç²¾åº¦ï¼ˆ0.8800ï¼‰ã®å¤§ããªä¹–é›¢ï¼ˆç´„7%ï¼‰ã¯ã€ä»¥ä¸‹ã®å•é¡Œã‚’ç¤ºå”†ã—ã¾ã™ï¼š</p>

<ol>
<li><strong>éå­¦ç¿’ï¼ˆOverfittingï¼‰</strong>: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«éåº¦ã«é©åˆã—ã€æœªçŸ¥ãƒ‡ãƒ¼ã‚¿ã¸ã®æ±åŒ–æ€§èƒ½ãŒä½ã„</li>
<li><strong>ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯</strong>: äº¤å·®æ¤œè¨¼ã®å®Ÿè£…ã«èª¤ã‚ŠãŒã‚ã‚‹å¯èƒ½æ€§</li>
<li><strong>ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®é•ã„</strong>: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒãŒç•°ãªã‚‹</li>
<li><strong>éåº¦ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</strong>: CVãƒ‡ãƒ¼ã‚¿ã«éé©åˆ</li>
</ol>

<p><strong>æ”¹å–„ç­–</strong>ï¼š</p>

<p><strong>1. Nested Cross-Validationï¼ˆå…¥ã‚Œå­äº¤å·®æ¤œè¨¼ï¼‰ã®å°å…¥</strong></p>
<pre><code class="language-python">from sklearn.model_selection import cross_val_score

# Outer loop: æ±åŒ–æ€§èƒ½ã®è©•ä¾¡
outer_scores = []
for train_idx, test_idx in KFold(n_splits=5).split(X, y):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # Inner loop: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
    grid_search = GridSearchCV(model, param_grid, cv=5)
    grid_search.fit(X_train, y_train)

    # Outer testã§è©•ä¾¡
    score = grid_search.score(X_test, y_test)
    outer_scores.append(score)

print(f"Nested CV score: {np.mean(outer_scores):.4f} Â± {np.std(outer_scores):.4f}")
# ã“ã®å€¤ãŒãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç²¾åº¦ã®ã‚ˆã‚Šæ­£ç¢ºãªæ¨å®šå€¤
</code></pre>
<p><strong>ç†ç”±</strong>: ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°éç¨‹ã§ã®éå­¦ç¿’ã‚’é˜²ãã€çœŸã®æ±åŒ–æ€§èƒ½ã‚’è©•ä¾¡ã§ãã‚‹</p>

<p><strong>2. ã‚ˆã‚Šå¼·ã„æ­£å‰‡åŒ–ã®é©ç”¨</strong></p>
<pre><code class="language-python"># æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¢ç´¢ç¯„å›²ã‚’æ‹¡å¤§
param_grid = {
    'max_depth': [3, 5, 7],  # ã‚ˆã‚Šæµ…ã„æœ¨
    'min_samples_split': [10, 20, 50],  # ã‚ˆã‚Šå¤šã„ã‚µãƒ³ãƒ—ãƒ«æ•°
    'min_samples_leaf': [5, 10, 20],
    'reg_alpha': [0.1, 1.0, 10.0],  # ã‚ˆã‚Šå¼·ã„L1æ­£å‰‡åŒ–
    'reg_lambda': [1.0, 10.0, 100.0]  # ã‚ˆã‚Šå¼·ã„L2æ­£å‰‡åŒ–
}
</code></pre>
<p><strong>ç†ç”±</strong>: ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•ã‚’åˆ¶é™ã—ã€éå­¦ç¿’ã‚’æŠ‘åˆ¶</p>

<p><strong>3. ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã§ã®æ¤œè¨¼</strong></p>
<pre><code class="language-python"># Holdoutã‚»ãƒƒãƒˆã®è¿½åŠ 
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.2, random_state=42
)

# Train + Validation ã§ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€Testã§æœ€çµ‚è©•ä¾¡
grid_search = GridSearchCV(model, param_grid, cv=5)
grid_search.fit(X_train, y_train)

val_score = grid_search.score(X_val, y_val)
test_score = grid_search.score(X_test, y_test)

print(f"Validation score: {val_score:.4f}")
print(f"Test score: {test_score:.4f}")
</code></pre>
<p><strong>ç†ç”±</strong>: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ãšã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€æœ€çµ‚è©•ä¾¡ã‚’è¡Œã†</p>

<p><strong>4. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®å¾¹åº•ãƒã‚§ãƒƒã‚¯</strong></p>
<pre><code class="language-python"># Pipelineã§å‰å‡¦ç†ã‚’çµ±åˆ
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('imputer', SimpleImputer()),
    ('feature_selection', SelectKBest(k=10)),
    ('classifier', RandomForestClassifier())
])

# äº¤å·®æ¤œè¨¼å†…ã§å…¨ã¦ã®å‰å‡¦ç†ãŒå®Ÿè¡Œã•ã‚Œã‚‹
grid_search = GridSearchCV(pipeline, param_grid, cv=5)
grid_search.fit(X_train, y_train)
</code></pre>
<p><strong>ç†ç”±</strong>: å‰å‡¦ç†ã§ã®ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’é˜²ã</p>

<p><strong>5. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ»ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã®æ´»ç”¨</strong></p>
<pre><code class="language-python">from sklearn.ensemble import StackingClassifier

# è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
estimators = [
    ('rf', RandomForestClassifier(**best_params_rf)),
    ('gb', GradientBoostingClassifier(**best_params_gb)),
    ('svm', SVC(**best_params_svm))
]

stacking = StackingClassifier(
    estimators=estimators,
    final_estimator=LogisticRegression(),
    cv=5
)

stacking.fit(X_train, y_train)
test_score = stacking.score(X_test, y_test)
</code></pre>
<p><strong>ç†ç”±</strong>: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’çµ±åˆã—ã€æ±åŒ–æ€§èƒ½ã‚’å‘ä¸Š</p>

<p><strong>6. æ—©æœŸåœæ­¢ï¼ˆEarly Stoppingï¼‰ã®å°å…¥</strong></p>
<pre><code class="language-python"># XGBoost/LightGBMã®å ´åˆ
model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=50,
    verbose=False
)
</code></pre>
<p><strong>ç†ç”±</strong>: å­¦ç¿’éç¨‹ã§éå­¦ç¿’ã‚’æ¤œå‡ºã—ã€æœ€é©ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§åœæ­¢</p>

<p><strong>7. ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªã¨å‰å‡¦ç†ã®è¦‹ç›´ã—</strong></p>
<ul>
<li>è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã‚’æ¯”è¼ƒï¼ˆKLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã€KSçµ±è¨ˆé‡ï¼‰</li>
<li>å¤–ã‚Œå€¤ã®å‡¦ç†æ–¹æ³•ã‚’è¦‹ç›´ã™</li>
<li>ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®æ”¹å–„</li>
<li>ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆå°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã®å ´åˆï¼‰</li>
</ul>

<p><strong>å®Ÿè£…ä¾‹ï¼ˆåˆ†å¸ƒã®æ¯”è¼ƒï¼‰</strong>ï¼š</p>
<pre><code class="language-python">from scipy.stats import ks_2samp

# å„ç‰¹å¾´é‡ã®åˆ†å¸ƒã‚’æ¯”è¼ƒ
for i in range(X.shape[1]):
    stat, p_value = ks_2samp(X_train[:, i], X_test[:, i])
    if p_value < 0.05:
        print(f"Feature {i}: åˆ†å¸ƒãŒç•°ãªã‚‹å¯èƒ½æ€§ (p={p_value:.4f})")
</code></pre>

<p><strong>ã¾ã¨ã‚</strong>ï¼š</p>
<p>æœ€ã‚‚é‡è¦ãªã®ã¯ã€<strong>Nested Cross-Validation</strong>ã¨<strong>ã‚ˆã‚Šå¼·ã„æ­£å‰‡åŒ–</strong>ã®çµ„åˆã›ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€çœŸã®æ±åŒ–æ€§èƒ½ã‚’æ­£ç¢ºã«è©•ä¾¡ã—ãªãŒã‚‰ã€éå­¦ç¿’ã‚’æŠ‘åˆ¶ã§ãã¾ã™ã€‚</p>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Bergstra, J., & Bengio, Y. (2012). "Random Search for Hyper-Parameter Optimization." <em>Journal of Machine Learning Research</em>, 13, 281-305.</li>
<li>Snoek, J., Larochelle, H., & Adams, R. P. (2012). "Practical Bayesian Optimization of Machine Learning Algorithms." <em>NIPS</em>.</li>
<li>Akiba, T., et al. (2019). "Optuna: A Next-generation Hyperparameter Optimization Framework." <em>KDD</em>.</li>
<li>Chen, T., & Guestrin, C. (2016). "XGBoost: A Scalable Tree Boosting System." <em>KDD</em>.</li>
<li>Ke, G., et al. (2017). "LightGBM: A Highly Efficient Gradient Boosting Decision Tree." <em>NIPS</em>.</li>
<li>Feurer, M., & Hutter, F. (2019). <em>Hyperparameter Optimization</em>. In: Automated Machine Learning. Springer.</li>
</ol>

<div class="navigation">
    <a href="chapter2-metrics-selection.html" class="nav-button">â† å‰ã®ç« : è©•ä¾¡æŒ‡æ¨™ã®é¸æŠ</a>
    <a href="chapter4-cross-validation.html" class="nav-button">æ¬¡ã®ç« : äº¤å·®æ¤œè¨¼ã®å®Ÿè·µ â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>