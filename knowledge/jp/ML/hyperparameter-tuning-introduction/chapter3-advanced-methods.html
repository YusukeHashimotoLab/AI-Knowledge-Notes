<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
<meta content="ç¬¬3ç« ï¼šé«˜åº¦ãªãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³• - AI Terakoya" name="description"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬3ç« ï¼šé«˜åº¦ãªãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³• - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/hyperparameter-tuning-introduction/chapter3-advanced-methods.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/hyperparameter-tuning-introduction/index.html">Hyperparameter Tuning</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬3ç« ï¼šé«˜åº¦ãªãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•</h1>
            <p class="subtitle">Hyperbandã€BOHBã€Population-based Trainingã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªæ¢ç´¢</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š-ä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 6å€‹</span>
                <span class="meta-item">ğŸš€ å®Ÿè·µçš„æ‰‹æ³•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… Successive Halvingã¨Hyperbandã®åŸç†ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… BOHBã«ã‚ˆã‚‹ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã¨Hyperbandã®èåˆã‚’æ´»ç”¨ã§ãã‚‹</li>
<li>âœ… Population-based Training (PBT)ã§ä¸¦åˆ—å­¦ç¿’ã‚’æœ€é©åŒ–ã§ãã‚‹</li>
<li>âœ… Hyperoptã€SMACã€Ax/BoTorchãªã©ä¸»è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ç‰¹å¾´ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Ray Tuneã§å¤§è¦æ¨¡åˆ†æ•£ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè£…ã§ãã‚‹</li>
</ul>

<hr>

<h2>3.1 Hyperband</h2>

<h3>Successive Halvingã®åŸç†</h3>

<p><strong>Successive Halving</strong>ã¯ã€é™ã‚‰ã‚ŒãŸè¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’åŠ¹ç‡çš„ã«é…åˆ†ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢ã¯ä»¥ä¸‹ã®é€šã‚Šï¼š</p>

<ol>
<li>å¤šæ•°ã®è¨­å®šã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å°‘é‡ã®ãƒªã‚½ãƒ¼ã‚¹ã§é–‹å§‹</li>
<li>æ€§èƒ½ã®æ‚ªã„è¨­å®šã‚’æ®µéšçš„ã«é™¤å¤–ï¼ˆåŠåˆ†ãšã¤ï¼‰</li>
<li>æ®‹ã£ãŸæœ‰æœ›ãªè¨­å®šã«ã‚ˆã‚Šå¤šãã®ãƒªã‚½ãƒ¼ã‚¹ã‚’å‰²ã‚Šå½“ã¦</li>
</ol>

<blockquote>
<p><strong>é‡è¦</strong>: æ—©æœŸã«æ€§èƒ½ãŒæ‚ªã„è¨­å®šã‚’é™¤å¤–ã™ã‚‹ã“ã¨ã§ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã¾ã™ã€‚</p>
</blockquote>

<h3>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æµã‚Œ</h3>

<div class="mermaid">
graph TD
    A[nå€‹ã®è¨­å®šã‚’ãƒ©ãƒ³ãƒ€ãƒ ç”Ÿæˆ] --> B[å„è¨­å®šã‚’rãƒªã‚½ãƒ¼ã‚¹ã§è©•ä¾¡]
    B --> C{æ€§èƒ½ã®ä¸Šä½n/2ã‚’é¸æŠ}
    C --> D[ãƒªã‚½ãƒ¼ã‚¹ã‚’2å€ã«å¢—ã‚„ã™]
    D --> E{ã•ã‚‰ã«ä¸Šä½n/4ã‚’é¸æŠ}
    E --> F[ãƒªã‚½ãƒ¼ã‚¹ã‚’2å€ã«å¢—ã‚„ã™]
    F --> G[æœ€çµ‚çš„ã«æœ€è‰¯ã®è¨­å®šãŒæ®‹ã‚‹]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#e3f2fd
    style D fill:#f3e5f5
    style E fill:#e3f2fd
    style F fill:#f3e5f5
    style G fill:#c8e6c9
</div>

<h3>Hyperbandã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h3>

<p><strong>Hyperband</strong>ã¯ã€Successive Halvingã‚’è¤‡æ•°ã®ç•°ãªã‚‹è¨­å®šã§å®Ÿè¡Œã—ã€ãƒªã‚½ãƒ¼ã‚¹é…åˆ†æˆ¦ç•¥ã‚’æœ€é©åŒ–ã—ã¾ã™ã€‚</p>

<p>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼š</p>
<ul>
<li><strong>R</strong>: 1ã¤ã®è¨­å®šã«å‰²ã‚Šå½“ã¦ã‚‹æœ€å¤§ãƒªã‚½ãƒ¼ã‚¹ï¼ˆã‚¨ãƒãƒƒã‚¯æ•°ãªã©ï¼‰</li>
<li><strong>Î·</strong>: å„ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã®å‰Šæ¸›ç‡ï¼ˆé€šå¸¸3ã¾ãŸã¯4ï¼‰</li>
</ul>

<p>$$
s_{\max} = \lfloor \log_\eta(R) \rfloor
$$</p>

<h3>Optunaã§ã®å®Ÿè£…ï¼ˆHyperbandPrunerï¼‰</h3>

<pre><code class="language-python">import optuna
from optuna.pruners import HyperbandPruner
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
import numpy as np

# Hyperbandã®è¨­å®š
pruner = HyperbandPruner(
    min_resource=1,      # æœ€å°ãƒªã‚½ãƒ¼ã‚¹ï¼ˆã‚¨ãƒãƒƒã‚¯ï¼‰
    max_resource=100,    # æœ€å¤§ãƒªã‚½ãƒ¼ã‚¹
    reduction_factor=3   # å‰Šæ¸›ç‡Î·
)

def objective(trial):
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ææ¡ˆ
    n_estimators = trial.suggest_int('n_estimators', 10, 200)
    max_depth = trial.suggest_int('max_depth', 2, 32)
    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)
    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)

    # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    X, y = load_iris(return_X_y=True)

    # æ®µéšçš„ã«n_estimatorsã‚’å¢—ã‚„ã—ã¦è©•ä¾¡ï¼ˆHyperbandå¯¾å¿œï¼‰
    for step in range(1, 6):
        # ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—ã«å¿œã˜ãŸæœ¨ã®æ•°
        current_n_estimators = int(n_estimators * step / 5)

        model = RandomForestClassifier(
            n_estimators=current_n_estimators,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            random_state=42
        )

        # äº¤å·®æ¤œè¨¼ã‚¹ã‚³ã‚¢
        score = cross_val_score(model, X, y, cv=3, n_jobs=-1).mean()

        # Optunaã«ä¸­é–“å€¤ã‚’å ±å‘Š
        trial.report(score, step)

        # æåˆˆã‚Šåˆ¤å®š
        if trial.should_prune():
            raise optuna.TrialPruned()

    return score

# ã‚¹ã‚¿ãƒ‡ã‚£ã®å®Ÿè¡Œ
study = optuna.create_study(
    direction='maximize',
    pruner=pruner,
    study_name='hyperband_example'
)

study.optimize(objective, n_trials=100, timeout=300)

print("\n=== Hyperband æœ€é©åŒ–çµæœ ===")
print(f"æœ€è‰¯ã‚¹ã‚³ã‚¢: {study.best_value:.4f}")
print(f"æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {study.best_params}")
print(f"\nå®Œäº†è©¦è¡Œ: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}")
print(f"æåˆˆã‚Šã•ã‚ŒãŸè©¦è¡Œ: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== Hyperband æœ€é©åŒ–çµæœ ===
æœ€è‰¯ã‚¹ã‚³ã‚¢: 0.9733
æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {'n_estimators': 142, 'max_depth': 8, 'min_samples_split': 3, 'min_samples_leaf': 1}

å®Œäº†è©¦è¡Œ: 28
æåˆˆã‚Šã•ã‚ŒãŸè©¦è¡Œ: 72
</code></pre>

<blockquote>
<p><strong>åŠ¹æœ</strong>: 100è©¦è¡Œã®ã†ã¡72è©¦è¡ŒãŒæ—©æœŸã«æåˆˆã‚Šã•ã‚Œã€è¨ˆç®—æ™‚é–“ã‚’å¤§å¹…ã«å‰Šæ¸›ã€‚</p>
</blockquote>

<hr>

<h2>3.2 BOHB (Bayesian Optimization and HyperBand)</h2>

<h3>Hyperbandã¨ãƒ™ã‚¤ã‚ºOPã®èåˆ</h3>

<p><strong>BOHB</strong>ã¯ã€Hyperbandã®åŠ¹ç‡çš„ãªãƒªã‚½ãƒ¼ã‚¹é…åˆ†ã¨ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®è³¢ã„æ¢ç´¢ã‚’çµ„ã¿åˆã‚ã›ãŸæ‰‹æ³•ã§ã™ã€‚</p>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>é•·æ‰€</th>
<th>çŸ­æ‰€</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Hyperband</strong></td>
<td>åŠ¹ç‡çš„ãªãƒªã‚½ãƒ¼ã‚¹é…åˆ†</td>
<td>ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</td>
</tr>
<tr>
<td><strong>ãƒ™ã‚¤ã‚ºæœ€é©åŒ–</strong></td>
<td>è³¢ã„æ¢ç´¢</td>
<td>å…¨ã¦ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’å‰²ã‚Šå½“ã¦</td>
</tr>
<tr>
<td><strong>BOHB</strong></td>
<td>åŠ¹ç‡çš„ + è³¢ã„æ¢ç´¢</td>
<td>å®Ÿè£…ãŒè¤‡é›‘</td>
</tr>
</tbody>
</table>

<h3>BOHBã®å‹•ä½œåŸç†</h3>

<ol>
<li><strong>Hyperbandãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯</strong>ã§ãƒªã‚½ãƒ¼ã‚¹é…åˆ†ã‚’ç®¡ç†</li>
<li>å„ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã€<strong>TPEï¼ˆTree-structured Parzen Estimatorï¼‰</strong>ã‚’ä½¿ç”¨ã—ã¦ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ææ¡ˆ</li>
<li>éå»ã®è©¦è¡Œçµæœã‹ã‚‰å­¦ç¿’ã—ã€æœ‰æœ›ãªé ˜åŸŸã‚’å„ªå…ˆçš„ã«æ¢ç´¢</li>
</ol>

<div class="mermaid">
graph LR
    A[éå»ã®è©¦è¡Œãƒ‡ãƒ¼ã‚¿] --> B[TPEãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰]
    B --> C[æœ‰æœ›ãªè¨­å®šã‚’ææ¡ˆ]
    C --> D[Successive Halvingã§è©•ä¾¡]
    D --> E[çµæœã‚’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯]
    E --> A

    style A fill:#e3f2fd
    style B fill:#f3e5f5
    style C fill:#fff3e0
    style D fill:#ffebee
    style E fill:#e8f5e9
</div>

<h3>å®Ÿè£…ã¨æ´»ç”¨ã‚·ãƒ¼ãƒ³</h3>

<pre><code class="language-python">import optuna
from optuna.samplers import TPESampler
from optuna.pruners import HyperbandPruner
from sklearn.datasets import load_digits
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import cross_val_score

# BOHBè¨­å®šï¼ˆTPE + Hyperbandï¼‰
sampler = TPESampler(seed=42, n_startup_trials=10)
pruner = HyperbandPruner(
    min_resource=5,
    max_resource=100,
    reduction_factor=3
)

def objective_bohb(trial):
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ææ¡ˆï¼ˆTPEãŒè³¢ãé¸æŠï¼‰
    hidden_layer_size = trial.suggest_int('hidden_layer_size', 50, 200)
    alpha = trial.suggest_float('alpha', 1e-5, 1e-1, log=True)
    learning_rate_init = trial.suggest_float('learning_rate_init', 1e-4, 1e-1, log=True)

    X, y = load_digits(return_X_y=True)

    # Hyperband: æ®µéšçš„ã«max_iterã‚’å¢—ã‚„ã™
    for step in range(1, 6):
        max_iter = int(100 * step / 5)

        model = MLPClassifier(
            hidden_layer_sizes=(hidden_layer_size,),
            alpha=alpha,
            learning_rate_init=learning_rate_init,
            max_iter=max_iter,
            random_state=42
        )

        score = cross_val_score(model, X, y, cv=3, n_jobs=-1).mean()

        trial.report(score, step)
        if trial.should_prune():
            raise optuna.TrialPruned()

    return score

# BOHBã‚¹ã‚¿ãƒ‡ã‚£
study_bohb = optuna.create_study(
    direction='maximize',
    sampler=sampler,
    pruner=pruner,
    study_name='bohb_example'
)

study_bohb.optimize(objective_bohb, n_trials=50, timeout=180)

print("\n=== BOHB æœ€é©åŒ–çµæœ ===")
print(f"æœ€è‰¯ã‚¹ã‚³ã‚¢: {study_bohb.best_value:.4f}")
print(f"æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
for key, value in study_bohb.best_params.items():
    print(f"  {key}: {value}")
print(f"\nå®Œäº†/æåˆˆã‚Š: {len([t for t in study_bohb.trials if t.state == optuna.trial.TrialState.COMPLETE])}/{len([t for t in study_bohb.trials if t.state == optuna.trial.TrialState.PRUNED])}")
</code></pre>

<h3>æ´»ç”¨ã‚·ãƒ¼ãƒ³</h3>

<ul>
<li><strong>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</strong>: ã‚¨ãƒãƒƒã‚¯æ•°ã‚’æ®µéšçš„ã«å¢—ã‚„ã™</li>
<li><strong>ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’</strong>: å¼±å­¦ç¿’å™¨ã®æ•°ã‚’æ®µéšçš„ã«å¢—ã‚„ã™</li>
<li><strong>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿</strong>: ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’æ®µéšçš„ã«å¢—ã‚„ã™</li>
</ul>

<hr>

<h2>3.3 Population-based Training (PBT)</h2>

<h3>PBTã®åŸç†</h3>

<p><strong>Population-based Training</strong>ã¯ã€è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸¦åˆ—ã«å­¦ç¿’ã•ã›ã€å®šæœŸçš„ã«ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š</p>

<ol>
<li><strong>Exploitï¼ˆæ´»ç”¨ï¼‰</strong>: æ€§èƒ½ã®æ‚ªã„ãƒ¢ãƒ‡ãƒ«ã‚’æ€§èƒ½ã®è‰¯ã„ãƒ¢ãƒ‡ãƒ«ã«ç½®ãæ›ãˆ</li>
<li><strong>Exploreï¼ˆæ¢ç´¢ï¼‰</strong>: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ‘‚å‹•ã•ã›ã¦æ–°ã—ã„è¨­å®šã‚’è©¦ã™</li>
</ol>

<blockquote>
<p><strong>ç‰¹å¾´</strong>: å­¦ç¿’ä¸­ã«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‹•çš„ã«èª¿æ•´ã§ãã‚‹ç‚¹ãŒã€å¾“æ¥æ‰‹æ³•ã¨ã®å¤§ããªé•ã„ã€‚</p>
</blockquote>

<h3>PBTã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</h3>

<div class="mermaid">
graph TD
    A[PopulationåˆæœŸåŒ–<br/>nå€‹ã®ãƒ¢ãƒ‡ãƒ«] --> B[å„ãƒ¢ãƒ‡ãƒ«ã‚’ä¸¦åˆ—å­¦ç¿’]
    B --> C{å®šæœŸçš„ãªè©•ä¾¡ãƒã‚¤ãƒ³ãƒˆ}
    C --> D[æ€§èƒ½ã®æ‚ªã„ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®š]
    D --> E[è‰¯ã„ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ã‚³ãƒ”ãƒ¼<br/>Exploit]
    E --> F[ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ‘‚å‹•<br/>Explore]
    F --> G{å­¦ç¿’çµ‚äº†?}
    G -->|No| B
    G -->|Yes| H[æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style E fill:#e3f2fd
    style F fill:#e8f5e9
    style H fill:#c8e6c9
</div>

<h3>Ray Tuneã§ã®å®Ÿè£…</h3>

<pre><code class="language-python">from ray import tune
from ray.tune.schedulers import PopulationBasedTraining
import numpy as np

def train_function(config):
    """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–¢æ•°ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
    # åˆæœŸè¨­å®š
    learning_rate = config["lr"]
    momentum = config["momentum"]

    # å­¦ç¿’ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    for step in range(100):
        # ãƒ€ãƒŸãƒ¼ã®æ€§èƒ½æŒ‡æ¨™ï¼ˆå®Ÿéš›ã«ã¯ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼‰
        # å­¦ç¿’ç‡ã¨ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ ãŒé©åˆ‡ãªç¯„å›²ã§è‰¯ã„æ€§èƒ½
        optimal_lr = 0.01
        optimal_momentum = 0.9

        score = 1.0 - (
            abs(learning_rate - optimal_lr) / optimal_lr +
            abs(momentum - optimal_momentum) / optimal_momentum
        ) / 2

        # ãƒã‚¤ã‚ºã‚’è¿½åŠ ã—ã¦ãƒªã‚¢ãƒ«ãªå­¦ç¿’ã‚’æ¨¡å€£
        score += np.random.normal(0, 0.05)

        # Ray Tuneã«çµæœã‚’å ±å‘Š
        tune.report(score=score, lr=learning_rate, momentum=momentum)

# PBTã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®è¨­å®š
pbt_scheduler = PopulationBasedTraining(
    time_attr="training_iteration",
    metric="score",
    mode="max",
    perturbation_interval=10,  # 10ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã”ã¨ã«æ‘‚å‹•
    hyperparam_mutations={
        "lr": lambda: np.random.uniform(0.001, 0.1),
        "momentum": lambda: np.random.uniform(0.8, 0.99)
    }
)

# Ray Tuneã®å®Ÿè¡Œ
analysis = tune.run(
    train_function,
    name="pbt_example",
    scheduler=pbt_scheduler,
    num_samples=8,  # 8å€‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸¦åˆ—å®Ÿè¡Œ
    config={
        "lr": tune.uniform(0.001, 0.1),
        "momentum": tune.uniform(0.8, 0.99)
    },
    stop={"training_iteration": 100},
    verbose=1
)

print("\n=== PBT æœ€é©åŒ–çµæœ ===")
best_config = analysis.get_best_config(metric="score", mode="max")
print(f"æœ€è‰¯è¨­å®š:")
print(f"  Learning Rate: {best_config['lr']:.4f}")
print(f"  Momentum: {best_config['momentum']:.4f}")
print(f"\næœ€è‰¯ã‚¹ã‚³ã‚¢: {analysis.best_result['score']:.4f}")
</code></pre>

<h3>ä¸¦åˆ—å­¦ç¿’ã¨ã®çµ„ã¿åˆã‚ã›</h3>

<p>PBTã®æœ€å¤§ã®åˆ©ç‚¹ã¯ã€ä¸¦åˆ—è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’æœ€å¤§é™æ´»ç”¨ã§ãã‚‹ç‚¹ã§ã™ï¼š</p>

<table>
<thead>
<tr>
<th>ã‚·ãƒŠãƒªã‚ª</th>
<th>å¾“æ¥æ‰‹æ³•</th>
<th>PBT</th>
</tr>
</thead>
<tbody>
<tr>
<td>8 GPUã§100ã‚¨ãƒãƒƒã‚¯</td>
<td>é€æ¬¡ã«8è¨­å®šã‚’è©¦ã™<br/>800ã‚¨ãƒãƒƒã‚¯åˆ†ã®æ™‚é–“</td>
<td>8è¨­å®šã‚’åŒæ™‚å­¦ç¿’<br/>100ã‚¨ãƒãƒƒã‚¯åˆ†ã®æ™‚é–“</td>
</tr>
<tr>
<td>å‹•çš„èª¿æ•´</td>
<td>ä¸å¯</td>
<td>å­¦ç¿’ä¸­ã«æœ€é©åŒ–</td>
</tr>
<tr>
<td>ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡</td>
<td>æ€§èƒ½æ‚ªã„è¨­å®šã‚‚æœ€å¾Œã¾ã§</td>
<td>æ—©æœŸã«è‰¯ã„è¨­å®šã«åæŸ</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.4 ãã®ä»–ã®é«˜åº¦ãªæ‰‹æ³•</h2>

<h3>Hyperopt (TPEå®Ÿè£…)</h3>

<p><strong>Hyperopt</strong>ã¯ã€Tree-structured Parzen Estimator (TPE)ã‚’å®Ÿè£…ã—ãŸäººæ°—ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚</p>

<pre><code class="language-python">from hyperopt import fmin, tpe, hp, STATUS_OK, Trials
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_val_score
import numpy as np

# æ¢ç´¢ç©ºé–“ã®å®šç¾©
space = {
    'n_estimators': hp.quniform('n_estimators', 50, 300, 1),
    'max_depth': hp.quniform('max_depth', 3, 15, 1),
    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.3)),
    'subsample': hp.uniform('subsample', 0.5, 1.0),
    'min_samples_split': hp.quniform('min_samples_split', 2, 20, 1)
}

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X, y = load_breast_cancer(return_X_y=True)

def objective_hyperopt(params):
    """Hyperoptç”¨ã®ç›®çš„é–¢æ•°"""
    # æ•´æ•°å‹ã«å¤‰æ›
    params['n_estimators'] = int(params['n_estimators'])
    params['max_depth'] = int(params['max_depth'])
    params['min_samples_split'] = int(params['min_samples_split'])

    model = GradientBoostingClassifier(**params, random_state=42)
    score = cross_val_score(model, X, y, cv=5, n_jobs=-1).mean()

    # Hyperoptã¯æœ€å°åŒ–ãªã®ã§ã€è² ã®å€¤ã‚’è¿”ã™
    return {'loss': -score, 'status': STATUS_OK}

# æœ€é©åŒ–å®Ÿè¡Œ
trials = Trials()
best = fmin(
    fn=objective_hyperopt,
    space=space,
    algo=tpe.suggest,  # TPEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    max_evals=50,
    trials=trials,
    rstate=np.random.default_rng(42)
)

print("\n=== Hyperopt (TPE) æœ€é©åŒ–çµæœ ===")
print("æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
for key, value in best.items():
    print(f"  {key}: {value}")
print(f"\næœ€è‰¯ã‚¹ã‚³ã‚¢: {-min(trials.losses()):.4f}")
</code></pre>

<h3>SMAC (Random Forest based)</h3>

<p><strong>SMAC (Sequential Model-based Algorithm Configuration)</strong>ã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚’ã‚µãƒ­ã‚²ãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚</p>

<p>ç‰¹å¾´ï¼š</p>
<ul>
<li>ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã¨æ¡ä»¶ä»˜ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¼·ã„</li>
<li>ä¸ç¢ºå®Ÿæ€§æ¨å®šãŒå„ªã‚Œã¦ã„ã‚‹</li>
<li>ãƒã‚¤ã‚ºã®å¤šã„ç›®çš„é–¢æ•°ã«ãƒ­ãƒã‚¹ãƒˆ</li>
</ul>

<h3>Ax/BoTorch (Facebook Research)</h3>

<p><strong>Ax</strong>ã¨<strong>BoTorch</strong>ã¯ã€Facebook ResearchãŒé–‹ç™ºã—ãŸæ¬¡ä¸–ä»£ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚</p>

<pre><code class="language-python">from ax.service.ax_client import AxClient
from sklearn.datasets import load_wine
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score

# Axã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä½œæˆ
ax_client = AxClient()

# æ¢ç´¢ç©ºé–“ã®å®šç¾©
ax_client.create_experiment(
    name="svm_optimization",
    parameters=[
        {"name": "C", "type": "range", "bounds": [0.1, 100.0], "log_scale": True},
        {"name": "gamma", "type": "range", "bounds": [0.0001, 1.0], "log_scale": True},
        {"name": "kernel", "type": "choice", "values": ["rbf", "poly", "sigmoid"]}
    ],
    objective_name="accuracy",
    minimize=False
)

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X, y = load_wine(return_X_y=True)

# æœ€é©åŒ–ãƒ«ãƒ¼ãƒ—
for i in range(30):
    # æ¬¡ã®è¨­å®šã‚’ææ¡ˆ
    parameters, trial_index = ax_client.get_next_trial()

    # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
    model = SVC(**parameters, random_state=42)
    score = cross_val_score(model, X, y, cv=5, n_jobs=-1).mean()

    # çµæœã‚’å ±å‘Š
    ax_client.complete_trial(trial_index=trial_index, raw_data=score)

# æœ€è‰¯è¨­å®šã®å–å¾—
best_parameters, metrics = ax_client.get_best_parameters()

print("\n=== Ax/BoTorch æœ€é©åŒ–çµæœ ===")
print("æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
for key, value in best_parameters.items():
    print(f"  {key}: {value}")
print(f"\næœ€è‰¯ç²¾åº¦: {metrics[0]['accuracy']:.4f}")
print(f"ä¿¡é ¼åŒºé–“: [{metrics[0]['accuracy'] - metrics[1]['accuracy']['accuracy']:.4f}, "
      f"{metrics[0]['accuracy'] + metrics[1]['accuracy']['accuracy']:.4f}]")
</code></pre>

<h3>æ‰‹æ³•æ¯”è¼ƒè¡¨</h3>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>ã‚µãƒ­ã‚²ãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«</th>
<th>å¼·ã¿</th>
<th>é©ç”¨å ´é¢</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Hyperopt (TPE)</strong></td>
<td>ã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®š</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ã€é«˜é€Ÿ</td>
<td>ä¸€èˆ¬çš„ãªæœ€é©åŒ–</td>
</tr>
<tr>
<td><strong>SMAC</strong></td>
<td>ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ</td>
<td>æ¡ä»¶ä»˜ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</td>
<td>è¤‡é›‘ãªæ¢ç´¢ç©ºé–“</td>
</tr>
<tr>
<td><strong>Ax/BoTorch</strong></td>
<td>ã‚¬ã‚¦ã‚¹éç¨‹</td>
<td>ä¸ç¢ºå®Ÿæ€§æ¨å®šã€ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯</td>
<td>ç ”ç©¶ãƒ»å®Ÿé¨“</td>
</tr>
<tr>
<td><strong>Optuna</strong></td>
<td>TPE/GP/CMA-ES</td>
<td>æŸ”è»Ÿã€æåˆˆã‚Š</td>
<td>å®Ÿç”¨çš„ãªæœ€é©åŒ–</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.5 å®Ÿè·µ: Ray Tuneã«ã‚ˆã‚‹å¤§è¦æ¨¡ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</h2>

<h3>Ray Tuneã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—</h3>

<p><strong>Ray Tune</strong>ã¯ã€åˆ†æ•£ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãŸã‚ã®çµ±ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚</p>

<pre><code class="language-python">import ray
from ray import tune
from ray.tune.schedulers import ASHAScheduler
from ray.tune.search.bayesopt import BayesOptSearch
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
import numpy as np

# Rayã®åˆæœŸåŒ–
ray.init(ignore_reinit_error=True)

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X, y = make_classification(
    n_samples=10000, n_features=20, n_informative=15,
    n_redundant=5, random_state=42
)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# PyTorchãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
train_dataset = TensorDataset(
    torch.FloatTensor(X_train),
    torch.LongTensor(y_train)
)
test_dataset = TensorDataset(
    torch.FloatTensor(X_test),
    torch.LongTensor(y_test)
)

def train_model(config):
    """Ray Tuneç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–¢æ•°"""
    # ãƒ¢ãƒ‡ãƒ«å®šç¾©
    model = nn.Sequential(
        nn.Linear(20, config["hidden_size_1"]),
        nn.ReLU(),
        nn.Dropout(config["dropout"]),
        nn.Linear(config["hidden_size_1"], config["hidden_size_2"]),
        nn.ReLU(),
        nn.Dropout(config["dropout"]),
        nn.Linear(config["hidden_size_2"], 2)
    )

    # æœ€é©åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=config["lr"])
    criterion = nn.CrossEntropyLoss()

    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    train_loader = DataLoader(
        train_dataset,
        batch_size=config["batch_size"],
        shuffle=True
    )
    test_loader = DataLoader(test_dataset, batch_size=256)

    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—
    for epoch in range(50):
        model.train()
        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

        # æ¤œè¨¼
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for batch_X, batch_y in test_loader:
                outputs = model(batch_X)
                _, predicted = torch.max(outputs.data, 1)
                total += batch_y.size(0)
                correct += (predicted == batch_y).sum().item()

        accuracy = correct / total

        # Ray Tuneã«å ±å‘Š
        tune.report(accuracy=accuracy, epoch=epoch)

# æ¢ç´¢ç©ºé–“
search_space = {
    "hidden_size_1": tune.choice([32, 64, 128, 256]),
    "hidden_size_2": tune.choice([16, 32, 64, 128]),
    "lr": tune.loguniform(1e-4, 1e-1),
    "batch_size": tune.choice([32, 64, 128]),
    "dropout": tune.uniform(0.1, 0.5)
}

print("=== Ray Tune ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº† ===")
print(f"æ¢ç´¢ç©ºé–“: {len(search_space)}æ¬¡å…ƒ")
</code></pre>

<h3>PBTã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®æ´»ç”¨</h3>

<pre><code class="language-python">from ray.tune.schedulers import PopulationBasedTraining

# PBTã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
pbt = PopulationBasedTraining(
    time_attr="epoch",
    metric="accuracy",
    mode="max",
    perturbation_interval=5,
    hyperparam_mutations={
        "lr": lambda: 10 ** np.random.uniform(-4, -1),
        "dropout": lambda: np.random.uniform(0.1, 0.5)
    }
)

# Ray Tuneå®Ÿè¡Œï¼ˆPBTï¼‰
analysis_pbt = tune.run(
    train_model,
    name="pbt_neural_net",
    scheduler=pbt,
    num_samples=8,  # 8ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸¦åˆ—å®Ÿè¡Œ
    config=search_space,
    resources_per_trial={"cpu": 2, "gpu": 0},  # GPUåˆ©ç”¨æ™‚ã¯å¤‰æ›´
    verbose=1
)

print("\n=== PBTå®Ÿè¡Œçµæœ ===")
best_trial_pbt = analysis_pbt.get_best_trial("accuracy", "max", "last")
print(f"æœ€è‰¯ç²¾åº¦: {best_trial_pbt.last_result['accuracy']:.4f}")
print(f"æœ€è‰¯è¨­å®š:")
for key, value in best_trial_pbt.config.items():
    print(f"  {key}: {value}")
</code></pre>

<h3>åˆ†æ•£ç’°å¢ƒã§ã®å®Ÿè¡Œ</h3>

<p>Ray Tuneã¯ã€è¤‡æ•°ãƒã‚·ãƒ³ã§ã®åˆ†æ•£å®Ÿè¡Œã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ï¼š</p>

<pre><code class="language-python"># ASHAã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ© + ãƒ™ã‚¤ã‚ºæœ€é©åŒ–
from ray.tune.schedulers import ASHAScheduler
from ray.tune.search.bayesopt import BayesOptSearch

# ASHAã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼ˆHyperbandã®æ”¹è‰¯ç‰ˆï¼‰
asha_scheduler = ASHAScheduler(
    max_t=50,              # æœ€å¤§ã‚¨ãƒãƒƒã‚¯
    grace_period=5,        # æœ€å°ã‚¨ãƒãƒƒã‚¯
    reduction_factor=3     # å‰Šæ¸›ç‡
)

# ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚µãƒ¼ãƒãƒ£ãƒ¼
bayesopt = BayesOptSearch(
    metric="accuracy",
    mode="max"
)

# åˆ†æ•£å®Ÿè¡Œ
analysis_distributed = tune.run(
    train_model,
    name="distributed_tuning",
    scheduler=asha_scheduler,
    search_alg=bayesopt,
    num_samples=100,  # 100è©¦è¡Œ
    config=search_space,
    resources_per_trial={"cpu": 2},
    verbose=1
)

print("\n=== åˆ†æ•£ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœ ===")
best_trial = analysis_distributed.get_best_trial("accuracy", "max", "last")
print(f"æœ€è‰¯ç²¾åº¦: {best_trial.last_result['accuracy']:.4f}")
print(f"\nè©¦è¡Œçµ±è¨ˆ:")
print(f"  å®Œäº†è©¦è¡Œ: {len(analysis_distributed.trials)}")
print(f"  å¹³å‡ç²¾åº¦: {np.mean([t.last_result['accuracy'] for t in analysis_distributed.trials if 'accuracy' in t.last_result]):.4f}")

# çµæœã®å¯è¦–åŒ–
import pandas as pd

df = analysis_distributed.results_df
print(f"\n=== ãƒˆãƒƒãƒ—5è¨­å®š ===")
top5 = df.nlargest(5, 'accuracy')[['accuracy', 'config/hidden_size_1', 'config/lr', 'config/dropout']]
print(top5)

# Rayã®ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
ray.shutdown()
</code></pre>

<h3>Ray Tuneã®åˆ©ç‚¹</h3>

<table>
<thead>
<tr>
<th>æ©Ÿèƒ½</th>
<th>èª¬æ˜</th>
<th>åˆ©ç‚¹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>çµ±ä¸€API</strong></td>
<td>è¤‡æ•°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©/ã‚µãƒ¼ãƒãƒ£ãƒ¼ã‚’çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹</td>
<td>ç°¡å˜ã«æ‰‹æ³•ã‚’åˆ‡ã‚Šæ›¿ãˆ</td>
</tr>
<tr>
<td><strong>åˆ†æ•£å®Ÿè¡Œ</strong></td>
<td>è¤‡æ•°ãƒã‚·ãƒ³ã§è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°</td>
<td>å¤§è¦æ¨¡æ¢ç´¢ãŒå¯èƒ½</td>
</tr>
<tr>
<td><strong>æ—©æœŸåœæ­¢</strong></td>
<td>ASHAã€Hyperbandã€Medianãªã©</td>
<td>è¨ˆç®—è³‡æºã®ç¯€ç´„</td>
</tr>
<tr>
<td><strong>ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ</strong></td>
<td>ä¸­æ–­ãƒ»å†é–‹ã‚µãƒãƒ¼ãƒˆ</td>
<td>é•·æ™‚é–“å®Ÿè¡Œã®å®‰å…¨æ€§</td>
</tr>
<tr>
<td><strong>å¯è¦–åŒ–</strong></td>
<td>TensorBoardçµ±åˆ</td>
<td>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.6 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>Hyperband</strong></p>
<ul>
<li>Successive Halvingã§åŠ¹ç‡çš„ãªãƒªã‚½ãƒ¼ã‚¹é…åˆ†</li>
<li>æ—©æœŸã«æ€§èƒ½ã®æ‚ªã„è¨­å®šã‚’é™¤å¤–</li>
<li>Optunaã§ç°¡å˜ã«å®Ÿè£…å¯èƒ½</li>
</ul></li>

<li><p><strong>BOHB</strong></p>
<ul>
<li>Hyperbandã¨TPEã®èåˆ</li>
<li>åŠ¹ç‡çš„ãªãƒªã‚½ãƒ¼ã‚¹é…åˆ†ã¨è³¢ã„æ¢ç´¢ã®ä¸¡ç«‹</li>
<li>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãªã©ã§ç‰¹ã«æœ‰åŠ¹</li>
</ul></li>

<li><p><strong>Population-based Training</strong></p>
<ul>
<li>ä¸¦åˆ—å­¦ç¿’ä¸­ã«å‹•çš„ã«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´</li>
<li>Exploitï¼ˆæ´»ç”¨ï¼‰ã¨Exploreï¼ˆæ¢ç´¢ï¼‰ã®ãƒãƒ©ãƒ³ã‚¹</li>
<li>å¤§è¦æ¨¡ä¸¦åˆ—ç’°å¢ƒã§çœŸä¾¡ã‚’ç™ºæ®</li>
</ul></li>

<li><p><strong>ãã®ä»–ã®æ‰‹æ³•</strong></p>
<ul>
<li>Hyperopt: ã‚·ãƒ³ãƒ—ãƒ«ã§é«˜é€ŸãªTPEå®Ÿè£…</li>
<li>SMAC: æ¡ä»¶ä»˜ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¼·ã„</li>
<li>Ax/BoTorch: æœ€å…ˆç«¯ã®ãƒ™ã‚¤ã‚ºæœ€é©åŒ–</li>
</ul></li>

<li><p><strong>Ray Tune</strong></p>
<ul>
<li>çµ±ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§è¤‡æ•°æ‰‹æ³•ã‚’æ´»ç”¨</li>
<li>åˆ†æ•£ç’°å¢ƒã§ã®å¤§è¦æ¨¡ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</li>
<li>å®Ÿç”¨çš„ãªãƒ„ãƒ¼ãƒ«ã¨ã®çµ±åˆ</li>
</ul></li>
</ol>

<h3>æ‰‹æ³•é¸æŠã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³</h3>

<table>
<thead>
<tr>
<th>ã‚·ãƒŠãƒªã‚ª</th>
<th>æ¨å¥¨æ‰‹æ³•</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td>é™ã‚‰ã‚ŒãŸè¨ˆç®—è³‡æº</td>
<td>Hyperband</td>
<td>åŠ¹ç‡çš„ãªãƒªã‚½ãƒ¼ã‚¹é…åˆ†</td>
</tr>
<tr>
<td>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ</td>
<td>BOHBã€PBT</td>
<td>æ®µéšçš„å­¦ç¿’ã¨å‹•çš„èª¿æ•´</td>
</tr>
<tr>
<td>å¤§è¦æ¨¡ä¸¦åˆ—ç’°å¢ƒ</td>
<td>PBTã€Ray Tune</td>
<td>ä¸¦åˆ—ãƒªã‚½ãƒ¼ã‚¹ã‚’æœ€å¤§æ´»ç”¨</td>
</tr>
<tr>
<td>æ¡ä»¶ä»˜ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</td>
<td>SMAC</td>
<td>è¤‡é›‘ãªæ¢ç´¢ç©ºé–“ã«å¯¾å¿œ</td>
</tr>
<tr>
<td>ç ”ç©¶ãƒ»å®Ÿé¨“</td>
<td>Ax/BoTorch</td>
<td>æœ€å…ˆç«¯æ‰‹æ³•ã¨ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºæ€§</td>
</tr>
<tr>
<td>å®Ÿç”¨çš„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ</td>
<td>Optunaã€Ray Tune</td>
<td>ä½¿ã„ã‚„ã™ã•ã¨å®Ÿç¸¾</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ç« ã¸</h3>

<p>ç¬¬4ç« ã§ã¯ã€<strong>å®Ÿè·µçš„ãªæœ€é©åŒ–æˆ¦ç•¥</strong>ã‚’å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>æ¢ç´¢ç©ºé–“ã®è¨­è¨ˆãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</li>
<li>ä¸¦åˆ—åŒ–ã¨åˆ†æ•£å®Ÿè¡Œã®æœ€é©åŒ–</li>
<li>çµæœã®åˆ†æã¨å¯è¦–åŒ–</li>
<li>æœ¬ç•ªç’°å¢ƒã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤</li>
</ul>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Li, L., et al. (2018). "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization". <em>Journal of Machine Learning Research</em>, 18(185), 1-52.</li>
<li>Falkner, S., Klein, A., & Hutter, F. (2018). "BOHB: Robust and Efficient Hyperparameter Optimization at Scale". <em>ICML 2018</em>.</li>
<li>Jaderberg, M., et al. (2017). "Population Based Training of Neural Networks". <em>arXiv:1711.09846</em>.</li>
<li>Liaw, R., et al. (2018). "Tune: A Research Platform for Distributed Model Selection and Training". <em>arXiv:1807.05118</em>.</li>
<li>Bergstra, J., et al. (2013). "Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures". <em>ICML 2013</em>.</li>
</ol>

<div class="navigation">
    <a href="chapter2-bayesian-optimization.html" class="nav-button">â† å‰ã®ç« : ãƒ™ã‚¤ã‚ºæœ€é©åŒ–</a>
    <a href="chapter4-practical-strategies.html" class="nav-button">æ¬¡ã®ç« : å®Ÿè·µçš„æœ€é©åŒ–æˆ¦ç•¥ â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
