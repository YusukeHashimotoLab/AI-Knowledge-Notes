<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="ç¬¬4ç« ï¼šæ©Ÿæ¢°å­¦ç¿’ã®æ¦‚è¦ - æ©Ÿæ¢°å­¦ç¿’å…¥é–€ã‚·ãƒªãƒ¼ã‚º">
    <title>ç¬¬4ç« ï¼šæ©Ÿæ¢°å­¦ç¿’ã®æ¦‚è¦ - æ©Ÿæ¢°å­¦ç¿’å…¥é–€ - AI Terakoya</title>

    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header .container {
            padding: 0 1.5rem;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }

        .meta span {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
        }

        /* Typography */
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            color: var(--primary-color);
        }

        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.6rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1.2rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--link-hover);
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1.2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Code blocks */
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border: 1px solid var(--border-color);
        }

        pre code {
            background: none;
            padding: 0;
            font-size: 0.9rem;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
            overflow-x: auto;
            display: block;
        }

        thead {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        tbody {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        th, td {
            padding: 0.8rem;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        th {
            background: var(--primary-color);
            color: white;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background: #f9f9f9;
        }

        /* Blockquotes */
        blockquote {
            border-left: 4px solid var(--secondary-color);
            padding-left: 1.5rem;
            margin: 1.5rem 0;
            font-style: italic;
            color: #666;
        }

        /* Mermaid diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
            background: white;
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        /* Details/Summary */
        details {
            margin: 1rem 0;
            padding: 1rem;
            background: #f8f9fa;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--primary-color);
            padding: 0.5rem;
        }

        summary:hover {
            color: var(--secondary-color);
        }

        /* Footer */
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        /* Navigation buttons */
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }

        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            h1 {
                font-size: 1.6rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.85rem;
            }

            table {
                font-size: 0.9rem;
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <!-- MathJax for equations -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({ startOnLoad: true, theme: 'default' });
            }
        });
    </script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/ml-introduction/index.html">Ml</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 4</span>
        </div>
    </nav>

        <header>
        <div class="container">
            <h1>ç¬¬4ç« ï¼šæ©Ÿæ¢°å­¦ç¿’ã®æ¦‚è¦</h1>
            <div class="meta">
                <span>ğŸ“– èª­äº†æ™‚é–“: 20-25åˆ†</span>
                <span>ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 10å€‹</span>
                <span>ğŸ“ æ¼”ç¿’: 5å•</span>
                <span>ğŸ“Š é›£æ˜“åº¦: å…¥é–€</span>
            </div>
        </div>
    </header>

    <main class="container">
        <p><strong>æ©Ÿæ¢°å­¦ç¿’ã®ä¸–ç•Œã¸ã‚ˆã†ã“ã - ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ã¶AIã®åŸºç¤</strong></p>

        <h2 id="introduction">ã¯ã˜ã‚ã«</h2>
        <p><strong>æ©Ÿæ¢°å­¦ç¿’ï¼ˆMachine Learningï¼‰</strong>ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è‡ªå‹•çš„ã«å­¦ç¿’ã—ã€äºˆæ¸¬ã‚„åˆ¤æ–­ã‚’è¡Œã†æŠ€è¡“ã§ã™ã€‚ã“ã‚Œã¾ã§å­¦ã‚“ã Pythonã€NumPyã€Pandasã®çŸ¥è­˜ã‚’ä½¿ã£ã¦ã€ã„ã‚ˆã„ã‚ˆæ©Ÿæ¢°å­¦ç¿’ã®ä¸–ç•Œã«è¸ã¿å‡ºã—ã¾ã—ã‚‡ã†ã€‚</p>

        <p>ã“ã®ç« ã§ã¯ã€ä»¥ä¸‹ã®å†…å®¹ã‚’å­¦ã³ã¾ã™ï¼š</p>
        <ul>
            <li>æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã‹ - å®šç¾©ã¨å¿œç”¨ä¾‹</li>
            <li>æ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼ˆå›å¸°ãƒ»åˆ†é¡ï¼‰</li>
            <li>æ•™å¸«ãªã—å­¦ç¿’ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ»æ¬¡å…ƒå‰Šæ¸›ï¼‰</li>
            <li>è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²</li>
            <li>scikit-learnã®åŸºæœ¬çš„ãªä½¿ã„æ–¹</li>
            <li>å®Ÿè·µçš„ãªæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…</li>
        </ul>

        <blockquote>
            <strong>æ©Ÿæ¢°å­¦ç¿’ã®å®šç¾©</strong><br>
            ã€Œæ˜ç¤ºçš„ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã•ã‚Œã‚‹ã“ã¨ãªãã€çµŒé¨“ï¼ˆãƒ‡ãƒ¼ã‚¿ï¼‰ã‹ã‚‰å­¦ç¿’ã™ã‚‹èƒ½åŠ›ã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«ä¸ãˆã‚‹ç ”ç©¶åˆ†é‡ã€- Arthur Samuel (1959)
        </blockquote>

        <h2 id="what-is-ml">1. æ©Ÿæ¢°å­¦ç¿’ã¨ã¯</h2>

        <h3>1.1 æ©Ÿæ¢°å­¦ç¿’ã®å®šç¾©</h3>
        <p>æ©Ÿæ¢°å­¦ç¿’ã¯ã€å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è‡ªå‹•çš„ã«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¦‹ã¤ã‘å‡ºã—ã€ãã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ã£ã¦æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦äºˆæ¸¬ã‚„åˆ¤æ–­ã‚’è¡Œã†æŠ€è¡“ã§ã™ã€‚</p>

        <p><strong>å¾“æ¥ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° vs æ©Ÿæ¢°å­¦ç¿’</strong></p>

        <table>
            <thead>
                <tr>
                    <th>é …ç›®</th>
                    <th>å¾“æ¥ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°</th>
                    <th>æ©Ÿæ¢°å­¦ç¿’</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>ãƒ«ãƒ¼ãƒ«</td>
                    <td>äººé–“ãŒæ˜ç¤ºçš„ã«è¨˜è¿°</td>
                    <td>ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è‡ªå‹•çš„ã«å­¦ç¿’</td>
                </tr>
                <tr>
                    <td>å…¥åŠ›</td>
                    <td>ãƒ‡ãƒ¼ã‚¿ + ãƒ«ãƒ¼ãƒ«</td>
                    <td>ãƒ‡ãƒ¼ã‚¿ + æ­£è§£ï¼ˆãƒ©ãƒ™ãƒ«ï¼‰</td>
                </tr>
                <tr>
                    <td>å‡ºåŠ›</td>
                    <td>å‡¦ç†çµæœ</td>
                    <td>å­¦ç¿’ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«</td>
                </tr>
                <tr>
                    <td>é©ç”¨ä¾‹</td>
                    <td>è¨ˆç®—ã€ãƒ‡ãƒ¼ã‚¿å‡¦ç†</td>
                    <td>äºˆæ¸¬ã€èªè­˜ã€æ¨è–¦</td>
                </tr>
            </tbody>
        </table>

        <div class="mermaid">
graph LR
    A[å¾“æ¥ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°] --> B[ãƒ‡ãƒ¼ã‚¿ + ãƒ«ãƒ¼ãƒ«]
    B --> C[å‡ºåŠ›]

    D[æ©Ÿæ¢°å­¦ç¿’] --> E[ãƒ‡ãƒ¼ã‚¿ + æ­£è§£]
    E --> F[å­¦ç¿’]
    F --> G[ãƒ¢ãƒ‡ãƒ«]
    G --> H[äºˆæ¸¬]

    style A fill:#e3f2fd
    style D fill:#fff3e0
    style G fill:#e8f5e9
        </div>

        <h3>1.2 æ©Ÿæ¢°å­¦ç¿’ã®å¿œç”¨ä¾‹</h3>

        <ul>
            <li><strong>ç”»åƒèªè­˜</strong>: é¡”èªè­˜ã€ç‰©ä½“æ¤œå‡ºã€åŒ»ç™‚ç”»åƒè¨ºæ–­</li>
            <li><strong>è‡ªç„¶è¨€èªå‡¦ç†</strong>: ç¿»è¨³ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã€æ„Ÿæƒ…åˆ†æ</li>
            <li><strong>éŸ³å£°èªè­˜</strong>: éŸ³å£°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆï¼ˆSiriã€Alexaï¼‰</li>
            <li><strong>æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ </strong>: å‹•ç”»ãƒ»éŸ³æ¥½ãƒ»å•†å“ã®æ¨è–¦ï¼ˆNetflixã€Amazonï¼‰</li>
            <li><strong>é‡‘è</strong>: æ ªä¾¡äºˆæ¸¬ã€ä¸æ­£æ¤œå‡ºã€ä¿¡ç”¨ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°</li>
            <li><strong>åŒ»ç™‚</strong>: ç—…æ°—ã®è¨ºæ–­ã€å‰µè–¬ã€ã‚²ãƒãƒ è§£æ</li>
            <li><strong>è‡ªå‹•é‹è»¢</strong>: éšœå®³ç‰©æ¤œå‡ºã€çµŒè·¯è¨ˆç”»</li>
        </ul>

        <h2 id="types">2. æ©Ÿæ¢°å­¦ç¿’ã®ç¨®é¡</h2>

        <div class="mermaid">
graph TD
    A[æ©Ÿæ¢°å­¦ç¿’] --> B[æ•™å¸«ã‚ã‚Šå­¦ç¿’]
    A --> C[æ•™å¸«ãªã—å­¦ç¿’]
    A --> D[å¼·åŒ–å­¦ç¿’]

    B --> E[åˆ†é¡]
    B --> F[å›å¸°]

    C --> G[ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°]
    C --> H[æ¬¡å…ƒå‰Šæ¸›]

    D --> I[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’]

    E --> J["ä¾‹: ã‚¹ãƒ‘ãƒ åˆ¤å®š"]
    F --> K["ä¾‹: ä½å®…ä¾¡æ ¼äºˆæ¸¬"]
    G --> L["ä¾‹: é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³"]
    H --> M["ä¾‹: ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–"]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
        </div>

        <h3>2.1 æ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼ˆSupervised Learningï¼‰</h3>
        <p>æ­£è§£ãƒ©ãƒ™ãƒ«ä»˜ãã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦äºˆæ¸¬ã‚’è¡Œã„ã¾ã™ã€‚</p>

        <ul>
            <li><strong>åˆ†é¡ï¼ˆClassificationï¼‰</strong>: ã‚«ãƒ†ã‚´ãƒªã‚’äºˆæ¸¬ï¼ˆä¾‹: ã‚¹ãƒ‘ãƒ ã‹å¦ã‹ã€ç—…æ°—ã®è¨ºæ–­ï¼‰</li>
            <li><strong>å›å¸°ï¼ˆRegressionï¼‰</strong>: æ•°å€¤ã‚’äºˆæ¸¬ï¼ˆä¾‹: ä½å®…ä¾¡æ ¼ã€å£²ä¸Šäºˆæ¸¬ï¼‰</li>
        </ul>

        <h3>2.2 æ•™å¸«ãªã—å­¦ç¿’ï¼ˆUnsupervised Learningï¼‰</h3>
        <p>æ­£è§£ãƒ©ãƒ™ãƒ«ãªã—ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€éš ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚„æ§‹é€ ã‚’ç™ºè¦‹ã—ã¾ã™ã€‚</p>

        <ul>
            <li><strong>ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆClusteringï¼‰</strong>: ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†ã‘ã‚‹ï¼ˆä¾‹: é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰</li>
            <li><strong>æ¬¡å…ƒå‰Šæ¸›ï¼ˆDimensionality Reductionï¼‰</strong>: ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´ã‚’æ¸›ã‚‰ã™ï¼ˆä¾‹: ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ï¼‰</li>
        </ul>

        <h3>2.3 å¼·åŒ–å­¦ç¿’ï¼ˆReinforcement Learningï¼‰</h3>
        <p>ç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦ã€å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹è¡Œå‹•ã‚’å­¦ç¿’ã—ã¾ã™ï¼ˆä¾‹: ã‚²ãƒ¼ãƒ AIã€ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡ï¼‰ã€‚</p>

        <h2 id="supervised">3. æ•™å¸«ã‚ã‚Šå­¦ç¿’ã®åŸºç¤</h2>

        <h3>3.1 åˆ†é¡ã¨å›å¸°ã®é•ã„</h3>

        <table>
            <thead>
                <tr>
                    <th>é …ç›®</th>
                    <th>åˆ†é¡ï¼ˆClassificationï¼‰</th>
                    <th>å›å¸°ï¼ˆRegressionï¼‰</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>äºˆæ¸¬å¯¾è±¡</td>
                    <td>ã‚«ãƒ†ã‚´ãƒªï¼ˆé›¢æ•£å€¤ï¼‰</td>
                    <td>æ•°å€¤ï¼ˆé€£ç¶šå€¤ï¼‰</td>
                </tr>
                <tr>
                    <td>ä¾‹</td>
                    <td>ã€ŒçŠ¬ã€ã‹ã€ŒçŒ«ã€ã‹</td>
                    <td>ä½å®…ä¾¡æ ¼ã¯ã€Œ450ä¸‡å††ã€</td>
                </tr>
                <tr>
                    <td>è©•ä¾¡æŒ‡æ¨™</td>
                    <td>æ­£è§£ç‡ã€F1ã‚¹ã‚³ã‚¢</td>
                    <td>å¹³å‡äºŒä¹—èª¤å·®ï¼ˆMSEï¼‰</td>
                </tr>
                <tr>
                    <td>ä»£è¡¨çš„ãªæ‰‹æ³•</td>
                    <td>ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã€æ±ºå®šæœ¨</td>
                    <td>ç·šå½¢å›å¸°ã€å¤šé …å¼å›å¸°</td>
                </tr>
            </tbody>
        </table>

        <h4>ä¾‹1ï¼šåˆ†é¡ã¨å›å¸°ã®é•ã„</h4>
        <pre><code class="language-python">import numpy as np
import pandas as pd

# åˆ†é¡ã®ä¾‹: ã‚¢ãƒ¤ãƒ¡ã®å“ç¨®åˆ†é¡
# å…¥åŠ›: èŠ±ã³ã‚‰ã®é•·ã•ã€å¹… â†’ å‡ºåŠ›: å“ç¨®ï¼ˆSetosa, Versicolor, Virginicaï¼‰
classification_data = {
    'èŠ±ã³ã‚‰ã®é•·ã•': [1.4, 4.7, 5.1],
    'èŠ±ã³ã‚‰ã®å¹…': [0.2, 1.4, 2.3],
    'å“ç¨®': ['Setosa', 'Versicolor', 'Virginica']  # ã‚«ãƒ†ã‚´ãƒª
}
print("åˆ†é¡ãƒ‡ãƒ¼ã‚¿:")
print(pd.DataFrame(classification_data))

# å›å¸°ã®ä¾‹: ä½å®…ä¾¡æ ¼äºˆæ¸¬
# å…¥åŠ›: é¢ç©ã€éƒ¨å±‹æ•° â†’ å‡ºåŠ›: ä¾¡æ ¼ï¼ˆæ•°å€¤ï¼‰
regression_data = {
    'é¢ç©ï¼ˆã¡ï¼‰': [50, 70, 90],
    'éƒ¨å±‹æ•°': [2, 3, 4],
    'ä¾¡æ ¼ï¼ˆä¸‡å††ï¼‰': [3000, 4200, 5500]  # é€£ç¶šå€¤
}
print("\nå›å¸°ãƒ‡ãƒ¼ã‚¿:")
print(pd.DataFrame(regression_data))
</code></pre>

        <h2 id="train-test">4. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿</h2>

        <h3>4.1 ãªãœåˆ†å‰²ãŒå¿…è¦ã‹ï¼Ÿ</h3>
        <p>æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¯ã€<strong>è¨“ç·´ãƒ‡ãƒ¼ã‚¿</strong>ã§å­¦ç¿’ã—ã€<strong>ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿</strong>ã§è©•ä¾¡ã—ã¾ã™ã€‚åŒã˜ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã¨è©•ä¾¡ã‚’è¡Œã†ã¨ã€ãƒ¢ãƒ‡ãƒ«ã®çœŸã®æ€§èƒ½ãŒåˆ†ã‹ã‚Šã¾ã›ã‚“ã€‚</p>

        <ul>
            <li><strong>è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆTraining Dataï¼‰</strong>: ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ä½¿ç”¨ï¼ˆé€šå¸¸70-80%ï¼‰</li>
            <li><strong>ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆTest Dataï¼‰</strong>: ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã«ä½¿ç”¨ï¼ˆé€šå¸¸20-30%ï¼‰</li>
        </ul>

        <h4>ä¾‹2ï¼šãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²</h4>
        <pre><code class="language-python">from sklearn.model_selection import train_test_split
import numpy as np

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])
y = np.array([0, 1, 0, 1, 0])

print("å…ƒã®ãƒ‡ãƒ¼ã‚¿:")
print("X (ç‰¹å¾´é‡):")
print(X)
print("y (ãƒ©ãƒ™ãƒ«):", y)

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²ï¼ˆ80%è¨“ç·´ã€20%ãƒ†ã‚¹ãƒˆï¼‰
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿:")
print("X_train:")
print(X_train)
print("y_train:", y_train)

print("\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿:")
print("X_test:")
print(X_test)
print("y_test:", y_test)

print("\nãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º:")
print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train)}å€‹, ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)}å€‹")
</code></pre>

        <div class="mermaid">
graph LR
    A[å…¨ãƒ‡ãƒ¼ã‚¿] --> B[è¨“ç·´ãƒ‡ãƒ¼ã‚¿ 80%]
    A --> C[ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ 20%]

    B --> D[å­¦ç¿’]
    D --> E[ãƒ¢ãƒ‡ãƒ«]

    E --> F[è©•ä¾¡]
    C --> F

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style E fill:#e8f5e9
        </div>

        <h2 id="sklearn">5. scikit-learnã®åŸºæœ¬</h2>

        <p><strong>scikit-learn</strong>ã¯ã€Pythonã§æœ€ã‚‚åºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚</p>

        <h3>5.1 scikit-learnã®åŸºæœ¬çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</h3>

        <ol>
            <li>ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™</li>
            <li>ãƒ¢ãƒ‡ãƒ«ã®é¸æŠã¨ä½œæˆ</li>
            <li>ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ï¼ˆ<code>fit</code>ï¼‰</li>
            <li>äºˆæ¸¬ï¼ˆ<code>predict</code>ï¼‰</li>
            <li>è©•ä¾¡ï¼ˆ<code>score</code>ï¼‰</li>
        </ol>

        <h4>ä¾‹3ï¼šscikit-learnã®åŸºæœ¬çš„ãªä½¿ã„æ–¹</h4>
        <pre><code class="language-python">from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# 1. ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆIrisãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰
iris = load_iris()
X = iris.data  # ç‰¹å¾´é‡ï¼ˆèŠ±ã³ã‚‰ã®é•·ã•ãƒ»å¹…ãªã©ï¼‰
y = iris.target  # ãƒ©ãƒ™ãƒ«ï¼ˆå“ç¨®ï¼‰

print("ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶:")
print("X:", X.shape)  # (150, 4) = 150ã‚µãƒ³ãƒ—ãƒ«ã€4ç‰¹å¾´é‡
print("y:", y.shape)  # (150,) = 150ãƒ©ãƒ™ãƒ«

# 2. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

print("\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿: {}å€‹".format(len(X_train)))
print("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {}å€‹".format(len(X_test)))

# 3. ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆï¼ˆk-è¿‘å‚æ³•ï¼‰
model = KNeighborsClassifier(n_neighbors=3)

# 4. ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
model.fit(X_train, y_train)
print("\nãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´å®Œäº†")

# 5. äºˆæ¸¬
y_pred = model.predict(X_test)
print("\näºˆæ¸¬çµæœï¼ˆæœ€åˆã®10å€‹ï¼‰:", y_pred[:10])
print("æ­£è§£ãƒ©ãƒ™ãƒ«ï¼ˆæœ€åˆã®10å€‹ï¼‰:", y_test[:10])

# 6. è©•ä¾¡
accuracy = accuracy_score(y_test, y_pred)
print("\næ­£è§£ç‡: {:.2f}%".format(accuracy * 100))

# ã¾ãŸã¯
score = model.score(X_test, y_test)
print("ã‚¹ã‚³ã‚¢: {:.2f}%".format(score * 100))
</code></pre>

        <h2 id="regression">6. å›å¸°å•é¡Œã®å®Ÿè£…</h2>

        <h3>6.1 ç·šå½¢å›å¸°</h3>
        <p>ç·šå½¢å›å¸°ã¯ã€å…¥åŠ›ã¨å‡ºåŠ›ã®é–¢ä¿‚ã‚’ç›´ç·šã§è¿‘ä¼¼ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</p>

        <p>æ•°å¼: \( y = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b \)</p>

        <h4>ä¾‹4ï¼šç·šå½¢å›å¸°ã®å®Ÿè£…</h4>
        <pre><code class="language-python">from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆï¼ˆä½å®…ä¾¡æ ¼äºˆæ¸¬ï¼‰
np.random.seed(42)
n_samples = 100

# ç‰¹å¾´é‡: é¢ç©ï¼ˆ50-150ã¡ï¼‰
area = np.random.uniform(50, 150, n_samples)

# ç›®çš„å¤‰æ•°: ä¾¡æ ¼ = 30 * é¢ç© + ãƒã‚¤ã‚º
price = 30 * area + np.random.normal(0, 200, n_samples)

# ãƒ‡ãƒ¼ã‚¿ã‚’2æ¬¡å…ƒé…åˆ—ã«å¤‰æ›
X = area.reshape(-1, 1)
y = price

print("ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:")
for i in range(5):
    print(f"é¢ç©: {X[i][0]:.1f}ã¡ â†’ ä¾¡æ ¼: {y[i]:.0f}ä¸‡å††")

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨è¨“ç·´
model = LinearRegression()
model.fit(X_train, y_train)

print("\nå­¦ç¿’æ¸ˆã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
print(f"å‚¾ãï¼ˆä¿‚æ•°ï¼‰: {model.coef_[0]:.2f}")
print(f"åˆ‡ç‰‡: {model.intercept_:.2f}")

# äºˆæ¸¬
y_pred = model.predict(X_test)

# è©•ä¾¡
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("\nè©•ä¾¡æŒ‡æ¨™:")
print(f"å¹³å‡äºŒä¹—èª¤å·®ï¼ˆMSEï¼‰: {mse:.2f}")
print(f"å¹³æ–¹æ ¹å¹³å‡äºŒä¹—èª¤å·®ï¼ˆRMSEï¼‰: {rmse:.2f}")
print(f"æ±ºå®šä¿‚æ•°ï¼ˆRÂ²ï¼‰: {r2:.3f}")

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.scatter(X_test, y_test, alpha=0.5, label='å®Ÿéš›ã®ä¾¡æ ¼')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='äºˆæ¸¬')
plt.xlabel('é¢ç©ï¼ˆã¡ï¼‰')
plt.ylabel('ä¾¡æ ¼ï¼ˆä¸‡å††ï¼‰')
plt.title('ä½å®…ä¾¡æ ¼äºˆæ¸¬ï¼ˆç·šå½¢å›å¸°ï¼‰')
plt.legend()
plt.grid(True, alpha=0.3)
# plt.savefig('linear_regression.png')
# plt.show()

print("\nã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")
</code></pre>

        <h2 id="classification">7. åˆ†é¡å•é¡Œã®å®Ÿè£…</h2>

        <h3>7.1 ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°</h3>
        <p>ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã¯ã€2å€¤åˆ†é¡ï¼ˆ0ã‹1ã‹ï¼‰ã‚’è¡Œã†ãŸã‚ã®åŸºæœ¬çš„ãªæ‰‹æ³•ã§ã™ã€‚</p>

        <h4>ä¾‹5ï¼šãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®å®Ÿè£…</h4>
        <pre><code class="language-python">from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.datasets import load_iris
import numpy as np

# ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆIrisãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€2ã‚¯ãƒ©ã‚¹ã®ã¿ä½¿ç”¨ï¼‰
iris = load_iris()
X = iris.data[:100]  # æœ€åˆã®100ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ2ã‚¯ãƒ©ã‚¹åˆ†ï¼‰
y = iris.target[:100]

print("ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶:")
print("X:", X.shape)
print("y:", y.shape)
print("ã‚¯ãƒ©ã‚¹:", np.unique(y))

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨è¨“ç·´
model = LogisticRegression(max_iter=200)
model.fit(X_train, y_train)

# äºˆæ¸¬
y_pred = model.predict(X_test)

# äºˆæ¸¬ç¢ºç‡
y_pred_proba = model.predict_proba(X_test)

print("\näºˆæ¸¬çµæœï¼ˆæœ€åˆã®5å€‹ï¼‰:")
for i in range(5):
    print(f"äºˆæ¸¬: {y_pred[i]}, æ­£è§£: {y_test[i]}, ç¢ºç‡: {y_pred_proba[i]}")

# è©•ä¾¡
accuracy = accuracy_score(y_test, y_pred)
print(f"\næ­£è§£ç‡: {accuracy:.2%}")

# æ··åŒè¡Œåˆ—
cm = confusion_matrix(y_test, y_pred)
print("\næ··åŒè¡Œåˆ—:")
print(cm)

# è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))
</code></pre>

        <div class="mermaid">
graph TD
    A[åˆ†é¡è©•ä¾¡] --> B[æ··åŒè¡Œåˆ—]
    B --> C[TP: çœŸé™½æ€§]
    B --> D[TN: çœŸé™°æ€§]
    B --> E[FP: å½é™½æ€§]
    B --> F[FN: å½é™°æ€§]

    A --> G[è©•ä¾¡æŒ‡æ¨™]
    G --> H["æ­£è§£ç‡ = (TP+TN)/å…¨ä½“"]
    G --> I["ç²¾åº¦ = TP/(TP+FP)"]
    G --> J["å†ç¾ç‡ = TP/(TP+FN)"]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style G fill:#f3e5f5
        </div>

        <h2 id="iris">8. å®Ÿè·µä¾‹ï¼šIrisãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å¤šã‚¯ãƒ©ã‚¹åˆ†é¡</h2>

        <h4>ä¾‹6ï¼š3ã‚¯ãƒ©ã‚¹åˆ†é¡ã®å®Œå…¨å®Ÿè£…</h4>
        <pre><code class="language-python">from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
import pandas as pd
import numpy as np

# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
iris = load_iris()
X = iris.data
y = iris.target

print("=== Irisãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ===")
print("ç‰¹å¾´é‡å:", iris.feature_names)
print("ã‚¯ãƒ©ã‚¹å:", iris.target_names)
print("ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶:", X.shape)

# ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
df = pd.DataFrame(X, columns=iris.feature_names)
df['species'] = y
df['species_name'] = df['species'].map({
    0: 'setosa', 1: 'versicolor', 2: 'virginica'
})

print("\nãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®5è¡Œ:")
print(df.head())

print("\nã‚¯ãƒ©ã‚¹ã”ã¨ã®ã‚µãƒ³ãƒ—ãƒ«æ•°:")
print(df['species_name'].value_counts())

# çµ±è¨ˆé‡
print("\nç‰¹å¾´é‡ã®çµ±è¨ˆ:")
print(df.describe())

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

print(f"\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train)}å€‹")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)}å€‹")

# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆï¼ˆæ±ºå®šæœ¨ï¼‰
model = DecisionTreeClassifier(max_depth=3, random_state=42)

# è¨“ç·´
model.fit(X_train, y_train)

# äºˆæ¸¬
y_pred = model.predict(X_test)

# è©•ä¾¡
accuracy = accuracy_score(y_test, y_pred)
print(f"\næ­£è§£ç‡: {accuracy:.2%}")

# æ··åŒè¡Œåˆ—
cm = confusion_matrix(y_test, y_pred)
print("\næ··åŒè¡Œåˆ—:")
cm_df = pd.DataFrame(
    cm,
    index=['setosa', 'versicolor', 'virginica'],
    columns=['setosa', 'versicolor', 'virginica']
)
print(cm_df)

# ã‚¯ãƒ©ã‚¹ã”ã¨ã®æ­£è§£ç‡
print("\nã‚¯ãƒ©ã‚¹ã”ã¨ã®çµæœ:")
for i, name in enumerate(iris.target_names):
    class_mask = (y_test == i)
    class_accuracy = accuracy_score(y_test[class_mask], y_pred[class_mask])
    print(f"{name}: {class_accuracy:.2%}")

# ç‰¹å¾´é‡ã®é‡è¦åº¦
print("\nç‰¹å¾´é‡ã®é‡è¦åº¦:")
feature_importance = pd.DataFrame({
    'feature': iris.feature_names,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)
print(feature_importance)
</code></pre>

        <h2 id="unsupervised">9. æ•™å¸«ãªã—å­¦ç¿’ã®åŸºç¤</h2>

        <h3>9.1 ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°</h3>
        <p>ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¯ã€ä¼¼ãŸãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•çš„ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

        <h4>ä¾‹7ï¼šK-Meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°</h4>
        <pre><code class="language-python">from sklearn.cluster import KMeans
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
import numpy as np

# ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
iris = load_iris()
X = iris.data[:, :2]  # æœ€åˆã®2ç‰¹å¾´é‡ã®ã¿ä½¿ç”¨ï¼ˆå¯è¦–åŒ–ã®ãŸã‚ï¼‰

# K-Meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆ3ã¤ã®ã‚¯ãƒ©ã‚¹ã‚¿ï¼‰
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X)

print("ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ:")
print("å„ã‚µãƒ³ãƒ—ãƒ«ãŒå±ã™ã‚‹ã‚¯ãƒ©ã‚¹ã‚¿:", clusters)

# ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®ã‚µãƒ³ãƒ—ãƒ«æ•°
unique, counts = np.unique(clusters, return_counts=True)
print("\nã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®ã‚µãƒ³ãƒ—ãƒ«æ•°:")
for cluster, count in zip(unique, counts):
    print(f"ã‚¯ãƒ©ã‚¹ã‚¿ {cluster}: {count}å€‹")

# ã‚¯ãƒ©ã‚¹ã‚¿ä¸­å¿ƒ
print("\nã‚¯ãƒ©ã‚¹ã‚¿ä¸­å¿ƒ:")
print(kmeans.cluster_centers_)

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', alpha=0.6)
plt.scatter(kmeans.cluster_centers_[:, 0],
           kmeans.cluster_centers_[:, 1],
           s=300, c='red', marker='X', edgecolors='black',
           label='ä¸­å¿ƒ')
plt.xlabel(iris.feature_names[0])
plt.ylabel(iris.feature_names[1])
plt.title('K-Meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°')
plt.colorbar(scatter)
plt.legend()
plt.grid(True, alpha=0.3)
# plt.savefig('kmeans_clustering.png')
# plt.show()

print("\nã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Œäº†ã€‚")
</code></pre>

        <h3>9.2 æ¬¡å…ƒå‰Šæ¸›</h3>
        <p>æ¬¡å…ƒå‰Šæ¸›ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ã‚’æ¸›ã‚‰ã—ã¦å¯è¦–åŒ–ã‚„å‡¦ç†ã‚’ç°¡å˜ã«ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

        <h4>ä¾‹8ï¼šPCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰</h4>
        <pre><code class="language-python">from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

# ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
iris = load_iris()
X = iris.data  # 4æ¬¡å…ƒ
y = iris.target

print("å…ƒã®ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶:", X.shape)  # (150, 4)

# PCAã§2æ¬¡å…ƒã«å‰Šæ¸›
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

print("å‰Šæ¸›å¾Œã®å½¢çŠ¶:", X_pca.shape)  # (150, 2)

# å¯„ä¸ç‡
print("\nå„ä¸»æˆåˆ†ã®å¯„ä¸ç‡:")
print(pca.explained_variance_ratio_)
print(f"ç´¯ç©å¯„ä¸ç‡: {sum(pca.explained_variance_ratio_):.2%}")

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
colors = ['red', 'green', 'blue']
for i, color in enumerate(colors):
    mask = (y == i)
    plt.scatter(X_pca[mask, 0], X_pca[mask, 1],
               c=color, label=iris.target_names[i], alpha=0.6)

plt.xlabel(f'ç¬¬1ä¸»æˆåˆ† (å¯„ä¸ç‡: {pca.explained_variance_ratio_[0]:.2%})')
plt.ylabel(f'ç¬¬2ä¸»æˆåˆ† (å¯„ä¸ç‡: {pca.explained_variance_ratio_[1]:.2%})')
plt.title('PCAã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›')
plt.legend()
plt.grid(True, alpha=0.3)
# plt.savefig('pca_visualization.png')
# plt.show()

print("\nPCAå®Œäº†ã€‚")
</code></pre>

        <h2 id="workflow">10. æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æµã‚Œ</h2>

        <div class="mermaid">
graph TD
    A[å•é¡Œå®šç¾©] --> B[ãƒ‡ãƒ¼ã‚¿åé›†]
    B --> C[ãƒ‡ãƒ¼ã‚¿æ¢ç´¢ãƒ»å¯è¦–åŒ–]
    C --> D[ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†]
    D --> E[ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°]
    E --> F[ãƒ¢ãƒ‡ãƒ«é¸æŠ]
    F --> G[è¨“ç·´ãƒ»æ¤œè¨¼]
    G --> H{æ€§èƒ½OK?}
    H -->|No| I[ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´]
    I --> G
    H -->|Yes| J[ãƒ†ã‚¹ãƒˆ]
    J --> K[ãƒ‡ãƒ—ãƒ­ã‚¤]

    style A fill:#e3f2fd
    style D fill:#fff3e0
    style G fill:#f3e5f5
    style K fill:#e8f5e9
        </div>

        <h4>ä¾‹9ï¼šå®Œå…¨ãªæ©Ÿæ¢°å­¦ç¿’ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</h4>
        <pre><code class="language-python">from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd
import numpy as np

print("=== æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å®Œå…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ ===\n")

# 1. ãƒ‡ãƒ¼ã‚¿åé›†
print("1. ãƒ‡ãƒ¼ã‚¿åé›†")
iris = load_iris()
X, y = iris.data, iris.target
print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {X.shape}")

# 2. ãƒ‡ãƒ¼ã‚¿æ¢ç´¢
print("\n2. ãƒ‡ãƒ¼ã‚¿æ¢ç´¢")
df = pd.DataFrame(X, columns=iris.feature_names)
df['target'] = y
print(df.describe())
print("\nã‚¯ãƒ©ã‚¹åˆ†å¸ƒ:")
print(df['target'].value_counts())

# 3. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
print("\n3. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†")
# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# æ¨™æº–åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print("æ¨™æº–åŒ–å®Œäº†")

# 4. ãƒ¢ãƒ‡ãƒ«é¸æŠ
print("\n4. ãƒ¢ãƒ‡ãƒ«é¸æŠã¨è¨“ç·´")
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)
print("è¨“ç·´å®Œäº†")

# 5. äº¤å·®æ¤œè¨¼
print("\n5. äº¤å·®æ¤œè¨¼ï¼ˆ5-foldï¼‰")
cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
print(f"å„Foldã®ã‚¹ã‚³ã‚¢: {cv_scores}")
print(f"å¹³å‡ã‚¹ã‚³ã‚¢: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")

# 6. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
print("\n6. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡")
y_pred = model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {accuracy:.2%}")

print("\nè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_test, y_pred,
                          target_names=iris.target_names))

# 7. ç‰¹å¾´é‡ã®é‡è¦åº¦
print("7. ç‰¹å¾´é‡ã®é‡è¦åº¦")
feature_importance = pd.DataFrame({
    'feature': iris.feature_names,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)
print(feature_importance)

print("\n=== ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Œäº† ===")
</code></pre>

        <h4>ä¾‹10ï¼šéå­¦ç¿’ã¨æ±åŒ–æ€§èƒ½</h4>
        <pre><code class="language-python">from sklearn.model_selection import learning_curve
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
import numpy as np
import matplotlib.pyplot as plt

# ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
iris = load_iris()
X, y = iris.data, iris.target

# ç•°ãªã‚‹æ·±ã•ã®æ±ºå®šæœ¨ã§æ¯”è¼ƒ
depths = [1, 2, 3, 5, 10, 20]

results = []
for depth in depths:
    model = DecisionTreeClassifier(max_depth=depth, random_state=42)

    # å­¦ç¿’æ›²ç·šã®è¨ˆç®—
    train_sizes, train_scores, val_scores = learning_curve(
        model, X, y, cv=5, train_sizes=np.linspace(0.1, 1.0, 10)
    )

    train_mean = train_scores.mean(axis=1)[-1]
    val_mean = val_scores.mean(axis=1)[-1]

    results.append({
        'depth': depth,
        'train_score': train_mean,
        'val_score': val_mean,
        'overfitting': train_mean - val_mean
    })

# çµæœã‚’è¡¨ç¤º
print("æ·±ã•ã¨éå­¦ç¿’ã®é–¢ä¿‚:")
print("=" * 60)
for r in results:
    print(f"æ·±ã•{r['depth']:2d}: è¨“ç·´={r['train_score']:.3f}, "
          f"æ¤œè¨¼={r['val_score']:.3f}, "
          f"éå­¦ç¿’åº¦={r['overfitting']:.3f}")

# æœ€é©ãªæ·±ã•ã‚’è¦‹ã¤ã‘ã‚‹
best = max(results, key=lambda x: x['val_score'])
print(f"\næœ€é©ãªæ·±ã•: {best['depth']}")
print(f"æ¤œè¨¼ã‚¹ã‚³ã‚¢: {best['val_score']:.3f}")

# å¯è¦–åŒ–
depths_list = [r['depth'] for r in results]
train_scores = [r['train_score'] for r in results]
val_scores = [r['val_score'] for r in results]

plt.figure(figsize=(10, 6))
plt.plot(depths_list, train_scores, 'o-', label='è¨“ç·´ã‚¹ã‚³ã‚¢')
plt.plot(depths_list, val_scores, 's-', label='æ¤œè¨¼ã‚¹ã‚³ã‚¢')
plt.xlabel('æ±ºå®šæœ¨ã®æ·±ã•')
plt.ylabel('ã‚¹ã‚³ã‚¢')
plt.title('ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•ã¨æ€§èƒ½ã®é–¢ä¿‚')
plt.legend()
plt.grid(True, alpha=0.3)
# plt.savefig('overfitting_analysis.png')
# plt.show()

print("\néå­¦ç¿’åˆ†æå®Œäº†ã€‚")
</code></pre>

        <h2 id="summary">ã¾ã¨ã‚</h2>
        <p>ã“ã®ç« ã§ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã®åŸºç¤ã‚’å­¦ã³ã¾ã—ãŸï¼š</p>

        <ul>
            <li>âœ… <strong>æ©Ÿæ¢°å­¦ç¿’ã®å®šç¾©</strong>: ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—äºˆæ¸¬ã‚’è¡Œã†æŠ€è¡“</li>
            <li>âœ… <strong>ç¨®é¡</strong>: æ•™å¸«ã‚ã‚Šå­¦ç¿’ã€æ•™å¸«ãªã—å­¦ç¿’ã€å¼·åŒ–å­¦ç¿’</li>
            <li>âœ… <strong>æ•™å¸«ã‚ã‚Šå­¦ç¿’</strong>: åˆ†é¡ï¼ˆã‚«ãƒ†ã‚´ãƒªäºˆæ¸¬ï¼‰ã¨å›å¸°ï¼ˆæ•°å€¤äºˆæ¸¬ï¼‰</li>
            <li>âœ… <strong>ãƒ‡ãƒ¼ã‚¿åˆ†å‰²</strong>: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿</li>
            <li>âœ… <strong>scikit-learn</strong>: fit, predict, scoreã®åŸºæœ¬ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</li>
            <li>âœ… <strong>å®Ÿè£…</strong>: ç·šå½¢å›å¸°ã€ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã€æ±ºå®šæœ¨</li>
            <li>âœ… <strong>æ•™å¸«ãªã—å­¦ç¿’</strong>: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã€æ¬¡å…ƒå‰Šæ¸›</li>
            <li>âœ… <strong>è©•ä¾¡</strong>: æ­£è§£ç‡ã€æ··åŒè¡Œåˆ—ã€éå­¦ç¿’</li>
        </ul>

        <p><strong>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</strong>: ã“ã®ã‚·ãƒªãƒ¼ã‚ºã‚’å®Œäº†ã—ãŸã‚ãªãŸã¯ã€ã‚ˆã‚Šå°‚é–€çš„ãªæ©Ÿæ¢°å­¦ç¿’ã‚·ãƒªãƒ¼ã‚ºï¼ˆæ•™å¸«ã‚ã‚Šå­¦ç¿’å…¥é–€ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å…¥é–€ï¼‰ã¸é€²ã‚€æº–å‚™ãŒã§ãã¦ã„ã¾ã™ï¼</p>

        <h2 id="exercises">æ¼”ç¿’å•é¡Œ</h2>

        <details>
            <summary>æ¼”ç¿’1ï¼šãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã®ç†è§£</summary>
            <p><strong>å•é¡Œ</strong>: 100å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è¨“ç·´80%ã€ãƒ†ã‚¹ãƒˆ20%ã«åˆ†å‰²ã—ã€å„ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã•ã‚‰ã«ã€<code>stratify</code>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åŠ¹æœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚</p>
            <pre><code class="language-python"># è§£ç­”ä¾‹
from sklearn.model_selection import train_test_split
import numpy as np

# ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆï¼ˆä¸å‡è¡¡ãªã‚¯ãƒ©ã‚¹ï¼‰
X = np.arange(100).reshape(-1, 1)
y = np.array([0]*30 + [1]*70)  # ã‚¯ãƒ©ã‚¹0: 30å€‹ã€ã‚¯ãƒ©ã‚¹1: 70å€‹

print("å…ƒã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ:", np.bincount(y))

# stratifyãªã—
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
print("\nstratifyãªã—:")
print("è¨“ç·´:", np.bincount(y_train))
print("ãƒ†ã‚¹ãƒˆ:", np.bincount(y_test))

# stratifyã‚ã‚Š
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print("\nstratifyã‚ã‚Š:")
print("è¨“ç·´:", np.bincount(y_train))
print("ãƒ†ã‚¹ãƒˆ:", np.bincount(y_test))
</code></pre>
        </details>

        <details>
            <summary>æ¼”ç¿’2ï¼šå›å¸°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ</summary>
            <p><strong>å•é¡Œ</strong>: ç·šå½¢å›å¸°ã¨å¤šé …å¼å›å¸°ï¼ˆ2æ¬¡ï¼‰ã‚’æ¯”è¼ƒã—ã€ã©ã¡ã‚‰ãŒãƒ‡ãƒ¼ã‚¿ã«é©åˆã—ã¦ã„ã‚‹ã‹è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚</p>
            <pre><code class="language-python"># è§£ç­”ä¾‹
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# éç·šå½¢ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
np.random.seed(42)
X = np.linspace(0, 10, 100).reshape(-1, 1)
y = 0.5 * X**2 + X + 2 + np.random.normal(0, 5, (100, 1)).flatten()

# ç·šå½¢å›å¸°
model_linear = LinearRegression()
model_linear.fit(X, y)
y_pred_linear = model_linear.predict(X)

# å¤šé …å¼å›å¸°ï¼ˆ2æ¬¡ï¼‰
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
model_poly = LinearRegression()
model_poly.fit(X_poly, y)
y_pred_poly = model_poly.predict(X_poly)

# è©•ä¾¡
print("ç·šå½¢å›å¸°:")
print(f"RMSE: {np.sqrt(mean_squared_error(y, y_pred_linear)):.2f}")
print(f"RÂ²: {r2_score(y, y_pred_linear):.3f}")

print("\nå¤šé …å¼å›å¸°:")
print(f"RMSE: {np.sqrt(mean_squared_error(y, y_pred_poly)):.2f}")
print(f"RÂ²: {r2_score(y, y_pred_poly):.3f}")
</code></pre>
        </details>

        <details>
            <summary>æ¼”ç¿’3ï¼šåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡</summary>
            <p><strong>å•é¡Œ</strong>: Irisãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€k-NNï¼ˆk=3ï¼‰ã¨æ±ºå®šæœ¨ã‚’æ¯”è¼ƒã—ã€æ··åŒè¡Œåˆ—ã¨æ­£è§£ç‡ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚</p>
            <pre><code class="language-python"># è§£ç­”ä¾‹
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

# ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# k-NN
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)

print("k-NN (k=3):")
print(f"æ­£è§£ç‡: {accuracy_score(y_test, y_pred_knn):.2%}")
print("æ··åŒè¡Œåˆ—:")
print(confusion_matrix(y_test, y_pred_knn))

# æ±ºå®šæœ¨
tree = DecisionTreeClassifier(max_depth=3, random_state=42)
tree.fit(X_train, y_train)
y_pred_tree = tree.predict(X_test)

print("\næ±ºå®šæœ¨:")
print(f"æ­£è§£ç‡: {accuracy_score(y_test, y_pred_tree):.2%}")
print("æ··åŒè¡Œåˆ—:")
print(confusion_matrix(y_test, y_pred_tree))
</code></pre>
        </details>

        <details>
            <summary>æ¼”ç¿’4ï¼šã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®æœ€é©ãªã‚¯ãƒ©ã‚¹ã‚¿æ•°</summary>
            <p><strong>å•é¡Œ</strong>: K-Meansã§ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’2-10ã¾ã§å¤‰åŒ–ã•ã›ã€ã‚¨ãƒ«ãƒœãƒ¼æ³•ã§æœ€é©ãªã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’è¦‹ã¤ã‘ã¦ãã ã•ã„ã€‚</p>
            <pre><code class="language-python"># è§£ç­”ä¾‹
from sklearn.cluster import KMeans
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

# ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
iris = load_iris()
X = iris.data

# ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’å¤‰åŒ–ã•ã›ã¦æ…£æ€§ã‚’è¨ˆç®—
inertias = []
k_range = range(2, 11)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)

# ã‚¨ãƒ«ãƒœãƒ¼æ³•ã§å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.plot(k_range, inertias, 'bo-')
plt.xlabel('ã‚¯ãƒ©ã‚¹ã‚¿æ•° k')
plt.ylabel('æ…£æ€§ï¼ˆInertiaï¼‰')
plt.title('ã‚¨ãƒ«ãƒœãƒ¼æ³•ã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã®æ±ºå®š')
plt.grid(True, alpha=0.3)
# plt.savefig('elbow_method.png')
# plt.show()

print("æ…£æ€§ã®å€¤:")
for k, inertia in zip(k_range, inertias):
    print(f"k={k}: {inertia:.2f}")
</code></pre>
        </details>

        <details>
            <summary>æ¼”ç¿’5ï¼šç·åˆå•é¡Œ - å®Œå…¨ãªMLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</summary>
            <p><strong>å•é¡Œ</strong>: Irisãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ã£ã¦ã€(1) ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã€(2) æ¨™æº–åŒ–ã€(3) ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã€(4) äº¤å·®æ¤œè¨¼ã€(5) ãƒ†ã‚¹ãƒˆè©•ä¾¡ã®å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>
            <pre><code class="language-python"># è§£ç­”ä¾‹
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# (1) ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("(1) ãƒ‡ãƒ¼ã‚¿åˆ†å‰²:")
print(f"è¨“ç·´: {X_train.shape}, ãƒ†ã‚¹ãƒˆ: {X_test.shape}")

# (2) æ¨™æº–åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\n(2) æ¨™æº–åŒ–:")
print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿å¹³å‡: {X_train_scaled.mean(axis=0)}")
print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ¨™æº–åå·®: {X_train_scaled.std(axis=0)}")

# (3) ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = SVC(kernel='rbf', random_state=42)
model.fit(X_train_scaled, y_train)

print("\n(3) ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")

# (4) äº¤å·®æ¤œè¨¼
cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
print("\n(4) äº¤å·®æ¤œè¨¼:")
print(f"å„Foldã®ã‚¹ã‚³ã‚¢: {cv_scores}")
print(f"å¹³å‡: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")

# (5) ãƒ†ã‚¹ãƒˆè©•ä¾¡
y_pred = model.predict(X_test_scaled)

print("\n(5) ãƒ†ã‚¹ãƒˆè©•ä¾¡:")
print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {model.score(X_test_scaled, y_test):.2%}")
print("\næ··åŒè¡Œåˆ—:")
print(confusion_matrix(y_test, y_pred))
print("\nè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_test, y_pred,
                          target_names=iris.target_names))
</code></pre>
        </details>

        <div class="nav-buttons">
            <a href="./chapter3-pandas-basics.html" class="nav-button">â† ç¬¬3ç« : PandasåŸºç¤</a>
            <a href="./index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºãƒˆãƒƒãƒ—ã¸</a>
        </div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <div class="container">
            <p>&copy; 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
