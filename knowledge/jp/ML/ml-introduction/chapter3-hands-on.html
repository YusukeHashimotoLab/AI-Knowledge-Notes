<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
<meta content="ç¬¬3ç« ï¼šPythonã§ä½“é¨“ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’ - å®Ÿè·µçš„ãªäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ - AI Terakoya. Pythonç’°å¢ƒã‚’3ã¤ã®æ–¹æ³•ï¼ˆAnaconda/venv/Colabï¼‰ã§æ§‹ç¯‰ã§ãã‚‹" name="description"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬3ç« ï¼šPythonã§ä½“é¨“ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’ - å®Ÿè·µçš„ãªäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #667eea;
            --color-accent-light: #764ba2;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(102, 126, 234, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        .info-box {
            background-color: #e3f2fd;
            border-left: 4px solid #2196f3;
            padding: var(--spacing-md);
            margin: var(--spacing-md) 0;
            border-radius: var(--border-radius);
        }

        .warning-box {
            background-color: #fff3e0;
            border-left: 4px solid #ff9800;
            padding: var(--spacing-md);
            margin: var(--spacing-md) 0;
            border-radius: var(--border-radius);
        }

        .success-box {
            background-color: #e8f5e9;
            border-left: 4px solid #4caf50;
            padding: var(--spacing-md);
            margin: var(--spacing-md) 0;
            border-radius: var(--border-radius);
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/ml-introduction/index.html">Ml</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬3ç« ï¼šPythonã§ä½“é¨“ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’</h1>
            <p class="subtitle">å®Ÿè·µçš„ãªäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ - ç’°å¢ƒæ§‹ç¯‰ã‹ã‚‰å®Ÿãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¾ã§</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 30-40åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 35å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">
        <p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #667eea; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">
            ã“ã®ç« ã§ã¯ã€Pythonã‚’ä½¿ã£ã¦å®Ÿéš›ã«æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚ç’°å¢ƒæ§‹ç¯‰ã‹ã‚‰å§‹ã‚ã¦ã€å›å¸°ãƒ»åˆ†é¡å•é¡Œã®å®Ÿè£…ã€ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€ãã—ã¦å®Ÿãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¾ã§ã€35å€‹ã®å®Ÿè¡Œå¯èƒ½ãªã‚³ãƒ¼ãƒ‰ä¾‹ã§å­¦ã³ã¾ã™ã€‚
        </p>

        <div class="learning-objectives">
            <h2>å­¦ç¿’ç›®æ¨™</h2>
            <ul>
                <li>âœ… Pythonç’°å¢ƒã‚’3ã¤ã®æ–¹æ³•ï¼ˆAnaconda/venv/Colabï¼‰ã§æ§‹ç¯‰ã§ãã‚‹</li>
                <li>âœ… ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã€å‰å‡¦ç†ã€å¯è¦–åŒ–ã®åŸºæœ¬æ“ä½œãŒã§ãã‚‹</li>
                <li>âœ… å›å¸°å•é¡Œï¼ˆä½å®…ä¾¡æ ¼äºˆæ¸¬ï¼‰ã‚’å®Ÿè£…ã—è©•ä¾¡ã§ãã‚‹</li>
                <li>âœ… åˆ†é¡å•é¡Œï¼ˆIrisåˆ†é¡ï¼‰ã§è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã§ãã‚‹</li>
                <li>âœ… ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã§ãã‚‹</li>
                <li>âœ… ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®åŸºæœ¬æŠ€è¡“ã‚’é©ç”¨ã§ãã‚‹</li>
                <li>âœ… Titanicãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å®Ÿãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å®Œæˆã§ãã‚‹</li>
            </ul>
        </div>

        <h2>3.1 ç’°å¢ƒæ§‹ç¯‰ï¼š3ã¤ã®é¸æŠè‚¢</h2>
        <p>æ©Ÿæ¢°å­¦ç¿’ã‚’å®Ÿè·µã™ã‚‹ã«ã¯ã€ã¾ãšPythonç’°å¢ƒã‚’æ§‹ç¯‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚çŠ¶æ³ã«å¿œã˜ã¦3ã¤ã®é¸æŠè‚¢ã‹ã‚‰é¸ã¹ã¾ã™ã€‚</p>

        <h3>3.1.1 Option 1: Anacondaï¼ˆåˆå¿ƒè€…æ¨å¥¨ï¼‰</h3>
        <p><strong>ç‰¹å¾´ï¼š</strong></p>
        <ul>
            <li>ç§‘å­¦è¨ˆç®—ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒæœ€åˆã‹ã‚‰æƒã£ã¦ã„ã‚‹</li>
            <li>ç’°å¢ƒç®¡ç†ãŒç°¡å˜ï¼ˆGUIåˆ©ç”¨å¯èƒ½ï¼‰</li>
            <li>Windows/Mac/Linuxå¯¾å¿œ</li>
        </ul>

        <p><strong>ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †ï¼š</strong></p>
        <pre><code class="language-bash"># ã‚³ãƒ¼ãƒ‰ä¾‹1: Anacondaç’°å¢ƒæ§‹ç¯‰

# 1. Anacondaã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# å…¬å¼ã‚µã‚¤ãƒˆ: https://www.anaconda.com/download
# Python 3.11ä»¥ä¸Šã‚’é¸æŠ

# 2. ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œã€Anaconda Promptã‚’èµ·å‹•

# 3. ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆï¼ˆMLå°‚ç”¨ç’°å¢ƒï¼‰
conda create -n ml_env python=3.11

# 4. ç’°å¢ƒã‚’æœ‰åŠ¹åŒ–
conda activate ml_env

# 5. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
conda install numpy pandas matplotlib seaborn scikit-learn jupyter

# 6. å‹•ä½œç¢ºèª
python --version
# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›: Python 3.11.x

# 7. Jupyter Notebookã‚’èµ·å‹•
jupyter notebook</code></pre>

        <div class="success-box">
            <strong>æˆåŠŸã®ç¢ºèªï¼š</strong> ã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ <code>(ml_env)</code> ã¨ã„ã†ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãŒè¡¨ç¤ºã•ã‚Œã‚Œã°ã€ç’°å¢ƒãŒæ­£ã—ãã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚
        </div>

        <p><strong>Anacondaã®åˆ©ç‚¹ã¨æ¬ ç‚¹ï¼š</strong></p>
        <table>
            <thead>
                <tr>
                    <th>åˆ©ç‚¹</th>
                    <th>æ¬ ç‚¹</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>NumPyã€SciPyãªã©ãŒæœ€åˆã‹ã‚‰å«ã¾ã‚Œã‚‹</td>
                    <td>ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã„ï¼ˆ3GBä»¥ä¸Šï¼‰</td>
                </tr>
                <tr>
                    <td>ä¾å­˜é–¢ä¿‚ã®å•é¡ŒãŒå°‘ãªã„</td>
                    <td>ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«æ™‚é–“ãŒã‹ã‹ã‚‹</td>
                </tr>
                <tr>
                    <td>Anaconda Navigatorã§è¦–è¦šçš„ã«ç®¡ç†å¯èƒ½</td>
                    <td>ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã‚’æ¶ˆè²»</td>
                </tr>
            </tbody>
        </table>

        <h3>3.1.2 Option 2: venvï¼ˆPythonæ¨™æº–ï¼‰</h3>
        <p><strong>ç‰¹å¾´ï¼š</strong></p>
        <ul>
            <li>Pythonæ¨™æº–ãƒ„ãƒ¼ãƒ«ï¼ˆè¿½åŠ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸è¦ï¼‰</li>
            <li>è»½é‡ï¼ˆå¿…è¦ãªã‚‚ã®ã ã‘ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰</li>
            <li>ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã”ã¨ã«ç’°å¢ƒã‚’åˆ†é›¢</li>
        </ul>

        <pre><code class="language-bash"># ã‚³ãƒ¼ãƒ‰ä¾‹2: venvç’°å¢ƒæ§‹ç¯‰

# 1. Python 3.11ä»¥ä¸ŠãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
python3 --version
# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›: Python 3.11.x ä»¥ä¸Š

# 2. ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆ
python3 -m venv ml_env

# 3. ç’°å¢ƒã‚’æœ‰åŠ¹åŒ–
# macOS/Linux:
source ml_env/bin/activate

# Windows (PowerShell):
# ml_env\Scripts\Activate.ps1

# Windows (Command Prompt):
# ml_env\Scripts\activate.bat

# 4. pipã‚’ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
pip install --upgrade pip

# 5. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install numpy pandas matplotlib seaborn scikit-learn jupyter

# 6. ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
pip list
# numpy, pandas, scikit-learnãªã©ãŒè¡¨ç¤ºã•ã‚Œã‚Œã°OK</code></pre>

        <div class="info-box">
            <strong>Tips:</strong> <code>requirements.txt</code>ã‚’ä½œæˆã—ã¦ãŠãã¨ã€ç’°å¢ƒã®å†ç¾ãŒç°¡å˜ã«ãªã‚Šã¾ã™ã€‚
            <pre><code># requirements.txt
numpy>=1.24.0
pandas>=2.0.0
matplotlib>=3.7.0
seaborn>=0.12.0
scikit-learn>=1.3.0
jupyter>=1.0.0</code></pre>
            ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: <code>pip install -r requirements.txt</code>
        </div>

        <h3>3.1.3 Option 3: Google Colabï¼ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸è¦ï¼‰</h3>
        <p><strong>ç‰¹å¾´ï¼š</strong></p>
        <ul>
            <li>ãƒ–ãƒ©ã‚¦ã‚¶ã ã‘ã§å®Ÿè¡Œå¯èƒ½</li>
            <li>ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸è¦ï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰å®Ÿè¡Œï¼‰</li>
            <li>GPU/TPUãŒç„¡æ–™ã§ä½¿ãˆã‚‹</li>
        </ul>

        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹3: Google Colabã§ã®å‹•ä½œç¢ºèª

# 1. Google Colabã«ã‚¢ã‚¯ã‚»ã‚¹: https://colab.research.google.com
# 2. æ–°ã—ã„ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½œæˆ
# 3. ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œï¼ˆå¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯è‡ªå‹•ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ï¼‰

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

print("ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼")
print(f"NumPy version: {np.__version__}")
print(f"Pandas version: {pd.__version__}")
print(f"scikit-learn version: {sklearn.__version__}")

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼
# NumPy version: 1.24.3
# Pandas version: 2.0.3
# scikit-learn version: 1.3.0</code></pre>

        <h3>3.1.4 ç’°å¢ƒé¸æŠã‚¬ã‚¤ãƒ‰</h3>
        <table>
            <thead>
                <tr>
                    <th>çŠ¶æ³</th>
                    <th>æ¨å¥¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³</th>
                    <th>ç†ç”±</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>åˆã‚ã¦ã®Pythonç’°å¢ƒ</td>
                    <td>Anaconda</td>
                    <td>ç’°å¢ƒæ§‹ç¯‰ãŒç°¡å˜ã€ãƒˆãƒ©ãƒ–ãƒ«ãŒå°‘ãªã„</td>
                </tr>
                <tr>
                    <td>æ—¢ã«Pythonç’°å¢ƒãŒã‚ã‚‹</td>
                    <td>venv</td>
                    <td>è»½é‡ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã”ã¨ã«ç‹¬ç«‹</td>
                </tr>
                <tr>
                    <td>ä»Šã™ãè©¦ã—ãŸã„</td>
                    <td>Google Colab</td>
                    <td>ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸è¦ã€å³åº§ã«é–‹å§‹å¯èƒ½</td>
                </tr>
                <tr>
                    <td>GPUè¨ˆç®—ãŒå¿…è¦</td>
                    <td>Google Colab</td>
                    <td>ç„¡æ–™ã§GPUã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½</td>
                </tr>
            </tbody>
        </table>

        <h2>3.2 ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã¨å¯è¦–åŒ–</h2>
        <p>æ©Ÿæ¢°å­¦ç¿’ã®æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’æ­£ã—ãèª­ã¿è¾¼ã¿ã€ç†è§£ã™ã‚‹ã“ã¨ã§ã™ã€‚</p>

        <h3>3.2.1 ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨åŸºæœ¬æ“ä½œ</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹4: Irisãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿

import pandas as pd
import numpy as np
from sklearn.datasets import load_iris

# Irisãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆã‚¢ãƒ¤ãƒ¡ã®å“ç¨®åˆ†é¡ï¼‰ã‚’èª­ã¿è¾¼ã¿
iris = load_iris()

# DataFrameã«å¤‰æ›
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['target'] = iris.target

# ãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­5è¡Œã‚’è¡¨ç¤º
print("ãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­5è¡Œ:")
print(df.head())

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
#    sepal length (cm)  sepal width (cm)  ...  petal width (cm)  target
# 0                5.1               3.5  ...               0.2       0
# 1                4.9               3.0  ...               0.2       0
# 2                4.7               3.2  ...               0.2       0
# 3                4.6               3.1  ...               0.2       0
# 4                5.0               3.6  ...               0.2       0

# ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶ã‚’ç¢ºèª
print(f"\nãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶: {df.shape}")
# å‡ºåŠ›: ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶: (150, 5)
# 150ã‚µãƒ³ãƒ—ãƒ«ã€5åˆ—ï¼ˆç‰¹å¾´é‡4ã¤ + ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ1ã¤ï¼‰</code></pre>

        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹5: åŸºæœ¬çµ±è¨ˆé‡ã®ç¢ºèª

# åŸºæœ¬çµ±è¨ˆé‡ã‚’è¡¨ç¤º
print("åŸºæœ¬çµ±è¨ˆé‡:")
print(df.describe())

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
#        sepal length (cm)  sepal width (cm)  ...  petal width (cm)     target
# count         150.000000        150.000000  ...        150.000000  150.000000
# mean            5.843333          3.057333  ...          1.199333    1.000000
# std             0.828066          0.435866  ...          0.762238    0.819232
# min             4.300000          2.000000  ...          0.100000    0.000000
# 25%             5.100000          2.800000  ...          0.300000    0.000000
# 50%             5.800000          3.000000  ...          1.300000    1.000000
# 75%             6.400000          3.300000  ...          1.800000    2.000000
# max             7.900000          4.400000  ...          2.500000    2.000000

# ãƒ‡ãƒ¼ã‚¿å‹ã‚’ç¢ºèª
print("\nãƒ‡ãƒ¼ã‚¿å‹:")
print(df.dtypes)

# æ¬ æå€¤ã‚’ç¢ºèª
print("\næ¬ æå€¤ã®æ•°:")
print(df.isnull().sum())
# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›: ã™ã¹ã¦0ï¼ˆIrisãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯æ¬ æå€¤ãŒãªã„ï¼‰</code></pre>

        <h3>3.2.2 ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹6: ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼ˆåˆ†å¸ƒã®å¯è¦–åŒ–ï¼‰

import matplotlib.pyplot as plt

# å„ç‰¹å¾´é‡ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’ä½œæˆ
df.hist(figsize=(12, 8), bins=20, edgecolor='black')
plt.suptitle('Iris Dataset - Feature Distributions', fontsize=16)
plt.tight_layout()
plt.show()

# è§£é‡ˆ:
# - sepal length: 5-6cmã‚ãŸã‚Šã«ãƒ”ãƒ¼ã‚¯
# - petal length: äºŒå³°æ€§ï¼ˆå“ç¨®ã«ã‚ˆã‚‹é•ã„ãŒå¤§ãã„ï¼‰</code></pre>

        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹7: æ•£å¸ƒå›³ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼ˆç‰¹å¾´é‡é–“ã®é–¢ä¿‚ï¼‰

import seaborn as sns

# æ•£å¸ƒå›³ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼ˆpairplotï¼‰
sns.pairplot(df, hue='target', palette='Set1', markers=['o', 's', 'D'])
plt.suptitle('Iris Dataset - Pairplot by Species', y=1.02)
plt.show()

# è§£é‡ˆ:
# - petal length vs petal width: å“ç¨®ãŒãã‚Œã„ã«åˆ†é›¢
# - sepal length vs sepal width: ä¸€éƒ¨é‡ãªã‚Šã‚ã‚Š
# â‡’ petalç³»ã®ç‰¹å¾´é‡ãŒåˆ†é¡ã«æœ‰åŠ¹</code></pre>

        <h2>3.3 å›å¸°å•é¡Œï¼šä½å®…ä¾¡æ ¼äºˆæ¸¬</h2>
        <p>å›å¸°å•é¡Œã§ã¯ã€é€£ç¶šå€¤ï¼ˆä¾¡æ ¼ã€æ¸©åº¦ãªã©ï¼‰ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚ã‚«ãƒªãƒ•ã‚©ãƒ«ãƒ‹ã‚¢ä½å®…ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã§ç·šå½¢å›å¸°ã‚’å®Ÿè£…ã—ã¾ã™ã€‚</p>

        <h3>3.3.1 ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹8: ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆã‚«ãƒªãƒ•ã‚©ãƒ«ãƒ‹ã‚¢ä½å®…ä¾¡æ ¼ï¼‰

from sklearn.datasets import fetch_california_housing

# ã‚«ãƒªãƒ•ã‚©ãƒ«ãƒ‹ã‚¢ä½å®…ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
housing = fetch_california_housing()
X = housing.data  # ç‰¹å¾´é‡ï¼ˆ8æ¬¡å…ƒï¼‰
y = housing.target  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆä½å®…ä¾¡æ ¼ã€å˜ä½: $100,000ï¼‰

# ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
print("ç‰¹å¾´é‡ã®å½¢çŠ¶:", X.shape)  # (20640, 8)
print("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®å½¢çŠ¶:", y.shape)  # (20640,)
print("\nç‰¹å¾´é‡å:")
print(housing.feature_names)
# ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms',
#  'Population', 'AveOccup', 'Latitude', 'Longitude']

print("\næœ€åˆã®3ã‚µãƒ³ãƒ—ãƒ«:")
print(X[:3])
print("å¯¾å¿œã™ã‚‹ä¾¡æ ¼:", y[:3])
# ä¾¡æ ¼: [4.526 3.585 3.521] ï¼ˆå˜ä½: $100,000ï¼‰</code></pre>

        <h3>3.3.2 ãƒ‡ãƒ¼ã‚¿åˆ†å‰²</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹9: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²

from sklearn.model_selection import train_test_split

# ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´80%ã€ãƒ†ã‚¹ãƒˆ20%ã«åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("è¨“ç·´ãƒ‡ãƒ¼ã‚¿:", X_train.shape)  # (16512, 8)
print("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿:", X_test.shape)  # (4128, 8)

# random_state=42: å†ç¾æ€§ã®ãŸã‚å›ºå®š
# test_size=0.2: ä¸€èˆ¬çš„ãªåˆ†å‰²æ¯”ç‡</code></pre>

        <h3>3.3.3 ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹10: ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆæ¨™æº–åŒ–ï¼‰

from sklearn.preprocessing import StandardScaler

# StandardScaler: å¹³å‡0ã€æ¨™æº–åå·®1ã«å¤‰æ›
scaler = StandardScaler()

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§fitï¼ˆå¹³å‡ãƒ»æ¨™æº–åå·®ã‚’è¨ˆç®—ï¼‰
X_train_scaled = scaler.fit_transform(X_train)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆé‡ã§transform
X_test_scaled = scaler.transform(X_test)

print("ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰ï¼ˆæœ€åˆã®1ã‚µãƒ³ãƒ—ãƒ«ï¼‰:")
print(X_train[0])
print("\nã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œ:")
print(X_train_scaled[0])

# é‡è¦: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§fitã—ãªã„ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’é˜²ãï¼‰</code></pre>

        <h3>3.3.4 ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹11: ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´

from sklearn.linear_model import LinearRegression

# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
model = LinearRegression()

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
model.fit(X_train_scaled, y_train)

# å­¦ç¿’ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¢ºèª
print("åˆ‡ç‰‡ï¼ˆbiasï¼‰:", model.intercept_)
print("\nä¿‚æ•°ï¼ˆweightsï¼‰:")
for feature, coef in zip(housing.feature_names, model.coef_):
    print(f"  {feature:12s}: {coef:7.4f}")

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ä¾‹:
# MedInc      :  0.8296  ï¼ˆæ‰€å¾—ãŒé«˜ã„ã»ã©ä¾¡æ ¼ãŒä¸ŠãŒã‚‹ï¼‰
# Latitude    : -0.8231  ï¼ˆç·¯åº¦ãŒé«˜ã„ã»ã©ä¾¡æ ¼ãŒä¸‹ãŒã‚‹ï¼‰</code></pre>

        <h3>3.3.5 äºˆæ¸¬ã¨è©•ä¾¡</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹12: äºˆæ¸¬ã®å®Ÿè¡Œ

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
y_pred = model.predict(X_test_scaled)

# æœ€åˆã®5ä»¶ã®äºˆæ¸¬ã‚’ç¢ºèª
print("å®Ÿéš›ã®ä¾¡æ ¼ vs äºˆæ¸¬ä¾¡æ ¼ï¼ˆæœ€åˆã®5ä»¶ï¼‰:")
for i in range(5):
    print(f"å®Ÿéš›: {y_test[i]:.3f}, äºˆæ¸¬: {y_pred[i]:.3f}")

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ä¾‹:
# å®Ÿéš›: 4.526, äºˆæ¸¬: 4.321
# å®Ÿéš›: 3.585, äºˆæ¸¬: 3.712
# å®Ÿéš›: 3.521, äºˆæ¸¬: 3.498
# å®Ÿéš›: 3.413, äºˆæ¸¬: 3.289
# å®Ÿéš›: 3.422, äºˆæ¸¬: 3.501</code></pre>

        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹13: ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# å¹³å‡äºŒä¹—èª¤å·®ï¼ˆMSEï¼‰
mse = mean_squared_error(y_test, y_pred)

# å¹³å‡äºŒä¹—å¹³æ–¹æ ¹èª¤å·®ï¼ˆRMSEï¼‰
rmse = np.sqrt(mse)

# å¹³å‡çµ¶å¯¾èª¤å·®ï¼ˆMAEï¼‰
mae = mean_absolute_error(y_test, y_pred)

# æ±ºå®šä¿‚æ•°ï¼ˆRÂ²ï¼‰
r2 = r2_score(y_test, y_pred)

print("ãƒ¢ãƒ‡ãƒ«è©•ä¾¡æŒ‡æ¨™:")
print(f"  RMSE: {rmse:.3f}")  # å°ã•ã„ã»ã©è‰¯ã„
print(f"  MAE:  {mae:.3f}")   # å°ã•ã„ã»ã©è‰¯ã„
print(f"  RÂ²:   {r2:.3f}")    # 1ã«è¿‘ã„ã»ã©è‰¯ã„ï¼ˆæœ€å¤§1.0ï¼‰

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
# RMSE: 0.729ï¼ˆç´„$72,900ã®èª¤å·®ï¼‰
# MAE:  0.526ï¼ˆç´„$52,600ã®èª¤å·®ï¼‰
# RÂ²:   0.576ï¼ˆ57.6%ã®åˆ†æ•£ã‚’èª¬æ˜ï¼‰</code></pre>

        <h3>3.3.6 äºˆæ¸¬çµæœã®å¯è¦–åŒ–</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹14: äºˆæ¸¬vså®Ÿæ¸¬ã®ãƒ—ãƒ­ãƒƒãƒˆ

plt.figure(figsize=(10, 6))

# æ•£å¸ƒå›³
plt.scatter(y_test, y_pred, alpha=0.5, edgecolor='black')

# ç†æƒ³çš„ãªäºˆæ¸¬ç·šï¼ˆy=xï¼‰
plt.plot([y_test.min(), y_test.max()],
         [y_test.min(), y_test.max()],
         'r--', lw=2, label='Perfect Prediction')

plt.xlabel('Actual Price ($100,000)', fontsize=12)
plt.ylabel('Predicted Price ($100,000)', fontsize=12)
plt.title('Linear Regression: Predictions vs Actual', fontsize=14)
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# è§£é‡ˆ:
# - ç‚¹ãŒèµ¤ç·šã«è¿‘ã„ã»ã©äºˆæ¸¬ç²¾åº¦ãŒé«˜ã„
# - ä½ä¾¡æ ¼å¸¯ï¼ˆ0-2ï¼‰: äºˆæ¸¬ç²¾åº¦ãŒé«˜ã„
# - é«˜ä¾¡æ ¼å¸¯ï¼ˆ4ä»¥ä¸Šï¼‰: äºˆæ¸¬ãŒéå°è©•ä¾¡å‚¾å‘</code></pre>

        <h2>3.4 åˆ†é¡å•é¡Œï¼šIriså“ç¨®åˆ†é¡</h2>
        <p>åˆ†é¡å•é¡Œã§ã¯ã€ã‚«ãƒ†ã‚´ãƒªï¼ˆå“ç¨®ã€è‰¯/ä¸è‰¯ãªã©ï¼‰ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚</p>

        <h3>3.4.1 ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹15: ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã«ã‚ˆã‚‹åˆ†é¡

from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.2, random_state=42
)

# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«
lr_model = LogisticRegression(max_iter=200, random_state=42)
lr_model.fit(X_train, y_train)

# äºˆæ¸¬
lr_pred = lr_model.predict(X_test)

# ç¢ºç‡ã‚‚å–å¾—å¯èƒ½
lr_proba = lr_model.predict_proba(X_test)

print("æœ€åˆã®3ã‚µãƒ³ãƒ—ãƒ«ã®äºˆæ¸¬:")
for i in range(3):
    print(f"å®Ÿéš›: {y_test[i]}, äºˆæ¸¬: {lr_pred[i]}, ç¢ºç‡: {lr_proba[i]}")

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ä¾‹:
# å®Ÿéš›: 1, äºˆæ¸¬: 1, ç¢ºç‡: [0.00 0.79 0.21]
# å®Ÿéš›: 0, äºˆæ¸¬: 0, ç¢ºç‡: [0.97 0.03 0.00]
# å®Ÿéš›: 2, äºˆæ¸¬: 2, ç¢ºç‡: [0.00 0.01 0.99]</code></pre>

        <h3>3.4.2 ç²¾åº¦è©•ä¾¡</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹16: åˆ†é¡ç²¾åº¦ã®è©•ä¾¡

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ç²¾åº¦ï¼ˆAccuracyï¼‰
accuracy = accuracy_score(y_test, lr_pred)
print(f"Accuracy: {accuracy:.3f}")  # æœŸå¾…: 1.000ï¼ˆ100%ï¼‰

# è©³ç´°ãªåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_test, lr_pred, target_names=iris.target_names))

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
#               precision    recall  f1-score   support
#     setosa       1.00      1.00      1.00        10
# versicolor       1.00      1.00      1.00         9
#  virginica       1.00      1.00      1.00        11
#   accuracy                           1.00        30

# precision: æ­£è§£ã¨äºˆæ¸¬ã—ãŸä¸­ã§å®Ÿéš›ã«æ­£è§£ã ã£ãŸå‰²åˆ
# recall: å®Ÿéš›ã®æ­£è§£ã®ä¸­ã§æ­£ã—ãäºˆæ¸¬ã§ããŸå‰²åˆ
# f1-score: precisionã¨recallã®èª¿å’Œå¹³å‡</code></pre>

        <h3>3.4.3 æ··åŒè¡Œåˆ—ã®å¯è¦–åŒ–</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹17: æ··åŒè¡Œåˆ—ï¼ˆConfusion Matrixï¼‰

import seaborn as sns

# æ··åŒè¡Œåˆ—ã‚’è¨ˆç®—
cm = confusion_matrix(y_test, lr_pred)

# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=iris.target_names,
            yticklabels=iris.target_names)
plt.title('Confusion Matrix - Logistic Regression', fontsize=14)
plt.ylabel('True Label', fontsize=12)
plt.xlabel('Predicted Label', fontsize=12)
plt.tight_layout()
plt.show()

# è§£é‡ˆ:
# - å¯¾è§’ç·š: æ­£ã—ãåˆ†é¡ã•ã‚ŒãŸæ•°
# - å¯¾è§’ç·šä»¥å¤–: èª¤åˆ†é¡
# - Irisãƒ‡ãƒ¼ã‚¿ã¯å˜ç´”ãªã®ã§èª¤åˆ†é¡ãŒã»ã¼ã‚¼ãƒ­</code></pre>

        <h3>3.4.4 æ±ºå®šæœ¨</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹18: æ±ºå®šæœ¨ã«ã‚ˆã‚‹åˆ†é¡

from sklearn.tree import DecisionTreeClassifier

# æ±ºå®šæœ¨ãƒ¢ãƒ‡ãƒ«ï¼ˆæ·±ã•3ã«åˆ¶é™ï¼‰
dt_model = DecisionTreeClassifier(max_depth=3, random_state=42)
dt_model.fit(X_train, y_train)

# äºˆæ¸¬
dt_pred = dt_model.predict(X_test)

# ç²¾åº¦
dt_accuracy = accuracy_score(y_test, dt_pred)
print(f"Decision Tree Accuracy: {dt_accuracy:.3f}")
# æœŸå¾…: 1.000

# æ±ºå®šæœ¨ã®åˆ©ç‚¹: è§£é‡ˆæ€§ãŒé«˜ã„
# ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’ç¢ºèª
print("\nç‰¹å¾´é‡ã®é‡è¦åº¦:")
for feature, importance in zip(iris.feature_names, dt_model.feature_importances_):
    print(f"  {feature:20s}: {importance:.3f}")</code></pre>

        <h3>3.4.5 ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹19: ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ï¼‰

from sklearn.ensemble import RandomForestClassifier

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼ˆ100æœ¬ã®æ±ºå®šæœ¨ï¼‰
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# äºˆæ¸¬
rf_pred = rf_model.predict(X_test)

# ç²¾åº¦
rf_accuracy = accuracy_score(y_test, rf_pred)
print(f"Random Forest Accuracy: {rf_accuracy:.3f}")
# æœŸå¾…: 1.000

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®åˆ©ç‚¹:
# - æ±ºå®šæœ¨ã‚ˆã‚Šéå­¦ç¿’ã—ã«ãã„
# - é«˜ã„äºˆæ¸¬ç²¾åº¦
# - ç‰¹å¾´é‡ã®é‡è¦åº¦ãŒå®‰å®š</code></pre>

        <h3>3.4.6 ç‰¹å¾´é‡ã®é‡è¦åº¦å¯è¦–åŒ–</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹20: ç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆFeature Importanceï¼‰

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ç‰¹å¾´é‡é‡è¦åº¦
importances = rf_model.feature_importances_
feature_names = iris.feature_names

# é™é †ã«ã‚½ãƒ¼ãƒˆ
indices = np.argsort(importances)[::-1]

# æ£’ã‚°ãƒ©ãƒ•ã§å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.barh(range(len(importances)), importances[indices], color='skyblue', edgecolor='black')
plt.yticks(range(len(importances)), [feature_names[i] for i in indices])
plt.xlabel('Importance', fontsize=12)
plt.title('Feature Importance (Random Forest)', fontsize=14)
plt.tight_layout()
plt.show()

# è§£é‡ˆ:
# - petal width (cm): æœ€ã‚‚é‡è¦ï¼ˆ0.45ï¼‰
# - petal length (cm): 2ç•ªç›®ã«é‡è¦ï¼ˆ0.42ï¼‰
# â‡’ èŠ±ã³ã‚‰ã®ç‰¹å¾´ãŒå“ç¨®åˆ†é¡ã«æœ€ã‚‚æœ‰åŠ¹</code></pre>

        <h3>3.4.7 ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³ï¼ˆSVMï¼‰</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹21: SVMï¼ˆSupport Vector Machineï¼‰

from sklearn.svm import SVC

# SVMãƒ¢ãƒ‡ãƒ«ï¼ˆRBFã‚«ãƒ¼ãƒãƒ«ï¼‰
svm_model = SVC(kernel='rbf', random_state=42)
svm_model.fit(X_train, y_train)

# äºˆæ¸¬
svm_pred = svm_model.predict(X_test)

# ç²¾åº¦
svm_accuracy = accuracy_score(y_test, svm_pred)
print(f"SVM Accuracy: {svm_accuracy:.3f}")
# æœŸå¾…: 1.000

# SVMã®åˆ©ç‚¹:
# - é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã«å¼·ã„
# - ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã§éç·šå½¢åˆ†é›¢å¯èƒ½
# æ¬ ç‚¹:
# - å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã¯é…ã„
# - ç¢ºç‡äºˆæ¸¬ãŒæ¨™æº–ã§ãªã„</code></pre>

        <h2>3.5 ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã¨é¸æŠ</h2>
        <p>è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’å…¬å¹³ã«æ¯”è¼ƒã—ã€æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¾ã™ã€‚</p>

        <h3>3.5.1 ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹22: ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ

from sklearn.model_selection import cross_val_score

# æ¯”è¼ƒã™ã‚‹ãƒ¢ãƒ‡ãƒ«
models = {
    'Logistic Regression': LogisticRegression(max_iter=200),
    'Decision Tree': DecisionTreeClassifier(max_depth=3),
    'Random Forest': RandomForestClassifier(n_estimators=100),
    'SVM': SVC(kernel='rbf')
}

print("5-Fold Cross-Validation Results:")
print("-" * 50)

results = []
for name, model in models.items():
    # 5åˆ†å‰²ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    scores = cross_val_score(model, X_train, y_train, cv=5)

    results.append({
        'Model': name,
        'Mean': scores.mean(),
        'Std': scores.std()
    })

    print(f"{name:20s}: {scores.mean():.3f} (+/- {scores.std():.3f})")

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
# Logistic Regression : 0.967 (+/- 0.033)
# Decision Tree       : 0.958 (+/- 0.042)
# Random Forest       : 0.967 (+/- 0.025)
# SVM                 : 0.975 (+/- 0.025)</code></pre>

        <h3>3.5.2 å­¦ç¿’æ›²ç·š</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹23: å­¦ç¿’æ›²ç·šï¼ˆLearning Curveï¼‰

from sklearn.model_selection import learning_curve

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®å­¦ç¿’æ›²ç·š
train_sizes, train_scores, val_scores = learning_curve(
    RandomForestClassifier(n_estimators=100, random_state=42),
    X_train, y_train, cv=5, n_jobs=-1,
    train_sizes=np.linspace(0.1, 1.0, 10)
)

# å¹³å‡ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—
train_mean = train_scores.mean(axis=1)
train_std = train_scores.std(axis=1)
val_mean = val_scores.mean(axis=1)
val_std = val_scores.std(axis=1)

# ãƒ—ãƒ­ãƒƒãƒˆ
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, label='Training score', color='blue', marker='o')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std,
                 alpha=0.15, color='blue')
plt.plot(train_sizes, val_mean, label='Validation score', color='red', marker='s')
plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std,
                 alpha=0.15, color='red')

plt.xlabel('Training Size', fontsize=12)
plt.ylabel('Score', fontsize=12)
plt.title('Learning Curve - Random Forest', fontsize=14)
plt.legend(loc='best')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# è§£é‡ˆ:
# - è¨“ç·´ã‚¹ã‚³ã‚¢ã¨æ¤œè¨¼ã‚¹ã‚³ã‚¢ãŒè¿‘ã„ â‡’ è‰¯å¥½ãªæ±åŒ–æ€§èƒ½
# - æ¤œè¨¼ã‚¹ã‚³ã‚¢ãŒåæŸ â‡’ ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¦ã‚‚æ”¹å–„ã¯é™å®šçš„</code></pre>

        <h3>3.5.3 ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒè¡¨</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹24: ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®åŒ…æ‹¬çš„æ¯”è¼ƒ

import time

# æ¯”è¼ƒçµæœã‚’æ ¼ç´
results = []

for name, model in models.items():
    # è¨“ç·´æ™‚é–“ã‚’è¨ˆæ¸¬
    start_time = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start_time

    # äºˆæ¸¬æ™‚é–“ã‚’è¨ˆæ¸¬
    start_time = time.time()
    pred = model.predict(X_test)
    predict_time = time.time() - start_time

    # ç²¾åº¦
    acc = accuracy_score(y_test, pred)

    results.append({
        'Model': name,
        'Accuracy': acc,
        'Train Time (s)': train_time,
        'Predict Time (s)': predict_time
    })

# DataFrameã«å¤‰æ›ã—ã¦è¡¨ç¤º
results_df = pd.DataFrame(results)
results_df = results_df.sort_values('Accuracy', ascending=False)

print("\nãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ:")
print(results_df.to_string(index=False))

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ä¾‹:
#                 Model  Accuracy  Train Time (s)  Predict Time (s)
#                   SVM     1.000           0.002             0.001
#     Random Forest     1.000           0.158             0.012
# Logistic Regression     1.000           0.005             0.001
#         Decision Tree     1.000           0.002             0.001</code></pre>

        <h2>3.6 ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</h2>
        <p>ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æœ€å¤§åŒ–ã™ã‚‹ãŸã‚ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’å‰ã«è¨­å®šã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã‚’èª¿æ•´ã—ã¾ã™ã€‚</p>

        <h3>3.6.1 Grid Search</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹25: Grid Searchï¼ˆå…¨æ¢ç´¢ï¼‰

from sklearn.model_selection import GridSearchCV

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€™è£œ
param_grid = {
    'n_estimators': [50, 100, 200],          # æœ¨ã®æ•°
    'max_depth': [3, 5, 10, None],           # æœ¨ã®æ·±ã•
    'min_samples_split': [2, 5, 10]          # åˆ†å‰²ã«å¿…è¦ãªæœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
}

# Grid Searchï¼ˆ5åˆ†å‰²ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid, cv=5, n_jobs=-1, verbose=1
)

# å®Ÿè¡Œï¼ˆ3Ã—4Ã—3=36é€šã‚Šã®çµ„ã¿åˆã‚ã›ã‚’è©¦ã™ï¼‰
grid_search.fit(X_train, y_train)

# æœ€è‰¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
print("æœ€è‰¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
print(grid_search.best_params_)
# æœŸå¾…: {'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100}

# æœ€è‰¯ã®ã‚¹ã‚³ã‚¢
print(f"\næœ€è‰¯ã®CV Score: {grid_search.best_score_:.3f}")
# æœŸå¾…: 0.967</code></pre>

        <h3>3.6.2 æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹26: ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ãƒ¢ãƒ‡ãƒ«è©•ä¾¡

# æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
best_model = grid_search.best_estimator_

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
best_pred = best_model.predict(X_test)
best_accuracy = accuracy_score(y_test, best_pred)

print(f"Test Accuracy (tuned model): {best_accuracy:.3f}")
# æœŸå¾…: 1.000

# ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‰ã¨ã®æ¯”è¼ƒ
print(f"Test Accuracy (default):     {rf_accuracy:.3f}")
# æœŸå¾…: 1.000

# Irisãƒ‡ãƒ¼ã‚¿ã¯å˜ç´”ãªã®ã§å·®ãŒå‡ºã«ãã„ãŒã€
# è¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§5-10%æ”¹å–„ã™ã‚‹ã“ã¨ã‚‚</code></pre>

        <h3>3.6.3 Random Search</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹27: Random Searchï¼ˆåŠ¹ç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆ†å¸ƒã‚’å®šç¾©
param_dist = {
    'n_estimators': randint(50, 300),        # 50-300ã®ç¯„å›²ã§ãƒ©ãƒ³ãƒ€ãƒ 
    'max_depth': randint(3, 20),             # 3-20ã®ç¯„å›²ã§ãƒ©ãƒ³ãƒ€ãƒ 
    'min_samples_split': randint(2, 20)      # 2-20ã®ç¯„å›²ã§ãƒ©ãƒ³ãƒ€ãƒ 
}

# Random Searchï¼ˆ20å›ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_dist, n_iter=20, cv=5, random_state=42, n_jobs=-1
)

random_search.fit(X_train, y_train)

print("Random Search æœ€è‰¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
print(random_search.best_params_)

print(f"\næœ€è‰¯ã®CV Score: {random_search.best_score_:.3f}")

# Random Searchã®åˆ©ç‚¹:
# - Grid Searchã‚ˆã‚Šé«˜é€Ÿï¼ˆ20å› vs 36å›ï¼‰
# - åºƒã„æ¢ç´¢ç©ºé–“ã‚’ã‚«ãƒãƒ¼å¯èƒ½
# - é€£ç¶šå€¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚‚å¯¾å¿œ</code></pre>

        <h3>3.6.4 ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹æœã®å¯è¦–åŒ–</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹28: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹æœã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—

# Grid Searchã®çµæœã‚’DataFrameã«å¤‰æ›
results_df = pd.DataFrame(grid_search.cv_results_)

# n_estimatorsã¨max_depthã®åŠ¹æœã‚’å¯è¦–åŒ–
pivot = results_df.pivot_table(
    values='mean_test_score',
    index='param_max_depth',
    columns='param_n_estimators'
)

# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
plt.figure(figsize=(10, 6))
sns.heatmap(pivot, annot=True, fmt='.3f', cmap='YlGnBu', cbar_kws={'label': 'CV Score'})
plt.title('Grid Search Results: max_depth vs n_estimators', fontsize=14)
plt.xlabel('n_estimators', fontsize=12)
plt.ylabel('max_depth', fontsize=12)
plt.tight_layout()
plt.show()

# è§£é‡ˆ:
# - max_depth=5, n_estimators=100: æœ€é«˜ã‚¹ã‚³ã‚¢
# - max_depth=Noneï¼ˆåˆ¶é™ãªã—ï¼‰: éå­¦ç¿’ã®ãƒªã‚¹ã‚¯</code></pre>

        <h2>3.7 ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</h2>
        <p>ç”Ÿã®ç‰¹å¾´é‡ã‚’å¤‰æ›ãƒ»æ‹¡å¼µã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚</p>

        <h3>3.7.1 å¤šé …å¼ç‰¹å¾´é‡</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹29: å¤šé …å¼ç‰¹å¾´é‡ï¼ˆPolynomial Featuresï¼‰

from sklearn.preprocessing import PolynomialFeatures

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
X_simple = np.array([[1, 2], [3, 4], [5, 6]])
print("å…ƒã®ç‰¹å¾´é‡:")
print(X_simple)
print("å½¢çŠ¶:", X_simple.shape)  # (3, 2)

# 2æ¬¡ã®å¤šé …å¼ç‰¹å¾´é‡ã‚’ç”Ÿæˆ
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X_simple)

print("\nå¤šé …å¼ç‰¹å¾´é‡:")
print(X_poly)
print("å½¢çŠ¶:", X_poly.shape)  # (3, 5)

# ç‰¹å¾´é‡åã‚’ç¢ºèª
print("\nç‰¹å¾´é‡å:")
print(poly.get_feature_names_out(['x1', 'x2']))
# ['x1', 'x2', 'x1^2', 'x1*x2', 'x2^2']

# è§£é‡ˆ:
# - å…ƒ: [1, 2] â†’ æ‹¡å¼µ: [1, 2, 1, 2, 4]
# - x1^2, x1*x2, x2^2 ãªã©ã®ç›¸äº’ä½œç”¨é …ã‚’è¿½åŠ </code></pre>

        <h3>3.7.2 ç‰¹å¾´é‡é¸æŠ</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹30: ç‰¹å¾´é‡é¸æŠï¼ˆFeature Selectionï¼‰

from sklearn.feature_selection import SelectKBest, f_classif

# Irisãƒ‡ãƒ¼ã‚¿ã§æœ€ã‚‚é‡è¦ãª2ã¤ã®ç‰¹å¾´é‡ã‚’é¸æŠ
selector = SelectKBest(f_classif, k=2)
X_selected = selector.fit_transform(iris.data, iris.target)

# é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã‚’ç¢ºèª
selected_features = np.array(iris.feature_names)[selector.get_support()]
print("é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡:")
print(selected_features)
# æœŸå¾…: ['petal length (cm)', 'petal width (cm)']

# Få€¤ï¼ˆANOVAï¼‰ã‚¹ã‚³ã‚¢ã‚’ç¢ºèª
print("\nå„ç‰¹å¾´é‡ã®Få€¤:")
for feature, score in zip(iris.feature_names, selector.scores_):
    print(f"  {feature:20s}: {score:7.2f}")

# è§£é‡ˆ:
# - petal length: 1179.03ï¼ˆæœ€ã‚‚é‡è¦ï¼‰
# - petal width: 960.01ï¼ˆ2ç•ªç›®ã«é‡è¦ï¼‰
# - sepalç³»ã¯ç›¸å¯¾çš„ã«é‡è¦åº¦ãŒä½ã„</code></pre>

        <h3>3.7.3 ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹31: PCAï¼ˆæ¬¡å…ƒå‰Šæ¸›ï¼‰

from sklearn.decomposition import PCA

# Irisãƒ‡ãƒ¼ã‚¿ï¼ˆ4æ¬¡å…ƒï¼‰ã‚’2æ¬¡å…ƒã«å‰Šæ¸›
pca = PCA(n_components=2)
X_pca = pca.fit_transform(iris.data)

print("å…ƒã®æ¬¡å…ƒ:", iris.data.shape)  # (150, 4)
print("PCAå¾Œã®æ¬¡å…ƒ:", X_pca.shape)   # (150, 2)

# èª¬æ˜ã•ã‚ŒãŸåˆ†æ•£ã®å‰²åˆ
print("\nå„ä¸»æˆåˆ†ã®å¯„ä¸ç‡:")
print(pca.explained_variance_ratio_)
# æœŸå¾…: [0.92, 0.05]ï¼ˆç¬¬1ä¸»æˆåˆ†ã§92%ã‚’èª¬æ˜ï¼‰

# ç´¯ç©å¯„ä¸ç‡
print(f"ç´¯ç©å¯„ä¸ç‡: {pca.explained_variance_ratio_.sum():.3f}")
# æœŸå¾…: 0.977ï¼ˆ97.7%ã®æƒ…å ±ã‚’ä¿æŒï¼‰

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=iris.target,
                      cmap='viridis', edgecolor='black', s=50)
plt.xlabel('First Principal Component', fontsize=12)
plt.ylabel('Second Principal Component', fontsize=12)
plt.title('PCA of Iris Dataset', fontsize=14)
plt.colorbar(scatter, label='Species')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# è§£é‡ˆ:
# - 4æ¬¡å…ƒã‚’2æ¬¡å…ƒã«å‰Šæ¸›ã—ã¦ã‚‚97.7%ã®æƒ…å ±ã‚’ä¿æŒ
# - å“ç¨®ãŒãã‚Œã„ã«åˆ†é›¢ã•ã‚Œã‚‹</code></pre>

        <h3>3.7.4 ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡é‡è¦åº¦</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹32: ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã§ç‰¹å¾´é‡é‡è¦åº¦ã‚’è¨ˆç®—
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(iris.data, iris.target)

# ç‰¹å¾´é‡é‡è¦åº¦ã‚’DataFrameã«æ•´ç†
feature_importance_df = pd.DataFrame({
    'feature': iris.feature_names,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

print("ç‰¹å¾´é‡é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
print(feature_importance_df)

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
#               feature  importance
# 2  petal length (cm)       0.445
# 3   petal width (cm)       0.425
# 0  sepal length (cm)       0.089
# 1   sepal width (cm)       0.041

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['feature'], feature_importance_df['importance'],
         color='coral', edgecolor='black')
plt.xlabel('Importance', fontsize=12)
plt.title('Feature Importance (Random Forest)', fontsize=14)
plt.tight_layout()
plt.show()</code></pre>

        <h2>3.8 ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°</h2>
        <p>æ©Ÿæ¢°å­¦ç¿’ã®å®Ÿè£…ã§ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨è§£æ±ºç­–ã‚’ã¾ã¨ã‚ã¾ã™ã€‚</p>

        <h3>3.8.1 ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨è§£æ±ºç­–</h3>
        <table>
            <thead>
                <tr>
                    <th>ã‚¨ãƒ©ãƒ¼</th>
                    <th>åŸå› </th>
                    <th>è§£æ±ºç­–</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><code>ModuleNotFoundError</code></td>
                    <td>ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«</td>
                    <td><code>pip install [library_name]</code></td>
                </tr>
                <tr>
                    <td><code>ValueError: shape mismatch</code></td>
                    <td>ãƒ‡ãƒ¼ã‚¿ã®æ¬¡å…ƒãŒåˆã‚ãªã„</td>
                    <td><code>X.shape</code>ã¨<code>y.shape</code>ã‚’ç¢ºèª</td>
                </tr>
                <tr>
                    <td><code>ConvergenceWarning</code></td>
                    <td>æœ€é©åŒ–ãŒåæŸã—ãªã„</td>
                    <td><code>max_iter</code>ã‚’å¢—ã‚„ã™ã€ã¾ãŸã¯ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°</td>
                </tr>
                <tr>
                    <td>ä½ç²¾åº¦ï¼ˆ<0.5ï¼‰</td>
                    <td>ãƒ‡ãƒ¼ã‚¿å“è³ªã€ãƒ¢ãƒ‡ãƒ«é¸æŠ</td>
                    <td>ãƒ‡ãƒ¼ã‚¿ç¢ºèªã€ç‰¹å¾´é‡è¿½åŠ ã€ãƒ¢ãƒ‡ãƒ«å¤‰æ›´</td>
                </tr>
                <tr>
                    <td>éå­¦ç¿’ï¼ˆtrain>testï¼‰</td>
                    <td>ãƒ¢ãƒ‡ãƒ«ãŒè¤‡é›‘ã™ãã‚‹</td>
                    <td>æ­£å‰‡åŒ–ã€ãƒ‡ãƒ¼ã‚¿è¿½åŠ ã€CVä½¿ç”¨</td>
                </tr>
                <tr>
                    <td><code>MemoryError</code></td>
                    <td>ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºéå¤§</td>
                    <td>ãƒãƒƒãƒå‡¦ç†ã€ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</td>
                </tr>
                <tr>
                    <td>è¨“ç·´ãŒé…ã„</td>
                    <td>ãƒ‡ãƒ¼ã‚¿é‡ã€ãƒ¢ãƒ‡ãƒ«è¤‡é›‘åº¦</td>
                    <td><code>n_jobs=-1</code>ã€GPUä½¿ç”¨ã€ãƒ¢ãƒ‡ãƒ«ç°¡ç•¥åŒ–</td>
                </tr>
            </tbody>
        </table>

        <h3>3.8.2 ãƒ‡ãƒãƒƒã‚°ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ</h3>
        <div class="info-box">
            <strong>5ã‚¹ãƒ†ãƒƒãƒ—ãƒ‡ãƒãƒƒã‚°æ‰‹é †ï¼š</strong>
            <ol>
                <li><strong>ãƒ‡ãƒ¼ã‚¿ç¢ºèª</strong>: <code>df.head()</code>, <code>df.info()</code>, <code>df.describe()</code></li>
                <li><strong>æ¬ æå€¤ãƒã‚§ãƒƒã‚¯</strong>: <code>df.isnull().sum()</code></li>
                <li><strong>ãƒ‡ãƒ¼ã‚¿å‹ç¢ºèª</strong>: <code>df.dtypes</code>ï¼ˆæ•°å€¤å‹ã«ãªã£ã¦ã„ã‚‹ã‹ï¼‰</li>
                <li><strong>å½¢çŠ¶ç¢ºèª</strong>: <code>X.shape</code>, <code>y.shape</code>ï¼ˆæ¬¡å…ƒãŒä¸€è‡´ã™ã‚‹ã‹ï¼‰</li>
                <li><strong>ç°¡å˜ãªãƒ¢ãƒ‡ãƒ«ã§è©¦ã™</strong>: ã¾ãš<code>LogisticRegression</code>ã§å‹•ä½œç¢ºèª</li>
            </ol>
        </div>

        <h2>3.9 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒ£ãƒ¬ãƒ³ã‚¸ï¼šTitanicç”Ÿå­˜äºˆæ¸¬</h2>
        <p>ã“ã“ã¾ã§å­¦ã‚“ã æŠ€è¡“ã‚’ä½¿ã£ã¦ã€å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«æŒ‘æˆ¦ã—ã¾ã™ã€‚</p>

        <h3>3.9.1 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦</h3>
        <div class="info-box">
            <strong>ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›®æ¨™ï¼š</strong>
            <ul>
                <li>ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: Titanicä¹—å®¢ãƒ‡ãƒ¼ã‚¿ï¼ˆKaggleï¼‰</li>
                <li>ã‚¿ã‚¹ã‚¯: ç”Ÿå­˜ï¼ˆSurvived: 0/1ï¼‰ã‚’äºˆæ¸¬</li>
                <li>ç›®æ¨™ç²¾åº¦: 80%ä»¥ä¸Š</li>
                <li>ä½¿ç”¨æŠ€è¡“: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã€ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã€ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ</li>
            </ul>
        </div>

        <h3>3.9.2 ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨EDA</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹33: Titanicãƒ‡ãƒ¼ã‚¿ã®æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆå®Ÿéš›ã«ã¯Kaggleã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰
# ã“ã“ã§ã¯ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
# å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ä»¥ä¸‹ã®URLã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰:
# https://www.kaggle.com/c/titanic/data

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
np.random.seed(42)
df = pd.DataFrame({
    'PassengerId': range(1, 892),
    'Survived': np.random.choice([0, 1], 891),
    'Pclass': np.random.choice([1, 2, 3], 891),
    'Sex': np.random.choice(['male', 'female'], 891),
    'Age': np.random.normal(30, 15, 891).clip(0, 80),
    'SibSp': np.random.choice([0, 1, 2, 3], 891),
    'Parch': np.random.choice([0, 1, 2], 891),
    'Fare': np.random.exponential(30, 891),
    'Embarked': np.random.choice(['S', 'C', 'Q'], 891)
})

# æ¬ æå€¤ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è¿½åŠ 
df.loc[np.random.choice(df.index, 177, replace=False), 'Age'] = np.nan
df.loc[np.random.choice(df.index, 2, replace=False), 'Embarked'] = np.nan

print("ãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­5è¡Œ:")
print(df.head())

print("\nãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±:")
print(df.info())

print("\nåŸºæœ¬çµ±è¨ˆé‡:")
print(df.describe())

print("\næ¬ æå€¤ã®ç¢ºèª:")
print(df.isnull().sum())

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
# Age: 177ä»¶ã®æ¬ æå€¤
# Embarked: 2ä»¶ã®æ¬ æå€¤</code></pre>

        <h3>3.9.3 ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹34: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

from sklearn.preprocessing import LabelEncoder

# ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ
df_processed = df.copy()

# 1. æ¬ æå€¤ã®å‡¦ç†
# Age: ä¸­å¤®å€¤ã§è£œå®Œ
df_processed['Age'].fillna(df_processed['Age'].median(), inplace=True)

# Embarked: æœ€é »å€¤ã§è£œå®Œ
df_processed['Embarked'].fillna(df_processed['Embarked'].mode()[0], inplace=True)

# 2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
# å®¶æ—ã‚µã‚¤ã‚º = SibSpï¼ˆå…„å¼Ÿãƒ»é…å¶è€…ï¼‰ + Parchï¼ˆè¦ªãƒ»å­ä¾›ï¼‰ + 1ï¼ˆæœ¬äººï¼‰
df_processed['FamilySize'] = df_processed['SibSp'] + df_processed['Parch'] + 1

# ä¸€äººæ—…ã‹ã©ã†ã‹
df_processed['IsAlone'] = (df_processed['FamilySize'] == 1).astype(int)

# å¹´é½¢å±¤ï¼ˆã‚«ãƒ†ã‚´ãƒªåŒ–ï¼‰
df_processed['AgeGroup'] = pd.cut(df_processed['Age'],
                                   bins=[0, 12, 18, 60, 100],
                                   labels=['Child', 'Teen', 'Adult', 'Senior'])

# 3. ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
# Sex: LabelEncoderï¼ˆmale=1, female=0ï¼‰
le = LabelEncoder()
df_processed['Sex'] = le.fit_transform(df_processed['Sex'])

# Embarked: One-Hot Encoding
df_processed = pd.get_dummies(df_processed, columns=['Embarked'], prefix='Embarked')

# AgeGroup: One-Hot Encoding
df_processed = pd.get_dummies(df_processed, columns=['AgeGroup'], prefix='Age')

# 4. ä¸è¦ãªåˆ—ã‚’å‰Šé™¤
df_processed.drop(['PassengerId', 'SibSp', 'Parch'], axis=1, inplace=True)

print("å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿:")
print(df_processed.head())
print("\nå½¢çŠ¶:", df_processed.shape)
print("\næ¬ æå€¤:", df_processed.isnull().sum().sum())  # æœŸå¾…: 0</code></pre>

        <h3>3.9.4 ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨è©•ä¾¡</h3>
        <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹35: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨æ¯”è¼ƒ

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’åˆ†é›¢
X = df_processed.drop('Survived', axis=1)
y = df_processed['Survived']

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆè¨“ç·´80%, ãƒ†ã‚¹ãƒˆ20%ï¼‰
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(kernel='rbf', random_state=42)
}

print("ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœ:")
print("-" * 70)

results = []
for name, model in models.items():
    # è¨“ç·´
    model.fit(X_train, y_train)

    # äºˆæ¸¬
    y_pred = model.predict(X_test)

    # è©•ä¾¡
    accuracy = accuracy_score(y_test, y_pred)

    # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    cv_scores = cross_val_score(model, X_train, y_train, cv=5)

    results.append({
        'Model': name,
        'Test Accuracy': accuracy,
        'CV Mean': cv_scores.mean(),
        'CV Std': cv_scores.std()
    })

    print(f"{name:20s}: Test={accuracy:.3f}, CV={cv_scores.mean():.3f} (+/-{cv_scores.std():.3f})")

# çµæœã‚’DataFrameã«å¤‰æ›
results_df = pd.DataFrame(results).sort_values('Test Accuracy', ascending=False)

print("\næœ€çµ‚çµæœï¼ˆç²¾åº¦é †ï¼‰:")
print(results_df.to_string(index=False))

# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
best_model_name = results_df.iloc[0]['Model']
best_model = models[best_model_name]
best_pred = best_model.predict(X_test)

print(f"\n{best_model_name} ã®è©³ç´°:")
print(classification_report(y_test, best_pred))

# æ··åŒè¡Œåˆ—
cm = confusion_matrix(y_test, best_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title(f'Confusion Matrix - {best_model_name}')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.show()

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæˆåŠŸåˆ¤å®š
if results_df.iloc[0]['Test Accuracy'] >= 0.80:
    print("\nğŸ‰ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæˆåŠŸï¼ç›®æ¨™ç²¾åº¦80%ã‚’é”æˆã—ã¾ã—ãŸï¼")
else:
    print(f"\nç›®æ¨™ç²¾åº¦80%ã¾ã§ã‚ã¨{0.80 - results_df.iloc[0]['Test Accuracy']:.1%}ã§ã™ã€‚")
    print("æ”¹å–„ã®ãƒ’ãƒ³ãƒˆ: ç‰¹å¾´é‡è¿½åŠ ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’")</code></pre>

        <h3>3.9.5 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ‹¡å¼µã‚¢ã‚¤ãƒ‡ã‚¢</h3>
        <div class="info-box">
            <strong>ã•ã‚‰ãªã‚‹æŒ‘æˆ¦ï¼š</strong>
            <ol>
                <li><strong>ç‰¹å¾´é‡ã®è¿½åŠ </strong>: åå‰ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆMr., Mrs.ç­‰ï¼‰ã‚’æŠ½å‡º</li>
                <li><strong>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</strong>: GridSearchCVã§æœ€é©åŒ–</li>
                <li><strong>ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’</strong>: VotingClassifierã§è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›</li>
                <li><strong>ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ</strong>: ã©ã®ç‰¹å¾´é‡ãŒç”Ÿå­˜ã«å½±éŸ¿ã—ãŸã‹åˆ†æ</li>
                <li><strong>Kaggleã‚µãƒ–ãƒŸãƒƒãƒˆ</strong>: å®Ÿéš›ã®Kaggleã‚³ãƒ³ãƒšã«æå‡º</li>
                <li><strong>æ·±å±¤å­¦ç¿’</strong>: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆKeras/PyTorchï¼‰ã§å®Ÿè£…</li>
            </ol>
        </div>

        <h2>æœ¬ç« ã®ã¾ã¨ã‚</h2>
        <p>ã“ã®ç« ã§ã¯ã€Pythonã‚’ä½¿ã£ãŸæ©Ÿæ¢°å­¦ç¿’ã®å®Ÿè·µçš„ãªæµã‚Œã‚’å­¦ã³ã¾ã—ãŸã€‚</p>

        <h3>ç¿’å¾—ã—ãŸã‚¹ã‚­ãƒ«</h3>
        <ul>
            <li>âœ… Pythonç’°å¢ƒã®æ§‹ç¯‰ï¼ˆAnaconda/venv/Colabï¼‰</li>
            <li>âœ… ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã€å‰å‡¦ç†ã€å¯è¦–åŒ–</li>
            <li>âœ… å›å¸°å•é¡Œã®å®Ÿè£…ï¼ˆç·šå½¢å›å¸°ã€ä½å®…ä¾¡æ ¼äºˆæ¸¬ï¼‰</li>
            <li>âœ… åˆ†é¡å•é¡Œã®å®Ÿè£…ï¼ˆIrisåˆ†é¡ã€è¤‡æ•°ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒï¼‰</li>
            <li>âœ… ãƒ¢ãƒ‡ãƒ«è©•ä¾¡æŒ‡æ¨™ï¼ˆRMSE, MAE, RÂ², Accuracy, F1-scoreï¼‰</li>
            <li>âœ… ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ5-fold CVï¼‰</li>
            <li>âœ… ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆGrid Search, Random Searchï¼‰</li>
            <li>âœ… ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆå¤šé …å¼ã€é¸æŠã€PCAï¼‰</li>
            <li>âœ… ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼ˆã‚¨ãƒ©ãƒ¼å¯¾å‡¦ã€ãƒ‡ãƒãƒƒã‚°æ‰‹é †ï¼‰</li>
            <li>âœ… å®Ÿãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼ˆTitanicç”Ÿå­˜äºˆæ¸¬ï¼‰</li>
        </ul>

        <h3>é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ</h3>
        <div class="success-box">
            <strong>æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®5ã‚¹ãƒ†ãƒƒãƒ—ï¼š</strong>
            <ol>
                <li><strong>ãƒ‡ãƒ¼ã‚¿ç†è§£</strong>: EDAã€å¯è¦–åŒ–ã€çµ±è¨ˆé‡ç¢ºèª</li>
                <li><strong>å‰å‡¦ç†</strong>: æ¬ æå€¤å‡¦ç†ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°</li>
                <li><strong>ãƒ¢ãƒ‡ãƒ«é¸æŠ</strong>: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ã¦æ¯”è¼ƒ</li>
                <li><strong>è©•ä¾¡ãƒ»æ”¹å–„</strong>: CVã€ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</li>
                <li><strong>ãƒ‡ãƒ—ãƒ­ã‚¤æº–å‚™</strong>: æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ</li>
            </ol>
        </div>

        <h3>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>
        <p>ç¬¬4ç« ã§ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã®å®Ÿä¸–ç•Œã¸ã®å¿œç”¨ã‚’å­¦ã³ã¾ã™ï¼š</p>
        <ul>
            <li>5ã¤ã®è©³ç´°ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ï¼ˆNetflixã€Googleç¿»è¨³ã€Teslaç­‰ï¼‰</li>
            <li>å°†æ¥ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆåŸºç›¤ãƒ¢ãƒ‡ãƒ«ã€AutoMLã€ã‚¨ãƒƒã‚¸AIï¼‰</li>
            <li>ã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã€MLã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€ç ”ç©¶è€…ï¼‰</li>
            <li>å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹ã¨ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£</li>
        </ul>

        <h2>æ¼”ç¿’å•é¡Œ</h2>

        <details>
            <summary><strong>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šEasyï¼‰</strong> - ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨åŸºæœ¬çµ±è¨ˆ</summary>
            <p><strong>å•é¡Œï¼š</strong> Irisãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ã€å„ç‰¹å¾´é‡ã®å¹³å‡å€¤ã€ä¸­å¤®å€¤ã€æ¨™æº–åå·®ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚</p>

            <p><strong>ãƒ’ãƒ³ãƒˆï¼š</strong> <code>df.describe()</code>ã‚’ä½¿ã†ã¨ä¸€åº¦ã«ç¢ºèªã§ãã¾ã™ã€‚</p>

            <details>
                <summary>è§£ç­”ä¾‹</summary>
                <pre><code class="language-python">from sklearn.datasets import load_iris
import pandas as pd

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)

# åŸºæœ¬çµ±è¨ˆé‡
print(df.describe())

# å€‹åˆ¥ã«è¨ˆç®—ã™ã‚‹å ´åˆ
print("\nå¹³å‡å€¤:")
print(df.mean())

print("\nä¸­å¤®å€¤:")
print(df.median())

print("\næ¨™æº–åå·®:")
print(df.std())</code></pre>
            </details>
        </details>

        <details>
            <summary><strong>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šEasyï¼‰</strong> - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²</summary>
            <p><strong>å•é¡Œï¼š</strong> ã‚«ãƒªãƒ•ã‚©ãƒ«ãƒ‹ã‚¢ä½å®…ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´70%ã€ãƒ†ã‚¹ãƒˆ30%ã«åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚åˆ†å‰²å¾Œã®ãƒ‡ãƒ¼ã‚¿æ•°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚</p>

            <p><strong>ãƒ’ãƒ³ãƒˆï¼š</strong> <code>train_test_split</code>ã®<code>test_size</code>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰æ›´ã—ã¾ã™ã€‚</p>

            <details>
                <summary>è§£ç­”ä¾‹</summary>
                <pre><code class="language-python">from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
housing = fetch_california_housing()
X, y = housing.data, housing.target

# è¨“ç·´70%, ãƒ†ã‚¹ãƒˆ30%ã«åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

print("å…ƒã®ãƒ‡ãƒ¼ã‚¿æ•°:", len(X))           # 20640
print("è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°:", len(X_train))     # 14448
print("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°:", len(X_test))   # 6192
print("åˆ†å‰²æ¯”ç‡:", len(X_train)/len(X))  # 0.70</code></pre>
            </details>
        </details>

        <details>
            <summary><strong>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šMediumï¼‰</strong> - ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦æ¯”è¼ƒ</summary>
            <p><strong>å•é¡Œï¼š</strong> Irisãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã€æ±ºå®šæœ¨ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®3ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€ãƒ†ã‚¹ãƒˆç²¾åº¦ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚æœ€ã‚‚ç²¾åº¦ã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚</p>

            <p><strong>ãƒ’ãƒ³ãƒˆï¼š</strong> <code>accuracy_score</code>ã‚’ä½¿ã£ã¦å„ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’è¨ˆç®—ã—ã¾ã™ã€‚</p>

            <details>
                <summary>è§£ç­”ä¾‹</summary>
                <pre><code class="language-python">from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.2, random_state=42
)

# ãƒ¢ãƒ‡ãƒ«å®šç¾©
models = {
    'Logistic Regression': LogisticRegression(max_iter=200),
    'Decision Tree': DecisionTreeClassifier(max_depth=3),
    'Random Forest': RandomForestClassifier(n_estimators=100)
}

# è¨“ç·´ã¨è©•ä¾¡
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, pred)
    results[name] = accuracy
    print(f"{name}: {accuracy:.3f}")

# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«
best_model = max(results, key=results.get)
print(f"\næœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model} ({results[best_model]:.3f})")</code></pre>
            </details>
        </details>

        <details>
            <summary><strong>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šMediumï¼‰</strong> - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</summary>
            <p><strong>å•é¡Œï¼š</strong> ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã§<code>n_estimators=[50, 100, 150]</code>ã¨<code>max_depth=[3, 5, 7]</code>ã®çµ„ã¿åˆã‚ã›ã§Grid Searchã‚’å®Ÿè¡Œã—ã€æœ€è‰¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¦‹ã¤ã‘ã¦ãã ã•ã„ã€‚</p>

            <p><strong>ãƒ’ãƒ³ãƒˆï¼š</strong> <code>GridSearchCV</code>ã‚’ä½¿ã„ã¾ã™ã€‚</p>

            <details>
                <summary>è§£ç­”ä¾‹</summary>
                <pre><code class="language-python">from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.2, random_state=42
)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [3, 5, 7]
}

# Grid Search
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid, cv=5, n_jobs=-1
)

grid_search.fit(X_train, y_train)

# çµæœ
print("æœ€è‰¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:", grid_search.best_params_)
print("æœ€è‰¯ã®CV Score:", grid_search.best_score_)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
test_accuracy = grid_search.best_estimator_.score(X_test, y_test)
print("ãƒ†ã‚¹ãƒˆç²¾åº¦:", test_accuracy)</code></pre>
            </details>
        </details>

        <details>
            <summary><strong>å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šHardï¼‰</strong> - å®Œå…¨ãªMLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</summary>
            <p><strong>å•é¡Œï¼š</strong> ã‚«ãƒªãƒ•ã‚©ãƒ«ãƒ‹ã‚¢ä½å®…ãƒ‡ãƒ¼ã‚¿ã§ã€ä»¥ä¸‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’å«ã‚€å®Œå…¨ãªMLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ã¦ãã ã•ã„ï¼š</p>
            <ol>
                <li>ãƒ‡ãƒ¼ã‚¿ã®80/20åˆ†å‰²</li>
                <li>StandardScalerã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°</li>
                <li>ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°ã§è¨“ç·´</li>
                <li>RMSEã€MAEã€RÂ²ã‚’è¨ˆç®—</li>
                <li>äºˆæ¸¬vså®Ÿæ¸¬ã®ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ</li>
            </ol>

            <p><strong>ãƒ’ãƒ³ãƒˆï¼š</strong> ã“ã‚Œã¾ã§å­¦ã‚“ã ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’çµ„ã¿åˆã‚ã›ã¾ã™ã€‚</p>

            <details>
                <summary>è§£ç­”ä¾‹</summary>
                <pre><code class="language-python">from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np
import matplotlib.pyplot as plt

# 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
housing = fetch_california_housing()
X, y = housing.data, housing.target

# 2. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 3. ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

# 5. äºˆæ¸¬
y_pred = model.predict(X_test_scaled)

# 6. è©•ä¾¡
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("è©•ä¾¡çµæœ:")
print(f"  RMSE: {rmse:.3f}")
print(f"  MAE:  {mae:.3f}")
print(f"  RÂ²:   {r2:.3f}")

# 7. å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5, edgecolor='black')
plt.plot([y_test.min(), y_test.max()],
         [y_test.min(), y_test.max()],
         'r--', lw=2, label='Perfect Prediction')
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Random Forest: Predictions vs Actual')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()</code></pre>
            </details>
        </details>

        <h2>å‚è€ƒæ–‡çŒ®</h2>
        <ol>
            <li>Pedregosa, F., et al. (2011). "Scikit-learn: Machine Learning in Python." <em>Journal of Machine Learning Research</em>, 12, 2825-2830.</li>
            <li>VanderPlas, J. (2016). <em>Python Data Science Handbook</em>. O'Reilly Media.</li>
            <li>GÃ©ron, A. (2019). <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em> (2nd ed.). O'Reilly Media.</li>
            <li>scikit-learn Documentation. (2024). "User Guide." URL: <a href="https://scikit-learn.org/stable/user_guide.html">https://scikit-learn.org/stable/user_guide.html</a></li>
            <li>Kaggle. (2024). "Titanic: Machine Learning from Disaster." URL: <a href="https://www.kaggle.com/c/titanic">https://www.kaggle.com/c/titanic</a></li>
        </ol>

        <div class="navigation">
            <a href="chapter2-fundamentals.html" class="nav-button">â† ç¬¬2ç« ï¼šåŸºç¤çŸ¥è­˜</a>
            <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a>
            <a href="chapter4-real-world.html" class="nav-button">ç¬¬4ç« ï¼šå®Ÿä¸–ç•Œã¸ã®å¿œç”¨ â†’</a>
        </div>
    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <div class="container">
            <p>&copy; 2025 ML Knowledge Hub - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
