<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 4 - æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ  - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
        <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/wp/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/ML/recommendation-systems-introduction/index.html">Recommendation Systems</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 4</span>
        </div>
    </nav>

    <header>
        <div class="header-content">
            <h1>ç¬¬4ç« ï¼šæ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ </h1>
            <p class="subtitle">Neural Collaborative Filteringã€Two-Tower Modelsã€Sequence-basedæ¨è–¦</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 40-50åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 6å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<div class="learning-objectives">
<h2>å­¦ç¿’ç›®æ¨™</h2>
<ul>
<li>Neural Collaborative Filtering (NCF) ã®ç†è«–ã¨å®Ÿè£…ã‚’ç†è§£ã™ã‚‹</li>
<li>Factorization Machinesã¨DeepFMã®ä»•çµ„ã¿ã‚’å­¦ã¶</li>
<li>Two-Tower Modelsã«ã‚ˆã‚‹å¤§è¦æ¨¡æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹</li>
<li>Sequence-basedæ¨è–¦ï¼ˆRNNã€Transformerï¼‰ã‚’å®Ÿè£…ã™ã‚‹</li>
<li>å®Ÿè·µçš„ãªæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’è¨­è¨ˆã™ã‚‹</li>
</ul>
</div>

<h2>4.1 Neural Collaborative Filtering (NCF)</h2>

<h3>4.1.1 NCFã®å‹•æ©Ÿ</h3>

<p><strong>å¾“æ¥ã®å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®é™ç•Œï¼š</strong></p>
<ul>
<li>Matrix Factorization: ç·šå½¢ãªå†…ç©ã®ã¿ã§ç›¸äº’ä½œç”¨ã‚’è¡¨ç¾</li>
<li>$\hat{r}_{ui} = p_u^T q_i$ ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼åŸ‹ã‚è¾¼ã¿ Ã— ã‚¢ã‚¤ãƒ†ãƒ åŸ‹ã‚è¾¼ã¿ï¼‰</li>
<li>å•é¡Œï¼šéç·šå½¢ãªå—œå¥½ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ‰ãˆã‚‰ã‚Œãªã„</li>
</ul>

<p><strong>NCFã®ææ¡ˆï¼š</strong></p>
<ul>
<li>Multi-Layer Perceptron (MLP) ã§éç·šå½¢æ€§ã‚’å°å…¥</li>
<li>è«–æ–‡: He et al. "Neural Collaborative Filtering" (WWW 2017)</li>
<li>å¼•ç”¨æ•°: 4,000+ï¼ˆ2024å¹´æ™‚ç‚¹ï¼‰</li>
</ul>

<h3>4.1.2 NCFã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h3>

<p><strong>æ§‹æˆè¦ç´ ï¼š</strong></p>

<pre><code>Input Layer
â”œâ”€ User ID â†’ User Embedding (dim=K)
â””â”€ Item ID â†’ Item Embedding (dim=K)
    â†“
Interaction Layer (è¤‡æ•°ã®é¸æŠè‚¢)
â”œâ”€ (1) GMF: p_u âŠ™ q_i (è¦ç´ ç©)
â”œâ”€ (2) MLP: f(concat(p_u, q_i))
â””â”€ (3) NeuMF: GMF + MLP ã®èåˆ
    â†“
Output Layer: Ïƒ(weighted sum) â†’ äºˆæ¸¬è©•ä¾¡å€¤
</code></pre>

<p><strong>æ•°å¼è¡¨ç¾ï¼š</strong></p>

<p><strong>GMF (Generalized Matrix Factorization):</strong></p>
$$
\hat{y}_{ui}^{GMF} = \sigma(h^T (p_u \odot q_i))
$$

<p><strong>MLP:</strong></p>
$$
\begin{aligned}
z_1 &= \phi_1(p_u, q_i) = \text{concat}(p_u, q_i) \\
z_2 &= \sigma(W_2^T z_1 + b_2) \\
&\vdots \\
z_L &= \sigma(W_L^T z_{L-1} + b_L) \\
\hat{y}_{ui}^{MLP} &= \sigma(h^T z_L)
\end{aligned}
$$

<p><strong>NeuMF (Neural Matrix Factorization):</strong></p>
$$
\hat{y}_{ui} = \sigma(h^T [\text{GMF output} \,||\, \text{MLP output}])
$$

<h3>4.1.3 PyTorchå®Ÿè£…</h3>

<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd

class NCFDataset(Dataset):
    """NCFç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    def __init__(self, user_ids, item_ids, ratings):
        self.user_ids = torch.LongTensor(user_ids)
        self.item_ids = torch.LongTensor(item_ids)
        self.ratings = torch.FloatTensor(ratings)

    def __len__(self):
        return len(self.user_ids)

    def __getitem__(self, idx):
        return self.user_ids[idx], self.item_ids[idx], self.ratings[idx]


class GMF(nn.Module):
    """Generalized Matrix Factorization"""
    def __init__(self, n_users, n_items, embedding_dim=32):
        super(GMF, self).__init__()

        # Embeddings
        self.user_embedding = nn.Embedding(n_users, embedding_dim)
        self.item_embedding = nn.Embedding(n_items, embedding_dim)

        # Output layer
        self.fc = nn.Linear(embedding_dim, 1)

        # Initialize weights
        nn.init.normal_(self.user_embedding.weight, std=0.01)
        nn.init.normal_(self.item_embedding.weight, std=0.01)

    def forward(self, user_ids, item_ids):
        # Get embeddings
        user_emb = self.user_embedding(user_ids)  # (batch, dim)
        item_emb = self.item_embedding(item_ids)  # (batch, dim)

        # Element-wise product
        element_product = user_emb * item_emb  # (batch, dim)

        # Output
        output = self.fc(element_product)  # (batch, 1)
        return torch.sigmoid(output.squeeze())


class MLPModel(nn.Module):
    """MLP-based Collaborative Filtering"""
    def __init__(self, n_users, n_items, embedding_dim=32, layers=[64, 32, 16]):
        super(MLPModel, self).__init__()

        # Embeddings
        self.user_embedding = nn.Embedding(n_users, embedding_dim)
        self.item_embedding = nn.Embedding(n_items, embedding_dim)

        # MLP layers
        mlp_modules = []
        input_size = embedding_dim * 2
        for layer_size in layers:
            mlp_modules.append(nn.Linear(input_size, layer_size))
            mlp_modules.append(nn.ReLU())
            mlp_modules.append(nn.Dropout(0.2))
            input_size = layer_size

        self.mlp = nn.Sequential(*mlp_modules)
        self.fc = nn.Linear(layers[-1], 1)

        # Initialize weights
        nn.init.normal_(self.user_embedding.weight, std=0.01)
        nn.init.normal_(self.item_embedding.weight, std=0.01)

    def forward(self, user_ids, item_ids):
        # Get embeddings
        user_emb = self.user_embedding(user_ids)
        item_emb = self.item_embedding(item_ids)

        # Concatenate
        concat = torch.cat([user_emb, item_emb], dim=-1)  # (batch, dim*2)

        # MLP
        mlp_output = self.mlp(concat)
        output = self.fc(mlp_output)
        return torch.sigmoid(output.squeeze())


class NeuMF(nn.Module):
    """Neural Matrix Factorization (GMF + MLP)"""
    def __init__(self, n_users, n_items,
                 gmf_dim=32, mlp_dim=32, layers=[64, 32, 16]):
        super(NeuMF, self).__init__()

        # GMF Embeddings
        self.gmf_user_embedding = nn.Embedding(n_users, gmf_dim)
        self.gmf_item_embedding = nn.Embedding(n_items, gmf_dim)

        # MLP Embeddings
        self.mlp_user_embedding = nn.Embedding(n_users, mlp_dim)
        self.mlp_item_embedding = nn.Embedding(n_items, mlp_dim)

        # MLP layers
        mlp_modules = []
        input_size = mlp_dim * 2
        for layer_size in layers:
            mlp_modules.append(nn.Linear(input_size, layer_size))
            mlp_modules.append(nn.ReLU())
            mlp_modules.append(nn.Dropout(0.2))
            input_size = layer_size

        self.mlp = nn.Sequential(*mlp_modules)

        # Final prediction layer
        self.fc = nn.Linear(gmf_dim + layers[-1], 1)

        # Initialize
        self._init_weights()

    def _init_weights(self):
        nn.init.normal_(self.gmf_user_embedding.weight, std=0.01)
        nn.init.normal_(self.gmf_item_embedding.weight, std=0.01)
        nn.init.normal_(self.mlp_user_embedding.weight, std=0.01)
        nn.init.normal_(self.mlp_item_embedding.weight, std=0.01)

    def forward(self, user_ids, item_ids):
        # GMF part
        gmf_user_emb = self.gmf_user_embedding(user_ids)
        gmf_item_emb = self.gmf_item_embedding(item_ids)
        gmf_output = gmf_user_emb * gmf_item_emb  # (batch, gmf_dim)

        # MLP part
        mlp_user_emb = self.mlp_user_embedding(user_ids)
        mlp_item_emb = self.mlp_item_embedding(item_ids)
        mlp_concat = torch.cat([mlp_user_emb, mlp_item_emb], dim=-1)
        mlp_output = self.mlp(mlp_concat)  # (batch, layers[-1])

        # Concatenate GMF and MLP
        concat = torch.cat([gmf_output, mlp_output], dim=-1)
        output = self.fc(concat)
        return torch.sigmoid(output.squeeze())


def train_ncf(model, train_loader, n_epochs=10, lr=0.001):
    """NCFãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(n_epochs):
        model.train()
        total_loss = 0

        for user_ids, item_ids, ratings in train_loader:
            user_ids = user_ids.to(device)
            item_ids = item_ids.to(device)
            ratings = ratings.to(device)

            # Forward pass
            predictions = model(user_ids, item_ids)
            loss = criterion(predictions, ratings)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}")

    return model


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    n_users = 1000
    n_items = 500
    n_samples = 10000

    user_ids = np.random.randint(0, n_users, n_samples)
    item_ids = np.random.randint(0, n_items, n_samples)
    ratings = np.random.randint(0, 2, n_samples)  # Binary ratings

    # Dataset and DataLoader
    dataset = NCFDataset(user_ids, item_ids, ratings)
    train_loader = DataLoader(dataset, batch_size=256, shuffle=True)

    # NeuMFãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
    model = NeuMF(n_users, n_items, gmf_dim=32, mlp_dim=32, layers=[64, 32, 16])
    trained_model = train_ncf(model, train_loader, n_epochs=5, lr=0.001)

    print("\nNeuMFãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´å®Œäº†")
</code></pre>

<h3>4.1.4 NCFã®æ€§èƒ½</h3>

<p><strong>å®Ÿé¨“çµæœï¼ˆMovieLens-1Mï¼‰ï¼š</strong></p>

<table>
<tr>
<th>ãƒ¢ãƒ‡ãƒ«</th>
<th>HR@10</th>
<th>NDCG@10</th>
<th>è¨“ç·´æ™‚é–“</th>
</tr>
<tr>
<td>Matrix Factorization</td>
<td>0.692</td>
<td>0.425</td>
<td>5åˆ†</td>
</tr>
<tr>
<td>GMF</td>
<td>0.704</td>
<td>0.432</td>
<td>8åˆ†</td>
</tr>
<tr>
<td>MLP</td>
<td>0.718</td>
<td>0.445</td>
<td>12åˆ†</td>
</tr>
<tr>
<td>NeuMF</td>
<td>0.726</td>
<td>0.463</td>
<td>15åˆ†</td>
</tr>
</table>

<p><strong>çµè«–ï¼š</strong> NeuMFãŒæœ€é«˜æ€§èƒ½ã€MLPã¨GMFã®çµ„ã¿åˆã‚ã›ãŒåŠ¹æœçš„</p>

<hr>

<h2>4.2 Factorization Machines</h2>

<h3>4.2.1 FMã®å‹•æ©Ÿ</h3>

<p><strong>å•é¡Œè¨­å®šï¼š</strong></p>
<ul>
<li>ã‚¹ãƒ‘ãƒ¼ã‚¹ãªç‰¹å¾´é‡ï¼ˆcategorical featuresï¼‰ã®å‡¦ç†</li>
<li>ä¾‹ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼IDã€ã‚¢ã‚¤ãƒ†ãƒ IDã€ã‚«ãƒ†ã‚´ãƒªã€æ™‚åˆ»ã€ãƒ‡ãƒã‚¤ã‚¹</li>
<li>One-hot encodingã§æ•°ä¸‡ã€œæ•°ç™¾ä¸‡æ¬¡å…ƒ</li>
</ul>

<p><strong>å¾“æ¥æ‰‹æ³•ã®é™ç•Œï¼š</strong></p>
<ul>
<li>ç·šå½¢å›å¸°ï¼šç‰¹å¾´é‡é–“ã®ç›¸äº’ä½œç”¨ã‚’æ‰ãˆã‚‰ã‚Œãªã„</li>
<li>å¤šé …å¼å›å¸°ï¼šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒçˆ†ç™ºï¼ˆ$O(n^2)$ï¼‰</li>
<li>SVMãªã©ï¼šè¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ã€ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã§æ€§èƒ½ä½ä¸‹</li>
</ul>

<p><strong>FMã®ææ¡ˆï¼š</strong></p>
<ul>
<li>è«–æ–‡: Rendle "Factorization Machines" (ICDM 2010)</li>
<li>ç‰¹å¾´é‡é–“ã®2æ¬¡ç›¸äº’ä½œç”¨ã‚’åŠ¹ç‡çš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–</li>
<li>æ™‚é–“è¨ˆç®—é‡ï¼š$O(kn)$ï¼ˆkã¯åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã€nã¯ç‰¹å¾´é‡æ•°ï¼‰</li>
</ul>

<h3>4.2.2 FMæ•°å¼</h3>

<p><strong>äºˆæ¸¬å¼ï¼š</strong></p>
$$
\hat{y}(x) = w_0 + \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n \langle v_i, v_j \rangle x_i x_j
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$w_0$: ãƒã‚¤ã‚¢ã‚¹é …</li>
<li>$w_i$: 1æ¬¡ä¿‚æ•°ï¼ˆç·šå½¢é …ï¼‰</li>
<li>$\langle v_i, v_j \rangle = \sum_{f=1}^k v_{i,f} \cdot v_{j,f}$: 2æ¬¡ä¿‚æ•°ï¼ˆç›¸äº’ä½œç”¨é …ï¼‰</li>
<li>$v_i \in \mathbb{R}^k$: ç‰¹å¾´é‡$i$ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆ$k$æ¬¡å…ƒï¼‰</li>
</ul>

<p><strong>è¨ˆç®—é‡å‰Šæ¸›ã®ãƒˆãƒªãƒƒã‚¯ï¼š</strong></p>
$$
\sum_{i=1}^n \sum_{j=i+1}^n \langle v_i, v_j \rangle x_i x_j = \frac{1}{2} \sum_{f=1}^k \left[ \left( \sum_{i=1}^n v_{i,f} x_i \right)^2 - \sum_{i=1}^n v_{i,f}^2 x_i^2 \right]
$$

<p>ã“ã‚Œã«ã‚ˆã‚Šè¨ˆç®—é‡ãŒ $O(n^2) \to O(kn)$ ã«å‰Šæ¸›</p>

<h3>4.2.3 Field-aware FM (FFM)</h3>

<p><strong>FMã®æ‹¡å¼µï¼š</strong></p>
<ul>
<li>å„ç‰¹å¾´é‡ãŒè¤‡æ•°ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆfieldï¼‰ã«åˆ†é¡ã•ã‚Œã‚‹</li>
<li>ä¾‹ï¼šUser fieldï¼ˆuser_id, age, genderï¼‰ã€Item fieldï¼ˆitem_id, categoryï¼‰</li>
<li>ç•°ãªã‚‹fieldãƒšã‚¢ã§ç•°ãªã‚‹åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨</li>
</ul>

<p><strong>FFMäºˆæ¸¬å¼ï¼š</strong></p>
$$
\hat{y}(x) = w_0 + \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n \langle v_{i,f_j}, v_{j,f_i} \rangle x_i x_j
$$

<ul>
<li>$v_{i,f_j}$: ç‰¹å¾´é‡$i$ãŒfield $f_j$ã¨ç›¸äº’ä½œç”¨ã™ã‚‹ã¨ãã®åŸ‹ã‚è¾¼ã¿</li>
<li>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ï¼š$nfk$ï¼ˆ$f$ã¯ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•°ï¼‰</li>
</ul>

<h3>4.2.4 DeepFM</h3>

<p><strong>FM + Deep Neural Networkã®èåˆï¼š</strong></p>
<ul>
<li>è«–æ–‡: Guo et al. "DeepFM" (IJCAI 2017)</li>
<li>ä½æ¬¡ç›¸äº’ä½œç”¨ï¼ˆFMï¼‰+ é«˜æ¬¡ç›¸äº’ä½œç”¨ï¼ˆDNNï¼‰</li>
</ul>

<p><strong>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼š</strong></p>
<pre><code>Input: Sparse Features (one-hot)
    â†“
Embedding Layer (å…±æœ‰)
    â”œâ”€ FM Component
    â”‚   â””â”€ 1æ¬¡é … + 2æ¬¡é …ï¼ˆç›¸äº’ä½œç”¨ï¼‰
    â””â”€ Deep Component
        â””â”€ MLP (è¤‡æ•°å±¤ã€ReLU)
    â†“
Output: Ïƒ(FM output + Deep output)
</code></pre>

<p><strong>æ•°å¼ï¼š</strong></p>
$$
\hat{y} = \sigma(y_{FM} + y_{DNN})
$$

<h3>4.2.5 DeepFMå®Ÿè£…</h3>

<pre><code>import torch
import torch.nn as nn

class FMLayer(nn.Module):
    """Factorization Machine Layer"""
    def __init__(self, n_features, embedding_dim=10):
        super(FMLayer, self).__init__()

        # 1æ¬¡é …
        self.linear = nn.Embedding(n_features, 1)

        # 2æ¬¡é …ï¼ˆåŸ‹ã‚è¾¼ã¿ï¼‰
        self.embedding = nn.Embedding(n_features, embedding_dim)

        # Initialize
        nn.init.xavier_uniform_(self.linear.weight)
        nn.init.xavier_uniform_(self.embedding.weight)

    def forward(self, x):
        """
        Args:
            x: (batch, n_fields) - feature indices
        Returns:
            fm_output: (batch,) - FM prediction
        """
        # 1æ¬¡é …
        linear_part = self.linear(x).sum(dim=1).squeeze()  # (batch,)

        # 2æ¬¡é …
        embeddings = self.embedding(x)  # (batch, n_fields, dim)

        # (sum of squares) - (square of sum)
        square_of_sum = torch.sum(embeddings, dim=1) ** 2  # (batch, dim)
        sum_of_square = torch.sum(embeddings ** 2, dim=1)  # (batch, dim)

        interaction = 0.5 * (square_of_sum - sum_of_square).sum(dim=1)  # (batch,)

        return linear_part + interaction


class DeepFM(nn.Module):
    """DeepFM: FM + Deep Neural Network"""
    def __init__(self, n_features, n_fields, embedding_dim=10,
                 deep_layers=[256, 128, 64]):
        super(DeepFM, self).__init__()

        self.n_fields = n_fields

        # å…±æœ‰Embedding layer
        self.embedding = nn.Embedding(n_features, embedding_dim)

        # FM component
        self.linear = nn.Embedding(n_features, 1)
        self.bias = nn.Parameter(torch.zeros(1))

        # Deep component
        deep_input_dim = n_fields * embedding_dim
        deep_modules = []

        for i, layer_size in enumerate(deep_layers):
            if i == 0:
                deep_modules.append(nn.Linear(deep_input_dim, layer_size))
            else:
                deep_modules.append(nn.Linear(deep_layers[i-1], layer_size))
            deep_modules.append(nn.BatchNorm1d(layer_size))
            deep_modules.append(nn.ReLU())
            deep_modules.append(nn.Dropout(0.5))

        self.deep = nn.Sequential(*deep_modules)
        self.deep_fc = nn.Linear(deep_layers[-1], 1)

        # Initialize
        nn.init.xavier_uniform_(self.embedding.weight)
        nn.init.xavier_uniform_(self.linear.weight)

    def forward(self, x):
        """
        Args:
            x: (batch, n_fields) - feature indices
        Returns:
            output: (batch,) - prediction
        """
        # Embeddings
        embeddings = self.embedding(x)  # (batch, n_fields, dim)

        # FM part
        # 1æ¬¡é …
        linear_part = self.linear(x).sum(dim=1).squeeze() + self.bias

        # 2æ¬¡é …
        square_of_sum = torch.sum(embeddings, dim=1) ** 2
        sum_of_square = torch.sum(embeddings ** 2, dim=1)
        fm_interaction = 0.5 * (square_of_sum - sum_of_square).sum(dim=1)

        fm_output = linear_part + fm_interaction

        # Deep part
        deep_input = embeddings.view(-1, self.n_fields * embeddings.size(2))
        deep_output = self.deep(deep_input)
        deep_output = self.deep_fc(deep_output).squeeze()

        # Combine
        output = torch.sigmoid(fm_output + deep_output)
        return output


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    batch_size = 128
    n_features = 10000  # Total unique features
    n_fields = 20       # Number of feature fields

    # ãƒ©ãƒ³ãƒ€ãƒ ãªç‰¹å¾´ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    x = torch.randint(0, n_features, (batch_size, n_fields))

    # ãƒ¢ãƒ‡ãƒ«
    model = DeepFM(n_features, n_fields, embedding_dim=10,
                   deep_layers=[256, 128, 64])

    # Forward pass
    output = model(x)
    print(f"Input shape: {x.shape}")
    print(f"Output shape: {output.shape}")
    print(f"Sample outputs: {output[:5]}")
</code></pre>

<hr>

<h2>4.3 Two-Tower Models</h2>

<h3>4.3.1 Two-Towerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h3>

<p><strong>å‹•æ©Ÿï¼š</strong></p>
<ul>
<li>å¤§è¦æ¨¡æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ï¼ˆæ•°å„„ãƒ¦ãƒ¼ã‚¶ãƒ¼ Ã— æ•°åƒä¸‡ã‚¢ã‚¤ãƒ†ãƒ ï¼‰</li>
<li>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã®å¿…è¦æ€§</li>
<li>User Towerã¨Item Towerã‚’åˆ†é›¢ã—ã¦ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã«</li>
</ul>

<p><strong>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼š</strong></p>
<pre><code>User Features          Item Features
    â†“                      â†“
User Tower            Item Tower
(DNN)                 (DNN)
    â†“                      â†“
User Embedding        Item Embedding
(dim=128)             (dim=128)
    â†“                      â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”€ dot product â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
          Similarity Score
</code></pre>

<p><strong>ç‰¹å¾´ï¼š</strong></p>
<ul>
<li>User/ItemåŸ‹ã‚è¾¼ã¿ã‚’äº‹å‰è¨ˆç®—å¯èƒ½</li>
<li>æ¨è«–æ™‚ï¼šæœ€è¿‘å‚æ¢ç´¢ï¼ˆANN: Approximate Nearest Neighborï¼‰</li>
<li>è¨ˆç®—é‡ï¼š$O(\log N)$ with ANN index (FAISS, ScaNN)</li>
</ul>

<h3>4.3.2 Contrastive Learning</h3>

<p><strong>è¨“ç·´ç›®æ¨™ï¼š</strong></p>
<ul>
<li>Positive pairï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå®Ÿéš›ã«é–²è¦§ã—ãŸã‚¢ã‚¤ãƒ†ãƒ ï¼‰ã®é¡ä¼¼åº¦ã‚’æœ€å¤§åŒ–</li>
<li>Negative pairï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã®é¡ä¼¼åº¦ã‚’æœ€å°åŒ–</li>
</ul>

<p><strong>æå¤±é–¢æ•°ï¼ˆTriplet Lossï¼‰ï¼š</strong></p>
$$
L = \sum_{(u,i^+,i^-)} \max(0, \alpha - \text{sim}(u, i^+) + \text{sim}(u, i^-))
$$

<p>ã¾ãŸã¯<strong>InfoNCE Loss</strong>:</p>
$$
L = -\log \frac{\exp(\text{sim}(u, i^+) / \tau)}{\sum_{j \in \mathcal{N}} \exp(\text{sim}(u, i_j) / \tau)}
$$

<ul>
<li>$\tau$: temperature parameter</li>
<li>$\mathcal{N}$: negative samples</li>
</ul>

<h3>4.3.3 Two-Towerå®Ÿè£…</h3>

<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

class TowerNetwork(nn.Module):
    """Single Tower (User or Item)"""
    def __init__(self, input_dim, embedding_dim=128, hidden_dims=[256, 128]):
        super(TowerNetwork, self).__init__()

        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.3))
            prev_dim = hidden_dim

        # Final embedding layer
        layers.append(nn.Linear(prev_dim, embedding_dim))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        embedding = self.network(x)
        # L2 normalization
        embedding = F.normalize(embedding, p=2, dim=1)
        return embedding


class TwoTowerModel(nn.Module):
    """Two-Tower Recommendation Model"""
    def __init__(self, user_feature_dim, item_feature_dim,
                 embedding_dim=128, hidden_dims=[256, 128]):
        super(TwoTowerModel, self).__init__()

        self.user_tower = TowerNetwork(user_feature_dim, embedding_dim, hidden_dims)
        self.item_tower = TowerNetwork(item_feature_dim, embedding_dim, hidden_dims)

        self.temperature = nn.Parameter(torch.ones(1) * 0.07)

    def forward(self, user_features, item_features):
        """
        Args:
            user_features: (batch, user_dim)
            item_features: (batch, item_dim)
        Returns:
            similarity: (batch,)
        """
        user_emb = self.user_tower(user_features)  # (batch, emb_dim)
        item_emb = self.item_tower(item_features)  # (batch, emb_dim)

        # Cosine similarity (already L2 normalized)
        similarity = (user_emb * item_emb).sum(dim=1)
        return similarity

    def get_user_embedding(self, user_features):
        """Get user embedding for indexing"""
        return self.user_tower(user_features)

    def get_item_embedding(self, item_features):
        """Get item embedding for indexing"""
        return self.item_tower(item_features)


class InfoNCELoss(nn.Module):
    """InfoNCE Loss for contrastive learning"""
    def __init__(self, temperature=0.07):
        super(InfoNCELoss, self).__init__()
        self.temperature = temperature

    def forward(self, user_emb, pos_item_emb, neg_item_embs):
        """
        Args:
            user_emb: (batch, dim)
            pos_item_emb: (batch, dim) - positive items
            neg_item_embs: (batch, n_neg, dim) - negative items
        Returns:
            loss: scalar
        """
        # Positive similarity
        pos_sim = (user_emb * pos_item_emb).sum(dim=1) / self.temperature  # (batch,)

        # Negative similarities
        neg_sim = torch.bmm(neg_item_embs, user_emb.unsqueeze(2)).squeeze(2)  # (batch, n_neg)
        neg_sim = neg_sim / self.temperature

        # InfoNCE loss
        logits = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)  # (batch, 1+n_neg)
        labels = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)

        loss = F.cross_entropy(logits, labels)
        return loss


def train_two_tower(model, train_loader, n_epochs=10, lr=0.001, n_negatives=5):
    """Two-Towerãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    criterion = InfoNCELoss(temperature=0.07)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    for epoch in range(n_epochs):
        model.train()
        total_loss = 0

        for batch in train_loader:
            user_feat = batch['user_features'].to(device)
            pos_item_feat = batch['pos_item_features'].to(device)
            neg_item_feats = batch['neg_item_features'].to(device)  # (batch, n_neg, dim)

            # Get embeddings
            user_emb = model.get_user_embedding(user_feat)
            pos_item_emb = model.get_item_embedding(pos_item_feat)

            # Negative embeddings
            batch_size, n_neg, feat_dim = neg_item_feats.shape
            neg_item_feats_flat = neg_item_feats.view(-1, feat_dim)
            neg_item_embs = model.get_item_embedding(neg_item_feats_flat)
            neg_item_embs = neg_item_embs.view(batch_size, n_neg, -1)

            # Compute loss
            loss = criterion(user_emb, pos_item_emb, neg_item_embs)

            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}")

    return model


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ¢ãƒ‡ãƒ«å®šç¾©
    user_feature_dim = 50   # User features (age, gender, history, etc.)
    item_feature_dim = 100  # Item features (category, price, brand, etc.)

    model = TwoTowerModel(user_feature_dim, item_feature_dim,
                          embedding_dim=128, hidden_dims=[256, 128])

    # ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›
    batch_size = 64
    user_features = torch.randn(batch_size, user_feature_dim)
    item_features = torch.randn(batch_size, item_feature_dim)

    # Forward
    similarity = model(user_features, item_features)
    print(f"Similarity scores: {similarity[:5]}")

    # Get embeddings for indexing
    user_emb = model.get_user_embedding(user_features)
    item_emb = model.get_item_embedding(item_features)
    print(f"User embedding shape: {user_emb.shape}")
    print(f"Item embedding shape: {item_emb.shape}")
</code></pre>

<h3>4.3.4 åŠ¹ç‡çš„ãªæ¨è«–ï¼ˆANNï¼‰</h3>

<p><strong>FAISS (Facebook AI Similarity Search) ã®åˆ©ç”¨ï¼š</strong></p>

<pre><code>import faiss
import numpy as np

def build_item_index(item_embeddings, use_gpu=False):
    """
    Build FAISS index for fast retrieval

    Args:
        item_embeddings: (n_items, embedding_dim) numpy array
        use_gpu: whether to use GPU for indexing
    Returns:
        index: FAISS index
    """
    n_items, dim = item_embeddings.shape

    # Normalize embeddings
    faiss.normalize_L2(item_embeddings)

    # Build index (Inner Product = Cosine Similarity for normalized vectors)
    index = faiss.IndexFlatIP(dim)

    if use_gpu and faiss.get_num_gpus() > 0:
        res = faiss.StandardGpuResources()
        index = faiss.index_cpu_to_gpu(res, 0, index)

    index.add(item_embeddings)

    print(f"Built FAISS index with {index.ntotal} items")
    return index


def retrieve_top_k(user_embedding, item_index, k=10):
    """
    Retrieve top-k items for a user

    Args:
        user_embedding: (embedding_dim,) or (1, embedding_dim)
        item_index: FAISS index
        k: number of items to retrieve
    Returns:
        scores: (k,) similarity scores
        indices: (k,) item indices
    """
    if len(user_embedding.shape) == 1:
        user_embedding = user_embedding.reshape(1, -1)

    # Normalize
    faiss.normalize_L2(user_embedding)

    # Search
    scores, indices = item_index.search(user_embedding, k)

    return scores[0], indices[0]


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ä»®ã®ã‚¢ã‚¤ãƒ†ãƒ åŸ‹ã‚è¾¼ã¿ï¼ˆ100ä¸‡ã‚¢ã‚¤ãƒ†ãƒ ï¼‰
    n_items = 1_000_000
    embedding_dim = 128

    item_embeddings = np.random.randn(n_items, embedding_dim).astype('float32')

    # Build index
    index = build_item_index(item_embeddings, use_gpu=False)

    # User embedding
    user_emb = np.random.randn(embedding_dim).astype('float32')

    # Retrieve top-10
    scores, indices = retrieve_top_k(user_emb, index, k=10)

    print(f"Top-10 item indices: {indices}")
    print(f"Top-10 scores: {scores}")
</code></pre>

<hr>

<h2>4.4 Sequence-basedæ¨è–¦</h2>

<h3>4.4.1 Session-based Recommendation</h3>

<p><strong>å•é¡Œè¨­å®šï¼š</strong></p>
<ul>
<li>ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•å±¥æ­´ï¼ˆsessionï¼‰ã‹ã‚‰æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’äºˆæ¸¬</li>
<li>ä¾‹ï¼šE-commerceï¼ˆé–²è¦§å±¥æ­´ â†’ æ¬¡ã«é–²è¦§/è³¼å…¥ã™ã‚‹å•†å“ï¼‰</li>
<li>åŒ¿åãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ã‚‚é©ç”¨å¯èƒ½ï¼ˆuser IDãªã—ï¼‰</li>
</ul>

<p><strong>ãƒ‡ãƒ¼ã‚¿å½¢å¼ï¼š</strong></p>
<pre><code>Session 1: [item_5, item_12, item_3, item_8] â†’ item_15
Session 2: [item_1, item_6] â†’ item_9
Session 3: [item_12, item_15, item_2] â†’ item_7
</code></pre>

<h3>4.4.2 RNN/GRU for Sequential Recommendation</h3>

<p><strong>GRU4Rec (Session-based Recommendations with RNNs):</strong></p>
<ul>
<li>è«–æ–‡: Hidasi et al. (ICLR 2016)</li>
<li>GRU (Gated Recurrent Unit) ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–</li>
</ul>

<p><strong>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼š</strong></p>
<pre><code>Input: item sequence [i_1, i_2, ..., i_t]
    â†“
Embedding Layer: [e_1, e_2, ..., e_t]
    â†“
GRU Layer: h_t = GRU(e_t, h_{t-1})
    â†“
Output Layer: softmax(W h_t + b) â†’ P(next item)
</code></pre>

<h3>4.4.3 GRU4Recå®Ÿè£…</h3>

<pre><code>import torch
import torch.nn as nn

class GRU4Rec(nn.Module):
    """GRU-based Session Recommendation"""
    def __init__(self, n_items, embedding_dim=100, hidden_dim=100, n_layers=1):
        super(GRU4Rec, self).__init__()

        self.n_items = n_items
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim

        # Embedding layer
        self.item_embedding = nn.Embedding(n_items, embedding_dim, padding_idx=0)

        # GRU layer
        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers,
                          batch_first=True, dropout=0.2 if n_layers > 1 else 0)

        # Output layer
        self.fc = nn.Linear(hidden_dim, n_items)

        # Initialize
        nn.init.xavier_uniform_(self.item_embedding.weight)
        nn.init.xavier_uniform_(self.fc.weight)

    def forward(self, item_seq, lengths=None):
        """
        Args:
            item_seq: (batch, seq_len) - item ID sequence
            lengths: (batch,) - actual sequence lengths
        Returns:
            logits: (batch, n_items) - prediction for next item
        """
        # Embedding
        emb = self.item_embedding(item_seq)  # (batch, seq_len, emb_dim)

        # GRU
        if lengths is not None:
            # Pack sequence for efficiency
            packed = nn.utils.rnn.pack_padded_sequence(
                emb, lengths.cpu(), batch_first=True, enforce_sorted=False
            )
            gru_out, hidden = self.gru(packed)
            gru_out, _ = nn.utils.rnn.pad_packed_sequence(gru_out, batch_first=True)
        else:
            gru_out, hidden = self.gru(emb)  # (batch, seq_len, hidden)

        # Get last hidden state
        if lengths is not None:
            idx = (lengths - 1).view(-1, 1, 1).expand(-1, 1, gru_out.size(2))
            last_hidden = gru_out.gather(1, idx).squeeze(1)  # (batch, hidden)
        else:
            last_hidden = gru_out[:, -1, :]  # (batch, hidden)

        # Output
        logits = self.fc(last_hidden)  # (batch, n_items)
        return logits


def train_gru4rec(model, train_loader, n_epochs=10, lr=0.001):
    """GRU4Recã®è¨“ç·´"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    for epoch in range(n_epochs):
        model.train()
        total_loss = 0

        for batch in train_loader:
            item_seq = batch['item_seq'].to(device)  # (batch, seq_len)
            target = batch['target'].to(device)      # (batch,)
            lengths = batch['lengths'].to(device)    # (batch,)

            # Forward
            logits = model(item_seq, lengths)
            loss = criterion(logits, target)

            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}")

    return model


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    n_items = 10000
    model = GRU4Rec(n_items, embedding_dim=100, hidden_dim=100, n_layers=2)

    # ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›
    batch_size = 32
    seq_len = 10
    item_seq = torch.randint(1, n_items, (batch_size, seq_len))
    lengths = torch.randint(5, seq_len+1, (batch_size,))

    logits = model(item_seq, lengths)
    print(f"Input shape: {item_seq.shape}")
    print(f"Output shape: {logits.shape}")

    # Top-k prediction
    k = 10
    _, top_k_items = torch.topk(logits, k, dim=1)
    print(f"Top-{k} predicted items (first sample): {top_k_items[0]}")
</code></pre>

<h3>4.4.4 Self-Attention for Sequential Recommendation (SASRec)</h3>

<p><strong>Self-Attentionã®åˆ©ç‚¹ï¼š</strong></p>
<ul>
<li>RNN/GRUã®é€æ¬¡å‡¦ç†ã®åˆ¶ç´„ã‚’å…‹æœ</li>
<li>ä¸¦åˆ—è¨ˆç®—å¯èƒ½ã€é•·è·é›¢ä¾å­˜é–¢ä¿‚ã‚’æ‰ãˆã‚„ã™ã„</li>
<li>è«–æ–‡: Kang & McAuley "SASRec" (ICDM 2018)</li>
</ul>

<p><strong>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼š</strong></p>
<pre><code>Input: [i_1, i_2, ..., i_t]
    â†“
Embedding + Positional Encoding
    â†“
Self-Attention Blocks (Ã—L layers)
â”œâ”€ Multi-Head Self-Attention
â”œâ”€ Feed-Forward Network
â””â”€ Layer Normalization + Residual
    â†“
Output Layer: Predict i_{t+1}
</code></pre>

<h3>4.4.5 SASRecå®Ÿè£…</h3>

<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class SelfAttention(nn.Module):
    """Multi-Head Self-Attention"""
    def __init__(self, hidden_dim, n_heads, dropout=0.1):
        super(SelfAttention, self).__init__()

        assert hidden_dim % n_heads == 0

        self.hidden_dim = hidden_dim
        self.n_heads = n_heads
        self.head_dim = hidden_dim // n_heads

        self.q_linear = nn.Linear(hidden_dim, hidden_dim)
        self.k_linear = nn.Linear(hidden_dim, hidden_dim)
        self.v_linear = nn.Linear(hidden_dim, hidden_dim)

        self.out_linear = nn.Linear(hidden_dim, hidden_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        """
        Args:
            x: (batch, seq_len, hidden_dim)
            mask: (batch, seq_len, seq_len) - causal mask
        Returns:
            out: (batch, seq_len, hidden_dim)
        """
        batch_size, seq_len, _ = x.shape

        # Linear projections
        Q = self.q_linear(x)  # (batch, seq_len, hidden)
        K = self.k_linear(x)
        V = self.v_linear(x)

        # Reshape for multi-head
        Q = Q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        # (batch, n_heads, seq_len, head_dim)

        # Attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        # (batch, n_heads, seq_len, seq_len)

        # Apply mask (causal)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # Softmax
        attn = F.softmax(scores, dim=-1)
        attn = self.dropout(attn)

        # Apply attention to values
        out = torch.matmul(attn, V)  # (batch, n_heads, seq_len, head_dim)

        # Reshape back
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim)

        # Final linear
        out = self.out_linear(out)
        return out


class FeedForward(nn.Module):
    """Position-wise Feed-Forward Network"""
    def __init__(self, hidden_dim, ff_dim, dropout=0.1):
        super(FeedForward, self).__init__()

        self.linear1 = nn.Linear(hidden_dim, ff_dim)
        self.linear2 = nn.Linear(ff_dim, hidden_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.linear2(self.dropout(F.relu(self.linear1(x))))


class SASRecBlock(nn.Module):
    """Single SASRec Transformer Block"""
    def __init__(self, hidden_dim, n_heads, ff_dim, dropout=0.1):
        super(SASRecBlock, self).__init__()

        self.attn = SelfAttention(hidden_dim, n_heads, dropout)
        self.ff = FeedForward(hidden_dim, ff_dim, dropout)

        self.ln1 = nn.LayerNorm(hidden_dim)
        self.ln2 = nn.LayerNorm(hidden_dim)

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask):
        # Self-attention + residual
        attn_out = self.attn(self.ln1(x), mask)
        x = x + self.dropout(attn_out)

        # Feed-forward + residual
        ff_out = self.ff(self.ln2(x))
        x = x + self.dropout(ff_out)

        return x


class SASRec(nn.Module):
    """Self-Attentive Sequential Recommendation"""
    def __init__(self, n_items, max_len=50, hidden_dim=100,
                 n_heads=2, n_blocks=2, dropout=0.1):
        super(SASRec, self).__init__()

        self.n_items = n_items
        self.max_len = max_len

        # Embeddings
        self.item_embedding = nn.Embedding(n_items + 1, hidden_dim, padding_idx=0)
        self.pos_embedding = nn.Embedding(max_len, hidden_dim)

        # Transformer blocks
        self.blocks = nn.ModuleList([
            SASRecBlock(hidden_dim, n_heads, hidden_dim * 4, dropout)
            for _ in range(n_blocks)
        ])

        self.ln = nn.LayerNorm(hidden_dim)
        self.dropout = nn.Dropout(dropout)

        # Initialize
        nn.init.xavier_uniform_(self.item_embedding.weight)
        nn.init.xavier_uniform_(self.pos_embedding.weight)

    def forward(self, item_seq):
        """
        Args:
            item_seq: (batch, seq_len)
        Returns:
            logits: (batch, seq_len, n_items)
        """
        batch_size, seq_len = item_seq.shape

        # Item embeddings
        item_emb = self.item_embedding(item_seq)  # (batch, seq_len, hidden)

        # Positional embeddings
        positions = torch.arange(seq_len, device=item_seq.device).unsqueeze(0)
        pos_emb = self.pos_embedding(positions)  # (1, seq_len, hidden)

        # Combine
        x = item_emb + pos_emb
        x = self.dropout(x)

        # Causal mask (prevent looking at future items)
        mask = torch.tril(torch.ones((seq_len, seq_len), device=item_seq.device))
        mask = mask.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, seq_len)

        # Transformer blocks
        for block in self.blocks:
            x = block(x, mask)

        x = self.ln(x)

        # Predict next item (using item embeddings as output weights)
        logits = torch.matmul(x, self.item_embedding.weight.T)  # (batch, seq_len, n_items+1)

        return logits[:, :, 1:]  # Remove padding index


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    n_items = 10000
    max_len = 50

    model = SASRec(n_items, max_len=max_len, hidden_dim=128,
                   n_heads=4, n_blocks=2, dropout=0.2)

    # ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›
    batch_size = 16
    seq_len = 20
    item_seq = torch.randint(1, n_items, (batch_size, seq_len))

    logits = model(item_seq)
    print(f"Input shape: {item_seq.shape}")
    print(f"Output shape: {logits.shape}")

    # Predict next item (using last position)
    last_logits = logits[:, -1, :]  # (batch, n_items)
    _, top_k = torch.topk(last_logits, 10, dim=1)
    print(f"Top-10 predictions: {top_k[0]}")
</code></pre>

<h3>4.4.6 BERT4Rec</h3>

<p><strong>BERT for Sequential Recommendation:</strong></p>
<ul>
<li>è«–æ–‡: Sun et al. "BERT4Rec" (CIKM 2019)</li>
<li>åŒæ–¹å‘Transformerï¼ˆéå»+æœªæ¥ã®æ–‡è„ˆã‚’åˆ©ç”¨ï¼‰</li>
<li>Masked Item Prediction (MLMé¢¨)</li>
</ul>

<p><strong>è¨“ç·´æ–¹æ³•ï¼š</strong></p>
<ul>
<li>å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ä¸€éƒ¨ã‚’ãƒã‚¹ã‚¯ï¼ˆ[MASK]ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰</li>
<li>ãƒã‚¹ã‚¯ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’äºˆæ¸¬</li>
<li>åŒæ–¹å‘ã®æ–‡è„ˆã‚’åˆ©ç”¨å¯èƒ½</li>
</ul>

<hr>

<h2>4.5 å®Ÿè·µçš„ãªæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h2>

<h3>4.5.1 End-to-Endãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h3>

<p><strong>æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®2æ®µéšã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼š</strong></p>

<pre><code>Stage 1: Candidate Generationï¼ˆå€™è£œç”Ÿæˆï¼‰
â”œâ”€ Input: User context
â”œâ”€ Method: Two-Tower, Matrix Factorization, Graph-based
â”œâ”€ Output: æ•°ç™¾ã€œæ•°åƒã®å€™è£œã‚¢ã‚¤ãƒ†ãƒ 
â””â”€ ç›®çš„: Recallæœ€å¤§åŒ–ã€é«˜é€Ÿ

Stage 2: Rankingï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰
â”œâ”€ Input: User + å€™è£œã‚¢ã‚¤ãƒ†ãƒ 
â”œâ”€ Method: DeepFM, Wide&Deep, DNN
â”œâ”€ Output: Top-Kæ¨è–¦ãƒªã‚¹ãƒˆ
â””â”€ ç›®çš„: Precisionæœ€å¤§åŒ–ã€ç²¾åº¦é‡è¦–
</code></pre>

<h3>4.5.2 Candidate Generationä¾‹</h3>

<pre><code>import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class CandidateGenerator:
    """å€™è£œç”Ÿæˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«"""
    def __init__(self, user_embeddings, item_embeddings):
        """
        Args:
            user_embeddings: (n_users, dim)
            item_embeddings: (n_items, dim)
        """
        self.user_embeddings = user_embeddings
        self.item_embeddings = item_embeddings

        # Build FAISS index for fast retrieval
        import faiss
        dim = item_embeddings.shape[1]

        # Normalize for cosine similarity
        faiss.normalize_L2(item_embeddings)

        self.index = faiss.IndexFlatIP(dim)
        self.index.add(item_embeddings.astype('float32'))

    def generate_candidates(self, user_id, n_candidates=500, exclude_items=None):
        """
        Generate candidate items for a user

        Args:
            user_id: int
            n_candidates: number of candidates to retrieve
            exclude_items: set of item IDs to exclude
        Returns:
            candidate_items: list of item IDs
            scores: list of similarity scores
        """
        # Get user embedding
        user_emb = self.user_embeddings[user_id:user_id+1].astype('float32')

        # Normalize
        import faiss
        faiss.normalize_L2(user_emb)

        # Retrieve top candidates
        scores, indices = self.index.search(user_emb, n_candidates * 2)
        scores = scores[0]
        indices = indices[0]

        # Filter out excluded items
        if exclude_items is not None:
            mask = ~np.isin(indices, list(exclude_items))
            indices = indices[mask]
            scores = scores[mask]

        # Return top n_candidates
        return indices[:n_candidates].tolist(), scores[:n_candidates].tolist()


class Ranker:
    """ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆDeepFMãªã©ï¼‰"""
    def __init__(self, ranking_model):
        self.model = ranking_model

    def rank(self, user_features, candidate_item_features, top_k=10):
        """
        Rank candidate items

        Args:
            user_features: user feature vector
            candidate_item_features: (n_candidates, item_dim)
            top_k: number of items to return
        Returns:
            ranked_items: list of item indices
            scores: list of ranking scores
        """
        import torch

        n_candidates = len(candidate_item_features)

        # Replicate user features
        user_feat_batch = np.tile(user_features, (n_candidates, 1))

        # Combine features
        features = np.concatenate([user_feat_batch, candidate_item_features], axis=1)
        features_tensor = torch.FloatTensor(features)

        # Predict scores
        with torch.no_grad():
            scores = self.model(features_tensor).numpy()

        # Sort by score
        sorted_indices = np.argsort(scores)[::-1]

        return sorted_indices[:top_k].tolist(), scores[sorted_indices[:top_k]].tolist()


class RecommendationPipeline:
    """æ¨è–¦ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆå€™è£œç”Ÿæˆ + ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰"""
    def __init__(self, candidate_generator, ranker):
        self.candidate_generator = candidate_generator
        self.ranker = ranker

    def recommend(self, user_id, user_features, item_features_db,
                  n_candidates=500, top_k=10, exclude_items=None):
        """
        Generate recommendations for a user

        Args:
            user_id: int
            user_features: user feature vector
            item_features_db: dict {item_id: features}
            n_candidates: number of candidates
            top_k: number of final recommendations
            exclude_items: set of item IDs to exclude
        Returns:
            recommendations: list of (item_id, score) tuples
        """
        # Stage 1: Candidate Generation
        candidate_ids, _ = self.candidate_generator.generate_candidates(
            user_id, n_candidates, exclude_items
        )

        # Get item features for candidates
        candidate_features = np.array([item_features_db[iid] for iid in candidate_ids])

        # Stage 2: Ranking
        ranked_indices, scores = self.ranker.rank(user_features, candidate_features, top_k)

        # Map back to item IDs
        recommendations = [(candidate_ids[idx], scores[i])
                          for i, idx in enumerate(ranked_indices)]

        return recommendations


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ä»®ã®ãƒ‡ãƒ¼ã‚¿
    n_users = 10000
    n_items = 50000
    emb_dim = 128

    user_embeddings = np.random.randn(n_users, emb_dim).astype('float32')
    item_embeddings = np.random.randn(n_items, emb_dim).astype('float32')

    # Candidate Generator
    candidate_gen = CandidateGenerator(user_embeddings, item_embeddings)

    # ä»®ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ï¼ˆå®Ÿéš›ã«ã¯DeepFMãªã©ï¼‰
    class DummyRanker:
        def __call__(self, x):
            return torch.rand(len(x))

    ranker = Ranker(DummyRanker())

    # Pipeline
    pipeline = RecommendationPipeline(candidate_gen, ranker)

    # Recommend
    user_id = 42
    user_feat = np.random.randn(50)
    item_feat_db = {i: np.random.randn(100) for i in range(n_items)}

    recommendations = pipeline.recommend(
        user_id, user_feat, item_feat_db,
        n_candidates=500, top_k=10, exclude_items={1, 5, 10}
    )

    print("Top-10 Recommendations:")
    for item_id, score in recommendations:
        print(f"  Item {item_id}: {score:.4f}")
</code></pre>

<h3>4.5.3 A/Bãƒ†ã‚¹ãƒˆ</h3>

<p><strong>å®Ÿé¨“è¨­è¨ˆï¼š</strong></p>
<ul>
<li>Control Group: æ—¢å­˜ã®æ¨è–¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </li>
<li>Treatment Group: æ–°ã—ã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆDeepFMã€Two-Towerãªã©ï¼‰</li>
<li>è©•ä¾¡æŒ‡æ¨™ï¼šCTRã€Conversion Rateã€Revenueã€User Engagement</li>
</ul>

<p><strong>çµ±è¨ˆçš„æœ‰æ„æ€§ã®æ¤œå®šï¼š</strong></p>
<ul>
<li>å¸°ç„¡ä»®èª¬ï¼š$H_0$: æ–°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®åŠ¹æœãªã—</li>
<li>å¯¾ç«‹ä»®èª¬ï¼š$H_1$: æ–°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®åŠ¹æœã‚ã‚Š</li>
<li>æœ‰æ„æ°´æº–ï¼š$\alpha = 0.05$ï¼ˆ5%ï¼‰</li>
<li>æ¤œå®šæ–¹æ³•ï¼štæ¤œå®šã€Welch's t-testã€Mann-Whitney Uæ¤œå®š</li>
</ul>

<h3>4.5.4 Production Deployment</h3>

<p><strong>ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ï¼š</strong></p>
<ul>
<li>ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼š< 100msï¼ˆæ¨è«–æ™‚é–“ï¼‰</li>
<li>ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼š> 10,000 QPSï¼ˆQueries Per Secondï¼‰</li>
<li>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ï¼šæ°´å¹³ã‚¹ã‚±ãƒ¼ãƒ«å¯èƒ½</li>
</ul>

<p><strong>æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ä¾‹ï¼š</strong></p>
<ul>
<li>ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒ–ï¼šTensorFlow Servingã€TorchServeã€NVIDIA Triton</li>
<li>ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼šRedisï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼åŸ‹ã‚è¾¼ã¿ã€ã‚¢ã‚¤ãƒ†ãƒ åŸ‹ã‚è¾¼ã¿ï¼‰</li>
<li>ANNæ¤œç´¢ï¼šFAISSã€ScaNNã€Milvus</li>
<li>ãƒ­ã‚°åé›†ï¼šKafkaã€Fluentd</li>
<li>ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ï¼šPrometheusã€Grafana</li>
</ul>

<hr>

<h2>ã¾ã¨ã‚</h2>

<p>æœ¬ç« ã§å­¦ã‚“ã ã“ã¨ï¼š</p>

<ol>
<li><strong>Neural Collaborative Filtering (NCF):</strong>
<ul>
<li>GMFã€MLPã€NeuMFã®3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</li>
<li>éç·šå½¢ãªç›¸äº’ä½œç”¨ã‚’MLPã§è¡¨ç¾</li>
<li>MovieLensã§HR@10 0.726é”æˆ</li>
</ul>
</li>

<li><strong>Factorization Machines:</strong>
<ul>
<li>2æ¬¡ç›¸äº’ä½œç”¨ã‚’$O(kn)$ã§è¨ˆç®—</li>
<li>FFMã§ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åˆ¥åŸ‹ã‚è¾¼ã¿</li>
<li>DeepFMã§FM+DNNã‚’èåˆ</li>
</ul>
</li>

<li><strong>Two-Tower Models:</strong>
<ul>
<li>User Towerã¨Item Towerã‚’åˆ†é›¢</li>
<li>FAISS/ScaNNã§é«˜é€Ÿæ¨è«–ï¼ˆ$O(\log N)$ï¼‰</li>
<li>InfoNCE Lossã§å¯¾ç…§å­¦ç¿’</li>
</ul>
</li>

<li><strong>Sequence-basedæ¨è–¦:</strong>
<ul>
<li>GRU4Rec: ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹æ¨è–¦</li>
<li>SASRec: Self-Attentionã§é•·è·é›¢ä¾å­˜</li>
<li>BERT4Rec: åŒæ–¹å‘Transformerã§ãƒã‚¹ã‚¯äºˆæ¸¬</li>
</ul>
</li>

<li><strong>å®Ÿè·µçš„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³:</strong>
<ul>
<li>å€™è£œç”Ÿæˆï¼ˆRecallé‡è¦–ï¼‰+ ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆPrecisioné‡è¦–ï¼‰</li>
<li>A/Bãƒ†ã‚¹ãƒˆã§åŠ¹æœæ¤œè¨¼</li>
<li>Production deploymentã®æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯</li>
</ul>
</li>
</ol>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<p><strong>å•1:</strong> NeuMFãŒMatrix Factorizationã‚ˆã‚Šé«˜æ€§èƒ½ãªç†ç”±ã‚’ã€ç·šå½¢æ€§ã¨éç·šå½¢æ€§ã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚</p>

<p><strong>å•2:</strong> Factorization Machinesã®2æ¬¡ç›¸äº’ä½œç”¨ã®è¨ˆç®—é‡ãŒ$O(n^2)$ã‹ã‚‰$O(kn)$ã«å‰Šæ¸›ã•ã‚Œã‚‹ä»•çµ„ã¿ã‚’æ•°å¼ã‚’ç”¨ã„ã¦ç¤ºã›ã€‚</p>

<p><strong>å•3:</strong> Two-Tower Modelã§ã€User Towerã¨Item Towerã‚’åˆ†é›¢ã™ã‚‹ãƒ¡ãƒªãƒƒãƒˆã¨ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã‚’3ã¤ãšã¤æŒ™ã’ã‚ˆã€‚</p>

<p><strong>å•4:</strong> SASRecã¨GRU4Recã®é•ã„ã‚’ã€ä¸¦åˆ—åŒ–å¯èƒ½æ€§ã¨é•·è·é›¢ä¾å­˜ã®æ‰ãˆæ–¹ã®2ã¤ã®è¦³ç‚¹ã‹ã‚‰è«–ã˜ã‚ˆã€‚</p>

<p><strong>å•5:</strong> æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®Candidate Generationæ®µéšã§Recall@500 = 0.8ã€Rankingæ®µéšã§Precision@10 = 0.3ã®å ´åˆã€æœ€çµ‚çš„ãªTop-10æ¨è–¦ã®æœŸå¾…çš„ä¸­æ•°ã‚’è¨ˆç®—ã›ã‚ˆã€‚</p>

<p><strong>å•6:</strong> å¤§è¦æ¨¡æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ï¼ˆ1å„„ãƒ¦ãƒ¼ã‚¶ãƒ¼ Ã— 1000ä¸‡ã‚¢ã‚¤ãƒ†ãƒ ï¼‰ã§ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· < 100msã‚’é”æˆã™ã‚‹ãŸã‚ã®æŠ€è¡“çš„å·¥å¤«ã‚’5ã¤ææ¡ˆã›ã‚ˆã€‚</p>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>He, X. et al. "Neural Collaborative Filtering." <em>Proceedings of WWW</em> (2017).</li>
<li>Rendle, S. "Factorization Machines." <em>Proceedings of ICDM</em> (2010).</li>
<li>Juan, Y. et al. "Field-aware Factorization Machines for CTR Prediction." <em>RecSys</em> (2016).</li>
<li>Guo, H. et al. "DeepFM: A Factorization-Machine based Neural Network for CTR Prediction." <em>IJCAI</em> (2017).</li>
<li>Yi, X. et al. "Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations." <em>RecSys</em> (2019). [Two-Tower]</li>
<li>Hidasi, B. et al. "Session-based Recommendations with Recurrent Neural Networks." <em>ICLR</em> (2016).</li>
<li>Kang, W. & McAuley, J. "Self-Attentive Sequential Recommendation." <em>ICDM</em> (2018).</li>
<li>Sun, F. et al. "BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer." <em>CIKM</em> (2019).</li>
<li>Covington, P. et al. "Deep Neural Networks for YouTube Recommendations." <em>RecSys</em> (2016).</li>
<li>Cheng, H. et al. "Wide & Deep Learning for Recommender Systems." <em>DLRS Workshop</em> (2016).</li>
</ol>

<hr>

<div class="navigation">
    <a href="chapter3-hybrid-methods.html" class="nav-button">â† å‰ã®ç« </a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter5-advanced-topics.html" class="nav-button">æ¬¡ã®ç«  â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
