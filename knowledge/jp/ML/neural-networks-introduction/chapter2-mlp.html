<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬2ç« ï¼šå¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ã¨èª¤å·®é€†ä¼æ’­æ³• - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬2ç« ï¼šå¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ã¨èª¤å·®é€†ä¼æ’­æ³•</h1>
            <p class="subtitle">æ·±å±¤å­¦ç¿’ã®æ ¸å¿ƒã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  - Backpropagation</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 30-35åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´šã€œä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 15å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ï¼ˆMLPï¼‰ã®æ§‹é€ ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… èª¤å·®é€†ä¼æ’­æ³•ï¼ˆBackpropagationï¼‰ã®ä»•çµ„ã¿ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>âœ… å‹¾é…é™ä¸‹æ³•ã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… é€£é–å¾‹ï¼ˆChain Ruleï¼‰ã®æ•°å­¦çš„åŸºç¤ã‚’å­¦ã¶</li>
<li>âœ… NumPyã§MLPã‚’å®Œå…¨å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… XORå•é¡Œã‚’å®Ÿéš›ã«è§£æ±ºã§ãã‚‹</li>
</ul>

<hr>

<h2>2.1 å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ï¼ˆMLPï¼‰ã®æ§‹é€ </h2>

<h3>MLPã¨ã¯</h3>

<p><strong>å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ï¼ˆMultilayer Perceptron, MLPï¼‰</strong>ã¯ã€è¤‡æ•°å±¤ã®ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ã‚’çµ„ã¿åˆã‚ã›ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚</p>

<div class="mermaid">
graph LR
    x1[å…¥åŠ›å±¤<br/>x1] --> h1[éš ã‚Œå±¤<br/>h1]
    x2[å…¥åŠ›å±¤<br/>x2] --> h1
    x1 --> h2[éš ã‚Œå±¤<br/>h2]
    x2 --> h2
    h1 --> y1[å‡ºåŠ›å±¤<br/>y1]
    h2 --> y1

    style x1 fill:#e3f2fd
    style x2 fill:#e3f2fd
    style h1 fill:#fff3e0
    style h2 fill:#fff3e0
    style y1 fill:#e8f5e9
</div>

<h3>å±¤ã®ç¨®é¡</h3>

<table>
<thead>
<tr>
<th>å±¤ã®ç¨®é¡</th>
<th>å½¹å‰²</th>
<th>èª¬æ˜</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å…¥åŠ›å±¤</strong></td>
<td>Input Layer</td>
<td>ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚‹å±¤ï¼ˆå­¦ç¿’å¯¾è±¡ã§ã¯ãªã„ï¼‰</td>
</tr>
<tr>
<td><strong>éš ã‚Œå±¤</strong></td>
<td>Hidden Layer</td>
<td>ç‰¹å¾´æŠ½å‡ºã‚’è¡Œã†å±¤ï¼ˆå­¦ç¿’å¯¾è±¡ï¼‰</td>
</tr>
<tr>
<td><strong>å‡ºåŠ›å±¤</strong></td>
<td>Output Layer</td>
<td>æœ€çµ‚çµæœã‚’å‡ºåŠ›ã™ã‚‹å±¤ï¼ˆå­¦ç¿’å¯¾è±¡ï¼‰</td>
</tr>
</tbody>
</table>

<h3>2å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ•°å¼</h3>

<p>å…¥åŠ› $\mathbf{x} = [x_1, x_2]^T$ ã‹ã‚‰å‡ºåŠ› $y$ ã¾ã§ã®è¨ˆç®—ï¼š</p>

<p><strong>ç¬¬1å±¤ï¼ˆå…¥åŠ› â†’ éš ã‚Œå±¤ï¼‰</strong>ï¼š</p>
<p>$$
\mathbf{h} = \sigma(\mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)})
$$</p>

<p><strong>ç¬¬2å±¤ï¼ˆéš ã‚Œå±¤ â†’ å‡ºåŠ›ï¼‰</strong>ï¼š</p>
<p>$$
y = \sigma(\mathbf{W}^{(2)} \mathbf{h} + b^{(2)})
$$</p>

<p>ã“ã“ã§ã€$\sigma$ã¯æ´»æ€§åŒ–é–¢æ•°ï¼ˆã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ãªã©ï¼‰ã§ã™ã€‚</p>

<h3>Pythonå®Ÿè£…ã®åŸºæœ¬æ§‹é€ </h3>

<pre><code class="language-python">import numpy as np

def sigmoid(x):
    """ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°"""
    return 1 / (1 + np.exp(-x))

class TwoLayerNet:
    """2å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""

    def __init__(self, input_size, hidden_size, output_size):
        """
        Args:
            input_size: å…¥åŠ›å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°
            hidden_size: éš ã‚Œå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°
            output_size: å‡ºåŠ›å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°
        """
        # é‡ã¿ã®åˆæœŸåŒ–ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ï¼‰
        self.W1 = np.random.randn(input_size, hidden_size)
        self.b1 = np.zeros(hidden_size)

        self.W2 = np.random.randn(hidden_size, output_size)
        self.b2 = np.zeros(output_size)

    def forward(self, x):
        """
        é †ä¼æ’­ï¼ˆForward Propagationï¼‰

        Args:
            x: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ (n_samples, input_size)

        Returns:
            å‡ºåŠ› (n_samples, output_size)
        """
        # ç¬¬1å±¤
        self.z1 = np.dot(x, self.W1) + self.b1
        self.a1 = sigmoid(self.z1)

        # ç¬¬2å±¤
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = sigmoid(self.z2)

        return self.a2

# ãƒ†ã‚¹ãƒˆ
net = TwoLayerNet(input_size=2, hidden_size=3, output_size=1)
x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
output = net.forward(x)

print("=== åˆæœŸåŒ–ç›´å¾Œã®å‡ºåŠ› ===")
print(output)
</code></pre>

<hr>

<h2>2.2 æå¤±é–¢æ•°ï¼ˆLoss Functionï¼‰</h2>

<h3>æå¤±é–¢æ•°ã¨ã¯</h3>

<p><strong>æå¤±é–¢æ•°ï¼ˆLoss Functionï¼‰</strong>ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®äºˆæ¸¬å€¤ã¨æ­£è§£å€¤ã®å·®ã‚’æ•°å€¤åŒ–ã—ã¾ã™ã€‚å­¦ç¿’ã®ç›®çš„ã¯ã€ã“ã®æå¤±ã‚’<strong>æœ€å°åŒ–</strong>ã™ã‚‹ã“ã¨ã§ã™ã€‚</p>

<h3>å¹³å‡äºŒä¹—èª¤å·®ï¼ˆMSEï¼‰</h3>

<p>å›å¸°å•é¡Œã§ã‚ˆãä½¿ã‚ã‚Œã‚‹æå¤±é–¢æ•°ï¼š</p>

<p>$$
L = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$</p>

<p>ã“ã“ã§ã€$y_i$ã¯æ­£è§£å€¤ã€$\hat{y}_i$ã¯äºˆæ¸¬å€¤ã§ã™ã€‚</p>

<pre><code class="language-python">def mean_squared_error(y_true, y_pred):
    """
    å¹³å‡äºŒä¹—èª¤å·®ï¼ˆMean Squared Error, MSEï¼‰

    Args:
        y_true: æ­£è§£ãƒ©ãƒ™ãƒ« (n_samples,)
        y_pred: äºˆæ¸¬å€¤ (n_samples,)

    Returns:
        MSEå€¤
    """
    return np.mean((y_true - y_pred) ** 2)

# ä¾‹
y_true = np.array([0, 1, 1, 0])
y_pred = np.array([0.1, 0.9, 0.8, 0.2])
loss = mean_squared_error(y_true, y_pred)
print(f"MSE: {loss:.4f}")  # 0.0125
</code></pre>

<h3>äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼èª¤å·®ï¼ˆCross-Entropyï¼‰</h3>

<p>åˆ†é¡å•é¡Œã§ä½¿ã‚ã‚Œã‚‹æå¤±é–¢æ•°ï¼š</p>

<p>$$
L = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i) \right]
$$</p>

<pre><code class="language-python">def binary_cross_entropy(y_true, y_pred):
    """
    äºŒå€¤äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆBinary Cross-Entropyï¼‰

    Args:
        y_true: æ­£è§£ãƒ©ãƒ™ãƒ« (n_samples,)
        y_pred: äºˆæ¸¬ç¢ºç‡ (n_samples,)

    Returns:
        BCEå€¤
    """
    # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã€å°ã•ãªå€¤ã§ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)

    return -np.mean(y_true * np.log(y_pred) +
                    (1 - y_true) * np.log(1 - y_pred))

# ä¾‹
loss_ce = binary_cross_entropy(y_true, y_pred)
print(f"Cross-Entropy: {loss_ce:.4f}")  # 0.1625
</code></pre>

<hr>

<h2>2.3 å‹¾é…é™ä¸‹æ³•ï¼ˆGradient Descentï¼‰</h2>

<h3>åŸºæœ¬çš„ãªã‚¢ã‚¤ãƒ‡ã‚¢</h3>

<p><strong>å‹¾é…é™ä¸‹æ³•</strong>ã¯ã€æå¤±é–¢æ•°ã‚’æœ€å°åŒ–ã™ã‚‹ãŸã‚ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æå¤±é–¢æ•°ã®å‹¾é…ï¼ˆå¾®åˆ†ï¼‰ã®<strong>é€†æ–¹å‘</strong>ã«å°‘ã—ãšã¤æ›´æ–°ã—ã¾ã™ã€‚</p>

<div class="mermaid">
graph TD
    A[åˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿] --> B[æå¤±ã‚’è¨ˆç®—]
    B --> C[å‹¾é…ã‚’è¨ˆç®—]
    C --> D[ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°]
    D --> E{åæŸï¼Ÿ}
    E -->|No| B
    E -->|Yes| F[å­¦ç¿’å®Œäº†]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style F fill:#c8e6c9
</div>

<h3>æ›´æ–°å¼</h3>

<p>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $w$ ã®æ›´æ–°ï¼š</p>

<p>$$
w \leftarrow w - \eta \frac{\partial L}{\partial w}
$$</p>

<p>ã“ã“ã§ã€$\eta$ã¯<strong>å­¦ç¿’ç‡ï¼ˆLearning Rateï¼‰</strong>ã§ã™ã€‚</p>

<h3>Pythonå®Ÿè£…</h3>

<pre><code class="language-python">def gradient_descent_demo():
    """å‹¾é…é™ä¸‹æ³•ã®ãƒ‡ãƒ¢"""

    # ç°¡å˜ãªé–¢æ•°: f(x) = x^2
    def f(x):
        return x ** 2

    # å°é–¢æ•°: f'(x) = 2x
    def df(x):
        return 2 * x

    # åˆæœŸå€¤ã¨å­¦ç¿’ç‡
    x = 10.0
    learning_rate = 0.1
    n_iterations = 20

    print("=== å‹¾é…é™ä¸‹æ³•ã®ãƒ‡ãƒ¢ ===")
    print(f"ç›®æ¨™: f(x) = x^2 ã®æœ€å°å€¤ã‚’è¦‹ã¤ã‘ã‚‹")
    print(f"åˆæœŸå€¤: x = {x}")
    print()

    for i in range(n_iterations):
        grad = df(x)
        x = x - learning_rate * grad

        if i % 5 == 0:
            print(f"Iteration {i:2d}: x = {x:8.4f}, f(x) = {f(x):8.4f}, grad = {grad:8.4f}")

    print()
    print(f"æœ€çµ‚çµæœ: x = {x:.4f}, f(x) = {f(x):.4f}")
    print(f"ç†è«–å€¤: x = 0.0000, f(x) = 0.0000")

gradient_descent_demo()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== å‹¾é…é™ä¸‹æ³•ã®ãƒ‡ãƒ¢ ===
ç›®æ¨™: f(x) = x^2 ã®æœ€å°å€¤ã‚’è¦‹ã¤ã‘ã‚‹
åˆæœŸå€¤: x = 10.0

Iteration  0: x =   8.0000, f(x) =  64.0000, grad =  20.0000
Iteration  5: x =   2.6214, f(x) =   6.8718, grad =   5.2429
Iteration 10: x =   0.8590, f(x) =   0.7379, grad =   1.7179
Iteration 15: x =   0.2815, f(x) =   0.0792, grad =   0.5630

æœ€çµ‚çµæœ: x = 0.0922, f(x) = 0.0085
ç†è«–å€¤: x = 0.0000, f(x) = 0.0000
</code></pre>

<h3>å­¦ç¿’ç‡ã®å½±éŸ¿</h3>

<pre><code class="language-python">def compare_learning_rates():
    """å­¦ç¿’ç‡ã®é•ã„ã‚’æ¯”è¼ƒ"""
    def f(x):
        return x ** 2

    def df(x):
        return 2 * x

    learning_rates = [0.01, 0.1, 0.5, 0.9]
    x_init = 10.0
    n_iterations = 10

    print("=== å­¦ç¿’ç‡ã®æ¯”è¼ƒ ===")
    for lr in learning_rates:
        x = x_init
        for _ in range(n_iterations):
            x = x - lr * df(x)

        print(f"å­¦ç¿’ç‡ Î·={lr:.2f} â†’ æœ€çµ‚å€¤ x={x:8.4f}, f(x)={f(x):8.4f}")

compare_learning_rates()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== å­¦ç¿’ç‡ã®æ¯”è¼ƒ ===
å­¦ç¿’ç‡ Î·=0.01 â†’ æœ€çµ‚å€¤ x=   8.1707, f(x)=  66.7604
å­¦ç¿’ç‡ Î·=0.10 â†’ æœ€çµ‚å€¤ x=   0.0922, f(x)=   0.0085
å­¦ç¿’ç‡ Î·=0.50 â†’ æœ€çµ‚å€¤ x=   0.0098, f(x)=   0.0001
å­¦ç¿’ç‡ Î·=0.90 â†’ æœ€çµ‚å€¤ x= -10.0000, f(x)= 100.0000ï¼ˆç™ºæ•£ï¼ï¼‰
</code></pre>

<blockquote>
<p><strong>é‡è¦</strong>: å­¦ç¿’ç‡ãŒå¤§ãã™ãã‚‹ã¨ç™ºæ•£ã€å°ã•ã™ãã‚‹ã¨åæŸãŒé…ã„ï¼</p>
</blockquote>

<hr>

<h2>2.4 èª¤å·®é€†ä¼æ’­æ³•ï¼ˆBackpropagationï¼‰</h2>

<h3>ãªãœé€†ä¼æ’­ãŒå¿…è¦ã‹</h3>

<p>å¤šå±¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã¯ã€å„å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚<strong>èª¤å·®é€†ä¼æ’­æ³•</strong>ã¯ã€å‡ºåŠ›å±¤ã‹ã‚‰å…¥åŠ›å±¤ã«å‘ã‹ã£ã¦å‹¾é…ã‚’åŠ¹ç‡çš„ã«è¨ˆç®—ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<h3>é€£é–å¾‹ï¼ˆChain Ruleï¼‰</h3>

<p>åˆæˆé–¢æ•°ã®å¾®åˆ†ï¼š</p>

<p>$$
\frac{\partial L}{\partial w} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z} \cdot \frac{\partial z}{\partial w}
$$</p>

<p>ã“ã‚ŒãŒèª¤å·®é€†ä¼æ’­ã®æ•°å­¦çš„åŸºç¤ã§ã™ã€‚</p>

<h3>2å±¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é€†ä¼æ’­</h3>

<p>å‰å‘ãè¨ˆç®—ï¼š</p>
<p>$$
\begin{align}
z^{(1)} &= W^{(1)} x + b^{(1)} \\
a^{(1)} &= \sigma(z^{(1)}) \\
z^{(2)} &= W^{(2)} a^{(1)} + b^{(2)} \\
y &= \sigma(z^{(2)}) \\
L &= \frac{1}{2}(y - t)^2
\end{align}
$$</p>

<p>é€†å‘ãè¨ˆç®—ï¼ˆå‹¾é…ï¼‰ï¼š</p>
<p>$$
\begin{align}
\frac{\partial L}{\partial y} &= y - t \\
\frac{\partial L}{\partial z^{(2)}} &= \frac{\partial L}{\partial y} \cdot \sigma'(z^{(2)}) \\
\frac{\partial L}{\partial W^{(2)}} &= \frac{\partial L}{\partial z^{(2)}} \cdot (a^{(1)})^T \\
\frac{\partial L}{\partial b^{(2)}} &= \frac{\partial L}{\partial z^{(2)}} \\
\frac{\partial L}{\partial a^{(1)}} &= (W^{(2)})^T \cdot \frac{\partial L}{\partial z^{(2)}} \\
\frac{\partial L}{\partial z^{(1)}} &= \frac{\partial L}{\partial a^{(1)}} \cdot \sigma'(z^{(1)}) \\
\frac{\partial L}{\partial W^{(1)}} &= \frac{\partial L}{\partial z^{(1)}} \cdot x^T \\
\frac{\partial L}{\partial b^{(1)}} &= \frac{\partial L}{\partial z^{(1)}}
\end{align}
$$</p>

<h3>å®Œå…¨å®Ÿè£…</h3>

<pre><code class="language-python">import numpy as np

def sigmoid(x):
    """ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°"""
    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))

def sigmoid_derivative(x):
    """ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã®å°é–¢æ•°"""
    s = sigmoid(x)
    return s * (1 - s)

class TwoLayerNetWithBackprop:
    """èª¤å·®é€†ä¼æ’­æ³•ã‚’å®Ÿè£…ã—ãŸ2å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""

    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):
        """
        Args:
            input_size: å…¥åŠ›å±¤ã®ã‚µã‚¤ã‚º
            hidden_size: éš ã‚Œå±¤ã®ã‚µã‚¤ã‚º
            output_size: å‡ºåŠ›å±¤ã®ã‚µã‚¤ã‚º
            learning_rate: å­¦ç¿’ç‡
        """
        # é‡ã¿ã®åˆæœŸåŒ–ï¼ˆHeã®åˆæœŸåŒ–ï¼‰
        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)
        self.b1 = np.zeros(hidden_size)

        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)
        self.b2 = np.zeros(output_size)

        self.learning_rate = learning_rate

    def forward(self, x):
        """é †ä¼æ’­"""
        # ç¬¬1å±¤
        self.z1 = np.dot(x, self.W1) + self.b1
        self.a1 = sigmoid(self.z1)

        # ç¬¬2å±¤
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = sigmoid(self.z2)

        return self.a2

    def backward(self, x, y_true, y_pred):
        """
        èª¤å·®é€†ä¼æ’­æ³•

        Args:
            x: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            y_true: æ­£è§£ãƒ©ãƒ™ãƒ«
            y_pred: äºˆæ¸¬å€¤
        """
        batch_size = x.shape[0]

        # å‡ºåŠ›å±¤ã®å‹¾é…
        delta2 = (y_pred - y_true) * sigmoid_derivative(self.z2)

        # ç¬¬2å±¤ã®é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã®å‹¾é…
        dW2 = np.dot(self.a1.T, delta2) / batch_size
        db2 = np.sum(delta2, axis=0) / batch_size

        # éš ã‚Œå±¤ã®å‹¾é…
        delta1 = np.dot(delta2, self.W2.T) * sigmoid_derivative(self.z1)

        # ç¬¬1å±¤ã®é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã®å‹¾é…
        dW1 = np.dot(x.T, delta1) / batch_size
        db1 = np.sum(delta1, axis=0) / batch_size

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´æ–°
        self.W1 -= self.learning_rate * dW1
        self.b1 -= self.learning_rate * db1
        self.W2 -= self.learning_rate * dW2
        self.b2 -= self.learning_rate * db2

    def train(self, x, y_true, epochs=1000, verbose=True):
        """
        å­¦ç¿’ãƒ«ãƒ¼ãƒ—

        Args:
            x: è¨“ç·´ãƒ‡ãƒ¼ã‚¿
            y_true: æ­£è§£ãƒ©ãƒ™ãƒ«
            epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            verbose: é€²æ—è¡¨ç¤º
        """
        losses = []

        for epoch in range(epochs):
            # é †ä¼æ’­
            y_pred = self.forward(x)

            # æå¤±è¨ˆç®—
            loss = np.mean((y_true - y_pred) ** 2)
            losses.append(loss)

            # é€†ä¼æ’­
            self.backward(x, y_true, y_pred)

            # é€²æ—è¡¨ç¤º
            if verbose and (epoch % 100 == 0 or epoch == epochs - 1):
                print(f"Epoch {epoch:4d}: Loss = {loss:.6f}")

        return losses

# XORå•é¡Œã§ãƒ†ã‚¹ãƒˆ
print("=== XORå•é¡Œã®å­¦ç¿’ ===")

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ä½œæˆã¨å­¦ç¿’
net = TwoLayerNetWithBackprop(input_size=2, hidden_size=4, output_size=1, learning_rate=0.5)
losses = net.train(X, y, epochs=5000, verbose=True)

# æœ€çµ‚çµæœ
print("\n=== æœ€çµ‚äºˆæ¸¬çµæœ ===")
predictions = net.forward(X)
for i in range(len(X)):
    pred_label = 1 if predictions[i] > 0.5 else 0
    print(f"å…¥åŠ›: {X[i]} â†’ äºˆæ¸¬: {predictions[i][0]:.4f} â†’ ãƒ©ãƒ™ãƒ«: {pred_label} (æ­£è§£: {y[i][0]})")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== XORå•é¡Œã®å­¦ç¿’ ===
Epoch    0: Loss = 0.259762
Epoch  100: Loss = 0.249876
Epoch  200: Loss = 0.249011
Epoch  300: Loss = 0.246863
...
Epoch 4900: Loss = 0.000625
Epoch 4999: Loss = 0.000612

=== æœ€çµ‚äºˆæ¸¬çµæœ ===
å…¥åŠ›: [0 0] â†’ äºˆæ¸¬: 0.0247 â†’ ãƒ©ãƒ™ãƒ«: 0 (æ­£è§£: 0)
å…¥åŠ›: [0 1] â†’ äºˆæ¸¬: 0.9753 â†’ ãƒ©ãƒ™ãƒ«: 1 (æ­£è§£: 1)
å…¥åŠ›: [1 0] â†’ äºˆæ¸¬: 0.9751 â†’ ãƒ©ãƒ™ãƒ«: 1 (æ­£è§£: 1)
å…¥åŠ›: [1 1] â†’ äºˆæ¸¬: 0.0254 â†’ ãƒ©ãƒ™ãƒ«: 0 (æ­£è§£: 0)
</code></pre>

<blockquote>
<p><strong>æˆåŠŸï¼</strong> å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ã¨èª¤å·®é€†ä¼æ’­æ³•ã«ã‚ˆã‚Šã€XORå•é¡Œã‚’è§£æ±ºã§ãã¾ã—ãŸï¼</p>
</blockquote>

<hr>

<h2>2.5 å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–</h2>

<pre><code class="language-python">import matplotlib.pyplot as plt

def plot_learning_curve(losses):
    """å­¦ç¿’æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
    plt.figure(figsize=(10, 6))
    plt.plot(losses, linewidth=2)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss (MSE)', fontsize=12)
    plt.title('XORå•é¡Œã®å­¦ç¿’æ›²ç·š', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.yscale('log')  # å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«
    plt.show()

plot_learning_curve(losses)
</code></pre>

<h3>æ±ºå®šå¢ƒç•Œã®å¯è¦–åŒ–</h3>

<pre><code class="language-python">def plot_decision_boundary(net, X, y):
    """æ±ºå®šå¢ƒç•Œã‚’å¯è¦–åŒ–"""
    # ã‚°ãƒªãƒƒãƒ‰ã®ä½œæˆ
    x_min, x_max = -0.5, 1.5
    y_min, y_max = -0.5, 1.5
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),
                         np.linspace(y_min, y_max, 200))

    # å„ç‚¹ã®äºˆæ¸¬
    Z = net.forward(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    # ãƒ—ãƒ­ãƒƒãƒˆ
    plt.figure(figsize=(10, 8))

    # èƒŒæ™¯è‰²ï¼ˆæ±ºå®šå¢ƒç•Œï¼‰
    plt.contourf(xx, yy, Z, levels=20, cmap='RdYlBu', alpha=0.8)
    plt.colorbar(label='äºˆæ¸¬å€¤')

    # ãƒ‡ãƒ¼ã‚¿ç‚¹
    plt.scatter(X[y.flatten()==0][:, 0], X[y.flatten()==0][:, 1],
                s=200, c='blue', marker='o', edgecolors='k', linewidths=2,
                label='ã‚¯ãƒ©ã‚¹ 0')
    plt.scatter(X[y.flatten()==1][:, 0], X[y.flatten()==1][:, 1],
                s=200, c='red', marker='s', edgecolors='k', linewidths=2,
                label='ã‚¯ãƒ©ã‚¹ 1')

    plt.xlim(x_min, x_max)
    plt.ylim(y_min, y_max)
    plt.xlabel('x1', fontsize=14)
    plt.ylabel('x2', fontsize=14)
    plt.title('XORå•é¡Œã®æ±ºå®šå¢ƒç•Œï¼ˆå¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ï¼‰', fontsize=16, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.show()

plot_decision_boundary(net, X, y)
</code></pre>

<hr>

<h2>2.6 ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’</h2>

<h3>ãƒãƒƒãƒå­¦ç¿’ã®ç¨®é¡</h3>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>ãƒãƒƒãƒã‚µã‚¤ã‚º</th>
<th>ç‰¹å¾´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒãƒƒãƒå‹¾é…é™ä¸‹æ³•</strong></td>
<td>å…¨ãƒ‡ãƒ¼ã‚¿</td>
<td>å®‰å®šã ãŒé…ã„</td>
</tr>
<tr>
<td><strong>ç¢ºç‡çš„å‹¾é…é™ä¸‹æ³•ï¼ˆSGDï¼‰</strong></td>
<td>1ã‚µãƒ³ãƒ—ãƒ«</td>
<td>é«˜é€Ÿã ãŒä¸å®‰å®š</td>
</tr>
<tr>
<td><strong>ãƒŸãƒ‹ãƒãƒƒãƒå‹¾é…é™ä¸‹æ³•</strong></td>
<td>æ•°åã€œæ•°ç™¾</td>
<td>ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„ï¼ˆå®Ÿç”¨çš„ï¼‰</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">def create_mini_batches(X, y, batch_size):
    """
    ãƒŸãƒ‹ãƒãƒƒãƒã®ä½œæˆ

    Args:
        X: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        y: ãƒ©ãƒ™ãƒ«
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º

    Yields:
        (X_batch, y_batch)ã®ã‚¿ãƒ—ãƒ«
    """
    n_samples = X.shape[0]
    indices = np.random.permutation(n_samples)

    for start_idx in range(0, n_samples, batch_size):
        end_idx = min(start_idx + batch_size, n_samples)
        batch_indices = indices[start_idx:end_idx]

        yield X[batch_indices], y[batch_indices]

# ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ã®ä¾‹
def train_with_minibatch(net, X, y, epochs=1000, batch_size=2):
    """ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’"""
    losses = []

    for epoch in range(epochs):
        epoch_loss = 0
        n_batches = 0

        for X_batch, y_batch in create_mini_batches(X, y, batch_size):
            # é †ä¼æ’­
            y_pred = net.forward(X_batch)

            # æå¤±
            loss = np.mean((y_batch - y_pred) ** 2)
            epoch_loss += loss
            n_batches += 1

            # é€†ä¼æ’­
            net.backward(X_batch, y_batch, y_pred)

        avg_loss = epoch_loss / n_batches
        losses.append(avg_loss)

        if epoch % 100 == 0:
            print(f"Epoch {epoch:4d}: Loss = {avg_loss:.6f}")

    return losses

# ãƒ†ã‚¹ãƒˆ
print("\n=== ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ ===")
net_mini = TwoLayerNetWithBackprop(input_size=2, hidden_size=4, output_size=1, learning_rate=0.5)
losses_mini = train_with_minibatch(net_mini, X, y, epochs=2000, batch_size=2)
</code></pre>

<hr>

<h2>2.7 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ã®æ§‹é€ </strong></p>
<ul>
<li>å…¥åŠ›å±¤ã€éš ã‚Œå±¤ã€å‡ºåŠ›å±¤</li>
<li>å„å±¤ã¯é‡ã¿ $W$ ã¨ãƒã‚¤ã‚¢ã‚¹ $b$ ã‚’æŒã¤</li>
<li>æ´»æ€§åŒ–é–¢æ•°ã§éç·šå½¢æ€§ã‚’å°å…¥</li>
</ul></li>
<li><p><strong>æå¤±é–¢æ•°</strong></p>
<ul>
<li>MSE: å›å¸°å•é¡Œ</li>
<li>Cross-Entropy: åˆ†é¡å•é¡Œ</li>
</ul></li>
<li><p><strong>å‹¾é…é™ä¸‹æ³•</strong></p>
<ul>
<li>$w \leftarrow w - \eta \frac{\partial L}{\partial w}$</li>
<li>å­¦ç¿’ç‡ $\eta$ ã®é‡è¦æ€§</li>
</ul></li>
<li><p><strong>èª¤å·®é€†ä¼æ’­æ³•</strong></p>
<ul>
<li>é€£é–å¾‹ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªå‹¾é…è¨ˆç®—</li>
<li>å‡ºåŠ›å±¤ã‹ã‚‰å…¥åŠ›å±¤ã¸ã®é€†å‘ãè¨ˆç®—</li>
<li>NumPyã«ã‚ˆã‚‹å®Œå…¨å®Ÿè£…</li>
</ul></li>
<li><p><strong>XORå•é¡Œã®è§£æ±º</strong></p>
<ul>
<li>å¤šå±¤åŒ–ã«ã‚ˆã‚Šéç·šå½¢å•é¡Œã‚’è§£æ±º</li>
<li>å®Ÿéš›ã«å­¦ç¿’ã—ã¦ç²¾åº¦100%ã‚’é”æˆ</li>
</ul></li>
</ol>

<h3>é‡è¦ãªæ•°å¼</h3>

<table>
<thead>
<tr>
<th>æ¦‚å¿µ</th>
<th>æ•°å¼</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>é †ä¼æ’­</strong></td>
<td>$y = \sigma(W^{(2)} \sigma(W^{(1)} x + b^{(1)}) + b^{(2)})$</td>
</tr>
<tr>
<td><strong>MSEæå¤±</strong></td>
<td>$L = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$</td>
</tr>
<tr>
<td><strong>å‹¾é…é™ä¸‹</strong></td>
<td>$w \leftarrow w - \eta \frac{\partial L}{\partial w}$</td>
</tr>
<tr>
<td><strong>é€£é–å¾‹</strong></td>
<td>$\frac{\partial L}{\partial w} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z} \cdot \frac{\partial z}{\partial w}$</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ç« ã¸</h3>

<p>ç¬¬3ç« ã§ã¯ã€<strong>æ´»æ€§åŒ–é–¢æ•°ã¨æœ€é©åŒ–</strong>ã‚’å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>æ§˜ã€…ãªæ´»æ€§åŒ–é–¢æ•°ï¼ˆReLUã€Leaky ReLUã€ELUï¼‰</li>
<li>å‹¾é…æ¶ˆå¤±å•é¡Œã¨ãã®å¯¾ç­–</li>
<li>é«˜åº¦ãªæœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆMomentumã€Adamï¼‰</li>
<li>é‡ã¿ã®åˆæœŸåŒ–æˆ¦ç•¥</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>
<p>ä»¥ä¸‹ã®æ–‡ç« ã®æ­£èª¤ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚</p>
<ol>
<li>å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ã¯éš ã‚Œå±¤ã‚’æŒã¤</li>
<li>èª¤å·®é€†ä¼æ’­æ³•ã¯å…¥åŠ›å±¤ã‹ã‚‰å‡ºåŠ›å±¤ã«å‘ã‹ã£ã¦è¨ˆç®—ã™ã‚‹</li>
<li>å­¦ç¿’ç‡ãŒå¤§ãã™ãã‚‹ã¨ç™ºæ•£ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹</li>
<li>XORå•é¡Œã¯å˜å±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ã§è§£ã‘ã‚‹</li>
</ol>

<details>
<summary>è§£ç­”ä¾‹</summary>

<ol>
<li><strong>æ­£</strong> - MLPã®å®šç¾©</li>
<li><strong>èª¤</strong> - é€†ä¼æ’­ã¯å‡ºåŠ›å±¤ã‹ã‚‰å…¥åŠ›å±¤ã¸</li>
<li><strong>æ­£</strong> - å­¦ç¿’ç‡ãŒå¤§ãã„ã¨æŒ¯å‹•ãƒ»ç™ºæ•£</li>
<li><strong>èª¤</strong> - XORã¯ç·šå½¢åˆ†é›¢ä¸å¯èƒ½ã€å¤šå±¤åŒ–ãŒå¿…è¦</li>
</ol>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã®å°é–¢æ•°ã‚’å°å‡ºã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€Pythonã§å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

<p>ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°: $\sigma(x) = \frac{1}{1 + e^{-x}}$</p>

<p>å•†ã®å¾®åˆ†å…¬å¼ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚</p>

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>å°å‡º</strong>ï¼š</p>

<p>$$
\begin{align}
\sigma(x) &= \frac{1}{1 + e^{-x}} \\
\sigma'(x) &= \frac{d}{dx} (1 + e^{-x})^{-1} \\
&= -(1 + e^{-x})^{-2} \cdot (-e^{-x}) \\
&= \frac{e^{-x}}{(1 + e^{-x})^2} \\
&= \frac{1}{1 + e^{-x}} \cdot \frac{e^{-x}}{1 + e^{-x}} \\
&= \sigma(x) \cdot \frac{e^{-x}}{1 + e^{-x}} \\
&= \sigma(x) \cdot \frac{1 + e^{-x} - 1}{1 + e^{-x}} \\
&= \sigma(x) \cdot (1 - \sigma(x))
\end{align}
$$</p>

<p><strong>Pythonå®Ÿè£…</strong>ï¼š</p>

<pre><code class="language-python">def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)

# ãƒ†ã‚¹ãƒˆ
x_test = np.linspace(-5, 5, 100)
plt.figure(figsize=(10, 6))
plt.plot(x_test, sigmoid(x_test), label='Ïƒ(x)', linewidth=2)
plt.plot(x_test, sigmoid_derivative(x_test), label="Ïƒ'(x)", linewidth=2)
plt.xlabel('x', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã¨ãã®å°é–¢æ•°', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>3å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆå…¥åŠ›å±¤ã€éš ã‚Œå±¤2ã¤ã€å‡ºåŠ›å±¤ï¼‰ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">class ThreeLayerNet:
    """3å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""

    def __init__(self, input_size, hidden1_size, hidden2_size, output_size, learning_rate=0.1):
        # é‡ã¿ã®åˆæœŸåŒ–
        self.W1 = np.random.randn(input_size, hidden1_size) * 0.1
        self.b1 = np.zeros(hidden1_size)

        self.W2 = np.random.randn(hidden1_size, hidden2_size) * 0.1
        self.b2 = np.zeros(hidden2_size)

        self.W3 = np.random.randn(hidden2_size, output_size) * 0.1
        self.b3 = np.zeros(output_size)

        self.learning_rate = learning_rate

    def forward(self, x):
        """é †ä¼æ’­"""
        # ç¬¬1å±¤
        self.z1 = np.dot(x, self.W1) + self.b1
        self.a1 = sigmoid(self.z1)

        # ç¬¬2å±¤
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = sigmoid(self.z2)

        # ç¬¬3å±¤
        self.z3 = np.dot(self.a2, self.W3) + self.b3
        self.a3 = sigmoid(self.z3)

        return self.a3

    def backward(self, x, y_true, y_pred):
        """èª¤å·®é€†ä¼æ’­æ³•"""
        batch_size = x.shape[0]

        # å‡ºåŠ›å±¤
        delta3 = (y_pred - y_true) * sigmoid_derivative(self.z3)
        dW3 = np.dot(self.a2.T, delta3) / batch_size
        db3 = np.sum(delta3, axis=0) / batch_size

        # ç¬¬2éš ã‚Œå±¤
        delta2 = np.dot(delta3, self.W3.T) * sigmoid_derivative(self.z2)
        dW2 = np.dot(self.a1.T, delta2) / batch_size
        db2 = np.sum(delta2, axis=0) / batch_size

        # ç¬¬1éš ã‚Œå±¤
        delta1 = np.dot(delta2, self.W2.T) * sigmoid_derivative(self.z1)
        dW1 = np.dot(x.T, delta1) / batch_size
        db1 = np.sum(delta1, axis=0) / batch_size

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
        self.W1 -= self.learning_rate * dW1
        self.b1 -= self.learning_rate * db1
        self.W2 -= self.learning_rate * dW2
        self.b2 -= self.learning_rate * db2
        self.W3 -= self.learning_rate * dW3
        self.b3 -= self.learning_rate * db3

# ãƒ†ã‚¹ãƒˆ
print("=== 3å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ ===")
net3 = ThreeLayerNet(input_size=2, hidden1_size=4, hidden2_size=4, output_size=1, learning_rate=0.5)

# XORå•é¡Œã§å­¦ç¿’
for epoch in range(3000):
    y_pred = net3.forward(X)
    net3.backward(X, y, y_pred)

    if epoch % 500 == 0:
        loss = np.mean((y - y_pred) ** 2)
        print(f"Epoch {epoch:4d}: Loss = {loss:.6f}")

# æœ€çµ‚çµæœ
print("\n=== æœ€çµ‚äºˆæ¸¬ ===")
final_pred = net3.forward(X)
for i in range(len(X)):
    print(f"å…¥åŠ›: {X[i]} â†’ äºˆæ¸¬: {final_pred[i][0]:.4f} (æ­£è§£: {y[i][0]})")
</code></pre>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>ANDã€ORã€NANDã®3ã¤ã®è«–ç†ã‚²ãƒ¼ãƒˆã‚’åŒæ™‚ã«å­¦ç¿’ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼ˆãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ï¼‰ã€‚</p>

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

<ul>
<li>å‡ºåŠ›å±¤ã‚’3ãƒ¦ãƒ‹ãƒƒãƒˆã«ã™ã‚‹</li>
<li>å„å‡ºåŠ›ãŒANDã€ORã€NANDã«å¯¾å¿œ</li>
</ul>

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python"># ãƒ‡ãƒ¼ã‚¿æº–å‚™
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_multi = np.array([
    [0, 0, 1],  # AND=0, OR=0, NAND=1
    [0, 1, 1],  # AND=0, OR=1, NAND=1
    [0, 1, 1],  # AND=0, OR=1, NAND=1
    [1, 1, 0]   # AND=1, OR=1, NAND=0
])

# ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
net_multi = TwoLayerNetWithBackprop(input_size=2, hidden_size=6, output_size=3, learning_rate=0.5)

print("=== ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ï¼ˆAND, OR, NANDï¼‰ ===")

# å­¦ç¿’
for epoch in range(5000):
    y_pred = net_multi.forward(X)
    net_multi.backward(X, y_multi, y_pred)

    if epoch % 1000 == 0:
        loss = np.mean((y_multi - y_pred) ** 2)
        print(f"Epoch {epoch:4d}: Loss = {loss:.6f}")

# æœ€çµ‚çµæœ
print("\n=== æœ€çµ‚äºˆæ¸¬ ===")
print("å…¥åŠ›  | ANDäºˆæ¸¬ | ORäºˆæ¸¬  | NANDäºˆæ¸¬ | ANDæ­£è§£ | ORæ­£è§£  | NANDæ­£è§£")
print("-" * 75)
final_pred = net_multi.forward(X)
for i in range(len(X)):
    print(f"{X[i]} | {final_pred[i][0]:.4f}  | {final_pred[i][1]:.4f}  | {final_pred[i][2]:.4f}   | "
          f"{y_multi[i][0]}       | {y_multi[i][1]}       | {y_multi[i][2]}")
</code></pre>

</details>

<h3>å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ï¼ˆå­¦ç¿’ç‡ã‚’å¾ã€…ã«å°ã•ãã™ã‚‹ï¼‰ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">def learning_rate_decay(initial_lr, epoch, decay_rate=0.95, decay_step=100):
    """
    å­¦ç¿’ç‡ã®æ¸›è¡°

    Args:
        initial_lr: åˆæœŸå­¦ç¿’ç‡
        epoch: ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯
        decay_rate: æ¸›è¡°ç‡
        decay_step: æ¸›è¡°ã®ã‚¹ãƒ†ãƒƒãƒ—

    Returns:
        æ¸›è¡°å¾Œã®å­¦ç¿’ç‡
    """
    return initial_lr * (decay_rate ** (epoch // decay_step))

class TwoLayerNetWithLRScheduling(TwoLayerNetWithBackprop):
    """å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ä»˜ããƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""

    def __init__(self, input_size, hidden_size, output_size, initial_lr=0.5, decay_rate=0.95):
        super().__init__(input_size, hidden_size, output_size, initial_lr)
        self.initial_lr = initial_lr
        self.decay_rate = decay_rate

    def train_with_scheduling(self, X, y, epochs=5000):
        """å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ä»˜ãå­¦ç¿’"""
        losses = []

        for epoch in range(epochs):
            # å­¦ç¿’ç‡ã®æ›´æ–°
            self.learning_rate = learning_rate_decay(
                self.initial_lr, epoch, self.decay_rate, decay_step=500
            )

            # é †ä¼æ’­
            y_pred = self.forward(X)

            # æå¤±
            loss = np.mean((y - y_pred) ** 2)
            losses.append(loss)

            # é€†ä¼æ’­
            self.backward(X, y, y_pred)

            if epoch % 500 == 0:
                print(f"Epoch {epoch:4d}: LR = {self.learning_rate:.6f}, Loss = {loss:.6f}")

        return losses

# ãƒ†ã‚¹ãƒˆ
print("=== å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚° ===")
net_sched = TwoLayerNetWithLRScheduling(input_size=2, hidden_size=4, output_size=1,
                                        initial_lr=1.0, decay_rate=0.9)
losses_sched = net_sched.train_with_scheduling(X, y, epochs=5000)
</code></pre>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). "Learning representations by back-propagating errors." <em>Nature</em>, 323(6088), 533-536.</li>
<li>LeCun, Y., Bengio, Y., & Hinton, G. (2015). "Deep learning." <em>Nature</em>, 521(7553), 436-444.</li>
<li>Goodfellow, I., Bengio, Y., & Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</li>
<li>æ–è—¤åº·æ¯… (2016). ã€ã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹Deep Learningã€ã‚ªãƒ©ã‚¤ãƒªãƒ¼ã‚¸ãƒ£ãƒ‘ãƒ³.</li>
</ol>

<div class="navigation">
    <a href="chapter1-perceptron.html" class="nav-button">â† å‰ã®ç« : ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³</a>
    <a href="chapter3-activation-optimization.html" class="nav-button">æ¬¡ã®ç« : æ´»æ€§åŒ–é–¢æ•°ã¨æœ€é©åŒ– â†’</a>
</div>

    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-20</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
