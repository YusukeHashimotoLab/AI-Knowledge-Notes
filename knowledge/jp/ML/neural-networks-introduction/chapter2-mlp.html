<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第2章：多層パーセプトロンと誤差逆伝播法 - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>第2章：多層パーセプトロンと誤差逆伝播法</h1>
            <p class="subtitle">深層学習の核心アルゴリズム - Backpropagation</p>
            <div class="meta">
                <span class="meta-item">📖 読了時間: 30-35分</span>
                <span class="meta-item">📊 難易度: 初級〜中級</span>
                <span class="meta-item">💻 コード例: 15個</span>
                <span class="meta-item">📝 演習問題: 5問</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>学習目標</h2>
<p>この章を読むことで、以下を習得できます：</p>
<ul>
<li>✅ 多層パーセプトロン（MLP）の構造を理解する</li>
<li>✅ 誤差逆伝播法（Backpropagation）の仕組みを説明できる</li>
<li>✅ 勾配降下法によるパラメータ更新を理解する</li>
<li>✅ 連鎖律（Chain Rule）の数学的基礎を学ぶ</li>
<li>✅ NumPyでMLPを完全実装できる</li>
<li>✅ XOR問題を実際に解決できる</li>
</ul>

<hr>

<h2>2.1 多層パーセプトロン（MLP）の構造</h2>

<h3>MLPとは</h3>

<p><strong>多層パーセプトロン（Multilayer Perceptron, MLP）</strong>は、複数層のパーセプトロンを組み合わせたニューラルネットワークです。</p>

<div class="mermaid">
graph LR
    x1[入力層<br/>x1] --> h1[隠れ層<br/>h1]
    x2[入力層<br/>x2] --> h1
    x1 --> h2[隠れ層<br/>h2]
    x2 --> h2
    h1 --> y1[出力層<br/>y1]
    h2 --> y1

    style x1 fill:#e3f2fd
    style x2 fill:#e3f2fd
    style h1 fill:#fff3e0
    style h2 fill:#fff3e0
    style y1 fill:#e8f5e9
</div>

<h3>層の種類</h3>

<table>
<thead>
<tr>
<th>層の種類</th>
<th>役割</th>
<th>説明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>入力層</strong></td>
<td>Input Layer</td>
<td>データを受け取る層（学習対象ではない）</td>
</tr>
<tr>
<td><strong>隠れ層</strong></td>
<td>Hidden Layer</td>
<td>特徴抽出を行う層（学習対象）</td>
</tr>
<tr>
<td><strong>出力層</strong></td>
<td>Output Layer</td>
<td>最終結果を出力する層（学習対象）</td>
</tr>
</tbody>
</table>

<h3>2層ニューラルネットワークの数式</h3>

<p>入力 $\mathbf{x} = [x_1, x_2]^T$ から出力 $y$ までの計算：</p>

<p><strong>第1層（入力 → 隠れ層）</strong>：</p>
<p>$$
\mathbf{h} = \sigma(\mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)})
$$</p>

<p><strong>第2層（隠れ層 → 出力）</strong>：</p>
<p>$$
y = \sigma(\mathbf{W}^{(2)} \mathbf{h} + b^{(2)})
$$</p>

<p>ここで、$\sigma$は活性化関数（シグモイド関数など）です。</p>

<h3>Python実装の基本構造</h3>

<pre><code class="language-python">import numpy as np

def sigmoid(x):
    """シグモイド関数"""
    return 1 / (1 + np.exp(-x))

class TwoLayerNet:
    """2層ニューラルネットワーク"""

    def __init__(self, input_size, hidden_size, output_size):
        """
        Args:
            input_size: 入力層のニューロン数
            hidden_size: 隠れ層のニューロン数
            output_size: 出力層のニューロン数
        """
        # 重みの初期化（ランダム）
        self.W1 = np.random.randn(input_size, hidden_size)
        self.b1 = np.zeros(hidden_size)

        self.W2 = np.random.randn(hidden_size, output_size)
        self.b2 = np.zeros(output_size)

    def forward(self, x):
        """
        順伝播（Forward Propagation）

        Args:
            x: 入力データ (n_samples, input_size)

        Returns:
            出力 (n_samples, output_size)
        """
        # 第1層
        self.z1 = np.dot(x, self.W1) + self.b1
        self.a1 = sigmoid(self.z1)

        # 第2層
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = sigmoid(self.z2)

        return self.a2

# テスト
net = TwoLayerNet(input_size=2, hidden_size=3, output_size=1)
x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
output = net.forward(x)

print("=== 初期化直後の出力 ===")
print(output)
</code></pre>

<hr>

<h2>2.2 損失関数（Loss Function）</h2>

<h3>損失関数とは</h3>

<p><strong>損失関数（Loss Function）</strong>は、ニューラルネットワークの予測値と正解値の差を数値化します。学習の目的は、この損失を<strong>最小化</strong>することです。</p>

<h3>平均二乗誤差（MSE）</h3>

<p>回帰問題でよく使われる損失関数：</p>

<p>$$
L = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$</p>

<p>ここで、$y_i$は正解値、$\hat{y}_i$は予測値です。</p>

<pre><code class="language-python">def mean_squared_error(y_true, y_pred):
    """
    平均二乗誤差（Mean Squared Error, MSE）

    Args:
        y_true: 正解ラベル (n_samples,)
        y_pred: 予測値 (n_samples,)

    Returns:
        MSE値
    """
    return np.mean((y_true - y_pred) ** 2)

# 例
y_true = np.array([0, 1, 1, 0])
y_pred = np.array([0.1, 0.9, 0.8, 0.2])
loss = mean_squared_error(y_true, y_pred)
print(f"MSE: {loss:.4f}")  # 0.0125
</code></pre>

<h3>交差エントロピー誤差（Cross-Entropy）</h3>

<p>分類問題で使われる損失関数：</p>

<p>$$
L = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i) \right]
$$</p>

<pre><code class="language-python">def binary_cross_entropy(y_true, y_pred):
    """
    二値交差エントロピー（Binary Cross-Entropy）

    Args:
        y_true: 正解ラベル (n_samples,)
        y_pred: 予測確率 (n_samples,)

    Returns:
        BCE値
    """
    # 数値安定性のため、小さな値でクリッピング
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)

    return -np.mean(y_true * np.log(y_pred) +
                    (1 - y_true) * np.log(1 - y_pred))

# 例
loss_ce = binary_cross_entropy(y_true, y_pred)
print(f"Cross-Entropy: {loss_ce:.4f}")  # 0.1625
</code></pre>

<hr>

<h2>2.3 勾配降下法（Gradient Descent）</h2>

<h3>基本的なアイデア</h3>

<p><strong>勾配降下法</strong>は、損失関数を最小化するためのアルゴリズムです。パラメータを損失関数の勾配（微分）の<strong>逆方向</strong>に少しずつ更新します。</p>

<div class="mermaid">
graph TD
    A[初期パラメータ] --> B[損失を計算]
    B --> C[勾配を計算]
    C --> D[パラメータを更新]
    D --> E{収束？}
    E -->|No| B
    E -->|Yes| F[学習完了]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style F fill:#c8e6c9
</div>

<h3>更新式</h3>

<p>パラメータ $w$ の更新：</p>

<p>$$
w \leftarrow w - \eta \frac{\partial L}{\partial w}
$$</p>

<p>ここで、$\eta$は<strong>学習率（Learning Rate）</strong>です。</p>

<h3>Python実装</h3>

<pre><code class="language-python">def gradient_descent_demo():
    """勾配降下法のデモ"""

    # 簡単な関数: f(x) = x^2
    def f(x):
        return x ** 2

    # 導関数: f'(x) = 2x
    def df(x):
        return 2 * x

    # 初期値と学習率
    x = 10.0
    learning_rate = 0.1
    n_iterations = 20

    print("=== 勾配降下法のデモ ===")
    print(f"目標: f(x) = x^2 の最小値を見つける")
    print(f"初期値: x = {x}")
    print()

    for i in range(n_iterations):
        grad = df(x)
        x = x - learning_rate * grad

        if i % 5 == 0:
            print(f"Iteration {i:2d}: x = {x:8.4f}, f(x) = {f(x):8.4f}, grad = {grad:8.4f}")

    print()
    print(f"最終結果: x = {x:.4f}, f(x) = {f(x):.4f}")
    print(f"理論値: x = 0.0000, f(x) = 0.0000")

gradient_descent_demo()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== 勾配降下法のデモ ===
目標: f(x) = x^2 の最小値を見つける
初期値: x = 10.0

Iteration  0: x =   8.0000, f(x) =  64.0000, grad =  20.0000
Iteration  5: x =   2.6214, f(x) =   6.8718, grad =   5.2429
Iteration 10: x =   0.8590, f(x) =   0.7379, grad =   1.7179
Iteration 15: x =   0.2815, f(x) =   0.0792, grad =   0.5630

最終結果: x = 0.0922, f(x) = 0.0085
理論値: x = 0.0000, f(x) = 0.0000
</code></pre>

<h3>学習率の影響</h3>

<pre><code class="language-python">def compare_learning_rates():
    """学習率の違いを比較"""
    def f(x):
        return x ** 2

    def df(x):
        return 2 * x

    learning_rates = [0.01, 0.1, 0.5, 0.9]
    x_init = 10.0
    n_iterations = 10

    print("=== 学習率の比較 ===")
    for lr in learning_rates:
        x = x_init
        for _ in range(n_iterations):
            x = x - lr * df(x)

        print(f"学習率 η={lr:.2f} → 最終値 x={x:8.4f}, f(x)={f(x):8.4f}")

compare_learning_rates()
</code></pre>

<p><strong>出力</strong>：</p>
<pre><code>=== 学習率の比較 ===
学習率 η=0.01 → 最終値 x=   8.1707, f(x)=  66.7604
学習率 η=0.10 → 最終値 x=   0.0922, f(x)=   0.0085
学習率 η=0.50 → 最終値 x=   0.0098, f(x)=   0.0001
学習率 η=0.90 → 最終値 x= -10.0000, f(x)= 100.0000（発散！）
</code></pre>

<blockquote>
<p><strong>重要</strong>: 学習率が大きすぎると発散、小さすぎると収束が遅い！</p>
</blockquote>

<hr>

<h2>2.4 誤差逆伝播法（Backpropagation）</h2>

<h3>なぜ逆伝播が必要か</h3>

<p>多層ネットワークでは、各層のパラメータの勾配を計算する必要があります。<strong>誤差逆伝播法</strong>は、出力層から入力層に向かって勾配を効率的に計算する手法です。</p>

<h3>連鎖律（Chain Rule）</h3>

<p>合成関数の微分：</p>

<p>$$
\frac{\partial L}{\partial w} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z} \cdot \frac{\partial z}{\partial w}
$$</p>

<p>これが誤差逆伝播の数学的基礎です。</p>

<h3>2層ネットワークの逆伝播</h3>

<p>前向き計算：</p>
<p>$$
\begin{align}
z^{(1)} &= W^{(1)} x + b^{(1)} \\
a^{(1)} &= \sigma(z^{(1)}) \\
z^{(2)} &= W^{(2)} a^{(1)} + b^{(2)} \\
y &= \sigma(z^{(2)}) \\
L &= \frac{1}{2}(y - t)^2
\end{align}
$$</p>

<p>逆向き計算（勾配）：</p>
<p>$$
\begin{align}
\frac{\partial L}{\partial y} &= y - t \\
\frac{\partial L}{\partial z^{(2)}} &= \frac{\partial L}{\partial y} \cdot \sigma'(z^{(2)}) \\
\frac{\partial L}{\partial W^{(2)}} &= \frac{\partial L}{\partial z^{(2)}} \cdot (a^{(1)})^T \\
\frac{\partial L}{\partial b^{(2)}} &= \frac{\partial L}{\partial z^{(2)}} \\
\frac{\partial L}{\partial a^{(1)}} &= (W^{(2)})^T \cdot \frac{\partial L}{\partial z^{(2)}} \\
\frac{\partial L}{\partial z^{(1)}} &= \frac{\partial L}{\partial a^{(1)}} \cdot \sigma'(z^{(1)}) \\
\frac{\partial L}{\partial W^{(1)}} &= \frac{\partial L}{\partial z^{(1)}} \cdot x^T \\
\frac{\partial L}{\partial b^{(1)}} &= \frac{\partial L}{\partial z^{(1)}}
\end{align}
$$</p>

<h3>完全実装</h3>

<pre><code class="language-python">import numpy as np

def sigmoid(x):
    """シグモイド関数"""
    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))

def sigmoid_derivative(x):
    """シグモイド関数の導関数"""
    s = sigmoid(x)
    return s * (1 - s)

class TwoLayerNetWithBackprop:
    """誤差逆伝播法を実装した2層ニューラルネットワーク"""

    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):
        """
        Args:
            input_size: 入力層のサイズ
            hidden_size: 隠れ層のサイズ
            output_size: 出力層のサイズ
            learning_rate: 学習率
        """
        # 重みの初期化（Heの初期化）
        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)
        self.b1 = np.zeros(hidden_size)

        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)
        self.b2 = np.zeros(output_size)

        self.learning_rate = learning_rate

    def forward(self, x):
        """順伝播"""
        # 第1層
        self.z1 = np.dot(x, self.W1) + self.b1
        self.a1 = sigmoid(self.z1)

        # 第2層
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = sigmoid(self.z2)

        return self.a2

    def backward(self, x, y_true, y_pred):
        """
        誤差逆伝播法

        Args:
            x: 入力データ
            y_true: 正解ラベル
            y_pred: 予測値
        """
        batch_size = x.shape[0]

        # 出力層の勾配
        delta2 = (y_pred - y_true) * sigmoid_derivative(self.z2)

        # 第2層の重みとバイアスの勾配
        dW2 = np.dot(self.a1.T, delta2) / batch_size
        db2 = np.sum(delta2, axis=0) / batch_size

        # 隠れ層の勾配
        delta1 = np.dot(delta2, self.W2.T) * sigmoid_derivative(self.z1)

        # 第1層の重みとバイアスの勾配
        dW1 = np.dot(x.T, delta1) / batch_size
        db1 = np.sum(delta1, axis=0) / batch_size

        # パラメータの更新
        self.W1 -= self.learning_rate * dW1
        self.b1 -= self.learning_rate * db1
        self.W2 -= self.learning_rate * dW2
        self.b2 -= self.learning_rate * db2

    def train(self, x, y_true, epochs=1000, verbose=True):
        """
        学習ループ

        Args:
            x: 訓練データ
            y_true: 正解ラベル
            epochs: エポック数
            verbose: 進捗表示
        """
        losses = []

        for epoch in range(epochs):
            # 順伝播
            y_pred = self.forward(x)

            # 損失計算
            loss = np.mean((y_true - y_pred) ** 2)
            losses.append(loss)

            # 逆伝播
            self.backward(x, y_true, y_pred)

            # 進捗表示
            if verbose and (epoch % 100 == 0 or epoch == epochs - 1):
                print(f"Epoch {epoch:4d}: Loss = {loss:.6f}")

        return losses

# XOR問題でテスト
print("=== XOR問題の学習 ===")

# データ準備
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

# ネットワークの作成と学習
net = TwoLayerNetWithBackprop(input_size=2, hidden_size=4, output_size=1, learning_rate=0.5)
losses = net.train(X, y, epochs=5000, verbose=True)

# 最終結果
print("\n=== 最終予測結果 ===")
predictions = net.forward(X)
for i in range(len(X)):
    pred_label = 1 if predictions[i] > 0.5 else 0
    print(f"入力: {X[i]} → 予測: {predictions[i][0]:.4f} → ラベル: {pred_label} (正解: {y[i][0]})")
</code></pre>

<p><strong>出力例</strong>：</p>
<pre><code>=== XOR問題の学習 ===
Epoch    0: Loss = 0.259762
Epoch  100: Loss = 0.249876
Epoch  200: Loss = 0.249011
Epoch  300: Loss = 0.246863
...
Epoch 4900: Loss = 0.000625
Epoch 4999: Loss = 0.000612

=== 最終予測結果 ===
入力: [0 0] → 予測: 0.0247 → ラベル: 0 (正解: 0)
入力: [0 1] → 予測: 0.9753 → ラベル: 1 (正解: 1)
入力: [1 0] → 予測: 0.9751 → ラベル: 1 (正解: 1)
入力: [1 1] → 予測: 0.0254 → ラベル: 0 (正解: 0)
</code></pre>

<blockquote>
<p><strong>成功！</strong> 多層パーセプトロンと誤差逆伝播法により、XOR問題を解決できました！</p>
</blockquote>

<hr>

<h2>2.5 学習曲線の可視化</h2>

<pre><code class="language-python">import matplotlib.pyplot as plt

def plot_learning_curve(losses):
    """学習曲線をプロット"""
    plt.figure(figsize=(10, 6))
    plt.plot(losses, linewidth=2)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss (MSE)', fontsize=12)
    plt.title('XOR問題の学習曲線', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.yscale('log')  # 対数スケール
    plt.show()

plot_learning_curve(losses)
</code></pre>

<h3>決定境界の可視化</h3>

<pre><code class="language-python">def plot_decision_boundary(net, X, y):
    """決定境界を可視化"""
    # グリッドの作成
    x_min, x_max = -0.5, 1.5
    y_min, y_max = -0.5, 1.5
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),
                         np.linspace(y_min, y_max, 200))

    # 各点の予測
    Z = net.forward(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    # プロット
    plt.figure(figsize=(10, 8))

    # 背景色（決定境界）
    plt.contourf(xx, yy, Z, levels=20, cmap='RdYlBu', alpha=0.8)
    plt.colorbar(label='予測値')

    # データ点
    plt.scatter(X[y.flatten()==0][:, 0], X[y.flatten()==0][:, 1],
                s=200, c='blue', marker='o', edgecolors='k', linewidths=2,
                label='クラス 0')
    plt.scatter(X[y.flatten()==1][:, 0], X[y.flatten()==1][:, 1],
                s=200, c='red', marker='s', edgecolors='k', linewidths=2,
                label='クラス 1')

    plt.xlim(x_min, x_max)
    plt.ylim(y_min, y_max)
    plt.xlabel('x1', fontsize=14)
    plt.ylabel('x2', fontsize=14)
    plt.title('XOR問題の決定境界（多層パーセプトロン）', fontsize=16, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.show()

plot_decision_boundary(net, X, y)
</code></pre>

<hr>

<h2>2.6 ミニバッチ学習</h2>

<h3>バッチ学習の種類</h3>

<table>
<thead>
<tr>
<th>手法</th>
<th>バッチサイズ</th>
<th>特徴</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>バッチ勾配降下法</strong></td>
<td>全データ</td>
<td>安定だが遅い</td>
</tr>
<tr>
<td><strong>確率的勾配降下法（SGD）</strong></td>
<td>1サンプル</td>
<td>高速だが不安定</td>
</tr>
<tr>
<td><strong>ミニバッチ勾配降下法</strong></td>
<td>数十〜数百</td>
<td>バランスが良い（実用的）</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">def create_mini_batches(X, y, batch_size):
    """
    ミニバッチの作成

    Args:
        X: 入力データ
        y: ラベル
        batch_size: バッチサイズ

    Yields:
        (X_batch, y_batch)のタプル
    """
    n_samples = X.shape[0]
    indices = np.random.permutation(n_samples)

    for start_idx in range(0, n_samples, batch_size):
        end_idx = min(start_idx + batch_size, n_samples)
        batch_indices = indices[start_idx:end_idx]

        yield X[batch_indices], y[batch_indices]

# ミニバッチ学習の例
def train_with_minibatch(net, X, y, epochs=1000, batch_size=2):
    """ミニバッチ学習"""
    losses = []

    for epoch in range(epochs):
        epoch_loss = 0
        n_batches = 0

        for X_batch, y_batch in create_mini_batches(X, y, batch_size):
            # 順伝播
            y_pred = net.forward(X_batch)

            # 損失
            loss = np.mean((y_batch - y_pred) ** 2)
            epoch_loss += loss
            n_batches += 1

            # 逆伝播
            net.backward(X_batch, y_batch, y_pred)

        avg_loss = epoch_loss / n_batches
        losses.append(avg_loss)

        if epoch % 100 == 0:
            print(f"Epoch {epoch:4d}: Loss = {avg_loss:.6f}")

    return losses

# テスト
print("\n=== ミニバッチ学習 ===")
net_mini = TwoLayerNetWithBackprop(input_size=2, hidden_size=4, output_size=1, learning_rate=0.5)
losses_mini = train_with_minibatch(net_mini, X, y, epochs=2000, batch_size=2)
</code></pre>

<hr>

<h2>2.7 本章のまとめ</h2>

<h3>学んだこと</h3>

<ol>
<li><p><strong>多層パーセプトロンの構造</strong></p>
<ul>
<li>入力層、隠れ層、出力層</li>
<li>各層は重み $W$ とバイアス $b$ を持つ</li>
<li>活性化関数で非線形性を導入</li>
</ul></li>
<li><p><strong>損失関数</strong></p>
<ul>
<li>MSE: 回帰問題</li>
<li>Cross-Entropy: 分類問題</li>
</ul></li>
<li><p><strong>勾配降下法</strong></p>
<ul>
<li>$w \leftarrow w - \eta \frac{\partial L}{\partial w}$</li>
<li>学習率 $\eta$ の重要性</li>
</ul></li>
<li><p><strong>誤差逆伝播法</strong></p>
<ul>
<li>連鎖律による効率的な勾配計算</li>
<li>出力層から入力層への逆向き計算</li>
<li>NumPyによる完全実装</li>
</ul></li>
<li><p><strong>XOR問題の解決</strong></p>
<ul>
<li>多層化により非線形問題を解決</li>
<li>実際に学習して精度100%を達成</li>
</ul></li>
</ol>

<h3>重要な数式</h3>

<table>
<thead>
<tr>
<th>概念</th>
<th>数式</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>順伝播</strong></td>
<td>$y = \sigma(W^{(2)} \sigma(W^{(1)} x + b^{(1)}) + b^{(2)})$</td>
</tr>
<tr>
<td><strong>MSE損失</strong></td>
<td>$L = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$</td>
</tr>
<tr>
<td><strong>勾配降下</strong></td>
<td>$w \leftarrow w - \eta \frac{\partial L}{\partial w}$</td>
</tr>
<tr>
<td><strong>連鎖律</strong></td>
<td>$\frac{\partial L}{\partial w} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z} \cdot \frac{\partial z}{\partial w}$</td>
</tr>
</tbody>
</table>

<h3>次の章へ</h3>

<p>第3章では、<strong>活性化関数と最適化</strong>を学びます：</p>
<ul>
<li>様々な活性化関数（ReLU、Leaky ReLU、ELU）</li>
<li>勾配消失問題とその対策</li>
<li>高度な最適化アルゴリズム（Momentum、Adam）</li>
<li>重みの初期化戦略</li>
</ul>

<hr>

<h2>演習問題</h2>

<h3>問題1（難易度：easy）</h3>
<p>以下の文章の正誤を判定してください。</p>
<ol>
<li>多層パーセプトロンは隠れ層を持つ</li>
<li>誤差逆伝播法は入力層から出力層に向かって計算する</li>
<li>学習率が大きすぎると発散する可能性がある</li>
<li>XOR問題は単層パーセプトロンで解ける</li>
</ol>

<details>
<summary>解答例</summary>

<ol>
<li><strong>正</strong> - MLPの定義</li>
<li><strong>誤</strong> - 逆伝播は出力層から入力層へ</li>
<li><strong>正</strong> - 学習率が大きいと振動・発散</li>
<li><strong>誤</strong> - XORは線形分離不可能、多層化が必要</li>
</ol>

</details>

<h3>問題2（難易度：medium）</h3>
<p>シグモイド関数の導関数を導出してください。また、Pythonで実装してください。</p>

<details>
<summary>ヒント</summary>

<p>シグモイド関数: $\sigma(x) = \frac{1}{1 + e^{-x}}$</p>

<p>商の微分公式を使用します。</p>

</details>

<details>
<summary>解答例</summary>

<p><strong>導出</strong>：</p>

<p>$$
\begin{align}
\sigma(x) &= \frac{1}{1 + e^{-x}} \\
\sigma'(x) &= \frac{d}{dx} (1 + e^{-x})^{-1} \\
&= -(1 + e^{-x})^{-2} \cdot (-e^{-x}) \\
&= \frac{e^{-x}}{(1 + e^{-x})^2} \\
&= \frac{1}{1 + e^{-x}} \cdot \frac{e^{-x}}{1 + e^{-x}} \\
&= \sigma(x) \cdot \frac{e^{-x}}{1 + e^{-x}} \\
&= \sigma(x) \cdot \frac{1 + e^{-x} - 1}{1 + e^{-x}} \\
&= \sigma(x) \cdot (1 - \sigma(x))
\end{align}
$$</p>

<p><strong>Python実装</strong>：</p>

<pre><code class="language-python">def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)

# テスト
x_test = np.linspace(-5, 5, 100)
plt.figure(figsize=(10, 6))
plt.plot(x_test, sigmoid(x_test), label='σ(x)', linewidth=2)
plt.plot(x_test, sigmoid_derivative(x_test), label="σ'(x)", linewidth=2)
plt.xlabel('x', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('シグモイド関数とその導関数', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

</details>

<h3>問題3（難易度：medium）</h3>
<p>3層ニューラルネットワーク（入力層、隠れ層2つ、出力層）を実装してください。</p>

<details>
<summary>解答例</summary>

<pre><code class="language-python">class ThreeLayerNet:
    """3層ニューラルネットワーク"""

    def __init__(self, input_size, hidden1_size, hidden2_size, output_size, learning_rate=0.1):
        # 重みの初期化
        self.W1 = np.random.randn(input_size, hidden1_size) * 0.1
        self.b1 = np.zeros(hidden1_size)

        self.W2 = np.random.randn(hidden1_size, hidden2_size) * 0.1
        self.b2 = np.zeros(hidden2_size)

        self.W3 = np.random.randn(hidden2_size, output_size) * 0.1
        self.b3 = np.zeros(output_size)

        self.learning_rate = learning_rate

    def forward(self, x):
        """順伝播"""
        # 第1層
        self.z1 = np.dot(x, self.W1) + self.b1
        self.a1 = sigmoid(self.z1)

        # 第2層
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = sigmoid(self.z2)

        # 第3層
        self.z3 = np.dot(self.a2, self.W3) + self.b3
        self.a3 = sigmoid(self.z3)

        return self.a3

    def backward(self, x, y_true, y_pred):
        """誤差逆伝播法"""
        batch_size = x.shape[0]

        # 出力層
        delta3 = (y_pred - y_true) * sigmoid_derivative(self.z3)
        dW3 = np.dot(self.a2.T, delta3) / batch_size
        db3 = np.sum(delta3, axis=0) / batch_size

        # 第2隠れ層
        delta2 = np.dot(delta3, self.W3.T) * sigmoid_derivative(self.z2)
        dW2 = np.dot(self.a1.T, delta2) / batch_size
        db2 = np.sum(delta2, axis=0) / batch_size

        # 第1隠れ層
        delta1 = np.dot(delta2, self.W2.T) * sigmoid_derivative(self.z1)
        dW1 = np.dot(x.T, delta1) / batch_size
        db1 = np.sum(delta1, axis=0) / batch_size

        # パラメータ更新
        self.W1 -= self.learning_rate * dW1
        self.b1 -= self.learning_rate * db1
        self.W2 -= self.learning_rate * dW2
        self.b2 -= self.learning_rate * db2
        self.W3 -= self.learning_rate * dW3
        self.b3 -= self.learning_rate * db3

# テスト
print("=== 3層ニューラルネットワーク ===")
net3 = ThreeLayerNet(input_size=2, hidden1_size=4, hidden2_size=4, output_size=1, learning_rate=0.5)

# XOR問題で学習
for epoch in range(3000):
    y_pred = net3.forward(X)
    net3.backward(X, y, y_pred)

    if epoch % 500 == 0:
        loss = np.mean((y - y_pred) ** 2)
        print(f"Epoch {epoch:4d}: Loss = {loss:.6f}")

# 最終結果
print("\n=== 最終予測 ===")
final_pred = net3.forward(X)
for i in range(len(X)):
    print(f"入力: {X[i]} → 予測: {final_pred[i][0]:.4f} (正解: {y[i][0]})")
</code></pre>

</details>

<h3>問題4（難易度：hard）</h3>
<p>AND、OR、NANDの3つの論理ゲートを同時に学習するニューラルネットワークを実装してください（マルチタスク学習）。</p>

<details>
<summary>ヒント</summary>

<ul>
<li>出力層を3ユニットにする</li>
<li>各出力がAND、OR、NANDに対応</li>
</ul>

</details>

<details>
<summary>解答例</summary>

<pre><code class="language-python"># データ準備
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_multi = np.array([
    [0, 0, 1],  # AND=0, OR=0, NAND=1
    [0, 1, 1],  # AND=0, OR=1, NAND=1
    [0, 1, 1],  # AND=0, OR=1, NAND=1
    [1, 1, 0]   # AND=1, OR=1, NAND=0
])

# マルチタスクネットワーク
net_multi = TwoLayerNetWithBackprop(input_size=2, hidden_size=6, output_size=3, learning_rate=0.5)

print("=== マルチタスク学習（AND, OR, NAND） ===")

# 学習
for epoch in range(5000):
    y_pred = net_multi.forward(X)
    net_multi.backward(X, y_multi, y_pred)

    if epoch % 1000 == 0:
        loss = np.mean((y_multi - y_pred) ** 2)
        print(f"Epoch {epoch:4d}: Loss = {loss:.6f}")

# 最終結果
print("\n=== 最終予測 ===")
print("入力  | AND予測 | OR予測  | NAND予測 | AND正解 | OR正解  | NAND正解")
print("-" * 75)
final_pred = net_multi.forward(X)
for i in range(len(X)):
    print(f"{X[i]} | {final_pred[i][0]:.4f}  | {final_pred[i][1]:.4f}  | {final_pred[i][2]:.4f}   | "
          f"{y_multi[i][0]}       | {y_multi[i][1]}       | {y_multi[i][2]}")
</code></pre>

</details>

<h3>問題5（難易度：hard）</h3>
<p>学習率スケジューリング（学習率を徐々に小さくする）を実装してください。</p>

<details>
<summary>解答例</summary>

<pre><code class="language-python">def learning_rate_decay(initial_lr, epoch, decay_rate=0.95, decay_step=100):
    """
    学習率の減衰

    Args:
        initial_lr: 初期学習率
        epoch: 現在のエポック
        decay_rate: 減衰率
        decay_step: 減衰のステップ

    Returns:
        減衰後の学習率
    """
    return initial_lr * (decay_rate ** (epoch // decay_step))

class TwoLayerNetWithLRScheduling(TwoLayerNetWithBackprop):
    """学習率スケジューリング付きネットワーク"""

    def __init__(self, input_size, hidden_size, output_size, initial_lr=0.5, decay_rate=0.95):
        super().__init__(input_size, hidden_size, output_size, initial_lr)
        self.initial_lr = initial_lr
        self.decay_rate = decay_rate

    def train_with_scheduling(self, X, y, epochs=5000):
        """学習率スケジューリング付き学習"""
        losses = []

        for epoch in range(epochs):
            # 学習率の更新
            self.learning_rate = learning_rate_decay(
                self.initial_lr, epoch, self.decay_rate, decay_step=500
            )

            # 順伝播
            y_pred = self.forward(X)

            # 損失
            loss = np.mean((y - y_pred) ** 2)
            losses.append(loss)

            # 逆伝播
            self.backward(X, y, y_pred)

            if epoch % 500 == 0:
                print(f"Epoch {epoch:4d}: LR = {self.learning_rate:.6f}, Loss = {loss:.6f}")

        return losses

# テスト
print("=== 学習率スケジューリング ===")
net_sched = TwoLayerNetWithLRScheduling(input_size=2, hidden_size=4, output_size=1,
                                        initial_lr=1.0, decay_rate=0.9)
losses_sched = net_sched.train_with_scheduling(X, y, epochs=5000)
</code></pre>

</details>

<hr>

<h2>参考文献</h2>

<ol>
<li>Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). "Learning representations by back-propagating errors." <em>Nature</em>, 323(6088), 533-536.</li>
<li>LeCun, Y., Bengio, Y., & Hinton, G. (2015). "Deep learning." <em>Nature</em>, 521(7553), 436-444.</li>
<li>Goodfellow, I., Bengio, Y., & Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</li>
<li>斎藤康毅 (2016). 『ゼロから作るDeep Learning』オライリージャパン.</li>
</ol>

<div class="navigation">
    <a href="chapter1-perceptron.html" class="nav-button">← 前の章: パーセプトロン</a>
    <a href="chapter3-activation-optimization.html" class="nav-button">次の章: 活性化関数と最適化 →</a>
</div>

    </main>

    <footer>
        <p><strong>作成者</strong>: AI Terakoya Content Team</p>
        <p><strong>バージョン</strong>: 1.0 | <strong>作成日</strong>: 2025-10-20</p>
        <p><strong>ライセンス</strong>: Creative Commons BY 4.0</p>
        <p>© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
