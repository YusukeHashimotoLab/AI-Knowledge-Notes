<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬3ç« ï¼šLIMEã¨ãã®ä»–ã®è§£é‡ˆæ‰‹æ³• - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬3ç« ï¼šLIMEã¨ãã®ä»–ã®è§£é‡ˆæ‰‹æ³•</h1>
            <p class="subtitle">å±€æ‰€çš„ãƒ»å¤§åŸŸçš„è§£é‡ˆã®ãŸã‚ã®å¤šæ§˜ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 30-35åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 10å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… LIMEã®åŸç†ã¨å±€æ‰€ç·šå½¢è¿‘ä¼¼ã®ä»•çµ„ã¿ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Permutation Importanceã§ãƒ¢ãƒ‡ãƒ«éä¾å­˜ã®ç‰¹å¾´é‡é‡è¦åº¦ã‚’è¨ˆç®—ã§ãã‚‹</li>
<li>âœ… Partial Dependence Plotsï¼ˆPDPï¼‰ã§ç‰¹å¾´é‡ã®å½±éŸ¿ã‚’å¯è¦–åŒ–ã§ãã‚‹</li>
<li>âœ… Anchorsã‚„åäº‹å®Ÿçš„èª¬æ˜ãªã©æœ€æ–°ã®æ‰‹æ³•ã‚’æ´»ç”¨ã§ãã‚‹</li>
<li>âœ… SHAP vs LIMEãªã©æ‰‹æ³•ã®ä½¿ã„åˆ†ã‘ãŒã§ãã‚‹</li>
<li>âœ… è¨ˆç®—ã‚³ã‚¹ãƒˆã¨è§£é‡ˆã®ç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ç†è§£ã™ã‚‹</li>
</ul>

<hr>

<h2>3.1 LIMEï¼ˆLocal Interpretable Model-agnostic Explanationsï¼‰</h2>

<h3>LIMEã¨ã¯</h3>
<p><strong>LIMEï¼ˆLocal Interpretable Model-agnostic Explanationsï¼‰</strong>ã¯ã€ä»»æ„ã®ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å€‹åˆ¥äºˆæ¸¬ã‚’ã€å±€æ‰€çš„ã«è§£é‡ˆå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã§è¿‘ä¼¼ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<blockquote>
<p>ã€Œè¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã‚‚ã€ã‚ã‚‹1ç‚¹ã®å‘¨è¾ºã§ã¯å˜ç´”ãªç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§è¿‘ä¼¼ã§ãã‚‹ã€</p>
</blockquote>

<h3>LIMEã®åŸºæœ¬åŸç†</h3>

<div class="mermaid">
graph LR
    A[å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ] --> B[å‘¨è¾ºã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°]
    B --> C[ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬]
    C --> D[è·é›¢é‡ã¿ä»˜ã‘]
    D --> E[ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§è¿‘ä¼¼]
    E --> F[ç‰¹å¾´é‡ã®é‡è¦åº¦]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#ffebee
    style F fill:#c8e6c9
</div>

<h4>æ•°å­¦çš„å®šå¼åŒ–</h4>

<p>LIMEã¯ä»¥ä¸‹ã®æœ€é©åŒ–å•é¡Œã‚’è§£ãã¾ã™ï¼š</p>

<p>$$
\xi(x) = \arg\min_{g \in G} \mathcal{L}(f, g, \pi_x) + \Omega(g)
$$</p>

<ul>
<li>$f$: ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«</li>
<li>$g$: è§£é‡ˆå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ï¼ˆç·šå½¢ãƒ¢ãƒ‡ãƒ«ãªã©ï¼‰</li>
<li>$\mathcal{L}$: æå¤±é–¢æ•°ï¼ˆ$f$ã¨$g$ã®äºˆæ¸¬ã®é•ã„ï¼‰</li>
<li>$\pi_x$: å…ƒã®ãƒ‡ãƒ¼ã‚¿ç‚¹$x$ã‹ã‚‰ã®è·é›¢ã«åŸºã¥ãé‡ã¿</li>
<li>$\Omega(g)$: ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•ã®ãƒšãƒŠãƒ«ãƒ†ã‚£</li>
</ul>

<h3>LIMEå®Ÿè£…ï¼šè¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from lime import lime_tabular

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼‰
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

print(f"ãƒ¢ãƒ‡ãƒ«ç²¾åº¦: {model.score(X_test, y_test):.3f}")

# LIME Explainerã®ä½œæˆ
explainer = lime_tabular.LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=X_train.columns.tolist(),
    class_names=['malignant', 'benign'],
    mode='classification'
)

# 1ã¤ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’èª¬æ˜
sample_idx = 0
sample = X_test.iloc[sample_idx].values

# èª¬æ˜ã‚’ç”Ÿæˆ
explanation = explainer.explain_instance(
    data_row=sample,
    predict_fn=model.predict_proba,
    num_features=10
)

print("\n=== LIMEèª¬æ˜ ===")
print(f"äºˆæ¸¬ã‚¯ãƒ©ã‚¹: {data.target_names[model.predict([sample])[0]]}")
print(f"äºˆæ¸¬ç¢ºç‡: {model.predict_proba([sample])[0]}")
print("\nç‰¹å¾´é‡ã®å¯„ä¸:")
for feature, weight in explanation.as_list():
    print(f"  {feature}: {weight:+.4f}")

# å¯è¦–åŒ–
explanation.as_pyplot_figure()
plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>ãƒ¢ãƒ‡ãƒ«ç²¾åº¦: 0.965

=== LIMEèª¬æ˜ ===
äºˆæ¸¬ã‚¯ãƒ©ã‚¹: benign
äºˆæ¸¬ç¢ºç‡: [0.03 0.97]

ç‰¹å¾´é‡ã®å¯„ä¸:
  worst concave points <= 0.10: +0.2845
  worst radius <= 13.43: +0.1234
  worst perimeter <= 86.60: +0.0987
  mean concave points <= 0.05: +0.0765
  worst area <= 549.20: +0.0543
</code></pre>

<blockquote>
<p><strong>é‡è¦</strong>: LIMEã¯å±€æ‰€çš„ãªèª¬æ˜ã§ã‚ã‚Šã€ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®æŒ™å‹•ã‚’ç¤ºã™ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</p>
</blockquote>

<h3>LIMEã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•</h3>

<h4>ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹</h4>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier

# ã‚·ãƒ³ãƒ—ãƒ«ãª2æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿
np.random.seed(42)
X, y = make_classification(
    n_samples=200, n_features=2, n_informative=2,
    n_redundant=0, n_clusters_per_class=1, random_state=42
)

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = RandomForestClassifier(n_estimators=50, random_state=42)
model.fit(X, y)

# èª¬æ˜ã—ãŸã„ã‚µãƒ³ãƒ—ãƒ«
sample = X[0]

# LIMEã‚¹ã‚¿ã‚¤ãƒ«ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæ­£è¦åˆ†å¸ƒã§è¿‘å‚ç”Ÿæˆï¼‰
n_samples = 1000
noise_scale = 0.5

# ã‚µãƒ³ãƒ—ãƒ«å‘¨è¾ºã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
samples = np.random.normal(
    loc=sample,
    scale=noise_scale,
    size=(n_samples, 2)
)

# ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
predictions = model.predict_proba(samples)[:, 1]

# è·é›¢è¨ˆç®—ï¼ˆãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ï¼‰
distances = np.sqrt(np.sum((samples - sample)**2, axis=1))

# ã‚«ãƒ¼ãƒãƒ«é‡ã¿ï¼ˆè·é›¢ã«åæ¯”ä¾‹ï¼‰
kernel_width = 0.75
weights = np.exp(-(distances**2) / (kernel_width**2))

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# å…ƒã®ãƒ‡ãƒ¼ã‚¿ç©ºé–“
axes[0].scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm',
                alpha=0.5, edgecolors='black')
axes[0].scatter(sample[0], sample[1], color='green',
                s=300, marker='*', edgecolors='black', linewidth=2,
                label='èª¬æ˜å¯¾è±¡ã‚µãƒ³ãƒ—ãƒ«')
axes[0].set_xlabel('Feature 1')
axes[0].set_ylabel('Feature 2')
axes[0].set_title('å…ƒã®ãƒ‡ãƒ¼ã‚¿ç©ºé–“', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸç‚¹
scatter = axes[1].scatter(samples[:, 0], samples[:, 1],
                         c=predictions, cmap='coolwarm',
                         alpha=0.4, s=20, edgecolors='none')
axes[1].scatter(sample[0], sample[1], color='green',
                s=300, marker='*', edgecolors='black', linewidth=2)
axes[1].set_xlabel('Feature 1')
axes[1].set_ylabel('Feature 2')
axes[1].set_title('ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸè¿‘å‚ç‚¹', fontsize=14)
axes[1].grid(True, alpha=0.3)
plt.colorbar(scatter, ax=axes[1], label='äºˆæ¸¬ç¢ºç‡')

# é‡ã¿ä»˜ã‘ã•ã‚ŒãŸç‚¹
scatter2 = axes[2].scatter(samples[:, 0], samples[:, 1],
                          c=predictions, cmap='coolwarm',
                          alpha=weights, s=weights*100, edgecolors='none')
axes[2].scatter(sample[0], sample[1], color='green',
                s=300, marker='*', edgecolors='black', linewidth=2)
axes[2].set_xlabel('Feature 1')
axes[2].set_ylabel('Feature 2')
axes[2].set_title('è·é›¢é‡ã¿ä»˜ã‘ï¼ˆè¿‘ã„ã»ã©å¤§ããï¼‰', fontsize=14)
axes[2].grid(True, alpha=0.3)
plt.colorbar(scatter2, ax=axes[2], label='äºˆæ¸¬ç¢ºç‡')

plt.tight_layout()
plt.show()

print(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ•°: {n_samples}")
print(f"å¹³å‡è·é›¢: {distances.mean():.3f}")
print(f"æœ€å°/æœ€å¤§é‡ã¿: {weights.min():.4f} / {weights.max():.4f}")
</code></pre>

<h3>å®Œå…¨ãªLIMEå®Ÿè£…ä¾‹</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.linear_model import Ridge
from sklearn.metrics.pairwise import rbf_kernel

class SimpleLIME:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªLIMEå®Ÿè£…"""

    def __init__(self, kernel_width=0.75, n_samples=5000):
        self.kernel_width = kernel_width
        self.n_samples = n_samples

    def explain_instance(self, model, instance, X_train,
                        feature_names=None, n_features=10):
        """
        å€‹åˆ¥ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’èª¬æ˜

        Parameters:
        -----------
        model : è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆpredict_probaãƒ¡ã‚½ãƒƒãƒ‰å¿…é ˆï¼‰
        instance : èª¬æ˜ã—ãŸã„ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ1D arrayï¼‰
        X_train : è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆçµ±è¨ˆæƒ…å ±ç”¨ï¼‰
        feature_names : ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
        n_features : ä¸Šä½ä½•å€‹ã®ç‰¹å¾´é‡ã‚’è¿”ã™ã‹

        Returns:
        --------
        explanations : ç‰¹å¾´é‡ã¨é‡è¦åº¦ã®ãƒªã‚¹ãƒˆ
        """
        # è¿‘å‚ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        samples = self._sample_around_instance(instance, X_train)

        # ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
        predictions = model.predict_proba(samples)[:, 1]

        # è·é›¢ãƒ™ãƒ¼ã‚¹ã®é‡ã¿è¨ˆç®—
        distances = np.sqrt(np.sum((samples - instance)**2, axis=1))
        weights = np.exp(-(distances**2) / (self.kernel_width**2))

        # ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§è¿‘ä¼¼
        linear_model = Ridge(alpha=1.0)
        linear_model.fit(samples, predictions, sample_weight=weights)

        # ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å–å¾—
        feature_importance = linear_model.coef_

        # ç‰¹å¾´é‡åã®è¨­å®š
        if feature_names is None:
            feature_names = [f'Feature {i}' for i in range(len(instance))]

        # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
        sorted_idx = np.argsort(np.abs(feature_importance))[::-1][:n_features]

        explanations = [
            (feature_names[idx], feature_importance[idx])
            for idx in sorted_idx
        ]

        return explanations, linear_model.score(samples, predictions,
                                               sample_weight=weights)

    def _sample_around_instance(self, instance, X_train):
        """ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å‘¨è¾ºã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆã‚’ä½¿ç”¨
        means = X_train.mean(axis=0)
        stds = X_train.std(axis=0)

        # æ­£è¦åˆ†å¸ƒã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        samples = np.random.normal(
            loc=instance,
            scale=stds * 0.5,  # ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´
            size=(self.n_samples, len(instance))
        )

        return samples

# ä½¿ç”¨ä¾‹
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
data = load_breast_cancer()
X, y = data.data, data.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# SimpleLIMEã§èª¬æ˜
lime_explainer = SimpleLIME(kernel_width=0.75, n_samples=5000)
sample = X_test[0]

explanations, r2_score = lime_explainer.explain_instance(
    model=model,
    instance=sample,
    X_train=X_train,
    feature_names=data.feature_names,
    n_features=10
)

print("=== SimpleLIMEèª¬æ˜ ===")
print(f"å±€æ‰€ãƒ¢ãƒ‡ãƒ«ã®RÂ²ã‚¹ã‚³ã‚¢: {r2_score:.3f}")
print("\nç‰¹å¾´é‡ã®é‡è¦åº¦:")
for feature, importance in explanations:
    print(f"  {feature}: {importance:+.4f}")
</code></pre>

<hr>

<h2>3.2 Permutation Importance</h2>

<h3>Permutation Importanceã¨ã¯</h3>

<p><strong>Permutation Importanceï¼ˆé †åˆ—é‡è¦åº¦ï¼‰</strong>ã¯ã€å„ç‰¹å¾´é‡ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ãŸéš›ã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®ä½ä¸‹ã‚’æ¸¬å®šã™ã‚‹ã€ãƒ¢ãƒ‡ãƒ«éä¾å­˜ã®ç‰¹å¾´é‡é‡è¦åº¦è¨ˆç®—æ‰‹æ³•ã§ã™ã€‚</p>

<h4>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h4>

<ol>
<li>ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’è¨ˆç®—</li>
<li>å„ç‰¹å¾´é‡ã«ã¤ã„ã¦ï¼š
<ul>
<li>ãã®ç‰¹å¾´é‡ã®å€¤ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«</li>
<li>ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’å†è¨ˆç®—</li>
<li>æ€§èƒ½ã®ä½ä¸‹é‡ = é‡è¦åº¦</li>
</ul></li>
<li>å…ƒã«æˆ»ã—ã¦æ¬¡ã®ç‰¹å¾´é‡ã¸</li>
</ol>

<div class="mermaid">
graph TD
    A[å…ƒã®ãƒ‡ãƒ¼ã‚¿] --> B[ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½æ¸¬å®š]
    B --> C[ç‰¹å¾´é‡1ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«]
    C --> D[æ€§èƒ½æ¸¬å®š]
    D --> E[é‡è¦åº¦ = æ€§èƒ½ä½ä¸‹]
    E --> F[ç‰¹å¾´é‡2ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«]
    F --> G[...]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#c8e6c9
</div>

<h3>scikit-learnã§ã®å®Ÿè£…</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import permutation_importance

# ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆç³–å°¿ç—…ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰
data = load_diabetes()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½
baseline_score = model.score(X_test, y_test)
print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³RÂ²ã‚¹ã‚³ã‚¢: {baseline_score:.3f}")

# Permutation Importanceã®è¨ˆç®—
perm_importance = permutation_importance(
    model, X_test, y_test,
    n_repeats=30,  # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã‚’30å›ç¹°ã‚Šè¿”ã—
    random_state=42,
    n_jobs=-1
)

# çµæœã®æ•´ç†
importance_df = pd.DataFrame({
    'feature': X.columns,
    'importance_mean': perm_importance.importances_mean,
    'importance_std': perm_importance.importances_std
}).sort_values('importance_mean', ascending=False)

print("\n=== Permutation Importance ===")
print(importance_df)

# å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(10, 6))

y_pos = np.arange(len(importance_df))
ax.barh(y_pos, importance_df['importance_mean'],
        xerr=importance_df['importance_std'],
        align='center', alpha=0.7, edgecolor='black')
ax.set_yticks(y_pos)
ax.set_yticklabels(importance_df['feature'])
ax.invert_yaxis()
ax.set_xlabel('Importance (RÂ² decrease)')
ax.set_title('Permutation Importance', fontsize=14)
ax.grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()
</code></pre>

<h3>ç‹¬è‡ªå®Ÿè£…ï¼šPermutation Importance</h3>

<pre><code class="language-python">import numpy as np
from sklearn.metrics import r2_score, accuracy_score

def custom_permutation_importance(model, X, y, metric='r2', n_repeats=10):
    """
    Permutation Importanceã®ã‚«ã‚¹ã‚¿ãƒ å®Ÿè£…

    Parameters:
    -----------
    model : è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
    X : ç‰¹å¾´é‡ï¼ˆDataFrame or ndarrayï¼‰
    y : ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
    metric : è©•ä¾¡æŒ‡æ¨™ ('r2' or 'accuracy')
    n_repeats : å„ç‰¹å¾´é‡ã®ã‚·ãƒ£ãƒƒãƒ•ãƒ«ç¹°ã‚Šè¿”ã—å›æ•°

    Returns:
    --------
    importances : å„ç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆå¹³å‡ã¨æ¨™æº–åå·®ï¼‰
    """
    X_array = X.values if hasattr(X, 'values') else X
    n_features = X_array.shape[1]

    # ãƒ¡ãƒˆãƒªãƒƒã‚¯é–¢æ•°ã®é¸æŠ
    if metric == 'r2':
        score_func = r2_score
        predictions = model.predict(X_array)
    elif metric == 'accuracy':
        score_func = accuracy_score
        predictions = model.predict(X_array)
    else:
        raise ValueError(f"Unsupported metric: {metric}")

    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¹ã‚³ã‚¢
    baseline_score = score_func(y, predictions)

    # å„ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’è¨ˆç®—
    importances = np.zeros((n_features, n_repeats))

    for feature_idx in range(n_features):
        for repeat in range(n_repeats):
            # ç‰¹å¾´é‡ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
            X_permuted = X_array.copy()
            np.random.shuffle(X_permuted[:, feature_idx])

            # äºˆæ¸¬ã¨è©•ä¾¡
            if metric == 'r2':
                perm_predictions = model.predict(X_permuted)
            else:
                perm_predictions = model.predict(X_permuted)

            perm_score = score_func(y, perm_predictions)

            # ã‚¹ã‚³ã‚¢ã®ä½ä¸‹ = é‡è¦åº¦
            importances[feature_idx, repeat] = baseline_score - perm_score

    # çµ±è¨ˆé‡ã‚’è¨ˆç®—
    result = {
        'importances_mean': importances.mean(axis=1),
        'importances_std': importances.std(axis=1),
        'importances': importances
    }

    return result

# ä½¿ç”¨ä¾‹
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier

data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# ã‚«ã‚¹ã‚¿ãƒ å®Ÿè£…ã§è¨ˆç®—
custom_perm = custom_permutation_importance(
    model, X_test, y_test,
    metric='accuracy',
    n_repeats=30
)

# çµæœã®æ•´ç†
results_df = pd.DataFrame({
    'feature': X.columns,
    'importance_mean': custom_perm['importances_mean'],
    'importance_std': custom_perm['importances_std']
}).sort_values('importance_mean', ascending=False)

print("=== ã‚«ã‚¹ã‚¿ãƒ Permutation Importance ===")
print(results_df.head(10))
</code></pre>

<h3>Permutation Importanceã¨SHAPã®æ¯”è¼ƒ</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import permutation_importance
import shap

# ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«
data = load_diabetes()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X, y)

# 1. Permutation Importance
perm_imp = permutation_importance(
    model, X, y, n_repeats=30, random_state=42
)

# 2. SHAPå€¤
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X)
shap_importance = np.abs(shap_values).mean(axis=0)

# 3. Tree Feature Importanceï¼ˆæ¯”è¼ƒç”¨ï¼‰
tree_importance = model.feature_importances_

# æ¯”è¼ƒå¯è¦–åŒ–
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

methods = [
    ('Permutation\nImportance', perm_imp.importances_mean),
    ('SHAP\nImportance', shap_importance),
    ('Tree Feature\nImportance', tree_importance)
]

for ax, (title, importance) in zip(axes, methods):
    sorted_idx = np.argsort(importance)
    y_pos = np.arange(len(sorted_idx))

    ax.barh(y_pos, importance[sorted_idx], alpha=0.7, edgecolor='black')
    ax.set_yticks(y_pos)
    ax.set_yticklabels(X.columns[sorted_idx])
    ax.set_xlabel('Importance')
    ax.set_title(title, fontsize=14)
    ax.grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

# ç›¸é–¢åˆ†æ
print("=== æ‰‹æ³•é–“ã®ç›¸é–¢ ===")
comparison_df = pd.DataFrame({
    'Permutation': perm_imp.importances_mean,
    'SHAP': shap_importance,
    'Tree': tree_importance
})
print(comparison_df.corr())
</code></pre>

<hr>

<h2>3.3 Partial Dependence Plotsï¼ˆPDPï¼‰</h2>

<h3>PDPã¨ã¯</h3>

<p><strong>Partial Dependence Plotï¼ˆéƒ¨åˆ†ä¾å­˜ãƒ—ãƒ­ãƒƒãƒˆï¼‰</strong>ã¯ã€ç‰¹å¾´é‡ãŒãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã«ä¸ãˆã‚‹å¹³å‡çš„ãªå½±éŸ¿ã‚’å¯è¦–åŒ–ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<h4>æ•°å­¦çš„å®šç¾©</h4>

<p>ç‰¹å¾´é‡$x_S$ã«å¯¾ã™ã‚‹éƒ¨åˆ†ä¾å­˜é–¢æ•°ï¼š</p>

<p>$$
\hat{f}_{x_S}(x_S) = \mathbb{E}_{x_C}[\hat{f}(x_S, x_C)] = \frac{1}{n}\sum_{i=1}^{n}\hat{f}(x_S, x_C^{(i)})
$$</p>

<ul>
<li>$x_S$: å¯¾è±¡ã®ç‰¹å¾´é‡</li>
<li>$x_C$: ãã®ä»–ã®ç‰¹å¾´é‡</li>
<li>$\hat{f}$: ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬é–¢æ•°</li>
</ul>

<h3>1æ¬¡å…ƒPDPã®å®Ÿè£…</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.inspection import PartialDependenceDisplay

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
data = load_diabetes()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = GradientBoostingRegressor(n_estimators=100, random_state=42)
model.fit(X, y)

# PDPã®è¨ˆç®—ã¨å¯è¦–åŒ–
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

features_to_plot = ['age', 'bmi', 's5', 'bp', 's1', 's3']

for idx, feature in enumerate(features_to_plot):
    ax = axes.flatten()[idx]

    # PDPã®è¡¨ç¤º
    display = PartialDependenceDisplay.from_estimator(
        model, X, features=[feature],
        ax=ax, kind='average'
    )

    ax.set_title(f'PDP: {feature}', fontsize=14)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Partial Dependence Plotç”Ÿæˆå®Œäº† ===")
print("å„ç‰¹å¾´é‡ã®å¹³å‡çš„ãªå½±éŸ¿ã‚’å¯è¦–åŒ–")
</code></pre>

<h3>2D PDPï¼ˆç›¸äº’ä½œç”¨ã®å¯è¦–åŒ–ï¼‰</h3>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.inspection import PartialDependenceDisplay

# 2ã¤ã®ç‰¹å¾´é‡ã®ç›¸äº’ä½œç”¨ã‚’å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# 2D PDPã®ä¾‹1: bmi vs s5
display1 = PartialDependenceDisplay.from_estimator(
    model, X, features=[('bmi', 's5')],
    ax=axes[0], kind='average'
)
axes[0].set_title('2D PDP: BMI vs S5 (ç›¸äº’ä½œç”¨)', fontsize=14)

# 2D PDPã®ä¾‹2: age vs bmi
display2 = PartialDependenceDisplay.from_estimator(
    model, X, features=[('age', 'bmi')],
    ax=axes[1], kind='average'
)
axes[1].set_title('2D PDP: Age vs BMI (ç›¸äº’ä½œç”¨)', fontsize=14)

plt.tight_layout()
plt.show()
</code></pre>

<h3>ICEï¼ˆIndividual Conditional Expectationï¼‰</h3>

<p><strong>ICE</strong>ã¯ã€å„ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã®æ¡ä»¶ä»˜ãæœŸå¾…å€¤ã‚’å¯è¦–åŒ–ã—ã€ç•°è³ªæ€§ã‚’æ‰ãˆã¾ã™ã€‚</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.inspection import PartialDependenceDisplay

# ICEãƒ—ãƒ­ãƒƒãƒˆï¼ˆå€‹åˆ¥ã®æ¡ä»¶ä»˜ãæœŸå¾…å€¤ï¼‰
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

features_for_ice = ['bmi', 's5', 'bp']

for ax, feature in zip(axes, features_for_ice):
    # ICEãƒ—ãƒ­ãƒƒãƒˆï¼ˆindividual=Trueï¼‰
    display = PartialDependenceDisplay.from_estimator(
        model, X, features=[feature],
        kind='individual',  # å€‹åˆ¥ã®ç·š
        ax=ax,
        subsample=50,  # 50ã‚µãƒ³ãƒ—ãƒ«ã®ã¿è¡¨ç¤º
        random_state=42
    )

    # PDPã‚‚é‡ã­ã¦è¡¨ç¤º
    display = PartialDependenceDisplay.from_estimator(
        model, X, features=[feature],
        kind='average',  # å¹³å‡ç·š
        ax=ax,
        line_kw={'color': 'red', 'linewidth': 3, 'label': 'PDP (average)'}
    )

    ax.set_title(f'ICE + PDP: {feature}', fontsize=14)
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== ICEãƒ—ãƒ­ãƒƒãƒˆèª¬æ˜ ===")
print("- ç´°ã„ç·š: å€‹åˆ¥ã‚µãƒ³ãƒ—ãƒ«ã®æ¡ä»¶ä»˜ãæœŸå¾…å€¤ï¼ˆICEï¼‰")
print("- å¤ªã„èµ¤ç·š: å¹³å‡çš„ãªåŠ¹æœï¼ˆPDPï¼‰")
print("- ç·šã®ã°ã‚‰ã¤ã = ç•°è³ªæ€§ï¼ˆå€‹ä½“å·®ï¼‰")
</code></pre>

<h3>ã‚«ã‚¹ã‚¿ãƒ PDPå®Ÿè£…</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

def compute_partial_dependence(model, X, feature_idx, grid_resolution=50):
    """
    éƒ¨åˆ†ä¾å­˜ã‚’è¨ˆç®—

    Parameters:
    -----------
    model : è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
    X : ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
    feature_idx : å¯¾è±¡ç‰¹å¾´é‡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    grid_resolution : ã‚°ãƒªãƒƒãƒ‰ã®è§£åƒåº¦

    Returns:
    --------
    grid_values : ã‚°ãƒªãƒƒãƒ‰ç‚¹ã®å€¤
    pd_values : éƒ¨åˆ†ä¾å­˜ã®å€¤
    """
    X_array = X.values if hasattr(X, 'values') else X

    # å¯¾è±¡ç‰¹å¾´é‡ã®ç¯„å›²ã§ã‚°ãƒªãƒƒãƒ‰ç”Ÿæˆ
    feature_min = X_array[:, feature_idx].min()
    feature_max = X_array[:, feature_idx].max()
    grid_values = np.linspace(feature_min, feature_max, grid_resolution)

    # éƒ¨åˆ†ä¾å­˜ã®è¨ˆç®—
    pd_values = []

    for grid_value in grid_values:
        # å…¨ã‚µãƒ³ãƒ—ãƒ«ã®å¯¾è±¡ç‰¹å¾´é‡ã‚’grid_valueã«å›ºå®š
        X_modified = X_array.copy()
        X_modified[:, feature_idx] = grid_value

        # äºˆæ¸¬ã®å¹³å‡
        predictions = model.predict(X_modified)
        pd_values.append(predictions.mean())

    return grid_values, np.array(pd_values)

# ä½¿ç”¨ä¾‹
from sklearn.datasets import load_diabetes
from sklearn.ensemble import GradientBoostingRegressor

data = load_diabetes()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

model = GradientBoostingRegressor(n_estimators=100, random_state=42)
model.fit(X, y)

# ã‚«ã‚¹ã‚¿ãƒ å®Ÿè£…ã§PDPè¨ˆç®—
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

for idx, (ax, feature) in enumerate(zip(axes.flatten(), X.columns[:6])):
    grid, pd_vals = compute_partial_dependence(
        model, X, feature_idx=idx, grid_resolution=100
    )

    ax.plot(grid, pd_vals, linewidth=2, color='blue')
    ax.set_xlabel(feature)
    ax.set_ylabel('Partial Dependence')
    ax.set_title(f'Custom PDP: {feature}', fontsize=14)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<hr>

<h2>3.4 ãã®ä»–ã®è§£é‡ˆæ‰‹æ³•</h2>

<h3>Anchors</h3>

<p><strong>Anchors</strong>ã¯ã€äºˆæ¸¬ã‚’ä¿è¨¼ã™ã‚‹æœ€å°é™ã®ãƒ«ãƒ¼ãƒ«ã‚»ãƒƒãƒˆã‚’è¦‹ã¤ã‘ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<blockquote>
<p>ã€Œã“ã®æ¡ä»¶ãŒæº€ãŸã•ã‚Œã‚Œã°ã€95%ä»¥ä¸Šã®ç¢ºç‡ã§åŒã˜äºˆæ¸¬ã«ãªã‚‹ã€</p>
</blockquote>

<pre><code class="language-python">from anchor import anchor_tabular
import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Anchors Explainerä½œæˆ
explainer = anchor_tabular.AnchorTabularExplainer(
    class_names=data.target_names,
    feature_names=data.feature_names,
    train_data=X_train.values
)

# èª¬æ˜ç”Ÿæˆ
sample_idx = 0
sample = X_test.iloc[sample_idx].values

explanation = explainer.explain_instance(
    data_row=sample,
    classifier_fn=model.predict,
    threshold=0.95  # 95%ã®ä¿¡é ¼åº¦
)

print("=== Anchorsèª¬æ˜ ===")
print(f"äºˆæ¸¬: {data.target_names[model.predict([sample])[0]]}")
print(f"\nAnchor (ç²¾åº¦={explanation.precision():.2f}):")
print('AND'.join(explanation.names()))
print(f"\nã‚«ãƒãƒ¬ãƒƒã‚¸: {explanation.coverage():.2%}")
</code></pre>

<h3>Counterfactual Explanationsï¼ˆåäº‹å®Ÿçš„èª¬æ˜ï¼‰</h3>

<p><strong>Counterfactual Explanations</strong>ã¯ã€ã€Œä½•ã‚’å¤‰ãˆã‚Œã°äºˆæ¸¬ãŒå¤‰ã‚ã‚‹ã‹ã€ã‚’ç¤ºã—ã¾ã™ã€‚</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

def find_counterfactual(model, instance, target_class,
                       X_train, max_iterations=1000,
                       step_size=0.1):
    """
    åäº‹å®Ÿçš„èª¬æ˜ã‚’æ¢ç´¢ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªå‹¾é…ãƒ™ãƒ¼ã‚¹ï¼‰

    Parameters:
    -----------
    model : è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
    instance : å…ƒã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    target_class : ç›®æ¨™ã‚¯ãƒ©ã‚¹
    X_train : è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆç¯„å›²ã®å‚è€ƒç”¨ï¼‰
    max_iterations : æœ€å¤§åå¾©å›æ•°
    step_size : ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º

    Returns:
    --------
    counterfactual : åäº‹å®Ÿçš„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    changes : å¤‰æ›´å†…å®¹
    """
    counterfactual = instance.copy()

    for iteration in range(max_iterations):
        # ç¾åœ¨ã®äºˆæ¸¬
        pred_class = model.predict([counterfactual])[0]

        if pred_class == target_class:
            break

        # ãƒ©ãƒ³ãƒ€ãƒ ã«ç‰¹å¾´é‡ã‚’é¸ã‚“ã§å¤‰æ›´
        feature_idx = np.random.randint(0, len(counterfactual))

        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²å†…ã§ãƒ©ãƒ³ãƒ€ãƒ å¤‰æ›´
        feature_range = X_train.iloc[:, feature_idx]
        new_value = np.random.uniform(
            feature_range.min(),
            feature_range.max()
        )

        counterfactual[feature_idx] = new_value

    # å¤‰æ›´å†…å®¹ã‚’è¨ˆç®—
    changes = {}
    for idx, (orig, cf) in enumerate(zip(instance, counterfactual)):
        if not np.isclose(orig, cf):
            changes[X.columns[idx]] = {
                'original': orig,
                'counterfactual': cf,
                'change': cf - orig
            }

    return counterfactual, changes

# ä½¿ç”¨ä¾‹
sample_idx = 0
sample = X_test.iloc[sample_idx].values
original_pred = model.predict([sample])[0]
target = 1 - original_pred  # åå¯¾ã®ã‚¯ãƒ©ã‚¹

counterfactual, changes = find_counterfactual(
    model, sample, target, X_train,
    max_iterations=5000
)

print("=== Counterfactual Explanation ===")
print(f"å…ƒã®äºˆæ¸¬: {data.target_names[original_pred]}")
print(f"ç›®æ¨™äºˆæ¸¬: {data.target_names[target]}")
print(f"åäº‹å®Ÿå¾Œ: {data.target_names[model.predict([counterfactual])[0]]}")
print(f"\nå¤‰æ›´ãŒå¿…è¦ãªç‰¹å¾´é‡ï¼ˆä¸Šä½5å€‹ï¼‰:")

sorted_changes = sorted(
    changes.items(),
    key=lambda x: abs(x[1]['change']),
    reverse=True
)[:5]

for feature, change_info in sorted_changes:
    print(f"\n{feature}:")
    print(f"  å…ƒã®å€¤: {change_info['original']:.2f}")
    print(f"  å¤‰æ›´å¾Œ: {change_info['counterfactual']:.2f}")
    print(f"  å¤‰åŒ–é‡: {change_info['change']:+.2f}")
</code></pre>

<h3>Feature Ablationï¼ˆç‰¹å¾´é‡å‰Šé™¤ï¼‰</h3>

<p><strong>Feature Ablation</strong>ã¯ã€ç‰¹å¾´é‡ã‚’å‰Šé™¤ã—ãŸéš›ã®æ€§èƒ½å¤‰åŒ–ã‚’æ¸¬å®šã—ã¾ã™ã€‚</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
data = load_diabetes()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X, y)

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½
baseline_score = r2_score(y, model.predict(X))

# å„ç‰¹å¾´é‡ã‚’å‰Šé™¤ã—ãŸéš›ã®æ€§èƒ½
ablation_results = []

for feature in X.columns:
    # ç‰¹å¾´é‡ã‚’å‰Šé™¤
    X_ablated = X.drop(columns=[feature])

    # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
    model_ablated = RandomForestRegressor(n_estimators=100, random_state=42)
    model_ablated.fit(X_ablated, y)

    # æ€§èƒ½æ¸¬å®š
    score = r2_score(y, model_ablated.predict(X_ablated))
    importance = baseline_score - score

    ablation_results.append({
        'feature': feature,
        'score_without': score,
        'importance': importance
    })

# çµæœã®æ•´ç†
ablation_df = pd.DataFrame(ablation_results).sort_values(
    'importance', ascending=False
)

print("=== Feature Ablation Results ===")
print(f"Baseline RÂ²: {baseline_score:.3f}\n")
print(ablation_df)

# å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(10, 6))

y_pos = np.arange(len(ablation_df))
ax.barh(y_pos, ablation_df['importance'], alpha=0.7, edgecolor='black')
ax.set_yticks(y_pos)
ax.set_yticklabels(ablation_df['feature'])
ax.invert_yaxis()
ax.set_xlabel('Importance (RÂ² decrease when removed)')
ax.set_title('Feature Ablation Importance', fontsize=14)
ax.grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()
</code></pre>

<hr>

<h2>3.5 æ‰‹æ³•ã®æ¯”è¼ƒã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h2>

<h3>SHAP vs LIME</h3>

<table>
<thead>
<tr>
<th>è¦³ç‚¹</th>
<th>SHAP</th>
<th>LIME</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ç†è«–çš„åŸºç›¤</strong></td>
<td>ã‚²ãƒ¼ãƒ ç†è«–ï¼ˆShapleyå€¤ï¼‰</td>
<td>å±€æ‰€ç·šå½¢è¿‘ä¼¼</td>
</tr>
<tr>
<td><strong>ä¸€è²«æ€§</strong></td>
<td>é«˜ã„ï¼ˆæ•°å­¦çš„ä¿è¨¼ã‚ã‚Šï¼‰</td>
<td>ä½ã„ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ä¾å­˜ï¼‰</td>
</tr>
<tr>
<td><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆ</strong></td>
<td>é«˜ã„ï¼ˆç‰¹ã«KernelSHAPï¼‰</td>
<td>ä¸­ç¨‹åº¦</td>
</tr>
<tr>
<td><strong>è§£é‡ˆã®ç²’åº¦</strong></td>
<td>å±€æ‰€ãƒ»å¤§åŸŸä¸¡æ–¹</td>
<td>ä¸»ã«å±€æ‰€</td>
</tr>
<tr>
<td><strong>ãƒ¢ãƒ‡ãƒ«éä¾å­˜æ€§</strong></td>
<td>å®Œå…¨ã«éä¾å­˜</td>
<td>å®Œå…¨ã«éä¾å­˜</td>
</tr>
<tr>
<td><strong>å†ç¾æ€§</strong></td>
<td>é«˜ã„</td>
<td>ä¸­ç¨‹åº¦ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰</td>
</tr>
<tr>
<td><strong>ä½¿ã„ã‚„ã™ã•</strong></td>
<td>éå¸¸ã«é«˜ã„</td>
<td>é«˜ã„</td>
</tr>
</tbody>
</table>

<h3>ã‚°ãƒ­ãƒ¼ãƒãƒ« vs ãƒ­ãƒ¼ã‚«ãƒ«è§£é‡ˆ</h3>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>ã‚¿ã‚¤ãƒ—</th>
<th>ç”¨é€”</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>LIME</strong></td>
<td>ãƒ­ãƒ¼ã‚«ãƒ«</td>
<td>å€‹åˆ¥äºˆæ¸¬ã®èª¬æ˜</td>
</tr>
<tr>
<td><strong>SHAP</strong></td>
<td>ä¸¡æ–¹</td>
<td>å€‹åˆ¥ã¨å…¨ä½“ã®ç†è§£</td>
</tr>
<tr>
<td><strong>Permutation Importance</strong></td>
<td>ã‚°ãƒ­ãƒ¼ãƒãƒ«</td>
<td>å…¨ä½“çš„ãªç‰¹å¾´é‡é‡è¦åº¦</td>
</tr>
<tr>
<td><strong>PDP/ICE</strong></td>
<td>ã‚°ãƒ­ãƒ¼ãƒãƒ«</td>
<td>ç‰¹å¾´é‡ã®å¹³å‡çš„å½±éŸ¿</td>
</tr>
<tr>
<td><strong>Anchors</strong></td>
<td>ãƒ­ãƒ¼ã‚«ãƒ«</td>
<td>ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®èª¬æ˜</td>
</tr>
<tr>
<td><strong>Counterfactuals</strong></td>
<td>ãƒ­ãƒ¼ã‚«ãƒ«</td>
<td>å¤‰æ›´ã®ææ¡ˆ</td>
</tr>
</tbody>
</table>

<h3>è¨ˆç®—ã‚³ã‚¹ãƒˆã®æ¯”è¼ƒ</h3>

<pre><code class="language-python">import time
import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.inspection import permutation_importance
import shap
from lime import lime_tabular

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 1ã‚µãƒ³ãƒ—ãƒ«ã®èª¬æ˜ã«ã‹ã‹ã‚‹æ™‚é–“ã‚’æ¸¬å®š
sample = X_test.iloc[0].values

results = {}

# SHAP TreeExplainer
start = time.time()
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test[:10])
results['SHAP (Tree)'] = time.time() - start

# SHAP KernelExplainerï¼ˆé…ã„ï¼‰
start = time.time()
explainer_kernel = shap.KernelExplainer(
    model.predict_proba,
    shap.sample(X_train, 50)
)
shap_kernel = explainer_kernel.shap_values(X_test.iloc[:5])
results['SHAP (Kernel)'] = time.time() - start

# LIME
start = time.time()
lime_explainer = lime_tabular.LimeTabularExplainer(
    X_train.values,
    feature_names=X_train.columns.tolist(),
    class_names=['malignant', 'benign'],
    mode='classification'
)
for i in range(10):
    exp = lime_explainer.explain_instance(
        X_test.iloc[i].values,
        model.predict_proba,
        num_features=10
    )
results['LIME'] = time.time() - start

# Permutation Importance
start = time.time()
perm_imp = permutation_importance(
    model, X_test, y_test,
    n_repeats=10, random_state=42
)
results['Permutation'] = time.time() - start

# çµæœè¡¨ç¤º
print("=== è¨ˆç®—æ™‚é–“ã®æ¯”è¼ƒï¼ˆ10ã‚µãƒ³ãƒ—ãƒ«ï¼‰===")
for method, duration in sorted(results.items(), key=lambda x: x[1]):
    print(f"{method:20s}: {duration:6.2f}ç§’")

# å¯è¦–åŒ–
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(10, 6))

methods = list(results.keys())
times = list(results.values())

colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
bars = ax.barh(methods, times, color=colors, alpha=0.7, edgecolor='black')

ax.set_xlabel('è¨ˆç®—æ™‚é–“ï¼ˆç§’ï¼‰', fontsize=12)
ax.set_title('è§£é‡ˆæ‰‹æ³•ã®è¨ˆç®—ã‚³ã‚¹ãƒˆæ¯”è¼ƒ', fontsize=14)
ax.grid(True, alpha=0.3, axis='x')

# å€¤ã‚’è¡¨ç¤º
for bar, time_val in zip(bars, times):
    ax.text(time_val + 0.1, bar.get_y() + bar.get_height()/2,
            f'{time_val:.2f}s', va='center')

plt.tight_layout()
plt.show()
</code></pre>

<h3>ä½¿ã„åˆ†ã‘ã‚¬ã‚¤ãƒ‰</h3>

<table>
<thead>
<tr>
<th>çŠ¶æ³</th>
<th>æ¨å¥¨æ‰‹æ³•</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td>å€‹åˆ¥äºˆæ¸¬ã®è©³ç´°èª¬æ˜</td>
<td>SHAP (Tree)</td>
<td>é«˜é€Ÿã§ä¸€è²«æ€§ã‚ã‚Š</td>
</tr>
<tr>
<td>ä»»æ„ãƒ¢ãƒ‡ãƒ«ã®å€‹åˆ¥èª¬æ˜</td>
<td>LIME, KernelSHAP</td>
<td>ãƒ¢ãƒ‡ãƒ«éä¾å­˜</td>
</tr>
<tr>
<td>å…¨ä½“çš„ãªç‰¹å¾´é‡é‡è¦åº¦</td>
<td>Permutation, SHAP</td>
<td>ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªç†è§£</td>
</tr>
<tr>
<td>ç‰¹å¾´é‡ã®å½±éŸ¿ã®å¯è¦–åŒ–</td>
<td>PDP/ICE</td>
<td>ç›´æ„Ÿçš„ãªç†è§£</td>
</tr>
<tr>
<td>ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®èª¬æ˜</td>
<td>Anchors</td>
<td>if-thenå½¢å¼</td>
</tr>
<tr>
<td>å¤‰æ›´ææ¡ˆ</td>
<td>Counterfactuals</td>
<td>å®Ÿç”¨çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³</td>
</tr>
<tr>
<td>è¨ˆç®—æ™‚é–“ãŒé™ã‚‰ã‚Œã‚‹</td>
<td>LIME, Tree SHAP</td>
<td>æ¯”è¼ƒçš„é«˜é€Ÿ</td>
</tr>
<tr>
<td>é«˜ç²¾åº¦ãŒå¿…è¦</td>
<td>SHAP</td>
<td>ç†è«–çš„ä¿è¨¼</td>
</tr>
</tbody>
</table>

<h3>ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>

<ol>
<li><p><strong>è¤‡æ•°æ‰‹æ³•ã®ä½µç”¨</strong></p>
<ul>
<li>SHAPï¼ˆå¤§åŸŸï¼‰+ LIMEï¼ˆå±€æ‰€ï¼‰ã§å¤šè§’çš„ã«ç†è§£</li>
<li>PDPï¼ˆå¹³å‡ï¼‰+ ICEï¼ˆå€‹åˆ¥ï¼‰ã§ç•°è³ªæ€§ã‚’æŠŠæ¡</li>
</ul></li>

<li><p><strong>è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã®è€ƒæ…®</strong></p>
<ul>
<li>æœ¬ç•ªç’°å¢ƒ: TreeSHAP, LIME</li>
<li>ç ”ç©¶ãƒ»åˆ†æ: KernelSHAP, å…¨æ‰‹æ³•ä½µç”¨</li>
</ul></li>

<li><p><strong>ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã®çµ±åˆ</strong></p>
<ul>
<li>è§£é‡ˆçµæœã‚’æ¥­å‹™çŸ¥è­˜ã§æ¤œè¨¼</li>
<li>ä¸è‡ªç„¶ãªèª¬æ˜ã¯è¦èª¿æŸ»</li>
</ul></li>

<li><p><strong>å¯è¦–åŒ–ã®å·¥å¤«</strong></p>
<ul>
<li>éå°‚é–€å®¶å‘ã‘ã«ã¯ç°¡æ½”ã«</li>
<li>å°‚é–€å®¶å‘ã‘ã«ã¯è©³ç´°ã«</li>
</ul></li>

<li><p><strong>å†ç¾æ€§ã®ç¢ºä¿</strong></p>
<ul>
<li>random_stateã®å›ºå®š</li>
<li>èª¬æ˜ã®ä¿å­˜ã¨å…±æœ‰</li>
</ul></li>
</ol>

<hr>

<h2>3.6 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>LIME</strong></p>
<ul>
<li>å±€æ‰€ç·šå½¢è¿‘ä¼¼ã«ã‚ˆã‚‹ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹è§£é‡ˆ</li>
<li>ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨é‡ã¿ä»˜ã‘ã®ä»•çµ„ã¿</li>
<li>å®Ÿè£…ã¨å¯è¦–åŒ–ã®æ–¹æ³•</li>
</ul></li>

<li><p><strong>Permutation Importance</strong></p>
<ul>
<li>ãƒ¢ãƒ‡ãƒ«éä¾å­˜ã®ç‰¹å¾´é‡é‡è¦åº¦</li>
<li>ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã«ã‚ˆã‚‹æ€§èƒ½ä½ä¸‹ã®æ¸¬å®š</li>
<li>SHAPã‚„Tree Importanceã¨ã®é•ã„</li>
</ul></li>

<li><p><strong>Partial Dependence Plots</strong></p>
<ul>
<li>ç‰¹å¾´é‡ã®å¹³å‡çš„å½±éŸ¿ã®å¯è¦–åŒ–</li>
<li>2D PDPã«ã‚ˆã‚‹ç›¸äº’ä½œç”¨ã®ç†è§£</li>
<li>ICEã«ã‚ˆã‚‹ç•°è³ªæ€§ã®æ‰ãˆæ–¹</li>
</ul></li>

<li><p><strong>ãã®ä»–ã®æ‰‹æ³•</strong></p>
<ul>
<li>Anchors: ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®èª¬æ˜</li>
<li>Counterfactuals: å¤‰æ›´ææ¡ˆ</li>
<li>Feature Ablation: å‰Šé™¤ã«ã‚ˆã‚‹é‡è¦åº¦æ¸¬å®š</li>
</ul></li>

<li><p><strong>æ‰‹æ³•ã®ä½¿ã„åˆ†ã‘</strong></p>
<ul>
<li>SHAP vs LIMEã®ç‰¹æ€§æ¯”è¼ƒ</li>
<li>è¨ˆç®—ã‚³ã‚¹ãƒˆã¨ç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•</li>
<li>çŠ¶æ³ã«å¿œã˜ãŸæœ€é©ãªæ‰‹æ³•é¸æŠ</li>
</ul></li>
</ol>

<h3>ä¸»è¦ãªæ‰‹æ³•ã®ç‰¹æ€§ã¾ã¨ã‚</h3>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>å¼·ã¿</th>
<th>å¼±ã¿</th>
<th>é©ç”¨å ´é¢</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>LIME</strong></td>
<td>ç†è§£ã—ã‚„ã™ã„ã€é«˜é€Ÿ</td>
<td>ä¸å®‰å®šã€å±€æ‰€ã®ã¿</td>
<td>å€‹åˆ¥äºˆæ¸¬ã®ç°¡æ˜“èª¬æ˜</td>
</tr>
<tr>
<td><strong>SHAP</strong></td>
<td>ç†è«–çš„ä¿è¨¼ã€ä¸€è²«æ€§</td>
<td>è¨ˆç®—ã‚³ã‚¹ãƒˆï¼ˆKernelï¼‰</td>
<td>ç²¾å¯†ãªè§£é‡ˆãŒå¿…è¦</td>
</tr>
<tr>
<td><strong>Permutation</strong></td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ã€ç›´æ„Ÿçš„</td>
<td>ç›¸é–¢ç‰¹å¾´ã§ä¸å®‰å®š</td>
<td>å…¨ä½“çš„é‡è¦åº¦æŠŠæ¡</td>
</tr>
<tr>
<td><strong>PDP/ICE</strong></td>
<td>å¯è¦–åŒ–ãŒç›´æ„Ÿçš„</td>
<td>ç›¸äº’ä½œç”¨ã®é™ç•Œ</td>
<td>ç‰¹å¾´é‡å½±éŸ¿ã®ç†è§£</td>
</tr>
<tr>
<td><strong>Anchors</strong></td>
<td>ãƒ«ãƒ¼ãƒ«å½¢å¼ã€æ˜ç¢º</td>
<td>ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ¶é™</td>
<td>ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹èª¬æ˜</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ç« ã¸</h3>

<p>ç¬¬4ç« ã§ã¯ã€<strong>ç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®è§£é‡ˆ</strong>ã‚’å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>Grad-CAMã«ã‚ˆã‚‹ç”»åƒè§£é‡ˆ</li>
<li>Attentionæ©Ÿæ§‹ã®å¯è¦–åŒ–</li>
<li>BERTãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆ</li>
<li>Integrated Gradients</li>
<li>å®Ÿç”¨çš„ãªå¿œç”¨ä¾‹</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>
<p>LIMEã¨SHAPã®ä¸»ãªé•ã„ã‚’3ã¤æŒ™ã’ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<ol>
<li><p><strong>ç†è«–çš„åŸºç›¤</strong></p>
<ul>
<li>LIME: å±€æ‰€ç·šå½¢è¿‘ä¼¼ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‹ç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼‰</li>
<li>SHAP: ã‚²ãƒ¼ãƒ ç†è«–ã®Shapleyå€¤ï¼ˆå…¬ç†çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰</li>
</ul></li>

<li><p><strong>ä¸€è²«æ€§ã¨å†ç¾æ€§</strong></p>
<ul>
<li>LIME: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ä¾å­˜ã™ã‚‹ãŸã‚ã€å®Ÿè¡Œã”ã¨ã«çµæœãŒå¤‰ã‚ã‚‹å¯èƒ½æ€§</li>
<li>SHAP: æ•°å­¦çš„ã«ä¸€è²«ã—ãŸçµæœãŒä¿è¨¼ã•ã‚Œã‚‹</li>
</ul></li>

<li><p><strong>é©ç”¨ç¯„å›²</strong></p>
<ul>
<li>LIME: ä¸»ã«å±€æ‰€çš„ãªèª¬æ˜ï¼ˆå€‹åˆ¥ã‚µãƒ³ãƒ—ãƒ«ï¼‰</li>
<li>SHAP: å±€æ‰€ã¨å¤§åŸŸã®ä¸¡æ–¹ï¼ˆå€‹åˆ¥ï¼‹å…¨ä½“ã®é‡è¦åº¦ï¼‰</li>
</ul></li>
</ol>

<p><strong>é¸æŠã®ç›®å®‰</strong>ï¼š</p>
<ul>
<li>é€Ÿåº¦é‡è¦–ãƒ»ç°¡æ˜“èª¬æ˜ â†’ LIME</li>
<li>ç²¾åº¦é‡è¦–ãƒ»ç†è«–çš„ä¿è¨¼ â†’ SHAP</li>
<li>ç†æƒ³çš„ã«ã¯ä¸¡æ–¹ã‚’ä½¿ã£ã¦å¤šè§’çš„ã«ç†è§£</li>
</ul>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>Permutation Importanceã‚’æ‰‹å‹•ã§å®Ÿè£…ã—ã€scikit-learnã®çµæœã¨æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.inspection import permutation_importance
from sklearn.metrics import accuracy_score

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
data = load_iris()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# æ‰‹å‹•å®Ÿè£…
def manual_permutation_importance(model, X, y, n_repeats=10):
    """Permutation Importanceã®æ‰‹å‹•å®Ÿè£…"""
    baseline_score = accuracy_score(y, model.predict(X))
    n_features = X.shape[1]
    importances = np.zeros((n_features, n_repeats))

    for feature_idx in range(n_features):
        for repeat in range(n_repeats):
            # ç‰¹å¾´é‡ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
            X_permuted = X.copy()
            X_permuted.iloc[:, feature_idx] = np.random.permutation(
                X_permuted.iloc[:, feature_idx]
            )

            # ã‚¹ã‚³ã‚¢è¨ˆç®—
            perm_score = accuracy_score(y, model.predict(X_permuted))
            importances[feature_idx, repeat] = baseline_score - perm_score

    return {
        'importances_mean': importances.mean(axis=1),
        'importances_std': importances.std(axis=1)
    }

# æ‰‹å‹•å®Ÿè£…ã§è¨ˆç®—
manual_result = manual_permutation_importance(
    model, X_test, y_test, n_repeats=30
)

# scikit-learnå®Ÿè£…
sklearn_result = permutation_importance(
    model, X_test, y_test, n_repeats=30, random_state=42
)

# æ¯”è¼ƒ
comparison_df = pd.DataFrame({
    'Feature': X.columns,
    'Manual_Mean': manual_result['importances_mean'],
    'Manual_Std': manual_result['importances_std'],
    'Sklearn_Mean': sklearn_result.importances_mean,
    'Sklearn_Std': sklearn_result.importances_std
}).sort_values('Manual_Mean', ascending=False)

print("=== Permutation Importanceæ¯”è¼ƒ ===")
print(comparison_df)

# ç›¸é–¢ãƒã‚§ãƒƒã‚¯
correlation = np.corrcoef(
    manual_result['importances_mean'],
    sklearn_result.importances_mean
)[0, 1]
print(f"\næ‰‹å‹•å®Ÿè£…ã¨sklearnå®Ÿè£…ã®ç›¸é–¢: {correlation:.4f}")
print("ï¼ˆ1ã«è¿‘ã„ã»ã©ä¸€è‡´ï¼‰")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== Permutation Importanceæ¯”è¼ƒ ===
                   Feature  Manual_Mean  Manual_Std  Sklearn_Mean  Sklearn_Std
2       petal length (cm)     0.3156      0.0289        0.3200       0.0265
3        petal width (cm)     0.2933      0.0312        0.2867       0.0298
0       sepal length (cm)     0.0089      0.0145        0.0111       0.0134
1        sepal width (cm)     0.0067      0.0123        0.0044       0.0098

æ‰‹å‹•å®Ÿè£…ã¨sklearnå®Ÿè£…ã®ç›¸é–¢: 0.9987
ï¼ˆ1ã«è¿‘ã„ã»ã©ä¸€è‡´ï¼‰
</code></pre>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>Partial Dependence Plotã¨ICEãƒ—ãƒ­ãƒƒãƒˆã®é•ã„ã‚’èª¬æ˜ã—ã€ã©ã®ã‚ˆã†ãªå ´åˆã«ICEãŒæœ‰ç”¨ã‹è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>é•ã„</strong>ï¼š</p>

<ul>
<li><p><strong>PDPï¼ˆPartial Dependence Plotï¼‰</strong></p>
<ul>
<li>å…¨ã‚µãƒ³ãƒ—ãƒ«ã®å¹³å‡çš„ãªåŠ¹æœã‚’ç¤ºã™</li>
<li>å¼: $\hat{f}_{PDP}(x_s) = \frac{1}{n}\sum_{i=1}^{n}\hat{f}(x_s, x_c^{(i)})$</li>
<li>1æœ¬ã®ç·šã§è¡¨ç¾</li>
</ul></li>

<li><p><strong>ICEï¼ˆIndividual Conditional Expectationï¼‰</strong></p>
<ul>
<li>å„ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã®åŠ¹æœã‚’å€‹åˆ¥ã«ç¤ºã™</li>
<li>å¼: $\hat{f}^{(i)}_{ICE}(x_s) = \hat{f}(x_s, x_c^{(i)})$</li>
<li>næœ¬ã®ç·šï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°åˆ†ï¼‰</li>
</ul></li>
</ul>

<p><strong>ICEãŒæœ‰ç”¨ãªå ´åˆ</strong>ï¼š</p>

<ol>
<li><p><strong>ç•°è³ªæ€§ã®æ¤œå‡º</strong></p>
<ul>
<li>ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ã§ç•°ãªã‚‹åŠ¹æœãŒã‚ã‚‹å ´åˆ</li>
<li>ä¾‹: å¹´é½¢ã®å½±éŸ¿ãŒæ€§åˆ¥ã§ç•°ãªã‚‹</li>
</ul></li>

<li><p><strong>ç›¸äº’ä½œç”¨ã®ç™ºè¦‹</strong></p>
<ul>
<li>ç‰¹å¾´é‡é–“ã®è¤‡é›‘ãªç›¸äº’ä½œç”¨</li>
<li>PDPã§ã¯å¹³å‡åŒ–ã•ã‚Œã¦è¦‹ãˆãªã„</li>
</ul></li>

<li><p><strong>éç·šå½¢æ€§ã®ç†è§£</strong></p>
<ul>
<li>å€‹åˆ¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå¤šæ§˜</li>
<li>å¹³å‡ã§ã¯å˜ç´”ã«è¦‹ãˆã¦ã‚‚å®Ÿã¯è¤‡é›‘</li>
</ul></li>
</ol>

<p><strong>å¯è¦–åŒ–ä¾‹</strong>ï¼š</p>
<pre><code class="language-python">from sklearn.inspection import PartialDependenceDisplay

# PDPã®ã¿ï¼ˆå¹³å‡ï¼‰
PartialDependenceDisplay.from_estimator(
    model, X, features=['age'], kind='average'
)

# ICE + PDPï¼ˆå€‹åˆ¥ï¼‹å¹³å‡ï¼‰
PartialDependenceDisplay.from_estimator(
    model, X, features=['age'],
    kind='both',  # ä¸¡æ–¹è¡¨ç¤º
    subsample=50
)
</code></pre>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã€LIMEã€SHAPã€Permutation Importanceã®3æ‰‹æ³•ã‚’é©ç”¨ã—ã€çµæœã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚ç‰¹å¾´é‡ã®é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãŒç•°ãªã‚‹å ´åˆã€ãã®ç†ç”±ã‚’è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">from sklearn.datasets import load_breast_cancer
data = load_breast_cancer()
X, y = data.data, data.target
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance
import shap
from lime import lime_tabular

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 1. SHAPï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ï¼‰
explainer_shap = shap.TreeExplainer(model)
shap_values = explainer_shap.shap_values(X_test)
shap_importance = np.abs(shap_values[1]).mean(axis=0)

# 2. LIMEï¼ˆè¤‡æ•°ã‚µãƒ³ãƒ—ãƒ«ã®å¹³å‡ï¼‰
explainer_lime = lime_tabular.LimeTabularExplainer(
    X_train.values,
    feature_names=X_train.columns.tolist(),
    class_names=['malignant', 'benign'],
    mode='classification'
)

lime_importances = []
for i in range(min(50, len(X_test))):  # 50ã‚µãƒ³ãƒ—ãƒ«
    exp = explainer_lime.explain_instance(
        X_test.iloc[i].values,
        model.predict_proba,
        num_features=len(X.columns)
    )
    weights = dict(exp.as_list())
    # ç‰¹å¾´é‡åã‚’æŠ½å‡ºï¼ˆæ¡ä»¶éƒ¨åˆ†ã‚’é™¤å»ï¼‰
    feature_weights = {}
    for key, val in weights.items():
        feature_name = key.split('<=')[0].split('>')[0].strip()
        if feature_name in X.columns:
            feature_weights[feature_name] = abs(val)
    lime_importances.append(feature_weights)

# LIMEé‡è¦åº¦ã®å¹³å‡
lime_importance_mean = pd.DataFrame(lime_importances).mean().reindex(X.columns).fillna(0)

# 3. Permutation Importance
perm_importance = permutation_importance(
    model, X_test, y_test, n_repeats=30, random_state=42
)

# çµæœã®çµ±åˆ
comparison_df = pd.DataFrame({
    'Feature': X.columns,
    'SHAP': shap_importance,
    'LIME': lime_importance_mean.values,
    'Permutation': perm_importance.importances_mean
})

# æ­£è¦åŒ–ï¼ˆæ¯”è¼ƒã®ãŸã‚ï¼‰
for col in ['SHAP', 'LIME', 'Permutation']:
    comparison_df[col] = comparison_df[col] / comparison_df[col].sum()

# ãƒ©ãƒ³ã‚­ãƒ³ã‚°
comparison_df['SHAP_Rank'] = comparison_df['SHAP'].rank(ascending=False)
comparison_df['LIME_Rank'] = comparison_df['LIME'].rank(ascending=False)
comparison_df['Perm_Rank'] = comparison_df['Permutation'].rank(ascending=False)

print("=== 3æ‰‹æ³•ã®ç‰¹å¾´é‡é‡è¦åº¦æ¯”è¼ƒï¼ˆä¸Šä½10å€‹ï¼‰===\n")
top_features = comparison_df.nlargest(10, 'SHAP')[
    ['Feature', 'SHAP', 'LIME', 'Permutation',
     'SHAP_Rank', 'LIME_Rank', 'Perm_Rank']
]
print(top_features)

# ç›¸é–¢åˆ†æ
print("\n=== æ‰‹æ³•é–“ã®ç›¸é–¢ ===")
correlation_matrix = comparison_df[['SHAP', 'LIME', 'Permutation']].corr()
print(correlation_matrix)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

for ax, method in zip(axes, ['SHAP', 'LIME', 'Permutation']):
    sorted_df = comparison_df.sort_values(method, ascending=True).tail(10)

    ax.barh(range(len(sorted_df)), sorted_df[method],
            alpha=0.7, edgecolor='black')
    ax.set_yticks(range(len(sorted_df)))
    ax.set_yticklabels(sorted_df['Feature'])
    ax.set_xlabel('æ­£è¦åŒ–é‡è¦åº¦')
    ax.set_title(f'{method}', fontsize=14)
    ax.grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

# ãƒ©ãƒ³ã‚¯å·®åˆ†æ
print("\n=== ãƒ©ãƒ³ã‚¯ã®å¤§ããªå·®ç•°ãŒã‚ã‚‹ç‰¹å¾´é‡ ===")
comparison_df['Rank_Std'] = comparison_df[
    ['SHAP_Rank', 'LIME_Rank', 'Perm_Rank']
].std(axis=1)

disagreement = comparison_df.nlargest(5, 'Rank_Std')[
    ['Feature', 'SHAP_Rank', 'LIME_Rank', 'Perm_Rank', 'Rank_Std']
]
print(disagreement)

print("\n=== è€ƒå¯Ÿ ===")
print("""
ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®é•ã„ãŒç”Ÿã˜ã‚‹ç†ç”±ï¼š

1. **æ¸¬å®šå¯¾è±¡ã®é•ã„**
   - SHAP: å„ç‰¹å¾´é‡ã®å¯„ä¸ï¼ˆã‚²ãƒ¼ãƒ ç†è«–ï¼‰
   - LIME: å±€æ‰€ç·šå½¢è¿‘ä¼¼ã®ä¿‚æ•°
   - Permutation: ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ™‚ã®æ€§èƒ½ä½ä¸‹

2. **å±€æ‰€ vs å¤§åŸŸ**
   - LIME: å±€æ‰€çš„ï¼ˆé¸æŠã—ãŸã‚µãƒ³ãƒ—ãƒ«å‘¨è¾ºï¼‰
   - SHAP/Permutation: ã‚ˆã‚Šå¤§åŸŸçš„

3. **ç‰¹å¾´é‡ã®ç›¸é–¢**
   - ç›¸é–¢ãŒé«˜ã„ç‰¹å¾´é‡ç¾¤ã§ã¯æ‰‹æ³•ã«ã‚ˆã‚Šé‡è¦åº¦ãŒåˆ†æ•£
   - Permutationã¯ç›¸é–¢ã‚’è€ƒæ…®ã—ãªã„

4. **è¨ˆç®—æ–¹æ³•ã®é•ã„**
   - SHAP: ã™ã¹ã¦ã®ç‰¹å¾´é‡ã®çµ„ã¿åˆã‚ã›ã‚’è€ƒæ…®
   - Permutation: 1ã¤ãšã¤ç‹¬ç«‹ã«è©•ä¾¡
   - LIME: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ™ãƒ¼ã‚¹ï¼ˆãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚ã‚Šï¼‰

æ¨å¥¨ï¼šè¤‡æ•°æ‰‹æ³•ã‚’ä½µç”¨ã—ã¦å¤šè§’çš„ã«è§£é‡ˆ
""")
</code></pre>

</details>

<h3>å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>Counterfactual Explanationã‚’ä½¿ã£ã¦ã€äºˆæ¸¬ãŒå¤‰ã‚ã‚‹æœ€å°ã®å¤‰æ›´ã‚’è¦‹ã¤ã‘ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚å¤‰æ›´ã®å¦¥å½“æ€§ã‚’ã©ã®ã‚ˆã†ã«è©•ä¾¡ã™ã¹ãã‹è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

class CounterfactualExplainer:
    """æœ€å°å¤‰æ›´ã®åäº‹å®Ÿçš„èª¬æ˜ã‚’ç”Ÿæˆ"""

    def __init__(self, model, X_train):
        self.model = model
        self.X_train = X_train
        self.feature_ranges = {
            'min': X_train.min(axis=0),
            'max': X_train.max(axis=0),
            'median': X_train.median(axis=0)
        }

    def find_minimal_counterfactual(self, instance, target_class,
                                   max_iterations=1000,
                                   change_penalty=0.1):
        """
        æœ€å°ã®å¤‰æ›´ã§ç›®æ¨™ã‚¯ãƒ©ã‚¹ã‚’é”æˆã™ã‚‹åäº‹å®Ÿã‚’æ¢ç´¢

        Parameters:
        -----------
        instance : å…ƒã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        target_class : ç›®æ¨™ã‚¯ãƒ©ã‚¹
        max_iterations : æœ€å¤§åå¾©å›æ•°
        change_penalty : å¤‰æ›´ã«å¯¾ã™ã‚‹ãƒšãƒŠãƒ«ãƒ†ã‚£

        Returns:
        --------
        best_counterfactual : æœ€è‰¯ã®åäº‹å®Ÿçš„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        changes : å¤‰æ›´å†…å®¹
        metadata : ãƒ¡ã‚¿æƒ…å ±
        """
        best_counterfactual = None
        best_distance = float('inf')

        current = instance.copy()

        for iteration in range(max_iterations):
            # ç¾åœ¨ã®äºˆæ¸¬
            pred_class = self.model.predict([current])[0]

            if pred_class == target_class:
                # ç›®æ¨™é”æˆï¼šè·é›¢ã‚’è¨ˆç®—
                distance = self._compute_distance(instance, current)

                if distance < best_distance:
                    best_distance = distance
                    best_counterfactual = current.copy()

            # ãƒ©ãƒ³ãƒ€ãƒ ã«1ã¤ã®ç‰¹å¾´é‡ã‚’å¤‰æ›´
            feature_idx = np.random.randint(0, len(current))

            # å®Ÿç¾å¯èƒ½ãªç¯„å›²ã§å¤‰æ›´
            feature_range = (
                self.feature_ranges['min'][feature_idx],
                self.feature_ranges['max'][feature_idx]
            )

            # å…ƒã®å€¤ã«è¿‘ã„å€¤ã‚’å„ªå…ˆï¼ˆæ­£è¦åˆ†å¸ƒï¼‰
            new_value = np.random.normal(
                loc=instance[feature_idx],
                scale=(feature_range[1] - feature_range[0]) * 0.1
            )

            # ç¯„å›²å†…ã«ã‚¯ãƒªãƒƒãƒ—
            new_value = np.clip(new_value, feature_range[0], feature_range[1])
            current[feature_idx] = new_value

        if best_counterfactual is None:
            return None, None, {'success': False}

        # å¤‰æ›´å†…å®¹ã®åˆ†æ
        changes = self._analyze_changes(instance, best_counterfactual)

        # ãƒ¡ã‚¿æƒ…å ±
        metadata = {
            'success': True,
            'distance': best_distance,
            'n_changes': len(changes),
            'validity': self._check_validity(best_counterfactual)
        }

        return best_counterfactual, changes, metadata

    def _compute_distance(self, instance1, instance2):
        """L2è·é›¢ï¼ˆæ­£è¦åŒ–ï¼‰"""
        # å„ç‰¹å¾´é‡ã‚’ç¯„å›²ã§æ­£è¦åŒ–
        ranges = self.feature_ranges['max'] - self.feature_ranges['min']
        normalized_diff = (instance1 - instance2) / ranges
        return np.sqrt(np.sum(normalized_diff**2))

    def _analyze_changes(self, original, counterfactual, threshold=0.01):
        """å¤‰æ›´ã‚’åˆ†æ"""
        changes = {}
        for idx, (orig, cf) in enumerate(zip(original, counterfactual)):
            relative_change = abs((cf - orig) / (orig + 1e-10))

            if relative_change > threshold:
                changes[idx] = {
                    'original': orig,
                    'counterfactual': cf,
                    'absolute_change': cf - orig,
                    'relative_change': relative_change
                }
        return changes

    def _check_validity(self, instance):
        """å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆç¯„å›²å†…ã‹ï¼‰"""
        within_range = (
            (instance >= self.feature_ranges['min']).all() and
            (instance <= self.feature_ranges['max']).all()
        )
        return within_range

# ä½¿ç”¨ä¾‹
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Counterfactual Explainer
cf_explainer = CounterfactualExplainer(model, X_train)

# ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ã§è©¦è¡Œ
sample_idx = 0
sample = X_test.iloc[sample_idx].values
original_pred = model.predict([sample])[0]
target_class = 1 - original_pred

print("=== Counterfactual Explanation ===")
print(f"å…ƒã®äºˆæ¸¬: {data.target_names[original_pred]}")
print(f"ç›®æ¨™äºˆæ¸¬: {data.target_names[target_class]}\n")

counterfactual, changes, metadata = cf_explainer.find_minimal_counterfactual(
    sample, target_class, max_iterations=5000
)

if metadata['success']:
    print(f"âœ“ åäº‹å®Ÿçš„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç™ºè¦‹")
    print(f"  è·é›¢: {metadata['distance']:.4f}")
    print(f"  å¤‰æ›´æ•°: {metadata['n_changes']}")
    print(f"  å¦¥å½“æ€§: {metadata['validity']}")

    cf_pred = model.predict([counterfactual])[0]
    print(f"  åäº‹å®Ÿå¾Œã®äºˆæ¸¬: {data.target_names[cf_pred]}")

    print(f"\nå¿…è¦ãªå¤‰æ›´ï¼ˆä¸Šä½5å€‹ï¼‰:")
    sorted_changes = sorted(
        changes.items(),
        key=lambda x: abs(x[1]['absolute_change']),
        reverse=True
    )[:5]

    for idx, change_info in sorted_changes:
        feature_name = X.columns[idx]
        print(f"\n{feature_name}:")
        print(f"  å…ƒ: {change_info['original']:.2f}")
        print(f"  å¤‰æ›´å¾Œ: {change_info['counterfactual']:.2f}")
        print(f"  å¤‰åŒ–: {change_info['absolute_change']:+.2f} "
              f"({change_info['relative_change']:.1%})")

    # å¦¥å½“æ€§è©•ä¾¡
    print("\n=== å¦¥å½“æ€§è©•ä¾¡ ===")
    print("1. å®Ÿç¾å¯èƒ½æ€§: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²å†…ã‹")
    print(f"   â†’ {metadata['validity']}")

    print("\n2. æœ€å°æ€§: å¤‰æ›´ã®æ•°ãŒå°‘ãªã„ã‹")
    print(f"   â†’ {metadata['n_changes']}/{len(sample)}ç‰¹å¾´é‡ã‚’å¤‰æ›´")

    print("\n3. å®Ÿç”¨æ€§: å¤‰æ›´ãŒå®Ÿè¡Œå¯èƒ½ã‹")
    print("   â†’ ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã§æ¤œè¨¼å¿…è¦")
    print("   ï¼ˆä¾‹: å¹´é½¢ã‚’è‹¥ãã™ã‚‹ã®ã¯ä¸å¯èƒ½ï¼‰")

    print("\n4. è¿‘æ¥æ€§: å…ƒã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«è¿‘ã„ã‹")
    print(f"   â†’ æ­£è¦åŒ–è·é›¢: {metadata['distance']:.4f}")

else:
    print("âœ— åäº‹å®Ÿçš„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

print("\n=== è©•ä¾¡åŸºæº–ã®ã¾ã¨ã‚ ===")
print("""
åäº‹å®Ÿçš„èª¬æ˜ã®å¦¥å½“æ€§è©•ä¾¡ï¼š

1. **å®Ÿç¾å¯èƒ½æ€§ï¼ˆFeasibilityï¼‰**
   - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒå†…ã«å­˜åœ¨ã™ã‚‹ã‹
   - ç‰©ç†çš„/è«–ç†çš„ã«å¯èƒ½ãªå€¤ã‹

2. **æœ€å°æ€§ï¼ˆMinimalityï¼‰**
   - å¤‰æ›´ã™ã‚‹ç‰¹å¾´é‡ã®æ•°ãŒæœ€å°ã‹
   - å¤‰æ›´ã®å¤§ãã•ãŒæœ€å°ã‹

3. **å®Ÿç”¨æ€§ï¼ˆActionabilityï¼‰**
   - å®Ÿéš›ã«å¤‰æ›´å¯èƒ½ãªç‰¹å¾´é‡ã‹
   - ã‚³ã‚¹ãƒˆãŒç¾å®Ÿçš„ã‹

4. **è¿‘æ¥æ€§ï¼ˆProximityï¼‰**
   - å…ƒã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«è¿‘ã„ã‹
   - è§£é‡ˆãŒå®¹æ˜“ã‹

5. **å¤šæ§˜æ€§ï¼ˆDiversityï¼‰**
   - è¤‡æ•°ã®è§£æ±ºç­–ã‚’æç¤ºã§ãã‚‹ã‹
   - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é¸æŠè‚¢ãŒã‚ã‚‹ã‹
""")
</code></pre>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). "Why Should I Trust You?": Explaining the Predictions of Any Classifier. <em>KDD</em>.</li>
<li>Lundberg, S. M., & Lee, S. I. (2017). A Unified Approach to Interpreting Model Predictions. <em>NeurIPS</em>.</li>
<li>Molnar, C. (2022). <em>Interpretable Machine Learning</em> (2nd ed.). Available at: https://christophm.github.io/interpretable-ml-book/</li>
<li>Goldstein, A., et al. (2015). Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation. <em>Journal of Computational and Graphical Statistics</em>.</li>
<li>Ribeiro, M. T., Singh, S., & Guestrin, C. (2018). Anchors: High-Precision Model-Agnostic Explanations. <em>AAAI</em>.</li>
<li>Wachter, S., Mittelstadt, B., & Russell, C. (2017). Counterfactual Explanations without Opening the Black Box. <em>Harvard Journal of Law & Technology</em>.</li>
<li>Breiman, L. (2001). Random Forests. <em>Machine Learning</em>, 45(1), 5-32.</li>
</ol>

<div class="navigation">
    <a href="chapter2-shap.html" class="nav-button">â† å‰ã®ç« : SHAPå€¤ã®ç†è«–ã¨å®Ÿè£…</a>
    <a href="chapter4-image-text-interpretation.html" class="nav-button">æ¬¡ã®ç« : ç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆã®è§£é‡ˆ â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
