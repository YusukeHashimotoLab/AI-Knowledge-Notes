---
title: ç¬¬1ç« ï¼šãƒ¢ãƒ‡ãƒ«è§£é‡ˆæ€§åŸºç¤
chapter_title: ç¬¬1ç« ï¼šãƒ¢ãƒ‡ãƒ«è§£é‡ˆæ€§åŸºç¤
subtitle: ä¿¡é ¼ã§ãã‚‹AIã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®è§£é‡ˆæ€§ã®ç†è§£
reading_time: 30-35åˆ†
difficulty: åˆç´š
code_examples: 8
exercises: 6
---

## å­¦ç¿’ç›®æ¨™

ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š

  * âœ… ãƒ¢ãƒ‡ãƒ«è§£é‡ˆæ€§ãŒé‡è¦ãªç†ç”±ã‚’ç†è§£ã™ã‚‹
  * âœ… è§£é‡ˆæ€§ã®åˆ†é¡ä½“ç³»ã‚’æŠŠæ¡ã™ã‚‹
  * âœ… è§£é‡ˆå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´ã‚’çŸ¥ã‚‹
  * âœ… ä¸»è¦ãªè§£é‡ˆæ‰‹æ³•ã®æ¦‚è¦ã‚’ç†è§£ã™ã‚‹
  * âœ… è§£é‡ˆæ€§ã‚’è©•ä¾¡ã™ã‚‹åŸºæº–ã‚’å­¦ã¶
  * âœ… å®Ÿè·µçš„ãªè§£é‡ˆå¯èƒ½ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã§ãã‚‹

* * *

## 1.1 ãªãœãƒ¢ãƒ‡ãƒ«è§£é‡ˆæ€§ãŒé‡è¦ã‹

### ä¿¡é ¼æ€§ã¨èª¬æ˜è²¬ä»»

æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’ä¿¡é ¼ã™ã‚‹ãŸã‚ã«ã¯ã€ã€Œãªãœãã®äºˆæ¸¬ã«è‡³ã£ãŸã®ã‹ã€ã‚’ç†è§£ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ç‰¹ã«é«˜ãƒªã‚¹ã‚¯ãªæ„æ€æ±ºå®šï¼ˆåŒ»ç™‚è¨ºæ–­ã€èè³‡å¯©æŸ»ã€åˆ‘äº‹å¸æ³•ãªã©ï¼‰ã§ã¯ã€èª¬æ˜è²¬ä»»ãŒä¸å¯æ¬ ã§ã™ã€‚

é©ç”¨é ˜åŸŸ | è§£é‡ˆæ€§ãŒå¿…è¦ãªç†ç”± | ãƒªã‚¹ã‚¯  
---|---|---  
**åŒ»ç™‚è¨ºæ–­** | åŒ»å¸«ãŒè¨ºæ–­æ ¹æ‹ ã‚’ç†è§£ã—ã€æ‚£è€…ã«èª¬æ˜ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ | èª¤è¨ºã«ã‚ˆã‚‹ç”Ÿå‘½ã®å±é™º  
**èè³‡å¯©æŸ»** | æ‹’å¦ç†ç”±ã®èª¬æ˜ç¾©å‹™ã€å…¬æ­£æ€§ã®ç¢ºä¿ | å·®åˆ¥çš„ãªåˆ¤æ–­ã€æ³•çš„è¨´è¨Ÿ  
**åˆ‘äº‹å¸æ³•** | å†çŠ¯ãƒªã‚¹ã‚¯è©•ä¾¡ã®æ ¹æ‹ ã‚’ç¤ºã™å¿…è¦ãŒã‚ã‚‹ | ä¸å½“ãªåˆ¤æ±ºã€äººæ¨©ä¾µå®³  
**è‡ªå‹•é‹è»¢** | äº‹æ•…æ™‚ã®è²¬ä»»è¿½åŠã€å®‰å…¨æ€§ã®æ¤œè¨¼ | äººå‘½æå¤±ã€æ³•çš„è²¬ä»»  
  
> **é‡è¦** : ã€Œäºˆæ¸¬ç²¾åº¦ãŒé«˜ã„ã€ã ã‘ã§ã¯ä¸ååˆ†ã§ã™ã€‚ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ãŒãƒ¢ãƒ‡ãƒ«ã‚’ä¿¡é ¼ã—ã€é©åˆ‡ã«åˆ©ç”¨ã™ã‚‹ã«ã¯ã€äºˆæ¸¬ã®æ ¹æ‹ ã‚’ç†è§£ã§ãã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

### è¦åˆ¶è¦ä»¶ï¼ˆGDPRã€AIè¦åˆ¶ï¼‰

ä¸–ç•Œä¸­ã§æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®é€æ˜æ€§ã«é–¢ã™ã‚‹è¦åˆ¶ãŒå¼·åŒ–ã•ã‚Œã¦ã„ã¾ã™ï¼š

  * **GDPRï¼ˆæ¬§å·ä¸€èˆ¬ãƒ‡ãƒ¼ã‚¿ä¿è­·è¦å‰‡ï¼‰** : è‡ªå‹•åŒ–ã•ã‚ŒãŸæ„æ€æ±ºå®šã«é–¢ã™ã‚‹ã€Œèª¬æ˜ã‚’å—ã‘ã‚‹æ¨©åˆ©ã€ã‚’è¦å®šï¼ˆç¬¬22æ¡ï¼‰
  * **EU AI Act** : é«˜ãƒªã‚¹ã‚¯AIã‚·ã‚¹ãƒ†ãƒ ã«å¯¾ã™ã‚‹é€æ˜æ€§ã¨èª¬æ˜å¯èƒ½æ€§ã®è¦ä»¶
  * **ç±³å›½å…¬æ­£ä¿¡ç”¨å ±å‘Šæ³•** : ä¿¡ç”¨ã‚¹ã‚³ã‚¢ã«é–¢ã™ã‚‹ã€Œä¸åˆ©ãªæªç½®ã®é€šçŸ¥ã€ç¾©å‹™
  * **æ—¥æœ¬ã®å€‹äººæƒ…å ±ä¿è­·æ³•** : è‡ªå‹•åŒ–ã•ã‚ŒãŸæ„æ€æ±ºå®šã«é–¢ã™ã‚‹æœ¬äººã¸ã®æƒ…å ±æä¾›

### ãƒ‡ãƒãƒƒã‚°ã¨ãƒ¢ãƒ‡ãƒ«æ”¹å–„

è§£é‡ˆæ€§ã¯ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½å‘ä¸Šã«ã‚‚ä¸å¯æ¬ ã§ã™ï¼š
    
    
    """
    ä¾‹: ãƒ¢ãƒ‡ãƒ«ãŒäºˆæœŸã—ãªã„äºˆæ¸¬ã‚’ã™ã‚‹å ´åˆã®è¨ºæ–­
    
    å•é¡Œ: é¡§å®¢ã®é›¢åäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ãŒå®Ÿé‹ç”¨ã§æ€§èƒ½ãŒä½ã„
    """
    
    import numpy as np
    import pandas as pd
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    n_samples = 1000
    
    data = pd.DataFrame({
        'age': np.random.randint(18, 80, n_samples),
        'tenure_months': np.random.randint(1, 120, n_samples),
        'monthly_charges': np.random.uniform(20, 150, n_samples),
        'total_charges': np.random.uniform(100, 10000, n_samples),
        'num_support_calls': np.random.poisson(2, n_samples),
        'contract_type': np.random.choice(['month', 'year', '2year'], n_samples),
        'customer_id': np.arange(n_samples)  # ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ï¼
    })
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼ˆé›¢åï¼‰
    data['churn'] = ((data['num_support_calls'] > 3) |
                     (data['monthly_charges'] > 100)).astype(int)
    
    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    X = data.drop('churn', axis=1)
    X_encoded = pd.get_dummies(X, columns=['contract_type'])
    y = data['churn']
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_encoded, y, test_size=0.2, random_state=42
    )
    
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    # Feature Importanceã§è¨ºæ–­
    feature_importance = pd.DataFrame({
        'feature': X_encoded.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("Feature Importance:")
    print(feature_importance.head(10))
    
    # å•é¡Œç™ºè¦‹: customer_idãŒæœ€ã‚‚é‡è¦ãªç‰¹å¾´é‡ã«ãªã£ã¦ã„ã‚‹ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ï¼‰
    print("\nâš ï¸ customer_idã®é‡è¦åº¦ãŒç•°å¸¸ã«é«˜ã„ â†’ ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®å¯èƒ½æ€§")
    

### ãƒã‚¤ã‚¢ã‚¹æ¤œå‡º

è§£é‡ˆæ€§ã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã—ãŸä¸å…¬å¹³ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç™ºè¦‹ã§ãã¾ã™ï¼š
    
    
    """
    ä¾‹: æ¡ç”¨ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ã‚¢ã‚¹æ¤œå‡º
    """
    
    import numpy as np
    import pandas as pd
    from sklearn.linear_model import LogisticRegression
    from sklearn.preprocessing import StandardScaler
    
    # ãƒã‚¤ã‚¢ã‚¹ã®ã‚ã‚‹ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    np.random.seed(42)
    n_samples = 1000
    
    data = pd.DataFrame({
        'years_experience': np.random.randint(0, 20, n_samples),
        'education_level': np.random.randint(1, 5, n_samples),
        'skills_score': np.random.uniform(0, 100, n_samples),
        'gender': np.random.choice(['M', 'F'], n_samples),
        'age': np.random.randint(22, 65, n_samples)
    })
    
    # ãƒã‚¤ã‚¢ã‚¹ã®ã‚ã‚‹ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆæ€§åˆ¥ã«ã‚ˆã‚‹å·®åˆ¥ãŒå«ã¾ã‚Œã‚‹ï¼‰
    data['hired'] = (
        (data['years_experience'] > 5) &
        (data['skills_score'] > 60) &
        (data['gender'] == 'M')  # æ€§åˆ¥ãƒã‚¤ã‚¢ã‚¹
    ).astype(int)
    
    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    X = pd.get_dummies(data.drop('hired', axis=1), columns=['gender'])
    y = data['hired']
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    model = LogisticRegression(random_state=42)
    model.fit(X_scaled, y)
    
    # ä¿‚æ•°ã‚’ç¢ºèªã—ã¦ãƒã‚¤ã‚¢ã‚¹ã‚’æ¤œå‡º
    coefficients = pd.DataFrame({
        'feature': X.columns,
        'coefficient': model.coef_[0]
    }).sort_values('coefficient', ascending=False)
    
    print("Model Coefficients:")
    print(coefficients)
    
    # gender_Mã®ä¿‚æ•°ãŒç•°å¸¸ã«é«˜ã„ â†’ æ€§åˆ¥ãƒã‚¤ã‚¢ã‚¹ã‚’æ¤œå‡º
    print("\nâš ï¸ gender_Mã®ä¿‚æ•°ãŒé«˜ã„ â†’ æ€§åˆ¥ã«ã‚ˆã‚‹å·®åˆ¥ã®å¯èƒ½æ€§")
    print("ğŸ“Š å…¬æ­£æ€§ã®è©•ä¾¡ãŒå¿…è¦")
    

* * *

## 1.2 è§£é‡ˆæ€§ã®åˆ†é¡

### ã‚°ãƒ­ãƒ¼ãƒãƒ«è§£é‡ˆ vs ãƒ­ãƒ¼ã‚«ãƒ«è§£é‡ˆ

åˆ†é¡ | èª¬æ˜ | è³ªå• | æ‰‹æ³•ä¾‹  
---|---|---|---  
**ã‚°ãƒ­ãƒ¼ãƒãƒ«è§£é‡ˆ** | ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®æŒ¯ã‚‹èˆã„ã‚’ç†è§£ | ã€Œãƒ¢ãƒ‡ãƒ«ã¯ä¸€èˆ¬çš„ã«ã©ã†äºˆæ¸¬ã™ã‚‹ã‹ï¼Ÿã€ | Feature Importance, Partial Dependence  
**ãƒ­ãƒ¼ã‚«ãƒ«è§£é‡ˆ** | å€‹åˆ¥ã®äºˆæ¸¬ã‚’èª¬æ˜ | ã€Œãªãœã“ã®é¡§å®¢ã¯é›¢åã™ã‚‹ã¨äºˆæ¸¬ã•ã‚ŒãŸã‹ï¼Ÿã€ | LIME, SHAP, Counterfactual  
      
    
    """
    ä¾‹: ã‚°ãƒ­ãƒ¼ãƒãƒ«è§£é‡ˆ vs ãƒ­ãƒ¼ã‚«ãƒ«è§£é‡ˆ
    """
    
    import numpy as np
    import pandas as pd
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
    import matplotlib.pyplot as plt
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    n_samples = 500
    
    data = pd.DataFrame({
        'age': np.random.randint(18, 70, n_samples),
        'income': np.random.uniform(20000, 150000, n_samples),
        'debt_ratio': np.random.uniform(0, 1, n_samples),
        'credit_history_months': np.random.randint(0, 360, n_samples)
    })
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: ãƒ­ãƒ¼ãƒ³æ‰¿èª
    data['approved'] = (
        (data['income'] > 50000) &
        (data['debt_ratio'] < 0.5) &
        (data['credit_history_months'] > 24)
    ).astype(int)
    
    X = data.drop('approved', axis=1)
    y = data['approved']
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    model = RandomForestClassifier(n_estimators=50, random_state=42)
    model.fit(X_train, y_train)
    
    # --- ã‚°ãƒ­ãƒ¼ãƒãƒ«è§£é‡ˆ: Feature Importance ---
    print("=== ã‚°ãƒ­ãƒ¼ãƒãƒ«è§£é‡ˆ ===")
    print("ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã§æœ€ã‚‚é‡è¦ãªç‰¹å¾´é‡:")
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    print(feature_importance)
    
    # --- ãƒ­ãƒ¼ã‚«ãƒ«è§£é‡ˆ: å€‹åˆ¥ã®äºˆæ¸¬èª¬æ˜ ---
    print("\n=== ãƒ­ãƒ¼ã‚«ãƒ«è§£é‡ˆ ===")
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰1ã‚µãƒ³ãƒ—ãƒ«é¸æŠ
    sample_idx = 0
    sample = X_test.iloc[sample_idx:sample_idx+1]
    prediction = model.predict(sample)[0]
    prediction_proba = model.predict_proba(sample)[0]
    
    print(f"ã‚µãƒ³ãƒ—ãƒ« {sample_idx} ã®ç‰¹å¾´:")
    print(sample.T)
    print(f"\näºˆæ¸¬: {'æ‰¿èª' if prediction == 1 else 'å´ä¸‹'}")
    print(f"ç¢ºç‡: {prediction_proba[1]:.2%}")
    
    # ç°¡æ˜“çš„ãªãƒ­ãƒ¼ã‚«ãƒ«é‡è¦åº¦ï¼ˆãƒ„ãƒªãƒ¼ãƒ™ãƒ¼ã‚¹ï¼‰
    # å®Ÿéš›ã«ã¯SHAPã‚„LIMEã‚’ä½¿ç”¨ã™ã‚‹ã®ãŒæœ›ã¾ã—ã„
    print("\nã“ã®äºˆæ¸¬ã«å¯„ä¸ã—ãŸç‰¹å¾´ï¼ˆæ¦‚ç®—ï¼‰:")
    for feature in X.columns:
        print(f"  {feature}: {sample[feature].values[0]:.2f}")
    

### ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ vs ãƒ¢ãƒ‡ãƒ«éä¾å­˜

åˆ†é¡ | èª¬æ˜ | åˆ©ç‚¹ | æ¬ ç‚¹  
---|---|---|---  
**ãƒ¢ãƒ‡ãƒ«å›ºæœ‰** | ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã«ç‰¹åŒ–ã—ãŸè§£é‡ˆ | æ­£ç¢ºã€åŠ¹ç‡çš„ | ä»–ã®ãƒ¢ãƒ‡ãƒ«ã«é©ç”¨ã§ããªã„  
**ãƒ¢ãƒ‡ãƒ«éä¾å­˜** | ã©ã®ãƒ¢ãƒ‡ãƒ«ã«ã‚‚é©ç”¨å¯èƒ½ | æ±ç”¨æ€§ãŒé«˜ã„ | è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„å ´åˆãŒã‚ã‚‹  
  
### äº‹å‰è§£é‡ˆæ€§ vs äº‹å¾Œè§£é‡ˆæ€§

  * **äº‹å‰è§£é‡ˆæ€§ï¼ˆIntrinsic Interpretabilityï¼‰** : ãƒ¢ãƒ‡ãƒ«è‡ªä½“ãŒè§£é‡ˆå¯èƒ½ï¼ˆç·šå½¢å›å¸°ã€æ±ºå®šæœ¨ï¼‰
  * **äº‹å¾Œè§£é‡ˆæ€§ï¼ˆPost-hoc Interpretabilityï¼‰** : ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’å¾Œã‹ã‚‰è§£é‡ˆï¼ˆSHAPã€LIMEï¼‰

### è§£é‡ˆæ€§ã®åˆ†é¡ä½“ç³»
    
    
    ```mermaid
    graph TB
        A[ãƒ¢ãƒ‡ãƒ«è§£é‡ˆæ€§] --> B[ã‚¹ã‚³ãƒ¼ãƒ—]
        A --> C[ä¾å­˜æ€§]
        A --> D[ã‚¿ã‚¤ãƒŸãƒ³ã‚°]
    
        B --> B1[ã‚°ãƒ­ãƒ¼ãƒãƒ«è§£é‡ˆãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®æŒ¯ã‚‹èˆã„]
        B --> B2[ãƒ­ãƒ¼ã‚«ãƒ«è§£é‡ˆå€‹åˆ¥äºˆæ¸¬ã®èª¬æ˜]
    
        C --> C1[ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ç‰¹å®šãƒ¢ãƒ‡ãƒ«ç”¨]
        C --> C2[ãƒ¢ãƒ‡ãƒ«éä¾å­˜æ±ç”¨çš„]
    
        D --> D1[äº‹å‰è§£é‡ˆæ€§æœ¬è³ªçš„ã«è§£é‡ˆå¯èƒ½]
        D --> D2[äº‹å¾Œè§£é‡ˆæ€§å¾Œä»˜ã‘èª¬æ˜]
    
        style A fill:#7b2cbf,color:#fff
        style B1 fill:#e3f2fd
        style B2 fill:#e3f2fd
        style C1 fill:#fff3e0
        style C2 fill:#fff3e0
        style D1 fill:#c8e6c9
        style D2 fill:#c8e6c9
    ```

* * *

## 1.3 è§£é‡ˆå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«

### ç·šå½¢å›å¸°

ç·šå½¢å›å¸°ã¯æœ€ã‚‚è§£é‡ˆã—ã‚„ã™ã„ãƒ¢ãƒ‡ãƒ«ã®ä¸€ã¤ã§ã™ã€‚å„ç‰¹å¾´é‡ã®ä¿‚æ•°ãŒç›´æ¥çš„ã«å½±éŸ¿ã‚’ç¤ºã—ã¾ã™ã€‚

**æ•°å¼** :

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n$$

$\beta_i$ ã¯ç‰¹å¾´é‡ $x_i$ ã®1å˜ä½å¤‰åŒ–ã«å¯¾ã™ã‚‹äºˆæ¸¬å€¤ã®å¤‰åŒ–é‡ã‚’ç¤ºã—ã¾ã™ã€‚
    
    
    """
    ä¾‹: ç·šå½¢å›å¸°ã«ã‚ˆã‚‹ä½å®…ä¾¡æ ¼äºˆæ¸¬
    """
    
    import numpy as np
    import pandas as pd
    from sklearn.linear_model import LinearRegression
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    n_samples = 200
    
    data = pd.DataFrame({
        'square_feet': np.random.randint(500, 4000, n_samples),
        'bedrooms': np.random.randint(1, 6, n_samples),
        'age_years': np.random.randint(0, 50, n_samples),
        'distance_to_city': np.random.uniform(0, 50, n_samples)
    })
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: ä¾¡æ ¼ï¼ˆä¸‡å††ï¼‰
    data['price'] = (
        data['square_feet'] * 0.5 +
        data['bedrooms'] * 50 -
        data['age_years'] * 5 -
        data['distance_to_city'] * 10 +
        np.random.normal(0, 100, n_samples)
    )
    
    X = data.drop('price', axis=1)
    y = data['price']
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # æ¨™æº–åŒ–ï¼ˆä¿‚æ•°ã®æ¯”è¼ƒã®ãŸã‚ï¼‰
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    model = LinearRegression()
    model.fit(X_train_scaled, y_train)
    
    # ä¿‚æ•°ã®è§£é‡ˆ
    coefficients = pd.DataFrame({
        'feature': X.columns,
        'coefficient': model.coef_,
        'abs_coefficient': np.abs(model.coef_)
    }).sort_values('abs_coefficient', ascending=False)
    
    print("ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã®ä¿‚æ•°:")
    print(coefficients)
    print(f"\nåˆ‡ç‰‡: {model.intercept_:.2f}")
    
    print("\nè§£é‡ˆ:")
    print("- square_feet ã®ä¿‚æ•°ãŒæœ€ã‚‚å¤§ãã„ â†’ é¢ç©ãŒä¾¡æ ¼ã«æœ€ã‚‚å½±éŸ¿")
    print("- age_years ã®ä¿‚æ•°ãŒè²  â†’ ç¯‰å¹´æ•°ãŒå¤ã„ã»ã©ä¾¡æ ¼ãŒä½ã„")
    print("- ä¿‚æ•°ãŒæ¨™æº–åŒ–ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ç›´æ¥æ¯”è¼ƒå¯èƒ½")
    
    # äºˆæ¸¬ä¾‹
    sample = X_test_scaled[0:1]
    prediction = model.predict(sample)[0]
    print(f"\nã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬ä¾¡æ ¼: {prediction:.2f}ä¸‡å††")
    

### æ±ºå®šæœ¨

æ±ºå®šæœ¨ã¯äººé–“ãŒç†è§£ã—ã‚„ã™ã„ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®åˆ†å²æ§‹é€ ã‚’æŒã¡ã¾ã™ã€‚
    
    
    """
    ä¾‹: æ±ºå®šæœ¨ã«ã‚ˆã‚‹ã‚¢ã‚¤ãƒªã‚¹åˆ†é¡
    """
    
    import numpy as np
    import pandas as pd
    from sklearn.datasets import load_iris
    from sklearn.tree import DecisionTreeClassifier, plot_tree
    from sklearn.model_selection import train_test_split
    import matplotlib.pyplot as plt
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    iris = load_iris()
    X = pd.DataFrame(iris.data, columns=iris.feature_names)
    y = iris.target
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # æ±ºå®šæœ¨ãƒ¢ãƒ‡ãƒ«ï¼ˆæ·±ã•ã‚’åˆ¶é™ã—ã¦è§£é‡ˆã—ã‚„ã™ãï¼‰
    model = DecisionTreeClassifier(max_depth=3, random_state=42)
    model.fit(X_train, y_train)
    
    # ç²¾åº¦
    accuracy = model.score(X_test, y_test)
    print(f"ç²¾åº¦: {accuracy:.2%}")
    
    # ãƒ«ãƒ¼ãƒ«ã®æŠ½å‡ºï¼ˆãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ï¼‰
    from sklearn.tree import export_text
    tree_rules = export_text(model, feature_names=list(iris.feature_names))
    print("\næ±ºå®šæœ¨ã®ãƒ«ãƒ¼ãƒ«:")
    print(tree_rules[:500] + "...")  # æœ€åˆã®500æ–‡å­—ã®ã¿è¡¨ç¤º
    
    # è§£é‡ˆä¾‹
    print("\nè§£é‡ˆ:")
    print("- petal width (cm) <= 0.8 ã§ setosa ã¨åˆ¤å®š")
    print("- ãã‚Œä»¥å¤–ã¯ petal width ã‚„ petal length ã§ versicolor/virginica ã‚’åˆ¤å®š")
    print("- æ±ºå®šå¢ƒç•ŒãŒæ˜ç¢ºã§ã€å°‚é–€å®¶ã§ãªãã¦ã‚‚ç†è§£å¯èƒ½")
    

### ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«

IF-THENãƒ«ãƒ¼ãƒ«ã§æ§‹æˆã•ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«ã¨ã—ã¦ç›´æ¥åˆ©ç”¨å¯èƒ½ã§ã™ã€‚
    
    
    """
    ä¾‹: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹åˆ†é¡å™¨
    """
    
    import numpy as np
    import pandas as pd
    
    class SimpleRuleClassifier:
        """è§£é‡ˆå¯èƒ½ãªãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹åˆ†é¡å™¨"""
    
        def __init__(self):
            self.rules = []
    
        def add_rule(self, condition, prediction, description=""):
            """ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ """
            self.rules.append({
                'condition': condition,
                'prediction': prediction,
                'description': description
            })
    
        def predict(self, X):
            """äºˆæ¸¬"""
            predictions = []
            for _, row in X.iterrows():
                prediction = None
                for rule in self.rules:
                    if rule['condition'](row):
                        prediction = rule['prediction']
                        break
                predictions.append(prediction if prediction is not None else 0)
            return np.array(predictions)
    
        def explain(self):
            """ãƒ«ãƒ¼ãƒ«ã‚’èª¬æ˜"""
            print("åˆ†é¡ãƒ«ãƒ¼ãƒ«:")
            for i, rule in enumerate(self.rules, 1):
                print(f"  Rule {i}: {rule['description']} â†’ {rule['prediction']}")
    
    # ä½¿ç”¨ä¾‹: ãƒ­ãƒ¼ãƒ³æ‰¿èªãƒ«ãƒ¼ãƒ«
    classifier = SimpleRuleClassifier()
    
    # ãƒ«ãƒ¼ãƒ«1: é«˜åå…¥ã§ä½è² å‚µ
    classifier.add_rule(
        condition=lambda row: row['income'] > 100000 and row['debt_ratio'] < 0.3,
        prediction=1,
        description="é«˜åå…¥ï¼ˆ>100Kï¼‰ã‹ã¤ä½è² å‚µç‡ï¼ˆ<30%ï¼‰"
    )
    
    # ãƒ«ãƒ¼ãƒ«2: ä¸­åå…¥ã§è‰¯å¥½ãªä¿¡ç”¨å±¥æ­´
    classifier.add_rule(
        condition=lambda row: row['income'] > 50000 and row['credit_history_months'] > 36,
        prediction=1,
        description="ä¸­åå…¥ï¼ˆ>50Kï¼‰ã‹ã¤ä¿¡ç”¨å±¥æ­´3å¹´ä»¥ä¸Š"
    )
    
    # ãƒ«ãƒ¼ãƒ«3: ãã‚Œä»¥å¤–ã¯å´ä¸‹
    classifier.add_rule(
        condition=lambda row: True,
        prediction=0,
        description="ãã®ä»–ã®ã‚±ãƒ¼ã‚¹"
    )
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_data = pd.DataFrame({
        'income': [120000, 60000, 30000],
        'debt_ratio': [0.2, 0.4, 0.6],
        'credit_history_months': [48, 40, 12]
    })
    
    predictions = classifier.predict(test_data)
    classifier.explain()
    
    print("\näºˆæ¸¬çµæœ:")
    for i, (pred, income) in enumerate(zip(predictions, test_data['income'])):
        print(f"  ç”³è«‹è€… {i+1} (åå…¥: ${income:,.0f}): {'æ‰¿èª' if pred == 1 else 'å´ä¸‹'}")
    

### GAM (Generalized Additive Models)

GAMã¯ã€å„ç‰¹å¾´é‡ã®éç·šå½¢åŠ¹æœã‚’å¯è¦–åŒ–ã§ãã‚‹è§£é‡ˆå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚

**æ•°å¼** :

$$g(\mathbb{E}[y]) = \beta_0 + f_1(x_1) + f_2(x_2) + \cdots + f_n(x_n)$$

$f_i$ ã¯ç‰¹å¾´é‡ $x_i$ ã®éç·šå½¢é–¢æ•°ã§ã™ã€‚
    
    
    """
    ä¾‹: GAMã«ã‚ˆã‚‹éç·šå½¢é–¢ä¿‚ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°
    """
    
    import numpy as np
    import pandas as pd
    from sklearn.preprocessing import StandardScaler
    from sklearn.linear_model import Ridge
    from sklearn.model_selection import train_test_split
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆéç·šå½¢é–¢ä¿‚ï¼‰
    np.random.seed(42)
    n_samples = 300
    
    x1 = np.random.uniform(-3, 3, n_samples)
    x2 = np.random.uniform(-3, 3, n_samples)
    
    # éç·šå½¢é–¢ä¿‚: siné–¢æ•°ã¨2æ¬¡é–¢æ•°
    y = np.sin(x1) + x2**2 + np.random.normal(0, 0.2, n_samples)
    
    data = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})
    
    # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°: å¤šé …å¼ç‰¹å¾´ã‚’è¿½åŠ ï¼ˆGAMã®è¿‘ä¼¼ï¼‰
    from sklearn.preprocessing import PolynomialFeatures
    
    X = data[['x1', 'x2']]
    poly = PolynomialFeatures(degree=3, include_bias=False, interaction_features=False)
    X_poly = poly.fit_transform(X)
    
    feature_names = poly.get_feature_names_out(['x1', 'x2'])
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_poly, data['y'], test_size=0.2, random_state=42
    )
    
    # ãƒªãƒƒã‚¸å›å¸°ã§è¨“ç·´
    model = Ridge(alpha=1.0)
    model.fit(X_train, y_train)
    
    print(f"ãƒ†ã‚¹ãƒˆRÂ²ã‚¹ã‚³ã‚¢: {model.score(X_test, y_test):.3f}")
    
    # å„ç‰¹å¾´é‡ã®åŠ¹æœã‚’å¯è¦–åŒ–
    print("\nå„ç‰¹å¾´é‡ã®å¤šé …å¼ä¿‚æ•°:")
    coef_df = pd.DataFrame({
        'feature': feature_names,
        'coefficient': model.coef_
    })
    print(coef_df)
    
    print("\nè§£é‡ˆ:")
    print("- x1ã®å¥‡æ•°æ¬¡ã®é …ãŒé‡è¦ â†’ siné–¢æ•°çš„ãªéç·šå½¢æ€§")
    print("- x2ã®2æ¬¡ã®é …ãŒé‡è¦ â†’ 2æ¬¡é–¢æ•°çš„ãªé–¢ä¿‚")
    print("- å„å¤‰æ•°ã®åŠ¹æœã‚’å€‹åˆ¥ã«è§£é‡ˆå¯èƒ½")
    

* * *

## 1.4 è§£é‡ˆæ‰‹æ³•ã®æ¦‚è¦

### Feature Importance

ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å®šé‡åŒ–ã™ã‚‹æ‰‹æ³•ã€‚ãƒ„ãƒªãƒ¼ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§é »ç¹ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

  * **Mean Decrease Impurity** : ä¸ç´”åº¦ã®æ¸›å°‘é‡ã§é‡è¦åº¦ã‚’æ¸¬å®š
  * **Permutation Importance** : ç‰¹å¾´é‡ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ãŸéš›ã®æ€§èƒ½ä½ä¸‹ã§æ¸¬å®š

### Partial Dependence Plot (PDP)

ç‰¹å®šã®ç‰¹å¾´é‡ã¨ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã®é–¢ä¿‚ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚

**æ•°å¼** :

$$\text{PDP}(x_s) = \mathbb{E}_{x_c}[f(x_s, x_c)]$$

$x_s$ ã¯å¯¾è±¡ç‰¹å¾´é‡ã€$x_c$ ã¯ãã®ä»–ã®ç‰¹å¾´é‡ã§ã™ã€‚

### SHAP (SHapley Additive exPlanations)

ã‚²ãƒ¼ãƒ ç†è«–ã®Shapleyå€¤ã‚’ç”¨ã„ã¦ã€å„ç‰¹å¾´é‡ã®è²¢çŒ®åº¦ã‚’è¨ˆç®—ã—ã¾ã™ã€‚

**ç‰¹å¾´** :

  * ä¸€è²«æ€§ã®ã‚ã‚‹èª¬æ˜
  * å±€æ‰€çš„ãŠã‚ˆã³å¤§åŸŸçš„è§£é‡ˆãŒå¯èƒ½
  * ãƒ¢ãƒ‡ãƒ«éä¾å­˜

### LIME (Local Interpretable Model-agnostic Explanations)

å€‹åˆ¥ã®äºˆæ¸¬ã‚’ã€å±€æ‰€çš„ã«ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§è¿‘ä¼¼ã—ã¦èª¬æ˜ã—ã¾ã™ã€‚

**æ‰‹é †** :

  1. äºˆæ¸¬ã—ãŸã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®è¿‘å‚ã«ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆ
  2. ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ã‚’å–å¾—
  3. è§£é‡ˆå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ï¼ˆç·šå½¢å›å¸°ãªã©ï¼‰ã§ãƒ­ãƒ¼ã‚«ãƒ«ã«è¿‘ä¼¼
  4. è¿‘ä¼¼ãƒ¢ãƒ‡ãƒ«ã®ä¿‚æ•°ã‚’è§£é‡ˆ

### Saliency Mapsï¼ˆé¡•è‘—æ€§ãƒãƒƒãƒ—ï¼‰

ç”»åƒåˆ†é¡ã«ãŠã„ã¦ã€ã©ã®ãƒ”ã‚¯ã‚»ãƒ«ãŒäºˆæ¸¬ã«é‡è¦ã‹ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚

**è¨ˆç®—æ–¹æ³•** :

$$S(x) = \left| \frac{\partial f(x)}{\partial x} \right|$$

å…¥åŠ›ç”»åƒã«å¯¾ã™ã‚‹å‹¾é…ã‚’è¨ˆç®—ã—ã€é‡è¦ãªé ˜åŸŸã‚’å¼·èª¿è¡¨ç¤ºã—ã¾ã™ã€‚

* * *

## 1.5 è§£é‡ˆæ€§ã®è©•ä¾¡

### Fidelityï¼ˆå¿ å®Ÿåº¦ï¼‰

è§£é‡ˆæ‰‹æ³•ãŒå…ƒã®ãƒ¢ãƒ‡ãƒ«ã®æŒ¯ã‚‹èˆã„ã‚’ã©ã‚Œã ã‘æ­£ç¢ºã«èª¬æ˜ã—ã¦ã„ã‚‹ã‹ã‚’æ¸¬å®šã—ã¾ã™ã€‚

è©•ä¾¡æŒ‡æ¨™ | èª¬æ˜ | è¨ˆç®—æ–¹æ³•  
---|---|---  
**RÂ²ã‚¹ã‚³ã‚¢** | èª¬æ˜ãƒ¢ãƒ‡ãƒ«ã¨å…ƒãƒ¢ãƒ‡ãƒ«ã®ä¸€è‡´åº¦ | $R^2 = 1 - \frac{\sum(y_{\text{true}} - y_{\text{approx}})^2}{\sum(y_{\text{true}} - \bar{y})^2}$  
**Local Fidelity** | å±€æ‰€çš„ãªäºˆæ¸¬ã®ä¸€è‡´åº¦ | è¿‘å‚ã‚µãƒ³ãƒ—ãƒ«ã§ã®äºˆæ¸¬èª¤å·®  
  
### Consistencyï¼ˆä¸€è²«æ€§ï¼‰

é¡ä¼¼ã—ãŸã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«å¯¾ã—ã¦é¡ä¼¼ã—ãŸèª¬æ˜ãŒå¾—ã‚‰ã‚Œã‚‹ã‹ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
    
    
    """
    ä¾‹: è§£é‡ˆã®ä¸€è²«æ€§è©•ä¾¡
    """
    
    import numpy as np
    import pandas as pd
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    np.random.seed(42)
    n_samples = 500
    
    data = pd.DataFrame({
        'feature1': np.random.normal(0, 1, n_samples),
        'feature2': np.random.normal(0, 1, n_samples),
        'feature3': np.random.normal(0, 1, n_samples)
    })
    data['target'] = (data['feature1'] + data['feature2'] > 0).astype(int)
    
    X = data.drop('target', axis=1)
    y = data['target']
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    model = RandomForestClassifier(n_estimators=50, random_state=42)
    model.fit(X_train, y_train)
    
    # é¡ä¼¼ã‚µãƒ³ãƒ—ãƒ«ã®Feature Importanceæ¯”è¼ƒ
    sample1 = X_test.iloc[0:1]
    sample2 = X_test.iloc[1:2]  # é¡ä¼¼ã‚µãƒ³ãƒ—ãƒ«
    
    # ãƒ„ãƒªãƒ¼ãƒ‘ã‚¹ã‚’ä½¿ã£ãŸç°¡æ˜“çš„ãªãƒ­ãƒ¼ã‚«ãƒ«é‡è¦åº¦
    # ï¼ˆå®Ÿéš›ã«ã¯SHAPã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ï¼‰
    
    print("ã‚µãƒ³ãƒ—ãƒ«1ã®ç‰¹å¾´:")
    print(sample1.values)
    print(f"äºˆæ¸¬: {model.predict(sample1)[0]}")
    
    print("\nã‚µãƒ³ãƒ—ãƒ«2ã®ç‰¹å¾´:")
    print(sample2.values)
    print(f"äºˆæ¸¬: {model.predict(sample2)[0]}")
    
    # è·é›¢è¨ˆç®—
    distance = np.linalg.norm(sample1.values - sample2.values)
    print(f"\nã‚µãƒ³ãƒ—ãƒ«é–“ã®è·é›¢: {distance:.3f}")
    print("ä¸€è²«æ€§è©•ä¾¡: é¡ä¼¼ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã™ã‚‹èª¬æ˜ãŒé¡ä¼¼ã—ã¦ã„ã‚‹ã‹ç¢ºèªãŒå¿…è¦")
    

### Stabilityï¼ˆå®‰å®šæ€§ï¼‰

å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®ã‚ãšã‹ãªå¤‰åŒ–ã«å¯¾ã—ã¦ã€è§£é‡ˆãŒå¤§ããå¤‰ã‚ã‚‰ãªã„ã‹ã‚’è©•ä¾¡ã—ã¾ã™ã€‚

### Comprehensibilityï¼ˆç†è§£å®¹æ˜“æ€§ï¼‰

äººé–“ãŒèª¬æ˜ã‚’ç†è§£ã—ã‚„ã™ã„ã‹ã‚’è©•ä¾¡ã—ã¾ã™ã€‚å®šé‡åŒ–ãŒé›£ã—ã„ãŸã‚ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼èª¿æŸ»ãŒä¸€èˆ¬çš„ã§ã™ã€‚

è©•ä¾¡æ–¹æ³• | èª¬æ˜  
---|---  
**ãƒ«ãƒ¼ãƒ«æ•°** | æ±ºå®šæœ¨ã‚„ãƒ«ãƒ¼ãƒ«ã‚»ãƒƒãƒˆã®ãƒ«ãƒ¼ãƒ«æ•°ï¼ˆå°‘ãªã„ã»ã©ç†è§£ã—ã‚„ã™ã„ï¼‰  
**ç‰¹å¾´é‡æ•°** | èª¬æ˜ã«ä½¿ç”¨ã•ã‚Œã‚‹ç‰¹å¾´é‡ã®æ•°ï¼ˆå°‘ãªã„ã»ã©è‰¯ã„ï¼‰  
**ãƒ¦ãƒ¼ã‚¶ãƒ¼èª¿æŸ»** | å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ç†è§£åº¦ãƒ†ã‚¹ãƒˆ  
  
* * *

## ç·´ç¿’å•é¡Œ

å•é¡Œ1: ãƒ¢ãƒ‡ãƒ«è§£é‡ˆæ€§ã®å¿…è¦æ€§

**å•é¡Œ** : ä»¥ä¸‹ã®ã‚·ãƒŠãƒªã‚ªã§ã€ãƒ¢ãƒ‡ãƒ«è§£é‡ˆæ€§ãŒç‰¹ã«é‡è¦ã§ã‚ã‚‹ç†ç”±ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

  1. éŠ€è¡Œã®èè³‡å¯©æŸ»ã‚·ã‚¹ãƒ†ãƒ 
  2. åŒ»ç™‚ç”»åƒè¨ºæ–­æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ 
  3. ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ã‚¹ãƒ†ãƒ 

**è§£ç­”ä¾‹** :

  1. **èè³‡å¯©æŸ»** : æ‹’å¦ç†ç”±ã®èª¬æ˜ç¾©å‹™ï¼ˆæ³•çš„è¦ä»¶ï¼‰ã€å…¬æ­£æ€§ã®ç¢ºä¿ã€å·®åˆ¥çš„ãªåˆ¤æ–­ã®é˜²æ­¢
  2. **åŒ»ç™‚è¨ºæ–­** : åŒ»å¸«ã«ã‚ˆã‚‹è¨ºæ–­æ ¹æ‹ ã®ç†è§£ã€æ‚£è€…ã¸ã®èª¬æ˜ã€èª¤è¨ºã®ãƒªã‚¹ã‚¯è»½æ¸›ã€åŒ»ç™‚éèª¤è¨´è¨Ÿã¸ã®å¯¾å¿œ
  3. **ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³** : ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¿¡é ¼ã®å‘ä¸Šã€æ¨è–¦ç†ç”±ã®é€æ˜æ€§ã€ãƒã‚¤ã‚¢ã‚¹ã®æ¤œå‡ºï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒãƒ–ãƒ«ã®å›é¿ï¼‰

å•é¡Œ2: ã‚°ãƒ­ãƒ¼ãƒãƒ«è§£é‡ˆã¨ãƒ­ãƒ¼ã‚«ãƒ«è§£é‡ˆ

**å•é¡Œ** : ã€Œé¡§å®¢é›¢åäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã€ã«ãŠã„ã¦ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«è§£é‡ˆã¨ãƒ­ãƒ¼ã‚«ãƒ«è§£é‡ˆãã‚Œãã‚Œã§çŸ¥ã‚ŠãŸã„æƒ…å ±ã®ä¾‹ã‚’æŒ™ã’ã¦ãã ã•ã„ã€‚

**è§£ç­”ä¾‹** :

  * **ã‚°ãƒ­ãƒ¼ãƒãƒ«è§£é‡ˆ** : 
    * ã€Œã‚µãƒãƒ¼ãƒˆå•ã„åˆã‚ã›å›æ•°ã€ãŒé›¢åã«æœ€ã‚‚å½±éŸ¿ã™ã‚‹ç‰¹å¾´é‡ã§ã‚ã‚‹
    * ã€Œå¥‘ç´„æœŸé–“ã€ã¨é›¢åç¢ºç‡ã®é–¢ä¿‚ï¼ˆé•·ã„ã»ã©é›¢åç‡ãŒä½ã„å‚¾å‘ï¼‰
    * ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã§é‡è¦ãªä¸Šä½5ã¤ã®ç‰¹å¾´é‡
  * **ãƒ­ãƒ¼ã‚«ãƒ«è§£é‡ˆ** : 
    * é¡§å®¢Aï¼ˆID=12345ï¼‰ãŒé›¢åã™ã‚‹ã¨äºˆæ¸¬ã•ã‚ŒãŸç†ç”±ï¼ˆã‚µãƒãƒ¼ãƒˆå•ã„åˆã‚ã›ãŒ10å›ä»¥ä¸Šã€å¥‘ç´„æœŸé–“ãŒ3ãƒ¶æœˆæœªæº€ãªã©ï¼‰
    * ã“ã®é¡§å®¢ã®é›¢åç¢ºç‡ã‚’ä¸‹ã’ã‚‹ãŸã‚ã«æ”¹å–„ã™ã¹ãè¦å› 

å•é¡Œ3: è§£é‡ˆå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®é¸æŠ

**å•é¡Œ** : ä»¥ä¸‹ã®ã‚·ãƒŠãƒªã‚ªã§ã€ã©ã®è§£é‡ˆå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒé©åˆ‡ã‹é¸æŠã—ã€ç†ç”±ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

  1. ä½å®…ä¾¡æ ¼äºˆæ¸¬ï¼ˆç‰¹å¾´é‡: é¢ç©ã€éƒ¨å±‹æ•°ã€ç¯‰å¹´æ•°ãªã©ï¼‰
  2. ã‚¹ãƒ‘ãƒ ãƒ¡ãƒ¼ãƒ«åˆ†é¡ï¼ˆç‰¹å¾´é‡: å˜èªã®å‡ºç¾é »åº¦ï¼‰
  3. æ‚£è€…ã®å†å…¥é™¢ãƒªã‚¹ã‚¯äºˆæ¸¬ï¼ˆç‰¹å¾´é‡: å¹´é½¢ã€è¨ºæ–­å±¥æ­´ã€æ¤œæŸ»å€¤ãªã©ï¼‰

**è§£ç­”ä¾‹** :

  1. **ç·šå½¢å›å¸°** : å„ç‰¹å¾´é‡ã®ä¿‚æ•°ãŒä¾¡æ ¼ã¸ã®å½±éŸ¿ã‚’ç›´æ¥ç¤ºã™ãŸã‚ã€ä¸å‹•ç”£æ¥­è€…ã‚„é¡§å®¢ãŒç†è§£ã—ã‚„ã™ã„
  2. **æ±ºå®šæœ¨ã¾ãŸã¯ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹** : ã€Œ"ç„¡æ–™"ã¨ã„ã†å˜èªãŒ5å›ä»¥ä¸Š â†’ ã‚¹ãƒ‘ãƒ ã€ã®ã‚ˆã†ãªãƒ«ãƒ¼ãƒ«ãŒç›´æ„Ÿçš„
  3. **GAMã¾ãŸã¯æ±ºå®šæœ¨** : éç·šå½¢ãªé–¢ä¿‚ï¼ˆä¾‹: å¹´é½¢ã¨å†å…¥é™¢ãƒªã‚¹ã‚¯ã®Uå­—å‹é–¢ä¿‚ï¼‰ã‚’å¯è¦–åŒ–ã§ãã‚‹ã€‚åŒ»å¸«ãŒè¨ºæ–­ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç†è§£ã—ã‚„ã™ã„

å•é¡Œ4: ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®æ¤œå‡º

**å•é¡Œ** : Feature Importanceã‚’ä½¿ã£ã¦ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’æ¤œå‡ºã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã€ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚

**è§£ç­”ä¾‹** :
    
    
    """
    ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®æ¤œå‡ºæ–¹æ³•
    """
    import pandas as pd
    from sklearn.ensemble import RandomForestClassifier
    
    # ç–‘ã‚ã—ã„ç‰¹å¾´é‡ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ
    suspicious_features = [
        'id', 'timestamp', 'created_at', 'updated_at',
        'target', 'label', 'outcome'  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ãã®ã‚‚ã®ã‚„ãã®ãƒªãƒ¼ã‚¯
    ]
    
    # Feature Importanceã‚’è¨ˆç®—
    model = RandomForestClassifier()
    # model.fit(X_train, y_train)
    
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    # ä¸Šä½ã®ç‰¹å¾´é‡ã‚’ãƒã‚§ãƒƒã‚¯
    top_features = feature_importance.head(5)
    for _, row in top_features.iterrows():
        feature = row['feature']
        importance = row['importance']
    
        # ç–‘ã‚ã—ã„ç‰¹å¾´é‡ãŒä¸Šä½ã«ã‚ã‚‹ã‹
        if any(suspect in feature.lower() for suspect in suspicious_features):
            print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®å¯èƒ½æ€§: {feature} (é‡è¦åº¦: {importance:.3f})")
    
        # é‡è¦åº¦ãŒç•°å¸¸ã«é«˜ã„ã‹ï¼ˆ>0.9ï¼‰
        if importance > 0.9:
            print(f"âš ï¸ ç•°å¸¸ã«é«˜ã„é‡è¦åº¦: {feature} (é‡è¦åº¦: {importance:.3f})")
    

å•é¡Œ5: è§£é‡ˆæ€§ã®è©•ä¾¡

**å•é¡Œ** : è§£é‡ˆæ‰‹æ³•ã®ã€ŒFidelityï¼ˆå¿ å®Ÿåº¦ï¼‰ã€ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã€ã©ã®ã‚ˆã†ãªæŒ‡æ¨™ã‚„æ–¹æ³•ã‚’ä½¿ç”¨ã§ãã¾ã™ã‹ï¼Ÿ

**è§£ç­”ä¾‹** :

  * **RÂ²ã‚¹ã‚³ã‚¢** : èª¬æ˜ãƒ¢ãƒ‡ãƒ«ï¼ˆLIMEã®ç·šå½¢è¿‘ä¼¼ãªã©ï¼‰ã¨å…ƒã®ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã®ä¸€è‡´åº¦
  * **äºˆæ¸¬èª¤å·®** : èª¬æ˜ãƒ¢ãƒ‡ãƒ«ã¨å…ƒãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã®å¹³å‡çµ¶å¯¾èª¤å·®ï¼ˆMAEï¼‰
  * **åˆ†é¡ç²¾åº¦ã®æ¯”è¼ƒ** : èª¬æ˜ãƒ¢ãƒ‡ãƒ«ãŒå…ƒãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’ã©ã‚Œã ã‘æ­£ç¢ºã«å†ç¾ã§ãã‚‹ã‹
  * **ãƒ­ãƒ¼ã‚«ãƒ«å¿ å®Ÿåº¦** : ç‰¹å®šã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®è¿‘å‚ã«ãŠã„ã¦ã€èª¬æ˜ãƒ¢ãƒ‡ãƒ«ãŒã©ã‚Œã ã‘æ­£ç¢ºã‹

å•é¡Œ6: å®Ÿè£…èª²é¡Œ

**å•é¡Œ** : scikit-learnã®ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆã¾ãŸã¯ä»»æ„ã®ãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€ä»¥ä¸‹ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚

  1. ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€ä¿‚æ•°ã‚’è§£é‡ˆã™ã‚‹
  2. æ±ºå®šæœ¨ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€ãƒ«ãƒ¼ãƒ«ã‚’æŠ½å‡ºã™ã‚‹
  3. ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€Feature Importanceã‚’å¯è¦–åŒ–ã™ã‚‹
  4. 3ã¤ã®ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆå®¹æ˜“æ€§ã‚’æ¯”è¼ƒã™ã‚‹

**ãƒ’ãƒ³ãƒˆ** :
    
    
    from sklearn.datasets import fetch_openml
    import pandas as pd
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    titanic = fetch_openml('titanic', version=1, as_frame=True, parser='auto')
    df = titanic.frame
    
    # å‰å‡¦ç†ï¼ˆæ¬ æå€¤å‡¦ç†ã€ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã©ï¼‰
    # ...
    
    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨è§£é‡ˆ
    # ...
    

* * *

## ã¾ã¨ã‚

ã“ã®ç« ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«è§£é‡ˆæ€§ã®åŸºç¤ã«ã¤ã„ã¦å­¦ã³ã¾ã—ãŸï¼š

  * âœ… **é‡è¦æ€§** : ä¿¡é ¼æ€§ã€è¦åˆ¶å¯¾å¿œã€ãƒ‡ãƒãƒƒã‚°ã€ãƒã‚¤ã‚¢ã‚¹æ¤œå‡ºã«ä¸å¯æ¬ 
  * âœ… **åˆ†é¡** : ã‚°ãƒ­ãƒ¼ãƒãƒ«/ãƒ­ãƒ¼ã‚«ãƒ«ã€ãƒ¢ãƒ‡ãƒ«å›ºæœ‰/éä¾å­˜ã€äº‹å‰/äº‹å¾Œè§£é‡ˆæ€§
  * âœ… **è§£é‡ˆå¯èƒ½ãƒ¢ãƒ‡ãƒ«** : ç·šå½¢å›å¸°ã€æ±ºå®šæœ¨ã€ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã€GAM
  * âœ… **è§£é‡ˆæ‰‹æ³•** : Feature Importance, PDP, SHAP, LIME, Saliency Maps
  * âœ… **è©•ä¾¡åŸºæº–** : Fidelity, Consistency, Stability, Comprehensibility

æ¬¡ç« ã§ã¯ã€Feature Importanceã¨Permutation Importanceã«ã¤ã„ã¦è©³ã—ãå­¦ã³ã¾ã™ã€‚

* * *

## å‚è€ƒæ–‡çŒ®

  * Molnar, C. (2022). _Interpretable Machine Learning: A Guide for Making Black Box Models Explainable_. <https://christophm.github.io/interpretable-ml-book/>
  * Lundberg, S. M., & Lee, S. I. (2017). "A Unified Approach to Interpreting Model Predictions." _NIPS 2017_.
  * Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). "Why Should I Trust You?: Explaining the Predictions of Any Classifier." _KDD 2016_.
  * Rudin, C. (2019). "Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead." _Nature Machine Intelligence_.
  * European Union. (2016). _General Data Protection Regulation (GDPR)_. Article 22.
  * Doshi-Velez, F., & Kim, B. (2017). "Towards A Rigorous Science of Interpretable Machine Learning." _arXiv:1702.08608_.
