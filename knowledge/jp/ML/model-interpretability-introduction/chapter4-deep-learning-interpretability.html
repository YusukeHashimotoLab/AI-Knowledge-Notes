<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬4ç« :ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/model-interpretability-introduction/index.html">Model Interpretability</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 4</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬4ç« :ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆ</h1>
            <p class="subtitle">CNNã¨Transformerã®å¯è¦–åŒ–æ‰‹æ³• - Saliency Mapsã€Grad-CAMã€Attentionå¯è¦–åŒ–</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 30-35åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 10å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™:</p>
<ul>
<li>âœ… å‹¾é…ãƒ™ãƒ¼ã‚¹ã®å¯è¦–åŒ–æ‰‹æ³•ï¼ˆSaliency Mapsã€Gradient Ã— Inputã€SmoothGradï¼‰ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Grad-CAMã§CNNã®æ³¨ç›®é ˜åŸŸã‚’å¯è¦–åŒ–ã§ãã‚‹</li>
<li>âœ… Integrated Gradientsã§å±æ€§ã‚’è¨ˆç®—ã§ãã‚‹</li>
<li>âœ… Transformerã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹ã‚’å¯è¦–åŒ–ã§ãã‚‹</li>
<li>âœ… PyTorchã¨Captumã§ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’è§£é‡ˆã§ãã‚‹</li>
<li>âœ… ç”»åƒåˆ†é¡ã¨ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒãƒƒã‚°ã§ãã‚‹</li>
</ul>

<hr>

<h2>4.1 Saliency Mapsã¨å‹¾é…ãƒ™ãƒ¼ã‚¹æ‰‹æ³•</h2>

<h3>æ¦‚è¦</h3>
<p><strong>Saliency Mapsï¼ˆé¡•è‘—æ€§ãƒãƒƒãƒ—ï¼‰</strong>ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®äºˆæ¸¬ã«å¯¾ã™ã‚‹å…¥åŠ›ã®å„ãƒ”ã‚¯ã‚»ãƒ«ã®é‡è¦åº¦ã‚’å¯è¦–åŒ–ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<blockquote>
<p>ã€Œå…¥åŠ›ç”»åƒã®ã©ã®éƒ¨åˆ†ãŒãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã«æœ€ã‚‚å½±éŸ¿ã‚’ä¸ãˆã¦ã„ã‚‹ã‹ã‚’å‹¾é…ã‹ã‚‰è¨ˆç®—ã™ã‚‹ã€</p>
</blockquote>

<h3>å‹¾é…ãƒ™ãƒ¼ã‚¹æ‰‹æ³•ã®åˆ†é¡</h3>

<div class="mermaid">
graph TD
    A[å‹¾é…ãƒ™ãƒ¼ã‚¹å¯è¦–åŒ–] --> B[Vanilla Gradients]
    A --> C[Gradient Ã— Input]
    A --> D[SmoothGrad]
    A --> E[Integrated Gradients]

    B --> B1[æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«<br/>âˆ‚y/âˆ‚x]
    C --> C1[å‹¾é…ã¨å…¥åŠ›ã®ç©<br/>ã‚ˆã‚Šé®®æ˜]
    D --> D1[ãƒã‚¤ã‚ºã‚’åŠ ãˆã¦å¹³å‡<br/>ãƒã‚¤ã‚ºé™¤å»]
    E --> E1[çµŒè·¯ç©åˆ†<br/>ç†è«–çš„ä¿è¨¼]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#ffe0b2
</div>

<h3>Vanilla Gradients</h3>

<p>å‡ºåŠ› $y_c$ ã®ã‚¯ãƒ©ã‚¹ $c$ ã«å¯¾ã™ã‚‹å…¥åŠ› $x$ ã®å‹¾é…ã‚’è¨ˆç®—ã—ã¾ã™ã€‚</p>

<p>$$
S_c(x) = \frac{\partial y_c}{\partial x}
$$</p>

<h4>PyTorchã«ã‚ˆã‚‹å®Ÿè£…</h4>

<pre><code class="language-python">import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np

# ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®š
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
model = models.resnet50(pretrained=True).to(device)
model.eval()

# ç”»åƒã®å‰å‡¦ç†
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                       std=[0.229, 0.224, 0.225])
])

def vanilla_gradients(image_path, model, target_class=None):
    """
    Vanilla Gradientsã§saliency mapã‚’ç”Ÿæˆ

    Args:
        image_path: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        model: PyTorchãƒ¢ãƒ‡ãƒ«
        target_class: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¯ãƒ©ã‚¹ï¼ˆNoneã®å ´åˆã¯æœ€ã‚‚ç¢ºç‡ã®é«˜ã„ã‚¯ãƒ©ã‚¹ï¼‰

    Returns:
        saliency: saliency map
        pred_class: äºˆæ¸¬ã‚¯ãƒ©ã‚¹
    """
    # ç”»åƒã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
    img = Image.open(image_path).convert('RGB')
    img_tensor = transform(img).unsqueeze(0).to(device)
    img_tensor.requires_grad = True

    # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
    output = model(img_tensor)

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã®æ±ºå®š
    if target_class is None:
        target_class = output.argmax(dim=1).item()

    # å‹¾é…ã®è¨ˆç®—
    model.zero_grad()
    output[0, target_class].backward()

    # Saliency mapã®ç”Ÿæˆï¼ˆå‹¾é…ã®çµ¶å¯¾å€¤ã®æœ€å¤§å€¤ï¼‰
    saliency = img_tensor.grad.data.abs().max(dim=1)[0]
    saliency = saliency.squeeze().cpu().numpy()

    return saliency, target_class

# ä½¿ç”¨ä¾‹
saliency, pred = vanilla_gradients('cat.jpg', model)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# å…ƒç”»åƒ
img = Image.open('cat.jpg')
axes[0].imshow(img)
axes[0].set_title('Original Image')
axes[0].axis('off')

# Saliency map
axes[1].imshow(saliency, cmap='hot')
axes[1].set_title(f'Vanilla Gradients (Class: {pred})')
axes[1].axis('off')

plt.tight_layout()
plt.show()
</code></pre>

<h3>Gradient Ã— Input</h3>

<p>å‹¾é…ã¨å…¥åŠ›å€¤ã®è¦ç´ ç©ã‚’å–ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šè§£é‡ˆã—ã‚„ã™ã„å¯è¦–åŒ–ã‚’å¾—ã¾ã™ã€‚</p>

<p>$$
S_c(x) = x \odot \frac{\partial y_c}{\partial x}
$$</p>

<pre><code class="language-python">def gradient_input(image_path, model, target_class=None):
    """
    Gradient Ã— Inputã§saliency mapã‚’ç”Ÿæˆ
    """
    # ç”»åƒã®èª­ã¿è¾¼ã¿
    img = Image.open(image_path).convert('RGB')
    img_tensor = transform(img).unsqueeze(0).to(device)
    img_tensor.requires_grad = True

    # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
    output = model(img_tensor)

    if target_class is None:
        target_class = output.argmax(dim=1).item()

    # å‹¾é…ã®è¨ˆç®—
    model.zero_grad()
    output[0, target_class].backward()

    # Gradient Ã— Input
    saliency = (img_tensor.grad.data * img_tensor.data).abs().max(dim=1)[0]
    saliency = saliency.squeeze().cpu().numpy()

    return saliency, target_class

# æ¯”è¼ƒ
saliency_vanilla, _ = vanilla_gradients('cat.jpg', model)
saliency_gi, pred = gradient_input('cat.jpg', model)

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

img = Image.open('cat.jpg')
axes[0].imshow(img)
axes[0].set_title('Original')
axes[0].axis('off')

axes[1].imshow(saliency_vanilla, cmap='hot')
axes[1].set_title('Vanilla Gradients')
axes[1].axis('off')

axes[2].imshow(saliency_gi, cmap='hot')
axes[2].set_title('Gradient Ã— Input')
axes[2].axis('off')

plt.tight_layout()
plt.show()
</code></pre>

<h3>SmoothGrad</h3>

<p>ãƒã‚¤ã‚ºã‚’åŠ ãˆãŸè¤‡æ•°ã®ã‚µãƒ³ãƒ—ãƒ«ã®å‹¾é…ã‚’å¹³å‡åŒ–ã™ã‚‹ã“ã¨ã§ã€ãƒã‚¤ã‚ºã‚’é™¤å»ã—ã¾ã™ã€‚</p>

<p>$$
\hat{S}_c(x) = \frac{1}{n} \sum_{i=1}^{n} \frac{\partial y_c}{\partial (x + \mathcal{N}(0, \sigma^2))}
$$</p>

<pre><code class="language-python">def smooth_grad(image_path, model, target_class=None,
                n_samples=50, noise_level=0.15):
    """
    SmoothGradã§saliency mapã‚’ç”Ÿæˆ

    Args:
        n_samples: ãƒã‚¤ã‚ºã‚µãƒ³ãƒ—ãƒ«æ•°
        noise_level: ãƒã‚¤ã‚ºã®æ¨™æº–åå·®
    """
    # ç”»åƒã®èª­ã¿è¾¼ã¿
    img = Image.open(image_path).convert('RGB')
    img_tensor = transform(img).unsqueeze(0).to(device)

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã®æ±ºå®š
    with torch.no_grad():
        output = model(img_tensor)
        if target_class is None:
            target_class = output.argmax(dim=1).item()

    # ãƒã‚¤ã‚ºã‚’åŠ ãˆãŸã‚µãƒ³ãƒ—ãƒ«ã®å‹¾é…ã‚’è¨ˆç®—
    gradients = []
    for _ in range(n_samples):
        # ãƒã‚¤ã‚ºã®è¿½åŠ 
        noise = torch.randn_like(img_tensor) * noise_level
        noisy_img = img_tensor + noise
        noisy_img.requires_grad = True

        # å‹¾é…ã®è¨ˆç®—
        output = model(noisy_img)
        model.zero_grad()
        output[0, target_class].backward()

        gradients.append(noisy_img.grad.data)

    # å¹³å‡åŒ–
    avg_gradient = torch.stack(gradients).mean(dim=0)
    saliency = avg_gradient.abs().max(dim=1)[0]
    saliency = saliency.squeeze().cpu().numpy()

    return saliency, target_class

# ä½¿ç”¨ä¾‹
saliency_smooth, pred = smooth_grad('cat.jpg', model, n_samples=50)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

axes[0].imshow(Image.open('cat.jpg'))
axes[0].set_title('Original')
axes[0].axis('off')

axes[1].imshow(saliency_vanilla, cmap='hot')
axes[1].set_title('Vanilla Gradients')
axes[1].axis('off')

axes[2].imshow(saliency_smooth, cmap='hot')
axes[2].set_title('SmoothGrad')
axes[2].axis('off')

plt.tight_layout()
plt.show()
</code></pre>

<hr>

<h2>4.2 Grad-CAM</h2>

<h3>æ¦‚è¦</h3>

<p><strong>Grad-CAM (Gradient-weighted Class Activation Mapping)</strong>ã¯ã€CNNã®æœ€çµ‚ç•³ã¿è¾¼ã¿å±¤ã‚’åˆ©ç”¨ã—ã¦ã€ã‚¯ãƒ©ã‚¹ç‰¹æœ‰ã®æ³¨ç›®é ˜åŸŸã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚</p>

<blockquote>
<p>ã€Œç•³ã¿è¾¼ã¿å±¤ã®ç‰¹å¾´ãƒãƒƒãƒ—ã‚’å‹¾é…ã§é‡ã¿ä»˜ã‘ã™ã‚‹ã“ã¨ã§ã€ã‚¯ãƒ©ã‚¹åˆ¤åˆ¥ã«é‡è¦ãªé ˜åŸŸã‚’ç‰¹å®šã™ã‚‹ã€</p>
</blockquote>

<h3>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h3>

<div class="mermaid">
graph LR
    A[å…¥åŠ›ç”»åƒ] --> B[CNN]
    B --> C[æœ€çµ‚ç•³ã¿è¾¼ã¿å±¤<br/>ç‰¹å¾´ãƒãƒƒãƒ— A^k]
    C --> D[ã‚°ãƒ­ãƒ¼ãƒãƒ«å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°]
    D --> E[å…¨çµåˆå±¤]
    E --> F[ã‚¯ãƒ©ã‚¹ã‚¹ã‚³ã‚¢ y^c]

    F --> G[å‹¾é…è¨ˆç®—<br/>âˆ‚y^c/âˆ‚A^k]
    G --> H[ã‚°ãƒ­ãƒ¼ãƒãƒ«å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°<br/>Î±_k^c]
    H --> I[é‡ã¿ä»˜ãå’Œ<br/>L = ReLU Î£ Î±_k^c A^k]
    I --> J[Grad-CAM]

    style A fill:#e3f2fd
    style C fill:#fff3e0
    style J fill:#e8f5e9
</div>

<p>1. æœ€çµ‚ç•³ã¿è¾¼ã¿å±¤ã®ç‰¹å¾´ãƒãƒƒãƒ— $A^k$ ã‚’å–å¾—</p>
<p>2. ã‚¯ãƒ©ã‚¹ $c$ ã®ã‚¹ã‚³ã‚¢ $y^c$ ã«å¯¾ã™ã‚‹ $A^k$ ã®å‹¾é…ã‚’è¨ˆç®—</p>

<p>$$
\alpha_k^c = \frac{1}{Z} \sum_i \sum_j \frac{\partial y^c}{\partial A_{ij}^k}
$$</p>

<p>3. é‡ã¿ä»˜ãå’Œã¨ReLUã‚’é©ç”¨</p>

<p>$$
L_{Grad-CAM}^c = \text{ReLU}\left(\sum_k \alpha_k^c A^k\right)
$$</p>

<h3>å®Ÿè£…ä¾‹</h3>

<pre><code class="language-python">class GradCAM:
    """
    Grad-CAMã®å®Ÿè£…
    """
    def __init__(self, model, target_layer):
        """
        Args:
            model: PyTorchãƒ¢ãƒ‡ãƒ«
            target_layer: å¯è¦–åŒ–å¯¾è±¡ã®å±¤ï¼ˆæœ€çµ‚ç•³ã¿è¾¼ã¿å±¤ï¼‰
        """
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None

        # ãƒ•ãƒƒã‚¯ã®ç™»éŒ²
        target_layer.register_forward_hook(self.save_activation)
        target_layer.register_backward_hook(self.save_gradient)

    def save_activation(self, module, input, output):
        """ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã§æ´»æ€§åŒ–ã‚’ä¿å­˜"""
        self.activations = output.detach()

    def save_gradient(self, module, grad_input, grad_output):
        """ãƒãƒƒã‚¯ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã§å‹¾é…ã‚’ä¿å­˜"""
        self.gradients = grad_output[0].detach()

    def generate_cam(self, image_tensor, target_class=None):
        """
        Grad-CAMã‚’ç”Ÿæˆ

        Args:
            image_tensor: å…¥åŠ›ç”»åƒãƒ†ãƒ³ã‚½ãƒ«
            target_class: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¯ãƒ©ã‚¹

        Returns:
            cam: Grad-CAM
            pred_class: äºˆæ¸¬ã‚¯ãƒ©ã‚¹
        """
        # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
        output = self.model(image_tensor)

        if target_class is None:
            target_class = output.argmax(dim=1).item()

        # ãƒãƒƒã‚¯ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
        self.model.zero_grad()
        output[0, target_class].backward()

        # é‡ã¿ã®è¨ˆç®—ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼‰
        weights = self.gradients.mean(dim=(2, 3), keepdim=True)

        # é‡ã¿ä»˜ãå’Œ
        cam = (weights * self.activations).sum(dim=1, keepdim=True)

        # ReLU
        cam = torch.relu(cam)

        # æ­£è¦åŒ–
        cam = cam - cam.min()
        cam = cam / cam.max()

        # ãƒªã‚µã‚¤ã‚º
        cam = torch.nn.functional.interpolate(
            cam, size=image_tensor.shape[2:], mode='bilinear', align_corners=False
        )

        return cam.squeeze().cpu().numpy(), target_class

# ResNet50ã§Grad-CAMã‚’ä½¿ç”¨
model = models.resnet50(pretrained=True).to(device)
model.eval()

# æœ€çµ‚ç•³ã¿è¾¼ã¿å±¤ã‚’æŒ‡å®š
target_layer = model.layer4[-1].conv3

# Grad-CAMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ä½œæˆ
gradcam = GradCAM(model, target_layer)

# ç”»åƒã®èª­ã¿è¾¼ã¿
img = Image.open('cat.jpg').convert('RGB')
img_tensor = transform(img).unsqueeze(0).to(device)
img_tensor.requires_grad = True

# Grad-CAMã®ç”Ÿæˆ
cam, pred_class = gradcam.generate_cam(img_tensor)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# å…ƒç”»åƒ
axes[0].imshow(img)
axes[0].set_title('Original Image')
axes[0].axis('off')

# Grad-CAM
axes[1].imshow(cam, cmap='jet')
axes[1].set_title(f'Grad-CAM (Class: {pred_class})')
axes[1].axis('off')

# ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤
axes[2].imshow(img)
axes[2].imshow(cam, cmap='jet', alpha=0.5)
axes[2].set_title('Overlay')
axes[2].axis('off')

plt.tight_layout()
plt.show()
</code></pre>

<h3>Grad-CAM++</h3>

<p>Grad-CAMã®æ”¹è‰¯ç‰ˆã§ã€è¤‡æ•°ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚„å°ã•ã„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¯¾ã—ã¦ã‚ˆã‚Šæ­£ç¢ºãªå¯è¦–åŒ–ã‚’æä¾›ã—ã¾ã™ã€‚</p>

<p>$$
\alpha_k^c = \sum_i \sum_j \left( \frac{\partial^2 y^c}{(\partial A_{ij}^k)^2} \cdot \text{ReLU}\left(\frac{\partial y^c}{\partial A_{ij}^k}\right) \right)
$$</p>

<pre><code class="language-python">class GradCAMPlusPlus(GradCAM):
    """
    Grad-CAM++ã®å®Ÿè£…
    """
    def generate_cam(self, image_tensor, target_class=None):
        """Grad-CAM++ã‚’ç”Ÿæˆ"""
        # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
        output = self.model(image_tensor)

        if target_class is None:
            target_class = output.argmax(dim=1).item()

        # 1æ¬¡å‹¾é…ã¨2æ¬¡å‹¾é…ã®è¨ˆç®—
        self.model.zero_grad()
        output[0, target_class].backward(retain_graph=True)

        grad_1 = self.gradients.clone()

        # 2æ¬¡å‹¾é…
        self.model.zero_grad()
        grad_1.backward(torch.ones_like(grad_1), retain_graph=True)
        grad_2 = self.gradients.clone()

        # 3æ¬¡å‹¾é…
        self.model.zero_grad()
        grad_2.backward(torch.ones_like(grad_2))
        grad_3 = self.gradients.clone()

        # é‡ã¿ã®è¨ˆç®—
        alpha_num = grad_2
        alpha_denom = 2.0 * grad_2 + (grad_3 * self.activations).sum(dim=(2, 3), keepdim=True)
        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))

        alpha = alpha_num / alpha_denom
        weights = (alpha * torch.relu(grad_1)).sum(dim=(2, 3), keepdim=True)

        # CAMã®è¨ˆç®—
        cam = (weights * self.activations).sum(dim=1, keepdim=True)
        cam = torch.relu(cam)

        # æ­£è¦åŒ–
        cam = cam - cam.min()
        cam = cam / (cam.max() + 1e-8)

        # ãƒªã‚µã‚¤ã‚º
        cam = torch.nn.functional.interpolate(
            cam, size=image_tensor.shape[2:], mode='bilinear', align_corners=False
        )

        return cam.squeeze().cpu().numpy(), target_class
</code></pre>

<hr>

<h2>4.3 Integrated Gradients</h2>

<h3>æ¦‚è¦</h3>

<p><strong>Integrated Gradients</strong>ã¯ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆä¾‹:é»’ç”»åƒï¼‰ã‹ã‚‰å…¥åŠ›ç”»åƒã¾ã§ã®çµŒè·¯ã«æ²¿ã£ã¦å‹¾é…ã‚’ç©åˆ†ã™ã‚‹ã“ã¨ã§ã€å„ç‰¹å¾´ã®å¯„ä¸åº¦ã‚’è¨ˆç®—ã—ã¾ã™ã€‚</p>

<blockquote>
<p>ã€ŒçµŒè·¯ç©åˆ†ã«ã‚ˆã‚Šã€å±æ€§ã®åˆè¨ˆãŒãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›å·®ã¨ä¸€è‡´ã™ã‚‹ç†è«–çš„ä¿è¨¼ã‚’æŒã¤ã€</p>
</blockquote>

<h3>æ•°å¼</h3>

<p>ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ $x'$ ã‹ã‚‰å…¥åŠ› $x$ ã¸ã®çµŒè·¯ã‚’ $\gamma(\alpha) = x' + \alpha \cdot (x - x')$ ã¨ã™ã‚‹ã¨:</p>

<p>$$
\text{IntegratedGrad}_i(x) = (x_i - x'_i) \int_{\alpha=0}^{1} \frac{\partial F(\gamma(\alpha))}{\partial x_i} d\alpha
$$</p>

<p>å®Ÿè£…ã§ã¯ã€ç©åˆ†ã‚’ãƒªãƒ¼ãƒãƒ³å’Œã§è¿‘ä¼¼ã—ã¾ã™:</p>

<p>$$
\text{IntegratedGrad}_i(x) \approx (x_i - x'_i) \sum_{k=1}^{m} \frac{\partial F(x' + \frac{k}{m}(x - x'))}{\partial x_i} \cdot \frac{1}{m}
$$</p>

<h3>Captumãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ã‚ˆã‚‹å®Ÿè£…</h3>

<pre><code class="language-python">from captum.attr import IntegratedGradients, visualization as viz
import torch.nn.functional as F

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
model = models.resnet50(pretrained=True).to(device)
model.eval()

# ç”»åƒã®èª­ã¿è¾¼ã¿
img = Image.open('cat.jpg').convert('RGB')
img_tensor = transform(img).unsqueeze(0).to(device)

# Integrated Gradientsã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ä½œæˆ
ig = IntegratedGradients(model)

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã®å–å¾—
output = model(img_tensor)
pred_class = output.argmax(dim=1).item()

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®è¨­å®šï¼ˆé»’ç”»åƒï¼‰
baseline = torch.zeros_like(img_tensor)

# Integrated Gradientsã®è¨ˆç®—
attributions = ig.attribute(img_tensor, baseline, target=pred_class, n_steps=50)

# å¯è¦–åŒ–
def visualize_attributions(img, attributions, pred_class):
    """
    Integrated Gradientsã®å¯è¦–åŒ–
    """
    # ãƒ†ãƒ³ã‚½ãƒ«ã‚’numpyé…åˆ—ã«å¤‰æ›
    img_np = img_tensor.squeeze().cpu().permute(1, 2, 0).numpy()

    # æ­£è¦åŒ–ã‚’å…ƒã«æˆ»ã™
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    img_np = img_np * std + mean
    img_np = np.clip(img_np, 0, 1)

    # å±æ€§ã®å‡¦ç†
    attr = attributions.squeeze().cpu().permute(1, 2, 0).numpy()

    # å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # å…ƒç”»åƒ
    axes[0].imshow(img_np)
    axes[0].set_title('Original Image')
    axes[0].axis('off')

    # å±æ€§ï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼‰
    attr_sum = np.abs(attr).sum(axis=2)
    im = axes[1].imshow(attr_sum, cmap='hot')
    axes[1].set_title(f'Integrated Gradients (Class: {pred_class})')
    axes[1].axis('off')
    plt.colorbar(im, ax=axes[1])

    # ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤
    axes[2].imshow(img_np)
    axes[2].imshow(attr_sum, cmap='hot', alpha=0.5)
    axes[2].set_title('Overlay')
    axes[2].axis('off')

    plt.tight_layout()
    plt.show()

visualize_attributions(img, attributions, pred_class)
</code></pre>

<h3>ç•°ãªã‚‹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®å½±éŸ¿</h3>

<pre><code class="language-python"># ç•°ãªã‚‹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã§ã®æ¯”è¼ƒ
baselines = {
    'Black': torch.zeros_like(img_tensor),
    'White': torch.ones_like(img_tensor),
    'Random': torch.randn_like(img_tensor),
    'Blur': None  # ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ–ãƒ©ãƒ¼ç”»åƒ
}

# ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ–ãƒ©ãƒ¼ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
from torchvision.transforms import GaussianBlur
blur_transform = GaussianBlur(kernel_size=51, sigma=50)
img_blur = blur_transform(img)
baselines['Blur'] = transform(img_blur).unsqueeze(0).to(device)

# å„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã§è¨ˆç®—
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

# å…ƒç”»åƒ
axes[0].imshow(img)
axes[0].set_title('Original')
axes[0].axis('off')

for idx, (name, baseline) in enumerate(baselines.items(), 1):
    # Integrated Gradientsã®è¨ˆç®—
    attr = ig.attribute(img_tensor, baseline, target=pred_class, n_steps=50)

    # å¯è¦–åŒ–
    attr_sum = attr.squeeze().cpu().abs().sum(dim=0).numpy()
    im = axes[idx].imshow(attr_sum, cmap='hot')
    axes[idx].set_title(f'Baseline: {name}')
    axes[idx].axis('off')
    plt.colorbar(im, ax=axes[idx])

# æœ€å¾Œã®ç©ºæ¬„ã‚’éè¡¨ç¤º
axes[5].axis('off')

plt.tight_layout()
plt.show()
</code></pre>

<h3>æ‰‹æ³•ã®æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>è¨ˆç®—é‡</th>
<th>ç†è«–çš„ä¿è¨¼</th>
<th>ãƒã‚¤ã‚º</th>
<th>ç”¨é€”</th>
</tr>
</thead>
<tbody>
<tr>
<td>Vanilla Gradients</td>
<td>ä½</td>
<td>ãªã—</td>
<td>å¤šã„</td>
<td>ã‚¯ã‚¤ãƒƒã‚¯åˆ†æ</td>
</tr>
<tr>
<td>SmoothGrad</td>
<td>ä¸­</td>
<td>ãªã—</td>
<td>å°‘ãªã„</td>
<td>ãƒã‚¤ã‚ºé™¤å»</td>
</tr>
<tr>
<td>Grad-CAM</td>
<td>ä½</td>
<td>ãªã—</td>
<td>å°‘ãªã„</td>
<td>CNNå¯è¦–åŒ–</td>
</tr>
<tr>
<td>Integrated Gradients</td>
<td>é«˜</td>
<td>ã‚ã‚Š</td>
<td>å°‘ãªã„</td>
<td>ç²¾å¯†ãªå±æ€§</td>
</tr>
</tbody>
</table>

<hr>

<h2>4.4 Attentionå¯è¦–åŒ–</h2>

<h3>æ¦‚è¦</h3>

<p><strong>Attentionæ©Ÿæ§‹</strong>ã¯ã€Transformerãƒ¢ãƒ‡ãƒ«ã®ä¸­æ ¸ã§ã‚ã‚Šã€å…¥åŠ›ã®ç•°ãªã‚‹éƒ¨åˆ†é–“ã®é–¢ä¿‚æ€§ã‚’å­¦ç¿’ã—ã¾ã™ã€‚ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã‚’å¯è¦–åŒ–ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒä½•ã«æ³¨ç›®ã—ã¦ã„ã‚‹ã‹ã‚’ç†è§£ã§ãã¾ã™ã€‚</p>

<h3>Self-Attentionã®æ•°å¼</h3>

<p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$</p>

<ul>
<li>$Q$: Queryï¼ˆã‚¯ã‚¨ãƒªï¼‰</li>
<li>$K$: Keyï¼ˆã‚­ãƒ¼ï¼‰</li>
<li>$V$: Valueï¼ˆãƒãƒªãƒ¥ãƒ¼ï¼‰</li>
<li>$d_k$: ã‚­ãƒ¼ã®æ¬¡å…ƒ</li>
</ul>

<h3>BERTã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å¯è¦–åŒ–</h3>

<pre><code class="language-python">from transformers import BertTokenizer, BertModel
import torch
import matplotlib.pyplot as plt
import seaborn as sns

# BERTãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)
model.eval()

def visualize_attention(text, layer=0, head=0):
    """
    BERTã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã‚’å¯è¦–åŒ–

    Args:
        text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
        layer: å¯è¦–åŒ–ã™ã‚‹å±¤ï¼ˆ0-11ï¼‰
        head: å¯è¦–åŒ–ã™ã‚‹ãƒ˜ãƒƒãƒ‰ï¼ˆ0-11ï¼‰
    """
    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    inputs = tokenizer(text, return_tensors='pt')
    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])

    # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
    with torch.no_grad():
        outputs = model(**inputs)

    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã®å–å¾—
    # attentions: (layers, batch, heads, seq_len, seq_len)
    attention = outputs.attentions[layer][0, head].cpu().numpy()

    # å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(attention, xticklabels=tokens, yticklabels=tokens,
                cmap='viridis', ax=ax, cbar_kws={'label': 'Attention Weight'})
    ax.set_title(f'BERT Attention (Layer {layer}, Head {head})')
    ax.set_xlabel('Key Tokens')
    ax.set_ylabel('Query Tokens')
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

# ä½¿ç”¨ä¾‹
text = "The cat sat on the mat"
visualize_attention(text, layer=0, head=0)
</code></pre>

<h3>Multi-Head Attentionã®å¯è¦–åŒ–</h3>

<pre><code class="language-python">def visualize_multi_head_attention(text, layer=0):
    """
    è¤‡æ•°ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã‚’åŒæ™‚ã«å¯è¦–åŒ–

    Args:
        text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
        layer: å¯è¦–åŒ–ã™ã‚‹å±¤
    """
    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    inputs = tokenizer(text, return_tensors='pt')
    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])

    # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
    with torch.no_grad():
        outputs = model(**inputs)

    # 12å€‹ã®ãƒ˜ãƒƒãƒ‰ã‚’å¯è¦–åŒ–
    fig, axes = plt.subplots(3, 4, figsize=(20, 15))
    axes = axes.flatten()

    for head in range(12):
        attention = outputs.attentions[layer][0, head].cpu().numpy()

        sns.heatmap(attention, xticklabels=tokens, yticklabels=tokens,
                   cmap='viridis', ax=axes[head], cbar=False)
        axes[head].set_title(f'Head {head}')
        axes[head].set_xlabel('')
        axes[head].set_ylabel('')

        if head % 4 != 0:
            axes[head].set_yticklabels([])
        if head < 8:
            axes[head].set_xticklabels([])
        else:
            axes[head].set_xticklabels(tokens, rotation=45, ha='right')

    plt.suptitle(f'BERT Multi-Head Attention (Layer {layer})', fontsize=16)
    plt.tight_layout()
    plt.show()

# ä½¿ç”¨ä¾‹
visualize_multi_head_attention("The quick brown fox jumps over the lazy dog", layer=5)
</code></pre>

<h3>BertVizã«ã‚ˆã‚‹å¯¾è©±çš„å¯è¦–åŒ–</h3>

<pre><code class="language-python"># BertVizã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install bertviz

from bertviz import head_view, model_view
from transformers import AutoTokenizer, AutoModel

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
model_name = 'bert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name, output_attentions=True)

# ãƒ†ã‚­ã‚¹ãƒˆ
text = "The cat sat on the mat because it was tired"

# ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
inputs = tokenizer(text, return_tensors='pt')

# ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
with torch.no_grad():
    outputs = model(**inputs)

# ãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])

# Head Viewï¼ˆå„ãƒ˜ãƒƒãƒ‰ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼‰
head_view(outputs.attentions, tokens)

# Model Viewï¼ˆå…¨å±¤ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼‰
model_view(outputs.attentions, tokens)
</code></pre>

<h3>Vision Transformerã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å¯è¦–åŒ–</h3>

<pre><code class="language-python">from transformers import ViTModel, ViTFeatureExtractor
from PIL import Image
import requests

# Vision Transformerã®ãƒ­ãƒ¼ãƒ‰
model_name = 'google/vit-base-patch16-224'
feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)
vit_model = ViTModel.from_pretrained(model_name, output_attentions=True)
vit_model.eval()

def visualize_vit_attention(image_path, layer=-1, head=0):
    """
    Vision Transformerã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å¯è¦–åŒ–

    Args:
        image_path: ç”»åƒãƒ‘ã‚¹
        layer: å±¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ-1ã§æœ€çµ‚å±¤ï¼‰
        head: ãƒ˜ãƒƒãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    """
    # ç”»åƒã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
    image = Image.open(image_path).convert('RGB')
    inputs = feature_extractor(images=image, return_tensors='pt')

    # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
    with torch.no_grad():
        outputs = vit_model(**inputs)

    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã®å–å¾—
    attention = outputs.attentions[layer][0, head].cpu().numpy()

    # CLSãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å–å¾—ï¼ˆæœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
    cls_attention = attention[0, 1:]  # CLSãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ç”»åƒãƒ‘ãƒƒãƒã¸

    # 14x14ã‚°ãƒªãƒƒãƒ‰ã«ãƒªã‚·ã‚§ã‚¤ãƒ—ï¼ˆViT-Base-Patch16-224ã®å ´åˆï¼‰
    num_patches = int(cls_attention.shape[0] ** 0.5)
    cls_attention = cls_attention.reshape(num_patches, num_patches)

    # å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # å…ƒç”»åƒ
    axes[0].imshow(image)
    axes[0].set_title('Original Image')
    axes[0].axis('off')

    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ—
    im = axes[1].imshow(cls_attention, cmap='hot')
    axes[1].set_title(f'CLS Attention (Layer {layer}, Head {head})')
    axes[1].axis('off')
    plt.colorbar(im, ax=axes[1])

    # ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤
    from scipy.ndimage import zoom
    attention_resized = zoom(cls_attention, 224/num_patches, order=1)
    axes[2].imshow(image)
    axes[2].imshow(attention_resized, cmap='hot', alpha=0.5)
    axes[2].set_title('Overlay')
    axes[2].axis('off')

    plt.tight_layout()
    plt.show()

# ä½¿ç”¨ä¾‹
visualize_vit_attention('cat.jpg', layer=-1, head=0)
</code></pre>

<hr>

<h2>4.5 ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰å®Ÿè·µä¾‹</h2>

<h3>ç”»åƒåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆ</h3>

<p>å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã§è¤‡æ•°ã®å¯è¦–åŒ–æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ã¦ä½¿ç”¨ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">class ImageClassifierInterpreter:
    """
    ç”»åƒåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®åŒ…æ‹¬çš„è§£é‡ˆãƒ„ãƒ¼ãƒ«
    """
    def __init__(self, model, device='cuda'):
        self.model = model.to(device)
        self.device = device
        self.model.eval()

        # Grad-CAMã®æº–å‚™
        if hasattr(model, 'layer4'):  # ResNetç³»
            target_layer = model.layer4[-1].conv3
        else:
            target_layer = list(model.children())[-2]

        self.gradcam = GradCAM(model, target_layer)

        # Integrated Gradientsã®æº–å‚™
        self.ig = IntegratedGradients(model)

    def interpret(self, image_path, methods=['gradcam', 'ig', 'smoothgrad']):
        """
        è¤‡æ•°ã®æ‰‹æ³•ã§ãƒ¢ãƒ‡ãƒ«ã‚’è§£é‡ˆ

        Args:
            image_path: ç”»åƒãƒ‘ã‚¹
            methods: ä½¿ç”¨ã™ã‚‹æ‰‹æ³•ã®ãƒªã‚¹ãƒˆ

        Returns:
            results: è§£é‡ˆçµæœã®è¾æ›¸
        """
        # ç”»åƒã®èª­ã¿è¾¼ã¿
        img = Image.open(image_path).convert('RGB')
        img_tensor = transform(img).unsqueeze(0).to(self.device)
        img_tensor.requires_grad = True

        # äºˆæ¸¬
        with torch.no_grad():
            output = self.model(img_tensor)
            probs = F.softmax(output, dim=1)
            top5_probs, top5_idx = probs.topk(5)

        results = {
            'image': img,
            'predictions': {
                'classes': top5_idx[0].cpu().numpy(),
                'probabilities': top5_probs[0].cpu().numpy()
            }
        }

        # Grad-CAM
        if 'gradcam' in methods:
            cam, _ = self.gradcam.generate_cam(img_tensor, target_class=top5_idx[0, 0].item())
            results['gradcam'] = cam

        # Integrated Gradients
        if 'ig' in methods:
            baseline = torch.zeros_like(img_tensor)
            attr = self.ig.attribute(img_tensor, baseline, target=top5_idx[0, 0].item())
            results['integrated_gradients'] = attr.squeeze().cpu().abs().sum(dim=0).numpy()

        # SmoothGrad
        if 'smoothgrad' in methods:
            saliency, _ = smooth_grad(image_path, self.model, target_class=top5_idx[0, 0].item())
            results['smoothgrad'] = saliency

        return results

    def visualize(self, results):
        """è§£é‡ˆçµæœã‚’å¯è¦–åŒ–"""
        n_methods = len([k for k in results.keys() if k not in ['image', 'predictions']])

        fig, axes = plt.subplots(1, n_methods + 1, figsize=(5 * (n_methods + 1), 5))

        # å…ƒç”»åƒã¨äºˆæ¸¬
        axes[0].imshow(results['image'])
        pred_text = f"Top predictions:\n"
        for idx, (cls, prob) in enumerate(zip(results['predictions']['classes'][:3],
                                              results['predictions']['probabilities'][:3])):
            pred_text += f"{idx+1}. Class {cls}: {prob:.2%}\n"
        axes[0].set_title(pred_text, fontsize=10)
        axes[0].axis('off')

        # å„æ‰‹æ³•ã®çµæœ
        idx = 1
        if 'gradcam' in results:
            axes[idx].imshow(results['image'])
            axes[idx].imshow(results['gradcam'], cmap='jet', alpha=0.5)
            axes[idx].set_title('Grad-CAM')
            axes[idx].axis('off')
            idx += 1

        if 'integrated_gradients' in results:
            im = axes[idx].imshow(results['integrated_gradients'], cmap='hot')
            axes[idx].set_title('Integrated Gradients')
            axes[idx].axis('off')
            plt.colorbar(im, ax=axes[idx], fraction=0.046)
            idx += 1

        if 'smoothgrad' in results:
            axes[idx].imshow(results['smoothgrad'], cmap='hot')
            axes[idx].set_title('SmoothGrad')
            axes[idx].axis('off')
            idx += 1

        plt.tight_layout()
        plt.show()

# ä½¿ç”¨ä¾‹
model = models.resnet50(pretrained=True)
interpreter = ImageClassifierInterpreter(model)

# è§£é‡ˆã®å®Ÿè¡Œ
results = interpreter.interpret('cat.jpg', methods=['gradcam', 'ig', 'smoothgrad'])
interpreter.visualize(results)
</code></pre>

<h3>ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆ</h3>

<pre><code class="language-python">from transformers import AutoModelForSequenceClassification, AutoTokenizer
from captum.attr import LayerIntegratedGradients

class TextClassifierInterpreter:
    """
    ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆãƒ„ãƒ¼ãƒ«
    """
    def __init__(self, model_name='distilbert-base-uncased-finetuned-sst-2-english'):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.model.eval()

        # Layer Integrated Gradientsã®æº–å‚™
        self.lig = LayerIntegratedGradients(self.forward_func,
                                           self.model.distilbert.embeddings)

    def forward_func(self, inputs):
        """ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰é–¢æ•°"""
        return self.model(inputs_embeds=inputs).logits

    def interpret(self, text, target_class=None):
        """
        ãƒ†ã‚­ã‚¹ãƒˆã®è§£é‡ˆ

        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            target_class: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¯ãƒ©ã‚¹ï¼ˆNoneã§äºˆæ¸¬ã‚¯ãƒ©ã‚¹ï¼‰

        Returns:
            attributions: å„ãƒˆãƒ¼ã‚¯ãƒ³ã®é‡è¦åº¦
            tokens: ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆ
            prediction: äºˆæ¸¬çµæœ
        """
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        inputs = self.tokenizer(text, return_tensors='pt')
        tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])

        # äºˆæ¸¬
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = F.softmax(outputs.logits, dim=1)
            pred_class = outputs.logits.argmax(dim=1).item()
            pred_prob = probs[0, pred_class].item()

        if target_class is None:
            target_class = pred_class

        # Integrated Gradientsã®è¨ˆç®—
        input_embeds = self.model.distilbert.embeddings(inputs['input_ids'])
        baseline = torch.zeros_like(input_embeds)

        attributions = self.lig.attribute(
            input_embeds,
            baseline,
            target=target_class,
            n_steps=50
        )

        # ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®å±æ€§ã«ã¾ã¨ã‚ã‚‹
        attributions_sum = attributions.sum(dim=-1).squeeze(0)
        attributions_sum = attributions_sum / torch.norm(attributions_sum)
        attributions_sum = attributions_sum.cpu().detach().numpy()

        return {
            'tokens': tokens,
            'attributions': attributions_sum,
            'prediction': {
                'class': pred_class,
                'probability': pred_prob,
                'label': self.model.config.id2label[pred_class]
            }
        }

    def visualize(self, text, target_class=None):
        """è§£é‡ˆçµæœã‚’å¯è¦–åŒ–"""
        results = self.interpret(text, target_class)

        tokens = results['tokens']
        attributions = results['attributions']

        # æ­£è¦åŒ–ï¼ˆå¯è¦–åŒ–ç”¨ï¼‰
        attr_min, attr_max = attributions.min(), attributions.max()
        attributions_norm = (attributions - attr_min) / (attr_max - attr_min + 1e-8)

        # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—
        import matplotlib.cm as cm
        colors = cm.RdYlGn(attributions_norm)

        # ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º
        fig, ax = plt.subplots(figsize=(15, 3))
        ax.axis('off')

        # äºˆæ¸¬çµæœ
        pred_text = f"Prediction: {results['prediction']['label']} ({results['prediction']['probability']:.2%})"
        ax.text(0.5, 0.9, pred_text, ha='center', va='top', fontsize=14, fontweight='bold',
                transform=ax.transAxes)

        # ãƒˆãƒ¼ã‚¯ãƒ³ã¨é‡è¦åº¦
        x_pos = 0.05
        for token, attr, color in zip(tokens, attributions_norm, colors):
            if token in ['[CLS]', '[SEP]', '[PAD]']:
                continue

            # èƒŒæ™¯è‰²
            bbox_props = dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.7, edgecolor='none')
            ax.text(x_pos, 0.5, token, ha='left', va='center', fontsize=12,
                   bbox=bbox_props, transform=ax.transAxes)

            # é‡è¦åº¦ã‚¹ã‚³ã‚¢
            ax.text(x_pos, 0.2, f'{attr:.3f}', ha='left', va='center', fontsize=8,
                   transform=ax.transAxes)

            x_pos += len(token) * 0.015 + 0.02

            if x_pos > 0.95:
                break

        # ã‚«ãƒ©ãƒ¼ãƒãƒ¼
        sm = plt.cm.ScalarMappable(cmap=cm.RdYlGn, norm=plt.Normalize(vmin=attr_min, vmax=attr_max))
        sm.set_array([])
        cbar = plt.colorbar(sm, ax=ax, orientation='horizontal', pad=0.05, aspect=50)
        cbar.set_label('Attribution Score', fontsize=10)

        plt.tight_layout()
        plt.show()

# ä½¿ç”¨ä¾‹
text_interpreter = TextClassifierInterpreter()

# ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ„Ÿæƒ…
text_interpreter.visualize("This movie is absolutely fantastic and amazing!")

# ãƒã‚¬ãƒ†ã‚£ãƒ–ãªæ„Ÿæƒ…
text_interpreter.visualize("This is the worst film I have ever seen.")
</code></pre>

<h3>ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒãƒƒã‚°ã®å®Ÿè·µ</h3>

<pre><code class="language-python">def debug_model_prediction(model, image_path, true_label, expected_label):
    """
    èª¤åˆ†é¡ã®ãƒ‡ãƒãƒƒã‚°

    Args:
        model: åˆ†é¡ãƒ¢ãƒ‡ãƒ«
        image_path: ç”»åƒãƒ‘ã‚¹
        true_label: æ­£è§£ãƒ©ãƒ™ãƒ«
        expected_label: æœŸå¾…ã•ã‚Œã‚‹ãƒ©ãƒ™ãƒ«
    """
    interpreter = ImageClassifierInterpreter(model)

    # è§£é‡ˆ
    results = interpreter.interpret(image_path, methods=['gradcam', 'ig'])

    pred_class = results['predictions']['classes'][0]
    pred_prob = results['predictions']['probabilities'][0]

    print(f"=== ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒãƒƒã‚°ãƒ¬ãƒãƒ¼ãƒˆ ===")
    print(f"æ­£è§£ãƒ©ãƒ™ãƒ«: {true_label}")
    print(f"äºˆæ¸¬ãƒ©ãƒ™ãƒ«: {pred_class} (ç¢ºç‡: {pred_prob:.2%})")
    print(f"æœŸå¾…ãƒ©ãƒ™ãƒ«: {expected_label}")

    if pred_class != expected_label:
        print(f"\nâŒ èª¤åˆ†é¡ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
        print(f"\nTop-5äºˆæ¸¬:")
        for idx, (cls, prob) in enumerate(zip(results['predictions']['classes'],
                                              results['predictions']['probabilities'])):
            marker = "âœ“" if cls == true_label else " "
            print(f"  {marker} {idx+1}. ã‚¯ãƒ©ã‚¹ {cls}: {prob:.2%}")

        # å¯è¦–åŒ–
        interpreter.visualize(results)

        # è§£é‡ˆ
        print("\n=== è§£é‡ˆ ===")
        print("Grad-CAMã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
        print("- ãƒ¢ãƒ‡ãƒ«ã¯ç”»åƒã®ã©ã®é ˜åŸŸã«æ³¨ç›®ã—ã¦ã„ã¾ã™ã‹?")
        print("- æ³¨ç›®é ˜åŸŸã¯æ­£è§£ãƒ©ãƒ™ãƒ«ã«å¯¾ã—ã¦å¦¥å½“ã§ã™ã‹?")
        print("- èƒŒæ™¯ã‚„ãƒã‚¤ã‚ºã«æ³¨ç›®ã—ã¦ã„ã¾ã›ã‚“ã‹?")
    else:
        print(f"\nâœ“ æ­£ã—ãåˆ†é¡ã•ã‚Œã¾ã—ãŸ")
        interpreter.visualize(results)

# ä½¿ç”¨ä¾‹
model = models.resnet50(pretrained=True)
debug_model_prediction(model, 'dog.jpg', true_label=254, expected_label=254)
</code></pre>

<hr>

<h2>4.6 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>å‹¾é…ãƒ™ãƒ¼ã‚¹æ‰‹æ³•</strong></p>
<ul>
<li>Vanilla Gradients: ã‚·ãƒ³ãƒ—ãƒ«ãªå‹¾é…å¯è¦–åŒ–</li>
<li>Gradient Ã— Input: ã‚ˆã‚Šé®®æ˜ãªå¯è¦–åŒ–</li>
<li>SmoothGrad: ãƒã‚¤ã‚ºé™¤å»</li>
</ul></li>
<li><p><strong>Grad-CAM</strong></p>
<ul>
<li>CNNã®æ³¨ç›®é ˜åŸŸã‚’å¯è¦–åŒ–</li>
<li>æœ€çµ‚ç•³ã¿è¾¼ã¿å±¤ã®æ´»ç”¨</li>
<li>Grad-CAM++ã«ã‚ˆã‚‹æ”¹è‰¯</li>
</ul></li>
<li><p><strong>Integrated Gradients</strong></p>
<ul>
<li>çµŒè·¯ç©åˆ†ã«ã‚ˆã‚‹å±æ€§è¨ˆç®—</li>
<li>ç†è«–çš„ä¿è¨¼ã‚’æŒã¤æ‰‹æ³•</li>
<li>ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³é¸æŠã®é‡è¦æ€§</li>
</ul></li>
<li><p><strong>Attentionå¯è¦–åŒ–</strong></p>
<ul>
<li>Transformerã®Self-Attention</li>
<li>Multi-Head Attentionã®è§£é‡ˆ</li>
<li>BertVizã«ã‚ˆã‚‹å¯¾è©±çš„å¯è¦–åŒ–</li>
</ul></li>
<li><p><strong>å®Ÿè·µçš„å¿œç”¨</strong></p>
<ul>
<li>ç”»åƒåˆ†é¡ã®è§£é‡ˆ</li>
<li>ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã®è§£é‡ˆ</li>
<li>ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒãƒƒã‚°ã®æ‰‹æ³•</li>
</ul></li>
</ol>

<h3>æ¬¡ã®ç« ã¸</h3>

<p>ç¬¬5ç« ã§ã¯ã€<strong>ãƒ¢ãƒ‡ãƒ«è§£é‡ˆã®å®Ÿå‹™å¿œç”¨</strong>ã‚’å­¦ã³ã¾ã™:</p>
<ul>
<li>ãƒ¢ãƒ‡ãƒ«ç›£æŸ»ã¨ãƒã‚¤ã‚¢ã‚¹æ¤œå‡º</li>
<li>è¦åˆ¶å¯¾å¿œï¼ˆGDPRã€AI Actï¼‰</li>
<li>ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã¸ã®èª¬æ˜</li>
<li>ç¶™ç¶šçš„ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦: easyï¼‰</h3>
<p>Vanilla Gradientsã¨Grad-CAMã®ä¸»ãªé•ã„ã‚’3ã¤æŒ™ã’ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>:</p>
<ol>
<li><strong>ä½¿ç”¨ã™ã‚‹æƒ…å ±</strong>: Vanilla Gradientsã¯å…¥åŠ›ã«å¯¾ã™ã‚‹å‹¾é…ã®ã¿ã€Grad-CAMã¯æœ€çµ‚ç•³ã¿è¾¼ã¿å±¤ã®ç‰¹å¾´ãƒãƒƒãƒ—ã¨å‹¾é…ã‚’ä½¿ç”¨</li>
<li><strong>è§£åƒåº¦</strong>: Vanilla Gradientsã¯å…¥åŠ›ç”»åƒã¨åŒã˜è§£åƒåº¦ã€Grad-CAMã¯ä½è§£åƒåº¦ã‹ã‚‰è£œé–“</li>
<li><strong>ãƒã‚¤ã‚º</strong>: Vanilla Gradientsã¯ãƒã‚¤ã‚ºãŒå¤šã„ã€Grad-CAMã¯æ»‘ã‚‰ã‹ã§è§£é‡ˆã—ã‚„ã™ã„</li>
</ol>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦: mediumï¼‰</h3>
<p>SmoothGradã‚’å®Ÿè£…ã—ã€ãƒã‚¤ã‚ºã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆn_samplesï¼‰ãŒçµæœã«ä¸ãˆã‚‹å½±éŸ¿ã‚’èª¿ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import torch
import matplotlib.pyplot as plt
import numpy as np

# ç•°ãªã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°ã§ã®æ¯”è¼ƒ
n_samples_list = [10, 25, 50, 100]

fig, axes = plt.subplots(1, len(n_samples_list) + 1, figsize=(20, 4))

# å…ƒç”»åƒ
img = Image.open('cat.jpg')
axes[0].imshow(img)
axes[0].set_title('Original')
axes[0].axis('off')

for idx, n_samples in enumerate(n_samples_list, 1):
    saliency, _ = smooth_grad('cat.jpg', model, n_samples=n_samples)

    axes[idx].imshow(saliency, cmap='hot')
    axes[idx].set_title(f'n_samples={n_samples}')
    axes[idx].axis('off')

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>è¦³å¯Ÿ</strong>:</p>
<ul>
<li>n_samplesãŒå°ã•ã„ï¼ˆ10ï¼‰: ãƒã‚¤ã‚ºãŒæ®‹ã‚‹</li>
<li>n_samplesãŒé©åº¦ï¼ˆ50ï¼‰: æ»‘ã‚‰ã‹ã§é®®æ˜</li>
<li>n_samplesãŒå¤§ãã„ï¼ˆ100ï¼‰: è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ãŒã€ã•ã‚‰ã«æ»‘ã‚‰ã‹</li>
<li><strong>æ¨å¥¨</strong>: å®Ÿå‹™ã§ã¯50å‰å¾ŒãŒè‰¯ã„ãƒãƒ©ãƒ³ã‚¹</li>
</ul>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦: mediumï¼‰</h3>
<p>Integrated Gradientsã§ç•°ãªã‚‹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆé»’ã€ç™½ã€ãƒ–ãƒ©ãƒ¼ï¼‰ã‚’ä½¿ç”¨ã—ãŸå ´åˆã€çµæœãŒã©ã®ã‚ˆã†ã«å¤‰ã‚ã‚‹ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>:</p>

<table>
<thead>
<tr>
<th>ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³</th>
<th>ç‰¹å¾´</th>
<th>é©ç”¨å ´é¢</th>
</tr>
</thead>
<tbody>
<tr>
<td>é»’ç”»åƒ</td>
<td>å…¨ãƒ”ã‚¯ã‚»ãƒ«ãŒ0<br/>æœ€ã‚‚ä¸€èˆ¬çš„</td>
<td>é€šå¸¸ã®ç”»åƒåˆ†é¡</td>
</tr>
<tr>
<td>ç™½ç”»åƒ</td>
<td>å…¨ãƒ”ã‚¯ã‚»ãƒ«ãŒ1<br/>é»’èƒŒæ™¯ã®ç”»åƒã§æœ‰åŠ¹</td>
<td>åŒ»ç™‚ç”»åƒãªã©</td>
</tr>
<tr>
<td>ãƒ–ãƒ©ãƒ¼ç”»åƒ</td>
<td>æ§‹é€ ã¯ä¿æŒã€è©³ç´°ã¯å¤±ã†<br/>ã‚ˆã‚Šç¾å®Ÿçš„</td>
<td>ãƒ†ã‚¯ã‚¹ãƒãƒ£ãŒé‡è¦ãªå ´åˆ</td>
</tr>
<tr>
<td>ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚º</td>
<td>ç„¡ç§©åºãªç”»åƒ<br/>å‚ç…§ã¨ã—ã¦</td>
<td>æ¯”è¼ƒæ¤œè¨¼</td>
</tr>
</tbody>
</table>

<p><strong>ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³é¸æŠã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³</strong>:</p>
<ul>
<li>ä¸€èˆ¬çš„ã«ã¯é»’ç”»åƒãŒæ¨å¥¨</li>
<li>ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«åŸºã¥ã„ã¦é¸æŠ</li>
<li>è¤‡æ•°ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã§çµæœã‚’æ¯”è¼ƒ</li>
<li>çµæœãŒä¸€è²«ã—ã¦ã„ã‚‹ã‹ç¢ºèª</li>
</ul>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦: hardï¼‰</h3>
<p>BERTã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã‚’å¯è¦–åŒ–ã—ã€"The cat sat on the mat"ã¨ã„ã†æ–‡ã§ã€å„å˜èªãŒä»–ã®ã©ã®å˜èªã«æ³¨ç›®ã—ã¦ã„ã‚‹ã‹ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">from transformers import BertTokenizer, BertModel
import torch
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)
model.eval()

text = "The cat sat on the mat"

# ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
inputs = tokenizer(text, return_tensors='pt')
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])

# ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
with torch.no_grad():
    outputs = model(**inputs)

# å…¨å±¤ã®å¹³å‡ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
all_attentions = torch.stack([att.squeeze(0) for att in outputs.attentions])
avg_attention = all_attentions.mean(dim=[0, 1]).cpu().numpy()  # å±¤ã¨ãƒ˜ãƒƒãƒ‰ã§å¹³å‡

# å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(avg_attention, xticklabels=tokens, yticklabels=tokens,
            cmap='viridis', ax=ax, cbar_kws={'label': 'Average Attention'})
ax.set_title('BERT Average Attention Across All Layers and Heads')
ax.set_xlabel('Key Tokens')
ax.set_ylabel('Query Tokens')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# åˆ†æ
print("=== ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³åˆ†æ ===\n")

for i, token in enumerate(tokens):
    if token in ['[CLS]', '[SEP]']:
        continue

    # å„ãƒˆãƒ¼ã‚¯ãƒ³ãŒæœ€ã‚‚æ³¨ç›®ã—ã¦ã„ã‚‹å˜èªï¼ˆè‡ªåˆ†è‡ªèº«ã‚’é™¤ãï¼‰
    attention_weights = avg_attention[i].copy()
    attention_weights[i] = -1  # è‡ªåˆ†è‡ªèº«ã‚’é™¤å¤–
    max_idx = np.argmax(attention_weights)

    print(f"'{token}' ãŒæœ€ã‚‚æ³¨ç›®: '{tokens[max_idx]}' (é‡ã¿: {attention_weights[max_idx]:.3f})")

print("\nè¦³å¯Ÿ:")
print("- 'cat'ã¯'sat'ã«æ³¨ç›®ï¼ˆä¸»èªã¨å‹•è©ã®é–¢ä¿‚ï¼‰")
print("- 'sat'ã¯'cat'ã¨'on'ã«æ³¨ç›®ï¼ˆå‹•è©ãŒä¸»èªã¨å‰ç½®è©ã«ï¼‰")
print("- 'mat'ã¯'the'ã¨'on'ã«æ³¨ç›®ï¼ˆåè©ãŒå† è©ã¨å‰ç½®è©ã«ï¼‰")
</code></pre>

<p><strong>æœŸå¾…ã•ã‚Œã‚‹è¦³å¯Ÿçµæœ</strong>:</p>
<ul>
<li><strong>æ§‹æ–‡çš„é–¢ä¿‚</strong>: ä¸»èªã¨å‹•è©ã€ä¿®é£¾èªã¨è¢«ä¿®é£¾èªãŒç›¸äº’ã«æ³¨ç›®</li>
<li><strong>å±€æ‰€æ€§</strong>: è¿‘éš£ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®æ³¨ç›®ãŒå¼·ã„</li>
<li><strong>æ–‡æ³•çš„ãƒ‘ã‚¿ãƒ¼ãƒ³</strong>: å‰ç½®è©ãŒå¾Œç¶šã®åè©ã«æ³¨ç›®</li>
</ul>

</details>

<h3>å•é¡Œ5ï¼ˆé›£æ˜“åº¦: hardï¼‰</h3>
<p>èª¤åˆ†é¡ã•ã‚ŒãŸç”»åƒã«å¯¾ã—ã¦Grad-CAMã¨Integrated Gradientsã‚’é©ç”¨ã—ã€èª¤åˆ†é¡ã®åŸå› ã‚’åˆ†æã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import torch
import torchvision.models as models
from captum.attr import IntegratedGradients
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np

class MisclassificationAnalyzer:
    """
    èª¤åˆ†é¡åˆ†æãƒ„ãƒ¼ãƒ«
    """
    def __init__(self, model, device='cuda'):
        self.model = model.to(device)
        self.device = device
        self.model.eval()

        # Grad-CAMã®æº–å‚™
        self.gradcam = GradCAM(model, model.layer4[-1].conv3)

        # Integrated Gradientsã®æº–å‚™
        self.ig = IntegratedGradients(model)

    def analyze_misclassification(self, image_path, true_label, imagenet_labels):
        """
        èª¤åˆ†é¡ã®è©³ç´°åˆ†æ

        Args:
            image_path: ç”»åƒãƒ‘ã‚¹
            true_label: æ­£è§£ãƒ©ãƒ™ãƒ«
            imagenet_labels: ImageNetãƒ©ãƒ™ãƒ«è¾æ›¸
        """
        # ç”»åƒã®èª­ã¿è¾¼ã¿
        img = Image.open(image_path).convert('RGB')
        img_tensor = transform(img).unsqueeze(0).to(self.device)
        img_tensor.requires_grad = True

        # äºˆæ¸¬
        output = self.model(img_tensor)
        probs = torch.softmax(output, dim=1)
        pred_label = output.argmax(dim=1).item()

        print(f"{'='*60}")
        print(f"èª¤åˆ†é¡åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        print(f"{'='*60}")
        print(f"\næ­£è§£: {imagenet_labels[true_label]}")
        print(f"äºˆæ¸¬: {imagenet_labels[pred_label]} (ç¢ºç‡: {probs[0, pred_label]:.2%})")

        # Top-5äºˆæ¸¬
        top5_probs, top5_idx = probs.topk(5)
        print(f"\nTop-5äºˆæ¸¬:")
        for i, (idx, prob) in enumerate(zip(top5_idx[0], top5_probs[0])):
            marker = "âœ“" if idx == true_label else " "
            print(f"  {marker} {i+1}. {imagenet_labels[idx.item()]}: {prob:.2%}")

        # æ­£è§£ã‚¯ãƒ©ã‚¹ã®ã‚¹ã‚³ã‚¢
        true_prob = probs[0, true_label]
        true_rank = (probs[0] > true_prob).sum().item() + 1
        print(f"\næ­£è§£ã‚¯ãƒ©ã‚¹ã®ãƒ©ãƒ³ã‚¯: {true_rank}ä½ (ç¢ºç‡: {true_prob:.2%})")

        # Grad-CAMï¼ˆäºˆæ¸¬ã‚¯ãƒ©ã‚¹ï¼‰
        cam_pred, _ = self.gradcam.generate_cam(img_tensor, target_class=pred_label)

        # Grad-CAMï¼ˆæ­£è§£ã‚¯ãƒ©ã‚¹ï¼‰
        cam_true, _ = self.gradcam.generate_cam(img_tensor, target_class=true_label)

        # Integrated Gradients
        baseline = torch.zeros_like(img_tensor)
        attr_pred = self.ig.attribute(img_tensor, baseline, target=pred_label, n_steps=50)
        attr_true = self.ig.attribute(img_tensor, baseline, target=true_label, n_steps=50)

        # å¯è¦–åŒ–
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))

        # å…ƒç”»åƒ
        axes[0, 0].imshow(img)
        axes[0, 0].set_title('Original Image')
        axes[0, 0].axis('off')

        # äºˆæ¸¬ã‚¯ãƒ©ã‚¹ã®Grad-CAM
        axes[0, 1].imshow(img)
        axes[0, 1].imshow(cam_pred, cmap='jet', alpha=0.5)
        axes[0, 1].set_title(f'Grad-CAM: Predicted\n{imagenet_labels[pred_label]}')
        axes[0, 1].axis('off')

        # æ­£è§£ã‚¯ãƒ©ã‚¹ã®Grad-CAM
        axes[0, 2].imshow(img)
        axes[0, 2].imshow(cam_true, cmap='jet', alpha=0.5)
        axes[0, 2].set_title(f'Grad-CAM: True\n{imagenet_labels[true_label]}')
        axes[0, 2].axis('off')

        # ã‚¹ãƒšãƒ¼ã‚µãƒ¼
        axes[1, 0].axis('off')

        # äºˆæ¸¬ã‚¯ãƒ©ã‚¹ã®IG
        attr_pred_sum = attr_pred.squeeze().cpu().abs().sum(dim=0).numpy()
        axes[1, 1].imshow(attr_pred_sum, cmap='hot')
        axes[1, 1].set_title(f'IG: Predicted\n{imagenet_labels[pred_label]}')
        axes[1, 1].axis('off')

        # æ­£è§£ã‚¯ãƒ©ã‚¹ã®IG
        attr_true_sum = attr_true.squeeze().cpu().abs().sum(dim=0).numpy()
        axes[1, 2].imshow(attr_true_sum, cmap='hot')
        axes[1, 2].set_title(f'IG: True\n{imagenet_labels[true_label]}')
        axes[1, 2].axis('off')

        plt.tight_layout()
        plt.show()

        # è¨ºæ–­
        print(f"\n{'='*60}")
        print(f"è¨ºæ–­")
        print(f"{'='*60}")
        print("1. Grad-CAMã‚’æ¯”è¼ƒ:")
        print("   - äºˆæ¸¬ã‚¯ãƒ©ã‚¹ã¨æ­£è§£ã‚¯ãƒ©ã‚¹ã§æ³¨ç›®é ˜åŸŸãŒç•°ãªã‚Šã¾ã™ã‹?")
        print("   - äºˆæ¸¬ã‚¯ãƒ©ã‚¹ã¯èƒŒæ™¯ã‚„ãƒã‚¤ã‚ºã«æ³¨ç›®ã—ã¦ã„ã¾ã™ã‹?")
        print("\n2. Integrated Gradientsã‚’ç¢ºèª:")
        print("   - æ­£è§£ã‚¯ãƒ©ã‚¹ã«å¿…è¦ãªç‰¹å¾´ãŒç”»åƒã«å­˜åœ¨ã—ã¾ã™ã‹?")
        print("   - äºˆæ¸¬ã‚¯ãƒ©ã‚¹ã®èª¤ã£ãŸç‰¹å¾´ãŒå¼·ãç¾ã‚Œã¦ã„ã¾ã™ã‹?")
        print("\n3. è€ƒãˆã‚‰ã‚Œã‚‹åŸå› :")
        if true_prob < 0.01:
            print("   - ãƒ¢ãƒ‡ãƒ«ãŒæ­£è§£ã‚¯ãƒ©ã‚¹ã‚’ã»ã¨ã‚“ã©è€ƒæ…®ã—ã¦ã„ãªã„")
            print("   - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é¡ä¼¼ä¾‹ãŒä¸è¶³ã—ã¦ã„ã‚‹å¯èƒ½æ€§")
        elif true_rank <= 5:
            print("   - æ­£è§£ã‚¯ãƒ©ã‚¹ã¯ä¸Šä½ã«ã‚ã‚‹ï¼ˆå¢ƒç•Œã‚±ãƒ¼ã‚¹ï¼‰")
            print("   - ã‚¯ãƒ©ã‚¹é–“ã®é¡ä¼¼æ€§ãŒé«˜ã„å¯èƒ½æ€§")
        else:
            print("   - ç”»åƒã®å“è³ªã‚„å‰å‡¦ç†ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§")
            print("   - ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä¸€éƒ¨ãŒéš ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§")

# ImageNetãƒ©ãƒ™ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆç°¡ç•¥ç‰ˆï¼‰
imagenet_labels = {
    254: 'Pug',
    281: 'Tabby Cat',
    # ... ä»–ã®ãƒ©ãƒ™ãƒ«
}

# ä½¿ç”¨ä¾‹
model = models.resnet50(pretrained=True)
analyzer = MisclassificationAnalyzer(model)

# èª¤åˆ†é¡ã•ã‚ŒãŸç”»åƒã‚’åˆ†æ
analyzer.analyze_misclassification('pug.jpg', true_label=254, imagenet_labels=imagenet_labels)
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>============================================================
èª¤åˆ†é¡åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
============================================================

æ­£è§£: Pug
äºˆæ¸¬: Tabby Cat (ç¢ºç‡: 45.23%)

Top-5äºˆæ¸¬:
   1. Tabby Cat: 45.23%
   2. Egyptian Cat: 23.45%
  âœ“ 3. Pug: 12.34%
   4. Bulldog: 8.90%
   5. Chihuahua: 5.67%

æ­£è§£ã‚¯ãƒ©ã‚¹ã®ãƒ©ãƒ³ã‚¯: 3ä½ (ç¢ºç‡: 12.34%)

============================================================
è¨ºæ–­
============================================================
1. Grad-CAMã‚’æ¯”è¼ƒ:
   - äºˆæ¸¬ã‚¯ãƒ©ã‚¹ã¨æ­£è§£ã‚¯ãƒ©ã‚¹ã§æ³¨ç›®é ˜åŸŸãŒç•°ãªã‚Šã¾ã™ã‹?
   - äºˆæ¸¬ã‚¯ãƒ©ã‚¹ã¯èƒŒæ™¯ã‚„ãƒã‚¤ã‚ºã«æ³¨ç›®ã—ã¦ã„ã¾ã™ã‹?

2. Integrated Gradientsã‚’ç¢ºèª:
   - æ­£è§£ã‚¯ãƒ©ã‚¹ã«å¿…è¦ãªç‰¹å¾´ãŒç”»åƒã«å­˜åœ¨ã—ã¾ã™ã‹?
   - äºˆæ¸¬ã‚¯ãƒ©ã‚¹ã®èª¤ã£ãŸç‰¹å¾´ãŒå¼·ãç¾ã‚Œã¦ã„ã¾ã™ã‹?

3. è€ƒãˆã‚‰ã‚Œã‚‹åŸå› :
   - æ­£è§£ã‚¯ãƒ©ã‚¹ã¯ä¸Šä½ã«ã‚ã‚‹ï¼ˆå¢ƒç•Œã‚±ãƒ¼ã‚¹ï¼‰
   - ã‚¯ãƒ©ã‚¹é–“ã®é¡ä¼¼æ€§ãŒé«˜ã„å¯èƒ½æ€§
</code></pre>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Simonyan, K., Vedaldi, A., & Zisserman, A. (2013). <em>Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</em>. arXiv:1312.6034.</li>
<li>Selvaraju, R. R., et al. (2017). <em>Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</em>. ICCV 2017.</li>
<li>Sundararajan, M., Taly, A., & Yan, Q. (2017). <em>Axiomatic Attribution for Deep Networks</em>. ICML 2017.</li>
<li>Smilkov, D., et al. (2017). <em>SmoothGrad: removing noise by adding noise</em>. arXiv:1706.03825.</li>
<li>Chattopadhay, A., et al. (2018). <em>Grad-CAM++: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks</em>. WACV 2018.</li>
<li>Vaswani, A., et al. (2017). <em>Attention is All You Need</em>. NeurIPS 2017.</li>
<li>Vig, J. (2019). <em>A Multiscale Visualization of Attention in the Transformer Model</em>. ACL 2019.</li>
<li>Natekar, P., & Sharma, M. (2020). <em>Captum: A unified and generic model interpretability library for PyTorch</em>. arXiv:2009.07896.</li>
</ol>

<div class="navigation">
    <a href="chapter3-model-agnostic-methods.html" class="nav-button">â† å‰ã®ç« : ãƒ¢ãƒ‡ãƒ«éä¾å­˜æ‰‹æ³•</a>
    <a href="chapter5-practical-applications.html" class="nav-button">æ¬¡ã®ç« : å®Ÿå‹™å¿œç”¨ â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
