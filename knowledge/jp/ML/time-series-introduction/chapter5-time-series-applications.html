<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬5ç« ï¼šæ™‚ç³»åˆ—åˆ†æã®å®Ÿè·µå¿œç”¨ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
        <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/wp/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/ML/time-series-introduction/index.html">Time Series</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 5</span>
        </div>
    </nav>

    <header>
        <div class="header-content">
            <h1>ç¬¬5ç« ï¼šæ™‚ç³»åˆ—åˆ†æã®å®Ÿè·µå¿œç”¨</h1>
            <p class="subtitle">ç•°å¸¸æ¤œçŸ¥ãƒ»å¤šå¤‰é‡äºˆæ¸¬ãƒ»å› æœæ¨è«–ãƒ»ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã‚·ã‚¹ãƒ†ãƒ </p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 30-35åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 9å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… æ™‚ç³»åˆ—ç•°å¸¸æ¤œçŸ¥ã®å¤šæ§˜ãªæ‰‹æ³•ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… å¤šå¤‰é‡æ™‚ç³»åˆ—äºˆæ¸¬ã¨Grangerå› æœæ€§åˆ†æã‚’è¡Œãˆã‚‹</li>
<li>âœ… å› æœæ¨è«–ã¨ä»‹å…¥åˆ†æã‚’å®Ÿæ–½ã§ãã‚‹</li>
<li>âœ… Facebook Prophetã‚’ç”¨ã„ãŸé«˜åº¦ãªäºˆæ¸¬ãŒå¯èƒ½</li>
<li>âœ… ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
</ul>

<hr>

<h2>5.1 æ™‚ç³»åˆ—ç•°å¸¸æ¤œçŸ¥</h2>

<h3>ç•°å¸¸æ¤œçŸ¥ã®æ¦‚è¦</h3>
<p><strong>æ™‚ç³»åˆ—ç•°å¸¸æ¤œçŸ¥ï¼ˆTime Series Anomaly Detectionï¼‰</strong>ã¯ã€é€šå¸¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰é€¸è„±ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’è­˜åˆ¥ã™ã‚‹æŠ€è¡“ã§ã™ã€‚</p>

<blockquote>
<p>ç•°å¸¸æ¤œçŸ¥ã¯ã€ã‚·ã‚¹ãƒ†ãƒ éšœå®³ã®æ—©æœŸç™ºè¦‹ã€ä¸æ­£æ¤œçŸ¥ã€å“è³ªç®¡ç†ãªã©ã€å¤šãã®å®Ÿç”¨çš„ãªå¿œç”¨ãŒã‚ã‚Šã¾ã™ã€‚</p>
</blockquote>

<h3>1. çµ±è¨ˆçš„æ‰‹æ³•ï¼ˆZ-scoreã€IQRï¼‰</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆæ­£å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ + ç•°å¸¸å€¤ï¼‰
np.random.seed(42)
n = 365
dates = pd.date_range('2023-01-01', periods=n, freq='D')

# æ­£å¸¸ãªãƒˆãƒ¬ãƒ³ãƒ‰ + å­£ç¯€æ€§
trend = np.linspace(100, 150, n)
seasonal = 20 * np.sin(2 * np.pi * np.arange(n) / 365)
noise = np.random.normal(0, 5, n)
normal_data = trend + seasonal + noise

# ç•°å¸¸å€¤ã‚’è¿½åŠ 
data = normal_data.copy()
anomaly_indices = [50, 120, 200, 280]
data[anomaly_indices] = data[anomaly_indices] + np.array([40, -35, 50, -40])

df = pd.DataFrame({'date': dates, 'value': data})
df.set_index('date', inplace=True)

# Z-scoreæ³•ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥
z_scores = np.abs(stats.zscore(df['value']))
threshold_z = 3
anomalies_z = z_scores > threshold_z

# IQRæ³•ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥
Q1 = df['value'].quantile(0.25)
Q3 = df['value'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
anomalies_iqr = (df['value'] < lower_bound) | (df['value'] > upper_bound)

print("=== çµ±è¨ˆçš„ç•°å¸¸æ¤œçŸ¥ ===")
print(f"Z-scoreæ³•ã§æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸: {anomalies_z.sum()}å€‹")
print(f"IQRæ³•ã§æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸: {anomalies_iqr.sum()}å€‹")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 1, figsize=(15, 10))

# Z-scoreæ³•
axes[0].plot(df.index, df['value'], label='ãƒ‡ãƒ¼ã‚¿', alpha=0.7)
axes[0].scatter(df.index[anomalies_z], df['value'][anomalies_z],
                color='red', s=100, label='ç•°å¸¸å€¤', zorder=5)
axes[0].set_ylabel('å€¤')
axes[0].set_title('Z-scoreæ³•ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥ï¼ˆé–¾å€¤=3ï¼‰', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# IQRæ³•
axes[1].plot(df.index, df['value'], label='ãƒ‡ãƒ¼ã‚¿', alpha=0.7)
axes[1].scatter(df.index[anomalies_iqr], df['value'][anomalies_iqr],
                color='red', s=100, label='ç•°å¸¸å€¤', zorder=5)
axes[1].axhline(y=lower_bound, color='r', linestyle='--',
                label=f'ä¸‹é™: {lower_bound:.1f}')
axes[1].axhline(y=upper_bound, color='r', linestyle='--',
                label=f'ä¸Šé™: {upper_bound:.1f}')
axes[1].set_xlabel('æ—¥ä»˜')
axes[1].set_ylabel('å€¤')
axes[1].set_title('IQRæ³•ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥', fontsize=14)
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3>2. Isolation Forestã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥</h3>

<pre><code class="language-python">from sklearn.ensemble import IsolationForest

# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
df['rolling_mean'] = df['value'].rolling(window=7).mean()
df['rolling_std'] = df['value'].rolling(window=7).std()
df['diff'] = df['value'].diff()

# æ¬ æå€¤ã‚’å‰Šé™¤
df_features = df[['value', 'rolling_mean', 'rolling_std', 'diff']].dropna()

# Isolation Forest
iso_forest = IsolationForest(
    contamination=0.05,  # ç•°å¸¸å€¤ã®å‰²åˆã‚’5%ã¨ä»®å®š
    random_state=42,
    n_estimators=100
)

# ç•°å¸¸ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
anomaly_labels = iso_forest.fit_predict(df_features)
anomaly_scores = iso_forest.score_samples(df_features)

# -1: ç•°å¸¸, 1: æ­£å¸¸
anomalies_iso = anomaly_labels == -1

print("\n=== Isolation Forestã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥ ===")
print(f"æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸: {anomalies_iso.sum()}å€‹")
print(f"ç•°å¸¸ç‡: {anomalies_iso.sum() / len(df_features) * 100:.2f}%")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 1, figsize=(15, 10))

# æ™‚ç³»åˆ—ã¨ç•°å¸¸å€¤
axes[0].plot(df_features.index, df_features['value'], alpha=0.7, label='ãƒ‡ãƒ¼ã‚¿')
axes[0].scatter(df_features.index[anomalies_iso],
                df_features['value'][anomalies_iso],
                color='red', s=100, label='ç•°å¸¸å€¤', zorder=5)
axes[0].set_ylabel('å€¤')
axes[0].set_title('Isolation Forestã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# ç•°å¸¸ã‚¹ã‚³ã‚¢
axes[1].plot(df_features.index, anomaly_scores, alpha=0.7)
axes[1].scatter(df_features.index[anomalies_iso],
                anomaly_scores[anomalies_iso],
                color='red', s=50, label='ç•°å¸¸å€¤')
axes[1].set_xlabel('æ—¥ä»˜')
axes[1].set_ylabel('ç•°å¸¸ã‚¹ã‚³ã‚¢')
axes[1].set_title('ç•°å¸¸ã‚¹ã‚³ã‚¢ï¼ˆä½ã„ã»ã©ç•°å¸¸ï¼‰', fontsize=14)
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3>3. LSTM Autoencoderã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.preprocessing import StandardScaler

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
scaler = StandardScaler()
data_scaled = scaler.fit_transform(df[['value']].values)

# ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ
def create_sequences(data, seq_length):
    sequences = []
    for i in range(len(data) - seq_length):
        sequences.append(data[i:i+seq_length])
    return np.array(sequences)

seq_length = 30
sequences = create_sequences(data_scaled, seq_length)

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
train_size = int(0.8 * len(sequences))
train_seq = sequences[:train_size]
test_seq = sequences[train_size:]

# LSTM Autoencoderãƒ¢ãƒ‡ãƒ«
input_dim = 1
latent_dim = 16

# ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€
encoder_inputs = keras.Input(shape=(seq_length, input_dim))
x = layers.LSTM(32, return_sequences=True)(encoder_inputs)
x = layers.LSTM(latent_dim)(x)
encoder = keras.Model(encoder_inputs, x, name='encoder')

# ãƒ‡ã‚³ãƒ¼ãƒ€
decoder_inputs = keras.Input(shape=(latent_dim,))
x = layers.RepeatVector(seq_length)(decoder_inputs)
x = layers.LSTM(latent_dim, return_sequences=True)(x)
x = layers.LSTM(32, return_sequences=True)(x)
decoder_outputs = layers.TimeDistributed(layers.Dense(input_dim))(x)
decoder = keras.Model(decoder_inputs, decoder_outputs, name='decoder')

# Autoencoder
autoencoder_inputs = keras.Input(shape=(seq_length, input_dim))
encoded = encoder(autoencoder_inputs)
decoded = decoder(encoded)
autoencoder = keras.Model(autoencoder_inputs, decoded, name='autoencoder')

autoencoder.compile(optimizer='adam', loss='mse')

# å­¦ç¿’
print("\n=== LSTM Autoencoderã®å­¦ç¿’ ===")
history = autoencoder.fit(
    train_seq, train_seq,
    epochs=50,
    batch_size=32,
    validation_split=0.1,
    verbose=0
)

# å†æ§‹æˆèª¤å·®ã®è¨ˆç®—
train_pred = autoencoder.predict(train_seq, verbose=0)
train_mse = np.mean(np.square(train_seq - train_pred), axis=(1, 2))

test_pred = autoencoder.predict(test_seq, verbose=0)
test_mse = np.mean(np.square(test_seq - test_pred), axis=(1, 2))

# ç•°å¸¸æ¤œçŸ¥ã®é–¾å€¤è¨­å®šï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®99ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼‰
threshold = np.percentile(train_mse, 99)
anomalies_ae = test_mse > threshold

print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®MSEç¯„å›²: [{train_mse.min():.4f}, {train_mse.max():.4f}]")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®MSEç¯„å›²: [{test_mse.min():.4f}, {test_mse.max():.4f}]")
print(f"ç•°å¸¸æ¤œçŸ¥é–¾å€¤: {threshold:.4f}")
print(f"æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸: {anomalies_ae.sum()}å€‹ ({anomalies_ae.sum()/len(test_mse)*100:.1f}%)")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 1, figsize=(15, 10))

# å­¦ç¿’æ›²ç·š
axes[0].plot(history.history['loss'], label='è¨“ç·´æå¤±')
axes[0].plot(history.history['val_loss'], label='æ¤œè¨¼æå¤±')
axes[0].set_xlabel('ã‚¨ãƒãƒƒã‚¯')
axes[0].set_ylabel('MSE')
axes[0].set_title('LSTM Autoencoderã®å­¦ç¿’æ›²ç·š', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# å†æ§‹æˆèª¤å·®
axes[1].plot(test_mse, alpha=0.7, label='å†æ§‹æˆèª¤å·®')
axes[1].scatter(np.where(anomalies_ae)[0], test_mse[anomalies_ae],
                color='red', s=50, label='ç•°å¸¸å€¤', zorder=5)
axes[1].axhline(y=threshold, color='r', linestyle='--',
                label=f'é–¾å€¤: {threshold:.4f}')
axes[1].set_xlabel('ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«')
axes[1].set_ylabel('MSE')
axes[1].set_title('å†æ§‹æˆèª¤å·®ã¨ç•°å¸¸æ¤œçŸ¥', fontsize=14)
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3>4. Prophetã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥</h3>

<pre><code class="language-python">from prophet import Prophet

# Prophetç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æº–å‚™
df_prophet = df[['value']].reset_index()
df_prophet.columns = ['ds', 'y']

# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
model = Prophet(
    interval_width=0.99,  # 99%ä¿¡é ¼åŒºé–“
    daily_seasonality=False,
    weekly_seasonality=True,
    yearly_seasonality=True
)
model.fit(df_prophet)

# äºˆæ¸¬ã¨ä¿¡é ¼åŒºé–“
forecast = model.predict(df_prophet)

# ç•°å¸¸æ¤œçŸ¥ï¼šå®Ÿæ¸¬å€¤ãŒä¿¡é ¼åŒºé–“å¤–
anomalies_prophet = (
    (df_prophet['y'] < forecast['yhat_lower']) |
    (df_prophet['y'] > forecast['yhat_upper'])
)

print("\n=== Prophetã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥ ===")
print(f"æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸: {anomalies_prophet.sum()}å€‹")

# å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(15, 6))

ax.plot(df_prophet['ds'], df_prophet['y'], 'k.', alpha=0.5, label='å®Ÿæ¸¬å€¤')
ax.plot(forecast['ds'], forecast['yhat'], 'b-', label='äºˆæ¸¬å€¤')
ax.fill_between(forecast['ds'],
                 forecast['yhat_lower'],
                 forecast['yhat_upper'],
                 alpha=0.3, label='99%ä¿¡é ¼åŒºé–“')
ax.scatter(df_prophet['ds'][anomalies_prophet],
           df_prophet['y'][anomalies_prophet],
           color='red', s=100, label='ç•°å¸¸å€¤', zorder=5)
ax.set_xlabel('æ—¥ä»˜')
ax.set_ylabel('å€¤')
ax.set_title('Prophetã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥', fontsize=14)
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<hr>

<h2>5.2 å¤šå¤‰é‡æ™‚ç³»åˆ—äºˆæ¸¬</h2>

<h3>å¤šå¤‰é‡æ™‚ç³»åˆ—ã¨ã¯</h3>
<p><strong>å¤šå¤‰é‡æ™‚ç³»åˆ—ï¼ˆMultivariate Time Seriesï¼‰</strong>ã¯ã€è¤‡æ•°ã®ç›¸äº’ä¾å­˜ã™ã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’åŒæ™‚ã«æ‰±ã„ã¾ã™ã€‚</p>

<h3>1. VARï¼ˆVector AutoRegressionï¼‰ãƒ¢ãƒ‡ãƒ«</h3>

<p>VARãƒ¢ãƒ‡ãƒ«ã¯ã€è¤‡æ•°ã®æ™‚ç³»åˆ—ã®ç›¸äº’é–¢ä¿‚ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¾ã™ï¼š</p>

<p>$$
\mathbf{y}_t = \mathbf{c} + \mathbf{A}_1 \mathbf{y}_{t-1} + \mathbf{A}_2 \mathbf{y}_{t-2} + \cdots + \mathbf{A}_p \mathbf{y}_{t-p} + \mathbf{\epsilon}_t
$$</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.api import VAR
from statsmodels.tsa.stattools import adfuller

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆ3å¤‰é‡æ™‚ç³»åˆ—ï¼‰
np.random.seed(42)
n = 500
dates = pd.date_range('2020-01-01', periods=n, freq='D')

# ç›¸äº’ä¾å­˜ã™ã‚‹3ã¤ã®æ™‚ç³»åˆ—
y1 = np.cumsum(np.random.normal(0, 1, n))
y2 = 0.8 * y1 + np.cumsum(np.random.normal(0, 0.5, n))
y3 = 0.5 * y1 - 0.3 * y2 + np.cumsum(np.random.normal(0, 0.3, n))

df_multi = pd.DataFrame({
    'y1': y1,
    'y2': y2,
    'y3': y3
}, index=dates)

print("=== å¤šå¤‰é‡æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ ===")
print(df_multi.head())
print(f"\nå½¢çŠ¶: {df_multi.shape}")

# å®šå¸¸æ€§ã®ç¢ºèª
print("\n=== å®šå¸¸æ€§ãƒ†ã‚¹ãƒˆï¼ˆADFæ¤œå®šï¼‰===")
for col in df_multi.columns:
    result = adfuller(df_multi[col])
    print(f"{col}: på€¤={result[1]:.4f} {'ï¼ˆå®šå¸¸ï¼‰' if result[1] < 0.05 else 'ï¼ˆéå®šå¸¸ï¼‰'}")

# å·®åˆ†ã‚’å–ã£ã¦å®šå¸¸åŒ–
df_diff = df_multi.diff().dropna()

print("\n=== å·®åˆ†å¾Œã®å®šå¸¸æ€§ãƒ†ã‚¹ãƒˆ ===")
for col in df_diff.columns:
    result = adfuller(df_diff[col])
    print(f"{col}: på€¤={result[1]:.4f} {'ï¼ˆå®šå¸¸ï¼‰' if result[1] < 0.05 else 'ï¼ˆéå®šå¸¸ï¼‰'}")

# VARãƒ¢ãƒ‡ãƒ«ã®æ¬¡æ•°é¸æŠ
model = VAR(df_diff)
lag_order = model.select_order(maxlags=10)
print("\n=== VARãƒ¢ãƒ‡ãƒ«ã®æ¬¡æ•°é¸æŠ ===")
print(lag_order.summary())

# æœ€é©ãªæ¬¡æ•°ã§VARãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
optimal_lag = lag_order.aic
var_model = model.fit(optimal_lag)
print(f"\n=== VARãƒ¢ãƒ‡ãƒ«ï¼ˆæ¬¡æ•°={optimal_lag}ï¼‰===")
print(var_model.summary())

# äºˆæ¸¬
forecast_steps = 30
forecast = var_model.forecast(df_diff.values[-optimal_lag:], steps=forecast_steps)
forecast_dates = pd.date_range(df_multi.index[-1] + pd.Timedelta(days=1),
                                periods=forecast_steps, freq='D')
df_forecast = pd.DataFrame(forecast, index=forecast_dates, columns=df_multi.columns)

# å¯è¦–åŒ–
fig, axes = plt.subplots(3, 1, figsize=(15, 12))

for i, col in enumerate(df_multi.columns):
    # å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆå·®åˆ†ï¼‰
    axes[i].plot(df_diff.index, df_diff[col], label='å®Ÿæ¸¬å€¤ï¼ˆå·®åˆ†ï¼‰', alpha=0.7)
    # äºˆæ¸¬å€¤
    axes[i].plot(df_forecast.index, df_forecast[col],
                 color='red', label='äºˆæ¸¬å€¤', linewidth=2)
    axes[i].set_ylabel(col)
    axes[i].set_title(f'{col}ã®äºˆæ¸¬', fontsize=12)
    axes[i].legend()
    axes[i].grid(True, alpha=0.3)

axes[-1].set_xlabel('æ—¥ä»˜')
plt.tight_layout()
plt.show()
</code></pre>

<h3>2. Grangerå› æœæ€§ãƒ†ã‚¹ãƒˆ</h3>

<p><strong>Grangerå› æœæ€§ï¼ˆGranger Causalityï¼‰</strong>ã¯ã€ã‚ã‚‹æ™‚ç³»åˆ—ãŒåˆ¥ã®æ™‚ç³»åˆ—ã®äºˆæ¸¬ã«æœ‰ç”¨ã‹ã‚’æ¤œå®šã—ã¾ã™ã€‚</p>

<pre><code class="language-python">from statsmodels.tsa.stattools import grangercausalitytests

print("\n=== Grangerå› æœæ€§ãƒ†ã‚¹ãƒˆ ===")

# å„ãƒšã‚¢ã§Grangerå› æœæ€§ã‚’ãƒ†ã‚¹ãƒˆ
max_lag = 5
variables = df_diff.columns.tolist()

for i, var1 in enumerate(variables):
    for var2 in variables:
        if var1 != var2:
            print(f"\n{var1} â†’ {var2} ã®å› æœæ€§:")
            test_data = df_diff[[var2, var1]]
            try:
                result = grangercausalitytests(test_data, maxlag=max_lag, verbose=False)

                # på€¤ã‚’æŠ½å‡º
                p_values = [result[lag][0]['ssr_ftest'][1] for lag in range(1, max_lag+1)]
                min_p = min(p_values)

                if min_p < 0.05:
                    print(f"  âœ“ å› æœæ€§ã‚ã‚Šï¼ˆpå€¤={min_p:.4f}ï¼‰")
                else:
                    print(f"  âœ— å› æœæ€§ãªã—ï¼ˆpå€¤={min_p:.4f}ï¼‰")
            except:
                print("  ãƒ†ã‚¹ãƒˆå¤±æ•—")
</code></pre>

<h3>3. å¤šå‡ºåŠ›ãƒ¢ãƒ‡ãƒ«ï¼ˆMulti-outputï¼‰</h3>

<pre><code class="language-python">from sklearn.ensemble import RandomForestRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# ç‰¹å¾´é‡ä½œæˆï¼ˆãƒ©ã‚°ç‰¹å¾´é‡ï¼‰
def create_lagged_features(df, lags):
    df_lagged = df.copy()
    for col in df.columns:
        for lag in range(1, lags + 1):
            df_lagged[f'{col}_lag{lag}'] = df[col].shift(lag)
    return df_lagged.dropna()

# ãƒ©ã‚°ç‰¹å¾´é‡ã®ä½œæˆ
lags = 5
df_lagged = create_lagged_features(df_multi, lags)

# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
target_cols = ['y1', 'y2', 'y3']
feature_cols = [col for col in df_lagged.columns if col not in target_cols]

X = df_lagged[feature_cols]
y = df_lagged[target_cols]

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
train_size = int(0.8 * len(X))
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Multi-output Random Forest
multi_rf = MultiOutputRegressor(
    RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
)

print("\n=== Multi-output Random Forestã®å­¦ç¿’ ===")
multi_rf.fit(X_train, y_train)

# äºˆæ¸¬
y_pred = multi_rf.predict(X_test)

# è©•ä¾¡
print("\n=== äºˆæ¸¬æ€§èƒ½ ===")
for i, col in enumerate(target_cols):
    mse = mean_squared_error(y_test[col], y_pred[:, i])
    rmse = np.sqrt(mse)
    print(f"{col}: RMSE={rmse:.4f}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(3, 1, figsize=(15, 12))

for i, col in enumerate(target_cols):
    axes[i].plot(y_test.index, y_test[col], label='å®Ÿæ¸¬å€¤', alpha=0.7)
    axes[i].plot(y_test.index, y_pred[:, i],
                 label='äºˆæ¸¬å€¤', alpha=0.7, linewidth=2)
    axes[i].set_ylabel(col)
    axes[i].set_title(f'{col}ã®äºˆæ¸¬ï¼ˆMulti-output RFï¼‰', fontsize=12)
    axes[i].legend()
    axes[i].grid(True, alpha=0.3)

axes[-1].set_xlabel('æ—¥ä»˜')
plt.tight_layout()
plt.show()
</code></pre>

<hr>

<h2>5.3 å› æœæ¨è«–</h2>

<h3>å› æœæ¨è«–ã®åŸºç¤</h3>
<p><strong>å› æœæ¨è«–ï¼ˆCausal Inferenceï¼‰</strong>ã¯ã€ä»‹å…¥ã‚„æ–½ç­–ã®åŠ¹æœã‚’æ¸¬å®šã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<h3>1. ä»‹å…¥åˆ†æï¼ˆIntervention Analysisï¼‰</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆä»‹å…¥ã‚ã‚Šï¼‰
np.random.seed(42)
n_pre = 200
n_post = 100
n_total = n_pre + n_post

dates = pd.date_range('2020-01-01', periods=n_total, freq='D')

# ä»‹å…¥å‰ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
baseline = 100 + np.cumsum(np.random.normal(0.1, 1, n_total))

# ä»‹å…¥åŠ¹æœï¼ˆ200æ—¥ç›®ã‹ã‚‰+20ã®åŠ¹æœï¼‰
intervention_effect = np.concatenate([
    np.zeros(n_pre),
    np.linspace(0, 20, 50),  # å¾ã€…ã«åŠ¹æœãŒç¾ã‚Œã‚‹
    20 * np.ones(n_post - 50)
])

# è¦³æ¸¬å€¤
observed = baseline + intervention_effect + np.random.normal(0, 2, n_total)

df_intervention = pd.DataFrame({
    'date': dates,
    'value': observed,
    'intervention': np.concatenate([np.zeros(n_pre), np.ones(n_post)])
}, index=dates)

# ä»‹å…¥å‰ãƒ‡ãƒ¼ã‚¿ã§ARIMAãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
pre_intervention = df_intervention[:n_pre]['value']
model = ARIMA(pre_intervention, order=(2, 1, 2))
fitted_model = model.fit()

# ä»‹å…¥å¾Œã®äºˆæ¸¬ï¼ˆåå®Ÿä»®æƒ³ï¼šä»‹å…¥ãŒãªã‹ã£ãŸå ´åˆã®äºˆæ¸¬ï¼‰
forecast = fitted_model.forecast(steps=n_post)
forecast_index = df_intervention.index[n_pre:]

# å› æœåŠ¹æœã®æ¨å®š
actual_post = df_intervention[n_pre:]['value']
causal_effect = actual_post.values - forecast.values

print("=== ä»‹å…¥åˆ†æ ===")
print(f"ä»‹å…¥å‰å¹³å‡: {pre_intervention.mean():.2f}")
print(f"ä»‹å…¥å¾Œå¹³å‡ï¼ˆå®Ÿæ¸¬ï¼‰: {actual_post.mean():.2f}")
print(f"ä»‹å…¥å¾Œå¹³å‡ï¼ˆäºˆæ¸¬ï¼‰: {forecast.mean():.2f}")
print(f"å¹³å‡å› æœåŠ¹æœ: {causal_effect.mean():.2f}")
print(f"ç´¯ç©å› æœåŠ¹æœ: {causal_effect.sum():.2f}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 1, figsize=(15, 10))

# æ™‚ç³»åˆ—ã¨äºˆæ¸¬
axes[0].plot(df_intervention.index[:n_pre],
             df_intervention['value'][:n_pre],
             label='ä»‹å…¥å‰ï¼ˆå®Ÿæ¸¬ï¼‰', color='blue', alpha=0.7)
axes[0].plot(df_intervention.index[n_pre:],
             df_intervention['value'][n_pre:],
             label='ä»‹å…¥å¾Œï¼ˆå®Ÿæ¸¬ï¼‰', color='green', alpha=0.7)
axes[0].plot(forecast_index, forecast,
             label='åå®Ÿä»®æƒ³ï¼ˆä»‹å…¥ãªã—äºˆæ¸¬ï¼‰', color='red',
             linestyle='--', linewidth=2)
axes[0].axvline(x=dates[n_pre], color='black', linestyle=':',
                label='ä»‹å…¥æ™‚ç‚¹', linewidth=2)
axes[0].set_ylabel('å€¤')
axes[0].set_title('ä»‹å…¥åˆ†æï¼šå®Ÿæ¸¬å€¤ vs åå®Ÿä»®æƒ³', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# å› æœåŠ¹æœ
axes[1].plot(forecast_index, causal_effect, color='purple', linewidth=2)
axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)
axes[1].fill_between(forecast_index, 0, causal_effect,
                      alpha=0.3, color='purple')
axes[1].set_xlabel('æ—¥ä»˜')
axes[1].set_ylabel('å› æœåŠ¹æœ')
axes[1].set_title(f'æ¨å®šå› æœåŠ¹æœï¼ˆå¹³å‡={causal_effect.mean():.2f}ï¼‰', fontsize=14)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3>2. CausalImpactåˆ†æï¼ˆpycausalimpactï¼‰</h3>

<pre><code class="language-python">from causalimpact import CausalImpact

# ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆåˆ¶å¾¡å¤‰æ•°ã‚’å«ã‚€ï¼‰
np.random.seed(42)
n = 300
dates = pd.date_range('2020-01-01', periods=n, freq='D')

# åˆ¶å¾¡å¤‰æ•°ï¼ˆä»‹å…¥ã®å½±éŸ¿ã‚’å—ã‘ãªã„é–¢é€£å¤‰æ•°ï¼‰
control1 = np.cumsum(np.random.normal(0, 1, n))
control2 = np.cumsum(np.random.normal(0, 0.8, n))

# ç›®æ¨™å¤‰æ•°ï¼ˆåˆ¶å¾¡å¤‰æ•°ã¨ç›¸é–¢ã€ä»‹å…¥å¾Œã«åŠ¹æœï¼‰
intervention_point = 200
baseline_correlation = 0.7 * control1 + 0.5 * control2
intervention_effect = np.concatenate([
    np.zeros(intervention_point),
    15 * np.ones(n - intervention_point)
])
target = baseline_correlation + intervention_effect + np.random.normal(0, 3, n)

df_causal = pd.DataFrame({
    'target': target,
    'control1': control1,
    'control2': control2
}, index=dates)

# CausalImpactåˆ†æ
pre_period = [0, intervention_point - 1]
post_period = [intervention_point, n - 1]

ci = CausalImpact(df_causal, pre_period, post_period)

print("\n=== CausalImpactåˆ†æ ===")
print(ci.summary())
print("\n=== è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ ===")
print(ci.summary(output='report'))

# å¯è¦–åŒ–
ci.plot()
plt.tight_layout()
plt.show()
</code></pre>

<h3>3. Difference-in-Differencesï¼ˆå·®åˆ†ã®å·®åˆ†æ³•ï¼‰</h3>

<pre><code class="language-python"># Difference-in-Differencesï¼ˆDIDï¼‰ã‚µãƒ³ãƒ—ãƒ«
np.random.seed(42)
n_time = 100
intervention_time = 50

# å‡¦ç½®ç¾¤ï¼ˆtreatment groupï¼‰
treatment_pre = 50 + np.cumsum(np.random.normal(0.1, 1, intervention_time))
treatment_post = 50 + np.cumsum(np.random.normal(0.1, 1, intervention_time)) + 20

# å¯¾ç…§ç¾¤ï¼ˆcontrol groupï¼‰: ä»‹å…¥åŠ¹æœãªã—
control_pre = 45 + np.cumsum(np.random.normal(0.1, 1, intervention_time))
control_post = 45 + np.cumsum(np.random.normal(0.1, 1, intervention_time))

# DIDæ¨å®š
treatment_diff = treatment_post.mean() - treatment_pre.mean()
control_diff = control_post.mean() - control_pre.mean()
did_estimate = treatment_diff - control_diff

print("\n=== Difference-in-Differencesåˆ†æ ===")
print(f"å‡¦ç½®ç¾¤ã®å¤‰åŒ–: {treatment_diff:.2f}")
print(f"å¯¾ç…§ç¾¤ã®å¤‰åŒ–: {control_diff:.2f}")
print(f"DIDæ¨å®šå€¤ï¼ˆä»‹å…¥åŠ¹æœï¼‰: {did_estimate:.2f}")

# å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(12, 6))

time_points = np.arange(n_time)
treatment_values = np.concatenate([treatment_pre, treatment_post])
control_values = np.concatenate([control_pre, control_post])

ax.plot(time_points[:intervention_time], treatment_pre,
        'b-', label='å‡¦ç½®ç¾¤ï¼ˆä»‹å…¥å‰ï¼‰', linewidth=2)
ax.plot(time_points[intervention_time:], treatment_post,
        'b--', label='å‡¦ç½®ç¾¤ï¼ˆä»‹å…¥å¾Œï¼‰', linewidth=2)
ax.plot(time_points[:intervention_time], control_pre,
        'r-', label='å¯¾ç…§ç¾¤ï¼ˆä»‹å…¥å‰ï¼‰', linewidth=2)
ax.plot(time_points[intervention_time:], control_post,
        'r--', label='å¯¾ç…§ç¾¤ï¼ˆä»‹å…¥å¾Œï¼‰', linewidth=2)
ax.axvline(x=intervention_time, color='black', linestyle=':',
           label='ä»‹å…¥æ™‚ç‚¹', linewidth=2)
ax.set_xlabel('æ™‚é–“')
ax.set_ylabel('å€¤')
ax.set_title(f'Difference-in-Differencesï¼ˆDIDæ¨å®šå€¤={did_estimate:.2f}ï¼‰', fontsize=14)
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<hr>

<h2>5.4 Prophet: Facebookæ™‚ç³»åˆ—äºˆæ¸¬</h2>

<h3>Prophetã®ç‰¹å¾´</h3>
<p><strong>Prophet</strong>ã¯ã€FacebookãŒé–‹ç™ºã—ãŸæ™‚ç³»åˆ—äºˆæ¸¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã€ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ï¼š</p>

<ul>
<li>ãƒˆãƒ¬ãƒ³ãƒ‰ã€å­£ç¯€æ€§ã€ä¼‘æ—¥åŠ¹æœã‚’è‡ªå‹•çš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–</li>
<li>æ¬ æå€¤ã‚„ãƒˆãƒ¬ãƒ³ãƒ‰ã®å¤‰åŒ–ã«é ‘å¥</li>
<li>ç›´æ„Ÿçš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´</li>
</ul>

<h3>Prophetã®åŠ æ³•ãƒ¢ãƒ‡ãƒ«</h3>

<p>$$
y(t) = g(t) + s(t) + h(t) + \epsilon_t
$$</p>

<ul>
<li>$g(t)$: ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆæˆé•·é–¢æ•°ï¼‰</li>
<li>$s(t)$: å­£ç¯€æ€§ï¼ˆå‘¨æœŸçš„å¤‰å‹•ï¼‰</li>
<li>$h(t)$: ä¼‘æ—¥åŠ¹æœ</li>
<li>$\epsilon_t$: èª¤å·®é …</li>
</ul>

<h3>1. åŸºæœ¬çš„ãªProphetäºˆæ¸¬</h3>

<pre><code class="language-python">from prophet import Prophet
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
n = 730  # 2å¹´åˆ†
dates = pd.date_range('2021-01-01', periods=n, freq='D')

# ãƒˆãƒ¬ãƒ³ãƒ‰ + å¹´æ¬¡å­£ç¯€æ€§ + é€±æ¬¡å­£ç¯€æ€§
trend = np.linspace(100, 200, n)
yearly_seasonality = 30 * np.sin(2 * np.pi * np.arange(n) / 365.25)
weekly_seasonality = 10 * np.sin(2 * np.pi * np.arange(n) / 7)
noise = np.random.normal(0, 5, n)

y = trend + yearly_seasonality + weekly_seasonality + noise

df_prophet = pd.DataFrame({'ds': dates, 'y': y})

# Prophet ãƒ¢ãƒ‡ãƒ«
model = Prophet(
    yearly_seasonality=True,
    weekly_seasonality=True,
    daily_seasonality=False,
    changepoint_prior_scale=0.05  # ãƒˆãƒ¬ãƒ³ãƒ‰å¤‰åŒ–ã®æŸ”è»Ÿæ€§
)

print("=== Prophetãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ ===")
model.fit(df_prophet)

# å°†æ¥äºˆæ¸¬ï¼ˆ90æ—¥å…ˆã¾ã§ï¼‰
future = model.make_future_dataframe(periods=90)
forecast = model.predict(future)

print("\n=== äºˆæ¸¬çµæœï¼ˆæœ€å¾Œã®5è¡Œï¼‰===")
print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail())

# å¯è¦–åŒ–
fig1 = model.plot(forecast)
plt.title('Prophetäºˆæ¸¬çµæœ', fontsize=14)
plt.tight_layout()
plt.show()

# ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å¯è¦–åŒ–
fig2 = model.plot_components(forecast)
plt.tight_layout()
plt.show()
</code></pre>

<h3>2. ä¼‘æ—¥åŠ¹æœã®è¿½åŠ </h3>

<pre><code class="language-python"># ä¼‘æ—¥ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä½œæˆ
holidays = pd.DataFrame({
    'holiday': 'special_sale',
    'ds': pd.to_datetime(['2021-11-26', '2021-12-24', '2022-11-25', '2022-12-24']),
    'lower_window': -1,  # ä¼‘æ—¥ã®å‰æ—¥ã‹ã‚‰
    'upper_window': 1,   # ä¼‘æ—¥ã®ç¿Œæ—¥ã¾ã§
})

# ä¼‘æ—¥åŠ¹æœã‚’å«ã‚€ãƒ¢ãƒ‡ãƒ«
model_holidays = Prophet(
    holidays=holidays,
    yearly_seasonality=True,
    weekly_seasonality=True
)

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã«ä¼‘æ—¥åŠ¹æœã‚’è¿½åŠ 
y_with_holidays = y.copy()
holiday_dates = holidays['ds'].values
for date in holiday_dates:
    idx = np.where(dates == date)[0]
    if len(idx) > 0:
        y_with_holidays[idx[0]] += 50  # ä¼‘æ—¥ã«+50ã®åŠ¹æœ

df_prophet_holidays = pd.DataFrame({'ds': dates, 'y': y_with_holidays})

print("\n=== ä¼‘æ—¥åŠ¹æœã‚’å«ã‚€Prophetãƒ¢ãƒ‡ãƒ« ===")
model_holidays.fit(df_prophet_holidays)

# äºˆæ¸¬
future_holidays = model_holidays.make_future_dataframe(periods=90)
forecast_holidays = model_holidays.predict(future_holidays)

# å¯è¦–åŒ–
fig = model_holidays.plot(forecast_holidays)
plt.title('ä¼‘æ—¥åŠ¹æœã‚’å«ã‚€Prophetäºˆæ¸¬', fontsize=14)
plt.tight_layout()
plt.show()

# ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆä¼‘æ—¥åŠ¹æœã‚‚è¡¨ç¤ºï¼‰
fig_comp = model_holidays.plot_components(forecast_holidays)
plt.tight_layout()
plt.show()
</code></pre>

<h3>3. å¤‰åŒ–ç‚¹æ¤œå‡ºï¼ˆChangepoint Detectionï¼‰</h3>

<pre><code class="language-python"># ãƒˆãƒ¬ãƒ³ãƒ‰å¤‰åŒ–ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
n = 500
dates = pd.date_range('2020-01-01', periods=n, freq='D')

# ãƒˆãƒ¬ãƒ³ãƒ‰å¤‰åŒ–ï¼ˆ250æ—¥ç›®ã§å‚¾ããŒå¤‰ã‚ã‚‹ï¼‰
trend1 = np.linspace(100, 150, 250)
trend2 = np.linspace(150, 120, 250)
trend = np.concatenate([trend1, trend2])

y = trend + 20 * np.sin(2 * np.pi * np.arange(n) / 365.25) + np.random.normal(0, 5, n)

df_changepoint = pd.DataFrame({'ds': dates, 'y': y})

# å¤‰åŒ–ç‚¹æ¤œå‡ºã‚’æœ‰åŠ¹ã«ã—ãŸãƒ¢ãƒ‡ãƒ«
model_cp = Prophet(
    changepoint_prior_scale=0.5,  # å¤§ãã„ã»ã©æŸ”è»Ÿã«å¤‰åŒ–ç‚¹ã‚’æ¤œå‡º
    n_changepoints=25  # å€™è£œå¤‰åŒ–ç‚¹ã®æ•°
)

model_cp.fit(df_changepoint)

# å¤‰åŒ–ç‚¹ã®å–å¾—
changepoints = model_cp.changepoints
changepoint_dates = pd.to_datetime(changepoints)

print("\n=== æ¤œå‡ºã•ã‚ŒãŸå¤‰åŒ–ç‚¹ ===")
print(f"å¤‰åŒ–ç‚¹ã®æ•°: {len(changepoint_dates)}")
print(f"ä¸»è¦ãªå¤‰åŒ–ç‚¹:")
# å¤‰åŒ–ã®å¤§ãã•ã§ã‚½ãƒ¼ãƒˆ
deltas = model_cp.params['delta'].mean(axis=0)
sorted_indices = np.argsort(np.abs(deltas))[-5:]  # ä¸Šä½5ã¤
for idx in sorted_indices:
    if idx < len(changepoint_dates):
        print(f"  {changepoint_dates[idx].date()}: å¤‰åŒ–é‡={deltas[idx]:.3f}")

# äºˆæ¸¬
future = model_cp.make_future_dataframe(periods=60)
forecast = model_cp.predict(future)

# å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(15, 6))
model_cp.plot(forecast, ax=ax)

# å¤‰åŒ–ç‚¹ã‚’ãƒãƒ¼ã‚¯
for cp in changepoint_dates:
    ax.axvline(x=cp, color='red', linestyle='--', alpha=0.3)

ax.set_title('å¤‰åŒ–ç‚¹æ¤œå‡ºã‚’å«ã‚€Prophetäºˆæ¸¬', fontsize=14)
plt.tight_layout()
plt.show()
</code></pre>

<hr>

<h2>5.5 ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ </h2>

<h3>äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®å…¨ä½“åƒ</h3>

<div class="mermaid">
graph LR
    A[ãƒ‡ãƒ¼ã‚¿å–å¾—] --> B[å‰å‡¦ç†]
    B --> C[ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°]
    C --> D[ãƒ¢ãƒ‡ãƒ«é¸æŠ]
    D --> E[å­¦ç¿’ãƒ»è©•ä¾¡]
    E --> F[äºˆæ¸¬]
    F --> G[ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°]
    G --> H{å†å­¦ç¿’ãŒå¿…è¦?}
    H -->|Yes| B
    H -->|No| F

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
    style F fill:#c8e6c9
    style G fill:#ffe0b2
    style H fill:#ffccbc
</div>

<h3>å®Œå…¨ãªäºˆæ¸¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge
from prophet import Prophet
import warnings
warnings.filterwarnings('ignore')

class TimeSeriesPipeline:
    """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æ™‚ç³»åˆ—äºˆæ¸¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(self):
        self.models = {}
        self.best_model = None
        self.best_model_name = None
        self.best_score = float('inf')
        self.scaler = None

    def load_data(self, filepath=None, df=None):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        if df is not None:
            self.data = df
        else:
            self.data = pd.read_csv(filepath, parse_dates=['date'], index_col='date')
        print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {self.data.shape}")
        return self

    def preprocess(self, target_col='value'):
        """å‰å‡¦ç†"""
        self.target_col = target_col

        # æ¬ æå€¤å‡¦ç†
        self.data = self.data.interpolate(method='linear')

        # å¤–ã‚Œå€¤å‡¦ç†ï¼ˆIQRæ³•ï¼‰
        Q1 = self.data[target_col].quantile(0.25)
        Q3 = self.data[target_col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 3 * IQR
        upper = Q3 + 3 * IQR
        self.data[target_col] = self.data[target_col].clip(lower, upper)

        print("å‰å‡¦ç†å®Œäº†")
        return self

    def create_features(self, lags=[1, 2, 3, 7, 14, 30]):
        """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
        df = self.data.copy()

        # ãƒ©ã‚°ç‰¹å¾´é‡
        for lag in lags:
            df[f'lag_{lag}'] = df[self.target_col].shift(lag)

        # ç§»å‹•å¹³å‡
        for window in [7, 14, 30]:
            df[f'ma_{window}'] = df[self.target_col].rolling(window=window).mean()
            df[f'std_{window}'] = df[self.target_col].rolling(window=window).std()

        # æ™‚é–“ç‰¹å¾´é‡
        df['day_of_week'] = df.index.dayofweek
        df['day_of_month'] = df.index.day
        df['month'] = df.index.month
        df['quarter'] = df.index.quarter

        # å·®åˆ†
        df['diff_1'] = df[self.target_col].diff(1)
        df['diff_7'] = df[self.target_col].diff(7)

        # æ¬ æå€¤ã‚’å‰Šé™¤
        df = df.dropna()

        self.feature_data = df
        print(f"ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†: {df.shape[1]}å€‹ã®ç‰¹å¾´é‡")
        return self

    def prepare_train_test(self, test_size=0.2):
        """è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²"""
        split_idx = int(len(self.feature_data) * (1 - test_size))

        self.train = self.feature_data[:split_idx]
        self.test = self.feature_data[split_idx:]

        # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
        feature_cols = [col for col in self.feature_data.columns
                       if col != self.target_col]

        self.X_train = self.train[feature_cols]
        self.y_train = self.train[self.target_col]
        self.X_test = self.test[feature_cols]
        self.y_test = self.test[self.target_col]

        print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(self.train)}, ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(self.test)}")
        return self

    def add_model(self, name, model):
        """ãƒ¢ãƒ‡ãƒ«è¿½åŠ """
        self.models[name] = model
        return self

    def train_and_evaluate(self):
        """å…¨ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨è©•ä¾¡"""
        results = {}

        for name, model in self.models.items():
            print(f"\n=== {name}ã®å­¦ç¿’ ===")

            # å­¦ç¿’
            model.fit(self.X_train, self.y_train)

            # äºˆæ¸¬
            y_pred_train = model.predict(self.X_train)
            y_pred_test = model.predict(self.X_test)

            # è©•ä¾¡
            train_rmse = np.sqrt(mean_squared_error(self.y_train, y_pred_train))
            test_rmse = np.sqrt(mean_squared_error(self.y_test, y_pred_test))
            test_mae = mean_absolute_error(self.y_test, y_pred_test)

            results[name] = {
                'train_rmse': train_rmse,
                'test_rmse': test_rmse,
                'test_mae': test_mae,
                'predictions': y_pred_test
            }

            print(f"è¨“ç·´RMSE: {train_rmse:.4f}")
            print(f"ãƒ†ã‚¹ãƒˆRMSE: {test_rmse:.4f}")
            print(f"ãƒ†ã‚¹ãƒˆMAE: {test_mae:.4f}")

            # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°
            if test_rmse < self.best_score:
                self.best_score = test_rmse
                self.best_model = model
                self.best_model_name = name

        self.results = results
        print(f"\næœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {self.best_model_name} (RMSE={self.best_score:.4f})")
        return self

    def cross_validate(self, n_splits=5):
        """æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼"""
        tscv = TimeSeriesSplit(n_splits=n_splits)
        cv_results = {}

        for name, model in self.models.items():
            scores = []

            for train_idx, val_idx in tscv.split(self.X_train):
                X_cv_train = self.X_train.iloc[train_idx]
                y_cv_train = self.y_train.iloc[train_idx]
                X_cv_val = self.X_train.iloc[val_idx]
                y_cv_val = self.y_train.iloc[val_idx]

                model.fit(X_cv_train, y_cv_train)
                y_pred = model.predict(X_cv_val)
                rmse = np.sqrt(mean_squared_error(y_cv_val, y_pred))
                scores.append(rmse)

            cv_results[name] = {
                'mean_rmse': np.mean(scores),
                'std_rmse': np.std(scores)
            }

        print("\n=== äº¤å·®æ¤œè¨¼çµæœ ===")
        for name, result in cv_results.items():
            print(f"{name}: RMSE={result['mean_rmse']:.4f} (Â±{result['std_rmse']:.4f})")

        return cv_results

    def plot_results(self):
        """çµæœã®å¯è¦–åŒ–"""
        n_models = len(self.results)
        fig, axes = plt.subplots(n_models + 1, 1, figsize=(15, 4 * (n_models + 1)))

        # å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬
        for i, (name, result) in enumerate(self.results.items()):
            ax = axes[i]
            ax.plot(self.test.index, self.y_test, label='å®Ÿæ¸¬å€¤', alpha=0.7)
            ax.plot(self.test.index, result['predictions'],
                   label=f'äºˆæ¸¬å€¤ï¼ˆ{name}ï¼‰', alpha=0.7)
            ax.set_ylabel('å€¤')
            ax.set_title(f'{name}: RMSE={result["test_rmse"]:.4f}', fontsize=12)
            ax.legend()
            ax.grid(True, alpha=0.3)

        # å…¨ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
        ax = axes[-1]
        ax.plot(self.test.index, self.y_test, label='å®Ÿæ¸¬å€¤',
               linewidth=2, alpha=0.8, color='black')
        for name, result in self.results.items():
            ax.plot(self.test.index, result['predictions'],
                   label=name, alpha=0.6)
        ax.set_xlabel('æ—¥ä»˜')
        ax.set_ylabel('å€¤')
        ax.set_title('å…¨ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ', fontsize=12)
        ax.legend()
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    def predict_future(self, steps=30):
        """å°†æ¥äºˆæ¸¬"""
        # æœ€å¾Œã®ç‰¹å¾´é‡ã‚’ä½¿ç”¨ï¼ˆç°¡ç•¥åŒ–ï¼‰
        last_features = self.X_test.iloc[-1:].copy()

        predictions = []
        for _ in range(steps):
            pred = self.best_model.predict(last_features)[0]
            predictions.append(pred)

            # ç‰¹å¾´é‡ã®æ›´æ–°ï¼ˆç°¡ç•¥åŒ–: ãƒ©ã‚°ç‰¹å¾´é‡ã®ã¿æ›´æ–°ï¼‰
            # å®Ÿéš›ã«ã¯ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸæ›´æ–°ãŒå¿…è¦

        future_dates = pd.date_range(
            self.test.index[-1] + pd.Timedelta(days=1),
            periods=steps,
            freq='D'
        )

        return pd.DataFrame({'date': future_dates, 'prediction': predictions})

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œä¾‹
print("=== ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰äºˆæ¸¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ===")

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
n = 500
dates = pd.date_range('2021-01-01', periods=n, freq='D')
trend = np.linspace(100, 200, n)
seasonal = 30 * np.sin(2 * np.pi * np.arange(n) / 365.25)
noise = np.random.normal(0, 5, n)
value = trend + seasonal + noise

df_sample = pd.DataFrame({'value': value}, index=dates)

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
pipeline = TimeSeriesPipeline()
pipeline.load_data(df=df_sample)
pipeline.preprocess(target_col='value')
pipeline.create_features(lags=[1, 2, 3, 7, 14])
pipeline.prepare_train_test(test_size=0.2)

# ãƒ¢ãƒ‡ãƒ«è¿½åŠ 
pipeline.add_model('RandomForest', RandomForestRegressor(n_estimators=100, random_state=42))
pipeline.add_model('GradientBoosting', GradientBoostingRegressor(n_estimators=100, random_state=42))
pipeline.add_model('Ridge', Ridge(alpha=1.0))

# å­¦ç¿’ã¨è©•ä¾¡
pipeline.train_and_evaluate()

# äº¤å·®æ¤œè¨¼
pipeline.cross_validate(n_splits=5)

# çµæœå¯è¦–åŒ–
pipeline.plot_results()

# å°†æ¥äºˆæ¸¬
future_forecast = pipeline.predict_future(steps=30)
print("\n=== å°†æ¥äºˆæ¸¬ï¼ˆ30æ—¥å…ˆï¼‰===")
print(future_forecast.head(10))
</code></pre>

<hr>

<h2>5.6 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>æ™‚ç³»åˆ—ç•°å¸¸æ¤œçŸ¥</strong></p>
<ul>
<li>çµ±è¨ˆçš„æ‰‹æ³•ï¼ˆZ-scoreã€IQRï¼‰</li>
<li>æ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•ï¼ˆIsolation Forestï¼‰</li>
<li>æ·±å±¤å­¦ç¿’ï¼ˆLSTM Autoencoderï¼‰</li>
<li>Prophet ã«ã‚ˆã‚‹ä¿¡é ¼åŒºé–“ãƒ™ãƒ¼ã‚¹æ¤œçŸ¥</li>
</ul></li>

<li><p><strong>å¤šå¤‰é‡æ™‚ç³»åˆ—äºˆæ¸¬</strong></p>
<ul>
<li>VAR ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ç›¸äº’ä¾å­˜ã®ãƒ¢ãƒ‡ãƒ«åŒ–</li>
<li>Granger å› æœæ€§ãƒ†ã‚¹ãƒˆ</li>
<li>Multi-output æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«</li>
</ul></li>

<li><p><strong>å› æœæ¨è«–</strong></p>
<ul>
<li>ä»‹å…¥åˆ†æã¨åå®Ÿä»®æƒ³äºˆæ¸¬</li>
<li>CausalImpact ã«ã‚ˆã‚‹åŠ¹æœæ¸¬å®š</li>
<li>Difference-in-Differences æ³•</li>
</ul></li>

<li><p><strong>Prophet</strong></p>
<ul>
<li>ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»å­£ç¯€æ€§ãƒ»ä¼‘æ—¥åŠ¹æœã®è‡ªå‹•ãƒ¢ãƒ‡ãƒ«åŒ–</li>
<li>å¤‰åŒ–ç‚¹æ¤œå‡º</li>
<li>ç›´æ„Ÿçš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´</li>
</ul></li>

<li><p><strong>ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã‚·ã‚¹ãƒ†ãƒ </strong></p>
<ul>
<li>å®Œå…¨ãªäºˆæ¸¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰</li>
<li>ãƒ¢ãƒ‡ãƒ«é¸æŠã®è‡ªå‹•åŒ–</li>
<li>äº¤å·®æ¤œè¨¼ã¨æ€§èƒ½è©•ä¾¡</li>
<li>æœ¬ç•ªé‹ç”¨ã‚’è¦‹æ®ãˆãŸè¨­è¨ˆ</li>
</ul></li>
</ol>

<h3>å®Ÿå‹™ã¸ã®å¿œç”¨</h3>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>é©ç”¨ä¾‹</th>
<th>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ç•°å¸¸æ¤œçŸ¥</strong></td>
<td>ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ã€ä¸æ­£æ¤œçŸ¥</td>
<td>è¤‡æ•°æ‰‹æ³•ã®çµ„ã¿åˆã‚ã›</td>
</tr>
<tr>
<td><strong>å¤šå¤‰é‡äºˆæ¸¬</strong></td>
<td>éœ€è¦äºˆæ¸¬ã€åœ¨åº«ç®¡ç†</td>
<td>å¤‰æ•°é–“ã®å› æœé–¢ä¿‚ã®ç†è§£</td>
</tr>
<tr>
<td><strong>å› æœæ¨è«–</strong></td>
<td>A/Bãƒ†ã‚¹ãƒˆã€æ–½ç­–è©•ä¾¡</td>
<td>åå®Ÿä»®æƒ³ã®é©åˆ‡ãªè¨­å®š</td>
</tr>
<tr>
<td><strong>Prophet</strong></td>
<td>ãƒ“ã‚¸ãƒã‚¹äºˆæ¸¬å…¨èˆ¬</td>
<td>ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã®æ´»ç”¨</td>
</tr>
<tr>
<td><strong>E2Eã‚·ã‚¹ãƒ†ãƒ </strong></td>
<td>æœ¬ç•ªäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ </td>
<td>ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨å†å­¦ç¿’</td>
</tr>
</tbody>
</table>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>Z-scoreæ³•ã¨IQRæ³•ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥ã®é•ã„ã‚’èª¬æ˜ã—ã€ãã‚Œãã‚Œã©ã®ã‚ˆã†ãªå ´åˆã«é©ã—ã¦ã„ã‚‹ã‹è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>Z-scoreæ³•</strong>ï¼š</p>
<ul>
<li>å¹³å‡ã¨æ¨™æº–åå·®ã‚’ä½¿ç”¨: $z = \frac{x - \mu}{\sigma}$</li>
<li>é€šå¸¸ã€$|z| > 3$ ã‚’ç•°å¸¸ã¨ã™ã‚‹</li>
<li>æ­£è¦åˆ†å¸ƒã‚’ä»®å®š</li>
</ul>

<p><strong>IQRæ³•</strong>ï¼š</p>
<ul>
<li>å››åˆ†ä½ç¯„å›²ã‚’ä½¿ç”¨: $IQR = Q3 - Q1$</li>
<li>$x < Q1 - 1.5 \times IQR$ ã¾ãŸã¯ $x > Q3 + 1.5 \times IQR$ ã‚’ç•°å¸¸ã¨ã™ã‚‹</li>
<li>åˆ†å¸ƒã®ä»®å®šãªã—</li>
</ul>

<p><strong>ä½¿ã„åˆ†ã‘</strong>ï¼š</p>

<table>
<thead>
<tr>
<th>çŠ¶æ³</th>
<th>æ¨å¥¨æ‰‹æ³•</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td>æ­£è¦åˆ†å¸ƒã«è¿‘ã„ãƒ‡ãƒ¼ã‚¿</td>
<td>Z-scoreæ³•</td>
<td>çµ±è¨ˆçš„ã«è§£é‡ˆã—ã‚„ã™ã„</td>
</tr>
<tr>
<td>æ­ªã‚“ã åˆ†å¸ƒ</td>
<td>IQRæ³•</td>
<td>å¤–ã‚Œå€¤ã«é ‘å¥</td>
</tr>
<tr>
<td>å°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</td>
<td>IQRæ³•</td>
<td>å¹³å‡ãŒä¸å®‰å®š</td>
</tr>
<tr>
<td>æ¥µç«¯ãªå¤–ã‚Œå€¤ã‚ã‚Š</td>
<td>IQRæ³•</td>
<td>ä¸­å¤®å€¤ãƒ™ãƒ¼ã‚¹ã§é ‘å¥</td>
</tr>
</tbody>
</table>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>Grangerå› æœæ€§ãƒ†ã‚¹ãƒˆã¯ã€çœŸã®å› æœé–¢ä¿‚ã‚’è¨¼æ˜ã§ãã¾ã™ã‹ï¼Ÿç†ç”±ã¨ã¨ã‚‚ã«ç­”ãˆã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>ã„ã„ãˆã€Grangerå› æœæ€§ã¯çœŸã®å› æœé–¢ä¿‚ã‚’è¨¼æ˜ã§ãã¾ã›ã‚“ã€‚</strong></p>

<p><strong>ç†ç”±</strong>ï¼š</p>

<ol>
<li><p><strong>äºˆæ¸¬çš„å› æœæ€§</strong>:</p>
<ul>
<li>Grangerå› æœæ€§ã¯ã€ŒXã®éå»ã®å€¤ãŒYã®äºˆæ¸¬ã«æœ‰ç”¨ã‹ã€ã‚’æ¤œå®š</li>
<li>äºˆæ¸¬å¯èƒ½æ€§ã¨å› æœé–¢ä¿‚ã¯ç•°ãªã‚‹</li>
</ul></li>

<li><p><strong>ç¬¬ä¸‰ã®å¤‰æ•°ã®å•é¡Œ</strong>:</p>
<ul>
<li>Xã¨Yã®ä¸¡æ–¹ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ç¬¬ä¸‰ã®å¤‰æ•°ZãŒå­˜åœ¨ã™ã‚‹å¯èƒ½æ€§</li>
<li>è¦‹ã‹ã‘ä¸Šã®å› æœé–¢ä¿‚ï¼ˆç–‘ä¼¼ç›¸é–¢ï¼‰</li>
</ul></li>

<li><p><strong>é€†å‘ãå› æœã®å¯èƒ½æ€§</strong>:</p>
<ul>
<li>Xâ†’Yã®å› æœæ€§ãŒæ¤œå‡ºã•ã‚Œã¦ã‚‚ã€Yâ†’Xã‚‚åŒæ™‚ã«æˆç«‹ã™ã‚‹å¯èƒ½æ€§</li>
<li>åŒæ–¹å‘ã®é–¢ä¿‚ã‚’å®Œå…¨ã«ã¯æ’é™¤ã§ããªã„</li>
</ul></li>

<li><p><strong>æ™‚é–“é…ã‚Œã®ä»®å®š</strong>:</p>
<ul>
<li>é©åˆ‡ãªãƒ©ã‚°æ¬¡æ•°ã®é¸æŠã«ä¾å­˜</li>
<li>ç¬æ™‚çš„ãªå› æœé–¢ä¿‚ã¯æ‰ãˆã‚‰ã‚Œãªã„</li>
</ul></li>
</ol>

<p><strong>æ­£ã—ã„è§£é‡ˆ</strong>ï¼š</p>
<p>Grangerå› æœæ€§ã¯ã€ŒXãŒYã®äºˆæ¸¬ã«æœ‰ç”¨ã§ã‚ã‚‹ã€ã¨ã„ã†è¨¼æ‹ ã‚’æä¾›ã™ã‚‹ãŒã€çœŸã®å› æœãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®è¨¼æ˜ã«ã¯ã€å®Ÿé¨“çš„ä»‹å…¥ã‚„ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ãŒå¿…è¦ã€‚</p>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>ä»¥ä¸‹ã®ã‚·ãƒŠãƒªã‚ªã§ã€é©åˆ‡ãªå› æœæ¨è«–æ‰‹æ³•ã‚’é¸æŠã—ã€ç†ç”±ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ï¼š</p>

<p>ã€Œæ–°ã—ã„åºƒå‘Šã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’ç‰¹å®šã®åœ°åŸŸã§å®Ÿæ–½ã—ã€å£²ä¸Šã¸ã®å½±éŸ¿ã‚’æ¸¬å®šã—ãŸã„ã€‚åºƒå‘Šã‚’å®Ÿæ–½ã—ãªã‹ã£ãŸé¡ä¼¼åœ°åŸŸã‚‚ã‚ã‚‹ã€‚ã€</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>æ¨å¥¨æ‰‹æ³•</strong>: Difference-in-Differencesï¼ˆDIDï¼‰æ³•ã¾ãŸã¯CausalImpact</p>

<p><strong>ç†ç”±</strong>ï¼š</p>

<ol>
<li><p><strong>DIDæ³•ãŒé©ã—ã¦ã„ã‚‹ç‚¹</strong>:</p>
<ul>
<li>å‡¦ç½®ç¾¤ï¼ˆåºƒå‘Šå®Ÿæ–½åœ°åŸŸï¼‰ã¨å¯¾ç…§ç¾¤ï¼ˆæœªå®Ÿæ–½åœ°åŸŸï¼‰ãŒå­˜åœ¨</li>
<li>ä»‹å…¥å‰å¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨å¯èƒ½</li>
<li>æ™‚é–“çš„ãƒˆãƒ¬ãƒ³ãƒ‰ã®å½±éŸ¿ã‚’é™¤å»ã§ãã‚‹</li>
<li>æ¯”è¼ƒçš„å˜ç´”ãªä»®å®šï¼ˆå¹³è¡Œãƒˆãƒ¬ãƒ³ãƒ‰ä»®å®šï¼‰</li>
</ul></li>

<li><p><strong>CausalImpactãŒé©ã—ã¦ã„ã‚‹ç‚¹</strong>:</p>
<ul>
<li>å¯¾ç…§ç¾¤ãŒè¤‡æ•°ã‚ã‚‹å ´åˆã€åˆ¶å¾¡å¤‰æ•°ã¨ã—ã¦åˆ©ç”¨å¯èƒ½</li>
<li>åå®Ÿä»®æƒ³ï¼ˆåºƒå‘Šãªã—ã®å ´åˆï¼‰ã‚’çµ±è¨ˆçš„ã«æ¨å®š</li>
<li>ä¿¡é ¼åŒºé–“ã«ã‚ˆã‚‹åŠ¹æœã®ä¸ç¢ºå®Ÿæ€§è©•ä¾¡</li>
<li>ãƒ™ã‚¤ã‚ºæ§‹é€ æ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ã§é ‘å¥ãªæ¨å®š</li>
</ul></li>

<li><p><strong>å®Ÿè£…ä¾‹ï¼ˆDIDï¼‰</strong>:</p>
</li>
</ol>

<pre><code class="language-python"># å‡¦ç½®ç¾¤: åºƒå‘Šå®Ÿæ–½åœ°åŸŸã®å£²ä¸Š
# å¯¾ç…§ç¾¤: åºƒå‘Šæœªå®Ÿæ–½åœ°åŸŸã®å£²ä¸Š
# ä»‹å…¥æ™‚ç‚¹: åºƒå‘Šé–‹å§‹æ—¥

treatment_before = åºƒå‘Šå‰ã®å‡¦ç½®ç¾¤å¹³å‡
treatment_after = åºƒå‘Šå¾Œã®å‡¦ç½®ç¾¤å¹³å‡
control_before = åºƒå‘Šå‰ã®å¯¾ç…§ç¾¤å¹³å‡
control_after = åºƒå‘Šå¾Œã®å¯¾ç…§ç¾¤å¹³å‡

# DIDæ¨å®šé‡
did_estimate = (treatment_after - treatment_before) - (control_after - control_before)

# did_estimate ãŒåºƒå‘Šã®å› æœåŠ¹æœ
</code></pre>

<p><strong>æ³¨æ„ç‚¹</strong>ï¼š</p>
<ul>
<li>å¹³è¡Œãƒˆãƒ¬ãƒ³ãƒ‰ä»®å®š: ä»‹å…¥ãŒãªã‘ã‚Œã°ä¸¡ç¾¤ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã¯åŒã˜ã¨ä»®å®š</li>
<li>åœ°åŸŸã®é¡ä¼¼æ€§: å¯¾ç…§ç¾¤ãŒå‡¦ç½®ç¾¤ã¨å¯èƒ½ãªé™ã‚Šé¡ä¼¼ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª</li>
<li>å¤–éƒ¨è¦å› : åŒæ™‚æœŸã®ä»–ã®ã‚¤ãƒ™ãƒ³ãƒˆã®å½±éŸ¿ã‚’è€ƒæ…®</li>
</ul>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>Prophetãƒ¢ãƒ‡ãƒ«ã§ã€ãƒˆãƒ¬ãƒ³ãƒ‰ã®å¤‰åŒ–ç‚¹ï¼ˆchangepointï¼‰ãŒå¤šã™ãã‚‹å ´åˆã¨å°‘ãªã™ãã‚‹å ´åˆã€ãã‚Œãã‚Œã©ã®ã‚ˆã†ãªå•é¡ŒãŒç™ºç”Ÿã—ã¾ã™ã‹ï¼Ÿé©åˆ‡ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´æ–¹æ³•ã‚‚èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>å¤‰åŒ–ç‚¹ãŒå¤šã™ãã‚‹å ´åˆï¼ˆéå­¦ç¿’ï¼‰</strong>ï¼š</p>
<ul>
<li>å•é¡Œ: ãƒã‚¤ã‚ºã‚’ãƒˆãƒ¬ãƒ³ãƒ‰å¤‰åŒ–ã¨èª¤èªè­˜</li>
<li>çµæœ: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«éå‰°é©åˆã€æ±åŒ–æ€§èƒ½ã®ä½ä¸‹</li>
<li>äºˆæ¸¬: å°†æ¥äºˆæ¸¬ãŒä¸å®‰å®šã§ä¿¡é ¼æ€§ãŒä½ã„</li>
</ul>

<p><strong>å¤‰åŒ–ç‚¹ãŒå°‘ãªã™ãã‚‹å ´åˆï¼ˆéå°é©åˆï¼‰</strong>ï¼š</p>
<ul>
<li>å•é¡Œ: çœŸã®ãƒˆãƒ¬ãƒ³ãƒ‰å¤‰åŒ–ã‚’æ‰ãˆã‚‰ã‚Œãªã„</li>
<li>çµæœ: ãƒ¢ãƒ‡ãƒ«ãŒå˜ç´”ã™ãã€é‡è¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¦‹é€ƒã™</li>
<li>äºˆæ¸¬: ç³»çµ±çš„ãªèª¤å·®ã€äºˆæ¸¬ç²¾åº¦ã®ä½ä¸‹</li>
</ul>

<p><strong>é©åˆ‡ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´</strong>ï¼š</p>

<ol>
<li><p><strong>changepoint_prior_scale</strong>ï¼ˆå¤‰åŒ–ç‚¹ã®æŸ”è»Ÿæ€§ï¼‰:</p>
</li>
</ol>

<pre><code class="language-python"># ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.05
# å°ã•ã„å€¤ï¼ˆ0.001-0.01ï¼‰: å¤‰åŒ–ã‚’æŠ‘åˆ¶ã€æ»‘ã‚‰ã‹
# å¤§ãã„å€¤ï¼ˆ0.1-0.5ï¼‰: å¤‰åŒ–ã‚’è¨±å®¹ã€æŸ”è»Ÿ

# èª¿æ•´ä¾‹
model_smooth = Prophet(changepoint_prior_scale=0.01)  # ä¿å®ˆçš„
model_flexible = Prophet(changepoint_prior_scale=0.5)  # æŸ”è»Ÿ
</code></pre>

<ol start="2">
<li><strong>n_changepoints</strong>ï¼ˆå€™è£œå¤‰åŒ–ç‚¹ã®æ•°ï¼‰:</li>
</ol>

<pre><code class="language-python"># ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 25
# ãƒ‡ãƒ¼ã‚¿é•·ã«å¿œã˜ã¦èª¿æ•´

model = Prophet(n_changepoints=50)  # é•·ã„æ™‚ç³»åˆ—ã®å ´åˆ
</code></pre>

<ol start="3">
<li><strong>äº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹æœ€é©åŒ–</strong>:</li>
</ol>

<pre><code class="language-python">from prophet.diagnostics import cross_validation, performance_metrics

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€™è£œ
param_grid = {
    'changepoint_prior_scale': [0.001, 0.01, 0.05, 0.1, 0.5],
}

best_params = None
best_rmse = float('inf')

for scale in param_grid['changepoint_prior_scale']:
    model = Prophet(changepoint_prior_scale=scale)
    model.fit(df)

    # äº¤å·®æ¤œè¨¼
    df_cv = cross_validation(model, initial='730 days',
                             period='180 days', horizon='90 days')
    df_p = performance_metrics(df_cv)

    rmse = df_p['rmse'].mean()
    if rmse < best_rmse:
        best_rmse = rmse
        best_params = {'changepoint_prior_scale': scale}

print(f"æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {best_params}")
print(f"RMSE: {best_rmse}")
</code></pre>

<p><strong>é¸æŠã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³</strong>ï¼š</p>

<table>
<thead>
<tr>
<th>ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§</th>
<th>changepoint_prior_scale</th>
</tr>
</thead>
<tbody>
<tr>
<td>å®‰å®šã—ãŸãƒˆãƒ¬ãƒ³ãƒ‰</td>
<td>0.001 - 0.01</td>
</tr>
<tr>
<td>é€šå¸¸ã®ãƒ“ã‚¸ãƒã‚¹ãƒ‡ãƒ¼ã‚¿</td>
<td>0.05ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰</td>
</tr>
<tr>
<td>é »ç¹ãªãƒˆãƒ¬ãƒ³ãƒ‰å¤‰åŒ–</td>
<td>0.1 - 0.5</td>
</tr>
<tr>
<td>ä¸ç¢ºå®Ÿãªå ´åˆ</td>
<td>äº¤å·®æ¤œè¨¼ã§æ±ºå®š</td>
</tr>
</tbody>
</table>

</details>

<h3>å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‰ãƒªãƒ•ãƒˆï¼ˆæ€§èƒ½åŠ£åŒ–ï¼‰ã‚’æ¤œå‡ºã—ã€è‡ªå‹•å†å­¦ç¿’ã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹ä»•çµ„ã¿ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings('ignore')

class ModelMonitoring:
    """ãƒ¢ãƒ‡ãƒ«ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã¨è‡ªå‹•å†å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, model, threshold_rmse_increase=0.2,
                 window_size=30, retrain_frequency=90):
        """
        Parameters:
        -----------
        model: äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
        threshold_rmse_increase: RMSEå¢—åŠ ã®é–¾å€¤ï¼ˆ20%å¢—åŠ ã§å†å­¦ç¿’ï¼‰
        window_size: ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆç›´è¿‘30æ—¥ï¼‰
        retrain_frequency: æœ€å°å†å­¦ç¿’é–“éš”ï¼ˆ90æ—¥ï¼‰
        """
        self.model = model
        self.threshold = threshold_rmse_increase
        self.window_size = window_size
        self.retrain_frequency = retrain_frequency

        # ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æŒ‡æ¨™
        self.baseline_rmse = None
        self.current_errors = []
        self.last_retrain_date = None
        self.retrain_history = []

    def set_baseline(self, X_val, y_val):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½ã‚’è¨­å®š"""
        y_pred = self.model.predict(X_val)
        self.baseline_rmse = np.sqrt(mean_squared_error(y_val, y_pred))
        print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³RMSE: {self.baseline_rmse:.4f}")
        return self

    def monitor_prediction(self, X_new, y_true, date):
        """æ–°ã—ã„äºˆæ¸¬ã‚’ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°"""
        # äºˆæ¸¬
        y_pred = self.model.predict(X_new)

        # èª¤å·®ã‚’è¨˜éŒ²
        error = np.abs(y_true - y_pred)
        self.current_errors.append({
            'date': date,
            'error': error,
            'y_true': y_true,
            'y_pred': y_pred
        })

        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã‚’è¶…ãˆãŸã‚‰å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
        if len(self.current_errors) > self.window_size:
            self.current_errors.pop(0)

        # ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º
        if len(self.current_errors) >= self.window_size:
            drift_detected = self._detect_drift()

            if drift_detected:
                print(f"\nâš ï¸ ãƒ¢ãƒ‡ãƒ«ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºï¼ˆæ—¥ä»˜: {date}ï¼‰")

                # å†å­¦ç¿’ã®å¿…è¦æ€§ãƒã‚§ãƒƒã‚¯
                if self._should_retrain(date):
                    print("ğŸ”„ è‡ªå‹•å†å­¦ç¿’ã‚’é–‹å§‹")
                    return True  # å†å­¦ç¿’ãŒå¿…è¦
                else:
                    print(f"æœ€çµ‚å†å­¦ç¿’ã‹ã‚‰{self.retrain_frequency}æ—¥æœªæº€ã®ãŸã‚å¾…æ©Ÿ")

        return False  # å†å­¦ç¿’ä¸è¦

    def _detect_drift(self):
        """ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡º"""
        # ç›´è¿‘ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®RMSE
        recent_errors = [e['error'] for e in self.current_errors]
        current_rmse = np.sqrt(np.mean(np.square(recent_errors)))

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒ
        rmse_increase = (current_rmse - self.baseline_rmse) / self.baseline_rmse

        print(f"ç¾åœ¨ã®RMSE: {current_rmse:.4f} (å¢—åŠ ç‡: {rmse_increase*100:.1f}%)")

        # é–¾å€¤ã‚’è¶…ãˆãŸã‚‰ãƒ‰ãƒªãƒ•ãƒˆã¨åˆ¤å®š
        return rmse_increase > self.threshold

    def _should_retrain(self, current_date):
        """å†å­¦ç¿’ã™ã¹ãã‹åˆ¤å®š"""
        if self.last_retrain_date is None:
            return True

        days_since_retrain = (current_date - self.last_retrain_date).days
        return days_since_retrain >= self.retrain_frequency

    def retrain(self, X_train, y_train, X_val, y_val, date):
        """ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’"""
        # å†å­¦ç¿’å®Ÿè¡Œ
        self.model.fit(X_train, y_train)

        # æ–°ã—ã„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¨­å®š
        self.set_baseline(X_val, y_val)

        # å†å­¦ç¿’å±¥æ­´ã‚’è¨˜éŒ²
        self.last_retrain_date = date
        self.retrain_history.append({
            'date': date,
            'new_baseline_rmse': self.baseline_rmse
        })

        # ã‚¨ãƒ©ãƒ¼å±¥æ­´ã‚’ã‚¯ãƒªã‚¢
        self.current_errors = []

        print(f"âœ… å†å­¦ç¿’å®Œäº†ï¼ˆæ–°ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³RMSE: {self.baseline_rmse:.4f}ï¼‰")

    def get_monitoring_report(self):
        """ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆ"""
        report = {
            'baseline_rmse': self.baseline_rmse,
            'num_retrains': len(self.retrain_history),
            'last_retrain': self.last_retrain_date,
            'retrain_history': self.retrain_history
        }
        return report


# ä½¿ç”¨ä¾‹
from sklearn.ensemble import RandomForestRegressor

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆãƒ‰ãƒªãƒ•ãƒˆã‚’å«ã‚€ï¼‰
np.random.seed(42)
n = 365
dates = pd.date_range('2023-01-01', periods=n, freq='D')

# æœ€åˆã¯å®‰å®š
data1 = 100 + np.cumsum(np.random.normal(0, 1, 200))
# 200æ—¥ç›®ã‹ã‚‰ãƒ‰ãƒªãƒ•ãƒˆï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰å¤‰åŒ–ï¼‰
data2 = data1[-1] + np.cumsum(np.random.normal(0.5, 2, n - 200))
data = np.concatenate([data1, data2])

# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆç°¡ç•¥åŒ–ï¼‰
X = np.arange(n).reshape(-1, 1)
y = data

# åˆæœŸå­¦ç¿’
train_size = 150
X_train, y_train = X[:train_size], y[:train_size]
X_val, y_val = X[train_size:200], y[train_size:200]

model = RandomForestRegressor(n_estimators=50, random_state=42)
model.fit(X_train, y_train)

# ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
monitor = ModelMonitoring(
    model=model,
    threshold_rmse_increase=0.2,
    window_size=30,
    retrain_frequency=90
)
monitor.set_baseline(X_val, y_val)

# ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
print("\n=== ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°é–‹å§‹ ===")
for i in range(200, n):
    X_new = X[i:i+1]
    y_true = y[i]
    date = dates[i]

    # ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
    needs_retrain = monitor.monitor_prediction(X_new, y_true, date)

    # å†å­¦ç¿’ãŒå¿…è¦ãªå ´åˆ
    if needs_retrain:
        # å†å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ï¼ˆç›´è¿‘ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
        retrain_start = max(0, i - 150)
        X_retrain = X[retrain_start:i]
        y_retrain = y[retrain_start:i]
        X_val_new = X[i-50:i]
        y_val_new = y[i-50:i]

        monitor.retrain(X_retrain, y_retrain, X_val_new, y_val_new, date)

# ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆ
print("\n=== ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆ ===")
report = monitor.get_monitoring_report()
print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³RMSE: {report['baseline_rmse']:.4f}")
print(f"å†å­¦ç¿’å›æ•°: {report['num_retrains']}")
print(f"æœ€çµ‚å†å­¦ç¿’æ—¥: {report['last_retrain']}")
print("\nå†å­¦ç¿’å±¥æ­´:")
for h in report['retrain_history']:
    print(f"  {h['date'].date()}: RMSE={h['new_baseline_rmse']:.4f}")
</code></pre>

<p><strong>è¨­è¨ˆã®ãƒã‚¤ãƒ³ãƒˆ</strong>ï¼š</p>

<ol>
<li><strong>ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºæŒ‡æ¨™</strong>:
<ul>
<li>RMSEå¢—åŠ ç‡ï¼ˆæ€§èƒ½åŠ£åŒ–ï¼‰</li>
<li>äºˆæ¸¬èª¤å·®ã®åˆ†å¸ƒå¤‰åŒ–ï¼ˆKSãƒ†ã‚¹ãƒˆãªã©ï¼‰</li>
<li>ç‰¹å¾´é‡åˆ†å¸ƒã®å¤‰åŒ–</li>
</ul></li>

<li><strong>å†å­¦ç¿’æˆ¦ç•¥</strong>:
<ul>
<li>å®šæœŸçš„å†å­¦ç¿’ï¼ˆæ™‚é–“ãƒ™ãƒ¼ã‚¹ï¼‰</li>
<li>æ€§èƒ½ãƒ™ãƒ¼ã‚¹å†å­¦ç¿’ï¼ˆãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºæ™‚ï¼‰</li>
<li>ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼ˆä¸¡æ–¹ã®æ¡ä»¶ï¼‰</li>
</ul></li>

<li><strong>å®Ÿè£…ä¸Šã®è€ƒæ…®ç‚¹</strong>:
<ul>
<li>A/Bãƒ†ã‚¹ãƒˆ: æ–°æ—§ãƒ¢ãƒ‡ãƒ«ã‚’ä¸¦è¡Œç¨¼åƒ</li>
<li>ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½: å†å­¦ç¿’å¾Œã®æ€§èƒ½æ‚ªåŒ–ã«å¯¾å¿œ</li>
<li>ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½: äººé–“ã®ä»‹å…¥ãŒå¿…è¦ãªå ´åˆ</li>
</ul></li>
</ol>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Taylor, S. J., & Letham, B. (2018). Forecasting at scale. <em>The American Statistician</em>, 72(1), 37-45.</li>
<li>Brodersen, K. H., et al. (2015). Inferring causal impact using Bayesian structural time-series models. <em>Annals of Applied Statistics</em>, 9(1), 247-274.</li>
<li>Chandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly detection: A survey. <em>ACM Computing Surveys</em>, 41(3), 1-58.</li>
<li>LÃ¼tkepohl, H. (2005). <em>New Introduction to Multiple Time Series Analysis</em>. Springer.</li>
<li>Hyndman, R. J., & Athanasopoulos, G. (2021). <em>Forecasting: Principles and Practice</em> (3rd ed.). OTexts.</li>
<li>Pearl, J., & Mackenzie, D. (2018). <em>The Book of Why: The New Science of Cause and Effect</em>. Basic Books.</li>
</ol>

<div class="navigation">
    <a href="chapter4-deep-learning-models.html" class="nav-button">â† å‰ã®ç« : æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«</a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
