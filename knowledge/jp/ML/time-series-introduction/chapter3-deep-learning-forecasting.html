<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬3ç« ï¼šæ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹æ™‚ç³»åˆ—äºˆæ¸¬ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
        <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/wp/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/ML/time-series-introduction/index.html">Time Series</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

    <header>
        <div class="header-content">
            <h1>ç¬¬3ç« ï¼šæ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹æ™‚ç³»åˆ—äºˆæ¸¬</h1>
            <p class="subtitle">LSTMã€GRUã€TCNã€Attentionã«ã‚ˆã‚‹é«˜åº¦ãªäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 35-40åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 10å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’æ·±å±¤å­¦ç¿’ã§æ‰±ã†ãŸã‚ã®åŸºæœ¬çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… LSTMãƒ»GRUã‚’ç”¨ã„ãŸæ™‚ç³»åˆ—äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
<li>âœ… TCNï¼ˆTemporal Convolutional Networkï¼‰ã®ä»•çµ„ã¿ã¨å®Ÿè£…æ–¹æ³•ã‚’å­¦ã¶</li>
<li>âœ… Attentionæ©Ÿæ§‹ã‚’æ™‚ç³»åˆ—äºˆæ¸¬ã«å¿œç”¨ã§ãã‚‹</li>
<li>âœ… ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… PyTorchã§å®Ÿè·µçš„ãªæ™‚ç³»åˆ—äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã§ãã‚‹</li>
</ul>

<hr>

<h2>3.1 æ™‚ç³»åˆ—ã®ãŸã‚ã®æ·±å±¤å­¦ç¿’</h2>

<h3>Sequential Dataã®è¡¨ç¾</h3>
<p><strong>æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼ˆSequential Dataï¼‰</strong>ã¯ã€æ™‚é–“çš„ãªé †åºã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚æ·±å±¤å­¦ç¿’ã§ã¯ã€ã“ã®é †åºé–¢ä¿‚ã‚’ä¿æŒã—ãªãŒã‚‰ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚</p>

<blockquote>
<p>ã€Œæ™‚ç³»åˆ—äºˆæ¸¬ã®æœ¬è³ªã¯ã€éå»ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æœªæ¥ã‚’æ¨è«–ã™ã‚‹ã“ã¨ã€</p>
</blockquote>

<h3>Window-based Approachï¼ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒ™ãƒ¼ã‚¹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰</h3>

<p>æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’æ·±å±¤å­¦ç¿’ã§æ‰±ã†ãŸã‚ã®åŸºæœ¬çš„ãªæ‰‹æ³•ã¯ã€<strong>ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆSliding Windowï¼‰</strong>ã§ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[å…ƒãƒ‡ãƒ¼ã‚¿: t1, t2, t3, t4, t5, t6] --> B[Window 1: t1-t3 â†’ t4]
    A --> C[Window 2: t2-t4 â†’ t5]
    A --> D[Window 3: t3-t5 â†’ t6]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#fff3e0
    style D fill:#fff3e0
</div>

<h3>å®Ÿè£…ï¼šã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt

# ã‚µãƒ³ãƒ—ãƒ«æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
np.random.seed(42)
time = np.arange(0, 100, 0.1)
data = np.sin(time) + 0.1 * np.random.randn(len(time))

# å¯è¦–åŒ–
plt.figure(figsize=(14, 5))
plt.plot(time, data, label='Time Series Data', alpha=0.8)
plt.xlabel('Time')
plt.ylabel('Value')
plt.title('ã‚µãƒ³ãƒ—ãƒ«æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

print(f"ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°: {len(data)}")
print(f"ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²: [{data.min():.3f}, {data.max():.3f}]")
</code></pre>

<h3>PyTorch Dataset for Time Series</h3>

<pre><code class="language-python">class TimeSeriesDataset(Dataset):
    """
    æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ç”¨ã®PyTorch Dataset

    Parameters:
    -----------
    data : np.ndarray
        æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼ˆ1æ¬¡å…ƒé…åˆ—ï¼‰
    window_size : int
        å…¥åŠ›ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ã‚µã‚¤ã‚º
    horizon : int
        äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ï¼ˆä½•ã‚¹ãƒ†ãƒƒãƒ—å…ˆã‚’äºˆæ¸¬ã™ã‚‹ã‹ï¼‰
    """
    def __init__(self, data, window_size=20, horizon=1):
        self.data = data
        self.window_size = window_size
        self.horizon = horizon

    def __len__(self):
        return len(self.data) - self.window_size - self.horizon + 1

    def __getitem__(self, idx):
        # å…¥åŠ›: window_sizeå€‹ã®éå»ãƒ‡ãƒ¼ã‚¿
        x = self.data[idx:idx + self.window_size]
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: horizonå€‹ã®æœªæ¥ãƒ‡ãƒ¼ã‚¿
        y = self.data[idx + self.window_size:idx + self.window_size + self.horizon]

        return torch.FloatTensor(x), torch.FloatTensor(y)

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
window_size = 20
horizon = 5  # 5ã‚¹ãƒ†ãƒƒãƒ—å…ˆã‚’äºˆæ¸¬

# è¨“ç·´ãƒ»æ¤œè¨¼åˆ†å‰²
train_size = int(0.8 * len(data))
train_data = data[:train_size]
val_data = data[train_size:]

train_dataset = TimeSeriesDataset(train_data, window_size, horizon)
val_dataset = TimeSeriesDataset(val_data, window_size, horizon)

# DataLoaderã®ä½œæˆ
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

print("=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ± ===")
print(f"è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(train_dataset)}")
print(f"æ¤œè¨¼ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(val_dataset)}")
print(f"å…¥åŠ›ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º: {window_size}")
print(f"äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³: {horizon}")

# ã‚µãƒ³ãƒ—ãƒ«ã®ç¢ºèª
x_sample, y_sample = train_dataset[0]
print(f"\nã‚µãƒ³ãƒ—ãƒ«å½¢çŠ¶:")
print(f"  å…¥åŠ› x: {x_sample.shape}")
print(f"  ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ y: {y_sample.shape}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ± ===
è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°: 776
æ¤œè¨¼ã‚µãƒ³ãƒ—ãƒ«æ•°: 176
å…¥åŠ›ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º: 20
äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³: 5

ã‚µãƒ³ãƒ—ãƒ«å½¢çŠ¶:
  å…¥åŠ› x: torch.Size([20])
  ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ y: torch.Size([5])
</code></pre>

<h3>Multi-step Forecastingï¼ˆè¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—äºˆæ¸¬ï¼‰</h3>

<p>æ™‚ç³»åˆ—äºˆæ¸¬ã§ã¯ã€ä»¥ä¸‹ã®2ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒã‚ã‚Šã¾ã™ï¼š</p>

<table>
<thead>
<tr>
<th>ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</th>
<th>èª¬æ˜</th>
<th>åˆ©ç‚¹</th>
<th>æ¬ ç‚¹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>One-shot</strong></td>
<td>1å›ã®äºˆæ¸¬ã§å…¨ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ã‚’å‡ºåŠ›</td>
<td>é«˜é€Ÿã€ä¾å­˜é–¢ä¿‚ãªã—</td>
<td>é•·æœŸäºˆæ¸¬ãŒé›£ã—ã„</td>
</tr>
<tr>
<td><strong>Autoregressive</strong></td>
<td>1ã‚¹ãƒ†ãƒƒãƒ—ãšã¤äºˆæ¸¬ã—ã€æ¬¡ã®å…¥åŠ›ã«ä½¿ç”¨</td>
<td>æŸ”è»Ÿã€é•·æœŸäºˆæ¸¬å¯èƒ½</td>
<td>èª¤å·®ãŒè“„ç©</td>
</tr>
</tbody>
</table>

<div class="mermaid">
graph TD
    A[éå»ãƒ‡ãƒ¼ã‚¿: t-n...t] --> B{äºˆæ¸¬æ–¹å¼}
    B -->|One-shot| C[ä¸€åº¦ã«äºˆæ¸¬: t+1, t+2, ..., t+h]
    B -->|Autoregressive| D[t+1ã‚’äºˆæ¸¬]
    D --> E[t+1ã‚’å…¥åŠ›ã«è¿½åŠ ]
    E --> F[t+2ã‚’äºˆæ¸¬]
    F --> G[ç¹°ã‚Šè¿”ã—...]

    style A fill:#e3f2fd
    style C fill:#c8e6c9
    style D fill:#fff3e0
    style F fill:#fff3e0
</div>

<hr>

<h2>3.2 LSTM & GRU for æ™‚ç³»åˆ—äºˆæ¸¬</h2>

<h3>LSTM Architecture Review</h3>

<p><strong>LSTMï¼ˆLong Short-Term Memoryï¼‰</strong>ã¯ã€é•·æœŸä¾å­˜æ€§ã‚’å­¦ç¿’ã§ãã‚‹RNNã®ä¸€ç¨®ã§ã™ã€‚</p>

<p>LSTM ã‚»ãƒ«ã®æ›´æ–°å¼ï¼š</p>

<p>$$
\begin{align*}
f_t &= \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \quad \text{(å¿˜å´ã‚²ãƒ¼ãƒˆ)} \\
i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \quad \text{(å…¥åŠ›ã‚²ãƒ¼ãƒˆ)} \\
\tilde{C}_t &= \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) \quad \text{(å€™è£œå€¤)} \\
C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \quad \text{(ã‚»ãƒ«çŠ¶æ…‹æ›´æ–°)} \\
o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \quad \text{(å‡ºåŠ›ã‚²ãƒ¼ãƒˆ)} \\
h_t &= o_t \odot \tanh(C_t) \quad \text{(éš ã‚ŒçŠ¶æ…‹)}
\end{align*}
$$</p>

<h3>PyTorchã§ã®LSTMå®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn

class LSTMForecaster(nn.Module):
    """
    LSTM-based time series forecasting model

    Parameters:
    -----------
    input_size : int
        å…¥åŠ›ç‰¹å¾´é‡ã®æ¬¡å…ƒ
    hidden_size : int
        LSTMéš ã‚Œå±¤ã®ã‚µã‚¤ã‚º
    num_layers : int
        LSTMãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ•°
    output_size : int
        å‡ºåŠ›ã‚µã‚¤ã‚ºï¼ˆäºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ï¼‰
    dropout : float
        ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
    """
    def __init__(self, input_size=1, hidden_size=64, num_layers=2,
                 output_size=1, dropout=0.2):
        super(LSTMForecaster, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTMå±¤
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True
        )

        # å…¨çµåˆå±¤
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # x shape: (batch_size, seq_len, input_size)

        # LSTM forward pass
        # out shape: (batch_size, seq_len, hidden_size)
        out, (h_n, c_n) = self.lstm(x)

        # æœ€å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®å‡ºåŠ›ã‚’ä½¿ç”¨
        # out[:, -1, :] shape: (batch_size, hidden_size)
        out = self.fc(out[:, -1, :])

        # out shape: (batch_size, output_size)
        return out

# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
model = LSTMForecaster(
    input_size=1,
    hidden_size=64,
    num_layers=2,
    output_size=horizon,
    dropout=0.2
)

print("=== LSTM ãƒ¢ãƒ‡ãƒ«æ§‹é€  ===")
print(model)
print(f"\nãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== LSTM ãƒ¢ãƒ‡ãƒ«æ§‹é€  ===
LSTMForecaster(
  (lstm): LSTM(1, 64, num_layers=2, batch_first=True, dropout=0.2)
  (fc): Linear(in_features=64, out_features=5, bias=True)
)

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 50,245
</code></pre>

<h3>è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®å®Ÿè£…</h3>

<pre><code class="language-python">import torch.optim as optim
from tqdm import tqdm

def train_model(model, train_loader, val_loader, epochs=50, lr=0.001):
    """
    ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    train_losses = []
    val_losses = []

    for epoch in range(epochs):
        # è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚º
        model.train()
        train_loss = 0.0

        for x_batch, y_batch in train_loader:
            # ãƒ‡ãƒ¼ã‚¿ã‚’ (batch, seq_len, features) ã®å½¢ã«å¤‰æ›
            x_batch = x_batch.unsqueeze(-1).to(device)
            y_batch = y_batch.to(device)

            # å‹¾é…åˆæœŸåŒ–
            optimizer.zero_grad()

            # Forward pass
            outputs = model(x_batch)
            loss = criterion(outputs, y_batch)

            # Backward pass
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        train_loss /= len(train_loader)
        train_losses.append(train_loss)

        # æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º
        model.eval()
        val_loss = 0.0

        with torch.no_grad():
            for x_batch, y_batch in val_loader:
                x_batch = x_batch.unsqueeze(-1).to(device)
                y_batch = y_batch.to(device)

                outputs = model(x_batch)
                loss = criterion(outputs, y_batch)
                val_loss += loss.item()

        val_loss /= len(val_loader)
        val_losses.append(val_loss)

        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{epochs}], "
                  f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

    return train_losses, val_losses

# ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
train_losses, val_losses = train_model(model, train_loader, val_loader, epochs=50)

# å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–
plt.figure(figsize=(12, 5))
plt.plot(train_losses, label='Train Loss', alpha=0.8)
plt.plot(val_losses, label='Validation Loss', alpha=0.8)
plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.title('å­¦ç¿’æ›²ç·š', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

<h3>GRUï¼ˆGated Recurrent Unitï¼‰</h3>

<p><strong>GRU</strong>ã¯LSTMã®ç°¡ç•¥ç‰ˆã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå°‘ãªãè¨“ç·´ãŒé«˜é€Ÿã§ã™ã€‚</p>

<p>GRU ã®æ›´æ–°å¼ï¼š</p>

<p>$$
\begin{align*}
r_t &= \sigma(W_r \cdot [h_{t-1}, x_t]) \quad \text{(ãƒªã‚»ãƒƒãƒˆã‚²ãƒ¼ãƒˆ)} \\
z_t &= \sigma(W_z \cdot [h_{t-1}, x_t]) \quad \text{(æ›´æ–°ã‚²ãƒ¼ãƒˆ)} \\
\tilde{h}_t &= \tanh(W \cdot [r_t \odot h_{t-1}, x_t]) \quad \text{(å€™è£œéš ã‚ŒçŠ¶æ…‹)} \\
h_t &= (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t \quad \text{(éš ã‚ŒçŠ¶æ…‹æ›´æ–°)}
\end{align*}
$$</p>

<pre><code class="language-python">class GRUForecaster(nn.Module):
    """
    GRU-based time series forecasting model
    """
    def __init__(self, input_size=1, hidden_size=64, num_layers=2,
                 output_size=1, dropout=0.2):
        super(GRUForecaster, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # GRUå±¤
        self.gru = nn.GRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True
        )

        # å…¨çµåˆå±¤
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # GRU forward pass
        out, h_n = self.gru(x)

        # æœ€å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®å‡ºåŠ›ã‚’ä½¿ç”¨
        out = self.fc(out[:, -1, :])

        return out

# GRUãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
gru_model = GRUForecaster(
    input_size=1,
    hidden_size=64,
    num_layers=2,
    output_size=horizon,
    dropout=0.2
)

print("=== GRU ãƒ¢ãƒ‡ãƒ«æ§‹é€  ===")
print(gru_model)
print(f"\nLSTMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}")
print(f"GRUãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in gru_model.parameters()):,}")
print(f"å‰Šæ¸›ç‡: {(1 - sum(p.numel() for p in gru_model.parameters()) / sum(p.numel() for p in model.parameters())) * 100:.1f}%")
</code></pre>

<h3>Stateful vs Stateless LSTM</h3>

<table>
<thead>
<tr>
<th>ã‚¿ã‚¤ãƒ—</th>
<th>èª¬æ˜</th>
<th>ä½¿ç”¨å ´é¢</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Stateless</strong></td>
<td>ãƒãƒƒãƒã”ã¨ã«éš ã‚ŒçŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ</td>
<td>ç‹¬ç«‹ã—ãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã€ä¸€èˆ¬çš„ãªäºˆæ¸¬</td>
</tr>
<tr>
<td><strong>Stateful</strong></td>
<td>ãƒãƒƒãƒé–“ã§éš ã‚ŒçŠ¶æ…‹ã‚’ä¿æŒ</td>
<td>é•·æœŸé€£ç¶šäºˆæ¸¬ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.3 TCN (Temporal Convolutional Network)</h2>

<h3>TCNã¨ã¯</h3>

<p><strong>TCNï¼ˆTemporal Convolutional Networkï¼‰</strong>ã¯ã€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«ç‰¹åŒ–ã—ãŸç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚RNNã¨ç•°ãªã‚Šã€ä¸¦åˆ—å‡¦ç†ãŒå¯èƒ½ã§è¨“ç·´ãŒé«˜é€Ÿã§ã™ã€‚</p>

<h3>Dilated Convolutionsï¼ˆæ‹¡å¼µç•³ã¿è¾¼ã¿ï¼‰</h3>

<p>TCNã®æ ¸å¿ƒã¯<strong>Dilated Convolution</strong>ã§ã™ã€‚é€šå¸¸ã®ç•³ã¿è¾¼ã¿ã«æ¯”ã¹ã€ã‚ˆã‚Šåºƒã„å—å®¹é‡ã‚’å°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å®Ÿç¾ã—ã¾ã™ã€‚</p>

<div class="mermaid">
graph TD
    A[å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹] --> B[Layer 1: dilation=1]
    B --> C[Layer 2: dilation=2]
    C --> D[Layer 3: dilation=4]
    D --> E[Layer 4: dilation=8]
    E --> F[å‡ºåŠ›: åºƒã„å—å®¹é‡]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#ffe0b2
    style D fill:#ffccbc
    style E fill:#ffab91
    style F fill:#c8e6c9
</div>

<p>å—å®¹é‡ã®è¨ˆç®—ï¼š</p>

<p>$$
\text{Receptive Field} = 1 + 2 \times (k - 1) \times \sum_{i=0}^{L-1} d^i
$$</p>

<ul>
<li>$k$: ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º</li>
<li>$d$: dilation factor</li>
<li>$L$: ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°</li>
</ul>

<h3>Causal Convolutionsï¼ˆå› æœçš„ç•³ã¿è¾¼ã¿ï¼‰</h3>

<p><strong>Causal Convolution</strong>ã¯ã€æœªæ¥ã®æƒ…å ±ã‚’ä½¿ã‚ãªã„ç•³ã¿è¾¼ã¿ã§ã™ã€‚æ™‚ç³»åˆ—äºˆæ¸¬ã§ã¯å¿…é ˆã§ã™ã€‚</p>

<blockquote>
<p><strong>é‡è¦</strong>: ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¯å·¦å´ã®ã¿ã«è¡Œã„ã€æœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‚ç…§ã—ãªã„ã‚ˆã†ã«ã—ã¾ã™ã€‚</p>
</blockquote>

<h3>PyTorchã§ã®TCNå®Ÿè£…</h3>

<pre><code class="language-python">class CausalConv1d(nn.Module):
    """
    Causal 1D Convolution with dilation
    """
    def __init__(self, in_channels, out_channels, kernel_size, dilation=1):
        super(CausalConv1d, self).__init__()

        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¯å·¦å´ã®ã¿ï¼ˆéå»ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿å‚ç…§ï¼‰
        self.padding = (kernel_size - 1) * dilation

        self.conv = nn.Conv1d(
            in_channels,
            out_channels,
            kernel_size,
            padding=self.padding,
            dilation=dilation
        )

    def forward(self, x):
        # x shape: (batch, channels, seq_len)
        x = self.conv(x)

        # å³å´ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’é™¤å»ï¼ˆæœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã‚ãªã„ï¼‰
        if self.padding != 0:
            x = x[:, :, :-self.padding]

        return x

class TemporalBlock(nn.Module):
    """
    TCNã®åŸºæœ¬ãƒ–ãƒ­ãƒƒã‚¯
    """
    def __init__(self, in_channels, out_channels, kernel_size, dilation, dropout=0.2):
        super(TemporalBlock, self).__init__()

        self.conv1 = CausalConv1d(in_channels, out_channels, kernel_size, dilation)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)

        self.conv2 = CausalConv1d(out_channels, out_channels, kernel_size, dilation)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)

        # Residual connection
        self.downsample = nn.Conv1d(in_channels, out_channels, 1) \
            if in_channels != out_channels else None

        self.relu = nn.ReLU()

    def forward(self, x):
        # Main path
        out = self.conv1(x)
        out = self.relu1(out)
        out = self.dropout1(out)

        out = self.conv2(out)
        out = self.relu2(out)
        out = self.dropout2(out)

        # Residual connection
        res = x if self.downsample is None else self.downsample(x)

        return self.relu(out + res)

class TCN(nn.Module):
    """
    Temporal Convolutional Network for time series forecasting
    """
    def __init__(self, input_size, output_size, num_channels, kernel_size=2, dropout=0.2):
        super(TCN, self).__init__()

        layers = []
        num_levels = len(num_channels)

        for i in range(num_levels):
            dilation_size = 2 ** i
            in_channels = input_size if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]

            layers.append(
                TemporalBlock(
                    in_channels,
                    out_channels,
                    kernel_size,
                    dilation_size,
                    dropout
                )
            )

        self.network = nn.Sequential(*layers)
        self.fc = nn.Linear(num_channels[-1], output_size)

    def forward(self, x):
        # x shape: (batch, seq_len, features)
        # Conv1dã¯ (batch, features, seq_len) ã‚’æœŸå¾…
        x = x.transpose(1, 2)

        # TCN forward pass
        y = self.network(x)

        # æœ€å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä½¿ç”¨
        y = y[:, :, -1]

        # å…¨çµåˆå±¤
        out = self.fc(y)

        return out

# TCNãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
tcn_model = TCN(
    input_size=1,
    output_size=horizon,
    num_channels=[32, 32, 64, 64],  # 4å±¤
    kernel_size=3,
    dropout=0.2
)

print("=== TCN ãƒ¢ãƒ‡ãƒ«æ§‹é€  ===")
print(tcn_model)
print(f"\nãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in tcn_model.parameters()):,}")
</code></pre>

<h3>TCN vs RNN/LSTM</h3>

<table>
<thead>
<tr>
<th>ç‰¹æ€§</th>
<th>RNN/LSTM</th>
<th>TCN</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ä¸¦åˆ—å‡¦ç†</strong></td>
<td>é€æ¬¡çš„ã€ä½é€Ÿ</td>
<td>ä¸¦åˆ—å¯èƒ½ã€é«˜é€Ÿ</td>
</tr>
<tr>
<td><strong>é•·æœŸä¾å­˜æ€§</strong></td>
<td>å‹¾é…æ¶ˆå¤±ã®å•é¡Œ</td>
<td>Dilationã§å¯¾å¿œ</td>
</tr>
<tr>
<td><strong>å—å®¹é‡</strong></td>
<td>ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã«ä¾å­˜</td>
<td>Dilation ã§åˆ¶å¾¡å¯èƒ½</td>
</tr>
<tr>
<td><strong>ãƒ¡ãƒ¢ãƒªåŠ¹ç‡</strong></td>
<td>éš ã‚ŒçŠ¶æ…‹ãŒå¿…è¦</td>
<td>ç•³ã¿è¾¼ã¿ã®ã¿</td>
</tr>
<tr>
<td><strong>è¨“ç·´æ™‚é–“</strong></td>
<td>é…ã„</td>
<td>é«˜é€Ÿ</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.4 Attention Mechanisms for æ™‚ç³»åˆ—</h2>

<h3>Self-Attention for Sequences</h3>

<p><strong>Attentionæ©Ÿæ§‹</strong>ã¯ã€å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é‡è¦ãªéƒ¨åˆ†ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ä»•çµ„ã¿ã§ã™ã€‚</p>

<p>Attention ã®è¨ˆç®—å¼ï¼š</p>

<p>$$
\begin{align*}
\text{Attention}(Q, K, V) &= \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \\
\text{where } Q &= XW_Q, \quad K = XW_K, \quad V = XW_V
\end{align*}
$$</p>

<ul>
<li>$Q$: Queryï¼ˆã‚¯ã‚¨ãƒªï¼‰</li>
<li>$K$: Keyï¼ˆã‚­ãƒ¼ï¼‰</li>
<li>$V$: Valueï¼ˆãƒãƒªãƒ¥ãƒ¼ï¼‰</li>
<li>$d_k$: ã‚­ãƒ¼ã®æ¬¡å…ƒ</li>
</ul>

<h3>PyTorchã§ã®Attentionå®Ÿè£…</h3>

<pre><code class="language-python">class AttentionLayer(nn.Module):
    """
    Self-Attention layer for time series
    """
    def __init__(self, hidden_size, num_heads=4):
        super(AttentionLayer, self).__init__()

        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads

        assert hidden_size % num_heads == 0, "hidden_size must be divisible by num_heads"

        # Query, Key, Value projections
        self.W_q = nn.Linear(hidden_size, hidden_size)
        self.W_k = nn.Linear(hidden_size, hidden_size)
        self.W_v = nn.Linear(hidden_size, hidden_size)

        # Output projection
        self.W_o = nn.Linear(hidden_size, hidden_size)

        self.dropout = nn.Dropout(0.1)

    def forward(self, x):
        # x shape: (batch, seq_len, hidden_size)
        batch_size = x.size(0)

        # Linear projections
        Q = self.W_q(x)  # (batch, seq_len, hidden_size)
        K = self.W_k(x)
        V = self.W_v(x)

        # Split into multiple heads
        # (batch, seq_len, num_heads, head_dim)
        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)

        # Scaled dot-product attention
        # scores shape: (batch, num_heads, seq_len, seq_len)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)

        # Apply softmax
        attn_weights = torch.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Apply attention to values
        # (batch, num_heads, seq_len, head_dim)
        attn_output = torch.matmul(attn_weights, V)

        # Concatenate heads
        # (batch, seq_len, hidden_size)
        attn_output = attn_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.hidden_size
        )

        # Final linear projection
        output = self.W_o(attn_output)

        return output, attn_weights

class LSTMWithAttention(nn.Module):
    """
    LSTM + Attention for time series forecasting
    """
    def __init__(self, input_size=1, hidden_size=64, num_layers=2,
                 output_size=1, num_heads=4, dropout=0.2):
        super(LSTMWithAttention, self).__init__()

        # LSTM encoder
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True
        )

        # Attention layer
        self.attention = AttentionLayer(hidden_size, num_heads)

        # Layer normalization
        self.layer_norm = nn.LayerNorm(hidden_size)

        # Output layer
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # LSTM encoding
        lstm_out, _ = self.lstm(x)

        # Apply attention
        attn_out, attn_weights = self.attention(lstm_out)

        # Residual connection + Layer norm
        out = self.layer_norm(lstm_out + attn_out)

        # Use last timestep
        out = self.fc(out[:, -1, :])

        return out, attn_weights

# Attentionä»˜ãLSTMãƒ¢ãƒ‡ãƒ«
attn_model = LSTMWithAttention(
    input_size=1,
    hidden_size=64,
    num_layers=2,
    output_size=horizon,
    num_heads=4,
    dropout=0.2
)

print("=== LSTM + Attention ãƒ¢ãƒ‡ãƒ«æ§‹é€  ===")
print(attn_model)
print(f"\nãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in attn_model.parameters()):,}")
</code></pre>

<h3>Attention Visualizationï¼ˆæ³¨æ„ã®å¯è¦–åŒ–ï¼‰</h3>

<pre><code class="language-python">def visualize_attention(model, data, window_size=20):
    """
    Attention weightsã‚’å¯è¦–åŒ–
    """
    model.eval()
    device = next(model.parameters()).device

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    x = torch.FloatTensor(data[:window_size]).unsqueeze(0).unsqueeze(-1).to(device)

    with torch.no_grad():
        _, attn_weights = model(x)

    # Attention weightsã®å–å¾—ï¼ˆæœ€åˆã®ãƒ˜ãƒƒãƒ‰ï¼‰
    attn = attn_weights[0, 0].cpu().numpy()

    # å¯è¦–åŒ–
    plt.figure(figsize=(12, 8))

    plt.subplot(2, 1, 1)
    plt.plot(data[:window_size], marker='o', alpha=0.7)
    plt.xlabel('Time Step')
    plt.ylabel('Value')
    plt.title('å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹', fontsize=14)
    plt.grid(True, alpha=0.3)

    plt.subplot(2, 1, 2)
    plt.imshow(attn, cmap='viridis', aspect='auto')
    plt.colorbar(label='Attention Weight')
    plt.xlabel('Key Position')
    plt.ylabel('Query Position')
    plt.title('Attention Weightsï¼ˆã©ã®ä½ç½®ã«æ³¨ç›®ã—ã¦ã„ã‚‹ã‹ï¼‰', fontsize=14)

    plt.tight_layout()
    plt.show()

# å¯è¦–åŒ–ã®å®Ÿè¡Œ
visualize_attention(attn_model, train_data, window_size=20)
</code></pre>

<h3>Seq2Seq with Attention</h3>

<p><strong>Encoder-Decoderæ§‹é€ </strong>ã‚’Attentionã§å¼·åŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«ã¯ã€è¤‡é›‘ãªæ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã§ãã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹] --> B[Encoder LSTM]
    B --> C[Context Vector]
    C --> D[Decoder LSTM]
    D --> E[Attention Layer]
    E --> D
    E --> F[äºˆæ¸¬ã‚·ãƒ¼ã‚±ãƒ³ã‚¹]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#ffe0b2
    style D fill:#ffccbc
    style E fill:#ffab91
    style F fill:#c8e6c9
</div>

<hr>

<h2>3.5 å®Ÿè·µçš„ãªãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</h2>

<h3>Feature Engineering for Deep Learning</h3>

<p>æ·±å±¤å­¦ç¿’ã§ã‚‚ã€é©åˆ‡ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã¯é‡è¦ã§ã™ã€‚</p>

<pre><code class="language-python">def create_time_features(data, timestamps=None):
    """
    æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ™‚é–“ãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡ã‚’ä½œæˆ
    """
    features = pd.DataFrame()

    if timestamps is not None:
        # æ™‚åˆ»ãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´
        features['hour'] = timestamps.hour / 24.0
        features['day_of_week'] = timestamps.dayofweek / 7.0
        features['day_of_month'] = timestamps.day / 31.0
        features['month'] = timestamps.month / 12.0
        features['is_weekend'] = (timestamps.dayofweek >= 5).astype(float)

    # ãƒ©ã‚°ç‰¹å¾´é‡
    for lag in [1, 2, 3, 7, 14]:
        features[f'lag_{lag}'] = pd.Series(data).shift(lag)

    # ç§»å‹•çµ±è¨ˆé‡
    for window in [3, 7, 14]:
        rolling = pd.Series(data).rolling(window)
        features[f'rolling_mean_{window}'] = rolling.mean()
        features[f'rolling_std_{window}'] = rolling.std()
        features[f'rolling_min_{window}'] = rolling.min()
        features[f'rolling_max_{window}'] = rolling.max()

    # å·®åˆ†ç‰¹å¾´é‡
    features['diff_1'] = pd.Series(data).diff(1)
    features['diff_7'] = pd.Series(data).diff(7)

    # æ¬ æå€¤ã‚’åŸ‹ã‚ã‚‹
    features = features.fillna(0)

    return features

# ç‰¹å¾´é‡ã®ä½œæˆä¾‹
timestamps = pd.date_range(start='2023-01-01', periods=len(train_data), freq='h')
time_features = create_time_features(train_data, timestamps)

print("=== ä½œæˆã•ã‚ŒãŸç‰¹å¾´é‡ ===")
print(time_features.head(20))
print(f"\nç‰¹å¾´é‡ã®æ•°: {time_features.shape[1]}")
</code></pre>

<h3>Ensemble Methodsï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ï¼‰</h3>

<p>è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€äºˆæ¸¬ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚</p>

<pre><code class="language-python">class EnsembleForecaster:
    """
    è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
    """
    def __init__(self, models, weights=None):
        self.models = models
        self.weights = weights if weights is not None else [1.0] * len(models)

    def predict(self, x):
        predictions = []

        for model in self.models:
            model.eval()
            with torch.no_grad():
                pred = model(x)
                predictions.append(pred)

        # é‡ã¿ä»˜ãå¹³å‡
        ensemble_pred = sum(w * p for w, p in zip(self.weights, predictions))
        ensemble_pred /= sum(self.weights)

        return ensemble_pred

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®ä½œæˆ
models = [model, gru_model, tcn_model]
ensemble = EnsembleForecaster(models, weights=[0.4, 0.3, 0.3])

print("=== ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ§‹æˆ ===")
print(f"ãƒ¢ãƒ‡ãƒ«æ•°: {len(models)}")
print(f"é‡ã¿: {ensemble.weights}")
</code></pre>

<h3>Transfer Learning for Time Series</h3>

<p>äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã«é©ç”¨ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">def transfer_learning(pretrained_model, new_output_size, freeze_layers=True):
    """
    è»¢ç§»å­¦ç¿’ã®è¨­å®š
    """
    # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒ”ãƒ¼
    model = pretrained_model

    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼éƒ¨åˆ†ã‚’å‡çµ
    if freeze_layers:
        for param in model.lstm.parameters():
            param.requires_grad = False

    # å‡ºåŠ›å±¤ã‚’æ–°ã—ã„ã‚¿ã‚¹ã‚¯ç”¨ã«å†åˆæœŸåŒ–
    model.fc = nn.Linear(model.hidden_size, new_output_size)

    return model

# ä½¿ç”¨ä¾‹
# pretrained_model ã‚’åˆ¥ã®ã‚¿ã‚¹ã‚¯ï¼ˆhorizon=10ï¼‰ã«é©ç”¨
new_model = transfer_learning(model, new_output_size=10, freeze_layers=True)

print("=== è»¢ç§»å­¦ç¿’ãƒ¢ãƒ‡ãƒ« ===")
print(f"å‡çµã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in new_model.parameters() if not p.requires_grad):,}")
print(f"å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in new_model.parameters() if p.requires_grad):,}")
</code></pre>

<h3>Hyperparameter Tuning</h3>

<p>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¢ç´¢ã§æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’è¦‹ã¤ã‘ã¾ã™ã€‚</p>

<pre><code class="language-python">from itertools import product

def grid_search(param_grid, train_loader, val_loader, epochs=20):
    """
    ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
    """
    best_loss = float('inf')
    best_params = None
    results = []

    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµ„ã¿åˆã‚ã›ã‚’ç”Ÿæˆ
    keys = param_grid.keys()
    values = param_grid.values()

    for params in product(*values):
        param_dict = dict(zip(keys, params))

        print(f"\nè©¦è¡Œä¸­: {param_dict}")

        # ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
        model = LSTMForecaster(
            input_size=1,
            hidden_size=param_dict['hidden_size'],
            num_layers=param_dict['num_layers'],
            output_size=horizon,
            dropout=param_dict['dropout']
        )

        # è¨“ç·´
        _, val_losses = train_model(
            model, train_loader, val_loader,
            epochs=epochs, lr=param_dict['lr']
        )

        # æœ€è‰¯ã®æ¤œè¨¼æå¤±
        min_val_loss = min(val_losses)
        results.append((param_dict, min_val_loss))

        if min_val_loss < best_loss:
            best_loss = min_val_loss
            best_params = param_dict

    return best_params, best_loss, results

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰
param_grid = {
    'hidden_size': [32, 64, 128],
    'num_layers': [1, 2, 3],
    'dropout': [0.1, 0.2, 0.3],
    'lr': [0.001, 0.0001]
}

# ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã®å®Ÿè¡Œï¼ˆå°è¦æ¨¡ãªä¾‹ï¼‰
small_grid = {
    'hidden_size': [32, 64],
    'num_layers': [1, 2],
    'dropout': [0.2],
    'lr': [0.001]
}

best_params, best_loss, all_results = grid_search(
    small_grid, train_loader, val_loader, epochs=10
)

print("\n=== æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===")
print(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {best_params}")
print(f"æ¤œè¨¼æå¤±: {best_loss:.4f}")
</code></pre>

<hr>

<h2>3.6 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>æ·±å±¤å­¦ç¿’ã®åŸºæœ¬ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</strong></p>
<ul>
<li>ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™</li>
<li>PyTorch Datasetã¨DataLoaderã®æ´»ç”¨</li>
<li>One-shot vs Autoregressiveäºˆæ¸¬</li>
</ul></li>

<li><p><strong>LSTM & GRU</strong></p>
<ul>
<li>é•·æœŸä¾å­˜æ€§ã®å­¦ç¿’</li>
<li>LSTMã¨GRUã®é•ã„ã¨ä½¿ã„åˆ†ã‘</li>
<li>Stateful vs Stateless</li>
</ul></li>

<li><p><strong>TCNï¼ˆTemporal Convolutional Networkï¼‰</strong></p>
<ul>
<li>Dilated Convolutionã«ã‚ˆã‚‹åºƒã„å—å®¹é‡</li>
<li>Causal Convolutionã§æœªæ¥ã‚’å‚ç…§ã—ãªã„</li>
<li>RNNã‚ˆã‚Šé«˜é€Ÿãªè¨“ç·´</li>
</ul></li>

<li><p><strong>Attentionæ©Ÿæ§‹</strong></p>
<ul>
<li>Self-Attentionã§é‡è¦ãªæ™‚ç‚¹ã«ç„¦ç‚¹</li>
<li>LSTM + Attentionã®çµ„ã¿åˆã‚ã›</li>
<li>Attention weightsã®å¯è¦–åŒ–</li>
</ul></li>

<li><p><strong>å®Ÿè·µãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</strong></p>
<ul>
<li>æ™‚é–“ç‰¹å¾´é‡ã¨ãƒ©ã‚°ç‰¹å¾´é‡ã®ä½œæˆ</li>
<li>ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬</li>
<li>è»¢ç§»å­¦ç¿’</li>
<li>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</li>
</ul></li>
</ol>

<h3>ãƒ¢ãƒ‡ãƒ«é¸æŠã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³</h3>

<table>
<thead>
<tr>
<th>çŠ¶æ³</th>
<th>æ¨å¥¨ãƒ¢ãƒ‡ãƒ«</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td>çŸ­æœŸäºˆæ¸¬ï¼ˆ< 10ã‚¹ãƒ†ãƒƒãƒ—ï¼‰</td>
<td>LSTM/GRU</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ã§åŠ¹æœçš„</td>
</tr>
<tr>
<td>é•·æœŸäºˆæ¸¬</td>
<td>TCNã€Attention</td>
<td>åºƒã„å—å®¹é‡ã€ä¸¦åˆ—å‡¦ç†</td>
</tr>
<tr>
<td>è¨“ç·´é€Ÿåº¦é‡è¦–</td>
<td>TCN</td>
<td>ä¸¦åˆ—å‡¦ç†å¯èƒ½</td>
</tr>
<tr>
<td>è§£é‡ˆæ€§é‡è¦–</td>
<td>Attentionä»˜ããƒ¢ãƒ‡ãƒ«</td>
<td>é‡è¦ãªæ™‚ç‚¹ã‚’å¯è¦–åŒ–</td>
</tr>
<tr>
<td>å¤šå¤‰é‡æ™‚ç³»åˆ—</td>
<td>Transformer</td>
<td>è¤‡é›‘ãªä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’</td>
</tr>
<tr>
<td>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬</td>
<td>Stateful LSTM/GRU</td>
<td>éš ã‚ŒçŠ¶æ…‹ã‚’ä¿æŒ</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ç« ã¸</h3>

<p>æ¬¡ã®ç« ã§ã¯ã€ã•ã‚‰ã«é«˜åº¦ãªãƒˆãƒ”ãƒƒã‚¯ã‚’æ‰±ã„ã¾ã™ï¼ˆåˆ¥ç« ã¨ã—ã¦å±•é–‹ï¼‰ï¼š</p>
<ul>
<li>Transformer for Time Series</li>
<li>ç•°å¸¸æ¤œçŸ¥</li>
<li>å¤šå¤‰é‡æ™‚ç³»åˆ—äºˆæ¸¬</li>
<li>ç¢ºç‡çš„äºˆæ¸¬ï¼ˆäºˆæ¸¬åŒºé–“ã®æ¨å®šï¼‰</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>
<p>LSTM ã¨ GRU ã®é•ã„ã‚’ã€ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>LSTMï¼ˆLong Short-Term Memoryï¼‰</strong>ï¼š</p>
<ul>
<li>ã‚²ãƒ¼ãƒˆ: 3ã¤ï¼ˆå¿˜å´ã‚²ãƒ¼ãƒˆã€å…¥åŠ›ã‚²ãƒ¼ãƒˆã€å‡ºåŠ›ã‚²ãƒ¼ãƒˆï¼‰</li>
<li>ã‚»ãƒ«çŠ¶æ…‹ã¨éš ã‚ŒçŠ¶æ…‹ã®2ã¤ã‚’ä¿æŒ</li>
<li>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: ã‚ˆã‚Šå¤šã„</li>
</ul>

<p><strong>GRUï¼ˆGated Recurrent Unitï¼‰</strong>ï¼š</p>
<ul>
<li>ã‚²ãƒ¼ãƒˆ: 2ã¤ï¼ˆãƒªã‚»ãƒƒãƒˆã‚²ãƒ¼ãƒˆã€æ›´æ–°ã‚²ãƒ¼ãƒˆï¼‰</li>
<li>éš ã‚ŒçŠ¶æ…‹ã®ã¿ã‚’ä¿æŒ</li>
<li>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: LSTMã®ç´„75%</li>
</ul>

<p><strong>ä½¿ã„åˆ†ã‘</strong>ï¼š</p>
<ul>
<li>LSTM: ã‚ˆã‚Šè¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã€é•·æœŸä¾å­˜æ€§ãŒå¼·ã„å ´åˆ</li>
<li>GRU: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›ã€è¨“ç·´é€Ÿåº¦é‡è¦–ã€ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å ´åˆ</li>
</ul>

<p>å®Ÿéš›ã®æ€§èƒ½å·®ã¯å°ã•ã„ã“ã¨ãŒå¤šãã€ã‚¿ã‚¹ã‚¯ã”ã¨ã«æ¤œè¨¼ãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚</p>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>Dilated Convolution ã®å—å®¹é‡ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®è¨­å®šã§ã€æœ€çµ‚å±¤ã®å—å®¹é‡ã¯ã„ãã¤ã«ãªã‚Šã¾ã™ã‹ï¼Ÿ</p>

<ul>
<li>ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º: k = 3</li>
<li>ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: 4å±¤</li>
<li>Dilation: [1, 2, 4, 8]</li>
</ul>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p>å—å®¹é‡ã®è¨ˆç®—å¼ï¼š</p>

<p>$$
\text{RF} = 1 + \sum_{i=1}^{L} (k - 1) \times d_i
$$</p>

<ul>
<li>$k = 3$: ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º</li>
<li>$L = 4$: ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°</li>
<li>$d_i$: å„å±¤ã®dilation</li>
</ul>

<p>è¨ˆç®—ï¼š</p>

<pre><code class="language-python">k = 3
dilations = [1, 2, 4, 8]

receptive_field = 1
for d in dilations:
    receptive_field += (k - 1) * d

print(f"å—å®¹é‡: {receptive_field}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>å—å®¹é‡: 31
</code></pre>

<p>ã¤ã¾ã‚Šã€31å€‹ã®éå»ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’è€ƒæ…®ã§ãã¾ã™ã€‚</p>

<p>å„å±¤ã§ã®å—å®¹é‡ã®å¢—åŠ ï¼š</p>
<ul>
<li>Layer 1 (d=1): RF = 1 + 2Ã—1 = 3</li>
<li>Layer 2 (d=2): RF = 3 + 2Ã—2 = 7</li>
<li>Layer 3 (d=4): RF = 7 + 2Ã—4 = 15</li>
<li>Layer 4 (d=8): RF = 15 + 2Ã—8 = 31</li>
</ul>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>Attentionæ©Ÿæ§‹ã®åˆ©ç‚¹ã‚’3ã¤æŒ™ã’ã€ãã‚Œãã‚Œã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<ol>
<li><p><strong>é‡è¦ãªæ™‚ç‚¹ã¸ã®ç„¦ç‚¹</strong></p>
<ul>
<li>èª¬æ˜: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®é–¢é€£æ€§ã®é«˜ã„éƒ¨åˆ†ã«è‡ªå‹•çš„ã«æ³¨ç›®</li>
<li>åˆ©ç‚¹: ãƒã‚¤ã‚ºã®å¤šã„ãƒ‡ãƒ¼ã‚¿ã§ã‚‚é‡è¦ãªæƒ…å ±ã‚’æŠ½å‡º</li>
<li>ä¾‹: å­£ç¯€æ€§ã®ãƒ”ãƒ¼ã‚¯ã‚„ç•°å¸¸ãªã‚¤ãƒ™ãƒ³ãƒˆã«æ³¨ç›®</li>
</ul></li>

<li><p><strong>é•·è·é›¢ä¾å­˜æ€§ã®å­¦ç¿’</strong></p>
<ul>
<li>èª¬æ˜: ç›´æ¥çš„ãªæ¥ç¶šã«ã‚ˆã‚Šã€é ã„æ™‚ç‚¹ã¨ã®é–¢ä¿‚ã‚‚å­¦ç¿’å¯èƒ½</li>
<li>åˆ©ç‚¹: RNNã®å‹¾é…æ¶ˆå¤±å•é¡Œã‚’å›é¿</li>
<li>ä¾‹: å¹´æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨æ—¥æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é–¢é€£æ€§ã‚’å­¦ç¿’</li>
</ul></li>

<li><p><strong>è§£é‡ˆå¯èƒ½æ€§</strong></p>
<ul>
<li>èª¬æ˜: Attention weightsã‚’å¯è¦–åŒ–ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®åˆ¤æ–­æ ¹æ‹ ãŒåˆ†ã‹ã‚‹</li>
<li>åˆ©ç‚¹: ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã§ã¯ãªãã€äºˆæ¸¬ã®ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>ä¾‹: ã€Œã“ã®äºˆæ¸¬ã¯1é€±é–“å‰ã¨2ã‹æœˆå‰ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãã€ã¨èª¬æ˜å¯èƒ½</li>
</ul></li>
</ol>

<p><strong>è¿½åŠ ã®åˆ©ç‚¹</strong>ï¼š</p>
<ul>
<li>ä¸¦åˆ—å‡¦ç†ãŒå¯èƒ½ï¼ˆSelf-Attentionã®å ´åˆï¼‰</li>
<li>å¯å¤‰é•·ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«æŸ”è»Ÿã«å¯¾å¿œ</li>
<li>å¤šæ§˜ãªã‚¿ã‚¹ã‚¯ã«é©ç”¨å¯èƒ½</li>
</ul>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>ä»¥ä¸‹ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ã€LSTM ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€äºˆæ¸¬ç²¾åº¦ã‚’ARIMAã¨æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import numpy as np

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæœˆæ¬¡å£²ä¸Šãƒ‡ãƒ¼ã‚¿ï¼‰
np.random.seed(42)
time = np.arange(0, 100)
trend = 0.5 * time
seasonality = 10 * np.sin(2 * np.pi * time / 12)
noise = np.random.randn(100) * 2
data = trend + seasonality + noise + 50
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
time = np.arange(0, 100)
trend = 0.5 * time
seasonality = 10 * np.sin(2 * np.pi * time / 12)
noise = np.random.randn(100) * 2
data = trend + seasonality + noise + 50

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
train_size = 80
train_data = data[:train_size]
test_data = data[train_size:]

# Datasetå®šç¾©ï¼ˆå‰è¿°ã®TimeSeriesDatasetã‚’ä½¿ç”¨ï¼‰
window_size = 12
horizon = 1

train_dataset = TimeSeriesDataset(train_data, window_size, horizon)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

# LSTMãƒ¢ãƒ‡ãƒ«ï¼ˆå‰è¿°ã®LSTMForecasterã‚’ä½¿ç”¨ï¼‰
model = LSTMForecaster(
    input_size=1,
    hidden_size=32,
    num_layers=2,
    output_size=horizon,
    dropout=0.1
)

# è¨“ç·´
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

epochs = 100
for epoch in range(epochs):
    model.train()
    epoch_loss = 0

    for x_batch, y_batch in train_loader:
        x_batch = x_batch.unsqueeze(-1).to(device)
        y_batch = y_batch.to(device)

        optimizer.zero_grad()
        outputs = model(x_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    if (epoch + 1) % 20 == 0:
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(train_loader):.4f}")

# LSTMäºˆæ¸¬
model.eval()
lstm_predictions = []

with torch.no_grad():
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§é€æ¬¡äºˆæ¸¬
    for i in range(len(test_data)):
        # ç›´è¿‘ã®window_sizeãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        if i == 0:
            input_seq = train_data[-window_size:]
        else:
            input_seq = np.concatenate([
                train_data[-(window_size-i):],
                test_data[:i]
            ])[-window_size:]

        x = torch.FloatTensor(input_seq).unsqueeze(0).unsqueeze(-1).to(device)
        pred = model(x).cpu().numpy()[0, 0]
        lstm_predictions.append(pred)

lstm_predictions = np.array(lstm_predictions)

# ARIMAã¨ã®æ¯”è¼ƒ
from statsmodels.tsa.arima.model import ARIMA

# ARIMAãƒ¢ãƒ‡ãƒ«
arima_model = ARIMA(train_data, order=(2, 1, 2))
arima_fitted = arima_model.fit()
arima_predictions = arima_fitted.forecast(steps=len(test_data))

# è©•ä¾¡
lstm_mse = mean_squared_error(test_data, lstm_predictions)
lstm_mae = mean_absolute_error(test_data, lstm_predictions)

arima_mse = mean_squared_error(test_data, arima_predictions)
arima_mae = mean_absolute_error(test_data, arima_predictions)

print("\n=== äºˆæ¸¬ç²¾åº¦ã®æ¯”è¼ƒ ===")
print(f"LSTM  - MSE: {lstm_mse:.3f}, MAE: {lstm_mae:.3f}")
print(f"ARIMA - MSE: {arima_mse:.3f}, MAE: {arima_mae:.3f}")

# å¯è¦–åŒ–
plt.figure(figsize=(14, 6))
plt.plot(time, data, label='å…ƒãƒ‡ãƒ¼ã‚¿', alpha=0.7)
plt.axvline(x=train_size, color='red', linestyle='--', label='è¨“ç·´/ãƒ†ã‚¹ãƒˆå¢ƒç•Œ')
plt.plot(time[train_size:], lstm_predictions, label='LSTMäºˆæ¸¬', marker='o')
plt.plot(time[train_size:], arima_predictions, label='ARIMAäºˆæ¸¬', marker='s')
plt.xlabel('Time')
plt.ylabel('Value')
plt.title('LSTM vs ARIMA äºˆæ¸¬æ¯”è¼ƒ', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== äºˆæ¸¬ç²¾åº¦ã®æ¯”è¼ƒ ===
LSTM  - MSE: 3.245, MAE: 1.432
ARIMA - MSE: 4.187, MAE: 1.678
</code></pre>

<p><strong>çµè«–</strong>ï¼š</p>
<ul>
<li>LSTMã¯éç·šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚ˆã‚Šè‰¯ãå­¦ç¿’</li>
<li>ARIMAã¯ç·šå½¢ãƒˆãƒ¬ãƒ³ãƒ‰ã¨å­£ç¯€æ€§ã«å¼·ã„</li>
<li>ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã«å¿œã˜ã¦é¸æŠãŒå¿…è¦</li>
</ul>

</details>

<h3>å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>Causal Convolution ãŒæœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‚ç…§ã—ãªã„ä»•çµ„ã¿ã‚’ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€ãªãœã“ã‚ŒãŒæ™‚ç³»åˆ—äºˆæ¸¬ã§é‡è¦ãªã®ã‹ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>Causal Convolution ã®ä»•çµ„ã¿</strong>ï¼š</p>

<ol>
<li><p><strong>é€šå¸¸ã®ç•³ã¿è¾¼ã¿ï¼ˆNon-Causalï¼‰</strong></p>
<ul>
<li>ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°: ä¸¡å´ï¼ˆå·¦å³ï¼‰ã«è¿½åŠ </li>
<li>å•é¡Œ: æœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚‚å‚ç…§ã—ã¦ã—ã¾ã†</li>
<li>ä¾‹: ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º3ã®å ´åˆã€ä½ç½®tã§ t-1, t, t+1 ã‚’å‚ç…§</li>
</ul></li>

<li><p><strong>Causal Convolution</strong></p>
<ul>
<li>ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°: å·¦å´ï¼ˆéå»å´ï¼‰ã®ã¿ã«è¿½åŠ </li>
<li>åˆ©ç‚¹: ä½ç½®tã§ã¯ t-2, t-1, t ã®ã¿ã‚’å‚ç…§ï¼ˆæœªæ¥ã¯è¦‹ãªã„ï¼‰</li>
<li>å®Ÿè£…: ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å¾Œã€å³å´ã‚’åˆ‡ã‚Šå–ã‚‹</li>
</ul></li>
</ol>

<p><strong>å®Ÿè£…ä¾‹</strong>ï¼š</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# é€šå¸¸ã®ç•³ã¿è¾¼ã¿ï¼ˆNon-Causalï¼‰
normal_conv = nn.Conv1d(1, 1, kernel_size=3, padding=1)

# Causal Convolution
class CausalConv1d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size):
        super().__init__()
        self.padding = kernel_size - 1
        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size,
                             padding=self.padding)

    def forward(self, x):
        x = self.conv(x)
        # å³å´ï¼ˆæœªæ¥å´ï¼‰ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’é™¤å»
        if self.padding != 0:
            x = x[:, :, :-self.padding]
        return x

causal_conv = CausalConv1d(1, 1, kernel_size=3)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
x = torch.randn(1, 1, 10)  # (batch, channels, seq_len)

print("å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·:", x.shape[2])
print("é€šå¸¸ã®ç•³ã¿è¾¼ã¿å‡ºåŠ›é•·:", normal_conv(x).shape[2])
print("Causalç•³ã¿è¾¼ã¿å‡ºåŠ›é•·:", causal_conv(x).shape[2])
</code></pre>

<p><strong>ãªãœé‡è¦ã‹</strong>ï¼š</p>

<ol>
<li><p><strong>ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®é˜²æ­¢</strong></p>
<ul>
<li>è¨“ç·´æ™‚ã«æœªæ¥ã®æƒ…å ±ã‚’ä½¿ã†ã¨ã€éå¤§è©•ä¾¡ã•ã‚Œã‚‹</li>
<li>å®Ÿé‹ç”¨ã§ã¯æœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ã¯åˆ©ç”¨ä¸å¯</li>
<li>Causalã«ã™ã‚‹ã“ã¨ã§ã€è¨“ç·´ã¨æ¨è«–ã®æ¡ä»¶ã‚’ä¸€è‡´ã•ã›ã‚‹</li>
</ul></li>

<li><p><strong>å…¬å¹³ãªè©•ä¾¡</strong></p>
<ul>
<li>æ™‚ç³»åˆ—ã®é †åºã‚’å°Šé‡</li>
<li>ãƒ¢ãƒ‡ãƒ«ã®çœŸã®äºˆæ¸¬èƒ½åŠ›ã‚’è©•ä¾¡</li>
<li>éå­¦ç¿’ã®æ¤œå‡ºãŒæ­£ç¢º</li>
</ul></li>

<li><p><strong>å®Ÿç”¨æ€§</strong></p>
<ul>
<li>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ã«ç›´æ¥é©ç”¨å¯èƒ½</li>
<li>ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ãŒå¯èƒ½</li>
<li>ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œ</li>
</ul></li>
</ol>

<p><strong>å›³è§£</strong>ï¼š</p>

<pre><code>é€šå¸¸ã®ç•³ã¿è¾¼ã¿ï¼ˆNon-Causalï¼‰:
æ™‚åˆ»t ã§ã®è¨ˆç®—: [t-1, t, t+1] â†’ NGï¼ˆæœªæ¥ã‚’è¦‹ã¦ã„ã‚‹ï¼‰

Causal Convolution:
æ™‚åˆ»t ã§ã®è¨ˆç®—: [t-2, t-1, t] â†’ OKï¼ˆéå»ã®ã¿ï¼‰
</code></pre>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Hochreiter, S., & Schmidhuber, J. (1997). <em>Long Short-Term Memory</em>. Neural Computation, 9(8), 1735-1780.</li>
<li>Cho, K., et al. (2014). <em>Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</em>. EMNLP.</li>
<li>Bai, S., Kolter, J. Z., & Koltun, V. (2018). <em>An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling</em>. arXiv:1803.01271.</li>
<li>Vaswani, A., et al. (2017). <em>Attention Is All You Need</em>. NeurIPS.</li>
<li>Lim, B., & Zohren, S. (2021). <em>Time-series forecasting with deep learning: a survey</em>. Philosophical Transactions of the Royal Society A, 379(2194).</li>
<li>Hewamalage, H., Bergmeir, C., & Bandara, K. (2021). <em>Recurrent Neural Networks for Time Series Forecasting: Current Status and Future Directions</em>. International Journal of Forecasting, 37(1), 388-427.</li>
</ol>

<div class="navigation">
    <a href="chapter2-arima-models.html" class="nav-button">â† å‰ã®ç« : ARIMAãƒ¢ãƒ‡ãƒ«</a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
