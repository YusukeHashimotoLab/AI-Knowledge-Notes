<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬1ç« ï¼šMLOpsåŸºç¤ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬1ç« ï¼šMLOpsåŸºç¤</h1>
            <p class="subtitle">æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®é‹ç”¨ã‚’æ”¯ãˆã‚‹åŸºç›¤æŠ€è¡“</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 10å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… MLOpsã®å®šç¾©ã¨å¿…è¦æ€§ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«å…¨ä½“ã‚’æŠŠæ¡ã™ã‚‹</li>
<li>âœ… MLOpsã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… MLOpsãƒ„ãƒ¼ãƒ«ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã®æ¦‚è¦ã‚’çŸ¥ã‚‹</li>
<li>âœ… MLOpsæˆç†Ÿåº¦ãƒ¢ãƒ‡ãƒ«ã‚’ç†è§£ã—ã€ç¾çŠ¶ã‚’è©•ä¾¡ã§ãã‚‹</li>
<li>âœ… å®Ÿè·µçš„ãªMLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åŸºç¤ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
</ul>

<hr>

<h2>1.1 MLOpsã¨ã¯</h2>

<h3>æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®èª²é¡Œ</h3>

<p>æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å¤šãã¯ã€PoCï¼ˆæ¦‚å¿µå®Ÿè¨¼ï¼‰æ®µéšã§çµ‚ã‚ã‚Šã€æœ¬ç•ªç’°å¢ƒã¸ã®å±•é–‹ã«å¤±æ•—ã—ã¾ã™ã€‚ãã®ä¸»ãªç†ç”±ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š</p>

<table>
<thead>
<tr>
<th>èª²é¡Œ</th>
<th>èª¬æ˜</th>
<th>å½±éŸ¿</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ¢ãƒ‡ãƒ«ã¨ã‚³ãƒ¼ãƒ‰ã®ä¹–é›¢</strong></td>
<td>Jupyter Notebookã®ã‚³ãƒ¼ãƒ‰ãŒæœ¬ç•ªç’°å¢ƒã§å‹•ä½œã—ãªã„</td>
<td>ãƒ‡ãƒ—ãƒ­ã‚¤ã®é…å»¶ã€æ‰‹ä½œæ¥­ã®å¢—åŠ </td>
</tr>
<tr>
<td><strong>å†ç¾æ€§ã®æ¬ å¦‚</strong></td>
<td>åŒã˜çµæœã‚’å†ç¾ã§ããªã„ï¼ˆãƒ‡ãƒ¼ã‚¿ã€ã‚³ãƒ¼ãƒ‰ã€ç’°å¢ƒã®ä¸ä¸€è‡´ï¼‰</td>
<td>ãƒ‡ãƒãƒƒã‚°å›°é›£ã€å“è³ªä½ä¸‹</td>
</tr>
<tr>
<td><strong>ãƒ¢ãƒ‡ãƒ«ã®åŠ£åŒ–</strong></td>
<td>æ™‚é–“çµŒéã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®ä½ä¸‹</td>
<td>äºˆæ¸¬ç²¾åº¦ã®æ‚ªåŒ–ã€ãƒ“ã‚¸ãƒã‚¹æå¤±</td>
</tr>
<tr>
<td><strong>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£</strong></td>
<td>å¤§é‡ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å¯¾å¿œã§ããªã„</td>
<td>ã‚·ã‚¹ãƒ†ãƒ åœæ­¢ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹é…å»¶</td>
</tr>
<tr>
<td><strong>ã‚¬ãƒãƒŠãƒ³ã‚¹ã®æ¬ å¦‚</strong></td>
<td>èª°ãŒã„ã¤ã©ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ãŸã‹ä¸æ˜</td>
<td>ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹é•åã€ç›£æŸ»ä¸å¯</td>
</tr>
</tbody>
</table>

<blockquote>
<p><strong>çµ±è¨ˆ</strong>: Gartnerã®èª¿æŸ»ã«ã‚ˆã‚‹ã¨ã€æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç´„85%ãŒæœ¬ç•ªç’°å¢ƒã«åˆ°é”ã—ãªã„ã¨å ±å‘Šã•ã‚Œã¦ã„ã¾ã™ã€‚</p>
</blockquote>

<h3>MLOpsã®å®šç¾©ã¨ç›®çš„</h3>

<p><strong>MLOpsï¼ˆMachine Learning Operationsï¼‰</strong>ã¯ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ»é‹ç”¨ã‚’è‡ªå‹•åŒ–ãƒ»æ¨™æº–åŒ–ã™ã‚‹ãŸã‚ã®å®Ÿè·µæ‰‹æ³•ã¨ãƒ„ãƒ¼ãƒ«ç¾¤ã§ã™ã€‚</p>

<p><strong>MLOpsã®ç›®çš„</strong>ï¼š</p>
<ul>
<li><strong>è¿…é€Ÿãªãƒ‡ãƒ—ãƒ­ã‚¤</strong>: ãƒ¢ãƒ‡ãƒ«ã‚’è¿…é€Ÿã‹ã¤ç¢ºå®Ÿã«æœ¬ç•ªç’°å¢ƒã¸å±•é–‹</li>
<li><strong>å†ç¾æ€§</strong>: å®Ÿé¨“ã¨ãƒ¢ãƒ‡ãƒ«ã®å®Œå…¨ãªå†ç¾ã‚’ä¿è¨¼</li>
<li><strong>è‡ªå‹•åŒ–</strong>: æ‰‹ä½œæ¥­ã‚’å‰Šæ¸›ã—ã€ã‚¨ãƒ©ãƒ¼ã‚’æœ€å°åŒ–</li>
<li><strong>ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°</strong>: ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®ç¶™ç¶šçš„ãªç›£è¦–ã¨æ”¹å–„</li>
<li><strong>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£</strong>: å¤šæ•°ã®ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œ</li>
<li><strong>ã‚¬ãƒãƒŠãƒ³ã‚¹</strong>: ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ã¨ç›£æŸ»å¯¾å¿œ</li>
</ul>

<h3>DevOps/DataOpsã¨ã®é–¢ä¿‚</h3>

<p>MLOpsã¯ã€DevOpsã¨DataOpsã®åŸå‰‡ã‚’æ©Ÿæ¢°å­¦ç¿’ã«é©ç”¨ã—ãŸã‚‚ã®ã§ã™ï¼š</p>

<table>
<thead>
<tr>
<th>æ¦‚å¿µ</th>
<th>ç„¦ç‚¹</th>
<th>ä¸»ãªå®Ÿè·µ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>DevOps</strong></td>
<td>ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã¨é‹ç”¨</td>
<td>CI/CDã€ã‚¤ãƒ³ãƒ•ãƒ©è‡ªå‹•åŒ–ã€ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°</td>
</tr>
<tr>
<td><strong>DataOps</strong></td>
<td>ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨å“è³ª</td>
<td>ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã€å“è³ªãƒã‚§ãƒƒã‚¯ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†</td>
</tr>
<tr>
<td><strong>MLOps</strong></td>
<td>æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«</td>
<td>å®Ÿé¨“ç®¡ç†ã€ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã€è‡ªå‹•å†è¨“ç·´</td>
</tr>
</tbody>
</table>

<div class="mermaid">
graph LR
    A[DevOps] --> D[MLOps]
    B[DataOps] --> D
    C[Machine Learning] --> D

    D --> E[è‡ªå‹•åŒ–ã•ã‚ŒãŸML<br/>ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«]

    style A fill:#e3f2fd
    style B fill:#f3e5f5
    style C fill:#fff3e0
    style D fill:#c8e6c9
    style E fill:#ffccbc
</div>

<h3>MLOpsãŒè§£æ±ºã™ã‚‹å•é¡Œã®å®Ÿä¾‹</h3>

<pre><code class="language-python">"""
å•é¡Œ: Jupyter Notebookã§é–‹ç™ºã—ãŸãƒ¢ãƒ‡ãƒ«ãŒæœ¬ç•ªç’°å¢ƒã§å‹•ä½œã—ãªã„

åŸå› :
- é–‹ç™ºç’°å¢ƒã¨æœ¬ç•ªç’°å¢ƒã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒç•°ãªã‚‹
- ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ãŒæ–‡æ›¸åŒ–ã•ã‚Œã¦ã„ãªã„
- ãƒ¢ãƒ‡ãƒ«ã®ä¾å­˜é–¢ä¿‚ãŒä¸æ˜ç¢º
"""

# âŒ å•é¡Œã®ã‚ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆå†ç¾æ€§ãªã—ï¼‰
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆã©ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼Ÿã„ã¤ï¼Ÿï¼‰
df = pd.read_csv('data.csv')

# å‰å‡¦ç†ï¼ˆã‚¹ãƒ†ãƒƒãƒ—ãŒä¸æ˜ç¢ºï¼‰
df = df.dropna()
X = df.drop('target', axis=1)
y = df['target']

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨˜éŒ²ãªã—ï¼‰
model = RandomForestClassifier()
model.fit(X, y)

# ä¿å­˜ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰
import pickle
pickle.dump(model, open('model.pkl', 'wb'))
</code></pre>

<pre><code class="language-python">"""
âœ… MLOpsã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆå†ç¾æ€§ã‚ã‚Šï¼‰

ç‰¹å¾´:
- ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ï¼ˆã‚³ãƒ¼ãƒ‰ã€ãƒ‡ãƒ¼ã‚¿ã€ãƒ¢ãƒ‡ãƒ«ï¼‰
- ç’°å¢ƒã®æ˜ç¤ºï¼ˆrequirements.txt, Dockerï¼‰
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¨˜éŒ²ï¼ˆå®Ÿé¨“çµæœã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ï¼ˆå†ç¾å¯èƒ½ãªå‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼‰
"""

import mlflow
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import json
from datetime import datetime

# MLflowå®Ÿé¨“ã®é–‹å§‹
mlflow.set_experiment("customer_churn_prediction")

with mlflow.start_run():
    # 1. ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®è¨˜éŒ²
    data_version = "v1.2.3"
    mlflow.log_param("data_version", data_version)

    # 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ï¼‰
    df = pd.read_csv(f'data/{data_version}/data.csv')

    # 3. å‰å‡¦ç†ï¼ˆæ˜ç¤ºçš„ãªã‚¹ãƒ†ãƒƒãƒ—ï¼‰
    df = df.dropna()
    X = df.drop('target', axis=1)
    y = df['target']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # 4. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨˜éŒ²
    params = {
        'n_estimators': 100,
        'max_depth': 10,
        'random_state': 42
    }
    mlflow.log_params(params)

    # 5. ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    model = RandomForestClassifier(**params)
    model.fit(X_train, y_train)

    # 6. è©•ä¾¡ã¨è¨˜éŒ²
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    mlflow.log_metric("accuracy", accuracy)

    # 7. ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãï¼‰
    mlflow.sklearn.log_model(
        model,
        "model",
        registered_model_name="churn_predictor"
    )

    # 8. è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    mlflow.set_tag("model_type", "RandomForest")
    mlflow.set_tag("created_by", "data_science_team")
    mlflow.set_tag("timestamp", datetime.now().isoformat())

    print(f"âœ“ ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº† - ç²¾åº¦: {accuracy:.3f}")
    print(f"âœ“ å®Ÿé¨“ID: {mlflow.active_run().info.run_id}")
</code></pre>

<hr>

<h2>1.2 MLãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«</h2>

<h3>æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å…¨ä½“åƒ</h3>

<p>æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€ä»¥ä¸‹ã®ãƒ•ã‚§ãƒ¼ã‚ºã§æ§‹æˆã•ã‚Œã‚‹åå¾©çš„ãªãƒ—ãƒ­ã‚»ã‚¹ã§ã™ï¼š</p>

<div class="mermaid">
graph TB
    A[ãƒ“ã‚¸ãƒã‚¹ç†è§£] --> B[ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»æº–å‚™]
    B --> C[ãƒ¢ãƒ‡ãƒ«é–‹ç™ºãƒ»è¨“ç·´]
    C --> D[ãƒ¢ãƒ‡ãƒ«è©•ä¾¡]
    D --> E{æ€§èƒ½OK?}
    E -->|No| C
    E -->|Yes| F[ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ]
    F --> G[ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°]
    G --> H{å†è¨“ç·´å¿…è¦?}
    H -->|Yes| B
    H -->|No| G

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#fce4ec
    style F fill:#c8e6c9
    style G fill:#ffccbc
</div>

<h3>1. ãƒ‡ãƒ¼ã‚¿åé›†ã¨æº–å‚™</h3>

<p>ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰ã¨å“è³ªä¿è¨¼ï¼š</p>

<pre><code class="language-python">"""
ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»æº–å‚™ãƒ•ã‚§ãƒ¼ã‚º

ç›®çš„:
- é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹ç¯‰
- ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°
- å†ç¾å¯èƒ½ãªå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
"""

import pandas as pd
import great_expectations as ge
from sklearn.model_selection import train_test_split
import hashlib
import json

class DataPipeline:
    """ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""

    def __init__(self, data_path, version):
        self.data_path = data_path
        self.version = version
        self.metadata = {}

    def load_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        df = pd.read_csv(self.data_path)

        # ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ï¼ˆæ•´åˆæ€§ç¢ºèªç”¨ï¼‰
        data_hash = hashlib.md5(
            pd.util.hash_pandas_object(df).values
        ).hexdigest()

        self.metadata['data_hash'] = data_hash
        self.metadata['n_samples'] = len(df)
        self.metadata['n_features'] = len(df.columns)

        print(f"âœ“ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ, {len(df.columns)}åˆ—")
        print(f"âœ“ ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥: {data_hash[:8]}...")

        return df

    def validate_data(self, df):
        """ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼"""
        # Great Expectationsã‚’ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        df_ge = ge.from_pandas(df)

        # æœŸå¾…å€¤ã®å®šç¾©
        expectations = []

        # 1. æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
        for col in df.columns:
            missing_pct = df[col].isnull().mean()
            expectations.append({
                'column': col,
                'check': 'missing_values',
                'value': f"{missing_pct:.2%}"
            })
            if missing_pct > 0.3:
                print(f"âš ï¸  è­¦å‘Š: {col}ã®æ¬ æç‡ãŒ30%ã‚’è¶…ãˆã¦ã„ã¾ã™")

        # 2. ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
        expectations.append({
            'check': 'data_types',
            'dtypes': df.dtypes.to_dict()
        })

        # 3. é‡è¤‡ãƒã‚§ãƒƒã‚¯
        n_duplicates = df.duplicated().sum()
        expectations.append({
            'check': 'duplicates',
            'value': n_duplicates
        })

        self.metadata['validation'] = expectations

        print(f"âœ“ ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å®Œäº†")
        print(f"  - é‡è¤‡è¡Œ: {n_duplicates}")

        return df

    def preprocess_data(self, df):
        """ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"""
        # æ¬ æå€¤å‡¦ç†
        df_clean = df.copy()

        # æ•°å€¤åˆ—: ä¸­å¤®å€¤è£œå®Œ
        numeric_cols = df_clean.select_dtypes(include=['float64', 'int64']).columns
        for col in numeric_cols:
            if df_clean[col].isnull().any():
                median_val = df_clean[col].median()
                df_clean[col].fillna(median_val, inplace=True)

        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ—: æœ€é »å€¤è£œå®Œ
        cat_cols = df_clean.select_dtypes(include=['object']).columns
        for col in cat_cols:
            if df_clean[col].isnull().any():
                mode_val = df_clean[col].mode()[0]
                df_clean[col].fillna(mode_val, inplace=True)

        print(f"âœ“ å‰å‡¦ç†å®Œäº†")

        return df_clean

    def save_metadata(self, filepath):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
        with open(filepath, 'w') as f:
            json.dump(self.metadata, f, indent=2, default=str)
        print(f"âœ“ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {filepath}")

# ä½¿ç”¨ä¾‹
pipeline = DataPipeline('customer_data.csv', 'v1.0.0')
df = pipeline.load_data()
df = pipeline.validate_data(df)
df_clean = pipeline.preprocess_data(df)
pipeline.save_metadata('data_metadata.json')
</code></pre>

<h3>2. ãƒ¢ãƒ‡ãƒ«é–‹ç™ºã¨è¨“ç·´</h3>

<p>å®Ÿé¨“ç®¡ç†ã¨å†ç¾æ€§ã®ç¢ºä¿ï¼š</p>

<pre><code class="language-python">"""
ãƒ¢ãƒ‡ãƒ«é–‹ç™ºãƒ»è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚º

ç›®çš„:
- ä½“ç³»çš„ãªå®Ÿé¨“ç®¡ç†
- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–
- ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°
"""

import mlflow
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
import numpy as np

class ExperimentManager:
    """å®Ÿé¨“ç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, experiment_name):
        mlflow.set_experiment(experiment_name)
        self.experiment_name = experiment_name

    def train_and_log(self, model, X_train, y_train, model_name, params):
        """ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨è¨˜éŒ²"""
        with mlflow.start_run(run_name=model_name):
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨˜éŒ²
            mlflow.log_params(params)

            # Cross-validation
            cv_scores = cross_val_score(
                model, X_train, y_train, cv=5, scoring='accuracy'
            )

            # è¨“ç·´
            model.fit(X_train, y_train)

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
            mlflow.log_metric("cv_mean_accuracy", cv_scores.mean())
            mlflow.log_metric("cv_std_accuracy", cv_scores.std())

            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            mlflow.sklearn.log_model(model, "model")

            print(f"âœ“ {model_name}")
            print(f"  - CVç²¾åº¦: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")

            return cv_scores.mean()

    def compare_models(self, X_train, y_train):
        """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒå®Ÿé¨“"""
        models = {
            'LogisticRegression': {
                'model': LogisticRegression(max_iter=1000),
                'params': {'C': 1.0, 'max_iter': 1000}
            },
            'RandomForest': {
                'model': RandomForestClassifier(n_estimators=100, random_state=42),
                'params': {'n_estimators': 100, 'max_depth': 10}
            },
            'GradientBoosting': {
                'model': GradientBoostingClassifier(n_estimators=100, random_state=42),
                'params': {'n_estimators': 100, 'learning_rate': 0.1}
            }
        }

        results = {}
        for name, config in models.items():
            score = self.train_and_log(
                config['model'],
                X_train,
                y_train,
                name,
                config['params']
            )
            results[name] = score

        # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ
        best_model = max(results, key=results.get)
        print(f"\nğŸ† æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model} (ç²¾åº¦: {results[best_model]:.3f})")

        return results

# ä½¿ç”¨ä¾‹
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
X, y = make_classification(
    n_samples=1000, n_features=20, n_informative=15,
    n_redundant=5, random_state=42
)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# å®Ÿé¨“å®Ÿè¡Œ
exp_manager = ExperimentManager("model_comparison")
results = exp_manager.compare_models(X_train, y_train)
</code></pre>

<h3>3. ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã¨é‹ç”¨</h3>

<p>ãƒ¢ãƒ‡ãƒ«ã®æœ¬ç•ªç’°å¢ƒã¸ã®å±•é–‹ï¼š</p>

<pre><code class="language-python">"""
ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ•ã‚§ãƒ¼ã‚º

ç›®çš„:
- ãƒ¢ãƒ‡ãƒ«ã®APIåŒ–
- ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
- A/Bãƒ†ã‚¹ãƒˆã®ã‚µãƒãƒ¼ãƒˆ
"""

from flask import Flask, request, jsonify
import mlflow.pyfunc
import numpy as np
import logging

class ModelServer:
    """ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒãƒ¼ã‚¯ãƒ©ã‚¹"""

    def __init__(self, model_uri, model_version):
        """
        Args:
            model_uri: MLflowãƒ¢ãƒ‡ãƒ«ã®URI
            model_version: ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³
        """
        self.model = mlflow.pyfunc.load_model(model_uri)
        self.model_version = model_version
        self.prediction_count = 0

        # ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def predict(self, features):
        """äºˆæ¸¬å®Ÿè¡Œ"""
        try:
            # å…¥åŠ›æ¤œè¨¼
            if not isinstance(features, (list, np.ndarray)):
                raise ValueError("å…¥åŠ›ã¯é…åˆ—ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")

            # äºˆæ¸¬
            prediction = self.model.predict(np.array(features).reshape(1, -1))

            # ã‚«ã‚¦ãƒ³ãƒˆæ›´æ–°
            self.prediction_count += 1

            # ãƒ­ã‚°è¨˜éŒ²
            self.logger.info(f"äºˆæ¸¬å®Ÿè¡Œ #{self.prediction_count}")

            return {
                'prediction': int(prediction[0]),
                'model_version': self.model_version,
                'prediction_id': self.prediction_count
            }

        except Exception as e:
            self.logger.error(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'error': str(e)}

# Flask API
app = Flask(__name__)

# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆæœ¬ç•ªç’°å¢ƒã§ã¯MLflow Model Registryã‹ã‚‰ï¼‰
model_server = ModelServer(
    model_uri="models:/churn_predictor/production",
    model_version="1.0.0"
)

@app.route('/predict', methods=['POST'])
def predict():
    """äºˆæ¸¬ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    data = request.get_json()
    features = data.get('features')

    if features is None:
        return jsonify({'error': 'featuresãŒå¿…è¦ã§ã™'}), 400

    result = model_server.predict(features)

    if 'error' in result:
        return jsonify(result), 500

    return jsonify(result), 200

@app.route('/health', methods=['GET'])
def health():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return jsonify({
        'status': 'healthy',
        'model_version': model_server.model_version,
        'total_predictions': model_server.prediction_count
    }), 200

# ã‚µãƒ³ãƒ—ãƒ«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
def sample_client():
    """APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä½¿ç”¨ä¾‹"""
    import requests

    # äºˆæ¸¬ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    response = requests.post(
        'http://localhost:5000/predict',
        json={'features': [0.5, 1.2, -0.3, 2.1, 0.8]}
    )

    if response.status_code == 200:
        result = response.json()
        print(f"äºˆæ¸¬çµæœ: {result['prediction']}")
        print(f"ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {result['model_version']}")
    else:
        print(f"ã‚¨ãƒ©ãƒ¼: {response.json()}")

# if __name__ == '__main__':
#     app.run(host='0.0.0.0', port=5000)
</code></pre>

<h3>4. ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨æ”¹å–„</h3>

<p>æœ¬ç•ªç’°å¢ƒã§ã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ç›£è¦–ï¼š</p>

<pre><code class="language-python">"""
ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ•ã‚§ãƒ¼ã‚º

ç›®çš„:
- ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®ç¶™ç¶šçš„ç›£è¦–
- ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã®æ¤œå‡º
- ã‚¢ãƒ©ãƒ¼ãƒˆã¨è‡ªå‹•å†è¨“ç·´ãƒˆãƒªã‚¬ãƒ¼
"""

import numpy as np
import pandas as pd
from scipy import stats
from datetime import datetime, timedelta

class ModelMonitor:
    """ãƒ¢ãƒ‡ãƒ«ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚¯ãƒ©ã‚¹"""

    def __init__(self, baseline_data, threshold=0.05):
        """
        Args:
            baseline_data: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰
            threshold: ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã®é–¾å€¤
        """
        self.baseline_data = baseline_data
        self.threshold = threshold
        self.drift_history = []

    def detect_data_drift(self, new_data, feature_name):
        """ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºï¼ˆKolmogorov-Smirnovæ¤œå®šï¼‰"""
        baseline_feature = self.baseline_data[feature_name]
        new_feature = new_data[feature_name]

        # KSæ¤œå®š
        statistic, p_value = stats.ks_2samp(baseline_feature, new_feature)

        is_drift = p_value < self.threshold

        drift_info = {
            'timestamp': datetime.now(),
            'feature': feature_name,
            'statistic': statistic,
            'p_value': p_value,
            'drift_detected': is_drift
        }

        self.drift_history.append(drift_info)

        if is_drift:
            print(f"âš ï¸  ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º: {feature_name}")
            print(f"   KSçµ±è¨ˆé‡: {statistic:.4f}, på€¤: {p_value:.4f}")

        return is_drift

    def monitor_predictions(self, predictions, actuals=None):
        """äºˆæ¸¬ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°"""
        monitoring_report = {
            'timestamp': datetime.now(),
            'n_predictions': len(predictions),
            'prediction_distribution': {
                'mean': np.mean(predictions),
                'std': np.std(predictions),
                'min': np.min(predictions),
                'max': np.max(predictions)
            }
        }

        # å®Ÿæ¸¬å€¤ãŒã‚ã‚‹å ´åˆã¯ç²¾åº¦ã‚’è¨ˆç®—
        if actuals is not None:
            accuracy = np.mean(predictions == actuals)
            monitoring_report['accuracy'] = accuracy

            if accuracy < 0.7:  # é–¾å€¤ä¾‹
                print(f"âš ï¸  ç²¾åº¦ä½ä¸‹æ¤œå‡º: {accuracy:.3f}")
                print("   å†è¨“ç·´ã‚’æ¨å¥¨ã—ã¾ã™")

        return monitoring_report

    def generate_report(self):
        """ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if not self.drift_history:
            return "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"

        df_drift = pd.DataFrame(self.drift_history)

        report = f"""
=== ãƒ¢ãƒ‡ãƒ«ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆ ===
æœŸé–“: {df_drift['timestamp'].min()} ~ {df_drift['timestamp'].max()}
ç·ãƒã‚§ãƒƒã‚¯æ•°: {len(df_drift)}
ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºæ•°: {df_drift['drift_detected'].sum()}

ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã•ã‚ŒãŸç‰¹å¾´é‡:
{df_drift[df_drift['drift_detected']][['feature', 'p_value']].to_string()}
        """

        return report

# ä½¿ç”¨ä¾‹
from sklearn.datasets import make_classification

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼ˆè¨“ç·´æ™‚ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
X_baseline, _ = make_classification(
    n_samples=1000, n_features=5, random_state=42
)
df_baseline = pd.DataFrame(
    X_baseline,
    columns=[f'feature_{i}' for i in range(5)]
)

# æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ¬ç•ªç’°å¢ƒã§ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼‰
# ã‚·ãƒ•ãƒˆã‚’åŠ ãˆã¦ãƒ‰ãƒªãƒ•ãƒˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
X_new, _ = make_classification(
    n_samples=500, n_features=5, random_state=43
)
X_new[:, 0] += 1.5  # feature_0ã«ã‚·ãƒ•ãƒˆã‚’è¿½åŠ 
df_new = pd.DataFrame(
    X_new,
    columns=[f'feature_{i}' for i in range(5)]
)

# ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
monitor = ModelMonitor(df_baseline, threshold=0.05)

print("=== ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º ===")
for col in df_baseline.columns:
    monitor.detect_data_drift(df_new, col)

print("\n" + monitor.generate_report())
</code></pre>

<hr>

<h2>1.3 MLOpsã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</h2>

<h3>1. ãƒ‡ãƒ¼ã‚¿ç®¡ç†</h3>

<p>ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã€å“è³ªç®¡ç†ã€ç³»è­œè¿½è·¡ï¼š</p>

<table>
<thead>
<tr>
<th>ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</th>
<th>ç›®çš„</th>
<th>ãƒ„ãƒ¼ãƒ«ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°</strong></td>
<td>ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¤‰æ›´å±¥æ­´ç®¡ç†</td>
<td>DVC, LakeFS, Delta Lake</td>
</tr>
<tr>
<td><strong>ãƒ‡ãƒ¼ã‚¿å“è³ª</strong></td>
<td>ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã¨ç•°å¸¸æ¤œçŸ¥</td>
<td>Great Expectations, Deequ</td>
</tr>
<tr>
<td><strong>ãƒ‡ãƒ¼ã‚¿ç³»è­œ</strong></td>
<td>ãƒ‡ãƒ¼ã‚¿ã®èµ·æºã¨å¤‰æ›å±¥æ­´</td>
<td>Apache Atlas, Marquez</td>
</tr>
<tr>
<td><strong>ç‰¹å¾´é‡ã‚¹ãƒˆã‚¢</strong></td>
<td>ç‰¹å¾´é‡ã®å†åˆ©ç”¨ã¨ä¸€è²«æ€§</td>
<td>Feast, Tecton</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">"""
ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã®å®Ÿè£…ä¾‹ï¼ˆDVCé¢¨ï¼‰
"""

import os
import hashlib
import json
from pathlib import Path

class SimpleDataVersioning:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, storage_dir='.data_versions'):
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(exist_ok=True)
        self.manifest_file = self.storage_dir / 'manifest.json'
        self.manifest = self._load_manifest()

    def _load_manifest(self):
        """ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
        if self.manifest_file.exists():
            with open(self.manifest_file, 'r') as f:
                return json.load(f)
        return {}

    def _save_manifest(self):
        """ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜"""
        with open(self.manifest_file, 'w') as f:
            json.dump(self.manifest, f, indent=2)

    def _compute_hash(self, filepath):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤è¨ˆç®—"""
        hasher = hashlib.md5()
        with open(filepath, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b''):
                hasher.update(chunk)
        return hasher.hexdigest()

    def add(self, filepath, version_tag):
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã«è¿½åŠ """
        filepath = Path(filepath)

        if not filepath.exists():
            raise FileNotFoundError(f"{filepath}ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        # ãƒãƒƒã‚·ãƒ¥å€¤è¨ˆç®—
        file_hash = self._compute_hash(filepath)

        # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ã‚³ãƒ”ãƒ¼
        storage_path = self.storage_dir / f"{version_tag}_{file_hash[:8]}"
        import shutil
        shutil.copy(filepath, storage_path)

        # ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆæ›´æ–°
        self.manifest[version_tag] = {
            'original_path': str(filepath),
            'storage_path': str(storage_path),
            'hash': file_hash,
            'size': filepath.stat().st_size,
            'timestamp': str(pd.Timestamp.now())
        }

        self._save_manifest()

        print(f"âœ“ {filepath.name}ã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ³{version_tag}ã¨ã—ã¦è¿½åŠ ")
        print(f"  ãƒãƒƒã‚·ãƒ¥: {file_hash[:8]}...")

    def checkout(self, version_tag, output_path=None):
        """ç‰¹å®šãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        if version_tag not in self.manifest:
            raise ValueError(f"ãƒãƒ¼ã‚¸ãƒ§ãƒ³{version_tag}ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        version_info = self.manifest[version_tag]
        storage_path = Path(version_info['storage_path'])

        if output_path is None:
            output_path = version_info['original_path']

        import shutil
        shutil.copy(storage_path, output_path)

        print(f"âœ“ ãƒãƒ¼ã‚¸ãƒ§ãƒ³{version_tag}ã‚’ãƒã‚§ãƒƒã‚¯ã‚¢ã‚¦ãƒˆ")
        print(f"  å‡ºåŠ›å…ˆ: {output_path}")

    def list_versions(self):
        """å…¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ä¸€è¦§è¡¨ç¤º"""
        if not self.manifest:
            print("ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“")
            return

        print("=== ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¸€è¦§ ===")
        for tag, info in self.manifest.items():
            print(f"\nãƒãƒ¼ã‚¸ãƒ§ãƒ³: {tag}")
            print(f"  ãƒ‘ã‚¹: {info['original_path']}")
            print(f"  ã‚µã‚¤ã‚º: {info['size']:,} bytes")
            print(f"  ãƒãƒƒã‚·ãƒ¥: {info['hash'][:8]}...")
            print(f"  ä½œæˆæ—¥æ™‚: {info['timestamp']}")

# ä½¿ç”¨ä¾‹ï¼ˆãƒ‡ãƒ¢ï¼‰
# dvc = SimpleDataVersioning()
# dvc.add('data.csv', 'v1.0.0')
# dvc.add('data.csv', 'v1.1.0')  # ãƒ‡ãƒ¼ã‚¿æ›´æ–°å¾Œ
# dvc.list_versions()
# dvc.checkout('v1.0.0', 'data_old.csv')
</code></pre>

<h3>2. ãƒ¢ãƒ‡ãƒ«ç®¡ç†</h3>

<p>ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«å…¨ä½“ã‚’ç®¡ç†ï¼š</p>

<table>
<thead>
<tr>
<th>ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</th>
<th>ç›®çš„</th>
<th>æ©Ÿèƒ½</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å®Ÿé¨“ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°</strong></td>
<td>å®Ÿé¨“çµæœã®è¨˜éŒ²ã¨æ¯”è¼ƒ</td>
<td>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€æˆæœç‰©ã®è¨˜éŒ²</td>
</tr>
<tr>
<td><strong>ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒª</strong></td>
<td>ãƒ¢ãƒ‡ãƒ«ã®ä¸­å¤®ç®¡ç†</td>
<td>ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã€ã‚¹ãƒ†ãƒ¼ã‚¸ç®¡ç†ã€æ‰¿èªãƒ•ãƒ­ãƒ¼</td>
</tr>
<tr>
<td><strong>ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚°</strong></td>
<td>ãƒ‡ãƒ—ãƒ­ã‚¤å¯èƒ½ãªå½¢å¼ã«å¤‰æ›</td>
<td>ä¾å­˜é–¢ä¿‚ã®è§£æ±ºã€ã‚³ãƒ³ãƒ†ãƒŠåŒ–</td>
</tr>
</tbody>
</table>

<h3>3. ã‚¤ãƒ³ãƒ•ãƒ©ç®¡ç†</h3>

<p>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã§å†ç¾å¯èƒ½ãªã‚¤ãƒ³ãƒ•ãƒ©ï¼š</p>

<table>
<thead>
<tr>
<th>ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</th>
<th>ç›®çš„</th>
<th>ãƒ„ãƒ¼ãƒ«ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ã‚³ãƒ³ãƒ†ãƒŠåŒ–</strong></td>
<td>ç’°å¢ƒã®ä¸€è²«æ€§ç¢ºä¿</td>
<td>Docker, Kubernetes</td>
</tr>
<tr>
<td><strong>ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</strong></td>
<td>ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®è‡ªå‹•åŒ–</td>
<td>Airflow, Kubeflow, Argo</td>
</tr>
<tr>
<td><strong>ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†</strong></td>
<td>è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã®åŠ¹ç‡çš„åˆ©ç”¨</td>
<td>Kubernetes, Ray</td>
</tr>
</tbody>
</table>

<h3>4. ã‚¬ãƒãƒŠãƒ³ã‚¹</h3>

<p>ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ã¨ç›£æŸ»å¯¾å¿œï¼š</p>

<table>
<thead>
<tr>
<th>è¦ç´ </th>
<th>å†…å®¹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ¢ãƒ‡ãƒ«èª¬æ˜å¯èƒ½æ€§</strong></td>
<td>äºˆæ¸¬ã®æ ¹æ‹ ã‚’èª¬æ˜</td>
</tr>
<tr>
<td><strong>ãƒã‚¤ã‚¢ã‚¹æ¤œå‡º</strong></td>
<td>å…¬å¹³æ€§ã®æ¤œè¨¼</td>
</tr>
<tr>
<td><strong>ç›£æŸ»ãƒ­ã‚°</strong></td>
<td>å…¨ã¦ã®å¤‰æ›´å±¥æ­´ã‚’è¨˜éŒ²</td>
</tr>
<tr>
<td><strong>ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡</strong></td>
<td>æ¨©é™ç®¡ç†ã¨æ‰¿èªãƒ•ãƒ­ãƒ¼</td>
</tr>
</tbody>
</table>

<hr>

<h2>1.4 MLOpsãƒ„ãƒ¼ãƒ«ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ </h2>

<h3>å®Ÿé¨“ç®¡ç†ãƒ„ãƒ¼ãƒ«</h3>

<table>
<thead>
<tr>
<th>ãƒ„ãƒ¼ãƒ«</th>
<th>ç‰¹å¾´</th>
<th>ä¸»ãªç”¨é€”</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MLflow</strong></td>
<td>ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã€å¤šæ©Ÿèƒ½</td>
<td>å®Ÿé¨“ç®¡ç†ã€ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã€ãƒ‡ãƒ—ãƒ­ã‚¤</td>
</tr>
<tr>
<td><strong>Weights & Biases</strong></td>
<td>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–ã€ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</td>
<td>å®Ÿé¨“æ¯”è¼ƒã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</td>
</tr>
<tr>
<td><strong>Neptune.ai</strong></td>
<td>ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã«ç‰¹åŒ–</td>
<td>é•·æœŸçš„ãªå®Ÿé¨“ç®¡ç†ã€ãƒãƒ¼ãƒ å”åƒ</td>
</tr>
</tbody>
</table>

<h3>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</h3>

<table>
<thead>
<tr>
<th>ãƒ„ãƒ¼ãƒ«</th>
<th>ç‰¹å¾´</th>
<th>ä¸»ãªç”¨é€”</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Kubeflow</strong></td>
<td>Kubernetesä¸Šã®ML</td>
<td>ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</td>
</tr>
<tr>
<td><strong>Apache Airflow</strong></td>
<td>æ±ç”¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</td>
<td>ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°</td>
</tr>
<tr>
<td><strong>Prefect</strong></td>
<td>Pythonãƒã‚¤ãƒ†ã‚£ãƒ–ã€ãƒ¢ãƒ€ãƒ³API</td>
<td>ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°</td>
</tr>
</tbody>
</table>

<h3>ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ</h3>

<table>
<thead>
<tr>
<th>ãƒ„ãƒ¼ãƒ«</th>
<th>ç‰¹å¾´</th>
<th>ä¸»ãªç”¨é€”</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>BentoML</strong></td>
<td>ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒ“ãƒ³ã‚°ç‰¹åŒ–</td>
<td>REST APIã€ãƒãƒƒãƒæ¨è«–</td>
</tr>
<tr>
<td><strong>Seldon Core</strong></td>
<td>Kubernetesä¸Šã®ãƒ‡ãƒ—ãƒ­ã‚¤</td>
<td>ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ã€A/Bãƒ†ã‚¹ãƒˆ</td>
</tr>
<tr>
<td><strong>TensorFlow Serving</strong></td>
<td>TensorFlowå°‚ç”¨</td>
<td>é«˜é€Ÿæ¨è«–ã€GPUå¯¾å¿œ</td>
</tr>
</tbody>
</table>

<h3>ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«</h3>

<table>
<thead>
<tr>
<th>ãƒ„ãƒ¼ãƒ«</th>
<th>ç‰¹å¾´</th>
<th>ä¸»ãªç”¨é€”</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Evidently</strong></td>
<td>ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º</td>
<td>ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ç›£è¦–ã€ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ</td>
</tr>
<tr>
<td><strong>Prometheus + Grafana</strong></td>
<td>æ±ç”¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–</td>
<td>ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ã€ã‚¢ãƒ©ãƒ¼ãƒˆ</td>
</tr>
<tr>
<td><strong>Arize AI</strong></td>
<td>MLç‰¹åŒ–ã®å¯è¦³æ¸¬æ€§</td>
<td>ãƒ¢ãƒ‡ãƒ«ç›£è¦–ã€æ ¹æœ¬åŸå› åˆ†æ</td>
</tr>
</tbody>
</table>

<h3>çµ±åˆãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ </h3>

<table>
<thead>
<tr>
<th>ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ </th>
<th>ç‰¹å¾´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>AWS SageMaker</strong></td>
<td>AWSãƒã‚¤ãƒ†ã‚£ãƒ–ã€ãƒ•ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ‰</td>
</tr>
<tr>
<td><strong>Azure ML</strong></td>
<td>Azureã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ</td>
</tr>
<tr>
<td><strong>Google Vertex AI</strong></td>
<td>GCPã‚µãƒ¼ãƒ“ã‚¹çµ±åˆã€AutoML</td>
</tr>
<tr>
<td><strong>Databricks</strong></td>
<td>ãƒ‡ãƒ¼ã‚¿+MLçµ±åˆã€SparkåŸºç›¤</td>
</tr>
</tbody>
</table>

<hr>

<h2>1.5 MLOpsã®æˆç†Ÿåº¦ãƒ¢ãƒ‡ãƒ«</h2>

<p>çµ„ç¹”ã®MLOpsæˆç†Ÿåº¦ã‚’è©•ä¾¡ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆGoogleæå”±ï¼‰ï¼š</p>

<h3>Level 0: Manual Processï¼ˆæ‰‹å‹•ãƒ—ãƒ­ã‚»ã‚¹ï¼‰</h3>

<p><strong>ç‰¹å¾´</strong>ï¼š</p>
<ul>
<li>å…¨ã¦ã®ã‚¹ãƒ†ãƒƒãƒ—ãŒæ‰‹å‹•</li>
<li>Jupyter Notebookãƒ™ãƒ¼ã‚¹ã®é–‹ç™º</li>
<li>ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã¯æ‰‹ä½œæ¥­</li>
<li>å†ç¾æ€§ãªã—</li>
</ul>

<p><strong>èª²é¡Œ</strong>ï¼š</p>
<ul>
<li>ã‚¹ã‚±ãƒ¼ãƒ«ã—ãªã„</li>
<li>ã‚¨ãƒ©ãƒ¼ãŒå¤šç™º</li>
<li>ãƒ‡ãƒ—ãƒ­ã‚¤ã«æ™‚é–“ãŒã‹ã‹ã‚‹</li>
<li>ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãªã—</li>
</ul>

<h3>Level 1: ML Pipeline Automationï¼ˆMLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è‡ªå‹•åŒ–ï¼‰</h3>

<p><strong>ç‰¹å¾´</strong>ï¼š</p>
<ul>
<li>è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è‡ªå‹•åŒ–</li>
<li>ç¶™ç¶šçš„ãªè¨“ç·´ï¼ˆCT: Continuous Trainingï¼‰</li>
<li>å®Ÿé¨“ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°</li>
<li>ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®ä½¿ç”¨</li>
</ul>

<p><strong>å®Ÿç¾ã™ã‚‹ã“ã¨</strong>ï¼š</p>
<ul>
<li>æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§ã®è‡ªå‹•å†è¨“ç·´</li>
<li>ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°</li>
<li>åŸºæœ¬çš„ãªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°</li>
</ul>

<pre><code class="language-python">"""
Level 1ã®å®Ÿè£…ä¾‹: è‡ªå‹•è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
import mlflow
import schedule
import time

class AutoTrainingPipeline:
    """è‡ªå‹•è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(self, experiment_name):
        mlflow.set_experiment(experiment_name)
        self.experiment_name = experiment_name

    def create_pipeline(self):
        """MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰"""
        return Pipeline([
            ('scaler', StandardScaler()),
            ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
        ])

    def train(self, X_train, y_train, X_val, y_val):
        """è¨“ç·´å®Ÿè¡Œ"""
        with mlflow.start_run():
            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ
            pipeline = self.create_pipeline()

            # è¨“ç·´
            pipeline.fit(X_train, y_train)

            # è©•ä¾¡
            train_score = pipeline.score(X_train, y_train)
            val_score = pipeline.score(X_val, y_val)

            # MLflowã«è¨˜éŒ²
            mlflow.log_metric("train_accuracy", train_score)
            mlflow.log_metric("val_accuracy", val_score)
            mlflow.sklearn.log_model(pipeline, "model")

            # ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²
            if val_score > 0.8:  # é–¾å€¤ã‚’è¶…ãˆãŸå ´åˆã®ã¿
                mlflow.register_model(
                    f"runs:/{mlflow.active_run().info.run_id}/model",
                    "production_model"
                )
                print(f"âœ“ æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ²ï¼ˆæ¤œè¨¼ç²¾åº¦: {val_score:.3f}ï¼‰")
            else:
                print(f"âš ï¸  ç²¾åº¦ãŒé–¾å€¤æœªæº€ï¼ˆ{val_score:.3f} < 0.8ï¼‰")

            return pipeline

    def scheduled_training(self, data_loader, schedule_time="00:00"):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨“ç·´"""
        def job():
            print(f"=== è‡ªå‹•è¨“ç·´é–‹å§‹: {pd.Timestamp.now()} ===")
            X_train, X_val, y_train, y_val = data_loader()
            self.train(X_train, X_val, y_train, y_val)

        # æ¯æ—¥æŒ‡å®šæ™‚åˆ»ã«å®Ÿè¡Œ
        schedule.every().day.at(schedule_time).do(job)

        print(f"âœ“ è‡ªå‹•è¨“ç·´ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š: æ¯æ—¥{schedule_time}")

        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œï¼ˆå®Ÿéš›ã®ç’°å¢ƒã§ã¯ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã¨ã—ã¦å®Ÿè¡Œï¼‰
        # while True:
        #     schedule.run_pending()
        #     time.sleep(60)

# ä½¿ç”¨ä¾‹ï¼ˆãƒ‡ãƒ¢ï¼‰
# def load_latest_data():
#     # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãƒ­ã‚¸ãƒƒã‚¯
#     return X_train, X_val, y_train, y_val
#
# pipeline = AutoTrainingPipeline("auto_training")
# pipeline.scheduled_training(load_latest_data, "02:00")
</code></pre>

<h3>Level 2: CI/CD Pipeline Automationï¼ˆCI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è‡ªå‹•åŒ–ï¼‰</h3>

<p><strong>ç‰¹å¾´</strong>ï¼š</p>
<ul>
<li>å®Œå…¨ãªè‡ªå‹•åŒ–ï¼ˆã‚³ãƒ¼ãƒ‰å¤‰æ›´ã‹ã‚‰ãƒ‡ãƒ—ãƒ­ã‚¤ã¾ã§ï¼‰</li>
<li>CI/CDçµ±åˆ</li>
<li>è‡ªå‹•ãƒ†ã‚¹ãƒˆï¼ˆãƒ‡ãƒ¼ã‚¿ã€ãƒ¢ãƒ‡ãƒ«ã€ã‚¤ãƒ³ãƒ•ãƒ©ï¼‰</li>
<li>åŒ…æ‹¬çš„ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°</li>
</ul>

<p><strong>å®Ÿç¾ã™ã‚‹ã“ã¨</strong>ï¼š</p>
<ul>
<li>ã‚³ãƒ¼ãƒ‰å¤‰æ›´ã®è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤</li>
<li>A/Bãƒ†ã‚¹ãƒˆ</li>
<li>ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ</li>
<li>è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯</li>
</ul>

<div class="mermaid">
graph TB
    subgraph "Level 0: Manual"
        A1[Notebooké–‹ç™º] --> A2[æ‰‹å‹•è¨“ç·´]
        A2 --> A3[æ‰‹å‹•ãƒ‡ãƒ—ãƒ­ã‚¤]
    end

    subgraph "Level 1: ML Pipeline"
        B1[ã‚³ãƒ¼ãƒ‰é–‹ç™º] --> B2[è‡ªå‹•è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³]
        B2 --> B3[ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒª]
        B3 --> B4[æ‰‹å‹•ãƒ‡ãƒ—ãƒ­ã‚¤æ‰¿èª]
        B4 --> B5[ãƒ‡ãƒ—ãƒ­ã‚¤]
    end

    subgraph "Level 2: CI/CD"
        C1[ã‚³ãƒ¼ãƒ‰å¤‰æ›´] --> C2[CI: è‡ªå‹•ãƒ†ã‚¹ãƒˆ]
        C2 --> C3[è‡ªå‹•è¨“ç·´]
        C3 --> C4[è‡ªå‹•æ¤œè¨¼]
        C4 --> C5[CD: è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤]
        C5 --> C6[ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°]
        C6 --> C7{æ€§èƒ½OK?}
        C7 -->|No| C8[è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯]
        C7 -->|Yes| C6
    end

    style A1 fill:#ffebee
    style B2 fill:#fff3e0
    style C5 fill:#e8f5e9
</div>

<h3>æˆç†Ÿåº¦ãƒ¬ãƒ™ãƒ«ã®æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>å´é¢</th>
<th>Level 0</th>
<th>Level 1</th>
<th>Level 2</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ‡ãƒ—ãƒ­ã‚¤é »åº¦</strong></td>
<td>æœˆã€œå¹´å˜ä½</td>
<td>é€±ã€œæœˆå˜ä½</td>
<td>æ—¥ã€œé€±å˜ä½</td>
</tr>
<tr>
<td><strong>å†ç¾æ€§</strong></td>
<td>ä½ã„</td>
<td>ä¸­ç¨‹åº¦</td>
<td>é«˜ã„</td>
</tr>
<tr>
<td><strong>è‡ªå‹•åŒ–</strong></td>
<td>ãªã—</td>
<td>è¨“ç·´ã®ã¿</td>
<td>ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰</td>
</tr>
<tr>
<td><strong>ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°</strong></td>
<td>ãªã—/æ‰‹å‹•</td>
<td>åŸºæœ¬çš„</td>
<td>åŒ…æ‹¬çš„</td>
</tr>
<tr>
<td><strong>ãƒ†ã‚¹ãƒˆ</strong></td>
<td>ãªã—</td>
<td>ãƒ¢ãƒ‡ãƒ«ã®ã¿</td>
<td>å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</td>
</tr>
<tr>
<td><strong>é©ç”¨è¦æ¨¡</strong></td>
<td>1-2ãƒ¢ãƒ‡ãƒ«</td>
<td>æ•°ãƒ¢ãƒ‡ãƒ«</td>
<td>å¤šæ•°ã®ãƒ¢ãƒ‡ãƒ«</td>
</tr>
</tbody>
</table>

<hr>

<h2>1.6 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>MLOpsã®å¿…è¦æ€§</strong></p>
<ul>
<li>æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®85%ãŒæœ¬ç•ªç’°å¢ƒã«åˆ°é”ã—ãªã„</li>
<li>MLOpsã¯é–‹ç™ºã‹ã‚‰é‹ç”¨ã¾ã§ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’åŸ‹ã‚ã‚‹</li>
<li>DevOpsã€DataOpsã€MLã®çµ±åˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</li>
</ul></li>

<li><p><strong>MLãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«</strong></p>
<ul>
<li>ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»æº–å‚™ã€ãƒ¢ãƒ‡ãƒ«é–‹ç™ºã€ãƒ‡ãƒ—ãƒ­ã‚¤ã€ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã®4ãƒ•ã‚§ãƒ¼ã‚º</li>
<li>åå¾©çš„ãƒ»ç¶™ç¶šçš„ãªãƒ—ãƒ­ã‚»ã‚¹</li>
<li>å„ãƒ•ã‚§ãƒ¼ã‚ºã§ã®è‡ªå‹•åŒ–ã¨å“è³ªä¿è¨¼ãŒé‡è¦</li>
</ul></li>

<li><p><strong>ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</strong></p>
<ul>
<li>ãƒ‡ãƒ¼ã‚¿ç®¡ç†: ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã€å“è³ªã€ç³»è­œ</li>
<li>ãƒ¢ãƒ‡ãƒ«ç®¡ç†: å®Ÿé¨“ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã€ãƒ¬ã‚¸ã‚¹ãƒˆãƒª</li>
<li>ã‚¤ãƒ³ãƒ•ãƒ©ç®¡ç†: ã‚³ãƒ³ãƒ†ãƒŠåŒ–ã€ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</li>
<li>ã‚¬ãƒãƒŠãƒ³ã‚¹: èª¬æ˜å¯èƒ½æ€§ã€ç›£æŸ»ã€ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡</li>
</ul></li>

<li><p><strong>ãƒ„ãƒ¼ãƒ«ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ </strong></p>
<ul>
<li>å®Ÿé¨“ç®¡ç†: MLflow, Weights & Biases</li>
<li>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: Kubeflow, Airflow</li>
<li>ãƒ‡ãƒ—ãƒ­ã‚¤: BentoML, Seldon</li>
<li>ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°: Evidently, Prometheus</li>
</ul></li>

<li><p><strong>æˆç†Ÿåº¦ãƒ¢ãƒ‡ãƒ«</strong></p>
<ul>
<li>Level 0: å®Œå…¨æ‰‹å‹•ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ã—ãªã„ï¼‰</li>
<li>Level 1: è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è‡ªå‹•åŒ–</li>
<li>Level 2: CI/CDå®Œå…¨è‡ªå‹•åŒ–ï¼ˆã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºå¯¾å¿œï¼‰</li>
</ul></li>
</ol>

<h3>MLOpså°å…¥ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>

<table>
<thead>
<tr>
<th>åŸå‰‡</th>
<th>èª¬æ˜</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å°ã•ãå§‹ã‚ã‚‹</strong></td>
<td>Level 0 â†’ Level 1 â†’ Level 2 ã¨æ®µéšçš„ã«é€²åŒ–</td>
</tr>
<tr>
<td><strong>è‡ªå‹•åŒ–å„ªå…ˆ</strong></td>
<td>æ‰‹ä½œæ¥­ã‚’æœ€å°åŒ–ã—ã€ã‚¨ãƒ©ãƒ¼ã‚’å‰Šæ¸›</td>
</tr>
<tr>
<td><strong>ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°å¿…é ˆ</strong></td>
<td>æœ¬ç•ªç’°å¢ƒã§ã®æ€§èƒ½ã‚’ç¶™ç¶šçš„ã«ç›£è¦–</td>
</tr>
<tr>
<td><strong>å†ç¾æ€§ç¢ºä¿</strong></td>
<td>å…¨ã¦ã®å®Ÿé¨“ã¨ãƒ¢ãƒ‡ãƒ«ã‚’å†ç¾å¯èƒ½ã«</td>
</tr>
<tr>
<td><strong>ãƒãƒ¼ãƒ å”åƒ</strong></td>
<td>ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã¨ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã®å”åŠ›</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ç« ã¸</h3>

<p>ç¬¬2ç« ã§ã¯ã€<strong>å®Ÿé¨“ç®¡ç†ã¨ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°</strong>ã‚’è©³ã—ãå­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>MLflowã«ã‚ˆã‚‹å®Ÿé¨“ç®¡ç†</li>
<li>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</li>
<li>ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®æ´»ç”¨</li>
<li>å®Ÿé¨“çµæœã®å¯è¦–åŒ–ã¨æ¯”è¼ƒ</li>
<li>ãƒãƒ¼ãƒ ã§ã®å®Ÿé¨“å…±æœ‰</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>
<p>MLOpsã¨DevOpsã®é•ã„ã‚’3ã¤æŒ™ã’ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚æ©Ÿæ¢°å­¦ç¿’ç‰¹æœ‰ã®èª²é¡Œã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<ol>
<li><p><strong>ãƒ‡ãƒ¼ã‚¿ã®æ‰±ã„</strong></p>
<ul>
<li><strong>DevOps</strong>: ã‚³ãƒ¼ãƒ‰ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ãŒä¸­å¿ƒ</li>
<li><strong>MLOps</strong>: ã‚³ãƒ¼ãƒ‰ã€ãƒ‡ãƒ¼ã‚¿ã€ãƒ¢ãƒ‡ãƒ«ã®3ã¤ã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã™ã‚‹å¿…è¦ãŒã‚ã‚‹</li>
<li>æ©Ÿæ¢°å­¦ç¿’ã§ã¯ã€åŒã˜ã‚³ãƒ¼ãƒ‰ã§ã‚‚ãƒ‡ãƒ¼ã‚¿ãŒç•°ãªã‚Œã°çµæœãŒå¤‰ã‚ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ãŒå¿…é ˆ</li>
</ul></li>

<li><p><strong>ãƒ†ã‚¹ãƒˆã®è¤‡é›‘æ€§</strong></p>
<ul>
<li><strong>DevOps</strong>: æ±ºå®šè«–çš„ãªãƒ†ã‚¹ãƒˆï¼ˆåŒã˜å…¥åŠ› â†’ åŒã˜å‡ºåŠ›ï¼‰</li>
<li><strong>MLOps</strong>: ç¢ºç‡çš„ãªãƒ†ã‚¹ãƒˆï¼ˆãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã€ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã€ãƒã‚¤ã‚¢ã‚¹ãªã©ï¼‰</li>
<li>ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆã¯ç²¾åº¦ã ã‘ã§ãªãã€å…¬å¹³æ€§ã€è§£é‡ˆå¯èƒ½æ€§ãªã©ã‚‚è©•ä¾¡ã™ã‚‹å¿…è¦ãŒã‚ã‚‹</li>
</ul></li>

<li><p><strong>ç¶™ç¶šçš„ãªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°</strong></p>
<ul>
<li><strong>DevOps</strong>: ã‚·ã‚¹ãƒ†ãƒ ã®ç¨¼åƒçŠ¶æ…‹ã€ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒ¼ãƒˆã‚’ç›£è¦–</li>
<li><strong>MLOps</strong>: ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½åŠ£åŒ–ã€ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã€äºˆæ¸¬åˆ†å¸ƒã®å¤‰åŒ–ã‚’ç›£è¦–</li>
<li>æ™‚é–“çµŒéã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ã®åŠ£åŒ–ãŒé¿ã‘ã‚‰ã‚Œãªã„ãŸã‚ã€è‡ªå‹•å†è¨“ç·´ã®ä»•çµ„ã¿ãŒå¿…è¦</li>
</ul></li>
</ol>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’æ”¹å–„ã—ã¦ã€MLOpsã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«å¾“ã£ãŸå®Ÿé¨“ç®¡ç†ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚MLflowã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€ãƒ¢ãƒ‡ãƒ«ã‚’è¨˜éŒ²ã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_csv('data.csv')
X = df.drop('target', axis=1)
y = df['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)

# è©•ä¾¡
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import pandas as pd
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import hashlib
import json

# MLflowå®Ÿé¨“ã®è¨­å®š
mlflow.set_experiment("customer_classification")

# ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®è¨ˆç®—
def compute_data_version(df):
    """ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ã—ã¦ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ã—ã¦ä½¿ç”¨"""
    data_str = pd.util.hash_pandas_object(df).values.tobytes()
    return hashlib.md5(data_str).hexdigest()[:8]

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_csv('data.csv')
data_version = compute_data_version(df)

X = df.drop('target', axis=1)
y = df['target']

# å®Ÿé¨“é–‹å§‹
with mlflow.start_run(run_name="rf_baseline"):

    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å®šç¾©
    params = {
        'n_estimators': 100,
        'max_depth': 10,
        'min_samples_split': 5,
        'random_state': 42
    }

    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆå›ºå®šã‚·ãƒ¼ãƒ‰ã§å†ç¾æ€§ç¢ºä¿ï¼‰
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨˜éŒ²
    mlflow.log_params(params)
    mlflow.log_param("data_version", data_version)
    mlflow.log_param("test_size", 0.2)
    mlflow.log_param("n_train_samples", len(X_train))
    mlflow.log_param("n_test_samples", len(X_test))

    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    model = RandomForestClassifier(**params)
    model.fit(X_train, y_train)

    # Cross-validation
    cv_scores = cross_val_score(model, X_train, y_train, cv=5)
    mlflow.log_metric("cv_mean_accuracy", cv_scores.mean())
    mlflow.log_metric("cv_std_accuracy", cv_scores.std())

    # ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®è©•ä¾¡
    y_pred = model.predict(X_test)

    # è¤‡æ•°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²
    metrics = {
        'test_accuracy': accuracy_score(y_test, y_pred),
        'test_precision': precision_score(y_test, y_pred, average='weighted'),
        'test_recall': recall_score(y_test, y_pred, average='weighted'),
        'test_f1': f1_score(y_test, y_pred, average='weighted')
    }

    mlflow.log_metrics(metrics)

    # ç‰¹å¾´é‡é‡è¦åº¦ã®è¨˜éŒ²
    feature_importance = dict(zip(X.columns, model.feature_importances_))
    mlflow.log_dict(feature_importance, "feature_importance.json")

    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    mlflow.sklearn.log_model(
        model,
        "model",
        registered_model_name="customer_classifier"
    )

    # ã‚¿ã‚°ã®è¨­å®š
    mlflow.set_tag("model_type", "RandomForest")
    mlflow.set_tag("framework", "scikit-learn")
    mlflow.set_tag("environment", "development")

    # çµæœã®è¡¨ç¤º
    print("=== å®Ÿé¨“çµæœ ===")
    print(f"Run ID: {mlflow.active_run().info.run_id}")
    print(f"Data Version: {data_version}")
    print(f"\nMetrics:")
    for metric, value in metrics.items():
        print(f"  {metric}: {value:.4f}")
    print(f"\nCV Accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

    # ãƒ¢ãƒ‡ãƒ«ã®ç™»éŒ²ç¢ºèª
    print(f"\nâœ“ ãƒ¢ãƒ‡ãƒ«ã‚’MLflowã«è¨˜éŒ²ã—ã¾ã—ãŸ")
    print(f"âœ“ å®Ÿé¨“å: customer_classification")
</code></pre>

<p><strong>æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ</strong>ï¼š</p>
<ol>
<li>MLflowã§å®Ÿé¨“ã‚’ç®¡ç†</li>
<li>ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®è¨˜éŒ²</li>
<li>å…¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨˜éŒ²</li>
<li>è¤‡æ•°ã®è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²</li>
<li>Cross-validationã®å®Ÿæ–½</li>
<li>ç‰¹å¾´é‡é‡è¦åº¦ã®ä¿å­˜</li>
<li>ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã¸ã®ç™»éŒ²</li>
<li>å†ç¾æ€§ã®ç¢ºä¿ï¼ˆrandom_stateã€stratifyï¼‰</li>
</ol>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚Kolmogorov-Smirnovæ¤œå®šã‚’ä½¿ç”¨ã—ã¦ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã¨çµ±è¨ˆçš„ã«ç•°ãªã‚‹ã‹ã‚’åˆ¤å®šã—ã€ã‚¢ãƒ©ãƒ¼ãƒˆã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import numpy as np
import pandas as pd
from scipy import stats
from datetime import datetime
import json

class DataDriftMonitor:
    """ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆç›£è¦–ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, baseline_data, threshold=0.05, alert_features=None):
        """
        Args:
            baseline_data: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼ˆDataFrameï¼‰
            threshold: på€¤ã®é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.05ï¼‰
            alert_features: ç›£è¦–ã™ã‚‹ç‰¹å¾´é‡ã®ãƒªã‚¹ãƒˆï¼ˆNoneã®å ´åˆã¯å…¨ç‰¹å¾´é‡ï¼‰
        """
        self.baseline_data = baseline_data
        self.threshold = threshold
        self.alert_features = alert_features or baseline_data.columns.tolist()
        self.drift_history = []
        self.baseline_stats = self._compute_baseline_stats()

    def _compute_baseline_stats(self):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®çµ±è¨ˆé‡ã‚’è¨ˆç®—"""
        stats_dict = {}
        for col in self.baseline_data.columns:
            if pd.api.types.is_numeric_dtype(self.baseline_data[col]):
                stats_dict[col] = {
                    'mean': self.baseline_data[col].mean(),
                    'std': self.baseline_data[col].std(),
                    'min': self.baseline_data[col].min(),
                    'max': self.baseline_data[col].max(),
                    'median': self.baseline_data[col].median()
                }
        return stats_dict

    def detect_drift(self, new_data, feature):
        """å˜ä¸€ç‰¹å¾´é‡ã®ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º"""
        if feature not in self.baseline_data.columns:
            raise ValueError(f"ç‰¹å¾´é‡ {feature} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        # æ•°å€¤å‹ã®ã¿å‡¦ç†
        if not pd.api.types.is_numeric_dtype(self.baseline_data[feature]):
            return None

        # KSæ¤œå®š
        baseline_values = self.baseline_data[feature].dropna()
        new_values = new_data[feature].dropna()

        statistic, p_value = stats.ks_2samp(baseline_values, new_values)

        is_drift = p_value < self.threshold

        # çµ±è¨ˆé‡ã®å¤‰åŒ–ã‚’è¨ˆç®—
        baseline_mean = baseline_values.mean()
        new_mean = new_values.mean()
        mean_shift = (new_mean - baseline_mean) / baseline_mean * 100

        drift_info = {
            'timestamp': datetime.now().isoformat(),
            'feature': feature,
            'ks_statistic': float(statistic),
            'p_value': float(p_value),
            'drift_detected': bool(is_drift),
            'baseline_mean': float(baseline_mean),
            'new_mean': float(new_mean),
            'mean_shift_pct': float(mean_shift),
            'n_baseline': len(baseline_values),
            'n_new': len(new_values)
        }

        return drift_info

    def monitor_all_features(self, new_data):
        """å…¨ç‰¹å¾´é‡ã®ãƒ‰ãƒªãƒ•ãƒˆç›£è¦–"""
        results = []
        alerts = []

        print(f"=== ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆç›£è¦–å®Ÿè¡Œ ===")
        print(f"æ™‚åˆ»: {datetime.now()}")
        print(f"ç›£è¦–ç‰¹å¾´é‡æ•°: {len(self.alert_features)}")
        print(f"æ–°ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(new_data)}\n")

        for feature in self.alert_features:
            if feature not in new_data.columns:
                continue

            drift_info = self.detect_drift(new_data, feature)

            if drift_info is None:
                continue

            results.append(drift_info)
            self.drift_history.append(drift_info)

            # ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºæ™‚ã®ã‚¢ãƒ©ãƒ¼ãƒˆ
            if drift_info['drift_detected']:
                alert_msg = (
                    f"âš ï¸  ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º: {feature}\n"
                    f"   KSçµ±è¨ˆé‡: {drift_info['ks_statistic']:.4f}\n"
                    f"   på€¤: {drift_info['p_value']:.4f}\n"
                    f"   å¹³å‡å€¤ã‚·ãƒ•ãƒˆ: {drift_info['mean_shift_pct']:.2f}%"
                )
                alerts.append(alert_msg)
                print(alert_msg + "\n")

        # ã‚µãƒãƒªãƒ¼
        n_drift = sum(r['drift_detected'] for r in results)
        print(f"=== ç›£è¦–çµæœ ===")
        print(f"ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º: {n_drift}/{len(results)}ç‰¹å¾´é‡")

        if n_drift > len(results) * 0.3:  # 30%ä»¥ä¸Šã§ã‚¢ãƒ©ãƒ¼ãƒˆ
            print("âš ï¸  è­¦å‘Š: å¤šæ•°ã®ç‰¹å¾´é‡ã§ãƒ‰ãƒªãƒ•ãƒˆãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
            print("   ãƒ¢ãƒ‡ãƒ«ã®å†è¨“ç·´ã‚’æ¨å¥¨ã—ã¾ã™")

        return results, alerts

    def generate_report(self):
        """ãƒ‰ãƒªãƒ•ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if not self.drift_history:
            return "ãƒ‰ãƒªãƒ•ãƒˆå±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“"

        df_history = pd.DataFrame(self.drift_history)

        report = f"""
=== ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆ ===

ç›£è¦–æœŸé–“: {df_history['timestamp'].min()} ~ {df_history['timestamp'].max()}
ç·ç›£è¦–å›æ•°: {len(df_history)}
ãƒ¦ãƒ‹ãƒ¼ã‚¯ç‰¹å¾´é‡æ•°: {df_history['feature'].nunique()}

ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã‚µãƒãƒªãƒ¼:
{df_history.groupby('feature')['drift_detected'].agg(['sum', 'count']).to_string()}

ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºç‡ãƒˆãƒƒãƒ—5:
{df_history[df_history['drift_detected']].groupby('feature').size().sort_values(ascending=False).head().to_string()}

å¹³å‡ã‚·ãƒ•ãƒˆç‡ï¼ˆçµ¶å¯¾å€¤ï¼‰ãƒˆãƒƒãƒ—5:
{df_history.groupby('feature')['mean_shift_pct'].apply(lambda x: abs(x).mean()).sort_values(ascending=False).head().to_string()}
        """

        return report

    def save_report(self, filepath):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONã§ä¿å­˜"""
        report_data = {
            'baseline_stats': self.baseline_stats,
            'drift_history': self.drift_history,
            'summary': {
                'total_checks': len(self.drift_history),
                'total_drifts': sum(d['drift_detected'] for d in self.drift_history)
            }
        }

        with open(filepath, 'w') as f:
            json.dump(report_data, f, indent=2)

        print(f"âœ“ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {filepath}")

# ä½¿ç”¨ä¾‹
from sklearn.datasets import make_classification

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼ˆè¨“ç·´æ™‚ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
X_baseline, _ = make_classification(
    n_samples=1000, n_features=10, random_state=42
)
df_baseline = pd.DataFrame(
    X_baseline,
    columns=[f'feature_{i}' for i in range(10)]
)

# æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ‰ãƒªãƒ•ãƒˆã‚ã‚Šï¼‰
X_new, _ = make_classification(
    n_samples=500, n_features=10, random_state=43
)
# ã„ãã¤ã‹ã®ç‰¹å¾´é‡ã«ã‚·ãƒ•ãƒˆã‚’è¿½åŠ 
X_new[:, 0] += 2.0  # feature_0ã«å¤§ããªã‚·ãƒ•ãƒˆ
X_new[:, 3] += 0.5  # feature_3ã«å°ã•ãªã‚·ãƒ•ãƒˆ

df_new = pd.DataFrame(
    X_new,
    columns=[f'feature_{i}' for i in range(10)]
)

# ãƒ‰ãƒªãƒ•ãƒˆç›£è¦–ã®å®Ÿè¡Œ
monitor = DataDriftMonitor(df_baseline, threshold=0.05)
results, alerts = monitor.monitor_all_features(df_new)

# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
print("\n" + monitor.generate_report())

# ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
monitor.save_report('drift_report.json')
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆç›£è¦–å®Ÿè¡Œ ===
æ™‚åˆ»: 2025-10-21 10:30:45.123456
ç›£è¦–ç‰¹å¾´é‡æ•°: 10
æ–°ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«æ•°: 500

âš ï¸  ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º: feature_0
   KSçµ±è¨ˆé‡: 0.8920
   på€¤: 0.0000
   å¹³å‡å€¤ã‚·ãƒ•ãƒˆ: 412.34%

âš ï¸  ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º: feature_3
   KSçµ±è¨ˆé‡: 0.2145
   på€¤: 0.0023
   å¹³å‡å€¤ã‚·ãƒ•ãƒˆ: 87.56%

=== ç›£è¦–çµæœ ===
ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º: 2/10ç‰¹å¾´é‡
</code></pre>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>MLOpsæˆç†Ÿåº¦ãƒ¬ãƒ™ãƒ«1ã®è‡ªå‹•è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å«ã‚ã¦ãã ã•ã„ï¼š
<ul>
<li>ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•å–å¾—</li>
<li>å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</li>
<li>ãƒ¢ãƒ‡ãƒ«è¨“ç·´</li>
<li>æ€§èƒ½è©•ä¾¡ã¨é–¾å€¤åˆ¤å®š</li>
<li>MLflowã¸ã®è¨˜éŒ²</li>
<li>æ¡ä»¶ã‚’æº€ãŸã—ãŸå ´åˆã®ã¿ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²</li>
</ul>
</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report
from datetime import datetime
import logging

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AutoMLPipeline:
    """è‡ªå‹•MLè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆMLOps Level 1ï¼‰"""

    def __init__(
        self,
        experiment_name,
        model_name,
        accuracy_threshold=0.75,
        cv_folds=5
    ):
        """
        Args:
            experiment_name: MLflowå®Ÿé¨“å
            model_name: ãƒ¢ãƒ‡ãƒ«ç™»éŒ²å
            accuracy_threshold: ãƒ¢ãƒ‡ãƒ«ç™»éŒ²ã®ç²¾åº¦é–¾å€¤
            cv_folds: Cross-validationã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰æ•°
        """
        self.experiment_name = experiment_name
        self.model_name = model_name
        self.accuracy_threshold = accuracy_threshold
        self.cv_folds = cv_folds

        # MLflowå®Ÿé¨“ã®è¨­å®š
        mlflow.set_experiment(experiment_name)

        logger.info(f"è‡ªå‹•è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
        logger.info(f"å®Ÿé¨“å: {experiment_name}")
        logger.info(f"ç²¾åº¦é–¾å€¤: {accuracy_threshold}")

    def load_data(self, data_source):
        """ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•å–å¾—"""
        logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹: {data_source}")

        # å®Ÿéš›ã®ç’°å¢ƒã§ã¯ã€DBã‚„APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        # ã“ã“ã§ã¯ãƒ‡ãƒ¢ç”¨ã®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        from sklearn.datasets import make_classification

        X, y = make_classification(
            n_samples=1000,
            n_features=20,
            n_informative=15,
            n_redundant=5,
            random_state=42
        )

        df = pd.DataFrame(
            X,
            columns=[f'feature_{i}' for i in range(20)]
        )
        df['target'] = y

        # æ„å›³çš„ã«æ¬ æå€¤ã‚’è¿½åŠ 
        missing_mask = np.random.random(df.shape) < 0.05
        df = df.mask(missing_mask)

        logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ, {len(df.columns)}åˆ—")
        logger.info(f"æ¬ æå€¤: {df.isnull().sum().sum()}å€‹")

        return df

    def create_preprocessing_pipeline(self):
        """å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ"""
        return Pipeline([
            ('imputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler())
        ])

    def create_model_pipeline(self, preprocessing_pipeline):
        """ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ"""
        return Pipeline([
            ('preprocessing', preprocessing_pipeline),
            ('classifier', RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42
            ))
        ])

    def evaluate_model(self, pipeline, X_train, X_test, y_train, y_test):
        """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
        # Cross-validation
        cv_scores = cross_val_score(
            pipeline,
            X_train,
            y_train,
            cv=self.cv_folds,
            scoring='accuracy'
        )

        # ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡
        y_pred = pipeline.predict(X_test)
        test_accuracy = accuracy_score(y_test, y_pred)

        metrics = {
            'cv_mean_accuracy': cv_scores.mean(),
            'cv_std_accuracy': cv_scores.std(),
            'test_accuracy': test_accuracy
        }

        logger.info("ãƒ¢ãƒ‡ãƒ«è©•ä¾¡å®Œäº†")
        logger.info(f"CVç²¾åº¦: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
        logger.info(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_accuracy:.4f}")

        return metrics, y_pred

    def should_register_model(self, metrics):
        """ãƒ¢ãƒ‡ãƒ«ç™»éŒ²åˆ¤å®š"""
        test_acc = metrics['test_accuracy']
        cv_acc = metrics['cv_mean_accuracy']

        # æ¡ä»¶1: ãƒ†ã‚¹ãƒˆç²¾åº¦ãŒé–¾å€¤ä»¥ä¸Š
        # æ¡ä»¶2: CVç²¾åº¦ã¨ã®å·®ãŒå¤§ãã™ããªã„ï¼ˆéå­¦ç¿’ãƒã‚§ãƒƒã‚¯ï¼‰
        meets_threshold = test_acc >= self.accuracy_threshold
        not_overfitting = abs(test_acc - cv_acc) < 0.1

        should_register = meets_threshold and not_overfitting

        if should_register:
            logger.info(f"âœ“ ãƒ¢ãƒ‡ãƒ«ç™»éŒ²æ¡ä»¶ã‚’æº€ãŸã—ã¾ã—ãŸ")
        else:
            logger.warning(f"âš ï¸  ãƒ¢ãƒ‡ãƒ«ç™»éŒ²æ¡ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã›ã‚“")
            if not meets_threshold:
                logger.warning(f"   ç²¾åº¦ä¸è¶³: {test_acc:.4f} < {self.accuracy_threshold}")
            if not not_overfitting:
                logger.warning(f"   éå­¦ç¿’ã®å¯èƒ½æ€§: ãƒ†ã‚¹ãƒˆç²¾åº¦ã¨CVç²¾åº¦ã®å·®ãŒå¤§ãã„")

        return should_register

    def run_training(self, data_source):
        """è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        logger.info("=" * 50)
        logger.info(f"è‡ªå‹•è¨“ç·´é–‹å§‹: {datetime.now()}")
        logger.info("=" * 50)

        with mlflow.start_run(run_name=f"auto_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):

            # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            df = self.load_data(data_source)

            # ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã®è¨˜éŒ²
            mlflow.log_param("n_samples", len(df))
            mlflow.log_param("n_features", len(df.columns) - 1)
            mlflow.log_param("n_missing", df.isnull().sum().sum())

            # 2. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            X = df.drop('target', axis=1)
            y = df['target']

            X_train, X_test, y_train, y_test = train_test_split(
                X, y,
                test_size=0.2,
                random_state=42,
                stratify=y
            )

            mlflow.log_param("test_size", 0.2)
            mlflow.log_param("n_train", len(X_train))
            mlflow.log_param("n_test", len(X_test))

            # 3. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ
            preprocessing_pipeline = self.create_preprocessing_pipeline()
            model_pipeline = self.create_model_pipeline(preprocessing_pipeline)

            # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨˜éŒ²
            model_params = model_pipeline.named_steps['classifier'].get_params()
            mlflow.log_params({
                f"model_{k}": v for k, v in model_params.items()
                if not k.startswith('_')
            })

            # 4. ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            logger.info("ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹")
            model_pipeline.fit(X_train, y_train)
            logger.info("ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")

            # 5. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
            metrics, y_pred = self.evaluate_model(
                model_pipeline,
                X_train, X_test,
                y_train, y_test
            )

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
            mlflow.log_metrics(metrics)

            # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
            report = classification_report(y_test, y_pred, output_dict=True)
            mlflow.log_dict(report, "classification_report.json")

            # 6. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            mlflow.sklearn.log_model(
                model_pipeline,
                "model",
                signature=mlflow.models.signature.infer_signature(X_train, y_train)
            )

            # 7. ãƒ¢ãƒ‡ãƒ«ç™»éŒ²åˆ¤å®š
            if self.should_register_model(metrics):
                model_uri = f"runs:/{mlflow.active_run().info.run_id}/model"

                mlflow.register_model(
                    model_uri,
                    self.model_name
                )

                logger.info(f"âœ“ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²: {self.model_name}")
                logger.info(f"  ç²¾åº¦: {metrics['test_accuracy']:.4f}")

                # ã‚¿ã‚°è¨­å®š
                mlflow.set_tag("registered", "true")
                mlflow.set_tag("stage", "staging")
            else:
                logger.warning("ãƒ¢ãƒ‡ãƒ«ã¯ç™»éŒ²ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                mlflow.set_tag("registered", "false")

            # 8. å…±é€šã‚¿ã‚°
            mlflow.set_tag("training_type", "automatic")
            mlflow.set_tag("timestamp", datetime.now().isoformat())

            logger.info("=" * 50)
            logger.info(f"è‡ªå‹•è¨“ç·´å®Œäº†: {datetime.now()}")
            logger.info(f"Run ID: {mlflow.active_run().info.run_id}")
            logger.info("=" * 50)

            return metrics

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # è‡ªå‹•è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä½œæˆ
    auto_pipeline = AutoMLPipeline(
        experiment_name="auto_training_demo",
        model_name="auto_classifier",
        accuracy_threshold=0.75,
        cv_folds=5
    )

    # è¨“ç·´å®Ÿè¡Œ
    metrics = auto_pipeline.run_training(data_source="production_db")

    print("\n=== æœ€çµ‚çµæœ ===")
    print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {metrics['test_accuracy']:.4f}")
    print(f"CVç²¾åº¦: {metrics['cv_mean_accuracy']:.4f} Â± {metrics['cv_std_accuracy']:.4f}")
</code></pre>

<p><strong>ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œã®ä¾‹</strong>ï¼š</p>
<pre><code class="language-python">import schedule
import time

def scheduled_training():
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨“ç·´ã‚¸ãƒ§ãƒ–"""
    auto_pipeline = AutoMLPipeline(
        experiment_name="scheduled_training",
        model_name="production_model",
        accuracy_threshold=0.80
    )

    try:
        metrics = auto_pipeline.run_training("production_db")
        logger.info(f"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨“ç·´æˆåŠŸ: ç²¾åº¦={metrics['test_accuracy']:.4f}")
    except Exception as e:
        logger.error(f"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨“ç·´å¤±æ•—: {str(e)}")

# æ¯æ—¥åˆå‰2æ™‚ã«å®Ÿè¡Œ
schedule.every().day.at("02:00").do(scheduled_training)

# æ¯é€±æœˆæ›œæ—¥ã«å®Ÿè¡Œ
# schedule.every().monday.at("02:00").do(scheduled_training)

print("ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™...")
# while True:
#     schedule.run_pending()
#     time.sleep(60)
</code></pre>

</details>

<h3>å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>MLOpsæˆç†Ÿåº¦ãƒ¬ãƒ™ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚çµ„ç¹”ãŒLevel 0ã€1ã€2ã®ã©ã®ãƒ¬ãƒ™ãƒ«ã«ã‚ã‚‹ã‹ã‚’åˆ¤å®šã§ãã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<pre><code class="language-python">class MLOpsMaturityAssessment:
    """MLOpsæˆç†Ÿåº¦è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.criteria = {
            'Level 0': {
                'ãƒ‡ãƒ¼ã‚¿ç®¡ç†': [
                    'ãƒ‡ãƒ¼ã‚¿ã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã§ç®¡ç†',
                    'ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ãªã—',
                    'ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã¯æ‰‹å‹•'
                ],
                'ãƒ¢ãƒ‡ãƒ«é–‹ç™º': [
                    'Jupyter Notebookã§é–‹ç™º',
                    'å®Ÿé¨“ã®è¨˜éŒ²ã¯æ‰‹å‹•ï¼ˆExcelãªã©ï¼‰',
                    'ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯è©¦è¡ŒéŒ¯èª¤'
                ],
                'ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ': [
                    'ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã¯æ‰‹ä½œæ¥­',
                    'ãƒ‡ãƒ—ãƒ­ã‚¤é »åº¦ã¯æœˆã€œå¹´å˜ä½',
                    'æœ¬ç•ªç’°å¢ƒã¨ã®æ•´åˆæ€§ç¢ºèªãªã—'
                ],
                'ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°': [
                    'ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®ç›£è¦–ãªã—',
                    'ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½ãªã—',
                    'å•é¡Œç™ºç”Ÿæ™‚ã®å¯¾å¿œã¯äº‹å¾Œçš„'
                ]
            },
            'Level 1': {
                'ãƒ‡ãƒ¼ã‚¿ç®¡ç†': [
                    'ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ï¼ˆDVCç­‰ï¼‰',
                    'è‡ªå‹•ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯',
                    'ãƒ‡ãƒ¼ã‚¿ç³»è­œã®è¨˜éŒ²'
                ],
                'ãƒ¢ãƒ‡ãƒ«é–‹ç™º': [
                    'å®Ÿé¨“ç®¡ç†ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ï¼ˆMLflowç­‰ï¼‰',
                    'ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è‡ªå‹•è¨˜éŒ²',
                    'ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã§ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†'
                ],
                'ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ': [
                    'è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è‡ªå‹•åŒ–',
                    'ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚°',
                    'ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒã§ã®ãƒ†ã‚¹ãƒˆ'
                ],
                'ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°': [
                    'åŸºæœ¬çš„ãªæ€§èƒ½ç›£è¦–',
                    'ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º',
                    'æ‰‹å‹•ã§ã®å†è¨“ç·´ãƒˆãƒªã‚¬ãƒ¼'
                ]
            },
            'Level 2': {
                'ãƒ‡ãƒ¼ã‚¿ç®¡ç†': [
                    'ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Œå…¨è‡ªå‹•åŒ–',
                    'ç‰¹å¾´é‡ã‚¹ãƒˆã‚¢ã®æ´»ç”¨',
                    'ãƒ‡ãƒ¼ã‚¿å“è³ªã®ç¶™ç¶šçš„ç›£è¦–'
                ],
                'ãƒ¢ãƒ‡ãƒ«é–‹ç™º': [
                    'CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ',
                    'è‡ªå‹•ãƒ†ã‚¹ãƒˆï¼ˆãƒ‡ãƒ¼ã‚¿ã€ãƒ¢ãƒ‡ãƒ«ã€ã‚¤ãƒ³ãƒ•ãƒ©ï¼‰',
                    'è‡ªå‹•ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–'
                ],
                'ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ': [
                    'ã‚³ãƒ¼ãƒ‰å¤‰æ›´ã‹ã‚‰ãƒ‡ãƒ—ãƒ­ã‚¤ã¾ã§å®Œå…¨è‡ªå‹•',
                    'ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ/A/Bãƒ†ã‚¹ãƒˆ',
                    'è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½'
                ],
                'ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°': [
                    'åŒ…æ‹¬çš„ãªå¯è¦³æ¸¬æ€§',
                    'è‡ªå‹•å†è¨“ç·´ãƒˆãƒªã‚¬ãƒ¼',
                    'ç•°å¸¸æ¤œçŸ¥ã¨è‡ªå‹•å¯¾å¿œ'
                ]
            }
        }

        self.scores = {
            'Level 0': 0,
            'Level 1': 0,
            'Level 2': 0
        }

    def assess(self):
        """å¯¾è©±çš„ãªæˆç†Ÿåº¦è©•ä¾¡"""
        print("=" * 60)
        print("MLOpsæˆç†Ÿåº¦è©•ä¾¡")
        print("=" * 60)
        print("å„è³ªå•ã« 'yes' ã¾ãŸã¯ 'no' ã§ç­”ãˆã¦ãã ã•ã„\n")

        for level, categories in self.criteria.items():
            print(f"\n### {level}ã®è©•ä¾¡ ###\n")
            level_score = 0
            total_questions = 0

            for category, questions in categories.items():
                print(f"[{category}]")
                for i, question in enumerate(questions, 1):
                    total_questions += 1
                    while True:
                        answer = input(f"  {i}. {question}\n     â†’ ").strip().lower()
                        if answer in ['yes', 'no', 'y', 'n']:
                            if answer in ['yes', 'y']:
                                level_score += 1
                            break
                        else:
                            print("     'yes'ã¾ãŸã¯'no'ã§ç­”ãˆã¦ãã ã•ã„")
                print()

            self.scores[level] = (level_score / total_questions) * 100
            print(f"{level}é”æˆåº¦: {self.scores[level]:.1f}%")

        return self.scores

    def determine_level(self):
        """æˆç†Ÿåº¦ãƒ¬ãƒ™ãƒ«ã®åˆ¤å®š"""
        # Level 2ã®åŸºæº–ã‚’70%ä»¥ä¸Šæº€ãŸã™å ´åˆ
        if self.scores['Level 2'] >= 70:
            return 'Level 2', 'CI/CDå®Œå…¨è‡ªå‹•åŒ–'

        # Level 1ã®åŸºæº–ã‚’70%ä»¥ä¸Šæº€ãŸã™å ´åˆ
        elif self.scores['Level 1'] >= 70:
            return 'Level 1', 'MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è‡ªå‹•åŒ–'

        # ãã‚Œä»¥å¤–
        else:
            return 'Level 0', 'æ‰‹å‹•ãƒ—ãƒ­ã‚»ã‚¹'

    def generate_recommendations(self, current_level):
        """æ”¹å–„æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = {
            'Level 0': [
                '1. å®Ÿé¨“ç®¡ç†ãƒ„ãƒ¼ãƒ«ï¼ˆMLflowï¼‰ã®å°å…¥',
                '2. ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ï¼ˆDVCï¼‰ã®é–‹å§‹',
                '3. è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆåŒ–',
                '4. ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®æ§‹ç¯‰'
            ],
            'Level 1': [
                '1. CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰',
                '2. è‡ªå‹•ãƒ†ã‚¹ãƒˆã®å®Ÿè£…',
                '3. ãƒ¢ãƒ‡ãƒ«ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®å¼·åŒ–',
                '4. è‡ªå‹•å†è¨“ç·´ã®ä»•çµ„ã¿æ§‹ç¯‰'
            ],
            'Level 2': [
                '1. é«˜åº¦ãªA/Bãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯',
                '2. ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«ç®¡ç†ã®æœ€é©åŒ–',
                '3. MLOpsãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã®çµ±åˆ',
                '4. çµ„ç¹”å…¨ä½“ã¸ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹å±•é–‹'
            ]
        }

        return recommendations.get(current_level, [])

    def print_report(self):
        """è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã®å‡ºåŠ›"""
        print("\n" + "=" * 60)
        print("è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 60)

        # ã‚¹ã‚³ã‚¢è¡¨ç¤º
        print("\n### é”æˆåº¦ã‚¹ã‚³ã‚¢ ###")
        for level, score in self.scores.items():
            bar = "â–ˆ" * int(score / 5) + "â–‘" * (20 - int(score / 5))
            print(f"{level}: {bar} {score:.1f}%")

        # ãƒ¬ãƒ™ãƒ«åˆ¤å®š
        current_level, description = self.determine_level()
        print(f"\n### ç¾åœ¨ã®æˆç†Ÿåº¦ãƒ¬ãƒ™ãƒ« ###")
        print(f"ãƒ¬ãƒ™ãƒ«: {current_level} - {description}")

        # æ¨å¥¨äº‹é …
        print(f"\n### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸ã®æ¨å¥¨äº‹é … ###")
        recommendations = self.generate_recommendations(current_level)
        for rec in recommendations:
            print(f"  {rec}")

        print("\n" + "=" * 60)

# ä½¿ç”¨ä¾‹ï¼ˆãƒ‡ãƒ¢ç”¨ã®è‡ªå‹•è©•ä¾¡ï¼‰
def demo_assessment():
    """ãƒ‡ãƒ¢ç”¨ã®è©•ä¾¡ï¼ˆè‡ªå‹•å›ç­”ï¼‰"""
    assessment = MLOpsMaturityAssessment()

    # æ¨¡æ“¬ã‚¹ã‚³ã‚¢ï¼ˆå®Ÿéš›ã¯å¯¾è©±çš„ã«è©•ä¾¡ï¼‰
    assessment.scores = {
        'Level 0': 90.0,  # Level 0ã®åŸºæº–ã¯ã»ã¼æº€ãŸã—ã¦ã„ã‚‹
        'Level 1': 60.0,  # Level 1ã¯éƒ¨åˆ†çš„ã«é”æˆ
        'Level 2': 20.0   # Level 2ã¯ã¾ã åˆæœŸæ®µéš
    }

    assessment.print_report()

# å®Ÿéš›ã®è©•ä¾¡ã‚’å®Ÿè¡Œã™ã‚‹å ´åˆ
# assessment = MLOpsMaturityAssessment()
# assessment.assess()
# assessment.print_report()

# ãƒ‡ãƒ¢å®Ÿè¡Œ
demo_assessment()
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>============================================================
è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ
============================================================

### é”æˆåº¦ã‚¹ã‚³ã‚¢ ###
Level 0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 90.0%
Level 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 60.0%
Level 2: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 20.0%

### ç¾åœ¨ã®æˆç†Ÿåº¦ãƒ¬ãƒ™ãƒ« ###
ãƒ¬ãƒ™ãƒ«: Level 0 - æ‰‹å‹•ãƒ—ãƒ­ã‚»ã‚¹

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸ã®æ¨å¥¨äº‹é … ###
  1. å®Ÿé¨“ç®¡ç†ãƒ„ãƒ¼ãƒ«ï¼ˆMLflowï¼‰ã®å°å…¥
  2. ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ï¼ˆDVCï¼‰ã®é–‹å§‹
  3. è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆåŒ–
  4. ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®æ§‹ç¯‰

============================================================
</code></pre>

<p><strong>è©•ä¾¡åŸºæº–ã®è©³ç´°</strong>ï¼š</p>

<table>
<thead>
<tr>
<th>ã‚¹ã‚³ã‚¢ç¯„å›²</th>
<th>åˆ¤å®šãƒ¬ãƒ™ãƒ«</th>
<th>èª¬æ˜</th>
</tr>
</thead>
<tbody>
<tr>
<td>Level 2 â‰¥ 70%</td>
<td>Level 2é”æˆ</td>
<td>CI/CDå®Œå…¨è‡ªå‹•åŒ–ã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºå¯¾å¿œ</td>
</tr>
<tr>
<td>Level 1 â‰¥ 70%</td>
<td>Level 1é”æˆ</td>
<td>MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è‡ªå‹•åŒ–ã€ã‚¹ã‚±ãƒ¼ãƒ«å¯èƒ½</td>
</tr>
<tr>
<td>ä¸Šè¨˜ä»¥å¤–</td>
<td>Level 0</td>
<td>æ‰‹å‹•ãƒ—ãƒ­ã‚»ã‚¹ã€æ”¹å–„ãŒå¿…è¦</td>
</tr>
</tbody>
</table>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>GÃ©ron, A. (2022). <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em> (3rd ed.). O'Reilly Media.</li>
<li>Kreuzberger, D., KÃ¼hl, N., & Hirschl, S. (2023). Machine Learning Operations (MLOps): Overview, Definition, and Architecture. <em>IEEE Access</em>, 11, 31866-31879.</li>
<li>Google Cloud. (2023). <em>MLOps: Continuous delivery and automation pipelines in machine learning</em>. https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning</li>
<li>Huyen, C. (2022). <em>Designing Machine Learning Systems</em>. O'Reilly Media.</li>
<li>Treveil, M., et al. (2020). <em>Introducing MLOps</em>. O'Reilly Media.</li>
</ol>

<div class="navigation">
    <a href="index.html" class="nav-button">â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a>
    <a href="chapter2-experiment-tracking.html" class="nav-button">æ¬¡ã®ç« : å®Ÿé¨“ç®¡ç†ã¨ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚° â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
