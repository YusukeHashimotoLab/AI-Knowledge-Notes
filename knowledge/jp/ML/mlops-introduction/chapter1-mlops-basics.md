---
title: ç¬¬1ç« ï¼šMLOpsåŸºç¤
chapter_title: ç¬¬1ç« ï¼šMLOpsåŸºç¤
subtitle: æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®é‹ç”¨ã‚’æ”¯ãˆã‚‹åŸºç›¤æŠ€è¡“
reading_time: 25-30åˆ†
difficulty: åˆç´š
code_examples: 10
exercises: 5
---

## å­¦ç¿’ç›®æ¨™

ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š

  * âœ… MLOpsã®å®šç¾©ã¨å¿…è¦æ€§ã‚’ç†è§£ã™ã‚‹
  * âœ… æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«å…¨ä½“ã‚’æŠŠæ¡ã™ã‚‹
  * âœ… MLOpsã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ç†è§£ã™ã‚‹
  * âœ… MLOpsãƒ„ãƒ¼ãƒ«ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã®æ¦‚è¦ã‚’çŸ¥ã‚‹
  * âœ… MLOpsæˆç†Ÿåº¦ãƒ¢ãƒ‡ãƒ«ã‚’ç†è§£ã—ã€ç¾çŠ¶ã‚’è©•ä¾¡ã§ãã‚‹
  * âœ… å®Ÿè·µçš„ãªMLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åŸºç¤ã‚’æ§‹ç¯‰ã§ãã‚‹

* * *

## 1.1 MLOpsã¨ã¯

### æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®èª²é¡Œ

æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å¤šãã¯ã€PoCï¼ˆæ¦‚å¿µå®Ÿè¨¼ï¼‰æ®µéšã§çµ‚ã‚ã‚Šã€æœ¬ç•ªç’°å¢ƒã¸ã®å±•é–‹ã«å¤±æ•—ã—ã¾ã™ã€‚ãã®ä¸»ãªç†ç”±ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š

èª²é¡Œ | èª¬æ˜ | å½±éŸ¿  
---|---|---  
**ãƒ¢ãƒ‡ãƒ«ã¨ã‚³ãƒ¼ãƒ‰ã®ä¹–é›¢** | Jupyter Notebookã®ã‚³ãƒ¼ãƒ‰ãŒæœ¬ç•ªç’°å¢ƒã§å‹•ä½œã—ãªã„ | ãƒ‡ãƒ—ãƒ­ã‚¤ã®é…å»¶ã€æ‰‹ä½œæ¥­ã®å¢—åŠ   
**å†ç¾æ€§ã®æ¬ å¦‚** | åŒã˜çµæœã‚’å†ç¾ã§ããªã„ï¼ˆãƒ‡ãƒ¼ã‚¿ã€ã‚³ãƒ¼ãƒ‰ã€ç’°å¢ƒã®ä¸ä¸€è‡´ï¼‰ | ãƒ‡ãƒãƒƒã‚°å›°é›£ã€å“è³ªä½ä¸‹  
**ãƒ¢ãƒ‡ãƒ«ã®åŠ£åŒ–** | æ™‚é–“çµŒéã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®ä½ä¸‹ | äºˆæ¸¬ç²¾åº¦ã®æ‚ªåŒ–ã€ãƒ“ã‚¸ãƒã‚¹æå¤±  
**ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£** | å¤§é‡ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å¯¾å¿œã§ããªã„ | ã‚·ã‚¹ãƒ†ãƒ åœæ­¢ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹é…å»¶  
**ã‚¬ãƒãƒŠãƒ³ã‚¹ã®æ¬ å¦‚** | èª°ãŒã„ã¤ã©ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ãŸã‹ä¸æ˜ | ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹é•åã€ç›£æŸ»ä¸å¯  
  
> **çµ±è¨ˆ** : Gartnerã®èª¿æŸ»ã«ã‚ˆã‚‹ã¨ã€æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç´„85%ãŒæœ¬ç•ªç’°å¢ƒã«åˆ°é”ã—ãªã„ã¨å ±å‘Šã•ã‚Œã¦ã„ã¾ã™ã€‚

### MLOpsã®å®šç¾©ã¨ç›®çš„

**MLOpsï¼ˆMachine Learning Operationsï¼‰** ã¯ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ»é‹ç”¨ã‚’è‡ªå‹•åŒ–ãƒ»æ¨™æº–åŒ–ã™ã‚‹ãŸã‚ã®å®Ÿè·µæ‰‹æ³•ã¨ãƒ„ãƒ¼ãƒ«ç¾¤ã§ã™ã€‚

**MLOpsã®ç›®çš„** ï¼š

  * **è¿…é€Ÿãªãƒ‡ãƒ—ãƒ­ã‚¤** : ãƒ¢ãƒ‡ãƒ«ã‚’è¿…é€Ÿã‹ã¤ç¢ºå®Ÿã«æœ¬ç•ªç’°å¢ƒã¸å±•é–‹
  * **å†ç¾æ€§** : å®Ÿé¨“ã¨ãƒ¢ãƒ‡ãƒ«ã®å®Œå…¨ãªå†ç¾ã‚’ä¿è¨¼
  * **è‡ªå‹•åŒ–** : æ‰‹ä½œæ¥­ã‚’å‰Šæ¸›ã—ã€ã‚¨ãƒ©ãƒ¼ã‚’æœ€å°åŒ–
  * **ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°** : ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®ç¶™ç¶šçš„ãªç›£è¦–ã¨æ”¹å–„
  * **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£** : å¤šæ•°ã®ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œ
  * **ã‚¬ãƒãƒŠãƒ³ã‚¹** : ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ã¨ç›£æŸ»å¯¾å¿œ

### DevOps/DataOpsã¨ã®é–¢ä¿‚

MLOpsã¯ã€DevOpsã¨DataOpsã®åŸå‰‡ã‚’æ©Ÿæ¢°å­¦ç¿’ã«é©ç”¨ã—ãŸã‚‚ã®ã§ã™ï¼š

æ¦‚å¿µ | ç„¦ç‚¹ | ä¸»ãªå®Ÿè·µ  
---|---|---  
**DevOps** | ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã¨é‹ç”¨ | CI/CDã€ã‚¤ãƒ³ãƒ•ãƒ©è‡ªå‹•åŒ–ã€ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°  
**DataOps** | ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨å“è³ª | ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã€å“è³ªãƒã‚§ãƒƒã‚¯ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†  
**MLOps** | æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ« | å®Ÿé¨“ç®¡ç†ã€ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã€è‡ªå‹•å†è¨“ç·´  
      
    
    ```mermaid
    graph LR
        A[DevOps] --> D[MLOps]
        B[DataOps] --> D
        C[Machine Learning] --> D
    
        D --> E[è‡ªå‹•åŒ–ã•ã‚ŒãŸMLãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«]
    
        style A fill:#e3f2fd
        style B fill:#f3e5f5
        style C fill:#fff3e0
        style D fill:#c8e6c9
        style E fill:#ffccbc
    ```

### MLOpsãŒè§£æ±ºã™ã‚‹å•é¡Œã®å®Ÿä¾‹
    
    
    """
    å•é¡Œ: Jupyter Notebookã§é–‹ç™ºã—ãŸãƒ¢ãƒ‡ãƒ«ãŒæœ¬ç•ªç’°å¢ƒã§å‹•ä½œã—ãªã„
    
    åŸå› :
    - é–‹ç™ºç’°å¢ƒã¨æœ¬ç•ªç’°å¢ƒã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒç•°ãªã‚‹
    - ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ãŒæ–‡æ›¸åŒ–ã•ã‚Œã¦ã„ãªã„
    - ãƒ¢ãƒ‡ãƒ«ã®ä¾å­˜é–¢ä¿‚ãŒä¸æ˜ç¢º
    """
    
    # âŒ å•é¡Œã®ã‚ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆå†ç¾æ€§ãªã—ï¼‰
    import pandas as pd
    from sklearn.ensemble import RandomForestClassifier
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆã©ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼Ÿã„ã¤ï¼Ÿï¼‰
    df = pd.read_csv('data.csv')
    
    # å‰å‡¦ç†ï¼ˆã‚¹ãƒ†ãƒƒãƒ—ãŒä¸æ˜ç¢ºï¼‰
    df = df.dropna()
    X = df.drop('target', axis=1)
    y = df['target']
    
    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨˜éŒ²ãªã—ï¼‰
    model = RandomForestClassifier()
    model.fit(X, y)
    
    # ä¿å­˜ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰
    import pickle
    pickle.dump(model, open('model.pkl', 'wb'))
    
    
    
    """
    âœ… MLOpsã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆå†ç¾æ€§ã‚ã‚Šï¼‰
    
    ç‰¹å¾´:
    - ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ï¼ˆã‚³ãƒ¼ãƒ‰ã€ãƒ‡ãƒ¼ã‚¿ã€ãƒ¢ãƒ‡ãƒ«ï¼‰
    - ç’°å¢ƒã®æ˜ç¤ºï¼ˆrequirements.txt, Dockerï¼‰
    - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¨˜éŒ²ï¼ˆå®Ÿé¨“çµæœã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
    - ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ï¼ˆå†ç¾å¯èƒ½ãªå‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼‰
    """
    
    import mlflow
    import pandas as pd
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score
    import json
    from datetime import datetime
    
    # MLflowå®Ÿé¨“ã®é–‹å§‹
    mlflow.set_experiment("customer_churn_prediction")
    
    with mlflow.start_run():
        # 1. ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®è¨˜éŒ²
        data_version = "v1.2.3"
        mlflow.log_param("data_version", data_version)
    
        # 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ï¼‰
        df = pd.read_csv(f'data/{data_version}/data.csv')
    
        # 3. å‰å‡¦ç†ï¼ˆæ˜ç¤ºçš„ãªã‚¹ãƒ†ãƒƒãƒ—ï¼‰
        df = df.dropna()
        X = df.drop('target', axis=1)
        y = df['target']
    
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
    
        # 4. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨˜éŒ²
        params = {
            'n_estimators': 100,
            'max_depth': 10,
            'random_state': 42
        }
        mlflow.log_params(params)
    
        # 5. ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        model = RandomForestClassifier(**params)
        model.fit(X_train, y_train)
    
        # 6. è©•ä¾¡ã¨è¨˜éŒ²
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        mlflow.log_metric("accuracy", accuracy)
    
        # 7. ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãï¼‰
        mlflow.sklearn.log_model(
            model,
            "model",
            registered_model_name="churn_predictor"
        )
    
        # 8. è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        mlflow.set_tag("model_type", "RandomForest")
        mlflow.set_tag("created_by", "data_science_team")
        mlflow.set_tag("timestamp", datetime.now().isoformat())
    
        print(f"âœ“ ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº† - ç²¾åº¦: {accuracy:.3f}")
        print(f"âœ“ å®Ÿé¨“ID: {mlflow.active_run().info.run_id}")
    

* * *

## 1.2 MLãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«

### æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å…¨ä½“åƒ

æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€ä»¥ä¸‹ã®ãƒ•ã‚§ãƒ¼ã‚ºã§æ§‹æˆã•ã‚Œã‚‹åå¾©çš„ãªãƒ—ãƒ­ã‚»ã‚¹ã§ã™ï¼š
    
    
    ```mermaid
    graph TB
        A[ãƒ“ã‚¸ãƒã‚¹ç†è§£] --> B[ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»æº–å‚™]
        B --> C[ãƒ¢ãƒ‡ãƒ«é–‹ç™ºãƒ»è¨“ç·´]
        C --> D[ãƒ¢ãƒ‡ãƒ«è©•ä¾¡]
        D --> E{æ€§èƒ½OK?}
        E -->|No| C
        E -->|Yes| F[ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ]
        F --> G[ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°]
        G --> H{å†è¨“ç·´å¿…è¦?}
        H -->|Yes| B
        H -->|No| G
    
        style A fill:#e3f2fd
        style B fill:#fff3e0
        style C fill:#f3e5f5
        style D fill:#fce4ec
        style F fill:#c8e6c9
        style G fill:#ffccbc
    ```

### 1\. ãƒ‡ãƒ¼ã‚¿åé›†ã¨æº–å‚™

ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰ã¨å“è³ªä¿è¨¼ï¼š
    
    
    """
    ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»æº–å‚™ãƒ•ã‚§ãƒ¼ã‚º
    
    ç›®çš„:
    - é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹ç¯‰
    - ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°
    - å†ç¾å¯èƒ½ãªå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    """
    
    import pandas as pd
    import great_expectations as ge
    from sklearn.model_selection import train_test_split
    import hashlib
    import json
    
    class DataPipeline:
        """ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
        def __init__(self, data_path, version):
            self.data_path = data_path
            self.version = version
            self.metadata = {}
    
        def load_data(self):
            """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
            df = pd.read_csv(self.data_path)
    
            # ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ï¼ˆæ•´åˆæ€§ç¢ºèªç”¨ï¼‰
            data_hash = hashlib.md5(
                pd.util.hash_pandas_object(df).values
            ).hexdigest()
    
            self.metadata['data_hash'] = data_hash
            self.metadata['n_samples'] = len(df)
            self.metadata['n_features'] = len(df.columns)
    
            print(f"âœ“ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ, {len(df.columns)}åˆ—")
            print(f"âœ“ ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥: {data_hash[:8]}...")
    
            return df
    
        def validate_data(self, df):
            """ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼"""
            # Great Expectationsã‚’ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
            df_ge = ge.from_pandas(df)
    
            # æœŸå¾…å€¤ã®å®šç¾©
            expectations = []
    
            # 1. æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
            for col in df.columns:
                missing_pct = df[col].isnull().mean()
                expectations.append({
                    'column': col,
                    'check': 'missing_values',
                    'value': f"{missing_pct:.2%}"
                })
                if missing_pct > 0.3:
                    print(f"âš ï¸  è­¦å‘Š: {col}ã®æ¬ æç‡ãŒ30%ã‚’è¶…ãˆã¦ã„ã¾ã™")
    
            # 2. ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
            expectations.append({
                'check': 'data_types',
                'dtypes': df.dtypes.to_dict()
            })
    
            # 3. é‡è¤‡ãƒã‚§ãƒƒã‚¯
            n_duplicates = df.duplicated().sum()
            expectations.append({
                'check': 'duplicates',
                'value': n_duplicates
            })
    
            self.metadata['validation'] = expectations
    
            print(f"âœ“ ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å®Œäº†")
            print(f"  - é‡è¤‡è¡Œ: {n_duplicates}")
    
            return df
    
        def preprocess_data(self, df):
            """ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"""
            # æ¬ æå€¤å‡¦ç†
            df_clean = df.copy()
    
            # æ•°å€¤åˆ—: ä¸­å¤®å€¤è£œå®Œ
            numeric_cols = df_clean.select_dtypes(include=['float64', 'int64']).columns
            for col in numeric_cols:
                if df_clean[col].isnull().any():
                    median_val = df_clean[col].median()
                    df_clean[col].fillna(median_val, inplace=True)
    
            # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ—: æœ€é »å€¤è£œå®Œ
            cat_cols = df_clean.select_dtypes(include=['object']).columns
            for col in cat_cols:
                if df_clean[col].isnull().any():
                    mode_val = df_clean[col].mode()[0]
                    df_clean[col].fillna(mode_val, inplace=True)
    
            print(f"âœ“ å‰å‡¦ç†å®Œäº†")
    
            return df_clean
    
        def save_metadata(self, filepath):
            """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
            with open(filepath, 'w') as f:
                json.dump(self.metadata, f, indent=2, default=str)
            print(f"âœ“ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {filepath}")
    
    # ä½¿ç”¨ä¾‹
    pipeline = DataPipeline('customer_data.csv', 'v1.0.0')
    df = pipeline.load_data()
    df = pipeline.validate_data(df)
    df_clean = pipeline.preprocess_data(df)
    pipeline.save_metadata('data_metadata.json')
    

### 2\. ãƒ¢ãƒ‡ãƒ«é–‹ç™ºã¨è¨“ç·´

å®Ÿé¨“ç®¡ç†ã¨å†ç¾æ€§ã®ç¢ºä¿ï¼š
    
    
    """
    ãƒ¢ãƒ‡ãƒ«é–‹ç™ºãƒ»è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚º
    
    ç›®çš„:
    - ä½“ç³»çš„ãªå®Ÿé¨“ç®¡ç†
    - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–
    - ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°
    """
    
    import mlflow
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import cross_val_score
    import numpy as np
    
    class ExperimentManager:
        """å®Ÿé¨“ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
        def __init__(self, experiment_name):
            mlflow.set_experiment(experiment_name)
            self.experiment_name = experiment_name
    
        def train_and_log(self, model, X_train, y_train, model_name, params):
            """ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨è¨˜éŒ²"""
            with mlflow.start_run(run_name=model_name):
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨˜éŒ²
                mlflow.log_params(params)
    
                # Cross-validation
                cv_scores = cross_val_score(
                    model, X_train, y_train, cv=5, scoring='accuracy'
                )
    
                # è¨“ç·´
                model.fit(X_train, y_train)
    
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
                mlflow.log_metric("cv_mean_accuracy", cv_scores.mean())
                mlflow.log_metric("cv_std_accuracy", cv_scores.std())
    
                # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
                mlflow.sklearn.log_model(model, "model")
    
                print(f"âœ“ {model_name}")
                print(f"  - CVç²¾åº¦: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")
    
                return cv_scores.mean()
    
        def compare_models(self, X_train, y_train):
            """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒå®Ÿé¨“"""
            models = {
                'LogisticRegression': {
                    'model': LogisticRegression(max_iter=1000),
                    'params': {'C': 1.0, 'max_iter': 1000}
                },
                'RandomForest': {
                    'model': RandomForestClassifier(n_estimators=100, random_state=42),
                    'params': {'n_estimators': 100, 'max_depth': 10}
                },
                'GradientBoosting': {
                    'model': GradientBoostingClassifier(n_estimators=100, random_state=42),
                    'params': {'n_estimators': 100, 'learning_rate': 0.1}
                }
            }
    
            results = {}
            for name, config in models.items():
                score = self.train_and_log(
                    config['model'],
                    X_train,
                    y_train,
                    name,
                    config['params']
                )
                results[name] = score
    
            # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ
            best_model = max(results, key=results.get)
            print(f"\nğŸ† æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model} (ç²¾åº¦: {results[best_model]:.3f})")
    
            return results
    
    # ä½¿ç”¨ä¾‹
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    X, y = make_classification(
        n_samples=1000, n_features=20, n_informative=15,
        n_redundant=5, random_state=42
    )
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # å®Ÿé¨“å®Ÿè¡Œ
    exp_manager = ExperimentManager("model_comparison")
    results = exp_manager.compare_models(X_train, y_train)
    

### 3\. ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã¨é‹ç”¨

ãƒ¢ãƒ‡ãƒ«ã®æœ¬ç•ªç’°å¢ƒã¸ã®å±•é–‹ï¼š
    
    
    """
    ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ•ã‚§ãƒ¼ã‚º
    
    ç›®çš„:
    - ãƒ¢ãƒ‡ãƒ«ã®APIåŒ–
    - ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
    - A/Bãƒ†ã‚¹ãƒˆã®ã‚µãƒãƒ¼ãƒˆ
    """
    
    from flask import Flask, request, jsonify
    import mlflow.pyfunc
    import numpy as np
    import logging
    
    class ModelServer:
        """ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒãƒ¼ã‚¯ãƒ©ã‚¹"""
    
        def __init__(self, model_uri, model_version):
            """
            Args:
                model_uri: MLflowãƒ¢ãƒ‡ãƒ«ã®URI
                model_version: ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³
            """
            self.model = mlflow.pyfunc.load_model(model_uri)
            self.model_version = model_version
            self.prediction_count = 0
    
            # ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
            logging.basicConfig(level=logging.INFO)
            self.logger = logging.getLogger(__name__)
    
        def predict(self, features):
            """äºˆæ¸¬å®Ÿè¡Œ"""
            try:
                # å…¥åŠ›æ¤œè¨¼
                if not isinstance(features, (list, np.ndarray)):
                    raise ValueError("å…¥åŠ›ã¯é…åˆ—ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
    
                # äºˆæ¸¬
                prediction = self.model.predict(np.array(features).reshape(1, -1))
    
                # ã‚«ã‚¦ãƒ³ãƒˆæ›´æ–°
                self.prediction_count += 1
    
                # ãƒ­ã‚°è¨˜éŒ²
                self.logger.info(f"äºˆæ¸¬å®Ÿè¡Œ #{self.prediction_count}")
    
                return {
                    'prediction': int(prediction[0]),
                    'model_version': self.model_version,
                    'prediction_id': self.prediction_count
                }
    
            except Exception as e:
                self.logger.error(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {str(e)}")
                return {'error': str(e)}
    
    # Flask API
    app = Flask(__name__)
    
    # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆæœ¬ç•ªç’°å¢ƒã§ã¯MLflow Model Registryã‹ã‚‰ï¼‰
    model_server = ModelServer(
        model_uri="models:/churn_predictor/production",
        model_version="1.0.0"
    )
    
    @app.route('/predict', methods=['POST'])
    def predict():
        """äºˆæ¸¬ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
        data = request.get_json()
        features = data.get('features')
    
        if features is None:
            return jsonify({'error': 'featuresãŒå¿…è¦ã§ã™'}), 400
    
        result = model_server.predict(features)
    
        if 'error' in result:
            return jsonify(result), 500
    
        return jsonify(result), 200
    
    @app.route('/health', methods=['GET'])
    def health():
        """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
        return jsonify({
            'status': 'healthy',
            'model_version': model_server.model_version,
            'total_predictions': model_server.prediction_count
        }), 200
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    def sample_client():
        """APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä½¿ç”¨ä¾‹"""
        import requests
    
        # äºˆæ¸¬ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        response = requests.post(
            'http://localhost:5000/predict',
            json={'features': [0.5, 1.2, -0.3, 2.1, 0.8]}
        )
    
        if response.status_code == 200:
            result = response.json()
            print(f"äºˆæ¸¬çµæœ: {result['prediction']}")
            print(f"ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {result['model_version']}")
        else:
            print(f"ã‚¨ãƒ©ãƒ¼: {response.json()}")
    
    # if __name__ == '__main__':
    #     app.run(host='0.0.0.0', port=5000)
    

### 4\. ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨æ”¹å–„

æœ¬ç•ªç’°å¢ƒã§ã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ç›£è¦–ï¼š
    
    
    """
    ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ•ã‚§ãƒ¼ã‚º
    
    ç›®çš„:
    - ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®ç¶™ç¶šçš„ç›£è¦–
    - ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã®æ¤œå‡º
    - ã‚¢ãƒ©ãƒ¼ãƒˆã¨è‡ªå‹•å†è¨“ç·´ãƒˆãƒªã‚¬ãƒ¼
    """
    
    import numpy as np
    import pandas as pd
    from scipy import stats
    from datetime import datetime, timedelta
    
    class ModelMonitor:
        """ãƒ¢ãƒ‡ãƒ«ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚¯ãƒ©ã‚¹"""
    
        def __init__(self, baseline_data, threshold=0.05):
            """
            Args:
                baseline_data: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰
                threshold: ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã®é–¾å€¤
            """
            self.baseline_data = baseline_data
            self.threshold = threshold
            self.drift_history = []
    
        def detect_data_drift(self, new_data, feature_name):
            """ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºï¼ˆKolmogorov-Smirnovæ¤œå®šï¼‰"""
            baseline_feature = self.baseline_data[feature_name]
            new_feature = new_data[feature_name]
    
            # KSæ¤œå®š
            statistic, p_value = stats.ks_2samp(baseline_feature, new_feature)
    
            is_drift = p_value < self.threshold
    
            drift_info = {
                'timestamp': datetime.now(),
                'feature': feature_name,
                'statistic': statistic,
                'p_value': p_value,
                'drift_detected': is_drift
            }
    
            self.drift_history.append(drift_info)
    
            if is_drift:
                print(f"âš ï¸  ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º: {feature_name}")
                print(f"   KSçµ±è¨ˆé‡: {statistic:.4f}, på€¤: {p_value:.4f}")
    
            return is_drift
    
        def monitor_predictions(self, predictions, actuals=None):
            """äºˆæ¸¬ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°"""
            monitoring_report = {
                'timestamp': datetime.now(),
                'n_predictions': len(predictions),
                'prediction_distribution': {
                    'mean': np.mean(predictions),
                    'std': np.std(predictions),
                    'min': np.min(predictions),
                    'max': np.max(predictions)
                }
            }
    
            # å®Ÿæ¸¬å€¤ãŒã‚ã‚‹å ´åˆã¯ç²¾åº¦ã‚’è¨ˆç®—
            if actuals is not None:
                accuracy = np.mean(predictions == actuals)
                monitoring_report['accuracy'] = accuracy
    
                if accuracy < 0.7:  # é–¾å€¤ä¾‹
                    print(f"âš ï¸  ç²¾åº¦ä½ä¸‹æ¤œå‡º: {accuracy:.3f}")
                    print("   å†è¨“ç·´ã‚’æ¨å¥¨ã—ã¾ã™")
    
            return monitoring_report
    
        def generate_report(self):
            """ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
            if not self.drift_history:
                return "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"
    
            df_drift = pd.DataFrame(self.drift_history)
    
            report = f"""
    === ãƒ¢ãƒ‡ãƒ«ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆ ===
    æœŸé–“: {df_drift['timestamp'].min()} ~ {df_drift['timestamp'].max()}
    ç·ãƒã‚§ãƒƒã‚¯æ•°: {len(df_drift)}
    ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºæ•°: {df_drift['drift_detected'].sum()}
    
    ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã•ã‚ŒãŸç‰¹å¾´é‡:
    {df_drift[df_drift['drift_detected']][['feature', 'p_value']].to_string()}
            """
    
            return report
    
    # ä½¿ç”¨ä¾‹
    from sklearn.datasets import make_classification
    
    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼ˆè¨“ç·´æ™‚ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
    X_baseline, _ = make_classification(
        n_samples=1000, n_features=5, random_state=42
    )
    df_baseline = pd.DataFrame(
        X_baseline,
        columns=[f'feature_{i}' for i in range(5)]
    )
    
    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ¬ç•ªç’°å¢ƒã§ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼‰
    # ã‚·ãƒ•ãƒˆã‚’åŠ ãˆã¦ãƒ‰ãƒªãƒ•ãƒˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    X_new, _ = make_classification(
        n_samples=500, n_features=5, random_state=43
    )
    X_new[:, 0] += 1.5  # feature_0ã«ã‚·ãƒ•ãƒˆã‚’è¿½åŠ 
    df_new = pd.DataFrame(
        X_new,
        columns=[f'feature_{i}' for i in range(5)]
    )
    
    # ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
    monitor = ModelMonitor(df_baseline, threshold=0.05)
    
    print("=== ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º ===")
    for col in df_baseline.columns:
        monitor.detect_data_drift(df_new, col)
    
    print("\n" + monitor.generate_report())
    

* * *

## 1.3 MLOpsã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

### 1\. ãƒ‡ãƒ¼ã‚¿ç®¡ç†

ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã€å“è³ªç®¡ç†ã€ç³»è­œè¿½è·¡ï¼š

ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | ç›®çš„ | ãƒ„ãƒ¼ãƒ«ä¾‹  
---|---|---  
**ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°** | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¤‰æ›´å±¥æ­´ç®¡ç† | DVC, LakeFS, Delta Lake  
**ãƒ‡ãƒ¼ã‚¿å“è³ª** | ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã¨ç•°å¸¸æ¤œçŸ¥ | Great Expectations, Deequ  
**ãƒ‡ãƒ¼ã‚¿ç³»è­œ** | ãƒ‡ãƒ¼ã‚¿ã®èµ·æºã¨å¤‰æ›å±¥æ­´ | Apache Atlas, Marquez  
**ç‰¹å¾´é‡ã‚¹ãƒˆã‚¢** | ç‰¹å¾´é‡ã®å†åˆ©ç”¨ã¨ä¸€è²«æ€§ | Feast, Tecton  
      
    
    """
    ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã®å®Ÿè£…ä¾‹ï¼ˆDVCé¢¨ï¼‰
    """
    
    import os
    import hashlib
    import json
    from pathlib import Path
    
    class SimpleDataVersioning:
        """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ """
    
        def __init__(self, storage_dir='.data_versions'):
            self.storage_dir = Path(storage_dir)
            self.storage_dir.mkdir(exist_ok=True)
            self.manifest_file = self.storage_dir / 'manifest.json'
            self.manifest = self._load_manifest()
    
        def _load_manifest(self):
            """ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
            if self.manifest_file.exists():
                with open(self.manifest_file, 'r') as f:
                    return json.load(f)
            return {}
    
        def _save_manifest(self):
            """ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜"""
            with open(self.manifest_file, 'w') as f:
                json.dump(self.manifest, f, indent=2)
    
        def _compute_hash(self, filepath):
            """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤è¨ˆç®—"""
            hasher = hashlib.md5()
            with open(filepath, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b''):
                    hasher.update(chunk)
            return hasher.hexdigest()
    
        def add(self, filepath, version_tag):
            """ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã«è¿½åŠ """
            filepath = Path(filepath)
    
            if not filepath.exists():
                raise FileNotFoundError(f"{filepath}ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
            # ãƒãƒƒã‚·ãƒ¥å€¤è¨ˆç®—
            file_hash = self._compute_hash(filepath)
    
            # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ã‚³ãƒ”ãƒ¼
            storage_path = self.storage_dir / f"{version_tag}_{file_hash[:8]}"
            import shutil
            shutil.copy(filepath, storage_path)
    
            # ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆæ›´æ–°
            self.manifest[version_tag] = {
                'original_path': str(filepath),
                'storage_path': str(storage_path),
                'hash': file_hash,
                'size': filepath.stat().st_size,
                'timestamp': str(pd.Timestamp.now())
            }
    
            self._save_manifest()
    
            print(f"âœ“ {filepath.name}ã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ³{version_tag}ã¨ã—ã¦è¿½åŠ ")
            print(f"  ãƒãƒƒã‚·ãƒ¥: {file_hash[:8]}...")
    
        def checkout(self, version_tag, output_path=None):
            """ç‰¹å®šãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
            if version_tag not in self.manifest:
                raise ValueError(f"ãƒãƒ¼ã‚¸ãƒ§ãƒ³{version_tag}ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
            version_info = self.manifest[version_tag]
            storage_path = Path(version_info['storage_path'])
    
            if output_path is None:
                output_path = version_info['original_path']
    
            import shutil
            shutil.copy(storage_path, output_path)
    
            print(f"âœ“ ãƒãƒ¼ã‚¸ãƒ§ãƒ³{version_tag}ã‚’ãƒã‚§ãƒƒã‚¯ã‚¢ã‚¦ãƒˆ")
            print(f"  å‡ºåŠ›å…ˆ: {output_path}")
    
        def list_versions(self):
            """å…¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ä¸€è¦§è¡¨ç¤º"""
            if not self.manifest:
                print("ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“")
                return
    
            print("=== ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¸€è¦§ ===")
            for tag, info in self.manifest.items():
                print(f"\nãƒãƒ¼ã‚¸ãƒ§ãƒ³: {tag}")
                print(f"  ãƒ‘ã‚¹: {info['original_path']}")
                print(f"  ã‚µã‚¤ã‚º: {info['size']:,} bytes")
                print(f"  ãƒãƒƒã‚·ãƒ¥: {info['hash'][:8]}...")
                print(f"  ä½œæˆæ—¥æ™‚: {info['timestamp']}")
    
    # ä½¿ç”¨ä¾‹ï¼ˆãƒ‡ãƒ¢ï¼‰
    # dvc = SimpleDataVersioning()
    # dvc.add('data.csv', 'v1.0.0')
    # dvc.add('data.csv', 'v1.1.0')  # ãƒ‡ãƒ¼ã‚¿æ›´æ–°å¾Œ
    # dvc.list_versions()
    # dvc.checkout('v1.0.0', 'data_old.csv')
    

### 2\. ãƒ¢ãƒ‡ãƒ«ç®¡ç†

ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«å…¨ä½“ã‚’ç®¡ç†ï¼š

ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | ç›®çš„ | æ©Ÿèƒ½  
---|---|---  
**å®Ÿé¨“ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°** | å®Ÿé¨“çµæœã®è¨˜éŒ²ã¨æ¯”è¼ƒ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€æˆæœç‰©ã®è¨˜éŒ²  
**ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒª** | ãƒ¢ãƒ‡ãƒ«ã®ä¸­å¤®ç®¡ç† | ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã€ã‚¹ãƒ†ãƒ¼ã‚¸ç®¡ç†ã€æ‰¿èªãƒ•ãƒ­ãƒ¼  
**ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚°** | ãƒ‡ãƒ—ãƒ­ã‚¤å¯èƒ½ãªå½¢å¼ã«å¤‰æ› | ä¾å­˜é–¢ä¿‚ã®è§£æ±ºã€ã‚³ãƒ³ãƒ†ãƒŠåŒ–  
  
### 3\. ã‚¤ãƒ³ãƒ•ãƒ©ç®¡ç†

ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã§å†ç¾å¯èƒ½ãªã‚¤ãƒ³ãƒ•ãƒ©ï¼š

ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | ç›®çš„ | ãƒ„ãƒ¼ãƒ«ä¾‹  
---|---|---  
**ã‚³ãƒ³ãƒ†ãƒŠåŒ–** | ç’°å¢ƒã®ä¸€è²«æ€§ç¢ºä¿ | Docker, Kubernetes  
**ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³** | ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®è‡ªå‹•åŒ– | Airflow, Kubeflow, Argo  
**ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†** | è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã®åŠ¹ç‡çš„åˆ©ç”¨ | Kubernetes, Ray  
  
### 4\. ã‚¬ãƒãƒŠãƒ³ã‚¹

ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ã¨ç›£æŸ»å¯¾å¿œï¼š

è¦ç´  | å†…å®¹  
---|---  
**ãƒ¢ãƒ‡ãƒ«èª¬æ˜å¯èƒ½æ€§** | äºˆæ¸¬ã®æ ¹æ‹ ã‚’èª¬æ˜  
**ãƒã‚¤ã‚¢ã‚¹æ¤œå‡º** | å…¬å¹³æ€§ã®æ¤œè¨¼  
**ç›£æŸ»ãƒ­ã‚°** | å…¨ã¦ã®å¤‰æ›´å±¥æ­´ã‚’è¨˜éŒ²  
**ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡** | æ¨©é™ç®¡ç†ã¨æ‰¿èªãƒ•ãƒ­ãƒ¼  
  
* * *

## 1.4 MLOpsãƒ„ãƒ¼ãƒ«ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ 

### å®Ÿé¨“ç®¡ç†ãƒ„ãƒ¼ãƒ«

ãƒ„ãƒ¼ãƒ« | ç‰¹å¾´ | ä¸»ãªç”¨é€”  
---|---|---  
**MLflow** | ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã€å¤šæ©Ÿèƒ½ | å®Ÿé¨“ç®¡ç†ã€ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã€ãƒ‡ãƒ—ãƒ­ã‚¤  
**Weights & Biases** | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–ã€ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ | å®Ÿé¨“æ¯”è¼ƒã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–  
**Neptune.ai** | ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã«ç‰¹åŒ– | é•·æœŸçš„ãªå®Ÿé¨“ç®¡ç†ã€ãƒãƒ¼ãƒ å”åƒ  
  
### ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

ãƒ„ãƒ¼ãƒ« | ç‰¹å¾´ | ä¸»ãªç”¨é€”  
---|---|---  
**Kubeflow** | Kubernetesä¸Šã®ML | ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³  
**Apache Airflow** | æ±ç”¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ | ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°  
**Prefect** | Pythonãƒã‚¤ãƒ†ã‚£ãƒ–ã€ãƒ¢ãƒ€ãƒ³API | ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°  
  
### ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

ãƒ„ãƒ¼ãƒ« | ç‰¹å¾´ | ä¸»ãªç”¨é€”  
---|---|---  
**BentoML** | ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒ“ãƒ³ã‚°ç‰¹åŒ– | REST APIã€ãƒãƒƒãƒæ¨è«–  
**Seldon Core** | Kubernetesä¸Šã®ãƒ‡ãƒ—ãƒ­ã‚¤ | ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ã€A/Bãƒ†ã‚¹ãƒˆ  
**TensorFlow Serving** | TensorFlowå°‚ç”¨ | é«˜é€Ÿæ¨è«–ã€GPUå¯¾å¿œ  
  
### ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«

ãƒ„ãƒ¼ãƒ« | ç‰¹å¾´ | ä¸»ãªç”¨é€”  
---|---|---  
**Evidently** | ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º | ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ç›£è¦–ã€ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ  
**Prometheus + Grafana** | æ±ç”¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦– | ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ã€ã‚¢ãƒ©ãƒ¼ãƒˆ  
**Arize AI** | MLç‰¹åŒ–ã®å¯è¦³æ¸¬æ€§ | ãƒ¢ãƒ‡ãƒ«ç›£è¦–ã€æ ¹æœ¬åŸå› åˆ†æ  
  
### çµ±åˆãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 

ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ  | ç‰¹å¾´  
---|---  
**AWS SageMaker** | AWSãƒã‚¤ãƒ†ã‚£ãƒ–ã€ãƒ•ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ‰  
**Azure ML** | Azureã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ  
**Google Vertex AI** | GCPã‚µãƒ¼ãƒ“ã‚¹çµ±åˆã€AutoML  
**Databricks** | ãƒ‡ãƒ¼ã‚¿+MLçµ±åˆã€SparkåŸºç›¤  
  
* * *

## 1.5 MLOpsã®æˆç†Ÿåº¦ãƒ¢ãƒ‡ãƒ«

çµ„ç¹”ã®MLOpsæˆç†Ÿåº¦ã‚’è©•ä¾¡ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆGoogleæå”±ï¼‰ï¼š

### Level 0: Manual Processï¼ˆæ‰‹å‹•ãƒ—ãƒ­ã‚»ã‚¹ï¼‰

**ç‰¹å¾´** ï¼š

  * å…¨ã¦ã®ã‚¹ãƒ†ãƒƒãƒ—ãŒæ‰‹å‹•
  * Jupyter Notebookãƒ™ãƒ¼ã‚¹ã®é–‹ç™º
  * ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã¯æ‰‹ä½œæ¥­
  * å†ç¾æ€§ãªã—

**èª²é¡Œ** ï¼š

  * ã‚¹ã‚±ãƒ¼ãƒ«ã—ãªã„
  * ã‚¨ãƒ©ãƒ¼ãŒå¤šç™º
  * ãƒ‡ãƒ—ãƒ­ã‚¤ã«æ™‚é–“ãŒã‹ã‹ã‚‹
  * ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãªã—

### Level 1: ML Pipeline Automationï¼ˆMLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è‡ªå‹•åŒ–ï¼‰

**ç‰¹å¾´** ï¼š

  * è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è‡ªå‹•åŒ–
  * ç¶™ç¶šçš„ãªè¨“ç·´ï¼ˆCT: Continuous Trainingï¼‰
  * å®Ÿé¨“ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
  * ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®ä½¿ç”¨

**å®Ÿç¾ã™ã‚‹ã“ã¨** ï¼š

  * æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§ã®è‡ªå‹•å†è¨“ç·´
  * ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°
  * åŸºæœ¬çš„ãªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

    
    
    """
    Level 1ã®å®Ÿè£…ä¾‹: è‡ªå‹•è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    """
    
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.ensemble import RandomForestClassifier
    import mlflow
    import schedule
    import time
    
    class AutoTrainingPipeline:
        """è‡ªå‹•è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
        def __init__(self, experiment_name):
            mlflow.set_experiment(experiment_name)
            self.experiment_name = experiment_name
    
        def create_pipeline(self):
            """MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰"""
            return Pipeline([
                ('scaler', StandardScaler()),
                ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
            ])
    
        def train(self, X_train, y_train, X_val, y_val):
            """è¨“ç·´å®Ÿè¡Œ"""
            with mlflow.start_run():
                # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ
                pipeline = self.create_pipeline()
    
                # è¨“ç·´
                pipeline.fit(X_train, y_train)
    
                # è©•ä¾¡
                train_score = pipeline.score(X_train, y_train)
                val_score = pipeline.score(X_val, y_val)
    
                # MLflowã«è¨˜éŒ²
                mlflow.log_metric("train_accuracy", train_score)
                mlflow.log_metric("val_accuracy", val_score)
                mlflow.sklearn.log_model(pipeline, "model")
    
                # ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²
                if val_score > 0.8:  # é–¾å€¤ã‚’è¶…ãˆãŸå ´åˆã®ã¿
                    mlflow.register_model(
                        f"runs:/{mlflow.active_run().info.run_id}/model",
                        "production_model"
                    )
                    print(f"âœ“ æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ²ï¼ˆæ¤œè¨¼ç²¾åº¦: {val_score:.3f}ï¼‰")
                else:
                    print(f"âš ï¸  ç²¾åº¦ãŒé–¾å€¤æœªæº€ï¼ˆ{val_score:.3f} < 0.8ï¼‰")
    
                return pipeline
    
        def scheduled_training(self, data_loader, schedule_time="00:00"):
            """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨“ç·´"""
            def job():
                print(f"=== è‡ªå‹•è¨“ç·´é–‹å§‹: {pd.Timestamp.now()} ===")
                X_train, X_val, y_train, y_val = data_loader()
                self.train(X_train, X_val, y_train, y_val)
    
            # æ¯æ—¥æŒ‡å®šæ™‚åˆ»ã«å®Ÿè¡Œ
            schedule.every().day.at(schedule_time).do(job)
    
            print(f"âœ“ è‡ªå‹•è¨“ç·´ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š: æ¯æ—¥{schedule_time}")
    
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œï¼ˆå®Ÿéš›ã®ç’°å¢ƒã§ã¯ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã¨ã—ã¦å®Ÿè¡Œï¼‰
            # while True:
            #     schedule.run_pending()
            #     time.sleep(60)
    
    # ä½¿ç”¨ä¾‹ï¼ˆãƒ‡ãƒ¢ï¼‰
    # def load_latest_data():
    #     # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãƒ­ã‚¸ãƒƒã‚¯
    #     return X_train, X_val, y_train, y_val
    #
    # pipeline = AutoTrainingPipeline("auto_training")
    # pipeline.scheduled_training(load_latest_data, "02:00")
    

### Level 2: CI/CD Pipeline Automationï¼ˆCI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è‡ªå‹•åŒ–ï¼‰

**ç‰¹å¾´** ï¼š

  * å®Œå…¨ãªè‡ªå‹•åŒ–ï¼ˆã‚³ãƒ¼ãƒ‰å¤‰æ›´ã‹ã‚‰ãƒ‡ãƒ—ãƒ­ã‚¤ã¾ã§ï¼‰
  * CI/CDçµ±åˆ
  * è‡ªå‹•ãƒ†ã‚¹ãƒˆï¼ˆãƒ‡ãƒ¼ã‚¿ã€ãƒ¢ãƒ‡ãƒ«ã€ã‚¤ãƒ³ãƒ•ãƒ©ï¼‰
  * åŒ…æ‹¬çš„ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

**å®Ÿç¾ã™ã‚‹ã“ã¨** ï¼š

  * ã‚³ãƒ¼ãƒ‰å¤‰æ›´ã®è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤
  * A/Bãƒ†ã‚¹ãƒˆ
  * ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
  * è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯

    
    
    ```mermaid
    graph TB
        subgraph "Level 0: Manual"
            A1[Notebooké–‹ç™º] --> A2[æ‰‹å‹•è¨“ç·´]
            A2 --> A3[æ‰‹å‹•ãƒ‡ãƒ—ãƒ­ã‚¤]
        end
    
        subgraph "Level 1: ML Pipeline"
            B1[ã‚³ãƒ¼ãƒ‰é–‹ç™º] --> B2[è‡ªå‹•è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³]
            B2 --> B3[ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒª]
            B3 --> B4[æ‰‹å‹•ãƒ‡ãƒ—ãƒ­ã‚¤æ‰¿èª]
            B4 --> B5[ãƒ‡ãƒ—ãƒ­ã‚¤]
        end
    
        subgraph "Level 2: CI/CD"
            C1[ã‚³ãƒ¼ãƒ‰å¤‰æ›´] --> C2[CI: è‡ªå‹•ãƒ†ã‚¹ãƒˆ]
            C2 --> C3[è‡ªå‹•è¨“ç·´]
            C3 --> C4[è‡ªå‹•æ¤œè¨¼]
            C4 --> C5[CD: è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤]
            C5 --> C6[ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°]
            C6 --> C7{æ€§èƒ½OK?}
            C7 -->|No| C8[è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯]
            C7 -->|Yes| C6
        end
    
        style A1 fill:#ffebee
        style B2 fill:#fff3e0
        style C5 fill:#e8f5e9
    ```

### æˆç†Ÿåº¦ãƒ¬ãƒ™ãƒ«ã®æ¯”è¼ƒ

å´é¢ | Level 0 | Level 1 | Level 2  
---|---|---|---  
**ãƒ‡ãƒ—ãƒ­ã‚¤é »åº¦** | æœˆã€œå¹´å˜ä½ | é€±ã€œæœˆå˜ä½ | æ—¥ã€œé€±å˜ä½  
**å†ç¾æ€§** | ä½ã„ | ä¸­ç¨‹åº¦ | é«˜ã„  
**è‡ªå‹•åŒ–** | ãªã— | è¨“ç·´ã®ã¿ | ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰  
**ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°** | ãªã—/æ‰‹å‹• | åŸºæœ¬çš„ | åŒ…æ‹¬çš„  
**ãƒ†ã‚¹ãƒˆ** | ãªã— | ãƒ¢ãƒ‡ãƒ«ã®ã¿ | å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ  
**é©ç”¨è¦æ¨¡** | 1-2ãƒ¢ãƒ‡ãƒ« | æ•°ãƒ¢ãƒ‡ãƒ« | å¤šæ•°ã®ãƒ¢ãƒ‡ãƒ«  
  
* * *

## 1.6 æœ¬ç« ã®ã¾ã¨ã‚

### å­¦ã‚“ã ã“ã¨

  1. **MLOpsã®å¿…è¦æ€§**

     * æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®85%ãŒæœ¬ç•ªç’°å¢ƒã«åˆ°é”ã—ãªã„
     * MLOpsã¯é–‹ç™ºã‹ã‚‰é‹ç”¨ã¾ã§ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’åŸ‹ã‚ã‚‹
     * DevOpsã€DataOpsã€MLã®çµ±åˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
  2. **MLãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«**

     * ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»æº–å‚™ã€ãƒ¢ãƒ‡ãƒ«é–‹ç™ºã€ãƒ‡ãƒ—ãƒ­ã‚¤ã€ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã®4ãƒ•ã‚§ãƒ¼ã‚º
     * åå¾©çš„ãƒ»ç¶™ç¶šçš„ãªãƒ—ãƒ­ã‚»ã‚¹
     * å„ãƒ•ã‚§ãƒ¼ã‚ºã§ã®è‡ªå‹•åŒ–ã¨å“è³ªä¿è¨¼ãŒé‡è¦
  3. **ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ**

     * ãƒ‡ãƒ¼ã‚¿ç®¡ç†: ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã€å“è³ªã€ç³»è­œ
     * ãƒ¢ãƒ‡ãƒ«ç®¡ç†: å®Ÿé¨“ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã€ãƒ¬ã‚¸ã‚¹ãƒˆãƒª
     * ã‚¤ãƒ³ãƒ•ãƒ©ç®¡ç†: ã‚³ãƒ³ãƒ†ãƒŠåŒ–ã€ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
     * ã‚¬ãƒãƒŠãƒ³ã‚¹: èª¬æ˜å¯èƒ½æ€§ã€ç›£æŸ»ã€ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡
  4. **ãƒ„ãƒ¼ãƒ«ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ **

     * å®Ÿé¨“ç®¡ç†: MLflow, Weights & Biases
     * ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: Kubeflow, Airflow
     * ãƒ‡ãƒ—ãƒ­ã‚¤: BentoML, Seldon
     * ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°: Evidently, Prometheus
  5. **æˆç†Ÿåº¦ãƒ¢ãƒ‡ãƒ«**

     * Level 0: å®Œå…¨æ‰‹å‹•ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ã—ãªã„ï¼‰
     * Level 1: è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è‡ªå‹•åŒ–
     * Level 2: CI/CDå®Œå…¨è‡ªå‹•åŒ–ï¼ˆã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºå¯¾å¿œï¼‰

### MLOpså°å…¥ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

åŸå‰‡ | èª¬æ˜  
---|---  
**å°ã•ãå§‹ã‚ã‚‹** | Level 0 â†’ Level 1 â†’ Level 2 ã¨æ®µéšçš„ã«é€²åŒ–  
**è‡ªå‹•åŒ–å„ªå…ˆ** | æ‰‹ä½œæ¥­ã‚’æœ€å°åŒ–ã—ã€ã‚¨ãƒ©ãƒ¼ã‚’å‰Šæ¸›  
**ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°å¿…é ˆ** | æœ¬ç•ªç’°å¢ƒã§ã®æ€§èƒ½ã‚’ç¶™ç¶šçš„ã«ç›£è¦–  
**å†ç¾æ€§ç¢ºä¿** | å…¨ã¦ã®å®Ÿé¨“ã¨ãƒ¢ãƒ‡ãƒ«ã‚’å†ç¾å¯èƒ½ã«  
**ãƒãƒ¼ãƒ å”åƒ** | ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã¨ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã®å”åŠ›  
  
### æ¬¡ã®ç« ã¸

ç¬¬2ç« ã§ã¯ã€**å®Ÿé¨“ç®¡ç†ã¨ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°** ã‚’è©³ã—ãå­¦ã³ã¾ã™ï¼š

  * MLflowã«ã‚ˆã‚‹å®Ÿé¨“ç®¡ç†
  * ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
  * ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®æ´»ç”¨
  * å®Ÿé¨“çµæœã®å¯è¦–åŒ–ã¨æ¯”è¼ƒ
  * ãƒãƒ¼ãƒ ã§ã®å®Ÿé¨“å…±æœ‰

* * *

## æ¼”ç¿’å•é¡Œ

### å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰

MLOpsã¨DevOpsã®é•ã„ã‚’3ã¤æŒ™ã’ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚æ©Ÿæ¢°å­¦ç¿’ç‰¹æœ‰ã®èª²é¡Œã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãã ã•ã„ã€‚

è§£ç­”ä¾‹

**è§£ç­”** ï¼š

  1. **ãƒ‡ãƒ¼ã‚¿ã®æ‰±ã„**

     * **DevOps** : ã‚³ãƒ¼ãƒ‰ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ãŒä¸­å¿ƒ
     * **MLOps** : ã‚³ãƒ¼ãƒ‰ã€ãƒ‡ãƒ¼ã‚¿ã€ãƒ¢ãƒ‡ãƒ«ã®3ã¤ã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
     * æ©Ÿæ¢°å­¦ç¿’ã§ã¯ã€åŒã˜ã‚³ãƒ¼ãƒ‰ã§ã‚‚ãƒ‡ãƒ¼ã‚¿ãŒç•°ãªã‚Œã°çµæœãŒå¤‰ã‚ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ãŒå¿…é ˆ
  2. **ãƒ†ã‚¹ãƒˆã®è¤‡é›‘æ€§**

     * **DevOps** : æ±ºå®šè«–çš„ãªãƒ†ã‚¹ãƒˆï¼ˆåŒã˜å…¥åŠ› â†’ åŒã˜å‡ºåŠ›ï¼‰
     * **MLOps** : ç¢ºç‡çš„ãªãƒ†ã‚¹ãƒˆï¼ˆãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã€ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã€ãƒã‚¤ã‚¢ã‚¹ãªã©ï¼‰
     * ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆã¯ç²¾åº¦ã ã‘ã§ãªãã€å…¬å¹³æ€§ã€è§£é‡ˆå¯èƒ½æ€§ãªã©ã‚‚è©•ä¾¡ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
  3. **ç¶™ç¶šçš„ãªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°**

     * **DevOps** : ã‚·ã‚¹ãƒ†ãƒ ã®ç¨¼åƒçŠ¶æ…‹ã€ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒ¼ãƒˆã‚’ç›£è¦–
     * **MLOps** : ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½åŠ£åŒ–ã€ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã€äºˆæ¸¬åˆ†å¸ƒã®å¤‰åŒ–ã‚’ç›£è¦–
     * æ™‚é–“çµŒéã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ã®åŠ£åŒ–ãŒé¿ã‘ã‚‰ã‚Œãªã„ãŸã‚ã€è‡ªå‹•å†è¨“ç·´ã®ä»•çµ„ã¿ãŒå¿…è¦

### å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰

ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’æ”¹å–„ã—ã¦ã€MLOpsã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«å¾“ã£ãŸå®Ÿé¨“ç®¡ç†ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚MLflowã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€ãƒ¢ãƒ‡ãƒ«ã‚’è¨˜éŒ²ã—ã¦ãã ã•ã„ã€‚
    
    
    import pandas as pd
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = pd.read_csv('data.csv')
    X = df.drop('target', axis=1)
    y = df['target']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    
    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    model = RandomForestClassifier(n_estimators=100)
    model.fit(X_train, y_train)
    
    # è©•ä¾¡
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy}")
    

è§£ç­”ä¾‹
    
    
    import pandas as pd
    import mlflow
    import mlflow.sklearn
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    import hashlib
    import json
    
    # MLflowå®Ÿé¨“ã®è¨­å®š
    mlflow.set_experiment("customer_classification")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®è¨ˆç®—
    def compute_data_version(df):
        """ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ã—ã¦ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ã—ã¦ä½¿ç”¨"""
        data_str = pd.util.hash_pandas_object(df).values.tobytes()
        return hashlib.md5(data_str).hexdigest()[:8]
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = pd.read_csv('data.csv')
    data_version = compute_data_version(df)
    
    X = df.drop('target', axis=1)
    y = df['target']
    
    # å®Ÿé¨“é–‹å§‹
    with mlflow.start_run(run_name="rf_baseline"):
    
        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å®šç¾©
        params = {
            'n_estimators': 100,
            'max_depth': 10,
            'min_samples_split': 5,
            'random_state': 42
        }
    
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆå›ºå®šã‚·ãƒ¼ãƒ‰ã§å†ç¾æ€§ç¢ºä¿ï¼‰
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
    
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨˜éŒ²
        mlflow.log_params(params)
        mlflow.log_param("data_version", data_version)
        mlflow.log_param("test_size", 0.2)
        mlflow.log_param("n_train_samples", len(X_train))
        mlflow.log_param("n_test_samples", len(X_test))
    
        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        model = RandomForestClassifier(**params)
        model.fit(X_train, y_train)
    
        # Cross-validation
        cv_scores = cross_val_score(model, X_train, y_train, cv=5)
        mlflow.log_metric("cv_mean_accuracy", cv_scores.mean())
        mlflow.log_metric("cv_std_accuracy", cv_scores.std())
    
        # ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®è©•ä¾¡
        y_pred = model.predict(X_test)
    
        # è¤‡æ•°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²
        metrics = {
            'test_accuracy': accuracy_score(y_test, y_pred),
            'test_precision': precision_score(y_test, y_pred, average='weighted'),
            'test_recall': recall_score(y_test, y_pred, average='weighted'),
            'test_f1': f1_score(y_test, y_pred, average='weighted')
        }
    
        mlflow.log_metrics(metrics)
    
        # ç‰¹å¾´é‡é‡è¦åº¦ã®è¨˜éŒ²
        feature_importance = dict(zip(X.columns, model.feature_importances_))
        mlflow.log_dict(feature_importance, "feature_importance.json")
    
        # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        mlflow.sklearn.log_model(
            model,
            "model",
            registered_model_name="customer_classifier"
        )
    
        # ã‚¿ã‚°ã®è¨­å®š
        mlflow.set_tag("model_type", "RandomForest")
        mlflow.set_tag("framework", "scikit-learn")
        mlflow.set_tag("environment", "development")
    
        # çµæœã®è¡¨ç¤º
        print("=== å®Ÿé¨“çµæœ ===")
        print(f"Run ID: {mlflow.active_run().info.run_id}")
        print(f"Data Version: {data_version}")
        print(f"\nMetrics:")
        for metric, value in metrics.items():
            print(f"  {metric}: {value:.4f}")
        print(f"\nCV Accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
    
        # ãƒ¢ãƒ‡ãƒ«ã®ç™»éŒ²ç¢ºèª
        print(f"\nâœ“ ãƒ¢ãƒ‡ãƒ«ã‚’MLflowã«è¨˜éŒ²ã—ã¾ã—ãŸ")
        print(f"âœ“ å®Ÿé¨“å: customer_classification")
    

**æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ** ï¼š

  1. MLflowã§å®Ÿé¨“ã‚’ç®¡ç†
  2. ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®è¨˜éŒ²
  3. å…¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨˜éŒ²
  4. è¤‡æ•°ã®è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²
  5. Cross-validationã®å®Ÿæ–½
  6. ç‰¹å¾´é‡é‡è¦åº¦ã®ä¿å­˜
  7. ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã¸ã®ç™»éŒ²
  8. å†ç¾æ€§ã®ç¢ºä¿ï¼ˆrandom_stateã€stratifyï¼‰

### å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰

ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚Kolmogorov-Smirnovæ¤œå®šã‚’ä½¿ç”¨ã—ã¦ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã¨çµ±è¨ˆçš„ã«ç•°ãªã‚‹ã‹ã‚’åˆ¤å®šã—ã€ã‚¢ãƒ©ãƒ¼ãƒˆã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

è§£ç­”ä¾‹
    
    
    import numpy as np
    import pandas as pd
    from scipy import stats
    from datetime import datetime
    import json
    
    class DataDriftMonitor:
        """ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆç›£è¦–ã‚·ã‚¹ãƒ†ãƒ """
    
        def __init__(self, baseline_data, threshold=0.05, alert_features=None):
            """
            Args:
                baseline_data: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼ˆDataFrameï¼‰
                threshold: på€¤ã®é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.05ï¼‰
                alert_features: ç›£è¦–ã™ã‚‹ç‰¹å¾´é‡ã®ãƒªã‚¹ãƒˆï¼ˆNoneã®å ´åˆã¯å…¨ç‰¹å¾´é‡ï¼‰
            """
            self.baseline_data = baseline_data
            self.threshold = threshold
            self.alert_features = alert_features or baseline_data.columns.tolist()
            self.drift_history = []
            self.baseline_stats = self._compute_baseline_stats()
    
        def _compute_baseline_stats(self):
            """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®çµ±è¨ˆé‡ã‚’è¨ˆç®—"""
            stats_dict = {}
            for col in self.baseline_data.columns:
                if pd.api.types.is_numeric_dtype(self.baseline_data[col]):
                    stats_dict[col] = {
                        'mean': self.baseline_data[col].mean(),
                        'std': self.baseline_data[col].std(),
                        'min': self.baseline_data[col].min(),
                        'max': self.baseline_data[col].max(),
                        'median': self.baseline_data[col].median()
                    }
            return stats_dict
    
        def detect_drift(self, new_data, feature):
            """å˜ä¸€ç‰¹å¾´é‡ã®ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º"""
            if feature not in self.baseline_data.columns:
                raise ValueError(f"ç‰¹å¾´é‡ {feature} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
            # æ•°å€¤å‹ã®ã¿å‡¦ç†
            if not pd.api.types.is_numeric_dtype(self.baseline_data[feature]):
                return None
    
            # KSæ¤œå®š
            baseline_values = self.baseline_data[feature].dropna()
            new_values = new_data[feature].dropna()
    
            statistic, p_value = stats.ks_2samp(baseline_values, new_values)
    
            is_drift = p_value < self.threshold
    
            # çµ±è¨ˆé‡ã®å¤‰åŒ–ã‚’è¨ˆç®—
            baseline_mean = baseline_values.mean()
            new_mean = new_values.mean()
            mean_shift = (new_mean - baseline_mean) / baseline_mean * 100
    
            drift_info = {
                'timestamp': datetime.now().isoformat(),
                'feature': feature,
                'ks_statistic': float(statistic),
                'p_value': float(p_value),
                'drift_detected': bool(is_drift),
                'baseline_mean': float(baseline_mean),
                'new_mean': float(new_mean),
                'mean_shift_pct': float(mean_shift),
                'n_baseline': len(baseline_values),
                'n_new': len(new_values)
            }
    
            return drift_info
    
        def monitor_all_features(self, new_data):
            """å…¨ç‰¹å¾´é‡ã®ãƒ‰ãƒªãƒ•ãƒˆç›£è¦–"""
            results = []
            alerts = []
    
            print(f"=== ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆç›£è¦–å®Ÿè¡Œ ===")
            print(f"æ™‚åˆ»: {datetime.now()}")
            print(f"ç›£è¦–ç‰¹å¾´é‡æ•°: {len(self.alert_features)}")
            print(f"æ–°ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(new_data)}\n")
    
            for feature in self.alert_features:
                if feature not in new_data.columns:
                    continue
    
                drift_info = self.detect_drift(new_data, feature)
    
                if drift_info is None:
                    continue
    
                results.append(drift_info)
                self.drift_history.append(drift_info)
    
                # ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºæ™‚ã®ã‚¢ãƒ©ãƒ¼ãƒˆ
                if drift_info['drift_detected']:
                    alert_msg = (
                        f"âš ï¸  ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º: {feature}\n"
                        f"   KSçµ±è¨ˆé‡: {drift_info['ks_statistic']:.4f}\n"
                        f"   på€¤: {drift_info['p_value']:.4f}\n"
                        f"   å¹³å‡å€¤ã‚·ãƒ•ãƒˆ: {drift_info['mean_shift_pct']:.2f}%"
                    )
                    alerts.append(alert_msg)
                    print(alert_msg + "\n")
    
            # ã‚µãƒãƒªãƒ¼
            n_drift = sum(r['drift_detected'] for r in results)
            print(f"=== ç›£è¦–çµæœ ===")
            print(f"ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º: {n_drift}/{len(results)}ç‰¹å¾´é‡")
    
            if n_drift > len(results) * 0.3:  # 30%ä»¥ä¸Šã§ã‚¢ãƒ©ãƒ¼ãƒˆ
                print("âš ï¸  è­¦å‘Š: å¤šæ•°ã®ç‰¹å¾´é‡ã§ãƒ‰ãƒªãƒ•ãƒˆãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
                print("   ãƒ¢ãƒ‡ãƒ«ã®å†è¨“ç·´ã‚’æ¨å¥¨ã—ã¾ã™")
    
            return results, alerts
    
        def generate_report(self):
            """ãƒ‰ãƒªãƒ•ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
            if not self.drift_history:
                return "ãƒ‰ãƒªãƒ•ãƒˆå±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“"
    
            df_history = pd.DataFrame(self.drift_history)
    
            report = f"""
    === ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆ ===
    
    ç›£è¦–æœŸé–“: {df_history['timestamp'].min()} ~ {df_history['timestamp'].max()}
    ç·ç›£è¦–å›æ•°: {len(df_history)}
    ãƒ¦ãƒ‹ãƒ¼ã‚¯ç‰¹å¾´é‡æ•°: {df_history['feature'].nunique()}
    
    ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã‚µãƒãƒªãƒ¼:
    {df_history.groupby('feature')['drift_detected'].agg(['sum', 'count']).to_string()}
    
    ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºç‡ãƒˆãƒƒãƒ—5:
    {df_history[df_history['drift_detected']].groupby('feature').size().sort_values(ascending=False).head().to_string()}
    
    å¹³å‡ã‚·ãƒ•ãƒˆç‡ï¼ˆçµ¶å¯¾å€¤ï¼‰ãƒˆãƒƒãƒ—5:
    {df_history.groupby('feature')['mean_shift_pct'].apply(lambda x: abs(x).mean()).sort_values(ascending=False).head().to_string()}
            """
    
            return report
    
        def save_report(self, filepath):
            """ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONã§ä¿å­˜"""
            report_data = {
                'baseline_stats': self.baseline_stats,
                'drift_history': self.drift_history,
                'summary': {
                    'total_checks': len(self.drift_history),
                    'total_drifts': sum(d['drift_detected'] for d in self.drift_history)
                }
            }
    
            with open(filepath, 'w') as f:
                json.dump(report_data, f, indent=2)
    
            print(f"âœ“ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {filepath}")
    
    # ä½¿ç”¨ä¾‹
    from sklearn.datasets import make_classification
    
    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼ˆè¨“ç·´æ™‚ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
    X_baseline, _ = make_classification(
        n_samples=1000, n_features=10, random_state=42
    )
    df_baseline = pd.DataFrame(
        X_baseline,
        columns=[f'feature_{i}' for i in range(10)]
    )
    
    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ‰ãƒªãƒ•ãƒˆã‚ã‚Šï¼‰
    X_new, _ = make_classification(
        n_samples=500, n_features=10, random_state=43
    )
    # ã„ãã¤ã‹ã®ç‰¹å¾´é‡ã«ã‚·ãƒ•ãƒˆã‚’è¿½åŠ 
    X_new[:, 0] += 2.0  # feature_0ã«å¤§ããªã‚·ãƒ•ãƒˆ
    X_new[:, 3] += 0.5  # feature_3ã«å°ã•ãªã‚·ãƒ•ãƒˆ
    
    df_new = pd.DataFrame(
        X_new,
        columns=[f'feature_{i}' for i in range(10)]
    )
    
    # ãƒ‰ãƒªãƒ•ãƒˆç›£è¦–ã®å®Ÿè¡Œ
    monitor = DataDriftMonitor(df_baseline, threshold=0.05)
    results, alerts = monitor.monitor_all_features(df_new)
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("\n" + monitor.generate_report())
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    monitor.save_report('drift_report.json')
    

**å‡ºåŠ›ä¾‹** ï¼š
    
    
    === ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆç›£è¦–å®Ÿè¡Œ ===
    æ™‚åˆ»: 2025-10-21 10:30:45.123456
    ç›£è¦–ç‰¹å¾´é‡æ•°: 10
    æ–°ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«æ•°: 500
    
    âš ï¸  ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º: feature_0
       KSçµ±è¨ˆé‡: 0.8920
       på€¤: 0.0000
       å¹³å‡å€¤ã‚·ãƒ•ãƒˆ: 412.34%
    
    âš ï¸  ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º: feature_3
       KSçµ±è¨ˆé‡: 0.2145
       på€¤: 0.0023
       å¹³å‡å€¤ã‚·ãƒ•ãƒˆ: 87.56%
    
    === ç›£è¦–çµæœ ===
    ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º: 2/10ç‰¹å¾´é‡
    

### å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰

MLOpsæˆç†Ÿåº¦ãƒ¬ãƒ™ãƒ«1ã®è‡ªå‹•è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å«ã‚ã¦ãã ã•ã„ï¼š 

  * ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•å–å¾—
  * å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
  * ãƒ¢ãƒ‡ãƒ«è¨“ç·´
  * æ€§èƒ½è©•ä¾¡ã¨é–¾å€¤åˆ¤å®š
  * MLflowã¸ã®è¨˜éŒ²
  * æ¡ä»¶ã‚’æº€ãŸã—ãŸå ´åˆã®ã¿ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²

è§£ç­”ä¾‹
    
    
    import pandas as pd
    import numpy as np
    import mlflow
    import mlflow.sklearn
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.impute import SimpleImputer
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score, classification_report
    from datetime import datetime
    import logging
    
    # ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    class AutoMLPipeline:
        """è‡ªå‹•MLè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆMLOps Level 1ï¼‰"""
    
        def __init__(
            self,
            experiment_name,
            model_name,
            accuracy_threshold=0.75,
            cv_folds=5
        ):
            """
            Args:
                experiment_name: MLflowå®Ÿé¨“å
                model_name: ãƒ¢ãƒ‡ãƒ«ç™»éŒ²å
                accuracy_threshold: ãƒ¢ãƒ‡ãƒ«ç™»éŒ²ã®ç²¾åº¦é–¾å€¤
                cv_folds: Cross-validationã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰æ•°
            """
            self.experiment_name = experiment_name
            self.model_name = model_name
            self.accuracy_threshold = accuracy_threshold
            self.cv_folds = cv_folds
    
            # MLflowå®Ÿé¨“ã®è¨­å®š
            mlflow.set_experiment(experiment_name)
    
            logger.info(f"è‡ªå‹•è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
            logger.info(f"å®Ÿé¨“å: {experiment_name}")
            logger.info(f"ç²¾åº¦é–¾å€¤: {accuracy_threshold}")
    
        def load_data(self, data_source):
            """ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•å–å¾—"""
            logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹: {data_source}")
    
            # å®Ÿéš›ã®ç’°å¢ƒã§ã¯ã€DBã‚„APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            # ã“ã“ã§ã¯ãƒ‡ãƒ¢ç”¨ã®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            from sklearn.datasets import make_classification
    
            X, y = make_classification(
                n_samples=1000,
                n_features=20,
                n_informative=15,
                n_redundant=5,
                random_state=42
            )
    
            df = pd.DataFrame(
                X,
                columns=[f'feature_{i}' for i in range(20)]
            )
            df['target'] = y
    
            # æ„å›³çš„ã«æ¬ æå€¤ã‚’è¿½åŠ 
            missing_mask = np.random.random(df.shape) < 0.05
            df = df.mask(missing_mask)
    
            logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ, {len(df.columns)}åˆ—")
            logger.info(f"æ¬ æå€¤: {df.isnull().sum().sum()}å€‹")
    
            return df
    
        def create_preprocessing_pipeline(self):
            """å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ"""
            return Pipeline([
                ('imputer', SimpleImputer(strategy='mean')),
                ('scaler', StandardScaler())
            ])
    
        def create_model_pipeline(self, preprocessing_pipeline):
            """ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ"""
            return Pipeline([
                ('preprocessing', preprocessing_pipeline),
                ('classifier', RandomForestClassifier(
                    n_estimators=100,
                    max_depth=10,
                    random_state=42
                ))
            ])
    
        def evaluate_model(self, pipeline, X_train, X_test, y_train, y_test):
            """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
            # Cross-validation
            cv_scores = cross_val_score(
                pipeline,
                X_train,
                y_train,
                cv=self.cv_folds,
                scoring='accuracy'
            )
    
            # ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡
            y_pred = pipeline.predict(X_test)
            test_accuracy = accuracy_score(y_test, y_pred)
    
            metrics = {
                'cv_mean_accuracy': cv_scores.mean(),
                'cv_std_accuracy': cv_scores.std(),
                'test_accuracy': test_accuracy
            }
    
            logger.info("ãƒ¢ãƒ‡ãƒ«è©•ä¾¡å®Œäº†")
            logger.info(f"CVç²¾åº¦: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
            logger.info(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_accuracy:.4f}")
    
            return metrics, y_pred
    
        def should_register_model(self, metrics):
            """ãƒ¢ãƒ‡ãƒ«ç™»éŒ²åˆ¤å®š"""
            test_acc = metrics['test_accuracy']
            cv_acc = metrics['cv_mean_accuracy']
    
            # æ¡ä»¶1: ãƒ†ã‚¹ãƒˆç²¾åº¦ãŒé–¾å€¤ä»¥ä¸Š
            # æ¡ä»¶2: CVç²¾åº¦ã¨ã®å·®ãŒå¤§ãã™ããªã„ï¼ˆéå­¦ç¿’ãƒã‚§ãƒƒã‚¯ï¼‰
            meets_threshold = test_acc >= self.accuracy_threshold
            not_overfitting = abs(test_acc - cv_acc) < 0.1
    
            should_register = meets_threshold and not_overfitting
    
            if should_register:
                logger.info(f"âœ“ ãƒ¢ãƒ‡ãƒ«ç™»éŒ²æ¡ä»¶ã‚’æº€ãŸã—ã¾ã—ãŸ")
            else:
                logger.warning(f"âš ï¸  ãƒ¢ãƒ‡ãƒ«ç™»éŒ²æ¡ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã›ã‚“")
                if not meets_threshold:
                    logger.warning(f"   ç²¾åº¦ä¸è¶³: {test_acc:.4f} < {self.accuracy_threshold}")
                if not not_overfitting:
                    logger.warning(f"   éå­¦ç¿’ã®å¯èƒ½æ€§: ãƒ†ã‚¹ãƒˆç²¾åº¦ã¨CVç²¾åº¦ã®å·®ãŒå¤§ãã„")
    
            return should_register
    
        def run_training(self, data_source):
            """è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
            logger.info("=" * 50)
            logger.info(f"è‡ªå‹•è¨“ç·´é–‹å§‹: {datetime.now()}")
            logger.info("=" * 50)
    
            with mlflow.start_run(run_name=f"auto_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
    
                # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
                df = self.load_data(data_source)
    
                # ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã®è¨˜éŒ²
                mlflow.log_param("n_samples", len(df))
                mlflow.log_param("n_features", len(df.columns) - 1)
                mlflow.log_param("n_missing", df.isnull().sum().sum())
    
                # 2. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
                X = df.drop('target', axis=1)
                y = df['target']
    
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y,
                    test_size=0.2,
                    random_state=42,
                    stratify=y
                )
    
                mlflow.log_param("test_size", 0.2)
                mlflow.log_param("n_train", len(X_train))
                mlflow.log_param("n_test", len(X_test))
    
                # 3. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ
                preprocessing_pipeline = self.create_preprocessing_pipeline()
                model_pipeline = self.create_model_pipeline(preprocessing_pipeline)
    
                # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨˜éŒ²
                model_params = model_pipeline.named_steps['classifier'].get_params()
                mlflow.log_params({
                    f"model_{k}": v for k, v in model_params.items()
                    if not k.startswith('_')
                })
    
                # 4. ãƒ¢ãƒ‡ãƒ«è¨“ç·´
                logger.info("ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹")
                model_pipeline.fit(X_train, y_train)
                logger.info("ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")
    
                # 5. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
                metrics, y_pred = self.evaluate_model(
                    model_pipeline,
                    X_train, X_test,
                    y_train, y_test
                )
    
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
                mlflow.log_metrics(metrics)
    
                # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
                report = classification_report(y_test, y_pred, output_dict=True)
                mlflow.log_dict(report, "classification_report.json")
    
                # 6. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
                mlflow.sklearn.log_model(
                    model_pipeline,
                    "model",
                    signature=mlflow.models.signature.infer_signature(X_train, y_train)
                )
    
                # 7. ãƒ¢ãƒ‡ãƒ«ç™»éŒ²åˆ¤å®š
                if self.should_register_model(metrics):
                    model_uri = f"runs:/{mlflow.active_run().info.run_id}/model"
    
                    mlflow.register_model(
                        model_uri,
                        self.model_name
                    )
    
                    logger.info(f"âœ“ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²: {self.model_name}")
                    logger.info(f"  ç²¾åº¦: {metrics['test_accuracy']:.4f}")
    
                    # ã‚¿ã‚°è¨­å®š
                    mlflow.set_tag("registered", "true")
                    mlflow.set_tag("stage", "staging")
                else:
                    logger.warning("ãƒ¢ãƒ‡ãƒ«ã¯ç™»éŒ²ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                    mlflow.set_tag("registered", "false")
    
                # 8. å…±é€šã‚¿ã‚°
                mlflow.set_tag("training_type", "automatic")
                mlflow.set_tag("timestamp", datetime.now().isoformat())
    
                logger.info("=" * 50)
                logger.info(f"è‡ªå‹•è¨“ç·´å®Œäº†: {datetime.now()}")
                logger.info(f"Run ID: {mlflow.active_run().info.run_id}")
                logger.info("=" * 50)
    
                return metrics
    
    # ä½¿ç”¨ä¾‹
    if __name__ == "__main__":
        # è‡ªå‹•è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä½œæˆ
        auto_pipeline = AutoMLPipeline(
            experiment_name="auto_training_demo",
            model_name="auto_classifier",
            accuracy_threshold=0.75,
            cv_folds=5
        )
    
        # è¨“ç·´å®Ÿè¡Œ
        metrics = auto_pipeline.run_training(data_source="production_db")
    
        print("\n=== æœ€çµ‚çµæœ ===")
        print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {metrics['test_accuracy']:.4f}")
        print(f"CVç²¾åº¦: {metrics['cv_mean_accuracy']:.4f} Â± {metrics['cv_std_accuracy']:.4f}")
    

**ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œã®ä¾‹** ï¼š
    
    
    import schedule
    import time
    
    def scheduled_training():
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨“ç·´ã‚¸ãƒ§ãƒ–"""
        auto_pipeline = AutoMLPipeline(
            experiment_name="scheduled_training",
            model_name="production_model",
            accuracy_threshold=0.80
        )
    
        try:
            metrics = auto_pipeline.run_training("production_db")
            logger.info(f"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨“ç·´æˆåŠŸ: ç²¾åº¦={metrics['test_accuracy']:.4f}")
        except Exception as e:
            logger.error(f"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨“ç·´å¤±æ•—: {str(e)}")
    
    # æ¯æ—¥åˆå‰2æ™‚ã«å®Ÿè¡Œ
    schedule.every().day.at("02:00").do(scheduled_training)
    
    # æ¯é€±æœˆæ›œæ—¥ã«å®Ÿè¡Œ
    # schedule.every().monday.at("02:00").do(scheduled_training)
    
    print("ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™...")
    # while True:
    #     schedule.run_pending()
    #     time.sleep(60)
    

### å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰

MLOpsæˆç†Ÿåº¦ãƒ¬ãƒ™ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚çµ„ç¹”ãŒLevel 0ã€1ã€2ã®ã©ã®ãƒ¬ãƒ™ãƒ«ã«ã‚ã‚‹ã‹ã‚’åˆ¤å®šã§ãã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚

è§£ç­”ä¾‹

**è§£ç­”** ï¼š
    
    
    class MLOpsMaturityAssessment:
        """MLOpsæˆç†Ÿåº¦è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ """
    
        def __init__(self):
            self.criteria = {
                'Level 0': {
                    'ãƒ‡ãƒ¼ã‚¿ç®¡ç†': [
                        'ãƒ‡ãƒ¼ã‚¿ã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã§ç®¡ç†',
                        'ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ãªã—',
                        'ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã¯æ‰‹å‹•'
                    ],
                    'ãƒ¢ãƒ‡ãƒ«é–‹ç™º': [
                        'Jupyter Notebookã§é–‹ç™º',
                        'å®Ÿé¨“ã®è¨˜éŒ²ã¯æ‰‹å‹•ï¼ˆExcelãªã©ï¼‰',
                        'ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯è©¦è¡ŒéŒ¯èª¤'
                    ],
                    'ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ': [
                        'ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã¯æ‰‹ä½œæ¥­',
                        'ãƒ‡ãƒ—ãƒ­ã‚¤é »åº¦ã¯æœˆã€œå¹´å˜ä½',
                        'æœ¬ç•ªç’°å¢ƒã¨ã®æ•´åˆæ€§ç¢ºèªãªã—'
                    ],
                    'ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°': [
                        'ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®ç›£è¦–ãªã—',
                        'ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½ãªã—',
                        'å•é¡Œç™ºç”Ÿæ™‚ã®å¯¾å¿œã¯äº‹å¾Œçš„'
                    ]
                },
                'Level 1': {
                    'ãƒ‡ãƒ¼ã‚¿ç®¡ç†': [
                        'ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ï¼ˆDVCç­‰ï¼‰',
                        'è‡ªå‹•ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯',
                        'ãƒ‡ãƒ¼ã‚¿ç³»è­œã®è¨˜éŒ²'
                    ],
                    'ãƒ¢ãƒ‡ãƒ«é–‹ç™º': [
                        'å®Ÿé¨“ç®¡ç†ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ï¼ˆMLflowç­‰ï¼‰',
                        'ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è‡ªå‹•è¨˜éŒ²',
                        'ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã§ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†'
                    ],
                    'ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ': [
                        'è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è‡ªå‹•åŒ–',
                        'ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚°',
                        'ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒã§ã®ãƒ†ã‚¹ãƒˆ'
                    ],
                    'ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°': [
                        'åŸºæœ¬çš„ãªæ€§èƒ½ç›£è¦–',
                        'ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º',
                        'æ‰‹å‹•ã§ã®å†è¨“ç·´ãƒˆãƒªã‚¬ãƒ¼'
                    ]
                },
                'Level 2': {
                    'ãƒ‡ãƒ¼ã‚¿ç®¡ç†': [
                        'ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Œå…¨è‡ªå‹•åŒ–',
                        'ç‰¹å¾´é‡ã‚¹ãƒˆã‚¢ã®æ´»ç”¨',
                        'ãƒ‡ãƒ¼ã‚¿å“è³ªã®ç¶™ç¶šçš„ç›£è¦–'
                    ],
                    'ãƒ¢ãƒ‡ãƒ«é–‹ç™º': [
                        'CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ',
                        'è‡ªå‹•ãƒ†ã‚¹ãƒˆï¼ˆãƒ‡ãƒ¼ã‚¿ã€ãƒ¢ãƒ‡ãƒ«ã€ã‚¤ãƒ³ãƒ•ãƒ©ï¼‰',
                        'è‡ªå‹•ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–'
                    ],
                    'ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ': [
                        'ã‚³ãƒ¼ãƒ‰å¤‰æ›´ã‹ã‚‰ãƒ‡ãƒ—ãƒ­ã‚¤ã¾ã§å®Œå…¨è‡ªå‹•',
                        'ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ/A/Bãƒ†ã‚¹ãƒˆ',
                        'è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½'
                    ],
                    'ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°': [
                        'åŒ…æ‹¬çš„ãªå¯è¦³æ¸¬æ€§',
                        'è‡ªå‹•å†è¨“ç·´ãƒˆãƒªã‚¬ãƒ¼',
                        'ç•°å¸¸æ¤œçŸ¥ã¨è‡ªå‹•å¯¾å¿œ'
                    ]
                }
            }
    
            self.scores = {
                'Level 0': 0,
                'Level 1': 0,
                'Level 2': 0
            }
    
        def assess(self):
            """å¯¾è©±çš„ãªæˆç†Ÿåº¦è©•ä¾¡"""
            print("=" * 60)
            print("MLOpsæˆç†Ÿåº¦è©•ä¾¡")
            print("=" * 60)
            print("å„è³ªå•ã« 'yes' ã¾ãŸã¯ 'no' ã§ç­”ãˆã¦ãã ã•ã„\n")
    
            for level, categories in self.criteria.items():
                print(f"\n### {level}ã®è©•ä¾¡ ###\n")
                level_score = 0
                total_questions = 0
    
                for category, questions in categories.items():
                    print(f"[{category}]")
                    for i, question in enumerate(questions, 1):
                        total_questions += 1
                        while True:
                            answer = input(f"  {i}. {question}\n     â†’ ").strip().lower()
                            if answer in ['yes', 'no', 'y', 'n']:
                                if answer in ['yes', 'y']:
                                    level_score += 1
                                break
                            else:
                                print("     'yes'ã¾ãŸã¯'no'ã§ç­”ãˆã¦ãã ã•ã„")
                    print()
    
                self.scores[level] = (level_score / total_questions) * 100
                print(f"{level}é”æˆåº¦: {self.scores[level]:.1f}%")
    
            return self.scores
    
        def determine_level(self):
            """æˆç†Ÿåº¦ãƒ¬ãƒ™ãƒ«ã®åˆ¤å®š"""
            # Level 2ã®åŸºæº–ã‚’70%ä»¥ä¸Šæº€ãŸã™å ´åˆ
            if self.scores['Level 2'] >= 70:
                return 'Level 2', 'CI/CDå®Œå…¨è‡ªå‹•åŒ–'
    
            # Level 1ã®åŸºæº–ã‚’70%ä»¥ä¸Šæº€ãŸã™å ´åˆ
            elif self.scores['Level 1'] >= 70:
                return 'Level 1', 'MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è‡ªå‹•åŒ–'
    
            # ãã‚Œä»¥å¤–
            else:
                return 'Level 0', 'æ‰‹å‹•ãƒ—ãƒ­ã‚»ã‚¹'
    
        def generate_recommendations(self, current_level):
            """æ”¹å–„æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
            recommendations = {
                'Level 0': [
                    '1. å®Ÿé¨“ç®¡ç†ãƒ„ãƒ¼ãƒ«ï¼ˆMLflowï¼‰ã®å°å…¥',
                    '2. ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ï¼ˆDVCï¼‰ã®é–‹å§‹',
                    '3. è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆåŒ–',
                    '4. ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®æ§‹ç¯‰'
                ],
                'Level 1': [
                    '1. CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰',
                    '2. è‡ªå‹•ãƒ†ã‚¹ãƒˆã®å®Ÿè£…',
                    '3. ãƒ¢ãƒ‡ãƒ«ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®å¼·åŒ–',
                    '4. è‡ªå‹•å†è¨“ç·´ã®ä»•çµ„ã¿æ§‹ç¯‰'
                ],
                'Level 2': [
                    '1. é«˜åº¦ãªA/Bãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯',
                    '2. ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«ç®¡ç†ã®æœ€é©åŒ–',
                    '3. MLOpsãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã®çµ±åˆ',
                    '4. çµ„ç¹”å…¨ä½“ã¸ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹å±•é–‹'
                ]
            }
    
            return recommendations.get(current_level, [])
    
        def print_report(self):
            """è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã®å‡ºåŠ›"""
            print("\n" + "=" * 60)
            print("è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ")
            print("=" * 60)
    
            # ã‚¹ã‚³ã‚¢è¡¨ç¤º
            print("\n### é”æˆåº¦ã‚¹ã‚³ã‚¢ ###")
            for level, score in self.scores.items():
                bar = "â–ˆ" * int(score / 5) + "â–‘" * (20 - int(score / 5))
                print(f"{level}: {bar} {score:.1f}%")
    
            # ãƒ¬ãƒ™ãƒ«åˆ¤å®š
            current_level, description = self.determine_level()
            print(f"\n### ç¾åœ¨ã®æˆç†Ÿåº¦ãƒ¬ãƒ™ãƒ« ###")
            print(f"ãƒ¬ãƒ™ãƒ«: {current_level} - {description}")
    
            # æ¨å¥¨äº‹é …
            print(f"\n### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸ã®æ¨å¥¨äº‹é … ###")
            recommendations = self.generate_recommendations(current_level)
            for rec in recommendations:
                print(f"  {rec}")
    
            print("\n" + "=" * 60)
    
    # ä½¿ç”¨ä¾‹ï¼ˆãƒ‡ãƒ¢ç”¨ã®è‡ªå‹•è©•ä¾¡ï¼‰
    def demo_assessment():
        """ãƒ‡ãƒ¢ç”¨ã®è©•ä¾¡ï¼ˆè‡ªå‹•å›ç­”ï¼‰"""
        assessment = MLOpsMaturityAssessment()
    
        # æ¨¡æ“¬ã‚¹ã‚³ã‚¢ï¼ˆå®Ÿéš›ã¯å¯¾è©±çš„ã«è©•ä¾¡ï¼‰
        assessment.scores = {
            'Level 0': 90.0,  # Level 0ã®åŸºæº–ã¯ã»ã¼æº€ãŸã—ã¦ã„ã‚‹
            'Level 1': 60.0,  # Level 1ã¯éƒ¨åˆ†çš„ã«é”æˆ
            'Level 2': 20.0   # Level 2ã¯ã¾ã åˆæœŸæ®µéš
        }
    
        assessment.print_report()
    
    # å®Ÿéš›ã®è©•ä¾¡ã‚’å®Ÿè¡Œã™ã‚‹å ´åˆ
    # assessment = MLOpsMaturityAssessment()
    # assessment.assess()
    # assessment.print_report()
    
    # ãƒ‡ãƒ¢å®Ÿè¡Œ
    demo_assessment()
    

**å‡ºåŠ›ä¾‹** ï¼š
    
    
    ============================================================
    è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ
    ============================================================
    
    ### é”æˆåº¦ã‚¹ã‚³ã‚¢ ###
    Level 0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 90.0%
    Level 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 60.0%
    Level 2: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 20.0%
    
    ### ç¾åœ¨ã®æˆç†Ÿåº¦ãƒ¬ãƒ™ãƒ« ###
    ãƒ¬ãƒ™ãƒ«: Level 0 - æ‰‹å‹•ãƒ—ãƒ­ã‚»ã‚¹
    
    ### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸ã®æ¨å¥¨äº‹é … ###
      1. å®Ÿé¨“ç®¡ç†ãƒ„ãƒ¼ãƒ«ï¼ˆMLflowï¼‰ã®å°å…¥
      2. ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ï¼ˆDVCï¼‰ã®é–‹å§‹
      3. è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆåŒ–
      4. ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®æ§‹ç¯‰
    
    ============================================================
    

**è©•ä¾¡åŸºæº–ã®è©³ç´°** ï¼š

ã‚¹ã‚³ã‚¢ç¯„å›² | åˆ¤å®šãƒ¬ãƒ™ãƒ« | èª¬æ˜  
---|---|---  
Level 2 â‰¥ 70% | Level 2é”æˆ | CI/CDå®Œå…¨è‡ªå‹•åŒ–ã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºå¯¾å¿œ  
Level 1 â‰¥ 70% | Level 1é”æˆ | MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è‡ªå‹•åŒ–ã€ã‚¹ã‚±ãƒ¼ãƒ«å¯èƒ½  
ä¸Šè¨˜ä»¥å¤– | Level 0 | æ‰‹å‹•ãƒ—ãƒ­ã‚»ã‚¹ã€æ”¹å–„ãŒå¿…è¦  
  
* * *

## å‚è€ƒæ–‡çŒ®

  1. GÃ©ron, A. (2022). _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_ (3rd ed.). O'Reilly Media.
  2. Kreuzberger, D., KÃ¼hl, N., & Hirschl, S. (2023). Machine Learning Operations (MLOps): Overview, Definition, and Architecture. _IEEE Access_ , 11, 31866-31879.
  3. Google Cloud. (2023). _MLOps: Continuous delivery and automation pipelines in machine learning_. https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning
  4. Huyen, C. (2022). _Designing Machine Learning Systems_. O'Reilly Media.
  5. Treveil, M., et al. (2020). _Introducing MLOps_. O'Reilly Media.
