<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬5ç« ï¼šCI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/mlops-introduction/chapter5-ci-cd-ml.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/mlops-introduction/index.html">Mlops</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 5</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬5ç« ï¼šCI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰</h1>
            <p class="subtitle">æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•ãƒ†ã‚¹ãƒˆã¨ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 12å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… æ©Ÿæ¢°å­¦ç¿’ã«ãŠã‘ã‚‹CI/CDã®ç‰¹å¾´ã¨é‡è¦æ€§ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… MLãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã‚’è¨­è¨ˆã§ãã‚‹</li>
<li>âœ… GitHub Actionsã§ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ»æ¤œè¨¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
<li>âœ… å„ç¨®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæˆ¦ç•¥ï¼ˆBlue-Greenã€Canaryç­‰ï¼‰ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æœ¬ç•ªç’°å¢ƒã«é©ç”¨ã§ãã‚‹</li>
</ul>

<hr>

<h2>5.1 CI/CD for MLã®ç‰¹å¾´</h2>

<h3>å¾“æ¥ã®CI/CDã¨ã®é•ã„</h3>

<p><strong>CI/CDï¼ˆContinuous Integration/Continuous Deliveryï¼‰</strong>ã¯ã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºãƒ—ãƒ­ã‚»ã‚¹ã‚’è‡ªå‹•åŒ–ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã§ã¯ã€å¾“æ¥ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã«åŠ ãˆã¦ã€ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«ã®ç®¡ç†ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚</p>

<div class="mermaid">
graph TD
    A[ã‚³ãƒ¼ãƒ‰å¤‰æ›´] --> B[CI: è‡ªå‹•ãƒ†ã‚¹ãƒˆ]
    B --> C[ãƒ¢ãƒ‡ãƒ«å­¦ç¿’]
    C --> D[ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼]
    D --> E{æ€§èƒ½åŸºæº–OK?}
    E -->|Yes| F[CD: ãƒ‡ãƒ—ãƒ­ã‚¤]
    E -->|No| G[é€šçŸ¥ãƒ»ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯]
    F --> H[æœ¬ç•ªç’°å¢ƒ]
    G --> A

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#fce4ec
    style E fill:#ffebee
    style F fill:#e8f5e9
    style G fill:#ffccbc
    style H fill:#c8e6c9
</div>

<h3>MLç‰¹æœ‰ã®è€ƒæ…®äº‹é …</h3>

<table>
<thead>
<tr>
<th>è¦ç´ </th>
<th>å¾“æ¥ã®CI/CD</th>
<th>ML CI/CD</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ†ã‚¹ãƒˆå¯¾è±¡</strong></td>
<td>ã‚³ãƒ¼ãƒ‰</td>
<td>ã‚³ãƒ¼ãƒ‰ + ãƒ‡ãƒ¼ã‚¿ + ãƒ¢ãƒ‡ãƒ«</td>
</tr>
<tr>
<td><strong>å“è³ªæŒ‡æ¨™</strong></td>
<td>ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹ç‡</td>
<td>ç²¾åº¦ã€å†ç¾ç‡ã€F1ã‚¹ã‚³ã‚¢ç­‰</td>
</tr>
<tr>
<td><strong>å†ç¾æ€§</strong></td>
<td>ã‚³ãƒ¼ãƒ‰ãƒãƒ¼ã‚¸ãƒ§ãƒ³</td>
<td>ã‚³ãƒ¼ãƒ‰ + ãƒ‡ãƒ¼ã‚¿ + ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</td>
</tr>
<tr>
<td><strong>ãƒ‡ãƒ—ãƒ­ã‚¤æˆ¦ç•¥</strong></td>
<td>Blue-Greenã€Canary</td>
<td>A/Bãƒ†ã‚¹ãƒˆã€Shadow Modeå«ã‚€</td>
</tr>
<tr>
<td><strong>ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°</strong></td>
<td>ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹</td>
<td>ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã€ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆ</td>
</tr>
</tbody>
</table>

<h3>ãƒ‡ãƒ¼ã‚¿ã®å¤‰åŒ–ã¸ã®å¯¾å¿œ</h3>

<blockquote>
<p><strong>é‡è¦</strong>: MLãƒ¢ãƒ‡ãƒ«ã¯æ™‚é–“ã¨ã¨ã‚‚ã«ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒãŒå¤‰åŒ–ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆï¼‰ã™ã‚‹ãŸã‚ã€ç¶™ç¶šçš„ãªå†å­¦ç¿’ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãŒå¿…è¦ã§ã™ã€‚</p>
</blockquote>

<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
np.random.seed(42)

def generate_data(n_samples, drift_level=0.0):
    """drift_level: 0.0 (å¤‰åŒ–ãªã—) ~ 1.0 (å¤§ããªå¤‰åŒ–)"""
    X1 = np.random.normal(50 + drift_level * 20, 10, n_samples)
    X2 = np.random.normal(100 + drift_level * 30, 20, n_samples)
    y = ((X1 > 50) & (X2 > 100)).astype(int)
    return pd.DataFrame({'X1': X1, 'X2': X2}), y

# åˆæœŸãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
X_train, y_train = generate_data(1000, drift_level=0.0)
X_test, y_test = generate_data(200, drift_level=0.0)

model = RandomForestClassifier(n_estimators=50, random_state=42)
model.fit(X_train, y_train)

initial_accuracy = accuracy_score(y_test, model.predict(X_test))

# æ™‚é–“çµŒéã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆ
drift_levels = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
accuracies = []

for drift in drift_levels:
    X_test_drift, y_test_drift = generate_data(200, drift_level=drift)
    acc = accuracy_score(y_test_drift, model.predict(X_test_drift))
    accuracies.append(acc)

print("=== ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã®å½±éŸ¿ ===")
for drift, acc in zip(drift_levels, accuracies):
    print(f"ãƒ‰ãƒªãƒ•ãƒˆãƒ¬ãƒ™ãƒ« {drift:.1f}: ç²¾åº¦ = {acc:.3f}")

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.plot(drift_levels, accuracies, marker='o', linewidth=2, markersize=8)
plt.axhline(y=0.8, color='r', linestyle='--', label='è¨±å®¹ç²¾åº¦ä¸‹é™')
plt.xlabel('ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆãƒ¬ãƒ™ãƒ«', fontsize=12)
plt.ylabel('ãƒ¢ãƒ‡ãƒ«ç²¾åº¦', fontsize=12)
plt.title('ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®åŠ£åŒ–', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã®å½±éŸ¿ ===
ãƒ‰ãƒªãƒ•ãƒˆãƒ¬ãƒ™ãƒ« 0.0: ç²¾åº¦ = 0.920
ãƒ‰ãƒªãƒ•ãƒˆãƒ¬ãƒ™ãƒ« 0.2: ç²¾åº¦ = 0.885
ãƒ‰ãƒªãƒ•ãƒˆãƒ¬ãƒ™ãƒ« 0.4: ç²¾åº¦ = 0.835
ãƒ‰ãƒªãƒ•ãƒˆãƒ¬ãƒ™ãƒ« 0.6: ç²¾åº¦ = 0.775
ãƒ‰ãƒªãƒ•ãƒˆãƒ¬ãƒ™ãƒ« 0.8: ç²¾åº¦ = 0.720
ãƒ‰ãƒªãƒ•ãƒˆãƒ¬ãƒ™ãƒ« 1.0: ç²¾åº¦ = 0.670
</code></pre>

<hr>

<h2>5.2 è‡ªå‹•ãƒ†ã‚¹ãƒˆ</h2>

<h3>MLã«ãŠã‘ã‚‹ãƒ†ã‚¹ãƒˆæˆ¦ç•¥</h3>

<p>æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã«ã¯è¤‡æ•°ãƒ¬ãƒ™ãƒ«ã®ãƒ†ã‚¹ãƒˆãŒå¿…è¦ã§ã™ï¼š</p>

<ol>
<li><strong>Unit Tests</strong>: å€‹åˆ¥ã®é–¢æ•°ãƒ»ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ†ã‚¹ãƒˆ</li>
<li><strong>Data Validation Tests</strong>: ãƒ‡ãƒ¼ã‚¿å“è³ªã®ãƒ†ã‚¹ãƒˆ</li>
<li><strong>Model Performance Tests</strong>: ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®ãƒ†ã‚¹ãƒˆ</li>
<li><strong>Integration Tests</strong>: ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ãƒ†ã‚¹ãƒˆ</li>
</ol>

<h3>1. Unit Tests for ML Code</h3>

<pre><code class="language-python"># tests/test_preprocessing.py
import pytest
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

class TestPreprocessing:
    """å‰å‡¦ç†é–¢æ•°ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ"""

    def test_handle_missing_values(self):
        """æ¬ æå€¤å‡¦ç†ã®ãƒ†ã‚¹ãƒˆ"""
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        data = pd.DataFrame({
            'A': [1, 2, np.nan, 4],
            'B': [5, np.nan, 7, 8]
        })

        # æ¬ æå€¤ã‚’ä¸­å¤®å€¤ã§è£œå®Œ
        filled = data.fillna(data.median())

        # ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³
        assert filled.isnull().sum().sum() == 0, "æ¬ æå€¤ãŒæ®‹ã£ã¦ã„ã‚‹"
        assert filled['A'].iloc[2] == 2.0, "ä¸­å¤®å€¤ãŒæ­£ã—ãè¨ˆç®—ã•ã‚Œã¦ã„ãªã„"

    def test_scaling(self):
        """ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ"""
        data = np.array([[1, 2], [3, 4], [5, 6]])

        scaler = StandardScaler()
        scaled = scaler.fit_transform(data)

        # æ¨™æº–åŒ–å¾Œã®å¹³å‡ã¨æ¨™æº–åå·®ã‚’æ¤œè¨¼
        assert np.allclose(scaled.mean(axis=0), 0, atol=1e-7), "å¹³å‡ãŒ0ã§ãªã„"
        assert np.allclose(scaled.std(axis=0), 1, atol=1e-7), "æ¨™æº–åå·®ãŒ1ã§ãªã„"

    def test_feature_engineering(self):
        """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ"""
        df = pd.DataFrame({
            'height': [170, 180, 160],
            'weight': [65, 80, 55]
        })

        # BMIè¨ˆç®—
        df['bmi'] = df['weight'] / (df['height'] / 100) ** 2

        # æœŸå¾…å€¤ã¨æ¯”è¼ƒ
        expected_bmi = [22.49, 24.69, 21.48]
        assert np.allclose(df['bmi'].values, expected_bmi, atol=0.01)

# pytestã§å®Ÿè¡Œ
if __name__ == "__main__":
    pytest.main([__file__, '-v'])
</code></pre>

<h3>2. Data Validation Tests</h3>

<pre><code class="language-python"># tests/test_data_validation.py
import pytest
import pandas as pd
import numpy as np

class TestDataValidation:
    """ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼ãƒ†ã‚¹ãƒˆ"""

    @pytest.fixture
    def sample_data(self):
        """ãƒ†ã‚¹ãƒˆç”¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿"""
        return pd.DataFrame({
            'age': [25, 30, 35, 40, 45],
            'income': [50000, 60000, 70000, 80000, 90000],
            'score': [0.7, 0.8, 0.6, 0.9, 0.75]
        })

    def test_no_missing_values(self, sample_data):
        """æ¬ æå€¤ãŒãªã„ã“ã¨ã‚’ç¢ºèª"""
        assert sample_data.isnull().sum().sum() == 0, "æ¬ æå€¤ãŒå­˜åœ¨ã™ã‚‹"

    def test_data_types(self, sample_data):
        """ãƒ‡ãƒ¼ã‚¿å‹ãŒæ­£ã—ã„ã“ã¨ã‚’ç¢ºèª"""
        assert sample_data['age'].dtype in [np.int64, np.float64]
        assert sample_data['income'].dtype in [np.int64, np.float64]
        assert sample_data['score'].dtype == np.float64

    def test_value_ranges(self, sample_data):
        """å€¤ã®ç¯„å›²ãŒé©åˆ‡ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª"""
        assert (sample_data['age'] >= 0).all(), "å¹´é½¢ã«è² ã®å€¤ãŒã‚ã‚‹"
        assert (sample_data['age'] <= 120).all(), "å¹´é½¢ãŒç•°å¸¸ã«é«˜ã„"
        assert (sample_data['income'] >= 0).all(), "åå…¥ã«è² ã®å€¤ãŒã‚ã‚‹"
        assert (sample_data['score'] >= 0).all() and (sample_data['score'] <= 1).all(), \
               "ã‚¹ã‚³ã‚¢ãŒ0-1ã®ç¯„å›²å¤–"

    def test_data_shape(self, sample_data):
        """ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶ãŒæœŸå¾…é€šã‚Šã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª"""
        assert sample_data.shape == (5, 3), f"æœŸå¾…: (5, 3), å®Ÿéš›: {sample_data.shape}"

    def test_no_duplicates(self, sample_data):
        """é‡è¤‡è¡ŒãŒãªã„ã“ã¨ã‚’ç¢ºèª"""
        assert sample_data.duplicated().sum() == 0, "é‡è¤‡è¡ŒãŒå­˜åœ¨ã™ã‚‹"

    def test_statistical_properties(self, sample_data):
        """çµ±è¨ˆçš„æ€§è³ªãŒæœŸå¾…ç¯„å›²å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª"""
        # å¹´é½¢ã®å¹³å‡ãŒ20-60ã®ç¯„å›²å†…
        assert 20 <= sample_data['age'].mean() <= 60, "å¹´é½¢ã®å¹³å‡ãŒç•°å¸¸"

        # ã‚¹ã‚³ã‚¢ã®æ¨™æº–åå·®ãŒå¦¥å½“
        assert sample_data['score'].std() < 0.5, "ã‚¹ã‚³ã‚¢ã®ã°ã‚‰ã¤ããŒå¤§ãã™ãã‚‹"

if __name__ == "__main__":
    pytest.main([__file__, '-v'])
</code></pre>

<h3>3. Model Performance Tests</h3>

<pre><code class="language-python"># tests/test_model_performance.py
import pytest
import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

class TestModelPerformance:
    """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""

    @pytest.fixture
    def trained_model(self):
        """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã™"""
        X, y = make_classification(
            n_samples=1000, n_features=20, n_informative=15,
            n_redundant=5, random_state=42
        )
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)

        return model, X_test, y_test

    def test_minimum_accuracy(self, trained_model):
        """æœ€ä½ç²¾åº¦ã‚’æº€ãŸã™ã“ã¨ã‚’ç¢ºèª"""
        model, X_test, y_test = trained_model
        y_pred = model.predict(X_test)

        accuracy = accuracy_score(y_test, y_pred)
        min_accuracy = 0.80  # æœ€ä½80%ã®ç²¾åº¦ã‚’è¦æ±‚

        assert accuracy >= min_accuracy, \
               f"ç²¾åº¦ãŒåŸºæº–ä»¥ä¸‹: {accuracy:.3f} < {min_accuracy}"

    def test_precision_recall(self, trained_model):
        """é©åˆ‡ãªç²¾åº¦ã¨å†ç¾ç‡ã‚’ç¢ºèª"""
        model, X_test, y_test = trained_model
        y_pred = model.predict(X_test)

        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)

        assert precision >= 0.75, f"é©åˆç‡ãŒä½ã„: {precision:.3f}"
        assert recall >= 0.75, f"å†ç¾ç‡ãŒä½ã„: {recall:.3f}"

    def test_f1_score(self, trained_model):
        """F1ã‚¹ã‚³ã‚¢ãŒåŸºæº–ã‚’æº€ãŸã™ã“ã¨ã‚’ç¢ºèª"""
        model, X_test, y_test = trained_model
        y_pred = model.predict(X_test)

        f1 = f1_score(y_test, y_pred)
        min_f1 = 0.78

        assert f1 >= min_f1, f"F1ã‚¹ã‚³ã‚¢ãŒåŸºæº–ä»¥ä¸‹: {f1:.3f} < {min_f1}"

    def test_no_performance_regression(self, trained_model):
        """æ€§èƒ½ãŒå‰å›ã‚ˆã‚ŠåŠ£åŒ–ã—ã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèª"""
        model, X_test, y_test = trained_model
        y_pred = model.predict(X_test)

        current_accuracy = accuracy_score(y_test, y_pred)
        baseline_accuracy = 0.85  # å‰å›ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
        tolerance = 0.02  # è¨±å®¹èª¤å·®

        assert current_accuracy >= baseline_accuracy - tolerance, \
               f"æ€§èƒ½ãŒåŠ£åŒ–: {current_accuracy:.3f} < {baseline_accuracy - tolerance:.3f}"

    def test_prediction_distribution(self, trained_model):
        """äºˆæ¸¬åˆ†å¸ƒãŒå¦¥å½“ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª"""
        model, X_test, y_test = trained_model
        y_pred = model.predict(X_test)

        # ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ãŒæ¥µç«¯ã§ãªã„ã“ã¨ã‚’ç¢ºèª
        class_0_ratio = (y_pred == 0).sum() / len(y_pred)

        assert 0.2 <= class_0_ratio <= 0.8, \
               f"äºˆæ¸¬åˆ†å¸ƒãŒåã£ã¦ã„ã‚‹: ã‚¯ãƒ©ã‚¹0ã®å‰²åˆ = {class_0_ratio:.2%}"

if __name__ == "__main__":
    pytest.main([__file__, '-v'])
</code></pre>

<h3>4. Integration Tests</h3>

<pre><code class="language-python"># tests/test_integration.py
import pytest
import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier

class TestIntegration:
    """çµ±åˆãƒ†ã‚¹ãƒˆï¼šã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    @pytest.fixture
    def sample_pipeline(self):
        """å®Œå…¨ãªæ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        return Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler()),
            ('classifier', RandomForestClassifier(n_estimators=50, random_state=42))
        ])

    @pytest.fixture
    def sample_data_with_issues(self):
        """å•é¡Œã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¬ æå€¤ã€å¤–ã‚Œå€¤ç­‰ï¼‰"""
        np.random.seed(42)
        data = pd.DataFrame({
            'feature1': [1, 2, np.nan, 4, 5, 100],  # æ¬ æå€¤ã¨å¤–ã‚Œå€¤
            'feature2': [10, 20, 30, 40, np.nan, 60],
            'feature3': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
        })
        labels = np.array([0, 0, 1, 1, 0, 1])
        return data, labels

    def test_pipeline_handles_missing_values(self, sample_pipeline, sample_data_with_issues):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒæ¬ æå€¤ã‚’å‡¦ç†ã§ãã‚‹ã“ã¨ã‚’ç¢ºèª"""
        X, y = sample_data_with_issues

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ
        sample_pipeline.fit(X, y)
        predictions = sample_pipeline.predict(X)

        # äºˆæ¸¬ãŒå…¨ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦è¡Œã‚ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
        assert len(predictions) == len(y), "äºˆæ¸¬æ•°ãŒå…¥åŠ›ãƒ‡ãƒ¼ã‚¿æ•°ã¨ä¸€è‡´ã—ãªã„"
        assert not np.isnan(predictions).any(), "äºˆæ¸¬ã«NaNãŒå«ã¾ã‚Œã¦ã„ã‚‹"

    def test_pipeline_reproducibility(self, sample_pipeline, sample_data_with_issues):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å†ç¾æ€§ã‚’ç¢ºèª"""
        X, y = sample_data_with_issues

        # 1å›ç›®ã®å­¦ç¿’ã¨äºˆæ¸¬
        sample_pipeline.fit(X, y)
        pred1 = sample_pipeline.predict(X)

        # 2å›ç›®ã®å­¦ç¿’ã¨äºˆæ¸¬ï¼ˆåŒã˜ãƒ‡ãƒ¼ã‚¿ï¼‰
        sample_pipeline.fit(X, y)
        pred2 = sample_pipeline.predict(X)

        # çµæœãŒåŒä¸€ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert np.array_equal(pred1, pred2), "åŒã˜ãƒ‡ãƒ¼ã‚¿ã§çµæœãŒå†ç¾ã•ã‚Œãªã„"

    def test_pipeline_training_and_inference(self, sample_pipeline):
        """å­¦ç¿’ã¨æ¨è«–ã®ãƒ•ãƒ­ãƒ¼ãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª"""
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿
        np.random.seed(42)
        X_train = pd.DataFrame(np.random.randn(100, 3))
        y_train = np.random.randint(0, 2, 100)

        # å­¦ç¿’
        sample_pipeline.fit(X_train, y_train)

        # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§æ¨è«–
        X_new = pd.DataFrame(np.random.randn(10, 3))
        predictions = sample_pipeline.predict(X_new)

        # äºˆæ¸¬ãŒé©åˆ‡ãªå½¢å¼ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert predictions.shape == (10,), "äºˆæ¸¬ã®å½¢çŠ¶ãŒä¸æ­£"
        assert set(predictions).issubset({0, 1}), "äºˆæ¸¬å€¤ãŒæœŸå¾…ã•ã‚Œã‚‹ã‚¯ãƒ©ã‚¹ã§ãªã„"

if __name__ == "__main__":
    pytest.main([__file__, '-v'])
</code></pre>

<h3>pytestå®Ÿè¡Œä¾‹</h3>

<pre><code class="language-bash"># å…¨ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
pytest tests/ -v

# ç‰¹å®šã®ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å®Ÿè¡Œ
pytest tests/test_model_performance.py -v

# è©³ç´°ãªå‡ºåŠ›ã¨ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆ
pytest tests/ -v --cov=src --cov-report=html
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>tests/test_preprocessing.py::TestPreprocessing::test_handle_missing_values PASSED
tests/test_preprocessing.py::TestPreprocessing::test_scaling PASSED
tests/test_preprocessing.py::TestPreprocessing::test_feature_engineering PASSED
tests/test_data_validation.py::TestDataValidation::test_no_missing_values PASSED
tests/test_data_validation.py::TestDataValidation::test_data_types PASSED
tests/test_data_validation.py::TestDataValidation::test_value_ranges PASSED
tests/test_model_performance.py::TestModelPerformance::test_minimum_accuracy PASSED
tests/test_model_performance.py::TestModelPerformance::test_precision_recall PASSED
tests/test_model_performance.py::TestModelPerformance::test_f1_score PASSED
tests/test_integration.py::TestIntegration::test_pipeline_handles_missing_values PASSED

==================== 10 passed in 3.24s ====================
</code></pre>

<hr>

<h2>5.3 GitHub Actions for ML</h2>

<h3>GitHub Actionsã¨ã¯</h3>

<p><strong>GitHub Actions</strong>ã¯ã€GitHubä¸Šã§CI/CDãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’è‡ªå‹•åŒ–ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚ã‚³ãƒ¼ãƒ‰å¤‰æ›´æ™‚ã«è‡ªå‹•çš„ã«ãƒ†ã‚¹ãƒˆã€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã€ãƒ‡ãƒ—ãƒ­ã‚¤ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚</p>

<h3>åŸºæœ¬çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ§‹æˆ</h3>

<pre><code class="language-yaml"># .github/workflows/ml-ci.yml
name: ML CI Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run linting
      run: |
        pip install flake8
        flake8 src/ --max-line-length=100

    - name: Run unit tests
      run: |
        pytest tests/ -v --cov=src --cov-report=xml

    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
</code></pre>

<h3>ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®è‡ªå‹•åŒ–</h3>

<pre><code class="language-yaml"># .github/workflows/train-model.yml
name: Train and Validate Model

on:
  schedule:
    # æ¯æ—¥åˆå‰2æ™‚ã«å®Ÿè¡Œ
    - cron: '0 2 * * *'
  workflow_dispatch:  # æ‰‹å‹•å®Ÿè¡Œã‚‚å¯èƒ½

jobs:
  train:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt

    - name: Download training data
      run: |
        python scripts/download_data.py
      env:
        DATA_URL: ${{ secrets.DATA_URL }}

    - name: Train model
      run: |
        python src/train.py --config config/train_config.yaml

    - name: Validate model
      run: |
        python src/validate.py --model models/latest_model.pkl

    - name: Upload model artifact
      uses: actions/upload-artifact@v3
      with:
        name: trained-model
        path: models/latest_model.pkl

    - name: Save metrics
      run: |
        python scripts/save_metrics.py --output metrics/metrics.json

    - name: Upload metrics
      uses: actions/upload-artifact@v3
      with:
        name: model-metrics
        path: metrics/metrics.json
</code></pre>

<h3>æ€§èƒ½å›å¸°ãƒ†ã‚¹ãƒˆ</h3>

<pre><code class="language-yaml"># .github/workflows/performance-test.yml
name: Model Performance Regression Test

on:
  pull_request:
    branches: [ main ]

jobs:
  performance-test:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt

    - name: Download baseline model
      run: |
        # å‰å›ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
        python scripts/download_baseline.py

    - name: Train new model
      run: |
        python src/train.py --config config/train_config.yaml

    - name: Compare performance
      id: compare
      run: |
        python scripts/compare_models.py \
          --baseline models/baseline_model.pkl \
          --new models/latest_model.pkl \
          --output comparison_result.json

    - name: Check performance threshold
      run: |
        python scripts/check_threshold.py \
          --result comparison_result.json \
          --min-accuracy 0.85 \
          --max-regression 0.02

    - name: Comment PR with results
      if: always()
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const result = JSON.parse(fs.readFileSync('comparison_result.json', 'utf8'));

          const comment = `
          ## ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒçµæœ

          | ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ | æ–°ãƒ¢ãƒ‡ãƒ« | å¤‰åŒ– |
          |-----------|-------------|---------|------|
          | Accuracy  | ${result.baseline.accuracy.toFixed(3)} | ${result.new.accuracy.toFixed(3)} | ${(result.new.accuracy - result.baseline.accuracy).toFixed(3)} |
          | Precision | ${result.baseline.precision.toFixed(3)} | ${result.new.precision.toFixed(3)} | ${(result.new.precision - result.baseline.precision).toFixed(3)} |
          | Recall    | ${result.baseline.recall.toFixed(3)} | ${result.new.recall.toFixed(3)} | ${(result.new.recall - result.baseline.recall).toFixed(3)} |
          | F1 Score  | ${result.baseline.f1.toFixed(3)} | ${result.new.f1.toFixed(3)} | ${(result.new.f1 - result.baseline.f1).toFixed(3)} |
          `;

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
</code></pre>

<h3>å®Œå…¨ãªCI/CD YAMLã®ä¾‹</h3>

<pre><code class="language-yaml"># .github/workflows/complete-ml-pipeline.yml
name: Complete ML CI/CD Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  PYTHON_VERSION: '3.9'
  MODEL_REGISTRY: 's3://my-model-registry'

jobs:
  lint-and-test:
    name: Lint and Test
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install flake8 pytest pytest-cov

    - name: Lint with flake8
      run: |
        flake8 src/ tests/ --max-line-length=100

    - name: Run tests
      run: |
        pytest tests/ -v --cov=src --cov-report=xml

    - name: Upload coverage
      uses: codecov/codecov-action@v3

  data-validation:
    name: Data Validation
    runs-on: ubuntu-latest
    needs: lint-and-test

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: pip install -r requirements.txt

    - name: Validate data quality
      run: |
        python scripts/validate_data.py --data data/training_data.csv

  train-model:
    name: Train Model
    runs-on: ubuntu-latest
    needs: data-validation

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: pip install -r requirements.txt

    - name: Train model
      run: |
        python src/train.py \
          --data data/training_data.csv \
          --output models/model.pkl \
          --config config/train_config.yaml

    - name: Evaluate model
      run: |
        python src/evaluate.py \
          --model models/model.pkl \
          --data data/test_data.csv \
          --output metrics.json

    - name: Upload model
      uses: actions/upload-artifact@v3
      with:
        name: trained-model
        path: models/model.pkl

    - name: Upload metrics
      uses: actions/upload-artifact@v3
      with:
        name: metrics
        path: metrics.json

  deploy:
    name: Deploy Model
    runs-on: ubuntu-latest
    needs: train-model
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    steps:
    - uses: actions/checkout@v3

    - name: Download model
      uses: actions/download-artifact@v3
      with:
        name: trained-model
        path: models/

    - name: Deploy to staging
      run: |
        python scripts/deploy.py \
          --model models/model.pkl \
          --environment staging
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

    - name: Run smoke tests
      run: |
        python tests/smoke_tests.py --endpoint ${{ secrets.STAGING_ENDPOINT }}

    - name: Deploy to production
      if: success()
      run: |
        python scripts/deploy.py \
          --model models/model.pkl \
          --environment production
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
</code></pre>

<hr>

<h2>5.4 ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæˆ¦ç•¥</h2>

<h3>1. Blue-Green Deployment</h3>

<p><strong>Blue-Greenãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ</strong>ã¯ã€2ã¤ã®åŒä¸€ç’°å¢ƒï¼ˆBlueã¨Greenï¼‰ã‚’ç”¨æ„ã—ã€ç¬æ™‚ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ã“ã¨ã§ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ã‚’æœ€å°åŒ–ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯] --> B[ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼]
    B --> C[Blueç’°å¢ƒ<br/>æ—§ãƒ¢ãƒ‡ãƒ« v1.0]
    B -.åˆ‡æ›¿.-> D[Greenç’°å¢ƒ<br/>æ–°ãƒ¢ãƒ‡ãƒ« v2.0]

    style C fill:#a7c7e7
    style D fill:#90ee90
</div>

<pre><code class="language-python"># scripts/blue_green_deploy.py
import boto3
import time
from typing import Dict

class BlueGreenDeployer:
    """Blue-Greenãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®å®Ÿè£…"""

    def __init__(self, load_balancer_name: str):
        self.elb = boto3.client('elbv2')
        self.lb_name = load_balancer_name

    def deploy_green(self, model_path: str, target_group_name: str):
        """Greenç’°å¢ƒã«æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤"""
        print(f"Greenç’°å¢ƒã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ä¸­: {model_path}")

        # æ–°ã—ã„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤
        # ï¼ˆå®Ÿè£…ã¯ç’°å¢ƒã«ä¾å­˜ï¼‰
        self._deploy_model_to_target_group(model_path, target_group_name)

        print("Greenç’°å¢ƒã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†")

    def run_health_checks(self, target_group_arn: str) -> bool:
        """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ"""
        print("Greenç’°å¢ƒã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ä¸­...")

        response = self.elb.describe_target_health(
            TargetGroupArn=target_group_arn
        )

        healthy_count = sum(
            1 for target in response['TargetHealthDescriptions']
            if target['TargetHealth']['State'] == 'healthy'
        )

        total_count = len(response['TargetHealthDescriptions'])

        is_healthy = healthy_count == total_count and total_count > 0
        print(f"ãƒ˜ãƒ«ã‚¹: {healthy_count}/{total_count} æ­£å¸¸")

        return is_healthy

    def switch_traffic(self, listener_arn: str, green_target_group_arn: str):
        """ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’Greenç’°å¢ƒã«åˆ‡ã‚Šæ›¿ãˆ"""
        print("ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’Greenç’°å¢ƒã«åˆ‡ã‚Šæ›¿ãˆä¸­...")

        self.elb.modify_listener(
            ListenerArn=listener_arn,
            DefaultActions=[
                {
                    'Type': 'forward',
                    'TargetGroupArn': green_target_group_arn
                }
            ]
        )

        print("ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯åˆ‡ã‚Šæ›¿ãˆå®Œäº†")

    def rollback(self, listener_arn: str, blue_target_group_arn: str):
        """Blueç’°å¢ƒã«ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        print("Blueç’°å¢ƒã«ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¸­...")

        self.elb.modify_listener(
            ListenerArn=listener_arn,
            DefaultActions=[
                {
                    'Type': 'forward',
                    'TargetGroupArn': blue_target_group_arn
                }
            ]
        )

        print("ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Œäº†")

    def _deploy_model_to_target_group(self, model_path: str, target_group: str):
        """ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã•ã‚ŒãŸã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã«ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆå®Ÿè£…ä¾‹ï¼‰"""
        # å®Ÿéš›ã®å®Ÿè£…ã¯ã‚¤ãƒ³ãƒ•ãƒ©ã«ä¾å­˜
        time.sleep(2)  # ãƒ‡ãƒ—ãƒ­ã‚¤ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    deployer = BlueGreenDeployer("my-load-balancer")

    # Step 1: Greenç’°å¢ƒã«ãƒ‡ãƒ—ãƒ­ã‚¤
    deployer.deploy_green(
        model_path="s3://models/model_v2.pkl",
        target_group_name="green-target-group"
    )

    # Step 2: ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
    green_tg_arn = "arn:aws:elasticloadbalancing:region:account:targetgroup/green/xxx"
    if deployer.run_health_checks(green_tg_arn):
        # Step 3: ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯åˆ‡ã‚Šæ›¿ãˆ
        listener_arn = "arn:aws:elasticloadbalancing:region:account:listener/xxx"
        deployer.switch_traffic(listener_arn, green_tg_arn)
        print("âœ“ Blue-Greenãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæˆåŠŸ")
    else:
        print("âœ— ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å¤±æ•—: ãƒ‡ãƒ—ãƒ­ã‚¤ä¸­æ­¢")
</code></pre>

<h3>2. Canary Deployment</h3>

<p><strong>Canaryãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ</strong>ã¯ã€æ–°ãƒ¢ãƒ‡ãƒ«ã‚’å°‘æ•°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æ®µéšçš„ã«å…¬é–‹ã—ã€å•é¡ŒãŒãªã‘ã‚Œã°å¾ã€…ã«æ‹¡å¤§ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<pre><code class="language-python"># scripts/canary_deploy.py
import time
import random
from typing import List, Dict

class CanaryDeployer:
    """Canaryãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®å®Ÿè£…"""

    def __init__(self, old_model, new_model):
        self.old_model = old_model
        self.new_model = new_model
        self.canary_percentage = 0
        self.metrics = {'old': [], 'new': []}

    def predict(self, X):
        """Canaryæ¯”ç‡ã«åŸºã¥ã„ã¦äºˆæ¸¬"""
        if random.random() * 100 < self.canary_percentage:
            # æ–°ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
            prediction = self.new_model.predict(X)
            self.metrics['new'].append(prediction)
            return prediction, 'new'
        else:
            # æ—§ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
            prediction = self.old_model.predict(X)
            self.metrics['old'].append(prediction)
            return prediction, 'old'

    def increase_canary_traffic(self, increment: int = 10):
        """Canaryæ¯”ç‡ã‚’å¢—ã‚„ã™"""
        self.canary_percentage = min(100, self.canary_percentage + increment)
        print(f"Canaryæ¯”ç‡: {self.canary_percentage}%")

    def rollback(self):
        """Canaryã‚’ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        self.canary_percentage = 0
        print("Canaryã‚’ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ—§ãƒ¢ãƒ‡ãƒ«ã®ã¿ä½¿ç”¨")

    def full_rollout(self):
        """æ–°ãƒ¢ãƒ‡ãƒ«ã«å®Œå…¨ç§»è¡Œ"""
        self.canary_percentage = 100
        print("æ–°ãƒ¢ãƒ‡ãƒ«ã«å®Œå…¨ç§»è¡Œ")

    def get_metrics_comparison(self) -> Dict:
        """æ–°æ—§ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ¯”è¼ƒ"""
        return {
            'old_model_requests': len(self.metrics['old']),
            'new_model_requests': len(self.metrics['new']),
            'canary_percentage': self.canary_percentage
        }

# Canaryãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

# ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

old_model = RandomForestClassifier(n_estimators=50, random_state=42)
old_model.fit(X[:800], y[:800])

new_model = RandomForestClassifier(n_estimators=100, random_state=42)
new_model.fit(X[:800], y[:800])

# Canaryãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
deployer = CanaryDeployer(old_model, new_model)

# æ®µéšçš„ã«Canaryæ¯”ç‡ã‚’å¢—ã‚„ã™
stages = [10, 25, 50, 75, 100]

for stage in stages:
    print(f"\n=== Stage: Canary {stage}% ===")
    deployer.canary_percentage = stage

    # 100ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    for i in range(100):
        idx = random.randint(800, 999)
        prediction, model_used = deployer.predict(X[idx:idx+1])

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç¢ºèª
    metrics = deployer.get_metrics_comparison()
    print(f"æ—§ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨: {metrics['old_model_requests']} ãƒªã‚¯ã‚¨ã‚¹ãƒˆ")
    print(f"æ–°ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨: {metrics['new_model_requests']} ãƒªã‚¯ã‚¨ã‚¹ãƒˆ")

    # å•é¡ŒãŒã‚ã‚Œã°ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆã“ã®ä¾‹ã§ã¯å¸¸ã«æˆåŠŸï¼‰
    time.sleep(1)

print("\nâœ“ Canaryãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæˆåŠŸ: æ–°ãƒ¢ãƒ‡ãƒ«ã«å®Œå…¨ç§»è¡Œ")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== Stage: Canary 10% ===
Canaryæ¯”ç‡: 10%
æ—§ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨: 91 ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
æ–°ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨: 9 ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

=== Stage: Canary 25% ===
Canaryæ¯”ç‡: 25%
æ—§ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨: 166 ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
æ–°ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨: 34 ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

=== Stage: Canary 50% ===
Canaryæ¯”ç‡: 50%
æ—§ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨: 216 ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
æ–°ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨: 84 ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

=== Stage: Canary 75% ===
Canaryæ¯”ç‡: 75%
æ—§ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨: 241 ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
æ–°ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨: 159 ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

=== Stage: Canary 100% ===
Canaryæ¯”ç‡: 100%
æ—§ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨: 241 ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
æ–°ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨: 259 ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

âœ“ Canaryãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæˆåŠŸ: æ–°ãƒ¢ãƒ‡ãƒ«ã«å®Œå…¨ç§»è¡Œ
</code></pre>

<h3>3. Shadow Deployment</h3>

<p><strong>Shadowãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ</strong>ã¯ã€æ–°ãƒ¢ãƒ‡ãƒ«ã‚’æœ¬ç•ªç’°å¢ƒã§å®Ÿè¡Œã™ã‚‹ãŒã€å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯æ—§ãƒ¢ãƒ‡ãƒ«ã®çµæœã‚’è¿”ã™æ‰‹æ³•ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ–°ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ãƒªã‚¹ã‚¯ãªãè©•ä¾¡ã§ãã¾ã™ã€‚</p>

<pre><code class="language-python"># scripts/shadow_deploy.py
import time
import numpy as np
from typing import Tuple, Dict
from sklearn.metrics import accuracy_score

class ShadowDeployer:
    """Shadowãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®å®Ÿè£…"""

    def __init__(self, production_model, shadow_model):
        self.production_model = production_model
        self.shadow_model = shadow_model
        self.shadow_predictions = []
        self.production_predictions = []
        self.ground_truth = []

    def predict(self, X, y_true=None):
        """æœ¬ç•ªãƒ¢ãƒ‡ãƒ«ã¨Shadowãƒ¢ãƒ‡ãƒ«ã®ä¸¡æ–¹ã§äºˆæ¸¬"""
        # æœ¬ç•ªãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è¿”ã™ï¼‰
        prod_pred = self.production_model.predict(X)
        self.production_predictions.extend(prod_pred)

        # Shadowãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ï¼ˆãƒ­ã‚°ã®ã¿ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯è¿”ã•ãªã„ï¼‰
        shadow_pred = self.shadow_model.predict(X)
        self.shadow_predictions.extend(shadow_pred)

        # æ­£è§£ãƒ©ãƒ™ãƒ«ï¼ˆå–å¾—å¯èƒ½ãªå ´åˆï¼‰
        if y_true is not None:
            self.ground_truth.extend(y_true)

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯æœ¬ç•ªãƒ¢ãƒ‡ãƒ«ã®çµæœã®ã¿è¿”ã™
        return prod_pred

    def compare_models(self) -> Dict:
        """æœ¬ç•ªãƒ¢ãƒ‡ãƒ«ã¨Shadowãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¯”è¼ƒ"""
        if not self.ground_truth:
            return {
                'error': 'No ground truth available for comparison'
            }

        prod_accuracy = accuracy_score(
            self.ground_truth,
            self.production_predictions
        )
        shadow_accuracy = accuracy_score(
            self.ground_truth,
            self.shadow_predictions
        )

        # äºˆæ¸¬ã®ä¸€è‡´åº¦
        agreement = np.mean(
            np.array(self.production_predictions) == np.array(self.shadow_predictions)
        )

        return {
            'production_accuracy': prod_accuracy,
            'shadow_accuracy': shadow_accuracy,
            'improvement': shadow_accuracy - prod_accuracy,
            'prediction_agreement': agreement,
            'total_predictions': len(self.production_predictions)
        }

    def should_promote_shadow(self, min_improvement: float = 0.02) -> bool:
        """Shadowãƒ¢ãƒ‡ãƒ«ã‚’æœ¬ç•ªã«æ˜‡æ ¼ã™ã¹ãã‹åˆ¤å®š"""
        comparison = self.compare_models()

        if 'error' in comparison:
            return False

        return comparison['improvement'] >= min_improvement

# Shadowãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X, y = make_classification(
    n_samples=2000, n_features=20, n_informative=15,
    random_state=42
)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# æœ¬ç•ªãƒ¢ãƒ‡ãƒ«ï¼ˆRandomForestï¼‰
production_model = RandomForestClassifier(n_estimators=50, random_state=42)
production_model.fit(X_train, y_train)

# Shadowãƒ¢ãƒ‡ãƒ«ï¼ˆGradientBoosting - ã‚ˆã‚Šé«˜æ€§èƒ½ï¼‰
shadow_model = GradientBoostingClassifier(n_estimators=100, random_state=42)
shadow_model.fit(X_train, y_train)

# Shadowãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
deployer = ShadowDeployer(production_model, shadow_model)

print("=== Shadowãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆé–‹å§‹ ===\n")

# æœ¬ç•ªãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆãƒãƒƒãƒã§å‡¦ç†ï¼‰
batch_size = 50
for i in range(0, len(X_test), batch_size):
    X_batch = X_test[i:i+batch_size]
    y_batch = y_test[i:i+batch_size]

    # ä¸¡æ–¹ã®ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯æœ¬ç•ªãƒ¢ãƒ‡ãƒ«ã®çµæœã®ã¿ï¼‰
    deployer.predict(X_batch, y_batch)

    time.sleep(0.1)  # æœ¬ç•ªç’°å¢ƒã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

# ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
comparison = deployer.compare_models()

print("=== ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ ===")
print(f"æœ¬ç•ªãƒ¢ãƒ‡ãƒ«ç²¾åº¦: {comparison['production_accuracy']:.3f}")
print(f"Shadowãƒ¢ãƒ‡ãƒ«ç²¾åº¦: {comparison['shadow_accuracy']:.3f}")
print(f"æ”¹å–„: {comparison['improvement']:.3f} ({comparison['improvement']*100:+.1f}%)")
print(f"äºˆæ¸¬ä¸€è‡´åº¦: {comparison['prediction_agreement']:.2%}")
print(f"ç·äºˆæ¸¬æ•°: {comparison['total_predictions']}")

# æ˜‡æ ¼åˆ¤å®š
if deployer.should_promote_shadow(min_improvement=0.02):
    print("\nâœ“ Shadowãƒ¢ãƒ‡ãƒ«ã‚’æœ¬ç•ªã«æ˜‡æ ¼ã—ã¾ã™")
else:
    print("\nâœ— æ”¹å–„ãŒä¸ååˆ†: Shadowãƒ¢ãƒ‡ãƒ«ã¯æ˜‡æ ¼ã—ã¾ã›ã‚“")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== Shadowãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆé–‹å§‹ ===

=== ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ ===
æœ¬ç•ªãƒ¢ãƒ‡ãƒ«ç²¾åº¦: 0.883
Shadowãƒ¢ãƒ‡ãƒ«ç²¾åº¦: 0.915
æ”¹å–„: 0.032 (+3.2%)
äºˆæ¸¬ä¸€è‡´åº¦: 94.83%
ç·äºˆæ¸¬æ•°: 600

âœ“ Shadowãƒ¢ãƒ‡ãƒ«ã‚’æœ¬ç•ªã«æ˜‡æ ¼ã—ã¾ã™
</code></pre>

<h3>4. A/Bãƒ†ã‚¹ãƒˆ</h3>

<pre><code class="language-python"># scripts/ab_test.py
import numpy as np
from scipy import stats
from typing import Dict, List

class ABTester:
    """A/Bãƒ†ã‚¹ãƒˆã®å®Ÿè£…"""

    def __init__(self, model_a, model_b):
        self.model_a = model_a
        self.model_b = model_b
        self.results_a = []
        self.results_b = []

    def assign_and_predict(self, X, y_true, user_id: int):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ã‚°ãƒ«ãƒ¼ãƒ—A/Bã«å‰²ã‚Šå½“ã¦ã¦äºˆæ¸¬"""
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã®ãƒãƒƒã‚·ãƒ¥ã§ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æ±ºå®šï¼ˆä¸€è²«æ€§ã‚’ä¿ã¤ï¼‰
        group = 'A' if hash(user_id) % 2 == 0 else 'B'

        if group == 'A':
            prediction = self.model_a.predict(X)
            correct = (prediction == y_true).astype(int)
            self.results_a.extend(correct)
        else:
            prediction = self.model_b.predict(X)
            correct = (prediction == y_true).astype(int)
            self.results_b.extend(correct)

        return prediction, group

    def calculate_statistics(self) -> Dict:
        """çµ±è¨ˆçš„æœ‰æ„æ€§ã‚’è¨ˆç®—"""
        accuracy_a = np.mean(self.results_a)
        accuracy_b = np.mean(self.results_b)

        # 2æ¨™æœ¬tæ¤œå®š
        t_stat, p_value = stats.ttest_ind(self.results_a, self.results_b)

        # åŠ¹æœé‡ï¼ˆCohen's dï¼‰
        pooled_std = np.sqrt(
            (np.std(self.results_a)**2 + np.std(self.results_b)**2) / 2
        )
        cohens_d = (accuracy_b - accuracy_a) / pooled_std if pooled_std > 0 else 0

        return {
            'group_a_accuracy': accuracy_a,
            'group_b_accuracy': accuracy_b,
            'difference': accuracy_b - accuracy_a,
            'p_value': p_value,
            'is_significant': p_value < 0.05,
            'cohens_d': cohens_d,
            'sample_size_a': len(self.results_a),
            'sample_size_b': len(self.results_b)
        }

    def recommend_winner(self) -> str:
        """å‹è€…ã‚’æ¨å¥¨"""
        stats_result = self.calculate_statistics()

        if not stats_result['is_significant']:
            return "No significant difference"

        if stats_result['difference'] > 0:
            return "Model B (statistically significant improvement)"
        else:
            return "Model A (statistically significant better)"

# A/Bãƒ†ã‚¹ãƒˆã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X, y = make_classification(
    n_samples=3000, n_features=20, n_informative=15,
    random_state=42
)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.4, random_state=42
)

# ãƒ¢ãƒ‡ãƒ«Aï¼ˆç¾è¡Œï¼‰
model_a = RandomForestClassifier(n_estimators=50, random_state=42)
model_a.fit(X_train, y_train)

# ãƒ¢ãƒ‡ãƒ«Bï¼ˆæ–°è¦ï¼‰
model_b = GradientBoostingClassifier(n_estimators=100, random_state=42)
model_b.fit(X_train, y_train)

# A/Bãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
ab_tester = ABTester(model_a, model_b)

print("=== A/Bãƒ†ã‚¹ãƒˆé–‹å§‹ ===\n")

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«äºˆæ¸¬ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
for user_id in range(len(X_test)):
    prediction, group = ab_tester.assign_and_predict(
        X_test[user_id:user_id+1],
        y_test[user_id],
        user_id
    )

# çµ±è¨ˆåˆ†æ
stats_result = ab_tester.calculate_statistics()

print("=== A/Bãƒ†ã‚¹ãƒˆçµæœ ===")
print(f"ã‚°ãƒ«ãƒ¼ãƒ—Aï¼ˆç¾è¡Œãƒ¢ãƒ‡ãƒ«ï¼‰:")
print(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: {stats_result['sample_size_a']}")
print(f"  ç²¾åº¦: {stats_result['group_a_accuracy']:.3f}")

print(f"\nã‚°ãƒ«ãƒ¼ãƒ—Bï¼ˆæ–°è¦ãƒ¢ãƒ‡ãƒ«ï¼‰:")
print(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: {stats_result['sample_size_b']}")
print(f"  ç²¾åº¦: {stats_result['group_b_accuracy']:.3f}")

print(f"\nçµ±è¨ˆåˆ†æ:")
print(f"  å·®åˆ†: {stats_result['difference']:.3f} ({stats_result['difference']*100:+.1f}%)")
print(f"  på€¤: {stats_result['p_value']:.4f}")
print(f"  çµ±è¨ˆçš„æœ‰æ„: {'Yes' if stats_result['is_significant'] else 'No'}")
print(f"  åŠ¹æœé‡ (Cohen's d): {stats_result['cohens_d']:.3f}")

print(f"\næ¨å¥¨: {ab_tester.recommend_winner()}")
</code></pre>

<h3>ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæˆ¦ç•¥ã®æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>æˆ¦ç•¥</th>
<th>ãƒªã‚¹ã‚¯</th>
<th>ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯é€Ÿåº¦</th>
<th>é©ç”¨å ´é¢</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Blue-Green</strong></td>
<td>ä¸­</td>
<td>å³åº§</td>
<td>è¿…é€Ÿãªåˆ‡ã‚Šæ›¿ãˆãŒå¿…è¦</td>
</tr>
<tr>
<td><strong>Canary</strong></td>
<td>ä½</td>
<td>æ®µéšçš„</td>
<td>ãƒªã‚¹ã‚¯ã‚’æœ€å°åŒ–ã—ãŸã„</td>
</tr>
<tr>
<td><strong>Shadow</strong></td>
<td>æœ€ä½</td>
<td>ä¸è¦ï¼ˆæœ¬ç•ªã«æœªåæ˜ ï¼‰</td>
<td>æ€§èƒ½è©•ä¾¡ã®ã¿</td>
</tr>
<tr>
<td><strong>A/Bãƒ†ã‚¹ãƒˆ</strong></td>
<td>ä½ã€œä¸­</td>
<td>æ®µéšçš„</td>
<td>çµ±è¨ˆçš„ãªæ¤œè¨¼ãŒå¿…è¦</td>
</tr>
</tbody>
</table>

<hr>

<h2>5.5 ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ä¾‹</h2>

<h3>å®Œå…¨ãªCI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè£…</h3>

<p>ã“ã“ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã€ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã€ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¾ã§ã‚’å«ã‚€å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚</p>

<h4>ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ</h4>

<pre><code>ml-project/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml
â”‚       â”œâ”€â”€ train.yml
â”‚       â””â”€â”€ deploy.yml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ validation.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â””â”€â”€ evaluate.py
â”‚   â””â”€â”€ deploy/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ deployment.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_data_validation.py
â”‚   â”œâ”€â”€ test_model.py
â”‚   â””â”€â”€ test_integration.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ model_config.yaml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
</code></pre>

<h4>ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«</h4>

<pre><code class="language-python"># src/data/validation.py
import pandas as pd
import numpy as np
from typing import Dict, List
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataValidator:
    """ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼ã‚¯ãƒ©ã‚¹"""

    def __init__(self, schema: Dict):
        self.schema = schema
        self.validation_errors = []

    def validate(self, df: pd.DataFrame) -> bool:
        """å…¨æ¤œè¨¼ã‚’å®Ÿè¡Œ"""
        self.validation_errors = []

        checks = [
            self._check_schema(df),
            self._check_missing_values(df),
            self._check_value_ranges(df),
            self._check_data_types(df),
            self._check_duplicates(df)
        ]

        all_passed = all(checks)

        if all_passed:
            logger.info("âœ“ å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãŒæˆåŠŸã—ã¾ã—ãŸ")
        else:
            logger.error(f"âœ— {len(self.validation_errors)} ä»¶ã®æ¤œè¨¼ã‚¨ãƒ©ãƒ¼")
            for error in self.validation_errors:
                logger.error(f"  - {error}")

        return all_passed

    def _check_schema(self, df: pd.DataFrame) -> bool:
        """ã‚¹ã‚­ãƒ¼ãƒï¼ˆåˆ—åï¼‰ã®æ¤œè¨¼"""
        expected_columns = set(self.schema.keys())
        actual_columns = set(df.columns)

        if expected_columns != actual_columns:
            missing = expected_columns - actual_columns
            extra = actual_columns - expected_columns

            if missing:
                self.validation_errors.append(f"æ¬ è½ã—ã¦ã„ã‚‹åˆ—: {missing}")
            if extra:
                self.validation_errors.append(f"ä½™åˆ†ãªåˆ—: {extra}")
            return False

        return True

    def _check_missing_values(self, df: pd.DataFrame) -> bool:
        """æ¬ æå€¤ã®æ¤œè¨¼"""
        missing_counts = df.isnull().sum()
        total_rows = len(df)

        for col, count in missing_counts.items():
            if count > 0:
                threshold = self.schema.get(col, {}).get('max_missing_rate', 0.05)
                missing_rate = count / total_rows

                if missing_rate > threshold:
                    self.validation_errors.append(
                        f"{col}: æ¬ æç‡ {missing_rate:.2%} > {threshold:.2%}"
                    )
                    return False

        return True

    def _check_value_ranges(self, df: pd.DataFrame) -> bool:
        """å€¤ã®ç¯„å›²æ¤œè¨¼"""
        all_valid = True

        for col, col_schema in self.schema.items():
            if 'min' in col_schema:
                if (df[col] < col_schema['min']).any():
                    self.validation_errors.append(
                        f"{col}: æœ€å°å€¤é•å (< {col_schema['min']})"
                    )
                    all_valid = False

            if 'max' in col_schema:
                if (df[col] > col_schema['max']).any():
                    self.validation_errors.append(
                        f"{col}: æœ€å¤§å€¤é•å (> {col_schema['max']})"
                    )
                    all_valid = False

        return all_valid

    def _check_data_types(self, df: pd.DataFrame) -> bool:
        """ãƒ‡ãƒ¼ã‚¿å‹ã®æ¤œè¨¼"""
        for col, col_schema in self.schema.items():
            if 'dtype' in col_schema:
                expected_dtype = col_schema['dtype']
                actual_dtype = str(df[col].dtype)

                if expected_dtype not in actual_dtype:
                    self.validation_errors.append(
                        f"{col}: å‹ä¸ä¸€è‡´ (æœŸå¾…: {expected_dtype}, å®Ÿéš›: {actual_dtype})"
                    )
                    return False

        return True

    def _check_duplicates(self, df: pd.DataFrame) -> bool:
        """é‡è¤‡è¡Œã®æ¤œè¨¼"""
        duplicates = df.duplicated().sum()

        if duplicates > 0:
            max_duplicates = self.schema.get('_global', {}).get('max_duplicates', 0)
            if duplicates > max_duplicates:
                self.validation_errors.append(
                    f"é‡è¤‡è¡ŒãŒå¤šã™ãã‚‹: {duplicates} > {max_duplicates}"
                )
                return False

        return True

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ã‚¹ã‚­ãƒ¼ãƒå®šç¾©
    schema = {
        'age': {'dtype': 'int', 'min': 0, 'max': 120, 'max_missing_rate': 0.05},
        'income': {'dtype': 'float', 'min': 0, 'max_missing_rate': 0.1},
        'score': {'dtype': 'float', 'min': 0, 'max': 1, 'max_missing_rate': 0.02},
        '_global': {'max_duplicates': 10}
    }

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    df = pd.DataFrame({
        'age': [25, 30, 35, 40, 45],
        'income': [50000, 60000, 70000, 80000, 90000],
        'score': [0.7, 0.8, 0.6, 0.9, 0.75]
    })

    validator = DataValidator(schema)
    is_valid = validator.validate(df)

    if is_valid:
        print("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼æˆåŠŸ")
    else:
        print("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å¤±æ•—")
</code></pre>

<h4>ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜</h4>

<pre><code class="language-python"># src/models/train.py
import pandas as pd
import numpy as np
import joblib
import json
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelTrainer:
    """ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¯ãƒ©ã‚¹"""

    def __init__(self, config: dict):
        self.config = config
        self.model = None
        self.metrics = {}

    def train(self, X_train, y_train):
        """ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’"""
        logger.info("ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹...")

        self.model = RandomForestClassifier(
            n_estimators=self.config.get('n_estimators', 100),
            max_depth=self.config.get('max_depth', None),
            random_state=self.config.get('random_state', 42)
        )

        self.model.fit(X_train, y_train)

        # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        cv_scores = cross_val_score(
            self.model, X_train, y_train,
            cv=self.config.get('cv_folds', 5)
        )

        logger.info(f"CVç²¾åº¦: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})")

        self.metrics['cv_accuracy'] = cv_scores.mean()
        self.metrics['cv_std'] = cv_scores.std()

    def evaluate(self, X_test, y_test):
        """ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡"""
        logger.info("ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ä¸­...")

        y_pred = self.model.predict(X_test)

        self.metrics['test_accuracy'] = accuracy_score(y_test, y_pred)
        self.metrics['test_precision'] = precision_score(y_test, y_pred, average='weighted')
        self.metrics['test_recall'] = recall_score(y_test, y_pred, average='weighted')
        self.metrics['test_f1'] = f1_score(y_test, y_pred, average='weighted')

        logger.info(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {self.metrics['test_accuracy']:.3f}")
        logger.info(f"F1ã‚¹ã‚³ã‚¢: {self.metrics['test_f1']:.3f}")

        return self.metrics

    def save_model(self, path: str):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        joblib.dump(self.model, path)
        logger.info(f"ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {path}")

    def save_metrics(self, path: str):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¿å­˜"""
        with open(path, 'w') as f:
            json.dump(self.metrics, f, indent=2)
        logger.info(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¿å­˜: {path}")

    def check_performance_threshold(self, min_accuracy: float = 0.8) -> bool:
        """æ€§èƒ½é–¾å€¤ã‚’ãƒã‚§ãƒƒã‚¯"""
        if self.metrics['test_accuracy'] < min_accuracy:
            logger.error(
                f"ç²¾åº¦ãŒé–¾å€¤ä»¥ä¸‹: {self.metrics['test_accuracy']:.3f} < {min_accuracy}"
            )
            return False

        logger.info(f"âœ“ æ€§èƒ½é–¾å€¤ã‚’ã‚¯ãƒªã‚¢: {self.metrics['test_accuracy']:.3f}")
        return True

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    from sklearn.datasets import make_classification

    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    X, y = make_classification(
        n_samples=1000, n_features=20, n_informative=15,
        random_state=42
    )
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # è¨­å®š
    config = {
        'n_estimators': 100,
        'max_depth': 10,
        'random_state': 42,
        'cv_folds': 5
    }

    # å­¦ç¿’ã¨è©•ä¾¡
    trainer = ModelTrainer(config)
    trainer.train(X_train, y_train)
    trainer.evaluate(X_test, y_test)

    # æ€§èƒ½ãƒã‚§ãƒƒã‚¯
    if trainer.check_performance_threshold(min_accuracy=0.85):
        trainer.save_model('models/model.pkl')
        trainer.save_metrics('models/metrics.json')
    else:
        logger.error("ãƒ¢ãƒ‡ãƒ«ãŒåŸºæº–ã‚’æº€ãŸã—ã¦ã„ã¾ã›ã‚“")
</code></pre>

<h4>å®Œå…¨ãªGitHub Actionsãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</h4>

<pre><code class="language-yaml"># .github/workflows/complete-ml-cicd.yml
name: Complete ML CI/CD Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # æ¯æ—¥åˆå‰2æ™‚ã«å†å­¦ç¿’
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.9'
  MIN_ACCURACY: '0.85'

jobs:
  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        pip install flake8 black isort

    - name: Check code formatting (black)
      run: black --check src/ tests/

    - name: Check import sorting (isort)
      run: isort --check-only src/ tests/

    - name: Lint (flake8)
      run: flake8 src/ tests/ --max-line-length=100

  test:
    name: Run Tests
    runs-on: ubuntu-latest
    needs: code-quality

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-cov

    - name: Run unit tests
      run: pytest tests/ -v --cov=src --cov-report=xml

    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml

  data-validation:
    name: Validate Training Data
    runs-on: ubuntu-latest
    needs: test

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: pip install -r requirements.txt

    - name: Download data
      run: |
        # ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã«ç½®ãæ›ãˆï¼‰
        python scripts/download_data.py

    - name: Validate data quality
      run: |
        python src/data/validation.py --data data/training_data.csv

  train:
    name: Train Model
    runs-on: ubuntu-latest
    needs: data-validation

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: pip install -r requirements.txt

    - name: Train model
      run: |
        python src/models/train.py \
          --data data/training_data.csv \
          --config config/model_config.yaml \
          --output models/model.pkl

    - name: Evaluate model
      id: evaluate
      run: |
        python src/models/evaluate.py \
          --model models/model.pkl \
          --data data/test_data.csv \
          --metrics-output models/metrics.json

    - name: Check performance threshold
      run: |
        python scripts/check_threshold.py \
          --metrics models/metrics.json \
          --min-accuracy ${{ env.MIN_ACCURACY }}

    - name: Upload model artifact
      uses: actions/upload-artifact@v3
      with:
        name: trained-model
        path: |
          models/model.pkl
          models/metrics.json
        retention-days: 30

  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: train
    if: github.ref == 'refs/heads/main'

    steps:
    - uses: actions/checkout@v3

    - name: Download model
      uses: actions/download-artifact@v3
      with:
        name: trained-model
        path: models/

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install deployment tools
      run: pip install boto3

    - name: Deploy to staging (Canary)
      run: |
        python src/deploy/deployment.py \
          --model models/model.pkl \
          --environment staging \
          --strategy canary \
          --canary-percentage 10
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

    - name: Run smoke tests
      run: |
        python tests/smoke_tests.py \
          --endpoint ${{ secrets.STAGING_ENDPOINT }} \
          --timeout 300

  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: deploy-staging
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment:
      name: production
      url: https://ml-api.example.com

    steps:
    - uses: actions/checkout@v3

    - name: Download model
      uses: actions/download-artifact@v3
      with:
        name: trained-model
        path: models/

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Deploy with Blue-Green strategy
      run: |
        python src/deploy/deployment.py \
          --model models/model.pkl \
          --environment production \
          --strategy blue-green
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

    - name: Monitor deployment
      run: |
        python scripts/monitor_deployment.py \
          --duration 600 \
          --rollback-on-error

    - name: Notify deployment success
      if: success()
      uses: slackapi/slack-github-action@v1
      with:
        webhook-url: ${{ secrets.SLACK_WEBHOOK }}
        payload: |
          {
            "text": "âœ… ML Model deployed to production successfully"
          }
</code></pre>

<hr>

<h2>5.6 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>MLç‰¹æœ‰ã®CI/CD</strong></p>
<ul>
<li>ã‚³ãƒ¼ãƒ‰ + ãƒ‡ãƒ¼ã‚¿ + ãƒ¢ãƒ‡ãƒ«ã®çµ±åˆç®¡ç†</li>
<li>ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã¸ã®å¯¾å¿œ</li>
<li>ç¶™ç¶šçš„ãªå†å­¦ç¿’ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°</li>
</ul></li>

<li><p><strong>è‡ªå‹•ãƒ†ã‚¹ãƒˆæˆ¦ç•¥</strong></p>
<ul>
<li>Unit Tests: ã‚³ãƒ¼ãƒ‰ã®æ­£ç¢ºæ€§</li>
<li>Data Validation: ãƒ‡ãƒ¼ã‚¿å“è³ª</li>
<li>Model Performance Tests: ãƒ¢ãƒ‡ãƒ«æ€§èƒ½</li>
<li>Integration Tests: ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“</li>
</ul></li>

<li><p><strong>GitHub Actionsã®æ´»ç”¨</strong></p>
<ul>
<li>è‡ªå‹•åŒ–ã•ã‚ŒãŸãƒ†ã‚¹ãƒˆå®Ÿè¡Œ</li>
<li>ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</li>
<li>æ€§èƒ½å›å¸°ãƒ†ã‚¹ãƒˆ</li>
</ul></li>

<li><p><strong>ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæˆ¦ç•¥</strong></p>
<ul>
<li>Blue-Green: è¿…é€Ÿãªåˆ‡ã‚Šæ›¿ãˆ</li>
<li>Canary: ãƒªã‚¹ã‚¯æœ€å°åŒ–</li>
<li>Shadow: å®‰å…¨ãªè©•ä¾¡</li>
<li>A/Bãƒ†ã‚¹ãƒˆ: çµ±è¨ˆçš„æ¤œè¨¼</li>
</ul></li>

<li><p><strong>æœ¬ç•ªé‹ç”¨</strong></p>
<ul>
<li>ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</li>
<li>è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ</li>
<li>ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯</li>
</ul></li>
</ol>

<h3>ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>

<table>
<thead>
<tr>
<th>åŸå‰‡</th>
<th>èª¬æ˜</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>è‡ªå‹•åŒ–å„ªå…ˆ</strong></td>
<td>æ‰‹å‹•æ“ä½œã‚’æœ€å°åŒ–ã—ã€å†ç¾æ€§ã‚’ç¢ºä¿</td>
</tr>
<tr>
<td><strong>æ®µéšçš„ãƒ‡ãƒ—ãƒ­ã‚¤</strong></td>
<td>Canaryã‚„shadowã§æ®µéšçš„ã«ãƒªã‚¹ã‚¯ã‚’ä½æ¸›</td>
</tr>
<tr>
<td><strong>æ€§èƒ½é–¾å€¤ã®è¨­å®š</strong></td>
<td>æ˜ç¢ºãªåŸºæº–ã§è‡ªå‹•çš„ã«åˆå¦åˆ¤å®š</td>
</tr>
<tr>
<td><strong>è¿…é€Ÿãªãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯</strong></td>
<td>å•é¡Œç™ºç”Ÿæ™‚ã«å³åº§ã«å‰ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«æˆ»ã›ã‚‹ä»•çµ„ã¿</td>
</tr>
<tr>
<td><strong>åŒ…æ‹¬çš„ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°</strong></td>
<td>ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã¨ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å¸¸æ™‚ç›£è¦–</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>

<p>æœ¬ç« ã§å­¦ã‚“ã CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åŸºç›¤ã¨ã—ã¦ã€ä»¥ä¸‹ã®æ‹¡å¼µã‚’æ¤œè¨ã§ãã¾ã™ï¼š</p>
<ul>
<li>ç‰¹å¾´é‡ã‚¹ãƒˆã‚¢ã®çµ±åˆ</li>
<li>MLflowã«ã‚ˆã‚‹å®Ÿé¨“ç®¡ç†</li>
<li>Kubernetesã§ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ</li>
<li>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã‚·ã‚¹ãƒ†ãƒ </li>
<li>ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜å¯èƒ½æ€§ã®çµ±åˆ</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>
<p>å¾“æ¥ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã®CI/CDã¨ã€æ©Ÿæ¢°å­¦ç¿’ã®CI/CDã®ä¸»ãªé•ã„ã‚’3ã¤æŒ™ã’ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<ol>
<li><p><strong>ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®é•ã„</strong></p>
<ul>
<li>å¾“æ¥: ã‚³ãƒ¼ãƒ‰ã®ã¿</li>
<li>ML: ã‚³ãƒ¼ãƒ‰ + ãƒ‡ãƒ¼ã‚¿ + ãƒ¢ãƒ‡ãƒ«ã®3è¦ç´ </li>
<li>ç†ç”±: MLã§ã¯ãƒ‡ãƒ¼ã‚¿å“è³ªã¨ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãŒçµæœã«ç›´æ¥å½±éŸ¿</li>
</ul></li>

<li><p><strong>å“è³ªæŒ‡æ¨™ã®é•ã„</strong></p>
<ul>
<li>å¾“æ¥: ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹ç‡ã€ã‚³ãƒ¼ãƒ‰ã‚«ãƒãƒ¬ãƒƒã‚¸</li>
<li>ML: ç²¾åº¦ã€å†ç¾ç‡ã€F1ã‚¹ã‚³ã‚¢ã€ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆ</li>
<li>ç†ç”±: MLã¯çµ±è¨ˆçš„ãªæ€§èƒ½æŒ‡æ¨™ã§è©•ä¾¡</li>
</ul></li>

<li><p><strong>ç¶™ç¶šçš„ãªæ›´æ–°ã®å¿…è¦æ€§</strong></p>
<ul>
<li>å¾“æ¥: æ©Ÿèƒ½è¿½åŠ ã‚„ä¿®æ­£æ™‚ã«æ›´æ–°</li>
<li>ML: ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®å¤‰åŒ–ã«å¿œã˜ã¦å®šæœŸçš„ã«å†å­¦ç¿’</li>
<li>ç†ç”±: ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã«ã‚ˆã‚Šãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãŒåŠ£åŒ–</li>
</ul></li>
</ol>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>pytestã‚’ä½¿ã£ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®æœ€ä½ç²¾åº¦ãŒ85%ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ãƒ†ã‚¹ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import pytest
import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

class TestModelAccuracy:
    """ãƒ¢ãƒ‡ãƒ«ç²¾åº¦ã®ãƒ†ã‚¹ãƒˆ"""

    @pytest.fixture
    def trained_model_and_data(self):
        """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™"""
        X, y = make_classification(
            n_samples=1000, n_features=20,
            n_informative=15, random_state=42
        )
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        model = RandomForestClassifier(
            n_estimators=100, random_state=42
        )
        model.fit(X_train, y_train)

        return model, X_test, y_test

    def test_minimum_accuracy_85_percent(self, trained_model_and_data):
        """æœ€ä½ç²¾åº¦85%ã‚’æº€ãŸã™ã“ã¨ã‚’ç¢ºèª"""
        model, X_test, y_test = trained_model_and_data

        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)

        min_accuracy = 0.85

        assert accuracy >= min_accuracy, \
               f"ç²¾åº¦ãŒåŸºæº–ä»¥ä¸‹: {accuracy:.3f} < {min_accuracy}"

        print(f"âœ“ ãƒ†ã‚¹ãƒˆæˆåŠŸ: ç²¾åº¦ = {accuracy:.3f}")

if __name__ == "__main__":
    pytest.main([__file__, '-v'])
</code></pre>

<p><strong>å®Ÿè¡Œ</strong>ï¼š</p>
<pre><code>pytest test_model_accuracy.py -v
</code></pre>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>Blue-Greenãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã¨Canaryãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®é•ã„ã‚’èª¬æ˜ã—ã€ãã‚Œãã‚Œã‚’ã©ã®ã‚ˆã†ãªçŠ¶æ³ã§ä½¿ã†ã¹ãã‹è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>Blue-Greenãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ</strong>ï¼š</p>
<ul>
<li>ç‰¹å¾´: 2ã¤ã®å®Œå…¨ãªç’°å¢ƒã‚’ç”¨æ„ã—ã€ç¬æ™‚ã«åˆ‡ã‚Šæ›¿ãˆ</li>
<li>ãƒ¡ãƒªãƒƒãƒˆ: å³åº§ã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ãŒå¯èƒ½ã€ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ãªã—</li>
<li>ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ: 2å€ã®ãƒªã‚½ãƒ¼ã‚¹ãŒå¿…è¦ã€å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ä¸€åº¦ã«å½±éŸ¿</li>
</ul>

<p><strong>Canaryãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ</strong>ï¼š</p>
<ul>
<li>ç‰¹å¾´: æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä¸€éƒ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æ®µéšçš„ã«å…¬é–‹</li>
<li>ãƒ¡ãƒªãƒƒãƒˆ: ãƒªã‚¹ã‚¯æœ€å°åŒ–ã€å•é¡Œã®æ—©æœŸç™ºè¦‹</li>
<li>ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ: åˆ‡ã‚Šæ›¿ãˆã«æ™‚é–“ãŒã‹ã‹ã‚‹ã€è¤‡é›‘ãªç®¡ç†</li>
</ul>

<p><strong>ä½¿ã„åˆ†ã‘</strong>ï¼š</p>

<table>
<thead>
<tr>
<th>çŠ¶æ³</th>
<th>æ¨å¥¨æˆ¦ç•¥</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td>è¿…é€Ÿãªå±•é–‹ãŒå¿…è¦</td>
<td>Blue-Green</td>
<td>å³åº§ã«åˆ‡ã‚Šæ›¿ãˆå¯èƒ½</td>
</tr>
<tr>
<td>ãƒªã‚¹ã‚¯ã‚’æœ€å°åŒ–ã—ãŸã„</td>
<td>Canary</td>
<td>æ®µéšçš„ã«å½±éŸ¿ç¯„å›²ã‚’æ‹¡å¤§</td>
</tr>
<tr>
<td>å¤§ããªå¤‰æ›´</td>
<td>Canary</td>
<td>å•é¡Œã‚’æ—©æœŸã«æ¤œå‡º</td>
</tr>
<tr>
<td>å°ã•ãªæ”¹å–„</td>
<td>Blue-Green</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ã§è¿…é€Ÿ</td>
</tr>
<tr>
<td>ãƒªã‚½ãƒ¼ã‚¹åˆ¶ç´„ã‚ã‚Š</td>
<td>Canary</td>
<td>è¿½åŠ ãƒªã‚½ãƒ¼ã‚¹ä¸è¦</td>
</tr>
</tbody>
</table>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡ºã—ã€ç²¾åº¦ãŒé–¾å€¤ã‚’ä¸‹å›ã£ãŸå ´åˆã«ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’ã™ã‚‹Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from scipy.stats import ks_2samp
import joblib
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DriftDetectorAndRetrainer:
    """ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã¨è‡ªå‹•å†å­¦ç¿’"""

    def __init__(self, model_path: str, accuracy_threshold: float = 0.8):
        self.model_path = model_path
        self.accuracy_threshold = accuracy_threshold
        self.model = joblib.load(model_path) if model_path else None
        self.reference_data = None

    def set_reference_data(self, X_ref: np.ndarray):
        """åŸºæº–ãƒ‡ãƒ¼ã‚¿ã‚’è¨­å®š"""
        self.reference_data = X_ref
        logger.info(f"åŸºæº–ãƒ‡ãƒ¼ã‚¿ã‚’è¨­å®š: {X_ref.shape}")

    def detect_drift(self, X_new: np.ndarray, alpha: float = 0.05) -> bool:
        """Kolmogorov-Smirnovæ¤œå®šã§ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡º"""
        if self.reference_data is None:
            raise ValueError("åŸºæº–ãƒ‡ãƒ¼ã‚¿ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

        drifts = []

        for i in range(X_new.shape[1]):
            statistic, p_value = ks_2samp(
                self.reference_data[:, i],
                X_new[:, i]
            )

            is_drift = p_value < alpha
            drifts.append(is_drift)

            if is_drift:
                logger.warning(
                    f"ç‰¹å¾´é‡{i}ã«ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º: p={p_value:.4f} < {alpha}"
                )

        drift_ratio = sum(drifts) / len(drifts)
        logger.info(f"ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºç‡: {drift_ratio:.2%}")

        return drift_ratio > 0.3  # 30%ä»¥ä¸Šã®ç‰¹å¾´é‡ã§ãƒ‰ãƒªãƒ•ãƒˆ

    def check_performance(self, X_test: np.ndarray, y_test: np.ndarray) -> float:
        """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’ãƒã‚§ãƒƒã‚¯"""
        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)

        logger.info(f"ç¾åœ¨ã®ç²¾åº¦: {accuracy:.3f}")

        return accuracy

    def retrain(self, X_train: np.ndarray, y_train: np.ndarray):
        """ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’"""
        logger.info("ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’é–‹å§‹...")

        self.model = RandomForestClassifier(
            n_estimators=100, random_state=42
        )
        self.model.fit(X_train, y_train)

        logger.info("ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’å®Œäº†")

    def save_model(self, path: str = None):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        save_path = path or self.model_path
        joblib.dump(self.model, save_path)
        logger.info(f"ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {save_path}")

    def run_monitoring_cycle(
        self,
        X_new: np.ndarray,
        y_new: np.ndarray,
        X_train: np.ndarray,
        y_train: np.ndarray
    ):
        """ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œ"""
        logger.info("=== ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚µã‚¤ã‚¯ãƒ«é–‹å§‹ ===")

        # 1. ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º
        has_drift = self.detect_drift(X_new)

        # 2. æ€§èƒ½ãƒã‚§ãƒƒã‚¯
        accuracy = self.check_performance(X_new, y_new)

        # 3. å†å­¦ç¿’åˆ¤å®š
        needs_retrain = (
            has_drift or
            accuracy < self.accuracy_threshold
        )

        if needs_retrain:
            logger.warning(
                f"å†å­¦ç¿’ãŒå¿…è¦: ãƒ‰ãƒªãƒ•ãƒˆ={has_drift}, "
                f"ç²¾åº¦={accuracy:.3f} < {self.accuracy_threshold}"
            )

            # 4. å†å­¦ç¿’å®Ÿè¡Œ
            self.retrain(X_train, y_train)

            # 5. æ–°ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
            new_accuracy = self.check_performance(X_new, y_new)
            logger.info(f"å†å­¦ç¿’å¾Œã®ç²¾åº¦: {new_accuracy:.3f}")

            # 6. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            self.save_model()

            return True
        else:
            logger.info("å†å­¦ç¿’ä¸è¦: ãƒ¢ãƒ‡ãƒ«ã¯æ­£å¸¸ã«å‹•ä½œä¸­")
            return False

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split

    # åˆæœŸãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«
    X_initial, y_initial = make_classification(
        n_samples=1000, n_features=20, random_state=42
    )
    X_train, X_ref, y_train, y_ref = train_test_split(
        X_initial, y_initial, test_size=0.3, random_state=42
    )

    # åˆæœŸãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    initial_model = RandomForestClassifier(n_estimators=100, random_state=42)
    initial_model.fit(X_train, y_train)
    joblib.dump(initial_model, 'model.pkl')

    # ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
    monitor = DriftDetectorAndRetrainer(
        model_path='model.pkl',
        accuracy_threshold=0.85
    )
    monitor.set_reference_data(X_ref)

    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ‰ãƒªãƒ•ãƒˆã‚ã‚Šï¼‰
    X_new, y_new = make_classification(
        n_samples=200, n_features=20, random_state=100
    )
    # ãƒ‰ãƒªãƒ•ãƒˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    X_new = X_new + np.random.normal(0, 1.5, X_new.shape)

    # ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œ
    was_retrained = monitor.run_monitoring_cycle(
        X_new, y_new, X_train, y_train
    )

    if was_retrained:
        print("\nâœ“ ãƒ¢ãƒ‡ãƒ«ãŒå†å­¦ç¿’ã•ã‚Œã¾ã—ãŸ")
    else:
        print("\nâœ“ ãƒ¢ãƒ‡ãƒ«ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™")
</code></pre>

</details>

<h3>å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>GitHub Actionsã§ã€Pull Requestæ™‚ã«ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ãŒå‰å›ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ˆã‚ŠåŠ£åŒ–ã—ã¦ã„ãªã„ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€çµæœã‚’PRã«ã‚³ãƒ¡ãƒ³ãƒˆã™ã‚‹ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-yaml"># .github/workflows/pr-model-check.yml
name: PR Model Performance Check

on:
  pull_request:
    branches: [ main ]

jobs:
  compare-model-performance:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout PR branch
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install scikit-learn joblib

    - name: Download baseline model
      run: |
        # å‰å›ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        wget https://storage.example.com/models/baseline_model.pkl \
          -O models/baseline_model.pkl
        wget https://storage.example.com/models/baseline_metrics.json \
          -O models/baseline_metrics.json

    - name: Train new model (PR version)
      run: |
        python src/models/train.py \
          --data data/training_data.csv \
          --output models/new_model.pkl \
          --metrics-output models/new_metrics.json

    - name: Compare models
      id: compare
      run: |
        python scripts/compare_models.py \
          --baseline models/baseline_model.pkl \
          --new models/new_model.pkl \
          --output comparison.json

    - name: Read comparison results
      id: results
      run: |
        echo "comparison<<EOF" >> $GITHUB_OUTPUT
        cat comparison.json >> $GITHUB_OUTPUT
        echo "EOF" >> $GITHUB_OUTPUT

    - name: Comment on PR
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const comparison = JSON.parse(fs.readFileSync('comparison.json', 'utf8'));

          const accuracyDiff = comparison.new.accuracy - comparison.baseline.accuracy;
          const f1Diff = comparison.new.f1 - comparison.baseline.f1;

          const statusEmoji = accuracyDiff >= -0.01 ? 'âœ…' : 'âŒ';

          const comment = `
          ${statusEmoji} **ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒçµæœ**

          | ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ | æ–°ãƒ¢ãƒ‡ãƒ« (PR) | å¤‰åŒ– |
          |-----------|-------------|--------------|------|
          | **Accuracy** | ${comparison.baseline.accuracy.toFixed(3)} | ${comparison.new.accuracy.toFixed(3)} | ${accuracyDiff >= 0 ? '+' : ''}${accuracyDiff.toFixed(3)} |
          | **Precision** | ${comparison.baseline.precision.toFixed(3)} | ${comparison.new.precision.toFixed(3)} | ${(comparison.new.precision - comparison.baseline.precision).toFixed(3)} |
          | **Recall** | ${comparison.baseline.recall.toFixed(3)} | ${comparison.new.recall.toFixed(3)} | ${(comparison.new.recall - comparison.baseline.recall).toFixed(3)} |
          | **F1 Score** | ${comparison.baseline.f1.toFixed(3)} | ${comparison.new.f1.toFixed(3)} | ${f1Diff >= 0 ? '+' : ''}${f1Diff.toFixed(3)} |

          ### åˆ¤å®š
          ${accuracyDiff >= -0.01 ?
            'âœ… **åˆæ ¼**: æ€§èƒ½åŠ£åŒ–ã¯è¨±å®¹ç¯„å›²å†…ã§ã™' :
            'âŒ **ä¸åˆæ ¼**: æ€§èƒ½ãŒè‘—ã—ãåŠ£åŒ–ã—ã¦ã„ã¾ã™ï¼ˆè¨±å®¹: -1%ï¼‰'}

          <details>
          <summary>è©³ç´°æƒ…å ±</summary>

          - è¨“ç·´æ™‚é–“: ${comparison.training_time}ç§’
          - ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: ${(comparison.model_size_mb).toFixed(2)} MB
          - ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°: ${comparison.test_samples}

          </details>
          `;

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

    - name: Check if performance regression
      run: |
        python scripts/check_regression.py \
          --comparison comparison.json \
          --max-regression 0.01
</code></pre>

<p><strong>è£œåŠ©ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆscripts/compare_models.pyï¼‰</strong>ï¼š</p>

<pre><code class="language-python">import argparse
import json
import joblib
import time
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def compare_models(baseline_path, new_path, output_path):
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    baseline_model = joblib.load(baseline_path)
    new_model = joblib.load(new_path)

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    X, y = load_iris(return_X_y=True)
    _, X_test, _, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡
    baseline_pred = baseline_model.predict(X_test)
    baseline_metrics = {
        'accuracy': accuracy_score(y_test, baseline_pred),
        'precision': precision_score(y_test, baseline_pred, average='weighted'),
        'recall': recall_score(y_test, baseline_pred, average='weighted'),
        'f1': f1_score(y_test, baseline_pred, average='weighted')
    }

    # æ–°ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
    start = time.time()
    new_pred = new_model.predict(X_test)
    training_time = time.time() - start

    new_metrics = {
        'accuracy': accuracy_score(y_test, new_pred),
        'precision': precision_score(y_test, new_pred, average='weighted'),
        'recall': recall_score(y_test, new_pred, average='weighted'),
        'f1': f1_score(y_test, new_pred, average='weighted')
    }

    # æ¯”è¼ƒçµæœ
    result = {
        'baseline': baseline_metrics,
        'new': new_metrics,
        'training_time': training_time,
        'model_size_mb': 1.5,  # å®Ÿéš›ã®ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
        'test_samples': len(y_test)
    }

    # ä¿å­˜
    with open(output_path, 'w') as f:
        json.dump(result, f, indent=2)

    print(f"æ¯”è¼ƒçµæœã‚’ä¿å­˜: {output_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--baseline', required=True)
    parser.add_argument('--new', required=True)
    parser.add_argument('--output', required=True)
    args = parser.parse_args()

    compare_models(args.baseline, args.new, args.output)
</code></pre>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Huyen, C. (2022). <em>Designing Machine Learning Systems</em>. O'Reilly Media.</li>
<li>Kleppmann, M. (2017). <em>Designing Data-Intensive Applications</em>. O'Reilly Media.</li>
<li>GÃ©ron, A. (2019). <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em> (2nd ed.). O'Reilly Media.</li>
<li>Forsgren, N., Humble, J., & Kim, G. (2018). <em>Accelerate: The Science of Lean Software and DevOps</em>. IT Revolution Press.</li>
<li>Sato, D., Wider, A., & Windheuser, C. (2019). "Continuous Delivery for Machine Learning." <em>Martin Fowler's Blog</em>.</li>
</ol>

<div class="navigation">
    <a href="chapter4-experiment-tracking.html" class="nav-button">â† å‰ã®ç« : å®Ÿé¨“ç®¡ç†</a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
