<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第3章：クラウドデプロイメント - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>第3章：クラウドデプロイメント</h1>
            <p class="subtitle">AWS、GCP、Azureで実現するスケーラブルなMLシステム</p>
            <div class="meta">
                <span class="meta-item">📖 読了時間: 30-35分</span>
                <span class="meta-item">📊 難易度: 中級</span>
                <span class="meta-item">💻 コード例: 8個</span>
                <span class="meta-item">📝 演習問題: 3問</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>学習目標</h2>
<p>この章を読むことで、以下を習得できます：</p>
<ul>
<li>✅ 主要クラウドプラットフォーム（AWS、GCP、Azure）の特徴を理解する</li>
<li>✅ AWS SageMakerでモデルをデプロイできる</li>
<li>✅ AWS Lambdaでサーバーレス推論環境を構築できる</li>
<li>✅ GCP Vertex AIとAzure MLの基本的な使い方を理解する</li>
<li>✅ TerraformとCI/CDでマルチクラウド戦略を実装できる</li>
</ul>

<hr>

<h2>3.1 クラウドデプロイメントの選択肢</h2>

<h3>主要クラウドプラットフォーム比較</h3>

<p>機械学習モデルのデプロイメントには、主に3つの主要クラウドプラットフォームが使用されます。</p>

<table>
<thead>
<tr>
<th>プラットフォーム</th>
<th>MLサービス</th>
<th>強み</th>
<th>ユースケース</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>AWS</strong></td>
<td>SageMaker, Lambda, ECS</td>
<td>最大のシェア、豊富なサービス</td>
<td>エンタープライズ、大規模システム</td>
</tr>
<tr>
<td><strong>GCP</strong></td>
<td>Vertex AI, Cloud Run</td>
<td>TensorFlow統合、BigQuery連携</td>
<td>データ分析重視、スタートアップ</td>
</tr>
<tr>
<td><strong>Azure</strong></td>
<td>Azure ML, Functions</td>
<td>Microsoft製品統合</td>
<td>エンタープライズ（Microsoft環境）</td>
</tr>
</tbody>
</table>

<h3>デプロイメントサービスの種類</h3>

<table>
<thead>
<tr>
<th>種類</th>
<th>説明</th>
<th>AWS</th>
<th>GCP</th>
<th>Azure</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>マネージド</strong></td>
<td>フルマネージドMLプラットフォーム</td>
<td>SageMaker</td>
<td>Vertex AI</td>
<td>Azure ML</td>
</tr>
<tr>
<td><strong>サーバーレス</strong></td>
<td>イベント駆動、自動スケール</td>
<td>Lambda</td>
<td>Cloud Functions</td>
<td>Azure Functions</td>
</tr>
<tr>
<td><strong>コンテナ</strong></td>
<td>Docker/Kubernetes基盤</td>
<td>ECS/EKS</td>
<td>Cloud Run/GKE</td>
<td>AKS</td>
</tr>
</tbody>
</table>

<h3>コスト考慮</h3>

<p>クラウドデプロイメントのコスト要因：</p>

<ul>
<li><strong>コンピューティング</strong>: インスタンスタイプ、稼働時間</li>
<li><strong>ストレージ</strong>: モデルファイル、ログ保存</li>
<li><strong>ネットワーク</strong>: データ転送量</li>
<li><strong>推論リクエスト</strong>: API呼び出し回数</li>
</ul>

<blockquote>
<p><strong>コスト最適化のポイント</strong>: オートスケーリング、スポットインスタンス、適切なインスタンスサイズ選択が重要です。</p>
</blockquote>

<div class="mermaid">
graph TD
    A[デプロイメント戦略] --> B[トラフィックパターン]
    B --> C{リクエスト頻度}
    C -->|高頻度・安定| D[マネージド<br/>SageMaker/Vertex AI]
    C -->|低頻度・不規則| E[サーバーレス<br/>Lambda/Cloud Functions]
    C -->|バースト対応| F[オートスケーリング<br/>ECS/Cloud Run]

    A --> G[コスト制約]
    G --> H{予算}
    H -->|低予算| I[サーバーレス]
    H -->|中予算| J[コンテナ]
    H -->|高予算| K[専用マネージド]

    style D fill:#e3f2fd
    style E fill:#fff3e0
    style F fill:#f3e5f5
</div>

<hr>

<h2>3.2 AWS SageMakerデプロイメント</h2>

<h3>SageMaker Endpointとは</h3>

<p><strong>Amazon SageMaker</strong>は、機械学習モデルの構築、訓練、デプロイを統合的に行うマネージドサービスです。</p>

<h3>モデルのパッケージング</h3>

<pre><code class="language-python"># model_package.py
import joblib
import numpy as np
from sklearn.ensemble import RandomForestClassifier

# モデルの訓練
model = RandomForestClassifier(n_estimators=100, random_state=42)
X_train = np.random.randn(1000, 10)
y_train = np.random.randint(0, 2, 1000)
model.fit(X_train, y_train)

# モデルの保存
joblib.dump(model, 'model.joblib')
print("✓ モデルを保存しました: model.joblib")
</code></pre>

<h3>カスタム推論スクリプト</h3>

<pre><code class="language-python"># inference.py
import joblib
import json
import numpy as np

def model_fn(model_dir):
    """モデルをロード"""
    model = joblib.load(f"{model_dir}/model.joblib")
    return model

def input_fn(request_body, content_type):
    """入力データをパース"""
    if content_type == 'application/json':
        data = json.loads(request_body)
        return np.array(data['instances'])
    raise ValueError(f"Unsupported content type: {content_type}")

def predict_fn(input_data, model):
    """推論実行"""
    predictions = model.predict(input_data)
    probabilities = model.predict_proba(input_data)
    return {
        'predictions': predictions.tolist(),
        'probabilities': probabilities.tolist()
    }

def output_fn(prediction, accept):
    """レスポンスをフォーマット"""
    if accept == 'application/json':
        return json.dumps(prediction), accept
    raise ValueError(f"Unsupported accept type: {accept}")
</code></pre>

<h3>SageMakerへのデプロイ</h3>

<pre><code class="language-python">import boto3
import sagemaker
from sagemaker.sklearn.model import SKLearnModel
from datetime import datetime

# セッション設定
session = sagemaker.Session()
role = 'arn:aws:iam::123456789012:role/SageMakerRole'
bucket = session.default_bucket()

# モデルをS3にアップロード
model_data = session.upload_data(
    path='model.joblib',
    bucket=bucket,
    key_prefix='models/sklearn-model'
)

# SageMakerモデルの作成
sklearn_model = SKLearnModel(
    model_data=model_data,
    role=role,
    entry_point='inference.py',
    framework_version='1.0-1',
    py_version='py3'
)

# エンドポイントのデプロイ
endpoint_name = f'sklearn-endpoint-{datetime.now().strftime("%Y%m%d-%H%M%S")}'
predictor = sklearn_model.deploy(
    initial_instance_count=1,
    instance_type='ml.m5.large',
    endpoint_name=endpoint_name
)

print(f"✓ エンドポイントをデプロイしました: {endpoint_name}")
print(f"✓ インスタンスタイプ: ml.m5.large")
print(f"✓ インスタンス数: 1")
</code></pre>

<h3>推論リクエストの実行</h3>

<pre><code class="language-python">import boto3
import json
import numpy as np

# SageMaker Runtimeクライアント
runtime = boto3.client('sagemaker-runtime', region_name='us-east-1')

# テストデータ
test_data = {
    'instances': np.random.randn(5, 10).tolist()
}

# 推論リクエスト
response = runtime.invoke_endpoint(
    EndpointName=endpoint_name,
    ContentType='application/json',
    Accept='application/json',
    Body=json.dumps(test_data)
)

# レスポンスのパース
result = json.loads(response['Body'].read().decode())
print("\n=== 推論結果 ===")
print(f"予測: {result['predictions']}")
print(f"確率: {result['probabilities']}")

# パフォーマンス情報
print(f"\n推論時間: {response['ResponseMetadata']['HTTPHeaders'].get('x-amzn-invocation-timestamp', 'N/A')}")
</code></pre>

<h3>オートスケーリングの設定</h3>

<pre><code class="language-python">import boto3

# Auto Scalingクライアント
autoscaling = boto3.client('application-autoscaling', region_name='us-east-1')

# スケーラブルターゲットの登録
resource_id = f'endpoint/{endpoint_name}/variant/AllTraffic'
autoscaling.register_scalable_target(
    ServiceNamespace='sagemaker',
    ResourceId=resource_id,
    ScalableDimension='sagemaker:variant:DesiredInstanceCount',
    MinCapacity=1,
    MaxCapacity=5
)

# スケーリングポリシーの設定
autoscaling.put_scaling_policy(
    PolicyName=f'{endpoint_name}-scaling-policy',
    ServiceNamespace='sagemaker',
    ResourceId=resource_id,
    ScalableDimension='sagemaker:variant:DesiredInstanceCount',
    PolicyType='TargetTrackingScaling',
    TargetTrackingScalingPolicyConfiguration={
        'TargetValue': 70.0,  # 目標CPU使用率70%
        'PredefinedMetricSpecification': {
            'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance'
        },
        'ScaleInCooldown': 300,   # スケールイン待機時間（秒）
        'ScaleOutCooldown': 60    # スケールアウト待機時間（秒）
    }
)

print("✓ オートスケーリングを設定しました")
print(f"  最小インスタンス数: 1")
print(f"  最大インスタンス数: 5")
print(f"  目標メトリック: リクエスト/インスタンス")
</code></pre>

<blockquote>
<p><strong>ベストプラクティス</strong>: 本番環境では、最小2インスタンスで可用性を確保し、トラフィックパターンに応じてスケールアウト閾値を調整します。</p>
</blockquote>

<hr>

<h2>3.3 AWS Lambdaサーバーレスデプロイメント</h2>

<h3>サーバーレスアーキテクチャの利点</h3>

<ul>
<li><strong>コスト効率</strong>: 実行時間分のみ課金</li>
<li><strong>自動スケーリング</strong>: 同時実行数に応じて自動調整</li>
<li><strong>運用負荷軽減</strong>: インフラ管理不要</li>
<li><strong>高可用性</strong>: マルチAZ自動配置</li>
</ul>

<h3>Lambda関数の作成</h3>

<pre><code class="language-python"># lambda_function.py
import json
import joblib
import numpy as np
import base64
import io

# グローバルスコープでモデルをロード（コールドスタート最適化）
model = None

def load_model():
    """モデルのロード（初回のみ実行）"""
    global model
    if model is None:
        # S3からモデルをロード、またはレイヤーに含める
        model = joblib.load('/opt/model.joblib')
    return model

def lambda_handler(event, context):
    """Lambda関数のメインハンドラー"""
    try:
        # モデルのロード
        ml_model = load_model()

        # リクエストボディのパース
        body = json.loads(event.get('body', '{}'))
        instances = body.get('instances', [])

        if not instances:
            return {
                'statusCode': 400,
                'body': json.dumps({'error': 'No instances provided'})
            }

        # 推論実行
        input_data = np.array(instances)
        predictions = ml_model.predict(input_data)
        probabilities = ml_model.predict_proba(input_data)

        # レスポンス
        response = {
            'predictions': predictions.tolist(),
            'probabilities': probabilities.tolist(),
            'model_version': '1.0'
        }

        return {
            'statusCode': 200,
            'headers': {
                'Content-Type': 'application/json',
                'Access-Control-Allow-Origin': '*'
            },
            'body': json.dumps(response)
        }

    except Exception as e:
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }
</code></pre>

<h3>コンテナイメージでのデプロイ</h3>

<pre><code class="language-dockerfile"># Dockerfile
FROM public.ecr.aws/lambda/python:3.9

# 依存関係のインストール
COPY requirements.txt .
RUN pip install -r requirements.txt --target "${LAMBDA_TASK_ROOT}"

# モデルファイルのコピー
COPY model.joblib ${LAMBDA_TASK_ROOT}/opt/

# Lambda関数コードのコピー
COPY lambda_function.py ${LAMBDA_TASK_ROOT}

# ハンドラーの指定
CMD ["lambda_function.lambda_handler"]
</code></pre>

<pre><code class="language-bash">#!/bin/bash
# deploy.sh - Lambdaコンテナイメージのビルドとデプロイ

# 変数設定
AWS_REGION="us-east-1"
AWS_ACCOUNT_ID="123456789012"
ECR_REPO="ml-inference-lambda"
IMAGE_TAG="latest"

# ECRリポジトリの作成（初回のみ）
aws ecr create-repository \
    --repository-name ${ECR_REPO} \
    --region ${AWS_REGION} 2>/dev/null || true

# ECRにログイン
aws ecr get-login-password --region ${AWS_REGION} | \
    docker login --username AWS --password-stdin \
    ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com

# Dockerイメージのビルド
docker build -t ${ECR_REPO}:${IMAGE_TAG} .

# イメージのタグ付け
docker tag ${ECR_REPO}:${IMAGE_TAG} \
    ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/${ECR_REPO}:${IMAGE_TAG}

# ECRにプッシュ
docker push ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/${ECR_REPO}:${IMAGE_TAG}

echo "✓ イメージをECRにプッシュしました"
</code></pre>

<h3>API Gatewayとの統合</h3>

<pre><code class="language-python">import boto3
import json

# API Gateway作成
apigateway = boto3.client('apigateway', region_name='us-east-1')
lambda_client = boto3.client('lambda', region_name='us-east-1')

# REST APIの作成
api = apigateway.create_rest_api(
    name='ML-Inference-API',
    description='Machine Learning Inference API',
    endpointConfiguration={'types': ['REGIONAL']}
)
api_id = api['id']

# リソースの取得
resources = apigateway.get_resources(restApiId=api_id)
root_id = resources['items'][0]['id']

# /predictリソースの作成
predict_resource = apigateway.create_resource(
    restApiId=api_id,
    parentId=root_id,
    pathPart='predict'
)

# POSTメソッドの作成
apigateway.put_method(
    restApiId=api_id,
    resourceId=predict_resource['id'],
    httpMethod='POST',
    authorizationType='NONE'
)

# Lambda統合の設定
lambda_arn = f"arn:aws:lambda:us-east-1:123456789012:function:ml-inference"
apigateway.put_integration(
    restApiId=api_id,
    resourceId=predict_resource['id'],
    httpMethod='POST',
    type='AWS_PROXY',
    integrationHttpMethod='POST',
    uri=f'arn:aws:apigateway:us-east-1:lambda:path/2015-03-31/functions/{lambda_arn}/invocations'
)

# デプロイ
deployment = apigateway.create_deployment(
    restApiId=api_id,
    stageName='prod'
)

endpoint_url = f"https://{api_id}.execute-api.us-east-1.amazonaws.com/prod/predict"
print(f"✓ API Gatewayをデプロイしました")
print(f"✓ エンドポイント: {endpoint_url}")
</code></pre>

<h3>コールドスタート対策</h3>

<p>Lambdaの<strong>コールドスタート</strong>（初回起動の遅延）を軽減する方法：</p>

<table>
<thead>
<tr>
<th>手法</th>
<th>説明</th>
<th>効果</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>プロビジョニング同時実行</strong></td>
<td>常時起動インスタンスを確保</td>
<td>コールドスタート完全回避</td>
</tr>
<tr>
<td><strong>モデル最適化</strong></td>
<td>軽量モデル、量子化</td>
<td>ロード時間短縮</td>
</tr>
<tr>
<td><strong>レイヤー活用</strong></td>
<td>依存関係を別レイヤーに分離</td>
<td>デプロイパッケージ削減</td>
</tr>
<tr>
<td><strong>定期ウォームアップ</strong></td>
<td>EventBridgeで定期実行</td>
<td>アイドル状態回避</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.4 GCP Vertex AIとAzure ML</h2>

<h3>GCP Vertex AI Endpoints</h3>

<p><strong>Vertex AI</strong>は、GoogleのマネージドMLプラットフォームで、TensorFlowとの深い統合が特徴です。</p>

<pre><code class="language-python"># vertex_ai_deploy.py
from google.cloud import aiplatform

# Vertex AIの初期化
aiplatform.init(
    project='my-gcp-project',
    location='us-central1',
    staging_bucket='gs://my-ml-models'
)

# モデルのアップロード
model = aiplatform.Model.upload(
    display_name='sklearn-classifier',
    artifact_uri='gs://my-ml-models/sklearn-model',
    serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest'
)

# エンドポイントの作成
endpoint = aiplatform.Endpoint.create(display_name='sklearn-endpoint')

# モデルのデプロイ
endpoint.deploy(
    model=model,
    deployed_model_display_name='sklearn-v1',
    machine_type='n1-standard-4',
    min_replica_count=1,
    max_replica_count=5,
    traffic_percentage=100
)

print(f"✓ エンドポイントをデプロイしました: {endpoint.resource_name}")

# 推論リクエスト
instances = [[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]]
prediction = endpoint.predict(instances=instances)
print(f"予測結果: {prediction.predictions}")
</code></pre>

<h3>Azure ML Managed Endpoints</h3>

<p><strong>Azure Machine Learning</strong>は、Microsoft Azureのマネージド機械学習サービスです。</p>

<pre><code class="language-python"># azure_ml_deploy.py
from azure.ai.ml import MLClient
from azure.ai.ml.entities import (
    ManagedOnlineEndpoint,
    ManagedOnlineDeployment,
    Model,
    Environment,
    CodeConfiguration
)
from azure.identity import DefaultAzureCredential

# Azure ML Clientの初期化
credential = DefaultAzureCredential()
ml_client = MLClient(
    credential=credential,
    subscription_id='subscription-id',
    resource_group_name='ml-resources',
    workspace_name='ml-workspace'
)

# モデルの登録
model = Model(
    path='./model',
    name='sklearn-classifier',
    description='Scikit-learn classification model'
)
registered_model = ml_client.models.create_or_update(model)

# エンドポイントの作成
endpoint = ManagedOnlineEndpoint(
    name='sklearn-endpoint',
    description='Sklearn classification endpoint',
    auth_mode='key'
)
ml_client.online_endpoints.begin_create_or_update(endpoint).result()

# デプロイメントの作成
deployment = ManagedOnlineDeployment(
    name='blue',
    endpoint_name='sklearn-endpoint',
    model=registered_model,
    environment='AzureML-sklearn-1.0-ubuntu20.04-py38-cpu',
    code_configuration=CodeConfiguration(
        code='./src',
        scoring_script='score.py'
    ),
    instance_type='Standard_DS3_v2',
    instance_count=1
)
ml_client.online_deployments.begin_create_or_update(deployment).result()

# トラフィックの割り当て
endpoint.traffic = {'blue': 100}
ml_client.online_endpoints.begin_create_or_update(endpoint).result()

print(f"✓ Azure MLエンドポイントをデプロイしました: {endpoint.name}")
</code></pre>

<h3>クラウドプラットフォーム比較</h3>

<table>
<thead>
<tr>
<th>機能</th>
<th>AWS SageMaker</th>
<th>GCP Vertex AI</th>
<th>Azure ML</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>デプロイ方法</strong></td>
<td>Endpoint, Lambda</td>
<td>Endpoint, Cloud Run</td>
<td>Managed Endpoint</td>
</tr>
<tr>
<td><strong>オートスケール</strong></td>
<td>◎（柔軟）</td>
<td>◎（自動）</td>
<td>○（設定必要）</td>
</tr>
<tr>
<td><strong>モデル管理</strong></td>
<td>Model Registry</td>
<td>Model Registry</td>
<td>Model Registry</td>
</tr>
<tr>
<td><strong>モニタリング</strong></td>
<td>CloudWatch</td>
<td>Cloud Monitoring</td>
<td>Application Insights</td>
</tr>
<tr>
<td><strong>料金体系</strong></td>
<td>インスタンス時間</td>
<td>インスタンス時間</td>
<td>インスタンス時間</td>
</tr>
<tr>
<td><strong>学習コスト</strong></td>
<td>中</td>
<td>低（GCP経験者）</td>
<td>低（Azure経験者）</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.5 実践：マルチクラウド戦略とCI/CD</h2>

<h3>Terraformによるインフラ管理</h3>

<p><strong>Infrastructure as Code（IaC）</strong>で、再現可能なデプロイメントを実現します。</p>

<pre><code class="language-hcl"># terraform/main.tf - AWS SageMakerエンドポイント
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region = var.aws_region
}

# SageMaker実行ロール
resource "aws_iam_role" "sagemaker_role" {
  name = "sagemaker-execution-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "sagemaker.amazonaws.com"
      }
    }]
  })
}

# SageMakerモデル
resource "aws_sagemaker_model" "ml_model" {
  name               = "sklearn-model-${var.environment}"
  execution_role_arn = aws_iam_role.sagemaker_role.arn

  primary_container {
    image          = var.container_image
    model_data_url = var.model_data_s3_uri
  }

  tags = {
    Environment = var.environment
    ManagedBy   = "Terraform"
  }
}

# SageMakerエンドポイント設定
resource "aws_sagemaker_endpoint_configuration" "endpoint_config" {
  name = "sklearn-endpoint-config-${var.environment}"

  production_variants {
    variant_name           = "AllTraffic"
    model_name             = aws_sagemaker_model.ml_model.name
    initial_instance_count = var.initial_instance_count
    instance_type          = var.instance_type
  }

  tags = {
    Environment = var.environment
    ManagedBy   = "Terraform"
  }
}

# SageMakerエンドポイント
resource "aws_sagemaker_endpoint" "endpoint" {
  name                 = "sklearn-endpoint-${var.environment}"
  endpoint_config_name = aws_sagemaker_endpoint_configuration.endpoint_config.name

  tags = {
    Environment = var.environment
    ManagedBy   = "Terraform"
  }
}

# 変数定義
variable "aws_region" {
  default = "us-east-1"
}

variable "environment" {
  description = "Environment name (dev, staging, prod)"
  type        = string
}

variable "container_image" {
  description = "SageMaker container image URI"
  type        = string
}

variable "model_data_s3_uri" {
  description = "S3 URI of model artifacts"
  type        = string
}

variable "instance_type" {
  default = "ml.m5.large"
}

variable "initial_instance_count" {
  default = 1
}

# 出力
output "endpoint_name" {
  value = aws_sagemaker_endpoint.endpoint.name
}

output "endpoint_arn" {
  value = aws_sagemaker_endpoint.endpoint.arn
}
</code></pre>

<h3>GitHub ActionsによるCI/CD</h3>

<pre><code class="language-yaml"># .github/workflows/deploy-ml-model.yml
name: Deploy ML Model

on:
  push:
    branches:
      - main
      - develop
  pull_request:
    branches:
      - main

env:
  AWS_REGION: us-east-1
  MODEL_BUCKET: ml-models-artifacts

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-cov

      - name: Run tests
        run: |
          pytest tests/ --cov=src --cov-report=xml

      - name: Upload coverage
        uses: codecov/codecov-action@v3

  build-and-deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop'

    steps:
      - uses: actions/checkout@v3

      - name: Set environment
        run: |
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "ENVIRONMENT=prod" >> $GITHUB_ENV
          else
            echo "ENVIRONMENT=dev" >> $GITHUB_ENV
          fi

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Train and package model
        run: |
          python src/train.py
          tar -czf model.tar.gz model.joblib

      - name: Upload model to S3
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          aws s3 cp model.tar.gz \
            s3://${{ env.MODEL_BUCKET }}/${{ env.ENVIRONMENT }}/model-${TIMESTAMP}.tar.gz
          echo "MODEL_S3_URI=s3://${{ env.MODEL_BUCKET }}/${{ env.ENVIRONMENT }}/model-${TIMESTAMP}.tar.gz" >> $GITHUB_ENV

      - name: Build Docker image
        run: |
          docker build -t ml-inference:${{ github.sha }} .

      - name: Push to ECR
        run: |
          aws ecr get-login-password --region ${{ env.AWS_REGION }} | \
            docker login --username AWS --password-stdin \
            ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com

          docker tag ml-inference:${{ github.sha }} \
            ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ml-inference:${{ github.sha }}

          docker push \
            ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ml-inference:${{ github.sha }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2

      - name: Terraform Init
        working-directory: ./terraform
        run: terraform init

      - name: Terraform Plan
        working-directory: ./terraform
        run: |
          terraform plan \
            -var="environment=${{ env.ENVIRONMENT }}" \
            -var="model_data_s3_uri=${{ env.MODEL_S3_URI }}" \
            -var="container_image=${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ml-inference:${{ github.sha }}" \
            -out=tfplan

      - name: Terraform Apply
        if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop'
        working-directory: ./terraform
        run: terraform apply -auto-approve tfplan

      - name: Smoke test
        run: |
          ENDPOINT_NAME=$(terraform -chdir=./terraform output -raw endpoint_name)
          python scripts/smoke_test.py --endpoint-name $ENDPOINT_NAME

  notify:
    needs: build-and-deploy
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Send Slack notification
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: 'ML Model deployment ${{ job.status }}'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
</code></pre>

<h3>環境別デプロイメント戦略</h3>

<div class="mermaid">
graph LR
    A[コード変更] --> B[GitHub Push]
    B --> C{ブランチ}
    C -->|develop| D[Dev環境]
    C -->|staging| E[Staging環境]
    C -->|main| F[Production環境]

    D --> G[自動テスト]
    E --> H[統合テスト]
    F --> I[スモークテスト]

    G --> J[自動デプロイ]
    H --> K[手動承認]
    K --> L[デプロイ]
    I --> M[ヘルスチェック]

    style D fill:#e8f5e9
    style E fill:#fff3e0
    style F fill:#ffebee
</div>

<table>
<thead>
<tr>
<th>環境</th>
<th>用途</th>
<th>インスタンス数</th>
<th>デプロイ方法</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Dev</strong></td>
<td>開発・テスト</td>
<td>1</td>
<td>自動（developブランチ）</td>
</tr>
<tr>
<td><strong>Staging</strong></td>
<td>統合テスト・QA</td>
<td>2</td>
<td>自動（stagingブランチ）</td>
</tr>
<tr>
<td><strong>Production</strong></td>
<td>本番運用</td>
<td>3+（オートスケール）</td>
<td>手動承認後</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.6 本章のまとめ</h2>

<h3>学んだこと</h3>

<ol>
<li><p><strong>クラウドプラットフォームの選択</strong></p>
<ul>
<li>AWS、GCP、Azureの特徴と使い分け</li>
<li>マネージド、サーバーレス、コンテナの比較</li>
<li>コスト最適化の考慮事項</li>
</ul></li>

<li><p><strong>AWS SageMaker</strong></p>
<ul>
<li>エンドポイントの作成とデプロイ</li>
<li>カスタム推論スクリプトの実装</li>
<li>オートスケーリングの設定</li>
</ul></li>

<li><p><strong>AWS Lambdaサーバーレス</strong></p>
<ul>
<li>Lambda関数の作成とコンテナデプロイ</li>
<li>API Gatewayとの統合</li>
<li>コールドスタート対策</li>
</ul></li>

<li><p><strong>GCP Vertex AIとAzure ML</strong></p>
<ul>
<li>Vertex AI Endpointsのデプロイ</li>
<li>Azure ML Managed Endpointsの活用</li>
<li>プラットフォーム間の比較</li>
</ul></li>

<li><p><strong>マルチクラウド戦略</strong></p>
<ul>
<li>Terraformによるインフラコード化</li>
<li>GitHub ActionsでのCI/CDパイプライン</li>
<li>環境別デプロイメント管理</li>
</ul></li>
</ol>

<h3>選択ガイドライン</h3>

<table>
<thead>
<tr>
<th>要件</th>
<th>推奨ソリューション</th>
<th>理由</th>
</tr>
</thead>
<tbody>
<tr>
<td>高頻度リクエスト</td>
<td>SageMaker/Vertex AI</td>
<td>専用インスタンスで低レイテンシ</td>
</tr>
<tr>
<td>低頻度・不規則</td>
<td>Lambda/Cloud Functions</td>
<td>コスト効率が高い</td>
</tr>
<tr>
<td>バースト対応</td>
<td>ECS/Cloud Run</td>
<td>柔軟なスケーリング</td>
</tr>
<tr>
<td>マルチモデル</td>
<td>Kubernetes (EKS/GKE)</td>
<td>統一管理とリソース効率</td>
</tr>
<tr>
<td>TensorFlow中心</td>
<td>GCP Vertex AI</td>
<td>ネイティブ統合</td>
</tr>
<tr>
<td>Microsoft環境</td>
<td>Azure ML</td>
<td>既存システムとの親和性</td>
</tr>
</tbody>
</table>

<h3>次の章へ</h3>

<p>第4章では、<strong>モニタリングと運用管理</strong>を学びます：</p>
<ul>
<li>パフォーマンスモニタリング</li>
<li>ログ管理とトレーシング</li>
<li>モデルドリフト検出</li>
<li>A/Bテストとカナリアデプロイ</li>
<li>インシデント対応</li>
</ul>

<hr>

<h2>演習問題</h2>

<h3>問題1（難易度：medium）</h3>
<p>AWS SageMakerとAWS Lambdaのどちらを選ぶべきか、以下のシナリオで判断し、理由を説明してください。</p>

<p><strong>シナリオA</strong>: ECサイトの商品推薦システム（1日10万リクエスト、レスポンス時間100ms以内）<br>
<strong>シナリオB</strong>: バッチ処理による月次レポート生成（月1回、処理時間1時間）</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>

<p><strong>シナリオA：AWS SageMaker推奨</strong></p>
<ul>
<li><strong>理由</strong>:
<ul>
<li>高頻度リクエスト（1日10万 = 1秒あたり約1.2リクエスト）で安定したトラフィック</li>
<li>レスポンス時間100ms以内が求められ、コールドスタートは許容できない</li>
<li>専用インスタンスで常時稼働により、低レイテンシを保証</li>
<li>オートスケーリングでトラフィックのピークに対応可能</li>
</ul></li>
<li><strong>構成</strong>: ml.m5.large × 2インスタンス（最小）、オートスケール最大5インスタンス</li>
<li><strong>コスト</strong>: 月額約$300-500（インスタンス稼働時間ベース）</li>
</ul>

<p><strong>シナリオB：AWS Lambda推奨</strong></p>
<ul>
<li><strong>理由</strong>:
<ul>
<li>低頻度（月1回）の実行で、常時稼働は不要</li>
<li>処理時間1時間でも、Lambda（最大15分）× 4回の分割実行で対応可能</li>
<li>実行時間のみ課金で、大幅なコスト削減</li>
<li>レスポンス時間の厳しい要件がない</li>
</ul></li>
<li><strong>構成</strong>: Lambda（メモリ3008MB）、Step Functionsで処理オーケストレーション</li>
<li><strong>コスト</strong>: 月額約$5-10（実行時間のみ）</li>
</ul>

<p><strong>判断基準まとめ</strong>：</p>
<table>
<thead>
<tr>
<th>要因</th>
<th>SageMaker</th>
<th>Lambda</th>
</tr>
</thead>
<tbody>
<tr>
<td>リクエスト頻度</td>
<td>高頻度・安定</td>
<td>低頻度・不規則</td>
</tr>
<tr>
<td>レイテンシ要件</td>
<td>厳しい（< 100ms）</td>
<td>緩い（> 1秒OK）</td>
</tr>
<tr>
<td>コスト特性</td>
<td>固定コスト高</td>
<td>従量課金</td>
</tr>
<tr>
<td>運用負荷</td>
<td>中（スケール管理）</td>
<td>低（フルマネージド）</td>
</tr>
</tbody>
</table>

</details>

<h3>問題2（難易度：hard）</h3>
<p>TerraformとGitHub Actionsを使用して、開発環境と本番環境で異なる設定（インスタンス数、タイプ）を持つSageMakerエンドポイントをデプロイする構成を設計してください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>

<p><strong>1. Terraform変数ファイル（環境別）</strong></p>

<pre><code class="language-hcl"># terraform/environments/dev.tfvars
environment            = "dev"
instance_type          = "ml.t3.medium"
initial_instance_count = 1
min_capacity           = 1
max_capacity           = 2
enable_autoscaling     = false

# terraform/environments/prod.tfvars
environment            = "prod"
instance_type          = "ml.m5.xlarge"
initial_instance_count = 2
min_capacity           = 2
max_capacity           = 10
enable_autoscaling     = true
</code></pre>

<p><strong>2. Terraformメインファイル</strong></p>

<pre><code class="language-hcl"># terraform/main.tf
resource "aws_sagemaker_endpoint_configuration" "endpoint_config" {
  name = "sklearn-endpoint-config-${var.environment}"

  production_variants {
    variant_name           = "AllTraffic"
    model_name             = aws_sagemaker_model.ml_model.name
    initial_instance_count = var.initial_instance_count
    instance_type          = var.instance_type
  }
}

resource "aws_appautoscaling_target" "sagemaker_target" {
  count              = var.enable_autoscaling ? 1 : 0
  service_namespace  = "sagemaker"
  resource_id        = "endpoint/${aws_sagemaker_endpoint.endpoint.name}/variant/AllTraffic"
  scalable_dimension = "sagemaker:variant:DesiredInstanceCount"
  min_capacity       = var.min_capacity
  max_capacity       = var.max_capacity
}

resource "aws_appautoscaling_policy" "sagemaker_policy" {
  count              = var.enable_autoscaling ? 1 : 0
  name               = "sagemaker-scaling-policy-${var.environment}"
  service_namespace  = "sagemaker"
  resource_id        = aws_appautoscaling_target.sagemaker_target[0].resource_id
  scalable_dimension = aws_appautoscaling_target.sagemaker_target[0].scalable_dimension
  policy_type        = "TargetTrackingScaling"

  target_tracking_scaling_policy_configuration {
    predefined_metric_specification {
      predefined_metric_type = "SageMakerVariantInvocationsPerInstance"
    }
    target_value = 1000.0
  }
}
</code></pre>

<p><strong>3. GitHub Actions Workflow</strong></p>

<pre><code class="language-yaml"># .github/workflows/deploy.yml
jobs:
  deploy:
    steps:
      - name: Set environment variables
        run: |
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "TFVARS_FILE=prod.tfvars" >> $GITHUB_ENV
            echo "REQUIRE_APPROVAL=true" >> $GITHUB_ENV
          else
            echo "TFVARS_FILE=dev.tfvars" >> $GITHUB_ENV
            echo "REQUIRE_APPROVAL=false" >> $GITHUB_ENV
          fi

      - name: Terraform Plan
        working-directory: ./terraform
        run: |
          terraform plan \
            -var-file="environments/${{ env.TFVARS_FILE }}" \
            -out=tfplan

      - name: Wait for approval (prod only)
        if: env.REQUIRE_APPROVAL == 'true'
        uses: trstringer/manual-approval@v1
        with:
          approvers: platform-team
          minimum-approvals: 2

      - name: Terraform Apply
        working-directory: ./terraform
        run: terraform apply -auto-approve tfplan
</code></pre>

<p><strong>4. デプロイフロー</strong></p>

<div class="mermaid">
graph TD
    A[Git Push] --> B{ブランチ判定}
    B -->|develop| C[Dev環境設定<br/>ml.t3.medium×1]
    B -->|main| D[Prod環境設定<br/>ml.m5.xlarge×2]
    C --> E[自動デプロイ]
    D --> F[承認待機]
    F --> G[手動承認]
    G --> H[デプロイ実行]
    E --> I[スモークテスト]
    H --> I
</div>

<p><strong>構成のポイント</strong>：</p>
<ul>
<li>環境ごとに異なるインスタンスタイプ・数を定義</li>
<li>本番環境のみオートスケーリング有効化</li>
<li>本番デプロイは手動承認ゲート追加</li>
<li>Terraform Workspaceで環境分離</li>
</ul>

</details>

<h3>問題3（難易度：hard）</h3>
<p>AWS Lambdaのコールドスタート問題を軽減するために、どのような手法を組み合わせるべきか、具体的な実装を含めて説明してください。</p>

<details>
<summary>解答例</summary>

<p><strong>解答</strong>：</p>

<p><strong>1. プロビジョニング同時実行の設定</strong></p>

<pre><code class="language-python">import boto3

lambda_client = boto3.client('lambda')

# プロビジョニング同時実行の設定
lambda_client.put_provisioned_concurrency_config(
    FunctionName='ml-inference',
    Qualifier='$LATEST',  # またはバージョン/エイリアス
    ProvisionedConcurrentExecutions=5  # 常時5インスタンスを起動
)

print("✓ プロビジョニング同時実行を設定しました（5インスタンス）")
</code></pre>

<p><strong>効果</strong>: 常時起動インスタンスでコールドスタート完全回避<br>
<strong>コスト</strong>: 通常実行の約2倍（常時課金）</p>

<p><strong>2. 軽量モデルとレイヤー分離</strong></p>

<pre><code class="language-python"># モデルの軽量化
import joblib
from sklearn.ensemble import RandomForestClassifier

# 元のモデル
model = RandomForestClassifier(n_estimators=100, max_depth=10)
# サイズ: 約50MB

# 軽量化（木の数を削減）
model_light = RandomForestClassifier(n_estimators=20, max_depth=8)
# サイズ: 約10MB（80%削減）

# 量子化（オプション）
import onnx
import onnxruntime
# ONNX形式で量子化してサイズ削減
</code></pre>

<p><strong>Lambda Layerの活用</strong>:</p>

<pre><code class="language-bash"># 依存関係をレイヤーに分離
mkdir -p layer/python/lib/python3.9/site-packages
pip install scikit-learn numpy -t layer/python/lib/python3.9/site-packages
cd layer
zip -r layer.zip .

# レイヤーの公開
aws lambda publish-layer-version \
    --layer-name ml-dependencies \
    --zip-file fileb://layer.zip \
    --compatible-runtimes python3.9
</code></pre>

<p><strong>効果</strong>: デプロイパッケージ削減でコールドスタート時間短縮（10MB → 1-2秒、50MB → 5-10秒）</p>

<p><strong>3. EventBridgeによる定期ウォームアップ</strong></p>

<pre><code class="language-python">import boto3

events = boto3.client('events')
lambda_arn = 'arn:aws:lambda:us-east-1:123456789012:function:ml-inference'

# CloudWatch Eventsルールの作成
rule_response = events.put_rule(
    Name='lambda-warmup-rule',
    ScheduleExpression='rate(5 minutes)',  # 5分ごとに実行
    State='ENABLED',
    Description='Keep Lambda warm to avoid cold starts'
)

# Lambda関数をターゲットに設定
events.put_targets(
    Rule='lambda-warmup-rule',
    Targets=[
        {
            'Id': '1',
            'Arn': lambda_arn,
            'Input': json.dumps({'warmup': True})  # ウォームアップフラグ
        }
    ]
)

print("✓ 5分ごとのウォームアップを設定しました")
</code></pre>

<p><strong>Lambda関数の修正</strong>:</p>

<pre><code class="language-python">def lambda_handler(event, context):
    # ウォームアップリクエストの判定
    if event.get('warmup'):
        print("Warmup request - keeping instance alive")
        return {'statusCode': 200, 'body': 'warmed up'}

    # 通常の推論処理
    # ...
</code></pre>

<p><strong>効果</strong>: アイドル状態回避（コスト増加: 月額$1-5程度）</p>

<p><strong>4. 最適な組み合わせ戦略</strong></p>

<table>
<thead>
<tr>
<th>トラフィックパターン</th>
<th>推奨手法</th>
<th>期待効果</th>
</tr>
</thead>
<tbody>
<tr>
<td>常時高頻度（>10 req/s）</td>
<td>プロビジョニング同時実行</td>
<td>コールドスタート0%</td>
</tr>
<tr>
<td>中頻度（1-10 req/s）</td>
<td>軽量化 + 定期ウォームアップ</td>
<td>コールドスタート<5%</td>
</tr>
<tr>
<td>低頻度（<1 req/s）</td>
<td>軽量化のみ</td>
<td>起動時間1-2秒</td>
</tr>
<tr>
<td>バースト対応</td>
<td>全手法組み合わせ</td>
<td>最大パフォーマンス</td>
</tr>
</tbody>
</table>

<p><strong>実装例（全手法統合）</strong>:</p>

<pre><code class="language-python"># 統合戦略
# 1. 軽量モデル（10MB以下）
# 2. Lambda Layer活用
# 3. プロビジョニング同時実行（ピーク時間のみ）
# 4. EventBridgeウォームアップ（5分間隔）

# コスト試算（月間10万リクエスト想定）
# - 通常Lambda: $5
# - プロビジョニング: $30（ピーク8時間/日）
# - ウォームアップ: $3
# - 合計: 約$40/月（SageMaker比70%削減）
</code></pre>

</details>

<hr>

<h2>参考文献</h2>

<ol>
<li>Amazon Web Services. (2024). <em>Amazon SageMaker Developer Guide</em>. AWS Documentation.</li>
<li>Google Cloud. (2024). <em>Vertex AI Documentation</em>. Google Cloud Documentation.</li>
<li>Microsoft Azure. (2024). <em>Azure Machine Learning Documentation</em>. Microsoft Learn.</li>
<li>HashiCorp. (2024). <em>Terraform AWS Provider Documentation</em>. Terraform Registry.</li>
<li>Géron, A. (2022). <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em> (3rd ed.). O'Reilly Media.</li>
</ol>

<div class="navigation">
    <a href="chapter2-containerization.html" class="nav-button">← 前の章: コンテナ化とDocker</a>
    <a href="chapter4-monitoring.html" class="nav-button">次の章: モニタリングと運用管理 →</a>
</div>

    </main>

    <footer>
        <p><strong>作成者</strong>: AI Terakoya Content Team</p>
        <p><strong>バージョン</strong>: 1.0 | <strong>作成日</strong>: 2025-10-23</p>
        <p><strong>ライセンス</strong>: Creative Commons BY 4.0</p>
        <p>© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
