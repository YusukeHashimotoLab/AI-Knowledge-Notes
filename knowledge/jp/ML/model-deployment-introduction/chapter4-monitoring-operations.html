<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬4ç« ï¼šãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨é‹ç”¨ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
        <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/wp/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/ML/model-deployment-introduction/index.html">Model Deployment</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 4</span>
        </div>
    </nav>

    <header>
        <div class="header-content">
            <h1>ç¬¬4ç« ï¼šãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨é‹ç”¨</h1>
            <p class="subtitle">æœ¬ç•ªç’°å¢ƒã§ã®ç¶™ç¶šçš„ãªç›£è¦–ã¨æ”¹å–„</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… æ§‹é€ åŒ–ãƒ­ã‚°ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
<li>âœ… Prometheus + Grafanaã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡ºã§ãã‚‹</li>
<li>âœ… A/Bãƒ†ã‚¹ãƒˆã¨ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… ãƒ¢ãƒ‡ãƒ«æ›´æ–°ã¨å†è¨“ç·´ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’è¨­è¨ˆã§ãã‚‹</li>
<li>âœ… æœ¬ç•ªé‹ç”¨ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’é©ç”¨ã§ãã‚‹</li>
</ul>

<hr>

<h2>4.1 ãƒ­ã‚°ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°</h2>

<h3>æ§‹é€ åŒ–ãƒ­ã‚°ï¼ˆJSON Loggingï¼‰</h3>

<p><strong>æ§‹é€ åŒ–ãƒ­ã‚°</strong>ã¯ã€æ©Ÿæ¢°å¯èª­ãªå½¢å¼ï¼ˆJSONï¼‰ã§ãƒ­ã‚°ã‚’è¨˜éŒ²ã—ã€åˆ†æã¨æ¤œç´¢ã‚’å®¹æ˜“ã«ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">import json
import logging
from datetime import datetime
from typing import Dict, Any

class JSONFormatter(logging.Formatter):
    """æ§‹é€ åŒ–JSONãƒ­ã‚°ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼"""

    def format(self, record: logging.LogRecord) -> str:
        log_data = {
            'timestamp': datetime.utcnow().isoformat(),
            'level': record.levelname,
            'logger': record.name,
            'message': record.getMessage(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno
        }

        # è¿½åŠ ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        if hasattr(record, 'prediction_id'):
            log_data['prediction_id'] = record.prediction_id
        if hasattr(record, 'model_version'):
            log_data['model_version'] = record.model_version
        if hasattr(record, 'latency_ms'):
            log_data['latency_ms'] = record.latency_ms
        if hasattr(record, 'input_features'):
            log_data['input_features'] = record.input_features

        # ä¾‹å¤–æƒ…å ±
        if record.exc_info:
            log_data['exception'] = self.formatException(record.exc_info)

        return json.dumps(log_data)


# ãƒ­ã‚¬ãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
def setup_logger(name: str, log_file: str = 'model_predictions.log') -> logging.Logger:
    """æ§‹é€ åŒ–ãƒ­ã‚°ã‚’ä½¿ç”¨ã™ã‚‹ãƒ­ã‚¬ãƒ¼ã‚’è¨­å®š"""
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)

    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    file_handler = logging.FileHandler(log_file)
    file_handler.setFormatter(JSONFormatter())

    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆé–‹ç™ºç”¨ï¼‰
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(
        logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    )

    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    return logger


# ä½¿ç”¨ä¾‹
logger = setup_logger('ml_model')

# é€šå¸¸ã®ãƒ­ã‚°
logger.info('Model service started')

# äºˆæ¸¬ãƒ­ã‚°ï¼ˆè¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãï¼‰
import time
start_time = time.time()

# ... äºˆæ¸¬å‡¦ç† ...

logger.info(
    'Prediction completed',
    extra={
        'prediction_id': 'pred-12345',
        'model_version': 'v1.2.0',
        'latency_ms': (time.time() - start_time) * 1000,
        'input_features': {'age': 35, 'income': 50000}
    }
)

print("\n=== æ§‹é€ åŒ–ãƒ­ã‚°ã®ä¾‹ ===")
print("ãƒ­ã‚°ã¯ model_predictions.log ã« JSONå½¢å¼ã§ä¿å­˜ã•ã‚Œã¾ã™")
</code></pre>

<p><strong>å‡ºåŠ›ã•ã‚Œã‚‹JSONãƒ­ã‚°</strong>ï¼š</p>
<pre><code class="language-json">{
  "timestamp": "2025-10-23T12:34:56.789012",
  "level": "INFO",
  "logger": "ml_model",
  "message": "Prediction completed",
  "module": "main",
  "function": "predict",
  "line": 42,
  "prediction_id": "pred-12345",
  "model_version": "v1.2.0",
  "latency_ms": 123.45,
  "input_features": {"age": 35, "income": 50000}
}
</code></pre>

<h3>Prometheus + Grafanaã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—</h3>

<p><strong>Prometheus</strong>ã¯æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã€<strong>Grafana</strong>ã¯å¯è¦–åŒ–ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚</p>

<h4>Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å®Ÿè£…</h4>

<pre><code class="language-python">from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
import random

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å®šç¾©
prediction_counter = Counter(
    'model_predictions_total',
    'Total number of predictions',
    ['model_version', 'status']
)

prediction_latency = Histogram(
    'model_prediction_latency_seconds',
    'Prediction latency in seconds',
    ['model_version'],
    buckets=(0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0)
)

active_predictions = Gauge(
    'model_active_predictions',
    'Number of predictions currently being processed'
)

model_accuracy = Gauge(
    'model_accuracy',
    'Current model accuracy',
    ['model_version']
)


class ModelMonitor:
    """ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚¯ãƒ©ã‚¹"""

    def __init__(self, model_version: str = 'v1.0.0'):
        self.model_version = model_version

    def predict_with_monitoring(self, features: dict) -> dict:
        """ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ä»˜ãäºˆæ¸¬"""
        active_predictions.inc()  # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–äºˆæ¸¬æ•°ã‚’å¢—ã‚„ã™

        try:
            # äºˆæ¸¬æ™‚é–“ã®è¨ˆæ¸¬
            with prediction_latency.labels(model_version=self.model_version).time():
                # å®Ÿéš›ã®äºˆæ¸¬å‡¦ç†ï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
                time.sleep(random.uniform(0.01, 0.2))
                prediction = random.choice([0, 1])
                confidence = random.uniform(0.6, 0.99)

            # æˆåŠŸã‚«ã‚¦ãƒ³ãƒˆ
            prediction_counter.labels(
                model_version=self.model_version,
                status='success'
            ).inc()

            return {
                'prediction': prediction,
                'confidence': confidence,
                'model_version': self.model_version
            }

        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ãƒˆ
            prediction_counter.labels(
                model_version=self.model_version,
                status='error'
            ).inc()
            raise

        finally:
            active_predictions.dec()  # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–äºˆæ¸¬æ•°ã‚’æ¸›ã‚‰ã™

    def update_accuracy(self, accuracy: float):
        """ãƒ¢ãƒ‡ãƒ«ç²¾åº¦ã®æ›´æ–°"""
        model_accuracy.labels(model_version=self.model_version).set(accuracy)


# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•
print("Starting Prometheus metrics server on port 8000...")
start_http_server(8000)

# ä½¿ç”¨ä¾‹
monitor = ModelMonitor(model_version='v1.2.0')

# è¤‡æ•°ã®äºˆæ¸¬ã‚’å®Ÿè¡Œ
print("\n=== äºˆæ¸¬å®Ÿè¡Œã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›† ===")
for i in range(10):
    result = monitor.predict_with_monitoring({'feature1': i})
    print(f"äºˆæ¸¬ {i+1}: {result['prediction']} (ä¿¡é ¼åº¦: {result['confidence']:.2f})")
    time.sleep(0.1)

# ç²¾åº¦ã‚’æ›´æ–°
monitor.update_accuracy(0.92)
print(f"\nãƒ¢ãƒ‡ãƒ«ç²¾åº¦ã‚’æ›´æ–°: 92%")
print(f"\nãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯ http://localhost:8000/metrics ã§ç¢ºèªã§ãã¾ã™")
</code></pre>

<h4>Prometheusè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆprometheus.ymlï¼‰</h4>

<pre><code class="language-yaml">global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'ml_model'
    static_configs:
      - targets: ['localhost:8000']
        labels:
          environment: 'production'
          service: 'recommendation_model'
</code></pre>

<h3>ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹</h3>

<pre><code class="language-python">from prometheus_client import Summary, Info
import numpy as np
from typing import List

# ã‚ˆã‚Šè©³ç´°ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹
prediction_score_summary = Summary(
    'model_prediction_score',
    'Distribution of prediction scores',
    ['model_version']
)

feature_summary = Summary(
    'model_input_feature_value',
    'Distribution of input feature values',
    ['feature_name']
)

model_info = Info(
    'model_metadata',
    'Model metadata information'
)


class AdvancedModelMonitor:
    """é«˜åº¦ãªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½"""

    def __init__(self, model_version: str):
        self.model_version = model_version

        # ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¨­å®š
        model_info.info({
            'version': model_version,
            'framework': 'scikit-learn',
            'algorithm': 'RandomForest',
            'trained_date': '2025-10-20'
        })

    def track_prediction(self, features: dict, prediction_score: float):
        """äºˆæ¸¬ã‚¹ã‚³ã‚¢ã¨ç‰¹å¾´é‡ã‚’è¿½è·¡"""
        # äºˆæ¸¬ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ
        prediction_score_summary.labels(
            model_version=self.model_version
        ).observe(prediction_score)

        # å„ç‰¹å¾´é‡ã®åˆ†å¸ƒ
        for feature_name, value in features.items():
            if isinstance(value, (int, float)):
                feature_summary.labels(
                    feature_name=feature_name
                ).observe(value)

    def track_batch_predictions(self, batch_features: List[dict],
                               batch_scores: List[float]):
        """ãƒãƒƒãƒäºˆæ¸¬ã®è¿½è·¡"""
        for features, score in zip(batch_features, batch_scores):
            self.track_prediction(features, score)


# ä½¿ç”¨ä¾‹
advanced_monitor = AdvancedModelMonitor(model_version='v1.2.0')

# ãƒãƒƒãƒäºˆæ¸¬ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("\n=== ãƒãƒƒãƒäºˆæ¸¬ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° ===")
batch_size = 100
batch_features = [
    {
        'age': np.random.randint(18, 80),
        'income': np.random.randint(20000, 150000),
        'credit_score': np.random.randint(300, 850)
    }
    for _ in range(batch_size)
]
batch_scores = np.random.beta(5, 2, batch_size).tolist()

advanced_monitor.track_batch_predictions(batch_features, batch_scores)
print(f"{batch_size}ä»¶ã®äºˆæ¸¬ã‚’è¿½è·¡ã—ã¾ã—ãŸ")
</code></pre>

<h3>ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š</h3>

<h4>Prometheusã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«ï¼ˆalerts.ymlï¼‰</h4>

<pre><code class="language-yaml">groups:
  - name: ml_model_alerts
    interval: 30s
    rules:
      # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚¢ãƒ©ãƒ¼ãƒˆ
      - alert: HighPredictionLatency
        expr: histogram_quantile(0.95, rate(model_prediction_latency_seconds_bucket[5m])) > 1.0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High prediction latency detected"
          description: "95th percentile latency is {{ $value }}s (threshold: 1.0s)"

      # ã‚¨ãƒ©ãƒ¼ç‡ã‚¢ãƒ©ãƒ¼ãƒˆ
      - alert: HighErrorRate
        expr: rate(model_predictions_total{status="error"}[5m]) / rate(model_predictions_total[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"

      # ç²¾åº¦ä½ä¸‹ã‚¢ãƒ©ãƒ¼ãƒˆ
      - alert: ModelAccuracyDrop
        expr: model_accuracy < 0.85
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Model accuracy dropped below threshold"
          description: "Current accuracy is {{ $value }} (threshold: 0.85)"

      # ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯æ€¥å¢—ã‚¢ãƒ©ãƒ¼ãƒˆ
      - alert: TrafficSpike
        expr: rate(model_predictions_total[5m]) > 1000
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Unexpected traffic spike"
          description: "Request rate is {{ $value }} req/s (threshold: 1000 req/s)"
</code></pre>

<hr>

<h2>4.2 ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¿½è·¡</h2>

<h3>ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º</h3>

<p><strong>ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆ</strong>ã¯ã€å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒãŒæ™‚é–“ã¨ã¨ã‚‚ã«å¤‰åŒ–ã™ã‚‹ç¾è±¡ã§ã™ã€‚</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
from scipy.stats import ks_2samp, chi2_contingency
from typing import Tuple, Dict
import warnings

class DataDriftDetector:
    """ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºå™¨"""

    def __init__(self, reference_data: pd.DataFrame, threshold: float = 0.05):
        """
        Args:
            reference_data: åŸºæº–ãƒ‡ãƒ¼ã‚¿ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰
            threshold: çµ±è¨ˆçš„æœ‰æ„æ°´æº–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.05ï¼‰
        """
        self.reference_data = reference_data
        self.threshold = threshold

    def detect_drift_numerical(self, current_data: pd.DataFrame,
                               feature: str) -> Tuple[bool, float]:
        """
        æ•°å€¤ç‰¹å¾´é‡ã®ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºï¼ˆKolmogorov-Smirnovæ¤œå®šï¼‰

        Returns:
            (ãƒ‰ãƒªãƒ•ãƒˆæœ‰ç„¡, på€¤)
        """
        ref_values = self.reference_data[feature].dropna()
        cur_values = current_data[feature].dropna()

        # KSæ¤œå®š
        statistic, p_value = ks_2samp(ref_values, cur_values)

        # på€¤ãŒé–¾å€¤ã‚ˆã‚Šå°ã•ã„å ´åˆã€ãƒ‰ãƒªãƒ•ãƒˆã‚ã‚Š
        drift_detected = p_value < self.threshold

        return drift_detected, p_value

    def detect_drift_categorical(self, current_data: pd.DataFrame,
                                 feature: str) -> Tuple[bool, float]:
        """
        ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ã®ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºï¼ˆã‚«ã‚¤äºŒä¹—æ¤œå®šï¼‰

        Returns:
            (ãƒ‰ãƒªãƒ•ãƒˆæœ‰ç„¡, på€¤)
        """
        ref_counts = self.reference_data[feature].value_counts()
        cur_counts = current_data[feature].value_counts()

        # å…±é€šã®ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—
        all_categories = set(ref_counts.index) | set(cur_counts.index)

        # åº¦æ•°è¡¨ã®ä½œæˆ
        ref_freq = [ref_counts.get(cat, 0) for cat in all_categories]
        cur_freq = [cur_counts.get(cat, 0) for cat in all_categories]

        # ã‚«ã‚¤äºŒä¹—æ¤œå®š
        contingency_table = [ref_freq, cur_freq]

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            chi2, p_value, dof, expected = chi2_contingency(contingency_table)

        drift_detected = p_value < self.threshold

        return drift_detected, p_value

    def detect_all_features(self, current_data: pd.DataFrame) -> Dict[str, dict]:
        """å…¨ç‰¹å¾´é‡ã®ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º"""
        results = {}

        for feature in self.reference_data.columns:
            try:
                # ãƒ‡ãƒ¼ã‚¿å‹ã«å¿œã˜ã¦æ¤œå®šæ–¹æ³•ã‚’é¸æŠ
                if pd.api.types.is_numeric_dtype(self.reference_data[feature]):
                    drift, p_value = self.detect_drift_numerical(current_data, feature)
                    method = 'KS-test'
                else:
                    drift, p_value = self.detect_drift_categorical(current_data, feature)
                    method = 'Chi-squared'

                results[feature] = {
                    'drift_detected': drift,
                    'p_value': p_value,
                    'method': method
                }
            except Exception as e:
                results[feature] = {
                    'drift_detected': None,
                    'p_value': None,
                    'error': str(e)
                }

        return results


# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ‡ãƒ¢
np.random.seed(42)

# åŸºæº–ãƒ‡ãƒ¼ã‚¿ï¼ˆè¨“ç·´æ™‚ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
reference_data = pd.DataFrame({
    'age': np.random.normal(45, 12, 1000),
    'income': np.random.normal(60000, 15000, 1000),
    'category': np.random.choice(['A', 'B', 'C'], 1000, p=[0.5, 0.3, 0.2])
})

# ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ‰ãƒªãƒ•ãƒˆã‚ã‚Šï¼‰
current_data = pd.DataFrame({
    'age': np.random.normal(40, 12, 1000),  # å¹³å‡ãŒå¤‰åŒ–
    'income': np.random.normal(60000, 15000, 1000),  # å¤‰åŒ–ãªã—
    'category': np.random.choice(['A', 'B', 'C'], 1000, p=[0.3, 0.4, 0.3])  # åˆ†å¸ƒãŒå¤‰åŒ–
})

# ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º
detector = DataDriftDetector(reference_data, threshold=0.05)
drift_results = detector.detect_all_features(current_data)

print("=== ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºçµæœ ===\n")
for feature, result in drift_results.items():
    if 'error' not in result:
        status = "ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º" if result['drift_detected'] else "æ­£å¸¸"
        print(f"{feature}:")
        print(f"  çŠ¶æ…‹: {status}")
        print(f"  på€¤: {result['p_value']:.4f}")
        print(f"  æ¤œå®šæ–¹æ³•: {result['method']}\n")
</code></pre>

<h3>Evidently AIã®æ´»ç”¨</h3>

<p><strong>Evidently AI</strong>ã¯ã€MLãƒ¢ãƒ‡ãƒ«ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã®ãŸã‚ã®å°‚ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚</p>

<pre><code class="language-python"># Evidently AIã‚’ä½¿ç”¨ã—ãŸãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º
# pip install evidently

from evidently.report import Report
from evidently.metric_preset import DataDriftPreset, DataQualityPreset
from evidently.test_suite import TestSuite
from evidently.test_preset import DataDriftTestPreset
import pandas as pd
import numpy as np

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
n_samples = 1000

reference = pd.DataFrame({
    'age': np.random.normal(45, 12, n_samples),
    'income': np.random.normal(60000, 15000, n_samples),
    'credit_score': np.random.normal(700, 50, n_samples),
    'target': np.random.binomial(1, 0.3, n_samples)
})

# ãƒ‰ãƒªãƒ•ãƒˆã‚’å«ã‚€ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿
current = pd.DataFrame({
    'age': np.random.normal(42, 12, n_samples),  # ãƒ‰ãƒªãƒ•ãƒˆ
    'income': np.random.normal(65000, 18000, n_samples),  # ãƒ‰ãƒªãƒ•ãƒˆ
    'credit_score': np.random.normal(700, 50, n_samples),  # æ­£å¸¸
    'target': np.random.binomial(1, 0.35, n_samples)  # ãƒ‰ãƒªãƒ•ãƒˆ
})

# ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
data_drift_report = Report(metrics=[
    DataDriftPreset(),
])

data_drift_report.run(reference_data=reference, current_data=current)

# HTMLãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
data_drift_report.save_html('data_drift_report.html')

print("=== Evidently AI ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆãƒ¬ãƒãƒ¼ãƒˆ ===")
print("ãƒ¬ãƒãƒ¼ãƒˆã¯ data_drift_report.html ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
print("\nä¸»è¦ãªæ¤œå‡ºçµæœ:")
print(f"- å¹´é½¢: ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã®å¯èƒ½æ€§ã‚ã‚Š")
print(f"- åå…¥: ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã®å¯èƒ½æ€§ã‚ã‚Š")
print(f"- ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚¹ã‚³ã‚¢: æ­£å¸¸")

# ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆãƒ†ã‚¹ãƒˆ
data_drift_test = TestSuite(tests=[
    DataDriftTestPreset(),
])

data_drift_test.run(reference_data=reference, current_data=current)

# ãƒ†ã‚¹ãƒˆçµæœã‚’JSONå½¢å¼ã§å–å¾—
test_results = data_drift_test.as_dict()

print(f"\nãƒ†ã‚¹ãƒˆåˆæ ¼: {test_results['summary']['success']}")
print(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {test_results['summary']['total']}")
print(f"å¤±æ•—ãƒ†ã‚¹ãƒˆæ•°: {test_results['summary']['failed']}")
</code></pre>

<h3>ãƒ¢ãƒ‡ãƒ«ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º</h3>

<p><strong>ãƒ¢ãƒ‡ãƒ«ãƒ‰ãƒªãƒ•ãƒˆ</strong>ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬æ€§èƒ½ãŒæ™‚é–“ã¨ã¨ã‚‚ã«ä½ä¸‹ã™ã‚‹ç¾è±¡ã§ã™ã€‚</p>

<pre><code class="language-python">import numpy as np
from sklearn.metrics import accuracy_score, roc_auc_score
from collections import deque
from typing import List, Optional
import datetime

class ModelPerformanceMonitor:
    """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°"""

    def __init__(self, window_size: int = 1000, alert_threshold: float = 0.05):
        """
        Args:
            window_size: ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
            alert_threshold: ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç”Ÿã®é–¾å€¤ï¼ˆæ€§èƒ½ä½ä¸‹ç‡ï¼‰
        """
        self.window_size = window_size
        self.alert_threshold = alert_threshold

        # å±¥æ­´ãƒ‡ãƒ¼ã‚¿
        self.predictions = deque(maxlen=window_size)
        self.actuals = deque(maxlen=window_size)
        self.timestamps = deque(maxlen=window_size)

        # åŸºæº–æ€§èƒ½ï¼ˆè¨“ç·´æ™‚ï¼‰
        self.baseline_accuracy: Optional[float] = None
        self.baseline_auc: Optional[float] = None

    def set_baseline(self, accuracy: float, auc: float):
        """åŸºæº–æ€§èƒ½ã‚’è¨­å®š"""
        self.baseline_accuracy = accuracy
        self.baseline_auc = auc

    def add_prediction(self, prediction: int, actual: int,
                       timestamp: Optional[datetime.datetime] = None):
        """äºˆæ¸¬ã¨å®Ÿç¸¾ã‚’è¿½åŠ """
        self.predictions.append(prediction)
        self.actuals.append(actual)
        self.timestamps.append(timestamp or datetime.datetime.now())

    def get_current_metrics(self) -> dict:
        """ç¾åœ¨ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
        if len(self.predictions) < 100:  # æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
            return {'status': 'insufficient_data'}

        current_accuracy = accuracy_score(
            list(self.actuals),
            list(self.predictions)
        )

        try:
            current_auc = roc_auc_score(
                list(self.actuals),
                list(self.predictions)
            )
        except:
            current_auc = None

        return {
            'accuracy': current_accuracy,
            'auc': current_auc,
            'sample_count': len(self.predictions)
        }

    def detect_performance_drift(self) -> dict:
        """æ€§èƒ½ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡º"""
        current_metrics = self.get_current_metrics()

        if current_metrics.get('status') == 'insufficient_data':
            return {'drift_detected': False, 'reason': 'insufficient_data'}

        if self.baseline_accuracy is None:
            return {'drift_detected': False, 'reason': 'no_baseline'}

        # ç²¾åº¦ã®ä½ä¸‹ç‡ã‚’è¨ˆç®—
        accuracy_drop = self.baseline_accuracy - current_metrics['accuracy']
        accuracy_drop_pct = accuracy_drop / self.baseline_accuracy

        drift_detected = accuracy_drop_pct > self.alert_threshold

        result = {
            'drift_detected': drift_detected,
            'baseline_accuracy': self.baseline_accuracy,
            'current_accuracy': current_metrics['accuracy'],
            'accuracy_drop': accuracy_drop,
            'accuracy_drop_percentage': accuracy_drop_pct * 100,
            'threshold_percentage': self.alert_threshold * 100
        }

        if current_metrics['auc'] and self.baseline_auc:
            auc_drop = self.baseline_auc - current_metrics['auc']
            result['baseline_auc'] = self.baseline_auc
            result['current_auc'] = current_metrics['auc']
            result['auc_drop'] = auc_drop

        return result


# ä½¿ç”¨ä¾‹
monitor = ModelPerformanceMonitor(window_size=1000, alert_threshold=0.05)

# åŸºæº–æ€§èƒ½ã‚’è¨­å®šï¼ˆè¨“ç·´æ™‚ã®æ€§èƒ½ï¼‰
monitor.set_baseline(accuracy=0.92, auc=0.95)

print("=== ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° ===\n")

# åˆæœŸã¯æ­£å¸¸ãªæ€§èƒ½
print("ãƒ•ã‚§ãƒ¼ã‚º1: æ­£å¸¸é‹ç”¨ï¼ˆ1000ä»¶ï¼‰")
for i in range(1000):
    # ç²¾åº¦92%ç¨‹åº¦ã®äºˆæ¸¬ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    actual = np.random.binomial(1, 0.3)
    prediction = actual if np.random.random() < 0.92 else 1 - actual
    monitor.add_prediction(prediction, actual)

result1 = monitor.detect_performance_drift()
print(f"  ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º: {result1['drift_detected']}")
print(f"  ç¾åœ¨ã®ç²¾åº¦: {result1['current_accuracy']:.3f}")
print(f"  åŸºæº–ç²¾åº¦: {result1['baseline_accuracy']:.3f}")

# æ€§èƒ½åŠ£åŒ–ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
print("\nãƒ•ã‚§ãƒ¼ã‚º2: æ€§èƒ½åŠ£åŒ–ï¼ˆ1000ä»¶ï¼‰")
for i in range(1000):
    # ç²¾åº¦85%ã«ä½ä¸‹
    actual = np.random.binomial(1, 0.3)
    prediction = actual if np.random.random() < 0.85 else 1 - actual
    monitor.add_prediction(prediction, actual)

result2 = monitor.detect_performance_drift()
print(f"  ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º: {result2['drift_detected']}")
print(f"  ç¾åœ¨ã®ç²¾åº¦: {result2['current_accuracy']:.3f}")
print(f"  ç²¾åº¦ä½ä¸‹: {result2['accuracy_drop']:.3f} ({result2['accuracy_drop_percentage']:.1f}%)")

if result2['drift_detected']:
    print(f"\n  âš ï¸ ã‚¢ãƒ©ãƒ¼ãƒˆ: ç²¾åº¦ãŒ {result2['threshold_percentage']:.0f}% ä»¥ä¸Šä½ä¸‹ã—ã¾ã—ãŸï¼")
    print(f"  æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: ãƒ¢ãƒ‡ãƒ«ã®å†è¨“ç·´ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
</code></pre>

<hr>

<h2>4.3 A/Bãƒ†ã‚¹ãƒˆã¨ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤</h2>

<h3>ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯åˆ†å‰²</h3>

<pre><code class="language-python">import random
from typing import Callable, Dict, Any
from dataclasses import dataclass
from datetime import datetime

@dataclass
class ModelVariant:
    """ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒªã‚¢ãƒ³ãƒˆ"""
    name: str
    version: str
    traffic_percentage: float
    predict_fn: Callable


class ABTestRouter:
    """A/Bãƒ†ã‚¹ãƒˆç”¨ã®ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ãƒ«ãƒ¼ã‚¿ãƒ¼"""

    def __init__(self):
        self.variants: Dict[str, ModelVariant] = {}
        self.prediction_log = []

    def add_variant(self, variant: ModelVariant):
        """ãƒãƒªã‚¢ãƒ³ãƒˆã‚’è¿½åŠ """
        self.variants[variant.name] = variant

    def validate_traffic_split(self) -> bool:
        """ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯åˆ†å‰²ãŒ100%ã«ãªã‚‹ã“ã¨ã‚’æ¤œè¨¼"""
        total = sum(v.traffic_percentage for v in self.variants.values())
        return abs(total - 100.0) < 0.01

    def route_prediction(self, request_id: str, features: dict) -> dict:
        """
        ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’åˆ†å‰²ã—ã¦äºˆæ¸¬ã‚’ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

        Args:
            request_id: ãƒªã‚¯ã‚¨ã‚¹ãƒˆIDï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼IDãªã©ï¼‰
            features: å…¥åŠ›ç‰¹å¾´é‡

        Returns:
            äºˆæ¸¬çµæœã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        """
        if not self.validate_traffic_split():
            raise ValueError("Traffic split does not sum to 100%")

        # ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã®ä¸€è²«ã—ãŸãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
        # åŒã˜ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯å¸¸ã«åŒã˜ãƒãƒªã‚¢ãƒ³ãƒˆã«æŒ¯ã‚Šåˆ†ã‘ã‚‰ã‚Œã‚‹
        hash_value = hash(request_id) % 100

        cumulative_percentage = 0
        selected_variant = None

        for variant in self.variants.values():
            cumulative_percentage += variant.traffic_percentage
            if hash_value < cumulative_percentage:
                selected_variant = variant
                break

        # äºˆæ¸¬å®Ÿè¡Œ
        prediction = selected_variant.predict_fn(features)

        # ãƒ­ã‚°è¨˜éŒ²
        log_entry = {
            'request_id': request_id,
            'variant': selected_variant.name,
            'version': selected_variant.version,
            'prediction': prediction,
            'timestamp': datetime.now()
        }
        self.prediction_log.append(log_entry)

        return {
            'prediction': prediction,
            'model_variant': selected_variant.name,
            'model_version': selected_variant.version
        }


# ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ãƒŸãƒ¼å®Ÿè£…
def model_v1_predict(features: dict) -> int:
    """ãƒ¢ãƒ‡ãƒ«v1.0ã®äºˆæ¸¬"""
    # ãƒ€ãƒŸãƒ¼: ãƒ©ãƒ³ãƒ€ãƒ ã«äºˆæ¸¬ï¼ˆç²¾åº¦80%ç¨‹åº¦ï¼‰
    return random.choices([0, 1], weights=[0.6, 0.4])[0]

def model_v2_predict(features: dict) -> int:
    """ãƒ¢ãƒ‡ãƒ«v2.0ã®äºˆæ¸¬ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    # ãƒ€ãƒŸãƒ¼: ãƒ©ãƒ³ãƒ€ãƒ ã«äºˆæ¸¬ï¼ˆç²¾åº¦85%ç¨‹åº¦ï¼‰
    return random.choices([0, 1], weights=[0.55, 0.45])[0]


# A/Bãƒ†ã‚¹ãƒˆã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
router = ABTestRouter()

# ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆæ—¢å­˜ãƒ¢ãƒ‡ãƒ«ï¼‰: 90%
router.add_variant(ModelVariant(
    name='control',
    version='v1.0',
    traffic_percentage=90.0,
    predict_fn=model_v1_predict
))

# ãƒ†ã‚¹ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆæ–°ãƒ¢ãƒ‡ãƒ«ï¼‰: 10%
router.add_variant(ModelVariant(
    name='treatment',
    version='v2.0',
    traffic_percentage=10.0,
    predict_fn=model_v2_predict
))

print("=== A/Bãƒ†ã‚¹ãƒˆå®Ÿè¡Œ ===\n")
print("ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯åˆ†å‰²:")
print("  ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ« (v1.0): 90%")
print("  ãƒ†ã‚¹ãƒˆ (v2.0): 10%\n")

# äºˆæ¸¬ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
n_requests = 1000
for i in range(n_requests):
    request_id = f"user_{i % 100}"  # 100ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    features = {'feature1': random.random()}
    result = router.route_prediction(request_id, features)

# çµæœã®é›†è¨ˆ
variant_counts = {}
for log in router.prediction_log:
    variant = log['variant']
    variant_counts[variant] = variant_counts.get(variant, 0) + 1

print(f"ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {n_requests}")
print("\nå®Ÿéš›ã®ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯åˆ†å‰²:")
for variant, count in variant_counts.items():
    percentage = (count / n_requests) * 100
    print(f"  {variant}: {count}ä»¶ ({percentage:.1f}%)")
</code></pre>

<h3>çµ±è¨ˆçš„æ¤œå®š</h3>

<pre><code class="language-python">import numpy as np
from scipy import stats
from typing import List, Tuple

class ABTestAnalyzer:
    """A/Bãƒ†ã‚¹ãƒˆçµæœã®çµ±è¨ˆåˆ†æ"""

    @staticmethod
    def proportion_test(control_successes: int, control_total: int,
                        treatment_successes: int, treatment_total: int,
                        alpha: float = 0.05) -> dict:
        """
        æ¯”ç‡ã®å·®ã®æ¤œå®šï¼ˆäºŒé …åˆ†å¸ƒï¼‰

        Args:
            control_successes: ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã®æˆåŠŸæ•°
            control_total: ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã®ç·æ•°
            treatment_successes: ãƒ†ã‚¹ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã®æˆåŠŸæ•°
            treatment_total: ãƒ†ã‚¹ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã®ç·æ•°
            alpha: æœ‰æ„æ°´æº–

        Returns:
            æ¤œå®šçµæœ
        """
        # æ¯”ç‡ã®è¨ˆç®—
        p_control = control_successes / control_total
        p_treatment = treatment_successes / treatment_total

        # ãƒ—ãƒ¼ãƒ«ã•ã‚ŒãŸæ¯”ç‡
        p_pooled = (control_successes + treatment_successes) / (control_total + treatment_total)

        # æ¨™æº–èª¤å·®
        se = np.sqrt(p_pooled * (1 - p_pooled) * (1/control_total + 1/treatment_total))

        # zçµ±è¨ˆé‡
        z_stat = (p_treatment - p_control) / se

        # på€¤ï¼ˆä¸¡å´æ¤œå®šï¼‰
        p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))

        # ä¿¡é ¼åŒºé–“
        ci_margin = stats.norm.ppf(1 - alpha/2) * se
        ci_lower = (p_treatment - p_control) - ci_margin
        ci_upper = (p_treatment - p_control) + ci_margin

        # çµ±è¨ˆçš„æœ‰æ„æ€§
        is_significant = p_value < alpha

        # åŠ¹æœã‚µã‚¤ã‚ºï¼ˆç›¸å¯¾çš„æ”¹å–„ç‡ï¼‰
        relative_improvement = (p_treatment - p_control) / p_control * 100 if p_control > 0 else 0

        return {
            'control_rate': p_control,
            'treatment_rate': p_treatment,
            'absolute_difference': p_treatment - p_control,
            'relative_improvement_pct': relative_improvement,
            'z_statistic': z_stat,
            'p_value': p_value,
            'is_significant': is_significant,
            'confidence_interval': (ci_lower, ci_upper),
            'confidence_level': (1 - alpha) * 100
        }

    @staticmethod
    def sample_size_calculator(baseline_rate: float,
                               minimum_detectable_effect: float,
                               alpha: float = 0.05,
                               power: float = 0.8) -> int:
        """
        å¿…è¦ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®è¨ˆç®—

        Args:
            baseline_rate: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è»¢æ›ç‡
            minimum_detectable_effect: æ¤œå‡ºã—ãŸã„æœ€å°åŠ¹æœï¼ˆç›¸å¯¾å¤‰åŒ–ç‡ï¼‰
            alpha: æœ‰æ„æ°´æº–
            power: æ¤œå‡ºåŠ›

        Returns:
            ã‚°ãƒ«ãƒ¼ãƒ—ã‚ãŸã‚Šã®å¿…è¦ã‚µãƒ³ãƒ—ãƒ«æ•°
        """
        # Zå€¤
        z_alpha = stats.norm.ppf(1 - alpha/2)
        z_beta = stats.norm.ppf(power)

        # å‡¦ç½®ç¾¤ã®æœŸå¾…è»¢æ›ç‡
        treatment_rate = baseline_rate * (1 + minimum_detectable_effect)

        # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¨ˆç®—
        p_avg = (baseline_rate + treatment_rate) / 2

        n = (2 * p_avg * (1 - p_avg) * (z_alpha + z_beta)**2) / (baseline_rate - treatment_rate)**2

        return int(np.ceil(n))


# ä½¿ç”¨ä¾‹
print("=== A/Bãƒ†ã‚¹ãƒˆçµ±è¨ˆåˆ†æ ===\n")

# ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿
control_total = 10000
treatment_total = 1000

# ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«: è»¢æ›ç‡20%
control_successes = 2000

# ãƒ†ã‚¹ãƒˆ: è»¢æ›ç‡22%ï¼ˆ10%æ”¹å–„ï¼‰
treatment_successes = 220

# çµ±è¨ˆæ¤œå®šã®å®Ÿè¡Œ
analyzer = ABTestAnalyzer()
results = analyzer.proportion_test(
    control_successes, control_total,
    treatment_successes, treatment_total
)

print("æ¤œå®šçµæœ:")
print(f"  ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«è»¢æ›ç‡: {results['control_rate']:.3f}")
print(f"  ãƒ†ã‚¹ãƒˆè»¢æ›ç‡: {results['treatment_rate']:.3f}")
print(f"  çµ¶å¯¾å·®: {results['absolute_difference']:.3f}")
print(f"  ç›¸å¯¾æ”¹å–„: {results['relative_improvement_pct']:.1f}%")
print(f"  på€¤: {results['p_value']:.4f}")
print(f"  çµ±è¨ˆçš„æœ‰æ„: {'ã¯ã„' if results['is_significant'] else 'ã„ã„ãˆ'}")
print(f"  95%ä¿¡é ¼åŒºé–“: [{results['confidence_interval'][0]:.3f}, {results['confidence_interval'][1]:.3f}]")

# å¿…è¦ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®è¨ˆç®—
print("\n\n=== å¿…è¦ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¨ˆç®— ===\n")
sample_size = analyzer.sample_size_calculator(
    baseline_rate=0.20,
    minimum_detectable_effect=0.10,  # 10%ã®æ”¹å–„ã‚’æ¤œå‡º
    alpha=0.05,
    power=0.8
)

print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è»¢æ›ç‡: 20%")
print(f"æ¤œå‡ºã—ãŸã„æ”¹å–„: 10% (20% â†’ 22%)")
print(f"æœ‰æ„æ°´æº– (Î±): 5%")
print(f"æ¤œå‡ºåŠ› (1-Î²): 80%")
print(f"\nå¿…è¦ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆå„ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰: {sample_size:,}ä»¶")
</code></pre>

<h3>ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤æˆ¦ç•¥</h3>

<pre><code class="language-python">from datetime import datetime, timedelta
from typing import Optional
import time

class CanaryDeployment:
    """ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆç®¡ç†"""

    def __init__(self,
                 initial_traffic: float = 5.0,
                 max_traffic: float = 100.0,
                 increment_step: float = 10.0,
                 monitoring_window_minutes: int = 30):
        """
        Args:
            initial_traffic: åˆæœŸãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯æ¯”ç‡ï¼ˆ%ï¼‰
            max_traffic: æœ€å¤§ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯æ¯”ç‡ï¼ˆ%ï¼‰
            increment_step: ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯å¢—åŠ ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆ%ï¼‰
            monitoring_window_minutes: å„ã‚¹ãƒ†ãƒƒãƒ—ã®ç›£è¦–æ™‚é–“ï¼ˆåˆ†ï¼‰
        """
        self.current_traffic = 0.0
        self.initial_traffic = initial_traffic
        self.max_traffic = max_traffic
        self.increment_step = increment_step
        self.monitoring_window = timedelta(minutes=monitoring_window_minutes)

        self.deployment_start_time: Optional[datetime] = None
        self.current_stage_start_time: Optional[datetime] = None
        self.stages_completed = []

    def start_deployment(self):
        """ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ã‚’é–‹å§‹"""
        self.deployment_start_time = datetime.now()
        self.current_traffic = self.initial_traffic
        self.current_stage_start_time = datetime.now()

        print(f"ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤é–‹å§‹: {self.current_traffic}% ã®ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¸")

    def should_proceed_to_next_stage(self, health_metrics: dict) -> Tuple[bool, str]:
        """
        æ¬¡ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã¸é€²ã‚€ã¹ãã‹åˆ¤å®š

        Args:
            health_metrics: ãƒ˜ãƒ«ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                - error_rate: ã‚¨ãƒ©ãƒ¼ç‡
                - latency_p95: 95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
                - success_rate: æˆåŠŸç‡

        Returns:
            (é€²è¡Œå¯èƒ½ã‹, ç†ç”±)
        """
        # ç›£è¦–æ™‚é–“ã‚’çµŒéã—ãŸã‹
        elapsed = datetime.now() - self.current_stage_start_time
        if elapsed < self.monitoring_window:
            return False, f"ç›£è¦–æ™‚é–“æœªé”æˆï¼ˆ{elapsed.total_seconds()/60:.1f}/{self.monitoring_window.total_seconds()/60:.0f}åˆ†ï¼‰"

        # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
        if health_metrics.get('error_rate', 0) > 0.05:
            return False, f"ã‚¨ãƒ©ãƒ¼ç‡ãŒé«˜ã„ï¼ˆ{health_metrics['error_rate']*100:.1f}%ï¼‰"

        if health_metrics.get('latency_p95', 0) > 2000:
            return False, f"ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒé«˜ã„ï¼ˆ{health_metrics['latency_p95']}msï¼‰"

        if health_metrics.get('success_rate', 1.0) < 0.95:
            return False, f"æˆåŠŸç‡ãŒä½ã„ï¼ˆ{health_metrics['success_rate']*100:.1f}%ï¼‰"

        return True, "ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯é€šé"

    def proceed_to_next_stage(self) -> bool:
        """æ¬¡ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã¸é€²ã‚€"""
        if self.current_traffic >= self.max_traffic:
            print("ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†: 100% ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ç§»è¡Œ")
            return False

        # ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’è¨˜éŒ²
        self.stages_completed.append({
            'traffic': self.current_traffic,
            'start_time': self.current_stage_start_time,
            'end_time': datetime.now()
        })

        # ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’å¢—ã‚„ã™
        self.current_traffic = min(
            self.current_traffic + self.increment_step,
            self.max_traffic
        )
        self.current_stage_start_time = datetime.now()

        print(f"\næ¬¡ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã¸: {self.current_traffic}% ã®ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¸")
        return True

    def rollback(self, reason: str):
        """ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ"""
        print(f"\nğŸš¨ ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ: {reason}")
        print(f"ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’æ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¸æˆ»ã—ã¾ã™")
        self.current_traffic = 0.0


# ä½¿ç”¨ä¾‹
print("=== ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå®Ÿè¡Œ ===\n")

canary = CanaryDeployment(
    initial_traffic=5.0,
    max_traffic=100.0,
    increment_step=15.0,
    monitoring_window_minutes=1  # ãƒ‡ãƒ¢ç”¨ã«1åˆ†
)

canary.start_deployment()

# ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ—ãƒ­ã‚»ã‚¹ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
stages = []
while canary.current_traffic < canary.max_traffic:
    print(f"\nç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¸: {canary.current_traffic}% ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯")
    print(f"ç›£è¦–ä¸­... ({canary.monitoring_window.total_seconds()/60:.0f}åˆ†é–“)")

    # ç›£è¦–æ™‚é–“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå®Ÿéš›ã¯ time.sleep(monitoring_window) ï¼‰
    time.sleep(1)

    # ãƒ˜ãƒ«ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    health_metrics = {
        'error_rate': np.random.uniform(0, 0.03),  # 0-3% ã‚¨ãƒ©ãƒ¼ç‡
        'latency_p95': np.random.uniform(100, 500),  # 100-500ms
        'success_rate': np.random.uniform(0.97, 1.0)  # 97-100% æˆåŠŸç‡
    }

    print(f"  ã‚¨ãƒ©ãƒ¼ç‡: {health_metrics['error_rate']*100:.2f}%")
    print(f"  ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·P95: {health_metrics['latency_p95']:.0f}ms")
    print(f"  æˆåŠŸç‡: {health_metrics['success_rate']*100:.2f}%")

    # æ¬¡ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã¸é€²ã‚€ã‹åˆ¤å®š
    can_proceed, reason = canary.should_proceed_to_next_stage(health_metrics)

    if can_proceed:
        print(f"âœ“ {reason}")
        if not canary.proceed_to_next_stage():
            break
    else:
        print(f"â¸ {reason}")
        # æœ¬ç•ªã§ã¯ç›£è¦–ã‚’ç¶™ç¶š

print("\n\n=== ãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº† ===")
print(f"ç·çµŒéæ™‚é–“: {len(canary.stages_completed)}ã‚¹ãƒ†ãƒ¼ã‚¸")
print(f"æœ€çµ‚ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯: {canary.current_traffic}%")
</code></pre>

<hr>

<h2>4.4 ãƒ¢ãƒ‡ãƒ«æ›´æ–°ã¨å†è¨“ç·´</h2>

<h3>ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ vs ãƒãƒƒãƒå†è¨“ç·´</h3>

<table>
<thead>
<tr>
<th>ç‰¹æ€§</th>
<th>ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’</th>
<th>ãƒãƒƒãƒå†è¨“ç·´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ›´æ–°é »åº¦</strong></td>
<td>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã€œæ•°åˆ†</td>
<td>æ—¥æ¬¡ã€œé€±æ¬¡</td>
</tr>
<tr>
<td><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆ</strong></td>
<td>ä½ï¼ˆå¢—åˆ†æ›´æ–°ï¼‰</td>
<td>é«˜ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿å†è¨“ç·´ï¼‰</td>
</tr>
<tr>
<td><strong>é©å¿œé€Ÿåº¦</strong></td>
<td>é€Ÿã„</td>
<td>é…ã„</td>
</tr>
<tr>
<td><strong>å®‰å®šæ€§</strong></td>
<td>ä½ï¼ˆãƒã‚¤ã‚ºã«æ•æ„Ÿï¼‰</td>
<td>é«˜</td>
</tr>
<tr>
<td><strong>å®Ÿè£…é›£æ˜“åº¦</strong></td>
<td>é«˜</td>
<td>ä¸­</td>
</tr>
<tr>
<td><strong>é©ç”¨ä¾‹</strong></td>
<td>æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã€åºƒå‘Š</td>
<td>ä¿¡ç”¨ã‚¹ã‚³ã‚¢ã€ä¸æ­£æ¤œçŸ¥</td>
</tr>
</tbody>
</table>

<h3>ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°</h3>

<pre><code class="language-python">import joblib
import json
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, Optional
import hashlib

class ModelRegistry:
    """ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ãƒ¬ã‚¸ã‚¹ãƒˆãƒª"""

    def __init__(self, registry_path: str = "./model_registry"):
        self.registry_path = Path(registry_path)
        self.registry_path.mkdir(parents=True, exist_ok=True)
        self.metadata_file = self.registry_path / "registry.json"

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r') as f:
                self.metadata = json.load(f)
        else:
            self.metadata = {'models': {}}

    def register_model(self,
                       model: Any,
                       model_name: str,
                       version: str,
                       metrics: Dict[str, float],
                       description: str = "",
                       tags: Dict[str, str] = None) -> str:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ²

        Returns:
            ãƒ¢ãƒ‡ãƒ«ID
        """
        # ãƒ¢ãƒ‡ãƒ«IDã®ç”Ÿæˆ
        model_id = f"{model_name}_{version}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        model_path = self.registry_path / f"{model_id}.pkl"

        # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        joblib.dump(model, model_path)

        # ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒƒã‚·ãƒ¥è¨ˆç®—
        model_hash = self._calculate_file_hash(model_path)

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        model_metadata = {
            'model_id': model_id,
            'model_name': model_name,
            'version': version,
            'file_path': str(model_path),
            'file_hash': model_hash,
            'metrics': metrics,
            'description': description,
            'tags': tags or {},
            'registered_at': datetime.now().isoformat(),
            'status': 'registered'
        }

        self.metadata['models'][model_id] = model_metadata
        self._save_metadata()

        return model_id

    def load_model(self, model_id: str) -> Any:
        """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        if model_id not in self.metadata['models']:
            raise ValueError(f"Model {model_id} not found in registry")

        model_info = self.metadata['models'][model_id]
        model_path = Path(model_info['file_path'])

        # ãƒãƒƒã‚·ãƒ¥æ¤œè¨¼
        current_hash = self._calculate_file_hash(model_path)
        if current_hash != model_info['file_hash']:
            raise ValueError(f"Model file hash mismatch for {model_id}")

        return joblib.load(model_path)

    def promote_to_production(self, model_id: str):
        """ãƒ¢ãƒ‡ãƒ«ã‚’æœ¬ç•ªç’°å¢ƒã¸ãƒ—ãƒ­ãƒ¢ãƒ¼ãƒˆ"""
        if model_id not in self.metadata['models']:
            raise ValueError(f"Model {model_id} not found")

        # æ—¢å­˜ã®æœ¬ç•ªãƒ¢ãƒ‡ãƒ«ã‚’stagingé™æ ¼
        for mid, info in self.metadata['models'].items():
            if info['status'] == 'production':
                info['status'] = 'staging'
                info['demoted_at'] = datetime.now().isoformat()

        # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’æœ¬ç•ªã¸
        self.metadata['models'][model_id]['status'] = 'production'
        self.metadata['models'][model_id]['promoted_at'] = datetime.now().isoformat()

        self._save_metadata()

    def get_production_model(self) -> Optional[str]:
        """ç¾åœ¨ã®æœ¬ç•ªãƒ¢ãƒ‡ãƒ«IDã‚’å–å¾—"""
        for model_id, info in self.metadata['models'].items():
            if info['status'] == 'production':
                return model_id
        return None

    def list_models(self, status: Optional[str] = None) -> list:
        """ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—"""
        models = list(self.metadata['models'].values())
        if status:
            models = [m for m in models if m['status'] == status]
        return sorted(models, key=lambda x: x['registered_at'], reverse=True)

    def _calculate_file_hash(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    def _save_metadata(self):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        with open(self.metadata_file, 'w') as f:
            json.dump(self.metadata, f, indent=2)


# ä½¿ç”¨ä¾‹
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

print("=== ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚° ===\n")

# ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®åˆæœŸåŒ–
registry = ModelRegistry("./model_registry")

# ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
X, y = make_classification(n_samples=1000, n_features=10, random_state=42)
model_v1 = RandomForestClassifier(n_estimators=50, random_state=42)
model_v1.fit(X, y)

# ãƒ¢ãƒ‡ãƒ«ã®ç™»éŒ²
model_id_v1 = registry.register_model(
    model=model_v1,
    model_name="fraud_detector",
    version="1.0.0",
    metrics={
        'accuracy': 0.85,
        'precision': 0.83,
        'recall': 0.87,
        'f1': 0.85
    },
    description="Initial production model",
    tags={'framework': 'sklearn', 'algorithm': 'RandomForest'}
)

print(f"ãƒ¢ãƒ‡ãƒ«ç™»éŒ²: {model_id_v1}")

# æœ¬ç•ªç’°å¢ƒã¸ãƒ—ãƒ­ãƒ¢ãƒ¼ãƒˆ
registry.promote_to_production(model_id_v1)
print(f"æœ¬ç•ªç’°å¢ƒã¸ãƒ—ãƒ­ãƒ¢ãƒ¼ãƒˆ: {model_id_v1}")

# æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨ç™»éŒ²
model_v2 = RandomForestClassifier(n_estimators=100, random_state=42)
model_v2.fit(X, y)

model_id_v2 = registry.register_model(
    model=model_v2,
    model_name="fraud_detector",
    version="2.0.0",
    metrics={
        'accuracy': 0.89,
        'precision': 0.88,
        'recall': 0.90,
        'f1': 0.89
    },
    description="Improved model with more estimators",
    tags={'framework': 'sklearn', 'algorithm': 'RandomForest'}
)

print(f"\næ–°ãƒ¢ãƒ‡ãƒ«ç™»éŒ²: {model_id_v2}")

# ãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º
print("\n=== ç™»éŒ²ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ ===")
for model_info in registry.list_models():
    print(f"\nãƒ¢ãƒ‡ãƒ«: {model_info['model_name']} v{model_info['version']}")
    print(f"  ID: {model_info['model_id']}")
    print(f"  çŠ¶æ…‹: {model_info['status']}")
    print(f"  ç²¾åº¦: {model_info['metrics']['accuracy']:.3f}")
    print(f"  ç™»éŒ²æ—¥æ™‚: {model_info['registered_at']}")

# æœ¬ç•ªãƒ¢ãƒ‡ãƒ«ã®å–å¾—
prod_model_id = registry.get_production_model()
print(f"\nç¾åœ¨ã®æœ¬ç•ªãƒ¢ãƒ‡ãƒ«: {prod_model_id}")
</code></pre>

<h3>è‡ªå‹•å†è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h3>

<pre><code class="language-python">from datetime import datetime, timedelta
from typing import Optional
import schedule
import time

class AutoRetrainingPipeline:
    """è‡ªå‹•å†è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(self,
                 model_registry: ModelRegistry,
                 performance_monitor: ModelPerformanceMonitor,
                 retrain_threshold: float = 0.05):
        """
        Args:
            model_registry: ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒª
            performance_monitor: æ€§èƒ½ãƒ¢ãƒ‹ã‚¿ãƒ¼
            retrain_threshold: å†è¨“ç·´ãƒˆãƒªã‚¬ãƒ¼ã®é–¾å€¤ï¼ˆæ€§èƒ½ä½ä¸‹ç‡ï¼‰
        """
        self.registry = model_registry
        self.monitor = performance_monitor
        self.retrain_threshold = retrain_threshold
        self.last_retrain_time: Optional[datetime] = None

    def check_retrain_needed(self) -> bool:
        """å†è¨“ç·´ãŒå¿…è¦ã‹åˆ¤å®š"""
        drift_result = self.monitor.detect_performance_drift()

        if drift_result.get('drift_detected'):
            print(f"âš ï¸ æ€§èƒ½ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º: {drift_result['accuracy_drop_percentage']:.1f}% ä½ä¸‹")
            return True

        return False

    def retrain_model(self, training_data_path: str) -> str:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’å†è¨“ç·´

        Returns:
            æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ID
        """
        print(f"\n{'='*50}")
        print(f"è‡ªå‹•å†è¨“ç·´é–‹å§‹: {datetime.now().isoformat()}")
        print(f"{'='*50}\n")

        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
        print("1. è¨“ç·´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
        X, y = make_classification(n_samples=2000, n_features=10, random_state=int(time.time()))

        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        print("2. ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
        new_model = RandomForestClassifier(n_estimators=100, random_state=42)
        new_model.fit(X, y)

        # è©•ä¾¡
        print("3. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡...")
        from sklearn.model_selection import cross_val_score
        scores = cross_val_score(new_model, X, y, cv=5)
        accuracy = scores.mean()

        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã®ç”Ÿæˆ
        current_prod_id = self.registry.get_production_model()
        if current_prod_id:
            current_version = self.registry.metadata['models'][current_prod_id]['version']
            major, minor, patch = map(int, current_version.split('.'))
            new_version = f"{major}.{minor}.{patch + 1}"
        else:
            new_version = "1.0.0"

        # ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã¸ç™»éŒ²
        print("4. ãƒ¢ãƒ‡ãƒ«ç™»éŒ²...")
        new_model_id = self.registry.register_model(
            model=new_model,
            model_name="fraud_detector",
            version=new_version,
            metrics={
                'accuracy': accuracy,
                'cv_scores_mean': accuracy,
                'cv_scores_std': scores.std()
            },
            description=f"Auto-retrained model (trigger: performance drift)",
            tags={'retrain_type': 'automatic', 'trigger': 'performance_drift'}
        )

        print(f"âœ“ æ–°ãƒ¢ãƒ‡ãƒ«ç™»éŒ²å®Œäº†: {new_model_id}")
        print(f"  ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {new_version}")
        print(f"  ç²¾åº¦: {accuracy:.3f}")

        # æ€§èƒ½ãŒå‘ä¸Šã—ã¦ã„ã‚Œã°ãƒ—ãƒ­ãƒ¢ãƒ¼ãƒˆ
        if current_prod_id:
            current_accuracy = self.registry.metadata['models'][current_prod_id]['metrics']['accuracy']
            if accuracy > current_accuracy:
                print(f"\n5. æœ¬ç•ªç’°å¢ƒã¸ãƒ—ãƒ­ãƒ¢ãƒ¼ãƒˆï¼ˆç²¾åº¦å‘ä¸Š: {current_accuracy:.3f} â†’ {accuracy:.3f}ï¼‰")
                self.registry.promote_to_production(new_model_id)
            else:
                print(f"\n5. ãƒ—ãƒ­ãƒ¢ãƒ¼ãƒˆè¦‹é€ã‚Šï¼ˆç²¾åº¦å‘ä¸Šãªã—: {accuracy:.3f} vs {current_accuracy:.3f}ï¼‰")
        else:
            print(f"\n5. æœ¬ç•ªç’°å¢ƒã¸ãƒ—ãƒ­ãƒ¢ãƒ¼ãƒˆï¼ˆåˆå›ãƒ¢ãƒ‡ãƒ«ï¼‰")
            self.registry.promote_to_production(new_model_id)

        self.last_retrain_time = datetime.now()

        print(f"\n{'='*50}")
        print(f"å†è¨“ç·´å®Œäº†")
        print(f"{'='*50}\n")

        return new_model_id

    def scheduled_check(self, training_data_path: str):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã•ã‚ŒãŸãƒã‚§ãƒƒã‚¯"""
        print(f"\n[{datetime.now().isoformat()}] å®šæœŸãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ")

        if self.check_retrain_needed():
            print("â†’ å†è¨“ç·´ãŒå¿…è¦ã¨åˆ¤å®š")
            self.retrain_model(training_data_path)
        else:
            print("â†’ å†è¨“ç·´ä¸è¦ï¼ˆæ€§èƒ½æ­£å¸¸ï¼‰")


# ä½¿ç”¨ä¾‹
print("=== è‡ªå‹•å†è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ===\n")

# æ—¢å­˜ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ä½¿ç”¨
monitor = ModelPerformanceMonitor(window_size=1000, alert_threshold=0.05)
monitor.set_baseline(accuracy=0.85, auc=0.90)

registry = ModelRegistry("./model_registry")

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
pipeline = AutoRetrainingPipeline(
    model_registry=registry,
    performance_monitor=monitor,
    retrain_threshold=0.05
)

# æ€§èƒ½åŠ£åŒ–ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
print("æ€§èƒ½åŠ£åŒ–ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ...")
for i in range(1000):
    actual = np.random.binomial(1, 0.3)
    # ç²¾åº¦78%ã«ä½ä¸‹
    prediction = actual if np.random.random() < 0.78 else 1 - actual
    monitor.add_prediction(prediction, actual)

# å†è¨“ç·´ãƒã‚§ãƒƒã‚¯
pipeline.scheduled_check(training_data_path="data/training.csv")

# ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ï¼ˆå®Ÿé‹ç”¨ä¾‹ï¼‰
print("\n\n=== ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°è¨­å®š ===")
print("ä»¥ä¸‹ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã§è‡ªå‹•ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ:")
print("  - æ¯æ—¥ 02:00 ã«å®Ÿè¡Œ")
print("  - æ€§èƒ½ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºæ™‚ã«è‡ªå‹•å†è¨“ç·´")

# schedule.every().day.at("02:00").do(
#     pipeline.scheduled_check,
#     training_data_path="data/training.csv"
# )
#
# while True:
#     schedule.run_pending()
#     time.sleep(60)
</code></pre>

<hr>

<h2>4.5 å®Ÿè·µ: ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰é‹ç”¨</h2>

<h3>ç·åˆãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ </h3>

<pre><code class="language-python">from dataclasses import dataclass
from typing import List, Dict, Any
import json

@dataclass
class HealthStatus:
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""
    is_healthy: bool
    components: Dict[str, bool]
    alerts: List[str]
    metrics: Dict[str, float]
    timestamp: datetime


class ComprehensiveMonitoringSystem:
    """åŒ…æ‹¬çš„ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.model_monitor = ModelPerformanceMonitor(window_size=1000)
        self.drift_detector = None  # DataDriftDetectorã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        self.sla_thresholds = {
            'latency_p95_ms': 500,
            'latency_p99_ms': 1000,
            'error_rate': 0.01,
            'availability': 0.999
        }

    def check_system_health(self) -> HealthStatus:
        """ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        components = {}
        alerts = []
        metrics = {}

        # 1. ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒã‚§ãƒƒã‚¯
        performance_metrics = self.model_monitor.get_current_metrics()
        if performance_metrics.get('status') != 'insufficient_data':
            model_healthy = performance_metrics.get('accuracy', 0) > 0.85
            components['model_performance'] = model_healthy
            metrics['model_accuracy'] = performance_metrics.get('accuracy', 0)

            if not model_healthy:
                alerts.append(f"ãƒ¢ãƒ‡ãƒ«ç²¾åº¦ä½ä¸‹: {metrics['model_accuracy']:.3f}")

        # 2. ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒã‚§ãƒƒã‚¯
        current_latency_p95 = np.random.uniform(200, 600)  # ãƒ€ãƒŸãƒ¼
        latency_healthy = current_latency_p95 < self.sla_thresholds['latency_p95_ms']
        components['latency'] = latency_healthy
        metrics['latency_p95_ms'] = current_latency_p95

        if not latency_healthy:
            alerts.append(f"ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·SLAé•å: {current_latency_p95:.0f}ms")

        # 3. ã‚¨ãƒ©ãƒ¼ç‡ãƒã‚§ãƒƒã‚¯
        current_error_rate = np.random.uniform(0, 0.02)  # ãƒ€ãƒŸãƒ¼
        error_rate_healthy = current_error_rate < self.sla_thresholds['error_rate']
        components['error_rate'] = error_rate_healthy
        metrics['error_rate'] = current_error_rate

        if not error_rate_healthy:
            alerts.append(f"ã‚¨ãƒ©ãƒ¼ç‡SLAé•å: {current_error_rate*100:.2f}%")

        # 4. å¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯
        current_availability = np.random.uniform(0.995, 1.0)  # ãƒ€ãƒŸãƒ¼
        availability_healthy = current_availability >= self.sla_thresholds['availability']
        components['availability'] = availability_healthy
        metrics['availability'] = current_availability

        if not availability_healthy:
            alerts.append(f"å¯ç”¨æ€§SLAé•å: {current_availability*100:.3f}%")

        # ç·åˆåˆ¤å®š
        is_healthy = all(components.values())

        return HealthStatus(
            is_healthy=is_healthy,
            components=components,
            alerts=alerts,
            metrics=metrics,
            timestamp=datetime.now()
        )

    def generate_health_report(self) -> str:
        """ãƒ˜ãƒ«ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        status = self.check_system_health()

        report = f"""
{'='*60}
ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ
{'='*60}
æ™‚åˆ»: {status.timestamp.isoformat()}
ç·åˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {'âœ“ æ­£å¸¸' if status.is_healthy else 'âœ— ç•°å¸¸'}

ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆçŠ¶æ…‹:
"""
        for component, healthy in status.components.items():
            icon = 'âœ“' if healthy else 'âœ—'
            report += f"  {icon} {component}: {'æ­£å¸¸' if healthy else 'ç•°å¸¸'}\n"

        report += f"\nãƒ¡ãƒˆãƒªã‚¯ã‚¹:\n"
        for metric, value in status.metrics.items():
            if 'rate' in metric or 'availability' in metric:
                report += f"  {metric}: {value*100:.2f}%\n"
            else:
                report += f"  {metric}: {value:.2f}\n"

        if status.alerts:
            report += f"\nã‚¢ãƒ©ãƒ¼ãƒˆ ({len(status.alerts)}ä»¶):\n"
            for alert in status.alerts:
                report += f"  âš ï¸ {alert}\n"
        else:
            report += f"\nã‚¢ãƒ©ãƒ¼ãƒˆ: ãªã—\n"

        report += f"{'='*60}\n"

        return report


# ä½¿ç”¨ä¾‹
print("=== ç·åˆãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ  ===\n")

monitoring = ComprehensiveMonitoringSystem()

# ãƒ˜ãƒ«ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
report = monitoring.generate_health_report()
print(report)
</code></pre>

<h3>æœ¬ç•ªé‹ç”¨ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ</h3>

<table>
<thead>
<tr>
<th>ã‚«ãƒ†ã‚´ãƒª</th>
<th>ãƒã‚§ãƒƒã‚¯é …ç›®</th>
<th>å„ªå…ˆåº¦</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="4"><strong>ãƒ‡ãƒ—ãƒ­ã‚¤å‰</strong></td>
<td>ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ãŒãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã‚‹</td>
<td>ğŸ”´ å¿…é ˆ</td>
</tr>
<tr>
<td>A/Bãƒ†ã‚¹ãƒˆã§çµ±è¨ˆçš„æœ‰æ„æ€§ã‚’ç¢ºèª</td>
<td>ğŸ”´ å¿…é ˆ</td>
</tr>
<tr>
<td>ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆã§æ€§èƒ½è¦ä»¶ã‚’æº€ãŸã™</td>
<td>ğŸ”´ å¿…é ˆ</td>
</tr>
<tr>
<td>ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰‹é †ã‚’æ–‡æ›¸åŒ–</td>
<td>ğŸ”´ å¿…é ˆ</td>
</tr>
<tr>
<td rowspan="5"><strong>ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°</strong></td>
<td>æ§‹é€ åŒ–ãƒ­ã‚°ãŒæ­£ã—ãå‡ºåŠ›ã•ã‚Œã‚‹</td>
<td>ğŸ”´ å¿…é ˆ</td>
</tr>
<tr>
<td>Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒåé›†ã•ã‚Œã‚‹</td>
<td>ğŸ”´ å¿…é ˆ</td>
</tr>
<tr>
<td>Grafanaãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãŒç¨¼åƒ</td>
<td>ğŸŸ¡ æ¨å¥¨</td>
</tr>
<tr>
<td>ã‚¢ãƒ©ãƒ¼ãƒˆãŒæ­£ã—ãç™ºç«ã™ã‚‹</td>
<td>ğŸ”´ å¿…é ˆ</td>
</tr>
<tr>
<td>ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºãŒå‹•ä½œ</td>
<td>ğŸŸ¡ æ¨å¥¨</td>
</tr>
<tr>
<td rowspan="4"><strong>SLAå®šç¾©</strong></td>
<td>ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·P95 < 500ms</td>
<td>ğŸ”´ å¿…é ˆ</td>
</tr>
<tr>
<td>ã‚¨ãƒ©ãƒ¼ç‡ < 1%</td>
<td>ğŸ”´ å¿…é ˆ</td>
</tr>
<tr>
<td>å¯ç”¨æ€§ > 99.9%</td>
<td>ğŸ”´ å¿…é ˆ</td>
</tr>
<tr>
<td>ãƒ¢ãƒ‡ãƒ«ç²¾åº¦ > åŸºæº–å€¤ã®95%</td>
<td>ğŸŸ¡ æ¨å¥¨</td>
</tr>
<tr>
<td rowspan="3"><strong>ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œ</strong></td>
<td>ã‚ªãƒ³ã‚³ãƒ¼ãƒ«ä½“åˆ¶ãŒæ•´ã£ã¦ã„ã‚‹</td>
<td>ğŸ”´ å¿…é ˆ</td>
</tr>
<tr>
<td>ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œæ‰‹é †ãŒæ–‡æ›¸åŒ–</td>
<td>ğŸ”´ å¿…é ˆ</td>
</tr>
<tr>
<td>ãƒã‚¹ãƒˆãƒ¢ãƒ¼ãƒ†ãƒ ãƒ—ãƒ­ã‚»ã‚¹ãŒã‚ã‚‹</td>
<td>ğŸŸ¡ æ¨å¥¨</td>
</tr>
<tr>
<td rowspan="3"><strong>ãƒ‡ãƒ¼ã‚¿ç®¡ç†</strong></td>
<td>äºˆæ¸¬ãƒ­ã‚°ãŒä¿å­˜ã•ã‚Œã‚‹</td>
<td>ğŸ”´ å¿…é ˆ</td>
</tr>
<tr>
<td>å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ãŒåé›†ã•ã‚Œã‚‹</td>
<td>ğŸ”´ å¿…é ˆ</td>
</tr>
<tr>
<td>ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒå®šæœŸå®Ÿè¡Œ</td>
<td>ğŸŸ¡ æ¨å¥¨</td>
</tr>
</tbody>
</table>

<hr>

<h2>4.6 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>ãƒ­ã‚°ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°</strong></p>
<ul>
<li>æ§‹é€ åŒ–JSONãƒ­ã‚°ã§æ©Ÿæ¢°å¯èª­ãªè¨˜éŒ²</li>
<li>Prometheus + Grafanaã§æ™‚ç³»åˆ—ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–</li>
<li>ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§ãƒ“ã‚¸ãƒã‚¹æŒ‡æ¨™ã‚’è¿½è·¡</li>
<li>ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«ã§ç•°å¸¸ã‚’æ—©æœŸæ¤œå‡º</li>
</ul></li>

<li><p><strong>ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¿½è·¡</strong></p>
<ul>
<li>ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã®çµ±è¨ˆçš„æ¤œå‡ºï¼ˆKSæ¤œå®šã€ã‚«ã‚¤äºŒä¹—æ¤œå®šï¼‰</li>
<li>Evidently AIã§ã®åŒ…æ‹¬çš„ãƒ‰ãƒªãƒ•ãƒˆåˆ†æ</li>
<li>ãƒ¢ãƒ‡ãƒ«ãƒ‰ãƒªãƒ•ãƒˆã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–</li>
</ul></li>

<li><p><strong>A/Bãƒ†ã‚¹ãƒˆã¨ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤</strong></p>
<ul>
<li>ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯åˆ†å‰²ã«ã‚ˆã‚‹å®‰å…¨ãªãƒ†ã‚¹ãƒˆ</li>
<li>çµ±è¨ˆçš„æ¤œå®šã§ã®åŠ¹æœæ¤œè¨¼</li>
<li>æ®µéšçš„ãªã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤æˆ¦ç•¥</li>
</ul></li>

<li><p><strong>ãƒ¢ãƒ‡ãƒ«æ›´æ–°ã¨å†è¨“ç·´</strong></p>
<ul>
<li>ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã§ã®å±¥æ­´ç®¡ç†</li>
<li>è‡ªå‹•å†è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰</li>
<li>æ€§èƒ½ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã«ã‚ˆã‚‹å†è¨“ç·´ãƒˆãƒªã‚¬ãƒ¼</li>
</ul></li>

<li><p><strong>ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰é‹ç”¨</strong></p>
<ul>
<li>ç·åˆãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®è¨­è¨ˆ</li>
<li>SLAå®šç¾©ã¨ç¶™ç¶šçš„ãªç›£è¦–</li>
<li>æœ¬ç•ªé‹ç”¨ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã®æ´»ç”¨</li>
</ul></li>
</ol>

<h3>é‹ç”¨ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>

<table>
<thead>
<tr>
<th>åŸå‰‡</th>
<th>èª¬æ˜</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å¯è¦³æ¸¬æ€§å„ªå…ˆ</strong></td>
<td>å…¨ã¦ã®é‡è¦ãªå‹•ä½œã‚’ãƒ­ã‚°ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§è¿½è·¡</td>
</tr>
<tr>
<td><strong>æ®µéšçš„ãƒ‡ãƒ—ãƒ­ã‚¤</strong></td>
<td>ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ã§å½±éŸ¿ç¯„å›²ã‚’é™å®š</td>
</tr>
<tr>
<td><strong>è‡ªå‹•åŒ–ç¬¬ä¸€</strong></td>
<td>å†è¨“ç·´ã€ãƒ‡ãƒ—ãƒ­ã‚¤ã€ã‚¢ãƒ©ãƒ¼ãƒˆã‚’è‡ªå‹•åŒ–</td>
</tr>
<tr>
<td><strong>è¿…é€Ÿãªãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯</strong></td>
<td>å•é¡Œç™ºç”Ÿæ™‚ã¯å³åº§ã«å®‰å…¨ãªçŠ¶æ…‹ã¸å¾©å¸°</td>
</tr>
<tr>
<td><strong>ç¶™ç¶šçš„æ”¹å–„</strong></td>
<td>ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã‹ã‚‰å­¦ã³ã€ã‚·ã‚¹ãƒ†ãƒ ã‚’æ”¹å–„</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>

<p>ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®å­¦ç¿’ã‚’å®Œäº†ã—ã¾ã—ãŸã€‚ã•ã‚‰ã«æ·±ãå­¦ã¶ã«ã¯ï¼š</p>
<ul>
<li>Kubernetesä¸Šã§ã®MLãƒ¢ãƒ‡ãƒ«ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</li>
<li>MLOpsãƒ„ãƒ¼ãƒ«ï¼ˆKubeflowã€MLflowã€Vertex AIï¼‰</li>
<li>ãƒ•ã‚§ãƒ‡ãƒ¬ãƒ¼ãƒ†ãƒƒãƒ‰ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°</li>
<li>ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤</li>
<li>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã®æœ€é©åŒ–</li>
</ul>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Huyen, C. (2022). <em>Designing Machine Learning Systems</em>. O'Reilly Media.</li>
<li>Ameisen, E. (2020). <em>Building Machine Learning Powered Applications</em>. O'Reilly Media.</li>
<li>Kleppmann, M. (2017). <em>Designing Data-Intensive Applications</em>. O'Reilly Media.</li>
<li>Google. (2023). <em>Machine Learning Engineering for Production (MLOps)</em>. Coursera.</li>
<li>Neptune.ai. (2023). <em>MLOps: Model Monitoring Best Practices</em>.</li>
</ol>

<div class="navigation">
    <a href="chapter3-containerization.html" class="nav-button">â† å‰ã®ç« : ã‚³ãƒ³ãƒ†ãƒŠåŒ–ã¨ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã¸ â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-23</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
