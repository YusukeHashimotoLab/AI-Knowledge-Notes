<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬1ç« ï¼šãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®åŸºç¤ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/model-deployment-introduction/chapter1-deployment-basics.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/model-deployment-introduction/index.html">Model Deployment</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 1</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬1ç« ï¼šãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®åŸºç¤</h1>
            <p class="subtitle">æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’æœ¬ç•ªç’°å¢ƒã§å‹•ã‹ã™æŠ€è¡“</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´šã€œä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 9å€‹</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«å…¨ä½“ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ–¹å¼ï¼ˆãƒãƒƒãƒã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã€ã‚¨ãƒƒã‚¸ï¼‰ã‚’ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹</li>
<li>âœ… Flaskã¨FastAPIã§æ¨è«–APIã‚’æ§‹ç¯‰ã§ãã‚‹</li>
<li>âœ… ãƒ¢ãƒ‡ãƒ«ã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³å½¢å¼ã‚’é©åˆ‡ã«é¸æŠã§ãã‚‹</li>
<li>âœ… å®Ÿè·µçš„ãªç”»åƒåˆ†é¡APIã‚’å®Ÿè£…ã§ãã‚‹</li>
</ul>

<hr>

<h2>1.1 ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®åŸºç¤</h2>

<h3>æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«</h3>

<p>æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¯ã€é–‹ç™ºã‹ã‚‰æœ¬ç•ªé‹ç”¨ã¾ã§ä»¥ä¸‹ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’çµŒã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[å•é¡Œå®šç¾©] --> B[ãƒ‡ãƒ¼ã‚¿åé›†]
    B --> C[ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°]
    C --> D[ãƒ¢ãƒ‡ãƒ«é–‹ç™º]
    D --> E[ãƒ¢ãƒ‡ãƒ«è©•ä¾¡]
    E --> F[ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ]
    F --> G[ç›£è¦–ãƒ»é‹ç”¨]
    G --> H[å†å­¦ç¿’]
    H --> D

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#e8f5e9
    style F fill:#fce4ec
    style G fill:#ede7f6
    style H fill:#e0f2f1
</div>

<blockquote>
<p><strong>ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ</strong>ã¯ã€è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒåˆ©ç”¨ã§ãã‚‹ç’°å¢ƒã«é…ç½®ã—ã€äºˆæ¸¬ã‚’æä¾›ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã§ã™ã€‚</p>
</blockquote>

<h3>ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ–¹å¼ã®æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>æ–¹å¼</th>
<th>ç‰¹å¾´</th>
<th>ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·</th>
<th>é©ç”¨ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒãƒƒãƒæ¨è«–</strong></td>
<td>å®šæœŸçš„ã«å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬å‡¦ç†</td>
<td>æ•°åˆ†ã€œæ•°æ™‚é–“</td>
<td>ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ</td>
</tr>
<tr>
<td><strong>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–</strong></td>
<td>ãƒªã‚¯ã‚¨ã‚¹ãƒˆã”ã¨ã«å³åº§ã«å¿œç­”</td>
<td>æ•°åmsã€œæ•°ç§’</td>
<td>Web APIã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ</td>
</tr>
<tr>
<td><strong>ã‚¨ãƒƒã‚¸æ¨è«–</strong></td>
<td>ãƒ‡ãƒã‚¤ã‚¹ä¸Šã§ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ</td>
<td>æ•°ms</td>
<td>ã‚¹ãƒãƒ›ã‚¢ãƒ—ãƒªã€IoTãƒ‡ãƒã‚¤ã‚¹</td>
</tr>
</tbody>
</table>

<h3>REST APIã®åŸºæœ¬</h3>

<p>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã§ã¯ã€RESTful APIãŒæœ€ã‚‚ä¸€èˆ¬çš„ã§ã™ã€‚</p>

<pre><code class="language-python"># åŸºæœ¬çš„ãªAPIãƒªã‚¯ã‚¨ã‚¹ãƒˆã®æµã‚Œ
"""
1. ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ â†’ ã‚µãƒ¼ãƒãƒ¼: POST /predict
   {
       "features": [5.1, 3.5, 1.4, 0.2]
   }

2. ã‚µãƒ¼ãƒãƒ¼å‡¦ç†:
   - ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
   - å‰å‡¦ç†
   - ãƒ¢ãƒ‡ãƒ«æ¨è«–
   - çµæœãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

3. ã‚µãƒ¼ãƒãƒ¼ â†’ ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ: 200 OK
   {
       "prediction": "setosa",
       "confidence": 0.98
   }
"""
</code></pre>

<hr>

<h2>1.2 Flaskã«ã‚ˆã‚‹æ¨è«–API</h2>

<h3>FlaskåŸºæœ¬ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—</h3>

<p>Flaskã¯è»½é‡ã§å­¦ç¿’ã‚³ã‚¹ãƒˆãŒä½ã„Pythonã‚¦ã‚§ãƒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚</p>

<pre><code class="language-python"># app.py - åŸºæœ¬çš„ãªFlaskã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/health', methods=['GET'])
def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return jsonify({
        'status': 'healthy',
        'version': '1.0.0'
    })

@app.route('/predict', methods=['POST'])
def predict():
    """äºˆæ¸¬ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    data = request.get_json()

    # ç°¡æ˜“çš„ãªå¿œç­”ï¼ˆå¾Œã§ãƒ¢ãƒ‡ãƒ«æ¨è«–ã«ç½®ãæ›ãˆï¼‰
    return jsonify({
        'prediction': 'sample',
        'input_received': data
    })

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)
</code></pre>

<p><strong>èµ·å‹•æ–¹æ³•</strong>ï¼š</p>
<pre><code class="language-bash">python app.py
# â†’ http://localhost:5000 ã§ã‚µãƒ¼ãƒãƒ¼èµ·å‹•

# åˆ¥ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ãƒ†ã‚¹ãƒˆ
curl http://localhost:5000/health
</code></pre>

<h3>scikit-learnãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤</h3>

<pre><code class="language-python"># train_model.py - ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
import joblib
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.2, random_state=42
)

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜
joblib.dump(model, 'iris_model.pkl')
print(f"ãƒ¢ãƒ‡ãƒ«ç²¾åº¦: {model.score(X_test, y_test):.3f}")
print("ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: iris_model.pkl")
</code></pre>

<h3>POSTãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã®äºˆæ¸¬</h3>

<pre><code class="language-python"># flask_app.py - å®Œå…¨ãªæ¨è«–API
from flask import Flask, request, jsonify
import joblib
import numpy as np

app = Flask(__name__)

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆèµ·å‹•æ™‚ã«1å›ã ã‘ï¼‰
model = joblib.load('iris_model.pkl')
class_names = ['setosa', 'versicolor', 'virginica']

@app.route('/predict', methods=['POST'])
def predict():
    """
    Irisåˆ†é¡ã®äºˆæ¸¬

    ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¾‹:
    {
        "features": [5.1, 3.5, 1.4, 0.2]
    }
    """
    try:
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å–å¾—
        data = request.get_json()
        features = np.array(data['features']).reshape(1, -1)

        # äºˆæ¸¬
        prediction = model.predict(features)[0]
        probabilities = model.predict_proba(features)[0]

        # çµæœãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        return jsonify({
            'prediction': class_names[prediction],
            'class_id': int(prediction),
            'probabilities': {
                class_names[i]: float(prob)
                for i, prob in enumerate(probabilities)
            }
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 400

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)
</code></pre>

<p><strong>ãƒ†ã‚¹ãƒˆ</strong>ï¼š</p>
<pre><code class="language-bash">curl -X POST http://localhost:5000/predict \
  -H "Content-Type: application/json" \
  -d '{"features": [5.1, 3.5, 1.4, 0.2]}'

# å‡ºåŠ›:
# {
#   "prediction": "setosa",
#   "class_id": 0,
#   "probabilities": {
#     "setosa": 0.98,
#     "versicolor": 0.02,
#     "virginica": 0.0
#   }
# }
</code></pre>

<h3>ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°</h3>

<pre><code class="language-python"># error_handling.py - å …ç‰¢ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
from flask import Flask, request, jsonify
import joblib
import numpy as np

app = Flask(__name__)
model = joblib.load('iris_model.pkl')
class_names = ['setosa', 'versicolor', 'virginica']

def validate_input(data):
    """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
    if 'features' not in data:
        raise ValueError("'features' ã‚­ãƒ¼ãŒå¿…è¦ã§ã™")

    features = data['features']
    if not isinstance(features, list):
        raise ValueError("features ã¯ãƒªã‚¹ãƒˆã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")

    if len(features) != 4:
        raise ValueError(f"features ã¯4è¦ç´ å¿…è¦ã§ã™ï¼ˆå—ä¿¡: {len(features)}ï¼‰")

    return np.array(features).reshape(1, -1)

@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.get_json()

        # å…¥åŠ›æ¤œè¨¼
        features = validate_input(data)

        # äºˆæ¸¬
        prediction = model.predict(features)[0]
        probabilities = model.predict_proba(features)[0]

        return jsonify({
            'prediction': class_names[prediction],
            'class_id': int(prediction),
            'probabilities': {
                class_names[i]: float(prob)
                for i, prob in enumerate(probabilities)
            }
        }), 200

    except ValueError as e:
        return jsonify({'error': f'å…¥åŠ›ã‚¨ãƒ©ãƒ¼: {str(e)}'}), 400

    except Exception as e:
        return jsonify({'error': f'ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

@app.errorhandler(404)
def not_found(error):
    return jsonify({'error': 'ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}), 404

if __name__ == '__main__':
    app.run(debug=False, host='0.0.0.0', port=5000)
</code></pre>

<hr>

<h2>1.3 FastAPIã«ã‚ˆã‚‹é«˜é€Ÿæ¨è«–</h2>

<h3>FastAPIã®åˆ©ç‚¹</h3>

<p>FastAPIã¯ã€Flaskã‚ˆã‚Šé«˜é€Ÿã§ã€è‡ªå‹•ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆã‚„å‹æ¤œè¨¼ã‚’æä¾›ã—ã¾ã™ã€‚</p>

<table>
<thead>
<tr>
<th>ç‰¹å¾´</th>
<th>Flask</th>
<th>FastAPI</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>é€Ÿåº¦</strong></td>
<td>ä¸­ç¨‹åº¦</td>
<td>é«˜é€Ÿï¼ˆéåŒæœŸå¯¾å¿œï¼‰</td>
</tr>
<tr>
<td><strong>å‹æ¤œè¨¼</strong></td>
<td>æ‰‹å‹•</td>
<td>Pydanticã§è‡ªå‹•</td>
</tr>
<tr>
<td><strong>ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ</strong></td>
<td>æ‰‹å‹•</td>
<td>è‡ªå‹•ç”Ÿæˆï¼ˆSwagger UIï¼‰</td>
</tr>
<tr>
<td><strong>å­¦ç¿’ã‚³ã‚¹ãƒˆ</strong></td>
<td>ä½ã„</td>
<td>ã‚„ã‚„é«˜ã„</td>
</tr>
</tbody>
</table>

<h3>Pydanticãƒ¢ãƒ‡ãƒ«å®šç¾©</h3>

<pre><code class="language-python"># models.py - Pydanticãƒ¢ãƒ‡ãƒ«å®šç¾©
from pydantic import BaseModel, Field, validator
from typing import List

class IrisFeatures(BaseModel):
    """Irisç‰¹å¾´é‡ã®å…¥åŠ›ã‚¹ã‚­ãƒ¼ãƒ"""
    features: List[float] = Field(
        ...,
        description="4ã¤ã®ç‰¹å¾´é‡ [sepal_length, sepal_width, petal_length, petal_width]",
        min_items=4,
        max_items=4
    )

    @validator('features')
    def check_positive(cls, v):
        """ç‰¹å¾´é‡ãŒæ­£ã®å€¤ã§ã‚ã‚‹ã“ã¨ã‚’æ¤œè¨¼"""
        if any(x < 0 for x in v):
            raise ValueError('ã™ã¹ã¦ã®ç‰¹å¾´é‡ã¯æ­£ã®å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™')
        return v

class PredictionResponse(BaseModel):
    """äºˆæ¸¬çµæœã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¹ã‚­ãƒ¼ãƒ"""
    prediction: str = Field(..., description="äºˆæ¸¬ã‚¯ãƒ©ã‚¹å")
    class_id: int = Field(..., description="ã‚¯ãƒ©ã‚¹ID (0-2)")
    probabilities: dict = Field(..., description="å„ã‚¯ãƒ©ã‚¹ã®ç¢ºç‡")

# ä½¿ç”¨ä¾‹
sample_input = IrisFeatures(features=[5.1, 3.5, 1.4, 0.2])
print(sample_input.json())
# â†’ {"features": [5.1, 3.5, 1.4, 0.2]}
</code></pre>

<h3>PyTorchãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤</h3>

<pre><code class="language-python"># fastapi_pytorch.py - FastAPI + PyTorchæ¨è«–API
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
import torch.nn as nn
from typing import List
import uvicorn

# Pydanticãƒ¢ãƒ‡ãƒ«
class InputData(BaseModel):
    features: List[float]

class PredictionOutput(BaseModel):
    prediction: int
    confidence: float

# PyTorchãƒ¢ãƒ‡ãƒ«å®šç¾©
class SimpleNN(nn.Module):
    def __init__(self, input_size=4, hidden_size=16, num_classes=3):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
app = FastAPI(
    title="Iris Classification API",
    description="PyTorchãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹Irisåˆ†é¡API",
    version="1.0.0"
)

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆèµ·å‹•æ™‚ï¼‰
model = SimpleNN()
model.load_state_dict(torch.load('iris_pytorch_model.pth'))
model.eval()

@app.post("/predict", response_model=PredictionOutput)
async def predict(data: InputData):
    """
    Irisåˆ†é¡ã®äºˆæ¸¬

    - **features**: 4ã¤ã®ç‰¹å¾´é‡ã®ãƒªã‚¹ãƒˆ
    """
    try:
        # ãƒ†ãƒ³ã‚½ãƒ«å¤‰æ›
        features_tensor = torch.tensor([data.features], dtype=torch.float32)

        # æ¨è«–
        with torch.no_grad():
            outputs = model(features_tensor)
            probabilities = torch.softmax(outputs, dim=1)
            prediction = torch.argmax(probabilities, dim=1).item()
            confidence = probabilities[0][prediction].item()

        return PredictionOutput(
            prediction=prediction,
            confidence=confidence
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
</code></pre>

<h3>Swagger UIè‡ªå‹•ç”Ÿæˆ</h3>

<p>FastAPIã‚’èµ·å‹•ã™ã‚‹ã¨ã€è‡ªå‹•çš„ã«å¯¾è©±çš„ãªAPIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚</p>

<pre><code class="language-bash"># èµ·å‹•
python fastapi_pytorch.py

# ãƒ–ãƒ©ã‚¦ã‚¶ã§ä»¥ä¸‹ã«ã‚¢ã‚¯ã‚»ã‚¹:
# - Swagger UI: http://localhost:8000/docs
# - ReDoc: http://localhost:8000/redoc
# - OpenAPIã‚¹ã‚­ãƒ¼ãƒ: http://localhost:8000/openapi.json
</code></pre>

<blockquote>
<p><strong>åˆ©ç‚¹</strong>: ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰ç›´æ¥APIã‚’ãƒ†ã‚¹ãƒˆã§ãã€é–‹ç™ºåŠ¹ç‡ãŒå¤§å¹…ã«å‘ä¸Šã—ã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>1.4 ãƒ¢ãƒ‡ãƒ«ã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³</h2>

<h3>pickle / joblib</h3>

<p>Pythonã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä¿å­˜ã™ã‚‹æ¨™æº–çš„ãªæ–¹æ³•ã§ã™ã€‚</p>

<pre><code class="language-python"># serialization_comparison.py
import pickle
import joblib
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
iris = load_iris()
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(iris.data, iris.target)

# pickleä¿å­˜
with open('model_pickle.pkl', 'wb') as f:
    pickle.dump(model, f)

# joblibä¿å­˜ï¼ˆã‚ˆã‚ŠåŠ¹ç‡çš„ï¼‰
joblib.dump(model, 'model_joblib.pkl')

# èª­ã¿è¾¼ã¿
model_pickle = pickle.load(open('model_pickle.pkl', 'rb'))
model_joblib = joblib.load('model_joblib.pkl')

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæ¯”è¼ƒ
import os
print(f"pickle: {os.path.getsize('model_pickle.pkl')} bytes")
print(f"joblib: {os.path.getsize('model_joblib.pkl')} bytes")
# â†’ joblibã®æ–¹ãŒåŠ¹ç‡çš„ï¼ˆç‰¹ã«å¤§ããªnumpyé…åˆ—ã‚’å«ã‚€å ´åˆï¼‰
</code></pre>

<h3>ONNXå½¢å¼</h3>

<p>ONNXï¼ˆOpen Neural Network Exchangeï¼‰ã¯ã€ç•°ãªã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é–“ã§äº’æ›æ€§ã®ã‚ã‚‹å½¢å¼ã§ã™ã€‚</p>

<pre><code class="language-python"># onnx_export.py - PyTorchãƒ¢ãƒ‡ãƒ«ã‚’ONNXã«å¤‰æ›
import torch
import torch.nn as nn
import onnxruntime as ort
import numpy as np

# PyTorchãƒ¢ãƒ‡ãƒ«å®šç¾©
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(4, 16)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(16, 3)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# ãƒ¢ãƒ‡ãƒ«ã®ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
model = SimpleNN()
model.eval()

dummy_input = torch.randn(1, 4)
torch.onnx.export(
    model,
    dummy_input,
    "iris_model.onnx",
    input_names=['features'],
    output_names=['logits'],
    dynamic_axes={
        'features': {0: 'batch_size'},
        'logits': {0: 'batch_size'}
    }
)

# ONNX Runtimeã§æ¨è«–
ort_session = ort.InferenceSession("iris_model.onnx")

def predict_onnx(features):
    ort_inputs = {'features': features.astype(np.float32)}
    ort_outputs = ort_session.run(None, ort_inputs)
    return ort_outputs[0]

# ãƒ†ã‚¹ãƒˆ
test_input = np.array([[5.1, 3.5, 1.4, 0.2]])
output = predict_onnx(test_input)
print(f"ONNXæ¨è«–çµæœ: {output}")
</code></pre>

<h3>TorchScript</h3>

<p>PyTorchãƒ¢ãƒ‡ãƒ«ã‚’æœ¬ç•ªç’°å¢ƒç”¨ã«æœ€é©åŒ–ã™ã‚‹æ–¹å¼ã§ã™ã€‚</p>

<pre><code class="language-python"># torchscript_export.py
import torch
import torch.nn as nn

class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(4, 16)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(16, 3)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

model = SimpleNN()
model.eval()

# TorchScriptå¤‰æ›ï¼ˆãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ï¼‰
example_input = torch.randn(1, 4)
traced_model = torch.jit.trace(model, example_input)

# ä¿å­˜
traced_model.save("iris_torchscript.pt")

# èª­ã¿è¾¼ã¿ã¨æ¨è«–
loaded_model = torch.jit.load("iris_torchscript.pt")
test_input = torch.tensor([[5.1, 3.5, 1.4, 0.2]])

with torch.no_grad():
    output = loaded_model(test_input)
    print(f"TorchScriptæ¨è«–çµæœ: {output}")
</code></pre>

<h3>å½¢å¼ã®æ¯”è¼ƒã¨é¸æŠ</h3>

<table>
<thead>
<tr>
<th>å½¢å¼</th>
<th>å¯¾è±¡</th>
<th>é•·æ‰€</th>
<th>çŸ­æ‰€</th>
<th>æ¨å¥¨ç”¨é€”</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>pickle/joblib</strong></td>
<td>scikit-learn</td>
<td>ç°¡å˜ã€è»½é‡</td>
<td>Pythonä¾å­˜ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯</td>
<td>é–‹ç™ºã€ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°</td>
</tr>
<tr>
<td><strong>ONNX</strong></td>
<td>å…¨èˆ¬</td>
<td>ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯éä¾å­˜ã€é«˜é€Ÿ</td>
<td>å¤‰æ›ã®æ‰‹é–“</td>
<td>æœ¬ç•ªç’°å¢ƒã€ãƒãƒ«ãƒè¨€èª</td>
</tr>
<tr>
<td><strong>TorchScript</strong></td>
<td>PyTorch</td>
<td>æœ€é©åŒ–ã€C++å®Ÿè¡Œå¯èƒ½</td>
<td>PyTorchå°‚ç”¨</td>
<td>æœ¬ç•ªç’°å¢ƒï¼ˆPyTorchï¼‰</td>
</tr>
</tbody>
</table>

<hr>

<h2>1.5 å®Ÿè·µ: ç”»åƒåˆ†é¡APIã®æ§‹ç¯‰</h2>

<h3>ResNetæ¨è«–API</h3>

<pre><code class="language-python"># image_classification_api.py - ResNetã«ã‚ˆã‚‹ç”»åƒåˆ†é¡API
from fastapi import FastAPI, File, UploadFile, HTTPException
from pydantic import BaseModel
import torch
import torchvision.transforms as transforms
from torchvision.models import resnet50, ResNet50_Weights
from PIL import Image
import io
import time

app = FastAPI(title="Image Classification API")

# ResNet50ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆèµ·å‹•æ™‚ï¼‰
print("ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
weights = ResNet50_Weights.DEFAULT
model = resnet50(weights=weights)
model.eval()

# å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

# ImageNetã‚¯ãƒ©ã‚¹å
categories = weights.meta["categories"]

class PredictionResult(BaseModel):
    top_class: str
    confidence: float
    top_5: dict
    inference_time_ms: float

@app.post("/predict", response_model=PredictionResult)
async def predict_image(file: UploadFile = File(...)):
    """
    ç”»åƒåˆ†é¡

    - **file**: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJPEG, PNGç­‰ï¼‰
    """
    try:
        # ç”»åƒèª­ã¿è¾¼ã¿
        image_bytes = await file.read()
        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')

        # å‰å‡¦ç†
        input_tensor = preprocess(image)
        input_batch = input_tensor.unsqueeze(0)

        # æ¨è«–
        start_time = time.time()
        with torch.no_grad():
            output = model(input_batch)
        inference_time = (time.time() - start_time) * 1000

        # ç¢ºç‡è¨ˆç®—
        probabilities = torch.nn.functional.softmax(output[0], dim=0)

        # Top-5äºˆæ¸¬
        top5_prob, top5_idx = torch.topk(probabilities, 5)
        top5_results = {
            categories[idx]: float(prob)
            for idx, prob in zip(top5_idx, top5_prob)
        }

        return PredictionResult(
            top_class=categories[top5_idx[0]],
            confidence=float(top5_prob[0]),
            top_5=top5_results,
            inference_time_ms=inference_time
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
</code></pre>

<h3>Base64ç”»åƒå‡¦ç†</h3>

<pre><code class="language-python"># base64_image_api.py - Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ç”»åƒã®å‡¦ç†
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import base64
import io
from PIL import Image
import torch
import torchvision.transforms as transforms
from torchvision.models import resnet50, ResNet50_Weights

app = FastAPI()

# ãƒ¢ãƒ‡ãƒ«è¨­å®š
weights = ResNet50_Weights.DEFAULT
model = resnet50(weights=weights)
model.eval()
categories = weights.meta["categories"]

preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

class Base64ImageInput(BaseModel):
    image: str  # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ–‡å­—åˆ—

class PredictionOutput(BaseModel):
    prediction: str
    confidence: float

@app.post("/predict", response_model=PredictionOutput)
async def predict_base64(data: Base64ImageInput):
    """
    Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ç”»åƒã®åˆ†é¡

    ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¾‹:
    {
        "image": "data:image/jpeg;base64,/9j/4AAQSkZJRg..."
    }
    """
    try:
        # Base64ãƒ‡ã‚³ãƒ¼ãƒ‰
        if ',' in data.image:
            image_data = data.image.split(',')[1]  # "data:image/jpeg;base64," ã‚’é™¤å»
        else:
            image_data = data.image

        image_bytes = base64.b64decode(image_data)
        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')

        # æ¨è«–
        input_tensor = preprocess(image).unsqueeze(0)
        with torch.no_grad():
            output = model(input_tensor)

        probabilities = torch.nn.functional.softmax(output[0], dim=0)
        top_prob, top_idx = torch.max(probabilities, dim=0)

        return PredictionOutput(
            prediction=categories[top_idx],
            confidence=float(top_prob)
        )

    except Exception as e:
        raise HTTPException(status_code=400, detail=f"ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
</code></pre>

<h3>ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š</h3>

<pre><code class="language-python"># benchmark.py - APIãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
import requests
import time
import numpy as np
from PIL import Image
import io

API_URL = "http://localhost:8000/predict"

def create_test_image():
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ€ãƒŸãƒ¼ç”»åƒä½œæˆ"""
    img = Image.new('RGB', (224, 224), color='red')
    buf = io.BytesIO()
    img.save(buf, format='JPEG')
    buf.seek(0)
    return buf

def benchmark_api(num_requests=100):
    """APIæ€§èƒ½æ¸¬å®š"""
    latencies = []

    print(f"{num_requests}å›ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ä¸­...")

    for i in range(num_requests):
        image_file = create_test_image()
        files = {'file': ('test.jpg', image_file, 'image/jpeg')}

        start = time.time()
        response = requests.post(API_URL, files=files)
        latency = (time.time() - start) * 1000

        if response.status_code == 200:
            latencies.append(latency)
        else:
            print(f"ã‚¨ãƒ©ãƒ¼: {response.status_code}")

        if (i + 1) % 10 == 0:
            print(f"  é€²æ—: {i + 1}/{num_requests}")

    # çµ±è¨ˆ
    latencies = np.array(latencies)
    print("\n=== ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ ===")
    print(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {len(latencies)}")
    print(f"å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: {latencies.mean():.2f} ms")
    print(f"ä¸­å¤®å€¤: {np.median(latencies):.2f} ms")
    print(f"æœ€å°: {latencies.min():.2f} ms")
    print(f"æœ€å¤§: {latencies.max():.2f} ms")
    print(f"æ¨™æº–åå·®: {latencies.std():.2f} ms")
    print(f"P95: {np.percentile(latencies, 95):.2f} ms")
    print(f"P99: {np.percentile(latencies, 99):.2f} ms")

if __name__ == "__main__":
    benchmark_api(num_requests=100)
</code></pre>

<p><strong>å®Ÿè¡Œä¾‹</strong>ï¼š</p>
<pre><code>=== ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ ===
ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: 100
å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: 125.34 ms
ä¸­å¤®å€¤: 120.12 ms
æœ€å°: 98.45 ms
æœ€å¤§: 210.67 ms
æ¨™æº–åå·®: 18.92 ms
P95: 155.23 ms
P99: 180.45 ms
</code></pre>

<hr>

<h2>1.6 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®åŸºç¤</strong></p>
<ul>
<li>æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«å…¨ä½“ã®ç†è§£</li>
<li>ãƒãƒƒãƒã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã€ã‚¨ãƒƒã‚¸æ¨è«–ã®ä½¿ã„åˆ†ã‘</li>
</ul></li>

<li><p><strong>Flaskã«ã‚ˆã‚‹æ¨è«–API</strong></p>
<ul>
<li>è»½é‡ã§å­¦ç¿’ã‚³ã‚¹ãƒˆãŒä½ã„</li>
<li>ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®å®Ÿè£…</li>
</ul></li>

<li><p><strong>FastAPIã«ã‚ˆã‚‹é«˜é€Ÿæ¨è«–</strong></p>
<ul>
<li>Pydanticã«ã‚ˆã‚‹å‹æ¤œè¨¼</li>
<li>è‡ªå‹•ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆï¼ˆSwagger UIï¼‰</li>
<li>éåŒæœŸå‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–</li>
</ul></li>

<li><p><strong>ãƒ¢ãƒ‡ãƒ«ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³</strong></p>
<ul>
<li>pickle/joblib: é–‹ç™ºãƒ»ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°</li>
<li>ONNX: ãƒãƒ«ãƒãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å¯¾å¿œ</li>
<li>TorchScript: PyTorchæœ€é©åŒ–</li>
</ul></li>

<li><p><strong>å®Ÿè·µçš„ãªç”»åƒåˆ†é¡API</strong></p>
<ul>
<li>ResNetã«ã‚ˆã‚‹ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–</li>
<li>Base64ç”»åƒå‡¦ç†</li>
<li>ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã¨æœ€é©åŒ–</li>
</ul></li>
</ol>

<h3>æ¬¡ã®ç« ã¸</h3>

<p>ç¬¬2ç« ã§ã¯ã€<strong>Dockerã‚³ãƒ³ãƒ†ãƒŠåŒ–ã¨ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ</strong>ã‚’å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>Dockerã®åŸºç¤ã¨ã‚³ãƒ³ãƒ†ãƒŠåŒ–</li>
<li>ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ã‚¸ãƒ“ãƒ«ãƒ‰</li>
<li>Docker Composeã«ã‚ˆã‚‹ç’°å¢ƒæ§‹ç¯‰</li>
<li>ã‚¯ãƒ©ã‚¦ãƒ‰ã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆAWSã€GCPï¼‰</li>
</ul>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>GÃ©ron, A. (2019). <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em> (2nd ed.). O'Reilly Media.</li>
<li>Huyen, C. (2022). <em>Designing Machine Learning Systems</em>. O'Reilly Media.</li>
<li>FastAPIå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: <a href="https://fastapi.tiangolo.com/">https://fastapi.tiangolo.com/</a></li>
<li>ONNXå…¬å¼ã‚µã‚¤ãƒˆ: <a href="https://onnx.ai/">https://onnx.ai/</a></li>
</ol>

<div class="navigation">
    <a href="index.html" class="nav-button">â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a>
    <a href="chapter2-docker-deployment.html" class="nav-button">æ¬¡ã®ç« : Dockerã‚³ãƒ³ãƒ†ãƒŠåŒ– â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-23</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
