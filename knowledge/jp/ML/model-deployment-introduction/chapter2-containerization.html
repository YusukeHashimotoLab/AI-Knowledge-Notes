<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第2章：コンテナ化技術 - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>第2章：コンテナ化技術</h1>
            <p class="subtitle">Dockerによる機械学習モデルの可搬性と再現性</p>
            <div class="meta">
                <span class="meta-item">📖 読了時間: 25-30分</span>
                <span class="meta-item">📊 難易度: 初級-中級</span>
                <span class="meta-item">💻 コード例: 8個</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>学習目標</h2>
<p>この章を読むことで、以下を習得できます：</p>
<ul>
<li>✅ Dockerコンテナの基礎概念と仮想マシンとの違いを理解する</li>
<li>✅ 機械学習モデル用のDockerfileを作成できる</li>
<li>✅ MLモデルを効率的にコンテナ化できる</li>
<li>✅ Docker Composeで複数サービスを管理できる</li>
<li>✅ GPU対応のMLコンテナを構築できる</li>
</ul>

<hr>

<h2>2.1 Dockerの基礎</h2>

<h3>コンテナとは</h3>
<p><strong>コンテナ（Container）</strong>は、アプリケーションとその依存関係を独立した環境にパッケージ化する技術です。</p>

<blockquote>
<p>「Build once, Run anywhere」- 一度ビルドすれば、どこでも同じように実行できる</p>
</blockquote>

<h3>Docker vs 仮想マシン</h3>

<table>
<thead>
<tr>
<th>特徴</th>
<th>Docker コンテナ</th>
<th>仮想マシン (VM)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>起動時間</strong></td>
<td>秒単位</td>
<td>分単位</td>
</tr>
<tr>
<td><strong>リソース</strong></td>
<td>軽量（MB単位）</td>
<td>重い（GB単位）</td>
</tr>
<tr>
<td><strong>分離レベル</strong></td>
<td>プロセスレベル</td>
<td>完全な OS 分離</td>
</tr>
<tr>
<td><strong>性能</strong></td>
<td>ネイティブに近い</td>
<td>オーバーヘッドあり</td>
</tr>
<tr>
<td><strong>可搬性</strong></td>
<td>高い</td>
<td>中程度</td>
</tr>
</tbody>
</table>

<div class="mermaid">
graph TD
    subgraph "仮想マシン"
        A1[アプリ1] --> B1[ゲストOS1]
        A2[アプリ2] --> B2[ゲストOS2]
        B1 --> C[ハイパーバイザー]
        B2 --> C
        C --> D[ホストOS]
        D --> E[物理サーバー]
    end

    subgraph "Docker コンテナ"
        F1[アプリ1] --> G[Docker Engine]
        F2[アプリ2] --> G
        G --> H[ホストOS]
        H --> I[物理サーバー]
    end

    style A1 fill:#e3f2fd
    style A2 fill:#e3f2fd
    style F1 fill:#c8e6c9
    style F2 fill:#c8e6c9
</div>

<h3>Docker基本コマンド</h3>

<pre><code class="language-bash"># Dockerバージョン確認
docker --version

# イメージ一覧表示
docker images

# コンテナ一覧表示（実行中）
docker ps

# コンテナ一覧表示（全て）
docker ps -a

# イメージのダウンロード
docker pull python:3.9-slim

# コンテナの実行
docker run -it python:3.9-slim bash

# コンテナの停止
docker stop &lt;container_id&gt;

# コンテナの削除
docker rm &lt;container_id&gt;

# イメージの削除
docker rmi &lt;image_id&gt;

# システム全体のクリーンアップ
docker system prune -a
</code></pre>

<h3>イメージとコンテナの関係</h3>

<p><strong>イメージ（Image）</strong>：アプリケーションの設計図（読み取り専用）</p>
<p><strong>コンテナ（Container）</strong>：イメージから作成された実行可能なインスタンス</p>

<div class="mermaid">
graph LR
    A[Dockerfile] -->|docker build| B[Docker Image]
    B -->|docker run| C[Container 1]
    B -->|docker run| D[Container 2]
    B -->|docker run| E[Container 3]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#e8f5e9
    style D fill:#e8f5e9
    style E fill:#e8f5e9
</div>

<blockquote>
<p><strong>重要</strong>: 1つのイメージから複数のコンテナを起動できます。各コンテナは独立した環境です。</p>
</blockquote>

<hr>

<h2>2.2 Dockerfileの作成</h2>

<h3>ベースイメージ選択</h3>

<p>機械学習モデル用の代表的なベースイメージ：</p>

<table>
<thead>
<tr>
<th>イメージ</th>
<th>サイズ</th>
<th>用途</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>python:3.9-slim</code></td>
<td>約120MB</td>
<td>軽量なPython環境</td>
</tr>
<tr>
<td><code>python:3.9</code></td>
<td>約900MB</td>
<td>フル機能のPython環境</td>
</tr>
<tr>
<td><code>nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04</code></td>
<td>約2GB</td>
<td>GPU推論用</td>
</tr>
<tr>
<td><code>nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04</code></td>
<td>約4GB</td>
<td>GPU開発・学習用</td>
</tr>
</tbody>
</table>

<h3>基本的なDockerfile構造</h3>

<pre><code class="language-dockerfile"># ベースイメージの指定
FROM python:3.9-slim

# 作業ディレクトリの設定
WORKDIR /app

# システムパッケージの更新とインストール
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Pythonパッケージのインストール
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# アプリケーションコードのコピー
COPY . .

# ポート公開
EXPOSE 8000

# 起動コマンド
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
</code></pre>

<h3>マルチステージビルド</h3>

<p>イメージサイズを削減し、セキュリティを向上させる技術：</p>

<pre><code class="language-dockerfile"># ステージ1: ビルド環境
FROM python:3.9 as builder

WORKDIR /build

# 依存関係のインストール
COPY requirements.txt .
RUN pip install --user --no-cache-dir -r requirements.txt

# ステージ2: 実行環境（軽量）
FROM python:3.9-slim

WORKDIR /app

# ビルドステージから必要なファイルのみコピー
COPY --from=builder /root/.local /root/.local
COPY . .

# PATHの設定
ENV PATH=/root/.local/bin:$PATH

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
</code></pre>

<blockquote>
<p><strong>効果</strong>: マルチステージビルドにより、イメージサイズを50-70%削減できることがあります。</p>
</blockquote>

<h3>最適化テクニック</h3>

<h4>レイヤーキャッシュの活用</h4>

<pre><code class="language-dockerfile"># ❌ 非効率: コードが変更されるたびに依存関係を再インストール
FROM python:3.9-slim
WORKDIR /app
COPY . .
RUN pip install -r requirements.txt

# ✅ 効率的: 依存関係が変更されない限りキャッシュを利用
FROM python:3.9-slim
WORKDIR /app

# 先に依存関係をインストール（変更頻度が低い）
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# 後からコードをコピー（変更頻度が高い）
COPY . .
</code></pre>

<h4>不要なファイルの除外</h4>

<p>.dockerignoreファイルの例：</p>

<pre><code class="language-plaintext"># .dockerignore
__pycache__
*.pyc
*.pyo
*.pyd
.Python
*.so
*.egg
*.egg-info
dist
build
.git
.gitignore
.env
.venv
venv/
data/
notebooks/
tests/
*.md
Dockerfile
docker-compose.yml
</code></pre>

<hr>

<h2>2.3 MLモデルのコンテナ化</h2>

<h3>FastAPI + PyTorchのDockerfile</h3>

<pre><code class="language-dockerfile"># マルチステージビルド
FROM python:3.9 as builder

WORKDIR /build

# 依存関係ファイルのコピー
COPY requirements.txt .

# 依存関係のインストール
RUN pip install --user --no-cache-dir \
    torch==2.0.0 \
    torchvision==0.15.0 \
    fastapi==0.104.0 \
    uvicorn[standard]==0.24.0 \
    pydantic==2.5.0 \
    pillow==10.1.0

# 実行環境
FROM python:3.9-slim

WORKDIR /app

# ビルドステージから依存関係をコピー
COPY --from=builder /root/.local /root/.local

# アプリケーションコードとモデルをコピー
COPY app/ ./app/
COPY models/ ./models/

# 環境変数の設定
ENV PATH=/root/.local/bin:$PATH \
    PYTHONUNBUFFERED=1 \
    MODEL_PATH=/app/models/model.pth

# 非rootユーザーの作成（セキュリティ向上）
RUN useradd -m -u 1000 appuser && \
    chown -R appuser:appuser /app

USER appuser

# ヘルスチェック
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health')"

EXPOSE 8000

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
</code></pre>

<h3>requirements.txtの例</h3>

<pre><code class="language-plaintext"># requirements.txt
torch==2.0.0
torchvision==0.15.0
fastapi==0.104.0
uvicorn[standard]==0.24.0
pydantic==2.5.0
pillow==10.1.0
numpy==1.24.3
python-multipart==0.0.6
</code></pre>

<h3>イメージビルドと実行</h3>

<pre><code class="language-bash"># イメージのビルド
docker build -t ml-api:v1.0 .

# ビルドログの詳細表示
docker build -t ml-api:v1.0 --progress=plain .

# キャッシュを使わずにビルド
docker build -t ml-api:v1.0 --no-cache .

# コンテナの実行
docker run -d \
    --name ml-api \
    -p 8000:8000 \
    -v $(pwd)/models:/app/models \
    ml-api:v1.0

# ログの確認
docker logs ml-api

# リアルタイムログ表示
docker logs -f ml-api

# コンテナ内でコマンド実行
docker exec -it ml-api bash

# コンテナの停止と削除
docker stop ml-api
docker rm ml-api
</code></pre>

<h3>ポートマッピング</h3>

<table>
<thead>
<tr>
<th>オプション</th>
<th>説明</th>
<th>例</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-p 8000:8000</code></td>
<td>ホスト:コンテナ</td>
<td>ホストの8000番をコンテナの8000番に</td>
</tr>
<tr>
<td><code>-p 8080:8000</code></td>
<td>異なるポート</td>
<td>ホストの8080番をコンテナの8000番に</td>
</tr>
<tr>
<td><code>-p 127.0.0.1:8000:8000</code></td>
<td>ローカルのみ</td>
<td>ローカルホストからのみアクセス可能</td>
</tr>
</tbody>
</table>

<hr>

<h2>2.4 Docker Composeによるオーケストレーション</h2>

<h3>docker-compose.yml構成</h3>

<p>複数のサービスを統合管理するための設定ファイル：</p>

<pre><code class="language-yaml"># docker-compose.yml
version: '3.8'

services:
  # FastAPI アプリケーション
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ml-api
    ports:
      - "8000:8000"
    environment:
      - MODEL_PATH=/app/models/model.pth
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    volumes:
      - ./models:/app/models:ro
      - ./logs:/app/logs
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - ml-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Redis キャッシュ
  redis:
    image: redis:7-alpine
    container_name: ml-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    restart: unless-stopped
    networks:
      - ml-network
    command: redis-server --appendonly yes

networks:
  ml-network:
    driver: bridge

volumes:
  redis-data:
</code></pre>

<h3>複数サービスの統合例</h3>

<pre><code class="language-yaml"># docker-compose.yml (拡張版)
version: '3.8'

services:
  # MLモデル推論API
  ml-api:
    build: ./api
    ports:
      - "8000:8000"
    environment:
      - REDIS_HOST=redis
      - DB_HOST=postgres
    volumes:
      - ./models:/app/models:ro
    depends_on:
      - redis
      - postgres
    networks:
      - ml-network

  # キャッシュ層
  redis:
    image: redis:7-alpine
    volumes:
      - redis-data:/data
    networks:
      - ml-network

  # データベース
  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_USER=mluser
      - POSTGRES_PASSWORD=mlpass
      - POSTGRES_DB=mldb
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - ml-network

  # モニタリング
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    networks:
      - ml-network

networks:
  ml-network:
    driver: bridge

volumes:
  redis-data:
  postgres-data:
  prometheus-data:
</code></pre>

<h3>ボリュームマウント</h3>

<table>
<thead>
<tr>
<th>タイプ</th>
<th>構文</th>
<th>用途</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>バインドマウント</strong></td>
<td><code>./host/path:/container/path</code></td>
<td>開発時のコード同期</td>
</tr>
<tr>
<td><strong>名前付きボリューム</strong></td>
<td><code>volume-name:/container/path</code></td>
<td>永続的なデータ保存</td>
</tr>
<tr>
<td><strong>読み取り専用</strong></td>
<td><code>./path:/path:ro</code></td>
<td>モデルファイルなど</td>
</tr>
</tbody>
</table>

<h3>環境変数管理</h3>

<p>.envファイルの例：</p>

<pre><code class="language-plaintext"># .env
MODEL_PATH=/app/models/resnet50.pth
REDIS_HOST=redis
REDIS_PORT=6379
LOG_LEVEL=INFO
MAX_WORKERS=4
</code></pre>

<p>docker-compose.ymlでの使用：</p>

<pre><code class="language-yaml">services:
  api:
    env_file:
      - .env
    # または個別に指定
    environment:
      - MODEL_PATH=${MODEL_PATH}
      - REDIS_HOST=${REDIS_HOST}
</code></pre>

<h3>Docker Compose コマンド</h3>

<pre><code class="language-bash"># サービスの起動（バックグラウンド）
docker-compose up -d

# サービスの起動（ログ表示）
docker-compose up

# サービスのビルドと起動
docker-compose up -d --build

# 特定のサービスのみ起動
docker-compose up -d api redis

# サービスの停止
docker-compose stop

# サービスの停止と削除
docker-compose down

# ボリュームも含めて削除
docker-compose down -v

# ログの確認
docker-compose logs -f

# 特定のサービスのログ
docker-compose logs -f api

# サービスの状態確認
docker-compose ps

# サービスの再起動
docker-compose restart api
</code></pre>

<hr>

<h2>2.5 実践: GPU対応MLコンテナ</h2>

<h3>NVIDIA Dockerセットアップ</h3>

<p>前提条件：</p>
<ul>
<li>NVIDIA GPU搭載マシン</li>
<li>NVIDIAドライバーのインストール</li>
<li>NVIDIA Container Toolkitのインストール</li>
</ul>

<pre><code class="language-bash"># NVIDIA Container Toolkit のインストール
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
    sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

# Dockerの再起動
sudo systemctl restart docker

# GPU の確認
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
</code></pre>

<h3>CUDAイメージ使用のDockerfile</h3>

<pre><code class="language-dockerfile"># GPU推論用 Dockerfile
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# Python のインストール
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# PyTorch GPU版のインストール
COPY requirements-gpu.txt .
RUN pip3 install --no-cache-dir -r requirements-gpu.txt

# アプリケーションコードとモデル
COPY app/ ./app/
COPY models/ ./models/

ENV PYTHONUNBUFFERED=1 \
    CUDA_VISIBLE_DEVICES=0

EXPOSE 8000

CMD ["python3", "-m", "uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
</code></pre>

<h3>requirements-gpu.txt</h3>

<pre><code class="language-plaintext"># requirements-gpu.txt
torch==2.0.0+cu118
torchvision==0.15.0+cu118
--extra-index-url https://download.pytorch.org/whl/cu118
fastapi==0.104.0
uvicorn[standard]==0.24.0
pydantic==2.5.0
pillow==10.1.0
numpy==1.24.3
</code></pre>

<h3>GPU推論の実装</h3>

<p>app/main.pyの例：</p>

<pre><code class="language-python"># app/main.py
import torch
from fastapi import FastAPI, File, UploadFile
from PIL import Image
import io

app = FastAPI()

# GPU使用可否の確認
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# モデルのロード
model = torch.load("/app/models/model.pth", map_location=device)
model.eval()

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "device": str(device),
        "cuda_available": torch.cuda.is_available(),
        "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None
    }

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    # 画像の読み込み
    image_bytes = await file.read()
    image = Image.open(io.BytesIO(image_bytes))

    # 前処理（省略）
    # tensor = preprocess(image)

    # GPU推論
    with torch.no_grad():
        # tensor = tensor.to(device)
        # output = model(tensor)
        pass

    return {"prediction": "result"}
</code></pre>

<h3>Docker ComposeでGPU使用</h3>

<pre><code class="language-yaml"># docker-compose-gpu.yml
version: '3.8'

services:
  ml-api-gpu:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    container_name: ml-api-gpu
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - CUDA_VISIBLE_DEVICES=0
    restart: unless-stopped
</code></pre>

<p>起動コマンド：</p>

<pre><code class="language-bash"># GPU対応コンテナの起動
docker-compose -f docker-compose-gpu.yml up -d

# GPU使用状況の確認
docker exec ml-api-gpu nvidia-smi

# ログの確認
docker-compose -f docker-compose-gpu.yml logs -f
</code></pre>

<h3>パフォーマンス比較</h3>

<table>
<thead>
<tr>
<th>環境</th>
<th>推論時間（1画像）</th>
<th>スループット（画像/秒）</th>
<th>備考</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>CPU (8コア)</strong></td>
<td>150ms</td>
<td>6.7</td>
<td>python:3.9-slim</td>
</tr>
<tr>
<td><strong>GPU (RTX 3090)</strong></td>
<td>15ms</td>
<td>66.7</td>
<td>nvidia/cuda:11.8.0</td>
</tr>
<tr>
<td><strong>高速化比</strong></td>
<td>10倍</td>
<td>10倍</td>
<td>バッチサイズ1</td>
</tr>
</tbody>
</table>

<blockquote>
<p><strong>注意</strong>: バッチサイズを増やすことで、GPUのスループットをさらに向上できます。</p>
</blockquote>

<hr>

<h2>2.6 本章のまとめ</h2>

<h3>学んだこと</h3>

<ol>
<li><p><strong>Dockerの基礎</strong></p>
<ul>
<li>コンテナと仮想マシンの違い</li>
<li>基本的なDockerコマンド</li>
<li>イメージとコンテナの関係</li>
</ul></li>

<li><p><strong>Dockerfileの作成</strong></p>
<ul>
<li>適切なベースイメージの選択</li>
<li>マルチステージビルドによる最適化</li>
<li>レイヤーキャッシュの活用</li>
</ul></li>

<li><p><strong>MLモデルのコンテナ化</strong></p>
<ul>
<li>FastAPI + PyTorchのDocker化</li>
<li>.dockerignoreによる効率化</li>
<li>セキュリティとヘルスチェック</li>
</ul></li>

<li><p><strong>Docker Composeオーケストレーション</strong></p>
<ul>
<li>複数サービスの統合管理</li>
<li>ボリュームと環境変数の管理</li>
<li>サービス間の依存関係</li>
</ul></li>

<li><p><strong>GPU対応MLコンテナ</strong></p>
<ul>
<li>NVIDIA Dockerのセットアップ</li>
<li>CUDAイメージの使用</li>
<li>CPU比10倍のパフォーマンス</li>
</ul></li>
</ol>

<h3>ベストプラクティス</h3>

<table>
<thead>
<tr>
<th>原則</th>
<th>説明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>軽量イメージ</strong></td>
<td>slimやalpineベースを優先</td>
</tr>
<tr>
<td><strong>レイヤー最適化</strong></td>
<td>変更頻度の低いものを先に</td>
</tr>
<tr>
<td><strong>マルチステージビルド</strong></td>
<td>ビルドと実行環境を分離</td>
</tr>
<tr>
<td><strong>非rootユーザー</strong></td>
<td>セキュリティ向上のため</td>
</tr>
<tr>
<td><strong>.dockerignore</strong></td>
<td>不要なファイルを除外</td>
</tr>
<tr>
<td><strong>ヘルスチェック</strong></td>
<td>サービスの健全性監視</td>
</tr>
<tr>
<td><strong>環境変数</strong></td>
<td>設定の外部化</td>
</tr>
</tbody>
</table>

<h3>次の章へ</h3>

<p>第3章では、<strong>Kubernetes によるオーケストレーション</strong>を学びます：</p>
<ul>
<li>Kubernetesの基本概念</li>
<li>Pod、Service、Deploymentの作成</li>
<li>スケーリングと負荷分散</li>
<li>ConfigMapとSecretの管理</li>
<li>本番環境へのデプロイ</li>
</ul>

<div class="navigation">
    <a href="chapter1-overview.html" class="nav-button">← 前の章: デプロイメント概要</a>
    <a href="chapter3-kubernetes.html" class="nav-button">次の章: Kubernetes →</a>
</div>

    </main>

    <footer>
        <p><strong>作成者</strong>: AI Terakoya Content Team</p>
        <p><strong>監修</strong>: Dr. Yusuke Hashimoto（東北大学）</p>
        <p><strong>バージョン</strong>: 1.0 | <strong>作成日</strong>: 2025-10-23</p>
        <p><strong>ライセンス</strong>: Creative Commons BY 4.0</p>
        <p>© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
