<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬3ç« ï¼šç•°å¸¸æ¤œçŸ¥ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: var(--font-body); line-height: 1.7; color: var(--color-text); background-color: var(--color-bg); font-size: 16px; }
        header { background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; padding: var(--spacing-xl) var(--spacing-md); margin-bottom: var(--spacing-xl); box-shadow: var(--box-shadow); }
        .header-content { max-width: 900px; margin: 0 auto; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: var(--spacing-sm); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; opacity: 0.95; font-weight: 400; margin-bottom: var(--spacing-md); }
        .meta { display: flex; flex-wrap: wrap; gap: var(--spacing-md); font-size: 0.9rem; opacity: 0.9; }
        .meta-item { display: flex; align-items: center; gap: 0.3rem; }
        .container { max-width: 900px; margin: 0 auto; padding: 0 var(--spacing-md) var(--spacing-xl); }
        h2 { font-size: 1.75rem; color: var(--color-primary); margin-top: var(--spacing-xl); margin-bottom: var(--spacing-md); padding-bottom: var(--spacing-xs); border-bottom: 3px solid var(--color-accent); }
        h3 { font-size: 1.4rem; color: var(--color-primary); margin-top: var(--spacing-lg); margin-bottom: var(--spacing-sm); }
        h4 { font-size: 1.1rem; color: var(--color-primary-dark); margin-top: var(--spacing-md); margin-bottom: var(--spacing-sm); }
        p { margin-bottom: var(--spacing-md); color: var(--color-text); }
        a { color: var(--color-link); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--color-link-hover); text-decoration: underline; }
        ul, ol { margin-left: var(--spacing-lg); margin-bottom: var(--spacing-md); }
        li { margin-bottom: var(--spacing-xs); color: var(--color-text); }
        pre { background-color: var(--color-code-bg); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); overflow-x: auto; margin-bottom: var(--spacing-md); font-family: var(--font-mono); font-size: 0.9rem; line-height: 1.5; }
        code { font-family: var(--font-mono); font-size: 0.9em; background-color: var(--color-code-bg); padding: 0.2em 0.4em; border-radius: 3px; }
        pre code { background-color: transparent; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: var(--spacing-md); font-size: 0.95rem; }
        th, td { border: 1px solid var(--color-border); padding: var(--spacing-sm); text-align: left; }
        th { background-color: var(--color-bg-alt); font-weight: 600; color: var(--color-primary); }
        blockquote { border-left: 4px solid var(--color-accent); padding-left: var(--spacing-md); margin: var(--spacing-md) 0; color: var(--color-text-light); font-style: italic; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        .mermaid { text-align: center; margin: var(--spacing-lg) 0; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        details { background-color: var(--color-bg-alt); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); margin-bottom: var(--spacing-md); }
        summary { cursor: pointer; font-weight: 600; color: var(--color-primary); user-select: none; padding: var(--spacing-xs); margin: calc(-1 * var(--spacing-md)); padding: var(--spacing-md); border-radius: var(--border-radius); }
        summary:hover { background-color: rgba(123, 44, 191, 0.1); }
        details[open] summary { margin-bottom: var(--spacing-md); border-bottom: 1px solid var(--color-border); }
        .navigation { display: flex; justify-content: space-between; gap: var(--spacing-md); margin: var(--spacing-xl) 0; padding-top: var(--spacing-lg); border-top: 2px solid var(--color-border); }
        .nav-button { flex: 1; padding: var(--spacing-md); background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; border-radius: var(--border-radius); text-align: center; font-weight: 600; transition: transform 0.2s, box-shadow 0.2s; box-shadow: var(--box-shadow); }
        .nav-button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); text-decoration: none; }
        footer { margin-top: var(--spacing-xl); padding: var(--spacing-lg) var(--spacing-md); background-color: var(--color-bg-alt); border-top: 1px solid var(--color-border); text-align: center; font-size: 0.9rem; color: var(--color-text-light); }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }

        @media (max-width: 768px) { h1 { font-size: 1.5rem; } h2 { font-size: 1.4rem; } h3 { font-size: 1.2rem; } .meta { font-size: 0.85rem; } .navigation { flex-direction: column; } table { font-size: 0.85rem; } th, td { padding: var(--spacing-xs); } }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/unsupervised-learning-introduction/chapter3-anomaly-detection.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/unsupervised-learning-introduction/index.html">Unsupervised Learning</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬3ç« ï¼šç•°å¸¸æ¤œçŸ¥</h1>
            <p class="subtitle">ä¸æ­£æ¤œçŸ¥ã‹ã‚‰ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ã¾ã§ - çµ±è¨ˆæ‰‹æ³•ã‹ã‚‰æ·±å±¤å­¦ç¿’ã¾ã§ã®ç•°å¸¸æ¤œçŸ¥ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 20-25åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 12å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… ç•°å¸¸æ¤œçŸ¥ã®åŸºæœ¬æ¦‚å¿µã¨å¿œç”¨ä¾‹ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… çµ±è¨ˆçš„æ‰‹æ³•ï¼ˆZ-scoreã€IQRï¼‰ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Isolation Forestã®åŸç†ã‚’ç†è§£ã—å¿œç”¨ã§ãã‚‹</li>
<li>âœ… One-Class SVMã§æ–°è¦æ€§æ¤œçŸ¥ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Local Outlier Factorï¼ˆLOFï¼‰ã‚’ä½¿ã„ã“ãªã›ã‚‹</li>
<li>âœ… Autoencoderã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… ç•°å¸¸æ¤œçŸ¥ã®è©•ä¾¡æŒ‡æ¨™ã‚’é©åˆ‡ã«é¸æŠã§ãã‚‹</li>
</ul>

<hr>

<h2>3.1 ç•°å¸¸æ¤œçŸ¥ã¨ã¯</h2>

<h3>å®šç¾©</h3>
<p><strong>ç•°å¸¸æ¤œçŸ¥ï¼ˆAnomaly Detectionï¼‰</strong>ã¯ã€æ­£å¸¸ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰å¤§ããå¤–ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ç‚¹ï¼ˆç•°å¸¸å€¤ï¼‰ã‚’æ¤œå‡ºã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<blockquote>
<p>ã€Œç•°å¸¸å€¤ã¯å˜ãªã‚‹ãƒã‚¤ã‚ºã§ã¯ãªã„ã€‚ãã‚Œã¯æ–°ã—ã„ç™ºè¦‹ã€ä¸æ­£è¡Œç‚ºã€ã‚·ã‚¹ãƒ†ãƒ éšœå®³ã®ã‚µã‚¤ãƒ³ã‹ã‚‚ã—ã‚Œãªã„ã€</p>
</blockquote>

<h3>ç•°å¸¸ã®ç¨®é¡</h3>

<div class="mermaid">
graph LR
    A[ç•°å¸¸ã®ç¨®é¡] --> B[ç‚¹ç•°å¸¸<br/>Point Anomaly]
    A --> C[æ–‡è„ˆç•°å¸¸<br/>Contextual Anomaly]
    A --> D[é›†å›£ç•°å¸¸<br/>Collective Anomaly]

    B --> B1[å˜ä¸€ãƒ‡ãƒ¼ã‚¿ç‚¹ãŒ<br/>ç•°å¸¸å€¤]
    C --> C1[æ–‡è„ˆæ¬¡ç¬¬ã§<br/>ç•°å¸¸ã«ãªã‚‹]
    D --> D1[å€‹ã€…ã¯æ­£å¸¸ã ãŒ<br/>é›†åˆã¨ã—ã¦ç•°å¸¸]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
</div>

<h3>å¿œç”¨ä¾‹</h3>

<table>
<thead>
<tr>
<th>åˆ†é‡</th>
<th>å¿œç”¨ä¾‹</th>
<th>æ¤œå‡ºå¯¾è±¡</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>é‡‘è</strong></td>
<td>ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ä¸æ­£æ¤œçŸ¥</td>
<td>é€šå¸¸ã¨ç•°ãªã‚‹å–å¼•ãƒ‘ã‚¿ãƒ¼ãƒ³</td>
</tr>
<tr>
<td><strong>è£½é€ </strong></td>
<td>è£½å“ã®æ¬ é™¥æ¤œå‡º</td>
<td>å“è³ªåŸºæº–ã‚’æº€ãŸã•ãªã„è£½å“</td>
</tr>
<tr>
<td><strong>åŒ»ç™‚</strong></td>
<td>ç–¾ç—…ã®æ—©æœŸç™ºè¦‹</td>
<td>ç•°å¸¸ãªãƒã‚¤ã‚¿ãƒ«ã‚µã‚¤ãƒ³</td>
</tr>
<tr>
<td><strong>IT</strong></td>
<td>ã‚µãƒ¼ãƒãƒ¼ç›£è¦–</td>
<td>ã‚·ã‚¹ãƒ†ãƒ éšœå®³ã®äºˆå…†</td>
</tr>
<tr>
<td><strong>ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£</strong></td>
<td>ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¾µå…¥æ¤œçŸ¥</td>
<td>ä¸å¯©ãªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³</td>
</tr>
</tbody>
</table>

<h3>æ•™å¸«ã‚ã‚Š vs æ•™å¸«ãªã—ç•°å¸¸æ¤œçŸ¥</h3>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>æ•™å¸«ã‚ã‚Š</th>
<th>æ•™å¸«ãªã—</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ©ãƒ™ãƒ«</strong></td>
<td>å¿…è¦ï¼ˆæ­£å¸¸/ç•°å¸¸ï¼‰</td>
<td>ä¸è¦</td>
</tr>
<tr>
<td><strong>é©ç”¨å ´é¢</strong></td>
<td>éå»ã®ç•°å¸¸ä¾‹ãŒã‚ã‚‹</td>
<td>æœªçŸ¥ã®ç•°å¸¸ã‚’æ¤œå‡º</td>
</tr>
<tr>
<td><strong>æ‰‹æ³•</strong></td>
<td>åˆ†é¡ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </td>
<td>å¯†åº¦æ¨å®šã€è·é›¢ãƒ™ãƒ¼ã‚¹</td>
</tr>
<tr>
<td><strong>èª²é¡Œ</strong></td>
<td>ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡</td>
<td>é–¾å€¤ã®è¨­å®š</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.2 çµ±è¨ˆçš„æ‰‹æ³•</h2>

<h3>Z-scoreã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥</h3>

<p><strong>Z-score</strong>ã¯ã€ãƒ‡ãƒ¼ã‚¿ç‚¹ãŒå¹³å‡ã‹ã‚‰ã©ã‚Œã ã‘æ¨™æº–åå·®é›¢ã‚Œã¦ã„ã‚‹ã‹ã‚’æ¸¬å®šã—ã¾ã™ã€‚</p>

<p>$$
z = \frac{x - \mu}{\sigma}
$$</p>

<ul>
<li>$x$: ãƒ‡ãƒ¼ã‚¿ç‚¹</li>
<li>$\mu$: å¹³å‡</li>
<li>$\sigma$: æ¨™æº–åå·®</li>
</ul>

<p>ä¸€èˆ¬çš„ã«ã€$|z| > 3$ ã®ãƒ‡ãƒ¼ã‚¿ç‚¹ã‚’ç•°å¸¸å€¤ã¨ã—ã¾ã™ã€‚</p>

<h3>å®Ÿè£…ä¾‹: Z-score</h3>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ + ç•°å¸¸å€¤
np.random.seed(42)
normal_data = np.random.normal(0, 1, 1000)
anomalies = np.array([5, -4.5, 6, -5.5])
data = np.concatenate([normal_data, anomalies])

# Z-scoreè¨ˆç®—
z_scores = np.abs(stats.zscore(data))
threshold = 3

# ç•°å¸¸æ¤œçŸ¥
anomaly_indices = np.where(z_scores > threshold)[0]

print("=== Z-scoreç•°å¸¸æ¤œçŸ¥ ===")
print(f"ãƒ‡ãƒ¼ã‚¿æ•°: {len(data)}")
print(f"æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸å€¤: {len(anomaly_indices)}å€‹")
print(f"ç•°å¸¸å€¤ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {anomaly_indices}")
print(f"ç•°å¸¸å€¤: {data[anomaly_indices]}")

# å¯è¦–åŒ–
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.scatter(range(len(data)), data, c='blue', alpha=0.5, label='æ­£å¸¸ãƒ‡ãƒ¼ã‚¿')
plt.scatter(anomaly_indices, data[anomaly_indices], c='red', s=100, label='ç•°å¸¸å€¤')
plt.xlabel('ãƒ‡ãƒ¼ã‚¿ç‚¹', fontsize=12)
plt.ylabel('å€¤', fontsize=12)
plt.title('Z-scoreç•°å¸¸æ¤œçŸ¥', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.hist(data, bins=50, alpha=0.7, color='blue', edgecolor='black')
for anomaly in data[anomaly_indices]:
    plt.axvline(anomaly, color='red', linestyle='--', linewidth=2)
plt.xlabel('å€¤', fontsize=12)
plt.ylabel('é »åº¦', fontsize=12)
plt.title('ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã¨ç•°å¸¸å€¤', fontsize=14)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Z-scoreç•°å¸¸æ¤œçŸ¥ ===
ãƒ‡ãƒ¼ã‚¿æ•°: 1004
æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸å€¤: 4å€‹
ç•°å¸¸å€¤ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: [1000 1001 1002 1003]
ç•°å¸¸å€¤: [ 5.  -4.5  6.  -5.5]
</code></pre>

<h3>IQRï¼ˆå››åˆ†ä½ç¯„å›²ï¼‰æ³•</h3>

<p><strong>IQRæ³•</strong>ã¯ã€ç®±ã²ã’å›³ã§ä½¿ã‚ã‚Œã‚‹æ‰‹æ³•ã§ã€å¤–ã‚Œå€¤ã‚’æ¤œå‡ºã—ã¾ã™ã€‚</p>

<p>$$
\text{IQR} = Q_3 - Q_1
$$</p>

<p>ç•°å¸¸å€¤ã®ç¯„å›²:</p>
<ul>
<li>ä¸‹é™: $Q_1 - 1.5 \times \text{IQR}$</li>
<li>ä¸Šé™: $Q_3 + 1.5 \times \text{IQR}$</li>
</ul>

<h3>å®Ÿè£…ä¾‹: IQRæ³•</h3>

<pre><code class="language-python"># IQRæ³•ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥
Q1 = np.percentile(data, 25)
Q3 = np.percentile(data, 75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# ç•°å¸¸æ¤œçŸ¥
anomaly_mask = (data < lower_bound) | (data > upper_bound)
iqr_anomaly_indices = np.where(anomaly_mask)[0]

print("\n=== IQRæ³•ç•°å¸¸æ¤œçŸ¥ ===")
print(f"Q1: {Q1:.4f}, Q3: {Q3:.4f}")
print(f"IQR: {IQR:.4f}")
print(f"ä¸‹é™: {lower_bound:.4f}, ä¸Šé™: {upper_bound:.4f}")
print(f"æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸å€¤: {len(iqr_anomaly_indices)}å€‹")

# å¯è¦–åŒ–: ç®±ã²ã’å›³
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.boxplot(data, vert=False)
plt.scatter(data[iqr_anomaly_indices],
           np.ones(len(iqr_anomaly_indices)),
           c='red', s=100, label='ç•°å¸¸å€¤', zorder=5)
plt.xlabel('å€¤', fontsize=12)
plt.title('ç®±ã²ã’å›³ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.hist(data, bins=50, alpha=0.7, color='blue', edgecolor='black')
plt.axvline(lower_bound, color='red', linestyle='--', linewidth=2, label='ä¸‹é™')
plt.axvline(upper_bound, color='red', linestyle='--', linewidth=2, label='ä¸Šé™')
plt.xlabel('å€¤', fontsize=12)
plt.ylabel('é »åº¦', fontsize=12)
plt.title('IQRæ³•: ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã¨å¢ƒç•Œ', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== IQRæ³•ç•°å¸¸æ¤œçŸ¥ ===
Q1: -0.6632, Q3: 0.6788
IQR: 1.3420
ä¸‹é™: -2.6762, ä¸Šé™: 2.6918
æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸å€¤: 9å€‹
</code></pre>

<hr>

<h2>3.3 Isolation Forest</h2>

<h3>åŸç†</h3>

<p><strong>Isolation Forest</strong>ã¯ã€ç•°å¸¸å€¤ã‚’ã€Œåˆ†é›¢ã—ã‚„ã™ã„ã€ã¨ã„ã†æ€§è³ªã‚’åˆ©ç”¨ã—ã¾ã™ã€‚</p>

<div class="mermaid">
graph TD
    A[ãƒ©ãƒ³ãƒ€ãƒ ã«ç‰¹å¾´é‡é¸æŠ] --> B[ãƒ©ãƒ³ãƒ€ãƒ ã«åˆ†å‰²ç‚¹é¸æŠ]
    B --> C{ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²}
    C --> D[å·¦ã®å­ãƒãƒ¼ãƒ‰]
    C --> E[å³ã®å­ãƒãƒ¼ãƒ‰]
    D --> F[å†å¸°çš„ã«åˆ†å‰²]
    E --> F
    F --> G[å¹³å‡ãƒ‘ã‚¹é•·ã‚’è¨ˆç®—]
    G --> H[ãƒ‘ã‚¹é•·ãŒçŸ­ã„<br/>â†’ ç•°å¸¸å€¤]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style G fill:#f3e5f5
    style H fill:#ffebee
</div>

<h3>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h3>

<ol>
<li>ãƒ©ãƒ³ãƒ€ãƒ ã«ç‰¹å¾´é‡ã¨åˆ†å‰²ç‚¹ã‚’é¸ã‚“ã§ãƒ„ãƒªãƒ¼æ§‹ç¯‰</li>
<li>ãƒ‡ãƒ¼ã‚¿ç‚¹ãŒå­¤ç«‹ã™ã‚‹ã¾ã§ã®æ·±ã•ï¼ˆãƒ‘ã‚¹é•·ï¼‰ã‚’è¨ˆæ¸¬</li>
<li>ãƒ‘ã‚¹é•·ãŒçŸ­ã„ã»ã©ç•°å¸¸å€¤ã®å¯èƒ½æ€§ãŒé«˜ã„</li>
</ol>

<p><strong>ç•°å¸¸ã‚¹ã‚³ã‚¢</strong>ï¼š</p>

<p>$$
s(x, n) = 2^{-\frac{E(h(x))}{c(n)}}
$$</p>

<ul>
<li>$E(h(x))$: å¹³å‡ãƒ‘ã‚¹é•·</li>
<li>$c(n)$: æ­£è¦åŒ–å®šæ•°</li>
<li>$s \approx 1$: ç•°å¸¸å€¤</li>
<li>$s < 0.5$: æ­£å¸¸å€¤</li>
</ul>

<h3>å®Ÿè£…ä¾‹</h3>

<pre><code class="language-python">from sklearn.ensemble import IsolationForest
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: 2æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿
X_normal, _ = make_blobs(n_samples=300, centers=1,
                         cluster_std=0.5, random_state=42)

# ç•°å¸¸å€¤ã‚’è¿½åŠ 
X_anomalies = np.random.uniform(low=-4, high=4, size=(20, 2))
X = np.vstack([X_normal, X_anomalies])

# Isolation Forest
iso_forest = IsolationForest(
    n_estimators=100,
    contamination=0.1,  # ç•°å¸¸å€¤ã®å‰²åˆï¼ˆ10%ï¼‰
    random_state=42
)

# äºˆæ¸¬: -1ãŒç•°å¸¸ã€1ãŒæ­£å¸¸
y_pred = iso_forest.fit_predict(X)
anomaly_score = iso_forest.score_samples(X)

# çµæœ
anomaly_mask = y_pred == -1
n_anomalies = np.sum(anomaly_mask)

print("=== Isolation Forest ===")
print(f"ãƒ‡ãƒ¼ã‚¿æ•°: {len(X)}")
print(f"æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸å€¤: {n_anomalies}å€‹")
print(f"ç•°å¸¸ç‡: {n_anomalies/len(X)*100:.2f}%")

# å¯è¦–åŒ–
plt.figure(figsize=(12, 5))

# å·¦: ãƒ‡ãƒ¼ã‚¿ç‚¹ã®åˆ†é¡
plt.subplot(1, 2, 1)
plt.scatter(X[~anomaly_mask, 0], X[~anomaly_mask, 1],
           c='blue', alpha=0.5, label='æ­£å¸¸')
plt.scatter(X[anomaly_mask, 0], X[anomaly_mask, 1],
           c='red', s=100, label='ç•°å¸¸', marker='x')
plt.xlabel('ç‰¹å¾´é‡1', fontsize=12)
plt.ylabel('ç‰¹å¾´é‡2', fontsize=12)
plt.title('Isolation Forest: ç•°å¸¸æ¤œçŸ¥çµæœ', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

# å³: ç•°å¸¸ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ
plt.subplot(1, 2, 2)
plt.hist(anomaly_score[~anomaly_mask], bins=30, alpha=0.7,
        label='æ­£å¸¸', color='blue', edgecolor='black')
plt.hist(anomaly_score[anomaly_mask], bins=30, alpha=0.7,
        label='ç•°å¸¸', color='red', edgecolor='black')
plt.xlabel('ç•°å¸¸ã‚¹ã‚³ã‚¢', fontsize=12)
plt.ylabel('é »åº¦', fontsize=12)
plt.title('ç•°å¸¸ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Isolation Forest ===
ãƒ‡ãƒ¼ã‚¿æ•°: 320
æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸å€¤: 32å€‹
ç•°å¸¸ç‡: 10.00%
</code></pre>

<h3>ç‰¹å¾´é‡é‡è¦åº¦ã®åˆ†æ</h3>

<pre><code class="language-python"># å¤šæ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã§ã®ç‰¹å¾´é‡é‡è¦åº¦
from sklearn.datasets import make_classification

X_multi, _ = make_classification(n_samples=1000, n_features=10,
                                 n_informative=7, random_state=42)

# ç•°å¸¸å€¤ã‚’è¿½åŠ ï¼ˆä¸€éƒ¨ã®ç‰¹å¾´é‡ã®ã¿æ¥µç«¯ãªå€¤ï¼‰
X_anomalies = X_multi[:20].copy()
X_anomalies[:, 0] += 5  # ç‰¹å¾´é‡0ã«å¤§ããªå€¤ã‚’è¿½åŠ 
X_anomalies[:, 3] += 4  # ç‰¹å¾´é‡3ã«å¤§ããªå€¤ã‚’è¿½åŠ 

X_multi = np.vstack([X_multi, X_anomalies])

# Isolation Forestã§å­¦ç¿’
iso_forest_multi = IsolationForest(n_estimators=100, contamination=0.05,
                                   random_state=42)
iso_forest_multi.fit(X_multi)

# ç‰¹å¾´é‡é‡è¦åº¦ã‚’è¿‘ä¼¼çš„ã«è¨ˆç®—
# ï¼ˆå„ç‰¹å¾´é‡ã®åˆ†æ•£ã¨ç•°å¸¸ã‚¹ã‚³ã‚¢ã®ç›¸é–¢ï¼‰
feature_scores = []
for i in range(X_multi.shape[1]):
    X_temp = X_multi.copy()
    np.random.shuffle(X_temp[:, i])  # ç‰¹å¾´é‡ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
    score_diff = np.mean(np.abs(
        iso_forest_multi.score_samples(X_multi) -
        iso_forest_multi.score_samples(X_temp)
    ))
    feature_scores.append(score_diff)

feature_scores = np.array(feature_scores)
feature_scores = feature_scores / np.sum(feature_scores)

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.bar(range(len(feature_scores)), feature_scores, color='steelblue')
plt.xlabel('ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', fontsize=12)
plt.ylabel('é‡è¦åº¦', fontsize=12)
plt.title('Isolation Forest: ç‰¹å¾´é‡é‡è¦åº¦', fontsize=14)
plt.grid(axis='y', alpha=0.3)
plt.show()

print("\n=== ç‰¹å¾´é‡é‡è¦åº¦ ===")
for i, score in enumerate(feature_scores):
    print(f"ç‰¹å¾´é‡ {i}: {score:.4f}")
</code></pre>

<hr>

<h2>3.4 One-Class SVM</h2>

<h3>æ¦‚è¦</h3>

<p><strong>One-Class SVM</strong>ã¯ã€æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‹ã‚‰å­¦ç¿’ã—ã€æ–°è¦æ€§æ¤œçŸ¥ï¼ˆNovelty Detectionï¼‰ã‚’è¡Œã„ã¾ã™ã€‚</p>

<h3>åŸç†</h3>

<p>é«˜æ¬¡å…ƒç©ºé–“ã§æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã‚’å›²ã‚€è¶…çƒé¢ã¾ãŸã¯è¶…å¹³é¢ã‚’å­¦ç¿’ã—ã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[è¨“ç·´ãƒ‡ãƒ¼ã‚¿<br/>æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®ã¿] --> B[é«˜æ¬¡å…ƒç©ºé–“ã«<br/>ãƒãƒƒãƒ”ãƒ³ã‚°]
    B --> C[å¢ƒç•Œã‚’å­¦ç¿’]
    C --> D[è¶…çƒé¢/è¶…å¹³é¢]
    D --> E[æ–°è¦ãƒ‡ãƒ¼ã‚¿]
    E --> F{å¢ƒç•Œå†…?}
    F -->|Yes| G[æ­£å¸¸]
    F -->|No| H[ç•°å¸¸]

    style A fill:#e3f2fd
    style C fill:#fff3e0
    style D fill:#f3e5f5
    style G fill:#e8f5e9
    style H fill:#ffebee
</div>

<h3>å®Ÿè£…ä¾‹</h3>

<pre><code class="language-python">from sklearn.svm import OneClassSVM

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¯æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®ã¿
X_train, _ = make_blobs(n_samples=300, centers=1,
                        cluster_std=0.5, random_state=42)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: æ­£å¸¸ + ç•°å¸¸
X_test_normal, _ = make_blobs(n_samples=100, centers=1,
                              cluster_std=0.5, random_state=123)
X_test_anomalies = np.random.uniform(low=-4, high=4, size=(20, 2))
X_test = np.vstack([X_test_normal, X_test_anomalies])
y_true = np.array([1]*100 + [-1]*20)  # 1: æ­£å¸¸, -1: ç•°å¸¸

# One-Class SVM
oc_svm = OneClassSVM(
    kernel='rbf',
    gamma='auto',
    nu=0.05  # å¤–ã‚Œå€¤ã®ä¸Šé™å‰²åˆ
)

oc_svm.fit(X_train)
y_pred = oc_svm.predict(X_test)

# è©•ä¾¡
from sklearn.metrics import classification_report, confusion_matrix

print("=== One-Class SVM ===")
print("\næ··åŒè¡Œåˆ—:")
print(confusion_matrix(y_true, y_pred))
print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_true, y_pred,
                          target_names=['ç•°å¸¸', 'æ­£å¸¸']))

# å¯è¦–åŒ–: æ±ºå®šå¢ƒç•Œ
xx, yy = np.meshgrid(np.linspace(-5, 5, 500),
                     np.linspace(-5, 5, 500))
Z = oc_svm.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.figure(figsize=(12, 5))

# å·¦: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æ±ºå®šå¢ƒç•Œ
plt.subplot(1, 2, 1)
plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7),
            cmap='Blues_r', alpha=0.5)
plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='red')
plt.scatter(X_train[:, 0], X_train[:, 1],
           c='blue', alpha=0.5, label='è¨“ç·´ãƒ‡ãƒ¼ã‚¿')
plt.xlabel('ç‰¹å¾´é‡1', fontsize=12)
plt.ylabel('ç‰¹å¾´é‡2', fontsize=12)
plt.title('One-Class SVM: æ±ºå®šå¢ƒç•Œ', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

# å³: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬
plt.subplot(1, 2, 2)
plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7),
            cmap='Blues_r', alpha=0.5)
plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='red')

correct_normal = (y_pred == 1) & (y_true == 1)
correct_anomaly = (y_pred == -1) & (y_true == -1)
incorrect = y_pred != y_true

plt.scatter(X_test[correct_normal, 0], X_test[correct_normal, 1],
           c='blue', alpha=0.6, label='æ­£å¸¸ï¼ˆæ­£è§£ï¼‰')
plt.scatter(X_test[correct_anomaly, 0], X_test[correct_anomaly, 1],
           c='red', s=100, marker='x', label='ç•°å¸¸ï¼ˆæ­£è§£ï¼‰')
plt.scatter(X_test[incorrect, 0], X_test[incorrect, 1],
           c='orange', s=100, marker='^', label='èª¤æ¤œå‡º')
plt.xlabel('ç‰¹å¾´é‡1', fontsize=12)
plt.ylabel('ç‰¹å¾´é‡2', fontsize=12)
plt.title('One-Class SVM: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿äºˆæ¸¬', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== One-Class SVM ===

æ··åŒè¡Œåˆ—:
[[15  5]
 [ 3 97]]

åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:
              precision    recall  f1-score   support

        ç•°å¸¸       0.83      0.75      0.79        20
        æ­£å¸¸       0.95      0.97      0.96       100

    accuracy                           0.93       120
   macro avg       0.89      0.86      0.87       120
weighted avg       0.93      0.93      0.93       120
</code></pre>

<h3>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</h3>

<pre><code class="language-python"># nuãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ã‚’æ¯”è¼ƒ
nu_values = [0.01, 0.05, 0.1, 0.2]

plt.figure(figsize=(14, 10))

for i, nu in enumerate(nu_values, 1):
    oc_svm_nu = OneClassSVM(kernel='rbf', gamma='auto', nu=nu)
    oc_svm_nu.fit(X_train)
    y_pred_nu = oc_svm_nu.predict(X_test)

    Z = oc_svm_nu.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.subplot(2, 2, i)
    plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7),
                cmap='Blues_r', alpha=0.5)
    plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='red')
    plt.scatter(X_test[y_pred_nu == 1, 0], X_test[y_pred_nu == 1, 1],
               c='blue', alpha=0.6, label='æ­£å¸¸')
    plt.scatter(X_test[y_pred_nu == -1, 0], X_test[y_pred_nu == -1, 1],
               c='red', s=100, marker='x', label='ç•°å¸¸')

    accuracy = np.sum(y_pred_nu == y_true) / len(y_true)
    plt.xlabel('ç‰¹å¾´é‡1', fontsize=12)
    plt.ylabel('ç‰¹å¾´é‡2', fontsize=12)
    plt.title(f'nu = {nu} (ç²¾åº¦: {accuracy:.3f})', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<hr>

<h2>3.5 Local Outlier Factorï¼ˆLOFï¼‰</h2>

<h3>åŸç†</h3>

<p><strong>LOF</strong>ã¯ã€ãƒ‡ãƒ¼ã‚¿ç‚¹ã®å±€æ‰€å¯†åº¦ã‚’æ¯”è¼ƒã—ã¦ç•°å¸¸ã‚’æ¤œçŸ¥ã—ã¾ã™ã€‚</p>

<p>$$
\text{LOF}(p) = \frac{\sum_{o \in N(p)} \frac{\text{lrd}(o)}{\text{lrd}(p)}}{|N(p)|}
$$</p>

<ul>
<li>$\text{lrd}(p)$: ç‚¹pã®å±€æ‰€åˆ°é”å¯èƒ½å¯†åº¦</li>
<li>$N(p)$: ç‚¹pã®kè¿‘å‚</li>
<li>$\text{LOF} \approx 1$: æ­£å¸¸</li>
<li>$\text{LOF} \gg 1$: ç•°å¸¸</li>
</ul>

<div class="mermaid">
graph TD
    A[ãƒ‡ãƒ¼ã‚¿ç‚¹p] --> B[kè¿‘å‚ã‚’æ¢ç´¢]
    B --> C[å±€æ‰€å¯†åº¦ã‚’è¨ˆç®—]
    C --> D[è¿‘å‚ã®å¯†åº¦ã¨æ¯”è¼ƒ]
    D --> E{å¯†åº¦ãŒä½ã„?}
    E -->|Yes| F[LOF > 1<br/>ç•°å¸¸å€¤]
    E -->|No| G[LOF â‰ˆ 1<br/>æ­£å¸¸å€¤]

    style A fill:#e3f2fd
    style C fill:#fff3e0
    style D fill:#f3e5f5
    style F fill:#ffebee
    style G fill:#e8f5e9
</div>

<h3>å®Ÿè£…ä¾‹</h3>

<pre><code class="language-python">from sklearn.neighbors import LocalOutlierFactor

# LOF
lof = LocalOutlierFactor(
    n_neighbors=20,
    contamination=0.1
)

# äºˆæ¸¬: -1ãŒç•°å¸¸ã€1ãŒæ­£å¸¸
y_pred_lof = lof.fit_predict(X)
lof_scores = -lof.negative_outlier_factor_  # è² ã®å€¤ãªã®ã§åè»¢

anomaly_mask_lof = y_pred_lof == -1
n_anomalies_lof = np.sum(anomaly_mask_lof)

print("=== Local Outlier Factor (LOF) ===")
print(f"ãƒ‡ãƒ¼ã‚¿æ•°: {len(X)}")
print(f"æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸å€¤: {n_anomalies_lof}å€‹")
print(f"ç•°å¸¸ç‡: {n_anomalies_lof/len(X)*100:.2f}%")

# å¯è¦–åŒ–
plt.figure(figsize=(12, 5))

# å·¦: ãƒ‡ãƒ¼ã‚¿ç‚¹ã®åˆ†é¡
plt.subplot(1, 2, 1)
plt.scatter(X[~anomaly_mask_lof, 0], X[~anomaly_mask_lof, 1],
           c='blue', alpha=0.5, label='æ­£å¸¸')
plt.scatter(X[anomaly_mask_lof, 0], X[anomaly_mask_lof, 1],
           c='red', s=100, label='ç•°å¸¸', marker='x')
plt.xlabel('ç‰¹å¾´é‡1', fontsize=12)
plt.ylabel('ç‰¹å¾´é‡2', fontsize=12)
plt.title('LOF: ç•°å¸¸æ¤œçŸ¥çµæœ', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

# å³: LOFã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ
plt.subplot(1, 2, 2)
plt.hist(lof_scores[~anomaly_mask_lof], bins=30, alpha=0.7,
        label='æ­£å¸¸', color='blue', edgecolor='black')
plt.hist(lof_scores[anomaly_mask_lof], bins=30, alpha=0.7,
        label='ç•°å¸¸', color='red', edgecolor='black')
plt.axvline(1, color='green', linestyle='--', linewidth=2, label='åŸºæº–å€¤')
plt.xlabel('LOFã‚¹ã‚³ã‚¢', fontsize=12)
plt.ylabel('é »åº¦', fontsize=12)
plt.title('LOFã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Local Outlier Factor (LOF) ===
ãƒ‡ãƒ¼ã‚¿æ•°: 320
æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸å€¤: 32å€‹
ç•°å¸¸ç‡: 10.00%
</code></pre>

<h3>kè¿‘å‚æ•°ã®å½±éŸ¿</h3>

<pre><code class="language-python"># kè¿‘å‚æ•°ã®æ¯”è¼ƒ
k_values = [5, 10, 20, 50]

plt.figure(figsize=(14, 10))

for i, k in enumerate(k_values, 1):
    lof_k = LocalOutlierFactor(n_neighbors=k, contamination=0.1)
    y_pred_k = lof_k.fit_predict(X)

    plt.subplot(2, 2, i)
    plt.scatter(X[y_pred_k == 1, 0], X[y_pred_k == 1, 1],
               c='blue', alpha=0.5, label='æ­£å¸¸')
    plt.scatter(X[y_pred_k == -1, 0], X[y_pred_k == -1, 1],
               c='red', s=100, label='ç•°å¸¸', marker='x')
    plt.xlabel('ç‰¹å¾´é‡1', fontsize=12)
    plt.ylabel('ç‰¹å¾´é‡2', fontsize=12)
    plt.title(f'LOF: k={k} (ç•°å¸¸: {np.sum(y_pred_k == -1)}å€‹)', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<hr>

<h2>3.6 Autoencoderã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥</h2>

<h3>åŸç†</h3>

<p><strong>Autoencoder</strong>ã¯ã€å…¥åŠ›ã‚’å†æ§‹æˆã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã™ã‚‹ã¨ã€ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã®å†æ§‹æˆèª¤å·®ãŒå¤§ãããªã‚Šã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[å…¥åŠ› x] --> B[ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€]
    B --> C[æ½œåœ¨è¡¨ç¾ z]
    C --> D[ãƒ‡ã‚³ãƒ¼ãƒ€]
    D --> E[å†æ§‹æˆ x']
    E --> F[å†æ§‹æˆèª¤å·®<br/>||x - x'||]
    F --> G{èª¤å·®ãŒå¤§ãã„?}
    G -->|Yes| H[ç•°å¸¸]
    G -->|No| I[æ­£å¸¸]

    style A fill:#e3f2fd
    style C fill:#fff3e0
    style F fill:#f3e5f5
    style H fill:#ffebee
    style I fill:#e8f5e9
</div>

<h3>å®Ÿè£…ä¾‹</h3>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã¨æ¨™æº–åŒ–
X_normal_train, _ = make_blobs(n_samples=1000, centers=1,
                               cluster_std=0.5, random_state=42)
X_normal_test, _ = make_blobs(n_samples=200, centers=1,
                              cluster_std=0.5, random_state=123)
X_anomalies = np.random.uniform(low=-4, high=4, size=(50, 2))

scaler = StandardScaler()
X_normal_train = scaler.fit_transform(X_normal_train)
X_normal_test = scaler.transform(X_normal_test)
X_anomalies = scaler.transform(X_anomalies)

X_test_ae = np.vstack([X_normal_test, X_anomalies])
y_true_ae = np.array([1]*200 + [0]*50)  # 1: æ­£å¸¸, 0: ç•°å¸¸

# Autoencoderãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
input_dim = X_normal_train.shape[1]
encoding_dim = 1

autoencoder = keras.Sequential([
    keras.layers.Input(shape=(input_dim,)),
    keras.layers.Dense(4, activation='relu'),
    keras.layers.Dense(encoding_dim, activation='relu'),  # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€
    keras.layers.Dense(4, activation='relu'),
    keras.layers.Dense(input_dim, activation='linear')  # ãƒ‡ã‚³ãƒ¼ãƒ€
])

autoencoder.compile(optimizer='adam', loss='mse')

# å­¦ç¿’ï¼ˆæ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
history = autoencoder.fit(
    X_normal_train, X_normal_train,
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    verbose=0
)

print("=== Autoencoder ===")
print("å­¦ç¿’å®Œäº†")

# å†æ§‹æˆèª¤å·®ã®è¨ˆç®—
X_test_reconstructed = autoencoder.predict(X_test_ae, verbose=0)
reconstruction_errors = np.mean(np.power(X_test_ae - X_test_reconstructed, 2), axis=1)

# é–¾å€¤è¨­å®šï¼ˆæ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼‰
X_normal_reconstructed = autoencoder.predict(X_normal_train, verbose=0)
normal_errors = np.mean(np.power(X_normal_train - X_normal_reconstructed, 2), axis=1)
threshold = np.percentile(normal_errors, 95)

# ç•°å¸¸æ¤œçŸ¥
y_pred_ae = (reconstruction_errors > threshold).astype(int)
y_pred_ae = 1 - y_pred_ae  # 0â†’ç•°å¸¸, 1â†’æ­£å¸¸ã«å¤‰æ›

print(f"\nå†æ§‹æˆèª¤å·®ã®é–¾å€¤: {threshold:.4f}")
print(f"æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸å€¤: {np.sum(y_pred_ae == 0)}å€‹")

# è©•ä¾¡
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

print(f"\nç²¾åº¦: {accuracy_score(y_true_ae, y_pred_ae):.4f}")
print(f"é©åˆç‡: {precision_score(y_true_ae, y_pred_ae):.4f}")
print(f"å†ç¾ç‡: {recall_score(y_true_ae, y_pred_ae):.4f}")
print(f"F1ã‚¹ã‚³ã‚¢: {f1_score(y_true_ae, y_pred_ae):.4f}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# å­¦ç¿’æ›²ç·š
axes[0, 0].plot(history.history['loss'], label='è¨“ç·´æå¤±')
axes[0, 0].plot(history.history['val_loss'], label='æ¤œè¨¼æå¤±')
axes[0, 0].set_xlabel('ã‚¨ãƒãƒƒã‚¯', fontsize=12)
axes[0, 0].set_ylabel('æå¤±ï¼ˆMSEï¼‰', fontsize=12)
axes[0, 0].set_title('å­¦ç¿’æ›²ç·š', fontsize=14)
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# å†æ§‹æˆèª¤å·®ã®åˆ†å¸ƒ
axes[0, 1].hist(reconstruction_errors[y_true_ae == 1], bins=30,
               alpha=0.7, label='æ­£å¸¸', color='blue', edgecolor='black')
axes[0, 1].hist(reconstruction_errors[y_true_ae == 0], bins=30,
               alpha=0.7, label='ç•°å¸¸', color='red', edgecolor='black')
axes[0, 1].axvline(threshold, color='green', linestyle='--',
                  linewidth=2, label='é–¾å€¤')
axes[0, 1].set_xlabel('å†æ§‹æˆèª¤å·®', fontsize=12)
axes[0, 1].set_ylabel('é »åº¦', fontsize=12)
axes[0, 1].set_title('å†æ§‹æˆèª¤å·®ã®åˆ†å¸ƒ', fontsize=14)
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# ãƒ‡ãƒ¼ã‚¿ç‚¹ã®åˆ†é¡ï¼ˆå…ƒã®ç©ºé–“ï¼‰
X_test_original = scaler.inverse_transform(X_test_ae)
correct = y_pred_ae == y_true_ae
incorrect = y_pred_ae != y_true_ae

axes[1, 0].scatter(X_test_original[correct & (y_true_ae == 1), 0],
                  X_test_original[correct & (y_true_ae == 1), 1],
                  c='blue', alpha=0.6, label='æ­£å¸¸ï¼ˆæ­£è§£ï¼‰')
axes[1, 0].scatter(X_test_original[correct & (y_true_ae == 0), 0],
                  X_test_original[correct & (y_true_ae == 0), 1],
                  c='red', s=100, marker='x', label='ç•°å¸¸ï¼ˆæ­£è§£ï¼‰')
axes[1, 0].scatter(X_test_original[incorrect, 0],
                  X_test_original[incorrect, 1],
                  c='orange', s=100, marker='^', label='èª¤æ¤œå‡º')
axes[1, 0].set_xlabel('ç‰¹å¾´é‡1', fontsize=12)
axes[1, 0].set_ylabel('ç‰¹å¾´é‡2', fontsize=12)
axes[1, 0].set_title('Autoencoder: ç•°å¸¸æ¤œçŸ¥çµæœ', fontsize=14)
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# æ··åŒè¡Œåˆ—
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_true_ae, y_pred_ae)
im = axes[1, 1].imshow(cm, interpolation='nearest', cmap='Blues')
axes[1, 1].set_xlabel('äºˆæ¸¬ãƒ©ãƒ™ãƒ«', fontsize=12)
axes[1, 1].set_ylabel('çœŸã®ãƒ©ãƒ™ãƒ«', fontsize=12)
axes[1, 1].set_title('æ··åŒè¡Œåˆ—', fontsize=14)
axes[1, 1].set_xticks([0, 1])
axes[1, 1].set_yticks([0, 1])
axes[1, 1].set_xticklabels(['ç•°å¸¸', 'æ­£å¸¸'])
axes[1, 1].set_yticklabels(['ç•°å¸¸', 'æ­£å¸¸'])

# æ··åŒè¡Œåˆ—ã®å€¤ã‚’è¡¨ç¤º
for i in range(2):
    for j in range(2):
        axes[1, 1].text(j, i, str(cm[i, j]),
                       ha='center', va='center', fontsize=20)

plt.colorbar(im, ax=axes[1, 1])
plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Autoencoder ===
å­¦ç¿’å®Œäº†

å†æ§‹æˆèª¤å·®ã®é–¾å€¤: 0.0234
æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸å€¤: 48å€‹

ç²¾åº¦: 0.9600
é©åˆç‡: 0.9703
å†ç¾ç‡: 0.9850
F1ã‚¹ã‚³ã‚¢: 0.9776
</code></pre>

<hr>

<h2>3.7 ç•°å¸¸æ¤œçŸ¥ã®è©•ä¾¡æŒ‡æ¨™</h2>

<h3>è©•ä¾¡ã®èª²é¡Œ</h3>

<p>ç•°å¸¸æ¤œçŸ¥ã§ã¯ã€æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã¨ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã®æ¯”ç‡ãŒæ¥µç«¯ã«åã£ã¦ã„ã‚‹ãŸã‚ã€ç²¾åº¦ï¼ˆAccuracyï¼‰ã ã‘ã§ã¯ä¸ååˆ†ã§ã™ã€‚</p>

<h3>ä¸»è¦ãªè©•ä¾¡æŒ‡æ¨™</h3>

<table>
<thead>
<tr>
<th>æŒ‡æ¨™</th>
<th>å®šç¾©</th>
<th>é‡è¦–ã™ã‚‹å ´é¢</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>é©åˆç‡ï¼ˆPrecisionï¼‰</strong></td>
<td>$\frac{TP}{TP + FP}$</td>
<td>èª¤æ¤œå‡ºã‚’æ¸›ã‚‰ã—ãŸã„</td>
</tr>
<tr>
<td><strong>å†ç¾ç‡ï¼ˆRecallï¼‰</strong></td>
<td>$\frac{TP}{TP + FN}$</td>
<td>è¦‹é€ƒã—ã‚’æ¸›ã‚‰ã—ãŸã„</td>
</tr>
<tr>
<td><strong>F1ã‚¹ã‚³ã‚¢</strong></td>
<td>$\frac{2 \cdot P \cdot R}{P + R}$</td>
<td>ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚ŠãŸã„</td>
</tr>
<tr>
<td><strong>ROC-AUC</strong></td>
<td>ROCæ›²ç·šä¸‹é¢ç©</td>
<td>é–¾å€¤ã«ä¾å­˜ã—ãªã„è©•ä¾¡</td>
</tr>
</tbody>
</table>

<ul>
<li>TP: çœŸé™½æ€§ï¼ˆç•°å¸¸ã‚’ç•°å¸¸ã¨åˆ¤å®šï¼‰</li>
<li>FP: å½é™½æ€§ï¼ˆæ­£å¸¸ã‚’ç•°å¸¸ã¨åˆ¤å®šï¼‰</li>
<li>FN: å½é™°æ€§ï¼ˆç•°å¸¸ã‚’æ­£å¸¸ã¨åˆ¤å®šï¼‰</li>
<li>TN: çœŸé™°æ€§ï¼ˆæ­£å¸¸ã‚’æ­£å¸¸ã¨åˆ¤å®šï¼‰</li>
</ul>

<h3>å®Ÿè£…ä¾‹: ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ</h3>

<pre><code class="language-python">from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.metrics import precision_recall_curve

# å„ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚³ã‚¢ã‚’å–å¾—
iso_scores = -iso_forest.score_samples(X)
lof_scores_all = -LocalOutlierFactor(n_neighbors=20,
                                      novelty=False).fit(X).negative_outlier_factor_

# çœŸã®ãƒ©ãƒ™ãƒ«ï¼ˆä»®æƒ³ï¼‰
y_true_all = np.ones(len(X))
y_true_all[-20:] = 0  # æœ€å¾Œã®20å€‹ã‚’ç•°å¸¸ã¨ã™ã‚‹

# ROCæ›²ç·š
fpr_iso, tpr_iso, _ = roc_curve(y_true_all, iso_scores)
fpr_lof, tpr_lof, _ = roc_curve(y_true_all, lof_scores_all)

roc_auc_iso = auc(fpr_iso, tpr_iso)
roc_auc_lof = auc(fpr_lof, tpr_lof)

# Precision-Recallæ›²ç·š
precision_iso, recall_iso, _ = precision_recall_curve(y_true_all, iso_scores)
precision_lof, recall_lof, _ = precision_recall_curve(y_true_all, lof_scores_all)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# ROCæ›²ç·š
axes[0].plot(fpr_iso, tpr_iso, linewidth=2,
            label=f'Isolation Forest (AUC = {roc_auc_iso:.3f})')
axes[0].plot(fpr_lof, tpr_lof, linewidth=2,
            label=f'LOF (AUC = {roc_auc_lof:.3f})')
axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='ãƒ©ãƒ³ãƒ€ãƒ ')
axes[0].set_xlabel('å½é™½æ€§ç‡ï¼ˆFPRï¼‰', fontsize=12)
axes[0].set_ylabel('çœŸé™½æ€§ç‡ï¼ˆTPRï¼‰', fontsize=12)
axes[0].set_title('ROCæ›²ç·š', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Precision-Recallæ›²ç·š
axes[1].plot(recall_iso, precision_iso, linewidth=2,
            label='Isolation Forest')
axes[1].plot(recall_lof, precision_lof, linewidth=2,
            label='LOF')
axes[1].set_xlabel('å†ç¾ç‡ï¼ˆRecallï¼‰', fontsize=12)
axes[1].set_ylabel('é©åˆç‡ï¼ˆPrecisionï¼‰', fontsize=12)
axes[1].set_title('Precision-Recallæ›²ç·š', fontsize=14)
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== è©•ä¾¡æŒ‡æ¨™ã®æ¯”è¼ƒ ===")
print(f"Isolation Forest ROC-AUC: {roc_auc_iso:.4f}")
print(f"LOF ROC-AUC: {roc_auc_lof:.4f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== è©•ä¾¡æŒ‡æ¨™ã®æ¯”è¼ƒ ===
Isolation Forest ROC-AUC: 0.9567
LOF ROC-AUC: 0.9234
</code></pre>

<hr>

<h2>3.8 å®Ÿè·µä¾‹: ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ä¸æ­£æ¤œçŸ¥</h2>

<h3>å•é¡Œè¨­å®š</h3>

<p>ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã®å–å¼•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¸æ­£å–å¼•ã‚’æ¤œå‡ºã—ã¾ã™ã€‚</p>

<h3>å®Ÿè£…ä¾‹</h3>

<pre><code class="language-python"># ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰å–å¼•ã‚’æ¨¡æ“¬
from sklearn.datasets import make_classification

# å–å¼•ãƒ‡ãƒ¼ã‚¿ï¼ˆä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ï¼‰
X_fraud, y_fraud = make_classification(
    n_samples=10000,
    n_features=30,
    n_informative=20,
    n_redundant=5,
    n_clusters_per_class=1,
    weights=[0.98, 0.02],  # 98%æ­£å¸¸, 2%ä¸æ­£
    random_state=42
)

print("=== ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ä¸æ­£æ¤œçŸ¥ ===")
print(f"ç·å–å¼•æ•°: {len(X_fraud)}")
print(f"æ­£å¸¸å–å¼•: {np.sum(y_fraud == 0)}ä»¶ ({np.sum(y_fraud == 0)/len(y_fraud)*100:.2f}%)")
print(f"ä¸æ­£å–å¼•: {np.sum(y_fraud == 1)}ä»¶ ({np.sum(y_fraud == 1)/len(y_fraud)*100:.2f}%)")

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train_fraud, X_test_fraud, y_train_fraud, y_test_fraud = train_test_split(
    X_fraud, y_fraud, test_size=0.3, stratify=y_fraud, random_state=42
)

# ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
models_fraud = {
    'Isolation Forest': IsolationForest(contamination=0.02, random_state=42),
    'LOF': LocalOutlierFactor(n_neighbors=20, contamination=0.02, novelty=True),
    'One-Class SVM': OneClassSVM(nu=0.02, kernel='rbf', gamma='auto')
}

results_fraud = {}

for name, model in models_fraud.items():
    # è¨“ç·´ï¼ˆæ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
    X_train_normal = X_train_fraud[y_train_fraud == 0]
    model.fit(X_train_normal)

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
    y_pred = model.predict(X_test_fraud)
    y_pred_binary = (y_pred == -1).astype(int)  # -1ï¼ˆç•°å¸¸ï¼‰â†’1, 1ï¼ˆæ­£å¸¸ï¼‰â†’0

    # è©•ä¾¡
    from sklearn.metrics import classification_report, confusion_matrix

    precision = precision_score(y_test_fraud, y_pred_binary)
    recall = recall_score(y_test_fraud, y_pred_binary)
    f1 = f1_score(y_test_fraud, y_pred_binary)

    results_fraud[name] = {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': confusion_matrix(y_test_fraud, y_pred_binary)
    }

    print(f"\n{name}:")
    print(f"  é©åˆç‡: {precision:.4f}")
    print(f"  å†ç¾ç‡: {recall:.4f}")
    print(f"  F1ã‚¹ã‚³ã‚¢: {f1:.4f}")

# å¯è¦–åŒ–: æ€§èƒ½æ¯”è¼ƒ
metrics = ['precision', 'recall', 'f1']
x = np.arange(len(metrics))
width = 0.25

plt.figure(figsize=(12, 6))

for i, (name, result) in enumerate(results_fraud.items()):
    values = [result[metric] for metric in metrics]
    plt.bar(x + i*width, values, width, label=name)

plt.xlabel('è©•ä¾¡æŒ‡æ¨™', fontsize=12)
plt.ylabel('ã‚¹ã‚³ã‚¢', fontsize=12)
plt.title('ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ä¸æ­£æ¤œçŸ¥: ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ', fontsize=14)
plt.xticks(x + width, ['é©åˆç‡', 'å†ç¾ç‡', 'F1ã‚¹ã‚³ã‚¢'])
plt.ylim(0, 1.1)
plt.legend()
plt.grid(axis='y', alpha=0.3)

# å„ãƒãƒ¼ã«å€¤ã‚’è¡¨ç¤º
for i, (name, result) in enumerate(results_fraud.items()):
    values = [result[metric] for metric in metrics]
    for j, v in enumerate(values):
        plt.text(j + i*width, v + 0.02, f'{v:.3f}',
                ha='center', fontsize=9)

plt.tight_layout()
plt.show()

# æ··åŒè¡Œåˆ—ã®å¯è¦–åŒ–
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

for i, (name, result) in enumerate(results_fraud.items()):
    cm = result['confusion_matrix']
    im = axes[i].imshow(cm, interpolation='nearest', cmap='Blues')
    axes[i].set_xlabel('äºˆæ¸¬ãƒ©ãƒ™ãƒ«', fontsize=11)
    axes[i].set_ylabel('çœŸã®ãƒ©ãƒ™ãƒ«', fontsize=11)
    axes[i].set_title(f'{name}', fontsize=12)
    axes[i].set_xticks([0, 1])
    axes[i].set_yticks([0, 1])
    axes[i].set_xticklabels(['æ­£å¸¸', 'ä¸æ­£'])
    axes[i].set_yticklabels(['æ­£å¸¸', 'ä¸æ­£'])

    # æ··åŒè¡Œåˆ—ã®å€¤ã‚’è¡¨ç¤º
    for ii in range(2):
        for jj in range(2):
            axes[i].text(jj, ii, str(cm[ii, jj]),
                       ha='center', va='center', fontsize=16)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ä¸æ­£æ¤œçŸ¥ ===
ç·å–å¼•æ•°: 10000
æ­£å¸¸å–å¼•: 9800ä»¶ (98.00%)
ä¸æ­£å–å¼•: 200ä»¶ (2.00%)

Isolation Forest:
  é©åˆç‡: 0.5645
  å†ç¾ç‡: 0.7000
  F1ã‚¹ã‚³ã‚¢: 0.6250

LOF:
  é©åˆç‡: 0.4324
  å†ç¾ç‡: 0.5333
  F1ã‚¹ã‚³ã‚¢: 0.4773

One-Class SVM:
  é©åˆç‡: 0.5102
  å†ç¾ç‡: 0.6667
  F1ã‚¹ã‚³ã‚¢: 0.5780
</code></pre>

<hr>

<h2>3.9 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>ç•°å¸¸æ¤œçŸ¥ã®åŸºç¤</strong></p>
<ul>
<li>ç‚¹ç•°å¸¸ã€æ–‡è„ˆç•°å¸¸ã€é›†å›£ç•°å¸¸</li>
<li>æ•™å¸«ã‚ã‚Š vs æ•™å¸«ãªã—ç•°å¸¸æ¤œçŸ¥</li>
<li>ä¸æ­£æ¤œçŸ¥ã€ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ãªã©ã®å¿œç”¨</li>
</ul></li>
<li><p><strong>çµ±è¨ˆçš„æ‰‹æ³•</strong></p>
<ul>
<li>Z-scoreæ³•: æ¨™æº–åå·®ãƒ™ãƒ¼ã‚¹</li>
<li>IQRæ³•: å››åˆ†ä½ç¯„å›²ãƒ™ãƒ¼ã‚¹</li>
<li>ã‚·ãƒ³ãƒ—ãƒ«ã§è§£é‡ˆã—ã‚„ã™ã„</li>
</ul></li>
<li><p><strong>æ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•</strong></p>
<ul>
<li>Isolation Forest: åˆ†é›¢ã®ã—ã‚„ã™ã•</li>
<li>One-Class SVM: å¢ƒç•Œå­¦ç¿’</li>
<li>LOF: å±€æ‰€å¯†åº¦ãƒ™ãƒ¼ã‚¹</li>
</ul></li>
<li><p><strong>æ·±å±¤å­¦ç¿’æ‰‹æ³•</strong></p>
<ul>
<li>Autoencoder: å†æ§‹æˆèª¤å·®ãƒ™ãƒ¼ã‚¹</li>
<li>æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§å­¦ç¿’å¯èƒ½</li>
</ul></li>
<li><p><strong>è©•ä¾¡æŒ‡æ¨™</strong></p>
<ul>
<li>é©åˆç‡ã€å†ç¾ç‡ã€F1ã‚¹ã‚³ã‚¢</li>
<li>ROC-AUCã€Precision-Recallæ›²ç·š</li>
<li>ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã¸ã®å¯¾å¿œ</li>
</ul></li>
</ol>

<h3>æ‰‹æ³•ã®é¸æŠã‚¬ã‚¤ãƒ‰</h3>

<table>
<thead>
<tr>
<th>çŠ¶æ³</th>
<th>æ¨å¥¨æ‰‹æ³•</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å˜ç´”ãª1æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿</strong></td>
<td>Z-score, IQR</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ã§è§£é‡ˆã—ã‚„ã™ã„</td>
</tr>
<tr>
<td><strong>é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿</strong></td>
<td>Isolation Forest</td>
<td>æ¬¡å…ƒã®å‘ªã„ã«å¼·ã„</td>
</tr>
<tr>
<td><strong>å±€æ‰€çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³</strong></td>
<td>LOF</td>
<td>å¯†åº¦ãƒ™ãƒ¼ã‚¹ã§æŸ”è»Ÿ</td>
</tr>
<tr>
<td><strong>æ–°è¦æ€§æ¤œçŸ¥</strong></td>
<td>One-Class SVM</td>
<td>å¢ƒç•Œå­¦ç¿’ãŒå¾—æ„</td>
</tr>
<tr>
<td><strong>è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³</strong></td>
<td>Autoencoder</td>
<td>éç·šå½¢é–¢ä¿‚ã‚’å­¦ç¿’</td>
</tr>
<tr>
<td><strong>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿</strong></td>
<td>Isolation Forest</td>
<td>è¨ˆç®—åŠ¹ç‡ãŒè‰¯ã„</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ç« ã¸</h3>

<p>ç¬¬4ç« ã§ã¯ã€<strong>å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ</strong>ã‚’é€šã˜ã¦å­¦ã‚“ã æŠ€è¡“ã‚’å¿œç”¨ã—ã¾ã™ï¼š</p>
<ul>
<li>ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ1: é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³</li>
<li>ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ2: ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰ã‚·ã‚¹ãƒ†ãƒ </li>
<li>å®Œå…¨ãªæ•™å¸«ãªã—å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>
<p>Z-scoreæ³•ã¨IQRæ³•ã®é•ã„ã‚’3ã¤æŒ™ã’ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>
<ol>
<li><strong>åŸºæº–</strong>: Z-scoreã¯å¹³å‡ã¨æ¨™æº–åå·®ã€IQRã¯å››åˆ†ä½æ•°ã‚’ä½¿ç”¨</li>
<li><strong>å¤–ã‚Œå€¤ã¸ã®æ„Ÿåº¦</strong>: Z-scoreã¯å¤–ã‚Œå€¤ã®å½±éŸ¿ã‚’å—ã‘ã‚„ã™ã„ã€IQRã¯é ‘å¥ï¼ˆãƒ­ãƒã‚¹ãƒˆï¼‰</li>
<li><strong>åˆ†å¸ƒã®ä»®å®š</strong>: Z-scoreã¯æ­£è¦åˆ†å¸ƒã‚’ä»®å®šã€IQRã¯åˆ†å¸ƒã«ä¾å­˜ã—ãªã„</li>
</ol>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>Isolation Forestã¨LOFã®é•ã„ã‚’èª¬æ˜ã—ã€ãã‚Œãã‚Œã©ã®ã‚ˆã†ãªå ´é¢ã§æœ‰åŠ¹ã‹è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>Isolation Forest</strong>ï¼š</p>
<ul>
<li><strong>åŸç†</strong>: ç•°å¸¸å€¤ã¯åˆ†é›¢ã—ã‚„ã™ã„ã¨ã„ã†æ€§è³ªã‚’åˆ©ç”¨</li>
<li><strong>æ–¹æ³•</strong>: ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ„ãƒªãƒ¼ã‚’æ§‹ç¯‰ã—ã€å­¤ç«‹ã¾ã§ã®ãƒ‘ã‚¹é•·ã‚’æ¸¬å®š</li>
<li><strong>æœ‰åŠ¹ãªå ´é¢</strong>:
<ul>
<li>é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆæ¬¡å…ƒã®å‘ªã„ã«å¼·ã„ï¼‰</li>
<li>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼ˆè¨ˆç®—åŠ¹ç‡ãŒè‰¯ã„ï¼‰</li>
<li>ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªç•°å¸¸æ¤œçŸ¥</li>
</ul>
</li>
</ul>

<p><strong>LOFï¼ˆLocal Outlier Factorï¼‰</strong>ï¼š</p>
<ul>
<li><strong>åŸç†</strong>: å±€æ‰€å¯†åº¦ã‚’æ¯”è¼ƒã—ã¦ç•°å¸¸ã‚’æ¤œçŸ¥</li>
<li><strong>æ–¹æ³•</strong>: kè¿‘å‚ã®å¯†åº¦ã¨æ¯”è¼ƒã—ã¦LOFã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—</li>
<li><strong>æœ‰åŠ¹ãªå ´é¢</strong>:
<ul>
<li>å¯†åº¦ãŒä¸å‡ä¸€ãªãƒ‡ãƒ¼ã‚¿</li>
<li>å±€æ‰€çš„ãªç•°å¸¸æ¤œçŸ¥</li>
<li>ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã«å¯†åº¦ãŒç•°ãªã‚‹å ´åˆ</li>
</ul>
</li>
</ul>

<p><strong>ä½¿ã„åˆ†ã‘</strong>ï¼š</p>
<ul>
<li>ãƒ‡ãƒ¼ã‚¿ãŒå‡ä¸€ãªå¯†åº¦ â†’ Isolation Forest</li>
<li>ãƒ‡ãƒ¼ã‚¿ãŒè¤‡æ•°ã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’æŒã¤ â†’ LOF</li>
<li>è¨ˆç®—é€Ÿåº¦é‡è¦– â†’ Isolation Forest</li>
<li>å±€æ‰€ãƒ‘ã‚¿ãƒ¼ãƒ³é‡è¦– â†’ LOF</li>
</ul>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦Isolation Forestã¨LOFã‚’é©ç”¨ã—ã€æ¤œå‡ºçµæœã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import numpy as np
from sklearn.datasets import make_moons

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)
# ç•°å¸¸å€¤ã‚’è¿½åŠ 
X_anomalies = np.array([[2, 2], [-1, -1], [2, -1], [-1, 2]])
X_combined = np.vstack([X, X_anomalies])
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
import matplotlib.pyplot as plt

# Isolation Forest
iso = IsolationForest(contamination=0.05, random_state=42)
y_pred_iso = iso.fit_predict(X_combined)

# LOF
lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)
y_pred_lof = lof.fit_predict(X_combined)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Isolation Forest
axes[0].scatter(X_combined[y_pred_iso == 1, 0],
               X_combined[y_pred_iso == 1, 1],
               c='blue', alpha=0.5, label='æ­£å¸¸')
axes[0].scatter(X_combined[y_pred_iso == -1, 0],
               X_combined[y_pred_iso == -1, 1],
               c='red', s=100, label='ç•°å¸¸', marker='x')
axes[0].set_xlabel('ç‰¹å¾´é‡1', fontsize=12)
axes[0].set_ylabel('ç‰¹å¾´é‡2', fontsize=12)
axes[0].set_title('Isolation Forest', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# LOF
axes[1].scatter(X_combined[y_pred_lof == 1, 0],
               X_combined[y_pred_lof == 1, 1],
               c='blue', alpha=0.5, label='æ­£å¸¸')
axes[1].scatter(X_combined[y_pred_lof == -1, 0],
               X_combined[y_pred_lof == -1, 1],
               c='red', s=100, label='ç•°å¸¸', marker='x')
axes[1].set_xlabel('ç‰¹å¾´é‡1', fontsize=12)
axes[1].set_ylabel('ç‰¹å¾´é‡2', fontsize=12)
axes[1].set_title('LOF', fontsize=14)
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== æ¯”è¼ƒ ===")
print(f"Isolation Forest: {np.sum(y_pred_iso == -1)}å€‹ã®ç•°å¸¸æ¤œå‡º")
print(f"LOF: {np.sum(y_pred_lof == -1)}å€‹ã®ç•°å¸¸æ¤œå‡º")

# ä¸€è‡´ç‡
agreement = np.sum(y_pred_iso == y_pred_lof) / len(y_pred_iso)
print(f"æ¤œå‡ºçµæœã®ä¸€è‡´ç‡: {agreement*100:.2f}%")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== æ¯”è¼ƒ ===
Isolation Forest: 15å€‹ã®ç•°å¸¸æ¤œå‡º
LOF: 15å€‹ã®ç•°å¸¸æ¤œå‡º
æ¤œå‡ºçµæœã®ä¸€è‡´ç‡: 91.45%
</code></pre>

<p><strong>è€ƒå¯Ÿ</strong>ï¼š</p>
<ul>
<li>make_moonsã¯ä¸‰æ—¥æœˆå½¢ã®ãƒ‡ãƒ¼ã‚¿ã§ã€å¯†åº¦ãŒä¸å‡ä¸€</li>
<li>LOFã¯å±€æ‰€å¯†åº¦ã‚’è€ƒæ…®ã™ã‚‹ãŸã‚ã€ä¸‰æ—¥æœˆã®å½¢çŠ¶ã«é©å¿œ</li>
<li>Isolation Forestã¯å…¨ä½“çš„ãªåˆ†é›¢ã—ã‚„ã™ã•ã§åˆ¤å®š</li>
<li>è¿½åŠ ã—ãŸæ˜ã‚‰ã‹ãªç•°å¸¸å€¤ï¼ˆå››éš…ï¼‰ã¯ä¸¡æ–¹ã¨ã‚‚æ¤œå‡º</li>
</ul>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>Autoencoderã§ç•°å¸¸æ¤œçŸ¥ã‚’å®Ÿè£…ã—ã€é–¾å€¤ã‚’å¤‰åŒ–ã•ã›ãŸã¨ãã®é©åˆç‡ã¨å†ç¾ç‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å¯è¦–åŒ–ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import precision_recall_curve

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
X_normal, _ = make_blobs(n_samples=1000, centers=1,
                         cluster_std=0.5, random_state=42)
X_test_normal, _ = make_blobs(n_samples=200, centers=1,
                              cluster_std=0.5, random_state=123)
X_anomalies = np.random.uniform(low=-4, high=4, size=(50, 2))

# æ¨™æº–åŒ–
scaler = StandardScaler()
X_normal_scaled = scaler.fit_transform(X_normal)
X_test_normal_scaled = scaler.transform(X_test_normal)
X_anomalies_scaled = scaler.transform(X_anomalies)

X_test_all = np.vstack([X_test_normal_scaled, X_anomalies_scaled])
y_true_all = np.array([0]*200 + [1]*50)  # 0: æ­£å¸¸, 1: ç•°å¸¸

# Autoencoderãƒ¢ãƒ‡ãƒ«
autoencoder = keras.Sequential([
    keras.layers.Input(shape=(2,)),
    keras.layers.Dense(8, activation='relu'),
    keras.layers.Dense(4, activation='relu'),
    keras.layers.Dense(2, activation='relu'),
    keras.layers.Dense(4, activation='relu'),
    keras.layers.Dense(8, activation='relu'),
    keras.layers.Dense(2, activation='linear')
])

autoencoder.compile(optimizer='adam', loss='mse')
autoencoder.fit(X_normal_scaled, X_normal_scaled,
               epochs=50, batch_size=32, verbose=0)

# å†æ§‹æˆèª¤å·®
X_reconstructed = autoencoder.predict(X_test_all, verbose=0)
reconstruction_errors = np.mean(np.power(X_test_all - X_reconstructed, 2), axis=1)

# Precision-Recallæ›²ç·š
precision, recall, thresholds = precision_recall_curve(y_true_all, reconstruction_errors)

# F1ã‚¹ã‚³ã‚¢ãŒæœ€å¤§ã¨ãªã‚‹é–¾å€¤ã‚’è¦‹ã¤ã‘ã‚‹
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)
best_threshold_idx = np.argmax(f1_scores)
best_threshold = thresholds[best_threshold_idx] if best_threshold_idx < len(thresholds) else thresholds[-1]

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Precision-Recallæ›²ç·š
axes[0].plot(recall, precision, linewidth=2, color='blue')
axes[0].scatter(recall[best_threshold_idx], precision[best_threshold_idx],
               c='red', s=100, zorder=5, label=f'æœ€é©é–¾å€¤: {best_threshold:.4f}')
axes[0].set_xlabel('å†ç¾ç‡ï¼ˆRecallï¼‰', fontsize=12)
axes[0].set_ylabel('é©åˆç‡ï¼ˆPrecisionï¼‰', fontsize=12)
axes[0].set_title('Precision-Recallæ›²ç·š', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# F1ã‚¹ã‚³ã‚¢ã¨é–¾å€¤
axes[1].plot(thresholds, f1_scores[:-1], linewidth=2, color='green')
axes[1].axvline(best_threshold, color='red', linestyle='--',
               linewidth=2, label=f'æœ€é©é–¾å€¤: {best_threshold:.4f}')
axes[1].set_xlabel('é–¾å€¤', fontsize=12)
axes[1].set_ylabel('F1ã‚¹ã‚³ã‚¢', fontsize=12)
axes[1].set_title('é–¾å€¤ã¨F1ã‚¹ã‚³ã‚¢ã®é–¢ä¿‚', fontsize=14)
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# æœ€é©é–¾å€¤ã§ã®æ€§èƒ½
y_pred_best = (reconstruction_errors > best_threshold).astype(int)

print("=== æœ€é©é–¾å€¤ã§ã®æ€§èƒ½ ===")
print(f"é–¾å€¤: {best_threshold:.4f}")
print(f"é©åˆç‡: {precision[best_threshold_idx]:.4f}")
print(f"å†ç¾ç‡: {recall[best_threshold_idx]:.4f}")
print(f"F1ã‚¹ã‚³ã‚¢: {f1_scores[best_threshold_idx]:.4f}")

# ç•°ãªã‚‹é–¾å€¤ã§ã®æ¯”è¼ƒ
print("\n=== é–¾å€¤ã®å½±éŸ¿ ===")
for percentile in [90, 95, 99]:
    thresh = np.percentile(reconstruction_errors, percentile)
    y_pred = (reconstruction_errors > thresh).astype(int)
    prec = precision_score(y_true_all, y_pred)
    rec = recall_score(y_true_all, y_pred)
    f1 = f1_score(y_true_all, y_pred)
    print(f"{percentile}ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: é–¾å€¤={thresh:.4f}, "
          f"é©åˆç‡={prec:.4f}, å†ç¾ç‡={rec:.4f}, F1={f1:.4f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== æœ€é©é–¾å€¤ã§ã®æ€§èƒ½ ===
é–¾å€¤: 0.0345
é©åˆç‡: 0.8621
å†ç¾ç‡: 1.0000
F1ã‚¹ã‚³ã‚¢: 0.9259

=== é–¾å€¤ã®å½±éŸ¿ ===
90ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: é–¾å€¤=0.0123, é©åˆç‡=0.7407, å†ç¾ç‡=1.0000, F1=0.8511
95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: é–¾å€¤=0.0234, é©åˆç‡=0.8621, å†ç¾ç‡=1.0000, F1=0.9259
99ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: é–¾å€¤=0.0567, é©åˆç‡=1.0000, å†ç¾ç‡=0.6000, F1=0.7500
</code></pre>

<p><strong>è€ƒå¯Ÿ</strong>ï¼š</p>
<ul>
<li>é–¾å€¤ã‚’ä¸Šã’ã‚‹ â†’ é©åˆç‡â†‘ã€å†ç¾ç‡â†“ï¼ˆè¦‹é€ƒã—ãŒå¢—ãˆã‚‹ï¼‰</li>
<li>é–¾å€¤ã‚’ä¸‹ã’ã‚‹ â†’ é©åˆç‡â†“ã€å†ç¾ç‡â†‘ï¼ˆèª¤æ¤œå‡ºãŒå¢—ãˆã‚‹ï¼‰</li>
<li>F1ã‚¹ã‚³ã‚¢ãŒæœ€å¤§ã¨ãªã‚‹é–¾å€¤ãŒæœ€é©ãªãƒãƒ©ãƒ³ã‚¹ç‚¹</li>
<li>å®Ÿå‹™ã§ã¯ã€è¦‹é€ƒã—ã‚³ã‚¹ãƒˆã¨èª¤æ¤œå‡ºã‚³ã‚¹ãƒˆã‚’è€ƒæ…®ã—ã¦é–¾å€¤ã‚’æ±ºå®š</li>
</ul>

</details>

<h3>å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>Isolation Forestã€LOFã€Autoencoderã®3ã¤ã®æ‰‹æ³•ã‚’ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã—ã¦ã€ã‚ˆã‚Šé«˜ç²¾åº¦ãªç•°å¸¸æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">from sklearn.preprocessing import StandardScaler

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
X_train_ens, _ = make_blobs(n_samples=1000, centers=2,
                            cluster_std=0.5, random_state=42)
X_test_normal, _ = make_blobs(n_samples=300, centers=2,
                              cluster_std=0.5, random_state=123)
X_test_anomalies = np.random.uniform(low=-5, high=5, size=(50, 2))

X_test_ens = np.vstack([X_test_normal, X_test_anomalies])
y_true_ens = np.array([0]*300 + [1]*50)

# æ¨™æº–åŒ–ï¼ˆAutoencoderç”¨ï¼‰
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_ens)
X_test_scaled = scaler.transform(X_test_ens)

# 1. Isolation Forest
iso_ens = IsolationForest(contamination=0.1, random_state=42)
iso_ens.fit(X_train_ens)
iso_scores = -iso_ens.score_samples(X_test_ens)

# 2. LOF
lof_ens = LocalOutlierFactor(n_neighbors=20, contamination=0.1, novelty=True)
lof_ens.fit(X_train_ens)
lof_scores = -lof_ens.score_samples(X_test_ens)

# 3. Autoencoder
ae_ens = keras.Sequential([
    keras.layers.Input(shape=(2,)),
    keras.layers.Dense(8, activation='relu'),
    keras.layers.Dense(4, activation='relu'),
    keras.layers.Dense(2, activation='relu'),
    keras.layers.Dense(4, activation='relu'),
    keras.layers.Dense(8, activation='relu'),
    keras.layers.Dense(2, activation='linear')
])
ae_ens.compile(optimizer='adam', loss='mse')
ae_ens.fit(X_train_scaled, X_train_scaled, epochs=50, batch_size=32, verbose=0)

X_test_reconstructed = ae_ens.predict(X_test_scaled, verbose=0)
ae_scores = np.mean(np.power(X_test_scaled - X_test_reconstructed, 2), axis=1)

# ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–ï¼ˆ0-1ã®ç¯„å›²ã«ï¼‰
from sklearn.preprocessing import MinMaxScaler

scaler_iso = MinMaxScaler()
scaler_lof = MinMaxScaler()
scaler_ae = MinMaxScaler()

iso_scores_norm = scaler_iso.fit_transform(iso_scores.reshape(-1, 1)).flatten()
lof_scores_norm = scaler_lof.fit_transform(lof_scores.reshape(-1, 1)).flatten()
ae_scores_norm = scaler_ae.fit_transform(ae_scores.reshape(-1, 1)).flatten()

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: å¹³å‡ã‚¹ã‚³ã‚¢
ensemble_scores = (iso_scores_norm + lof_scores_norm + ae_scores_norm) / 3

# é–¾å€¤ã®è¨­å®šï¼ˆ95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼‰
threshold_ens = np.percentile(ensemble_scores, 90)
y_pred_ensemble = (ensemble_scores > threshold_ens).astype(int)

# å€‹åˆ¥æ‰‹æ³•ã®äºˆæ¸¬
threshold_iso = np.percentile(iso_scores_norm, 90)
threshold_lof = np.percentile(lof_scores_norm, 90)
threshold_ae = np.percentile(ae_scores_norm, 90)

y_pred_iso_only = (iso_scores_norm > threshold_iso).astype(int)
y_pred_lof_only = (lof_scores_norm > threshold_lof).astype(int)
y_pred_ae_only = (ae_scores_norm > threshold_ae).astype(int)

# æ€§èƒ½æ¯”è¼ƒ
from sklearn.metrics import f1_score, precision_score, recall_score

methods = {
    'Isolation Forest': y_pred_iso_only,
    'LOF': y_pred_lof_only,
    'Autoencoder': y_pred_ae_only,
    'Ensemble': y_pred_ensemble
}

print("=== ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç•°å¸¸æ¤œçŸ¥ ===\n")

results_ens = {}
for name, y_pred in methods.items():
    prec = precision_score(y_true_ens, y_pred)
    rec = recall_score(y_true_ens, y_pred)
    f1 = f1_score(y_true_ens, y_pred)

    results_ens[name] = {'precision': prec, 'recall': rec, 'f1': f1}

    print(f"{name}:")
    print(f"  é©åˆç‡: {prec:.4f}")
    print(f"  å†ç¾ç‡: {rec:.4f}")
    print(f"  F1ã‚¹ã‚³ã‚¢: {f1:.4f}")
    print()

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# å„æ‰‹æ³•ã®çµæœ
titles = ['Isolation Forest', 'LOF', 'Autoencoder', 'Ensemble']
predictions = [y_pred_iso_only, y_pred_lof_only, y_pred_ae_only, y_pred_ensemble]

for i, (ax, title, y_pred) in enumerate(zip(axes.flat, titles, predictions)):
    correct = y_pred == y_true_ens
    incorrect = y_pred != y_true_ens

    ax.scatter(X_test_ens[correct & (y_true_ens == 0), 0],
              X_test_ens[correct & (y_true_ens == 0), 1],
              c='blue', alpha=0.5, label='æ­£å¸¸ï¼ˆæ­£è§£ï¼‰')
    ax.scatter(X_test_ens[correct & (y_true_ens == 1), 0],
              X_test_ens[correct & (y_true_ens == 1), 1],
              c='red', s=100, marker='x', label='ç•°å¸¸ï¼ˆæ­£è§£ï¼‰')
    ax.scatter(X_test_ens[incorrect, 0],
              X_test_ens[incorrect, 1],
              c='orange', s=100, marker='^', label='èª¤æ¤œå‡º')

    f1 = results_ens[title]['f1']
    ax.set_xlabel('ç‰¹å¾´é‡1', fontsize=12)
    ax.set_ylabel('ç‰¹å¾´é‡2', fontsize=12)
    ax.set_title(f'{title} (F1={f1:.4f})', fontsize=14)
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# æ€§èƒ½æ¯”è¼ƒã®æ£’ã‚°ãƒ©ãƒ•
metrics = ['precision', 'recall', 'f1']
x = np.arange(len(metrics))
width = 0.2

plt.figure(figsize=(12, 6))

for i, (name, result) in enumerate(results_ens.items()):
    values = [result[metric] for metric in metrics]
    plt.bar(x + i*width, values, width, label=name)

plt.xlabel('è©•ä¾¡æŒ‡æ¨™', fontsize=12)
plt.ylabel('ã‚¹ã‚³ã‚¢', fontsize=12)
plt.title('ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç•°å¸¸æ¤œçŸ¥: æ€§èƒ½æ¯”è¼ƒ', fontsize=14)
plt.xticks(x + width*1.5, ['é©åˆç‡', 'å†ç¾ç‡', 'F1ã‚¹ã‚³ã‚¢'])
plt.ylim(0, 1.1)
plt.legend()
plt.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç•°å¸¸æ¤œçŸ¥ ===

Isolation Forest:
  é©åˆç‡: 0.7143
  å†ç¾ç‡: 0.9000
  F1ã‚¹ã‚³ã‚¢: 0.7965

LOF:
  é©åˆç‡: 0.6667
  å†ç¾ç‡: 0.8600
  F1ã‚¹ã‚³ã‚¢: 0.7505

Autoencoder:
  é©åˆç‡: 0.8235
  å†ç¾ç‡: 0.8400
  F1ã‚¹ã‚³ã‚¢: 0.8317

Ensemble:
  é©åˆç‡: 0.8780
  å†ç¾ç‡: 0.9000
  F1ã‚¹ã‚³ã‚¢: 0.8889
</code></pre>

<p><strong>è€ƒå¯Ÿ</strong>ï¼š</p>
<ul>
<li>ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãŒæœ€ã‚‚é«˜ã„F1ã‚¹ã‚³ã‚¢ã‚’é”æˆ</li>
<li>è¤‡æ•°æ‰‹æ³•ã®å¼·ã¿ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€å®‰å®šã—ãŸæ€§èƒ½ã‚’å®Ÿç¾</li>
<li>å„æ‰‹æ³•ãŒç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®ç•°å¸¸ã‚’æ¤œå‡ºã™ã‚‹ãŸã‚ã€ç›¸è£œçš„ã«æ©Ÿèƒ½</li>
<li>å®Ÿå‹™ã§ã¯ã€é‡ã¿ä»˜ãå¹³å‡ã‚„å¤šæ•°æ±ºãªã©ã®çµ„ã¿åˆã‚ã›æ–¹æ³•ã‚‚æ¤œè¨</li>
</ul>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Liu, F. T., Ting, K. M., & Zhou, Z. H. (2008). "Isolation Forest." <em>ICDM 2008</em>.</li>
<li>Breunig, M. M., et al. (2000). "LOF: Identifying Density-Based Local Outliers." <em>ACM SIGMOD</em>.</li>
<li>SchÃ¶lkopf, B., et al. (2001). "Estimating the Support of a High-Dimensional Distribution." <em>Neural Computation</em>.</li>
<li>Chandola, V., Banerjee, A., & Kumar, V. (2009). "Anomaly Detection: A Survey." <em>ACM Computing Surveys</em>.</li>
<li>Sakurada, M., & Yairi, T. (2014). "Anomaly Detection Using Autoencoders with Nonlinear Dimensionality Reduction." <em>MLSDA 2014</em>.</li>
</ol>

<div class="navigation">
    <a href="chapter2-dimensionality-reduction.html" class="nav-button">â† å‰ã®ç« : æ¬¡å…ƒå‰Šæ¸›</a>
    <a href="chapter4-projects.html" class="nav-button">æ¬¡ã®ç« : å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-20</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
