<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬2ç« ï¼šMAML - Model-Agnostic Meta-Learning - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬2ç« ï¼šMAML - Model-Agnostic Meta-Learning</h1>
            <p class="subtitle">å‹¾é…ãƒ™ãƒ¼ã‚¹ã®ãƒ¡ã‚¿å­¦ç¿’ã®æœ€é‡è¦æ‰‹æ³•</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 30-35åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š-ä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 3å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… MAMLã®åŸç†ã¨äºŒæ®µéšæœ€é©åŒ–ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… å‹¾é…ã®å‹¾é…ï¼ˆSecond-order derivativesï¼‰ã®è¨ˆç®—æ–¹æ³•ã‚’ç¿’å¾—ã™ã‚‹</li>
<li>âœ… PyTorchã¨higherãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§MAMLã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… First-order MAML (FOMAML)ã®åŠ¹ç‡åŒ–ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Reptileã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè£…ã¨æ¯”è¼ƒãŒã§ãã‚‹</li>
<li>âœ… Omniglot Few-Shotåˆ†é¡ã§MAMLã‚’å®Ÿè·µã§ãã‚‹</li>
</ul>

<hr>

<h2>2.1 MAMLã®åŸç†</h2>

<h3>Model-Agnostic Meta-Learningæ¦‚è¦</h3>

<p><strong>MAML (Model-Agnostic Meta-Learning)</strong>ã¯ã€Chelsea Finnã‚‰ã«ã‚ˆã£ã¦2017å¹´ã«ææ¡ˆã•ã‚ŒãŸå‹¾é…ãƒ™ãƒ¼ã‚¹ã®ãƒ¡ã‚¿å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚</p>

<blockquote>
<p>ã€Œå°‘æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å‹¾é…æ›´æ–°ã§ç´ æ—©ãé©å¿œã§ãã‚‹ã€è‰¯ã„åˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã™ã‚‹ã€</p>
</blockquote>

<h3>MAMLã®æ ¸å¿ƒçš„ã‚¢ã‚¤ãƒ‡ã‚¢</h3>

<p>MAMLã¯ä»¥ä¸‹ã®å•ã„ã«ç­”ãˆã¾ã™ï¼š</p>
<ul>
<li><strong>å•ã„</strong>ï¼šã©ã®ã‚ˆã†ãªåˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’é¸ã¹ã°ã€æ–°ã—ã„ã‚¿ã‚¹ã‚¯ $\mathcal{T}_i$ ã«å¯¾ã—ã¦ã€ã‚ãšã‹ãªå‹¾é…ã‚¹ãƒ†ãƒƒãƒ—ã§é«˜ã„æ€§èƒ½ã‚’é”æˆã§ãã‚‹ã‹ï¼Ÿ</li>
<li><strong>ç­”ãˆ</strong>ï¼šè¤‡æ•°ã®ã‚¿ã‚¹ã‚¯ã§ã€Œå‹¾é…é™ä¸‹å¾Œã®æ€§èƒ½ã€ã‚’æœ€å¤§åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã™ã‚‹</li>
</ul>

<h3>äºŒæ®µéšæœ€é©åŒ–ï¼ˆInner/Outer Loopï¼‰</h3>

<p>MAMLã¯ä»¥ä¸‹ã®2ã¤ã®ãƒ«ãƒ¼ãƒ—ã‹ã‚‰ãªã‚Šã¾ã™ï¼š</p>

<table>
<thead>
<tr>
<th>ãƒ«ãƒ¼ãƒ—</th>
<th>ç›®çš„</th>
<th>ãƒ‡ãƒ¼ã‚¿</th>
<th>æ›´æ–°å¯¾è±¡</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Inner Loop</strong></td>
<td>ã‚¿ã‚¹ã‚¯é©å¿œ</td>
<td>ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆ $\mathcal{D}^{tr}_i$</td>
<td>ã‚¿ã‚¹ã‚¯å›ºæœ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta'_i$</td>
</tr>
<tr>
<td><strong>Outer Loop</strong></td>
<td>ãƒ¡ã‚¿å­¦ç¿’</td>
<td>ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆ $\mathcal{D}^{test}_i$</td>
<td>ãƒ¡ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$</td>
</tr>
</tbody>
</table>

<h3>MAMLã®å‹•ä½œãƒ•ãƒ­ãƒ¼</h3>

<div class="mermaid">
graph TD
    A[ãƒ¡ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ Î¸] --> B[ã‚¿ã‚¹ã‚¯1: Î¸ â†’ Î¸'â‚]
    A --> C[ã‚¿ã‚¹ã‚¯2: Î¸ â†’ Î¸'â‚‚]
    A --> D[ã‚¿ã‚¹ã‚¯N: Î¸ â†’ Î¸'â‚™]

    B --> E[ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆã§è©•ä¾¡ Lâ‚]
    C --> F[ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆã§è©•ä¾¡ Lâ‚‚]
    D --> G[ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆã§è©•ä¾¡ Lâ‚™]

    E --> H[ãƒ¡ã‚¿æå¤± = å¹³å‡]
    F --> H
    G --> H

    H --> I[Î¸ã‚’æ›´æ–°]
    I --> A

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#fff3e0
    style D fill:#fff3e0
    style H fill:#ffebee
    style I fill:#c8e6c9
</div>

<h3>å‹¾é…ã®å‹¾é…ï¼ˆSecond-order derivativesï¼‰</h3>

<p>MAMLã®ç‰¹å¾´ã¯<strong>å‹¾é…ã®å‹¾é…</strong>ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã§ã™ã€‚</p>

<p><strong>Inner Loopï¼ˆä¸€æ¬¡å‹¾é…ï¼‰</strong>ï¼š</p>
<p>$$
\theta'_i = \theta - \alpha \nabla_\theta \mathcal{L}_{\mathcal{T}_i}^{tr}(f_\theta)
$$</p>

<p><strong>Outer Loopï¼ˆäºŒæ¬¡å‹¾é…ï¼‰</strong>ï¼š</p>
<p>$$
\theta \leftarrow \theta - \beta \nabla_\theta \sum_{\mathcal{T}_i \sim p(\mathcal{T})} \mathcal{L}_{\mathcal{T}_i}^{test}(f_{\theta'_i})
$$</p>

<p>ã“ã“ã§ã€$\theta'_i$ ã¯ $\theta$ ã®é–¢æ•°ãªã®ã§ã€ä»¥ä¸‹ã®ã‚ˆã†ã«å±•é–‹ã•ã‚Œã¾ã™ï¼š</p>

<p>$$
\nabla_\theta \mathcal{L}_{\mathcal{T}_i}^{test}(f_{\theta'_i}) = \nabla_{\theta'_i} \mathcal{L}_{\mathcal{T}_i}^{test}(f_{\theta'_i}) \cdot \nabla_\theta \theta'_i
$$</p>

<blockquote>
<p><strong>é‡è¦</strong>ï¼šã“ã®äºŒæ¬¡å¾®åˆ†ãŒè¨ˆç®—ã‚³ã‚¹ãƒˆã®ä¸»ãªè¦å› ã§ã™ãŒã€é©å¿œèƒ½åŠ›ã‚’é«˜ã‚ã¾ã™ã€‚</p>
</blockquote>

<h3>MAMLã®è¦–è¦šçš„ç†è§£</h3>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# 2æ¬¡å…ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã§ã®MAMLã®ã‚¤ãƒ¡ãƒ¼ã‚¸
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# å·¦å›³ï¼šé€šå¸¸ã®å­¦ç¿’
ax1 = axes[0]
theta_init = np.array([0, 0])
task1_opt = np.array([3, 1])
task2_opt = np.array([1, 3])
task3_opt = np.array([-2, 2])

ax1.scatter(*theta_init, s=200, c='red', marker='X', label='ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–', zorder=5)
ax1.scatter(*task1_opt, s=100, c='blue', marker='o', alpha=0.7)
ax1.scatter(*task2_opt, s=100, c='blue', marker='o', alpha=0.7)
ax1.scatter(*task3_opt, s=100, c='blue', marker='o', alpha=0.7, label='ã‚¿ã‚¹ã‚¯æœ€é©è§£')

for opt in [task1_opt, task2_opt, task3_opt]:
    ax1.arrow(theta_init[0], theta_init[1],
              opt[0]-theta_init[0]*0.9, opt[1]-theta_init[1]*0.9,
              head_width=0.2, head_length=0.2, fc='gray', ec='gray', alpha=0.5)

ax1.set_xlim(-4, 4)
ax1.set_ylim(-1, 4)
ax1.set_xlabel('Î¸â‚')
ax1.set_ylabel('Î¸â‚‚')
ax1.set_title('é€šå¸¸ã®å­¦ç¿’ï¼šã‚¿ã‚¹ã‚¯ã”ã¨ã«ã‚¼ãƒ­ã‹ã‚‰å­¦ç¿’', fontsize=13)
ax1.legend()
ax1.grid(True, alpha=0.3)

# å³å›³ï¼šMAML
ax2 = axes[1]
maml_init = np.array([0.7, 2])

ax2.scatter(*maml_init, s=200, c='green', marker='X', label='MAMLåˆæœŸåŒ–', zorder=5)
ax2.scatter(*task1_opt, s=100, c='blue', marker='o', alpha=0.7)
ax2.scatter(*task2_opt, s=100, c='blue', marker='o', alpha=0.7)
ax2.scatter(*task3_opt, s=100, c='blue', marker='o', alpha=0.7, label='ã‚¿ã‚¹ã‚¯æœ€é©è§£')

for opt in [task1_opt, task2_opt, task3_opt]:
    ax2.arrow(maml_init[0], maml_init[1],
              opt[0]-maml_init[0]*0.7, opt[1]-maml_init[1]*0.7,
              head_width=0.2, head_length=0.2, fc='green', ec='green', alpha=0.5)

# ä¸­å¿ƒé ˜åŸŸã‚’å¼·èª¿
circle = plt.Circle(maml_init, 1.5, color='green', fill=False, linestyle='--', linewidth=2)
ax2.add_patch(circle)

ax2.set_xlim(-4, 4)
ax2.set_ylim(-1, 4)
ax2.set_xlabel('Î¸â‚')
ax2.set_ylabel('Î¸â‚‚')
ax2.set_title('MAMLï¼šå…¨ã‚¿ã‚¹ã‚¯ã«è¿‘ã„ä½ç½®ã‹ã‚‰é–‹å§‹', fontsize=13)
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== MAML ã®ç›´æ„Ÿçš„ç†è§£ ===")
print("âœ“ é€šå¸¸ã®å­¦ç¿’: å„ã‚¿ã‚¹ã‚¯ã‚’ã‚¼ãƒ­ã‹ã‚‰å­¦ç¿’ï¼ˆé ã„ï¼‰")
print("âœ“ MAML: å…¨ã‚¿ã‚¹ã‚¯ã®ã€Œä¸­å¿ƒã€ã«ä½ç½®ã™ã‚‹åˆæœŸåŒ–ã‚’å­¦ç¿’")
print("âœ“ çµæœ: ã‚ãšã‹ãªå‹¾é…ã‚¹ãƒ†ãƒƒãƒ—ã§å„ã‚¿ã‚¹ã‚¯ã«é©å¿œå¯èƒ½")
</code></pre>

<hr>

<h2>2.2 MAMLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h2>

<h3>æ•°å¼ã«ã‚ˆã‚‹å®šç¾©</h3>

<p><strong>ãƒ¡ã‚¿å­¦ç¿’ã®ç›®çš„</strong>ï¼š</p>

<p>$$
\min_\theta \sum_{\mathcal{T}_i \sim p(\mathcal{T})} \mathcal{L}_{\mathcal{T}_i}^{test}(f_{\theta'_i})
$$</p>

<p>ã“ã“ã§ã€$\theta'_i$ ã¯ã‚¿ã‚¹ã‚¯ $\mathcal{T}_i$ ã«å¯¾ã™ã‚‹é©å¿œå¾Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼š</p>

<p>$$
\theta'_i = \theta - \alpha \nabla_\theta \mathcal{L}_{\mathcal{T}_i}^{tr}(f_\theta)
$$</p>

<p><strong>è¨˜å·ã®æ„å‘³</strong>ï¼š</p>
<ul>
<li>$\theta$: ãƒ¡ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’å¯¾è±¡ï¼‰</li>
<li>$\theta'_i$: ã‚¿ã‚¹ã‚¯ $i$ ã«é©å¿œã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
<li>$\alpha$: Inner Loopå­¦ç¿’ç‡ï¼ˆã‚¿ã‚¹ã‚¯é©å¿œï¼‰</li>
<li>$\beta$: Outer Loopå­¦ç¿’ç‡ï¼ˆãƒ¡ã‚¿å­¦ç¿’ï¼‰</li>
<li>$\mathcal{L}_{\mathcal{T}_i}^{tr}$: ã‚¿ã‚¹ã‚¯ $i$ ã®ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆæå¤±</li>
<li>$\mathcal{L}_{\mathcal{T}_i}^{test}$: ã‚¿ã‚¹ã‚¯ $i$ ã®ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆæå¤±</li>
</ul>

<h3>Inner Loop: ã‚¿ã‚¹ã‚¯é©å¿œ</h3>

<p>å„ã‚¿ã‚¹ã‚¯ $\mathcal{T}_i$ ã«å¯¾ã—ã¦ã€ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆ $\mathcal{D}^{tr}_i$ ã‚’ä½¿ã£ã¦å‹¾é…é™ä¸‹ï¼š</p>

<p>$$
\theta'_i = \theta - \alpha \nabla_\theta \mathcal{L}_{\mathcal{T}_i}^{tr}(f_\theta)
$$</p>

<p>è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®å ´åˆï¼ˆ$K$ ã‚¹ãƒ†ãƒƒãƒ—ï¼‰ï¼š</p>

<p>$$
\begin{aligned}
\theta_i^{(0)} &= \theta \\
\theta_i^{(k+1)} &= \theta_i^{(k)} - \alpha \nabla_{\theta_i^{(k)}} \mathcal{L}_{\mathcal{T}_i}^{tr}(f_{\theta_i^{(k)}}) \\
\theta'_i &= \theta_i^{(K)}
\end{aligned}
$$</p>

<h3>Outer Loop: ãƒ¡ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°</h3>

<p>é©å¿œå¾Œã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta'_i$ ã§ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆ $\mathcal{D}^{test}_i$ ã‚’è©•ä¾¡ã—ã€ãƒ¡ã‚¿æå¤±ã‚’è¨ˆç®—ï¼š</p>

<p>$$
\mathcal{L}_{\text{meta}}(\theta) = \frac{1}{N} \sum_{i=1}^N \mathcal{L}_{\mathcal{T}_i}^{test}(f_{\theta'_i})
$$</p>

<p>ãƒ¡ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´æ–°ï¼š</p>

<p>$$
\theta \leftarrow \theta - \beta \nabla_\theta \mathcal{L}_{\text{meta}}(\theta)
$$</p>

<h3>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ“¬ä¼¼ã‚³ãƒ¼ãƒ‰</h3>

<pre><code>Algorithm: MAML

Require: p(T): ã‚¿ã‚¹ã‚¯åˆ†å¸ƒ
Require: Î±, Î²: Inner/Outer Loop å­¦ç¿’ç‡

1: ãƒ©ãƒ³ãƒ€ãƒ ã« Î¸ ã‚’åˆæœŸåŒ–
2: while not converged do
3:     B â† Sample batch of tasks {T_i} ~ p(T)
4:     for all T_i âˆˆ B do
5:         # Inner Loop: ã‚¿ã‚¹ã‚¯é©å¿œ
6:         D_i^tr, D_i^test â† Sample support/query sets from T_i
7:         Î¸'_i â† Î¸ - Î± âˆ‡_Î¸ L_{T_i}^tr(f_Î¸)
8:
9:         # Query ã‚»ãƒƒãƒˆã§æå¤±ã‚’è¨ˆç®—
10:        L_i â† L_{T_i}^test(f_{Î¸'_i})
11:    end for
12:
13:    # Outer Loop: ãƒ¡ã‚¿å­¦ç¿’
14:    Î¸ â† Î¸ - Î² âˆ‡_Î¸ Î£ L_i
15: end while
16: return Î¸
</code></pre>

<h3>First-order MAML (FOMAML)</h3>

<p><strong>èª²é¡Œ</strong>ï¼šäºŒæ¬¡å¾®åˆ†ã®è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„</p>

<p><strong>è§£æ±ºç­–</strong>ï¼šäºŒæ¬¡å¾®åˆ†é …ã‚’ç„¡è¦–ã™ã‚‹è¿‘ä¼¼</p>

<p>FOMAML ã§ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«è¿‘ä¼¼ã—ã¾ã™ï¼š</p>

<p>$$
\nabla_\theta \mathcal{L}_{\mathcal{T}_i}^{test}(f_{\theta'_i}) \approx \nabla_{\theta'_i} \mathcal{L}_{\mathcal{T}_i}^{test}(f_{\theta'_i})
$$</p>

<p>ã¤ã¾ã‚Šã€$\nabla_\theta \theta'_i$ ã®é …ã‚’ç„¡è¦–ã—ã¾ã™ã€‚</p>

<table>
<thead>
<tr>
<th>æ¯”è¼ƒé …ç›®</th>
<th>MAML</th>
<th>FOMAML</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å‹¾é…è¨ˆç®—</strong></td>
<td>äºŒæ¬¡å¾®åˆ†</td>
<td>ä¸€æ¬¡å¾®åˆ†ã®ã¿</td>
</tr>
<tr>
<td><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆ</strong></td>
<td>é«˜ã„</td>
<td>ä½ã„ï¼ˆç´„50%å‰Šæ¸›ï¼‰</td>
</tr>
<tr>
<td><strong>ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡</strong></td>
<td>å¤šã„</td>
<td>å°‘ãªã„</td>
</tr>
<tr>
<td><strong>æ€§èƒ½</strong></td>
<td>æœ€é«˜</td>
<td>ã‚ãšã‹ã«åŠ£ã‚‹ï¼ˆå®Ÿç”¨çš„ï¼‰</td>
</tr>
</tbody>
</table>

<blockquote>
<p><strong>å®Ÿè·µçš„ã«ã¯</strong>ã€FOMAMLã§ååˆ†ãªæ€§èƒ½ãŒå¾—ã‚‰ã‚Œã‚‹ã“ã¨ãŒå¤šãã€åºƒãä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>2.3 PyTorchã«ã‚ˆã‚‹MAMLå®Ÿè£…</h2>

<h3>higher ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æ´»ç”¨</h3>

<p><strong>higher</strong> ã¯ã€PyTorchã§é«˜æ¬¡å¾®åˆ†ã‚’æ‰±ã†ãŸã‚ã®ä¾¿åˆ©ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚MAMLã®å®Ÿè£…ã«æœ€é©ã§ã™ã€‚</p>

<pre><code class="language-python"># higherã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# pip install higher

import torch
import torch.nn as nn
import torch.optim as optim
import higher
import numpy as np

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
</code></pre>

<h3>ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«å®šç¾©</h3>

<pre><code class="language-python">class SimpleMLP(nn.Module):
    """Few-Shotå­¦ç¿’ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãªMLP"""
    def __init__(self, input_size=1, hidden_size=40, output_size=1):
        super(SimpleMLP, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, output_size)
        )

    def forward(self, x):
        return self.net(x)

# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
model = SimpleMLP().to(device)
print(f"Model parameters: {sum(p.numel() for p in model.parameters())}")
</code></pre>

<h3>ã‚¿ã‚¹ã‚¯ç”Ÿæˆé–¢æ•°</h3>

<pre><code class="language-python">def generate_sinusoid_task(amplitude=None, phase=None, n_samples=10):
    """
    æ­£å¼¦æ³¢å›å¸°ã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆ

    Args:
        amplitude: æŒ¯å¹…ï¼ˆNoneã®å ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ ï¼‰
        phase: ä½ç›¸ï¼ˆNoneã®å ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ ï¼‰
        n_samples: ã‚µãƒ³ãƒ—ãƒ«æ•°

    Returns:
        x, y: å…¥åŠ›ã¨å‡ºåŠ›ã®ãƒšã‚¢
    """
    if amplitude is None:
        amplitude = np.random.uniform(0.1, 5.0)
    if phase is None:
        phase = np.random.uniform(0, np.pi)

    x = np.random.uniform(-5, 5, n_samples)
    y = amplitude * np.sin(x + phase)

    x = torch.FloatTensor(x).unsqueeze(1).to(device)
    y = torch.FloatTensor(y).unsqueeze(1).to(device)

    return x, y, amplitude, phase

# ã‚¿ã‚¹ã‚¯ä¾‹ã®å¯è¦–åŒ–
import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 3, figsize=(15, 8))
for i, ax in enumerate(axes.flat):
    x_train, y_train, amp, ph = generate_sinusoid_task(n_samples=10)
    x_test = torch.linspace(-5, 5, 100).unsqueeze(1).to(device)
    y_test = amp * np.sin(x_test.cpu().numpy() + ph)

    ax.scatter(x_train.cpu(), y_train.cpu(), label='Training samples', s=50, alpha=0.7)
    ax.plot(x_test.cpu(), y_test, 'r--', label='True function', alpha=0.5)
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_title(f'Task {i+1}: A={amp:.2f}, Ï†={ph:.2f}', fontsize=11)
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== ã‚¿ã‚¹ã‚¯åˆ†å¸ƒ ===")
print("å„ã‚¿ã‚¹ã‚¯ã¯ç•°ãªã‚‹æŒ¯å¹…ã¨ä½ç›¸ã‚’æŒã¤æ­£å¼¦æ³¢")
print("ç›®æ¨™: å°‘æ•°ã®ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰æ–°ã—ã„æ­£å¼¦æ³¢ã«é©å¿œ")
</code></pre>

<h3>Inner Loopå®Ÿè£…</h3>

<pre><code class="language-python">def inner_loop(model, x_support, y_support, inner_lr=0.01, inner_steps=1):
    """
    Inner Loop: ã‚¿ã‚¹ã‚¯é©å¿œ

    Args:
        model: PyTorchãƒ¢ãƒ‡ãƒ«
        x_support: ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆå…¥åŠ›
        y_support: ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆå‡ºåŠ›
        inner_lr: Inner Loopå­¦ç¿’ç‡
        inner_steps: é©å¿œã‚¹ãƒ†ãƒƒãƒ—æ•°

    Returns:
        task_loss: ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆæå¤±
    """
    criterion = nn.MSELoss()

    # äºˆæ¸¬ã¨æå¤±è¨ˆç®—
    predictions = model(x_support)
    task_loss = criterion(predictions, y_support)

    # å‹¾é…è¨ˆç®—
    task_grad = torch.autograd.grad(
        task_loss,
        model.parameters(),
        create_graph=True  # äºŒæ¬¡å¾®åˆ†ã®ãŸã‚ã«å¿…è¦
    )

    # æ‰‹å‹•ã§å‹¾é…é™ä¸‹ï¼ˆinner_stepsãŒ1ã®å ´åˆï¼‰
    adapted_params = []
    for param, grad in zip(model.parameters(), task_grad):
        adapted_params.append(param - inner_lr * grad)

    return task_loss, adapted_params

# Inner Loop ã®å‹•ä½œç¢ºèª
print("\n=== Inner Loop ãƒ†ã‚¹ãƒˆ ===")
x_sup, y_sup, _, _ = generate_sinusoid_task(n_samples=5)
loss, adapted = inner_loop(model, x_sup, y_sup)
print(f"Support loss: {loss.item():.4f}")
print(f"Adapted parameters: {len(adapted)} tensors")
</code></pre>

<h3>Outer Loopå®Ÿè£…</h3>

<pre><code class="language-python">def outer_loop(model, tasks, inner_lr=0.01, inner_steps=1):
    """
    Outer Loop: ãƒ¡ã‚¿å­¦ç¿’

    Args:
        model: PyTorchãƒ¢ãƒ‡ãƒ«
        tasks: ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆ [(x_sup, y_sup, x_qry, y_qry), ...]
        inner_lr: Inner Loopå­¦ç¿’ç‡
        inner_steps: é©å¿œã‚¹ãƒ†ãƒƒãƒ—æ•°

    Returns:
        meta_loss: ãƒ¡ã‚¿æå¤±ï¼ˆå¹³å‡ã‚¯ã‚¨ãƒªæå¤±ï¼‰
    """
    criterion = nn.MSELoss()
    meta_loss = 0.0

    for x_support, y_support, x_query, y_query in tasks:
        # Inner Loop: ã‚¿ã‚¹ã‚¯é©å¿œï¼ˆhigherã‚’ä½¿ç”¨ï¼‰
        with higher.innerloop_ctx(
            model,
            optim.SGD(model.parameters(), lr=inner_lr),
            copy_initial_weights=False
        ) as (fmodel, diffopt):

            # Inner Loopæ›´æ–°
            for _ in range(inner_steps):
                support_loss = criterion(fmodel(x_support), y_support)
                diffopt.step(support_loss)

            # ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆã§è©•ä¾¡
            query_pred = fmodel(x_query)
            query_loss = criterion(query_pred, y_query)

            meta_loss += query_loss

    # ã‚¿ã‚¹ã‚¯æ•°ã§å¹³å‡
    meta_loss = meta_loss / len(tasks)

    return meta_loss

# Outer Loop ã®å‹•ä½œç¢ºèª
print("\n=== Outer Loop ãƒ†ã‚¹ãƒˆ ===")
test_tasks = []
for _ in range(4):
    x_s, y_s, _, _ = generate_sinusoid_task(n_samples=5)
    x_q, y_q, _, _ = generate_sinusoid_task(n_samples=10)
    test_tasks.append((x_s, y_s, x_q, y_q))

meta_loss = outer_loop(model, test_tasks)
print(f"Meta loss: {meta_loss.item():.4f}")
</code></pre>

<h3>ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å­¦ç¿’ãƒ«ãƒ¼ãƒ—</h3>

<pre><code class="language-python">def train_maml(model, n_iterations=10000, tasks_per_batch=4,
               k_shot=5, q_query=10, inner_lr=0.01, outer_lr=0.001,
               inner_steps=1, eval_interval=500):
    """
    MAMLå­¦ç¿’ãƒ«ãƒ¼ãƒ—

    Args:
        model: PyTorchãƒ¢ãƒ‡ãƒ«
        n_iterations: å­¦ç¿’ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°
        tasks_per_batch: ãƒãƒƒãƒã‚ãŸã‚Šã®ã‚¿ã‚¹ã‚¯æ•°
        k_shot: ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒ«æ•°
        q_query: ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒ«æ•°
        inner_lr: Inner Loopå­¦ç¿’ç‡
        outer_lr: Outer Loopå­¦ç¿’ç‡
        inner_steps: Inner Loopæ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—æ•°
        eval_interval: è©•ä¾¡é–“éš”

    Returns:
        losses: æå¤±å±¥æ­´
    """
    meta_optimizer = optim.Adam(model.parameters(), lr=outer_lr)
    criterion = nn.MSELoss()

    losses = []

    for iteration in range(n_iterations):
        meta_optimizer.zero_grad()

        # ã‚¿ã‚¹ã‚¯ãƒãƒƒãƒã®ç”Ÿæˆ
        tasks = []
        for _ in range(tasks_per_batch):
            # ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆ
            x_support, y_support, amp, phase = generate_sinusoid_task(n_samples=k_shot)
            # ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆï¼ˆåŒã˜ã‚¿ã‚¹ã‚¯ã‹ã‚‰ï¼‰
            x_query, y_query, _, _ = generate_sinusoid_task(
                amplitude=amp, phase=phase, n_samples=q_query
            )
            tasks.append((x_support, y_support, x_query, y_query))

        # Outer Loop
        meta_loss = outer_loop(model, tasks, inner_lr, inner_steps)

        # ãƒ¡ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
        meta_loss.backward()
        meta_optimizer.step()

        losses.append(meta_loss.item())

        # å®šæœŸçš„ãªè©•ä¾¡
        if (iteration + 1) % eval_interval == 0:
            print(f"Iteration {iteration+1}/{n_iterations}, Meta Loss: {meta_loss.item():.4f}")

    return losses

# MAMLå­¦ç¿’ã®å®Ÿè¡Œ
print("\n=== MAML Training ===")
model = SimpleMLP().to(device)

losses = train_maml(
    model,
    n_iterations=5000,
    tasks_per_batch=4,
    k_shot=5,
    q_query=10,
    inner_lr=0.01,
    outer_lr=0.001,
    inner_steps=1,
    eval_interval=1000
)

# æå¤±æ›²ç·šã®å¯è¦–åŒ–
plt.figure(figsize=(10, 5))
plt.plot(losses, alpha=0.7)
plt.xlabel('Iteration')
plt.ylabel('Meta Loss')
plt.title('MAML Training Progress', fontsize=14)
plt.grid(True, alpha=0.3)
plt.yscale('log')
plt.show()

print("\nâœ“ MAMLå­¦ç¿’å®Œäº†")
print(f"âœ“ æœ€çµ‚ãƒ¡ã‚¿æå¤±: {losses[-1]:.4f}")
</code></pre>

<h3>å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡</h3>

<pre><code class="language-python">def evaluate_maml(model, n_test_tasks=5, k_shot=5, inner_lr=0.01, inner_steps=5):
    """
    å­¦ç¿’æ¸ˆã¿MAMLãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡

    Args:
        model: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        n_test_tasks: ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯æ•°
        k_shot: ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒ«æ•°
        inner_lr: é©å¿œæ™‚ã®å­¦ç¿’ç‡
        inner_steps: é©å¿œã‚¹ãƒ†ãƒƒãƒ—æ•°
    """
    criterion = nn.MSELoss()

    fig, axes = plt.subplots(2, 3, figsize=(16, 10))

    for idx, ax in enumerate(axes.flat[:n_test_tasks]):
        # æ–°ã—ã„ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯ç”Ÿæˆ
        x_support, y_support, amp, phase = generate_sinusoid_task(n_samples=k_shot)
        x_test = torch.linspace(-5, 5, 100).unsqueeze(1).to(device)
        y_test = amp * np.sin(x_test.cpu().numpy() + phase)

        # é©å¿œå‰ã®äºˆæ¸¬
        with torch.no_grad():
            y_pred_before = model(x_test).cpu().numpy()

        # Inner Loop: ã‚¿ã‚¹ã‚¯é©å¿œ
        adapted_model = SimpleMLP().to(device)
        adapted_model.load_state_dict(model.state_dict())
        optimizer = optim.SGD(adapted_model.parameters(), lr=inner_lr)

        for step in range(inner_steps):
            optimizer.zero_grad()
            loss = criterion(adapted_model(x_support), y_support)
            loss.backward()
            optimizer.step()

        # é©å¿œå¾Œã®äºˆæ¸¬
        with torch.no_grad():
            y_pred_after = adapted_model(x_test).cpu().numpy()

        # å¯è¦–åŒ–
        ax.scatter(x_support.cpu(), y_support.cpu(),
                  label=f'{k_shot}-shot support', s=80, zorder=3, color='red')
        ax.plot(x_test.cpu(), y_test, 'k--',
               label='True function', linewidth=2, alpha=0.7)
        ax.plot(x_test.cpu(), y_pred_before, 'b-',
               label='Before adaptation', alpha=0.5, linewidth=2)
        ax.plot(x_test.cpu(), y_pred_after, 'g-',
               label=f'After {inner_steps} steps', linewidth=2)

        ax.set_xlabel('x')
        ax.set_ylabel('y')
        ax.set_title(f'Test Task {idx+1}', fontsize=12)
        ax.legend(fontsize=9)
        ax.grid(True, alpha=0.3)

    # æœ€å¾Œã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’å‰Šé™¤
    if n_test_tasks < 6:
        fig.delaxes(axes.flat[5])

    plt.tight_layout()
    plt.show()

    print("\n=== MAML Evaluation ===")
    print(f"âœ“ {n_test_tasks}å€‹ã®æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã§ãƒ†ã‚¹ãƒˆ")
    print(f"âœ“ {k_shot}-shot learning with {inner_steps} gradient steps")
    print("âœ“ é’ç·š: é©å¿œå‰ï¼ˆãƒ¡ã‚¿å­¦ç¿’ã®åˆæœŸåŒ–ï¼‰")
    print("âœ“ ç·‘ç·š: é©å¿œå¾Œï¼ˆã‚ãšã‹ãªãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ï¼‰")

# è©•ä¾¡ã®å®Ÿè¡Œ
evaluate_maml(model, n_test_tasks=5, k_shot=5, inner_steps=5)
</code></pre>

<hr>

<h2>2.4 Reptileã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h2>

<h3>MAMLã®ç°¡ç•¥ç‰ˆ</h3>

<p><strong>Reptile</strong>ã¯ã€OpenAIã«ã‚ˆã£ã¦ææ¡ˆã•ã‚ŒãŸMAMLã®ç°¡æ˜“ç‰ˆã§ã™ã€‚</p>

<blockquote>
<p>ã€ŒäºŒæ¬¡å¾®åˆ†ã‚’ä½¿ã‚ãšã«ã€ä¸€æ¬¡å¾®åˆ†ã®ã¿ã§ãƒ¡ã‚¿å­¦ç¿’ã‚’å®Ÿç¾ã€</p>
</blockquote>

<h3>MAMLã¨Reptileã®é•ã„</h3>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>MAML</th>
<th>Reptile</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å‹¾é…è¨ˆç®—</strong></td>
<td>äºŒæ¬¡å¾®åˆ†ï¼ˆå‹¾é…ã®å‹¾é…ï¼‰</td>
<td>ä¸€æ¬¡å¾®åˆ†ã®ã¿</td>
</tr>
<tr>
<td><strong>æ›´æ–°å¼</strong></td>
<td>$\theta \leftarrow \theta - \beta \nabla_\theta \mathcal{L}_{\text{meta}}$</td>
<td>$\theta \leftarrow \theta + \epsilon (\theta' - \theta)$</td>
</tr>
<tr>
<td><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆ</strong></td>
<td>é«˜ã„</td>
<td>ä½ã„ï¼ˆç´„70%å‰Šæ¸›ï¼‰</td>
</tr>
<tr>
<td><strong>å®Ÿè£…ã®ç°¡å˜ã•</strong></td>
<td>è¤‡é›‘ï¼ˆhigherãŒå¿…è¦ï¼‰</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«</td>
</tr>
<tr>
<td><strong>æ€§èƒ½</strong></td>
<td>ç†è«–çš„ã«æœ€é©</td>
<td>å®Ÿç”¨çš„ã«ååˆ†</td>
</tr>
</tbody>
</table>

<h3>Reptileã®æ›´æ–°å¼</h3>

<p>Reptileã¯ä»¥ä¸‹ã®ã‚·ãƒ³ãƒ—ãƒ«ãªæ›´æ–°ã‚’è¡Œã„ã¾ã™ï¼š</p>

<p>$$
\theta \leftarrow \theta + \epsilon (\theta'_i - \theta)
$$</p>

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$\theta$: ãƒ¡ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
<li>$\theta'_i$: ã‚¿ã‚¹ã‚¯ $i$ ã§ $K$ ã‚¹ãƒ†ãƒƒãƒ—å­¦ç¿’å¾Œã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
<li>$\epsilon$: ãƒ¡ã‚¿å­¦ç¿’ç‡</li>
</ul>

<p><strong>ç›´æ„Ÿçš„ç†è§£</strong>ï¼šé©å¿œå¾Œã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ–¹å‘ã«ãƒ¡ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç§»å‹•</p>

<h3>Reptileã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h3>

<pre><code>Algorithm: Reptile

Require: p(T): ã‚¿ã‚¹ã‚¯åˆ†å¸ƒ
Require: Î±: Inner learning rate
Require: Îµ: Meta learning rate (Outer)

1: ãƒ©ãƒ³ãƒ€ãƒ ã« Î¸ ã‚’åˆæœŸåŒ–
2: while not converged do
3:     T_i ~ p(T)  # ã‚¿ã‚¹ã‚¯ã‚’ã‚µãƒ³ãƒ—ãƒ«
4:     D_i â† Sample data from T_i
5:
6:     # ã‚¿ã‚¹ã‚¯ã§é€šå¸¸ã®å­¦ç¿’
7:     Î¸' â† Î¸
8:     for k = 1 to K do
9:         Î¸' â† Î¸' - Î± âˆ‡_{Î¸'} L_{T_i}(f_{Î¸'})
10:    end for
11:
12:    # ãƒ¡ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é©å¿œæ–¹å‘ã«ç§»å‹•
13:    Î¸ â† Î¸ + Îµ(Î¸' - Î¸)
14: end while
15: return Î¸
</code></pre>

<h3>Reptileå®Ÿè£…</h3>

<pre><code class="language-python">def train_reptile(model, n_iterations=5000, k_shot=10,
                  inner_lr=0.01, meta_lr=0.1, inner_steps=5,
                  eval_interval=500):
    """
    Reptile ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

    Args:
        model: PyTorchãƒ¢ãƒ‡ãƒ«
        n_iterations: å­¦ç¿’ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°
        k_shot: ã‚¿ã‚¹ã‚¯ã”ã¨ã®ã‚µãƒ³ãƒ—ãƒ«æ•°
        inner_lr: Inner Loopå­¦ç¿’ç‡
        meta_lr: ãƒ¡ã‚¿å­¦ç¿’ç‡
        inner_steps: ã‚¿ã‚¹ã‚¯ã”ã¨ã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°
        eval_interval: è©•ä¾¡é–“éš”

    Returns:
        losses: æå¤±å±¥æ­´
    """
    criterion = nn.MSELoss()
    losses = []

    for iteration in range(n_iterations):
        # ãƒ¡ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼
        meta_params = [p.clone() for p in model.parameters()]

        # æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’ã‚µãƒ³ãƒ—ãƒ«
        x_task, y_task, _, _ = generate_sinusoid_task(n_samples=k_shot)

        # ã‚¿ã‚¹ã‚¯ã§é€šå¸¸ã®å­¦ç¿’ï¼ˆInner Loopï¼‰
        optimizer = optim.SGD(model.parameters(), lr=inner_lr)

        for step in range(inner_steps):
            optimizer.zero_grad()
            predictions = model(x_task)
            loss = criterion(predictions, y_task)
            loss.backward()
            optimizer.step()

        losses.append(loss.item())

        # ãƒ¡ã‚¿æ›´æ–°: Î¸ â† Î¸ + Îµ(Î¸' - Î¸)
        with torch.no_grad():
            for meta_param, task_param in zip(meta_params, model.parameters()):
                meta_param.add_(task_param - meta_param, alpha=meta_lr)

            # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°
            for param, meta_param in zip(model.parameters(), meta_params):
                param.copy_(meta_param)

        # å®šæœŸçš„ãªè©•ä¾¡
        if (iteration + 1) % eval_interval == 0:
            print(f"Iteration {iteration+1}/{n_iterations}, Loss: {loss.item():.4f}")

    return losses

# Reptileå­¦ç¿’ã®å®Ÿè¡Œ
print("\n=== Reptile Training ===")
reptile_model = SimpleMLP().to(device)

reptile_losses = train_reptile(
    reptile_model,
    n_iterations=5000,
    k_shot=10,
    inner_lr=0.01,
    meta_lr=0.1,
    inner_steps=5,
    eval_interval=1000
)

# æå¤±æ›²ç·šã®å¯è¦–åŒ–
plt.figure(figsize=(10, 5))
plt.plot(reptile_losses, alpha=0.7, color='purple')
plt.xlabel('Iteration')
plt.ylabel('Task Loss')
plt.title('Reptile Training Progress', fontsize=14)
plt.grid(True, alpha=0.3)
plt.yscale('log')
plt.show()

print("\nâœ“ Reptileå­¦ç¿’å®Œäº†")
print(f"âœ“ æœ€çµ‚æå¤±: {reptile_losses[-1]:.4f}")
</code></pre>

<h3>MAMLã¨Reptileã®æ¯”è¼ƒ</h3>

<pre><code class="language-python"># MAMLã¨Reptileã®æ€§èƒ½æ¯”è¼ƒ
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# æå¤±æ›²ç·šã®æ¯”è¼ƒ
ax1 = axes[0]
ax1.plot(losses, label='MAML', alpha=0.7, linewidth=2)
ax1.plot(reptile_losses, label='Reptile', alpha=0.7, linewidth=2)
ax1.set_xlabel('Iteration')
ax1.set_ylabel('Loss')
ax1.set_title('Training Loss Comparison', fontsize=14)
ax1.legend()
ax1.grid(True, alpha=0.3)
ax1.set_yscale('log')

# é©å¿œé€Ÿåº¦ã®æ¯”è¼ƒ
ax2 = axes[1]

# ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯ç”Ÿæˆ
x_support, y_support, amp, phase = generate_sinusoid_task(n_samples=5)
x_test = torch.linspace(-5, 5, 100).unsqueeze(1).to(device)
y_test = amp * np.sin(x_test.cpu().numpy() + phase)

# MAMLé©å¿œ
maml_errors = []
adapted_maml = SimpleMLP().to(device)
adapted_maml.load_state_dict(model.state_dict())
optimizer_maml = optim.SGD(adapted_maml.parameters(), lr=0.01)

for step in range(10):
    with torch.no_grad():
        pred = adapted_maml(x_test)
        error = nn.MSELoss()(pred, torch.FloatTensor(y_test).to(device))
        maml_errors.append(error.item())

    optimizer_maml.zero_grad()
    loss = nn.MSELoss()(adapted_maml(x_support), y_support)
    loss.backward()
    optimizer_maml.step()

# Reptileé©å¿œ
reptile_errors = []
adapted_reptile = SimpleMLP().to(device)
adapted_reptile.load_state_dict(reptile_model.state_dict())
optimizer_reptile = optim.SGD(adapted_reptile.parameters(), lr=0.01)

for step in range(10):
    with torch.no_grad():
        pred = adapted_reptile(x_test)
        error = nn.MSELoss()(pred, torch.FloatTensor(y_test).to(device))
        reptile_errors.append(error.item())

    optimizer_reptile.zero_grad()
    loss = nn.MSELoss()(adapted_reptile(x_support), y_support)
    loss.backward()
    optimizer_reptile.step()

ax2.plot(maml_errors, 'o-', label='MAML', linewidth=2, markersize=6)
ax2.plot(reptile_errors, 's-', label='Reptile', linewidth=2, markersize=6)
ax2.set_xlabel('Adaptation Step')
ax2.set_ylabel('Test MSE')
ax2.set_title('Adaptation Speed on New Task', fontsize=14)
ax2.legend()
ax2.grid(True, alpha=0.3)
ax2.set_yscale('log')

plt.tight_layout()
plt.show()

print("\n=== MAML vs Reptile ===")
print(f"MAML - åˆæœŸèª¤å·®: {maml_errors[0]:.4f}, æœ€çµ‚èª¤å·®: {maml_errors[-1]:.4f}")
print(f"Reptile - åˆæœŸèª¤å·®: {reptile_errors[0]:.4f}, æœ€çµ‚èª¤å·®: {reptile_errors[-1]:.4f}")
print("\nâœ“ ä¸¡æ‰‹æ³•ã¨ã‚‚é«˜é€Ÿé©å¿œãŒå¯èƒ½")
print("âœ“ MAMLã¯ã‚ãšã‹ã«è‰¯ã„åˆæœŸåŒ–ã‚’æä¾›")
print("âœ“ Reptileã¯å®Ÿè£…ãŒã‚·ãƒ³ãƒ—ãƒ«ã§è¨ˆç®—åŠ¹ç‡ãŒé«˜ã„")
</code></pre>

<hr>

<h2>2.5 å®Ÿè·µ: Omniglot Few-Shotåˆ†é¡</h2>

<h3>Omniglotãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</h3>

<p><strong>Omniglot</strong>ã¯ã€Few-Shotå­¦ç¿’ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã—ã¦åºƒãä½¿ã‚ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚</p>

<ul>
<li>50ç¨®é¡ã®è¨€èªã‹ã‚‰1,623ç¨®é¡ã®æ–‡å­—</li>
<li>å„æ–‡å­—ã«ã¤ã20æšã®æ‰‹æ›¸ãç”»åƒ</li>
<li>ã€ŒMNISTã®è»¢ç½®ç‰ˆã€ã¨å‘¼ã°ã‚Œã‚‹</li>
</ul>

<h3>5-way 1-shot ã‚¿ã‚¹ã‚¯</h3>

<p><strong>ã‚¿ã‚¹ã‚¯è¨­å®š</strong>ï¼š</p>
<ul>
<li><strong>5-way</strong>: 5ã‚¯ãƒ©ã‚¹åˆ†é¡</li>
<li><strong>1-shot</strong>: å„ã‚¯ãƒ©ã‚¹1æšã®ã‚µãƒ³ãƒ—ãƒ«ã®ã¿</li>
<li><strong>ç›®æ¨™</strong>: ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆ5æšã‹ã‚‰å­¦ç¿’ã—ã€ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆã‚’åˆ†é¡</li>
</ul>

<h3>ãƒ‡ãƒ¼ã‚¿æº–å‚™</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import numpy as np
import os

# Omniglotç”¨ã®ç•³ã¿è¾¼ã¿ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
class OmniglotCNN(nn.Module):
    """Omniglotç”¨ã®4å±¤CNN"""
    def __init__(self, n_way=5):
        super(OmniglotCNN, self).__init__()
        self.features = nn.Sequential(
            # Layer 1
            nn.Conv2d(1, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),

            # Layer 2
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),

            # Layer 3
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),

            # Layer 4
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )

        self.classifier = nn.Linear(64, n_way)

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
omniglot_model = OmniglotCNN(n_way=5).to(device)
print(f"\n=== Omniglot Model ===")
print(f"Parameters: {sum(p.numel() for p in omniglot_model.parameters()):,}")
print(f"Architecture: 4-layer CNN + Linear classifier")
</code></pre>

<h3>Few-Shotã‚¿ã‚¹ã‚¯ç”Ÿæˆ</h3>

<pre><code class="language-python">class OmniglotTaskGenerator:
    """Omniglotç”¨ã®Few-Shotã‚¿ã‚¹ã‚¯ç”Ÿæˆ"""

    def __init__(self, n_way=5, k_shot=1, q_query=15):
        """
        Args:
            n_way: ã‚¯ãƒ©ã‚¹æ•°
            k_shot: ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒ«æ•°
            q_query: ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒ«æ•°
        """
        self.n_way = n_way
        self.k_shot = k_shot
        self.q_query = q_query

        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå®Ÿéš›ã«ã¯Omniglotãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ï¼‰
        # ã“ã“ã§ã¯28x28ã®ç”»åƒã‚’ç”Ÿæˆ
        self.n_classes = 100  # ç°¡ç•¥åŒ–ã®ãŸã‚100ã‚¯ãƒ©ã‚¹
        self.images_per_class = 20

    def generate_task(self):
        """
        N-way K-shot ã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆ

        Returns:
            support_x, support_y, query_x, query_y
        """
        # ãƒ©ãƒ³ãƒ€ãƒ ã«Nå€‹ã®ã‚¯ãƒ©ã‚¹ã‚’é¸æŠ
        selected_classes = np.random.choice(
            self.n_classes, self.n_way, replace=False
        )

        support_x, support_y = [], []
        query_x, query_y = [], []

        for class_idx, class_id in enumerate(selected_classes):
            # å„ã‚¯ãƒ©ã‚¹ã‹ã‚‰ç”»åƒã‚’ã‚µãƒ³ãƒ—ãƒ«
            n_samples = self.k_shot + self.q_query

            # ãƒ€ãƒŸãƒ¼ç”»åƒç”Ÿæˆï¼ˆå®Ÿéš›ã«ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
            images = torch.randn(n_samples, 1, 28, 28)

            # ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆ
            support_x.append(images[:self.k_shot])
            support_y.extend([class_idx] * self.k_shot)

            # ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆ
            query_x.append(images[self.k_shot:])
            query_y.extend([class_idx] * self.q_query)

        # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        support_x = torch.cat(support_x, dim=0).to(device)
        support_y = torch.LongTensor(support_y).to(device)
        query_x = torch.cat(query_x, dim=0).to(device)
        query_y = torch.LongTensor(query_y).to(device)

        return support_x, support_y, query_x, query_y

# ã‚¿ã‚¹ã‚¯ç”Ÿæˆå™¨ã®ãƒ†ã‚¹ãƒˆ
task_gen = OmniglotTaskGenerator(n_way=5, k_shot=1, q_query=15)
sup_x, sup_y, qry_x, qry_y = task_gen.generate_task()

print(f"\n=== Task Generation ===")
print(f"Support set: {sup_x.shape}, labels: {sup_y.shape}")
print(f"Query set: {qry_x.shape}, labels: {qry_y.shape}")
print(f"Support labels: {sup_y.cpu().numpy()}")
print(f"Query labels distribution: {np.bincount(qry_y.cpu().numpy())}")
</code></pre>

<h3>MAMLã¨Reptileã®æ¯”è¼ƒå®Ÿé¨“</h3>

<pre><code class="language-python">def train_meta_learning(model, algorithm='maml', n_iterations=1000,
                       n_way=5, k_shot=1, q_query=15,
                       inner_lr=0.01, outer_lr=0.001, inner_steps=5,
                       eval_interval=100):
    """
    ãƒ¡ã‚¿å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆMAMLã¾ãŸã¯Reptileï¼‰

    Args:
        model: PyTorchãƒ¢ãƒ‡ãƒ«
        algorithm: 'maml' ã¾ãŸã¯ 'reptile'
        n_iterations: å­¦ç¿’ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°
        n_way: N-wayåˆ†é¡
        k_shot: K-shotå­¦ç¿’
        q_query: ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆã‚µã‚¤ã‚º
        inner_lr: Inner Loopå­¦ç¿’ç‡
        outer_lr: Outer Loopå­¦ç¿’ç‡
        inner_steps: Inner Loopæ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—æ•°
        eval_interval: è©•ä¾¡é–“éš”

    Returns:
        train_accs, val_accs: ç²¾åº¦å±¥æ­´
    """
    task_gen = OmniglotTaskGenerator(n_way=n_way, k_shot=k_shot, q_query=q_query)
    criterion = nn.CrossEntropyLoss()

    if algorithm == 'maml':
        meta_optimizer = optim.Adam(model.parameters(), lr=outer_lr)

    train_accs = []

    for iteration in range(n_iterations):
        # ã‚¿ã‚¹ã‚¯ç”Ÿæˆ
        support_x, support_y, query_x, query_y = task_gen.generate_task()

        if algorithm == 'maml':
            # MAMLæ›´æ–°
            meta_optimizer.zero_grad()

            with higher.innerloop_ctx(
                model,
                optim.SGD(model.parameters(), lr=inner_lr),
                copy_initial_weights=False
            ) as (fmodel, diffopt):

                # Inner Loop
                for _ in range(inner_steps):
                    support_loss = criterion(fmodel(support_x), support_y)
                    diffopt.step(support_loss)

                # Query loss
                query_pred = fmodel(query_x)
                query_loss = criterion(query_pred, query_y)

                # ç²¾åº¦è¨ˆç®—
                accuracy = (query_pred.argmax(1) == query_y).float().mean()
                train_accs.append(accuracy.item())

                # Outer Loop
                query_loss.backward()
                meta_optimizer.step()

        elif algorithm == 'reptile':
            # Reptileæ›´æ–°
            meta_params = [p.clone() for p in model.parameters()]

            # Inner Loop
            optimizer = optim.SGD(model.parameters(), lr=inner_lr)
            for _ in range(inner_steps):
                optimizer.zero_grad()
                loss = criterion(model(support_x), support_y)
                loss.backward()
                optimizer.step()

            # ç²¾åº¦è¨ˆç®—
            with torch.no_grad():
                query_pred = model(query_x)
                accuracy = (query_pred.argmax(1) == query_y).float().mean()
                train_accs.append(accuracy.item())

            # ãƒ¡ã‚¿æ›´æ–°
            with torch.no_grad():
                for meta_param, task_param in zip(meta_params, model.parameters()):
                    meta_param.add_(task_param - meta_param, alpha=outer_lr)

                for param, meta_param in zip(model.parameters(), meta_params):
                    param.copy_(meta_param)

        # å®šæœŸçš„ãªè©•ä¾¡
        if (iteration + 1) % eval_interval == 0:
            avg_acc = np.mean(train_accs[-eval_interval:])
            print(f"{algorithm.upper()} - Iter {iteration+1}/{n_iterations}, "
                  f"Avg Accuracy: {avg_acc:.3f}")

    return train_accs

# MAMLã§å­¦ç¿’
print("\n=== Training MAML on Omniglot ===")
maml_model = OmniglotCNN(n_way=5).to(device)
maml_accs = train_meta_learning(
    maml_model,
    algorithm='maml',
    n_iterations=1000,
    n_way=5,
    k_shot=1,
    q_query=15,
    inner_lr=0.01,
    outer_lr=0.001,
    inner_steps=5,
    eval_interval=200
)

# Reptileã§å­¦ç¿’
print("\n=== Training Reptile on Omniglot ===")
reptile_model_omniglot = OmniglotCNN(n_way=5).to(device)
reptile_accs = train_meta_learning(
    reptile_model_omniglot,
    algorithm='reptile',
    n_iterations=1000,
    n_way=5,
    k_shot=1,
    q_query=15,
    inner_lr=0.01,
    outer_lr=0.1,
    inner_steps=5,
    eval_interval=200
)
</code></pre>

<h3>åæŸæ€§ã¨ç²¾åº¦è©•ä¾¡</h3>

<pre><code class="language-python"># çµæœã®å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# ç²¾åº¦æ›²ç·š
ax1 = axes[0]
window = 50
maml_smooth = np.convolve(maml_accs, np.ones(window)/window, mode='valid')
reptile_smooth = np.convolve(reptile_accs, np.ones(window)/window, mode='valid')

ax1.plot(maml_smooth, label='MAML', linewidth=2, alpha=0.8)
ax1.plot(reptile_smooth, label='Reptile', linewidth=2, alpha=0.8)
ax1.set_xlabel('Iteration')
ax1.set_ylabel('Query Accuracy')
ax1.set_title('5-way 1-shot Learning Curve (Smoothed)', fontsize=14)
ax1.legend()
ax1.grid(True, alpha=0.3)
ax1.set_ylim([0, 1])

# æœ€çµ‚æ€§èƒ½ã®æ¯”è¼ƒ
ax2 = axes[1]
final_window = 100
maml_final = np.mean(maml_accs[-final_window:])
reptile_final = np.mean(reptile_accs[-final_window:])

methods = ['MAML', 'Reptile']
accuracies = [maml_final, reptile_final]
colors = ['#1f77b4', '#ff7f0e']

bars = ax2.bar(methods, accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=2)
ax2.set_ylabel('Final Accuracy')
ax2.set_title('Final Performance Comparison', fontsize=14)
ax2.set_ylim([0, 1])
ax2.grid(True, alpha=0.3, axis='y')

# å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
for bar, acc in zip(bars, accuracies):
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,
            f'{acc:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.show()

print("\n=== Final Results ===")
print(f"MAML - Final Accuracy: {maml_final:.3f} Â± {np.std(maml_accs[-final_window:]):.3f}")
print(f"Reptile - Final Accuracy: {reptile_final:.3f} Â± {np.std(reptile_accs[-final_window:]):.3f}")
print(f"\nâœ“ 5-way 1-shot classification on Omniglot")
print(f"âœ“ Random baseline: 20% (1/5)")
print(f"âœ“ Both methods significantly outperform random")

# çµ±è¨ˆçš„æ¯”è¼ƒ
from scipy import stats
t_stat, p_value = stats.ttest_ind(maml_accs[-final_window:],
                                    reptile_accs[-final_window:])
print(f"\nçµ±è¨ˆçš„æ¤œå®š (t-test):")
print(f"  t-statistic: {t_stat:.3f}")
print(f"  p-value: {p_value:.3f}")
if p_value < 0.05:
    print(f"  â†’ æœ‰æ„å·®ã‚ã‚Šï¼ˆp < 0.05ï¼‰")
else:
    print(f"  â†’ æœ‰æ„å·®ãªã—ï¼ˆp >= 0.05ï¼‰")
</code></pre>

<hr>

<h2>2.6 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>MAMLã®åŸç†</strong></p>
<ul>
<li>å°‘æ•°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®é«˜é€Ÿé©å¿œã‚’å®Ÿç¾ã™ã‚‹åˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å­¦ç¿’</li>
<li>äºŒæ®µéšæœ€é©åŒ–: Inner Loopï¼ˆã‚¿ã‚¹ã‚¯é©å¿œï¼‰ã¨Outer Loopï¼ˆãƒ¡ã‚¿å­¦ç¿’ï¼‰</li>
<li>å‹¾é…ã®å‹¾é…ï¼ˆäºŒæ¬¡å¾®åˆ†ï¼‰ã«ã‚ˆã‚Šå¼·åŠ›ãªé©å¿œèƒ½åŠ›ã‚’ç²å¾—</li>
</ul></li>

<li><p><strong>MAMLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </strong></p>
<ul>
<li>Inner Loop: $\theta'_i = \theta - \alpha \nabla_\theta \mathcal{L}_{\mathcal{T}_i}^{tr}(f_\theta)$</li>
<li>Outer Loop: $\theta \leftarrow \theta - \beta \nabla_\theta \sum \mathcal{L}_{\mathcal{T}_i}^{test}(f_{\theta'_i})$</li>
<li>FOMAML: ä¸€æ¬¡å¾®åˆ†ã®ã¿ã§åŠ¹ç‡åŒ–</li>
</ul></li>

<li><p><strong>PyTorchå®Ÿè£…</strong></p>
<ul>
<li>higherãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§äºŒæ¬¡å¾®åˆ†ã‚’ç°¡å˜ã«å®Ÿè£…</li>
<li>ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®æ§‹ç¯‰</li>
<li>æ­£å¼¦æ³¢å›å¸°ã‚¿ã‚¹ã‚¯ã§ã®å‹•ä½œç¢ºèª</li>
</ul></li>

<li><p><strong>Reptileã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </strong></p>
<ul>
<li>ä¸€æ¬¡å¾®åˆ†ã®ã¿ã§ãƒ¡ã‚¿å­¦ç¿’ã‚’å®Ÿç¾</li>
<li>æ›´æ–°å¼: $\theta \leftarrow \theta + \epsilon (\theta' - \theta)$</li>
<li>å®Ÿè£…ãŒã‚·ãƒ³ãƒ—ãƒ«ã§è¨ˆç®—åŠ¹ç‡ãŒé«˜ã„</li>
</ul></li>

<li><p><strong>Omniglotå®Ÿé¨“</strong></p>
<ul>
<li>5-way 1-shotåˆ†é¡ã‚¿ã‚¹ã‚¯</li>
<li>MAMLã¨Reptileã®æ€§èƒ½æ¯”è¼ƒ</li>
<li>ä¸¡æ‰‹æ³•ã¨ã‚‚ãƒ©ãƒ³ãƒ€ãƒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’å¤§ããä¸Šå›ã‚‹</li>
</ul></li>
</ol>

<h3>MAML vs Reptile ã¾ã¨ã‚</h3>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>MAML</th>
<th>Reptile</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ç†è«–çš„åŸºç›¤</strong></td>
<td>äºŒæ¬¡å¾®åˆ†ã«ã‚ˆã‚‹æœ€é©åŒ–</td>
<td>ä¸€æ¬¡å¾®åˆ†ã®æ–¹å‘ã¸ç§»å‹•</td>
</tr>
<tr>
<td><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆ</strong></td>
<td>é«˜ã„ï¼ˆäºŒæ¬¡å¾®åˆ†ï¼‰</td>
<td>ä½ã„ï¼ˆä¸€æ¬¡å¾®åˆ†ã®ã¿ï¼‰</td>
</tr>
<tr>
<td><strong>ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡</strong></td>
<td>å¤šã„</td>
<td>å°‘ãªã„</td>
</tr>
<tr>
<td><strong>å®Ÿè£…ã®è¤‡é›‘ã•</strong></td>
<td>è¤‡é›‘ï¼ˆhigherãŒå¿…è¦ï¼‰</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«</td>
</tr>
<tr>
<td><strong>æ€§èƒ½</strong></td>
<td>ç†è«–çš„ã«æœ€é©</td>
<td>å®Ÿç”¨çš„ã«ååˆ†</td>
</tr>
<tr>
<td><strong>é©ç”¨ç¯„å›²</strong></td>
<td>ä»»æ„ã®ãƒ¢ãƒ‡ãƒ«</td>
<td>ä»»æ„ã®ãƒ¢ãƒ‡ãƒ«</td>
</tr>
<tr>
<td><strong>æ¨å¥¨ç”¨é€”</strong></td>
<td>æœ€é«˜æ€§èƒ½ãŒå¿…è¦ãªå ´åˆ</td>
<td>åŠ¹ç‡é‡è¦–ã®å ´åˆ</td>
</tr>
</tbody>
</table>

<h3>å®Ÿè·µçš„ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³</h3>

<table>
<thead>
<tr>
<th>çŠ¶æ³</th>
<th>æ¨å¥¨æ‰‹æ³•</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td>ç ”ç©¶ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯</td>
<td>MAML</td>
<td>æœ€é«˜æ€§èƒ½ã‚’è¿½æ±‚</td>
</tr>
<tr>
<td>ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°</td>
<td>Reptile</td>
<td>å®Ÿè£…ãŒç°¡å˜</td>
</tr>
<tr>
<td>è¨ˆç®—è³‡æºåˆ¶ç´„</td>
<td>FOMAML/Reptile</td>
<td>åŠ¹ç‡çš„</td>
</tr>
<tr>
<td>å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«</td>
<td>Reptile</td>
<td>ãƒ¡ãƒ¢ãƒªåŠ¹ç‡</td>
</tr>
<tr>
<td>å°‘æ•°ã‚¹ãƒ†ãƒƒãƒ—é©å¿œ</td>
<td>MAML</td>
<td>ã‚ˆã‚Šè‰¯ã„åˆæœŸåŒ–</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ç« ã¸</h3>

<p>ç¬¬3ç« ã§ã¯ã€<strong>Prototypical Networks</strong>ã‚’å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>åŸ‹ã‚è¾¼ã¿ç©ºé–“ã§ã®ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—å­¦ç¿’</li>
<li>è·é›¢ãƒ™ãƒ¼ã‚¹ã®åˆ†é¡</li>
<li>å®Ÿè£…ã¨MAMLã¨ã®æ¯”è¼ƒ</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>MAMLã¨Reptileã®æ›´æ–°å¼ã®é•ã„ã‚’æ•°å¼ã§èª¬æ˜ã—ã€ãªãœReptileã®æ–¹ãŒè¨ˆç®—åŠ¹ç‡ãŒè‰¯ã„ã®ã‹è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>MAMLæ›´æ–°å¼</strong>ï¼š</p>
<p>$$
\theta \leftarrow \theta - \beta \nabla_\theta \sum_{\mathcal{T}_i} \mathcal{L}_{\mathcal{T}_i}^{test}(f_{\theta'_i})
$$</p>

<p>ã“ã“ã§ã€$\theta'_i = \theta - \alpha \nabla_\theta \mathcal{L}_{\mathcal{T}_i}^{tr}(f_\theta)$ ãªã®ã§ã€$\theta'_i$ ã¯ $\theta$ ã®é–¢æ•°ã§ã™ã€‚</p>

<p>ã—ãŸãŒã£ã¦ã€$\nabla_\theta \mathcal{L}_{\mathcal{T}_i}^{test}(f_{\theta'_i})$ ã‚’è¨ˆç®—ã™ã‚‹ã«ã¯<strong>é€£é–å¾‹</strong>ãŒå¿…è¦ï¼š</p>
<p>$$
\nabla_\theta \mathcal{L}_{\mathcal{T}_i}^{test}(f_{\theta'_i}) = \nabla_{\theta'_i} \mathcal{L}_{\mathcal{T}_i}^{test} \cdot \nabla_\theta \theta'_i
$$</p>

<p>ã“ã® $\nabla_\theta \theta'_i$ ã®è¨ˆç®—ãŒ<strong>äºŒæ¬¡å¾®åˆ†</strong>ã§ã‚ã‚Šã€è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ã€‚</p>

<p><strong>Reptileæ›´æ–°å¼</strong>ï¼š</p>
<p>$$
\theta \leftarrow \theta + \epsilon (\theta'_i - \theta)
$$</p>

<p>ã“ã®å¼ã¯<strong>å˜ç´”ãªé‡ã¿ä»˜ãå¹³å‡</strong>ã§ã‚ã‚Šã€å¾®åˆ†è¨ˆç®—ã¯ä¸è¦ã§ã™ã€‚</p>

<p><strong>è¨ˆç®—åŠ¹ç‡ã®é•ã„</strong>ï¼š</p>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>MAML</th>
<th>Reptile</th>
</tr>
</thead>
<tbody>
<tr>
<td>å‹¾é…è¨ˆç®—</td>
<td>$\nabla_\theta \nabla_{\theta'} \mathcal{L}$ï¼ˆäºŒæ¬¡ï¼‰</td>
<td>$\nabla_\theta \mathcal{L}$ï¼ˆä¸€æ¬¡ã®ã¿ï¼‰</td>
</tr>
<tr>
<td>è¨ˆç®—ã‚°ãƒ©ãƒ•</td>
<td>Inner Loopã®å±¥æ­´ã‚’ä¿æŒ</td>
<td>ä¸è¦</td>
</tr>
<tr>
<td>ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡</td>
<td>é«˜ã„ï¼ˆä¸­é–“å‹¾é…ã‚’ä¿å­˜ï¼‰</td>
<td>ä½ã„</td>
</tr>
<tr>
<td>è¨ˆç®—æ™‚é–“</td>
<td>ç´„2å€</td>
<td>åŸºæº–</td>
</tr>
</tbody>
</table>

<p><strong>çµè«–</strong>ï¼šReptileã¯äºŒæ¬¡å¾®åˆ†ã‚’è¨ˆç®—ã›ãšã€å˜ç´”ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ã®ã¿ãªã®ã§ã€ç´„50-70%ã®è¨ˆç®—ã‚³ã‚¹ãƒˆå‰Šæ¸›ã‚’å®Ÿç¾ã—ã¾ã™ã€‚</p>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ã€MAMLã®Inner Loopã‚’å®Ÿè£…ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™ãŒã€èª¤ã‚ŠãŒã‚ã‚Šã¾ã™ã€‚å•é¡Œç‚¹ã‚’æŒ‡æ‘˜ã—ã€æ­£ã—ã„ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python"># èª¤ã£ãŸã‚³ãƒ¼ãƒ‰
def wrong_inner_loop(model, x_support, y_support, inner_lr=0.01):
    criterion = nn.MSELoss()
    optimizer = optim.SGD(model.parameters(), lr=inner_lr)

    optimizer.zero_grad()
    predictions = model(x_support)
    loss = criterion(predictions, y_support)
    loss.backward()
    optimizer.step()

    return loss
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>å•é¡Œç‚¹</strong>ï¼š</p>

<ol>
<li><strong>è¨ˆç®—ã‚°ãƒ©ãƒ•ãŒåˆ‡æ–­ã•ã‚Œã‚‹</strong>: <code>optimizer.step()</code> ã‚’ä½¿ã†ã¨ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ›´æ–°ã•ã‚Œã¾ã™ãŒã€Outer Loopã§å¿…è¦ãªäºŒæ¬¡å¾®åˆ†ã®ãŸã‚ã®è¨ˆç®—ã‚°ãƒ©ãƒ•ãŒä¿æŒã•ã‚Œã¾ã›ã‚“ã€‚</li>
<li><strong>higherãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœªä½¿ç”¨</strong>: MAMLã§ã¯ã€Inner Loopã®æ›´æ–°ã‚’è¿½è·¡ã—ã€Outer Loopã§äºŒæ¬¡å¾®åˆ†ã‚’è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚</li>
</ol>

<p><strong>æ­£ã—ã„å®Ÿè£…</strong>ï¼š</p>

<pre><code class="language-python">import higher

def correct_inner_loop(model, x_support, y_support, x_query, y_query,
                       inner_lr=0.01, inner_steps=1):
    """
    MAMLã®Inner Loopï¼ˆæ­£ã—ã„å®Ÿè£…ï¼‰

    Args:
        model: PyTorchãƒ¢ãƒ‡ãƒ«
        x_support: ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆå…¥åŠ›
        y_support: ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆå‡ºåŠ›
        x_query: ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆå…¥åŠ›
        y_query: ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆå‡ºåŠ›
        inner_lr: Inner Loopå­¦ç¿’ç‡
        inner_steps: é©å¿œã‚¹ãƒ†ãƒƒãƒ—æ•°

    Returns:
        query_loss: ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆæå¤±ï¼ˆå‹¾é…ãŒè¿½è·¡ã•ã‚Œã‚‹ï¼‰
    """
    criterion = nn.MSELoss()

    # higherã‚’ä½¿ç”¨ã—ã¦Inner Loopã‚’å®Ÿè£…
    with higher.innerloop_ctx(
        model,
        optim.SGD(model.parameters(), lr=inner_lr),
        copy_initial_weights=False,
        track_higher_grads=True  # äºŒæ¬¡å¾®åˆ†ã‚’è¿½è·¡
    ) as (fmodel, diffopt):

        # Inner Loop: ã‚¿ã‚¹ã‚¯é©å¿œ
        for _ in range(inner_steps):
            support_pred = fmodel(x_support)
            support_loss = criterion(support_pred, y_support)
            diffopt.step(support_loss)

        # ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆã§è©•ä¾¡ï¼ˆå‹¾é…ãŒè¿½è·¡ã•ã‚Œã‚‹ï¼‰
        query_pred = fmodel(x_query)
        query_loss = criterion(query_pred, y_query)

    return query_loss

# ä½¿ç”¨ä¾‹
model = SimpleMLP().to(device)
meta_optimizer = optim.Adam(model.parameters(), lr=0.001)

# ã‚¿ã‚¹ã‚¯ç”Ÿæˆ
x_sup, y_sup, amp, phase = generate_sinusoid_task(n_samples=5)
x_qry, y_qry, _, _ = generate_sinusoid_task(
    amplitude=amp, phase=phase, n_samples=10
)

# MAMLæ›´æ–°
meta_optimizer.zero_grad()
query_loss = correct_inner_loop(model, x_sup, y_sup, x_qry, y_qry)
query_loss.backward()  # äºŒæ¬¡å¾®åˆ†ãŒè¨ˆç®—ã•ã‚Œã‚‹
meta_optimizer.step()

print(f"âœ“ Query loss: {query_loss.item():.4f}")
print("âœ“ äºŒæ¬¡å¾®åˆ†ãŒæ­£ã—ãè¨ˆç®—ã•ã‚Œã¾ã—ãŸ")
</code></pre>

<p><strong>é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ</strong>ï¼š</p>
<ul>
<li><code>higher.innerloop_ctx</code>ã‚’ä½¿ç”¨ã—ã¦ã€Inner Loopã®æ›´æ–°ã‚’è¿½è·¡</li>
<li><code>track_higher_grads=True</code>ã§äºŒæ¬¡å¾®åˆ†ã‚’æœ‰åŠ¹åŒ–</li>
<li><code>fmodel</code>ã¯å…ƒã®modelã®ã‚³ãƒ”ãƒ¼ã§ã€å‹¾é…ãŒè¿½è·¡ã•ã‚Œã‚‹</li>
<li>Outer Loopã§<code>query_loss.backward()</code>ã‚’å‘¼ã¶ã¨ã€ãƒ¡ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹å‹¾é…ãŒè¨ˆç®—ã•ã‚Œã‚‹</li>
</ul>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>5-way 5-shot Omniglotåˆ†é¡ã‚¿ã‚¹ã‚¯ã§ã€MAMLã¨Reptileã®æ€§èƒ½ã‚’æ¯”è¼ƒã™ã‚‹å®Ÿé¨“ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã‚’å«ã‚ã‚‹ã“ã¨ï¼š</p>
<ul>
<li>ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆè¨“ç·´/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆï¼‰</li>
<li>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š</li>
<li>è©•ä¾¡æŒ‡æ¨™</li>
<li>æœŸå¾…ã•ã‚Œã‚‹çµæœ</li>
</ul>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>å®Ÿé¨“è¨­è¨ˆ</strong>ï¼š</p>

<p><strong>1. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²</strong>ï¼š</p>
<ul>
<li><strong>è¨“ç·´ç”¨æ–‡å­—</strong>: 1,200æ–‡å­—ï¼ˆãƒ¡ã‚¿è¨“ç·´ï¼‰</li>
<li><strong>æ¤œè¨¼ç”¨æ–‡å­—</strong>: 200æ–‡å­—ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ï¼‰</li>
<li><strong>ãƒ†ã‚¹ãƒˆç”¨æ–‡å­—</strong>: 223æ–‡å­—ï¼ˆæœ€çµ‚è©•ä¾¡ï¼‰</li>
</ul>

<p><strong>2. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</strong>ï¼š</p>

<table>
<thead>
<tr>
<th>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</th>
<th>MAML</th>
<th>Reptile</th>
</tr>
</thead>
<tbody>
<tr>
<td>Inner LR (Î±)</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td>Outer LR (Î²/Îµ)</td>
<td>0.001</td>
<td>0.1</td>
</tr>
<tr>
<td>Inner Steps</td>
<td>5</td>
<td>5</td>
</tr>
<tr>
<td>Batch Size</td>
<td>4 tasks</td>
<td>1 task</td>
</tr>
<tr>
<td>Iterations</td>
<td>60,000</td>
<td>60,000</td>
</tr>
</tbody>
</table>

<p><strong>3. è©•ä¾¡æŒ‡æ¨™</strong>ï¼š</p>
<ul>
<li><strong>ç²¾åº¦</strong>: æ­£è§£ç‡ï¼ˆAccuracyï¼‰</li>
<li><strong>åæŸé€Ÿåº¦</strong>: ç›®æ¨™ç²¾åº¦ï¼ˆä¾‹: 95%ï¼‰åˆ°é”ã¾ã§ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°</li>
<li><strong>é©å¿œé€Ÿåº¦</strong>: ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯ã§ã®å°‘æ•°ã‚¹ãƒ†ãƒƒãƒ—å¾Œã®ç²¾åº¦å‘ä¸Š</li>
<li><strong>è¨ˆç®—åŠ¹ç‡</strong>: ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚ãŸã‚Šã®å®Ÿè¡Œæ™‚é–“</li>
</ul>

<p><strong>å®Ÿè£…ã‚³ãƒ¼ãƒ‰</strong>ï¼š</p>

<pre><code class="language-python">import time
import numpy as np
import matplotlib.pyplot as plt

def comprehensive_comparison(n_iterations=5000, n_way=5, k_shot=5):
    """
    MAMLã¨Reptileã®åŒ…æ‹¬çš„æ¯”è¼ƒå®Ÿé¨“
    """
    results = {
        'maml': {'train_acc': [], 'val_acc': [], 'time': []},
        'reptile': {'train_acc': [], 'val_acc': [], 'time': []}
    }

    # MAMLå­¦ç¿’
    print("=== Training MAML ===")
    maml_model = OmniglotCNN(n_way=n_way).to(device)
    start_time = time.time()

    maml_train_acc = train_meta_learning(
        maml_model, algorithm='maml',
        n_iterations=n_iterations, n_way=n_way, k_shot=k_shot,
        inner_lr=0.01, outer_lr=0.001, inner_steps=5
    )

    maml_time = time.time() - start_time
    results['maml']['train_acc'] = maml_train_acc
    results['maml']['time'] = maml_time

    # Reptileå­¦ç¿’
    print("\n=== Training Reptile ===")
    reptile_model = OmniglotCNN(n_way=n_way).to(device)
    start_time = time.time()

    reptile_train_acc = train_meta_learning(
        reptile_model, algorithm='reptile',
        n_iterations=n_iterations, n_way=n_way, k_shot=k_shot,
        inner_lr=0.01, outer_lr=0.1, inner_steps=5
    )

    reptile_time = time.time() - start_time
    results['reptile']['train_acc'] = reptile_train_acc
    results['reptile']['time'] = reptile_time

    # ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®è©•ä¾¡
    print("\n=== Test Set Evaluation ===")

    def evaluate_test(model, n_test_tasks=100):
        task_gen = OmniglotTaskGenerator(n_way=n_way, k_shot=k_shot, q_query=15)
        accuracies = []

        for _ in range(n_test_tasks):
            support_x, support_y, query_x, query_y = task_gen.generate_task()

            # é©å¿œ
            optimizer = optim.SGD(model.parameters(), lr=0.01)
            for _ in range(5):
                optimizer.zero_grad()
                loss = nn.CrossEntropyLoss()(model(support_x), support_y)
                loss.backward()
                optimizer.step()

            # è©•ä¾¡
            with torch.no_grad():
                pred = model(query_x)
                acc = (pred.argmax(1) == query_y).float().mean()
                accuracies.append(acc.item())

        return np.mean(accuracies), np.std(accuracies)

    maml_test_acc, maml_test_std = evaluate_test(maml_model)
    reptile_test_acc, reptile_test_std = evaluate_test(reptile_model)

    # çµæœã®å¯è¦–åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # å­¦ç¿’æ›²ç·š
    ax1 = axes[0, 0]
    window = 100
    maml_smooth = np.convolve(maml_train_acc, np.ones(window)/window, mode='valid')
    reptile_smooth = np.convolve(reptile_train_acc, np.ones(window)/window, mode='valid')
    ax1.plot(maml_smooth, label='MAML', linewidth=2)
    ax1.plot(reptile_smooth, label='Reptile', linewidth=2)
    ax1.set_xlabel('Iteration')
    ax1.set_ylabel('Training Accuracy')
    ax1.set_title(f'{n_way}-way {k_shot}-shot Learning Curves', fontsize=13)
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # ãƒ†ã‚¹ãƒˆç²¾åº¦
    ax2 = axes[0, 1]
    methods = ['MAML', 'Reptile']
    test_accs = [maml_test_acc, reptile_test_acc]
    test_stds = [maml_test_std, reptile_test_std]
    bars = ax2.bar(methods, test_accs, yerr=test_stds,
                   capsize=10, color=['#1f77b4', '#ff7f0e'],
                   alpha=0.7, edgecolor='black', linewidth=2)
    ax2.set_ylabel('Test Accuracy')
    ax2.set_title('Test Set Performance', fontsize=13)
    ax2.set_ylim([0, 1])
    ax2.grid(True, alpha=0.3, axis='y')

    for bar, acc in zip(bars, test_accs):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                f'{acc:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')

    # è¨ˆç®—æ™‚é–“
    ax3 = axes[1, 0]
    times = [maml_time, reptile_time]
    bars = ax3.bar(methods, times, color=['#1f77b4', '#ff7f0e'],
                   alpha=0.7, edgecolor='black', linewidth=2)
    ax3.set_ylabel('Training Time (seconds)')
    ax3.set_title('Computational Efficiency', fontsize=13)
    ax3.grid(True, alpha=0.3, axis='y')

    for bar, t in zip(bars, times):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height + 5,
                f'{t:.1f}s', ha='center', va='bottom', fontsize=11, fontweight='bold')

    # ã‚µãƒãƒªãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«
    ax4 = axes[1, 1]
    ax4.axis('off')

    summary_data = [
        ['Metric', 'MAML', 'Reptile'],
        ['Train Acc (final)', f'{np.mean(maml_train_acc[-100:]):.3f}',
         f'{np.mean(reptile_train_acc[-100:]):.3f}'],
        ['Test Acc', f'{maml_test_acc:.3f}Â±{maml_test_std:.3f}',
         f'{reptile_test_acc:.3f}Â±{reptile_test_std:.3f}'],
        ['Time (s)', f'{maml_time:.1f}', f'{reptile_time:.1f}'],
        ['Time per iter (ms)', f'{maml_time/n_iterations*1000:.2f}',
         f'{reptile_time/n_iterations*1000:.2f}']
    ]

    table = ax4.table(cellText=summary_data, cellLoc='center',
                     loc='center', bbox=[0, 0, 1, 1])
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2)

    for i in range(len(summary_data)):
        for j in range(len(summary_data[0])):
            cell = table[(i, j)]
            if i == 0:
                cell.set_facecolor('#d0d0d0')
                cell.set_text_props(weight='bold')
            else:
                cell.set_facecolor('#f0f0f0' if i % 2 == 0 else 'white')

    plt.tight_layout()
    plt.show()

    # çµæœãƒ¬ãƒãƒ¼ãƒˆ
    print("\n" + "="*60)
    print("COMPREHENSIVE COMPARISON RESULTS")
    print("="*60)
    print(f"\nTask: {n_way}-way {k_shot}-shot classification")
    print(f"Iterations: {n_iterations}")
    print(f"\nMAML:")
    print(f"  Final Train Acc: {np.mean(maml_train_acc[-100:]):.3f}")
    print(f"  Test Acc: {maml_test_acc:.3f} Â± {maml_test_std:.3f}")
    print(f"  Training Time: {maml_time:.1f}s ({maml_time/n_iterations*1000:.2f}ms/iter)")
    print(f"\nReptile:")
    print(f"  Final Train Acc: {np.mean(reptile_train_acc[-100:]):.3f}")
    print(f"  Test Acc: {reptile_test_acc:.3f} Â± {reptile_test_std:.3f}")
    print(f"  Training Time: {reptile_time:.1f}s ({reptile_time/n_iterations*1000:.2f}ms/iter)")
    print(f"\nSpeedup: {maml_time/reptile_time:.2f}x")
    print("="*60)

    return results

# å®Ÿé¨“å®Ÿè¡Œ
results = comprehensive_comparison(n_iterations=2000, n_way=5, k_shot=5)
</code></pre>

<p><strong>4. æœŸå¾…ã•ã‚Œã‚‹çµæœ</strong>ï¼š</p>

<table>
<thead>
<tr>
<th>ãƒ¡ãƒˆãƒªãƒƒã‚¯</th>
<th>MAML</th>
<th>Reptile</th>
</tr>
</thead>
<tbody>
<tr>
<td>ãƒ†ã‚¹ãƒˆç²¾åº¦</td>
<td>95-98%</td>
<td>94-97%</td>
</tr>
<tr>
<td>åæŸé€Ÿåº¦</td>
<td>ã‚„ã‚„é€Ÿã„</td>
<td>ã‚„ã‚„é…ã„</td>
</tr>
<tr>
<td>è¨ˆç®—æ™‚é–“</td>
<td>åŸºæº–</td>
<td>50-70%å‰Šæ¸›</td>
</tr>
<tr>
<td>ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡</td>
<td>é«˜ã„</td>
<td>ä½ã„</td>
</tr>
</tbody>
</table>

<p><strong>çµè«–</strong>ï¼š</p>
<ul>
<li>MAMLã¯ã‚ãšã‹ã«é«˜ã„ç²¾åº¦ã‚’é”æˆã™ã‚‹ãŒã€è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„</li>
<li>Reptileã¯å®Ÿç”¨çš„ã«ååˆ†ãªç²¾åº¦ã‚’é”æˆã—ã€åŠ¹ç‡çš„</li>
<li>5-shotã®å ´åˆã€å·®ã¯1-shotã‚ˆã‚Šå°ã•ããªã‚‹å‚¾å‘</li>
<li>å®Ÿå‹™ã§ã¯Reptileã€ç ”ç©¶ã§ã¯MAMLãŒæ¨å¥¨ã•ã‚Œã‚‹</li>
</ul>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Finn, C., Abbeel, P., & Levine, S. (2017). <em>Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</em>. ICML 2017.</li>
<li>Nichol, A., Achiam, J., & Schulman, J. (2018). <em>On First-Order Meta-Learning Algorithms</em>. arXiv preprint arXiv:1803.02999.</li>
<li>Antoniou, A., Edwards, H., & Storkey, A. (2018). <em>How to train your MAML</em>. ICLR 2019.</li>
<li>Lake, B. M., Salakhutdinov, R., & Tenenbaum, J. B. (2015). <em>Human-level concept learning through probabilistic program induction</em>. Science, 350(6266), 1332-1338.</li>
<li>Grefenstette, E., et al. (2019). <em>Higher: A pytorch library for meta-learning</em>. GitHub repository.</li>
</ol>

<div class="navigation">
    <a href="chapter1-intro.html" class="nav-button">â† ç¬¬1ç« : ãƒ¡ã‚¿å­¦ç¿’å…¥é–€</a>
    <a href="chapter3-prototypical-networks.html" class="nav-button">æ¬¡ã®ç« : Prototypical Networks â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-23</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
