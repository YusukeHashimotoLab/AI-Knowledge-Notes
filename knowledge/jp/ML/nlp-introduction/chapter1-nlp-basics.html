<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
<meta content="ç¬¬1ç« ï¼šNLPåŸºç¤ - AI Terakoya" name="description"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬1ç« ï¼šNLPåŸºç¤ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/nlp-introduction/chapter1-nlp-basics.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/nlp-introduction/index.html">Nlp</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 1</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬1ç« ï¼šNLPåŸºç¤</h1>
            <p class="subtitle">è‡ªç„¶è¨€èªå‡¦ç†ã®åŸºç¤æŠ€è¡“ - ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã‹ã‚‰å˜èªåŸ‹ã‚è¾¼ã¿ã¾ã§</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 30-35åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´šã€œä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 10å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã®å„æ‰‹æ³•ã‚’ç†è§£ã—å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®ç¨®é¡ã¨ä½¿ã„åˆ†ã‘ã‚’ç¿’å¾—ã™ã‚‹</li>
<li>âœ… å˜èªã®æ•°å€¤è¡¨ç¾æ–¹æ³•ï¼ˆBag of Wordsã€TF-IDFï¼‰ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Word2Vecã€GloVeã€FastTextã®åŸç†ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… æ—¥æœ¬èªNLPã®ç‰¹æœ‰ã®èª²é¡Œã¨å¯¾å‡¦æ³•ã‚’çŸ¥ã‚‹</li>
<li>âœ… åŸºæœ¬çš„ãªNLPã‚¿ã‚¹ã‚¯ã‚’å®Ÿè£…ã§ãã‚‹</li>
</ul>

<hr>

<h2>1.1 ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†</h2>

<h3>ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã¨ã¯</h3>
<p><strong>ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ï¼ˆText Preprocessingï¼‰</strong>ã¯ã€ç”Ÿã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ãŒå‡¦ç†ã§ãã‚‹å½¢å¼ã«å¤‰æ›ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã§ã™ã€‚</p>

<blockquote>
<p>ã€ŒNLPã®8å‰²ã¯å‰å‡¦ç†ã€ã¨è¨€ã‚ã‚Œã‚‹ã»ã©ã€å‰å‡¦ç†ã®å“è³ªãŒæœ€çµ‚çš„ãªãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’å·¦å³ã—ã¾ã™ã€‚</p>
</blockquote>

<h3>ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã®å…¨ä½“åƒ</h3>

<div class="mermaid">
graph TD
    A[ç”Ÿãƒ†ã‚­ã‚¹ãƒˆ] --> B[ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°]
    B --> C[ãƒˆãƒ¼ã‚¯ãƒ³åŒ–]
    C --> D[æ­£è¦åŒ–]
    D --> E[ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»]
    E --> F[ã‚¹ãƒ†ãƒŸãƒ³ã‚°/ãƒ¬ãƒ³ãƒåŒ–]
    F --> G[ãƒ™ã‚¯ãƒˆãƒ«åŒ–]
    G --> H[ãƒ¢ãƒ‡ãƒ«å…¥åŠ›]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#e8f5e9
    style F fill:#fce4ec
    style G fill:#e1f5fe
    style H fill:#c8e6c9
</div>

<h3>1.1.1 ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆTokenizationï¼‰</h3>

<p><strong>ãƒˆãƒ¼ã‚¯ãƒ³åŒ–</strong>ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’æ„å‘³ã®ã‚ã‚‹å˜ä½ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã«åˆ†å‰²ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã§ã™ã€‚</p>

<h4>å˜èªãƒ¬ãƒ™ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–</h4>

<pre><code class="language-python">import nltk
from nltk.tokenize import word_tokenize, sent_tokenize

# åˆå›ã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# nltk.download('punkt')

text = """Natural Language Processing (NLP) is a field of AI.
It helps computers understand human language."""

# æ–‡åˆ†å‰²
sentences = sent_tokenize(text)
print("=== æ–‡åˆ†å‰² ===")
for i, sent in enumerate(sentences, 1):
    print(f"{i}. {sent}")

# å˜èªåˆ†å‰²
words = word_tokenize(text)
print("\n=== å˜èªãƒˆãƒ¼ã‚¯ãƒ³åŒ– ===")
print(f"ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(words)}")
print(f"æœ€åˆã®10ãƒˆãƒ¼ã‚¯ãƒ³: {words[:10]}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== æ–‡åˆ†å‰² ===
1. Natural Language Processing (NLP) is a field of AI.
2. It helps computers understand human language.

=== å˜èªãƒˆãƒ¼ã‚¯ãƒ³åŒ– ===
ãƒˆãƒ¼ã‚¯ãƒ³æ•°: 20
æœ€åˆã®10ãƒˆãƒ¼ã‚¯ãƒ³: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of']
</code></pre>

<h4>ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³åŒ–</h4>

<pre><code class="language-python">from transformers import BertTokenizer

# BERTã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

text = "Tokenization is fundamental for NLP preprocessing."

# ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
tokens = tokenizer.tokenize(text)
print("=== ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆBERTï¼‰===")
print(f"ãƒˆãƒ¼ã‚¯ãƒ³: {tokens}")

# IDã«å¤‰æ›
token_ids = tokenizer.encode(text, add_special_tokens=True)
print(f"\nãƒˆãƒ¼ã‚¯ãƒ³ID: {token_ids}")

# ãƒ‡ã‚³ãƒ¼ãƒ‰
decoded = tokenizer.decode(token_ids)
print(f"ãƒ‡ã‚³ãƒ¼ãƒ‰: {decoded}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆBERTï¼‰===
ãƒˆãƒ¼ã‚¯ãƒ³: ['token', '##ization', 'is', 'fundamental', 'for', 'nl', '##p', 'pre', '##processing', '.']

ãƒˆãƒ¼ã‚¯ãƒ³ID: [101, 19204, 3989, 2003, 8148, 2005, 17953, 2243, 3653, 6693, 1012, 102]
ãƒ‡ã‚³ãƒ¼ãƒ‰: [CLS] tokenization is fundamental for nlp preprocessing. [SEP]
</code></pre>

<h4>æ–‡å­—ãƒ¬ãƒ™ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–</h4>

<pre><code class="language-python">text = "Hello, NLP!"

# æ–‡å­—ãƒ¬ãƒ™ãƒ«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
char_tokens = list(text)
print("=== æ–‡å­—ãƒ¬ãƒ™ãƒ«ãƒˆãƒ¼ã‚¯ãƒ³åŒ– ===")
print(f"ãƒˆãƒ¼ã‚¯ãƒ³: {char_tokens}")
print(f"ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(char_tokens)}")

# ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªæ–‡å­—
unique_chars = sorted(set(char_tokens))
print(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯æ–‡å­—æ•°: {len(unique_chars)}")
print(f"èªå½™: {unique_chars}")
</code></pre>

<h3>ãƒˆãƒ¼ã‚¯ãƒ³åŒ–æ‰‹æ³•ã®æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>ç²’åº¦</th>
<th>é•·æ‰€</th>
<th>çŸ­æ‰€</th>
<th>ç”¨é€”</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å˜èªãƒ¬ãƒ™ãƒ«</strong></td>
<td>å˜èª</td>
<td>è§£é‡ˆã—ã‚„ã™ã„</td>
<td>èªå½™ã‚µã‚¤ã‚ºå¤§ã€OOVå•é¡Œ</td>
<td>ä¼çµ±çš„NLP</td>
</tr>
<tr>
<td><strong>ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰</strong></td>
<td>å˜èªã®ä¸€éƒ¨</td>
<td>OOVå¯¾å¿œã€èªå½™åœ§ç¸®</td>
<td>ã‚„ã‚„è¤‡é›‘</td>
<td>ç¾ä»£ã®Transformer</td>
</tr>
<tr>
<td><strong>æ–‡å­—ãƒ¬ãƒ™ãƒ«</strong></td>
<td>æ–‡å­—</td>
<td>èªå½™æœ€å°ã€OOVãªã—</td>
<td>ç³»åˆ—é•·å¢—å¤§</td>
<td>è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°</td>
</tr>
</tbody>
</table>

<h3>1.1.2 æ­£è¦åŒ–ã¨æ¨™æº–åŒ–</h3>

<pre><code class="language-python">import re
import string

def normalize_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–"""
    # å°æ–‡å­—åŒ–
    text = text.lower()

    # URLã®é™¤å»
    text = re.sub(r'http\S+|www\S+', '', text)

    # ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã®é™¤å»
    text = re.sub(r'@\w+', '', text)

    # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã®é™¤å»
    text = re.sub(r'#\w+', '', text)

    # æ•°å­—ã®é™¤å»
    text = re.sub(r'\d+', '', text)

    # å¥èª­ç‚¹ã®é™¤å»
    text = text.translate(str.maketrans('', '', string.punctuation))

    # è¤‡æ•°ã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«
    text = re.sub(r'\s+', ' ', text).strip()

    return text

# ãƒ†ã‚¹ãƒˆ
raw_text = """Check out https://example.com! @user tweeted #NLP is AMAZING!!
Contact us at 123-456-7890."""

normalized = normalize_text(raw_text)

print("=== ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ– ===")
print(f"å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ:\n{raw_text}\n")
print(f"æ­£è¦åŒ–å¾Œ:\n{normalized}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ– ===
å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ:
Check out https://example.com! @user tweeted #NLP is AMAZING!!
Contact us at 123-456-7890.

æ­£è¦åŒ–å¾Œ:
check out tweeted is amazing contact us at
</code></pre>

<h3>1.1.3 ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»</h3>

<p><strong>ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰</strong>ã¯ã€é »å‡ºã™ã‚‹ãŒæ„å‘³çš„ã«é‡è¦ã§ãªã„å˜èªï¼ˆã€Œã¯ã€ã€Œã®ã€ã€Œaã€ã€Œtheã€ãªã©ï¼‰ã§ã™ã€‚</p>

<pre><code class="language-python">import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# nltk.download('stopwords')

text = "Natural language processing is a subfield of artificial intelligence that focuses on the interaction between computers and humans."

# ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
tokens = word_tokenize(text.lower())

# è‹±èªã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
stop_words = set(stopwords.words('english'))

# ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»
filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]

print("=== ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å» ===")
print(f"å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(tokens)}")
print(f"å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³: {tokens[:15]}")
print(f"\né™¤å»å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(filtered_tokens)}")
print(f"é™¤å»å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³: {filtered_tokens}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å» ===
å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: 21
å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³: ['natural', 'language', 'processing', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between']

é™¤å»å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: 10
é™¤å»å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³: ['natural', 'language', 'processing', 'subfield', 'artificial', 'intelligence', 'focuses', 'interaction', 'computers', 'humans']
</code></pre>

<h3>1.1.4 ã‚¹ãƒ†ãƒŸãƒ³ã‚°ã¨ãƒ¬ãƒ³ãƒåŒ–</h3>

<p><strong>ã‚¹ãƒ†ãƒŸãƒ³ã‚°ï¼ˆStemmingï¼‰</strong>: å˜èªã‚’èªå¹¹ã«å¤‰æ›ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰<br>
<strong>ãƒ¬ãƒ³ãƒåŒ–ï¼ˆLemmatizationï¼‰</strong>: å˜èªã‚’è¾æ›¸å½¢ã«å¤‰æ›ï¼ˆè¾æ›¸ãƒ™ãƒ¼ã‚¹ï¼‰</p>

<pre><code class="language-python">from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.tokenize import word_tokenize
import nltk

# nltk.download('wordnet')
# nltk.download('omw-1.4')

# ã‚¹ãƒ†ãƒŸãƒ³ã‚°
stemmer = PorterStemmer()

# ãƒ¬ãƒ³ãƒåŒ–
lemmatizer = WordNetLemmatizer()

words = ["running", "runs", "ran", "easily", "fairly", "better", "worse"]

print("=== ã‚¹ãƒ†ãƒŸãƒ³ã‚° vs ãƒ¬ãƒ³ãƒåŒ– ===")
print(f"{'å˜èª':<15} {'ã‚¹ãƒ†ãƒŸãƒ³ã‚°':<15} {'ãƒ¬ãƒ³ãƒåŒ–':<15}")
print("-" * 45)

for word in words:
    stemmed = stemmer.stem(word)
    lemmatized = lemmatizer.lemmatize(word, pos='v')  # å‹•è©ã¨ã—ã¦å‡¦ç†
    print(f"{word:<15} {stemmed:<15} {lemmatized:<15}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ã‚¹ãƒ†ãƒŸãƒ³ã‚° vs ãƒ¬ãƒ³ãƒåŒ– ===
å˜èª             ã‚¹ãƒ†ãƒŸãƒ³ã‚°       ãƒ¬ãƒ³ãƒåŒ–
---------------------------------------------
running         run             run
runs            run             run
ran             ran             run
easily          easili          easily
fairly          fairli          fairly
better          better          better
worse           wors            worse
</code></pre>

<blockquote>
<p><strong>é¸æŠã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³</strong>: ã‚¹ãƒ†ãƒŸãƒ³ã‚°ã¯é«˜é€Ÿã ãŒç²—ã„ã€‚ãƒ¬ãƒ³ãƒåŒ–ã¯æ­£ç¢ºã ãŒé…ã„ã€‚ã‚¿ã‚¹ã‚¯ã®è¦æ±‚ã«å¿œã˜ã¦é¸æŠã€‚</p>
</blockquote>

<hr>

<h2>1.2 å˜èªã®è¡¨ç¾</h2>

<h3>1.2.1 One-Hot Encoding</h3>

<p><strong>One-Hot Encoding</strong>ã¯ã€å„å˜èªã‚’èªå½™ã‚µã‚¤ã‚ºã®æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã§è¡¨ç¾ã—ã€è©²å½“ä½ç½®ã®ã¿1ã€ä»–ã¯0ã¨ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<pre><code class="language-python">import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

sentences = ["I love NLP", "NLP is amazing", "I love AI"]

# ã™ã¹ã¦ã®å˜èªã‚’é›†ã‚ã‚‹
words = ' '.join(sentences).lower().split()
unique_words = sorted(set(words))

print("=== One-Hot Encoding ===")
print(f"èªå½™: {unique_words}")
print(f"èªå½™ã‚µã‚¤ã‚º: {len(unique_words)}")

# One-Hot Encodingã®ä½œæˆ
word_to_idx = {word: idx for idx, word in enumerate(unique_words)}
idx_to_word = {idx: word for word, idx in word_to_idx.items()}

def one_hot_encode(word, vocab_size):
    """å˜èªã‚’One-Hotãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›"""
    vector = np.zeros(vocab_size)
    if word in word_to_idx:
        vector[word_to_idx[word]] = 1
    return vector

# å„å˜èªã®One-Hotè¡¨ç¾
print("\nå˜èªã®One-Hotè¡¨ç¾:")
for word in ["nlp", "love", "ai"]:
    vector = one_hot_encode(word, len(unique_words))
    print(f"{word}: {vector}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== One-Hot Encoding ===
èªå½™: ['ai', 'amazing', 'i', 'is', 'love', 'nlp']
èªå½™ã‚µã‚¤ã‚º: 6

å˜èªã®One-Hotè¡¨ç¾:
nlp: [0. 0. 0. 0. 0. 1.]
love: [0. 0. 0. 0. 1. 0.]
ai: [1. 0. 0. 0. 0. 0.]
</code></pre>

<blockquote>
<p><strong>å•é¡Œç‚¹</strong>: èªå½™ãŒå¢—ãˆã‚‹ã¨ãƒ™ã‚¯ãƒˆãƒ«ãŒå·¨å¤§åŒ–ï¼ˆæ¬¡å…ƒã®å‘ªã„ï¼‰ã€å˜èªé–“ã®æ„å‘³çš„é–¢ä¿‚ã‚’è¡¨ç¾ã§ããªã„ã€‚</p>
</blockquote>

<h3>1.2.2 Bag of Words (BoW)</h3>

<p><strong>Bag of Words</strong>ã¯ã€æ–‡æ›¸ã‚’å˜èªã®å‡ºç¾é »åº¦ãƒ™ã‚¯ãƒˆãƒ«ã§è¡¨ç¾ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    "I love machine learning",
    "I love deep learning",
    "Deep learning is powerful",
    "Machine learning is interesting"
]

# BoWãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)

# èªå½™
vocab = vectorizer.get_feature_names_out()

print("=== Bag of Words ===")
print(f"èªå½™: {vocab}")
print(f"èªå½™ã‚µã‚¤ã‚º: {len(vocab)}")
print(f"\næ–‡æ›¸-å˜èªè¡Œåˆ—:")
print(X.toarray())

# æ–‡æ›¸ã”ã¨ã®è¡¨ç¾
import pandas as pd
df = pd.DataFrame(X.toarray(), columns=vocab)
print("\næ–‡æ›¸-å˜èªè¡Œåˆ—ï¼ˆDataFrameï¼‰:")
print(df)
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Bag of Words ===
èªå½™: ['deep' 'interesting' 'is' 'learning' 'love' 'machine' 'powerful']
èªå½™ã‚µã‚¤ã‚º: 7

æ–‡æ›¸-å˜èªè¡Œåˆ—:
[[0 0 0 1 1 1 0]
 [1 0 0 1 1 0 0]
 [1 0 1 1 0 0 1]
 [0 1 1 1 0 1 0]]

æ–‡æ›¸-å˜èªè¡Œåˆ—ï¼ˆDataFrameï¼‰:
   deep  interesting  is  learning  love  machine  powerful
0     0            0   0         1     1        1         0
1     1            0   0         1     1        0         0
2     1            0   1         1     0        0         1
3     0            1   1         1     0        1         0
</code></pre>

<h3>1.2.3 TF-IDF</h3>

<p><strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong>ã¯ã€å˜èªã®é‡è¦åº¦ã‚’è©•ä¾¡ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<p>$$
\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t)
$$</p>

<p>$$
\text{IDF}(t) = \log\left(\frac{N}{df(t)}\right)
$$</p>

<ul>
<li>$\text{TF}(t, d)$: æ–‡æ›¸$d$ã«ãŠã‘ã‚‹å˜èª$t$ã®å‡ºç¾é »åº¦</li>
<li>$N$: ç·æ–‡æ›¸æ•°</li>
<li>$df(t)$: å˜èª$t$ã‚’å«ã‚€æ–‡æ›¸æ•°</li>
</ul>

<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
    "The cat sat on the mat",
    "The dog sat on the log",
    "Cats and dogs are animals",
    "The mat is on the floor"
]

# TF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼
tfidf_vectorizer = TfidfVectorizer()
X_tfidf = tfidf_vectorizer.fit_transform(corpus)

# èªå½™
vocab = tfidf_vectorizer.get_feature_names_out()

print("=== TF-IDF ===")
print(f"èªå½™: {vocab}")
print(f"\nTF-IDFè¡Œåˆ—:")
df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=vocab)
print(df_tfidf.round(3))

# æœ€ã‚‚é‡è¦ãªå˜èªï¼ˆæ–‡æ›¸0ï¼‰
doc_idx = 0
feature_scores = list(zip(vocab, X_tfidf.toarray()[doc_idx]))
sorted_scores = sorted(feature_scores, key=lambda x: x[1], reverse=True)

print(f"\næ–‡æ›¸ {doc_idx} ã®é‡è¦å˜èª Top 3:")
for word, score in sorted_scores[:3]:
    print(f"  {word}: {score:.3f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== TF-IDF ===
èªå½™: ['and' 'animals' 'are' 'cat' 'cats' 'dog' 'dogs' 'floor' 'is' 'log' 'mat' 'on' 'sat' 'the']

TF-IDFè¡Œåˆ—:
    and  animals   are   cat  cats   dog  dogs  floor    is   log   mat    on   sat   the
0  0.00    0.000  0.00  0.48  0.00  0.00  0.00   0.00  0.00  0.00  0.48  0.35  0.35  0.58
1  0.00    0.000  0.00  0.00  0.00  0.50  0.00   0.00  0.00  0.50  0.00  0.36  0.36  0.60
2  0.41    0.410  0.41  0.00  0.31  0.00  0.31   0.00  0.00  0.00  0.00  0.00  0.00  0.52
3  0.00    0.000  0.00  0.00  0.00  0.00  0.00   0.50  0.50  0.00  0.38  0.28  0.00  0.55

æ–‡æ›¸ 0 ã®é‡è¦å˜èª Top 3:
  the: 0.576
  cat: 0.478
  mat: 0.478
</code></pre>

<h3>1.2.4 N-gram ãƒ¢ãƒ‡ãƒ«</h3>

<p><strong>N-gram</strong>ã¯ã€é€£ç¶šã™ã‚‹Nå€‹ã®å˜èªã®çµ„ã¿åˆã‚ã›ã§ã™ã€‚</p>

<pre><code class="language-python">from sklearn.feature_extraction.text import CountVectorizer

text = ["Natural language processing is fun"]

# Unigram (1-gram)
unigram_vec = CountVectorizer(ngram_range=(1, 1))
unigrams = unigram_vec.fit_transform(text)
print("=== Unigram (1-gram) ===")
print(unigram_vec.get_feature_names_out())

# Bigram (2-gram)
bigram_vec = CountVectorizer(ngram_range=(2, 2))
bigrams = bigram_vec.fit_transform(text)
print("\n=== Bigram (2-gram) ===")
print(bigram_vec.get_feature_names_out())

# Trigram (3-gram)
trigram_vec = CountVectorizer(ngram_range=(3, 3))
trigrams = trigram_vec.fit_transform(text)
print("\n=== Trigram (3-gram) ===")
print(trigram_vec.get_feature_names_out())

# 1-gramã‹ã‚‰3-gramã¾ã§
combined_vec = CountVectorizer(ngram_range=(1, 3))
combined = combined_vec.fit_transform(text)
print(f"\n=== Combined (1-3 gram) ===")
print(f"ç·ç‰¹å¾´é‡æ•°: {len(combined_vec.get_feature_names_out())}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Unigram (1-gram) ===
['fun' 'is' 'language' 'natural' 'processing']

=== Bigram (2-gram) ===
['is fun' 'language processing' 'natural language' 'processing is']

=== Trigram (3-gram) ===
['language processing is' 'natural language processing' 'processing is fun']

=== Combined (1-3 gram) ===
ç·ç‰¹å¾´é‡æ•°: 12
</code></pre>

<hr>

<h2>1.3 Word Embeddingsï¼ˆå˜èªåŸ‹ã‚è¾¼ã¿ï¼‰</h2>

<h3>å˜èªåŸ‹ã‚è¾¼ã¿ã¨ã¯</h3>

<p><strong>Word Embeddings</strong>ã¯ã€å˜èªã‚’ä½æ¬¡å…ƒã®å¯†ãªãƒ™ã‚¯ãƒˆãƒ«ã§è¡¨ç¾ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚æ„å‘³çš„ã«é¡ä¼¼ã—ãŸå˜èªãŒè¿‘ã„ä½ç½®ã«é…ç½®ã•ã‚Œã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[One-Hot<br>ç–ãƒ»é«˜æ¬¡å…ƒ] --> B[Word2Vec<br>å¯†ãƒ»ä½æ¬¡å…ƒ]
    A --> C[GloVe<br>å¯†ãƒ»ä½æ¬¡å…ƒ]
    A --> D[FastText<br>å¯†ãƒ»ä½æ¬¡å…ƒ]

    style A fill:#ffebee
    style B fill:#e8f5e9
    style C fill:#e3f2fd
    style D fill:#fff3e0
</div>

<h3>1.3.1 Word2Vec</h3>

<p><strong>Word2Vec</strong>ã¯ã€å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰å˜èªã®åˆ†æ•£è¡¨ç¾ã‚’å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚2ã¤ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒã‚ã‚Šã¾ã™ï¼š</p>

<ul>
<li><strong>CBOW (Continuous Bag of Words)</strong>: å‘¨è¾ºå˜èªã‹ã‚‰ä¸­å¿ƒå˜èªã‚’äºˆæ¸¬</li>
<li><strong>Skip-gram</strong>: ä¸­å¿ƒå˜èªã‹ã‚‰å‘¨è¾ºå˜èªã‚’äºˆæ¸¬</li>
</ul>

<pre><code class="language-python">from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import numpy as np

# ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‘ã‚¹
corpus = [
    "Natural language processing with deep learning",
    "Machine learning is a subset of artificial intelligence",
    "Deep learning uses neural networks",
    "Neural networks are inspired by biological neurons",
    "Natural language understanding requires context",
    "Context is important in language processing"
]

# ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
tokenized_corpus = [word_tokenize(sent.lower()) for sent in corpus]

# Word2Vecãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
# Skip-gramãƒ¢ãƒ‡ãƒ« (sg=1), CBOW (sg=0)
model = Word2Vec(
    sentences=tokenized_corpus,
    vector_size=100,  # åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
    window=5,         # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
    min_count=1,      # æœ€å°å‡ºç¾å›æ•°
    sg=1,             # Skip-gram
    workers=4
)

print("=== Word2Vec ===")
print(f"èªå½™ã‚µã‚¤ã‚º: {len(model.wv)}")
print(f"åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {model.wv.vector_size}")

# å˜èªãƒ™ã‚¯ãƒˆãƒ«ã®å–å¾—
word = "learning"
vector = model.wv[word]
print(f"\n'{word}' ã®ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆæœ€åˆã®10æ¬¡å…ƒï¼‰:")
print(vector[:10])

# é¡ä¼¼å˜èªã®æ¤œç´¢
similar_words = model.wv.most_similar("learning", topn=5)
print(f"\n'{word}' ã«é¡ä¼¼ã—ãŸå˜èª:")
for word, similarity in similar_words:
    print(f"  {word}: {similarity:.3f}")

# å˜èªã®é¡ä¼¼åº¦
similarity = model.wv.similarity("neural", "networks")
print(f"\n'neural' ã¨ 'networks' ã®é¡ä¼¼åº¦: {similarity:.3f}")

# å˜èªæ¼”ç®—ï¼ˆKing - Man + Woman â‰ˆ Queen ã®ä¾‹ï¼‰
# ç°¡å˜ãªä¾‹: deep - neural + machine
try:
    result = model.wv.most_similar(
        positive=['deep', 'machine'],
        negative=['neural'],
        topn=3
    )
    print("\nå˜èªæ¼”ç®— (deep - neural + machine):")
    for word, score in result:
        print(f"  {word}: {score:.3f}")
except:
    print("\nå˜èªæ¼”ç®—: ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== Word2Vec ===
èªå½™ã‚µã‚¤ã‚º: 27
åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: 100

'learning' ã®ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆæœ€åˆã®10æ¬¡å…ƒï¼‰:
[-0.00234  0.00891 -0.00156  0.00423 -0.00678  0.00234  0.00567 -0.00123  0.00789 -0.00345]

'learning' ã«é¡ä¼¼ã—ãŸå˜èª:
  deep: 0.876
  neural: 0.823
  processing: 0.791
  networks: 0.765
  natural: 0.734

'neural' ã¨ 'networks' ã®é¡ä¼¼åº¦: 0.892
</code></pre>

<h3>1.3.2 GloVe (Global Vectors)</h3>

<p><strong>GloVe</strong>ã¯ã€å˜èªã®å…±èµ·çµ±è¨ˆã‚’åˆ©ç”¨ã—ã¦åŸ‹ã‚è¾¼ã¿ã‚’å­¦ç¿’ã—ã¾ã™ã€‚Word2Vecã¨ã¯ç•°ãªã‚Šã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªå…±èµ·æƒ…å ±ã‚’æ´»ç”¨ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">import gensim.downloader as api
import numpy as np

# äº‹å‰å­¦ç¿’æ¸ˆã¿GloVeãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆåˆå›ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰
print("GloVeãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
glove_model = api.load("glove-wiki-gigaword-100")

print("\n=== GloVe (äº‹å‰å­¦ç¿’æ¸ˆã¿) ===")
print(f"èªå½™ã‚µã‚¤ã‚º: {len(glove_model)}")
print(f"åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {glove_model.vector_size}")

# å˜èªãƒ™ã‚¯ãƒˆãƒ«
word = "computer"
vector = glove_model[word]
print(f"\n'{word}' ã®ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆæœ€åˆã®10æ¬¡å…ƒï¼‰:")
print(vector[:10])

# é¡ä¼¼å˜èª
similar_words = glove_model.most_similar(word, topn=5)
print(f"\n'{word}' ã«é¡ä¼¼ã—ãŸå˜èª:")
for w, sim in similar_words:
    print(f"  {w}: {sim:.3f}")

# æœ‰åãªå˜èªæ¼”ç®—ï¼šKing - Man + Woman â‰ˆ Queen
result = glove_model.most_similar(
    positive=['king', 'woman'],
    negative=['man'],
    topn=5
)
print("\nå˜èªæ¼”ç®— (King - Man + Woman):")
for w, sim in result:
    print(f"  {w}: {sim:.3f}")

# é¡ä¼¼åº¦è¨ˆç®—
pairs = [
    ("good", "bad"),
    ("good", "excellent"),
    ("cat", "dog"),
    ("cat", "car")
]
print("\nå˜èªãƒšã‚¢ã®é¡ä¼¼åº¦:")
for w1, w2 in pairs:
    sim = glove_model.similarity(w1, w2)
    print(f"  {w1} - {w2}: {sim:.3f}")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== GloVe (äº‹å‰å­¦ç¿’æ¸ˆã¿) ===
èªå½™ã‚µã‚¤ã‚º: 400000
åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: 100

'computer' ã®ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆæœ€åˆã®10æ¬¡å…ƒï¼‰:
[ 0.45893  0.19521 -0.23456  0.67234 -0.34521  0.12345  0.89012 -0.45678  0.23456 -0.78901]

'computer' ã«é¡ä¼¼ã—ãŸå˜èª:
  computers: 0.887
  software: 0.756
  hardware: 0.734
  pc: 0.712
  system: 0.689

å˜èªæ¼”ç®— (King - Man + Woman):
  queen: 0.768
  monarch: 0.654
  princess: 0.621
  crown: 0.598
  prince: 0.587

å˜èªãƒšã‚¢ã®é¡ä¼¼åº¦:
  good - bad: 0.523
  good - excellent: 0.791
  cat - dog: 0.821
  cat - car: 0.234
</code></pre>

<h3>1.3.3 FastText</h3>

<p><strong>FastText</strong>ã¯ã€ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰æƒ…å ±ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€æœªçŸ¥èªï¼ˆOOVï¼‰ã«å¯¾å¿œã§ãã‚‹å˜èªåŸ‹ã‚è¾¼ã¿ã§ã™ã€‚</p>

<pre><code class="language-python">from gensim.models import FastText

# ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‘ã‚¹
sentences = [
    ["machine", "learning", "is", "awesome"],
    ["deep", "learning", "with", "neural", "networks"],
    ["natural", "language", "processing"],
    ["fasttext", "handles", "unknown", "words"]
]

# FastTextãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
ft_model = FastText(
    sentences=sentences,
    vector_size=100,
    window=3,
    min_count=1,
    sg=1  # Skip-gram
)

print("=== FastText ===")
print(f"èªå½™ã‚µã‚¤ã‚º: {len(ft_model.wv)}")

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹å˜èª
word = "learning"
vector = ft_model.wv[word]
print(f"\n'{word}' ã®ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆæœ€åˆã®5æ¬¡å…ƒï¼‰:")
print(vector[:5])

# æœªçŸ¥èªï¼ˆOOVï¼‰ã§ã‚‚ãƒ™ã‚¯ãƒˆãƒ«å–å¾—å¯èƒ½
unknown_word = "machinelearning"  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ãªã„
try:
    unknown_vector = ft_model.wv[unknown_word]
    print(f"\næœªçŸ¥èª '{unknown_word}' ã®ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆæœ€åˆã®5æ¬¡å…ƒï¼‰:")
    print(unknown_vector[:5])
    print("âœ“ FastTextã¯æœªçŸ¥èªã«ã‚‚å¯¾å¿œã§ãã¾ã™")
except:
    print(f"\næœªçŸ¥èª '{unknown_word}' ã¯ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã§ãã¾ã›ã‚“")

# é¡ä¼¼å˜èª
similar = ft_model.wv.most_similar("learning", topn=3)
print(f"\n'{word}' ã«é¡ä¼¼ã—ãŸå˜èª:")
for w, sim in similar:
    print(f"  {w}: {sim:.3f}")
</code></pre>

<h3>Word2Vec vs GloVe vs FastText</h3>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>å­¦ç¿’æ–¹æ³•</th>
<th>é•·æ‰€</th>
<th>çŸ­æ‰€</th>
<th>OOVå¯¾å¿œ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Word2Vec</strong></td>
<td>å±€æ‰€çš„ãªå…±èµ·ï¼ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼‰</td>
<td>é«˜é€Ÿã€åŠ¹ç‡çš„</td>
<td>ã‚°ãƒ­ãƒ¼ãƒãƒ«çµ±è¨ˆã‚’ç„¡è¦–</td>
<td>ä¸å¯</td>
</tr>
<tr>
<td><strong>GloVe</strong></td>
<td>ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªå…±èµ·è¡Œåˆ—</td>
<td>ã‚°ãƒ­ãƒ¼ãƒãƒ«çµ±è¨ˆæ´»ç”¨</td>
<td>ã‚„ã‚„é…ã„</td>
<td>ä¸å¯</td>
</tr>
<tr>
<td><strong>FastText</strong></td>
<td>ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰æƒ…å ±</td>
<td>OOVå¯¾å¿œã€å½¢æ…‹ç´ æƒ…å ±</td>
<td>ã‚„ã‚„è¤‡é›‘</td>
<td>å¯èƒ½</td>
</tr>
</tbody>
</table>

<hr>

<h2>1.4 æ—¥æœ¬èªNLP</h2>

<h3>æ—¥æœ¬èªã®ç‰¹å¾´ã¨èª²é¡Œ</h3>

<p>æ—¥æœ¬èªã¯è‹±èªã¨ç•°ãªã‚Šã€ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ï¼š</p>

<ul>
<li>å˜èªé–“ã«ã‚¹ãƒšãƒ¼ã‚¹ãŒãªã„ï¼ˆåˆ†ã‹ã¡æ›¸ããŒå¿…è¦ï¼‰</li>
<li>è¤‡æ•°ã®æ–‡å­—ç¨®ï¼ˆã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã€ãƒ­ãƒ¼ãƒå­—ï¼‰</li>
<li>åŒã˜æ„å‘³ã§ã‚‚è¡¨è¨˜ã‚†ã‚Œï¼ˆä¾‹ï¼šã€Œã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã€ã€Œã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã€ï¼‰</li>
<li>æ–‡è„ˆä¾å­˜ã®æ„å‘³æ±ºå®š</li>
</ul>

<h3>1.4.1 MeCabã«ã‚ˆã‚‹å½¢æ…‹ç´ è§£æ</h3>

<pre><code class="language-python">import MeCab

# MeCabã®åˆæœŸåŒ–
mecab = MeCab.Tagger()

text = "è‡ªç„¶è¨€èªå‡¦ç†ã¯äººå·¥çŸ¥èƒ½ã®ä¸€åˆ†é‡ã§ã™ã€‚"

# å½¢æ…‹ç´ è§£æ
print("=== MeCab å½¢æ…‹ç´ è§£æ ===")
print(mecab.parse(text))

# åˆ†ã‹ã¡æ›¸ãï¼ˆå˜èªåˆ†å‰²ï¼‰
mecab_wakati = MeCab.Tagger("-Owakati")
wakati_text = mecab_wakati.parse(text).strip()
print(f"åˆ†ã‹ã¡æ›¸ã: {wakati_text}")

# å“è©æƒ…å ±ã®æŠ½å‡º
node = mecab.parseToNode(text)
words = []
pos_tags = []

while node:
    features = node.feature.split(',')
    if node.surface:
        words.append(node.surface)
        pos_tags.append(features[0])
    node = node.next

print("\nå˜èªã¨å“è©:")
for word, pos in zip(words, pos_tags):
    print(f"  {word}: {pos}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== MeCab å½¢æ…‹ç´ è§£æ ===
è‡ªç„¶	åè©,ä¸€èˆ¬,*,*,*,*,è‡ªç„¶,ã‚·ã‚¼ãƒ³,ã‚·ã‚¼ãƒ³
è¨€èª	åè©,ä¸€èˆ¬,*,*,*,*,è¨€èª,ã‚²ãƒ³ã‚´,ã‚²ãƒ³ã‚´
å‡¦ç†	åè©,ã‚µå¤‰æ¥ç¶š,*,*,*,*,å‡¦ç†,ã‚·ãƒ§ãƒª,ã‚·ãƒ§ãƒª
ã¯	åŠ©è©,ä¿‚åŠ©è©,*,*,*,*,ã¯,ãƒ,ãƒ¯
äººå·¥	åè©,ä¸€èˆ¬,*,*,*,*,äººå·¥,ã‚¸ãƒ³ã‚³ã‚¦,ã‚¸ãƒ³ã‚³ãƒ¼
çŸ¥èƒ½	åè©,ä¸€èˆ¬,*,*,*,*,çŸ¥èƒ½,ãƒãƒã‚¦,ãƒãƒãƒ¼
ã®	åŠ©è©,é€£ä½“åŒ–,*,*,*,*,ã®,ãƒ,ãƒ
ä¸€	åè©,æ•°,*,*,*,*,ä¸€,ã‚¤ãƒ,ã‚¤ãƒ
åˆ†é‡	åè©,ä¸€èˆ¬,*,*,*,*,åˆ†é‡,ãƒ–ãƒ³ãƒ¤,ãƒ–ãƒ³ãƒ¤
ã§ã™	åŠ©å‹•è©,*,*,*,ç‰¹æ®Šãƒ»ãƒ‡ã‚¹,åŸºæœ¬å½¢,ã§ã™,ãƒ‡ã‚¹,ãƒ‡ã‚¹
ã€‚	è¨˜å·,å¥ç‚¹,*,*,*,*,ã€‚,ã€‚,ã€‚
EOS

åˆ†ã‹ã¡æ›¸ã: è‡ªç„¶ è¨€èª å‡¦ç† ã¯ äººå·¥ çŸ¥èƒ½ ã® ä¸€ åˆ†é‡ ã§ã™ ã€‚

å˜èªã¨å“è©:
  è‡ªç„¶: åè©
  è¨€èª: åè©
  å‡¦ç†: åè©
  ã¯: åŠ©è©
  äººå·¥: åè©
  çŸ¥èƒ½: åè©
  ã®: åŠ©è©
  ä¸€: åè©
  åˆ†é‡: åè©
  ã§ã™: åŠ©å‹•è©
  ã€‚: è¨˜å·
</code></pre>

<h3>1.4.2 SudachiPyã«ã‚ˆã‚‹å½¢æ…‹ç´ è§£æ</h3>

<p><strong>SudachiPy</strong>ã¯ã€è¤‡æ•°ã®åˆ†å‰²ãƒ¢ãƒ¼ãƒ‰ï¼ˆA: çŸ­å˜ä½ã€B: ä¸­å˜ä½ã€C: é•·å˜ä½ï¼‰ã‚’æä¾›ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">from sudachipy import tokenizer
from sudachipy import dictionary

# Sudachiã®åˆæœŸåŒ–
tokenizer_obj = dictionary.Dictionary().create()

text = "æ±äº¬éƒ½æ¸‹è°·åŒºã«è¡Œãã¾ã—ãŸã€‚"

print("=== SudachiPy åˆ†å‰²ãƒ¢ãƒ¼ãƒ‰æ¯”è¼ƒ ===\n")

# ãƒ¢ãƒ¼ãƒ‰Aï¼ˆçŸ­å˜ä½ï¼‰
mode_a = tokenizer_obj.tokenize(text, tokenizer.Tokenizer.SplitMode.A)
print("ãƒ¢ãƒ¼ãƒ‰Aï¼ˆçŸ­å˜ä½ï¼‰:")
print([m.surface() for m in mode_a])

# ãƒ¢ãƒ¼ãƒ‰Bï¼ˆä¸­å˜ä½ï¼‰
mode_b = tokenizer_obj.tokenize(text, tokenizer.Tokenizer.SplitMode.B)
print("\nãƒ¢ãƒ¼ãƒ‰Bï¼ˆä¸­å˜ä½ï¼‰:")
print([m.surface() for m in mode_b])

# ãƒ¢ãƒ¼ãƒ‰Cï¼ˆé•·å˜ä½ï¼‰
mode_c = tokenizer_obj.tokenize(text, tokenizer.Tokenizer.SplitMode.C)
print("\nãƒ¢ãƒ¼ãƒ‰Cï¼ˆé•·å˜ä½ï¼‰:")
print([m.surface() for m in mode_c])

# è©³ç´°æƒ…å ±
print("\nè©³ç´°æƒ…å ±ï¼ˆãƒ¢ãƒ¼ãƒ‰Bï¼‰:")
for token in mode_b:
    print(f"  è¡¨å±¤å½¢: {token.surface()}")
    print(f"  åŸå½¢: {token.dictionary_form()}")
    print(f"  å“è©: {token.part_of_speech()[0]}")
    print(f"  èª­ã¿: {token.reading_form()}")
    print()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== SudachiPy åˆ†å‰²ãƒ¢ãƒ¼ãƒ‰æ¯”è¼ƒ ===

ãƒ¢ãƒ¼ãƒ‰Aï¼ˆçŸ­å˜ä½ï¼‰:
['æ±äº¬', 'éƒ½', 'æ¸‹è°·', 'åŒº', 'ã«', 'è¡Œã', 'ã¾ã—', 'ãŸ', 'ã€‚']

ãƒ¢ãƒ¼ãƒ‰Bï¼ˆä¸­å˜ä½ï¼‰:
['æ±äº¬éƒ½', 'æ¸‹è°·åŒº', 'ã«', 'è¡Œã', 'ãŸ', 'ã€‚']

ãƒ¢ãƒ¼ãƒ‰Cï¼ˆé•·å˜ä½ï¼‰:
['æ±äº¬éƒ½æ¸‹è°·åŒº', 'ã«', 'è¡Œã', 'ãŸ', 'ã€‚']

è©³ç´°æƒ…å ±ï¼ˆãƒ¢ãƒ¼ãƒ‰Bï¼‰:
  è¡¨å±¤å½¢: æ±äº¬éƒ½
  åŸå½¢: æ±äº¬éƒ½
  å“è©: åè©
  èª­ã¿: ãƒˆã‚¦ã‚­ãƒ§ã‚¦ãƒˆ
  ...
</code></pre>

<h3>1.4.3 æ—¥æœ¬èªã®æ­£è¦åŒ–</h3>

<pre><code class="language-python">import unicodedata

def normalize_japanese(text):
    """æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–"""
    # Unicodeæ­£è¦åŒ–ï¼ˆNFKC: äº’æ›æ–‡å­—ã‚’æ¨™æº–å½¢ã«ï¼‰
    text = unicodedata.normalize('NFKC', text)

    # å…¨è§’è‹±æ•°å­—ã‚’åŠè§’ã«
    text = text.translate(str.maketrans(
        'ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½š',
        '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'
    ))

    # é•·éŸ³è¨˜å·ã®çµ±ä¸€
    text = text.replace('ãƒ¼', '')

    return text

# ãƒ†ã‚¹ãƒˆ
texts = [
    "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿",
    "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼",
    "ï¼¡ï¼©æŠ€è¡“",
    "AIæŠ€è¡“",
    "ï¼‘ï¼’ï¼“ï¼”ï¼•"
]

print("=== æ—¥æœ¬èªæ­£è¦åŒ– ===")
for original in texts:
    normalized = normalize_japanese(original)
    print(f"{original} â†’ {normalized}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== æ—¥æœ¬èªæ­£è¦åŒ– ===
ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ â†’ ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿
ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ â†’ ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿
ï¼¡ï¼©æŠ€è¡“ â†’ AIæŠ€è¡“
AIæŠ€è¡“ â†’ AIæŠ€è¡“
ï¼‘ï¼’ï¼“ï¼”ï¼• â†’ 12345
</code></pre>

<hr>

<h2>1.5 åŸºæœ¬çš„ãªNLPã‚¿ã‚¹ã‚¯</h2>

<h3>1.5.1 æ–‡æ›¸åˆ†é¡</h3>

<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
texts = [
    "Machine learning is a subset of AI",
    "Deep learning uses neural networks",
    "Natural language processing analyzes text",
    "Computer vision recognizes images",
    "Reinforcement learning learns from rewards",
    "Supervised learning uses labeled data",
    "Unsupervised learning finds patterns",
    "NLP understands human language",
    "CNN is used for image classification",
    "RNN is good for sequence data"
]

labels = [
    "ML", "DL", "NLP", "CV", "RL",
    "ML", "ML", "NLP", "CV", "DL"
]

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    texts, labels, test_size=0.3, random_state=42
)

# TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
vectorizer = TfidfVectorizer()
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# ãƒŠã‚¤ãƒ¼ãƒ–ãƒ™ã‚¤ã‚ºåˆ†é¡å™¨
classifier = MultinomialNB()
classifier.fit(X_train_tfidf, y_train)

# äºˆæ¸¬
y_pred = classifier.predict(X_test_tfidf)

# è©•ä¾¡
print("=== æ–‡æ›¸åˆ†é¡ ===")
print(f"ç²¾åº¦: {accuracy_score(y_test, y_pred):.3f}")
print(f"\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_test, y_pred))

# æ–°ã—ã„æ–‡æ›¸ã®åˆ†é¡
new_texts = [
    "Neural networks are powerful",
    "Text mining extracts information"
]
new_tfidf = vectorizer.transform(new_texts)
predictions = classifier.predict(new_tfidf)

print("\næ–°ã—ã„æ–‡æ›¸ã®åˆ†é¡:")
for text, pred in zip(new_texts, predictions):
    print(f"  '{text}' â†’ {pred}")
</code></pre>

<h3>1.5.2 é¡ä¼¼åº¦è¨ˆç®—</h3>

<pre><code class="language-python">from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

documents = [
    "Machine learning is fun",
    "Deep learning is exciting",
    "Natural language processing is interesting",
    "I love pizza and pasta",
    "Python is a great programming language"
]

# TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)

# ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®è¨ˆç®—
similarity_matrix = cosine_similarity(tfidf_matrix)

print("=== æ–‡æ›¸é–“ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ ===\n")
print("é¡ä¼¼åº¦è¡Œåˆ—:")
import pandas as pd
df_sim = pd.DataFrame(
    similarity_matrix,
    index=[f"Doc{i}" for i in range(len(documents))],
    columns=[f"Doc{i}" for i in range(len(documents))]
)
print(df_sim.round(3))

# æœ€ã‚‚é¡ä¼¼ã—ãŸæ–‡æ›¸ãƒšã‚¢
print("\nå„æ–‡æ›¸ã«æœ€ã‚‚é¡ä¼¼ã—ãŸæ–‡æ›¸:")
for i, doc in enumerate(documents):
    # è‡ªåˆ†è‡ªèº«ã‚’é™¤ã
    similarities = similarity_matrix[i].copy()
    similarities[i] = -1
    most_similar_idx = similarities.argmax()
    print(f"Doc{i}: '{doc[:30]}...'")
    print(f"  â†’ Doc{most_similar_idx}: '{documents[most_similar_idx][:30]}...' (é¡ä¼¼åº¦: {similarities[most_similar_idx]:.3f})\n")
</code></pre>

<h3>1.5.3 ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°</h3>

<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import numpy as np

documents = [
    "Machine learning algorithms are powerful",
    "Deep learning uses neural networks",
    "Supervised learning needs labeled data",
    "Pizza is delicious food",
    "I love eating pasta",
    "Italian cuisine is amazing",
    "Python is a programming language",
    "JavaScript is used for web development",
    "Java is object-oriented"
]

# TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
vectorizer = TfidfVectorizer(max_features=20)
X = vectorizer.fit_transform(documents)

# K-Meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
n_clusters = 3
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
clusters = kmeans.fit_predict(X)

print("=== ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° ===\n")
print(f"ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {n_clusters}\n")

# ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã«æ–‡æ›¸ã‚’è¡¨ç¤º
for i in range(n_clusters):
    print(f"ã‚¯ãƒ©ã‚¹ã‚¿ {i}:")
    cluster_docs = [doc for doc, cluster in zip(documents, clusters) if cluster == i]
    for doc in cluster_docs:
        print(f"  - {doc}")
    print()

# ã‚¯ãƒ©ã‚¹ã‚¿ã®ä¸­å¿ƒã«è¿‘ã„å˜èª
feature_names = vectorizer.get_feature_names_out()
print("å„ã‚¯ãƒ©ã‚¹ã‚¿ã®ç‰¹å¾´çš„ãªå˜èªï¼ˆä¸Šä½5å€‹ï¼‰:")
for i in range(n_clusters):
    center = kmeans.cluster_centers_[i]
    top_indices = center.argsort()[-5:][::-1]
    top_words = [feature_names[idx] for idx in top_indices]
    print(f"ã‚¯ãƒ©ã‚¹ã‚¿ {i}: {', '.join(top_words)}")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° ===

ã‚¯ãƒ©ã‚¹ã‚¿æ•°: 3

ã‚¯ãƒ©ã‚¹ã‚¿ 0:
  - Machine learning algorithms are powerful
  - Deep learning uses neural networks
  - Supervised learning needs labeled data

ã‚¯ãƒ©ã‚¹ã‚¿ 1:
  - Pizza is delicious food
  - I love eating pasta
  - Italian cuisine is amazing

ã‚¯ãƒ©ã‚¹ã‚¿ 2:
  - Python is a programming language
  - JavaScript is used for web development
  - Java is object-oriented

å„ã‚¯ãƒ©ã‚¹ã‚¿ã®ç‰¹å¾´çš„ãªå˜èªï¼ˆä¸Šä½5å€‹ï¼‰:
ã‚¯ãƒ©ã‚¹ã‚¿ 0: learning, neural, deep, machine, supervised
ã‚¯ãƒ©ã‚¹ã‚¿ 1: italian, food, pizza, pasta, cuisine
ã‚¯ãƒ©ã‚¹ã‚¿ 2: programming, language, python, java, javascript
</code></pre>

<hr>

<h2>1.6 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†</strong></p>
<ul>
<li>ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆå˜èªã€ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã€æ–‡å­—ãƒ¬ãƒ™ãƒ«ï¼‰</li>
<li>æ­£è¦åŒ–ã¨æ¨™æº–åŒ–</li>
<li>ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»</li>
<li>ã‚¹ãƒ†ãƒŸãƒ³ã‚°ã¨ãƒ¬ãƒ³ãƒåŒ–</li>
</ul></li>

<li><p><strong>å˜èªã®è¡¨ç¾</strong></p>
<ul>
<li>One-Hot Encoding: ã‚·ãƒ³ãƒ—ãƒ«ã ãŒé«˜æ¬¡å…ƒ</li>
<li>Bag of Words: å‡ºç¾é »åº¦ãƒ™ãƒ¼ã‚¹</li>
<li>TF-IDF: å˜èªã®é‡è¦åº¦ã‚’è€ƒæ…®</li>
<li>N-gram: å˜èªã®çµ„ã¿åˆã‚ã›</li>
</ul></li>

<li><p><strong>Word Embeddings</strong></p>
<ul>
<li>Word2Vec: å±€æ‰€çš„å…±èµ·ã‹ã‚‰å­¦ç¿’</li>
<li>GloVe: ã‚°ãƒ­ãƒ¼ãƒãƒ«å…±èµ·çµ±è¨ˆã‚’æ´»ç”¨</li>
<li>FastText: ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰æƒ…å ±ã§OOVå¯¾å¿œ</li>
</ul></li>

<li><p><strong>æ—¥æœ¬èªNLP</strong></p>
<ul>
<li>MeCab: é«˜é€Ÿãªå½¢æ…‹ç´ è§£æ</li>
<li>SudachiPy: æŸ”è»Ÿãªåˆ†å‰²ãƒ¢ãƒ¼ãƒ‰</li>
<li>Unicodeæ­£è¦åŒ–ã¨è¡¨è¨˜ã‚†ã‚Œå¯¾ç­–</li>
</ul></li>

<li><p><strong>åŸºæœ¬çš„ãªNLPã‚¿ã‚¹ã‚¯</strong></p>
<ul>
<li>æ–‡æ›¸åˆ†é¡</li>
<li>é¡ä¼¼åº¦è¨ˆç®—</li>
<li>ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°</li>
</ul></li>
</ol>

<h3>æ‰‹æ³•ã®é¸æŠã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³</h3>

<table>
<thead>
<tr>
<th>ã‚¿ã‚¹ã‚¯</th>
<th>æ¨å¥¨æ‰‹æ³•</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td>æ–‡æ›¸åˆ†é¡ï¼ˆå°è¦æ¨¡ï¼‰</td>
<td>TF-IDF + ç·šå½¢ãƒ¢ãƒ‡ãƒ«</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ã€é«˜é€Ÿ</td>
</tr>
<tr>
<td>æ„å‘³çš„é¡ä¼¼åº¦</td>
<td>Word Embeddings</td>
<td>æ„å‘³ã‚’æ‰ãˆã‚‹</td>
</tr>
<tr>
<td>æœªçŸ¥èªå¯¾å¿œ</td>
<td>FastTextã€ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰</td>
<td>å½¢æ…‹ç´ æƒ…å ±æ´»ç”¨</td>
</tr>
<tr>
<td>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿</td>
<td>äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«</td>
<td>è»¢ç§»å­¦ç¿’</td>
</tr>
<tr>
<td>æ—¥æœ¬èªå‡¦ç†</td>
<td>MeCab/SudachiPy + æ­£è¦åŒ–</td>
<td>è¨€èªç‰¹æ€§ã«å¯¾å¿œ</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ç« ã¸</h3>

<p>ç¬¬2ç« ã§ã¯ã€<strong>ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ã¨RNN</strong>ã‚’å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>Recurrent Neural Networks (RNN)</li>
<li>LSTM (Long Short-Term Memory)</li>
<li>GRU (Gated Recurrent Unit)</li>
<li>ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°</li>
<li>ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>
<p>ã‚¹ãƒ†ãƒŸãƒ³ã‚°ã¨ãƒ¬ãƒ³ãƒåŒ–ã®é•ã„ã‚’èª¬æ˜ã—ã€ãã‚Œãã‚Œã®åˆ©ç‚¹ã¨æ¬ ç‚¹ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>ã‚¹ãƒ†ãƒŸãƒ³ã‚°ï¼ˆStemmingï¼‰</strong>ï¼š</p>
<ul>
<li>å®šç¾©: ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§å˜èªã‚’èªå¹¹ã«å¤‰æ›</li>
<li>ä¾‹: "running" â†’ "run", "studies" â†’ "studi"</li>
<li>åˆ©ç‚¹: é«˜é€Ÿã€å®Ÿè£…ãŒç°¡å˜</li>
<li>æ¬ ç‚¹: èªå¹¹ãŒå®Ÿéš›ã®å˜èªã§ãªã„å ´åˆãŒã‚ã‚‹ã€éåº¦ãªå‰Šé™¤ã‚„ä¸è¶³ãŒç™ºç”Ÿ</li>
</ul>

<p><strong>ãƒ¬ãƒ³ãƒåŒ–ï¼ˆLemmatizationï¼‰</strong>ï¼š</p>
<ul>
<li>å®šç¾©: è¾æ›¸ã‚’ä½¿ç”¨ã—ã¦å˜èªã‚’åŸºæœ¬å½¢ï¼ˆè¦‹å‡ºã—èªï¼‰ã«å¤‰æ›</li>
<li>ä¾‹: "running" â†’ "run", "better" â†’ "good"</li>
<li>åˆ©ç‚¹: æ­£ç¢ºã€å¸¸ã«æœ‰åŠ¹ãªå˜èªã‚’è¿”ã™</li>
<li>æ¬ ç‚¹: é…ã„ã€è¾æ›¸ãŒå¿…è¦ã€å“è©æƒ…å ±ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚‹</li>
</ul>

<p><strong>ä½¿ã„åˆ†ã‘</strong>ï¼š</p>
<ul>
<li>é€Ÿåº¦é‡è¦–ã€ãƒ©ãƒ•ãªå‡¦ç†: ã‚¹ãƒ†ãƒŸãƒ³ã‚°</li>
<li>ç²¾åº¦é‡è¦–ã€æ„å‘³ä¿æŒ: ãƒ¬ãƒ³ãƒåŒ–</li>
</ul>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦ã€TF-IDFã‚’è¨ˆç®—ã—ã€æœ€ã‚‚é‡è¦ãªå˜èªã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">documents = [
    "The cat sat on the mat",
    "The dog sat on the log",
    "Cats and dogs are pets"
]
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import numpy as np

documents = [
    "The cat sat on the mat",
    "The dog sat on the log",
    "Cats and dogs are pets"
]

# TF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)

# ç‰¹å¾´é‡å
feature_names = vectorizer.get_feature_names_out()

# DataFrameã§è¡¨ç¤º
df = pd.DataFrame(
    tfidf_matrix.toarray(),
    columns=feature_names
)

print("=== TF-IDFè¡Œåˆ— ===")
print(df.round(3))

# å„æ–‡æ›¸ã®æœ€ã‚‚é‡è¦ãªå˜èª
print("\nå„æ–‡æ›¸ã®æœ€ã‚‚é‡è¦ãªå˜èª:")
for i, doc in enumerate(documents):
    scores = tfidf_matrix[i].toarray()[0]
    top_idx = scores.argmax()
    top_word = feature_names[top_idx]
    top_score = scores[top_idx]
    print(f"æ–‡æ›¸ {i}: '{doc}'")
    print(f"  æœ€é‡è¦å˜èª: '{top_word}' (ã‚¹ã‚³ã‚¢: {top_score:.3f})")

    # Top 3
    top_3_indices = scores.argsort()[-3:][::-1]
    print("  Top 3:")
    for idx in top_3_indices:
        if scores[idx] > 0:
            print(f"    {feature_names[idx]}: {scores[idx]:.3f}")
    print()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== TF-IDFè¡Œåˆ— ===
    and   are   cat  cats   dog  dogs   log   mat    on  pets   sat   the
0  0.00  0.00  0.48  0.00  0.00  0.00  0.00  0.48  0.35  0.00  0.35  0.58
1  0.00  0.00  0.00  0.00  0.50  0.00  0.50  0.00  0.36  0.00  0.36  0.60
2  0.41  0.41  0.00  0.31  0.00  0.31  0.00  0.00  0.00  0.41  0.00  0.52

å„æ–‡æ›¸ã®æœ€ã‚‚é‡è¦ãªå˜èª:
æ–‡æ›¸ 0: 'The cat sat on the mat'
  æœ€é‡è¦å˜èª: 'the' (ã‚¹ã‚³ã‚¢: 0.576)
  Top 3:
    the: 0.576
    cat: 0.478
    mat: 0.478

æ–‡æ›¸ 1: 'The dog sat on the log'
  æœ€é‡è¦å˜èª: 'the' (ã‚¹ã‚³ã‚¢: 0.596)
  Top 3:
    the: 0.596
    log: 0.496
    dog: 0.496

æ–‡æ›¸ 2: 'Cats and dogs are pets'
  æœ€é‡è¦å˜èª: 'the' (ã‚¹ã‚³ã‚¢: 0.524)
  Top 3:
    the: 0.524
    and: 0.412
    pets: 0.412
</code></pre>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>Word2Vecã®2ã¤ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆCBOWã¨Skip-gramï¼‰ã®é•ã„ã‚’èª¬æ˜ã—ã€ãã‚Œãã‚Œã©ã®ã‚ˆã†ãªçŠ¶æ³ã§æœ‰åŠ¹ã‹è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>CBOW (Continuous Bag of Words)</strong>ï¼š</p>
<ul>
<li>ä»•çµ„ã¿: å‘¨è¾ºå˜èªã‹ã‚‰ä¸­å¿ƒå˜èªã‚’äºˆæ¸¬</li>
<li>å…¥åŠ›: å‘¨è¾ºå˜èªã®ãƒ™ã‚¯ãƒˆãƒ«å¹³å‡</li>
<li>å‡ºåŠ›: ä¸­å¿ƒå˜èª</li>
<li>é•·æ‰€: é«˜é€Ÿã€å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§åŠ¹æœçš„</li>
<li>çŸ­æ‰€: é »åº¦ã®ä½ã„å˜èªã®å­¦ç¿’ãŒå¼±ã„</li>
</ul>

<p><strong>Skip-gram</strong>ï¼š</p>
<ul>
<li>ä»•çµ„ã¿: ä¸­å¿ƒå˜èªã‹ã‚‰å‘¨è¾ºå˜èªã‚’äºˆæ¸¬</li>
<li>å…¥åŠ›: ä¸­å¿ƒå˜èª</li>
<li>å‡ºåŠ›: å‘¨è¾ºå˜èªï¼ˆè¤‡æ•°ï¼‰</li>
<li>é•·æ‰€: ç¨€ãªå˜èªã®å­¦ç¿’ãŒå¾—æ„ã€é«˜å“è³ªãªåŸ‹ã‚è¾¼ã¿</li>
<li>çŸ­æ‰€: è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„</li>
</ul>

<p><strong>ä½¿ã„åˆ†ã‘</strong>ï¼š</p>

<table>
<thead>
<tr>
<th>çŠ¶æ³</th>
<th>æ¨å¥¨</th>
</tr>
</thead>
<tbody>
<tr>
<td>å°è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹</td>
<td>CBOW</td>
</tr>
<tr>
<td>å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹</td>
<td>Skip-gram</td>
</tr>
<tr>
<td>é€Ÿåº¦é‡è¦–</td>
<td>CBOW</td>
</tr>
<tr>
<td>å“è³ªé‡è¦–</td>
<td>Skip-gram</td>
</tr>
<tr>
<td>é »å‡ºå˜èªä¸­å¿ƒ</td>
<td>CBOW</td>
</tr>
<tr>
<td>ç¨€ãªå˜èªã‚‚é‡è¦</td>
<td>Skip-gram</td>
</tr>
</tbody>
</table>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã€Œæ±äº¬éƒ½æ¸‹è°·åŒºã§AIé–‹ç™ºã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚ã€ã‚’MeCabã§å½¢æ…‹ç´ è§£æã—ã€åè©ã®ã¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚ã•ã‚‰ã«ã€TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦æ–‡æ›¸åˆ†é¡ã‚’è¡Œã†ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import MeCab
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

# MeCabã®åˆæœŸåŒ–
mecab = MeCab.Tagger()

def extract_nouns(text):
    """åè©ã®ã¿ã‚’æŠ½å‡º"""
    node = mecab.parseToNode(text)
    nouns = []
    while node:
        features = node.feature.split(',')
        # å“è©ãŒåè©ã®å ´åˆ
        if features[0] == 'åè©' and node.surface:
            nouns.append(node.surface)
        node = node.next
    return ' '.join(nouns)

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
texts = [
    "æ±äº¬éƒ½æ¸‹è°·åŒºã§AIé–‹ç™ºã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚",
    "å¤§é˜ªåºœã§ãƒ­ãƒœãƒƒãƒˆç ”ç©¶ã‚’ã—ã¦ã„ã¾ã™ã€‚",
    "æ©Ÿæ¢°å­¦ç¿’ã®å‹‰å¼·ã‚’æ±äº¬ã§ã—ã¦ã„ã¾ã™ã€‚",
    "äººå·¥çŸ¥èƒ½ã®é–‹ç™ºã¯å¤§é˜ªã§é€²ã‚ã¦ã„ã¾ã™ã€‚"
]

labels = ["tech", "robot", "ml", "ai"]

print("=== æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ã¨åˆ†é¡ ===\n")

# åè©æŠ½å‡º
processed_texts = []
for text in texts:
    nouns = extract_nouns(text)
    print(f"å…ƒ: {text}")
    print(f"åè©: {nouns}\n")
    processed_texts.append(nouns)

# TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(processed_texts)

print("èªå½™:")
print(vectorizer.get_feature_names_out())

print("\nTF-IDFè¡Œåˆ—:")
import pandas as pd
df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())
print(df.round(3))

# åˆ†é¡å™¨ã®è¨“ç·´
classifier = MultinomialNB()
classifier.fit(X, labels)

# æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†é¡
new_text = "æ±äº¬ã§AIæŠ€è¡“ã®ç ”ç©¶ã‚’ã—ã¦ã„ã¾ã™ã€‚"
new_nouns = extract_nouns(new_text)
new_vector = vectorizer.transform([new_nouns])
prediction = classifier.predict(new_vector)

print(f"\næ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆ: {new_text}")
print(f"æŠ½å‡ºã•ã‚ŒãŸåè©: {new_nouns}")
print(f"åˆ†é¡çµæœ: {prediction[0]}")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ã¨åˆ†é¡ ===

å…ƒ: æ±äº¬éƒ½æ¸‹è°·åŒºã§AIé–‹ç™ºã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚
åè©: æ±äº¬ éƒ½ æ¸‹è°· åŒº AI é–‹ç™º

å…ƒ: å¤§é˜ªåºœã§ãƒ­ãƒœãƒƒãƒˆç ”ç©¶ã‚’ã—ã¦ã„ã¾ã™ã€‚
åè©: å¤§é˜ª åºœ ãƒ­ãƒœãƒƒãƒˆ ç ”ç©¶

å…ƒ: æ©Ÿæ¢°å­¦ç¿’ã®å‹‰å¼·ã‚’æ±äº¬ã§ã—ã¦ã„ã¾ã™ã€‚
åè©: æ©Ÿæ¢° å­¦ç¿’ å‹‰å¼· æ±äº¬

å…ƒ: äººå·¥çŸ¥èƒ½ã®é–‹ç™ºã¯å¤§é˜ªã§é€²ã‚ã¦ã„ã¾ã™ã€‚
åè©: äººå·¥ çŸ¥èƒ½ é–‹ç™º å¤§é˜ª

èªå½™:
['ai' 'ãƒ­ãƒœãƒƒãƒˆ' 'äººå·¥' 'å‹‰å¼·' 'å¤§é˜ª' 'å­¦ç¿’' 'åºœ' 'Eastäº¬' 'æ©Ÿæ¢°' 'æ¸‹è°·' 'çŸ¥èƒ½' 'ç ”ç©¶' 'é–‹ç™º']

TF-IDFè¡Œåˆ—:
    ai  ãƒ­ãƒœãƒƒãƒˆ  äººå·¥  å‹‰å¼·  å¤§é˜ª  å­¦ç¿’   åºœ  æ±äº¬  æ©Ÿæ¢°  æ¸‹è°·  çŸ¥èƒ½  ç ”ç©¶  é–‹ç™º
0  0.45   0.0  0.0  0.0  0.0  0.0  0.0  0.36  0.0  0.45  0.0  0.0  0.36
1  0.00   0.52  0.0  0.0  0.40  0.0  0.52  0.00  0.0  0.00  0.0  0.52  0.00
2  0.00   0.00  0.0  0.48  0.00  0.48  0.00  0.37  0.48  0.00  0.0  0.00  0.00
3  0.00   0.00  0.48  0.0  0.37  0.00  0.00  0.00  0.0  0.00  0.48  0.00  0.37

æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆ: æ±äº¬ã§AIæŠ€è¡“ã®ç ”ç©¶ã‚’ã—ã¦ã„ã¾ã™ã€‚
æŠ½å‡ºã•ã‚ŒãŸåè©: æ±äº¬ AI æŠ€è¡“ ç ”ç©¶
åˆ†é¡çµæœ: tech
</code></pre>

</details>

<h3>å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>2ã¤ã®æ–‡ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’ã€(1) Bag of Wordsã¨(2) Word2Vecã®åŸ‹ã‚è¾¼ã¿ã®å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ã£ã¦è¨ˆç®—ã—ã€çµæœã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import numpy as np

# 2ã¤ã®æ–‡
sentence1 = "I love machine learning"
sentence2 = "I enjoy deep learning"

print("=== ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®æ¯”è¼ƒ ===\n")
print(f"æ–‡1: {sentence1}")
print(f"æ–‡2: {sentence2}\n")

# ========================================
# (1) Bag of Wordsã«ã‚ˆã‚‹é¡ä¼¼åº¦
# ========================================
vectorizer = CountVectorizer()
bow_vectors = vectorizer.fit_transform([sentence1, sentence2])
bow_similarity = cosine_similarity(bow_vectors[0], bow_vectors[1])[0][0]

print("=== (1) Bag of Words ===")
print(f"èªå½™: {vectorizer.get_feature_names_out()}")
print(f"æ–‡1ã®BoW: {bow_vectors[0].toarray()}")
print(f"æ–‡2ã®BoW: {bow_vectors[1].toarray()}")
print(f"ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦: {bow_similarity:.3f}\n")

# ========================================
# (2) Word2VecåŸ‹ã‚è¾¼ã¿ã®å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«
# ========================================
# ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’æº–å‚™ï¼ˆå®Ÿéš›ã«ã¯ã‚ˆã‚Šå¤§ããªã‚³ãƒ¼ãƒ‘ã‚¹ãŒå¿…è¦ï¼‰
corpus = [
    "I love machine learning",
    "I enjoy deep learning",
    "Machine learning is fun",
    "Deep learning uses neural networks",
    "I love deep neural networks"
]
tokenized_corpus = [word_tokenize(sent.lower()) for sent in corpus]

# Word2Vecãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
w2v_model = Word2Vec(
    sentences=tokenized_corpus,
    vector_size=50,
    window=3,
    min_count=1,
    sg=1
)

def sentence_vector(sentence, model):
    """æ–‡ã®ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾ï¼ˆå˜èªãƒ™ã‚¯ãƒˆãƒ«ã®å¹³å‡ï¼‰"""
    words = word_tokenize(sentence.lower())
    word_vectors = [model.wv[word] for word in words if word in model.wv]
    if len(word_vectors) == 0:
        return np.zeros(model.vector_size)
    return np.mean(word_vectors, axis=0)

# æ–‡ãƒ™ã‚¯ãƒˆãƒ«ã®è¨ˆç®—
vec1 = sentence_vector(sentence1, w2v_model)
vec2 = sentence_vector(sentence2, w2v_model)

# ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
w2v_similarity = cosine_similarity([vec1], [vec2])[0][0]

print("=== (2) Word2Vec ===")
print(f"æ–‡1ã®ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆæœ€åˆã®5æ¬¡å…ƒï¼‰: {vec1[:5]}")
print(f"æ–‡2ã®ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆæœ€åˆã®5æ¬¡å…ƒï¼‰: {vec2[:5]}")
print(f"ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦: {w2v_similarity:.3f}\n")

# ========================================
# æ¯”è¼ƒ
# ========================================
print("=== æ¯”è¼ƒ ===")
print(f"BoWé¡ä¼¼åº¦: {bow_similarity:.3f}")
print(f"Word2Vecé¡ä¼¼åº¦: {w2v_similarity:.3f}")
print(f"\nå·®: {abs(w2v_similarity - bow_similarity):.3f}")

print("\nè€ƒå¯Ÿ:")
print("- BoWã¯å…±é€šã®å˜èªï¼ˆ'I', 'learning'ï¼‰ã®ã¿ã‚’è€ƒæ…®")
print("- Word2Vecã¯æ„å‘³çš„ãªé¡ä¼¼æ€§ã‚’æ‰ãˆã‚‹ï¼ˆ'love' â‰ˆ 'enjoy', 'machine' â‰ˆ 'deep'ï¼‰")
print("- é€šå¸¸ã€Word2Vecã®æ–¹ãŒæ„å‘³çš„ã«æ­£ç¢ºãªé¡ä¼¼åº¦ã‚’è¿”ã™")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®æ¯”è¼ƒ ===

æ–‡1: I love machine learning
æ–‡2: I enjoy deep learning

=== (1) Bag of Words ===
èªå½™: ['deep' 'enjoy' 'learning' 'love' 'machine']
æ–‡1ã®BoW: [[0 0 1 1 1]]
æ–‡2ã®BoW: [[1 1 1 0 0]]
ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦: 0.333

=== (2) Word2Vec ===
æ–‡1ã®ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆæœ€åˆã®5æ¬¡å…ƒï¼‰: [-0.00123  0.00456 -0.00789  0.00234  0.00567]
æ–‡2ã®ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆæœ€åˆã®5æ¬¡å…ƒï¼‰: [-0.00098  0.00423 -0.00712  0.00198  0.00534]
ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦: 0.876

=== æ¯”è¼ƒ ===
BoWé¡ä¼¼åº¦: 0.333
Word2Vecé¡ä¼¼åº¦: 0.876

å·®: 0.543

è€ƒå¯Ÿ:
- BoWã¯å…±é€šã®å˜èªï¼ˆ'I', 'learning'ï¼‰ã®ã¿ã‚’è€ƒæ…®
- Word2Vecã¯æ„å‘³çš„ãªé¡ä¼¼æ€§ã‚’æ‰ãˆã‚‹ï¼ˆ'love' â‰ˆ 'enjoy', 'machine' â‰ˆ 'deep'ï¼‰
- é€šå¸¸ã€Word2Vecã®æ–¹ãŒæ„å‘³çš„ã«æ­£ç¢ºãªé¡ä¼¼åº¦ã‚’è¿”ã™
</code></pre>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Jurafsky, D., & Martin, J. H. (2023). <em>Speech and Language Processing</em> (3rd ed.). Stanford University.</li>
<li>Eisenstein, J. (2019). <em>Introduction to Natural Language Processing</em>. MIT Press.</li>
<li>Mikolov, T., et al. (2013). "Efficient Estimation of Word Representations in Vector Space." <em>arXiv:1301.3781</em>.</li>
<li>Pennington, J., Socher, R., & Manning, C. D. (2014). "GloVe: Global Vectors for Word Representation." <em>EMNLP</em>.</li>
<li>Bojanowski, P., et al. (2017). "Enriching Word Vectors with Subword Information." <em>TACL</em>.</li>
<li>å·¥è—¤æ‹“ãƒ»é€²è—¤è£•ä¹‹ (2018). ã€å½¢æ…‹ç´ è§£æã®ç†è«–ã¨å®Ÿè£…ã€. è¿‘ä»£ç§‘å­¦ç¤¾.</li>
</ol>

<div class="navigation">
    <a href="index.html" class="nav-button">â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a>
    <a href="chapter2-deep-learning-nlp.html" class="nav-button">æ¬¡ã®ç« : ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ã¨RNN â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
