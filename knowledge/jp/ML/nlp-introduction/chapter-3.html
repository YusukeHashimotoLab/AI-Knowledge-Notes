<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 3: å®Ÿè·µ - ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Chapter 3: å®Ÿè·µ - ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 35-40åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 0å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 0å•</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 3: å®Ÿè·µ - ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡</h1>
<h2>æœ¬ç« ã®æ¦‚è¦</h2>
<p>ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ï¼ˆText Classificationï¼‰ã¯ã€NLPã®æœ€ã‚‚å®Ÿç”¨çš„ãªã‚¿ã‚¹ã‚¯ã®1ã¤ã§ã™ã€‚æ„Ÿæƒ…åˆ†æã€ã‚¹ãƒ‘ãƒ ãƒ•ã‚£ãƒ«ã‚¿ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ãªã©ã€å¹…åºƒã„å¿œç”¨ãŒã‚ã‚Šã¾ã™ã€‚</p>
<p>æœ¬ç« ã§ã¯ã€å®Ÿéš›ã«ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹æ–¹æ³•ã‚’ã€å®Œå…¨ãªå®Ÿè£…ä¾‹ã¨ã¨ã‚‚ã«å­¦ã³ã¾ã™ã€‚</p>
<h3>å­¦ç¿’ç›®æ¨™</h3>
<ul>
<li>âœ… ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè£…</li>
<li>âœ… Bag-of-Words ã¨ TF-IDF ã«ã‚ˆã‚‹ç‰¹å¾´é‡æŠ½å‡º</li>
<li>âœ… æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹åˆ†é¡</li>
<li>âœ… ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¨æ”¹å–„æ‰‹æ³•</li>
<li>âœ… å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®å®Ÿè£…çµŒé¨“</li>
</ul>
<hr />
<h2>1. ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã¨ã¯</h2>
<h3>1.1 å®šç¾©ã¨å¿œç”¨ä¾‹</h3>
<p><strong>ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ï¼ˆText Classificationï¼‰</strong> ã¨ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’äºˆã‚å®šç¾©ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã«è‡ªå‹•çš„ã«åˆ†é¡ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚</p>
<h4>ä¸»ãªå¿œç”¨ä¾‹</h4>
<table>
<thead>
<tr>
<th>å¿œç”¨åˆ†é‡</th>
<th>å…·ä½“ä¾‹</th>
<th>ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ„Ÿæƒ…åˆ†æ</strong></td>
<td>å•†å“ãƒ¬ãƒ“ãƒ¥ãƒ¼ã® Positive/Negative åˆ¤å®š</td>
<td>é¡§å®¢æº€è¶³åº¦ã®æŠŠæ¡</td>
</tr>
<tr>
<td><strong>ã‚¹ãƒ‘ãƒ ãƒ•ã‚£ãƒ«ã‚¿</strong></td>
<td>ãƒ¡ãƒ¼ãƒ«ã®ã‚¹ãƒ‘ãƒ /éã‚¹ãƒ‘ãƒ åˆ†é¡</td>
<td>ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å‘ä¸Š</td>
</tr>
<tr>
<td><strong>ã‚«ãƒ†ã‚´ãƒªåˆ†é¡</strong></td>
<td>ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ã‚¸ãƒ£ãƒ³ãƒ«åˆ†é¡</td>
<td>ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¨è–¦</td>
</tr>
<tr>
<td><strong>æ„å›³æ¨å®š</strong></td>
<td>å•ã„åˆã‚ã›ã®ç¨®é¡åˆ¤å®š</td>
<td>ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆåŠ¹ç‡åŒ–</td>
</tr>
<tr>
<td><strong>ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡</strong></td>
<td>è«–æ–‡ã®ç ”ç©¶åˆ†é‡åˆ†é¡</td>
<td>æƒ…å ±æ•´ç†</td>
</tr>
</tbody>
</table>
<h3>1.2 ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã®æµã‚Œ</h3>
<div class="mermaid">
graph LR
    A[ç”Ÿãƒ†ã‚­ã‚¹ãƒˆ] --> B[å‰å‡¦ç†]
    B --> C[ç‰¹å¾´é‡æŠ½å‡º]
    C --> D[ãƒ¢ãƒ‡ãƒ«å­¦ç¿’]
    D --> E[äºˆæ¸¬]
    E --> F[è©•ä¾¡]
    F -->|æ”¹å–„| B
</div>

<ol>
<li><strong>å‰å‡¦ç†</strong>: ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã€æ­£è¦åŒ–</li>
<li><strong>ç‰¹å¾´é‡æŠ½å‡º</strong>: ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›</li>
<li><strong>ãƒ¢ãƒ‡ãƒ«å­¦ç¿’</strong>: æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§å­¦ç¿’</li>
<li><strong>äºˆæ¸¬</strong>: æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†é¡</li>
<li><strong>è©•ä¾¡</strong>: ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æ¸¬å®š</li>
</ol>
<hr />
<h2>2. ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h2>
<h3>2.1 ãªãœå‰å‡¦ç†ãŒé‡è¦ã‹</h3>
<p>ç”Ÿã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã¯ä»¥ä¸‹ã®å•é¡ŒãŒã‚ã‚Šã¾ã™ï¼š</p>
<ul>
<li><strong>ãƒã‚¤ã‚º</strong>: URLã€è¨˜å·ã€çµµæ–‡å­—ãªã©ä¸è¦ãªæƒ…å ±</li>
<li><strong>è¡¨è¨˜ã‚†ã‚Œ</strong>: "ãƒ‘ã‚½ã‚³ãƒ³" vs "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿"</li>
<li><strong>å¤§æ–‡å­—å°æ–‡å­—</strong>: "Apple" vs "apple"</li>
<li><strong>æ´»ç”¨å½¢</strong>: "èµ°ã‚‹", "èµ°ã£ãŸ", "èµ°ã‚Œã°"</li>
</ul>
<p>å‰å‡¦ç†ã«ã‚ˆã£ã¦ã“ã‚Œã‚‰ã‚’çµ±ä¸€ã—ã€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’åŠ¹ç‡ã‚’ä¸Šã’ã¾ã™ã€‚</p>
<h3>2.2 åŸºæœ¬çš„ãªå‰å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—</h3>
<h4>è‹±èªãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†</h4>
<pre><code class="language-python">import re
import string
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.tokenize import word_tokenize

class EnglishTextPreprocessor:
    &quot;&quot;&quot;è‹±èªãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã‚¯ãƒ©ã‚¹&quot;&quot;&quot;

    def __init__(self):
        self.stop_words = set(stopwords.words('english'))
        self.stemmer = PorterStemmer()
        self.lemmatizer = WordNetLemmatizer()

    def clean_text(self, text):
        &quot;&quot;&quot;ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°&quot;&quot;&quot;
        # å°æ–‡å­—åŒ–
        text = text.lower()

        # URLã®é™¤å»
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

        # ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã®é™¤å»ï¼ˆTwitterç”¨ï¼‰
        text = re.sub(r'@\w+|#\w+', '', text)

        # HTMLã‚¿ã‚°ã®é™¤å»
        text = re.sub(r'&lt;.*?&gt;', '', text)

        # å¥èª­ç‚¹ã®é™¤å»
        text = text.translate(str.maketrans('', '', string.punctuation))

        # æ•°å­—ã®é™¤å»ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        text = re.sub(r'\d+', '', text)

        # ä½™åˆ†ãªç©ºç™½ã®é™¤å»
        text = ' '.join(text.split())

        return text

    def tokenize_and_filter(self, text):
        &quot;&quot;&quot;ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»&quot;&quot;&quot;
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        tokens = word_tokenize(text)

        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»
        tokens = [word for word in tokens if word not in self.stop_words]

        # çŸ­ã™ãã‚‹å˜èªã‚’é™¤å»ï¼ˆ2æ–‡å­—æœªæº€ï¼‰
        tokens = [word for word in tokens if len(word) &gt; 2]

        return tokens

    def stem_tokens(self, tokens):
        &quot;&quot;&quot;ã‚¹ãƒ†ãƒŸãƒ³ã‚°ï¼ˆèªå¹¹æŠ½å‡ºï¼‰&quot;&quot;&quot;
        return [self.stemmer.stem(token) for token in tokens]

    def lemmatize_tokens(self, tokens):
        &quot;&quot;&quot;ãƒ¬ãƒ³ãƒåŒ–ï¼ˆè¦‹å‡ºã—èªåŒ–ï¼‰&quot;&quot;&quot;
        return [self.lemmatizer.lemmatize(token) for token in tokens]

    def preprocess(self, text, use_lemmatization=True):
        &quot;&quot;&quot;å®Œå…¨ãªå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³&quot;&quot;&quot;
        # 1. ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        text = self.clean_text(text)

        # 2. ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        tokens = self.tokenize_and_filter(text)

        # 3. æ­£è¦åŒ–ï¼ˆã‚¹ãƒ†ãƒŸãƒ³ã‚° or ãƒ¬ãƒ³ãƒåŒ–ï¼‰
        if use_lemmatization:
            tokens = self.lemmatize_tokens(tokens)
        else:
            tokens = self.stem_tokens(tokens)

        return ' '.join(tokens)

# ä½¿ç”¨ä¾‹
preprocessor = EnglishTextPreprocessor()

raw_text = &quot;&quot;&quot;
This is AMAZING!!! Check out http://example.com
I absolutely love this product ğŸ˜ #happy @company
&quot;&quot;&quot;

processed_text = preprocessor.preprocess(raw_text)
print(f&quot;å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ: {raw_text}&quot;)
print(f&quot;å‰å‡¦ç†å¾Œ: {processed_text}&quot;)
# å‡ºåŠ›ä¾‹: &quot;amazing absolutely love product happy company&quot;
</code></pre>
<h4>æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†</h4>
<pre><code class="language-python">import re
import MeCab
import unicodedata

class JapaneseTextPreprocessor:
    &quot;&quot;&quot;æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã‚¯ãƒ©ã‚¹&quot;&quot;&quot;

    def __init__(self):
        self.mecab = MeCab.Tagger()

        # æ—¥æœ¬èªã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¸€éƒ¨ï¼‰
        self.stop_words = set([
            'ã®', 'ã¯', 'ã‚’', 'ãŒ', 'ã«', 'ã§', 'ã¨', 'ãŸ', 'ã—',
            'ã¦', 'ã¾ã™', 'ã§ã™', 'ã‚ã‚‹', 'ã„ã‚‹', 'ã“ã®', 'ãã®',
            'ã‚Œã‚‹', 'ã‚‰ã‚Œã‚‹', 'ã›ã‚‹', 'ã•ã›ã‚‹', 'ã“ã¨', 'ã‚‚ã®'
        ])

        # é™¤å¤–ã™ã‚‹å“è©
        self.exclude_pos = set([
            'åŠ©è©', 'åŠ©å‹•è©', 'æ¥ç¶šè©', 'è¨˜å·', 'ãƒ•ã‚£ãƒ©ãƒ¼', 'æ„Ÿå‹•è©'
        ])

    def clean_text(self, text):
        &quot;&quot;&quot;ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°&quot;&quot;&quot;
        # Unicodeæ­£è¦åŒ–ï¼ˆNFKCã§åŠè§’ãƒ»å…¨è§’ã‚’çµ±ä¸€ï¼‰
        text = unicodedata.normalize('NFKC', text)

        # URLã®é™¤å»
        text = re.sub(r'http\S+|www\S+|https\S+', '', text)

        # ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã®é™¤å»
        text = re.sub(r'@\w+|#\w+', '', text)

        # æ”¹è¡Œãƒ»ã‚¿ãƒ–ã‚’ç©ºç™½ã«
        text = re.sub(r'[\n\t]', ' ', text)

        # çµµæ–‡å­—ã®é™¤å»ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        emoji_pattern = re.compile(
            &quot;[&quot;
            &quot;\U0001F600-\U0001F64F&quot;  # é¡”æ–‡å­—
            &quot;\U0001F300-\U0001F5FF&quot;  # è¨˜å·ãƒ»çµµæ–‡å­—
            &quot;\U0001F680-\U0001F6FF&quot;  # ä¹—ã‚Šç‰©ãƒ»å»ºç‰©
            &quot;\U0001F1E0-\U0001F1FF&quot;  # æ——
            &quot;]+&quot;, flags=re.UNICODE
        )
        text = emoji_pattern.sub('', text)

        # ä½™åˆ†ãªç©ºç™½ã®é™¤å»
        text = ' '.join(text.split())

        return text

    def tokenize_with_filter(self, text):
        &quot;&quot;&quot;MeCabã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ– + ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°&quot;&quot;&quot;
        # MeCabã§å½¢æ…‹ç´ è§£æ
        node = self.mecab.parseToNode(text)

        words = []
        while node:
            if node.surface:
                features = node.feature.split(',')
                surface = node.surface
                pos = features[0]  # å“è©
                base_form = features[6] if len(features) &gt; 6 else surface

                # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶
                if (pos not in self.exclude_pos and
                    surface not in self.stop_words and
                    len(surface) &gt; 1):  # 1æ–‡å­—å˜èªã‚’é™¤å¤–

                    # åŸºæœ¬å½¢ã‚’ä½¿ç”¨
                    words.append(base_form if base_form != '*' else surface)

            node = node.next

        return words

    def preprocess(self, text):
        &quot;&quot;&quot;å®Œå…¨ãªå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³&quot;&quot;&quot;
        # 1. ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        text = self.clean_text(text)

        # 2. ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        tokens = self.tokenize_with_filter(text)

        return ' '.join(tokens)

# ä½¿ç”¨ä¾‹
jp_preprocessor = JapaneseTextPreprocessor()

raw_jp_text = &quot;&quot;&quot;
ã“ã®å•†å“ã¯æœ¬å½“ã«ç´ æ™´ã‚‰ã—ã„ã§ã™ï¼ï¼ï¼ğŸ˜
çµ¶å¯¾ã«ãŠã™ã™ã‚ã—ã¾ã™ğŸ‰ #æœ€é«˜
è©³ã—ãã¯ã“ã¡ã‚‰â†’ http://example.com
&quot;&quot;&quot;

processed_jp_text = jp_preprocessor.preprocess(raw_jp_text)
print(f&quot;å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ: {raw_jp_text}&quot;)
print(f&quot;å‰å‡¦ç†å¾Œ: {processed_jp_text}&quot;)
# å‡ºåŠ›ä¾‹: &quot;å•†å“ æœ¬å½“ ç´ æ™´ã‚‰ã—ã„ çµ¶å¯¾ ãŠã™ã™ã‚ æœ€é«˜ è©³ã—ã„&quot;
</code></pre>
<hr />
<h2>3. ç‰¹å¾´é‡æŠ½å‡º</h2>
<p>æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¯æ•°å€¤ã—ã‹æ‰±ãˆã¾ã›ã‚“ã€‚ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚</p>
<h3>3.1 Bag-of-Words (BoW)</h3>
<p><strong>Bag-of-Words</strong> ã¯æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªæ–¹æ³•ã§ã€å„æ–‡æ›¸ã‚’å˜èªã®å‡ºç¾å›æ•°ã§è¡¨ç¾ã—ã¾ã™ã€‚</p>
<h4>ä»•çµ„ã¿</h4>
<pre><code>æ–‡æ›¸1: &quot;çŒ«ãŒå¥½ã&quot;
æ–‡æ›¸2: &quot;çŠ¬ãŒå¥½ã&quot;
æ–‡æ›¸3: &quot;çŒ«ã‚‚çŠ¬ã‚‚å¥½ã&quot;

èªå½™: [çŒ«, çŠ¬, å¥½ã, ãŒ, ã‚‚]

BoWè¡¨ç¾:
æ–‡æ›¸1: [1, 0, 1, 1, 0]  # çŒ«:1å›, çŠ¬:0å›, å¥½ã:1å›, ãŒ:1å›, ã‚‚:0å›
æ–‡æ›¸2: [0, 1, 1, 1, 0]
æ–‡æ›¸3: [1, 1, 1, 0, 2]
</code></pre>
<h4>å®Ÿè£…</h4>
<pre><code class="language-python">from sklearn.feature_extraction.text import CountVectorizer

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
documents = [
    &quot;æ©Ÿæ¢°å­¦ç¿’ã¯é¢ç™½ã„&quot;,
    &quot;æ·±å±¤å­¦ç¿’ã¯é›£ã—ã„&quot;,
    &quot;æ©Ÿæ¢°å­¦ç¿’ã¨æ·±å±¤å­¦ç¿’ã¯é–¢é€£ã—ã¦ã„ã‚‹&quot;,
    &quot;è‡ªç„¶è¨€èªå‡¦ç†ã‚‚é¢ç™½ã„&quot;
]

# BoWå¤‰æ›
vectorizer = CountVectorizer()
bow_matrix = vectorizer.fit_transform(documents)

# èªå½™ã®ç¢ºèª
print(&quot;èªå½™:&quot;, vectorizer.get_feature_names_out())
print(&quot;\nBoWè¡Œåˆ—:&quot;)
print(bow_matrix.toarray())
</code></pre>
<p><strong>å‡ºåŠ›:</strong></p>
<pre><code>èªå½™: ['ã¨' 'æ·±å±¤' 'æ©Ÿæ¢°' 'è‡ªç„¶' 'è¨€èª' 'å‡¦ç†' 'é–¢é€£' 'é›£ã—ã„' 'é¢ç™½ã„' 'å­¦ç¿’']

BoWè¡Œåˆ—:
[[0 0 1 0 0 0 0 0 1 1]  # æ©Ÿæ¢°å­¦ç¿’ã¯é¢ç™½ã„
 [0 1 0 0 0 0 0 1 0 1]  # æ·±å±¤å­¦ç¿’ã¯é›£ã—ã„
 [1 1 1 0 0 0 1 0 0 2]  # æ©Ÿæ¢°å­¦ç¿’ã¨æ·±å±¤å­¦ç¿’ã¯é–¢é€£ã—ã¦ã„ã‚‹
 [0 0 0 1 1 1 0 0 1 0]] # è‡ªç„¶è¨€èªå‡¦ç†ã‚‚é¢ç™½ã„
</code></pre>
<h4>å•é¡Œç‚¹</h4>
<ul>
<li><strong>èªé †ã‚’ç„¡è¦–</strong>: "çŠ¬ãŒçŒ«ã‚’è¿½ã„ã‹ã‘ã‚‹" ã¨ "çŒ«ãŒçŠ¬ã‚’è¿½ã„ã‹ã‘ã‚‹" ãŒåŒã˜</li>
<li><strong>æ–‡æ›¸é•·ã®å½±éŸ¿</strong>: é•·ã„æ–‡æ›¸ã»ã©å€¤ãŒå¤§ãããªã‚‹</li>
<li><strong>é‡è¦åº¦ã‚’è€ƒæ…®ã—ãªã„</strong>: ã™ã¹ã¦ã®å˜èªãŒåŒç­‰ã«æ‰±ã‚ã‚Œã‚‹</li>
</ul>
<h3>3.2 TF-IDF (Term Frequency-Inverse Document Frequency)</h3>
<p><strong>TF-IDF</strong> ã¯ã€å˜èªã®é‡è¦åº¦ã‚’è€ƒæ…®ã—ãŸç‰¹å¾´é‡æŠ½å‡ºæ‰‹æ³•ã§ã™ã€‚</p>
<h4>è¨ˆç®—å¼</h4>
<p>$$
\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t)
$$</p>
<ul>
<li>
<p><strong>TF (Term Frequency)</strong>: æ–‡æ›¸å†…ã§ã®å˜èªã®å‡ºç¾é »åº¦
$$
\text{TF}(t, d) = \frac{\text{æ–‡æ›¸}d\text{ã«ãŠã‘ã‚‹å˜èª}t\text{ã®å‡ºç¾å›æ•°}}{\text{æ–‡æ›¸}d\text{ã®ç·å˜èªæ•°}}
$$</p>
</li>
<li>
<p><strong>IDF (Inverse Document Frequency)</strong>: å˜èªã®å¸Œå°‘æ€§
$$
\text{IDF}(t) = \log \frac{\text{ç·æ–‡æ›¸æ•°}}{\text{å˜èª}t\text{ã‚’å«ã‚€æ–‡æ›¸æ•°}}
$$</p>
</li>
</ul>
<h4>ç›´æ„Ÿçš„ç†è§£</h4>
<ul>
<li><strong>é »å‡ºã™ã‚‹å˜èª</strong> (TF ãŒé«˜ã„) â†’ é‡è¦åº¦ãŒé«˜ã„</li>
<li><strong>å¤šãã®æ–‡æ›¸ã«å‡ºç¾ã™ã‚‹å˜èª</strong> (IDF ãŒä½ã„) â†’ ã‚ã¾ã‚Šé‡è¦ã§ã¯ãªã„</li>
<li>ä¾‹: "æ©Ÿæ¢°å­¦ç¿’" ã¯ç‰¹å®šã®æ–‡æ›¸ã«é »å‡º â†’ TF-IDF é«˜ã„</li>
<li>ä¾‹: "ã§ã™", "ã¾ã™" ã¯å…¨æ–‡æ›¸ã«å‡ºç¾ â†’ TF-IDF ä½ã„</li>
</ul>
<h4>å®Ÿè£…</h4>
<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
documents = [
    &quot;æ©Ÿæ¢°å­¦ç¿’ã¯æ•™å¸«ã‚ã‚Šå­¦ç¿’ã¨æ•™å¸«ãªã—å­¦ç¿’ã«åˆ†é¡ã•ã‚Œã‚‹&quot;,
    &quot;æ·±å±¤å­¦ç¿’ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ãŸæ©Ÿæ¢°å­¦ç¿’ã®æ‰‹æ³•&quot;,
    &quot;è‡ªç„¶è¨€èªå‡¦ç†ã¯äººé–“ã®è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§å‡¦ç†ã™ã‚‹æŠ€è¡“&quot;,
    &quot;ç”»åƒèªè­˜ã«ã¯ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒä½¿ã‚ã‚Œã‚‹&quot;
]

# TF-IDFå¤‰æ›
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(documents)

# èªå½™ã¨ TF-IDF å€¤
feature_names = tfidf_vectorizer.get_feature_names_out()
print(&quot;èªå½™:&quot;, feature_names)
print(&quot;\nTF-IDFè¡Œåˆ—ã®å½¢çŠ¶:&quot;, tfidf_matrix.shape)

# æœ€åˆã®æ–‡æ›¸ã® TF-IDF å€¤ã‚’è¡¨ç¤º
doc_index = 0
feature_index = tfidf_matrix[doc_index].nonzero()[1]
tfidf_scores = zip(
    [feature_names[i] for i in feature_index],
    [tfidf_matrix[doc_index, i] for i in feature_index]
)

print(f&quot;\næ–‡æ›¸{doc_index+1}ã®TF-IDFã‚¹ã‚³ã‚¢:&quot;)
for word, score in sorted(tfidf_scores, key=lambda x: x[1], reverse=True):
    print(f&quot;  {word}: {score:.4f}&quot;)
</code></pre>
<p><strong>å‡ºåŠ›ä¾‹:</strong></p>
<pre><code>æ–‡æ›¸1ã®TF-IDFã‚¹ã‚³ã‚¢:
  æ•™å¸«: 0.5774  # ä»–ã®æ–‡æ›¸ã«ã¯ãªã„ â†’ IDFé«˜ã„
  å­¦ç¿’: 0.4082  # è¤‡æ•°æ–‡æ›¸ã«å‡ºç¾ â†’ IDFä¸­ç¨‹åº¦
  åˆ†é¡: 0.4082
  æ©Ÿæ¢°: 0.2887  # 2æ–‡æ›¸ã«å‡ºç¾ â†’ IDFä½ã‚
</code></pre>
<h3>3.3 N-gram</h3>
<p><strong>N-gram</strong> ã¯é€£ç¶šã™ã‚‹Nå€‹ã®å˜èªã‚’1ã¤ã®ç‰¹å¾´ã¨ã—ã¦æ‰±ã„ã¾ã™ã€‚</p>
<ul>
<li><strong>Unigram (1-gram)</strong>: "æ©Ÿæ¢°", "å­¦ç¿’"</li>
<li><strong>Bigram (2-gram)</strong>: "æ©Ÿæ¢°å­¦ç¿’"</li>
<li><strong>Trigram (3-gram)</strong>: "æ©Ÿæ¢°å­¦ç¿’ã®æ‰‹æ³•"</li>
</ul>
<h4>å®Ÿè£…</h4>
<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer

documents = [
    &quot;æ©Ÿæ¢° å­¦ç¿’ ã¯ é¢ç™½ã„&quot;,
    &quot;æ·±å±¤ å­¦ç¿’ ã‚‚ é¢ç™½ã„&quot;,
    &quot;æ©Ÿæ¢° å­¦ç¿’ ã¨ æ·±å±¤ å­¦ç¿’&quot;
]

# Bigram + Unigram ã® TF-IDF
vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # 1-gram ã¨ 2-gram
tfidf = vectorizer.fit_transform(documents)

print(&quot;ç‰¹å¾´èªï¼ˆUnigram + Bigramï¼‰:&quot;)
print(vectorizer.get_feature_names_out())
</code></pre>
<p><strong>å‡ºåŠ›:</strong></p>
<pre><code>['ã¨' 'ã¯' 'ã‚‚' 'å­¦ç¿’' 'å­¦ç¿’ ã¨' 'å­¦ç¿’ ã¯' 'å­¦ç¿’ ã‚‚' 'æ©Ÿæ¢°' 'æ©Ÿæ¢° å­¦ç¿’'
 'æ·±å±¤' 'æ·±å±¤ å­¦ç¿’' 'é¢ç™½ã„']
</code></pre>
<hr />
<h2>4. æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹åˆ†é¡</h2>
<h3>4.1 Naive Bayes (ãƒŠã‚¤ãƒ¼ãƒ–ãƒ™ã‚¤ã‚º)</h3>
<p>Naive Bayes ã¯ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã§æœ€ã‚‚ã‚ˆãä½¿ã‚ã‚Œã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®1ã¤ã§ã™ã€‚</p>
<h4>ç†è«–</h4>
<p>ãƒ™ã‚¤ã‚ºã®å®šç†ã«åŸºã¥ãã€å„ã‚¯ãƒ©ã‚¹ã®ç¢ºç‡ã‚’è¨ˆç®—ï¼š</p>
<p>$$
P(C|D) = \frac{P(D|C) \times P(C)}{P(D)}
$$</p>
<ul>
<li>$P(C|D)$: æ–‡æ›¸ $D$ ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã®ã‚¯ãƒ©ã‚¹ $C$ ã®ç¢ºç‡</li>
<li>$P(D|C)$: ã‚¯ãƒ©ã‚¹ $C$ ã«ãŠã‘ã‚‹æ–‡æ›¸ $D$ ã®å°¤åº¦</li>
<li>$P(C)$: ã‚¯ãƒ©ã‚¹ $C$ ã®äº‹å‰ç¢ºç‡</li>
</ul>
<h4>å®Ÿè£…: ã‚¹ãƒ‘ãƒ ãƒ•ã‚£ãƒ«ã‚¿</h4>
<pre><code class="language-python">from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆå®Ÿéš›ã¯å¤§é‡ã®ãƒ¡ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
emails = [
    &quot;ç„¡æ–™ã§iPhoneãŒå½“ãŸã‚Šã¾ã™ï¼ä»Šã™ãã‚¯ãƒªãƒƒã‚¯ï¼&quot;,
    &quot;ä¼šè­°ã®è³‡æ–™ã‚’é€ä»˜ã—ã¾ã™&quot;,
    &quot;æ¿€å®‰ã‚»ãƒ¼ãƒ«ï¼æœ¬æ—¥é™å®š50%ã‚ªãƒ•&quot;,
    &quot;ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®é€²æ—å ±å‘Š&quot;,
    &quot;ã‚ãªãŸã«100ä¸‡å††ãŒå½“ãŸã‚Šã¾ã—ãŸ&quot;,
    &quot;æ¥é€±ã®äºˆå®šã«ã¤ã„ã¦ç¢ºèªã•ã›ã¦ãã ã•ã„&quot;,
    &quot;é™å®šã‚ªãƒ•ã‚¡ãƒ¼ï¼ä»Šã™ãç™»éŒ²&quot;,
    &quot;è­°äº‹éŒ²ã‚’å…±æœ‰ã—ã¾ã™&quot;
]

labels = [1, 0, 1, 0, 1, 0, 1, 0]  # 1: ã‚¹ãƒ‘ãƒ , 0: æ­£å¸¸

# å‰å‡¦ç†ï¼ˆå®Ÿéš›ã¯æ—¥æœ¬èªå½¢æ…‹ç´ è§£æã‚’ä½¿ç”¨ï¼‰
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features=100)
X = vectorizer.fit_transform(emails)
y = labels

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42
)

# Naive Bayes ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)

# äºˆæ¸¬
y_pred = nb_model.predict(X_test)

# è©•ä¾¡
print(&quot;åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:&quot;)
print(classification_report(y_test, y_pred,
                          target_names=['æ­£å¸¸ãƒ¡ãƒ¼ãƒ«', 'ã‚¹ãƒ‘ãƒ ']))

print(&quot;\næ··åŒè¡Œåˆ—:&quot;)
print(confusion_matrix(y_test, y_pred))

# æ–°ã—ã„ãƒ¡ãƒ¼ãƒ«ã®äºˆæ¸¬
new_emails = [
    &quot;ä¼šè­°å®¤ã®äºˆç´„ã‚’ãŠé¡˜ã„ã—ã¾ã™&quot;,
    &quot;ä»Šã™ãã‚¯ãƒªãƒƒã‚¯ï¼å¤§é‡‘ãŒæ‰‹ã«å…¥ã‚Šã¾ã™&quot;
]
new_emails_vec = vectorizer.transform(new_emails)
predictions = nb_model.predict(new_emails_vec)

for email, pred in zip(new_emails, predictions):
    label = &quot;ã‚¹ãƒ‘ãƒ &quot; if pred == 1 else &quot;æ­£å¸¸&quot;
    print(f&quot;'{email}' â†’ {label}&quot;)
</code></pre>
<h3>4.2 Support Vector Machine (SVM)</h3>
<p>SVM ã¯é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã§ã‚‚åŠ¹æœçš„ãªåˆ†é¡ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚</p>
<h4>å®Ÿè£…: æ„Ÿæƒ…åˆ†æ</h4>
<pre><code class="language-python">from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer

# ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿
reviews = [
    &quot;ã“ã®æ˜ ç”»ã¯æœ€é«˜ã§ã—ãŸï¼æ„Ÿå‹•ã—ã¾ã—ãŸ&quot;,
    &quot;ã¤ã¾ã‚‰ãªã„æ˜ ç”»ã€‚æ™‚é–“ã®ç„¡é§„&quot;,
    &quot;ç´ æ™´ã‚‰ã—ã„æ¼”æŠ€ã¨ç¾ã—ã„æ˜ åƒ&quot;,
    &quot;é€€å±ˆã§çœ ããªã£ãŸ&quot;,
    &quot;æœŸå¾…ä»¥ä¸Šã®ä½œå“ï¼ã¾ãŸè¦³ãŸã„&quot;,
    &quot;ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãŒè–„ãã€ãŒã£ã‹ã‚Š&quot;,
    &quot;åœ§å·»ã®ãƒ©ã‚¹ãƒˆã‚·ãƒ¼ãƒ³&quot;,
    &quot;äºŒåº¦ã¨è¦³ãŸããªã„&quot;
]

sentiments = [1, 0, 1, 0, 1, 0, 1, 0]  # 1: Positive, 0: Negative

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ï¼ˆå‰å‡¦ç† â†’ ç‰¹å¾´æŠ½å‡º â†’ ãƒ¢ãƒ‡ãƒ«ï¼‰
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=100)),
    ('svm', LinearSVC(random_state=42))
])

# è¨“ç·´
X_train, X_test, y_train, y_test = train_test_split(
    reviews, sentiments, test_size=0.25, random_state=42
)

pipeline.fit(X_train, y_train)

# è©•ä¾¡
y_pred = pipeline.predict(X_test)
print(&quot;æ­£è§£ç‡:&quot;, pipeline.score(X_test, y_test))

# æ–°ã—ã„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®æ„Ÿæƒ…äºˆæ¸¬
new_reviews = [
    &quot;æœŸå¾…å¤–ã‚Œã®ä½œå“&quot;,
    &quot;æ„Ÿå‹•çš„ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼&quot;
]

for review in new_reviews:
    sentiment = pipeline.predict([review])[0]
    label = &quot;Positive&quot; if sentiment == 1 else &quot;Negative&quot;
    print(f&quot;'{review}' â†’ {label}&quot;)
</code></pre>
<h3>4.3 å®Œå…¨ãªåˆ†é¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h3>
<p>å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ä½¿ãˆã‚‹å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¾‹ï¼š</p>
<pre><code class="language-python">import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
import joblib

class TextClassifier:
    &quot;&quot;&quot;ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡å™¨ã‚¯ãƒ©ã‚¹&quot;&quot;&quot;

    def __init__(self, model_type='nb'):
        &quot;&quot;&quot;
        Parameters:
        -----------
        model_type : str
            'nb': Naive Bayes
            'svm': Support Vector Machine
            'lr': Logistic Regression
            'rf': Random Forest
        &quot;&quot;&quot;
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.8
        )

        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        if model_type == 'nb':
            self.model = MultinomialNB()
        elif model_type == 'svm':
            self.model = LinearSVC(random_state=42)
        elif model_type == 'lr':
            self.model = LogisticRegression(random_state=42, max_iter=1000)
        elif model_type == 'rf':
            self.model = RandomForestClassifier(random_state=42, n_estimators=100)
        else:
            raise ValueError(f&quot;Unknown model type: {model_type}&quot;)

    def train(self, X_train, y_train):
        &quot;&quot;&quot;ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´&quot;&quot;&quot;
        # TF-IDFå¤‰æ›
        X_train_vec = self.vectorizer.fit_transform(X_train)

        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        self.model.fit(X_train_vec, y_train)

    def predict(self, X_test):
        &quot;&quot;&quot;äºˆæ¸¬&quot;&quot;&quot;
        X_test_vec = self.vectorizer.transform(X_test)
        return self.model.predict(X_test_vec)

    def evaluate(self, X_test, y_test):
        &quot;&quot;&quot;è©•ä¾¡&quot;&quot;&quot;
        y_pred = self.predict(X_test)

        print(&quot;æ­£è§£ç‡:&quot;, accuracy_score(y_test, y_pred))
        print(&quot;\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:&quot;)
        print(classification_report(y_test, y_pred))

        return y_pred

    def save(self, filepath):
        &quot;&quot;&quot;ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜&quot;&quot;&quot;
        joblib.dump((self.vectorizer, self.model), filepath)

    def load(self, filepath):
        &quot;&quot;&quot;ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿&quot;&quot;&quot;
        self.vectorizer, self.model = joblib.load(filepath)

# ä½¿ç”¨ä¾‹
if __name__ == &quot;__main__&quot;:
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆå®Ÿéš›ã¯CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
    texts = [
        &quot;ç´ æ™´ã‚‰ã—ã„å•†å“ã§ã™&quot;,
        &quot;æœ€æ‚ªã®å“è³ª&quot;,
        &quot;æœŸå¾…é€šã‚Šã®æ€§èƒ½&quot;,
        &quot;äºŒåº¦ã¨è²·ã„ã¾ã›ã‚“&quot;,
        # ... å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿
    ]
    labels = [1, 0, 1, 0]  # 1: Positive, 0: Negative

    # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        texts, labels, test_size=0.2, random_state=42, stratify=labels
    )

    # åˆ†é¡å™¨ã®è¨“ç·´
    classifier = TextClassifier(model_type='svm')
    classifier.train(X_train, y_train)

    # è©•ä¾¡
    y_pred = classifier.evaluate(X_test, y_test)

    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    classifier.save('text_classifier.pkl')

    # æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã®äºˆæ¸¬
    new_texts = [&quot;ã“ã®å•†å“ã¯æœ€é«˜ã§ã™&quot;, &quot;ãŒã£ã‹ã‚Šã—ã¾ã—ãŸ&quot;]
    predictions = classifier.predict(new_texts)
    print(&quot;\næ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã®äºˆæ¸¬:&quot;)
    for text, pred in zip(new_texts, predictions):
        print(f&quot;'{text}' â†’ {'Positive' if pred == 1 else 'Negative'}&quot;)
</code></pre>
<hr />
<h2>5. ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡</h2>
<h3>5.1 è©•ä¾¡æŒ‡æ¨™</h3>
<h4>æ­£è§£ç‡ (Accuracy)</h4>
<p>$$
\text{Accuracy} = \frac{\text{æ­£ã—ãåˆ†é¡ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«æ•°}}{\text{å…¨ã‚µãƒ³ãƒ—ãƒ«æ•°}}
$$</p>
<ul>
<li><strong>ä½¿ã„ã©ã“ã‚</strong>: ã‚¯ãƒ©ã‚¹ã®ãƒãƒ©ãƒ³ã‚¹ãŒå–ã‚Œã¦ã„ã‚‹å ´åˆ</li>
<li><strong>æ³¨æ„ç‚¹</strong>: ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã§ã¯ä¸é©åˆ‡</li>
</ul>
<h4>é©åˆç‡ (Precision)</h4>
<p>$$
\text{Precision} = \frac{TP}{TP + FP}
$$</p>
<ul>
<li><strong>æ„å‘³</strong>: ã€ŒPositive ã¨äºˆæ¸¬ã—ãŸã‚‚ã®ã®ã†ã¡ã€å®Ÿéš›ã« Positive ã ã£ãŸå‰²åˆã€</li>
<li><strong>ä½¿ã„ã©ã“ã‚</strong>: å½é™½æ€§ã‚’é¿ã‘ãŸã„å ´åˆï¼ˆã‚¹ãƒ‘ãƒ ãƒ•ã‚£ãƒ«ã‚¿ãªã©ï¼‰</li>
</ul>
<h4>å†ç¾ç‡ (Recall)</h4>
<p>$$
\text{Recall} = \frac{TP}{TP + FN}
$$</p>
<ul>
<li><strong>æ„å‘³</strong>: ã€Œå®Ÿéš›ã« Positive ã®ã‚‚ã®ã®ã†ã¡ã€æ­£ã—ã Positive ã¨äºˆæ¸¬ã§ããŸå‰²åˆã€</li>
<li><strong>ä½¿ã„ã©ã“ã‚</strong>: å½é™°æ€§ã‚’é¿ã‘ãŸã„å ´åˆï¼ˆç—…æ°—è¨ºæ–­ãªã©ï¼‰</li>
</ul>
<h4>F1ã‚¹ã‚³ã‚¢</h4>
<p>$$
F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$</p>
<ul>
<li><strong>æ„å‘³</strong>: Precision ã¨ Recall ã®èª¿å’Œå¹³å‡</li>
<li><strong>ä½¿ã„ã©ã“ã‚</strong>: ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸè©•ä¾¡ãŒå¿…è¦ãªå ´åˆ</li>
</ul>
<h3>5.2 æ··åŒè¡Œåˆ— (Confusion Matrix)</h3>
<pre><code class="language-python">from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# äºˆæ¸¬çµæœï¼ˆä¾‹ï¼‰
y_true = [0, 1, 0, 1, 0, 1, 1, 0]
y_pred = [0, 1, 0, 0, 0, 1, 1, 1]

# æ··åŒè¡Œåˆ—
cm = confusion_matrix(y_true, y_pred)

# å¯è¦–åŒ–
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'])
plt.ylabel('å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«')
plt.xlabel('äºˆæ¸¬ãƒ©ãƒ™ãƒ«')
plt.title('æ··åŒè¡Œåˆ—')
plt.show()

print(&quot;æ··åŒè¡Œåˆ—:&quot;)
print(cm)
print(f&quot;\nTrue Negative: {cm[0,0]}&quot;)
print(f&quot;False Positive: {cm[0,1]}&quot;)
print(f&quot;False Negative: {cm[1,0]}&quot;)
print(f&quot;True Positive: {cm[1,1]}&quot;)
</code></pre>
<h3>5.3 äº¤å·®æ¤œè¨¼ (Cross-Validation)</h3>
<p>ãƒ‡ãƒ¼ã‚¿ã‚’è¤‡æ•°ã® fold ã«åˆ†ã‘ã€ã‚ˆã‚Šä¿¡é ¼æ€§ã®é«˜ã„è©•ä¾¡ã‚’è¡Œã„ã¾ã™ã€‚</p>
<pre><code class="language-python">from sklearn.model_selection import cross_val_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=1000)),
    ('nb', MultinomialNB())
])

# 5-fold äº¤å·®æ¤œè¨¼
scores = cross_val_score(
    pipeline,
    texts,  # å…¨ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    labels,  # å…¨ãƒ©ãƒ™ãƒ«
    cv=5,  # 5åˆ†å‰²
    scoring='f1'
)

print(f&quot;å„foldã®F1ã‚¹ã‚³ã‚¢: {scores}&quot;)
print(f&quot;å¹³å‡F1ã‚¹ã‚³ã‚¢: {scores.mean():.4f} (+/- {scores.std():.4f})&quot;)
</code></pre>
<hr />
<h2>6. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</h2>
<h3>6.1 Grid Search</h3>
<pre><code class="language-python">from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('svm', LinearSVC())
])

# ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
param_grid = {
    'tfidf__max_features': [1000, 3000, 5000],
    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'tfidf__min_df': [1, 2, 5],
    'svm__C': [0.1, 1, 10]
}

# Grid Search
grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=5,
    scoring='f1',
    n_jobs=-1,  # ä¸¦åˆ—å‡¦ç†
    verbose=1
)

grid_search.fit(X_train, y_train)

# æœ€è‰¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
print(&quot;æœ€è‰¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:&quot;)
print(grid_search.best_params_)
print(f&quot;\næœ€è‰¯ã®F1ã‚¹ã‚³ã‚¢: {grid_search.best_score_:.4f}&quot;)

# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
</code></pre>
<hr />
<h2>7. å®Ÿè·µä¾‹: æ—¥æœ¬èªæ„Ÿæƒ…åˆ†æã‚·ã‚¹ãƒ†ãƒ </h2>
<h3>7.1 å®Œå…¨ãªå®Ÿè£…</h3>
<pre><code class="language-python">import pandas as pd
import MeCab
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report
import pickle

class JapaneseSentimentAnalyzer:
    &quot;&quot;&quot;æ—¥æœ¬èªæ„Ÿæƒ…åˆ†æã‚·ã‚¹ãƒ†ãƒ &quot;&quot;&quot;

    def __init__(self):
        self.mecab = MeCab.Tagger()
        self.vectorizer = None
        self.model = None

    def tokenize(self, text):
        &quot;&quot;&quot;MeCabã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–&quot;&quot;&quot;
        node = self.mecab.parseToNode(text)
        tokens = []

        while node:
            if node.surface:
                features = node.feature.split(',')
                pos = features[0]

                # åè©ã€å‹•è©ã€å½¢å®¹è©ã®ã¿æŠ½å‡º
                if pos in ['åè©', 'å‹•è©', 'å½¢å®¹è©']:
                    base_form = features[6] if len(features) &gt; 6 else node.surface
                    if base_form != '*':
                        tokens.append(base_form)

            node = node.next

        return ' '.join(tokens)

    def prepare_data(self, texts):
        &quot;&quot;&quot;ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†&quot;&quot;&quot;
        return [self.tokenize(text) for text in texts]

    def train(self, texts, labels):
        &quot;&quot;&quot;ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´&quot;&quot;&quot;
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        tokenized_texts = self.prepare_data(texts)

        # TF-IDF ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        self.vectorizer = TfidfVectorizer(
            max_features=3000,
            ngram_range=(1, 2),
            min_df=2
        )
        X = self.vectorizer.fit_transform(tokenized_texts)

        # SVM è¨“ç·´
        self.model = LinearSVC(C=1.0, random_state=42)
        self.model.fit(X, labels)

    def predict(self, texts):
        &quot;&quot;&quot;æ„Ÿæƒ…äºˆæ¸¬&quot;&quot;&quot;
        tokenized_texts = self.prepare_data(texts)
        X = self.vectorizer.transform(tokenized_texts)
        return self.model.predict(X)

    def predict_proba(self, texts):
        &quot;&quot;&quot;ç¢ºç‡ä»˜ãäºˆæ¸¬ï¼ˆSVMã®æ±ºå®šé–¢æ•°ã‚’ä½¿ç”¨ï¼‰&quot;&quot;&quot;
        tokenized_texts = self.prepare_data(texts)
        X = self.vectorizer.transform(tokenized_texts)
        decision = self.model.decision_function(X)
        return decision

    def save(self, filepath):
        &quot;&quot;&quot;ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜&quot;&quot;&quot;
        with open(filepath, 'wb') as f:
            pickle.dump((self.vectorizer, self.model), f)

    def load(self, filepath):
        &quot;&quot;&quot;ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿&quot;&quot;&quot;
        with open(filepath, 'rb') as f:
            self.vectorizer, self.model = pickle.load(f)

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
if __name__ == &quot;__main__&quot;:
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã¯å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ï¼‰
    reviews = [
        &quot;ã“ã®å•†å“ã¯æœ¬å½“ã«ç´ æ™´ã‚‰ã—ã„ã€‚æœŸå¾…ä»¥ä¸Šã§ã—ãŸ&quot;,
        &quot;æœ€æ‚ªã®å“è³ªã€‚äºŒåº¦ã¨è²·ã„ã¾ã›ã‚“&quot;,
        &quot;å€¤æ®µã®å‰²ã«ã¯è‰¯ã„å•†å“ã ã¨æ€ã„ã¾ã™&quot;,
        &quot;é…é€ãŒé…ãã€å•†å“ã‚‚å‚·ã ã‚‰ã‘&quot;,
        &quot;ä½¿ã„ã‚„ã™ãã¦æº€è¶³ã—ã¦ã„ã¾ã™&quot;,
        &quot;èª¬æ˜ã¨é•ã†å•†å“ãŒå±Šã„ãŸ&quot;,
        &quot;ã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒé«˜ã„&quot;,
        &quot;æœŸå¾…å¤–ã‚Œã§ãŒã£ã‹ã‚Š&quot;,
        &quot;ãƒ‡ã‚¶ã‚¤ãƒ³ãŒç¾ã—ãã€æ©Ÿèƒ½ã‚‚å……å®Ÿ&quot;,
        &quot;å£Šã‚Œã‚„ã™ãã€ã™ãã«æ•…éšœã—ãŸ&quot;
    ]

    sentiments = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1: Positive, 0: Negative

    # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        reviews, sentiments, test_size=0.2, random_state=42
    )

    # åˆ†æå™¨ã®è¨“ç·´
    analyzer = JapaneseSentimentAnalyzer()
    analyzer.train(X_train, y_train)

    # è©•ä¾¡
    y_pred = analyzer.predict(X_test)
    print(&quot;åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:&quot;)
    print(classification_report(y_test, y_pred,
                              target_names=['Negative', 'Positive']))

    # æ–°ã—ã„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®æ„Ÿæƒ…åˆ†æ
    new_reviews = [
        &quot;ã¨ã¦ã‚‚è‰¯ã„è²·ã„ç‰©ãŒã§ãã¾ã—ãŸ&quot;,
        &quot;ãŠé‡‘ã‚’ç„¡é§„ã«ã—ãŸ&quot;,
        &quot;æ™®é€šã®å•†å“ã§ã™&quot;
    ]

    predictions = analyzer.predict(new_reviews)
    scores = analyzer.predict_proba(new_reviews)

    print(&quot;\næ–°ã—ã„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®æ„Ÿæƒ…åˆ†æ:&quot;)
    for review, pred, score in zip(new_reviews, predictions, scores):
        sentiment = &quot;Positive&quot; if pred == 1 else &quot;Negative&quot;
        confidence = abs(score)
        print(f&quot;'{review}'&quot;)
        print(f&quot;  â†’ {sentiment} (ä¿¡é ¼åº¦: {confidence:.2f})\n&quot;)

    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    analyzer.save('sentiment_model.pkl')
</code></pre>
<hr />
<h2>8. ã¾ã¨ã‚</h2>
<h3>8.1 æœ¬ç« ã§å­¦ã‚“ã ã“ã¨</h3>
<p>âœ… <strong>ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†</strong>
- ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã€æ­£è¦åŒ–ã®å®Ÿè£…
- æ—¥æœ¬èªã¨è‹±èªã®å‰å‡¦ç†ã®é•ã„</p>
<p>âœ… <strong>ç‰¹å¾´é‡æŠ½å‡º</strong>
- Bag-of-Words (BoW) ã®ä»•çµ„ã¿
- TF-IDF ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
- N-gram ã®æ´»ç”¨</p>
<p>âœ… <strong>æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«</strong>
- Naive Bayes ã«ã‚ˆã‚‹åˆ†é¡
- SVM ã«ã‚ˆã‚‹é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿åˆ†é¡
- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰</p>
<p>âœ… <strong>ãƒ¢ãƒ‡ãƒ«è©•ä¾¡</strong>
- æ­£è§£ç‡ã€é©åˆç‡ã€å†ç¾ç‡ã€F1ã‚¹ã‚³ã‚¢
- æ··åŒè¡Œåˆ—ã®èª­ã¿æ–¹
- äº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹ä¿¡é ¼æ€§å‘ä¸Š</p>
<p>âœ… <strong>å®Ÿè·µã‚·ã‚¹ãƒ†ãƒ </strong>
- æ—¥æœ¬èªæ„Ÿæƒ…åˆ†æã‚·ã‚¹ãƒ†ãƒ ã®å®Œå…¨å®Ÿè£…</p>
<h3>8.2 æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>
<p>æœ¬ç« ã§åŸºç¤çš„ãªãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚’å­¦ã³ã¾ã—ãŸã€‚ã•ã‚‰ã«ç™ºå±•ã•ã›ã‚‹ã«ã¯ï¼š</p>
<ol>
<li><strong>æ·±å±¤å­¦ç¿’ã®æ´»ç”¨</strong>: LSTMã€Transformer ãªã©ã®ãƒ¢ãƒ‡ãƒ«</li>
<li><strong>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</strong>: å®Ÿéš›ã®ãƒ“ã‚¸ãƒã‚¹ãƒ‡ãƒ¼ã‚¿ã§ã®å®Ÿé¨“</li>
<li><strong>å¤šã‚¯ãƒ©ã‚¹åˆ†é¡</strong>: 3ã¤ä»¥ä¸Šã®ã‚«ãƒ†ã‚´ãƒªã¸ã®æ‹¡å¼µ</li>
<li><strong>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†é¡</strong>: Web API ã¨ã—ã¦ãƒ‡ãƒ—ãƒ­ã‚¤</li>
</ol>
<hr />
<h2>9. ç·´ç¿’å•é¡Œ</h2>
<h3>å•é¡Œ1: å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆåŸºç¤ï¼‰</h3>
<p>æ¬¡ã®ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦ã€é©åˆ‡ãªå‰å‡¦ç†ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>
<pre><code class="language-python">text = &quot;&quot;&quot;
ã€é€Ÿå ±ã€‘æ–°è£½å“ç™ºå£²ï¼ï¼ï¼ğŸ‰
ä»Šãªã‚‰50%OFF â†’ http://example.com
#ã‚»ãƒ¼ãƒ« #ãŠå¾— @å…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
&quot;&quot;&quot;

# ã‚ãªãŸã®å®Ÿè£…:
def preprocess(text):
    # ã“ã“ã«ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã
    pass

result = preprocess(text)
print(result)
</code></pre>
<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python">import re
import unicodedata

def preprocess(text):
    # Unicodeæ­£è¦åŒ–
    text = unicodedata.normalize('NFKC', text)

    # URLé™¤å»
    text = re.sub(r'http\S+', '', text)

    # ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°é™¤å»
    text = re.sub(r'[@#]\S+', '', text)

    # çµµæ–‡å­—é™¤å»
    text = re.sub(r'[^\w\s]', '', text, flags=re.UNICODE)

    # ä½™åˆ†ãªç©ºç™½é™¤å»
    text = ' '.join(text.split())

    return text

result = preprocess(text)
print(result)  # å‡ºåŠ›: &quot;é€Ÿå ± æ–°è£½å“ç™ºå£² ä»Šãªã‚‰50OFF&quot;
</code></pre>


</details>

<h3>å•é¡Œ2: TF-IDF å®Ÿè£…ï¼ˆä¸­ç´šï¼‰</h3>
<p>ä»¥ä¸‹ã®æ–‡æ›¸é›†åˆã«å¯¾ã—ã¦ã€TF-IDF ã‚’è¨ˆç®—ã—ã€å„æ–‡æ›¸ã§æœ€ã‚‚é‡è¦ãªå˜èªãƒˆãƒƒãƒ—3ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚</p>
<pre><code class="language-python">documents = [
    &quot;æ©Ÿæ¢°å­¦ç¿’ã¯äººå·¥çŸ¥èƒ½ã®ä¸€åˆ†é‡&quot;,
    &quot;æ·±å±¤å­¦ç¿’ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã†&quot;,
    &quot;è‡ªç„¶è¨€èªå‡¦ç†ã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’æ‰±ã†äººå·¥çŸ¥èƒ½æŠ€è¡“&quot;,
    &quot;ç”»åƒèªè­˜ã¯æ·±å±¤å­¦ç¿’ã®å¿œç”¨ä¾‹&quot;
]

# ã‚ãªãŸã®å®Ÿè£…
</code></pre>
<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer

documents = [
    &quot;æ©Ÿæ¢°å­¦ç¿’ã¯äººå·¥çŸ¥èƒ½ã®ä¸€åˆ†é‡&quot;,
    &quot;æ·±å±¤å­¦ç¿’ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã†&quot;,
    &quot;è‡ªç„¶è¨€èªå‡¦ç†ã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’æ‰±ã†äººå·¥çŸ¥èƒ½æŠ€è¡“&quot;,
    &quot;ç”»åƒèªè­˜ã¯æ·±å±¤å­¦ç¿’ã®å¿œç”¨ä¾‹&quot;
]

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)
feature_names = vectorizer.get_feature_names_out()

for doc_idx in range(len(documents)):
    print(f&quot;\næ–‡æ›¸{doc_idx+1}: {documents[doc_idx]}&quot;)

    # å„æ–‡æ›¸ã®TF-IDFã‚¹ã‚³ã‚¢ã‚’å–å¾—
    feature_index = tfidf_matrix[doc_idx].nonzero()[1]
    tfidf_scores = [(feature_names[i], tfidf_matrix[doc_idx, i])
                    for i in feature_index]

    # ã‚¹ã‚³ã‚¢é™é †ã§ã‚½ãƒ¼ãƒˆ
    tfidf_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)

    print(&quot;  é‡è¦å˜èªãƒˆãƒƒãƒ—3:&quot;)
    for word, score in tfidf_scores[:3]:
        print(f&quot;    {word}: {score:.4f}&quot;)
</code></pre>


</details>

<h3>å•é¡Œ3: æ„Ÿæƒ…åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå¿œç”¨ï¼‰</h3>
<p>ä»¥ä¸‹ã®ä»•æ§˜ã§æ„Ÿæƒ…åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š</p>
<p><strong>ä»•æ§˜:</strong>
- å…¥åŠ›: æ—¥æœ¬èªãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
- å‡ºåŠ›: Positive/Negative + ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
- ãƒ¢ãƒ‡ãƒ«: Naive Bayes ã¾ãŸã¯ SVM
- è©•ä¾¡æŒ‡æ¨™: F1ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º</p>
<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python">from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, f1_score
import numpy as np

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
reviews = [
    &quot;ç´ æ™´ã‚‰ã—ã„å•†å“ã§ã™&quot;, &quot;æœ€æ‚ªã®å“è³ª&quot;, &quot;æœŸå¾…é€šã‚Š&quot;,
    &quot;ãŒã£ã‹ã‚Š&quot;, &quot;å¤§æº€è¶³&quot;, &quot;äºŒåº¦ã¨è²·ã‚ãªã„&quot;,
    &quot;ãŠã™ã™ã‚ã§ã™&quot;, &quot;ä¾¡å€¤ãŒãªã„&quot;
]
labels = [1, 0, 1, 0, 1, 0, 1, 0]

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    reviews, labels, test_size=0.25, random_state=42
)

# TF-IDF + Naive Bayes
vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

model = MultinomialNB()
model.fit(X_train_vec, y_train)

# äºˆæ¸¬ã¨è©•ä¾¡
y_pred = model.predict(X_test_vec)
f1 = f1_score(y_test, y_pred)

print(f&quot;F1ã‚¹ã‚³ã‚¢: {f1:.4f}&quot;)
print(&quot;\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:&quot;)
print(classification_report(y_test, y_pred,
                          target_names=['Negative', 'Positive']))

# æ–°ã—ã„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®äºˆæ¸¬
new_review = [&quot;ã“ã®å•†å“ã¯æœ€é«˜ã§ã™&quot;]
new_vec = vectorizer.transform(new_review)
prediction = model.predict(new_vec)[0]
proba = model.predict_proba(new_vec)[0]

sentiment = &quot;Positive&quot; if prediction == 1 else &quot;Negative&quot;
confidence = proba[prediction]

print(f&quot;\n'{new_review[0]}'&quot;)
print(f&quot;â†’ {sentiment} (ä¿¡é ¼åº¦: {confidence:.2f})&quot;)
</code></pre>


</details>

<hr />
<h2>10. å‚è€ƒæ–‡çŒ®</h2>
<h3>æ›¸ç±</h3>
<ol>
<li>ã€Œå…¥é–€ è‡ªç„¶è¨€èªå‡¦ç†ã€Steven Bird ä»–ï¼ˆã‚ªãƒ©ã‚¤ãƒªãƒ¼ãƒ»ã‚¸ãƒ£ãƒ‘ãƒ³ï¼‰</li>
<li>ã€Œscikit-learn ã¨ TensorFlow ã«ã‚ˆã‚‹å®Ÿè·µæ©Ÿæ¢°å­¦ç¿’ã€AurÃ©lien GÃ©ron</li>
<li>ã€ŒPythonã§ã¯ã˜ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹å…¥é–€ã€ä½è—¤ æ•ç´€</li>
</ol>
<h3>ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹</h3>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction">scikit-learn Text Feature Extraction</a></li>
<li><a href="https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews">Kaggle: Sentiment Analysis Tutorial</a></li>
<li><a href="https://qiita.com/Hironsan/items/2466fe0f344115aff177">æ—¥æœ¬èªè‡ªç„¶è¨€èªå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¾ã¨ã‚</a></li>
</ul>
<h3>ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</h3>
<ul>
<li><a href="https://www.rondhuit.com/download.html">livedoor ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹</a> - æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡</li>
<li><a href="http://www.db.info.gifu-u.ac.jp/data/Data_5d832973308d57446583ed9f">æ—¥æœ¬èªè©•åˆ¤åˆ†æãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</a> - æ„Ÿæƒ…åˆ†æ</li>
<li><a href="https://ai.stanford.edu/~amaas/data/sentiment/">IMDB Movie Reviews</a> - è‹±èªæ„Ÿæƒ…åˆ†æ</li>
</ul>
<hr />
<p><strong>æ¬¡ã¸</strong>: <a href="chapter-4.html">Chapter 4: å®Ÿä¸–ç•Œã®NLPå¿œç”¨ â†’</a></p>
<p><strong>å‰ã¸</strong>: <a href="chapter-2.html">â† Chapter 2: å½¢æ…‹ç´ è§£æãƒ»æ§‹æ–‡è§£æ</a></p>
<p><strong>ç›®æ¬¡ã¸</strong>: <a href="index.html">â†‘ ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a></p><div class="navigation">
    <a href="chapter-2.html" class="nav-button">â† ç¬¬2ç« </a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter-4.html" class="nav-button">ç¬¬4ç«  â†’</a>
</div>
    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimotoï¼ˆæ±åŒ—å¤§å­¦ï¼‰</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
