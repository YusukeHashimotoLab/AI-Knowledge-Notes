<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 3: ÂÆüË∑µ - „ÉÜ„Ç≠„Çπ„ÉàÂàÜÈ°û - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Chapter 3: ÂÆüË∑µ - „ÉÜ„Ç≠„Çπ„ÉàÂàÜÈ°û</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 35-40ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ÂàùÁ¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 0ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 0Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 3: ÂÆüË∑µ - „ÉÜ„Ç≠„Çπ„ÉàÂàÜÈ°û</h1>
<h2>Êú¨Á´†„ÅÆÊ¶ÇË¶Å</h2>
<p>„ÉÜ„Ç≠„Çπ„ÉàÂàÜÈ°ûÔºàText ClassificationÔºâ„ÅØ„ÄÅNLP„ÅÆÊúÄ„ÇÇÂÆüÁî®ÁöÑ„Å™„Çø„Çπ„ÇØ„ÅÆ1„Å§„Åß„Åô„ÄÇÊÑüÊÉÖÂàÜÊûê„ÄÅ„Çπ„Éë„É†„Éï„Ç£„É´„Çø„ÄÅ„Éã„É•„Éº„ÇπË®ò‰∫ã„ÅÆ„Ç´„ÉÜ„Ç¥„É™ÂàÜÈ°û„Å™„Å©„ÄÅÂπÖÂ∫É„ÅÑÂøúÁî®„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ</p>
<p>Êú¨Á´†„Åß„ÅØ„ÄÅÂÆüÈöõ„Å´„ÉÜ„Ç≠„Çπ„ÉàÂàÜÈ°û„É¢„Éá„É´„ÇíÊßãÁØâ„Åô„ÇãÊñπÊ≥ï„Çí„ÄÅÂÆåÂÖ®„Å™ÂÆüË£Ö‰æã„Å®„Å®„ÇÇ„Å´Â≠¶„Å≥„Åæ„Åô„ÄÇ</p>
<h3>Â≠¶ÁøíÁõÆÊ®ô</h3>
<ul>
<li>‚úÖ „ÉÜ„Ç≠„Çπ„ÉàÂâçÂá¶ÁêÜ„Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆÂÆüË£Ö</li>
<li>‚úÖ Bag-of-Words „Å® TF-IDF „Å´„Çà„ÇãÁâπÂæ¥ÈáèÊäΩÂá∫</li>
<li>‚úÖ Ê©üÊ¢∞Â≠¶Áøí„Ç¢„É´„Ç¥„É™„Ç∫„É†„Å´„Çà„ÇãÂàÜÈ°û</li>
<li>‚úÖ „É¢„Éá„É´„ÅÆË©ï‰æ°„Å®ÊîπÂñÑÊâãÊ≥ï</li>
<li>‚úÖ ÂÆüÈöõ„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÅÆÂÆüË£ÖÁµåÈ®ì</li>
</ul>
<hr />
<h2>1. „ÉÜ„Ç≠„Çπ„ÉàÂàÜÈ°û„Å®„ÅØ</h2>
<h3>1.1 ÂÆöÁæ©„Å®ÂøúÁî®‰æã</h3>
<p><strong>„ÉÜ„Ç≠„Çπ„ÉàÂàÜÈ°ûÔºàText ClassificationÔºâ</strong> „Å®„ÅØ„ÄÅ„ÉÜ„Ç≠„Çπ„Éà„Çí‰∫à„ÇÅÂÆöÁæ©„Åï„Çå„Åü„Ç´„ÉÜ„Ç¥„É™„Å´Ëá™ÂãïÁöÑ„Å´ÂàÜÈ°û„Åô„Çã„Çø„Çπ„ÇØ„Åß„Åô„ÄÇ</p>
<h4>‰∏ª„Å™ÂøúÁî®‰æã</h4>
<table>
<thead>
<tr>
<th>ÂøúÁî®ÂàÜÈáé</th>
<th>ÂÖ∑‰Ωì‰æã</th>
<th>„Éì„Ç∏„Éç„Çπ‰æ°ÂÄ§</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ÊÑüÊÉÖÂàÜÊûê</strong></td>
<td>ÂïÜÂìÅ„É¨„Éì„É•„Éº„ÅÆ Positive/Negative Âà§ÂÆö</td>
<td>È°ßÂÆ¢Ê∫ÄË∂≥Â∫¶„ÅÆÊääÊè°</td>
</tr>
<tr>
<td><strong>„Çπ„Éë„É†„Éï„Ç£„É´„Çø</strong></td>
<td>„É°„Éº„É´„ÅÆ„Çπ„Éë„É†/Èùû„Çπ„Éë„É†ÂàÜÈ°û</td>
<td>„Çª„Ç≠„É•„É™„ÉÜ„Ç£Âêë‰∏ä</td>
</tr>
<tr>
<td><strong>„Ç´„ÉÜ„Ç¥„É™ÂàÜÈ°û</strong></td>
<td>„Éã„É•„Éº„ÇπË®ò‰∫ã„ÅÆ„Ç∏„É£„É≥„É´ÂàÜÈ°û</td>
<td>„Ç≥„É≥„ÉÜ„É≥„ÉÑÊé®Ëñ¶</td>
</tr>
<tr>
<td><strong>ÊÑèÂõ≥Êé®ÂÆö</strong></td>
<td>Âïè„ÅÑÂêà„Çè„Åõ„ÅÆÁ®ÆÈ°ûÂà§ÂÆö</td>
<td>„Ç´„Çπ„Çø„Éû„Éº„Çµ„Éù„Éº„ÉàÂäπÁéáÂåñ</td>
</tr>
<tr>
<td><strong>„Éà„Éî„ÉÉ„ÇØÂàÜÈ°û</strong></td>
<td>Ë´ñÊñá„ÅÆÁ†îÁ©∂ÂàÜÈáéÂàÜÈ°û</td>
<td>ÊÉÖÂ†±Êï¥ÁêÜ</td>
</tr>
</tbody>
</table>
<h3>1.2 „ÉÜ„Ç≠„Çπ„ÉàÂàÜÈ°û„ÅÆÊµÅ„Çå</h3>
<div class="mermaid">
graph LR
    A[Áîü„ÉÜ„Ç≠„Çπ„Éà] --> B[ÂâçÂá¶ÁêÜ]
    B --> C[ÁâπÂæ¥ÈáèÊäΩÂá∫]
    C --> D[„É¢„Éá„É´Â≠¶Áøí]
    D --> E[‰∫àÊ∏¨]
    E --> F[Ë©ï‰æ°]
    F -->|ÊîπÂñÑ| B
</div>

<ol>
<li><strong>ÂâçÂá¶ÁêÜ</strong>: „ÉÜ„Ç≠„Çπ„Éà„ÅÆ„ÇØ„É™„Éº„Éã„É≥„Ç∞„ÄÅÊ≠£Ë¶èÂåñ</li>
<li><strong>ÁâπÂæ¥ÈáèÊäΩÂá∫</strong>: „ÉÜ„Ç≠„Çπ„Éà„ÇíÊï∞ÂÄ§„Éô„ÇØ„Éà„É´„Å´Â§âÊèõ</li>
<li><strong>„É¢„Éá„É´Â≠¶Áøí</strong>: Ê©üÊ¢∞Â≠¶Áøí„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅßÂ≠¶Áøí</li>
<li><strong>‰∫àÊ∏¨</strong>: Êñ∞„Åó„ÅÑ„ÉÜ„Ç≠„Çπ„Éà„ÇíÂàÜÈ°û</li>
<li><strong>Ë©ï‰æ°</strong>: „É¢„Éá„É´„ÅÆÊÄßËÉΩ„ÇíÊ∏¨ÂÆö</li>
</ol>
<hr />
<h2>2. „ÉÜ„Ç≠„Çπ„ÉàÂâçÂá¶ÁêÜ„Éë„Ç§„Éó„É©„Ç§„É≥</h2>
<h3>2.1 „Å™„ÅúÂâçÂá¶ÁêÜ„ÅåÈáçË¶Å„Åã</h3>
<p>Áîü„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„Éá„Éº„Çø„Å´„ÅØ‰ª•‰∏ã„ÅÆÂïèÈ°å„Åå„ÅÇ„Çä„Åæ„ÅôÔºö</p>
<ul>
<li><strong>„Éé„Ç§„Ç∫</strong>: URL„ÄÅË®òÂè∑„ÄÅÁµµÊñáÂ≠ó„Å™„Å©‰∏çË¶Å„Å™ÊÉÖÂ†±</li>
<li><strong>Ë°®Ë®ò„ÇÜ„Çå</strong>: "„Éë„ÇΩ„Ç≥„É≥" vs "„Éë„Éº„ÇΩ„Éä„É´„Ç≥„É≥„Éî„É•„Éº„Çø"</li>
<li><strong>Â§ßÊñáÂ≠óÂ∞èÊñáÂ≠ó</strong>: "Apple" vs "apple"</li>
<li><strong>Ê¥ªÁî®ÂΩ¢</strong>: "Ëµ∞„Çã", "Ëµ∞„Å£„Åü", "Ëµ∞„Çå„Å∞"</li>
</ul>
<p>ÂâçÂá¶ÁêÜ„Å´„Çà„Å£„Å¶„Åì„Çå„Çâ„ÇíÁµ±‰∏Ä„Åó„ÄÅ„É¢„Éá„É´„ÅÆÂ≠¶ÁøíÂäπÁéá„Çí‰∏ä„Åí„Åæ„Åô„ÄÇ</p>
<h3>2.2 Âü∫Êú¨ÁöÑ„Å™ÂâçÂá¶ÁêÜ„Çπ„ÉÜ„ÉÉ„Éó</h3>
<h4>Ëã±Ë™û„ÉÜ„Ç≠„Çπ„Éà„ÅÆÂâçÂá¶ÁêÜ</h4>
<pre><code class="language-python">import re
import string
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.tokenize import word_tokenize

class EnglishTextPreprocessor:
    &quot;&quot;&quot;Ëã±Ë™û„ÉÜ„Ç≠„Çπ„ÉàÂâçÂá¶ÁêÜ„ÇØ„É©„Çπ&quot;&quot;&quot;

    def __init__(self):
        self.stop_words = set(stopwords.words('english'))
        self.stemmer = PorterStemmer()
        self.lemmatizer = WordNetLemmatizer()

    def clean_text(self, text):
        &quot;&quot;&quot;„ÉÜ„Ç≠„Çπ„Éà„ÅÆ„ÇØ„É™„Éº„Éã„É≥„Ç∞&quot;&quot;&quot;
        # Â∞èÊñáÂ≠óÂåñ
        text = text.lower()

        # URL„ÅÆÈô§Âéª
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

        # „É°„É≥„Ç∑„Éß„É≥„Éª„Éè„ÉÉ„Ç∑„É•„Çø„Ç∞„ÅÆÈô§ÂéªÔºàTwitterÁî®Ôºâ
        text = re.sub(r'@\w+|#\w+', '', text)

        # HTML„Çø„Ç∞„ÅÆÈô§Âéª
        text = re.sub(r'&lt;.*?&gt;', '', text)

        # Âè•Ë™≠ÁÇπ„ÅÆÈô§Âéª
        text = text.translate(str.maketrans('', '', string.punctuation))

        # Êï∞Â≠ó„ÅÆÈô§ÂéªÔºà„Ç™„Éó„Ç∑„Éß„É≥Ôºâ
        text = re.sub(r'\d+', '', text)

        # ‰ΩôÂàÜ„Å™Á©∫ÁôΩ„ÅÆÈô§Âéª
        text = ' '.join(text.split())

        return text

    def tokenize_and_filter(self, text):
        &quot;&quot;&quot;„Éà„Éº„ÇØ„É≥Âåñ„Å®„Çπ„Éà„ÉÉ„Éó„ÉØ„Éº„ÉâÈô§Âéª&quot;&quot;&quot;
        # „Éà„Éº„ÇØ„É≥Âåñ
        tokens = word_tokenize(text)

        # „Çπ„Éà„ÉÉ„Éó„ÉØ„Éº„ÉâÈô§Âéª
        tokens = [word for word in tokens if word not in self.stop_words]

        # Áü≠„Åô„Åé„ÇãÂçòË™û„ÇíÈô§ÂéªÔºà2ÊñáÂ≠óÊú™Ê∫ÄÔºâ
        tokens = [word for word in tokens if len(word) &gt; 2]

        return tokens

    def stem_tokens(self, tokens):
        &quot;&quot;&quot;„Çπ„ÉÜ„Éü„É≥„Ç∞ÔºàË™ûÂππÊäΩÂá∫Ôºâ&quot;&quot;&quot;
        return [self.stemmer.stem(token) for token in tokens]

    def lemmatize_tokens(self, tokens):
        &quot;&quot;&quot;„É¨„É≥„ÉûÂåñÔºàË¶ãÂá∫„ÅóË™ûÂåñÔºâ&quot;&quot;&quot;
        return [self.lemmatizer.lemmatize(token) for token in tokens]

    def preprocess(self, text, use_lemmatization=True):
        &quot;&quot;&quot;ÂÆåÂÖ®„Å™ÂâçÂá¶ÁêÜ„Éë„Ç§„Éó„É©„Ç§„É≥&quot;&quot;&quot;
        # 1. „ÇØ„É™„Éº„Éã„É≥„Ç∞
        text = self.clean_text(text)

        # 2. „Éà„Éº„ÇØ„É≥Âåñ„Å®„Éï„Ç£„É´„Çø„É™„É≥„Ç∞
        tokens = self.tokenize_and_filter(text)

        # 3. Ê≠£Ë¶èÂåñÔºà„Çπ„ÉÜ„Éü„É≥„Ç∞ or „É¨„É≥„ÉûÂåñÔºâ
        if use_lemmatization:
            tokens = self.lemmatize_tokens(tokens)
        else:
            tokens = self.stem_tokens(tokens)

        return ' '.join(tokens)

# ‰ΩøÁî®‰æã
preprocessor = EnglishTextPreprocessor()

raw_text = &quot;&quot;&quot;
This is AMAZING!!! Check out http://example.com
I absolutely love this product üòç #happy @company
&quot;&quot;&quot;

processed_text = preprocessor.preprocess(raw_text)
print(f&quot;ÂÖÉ„ÅÆ„ÉÜ„Ç≠„Çπ„Éà: {raw_text}&quot;)
print(f&quot;ÂâçÂá¶ÁêÜÂæå: {processed_text}&quot;)
# Âá∫Âäõ‰æã: &quot;amazing absolutely love product happy company&quot;
</code></pre>
<h4>Êó•Êú¨Ë™û„ÉÜ„Ç≠„Çπ„Éà„ÅÆÂâçÂá¶ÁêÜ</h4>
<pre><code class="language-python">import re
import MeCab
import unicodedata

class JapaneseTextPreprocessor:
    &quot;&quot;&quot;Êó•Êú¨Ë™û„ÉÜ„Ç≠„Çπ„ÉàÂâçÂá¶ÁêÜ„ÇØ„É©„Çπ&quot;&quot;&quot;

    def __init__(self):
        self.mecab = MeCab.Tagger()

        # Êó•Êú¨Ë™û„Çπ„Éà„ÉÉ„Éó„ÉØ„Éº„ÉâÔºà‰∏ÄÈÉ®Ôºâ
        self.stop_words = set([
            '„ÅÆ', '„ÅØ', '„Çí', '„Åå', '„Å´', '„Åß', '„Å®', '„Åü', '„Åó',
            '„Å¶', '„Åæ„Åô', '„Åß„Åô', '„ÅÇ„Çã', '„ÅÑ„Çã', '„Åì„ÅÆ', '„Åù„ÅÆ',
            '„Çå„Çã', '„Çâ„Çå„Çã', '„Åõ„Çã', '„Åï„Åõ„Çã', '„Åì„Å®', '„ÇÇ„ÅÆ'
        ])

        # Èô§Â§ñ„Åô„ÇãÂìÅË©û
        self.exclude_pos = set([
            'Âä©Ë©û', 'Âä©ÂãïË©û', 'Êé•Á∂öË©û', 'Ë®òÂè∑', '„Éï„Ç£„É©„Éº', 'ÊÑüÂãïË©û'
        ])

    def clean_text(self, text):
        &quot;&quot;&quot;„ÉÜ„Ç≠„Çπ„Éà„ÅÆ„ÇØ„É™„Éº„Éã„É≥„Ç∞&quot;&quot;&quot;
        # UnicodeÊ≠£Ë¶èÂåñÔºàNFKC„ÅßÂçäËßí„ÉªÂÖ®Ëßí„ÇíÁµ±‰∏ÄÔºâ
        text = unicodedata.normalize('NFKC', text)

        # URL„ÅÆÈô§Âéª
        text = re.sub(r'http\S+|www\S+|https\S+', '', text)

        # „É°„É≥„Ç∑„Éß„É≥„Éª„Éè„ÉÉ„Ç∑„É•„Çø„Ç∞„ÅÆÈô§Âéª
        text = re.sub(r'@\w+|#\w+', '', text)

        # ÊîπË°å„Éª„Çø„Éñ„ÇíÁ©∫ÁôΩ„Å´
        text = re.sub(r'[\n\t]', ' ', text)

        # ÁµµÊñáÂ≠ó„ÅÆÈô§ÂéªÔºà„Ç™„Éó„Ç∑„Éß„É≥Ôºâ
        emoji_pattern = re.compile(
            &quot;[&quot;
            &quot;\U0001F600-\U0001F64F&quot;  # È°îÊñáÂ≠ó
            &quot;\U0001F300-\U0001F5FF&quot;  # Ë®òÂè∑„ÉªÁµµÊñáÂ≠ó
            &quot;\U0001F680-\U0001F6FF&quot;  # ‰πó„ÇäÁâ©„ÉªÂª∫Áâ©
            &quot;\U0001F1E0-\U0001F1FF&quot;  # Êóó
            &quot;]+&quot;, flags=re.UNICODE
        )
        text = emoji_pattern.sub('', text)

        # ‰ΩôÂàÜ„Å™Á©∫ÁôΩ„ÅÆÈô§Âéª
        text = ' '.join(text.split())

        return text

    def tokenize_with_filter(self, text):
        &quot;&quot;&quot;MeCab„Åß„Éà„Éº„ÇØ„É≥Âåñ + „Éï„Ç£„É´„Çø„É™„É≥„Ç∞&quot;&quot;&quot;
        # MeCab„ÅßÂΩ¢ÊÖãÁ¥†Ëß£Êûê
        node = self.mecab.parseToNode(text)

        words = []
        while node:
            if node.surface:
                features = node.feature.split(',')
                surface = node.surface
                pos = features[0]  # ÂìÅË©û
                base_form = features[6] if len(features) &gt; 6 else surface

                # „Éï„Ç£„É´„Çø„É™„É≥„Ç∞Êù°‰ª∂
                if (pos not in self.exclude_pos and
                    surface not in self.stop_words and
                    len(surface) &gt; 1):  # 1ÊñáÂ≠óÂçòË™û„ÇíÈô§Â§ñ

                    # Âü∫Êú¨ÂΩ¢„Çí‰ΩøÁî®
                    words.append(base_form if base_form != '*' else surface)

            node = node.next

        return words

    def preprocess(self, text):
        &quot;&quot;&quot;ÂÆåÂÖ®„Å™ÂâçÂá¶ÁêÜ„Éë„Ç§„Éó„É©„Ç§„É≥&quot;&quot;&quot;
        # 1. „ÇØ„É™„Éº„Éã„É≥„Ç∞
        text = self.clean_text(text)

        # 2. „Éà„Éº„ÇØ„É≥Âåñ„Å®„Éï„Ç£„É´„Çø„É™„É≥„Ç∞
        tokens = self.tokenize_with_filter(text)

        return ' '.join(tokens)

# ‰ΩøÁî®‰æã
jp_preprocessor = JapaneseTextPreprocessor()

raw_jp_text = &quot;&quot;&quot;
„Åì„ÅÆÂïÜÂìÅ„ÅØÊú¨ÂΩì„Å´Á¥†Êô¥„Çâ„Åó„ÅÑ„Åß„ÅôÔºÅÔºÅÔºÅüòç
Áµ∂ÂØæ„Å´„Åä„Åô„Åô„ÇÅ„Åó„Åæ„Åôüéâ #ÊúÄÈ´ò
Ë©≥„Åó„Åè„ÅØ„Åì„Å°„Çâ‚Üí http://example.com
&quot;&quot;&quot;

processed_jp_text = jp_preprocessor.preprocess(raw_jp_text)
print(f&quot;ÂÖÉ„ÅÆ„ÉÜ„Ç≠„Çπ„Éà: {raw_jp_text}&quot;)
print(f&quot;ÂâçÂá¶ÁêÜÂæå: {processed_jp_text}&quot;)
# Âá∫Âäõ‰æã: &quot;ÂïÜÂìÅ Êú¨ÂΩì Á¥†Êô¥„Çâ„Åó„ÅÑ Áµ∂ÂØæ „Åä„Åô„Åô„ÇÅ ÊúÄÈ´ò Ë©≥„Åó„ÅÑ&quot;
</code></pre>
<hr />
<h2>3. ÁâπÂæ¥ÈáèÊäΩÂá∫</h2>
<p>Ê©üÊ¢∞Â≠¶Áøí„É¢„Éá„É´„ÅØÊï∞ÂÄ§„Åó„ÅãÊâ±„Åà„Åæ„Åõ„Çì„ÄÇ„ÉÜ„Ç≠„Çπ„Éà„ÇíÊï∞ÂÄ§„Éô„ÇØ„Éà„É´„Å´Â§âÊèõ„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ</p>
<h3>3.1 Bag-of-Words (BoW)</h3>
<p><strong>Bag-of-Words</strong> „ÅØÊúÄ„ÇÇ„Ç∑„É≥„Éó„É´„Å™ÊñπÊ≥ï„Åß„ÄÅÂêÑÊñáÊõ∏„ÇíÂçòË™û„ÅÆÂá∫ÁèæÂõûÊï∞„ÅßË°®Áèæ„Åó„Åæ„Åô„ÄÇ</p>
<h4>‰ªïÁµÑ„Åø</h4>
<pre><code>ÊñáÊõ∏1: &quot;Áå´„ÅåÂ•Ω„Åç&quot;
ÊñáÊõ∏2: &quot;Áä¨„ÅåÂ•Ω„Åç&quot;
ÊñáÊõ∏3: &quot;Áå´„ÇÇÁä¨„ÇÇÂ•Ω„Åç&quot;

Ë™ûÂΩô: [Áå´, Áä¨, Â•Ω„Åç, „Åå, „ÇÇ]

BoWË°®Áèæ:
ÊñáÊõ∏1: [1, 0, 1, 1, 0]  # Áå´:1Âõû, Áä¨:0Âõû, Â•Ω„Åç:1Âõû, „Åå:1Âõû, „ÇÇ:0Âõû
ÊñáÊõ∏2: [0, 1, 1, 1, 0]
ÊñáÊõ∏3: [1, 1, 1, 0, 2]
</code></pre>
<h4>ÂÆüË£Ö</h4>
<pre><code class="language-python">from sklearn.feature_extraction.text import CountVectorizer

# „Çµ„É≥„Éó„É´„Éá„Éº„Çø
documents = [
    &quot;Ê©üÊ¢∞Â≠¶Áøí„ÅØÈù¢ÁôΩ„ÅÑ&quot;,
    &quot;Ê∑±Â±§Â≠¶Áøí„ÅØÈõ£„Åó„ÅÑ&quot;,
    &quot;Ê©üÊ¢∞Â≠¶Áøí„Å®Ê∑±Â±§Â≠¶Áøí„ÅØÈñ¢ÈÄ£„Åó„Å¶„ÅÑ„Çã&quot;,
    &quot;Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„ÇÇÈù¢ÁôΩ„ÅÑ&quot;
]

# BoWÂ§âÊèõ
vectorizer = CountVectorizer()
bow_matrix = vectorizer.fit_transform(documents)

# Ë™ûÂΩô„ÅÆÁ¢∫Ë™ç
print(&quot;Ë™ûÂΩô:&quot;, vectorizer.get_feature_names_out())
print(&quot;\nBoWË°åÂàó:&quot;)
print(bow_matrix.toarray())
</code></pre>
<p><strong>Âá∫Âäõ:</strong></p>
<pre><code>Ë™ûÂΩô: ['„Å®' 'Ê∑±Â±§' 'Ê©üÊ¢∞' 'Ëá™ÁÑ∂' 'Ë®ÄË™û' 'Âá¶ÁêÜ' 'Èñ¢ÈÄ£' 'Èõ£„Åó„ÅÑ' 'Èù¢ÁôΩ„ÅÑ' 'Â≠¶Áøí']

BoWË°åÂàó:
[[0 0 1 0 0 0 0 0 1 1]  # Ê©üÊ¢∞Â≠¶Áøí„ÅØÈù¢ÁôΩ„ÅÑ
 [0 1 0 0 0 0 0 1 0 1]  # Ê∑±Â±§Â≠¶Áøí„ÅØÈõ£„Åó„ÅÑ
 [1 1 1 0 0 0 1 0 0 2]  # Ê©üÊ¢∞Â≠¶Áøí„Å®Ê∑±Â±§Â≠¶Áøí„ÅØÈñ¢ÈÄ£„Åó„Å¶„ÅÑ„Çã
 [0 0 0 1 1 1 0 0 1 0]] # Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„ÇÇÈù¢ÁôΩ„ÅÑ
</code></pre>
<h4>ÂïèÈ°åÁÇπ</h4>
<ul>
<li><strong>Ë™ûÈ†Ü„ÇíÁÑ°Ë¶ñ</strong>: "Áä¨„ÅåÁå´„ÇíËøΩ„ÅÑ„Åã„Åë„Çã" „Å® "Áå´„ÅåÁä¨„ÇíËøΩ„ÅÑ„Åã„Åë„Çã" „ÅåÂêå„Åò</li>
<li><strong>ÊñáÊõ∏Èï∑„ÅÆÂΩ±Èüø</strong>: Èï∑„ÅÑÊñáÊõ∏„Åª„Å©ÂÄ§„ÅåÂ§ß„Åç„Åè„Å™„Çã</li>
<li><strong>ÈáçË¶ÅÂ∫¶„ÇíËÄÉÊÖÆ„Åó„Å™„ÅÑ</strong>: „Åô„Åπ„Å¶„ÅÆÂçòË™û„ÅåÂêåÁ≠â„Å´Êâ±„Çè„Çå„Çã</li>
</ul>
<h3>3.2 TF-IDF (Term Frequency-Inverse Document Frequency)</h3>
<p><strong>TF-IDF</strong> „ÅØ„ÄÅÂçòË™û„ÅÆÈáçË¶ÅÂ∫¶„ÇíËÄÉÊÖÆ„Åó„ÅüÁâπÂæ¥ÈáèÊäΩÂá∫ÊâãÊ≥ï„Åß„Åô„ÄÇ</p>
<h4>Ë®àÁÆóÂºè</h4>
<p>$$
\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t)
$$</p>
<ul>
<li>
<p><strong>TF (Term Frequency)</strong>: ÊñáÊõ∏ÂÜÖ„Åß„ÅÆÂçòË™û„ÅÆÂá∫ÁèæÈ†ªÂ∫¶
$$
\text{TF}(t, d) = \frac{\text{ÊñáÊõ∏}d\text{„Å´„Åä„Åë„ÇãÂçòË™û}t\text{„ÅÆÂá∫ÁèæÂõûÊï∞}}{\text{ÊñáÊõ∏}d\text{„ÅÆÁ∑èÂçòË™ûÊï∞}}
$$</p>
</li>
<li>
<p><strong>IDF (Inverse Document Frequency)</strong>: ÂçòË™û„ÅÆÂ∏åÂ∞ëÊÄß
$$
\text{IDF}(t) = \log \frac{\text{Á∑èÊñáÊõ∏Êï∞}}{\text{ÂçòË™û}t\text{„ÇíÂê´„ÇÄÊñáÊõ∏Êï∞}}
$$</p>
</li>
</ul>
<h4>Áõ¥ÊÑüÁöÑÁêÜËß£</h4>
<ul>
<li><strong>È†ªÂá∫„Åô„ÇãÂçòË™û</strong> (TF „ÅåÈ´ò„ÅÑ) ‚Üí ÈáçË¶ÅÂ∫¶„ÅåÈ´ò„ÅÑ</li>
<li><strong>Â§ö„Åè„ÅÆÊñáÊõ∏„Å´Âá∫Áèæ„Åô„ÇãÂçòË™û</strong> (IDF „Åå‰Ωé„ÅÑ) ‚Üí „ÅÇ„Åæ„ÇäÈáçË¶Å„Åß„ÅØ„Å™„ÅÑ</li>
<li>‰æã: "Ê©üÊ¢∞Â≠¶Áøí" „ÅØÁâπÂÆö„ÅÆÊñáÊõ∏„Å´È†ªÂá∫ ‚Üí TF-IDF È´ò„ÅÑ</li>
<li>‰æã: "„Åß„Åô", "„Åæ„Åô" „ÅØÂÖ®ÊñáÊõ∏„Å´Âá∫Áèæ ‚Üí TF-IDF ‰Ωé„ÅÑ</li>
</ul>
<h4>ÂÆüË£Ö</h4>
<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer

# „Çµ„É≥„Éó„É´„Éá„Éº„Çø
documents = [
    &quot;Ê©üÊ¢∞Â≠¶Áøí„ÅØÊïôÂ∏´„ÅÇ„ÇäÂ≠¶Áøí„Å®ÊïôÂ∏´„Å™„ÅóÂ≠¶Áøí„Å´ÂàÜÈ°û„Åï„Çå„Çã&quot;,
    &quot;Ê∑±Â±§Â≠¶Áøí„ÅØ„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÇíÁî®„ÅÑ„ÅüÊ©üÊ¢∞Â≠¶Áøí„ÅÆÊâãÊ≥ï&quot;,
    &quot;Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„ÅØ‰∫∫Èñì„ÅÆË®ÄË™û„Çí„Ç≥„É≥„Éî„É•„Éº„Çø„ÅßÂá¶ÁêÜ„Åô„ÇãÊäÄË°ì&quot;,
    &quot;ÁîªÂÉèË™çË≠ò„Å´„ÅØÁï≥„ÅøËæº„Åø„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Åå‰Ωø„Çè„Çå„Çã&quot;
]

# TF-IDFÂ§âÊèõ
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(documents)

# Ë™ûÂΩô„Å® TF-IDF ÂÄ§
feature_names = tfidf_vectorizer.get_feature_names_out()
print(&quot;Ë™ûÂΩô:&quot;, feature_names)
print(&quot;\nTF-IDFË°åÂàó„ÅÆÂΩ¢Áä∂:&quot;, tfidf_matrix.shape)

# ÊúÄÂàù„ÅÆÊñáÊõ∏„ÅÆ TF-IDF ÂÄ§„ÇíË°®Á§∫
doc_index = 0
feature_index = tfidf_matrix[doc_index].nonzero()[1]
tfidf_scores = zip(
    [feature_names[i] for i in feature_index],
    [tfidf_matrix[doc_index, i] for i in feature_index]
)

print(f&quot;\nÊñáÊõ∏{doc_index+1}„ÅÆTF-IDF„Çπ„Ç≥„Ç¢:&quot;)
for word, score in sorted(tfidf_scores, key=lambda x: x[1], reverse=True):
    print(f&quot;  {word}: {score:.4f}&quot;)
</code></pre>
<p><strong>Âá∫Âäõ‰æã:</strong></p>
<pre><code>ÊñáÊõ∏1„ÅÆTF-IDF„Çπ„Ç≥„Ç¢:
  ÊïôÂ∏´: 0.5774  # ‰ªñ„ÅÆÊñáÊõ∏„Å´„ÅØ„Å™„ÅÑ ‚Üí IDFÈ´ò„ÅÑ
  Â≠¶Áøí: 0.4082  # Ë§áÊï∞ÊñáÊõ∏„Å´Âá∫Áèæ ‚Üí IDF‰∏≠Á®ãÂ∫¶
  ÂàÜÈ°û: 0.4082
  Ê©üÊ¢∞: 0.2887  # 2ÊñáÊõ∏„Å´Âá∫Áèæ ‚Üí IDF‰Ωé„ÇÅ
</code></pre>
<h3>3.3 N-gram</h3>
<p><strong>N-gram</strong> „ÅØÈÄ£Á∂ö„Åô„ÇãNÂÄã„ÅÆÂçòË™û„Çí1„Å§„ÅÆÁâπÂæ¥„Å®„Åó„Å¶Êâ±„ÅÑ„Åæ„Åô„ÄÇ</p>
<ul>
<li><strong>Unigram (1-gram)</strong>: "Ê©üÊ¢∞", "Â≠¶Áøí"</li>
<li><strong>Bigram (2-gram)</strong>: "Ê©üÊ¢∞Â≠¶Áøí"</li>
<li><strong>Trigram (3-gram)</strong>: "Ê©üÊ¢∞Â≠¶Áøí„ÅÆÊâãÊ≥ï"</li>
</ul>
<h4>ÂÆüË£Ö</h4>
<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer

documents = [
    &quot;Ê©üÊ¢∞ Â≠¶Áøí „ÅØ Èù¢ÁôΩ„ÅÑ&quot;,
    &quot;Ê∑±Â±§ Â≠¶Áøí „ÇÇ Èù¢ÁôΩ„ÅÑ&quot;,
    &quot;Ê©üÊ¢∞ Â≠¶Áøí „Å® Ê∑±Â±§ Â≠¶Áøí&quot;
]

# Bigram + Unigram „ÅÆ TF-IDF
vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # 1-gram „Å® 2-gram
tfidf = vectorizer.fit_transform(documents)

print(&quot;ÁâπÂæ¥Ë™ûÔºàUnigram + BigramÔºâ:&quot;)
print(vectorizer.get_feature_names_out())
</code></pre>
<p><strong>Âá∫Âäõ:</strong></p>
<pre><code>['„Å®' '„ÅØ' '„ÇÇ' 'Â≠¶Áøí' 'Â≠¶Áøí „Å®' 'Â≠¶Áøí „ÅØ' 'Â≠¶Áøí „ÇÇ' 'Ê©üÊ¢∞' 'Ê©üÊ¢∞ Â≠¶Áøí'
 'Ê∑±Â±§' 'Ê∑±Â±§ Â≠¶Áøí' 'Èù¢ÁôΩ„ÅÑ']
</code></pre>
<hr />
<h2>4. Ê©üÊ¢∞Â≠¶Áøí„Å´„Çà„ÇãÂàÜÈ°û</h2>
<h3>4.1 Naive Bayes („Éä„Ç§„Éº„Éñ„Éô„Ç§„Ç∫)</h3>
<p>Naive Bayes „ÅØ„ÉÜ„Ç≠„Çπ„ÉàÂàÜÈ°û„ÅßÊúÄ„ÇÇ„Çà„Åè‰Ωø„Çè„Çå„Çã„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅÆ1„Å§„Åß„Åô„ÄÇ</p>
<h4>ÁêÜË´ñ</h4>
<p>„Éô„Ç§„Ç∫„ÅÆÂÆöÁêÜ„Å´Âü∫„Å•„Åç„ÄÅÂêÑ„ÇØ„É©„Çπ„ÅÆÁ¢∫Áéá„ÇíË®àÁÆóÔºö</p>
<p>$$
P(C|D) = \frac{P(D|C) \times P(C)}{P(D)}
$$</p>
<ul>
<li>$P(C|D)$: ÊñáÊõ∏ $D$ „Åå‰∏é„Åà„Çâ„Çå„Åü„Å®„Åç„ÅÆ„ÇØ„É©„Çπ $C$ „ÅÆÁ¢∫Áéá</li>
<li>$P(D|C)$: „ÇØ„É©„Çπ $C$ „Å´„Åä„Åë„ÇãÊñáÊõ∏ $D$ „ÅÆÂ∞§Â∫¶</li>
<li>$P(C)$: „ÇØ„É©„Çπ $C$ „ÅÆ‰∫ãÂâçÁ¢∫Áéá</li>
</ul>
<h4>ÂÆüË£Ö: „Çπ„Éë„É†„Éï„Ç£„É´„Çø</h4>
<pre><code class="language-python">from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd

# „Çµ„É≥„Éó„É´„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºàÂÆüÈöõ„ÅØÂ§ßÈáè„ÅÆ„É°„Éº„É´„Éá„Éº„Çø„Çí‰ΩøÁî®Ôºâ
emails = [
    &quot;ÁÑ°Êñô„ÅßiPhone„ÅåÂΩì„Åü„Çä„Åæ„ÅôÔºÅ‰ªä„Åô„Åê„ÇØ„É™„ÉÉ„ÇØÔºÅ&quot;,
    &quot;‰ºöË≠∞„ÅÆË≥áÊñô„ÇíÈÄÅ‰ªò„Åó„Åæ„Åô&quot;,
    &quot;ÊøÄÂÆâ„Çª„Éº„É´ÔºÅÊú¨Êó•ÈôêÂÆö50%„Ç™„Éï&quot;,
    &quot;„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆÈÄ≤ÊçóÂ†±Âëä&quot;,
    &quot;„ÅÇ„Å™„Åü„Å´100‰∏áÂÜÜ„ÅåÂΩì„Åü„Çä„Åæ„Åó„Åü&quot;,
    &quot;Êù•ÈÄ±„ÅÆ‰∫àÂÆö„Å´„Å§„ÅÑ„Å¶Á¢∫Ë™ç„Åï„Åõ„Å¶„Åè„Å†„Åï„ÅÑ&quot;,
    &quot;ÈôêÂÆö„Ç™„Éï„Ç°„ÉºÔºÅ‰ªä„Åô„ÅêÁôªÈå≤&quot;,
    &quot;Ë≠∞‰∫ãÈå≤„ÇíÂÖ±Êúâ„Åó„Åæ„Åô&quot;
]

labels = [1, 0, 1, 0, 1, 0, 1, 0]  # 1: „Çπ„Éë„É†, 0: Ê≠£Â∏∏

# ÂâçÂá¶ÁêÜÔºàÂÆüÈöõ„ÅØÊó•Êú¨Ë™ûÂΩ¢ÊÖãÁ¥†Ëß£Êûê„Çí‰ΩøÁî®Ôºâ
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features=100)
X = vectorizer.fit_transform(emails)
y = labels

# Ë®ìÁ∑¥„Éá„Éº„Çø„Å®„ÉÜ„Çπ„Éà„Éá„Éº„Çø„Å´ÂàÜÂâ≤
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42
)

# Naive Bayes „É¢„Éá„É´„ÅÆË®ìÁ∑¥
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)

# ‰∫àÊ∏¨
y_pred = nb_model.predict(X_test)

# Ë©ï‰æ°
print(&quot;ÂàÜÈ°û„É¨„Éù„Éº„Éà:&quot;)
print(classification_report(y_test, y_pred,
                          target_names=['Ê≠£Â∏∏„É°„Éº„É´', '„Çπ„Éë„É†']))

print(&quot;\nÊ∑∑ÂêåË°åÂàó:&quot;)
print(confusion_matrix(y_test, y_pred))

# Êñ∞„Åó„ÅÑ„É°„Éº„É´„ÅÆ‰∫àÊ∏¨
new_emails = [
    &quot;‰ºöË≠∞ÂÆ§„ÅÆ‰∫àÁ¥Ñ„Çí„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô&quot;,
    &quot;‰ªä„Åô„Åê„ÇØ„É™„ÉÉ„ÇØÔºÅÂ§ßÈáë„ÅåÊâã„Å´ÂÖ•„Çä„Åæ„Åô&quot;
]
new_emails_vec = vectorizer.transform(new_emails)
predictions = nb_model.predict(new_emails_vec)

for email, pred in zip(new_emails, predictions):
    label = &quot;„Çπ„Éë„É†&quot; if pred == 1 else &quot;Ê≠£Â∏∏&quot;
    print(f&quot;'{email}' ‚Üí {label}&quot;)
</code></pre>
<h3>4.2 Support Vector Machine (SVM)</h3>
<p>SVM „ÅØÈ´òÊ¨°ÂÖÉ„Éá„Éº„Çø„Åß„ÇÇÂäπÊûúÁöÑ„Å™ÂàÜÈ°û„Ç¢„É´„Ç¥„É™„Ç∫„É†„Åß„Åô„ÄÇ</p>
<h4>ÂÆüË£Ö: ÊÑüÊÉÖÂàÜÊûê</h4>
<pre><code class="language-python">from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer

# „Çµ„É≥„Éó„É´„É¨„Éì„É•„Éº„Éá„Éº„Çø
reviews = [
    &quot;„Åì„ÅÆÊò†Áîª„ÅØÊúÄÈ´ò„Åß„Åó„ÅüÔºÅÊÑüÂãï„Åó„Åæ„Åó„Åü&quot;,
    &quot;„Å§„Åæ„Çâ„Å™„ÅÑÊò†Áîª„ÄÇÊôÇÈñì„ÅÆÁÑ°ÈßÑ&quot;,
    &quot;Á¥†Êô¥„Çâ„Åó„ÅÑÊºîÊäÄ„Å®Áæé„Åó„ÅÑÊò†ÂÉè&quot;,
    &quot;ÈÄÄÂ±à„ÅßÁú†„Åè„Å™„Å£„Åü&quot;,
    &quot;ÊúüÂæÖ‰ª•‰∏ä„ÅÆ‰ΩúÂìÅÔºÅ„Åæ„ÅüË¶≥„Åü„ÅÑ&quot;,
    &quot;„Çπ„Éà„Éº„É™„Éº„ÅåËñÑ„Åè„ÄÅ„Åå„Å£„Åã„Çä&quot;,
    &quot;ÂúßÂ∑ª„ÅÆ„É©„Çπ„Éà„Ç∑„Éº„É≥&quot;,
    &quot;‰∫åÂ∫¶„Å®Ë¶≥„Åü„Åè„Å™„ÅÑ&quot;
]

sentiments = [1, 0, 1, 0, 1, 0, 1, 0]  # 1: Positive, 0: Negative

# „Éë„Ç§„Éó„É©„Ç§„É≥ÊßãÁØâÔºàÂâçÂá¶ÁêÜ ‚Üí ÁâπÂæ¥ÊäΩÂá∫ ‚Üí „É¢„Éá„É´Ôºâ
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=100)),
    ('svm', LinearSVC(random_state=42))
])

# Ë®ìÁ∑¥
X_train, X_test, y_train, y_test = train_test_split(
    reviews, sentiments, test_size=0.25, random_state=42
)

pipeline.fit(X_train, y_train)

# Ë©ï‰æ°
y_pred = pipeline.predict(X_test)
print(&quot;Ê≠£Ëß£Áéá:&quot;, pipeline.score(X_test, y_test))

# Êñ∞„Åó„ÅÑ„É¨„Éì„É•„Éº„ÅÆÊÑüÊÉÖ‰∫àÊ∏¨
new_reviews = [
    &quot;ÊúüÂæÖÂ§ñ„Çå„ÅÆ‰ΩúÂìÅ&quot;,
    &quot;ÊÑüÂãïÁöÑ„Å™„Çπ„Éà„Éº„É™„Éº&quot;
]

for review in new_reviews:
    sentiment = pipeline.predict([review])[0]
    label = &quot;Positive&quot; if sentiment == 1 else &quot;Negative&quot;
    print(f&quot;'{review}' ‚Üí {label}&quot;)
</code></pre>
<h3>4.3 ÂÆåÂÖ®„Å™ÂàÜÈ°û„Éë„Ç§„Éó„É©„Ç§„É≥</h3>
<p>ÂÆüÈöõ„ÅÆ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„Åß‰Ωø„Åà„ÇãÂÆåÂÖ®„Å™„Éë„Ç§„Éó„É©„Ç§„É≥‰æãÔºö</p>
<pre><code class="language-python">import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
import joblib

class TextClassifier:
    &quot;&quot;&quot;„ÉÜ„Ç≠„Çπ„ÉàÂàÜÈ°ûÂô®„ÇØ„É©„Çπ&quot;&quot;&quot;

    def __init__(self, model_type='nb'):
        &quot;&quot;&quot;
        Parameters:
        -----------
        model_type : str
            'nb': Naive Bayes
            'svm': Support Vector Machine
            'lr': Logistic Regression
            'rf': Random Forest
        &quot;&quot;&quot;
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.8
        )

        # „É¢„Éá„É´ÈÅ∏Êäû
        if model_type == 'nb':
            self.model = MultinomialNB()
        elif model_type == 'svm':
            self.model = LinearSVC(random_state=42)
        elif model_type == 'lr':
            self.model = LogisticRegression(random_state=42, max_iter=1000)
        elif model_type == 'rf':
            self.model = RandomForestClassifier(random_state=42, n_estimators=100)
        else:
            raise ValueError(f&quot;Unknown model type: {model_type}&quot;)

    def train(self, X_train, y_train):
        &quot;&quot;&quot;„É¢„Éá„É´„ÅÆË®ìÁ∑¥&quot;&quot;&quot;
        # TF-IDFÂ§âÊèõ
        X_train_vec = self.vectorizer.fit_transform(X_train)

        # „É¢„Éá„É´Â≠¶Áøí
        self.model.fit(X_train_vec, y_train)

    def predict(self, X_test):
        &quot;&quot;&quot;‰∫àÊ∏¨&quot;&quot;&quot;
        X_test_vec = self.vectorizer.transform(X_test)
        return self.model.predict(X_test_vec)

    def evaluate(self, X_test, y_test):
        &quot;&quot;&quot;Ë©ï‰æ°&quot;&quot;&quot;
        y_pred = self.predict(X_test)

        print(&quot;Ê≠£Ëß£Áéá:&quot;, accuracy_score(y_test, y_pred))
        print(&quot;\nÂàÜÈ°û„É¨„Éù„Éº„Éà:&quot;)
        print(classification_report(y_test, y_pred))

        return y_pred

    def save(self, filepath):
        &quot;&quot;&quot;„É¢„Éá„É´„ÅÆ‰øùÂ≠ò&quot;&quot;&quot;
        joblib.dump((self.vectorizer, self.model), filepath)

    def load(self, filepath):
        &quot;&quot;&quot;„É¢„Éá„É´„ÅÆË™≠„ÅøËæº„Åø&quot;&quot;&quot;
        self.vectorizer, self.model = joblib.load(filepath)

# ‰ΩøÁî®‰æã
if __name__ == &quot;__main__&quot;:
    # „Éá„Éº„ÇøÊ∫ñÂÇôÔºàÂÆüÈöõ„ÅØCSV„Éï„Ç°„Ç§„É´„Åã„ÇâË™≠„ÅøËæº„ÅøÔºâ
    texts = [
        &quot;Á¥†Êô¥„Çâ„Åó„ÅÑÂïÜÂìÅ„Åß„Åô&quot;,
        &quot;ÊúÄÊÇ™„ÅÆÂìÅË≥™&quot;,
        &quot;ÊúüÂæÖÈÄö„Çä„ÅÆÊÄßËÉΩ&quot;,
        &quot;‰∫åÂ∫¶„Å®Ë≤∑„ÅÑ„Åæ„Åõ„Çì&quot;,
        # ... Â§ßÈáè„ÅÆ„Éá„Éº„Çø
    ]
    labels = [1, 0, 1, 0]  # 1: Positive, 0: Negative

    # Ë®ìÁ∑¥„Éª„ÉÜ„Çπ„ÉàÂàÜÂâ≤
    X_train, X_test, y_train, y_test = train_test_split(
        texts, labels, test_size=0.2, random_state=42, stratify=labels
    )

    # ÂàÜÈ°ûÂô®„ÅÆË®ìÁ∑¥
    classifier = TextClassifier(model_type='svm')
    classifier.train(X_train, y_train)

    # Ë©ï‰æ°
    y_pred = classifier.evaluate(X_test, y_test)

    # „É¢„Éá„É´„ÅÆ‰øùÂ≠ò
    classifier.save('text_classifier.pkl')

    # Êñ∞„Åó„ÅÑ„ÉÜ„Ç≠„Çπ„Éà„ÅÆ‰∫àÊ∏¨
    new_texts = [&quot;„Åì„ÅÆÂïÜÂìÅ„ÅØÊúÄÈ´ò„Åß„Åô&quot;, &quot;„Åå„Å£„Åã„Çä„Åó„Åæ„Åó„Åü&quot;]
    predictions = classifier.predict(new_texts)
    print(&quot;\nÊñ∞„Åó„ÅÑ„ÉÜ„Ç≠„Çπ„Éà„ÅÆ‰∫àÊ∏¨:&quot;)
    for text, pred in zip(new_texts, predictions):
        print(f&quot;'{text}' ‚Üí {'Positive' if pred == 1 else 'Negative'}&quot;)
</code></pre>
<hr />
<h2>5. „É¢„Éá„É´„ÅÆË©ï‰æ°</h2>
<h3>5.1 Ë©ï‰æ°ÊåáÊ®ô</h3>
<h4>Ê≠£Ëß£Áéá (Accuracy)</h4>
<p>$$
\text{Accuracy} = \frac{\text{Ê≠£„Åó„ÅèÂàÜÈ°û„Åï„Çå„Åü„Çµ„É≥„Éó„É´Êï∞}}{\text{ÂÖ®„Çµ„É≥„Éó„É´Êï∞}}
$$</p>
<ul>
<li><strong>‰Ωø„ÅÑ„Å©„Åì„Çç</strong>: „ÇØ„É©„Çπ„ÅÆ„Éê„É©„É≥„Çπ„ÅåÂèñ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà</li>
<li><strong>Ê≥®ÊÑèÁÇπ</strong>: ‰∏çÂùáË°°„Éá„Éº„Çø„Åß„ÅØ‰∏çÈÅ©Âàá</li>
</ul>
<h4>ÈÅ©ÂêàÁéá (Precision)</h4>
<p>$$
\text{Precision} = \frac{TP}{TP + FP}
$$</p>
<ul>
<li><strong>ÊÑèÂë≥</strong>: „ÄåPositive „Å®‰∫àÊ∏¨„Åó„Åü„ÇÇ„ÅÆ„ÅÆ„ÅÜ„Å°„ÄÅÂÆüÈöõ„Å´ Positive „Å†„Å£„ÅüÂâ≤Âêà„Äç</li>
<li><strong>‰Ωø„ÅÑ„Å©„Åì„Çç</strong>: ÂÅΩÈôΩÊÄß„ÇíÈÅø„Åë„Åü„ÅÑÂ†¥ÂêàÔºà„Çπ„Éë„É†„Éï„Ç£„É´„Çø„Å™„Å©Ôºâ</li>
</ul>
<h4>ÂÜçÁèæÁéá (Recall)</h4>
<p>$$
\text{Recall} = \frac{TP}{TP + FN}
$$</p>
<ul>
<li><strong>ÊÑèÂë≥</strong>: „ÄåÂÆüÈöõ„Å´ Positive „ÅÆ„ÇÇ„ÅÆ„ÅÆ„ÅÜ„Å°„ÄÅÊ≠£„Åó„Åè Positive „Å®‰∫àÊ∏¨„Åß„Åç„ÅüÂâ≤Âêà„Äç</li>
<li><strong>‰Ωø„ÅÑ„Å©„Åì„Çç</strong>: ÂÅΩÈô∞ÊÄß„ÇíÈÅø„Åë„Åü„ÅÑÂ†¥ÂêàÔºàÁóÖÊ∞óË®∫Êñ≠„Å™„Å©Ôºâ</li>
</ul>
<h4>F1„Çπ„Ç≥„Ç¢</h4>
<p>$$
F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$</p>
<ul>
<li><strong>ÊÑèÂë≥</strong>: Precision „Å® Recall „ÅÆË™øÂíåÂπ≥Âùá</li>
<li><strong>‰Ωø„ÅÑ„Å©„Åì„Çç</strong>: „Éê„É©„É≥„Çπ„ÅÆÂèñ„Çå„ÅüË©ï‰æ°„ÅåÂøÖË¶Å„Å™Â†¥Âêà</li>
</ul>
<h3>5.2 Ê∑∑ÂêåË°åÂàó (Confusion Matrix)</h3>
<pre><code class="language-python">from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# ‰∫àÊ∏¨ÁµêÊûúÔºà‰æãÔºâ
y_true = [0, 1, 0, 1, 0, 1, 1, 0]
y_pred = [0, 1, 0, 0, 0, 1, 1, 1]

# Ê∑∑ÂêåË°åÂàó
cm = confusion_matrix(y_true, y_pred)

# ÂèØË¶ñÂåñ
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'])
plt.ylabel('ÂÆüÈöõ„ÅÆ„É©„Éô„É´')
plt.xlabel('‰∫àÊ∏¨„É©„Éô„É´')
plt.title('Ê∑∑ÂêåË°åÂàó')
plt.show()

print(&quot;Ê∑∑ÂêåË°åÂàó:&quot;)
print(cm)
print(f&quot;\nTrue Negative: {cm[0,0]}&quot;)
print(f&quot;False Positive: {cm[0,1]}&quot;)
print(f&quot;False Negative: {cm[1,0]}&quot;)
print(f&quot;True Positive: {cm[1,1]}&quot;)
</code></pre>
<h3>5.3 ‰∫§Â∑ÆÊ§úË®º (Cross-Validation)</h3>
<p>„Éá„Éº„Çø„ÇíË§áÊï∞„ÅÆ fold „Å´ÂàÜ„Åë„ÄÅ„Çà„Çä‰ø°È†ºÊÄß„ÅÆÈ´ò„ÅÑË©ï‰æ°„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ</p>
<pre><code class="language-python">from sklearn.model_selection import cross_val_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline

# „Éë„Ç§„Éó„É©„Ç§„É≥
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=1000)),
    ('nb', MultinomialNB())
])

# 5-fold ‰∫§Â∑ÆÊ§úË®º
scores = cross_val_score(
    pipeline,
    texts,  # ÂÖ®„ÉÜ„Ç≠„Çπ„Éà„Éá„Éº„Çø
    labels,  # ÂÖ®„É©„Éô„É´
    cv=5,  # 5ÂàÜÂâ≤
    scoring='f1'
)

print(f&quot;ÂêÑfold„ÅÆF1„Çπ„Ç≥„Ç¢: {scores}&quot;)
print(f&quot;Âπ≥ÂùáF1„Çπ„Ç≥„Ç¢: {scores.mean():.4f} (+/- {scores.std():.4f})&quot;)
</code></pre>
<hr />
<h2>6. „Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÉÅ„É•„Éº„Éã„É≥„Ç∞</h2>
<h3>6.1 Grid Search</h3>
<pre><code class="language-python">from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC

# „Éë„Ç§„Éó„É©„Ç§„É≥
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('svm', LinearSVC())
])

# „ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åô„Çã„Éë„É©„É°„Éº„Çø
param_grid = {
    'tfidf__max_features': [1000, 3000, 5000],
    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'tfidf__min_df': [1, 2, 5],
    'svm__C': [0.1, 1, 10]
}

# Grid Search
grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=5,
    scoring='f1',
    n_jobs=-1,  # ‰∏¶ÂàóÂá¶ÁêÜ
    verbose=1
)

grid_search.fit(X_train, y_train)

# ÊúÄËâØ„ÅÆ„Éë„É©„É°„Éº„Çø
print(&quot;ÊúÄËâØ„ÅÆ„Éë„É©„É°„Éº„Çø:&quot;)
print(grid_search.best_params_)
print(f&quot;\nÊúÄËâØ„ÅÆF1„Çπ„Ç≥„Ç¢: {grid_search.best_score_:.4f}&quot;)

# ÊúÄËâØ„É¢„Éá„É´„Åß‰∫àÊ∏¨
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
</code></pre>
<hr />
<h2>7. ÂÆüË∑µ‰æã: Êó•Êú¨Ë™ûÊÑüÊÉÖÂàÜÊûê„Ç∑„Çπ„ÉÜ„É†</h2>
<h3>7.1 ÂÆåÂÖ®„Å™ÂÆüË£Ö</h3>
<pre><code class="language-python">import pandas as pd
import MeCab
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report
import pickle

class JapaneseSentimentAnalyzer:
    &quot;&quot;&quot;Êó•Êú¨Ë™ûÊÑüÊÉÖÂàÜÊûê„Ç∑„Çπ„ÉÜ„É†&quot;&quot;&quot;

    def __init__(self):
        self.mecab = MeCab.Tagger()
        self.vectorizer = None
        self.model = None

    def tokenize(self, text):
        &quot;&quot;&quot;MeCab„Åß„Éà„Éº„ÇØ„É≥Âåñ&quot;&quot;&quot;
        node = self.mecab.parseToNode(text)
        tokens = []

        while node:
            if node.surface:
                features = node.feature.split(',')
                pos = features[0]

                # ÂêçË©û„ÄÅÂãïË©û„ÄÅÂΩ¢ÂÆπË©û„ÅÆ„ÅøÊäΩÂá∫
                if pos in ['ÂêçË©û', 'ÂãïË©û', 'ÂΩ¢ÂÆπË©û']:
                    base_form = features[6] if len(features) &gt; 6 else node.surface
                    if base_form != '*':
                        tokens.append(base_form)

            node = node.next

        return ' '.join(tokens)

    def prepare_data(self, texts):
        &quot;&quot;&quot;„Éá„Éº„Çø„ÅÆÂâçÂá¶ÁêÜ&quot;&quot;&quot;
        return [self.tokenize(text) for text in texts]

    def train(self, texts, labels):
        &quot;&quot;&quot;„É¢„Éá„É´„ÅÆË®ìÁ∑¥&quot;&quot;&quot;
        # „Éà„Éº„ÇØ„É≥Âåñ
        tokenized_texts = self.prepare_data(texts)

        # TF-IDF „Éô„ÇØ„Éà„É´Âåñ
        self.vectorizer = TfidfVectorizer(
            max_features=3000,
            ngram_range=(1, 2),
            min_df=2
        )
        X = self.vectorizer.fit_transform(tokenized_texts)

        # SVM Ë®ìÁ∑¥
        self.model = LinearSVC(C=1.0, random_state=42)
        self.model.fit(X, labels)

    def predict(self, texts):
        &quot;&quot;&quot;ÊÑüÊÉÖ‰∫àÊ∏¨&quot;&quot;&quot;
        tokenized_texts = self.prepare_data(texts)
        X = self.vectorizer.transform(tokenized_texts)
        return self.model.predict(X)

    def predict_proba(self, texts):
        &quot;&quot;&quot;Á¢∫Áéá‰ªò„Åç‰∫àÊ∏¨ÔºàSVM„ÅÆÊ±∫ÂÆöÈñ¢Êï∞„Çí‰ΩøÁî®Ôºâ&quot;&quot;&quot;
        tokenized_texts = self.prepare_data(texts)
        X = self.vectorizer.transform(tokenized_texts)
        decision = self.model.decision_function(X)
        return decision

    def save(self, filepath):
        &quot;&quot;&quot;„É¢„Éá„É´„ÅÆ‰øùÂ≠ò&quot;&quot;&quot;
        with open(filepath, 'wb') as f:
            pickle.dump((self.vectorizer, self.model), f)

    def load(self, filepath):
        &quot;&quot;&quot;„É¢„Éá„É´„ÅÆË™≠„ÅøËæº„Åø&quot;&quot;&quot;
        with open(filepath, 'rb') as f:
            self.vectorizer, self.model = pickle.load(f)

# „Éá„Éº„ÇøÊ∫ñÂÇô
if __name__ == &quot;__main__&quot;:
    # „Çµ„É≥„Éó„É´„É¨„Éì„É•„Éº„Éá„Éº„ÇøÔºàÂÆüÈöõ„ÅØÂ§ßË¶èÊ®°„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí‰ΩøÁî®Ôºâ
    reviews = [
        &quot;„Åì„ÅÆÂïÜÂìÅ„ÅØÊú¨ÂΩì„Å´Á¥†Êô¥„Çâ„Åó„ÅÑ„ÄÇÊúüÂæÖ‰ª•‰∏ä„Åß„Åó„Åü&quot;,
        &quot;ÊúÄÊÇ™„ÅÆÂìÅË≥™„ÄÇ‰∫åÂ∫¶„Å®Ë≤∑„ÅÑ„Åæ„Åõ„Çì&quot;,
        &quot;ÂÄ§ÊÆµ„ÅÆÂâ≤„Å´„ÅØËâØ„ÅÑÂïÜÂìÅ„Å†„Å®ÊÄù„ÅÑ„Åæ„Åô&quot;,
        &quot;ÈÖçÈÄÅ„ÅåÈÅÖ„Åè„ÄÅÂïÜÂìÅ„ÇÇÂÇ∑„Å†„Çâ„Åë&quot;,
        &quot;‰Ωø„ÅÑ„ÇÑ„Åô„Åè„Å¶Ê∫ÄË∂≥„Åó„Å¶„ÅÑ„Åæ„Åô&quot;,
        &quot;Ë™¨Êòé„Å®ÈÅï„ÅÜÂïÜÂìÅ„ÅåÂ±ä„ÅÑ„Åü&quot;,
        &quot;„Ç≥„Çπ„Éà„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÅåÈ´ò„ÅÑ&quot;,
        &quot;ÊúüÂæÖÂ§ñ„Çå„Åß„Åå„Å£„Åã„Çä&quot;,
        &quot;„Éá„Ç∂„Ç§„É≥„ÅåÁæé„Åó„Åè„ÄÅÊ©üËÉΩ„ÇÇÂÖÖÂÆü&quot;,
        &quot;Â£ä„Çå„ÇÑ„Åô„Åè„ÄÅ„Åô„Åê„Å´ÊïÖÈöú„Åó„Åü&quot;
    ]

    sentiments = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1: Positive, 0: Negative

    # Ë®ìÁ∑¥„Éª„ÉÜ„Çπ„ÉàÂàÜÂâ≤
    X_train, X_test, y_train, y_test = train_test_split(
        reviews, sentiments, test_size=0.2, random_state=42
    )

    # ÂàÜÊûêÂô®„ÅÆË®ìÁ∑¥
    analyzer = JapaneseSentimentAnalyzer()
    analyzer.train(X_train, y_train)

    # Ë©ï‰æ°
    y_pred = analyzer.predict(X_test)
    print(&quot;ÂàÜÈ°û„É¨„Éù„Éº„Éà:&quot;)
    print(classification_report(y_test, y_pred,
                              target_names=['Negative', 'Positive']))

    # Êñ∞„Åó„ÅÑ„É¨„Éì„É•„Éº„ÅÆÊÑüÊÉÖÂàÜÊûê
    new_reviews = [
        &quot;„Å®„Å¶„ÇÇËâØ„ÅÑË≤∑„ÅÑÁâ©„Åå„Åß„Åç„Åæ„Åó„Åü&quot;,
        &quot;„ÅäÈáë„ÇíÁÑ°ÈßÑ„Å´„Åó„Åü&quot;,
        &quot;ÊôÆÈÄö„ÅÆÂïÜÂìÅ„Åß„Åô&quot;
    ]

    predictions = analyzer.predict(new_reviews)
    scores = analyzer.predict_proba(new_reviews)

    print(&quot;\nÊñ∞„Åó„ÅÑ„É¨„Éì„É•„Éº„ÅÆÊÑüÊÉÖÂàÜÊûê:&quot;)
    for review, pred, score in zip(new_reviews, predictions, scores):
        sentiment = &quot;Positive&quot; if pred == 1 else &quot;Negative&quot;
        confidence = abs(score)
        print(f&quot;'{review}'&quot;)
        print(f&quot;  ‚Üí {sentiment} (‰ø°È†ºÂ∫¶: {confidence:.2f})\n&quot;)

    # „É¢„Éá„É´„ÅÆ‰øùÂ≠ò
    analyzer.save('sentiment_model.pkl')
</code></pre>
<hr />
<h2>8. „Åæ„Å®„ÇÅ</h2>
<h3>8.1 Êú¨Á´†„ÅßÂ≠¶„Çì„Å†„Åì„Å®</h3>
<p>‚úÖ <strong>„ÉÜ„Ç≠„Çπ„ÉàÂâçÂá¶ÁêÜ</strong>
- „ÇØ„É™„Éº„Éã„É≥„Ç∞„ÄÅ„Éà„Éº„ÇØ„É≥Âåñ„ÄÅÊ≠£Ë¶èÂåñ„ÅÆÂÆüË£Ö
- Êó•Êú¨Ë™û„Å®Ëã±Ë™û„ÅÆÂâçÂá¶ÁêÜ„ÅÆÈÅï„ÅÑ</p>
<p>‚úÖ <strong>ÁâπÂæ¥ÈáèÊäΩÂá∫</strong>
- Bag-of-Words (BoW) „ÅÆ‰ªïÁµÑ„Åø
- TF-IDF „Å´„Çà„ÇãÈáç„Åø‰ªò„Åë
- N-gram „ÅÆÊ¥ªÁî®</p>
<p>‚úÖ <strong>Ê©üÊ¢∞Â≠¶Áøí„É¢„Éá„É´</strong>
- Naive Bayes „Å´„Çà„ÇãÂàÜÈ°û
- SVM „Å´„Çà„ÇãÈ´òÊ¨°ÂÖÉ„Éá„Éº„ÇøÂàÜÈ°û
- „Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆÊßãÁØâ</p>
<p>‚úÖ <strong>„É¢„Éá„É´Ë©ï‰æ°</strong>
- Ê≠£Ëß£Áéá„ÄÅÈÅ©ÂêàÁéá„ÄÅÂÜçÁèæÁéá„ÄÅF1„Çπ„Ç≥„Ç¢
- Ê∑∑ÂêåË°åÂàó„ÅÆË™≠„ÅøÊñπ
- ‰∫§Â∑ÆÊ§úË®º„Å´„Çà„Çã‰ø°È†ºÊÄßÂêë‰∏ä</p>
<p>‚úÖ <strong>ÂÆüË∑µ„Ç∑„Çπ„ÉÜ„É†</strong>
- Êó•Êú¨Ë™ûÊÑüÊÉÖÂàÜÊûê„Ç∑„Çπ„ÉÜ„É†„ÅÆÂÆåÂÖ®ÂÆüË£Ö</p>
<h3>8.2 Ê¨°„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó</h3>
<p>Êú¨Á´†„ÅßÂü∫Á§éÁöÑ„Å™„ÉÜ„Ç≠„Çπ„ÉàÂàÜÈ°û„ÇíÂ≠¶„Å≥„Åæ„Åó„Åü„ÄÇ„Åï„Çâ„Å´Áô∫Â±ï„Åï„Åõ„Çã„Å´„ÅØÔºö</p>
<ol>
<li><strong>Ê∑±Â±§Â≠¶Áøí„ÅÆÊ¥ªÁî®</strong>: LSTM„ÄÅTransformer „Å™„Å©„ÅÆ„É¢„Éá„É´</li>
<li><strong>Â§ßË¶èÊ®°„Éá„Éº„Çø„Çª„ÉÉ„Éà</strong>: ÂÆüÈöõ„ÅÆ„Éì„Ç∏„Éç„Çπ„Éá„Éº„Çø„Åß„ÅÆÂÆüÈ®ì</li>
<li><strong>Â§ö„ÇØ„É©„ÇπÂàÜÈ°û</strong>: 3„Å§‰ª•‰∏ä„ÅÆ„Ç´„ÉÜ„Ç¥„É™„Å∏„ÅÆÊã°Âºµ</li>
<li><strong>„É™„Ç¢„É´„Çø„Ç§„É†ÂàÜÈ°û</strong>: Web API „Å®„Åó„Å¶„Éá„Éó„É≠„Ç§</li>
</ol>
<hr />
<h2>9. Á∑¥ÁøíÂïèÈ°å</h2>
<h3>ÂïèÈ°å1: ÂâçÂá¶ÁêÜ„Éë„Ç§„Éó„É©„Ç§„É≥ÔºàÂü∫Á§éÔºâ</h3>
<p>Ê¨°„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„Å´ÂØæ„Åó„Å¶„ÄÅÈÅ©Âàá„Å™ÂâçÂá¶ÁêÜ„ÇíÂÆüË£Ö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<pre><code class="language-python">text = &quot;&quot;&quot;
„ÄêÈÄüÂ†±„ÄëÊñ∞Ë£ΩÂìÅÁô∫Â£≤ÔºÅÔºÅÔºÅüéâ
‰ªä„Å™„Çâ50%OFF ‚Üí http://example.com
#„Çª„Éº„É´ #„ÅäÂæó @ÂÖ¨Âºè„Ç¢„Ç´„Ç¶„É≥„Éà
&quot;&quot;&quot;

# „ÅÇ„Å™„Åü„ÅÆÂÆüË£Ö:
def preprocess(text):
    # „Åì„Åì„Å´„Ç≥„Éº„Éâ„ÇíÊõ∏„Åè
    pass

result = preprocess(text)
print(result)
</code></pre>
<details>
<summary>Ëß£Á≠î‰æã</summary>


<pre><code class="language-python">import re
import unicodedata

def preprocess(text):
    # UnicodeÊ≠£Ë¶èÂåñ
    text = unicodedata.normalize('NFKC', text)

    # URLÈô§Âéª
    text = re.sub(r'http\S+', '', text)

    # „É°„É≥„Ç∑„Éß„É≥„Éª„Éè„ÉÉ„Ç∑„É•„Çø„Ç∞Èô§Âéª
    text = re.sub(r'[@#]\S+', '', text)

    # ÁµµÊñáÂ≠óÈô§Âéª
    text = re.sub(r'[^\w\s]', '', text, flags=re.UNICODE)

    # ‰ΩôÂàÜ„Å™Á©∫ÁôΩÈô§Âéª
    text = ' '.join(text.split())

    return text

result = preprocess(text)
print(result)  # Âá∫Âäõ: &quot;ÈÄüÂ†± Êñ∞Ë£ΩÂìÅÁô∫Â£≤ ‰ªä„Å™„Çâ50OFF&quot;
</code></pre>


</details>

<h3>ÂïèÈ°å2: TF-IDF ÂÆüË£ÖÔºà‰∏≠Á¥öÔºâ</h3>
<p>‰ª•‰∏ã„ÅÆÊñáÊõ∏ÈõÜÂêà„Å´ÂØæ„Åó„Å¶„ÄÅTF-IDF „ÇíË®àÁÆó„Åó„ÄÅÂêÑÊñáÊõ∏„ÅßÊúÄ„ÇÇÈáçË¶Å„Å™ÂçòË™û„Éà„ÉÉ„Éó3„ÇíÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<pre><code class="language-python">documents = [
    &quot;Ê©üÊ¢∞Â≠¶Áøí„ÅØ‰∫∫Â∑•Áü•ËÉΩ„ÅÆ‰∏ÄÂàÜÈáé&quot;,
    &quot;Ê∑±Â±§Â≠¶Áøí„ÅØ„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Çí‰Ωø„ÅÜ&quot;,
    &quot;Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„ÅØ„ÉÜ„Ç≠„Çπ„Éà„ÇíÊâ±„ÅÜ‰∫∫Â∑•Áü•ËÉΩÊäÄË°ì&quot;,
    &quot;ÁîªÂÉèË™çË≠ò„ÅØÊ∑±Â±§Â≠¶Áøí„ÅÆÂøúÁî®‰æã&quot;
]

# „ÅÇ„Å™„Åü„ÅÆÂÆüË£Ö
</code></pre>
<details>
<summary>Ëß£Á≠î‰æã</summary>


<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer

documents = [
    &quot;Ê©üÊ¢∞Â≠¶Áøí„ÅØ‰∫∫Â∑•Áü•ËÉΩ„ÅÆ‰∏ÄÂàÜÈáé&quot;,
    &quot;Ê∑±Â±§Â≠¶Áøí„ÅØ„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Çí‰Ωø„ÅÜ&quot;,
    &quot;Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„ÅØ„ÉÜ„Ç≠„Çπ„Éà„ÇíÊâ±„ÅÜ‰∫∫Â∑•Áü•ËÉΩÊäÄË°ì&quot;,
    &quot;ÁîªÂÉèË™çË≠ò„ÅØÊ∑±Â±§Â≠¶Áøí„ÅÆÂøúÁî®‰æã&quot;
]

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)
feature_names = vectorizer.get_feature_names_out()

for doc_idx in range(len(documents)):
    print(f&quot;\nÊñáÊõ∏{doc_idx+1}: {documents[doc_idx]}&quot;)

    # ÂêÑÊñáÊõ∏„ÅÆTF-IDF„Çπ„Ç≥„Ç¢„ÇíÂèñÂæó
    feature_index = tfidf_matrix[doc_idx].nonzero()[1]
    tfidf_scores = [(feature_names[i], tfidf_matrix[doc_idx, i])
                    for i in feature_index]

    # „Çπ„Ç≥„Ç¢ÈôçÈ†Ü„Åß„ÇΩ„Éº„Éà
    tfidf_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)

    print(&quot;  ÈáçË¶ÅÂçòË™û„Éà„ÉÉ„Éó3:&quot;)
    for word, score in tfidf_scores[:3]:
        print(f&quot;    {word}: {score:.4f}&quot;)
</code></pre>


</details>

<h3>ÂïèÈ°å3: ÊÑüÊÉÖÂàÜÈ°û„Ç∑„Çπ„ÉÜ„É†ÔºàÂøúÁî®Ôºâ</h3>
<p>‰ª•‰∏ã„ÅÆ‰ªïÊßò„ÅßÊÑüÊÉÖÂàÜÈ°û„Ç∑„Çπ„ÉÜ„É†„ÇíÂÆüË£Ö„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö</p>
<p><strong>‰ªïÊßò:</strong>
- ÂÖ•Âäõ: Êó•Êú¨Ë™û„É¨„Éì„É•„Éº„ÉÜ„Ç≠„Çπ„Éà
- Âá∫Âäõ: Positive/Negative + ‰ø°È†ºÂ∫¶„Çπ„Ç≥„Ç¢
- „É¢„Éá„É´: Naive Bayes „Åæ„Åü„ÅØ SVM
- Ë©ï‰æ°ÊåáÊ®ô: F1„Çπ„Ç≥„Ç¢„ÇíË°®Á§∫</p>
<details>
<summary>Ëß£Á≠î‰æã</summary>


<pre><code class="language-python">from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, f1_score
import numpy as np

# „Éá„Éº„ÇøÊ∫ñÂÇô
reviews = [
    &quot;Á¥†Êô¥„Çâ„Åó„ÅÑÂïÜÂìÅ„Åß„Åô&quot;, &quot;ÊúÄÊÇ™„ÅÆÂìÅË≥™&quot;, &quot;ÊúüÂæÖÈÄö„Çä&quot;,
    &quot;„Åå„Å£„Åã„Çä&quot;, &quot;Â§ßÊ∫ÄË∂≥&quot;, &quot;‰∫åÂ∫¶„Å®Ë≤∑„Çè„Å™„ÅÑ&quot;,
    &quot;„Åä„Åô„Åô„ÇÅ„Åß„Åô&quot;, &quot;‰æ°ÂÄ§„Åå„Å™„ÅÑ&quot;
]
labels = [1, 0, 1, 0, 1, 0, 1, 0]

# Ë®ìÁ∑¥„Éª„ÉÜ„Çπ„ÉàÂàÜÂâ≤
X_train, X_test, y_train, y_test = train_test_split(
    reviews, labels, test_size=0.25, random_state=42
)

# TF-IDF + Naive Bayes
vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

model = MultinomialNB()
model.fit(X_train_vec, y_train)

# ‰∫àÊ∏¨„Å®Ë©ï‰æ°
y_pred = model.predict(X_test_vec)
f1 = f1_score(y_test, y_pred)

print(f&quot;F1„Çπ„Ç≥„Ç¢: {f1:.4f}&quot;)
print(&quot;\nÂàÜÈ°û„É¨„Éù„Éº„Éà:&quot;)
print(classification_report(y_test, y_pred,
                          target_names=['Negative', 'Positive']))

# Êñ∞„Åó„ÅÑ„É¨„Éì„É•„Éº„ÅÆ‰∫àÊ∏¨
new_review = [&quot;„Åì„ÅÆÂïÜÂìÅ„ÅØÊúÄÈ´ò„Åß„Åô&quot;]
new_vec = vectorizer.transform(new_review)
prediction = model.predict(new_vec)[0]
proba = model.predict_proba(new_vec)[0]

sentiment = &quot;Positive&quot; if prediction == 1 else &quot;Negative&quot;
confidence = proba[prediction]

print(f&quot;\n'{new_review[0]}'&quot;)
print(f&quot;‚Üí {sentiment} (‰ø°È†ºÂ∫¶: {confidence:.2f})&quot;)
</code></pre>


</details>

<hr />
<h2>10. ÂèÇËÄÉÊñáÁåÆ</h2>
<h3>Êõ∏Á±ç</h3>
<ol>
<li>„ÄåÂÖ•ÈñÄ Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„ÄçSteven Bird ‰ªñÔºà„Ç™„É©„Ç§„É™„Éº„Éª„Ç∏„É£„Éë„É≥Ôºâ</li>
<li>„Äåscikit-learn „Å® TensorFlow „Å´„Çà„ÇãÂÆüË∑µÊ©üÊ¢∞Â≠¶Áøí„ÄçAur√©lien G√©ron</li>
<li>„ÄåPython„Åß„ÅØ„Åò„ÇÅ„Çã„ÉÜ„Ç≠„Çπ„Éà„Ç¢„Éä„É™„ÉÜ„Ç£„ÇØ„ÇπÂÖ•ÈñÄ„Äç‰ΩêËó§ ÊïèÁ¥Ä</li>
</ol>
<h3>„Ç™„É≥„É©„Ç§„É≥„É™„ÇΩ„Éº„Çπ</h3>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction">scikit-learn Text Feature Extraction</a></li>
<li><a href="https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews">Kaggle: Sentiment Analysis Tutorial</a></li>
<li><a href="https://qiita.com/Hironsan/items/2466fe0f344115aff177">Êó•Êú¨Ë™ûËá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„É©„Ç§„Éñ„É©„É™„Åæ„Å®„ÇÅ</a></li>
</ul>
<h3>„Éá„Éº„Çø„Çª„ÉÉ„Éà</h3>
<ul>
<li><a href="https://www.rondhuit.com/download.html">livedoor „Éã„É•„Éº„Çπ„Ç≥„Éº„Éë„Çπ</a> - Êó•Êú¨Ë™û„ÉÜ„Ç≠„Çπ„ÉàÂàÜÈ°û</li>
<li><a href="http://www.db.info.gifu-u.ac.jp/data/Data_5d832973308d57446583ed9f">Êó•Êú¨Ë™ûË©ïÂà§ÂàÜÊûê„Éá„Éº„Çø„Çª„ÉÉ„Éà</a> - ÊÑüÊÉÖÂàÜÊûê</li>
<li><a href="https://ai.stanford.edu/~amaas/data/sentiment/">IMDB Movie Reviews</a> - Ëã±Ë™ûÊÑüÊÉÖÂàÜÊûê</li>
</ul>
<hr />
<p><strong>Ê¨°„Å∏</strong>: <a href="chapter-4.html">Chapter 4: ÂÆü‰∏ñÁïå„ÅÆNLPÂøúÁî® ‚Üí</a></p>
<p><strong>Ââç„Å∏</strong>: <a href="chapter-2.html">‚Üê Chapter 2: ÂΩ¢ÊÖãÁ¥†Ëß£Êûê„ÉªÊßãÊñáËß£Êûê</a></p>
<p><strong>ÁõÆÊ¨°„Å∏</strong>: <a href="index.html">‚Üë „Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°</a></p><div class="navigation">
    <a href="chapter-2.html" class="nav-button">‚Üê Á¨¨2Á´†</a>
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
    <a href="chapter-4.html" class="nav-button">Á¨¨4Á´† ‚Üí</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>Áõ£‰øÆ</strong>: Dr. Yusuke HashimotoÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-17</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
