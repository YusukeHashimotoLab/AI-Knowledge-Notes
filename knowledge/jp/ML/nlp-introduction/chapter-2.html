<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>å½¢æ…‹ç´ è§£æãƒ»æ§‹æ–‡è§£æ</h1>
            <p class="subtitle">ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨€èªå­¦çš„ã«åˆ†æã™ã‚‹åŸºæœ¬æŠ€è¡“</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 30-35åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: Trueå€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: Trueå•</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 2: å½¢æ…‹ç´ è§£æãƒ»æ§‹æ–‡è§£æ</h1>
<h2>ã‚¤ãƒ³ãƒˆãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³</h2>
<p>å‰ç« ã§NLPã®å…¨ä½“åƒã‚’å­¦ã³ã¾ã—ãŸã€‚ã“ã®ç« ã§ã¯ã€NLPã®æœ€ã‚‚åŸºæœ¬çš„ãªå‡¦ç†ã§ã‚ã‚‹<strong>å½¢æ…‹ç´ è§£æ</strong>ã¨<strong>æ§‹æ–‡è§£æ</strong>ã«ã¤ã„ã¦ã€å®Ÿéš›ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã„ãªãŒã‚‰å­¦ç¿’ã—ã¾ã™ã€‚</p>
<p>ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æã™ã‚‹ã«ã¯ã€ã¾ãšæ–‡ã‚’å˜èªã«åˆ†å‰²ã—ã€ãã‚Œãã‚Œã®å“è©ã‚„æ–‡æ³•çš„å½¹å‰²ã‚’ç‰¹å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚ŒãŒã“ã®ç« ã®ä¸­å¿ƒãƒ†ãƒ¼ãƒã§ã™ã€‚</p>
<hr />
<h2>1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³</h2>
<h3>1.1 ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã¯</h3>
<p><strong>ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆTokenizationï¼‰</strong>ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’æ„å‘³ã®ã‚ã‚‹å˜ä½ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã«åˆ†å‰²ã™ã‚‹å‡¦ç†ã§ã™ã€‚</p>
<div class="mermaid">
graph LR
    A[ã€Œä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã€] --> B[ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³]
    B --> C[ã€Œä»Šæ—¥ã€ã€Œã¯ã€ã€Œè‰¯ã„ã€ã€Œå¤©æ°—ã€ã€Œã§ã™ã€]
</div>

<h3>1.2 è‹±èªã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³</h3>
<p>è‹±èªã¯æ¯”è¼ƒçš„ç°¡å˜ã§ã™ï¼š</p>
<pre><code class="language-python">text = &quot;I love natural language processing!&quot;

# ã‚¹ãƒšãƒ¼ã‚¹ã§åˆ†å‰²ï¼ˆæœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ï¼‰
tokens_simple = text.split()
print(tokens_simple)
# ['I', 'love', 'natural', 'language', 'processing!']

# NLTKã‚’ä½¿ç”¨ï¼ˆå¥èª­ç‚¹ã‚‚åˆ†é›¢ï¼‰
import nltk
tokens_nltk = nltk.word_tokenize(text)
print(tokens_nltk)
# ['I', 'love', 'natural', 'language', 'processing', '!']
</code></pre>
<p><strong>èª²é¡Œ:</strong>
- å¥èª­ç‚¹ã®å‡¦ç†ï¼ˆ"processing!" â†’ "processing" + "!"ï¼‰
- çŸ­ç¸®å½¢ï¼ˆ"don't" â†’ "do" + "n't"ï¼‰
- ãƒã‚¤ãƒ•ãƒ³ä»˜ãå˜èªï¼ˆ"state-of-the-art"ï¼‰</p>
<h3>1.3 æ—¥æœ¬èªã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³</h3>
<p>æ—¥æœ¬èªã¯å˜èªã®å¢ƒç•ŒãŒä¸æ˜ç¢ºãªãŸã‚ã€ã‚ˆã‚Šè¤‡é›‘ã§ã™ï¼š</p>
<pre><code class="language-python">text = &quot;è‡ªç„¶è¨€èªå‡¦ç†ã‚’å‹‰å¼·ã—ã¦ã„ã¾ã™&quot;

# å˜ç´”ãªåˆ†å‰²ã§ã¯ä¸å¯èƒ½
# ã©ã“ã§åŒºåˆ‡ã‚‹ï¼Ÿ
# âŒ &quot;è‡ªç„¶&quot; &quot;è¨€èª&quot; &quot;å‡¦ç†&quot; &quot;ã‚’&quot; &quot;å‹‰å¼·&quot; &quot;ã—&quot; &quot;ã¦&quot; &quot;ã„&quot; &quot;ã¾ã™&quot;
# âœ“ &quot;è‡ªç„¶è¨€èªå‡¦ç†&quot; &quot;ã‚’&quot; &quot;å‹‰å¼·&quot; &quot;ã—&quot; &quot;ã¦&quot; &quot;ã„&quot; &quot;ã¾ã™&quot;
</code></pre>
<p>ãã®ãŸã‚ã€æ—¥æœ¬èªã§ã¯<strong>å½¢æ…‹ç´ è§£æ</strong>ãŒå¿…é ˆã§ã™ã€‚</p>
<hr />
<h2>2. å½¢æ…‹ç´ è§£æ</h2>
<h3>2.1 å½¢æ…‹ç´ è§£æã¨ã¯</h3>
<p><strong>å½¢æ…‹ç´ è§£æï¼ˆMorphological Analysisï¼‰</strong>ã¯ã€æ–‡ã‚’å½¢æ…‹ç´ ï¼ˆæ„å‘³ã‚’æŒã¤æœ€å°å˜ä½ï¼‰ã«åˆ†å‰²ã—ã€ãã‚Œãã‚Œã®å“è©ã‚„æ´»ç”¨å½¢ã‚’ç‰¹å®šã™ã‚‹å‡¦ç†ã§ã™ã€‚</p>
<div class="mermaid">
graph TD
    A[ã€Œé£Ÿã¹ã¾ã—ãŸã€] --> B[å½¢æ…‹ç´ è§£æ]
    B --> C[ã€Œé£Ÿã¹ã€: å‹•è©ãƒ»æœªç„¶å½¢]
    B --> D[ã€Œã¾ã—ã€: åŠ©å‹•è©]
    B --> E[ã€ŒãŸã€: åŠ©å‹•è©ãƒ»éå»]
</div>

<h3>2.2 æ—¥æœ¬èªå½¢æ…‹ç´ è§£æãƒ„ãƒ¼ãƒ«</h3>
<h4>MeCabï¼ˆãƒ¡ã‚«ãƒ–ï¼‰</h4>
<p><strong>ç‰¹å¾´:</strong>
- é«˜é€Ÿãƒ»é«˜ç²¾åº¦
- C++å®Ÿè£…ï¼ˆPythonãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚ã‚Šï¼‰
- IPAè¾æ›¸ã€NEologdè¾æ›¸ãªã©</p>
<p><strong>ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:</strong></p>
<pre><code class="language-bash"># macOS
brew install mecab mecab-ipadic

# Python ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°
pip install mecab-python3
</code></pre>
<p><strong>åŸºæœ¬çš„ãªä½¿ã„æ–¹:</strong></p>
<pre><code class="language-python">import MeCab

# MeCabã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
mecab = MeCab.Tagger()

# å½¢æ…‹ç´ è§£æ
text = &quot;ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™&quot;
result = mecab.parse(text)
print(result)
</code></pre>
<p><strong>å‡ºåŠ›ä¾‹:</strong></p>
<pre><code>ä»Šæ—¥    åè©,å‰¯è©å¯èƒ½,*,*,*,*,ä»Šæ—¥,ã‚­ãƒ§ã‚¦,ã‚­ãƒ§ãƒ¼
ã¯      åŠ©è©,ä¿‚åŠ©è©,*,*,*,*,ã¯,ãƒ,ãƒ¯
è‰¯ã„    å½¢å®¹è©,è‡ªç«‹,*,*,å½¢å®¹è©ãƒ»ã‚¤æ®µ,åŸºæœ¬å½¢,è‰¯ã„,ãƒ¨ã‚¤,ãƒ¨ã‚¤
å¤©æ°—    åè©,ä¸€èˆ¬,*,*,*,*,å¤©æ°—,ãƒ†ãƒ³ã‚­,ãƒ†ãƒ³ã‚­
ã§ã™    åŠ©å‹•è©,*,*,*,ç‰¹æ®Šãƒ»ãƒ‡ã‚¹,åŸºæœ¬å½¢,ã§ã™,ãƒ‡ã‚¹,ãƒ‡ã‚¹
EOS
</code></pre>
<p><strong>æ§‹é€ åŒ–ã—ãŸå‡ºåŠ›:</strong></p>
<pre><code class="language-python">def parse_mecab(text):
    &quot;&quot;&quot;MeCabã®å‡ºåŠ›ã‚’æ§‹é€ åŒ–&quot;&quot;&quot;
    mecab = MeCab.Tagger()
    node = mecab.parseToNode(text)

    words = []
    while node:
        if node.surface:  # ç©ºæ–‡å­—åˆ—ã‚’é™¤å¤–
            features = node.feature.split(',')
            words.append({
                'surface': node.surface,      # è¡¨å±¤å½¢
                'pos': features[0],           # å“è©
                'pos_detail': features[1],    # å“è©ç´°åˆ†é¡
                'base_form': features[6],     # åŸºæœ¬å½¢
                'reading': features[7] if len(features) &gt; 7 else ''
            })
        node = node.next
    return words

result = parse_mecab(&quot;è‡ªç„¶è¨€èªå‡¦ç†ã‚’å­¦ã‚“ã§ã„ã¾ã™&quot;)
for word in result:
    print(f&quot;{word['surface']:8s} | {word['pos']:6s} | {word['base_form']}&quot;)
</code></pre>
<p><strong>å‡ºåŠ›:</strong></p>
<pre><code>è‡ªç„¶è¨€èªå‡¦ç† | åè©    | è‡ªç„¶è¨€èªå‡¦ç†
ã‚’       | åŠ©è©    | ã‚’
å­¦ã‚“     | å‹•è©    | å­¦ã¶
ã§       | åŠ©è©    | ã§
ã„       | å‹•è©    | ã„ã‚‹
ã¾ã™     | åŠ©å‹•è©  | ã¾ã™
</code></pre>
<h4>Janomeï¼ˆã‚¸ãƒ£ãƒãƒ¡ï¼‰</h4>
<p><strong>ç‰¹å¾´:</strong>
- Pure Pythonå®Ÿè£…
- ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒç°¡å˜ï¼ˆpip ã®ã¿ï¼‰
- MeCabã‚ˆã‚Šé…ã„ãŒä½¿ã„ã‚„ã™ã„</p>
<p><strong>ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:</strong></p>
<pre><code class="language-bash">pip install janome
</code></pre>
<p><strong>åŸºæœ¬çš„ãªä½¿ã„æ–¹:</strong></p>
<pre><code class="language-python">from janome.tokenizer import Tokenizer

tokenizer = Tokenizer()

text = &quot;è‡ªç„¶è¨€èªå‡¦ç†ã¯é¢ç™½ã„&quot;
tokens = tokenizer.tokenize(text)

for token in tokens:
    print(token)
</code></pre>
<p><strong>å‡ºåŠ›:</strong></p>
<pre><code>è‡ªç„¶    åè©,ä¸€èˆ¬,*,*,*,*,è‡ªç„¶,ã‚·ã‚¼ãƒ³,ã‚·ã‚¼ãƒ³
è¨€èª    åè©,ä¸€èˆ¬,*,*,*,*,è¨€èª,ã‚²ãƒ³ã‚´,ã‚²ãƒ³ã‚´
å‡¦ç†    åè©,ã‚µå¤‰æ¥ç¶š,*,*,*,*,å‡¦ç†,ã‚·ãƒ§ãƒª,ã‚·ãƒ§ãƒª
ã¯      åŠ©è©,ä¿‚åŠ©è©,*,*,*,*,ã¯,ãƒ,ãƒ¯
é¢ç™½ã„  å½¢å®¹è©,è‡ªç«‹,*,*,å½¢å®¹è©ãƒ»ã‚¤æ®µ,åŸºæœ¬å½¢,é¢ç™½ã„,ã‚ªãƒ¢ã‚·ãƒ­ã‚¤,ã‚ªãƒ¢ã‚·ãƒ­ã‚¤
</code></pre>
<p><strong>å˜èªã®ã¿ã‚’æŠ½å‡º:</strong></p>
<pre><code class="language-python">def extract_words(text):
    &quot;&quot;&quot;åè©ãƒ»å‹•è©ãƒ»å½¢å®¹è©ã®ã¿æŠ½å‡º&quot;&quot;&quot;
    tokenizer = Tokenizer()
    words = []

    for token in tokenizer.tokenize(text):
        parts = str(token).split('\t')
        surface = parts[0]
        pos = parts[1].split(',')[0]

        # åè©ãƒ»å‹•è©ãƒ»å½¢å®¹è©ã®ã¿
        if pos in ['åè©', 'å‹•è©', 'å½¢å®¹è©']:
            words.append(surface)

    return words

text = &quot;ä»Šæ—¥ã¯æ±äº¬ã§ä¼šè­°ãŒã‚ã‚Šã¾ã™&quot;
words = extract_words(text)
print(words)
# ['ä»Šæ—¥', 'æ±äº¬', 'ä¼šè­°', 'ã‚ã‚Š', 'ã¾ã™']
</code></pre>
<h4>SudachiPyï¼ˆã‚¹ãƒ€ãƒãƒ‘ã‚¤ï¼‰</h4>
<p><strong>ç‰¹å¾´:</strong>
- è¤‡æ•°ç²’åº¦ã§ã®åˆ†å‰²ï¼ˆA/B/Cï¼‰
- æ–°ã—ã„è¾æ›¸ä½“ç³»
- æ­£è¦åŒ–æ©Ÿèƒ½</p>
<p><strong>ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:</strong></p>
<pre><code class="language-bash">pip install sudachipy sudachidict_core
</code></pre>
<p><strong>è¤‡æ•°ç²’åº¦ã§ã®åˆ†å‰²:</strong></p>
<pre><code class="language-python">from sudachipy import tokenizer
from sudachipy import dictionary

# è¾æ›¸ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä½œæˆ
tokenizer_obj = dictionary.Dictionary().create()

text = &quot;æ±äº¬ã‚¹ã‚«ã‚¤ãƒ„ãƒªãƒ¼&quot;

# A: æœ€çŸ­å˜ä½
mode = tokenizer.Tokenizer.SplitMode.A
print([token.surface() for token in tokenizer_obj.tokenize(text, mode)])
# ['æ±äº¬', 'ã‚¹ã‚«ã‚¤', 'ãƒ„ãƒªãƒ¼']

# B: ä¸­å˜ä½
mode = tokenizer.Tokenizer.SplitMode.B
print([token.surface() for token in tokenizer_obj.tokenize(text, mode)])
# ['æ±äº¬', 'ã‚¹ã‚«ã‚¤ãƒ„ãƒªãƒ¼']

# C: æœ€é•·å˜ä½
mode = tokenizer.Tokenizer.SplitMode.C
print([token.surface() for token in tokenizer_obj.tokenize(text, mode)])
# ['æ±äº¬ã‚¹ã‚«ã‚¤ãƒ„ãƒªãƒ¼']
</code></pre>
<h3>2.3 å½¢æ…‹ç´ è§£æã®å¿œç”¨</h3>
<h4>ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–</h4>
<pre><code class="language-python">from janome.tokenizer import Tokenizer

def normalize_text(text):
    &quot;&quot;&quot;ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸºæœ¬å½¢ã«æ­£è¦åŒ–&quot;&quot;&quot;
    tokenizer = Tokenizer()
    words = []

    for token in tokenizer.tokenize(text):
        parts = str(token).split('\t')
        features = parts[1].split(',')
        base_form = features[6]  # åŸºæœ¬å½¢

        if base_form != '*':
            words.append(base_form)
        else:
            words.append(parts[0])

    return ' '.join(words)

text = &quot;èµ°ã£ã¦ã„ã‚‹äººã‚’è¦‹ã¾ã—ãŸ&quot;
normalized = normalize_text(text)
print(normalized)
# 'èµ°ã‚‹ ã¦ ã„ã‚‹ äºº ã‚’ è¦‹ã‚‹ ã¾ã™ ãŸ'
</code></pre>
<h4>ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»</h4>
<pre><code class="language-python">def remove_stopwords(text):
    &quot;&quot;&quot;åŠ©è©ãƒ»åŠ©å‹•è©ã‚’é™¤å»&quot;&quot;&quot;
    tokenizer = Tokenizer()
    words = []

    stopword_pos = ['åŠ©è©', 'åŠ©å‹•è©', 'è¨˜å·']

    for token in tokenizer.tokenize(text):
        parts = str(token).split('\t')
        surface = parts[0]
        pos = parts[1].split(',')[0]

        if pos not in stopword_pos:
            words.append(surface)

    return words

text = &quot;ä»Šæ—¥ã¯æ±äº¬ã§ä¼šè­°ãŒã‚ã‚Šã¾ã™&quot;
filtered = remove_stopwords(text)
print(filtered)
# ['ä»Šæ—¥', 'æ±äº¬', 'ä¼šè­°', 'ã‚ã‚Š']
</code></pre>
<hr />
<h2>3. å“è©ã‚¿ã‚°ä»˜ã‘</h2>
<h3>3.1 å“è©ã‚¿ã‚°ã¨ã¯</h3>
<p><strong>å“è©ã‚¿ã‚°ä»˜ã‘ï¼ˆPOS Taggingï¼‰</strong>ã¯ã€å„å˜èªã®å“è©ã‚’ç‰¹å®šã™ã‚‹å‡¦ç†ã§ã™ã€‚</p>
<p>æ—¥æœ¬èªã®ä¸»ãªå“è©ï¼š
- åè©ã€å‹•è©ã€å½¢å®¹è©ã€å‰¯è©
- åŠ©è©ã€åŠ©å‹•è©
- æ¥ç¶šè©ã€æ„Ÿå‹•è©
- è¨˜å·</p>
<h3>3.2 spaCy + Ginzaã‚’ä½¿ã£ãŸå“è©ã‚¿ã‚°ä»˜ã‘</h3>
<p>Ginzaã¯æ—¥æœ¬èªã«ç‰¹åŒ–ã—ãŸspaCyãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</p>
<p><strong>ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:</strong></p>
<pre><code class="language-bash">pip install spacy
pip install ginza ja-ginza
</code></pre>
<p><strong>åŸºæœ¬çš„ãªä½¿ã„æ–¹:</strong></p>
<pre><code class="language-python">import spacy

# æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
nlp = spacy.load(&quot;ja_ginza&quot;)

text = &quot;ä»Šæ—¥ã¯æ±äº¬ã§ä¼šè­°ãŒã‚ã‚Šã¾ã™&quot;
doc = nlp(text)

# å“è©ã‚¿ã‚°ä»˜ã‘çµæœ
for token in doc:
    print(f&quot;{token.text:10s} | {token.pos_:10s} | {token.tag_}&quot;)
</code></pre>
<p><strong>å‡ºåŠ›:</strong></p>
<pre><code>ä»Šæ—¥        | NOUN       | åè©-æ™®é€šåè©-å‰¯è©å¯èƒ½
ã¯          | ADP        | åŠ©è©-ä¿‚åŠ©è©
æ±äº¬        | PROPN      | åè©-å›ºæœ‰åè©-åœ°å-ä¸€èˆ¬
ã§          | ADP        | åŠ©è©-æ ¼åŠ©è©
ä¼šè­°        | NOUN       | åè©-æ™®é€šåè©-ã‚µå¤‰å¯èƒ½
ãŒ          | ADP        | åŠ©è©-æ ¼åŠ©è©
ã‚ã‚Š        | VERB       | å‹•è©-éè‡ªç«‹å¯èƒ½
ã¾ã™        | AUX        | åŠ©å‹•è©
</code></pre>
<hr />
<h2>4. æ§‹æ–‡è§£æ</h2>
<h3>4.1 æ§‹æ–‡è§£æã¨ã¯</h3>
<p><strong>æ§‹æ–‡è§£æï¼ˆSyntactic Parsingï¼‰</strong>ã¯ã€æ–‡ã®æ–‡æ³•çš„æ§‹é€ ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹å‡¦ç†ã§ã™ã€‚</p>
<p>ä¸»ã«2ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒã‚ã‚Šã¾ã™ï¼š
1. <strong>å¥æ§‹é€ è§£æ</strong>: æ–‡ã‚’æœ¨æ§‹é€ ã§è¡¨ç¾
2. <strong>ä¾å­˜æ§‹é€ è§£æ</strong>: å˜èªé–“ã®ä¾å­˜é–¢ä¿‚ã‚’è¡¨ç¾</p>
<h3>4.2 ä¾å­˜æ§‹é€ è§£æ</h3>
<p>ä¾å­˜æ§‹é€ è§£æã§ã¯ã€å„å˜èªãŒæ–‡ä¸­ã§ã©ã®å˜èªã«ä¾å­˜ã—ã¦ã„ã‚‹ã‹ã‚’ç‰¹å®šã—ã¾ã™ã€‚</p>
<pre><code class="language-python">import spacy

nlp = spacy.load(&quot;ja_ginza&quot;)

text = &quot;å¤ªéƒãŒèŠ±å­ã«ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆã‚’ã‚ã’ãŸ&quot;
doc = nlp(text)

# ä¾å­˜é–¢ä¿‚ã‚’è¡¨ç¤º
for token in doc:
    print(f&quot;{token.text:10s} &lt;- {token.dep_:10s} - {token.head.text}&quot;)
</code></pre>
<p><strong>å‡ºåŠ›:</strong></p>
<pre><code>å¤ªéƒ        &lt;- nsubj      - ã‚ã’
ãŒ          &lt;- case       - å¤ªéƒ
èŠ±å­        &lt;- obl        - ã‚ã’
ã«          &lt;- case       - èŠ±å­
ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ  &lt;- obj        - ã‚ã’
ã‚’          &lt;- case       - ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ
ã‚ã’        &lt;- ROOT       - ã‚ã’
ãŸ          &lt;- aux        - ã‚ã’
</code></pre>
<p><strong>ä¾å­˜é–¢ä¿‚ã®æ„å‘³:</strong>
- <code>nsubj</code>: ä¸»èªï¼ˆå¤ªéƒãŒï¼‰
- <code>obl</code>: æ–œæ ¼ï¼ˆèŠ±å­ã«ï¼‰
- <code>obj</code>: ç›®çš„èªï¼ˆãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆã‚’ï¼‰
- <code>ROOT</code>: æ–‡ã®ä¸­å¿ƒï¼ˆã‚ã’ãŸï¼‰
- <code>aux</code>: åŠ©å‹•è©ï¼ˆãŸï¼‰</p>
<h3>4.3 ä¾å­˜æ§‹é€ ã®å¯è¦–åŒ–</h3>
<pre><code class="language-python">from spacy import displacy

nlp = spacy.load(&quot;ja_ginza&quot;)
doc = nlp(&quot;å¤ªéƒãŒèŠ±å­ã«ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆã‚’ã‚ã’ãŸ&quot;)

# ä¾å­˜æ§‹é€ ã‚’å¯è¦–åŒ–ï¼ˆSVGå½¢å¼ï¼‰
svg = displacy.render(doc, style=&quot;dep&quot;, jupyter=False)

# HTMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
with open(&quot;dependency.html&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
    f.write(svg)
</code></pre>
<p><strong>å¯è¦–åŒ–ã‚¤ãƒ¡ãƒ¼ã‚¸:</strong></p>
<pre><code>å¤ªéƒ â”€â”€nsubjâ”€â”€&gt; ã‚ã’ãŸ
  â”‚              â†‘
  â””â”€caseâ”€ ãŒ    â”‚
                 â”‚
èŠ±å­ â”€â”€oblâ”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â””â”€caseâ”€ ã«

ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ â”€â”€objâ”€â”€&gt; ã‚ã’ãŸ
  â”‚                  â†‘
  â””â”€caseâ”€ ã‚’        â”‚
                     â”‚
ãŸ â”€â”€â”€â”€â”€â”€auxâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h3>4.4 å¥æ§‹é€ è§£æ</h3>
<p>å¥æ§‹é€ è§£æã§ã¯ã€æ–‡ã‚’ãƒ„ãƒªãƒ¼æ§‹é€ ã§è¡¨ç¾ã—ã¾ã™ã€‚</p>
<pre><code>        S (æ–‡)
       / \
      NP  VP
     /    / \
    N    NP  V
    |   /  \ |
   å¤ªéƒ èŠ±å­ ã« ã‚ã’ãŸ
</code></pre>
<p>spaCyã§ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ä¾å­˜æ§‹é€ è§£æã‚’ä½¿ç”¨ã—ã¾ã™ãŒã€å¥æ§‹é€ ãŒå¿…è¦ãªå ´åˆã¯Berkeleyãªã©ã®ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚</p>
<hr />
<h2>5. å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰</h2>
<h3>5.1 å›ºæœ‰è¡¨ç¾æŠ½å‡ºã¨ã¯</h3>
<p><strong>å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNamed Entity Recognition, NERï¼‰</strong>ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰äººåãƒ»åœ°åãƒ»çµ„ç¹”åãªã©ã®å›ºæœ‰åè©ã‚’æŠ½å‡ºã™ã‚‹å‡¦ç†ã§ã™ã€‚</p>
<p>ä¸»ãªå›ºæœ‰è¡¨ç¾ã®ã‚¿ã‚¤ãƒ—ï¼š
- <strong>PERSON</strong>: äººå
- <strong>LOC</strong>: åœ°å
- <strong>ORG</strong>: çµ„ç¹”å
- <strong>DATE</strong>: æ—¥ä»˜
- <strong>TIME</strong>: æ™‚åˆ»
- <strong>MONEY</strong>: é‡‘é¡
- <strong>PERCENT</strong>: ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆ</p>
<h3>5.2 Ginzaã‚’ä½¿ã£ãŸNER</h3>
<pre><code class="language-python">import spacy

nlp = spacy.load(&quot;ja_ginza&quot;)

text = &quot;æ˜¨æ—¥ã€æ±äº¬ã®é¦–ç›¸å®˜é‚¸ã§å®‰å€é¦–ç›¸ãŒä¼šè¦‹ã‚’é–‹ã„ãŸã€‚&quot;
doc = nlp(text)

# å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡º
for ent in doc.ents:
    print(f&quot;{ent.text:15s} | {ent.label_:10s}&quot;)
</code></pre>
<p><strong>å‡ºåŠ›:</strong></p>
<pre><code>æ˜¨æ—¥            | DATE
æ±äº¬            | GPE
é¦–ç›¸å®˜é‚¸        | FAC
å®‰å€            | PERSON
é¦–ç›¸            | PERSON
</code></pre>
<p><strong>å›ºæœ‰è¡¨ç¾ã®ãƒ©ãƒ™ãƒ«:</strong>
- <code>PERSON</code>: äººå
- <code>GPE</code>: åœ°æ”¿å­¦çš„å®Ÿä½“ï¼ˆéƒ½å¸‚ã€å›½ãªã©ï¼‰
- <code>FAC</code>: æ–½è¨­
- <code>DATE</code>: æ—¥ä»˜
- <code>ORG</code>: çµ„ç¹”</p>
<h3>5.3 å®Ÿè·µ: ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‹ã‚‰ã®æƒ…å ±æŠ½å‡º</h3>
<pre><code class="language-python">import spacy

def extract_entities(text):
    &quot;&quot;&quot;ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‹ã‚‰å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡º&quot;&quot;&quot;
    nlp = spacy.load(&quot;ja_ginza&quot;)
    doc = nlp(text)

    entities = {
        'persons': [],
        'locations': [],
        'organizations': [],
        'dates': []
    }

    for ent in doc.ents:
        if ent.label_ == 'PERSON':
            entities['persons'].append(ent.text)
        elif ent.label_ in ['GPE', 'LOC']:
            entities['locations'].append(ent.text)
        elif ent.label_ == 'ORG':
            entities['organizations'].append(ent.text)
        elif ent.label_ == 'DATE':
            entities['dates'].append(ent.text)

    return entities

# ãƒ†ã‚¹ãƒˆ
article = &quot;&quot;&quot;
2023å¹´10æœˆ15æ—¥ã€æ±äº¬ã§é–‹å‚¬ã•ã‚ŒãŸAIæŠ€è¡“ã‚«ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã«
Googleã€Microsoftã€OpenAIã®ä»£è¡¨è€…ãŒå‚åŠ ã—ãŸã€‚
CEOã®å±±ç”°å¤ªéƒæ°ãŒã‚­ãƒ¼ãƒãƒ¼ãƒˆã‚¹ãƒ”ãƒ¼ãƒã‚’è¡Œã£ãŸã€‚
&quot;&quot;&quot;

result = extract_entities(article)
print(&quot;äººç‰©:&quot;, result['persons'])
print(&quot;å ´æ‰€:&quot;, result['locations'])
print(&quot;çµ„ç¹”:&quot;, result['organizations'])
print(&quot;æ—¥ä»˜:&quot;, result['dates'])
</code></pre>
<p><strong>å‡ºåŠ›:</strong></p>
<pre><code>äººç‰©: ['å±±ç”°å¤ªéƒ', 'CEO']
å ´æ‰€: ['æ±äº¬']
çµ„ç¹”: ['Google', 'Microsoft', 'OpenAI', 'AIæŠ€è¡“ã‚«ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹']
æ—¥ä»˜: ['2023å¹´10æœˆ15æ—¥']
</code></pre>
<hr />
<h2>6. å®Ÿè·µ: ç·åˆçš„ãªãƒ†ã‚­ã‚¹ãƒˆåˆ†æ</h2>
<p>ã“ã‚Œã¾ã§å­¦ã‚“ã æŠ€è¡“ã‚’çµ„ã¿åˆã‚ã›ã¦ã€åŒ…æ‹¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚’è¡Œã„ã¾ã™ã€‚</p>
<pre><code class="language-python">import spacy
from collections import Counter

def analyze_text(text):
    &quot;&quot;&quot;ãƒ†ã‚­ã‚¹ãƒˆã®åŒ…æ‹¬çš„åˆ†æ&quot;&quot;&quot;
    nlp = spacy.load(&quot;ja_ginza&quot;)
    doc = nlp(text)

    # 1. ãƒˆãƒ¼ã‚¯ãƒ³çµ±è¨ˆ
    tokens = [token.text for token in doc]
    token_count = len(tokens)

    # 2. å“è©çµ±è¨ˆ
    pos_counts = Counter([token.pos_ for token in doc])

    # 3. åè©æŠ½å‡º
    nouns = [token.text for token in doc if token.pos_ == 'NOUN']

    # 4. å›ºæœ‰è¡¨ç¾æŠ½å‡º
    entities = [(ent.text, ent.label_) for ent in doc.ents]

    # 5. ä¾å­˜é–¢ä¿‚ï¼ˆä¸»èª-å‹•è©-ç›®çš„èªï¼‰
    svo_triples = []
    for token in doc:
        if token.pos_ == 'VERB':
            subj = [child.text for child in token.children if child.dep_ == 'nsubj']
            obj = [child.text for child in token.children if child.dep_ == 'obj']
            if subj and obj:
                svo_triples.append((subj[0], token.text, obj[0]))

    return {
        'token_count': token_count,
        'pos_distribution': dict(pos_counts),
        'nouns': nouns,
        'entities': entities,
        'svo_triples': svo_triples
    }

# ãƒ†ã‚¹ãƒˆ
text = &quot;&quot;&quot;
æ˜¨æ—¥ã€æ±äº¬ã§é–‹ã‹ã‚ŒãŸä¼šè­°ã§ã€å±±ç”°éƒ¨é•·ãŒæ–°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¡¨ã—ãŸã€‚
ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯æ¥å¹´4æœˆã«é–‹å§‹ã•ã‚Œã‚‹äºˆå®šã§ã‚ã‚‹ã€‚
&quot;&quot;&quot;

result = analyze_text(text)

print(&quot;=== ãƒ†ã‚­ã‚¹ãƒˆåˆ†æçµæœ ===&quot;)
print(f&quot;ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {result['token_count']}&quot;)
print(f&quot;\nå“è©åˆ†å¸ƒ:&quot;)
for pos, count in result['pos_distribution'].items():
    print(f&quot;  {pos}: {count}&quot;)
print(f&quot;\nåè©: {result['nouns']}&quot;)
print(f&quot;\nå›ºæœ‰è¡¨ç¾: {result['entities']}&quot;)
print(f&quot;\nSVOãƒˆãƒªãƒ—ãƒ«: {result['svo_triples']}&quot;)
</code></pre>
<hr />
<h2>7. ç·´ç¿’å•é¡Œ</h2>
<h3>å•é¡Œ1: å½¢æ…‹ç´ è§£æ</h3>
<p>æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã®å‡ºåŠ›ã‚’äºˆæƒ³ã—ãªã•ã„ï¼š</p>
<pre><code class="language-python">from janome.tokenizer import Tokenizer
tokenizer = Tokenizer()

text = &quot;é€Ÿãèµ°ã‚‹&quot;
for token in tokenizer.tokenize(text):
    print(token)
</code></pre>
<details>
<summary>è§£ç­”ã‚’è¦‹ã‚‹</summary>


<pre><code>é€Ÿã    å‰¯è©,ä¸€èˆ¬,*,*,*,*,é€Ÿã,ãƒãƒ¤ã‚¯,ãƒãƒ¤ã‚¯
èµ°ã‚‹    å‹•è©,è‡ªç«‹,*,*,äº”æ®µãƒ»ãƒ©è¡Œ,åŸºæœ¬å½¢,èµ°ã‚‹,ãƒã‚·ãƒ«,ãƒã‚·ãƒ«
</code></pre>

ã€Œé€Ÿãã€ã¯å‰¯è©ã€ã€Œèµ°ã‚‹ã€ã¯å‹•è©ã¨ã—ã¦è§£æã•ã‚Œã¾ã™ã€‚
</details>

<h3>å•é¡Œ2: å“è©ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°</h3>
<p>ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€åè©ã®ã¿ã‚’æŠ½å‡ºã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ããªã•ã„ï¼š</p>
<pre><code class="language-python">text = &quot;ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ãªã®ã§ã€å…¬åœ’ã§æ•£æ­©ã—ã¾ã™&quot;
</code></pre>
<details>
<summary>è§£ç­”ã‚’è¦‹ã‚‹</summary>


<pre><code class="language-python">from janome.tokenizer import Tokenizer

tokenizer = Tokenizer()
nouns = []

for token in tokenizer.tokenize(text):
    parts = str(token).split('\t')
    surface = parts[0]
    pos = parts[1].split(',')[0]

    if pos == 'åè©':
        nouns.append(surface)

print(nouns)
# ['ä»Šæ—¥', 'å¤©æ°—', 'å…¬åœ’', 'æ•£æ­©']
</code></pre>

</details>

<h3>å•é¡Œ3: å›ºæœ‰è¡¨ç¾æŠ½å‡º</h3>
<p>æ¬¡ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰äººåã¨åœ°åã‚’æŠ½å‡ºã—ãªã•ã„ï¼š</p>
<pre><code class="language-python">text = &quot;ç”°ä¸­ã•ã‚“ã¯å¤§é˜ªã‹ã‚‰æ±äº¬ã¸å‡ºå¼µã«è¡Œã£ãŸ&quot;
</code></pre>
<details>
<summary>è§£ç­”ã‚’è¦‹ã‚‹</summary>


<pre><code class="language-python">import spacy

nlp = spacy.load(&quot;ja_ginza&quot;)
doc = nlp(text)

persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']
locations = [ent.text for ent in doc.ents if ent.label_ in ['GPE', 'LOC']]

print(&quot;äººå:&quot;, persons)  # ['ç”°ä¸­']
print(&quot;åœ°å:&quot;, locations)  # ['å¤§é˜ª', 'æ±äº¬']
</code></pre>

</details>

<hr />
<h2>ã¾ã¨ã‚</h2>
<p>ã“ã®ç« ã§ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨€èªå­¦çš„ã«åˆ†æã™ã‚‹åŸºæœ¬æŠ€è¡“ã‚’å­¦ã³ã¾ã—ãŸï¼š</p>
<h3>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</h3>
<ul>
<li>âœ… <strong>ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³</strong>: ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªã«åˆ†å‰²</li>
<li>âœ… <strong>å½¢æ…‹ç´ è§£æ</strong>: æ—¥æœ¬èªã§ã¯å¿…é ˆï¼ˆMeCab, Janome, SudachiPyï¼‰</li>
<li>âœ… <strong>å“è©ã‚¿ã‚°ä»˜ã‘</strong>: å„å˜èªã®æ–‡æ³•çš„å½¹å‰²ã‚’ç‰¹å®š</li>
<li>âœ… <strong>æ§‹æ–‡è§£æ</strong>: æ–‡ã®æ§‹é€ ã‚’è§£æï¼ˆä¾å­˜æ§‹é€ ãƒ»å¥æ§‹é€ ï¼‰</li>
<li>âœ… <strong>å›ºæœ‰è¡¨ç¾æŠ½å‡º</strong>: äººåãƒ»åœ°åãƒ»çµ„ç¹”åãªã©ã‚’æŠ½å‡º</li>
</ul>
<h3>ä¸»è¦ãƒ„ãƒ¼ãƒ«</h3>
<pre><code class="language-python"># å½¢æ…‹ç´ è§£æ
MeCab, Janome, SudachiPy

# ç·åˆçš„NLP
spaCy + Ginza

# ä¾å­˜æ§‹é€ è§£æãƒ»NER
spaCy, Ginza
</code></pre>
<h3>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>
<p>ã“ã‚Œã‚‰ã®å‰å‡¦ç†æŠ€è¡“ã‚’ä½¿ã£ã¦ã€æ¬¡ç« ã§ã¯<strong>å®Ÿè·µçš„ãªãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡</strong>ã‚’å®Ÿè£…ã—ã¾ã™ã€‚</p>
<hr />
<p><strong>æ¬¡ã¸</strong>: <a href="chapter-3.html">Chapter 3: å®Ÿè·µ - ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ â†’</a></p>
<p><strong>æˆ»ã‚‹</strong>: <a href="index.html">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a></p><div class="navigation">
    <a href="chapter-1.html" class="nav-button">â† ç¬¬1ç« </a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter-3.html" class="nav-button">ç¬¬3ç«  â†’</a>
</div>
    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimotoï¼ˆæ±åŒ—å¤§å­¦ï¼‰</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
