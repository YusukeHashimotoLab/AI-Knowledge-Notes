<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ÂΩ¢ÊÖãÁ¥†Ëß£Êûê„ÉªÊßãÊñáËß£Êûê</h1>
            <p class="subtitle">„ÉÜ„Ç≠„Çπ„Éà„ÇíË®ÄË™ûÂ≠¶ÁöÑ„Å´ÂàÜÊûê„Åô„ÇãÂü∫Êú¨ÊäÄË°ì</p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 30-35ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ‰∏≠Á¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: TrueÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: TrueÂïè</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 2: ÂΩ¢ÊÖãÁ¥†Ëß£Êûê„ÉªÊßãÊñáËß£Êûê</h1>
<h2>„Ç§„É≥„Éà„É≠„ÉÄ„ÇØ„Ç∑„Éß„É≥</h2>
<p>ÂâçÁ´†„ÅßNLP„ÅÆÂÖ®‰ΩìÂÉè„ÇíÂ≠¶„Å≥„Åæ„Åó„Åü„ÄÇ„Åì„ÅÆÁ´†„Åß„ÅØ„ÄÅNLP„ÅÆÊúÄ„ÇÇÂü∫Êú¨ÁöÑ„Å™Âá¶ÁêÜ„Åß„ÅÇ„Çã<strong>ÂΩ¢ÊÖãÁ¥†Ëß£Êûê</strong>„Å®<strong>ÊßãÊñáËß£Êûê</strong>„Å´„Å§„ÅÑ„Å¶„ÄÅÂÆüÈöõ„ÅÆ„ÉÑ„Éº„É´„Çí‰Ωø„ÅÑ„Å™„Åå„ÇâÂ≠¶Áøí„Åó„Åæ„Åô„ÄÇ</p>
<p>„ÉÜ„Ç≠„Çπ„Éà„ÇíÂàÜÊûê„Åô„Çã„Å´„ÅØ„ÄÅ„Åæ„ÅöÊñá„ÇíÂçòË™û„Å´ÂàÜÂâ≤„Åó„ÄÅ„Åù„Çå„Åû„Çå„ÅÆÂìÅË©û„ÇÑÊñáÊ≥ïÁöÑÂΩπÂâ≤„ÇíÁâπÂÆö„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ„Åì„Çå„Åå„Åì„ÅÆÁ´†„ÅÆ‰∏≠ÂøÉ„ÉÜ„Éº„Éû„Åß„Åô„ÄÇ</p>
<hr />
<h2>1. „Éà„Éº„ÇØ„Éä„Ç§„Çº„Éº„Ç∑„Éß„É≥</h2>
<h3>1.1 „Éà„Éº„ÇØ„Éä„Ç§„Çº„Éº„Ç∑„Éß„É≥„Å®„ÅØ</h3>
<p><strong>„Éà„Éº„ÇØ„Éä„Ç§„Çº„Éº„Ç∑„Éß„É≥ÔºàTokenizationÔºâ</strong>„ÅØ„ÄÅ„ÉÜ„Ç≠„Çπ„Éà„ÇíÊÑèÂë≥„ÅÆ„ÅÇ„ÇãÂçò‰ΩçÔºà„Éà„Éº„ÇØ„É≥Ôºâ„Å´ÂàÜÂâ≤„Åô„ÇãÂá¶ÁêÜ„Åß„Åô„ÄÇ</p>
<div class="mermaid">
graph LR
    A[„Äå‰ªäÊó•„ÅØËâØ„ÅÑÂ§©Ê∞ó„Åß„Åô„Äç] --> B[„Éà„Éº„ÇØ„Éä„Ç§„Çº„Éº„Ç∑„Éß„É≥]
    B --> C[„Äå‰ªäÊó•„Äç„Äå„ÅØ„Äç„ÄåËâØ„ÅÑ„Äç„ÄåÂ§©Ê∞ó„Äç„Äå„Åß„Åô„Äç]
</div>

<h3>1.2 Ëã±Ë™û„ÅÆ„Éà„Éº„ÇØ„Éä„Ç§„Çº„Éº„Ç∑„Éß„É≥</h3>
<p>Ëã±Ë™û„ÅØÊØîËºÉÁöÑÁ∞°Âçò„Åß„ÅôÔºö</p>
<pre><code class="language-python">text = &quot;I love natural language processing!&quot;

# „Çπ„Éö„Éº„Çπ„ÅßÂàÜÂâ≤ÔºàÊúÄ„ÇÇ„Ç∑„É≥„Éó„É´Ôºâ
tokens_simple = text.split()
print(tokens_simple)
# ['I', 'love', 'natural', 'language', 'processing!']

# NLTK„Çí‰ΩøÁî®ÔºàÂè•Ë™≠ÁÇπ„ÇÇÂàÜÈõ¢Ôºâ
import nltk
tokens_nltk = nltk.word_tokenize(text)
print(tokens_nltk)
# ['I', 'love', 'natural', 'language', 'processing', '!']
</code></pre>
<p><strong>Ë™≤È°å:</strong>
- Âè•Ë™≠ÁÇπ„ÅÆÂá¶ÁêÜÔºà"processing!" ‚Üí "processing" + "!"Ôºâ
- Áü≠Á∏ÆÂΩ¢Ôºà"don't" ‚Üí "do" + "n't"Ôºâ
- „Éè„Ç§„Éï„É≥‰ªò„ÅçÂçòË™ûÔºà"state-of-the-art"Ôºâ</p>
<h3>1.3 Êó•Êú¨Ë™û„ÅÆ„Éà„Éº„ÇØ„Éä„Ç§„Çº„Éº„Ç∑„Éß„É≥</h3>
<p>Êó•Êú¨Ë™û„ÅØÂçòË™û„ÅÆÂ¢ÉÁïå„Åå‰∏çÊòéÁ¢∫„Å™„Åü„ÇÅ„ÄÅ„Çà„ÇäË§áÈõë„Åß„ÅôÔºö</p>
<pre><code class="language-python">text = &quot;Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„ÇíÂãâÂº∑„Åó„Å¶„ÅÑ„Åæ„Åô&quot;

# ÂçòÁ¥î„Å™ÂàÜÂâ≤„Åß„ÅØ‰∏çÂèØËÉΩ
# „Å©„Åì„ÅßÂå∫Âàá„ÇãÔºü
# ‚ùå &quot;Ëá™ÁÑ∂&quot; &quot;Ë®ÄË™û&quot; &quot;Âá¶ÁêÜ&quot; &quot;„Çí&quot; &quot;ÂãâÂº∑&quot; &quot;„Åó&quot; &quot;„Å¶&quot; &quot;„ÅÑ&quot; &quot;„Åæ„Åô&quot;
# ‚úì &quot;Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ&quot; &quot;„Çí&quot; &quot;ÂãâÂº∑&quot; &quot;„Åó&quot; &quot;„Å¶&quot; &quot;„ÅÑ&quot; &quot;„Åæ„Åô&quot;
</code></pre>
<p>„Åù„ÅÆ„Åü„ÇÅ„ÄÅÊó•Êú¨Ë™û„Åß„ÅØ<strong>ÂΩ¢ÊÖãÁ¥†Ëß£Êûê</strong>„ÅåÂøÖÈ†à„Åß„Åô„ÄÇ</p>
<hr />
<h2>2. ÂΩ¢ÊÖãÁ¥†Ëß£Êûê</h2>
<h3>2.1 ÂΩ¢ÊÖãÁ¥†Ëß£Êûê„Å®„ÅØ</h3>
<p><strong>ÂΩ¢ÊÖãÁ¥†Ëß£ÊûêÔºàMorphological AnalysisÔºâ</strong>„ÅØ„ÄÅÊñá„ÇíÂΩ¢ÊÖãÁ¥†ÔºàÊÑèÂë≥„ÇíÊåÅ„Å§ÊúÄÂ∞èÂçò‰ΩçÔºâ„Å´ÂàÜÂâ≤„Åó„ÄÅ„Åù„Çå„Åû„Çå„ÅÆÂìÅË©û„ÇÑÊ¥ªÁî®ÂΩ¢„ÇíÁâπÂÆö„Åô„ÇãÂá¶ÁêÜ„Åß„Åô„ÄÇ</p>
<div class="mermaid">
graph TD
    A[„ÄåÈ£ü„Åπ„Åæ„Åó„Åü„Äç] --> B[ÂΩ¢ÊÖãÁ¥†Ëß£Êûê]
    B --> C[„ÄåÈ£ü„Åπ„Äç: ÂãïË©û„ÉªÊú™ÁÑ∂ÂΩ¢]
    B --> D[„Äå„Åæ„Åó„Äç: Âä©ÂãïË©û]
    B --> E[„Äå„Åü„Äç: Âä©ÂãïË©û„ÉªÈÅéÂéª]
</div>

<h3>2.2 Êó•Êú¨Ë™ûÂΩ¢ÊÖãÁ¥†Ëß£Êûê„ÉÑ„Éº„É´</h3>
<h4>MeCabÔºà„É°„Ç´„ÉñÔºâ</h4>
<p><strong>ÁâπÂæ¥:</strong>
- È´òÈÄü„ÉªÈ´òÁ≤æÂ∫¶
- C++ÂÆüË£ÖÔºàPython„Éê„Ç§„É≥„Éá„Ç£„É≥„Ç∞„ÅÇ„ÇäÔºâ
- IPAËæûÊõ∏„ÄÅNEologdËæûÊõ∏„Å™„Å©</p>
<p><strong>„Ç§„É≥„Çπ„Éà„Éº„É´:</strong></p>
<pre><code class="language-bash"># macOS
brew install mecab mecab-ipadic

# Python „Éê„Ç§„É≥„Éá„Ç£„É≥„Ç∞
pip install mecab-python3
</code></pre>
<p><strong>Âü∫Êú¨ÁöÑ„Å™‰Ωø„ÅÑÊñπ:</strong></p>
<pre><code class="language-python">import MeCab

# MeCab„Ç§„É≥„Çπ„Çø„É≥„Çπ‰ΩúÊàê
mecab = MeCab.Tagger()

# ÂΩ¢ÊÖãÁ¥†Ëß£Êûê
text = &quot;‰ªäÊó•„ÅØËâØ„ÅÑÂ§©Ê∞ó„Åß„Åô&quot;
result = mecab.parse(text)
print(result)
</code></pre>
<p><strong>Âá∫Âäõ‰æã:</strong></p>
<pre><code>‰ªäÊó•    ÂêçË©û,ÂâØË©ûÂèØËÉΩ,*,*,*,*,‰ªäÊó•,„Ç≠„Éß„Ç¶,„Ç≠„Éß„Éº
„ÅØ      Âä©Ë©û,‰øÇÂä©Ë©û,*,*,*,*,„ÅØ,„Éè,„ÉØ
ËâØ„ÅÑ    ÂΩ¢ÂÆπË©û,Ëá™Á´ã,*,*,ÂΩ¢ÂÆπË©û„Éª„Ç§ÊÆµ,Âü∫Êú¨ÂΩ¢,ËâØ„ÅÑ,„É®„Ç§,„É®„Ç§
Â§©Ê∞ó    ÂêçË©û,‰∏ÄËà¨,*,*,*,*,Â§©Ê∞ó,„ÉÜ„É≥„Ç≠,„ÉÜ„É≥„Ç≠
„Åß„Åô    Âä©ÂãïË©û,*,*,*,ÁâπÊÆä„Éª„Éá„Çπ,Âü∫Êú¨ÂΩ¢,„Åß„Åô,„Éá„Çπ,„Éá„Çπ
EOS
</code></pre>
<p><strong>ÊßãÈÄ†Âåñ„Åó„ÅüÂá∫Âäõ:</strong></p>
<pre><code class="language-python">def parse_mecab(text):
    &quot;&quot;&quot;MeCab„ÅÆÂá∫Âäõ„ÇíÊßãÈÄ†Âåñ&quot;&quot;&quot;
    mecab = MeCab.Tagger()
    node = mecab.parseToNode(text)

    words = []
    while node:
        if node.surface:  # Á©∫ÊñáÂ≠óÂàó„ÇíÈô§Â§ñ
            features = node.feature.split(',')
            words.append({
                'surface': node.surface,      # Ë°®Â±§ÂΩ¢
                'pos': features[0],           # ÂìÅË©û
                'pos_detail': features[1],    # ÂìÅË©ûÁ¥∞ÂàÜÈ°û
                'base_form': features[6],     # Âü∫Êú¨ÂΩ¢
                'reading': features[7] if len(features) &gt; 7 else ''
            })
        node = node.next
    return words

result = parse_mecab(&quot;Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„ÇíÂ≠¶„Çì„Åß„ÅÑ„Åæ„Åô&quot;)
for word in result:
    print(f&quot;{word['surface']:8s} | {word['pos']:6s} | {word['base_form']}&quot;)
</code></pre>
<p><strong>Âá∫Âäõ:</strong></p>
<pre><code>Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ | ÂêçË©û    | Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ
„Çí       | Âä©Ë©û    | „Çí
Â≠¶„Çì     | ÂãïË©û    | Â≠¶„Å∂
„Åß       | Âä©Ë©û    | „Åß
„ÅÑ       | ÂãïË©û    | „ÅÑ„Çã
„Åæ„Åô     | Âä©ÂãïË©û  | „Åæ„Åô
</code></pre>
<h4>JanomeÔºà„Ç∏„É£„Éé„É°Ôºâ</h4>
<p><strong>ÁâπÂæ¥:</strong>
- Pure PythonÂÆüË£Ö
- „Ç§„É≥„Çπ„Éà„Éº„É´„ÅåÁ∞°ÂçòÔºàpip „ÅÆ„ÅøÔºâ
- MeCab„Çà„ÇäÈÅÖ„ÅÑ„Åå‰Ωø„ÅÑ„ÇÑ„Åô„ÅÑ</p>
<p><strong>„Ç§„É≥„Çπ„Éà„Éº„É´:</strong></p>
<pre><code class="language-bash">pip install janome
</code></pre>
<p><strong>Âü∫Êú¨ÁöÑ„Å™‰Ωø„ÅÑÊñπ:</strong></p>
<pre><code class="language-python">from janome.tokenizer import Tokenizer

tokenizer = Tokenizer()

text = &quot;Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„ÅØÈù¢ÁôΩ„ÅÑ&quot;
tokens = tokenizer.tokenize(text)

for token in tokens:
    print(token)
</code></pre>
<p><strong>Âá∫Âäõ:</strong></p>
<pre><code>Ëá™ÁÑ∂    ÂêçË©û,‰∏ÄËà¨,*,*,*,*,Ëá™ÁÑ∂,„Ç∑„Çº„É≥,„Ç∑„Çº„É≥
Ë®ÄË™û    ÂêçË©û,‰∏ÄËà¨,*,*,*,*,Ë®ÄË™û,„Ç≤„É≥„Ç¥,„Ç≤„É≥„Ç¥
Âá¶ÁêÜ    ÂêçË©û,„ÇµÂ§âÊé•Á∂ö,*,*,*,*,Âá¶ÁêÜ,„Ç∑„Éß„É™,„Ç∑„Éß„É™
„ÅØ      Âä©Ë©û,‰øÇÂä©Ë©û,*,*,*,*,„ÅØ,„Éè,„ÉØ
Èù¢ÁôΩ„ÅÑ  ÂΩ¢ÂÆπË©û,Ëá™Á´ã,*,*,ÂΩ¢ÂÆπË©û„Éª„Ç§ÊÆµ,Âü∫Êú¨ÂΩ¢,Èù¢ÁôΩ„ÅÑ,„Ç™„É¢„Ç∑„É≠„Ç§,„Ç™„É¢„Ç∑„É≠„Ç§
</code></pre>
<p><strong>ÂçòË™û„ÅÆ„Åø„ÇíÊäΩÂá∫:</strong></p>
<pre><code class="language-python">def extract_words(text):
    &quot;&quot;&quot;ÂêçË©û„ÉªÂãïË©û„ÉªÂΩ¢ÂÆπË©û„ÅÆ„ÅøÊäΩÂá∫&quot;&quot;&quot;
    tokenizer = Tokenizer()
    words = []

    for token in tokenizer.tokenize(text):
        parts = str(token).split('\t')
        surface = parts[0]
        pos = parts[1].split(',')[0]

        # ÂêçË©û„ÉªÂãïË©û„ÉªÂΩ¢ÂÆπË©û„ÅÆ„Åø
        if pos in ['ÂêçË©û', 'ÂãïË©û', 'ÂΩ¢ÂÆπË©û']:
            words.append(surface)

    return words

text = &quot;‰ªäÊó•„ÅØÊù±‰∫¨„Åß‰ºöË≠∞„Åå„ÅÇ„Çä„Åæ„Åô&quot;
words = extract_words(text)
print(words)
# ['‰ªäÊó•', 'Êù±‰∫¨', '‰ºöË≠∞', '„ÅÇ„Çä', '„Åæ„Åô']
</code></pre>
<h4>SudachiPyÔºà„Çπ„ÉÄ„ÉÅ„Éë„Ç§Ôºâ</h4>
<p><strong>ÁâπÂæ¥:</strong>
- Ë§áÊï∞Á≤íÂ∫¶„Åß„ÅÆÂàÜÂâ≤ÔºàA/B/CÔºâ
- Êñ∞„Åó„ÅÑËæûÊõ∏‰ΩìÁ≥ª
- Ê≠£Ë¶èÂåñÊ©üËÉΩ</p>
<p><strong>„Ç§„É≥„Çπ„Éà„Éº„É´:</strong></p>
<pre><code class="language-bash">pip install sudachipy sudachidict_core
</code></pre>
<p><strong>Ë§áÊï∞Á≤íÂ∫¶„Åß„ÅÆÂàÜÂâ≤:</strong></p>
<pre><code class="language-python">from sudachipy import tokenizer
from sudachipy import dictionary

# ËæûÊõ∏„Å®„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº‰ΩúÊàê
tokenizer_obj = dictionary.Dictionary().create()

text = &quot;Êù±‰∫¨„Çπ„Ç´„Ç§„ÉÑ„É™„Éº&quot;

# A: ÊúÄÁü≠Âçò‰Ωç
mode = tokenizer.Tokenizer.SplitMode.A
print([token.surface() for token in tokenizer_obj.tokenize(text, mode)])
# ['Êù±‰∫¨', '„Çπ„Ç´„Ç§', '„ÉÑ„É™„Éº']

# B: ‰∏≠Âçò‰Ωç
mode = tokenizer.Tokenizer.SplitMode.B
print([token.surface() for token in tokenizer_obj.tokenize(text, mode)])
# ['Êù±‰∫¨', '„Çπ„Ç´„Ç§„ÉÑ„É™„Éº']

# C: ÊúÄÈï∑Âçò‰Ωç
mode = tokenizer.Tokenizer.SplitMode.C
print([token.surface() for token in tokenizer_obj.tokenize(text, mode)])
# ['Êù±‰∫¨„Çπ„Ç´„Ç§„ÉÑ„É™„Éº']
</code></pre>
<h3>2.3 ÂΩ¢ÊÖãÁ¥†Ëß£Êûê„ÅÆÂøúÁî®</h3>
<h4>„ÉÜ„Ç≠„Çπ„ÉàÊ≠£Ë¶èÂåñ</h4>
<pre><code class="language-python">from janome.tokenizer import Tokenizer

def normalize_text(text):
    &quot;&quot;&quot;„ÉÜ„Ç≠„Çπ„Éà„ÇíÂü∫Êú¨ÂΩ¢„Å´Ê≠£Ë¶èÂåñ&quot;&quot;&quot;
    tokenizer = Tokenizer()
    words = []

    for token in tokenizer.tokenize(text):
        parts = str(token).split('\t')
        features = parts[1].split(',')
        base_form = features[6]  # Âü∫Êú¨ÂΩ¢

        if base_form != '*':
            words.append(base_form)
        else:
            words.append(parts[0])

    return ' '.join(words)

text = &quot;Ëµ∞„Å£„Å¶„ÅÑ„Çã‰∫∫„ÇíË¶ã„Åæ„Åó„Åü&quot;
normalized = normalize_text(text)
print(normalized)
# 'Ëµ∞„Çã „Å¶ „ÅÑ„Çã ‰∫∫ „Çí Ë¶ã„Çã „Åæ„Åô „Åü'
</code></pre>
<h4>„Çπ„Éà„ÉÉ„Éó„ÉØ„Éº„ÉâÈô§Âéª</h4>
<pre><code class="language-python">def remove_stopwords(text):
    &quot;&quot;&quot;Âä©Ë©û„ÉªÂä©ÂãïË©û„ÇíÈô§Âéª&quot;&quot;&quot;
    tokenizer = Tokenizer()
    words = []

    stopword_pos = ['Âä©Ë©û', 'Âä©ÂãïË©û', 'Ë®òÂè∑']

    for token in tokenizer.tokenize(text):
        parts = str(token).split('\t')
        surface = parts[0]
        pos = parts[1].split(',')[0]

        if pos not in stopword_pos:
            words.append(surface)

    return words

text = &quot;‰ªäÊó•„ÅØÊù±‰∫¨„Åß‰ºöË≠∞„Åå„ÅÇ„Çä„Åæ„Åô&quot;
filtered = remove_stopwords(text)
print(filtered)
# ['‰ªäÊó•', 'Êù±‰∫¨', '‰ºöË≠∞', '„ÅÇ„Çä']
</code></pre>
<hr />
<h2>3. ÂìÅË©û„Çø„Ç∞‰ªò„Åë</h2>
<h3>3.1 ÂìÅË©û„Çø„Ç∞„Å®„ÅØ</h3>
<p><strong>ÂìÅË©û„Çø„Ç∞‰ªò„ÅëÔºàPOS TaggingÔºâ</strong>„ÅØ„ÄÅÂêÑÂçòË™û„ÅÆÂìÅË©û„ÇíÁâπÂÆö„Åô„ÇãÂá¶ÁêÜ„Åß„Åô„ÄÇ</p>
<p>Êó•Êú¨Ë™û„ÅÆ‰∏ª„Å™ÂìÅË©ûÔºö
- ÂêçË©û„ÄÅÂãïË©û„ÄÅÂΩ¢ÂÆπË©û„ÄÅÂâØË©û
- Âä©Ë©û„ÄÅÂä©ÂãïË©û
- Êé•Á∂öË©û„ÄÅÊÑüÂãïË©û
- Ë®òÂè∑</p>
<h3>3.2 spaCy + Ginza„Çí‰Ωø„Å£„ÅüÂìÅË©û„Çø„Ç∞‰ªò„Åë</h3>
<p>Ginza„ÅØÊó•Êú¨Ë™û„Å´ÁâπÂåñ„Åó„ÅüspaCy„É¢„Éá„É´„Åß„Åô„ÄÇ</p>
<p><strong>„Ç§„É≥„Çπ„Éà„Éº„É´:</strong></p>
<pre><code class="language-bash">pip install spacy
pip install ginza ja-ginza
</code></pre>
<p><strong>Âü∫Êú¨ÁöÑ„Å™‰Ωø„ÅÑÊñπ:</strong></p>
<pre><code class="language-python">import spacy

# Êó•Êú¨Ë™û„É¢„Éá„É´Ë™≠„ÅøËæº„Åø
nlp = spacy.load(&quot;ja_ginza&quot;)

text = &quot;‰ªäÊó•„ÅØÊù±‰∫¨„Åß‰ºöË≠∞„Åå„ÅÇ„Çä„Åæ„Åô&quot;
doc = nlp(text)

# ÂìÅË©û„Çø„Ç∞‰ªò„ÅëÁµêÊûú
for token in doc:
    print(f&quot;{token.text:10s} | {token.pos_:10s} | {token.tag_}&quot;)
</code></pre>
<p><strong>Âá∫Âäõ:</strong></p>
<pre><code>‰ªäÊó•        | NOUN       | ÂêçË©û-ÊôÆÈÄöÂêçË©û-ÂâØË©ûÂèØËÉΩ
„ÅØ          | ADP        | Âä©Ë©û-‰øÇÂä©Ë©û
Êù±‰∫¨        | PROPN      | ÂêçË©û-Âõ∫ÊúâÂêçË©û-Âú∞Âêç-‰∏ÄËà¨
„Åß          | ADP        | Âä©Ë©û-Ê†ºÂä©Ë©û
‰ºöË≠∞        | NOUN       | ÂêçË©û-ÊôÆÈÄöÂêçË©û-„ÇµÂ§âÂèØËÉΩ
„Åå          | ADP        | Âä©Ë©û-Ê†ºÂä©Ë©û
„ÅÇ„Çä        | VERB       | ÂãïË©û-ÈùûËá™Á´ãÂèØËÉΩ
„Åæ„Åô        | AUX        | Âä©ÂãïË©û
</code></pre>
<hr />
<h2>4. ÊßãÊñáËß£Êûê</h2>
<h3>4.1 ÊßãÊñáËß£Êûê„Å®„ÅØ</h3>
<p><strong>ÊßãÊñáËß£ÊûêÔºàSyntactic ParsingÔºâ</strong>„ÅØ„ÄÅÊñá„ÅÆÊñáÊ≥ïÁöÑÊßãÈÄ†„ÇíÊòé„Çâ„Åã„Å´„Åô„ÇãÂá¶ÁêÜ„Åß„Åô„ÄÇ</p>
<p>‰∏ª„Å´2„Å§„ÅÆ„Ç¢„Éó„É≠„Éº„ÉÅ„Åå„ÅÇ„Çä„Åæ„ÅôÔºö
1. <strong>Âè•ÊßãÈÄ†Ëß£Êûê</strong>: Êñá„ÇíÊú®ÊßãÈÄ†„ÅßË°®Áèæ
2. <strong>‰æùÂ≠òÊßãÈÄ†Ëß£Êûê</strong>: ÂçòË™ûÈñì„ÅÆ‰æùÂ≠òÈñ¢‰øÇ„ÇíË°®Áèæ</p>
<h3>4.2 ‰æùÂ≠òÊßãÈÄ†Ëß£Êûê</h3>
<p>‰æùÂ≠òÊßãÈÄ†Ëß£Êûê„Åß„ÅØ„ÄÅÂêÑÂçòË™û„ÅåÊñá‰∏≠„Åß„Å©„ÅÆÂçòË™û„Å´‰æùÂ≠ò„Åó„Å¶„ÅÑ„Çã„Åã„ÇíÁâπÂÆö„Åó„Åæ„Åô„ÄÇ</p>
<pre><code class="language-python">import spacy

nlp = spacy.load(&quot;ja_ginza&quot;)

text = &quot;Â§™ÈÉé„ÅåËä±Â≠ê„Å´„Éó„É¨„Çº„É≥„Éà„Çí„ÅÇ„Åí„Åü&quot;
doc = nlp(text)

# ‰æùÂ≠òÈñ¢‰øÇ„ÇíË°®Á§∫
for token in doc:
    print(f&quot;{token.text:10s} &lt;- {token.dep_:10s} - {token.head.text}&quot;)
</code></pre>
<p><strong>Âá∫Âäõ:</strong></p>
<pre><code>Â§™ÈÉé        &lt;- nsubj      - „ÅÇ„Åí
„Åå          &lt;- case       - Â§™ÈÉé
Ëä±Â≠ê        &lt;- obl        - „ÅÇ„Åí
„Å´          &lt;- case       - Ëä±Â≠ê
„Éó„É¨„Çº„É≥„Éà  &lt;- obj        - „ÅÇ„Åí
„Çí          &lt;- case       - „Éó„É¨„Çº„É≥„Éà
„ÅÇ„Åí        &lt;- ROOT       - „ÅÇ„Åí
„Åü          &lt;- aux        - „ÅÇ„Åí
</code></pre>
<p><strong>‰æùÂ≠òÈñ¢‰øÇ„ÅÆÊÑèÂë≥:</strong>
- <code>nsubj</code>: ‰∏ªË™ûÔºàÂ§™ÈÉé„ÅåÔºâ
- <code>obl</code>: ÊñúÊ†ºÔºàËä±Â≠ê„Å´Ôºâ
- <code>obj</code>: ÁõÆÁöÑË™ûÔºà„Éó„É¨„Çº„É≥„Éà„ÇíÔºâ
- <code>ROOT</code>: Êñá„ÅÆ‰∏≠ÂøÉÔºà„ÅÇ„Åí„ÅüÔºâ
- <code>aux</code>: Âä©ÂãïË©ûÔºà„ÅüÔºâ</p>
<h3>4.3 ‰æùÂ≠òÊßãÈÄ†„ÅÆÂèØË¶ñÂåñ</h3>
<pre><code class="language-python">from spacy import displacy

nlp = spacy.load(&quot;ja_ginza&quot;)
doc = nlp(&quot;Â§™ÈÉé„ÅåËä±Â≠ê„Å´„Éó„É¨„Çº„É≥„Éà„Çí„ÅÇ„Åí„Åü&quot;)

# ‰æùÂ≠òÊßãÈÄ†„ÇíÂèØË¶ñÂåñÔºàSVGÂΩ¢ÂºèÔºâ
svg = displacy.render(doc, style=&quot;dep&quot;, jupyter=False)

# HTML„Éï„Ç°„Ç§„É´„Å®„Åó„Å¶‰øùÂ≠ò
with open(&quot;dependency.html&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
    f.write(svg)
</code></pre>
<p><strong>ÂèØË¶ñÂåñ„Ç§„É°„Éº„Ç∏:</strong></p>
<pre><code>Â§™ÈÉé ‚îÄ‚îÄnsubj‚îÄ‚îÄ&gt; „ÅÇ„Åí„Åü
  ‚îÇ              ‚Üë
  ‚îî‚îÄcase‚îÄ „Åå    ‚îÇ
                 ‚îÇ
Ëä±Â≠ê ‚îÄ‚îÄobl‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ‚îÇ
  ‚îî‚îÄcase‚îÄ „Å´

„Éó„É¨„Çº„É≥„Éà ‚îÄ‚îÄobj‚îÄ‚îÄ&gt; „ÅÇ„Åí„Åü
  ‚îÇ                  ‚Üë
  ‚îî‚îÄcase‚îÄ „Çí        ‚îÇ
                     ‚îÇ
„Åü ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄaux‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre>
<h3>4.4 Âè•ÊßãÈÄ†Ëß£Êûê</h3>
<p>Âè•ÊßãÈÄ†Ëß£Êûê„Åß„ÅØ„ÄÅÊñá„Çí„ÉÑ„É™„ÉºÊßãÈÄ†„ÅßË°®Áèæ„Åó„Åæ„Åô„ÄÇ</p>
<pre><code>        S (Êñá)
       / \
      NP  VP
     /    / \
    N    NP  V
    |   /  \ |
   Â§™ÈÉé Ëä±Â≠ê „Å´ „ÅÇ„Åí„Åü
</code></pre>
<p>spaCy„Åß„ÅØ„Éá„Éï„Ç©„É´„Éà„Åß‰æùÂ≠òÊßãÈÄ†Ëß£Êûê„Çí‰ΩøÁî®„Åó„Åæ„Åô„Åå„ÄÅÂè•ÊßãÈÄ†„ÅåÂøÖË¶Å„Å™Â†¥Âêà„ÅØBerkeley„Å™„Å©„ÅÆ„Éë„Éº„Çµ„Éº„Çí‰ΩøÁî®„Åó„Åæ„Åô„ÄÇ</p>
<hr />
<h2>5. Âõ∫ÊúâË°®ÁèæÊäΩÂá∫ÔºàNERÔºâ</h2>
<h3>5.1 Âõ∫ÊúâË°®ÁèæÊäΩÂá∫„Å®„ÅØ</h3>
<p><strong>Âõ∫ÊúâË°®ÁèæÊäΩÂá∫ÔºàNamed Entity Recognition, NERÔºâ</strong>„ÅØ„ÄÅ„ÉÜ„Ç≠„Çπ„Éà„Åã„Çâ‰∫∫Âêç„ÉªÂú∞Âêç„ÉªÁµÑÁπîÂêç„Å™„Å©„ÅÆÂõ∫ÊúâÂêçË©û„ÇíÊäΩÂá∫„Åô„ÇãÂá¶ÁêÜ„Åß„Åô„ÄÇ</p>
<p>‰∏ª„Å™Âõ∫ÊúâË°®Áèæ„ÅÆ„Çø„Ç§„ÉóÔºö
- <strong>PERSON</strong>: ‰∫∫Âêç
- <strong>LOC</strong>: Âú∞Âêç
- <strong>ORG</strong>: ÁµÑÁπîÂêç
- <strong>DATE</strong>: Êó•‰ªò
- <strong>TIME</strong>: ÊôÇÂàª
- <strong>MONEY</strong>: ÈáëÈ°ç
- <strong>PERCENT</strong>: „Éë„Éº„Çª„É≥„Éà</p>
<h3>5.2 Ginza„Çí‰Ωø„Å£„ÅüNER</h3>
<pre><code class="language-python">import spacy

nlp = spacy.load(&quot;ja_ginza&quot;)

text = &quot;Êò®Êó•„ÄÅÊù±‰∫¨„ÅÆÈ¶ñÁõ∏ÂÆòÈÇ∏„ÅßÂÆâÂÄçÈ¶ñÁõ∏„Åå‰ºöË¶ã„ÇíÈñã„ÅÑ„Åü„ÄÇ&quot;
doc = nlp(text)

# Âõ∫ÊúâË°®Áèæ„ÇíÊäΩÂá∫
for ent in doc.ents:
    print(f&quot;{ent.text:15s} | {ent.label_:10s}&quot;)
</code></pre>
<p><strong>Âá∫Âäõ:</strong></p>
<pre><code>Êò®Êó•            | DATE
Êù±‰∫¨            | GPE
È¶ñÁõ∏ÂÆòÈÇ∏        | FAC
ÂÆâÂÄç            | PERSON
È¶ñÁõ∏            | PERSON
</code></pre>
<p><strong>Âõ∫ÊúâË°®Áèæ„ÅÆ„É©„Éô„É´:</strong>
- <code>PERSON</code>: ‰∫∫Âêç
- <code>GPE</code>: Âú∞ÊîøÂ≠¶ÁöÑÂÆü‰ΩìÔºàÈÉΩÂ∏Ç„ÄÅÂõΩ„Å™„Å©Ôºâ
- <code>FAC</code>: ÊñΩË®≠
- <code>DATE</code>: Êó•‰ªò
- <code>ORG</code>: ÁµÑÁπî</p>
<h3>5.3 ÂÆüË∑µ: „Éã„É•„Éº„ÇπË®ò‰∫ã„Åã„Çâ„ÅÆÊÉÖÂ†±ÊäΩÂá∫</h3>
<pre><code class="language-python">import spacy

def extract_entities(text):
    &quot;&quot;&quot;„Éã„É•„Éº„ÇπË®ò‰∫ã„Åã„ÇâÂõ∫ÊúâË°®Áèæ„ÇíÊäΩÂá∫&quot;&quot;&quot;
    nlp = spacy.load(&quot;ja_ginza&quot;)
    doc = nlp(text)

    entities = {
        'persons': [],
        'locations': [],
        'organizations': [],
        'dates': []
    }

    for ent in doc.ents:
        if ent.label_ == 'PERSON':
            entities['persons'].append(ent.text)
        elif ent.label_ in ['GPE', 'LOC']:
            entities['locations'].append(ent.text)
        elif ent.label_ == 'ORG':
            entities['organizations'].append(ent.text)
        elif ent.label_ == 'DATE':
            entities['dates'].append(ent.text)

    return entities

# „ÉÜ„Çπ„Éà
article = &quot;&quot;&quot;
2023Âπ¥10Êúà15Êó•„ÄÅÊù±‰∫¨„ÅßÈñãÂÇ¨„Åï„Çå„ÅüAIÊäÄË°ì„Ç´„É≥„Éï„Ç°„É¨„É≥„Çπ„Å´
Google„ÄÅMicrosoft„ÄÅOpenAI„ÅÆ‰ª£Ë°®ËÄÖ„ÅåÂèÇÂä†„Åó„Åü„ÄÇ
CEO„ÅÆÂ±±Áî∞Â§™ÈÉéÊ∞è„Åå„Ç≠„Éº„Éé„Éº„Éà„Çπ„Éî„Éº„ÉÅ„ÇíË°å„Å£„Åü„ÄÇ
&quot;&quot;&quot;

result = extract_entities(article)
print(&quot;‰∫∫Áâ©:&quot;, result['persons'])
print(&quot;Â†¥ÊâÄ:&quot;, result['locations'])
print(&quot;ÁµÑÁπî:&quot;, result['organizations'])
print(&quot;Êó•‰ªò:&quot;, result['dates'])
</code></pre>
<p><strong>Âá∫Âäõ:</strong></p>
<pre><code>‰∫∫Áâ©: ['Â±±Áî∞Â§™ÈÉé', 'CEO']
Â†¥ÊâÄ: ['Êù±‰∫¨']
ÁµÑÁπî: ['Google', 'Microsoft', 'OpenAI', 'AIÊäÄË°ì„Ç´„É≥„Éï„Ç°„É¨„É≥„Çπ']
Êó•‰ªò: ['2023Âπ¥10Êúà15Êó•']
</code></pre>
<hr />
<h2>6. ÂÆüË∑µ: Á∑èÂêàÁöÑ„Å™„ÉÜ„Ç≠„Çπ„ÉàÂàÜÊûê</h2>
<p>„Åì„Çå„Åæ„ÅßÂ≠¶„Çì„Å†ÊäÄË°ì„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„Å¶„ÄÅÂåÖÊã¨ÁöÑ„Å™„ÉÜ„Ç≠„Çπ„ÉàÂàÜÊûê„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ</p>
<pre><code class="language-python">import spacy
from collections import Counter

def analyze_text(text):
    &quot;&quot;&quot;„ÉÜ„Ç≠„Çπ„Éà„ÅÆÂåÖÊã¨ÁöÑÂàÜÊûê&quot;&quot;&quot;
    nlp = spacy.load(&quot;ja_ginza&quot;)
    doc = nlp(text)

    # 1. „Éà„Éº„ÇØ„É≥Áµ±Ë®à
    tokens = [token.text for token in doc]
    token_count = len(tokens)

    # 2. ÂìÅË©ûÁµ±Ë®à
    pos_counts = Counter([token.pos_ for token in doc])

    # 3. ÂêçË©ûÊäΩÂá∫
    nouns = [token.text for token in doc if token.pos_ == 'NOUN']

    # 4. Âõ∫ÊúâË°®ÁèæÊäΩÂá∫
    entities = [(ent.text, ent.label_) for ent in doc.ents]

    # 5. ‰æùÂ≠òÈñ¢‰øÇÔºà‰∏ªË™û-ÂãïË©û-ÁõÆÁöÑË™ûÔºâ
    svo_triples = []
    for token in doc:
        if token.pos_ == 'VERB':
            subj = [child.text for child in token.children if child.dep_ == 'nsubj']
            obj = [child.text for child in token.children if child.dep_ == 'obj']
            if subj and obj:
                svo_triples.append((subj[0], token.text, obj[0]))

    return {
        'token_count': token_count,
        'pos_distribution': dict(pos_counts),
        'nouns': nouns,
        'entities': entities,
        'svo_triples': svo_triples
    }

# „ÉÜ„Çπ„Éà
text = &quot;&quot;&quot;
Êò®Êó•„ÄÅÊù±‰∫¨„ÅßÈñã„Åã„Çå„Åü‰ºöË≠∞„Åß„ÄÅÂ±±Áî∞ÈÉ®Èï∑„ÅåÊñ∞„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë°®„Åó„Åü„ÄÇ
„Åì„ÅÆ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅØÊù•Âπ¥4Êúà„Å´ÈñãÂßã„Åï„Çå„Çã‰∫àÂÆö„Åß„ÅÇ„Çã„ÄÇ
&quot;&quot;&quot;

result = analyze_text(text)

print(&quot;=== „ÉÜ„Ç≠„Çπ„ÉàÂàÜÊûêÁµêÊûú ===&quot;)
print(f&quot;„Éà„Éº„ÇØ„É≥Êï∞: {result['token_count']}&quot;)
print(f&quot;\nÂìÅË©ûÂàÜÂ∏É:&quot;)
for pos, count in result['pos_distribution'].items():
    print(f&quot;  {pos}: {count}&quot;)
print(f&quot;\nÂêçË©û: {result['nouns']}&quot;)
print(f&quot;\nÂõ∫ÊúâË°®Áèæ: {result['entities']}&quot;)
print(f&quot;\nSVO„Éà„É™„Éó„É´: {result['svo_triples']}&quot;)
</code></pre>
<hr />
<h2>7. Á∑¥ÁøíÂïèÈ°å</h2>
<h3>ÂïèÈ°å1: ÂΩ¢ÊÖãÁ¥†Ëß£Êûê</h3>
<p>Ê¨°„ÅÆ„Ç≥„Éº„Éâ„ÅÆÂá∫Âäõ„Çí‰∫àÊÉ≥„Åó„Å™„Åï„ÅÑÔºö</p>
<pre><code class="language-python">from janome.tokenizer import Tokenizer
tokenizer = Tokenizer()

text = &quot;ÈÄü„ÅèËµ∞„Çã&quot;
for token in tokenizer.tokenize(text):
    print(token)
</code></pre>
<details>
<summary>Ëß£Á≠î„ÇíË¶ã„Çã</summary>


<pre><code>ÈÄü„Åè    ÂâØË©û,‰∏ÄËà¨,*,*,*,*,ÈÄü„Åè,„Éè„É§„ÇØ,„Éè„É§„ÇØ
Ëµ∞„Çã    ÂãïË©û,Ëá™Á´ã,*,*,‰∫îÊÆµ„Éª„É©Ë°å,Âü∫Êú¨ÂΩ¢,Ëµ∞„Çã,„Éè„Ç∑„É´,„Éè„Ç∑„É´
</code></pre>

„ÄåÈÄü„Åè„Äç„ÅØÂâØË©û„ÄÅ„ÄåËµ∞„Çã„Äç„ÅØÂãïË©û„Å®„Åó„Å¶Ëß£Êûê„Åï„Çå„Åæ„Åô„ÄÇ
</details>

<h3>ÂïèÈ°å2: ÂìÅË©û„Éï„Ç£„É´„Çø„É™„É≥„Ç∞</h3>
<p>‰ª•‰∏ã„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„Åã„Çâ„ÄÅÂêçË©û„ÅÆ„Åø„ÇíÊäΩÂá∫„Åô„Çã„Ç≥„Éº„Éâ„ÇíÊõ∏„Åç„Å™„Åï„ÅÑÔºö</p>
<pre><code class="language-python">text = &quot;‰ªäÊó•„ÅØËâØ„ÅÑÂ§©Ê∞ó„Å™„ÅÆ„Åß„ÄÅÂÖ¨Âúí„ÅßÊï£Ê≠©„Åó„Åæ„Åô&quot;
</code></pre>
<details>
<summary>Ëß£Á≠î„ÇíË¶ã„Çã</summary>


<pre><code class="language-python">from janome.tokenizer import Tokenizer

tokenizer = Tokenizer()
nouns = []

for token in tokenizer.tokenize(text):
    parts = str(token).split('\t')
    surface = parts[0]
    pos = parts[1].split(',')[0]

    if pos == 'ÂêçË©û':
        nouns.append(surface)

print(nouns)
# ['‰ªäÊó•', 'Â§©Ê∞ó', 'ÂÖ¨Âúí', 'Êï£Ê≠©']
</code></pre>

</details>

<h3>ÂïèÈ°å3: Âõ∫ÊúâË°®ÁèæÊäΩÂá∫</h3>
<p>Ê¨°„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„Åã„Çâ‰∫∫Âêç„Å®Âú∞Âêç„ÇíÊäΩÂá∫„Åó„Å™„Åï„ÅÑÔºö</p>
<pre><code class="language-python">text = &quot;Áî∞‰∏≠„Åï„Çì„ÅØÂ§ßÈò™„Åã„ÇâÊù±‰∫¨„Å∏Âá∫Âºµ„Å´Ë°å„Å£„Åü&quot;
</code></pre>
<details>
<summary>Ëß£Á≠î„ÇíË¶ã„Çã</summary>


<pre><code class="language-python">import spacy

nlp = spacy.load(&quot;ja_ginza&quot;)
doc = nlp(text)

persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']
locations = [ent.text for ent in doc.ents if ent.label_ in ['GPE', 'LOC']]

print(&quot;‰∫∫Âêç:&quot;, persons)  # ['Áî∞‰∏≠']
print(&quot;Âú∞Âêç:&quot;, locations)  # ['Â§ßÈò™', 'Êù±‰∫¨']
</code></pre>

</details>

<hr />
<h2>„Åæ„Å®„ÇÅ</h2>
<p>„Åì„ÅÆÁ´†„Åß„ÅØ„ÄÅ„ÉÜ„Ç≠„Çπ„Éà„ÇíË®ÄË™ûÂ≠¶ÁöÑ„Å´ÂàÜÊûê„Åô„ÇãÂü∫Êú¨ÊäÄË°ì„ÇíÂ≠¶„Å≥„Åæ„Åó„ÅüÔºö</p>
<h3>ÈáçË¶Å„Éù„Ç§„É≥„Éà</h3>
<ul>
<li>‚úÖ <strong>„Éà„Éº„ÇØ„Éä„Ç§„Çº„Éº„Ç∑„Éß„É≥</strong>: „ÉÜ„Ç≠„Çπ„Éà„ÇíÂçòË™û„Å´ÂàÜÂâ≤</li>
<li>‚úÖ <strong>ÂΩ¢ÊÖãÁ¥†Ëß£Êûê</strong>: Êó•Êú¨Ë™û„Åß„ÅØÂøÖÈ†àÔºàMeCab, Janome, SudachiPyÔºâ</li>
<li>‚úÖ <strong>ÂìÅË©û„Çø„Ç∞‰ªò„Åë</strong>: ÂêÑÂçòË™û„ÅÆÊñáÊ≥ïÁöÑÂΩπÂâ≤„ÇíÁâπÂÆö</li>
<li>‚úÖ <strong>ÊßãÊñáËß£Êûê</strong>: Êñá„ÅÆÊßãÈÄ†„ÇíËß£ÊûêÔºà‰æùÂ≠òÊßãÈÄ†„ÉªÂè•ÊßãÈÄ†Ôºâ</li>
<li>‚úÖ <strong>Âõ∫ÊúâË°®ÁèæÊäΩÂá∫</strong>: ‰∫∫Âêç„ÉªÂú∞Âêç„ÉªÁµÑÁπîÂêç„Å™„Å©„ÇíÊäΩÂá∫</li>
</ul>
<h3>‰∏ªË¶Å„ÉÑ„Éº„É´</h3>
<pre><code class="language-python"># ÂΩ¢ÊÖãÁ¥†Ëß£Êûê
MeCab, Janome, SudachiPy

# Á∑èÂêàÁöÑNLP
spaCy + Ginza

# ‰æùÂ≠òÊßãÈÄ†Ëß£Êûê„ÉªNER
spaCy, Ginza
</code></pre>
<h3>Ê¨°„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó</h3>
<p>„Åì„Çå„Çâ„ÅÆÂâçÂá¶ÁêÜÊäÄË°ì„Çí‰Ωø„Å£„Å¶„ÄÅÊ¨°Á´†„Åß„ÅØ<strong>ÂÆüË∑µÁöÑ„Å™„ÉÜ„Ç≠„Çπ„ÉàÂàÜÈ°û</strong>„ÇíÂÆüË£Ö„Åó„Åæ„Åô„ÄÇ</p>
<hr />
<p><strong>Ê¨°„Å∏</strong>: <a href="chapter-3.html">Chapter 3: ÂÆüË∑µ - „ÉÜ„Ç≠„Çπ„ÉàÂàÜÈ°û ‚Üí</a></p>
<p><strong>Êàª„Çã</strong>: <a href="index.html">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°</a></p><div class="navigation">
    <a href="chapter-1.html" class="nav-button">‚Üê Á¨¨1Á´†</a>
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
    <a href="chapter-3.html" class="nav-button">Á¨¨3Á´† ‚Üí</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>Áõ£‰øÆ</strong>: Dr. Yusuke HashimotoÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-17</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
