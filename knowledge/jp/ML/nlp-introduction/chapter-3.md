---
title: "Chapter 3: å®Ÿè·µ - ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡"
chapter_number: 3
series: è‡ªç„¶è¨€èªå‡¦ç†å…¥é–€
difficulty: åˆç´š
reading_time: 35-40åˆ†
tags: [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡, TF-IDF, æ©Ÿæ¢°å­¦ç¿’, æ„Ÿæƒ…åˆ†æ, scikit-learn]
prerequisites: [PythonåŸºç¤, Chapter 1, Chapter 2]
ai_generated: true
ai_model: Claude 3.5 Sonnet
generation_date: 2025-10-20
last_updated: 2025-10-20
version: 1.0
---

# Chapter 3: å®Ÿè·µ - ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡

## æœ¬ç« ã®æ¦‚è¦

ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ï¼ˆText Classificationï¼‰ã¯ã€NLPã®æœ€ã‚‚å®Ÿç”¨çš„ãªã‚¿ã‚¹ã‚¯ã®1ã¤ã§ã™ã€‚æ„Ÿæƒ…åˆ†æã€ã‚¹ãƒ‘ãƒ ãƒ•ã‚£ãƒ«ã‚¿ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ãªã©ã€å¹…åºƒã„å¿œç”¨ãŒã‚ã‚Šã¾ã™ã€‚

æœ¬ç« ã§ã¯ã€å®Ÿéš›ã«ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹æ–¹æ³•ã‚’ã€å®Œå…¨ãªå®Ÿè£…ä¾‹ã¨ã¨ã‚‚ã«å­¦ã³ã¾ã™ã€‚

### å­¦ç¿’ç›®æ¨™

- âœ… ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè£…
- âœ… Bag-of-Words ã¨ TF-IDF ã«ã‚ˆã‚‹ç‰¹å¾´é‡æŠ½å‡º
- âœ… æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹åˆ†é¡
- âœ… ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¨æ”¹å–„æ‰‹æ³•
- âœ… å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®å®Ÿè£…çµŒé¨“

---

## 1. ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã¨ã¯

### 1.1 å®šç¾©ã¨å¿œç”¨ä¾‹

**ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ï¼ˆText Classificationï¼‰** ã¨ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’äºˆã‚å®šç¾©ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã«è‡ªå‹•çš„ã«åˆ†é¡ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚

#### ä¸»ãªå¿œç”¨ä¾‹

| å¿œç”¨åˆ†é‡ | å…·ä½“ä¾‹ | ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ |
|---------|--------|-------------|
| **æ„Ÿæƒ…åˆ†æ** | å•†å“ãƒ¬ãƒ“ãƒ¥ãƒ¼ã® Positive/Negative åˆ¤å®š | é¡§å®¢æº€è¶³åº¦ã®æŠŠæ¡ |
| **ã‚¹ãƒ‘ãƒ ãƒ•ã‚£ãƒ«ã‚¿** | ãƒ¡ãƒ¼ãƒ«ã®ã‚¹ãƒ‘ãƒ /éã‚¹ãƒ‘ãƒ åˆ†é¡ | ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å‘ä¸Š |
| **ã‚«ãƒ†ã‚´ãƒªåˆ†é¡** | ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ã‚¸ãƒ£ãƒ³ãƒ«åˆ†é¡ | ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¨è–¦ |
| **æ„å›³æ¨å®š** | å•ã„åˆã‚ã›ã®ç¨®é¡åˆ¤å®š | ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆåŠ¹ç‡åŒ– |
| **ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡** | è«–æ–‡ã®ç ”ç©¶åˆ†é‡åˆ†é¡ | æƒ…å ±æ•´ç† |

### 1.2 ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã®æµã‚Œ

```mermaid
graph LR
    A[ç”Ÿãƒ†ã‚­ã‚¹ãƒˆ] --> B[å‰å‡¦ç†]
    B --> C[ç‰¹å¾´é‡æŠ½å‡º]
    C --> D[ãƒ¢ãƒ‡ãƒ«å­¦ç¿’]
    D --> E[äºˆæ¸¬]
    E --> F[è©•ä¾¡]
    F -->|æ”¹å–„| B
```

1. **å‰å‡¦ç†**: ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã€æ­£è¦åŒ–
2. **ç‰¹å¾´é‡æŠ½å‡º**: ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›
3. **ãƒ¢ãƒ‡ãƒ«å­¦ç¿’**: æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§å­¦ç¿’
4. **äºˆæ¸¬**: æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†é¡
5. **è©•ä¾¡**: ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æ¸¬å®š

---

## 2. ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### 2.1 ãªãœå‰å‡¦ç†ãŒé‡è¦ã‹

ç”Ÿã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã¯ä»¥ä¸‹ã®å•é¡ŒãŒã‚ã‚Šã¾ã™ï¼š

- **ãƒã‚¤ã‚º**: URLã€è¨˜å·ã€çµµæ–‡å­—ãªã©ä¸è¦ãªæƒ…å ±
- **è¡¨è¨˜ã‚†ã‚Œ**: "ãƒ‘ã‚½ã‚³ãƒ³" vs "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿"
- **å¤§æ–‡å­—å°æ–‡å­—**: "Apple" vs "apple"
- **æ´»ç”¨å½¢**: "èµ°ã‚‹", "èµ°ã£ãŸ", "èµ°ã‚Œã°"

å‰å‡¦ç†ã«ã‚ˆã£ã¦ã“ã‚Œã‚‰ã‚’çµ±ä¸€ã—ã€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’åŠ¹ç‡ã‚’ä¸Šã’ã¾ã™ã€‚

### 2.2 åŸºæœ¬çš„ãªå‰å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—

#### è‹±èªãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†

```python
import re
import string
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.tokenize import word_tokenize

class EnglishTextPreprocessor:
    """è‹±èªãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.stop_words = set(stopwords.words('english'))
        self.stemmer = PorterStemmer()
        self.lemmatizer = WordNetLemmatizer()

    def clean_text(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        # å°æ–‡å­—åŒ–
        text = text.lower()

        # URLã®é™¤å»
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

        # ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã®é™¤å»ï¼ˆTwitterç”¨ï¼‰
        text = re.sub(r'@\w+|#\w+', '', text)

        # HTMLã‚¿ã‚°ã®é™¤å»
        text = re.sub(r'<.*?>', '', text)

        # å¥èª­ç‚¹ã®é™¤å»
        text = text.translate(str.maketrans('', '', string.punctuation))

        # æ•°å­—ã®é™¤å»ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        text = re.sub(r'\d+', '', text)

        # ä½™åˆ†ãªç©ºç™½ã®é™¤å»
        text = ' '.join(text.split())

        return text

    def tokenize_and_filter(self, text):
        """ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»"""
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        tokens = word_tokenize(text)

        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»
        tokens = [word for word in tokens if word not in self.stop_words]

        # çŸ­ã™ãã‚‹å˜èªã‚’é™¤å»ï¼ˆ2æ–‡å­—æœªæº€ï¼‰
        tokens = [word for word in tokens if len(word) > 2]

        return tokens

    def stem_tokens(self, tokens):
        """ã‚¹ãƒ†ãƒŸãƒ³ã‚°ï¼ˆèªå¹¹æŠ½å‡ºï¼‰"""
        return [self.stemmer.stem(token) for token in tokens]

    def lemmatize_tokens(self, tokens):
        """ãƒ¬ãƒ³ãƒåŒ–ï¼ˆè¦‹å‡ºã—èªåŒ–ï¼‰"""
        return [self.lemmatizer.lemmatize(token) for token in tokens]

    def preprocess(self, text, use_lemmatization=True):
        """å®Œå…¨ãªå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        # 1. ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        text = self.clean_text(text)

        # 2. ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        tokens = self.tokenize_and_filter(text)

        # 3. æ­£è¦åŒ–ï¼ˆã‚¹ãƒ†ãƒŸãƒ³ã‚° or ãƒ¬ãƒ³ãƒåŒ–ï¼‰
        if use_lemmatization:
            tokens = self.lemmatize_tokens(tokens)
        else:
            tokens = self.stem_tokens(tokens)

        return ' '.join(tokens)

# ä½¿ç”¨ä¾‹
preprocessor = EnglishTextPreprocessor()

raw_text = """
This is AMAZING!!! Check out http://example.com
I absolutely love this product ğŸ˜ #happy @company
"""

processed_text = preprocessor.preprocess(raw_text)
print(f"å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ: {raw_text}")
print(f"å‰å‡¦ç†å¾Œ: {processed_text}")
# å‡ºåŠ›ä¾‹: "amazing absolutely love product happy company"
```

#### æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†

```python
import re
import MeCab
import unicodedata

class JapaneseTextPreprocessor:
    """æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.mecab = MeCab.Tagger()

        # æ—¥æœ¬èªã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¸€éƒ¨ï¼‰
        self.stop_words = set([
            'ã®', 'ã¯', 'ã‚’', 'ãŒ', 'ã«', 'ã§', 'ã¨', 'ãŸ', 'ã—',
            'ã¦', 'ã¾ã™', 'ã§ã™', 'ã‚ã‚‹', 'ã„ã‚‹', 'ã“ã®', 'ãã®',
            'ã‚Œã‚‹', 'ã‚‰ã‚Œã‚‹', 'ã›ã‚‹', 'ã•ã›ã‚‹', 'ã“ã¨', 'ã‚‚ã®'
        ])

        # é™¤å¤–ã™ã‚‹å“è©
        self.exclude_pos = set([
            'åŠ©è©', 'åŠ©å‹•è©', 'æ¥ç¶šè©', 'è¨˜å·', 'ãƒ•ã‚£ãƒ©ãƒ¼', 'æ„Ÿå‹•è©'
        ])

    def clean_text(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        # Unicodeæ­£è¦åŒ–ï¼ˆNFKCã§åŠè§’ãƒ»å…¨è§’ã‚’çµ±ä¸€ï¼‰
        text = unicodedata.normalize('NFKC', text)

        # URLã®é™¤å»
        text = re.sub(r'http\S+|www\S+|https\S+', '', text)

        # ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã®é™¤å»
        text = re.sub(r'@\w+|#\w+', '', text)

        # æ”¹è¡Œãƒ»ã‚¿ãƒ–ã‚’ç©ºç™½ã«
        text = re.sub(r'[\n\t]', ' ', text)

        # çµµæ–‡å­—ã®é™¤å»ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # é¡”æ–‡å­—
            "\U0001F300-\U0001F5FF"  # è¨˜å·ãƒ»çµµæ–‡å­—
            "\U0001F680-\U0001F6FF"  # ä¹—ã‚Šç‰©ãƒ»å»ºç‰©
            "\U0001F1E0-\U0001F1FF"  # æ——
            "]+", flags=re.UNICODE
        )
        text = emoji_pattern.sub('', text)

        # ä½™åˆ†ãªç©ºç™½ã®é™¤å»
        text = ' '.join(text.split())

        return text

    def tokenize_with_filter(self, text):
        """MeCabã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ– + ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        # MeCabã§å½¢æ…‹ç´ è§£æ
        node = self.mecab.parseToNode(text)

        words = []
        while node:
            if node.surface:
                features = node.feature.split(',')
                surface = node.surface
                pos = features[0]  # å“è©
                base_form = features[6] if len(features) > 6 else surface

                # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶
                if (pos not in self.exclude_pos and
                    surface not in self.stop_words and
                    len(surface) > 1):  # 1æ–‡å­—å˜èªã‚’é™¤å¤–

                    # åŸºæœ¬å½¢ã‚’ä½¿ç”¨
                    words.append(base_form if base_form != '*' else surface)

            node = node.next

        return words

    def preprocess(self, text):
        """å®Œå…¨ãªå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        # 1. ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        text = self.clean_text(text)

        # 2. ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        tokens = self.tokenize_with_filter(text)

        return ' '.join(tokens)

# ä½¿ç”¨ä¾‹
jp_preprocessor = JapaneseTextPreprocessor()

raw_jp_text = """
ã“ã®å•†å“ã¯æœ¬å½“ã«ç´ æ™´ã‚‰ã—ã„ã§ã™ï¼ï¼ï¼ğŸ˜
çµ¶å¯¾ã«ãŠã™ã™ã‚ã—ã¾ã™ğŸ‰ #æœ€é«˜
è©³ã—ãã¯ã“ã¡ã‚‰â†’ http://example.com
"""

processed_jp_text = jp_preprocessor.preprocess(raw_jp_text)
print(f"å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ: {raw_jp_text}")
print(f"å‰å‡¦ç†å¾Œ: {processed_jp_text}")
# å‡ºåŠ›ä¾‹: "å•†å“ æœ¬å½“ ç´ æ™´ã‚‰ã—ã„ çµ¶å¯¾ ãŠã™ã™ã‚ æœ€é«˜ è©³ã—ã„"
```

---

## 3. ç‰¹å¾´é‡æŠ½å‡º

æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¯æ•°å€¤ã—ã‹æ‰±ãˆã¾ã›ã‚“ã€‚ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

### 3.1 Bag-of-Words (BoW)

**Bag-of-Words** ã¯æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªæ–¹æ³•ã§ã€å„æ–‡æ›¸ã‚’å˜èªã®å‡ºç¾å›æ•°ã§è¡¨ç¾ã—ã¾ã™ã€‚

#### ä»•çµ„ã¿

```
æ–‡æ›¸1: "çŒ«ãŒå¥½ã"
æ–‡æ›¸2: "çŠ¬ãŒå¥½ã"
æ–‡æ›¸3: "çŒ«ã‚‚çŠ¬ã‚‚å¥½ã"

èªå½™: [çŒ«, çŠ¬, å¥½ã, ãŒ, ã‚‚]

BoWè¡¨ç¾:
æ–‡æ›¸1: [1, 0, 1, 1, 0]  # çŒ«:1å›, çŠ¬:0å›, å¥½ã:1å›, ãŒ:1å›, ã‚‚:0å›
æ–‡æ›¸2: [0, 1, 1, 1, 0]
æ–‡æ›¸3: [1, 1, 1, 0, 2]
```

#### å®Ÿè£…

```python
from sklearn.feature_extraction.text import CountVectorizer

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
documents = [
    "æ©Ÿæ¢°å­¦ç¿’ã¯é¢ç™½ã„",
    "æ·±å±¤å­¦ç¿’ã¯é›£ã—ã„",
    "æ©Ÿæ¢°å­¦ç¿’ã¨æ·±å±¤å­¦ç¿’ã¯é–¢é€£ã—ã¦ã„ã‚‹",
    "è‡ªç„¶è¨€èªå‡¦ç†ã‚‚é¢ç™½ã„"
]

# BoWå¤‰æ›
vectorizer = CountVectorizer()
bow_matrix = vectorizer.fit_transform(documents)

# èªå½™ã®ç¢ºèª
print("èªå½™:", vectorizer.get_feature_names_out())
print("\nBoWè¡Œåˆ—:")
print(bow_matrix.toarray())
```

**å‡ºåŠ›:**
```
èªå½™: ['ã¨' 'æ·±å±¤' 'æ©Ÿæ¢°' 'è‡ªç„¶' 'è¨€èª' 'å‡¦ç†' 'é–¢é€£' 'é›£ã—ã„' 'é¢ç™½ã„' 'å­¦ç¿’']

BoWè¡Œåˆ—:
[[0 0 1 0 0 0 0 0 1 1]  # æ©Ÿæ¢°å­¦ç¿’ã¯é¢ç™½ã„
 [0 1 0 0 0 0 0 1 0 1]  # æ·±å±¤å­¦ç¿’ã¯é›£ã—ã„
 [1 1 1 0 0 0 1 0 0 2]  # æ©Ÿæ¢°å­¦ç¿’ã¨æ·±å±¤å­¦ç¿’ã¯é–¢é€£ã—ã¦ã„ã‚‹
 [0 0 0 1 1 1 0 0 1 0]] # è‡ªç„¶è¨€èªå‡¦ç†ã‚‚é¢ç™½ã„
```

#### å•é¡Œç‚¹

- **èªé †ã‚’ç„¡è¦–**: "çŠ¬ãŒçŒ«ã‚’è¿½ã„ã‹ã‘ã‚‹" ã¨ "çŒ«ãŒçŠ¬ã‚’è¿½ã„ã‹ã‘ã‚‹" ãŒåŒã˜
- **æ–‡æ›¸é•·ã®å½±éŸ¿**: é•·ã„æ–‡æ›¸ã»ã©å€¤ãŒå¤§ãããªã‚‹
- **é‡è¦åº¦ã‚’è€ƒæ…®ã—ãªã„**: ã™ã¹ã¦ã®å˜èªãŒåŒç­‰ã«æ‰±ã‚ã‚Œã‚‹

### 3.2 TF-IDF (Term Frequency-Inverse Document Frequency)

**TF-IDF** ã¯ã€å˜èªã®é‡è¦åº¦ã‚’è€ƒæ…®ã—ãŸç‰¹å¾´é‡æŠ½å‡ºæ‰‹æ³•ã§ã™ã€‚

#### è¨ˆç®—å¼

$$
\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t)
$$

- **TF (Term Frequency)**: æ–‡æ›¸å†…ã§ã®å˜èªã®å‡ºç¾é »åº¦
$$
\text{TF}(t, d) = \frac{\text{æ–‡æ›¸}d\text{ã«ãŠã‘ã‚‹å˜èª}t\text{ã®å‡ºç¾å›æ•°}}{\text{æ–‡æ›¸}d\text{ã®ç·å˜èªæ•°}}
$$

- **IDF (Inverse Document Frequency)**: å˜èªã®å¸Œå°‘æ€§
$$
\text{IDF}(t) = \log \frac{\text{ç·æ–‡æ›¸æ•°}}{\text{å˜èª}t\text{ã‚’å«ã‚€æ–‡æ›¸æ•°}}
$$

#### ç›´æ„Ÿçš„ç†è§£

- **é »å‡ºã™ã‚‹å˜èª** (TF ãŒé«˜ã„) â†’ é‡è¦åº¦ãŒé«˜ã„
- **å¤šãã®æ–‡æ›¸ã«å‡ºç¾ã™ã‚‹å˜èª** (IDF ãŒä½ã„) â†’ ã‚ã¾ã‚Šé‡è¦ã§ã¯ãªã„
- ä¾‹: "æ©Ÿæ¢°å­¦ç¿’" ã¯ç‰¹å®šã®æ–‡æ›¸ã«é »å‡º â†’ TF-IDF é«˜ã„
- ä¾‹: "ã§ã™", "ã¾ã™" ã¯å…¨æ–‡æ›¸ã«å‡ºç¾ â†’ TF-IDF ä½ã„

#### å®Ÿè£…

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
documents = [
    "æ©Ÿæ¢°å­¦ç¿’ã¯æ•™å¸«ã‚ã‚Šå­¦ç¿’ã¨æ•™å¸«ãªã—å­¦ç¿’ã«åˆ†é¡ã•ã‚Œã‚‹",
    "æ·±å±¤å­¦ç¿’ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ãŸæ©Ÿæ¢°å­¦ç¿’ã®æ‰‹æ³•",
    "è‡ªç„¶è¨€èªå‡¦ç†ã¯äººé–“ã®è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§å‡¦ç†ã™ã‚‹æŠ€è¡“",
    "ç”»åƒèªè­˜ã«ã¯ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒä½¿ã‚ã‚Œã‚‹"
]

# TF-IDFå¤‰æ›
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(documents)

# èªå½™ã¨ TF-IDF å€¤
feature_names = tfidf_vectorizer.get_feature_names_out()
print("èªå½™:", feature_names)
print("\nTF-IDFè¡Œåˆ—ã®å½¢çŠ¶:", tfidf_matrix.shape)

# æœ€åˆã®æ–‡æ›¸ã® TF-IDF å€¤ã‚’è¡¨ç¤º
doc_index = 0
feature_index = tfidf_matrix[doc_index].nonzero()[1]
tfidf_scores = zip(
    [feature_names[i] for i in feature_index],
    [tfidf_matrix[doc_index, i] for i in feature_index]
)

print(f"\næ–‡æ›¸{doc_index+1}ã®TF-IDFã‚¹ã‚³ã‚¢:")
for word, score in sorted(tfidf_scores, key=lambda x: x[1], reverse=True):
    print(f"  {word}: {score:.4f}")
```

**å‡ºåŠ›ä¾‹:**
```
æ–‡æ›¸1ã®TF-IDFã‚¹ã‚³ã‚¢:
  æ•™å¸«: 0.5774  # ä»–ã®æ–‡æ›¸ã«ã¯ãªã„ â†’ IDFé«˜ã„
  å­¦ç¿’: 0.4082  # è¤‡æ•°æ–‡æ›¸ã«å‡ºç¾ â†’ IDFä¸­ç¨‹åº¦
  åˆ†é¡: 0.4082
  æ©Ÿæ¢°: 0.2887  # 2æ–‡æ›¸ã«å‡ºç¾ â†’ IDFä½ã‚
```

### 3.3 N-gram

**N-gram** ã¯é€£ç¶šã™ã‚‹Nå€‹ã®å˜èªã‚’1ã¤ã®ç‰¹å¾´ã¨ã—ã¦æ‰±ã„ã¾ã™ã€‚

- **Unigram (1-gram)**: "æ©Ÿæ¢°", "å­¦ç¿’"
- **Bigram (2-gram)**: "æ©Ÿæ¢°å­¦ç¿’"
- **Trigram (3-gram)**: "æ©Ÿæ¢°å­¦ç¿’ã®æ‰‹æ³•"

#### å®Ÿè£…

```python
from sklearn.feature_extraction.text import TfidfVectorizer

documents = [
    "æ©Ÿæ¢° å­¦ç¿’ ã¯ é¢ç™½ã„",
    "æ·±å±¤ å­¦ç¿’ ã‚‚ é¢ç™½ã„",
    "æ©Ÿæ¢° å­¦ç¿’ ã¨ æ·±å±¤ å­¦ç¿’"
]

# Bigram + Unigram ã® TF-IDF
vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # 1-gram ã¨ 2-gram
tfidf = vectorizer.fit_transform(documents)

print("ç‰¹å¾´èªï¼ˆUnigram + Bigramï¼‰:")
print(vectorizer.get_feature_names_out())
```

**å‡ºåŠ›:**
```
['ã¨' 'ã¯' 'ã‚‚' 'å­¦ç¿’' 'å­¦ç¿’ ã¨' 'å­¦ç¿’ ã¯' 'å­¦ç¿’ ã‚‚' 'æ©Ÿæ¢°' 'æ©Ÿæ¢° å­¦ç¿’'
 'æ·±å±¤' 'æ·±å±¤ å­¦ç¿’' 'é¢ç™½ã„']
```

---

## 4. æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹åˆ†é¡

### 4.1 Naive Bayes (ãƒŠã‚¤ãƒ¼ãƒ–ãƒ™ã‚¤ã‚º)

Naive Bayes ã¯ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã§æœ€ã‚‚ã‚ˆãä½¿ã‚ã‚Œã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®1ã¤ã§ã™ã€‚

#### ç†è«–

ãƒ™ã‚¤ã‚ºã®å®šç†ã«åŸºã¥ãã€å„ã‚¯ãƒ©ã‚¹ã®ç¢ºç‡ã‚’è¨ˆç®—ï¼š

$$
P(C|D) = \frac{P(D|C) \times P(C)}{P(D)}
$$

- $P(C|D)$: æ–‡æ›¸ $D$ ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã®ã‚¯ãƒ©ã‚¹ $C$ ã®ç¢ºç‡
- $P(D|C)$: ã‚¯ãƒ©ã‚¹ $C$ ã«ãŠã‘ã‚‹æ–‡æ›¸ $D$ ã®å°¤åº¦
- $P(C)$: ã‚¯ãƒ©ã‚¹ $C$ ã®äº‹å‰ç¢ºç‡

#### å®Ÿè£…: ã‚¹ãƒ‘ãƒ ãƒ•ã‚£ãƒ«ã‚¿

```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆå®Ÿéš›ã¯å¤§é‡ã®ãƒ¡ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
emails = [
    "ç„¡æ–™ã§iPhoneãŒå½“ãŸã‚Šã¾ã™ï¼ä»Šã™ãã‚¯ãƒªãƒƒã‚¯ï¼",
    "ä¼šè­°ã®è³‡æ–™ã‚’é€ä»˜ã—ã¾ã™",
    "æ¿€å®‰ã‚»ãƒ¼ãƒ«ï¼æœ¬æ—¥é™å®š50%ã‚ªãƒ•",
    "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®é€²æ—å ±å‘Š",
    "ã‚ãªãŸã«100ä¸‡å††ãŒå½“ãŸã‚Šã¾ã—ãŸ",
    "æ¥é€±ã®äºˆå®šã«ã¤ã„ã¦ç¢ºèªã•ã›ã¦ãã ã•ã„",
    "é™å®šã‚ªãƒ•ã‚¡ãƒ¼ï¼ä»Šã™ãç™»éŒ²",
    "è­°äº‹éŒ²ã‚’å…±æœ‰ã—ã¾ã™"
]

labels = [1, 0, 1, 0, 1, 0, 1, 0]  # 1: ã‚¹ãƒ‘ãƒ , 0: æ­£å¸¸

# å‰å‡¦ç†ï¼ˆå®Ÿéš›ã¯æ—¥æœ¬èªå½¢æ…‹ç´ è§£æã‚’ä½¿ç”¨ï¼‰
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features=100)
X = vectorizer.fit_transform(emails)
y = labels

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42
)

# Naive Bayes ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)

# äºˆæ¸¬
y_pred = nb_model.predict(X_test)

# è©•ä¾¡
print("åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_test, y_pred,
                          target_names=['æ­£å¸¸ãƒ¡ãƒ¼ãƒ«', 'ã‚¹ãƒ‘ãƒ ']))

print("\næ··åŒè¡Œåˆ—:")
print(confusion_matrix(y_test, y_pred))

# æ–°ã—ã„ãƒ¡ãƒ¼ãƒ«ã®äºˆæ¸¬
new_emails = [
    "ä¼šè­°å®¤ã®äºˆç´„ã‚’ãŠé¡˜ã„ã—ã¾ã™",
    "ä»Šã™ãã‚¯ãƒªãƒƒã‚¯ï¼å¤§é‡‘ãŒæ‰‹ã«å…¥ã‚Šã¾ã™"
]
new_emails_vec = vectorizer.transform(new_emails)
predictions = nb_model.predict(new_emails_vec)

for email, pred in zip(new_emails, predictions):
    label = "ã‚¹ãƒ‘ãƒ " if pred == 1 else "æ­£å¸¸"
    print(f"'{email}' â†’ {label}")
```

### 4.2 Support Vector Machine (SVM)

SVM ã¯é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã§ã‚‚åŠ¹æœçš„ãªåˆ†é¡ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚

#### å®Ÿè£…: æ„Ÿæƒ…åˆ†æ

```python
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer

# ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿
reviews = [
    "ã“ã®æ˜ ç”»ã¯æœ€é«˜ã§ã—ãŸï¼æ„Ÿå‹•ã—ã¾ã—ãŸ",
    "ã¤ã¾ã‚‰ãªã„æ˜ ç”»ã€‚æ™‚é–“ã®ç„¡é§„",
    "ç´ æ™´ã‚‰ã—ã„æ¼”æŠ€ã¨ç¾ã—ã„æ˜ åƒ",
    "é€€å±ˆã§çœ ããªã£ãŸ",
    "æœŸå¾…ä»¥ä¸Šã®ä½œå“ï¼ã¾ãŸè¦³ãŸã„",
    "ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãŒè–„ãã€ãŒã£ã‹ã‚Š",
    "åœ§å·»ã®ãƒ©ã‚¹ãƒˆã‚·ãƒ¼ãƒ³",
    "äºŒåº¦ã¨è¦³ãŸããªã„"
]

sentiments = [1, 0, 1, 0, 1, 0, 1, 0]  # 1: Positive, 0: Negative

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ï¼ˆå‰å‡¦ç† â†’ ç‰¹å¾´æŠ½å‡º â†’ ãƒ¢ãƒ‡ãƒ«ï¼‰
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=100)),
    ('svm', LinearSVC(random_state=42))
])

# è¨“ç·´
X_train, X_test, y_train, y_test = train_test_split(
    reviews, sentiments, test_size=0.25, random_state=42
)

pipeline.fit(X_train, y_train)

# è©•ä¾¡
y_pred = pipeline.predict(X_test)
print("æ­£è§£ç‡:", pipeline.score(X_test, y_test))

# æ–°ã—ã„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®æ„Ÿæƒ…äºˆæ¸¬
new_reviews = [
    "æœŸå¾…å¤–ã‚Œã®ä½œå“",
    "æ„Ÿå‹•çš„ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼"
]

for review in new_reviews:
    sentiment = pipeline.predict([review])[0]
    label = "Positive" if sentiment == 1 else "Negative"
    print(f"'{review}' â†’ {label}")
```

### 4.3 å®Œå…¨ãªåˆ†é¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ä½¿ãˆã‚‹å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¾‹ï¼š

```python
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
import joblib

class TextClassifier:
    """ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡å™¨ã‚¯ãƒ©ã‚¹"""

    def __init__(self, model_type='nb'):
        """
        Parameters:
        -----------
        model_type : str
            'nb': Naive Bayes
            'svm': Support Vector Machine
            'lr': Logistic Regression
            'rf': Random Forest
        """
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.8
        )

        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        if model_type == 'nb':
            self.model = MultinomialNB()
        elif model_type == 'svm':
            self.model = LinearSVC(random_state=42)
        elif model_type == 'lr':
            self.model = LogisticRegression(random_state=42, max_iter=1000)
        elif model_type == 'rf':
            self.model = RandomForestClassifier(random_state=42, n_estimators=100)
        else:
            raise ValueError(f"Unknown model type: {model_type}")

    def train(self, X_train, y_train):
        """ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
        # TF-IDFå¤‰æ›
        X_train_vec = self.vectorizer.fit_transform(X_train)

        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        self.model.fit(X_train_vec, y_train)

    def predict(self, X_test):
        """äºˆæ¸¬"""
        X_test_vec = self.vectorizer.transform(X_test)
        return self.model.predict(X_test_vec)

    def evaluate(self, X_test, y_test):
        """è©•ä¾¡"""
        y_pred = self.predict(X_test)

        print("æ­£è§£ç‡:", accuracy_score(y_test, y_pred))
        print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
        print(classification_report(y_test, y_pred))

        return y_pred

    def save(self, filepath):
        """ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        joblib.dump((self.vectorizer, self.model), filepath)

    def load(self, filepath):
        """ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        self.vectorizer, self.model = joblib.load(filepath)

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆå®Ÿéš›ã¯CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
    texts = [
        "ç´ æ™´ã‚‰ã—ã„å•†å“ã§ã™",
        "æœ€æ‚ªã®å“è³ª",
        "æœŸå¾…é€šã‚Šã®æ€§èƒ½",
        "äºŒåº¦ã¨è²·ã„ã¾ã›ã‚“",
        # ... å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿
    ]
    labels = [1, 0, 1, 0]  # 1: Positive, 0: Negative

    # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        texts, labels, test_size=0.2, random_state=42, stratify=labels
    )

    # åˆ†é¡å™¨ã®è¨“ç·´
    classifier = TextClassifier(model_type='svm')
    classifier.train(X_train, y_train)

    # è©•ä¾¡
    y_pred = classifier.evaluate(X_test, y_test)

    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    classifier.save('text_classifier.pkl')

    # æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã®äºˆæ¸¬
    new_texts = ["ã“ã®å•†å“ã¯æœ€é«˜ã§ã™", "ãŒã£ã‹ã‚Šã—ã¾ã—ãŸ"]
    predictions = classifier.predict(new_texts)
    print("\næ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã®äºˆæ¸¬:")
    for text, pred in zip(new_texts, predictions):
        print(f"'{text}' â†’ {'Positive' if pred == 1 else 'Negative'}")
```

---

## 5. ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡

### 5.1 è©•ä¾¡æŒ‡æ¨™

#### æ­£è§£ç‡ (Accuracy)

$$
\text{Accuracy} = \frac{\text{æ­£ã—ãåˆ†é¡ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«æ•°}}{\text{å…¨ã‚µãƒ³ãƒ—ãƒ«æ•°}}
$$

- **ä½¿ã„ã©ã“ã‚**: ã‚¯ãƒ©ã‚¹ã®ãƒãƒ©ãƒ³ã‚¹ãŒå–ã‚Œã¦ã„ã‚‹å ´åˆ
- **æ³¨æ„ç‚¹**: ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã§ã¯ä¸é©åˆ‡

#### é©åˆç‡ (Precision)

$$
\text{Precision} = \frac{TP}{TP + FP}
$$

- **æ„å‘³**: ã€ŒPositive ã¨äºˆæ¸¬ã—ãŸã‚‚ã®ã®ã†ã¡ã€å®Ÿéš›ã« Positive ã ã£ãŸå‰²åˆã€
- **ä½¿ã„ã©ã“ã‚**: å½é™½æ€§ã‚’é¿ã‘ãŸã„å ´åˆï¼ˆã‚¹ãƒ‘ãƒ ãƒ•ã‚£ãƒ«ã‚¿ãªã©ï¼‰

#### å†ç¾ç‡ (Recall)

$$
\text{Recall} = \frac{TP}{TP + FN}
$$

- **æ„å‘³**: ã€Œå®Ÿéš›ã« Positive ã®ã‚‚ã®ã®ã†ã¡ã€æ­£ã—ã Positive ã¨äºˆæ¸¬ã§ããŸå‰²åˆã€
- **ä½¿ã„ã©ã“ã‚**: å½é™°æ€§ã‚’é¿ã‘ãŸã„å ´åˆï¼ˆç—…æ°—è¨ºæ–­ãªã©ï¼‰

#### F1ã‚¹ã‚³ã‚¢

$$
F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$

- **æ„å‘³**: Precision ã¨ Recall ã®èª¿å’Œå¹³å‡
- **ä½¿ã„ã©ã“ã‚**: ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸè©•ä¾¡ãŒå¿…è¦ãªå ´åˆ

### 5.2 æ··åŒè¡Œåˆ— (Confusion Matrix)

```python
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# äºˆæ¸¬çµæœï¼ˆä¾‹ï¼‰
y_true = [0, 1, 0, 1, 0, 1, 1, 0]
y_pred = [0, 1, 0, 0, 0, 1, 1, 1]

# æ··åŒè¡Œåˆ—
cm = confusion_matrix(y_true, y_pred)

# å¯è¦–åŒ–
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'])
plt.ylabel('å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«')
plt.xlabel('äºˆæ¸¬ãƒ©ãƒ™ãƒ«')
plt.title('æ··åŒè¡Œåˆ—')
plt.show()

print("æ··åŒè¡Œåˆ—:")
print(cm)
print(f"\nTrue Negative: {cm[0,0]}")
print(f"False Positive: {cm[0,1]}")
print(f"False Negative: {cm[1,0]}")
print(f"True Positive: {cm[1,1]}")
```

### 5.3 äº¤å·®æ¤œè¨¼ (Cross-Validation)

ãƒ‡ãƒ¼ã‚¿ã‚’è¤‡æ•°ã® fold ã«åˆ†ã‘ã€ã‚ˆã‚Šä¿¡é ¼æ€§ã®é«˜ã„è©•ä¾¡ã‚’è¡Œã„ã¾ã™ã€‚

```python
from sklearn.model_selection import cross_val_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=1000)),
    ('nb', MultinomialNB())
])

# 5-fold äº¤å·®æ¤œè¨¼
scores = cross_val_score(
    pipeline,
    texts,  # å…¨ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    labels,  # å…¨ãƒ©ãƒ™ãƒ«
    cv=5,  # 5åˆ†å‰²
    scoring='f1'
)

print(f"å„foldã®F1ã‚¹ã‚³ã‚¢: {scores}")
print(f"å¹³å‡F1ã‚¹ã‚³ã‚¢: {scores.mean():.4f} (+/- {scores.std():.4f})")
```

---

## 6. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### 6.1 Grid Search

```python
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('svm', LinearSVC())
])

# ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
param_grid = {
    'tfidf__max_features': [1000, 3000, 5000],
    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'tfidf__min_df': [1, 2, 5],
    'svm__C': [0.1, 1, 10]
}

# Grid Search
grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=5,
    scoring='f1',
    n_jobs=-1,  # ä¸¦åˆ—å‡¦ç†
    verbose=1
)

grid_search.fit(X_train, y_train)

# æœ€è‰¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
print("æœ€è‰¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
print(grid_search.best_params_)
print(f"\næœ€è‰¯ã®F1ã‚¹ã‚³ã‚¢: {grid_search.best_score_:.4f}")

# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
```

---

## 7. å®Ÿè·µä¾‹: æ—¥æœ¬èªæ„Ÿæƒ…åˆ†æã‚·ã‚¹ãƒ†ãƒ 

### 7.1 å®Œå…¨ãªå®Ÿè£…

```python
import pandas as pd
import MeCab
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report
import pickle

class JapaneseSentimentAnalyzer:
    """æ—¥æœ¬èªæ„Ÿæƒ…åˆ†æã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.mecab = MeCab.Tagger()
        self.vectorizer = None
        self.model = None

    def tokenize(self, text):
        """MeCabã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–"""
        node = self.mecab.parseToNode(text)
        tokens = []

        while node:
            if node.surface:
                features = node.feature.split(',')
                pos = features[0]

                # åè©ã€å‹•è©ã€å½¢å®¹è©ã®ã¿æŠ½å‡º
                if pos in ['åè©', 'å‹•è©', 'å½¢å®¹è©']:
                    base_form = features[6] if len(features) > 6 else node.surface
                    if base_form != '*':
                        tokens.append(base_form)

            node = node.next

        return ' '.join(tokens)

    def prepare_data(self, texts):
        """ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†"""
        return [self.tokenize(text) for text in texts]

    def train(self, texts, labels):
        """ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        tokenized_texts = self.prepare_data(texts)

        # TF-IDF ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        self.vectorizer = TfidfVectorizer(
            max_features=3000,
            ngram_range=(1, 2),
            min_df=2
        )
        X = self.vectorizer.fit_transform(tokenized_texts)

        # SVM è¨“ç·´
        self.model = LinearSVC(C=1.0, random_state=42)
        self.model.fit(X, labels)

    def predict(self, texts):
        """æ„Ÿæƒ…äºˆæ¸¬"""
        tokenized_texts = self.prepare_data(texts)
        X = self.vectorizer.transform(tokenized_texts)
        return self.model.predict(X)

    def predict_proba(self, texts):
        """ç¢ºç‡ä»˜ãäºˆæ¸¬ï¼ˆSVMã®æ±ºå®šé–¢æ•°ã‚’ä½¿ç”¨ï¼‰"""
        tokenized_texts = self.prepare_data(texts)
        X = self.vectorizer.transform(tokenized_texts)
        decision = self.model.decision_function(X)
        return decision

    def save(self, filepath):
        """ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        with open(filepath, 'wb') as f:
            pickle.dump((self.vectorizer, self.model), f)

    def load(self, filepath):
        """ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        with open(filepath, 'rb') as f:
            self.vectorizer, self.model = pickle.load(f)

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
if __name__ == "__main__":
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã¯å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ï¼‰
    reviews = [
        "ã“ã®å•†å“ã¯æœ¬å½“ã«ç´ æ™´ã‚‰ã—ã„ã€‚æœŸå¾…ä»¥ä¸Šã§ã—ãŸ",
        "æœ€æ‚ªã®å“è³ªã€‚äºŒåº¦ã¨è²·ã„ã¾ã›ã‚“",
        "å€¤æ®µã®å‰²ã«ã¯è‰¯ã„å•†å“ã ã¨æ€ã„ã¾ã™",
        "é…é€ãŒé…ãã€å•†å“ã‚‚å‚·ã ã‚‰ã‘",
        "ä½¿ã„ã‚„ã™ãã¦æº€è¶³ã—ã¦ã„ã¾ã™",
        "èª¬æ˜ã¨é•ã†å•†å“ãŒå±Šã„ãŸ",
        "ã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒé«˜ã„",
        "æœŸå¾…å¤–ã‚Œã§ãŒã£ã‹ã‚Š",
        "ãƒ‡ã‚¶ã‚¤ãƒ³ãŒç¾ã—ãã€æ©Ÿèƒ½ã‚‚å……å®Ÿ",
        "å£Šã‚Œã‚„ã™ãã€ã™ãã«æ•…éšœã—ãŸ"
    ]

    sentiments = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1: Positive, 0: Negative

    # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        reviews, sentiments, test_size=0.2, random_state=42
    )

    # åˆ†æå™¨ã®è¨“ç·´
    analyzer = JapaneseSentimentAnalyzer()
    analyzer.train(X_train, y_train)

    # è©•ä¾¡
    y_pred = analyzer.predict(X_test)
    print("åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
    print(classification_report(y_test, y_pred,
                              target_names=['Negative', 'Positive']))

    # æ–°ã—ã„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®æ„Ÿæƒ…åˆ†æ
    new_reviews = [
        "ã¨ã¦ã‚‚è‰¯ã„è²·ã„ç‰©ãŒã§ãã¾ã—ãŸ",
        "ãŠé‡‘ã‚’ç„¡é§„ã«ã—ãŸ",
        "æ™®é€šã®å•†å“ã§ã™"
    ]

    predictions = analyzer.predict(new_reviews)
    scores = analyzer.predict_proba(new_reviews)

    print("\næ–°ã—ã„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®æ„Ÿæƒ…åˆ†æ:")
    for review, pred, score in zip(new_reviews, predictions, scores):
        sentiment = "Positive" if pred == 1 else "Negative"
        confidence = abs(score)
        print(f"'{review}'")
        print(f"  â†’ {sentiment} (ä¿¡é ¼åº¦: {confidence:.2f})\n")

    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    analyzer.save('sentiment_model.pkl')
```

---

## 8. ã¾ã¨ã‚

### 8.1 æœ¬ç« ã§å­¦ã‚“ã ã“ã¨

âœ… **ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†**
- ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã€æ­£è¦åŒ–ã®å®Ÿè£…
- æ—¥æœ¬èªã¨è‹±èªã®å‰å‡¦ç†ã®é•ã„

âœ… **ç‰¹å¾´é‡æŠ½å‡º**
- Bag-of-Words (BoW) ã®ä»•çµ„ã¿
- TF-IDF ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
- N-gram ã®æ´»ç”¨

âœ… **æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«**
- Naive Bayes ã«ã‚ˆã‚‹åˆ†é¡
- SVM ã«ã‚ˆã‚‹é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿åˆ†é¡
- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰

âœ… **ãƒ¢ãƒ‡ãƒ«è©•ä¾¡**
- æ­£è§£ç‡ã€é©åˆç‡ã€å†ç¾ç‡ã€F1ã‚¹ã‚³ã‚¢
- æ··åŒè¡Œåˆ—ã®èª­ã¿æ–¹
- äº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹ä¿¡é ¼æ€§å‘ä¸Š

âœ… **å®Ÿè·µã‚·ã‚¹ãƒ†ãƒ **
- æ—¥æœ¬èªæ„Ÿæƒ…åˆ†æã‚·ã‚¹ãƒ†ãƒ ã®å®Œå…¨å®Ÿè£…

### 8.2 æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

æœ¬ç« ã§åŸºç¤çš„ãªãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚’å­¦ã³ã¾ã—ãŸã€‚ã•ã‚‰ã«ç™ºå±•ã•ã›ã‚‹ã«ã¯ï¼š

1. **æ·±å±¤å­¦ç¿’ã®æ´»ç”¨**: LSTMã€Transformer ãªã©ã®ãƒ¢ãƒ‡ãƒ«
2. **å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: å®Ÿéš›ã®ãƒ“ã‚¸ãƒã‚¹ãƒ‡ãƒ¼ã‚¿ã§ã®å®Ÿé¨“
3. **å¤šã‚¯ãƒ©ã‚¹åˆ†é¡**: 3ã¤ä»¥ä¸Šã®ã‚«ãƒ†ã‚´ãƒªã¸ã®æ‹¡å¼µ
4. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†é¡**: Web API ã¨ã—ã¦ãƒ‡ãƒ—ãƒ­ã‚¤

---

## 9. ç·´ç¿’å•é¡Œ

### å•é¡Œ1: å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆåŸºç¤ï¼‰

æ¬¡ã®ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦ã€é©åˆ‡ãªå‰å‡¦ç†ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚

```python
text = """
ã€é€Ÿå ±ã€‘æ–°è£½å“ç™ºå£²ï¼ï¼ï¼ğŸ‰
ä»Šãªã‚‰50%OFF â†’ http://example.com
#ã‚»ãƒ¼ãƒ« #ãŠå¾— @å…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
"""

# ã‚ãªãŸã®å®Ÿè£…:
def preprocess(text):
    # ã“ã“ã«ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã
    pass

result = preprocess(text)
print(result)
```

<details>
<summary>è§£ç­”ä¾‹</summary>

```python
import re
import unicodedata

def preprocess(text):
    # Unicodeæ­£è¦åŒ–
    text = unicodedata.normalize('NFKC', text)

    # URLé™¤å»
    text = re.sub(r'http\S+', '', text)

    # ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°é™¤å»
    text = re.sub(r'[@#]\S+', '', text)

    # çµµæ–‡å­—é™¤å»
    text = re.sub(r'[^\w\s]', '', text, flags=re.UNICODE)

    # ä½™åˆ†ãªç©ºç™½é™¤å»
    text = ' '.join(text.split())

    return text

result = preprocess(text)
print(result)  # å‡ºåŠ›: "é€Ÿå ± æ–°è£½å“ç™ºå£² ä»Šãªã‚‰50OFF"
```

</details>

### å•é¡Œ2: TF-IDF å®Ÿè£…ï¼ˆä¸­ç´šï¼‰

ä»¥ä¸‹ã®æ–‡æ›¸é›†åˆã«å¯¾ã—ã¦ã€TF-IDF ã‚’è¨ˆç®—ã—ã€å„æ–‡æ›¸ã§æœ€ã‚‚é‡è¦ãªå˜èªãƒˆãƒƒãƒ—3ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

```python
documents = [
    "æ©Ÿæ¢°å­¦ç¿’ã¯äººå·¥çŸ¥èƒ½ã®ä¸€åˆ†é‡",
    "æ·±å±¤å­¦ç¿’ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã†",
    "è‡ªç„¶è¨€èªå‡¦ç†ã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’æ‰±ã†äººå·¥çŸ¥èƒ½æŠ€è¡“",
    "ç”»åƒèªè­˜ã¯æ·±å±¤å­¦ç¿’ã®å¿œç”¨ä¾‹"
]

# ã‚ãªãŸã®å®Ÿè£…
```

<details>
<summary>è§£ç­”ä¾‹</summary>

```python
from sklearn.feature_extraction.text import TfidfVectorizer

documents = [
    "æ©Ÿæ¢°å­¦ç¿’ã¯äººå·¥çŸ¥èƒ½ã®ä¸€åˆ†é‡",
    "æ·±å±¤å­¦ç¿’ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã†",
    "è‡ªç„¶è¨€èªå‡¦ç†ã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’æ‰±ã†äººå·¥çŸ¥èƒ½æŠ€è¡“",
    "ç”»åƒèªè­˜ã¯æ·±å±¤å­¦ç¿’ã®å¿œç”¨ä¾‹"
]

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)
feature_names = vectorizer.get_feature_names_out()

for doc_idx in range(len(documents)):
    print(f"\næ–‡æ›¸{doc_idx+1}: {documents[doc_idx]}")

    # å„æ–‡æ›¸ã®TF-IDFã‚¹ã‚³ã‚¢ã‚’å–å¾—
    feature_index = tfidf_matrix[doc_idx].nonzero()[1]
    tfidf_scores = [(feature_names[i], tfidf_matrix[doc_idx, i])
                    for i in feature_index]

    # ã‚¹ã‚³ã‚¢é™é †ã§ã‚½ãƒ¼ãƒˆ
    tfidf_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)

    print("  é‡è¦å˜èªãƒˆãƒƒãƒ—3:")
    for word, score in tfidf_scores[:3]:
        print(f"    {word}: {score:.4f}")
```

</details>

### å•é¡Œ3: æ„Ÿæƒ…åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå¿œç”¨ï¼‰

ä»¥ä¸‹ã®ä»•æ§˜ã§æ„Ÿæƒ…åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š

**ä»•æ§˜:**
- å…¥åŠ›: æ—¥æœ¬èªãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
- å‡ºåŠ›: Positive/Negative + ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
- ãƒ¢ãƒ‡ãƒ«: Naive Bayes ã¾ãŸã¯ SVM
- è©•ä¾¡æŒ‡æ¨™: F1ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º

<details>
<summary>è§£ç­”ä¾‹</summary>

```python
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, f1_score
import numpy as np

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
reviews = [
    "ç´ æ™´ã‚‰ã—ã„å•†å“ã§ã™", "æœ€æ‚ªã®å“è³ª", "æœŸå¾…é€šã‚Š",
    "ãŒã£ã‹ã‚Š", "å¤§æº€è¶³", "äºŒåº¦ã¨è²·ã‚ãªã„",
    "ãŠã™ã™ã‚ã§ã™", "ä¾¡å€¤ãŒãªã„"
]
labels = [1, 0, 1, 0, 1, 0, 1, 0]

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    reviews, labels, test_size=0.25, random_state=42
)

# TF-IDF + Naive Bayes
vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

model = MultinomialNB()
model.fit(X_train_vec, y_train)

# äºˆæ¸¬ã¨è©•ä¾¡
y_pred = model.predict(X_test_vec)
f1 = f1_score(y_test, y_pred)

print(f"F1ã‚¹ã‚³ã‚¢: {f1:.4f}")
print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_test, y_pred,
                          target_names=['Negative', 'Positive']))

# æ–°ã—ã„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®äºˆæ¸¬
new_review = ["ã“ã®å•†å“ã¯æœ€é«˜ã§ã™"]
new_vec = vectorizer.transform(new_review)
prediction = model.predict(new_vec)[0]
proba = model.predict_proba(new_vec)[0]

sentiment = "Positive" if prediction == 1 else "Negative"
confidence = proba[prediction]

print(f"\n'{new_review[0]}'")
print(f"â†’ {sentiment} (ä¿¡é ¼åº¦: {confidence:.2f})")
```

</details>

---

## 10. å‚è€ƒæ–‡çŒ®

### æ›¸ç±
1. ã€Œå…¥é–€ è‡ªç„¶è¨€èªå‡¦ç†ã€Steven Bird ä»–ï¼ˆã‚ªãƒ©ã‚¤ãƒªãƒ¼ãƒ»ã‚¸ãƒ£ãƒ‘ãƒ³ï¼‰
2. ã€Œscikit-learn ã¨ TensorFlow ã«ã‚ˆã‚‹å®Ÿè·µæ©Ÿæ¢°å­¦ç¿’ã€AurÃ©lien GÃ©ron
3. ã€ŒPythonã§ã¯ã˜ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹å…¥é–€ã€ä½è—¤ æ•ç´€

### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹
- [scikit-learn Text Feature Extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)
- [Kaggle: Sentiment Analysis Tutorial](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews)
- [æ—¥æœ¬èªè‡ªç„¶è¨€èªå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¾ã¨ã‚](https://qiita.com/Hironsan/items/2466fe0f344115aff177)

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- [livedoor ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹](https://www.rondhuit.com/download.html) - æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡
- [æ—¥æœ¬èªè©•åˆ¤åˆ†æãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ](http://www.db.info.gifu-u.ac.jp/data/Data_5d832973308d57446583ed9f) - æ„Ÿæƒ…åˆ†æ
- [IMDB Movie Reviews](https://ai.stanford.edu/~amaas/data/sentiment/) - è‹±èªæ„Ÿæƒ…åˆ†æ

---

**æ¬¡ã¸**: [Chapter 4: å®Ÿä¸–ç•Œã®NLPå¿œç”¨ â†’](chapter-4.html)

**å‰ã¸**: [â† Chapter 2: å½¢æ…‹ç´ è§£æãƒ»æ§‹æ–‡è§£æ](chapter-2.html)

**ç›®æ¬¡ã¸**: [â†‘ ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡](index.html)
