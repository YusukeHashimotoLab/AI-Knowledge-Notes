<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬5ç« ï¼šNLPå¿œç”¨å®Ÿè·µ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬5ç« ï¼šNLPå¿œç”¨å®Ÿè·µ</h1>
            <p class="subtitle">æ„Ÿæƒ…åˆ†æã‹ã‚‰è³ªå•å¿œç­”ã¾ã§ - å®Ÿä¸–ç•Œã®NLPã‚¿ã‚¹ã‚¯å®Œå…¨å®Ÿè£…</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 35-40åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 10å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… æ„Ÿæƒ…åˆ†æï¼ˆSentiment Analysisï¼‰ã®å®Ÿè£…ã¨è©•ä¾¡</li>
<li>âœ… å›ºæœ‰è¡¨ç¾èªè­˜ï¼ˆNERï¼‰ã§ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æŠ½å‡ºã§ãã‚‹</li>
<li>âœ… è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ï¼ˆQAï¼‰ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
<li>âœ… ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ï¼ˆSummarizationï¼‰ã®å®Ÿè£…æ–¹æ³•ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®NLPãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
<li>âœ… æœ¬ç•ªç’°å¢ƒã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•ã‚’ç¿’å¾—ã™ã‚‹</li>
</ul>

<hr>

<h2>5.1 æ„Ÿæƒ…åˆ†æï¼ˆSentiment Analysisï¼‰</h2>

<h3>æ„Ÿæƒ…åˆ†æã¨ã¯</h3>
<p><strong>æ„Ÿæƒ…åˆ†æï¼ˆSentiment Analysisï¼‰</strong>ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è‘—è€…ã®æ„è¦‹ã‚„æ„Ÿæƒ…ï¼ˆè‚¯å®šçš„ãƒ»å¦å®šçš„ãƒ»ä¸­ç«‹çš„ï¼‰ã‚’åˆ¤å®šã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚</p>

<blockquote>
<p>å¿œç”¨ä¾‹ï¼šè£½å“ãƒ¬ãƒ“ãƒ¥ãƒ¼åˆ†æã€SNSç›£è¦–ã€ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã€ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°</p>
</blockquote>

<h3>æ„Ÿæƒ…åˆ†æã®ã‚¿ã‚¤ãƒ—</h3>

<table>
<thead>
<tr>
<th>ã‚¿ã‚¤ãƒ—</th>
<th>èª¬æ˜</th>
<th>ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Binary Classification</strong></td>
<td>è‚¯å®š/å¦å®šã®2ã‚¯ãƒ©ã‚¹åˆ†é¡</td>
<td>ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒå¥½æ„çš„ã‹å¦å®šçš„ã‹</td>
</tr>
<tr>
<td><strong>Multi-class Classification</strong></td>
<td>è¤‡æ•°ã®æ„Ÿæƒ…ã‚«ãƒ†ã‚´ãƒª</td>
<td>Very Negative, Negative, Neutral, Positive, Very Positive</td>
</tr>
<tr>
<td><strong>Aspect-based Sentiment</strong></td>
<td>ç‰¹å®šã®å´é¢ã«å¯¾ã™ã‚‹æ„Ÿæƒ…</td>
<td>ã€Œæ–™ç†ã¯ç¾å‘³ã—ã„ãŒã‚µãƒ¼ãƒ“ã‚¹ãŒæ‚ªã„ã€â†’æ–™ç†:è‚¯å®šã€ã‚µãƒ¼ãƒ“ã‚¹:å¦å®š</td>
</tr>
<tr>
<td><strong>Emotion Detection</strong></td>
<td>æ„Ÿæƒ…ã®ç¨®é¡ã‚’æ¤œå‡º</td>
<td>å–œã³ã€æ€’ã‚Šã€æ‚²ã—ã¿ã€æã‚Œã€é©šã</td>
</tr>
</tbody>
</table>

<h3>Binaryæ„Ÿæƒ…åˆ†æã®å®Ÿè£…</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæ˜ ç”»ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰
reviews = [
    "This movie is absolutely fantastic! I loved every minute.",
    "Terrible film, waste of time and money.",
    "An amazing masterpiece with brilliant acting.",
    "Boring and predictable. Would not recommend.",
    "One of the best movies I've ever seen!",
    "Awful story, poor direction, disappointing overall.",
    "Great cinematography and compelling narrative.",
    "Not worth watching. Very disappointing.",
    "Excellent performances by all actors!",
    "Dull and uninspiring. Fell asleep halfway through.",
    "A true work of art! Highly recommended!",
    "Complete disaster. Avoid at all costs.",
    "Wonderful film with a heartwarming message.",
    "Poorly executed and hard to follow.",
    "Outstanding! A must-see for everyone.",
    "Waste of time. Very poor quality.",
    "Beautiful story and great music.",
    "Terrible acting and weak plot.",
    "Phenomenal! Best movie this year!",
    "Boring and overrated. Not impressed."
]

# ãƒ©ãƒ™ãƒ«ï¼ˆ1: Positive, 0: Negativeï¼‰
labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
          1, 0, 1, 0, 1, 0, 1, 0, 1, 0]

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
df = pd.DataFrame({'review': reviews, 'sentiment': labels})

print("=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ===")
print(df.head(10))
print(f"\nãƒ‡ãƒ¼ã‚¿æ•°: {len(df)}")
print(f"Positive: {sum(labels)}, Negative: {len(labels) - sum(labels)}")

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    df['review'], df['sentiment'],
    test_size=0.3, random_state=42, stratify=df['sentiment']
)

# TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(X_train_tfidf, y_train)

# äºˆæ¸¬ã¨è©•ä¾¡
y_pred = model.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)

print("\n=== ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ ===")
print(f"Accuracy: {accuracy:.3f}")
print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_test, y_pred,
                           target_names=['Negative', 'Positive']))

# æ··åŒè¡Œåˆ—
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Sentiment Analysis')
plt.tight_layout()
plt.show()

# æ–°ã—ã„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®äºˆæ¸¬
new_reviews = [
    "This is an incredible movie!",
    "What a terrible waste of time.",
    "Pretty good, I enjoyed it."
]

new_tfidf = vectorizer.transform(new_reviews)
predictions = model.predict(new_tfidf)
probabilities = model.predict_proba(new_tfidf)

print("\n=== æ–°ã—ã„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®äºˆæ¸¬ ===")
for review, pred, prob in zip(new_reviews, predictions, probabilities):
    sentiment = "Positive" if pred == 1 else "Negative"
    confidence = prob[pred]
    print(f"Review: {review}")
    print(f"  â†’ {sentiment} (confidence: {confidence:.2%})\n")
</code></pre>

<h3>BERTã«ã‚ˆã‚‹æ„Ÿæƒ…åˆ†æ</h3>

<pre><code class="language-python">from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import pipeline
import torch

# äº‹å‰å­¦ç¿’æ¸ˆã¿BERTæ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ
sentiment_pipeline = pipeline(
    "sentiment-analysis",
    model=model_name,
    tokenizer=model_name
)

# ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆè‹±èªã¨æ—¥æœ¬èªï¼‰
reviews = [
    "This product is absolutely amazing! Best purchase ever!",
    "Terrible quality. Very disappointed with this item.",
    "ã“ã®å•†å“ã¯ç´ æ™´ã‚‰ã—ã„ã§ã™ï¼ã¨ã¦ã‚‚æº€è¶³ã—ã¦ã„ã¾ã™ã€‚",
    "æœ€æ‚ªã®å“è³ªã§ã™ã€‚ãŒã£ã‹ã‚Šã—ã¾ã—ãŸã€‚",
    "It's okay. Nothing special but does the job."
]

print("=== BERTæ„Ÿæƒ…åˆ†æ ===\n")
for review in reviews:
    result = sentiment_pipeline(review)[0]
    stars = int(result['label'].split()[0])
    confidence = result['score']

    print(f"Review: {review}")
    print(f"  â†’ Rating: {stars} stars (confidence: {confidence:.2%})")
    print(f"  â†’ Sentiment: {'Positive' if stars >= 4 else 'Negative' if stars <= 2 else 'Neutral'}\n")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== BERTæ„Ÿæƒ…åˆ†æ ===

Review: This product is absolutely amazing! Best purchase ever!
  â†’ Rating: 5 stars (confidence: 87.34%)
  â†’ Sentiment: Positive

Review: Terrible quality. Very disappointed with this item.
  â†’ Rating: 1 stars (confidence: 92.15%)
  â†’ Sentiment: Negative

Review: ã“ã®å•†å“ã¯ç´ æ™´ã‚‰ã—ã„ã§ã™ï¼ã¨ã¦ã‚‚æº€è¶³ã—ã¦ã„ã¾ã™ã€‚
  â†’ Rating: 5 stars (confidence: 78.92%)
  â†’ Sentiment: Positive

Review: æœ€æ‚ªã®å“è³ªã§ã™ã€‚ãŒã£ã‹ã‚Šã—ã¾ã—ãŸã€‚
  â†’ Rating: 1 stars (confidence: 85.67%)
  â†’ Sentiment: Negative

Review: It's okay. Nothing special but does the job.
  â†’ Rating: 3 stars (confidence: 65.43%)
  â†’ Sentiment: Neutral
</code></pre>

<h3>Aspect-basedæ„Ÿæƒ…åˆ†æ</h3>

<pre><code class="language-python">import spacy
from transformers import pipeline

# ABSAï¼ˆAspect-Based Sentiment Analysisï¼‰ã®å®Ÿè£…
class AspectBasedSentimentAnalyzer:
    def __init__(self):
        # æ„Ÿæƒ…åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        self.sentiment_analyzer = pipeline(
            "sentiment-analysis",
            model="nlptown/bert-base-multilingual-uncased-sentiment"
        )
        # åè©å¥æŠ½å‡ºç”¨ï¼ˆaspectã®å€™è£œï¼‰
        self.nlp = spacy.load("en_core_web_sm")

    def extract_aspects(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰aspectå€™è£œã‚’æŠ½å‡º"""
        doc = self.nlp(text)
        aspects = []

        # åè©ã¨å½¢å®¹è©ã®çµ„ã¿åˆã‚ã›ã‚’æŠ½å‡º
        for chunk in doc.noun_chunks:
            aspects.append(chunk.text)

        return aspects

    def analyze_aspect_sentiment(self, text, aspect):
        """ç‰¹å®šaspectã«å¯¾ã™ã‚‹æ„Ÿæƒ…ã‚’åˆ†æ"""
        # aspectã‚’å«ã‚€æ–‡ã‚’æŠ½å‡º
        sentences = text.split('.')
        relevant_sentences = [s for s in sentences if aspect.lower() in s.lower()]

        if not relevant_sentences:
            return None

        # æ„Ÿæƒ…åˆ†æ
        combined_text = '. '.join(relevant_sentences)
        result = self.sentiment_analyzer(combined_text[:512])[0]  # BERT max length

        stars = int(result['label'].split()[0])
        sentiment = 'Positive' if stars >= 4 else 'Negative' if stars <= 2 else 'Neutral'

        return {
            'aspect': aspect,
            'sentiment': sentiment,
            'stars': stars,
            'confidence': result['score']
        }

    def analyze(self, text):
        """å®Œå…¨ãªABSAåˆ†æ"""
        aspects = self.extract_aspects(text)
        results = []

        for aspect in aspects:
            result = self.analyze_aspect_sentiment(text, aspect)
            if result:
                results.append(result)

        return results

# ä½¿ç”¨ä¾‹
analyzer = AspectBasedSentimentAnalyzer()

review = """
The food at this restaurant was absolutely delicious, especially the pasta.
However, the service was quite slow and the staff seemed unfriendly.
The ambiance was nice and cozy. The prices are a bit high but worth it for the quality.
"""

print("=== Aspect-Based Sentiment Analysis ===\n")
print(f"Review:\n{review}\n")

results = analyzer.analyze(review)

print("Aspect-level Sentiments:")
for r in results:
    print(f"  {r['aspect']}: {r['sentiment']} ({r['stars']} stars, {r['confidence']:.1%} confidence)")

# å…¨ä½“çš„ãªé›†è¨ˆ
positive = sum(1 for r in results if r['sentiment'] == 'Positive')
negative = sum(1 for r in results if r['sentiment'] == 'Negative')
neutral = sum(1 for r in results if r['sentiment'] == 'Neutral')

print(f"\nOverall Summary:")
print(f"  Positive aspects: {positive}")
print(f"  Negative aspects: {negative}")
print(f"  Neutral aspects: {neutral}")
</code></pre>

<hr>

<h2>5.2 å›ºæœ‰è¡¨ç¾èªè­˜ï¼ˆNamed Entity Recognitionï¼‰</h2>

<h3>å›ºæœ‰è¡¨ç¾èªè­˜ã¨ã¯</h3>
<p><strong>å›ºæœ‰è¡¨ç¾èªè­˜ï¼ˆNER: Named Entity Recognitionï¼‰</strong>ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰äººåã€çµ„ç¹”åã€åœ°åã€æ—¥ä»˜ãªã©ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æŠ½å‡ºãƒ»åˆ†é¡ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚</p>

<h3>ä¸»è¦ãªã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚¿ã‚¤ãƒ—</h3>

<table>
<thead>
<tr>
<th>ã‚¿ã‚¤ãƒ—</th>
<th>èª¬æ˜</th>
<th>ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>PERSON</strong></td>
<td>äººå</td>
<td>Barack Obama, å±±ç”°å¤ªéƒ</td>
</tr>
<tr>
<td><strong>ORG</strong></td>
<td>çµ„ç¹”å</td>
<td>Google, æ±äº¬å¤§å­¦</td>
</tr>
<tr>
<td><strong>GPE</strong></td>
<td>åœ°åï¼ˆå›½ã€éƒ½å¸‚ãªã©ï¼‰</td>
<td>Tokyo, United States</td>
</tr>
<tr>
<td><strong>DATE</strong></td>
<td>æ—¥ä»˜</td>
<td>2025å¹´10æœˆ21æ—¥, yesterday</td>
</tr>
<tr>
<td><strong>MONEY</strong></td>
<td>é‡‘é¡</td>
<td>$100, 1ä¸‡å††</td>
</tr>
<tr>
<td><strong>PRODUCT</strong></td>
<td>è£½å“å</td>
<td>iPhone, Windows</td>
</tr>
</tbody>
</table>

<h3>BIO ã‚¿ã‚®ãƒ³ã‚°æ–¹å¼</h3>

<p>NERã§ã¯<strong>BIO tagging scheme</strong>ãŒä¸€èˆ¬çš„ã§ã™ï¼š</p>

<ul>
<li><strong>B</strong> (Begin): ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®é–‹å§‹</li>
<li><strong>I</strong> (Inside): ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®å†…éƒ¨</li>
<li><strong>O</strong> (Outside): ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£å¤–</li>
</ul>

<p>ä¾‹ï¼šã€ŒBarack Obama visited New Yorkã€</p>
<ul>
<li>Barack: <code>B-PERSON</code></li>
<li>Obama: <code>I-PERSON</code></li>
<li>visited: <code>O</code></li>
<li>New: <code>B-GPE</code></li>
<li>York: <code>I-GPE</code></li>
</ul>

<h3>spaCyã«ã‚ˆã‚‹NER</h3>

<pre><code class="language-python">import spacy
from spacy import displacy
import pandas as pd

# è‹±èªãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
nlp = spacy.load("en_core_web_sm")

# ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
text = """
Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne
in April 1976 in Cupertino, California. The company's first product was
the Apple I computer. In 2011, Apple became the world's most valuable
publicly traded company. Tim Cook became CEO in August 2011, succeeding
Steve Jobs. Today, Apple employs over 150,000 people worldwide and
generates over $300 billion in annual revenue.
"""

# NERå®Ÿè¡Œ
doc = nlp(text)

print("=== Named Entity Recognition (spaCy) ===\n")
print(f"Text:\n{text}\n")

# ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º
entities = []
for ent in doc.ents:
    entities.append({
        'text': ent.text,
        'label': ent.label_,
        'start': ent.start_char,
        'end': ent.end_char
    })

# çµæœè¡¨ç¤º
df_entities = pd.DataFrame(entities)
print("\nExtracted Entities:")
print(df_entities.to_string(index=False))

# ãƒ©ãƒ™ãƒ«ã”ã¨ã«é›†è¨ˆ
print("\n\nEntity Count by Type:")
label_counts = df_entities['label'].value_counts()
for label, count in label_counts.items():
    print(f"  {label}: {count}")

# ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤ºï¼ˆHTMLã¨ã—ã¦ä¿å­˜å¯èƒ½ï¼‰
print("\n\nVisualizing entities...")
html = displacy.render(doc, style="ent", jupyter=False)

# ã‚«ã‚¹ã‚¿ãƒ è‰²è¨­å®šã§å¯è¦–åŒ–
colors = {
    "ORG": "#7aecec",
    "PERSON": "#aa9cfc",
    "GPE": "#feca74",
    "DATE": "#ff9561",
    "MONEY": "#9cc9cc"
}
options = {"ents": ["ORG", "PERSON", "GPE", "DATE", "MONEY"], "colors": colors}
displacy.render(doc, style="ent", options=options, jupyter=False)
</code></pre>

<h3>BERT-based NERï¼ˆTransformersï¼‰</h3>

<pre><code class="language-python">from transformers import pipeline
import pandas as pd

# BERTãƒ™ãƒ¼ã‚¹ã®NERãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
ner_pipeline = pipeline(
    "ner",
    model="dbmdz/bert-large-cased-finetuned-conll03-english",
    aggregation_strategy="simple"
)

# ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
text = """
Elon Musk announced that Tesla will open a new factory in Berlin, Germany.
The facility is expected to produce 500,000 vehicles per year starting in 2024.
This follows Tesla's successful Shanghai factory which opened in 2019.
"""

print("=== BERT-based NER ===\n")
print(f"Text:\n{text}\n")

# NERå®Ÿè¡Œ
entities = ner_pipeline(text)

# çµæœè¡¨ç¤º
print("\nExtracted Entities:")
for ent in entities:
    print(f"  {ent['word']:<20} â†’ {ent['entity_group']:<10} (score: {ent['score']:.3f})")

# ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
entity_dict = {}
for ent in entities:
    entity_type = ent['entity_group']
    if entity_type not in entity_dict:
        entity_dict[entity_type] = []
    entity_dict[entity_type].append(ent['word'])

print("\n\nGrouped by Entity Type:")
for entity_type, words in entity_dict.items():
    print(f"  {entity_type}: {', '.join(words)}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== BERT-based NER ===

Text:
Elon Musk announced that Tesla will open a new factory in Berlin, Germany.
The facility is expected to produce 500,000 vehicles per year starting in 2024.
This follows Tesla's successful Shanghai factory which opened in 2019.

Extracted Entities:
  Elon Musk            â†’ PER        (score: 0.999)
  Tesla                â†’ ORG        (score: 0.997)
  Berlin               â†’ LOC        (score: 0.999)
  Germany              â†’ LOC        (score: 0.999)
  Tesla                â†’ ORG        (score: 0.998)
  Shanghai             â†’ LOC        (score: 0.999)

Grouped by Entity Type:
  PER: Elon Musk
  ORG: Tesla, Tesla
  LOC: Berlin, Germany, Shanghai
</code></pre>

<h3>æ—¥æœ¬èªNERï¼ˆGiNZA + BERTï¼‰</h3>

<pre><code class="language-python">import spacy

# æ—¥æœ¬èªNERï¼ˆGiNZAãƒ¢ãƒ‡ãƒ«ï¼‰
nlp_ja = spacy.load("ja_ginza")

# æ—¥æœ¬èªã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
text_ja = """
2025å¹´10æœˆ21æ—¥ã€ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šã®è±Šç”°ç« ç”·ç¤¾é•·ãŒæ±äº¬ã§è¨˜è€…ä¼šè¦‹ã‚’é–‹ãã€
æ–°å‹é›»æ°—è‡ªå‹•è»Šã®é–‹ç™ºè¨ˆç”»ã‚’ç™ºè¡¨ã—ãŸã€‚åŒç¤¾ã¯2030å¹´ã¾ã§ã«100ä¸‡å°ã®
ç”Ÿç”£ã‚’ç›®æŒ‡ã™ã¨ã—ã¦ã„ã‚‹ã€‚ä¼šè¦‹ã«ã¯æ—¥çµŒæ–°èã‚„NHKãªã©ã®ãƒ¡ãƒ‡ã‚£ã‚¢ãŒå‚åŠ ã—ãŸã€‚
"""

print("=== æ—¥æœ¬èª Named Entity Recognition ===\n")
print(f"ãƒ†ã‚­ã‚¹ãƒˆ:\n{text_ja}\n")

# NERå®Ÿè¡Œ
doc_ja = nlp_ja(text_ja)

# ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º
print("æŠ½å‡ºã•ã‚ŒãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£:")
entities_ja = []
for ent in doc_ja.ents:
    entities_ja.append({
        'ãƒ†ã‚­ã‚¹ãƒˆ': ent.text,
        'ã‚¿ã‚¤ãƒ—': ent.label_,
        'è©³ç´°': spacy.explain(ent.label_)
    })
    print(f"  {ent.text:<15} â†’ {ent.label_:<10} ({spacy.explain(ent.label_)})")

# DataFrameåŒ–
df_ja = pd.DataFrame(entities_ja)
print("\n\nã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ä¸€è¦§:")
print(df_ja.to_string(index=False))

# ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
print("\n\nã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ:")
for label, count in df_ja['ã‚¿ã‚¤ãƒ—'].value_counts().items():
    print(f"  {label}: {count}å€‹")
</code></pre>

<h3>ã‚«ã‚¹ã‚¿ãƒ NERãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´</h3>

<pre><code class="language-python">from transformers import (
    AutoTokenizer,
    AutoModelForTokenClassification,
    TrainingArguments,
    Trainer,
    DataCollatorForTokenClassification
)
from datasets import Dataset
import numpy as np

# ã‚«ã‚¹ã‚¿ãƒ NERãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆï¼ˆç°¡ç•¥ç‰ˆï¼‰
train_data = [
    {
        "tokens": ["Apple", "is", "headquartered", "in", "Cupertino"],
        "ner_tags": [3, 0, 0, 0, 5]  # 3: B-ORG, 0: O, 5: B-LOC
    },
    {
        "tokens": ["Steve", "Jobs", "founded", "Apple", "Inc"],
        "ner_tags": [1, 2, 0, 3, 4]  # 1: B-PER, 2: I-PER, 3: B-ORG, 4: I-ORG
    },
    # ... å®Ÿéš›ã«ã¯ã‚‚ã£ã¨å¤šãã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
]

# ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°
label_list = [
    "O",           # 0
    "B-PER",       # 1: Person (Begin)
    "I-PER",       # 2: Person (Inside)
    "B-ORG",       # 3: Organization (Begin)
    "I-ORG",       # 4: Organization (Inside)
    "B-LOC",       # 5: Location (Begin)
    "I-LOC"        # 6: Location (Inside)
]

id2label = {i: label for i, label in enumerate(label_list)}
label2id = {label: i for i, label in enumerate(label_list)}

print("=== ã‚«ã‚¹ã‚¿ãƒ NERãƒ¢ãƒ‡ãƒ«è¨“ç·´ ===\n")
print(f"ãƒ©ãƒ™ãƒ«æ•°: {len(label_list)}")
print(f"ãƒ©ãƒ™ãƒ«: {label_list}\n")

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«
model_name = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(
    model_name,
    num_labels=len(label_list),
    id2label=id2label,
    label2id=label2id
)

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
def tokenize_and_align_labels(examples):
    """ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨ãƒ©ãƒ™ãƒ«ã®æ•´åˆ—"""
    tokenized_inputs = tokenizer(
        examples["tokens"],
        truncation=True,
        is_split_into_words=True,
        padding=True
    )

    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        label_ids = []
        previous_word_idx = None

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¯ç„¡è¦–
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            else:
                label_ids.append(-100)  # ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã¯ç„¡è¦–
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¤‰æ›
dataset = Dataset.from_list(train_data)
tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)

print("è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™å®Œäº†")
print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(tokenized_dataset)}")
print("\næ³¨æ„: å®Ÿéš›ã®è¨“ç·´ã«ã¯æ•°åƒã€œæ•°ä¸‡ã®ã‚µãƒ³ãƒ—ãƒ«ãŒå¿…è¦ã§ã™")
</code></pre>

<hr>

<h2>5.3 è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ï¼ˆQuestion Answeringï¼‰</h2>

<h3>è³ªå•å¿œç­”ã®ã‚¿ã‚¤ãƒ—</h3>

<table>
<thead>
<tr>
<th>ã‚¿ã‚¤ãƒ—</th>
<th>èª¬æ˜</th>
<th>ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Extractive QA</strong></td>
<td>æ–‡æ›¸ã‹ã‚‰ç­”ãˆã®ç®‡æ‰€ã‚’æŠ½å‡º</td>
<td>SQuAD, NewsQA</td>
</tr>
<tr>
<td><strong>Abstractive QA</strong></td>
<td>æ–‡æ›¸ã‚’ç†è§£ã—ã¦æ–°ã—ã„æ–‡ã‚’ç”Ÿæˆ</td>
<td>è¦ç´„å‹QA</td>
</tr>
<tr>
<td><strong>Multiple Choice</strong></td>
<td>é¸æŠè‚¢ã‹ã‚‰æ­£è§£ã‚’é¸ã¶</td>
<td>RACE, ARC</td>
</tr>
<tr>
<td><strong>Open-domain QA</strong></td>
<td>çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å…¨ä½“ã‹ã‚‰å›ç­”</td>
<td>Googleæ¤œç´¢çš„QA</td>
</tr>
</tbody>
</table>

<h3>Extractive QAï¼ˆBERTï¼‰</h3>

<pre><code class="language-python">from transformers import pipeline

# BERTãƒ™ãƒ¼ã‚¹ã®QAãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
qa_pipeline = pipeline(
    "question-answering",
    model="deepset/bert-base-cased-squad2"
)

# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ–‡æ›¸ï¼‰
context = """
The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical
rainforest in the Amazon biome that covers most of the Amazon basin of South America.
This basin encompasses 7 million square kilometers, of which 5.5 million square
kilometers are covered by the rainforest. The majority of the forest is contained
within Brazil, with 60% of the rainforest, followed by Peru with 13%, and Colombia
with 10%. The Amazon represents over half of the planet's remaining rainforests and
comprises the largest and most biodiverse tract of tropical rainforest in the world,
with an estimated 390 billion individual trees divided into 16,000 species.
"""

# è³ªå•ãƒªã‚¹ãƒˆ
questions = [
    "Where is the Amazon rainforest located?",
    "How many square kilometers does the Amazon basin cover?",
    "What percentage of the Amazon rainforest is in Brazil?",
    "How many tree species are in the Amazon?",
    "Which country has the second largest portion of the Amazon?"
]

print("=== Extractive Question Answering ===\n")
print(f"Context:\n{context}\n")
print("=" * 70)

for i, question in enumerate(questions, 1):
    result = qa_pipeline(question=question, context=context)

    print(f"\nQ{i}: {question}")
    print(f"A{i}: {result['answer']}")
    print(f"   Confidence: {result['score']:.2%}")
    print(f"   Position: characters {result['start']}-{result['end']}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Extractive Question Answering ===

Context:
The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical
rainforest in the Amazon biome that covers most of the Amazon basin of South America.
...

======================================================================

Q1: Where is the Amazon rainforest located?
A1: South America
   Confidence: 98.76%
   Position: characters 159-172

Q2: How many square kilometers does the Amazon basin cover?
A2: 7 million square kilometers
   Confidence: 95.43%
   Position: characters 193-218

Q3: What percentage of the Amazon rainforest is in Brazil?
A3: 60%
   Confidence: 99.12%
   Position: characters 333-336

Q4: How many tree species are in the Amazon?
A4: 16,000 species
   Confidence: 97.58%
   Position: characters 602-616

Q5: Which country has the second largest portion of the Amazon?
A5: Peru
   Confidence: 96.34%
   Position: characters 364-368
</code></pre>

<h3>æ—¥æœ¬èªè³ªå•å¿œç­”</h3>

<pre><code class="language-python">from transformers import pipeline

# æ—¥æœ¬èªQAãƒ¢ãƒ‡ãƒ«
qa_pipeline_ja = pipeline(
    "question-answering",
    model="cl-tohoku/bert-base-japanese-whole-word-masking"
)

# æ—¥æœ¬èªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
context_ja = """
å¯Œå£«å±±ã¯æ—¥æœ¬ã®æœ€é«˜å³°ã§ã€æ¨™é«˜3,776ãƒ¡ãƒ¼ãƒˆãƒ«ã®æ´»ç«å±±ã§ã™ã€‚
å±±æ¢¨çœŒã¨é™å²¡çœŒã«ã¾ãŸãŒã‚Šã€æ—¥æœ¬ã®è±¡å¾´ã¨ã—ã¦å›½å†…å¤–ã«çŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚
2013å¹´6æœˆã«ãƒ¦ãƒã‚¹ã‚³ã®ä¸–ç•Œæ–‡åŒ–éºç”£ã«ç™»éŒ²ã•ã‚Œã¾ã—ãŸã€‚
å¯Œå£«å±±ã¯ç´„10ä¸‡å¹´å‰ã‹ã‚‰ç¾åœ¨ã®å½¢ã«ãªã‚Šã€æœ€å¾Œã®å™´ç«ã¯1707å¹´ã®å®æ°¸å¤§å™´ç«ã§ã™ã€‚
æ¯å¹´7æœˆã¨8æœˆã®ç™»å±±ã‚·ãƒ¼ã‚ºãƒ³ã«ã¯ã€ç´„30ä¸‡äººã®ç™»å±±è€…ãŒè¨ªã‚Œã¾ã™ã€‚
"""

questions_ja = [
    "å¯Œå£«å±±ã®æ¨™é«˜ã¯ä½•ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã‹ï¼Ÿ",
    "å¯Œå£«å±±ãŒä¸–ç•Œéºç”£ã«ç™»éŒ²ã•ã‚ŒãŸã®ã¯ã„ã¤ã§ã™ã‹ï¼Ÿ",
    "å¯Œå£«å±±ã®æœ€å¾Œã®å™´ç«ã¯ã„ã¤ã§ã™ã‹ï¼Ÿ",
    "ç™»å±±ã‚·ãƒ¼ã‚ºãƒ³ã«ä½•äººãã‚‰ã„ãŒè¨ªã‚Œã¾ã™ã‹ï¼Ÿ"
]

print("=== æ—¥æœ¬èªè³ªå•å¿œç­” ===\n")
print(f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:\n{context_ja}\n")
print("=" * 70)

for i, question in enumerate(questions_ja, 1):
    result = qa_pipeline_ja(question=question, context=context_ja)

    print(f"\nQ{i}: {question}")
    print(f"A{i}: {result['answer']}")
    print(f"   ä¿¡é ¼åº¦: {result['score']:.2%}")
</code></pre>

<h3>Retrieval-based QAï¼ˆæ¤œç´¢æ‹¡å¼µï¼‰</h3>

<pre><code class="language-python">from transformers import pipeline, AutoTokenizer, AutoModel
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class RetrievalQA:
    """æ¤œç´¢ãƒ™ãƒ¼ã‚¹ã®è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, documents):
        self.documents = documents

        # æ–‡æ›¸åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
        self.tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')
        self.encoder = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')

        # QAãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        self.qa_pipeline = pipeline(
            "question-answering",
            model="deepset/bert-base-cased-squad2"
        )

        # æ–‡æ›¸ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆäº‹å‰è¨ˆç®—ï¼‰
        self.doc_embeddings = self._encode_documents()

    def _encode_text(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
        inputs = self.tokenizer(text, return_tensors='pt',
                               truncation=True, padding=True, max_length=512)
        with torch.no_grad():
            outputs = self.encoder(**inputs)
        # Mean pooling
        embeddings = outputs.last_hidden_state.mean(dim=1)
        return embeddings.numpy()

    def _encode_documents(self):
        """å…¨æ–‡æ›¸ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
        embeddings = []
        for doc in self.documents:
            emb = self._encode_text(doc)
            embeddings.append(emb)
        return np.vstack(embeddings)

    def retrieve_relevant_docs(self, query, top_k=3):
        """è³ªå•ã«é–¢é€£ã™ã‚‹æ–‡æ›¸ã‚’æ¤œç´¢"""
        query_emb = self._encode_text(query)
        similarities = cosine_similarity(query_emb, self.doc_embeddings)[0]

        # Top-kæ–‡æ›¸ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        top_indices = np.argsort(similarities)[::-1][:top_k]

        relevant_docs = []
        for idx in top_indices:
            relevant_docs.append({
                'document': self.documents[idx],
                'similarity': similarities[idx],
                'index': idx
            })

        return relevant_docs

    def answer_question(self, question, top_k=3):
        """è³ªå•ã«å›ç­”"""
        # é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢
        relevant_docs = self.retrieve_relevant_docs(question, top_k=top_k)

        # æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„æ–‡æ›¸ã§å›ç­”
        best_doc = relevant_docs[0]['document']
        result = self.qa_pipeline(question=question, context=best_doc)

        return {
            'question': question,
            'answer': result['answer'],
            'confidence': result['score'],
            'source_document': relevant_docs[0]['index'],
            'similarity': relevant_docs[0]['similarity'],
            'all_relevant_docs': relevant_docs
        }

# æ–‡æ›¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
documents = [
    """Python is a high-level programming language created by Guido van Rossum
    and first released in 1991. It emphasizes code readability and uses
    significant indentation. Python is dynamically typed and garbage-collected.""",

    """Machine learning is a branch of artificial intelligence that focuses on
    building systems that learn from data. Common algorithms include decision trees,
    neural networks, and support vector machines.""",

    """Deep learning is a subset of machine learning based on artificial neural
    networks with multiple layers. It has achieved remarkable results in computer
    vision, natural language processing, and speech recognition.""",

    """Natural language processing (NLP) is a field of AI concerned with the
    interaction between computers and human language. Tasks include sentiment
    analysis, machine translation, and question answering.""",

    """The Transformer architecture, introduced in 2017, revolutionized NLP.
    It uses self-attention mechanisms and has led to models like BERT, GPT,
    and T5 that achieve state-of-the-art results."""
]

# ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
print("=== Retrieval-based Question Answering ===\n")
print("æ–‡æ›¸ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­...")
qa_system = RetrievalQA(documents)
print(f"å®Œäº†ï¼ {len(documents)}å€‹ã®æ–‡æ›¸ã‚’æº–å‚™ã—ã¾ã—ãŸ\n")

# è³ªå•ãƒªã‚¹ãƒˆ
questions = [
    "Who created Python?",
    "What is deep learning?",
    "What does NLP stand for?",
    "When was the Transformer architecture introduced?"
]

for question in questions:
    print(f"\nQuestion: {question}")
    result = qa_system.answer_question(question, top_k=2)

    print(f"Answer: {result['answer']}")
    print(f"Confidence: {result['confidence']:.2%}")
    print(f"Source: Document #{result['source_document']} (similarity: {result['similarity']:.3f})")

    print(f"\nRelevant documents:")
    for i, doc in enumerate(result['all_relevant_docs'], 1):
        print(f"  {i}. Doc #{doc['index']} (similarity: {doc['similarity']:.3f})")
        print(f"     {doc['document'][:100]}...")
</code></pre>

<hr>

<h2>5.4 ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ï¼ˆText Summarizationï¼‰</h2>

<h3>è¦ç´„ã®ã‚¿ã‚¤ãƒ—</h3>

<table>
<thead>
<tr>
<th>ã‚¿ã‚¤ãƒ—</th>
<th>èª¬æ˜</th>
<th>æ‰‹æ³•</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Extractive</strong></td>
<td>å…ƒãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡è¦æ–‡ã‚’æŠ½å‡º</td>
<td>TextRank, LexRank</td>
</tr>
<tr>
<td><strong>Abstractive</strong></td>
<td>å†…å®¹ã‚’ç†è§£ã—ã¦æ–°ã—ã„æ–‡ã‚’ç”Ÿæˆ</td>
<td>BART, T5, GPT</td>
</tr>
<tr>
<td><strong>Single-document</strong></td>
<td>1ã¤ã®æ–‡æ›¸ã‚’è¦ç´„</td>
<td>ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹è¦ç´„</td>
</tr>
<tr>
<td><strong>Multi-document</strong></td>
<td>è¤‡æ•°æ–‡æ›¸ã‚’çµ±åˆè¦ç´„</td>
<td>ãƒˆãƒ”ãƒƒã‚¯è¦ç´„</td>
</tr>
</tbody>
</table>

<h3>Extractive Summarizationï¼ˆTextRankï¼‰</h3>

<pre><code class="language-python">import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
import nltk
from nltk.tokenize import sent_tokenize

# NLTK ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆåˆå›ã®ã¿ï¼‰
# nltk.download('punkt')

class TextRankSummarizer:
    """TextRankã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹æŠ½å‡ºå‹è¦ç´„"""

    def __init__(self, similarity_threshold=0.1):
        self.similarity_threshold = similarity_threshold

    def _build_similarity_matrix(self, sentences):
        """æ–‡é–“ã®é¡ä¼¼åº¦è¡Œåˆ—ã‚’æ§‹ç¯‰"""
        # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(sentences)

        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        similarity_matrix = cosine_similarity(tfidf_matrix)

        # é–¾å€¤ä»¥ä¸‹ã‚’0ã«
        similarity_matrix[similarity_matrix < self.similarity_threshold] = 0

        return similarity_matrix

    def summarize(self, text, num_sentences=3):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„"""
        # æ–‡åˆ†å‰²
        sentences = sent_tokenize(text)

        if len(sentences) <= num_sentences:
            return text

        # é¡ä¼¼åº¦è¡Œåˆ—æ§‹ç¯‰
        similarity_matrix = self._build_similarity_matrix(sentences)

        # ã‚°ãƒ©ãƒ•æ§‹ç¯‰
        graph = nx.from_numpy_array(similarity_matrix)

        # PageRankè¨ˆç®—
        scores = nx.pagerank(graph)

        # ã‚¹ã‚³ã‚¢ã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        ranked_sentences = sorted(
            ((scores[i], s) for i, s in enumerate(sentences)),
            reverse=True
        )

        # Top-kæ–‡ã‚’å–å¾—ï¼ˆå…ƒã®é †åºã‚’ä¿æŒï¼‰
        top_sentences = sorted(
            ranked_sentences[:num_sentences],
            key=lambda x: sentences.index(x[1])
        )

        # è¦ç´„ç”Ÿæˆ
        summary = ' '.join([sent for score, sent in top_sentences])

        return summary, scores

# ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
article = """
Artificial intelligence has made remarkable progress in recent years.
Deep learning, a subset of machine learning, has been particularly successful.
Neural networks with many layers can learn complex patterns from data.
These models have achieved human-level performance on many tasks.
Computer vision has benefited greatly from deep learning advances.
Image classification, object detection, and segmentation are now highly accurate.
Natural language processing has also seen dramatic improvements.
Machine translation quality has improved significantly with neural approaches.
Language models can now generate coherent and contextually appropriate text.
However, challenges remain in areas like reasoning and common sense understanding.
AI systems still struggle with tasks that humans find easy.
Researchers are working on more robust and interpretable AI systems.
The future of AI holds both great promise and important challenges.
"""

print("=== Extractive Summarization (TextRank) ===\n")
print(f"Original Text ({len(sent_tokenize(article))} sentences):")
print(article)
print("\n" + "=" * 70)

summarizer = TextRankSummarizer()

for num_sents in [3, 5]:
    summary, scores = summarizer.summarize(article, num_sentences=num_sents)
    print(f"\n{num_sents}-Sentence Summary:")
    print(summary)
    print(f"\nCompression ratio: {len(summary) / len(article):.1%}")
</code></pre>

<h3>Abstractive Summarizationï¼ˆBART/T5ï¼‰</h3>

<pre><code class="language-python">from transformers import pipeline

# BARTãƒ™ãƒ¼ã‚¹ã®è¦ç´„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
summarizer_bart = pipeline(
    "summarization",
    model="facebook/bart-large-cnn"
)

# T5ãƒ™ãƒ¼ã‚¹ã®è¦ç´„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
summarizer_t5 = pipeline(
    "summarization",
    model="t5-base"
)

# é•·ã„è¨˜äº‹
long_article = """
Climate change is one of the most pressing challenges facing humanity today.
The Earth's average temperature has increased by approximately 1.1 degrees Celsius
since the pre-industrial era, primarily due to human activities that release
greenhouse gases into the atmosphere. The burning of fossil fuels for energy,
deforestation, and industrial processes are the main contributors to this warming trend.

The effects of climate change are already visible worldwide. Extreme weather events,
such as hurricanes, droughts, and heatwaves, are becoming more frequent and severe.
Sea levels are rising due to thermal expansion of water and melting ice sheets,
threatening coastal communities. Ecosystems are being disrupted, with many species
facing extinction as their habitats change faster than they can adapt.

To address climate change, a global effort is required. The Paris Agreement,
adopted in 2015, aims to limit global warming to well below 2 degrees Celsius
above pre-industrial levels. Countries are implementing various strategies,
including transitioning to renewable energy sources, improving energy efficiency,
and developing carbon capture technologies. Individual actions, such as reducing
energy consumption and supporting sustainable practices, also play a crucial role.

Despite progress, significant challenges remain. Many countries still rely heavily
on fossil fuels, and the transition to clean energy requires substantial investment.
Political will and international cooperation are essential for achieving climate goals.
Scientists emphasize that immediate and sustained action is necessary to prevent
the most catastrophic impacts of climate change and ensure a livable planet for
future generations.
"""

print("=== Abstractive Summarization ===\n")
print(f"Original Article ({len(long_article.split())} words):")
print(long_article)
print("\n" + "=" * 70)

# BARTè¦ç´„
print("\n### BART Summary ###")
bart_summary = summarizer_bart(
    long_article,
    max_length=100,
    min_length=50,
    do_sample=False
)
print(bart_summary[0]['summary_text'])
print(f"Length: {len(bart_summary[0]['summary_text'].split())} words")

# T5è¦ç´„ï¼ˆç•°ãªã‚‹é•·ã•ï¼‰
print("\n### T5 Summary (Short) ###")
t5_summary_short = summarizer_t5(
    long_article,
    max_length=60,
    min_length=30
)
print(t5_summary_short[0]['summary_text'])

print("\n### T5 Summary (Long) ###")
t5_summary_long = summarizer_t5(
    long_article,
    max_length=120,
    min_length=60
)
print(t5_summary_long[0]['summary_text'])
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Abstractive Summarization ===

Original Article (234 words):
Climate change is one of the most pressing challenges...

======================================================================

### BART Summary ###
Climate change is one of the most pressing challenges facing humanity today.
The Earth's average temperature has increased by approximately 1.1 degrees Celsius.
Effects include extreme weather events, rising sea levels, and ecosystem disruption.
The Paris Agreement aims to limit global warming to below 2 degrees Celsius.
Length: 51 words

### T5 Summary (Short) ###
climate change is caused by human activities that release greenhouse gases.
extreme weather events are becoming more frequent and severe.
Length: 19 words

### T5 Summary (Long) ###
the earth's average temperature has increased by 1.1 degrees celsius since
pre-industrial era. burning of fossil fuels, deforestation are main contributors.
paris agreement aims to limit global warming to below 2 degrees. countries are
implementing strategies including renewable energy and carbon capture.
Length: 45 words
</code></pre>

<h3>æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆè¦ç´„</h3>

<pre><code class="language-python">from transformers import pipeline

# æ—¥æœ¬èªè¦ç´„ãƒ¢ãƒ‡ãƒ«
summarizer_ja = pipeline(
    "summarization",
    model="sonoisa/t5-base-japanese"
)

# æ—¥æœ¬èªè¨˜äº‹
article_ja = """
äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰æŠ€è¡“ã¯è¿‘å¹´æ€¥é€Ÿã«ç™ºå±•ã—ã¦ãŠã‚Šã€ç§ãŸã¡ã®ç”Ÿæ´»ã®ã‚ã‚‰ã‚†ã‚‹é¢ã«å½±éŸ¿ã‚’ä¸ãˆã¦ã„ã¾ã™ã€‚
ç‰¹ã«ã€æ·±å±¤å­¦ç¿’ã¨å‘¼ã°ã‚Œã‚‹æŠ€è¡“ã®é€²æ­©ã«ã‚ˆã‚Šã€ç”»åƒèªè­˜ã‚„è‡ªç„¶è¨€èªå‡¦ç†ã®åˆ†é‡ã§é£›èºçš„ãª
æ€§èƒ½å‘ä¸ŠãŒå®Ÿç¾ã•ã‚Œã¾ã—ãŸã€‚

ç¾åœ¨ã€AIã¯åŒ»ç™‚è¨ºæ–­ã€è‡ªå‹•é‹è»¢ã€éŸ³å£°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ã‚¹ãƒ†ãƒ ãªã©ã€
å¤šæ§˜ãªåˆ†é‡ã§æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚åŒ»ç™‚åˆ†é‡ã§ã¯ã€AIãŒç”»åƒè¨ºæ–­ã§åŒ»å¸«ã‚’æ”¯æ´ã—ã€
ç—…æ°—ã®æ—©æœŸç™ºè¦‹ã«è²¢çŒ®ã—ã¦ã„ã¾ã™ã€‚è‡ªå‹•é‹è»¢æŠ€è¡“ã¯ã€äº¤é€šäº‹æ•…ã®å‰Šæ¸›ã¨
ç§»å‹•ã®åŠ¹ç‡åŒ–ã‚’ç›®æŒ‡ã—ã¦é–‹ç™ºãŒé€²ã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

ã—ã‹ã—ã€AIæŠ€è¡“ã®ç™ºå±•ã«ã¯èª²é¡Œã‚‚å­˜åœ¨ã—ã¾ã™ã€‚å€«ç†çš„ãªå•é¡Œã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã®ä¿è­·ã€
é›‡ç”¨ã¸ã®å½±éŸ¿ãªã©ãŒæ‡¸å¿µã•ã‚Œã¦ã„ã¾ã™ã€‚ã¾ãŸã€AIã®åˆ¤æ–­ãƒ—ãƒ­ã‚»ã‚¹ãŒä¸é€æ˜ã§ã‚ã‚‹
ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹å•é¡Œã‚‚æŒ‡æ‘˜ã•ã‚Œã¦ã„ã¾ã™ã€‚

ä»Šå¾Œã€AIæŠ€è¡“ã‚’ã‚ˆã‚Šè‰¯ãæ´»ç”¨ã™ã‚‹ãŸã‚ã«ã¯ã€æŠ€è¡“çš„ãªé€²æ­©ã ã‘ã§ãªãã€
ç¤¾ä¼šçš„ãªè­°è«–ã¨é©åˆ‡ãªè¦åˆ¶ã®æ•´å‚™ãŒå¿…è¦ã§ã™ã€‚äººé–“ã¨AIãŒå”èª¿ã™ã‚‹ç¤¾ä¼šã®
å®Ÿç¾ã«å‘ã‘ã¦ã€ç¶™ç¶šçš„ãªå–ã‚Šçµ„ã¿ãŒæ±‚ã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚
"""

print("=== æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ ===\n")
print(f"å…ƒè¨˜äº‹ ({len(article_ja)}æ–‡å­—):")
print(article_ja)
print("\n" + "=" * 70)

# è¦ç´„ç”Ÿæˆ
summary_ja = summarizer_ja(
    article_ja,
    max_length=100,
    min_length=30
)

print("\nè¦ç´„:")
print(summary_ja[0]['summary_text'])
print(f"\nåœ§ç¸®ç‡: {len(summary_ja[0]['summary_text']) / len(article_ja):.1%}")
</code></pre>

<hr>

<h2>5.5 ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ</h2>

<h3>Multi-task NLPãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h3>

<pre><code class="language-python">from transformers import pipeline
import spacy
from typing import Dict, List
import json

class NLPPipeline:
    """åŒ…æ‹¬çš„ãªNLPãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(self):
        print("NLPãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åˆæœŸåŒ–ä¸­...")

        # å„ã‚¿ã‚¹ã‚¯ã®ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        self.sentiment_analyzer = pipeline(
            "sentiment-analysis",
            model="distilbert-base-uncased-finetuned-sst-2-english"
        )

        self.ner_pipeline = pipeline(
            "ner",
            model="dbmdz/bert-large-cased-finetuned-conll03-english",
            aggregation_strategy="simple"
        )

        self.qa_pipeline = pipeline(
            "question-answering",
            model="deepset/bert-base-cased-squad2"
        )

        self.summarizer = pipeline(
            "summarization",
            model="facebook/bart-large-cnn"
        )

        # spaCyï¼ˆãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã€å“è©ã‚¿ã‚°ä»˜ã‘ï¼‰
        self.nlp = spacy.load("en_core_web_sm")

        print("åˆæœŸåŒ–å®Œäº†ï¼\n")

    def analyze_text(self, text: str) -> Dict:
        """ãƒ†ã‚­ã‚¹ãƒˆã®åŒ…æ‹¬çš„åˆ†æ"""
        results = {}

        # 1. åŸºæœ¬çµ±è¨ˆ
        doc = self.nlp(text)
        results['statistics'] = {
            'num_characters': len(text),
            'num_words': len([token for token in doc if not token.is_punct]),
            'num_sentences': len(list(doc.sents)),
            'num_unique_words': len(set([token.text.lower() for token in doc
                                        if not token.is_punct]))
        }

        # 2. æ„Ÿæƒ…åˆ†æ
        sentiment = self.sentiment_analyzer(text[:512])[0]
        results['sentiment'] = {
            'label': sentiment['label'],
            'score': round(sentiment['score'], 4)
        }

        # 3. å›ºæœ‰è¡¨ç¾èªè­˜
        entities = self.ner_pipeline(text)
        results['entities'] = [
            {
                'text': ent['word'],
                'type': ent['entity_group'],
                'score': round(ent['score'], 4)
            }
            for ent in entities
        ]

        # 4. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆåè©å¥ï¼‰
        keywords = []
        for chunk in doc.noun_chunks:
            if len(chunk.text.split()) <= 3:  # 3èªä»¥ä¸‹
                keywords.append(chunk.text)
        results['keywords'] = list(set(keywords))[:10]

        # 5. å“è©ã‚¿ã‚°åˆ†å¸ƒ
        pos_counts = {}
        for token in doc:
            pos = token.pos_
            pos_counts[pos] = pos_counts.get(pos, 0) + 1
        results['pos_distribution'] = pos_counts

        return results

    def process_document(self, text: str,
                        questions: List[str] = None,
                        summarize: bool = True) -> Dict:
        """æ–‡æ›¸ã®å®Œå…¨å‡¦ç†"""
        results = {
            'original_text': text,
            'analysis': self.analyze_text(text)
        }

        # è¦ç´„
        if summarize and len(text.split()) > 50:
            summary = self.summarizer(
                text,
                max_length=100,
                min_length=30,
                do_sample=False
            )
            results['summary'] = summary[0]['summary_text']

        # è³ªå•å¿œç­”
        if questions:
            results['qa'] = []
            for q in questions:
                answer = self.qa_pipeline(question=q, context=text)
                results['qa'].append({
                    'question': q,
                    'answer': answer['answer'],
                    'confidence': round(answer['score'], 4)
                })

        return results

# ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
pipeline = NLPPipeline()

# ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸
document = """
Apple Inc. announced record quarterly earnings on Tuesday, with revenue
reaching $90 billion. CEO Tim Cook stated that the strong performance was
driven by robust iPhone sales and growing services revenue. The company's
stock price jumped 5% following the announcement.

Apple also revealed plans to invest $50 billion in research and development
over the next five years, focusing on artificial intelligence and augmented
reality technologies. The investment will create thousands of new jobs in
the United States and internationally.

However, analysts expressed concerns about potential supply chain disruptions
and increasing competition in the smartphone market. Despite these challenges,
Apple remains optimistic about future growth prospects.
"""

# è³ªå•ãƒªã‚¹ãƒˆ
questions = [
    "How much revenue did Apple report?",
    "Who is the CEO of Apple?",
    "How much will Apple invest in R&D?",
    "What technologies will Apple focus on?"
]

print("=== Multi-task NLP Pipeline ===\n")
print("æ–‡æ›¸ã‚’å‡¦ç†ä¸­...\n")

# å®Œå…¨å‡¦ç†
results = pipeline.process_document(
    text=document,
    questions=questions,
    summarize=True
)

# çµæœè¡¨ç¤º
print("### 1. åŸºæœ¬çµ±è¨ˆ ###")
stats = results['analysis']['statistics']
for key, value in stats.items():
    print(f"  {key}: {value}")

print("\n### 2. æ„Ÿæƒ…åˆ†æ ###")
sentiment = results['analysis']['sentiment']
print(f"  Sentiment: {sentiment['label']} (confidence: {sentiment['score']:.1%})")

print("\n### 3. å›ºæœ‰è¡¨ç¾ ###")
for ent in results['analysis']['entities'][:10]:
    print(f"  {ent['text']:<20} â†’ {ent['type']:<10} ({ent['score']:.1%})")

print("\n### 4. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ ###")
print(f"  {', '.join(results['analysis']['keywords'])}")

print("\n### 5. è¦ç´„ ###")
print(f"  {results['summary']}")

print("\n### 6. è³ªå•å¿œç­” ###")
for qa in results['qa']:
    print(f"  Q: {qa['question']}")
    print(f"  A: {qa['answer']} (confidence: {qa['confidence']:.1%})\n")

# JSONå‡ºåŠ›
print("\n### JSONå‡ºåŠ› ###")
json_output = json.dumps(results, indent=2, ensure_ascii=False)
print(json_output[:500] + "...")
</code></pre>

<h3>FastAPIã§ã®APIé–‹ç™º</h3>

<pre><code class="language-python"># ãƒ•ã‚¡ã‚¤ãƒ«å: nlp_api.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import pipeline
from typing import List, Optional
import uvicorn

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
app = FastAPI(
    title="NLP API",
    description="åŒ…æ‹¬çš„ãªè‡ªç„¶è¨€èªå‡¦ç†API",
    version="1.0.0"
)

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
class TextInput(BaseModel):
    text: str
    max_length: Optional[int] = 100

class QAInput(BaseModel):
    question: str
    context: str

class BatchTextInput(BaseModel):
    texts: List[str]

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
sentiment_analyzer = pipeline("sentiment-analysis")
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
qa_pipeline = pipeline("question-answering")
ner_pipeline = pipeline("ner", aggregation_strategy="simple")

# ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

@app.get("/")
async def root():
    """APIã®ãƒ«ãƒ¼ãƒˆ"""
    return {
        "message": "NLP API ã¸ã‚ˆã†ã“ã",
        "endpoints": [
            "/sentiment",
            "/summarize",
            "/qa",
            "/ner",
            "/batch-sentiment"
        ]
    }

@app.post("/sentiment")
async def analyze_sentiment(input_data: TextInput):
    """æ„Ÿæƒ…åˆ†æã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        result = sentiment_analyzer(input_data.text[:512])[0]
        return {
            "text": input_data.text,
            "sentiment": result['label'],
            "confidence": round(result['score'], 4)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/summarize")
async def summarize_text(input_data: TextInput):
    """ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        summary = summarizer(
            input_data.text,
            max_length=input_data.max_length,
            min_length=30,
            do_sample=False
        )
        return {
            "original_text": input_data.text,
            "summary": summary[0]['summary_text'],
            "compression_ratio": len(summary[0]['summary_text']) / len(input_data.text)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/qa")
async def answer_question(input_data: QAInput):
    """è³ªå•å¿œç­”ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        result = qa_pipeline(
            question=input_data.question,
            context=input_data.context
        )
        return {
            "question": input_data.question,
            "answer": result['answer'],
            "confidence": round(result['score'], 4),
            "start": result['start'],
            "end": result['end']
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/ner")
async def extract_entities(input_data: TextInput):
    """å›ºæœ‰è¡¨ç¾èªè­˜ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        entities = ner_pipeline(input_data.text)
        return {
            "text": input_data.text,
            "entities": [
                {
                    "text": ent['word'],
                    "type": ent['entity_group'],
                    "score": round(ent['score'], 4)
                }
                for ent in entities
            ]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/batch-sentiment")
async def batch_sentiment_analysis(input_data: BatchTextInput):
    """ãƒãƒƒãƒæ„Ÿæƒ…åˆ†æ"""
    try:
        results = []
        for text in input_data.texts:
            result = sentiment_analyzer(text[:512])[0]
            results.append({
                "text": text,
                "sentiment": result['label'],
                "confidence": round(result['score'], 4)
            })
        return {"results": results}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
@app.get("/health")
async def health_check():
    """APIã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {"status": "healthy", "models_loaded": True}

# ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
if __name__ == "__main__":
    print("NLP APIã‚’èµ·å‹•ä¸­...")
    print("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: http://localhost:8000/docs")
    uvicorn.run(app, host="0.0.0.0", port=8000)
</code></pre>

<p><strong>ä½¿ç”¨æ–¹æ³•</strong>ï¼š</p>
<pre><code class="language-bash"># ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
python nlp_api.py

# curlã§ãƒ†ã‚¹ãƒˆ
curl -X POST "http://localhost:8000/sentiment" \
  -H "Content-Type: application/json" \
  -d '{"text": "This is an amazing product!"}'

# Pythonã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
import requests

response = requests.post(
    "http://localhost:8000/sentiment",
    json={"text": "I love this API!"}
)
print(response.json())
</code></pre>

<hr>

<h2>5.6 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>æ„Ÿæƒ…åˆ†æ</strong></p>
<ul>
<li>Binaryã€Multi-classã€Aspect-basedåˆ†é¡</li>
<li>TF-IDF + ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°</li>
<li>BERTäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æ´»ç”¨</li>
<li>æ—¥æœ¬èªæ„Ÿæƒ…åˆ†æ</li>
</ul></li>

<li><p><strong>å›ºæœ‰è¡¨ç¾èªè­˜</strong></p>
<ul>
<li>BIOã‚¿ã‚®ãƒ³ã‚°æ–¹å¼</li>
<li>spaCyã€BERT-based NER</li>
<li>æ—¥æœ¬èªNERï¼ˆGiNZAï¼‰</li>
<li>ã‚«ã‚¹ã‚¿ãƒ NERãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´</li>
</ul></li>

<li><p><strong>è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ </strong></p>
<ul>
<li>Extractive QAï¼ˆBERTï¼‰</li>
<li>Retrieval-based QA</li>
<li>æ—¥æœ¬èªQA</li>
<li>æ–‡æ›¸æ¤œç´¢ã¨å›ç­”ç”Ÿæˆã®çµ±åˆ</li>
</ul></li>

<li><p><strong>ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„</strong></p>
<ul>
<li>Extractiveï¼ˆTextRankï¼‰</li>
<li>Abstractiveï¼ˆBARTã€T5ï¼‰</li>
<li>æ—¥æœ¬èªè¦ç´„</li>
<li>è¦ç´„å“è³ªã®è©•ä¾¡</li>
</ul></li>

<li><p><strong>ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰å®Ÿè£…</strong></p>
<ul>
<li>Multi-task NLPãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</li>
<li>FastAPIã§ã®APIé–‹ç™º</li>
<li>æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤</li>
<li>ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨è©•ä¾¡</li>
</ul></li>
</ol>

<h3>å®Ÿè£…ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>æ¨å¥¨äº‹é …</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ¢ãƒ‡ãƒ«é¸æŠ</strong></td>
<td>ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠï¼ˆç²¾åº¦ vs é€Ÿåº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰</td>
</tr>
<tr>
<td><strong>å‰å‡¦ç†</strong></td>
<td>ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã€æ­£è¦åŒ–ã‚’çµ±ä¸€</td>
</tr>
<tr>
<td><strong>è©•ä¾¡æŒ‡æ¨™</strong></td>
<td>ã‚¿ã‚¹ã‚¯ã”ã¨ã«é©åˆ‡ãªæŒ‡æ¨™ï¼ˆF1ã€BLEUã€ROUGEç­‰ï¼‰</td>
</tr>
<tr>
<td><strong>ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°</strong></td>
<td>å…¥åŠ›é•·åˆ¶é™ã€ä¾‹å¤–å‡¦ç†ã‚’å®Ÿè£…</td>
</tr>
<tr>
<td><strong>ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹</strong></td>
<td>ãƒãƒƒãƒå‡¦ç†ã€ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€GPUæ´»ç”¨</td>
</tr>
<tr>
<td><strong>ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°</strong></td>
<td>æ¨è«–æ™‚é–“ã€ç²¾åº¦ã€ã‚¨ãƒ©ãƒ¼ç‡ã‚’è¨˜éŒ²</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>

<ul>
<li>å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®ç†è§£ã¨æ´»ç”¨</li>
<li>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</li>
<li>RAGï¼ˆRetrieval-Augmented Generationï¼‰</li>
<li>Fine-tuningã¨ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œ</li>
<li>ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«NLPï¼ˆãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒï¼‰</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>
<p>Extractiveè¦ç´„ã¨Abstractiveè¦ç´„ã®é•ã„ã‚’èª¬æ˜ã—ã€ãã‚Œãã‚Œã®é•·æ‰€ã¨çŸ­æ‰€ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>Extractiveè¦ç´„</strong>ï¼š</p>
<ul>
<li>å®šç¾©: å…ƒãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡è¦ãªæ–‡ã‚’ãã®ã¾ã¾æŠ½å‡º</li>
<li>æ‰‹æ³•: TextRank, LexRank, TF-IDF</li>
<li>é•·æ‰€:
<ul>
<li>æ–‡æ³•çš„ã«æ­£ã—ã„ï¼ˆå…ƒã®æ–‡ã‚’ä½¿ç”¨ï¼‰</li>
<li>è¨ˆç®—ã‚³ã‚¹ãƒˆãŒä½ã„</li>
<li>äº‹å®Ÿã®æ­ªæ›²ãŒå°‘ãªã„</li>
</ul>
</li>
<li>çŸ­æ‰€:
<ul>
<li>å†—é•·æ€§ãŒæ®‹ã‚‹</li>
<li>æ–‡è„ˆã«å¿œã˜ãŸè¡¨ç¾å¤‰æ›´ãŒã§ããªã„</li>
<li>è¦ç´„ã®æµæš¢æ€§ãŒä½ã„å ´åˆãŒã‚ã‚‹</li>
</ul>
</li>
</ul>

<p><strong>Abstractiveè¦ç´„</strong>ï¼š</p>
<ul>
<li>å®šç¾©: å†…å®¹ã‚’ç†è§£ã—ã¦æ–°ã—ã„æ–‡ã‚’ç”Ÿæˆ</li>
<li>æ‰‹æ³•: BART, T5, GPT</li>
<li>é•·æ‰€:
<ul>
<li>ç°¡æ½”ã§æµæš¢ãªè¦ç´„</li>
<li>ãƒ‘ãƒ©ãƒ•ãƒ¬ãƒ¼ã‚ºã‚„è¨€ã„æ›ãˆãŒå¯èƒ½</li>
<li>ã‚ˆã‚Šäººé–“ã‚‰ã—ã„è¦ç´„</li>
</ul>
</li>
<li>çŸ­æ‰€:
<ul>
<li>äº‹å®Ÿã®èª¤ã‚Šï¼ˆHallucinationï¼‰ã®ãƒªã‚¹ã‚¯</li>
<li>è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„</li>
<li>å¤§é‡ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦</li>
</ul>
</li>
</ul>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Œæˆã•ã›ã¦ã€ã‚«ã‚¹ã‚¿ãƒ æ„Ÿæƒ…åˆ†æå™¨ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯æ˜ ç”»ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ã—ã€Positive/Negativeã®2ã‚¯ãƒ©ã‚¹åˆ†é¡ã‚’è¡Œã„ã¾ã™ã€‚</p>

<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Œæˆã•ã›ã¦ãã ã•ã„ï¼‰
reviews = [
    # Positive reviews (å°‘ãªãã¨ã‚‚5å€‹)

    # Negative reviews (å°‘ãªãã¨ã‚‚5å€‹)
]
labels = []  # å¯¾å¿œã™ã‚‹ãƒ©ãƒ™ãƒ«

# ãƒ¢ãƒ‡ãƒ«å®Ÿè£…ï¼ˆå®Œæˆã•ã›ã¦ãã ã•ã„ï¼‰
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
reviews = [
    # Positive reviews
    "This movie is absolutely fantastic! Loved it!",
    "Amazing performances and brilliant storyline.",
    "One of the best films I've ever seen.",
    "Highly recommended. A true masterpiece!",
    "Wonderful cinematography and great acting.",
    "Excellent movie with a heartwarming message.",
    "Phenomenal! Must-see for everyone.",

    # Negative reviews
    "Terrible film. Complete waste of time.",
    "Boring and poorly executed.",
    "Very disappointing. Would not recommend.",
    "Awful story and weak performances.",
    "Dull and uninspiring throughout.",
    "Poor quality. Not worth watching.",
    "Complete disaster. Avoid at all costs."
]

labels = [1, 1, 1, 1, 1, 1, 1,  # Positive
          0, 0, 0, 0, 0, 0, 0]  # Negative

print("=== ã‚«ã‚¹ã‚¿ãƒ æ„Ÿæƒ…åˆ†æå™¨ ===\n")
print(f"ãƒ‡ãƒ¼ã‚¿æ•°: {len(reviews)}")
print(f"Positive: {sum(labels)}, Negative: {len(labels) - sum(labels)}\n")

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    reviews, labels, test_size=0.3, random_state=42, stratify=labels
)

# TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(X_train_tfidf, y_train)

# è©•ä¾¡
y_pred = model.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)

print("=== ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ ===")
print(f"Accuracy: {accuracy:.3f}\n")
print("Classification Report:")
print(classification_report(y_test, y_pred,
                           target_names=['Negative', 'Positive']))

# æ–°ã—ã„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§äºˆæ¸¬
new_reviews = [
    "Incredible movie! Best I've seen this year!",
    "Absolutely terrible. Don't waste your money."
]

new_tfidf = vectorizer.transform(new_reviews)
predictions = model.predict(new_tfidf)
probabilities = model.predict_proba(new_tfidf)

print("\n=== æ–°ã—ã„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®äºˆæ¸¬ ===")
for review, pred, prob in zip(new_reviews, predictions, probabilities):
    sentiment = "Positive" if pred == 1 else "Negative"
    confidence = prob[pred]
    print(f"Review: {review}")
    print(f"  â†’ {sentiment} (confidence: {confidence:.1%})\n")
</code></pre>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>BIO ã‚¿ã‚®ãƒ³ã‚°æ–¹å¼ã‚’ä½¿ã£ã¦ã€ä»¥ä¸‹ã®æ–‡ã«NERãƒ©ãƒ™ãƒ«ã‚’ä»˜ã‘ã¦ãã ã•ã„ã€‚</p>
<p>æ–‡ï¼šã€ŒApple Inc. CEO Tim Cook visited Tokyo on October 21, 2025.ã€</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<table>
<thead>
<tr>
<th>Token</th>
<th>BIO Tag</th>
<th>Entity Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>Apple</td>
<td>B-ORG</td>
<td>Organization (Begin)</td>
</tr>
<tr>
<td>Inc.</td>
<td>I-ORG</td>
<td>Organization (Inside)</td>
</tr>
<tr>
<td>CEO</td>
<td>O</td>
<td>Outside</td>
</tr>
<tr>
<td>Tim</td>
<td>B-PER</td>
<td>Person (Begin)</td>
</tr>
<tr>
<td>Cook</td>
<td>I-PER</td>
<td>Person (Inside)</td>
</tr>
<tr>
<td>visited</td>
<td>O</td>
<td>Outside</td>
</tr>
<tr>
<td>Tokyo</td>
<td>B-LOC</td>
<td>Location (Begin)</td>
</tr>
<tr>
<td>on</td>
<td>O</td>
<td>Outside</td>
</tr>
<tr>
<td>October</td>
<td>B-DATE</td>
<td>Date (Begin)</td>
</tr>
<tr>
<td>21</td>
<td>I-DATE</td>
<td>Date (Inside)</td>
</tr>
<tr>
<td>,</td>
<td>I-DATE</td>
<td>Date (Inside)</td>
</tr>
<tr>
<td>2025</td>
<td>I-DATE</td>
<td>Date (Inside)</td>
</tr>
<tr>
<td>.</td>
<td>O</td>
<td>Outside</td>
</tr>
</tbody>
</table>

<p><strong>ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¾ã¨ã‚</strong>ï¼š</p>
<ul>
<li>ORG: Apple Inc.</li>
<li>PER: Tim Cook</li>
<li>LOC: Tokyo</li>
<li>DATE: October 21, 2025</li>
</ul>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>Retrieval-based QAã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚è¤‡æ•°ã®æ–‡æ›¸ã‹ã‚‰è³ªå•ã«é–¢é€£ã™ã‚‹æ–‡æ›¸ã‚’æ¤œç´¢ã—ã€ãã®æ–‡æ›¸ã‚’ä½¿ã£ã¦å›ç­”ã‚’ç”Ÿæˆã™ã‚‹ä»•çµ„ã¿ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">from transformers import pipeline, AutoTokenizer, AutoModel
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class SimpleRetrievalQA:
    def __init__(self, documents):
        self.documents = documents

        # æ–‡æ›¸åŸ‹ã‚è¾¼ã¿ç”¨ãƒ¢ãƒ‡ãƒ«
        self.tokenizer = AutoTokenizer.from_pretrained(
            'sentence-transformers/all-MiniLM-L6-v2'
        )
        self.encoder = AutoModel.from_pretrained(
            'sentence-transformers/all-MiniLM-L6-v2'
        )

        # QAãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        self.qa = pipeline(
            "question-answering",
            model="deepset/bert-base-cased-squad2"
        )

        # æ–‡æ›¸ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        print("æ–‡æ›¸ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­...")
        self.doc_embeddings = self._encode_documents()
        print(f"å®Œäº†ï¼ {len(documents)}å€‹ã®æ–‡æ›¸ã‚’æº–å‚™")

    def _encode_text(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
        inputs = self.tokenizer(
            text, return_tensors='pt',
            truncation=True, padding=True, max_length=512
        )
        with torch.no_grad():
            outputs = self.encoder(**inputs)
        # Mean pooling
        return outputs.last_hidden_state.mean(dim=1).numpy()

    def _encode_documents(self):
        """å…¨æ–‡æ›¸ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
        return np.vstack([self._encode_text(doc) for doc in self.documents])

    def retrieve(self, query, top_k=2):
        """é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢"""
        query_emb = self._encode_text(query)
        similarities = cosine_similarity(query_emb, self.doc_embeddings)[0]
        top_indices = np.argsort(similarities)[::-1][:top_k]

        return [
            {
                'doc': self.documents[i],
                'similarity': similarities[i],
                'index': i
            }
            for i in top_indices
        ]

    def answer(self, question, top_k=2):
        """è³ªå•ã«å›ç­”"""
        # é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢
        docs = self.retrieve(question, top_k)

        # æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„æ–‡æ›¸ã§å›ç­”
        best_doc = docs[0]['doc']
        result = self.qa(question=question, context=best_doc)

        return {
            'question': question,
            'answer': result['answer'],
            'confidence': result['score'],
            'source_doc_index': docs[0]['index'],
            'source_similarity': docs[0]['similarity'],
            'retrieved_docs': docs
        }

# æ–‡æ›¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
documents = [
    """Python is a high-level programming language created by Guido van Rossum.
    It was first released in 1991 and emphasizes code readability.""",

    """Machine learning is a subset of AI that enables systems to learn from data.
    Popular algorithms include decision trees and neural networks.""",

    """Deep learning uses neural networks with multiple layers. It excels at
    computer vision, NLP, and speech recognition tasks.""",

    """Natural language processing (NLP) deals with human-computer language
    interaction. Tasks include sentiment analysis and machine translation.""",

    """The Transformer architecture, introduced in 2017, revolutionized NLP
    with self-attention mechanisms. It led to BERT and GPT models."""
]

# ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã¨ä½¿ç”¨
print("=== Retrieval-based QA System ===\n")
qa_system = SimpleRetrievalQA(documents)

questions = [
    "Who created Python?",
    "What is deep learning good at?",
    "When was the Transformer introduced?"
]

for q in questions:
    print(f"\nQ: {q}")
    result = qa_system.answer(q)
    print(f"A: {result['answer']}")
    print(f"   Confidence: {result['confidence']:.1%}")
    print(f"   Source: Doc #{result['source_doc_index']} "
          f"(similarity: {result['source_similarity']:.3f})")
</code></pre>

</details>

<h3>å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>FastAPIã‚’ä½¿ã£ã¦ã€æ„Ÿæƒ…åˆ†æã€NERã€è¦ç´„ã®3ã¤ã®æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹REST APIã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã®çµ±ä¸€ã‚‚è€ƒæ…®ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python"># ãƒ•ã‚¡ã‚¤ãƒ«å: complete_nlp_api.py
from fastapi import FastAPI, HTTPException, status
from pydantic import BaseModel, validator
from transformers import pipeline
from typing import Optional, List, Dict, Any
import uvicorn
from datetime import datetime

app = FastAPI(
    title="Complete NLP API",
    description="æ„Ÿæƒ…åˆ†æã€NERã€è¦ç´„ã‚’æä¾›ã™ã‚‹API",
    version="1.0.0"
)

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
class TextInput(BaseModel):
    text: str

    @validator('text')
    def text_not_empty(cls, v):
        if not v or not v.strip():
            raise ValueError('ãƒ†ã‚­ã‚¹ãƒˆã¯ç©ºã«ã§ãã¾ã›ã‚“')
        return v

class SummarizeInput(TextInput):
    max_length: Optional[int] = 100
    min_length: Optional[int] = 30

    @validator('max_length')
    def valid_max_length(cls, v):
        if v < 10 or v > 500:
            raise ValueError('max_lengthã¯10-500ã®ç¯„å›²ã§æŒ‡å®šã—ã¦ãã ã•ã„')
        return v

# ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«
class APIResponse(BaseModel):
    success: bool
    timestamp: str
    data: Optional[Dict[Any, Any]] = None
    error: Optional[str] = None

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
print("ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
sentiment_analyzer = pipeline("sentiment-analysis")
ner_pipeline = pipeline("ner", aggregation_strategy="simple")
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
print("å®Œäº†ï¼")

# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
def create_response(success: bool, data: Dict = None, error: str = None) -> APIResponse:
    """çµ±ä¸€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä½œæˆ"""
    return APIResponse(
        success=success,
        timestamp=datetime.utcnow().isoformat(),
        data=data,
        error=error
    )

# ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/")
async def root():
    return create_response(
        success=True,
        data={
            "message": "Complete NLP API",
            "endpoints": {
                "sentiment": "/api/sentiment",
                "ner": "/api/ner",
                "summarize": "/api/summarize",
                "health": "/health"
            }
        }
    )

@app.post("/api/sentiment", response_model=APIResponse)
async def analyze_sentiment(input_data: TextInput):
    """æ„Ÿæƒ…åˆ†æ"""
    try:
        result = sentiment_analyzer(input_data.text[:512])[0]
        return create_response(
            success=True,
            data={
                "text": input_data.text,
                "sentiment": result['label'],
                "confidence": round(result['score'], 4)
            }
        )
    except Exception as e:
        return create_response(
            success=False,
            error=f"æ„Ÿæƒ…åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
        )

@app.post("/api/ner", response_model=APIResponse)
async def extract_entities(input_data: TextInput):
    """å›ºæœ‰è¡¨ç¾èªè­˜"""
    try:
        entities = ner_pipeline(input_data.text)
        return create_response(
            success=True,
            data={
                "text": input_data.text,
                "entities": [
                    {
                        "text": ent['word'],
                        "type": ent['entity_group'],
                        "confidence": round(ent['score'], 4)
                    }
                    for ent in entities
                ],
                "count": len(entities)
            }
        )
    except Exception as e:
        return create_response(
            success=False,
            error=f"NERã‚¨ãƒ©ãƒ¼: {str(e)}"
        )

@app.post("/api/summarize", response_model=APIResponse)
async def summarize_text(input_data: SummarizeInput):
    """ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„"""
    try:
        if len(input_data.text.split()) < 30:
            return create_response(
                success=False,
                error="ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã¾ã™ï¼ˆæœ€ä½30èªå¿…è¦ï¼‰"
            )

        summary = summarizer(
            input_data.text,
            max_length=input_data.max_length,
            min_length=input_data.min_length,
            do_sample=False
        )

        return create_response(
            success=True,
            data={
                "original_text": input_data.text,
                "summary": summary[0]['summary_text'],
                "original_length": len(input_data.text.split()),
                "summary_length": len(summary[0]['summary_text'].split()),
                "compression_ratio": round(
                    len(summary[0]['summary_text']) / len(input_data.text), 3
                )
            }
        )
    except Exception as e:
        return create_response(
            success=False,
            error=f"è¦ç´„ã‚¨ãƒ©ãƒ¼: {str(e)}"
        )

@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return create_response(
        success=True,
        data={
            "status": "healthy",
            "models": {
                "sentiment": "loaded",
                "ner": "loaded",
                "summarizer": "loaded"
            }
        }
    )

if __name__ == "__main__":
    print("\n=== Complete NLP API èµ·å‹• ===")
    print("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: http://localhost:8000/docs")
    print("API: http://localhost:8000/")
    uvicorn.run(app, host="0.0.0.0", port=8000)
</code></pre>

<p><strong>ä½¿ç”¨ä¾‹ï¼ˆPythonã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼‰</strong>ï¼š</p>
<pre><code class="language-python">import requests

API_URL = "http://localhost:8000"

# æ„Ÿæƒ…åˆ†æ
response = requests.post(
    f"{API_URL}/api/sentiment",
    json={"text": "This API is amazing!"}
)
print("Sentiment:", response.json())

# NER
response = requests.post(
    f"{API_URL}/api/ner",
    json={"text": "Apple Inc. CEO Tim Cook visited Tokyo."}
)
print("\nNER:", response.json())

# è¦ç´„
long_text = """
Artificial intelligence has made remarkable progress...
(é•·ã„ãƒ†ã‚­ã‚¹ãƒˆ)
"""
response = requests.post(
    f"{API_URL}/api/summarize",
    json={"text": long_text, "max_length": 80}
)
print("\nSummary:", response.json())
</code></pre>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Devlin, J., et al. (2019). <em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em>. NAACL.</li>
<li>Lewis, M., et al. (2020). <em>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</em>. ACL.</li>
<li>Raffel, C., et al. (2020). <em>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)</em>. JMLR.</li>
<li>Rajpurkar, P., et al. (2016). <em>SQuAD: 100,000+ Questions for Machine Comprehension of Text</em>. EMNLP.</li>
<li>Mihalcea, R., & Tarau, P. (2004). <em>TextRank: Bringing Order into Text</em>. EMNLP.</li>
<li>Lample, G., et al. (2016). <em>Neural Architectures for Named Entity Recognition</em>. NAACL.</li>
<li>Socher, R., et al. (2013). <em>Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</em>. EMNLP.</li>
<li>Jurafsky, D., & Martin, J. H. (2023). <em>Speech and Language Processing</em> (3rd ed.). Prentice Hall.</li>
</ol>

<div class="navigation">
    <a href="chapter4-transformers-bert.html" class="nav-button">â† å‰ã®ç« : Transformers & BERT</a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a>
</div>

    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
