<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 4: å®Ÿä¸–ç•Œã®NLPå¿œç”¨ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Chapter 4: å®Ÿä¸–ç•Œã®NLPå¿œç”¨</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 0å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 0å•</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 4: å®Ÿä¸–ç•Œã®NLPå¿œç”¨</h1>
<h2>æœ¬ç« ã®æ¦‚è¦</h2>
<p>ã“ã‚Œã¾ã§å­¦ã‚“ã NLPæŠ€è¡“ãŒã€å®Ÿéš›ã®ãƒ“ã‚¸ãƒã‚¹ã‚„æ—¥å¸¸ç”Ÿæ´»ã§ã©ã®ã‚ˆã†ã«æ´»ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’å­¦ã³ã¾ã™ã€‚</p>
<p>æœ¬ç« ã§ã¯ã€æƒ…å ±æŠ½å‡ºã€è³ªå•å¿œç­”ã€ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ã€æ©Ÿæ¢°ç¿»è¨³ãªã©ã®å®Ÿç”¨çš„ãªNLPã‚¿ã‚¹ã‚¯ã¨ã€æœ€æ–°ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®å‹•å‘ã‚’æ‰±ã„ã¾ã™ã€‚</p>
<h3>å­¦ç¿’ç›®æ¨™</h3>
<ul>
<li>âœ… å®Ÿç”¨çš„ãªNLPã‚·ã‚¹ãƒ†ãƒ ã®ä»•çµ„ã¿ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… å„ã‚¿ã‚¹ã‚¯ã®å¿œç”¨ä¾‹ã¨ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ã‚’æŠŠæ¡ã™ã‚‹</li>
<li>âœ… æœ€æ–°ã®NLPæŠ€è¡“å‹•å‘ï¼ˆLLMã€ChatGPTç­‰ï¼‰ã‚’çŸ¥ã‚‹</li>
<li>âœ… é©åˆ‡ãªNLPã‚¿ã‚¹ã‚¯ã‚’é¸æŠã§ãã‚‹ã‚ˆã†ã«ãªã‚‹</li>
</ul>
<hr />
<h2>1. æƒ…å ±æŠ½å‡ºï¼ˆInformation Extractionï¼‰</h2>
<h3>1.1 æƒ…å ±æŠ½å‡ºã¨ã¯</h3>
<p><strong>æƒ…å ±æŠ½å‡ºï¼ˆIE: Information Extractionï¼‰</strong> ã¯ã€éæ§‹é€ åŒ–ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ§‹é€ åŒ–ã•ã‚ŒãŸæƒ…å ±ã‚’è‡ªå‹•çš„ã«æŠ½å‡ºã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚</p>
<h4>ä¸»ãªã‚¿ã‚¹ã‚¯</h4>
<div class="mermaid">
graph TD
    A[éæ§‹é€ åŒ–ãƒ†ã‚­ã‚¹ãƒˆ] --> B[å›ºæœ‰è¡¨ç¾æŠ½å‡º NER]
    A --> C[é–¢ä¿‚æŠ½å‡º RE]
    A --> D[ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡º]
    B --> E[æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿]
    C --> E
    D --> E
</div>

<table>
<thead>
<tr>
<th>ã‚¿ã‚¹ã‚¯</th>
<th>èª¬æ˜</th>
<th>ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å›ºæœ‰è¡¨ç¾æŠ½å‡º (NER)</strong></td>
<td>äººåã€åœ°åã€çµ„ç¹”åãªã©ã‚’æŠ½å‡º</td>
<td>"ç”°ä¸­å¤ªéƒã•ã‚“" â†’ PERSON</td>
</tr>
<tr>
<td><strong>é–¢ä¿‚æŠ½å‡º (RE)</strong></td>
<td>ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–“ã®é–¢ä¿‚ã‚’æŠ½å‡º</td>
<td>"ç”°ä¸­å¤ªéƒã¯æ±äº¬å¤§å­¦ã‚’å’æ¥­" â†’ (ç”°ä¸­å¤ªéƒ, å’æ¥­, æ±äº¬å¤§å­¦)</td>
</tr>
<tr>
<td><strong>ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡º</strong></td>
<td>å‡ºæ¥äº‹ã¨ãã®å‚åŠ è€…ã‚’æŠ½å‡º</td>
<td>"AppleãŒæ–°è£½å“ã‚’ç™ºè¡¨" â†’ (ç™ºè¡¨, ä¸»ä½“:Apple, å¯¾è±¡:æ–°è£½å“)</td>
</tr>
</tbody>
</table>
<h3>1.2 å®Ÿè£…: ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‹ã‚‰ã®æƒ…å ±æŠ½å‡º</h3>
<pre><code class="language-python">import spacy
import re
from typing import List, Dict, Tuple

class NewsInfoExtractor:
    &quot;&quot;&quot;ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‹ã‚‰ã®æƒ…å ±æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ &quot;&quot;&quot;

    def __init__(self, model_name='ja_ginza'):
        &quot;&quot;&quot;
        Parameters:
        -----------
        model_name : str
            spaCyãƒ¢ãƒ‡ãƒ«åï¼ˆæ—¥æœ¬èª: 'ja_ginza', è‹±èª: 'en_core_web_sm'ï¼‰
        &quot;&quot;&quot;
        self.nlp = spacy.load(model_name)

    def extract_named_entities(self, text: str) -&gt; Dict[str, List[str]]:
        &quot;&quot;&quot;å›ºæœ‰è¡¨ç¾æŠ½å‡º&quot;&quot;&quot;
        doc = self.nlp(text)

        entities = {
            'PERSON': [],      # äººå
            'ORG': [],         # çµ„ç¹”å
            'GPE': [],         # åœ°åï¼ˆå›½ãƒ»éƒ½å¸‚ï¼‰
            'DATE': [],        # æ—¥ä»˜
            'MONEY': [],       # é‡‘é¡
            'PRODUCT': [],     # è£½å“å
        }

        for ent in doc.ents:
            if ent.label_ in entities:
                # é‡è¤‡ã‚’é¿ã‘ã‚‹
                if ent.text not in entities[ent.label_]:
                    entities[ent.label_].append(ent.text)

        return entities

    def extract_relationships(self, text: str) -&gt; List[Tuple[str, str, str]]:
        &quot;&quot;&quot;é–¢ä¿‚æŠ½å‡ºï¼ˆä¸»èª-è¿°èª-ç›®çš„èªï¼‰&quot;&quot;&quot;
        doc = self.nlp(text)
        relations = []

        for sent in doc.sents:
            # ä¸»èªã‚’æ¢ã™
            subject = None
            verb = None
            obj = None

            for token in sent:
                # ä¸»èªï¼ˆåè©ã§ã€ä¸»æ ¼åŠ©è©ã€ŒãŒã€ã€Œã¯ã€ãŒä»˜ãï¼‰
                if token.dep_ in ['nsubj', 'nsubjpass'] and token.pos_ in ['NOUN', 'PROPN']:
                    subject = token.text

                # è¿°èªï¼ˆå‹•è©ï¼‰
                if token.pos_ == 'VERB' and token.dep_ == 'ROOT':
                    verb = token.lemma_  # åŸºæœ¬å½¢

                # ç›®çš„èªï¼ˆåè©ã§ã€å¯¾æ ¼åŠ©è©ã€Œã‚’ã€ãŒä»˜ãï¼‰
                if token.dep_ == 'obj' and token.pos_ in ['NOUN', 'PROPN']:
                    obj = token.text

            # ä¸»èªãƒ»è¿°èªãƒ»ç›®çš„èªãŒæƒã£ãŸå ´åˆ
            if subject and verb and obj:
                relations.append((subject, verb, obj))

        return relations

    def extract_dates(self, text: str) -&gt; List[str]:
        &quot;&quot;&quot;æ—¥ä»˜è¡¨ç¾ã®æŠ½å‡º&quot;&quot;&quot;
        # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns = [
            r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
            r'\d{1,2}/\d{1,2}/\d{4}',
            r'\d{4}-\d{2}-\d{2}',
        ]

        dates = []
        for pattern in patterns:
            matches = re.findall(pattern, text)
            dates.extend(matches)

        return dates

    def extract_money(self, text: str) -&gt; List[str]:
        &quot;&quot;&quot;é‡‘é¡è¡¨ç¾ã®æŠ½å‡º&quot;&quot;&quot;
        # é‡‘é¡ãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns = [
            r'\d+å„„å††',
            r'\d+ä¸‡å††',
            r'\d+å††',
            r'Â¥\d+',
            r'\$\d+'
        ]

        money = []
        for pattern in patterns:
            matches = re.findall(pattern, text)
            money.extend(matches)

        return money

    def analyze_news(self, text: str) -&gt; Dict:
        &quot;&quot;&quot;ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ç·åˆåˆ†æ&quot;&quot;&quot;
        return {
            'entities': self.extract_named_entities(text),
            'relationships': self.extract_relationships(text),
            'dates': self.extract_dates(text),
            'money': self.extract_money(text)
        }

# ä½¿ç”¨ä¾‹
if __name__ == &quot;__main__&quot;:
    extractor = NewsInfoExtractor()

    news_text = &quot;&quot;&quot;
    2025å¹´10æœˆ20æ—¥ã€æ±äº¬ã§é–‹å‚¬ã•ã‚ŒãŸæŠ€è¡“ã‚«ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã«ã¦ã€
    æ ªå¼ä¼šç¤¾ãƒ†ãƒƒã‚¯ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãŒæ–°å‹AIãƒãƒƒãƒ—ã‚’ç™ºè¡¨ã—ãŸã€‚
    åŒç¤¾ã®ç”°ä¸­å¤ªéƒCEOã¯ã€ã€Œã“ã®è£½å“ã¯æ¥­ç•Œã‚’å¤‰é©ã™ã‚‹ã€ã¨è¿°ã¹ãŸã€‚
    é–‹ç™ºè²»ã¯ç´„100å„„å††ã§ã€æ¥å¹´æ˜¥ã«è²©å£²é–‹å§‹äºˆå®šã€‚
    &quot;&quot;&quot;

    result = extractor.analyze_news(news_text)

    print(&quot;ã€å›ºæœ‰è¡¨ç¾ã€‘&quot;)
    for entity_type, entities in result['entities'].items():
        if entities:
            print(f&quot;  {entity_type}: {', '.join(entities)}&quot;)

    print(&quot;\nã€é–¢ä¿‚ã€‘&quot;)
    for subj, verb, obj in result['relationships']:
        print(f&quot;  {subj} â†’ {verb} â†’ {obj}&quot;)

    print(&quot;\nã€æ—¥ä»˜ã€‘&quot;)
    print(f&quot;  {', '.join(result['dates'])}&quot;)

    print(&quot;\nã€é‡‘é¡ã€‘&quot;)
    print(f&quot;  {', '.join(result['money'])}&quot;)
</code></pre>
<p><strong>å‡ºåŠ›ä¾‹:</strong></p>
<pre><code>ã€å›ºæœ‰è¡¨ç¾ã€‘
  ORG: æ ªå¼ä¼šç¤¾ãƒ†ãƒƒã‚¯ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³
  PERSON: ç”°ä¸­å¤ªéƒ
  GPE: æ±äº¬
  DATE: 2025å¹´10æœˆ20æ—¥
  PRODUCT: AIãƒãƒƒãƒ—

ã€é–¢ä¿‚ã€‘
  æ ªå¼ä¼šç¤¾ãƒ†ãƒƒã‚¯ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ â†’ ç™ºè¡¨ â†’ AIãƒãƒƒãƒ—
  ç”°ä¸­å¤ªéƒ â†’ è¿°ã¹ã‚‹ â†’ è£½å“

ã€æ—¥ä»˜ã€‘
  2025å¹´10æœˆ20æ—¥

ã€é‡‘é¡ã€‘
  100å„„å††
</code></pre>
<h3>1.3 å¿œç”¨ä¾‹</h3>
<h4>ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆè‡ªå‹•åŒ–</h4>
<pre><code class="language-python">class SupportTicketExtractor:
    &quot;&quot;&quot;å•ã„åˆã‚ã›ãƒã‚±ãƒƒãƒˆã‹ã‚‰ã®æƒ…å ±æŠ½å‡º&quot;&quot;&quot;

    def __init__(self):
        self.nlp = spacy.load('ja_ginza')

        # å•ã„åˆã‚ã›ã‚«ãƒ†ã‚´ãƒªã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.category_keywords = {
            'é…é€': ['é…é€', 'å±Šã‹ãªã„', 'é…ã„', 'è¿½è·¡'],
            'è¿”é‡‘': ['è¿”é‡‘', 'è¿”å“', 'ã‚­ãƒ£ãƒ³ã‚»ãƒ«'],
            'ä¸å…·åˆ': ['å£Šã‚ŒãŸ', 'å‹•ã‹ãªã„', 'ã‚¨ãƒ©ãƒ¼', 'ä¸è‰¯å“'],
            'ä½¿ã„æ–¹': ['ä½¿ã„æ–¹', 'è¨­å®š', 'æ–¹æ³•', 'ã‚„ã‚Šæ–¹']
        }

    def extract_category(self, text: str) -&gt; str:
        &quot;&quot;&quot;å•ã„åˆã‚ã›ã‚«ãƒ†ã‚´ãƒªã®æ¨å®š&quot;&quot;&quot;
        text_lower = text.lower()

        scores = {}
        for category, keywords in self.category_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            scores[category] = score

        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚«ãƒ†ã‚´ãƒªã‚’è¿”ã™
        return max(scores, key=scores.get) if max(scores.values()) &gt; 0 else 'ä¸€èˆ¬'

    def extract_product_name(self, text: str) -&gt; str:
        &quot;&quot;&quot;è£½å“åã®æŠ½å‡º&quot;&quot;&quot;
        doc = self.nlp(text)
        for ent in doc.ents:
            if ent.label_ == 'PRODUCT':
                return ent.text
        return None

    def analyze_ticket(self, text: str) -&gt; Dict:
        &quot;&quot;&quot;ãƒã‚±ãƒƒãƒˆã®åˆ†æ&quot;&quot;&quot;
        return {
            'category': self.extract_category(text),
            'product': self.extract_product_name(text),
            'entities': NewsInfoExtractor().extract_named_entities(text)
        }

# ä½¿ç”¨ä¾‹
extractor = SupportTicketExtractor()
ticket_text = &quot;å…ˆé€±æ³¨æ–‡ã—ãŸãƒãƒ¼ãƒˆPCãŒå±Šãã¾ã›ã‚“ã€‚é…é€çŠ¶æ³ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚&quot;

result = extractor.analyze_ticket(ticket_text)
print(f&quot;ã‚«ãƒ†ã‚´ãƒª: {result['category']}&quot;)
print(f&quot;è£½å“: {result['product']}&quot;)
</code></pre>
<hr />
<h2>2. è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ï¼ˆQuestion Answeringï¼‰</h2>
<h3>2.1 QAã‚·ã‚¹ãƒ†ãƒ ã®ç¨®é¡</h3>
<table>
<thead>
<tr>
<th>ç¨®é¡</th>
<th>èª¬æ˜</th>
<th>ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æŠ½å‡ºå‹QA</strong></td>
<td>æ–‡æ›¸ã‹ã‚‰ç­”ãˆã‚’æŠ½å‡º</td>
<td>"å‰µæ¥­è€…ã¯èª°ï¼Ÿ" â†’ "ã‚¹ãƒ†ã‚£ãƒ¼ãƒ–ãƒ»ã‚¸ãƒ§ãƒ–ã‚º"</td>
</tr>
<tr>
<td><strong>ç”Ÿæˆå‹QA</strong></td>
<td>ç­”ãˆã‚’ç”Ÿæˆ</td>
<td>"ãªãœäººæ°—ï¼Ÿ" â†’ "ãƒ‡ã‚¶ã‚¤ãƒ³ãŒå„ªã‚Œã¦ã„ã‚‹ãŸã‚"</td>
</tr>
<tr>
<td><strong>ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‰ãƒ¡ã‚¤ãƒ³QA</strong></td>
<td>å¹…åºƒã„åˆ†é‡ã®è³ªå•</td>
<td>Googleæ¤œç´¢ã®ã‚ˆã†ãªæ±ç”¨QA</td>
</tr>
<tr>
<td><strong>ã‚¯ãƒ­ãƒ¼ã‚ºãƒ‰ãƒ‰ãƒ¡ã‚¤ãƒ³QA</strong></td>
<td>ç‰¹å®šåˆ†é‡ã«ç‰¹åŒ–</td>
<td>ç¤¾å†…FAQã‚·ã‚¹ãƒ†ãƒ </td>
</tr>
</tbody>
</table>
<h3>2.2 å®Ÿè£…: æ–‡æ›¸ãƒ™ãƒ¼ã‚¹QAã‚·ã‚¹ãƒ†ãƒ </h3>
<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class DocumentQASystem:
    &quot;&quot;&quot;æ–‡æ›¸ãƒ™ãƒ¼ã‚¹ã®è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ &quot;&quot;&quot;

    def __init__(self, documents: List[str]):
        &quot;&quot;&quot;
        Parameters:
        -----------
        documents : List[str]
            çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹æ–‡æ›¸ã®ãƒªã‚¹ãƒˆ
        &quot;&quot;&quot;
        self.documents = documents
        self.vectorizer = TfidfVectorizer()
        self.doc_vectors = self.vectorizer.fit_transform(documents)

    def answer_question(self, question: str, top_k: int = 3) -&gt; List[Tuple[str, float]]:
        &quot;&quot;&quot;
        è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’æ¤œç´¢

        Parameters:
        -----------
        question : str
            è³ªå•æ–‡
        top_k : int
            ä¸Šä½ä½•ä»¶ã‚’è¿”ã™ã‹

        Returns:
        --------
        List[Tuple[str, float]]
            (å›ç­”æ–‡, é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢) ã®ãƒªã‚¹ãƒˆ
        &quot;&quot;&quot;
        # è³ªå•ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        question_vector = self.vectorizer.transform([question])

        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
        similarities = cosine_similarity(question_vector, self.doc_vectors)[0]

        # é¡ä¼¼åº¦ãŒé«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
        top_indices = np.argsort(similarities)[::-1][:top_k]

        results = [
            (self.documents[idx], similarities[idx])
            for idx in top_indices
        ]

        return results

# ä½¿ç”¨ä¾‹
if __name__ == &quot;__main__&quot;:
    # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ï¼ˆFAQãªã©ï¼‰
    knowledge_base = [
        &quot;å–¶æ¥­æ™‚é–“ã¯å¹³æ—¥9æ™‚ã‹ã‚‰18æ™‚ã¾ã§ã§ã™ã€‚&quot;,
        &quot;è¿”å“ã¯è³¼å…¥å¾Œ30æ—¥ä»¥å†…ã§ã‚ã‚Œã°å¯èƒ½ã§ã™ã€‚&quot;,
        &quot;é€æ–™ã¯å…¨å›½ä¸€å¾‹500å††ã§ã™ã€‚5000å††ä»¥ä¸Šã®ã”è³¼å…¥ã§ç„¡æ–™ã«ãªã‚Šã¾ã™ã€‚&quot;,
        &quot;ãŠæ”¯æ‰•ã„æ–¹æ³•ã¯ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã€éŠ€è¡ŒæŒ¯è¾¼ã€ä»£é‡‘å¼•æ›ãŒåˆ©ç”¨ã§ãã¾ã™ã€‚&quot;,
        &quot;ä¼šå“¡ç™»éŒ²ã™ã‚‹ã¨ãƒã‚¤ãƒ³ãƒˆãŒè²¯ã¾ã‚Šã€æ¬¡å›è³¼å…¥æ™‚ã«ä½¿ç”¨ã§ãã¾ã™ã€‚&quot;,
        &quot;ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å¿˜ã‚ŒãŸå ´åˆã¯ã€ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢ã®ã€Œãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å¿˜ã‚ŒãŸæ–¹ã€ã‹ã‚‰ãƒªã‚»ãƒƒãƒˆã§ãã¾ã™ã€‚&quot;,
    ]

    qa_system = DocumentQASystem(knowledge_base)

    # è³ªå•
    questions = [
        &quot;é€æ–™ã¯ã„ãã‚‰ã§ã™ã‹ï¼Ÿ&quot;,
        &quot;è¿”å“ã§ãã¾ã™ã‹ï¼Ÿ&quot;,
        &quot;ä½•æ™‚ã¾ã§å–¶æ¥­ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ&quot;
    ]

    for question in questions:
        print(f&quot;\nè³ªå•: {question}&quot;)
        answers = qa_system.answer_question(question, top_k=1)

        for answer, score in answers:
            print(f&quot;å›ç­”: {answer}&quot;)
            print(f&quot;ã‚¹ã‚³ã‚¢: {score:.4f}&quot;)
</code></pre>
<h3>2.3 Transformer ãƒ™ãƒ¼ã‚¹ QAï¼ˆBERTï¼‰</h3>
<pre><code class="language-python">from transformers import pipeline

class BertQASystem:
    &quot;&quot;&quot;BERTã‚’ä½¿ã£ãŸæŠ½å‡ºå‹QAã‚·ã‚¹ãƒ†ãƒ &quot;&quot;&quot;

    def __init__(self, model_name='deepset/bert-base-cased-squad2'):
        &quot;&quot;&quot;
        Parameters:
        -----------
        model_name : str
            ä½¿ç”¨ã™ã‚‹BERTãƒ¢ãƒ‡ãƒ«
        &quot;&quot;&quot;
        self.qa_pipeline = pipeline('question-answering', model=model_name)

    def answer(self, question: str, context: str) -&gt; Dict:
        &quot;&quot;&quot;
        è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’æŠ½å‡º

        Parameters:
        -----------
        question : str
            è³ªå•æ–‡
        context : str
            æ–‡è„ˆï¼ˆç­”ãˆãŒå«ã¾ã‚Œã‚‹æ–‡æ›¸ï¼‰

        Returns:
        --------
        Dict
            answer: å›ç­”, score: ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
        &quot;&quot;&quot;
        result = self.qa_pipeline({
            'question': question,
            'context': context
        })

        return {
            'answer': result['answer'],
            'score': result['score'],
            'start': result['start'],
            'end': result['end']
        }

# ä½¿ç”¨ä¾‹ï¼ˆè‹±èªã®ä¾‹ï¼‰
if __name__ == &quot;__main__&quot;:
    qa = BertQASystem()

    context = &quot;&quot;&quot;
    Apple Inc. is an American multinational technology company
    headquartered in Cupertino, California. It was founded by
    Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976.
    &quot;&quot;&quot;

    question = &quot;Who founded Apple?&quot;

    result = qa.answer(question, context)
    print(f&quot;Question: {question}&quot;)
    print(f&quot;Answer: {result['answer']}&quot;)
    print(f&quot;Confidence: {result['score']:.4f}&quot;)
</code></pre>
<hr />
<h2>3. ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ï¼ˆText Summarizationï¼‰</h2>
<h3>3.1 è¦ç´„ã®ç¨®é¡</h3>
<h4>æŠ½å‡ºå‹è¦ç´„ (Extractive Summarization)</h4>
<p>é‡è¦ãªæ–‡ã‚’æŠ½å‡ºã—ã¦è¦ç´„ã‚’ä½œæˆ</p>
<div class="mermaid">
graph LR
    A[å…ƒã®æ–‡æ›¸] --> B[æ–‡ã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°]
    B --> C[ä¸Šä½Næ–‡ã‚’é¸æŠ]
    C --> D[è¦ç´„æ–‡]
</div>

<h4>ç”Ÿæˆå‹è¦ç´„ (Abstractive Summarization)</h4>
<p>å†…å®¹ã‚’ç†è§£ã—ã¦æ–°ã—ã„æ–‡ã‚’ç”Ÿæˆ</p>
<div class="mermaid">
graph LR
    A[å…ƒã®æ–‡æ›¸] --> B[æ„å‘³ç†è§£]
    B --> C[æ–°ã—ã„æ–‡ã‚’ç”Ÿæˆ]
    C --> D[è¦ç´„æ–‡]
</div>

<h3>3.2 å®Ÿè£…: æŠ½å‡ºå‹è¦ç´„</h3>
<pre><code class="language-python">import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx

class ExtractiveSummarizer:
    &quot;&quot;&quot;æŠ½å‡ºå‹ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ã‚·ã‚¹ãƒ†ãƒ &quot;&quot;&quot;

    def __init__(self):
        self.vectorizer = TfidfVectorizer()

    def text_rank_summarize(self, text: str, num_sentences: int = 3) -&gt; str:
        &quot;&quot;&quot;
        TextRankã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹è¦ç´„

        Parameters:
        -----------
        text : str
            è¦ç´„å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
        num_sentences : int
            æŠ½å‡ºã™ã‚‹æ–‡ã®æ•°

        Returns:
        --------
        str
            è¦ç´„æ–‡
        &quot;&quot;&quot;
        # æ–‡ã«åˆ†å‰²
        sentences = [s.strip() for s in text.split('ã€‚') if s.strip()]

        if len(sentences) &lt;= num_sentences:
            return text

        # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
        tfidf_matrix = self.vectorizer.fit_transform(sentences)

        # æ–‡é–“ã®é¡ä¼¼åº¦è¡Œåˆ—ã‚’è¨ˆç®—
        similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)

        # ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
        nx_graph = nx.from_numpy_array(similarity_matrix)

        # PageRankã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        scores = nx.pagerank(nx_graph)

        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        ranked_sentences = sorted(
            ((scores[i], s) for i, s in enumerate(sentences)),
            reverse=True
        )

        # ä¸Šä½Næ–‡ã‚’å…ƒã®é †åºã§æŠ½å‡º
        top_sentences = sorted(
            ranked_sentences[:num_sentences],
            key=lambda x: sentences.index(x[1])
        )

        summary = 'ã€‚'.join([sentence for _, sentence in top_sentences]) + 'ã€‚'
        return summary

    def tfidf_summarize(self, text: str, num_sentences: int = 3) -&gt; str:
        &quot;&quot;&quot;
        TF-IDFã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹è¦ç´„

        Parameters:
        -----------
        text : str
            è¦ç´„å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
        num_sentences : int
            æŠ½å‡ºã™ã‚‹æ–‡ã®æ•°

        Returns:
        --------
        str
            è¦ç´„æ–‡
        &quot;&quot;&quot;
        sentences = [s.strip() for s in text.split('ã€‚') if s.strip()]

        if len(sentences) &lt;= num_sentences:
            return text

        # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
        tfidf_matrix = self.vectorizer.fit_transform(sentences)

        # å„æ–‡ã®TF-IDFã‚¹ã‚³ã‚¢åˆè¨ˆã‚’è¨ˆç®—
        sentence_scores = np.array(tfidf_matrix.sum(axis=1)).flatten()

        # ã‚¹ã‚³ã‚¢ãŒé«˜ã„é †ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
        top_indices = np.argsort(sentence_scores)[::-1][:num_sentences]

        # å…ƒã®é †åºã§ã‚½ãƒ¼ãƒˆ
        top_indices = sorted(top_indices)

        # è¦ç´„æ–‡ã‚’ç”Ÿæˆ
        summary = 'ã€‚'.join([sentences[i] for i in top_indices]) + 'ã€‚'
        return summary

# ä½¿ç”¨ä¾‹
if __name__ == &quot;__main__&quot;:
    summarizer = ExtractiveSummarizer()

    long_text = &quot;&quot;&quot;
    è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNLPï¼‰ã¯ã€äººé–“ãŒæ—¥å¸¸çš„ã«ä½¿ã†è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§å‡¦ç†ã™ã‚‹æŠ€è¡“ã§ã™ã€‚
    NLPã«ã¯æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ãŒã‚ã‚Šã€æ©Ÿæ¢°ç¿»è¨³ã€æ„Ÿæƒ…åˆ†æã€è³ªå•å¿œç­”ãªã©ãŒå«ã¾ã‚Œã¾ã™ã€‚
    è¿‘å¹´ã€æ·±å±¤å­¦ç¿’ã®ç™ºå±•ã«ã‚ˆã‚Šã€NLPã®ç²¾åº¦ãŒå¤§ããå‘ä¸Šã—ã¾ã—ãŸã€‚
    ç‰¹ã«ã€Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç™»å ´ã¯é©å‘½çš„ã§ã—ãŸã€‚
    BERTã‚„GPTã¨ã„ã£ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ãŒé–‹ç™ºã•ã‚Œã€å¤šãã®ã‚¿ã‚¹ã‚¯ã§äººé–“ã«è¿‘ã„æ€§èƒ½ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚
    ç¾åœ¨ã§ã¯ã€ChatGPTã®ã‚ˆã†ãªå¯¾è©±å‹AIãŒåºƒãåˆ©ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
    ä»Šå¾Œã‚‚NLPæŠ€è¡“ã®é€²åŒ–ã¯ç¶šãã¨äºˆæƒ³ã•ã‚Œã¾ã™ã€‚
    &quot;&quot;&quot;

    print(&quot;ã€TextRankè¦ç´„ã€‘&quot;)
    summary1 = summarizer.text_rank_summarize(long_text, num_sentences=3)
    print(summary1)

    print(&quot;\nã€TF-IDFè¦ç´„ã€‘&quot;)
    summary2 = summarizer.tfidf_summarize(long_text, num_sentences=3)
    print(summary2)
</code></pre>
<h3>3.3 ç”Ÿæˆå‹è¦ç´„ï¼ˆTransformerãƒ™ãƒ¼ã‚¹ï¼‰</h3>
<pre><code class="language-python">from transformers import pipeline

class AbstractiveSummarizer:
    &quot;&quot;&quot;ç”Ÿæˆå‹ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ã‚·ã‚¹ãƒ†ãƒ &quot;&quot;&quot;

    def __init__(self, model_name='facebook/bart-large-cnn'):
        &quot;&quot;&quot;
        Parameters:
        -----------
        model_name : str
            ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        &quot;&quot;&quot;
        self.summarizer = pipeline('summarization', model=model_name)

    def summarize(self, text: str, max_length: int = 130,
                  min_length: int = 30) -&gt; str:
        &quot;&quot;&quot;
        ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„

        Parameters:
        -----------
        text : str
            è¦ç´„å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
        max_length : int
            è¦ç´„ã®æœ€å¤§é•·
        min_length : int
            è¦ç´„ã®æœ€å°é•·

        Returns:
        --------
        str
            è¦ç´„æ–‡
        &quot;&quot;&quot;
        summary = self.summarizer(
            text,
            max_length=max_length,
            min_length=min_length,
            do_sample=False
        )

        return summary[0]['summary_text']

# ä½¿ç”¨ä¾‹ï¼ˆè‹±èªï¼‰
if __name__ == &quot;__main__&quot;:
    summarizer = AbstractiveSummarizer()

    article = &quot;&quot;&quot;
    Artificial intelligence (AI) is intelligence demonstrated by machines,
    in contrast to the natural intelligence displayed by humans and animals.
    Leading AI textbooks define the field as the study of &quot;intelligent agents&quot;:
    any device that perceives its environment and takes actions that maximize
    its chance of successfully achieving its goals. Colloquially, the term
    &quot;artificial intelligence&quot; is often used to describe machines (or computers)
    that mimic &quot;cognitive&quot; functions that humans associate with the human mind,
    such as &quot;learning&quot; and &quot;problem solving&quot;.
    &quot;&quot;&quot;

    summary = summarizer.summarize(article)
    print(&quot;Summary:&quot;, summary)
</code></pre>
<hr />
<h2>4. æ©Ÿæ¢°ç¿»è¨³ï¼ˆMachine Translationï¼‰</h2>
<h3>4.1 æ©Ÿæ¢°ç¿»è¨³ã®æ­´å²</h3>
<div class="mermaid">
timeline
    title æ©Ÿæ¢°ç¿»è¨³ã®ç™ºå±•
    1950s-1980s : ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ç¿»è¨³ : æ–‡æ³•è¦å‰‡ã‚’æ‰‹å‹•å®šç¾©
    1990s-2000s : çµ±è¨ˆçš„æ©Ÿæ¢°ç¿»è¨³ : ã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰çµ±è¨ˆãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    2014-2016 : ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ©Ÿæ¢°ç¿»è¨³ : Seq2Seq + Attention
    2017-ç¾åœ¨ : Transformerç¿»è¨³ : é«˜ç²¾åº¦ãƒ»é«˜é€Ÿãªç¿»è¨³
</div>

<h3>4.2 å®Ÿè£…: Transformer ãƒ™ãƒ¼ã‚¹ç¿»è¨³</h3>
<pre><code class="language-python">from transformers import MarianMTModel, MarianTokenizer

class NeuralTranslator:
    &quot;&quot;&quot;ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ©Ÿæ¢°ç¿»è¨³ã‚·ã‚¹ãƒ†ãƒ &quot;&quot;&quot;

    def __init__(self, src_lang: str = 'en', tgt_lang: str = 'ja'):
        &quot;&quot;&quot;
        Parameters:
        -----------
        src_lang : str
            ç¿»è¨³å…ƒè¨€èªã‚³ãƒ¼ãƒ‰
        tgt_lang : str
            ç¿»è¨³å…ˆè¨€èªã‚³ãƒ¼ãƒ‰
        &quot;&quot;&quot;
        model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}'

        try:
            self.tokenizer = MarianTokenizer.from_pretrained(model_name)
            self.model = MarianMTModel.from_pretrained(model_name)
        except Exception as e:
            print(f&quot;ãƒ¢ãƒ‡ãƒ« {model_name} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}&quot;)
            raise

    def translate(self, text: str) -&gt; str:
        &quot;&quot;&quot;
        ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¿»è¨³

        Parameters:
        -----------
        text : str
            ç¿»è¨³ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
        --------
        str
            ç¿»è¨³ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        &quot;&quot;&quot;
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        inputs = self.tokenizer(text, return_tensors='pt', padding=True)

        # ç¿»è¨³
        translated = self.model.generate(**inputs)

        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        translated_text = self.tokenizer.decode(
            translated[0],
            skip_special_tokens=True
        )

        return translated_text

    def batch_translate(self, texts: List[str]) -&gt; List[str]:
        &quot;&quot;&quot;è¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆã®ä¸€æ‹¬ç¿»è¨³&quot;&quot;&quot;
        inputs = self.tokenizer(texts, return_tensors='pt', padding=True)
        translated = self.model.generate(**inputs)

        return [
            self.tokenizer.decode(t, skip_special_tokens=True)
            for t in translated
        ]

# ä½¿ç”¨ä¾‹
if __name__ == &quot;__main__&quot;:
    # è‹±èªâ†’æ—¥æœ¬èª
    translator_en_ja = NeuralTranslator(src_lang='en', tgt_lang='ja')

    english_texts = [
        &quot;Hello, how are you?&quot;,
        &quot;Natural language processing is fascinating.&quot;,
        &quot;Machine learning has revolutionized AI.&quot;
    ]

    print(&quot;ã€è‹±èª â†’ æ—¥æœ¬èªã€‘&quot;)
    for eng, jpn in zip(english_texts, translator_en_ja.batch_translate(english_texts)):
        print(f&quot;{eng} â†’ {jpn}&quot;)
</code></pre>
<h3>4.3 å¿œç”¨: å¤šè¨€èªå¯¾å¿œãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ</h3>
<pre><code class="language-python">class MultilingualChatbot:
    &quot;&quot;&quot;å¤šè¨€èªå¯¾å¿œãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ&quot;&quot;&quot;

    def __init__(self):
        # å„è¨€èªãƒšã‚¢ã®ç¿»è¨³å™¨
        self.translators = {
            ('en', 'ja'): NeuralTranslator('en', 'ja'),
            ('ja', 'en'): NeuralTranslator('ja', 'en'),
        }

        # QAã‚·ã‚¹ãƒ†ãƒ ï¼ˆè‹±èªãƒ™ãƒ¼ã‚¹ï¼‰
        self.qa_system = DocumentQASystem([
            &quot;Our business hours are 9 AM to 6 PM on weekdays.&quot;,
            &quot;Returns are possible within 30 days of purchase.&quot;,
            &quot;Shipping is 500 yen nationwide. Free for purchases over 5000 yen.&quot;
        ])

    def answer_question(self, question: str, lang: str = 'ja') -&gt; str:
        &quot;&quot;&quot;
        è³ªå•ã«å¤šè¨€èªã§å›ç­”

        Parameters:
        -----------
        question : str
            è³ªå•æ–‡
        lang : str
            è³ªå•ã®è¨€èªã‚³ãƒ¼ãƒ‰

        Returns:
        --------
        str
            å›ç­”ï¼ˆè³ªå•ã¨åŒã˜è¨€èªï¼‰
        &quot;&quot;&quot;
        # æ—¥æœ¬èªã®è³ªå•ã‚’è‹±èªã«ç¿»è¨³
        if lang == 'ja':
            question_en = self.translators[('ja', 'en')].translate(question)
        else:
            question_en = question

        # è‹±èªã§å›ç­”ã‚’æ¤œç´¢
        answers = self.qa_system.answer_question(question_en, top_k=1)
        answer_en = answers[0][0]

        # å›ç­”ã‚’æ—¥æœ¬èªã«ç¿»è¨³
        if lang == 'ja':
            answer_ja = self.translators[('en', 'ja')].translate(answer_en)
            return answer_ja
        else:
            return answer_en

# ä½¿ç”¨ä¾‹
chatbot = MultilingualChatbot()
print(chatbot.answer_question(&quot;é€æ–™ã¯ã„ãã‚‰ã§ã™ã‹ï¼Ÿ&quot;, lang='ja'))
</code></pre>
<hr />
<h2>5. æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰: å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰</h2>
<h3>5.1 LLMã¨ã¯</h3>
<p><strong>å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLarge Language Model, LLMï¼‰</strong> ã¯ã€è†¨å¤§ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã•ã‚ŒãŸå·¨å¤§ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</p>
<h4>ä¸»ãªLLM</h4>
<table>
<thead>
<tr>
<th>ãƒ¢ãƒ‡ãƒ«</th>
<th>é–‹ç™ºå…ƒ</th>
<th>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°</th>
<th>ç‰¹å¾´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>GPT-4</strong></td>
<td>OpenAI</td>
<td>éå…¬é–‹ï¼ˆæ¨å®š1å…†ä»¥ä¸Šï¼‰</td>
<td>ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã€é«˜ç²¾åº¦</td>
</tr>
<tr>
<td><strong>Claude</strong></td>
<td>Anthropic</td>
<td>éå…¬é–‹</td>
<td>é•·æ–‡å¯¾å¿œã€å®‰å…¨æ€§é‡è¦–</td>
</tr>
<tr>
<td><strong>Gemini</strong></td>
<td>Google</td>
<td>éå…¬é–‹</td>
<td>ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«</td>
</tr>
<tr>
<td><strong>LLaMA</strong></td>
<td>Meta</td>
<td>7B-70B</td>
<td>ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹</td>
</tr>
<tr>
<td><strong>GPT-3.5</strong></td>
<td>OpenAI</td>
<td>175B</td>
<td>ChatGPTã®ãƒ™ãƒ¼ã‚¹</td>
</tr>
</tbody>
</table>
<h3>5.2 LLMã®èƒ½åŠ›</h3>
<div class="mermaid">
graph TD
    A[LLM] --> B[ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ]
    A --> C[è³ªå•å¿œç­”]
    A --> D[è¦ç´„]
    A --> E[ç¿»è¨³]
    A --> F[ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ]
    A --> G[æ¨è«–]
    A --> H[å‰µä½œ]
</div>

<h4>ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆå­¦ç¿’</h4>
<p>è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãªã—ã§ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ</p>
<pre><code class="language-python"># ä¾‹: GPT-3.5ã‚’ä½¿ã£ãŸæ„Ÿæƒ…åˆ†æï¼ˆå­¦ç¿’ä¸è¦ï¼‰
import openai

def zero_shot_sentiment(text: str) -&gt; str:
    &quot;&quot;&quot;ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ„Ÿæƒ…åˆ†æ&quot;&quot;&quot;
    prompt = f&quot;&quot;&quot;
    æ¬¡ã®ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…ã‚’ã€ŒPositiveã€ã€ŒNegativeã€ã€ŒNeutralã€ã§åˆ†é¡ã—ã¦ãã ã•ã„ã€‚

    ãƒ†ã‚­ã‚¹ãƒˆ: {text}
    æ„Ÿæƒ…:
    &quot;&quot;&quot;

    response = openai.ChatCompletion.create(
        model=&quot;gpt-3.5-turbo&quot;,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}]
    )

    return response.choices[0].message.content.strip()
</code></pre>
<h4>Few-Shotå­¦ç¿’</h4>
<p>å°‘æ•°ã®ä¾‹ã‹ã‚‰å­¦ç¿’</p>
<pre><code class="language-python">def few_shot_classification(text: str) -&gt; str:
    &quot;&quot;&quot;Few-Shotåˆ†é¡&quot;&quot;&quot;
    prompt = f&quot;&quot;&quot;
    ä»¥ä¸‹ã®ä¾‹ã‚’å‚è€ƒã«ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã—ã¦ãã ã•ã„ã€‚

    ä¾‹1:
    ãƒ†ã‚­ã‚¹ãƒˆ: å•†å“ãŒå±Šãã¾ã›ã‚“
    ã‚«ãƒ†ã‚´ãƒª: é…é€

    ä¾‹2:
    ãƒ†ã‚­ã‚¹ãƒˆ: è¿”é‡‘ã—ã¦ãã ã•ã„
    ã‚«ãƒ†ã‚´ãƒª: è¿”é‡‘

    ä¾‹3:
    ãƒ†ã‚­ã‚¹ãƒˆ: ä½¿ã„æ–¹ãŒã‚ã‹ã‚Šã¾ã›ã‚“
    ã‚«ãƒ†ã‚´ãƒª: ä½¿ã„æ–¹

    æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆ:
    ãƒ†ã‚­ã‚¹ãƒˆ: {text}
    ã‚«ãƒ†ã‚´ãƒª:
    &quot;&quot;&quot;

    response = openai.ChatCompletion.create(
        model=&quot;gpt-3.5-turbo&quot;,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}]
    )

    return response.choices[0].message.content.strip()
</code></pre>
<h3>5.3 ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</h3>
<p>LLMã‹ã‚‰æœ€è‰¯ã®çµæœã‚’å¾—ã‚‹ãŸã‚ã®æŠ€è¡“</p>
<h4>åŸºæœ¬åŸå‰‡</h4>
<ol>
<li><strong>æ˜ç¢ºãªæŒ‡ç¤º</strong>: æ›–æ˜§ã•ã‚’é¿ã‘ã‚‹</li>
<li><strong>æ–‡è„ˆã®æä¾›</strong>: å¿…è¦ãªèƒŒæ™¯æƒ…å ±ã‚’å«ã‚ã‚‹</li>
<li><strong>å‡ºåŠ›å½¢å¼ã®æŒ‡å®š</strong>: JSONã€ç®‡æ¡æ›¸ããªã©</li>
<li><strong>åˆ¶ç´„ã®è¨­å®š</strong>: æ–‡å­—æ•°åˆ¶é™ã€ãƒˆãƒ¼ãƒ³æŒ‡å®š</li>
</ol>
<h4>å®Ÿè·µä¾‹</h4>
<pre><code class="language-python">class PromptEngineer:
    &quot;&quot;&quot;ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹&quot;&quot;&quot;

    @staticmethod
    def create_structured_prompt(task: str, context: str,
                                 format: str, constraints: List[str]) -&gt; str:
        &quot;&quot;&quot;
        æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ

        Parameters:
        -----------
        task : str
            ã‚¿ã‚¹ã‚¯ã®èª¬æ˜
        context : str
            æ–‡è„ˆæƒ…å ±
        format : str
            å‡ºåŠ›å½¢å¼
        constraints : List[str]
            åˆ¶ç´„æ¡ä»¶

        Returns:
        --------
        str
            å®Œæˆã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        &quot;&quot;&quot;
        prompt = f&quot;&quot;&quot;
## ã‚¿ã‚¹ã‚¯
{task}

## æ–‡è„ˆ
{context}

## å‡ºåŠ›å½¢å¼
{format}

## åˆ¶ç´„æ¡ä»¶
&quot;&quot;&quot;
        for i, constraint in enumerate(constraints, 1):
            prompt += f&quot;{i}. {constraint}\n&quot;

        return prompt

# ä½¿ç”¨ä¾‹
prompt = PromptEngineer.create_structured_prompt(
    task=&quot;è£½å“ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’åˆ†æã—ã€æ”¹å–„ç‚¹ã‚’ææ¡ˆã—ã¦ãã ã•ã„&quot;,
    context=&quot;æœ€æ–°ã®ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã«é–¢ã™ã‚‹100ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼&quot;,
    format=&quot;JSONå½¢å¼ã§ã€ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«æ”¹å–„ç‚¹ã‚’ãƒªã‚¹ãƒˆåŒ–&quot;,
    constraints=[
        &quot;æœ€å¤§5ã¤ã®æ”¹å–„ç‚¹&quot;,
        &quot;å…·ä½“çš„ã§å®Ÿè¡Œå¯èƒ½ãªææ¡ˆ&quot;,
        &quot;è‚¯å®šçš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚‚å«ã‚ã‚‹&quot;
    ]
)

print(prompt)
</code></pre>
<h3>5.4 RAG (Retrieval-Augmented Generation)</h3>
<p>å¤–éƒ¨çŸ¥è­˜ã‚’æ´»ç”¨ã—ã¦LLMã®å›ç­”ç²¾åº¦ã‚’å‘ä¸Š</p>
<pre><code class="language-python">from typing import List
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class RAGSystem:
    &quot;&quot;&quot;RAGï¼ˆæ¤œç´¢æ‹¡å¼µç”Ÿæˆï¼‰ã‚·ã‚¹ãƒ†ãƒ &quot;&quot;&quot;

    def __init__(self, knowledge_base: List[str]):
        &quot;&quot;&quot;
        Parameters:
        -----------
        knowledge_base : List[str]
            çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ï¼ˆæ–‡æ›¸ã®ãƒªã‚¹ãƒˆï¼‰
        &quot;&quot;&quot;
        self.knowledge_base = knowledge_base
        self.vectorizer = TfidfVectorizer()
        self.doc_vectors = self.vectorizer.fit_transform(knowledge_base)

    def retrieve_relevant_docs(self, query: str, top_k: int = 3) -&gt; List[str]:
        &quot;&quot;&quot;
        ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹æ–‡æ›¸ã‚’æ¤œç´¢

        Parameters:
        -----------
        query : str
            æ¤œç´¢ã‚¯ã‚¨ãƒª
        top_k : int
            å–å¾—ã™ã‚‹æ–‡æ›¸æ•°

        Returns:
        --------
        List[str]
            é–¢é€£æ–‡æ›¸ã®ãƒªã‚¹ãƒˆ
        &quot;&quot;&quot;
        query_vector = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vector, self.doc_vectors)[0]

        top_indices = np.argsort(similarities)[::-1][:top_k]
        return [self.knowledge_base[i] for i in top_indices]

    def generate_answer(self, question: str) -&gt; str:
        &quot;&quot;&quot;
        RAGã«ã‚ˆã‚‹å›ç­”ç”Ÿæˆ

        Parameters:
        -----------
        question : str
            è³ªå•

        Returns:
        --------
        str
            ç”Ÿæˆã•ã‚ŒãŸå›ç­”
        &quot;&quot;&quot;
        # 1. é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢
        relevant_docs = self.retrieve_relevant_docs(question)

        # 2. æ–‡è„ˆã‚’æ§‹ç¯‰
        context = &quot;\n&quot;.join(relevant_docs)

        # 3. LLMã«è³ªå•ã¨æ–‡è„ˆã‚’æ¸¡ã—ã¦å›ç­”ç”Ÿæˆ
        prompt = f&quot;&quot;&quot;
ä»¥ä¸‹ã®æ–‡è„ˆã‚’å‚è€ƒã«ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

ã€æ–‡è„ˆã€‘
{context}

ã€è³ªå•ã€‘
{question}

ã€å›ç­”ã€‘
&quot;&quot;&quot;

        # å®Ÿéš›ã«ã¯ã“ã“ã§LLM APIã‚’å‘¼ã³å‡ºã™
        # response = openai.ChatCompletion.create(...)
        # return response.choices[0].message.content

        # ãƒ‡ãƒ¢ç”¨ã®ç°¡æ˜“å®Ÿè£…
        return f&quot;æ–‡è„ˆã«åŸºã¥ãå›ç­”ï¼ˆå®Ÿéš›ã«ã¯LLMãŒç”Ÿæˆï¼‰\né–¢é€£æƒ…å ±: {relevant_docs[0][:100]}...&quot;

# ä½¿ç”¨ä¾‹
knowledge_base = [
    &quot;å½“ç¤¾ã¯1980å¹´ã«æ±äº¬ã§å‰µæ¥­ã—ã¾ã—ãŸã€‚&quot;,
    &quot;ä¸»åŠ›è£½å“ã¯ç”£æ¥­ç”¨ãƒ­ãƒœãƒƒãƒˆã§ã™ã€‚&quot;,
    &quot;å¾“æ¥­å“¡æ•°ã¯ç´„5000äººã§ã€ä¸–ç•Œ20ã‚«å›½ã«æ‹ ç‚¹ãŒã‚ã‚Šã¾ã™ã€‚&quot;,
    &quot;å¹´é–“å£²ä¸Šé«˜ã¯ç´„1000å„„å††ã§ã™ã€‚&quot;,
    &quot;ç ”ç©¶é–‹ç™ºã«å£²ä¸Šã®15%ã‚’æŠ•è³‡ã—ã¦ã„ã¾ã™ã€‚&quot;
]

rag = RAGSystem(knowledge_base)
answer = rag.generate_answer(&quot;ä¼šç¤¾ã®å‰µæ¥­å¹´ã¨å ´æ‰€ã‚’æ•™ãˆã¦ãã ã•ã„&quot;)
print(answer)
</code></pre>
<hr />
<h2>6. NLPã®é¸æŠã‚¬ã‚¤ãƒ‰</h2>
<h3>6.1 ã‚¿ã‚¹ã‚¯åˆ¥ã®é¸æŠãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ</h3>
<div class="mermaid">
graph TD
    A[NLPã‚¿ã‚¹ã‚¯é¸æŠ] --> B{ãƒ‡ãƒ¼ã‚¿æ§‹é€ åŒ–?}
    B -->|Yes| C[æƒ…å ±æŠ½å‡º]
    B -->|No| D{å›ç­”ç”Ÿæˆ?}
    D -->|Yes| E[è³ªå•å¿œç­”]
    D -->|No| F{è¦ç´„ãŒå¿…è¦?}
    F -->|Yes| G[ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„]
    F -->|No| H{è¨€èªå¤‰æ›?}
    H -->|Yes| I[æ©Ÿæ¢°ç¿»è¨³]
    H -->|No| J{åˆ†é¡?}
    J -->|Yes| K[ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡]
    J -->|No| L[ã‚«ã‚¹ã‚¿ãƒ NLP]
</div>

<h3>6.2 ãƒ“ã‚¸ãƒã‚¹å¿œç”¨ãƒãƒˆãƒªã‚¯ã‚¹</h3>
<table>
<thead>
<tr>
<th>ãƒ“ã‚¸ãƒã‚¹èª²é¡Œ</th>
<th>æ¨å¥¨NLPã‚¿ã‚¹ã‚¯</th>
<th>æœŸå¾…åŠ¹æœ</th>
</tr>
</thead>
<tbody>
<tr>
<td>é¡§å®¢ã®å£°ã®åˆ†æ</td>
<td>æ„Ÿæƒ…åˆ†æ + æƒ…å ±æŠ½å‡º</td>
<td>æº€è¶³åº¦å‘ä¸Š</td>
</tr>
<tr>
<td>ã‚µãƒãƒ¼ãƒˆè‡ªå‹•åŒ–</td>
<td>è³ªå•å¿œç­” + åˆ†é¡</td>
<td>ã‚³ã‚¹ãƒˆå‰Šæ¸›</td>
</tr>
<tr>
<td>æ–‡æ›¸ç®¡ç†</td>
<td>è¦ç´„ + åˆ†é¡</td>
<td>æ¥­å‹™åŠ¹ç‡åŒ–</td>
</tr>
<tr>
<td>ã‚°ãƒ­ãƒ¼ãƒãƒ«å±•é–‹</td>
<td>æ©Ÿæ¢°ç¿»è¨³</td>
<td>å¸‚å ´æ‹¡å¤§</td>
</tr>
<tr>
<td>æ„æ€æ±ºå®šæ”¯æ´</td>
<td>æƒ…å ±æŠ½å‡º + è¦ç´„</td>
<td>è¿…é€Ÿãªåˆ¤æ–­</td>
</tr>
</tbody>
</table>
<hr />
<h2>7. ã¾ã¨ã‚</h2>
<h3>7.1 æœ¬ã‚·ãƒªãƒ¼ã‚ºã§å­¦ã‚“ã ã“ã¨</h3>
<h4>Chapter 1: NLPã®åŸºç¤</h4>
<ul>
<li>NLPã®å®šç¾©ã¨ä¸»è¦ã‚¿ã‚¹ã‚¯</li>
<li>æ­´å²çš„ç™ºå±•ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹â†’çµ±è¨ˆâ†’æ·±å±¤å­¦ç¿’ï¼‰</li>
<li>æ—¥æœ¬èªNLPã®ç‰¹å¾´</li>
</ul>
<h4>Chapter 2: å½¢æ…‹ç´ è§£æãƒ»æ§‹æ–‡è§£æ</h4>
<ul>
<li>ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³</li>
<li>å½¢æ…‹ç´ è§£æï¼ˆMeCab, Janomeï¼‰</li>
<li>æ§‹æ–‡è§£æã¨ä¾å­˜é–¢ä¿‚</li>
<li>å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰</li>
</ul>
<h4>Chapter 3: ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡</h4>
<ul>
<li>ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</li>
<li>ç‰¹å¾´é‡æŠ½å‡ºï¼ˆBoW, TF-IDFï¼‰</li>
<li>æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆNaive Bayes, SVMï¼‰</li>
<li>ãƒ¢ãƒ‡ãƒ«è©•ä¾¡æ‰‹æ³•</li>
</ul>
<h4>Chapter 4: å®Ÿä¸–ç•Œã®å¿œç”¨</h4>
<ul>
<li>æƒ…å ±æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ </li>
<li>è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ </li>
<li>ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„</li>
<li>æ©Ÿæ¢°ç¿»è¨³</li>
<li>æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆLLM, RAGï¼‰</li>
</ul>
<h3>7.2 æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>
<p>æœ¬ã‚·ãƒªãƒ¼ã‚ºã§NLPã®åŸºç¤ã‚’å­¦ã³ã¾ã—ãŸã€‚ã•ã‚‰ã«æ·±ã‚ã‚‹ã«ã¯ï¼š</p>
<h4>ç™ºå±•çš„ãªãƒˆãƒ”ãƒƒã‚¯</h4>
<ol>
<li>
<p><strong>æ·±å±¤å­¦ç¿’NLP</strong>
   - LSTM, GRUï¼ˆå†å¸°å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰
   - Transformerè©³ç´°
   - BERT, GPTã®å†…éƒ¨æ§‹é€ </p>
</li>
<li>
<p><strong>å®Ÿè·µçš„ãªã‚¹ã‚­ãƒ«</strong>
   - å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ‰±ã„
   - ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆWeb APIåŒ–ï¼‰
   - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</p>
</li>
<li>
<p><strong>å°‚é–€åˆ†é‡</strong>
   - ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰NLPï¼ˆåŒ»ç™‚ã€æ³•å¾‹ã€é‡‘èï¼‰
   - ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIï¼ˆãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒï¼‰
   - ä¼šè©±AIãƒ»å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ </p>
</li>
</ol>
<h4>æ¨å¥¨å­¦ç¿’ãƒ‘ã‚¹</h4>
<div class="mermaid">
graph LR
    A[è‡ªç„¶è¨€èªå‡¦ç†å…¥é–€] --> B[å˜èªåŸ‹ã‚è¾¼ã¿å…¥é–€]
    B --> C[Transformerå…¥é–€]
    C --> D[BERTå…¥é–€]
    C --> E[GPTå…¥é–€]
    D --> F[å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€]
    E --> F
</div>

<h3>7.3 å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¢ã‚¤ãƒ‡ã‚¢</h3>
<ol>
<li>
<p><strong>æ„Ÿæƒ…åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰</strong>
   - Twitter APIã§ç‰¹å®šãƒˆãƒ”ãƒƒã‚¯ã®ãƒ„ã‚¤ãƒ¼ãƒˆã‚’åé›†
   - æ„Ÿæƒ…åˆ†æã‚’å®Ÿè¡Œ
   - æ™‚ç³»åˆ—ã§å¯è¦–åŒ–</p>
</li>
<li>
<p><strong>ç¤¾å†…FAQè‡ªå‹•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ </strong>
   - éå»ã®å•ã„åˆã‚ã›ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’
   - QAã‚·ã‚¹ãƒ†ãƒ ã§è‡ªå‹•å›ç­”
   - ç²¾åº¦ã‚’ç¶™ç¶šçš„ã«æ”¹å–„</p>
</li>
<li>
<p><strong>ãƒ‹ãƒ¥ãƒ¼ã‚¹è¦ç´„ã‚¢ãƒ—ãƒª</strong>
   - RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—
   - è‡ªå‹•è¦ç´„
   - ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã¨ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰</p>
</li>
<li>
<p><strong>å¤šè¨€èªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ</strong>
   - æ©Ÿæ¢°ç¿»è¨³ã‚’æ´»ç”¨
   - è¤‡æ•°è¨€èªã§å¯¾å¿œ
   - RAGã§çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ´»ç”¨</p>
</li>
</ol>
<hr />
<h2>8. ç·´ç¿’å•é¡Œ</h2>
<h3>å•é¡Œ1: æƒ…å ±æŠ½å‡ºï¼ˆåŸºç¤ï¼‰</h3>
<p>æ¬¡ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‹ã‚‰ã€äººåã€çµ„ç¹”åã€æ—¥ä»˜ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚</p>
<pre><code class="language-python">news = &quot;&quot;&quot;
2025å¹´11æœˆ1æ—¥ã€ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šã®è±Šç”°ç« ç”·ç¤¾é•·ãŒæ–°å‹é›»æ°—è‡ªå‹•è»Šã‚’ç™ºè¡¨ã—ãŸã€‚
ã“ã®ç™ºè¡¨ã¯æ±äº¬ãƒ¢ãƒ¼ã‚¿ãƒ¼ã‚·ãƒ§ãƒ¼ã§è¡Œã‚ã‚Œã€200åä»¥ä¸Šã®å ±é“é–¢ä¿‚è€…ãŒå‚åŠ ã—ãŸã€‚
&quot;&quot;&quot;

# ã‚ãªãŸã®å®Ÿè£…
</code></pre>
<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python">import spacy

nlp = spacy.load('ja_ginza')
doc = nlp(news)

entities = {
    'PERSON': [],
    'ORG': [],
    'DATE': []
}

for ent in doc.ents:
    if ent.label_ in entities:
        entities[ent.label_].append(ent.text)

print(&quot;äººå:&quot;, entities['PERSON'])
print(&quot;çµ„ç¹”å:&quot;, entities['ORG'])
print(&quot;æ—¥ä»˜:&quot;, entities['DATE'])
</code></pre>


</details>

<h3>å•é¡Œ2: ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ï¼ˆä¸­ç´šï¼‰</h3>
<p>æ¬¡ã®é•·æ–‡ã‚’3æ–‡ã«è¦ç´„ã—ã¦ãã ã•ã„ï¼ˆæŠ½å‡ºå‹è¦ç´„ï¼‰ã€‚</p>
<pre><code class="language-python">article = &quot;&quot;&quot;
äººå·¥çŸ¥èƒ½ã®ç™ºå±•ã¯ç›®è¦šã¾ã—ãã€æ§˜ã€…ãªåˆ†é‡ã§æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
ç‰¹ã«è‡ªç„¶è¨€èªå‡¦ç†ã®åˆ†é‡ã§ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ãŒç™»å ´ã—ã€
äººé–“ã¨åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚
æ©Ÿæ¢°ç¿»è¨³ã®ç²¾åº¦ã‚‚å¤§ããå‘ä¸Šã—ã€å®Ÿç”¨ãƒ¬ãƒ™ãƒ«ã«é”ã—ã¦ã„ã¾ã™ã€‚
ã¾ãŸã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚„ãƒãƒ¼ãƒãƒ£ãƒ«ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‚‚æ™®åŠã—ã¦ã„ã¾ã™ã€‚
ä»Šå¾Œã‚‚AIæŠ€è¡“ã®é€²åŒ–ã¯ç¶šãã¨äºˆæƒ³ã•ã‚Œã¾ã™ã€‚
åŒ»ç™‚ã€æ•™è‚²ã€ãƒ“ã‚¸ãƒã‚¹ãªã©ã€ã‚ã‚‰ã‚†ã‚‹åˆ†é‡ã§AIãŒæ´»ç”¨ã•ã‚Œã‚‹ã§ã—ã‚‡ã†ã€‚
&quot;&quot;&quot;

# ã‚ãªãŸã®å®Ÿè£…
</code></pre>
<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# æ–‡ã«åˆ†å‰²
sentences = [s.strip() for s in article.split('ã€‚') if s.strip()]

# TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(sentences)

# å„æ–‡ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
scores = np.array(tfidf_matrix.sum(axis=1)).flatten()

# ä¸Šä½3æ–‡ã‚’æŠ½å‡º
top_indices = np.argsort(scores)[::-1][:3]
top_indices = sorted(top_indices)

summary = 'ã€‚'.join([sentences[i] for i in top_indices]) + 'ã€‚'
print(&quot;è¦ç´„:&quot;, summary)
</code></pre>


</details>

<h3>å•é¡Œ3: RAGã‚·ã‚¹ãƒ†ãƒ ï¼ˆå¿œç”¨ï¼‰</h3>
<p>çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ã£ã¦è³ªå•ã«ç­”ãˆã‚‹RAGã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>ä»•æ§˜:</strong>
- çŸ¥è­˜ãƒ™ãƒ¼ã‚¹: ä¼šç¤¾æƒ…å ±ã®æ–‡æ›¸ãƒªã‚¹ãƒˆ
- å…¥åŠ›: è³ªå•æ–‡
- å‡ºåŠ›: é–¢é€£æ–‡æ›¸ + å›ç­”</p>
<details>
<summary>è§£ç­”ä¾‹</summary>


<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class SimpleRAG:
    def __init__(self, knowledge_base):
        self.kb = knowledge_base
        self.vectorizer = TfidfVectorizer()
        self.doc_vectors = self.vectorizer.fit_transform(knowledge_base)

    def answer(self, question, top_k=2):
        # è³ªå•ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        q_vector = self.vectorizer.transform([question])

        # é¡ä¼¼åº¦è¨ˆç®—
        similarities = cosine_similarity(q_vector, self.doc_vectors)[0]

        # ä¸Šä½kä»¶ã‚’å–å¾—
        top_indices = np.argsort(similarities)[::-1][:top_k]

        results = [(self.kb[i], similarities[i]) for i in top_indices]
        return results

# çŸ¥è­˜ãƒ™ãƒ¼ã‚¹
kb = [
    &quot;å½“ç¤¾ã¯2000å¹´ã«è¨­ç«‹ã•ã‚Œã¾ã—ãŸ&quot;,
    &quot;ä¸»åŠ›è£½å“ã¯AIã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã§ã™&quot;,
    &quot;å¾“æ¥­å“¡æ•°ã¯300åã§ã™&quot;,
    &quot;æœ¬ç¤¾ã¯æ±äº¬ã«ã‚ã‚Šã¾ã™&quot;
]

rag = SimpleRAG(kb)
results = rag.answer(&quot;ä¼šç¤¾ã®è¨­ç«‹å¹´ã¯ï¼Ÿ&quot;)

for doc, score in results:
    print(f&quot;é–¢é€£æ–‡æ›¸: {doc} (ã‚¹ã‚³ã‚¢: {score:.4f})&quot;)
</code></pre>


</details>

<hr />
<h2>9. å‚è€ƒæ–‡çŒ®ã¨ãƒªã‚½ãƒ¼ã‚¹</h2>
<h3>æ›¸ç±</h3>
<ol>
<li>ã€Œè‡ªç„¶è¨€èªå‡¦ç†ã®åŸºç¤ã€å¥¥æ‘å­¦ï¼ˆã‚³ãƒ­ãƒŠç¤¾ï¼‰</li>
<li>ã€Œã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹Deep Learning â·ã€æ–è—¤åº·æ¯…ï¼ˆã‚ªãƒ©ã‚¤ãƒªãƒ¼ãƒ»ã‚¸ãƒ£ãƒ‘ãƒ³ï¼‰</li>
<li>ã€ŒLarge Language Modelsã€Jurafsky &amp; Martin</li>
</ol>
<h3>ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ã‚¹</h3>
<ul>
<li><a href="http://web.stanford.edu/class/cs224n/">Stanford CS224N: NLP with Deep Learning</a></li>
<li><a href="https://huggingface.co/course/">Hugging Face Course</a></li>
<li><a href="https://www.fast.ai/">fast.ai NLP Course</a></li>
</ul>
<h3>ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ»ãƒ„ãƒ¼ãƒ«</h3>
<ul>
<li><a href="https://huggingface.co/transformers/">Hugging Face Transformers</a> - æœ€æ–°ã®NLPãƒ¢ãƒ‡ãƒ«</li>
<li><a href="https://spacy.io/">spaCy</a> - ç”£æ¥­ç”¨NLP</li>
<li><a href="https://www.nltk.org/">NLTK</a> - æ•™è‚²ç”¨NLP</li>
<li><a href="https://radimrehurek.com/gensim/">Gensim</a> - ãƒˆãƒ”ãƒƒã‚¯ãƒ¢ãƒ‡ãƒªãƒ³ã‚°</li>
</ul>
<h3>æœ€æ–°å‹•å‘</h3>
<ul>
<li><a href="https://paperswithcode.com/area/natural-language-processing">Papers with Code - NLP</a></li>
<li><a href="https://aclanthology.org/">ACL Anthology</a> - NLPè«–æ–‡ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–</li>
<li><a href="https://arxiv.org/list/cs.CL/recent">arXiv cs.CL</a> - æœ€æ–°è«–æ–‡</li>
</ul>
<hr />
<h2>10. ãŠã‚ã‚Šã«</h2>
<p>è‡ªç„¶è¨€èªå‡¦ç†ã¯ã€AIæŠ€è¡“ã®ä¸­ã§ã‚‚ç‰¹ã«æ€¥é€Ÿã«ç™ºå±•ã—ã¦ã„ã‚‹åˆ†é‡ã§ã™ã€‚æœ¬ã‚·ãƒªãƒ¼ã‚ºã§å­¦ã‚“ã åŸºç¤çŸ¥è­˜ã‚’åœŸå°ã«ã€ãœã²å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«æŒ‘æˆ¦ã—ã¦ãã ã•ã„ã€‚</p>
<h3>ã•ã‚‰ãªã‚‹å­¦ç¿’ã®ãŸã‚ã«</h3>
<ol>
<li><strong>å®Ÿãƒ‡ãƒ¼ã‚¿ã§å®Ÿé¨“</strong>: Kaggleã®ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«å‚åŠ </li>
<li><strong>æœ€æ–°è«–æ–‡ã‚’èª­ã‚€</strong>: arXivã§æœ€æ–°ç ”ç©¶ã‚’ãƒ•ã‚©ãƒ­ãƒ¼</li>
<li><strong>ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å‚åŠ </strong>: GitHubã€Stack Overflowã€å‹‰å¼·ä¼š</li>
<li><strong>è‡ªåˆ†ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ</strong>: èˆˆå‘³ã®ã‚ã‚‹åˆ†é‡ã§NLPã‚¢ãƒ—ãƒªã‚’é–‹ç™º</li>
</ol>
<h3>ç¶™ç¶šçš„ãªå­¦ç¿’</h3>
<p>NLPã¯é€²åŒ–ãŒé€Ÿã„åˆ†é‡ã§ã™ã€‚ä»¥ä¸‹ã‚’å®šæœŸçš„ã«ãƒã‚§ãƒƒã‚¯ã—ã¾ã—ã‚‡ã†ï¼š</p>
<ul>
<li>âœ… ä¸»è¦ã‚«ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ï¼ˆACL, EMNLP, NAACLï¼‰ã®è«–æ–‡</li>
<li>âœ… OpenAI, Google AI, Meta AIã®ãƒ–ãƒ­ã‚°</li>
<li>âœ… Hugging Faceã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«</li>
<li>âœ… NLPç³»YouTubeãƒãƒ£ãƒ³ãƒãƒ«</li>
</ul>
<hr />
<p><strong>æœ¬ã‚·ãƒªãƒ¼ã‚ºã‚’æœ€å¾Œã¾ã§ãŠèª­ã¿ã„ãŸã ãã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸï¼</strong></p>
<p><strong>å‰ã¸</strong>: <a href="chapter-3.html">â† Chapter 3: å®Ÿè·µ - ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡</a></p>
<p><strong>ç›®æ¬¡ã¸</strong>: <a href="index.html">â†‘ ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a></p>
<hr />
<h2>ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ãŠå¾…ã¡ã—ã¦ã„ã¾ã™</h2>
<p>ã“ã®ã‚·ãƒªãƒ¼ã‚ºã®æ”¹å–„ã«ã”å”åŠ›ãã ã•ã„ï¼š</p>
<ul>
<li><strong>èª¤ã‚Šã‚’ç™ºè¦‹</strong>: <a href="https://github.com/YusukeHashimotoLab/AI-Knowledge-Notes/issues">GitHubã§Issueã‚’é–‹ã</a></li>
<li><strong>æ”¹å–„ææ¡ˆ</strong>: ã‚ˆã‚Šè‰¯ã„èª¬æ˜æ–¹æ³•ã®ã‚¢ã‚¤ãƒ‡ã‚¢</li>
<li><strong>è³ªå•</strong>: ç†è§£ãŒé›£ã—ã‹ã£ãŸéƒ¨åˆ†</li>
</ul>
<p>è©³ç´°ã¯<a href="../../../CONTRIBUTING.md">CONTRIBUTING.md</a>ã‚’ã”è¦§ãã ã•ã„ã€‚</p><div class="navigation">
    <a href="chapter-3.html" class="nav-button">â† ç¬¬3ç« </a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
</div>
    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimotoï¼ˆæ±åŒ—å¤§å­¦ï¼‰</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
