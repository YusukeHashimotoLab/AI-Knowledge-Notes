<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 4: ÂÆü‰∏ñÁïå„ÅÆNLPÂøúÁî® - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Chapter 4: ÂÆü‰∏ñÁïå„ÅÆNLPÂøúÁî®</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 25-30ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ÂàùÁ¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 0ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 0Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 4: ÂÆü‰∏ñÁïå„ÅÆNLPÂøúÁî®</h1>
<h2>Êú¨Á´†„ÅÆÊ¶ÇË¶Å</h2>
<p>„Åì„Çå„Åæ„ÅßÂ≠¶„Çì„Å†NLPÊäÄË°ì„Åå„ÄÅÂÆüÈöõ„ÅÆ„Éì„Ç∏„Éç„Çπ„ÇÑÊó•Â∏∏ÁîüÊ¥ª„Åß„Å©„ÅÆ„Çà„ÅÜ„Å´Ê¥ªÁî®„Åï„Çå„Å¶„ÅÑ„Çã„Åã„ÇíÂ≠¶„Å≥„Åæ„Åô„ÄÇ</p>
<p>Êú¨Á´†„Åß„ÅØ„ÄÅÊÉÖÂ†±ÊäΩÂá∫„ÄÅË≥™ÂïèÂøúÁ≠î„ÄÅ„ÉÜ„Ç≠„Çπ„ÉàË¶ÅÁ¥Ñ„ÄÅÊ©üÊ¢∞ÁøªË®≥„Å™„Å©„ÅÆÂÆüÁî®ÁöÑ„Å™NLP„Çø„Çπ„ÇØ„Å®„ÄÅÊúÄÊñ∞„ÅÆÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàLLMÔºâ„ÅÆÂãïÂêë„ÇíÊâ±„ÅÑ„Åæ„Åô„ÄÇ</p>
<h3>Â≠¶ÁøíÁõÆÊ®ô</h3>
<ul>
<li>‚úÖ ÂÆüÁî®ÁöÑ„Å™NLP„Ç∑„Çπ„ÉÜ„É†„ÅÆ‰ªïÁµÑ„Åø„ÇíÁêÜËß£„Åô„Çã</li>
<li>‚úÖ ÂêÑ„Çø„Çπ„ÇØ„ÅÆÂøúÁî®‰æã„Å®„Éì„Ç∏„Éç„Çπ‰æ°ÂÄ§„ÇíÊääÊè°„Åô„Çã</li>
<li>‚úÖ ÊúÄÊñ∞„ÅÆNLPÊäÄË°ìÂãïÂêëÔºàLLM„ÄÅChatGPTÁ≠âÔºâ„ÇíÁü•„Çã</li>
<li>‚úÖ ÈÅ©Âàá„Å™NLP„Çø„Çπ„ÇØ„ÇíÈÅ∏Êäû„Åß„Åç„Çã„Çà„ÅÜ„Å´„Å™„Çã</li>
</ul>
<hr />
<h2>1. ÊÉÖÂ†±ÊäΩÂá∫ÔºàInformation ExtractionÔºâ</h2>
<h3>1.1 ÊÉÖÂ†±ÊäΩÂá∫„Å®„ÅØ</h3>
<p><strong>ÊÉÖÂ†±ÊäΩÂá∫ÔºàIE: Information ExtractionÔºâ</strong> „ÅØ„ÄÅÈùûÊßãÈÄ†Âåñ„ÉÜ„Ç≠„Çπ„Éà„Åã„ÇâÊßãÈÄ†Âåñ„Åï„Çå„ÅüÊÉÖÂ†±„ÇíËá™ÂãïÁöÑ„Å´ÊäΩÂá∫„Åô„Çã„Çø„Çπ„ÇØ„Åß„Åô„ÄÇ</p>
<h4>‰∏ª„Å™„Çø„Çπ„ÇØ</h4>
<div class="mermaid">
graph TD
    A[ÈùûÊßãÈÄ†Âåñ„ÉÜ„Ç≠„Çπ„Éà] --> B[Âõ∫ÊúâË°®ÁèæÊäΩÂá∫ NER]
    A --> C[Èñ¢‰øÇÊäΩÂá∫ RE]
    A --> D[„Ç§„Éô„É≥„ÉàÊäΩÂá∫]
    B --> E[ÊßãÈÄ†Âåñ„Éá„Éº„Çø]
    C --> E
    D --> E
</div>

<table>
<thead>
<tr>
<th>„Çø„Çπ„ÇØ</th>
<th>Ë™¨Êòé</th>
<th>‰æã</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Âõ∫ÊúâË°®ÁèæÊäΩÂá∫ (NER)</strong></td>
<td>‰∫∫Âêç„ÄÅÂú∞Âêç„ÄÅÁµÑÁπîÂêç„Å™„Å©„ÇíÊäΩÂá∫</td>
<td>"Áî∞‰∏≠Â§™ÈÉé„Åï„Çì" ‚Üí PERSON</td>
</tr>
<tr>
<td><strong>Èñ¢‰øÇÊäΩÂá∫ (RE)</strong></td>
<td>„Ç®„É≥„ÉÜ„Ç£„ÉÜ„Ç£Èñì„ÅÆÈñ¢‰øÇ„ÇíÊäΩÂá∫</td>
<td>"Áî∞‰∏≠Â§™ÈÉé„ÅØÊù±‰∫¨Â§ßÂ≠¶„ÇíÂçíÊ•≠" ‚Üí (Áî∞‰∏≠Â§™ÈÉé, ÂçíÊ•≠, Êù±‰∫¨Â§ßÂ≠¶)</td>
</tr>
<tr>
<td><strong>„Ç§„Éô„É≥„ÉàÊäΩÂá∫</strong></td>
<td>Âá∫Êù•‰∫ã„Å®„Åù„ÅÆÂèÇÂä†ËÄÖ„ÇíÊäΩÂá∫</td>
<td>"Apple„ÅåÊñ∞Ë£ΩÂìÅ„ÇíÁô∫Ë°®" ‚Üí (Áô∫Ë°®, ‰∏ª‰Ωì:Apple, ÂØæË±°:Êñ∞Ë£ΩÂìÅ)</td>
</tr>
</tbody>
</table>
<h3>1.2 ÂÆüË£Ö: „Éã„É•„Éº„ÇπË®ò‰∫ã„Åã„Çâ„ÅÆÊÉÖÂ†±ÊäΩÂá∫</h3>
<pre><code class="language-python">import spacy
import re
from typing import List, Dict, Tuple

class NewsInfoExtractor:
    &quot;&quot;&quot;„Éã„É•„Éº„ÇπË®ò‰∫ã„Åã„Çâ„ÅÆÊÉÖÂ†±ÊäΩÂá∫„Ç∑„Çπ„ÉÜ„É†&quot;&quot;&quot;

    def __init__(self, model_name='ja_ginza'):
        &quot;&quot;&quot;
        Parameters:
        -----------
        model_name : str
            spaCy„É¢„Éá„É´ÂêçÔºàÊó•Êú¨Ë™û: 'ja_ginza', Ëã±Ë™û: 'en_core_web_sm'Ôºâ
        &quot;&quot;&quot;
        self.nlp = spacy.load(model_name)

    def extract_named_entities(self, text: str) -&gt; Dict[str, List[str]]:
        &quot;&quot;&quot;Âõ∫ÊúâË°®ÁèæÊäΩÂá∫&quot;&quot;&quot;
        doc = self.nlp(text)

        entities = {
            'PERSON': [],      # ‰∫∫Âêç
            'ORG': [],         # ÁµÑÁπîÂêç
            'GPE': [],         # Âú∞ÂêçÔºàÂõΩ„ÉªÈÉΩÂ∏ÇÔºâ
            'DATE': [],        # Êó•‰ªò
            'MONEY': [],       # ÈáëÈ°ç
            'PRODUCT': [],     # Ë£ΩÂìÅÂêç
        }

        for ent in doc.ents:
            if ent.label_ in entities:
                # ÈáçË§á„ÇíÈÅø„Åë„Çã
                if ent.text not in entities[ent.label_]:
                    entities[ent.label_].append(ent.text)

        return entities

    def extract_relationships(self, text: str) -&gt; List[Tuple[str, str, str]]:
        &quot;&quot;&quot;Èñ¢‰øÇÊäΩÂá∫Ôºà‰∏ªË™û-Ëø∞Ë™û-ÁõÆÁöÑË™ûÔºâ&quot;&quot;&quot;
        doc = self.nlp(text)
        relations = []

        for sent in doc.sents:
            # ‰∏ªË™û„ÇíÊé¢„Åô
            subject = None
            verb = None
            obj = None

            for token in sent:
                # ‰∏ªË™ûÔºàÂêçË©û„Åß„ÄÅ‰∏ªÊ†ºÂä©Ë©û„Äå„Åå„Äç„Äå„ÅØ„Äç„Åå‰ªò„ÅèÔºâ
                if token.dep_ in ['nsubj', 'nsubjpass'] and token.pos_ in ['NOUN', 'PROPN']:
                    subject = token.text

                # Ëø∞Ë™ûÔºàÂãïË©ûÔºâ
                if token.pos_ == 'VERB' and token.dep_ == 'ROOT':
                    verb = token.lemma_  # Âü∫Êú¨ÂΩ¢

                # ÁõÆÁöÑË™ûÔºàÂêçË©û„Åß„ÄÅÂØæÊ†ºÂä©Ë©û„Äå„Çí„Äç„Åå‰ªò„ÅèÔºâ
                if token.dep_ == 'obj' and token.pos_ in ['NOUN', 'PROPN']:
                    obj = token.text

            # ‰∏ªË™û„ÉªËø∞Ë™û„ÉªÁõÆÁöÑË™û„ÅåÊèÉ„Å£„ÅüÂ†¥Âêà
            if subject and verb and obj:
                relations.append((subject, verb, obj))

        return relations

    def extract_dates(self, text: str) -&gt; List[str]:
        &quot;&quot;&quot;Êó•‰ªòË°®Áèæ„ÅÆÊäΩÂá∫&quot;&quot;&quot;
        # Êó•‰ªò„Éë„Çø„Éº„É≥
        patterns = [
            r'\d{4}Âπ¥\d{1,2}Êúà\d{1,2}Êó•',
            r'\d{1,2}/\d{1,2}/\d{4}',
            r'\d{4}-\d{2}-\d{2}',
        ]

        dates = []
        for pattern in patterns:
            matches = re.findall(pattern, text)
            dates.extend(matches)

        return dates

    def extract_money(self, text: str) -&gt; List[str]:
        &quot;&quot;&quot;ÈáëÈ°çË°®Áèæ„ÅÆÊäΩÂá∫&quot;&quot;&quot;
        # ÈáëÈ°ç„Éë„Çø„Éº„É≥
        patterns = [
            r'\d+ÂÑÑÂÜÜ',
            r'\d+‰∏áÂÜÜ',
            r'\d+ÂÜÜ',
            r'¬•\d+',
            r'\$\d+'
        ]

        money = []
        for pattern in patterns:
            matches = re.findall(pattern, text)
            money.extend(matches)

        return money

    def analyze_news(self, text: str) -&gt; Dict:
        &quot;&quot;&quot;„Éã„É•„Éº„ÇπË®ò‰∫ã„ÅÆÁ∑èÂêàÂàÜÊûê&quot;&quot;&quot;
        return {
            'entities': self.extract_named_entities(text),
            'relationships': self.extract_relationships(text),
            'dates': self.extract_dates(text),
            'money': self.extract_money(text)
        }

# ‰ΩøÁî®‰æã
if __name__ == &quot;__main__&quot;:
    extractor = NewsInfoExtractor()

    news_text = &quot;&quot;&quot;
    2025Âπ¥10Êúà20Êó•„ÄÅÊù±‰∫¨„ÅßÈñãÂÇ¨„Åï„Çå„ÅüÊäÄË°ì„Ç´„É≥„Éï„Ç°„É¨„É≥„Çπ„Å´„Å¶„ÄÅ
    Ê†™Âºè‰ºöÁ§æ„ÉÜ„ÉÉ„ÇØ„Ç§„Éé„Éô„Éº„Ç∑„Éß„É≥„ÅåÊñ∞ÂûãAI„ÉÅ„ÉÉ„Éó„ÇíÁô∫Ë°®„Åó„Åü„ÄÇ
    ÂêåÁ§æ„ÅÆÁî∞‰∏≠Â§™ÈÉéCEO„ÅØ„ÄÅ„Äå„Åì„ÅÆË£ΩÂìÅ„ÅØÊ•≠Áïå„ÇíÂ§âÈù©„Åô„Çã„Äç„Å®Ëø∞„Åπ„Åü„ÄÇ
    ÈñãÁô∫Ë≤ª„ÅØÁ¥Ñ100ÂÑÑÂÜÜ„Åß„ÄÅÊù•Âπ¥Êò•„Å´Ë≤©Â£≤ÈñãÂßã‰∫àÂÆö„ÄÇ
    &quot;&quot;&quot;

    result = extractor.analyze_news(news_text)

    print(&quot;„ÄêÂõ∫ÊúâË°®Áèæ„Äë&quot;)
    for entity_type, entities in result['entities'].items():
        if entities:
            print(f&quot;  {entity_type}: {', '.join(entities)}&quot;)

    print(&quot;\n„ÄêÈñ¢‰øÇ„Äë&quot;)
    for subj, verb, obj in result['relationships']:
        print(f&quot;  {subj} ‚Üí {verb} ‚Üí {obj}&quot;)

    print(&quot;\n„ÄêÊó•‰ªò„Äë&quot;)
    print(f&quot;  {', '.join(result['dates'])}&quot;)

    print(&quot;\n„ÄêÈáëÈ°ç„Äë&quot;)
    print(f&quot;  {', '.join(result['money'])}&quot;)
</code></pre>
<p><strong>Âá∫Âäõ‰æã:</strong></p>
<pre><code>„ÄêÂõ∫ÊúâË°®Áèæ„Äë
  ORG: Ê†™Âºè‰ºöÁ§æ„ÉÜ„ÉÉ„ÇØ„Ç§„Éé„Éô„Éº„Ç∑„Éß„É≥
  PERSON: Áî∞‰∏≠Â§™ÈÉé
  GPE: Êù±‰∫¨
  DATE: 2025Âπ¥10Êúà20Êó•
  PRODUCT: AI„ÉÅ„ÉÉ„Éó

„ÄêÈñ¢‰øÇ„Äë
  Ê†™Âºè‰ºöÁ§æ„ÉÜ„ÉÉ„ÇØ„Ç§„Éé„Éô„Éº„Ç∑„Éß„É≥ ‚Üí Áô∫Ë°® ‚Üí AI„ÉÅ„ÉÉ„Éó
  Áî∞‰∏≠Â§™ÈÉé ‚Üí Ëø∞„Åπ„Çã ‚Üí Ë£ΩÂìÅ

„ÄêÊó•‰ªò„Äë
  2025Âπ¥10Êúà20Êó•

„ÄêÈáëÈ°ç„Äë
  100ÂÑÑÂÜÜ
</code></pre>
<h3>1.3 ÂøúÁî®‰æã</h3>
<h4>„Ç´„Çπ„Çø„Éû„Éº„Çµ„Éù„Éº„ÉàËá™ÂãïÂåñ</h4>
<pre><code class="language-python">class SupportTicketExtractor:
    &quot;&quot;&quot;Âïè„ÅÑÂêà„Çè„Åõ„ÉÅ„Ç±„ÉÉ„Éà„Åã„Çâ„ÅÆÊÉÖÂ†±ÊäΩÂá∫&quot;&quot;&quot;

    def __init__(self):
        self.nlp = spacy.load('ja_ginza')

        # Âïè„ÅÑÂêà„Çè„Åõ„Ç´„ÉÜ„Ç¥„É™„ÅÆ„Ç≠„Éº„ÉØ„Éº„Éâ
        self.category_keywords = {
            'ÈÖçÈÄÅ': ['ÈÖçÈÄÅ', 'Â±ä„Åã„Å™„ÅÑ', 'ÈÅÖ„ÅÑ', 'ËøΩË∑°'],
            'ËøîÈáë': ['ËøîÈáë', 'ËøîÂìÅ', '„Ç≠„É£„É≥„Çª„É´'],
            '‰∏çÂÖ∑Âêà': ['Â£ä„Çå„Åü', 'Âãï„Åã„Å™„ÅÑ', '„Ç®„É©„Éº', '‰∏çËâØÂìÅ'],
            '‰Ωø„ÅÑÊñπ': ['‰Ωø„ÅÑÊñπ', 'Ë®≠ÂÆö', 'ÊñπÊ≥ï', '„ÇÑ„ÇäÊñπ']
        }

    def extract_category(self, text: str) -&gt; str:
        &quot;&quot;&quot;Âïè„ÅÑÂêà„Çè„Åõ„Ç´„ÉÜ„Ç¥„É™„ÅÆÊé®ÂÆö&quot;&quot;&quot;
        text_lower = text.lower()

        scores = {}
        for category, keywords in self.category_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            scores[category] = score

        # ÊúÄÈ´ò„Çπ„Ç≥„Ç¢„ÅÆ„Ç´„ÉÜ„Ç¥„É™„ÇíËøî„Åô
        return max(scores, key=scores.get) if max(scores.values()) &gt; 0 else '‰∏ÄËà¨'

    def extract_product_name(self, text: str) -&gt; str:
        &quot;&quot;&quot;Ë£ΩÂìÅÂêç„ÅÆÊäΩÂá∫&quot;&quot;&quot;
        doc = self.nlp(text)
        for ent in doc.ents:
            if ent.label_ == 'PRODUCT':
                return ent.text
        return None

    def analyze_ticket(self, text: str) -&gt; Dict:
        &quot;&quot;&quot;„ÉÅ„Ç±„ÉÉ„Éà„ÅÆÂàÜÊûê&quot;&quot;&quot;
        return {
            'category': self.extract_category(text),
            'product': self.extract_product_name(text),
            'entities': NewsInfoExtractor().extract_named_entities(text)
        }

# ‰ΩøÁî®‰æã
extractor = SupportTicketExtractor()
ticket_text = &quot;ÂÖàÈÄ±Ê≥®Êñá„Åó„Åü„Éé„Éº„ÉàPC„ÅåÂ±ä„Åç„Åæ„Åõ„Çì„ÄÇÈÖçÈÄÅÁä∂Ê≥Å„ÇíÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ&quot;

result = extractor.analyze_ticket(ticket_text)
print(f&quot;„Ç´„ÉÜ„Ç¥„É™: {result['category']}&quot;)
print(f&quot;Ë£ΩÂìÅ: {result['product']}&quot;)
</code></pre>
<hr />
<h2>2. Ë≥™ÂïèÂøúÁ≠î„Ç∑„Çπ„ÉÜ„É†ÔºàQuestion AnsweringÔºâ</h2>
<h3>2.1 QA„Ç∑„Çπ„ÉÜ„É†„ÅÆÁ®ÆÈ°û</h3>
<table>
<thead>
<tr>
<th>Á®ÆÈ°û</th>
<th>Ë™¨Êòé</th>
<th>‰æã</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ÊäΩÂá∫ÂûãQA</strong></td>
<td>ÊñáÊõ∏„Åã„ÇâÁ≠î„Åà„ÇíÊäΩÂá∫</td>
<td>"ÂâµÊ•≠ËÄÖ„ÅØË™∞Ôºü" ‚Üí "„Çπ„ÉÜ„Ç£„Éº„Éñ„Éª„Ç∏„Éß„Éñ„Ç∫"</td>
</tr>
<tr>
<td><strong>ÁîüÊàêÂûãQA</strong></td>
<td>Á≠î„Åà„ÇíÁîüÊàê</td>
<td>"„Å™„Åú‰∫∫Ê∞óÔºü" ‚Üí "„Éá„Ç∂„Ç§„É≥„ÅåÂÑ™„Çå„Å¶„ÅÑ„Çã„Åü„ÇÅ"</td>
</tr>
<tr>
<td><strong>„Ç™„Éº„Éó„É≥„Éâ„É°„Ç§„É≥QA</strong></td>
<td>ÂπÖÂ∫É„ÅÑÂàÜÈáé„ÅÆË≥™Âïè</td>
<td>GoogleÊ§úÁ¥¢„ÅÆ„Çà„ÅÜ„Å™Ê±éÁî®QA</td>
</tr>
<tr>
<td><strong>„ÇØ„É≠„Éº„Ç∫„Éâ„Éâ„É°„Ç§„É≥QA</strong></td>
<td>ÁâπÂÆöÂàÜÈáé„Å´ÁâπÂåñ</td>
<td>Á§æÂÜÖFAQ„Ç∑„Çπ„ÉÜ„É†</td>
</tr>
</tbody>
</table>
<h3>2.2 ÂÆüË£Ö: ÊñáÊõ∏„Éô„Éº„ÇπQA„Ç∑„Çπ„ÉÜ„É†</h3>
<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class DocumentQASystem:
    &quot;&quot;&quot;ÊñáÊõ∏„Éô„Éº„Çπ„ÅÆË≥™ÂïèÂøúÁ≠î„Ç∑„Çπ„ÉÜ„É†&quot;&quot;&quot;

    def __init__(self, documents: List[str]):
        &quot;&quot;&quot;
        Parameters:
        -----------
        documents : List[str]
            Áü•Ë≠ò„Éô„Éº„Çπ„Å®„Å™„ÇãÊñáÊõ∏„ÅÆ„É™„Çπ„Éà
        &quot;&quot;&quot;
        self.documents = documents
        self.vectorizer = TfidfVectorizer()
        self.doc_vectors = self.vectorizer.fit_transform(documents)

    def answer_question(self, question: str, top_k: int = 3) -&gt; List[Tuple[str, float]]:
        &quot;&quot;&quot;
        Ë≥™Âïè„Å´ÂØæ„Åô„ÇãÂõûÁ≠î„ÇíÊ§úÁ¥¢

        Parameters:
        -----------
        question : str
            Ë≥™ÂïèÊñá
        top_k : int
            ‰∏ä‰Ωç‰Ωï‰ª∂„ÇíËøî„Åô„Åã

        Returns:
        --------
        List[Tuple[str, float]]
            (ÂõûÁ≠îÊñá, È°û‰ººÂ∫¶„Çπ„Ç≥„Ç¢) „ÅÆ„É™„Çπ„Éà
        &quot;&quot;&quot;
        # Ë≥™Âïè„Çí„Éô„ÇØ„Éà„É´Âåñ
        question_vector = self.vectorizer.transform([question])

        # „Ç≥„Çµ„Ç§„É≥È°û‰ººÂ∫¶„ÇíË®àÁÆó
        similarities = cosine_similarity(question_vector, self.doc_vectors)[0]

        # È°û‰ººÂ∫¶„ÅåÈ´ò„ÅÑÈ†Ü„Å´„ÇΩ„Éº„Éà
        top_indices = np.argsort(similarities)[::-1][:top_k]

        results = [
            (self.documents[idx], similarities[idx])
            for idx in top_indices
        ]

        return results

# ‰ΩøÁî®‰æã
if __name__ == &quot;__main__&quot;:
    # Áü•Ë≠ò„Éô„Éº„ÇπÔºàFAQ„Å™„Å©Ôºâ
    knowledge_base = [
        &quot;Âñ∂Ê•≠ÊôÇÈñì„ÅØÂπ≥Êó•9ÊôÇ„Åã„Çâ18ÊôÇ„Åæ„Åß„Åß„Åô„ÄÇ&quot;,
        &quot;ËøîÂìÅ„ÅØË≥ºÂÖ•Âæå30Êó•‰ª•ÂÜÖ„Åß„ÅÇ„Çå„Å∞ÂèØËÉΩ„Åß„Åô„ÄÇ&quot;,
        &quot;ÈÄÅÊñô„ÅØÂÖ®ÂõΩ‰∏ÄÂæã500ÂÜÜ„Åß„Åô„ÄÇ5000ÂÜÜ‰ª•‰∏ä„ÅÆ„ÅîË≥ºÂÖ•„ÅßÁÑ°Êñô„Å´„Å™„Çä„Åæ„Åô„ÄÇ&quot;,
        &quot;„ÅäÊîØÊâï„ÅÑÊñπÊ≥ï„ÅØ„ÇØ„É¨„Ç∏„ÉÉ„Éà„Ç´„Éº„Éâ„ÄÅÈäÄË°åÊåØËæº„ÄÅ‰ª£ÈáëÂºïÊèõ„ÅåÂà©Áî®„Åß„Åç„Åæ„Åô„ÄÇ&quot;,
        &quot;‰ºöÂì°ÁôªÈå≤„Åô„Çã„Å®„Éù„Ç§„É≥„Éà„ÅåË≤Ø„Åæ„Çä„ÄÅÊ¨°ÂõûË≥ºÂÖ•ÊôÇ„Å´‰ΩøÁî®„Åß„Åç„Åæ„Åô„ÄÇ&quot;,
        &quot;„Éë„Çπ„ÉØ„Éº„Éâ„ÇíÂøò„Çå„ÅüÂ†¥Âêà„ÅØ„ÄÅ„É≠„Ç∞„Ç§„É≥ÁîªÈù¢„ÅÆ„Äå„Éë„Çπ„ÉØ„Éº„Éâ„ÇíÂøò„Çå„ÅüÊñπ„Äç„Åã„Çâ„É™„Çª„ÉÉ„Éà„Åß„Åç„Åæ„Åô„ÄÇ&quot;,
    ]

    qa_system = DocumentQASystem(knowledge_base)

    # Ë≥™Âïè
    questions = [
        &quot;ÈÄÅÊñô„ÅØ„ÅÑ„Åè„Çâ„Åß„Åô„ÅãÔºü&quot;,
        &quot;ËøîÂìÅ„Åß„Åç„Åæ„Åô„ÅãÔºü&quot;,
        &quot;‰ΩïÊôÇ„Åæ„ÅßÂñ∂Ê•≠„Åó„Å¶„ÅÑ„Åæ„Åô„ÅãÔºü&quot;
    ]

    for question in questions:
        print(f&quot;\nË≥™Âïè: {question}&quot;)
        answers = qa_system.answer_question(question, top_k=1)

        for answer, score in answers:
            print(f&quot;ÂõûÁ≠î: {answer}&quot;)
            print(f&quot;„Çπ„Ç≥„Ç¢: {score:.4f}&quot;)
</code></pre>
<h3>2.3 Transformer „Éô„Éº„Çπ QAÔºàBERTÔºâ</h3>
<pre><code class="language-python">from transformers import pipeline

class BertQASystem:
    &quot;&quot;&quot;BERT„Çí‰Ωø„Å£„ÅüÊäΩÂá∫ÂûãQA„Ç∑„Çπ„ÉÜ„É†&quot;&quot;&quot;

    def __init__(self, model_name='deepset/bert-base-cased-squad2'):
        &quot;&quot;&quot;
        Parameters:
        -----------
        model_name : str
            ‰ΩøÁî®„Åô„ÇãBERT„É¢„Éá„É´
        &quot;&quot;&quot;
        self.qa_pipeline = pipeline('question-answering', model=model_name)

    def answer(self, question: str, context: str) -&gt; Dict:
        &quot;&quot;&quot;
        Ë≥™Âïè„Å´ÂØæ„Åô„ÇãÂõûÁ≠î„ÇíÊäΩÂá∫

        Parameters:
        -----------
        question : str
            Ë≥™ÂïèÊñá
        context : str
            ÊñáËÑàÔºàÁ≠î„Åà„ÅåÂê´„Åæ„Çå„ÇãÊñáÊõ∏Ôºâ

        Returns:
        --------
        Dict
            answer: ÂõûÁ≠î, score: ‰ø°È†ºÂ∫¶„Çπ„Ç≥„Ç¢
        &quot;&quot;&quot;
        result = self.qa_pipeline({
            'question': question,
            'context': context
        })

        return {
            'answer': result['answer'],
            'score': result['score'],
            'start': result['start'],
            'end': result['end']
        }

# ‰ΩøÁî®‰æãÔºàËã±Ë™û„ÅÆ‰æãÔºâ
if __name__ == &quot;__main__&quot;:
    qa = BertQASystem()

    context = &quot;&quot;&quot;
    Apple Inc. is an American multinational technology company
    headquartered in Cupertino, California. It was founded by
    Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976.
    &quot;&quot;&quot;

    question = &quot;Who founded Apple?&quot;

    result = qa.answer(question, context)
    print(f&quot;Question: {question}&quot;)
    print(f&quot;Answer: {result['answer']}&quot;)
    print(f&quot;Confidence: {result['score']:.4f}&quot;)
</code></pre>
<hr />
<h2>3. „ÉÜ„Ç≠„Çπ„ÉàË¶ÅÁ¥ÑÔºàText SummarizationÔºâ</h2>
<h3>3.1 Ë¶ÅÁ¥Ñ„ÅÆÁ®ÆÈ°û</h3>
<h4>ÊäΩÂá∫ÂûãË¶ÅÁ¥Ñ (Extractive Summarization)</h4>
<p>ÈáçË¶Å„Å™Êñá„ÇíÊäΩÂá∫„Åó„Å¶Ë¶ÅÁ¥Ñ„Çí‰ΩúÊàê</p>
<div class="mermaid">
graph LR
    A[ÂÖÉ„ÅÆÊñáÊõ∏] --> B[Êñá„ÅÆ„Çπ„Ç≥„Ç¢„É™„É≥„Ç∞]
    B --> C[‰∏ä‰ΩçNÊñá„ÇíÈÅ∏Êäû]
    C --> D[Ë¶ÅÁ¥ÑÊñá]
</div>

<h4>ÁîüÊàêÂûãË¶ÅÁ¥Ñ (Abstractive Summarization)</h4>
<p>ÂÜÖÂÆπ„ÇíÁêÜËß£„Åó„Å¶Êñ∞„Åó„ÅÑÊñá„ÇíÁîüÊàê</p>
<div class="mermaid">
graph LR
    A[ÂÖÉ„ÅÆÊñáÊõ∏] --> B[ÊÑèÂë≥ÁêÜËß£]
    B --> C[Êñ∞„Åó„ÅÑÊñá„ÇíÁîüÊàê]
    C --> D[Ë¶ÅÁ¥ÑÊñá]
</div>

<h3>3.2 ÂÆüË£Ö: ÊäΩÂá∫ÂûãË¶ÅÁ¥Ñ</h3>
<pre><code class="language-python">import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx

class ExtractiveSummarizer:
    &quot;&quot;&quot;ÊäΩÂá∫Âûã„ÉÜ„Ç≠„Çπ„ÉàË¶ÅÁ¥Ñ„Ç∑„Çπ„ÉÜ„É†&quot;&quot;&quot;

    def __init__(self):
        self.vectorizer = TfidfVectorizer()

    def text_rank_summarize(self, text: str, num_sentences: int = 3) -&gt; str:
        &quot;&quot;&quot;
        TextRank„Ç¢„É´„Ç¥„É™„Ç∫„É†„Å´„Çà„ÇãË¶ÅÁ¥Ñ

        Parameters:
        -----------
        text : str
            Ë¶ÅÁ¥ÑÂØæË±°„ÅÆ„ÉÜ„Ç≠„Çπ„Éà
        num_sentences : int
            ÊäΩÂá∫„Åô„ÇãÊñá„ÅÆÊï∞

        Returns:
        --------
        str
            Ë¶ÅÁ¥ÑÊñá
        &quot;&quot;&quot;
        # Êñá„Å´ÂàÜÂâ≤
        sentences = [s.strip() for s in text.split('„ÄÇ') if s.strip()]

        if len(sentences) &lt;= num_sentences:
            return text

        # TF-IDF„Éô„ÇØ„Éà„É´Âåñ
        tfidf_matrix = self.vectorizer.fit_transform(sentences)

        # ÊñáÈñì„ÅÆÈ°û‰ººÂ∫¶Ë°åÂàó„ÇíË®àÁÆó
        similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)

        # „Ç∞„É©„Éï„ÇíÊßãÁØâ
        nx_graph = nx.from_numpy_array(similarity_matrix)

        # PageRank„Çπ„Ç≥„Ç¢„ÇíË®àÁÆó
        scores = nx.pagerank(nx_graph)

        # „Çπ„Ç≥„Ç¢È†Ü„Å´„ÇΩ„Éº„Éà
        ranked_sentences = sorted(
            ((scores[i], s) for i, s in enumerate(sentences)),
            reverse=True
        )

        # ‰∏ä‰ΩçNÊñá„ÇíÂÖÉ„ÅÆÈ†ÜÂ∫è„ÅßÊäΩÂá∫
        top_sentences = sorted(
            ranked_sentences[:num_sentences],
            key=lambda x: sentences.index(x[1])
        )

        summary = '„ÄÇ'.join([sentence for _, sentence in top_sentences]) + '„ÄÇ'
        return summary

    def tfidf_summarize(self, text: str, num_sentences: int = 3) -&gt; str:
        &quot;&quot;&quot;
        TF-IDF„Çπ„Ç≥„Ç¢„Å´„Çà„ÇãË¶ÅÁ¥Ñ

        Parameters:
        -----------
        text : str
            Ë¶ÅÁ¥ÑÂØæË±°„ÅÆ„ÉÜ„Ç≠„Çπ„Éà
        num_sentences : int
            ÊäΩÂá∫„Åô„ÇãÊñá„ÅÆÊï∞

        Returns:
        --------
        str
            Ë¶ÅÁ¥ÑÊñá
        &quot;&quot;&quot;
        sentences = [s.strip() for s in text.split('„ÄÇ') if s.strip()]

        if len(sentences) &lt;= num_sentences:
            return text

        # TF-IDF„Éô„ÇØ„Éà„É´Âåñ
        tfidf_matrix = self.vectorizer.fit_transform(sentences)

        # ÂêÑÊñá„ÅÆTF-IDF„Çπ„Ç≥„Ç¢ÂêàË®à„ÇíË®àÁÆó
        sentence_scores = np.array(tfidf_matrix.sum(axis=1)).flatten()

        # „Çπ„Ç≥„Ç¢„ÅåÈ´ò„ÅÑÈ†Ü„Å´„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÇíÂèñÂæó
        top_indices = np.argsort(sentence_scores)[::-1][:num_sentences]

        # ÂÖÉ„ÅÆÈ†ÜÂ∫è„Åß„ÇΩ„Éº„Éà
        top_indices = sorted(top_indices)

        # Ë¶ÅÁ¥ÑÊñá„ÇíÁîüÊàê
        summary = '„ÄÇ'.join([sentences[i] for i in top_indices]) + '„ÄÇ'
        return summary

# ‰ΩøÁî®‰æã
if __name__ == &quot;__main__&quot;:
    summarizer = ExtractiveSummarizer()

    long_text = &quot;&quot;&quot;
    Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜÔºàNLPÔºâ„ÅØ„ÄÅ‰∫∫Èñì„ÅåÊó•Â∏∏ÁöÑ„Å´‰Ωø„ÅÜË®ÄË™û„Çí„Ç≥„É≥„Éî„É•„Éº„Çø„ÅßÂá¶ÁêÜ„Åô„ÇãÊäÄË°ì„Åß„Åô„ÄÇ
    NLP„Å´„ÅØÊßò„ÄÖ„Å™„Çø„Çπ„ÇØ„Åå„ÅÇ„Çä„ÄÅÊ©üÊ¢∞ÁøªË®≥„ÄÅÊÑüÊÉÖÂàÜÊûê„ÄÅË≥™ÂïèÂøúÁ≠î„Å™„Å©„ÅåÂê´„Åæ„Çå„Åæ„Åô„ÄÇ
    ËøëÂπ¥„ÄÅÊ∑±Â±§Â≠¶Áøí„ÅÆÁô∫Â±ï„Å´„Çà„Çä„ÄÅNLP„ÅÆÁ≤æÂ∫¶„ÅåÂ§ß„Åç„ÅèÂêë‰∏ä„Åó„Åæ„Åó„Åü„ÄÇ
    Áâπ„Å´„ÄÅTransformer„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÅÆÁôªÂ†¥„ÅØÈù©ÂëΩÁöÑ„Åß„Åó„Åü„ÄÇ
    BERT„ÇÑGPT„Å®„ÅÑ„Å£„ÅüÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÅåÈñãÁô∫„Åï„Çå„ÄÅÂ§ö„Åè„ÅÆ„Çø„Çπ„ÇØ„Åß‰∫∫Èñì„Å´Ëøë„ÅÑÊÄßËÉΩ„ÇíÂÆüÁèæ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
    ÁèæÂú®„Åß„ÅØ„ÄÅChatGPT„ÅÆ„Çà„ÅÜ„Å™ÂØæË©±ÂûãAI„ÅåÂ∫É„ÅèÂà©Áî®„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ
    ‰ªäÂæå„ÇÇNLPÊäÄË°ì„ÅÆÈÄ≤Âåñ„ÅØÁ∂ö„Åè„Å®‰∫àÊÉ≥„Åï„Çå„Åæ„Åô„ÄÇ
    &quot;&quot;&quot;

    print(&quot;„ÄêTextRankË¶ÅÁ¥Ñ„Äë&quot;)
    summary1 = summarizer.text_rank_summarize(long_text, num_sentences=3)
    print(summary1)

    print(&quot;\n„ÄêTF-IDFË¶ÅÁ¥Ñ„Äë&quot;)
    summary2 = summarizer.tfidf_summarize(long_text, num_sentences=3)
    print(summary2)
</code></pre>
<h3>3.3 ÁîüÊàêÂûãË¶ÅÁ¥ÑÔºàTransformer„Éô„Éº„ÇπÔºâ</h3>
<pre><code class="language-python">from transformers import pipeline

class AbstractiveSummarizer:
    &quot;&quot;&quot;ÁîüÊàêÂûã„ÉÜ„Ç≠„Çπ„ÉàË¶ÅÁ¥Ñ„Ç∑„Çπ„ÉÜ„É†&quot;&quot;&quot;

    def __init__(self, model_name='facebook/bart-large-cnn'):
        &quot;&quot;&quot;
        Parameters:
        -----------
        model_name : str
            ‰ΩøÁî®„Åô„Çã„É¢„Éá„É´Âêç
        &quot;&quot;&quot;
        self.summarizer = pipeline('summarization', model=model_name)

    def summarize(self, text: str, max_length: int = 130,
                  min_length: int = 30) -&gt; str:
        &quot;&quot;&quot;
        „ÉÜ„Ç≠„Çπ„Éà„ÇíË¶ÅÁ¥Ñ

        Parameters:
        -----------
        text : str
            Ë¶ÅÁ¥ÑÂØæË±°„ÅÆ„ÉÜ„Ç≠„Çπ„Éà
        max_length : int
            Ë¶ÅÁ¥Ñ„ÅÆÊúÄÂ§ßÈï∑
        min_length : int
            Ë¶ÅÁ¥Ñ„ÅÆÊúÄÂ∞èÈï∑

        Returns:
        --------
        str
            Ë¶ÅÁ¥ÑÊñá
        &quot;&quot;&quot;
        summary = self.summarizer(
            text,
            max_length=max_length,
            min_length=min_length,
            do_sample=False
        )

        return summary[0]['summary_text']

# ‰ΩøÁî®‰æãÔºàËã±Ë™ûÔºâ
if __name__ == &quot;__main__&quot;:
    summarizer = AbstractiveSummarizer()

    article = &quot;&quot;&quot;
    Artificial intelligence (AI) is intelligence demonstrated by machines,
    in contrast to the natural intelligence displayed by humans and animals.
    Leading AI textbooks define the field as the study of &quot;intelligent agents&quot;:
    any device that perceives its environment and takes actions that maximize
    its chance of successfully achieving its goals. Colloquially, the term
    &quot;artificial intelligence&quot; is often used to describe machines (or computers)
    that mimic &quot;cognitive&quot; functions that humans associate with the human mind,
    such as &quot;learning&quot; and &quot;problem solving&quot;.
    &quot;&quot;&quot;

    summary = summarizer.summarize(article)
    print(&quot;Summary:&quot;, summary)
</code></pre>
<hr />
<h2>4. Ê©üÊ¢∞ÁøªË®≥ÔºàMachine TranslationÔºâ</h2>
<h3>4.1 Ê©üÊ¢∞ÁøªË®≥„ÅÆÊ≠¥Âè≤</h3>
<div class="mermaid">
timeline
    title Ê©üÊ¢∞ÁøªË®≥„ÅÆÁô∫Â±ï
    1950s-1980s : „É´„Éº„É´„Éô„Éº„ÇπÁøªË®≥ : ÊñáÊ≥ïË¶èÂâá„ÇíÊâãÂãïÂÆöÁæ©
    1990s-2000s : Áµ±Ë®àÁöÑÊ©üÊ¢∞ÁøªË®≥ : „Ç≥„Éº„Éë„Çπ„Åã„ÇâÁµ±Ë®à„É¢„Éá„É´Â≠¶Áøí
    2014-2016 : „Éã„É•„Éº„É©„É´Ê©üÊ¢∞ÁøªË®≥ : Seq2Seq + Attention
    2017-ÁèæÂú® : TransformerÁøªË®≥ : È´òÁ≤æÂ∫¶„ÉªÈ´òÈÄü„Å™ÁøªË®≥
</div>

<h3>4.2 ÂÆüË£Ö: Transformer „Éô„Éº„ÇπÁøªË®≥</h3>
<pre><code class="language-python">from transformers import MarianMTModel, MarianTokenizer

class NeuralTranslator:
    &quot;&quot;&quot;„Éã„É•„Éº„É©„É´Ê©üÊ¢∞ÁøªË®≥„Ç∑„Çπ„ÉÜ„É†&quot;&quot;&quot;

    def __init__(self, src_lang: str = 'en', tgt_lang: str = 'ja'):
        &quot;&quot;&quot;
        Parameters:
        -----------
        src_lang : str
            ÁøªË®≥ÂÖÉË®ÄË™û„Ç≥„Éº„Éâ
        tgt_lang : str
            ÁøªË®≥ÂÖàË®ÄË™û„Ç≥„Éº„Éâ
        &quot;&quot;&quot;
        model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}'

        try:
            self.tokenizer = MarianTokenizer.from_pretrained(model_name)
            self.model = MarianMTModel.from_pretrained(model_name)
        except Exception as e:
            print(f&quot;„É¢„Éá„É´ {model_name} „ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì: {e}&quot;)
            raise

    def translate(self, text: str) -&gt; str:
        &quot;&quot;&quot;
        „ÉÜ„Ç≠„Çπ„Éà„ÇíÁøªË®≥

        Parameters:
        -----------
        text : str
            ÁøªË®≥„Åô„Çã„ÉÜ„Ç≠„Çπ„Éà

        Returns:
        --------
        str
            ÁøªË®≥„Åï„Çå„Åü„ÉÜ„Ç≠„Çπ„Éà
        &quot;&quot;&quot;
        # „Éà„Éº„ÇØ„É≥Âåñ
        inputs = self.tokenizer(text, return_tensors='pt', padding=True)

        # ÁøªË®≥
        translated = self.model.generate(**inputs)

        # „Éá„Ç≥„Éº„Éâ
        translated_text = self.tokenizer.decode(
            translated[0],
            skip_special_tokens=True
        )

        return translated_text

    def batch_translate(self, texts: List[str]) -&gt; List[str]:
        &quot;&quot;&quot;Ë§áÊï∞„ÉÜ„Ç≠„Çπ„Éà„ÅÆ‰∏ÄÊã¨ÁøªË®≥&quot;&quot;&quot;
        inputs = self.tokenizer(texts, return_tensors='pt', padding=True)
        translated = self.model.generate(**inputs)

        return [
            self.tokenizer.decode(t, skip_special_tokens=True)
            for t in translated
        ]

# ‰ΩøÁî®‰æã
if __name__ == &quot;__main__&quot;:
    # Ëã±Ë™û‚ÜíÊó•Êú¨Ë™û
    translator_en_ja = NeuralTranslator(src_lang='en', tgt_lang='ja')

    english_texts = [
        &quot;Hello, how are you?&quot;,
        &quot;Natural language processing is fascinating.&quot;,
        &quot;Machine learning has revolutionized AI.&quot;
    ]

    print(&quot;„ÄêËã±Ë™û ‚Üí Êó•Êú¨Ë™û„Äë&quot;)
    for eng, jpn in zip(english_texts, translator_en_ja.batch_translate(english_texts)):
        print(f&quot;{eng} ‚Üí {jpn}&quot;)
</code></pre>
<h3>4.3 ÂøúÁî®: Â§öË®ÄË™ûÂØæÂøú„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà</h3>
<pre><code class="language-python">class MultilingualChatbot:
    &quot;&quot;&quot;Â§öË®ÄË™ûÂØæÂøú„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà&quot;&quot;&quot;

    def __init__(self):
        # ÂêÑË®ÄË™û„Éö„Ç¢„ÅÆÁøªË®≥Âô®
        self.translators = {
            ('en', 'ja'): NeuralTranslator('en', 'ja'),
            ('ja', 'en'): NeuralTranslator('ja', 'en'),
        }

        # QA„Ç∑„Çπ„ÉÜ„É†ÔºàËã±Ë™û„Éô„Éº„ÇπÔºâ
        self.qa_system = DocumentQASystem([
            &quot;Our business hours are 9 AM to 6 PM on weekdays.&quot;,
            &quot;Returns are possible within 30 days of purchase.&quot;,
            &quot;Shipping is 500 yen nationwide. Free for purchases over 5000 yen.&quot;
        ])

    def answer_question(self, question: str, lang: str = 'ja') -&gt; str:
        &quot;&quot;&quot;
        Ë≥™Âïè„Å´Â§öË®ÄË™û„ÅßÂõûÁ≠î

        Parameters:
        -----------
        question : str
            Ë≥™ÂïèÊñá
        lang : str
            Ë≥™Âïè„ÅÆË®ÄË™û„Ç≥„Éº„Éâ

        Returns:
        --------
        str
            ÂõûÁ≠îÔºàË≥™Âïè„Å®Âêå„ÅòË®ÄË™ûÔºâ
        &quot;&quot;&quot;
        # Êó•Êú¨Ë™û„ÅÆË≥™Âïè„ÇíËã±Ë™û„Å´ÁøªË®≥
        if lang == 'ja':
            question_en = self.translators[('ja', 'en')].translate(question)
        else:
            question_en = question

        # Ëã±Ë™û„ÅßÂõûÁ≠î„ÇíÊ§úÁ¥¢
        answers = self.qa_system.answer_question(question_en, top_k=1)
        answer_en = answers[0][0]

        # ÂõûÁ≠î„ÇíÊó•Êú¨Ë™û„Å´ÁøªË®≥
        if lang == 'ja':
            answer_ja = self.translators[('en', 'ja')].translate(answer_en)
            return answer_ja
        else:
            return answer_en

# ‰ΩøÁî®‰æã
chatbot = MultilingualChatbot()
print(chatbot.answer_question(&quot;ÈÄÅÊñô„ÅØ„ÅÑ„Åè„Çâ„Åß„Åô„ÅãÔºü&quot;, lang='ja'))
</code></pre>
<hr />
<h2>5. ÊúÄÊñ∞„Éà„É¨„É≥„Éâ: Â§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàLLMÔºâ</h2>
<h3>5.1 LLM„Å®„ÅØ</h3>
<p><strong>Â§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàLarge Language Model, LLMÔºâ</strong> „ÅØ„ÄÅËÜ®Â§ß„Å™„ÉÜ„Ç≠„Çπ„Éà„Éá„Éº„Çø„ÅßË®ìÁ∑¥„Åï„Çå„ÅüÂ∑®Â§ß„Å™„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„É¢„Éá„É´„Åß„Åô„ÄÇ</p>
<h4>‰∏ª„Å™LLM</h4>
<table>
<thead>
<tr>
<th>„É¢„Éá„É´</th>
<th>ÈñãÁô∫ÂÖÉ</th>
<th>„Éë„É©„É°„Éº„ÇøÊï∞</th>
<th>ÁâπÂæ¥</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>GPT-4</strong></td>
<td>OpenAI</td>
<td>ÈùûÂÖ¨ÈñãÔºàÊé®ÂÆö1ÂÖÜ‰ª•‰∏äÔºâ</td>
<td>„Éû„É´„ÉÅ„É¢„Éº„ÉÄ„É´„ÄÅÈ´òÁ≤æÂ∫¶</td>
</tr>
<tr>
<td><strong>Claude</strong></td>
<td>Anthropic</td>
<td>ÈùûÂÖ¨Èñã</td>
<td>Èï∑ÊñáÂØæÂøú„ÄÅÂÆâÂÖ®ÊÄßÈáçË¶ñ</td>
</tr>
<tr>
<td><strong>Gemini</strong></td>
<td>Google</td>
<td>ÈùûÂÖ¨Èñã</td>
<td>„Éû„É´„ÉÅ„É¢„Éº„ÉÄ„É´</td>
</tr>
<tr>
<td><strong>LLaMA</strong></td>
<td>Meta</td>
<td>7B-70B</td>
<td>„Ç™„Éº„Éó„É≥„ÇΩ„Éº„Çπ</td>
</tr>
<tr>
<td><strong>GPT-3.5</strong></td>
<td>OpenAI</td>
<td>175B</td>
<td>ChatGPT„ÅÆ„Éô„Éº„Çπ</td>
</tr>
</tbody>
</table>
<h3>5.2 LLM„ÅÆËÉΩÂäõ</h3>
<div class="mermaid">
graph TD
    A[LLM] --> B[„ÉÜ„Ç≠„Çπ„ÉàÁîüÊàê]
    A --> C[Ë≥™ÂïèÂøúÁ≠î]
    A --> D[Ë¶ÅÁ¥Ñ]
    A --> E[ÁøªË®≥]
    A --> F[„Ç≥„Éº„ÉâÁîüÊàê]
    A --> G[Êé®Ë´ñ]
    A --> H[Ââµ‰Ωú]
</div>

<h4>„Çº„É≠„Ç∑„Éß„ÉÉ„ÉàÂ≠¶Áøí</h4>
<p>Ë®ìÁ∑¥„Éá„Éº„Çø„Å™„Åó„Åß„Çø„Çπ„ÇØ„ÇíÂÆüË°å</p>
<pre><code class="language-python"># ‰æã: GPT-3.5„Çí‰Ωø„Å£„ÅüÊÑüÊÉÖÂàÜÊûêÔºàÂ≠¶Áøí‰∏çË¶ÅÔºâ
import openai

def zero_shot_sentiment(text: str) -&gt; str:
    &quot;&quot;&quot;„Çº„É≠„Ç∑„Éß„ÉÉ„ÉàÊÑüÊÉÖÂàÜÊûê&quot;&quot;&quot;
    prompt = f&quot;&quot;&quot;
    Ê¨°„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÅÆÊÑüÊÉÖ„Çí„ÄåPositive„Äç„ÄåNegative„Äç„ÄåNeutral„Äç„ÅßÂàÜÈ°û„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

    „ÉÜ„Ç≠„Çπ„Éà: {text}
    ÊÑüÊÉÖ:
    &quot;&quot;&quot;

    response = openai.ChatCompletion.create(
        model=&quot;gpt-3.5-turbo&quot;,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}]
    )

    return response.choices[0].message.content.strip()
</code></pre>
<h4>Few-ShotÂ≠¶Áøí</h4>
<p>Â∞ëÊï∞„ÅÆ‰æã„Åã„ÇâÂ≠¶Áøí</p>
<pre><code class="language-python">def few_shot_classification(text: str) -&gt; str:
    &quot;&quot;&quot;Few-ShotÂàÜÈ°û&quot;&quot;&quot;
    prompt = f&quot;&quot;&quot;
    ‰ª•‰∏ã„ÅÆ‰æã„ÇíÂèÇËÄÉ„Å´„ÄÅ„ÉÜ„Ç≠„Çπ„Éà„Çí„Ç´„ÉÜ„Ç¥„É™ÂàÜÈ°û„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

    ‰æã1:
    „ÉÜ„Ç≠„Çπ„Éà: ÂïÜÂìÅ„ÅåÂ±ä„Åç„Åæ„Åõ„Çì
    „Ç´„ÉÜ„Ç¥„É™: ÈÖçÈÄÅ

    ‰æã2:
    „ÉÜ„Ç≠„Çπ„Éà: ËøîÈáë„Åó„Å¶„Åè„Å†„Åï„ÅÑ
    „Ç´„ÉÜ„Ç¥„É™: ËøîÈáë

    ‰æã3:
    „ÉÜ„Ç≠„Çπ„Éà: ‰Ωø„ÅÑÊñπ„Åå„Çè„Åã„Çä„Åæ„Åõ„Çì
    „Ç´„ÉÜ„Ç¥„É™: ‰Ωø„ÅÑÊñπ

    Êñ∞„Åó„ÅÑ„ÉÜ„Ç≠„Çπ„Éà:
    „ÉÜ„Ç≠„Çπ„Éà: {text}
    „Ç´„ÉÜ„Ç¥„É™:
    &quot;&quot;&quot;

    response = openai.ChatCompletion.create(
        model=&quot;gpt-3.5-turbo&quot;,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}]
    )

    return response.choices[0].message.content.strip()
</code></pre>
<h3>5.3 „Éó„É≠„É≥„Éó„Éà„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞</h3>
<p>LLM„Åã„ÇâÊúÄËâØ„ÅÆÁµêÊûú„ÇíÂæó„Çã„Åü„ÇÅ„ÅÆÊäÄË°ì</p>
<h4>Âü∫Êú¨ÂéüÂâá</h4>
<ol>
<li><strong>ÊòéÁ¢∫„Å™ÊåáÁ§∫</strong>: ÊõñÊòß„Åï„ÇíÈÅø„Åë„Çã</li>
<li><strong>ÊñáËÑà„ÅÆÊèê‰æõ</strong>: ÂøÖË¶Å„Å™ËÉåÊôØÊÉÖÂ†±„ÇíÂê´„ÇÅ„Çã</li>
<li><strong>Âá∫ÂäõÂΩ¢Âºè„ÅÆÊåáÂÆö</strong>: JSON„ÄÅÁÆáÊù°Êõ∏„Åç„Å™„Å©</li>
<li><strong>Âà∂Á¥Ñ„ÅÆË®≠ÂÆö</strong>: ÊñáÂ≠óÊï∞Âà∂Èôê„ÄÅ„Éà„Éº„É≥ÊåáÂÆö</li>
</ol>
<h4>ÂÆüË∑µ‰æã</h4>
<pre><code class="language-python">class PromptEngineer:
    &quot;&quot;&quot;„Éó„É≠„É≥„Éó„Éà„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞„ÅÆ„Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„Çπ&quot;&quot;&quot;

    @staticmethod
    def create_structured_prompt(task: str, context: str,
                                 format: str, constraints: List[str]) -&gt; str:
        &quot;&quot;&quot;
        ÊßãÈÄ†Âåñ„Åï„Çå„Åü„Éó„É≠„É≥„Éó„Éà„Çí‰ΩúÊàê

        Parameters:
        -----------
        task : str
            „Çø„Çπ„ÇØ„ÅÆË™¨Êòé
        context : str
            ÊñáËÑàÊÉÖÂ†±
        format : str
            Âá∫ÂäõÂΩ¢Âºè
        constraints : List[str]
            Âà∂Á¥ÑÊù°‰ª∂

        Returns:
        --------
        str
            ÂÆåÊàê„Åó„Åü„Éó„É≠„É≥„Éó„Éà
        &quot;&quot;&quot;
        prompt = f&quot;&quot;&quot;
## „Çø„Çπ„ÇØ
{task}

## ÊñáËÑà
{context}

## Âá∫ÂäõÂΩ¢Âºè
{format}

## Âà∂Á¥ÑÊù°‰ª∂
&quot;&quot;&quot;
        for i, constraint in enumerate(constraints, 1):
            prompt += f&quot;{i}. {constraint}\n&quot;

        return prompt

# ‰ΩøÁî®‰æã
prompt = PromptEngineer.create_structured_prompt(
    task=&quot;Ë£ΩÂìÅ„É¨„Éì„É•„Éº„ÇíÂàÜÊûê„Åó„ÄÅÊîπÂñÑÁÇπ„ÇíÊèêÊ°à„Åó„Å¶„Åè„Å†„Åï„ÅÑ&quot;,
    context=&quot;ÊúÄÊñ∞„ÅÆ„Çπ„Éû„Éº„Éà„Éï„Ç©„É≥„Å´Èñ¢„Åô„Çã100‰ª∂„ÅÆ„É¨„Éì„É•„Éº&quot;,
    format=&quot;JSONÂΩ¢Âºè„Åß„ÄÅ„Ç´„ÉÜ„Ç¥„É™„Åî„Å®„Å´ÊîπÂñÑÁÇπ„Çí„É™„Çπ„ÉàÂåñ&quot;,
    constraints=[
        &quot;ÊúÄÂ§ß5„Å§„ÅÆÊîπÂñÑÁÇπ&quot;,
        &quot;ÂÖ∑‰ΩìÁöÑ„ÅßÂÆüË°åÂèØËÉΩ„Å™ÊèêÊ°à&quot;,
        &quot;ËÇØÂÆöÁöÑ„Å™„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ„ÇÇÂê´„ÇÅ„Çã&quot;
    ]
)

print(prompt)
</code></pre>
<h3>5.4 RAG (Retrieval-Augmented Generation)</h3>
<p>Â§ñÈÉ®Áü•Ë≠ò„ÇíÊ¥ªÁî®„Åó„Å¶LLM„ÅÆÂõûÁ≠îÁ≤æÂ∫¶„ÇíÂêë‰∏ä</p>
<pre><code class="language-python">from typing import List
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class RAGSystem:
    &quot;&quot;&quot;RAGÔºàÊ§úÁ¥¢Êã°ÂºµÁîüÊàêÔºâ„Ç∑„Çπ„ÉÜ„É†&quot;&quot;&quot;

    def __init__(self, knowledge_base: List[str]):
        &quot;&quot;&quot;
        Parameters:
        -----------
        knowledge_base : List[str]
            Áü•Ë≠ò„Éô„Éº„ÇπÔºàÊñáÊõ∏„ÅÆ„É™„Çπ„ÉàÔºâ
        &quot;&quot;&quot;
        self.knowledge_base = knowledge_base
        self.vectorizer = TfidfVectorizer()
        self.doc_vectors = self.vectorizer.fit_transform(knowledge_base)

    def retrieve_relevant_docs(self, query: str, top_k: int = 3) -&gt; List[str]:
        &quot;&quot;&quot;
        „ÇØ„Ç®„É™„Å´Èñ¢ÈÄ£„Åô„ÇãÊñáÊõ∏„ÇíÊ§úÁ¥¢

        Parameters:
        -----------
        query : str
            Ê§úÁ¥¢„ÇØ„Ç®„É™
        top_k : int
            ÂèñÂæó„Åô„ÇãÊñáÊõ∏Êï∞

        Returns:
        --------
        List[str]
            Èñ¢ÈÄ£ÊñáÊõ∏„ÅÆ„É™„Çπ„Éà
        &quot;&quot;&quot;
        query_vector = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vector, self.doc_vectors)[0]

        top_indices = np.argsort(similarities)[::-1][:top_k]
        return [self.knowledge_base[i] for i in top_indices]

    def generate_answer(self, question: str) -&gt; str:
        &quot;&quot;&quot;
        RAG„Å´„Çà„ÇãÂõûÁ≠îÁîüÊàê

        Parameters:
        -----------
        question : str
            Ë≥™Âïè

        Returns:
        --------
        str
            ÁîüÊàê„Åï„Çå„ÅüÂõûÁ≠î
        &quot;&quot;&quot;
        # 1. Èñ¢ÈÄ£ÊñáÊõ∏„ÇíÊ§úÁ¥¢
        relevant_docs = self.retrieve_relevant_docs(question)

        # 2. ÊñáËÑà„ÇíÊßãÁØâ
        context = &quot;\n&quot;.join(relevant_docs)

        # 3. LLM„Å´Ë≥™Âïè„Å®ÊñáËÑà„ÇíÊ∏°„Åó„Å¶ÂõûÁ≠îÁîüÊàê
        prompt = f&quot;&quot;&quot;
‰ª•‰∏ã„ÅÆÊñáËÑà„ÇíÂèÇËÄÉ„Å´„ÄÅË≥™Âïè„Å´Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„ÄêÊñáËÑà„Äë
{context}

„ÄêË≥™Âïè„Äë
{question}

„ÄêÂõûÁ≠î„Äë
&quot;&quot;&quot;

        # ÂÆüÈöõ„Å´„ÅØ„Åì„Åì„ÅßLLM API„ÇíÂëº„Å≥Âá∫„Åô
        # response = openai.ChatCompletion.create(...)
        # return response.choices[0].message.content

        # „Éá„É¢Áî®„ÅÆÁ∞°ÊòìÂÆüË£Ö
        return f&quot;ÊñáËÑà„Å´Âü∫„Å•„ÅèÂõûÁ≠îÔºàÂÆüÈöõ„Å´„ÅØLLM„ÅåÁîüÊàêÔºâ\nÈñ¢ÈÄ£ÊÉÖÂ†±: {relevant_docs[0][:100]}...&quot;

# ‰ΩøÁî®‰æã
knowledge_base = [
    &quot;ÂΩìÁ§æ„ÅØ1980Âπ¥„Å´Êù±‰∫¨„ÅßÂâµÊ•≠„Åó„Åæ„Åó„Åü„ÄÇ&quot;,
    &quot;‰∏ªÂäõË£ΩÂìÅ„ÅØÁî£Ê•≠Áî®„É≠„Éú„ÉÉ„Éà„Åß„Åô„ÄÇ&quot;,
    &quot;ÂæìÊ•≠Âì°Êï∞„ÅØÁ¥Ñ5000‰∫∫„Åß„ÄÅ‰∏ñÁïå20„Ç´ÂõΩ„Å´Êã†ÁÇπ„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ&quot;,
    &quot;Âπ¥ÈñìÂ£≤‰∏äÈ´ò„ÅØÁ¥Ñ1000ÂÑÑÂÜÜ„Åß„Åô„ÄÇ&quot;,
    &quot;Á†îÁ©∂ÈñãÁô∫„Å´Â£≤‰∏ä„ÅÆ15%„ÇíÊäïË≥á„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ&quot;
]

rag = RAGSystem(knowledge_base)
answer = rag.generate_answer(&quot;‰ºöÁ§æ„ÅÆÂâµÊ•≠Âπ¥„Å®Â†¥ÊâÄ„ÇíÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ&quot;)
print(answer)
</code></pre>
<hr />
<h2>6. NLP„ÅÆÈÅ∏Êäû„Ç¨„Ç§„Éâ</h2>
<h3>6.1 „Çø„Çπ„ÇØÂà•„ÅÆÈÅ∏Êäû„Éï„É≠„Éº„ÉÅ„É£„Éº„Éà</h3>
<div class="mermaid">
graph TD
    A[NLP„Çø„Çπ„ÇØÈÅ∏Êäû] --> B{„Éá„Éº„ÇøÊßãÈÄ†Âåñ?}
    B -->|Yes| C[ÊÉÖÂ†±ÊäΩÂá∫]
    B -->|No| D{ÂõûÁ≠îÁîüÊàê?}
    D -->|Yes| E[Ë≥™ÂïèÂøúÁ≠î]
    D -->|No| F{Ë¶ÅÁ¥Ñ„ÅåÂøÖË¶Å?}
    F -->|Yes| G[„ÉÜ„Ç≠„Çπ„ÉàË¶ÅÁ¥Ñ]
    F -->|No| H{Ë®ÄË™ûÂ§âÊèõ?}
    H -->|Yes| I[Ê©üÊ¢∞ÁøªË®≥]
    H -->|No| J{ÂàÜÈ°û?}
    J -->|Yes| K[„ÉÜ„Ç≠„Çπ„ÉàÂàÜÈ°û]
    J -->|No| L[„Ç´„Çπ„Çø„É†NLP]
</div>

<h3>6.2 „Éì„Ç∏„Éç„ÇπÂøúÁî®„Éû„Éà„É™„ÇØ„Çπ</h3>
<table>
<thead>
<tr>
<th>„Éì„Ç∏„Éç„ÇπË™≤È°å</th>
<th>Êé®Â•®NLP„Çø„Çπ„ÇØ</th>
<th>ÊúüÂæÖÂäπÊûú</th>
</tr>
</thead>
<tbody>
<tr>
<td>È°ßÂÆ¢„ÅÆÂ£∞„ÅÆÂàÜÊûê</td>
<td>ÊÑüÊÉÖÂàÜÊûê + ÊÉÖÂ†±ÊäΩÂá∫</td>
<td>Ê∫ÄË∂≥Â∫¶Âêë‰∏ä</td>
</tr>
<tr>
<td>„Çµ„Éù„Éº„ÉàËá™ÂãïÂåñ</td>
<td>Ë≥™ÂïèÂøúÁ≠î + ÂàÜÈ°û</td>
<td>„Ç≥„Çπ„ÉàÂâäÊ∏õ</td>
</tr>
<tr>
<td>ÊñáÊõ∏ÁÆ°ÁêÜ</td>
<td>Ë¶ÅÁ¥Ñ + ÂàÜÈ°û</td>
<td>Ê•≠ÂãôÂäπÁéáÂåñ</td>
</tr>
<tr>
<td>„Ç∞„É≠„Éº„Éê„É´Â±ïÈñã</td>
<td>Ê©üÊ¢∞ÁøªË®≥</td>
<td>Â∏ÇÂ†¥Êã°Â§ß</td>
</tr>
<tr>
<td>ÊÑèÊÄùÊ±∫ÂÆöÊîØÊè¥</td>
<td>ÊÉÖÂ†±ÊäΩÂá∫ + Ë¶ÅÁ¥Ñ</td>
<td>ËøÖÈÄü„Å™Âà§Êñ≠</td>
</tr>
</tbody>
</table>
<hr />
<h2>7. „Åæ„Å®„ÇÅ</h2>
<h3>7.1 Êú¨„Ç∑„É™„Éº„Ç∫„ÅßÂ≠¶„Çì„Å†„Åì„Å®</h3>
<h4>Chapter 1: NLP„ÅÆÂü∫Á§é</h4>
<ul>
<li>NLP„ÅÆÂÆöÁæ©„Å®‰∏ªË¶Å„Çø„Çπ„ÇØ</li>
<li>Ê≠¥Âè≤ÁöÑÁô∫Â±ïÔºà„É´„Éº„É´„Éô„Éº„Çπ‚ÜíÁµ±Ë®à‚ÜíÊ∑±Â±§Â≠¶ÁøíÔºâ</li>
<li>Êó•Êú¨Ë™ûNLP„ÅÆÁâπÂæ¥</li>
</ul>
<h4>Chapter 2: ÂΩ¢ÊÖãÁ¥†Ëß£Êûê„ÉªÊßãÊñáËß£Êûê</h4>
<ul>
<li>„Éà„Éº„ÇØ„Éä„Ç§„Çº„Éº„Ç∑„Éß„É≥</li>
<li>ÂΩ¢ÊÖãÁ¥†Ëß£ÊûêÔºàMeCab, JanomeÔºâ</li>
<li>ÊßãÊñáËß£Êûê„Å®‰æùÂ≠òÈñ¢‰øÇ</li>
<li>Âõ∫ÊúâË°®ÁèæÊäΩÂá∫ÔºàNERÔºâ</li>
</ul>
<h4>Chapter 3: „ÉÜ„Ç≠„Çπ„ÉàÂàÜÈ°û</h4>
<ul>
<li>„ÉÜ„Ç≠„Çπ„ÉàÂâçÂá¶ÁêÜ„Éë„Ç§„Éó„É©„Ç§„É≥</li>
<li>ÁâπÂæ¥ÈáèÊäΩÂá∫ÔºàBoW, TF-IDFÔºâ</li>
<li>Ê©üÊ¢∞Â≠¶Áøí„É¢„Éá„É´ÔºàNaive Bayes, SVMÔºâ</li>
<li>„É¢„Éá„É´Ë©ï‰æ°ÊâãÊ≥ï</li>
</ul>
<h4>Chapter 4: ÂÆü‰∏ñÁïå„ÅÆÂøúÁî®</h4>
<ul>
<li>ÊÉÖÂ†±ÊäΩÂá∫„Ç∑„Çπ„ÉÜ„É†</li>
<li>Ë≥™ÂïèÂøúÁ≠î„Ç∑„Çπ„ÉÜ„É†</li>
<li>„ÉÜ„Ç≠„Çπ„ÉàË¶ÅÁ¥Ñ</li>
<li>Ê©üÊ¢∞ÁøªË®≥</li>
<li>ÊúÄÊñ∞„Éà„É¨„É≥„ÉâÔºàLLM, RAGÔºâ</li>
</ul>
<h3>7.2 Ê¨°„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó</h3>
<p>Êú¨„Ç∑„É™„Éº„Ç∫„ÅßNLP„ÅÆÂü∫Á§é„ÇíÂ≠¶„Å≥„Åæ„Åó„Åü„ÄÇ„Åï„Çâ„Å´Ê∑±„ÇÅ„Çã„Å´„ÅØÔºö</p>
<h4>Áô∫Â±ïÁöÑ„Å™„Éà„Éî„ÉÉ„ÇØ</h4>
<ol>
<li>
<p><strong>Ê∑±Â±§Â≠¶ÁøíNLP</strong>
   - LSTM, GRUÔºàÂÜçÂ∏∞Âûã„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÔºâ
   - TransformerË©≥Á¥∞
   - BERT, GPT„ÅÆÂÜÖÈÉ®ÊßãÈÄ†</p>
</li>
<li>
<p><strong>ÂÆüË∑µÁöÑ„Å™„Çπ„Ç≠„É´</strong>
   - Â§ßË¶èÊ®°„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÊâ±„ÅÑ
   - „É¢„Éá„É´„ÅÆ„Éá„Éó„É≠„Ç§ÔºàWeb APIÂåñÔºâ
   - „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÉÅ„É•„Éº„Éã„É≥„Ç∞</p>
</li>
<li>
<p><strong>Â∞ÇÈñÄÂàÜÈáé</strong>
   - „Éâ„É°„Ç§„É≥Âõ∫ÊúâNLPÔºàÂåªÁôÇ„ÄÅÊ≥ïÂæã„ÄÅÈáëËûçÔºâ
   - „Éû„É´„ÉÅ„É¢„Éº„ÉÄ„É´AIÔºà„ÉÜ„Ç≠„Çπ„Éà+ÁîªÂÉèÔºâ
   - ‰ºöË©±AI„ÉªÂØæË©±„Ç∑„Çπ„ÉÜ„É†</p>
</li>
</ol>
<h4>Êé®Â•®Â≠¶Áøí„Éë„Çπ</h4>
<div class="mermaid">
graph LR
    A[Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜÂÖ•ÈñÄ] --> B[ÂçòË™ûÂüã„ÇÅËæº„ÅøÂÖ•ÈñÄ]
    B --> C[TransformerÂÖ•ÈñÄ]
    C --> D[BERTÂÖ•ÈñÄ]
    C --> E[GPTÂÖ•ÈñÄ]
    D --> F[Â§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ]
    E --> F
</div>

<h3>7.3 ÂÆüË∑µ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆ„Ç¢„Ç§„Éá„Ç¢</h3>
<ol>
<li>
<p><strong>ÊÑüÊÉÖÂàÜÊûê„ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ</strong>
   - Twitter API„ÅßÁâπÂÆö„Éà„Éî„ÉÉ„ÇØ„ÅÆ„ÉÑ„Ç§„Éº„Éà„ÇíÂèéÈõÜ
   - ÊÑüÊÉÖÂàÜÊûê„ÇíÂÆüË°å
   - ÊôÇÁ≥ªÂàó„ÅßÂèØË¶ñÂåñ</p>
</li>
<li>
<p><strong>Á§æÂÜÖFAQËá™ÂãïÂøúÁ≠î„Ç∑„Çπ„ÉÜ„É†</strong>
   - ÈÅéÂéª„ÅÆÂïè„ÅÑÂêà„Çè„Åõ„Éá„Éº„Çø„ÇíÂ≠¶Áøí
   - QA„Ç∑„Çπ„ÉÜ„É†„ÅßËá™ÂãïÂõûÁ≠î
   - Á≤æÂ∫¶„ÇíÁ∂ôÁ∂öÁöÑ„Å´ÊîπÂñÑ</p>
</li>
<li>
<p><strong>„Éã„É•„Éº„ÇπË¶ÅÁ¥Ñ„Ç¢„Éó„É™</strong>
   - RSS„Éï„Ç£„Éº„Éâ„Åã„Çâ„Éã„É•„Éº„ÇπÂèñÂæó
   - Ëá™ÂãïË¶ÅÁ¥Ñ
   - „Ç´„ÉÜ„Ç¥„É™ÂàÜÈ°û„Å®„É¨„Ç≥„É°„É≥„Éâ</p>
</li>
<li>
<p><strong>Â§öË®ÄË™û„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà</strong>
   - Ê©üÊ¢∞ÁøªË®≥„ÇíÊ¥ªÁî®
   - Ë§áÊï∞Ë®ÄË™û„ÅßÂØæÂøú
   - RAG„ÅßÁü•Ë≠ò„Éô„Éº„ÇπÊ¥ªÁî®</p>
</li>
</ol>
<hr />
<h2>8. Á∑¥ÁøíÂïèÈ°å</h2>
<h3>ÂïèÈ°å1: ÊÉÖÂ†±ÊäΩÂá∫ÔºàÂü∫Á§éÔºâ</h3>
<p>Ê¨°„ÅÆ„Éã„É•„Éº„ÇπË®ò‰∫ã„Åã„Çâ„ÄÅ‰∫∫Âêç„ÄÅÁµÑÁπîÂêç„ÄÅÊó•‰ªò„ÇíÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<pre><code class="language-python">news = &quot;&quot;&quot;
2025Âπ¥11Êúà1Êó•„ÄÅ„Éà„É®„ÇøËá™ÂãïËªä„ÅÆË±äÁî∞Á´†Áî∑Á§æÈï∑„ÅåÊñ∞ÂûãÈõªÊ∞óËá™ÂãïËªä„ÇíÁô∫Ë°®„Åó„Åü„ÄÇ
„Åì„ÅÆÁô∫Ë°®„ÅØÊù±‰∫¨„É¢„Éº„Çø„Éº„Ç∑„Éß„Éº„ÅßË°å„Çè„Çå„ÄÅ200Âêç‰ª•‰∏ä„ÅÆÂ†±ÈÅìÈñ¢‰øÇËÄÖ„ÅåÂèÇÂä†„Åó„Åü„ÄÇ
&quot;&quot;&quot;

# „ÅÇ„Å™„Åü„ÅÆÂÆüË£Ö
</code></pre>
<details>
<summary>Ëß£Á≠î‰æã</summary>


<pre><code class="language-python">import spacy

nlp = spacy.load('ja_ginza')
doc = nlp(news)

entities = {
    'PERSON': [],
    'ORG': [],
    'DATE': []
}

for ent in doc.ents:
    if ent.label_ in entities:
        entities[ent.label_].append(ent.text)

print(&quot;‰∫∫Âêç:&quot;, entities['PERSON'])
print(&quot;ÁµÑÁπîÂêç:&quot;, entities['ORG'])
print(&quot;Êó•‰ªò:&quot;, entities['DATE'])
</code></pre>


</details>

<h3>ÂïèÈ°å2: „ÉÜ„Ç≠„Çπ„ÉàË¶ÅÁ¥ÑÔºà‰∏≠Á¥öÔºâ</h3>
<p>Ê¨°„ÅÆÈï∑Êñá„Çí3Êñá„Å´Ë¶ÅÁ¥Ñ„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºàÊäΩÂá∫ÂûãË¶ÅÁ¥ÑÔºâ„ÄÇ</p>
<pre><code class="language-python">article = &quot;&quot;&quot;
‰∫∫Â∑•Áü•ËÉΩ„ÅÆÁô∫Â±ï„ÅØÁõÆË¶ö„Åæ„Åó„Åè„ÄÅÊßò„ÄÖ„Å™ÂàÜÈáé„ÅßÊ¥ªÁî®„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ
Áâπ„Å´Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„ÅÆÂàÜÈáé„Åß„ÅØ„ÄÅÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÅåÁôªÂ†¥„Åó„ÄÅ
‰∫∫Èñì„Å®ÂêåÁ≠â‰ª•‰∏ä„ÅÆÊÄßËÉΩ„ÇíÁô∫ÊèÆ„Åô„Çã„Çà„ÅÜ„Å´„Å™„Çä„Åæ„Åó„Åü„ÄÇ
Ê©üÊ¢∞ÁøªË®≥„ÅÆÁ≤æÂ∫¶„ÇÇÂ§ß„Åç„ÅèÂêë‰∏ä„Åó„ÄÅÂÆüÁî®„É¨„Éô„É´„Å´ÈÅî„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
„Åæ„Åü„ÄÅ„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„ÇÑ„Éê„Éº„ÉÅ„É£„É´„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„ÇÇÊôÆÂèä„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
‰ªäÂæå„ÇÇAIÊäÄË°ì„ÅÆÈÄ≤Âåñ„ÅØÁ∂ö„Åè„Å®‰∫àÊÉ≥„Åï„Çå„Åæ„Åô„ÄÇ
ÂåªÁôÇ„ÄÅÊïôËÇ≤„ÄÅ„Éì„Ç∏„Éç„Çπ„Å™„Å©„ÄÅ„ÅÇ„Çâ„ÇÜ„ÇãÂàÜÈáé„ÅßAI„ÅåÊ¥ªÁî®„Åï„Çå„Çã„Åß„Åó„Çá„ÅÜ„ÄÇ
&quot;&quot;&quot;

# „ÅÇ„Å™„Åü„ÅÆÂÆüË£Ö
</code></pre>
<details>
<summary>Ëß£Á≠î‰æã</summary>


<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# Êñá„Å´ÂàÜÂâ≤
sentences = [s.strip() for s in article.split('„ÄÇ') if s.strip()]

# TF-IDF„Éô„ÇØ„Éà„É´Âåñ
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(sentences)

# ÂêÑÊñá„ÅÆ„Çπ„Ç≥„Ç¢„ÇíË®àÁÆó
scores = np.array(tfidf_matrix.sum(axis=1)).flatten()

# ‰∏ä‰Ωç3Êñá„ÇíÊäΩÂá∫
top_indices = np.argsort(scores)[::-1][:3]
top_indices = sorted(top_indices)

summary = '„ÄÇ'.join([sentences[i] for i in top_indices]) + '„ÄÇ'
print(&quot;Ë¶ÅÁ¥Ñ:&quot;, summary)
</code></pre>


</details>

<h3>ÂïèÈ°å3: RAG„Ç∑„Çπ„ÉÜ„É†ÔºàÂøúÁî®Ôºâ</h3>
<p>Áü•Ë≠ò„Éô„Éº„Çπ„Çí‰Ωø„Å£„Å¶Ë≥™Âïè„Å´Á≠î„Åà„ÇãRAG„Ç∑„Çπ„ÉÜ„É†„ÇíÂÆüË£Ö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<p><strong>‰ªïÊßò:</strong>
- Áü•Ë≠ò„Éô„Éº„Çπ: ‰ºöÁ§æÊÉÖÂ†±„ÅÆÊñáÊõ∏„É™„Çπ„Éà
- ÂÖ•Âäõ: Ë≥™ÂïèÊñá
- Âá∫Âäõ: Èñ¢ÈÄ£ÊñáÊõ∏ + ÂõûÁ≠î</p>
<details>
<summary>Ëß£Á≠î‰æã</summary>


<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class SimpleRAG:
    def __init__(self, knowledge_base):
        self.kb = knowledge_base
        self.vectorizer = TfidfVectorizer()
        self.doc_vectors = self.vectorizer.fit_transform(knowledge_base)

    def answer(self, question, top_k=2):
        # Ë≥™Âïè„Çí„Éô„ÇØ„Éà„É´Âåñ
        q_vector = self.vectorizer.transform([question])

        # È°û‰ººÂ∫¶Ë®àÁÆó
        similarities = cosine_similarity(q_vector, self.doc_vectors)[0]

        # ‰∏ä‰Ωçk‰ª∂„ÇíÂèñÂæó
        top_indices = np.argsort(similarities)[::-1][:top_k]

        results = [(self.kb[i], similarities[i]) for i in top_indices]
        return results

# Áü•Ë≠ò„Éô„Éº„Çπ
kb = [
    &quot;ÂΩìÁ§æ„ÅØ2000Âπ¥„Å´Ë®≠Á´ã„Åï„Çå„Åæ„Åó„Åü&quot;,
    &quot;‰∏ªÂäõË£ΩÂìÅ„ÅØAI„ÇΩ„Éï„Éà„Ç¶„Çß„Ç¢„Åß„Åô&quot;,
    &quot;ÂæìÊ•≠Âì°Êï∞„ÅØ300Âêç„Åß„Åô&quot;,
    &quot;Êú¨Á§æ„ÅØÊù±‰∫¨„Å´„ÅÇ„Çä„Åæ„Åô&quot;
]

rag = SimpleRAG(kb)
results = rag.answer(&quot;‰ºöÁ§æ„ÅÆË®≠Á´ãÂπ¥„ÅØÔºü&quot;)

for doc, score in results:
    print(f&quot;Èñ¢ÈÄ£ÊñáÊõ∏: {doc} („Çπ„Ç≥„Ç¢: {score:.4f})&quot;)
</code></pre>


</details>

<hr />
<h2>9. ÂèÇËÄÉÊñáÁåÆ„Å®„É™„ÇΩ„Éº„Çπ</h2>
<h3>Êõ∏Á±ç</h3>
<ol>
<li>„ÄåËá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„ÅÆÂü∫Á§é„ÄçÂ••ÊùëÂ≠¶Ôºà„Ç≥„É≠„ÉäÁ§æÔºâ</li>
<li>„Äå„Çº„É≠„Åã„Çâ‰Ωú„ÇãDeep Learning ‚ù∑„ÄçÊñéËó§Â∫∑ÊØÖÔºà„Ç™„É©„Ç§„É™„Éº„Éª„Ç∏„É£„Éë„É≥Ôºâ</li>
<li>„ÄåLarge Language Models„ÄçJurafsky &amp; Martin</li>
</ol>
<h3>„Ç™„É≥„É©„Ç§„É≥„Ç≥„Éº„Çπ</h3>
<ul>
<li><a href="http://web.stanford.edu/class/cs224n/">Stanford CS224N: NLP with Deep Learning</a></li>
<li><a href="https://huggingface.co/course/">Hugging Face Course</a></li>
<li><a href="https://www.fast.ai/">fast.ai NLP Course</a></li>
</ul>
<h3>„É©„Ç§„Éñ„É©„É™„Éª„ÉÑ„Éº„É´</h3>
<ul>
<li><a href="https://huggingface.co/transformers/">Hugging Face Transformers</a> - ÊúÄÊñ∞„ÅÆNLP„É¢„Éá„É´</li>
<li><a href="https://spacy.io/">spaCy</a> - Áî£Ê•≠Áî®NLP</li>
<li><a href="https://www.nltk.org/">NLTK</a> - ÊïôËÇ≤Áî®NLP</li>
<li><a href="https://radimrehurek.com/gensim/">Gensim</a> - „Éà„Éî„ÉÉ„ÇØ„É¢„Éá„É™„É≥„Ç∞</li>
</ul>
<h3>ÊúÄÊñ∞ÂãïÂêë</h3>
<ul>
<li><a href="https://paperswithcode.com/area/natural-language-processing">Papers with Code - NLP</a></li>
<li><a href="https://aclanthology.org/">ACL Anthology</a> - NLPË´ñÊñá„Ç¢„Éº„Ç´„Ç§„Éñ</li>
<li><a href="https://arxiv.org/list/cs.CL/recent">arXiv cs.CL</a> - ÊúÄÊñ∞Ë´ñÊñá</li>
</ul>
<hr />
<h2>10. „Åä„Çè„Çä„Å´</h2>
<p>Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„ÅØ„ÄÅAIÊäÄË°ì„ÅÆ‰∏≠„Åß„ÇÇÁâπ„Å´ÊÄ•ÈÄü„Å´Áô∫Â±ï„Åó„Å¶„ÅÑ„ÇãÂàÜÈáé„Åß„Åô„ÄÇÊú¨„Ç∑„É™„Éº„Ç∫„ÅßÂ≠¶„Çì„Å†Âü∫Á§éÁü•Ë≠ò„ÇíÂúüÂè∞„Å´„ÄÅ„Åú„Å≤ÂÆüÈöõ„ÅÆ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„Å´ÊåëÊà¶„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<h3>„Åï„Çâ„Å™„ÇãÂ≠¶Áøí„ÅÆ„Åü„ÇÅ„Å´</h3>
<ol>
<li><strong>ÂÆü„Éá„Éº„Çø„ÅßÂÆüÈ®ì</strong>: Kaggle„ÅÆ„Ç≥„É≥„Éö„ÉÜ„Ç£„Ç∑„Éß„É≥„Å´ÂèÇÂä†</li>
<li><strong>ÊúÄÊñ∞Ë´ñÊñá„ÇíË™≠„ÇÄ</strong>: arXiv„ÅßÊúÄÊñ∞Á†îÁ©∂„Çí„Éï„Ç©„É≠„Éº</li>
<li><strong>„Ç≥„Éü„É•„Éã„ÉÜ„Ç£ÂèÇÂä†</strong>: GitHub„ÄÅStack Overflow„ÄÅÂãâÂº∑‰ºö</li>
<li><strong>Ëá™ÂàÜ„ÅÆ„Éó„É≠„Ç∏„Çß„ÇØ„Éà</strong>: ËààÂë≥„ÅÆ„ÅÇ„ÇãÂàÜÈáé„ÅßNLP„Ç¢„Éó„É™„ÇíÈñãÁô∫</li>
</ol>
<h3>Á∂ôÁ∂öÁöÑ„Å™Â≠¶Áøí</h3>
<p>NLP„ÅØÈÄ≤Âåñ„ÅåÈÄü„ÅÑÂàÜÈáé„Åß„Åô„ÄÇ‰ª•‰∏ã„ÇíÂÆöÊúüÁöÑ„Å´„ÉÅ„Çß„ÉÉ„ÇØ„Åó„Åæ„Åó„Çá„ÅÜÔºö</p>
<ul>
<li>‚úÖ ‰∏ªË¶Å„Ç´„É≥„Éï„Ç°„É¨„É≥„ÇπÔºàACL, EMNLP, NAACLÔºâ„ÅÆË´ñÊñá</li>
<li>‚úÖ OpenAI, Google AI, Meta AI„ÅÆ„Éñ„É≠„Ç∞</li>
<li>‚úÖ Hugging Face„ÅÆÊúÄÊñ∞„É¢„Éá„É´</li>
<li>‚úÖ NLPÁ≥ªYouTube„ÉÅ„É£„É≥„Éç„É´</li>
</ul>
<hr />
<p><strong>Êú¨„Ç∑„É™„Éº„Ç∫„ÇíÊúÄÂæå„Åæ„Åß„ÅäË™≠„Åø„ÅÑ„Åü„Å†„Åç„ÄÅ„ÅÇ„Çä„Åå„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åó„ÅüÔºÅ</strong></p>
<p><strong>Ââç„Å∏</strong>: <a href="chapter-3.html">‚Üê Chapter 3: ÂÆüË∑µ - „ÉÜ„Ç≠„Çπ„ÉàÂàÜÈ°û</a></p>
<p><strong>ÁõÆÊ¨°„Å∏</strong>: <a href="index.html">‚Üë „Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°</a></p>
<hr />
<h2>„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ„Çí„ÅäÂæÖ„Å°„Åó„Å¶„ÅÑ„Åæ„Åô</h2>
<p>„Åì„ÅÆ„Ç∑„É™„Éº„Ç∫„ÅÆÊîπÂñÑ„Å´„ÅîÂçîÂäõ„Åè„Å†„Åï„ÅÑÔºö</p>
<ul>
<li><strong>Ë™§„Çä„ÇíÁô∫Ë¶ã</strong>: <a href="https://github.com/YusukeHashimotoLab/AI-Knowledge-Notes/issues">GitHub„ÅßIssue„ÇíÈñã„Åè</a></li>
<li><strong>ÊîπÂñÑÊèêÊ°à</strong>: „Çà„ÇäËâØ„ÅÑË™¨ÊòéÊñπÊ≥ï„ÅÆ„Ç¢„Ç§„Éá„Ç¢</li>
<li><strong>Ë≥™Âïè</strong>: ÁêÜËß£„ÅåÈõ£„Åó„Åã„Å£„ÅüÈÉ®ÂàÜ</li>
</ul>
<p>Ë©≥Á¥∞„ÅØ<a href="../../../CONTRIBUTING.md">CONTRIBUTING.md</a>„Çí„ÅîË¶ß„Åè„Å†„Åï„ÅÑ„ÄÇ</p><div class="navigation">
    <a href="chapter-3.html" class="nav-button">‚Üê Á¨¨3Á´†</a>
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>Áõ£‰øÆ</strong>: Dr. Yusuke HashimotoÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-17</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
