<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬4ç« ï¼šå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/nlp-introduction/chapter4-large-language-models.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/nlp-introduction/index.html">Nlp</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 4</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬4ç« ï¼šå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰</h1>
            <p class="subtitle">GPTã€LLaMAã€Prompt Engineering - è¨€èªç†è§£ã®æœ€å‰ç·š</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 35-40åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… GPTãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨è‡ªå·±å›å¸°ç”Ÿæˆã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… LLMsã®å­¦ç¿’æ‰‹æ³•ï¼ˆäº‹å‰å­¦ç¿’ã€Instruction Tuningã€RLHFï¼‰ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Prompt Engineeringã®æŠ€æ³•ã‚’å®Ÿè·µã§ãã‚‹</li>
<li>âœ… ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLMsã®ç‰¹æ€§ã¨é‡å­åŒ–æŠ€è¡“ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… RAGã‚„Function Callingãªã©å®Ÿè·µçš„å¿œç”¨ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… å®Œå…¨ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
</ul>

<hr>

<h2>4.1 GPTãƒ•ã‚¡ãƒŸãƒªãƒ¼</h2>

<h3>GPTã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ¦‚è¦</h3>

<p><strong>GPTï¼ˆGenerative Pre-trained Transformerï¼‰</strong>ã¯ã€Decoder-onlyã®Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¡ç”¨ã—ãŸè‡ªå·±å›å¸°å‹è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</p>

<blockquote>
<p><strong>Decoder-only</strong>: Encoderã¨Decoderã‚’æŒã¤BERTç­‰ã¨ç•°ãªã‚Šã€GPTã¯Decoderã®ã¿ã§æ§‹æˆã•ã‚Œã€æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã«ç‰¹åŒ–ã—ã¦ã„ã¾ã™ã€‚</p>
</blockquote>

<h3>GPTã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç‰¹å¾´</h3>

<table>
<thead>
<tr>
<th>ç‰¹å¾´</th>
<th>èª¬æ˜</th>
<th>åˆ©ç‚¹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Decoder-only</strong></td>
<td>è‡ªå·±æ³¨æ„æ©Ÿæ§‹ã®ã¿ä½¿ç”¨</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ã§æ‹¡å¼µã—ã‚„ã™ã„</td>
</tr>
<tr>
<td><strong>Causal Masking</strong></td>
<td>æœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’éš ã™</td>
<td>è‡ªå·±å›å¸°ç”Ÿæˆã‚’å®Ÿç¾</td>
</tr>
<tr>
<td><strong>Autoregressive</strong></td>
<td>å·¦ã‹ã‚‰å³ã¸é †æ¬¡ç”Ÿæˆ</td>
<td>è‡ªç„¶ãªæ–‡ç”Ÿæˆ</td>
</tr>
<tr>
<td><strong>äº‹å‰å­¦ç¿’</strong></td>
<td>å¤§è¦æ¨¡ãƒ†ã‚­ã‚¹ãƒˆã§å­¦ç¿’</td>
<td>æ±ç”¨çš„ãªè¨€èªç†è§£</td>
</tr>
</tbody>
</table>

<h3>GPTã®é€²åŒ–</h3>

<div class="mermaid">
graph LR
    A[GPT-1<br/>117M params<br/>2018] --> B[GPT-2<br/>1.5B params<br/>2019]
    B --> C[GPT-3<br/>175B params<br/>2020]
    C --> D[GPT-3.5<br/>ChatGPT<br/>2022]
    D --> E[GPT-4<br/>Multimodal<br/>2023]

    style A fill:#e3f2fd
    style B fill:#bbdefb
    style C fill:#90caf9
    style D fill:#64b5f6
    style E fill:#42a5f5
</div>

<h3>GPT-2/GPT-3/GPT-4ã®æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>ãƒ¢ãƒ‡ãƒ«</th>
<th>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°</th>
<th>ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·</th>
<th>ä¸»ãªç‰¹å¾´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>GPT-2</strong></td>
<td>117M - 1.5B</td>
<td>1,024</td>
<td>é«˜å“è³ªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ</td>
</tr>
<tr>
<td><strong>GPT-3</strong></td>
<td>175B</td>
<td>2,048</td>
<td>Few-shotå­¦ç¿’ã€In-context Learning</td>
</tr>
<tr>
<td><strong>GPT-3.5</strong></td>
<td>~175B</td>
<td>4,096</td>
<td>Instruction Tuningã€å¯¾è©±æ€§èƒ½å‘ä¸Š</td>
</tr>
<tr>
<td><strong>GPT-4</strong></td>
<td>éå…¬é–‹</td>
<td>8,192 - 32,768</td>
<td>ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã€é«˜åº¦ãªæ¨è«–</td>
</tr>
</tbody>
</table>

<h3>è‡ªå·±å›å¸°ç”Ÿæˆï¼ˆAutoregressive Generationï¼‰</h3>

<p>GPTã¯ã€å‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã™ã‚‹<strong>è‡ªå·±å›å¸°çš„</strong>ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚</p>

<p>$$
P(x_1, x_2, \ldots, x_n) = \prod_{i=1}^{n} P(x_i | x_1, x_2, \ldots, x_{i-1})
$$</p>

<p>å„ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºç‡ã¯ã€ãã‚Œã¾ã§ã®ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«æ¡ä»¶ä»˜ã‘ã‚‰ã‚Œã¾ã™ã€‚</p>

<h3>å®Ÿä¾‹ï¼šGPT-2ã§ã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ</h3>

<pre><code class="language-python">from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# GPT-2ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
prompt = "Artificial intelligence is"
inputs = tokenizer(prompt, return_tensors="pt")

# ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
generation_config = {
    'max_length': 100,
    'num_return_sequences': 3,
    'temperature': 0.8,
    'top_k': 50,
    'top_p': 0.95,
    'do_sample': True,
    'no_repeat_ngram_size': 2
}

# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
with torch.no_grad():
    outputs = model.generate(
        inputs['input_ids'],
        **generation_config
    )

print("=== GPT-2ã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ ===")
print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: '{prompt}'\n")
for i, output in enumerate(outputs):
    text = tokenizer.decode(output, skip_special_tokens=True)
    print(f"ç”Ÿæˆ {i+1}:")
    print(f"{text}\n")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== GPT-2ã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ ===
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: 'Artificial intelligence is'

ç”Ÿæˆ 1:
Artificial intelligence is becoming more and more important in our daily lives. From smartphones to self-driving cars, AI systems are transforming the way we work and live. The technology has advanced rapidly...

ç”Ÿæˆ 2:
Artificial intelligence is a field of computer science that focuses on creating intelligent machines capable of performing tasks that typically require human intelligence, such as visual perception...

ç”Ÿæˆ 3:
Artificial intelligence is revolutionizing industries across the globe. Companies are investing billions in AI research to develop systems that can learn from data and make decisions autonomously...
</code></pre>

<h3>ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆ¶å¾¡</h3>

<pre><code class="language-python">import matplotlib.pyplot as plt
import numpy as np

# ç•°ãªã‚‹temperatureã§ã®ç”Ÿæˆ
prompt = "The future of AI is"
temperatures = [0.3, 0.7, 1.0, 1.5]

print("=== Temperature ã®å½±éŸ¿ ===\n")

for temp in temperatures:
    inputs = tokenizer(prompt, return_tensors="pt")

    with torch.no_grad():
        outputs = model.generate(
            inputs['input_ids'],
            max_length=50,
            temperature=temp,
            do_sample=True,
            top_k=50
        )

    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"Temperature = {temp}:")
    print(f"{text}\n")

# temperatureã¨ç¢ºç‡åˆ†å¸ƒã®å¯è¦–åŒ–
def softmax_with_temperature(logits, temperature):
    """æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é©ç”¨ã—ãŸsoftmax"""
    return torch.softmax(logits / temperature, dim=-1)

# ã‚µãƒ³ãƒ—ãƒ«logits
logits = torch.tensor([2.0, 1.0, 0.5, 0.2, 0.1])
temps = [0.5, 1.0, 2.0]

plt.figure(figsize=(12, 4))
for i, temp in enumerate(temps):
    probs = softmax_with_temperature(logits, temp)

    plt.subplot(1, 3, i+1)
    plt.bar(range(len(probs)), probs.numpy())
    plt.title(f'Temperature = {temp}')
    plt.xlabel('Token ID')
    plt.ylabel('ç¢ºç‡')
    plt.ylim(0, 1)
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nğŸ“Š ä½ã„temperature â†’ ç¢ºå®šçš„ï¼ˆé«˜ç¢ºç‡ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠï¼‰")
print("ğŸ“Š é«˜ã„temperature â†’ å¤šæ§˜æ€§ï¼ˆä½ç¢ºç‡ãƒˆãƒ¼ã‚¯ãƒ³ã‚‚é¸æŠï¼‰")
</code></pre>

<h3>Beam Searchã¨Sampling</h3>

<pre><code class="language-python">from transformers import GenerationConfig

prompt = "Machine learning can be used for"
inputs = tokenizer(prompt, return_tensors="pt")

print("=== ç”Ÿæˆæˆ¦ç•¥ã®æ¯”è¼ƒ ===\n")

# 1. Greedy Decodingï¼ˆè²ªæ¬²æ³•ï¼‰
print("1. Greedy Decoding:")
with torch.no_grad():
    outputs = model.generate(
        inputs['input_ids'],
        max_length=50,
        do_sample=False
    )
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
print()

# 2. Beam Search
print("2. Beam Search (num_beams=5):")
with torch.no_grad():
    outputs = model.generate(
        inputs['input_ids'],
        max_length=50,
        num_beams=5,
        do_sample=False
    )
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
print()

# 3. Top-k Sampling
print("3. Top-k Sampling (k=50):")
with torch.no_grad():
    outputs = model.generate(
        inputs['input_ids'],
        max_length=50,
        do_sample=True,
        top_k=50
    )
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
print()

# 4. Top-p (Nucleus) Sampling
print("4. Top-p Sampling (p=0.95):")
with torch.no_grad():
    outputs = model.generate(
        inputs['input_ids'],
        max_length=50,
        do_sample=True,
        top_p=0.95,
        top_k=0
    )
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
</code></pre>

<blockquote>
<p><strong>é‡è¦</strong>: Greedy/Beam Searchã¯æ±ºå®šè«–çš„ã§ä¸€è²«æ€§ãŒã‚ã‚Šã€Samplingã¯å¤šæ§˜æ€§ãŒã‚ã‚Šã¾ã™ãŒã€å†ç¾æ€§ãŒä½ããªã‚Šã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>4.2 LLMsã®å­¦ç¿’æ‰‹æ³•</h2>

<h3>LLMå­¦ç¿’ã®å…¨ä½“åƒ</h3>

<div class="mermaid">
graph TD
    A[å¤§è¦æ¨¡ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ¼ãƒ‘ã‚¹] --> B[äº‹å‰å­¦ç¿’<br/>Pre-training]
    B --> C[ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«<br/>Base LLM]
    C --> D[Instruction Tuning<br/>æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°]
    D --> E[æŒ‡ç¤ºã«å¾“ã†ãƒ¢ãƒ‡ãƒ«<br/>Instruction-following LLM]
    E --> F[RLHF<br/>äººé–“ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’]
    F --> G[ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆæ¸ˆã¿ãƒ¢ãƒ‡ãƒ«<br/>Aligned LLM]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#e3f2fd
    style D fill:#f3e5f5
    style E fill:#e8f5e9
    style F fill:#fce4ec
    style G fill:#c8e6c9
</div>

<h3>1. äº‹å‰å­¦ç¿’ï¼ˆPre-trainingï¼‰</h3>

<p><strong>äº‹å‰å­¦ç¿’</strong>ã¯ã€å¤§è¦æ¨¡ãªãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ¼ãƒ‘ã‚¹ã§æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã‚’å­¦ç¿’ã—ã¾ã™ã€‚</p>

<table>
<thead>
<tr>
<th>è¦ç´ </th>
<th>å†…å®¹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ‡ãƒ¼ã‚¿</strong></td>
<td>Webã€æ›¸ç±ã€è«–æ–‡ãªã©æ•°åƒå„„ã€œæ•°å…†ãƒˆãƒ¼ã‚¯ãƒ³</td>
</tr>
<tr>
<td><strong>ç›®çš„é–¢æ•°</strong></td>
<td>æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ï¼ˆLanguage Modelingï¼‰</td>
</tr>
<tr>
<td><strong>æœ€é©åŒ–</strong></td>
<td>AdamWã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°</td>
</tr>
<tr>
<td><strong>è¨ˆç®—è³‡æº</strong></td>
<td>æ•°åƒã€œæ•°ä¸‡GPUã€æ•°é€±é–“ã€œæ•°ãƒ¶æœˆ</td>
</tr>
</tbody>
</table>

<p><strong>æå¤±é–¢æ•°</strong>ï¼š</p>

<p>$$
\mathcal{L}_{\text{LM}} = -\sum_{i=1}^{n} \log P(x_i | x_{<i}; \theta)
$$</p>

<h3>2. Instruction Tuningï¼ˆæŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰</h3>

<p><strong>Instruction Tuning</strong>ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤ºã«å¾“ã†ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚</p>

<h4>Instruction Tuningã®ãƒ‡ãƒ¼ã‚¿å½¢å¼</h4>

<pre><code class="language-python">instruction_data = [
    {
        "instruction": "ä»¥ä¸‹ã®æ–‡ç« ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚",
        "input": "äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒäººé–“ã®ã‚ˆã†ã«è€ƒãˆã€å­¦ç¿’ã—ã€å•é¡Œã‚’è§£æ±ºã™ã‚‹æŠ€è¡“ã§ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã€æ·±å±¤å­¦ç¿’ã€è‡ªç„¶è¨€èªå‡¦ç†ãªã©ã®åˆ†é‡ã‚’å«ã¿ã¾ã™ã€‚",
        "output": "AIã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«äººé–“ã®ã‚ˆã†ãªçŸ¥èƒ½ã‚’æŒãŸã›ã‚‹æŠ€è¡“ã§ã€æ©Ÿæ¢°å­¦ç¿’ã‚„è‡ªç„¶è¨€èªå‡¦ç†ãªã©ã‚’å«ã¿ã¾ã™ã€‚"
    },
    {
        "instruction": "æ¬¡ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚",
        "input": "åœ°çƒã‹ã‚‰æœˆã¾ã§ã®è·é›¢ã¯ã©ã®ãã‚‰ã„ã§ã™ã‹ï¼Ÿ",
        "output": "åœ°çƒã‹ã‚‰æœˆã¾ã§ã®å¹³å‡è·é›¢ã¯ç´„384,400ã‚­ãƒ­ãƒ¡ãƒ¼ãƒˆãƒ«ï¼ˆ238,855ãƒã‚¤ãƒ«ï¼‰ã§ã™ã€‚"
    },
    {
        "instruction": "ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã®ãƒã‚°ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚",
        "input": "def add(a, b):\n    return a - b",
        "output": "def add(a, b):\n    return a + b"
    }
]

# ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®å¯è¦–åŒ–
import pandas as pd

df = pd.DataFrame(instruction_data)
print("=== Instruction Tuning ãƒ‡ãƒ¼ã‚¿å½¢å¼ ===")
print(df.to_string(index=False))
print(f"\nãƒ‡ãƒ¼ã‚¿æ•°: {len(instruction_data)}ä»¶")
</code></pre>

<h4>Instruction Tuningã®å®Ÿè£…ä¾‹ï¼ˆæ¦‚å¿µï¼‰</h4>

<pre><code class="language-python">from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from datasets import Dataset

# ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
def format_instruction(example):
    """æŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå½¢å¼ã«å¤‰æ›"""
    prompt = f"### Instruction:\n{example['instruction']}\n\n"
    if example['input']:
        prompt += f"### Input:\n{example['input']}\n\n"
    prompt += f"### Response:\n{example['output']}"
    return {"text": prompt}

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
dataset = Dataset.from_list(instruction_data)
dataset = dataset.map(format_instruction)

print("=== ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹ ===")
print(dataset[0]['text'])
print("\n" + "="*50 + "\n")

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™ï¼ˆå®Ÿéš›ã«ã¯å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
model_name = "gpt2"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºé–¢æ•°
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=512
    )

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
tokenized_dataset = dataset.map(tokenize_function, batched=True)

# è¨“ç·´è¨­å®šï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
training_args = TrainingArguments(
    output_dir="./instruction-tuned-model",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    learning_rate=2e-5,
    logging_steps=10,
    save_steps=100,
    evaluation_strategy="no"
)

print("=== Instruction Tuning è¨­å®š ===")
print(f"ã‚¨ãƒãƒƒã‚¯æ•°: {training_args.num_train_epochs}")
print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_args.per_device_train_batch_size}")
print(f"å­¦ç¿’ç‡: {training_args.learning_rate}")
print("\nâœ“ Instruction Tuningã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã¯æŒ‡ç¤ºã«å¾“ã†ã‚ˆã†ã«ãªã‚Šã¾ã™")
</code></pre>

<h3>3. RLHFï¼ˆReinforcement Learning from Human Feedbackï¼‰</h3>

<p><strong>RLHF</strong>ã¯ã€äººé–“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’äººé–“ã®ä¾¡å€¤è¦³ã«åˆã‚ã›ã‚‹æŠ€è¡“ã§ã™ã€‚</p>

<h4>RLHFã®3ã‚¹ãƒ†ãƒƒãƒ—</h4>

<div class="mermaid">
graph LR
    A[Step 1:<br/>æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°<br/>SFT] --> B[Step 2:<br/>å ±é…¬ãƒ¢ãƒ‡ãƒ«è¨“ç·´<br/>Reward Model]
    B --> C[Step 3:<br/>PPOå¼·åŒ–å­¦ç¿’<br/>Policy Optimization]

    style A fill:#e3f2fd
    style B fill:#f3e5f5
    style C fill:#e8f5e9
</div>

<h4>Step 1: Supervised Fine-Tuning (SFT)</h4>

<p>é«˜å“è³ªãªäººé–“ä½œæˆã®å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚</p>

<h4>Step 2: Reward Model Training</h4>

<p>äººé–“ã®è©•ä¾¡ï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰ã‹ã‚‰å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¾ã™ã€‚</p>

<pre><code class="language-python"># å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ï¼ˆæ¦‚å¿µï¼‰
reward_data = [
    {
        "prompt": "äººå·¥çŸ¥èƒ½ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "response_1": "äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒäººé–“ã®ã‚ˆã†ã«è€ƒãˆã‚‹æŠ€è¡“ã§ã™ã€‚",
        "response_2": "ã‚ã‹ã‚Šã¾ã›ã‚“ã€‚",
        "preference": 1  # response_1ã®æ–¹ãŒè‰¯ã„
    },
    {
        "prompt": "Pythonã§ãƒªã‚¹ãƒˆã‚’é€†é †ã«ã™ã‚‹æ–¹æ³•ã¯ï¼Ÿ",
        "response_1": "list.reverse()ãƒ¡ã‚½ãƒƒãƒ‰ã¾ãŸã¯list[::-1]ã‚¹ãƒ©ã‚¤ã‚·ãƒ³ã‚°ã‚’ä½¿ã„ã¾ã™ã€‚",
        "response_2": "ã§ãã¾ã›ã‚“ã€‚",
        "preference": 1
    }
]

import pandas as pd
df_reward = pd.DataFrame(reward_data)
print("=== å ±é…¬ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ‡ãƒ¼ã‚¿ ===")
print(df_reward.to_string(index=False))
print("\nâœ“ äººé–“ã®é¸å¥½ã‹ã‚‰å ±é…¬é–¢æ•°ã‚’å­¦ç¿’")
</code></pre>

<h4>Step 3: PPO (Proximal Policy Optimization)</h4>

<p>å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã€å¼·åŒ–å­¦ç¿’ã§ãƒ¢ãƒ‡ãƒ«ã‚’æœ€é©åŒ–ã—ã¾ã™ã€‚</p>

<p><strong>ç›®çš„é–¢æ•°</strong>ï¼š</p>

<p>$$
\mathcal{L}^{\text{PPO}} = \mathbb{E}_{x,y \sim \pi_\theta} \left[ r(x, y) - \beta \cdot \text{KL}(\pi_\theta || \pi_{\text{ref}}) \right]
$$</p>

<ul>
<li>$r(x, y)$: å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚³ã‚¢</li>
<li>$\beta$: KLæ­£å‰‡åŒ–ä¿‚æ•°</li>
<li>$\pi_{\text{ref}}$: å…ƒã®ãƒ¢ãƒ‡ãƒ«ï¼ˆå¤§ããé€¸è„±ã—ãªã„ãŸã‚ï¼‰</li>
</ul>

<blockquote>
<p><strong>RLHF ã®åŠ¹æœ</strong>: ChatGPTã®äººé–“ã‚‰ã—ã„å¯¾è©±èƒ½åŠ›ã¯ã€RLHFã«ã‚ˆã£ã¦å®Ÿç¾ã•ã‚Œã¦ã„ã¾ã™ã€‚</p>
</blockquote>

<h3>4. Parameter-Efficient Fine-Tuningï¼ˆPEFTï¼‰</h3>

<p>å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã‚’å¾®èª¿æ•´ã™ã‚‹ã®ã¯è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ãŸã‚ã€<strong>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡çš„ãªå¾®èª¿æ•´</strong>ãŒé‡è¦ã§ã™ã€‚</p>

<h4>LoRAï¼ˆLow-Rank Adaptationï¼‰</h4>

<p>LoRAã¯ã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿è¡Œåˆ—ã«ä½ãƒ©ãƒ³ã‚¯åˆ†è§£ã‚’é©ç”¨ã—ã¾ã™ã€‚</p>

<p>$$
W' = W + \Delta W = W + BA
$$</p>

<ul>
<li>$W$: å…ƒã®é‡ã¿è¡Œåˆ—ï¼ˆå›ºå®šï¼‰</li>
<li>$B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$: å­¦ç¿’å¯èƒ½ãªä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—</li>
<li>$r \ll \min(d, k)$: ãƒ©ãƒ³ã‚¯ï¼ˆä¾‹: 8, 16ï¼‰</li>
</ul>

<pre><code class="language-python">from peft import LoraConfig, get_peft_model, TaskType
from transformers import AutoModelForCausalLM

# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
model_name = "gpt2"
base_model = AutoModelForCausalLM.from_pretrained(model_name)

# LoRAè¨­å®š
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,  # ãƒ©ãƒ³ã‚¯
    lora_alpha=32,  # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°
    lora_dropout=0.1,
    target_modules=["c_attn"]  # é©ç”¨ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
)

# LoRAãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
lora_model = get_peft_model(base_model, lora_config)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®æ¯”è¼ƒ
total_params = sum(p.numel() for p in lora_model.parameters())
trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)

print("=== LoRA ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡ ===")
print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
print(f"è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}")
print(f"è¨“ç·´å¯èƒ½å‰²åˆ: {100 * trainable_params / total_params:.2f}%")
print(f"\nâœ“ ã‚ãšã‹{100 * trainable_params / total_params:.2f}%ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¾®èª¿æ•´å¯èƒ½")
print(lora_model.print_trainable_parameters())
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== LoRA ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡ ===
ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 124,439,808
è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 294,912
è¨“ç·´å¯èƒ½å‰²åˆ: 0.24%

âœ“ ã‚ãšã‹0.24%ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¾®èª¿æ•´å¯èƒ½
trainable params: 294,912 || all params: 124,439,808 || trainable%: 0.24
</code></pre>

<h4>Adapter Layers</h4>

<p>Transformerã®å„å±¤ã«å°ã•ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’è¿½åŠ ã—ã¾ã™ã€‚</p>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</th>
<th>ãƒ¡ãƒ¢ãƒª</th>
<th>é€Ÿåº¦</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Full Fine-tuning</strong></td>
<td>100%</td>
<td>é«˜</td>
<td>é…ã„</td>
</tr>
<tr>
<td><strong>LoRA</strong></td>
<td>0.1% - 1%</td>
<td>ä½</td>
<td>é€Ÿã„</td>
</tr>
<tr>
<td><strong>Adapter</strong></td>
<td>2% - 5%</td>
<td>ä¸­</td>
<td>ä¸­</td>
</tr>
<tr>
<td><strong>Prompt Tuning</strong></td>
<td>< 0.1%</td>
<td>éå¸¸ã«ä½</td>
<td>éå¸¸ã«é€Ÿã„</td>
</tr>
</tbody>
</table>

<hr>

<h2>4.3 Prompt Engineering</h2>

<h3>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã¨ã¯</h3>

<p><strong>Prompt Engineering</strong>ã¯ã€LLMã‹ã‚‰æœ›ã¾ã—ã„å‡ºåŠ›ã‚’å¼•ãå‡ºã™ãŸã‚ã®å…¥åŠ›ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰è¨­è¨ˆæŠ€è¡“ã§ã™ã€‚</p>

<blockquote>
<p>ã€Œé©åˆ‡ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’10å€ã«ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€ - OpenAI</p>
</blockquote>

<h3>1. Zero-shot Learning</h3>

<p>äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«ã€ã‚¿ã‚¹ã‚¯ã®ä¾‹ã‚’ä¸€åˆ‡ä¸ãˆãšã«ç›´æ¥è³ªå•ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">from transformers import pipeline

# GPT-2ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
generator = pipeline('text-generation', model='gpt2')

# Zero-shot ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
zero_shot_prompt = """
Question: What is the capital of France?
Answer:
"""

result = generator(zero_shot_prompt, max_length=50, num_return_sequences=1)
print("=== Zero-shot Learning ===")
print(result[0]['generated_text'])
</code></pre>

<h3>2. Few-shot Learning</h3>

<p>ã‚¿ã‚¹ã‚¯ã®ä¾‹ã‚’æ•°å€‹æç¤ºã—ã¦ã‹ã‚‰ã€æ–°ã—ã„å…¥åŠ›ã‚’ä¸ãˆã¾ã™ã€‚</p>

<pre><code class="language-python"># Few-shot ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆIn-Context Learningï¼‰
few_shot_prompt = """
Translate English to French:

English: Hello
French: Bonjour

English: Thank you
French: Merci

English: Good morning
French: Bon matin

English: How are you?
French:
"""

result = generator(few_shot_prompt, max_length=100, num_return_sequences=1)
print("\n=== Few-shot Learning ===")
print(result[0]['generated_text'])
print("\nâœ“ ä¾‹ã‚’ç¤ºã™ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ç¿»è¨³ã‚’å®Ÿè¡Œ")
</code></pre>

<h3>3. Chain-of-Thought (CoT) Prompting</h3>

<p><strong>Chain-of-Thought</strong>ã¯ã€ãƒ¢ãƒ‡ãƒ«ã«ä¸­é–“çš„ãªæ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç”Ÿæˆã•ã›ã‚‹æŠ€æ³•ã§ã™ã€‚</p>

<pre><code class="language-python"># é€šå¸¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆç›´æ¥å›ç­”ï¼‰
standard_prompt = """
Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.
Each can has 3 tennis balls. How many tennis balls does he have now?
Answer:
"""

# Chain-of-Thought ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
cot_prompt = """
Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.
Each can has 3 tennis balls. How many tennis balls does he have now?

Let's think step by step:
1. Roger starts with 5 tennis balls.
2. He buys 2 cans, each containing 3 tennis balls.
3. So he gets 2 Ã— 3 = 6 new tennis balls.
4. Total tennis balls = 5 + 6 = 11.

Answer: 11 tennis balls.

Question: A restaurant had 23 customers. Then 11 more customers arrived.
Each customer ordered 2 drinks. How many drinks were ordered in total?

Let's think step by step:
"""

result = generator(cot_prompt, max_length=200, num_return_sequences=1)
print("\n=== Chain-of-Thought Prompting ===")
print(result[0]['generated_text'])
print("\nâœ“ ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã®æ¨è«–ã«ã‚ˆã‚Šã€è¤‡é›‘ãªå•é¡Œã‚’è§£æ±º")
</code></pre>

<h4>CoTã®åŠ¹æœï¼ˆç ”ç©¶çµæœï¼‰</h4>

<table>
<thead>
<tr>
<th>ã‚¿ã‚¹ã‚¯</th>
<th>æ¨™æº–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ</th>
<th>CoTãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ</th>
<th>æ”¹å–„ç‡</th>
</tr>
</thead>
<tbody>
<tr>
<td>ç®—æ•°å•é¡Œ</td>
<td>34%</td>
<td>78%</td>
<td>+44%</td>
</tr>
<tr>
<td>å¸¸è­˜æ¨è«–</td>
<td>61%</td>
<td>89%</td>
<td>+28%</td>
</tr>
<tr>
<td>è«–ç†ãƒ‘ã‚ºãƒ«</td>
<td>42%</td>
<td>81%</td>
<td>+39%</td>
</tr>
</tbody>
</table>

<h3>4. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>

<pre><code class="language-python"># âŒ æ‚ªã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
bad_prompt = "Summarize this."

# âœ… è‰¯ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
good_prompt = """
Task: Summarize the following article in 3 bullet points.
Focus on key findings and implications.

Article: [é•·ã„è¨˜äº‹ãƒ†ã‚­ã‚¹ãƒˆ...]

Summary:
-
"""

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã®åŸå‰‡
prompt_principles = {
    "æ˜ç¢ºãªæŒ‡ç¤º": "ã‚¿ã‚¹ã‚¯ã‚’å…·ä½“çš„ã«è¨˜è¿°ã™ã‚‹",
    "ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæŒ‡å®š": "æœ›ã¾ã—ã„å‡ºåŠ›å½¢å¼ã‚’ç¤ºã™",
    "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæä¾›": "å¿…è¦ãªèƒŒæ™¯æƒ…å ±ã‚’å«ã‚ã‚‹",
    "åˆ¶ç´„ã®æ˜ç¤º": "æ–‡å­—æ•°åˆ¶é™ã€ã‚¹ã‚¿ã‚¤ãƒ«ãªã©ã‚’æŒ‡å®š",
    "ä¾‹ã®æç¤º": "Few-shotã§æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ã‚’ç¤ºã™",
    "ã‚¹ãƒ†ãƒƒãƒ—åˆ†è§£": "è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã¯æ®µéšçš„ã«"
}

print("=== ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã®åŸå‰‡ ===")
for principle, description in prompt_principles.items():
    print(f"âœ“ {principle}: {description}")

# å®Ÿè·µä¾‹ï¼šæ§‹é€ åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
structured_prompt = """
Role: You are an expert Python programmer.

Task: Review the following code and provide feedback.

Code:
```python
def calculate_average(numbers):
    return sum(numbers) / len(numbers)
```

Output Format:
1. Code Quality (1-10):
2. Issues Found:
3. Suggestions:
4. Improved Code:

Analysis:
"""

print("\n=== æ§‹é€ åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹ ===")
print(structured_prompt)
</code></pre>

<h3>5. LangChainã«ã‚ˆã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†</h3>

<pre><code class="language-python">from langchain import PromptTemplate
from langchain.chains import LLMChain
from langchain.llms import HuggingFacePipeline

# HuggingFace ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’LangChainã§ãƒ©ãƒƒãƒ—
llm = HuggingFacePipeline(pipeline=generator)

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ä½œæˆ
template = """
Question: {question}

Context: {context}

Please provide a detailed answer based on the context above.

Answer:
"""

prompt = PromptTemplate(
    input_variables=["question", "context"],
    template=template
)

# ãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ
chain = LLMChain(llm=llm, prompt=prompt)

# å®Ÿè¡Œ
question = "What is machine learning?"
context = "Machine learning is a subset of artificial intelligence that enables systems to learn from data."

result = chain.run(question=question, context=context)

print("=== LangChain ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ ===")
print(f"è³ªå•: {question}")
print(f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {context}")
print(f"\nç”Ÿæˆã•ã‚ŒãŸå›ç­”:")
print(result)

print("\nâœ“ LangChainã§å†åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç®¡ç†")
</code></pre>

<hr>

<h2>4.4 ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLMs</h2>

<h3>ä¸»è¦ãªã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLM</h3>

<table>
<thead>
<tr>
<th>ãƒ¢ãƒ‡ãƒ«</th>
<th>é–‹ç™ºå…ƒ</th>
<th>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</th>
<th>ç‰¹å¾´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>LLaMA</strong></td>
<td>Meta AI</td>
<td>7B - 65B</td>
<td>é«˜æ€§èƒ½ã€ç ”ç©¶ç”¨</td>
</tr>
<tr>
<td><strong>LLaMA-2</strong></td>
<td>Meta AI</td>
<td>7B - 70B</td>
<td>å•†ç”¨åˆ©ç”¨å¯ã€Chatç‰ˆã‚ã‚Š</td>
</tr>
<tr>
<td><strong>Falcon</strong></td>
<td>TII</td>
<td>7B - 180B</td>
<td>é«˜å“è³ªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</td>
</tr>
<tr>
<td><strong>ELYZA</strong></td>
<td>ELYZA Inc.</td>
<td>7B - 13B</td>
<td>æ—¥æœ¬èªç‰¹åŒ–</td>
</tr>
<tr>
<td><strong>rinna</strong></td>
<td>rinna</td>
<td>3.6B - 36B</td>
<td>æ—¥æœ¬èªã€å•†ç”¨åˆ©ç”¨å¯</td>
</tr>
</tbody>
</table>

<h3>1. LLaMA / LLaMA-2</h3>

<p><strong>LLaMAï¼ˆLarge Language Model Meta AIï¼‰</strong>ã¯ã€Meta AIãŒé–‹ç™ºã—ãŸã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLMã§ã™ã€‚</p>

<h4>LLaMAã®ç‰¹å¾´</h4>

<ul>
<li><strong>åŠ¹ç‡çš„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</strong>: GPT-3ã‚ˆã‚Šå°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§åŒç­‰æ€§èƒ½</li>
<li><strong>Pre-Normalization</strong>: RMSNormä½¿ç”¨</li>
<li><strong>SwiGLUæ´»æ€§åŒ–é–¢æ•°</strong>: æ€§èƒ½å‘ä¸Š</li>
<li><strong>Rotary Positional Embeddings (RoPE)</strong>: ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°</li>
</ul>

<pre><code class="language-python">from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# LLaMA-2ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆ7Bãƒ¢ãƒ‡ãƒ«ï¼‰
model_name = "meta-llama/Llama-2-7b-hf"  # Hugging Face Hubã‹ã‚‰

# æ³¨æ„: LLaMA-2ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯Hugging Faceã§ã‚¢ã‚¯ã‚»ã‚¹ç”³è«‹ãŒå¿…è¦
# ã“ã“ã§ã¯ãƒ‡ãƒ¢ç”¨ã«GPT-2ã‚’ä½¿ç”¨

model_name = "gpt2"  # å®Ÿéš›ã®ç’°å¢ƒã§ã¯LLaMA-2ã«ç½®ãæ›ãˆ
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

print("=== LLaMAãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®æƒ…å ± ===")
print(f"ãƒ¢ãƒ‡ãƒ«: {model_name}")
print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}")
print(f"ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: Decoder-only Transformer")
print(f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: 2048 (LLaMA) / 4096 (LLaMA-2)")

# LLaMA-2ã®æ€§èƒ½ï¼ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœï¼‰
benchmarks = {
    "MMLU": {"LLaMA-7B": 35.1, "LLaMA-2-7B": 45.3, "GPT-3.5": 70.0},
    "HellaSwag": {"LLaMA-7B": 76.1, "LLaMA-2-7B": 77.2, "GPT-3.5": 85.5},
    "HumanEval": {"LLaMA-7B": 10.5, "LLaMA-2-7B": 12.8, "GPT-3.5": 48.1}
}

import pandas as pd
df_bench = pd.DataFrame(benchmarks)
print("\n=== ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ€§èƒ½æ¯”è¼ƒ ===")
print(df_bench.to_string())
</code></pre>

<h3>2. æ—¥æœ¬èªLLMsï¼ˆELYZAã€rinnaï¼‰</h3>

<h4>ELYZA-japanese-Llama-2</h4>

<p>LLaMA-2ã‚’æ—¥æœ¬èªã§è¿½åŠ äº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</p>

<pre><code class="language-python"># ELYZA-japanese-Llama-2ã®ä½¿ç”¨ä¾‹ï¼ˆæ¦‚å¿µï¼‰
# model_name = "elyza/ELYZA-japanese-Llama-2-7b"

japanese_prompt = """
ä»¥ä¸‹ã®è³ªå•ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚

è³ªå•: æ©Ÿæ¢°å­¦ç¿’ã¨æ·±å±¤å­¦ç¿’ã®é•ã„ã¯ä½•ã§ã™ã‹ï¼Ÿ

å›ç­”:
"""

print("=== æ—¥æœ¬èªLLMï¼ˆELYZAï¼‰===")
print("âœ“ LLaMA-2ãƒ™ãƒ¼ã‚¹ + æ—¥æœ¬èªè¿½åŠ å­¦ç¿’")
print("âœ“ æ—¥æœ¬èªã§ã®è‡ªç„¶ãªå¯¾è©±ãŒå¯èƒ½")
print("âœ“ å•†ç”¨åˆ©ç”¨å¯èƒ½ï¼ˆLLaMA-2ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ï¼‰")
print(f"\nãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹:\n{japanese_prompt}")

# rinna GPT-NeoX
print("\n=== æ—¥æœ¬èªLLMï¼ˆrinnaï¼‰===")
print("âœ“ GPT-NeoXã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£")
print("âœ“ æ—¥æœ¬èªWikipediaãªã©ã§å­¦ç¿’")
print("âœ“ 3.6Bã€36Bãƒ¢ãƒ‡ãƒ«ã‚ã‚Š")
print("âœ“ å•†ç”¨åˆ©ç”¨å¯èƒ½ï¼ˆMIT Licenseï¼‰")
</code></pre>

<h3>3. ãƒ¢ãƒ‡ãƒ«é‡å­åŒ–ï¼ˆQuantizationï¼‰</h3>

<p><strong>é‡å­åŒ–</strong>ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’ä¸‹ã’ã¦ãƒ¡ãƒ¢ãƒªã¨è¨ˆç®—é‡ã‚’å‰Šæ¸›ã™ã‚‹æŠ€è¡“ã§ã™ã€‚</p>

<h4>é‡å­åŒ–ã®ç¨®é¡</h4>

<table>
<thead>
<tr>
<th>ç²¾åº¦</th>
<th>ãƒ¡ãƒ¢ãƒªå‰Šæ¸›</th>
<th>æ€§èƒ½ä½ä¸‹</th>
<th>ç”¨é€”</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>FP32ï¼ˆå…ƒï¼‰</strong></td>
<td>-</td>
<td>-</td>
<td>å­¦ç¿’</td>
</tr>
<tr>
<td><strong>FP16</strong></td>
<td>50%</td>
<td>ã»ã¼ãªã—</td>
<td>æ¨è«–</td>
</tr>
<tr>
<td><strong>8-bit</strong></td>
<td>75%</td>
<td>å°</td>
<td>æ¨è«–ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</td>
</tr>
<tr>
<td><strong>4-bit</strong></td>
<td>87.5%</td>
<td>ä¸­</td>
<td>ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ç’°å¢ƒ</td>
</tr>
</tbody>
</table>

<h4>8-bité‡å­åŒ–ã®å®Ÿè£…</h4>

<pre><code class="language-python">from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# 8-bité‡å­åŒ–è¨­å®š
quantization_config_8bit = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0
)

# 4-bité‡å­åŒ–è¨­å®šï¼ˆQLoRAï¼‰
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",  # NormalFloat4
    bnb_4bit_use_double_quant=True
)

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆ8-bitï¼‰
# model_8bit = AutoModelForCausalLM.from_pretrained(
#     "meta-llama/Llama-2-7b-hf",
#     quantization_config=quantization_config_8bit,
#     device_map="auto"
# )

print("=== ãƒ¢ãƒ‡ãƒ«é‡å­åŒ– ===")
print("\n8-bité‡å­åŒ–:")
print("  âœ“ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ç´„7GBï¼ˆ7Bãƒ¢ãƒ‡ãƒ«ã€FP32æ¯”75%å‰Šæ¸›ï¼‰")
print("  âœ“ æ€§èƒ½ä½ä¸‹: æœ€å°é™ï¼ˆ1-2%ç¨‹åº¦ï¼‰")
print("  âœ“ æ¨è«–é€Ÿåº¦: FP32ã¨ã»ã¼åŒç­‰")

print("\n4-bité‡å­åŒ– (QLoRA):")
print("  âœ“ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ç´„3.5GBï¼ˆ7Bãƒ¢ãƒ‡ãƒ«ã€FP32æ¯”87.5%å‰Šæ¸›ï¼‰")
print("  âœ“ æ€§èƒ½ä½ä¸‹: å°ï¼ˆ5-10%ç¨‹åº¦ï¼‰")
print("  âœ“ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½")

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è¨ˆç®—
def calculate_model_memory(num_params, precision):
    """ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¨ˆç®—"""
    bytes_per_param = {
        'fp32': 4,
        'fp16': 2,
        '8bit': 1,
        '4bit': 0.5
    }
    memory_gb = num_params * bytes_per_param[precision] / (1024**3)
    return memory_gb

params_7b = 7_000_000_000

print("\n=== 7Bãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ ===")
for precision in ['fp32', 'fp16', '8bit', '4bit']:
    memory = calculate_model_memory(params_7b, precision)
    print(f"{precision.upper():6s}: {memory:.2f} GB")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== 7Bãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ ===
FP32  : 26.08 GB
FP16  : 13.04 GB
8BIT  : 6.52 GB
4BIT  : 3.26 GB
</code></pre>

<blockquote>
<p><strong>QLoRAï¼ˆQuantized LoRAï¼‰</strong>: 4-bité‡å­åŒ–ã¨LoRAã‚’çµ„ã¿åˆã‚ã›ã€æ¶ˆè²»è€…å‘ã‘GPUã§å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ã«ã—ã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>4.5 LLMsã®å®Ÿè·µçš„å¿œç”¨</h2>

<h3>1. Retrieval-Augmented Generation (RAG)</h3>

<p><strong>RAG</strong>ã¯ã€å¤–éƒ¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æ¤œç´¢ã—ã€ãã‚Œã‚’åŸºã«å›ç­”ã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã§ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•] --> B[æ¤œç´¢<br/>Retriever]
    B --> C[çŸ¥è­˜ãƒ™ãƒ¼ã‚¹<br/>Vector DB]
    C --> D[é–¢é€£æ–‡æ›¸]
    D --> E[LLM<br/>Generator]
    A --> E
    E --> F[å›ç­”ç”Ÿæˆ]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
    style F fill:#c8e6c9
</div>

<h4>RAGã®å®Ÿè£…ä¾‹</h4>

<pre><code class="language-python">from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline

# çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆ
documents = [
    "äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒäººé–“ã®ã‚ˆã†ã«è€ƒãˆã€å­¦ç¿’ã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
    "æ©Ÿæ¢°å­¦ç¿’ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹AIã®ä¸€åˆ†é‡ã§ã™ã€‚",
    "æ·±å±¤å­¦ç¿’ã¯ã€å¤šå±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã†æ©Ÿæ¢°å­¦ç¿’ã®æ‰‹æ³•ã§ã™ã€‚",
    "è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNLPï¼‰ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒäººé–“ã®è¨€èªã‚’ç†è§£ã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
    "Transformerã¯ã€2017å¹´ã«ç™»å ´ã—ãŸé©æ–°çš„ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚"
]

# ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,
    chunk_overlap=20
)
texts = text_splitter.create_documents(documents)

# åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆ
vectorstore = FAISS.from_documents(texts, embeddings)

print("=== RAGã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰ ===")
print(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(documents)}")
print(f"ãƒãƒ£ãƒ³ã‚¯æ•°: {len(texts)}")
print(f"åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«: sentence-transformers/all-MiniLM-L6-v2")

# æ¤œç´¢ãƒ†ã‚¹ãƒˆ
query = "Transformerã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ"
relevant_docs = vectorstore.similarity_search(query, k=2)

print(f"\nè³ªå•: {query}")
print("\né–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:")
for i, doc in enumerate(relevant_docs, 1):
    print(f"{i}. {doc.page_content}")

# LLMã¨çµ±åˆï¼ˆæ¦‚å¿µï¼‰
# llm = HuggingFacePipeline(...)
# qa_chain = RetrievalQA.from_chain_type(
#     llm=llm,
#     retriever=vectorstore.as_retriever(),
#     return_source_documents=True
# )
# result = qa_chain({"query": query})

print("\nâœ“ RAGã«ã‚ˆã‚Šã€LLMã¯æœ€æ–°ãƒ»ç‰¹å®šåˆ†é‡ã®çŸ¥è­˜ã§å›ç­”å¯èƒ½")
</code></pre>

<h3>2. Function Calling</h3>

<p><strong>Function Calling</strong>ã¯ã€LLMãŒå¤–éƒ¨ãƒ„ãƒ¼ãƒ«ã‚„APIã‚’å‘¼ã³å‡ºã›ã‚‹ã‚ˆã†ã«ã™ã‚‹æŠ€è¡“ã§ã™ã€‚</p>

<pre><code class="language-python">import json

# åˆ©ç”¨å¯èƒ½ãªé–¢æ•°ã®å®šç¾©
available_functions = {
    "get_weather": {
        "description": "æŒ‡å®šã•ã‚ŒãŸéƒ½å¸‚ã®ç¾åœ¨ã®å¤©æ°—ã‚’å–å¾—",
        "parameters": {
            "city": {"type": "string", "description": "éƒ½å¸‚å"}
        }
    },
    "calculate": {
        "description": "æ•°å­¦çš„è¨ˆç®—ã‚’å®Ÿè¡Œ",
        "parameters": {
            "expression": {"type": "string", "description": "è¨ˆç®—å¼"}
        }
    },
    "search_web": {
        "description": "ã‚¦ã‚§ãƒ–ã‚’æ¤œç´¢",
        "parameters": {
            "query": {"type": "string", "description": "æ¤œç´¢ã‚¯ã‚¨ãƒª"}
        }
    }
}

# é–¢æ•°å®Ÿè£…ï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
def get_weather(city):
    """å¤©æ°—æƒ…å ±ã‚’å–å¾—ï¼ˆãƒ€ãƒŸãƒ¼ï¼‰"""
    return f"{city}ã®å¤©æ°—ã¯æ™´ã‚Œã€æ°—æ¸©ã¯25åº¦ã§ã™ã€‚"

def calculate(expression):
    """è¨ˆç®—ã‚’å®Ÿè¡Œ"""
    try:
        result = eval(expression)
        return f"{expression} = {result}"
    except:
        return "è¨ˆç®—ã‚¨ãƒ©ãƒ¼"

def search_web(query):
    """ã‚¦ã‚§ãƒ–æ¤œç´¢ï¼ˆãƒ€ãƒŸãƒ¼ï¼‰"""
    return f"'{query}'ã®æ¤œç´¢çµæœ: [é–¢é€£æƒ…å ±...]"

# Function Calling ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
def create_function_calling_prompt(user_query, functions):
    """Function Callingç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
    functions_desc = json.dumps(functions, ensure_ascii=False, indent=2)

    prompt = f"""
You are a helpful assistant with access to the following functions:

{functions_desc}

User query: {user_query}

Based on the query, determine which function to call and with what parameters.
Respond in JSON format:
{{
    "function": "function_name",
    "parameters": {{...}}
}}

Response:
"""
    return prompt

# ãƒ†ã‚¹ãƒˆ
user_query = "æ±äº¬ã®å¤©æ°—ã¯ã©ã†ã§ã™ã‹ï¼Ÿ"
prompt = create_function_calling_prompt(user_query, available_functions)

print("=== Function Calling ===")
print(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª: {user_query}")
print(f"\nãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\n{prompt}")

# æƒ³å®šã•ã‚Œã‚‹å¿œç­”ï¼ˆå®Ÿéš›ã«ã¯LLMãŒç”Ÿæˆï¼‰
function_call = {
    "function": "get_weather",
    "parameters": {"city": "æ±äº¬"}
}

print(f"\nLLMã®é–¢æ•°é¸æŠ:")
print(json.dumps(function_call, ensure_ascii=False, indent=2))

# é–¢æ•°å®Ÿè¡Œ
if function_call["function"] == "get_weather":
    result = get_weather(**function_call["parameters"])
    print(f"\nå®Ÿè¡Œçµæœ: {result}")

print("\nâœ“ LLMãŒãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦æƒ…å ±ã‚’å–å¾—ãƒ»å‡¦ç†å¯èƒ½")
</code></pre>

<h3>3. Multi-turn Conversationï¼ˆãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³å¯¾è©±ï¼‰</h3>

<p>ä¼šè©±å±¥æ­´ã‚’ç¶­æŒã—ã¦ã€æ–‡è„ˆã‚’ç†è§£ã—ãŸå¯¾è©±ã‚’å®Ÿç¾ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">from collections import deque

class ConversationManager:
    """ä¼šè©±å±¥æ­´ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, max_history=10):
        self.history = deque(maxlen=max_history)

    def add_message(self, role, content):
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ """
        self.history.append({"role": role, "content": content})

    def get_prompt(self, system_message=""):
        """ä¼šè©±å±¥æ­´ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
        prompt_parts = []

        if system_message:
            prompt_parts.append(f"System: {system_message}\n")

        for msg in self.history:
            prompt_parts.append(f"{msg['role'].capitalize()}: {msg['content']}")

        prompt_parts.append("Assistant:")
        return "\n".join(prompt_parts)

    def clear(self):
        """å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"""
        self.history.clear()

# ä½¿ç”¨ä¾‹
conv_manager = ConversationManager(max_history=6)

system_msg = "ã‚ãªãŸã¯è¦ªåˆ‡ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"

# ä¼šè©±ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
conversation = [
    ("User", "ã“ã‚“ã«ã¡ã¯ï¼"),
    ("Assistant", "ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
    ("User", "Pythonã®å­¦ç¿’æ–¹æ³•ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"),
    ("Assistant", "Pythonå­¦ç¿’ã«ã¯ã€ã¾ãšåŸºç¤æ–‡æ³•ã‚’å­¦ã³ã€ãã®å¾Œå®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«å–ã‚Šçµ„ã‚€ã®ãŒãŠã™ã™ã‚ã§ã™ã€‚"),
    ("User", "åˆå¿ƒè€…å‘ã‘ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ä½•ãŒã„ã„ã§ã™ã‹ï¼Ÿ"),
]

print("=== Multi-turn Conversation ===\n")

for i, (role, content) in enumerate(conversation):
    conv_manager.add_message(role, content)

    if role == "User":
        print(f"{role}: {content}")
        prompt = conv_manager.get_prompt(system_msg)
        print(f"\n[ç”Ÿæˆã•ã‚Œã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ]")
        print(prompt)
        print("\n" + "="*50 + "\n")

print("âœ“ ä¼šè©±å±¥æ­´ã«ã‚ˆã‚Šã€æ–‡è„ˆã‚’ä¿æŒã—ãŸå¯¾è©±ãŒå¯èƒ½")
print(f"âœ“ å±¥æ­´ã®é•·ã•: {len(conv_manager.history)}ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
</code></pre>

<h3>4. å®Œå…¨ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆå®Ÿè£…</h3>

<pre><code class="language-python">from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

class SimpleChatbot:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ"""

    def __init__(self, model_name="gpt2", max_history=5):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.conversation = ConversationManager(max_history=max_history)
        self.system_message = "You are a helpful AI assistant."

    def generate_response(self, user_input, max_length=100):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«å¯¾ã™ã‚‹å¿œç­”ã‚’ç”Ÿæˆ"""
        # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
        self.conversation.add_message("User", user_input)

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        prompt = self.conversation.get_prompt(self.system_message)

        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        inputs = self.tokenizer(prompt, return_tensors="pt")

        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                inputs['input_ids'],
                max_length=len(inputs['input_ids'][0]) + max_length,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
        response = response[len(prompt):].strip()

        # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
        self.conversation.add_message("Assistant", response)

        return response

    def chat(self):
        """å¯¾è©±ãƒ«ãƒ¼ãƒ—"""
        print("=== ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆèµ·å‹• ===")
        print("çµ‚äº†ã™ã‚‹ã«ã¯ 'quit' ã¨å…¥åŠ›ã—ã¦ãã ã•ã„\n")

        while True:
            user_input = input("You: ")

            if user_input.lower() in ['quit', 'exit', 'bye']:
                print("Assistant: ã•ã‚ˆã†ãªã‚‰ï¼")
                break

            response = self.generate_response(user_input)
            print(f"Assistant: {response}\n")

# ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
chatbot = SimpleChatbot(model_name="gpt2", max_history=5)

# ãƒ‡ãƒ¢ç”¨ã®å¯¾è©±ï¼ˆå®Ÿéš›ã¯chatbot.chat()ã§å¯¾è©±ãƒ«ãƒ¼ãƒ—ï¼‰
demo_inputs = [
    "Hello!",
    "What is AI?",
    "Can you explain more?"
]

print("=== ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ ãƒ‡ãƒ¢ ===\n")
for user_input in demo_inputs:
    print(f"You: {user_input}")
    response = chatbot.generate_response(user_input, max_length=50)
    print(f"Assistant: {response}\n")

print("âœ“ ä¼šè©±å±¥æ­´ã‚’ä¿æŒã—ãŸå®Œå…¨ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
print("âœ“ æ–‡è„ˆã‚’ç†è§£ã—ãŸå¿œç­”ç”Ÿæˆ")
print("âœ“ æ‹¡å¼µå¯èƒ½ãªè¨­è¨ˆï¼ˆRAGã€Function Callingç­‰ã‚’è¿½åŠ å¯èƒ½ï¼‰")
</code></pre>

<hr>

<h2>4.6 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>GPTãƒ•ã‚¡ãƒŸãƒªãƒ¼</strong></p>
<ul>
<li>Decoder-onlyã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</li>
<li>è‡ªå·±å›å¸°çš„ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ</li>
<li>GPT-2ã‹ã‚‰GPT-4ã¸ã®é€²åŒ–</li>
<li>ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆ¶å¾¡ï¼ˆtemperatureã€top-kã€top-pï¼‰</li>
</ul></li>

<li><p><strong>LLMsã®å­¦ç¿’æ‰‹æ³•</strong></p>
<ul>
<li>äº‹å‰å­¦ç¿’: å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹ã§æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬</li>
<li>Instruction Tuning: æŒ‡ç¤ºã«å¾“ã†ãƒ¢ãƒ‡ãƒ«ã¸</li>
<li>RLHF: äººé–“ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«ã‚ˆã‚‹ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ</li>
<li>PEFT: LoRAã€Adapterã«ã‚ˆã‚‹åŠ¹ç‡çš„å¾®èª¿æ•´</li>
</ul></li>

<li><p><strong>Prompt Engineering</strong></p>
<ul>
<li>Zero-shot / Few-shot Learning</li>
<li>Chain-of-Thought Prompting</li>
<li>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</li>
<li>LangChainã«ã‚ˆã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†</li>
</ul></li>

<li><p><strong>ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLMs</strong></p>
<ul>
<li>LLaMAã€Falconã€æ—¥æœ¬èªLLMs</li>
<li>ãƒ¢ãƒ‡ãƒ«é‡å­åŒ–ï¼ˆ8-bitã€4-bitï¼‰</li>
<li>QLoRAã«ã‚ˆã‚‹åŠ¹ç‡çš„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</li>
</ul></li>

<li><p><strong>å®Ÿè·µçš„å¿œç”¨</strong></p>
<ul>
<li>RAG: å¤–éƒ¨çŸ¥è­˜ã‚’æ´»ç”¨ã—ãŸç”Ÿæˆ</li>
<li>Function Calling: ãƒ„ãƒ¼ãƒ«ãƒ»APIçµ±åˆ</li>
<li>Multi-turn Conversation: æ–‡è„ˆä¿æŒå¯¾è©±</li>
<li>å®Œå…¨ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆå®Ÿè£…</li>
</ul></li>
</ol>

<h3>LLMsæ´»ç”¨ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>

<table>
<thead>
<tr>
<th>è¦³ç‚¹</th>
<th>æ¨å¥¨äº‹é …</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ¢ãƒ‡ãƒ«é¸æŠ</strong></td>
<td>ã‚¿ã‚¹ã‚¯ã«å¿œã˜ãŸã‚µã‚¤ã‚ºãƒ»æ€§èƒ½ã®ãƒãƒ©ãƒ³ã‚¹</td>
</tr>
<tr>
<td><strong>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆ</strong></td>
<td>æ˜ç¢ºãªæŒ‡ç¤ºã€ä¾‹ã®æç¤ºã€æ§‹é€ åŒ–</td>
</tr>
<tr>
<td><strong>ãƒ¡ãƒ¢ãƒªåŠ¹ç‡</strong></td>
<td>é‡å­åŒ–ã€PEFTæ´»ç”¨</td>
</tr>
<tr>
<td><strong>çŸ¥è­˜æ›´æ–°</strong></td>
<td>RAGã§æœ€æ–°æƒ…å ±ã‚’çµ±åˆ</td>
</tr>
<tr>
<td><strong>å®‰å…¨æ€§</strong></td>
<td>å‡ºåŠ›æ¤œè¨¼ã€æœ‰å®³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>

<p>å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç†è§£ã‚’æ·±ã‚ã‚‹ãŸã‚ã«ï¼š</p>
<ul>
<li>ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ï¼ˆLLaMA-2 13B/70Bï¼‰ã‚’è©¦ã™</li>
<li>ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆLoRAï¼‰</li>
<li>RAGã‚·ã‚¹ãƒ†ãƒ ã‚’æœ¬ç•ªç’°å¢ƒã«å°å…¥</li>
<li>ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMï¼ˆGPT-4Vç­‰ï¼‰ã‚’æ¢æ±‚</li>
<li>LLMOpsï¼ˆé‹ç”¨ãƒ»ç›£è¦–ï¼‰ã‚’å­¦ã¶</li>
</ul>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>
<p>GPTã®Decoder-onlyã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã€BERTã®Encoder-onlyã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é•ã„ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ãã‚Œãã‚Œã©ã®ã‚ˆã†ãªã‚¿ã‚¹ã‚¯ã«é©ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>GPTï¼ˆDecoder-onlyï¼‰</strong>ï¼š</p>
<ul>
<li>æ§‹é€ : è‡ªå·±æ³¨æ„æ©Ÿæ§‹ + Causal Maskingï¼ˆæœªæ¥ã‚’è¦‹ãªã„ï¼‰</li>
<li>è¨“ç·´: æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ï¼ˆè‡ªå·±å›å¸°ï¼‰</li>
<li>å¼·ã¿: ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã€å¯¾è©±ã€å‰µä½œ</li>
<li>ç”¨é€”: ChatGPTã€ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã€æ–‡ç« ä½œæˆ</li>
</ul>

<p><strong>BERTï¼ˆEncoder-onlyï¼‰</strong>ï¼š</p>
<ul>
<li>æ§‹é€ : åŒæ–¹å‘è‡ªå·±æ³¨æ„æ©Ÿæ§‹ï¼ˆå…¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‚ç…§ï¼‰</li>
<li>è¨“ç·´: Masked Language Modelingï¼ˆç©´åŸ‹ã‚ï¼‰</li>
<li>å¼·ã¿: ãƒ†ã‚­ã‚¹ãƒˆç†è§£ã€åˆ†é¡ã€æŠ½å‡º</li>
<li>ç”¨é€”: æ„Ÿæƒ…åˆ†æã€å›ºæœ‰è¡¨ç¾èªè­˜ã€è³ªå•å¿œç­”</li>
</ul>

<p><strong>ä½¿ã„åˆ†ã‘</strong>ï¼š</p>

<table>
<thead>
<tr>
<th>ã‚¿ã‚¹ã‚¯</th>
<th>æ¨å¥¨ãƒ¢ãƒ‡ãƒ«</th>
</tr>
</thead>
<tbody>
<tr>
<td>ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ</td>
<td>GPT</td>
</tr>
<tr>
<td>æ–‡æ›¸åˆ†é¡</td>
<td>BERT</td>
</tr>
<tr>
<td>è³ªå•å¿œç­”ï¼ˆæŠ½å‡ºå‹ï¼‰</td>
<td>BERT</td>
</tr>
<tr>
<td>è³ªå•å¿œç­”ï¼ˆç”Ÿæˆå‹ï¼‰</td>
<td>GPT</td>
</tr>
<tr>
<td>è¦ç´„</td>
<td>GPTï¼ˆã¾ãŸã¯Encoder-Decoderï¼‰</td>
</tr>
</tbody>
</table>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>Chain-of-Thought (CoT) PromptingãŒã€ãªãœè¤‡é›‘ãªæ¨è«–ã‚¿ã‚¹ã‚¯ã§åŠ¹æœçš„ãªã®ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚å…·ä½“ä¾‹ã‚’æŒ™ã’ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>CoTãŒåŠ¹æœçš„ãªç†ç”±</strong>ï¼š</p>

<ol>
<li><strong>ä¸­é–“æ¨è«–ã®æ˜ç¤ºåŒ–</strong>: ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¨€èªåŒ–ã™ã‚‹ã“ã¨ã§ã€LLMãŒè«–ç†çš„æ€è€ƒã‚’æ•´ç†</li>
<li><strong>ã‚¨ãƒ©ãƒ¼ã®æ—©æœŸæ¤œå‡º</strong>: å„ã‚¹ãƒ†ãƒƒãƒ—ã§èª¤ã‚Šã«æ°—ã¥ãã‚„ã™ã„</li>
<li><strong>è¤‡é›‘æ€§ã®åˆ†è§£</strong>: é›£ã—ã„å•é¡Œã‚’å°ã•ãªéƒ¨åˆ†å•é¡Œã«åˆ†å‰²</li>
<li><strong>In-context Learningå¼·åŒ–</strong>: æ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’</li>
</ol>

<p><strong>å…·ä½“ä¾‹ï¼šç®—æ•°å•é¡Œ</strong></p>

<pre><code class="language-python"># âŒ æ¨™æº–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
prompt_standard = """
Question: A store had 25 apples. They sold 8 apples in the morning
and 12 apples in the afternoon. How many apples are left?
Answer:
"""

# âœ… CoTãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
prompt_cot = """
Question: A store had 25 apples. They sold 8 apples in the morning
and 12 apples in the afternoon. How many apples are left?

Let's solve this step by step:
1. The store started with 25 apples.
2. They sold 8 apples in the morning: 25 - 8 = 17 apples remaining.
3. They sold 12 apples in the afternoon: 17 - 12 = 5 apples remaining.

Answer: 5 apples are left.
"""
</code></pre>

<p><strong>åŠ¹æœã®å®Ÿè¨¼</strong>ï¼ˆç ”ç©¶çµæœã‚ˆã‚Šï¼‰ï¼š</p>

<table>
<thead>
<tr>
<th>ã‚¿ã‚¹ã‚¯</th>
<th>æ¨™æº–</th>
<th>CoT</th>
<th>æ”¹å–„</th>
</tr>
</thead>
<tbody>
<tr>
<td>GSM8Kï¼ˆç®—æ•°ï¼‰</td>
<td>17.9%</td>
<td>58.1%</td>
<td>+40.2%</td>
</tr>
<tr>
<td>SVAMPï¼ˆç®—æ•°ï¼‰</td>
<td>69.4%</td>
<td>78.7%</td>
<td>+9.3%</td>
</tr>
</tbody>
</table>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>LoRAï¼ˆLow-Rank Adaptationï¼‰ãŒãªãœãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡çš„ãªã®ã‹ã€æ•°å¼ã‚’ç”¨ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>LoRAã®åŸç†</strong>ï¼š</p>

<p>å…ƒã®é‡ã¿è¡Œåˆ— $W \in \mathbb{R}^{d \times k}$ ã‚’æ›´æ–°ã™ã‚‹ä»£ã‚ã‚Šã«ã€ä½ãƒ©ãƒ³ã‚¯åˆ†è§£ã‚’ä½¿ç”¨ï¼š</p>

<p>$$
W' = W + \Delta W = W + BA
$$</p>

<ul>
<li>$W$: å…ƒã®é‡ã¿ï¼ˆå›ºå®šã€å­¦ç¿’ã—ãªã„ï¼‰</li>
<li>$B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$: å­¦ç¿’å¯èƒ½ãªè¡Œåˆ—</li>
<li>$r \ll \min(d, k)$: ãƒ©ãƒ³ã‚¯ï¼ˆä¾‹: $r=8$ï¼‰</li>
</ul>

<p><strong>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›ã®è¨ˆç®—</strong>ï¼š</p>

<p>ä¾‹: $d = 4096$, $k = 4096$, $r = 8$ ã®å ´åˆï¼š</p>

<ul>
<li>å…ƒã®é‡ã¿: $4096 \times 4096 = 16,777,216$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
<li>LoRA: $4096 \times 8 + 8 \times 4096 = 65,536$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
<li>å‰Šæ¸›ç‡: $\frac{65,536}{16,777,216} \approx 0.39\%$ï¼ˆ99.6%å‰Šæ¸›ï¼‰</li>
</ul>

<p><strong>å®Ÿè£…ä¾‹</strong>ï¼š</p>

<pre><code class="language-python">import torch
import torch.nn as nn

class LoRALayer(nn.Module):
    def __init__(self, in_features, out_features, rank=8):
        super().__init__()
        # å…ƒã®é‡ã¿ï¼ˆå›ºå®šï¼‰
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.weight.requires_grad = False

        # LoRAè¡Œåˆ—ï¼ˆå­¦ç¿’å¯èƒ½ï¼‰
        self.lora_A = nn.Parameter(torch.randn(rank, in_features))
        self.lora_B = nn.Parameter(torch.randn(out_features, rank))

        self.rank = rank

    def forward(self, x):
        # W*x + B*A*x
        return x @ self.weight.T + x @ self.lora_A.T @ self.lora_B.T

# ä¾‹
layer = LoRALayer(4096, 4096, rank=8)
total = sum(p.numel() for p in layer.parameters())
trainable = sum(p.numel() for p in layer.parameters() if p.requires_grad)

print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {total:,}")
print(f"è¨“ç·´å¯èƒ½: {trainable:,} ({100*trainable/total:.2f}%)")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: 16,842,752
è¨“ç·´å¯èƒ½: 65,536 (0.39%)
</code></pre>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>RAGï¼ˆRetrieval-Augmented Generationï¼‰ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®è¦ä»¶ã‚’æº€ãŸã™ã“ã¨ï¼š</p>
<ul>
<li>ã‚«ã‚¹ã‚¿ãƒ çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®æ¤œç´¢</li>
<li>é–¢é€£æ–‡æ›¸ã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°</li>
<li>æ¤œç´¢çµæœã‚’ç”¨ã„ãŸå›ç­”ç”Ÿæˆ</li>
</ul>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class SimpleRAG:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªRAGã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, knowledge_base, embedding_model="sentence-transformers/all-MiniLM-L6-v2"):
        self.knowledge_base = knowledge_base
        self.tokenizer = AutoTokenizer.from_pretrained(embedding_model)
        self.model = AutoModel.from_pretrained(embedding_model)

        # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®åŸ‹ã‚è¾¼ã¿ã‚’äº‹å‰è¨ˆç®—
        self.kb_embeddings = self._embed_documents(knowledge_base)

    def _mean_pooling(self, model_output, attention_mask):
        """å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°"""
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    def _embed_documents(self, documents):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›"""
        encoded = self.tokenizer(documents, padding=True, truncation=True, return_tensors='pt')

        with torch.no_grad():
            model_output = self.model(**encoded)

        embeddings = self._mean_pooling(model_output, encoded['attention_mask'])
        return embeddings.numpy()

    def retrieve(self, query, top_k=3):
        """ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹æ–‡æ›¸ã‚’æ¤œç´¢"""
        # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿
        query_embedding = self._embed_documents([query])

        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        similarities = cosine_similarity(query_embedding, self.kb_embeddings)[0]

        # Top-kæ–‡æ›¸ã‚’å–å¾—
        top_indices = np.argsort(similarities)[::-1][:top_k]

        results = []
        for idx in top_indices:
            results.append({
                'document': self.knowledge_base[idx],
                'score': similarities[idx]
            })

        return results

    def generate_answer(self, query, retrieved_docs):
        """æ¤œç´¢çµæœã‚’ç”¨ã„ã¦å›ç­”ã‚’ç”Ÿæˆï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆï¼‰"""
        context = "\n".join([f"- {doc['document']}" for doc in retrieved_docs])

        prompt = f"""
Based on the following context, answer the question.

Context:
{context}

Question: {query}

Answer:
"""
        return prompt

# çŸ¥è­˜ãƒ™ãƒ¼ã‚¹
knowledge_base = [
    "æ©Ÿæ¢°å­¦ç¿’ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã™ã‚‹AIã®åˆ†é‡ã§ã™ã€‚",
    "æ·±å±¤å­¦ç¿’ã¯ã€å¤šå±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’ã®æ‰‹æ³•ã§ã™ã€‚",
    "Transformerã¯ã€2017å¹´ã«ç™»å ´ã—ãŸé©æ–°çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚",
    "BERTã¯ã€åŒæ–¹å‘Transformerã‚’ä½¿ã£ãŸãƒ†ã‚­ã‚¹ãƒˆç†è§£ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚",
    "GPTã¯ã€Decoder-onlyã®Transformerã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«å„ªã‚Œã¦ã„ã¾ã™ã€‚",
    "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®šã‚¿ã‚¹ã‚¯ã«é©å¿œã•ã›ã‚‹æ‰‹æ³•ã§ã™ã€‚",
    "LoRAã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã§ã™ã€‚",
    "RAGã¯ã€æ¤œç´¢ã¨ç”Ÿæˆã‚’çµ„ã¿åˆã‚ã›ãŸæŠ€è¡“ã§ã™ã€‚"
]

# RAGã‚·ã‚¹ãƒ†ãƒ ã®ä½œæˆ
rag = SimpleRAG(knowledge_base)

# ãƒ†ã‚¹ãƒˆ
query = "Transformerã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„"

print("=== RAGã‚·ã‚¹ãƒ†ãƒ  ===")
print(f"çŸ¥è­˜ãƒ™ãƒ¼ã‚¹: {len(knowledge_base)}ä»¶\n")
print(f"ã‚¯ã‚¨ãƒª: {query}\n")

# æ¤œç´¢
retrieved = rag.retrieve(query, top_k=3)

print("æ¤œç´¢çµæœ:")
for i, doc in enumerate(retrieved, 1):
    print(f"{i}. [ã‚¹ã‚³ã‚¢: {doc['score']:.3f}] {doc['document']}")

# å›ç­”ç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
prompt = rag.generate_answer(query, retrieved)
print(f"\nç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\n{prompt}")

print("\nâœ“ RAGã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚Šã€é–¢é€£çŸ¥è­˜ã‚’æ¤œç´¢ã—å›ç­”ç”Ÿæˆã«æ´»ç”¨")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== RAGã‚·ã‚¹ãƒ†ãƒ  ===
çŸ¥è­˜ãƒ™ãƒ¼ã‚¹: 8ä»¶

ã‚¯ã‚¨ãƒª: Transformerã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„

æ¤œç´¢çµæœ:
1. [ã‚¹ã‚³ã‚¢: 0.712] Transformerã¯ã€2017å¹´ã«ç™»å ´ã—ãŸé©æ–°çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚
2. [ã‚¹ã‚³ã‚¢: 0.623] BERTã¯ã€åŒæ–¹å‘Transformerã‚’ä½¿ã£ãŸãƒ†ã‚­ã‚¹ãƒˆç†è§£ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
3. [ã‚¹ã‚³ã‚¢: 0.589] GPTã¯ã€Decoder-onlyã®Transformerã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«å„ªã‚Œã¦ã„ã¾ã™ã€‚

ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:

Based on the following context, answer the question.

Context:
- Transformerã¯ã€2017å¹´ã«ç™»å ´ã—ãŸé©æ–°çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚
- BERTã¯ã€åŒæ–¹å‘Transformerã‚’ä½¿ã£ãŸãƒ†ã‚­ã‚¹ãƒˆç†è§£ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- GPTã¯ã€Decoder-onlyã®Transformerã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«å„ªã‚Œã¦ã„ã¾ã™ã€‚

Question: Transformerã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„

Answer:
</code></pre>

</details>

<h3>å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>RLHFã®3ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆSFTã€Reward Model Trainingã€PPOï¼‰ã«ã¤ã„ã¦ã€ãã‚Œãã‚Œã®å½¹å‰²ã¨ã€ãªãœã“ã®é †åºã§è¡Œã†å¿…è¦ãŒã‚ã‚‹ã®ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>Step 1: Supervised Fine-Tuning (SFT)</strong></p>

<ul>
<li><strong>å½¹å‰²</strong>: äººé–“ãŒä½œæˆã—ãŸé«˜å“è³ªãªå¯¾è©±ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</li>
<li><strong>ç›®çš„</strong>: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’å¯¾è©±ã‚¿ã‚¹ã‚¯ã«é©å¿œã•ã›ã‚‹</li>
<li><strong>ãƒ‡ãƒ¼ã‚¿</strong>: (ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ, ç†æƒ³çš„ãªå¿œç­”) ã®ãƒšã‚¢</li>
<li><strong>å¿…è¦æ€§</strong>: äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¯å¯¾è©±ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ãªã„ãŸã‚</li>
</ul>

<p><strong>Step 2: Reward Model Training</strong></p>

<ul>
<li><strong>å½¹å‰²</strong>: äººé–“ã®é¸å¥½ã‚’å­¦ç¿’ã™ã‚‹å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´</li>
<li><strong>ç›®çš„</strong>: ã€Œè‰¯ã„å¿œç­”ã€vsã€Œæ‚ªã„å¿œç­”ã€ã‚’è‡ªå‹•è©•ä¾¡ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹</li>
<li><strong>ãƒ‡ãƒ¼ã‚¿</strong>: åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹è¤‡æ•°å¿œç­”ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°</li>
<li><strong>å¿…è¦æ€§</strong>: å¼·åŒ–å­¦ç¿’ã«å¿…è¦ãªå ±é…¬ä¿¡å·ã‚’æä¾›</li>
</ul>

<p><strong>æå¤±é–¢æ•°</strong>ï¼š</p>

<p>$$
\mathcal{L}_{\text{reward}} = -\mathbb{E}_{(x, y_w, y_l)} \left[ \log \sigma(r(x, y_w) - r(x, y_l)) \right]
$$</p>

<ul>
<li>$y_w$: å¥½ã¾ã‚Œã‚‹å¿œç­”</li>
<li>$y_l$: å¥½ã¾ã‚Œãªã„å¿œç­”</li>
<li>$r(x, y)$: å ±é…¬ã‚¹ã‚³ã‚¢</li>
</ul>

<p><strong>Step 3: PPO (Proximal Policy Optimization)</strong></p>

<ul>
<li><strong>å½¹å‰²</strong>: å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’æœ€é©åŒ–</li>
<li><strong>ç›®çš„</strong>: äººé–“ã®é¸å¥½ã«æ²¿ã£ãŸå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ã‚ˆã†å­¦ç¿’</li>
<li><strong>æ‰‹æ³•</strong>: å¼·åŒ–å­¦ç¿’ï¼ˆPPOã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰</li>
<li><strong>åˆ¶ç´„</strong>: å…ƒã®ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å¤§ããé€¸è„±ã—ãªã„ã‚ˆã†KLæ­£å‰‡åŒ–</li>
</ul>

<p><strong>ç›®çš„é–¢æ•°</strong>ï¼š</p>

<p>$$
\mathcal{L}^{\text{PPO}} = \mathbb{E}_{x,y \sim \pi_\theta} \left[ r(x, y) - \beta \cdot \text{KL}(\pi_\theta || \pi_{\text{SFT}}) \right]
$$</p>

<p><strong>ãªãœã“ã®é †åºãŒå¿…è¦ã‹</strong>ï¼š</p>

<ol>
<li><strong>SFTãŒæœ€åˆ</strong>:
<ul>
<li>ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¯å¯¾è©±ã«ä¸æ…£ã‚Œ</li>
<li>å¼·åŒ–å­¦ç¿’ã®åˆæœŸæ–¹ç­–ã¨ã—ã¦æ©Ÿèƒ½</li>
<li>å ±é…¬ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã«ã‚‚ä½¿ç”¨</li>
</ul></li>

<li><strong>Reward ModelãŒ2ç•ªç›®</strong>:
<ul>
<li>PPOã«å¿…è¦ãªå ±é…¬ä¿¡å·ã‚’æä¾›</li>
<li>äººé–“ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’</li>
<li>SFTãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆã—ãŸå¿œç­”ã‚’è©•ä¾¡</li>
</ul></li>

<li><strong>PPOãŒæœ€å¾Œ</strong>:
<ul>
<li>å ±é…¬ãƒ¢ãƒ‡ãƒ«ãŒãªã„ã¨æœ€é©åŒ–ã§ããªã„</li>
<li>SFTãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸæ–¹ç­–ã¨ã—ã¦ä½¿ç”¨</li>
<li>KLæ­£å‰‡åŒ–ã§SFTã‹ã‚‰å¤§ããé€¸è„±ã—ãªã„</li>
</ul></li>
</ol>

<p><strong>å…¨ä½“ã®ãƒ•ãƒ­ãƒ¼</strong>ï¼š</p>

<div class="mermaid">
graph TD
    A[äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«] --> B[SFT]
    B --> C[SFTãƒ¢ãƒ‡ãƒ«]
    C --> D[å¿œç­”ç”Ÿæˆ]
    D --> E[äººé–“ã«ã‚ˆã‚‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°]
    E --> F[å ±é…¬ãƒ¢ãƒ‡ãƒ«è¨“ç·´]
    F --> G[å ±é…¬ãƒ¢ãƒ‡ãƒ«]
    C --> H[PPO]
    G --> H
    H --> I[ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆæ¸ˆã¿ãƒ¢ãƒ‡ãƒ«<br/>ChatGPT]
</div>

<p><strong>åŠ¹æœ</strong>ï¼š</p>

<table>
<thead>
<tr>
<th>æ®µéš</th>
<th>æ€§èƒ½æŒ‡æ¨™</th>
</tr>
</thead>
<tbody>
<tr>
<td>ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«</td>
<td>å¯¾è©±å“è³ª: ä½</td>
</tr>
<tr>
<td>SFT</td>
<td>å¯¾è©±å“è³ª: ä¸­ï¼ˆæŒ‡ç¤ºã«å¾“ã†ï¼‰</td>
</tr>
<tr>
<td>RLHF (PPO)</td>
<td>å¯¾è©±å“è³ª: é«˜ï¼ˆäººé–“ã®é¸å¥½ã«æ²¿ã†ï¼‰</td>
</tr>
</tbody>
</table>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Vaswani, A., et al. (2017). <em>Attention is All You Need</em>. NeurIPS.</li>
<li>Radford, A., et al. (2019). <em>Language Models are Unsupervised Multitask Learners</em> (GPT-2). OpenAI.</li>
<li>Brown, T., et al. (2020). <em>Language Models are Few-Shot Learners</em> (GPT-3). NeurIPS.</li>
<li>Ouyang, L., et al. (2022). <em>Training language models to follow instructions with human feedback</em>. NeurIPS.</li>
<li>Hu, E. J., et al. (2021). <em>LoRA: Low-Rank Adaptation of Large Language Models</em>. ICLR.</li>
<li>Wei, J., et al. (2022). <em>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</em>. NeurIPS.</li>
<li>Touvron, H., et al. (2023). <em>LLaMA: Open and Efficient Foundation Language Models</em>. arXiv.</li>
<li>Touvron, H., et al. (2023). <em>Llama 2: Open Foundation and Fine-Tuned Chat Models</em>. arXiv.</li>
<li>Lewis, P., et al. (2020). <em>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</em>. NeurIPS.</li>
<li>Dettmers, T., et al. (2023). <em>QLoRA: Efficient Finetuning of Quantized LLMs</em>. arXiv.</li>
</ol>

<div class="navigation">
    <a href="chapter3-transformers.html" class="nav-button">â† å‰ã®ç« : Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>