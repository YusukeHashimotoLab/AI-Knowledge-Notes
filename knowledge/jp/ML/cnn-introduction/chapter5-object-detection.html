<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
<meta content="ç¬¬5ç« ï¼šç‰©ä½“æ¤œå‡ºå…¥é–€ - AI Terakoya" name="description"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬5ç« ï¼šç‰©ä½“æ¤œå‡ºå…¥é–€ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/cnn-introduction/chapter5-object-detection.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/cnn-introduction/index.html">Cnn</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 5</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬5ç« ï¼šç‰©ä½“æ¤œå‡ºå…¥é–€</h1>
            <p class="subtitle">ç”»åƒåˆ†é¡ã‹ã‚‰ç‰©ä½“æ¤œå‡ºã¸ - R-CNNã€YOLOã€ãã—ã¦æœ€æ–°æ‰‹æ³•</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 4å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… ç”»åƒåˆ†é¡ã¨ç‰©ä½“æ¤œå‡ºã®é•ã„ã‚’ç†è§£ã—ã€é©åˆ‡ãªã‚¿ã‚¹ã‚¯å®šç¾©ãŒã§ãã‚‹</li>
<li>âœ… Two-stage detectorï¼ˆR-CNNç³»ï¼‰ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨é€²åŒ–ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>âœ… One-stage detectorï¼ˆYOLOã€SSDï¼‰ã®è¨­è¨ˆæ€æƒ³ã¨åˆ©ç‚¹ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… IoUã€NMSã€mAPãªã©ã®è©•ä¾¡æŒ‡æ¨™ã‚’å®Ÿè£…ã—è§£é‡ˆã§ãã‚‹</li>
<li>âœ… PyTorchã§ç‰©ä½“æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ãƒ»æ¨è«–ã§ãã‚‹</li>
<li>âœ… COCOå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å®Ÿè·µçš„ãªç‰©ä½“æ¤œå‡ºã‚’å®Ÿç¾ã§ãã‚‹</li>
</ul>

<hr>

<h2>5.1 ç‰©ä½“æ¤œå‡ºã¨ã¯</h2>

<h3>ç”»åƒèªè­˜ã‚¿ã‚¹ã‚¯ã®ç¨®é¡</h3>

<p>ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã«ãŠã‘ã‚‹ç”»åƒèªè­˜ã‚¿ã‚¹ã‚¯ã¯ã€ç›®çš„ã«å¿œã˜ã¦ä¸»ã«3ã¤ã«åˆ†é¡ã•ã‚Œã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[ç”»åƒèªè­˜ã‚¿ã‚¹ã‚¯] --> B[Classification<br/>ç”»åƒåˆ†é¡]
    A --> C[Detection<br/>ç‰©ä½“æ¤œå‡º]
    A --> D[Segmentation<br/>ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³]

    B --> B1["ã€Œã“ã®ç”»åƒã¯ä½•ï¼Ÿã€<br/>ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã®ã¿"]
    C --> C1["ã€Œä½•ãŒã€ã©ã“ã«ï¼Ÿã€<br/>ä½ç½®+ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«"]
    D --> D1["ã€Œã©ã®ãƒ”ã‚¯ã‚»ãƒ«ãŒä½•ï¼Ÿã€<br/>ãƒ”ã‚¯ã‚»ãƒ«å˜ä½ã®åˆ†é¡"]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#e8f5e9
    style D fill:#f3e5f5
</div>

<table>
<thead>
<tr>
<th>ã‚¿ã‚¹ã‚¯</th>
<th>ç›®çš„</th>
<th>å‡ºåŠ›</th>
<th>å¿œç”¨ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Classification</strong></td>
<td>ç”»åƒå…¨ä½“ã®ã‚¯ãƒ©ã‚¹åˆ†é¡</td>
<td>ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ï¼ˆä¾‹: "çŒ«"ï¼‰</td>
<td>ç”»åƒæ¤œç´¢ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°</td>
</tr>
<tr>
<td><strong>Detection</strong></td>
<td>ç‰©ä½“ã®ä½ç½®ã¨ã‚¯ãƒ©ã‚¹ç‰¹å®š</td>
<td>Bounding Box + ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«</td>
<td>è‡ªå‹•é‹è»¢ã€ç›£è¦–ã‚«ãƒ¡ãƒ©ã€åŒ»ç™‚ç”»åƒ</td>
</tr>
<tr>
<td><strong>Segmentation</strong></td>
<td>ãƒ”ã‚¯ã‚»ãƒ«å˜ä½ã®é ˜åŸŸåˆ†å‰²</td>
<td>ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯</td>
<td>èƒŒæ™¯é™¤å»ã€3Då†æ§‹æˆã€åŒ»ç™‚è¨ºæ–­</td>
</tr>
</tbody>
</table>

<h3>ç‰©ä½“æ¤œå‡ºã®åŸºæœ¬æ¦‚å¿µ</h3>

<h4>Bounding Boxï¼ˆãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ï¼‰</h4>

<p><strong>Bounding Box</strong>ã¯ã€æ¤œå‡ºã•ã‚ŒãŸç‰©ä½“ã‚’å›²ã‚€çŸ©å½¢é ˜åŸŸã§ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’æŒã¡ã¾ã™ï¼š</p>

<ul>
<li><strong>åº§æ¨™è¡¨ç¾</strong>: $(x, y, w, h)$ ã¾ãŸã¯ $(x_{\min}, y_{\min}, x_{\max}, y_{\max})$</li>
<li><strong>ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«</strong>: ç‰©ä½“ã®ã‚«ãƒ†ã‚´ãƒªï¼ˆä¾‹: "person", "car"ï¼‰</li>
<li><strong>ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢</strong>: æ¤œå‡ºã®ç¢ºä¿¡åº¦ $[0, 1]$</li>
</ul>

<pre><code class="language-python">import torch
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from PIL import Image
import numpy as np

def visualize_bounding_boxes(image_path, boxes, labels, scores, class_names):
    """
    Bounding Boxã®å¯è¦–åŒ–

    Args:
        image_path: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        boxes: Bounding Boxåº§æ¨™ [[x_min, y_min, x_max, y_max], ...]
        labels: ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ« [0, 1, 2, ...]
        scores: ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ [0.95, 0.87, ...]
        class_names: ã‚¯ãƒ©ã‚¹åã®ãƒªã‚¹ãƒˆ ['person', 'car', ...]
    """
    # ç”»åƒã®èª­ã¿è¾¼ã¿
    img = Image.open(image_path)

    fig, ax = plt.subplots(1, figsize=(12, 8))
    ax.imshow(img)

    # å„Bounding Boxã‚’æç”»
    colors = ['red', 'blue', 'green', 'yellow', 'purple', 'orange']

    for box, label, score in zip(boxes, labels, scores):
        x_min, y_min, x_max, y_max = box
        width = x_max - x_min
        height = y_max - y_min

        # çŸ©å½¢ã‚’æç”»
        color = colors[label % len(colors)]
        rect = patches.Rectangle(
            (x_min, y_min), width, height,
            linewidth=2, edgecolor=color, facecolor='none'
        )
        ax.add_patch(rect)

        # ãƒ©ãƒ™ãƒ«ã¨ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º
        label_text = f'{class_names[label]}: {score:.2f}'
        ax.text(
            x_min, y_min - 5,
            label_text,
            bbox=dict(facecolor=color, alpha=0.7),
            fontsize=10, color='white'
        )

    ax.axis('off')
    plt.tight_layout()
    plt.show()

# ä½¿ç”¨ä¾‹
# boxes = [[50, 50, 200, 300], [250, 100, 400, 350]]
# labels = [0, 1]  # 0: person, 1: car
# scores = [0.95, 0.87]
# class_names = ['person', 'car', 'dog', 'cat']
# visualize_bounding_boxes('sample.jpg', boxes, labels, scores, class_names)
</code></pre>

<h4>IoU (Intersection over Union)</h4>

<p><strong>IoU</strong>ã¯ã€2ã¤ã®Bounding Boxã®é‡ãªã‚Šå…·åˆã‚’æ¸¬ã‚‹æŒ‡æ¨™ã§ã€ç‰©ä½“æ¤œå‡ºã®è©•ä¾¡ã«ä¸å¯æ¬ ã§ã™ã€‚</p>

<p>$$
\text{IoU} = \frac{\text{Area of Overlap}}{\text{Area of Union}} = \frac{|A \cap B|}{|A \cup B|}
$$</p>

<div class="mermaid">
graph LR
    A[äºˆæ¸¬Box] --> C[Intersection<br/>é‡ãªã‚Šé ˜åŸŸ]
    B[æ­£è§£Box] --> C
    C --> D[Union<br/>å’Œé›†åˆé ˜åŸŸ]
    D --> E[IoU = Intersection / Union]

    style A fill:#ffebee
    style B fill:#e8f5e9
    style C fill:#fff3e0
    style E fill:#e3f2fd
</div>

<pre><code class="language-python">def calculate_iou(box1, box2):
    """
    2ã¤ã®Bounding Boxé–“ã®IoUã‚’è¨ˆç®—

    Args:
        box1, box2: [x_min, y_min, x_max, y_max]

    Returns:
        iou: IoUå€¤ [0, 1]
    """
    # äº¤å·®é ˜åŸŸã®åº§æ¨™
    x_min_inter = max(box1[0], box2[0])
    y_min_inter = max(box1[1], box2[1])
    x_max_inter = min(box1[2], box2[2])
    y_max_inter = min(box1[3], box2[3])

    # äº¤å·®é ˜åŸŸã®é¢ç©
    inter_width = max(0, x_max_inter - x_min_inter)
    inter_height = max(0, y_max_inter - y_min_inter)
    intersection = inter_width * inter_height

    # å„Boxã®é¢ç©
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])

    # å’Œé›†åˆã®é¢ç©
    union = area1 + area2 - intersection

    # IoUè¨ˆç®—ï¼ˆã‚¼ãƒ­é™¤ç®—ã‚’å›é¿ï¼‰
    iou = intersection / union if union > 0 else 0

    return iou

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
box1 = [50, 50, 150, 150]   # æ­£è§£Box
box2 = [100, 100, 200, 200] # äºˆæ¸¬Boxï¼ˆéƒ¨åˆ†çš„é‡ãªã‚Šï¼‰
box3 = [50, 50, 150, 150]   # äºˆæ¸¬Boxï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
box4 = [200, 200, 300, 300] # äºˆæ¸¬Boxï¼ˆé‡ãªã‚Šãªã—ï¼‰

print(f"éƒ¨åˆ†çš„é‡ãªã‚Š IoU: {calculate_iou(box1, box2):.4f}")  # ~0.14
print(f"å®Œå…¨ä¸€è‡´ IoU: {calculate_iou(box1, box3):.4f}")      # 1.00
print(f"é‡ãªã‚Šãªã— IoU: {calculate_iou(box1, box4):.4f}")    # 0.00

# ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸãƒãƒƒãƒIoUè¨ˆç®—
def batch_iou(boxes1, boxes2):
    """
    è¤‡æ•°ã®Bounding Boxé–“ã®IoUã‚’åŠ¹ç‡çš„ã«è¨ˆç®—ï¼ˆPyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰

    Args:
        boxes1: Tensor of shape [N, 4]
        boxes2: Tensor of shape [M, 4]

    Returns:
        iou: Tensor of shape [N, M]
    """
    # äº¤å·®é ˜åŸŸã®è¨ˆç®—
    x_min = torch.max(boxes1[:, None, 0], boxes2[:, 0])
    y_min = torch.max(boxes1[:, None, 1], boxes2[:, 1])
    x_max = torch.min(boxes1[:, None, 2], boxes2[:, 2])
    y_max = torch.min(boxes1[:, None, 3], boxes2[:, 3])

    inter_width = torch.clamp(x_max - x_min, min=0)
    inter_height = torch.clamp(y_max - y_min, min=0)
    intersection = inter_width * inter_height

    # é¢ç©ã®è¨ˆç®—
    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])
    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])

    # å’Œé›†åˆã¨IoU
    union = area1[:, None] + area2 - intersection
    iou = intersection / union

    return iou

# ä½¿ç”¨ä¾‹
boxes1 = torch.tensor([[50, 50, 150, 150], [100, 100, 200, 200]], dtype=torch.float32)
boxes2 = torch.tensor([[50, 50, 150, 150], [200, 200, 300, 300]], dtype=torch.float32)

iou_matrix = batch_iou(boxes1, boxes2)
print("\nBatch IoU Matrix:")
print(iou_matrix)
# å‡ºåŠ›:
# tensor([[1.0000, 0.0000],
#         [0.1429, 0.0000]])
</code></pre>

<blockquote>
<p><strong>IoUã®åˆ¤å®šåŸºæº–</strong>:</p>
<ul>
<li>IoU â‰¥ 0.5: é€šå¸¸ã€æ­£è§£ã¨ã—ã¦æ‰±ã‚ã‚Œã‚‹ï¼ˆPASCAL VOCåŸºæº–ï¼‰</li>
<li>IoU â‰¥ 0.75: å³ã—ã„åŸºæº–ï¼ˆCOCOè©•ä¾¡ï¼‰</li>
<li>IoU &lt; 0.5: èª¤æ¤œå‡ºã¨ã—ã¦æ‰±ã‚ã‚Œã‚‹</li>
</ul>
</blockquote>

<hr>

<h2>5.2 Two-Stage Detectors</h2>

<h3>R-CNNç³»ã®é€²åŒ–</h3>

<p>Two-stage detectorã¯ã€<strong>â‘ å€™è£œé ˜åŸŸã®ææ¡ˆ</strong>ã¨<strong>â‘¡ç‰©ä½“ã®ã‚¯ãƒ©ã‚¹åˆ†é¡</strong>ã‚’2æ®µéšã§è¡Œã†ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[å…¥åŠ›ç”»åƒ] --> B[Stage 1<br/>Region Proposal]
    B --> C[å€™è£œé ˜åŸŸ<br/>~2000å€‹]
    C --> D[Stage 2<br/>Classification]
    D --> E[æœ€çµ‚æ¤œå‡ºçµæœ<br/>Box + Class]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style D fill:#f3e5f5
    style E fill:#e8f5e9
</div>

<h4>R-CNN (2014)</h4>

<p><strong>R-CNN (Regions with CNN features)</strong>ã¯ã€æ·±å±¤å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®ç‰©ä½“æ¤œå‡ºã®å…ˆé§†ã‘ã§ã™ã€‚</p>

<table>
<thead>
<tr>
<th>ã‚¹ãƒ†ãƒƒãƒ—</th>
<th>å‡¦ç†å†…å®¹</th>
<th>ç‰¹å¾´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1. Region Proposal</strong></td>
<td>Selective Searchã§å€™è£œé ˜åŸŸç”Ÿæˆï¼ˆ~2000å€‹ï¼‰</td>
<td>å¾“æ¥ã®ç”»åƒå‡¦ç†æ‰‹æ³•</td>
</tr>
<tr>
<td><strong>2. CNN Feature Extraction</strong></td>
<td>å„é ˜åŸŸã‚’AlexNetã§ç‰¹å¾´æŠ½å‡º</td>
<td>2000å›ã®é †ä¼æ’­ãŒå¿…è¦</td>
</tr>
<tr>
<td><strong>3. SVM Classification</strong></td>
<td>SVMã§ã‚¯ãƒ©ã‚¹åˆ†é¡</td>
<td>CNNã¨ã¯åˆ¥ã«è¨“ç·´</td>
</tr>
<tr>
<td><strong>4. Bounding Box Regression</strong></td>
<td>Boxåº§æ¨™ã‚’å¾®èª¿æ•´</td>
<td>ç²¾åº¦å‘ä¸Š</td>
</tr>
</tbody>
</table>

<p><strong>å•é¡Œç‚¹</strong>:</p>
<ul>
<li>æ¨è«–ãŒéå¸¸ã«é…ã„ï¼ˆ1ç”»åƒã‚ãŸã‚Š47ç§’ï¼‰</li>
<li>è¨“ç·´ãŒè¤‡é›‘ï¼ˆ3æ®µéšã®åˆ¥ã€…ã®å­¦ç¿’ï¼‰</li>
<li>ç‰¹å¾´æŠ½å‡ºã®é‡è¤‡è¨ˆç®—ãŒå¤šã„</li>
</ul>

<h4>Fast R-CNN (2015)</h4>

<p><strong>Fast R-CNN</strong>ã¯ã€R-CNNã®è¨ˆç®—åŠ¹ç‡ã‚’å¤§å¹…ã«æ”¹å–„ã—ã¾ã—ãŸã€‚</p>

<div class="mermaid">
graph LR
    A[å…¥åŠ›ç”»åƒ] --> B[CNN<br/>ç‰¹å¾´ãƒãƒƒãƒ—]
    B --> C[RoI Pooling]
    D[Region<br/>Proposals] --> C
    C --> E[FCå±¤]
    E --> F1[Softmax<br/>ã‚¯ãƒ©ã‚¹åˆ†é¡]
    E --> F2[Regressor<br/>Boxå›å¸°]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style F1 fill:#e8f5e9
    style F2 fill:#e8f5e9
</div>

<p><strong>æ”¹å–„ç‚¹</strong>:</p>
<ul>
<li>ç”»åƒå…¨ä½“ã§1å›ã ã‘CNNã‚’å®Ÿè¡Œ</li>
<li>RoI Poolingã§å€™è£œé ˜åŸŸã‹ã‚‰å›ºå®šã‚µã‚¤ã‚ºã®ç‰¹å¾´ã‚’æŠ½å‡º</li>
<li>Multi-task Lossï¼ˆåˆ†é¡ + Boxå›å¸°ï¼‰ã§End-to-endå­¦ç¿’</li>
<li>æ¨è«–é€Ÿåº¦: 47ç§’ â†’ 2ç§’ï¼ˆ23å€é«˜é€ŸåŒ–ï¼‰</li>
</ul>

<h4>Faster R-CNN (2016)</h4>

<p><strong>Faster R-CNN</strong>ã¯ã€Region Proposalã‚‚CNNã§å­¦ç¿’å¯èƒ½ã«ã—ã€å®Œå…¨ãªEnd-to-endåŒ–ã‚’å®Ÿç¾ã—ã¾ã—ãŸã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torchvision
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

def create_faster_rcnn(num_classes, pretrained=True):
    """
    Faster R-CNNãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ

    Args:
        num_classes: æ¤œå‡ºã™ã‚‹ã‚¯ãƒ©ã‚¹æ•°ï¼ˆèƒŒæ™¯ã‚’å«ã‚€ï¼‰
        pretrained: COCOäº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹

    Returns:
        model: Faster R-CNNãƒ¢ãƒ‡ãƒ«
    """
    # COCOäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    model = fasterrcnn_resnet50_fpn(pretrained=pretrained)

    # åˆ†é¡å™¨ã®ç½®ãæ›ãˆï¼ˆæœ€çµ‚å±¤ã®ã¿ï¼‰
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    return model

# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆï¼ˆä¾‹: COCO 80ã‚¯ãƒ©ã‚¹ + èƒŒæ™¯ï¼‰
model = create_faster_rcnn(num_classes=91, pretrained=True)
model.eval()

print("Faster R-CNNãƒ¢ãƒ‡ãƒ«ã®æ§‹é€ :")
print(f"- Backbone: ResNet-50 + FPN")
print(f"- RPN: Region Proposal Network")
print(f"- RoI Heads: Box Head + Class Predictor")

# æ¨è«–ã®å®Ÿè¡Œ
def run_faster_rcnn_inference(model, image_path, threshold=0.5):
    """
    Faster R-CNNã«ã‚ˆã‚‹ç‰©ä½“æ¤œå‡ºæ¨è«–

    Args:
        model: Faster R-CNNãƒ¢ãƒ‡ãƒ«
        image_path: å…¥åŠ›ç”»åƒãƒ‘ã‚¹
        threshold: æ¤œå‡ºã‚¹ã‚³ã‚¢ã®é–¾å€¤

    Returns:
        boxes, labels, scores: æ¤œå‡ºçµæœ
    """
    from PIL import Image
    from torchvision import transforms

    # ç”»åƒã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
    img = Image.open(image_path).convert('RGB')
    transform = transforms.Compose([transforms.ToTensor()])
    img_tensor = transform(img).unsqueeze(0)  # [1, 3, H, W]

    # æ¨è«–
    model.eval()
    with torch.no_grad():
        predictions = model(img_tensor)

    # é–¾å€¤ä»¥ä¸Šã®ã‚¹ã‚³ã‚¢ã®æ¤œå‡ºçµæœã®ã¿æŠ½å‡º
    pred = predictions[0]
    keep = pred['scores'] > threshold

    boxes = pred['boxes'][keep].cpu().numpy()
    labels = pred['labels'][keep].cpu().numpy()
    scores = pred['scores'][keep].cpu().numpy()

    print(f"\næ¤œå‡ºã•ã‚ŒãŸç‰©ä½“æ•°: {len(boxes)}")
    for i, (box, label, score) in enumerate(zip(boxes, labels, scores)):
        print(f"  {i+1}. Label: {label}, Score: {score:.3f}, Box: {box}")

    return boxes, labels, scores

# COCO ã‚¯ãƒ©ã‚¹åï¼ˆä¸€éƒ¨ï¼‰
COCO_INSTANCE_CATEGORY_NAMES = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',
    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow'
    # ... å…¨91ã‚¯ãƒ©ã‚¹
]

# ä½¿ç”¨ä¾‹
# boxes, labels, scores = run_faster_rcnn_inference(model, 'test_image.jpg', threshold=0.7)
# visualize_bounding_boxes('test_image.jpg', boxes, labels, scores, COCO_INSTANCE_CATEGORY_NAMES)
</code></pre>

<h3>Region Proposal Network (RPN)</h3>

<p><strong>RPN</strong>ã¯ã€Faster R-CNNã®æ ¸å¿ƒæŠ€è¡“ã§ã€å€™è£œé ˜åŸŸã‚’å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã§ææ¡ˆã—ã¾ã™ã€‚</p>

<blockquote>
<p><strong>RPNã®ä»•çµ„ã¿</strong>:</p>
<ol>
<li>ç‰¹å¾´ãƒãƒƒãƒ—ã®å„ä½ç½®ã«è¤‡æ•°ã®Anchor Boxï¼ˆç•°ãªã‚‹ã‚µã‚¤ã‚ºãƒ»ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ï¼‰ã‚’é…ç½®</li>
<li>å„Anchorã«å¯¾ã—ã¦ã€Œç‰©ä½“ã‚‰ã—ã•ï¼ˆObjectnessï¼‰ã€ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°</li>
<li>Bounding Box ã®åº§æ¨™ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’å›å¸°</li>
<li>é«˜ã‚¹ã‚³ã‚¢ã®Proposalã‚’RoI Poolingã«æ¸¡ã™</li>
</ol>
</blockquote>

<pre><code class="language-python">class SimpleRPN(nn.Module):
    """
    ç°¡ç•¥åŒ–ã•ã‚ŒãŸRegion Proposal Networkï¼ˆæ•™è‚²ç›®çš„ï¼‰
    """

    def __init__(self, in_channels=512, num_anchors=9):
        """
        Args:
            in_channels: å…¥åŠ›ç‰¹å¾´ãƒãƒƒãƒ—ã®ãƒãƒ£ãƒ³ãƒãƒ«æ•°
            num_anchors: å„ä½ç½®ã®Anchoræ•°ï¼ˆé€šå¸¸ 3ã‚¹ã‚±ãƒ¼ãƒ« Ã— 3ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯” = 9ï¼‰
        """
        super(SimpleRPN, self).__init__()

        # å…±æœ‰ç•³ã¿è¾¼ã¿å±¤
        self.conv = nn.Conv2d(in_channels, 512, kernel_size=3, padding=1)

        # Objectnessã‚¹ã‚³ã‚¢ï¼ˆç‰©ä½“ or èƒŒæ™¯ã®2ã‚¯ãƒ©ã‚¹ï¼‰
        self.cls_logits = nn.Conv2d(512, num_anchors * 2, kernel_size=1)

        # Bounding Boxå›å¸°ï¼ˆ4åº§æ¨™ Ã— num_anchorsï¼‰
        self.bbox_pred = nn.Conv2d(512, num_anchors * 4, kernel_size=1)

    def forward(self, feature_map):
        """
        Args:
            feature_map: [B, C, H, W] ç‰¹å¾´ãƒãƒƒãƒ—

        Returns:
            objectness: [B, num_anchors*2, H, W] ç‰©ä½“ã‚¹ã‚³ã‚¢
            bbox_deltas: [B, num_anchors*4, H, W] Boxåº§æ¨™ã‚ªãƒ•ã‚»ãƒƒãƒˆ
        """
        # å…±æœ‰ç‰¹å¾´æŠ½å‡º
        x = torch.relu(self.conv(feature_map))

        # Objectnessåˆ†é¡
        objectness = self.cls_logits(x)

        # Bounding Boxå›å¸°
        bbox_deltas = self.bbox_pred(x)

        return objectness, bbox_deltas

# RPNã®å‹•ä½œç¢ºèª
rpn = SimpleRPN(in_channels=512, num_anchors=9)
feature_map = torch.randn(1, 512, 38, 38)  # ä¾‹: ResNetã®ç‰¹å¾´ãƒãƒƒãƒ—

objectness, bbox_deltas = rpn(feature_map)
print(f"Objectness shape: {objectness.shape}")     # [1, 18, 38, 38]
print(f"BBox Deltas shape: {bbox_deltas.shape}")   # [1, 36, 38, 38]
print(f"Total Proposals: {38 * 38 * 9} anchors")   # 12,996å€‹ã®Anchor
</code></pre>

<hr>

<h2>5.3 One-Stage Detectors</h2>

<h3>YOLO (You Only Look Once)</h3>

<p><strong>YOLO</strong>ã¯ã€ç‰©ä½“æ¤œå‡ºã‚’ã€Œå›å¸°å•é¡Œã€ã¨ã—ã¦å®šå¼åŒ–ã—ã€å˜ä¸€ã®CNNã§ç›´æ¥Bounding Boxã¨ã‚¯ãƒ©ã‚¹ã‚’End-to-endäºˆæ¸¬ã—ã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[å…¥åŠ›ç”»åƒ<br/>448Ã—448] --> B[CNN Backbone<br/>ç‰¹å¾´æŠ½å‡º]
    B --> C[Gridåˆ†å‰²<br/>7Ã—7]
    C --> D[å„ã‚»ãƒ«ã§äºˆæ¸¬<br/>Box + Class]
    D --> E[NMS<br/>é‡è¤‡é™¤å»]
    E --> F[æœ€çµ‚æ¤œå‡ºçµæœ]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#ffe0b2
    style E fill:#e1bee7
    style F fill:#e8f5e9
</div>

<h4>YOLOã®è¨­è¨ˆæ€æƒ³</h4>

<ul>
<li><strong>é€Ÿåº¦é‡è¦–</strong>: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ï¼ˆ45+ FPSï¼‰ã‚’ç›®æŒ‡ã™</li>
<li><strong>Global Context</strong>: ç”»åƒå…¨ä½“ã‚’ä¸€åº¦ã«è¦‹ã‚‹ãŸã‚æ–‡è„ˆç†è§£ãŒè‰¯ã„</li>
<li><strong>ã‚·ãƒ³ãƒ—ãƒ«</strong>: è¤‡é›‘ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãªã—ã€End-to-endå­¦ç¿’</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn

# YOLOv5ã®ä½¿ç”¨ï¼ˆUltralyticså®Ÿè£…ï¼‰
def load_yolov5(model_size='yolov5s', pretrained=True):
    """
    YOLOv5ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰

    Args:
        model_size: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º ('yolov5n', 'yolov5s', 'yolov5m', 'yolov5l', 'yolov5x')
        pretrained: COCOäº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ä½¿ç”¨

    Returns:
        model: YOLOv5ãƒ¢ãƒ‡ãƒ«
    """
    # PyTorch Hubã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ï¼ˆUltralyticså®Ÿè£…ï¼‰
    model = torch.hub.load('ultralytics/yolov5', model_size, pretrained=pretrained)

    return model

# ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
model = load_yolov5('yolov5s', pretrained=True)
model.eval()

print("YOLOv5s ãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
print(f"- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}")
print(f"- æ¨è«–é€Ÿåº¦: ~140 FPS (GPU)")
print(f"- å…¥åŠ›ã‚µã‚¤ã‚º: 640Ã—640 (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)")

def run_yolo_inference(model, image_path, conf_threshold=0.25, iou_threshold=0.45):
    """
    YOLOv5ã«ã‚ˆã‚‹ç‰©ä½“æ¤œå‡ºæ¨è«–

    Args:
        model: YOLOv5ãƒ¢ãƒ‡ãƒ«
        image_path: å…¥åŠ›ç”»åƒãƒ‘ã‚¹
        conf_threshold: ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®é–¾å€¤
        iou_threshold: NMSã®IoUé–¾å€¤

    Returns:
        results: æ¤œå‡ºçµæœï¼ˆpandas DataFrameï¼‰
    """
    # æ¨è«–è¨­å®š
    model.conf = conf_threshold
    model.iou = iou_threshold

    # æ¨è«–å®Ÿè¡Œ
    results = model(image_path)

    # çµæœã®è¡¨ç¤º
    results.print()  # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›

    # çµæœã®å¯è¦–åŒ–
    results.show()   # ç”»åƒè¡¨ç¤º

    # çµæœã‚’DataFrameã§å–å¾—
    detections = results.pandas().xyxy[0]

    print(f"\næ¤œå‡ºã•ã‚ŒãŸç‰©ä½“æ•°: {len(detections)}")
    print(detections)

    return results

# ä½¿ç”¨ä¾‹
# results = run_yolo_inference(model, 'test_image.jpg', conf_threshold=0.5)

# ãƒãƒƒãƒæ¨è«–
def run_yolo_batch_inference(model, image_paths, save_dir='results/'):
    """
    è¤‡æ•°ç”»åƒã®ãƒãƒƒãƒæ¨è«–

    Args:
        model: YOLOv5ãƒ¢ãƒ‡ãƒ«
        image_paths: ç”»åƒãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        save_dir: çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    import os
    os.makedirs(save_dir, exist_ok=True)

    # ãƒãƒƒãƒæ¨è«–
    results = model(image_paths)

    # çµæœã‚’ä¿å­˜
    results.save(save_dir=save_dir)

    print(f"ãƒãƒƒãƒæ¨è«–å®Œäº†: {len(image_paths)}æšã®ç”»åƒ")
    print(f"çµæœä¿å­˜å…ˆ: {save_dir}")

    return results

# ä½¿ç”¨ä¾‹
# image_list = ['img1.jpg', 'img2.jpg', 'img3.jpg']
# batch_results = run_yolo_batch_inference(model, image_list)
</code></pre>

<h4>YOLOã®Lossé–¢æ•°</h4>

<p>YOLOã¯3ã¤ã®æå¤±ã‚’çµ„ã¿åˆã‚ã›ã¦å­¦ç¿’ã—ã¾ã™ï¼š</p>

<p>$$
\mathcal{L}_{\text{YOLO}} = \lambda_{\text{box}} \mathcal{L}_{\text{box}} + \lambda_{\text{obj}} \mathcal{L}_{\text{obj}} + \lambda_{\text{cls}} \mathcal{L}_{\text{cls}}
$$</p>

<ul>
<li>$\mathcal{L}_{\text{box}}$: Bounding Boxåº§æ¨™ã®å›å¸°æå¤±ï¼ˆCIoU Lossï¼‰</li>
<li>$\mathcal{L}_{\text{obj}}$: Objectnessï¼ˆç‰©ä½“ã‚‰ã—ã•ï¼‰ã®äºŒå€¤åˆ†é¡æå¤±</li>
<li>$\mathcal{L}_{\text{cls}}$: ã‚¯ãƒ©ã‚¹åˆ†é¡ã®å¤šã‚¯ãƒ©ã‚¹æå¤±</li>
</ul>

<h3>SSD (Single Shot Detector)</h3>

<p><strong>SSD</strong>ã¯ã€ç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã®ç‰¹å¾´ãƒãƒƒãƒ—ã§æ¤œå‡ºã‚’è¡Œã„ã€é€Ÿåº¦ã¨ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚Šã¾ã™ã€‚</p>

<blockquote>
<p><strong>SSDã®ç‰¹å¾´</strong>:</p>
<ul>
<li>Multi-scale Feature Mapsï¼ˆè¤‡æ•°è§£åƒåº¦ã§ã®æ¤œå‡ºï¼‰</li>
<li>Default Boxesï¼ˆAnchorã«ç›¸å½“ï¼‰ã‚’å„ç‰¹å¾´ãƒãƒƒãƒ—ã§ä½¿ç”¨</li>
<li>YOLOã‚ˆã‚Šç²¾åº¦ãŒé«˜ãã€Faster R-CNNã‚ˆã‚Šé€Ÿã„</li>
</ul>
</blockquote>

<pre><code class="language-python">from torchvision.models.detection import ssd300_vgg16

def create_ssd_model(num_classes=91, pretrained=True):
    """
    SSD300ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ

    Args:
        num_classes: æ¤œå‡ºã‚¯ãƒ©ã‚¹æ•°
        pretrained: äº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ä½¿ç”¨

    Returns:
        model: SSD300ãƒ¢ãƒ‡ãƒ«
    """
    # SSD300 with VGG16 backbone
    model = ssd300_vgg16(pretrained=pretrained, num_classes=num_classes)

    return model

# ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
ssd_model = create_ssd_model(num_classes=91, pretrained=True)
ssd_model.eval()

print("SSD300ãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
print(f"- å…¥åŠ›ã‚µã‚¤ã‚º: 300Ã—300")
print(f"- Backbone: VGG16")
print(f"- ç‰¹å¾´ãƒãƒƒãƒ—: 6å±¤ï¼ˆç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰")

def run_ssd_inference(model, image_path, threshold=0.5):
    """
    SSDã«ã‚ˆã‚‹ç‰©ä½“æ¤œå‡ºæ¨è«–
    """
    from PIL import Image
    from torchvision import transforms

    # ç”»åƒã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
    img = Image.open(image_path).convert('RGB')
    transform = transforms.Compose([transforms.ToTensor()])
    img_tensor = transform(img).unsqueeze(0)

    # æ¨è«–
    model.eval()
    with torch.no_grad():
        predictions = model(img_tensor)

    # çµæœã®æŠ½å‡º
    pred = predictions[0]
    keep = pred['scores'] > threshold

    boxes = pred['boxes'][keep].cpu().numpy()
    labels = pred['labels'][keep].cpu().numpy()
    scores = pred['scores'][keep].cpu().numpy()

    print(f"\næ¤œå‡ºã•ã‚ŒãŸç‰©ä½“æ•°: {len(boxes)}")
    for i, (box, label, score) in enumerate(zip(boxes, labels, scores)):
        print(f"  {i+1}. Label: {label}, Score: {score:.3f}")

    return boxes, labels, scores

# ä½¿ç”¨ä¾‹
# boxes, labels, scores = run_ssd_inference(ssd_model, 'test_image.jpg', threshold=0.6)
</code></pre>

<hr>

<h2>5.4 è©•ä¾¡æŒ‡æ¨™</h2>

<h3>Precision ã¨ Recall</h3>

<p>ç‰©ä½“æ¤œå‡ºã®è©•ä¾¡ã«ã¯ã€æƒ…å ±æ¤œç´¢ã¨åŒæ§˜ã®æŒ‡æ¨™ãŒä½¿ã‚ã‚Œã¾ã™ã€‚</p>

<p>$$
\text{Precision} = \frac{TP}{TP + FP} \quad \text{(æ¤œå‡ºã®æ­£ç¢ºæ€§)}
$$</p>

<p>$$
\text{Recall} = \frac{TP}{TP + FN} \quad \text{(æ¤œå‡ºã®ç¶²ç¾…æ€§)}
$$</p>

<ul>
<li><strong>TP (True Positive)</strong>: æ­£ã—ãæ¤œå‡ºã•ã‚ŒãŸç‰©ä½“ï¼ˆIoU â‰¥ é–¾å€¤ï¼‰</li>
<li><strong>FP (False Positive)</strong>: èª¤æ¤œå‡ºï¼ˆIoU &lt; é–¾å€¤ ã¾ãŸã¯ èƒŒæ™¯ã‚’ç‰©ä½“ã¨èª¤èªï¼‰</li>
<li><strong>FN (False Negative)</strong>: æ¤œå‡ºæ¼ã‚Œï¼ˆå­˜åœ¨ã™ã‚‹ç‰©ä½“ã‚’è¦‹é€ƒã—ãŸï¼‰</li>
</ul>

<h3>NMS (Non-Maximum Suppression)</h3>

<p><strong>NMS</strong>ã¯ã€é‡è¤‡ã™ã‚‹æ¤œå‡ºçµæœã‚’é™¤å»ã—ã€1ã¤ã®ç‰©ä½“ã«å¯¾ã—ã¦1ã¤ã®Boxã®ã¿ã‚’æ®‹ã™ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[æ¤œå‡ºBoxes<br/>ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ] --> B[æœ€é«˜ã‚¹ã‚³ã‚¢Boxé¸æŠ]
    B --> C[é‡è¤‡Boxé™¤å»<br/>IoU > threshold]
    C --> D{æ®‹ã‚ŠBoxã‚ã‚Š?}
    D -->|Yes| B
    D -->|No| E[æœ€çµ‚æ¤œå‡ºçµæœ]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style E fill:#e8f5e9
</div>

<pre><code class="language-python">def non_max_suppression(boxes, scores, iou_threshold=0.5):
    """
    Non-Maximum Suppression (NMS)ã®å®Ÿè£…

    Args:
        boxes: Bounding Boxåº§æ¨™ [[x_min, y_min, x_max, y_max], ...] (numpy array)
        scores: ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ [0.9, 0.8, ...]
        iou_threshold: IoUé–¾å€¤ï¼ˆã“ã‚Œä»¥ä¸Šé‡è¤‡ã™ã‚‹Boxã¯é™¤å»ï¼‰

    Returns:
        keep_indices: ä¿æŒã™ã‚‹Boxã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    """
    import numpy as np

    # ã‚¹ã‚³ã‚¢ã®é™é †ã§ã‚½ãƒ¼ãƒˆ
    sorted_indices = np.argsort(scores)[::-1]

    keep_indices = []

    while len(sorted_indices) > 0:
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®Boxã‚’ä¿æŒ
        current = sorted_indices[0]
        keep_indices.append(current)

        if len(sorted_indices) == 1:
            break

        # æ®‹ã‚Šã®Boxã¨ã®IoUã‚’è¨ˆç®—
        current_box = boxes[current]
        remaining_boxes = boxes[sorted_indices[1:]]

        ious = np.array([calculate_iou(current_box, box) for box in remaining_boxes])

        # IoUé–¾å€¤ä»¥ä¸‹ã®Boxã®ã¿æ®‹ã™
        keep_mask = ious < iou_threshold
        sorted_indices = sorted_indices[1:][keep_mask]

    return np.array(keep_indices)

# ä½¿ç”¨ä¾‹
boxes = np.array([
    [50, 50, 150, 150],
    [55, 55, 155, 155],   # æœ€åˆã®Boxã¨é‡è¤‡å¤§
    [200, 200, 300, 300],
    [205, 205, 305, 305]  # 3ç•ªç›®ã®Boxã¨é‡è¤‡å¤§
])
scores = np.array([0.9, 0.85, 0.95, 0.88])

keep_indices = non_max_suppression(boxes, scores, iou_threshold=0.5)
print(f"å…ƒã®Boxæ•°: {len(boxes)}")
print(f"NMSå¾Œã®Boxæ•°: {len(keep_indices)}")
print(f"ä¿æŒã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {keep_indices}")
print(f"ä¿æŒã•ã‚ŒãŸBoxes:\n{boxes[keep_indices]}")

# PyTorchã®å…¬å¼NMSå®Ÿè£…ï¼ˆã‚ˆã‚Šé«˜é€Ÿï¼‰
from torchvision.ops import nms

def nms_torch(boxes, scores, iou_threshold=0.5):
    """
    PyTorchç‰ˆNMSï¼ˆC++å®Ÿè£…ã§é«˜é€Ÿï¼‰

    Args:
        boxes: Tensor of shape [N, 4]
        scores: Tensor of shape [N]
        iou_threshold: IoUé–¾å€¤

    Returns:
        keep: ä¿æŒã™ã‚‹Boxã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆTensorï¼‰
    """
    keep = nms(boxes, scores, iou_threshold)
    return keep

# ä½¿ç”¨ä¾‹
boxes_tensor = torch.tensor(boxes, dtype=torch.float32)
scores_tensor = torch.tensor(scores, dtype=torch.float32)

keep_torch = nms_torch(boxes_tensor, scores_tensor, iou_threshold=0.5)
print(f"\nPyTorch NMSçµæœ: {keep_torch}")
</code></pre>

<h3>mAP (mean Average Precision)</h3>

<p><strong>mAP</strong>ã¯ã€ç‰©ä½“æ¤œå‡ºã®æ¨™æº–çš„ãªè©•ä¾¡æŒ‡æ¨™ã§ã€å…¨ã‚¯ãƒ©ã‚¹ã®å¹³å‡ç²¾åº¦ã‚’è¡¨ã—ã¾ã™ã€‚</p>

<h4>è¨ˆç®—æ‰‹é †</h4>

<ol>
<li><strong>å„ã‚¯ãƒ©ã‚¹ã”ã¨ã«</strong>Precision-Recallæ›²ç·šã‚’æç”»</li>
<li>æ›²ç·šã®ä¸‹å´é¢ç© <strong>AP (Average Precision)</strong> ã‚’è¨ˆç®—</li>
<li>å…¨ã‚¯ãƒ©ã‚¹ã®APã®å¹³å‡ã‚’å–ã‚Š <strong>mAP</strong> ã‚’ç®—å‡º</li>
</ol>

<p>$$
\text{AP} = \int_0^1 P(r) \, dr
$$</p>

<p>$$
\text{mAP} = \frac{1}{C} \sum_{c=1}^{C} \text{AP}_c
$$</p>

<pre><code class="language-python">def calculate_precision_recall(pred_boxes, pred_scores, true_boxes, iou_threshold=0.5):
    """
    Precision-Recallæ›²ç·šã®ãŸã‚ã®å€¤ã‚’è¨ˆç®—

    Args:
        pred_boxes: äºˆæ¸¬Boxes [N, 4]
        pred_scores: äºˆæ¸¬ã‚¹ã‚³ã‚¢ [N]
        true_boxes: æ­£è§£Boxes [M, 4]
        iou_threshold: IoUé–¾å€¤

    Returns:
        precisions, recalls: Precision-Recallå€¤ã®ãƒªã‚¹ãƒˆ
    """
    import numpy as np

    # ã‚¹ã‚³ã‚¢ã®é™é †ã§ã‚½ãƒ¼ãƒˆ
    sorted_indices = np.argsort(pred_scores)[::-1]
    pred_boxes = pred_boxes[sorted_indices]
    pred_scores = pred_scores[sorted_indices]

    num_true = len(true_boxes)
    matched_true = np.zeros(num_true, dtype=bool)

    tp = np.zeros(len(pred_boxes))
    fp = np.zeros(len(pred_boxes))

    for i, pred_box in enumerate(pred_boxes):
        # æ­£è§£Boxã¨ã®æœ€å¤§IoUã‚’è¨ˆç®—
        if len(true_boxes) == 0:
            fp[i] = 1
            continue

        ious = np.array([calculate_iou(pred_box, true_box) for true_box in true_boxes])
        max_iou_idx = np.argmax(ious)
        max_iou = ious[max_iou_idx]

        # IoUé–¾å€¤ã‚’è¶…ãˆã€ã¾ã ãƒãƒƒãƒã—ã¦ã„ãªã„æ­£è§£Boxãªã‚‰TP
        if max_iou >= iou_threshold and not matched_true[max_iou_idx]:
            tp[i] = 1
            matched_true[max_iou_idx] = True
        else:
            fp[i] = 1

    # ç´¯ç©å’Œ
    tp_cumsum = np.cumsum(tp)
    fp_cumsum = np.cumsum(fp)

    # Precision ã¨ Recall
    recalls = tp_cumsum / num_true if num_true > 0 else np.zeros_like(tp_cumsum)
    precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-10)

    return precisions, recalls

def calculate_ap(precisions, recalls):
    """
    Average Precision (AP)ã‚’è¨ˆç®—ï¼ˆ11ç‚¹è£œé–“æ³•ï¼‰

    Args:
        precisions: Precisionå€¤ã®ãƒªã‚¹ãƒˆ
        recalls: Recallå€¤ã®ãƒªã‚¹ãƒˆ

    Returns:
        ap: Average Precision
    """
    import numpy as np

    # 11ç‚¹è£œé–“
    ap = 0.0
    for t in np.linspace(0, 1, 11):
        # Recall â‰¥ t ã«ãŠã‘ã‚‹Precisionã®æœ€å¤§å€¤
        if np.sum(recalls >= t) == 0:
            p = 0
        else:
            p = np.max(precisions[recalls >= t])
        ap += p / 11

    return ap

# ä½¿ç”¨ä¾‹
pred_boxes = np.array([
    [50, 50, 150, 150],
    [55, 55, 155, 155],
    [200, 200, 300, 300]
])
pred_scores = np.array([0.9, 0.7, 0.85])
true_boxes = np.array([
    [52, 52, 152, 152],
    [205, 205, 305, 305]
])

precisions, recalls = calculate_precision_recall(
    pred_boxes, pred_scores, true_boxes, iou_threshold=0.5
)

ap = calculate_ap(precisions, recalls)
print(f"Average Precision: {ap:.4f}")

# Precision-Recallæ›²ç·šã®å¯è¦–åŒ–
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(recalls, precisions, marker='o', linewidth=2)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.grid(True, alpha=0.3)
plt.xlim([0, 1.05])
plt.ylim([0, 1.05])
plt.fill_between(recalls, precisions, alpha=0.2)
plt.text(0.5, 0.5, f'AP = {ap:.4f}', fontsize=14, bbox=dict(facecolor='white', alpha=0.8))
plt.tight_layout()
plt.show()

def calculate_map(all_precisions, all_recalls, num_classes):
    """
    mean Average Precision (mAP)ã‚’è¨ˆç®—

    Args:
        all_precisions: å„ã‚¯ãƒ©ã‚¹ã®Precisionãƒªã‚¹ãƒˆ [[p1, p2, ...], ...]
        all_recalls: å„ã‚¯ãƒ©ã‚¹ã®Recallãƒªã‚¹ãƒˆ [[r1, r2, ...], ...]
        num_classes: ã‚¯ãƒ©ã‚¹æ•°

    Returns:
        mAP: mean Average Precision
    """
    aps = []

    for i in range(num_classes):
        ap = calculate_ap(all_precisions[i], all_recalls[i])
        aps.append(ap)
        print(f"Class {i}: AP = {ap:.4f}")

    mAP = np.mean(aps)
    print(f"\nmAP: {mAP:.4f}")

    return mAP
</code></pre>

<blockquote>
<p><strong>COCO mAP</strong>: COCOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯ã€è¤‡æ•°ã®IoUé–¾å€¤ï¼ˆ0.5, 0.55, ..., 0.95ï¼‰ã§APã‚’è¨ˆç®—ã—ã€ãã®å¹³å‡ã‚’å–ã‚‹å³ã—ã„è©•ä¾¡ã‚’è¡Œã„ã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>5.5 PyTorchã§ã®ç‰©ä½“æ¤œå‡º</h2>

<h3>torchvision.models.detectionã®æ´»ç”¨</h3>

<p>PyTorchã®torchvisionã«ã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ç‰©ä½“æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ãŒè±Šå¯Œã«ç”¨æ„ã•ã‚Œã¦ã„ã¾ã™ã€‚</p>

<pre><code class="language-python">import torch
import torchvision
from torchvision.models.detection import (
    fasterrcnn_resnet50_fpn,
    fasterrcnn_mobilenet_v3_large_fpn,
    retinanet_resnet50_fpn,
    ssd300_vgg16
)

def compare_detection_models():
    """
    å„ç¨®ç‰©ä½“æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ
    """
    models_info = {
        'Faster R-CNN (ResNet-50)': {
            'model': fasterrcnn_resnet50_fpn,
            'type': 'Two-Stage',
            'speed': 'é…',
            'accuracy': 'é«˜'
        },
        'Faster R-CNN (MobileNetV3)': {
            'model': fasterrcnn_mobilenet_v3_large_fpn,
            'type': 'Two-Stage',
            'speed': 'ä¸­',
            'accuracy': 'ä¸­'
        },
        'RetinaNet (ResNet-50)': {
            'model': retinanet_resnet50_fpn,
            'type': 'One-Stage',
            'speed': 'ä¸­',
            'accuracy': 'é«˜'
        },
        'SSD300 (VGG16)': {
            'model': ssd300_vgg16,
            'type': 'One-Stage',
            'speed': 'é€Ÿ',
            'accuracy': 'ä¸­'
        }
    }

    print("ç‰©ä½“æ¤œå‡ºãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ:")
    print("-" * 80)
    for name, info in models_info.items():
        print(f"{name:35s} | Type: {info['type']:10s} | "
              f"Speed: {info['speed']:3s} | Accuracy: {info['accuracy']:3s}")
    print("-" * 80)

compare_detection_models()

# ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®Fine-tuning
from torch.utils.data import Dataset, DataLoader
import json

class CustomDetectionDataset(Dataset):
    """
    ã‚«ã‚¹ã‚¿ãƒ ç‰©ä½“æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆCOCOå½¢å¼ï¼‰
    """

    def __init__(self, image_dir, annotation_file, transforms=None):
        """
        Args:
            image_dir: ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
            annotation_file: COCOãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«
            transforms: ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
        """
        self.image_dir = image_dir
        self.transforms = transforms

        # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®èª­ã¿è¾¼ã¿
        with open(annotation_file, 'r') as f:
            self.coco_data = json.load(f)

        self.images = self.coco_data['images']
        self.annotations = self.coco_data['annotations']

        # ç”»åƒIDã”ã¨ã«ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        self.image_to_annotations = {}
        for ann in self.annotations:
            image_id = ann['image_id']
            if image_id not in self.image_to_annotations:
                self.image_to_annotations[image_id] = []
            self.image_to_annotations[image_id].append(ann)

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        # ç”»åƒæƒ…å ±
        img_info = self.images[idx]
        image_id = img_info['id']
        img_path = f"{self.image_dir}/{img_info['file_name']}"

        # ç”»åƒã®èª­ã¿è¾¼ã¿
        from PIL import Image
        img = Image.open(img_path).convert('RGB')

        # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®å–å¾—
        anns = self.image_to_annotations.get(image_id, [])

        boxes = []
        labels = []

        for ann in anns:
            # COCOå½¢å¼: [x, y, width, height] â†’ [x_min, y_min, x_max, y_max]
            x, y, w, h = ann['bbox']
            boxes.append([x, y, x + w, y + h])
            labels.append(ann['category_id'])

        # Tensorã«å¤‰æ›
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)

        target = {
            'boxes': boxes,
            'labels': labels,
            'image_id': torch.tensor([image_id])
        }

        # ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
        if self.transforms:
            img = self.transforms(img)

        return img, target

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½¿ç”¨ä¾‹
# dataset = CustomDetectionDataset(
#     image_dir='data/images',
#     annotation_file='data/annotations.json',
#     transforms=torchvision.transforms.ToTensor()
# )
#
# data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))
</code></pre>

<h3>è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®å®Ÿè£…</h3>

<pre><code class="language-python">def train_detection_model(model, data_loader, optimizer, device, epoch):
    """
    ç‰©ä½“æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ï¼ˆ1ã‚¨ãƒãƒƒã‚¯ï¼‰

    Args:
        model: ç‰©ä½“æ¤œå‡ºãƒ¢ãƒ‡ãƒ«
        data_loader: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        optimizer: ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        device: å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹
        epoch: ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯æ•°
    """
    model.train()

    total_loss = 0
    for batch_idx, (images, targets) in enumerate(data_loader):
        # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        # é †ä¼æ’­ï¼ˆtorchvisionã®ãƒ¢ãƒ‡ãƒ«ã¯è¨“ç·´æ™‚ã«lossã‚’è¿”ã™ï¼‰
        loss_dict = model(images, targets)

        # å…¨æå¤±ã®åˆè¨ˆ
        losses = sum(loss for loss in loss_dict.values())

        # é€†ä¼æ’­
        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

        total_loss += losses.item()

        # é€²æ—è¡¨ç¤º
        if batch_idx % 10 == 0:
            print(f'Epoch {epoch}, Batch {batch_idx}/{len(data_loader)}, '
                  f'Loss: {losses.item():.4f}')
            print(f'  Details: {", ".join([f"{k}: {v.item():.4f}" for k, v in loss_dict.items()])}')

    avg_loss = total_loss / len(data_loader)
    print(f'Epoch {epoch} - Average Loss: {avg_loss:.4f}\n')

    return avg_loss

def evaluate_detection_model(model, data_loader, device):
    """
    ç‰©ä½“æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡

    Args:
        model: ç‰©ä½“æ¤œå‡ºãƒ¢ãƒ‡ãƒ«
        data_loader: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        device: å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹

    Returns:
        metrics: è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
    """
    model.eval()

    all_predictions = []
    all_targets = []

    with torch.no_grad():
        for images, targets in data_loader:
            images = [img.to(device) for img in images]

            # æ¨è«–
            predictions = model(images)

            all_predictions.extend([{k: v.cpu() for k, v in p.items()} for p in predictions])
            all_targets.extend([{k: v.cpu() for k, v in t.items()} for t in targets])

    # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    print("è©•ä¾¡çµæœ:")
    print(f"  ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(all_predictions)}")

    # å¹³å‡æ¤œå‡ºæ•°
    avg_detections = sum(len(p['boxes']) for p in all_predictions) / len(all_predictions)
    print(f"  å¹³å‡æ¤œå‡ºæ•°: {avg_detections:.2f}")

    return {'avg_detections': avg_detections}

# è¨“ç·´ã®å®Ÿè¡Œä¾‹
def full_training_pipeline(num_epochs=10):
    """
    å®Œå…¨ãªè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
    model = fasterrcnn_resnet50_fpn(pretrained=True)
    model.to(device)

    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)

    # Learning Rate Scheduler
    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)

    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    for epoch in range(1, num_epochs + 1):
        # è¨“ç·´
        # train_loss = train_detection_model(model, train_loader, optimizer, device, epoch)

        # è©•ä¾¡
        # metrics = evaluate_detection_model(model, val_loader, device)

        # Learning Rateæ›´æ–°
        lr_scheduler.step()

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        # torch.save(model.state_dict(), f'detection_model_epoch_{epoch}.pth')

        print(f"Epoch {epoch} completed.\n")

# ä½¿ç”¨ä¾‹
# full_training_pipeline(num_epochs=10)
</code></pre>

<hr>

<h2>5.6 å®Ÿè·µï¼šCOCOå½¢å¼ãƒ‡ãƒ¼ã‚¿ã§ã®æ¤œå‡º</h2>

<h3>COCOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¦‚è¦</h3>

<p><strong>COCO (Common Objects in Context)</strong>ã¯ã€ç‰©ä½“æ¤œå‡ºã®æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚</p>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>è©³ç´°</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ç”»åƒæ•°</strong></td>
<td>è¨“ç·´: 118Kæšã€æ¤œè¨¼: 5Kæšã€ãƒ†ã‚¹ãƒˆ: 41Kæš</td>
</tr>
<tr>
<td><strong>ã‚¯ãƒ©ã‚¹æ•°</strong></td>
<td>80ã‚¯ãƒ©ã‚¹ï¼ˆperson, car, dog, etc.ï¼‰</td>
</tr>
<tr>
<td><strong>ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³</strong></td>
<td>Bounding Boxã€Segmentationã€Keypoints</td>
</tr>
<tr>
<td><strong>è©•ä¾¡æŒ‡æ¨™</strong></td>
<td>mAP @ IoU=[0.50:0.05:0.95]</td>
</tr>
</tbody>
</table>

<h3>å®Œå…¨ãªç‰©ä½“æ¤œå‡ºãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h3>

<pre><code class="language-python">import torch
import torchvision
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.patches as patches

class ObjectDetectionPipeline:
    """
    ç‰©ä½“æ¤œå‡ºã®å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    """

    def __init__(self, num_classes, pretrained=True, device=None):
        """
        Args:
            num_classes: æ¤œå‡ºã‚¯ãƒ©ã‚¹æ•°ï¼ˆèƒŒæ™¯ã‚’å«ã‚€ï¼‰
            pretrained: äº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ä½¿ç”¨
            device: å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹
        """
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.num_classes = num_classes

        # ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
        self.model = self._build_model(pretrained)
        self.model.to(self.device)

        print(f"ç‰©ä½“æ¤œå‡ºãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
        print(f"  ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        print(f"  ã‚¯ãƒ©ã‚¹æ•°: {num_classes}")

    def _build_model(self, pretrained):
        """ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰"""
        model = fasterrcnn_resnet50_fpn(pretrained=pretrained)

        # æœ€çµ‚å±¤ã‚’ç½®ãæ›ãˆ
        in_features = model.roi_heads.box_predictor.cls_score.in_features
        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, self.num_classes)

        return model

    def predict(self, image_path, conf_threshold=0.5, nms_threshold=0.5):
        """
        ç”»åƒã‹ã‚‰ç‰©ä½“ã‚’æ¤œå‡º

        Args:
            image_path: å…¥åŠ›ç”»åƒãƒ‘ã‚¹
            conf_threshold: ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®é–¾å€¤
            nms_threshold: NMSã®IoUé–¾å€¤

        Returns:
            detections: æ¤œå‡ºçµæœã®è¾æ›¸
        """
        # ç”»åƒã®èª­ã¿è¾¼ã¿
        img = Image.open(image_path).convert('RGB')
        img_tensor = torchvision.transforms.ToTensor()(img).unsqueeze(0).to(self.device)

        # æ¨è«–
        self.model.eval()
        with torch.no_grad():
            predictions = self.model(img_tensor)

        # å¾Œå‡¦ç†
        pred = predictions[0]

        # NMSï¼ˆtorchvisionã®ãƒ¢ãƒ‡ãƒ«ã¯å†…éƒ¨ã§NMSã‚’å®Ÿè¡Œã™ã‚‹ãŒã€è¿½åŠ ã§é©ç”¨å¯èƒ½ï¼‰
        keep = torchvision.ops.nms(pred['boxes'], pred['scores'], nms_threshold)

        # é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        keep = keep[pred['scores'][keep] > conf_threshold]

        detections = {
            'boxes': pred['boxes'][keep].cpu().numpy(),
            'labels': pred['labels'][keep].cpu().numpy(),
            'scores': pred['scores'][keep].cpu().numpy()
        }

        return detections, img

    def visualize(self, image, detections, class_names, save_path=None):
        """
        æ¤œå‡ºçµæœã®å¯è¦–åŒ–

        Args:
            image: PIL Image
            detections: predict()ã®è¿”ã‚Šå€¤
            class_names: ã‚¯ãƒ©ã‚¹åã®ãƒªã‚¹ãƒˆ
            save_path: ä¿å­˜å…ˆãƒ‘ã‚¹ï¼ˆNoneãªã‚‰è¡¨ç¤ºã®ã¿ï¼‰
        """
        fig, ax = plt.subplots(1, figsize=(12, 8))
        ax.imshow(image)

        colors = plt.cm.hsv(np.linspace(0, 1, len(class_names))).tolist()

        for box, label, score in zip(detections['boxes'], detections['labels'], detections['scores']):
            x_min, y_min, x_max, y_max = box
            width = x_max - x_min
            height = y_max - y_min

            color = colors[label % len(colors)]

            # Bounding Box
            rect = patches.Rectangle(
                (x_min, y_min), width, height,
                linewidth=2, edgecolor=color, facecolor='none'
            )
            ax.add_patch(rect)

            # ãƒ©ãƒ™ãƒ«
            label_text = f'{class_names[label]}: {score:.2f}'
            ax.text(
                x_min, y_min - 5,
                label_text,
                bbox=dict(facecolor=color, alpha=0.7),
                fontsize=10, color='white', weight='bold'
            )

        ax.axis('off')
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"çµæœã‚’ä¿å­˜: {save_path}")
        else:
            plt.show()

    def batch_predict(self, image_paths, conf_threshold=0.5):
        """
        ãƒãƒƒãƒæ¨è«–

        Args:
            image_paths: ç”»åƒãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            conf_threshold: ä¿¡é ¼åº¦é–¾å€¤

        Returns:
            all_detections: å„ç”»åƒã®æ¤œå‡ºçµæœãƒªã‚¹ãƒˆ
        """
        all_detections = []

        for img_path in image_paths:
            detections, img = self.predict(img_path, conf_threshold)
            all_detections.append({
                'path': img_path,
                'detections': detections,
                'image': img
            })

        return all_detections

    def evaluate_coco(self, data_loader, coco_gt):
        """
        COCOå½¢å¼ã§ã®è©•ä¾¡

        Args:
            data_loader: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
            coco_gt: COCO Ground Truth ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³

        Returns:
            metrics: è©•ä¾¡æŒ‡æ¨™
        """
        from pycocotools.coco import COCO
        from pycocotools.cocoeval import COCOeval

        self.model.eval()
        coco_results = []

        with torch.no_grad():
            for images, targets in data_loader:
                images = [img.to(self.device) for img in images]
                predictions = self.model(images)

                # COCOå½¢å¼ã«å¤‰æ›
                for target, pred in zip(targets, predictions):
                    image_id = target['image_id'].item()

                    for box, label, score in zip(pred['boxes'], pred['labels'], pred['scores']):
                        x_min, y_min, x_max, y_max = box.tolist()

                        coco_results.append({
                            'image_id': image_id,
                            'category_id': label.item(),
                            'bbox': [x_min, y_min, x_max - x_min, y_max - y_min],
                            'score': score.item()
                        })

        # COCOè©•ä¾¡
        coco_dt = coco_gt.loadRes(coco_results)
        coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')
        coco_eval.evaluate()
        coco_eval.accumulate()
        coco_eval.summarize()

        metrics = {
            'mAP': coco_eval.stats[0],
            'mAP_50': coco_eval.stats[1],
            'mAP_75': coco_eval.stats[2]
        }

        return metrics

# ä½¿ç”¨ä¾‹
if __name__ == '__main__':
    # COCO ã‚¯ãƒ©ã‚¹åï¼ˆç°¡ç•¥ç‰ˆï¼‰
    COCO_CLASSES = [
        '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
        'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',
        'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
        'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag'
        # ... å…¨91ã‚¯ãƒ©ã‚¹
    ]

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–
    pipeline = ObjectDetectionPipeline(num_classes=91, pretrained=True)

    # å˜ä¸€ç”»åƒã®æ¨è«–
    # detections, img = pipeline.predict('test_image.jpg', conf_threshold=0.7)
    # pipeline.visualize(img, detections, COCO_CLASSES, save_path='result.jpg')

    # ãƒãƒƒãƒæ¨è«–
    # image_list = ['img1.jpg', 'img2.jpg', 'img3.jpg']
    # results = pipeline.batch_predict(image_list, conf_threshold=0.6)

    print("ç‰©ä½“æ¤œå‡ºãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æº–å‚™å®Œäº†")
</code></pre>

<hr>

<h2>æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>ç‰©ä½“æ¤œå‡ºã®åŸºç¤</strong></p>
<ul>
<li>Classificationã€Detectionã€Segmentationã®é•ã„</li>
<li>Bounding Boxã¨IoUã®è¨ˆç®—æ–¹æ³•</li>
<li>ç‰©ä½“æ¤œå‡ºã®èª²é¡Œã¨è©•ä¾¡æŒ‡æ¨™</li>
</ul></li>
<li><p><strong>Two-Stage Detectors</strong></p>
<ul>
<li>R-CNNã€Fast R-CNNã€Faster R-CNNã®é€²åŒ–</li>
<li>Region Proposal Networkã®ä»•çµ„ã¿</li>
<li>ç²¾åº¦é‡è¦–ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</li>
</ul></li>
<li><p><strong>One-Stage Detectors</strong></p>
<ul>
<li>YOLOã€SSDã®è¨­è¨ˆæ€æƒ³</li>
<li>é€Ÿåº¦ã¨ç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•</li>
<li>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œå‡ºã®å®Ÿç¾</li>
</ul></li>
<li><p><strong>è©•ä¾¡æŒ‡æ¨™</strong></p>
<ul>
<li>NMSï¼ˆNon-Maximum Suppressionï¼‰ã®å®Ÿè£…</li>
<li>Precision-Recallæ›²ç·šã¨AP</li>
<li>mAPï¼ˆmean Average Precisionï¼‰ã®è¨ˆç®—</li>
</ul></li>
<li><p><strong>å®Ÿè£…ã‚¹ã‚­ãƒ«</strong></p>
<ul>
<li>PyTorch torchvisionã§ã®ç‰©ä½“æ¤œå‡º</li>
<li>è¨“ç·´ã¨è©•ä¾¡ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰</li>
<li>COCOå½¢å¼ãƒ‡ãƒ¼ã‚¿ã®æ‰±ã„æ–¹</li>
</ul></li>
</ol>

<h3>ãƒ¢ãƒ‡ãƒ«é¸æŠã‚¬ã‚¤ãƒ‰</h3>

<table>
<thead>
<tr>
<th>è¦ä»¶</th>
<th>æ¨å¥¨ãƒ¢ãƒ‡ãƒ«</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æœ€é«˜ç²¾åº¦</strong></td>
<td>Faster R-CNN (ResNet-101)</td>
<td>Two-stageã§ç²¾å¯†ãªæ¤œå‡º</td>
</tr>
<tr>
<td><strong>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ </strong></td>
<td>YOLOv5s / YOLOv8</td>
<td>140+ FPSã€è»½é‡</td>
</tr>
<tr>
<td><strong>ãƒãƒ©ãƒ³ã‚¹å‹</strong></td>
<td>YOLOv5m / RetinaNet</td>
<td>é€Ÿåº¦ã¨ç²¾åº¦ã®ä¸¡ç«‹</td>
</tr>
<tr>
<td><strong>ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹</strong></td>
<td>MobileNet SSD</td>
<td>ä½è¨ˆç®—é‡ã€çœãƒ¡ãƒ¢ãƒª</td>
</tr>
<tr>
<td><strong>å°ç‰©ä½“æ¤œå‡º</strong></td>
<td>Faster R-CNN + FPN</td>
<td>Multi-scaleç‰¹å¾´æŠ½å‡º</td>
</tr>
</tbody>
</table>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>IoUè¨ˆç®—é–¢æ•°ã‚’NumPyã§å®Ÿè£…ã—ã€ä»¥ä¸‹ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã§æ¤œè¨¼ã—ã¦ãã ã•ã„ï¼š</p>
<ul>
<li>Box1: [0, 0, 100, 100], Box2: [50, 50, 150, 150] â†’ IoU â‰ˆ 0.143</li>
<li>Box1: [0, 0, 100, 100], Box2: [0, 0, 100, 100] â†’ IoU = 1.0</li>
<li>Box1: [0, 0, 50, 50], Box2: [60, 60, 100, 100] â†’ IoU = 0.0</li>
</ul>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import numpy as np

def calculate_iou_numpy(box1, box2):
    """NumPyã«ã‚ˆã‚‹IoUè¨ˆç®—"""
    # äº¤å·®é ˜åŸŸ
    x_min_inter = max(box1[0], box2[0])
    y_min_inter = max(box1[1], box2[1])
    x_max_inter = min(box1[2], box2[2])
    y_max_inter = min(box1[3], box2[3])

    inter_area = max(0, x_max_inter - x_min_inter) * max(0, y_max_inter - y_min_inter)

    # å„Boxã®é¢ç©
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])

    # IoU
    union_area = box1_area + box2_area - inter_area
    iou = inter_area / union_area if union_area > 0 else 0

    return iou

# ãƒ†ã‚¹ãƒˆ
test_cases = [
    ([0, 0, 100, 100], [50, 50, 150, 150], 0.143),
    ([0, 0, 100, 100], [0, 0, 100, 100], 1.0),
    ([0, 0, 50, 50], [60, 60, 100, 100], 0.0)
]

for box1, box2, expected in test_cases:
    iou = calculate_iou_numpy(box1, box2)
    print(f"Box1: {box1}, Box2: {box2}")
    print(f"  è¨ˆç®—IoU: {iou:.4f}, æœŸå¾…å€¤: {expected:.4f}, ä¸€è‡´: {abs(iou - expected) < 0.001}")
</code></pre>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>NMSï¼ˆNon-Maximum Suppressionï¼‰ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ã‚¼ãƒ­ã‹ã‚‰å®Ÿè£…ã—ã€ä»¥ä¸‹ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§å‹•ä½œç¢ºèªã—ã¦ãã ã•ã„ï¼š</p>
<pre><code>boxes = [[50, 50, 150, 150], [55, 55, 155, 155], [200, 200, 300, 300], [205, 205, 305, 305]]
scores = [0.9, 0.85, 0.95, 0.88]
iou_threshold = 0.5
</code></pre>
<p>æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ [2, 0]ï¼ˆã‚¹ã‚³ã‚¢é †ï¼‰ãŒä¿æŒã•ã‚Œã‚‹</p>

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

<ul>
<li>ã‚¹ã‚³ã‚¢ã®é™é †ã§ã‚½ãƒ¼ãƒˆ</li>
<li>æœ€é«˜ã‚¹ã‚³ã‚¢ã®Boxã‚’ä¿æŒã—ã€é‡è¤‡ã™ã‚‹Boxã‚’é™¤å»</li>
<li>ç¹°ã‚Šè¿”ã—å‡¦ç†ã§å…¨Boxã‚’å‡¦ç†</li>
</ul>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
<p>Faster R-CNNã‚’ä½¿ã£ã¦ã€ã‚«ã‚¹ã‚¿ãƒ ç”»åƒã§ç‰©ä½“æ¤œå‡ºã‚’å®Ÿè¡Œã—ã€çµæœã‚’å¯è¦–åŒ–ã—ã¦ãã ã•ã„ã€‚æ¤œå‡ºã•ã‚ŒãŸç‰©ä½“ã®ã‚¯ãƒ©ã‚¹åã¨ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤ºã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import torch
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from PIL import Image
import torchvision.transforms as T

# ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
model = fasterrcnn_resnet50_fpn(pretrained=True)
model.eval()

# ç”»åƒã®èª­ã¿è¾¼ã¿
img = Image.open('your_image.jpg').convert('RGB')
img_tensor = T.ToTensor()(img).unsqueeze(0)

# æ¨è«–
with torch.no_grad():
    predictions = model(img_tensor)

# çµæœã®è¡¨ç¤º
pred = predictions[0]
for i, (box, label, score) in enumerate(zip(pred['boxes'], pred['labels'], pred['scores'])):
    if score > 0.5:
        print(f"æ¤œå‡º {i+1}: ã‚¯ãƒ©ã‚¹={COCO_CLASSES[label]}, ã‚¹ã‚³ã‚¢={score:.3f}, Box={box.tolist()}")

# å¯è¦–åŒ–
# visualize_bounding_boxesé–¢æ•°ã‚’ä½¿ç”¨
</code></pre>

</details>

<h3>å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
<p>YOLOv5ã‚’ä½¿ã£ã¦ã€å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã¾ãŸã¯Webã‚«ãƒ¡ãƒ©ï¼‰ã‹ã‚‰ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ç‰©ä½“æ¤œå‡ºã‚’è¡Œã†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚æ¤œå‡ºçµæœã‚’ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«è¡¨ç¤ºã—ã€FPSã‚‚è¨ˆæ¸¬ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

<ul>
<li>OpenCVã§å‹•ç”»ã‚’èª­ã¿è¾¼ã‚€ï¼ˆcv2.VideoCaptureï¼‰</li>
<li>å„ãƒ•ãƒ¬ãƒ¼ãƒ ã§YOLOv5æ¨è«–ã‚’å®Ÿè¡Œ</li>
<li>time.time()ã§FPSã‚’è¨ˆæ¸¬</li>
<li>çµæœã‚’cv2.imshow()ã§è¡¨ç¤º</li>
</ul>

</details>

<hr>

<h2>å‚è€ƒæ–‡çŒ®</h2>

<ol>
<li>Girshick, R., et al. (2014). "Rich feature hierarchies for accurate object detection and semantic segmentation." <em>CVPR</em>.</li>
<li>Girshick, R. (2015). "Fast R-CNN." <em>ICCV</em>.</li>
<li>Ren, S., et al. (2016). "Faster R-CNN: Towards real-time object detection with region proposal networks." <em>TPAMI</em>.</li>
<li>Redmon, J., et al. (2016). "You only look once: Unified, real-time object detection." <em>CVPR</em>.</li>
<li>Liu, W., et al. (2016). "SSD: Single shot multibox detector." <em>ECCV</em>.</li>
<li>Lin, T.-Y., et al. (2014). "Microsoft COCO: Common objects in context." <em>ECCV</em>.</li>
<li>Lin, T.-Y., et al. (2017). "Focal loss for dense object detection." <em>ICCV</em>. (RetinaNet)</li>
<li>Jocher, G., et al. (2022). "YOLOv5: State-of-the-art object detection." <em>Ultralytics</em>.</li>
</ol>

<div class="navigation">
    <a href="chapter4-augmentation-optimization.html" class="nav-button">â† å‰ã®ç« : æœ€æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a>
    <a href="../index.html" class="nav-button">æ©Ÿæ¢°å­¦ç¿’ãƒˆãƒƒãƒ— â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
