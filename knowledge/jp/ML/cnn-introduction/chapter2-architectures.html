<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬2ç« ï¼šãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã¨CNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
        <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/wp/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/ML/cnn-introduction/index.html">Cnn</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 2</span>
        </div>
    </nav>

    <header>
        <div class="header-content">
            <h1>ç¬¬2ç« ï¼šãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã¨CNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h1>
            <p class="subtitle">ä»£è¡¨çš„ãªCNNãƒ¢ãƒ‡ãƒ«ã®é€²åŒ–ã¨è¨­è¨ˆåŸç†ã‚’ç†è§£ã™ã‚‹</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´šã€œä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 10å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã®å½¹å‰²ã¨ç¨®é¡ï¼ˆMax Pooling, Average Poolingï¼‰ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›ã¨ä½ç½®ä¸å¤‰æ€§ã®ç²å¾—ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>âœ… LeNet-5ã‹ã‚‰ResNetã¾ã§ã®ä»£è¡¨çš„ãªCNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é€²åŒ–ã‚’æŠŠæ¡ã™ã‚‹</li>
<li>âœ… Batch Normalizationã®åŸç†ã¨åŠ¹æœã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Dropoutã‚’ç”¨ã„ãŸéå­¦ç¿’é˜²æ­¢æ‰‹æ³•ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Skip Connectionsï¼ˆResidual Connectionsï¼‰ã®é‡è¦æ€§ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… CIFAR-10ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å®Ÿè·µçš„ãªç”»åƒåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
</ul>

<hr>

<h2>2.1 ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã®å½¹å‰²</h2>

<h3>ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã¨ã¯</h3>

<p><strong>ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ï¼ˆPooling Layerï¼‰</strong>ã¯ã€ç•³ã¿è¾¼ã¿å±¤ã®å‡ºåŠ›ã‚’<strong>ç©ºé–“çš„ã«ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</strong>ã™ã‚‹å±¤ã§ã™ã€‚ä¸»ãªç›®çš„ã¯ä»¥ä¸‹ã®3ã¤ã§ã™ï¼š</p>

<ul>
<li><strong>æ¬¡å…ƒå‰Šæ¸›</strong>ï¼šç‰¹å¾´ãƒãƒƒãƒ—ã®ã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã—ã€è¨ˆç®—é‡ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›</li>
<li><strong>ä½ç½®ä¸å¤‰æ€§</strong>ï¼šç‰¹å¾´ã®å¾®å°ãªä½ç½®å¤‰åŒ–ã«å¯¾ã—ã¦é ‘å¥æ€§ã‚’ç²å¾—</li>
<li><strong>å—å®¹é‡ã®æ‹¡å¤§</strong>ï¼šã‚ˆã‚Šåºƒã„ç¯„å›²ã®æƒ…å ±ã‚’çµ±åˆ</li>
</ul>

<blockquote>
<p>ã€Œãƒ—ãƒ¼ãƒªãƒ³ã‚°ã¯ã€ç”»åƒã®é‡è¦ãªç‰¹å¾´ã‚’ä¿ã¡ãªãŒã‚‰ã€ä¸è¦ãªè©³ç´°ã‚’æ¨ã¦ã‚‹æ“ä½œã€</p>
</blockquote>

<h3>Max Pooling vs Average Pooling</h3>

<p>ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã«ã¯ä¸»ã«2ã¤ã®ç¨®é¡ãŒã‚ã‚Šã¾ã™ï¼š</p>

<table>
<thead>
<tr>
<th>ç¨®é¡</th>
<th>å‹•ä½œ</th>
<th>ç‰¹å¾´</th>
<th>ä½¿ç”¨å ´é¢</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Max Pooling</strong></td>
<td>é ˜åŸŸå†…ã®æœ€å¤§å€¤ã‚’å–ã‚‹</td>
<td>æœ€ã‚‚å¼·ã„ç‰¹å¾´ã‚’ä¿æŒ</td>
<td>ç‰©ä½“æ¤œå‡ºã€ä¸€èˆ¬çš„ãªç”»åƒåˆ†é¡</td>
</tr>
<tr>
<td><strong>Average Pooling</strong></td>
<td>é ˜åŸŸå†…ã®å¹³å‡å€¤ã‚’å–ã‚‹</td>
<td>å…¨ä½“çš„ãªç‰¹å¾´ã‚’ä¿æŒ</td>
<td>Global Average Poolingã€ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³</td>
</tr>
</tbody>
</table>

<div class="mermaid">
graph LR
    A["å…¥åŠ›ç‰¹å¾´ãƒãƒƒãƒ—<br/>4Ã—4"] --> B["Max Pooling<br/>2Ã—2, stride=2"]
    A --> C["Average Pooling<br/>2Ã—2, stride=2"]

    B --> D["å‡ºåŠ›<br/>2Ã—2<br/>æœ€å¤§å€¤ã‚’ä¿æŒ"]
    C --> E["å‡ºåŠ›<br/>2Ã—2<br/>å¹³å‡å€¤ã‚’ä¿æŒ"]

    style A fill:#e1f5ff
    style B fill:#b3e5fc
    style C fill:#81d4fa
    style D fill:#4fc3f7
    style E fill:#29b6f6
</div>

<h3>Max Poolingã®å‹•ä½œä¾‹</h3>

<pre><code class="language-python">import numpy as np
import torch
import torch.nn as nn

# å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆ1ãƒãƒ£ãƒ³ãƒãƒ«ã€4Ã—4ï¼‰
input_data = torch.tensor([[
    [1.0, 3.0, 2.0, 4.0],
    [5.0, 6.0, 1.0, 2.0],
    [7.0, 2.0, 8.0, 3.0],
    [1.0, 4.0, 6.0, 9.0]
]], dtype=torch.float32).unsqueeze(0)  # (1, 1, 4, 4)

print("å…¥åŠ›ç‰¹å¾´ãƒãƒƒãƒ—:")
print(input_data.squeeze().numpy())

# Max Pooling (2Ã—2, stride=2)
max_pool = nn.MaxPool2d(kernel_size=2, stride=2)
output_max = max_pool(input_data)

print("\nMax Pooling (2Ã—2) ã®å‡ºåŠ›:")
print(output_max.squeeze().numpy())

# Average Pooling (2Ã—2, stride=2)
avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)
output_avg = avg_pool(input_data)

print("\nAverage Pooling (2Ã—2) ã®å‡ºåŠ›:")
print(output_avg.squeeze().numpy())

# æ‰‹å‹•è¨ˆç®—ã«ã‚ˆã‚‹ç¢ºèªï¼ˆå·¦ä¸Šã®é ˜åŸŸï¼‰
print("\næ‰‹å‹•è¨ˆç®—ï¼ˆå·¦ä¸Šã®2Ã—2é ˜åŸŸï¼‰:")
region = input_data[0, 0, 0:2, 0:2].numpy()
print(f"é ˜åŸŸ: \n{region}")
print(f"Max: {region.max()}")
print(f"Average: {region.mean()}")
</code></pre>

<h3>ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã®åŠ¹æœï¼šä½ç½®ä¸å¤‰æ€§</h3>

<pre><code class="language-python">import torch
import torch.nn as nn

# å…ƒã®ç‰¹å¾´ãƒãƒƒãƒ—
original = torch.tensor([[
    [0, 0, 1, 0],
    [0, 1, 0, 0],
    [0, 0, 0, 0],
    [0, 0, 0, 0]
]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # (1, 1, 4, 4)

# å¾®å°ã«ã‚·ãƒ•ãƒˆã—ãŸç‰¹å¾´ãƒãƒƒãƒ—
shifted = torch.tensor([[
    [0, 1, 0, 0],
    [0, 0, 1, 0],
    [0, 0, 0, 0],
    [0, 0, 0, 0]
]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # (1, 1, 4, 4)

max_pool = nn.MaxPool2d(kernel_size=2, stride=2)

print("å…ƒã®ç‰¹å¾´ãƒãƒƒãƒ—ã®Max Pooling:")
print(max_pool(original).squeeze().numpy())

print("\nã‚·ãƒ•ãƒˆã—ãŸç‰¹å¾´ãƒãƒƒãƒ—ã®Max Pooling:")
print(max_pool(shifted).squeeze().numpy())

print("\nâ†’ ä½ç½®ãŒå¾®å°ã«å¤‰åŒ–ã—ã¦ã‚‚ã€Max Poolingã®å‡ºåŠ›ã¯åŒã˜é ˜åŸŸã«1ãŒå‡ºç¾")
print("  ã“ã‚ŒãŒã€Œä½ç½®ä¸å¤‰æ€§ã€ã§ã™")
</code></pre>

<h3>ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</h3>

<ul>
<li><strong>kernel_size</strong>ï¼šãƒ—ãƒ¼ãƒªãƒ³ã‚°é ˜åŸŸã®ã‚µã‚¤ã‚ºï¼ˆé€šå¸¸2Ã—2ã¾ãŸã¯3Ã—3ï¼‰</li>
<li><strong>stride</strong>ï¼šã‚¹ãƒ©ã‚¤ãƒ‰å¹…ï¼ˆé€šå¸¸kernel_sizeã¨åŒã˜ã§ã€é‡è¤‡ãªã—ï¼‰</li>
<li><strong>padding</strong>ï¼šã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆé€šå¸¸0ï¼‰</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn

# ç•°ãªã‚‹ãƒ—ãƒ¼ãƒªãƒ³ã‚°è¨­å®šã®æ¯”è¼ƒ
input_data = torch.randn(1, 1, 8, 8)  # (batch, channels, height, width)

# è¨­å®š1: 2Ã—2, stride=2ï¼ˆæ¨™æº–ï¼‰
pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
output1 = pool1(input_data)

# è¨­å®š2: 3Ã—3, stride=2ï¼ˆé‡è¤‡ã‚ã‚Šï¼‰
pool2 = nn.MaxPool2d(kernel_size=3, stride=2)
output2 = pool2(input_data)

# è¨­å®š3: 2Ã—2, stride=1ï¼ˆé«˜é‡è¤‡ï¼‰
pool3 = nn.MaxPool2d(kernel_size=2, stride=1)
output3 = pool3(input_data)

print(f"å…¥åŠ›ã‚µã‚¤ã‚º: {input_data.shape}")
print(f"2Ã—2, stride=2 å‡ºåŠ›ã‚µã‚¤ã‚º: {output1.shape}")
print(f"3Ã—3, stride=2 å‡ºåŠ›ã‚µã‚¤ã‚º: {output2.shape}")
print(f"2Ã—2, stride=1 å‡ºåŠ›ã‚µã‚¤ã‚º: {output3.shape}")

# æ¬¡å…ƒå‰Šæ¸›ç‡ã®è¨ˆç®—
reduction1 = (input_data.numel() - output1.numel()) / input_data.numel() * 100
reduction2 = (input_data.numel() - output2.numel()) / input_data.numel() * 100

print(f"\n2Ã—2, stride=2 ã®æ¬¡å…ƒå‰Šæ¸›ç‡: {reduction1:.1f}%")
print(f"3Ã—3, stride=2 ã®æ¬¡å…ƒå‰Šæ¸›ç‡: {reduction2:.1f}%")
</code></pre>

<h3>Global Average Pooling</h3>

<p><strong>Global Average Pooling (GAP)</strong>ã¯ã€ç‰¹å¾´ãƒãƒƒãƒ—å…¨ä½“ã®å¹³å‡ã‚’å–ã‚‹ç‰¹æ®Šãªãƒ—ãƒ¼ãƒªãƒ³ã‚°ã§ã™ã€‚ç¾ä»£ã®CNNã§ã¯ã€æœ€çµ‚å±¤ã®å…¨çµåˆå±¤ã®ä»£ã‚ã‚Šã«ä½¿ã‚ã‚Œã‚‹ã“ã¨ãŒå¤šããªã£ã¦ã„ã¾ã™ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# å…¥åŠ›: (batch_size, channels, height, width)
input_features = torch.randn(2, 512, 7, 7)  # 2ã‚µãƒ³ãƒ—ãƒ«ã€512ãƒãƒ£ãƒ³ãƒãƒ«ã€7Ã—7

# Global Average Pooling
gap = nn.AdaptiveAvgPool2d((1, 1))  # å‡ºåŠ›ã‚µã‚¤ã‚ºã‚’(1, 1)ã«æŒ‡å®š
output = gap(input_features)

print(f"å…¥åŠ›ã‚µã‚¤ã‚º: {input_features.shape}")
print(f"GAPå‡ºåŠ›ã‚µã‚¤ã‚º: {output.shape}")

# å¹³å¦åŒ–
output_flat = output.view(output.size(0), -1)
print(f"å¹³å¦åŒ–å¾Œ: {output_flat.shape}")

# GAPã®ãƒ¡ãƒªãƒƒãƒˆ
print("\nGlobal Average Poolingã®åˆ©ç‚¹:")
print("1. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚¼ãƒ­ï¼ˆå…¨çµåˆå±¤ã¨æ¯”è¼ƒï¼‰")
print("2. å…¥åŠ›ã‚µã‚¤ã‚ºã«ä¾å­˜ã—ãªã„ï¼ˆä»»æ„ã®ã‚µã‚¤ã‚ºã«å¯¾å¿œï¼‰")
print("3. éå­¦ç¿’ã®ãƒªã‚¹ã‚¯ä½æ¸›")
print("4. å„ãƒãƒ£ãƒ³ãƒãƒ«ã®ç©ºé–“çš„å¹³å‡ï¼ãã®ãƒãƒ£ãƒ³ãƒãƒ«ãŒè¡¨ã™æ¦‚å¿µã®å¼·åº¦")
</code></pre>

<hr>

<h2>2.2 ä»£è¡¨çš„ãªCNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h2>

<h3>CNNã®é€²åŒ–ï¼šæ­´å²çš„æ¦‚è¦³</h3>

<div class="mermaid">
graph LR
    A[LeNet-5<br/>1998] --> B[AlexNet<br/>2012]
    B --> C[VGGNet<br/>2014]
    C --> D[GoogLeNet<br/>2014]
    D --> E[ResNet<br/>2015]
    E --> F[DenseNet<br/>2017]
    F --> G[EfficientNet<br/>2019]
    G --> H[Vision Transformer<br/>2020+]

    style A fill:#e1f5ff
    style B fill:#b3e5fc
    style C fill:#81d4fa
    style D fill:#4fc3f7
    style E fill:#29b6f6
    style F fill:#03a9f4
    style G fill:#039be5
    style H fill:#0288d1
</div>

<h3>LeNet-5 (1998): CNNã®åŸç‚¹</h3>

<p><strong>LeNet-5</strong>ã¯ã€Yann LeCunã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸã€æ‰‹æ›¸ãæ•°å­—èªè­˜ï¼ˆMNISTï¼‰ã®ãŸã‚ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ç¾ä»£ã®CNNã®åŸºç¤ã¨ãªã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚</p>

<table>
<thead>
<tr>
<th>å±¤</th>
<th>å‡ºåŠ›ã‚µã‚¤ã‚º</th>
<th>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°</th>
</tr>
</thead>
<tbody>
<tr>
<td>å…¥åŠ›</td>
<td>1Ã—28Ã—28</td>
<td>-</td>
</tr>
<tr>
<td>Conv1 (5Ã—5, 6ch)</td>
<td>6Ã—24Ã—24</td>
<td>156</td>
</tr>
<tr>
<td>AvgPool (2Ã—2)</td>
<td>6Ã—12Ã—12</td>
<td>0</td>
</tr>
<tr>
<td>Conv2 (5Ã—5, 16ch)</td>
<td>16Ã—8Ã—8</td>
<td>2,416</td>
</tr>
<tr>
<td>AvgPool (2Ã—2)</td>
<td>16Ã—4Ã—4</td>
<td>0</td>
</tr>
<tr>
<td>FC1 (120)</td>
<td>120</td>
<td>30,840</td>
</tr>
<tr>
<td>FC2 (84)</td>
<td>84</td>
<td>10,164</td>
</tr>
<tr>
<td>FC3 (10)</td>
<td>10</td>
<td>850</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class LeNet5(nn.Module):
    def __init__(self, num_classes=10):
        super(LeNet5, self).__init__()

        # ç‰¹å¾´æŠ½å‡ºå±¤
        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)    # 28Ã—28 â†’ 24Ã—24
        self.pool1 = nn.AvgPool2d(kernel_size=2)        # 24Ã—24 â†’ 12Ã—12
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)   # 12Ã—12 â†’ 8Ã—8
        self.pool2 = nn.AvgPool2d(kernel_size=2)        # 8Ã—8 â†’ 4Ã—4

        # åˆ†é¡å±¤
        self.fc1 = nn.Linear(16 * 4 * 4, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, num_classes)

    def forward(self, x):
        # ç‰¹å¾´æŠ½å‡º
        x = F.relu(self.conv1(x))
        x = self.pool1(x)
        x = F.relu(self.conv2(x))
        x = self.pool2(x)

        # å¹³å¦åŒ–
        x = x.view(x.size(0), -1)

        # åˆ†é¡
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)

        return x

# ãƒ¢ãƒ‡ãƒ«ä½œæˆã¨ã‚µãƒãƒªãƒ¼
model = LeNet5(num_classes=10)
print(model)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®è¨ˆç®—
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"\nç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
print(f"å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}")

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
x = torch.randn(1, 1, 28, 28)
output = model(x)
print(f"\nå…¥åŠ›ã‚µã‚¤ã‚º: {x.shape}")
print(f"å‡ºåŠ›ã‚µã‚¤ã‚º: {output.shape}")
</code></pre>

<h3>AlexNet (2012): ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®å¹•é–‹ã‘</h3>

<p><strong>AlexNet</strong>ã¯ã€2012å¹´ã®ImageNetç«¶æŠ€ä¼šã§åœ§å€’çš„ãªæ€§èƒ½ã‚’ç¤ºã—ã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ–ãƒ¼ãƒ ã®ç«ä»˜ã‘å½¹ã¨ãªã‚Šã¾ã—ãŸã€‚</p>

<p>ä¸»ãªç‰¹å¾´ï¼š</p>
<ul>
<li><strong>ReLUæ´»æ€§åŒ–é–¢æ•°</strong>ã®ä½¿ç”¨ï¼ˆSigmoidã‚ˆã‚Šé«˜é€Ÿã«å­¦ç¿’ï¼‰</li>
<li><strong>Dropout</strong>ã«ã‚ˆã‚‹éå­¦ç¿’é˜²æ­¢</li>
<li><strong>Data Augmentation</strong>ã®æ´»ç”¨</li>
<li><strong>GPUä¸¦åˆ—å‡¦ç†</strong>ã®æ´»ç”¨</li>
<li><strong>Local Response Normalization</strong>ï¼ˆç¾åœ¨ã¯ã‚ã¾ã‚Šä½¿ã‚ã‚Œãªã„ï¼‰</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn

class AlexNet(nn.Module):
    def __init__(self, num_classes=1000):
        super(AlexNet, self).__init__()

        # ç‰¹å¾´æŠ½å‡ºå±¤
        self.features = nn.Sequential(
            # Conv1: 96 filters, 11Ã—11, stride=4
            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),

            # Conv2: 256 filters, 5Ã—5
            nn.Conv2d(96, 256, kernel_size=5, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),

            # Conv3: 384 filters, 3Ã—3
            nn.Conv2d(256, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),

            # Conv4: 384 filters, 3Ã—3
            nn.Conv2d(384, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),

            # Conv5: 256 filters, 3Ã—3
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
        )

        # åˆ†é¡å±¤
        self.classifier = nn.Sequential(
            nn.Dropout(p=0.5),
            nn.Linear(256 * 6 * 6, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)  # å¹³å¦åŒ–
        x = self.classifier(x)
        return x

# ãƒ¢ãƒ‡ãƒ«ä½œæˆ
model = AlexNet(num_classes=1000)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
total_params = sum(p.numel() for p in model.parameters())
print(f"AlexNet ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")

# å„å±¤ã®ã‚µã‚¤ã‚ºç¢ºèª
x = torch.randn(1, 3, 224, 224)
print(f"\nå…¥åŠ›: {x.shape}")

for i, layer in enumerate(model.features):
    x = layer(x)
    if isinstance(layer, (nn.Conv2d, nn.MaxPool2d)):
        print(f"Layer {i} ({layer.__class__.__name__}): {x.shape}")
</code></pre>

<h3>VGGNet (2014): ã‚·ãƒ³ãƒ—ãƒ«ã•ã®ç¾å­¦</h3>

<p><strong>VGGNet</strong>ã¯ã€3Ã—3ã®å°ã•ãªãƒ•ã‚£ãƒ«ã‚¿ã‚’ç¹°ã‚Šè¿”ã—ä½¿ã†ã‚·ãƒ³ãƒ—ãƒ«ãªè¨­è¨ˆã§ã€æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ‰åŠ¹æ€§ã‚’ç¤ºã—ã¾ã—ãŸã€‚</p>

<p>è¨­è¨ˆåŸå‰‡ï¼š</p>
<ul>
<li><strong>3Ã—3ãƒ•ã‚£ãƒ«ã‚¿ã®ã¿</strong>ä½¿ç”¨ï¼ˆå°ã•ã„ãƒ•ã‚£ãƒ«ã‚¿ã‚’é‡ã­ã‚‹æ–¹ãŒåŠ¹ç‡çš„ï¼‰</li>
<li><strong>2Ã—2 Max Pooling</strong>ã§æ®µéšçš„ã«ã‚µã‚¤ã‚ºã‚’åŠæ¸›</li>
<li><strong>ãƒãƒ£ãƒ³ãƒãƒ«æ•°ã‚’å€å¢—</strong>ã•ã›ãªãŒã‚‰æ·±ãã™ã‚‹ï¼ˆ64 â†’ 128 â†’ 256 â†’ 512ï¼‰</li>
<li>VGG-16ï¼ˆ16å±¤ï¼‰ã¨VGG-19ï¼ˆ19å±¤ï¼‰ãŒæœ‰å</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn

class VGGBlock(nn.Module):
    """VGGã®åŸºæœ¬ãƒ–ãƒ­ãƒƒã‚¯ï¼šConv â†’ ReLU ã‚’ç¹°ã‚Šè¿”ã™"""
    def __init__(self, in_channels, out_channels, num_convs):
        super(VGGBlock, self).__init__()

        layers = []
        for i in range(num_convs):
            layers.append(nn.Conv2d(
                in_channels if i == 0 else out_channels,
                out_channels,
                kernel_size=3,
                padding=1
            ))
            layers.append(nn.ReLU(inplace=True))

        self.block = nn.Sequential(*layers)

    def forward(self, x):
        return self.block(x)

class VGG16(nn.Module):
    def __init__(self, num_classes=1000):
        super(VGG16, self).__init__()

        # ç‰¹å¾´æŠ½å‡ºéƒ¨åˆ†
        self.features = nn.Sequential(
            VGGBlock(3, 64, 2),      # Block 1: 64 channels, 2 convs
            nn.MaxPool2d(2, 2),

            VGGBlock(64, 128, 2),    # Block 2: 128 channels, 2 convs
            nn.MaxPool2d(2, 2),

            VGGBlock(128, 256, 3),   # Block 3: 256 channels, 3 convs
            nn.MaxPool2d(2, 2),

            VGGBlock(256, 512, 3),   # Block 4: 512 channels, 3 convs
            nn.MaxPool2d(2, 2),

            VGGBlock(512, 512, 3),   # Block 5: 512 channels, 3 convs
            nn.MaxPool2d(2, 2),
        )

        # åˆ†é¡éƒ¨åˆ†
        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

# ãƒ¢ãƒ‡ãƒ«ä½œæˆ
model = VGG16(num_classes=1000)
total_params = sum(p.numel() for p in model.parameters())
print(f"VGG-16 ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")

# ãªãœ3Ã—3ãƒ•ã‚£ãƒ«ã‚¿ã‚’2å›é‡ã­ã‚‹ã®ã‹ï¼Ÿ
print("\n3Ã—3ãƒ•ã‚£ãƒ«ã‚¿ã‚’2å› vs 5Ã—5ãƒ•ã‚£ãƒ«ã‚¿ã‚’1å›:")
print("å—å®¹é‡: åŒã˜5Ã—5")
print("ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 3Ã—3Ã—2 = 18 < 5Ã—5 = 25")
print("éç·šå½¢æ€§: 2å›ã®ReLU > 1å›ã®ReLUï¼ˆè¡¨ç¾åŠ›ãŒé«˜ã„ï¼‰")
</code></pre>

<h3>ResNet (2015): Skip Connectionsã®é©å‘½</h3>

<p><strong>ResNetï¼ˆResidual Networkï¼‰</strong>ã¯ã€<strong>Skip Connectionsï¼ˆæ®‹å·®æ¥ç¶šï¼‰</strong>ã‚’å°å…¥ã—ã€éå¸¸ã«æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆ100å±¤ä»¥ä¸Šï¼‰ã®å­¦ç¿’ã‚’å¯èƒ½ã«ã—ã¾ã—ãŸã€‚</p>

<p>å•é¡Œï¼šæ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã»ã©æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã¯ãšãŒã€å®Ÿéš›ã«ã¯<strong>å‹¾é…æ¶ˆå¤±å•é¡Œ</strong>ã§å­¦ç¿’ãŒå›°é›£ã«ã€‚</p>

<p>è§£æ±ºç­–ï¼š<strong>Residual Block</strong>ã‚’å°å…¥</p>

<div class="mermaid">
graph TD
    A["å…¥åŠ› x"] --> B["Conv + ReLU"]
    B --> C["Conv"]
    A --> D["Identity<br/>ï¼ˆãã®ã¾ã¾ï¼‰"]
    C --> E["åŠ ç®— +"]
    D --> E
    E --> F["ReLU"]
    F --> G["å‡ºåŠ›"]

    style A fill:#e1f5ff
    style D fill:#fff9c4
    style E fill:#c8e6c9
    style G fill:#4fc3f7
</div>

<p>æ•°å¼è¡¨ç¾ï¼š</p>
$$
\mathbf{y} = F(\mathbf{x}) + \mathbf{x}
$$

<p>ã“ã“ã§ã€$F(\mathbf{x})$ã¯æ®‹å·®é–¢æ•°ï¼ˆå­¦ç¿’ã™ã‚‹éƒ¨åˆ†ï¼‰ã€$\mathbf{x}$ã¯ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆæ¥ç¶šã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class ResidualBlock(nn.Module):
    """ResNetã®åŸºæœ¬ãƒ–ãƒ­ãƒƒã‚¯"""
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(ResidualBlock, self).__init__()

        # Main path
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,
                               stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)

        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        # Shortcut pathï¼ˆå…¥å‡ºåŠ›ã®ãƒãƒ£ãƒ³ãƒãƒ«æ•°ãŒé•ã†å ´åˆã®èª¿æ•´ï¼‰
        self.downsample = downsample

    def forward(self, x):
        identity = x

        # Main path
        out = self.conv1(x)
        out = self.bn1(out)
        out = F.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        # Shortcut connection
        if self.downsample is not None:
            identity = self.downsample(x)

        # åŠ ç®—
        out += identity
        out = F.relu(out)

        return out

class SimpleResNet(nn.Module):
    """ç°¡æ˜“ç‰ˆResNetï¼ˆCIFAR-10ç”¨ï¼‰"""
    def __init__(self, num_classes=10):
        super(SimpleResNet, self).__init__()

        # åˆæœŸå±¤
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)

        # Residual blocks
        self.layer1 = self._make_layer(64, 64, num_blocks=2, stride=1)
        self.layer2 = self._make_layer(64, 128, num_blocks=2, stride=2)
        self.layer3 = self._make_layer(128, 256, num_blocks=2, stride=2)

        # åˆ†é¡å±¤
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(256, num_classes)

    def _make_layer(self, in_channels, out_channels, num_blocks, stride):
        downsample = None
        if stride != 1 or in_channels != out_channels:
            downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1,
                         stride=stride, bias=False),
                nn.BatchNorm2d(out_channels),
            )

        layers = []
        layers.append(ResidualBlock(in_channels, out_channels, stride, downsample))

        for _ in range(1, num_blocks):
            layers.append(ResidualBlock(out_channels, out_channels))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)

        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        return x

# ãƒ¢ãƒ‡ãƒ«ä½œæˆ
model = SimpleResNet(num_classes=10)
total_params = sum(p.numel() for p in model.parameters())
print(f"ResNet ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")

# Skip Connectionã®åŠ¹æœã‚’å¯è¦–åŒ–
x = torch.randn(1, 3, 32, 32)
output = model(x)
print(f"\nå…¥åŠ›: {x.shape} â†’ å‡ºåŠ›: {output.shape}")
</code></pre>

<h3>ãªãœSkip ConnectionsãŒæœ‰åŠ¹ãªã®ã‹</h3>

<details>
<summary><strong>Skip Connectionsã®ç†è«–çš„èƒŒæ™¯</strong></summary>

<p>å¾“æ¥ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã¯ã€æ·±ãã™ã‚‹ã¨å‹¾é…æ¶ˆå¤±å•é¡ŒãŒç™ºç”Ÿï¼š</p>

$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x}
$$

<p>å±¤ãŒæ·±ã„ã¨ã€$\frac{\partial y}{\partial x}$ãŒä½•åº¦ã‚‚æ›ã‘ç®—ã•ã‚Œã€å‹¾é…ãŒæ¶ˆå¤±ã€‚</p>

<p>Skip Connectionsã§ã¯ï¼š</p>

$$
\frac{\partial}{\partial x}(F(x) + x) = \frac{\partial F(x)}{\partial x} + 1
$$

<p>ã€Œ+1ã€ã®é …ãŒã‚ã‚‹ãŸã‚ã€å‹¾é…ãŒå¿…ãšæµã‚Œã‚‹ï¼</p>

<p>ã•ã‚‰ã«ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€Œæ’ç­‰å†™åƒã‚’å­¦ç¿’ã™ã‚‹ã ã‘ã§ã‚ˆã„ã€â†’ å­¦ç¿’ãŒå®¹æ˜“ã«ã€‚</p>

</details>

<hr>

<h2>2.3 Batch Normalization</h2>

<h3>Batch Normalizationã¨ã¯</h3>

<p><strong>Batch Normalization (BN)</strong>ã¯ã€å„ãƒŸãƒ‹ãƒãƒƒãƒã®å‡ºåŠ›ã‚’æ­£è¦åŒ–ã—ã€å­¦ç¿’ã‚’å®‰å®šåŒ–ã•ã›ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<p>å„å±¤ã®å‡ºåŠ›ã‚’ã€ãƒŸãƒ‹ãƒãƒƒãƒå…¨ä½“ã§å¹³å‡0ã€åˆ†æ•£1ã«æ­£è¦åŒ–ï¼š</p>

$$
\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
$$

$$
y_i = \gamma \hat{x}_i + \beta
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$\mu_B, \sigma_B^2$: ãƒŸãƒ‹ãƒãƒƒãƒã®å¹³å‡ã¨åˆ†æ•£</li>
<li>$\gamma, \beta$: å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ã¨ã‚·ãƒ•ãƒˆï¼‰</li>
<li>$\epsilon$: æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã®å°ã•ãªå€¤ï¼ˆä¾‹ï¼š1e-5ï¼‰</li>
</ul>

<h3>Batch Normalizationã®åŠ¹æœ</h3>

<ul>
<li><strong>å­¦ç¿’ã®é«˜é€ŸåŒ–</strong>ï¼šã‚ˆã‚Šå¤§ããªå­¦ç¿’ç‡ã‚’ä½¿ãˆã‚‹</li>
<li><strong>å‹¾é…ã®å®‰å®šåŒ–</strong>ï¼šInternal Covariate Shiftã‚’æŠ‘åˆ¶</li>
<li><strong>æ­£å‰‡åŒ–åŠ¹æœ</strong>ï¼šDropoutã®å¿…è¦æ€§ãŒæ¸›ã‚‹</li>
<li><strong>åˆæœŸå€¤ã¸ã®ä¾å­˜ä½æ¸›</strong>ï¼šé‡ã¿åˆæœŸåŒ–ãŒç°¡å˜ã«</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class ConvBlockWithoutBN(nn.Module):
    """Batch Normalizationãªã—"""
    def __init__(self, in_channels, out_channels):
        super(ConvBlockWithoutBN, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)

    def forward(self, x):
        return F.relu(self.conv(x))

class ConvBlockWithBN(nn.Module):
    """Batch Normalizationã‚ã‚Š"""
    def __init__(self, in_channels, out_channels):
        super(ConvBlockWithBN, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)
        self.bn = nn.BatchNorm2d(out_channels)

    def forward(self, x):
        return F.relu(self.bn(self.conv(x)))

# æ¯”è¼ƒå®Ÿé¨“
x = torch.randn(32, 3, 32, 32)  # ãƒãƒƒãƒã‚µã‚¤ã‚º32

# BNãªã—
block_without_bn = ConvBlockWithoutBN(3, 64)
output_without_bn = block_without_bn(x)

# BNã‚ã‚Š
block_with_bn = ConvBlockWithBN(3, 64)
output_with_bn = block_with_bn(x)

print("=== Batch Normalizationã®åŠ¹æœ ===")
print(f"BNãªã— - å¹³å‡: {output_without_bn.mean():.4f}, æ¨™æº–åå·®: {output_without_bn.std():.4f}")
print(f"BNã‚ã‚Š - å¹³å‡: {output_with_bn.mean():.4f}, æ¨™æº–åå·®: {output_with_bn.std():.4f}")

# åˆ†å¸ƒã®å¯è¦–åŒ–
print("\nå„ãƒãƒ£ãƒ³ãƒãƒ«ã®çµ±è¨ˆé‡ï¼ˆBNã‚ã‚Šï¼‰:")
for i in range(min(5, output_with_bn.size(1))):
    channel_data = output_with_bn[:, i, :, :]
    print(f"  Channel {i}: å¹³å‡={channel_data.mean():.4f}, æ¨™æº–åå·®={channel_data.std():.4f}")
</code></pre>

<h3>Batch Normalizationã®é…ç½®</h3>

<p>BNã¯é€šå¸¸ã€<strong>Conv â†’ BN â†’ Activation</strong>ã®é †åºã§é…ç½®ã•ã‚Œã¾ã™ï¼š</p>

<pre><code class="language-python">import torch.nn as nn

# æ¨å¥¨ã•ã‚Œã‚‹é †åº
class StandardConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(StandardConvBlock, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False)
        self.bn = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv(x)    # 1. ç•³ã¿è¾¼ã¿
        x = self.bn(x)      # 2. Batch Normalization
        x = self.relu(x)    # 3. æ´»æ€§åŒ–é–¢æ•°
        return x

# æ³¨æ„: Convã«bias=Falseã‚’æŒ‡å®š
# ç†ç”±: BNãŒå¹³å‡ã‚’0ã«ã™ã‚‹ãŸã‚ã€biasã¯ä¸è¦
block = StandardConvBlock(3, 64)
print("Conv â†’ BN â†’ ReLU ã®é †åº")
print(block)
</code></pre>

<hr>

<h2>2.4 Dropoutã«ã‚ˆã‚‹éå­¦ç¿’é˜²æ­¢</h2>

<h3>Dropoutã¨ã¯</h3>

<p><strong>Dropout</strong>ã¯ã€è¨“ç·´æ™‚ã«ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’ç„¡åŠ¹åŒ–ï¼ˆdropoutï¼‰ã™ã‚‹ã“ã¨ã§ã€éå­¦ç¿’ã‚’é˜²ãæ­£å‰‡åŒ–æ‰‹æ³•ã§ã™ã€‚</p>

<ul>
<li>è¨“ç·´æ™‚ï¼šç¢ºç‡$p$ã§ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’0ã«ã™ã‚‹</li>
<li>ãƒ†ã‚¹ãƒˆæ™‚ï¼šã™ã¹ã¦ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’ä½¿ç”¨ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚ã‚Šï¼‰</li>
</ul>

<div class="mermaid">
graph TD
    A["è¨“ç·´æ™‚"] --> B["å…¨ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³"]
    B --> C["50%ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ‰ãƒ­ãƒƒãƒ—"]
    C --> D["æ®‹ã‚Š50%ã§å­¦ç¿’"]

    E["ãƒ†ã‚¹ãƒˆæ™‚"] --> F["å…¨ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’ä½¿ç”¨"]
    F --> G["é‡ã¿ã‚’0.5å€ã«ã‚¹ã‚±ãƒ¼ãƒ«"]

    style A fill:#e1f5ff
    style E fill:#c8e6c9
    style D fill:#b3e5fc
    style G fill:#81d4fa
</div>

<h3>ãªãœDropoutãŒåŠ¹æœçš„ãªã®ã‹</h3>

<ul>
<li><strong>ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åŠ¹æœ</strong>ï¼šæ¯å›ç•°ãªã‚‹ã‚µãƒ–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’è¨“ç·´â†’è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡ã«è¿‘ã„</li>
<li><strong>å…±é©å¿œã®é˜²æ­¢</strong>ï¼šç‰¹å®šã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã«ä¾å­˜ã—ãªã„é ‘å¥ãªè¡¨ç¾ã‚’å­¦ç¿’</li>
<li><strong>æ­£å‰‡åŒ–</strong>ï¼šãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•ã‚’æŠ‘åˆ¶</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

# Dropoutã®å‹•ä½œã‚’ç¢ºèª
x = torch.ones(4, 10)  # å…¨ã¦1ã®ãƒ†ãƒ³ã‚½ãƒ«

dropout = nn.Dropout(p=0.5)  # 50%ã®ç¢ºç‡ã§ãƒ‰ãƒ­ãƒƒãƒ—

# è¨“ç·´ãƒ¢ãƒ¼ãƒ‰
dropout.train()
print("=== è¨“ç·´ãƒ¢ãƒ¼ãƒ‰ï¼ˆDropoutæœ‰åŠ¹ï¼‰ ===")
for i in range(3):
    output = dropout(x)
    print(f"è©¦è¡Œ {i+1}: {output[0, :5].numpy()}")  # æœ€åˆã®5è¦ç´ ã‚’è¡¨ç¤º

# è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰
dropout.eval()
print("\n=== è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ï¼ˆDropoutç„¡åŠ¹ï¼‰ ===")
output = dropout(x)
print(f"å‡ºåŠ›: {output[0, :5].numpy()}")
</code></pre>

<h3>CNNã§ã®Dropoutã®ä½¿ã„æ–¹</h3>

<p>CNNã§ã¯ã€é€šå¸¸<strong>å…¨çµåˆå±¤ã®å‰</strong>ã«Dropoutã‚’é…ç½®ã—ã¾ã™ã€‚ç•³ã¿è¾¼ã¿å±¤ã«ã¯ã‚ã¾ã‚Šä½¿ã„ã¾ã›ã‚“ã€‚</p>

<pre><code class="language-python">import torch.nn as nn

class CNNWithDropout(nn.Module):
    def __init__(self, num_classes=10):
        super(CNNWithDropout, self).__init__()

        # ç•³ã¿è¾¼ã¿å±¤ï¼ˆDropoutãªã—ï¼‰
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),

            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )

        # å…¨çµåˆå±¤ï¼ˆDropoutã‚ã‚Šï¼‰
        self.classifier = nn.Sequential(
            nn.Linear(128 * 8 * 8, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5),  # Dropout

            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5),  # Dropout

            nn.Linear(256, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

model = CNNWithDropout(num_classes=10)
print(model)

# Dropoutã®åŠ¹æœã‚’å®Ÿé¨“
model.train()
x = torch.randn(2, 3, 32, 32)
output1 = model(x)
output2 = model(x)
print(f"\nè¨“ç·´ãƒ¢ãƒ¼ãƒ‰: åŒã˜å…¥åŠ›ã§ã‚‚å‡ºåŠ›ãŒç•°ãªã‚‹ = {not torch.allclose(output1, output2)}")

model.eval()
output3 = model(x)
output4 = model(x)
print(f"è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰: åŒã˜å…¥åŠ›ã§åŒã˜å‡ºåŠ› = {torch.allclose(output3, output4)}")
</code></pre>

<h3>Dropout vs Batch Normalization</h3>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>Dropout</th>
<th>Batch Normalization</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ä¸»ãªç›®çš„</strong></td>
<td>éå­¦ç¿’é˜²æ­¢</td>
<td>å­¦ç¿’ã®å®‰å®šåŒ–ãƒ»é«˜é€ŸåŒ–</td>
</tr>
<tr>
<td><strong>ä½¿ç”¨ç®‡æ‰€</strong></td>
<td>å…¨çµåˆå±¤</td>
<td>ç•³ã¿è¾¼ã¿å±¤</td>
</tr>
<tr>
<td><strong>è¨“ç·´/ãƒ†ã‚¹ãƒˆ</strong></td>
<td>å‹•ä½œãŒç•°ãªã‚‹</td>
<td>å‹•ä½œãŒç•°ãªã‚‹</td>
</tr>
<tr>
<td><strong>ä½µç”¨</strong></td>
<td>å¯èƒ½ï¼ˆãŸã ã—BNãŒã‚ã‚Œã°ä¸è¦ãªå ´åˆã‚‚ï¼‰</td>
<td>-</td>
</tr>
</tbody>
</table>

<blockquote>
<p><strong>ç¾ä»£ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</strong>ï¼šç•³ã¿è¾¼ã¿å±¤ã«ã¯Batch Normalizationã€å…¨çµåˆå±¤ã«ã¯Dropoutï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ãŸã ã—ã€BNã®æ­£å‰‡åŒ–åŠ¹æœã«ã‚ˆã‚Šã€DropoutãŒä¸è¦ã«ãªã‚‹ã‚±ãƒ¼ã‚¹ã‚‚å¤šã„ã§ã™ã€‚</p>
</blockquote>

<hr>

<h2>2.5 å®Ÿè·µï¼šCIFAR-10ç”»åƒåˆ†é¡</h2>

<h3>CIFAR-10ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</h3>

<p><strong>CIFAR-10</strong>ã¯ã€10ã‚¯ãƒ©ã‚¹ã®32Ã—32ã‚«ãƒ©ãƒ¼ç”»åƒï¼ˆ60,000æšï¼‰ã‹ã‚‰ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ï¼š</p>

<ul>
<li>ã‚¯ãƒ©ã‚¹: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck</li>
<li>è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 50,000æš</li>
<li>ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: 10,000æš</li>
</ul>

<h3>å®Œå…¨ãªCNNåˆ†é¡å™¨ã®å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

# ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã¨æ­£è¦åŒ–
transform_train = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
train_dataset = datasets.CIFAR10(root='./data', train=True,
                                 download=True, transform=transform_train)
test_dataset = datasets.CIFAR10(root='./data', train=False,
                                download=True, transform=transform_test)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)

print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_dataset)}æš")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_dataset)}æš")

# ã‚¯ãƒ©ã‚¹å
classes = ('plane', 'car', 'bird', 'cat', 'deer',
           'dog', 'frog', 'horse', 'ship', 'truck')
</code></pre>

<h3>ãƒ¢ãƒ€ãƒ³ãªCNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h3>

<pre><code class="language-python">import torch.nn as nn
import torch.nn.functional as F

class CIFAR10Net(nn.Module):
    """CIFAR-10ç”¨ã®ãƒ¢ãƒ€ãƒ³ãªCNN"""
    def __init__(self, num_classes=10):
        super(CIFAR10Net, self).__init__()

        # Block 1
        self.conv1 = nn.Conv2d(3, 64, 3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)

        # Block 2
        self.conv2 = nn.Conv2d(64, 128, 3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(128)

        # Block 3
        self.conv3 = nn.Conv2d(128, 256, 3, padding=1, bias=False)
        self.bn3 = nn.BatchNorm2d(256)
        self.conv4 = nn.Conv2d(256, 256, 3, padding=1, bias=False)
        self.bn4 = nn.BatchNorm2d(256)

        # Block 4
        self.conv5 = nn.Conv2d(256, 512, 3, padding=1, bias=False)
        self.bn5 = nn.BatchNorm2d(512)
        self.conv6 = nn.Conv2d(512, 512, 3, padding=1, bias=False)
        self.bn6 = nn.BatchNorm2d(512)

        # Global Average Pooling
        self.gap = nn.AdaptiveAvgPool2d((1, 1))

        # åˆ†é¡å±¤
        self.fc = nn.Linear(512, num_classes)

        # Dropout
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        # Block 1
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.max_pool2d(x, 2)

        # Block 2
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.max_pool2d(x, 2)

        # Block 3
        x = F.relu(self.bn3(self.conv3(x)))
        x = F.relu(self.bn4(self.conv4(x)))
        x = F.max_pool2d(x, 2)

        # Block 4
        x = F.relu(self.bn5(self.conv5(x)))
        x = F.relu(self.bn6(self.conv6(x)))

        # Global Average Pooling
        x = self.gap(x)
        x = x.view(x.size(0), -1)

        # Dropout + åˆ†é¡
        x = self.dropout(x)
        x = self.fc(x)

        return x

# ãƒ¢ãƒ‡ãƒ«ä½œæˆ
model = CIFAR10Net(num_classes=10).to(device)
print(model)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
total_params = sum(p.numel() for p in model.parameters())
print(f"\nç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
</code></pre>

<h3>è¨“ç·´ãƒ«ãƒ¼ãƒ—</h3>

<pre><code class="language-python">import torch.optim as optim

# æå¤±é–¢æ•°ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)

def train_epoch(model, loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in loader:
        inputs, labels = inputs.to(device), labels.to(device)

        # Forward
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Backward
        loss.backward()
        optimizer.step()

        # çµ±è¨ˆ
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    epoch_loss = running_loss / len(loader)
    epoch_acc = 100. * correct / total
    return epoch_loss, epoch_acc

def test_epoch(model, loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    epoch_loss = running_loss / len(loader)
    epoch_acc = 100. * correct / total
    return epoch_loss, epoch_acc

# è¨“ç·´å®Ÿè¡Œ
num_epochs = 50
best_acc = 0

print("\nè¨“ç·´é–‹å§‹...")
for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
    test_loss, test_acc = test_epoch(model, test_loader, criterion, device)

    scheduler.step()

    if (epoch + 1) % 5 == 0 or epoch == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}]")
        print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
        print(f"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%")

    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    if test_acc > best_acc:
        best_acc = test_acc
        torch.save(model.state_dict(), 'best_cifar10_model.pth')

print(f"\nè¨“ç·´å®Œäº†ï¼ãƒ™ã‚¹ãƒˆç²¾åº¦: {best_acc:.2f}%")
</code></pre>

<h3>äºˆæ¸¬ã¨å¯è¦–åŒ–</h3>

<pre><code class="language-python">import matplotlib.pyplot as plt
import numpy as np

def imshow(img, title=None):
    """ç”»åƒè¡¨ç¤ºç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"""
    img = img.numpy().transpose((1, 2, 0))
    mean = np.array([0.4914, 0.4822, 0.4465])
    std = np.array([0.2023, 0.1994, 0.2010])
    img = std * img + mean
    img = np.clip(img, 0, 1)
    plt.imshow(img)
    if title:
        plt.title(title)
    plt.axis('off')

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«å–å¾—
dataiter = iter(test_loader)
images, labels = next(dataiter)
images, labels = images.to(device), labels.to(device)

# äºˆæ¸¬
model.eval()
with torch.no_grad():
    outputs = model(images)
    _, predicted = outputs.max(1)

# æœ€åˆã®8æšã‚’è¡¨ç¤º
fig, axes = plt.subplots(2, 4, figsize=(12, 6))
for i, ax in enumerate(axes.flat):
    imshow(images[i].cpu(), title=f"True: {classes[labels[i]]}\nPred: {classes[predicted[i]]}")
    ax.imshow(images[i].cpu().numpy().transpose((1, 2, 0)))

plt.tight_layout()
plt.savefig('cifar10_predictions.png', dpi=150, bbox_inches='tight')
print("äºˆæ¸¬çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: cifar10_predictions.png")
</code></pre>

<hr>

<h2>2.6 ãƒ¢ãƒ€ãƒ³ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ¦‚è¦³</h2>

<h3>EfficientNet (2019): åŠ¹ç‡çš„ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°</h3>

<p><strong>EfficientNet</strong>ã¯ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ·±ã•ãƒ»å¹…ãƒ»è§£åƒåº¦ã‚’<strong>ãƒãƒ©ãƒ³ã‚¹ã‚ˆãã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°</strong>ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ã¾ã—ãŸã€‚</p>

<p>Compound Scaling:</p>
$$
\text{depth} = \alpha^\phi, \quad \text{width} = \beta^\phi, \quad \text{resolution} = \gamma^\phi
$$

<p>åˆ¶ç´„æ¡ä»¶ï¼š$\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2$</p>

<ul>
<li>å°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§é«˜ç²¾åº¦ã‚’é”æˆ</li>
<li>Mobile Inverted Bottleneck Convolution (MBConv)ã‚’ä½¿ç”¨</li>
<li>EfficientNet-B0ã‹ã‚‰B7ã¾ã§ã€ç²¾åº¦ã¨ã‚µã‚¤ã‚ºã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³</li>
</ul>

<h3>Vision Transformer (2020+): CNNã‚’è¶…ãˆã‚‹</h3>

<p><strong>Vision Transformer (ViT)</strong>ã¯ã€ç”»åƒã‚’<strong>ãƒ‘ãƒƒãƒã«åˆ†å‰²</strong>ã—ã€Transformerã§å‡¦ç†ã™ã‚‹æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã™ã€‚</p>

<div class="mermaid">
graph LR
    A["ç”»åƒ<br/>224Ã—224"] --> B["ãƒ‘ãƒƒãƒåˆ†å‰²<br/>16Ã—16ãƒ‘ãƒƒãƒ"]
    B --> C["Linear Projection"]
    C --> D["Transformer Encoder"]
    D --> E["åˆ†é¡ãƒ˜ãƒƒãƒ‰"]
    E --> F["ã‚¯ãƒ©ã‚¹äºˆæ¸¬"]

    style A fill:#e1f5ff
    style D fill:#b3e5fc
    style F fill:#4fc3f7
</div>

<p>ç‰¹å¾´ï¼š</p>
<ul>
<li>CNNã®å¸°ç´çš„ãƒã‚¤ã‚¢ã‚¹ï¼ˆå±€æ‰€æ€§ã€å¹³è¡Œç§»å‹•ä¸å¤‰æ€§ï¼‰ã‚’æ¨ã¦ã‚‹</li>
<li>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§CNNã‚’ä¸Šå›ã‚‹æ€§èƒ½</li>
<li>Self-Attentionã§ç”»åƒå…¨ä½“ã®é–¢ä¿‚ã‚’æ‰ãˆã‚‹</li>
<li>ä»Šå¾Œã®ä¸»æµã«ãªã‚‹å¯èƒ½æ€§</li>
</ul>

<h3>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£é¸æŠã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³</h3>

<table>
<thead>
<tr>
<th>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</th>
<th>ç‰¹å¾´</th>
<th>æ¨å¥¨ã‚±ãƒ¼ã‚¹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>LeNet-5</strong></td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ã€è»½é‡</td>
<td>MNISTã€å­¦ç¿’ç›®çš„</td>
</tr>
<tr>
<td><strong>VGGNet</strong></td>
<td>ç†è§£ã—ã‚„ã™ã„æ§‹é€ </td>
<td>è»¢ç§»å­¦ç¿’ã®ãƒ™ãƒ¼ã‚¹ã€æ•™è‚²</td>
</tr>
<tr>
<td><strong>ResNet</strong></td>
<td>æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€å®‰å®š</td>
<td>ä¸€èˆ¬çš„ãªç”»åƒåˆ†é¡ã€æ¨™æº–é¸æŠ</td>
</tr>
<tr>
<td><strong>EfficientNet</strong></td>
<td>åŠ¹ç‡çš„ã€é«˜ç²¾åº¦</td>
<td>ãƒªã‚½ãƒ¼ã‚¹åˆ¶ç´„ã€ãƒ¢ãƒã‚¤ãƒ«</td>
</tr>
<tr>
<td><strong>Vision Transformer</strong></td>
<td>æœ€å…ˆç«¯ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿</td>
<td>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ç ”ç©¶</td>
</tr>
</tbody>
</table>

<hr>

<h2>æ¼”ç¿’å•é¡Œ</h2>

<details>
<summary><strong>æ¼”ç¿’1ï¼šãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã®åŠ¹æœ</strong></summary>

<p>Max Poolingã¨Average Poolingã‚’åŒã˜å…¥åŠ›ã«é©ç”¨ã—ã€å‡ºåŠ›ã®é•ã„ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã©ã®ã‚ˆã†ãªç”»åƒç‰¹å¾´ã«å¯¾ã—ã¦ã€ãã‚Œãã‚ŒãŒæœ‰åˆ©ã§ã—ã‚‡ã†ã‹ï¼Ÿ</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# ã‚¨ãƒƒã‚¸ã‚’å«ã‚€ç‰¹å¾´ãƒãƒƒãƒ—ã‚’ä½œæˆ
edge_feature = torch.tensor([[
    [0, 0, 0, 0, 0, 0],
    [0, 1, 1, 1, 1, 0],
    [0, 1, 0, 0, 1, 0],
    [0, 1, 0, 0, 1, 0],
    [0, 1, 1, 1, 1, 0],
    [0, 0, 0, 0, 0, 0]
]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)

# TODO: Max Poolingã¨Average Poolingã‚’é©ç”¨ã—ã€çµæœã‚’æ¯”è¼ƒ
# ãƒ’ãƒ³ãƒˆ: nn.MaxPool2d ã¨ nn.AvgPool2d ã‚’ä½¿ç”¨
</code></pre>

</details>

<details>
<summary><strong>æ¼”ç¿’2ï¼šResidualãƒ–ãƒ­ãƒƒã‚¯ã®Skip Connection</strong></summary>

<p>Residualãƒ–ãƒ­ãƒƒã‚¯ã§Skip Connectionã‚ã‚Šã¨ãªã—ã‚’æ¯”è¼ƒã—ã€å‹¾é…ã®æµã‚Œã«ã©ã®ã‚ˆã†ãªé•ã„ãŒã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# TODO: Skip Connectionã‚ã‚Šã¨ãªã—ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’å®Ÿè£…
# ãƒ’ãƒ³ãƒˆ: åŒã˜æ§‹é€ ã§ã€Skip Connectionã®æœ‰ç„¡ã®ã¿å¤‰ãˆã‚‹
# å‹¾é…ã‚’æ¯”è¼ƒã™ã‚‹ã«ã¯ã€backward()å¾Œã«gradå±æ€§ã‚’ç¢ºèª
</code></pre>

</details>

<details>
<summary><strong>æ¼”ç¿’3ï¼šBatch Normalizationã®åŠ¹æœ</strong></summary>

<p>åŒã˜ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§Batch Normalizationã‚ã‚Šã¨ãªã—ã‚’è¨“ç·´ã—ã€åæŸé€Ÿåº¦ã¨æœ€çµ‚ç²¾åº¦ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# TODO: BNã‚ã‚Šã¨ãªã—ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
# TODO: MNIST ã¾ãŸã¯ CIFAR-10 ã§è¨“ç·´
# TODO: è¨“ç·´æ›²ç·šã‚’æ¯”è¼ƒ
</code></pre>

</details>

<details>
<summary><strong>æ¼”ç¿’4ï¼šDropoutã®æ­£å‰‡åŒ–åŠ¹æœ</strong></summary>

<p>Dropoutã®ç¢ºç‡ã‚’å¤‰ãˆã¦ï¼ˆp=0.0, 0.3, 0.5, 0.7ï¼‰ã€éå­¦ç¿’ã¸ã®å½±éŸ¿ã‚’èª¿ã¹ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# TODO: ç•°ãªã‚‹Dropoutç¢ºç‡ã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
# TODO: è¨“ç·´èª¤å·®ã¨ãƒ†ã‚¹ãƒˆèª¤å·®ã®å·®ï¼ˆéå­¦ç¿’ã®åº¦åˆã„ï¼‰ã‚’æ¯”è¼ƒ
# ã©ã®Dropoutç¢ºç‡ãŒæœ€é©ã§ã™ã‹ï¼Ÿ
</code></pre>

</details>

<details>
<summary><strong>æ¼”ç¿’5ï¼šCIFAR-10ã§ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¯”è¼ƒ</strong></summary>

<p>LeNet-5ã€VGG-styleã€ResNet-styleã®3ã¤ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’CIFAR-10ã§æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# TODO: 3ã¤ã®ç•°ãªã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å®Ÿè£…
# TODO: åŒã˜è¨“ç·´è¨­å®šã§æ€§èƒ½ã‚’æ¯”è¼ƒ
# TODO: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€è¨“ç·´æ™‚é–“ã€ç²¾åº¦ã‚’è¨˜éŒ²

# è©•ä¾¡æŒ‡æ¨™:
# - ãƒ†ã‚¹ãƒˆç²¾åº¦
# - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
# - è¨“ç·´æ™‚é–“ï¼ˆ1ã‚¨ãƒãƒƒã‚¯ã‚ãŸã‚Šï¼‰
# - åæŸã¾ã§ã®ã‚¨ãƒãƒƒã‚¯æ•°
</code></pre>

</details>

<hr>

<h2>ã¾ã¨ã‚</h2>

<p>ã“ã®ç« ã§ã¯ã€ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã¨CNNã®ä»£è¡¨çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã¤ã„ã¦å­¦ã³ã¾ã—ãŸã€‚</p>

<h3>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</h3>

<ul>
<li><strong>ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤</strong>ï¼šæ¬¡å…ƒå‰Šæ¸›ã¨ä½ç½®ä¸å¤‰æ€§ã‚’æä¾›ã€‚Max Poolingã¨Average Poolingã®ä½¿ã„åˆ†ã‘</li>
<li><strong>LeNet-5</strong>ï¼šCNNã®åŸºç¤ã€‚Conv â†’ Pool â†’ FC ã®åŸºæœ¬æ§‹é€ </li>
<li><strong>AlexNet</strong>ï¼šReLUã€Dropoutã€Data Augmentationã®æ´»ç”¨</li>
<li><strong>VGGNet</strong>ï¼š3Ã—3ãƒ•ã‚£ãƒ«ã‚¿ã®ç¹°ã‚Šè¿”ã—ã«ã‚ˆã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªè¨­è¨ˆ</li>
<li><strong>ResNet</strong>ï¼šSkip Connectionsã§å‹¾é…æ¶ˆå¤±ã‚’è§£æ±ºã€100å±¤ä»¥ä¸Šã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒå¯èƒ½ã«</li>
<li><strong>Batch Normalization</strong>ï¼šå­¦ç¿’ã®å®‰å®šåŒ–ã¨é«˜é€ŸåŒ–ã€‚Conv â†’ BN â†’ ReLU ã®é †åº</li>
<li><strong>Dropout</strong>ï¼šéå­¦ç¿’é˜²æ­¢ã€‚å…¨çµåˆå±¤ã«ä½¿ç”¨</li>
<li><strong>Global Average Pooling</strong>ï¼šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›ã€å…¥åŠ›ã‚µã‚¤ã‚ºéä¾å­˜</li>
</ul>

<h3>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h3>

<p>æ¬¡ç« ã§ã¯ã€<strong>è»¢ç§»å­¦ç¿’</strong>ã¨<strong>Fine-tuning</strong>ã«ã¤ã„ã¦å­¦ã³ã¾ã™ã€‚äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ã€å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§é«˜ç²¾åº¦ãªãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹å®Ÿè·µçš„ãªæ‰‹æ³•ã‚’ç¿’å¾—ã—ã¾ã™ã€‚</p>

<div class="navigation">
    <a href="chapter1-cnn-basics.html" class="nav-button">â† ç¬¬1ç« ï¼šCNNã®åŸºç¤</a>
    <a href="chapter3-transfer-learning.html" class="nav-button">ç¬¬3ç« ï¼šè»¢ç§»å­¦ç¿’ã¨Fine-tuning â†’</a>
</div>

</main>


    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
    <p>&copy; 2024 AI Terakoya. All rights reserved.</p>
</footer>

</body>
</html>
