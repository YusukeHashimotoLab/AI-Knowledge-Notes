<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="高度なRAGテクニック - クエリ最適化、リランキング、ハイブリッド検索">
    <title>第3章：高度なRAGテクニック - RAG入門シリーズ</title>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script>
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }
        .container { max-width: 900px; margin: 0 auto; padding: 2rem 1.5rem; }
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        header .container { padding: 0 1.5rem; }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; font-weight: 700; }
        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }
        .meta span { display: inline-flex; align-items: center; gap: 0.3rem; }
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }
        h3 { font-size: 1.4rem; margin-top: 2rem; margin-bottom: 0.8rem; color: var(--primary-color); }
        h4 { font-size: 1.2rem; margin-top: 1.5rem; margin-bottom: 0.6rem; color: var(--primary-color); }
        p { margin-bottom: 1.2rem; }
        a { color: var(--link-color); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--link-hover); text-decoration: underline; }
        ul, ol { margin-left: 2rem; margin-bottom: 1.2rem; }
        li { margin-bottom: 0.5rem; }
        pre {
            background: var(--code-bg);
            padding: 1.2rem;
            border-radius: 6px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border-left: 4px solid var(--secondary-color);
        }
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }
        pre code { background: none; padding: 0; }
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }
        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }
        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }
        .example-box {
            background: #f8f9fa;
            border-left: 4px solid var(--accent-color);
            padding: 1.2rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }
        .note-box {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 1rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }
        @media (max-width: 768px) {
            .container { padding: 1rem; }
            h1 { font-size: 1.6rem; }
            h2 { font-size: 1.4rem; }
            .meta { font-size: 0.85rem; }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>第3章：高度なRAGテクニック</h1>
            <p style="font-size: 1.1rem; margin-top: 0.5rem; opacity: 0.95;">クエリ最適化とリランキング</p>
            <div class="meta">
                <span>📖 学習時間: 30-35分</span>
                <span>📊 難易度: 上級</span>
                <span>💻 コード例: 5個</span>
            </div>
        </div>
    </header>

    <main class="container">
        <h2>1. クエリ最適化</h2>

        <h3>1.1 Query Decomposition（クエリ分解）</h3>
        <p>複雑なクエリを複数のサブクエリに分解し、段階的に検索する手法です。</p>

        <div class="example-box">
            <strong>例:</strong>
            <p>元のクエリ: "2023年と2024年のAI市場規模の比較と将来予測"</p>
            <p>分解後:</p>
            <ol>
                <li>"2023年のAI市場規模"</li>
                <li>"2024年のAI市場規模"</li>
                <li>"AI市場の将来予測"</li>
            </ol>
        </div>

        <h4>実装例1: クエリ分解システム</h4>
        <pre><code>from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from typing import List

class DecomposedQuery(BaseModel):
    """分解されたクエリ"""
    sub_queries: List[str] = Field(description="サブクエリのリスト")
    reasoning: str = Field(description="分解の理由")

class QueryDecomposer:
    """クエリ分解システム"""

    def __init__(self, llm):
        self.llm = llm
        self.parser = PydanticOutputParser(pydantic_object=DecomposedQuery)

    def decompose(self, query: str) -> DecomposedQuery:
        """クエリを分解"""
        template = """以下のクエリを、より単純なサブクエリに分解してください。
        各サブクエリは独立して検索可能である必要があります。

        元のクエリ: {query}

        {format_instructions}
        """

        prompt = ChatPromptTemplate.from_template(template)
        messages = prompt.format_messages(
            query=query,
            format_instructions=self.parser.get_format_instructions()
        )

        response = self.llm(messages)
        result = self.parser.parse(response.content)

        return result

    def search_and_combine(self, query: str, vectorstore, k=3):
        """分解検索と結果統合"""
        # クエリ分解
        decomposed = self.decompose(query)

        # 各サブクエリで検索
        all_results = []
        for sub_query in decomposed.sub_queries:
            results = vectorstore.similarity_search(sub_query, k=k)
            all_results.extend(results)

        # 重複除去（content hashベース）
        unique_results = []
        seen_contents = set()

        for doc in all_results:
            content_hash = hash(doc.page_content)
            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                unique_results.append(doc)

        return {
            'sub_queries': decomposed.sub_queries,
            'reasoning': decomposed.reasoning,
            'results': unique_results
        }

# 使用例
llm = ChatOpenAI(temperature=0, model="gpt-4", openai_api_key="your-api-key")
decomposer = QueryDecomposer(llm)

query = "機械学習と深層学習の違い、およびそれぞれの応用例"
result = decomposer.search_and_combine(query, vectorstore)

print("サブクエリ:")
for i, sq in enumerate(result['sub_queries'], 1):
    print(f"{i}. {sq}")

print(f"\n検索結果: {len(result['results'])}件")</code></pre>

        <h3>1.2 HyDE（Hypothetical Document Embeddings）</h3>
        <p>クエリから仮想的な回答ドキュメントを生成し、それをクエリとして使用する手法です。</p>

        <h4>実装例2: HyDE実装</h4>
        <pre><code>from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate

class HyDERetriever:
    """HyDE検索システム"""

    def __init__(self, llm, vectorstore, embeddings):
        self.llm = llm
        self.vectorstore = vectorstore
        self.embeddings = embeddings

    def generate_hypothetical_document(self, query: str) -> str:
        """仮想ドキュメント生成"""
        template = """以下の質問に対する詳細な回答を書いてください。
        実際の知識がなくても構いません。質問に答える形式で、
        具体的で専門的な内容を含む文章を生成してください。

        質問: {query}

        回答:"""

        prompt = PromptTemplate(template=template, input_variables=["query"])
        response = self.llm(prompt.format(query=query))

        return response.content

    def search_with_hyde(self, query: str, k=5):
        """HyDE検索"""
        # 仮想ドキュメント生成
        hypothetical_doc = self.generate_hypothetical_document(query)

        print(f"仮想ドキュメント:\n{hypothetical_doc[:200]}...\n")

        # 仮想ドキュメントで検索
        results = self.vectorstore.similarity_search(hypothetical_doc, k=k)

        return results

    def hybrid_hyde_search(self, query: str, k=5, alpha=0.5):
        """HyDEと通常検索のハイブリッド

        alpha: HyDEの重み（0=通常検索のみ, 1=HyDEのみ）
        """
        # 通常検索
        normal_results = self.vectorstore.similarity_search_with_score(query, k=k)

        # HyDE検索
        hyde_doc = self.generate_hypothetical_document(query)
        hyde_results = self.vectorstore.similarity_search_with_score(hyde_doc, k=k)

        # スコア統合
        combined_scores = {}

        for doc, score in normal_results:
            doc_id = id(doc)
            combined_scores[doc_id] = {
                'doc': doc,
                'score': (1 - alpha) * score
            }

        for doc, score in hyde_results:
            doc_id = id(doc)
            if doc_id in combined_scores:
                combined_scores[doc_id]['score'] += alpha * score
            else:
                combined_scores[doc_id] = {
                    'doc': doc,
                    'score': alpha * score
                }

        # スコアでソート
        sorted_results = sorted(
            combined_scores.values(),
            key=lambda x: x['score'],
            reverse=True
        )[:k]

        return [item['doc'] for item in sorted_results]

# 使用例
llm = ChatOpenAI(temperature=0.7, model="gpt-3.5-turbo", openai_api_key="your-api-key")
hyde_retriever = HyDERetriever(llm, vectorstore, embeddings)

query = "Transformerモデルの注意機構の数学的原理"
results = hyde_retriever.search_with_hyde(query, k=3)

for i, doc in enumerate(results, 1):
    print(f"{i}. {doc.page_content[:100]}...")</code></pre>

        <h2>2. リランキング</h2>

        <h3>2.1 Cross-Encoderリランキング</h3>
        <p>初回検索結果を、より精度の高いモデルで再評価する手法です。</p>

        <div class="example-box">
            <strong>Bi-Encoder vs Cross-Encoder:</strong>
            <ul>
                <li><strong>Bi-Encoder</strong>: クエリとドキュメントを個別にエンコード（高速、初回検索向け）</li>
                <li><strong>Cross-Encoder</strong>: クエリとドキュメントを同時にエンコード（高精度、リランキング向け）</li>
            </ul>
        </div>

        <h4>実装例3: Cross-Encoderリランキング</h4>
        <pre><code>from sentence_transformers import CrossEncoder
import numpy as np

class ReRanker:
    """リランキングシステム"""

    def __init__(self, model_name='cross-encoder/ms-marco-MiniLM-L-6-v2'):
        self.cross_encoder = CrossEncoder(model_name)

    def rerank(self, query: str, documents: list, top_k: int = 5):
        """Cross-Encoderでリランキング"""
        # クエリとドキュメントのペア作成
        pairs = [[query, doc.page_content] for doc in documents]

        # スコア計算
        scores = self.cross_encoder.predict(pairs)

        # スコアでソート
        scored_docs = list(zip(documents, scores))
        scored_docs.sort(key=lambda x: x[1], reverse=True)

        # Top-K取得
        top_results = scored_docs[:top_k]

        return [
            {
                'document': doc,
                'score': float(score),
                'rank': i + 1
            }
            for i, (doc, score) in enumerate(top_results)
        ]

    def two_stage_retrieval(self, query: str, vectorstore,
                           first_k: int = 20, final_k: int = 5):
        """2段階検索（初回検索→リランキング）"""
        # 第1段階: ベクトル検索で候補取得
        candidates = vectorstore.similarity_search(query, k=first_k)
        print(f"第1段階: {len(candidates)}件取得")

        # 第2段階: Cross-Encoderでリランキング
        reranked = self.rerank(query, candidates, top_k=final_k)
        print(f"第2段階: Top {final_k}件にリランキング")

        return reranked

# 使用例
reranker = ReRanker()

query = "機械学習モデルの評価指標"
results = reranker.two_stage_retrieval(
    query,
    vectorstore,
    first_k=20,
    final_k=5
)

print("\nリランキング結果:")
for result in results:
    print(f"ランク{result['rank']}: スコア {result['score']:.4f}")
    print(f"  {result['document'].page_content[:80]}...\n")</code></pre>

        <h3>2.2 MMR（Maximal Marginal Relevance）</h3>
        <p>関連性と多様性のバランスを取る検索手法です。</p>

        <div class="example-box">
            <strong>MMRアルゴリズム:</strong>
            $$\text{MMR} = \arg\max_{D_i \in R \setminus S} [\lambda \cdot \text{Sim}_1(D_i, Q) - (1-\lambda) \cdot \max_{D_j \in S} \text{Sim}_2(D_i, D_j)]$$
            <p>λ: 関連性と多様性のバランスパラメータ（0-1）</p>
        </div>

        <h4>実装例4: カスタムMMR実装</h4>
        <pre><code>import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class MMRRetriever:
    """MMR検索実装"""

    def __init__(self, embeddings):
        self.embeddings = embeddings

    def mmr_search(self, query: str, documents: list,
                   lambda_param: float = 0.5, k: int = 5):
        """MMR検索

        Args:
            query: 検索クエリ
            documents: 候補ドキュメント
            lambda_param: 関連性の重み（0=多様性重視, 1=関連性重視）
            k: 返すドキュメント数
        """
        # エンベディング取得
        query_emb = self.embeddings.embed_query(query)
        doc_texts = [doc.page_content for doc in documents]
        doc_embs = self.embeddings.embed_documents(doc_texts)

        # クエリとの類似度
        query_similarity = cosine_similarity(
            [query_emb], doc_embs
        )[0]

        # 選択済みドキュメント
        selected_indices = []
        selected_docs = []

        # 最初のドキュメント（最も関連性の高いもの）
        first_idx = np.argmax(query_similarity)
        selected_indices.append(first_idx)
        selected_docs.append(documents[first_idx])

        # k個になるまで繰り返し
        while len(selected_indices) < k:
            mmr_scores = []

            for i, doc in enumerate(documents):
                if i in selected_indices:
                    mmr_scores.append(-np.inf)
                    continue

                # 関連性スコア
                relevance = query_similarity[i]

                # 冗長性スコア（選択済みドキュメントとの最大類似度）
                redundancy = max(
                    cosine_similarity(
                        [doc_embs[i]], [doc_embs[j]]
                    )[0][0]
                    for j in selected_indices
                )

                # MMRスコア
                mmr = lambda_param * relevance - (1 - lambda_param) * redundancy
                mmr_scores.append(mmr)

            # 最大MMRスコアのドキュメント選択
            next_idx = np.argmax(mmr_scores)
            selected_indices.append(next_idx)
            selected_docs.append(documents[next_idx])

        return selected_docs

    def compare_strategies(self, query: str, documents: list, k: int = 5):
        """異なるλ値での比較"""
        strategies = {
            '関連性重視 (λ=0.9)': 0.9,
            'バランス型 (λ=0.5)': 0.5,
            '多様性重視 (λ=0.1)': 0.1
        }

        results = {}
        for name, lambda_val in strategies.items():
            docs = self.mmr_search(query, documents, lambda_param=lambda_val, k=k)
            results[name] = docs

        return results

# 使用例
mmr_retriever = MMRRetriever(embeddings)

# 候補ドキュメント取得
query = "機械学習の評価方法"
candidates = vectorstore.similarity_search(query, k=20)

# 異なる戦略で比較
comparison = mmr_retriever.compare_strategies(query, candidates, k=5)

for strategy_name, docs in comparison.items():
    print(f"\n【{strategy_name}】")
    for i, doc in enumerate(docs, 1):
        print(f"{i}. {doc.page_content[:60]}...")</code></pre>

        <h2>3. ハイブリッド検索</h2>

        <h3>3.1 ベクトル検索とキーワード検索の融合</h3>
        <p>BM25などのキーワード検索とベクトル検索を組み合わせることで、両方の長所を活かします。</p>

        <h4>実装例5: ハイブリッド検索実装</h4>
        <pre><code>from rank_bm25 import BM25Okapi
import numpy as np

class HybridSearcher:
    """ハイブリッド検索システム"""

    def __init__(self, vectorstore, embeddings):
        self.vectorstore = vectorstore
        self.embeddings = embeddings
        self.bm25 = None
        self.documents = []

    def initialize_bm25(self, documents):
        """BM25インデックス初期化"""
        self.documents = documents

        # トークン化
        tokenized_docs = [
            doc.page_content.split() for doc in documents
        ]

        # BM25インデックス作成
        self.bm25 = BM25Okapi(tokenized_docs)
        print(f"BM25インデックス作成: {len(documents)}ドキュメント")

    def bm25_search(self, query: str, k: int = 10):
        """BM25キーワード検索"""
        if not self.bm25:
            raise ValueError("BM25が未初期化")

        tokenized_query = query.split()
        scores = self.bm25.get_scores(tokenized_query)

        # Top-K取得
        top_indices = np.argsort(scores)[::-1][:k]

        results = [
            {
                'document': self.documents[idx],
                'score': float(scores[idx])
            }
            for idx in top_indices
        ]

        return results

    def vector_search(self, query: str, k: int = 10):
        """ベクトル類似度検索"""
        results = self.vectorstore.similarity_search_with_score(query, k=k)

        return [
            {
                'document': doc,
                'score': float(score)
            }
            for doc, score in results
        ]

    def hybrid_search(self, query: str, k: int = 5,
                     vector_weight: float = 0.5):
        """ハイブリッド検索

        Args:
            query: 検索クエリ
            k: 返すドキュメント数
            vector_weight: ベクトル検索の重み（0-1）
        """
        # 両方の検索を実行
        bm25_results = self.bm25_search(query, k=k*2)
        vector_results = self.vector_search(query, k=k*2)

        # スコア正規化
        bm25_scores = [r['score'] for r in bm25_results]
        vector_scores = [r['score'] for r in vector_results]

        bm25_normalized = self._normalize_scores(bm25_scores)
        vector_normalized = self._normalize_scores(vector_scores)

        # スコア統合
        combined_scores = {}

        for i, result in enumerate(bm25_results):
            doc_hash = hash(result['document'].page_content)
            combined_scores[doc_hash] = {
                'document': result['document'],
                'score': (1 - vector_weight) * bm25_normalized[i]
            }

        for i, result in enumerate(vector_results):
            doc_hash = hash(result['document'].page_content)
            if doc_hash in combined_scores:
                combined_scores[doc_hash]['score'] += vector_weight * vector_normalized[i]
            else:
                combined_scores[doc_hash] = {
                    'document': result['document'],
                    'score': vector_weight * vector_normalized[i]
                }

        # ソートしてTop-K取得
        sorted_results = sorted(
            combined_scores.values(),
            key=lambda x: x['score'],
            reverse=True
        )[:k]

        return sorted_results

    def _normalize_scores(self, scores):
        """スコア正規化（0-1範囲）"""
        scores = np.array(scores)
        if scores.max() == scores.min():
            return np.ones_like(scores)
        return (scores - scores.min()) / (scores.max() - scores.min())

    def compare_search_methods(self, query: str, k: int = 5):
        """検索手法の比較"""
        results = {
            'BM25のみ': self.bm25_search(query, k=k),
            'ベクトル検索のみ': self.vector_search(query, k=k),
            'ハイブリッド(50:50)': self.hybrid_search(query, k=k, vector_weight=0.5),
            'ハイブリッド(ベクトル重視)': self.hybrid_search(query, k=k, vector_weight=0.7)
        }

        return results

# 使用例
hybrid_searcher = HybridSearcher(vectorstore, embeddings)

# BM25初期化（全ドキュメント）
all_docs = vectorstore.similarity_search("", k=1000)  # 全取得の代替
hybrid_searcher.initialize_bm25(all_docs)

# 比較検索
query = "機械学習 評価指標 精度"
comparison = hybrid_searcher.compare_search_methods(query, k=3)

for method_name, results in comparison.items():
    print(f"\n【{method_name}】")
    for i, result in enumerate(results, 1):
        print(f"{i}. スコア: {result['score']:.4f}")
        print(f"   {result['document'].page_content[:60]}...")</code></pre>

        <h2>4. コンテキスト圧縮</h2>

        <h3>4.1 コンテキスト圧縮の必要性</h3>
        <p>検索結果をLLMに渡す際、トークン数削減と関連情報抽出のために圧縮します。</p>

        <div class="note-box">
            <strong>圧縮の利点:</strong>
            <ul>
                <li><strong>コスト削減</strong>: トークン数削減によりAPI費用を低減</li>
                <li><strong>精度向上</strong>: ノイズ除去により回答品質が向上</li>
                <li><strong>レスポンス高速化</strong>: 処理時間短縮</li>
            </ul>
        </div>

        <h4>実装例6: コンテキスト圧縮</h4>
        <pre><code>from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.chat_models import ChatOpenAI

class ContextCompressor:
    """コンテキスト圧縮システム"""

    def __init__(self, llm):
        self.llm = llm

    def create_compression_retriever(self, base_retriever):
        """圧縮レトリーバー作成"""
        # LLMベースの抽出器
        compressor = LLMChainExtractor.from_llm(self.llm)

        # 圧縮レトリーバー
        compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=base_retriever
        )

        return compression_retriever

    def extract_relevant_parts(self, query: str, documents: list):
        """関連部分のみ抽出"""
        template = """以下のドキュメントから、質問に関連する部分のみを抽出してください。
        関連しない情報は除外してください。

        質問: {query}

        ドキュメント:
        {document}

        関連部分:"""

        extracted = []
        for doc in documents:
            prompt = template.format(
                query=query,
                document=doc.page_content
            )

            response = self.llm(prompt)
            extracted.append(response.content)

        return extracted

    def summarize_for_context(self, documents: list, max_tokens: int = 500):
        """コンテキスト用要約"""
        combined_text = "\n\n".join([doc.page_content for doc in documents])

        prompt = f"""以下のドキュメント群を{max_tokens}トークン以内に要約してください。
        重要な情報を保持しながら簡潔にまとめてください。

        ドキュメント:
        {combined_text}

        要約:"""

        summary = self.llm(prompt)
        return summary.content

# 使用例
llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo", openai_api_key="your-api-key")
compressor = ContextCompressor(llm)

# 検索結果取得
query = "機械学習モデルの過学習対策"
search_results = vectorstore.similarity_search(query, k=5)

# 関連部分抽出
extracted = compressor.extract_relevant_parts(query, search_results)

print("圧縮前:")
total_chars_before = sum(len(doc.page_content) for doc in search_results)
print(f"総文字数: {total_chars_before}")

print("\n圧縮後:")
total_chars_after = sum(len(text) for text in extracted)
print(f"総文字数: {total_chars_after}")
print(f"圧縮率: {(1 - total_chars_after/total_chars_before)*100:.1f}%")

# 要約版
summary = compressor.summarize_for_context(search_results, max_tokens=300)
print(f"\n要約:\n{summary}")</code></pre>

        <h2>まとめ</h2>
        <ul>
            <li>クエリ分解とHyDEにより検索精度を向上</li>
            <li>Cross-EncoderとMMRで検索結果を最適化</li>
            <li>ハイブリッド検索でキーワードとベクトルの長所を統合</li>
            <li>コンテキスト圧縮でコストと精度を両立</li>
        </ul>

        <div class="nav-buttons">
            <a href="./chapter2-embeddings.html" class="nav-button">← 第2章</a>
            <a href="./chapter4-production.html" class="nav-button">第4章へ →</a>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
