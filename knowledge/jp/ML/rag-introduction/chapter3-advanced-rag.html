<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="é«˜åº¦ãªRAGãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ - ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã€ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢">
    <title>ç¬¬3ç« ï¼šé«˜åº¦ãªRAGãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ - RAGå…¥é–€ã‚·ãƒªãƒ¼ã‚º</title>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script>
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }
        .container { max-width: 900px; margin: 0 auto; padding: 2rem 1.5rem; }
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        header .container { padding: 0 1.5rem; }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; font-weight: 700; }
        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }
        .meta span { display: inline-flex; align-items: center; gap: 0.3rem; }
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }
        h3 { font-size: 1.4rem; margin-top: 2rem; margin-bottom: 0.8rem; color: var(--primary-color); }
        h4 { font-size: 1.2rem; margin-top: 1.5rem; margin-bottom: 0.6rem; color: var(--primary-color); }
        p { margin-bottom: 1.2rem; }
        a { color: var(--link-color); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--link-hover); text-decoration: underline; }
        ul, ol { margin-left: 2rem; margin-bottom: 1.2rem; }
        li { margin-bottom: 0.5rem; }
        pre {
            background: var(--code-bg);
            padding: 1.2rem;
            border-radius: 6px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border-left: 4px solid var(--secondary-color);
        }
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }
        pre code { background: none; padding: 0; }
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }
        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }
        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }
        .example-box {
            background: #f8f9fa;
            border-left: 4px solid var(--accent-color);
            padding: 1.2rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }
        .note-box {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 1rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }
        @media (max-width: 768px) {
            .container { padding: 1rem; }
            h1 { font-size: 1.6rem; }
            h2 { font-size: 1.4rem; }
            .meta { font-size: 0.85rem; }
        }
    
        .feedback-notice {
            background: #fff3cd;
            border: 2px solid #ffc107;
            border-radius: 8px;
            padding: 2rem;
            margin: 3rem auto;
            max-width: 900px;
        }

        .feedback-notice h3 {
            color: #856404;
            font-size: 1.3rem;
            margin-bottom: 1rem;
            text-align: center;
        }

        .feedback-notice p {
            color: #856404;
            font-size: 1rem;
            margin-bottom: 1.5rem;
            text-align: center;
        }

        .feedback-options {
            display: flex;
            justify-content: center;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .feedback-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 600;
            transition: all 0.3s;
        }

        .feedback-button:hover {
            background: #2980b9;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/ML/rag-introduction/chapter3-advanced-rag.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">æ©Ÿæ¢°å­¦ç¿’</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/rag-introduction/index.html">Rag</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

        <header>
        <div class="container">
            <h1>ç¬¬3ç« ï¼šé«˜åº¦ãªRAGãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</h1>
            <p style="font-size: 1.1rem; margin-top: 0.5rem; opacity: 0.95;">ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã¨ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°</p>
            <div class="meta">
                <span>ğŸ“– å­¦ç¿’æ™‚é–“: 30-35åˆ†</span>
                <span>ğŸ“Š é›£æ˜“åº¦: ä¸Šç´š</span>
                <span>ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 5å€‹</span>
            </div>
        </div>
    </header>

    <main class="container">
        <h2>1. ã‚¯ã‚¨ãƒªæœ€é©åŒ–</h2>

        <h3>1.1 Query Decompositionï¼ˆã‚¯ã‚¨ãƒªåˆ†è§£ï¼‰</h3>
        <p>è¤‡é›‘ãªã‚¯ã‚¨ãƒªã‚’è¤‡æ•°ã®ã‚µãƒ–ã‚¯ã‚¨ãƒªã«åˆ†è§£ã—ã€æ®µéšçš„ã«æ¤œç´¢ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

        <div class="example-box">
            <strong>ä¾‹:</strong>
            <p>å…ƒã®ã‚¯ã‚¨ãƒª: "2023å¹´ã¨2024å¹´ã®AIå¸‚å ´è¦æ¨¡ã®æ¯”è¼ƒã¨å°†æ¥äºˆæ¸¬"</p>
            <p>åˆ†è§£å¾Œ:</p>
            <ol>
                <li>"2023å¹´ã®AIå¸‚å ´è¦æ¨¡"</li>
                <li>"2024å¹´ã®AIå¸‚å ´è¦æ¨¡"</li>
                <li>"AIå¸‚å ´ã®å°†æ¥äºˆæ¸¬"</li>
            </ol>
        </div>

        <h4>å®Ÿè£…ä¾‹1: ã‚¯ã‚¨ãƒªåˆ†è§£ã‚·ã‚¹ãƒ†ãƒ </h4>
        <pre><code>from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from typing import List

class DecomposedQuery(BaseModel):
    """åˆ†è§£ã•ã‚ŒãŸã‚¯ã‚¨ãƒª"""
    sub_queries: List[str] = Field(description="ã‚µãƒ–ã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆ")
    reasoning: str = Field(description="åˆ†è§£ã®ç†ç”±")

class QueryDecomposer:
    """ã‚¯ã‚¨ãƒªåˆ†è§£ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, llm):
        self.llm = llm
        self.parser = PydanticOutputParser(pydantic_object=DecomposedQuery)

    def decompose(self, query: str) -> DecomposedQuery:
        """ã‚¯ã‚¨ãƒªã‚’åˆ†è§£"""
        template = """ä»¥ä¸‹ã®ã‚¯ã‚¨ãƒªã‚’ã€ã‚ˆã‚Šå˜ç´”ãªã‚µãƒ–ã‚¯ã‚¨ãƒªã«åˆ†è§£ã—ã¦ãã ã•ã„ã€‚
        å„ã‚µãƒ–ã‚¯ã‚¨ãƒªã¯ç‹¬ç«‹ã—ã¦æ¤œç´¢å¯èƒ½ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

        å…ƒã®ã‚¯ã‚¨ãƒª: {query}

        {format_instructions}
        """

        prompt = ChatPromptTemplate.from_template(template)
        messages = prompt.format_messages(
            query=query,
            format_instructions=self.parser.get_format_instructions()
        )

        response = self.llm(messages)
        result = self.parser.parse(response.content)

        return result

    def search_and_combine(self, query: str, vectorstore, k=3):
        """åˆ†è§£æ¤œç´¢ã¨çµæœçµ±åˆ"""
        # ã‚¯ã‚¨ãƒªåˆ†è§£
        decomposed = self.decompose(query)

        # å„ã‚µãƒ–ã‚¯ã‚¨ãƒªã§æ¤œç´¢
        all_results = []
        for sub_query in decomposed.sub_queries:
            results = vectorstore.similarity_search(sub_query, k=k)
            all_results.extend(results)

        # é‡è¤‡é™¤å»ï¼ˆcontent hashãƒ™ãƒ¼ã‚¹ï¼‰
        unique_results = []
        seen_contents = set()

        for doc in all_results:
            content_hash = hash(doc.page_content)
            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                unique_results.append(doc)

        return {
            'sub_queries': decomposed.sub_queries,
            'reasoning': decomposed.reasoning,
            'results': unique_results
        }

# ä½¿ç”¨ä¾‹
llm = ChatOpenAI(temperature=0, model="gpt-4", openai_api_key="your-api-key")
decomposer = QueryDecomposer(llm)

query = "æ©Ÿæ¢°å­¦ç¿’ã¨æ·±å±¤å­¦ç¿’ã®é•ã„ã€ãŠã‚ˆã³ãã‚Œãã‚Œã®å¿œç”¨ä¾‹"
result = decomposer.search_and_combine(query, vectorstore)

print("ã‚µãƒ–ã‚¯ã‚¨ãƒª:")
for i, sq in enumerate(result['sub_queries'], 1):
    print(f"{i}. {sq}")

print(f"\næ¤œç´¢çµæœ: {len(result['results'])}ä»¶")</code></pre>

        <h3>1.2 HyDEï¼ˆHypothetical Document Embeddingsï¼‰</h3>
        <p>ã‚¯ã‚¨ãƒªã‹ã‚‰ä»®æƒ³çš„ãªå›ç­”ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã—ã€ãã‚Œã‚’ã‚¯ã‚¨ãƒªã¨ã—ã¦ä½¿ç”¨ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

        <h4>å®Ÿè£…ä¾‹2: HyDEå®Ÿè£…</h4>
        <pre><code>from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate

class HyDERetriever:
    """HyDEæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, llm, vectorstore, embeddings):
        self.llm = llm
        self.vectorstore = vectorstore
        self.embeddings = embeddings

    def generate_hypothetical_document(self, query: str) -> str:
        """ä»®æƒ³ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ"""
        template = """ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã™ã‚‹è©³ç´°ãªå›ç­”ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚
        å®Ÿéš›ã®çŸ¥è­˜ãŒãªãã¦ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚è³ªå•ã«ç­”ãˆã‚‹å½¢å¼ã§ã€
        å…·ä½“çš„ã§å°‚é–€çš„ãªå†…å®¹ã‚’å«ã‚€æ–‡ç« ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

        è³ªå•: {query}

        å›ç­”:"""

        prompt = PromptTemplate(template=template, input_variables=["query"])
        response = self.llm(prompt.format(query=query))

        return response.content

    def search_with_hyde(self, query: str, k=5):
        """HyDEæ¤œç´¢"""
        # ä»®æƒ³ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ
        hypothetical_doc = self.generate_hypothetical_document(query)

        print(f"ä»®æƒ³ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:\n{hypothetical_doc[:200]}...\n")

        # ä»®æƒ³ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§æ¤œç´¢
        results = self.vectorstore.similarity_search(hypothetical_doc, k=k)

        return results

    def hybrid_hyde_search(self, query: str, k=5, alpha=0.5):
        """HyDEã¨é€šå¸¸æ¤œç´¢ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰

        alpha: HyDEã®é‡ã¿ï¼ˆ0=é€šå¸¸æ¤œç´¢ã®ã¿, 1=HyDEã®ã¿ï¼‰
        """
        # é€šå¸¸æ¤œç´¢
        normal_results = self.vectorstore.similarity_search_with_score(query, k=k)

        # HyDEæ¤œç´¢
        hyde_doc = self.generate_hypothetical_document(query)
        hyde_results = self.vectorstore.similarity_search_with_score(hyde_doc, k=k)

        # ã‚¹ã‚³ã‚¢çµ±åˆ
        combined_scores = {}

        for doc, score in normal_results:
            doc_id = id(doc)
            combined_scores[doc_id] = {
                'doc': doc,
                'score': (1 - alpha) * score
            }

        for doc, score in hyde_results:
            doc_id = id(doc)
            if doc_id in combined_scores:
                combined_scores[doc_id]['score'] += alpha * score
            else:
                combined_scores[doc_id] = {
                    'doc': doc,
                    'score': alpha * score
                }

        # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        sorted_results = sorted(
            combined_scores.values(),
            key=lambda x: x['score'],
            reverse=True
        )[:k]

        return [item['doc'] for item in sorted_results]

# ä½¿ç”¨ä¾‹
llm = ChatOpenAI(temperature=0.7, model="gpt-3.5-turbo", openai_api_key="your-api-key")
hyde_retriever = HyDERetriever(llm, vectorstore, embeddings)

query = "Transformerãƒ¢ãƒ‡ãƒ«ã®æ³¨æ„æ©Ÿæ§‹ã®æ•°å­¦çš„åŸç†"
results = hyde_retriever.search_with_hyde(query, k=3)

for i, doc in enumerate(results, 1):
    print(f"{i}. {doc.page_content[:100]}...")</code></pre>

        <h2>2. ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°</h2>

        <h3>2.1 Cross-Encoderãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°</h3>
        <p>åˆå›æ¤œç´¢çµæœã‚’ã€ã‚ˆã‚Šç²¾åº¦ã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ã§å†è©•ä¾¡ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

        <div class="example-box">
            <strong>Bi-Encoder vs Cross-Encoder:</strong>
            <ul>
                <li><strong>Bi-Encoder</strong>: ã‚¯ã‚¨ãƒªã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å€‹åˆ¥ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆé«˜é€Ÿã€åˆå›æ¤œç´¢å‘ã‘ï¼‰</li>
                <li><strong>Cross-Encoder</strong>: ã‚¯ã‚¨ãƒªã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åŒæ™‚ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆé«˜ç²¾åº¦ã€ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°å‘ã‘ï¼‰</li>
            </ul>
        </div>

        <h4>å®Ÿè£…ä¾‹3: Cross-Encoderãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°</h4>
        <pre><code>from sentence_transformers import CrossEncoder
import numpy as np

class ReRanker:
    """ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, model_name='cross-encoder/ms-marco-MiniLM-L-6-v2'):
        self.cross_encoder = CrossEncoder(model_name)

    def rerank(self, query: str, documents: list, top_k: int = 5):
        """Cross-Encoderã§ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
        # ã‚¯ã‚¨ãƒªã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒšã‚¢ä½œæˆ
        pairs = [[query, doc.page_content] for doc in documents]

        # ã‚¹ã‚³ã‚¢è¨ˆç®—
        scores = self.cross_encoder.predict(pairs)

        # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        scored_docs = list(zip(documents, scores))
        scored_docs.sort(key=lambda x: x[1], reverse=True)

        # Top-Kå–å¾—
        top_results = scored_docs[:top_k]

        return [
            {
                'document': doc,
                'score': float(score),
                'rank': i + 1
            }
            for i, (doc, score) in enumerate(top_results)
        ]

    def two_stage_retrieval(self, query: str, vectorstore,
                           first_k: int = 20, final_k: int = 5):
        """2æ®µéšæ¤œç´¢ï¼ˆåˆå›æ¤œç´¢â†’ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰"""
        # ç¬¬1æ®µéš: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§å€™è£œå–å¾—
        candidates = vectorstore.similarity_search(query, k=first_k)
        print(f"ç¬¬1æ®µéš: {len(candidates)}ä»¶å–å¾—")

        # ç¬¬2æ®µéš: Cross-Encoderã§ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°
        reranked = self.rerank(query, candidates, top_k=final_k)
        print(f"ç¬¬2æ®µéš: Top {final_k}ä»¶ã«ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°")

        return reranked

# ä½¿ç”¨ä¾‹
reranker = ReRanker()

query = "æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡æŒ‡æ¨™"
results = reranker.two_stage_retrieval(
    query,
    vectorstore,
    first_k=20,
    final_k=5
)

print("\nãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœ:")
for result in results:
    print(f"ãƒ©ãƒ³ã‚¯{result['rank']}: ã‚¹ã‚³ã‚¢ {result['score']:.4f}")
    print(f"  {result['document'].page_content[:80]}...\n")</code></pre>

        <h3>2.2 MMRï¼ˆMaximal Marginal Relevanceï¼‰</h3>
        <p>é–¢é€£æ€§ã¨å¤šæ§˜æ€§ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹æ¤œç´¢æ‰‹æ³•ã§ã™ã€‚</p>

        <div class="example-box">
            <strong>MMRã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ :</strong>
            $$\text{MMR} = \arg\max_{D_i \in R \setminus S} [\lambda \cdot \text{Sim}_1(D_i, Q) - (1-\lambda) \cdot \max_{D_j \in S} \text{Sim}_2(D_i, D_j)]$$
            <p>Î»: é–¢é€£æ€§ã¨å¤šæ§˜æ€§ã®ãƒãƒ©ãƒ³ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ0-1ï¼‰</p>
        </div>

        <h4>å®Ÿè£…ä¾‹4: ã‚«ã‚¹ã‚¿ãƒ MMRå®Ÿè£…</h4>
        <pre><code>import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class MMRRetriever:
    """MMRæ¤œç´¢å®Ÿè£…"""

    def __init__(self, embeddings):
        self.embeddings = embeddings

    def mmr_search(self, query: str, documents: list,
                   lambda_param: float = 0.5, k: int = 5):
        """MMRæ¤œç´¢

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            documents: å€™è£œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
            lambda_param: é–¢é€£æ€§ã®é‡ã¿ï¼ˆ0=å¤šæ§˜æ€§é‡è¦–, 1=é–¢é€£æ€§é‡è¦–ï¼‰
            k: è¿”ã™ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
        """
        # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°å–å¾—
        query_emb = self.embeddings.embed_query(query)
        doc_texts = [doc.page_content for doc in documents]
        doc_embs = self.embeddings.embed_documents(doc_texts)

        # ã‚¯ã‚¨ãƒªã¨ã®é¡ä¼¼åº¦
        query_similarity = cosine_similarity(
            [query_emb], doc_embs
        )[0]

        # é¸æŠæ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        selected_indices = []
        selected_docs = []

        # æœ€åˆã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆæœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ã‚‚ã®ï¼‰
        first_idx = np.argmax(query_similarity)
        selected_indices.append(first_idx)
        selected_docs.append(documents[first_idx])

        # kå€‹ã«ãªã‚‹ã¾ã§ç¹°ã‚Šè¿”ã—
        while len(selected_indices) < k:
            mmr_scores = []

            for i, doc in enumerate(documents):
                if i in selected_indices:
                    mmr_scores.append(-np.inf)
                    continue

                # é–¢é€£æ€§ã‚¹ã‚³ã‚¢
                relevance = query_similarity[i]

                # å†—é•·æ€§ã‚¹ã‚³ã‚¢ï¼ˆé¸æŠæ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã®æœ€å¤§é¡ä¼¼åº¦ï¼‰
                redundancy = max(
                    cosine_similarity(
                        [doc_embs[i]], [doc_embs[j]]
                    )[0][0]
                    for j in selected_indices
                )

                # MMRã‚¹ã‚³ã‚¢
                mmr = lambda_param * relevance - (1 - lambda_param) * redundancy
                mmr_scores.append(mmr)

            # æœ€å¤§MMRã‚¹ã‚³ã‚¢ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé¸æŠ
            next_idx = np.argmax(mmr_scores)
            selected_indices.append(next_idx)
            selected_docs.append(documents[next_idx])

        return selected_docs

    def compare_strategies(self, query: str, documents: list, k: int = 5):
        """ç•°ãªã‚‹Î»å€¤ã§ã®æ¯”è¼ƒ"""
        strategies = {
            'é–¢é€£æ€§é‡è¦– (Î»=0.9)': 0.9,
            'ãƒãƒ©ãƒ³ã‚¹å‹ (Î»=0.5)': 0.5,
            'å¤šæ§˜æ€§é‡è¦– (Î»=0.1)': 0.1
        }

        results = {}
        for name, lambda_val in strategies.items():
            docs = self.mmr_search(query, documents, lambda_param=lambda_val, k=k)
            results[name] = docs

        return results

# ä½¿ç”¨ä¾‹
mmr_retriever = MMRRetriever(embeddings)

# å€™è£œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå–å¾—
query = "æ©Ÿæ¢°å­¦ç¿’ã®è©•ä¾¡æ–¹æ³•"
candidates = vectorstore.similarity_search(query, k=20)

# ç•°ãªã‚‹æˆ¦ç•¥ã§æ¯”è¼ƒ
comparison = mmr_retriever.compare_strategies(query, candidates, k=5)

for strategy_name, docs in comparison.items():
    print(f"\nã€{strategy_name}ã€‘")
    for i, doc in enumerate(docs, 1):
        print(f"{i}. {doc.page_content[:60]}...")</code></pre>

        <h2>3. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢</h2>

        <h3>3.1 ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã®èåˆ</h3>
        <p>BM25ãªã©ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã¨ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ä¸¡æ–¹ã®é•·æ‰€ã‚’æ´»ã‹ã—ã¾ã™ã€‚</p>

        <h4>å®Ÿè£…ä¾‹5: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢å®Ÿè£…</h4>
        <pre><code>from rank_bm25 import BM25Okapi
import numpy as np

class HybridSearcher:
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, vectorstore, embeddings):
        self.vectorstore = vectorstore
        self.embeddings = embeddings
        self.bm25 = None
        self.documents = []

    def initialize_bm25(self, documents):
        """BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åˆæœŸåŒ–"""
        self.documents = documents

        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        tokenized_docs = [
            doc.page_content.split() for doc in documents
        ]

        # BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        self.bm25 = BM25Okapi(tokenized_docs)
        print(f"BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ: {len(documents)}ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")

    def bm25_search(self, query: str, k: int = 10):
        """BM25ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢"""
        if not self.bm25:
            raise ValueError("BM25ãŒæœªåˆæœŸåŒ–")

        tokenized_query = query.split()
        scores = self.bm25.get_scores(tokenized_query)

        # Top-Kå–å¾—
        top_indices = np.argsort(scores)[::-1][:k]

        results = [
            {
                'document': self.documents[idx],
                'score': float(scores[idx])
            }
            for idx in top_indices
        ]

        return results

    def vector_search(self, query: str, k: int = 10):
        """ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦æ¤œç´¢"""
        results = self.vectorstore.similarity_search_with_score(query, k=k)

        return [
            {
                'document': doc,
                'score': float(score)
            }
            for doc, score in results
        ]

    def hybrid_search(self, query: str, k: int = 5,
                     vector_weight: float = 0.5):
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: è¿”ã™ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            vector_weight: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®é‡ã¿ï¼ˆ0-1ï¼‰
        """
        # ä¸¡æ–¹ã®æ¤œç´¢ã‚’å®Ÿè¡Œ
        bm25_results = self.bm25_search(query, k=k*2)
        vector_results = self.vector_search(query, k=k*2)

        # ã‚¹ã‚³ã‚¢æ­£è¦åŒ–
        bm25_scores = [r['score'] for r in bm25_results]
        vector_scores = [r['score'] for r in vector_results]

        bm25_normalized = self._normalize_scores(bm25_scores)
        vector_normalized = self._normalize_scores(vector_scores)

        # ã‚¹ã‚³ã‚¢çµ±åˆ
        combined_scores = {}

        for i, result in enumerate(bm25_results):
            doc_hash = hash(result['document'].page_content)
            combined_scores[doc_hash] = {
                'document': result['document'],
                'score': (1 - vector_weight) * bm25_normalized[i]
            }

        for i, result in enumerate(vector_results):
            doc_hash = hash(result['document'].page_content)
            if doc_hash in combined_scores:
                combined_scores[doc_hash]['score'] += vector_weight * vector_normalized[i]
            else:
                combined_scores[doc_hash] = {
                    'document': result['document'],
                    'score': vector_weight * vector_normalized[i]
                }

        # ã‚½ãƒ¼ãƒˆã—ã¦Top-Kå–å¾—
        sorted_results = sorted(
            combined_scores.values(),
            key=lambda x: x['score'],
            reverse=True
        )[:k]

        return sorted_results

    def _normalize_scores(self, scores):
        """ã‚¹ã‚³ã‚¢æ­£è¦åŒ–ï¼ˆ0-1ç¯„å›²ï¼‰"""
        scores = np.array(scores)
        if scores.max() == scores.min():
            return np.ones_like(scores)
        return (scores - scores.min()) / (scores.max() - scores.min())

    def compare_search_methods(self, query: str, k: int = 5):
        """æ¤œç´¢æ‰‹æ³•ã®æ¯”è¼ƒ"""
        results = {
            'BM25ã®ã¿': self.bm25_search(query, k=k),
            'ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã¿': self.vector_search(query, k=k),
            'ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰(50:50)': self.hybrid_search(query, k=k, vector_weight=0.5),
            'ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰(ãƒ™ã‚¯ãƒˆãƒ«é‡è¦–)': self.hybrid_search(query, k=k, vector_weight=0.7)
        }

        return results

# ä½¿ç”¨ä¾‹
hybrid_searcher = HybridSearcher(vectorstore, embeddings)

# BM25åˆæœŸåŒ–ï¼ˆå…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰
all_docs = vectorstore.similarity_search("", k=1000)  # å…¨å–å¾—ã®ä»£æ›¿
hybrid_searcher.initialize_bm25(all_docs)

# æ¯”è¼ƒæ¤œç´¢
query = "æ©Ÿæ¢°å­¦ç¿’ è©•ä¾¡æŒ‡æ¨™ ç²¾åº¦"
comparison = hybrid_searcher.compare_search_methods(query, k=3)

for method_name, results in comparison.items():
    print(f"\nã€{method_name}ã€‘")
    for i, result in enumerate(results, 1):
        print(f"{i}. ã‚¹ã‚³ã‚¢: {result['score']:.4f}")
        print(f"   {result['document'].page_content[:60]}...")</code></pre>

        <h2>4. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåœ§ç¸®</h2>

        <h3>4.1 ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåœ§ç¸®ã®å¿…è¦æ€§</h3>
        <p>æ¤œç´¢çµæœã‚’LLMã«æ¸¡ã™éš›ã€ãƒˆãƒ¼ã‚¯ãƒ³æ•°å‰Šæ¸›ã¨é–¢é€£æƒ…å ±æŠ½å‡ºã®ãŸã‚ã«åœ§ç¸®ã—ã¾ã™ã€‚</p>

        <div class="note-box">
            <strong>åœ§ç¸®ã®åˆ©ç‚¹:</strong>
            <ul>
                <li><strong>ã‚³ã‚¹ãƒˆå‰Šæ¸›</strong>: ãƒˆãƒ¼ã‚¯ãƒ³æ•°å‰Šæ¸›ã«ã‚ˆã‚ŠAPIè²»ç”¨ã‚’ä½æ¸›</li>
                <li><strong>ç²¾åº¦å‘ä¸Š</strong>: ãƒã‚¤ã‚ºé™¤å»ã«ã‚ˆã‚Šå›ç­”å“è³ªãŒå‘ä¸Š</li>
                <li><strong>ãƒ¬ã‚¹ãƒãƒ³ã‚¹é«˜é€ŸåŒ–</strong>: å‡¦ç†æ™‚é–“çŸ­ç¸®</li>
            </ul>
        </div>

        <h4>å®Ÿè£…ä¾‹6: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåœ§ç¸®</h4>
        <pre><code>from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.chat_models import ChatOpenAI

class ContextCompressor:
    """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåœ§ç¸®ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, llm):
        self.llm = llm

    def create_compression_retriever(self, base_retriever):
        """åœ§ç¸®ãƒ¬ãƒˆãƒªãƒ¼ãƒãƒ¼ä½œæˆ"""
        # LLMãƒ™ãƒ¼ã‚¹ã®æŠ½å‡ºå™¨
        compressor = LLMChainExtractor.from_llm(self.llm)

        # åœ§ç¸®ãƒ¬ãƒˆãƒªãƒ¼ãƒãƒ¼
        compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=base_retriever
        )

        return compression_retriever

    def extract_relevant_parts(self, query: str, documents: list):
        """é–¢é€£éƒ¨åˆ†ã®ã¿æŠ½å‡º"""
        template = """ä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã€è³ªå•ã«é–¢é€£ã™ã‚‹éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
        é–¢é€£ã—ãªã„æƒ…å ±ã¯é™¤å¤–ã—ã¦ãã ã•ã„ã€‚

        è³ªå•: {query}

        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:
        {document}

        é–¢é€£éƒ¨åˆ†:"""

        extracted = []
        for doc in documents:
            prompt = template.format(
                query=query,
                document=doc.page_content
            )

            response = self.llm(prompt)
            extracted.append(response.content)

        return extracted

    def summarize_for_context(self, documents: list, max_tokens: int = 500):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”¨è¦ç´„"""
        combined_text = "\n\n".join([doc.page_content for doc in documents])

        prompt = f"""ä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç¾¤ã‚’{max_tokens}ãƒˆãƒ¼ã‚¯ãƒ³ä»¥å†…ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚
        é‡è¦ãªæƒ…å ±ã‚’ä¿æŒã—ãªãŒã‚‰ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:
        {combined_text}

        è¦ç´„:"""

        summary = self.llm(prompt)
        return summary.content

# ä½¿ç”¨ä¾‹
llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo", openai_api_key="your-api-key")
compressor = ContextCompressor(llm)

# æ¤œç´¢çµæœå–å¾—
query = "æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®éå­¦ç¿’å¯¾ç­–"
search_results = vectorstore.similarity_search(query, k=5)

# é–¢é€£éƒ¨åˆ†æŠ½å‡º
extracted = compressor.extract_relevant_parts(query, search_results)

print("åœ§ç¸®å‰:")
total_chars_before = sum(len(doc.page_content) for doc in search_results)
print(f"ç·æ–‡å­—æ•°: {total_chars_before}")

print("\nåœ§ç¸®å¾Œ:")
total_chars_after = sum(len(text) for text in extracted)
print(f"ç·æ–‡å­—æ•°: {total_chars_after}")
print(f"åœ§ç¸®ç‡: {(1 - total_chars_after/total_chars_before)*100:.1f}%")

# è¦ç´„ç‰ˆ
summary = compressor.summarize_for_context(search_results, max_tokens=300)
print(f"\nè¦ç´„:\n{summary}")</code></pre>

        <h2>ã¾ã¨ã‚</h2>
        <ul>
            <li>ã‚¯ã‚¨ãƒªåˆ†è§£ã¨HyDEã«ã‚ˆã‚Šæ¤œç´¢ç²¾åº¦ã‚’å‘ä¸Š</li>
            <li>Cross-Encoderã¨MMRã§æ¤œç´¢çµæœã‚’æœ€é©åŒ–</li>
            <li>ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ãƒ™ã‚¯ãƒˆãƒ«ã®é•·æ‰€ã‚’çµ±åˆ</li>
            <li>ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåœ§ç¸®ã§ã‚³ã‚¹ãƒˆã¨ç²¾åº¦ã‚’ä¸¡ç«‹</li>
        </ul>

        <div class="nav-buttons">
            <a href="./chapter2-embeddings.html" class="nav-button">â† ç¬¬2ç« </a>
            <a href="./chapter4-production.html" class="nav-button">ç¬¬4ç« ã¸ â†’</a>
        </div>
    <section class="disclaimer">
<h3>å…è²¬äº‹é …</h3>
<ul>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹Code examplesã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
<li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
</ul>
</section>

</main>


        <div class="feedback-notice">
            <h3>âš ï¸ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å“è³ªå‘ä¸Šã«ã”å”åŠ›ãã ã•ã„</h3>
            <p>ã“ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯AIã‚’æ´»ç”¨ã—ã¦ä½œæˆã•ã‚Œã¦ã„ã¾ã™ã€‚èª¤ã‚Šã‚„æ”¹å–„ç‚¹ã‚’è¦‹ã¤ã‘ã‚‰ã‚ŒãŸå ´åˆã¯ã€ä»¥ä¸‹ã®æ–¹æ³•ã§ã”å ±å‘Šãã ã•ã„ï¼š</p>
            <div class="feedback-options">
                <a href="https://forms.gle/9GfVBa2Qa7Uy9taQA" target="_blank" class="feedback-button">
                    ğŸ“ ä¿®æ­£ä¾é ¼ãƒ•ã‚©ãƒ¼ãƒ 
                </a>
                <a href="mailto:yusuke.hashimoto.d8@tohoku.ac.jp" class="feedback-button">
                    âœ‰ï¸ ãƒ¡ãƒ¼ãƒ«ã§é€£çµ¡
                </a>
            </div>
        </div>

    <footer>
        <div class="container">
            <p>&copy; 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
