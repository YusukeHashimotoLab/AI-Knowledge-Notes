---
title: AIå¯ºå­å±‹
chapter_title: AIå¯ºå­å±‹
subtitle: AIã¨ãƒãƒ†ãƒªã‚¢ãƒ«ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
---

ğŸŒ JP | [ğŸ‡¬ğŸ‡§ EN](<../../../en/PI/ai-agent-process/chapter-4.html>) | Last sync: 2025-11-16

[AIå¯ºå­å±‹ãƒˆãƒƒãƒ—](<../../index.html>)â€º[ãƒ—ãƒ­ã‚»ã‚¹ãƒ»ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹](<../../PI/index.html>)â€º[Ai Agent Process](<../../PI/ai-agent-process/index.html>)â€ºChapter 4

AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹è‡ªå¾‹ãƒ—ãƒ­ã‚»ã‚¹é‹è»¢ ã‚·ãƒªãƒ¼ã‚º

# Chapter 4: ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå”èª¿åˆ¶å¾¡

[â† Chapter 3: ãƒ¢ãƒ‡ãƒ«ãƒ•ãƒªãƒ¼åˆ¶å¾¡](<chapter-3.html>) [Chapter 5: å®Ÿãƒ—ãƒ©ãƒ³ãƒˆã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã¨å®‰å…¨æ€§ â†’](<chapter-5.html>) [ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡](<index.html>)

## Chapter 4ã®æ¦‚è¦

è¤‡é›‘ãªãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ©ãƒ³ãƒˆã§ã¯ã€è¤‡æ•°ã®åå¿œå™¨ã‚„è’¸ç•™å¡”ãŒç›¸äº’ã«å½±éŸ¿ã—åˆã„ãªãŒã‚‰é‹è»¢ã•ã‚Œã¾ã™ã€‚ ã“ã®ã‚ˆã†ãªåˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€å˜ä¸€ã®AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã¯ãªãã€è¤‡æ•°ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå”èª¿ã—ã¦åˆ¶å¾¡ã‚’è¡Œã†ã“ã¨ã§ã€ ã‚ˆã‚ŠåŠ¹ç‡çš„ã§æŸ”è»Ÿãªé‹è»¢ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚ 

æœ¬ç« ã§ã¯ã€ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¼·åŒ–å­¦ç¿’ï¼ˆMARL: Multi-Agent Reinforcement Learningï¼‰ã®åŸºç¤ã‹ã‚‰ã€ å®Ÿéš›ã®ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã¸ã®å¿œç”¨ã¾ã§ã‚’7ã¤ã®å®Ÿè£…ä¾‹ã¨ã¨ã‚‚ã«è§£èª¬ã—ã¾ã™ã€‚ 

### æœ¬ç« ã§å­¦ã¶ã“ã¨

  * **CTDEï¼ˆCentralized Training with Decentralized Executionï¼‰** ï¼šå­¦ç¿’æ™‚ã¯ä¸­å¤®é›†æ¨©ã€å®Ÿè¡Œæ™‚ã¯åˆ†æ•£
  * **Independent Q-Learning** ï¼šå„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç‹¬ç«‹ã—ã¦å­¦ç¿’
  * **QMIX** ï¼šä¾¡å€¤é–¢æ•°ã®åˆ†è§£ã¨æ··åˆã«ã‚ˆã‚‹ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆå‰²å½“
  * **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“é€šä¿¡** ï¼šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã«ã‚ˆã‚‹å”èª¿
  * **å”èª¿ã‚¿ã‚¹ã‚¯** ï¼šè¤‡æ•°åå¿œå™¨ã®åŒæœŸåˆ¶å¾¡
  * **ç«¶äº‰ã‚¿ã‚¹ã‚¯** ï¼šé™ã‚‰ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ã®é…åˆ†
  * **æ··åˆã‚¿ã‚¹ã‚¯** ï¼šå”èª¿ã¨ç«¶äº‰ã®ä¸¡é¢ã‚’æŒã¤ç¾å®Ÿçš„ãªã‚·ãƒŠãƒªã‚ª

## ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¼·åŒ–å­¦ç¿’ã®åŸºç¤

### å®šå¼åŒ–

ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç³»ã¯ã€éƒ¨åˆ†è¦³æ¸¬ãƒãƒ«ã‚³ãƒ•ã‚²ãƒ¼ãƒ ï¼ˆPartially Observable Stochastic Gameï¼‰ã¨ã—ã¦å®šå¼åŒ–ã•ã‚Œã¾ã™ï¼š 

**ãƒãƒ«ã‚³ãƒ•ã‚²ãƒ¼ãƒ ï¼š**

\\[ \mathcal{G} = \langle \mathcal{N}, \mathcal{S}, \\{\mathcal{A}^i\\}_{i \in \mathcal{N}}, \mathcal{T}, \\{R^i\\}_{i \in \mathcal{N}}, \\{\mathcal{O}^i\\}_{i \in \mathcal{N}}, \gamma \rangle \\] 

  * \\(\mathcal{N} = \\{1, 2, \ldots, n\\}\\)ï¼šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé›†åˆ
  * \\(\mathcal{S}\\)ï¼šã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ç©ºé–“
  * \\(\mathcal{A}^i\\)ï¼šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ\\(i\\)ã®è¡Œå‹•ç©ºé–“
  * \\(\mathcal{T}: \mathcal{S} \times \mathcal{A}^1 \times \cdots \times \mathcal{A}^n \to \Delta(\mathcal{S})\\)ï¼šçŠ¶æ…‹é·ç§»é–¢æ•°
  * \\(R^i: \mathcal{S} \times \mathcal{A}^1 \times \cdots \times \mathcal{A}^n \to \mathbb{R}\\)ï¼šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ\\(i\\)ã®å ±é…¬é–¢æ•°
  * \\(\mathcal{O}^i\\)ï¼šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ\\(i\\)ã®è¦³æ¸¬ç©ºé–“

### å”èª¿ãƒ»ç«¶äº‰ãƒ»æ··åˆã®åˆ†é¡
    
    
    ```mermaid
    graph TD
                        A[ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¿ã‚¹ã‚¯] --> B[å®Œå…¨å”èª¿]
                        A --> C[å®Œå…¨ç«¶äº‰]
                        A --> D[æ··åˆ]
                        B --> E[å…±é€šå ±é…¬RÂ¹=RÂ²=...=Râ¿]
                        C --> F[ã‚¼ãƒ­ã‚µãƒ Î£áµ¢ Râ± = 0]
                        D --> G[ä¸€èˆ¬ã‚²ãƒ¼ãƒ å”èª¿+ç«¶äº‰]
    ```

ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã§ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªçŠ¶æ³ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ï¼š 

  * **å”èª¿ã‚¿ã‚¹ã‚¯** ï¼šè¤‡æ•°ã®åå¿œå™¨ã‚’å”èª¿ã•ã›ã¦å…¨ä½“ã®ç”Ÿç”£æ€§ã‚’æœ€å¤§åŒ–
  * **ç«¶äº‰ã‚¿ã‚¹ã‚¯** ï¼šé™ã‚‰ã‚ŒãŸãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆè’¸æ°—ã€å†·å´æ°´ï¼‰ã‚’å„ãƒ—ãƒ©ãƒ³ãƒˆãŒå–ã‚Šåˆã†
  * **æ··åˆã‚¿ã‚¹ã‚¯** ï¼šå„ãƒ—ãƒ©ãƒ³ãƒˆãŒè‡ªèº«ã®ç”Ÿç”£ç›®æ¨™ã‚’é”æˆã—ã¤ã¤ã€å…¨ä½“ã®å®‰å®šæ€§ã‚‚ç¶­æŒ

1 CTDEï¼ˆCentralized Training with Decentralized Executionï¼‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ 

CTDEã¯ã€å­¦ç¿’æ™‚ã«ã¯å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æƒ…å ±ã‚’ä½¿ã£ã¦ä¸­å¤®é›†æ¨©çš„ã«å­¦ç¿’ã—ã€ å®Ÿè¡Œæ™‚ã«ã¯å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè‡ªèº«ã®è¦³æ¸¬ã®ã¿ã§åˆ†æ•£çš„ã«è¡Œå‹•ã™ã‚‹æ çµ„ã¿ã§ã™ã€‚ ã“ã‚Œã«ã‚ˆã‚Šã€å­¦ç¿’ã®åŠ¹ç‡æ€§ã¨å®Ÿè¡Œæ™‚ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’ä¸¡ç«‹ã—ã¾ã™ã€‚ 
    
    
    ```mermaid
    graph LR
                            subgraph Training[å­¦ç¿’æ™‚ï¼ˆä¸­å¤®é›†æ¨©ï¼‰]
                                S[ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹] --> C[ä¸­å¤®Critic]
                                O1[è¦³æ¸¬1] --> A1[Actor1]
                                O2[è¦³æ¸¬2] --> A2[Actor2]
                                O3[è¦³æ¸¬3] --> A3[Actor3]
                                C --> A1
                                C --> A2
                                C --> A3
                            end
                            subgraph Execution[å®Ÿè¡Œæ™‚ï¼ˆåˆ†æ•£ï¼‰]
                                O1'[è¦³æ¸¬1] --> A1'[Actor1]
                                O2' [è¦³æ¸¬2] --> A2'[Actor2]
                                O3' [è¦³æ¸¬3] --> A3'[Actor3]
                            end
    ```
    
    
    # CTDEåŸºæœ¬ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å®Ÿè£…
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import numpy as np
    
    class Actor(nn.Module):
        """å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®Actorï¼ˆåˆ†æ•£å®Ÿè¡Œå¯èƒ½ï¼‰"""
        def __init__(self, obs_dim, action_dim):
            super().__init__()
            self.net = nn.Sequential(
                nn.Linear(obs_dim, 64), nn.ReLU(),
                nn.Linear(64, 64), nn.ReLU(),
                nn.Linear(64, action_dim), nn.Tanh()
            )
    
        def forward(self, obs):
            return self.net(obs)
    
    class CentralizedCritic(nn.Module):
        """ä¸­å¤®Criticï¼ˆå­¦ç¿’æ™‚ã®ã¿ä½¿ç”¨ï¼‰"""
        def __init__(self, state_dim, n_agents, action_dim):
            super().__init__()
            total_action_dim = n_agents * action_dim
            self.net = nn.Sequential(
                nn.Linear(state_dim + total_action_dim, 128), nn.ReLU(),
                nn.Linear(128, 128), nn.ReLU(),
                nn.Linear(128, 1)
            )
    
        def forward(self, state, actions):
            # actions: [n_agents, action_dim] -> flatten
            x = torch.cat([state, actions.flatten()], dim=-1)
            return self.net(x)
    
    class CTDEAgent:
        """CTDEå­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
        def __init__(self, n_agents, obs_dim, action_dim, state_dim):
            self.n_agents = n_agents
            self.actors = [Actor(obs_dim, action_dim) for _ in range(n_agents)]
            self.critic = CentralizedCritic(state_dim, n_agents, action_dim)
    
            self.actor_opts = [optim.Adam(a.parameters(), lr=3e-4) for a in self.actors]
            self.critic_opt = optim.Adam(self.critic.parameters(), lr=1e-3)
    
        def select_actions(self, observations):
            """å®Ÿè¡Œæ™‚ï¼šå„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç‹¬ç«‹ã«è¡Œå‹•é¸æŠ"""
            actions = []
            for i, obs in enumerate(observations):
                with torch.no_grad():
                    obs_t = torch.FloatTensor(obs)
                    action = self.actors[i](obs_t).numpy()
                actions.append(action)
            return np.array(actions)
    
        def train_step(self, batch):
            """å­¦ç¿’æ™‚ï¼šä¸­å¤®Criticã‚’ä½¿ã£ãŸæ›´æ–°"""
            states, obs, actions, rewards, next_states, next_obs, dones = batch
    
            # Criticæ›´æ–°ï¼ˆTDèª¤å·®ï¼‰
            with torch.no_grad():
                next_actions = torch.stack([
                    self.actors[i](torch.FloatTensor(next_obs[i]))
                    for i in range(self.n_agents)
                ])
                target_q = rewards + 0.99 * self.critic(
                    torch.FloatTensor(next_states), next_actions
                ) * (1 - dones)
    
            current_q = self.critic(torch.FloatTensor(states), torch.FloatTensor(actions))
            critic_loss = nn.MSELoss()(current_q, target_q)
    
            self.critic_opt.zero_grad()
            critic_loss.backward()
            self.critic_opt.step()
    
            # Actoræ›´æ–°ï¼ˆãƒãƒªã‚·ãƒ¼å‹¾é…ï¼‰
            for i in range(self.n_agents):
                new_actions = [
                    self.actors[j](torch.FloatTensor(obs[j])) if j == i
                    else torch.FloatTensor(actions[j])
                    for j in range(self.n_agents)
                ]
                new_actions = torch.stack(new_actions)
    
                actor_loss = -self.critic(torch.FloatTensor(states), new_actions).mean()
    
                self.actor_opts[i].zero_grad()
                actor_loss.backward()
                self.actor_opts[i].step()
    
    # ä½¿ç”¨ä¾‹ï¼š3ã¤ã®CSTRã®å”èª¿åˆ¶å¾¡
    n_agents = 3
    agent = CTDEAgent(n_agents=n_agents, obs_dim=4, action_dim=2, state_dim=12)
    
    # å®Ÿè¡Œæ™‚ã¯åˆ†æ•£ï¼ˆå„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯è‡ªèº«ã®è¦³æ¸¬ã®ã¿ä½¿ç”¨ï¼‰
    observations = [np.random.randn(4) for _ in range(n_agents)]
    actions = agent.select_actions(observations)  # åˆ†æ•£å®Ÿè¡Œ
    print(f"åˆ†æ•£å®Ÿè¡Œã§ã®è¡Œå‹•: {actions.shape}")  # (3, 2)

2 Independent Q-Learningï¼ˆIQLï¼‰ã«ã‚ˆã‚‹ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’ 

æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’æ‰‹æ³•ã¯ã€å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç‹¬ç«‹ã—ã¦Qå­¦ç¿’ã‚’è¡Œã†ã‚‚ã®ã§ã™ã€‚ ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ç’°å¢ƒã®ä¸€éƒ¨ã¨ã¿ãªã•ã‚Œã€éå®šå¸¸æ€§ã®å•é¡ŒãŒã‚ã‚Šã¾ã™ãŒã€å®Ÿè£…ãŒå®¹æ˜“ã§å®Ÿç”¨çš„ãªã‚±ãƒ¼ã‚¹ã‚‚å¤šã„ã§ã™ã€‚ 
    
    
    # Independent Q-Learningå®Ÿè£…
    import numpy as np
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from collections import deque
    import random
    
    class QNetwork(nn.Module):
        """å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç‹¬ç«‹ã®Qé–¢æ•°"""
        def __init__(self, obs_dim, action_dim):
            super().__init__()
            self.net = nn.Sequential(
                nn.Linear(obs_dim, 64), nn.ReLU(),
                nn.Linear(64, 64), nn.ReLU(),
                nn.Linear(64, action_dim)
            )
    
        def forward(self, obs):
            return self.net(obs)
    
    class IQLAgent:
        """Independent Q-Learningã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
        def __init__(self, obs_dim, action_dim, agent_id):
            self.agent_id = agent_id
            self.action_dim = action_dim
            self.q_net = QNetwork(obs_dim, action_dim)
            self.target_net = QNetwork(obs_dim, action_dim)
            self.target_net.load_state_dict(self.q_net.state_dict())
            self.optimizer = optim.Adam(self.q_net.parameters(), lr=1e-3)
            self.memory = deque(maxlen=10000)
            self.epsilon = 1.0
    
        def select_action(self, obs):
            """Îµ-greedyè¡Œå‹•é¸æŠ"""
            if random.random() < self.epsilon:
                return np.random.randint(self.action_dim)
            else:
                with torch.no_grad():
                    q_values = self.q_net(torch.FloatTensor(obs))
                    return q_values.argmax().item()
    
        def store_transition(self, obs, action, reward, next_obs, done):
            self.memory.append((obs, action, reward, next_obs, done))
    
        def train_step(self, batch_size=32):
            if len(self.memory) < batch_size:
                return 0.0
    
            batch = random.sample(self.memory, batch_size)
            obs, actions, rewards, next_obs, dones = zip(*batch)
    
            obs_t = torch.FloatTensor(obs)
            actions_t = torch.LongTensor(actions)
            rewards_t = torch.FloatTensor(rewards)
            next_obs_t = torch.FloatTensor(next_obs)
            dones_t = torch.FloatTensor(dones)
    
            # ç¾åœ¨ã®Qå€¤
            q_values = self.q_net(obs_t).gather(1, actions_t.unsqueeze(1)).squeeze()
    
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆQå€¤
            with torch.no_grad():
                next_q_values = self.target_net(next_obs_t).max(1)[0]
                target_q = rewards_t + 0.99 * next_q_values * (1 - dones_t)
    
            # æ›´æ–°
            loss = nn.MSELoss()(q_values, target_q)
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
    
            # Îµæ¸›è¡°
            self.epsilon = max(0.01, self.epsilon * 0.995)
    
            return loss.item()
    
        def update_target(self):
            self.target_net.load_state_dict(self.q_net.state_dict())
    
    # ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    class MultiCSTREnv:
        """3ã¤ã®CSTRãŒç›´åˆ—æ¥ç¶šã•ã‚ŒãŸç’°å¢ƒ"""
        def __init__(self):
            self.n_agents = 3
            self.reset()
    
        def reset(self):
            # å„åå¿œå™¨ã®çŠ¶æ…‹ [æ¸©åº¦, æ¿ƒåº¦]
            self.states = np.array([[350.0, 0.5], [340.0, 0.3], [330.0, 0.1]])
            return self.states
    
        def step(self, actions):
            # actions: [0=å†·å´, 1=åŠ ç†±, 2=ç¶­æŒ] for each agent
            rewards = []
            for i in range(self.n_agents):
                T, C = self.states[i]
    
                # è¡Œå‹•ã«ã‚ˆã‚‹æ¸©åº¦å¤‰åŒ–
                if actions[i] == 0:  # å†·å´
                    T -= 5
                elif actions[i] == 1:  # åŠ ç†±
                    T += 5
    
                # åå¿œé€²è¡Œï¼ˆç°¡æ˜“ãƒ¢ãƒ‡ãƒ«ï¼‰
                k = 0.1 * np.exp((T - 350) / 10)
                C = C * (1 - k * 0.1)
    
                # æ¬¡ã®åå¿œå™¨ã¸ã®æµå…¥
                if i < self.n_agents - 1:
                    self.states[i+1, 1] += C * 0.3
    
                self.states[i] = [T, C]
    
                # å ±é…¬ï¼šç›®æ¨™æ¸©åº¦ã¨ã®å·®ã¨ç”Ÿç”£æ€§
                temp_penalty = -abs(T - 350)
                production = k * C
                rewards.append(temp_penalty * 0.1 + production * 10)
    
            done = False
            return self.states.copy(), np.array(rewards), [done]*self.n_agents
    
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    env = MultiCSTREnv()
    agents = [IQLAgent(obs_dim=2, action_dim=3, agent_id=i) for i in range(3)]
    
    for episode in range(500):
        obs = env.reset()
        episode_rewards = [0] * 3
    
        for step in range(100):
            actions = [agents[i].select_action(obs[i]) for i in range(3)]
            next_obs, rewards, dones = env.step(actions)
    
            # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç‹¬ç«‹ã«å­¦ç¿’
            for i in range(3):
                agents[i].store_transition(obs[i], actions[i], rewards[i],
                                           next_obs[i], dones[i])
                agents[i].train_step()
                episode_rewards[i] += rewards[i]
    
            obs = next_obs
    
        # å®šæœŸçš„ã«ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ›´æ–°
        if episode % 10 == 0:
            for agent in agents:
                agent.update_target()
    
        if episode % 50 == 0:
            print(f"Episode {episode}, Total Rewards: {sum(episode_rewards):.2f}")

3 QMIXï¼šä¾¡å€¤åˆ†è§£ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆå‰²å½“ 

QMIXã¯ã€å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å€‹åˆ¥Qå€¤ã‚’æ··åˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§çµ±åˆã—ã€ å˜èª¿æ€§åˆ¶ç´„ï¼ˆIndividual-Global-Max: IGMï¼‰ã‚’ä¿ã¡ãªãŒã‚‰ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆå‰²å½“ã‚’å®Ÿç¾ã—ã¾ã™ã€‚ 

**QMIXä¾¡å€¤åˆ†è§£ï¼š**

\\[ Q_{tot}(\boldsymbol{\tau}, \mathbf{u}) = f_{mix}(Q_1(\tau^1, u^1), \ldots, Q_n(\tau^n, u^n); s) \\] 

å˜èª¿æ€§åˆ¶ç´„ï¼š

\\[ \frac{\partial Q_{tot}}{\partial Q_i} \geq 0, \quad \forall i \\] 
    
    
    # QMIXå®Ÿè£…
    import torch
    import torch.nn as nn
    import torch.optim as optim
    
    class AgentQNetwork(nn.Module):
        """å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®Qé–¢æ•°"""
        def __init__(self, obs_dim, action_dim):
            super().__init__()
            self.net = nn.Sequential(
                nn.Linear(obs_dim, 64), nn.ReLU(),
                nn.Linear(64, action_dim)
            )
    
        def forward(self, obs):
            return self.net(obs)
    
    class QMixerNetwork(nn.Module):
        """å˜èª¿æ€§ã‚’ä¿è¨¼ã™ã‚‹æ··åˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""
        def __init__(self, n_agents, state_dim):
            super().__init__()
            self.n_agents = n_agents
    
            # ãƒã‚¤ãƒ‘ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆçŠ¶æ…‹ã‹ã‚‰é‡ã¿ã‚’ç”Ÿæˆï¼‰
            self.hyper_w1 = nn.Linear(state_dim, n_agents * 32)
            self.hyper_b1 = nn.Linear(state_dim, 32)
            self.hyper_w2 = nn.Linear(state_dim, 32)
            self.hyper_b2 = nn.Sequential(
                nn.Linear(state_dim, 32), nn.ReLU(),
                nn.Linear(32, 1)
            )
    
        def forward(self, agent_qs, state):
            """
            agent_qs: [batch, n_agents]
            state: [batch, state_dim]
            """
            batch_size = agent_qs.size(0)
            agent_qs = agent_qs.view(batch_size, 1, self.n_agents)
    
            # ç¬¬1å±¤ã®é‡ã¿ï¼ˆçµ¶å¯¾å€¤ã§å˜èª¿æ€§ä¿è¨¼ï¼‰
            w1 = torch.abs(self.hyper_w1(state))
            w1 = w1.view(batch_size, self.n_agents, 32)
            b1 = self.hyper_b1(state).view(batch_size, 1, 32)
    
            # ç¬¬1å±¤ã®å‡ºåŠ›
            hidden = torch.bmm(agent_qs, w1) + b1
            hidden = torch.relu(hidden)
    
            # ç¬¬2å±¤ã®é‡ã¿ï¼ˆçµ¶å¯¾å€¤ã§å˜èª¿æ€§ä¿è¨¼ï¼‰
            w2 = torch.abs(self.hyper_w2(state))
            w2 = w2.view(batch_size, 32, 1)
            b2 = self.hyper_b2(state).view(batch_size, 1, 1)
    
            # æœ€çµ‚å‡ºåŠ›
            q_tot = torch.bmm(hidden, w2) + b2
            return q_tot.view(batch_size)
    
    class QMIX:
        """QMIXå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ """
        def __init__(self, n_agents, obs_dim, action_dim, state_dim):
            self.n_agents = n_agents
            self.agent_networks = [AgentQNetwork(obs_dim, action_dim)
                                   for _ in range(n_agents)]
            self.mixer = QMixerNetwork(n_agents, state_dim)
            self.target_networks = [AgentQNetwork(obs_dim, action_dim)
                                    for _ in range(n_agents)]
            self.target_mixer = QMixerNetwork(n_agents, state_dim)
    
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆæœŸåŒ–
            for i in range(n_agents):
                self.target_networks[i].load_state_dict(
                    self.agent_networks[i].state_dict())
            self.target_mixer.load_state_dict(self.mixer.state_dict())
    
            # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
            params = list(self.mixer.parameters())
            for net in self.agent_networks:
                params += list(net.parameters())
            self.optimizer = optim.Adam(params, lr=5e-4)
    
        def select_actions(self, observations, epsilon=0.05):
            """å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•é¸æŠ"""
            actions = []
            for i, obs in enumerate(observations):
                if torch.rand(1).item() < epsilon:
                    actions.append(torch.randint(0, 5, (1,)).item())
                else:
                    with torch.no_grad():
                        q_vals = self.agent_networks[i](torch.FloatTensor(obs))
                        actions.append(q_vals.argmax().item())
            return actions
    
        def train_step(self, batch):
            """QMIXæ›´æ–°"""
            states, obs_list, actions, rewards, next_states, next_obs_list, dones = batch
    
            # ç¾åœ¨ã®Qå€¤ï¼ˆå„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼‰
            agent_qs = []
            for i in range(self.n_agents):
                q_vals = self.agent_networks[i](torch.FloatTensor(obs_list[i]))
                q = q_vals.gather(1, torch.LongTensor(actions[:, i]).unsqueeze(1))
                agent_qs.append(q)
            agent_qs = torch.cat(agent_qs, dim=1)
    
            # æ··åˆã—ã¦Q_tot
            q_tot = self.mixer(agent_qs, torch.FloatTensor(states))
    
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆQå€¤
            with torch.no_grad():
                target_agent_qs = []
                for i in range(self.n_agents):
                    target_q = self.target_networks[i](
                        torch.FloatTensor(next_obs_list[i])).max(1)[0]
                    target_agent_qs.append(target_q.unsqueeze(1))
                target_agent_qs = torch.cat(target_agent_qs, dim=1)
                target_q_tot = self.target_mixer(target_agent_qs,
                                                 torch.FloatTensor(next_states))
                target = rewards + 0.99 * target_q_tot * (1 - dones)
    
            # æå¤±
            loss = nn.MSELoss()(q_tot, target)
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.mixer.parameters(), 10)
            self.optimizer.step()
    
            return loss.item()
    
    # ä½¿ç”¨ä¾‹
    qmix = QMIX(n_agents=3, obs_dim=4, action_dim=5, state_dim=12)
    observations = [torch.randn(4) for _ in range(3)]
    actions = qmix.select_actions(observations)
    print(f"QMIX actions: {actions}")

4 ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“é€šä¿¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«ï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ï¼‰ 

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã§æƒ…å ±ã‚’äº¤æ›ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚ŠåŠ¹æœçš„ãªå”èª¿ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚ CommNetã‚„TarMACãªã©ã®æ‰‹æ³•ã§ã¯ã€æ³¨æ„æ©Ÿæ§‹ã‚’ç”¨ã„ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã‚’å®Ÿè£…ã—ã¾ã™ã€‚ 
    
    
    ```mermaid
    graph LR
                            A1[Agent 1] -->|msgâ‚| C[CommunicationChannel]
                            A2[Agent 2] -->|msgâ‚‚| C
                            A3[Agent 3] -->|msgâ‚ƒ| C
                            C -->|aggregated| A1
                            C -->|aggregated| A2
                            C -->|aggregated| A3
    ```
    
    
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°æ©Ÿæ§‹ã®å®Ÿè£…
    import torch
    import torch.nn as nn
    
    class AttentionCommModule(nn.Module):
        """æ³¨æ„æ©Ÿæ§‹ãƒ™ãƒ¼ã‚¹ã®é€šä¿¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«"""
        def __init__(self, hidden_dim, n_agents):
            super().__init__()
            self.n_agents = n_agents
            self.hidden_dim = hidden_dim
    
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
            self.msg_encoder = nn.Linear(hidden_dim, hidden_dim)
    
            # æ³¨æ„æ©Ÿæ§‹
            self.query = nn.Linear(hidden_dim, hidden_dim)
            self.key = nn.Linear(hidden_dim, hidden_dim)
            self.value = nn.Linear(hidden_dim, hidden_dim)
    
        def forward(self, hidden_states):
            """
            hidden_states: [n_agents, hidden_dim]
            returns: [n_agents, hidden_dim] (é€šä¿¡å¾Œã®éš ã‚ŒçŠ¶æ…‹)
            """
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
            messages = self.msg_encoder(hidden_states)  # [n_agents, hidden_dim]
    
            # æ³¨æ„ã‚¹ã‚³ã‚¢è¨ˆç®—
            Q = self.query(hidden_states)  # [n_agents, hidden_dim]
            K = self.key(messages)  # [n_agents, hidden_dim]
            V = self.value(messages)  # [n_agents, hidden_dim]
    
            # ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‰ãƒ‰ãƒƒãƒˆç©æ³¨æ„
            attn_scores = torch.matmul(Q, K.T) / (self.hidden_dim ** 0.5)
            attn_weights = torch.softmax(attn_scores, dim=-1)
    
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é›†ç´„
            aggregated = torch.matmul(attn_weights, V)
    
            return hidden_states + aggregated  # æ®‹å·®æ¥ç¶š
    
    class CommunicativeAgent(nn.Module):
        """é€šä¿¡å¯èƒ½ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
        def __init__(self, obs_dim, action_dim, hidden_dim, n_agents):
            super().__init__()
            self.obs_encoder = nn.Sequential(
                nn.Linear(obs_dim, hidden_dim), nn.ReLU()
            )
            self.comm_module = AttentionCommModule(hidden_dim, n_agents)
            self.policy = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),
                nn.Linear(hidden_dim, action_dim)
            )
    
        def forward(self, observations, agent_idx):
            """
            observations: [n_agents, obs_dim]
            agent_idx: ã“ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            """
            # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¦³æ¸¬ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            hidden_states = self.obs_encoder(observations)
    
            # é€šä¿¡ãƒ•ã‚§ãƒ¼ã‚º
            comm_hidden = self.comm_module(hidden_states)
    
            # è‡ªåˆ†ã®éš ã‚ŒçŠ¶æ…‹ã§è¡Œå‹•é¸æŠ
            my_hidden = comm_hidden[agent_idx]
            action_logits = self.policy(my_hidden)
    
            return action_logits, comm_hidden
    
    # è¤‡æ•°ãƒ©ã‚¦ãƒ³ãƒ‰ã®é€šä¿¡
    class MultiRoundCommAgent(nn.Module):
        """è¤‡æ•°ãƒ©ã‚¦ãƒ³ãƒ‰ã®é€šä¿¡ã‚’è¡Œã†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
        def __init__(self, obs_dim, action_dim, hidden_dim, n_agents, n_comm_rounds=2):
            super().__init__()
            self.n_comm_rounds = n_comm_rounds
    
            self.obs_encoder = nn.Linear(obs_dim, hidden_dim)
            self.comm_modules = nn.ModuleList([
                AttentionCommModule(hidden_dim, n_agents)
                for _ in range(n_comm_rounds)
            ])
            self.policy = nn.Linear(hidden_dim, action_dim)
    
        def forward(self, observations):
            """
            observations: [n_agents, obs_dim]
            returns: [n_agents, action_dim]
            """
            hidden = torch.relu(self.obs_encoder(observations))
    
            # è¤‡æ•°ãƒ©ã‚¦ãƒ³ãƒ‰ã®é€šä¿¡
            for comm_module in self.comm_modules:
                hidden = comm_module(hidden)
    
            # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè¡Œå‹•é¸æŠ
            actions = self.policy(hidden)
            return actions
    
    # ä½¿ç”¨ä¾‹ï¼š3åå¿œå™¨ã®å”èª¿åˆ¶å¾¡
    n_agents = 3
    obs_dim = 4  # [æ¸©åº¦, æ¿ƒåº¦, æµé‡, åœ§åŠ›]
    action_dim = 5  # é›¢æ•£è¡Œå‹•
    hidden_dim = 64
    
    agent = MultiRoundCommAgent(obs_dim, action_dim, hidden_dim, n_agents, n_comm_rounds=2)
    
    # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    observations = torch.randn(n_agents, obs_dim)
    actions = agent(observations)
    print(f"é€šä¿¡å¾Œã®è¡Œå‹•: {actions.shape}")  # [3, 5]
    
    # å€‹åˆ¥ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã®ä½¿ç”¨
    single_agent = CommunicativeAgent(obs_dim, action_dim, hidden_dim, n_agents)
    action_logits, comm_hidden = single_agent(observations, agent_idx=0)
    print(f"Agent 0 ã®è¡Œå‹•: {torch.softmax(action_logits, dim=-1)}")
    print(f"é€šä¿¡å¾Œã®éš ã‚ŒçŠ¶æ…‹: {comm_hidden.shape}")  # [3, 64]

5 å”èª¿ã‚¿ã‚¹ã‚¯ï¼š3ã¤ã®CSTRã®åŒæœŸåˆ¶å¾¡ 

3ã¤ã®é€£ç¶šæ”ªæ‹Œæ§½åå¿œå™¨ï¼ˆCSTRï¼‰ã‚’ç›´åˆ—ã«æ¥ç¶šã—ã€å…¨ä½“ã®ç”Ÿç”£æ€§ã‚’æœ€å¤§åŒ–ã—ãªãŒã‚‰ å„åå¿œå™¨ã®æ¸©åº¦ã‚’é©åˆ‡ã«åˆ¶å¾¡ã™ã‚‹å”èª¿ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè£…ã—ã¾ã™ã€‚ 
    
    
    # å”èª¿ã‚¿ã‚¹ã‚¯ï¼š3ã¤ã®CSTRã®åŒæœŸåˆ¶å¾¡
    import numpy as np
    import torch
    import torch.nn as nn
    import torch.optim as optim
    
    class CooperativeCSTREnv:
        """3ã¤ã®CSTRãŒç›´åˆ—æ¥ç¶šã•ã‚ŒãŸå”èª¿ç’°å¢ƒ"""
        def __init__(self):
            self.n_agents = 3
            self.dt = 0.1  # æ™‚é–“åˆ»ã¿ [min]
            self.reset()
    
        def reset(self):
            # å„CSTR: [æ¸©åº¦T(K), æ¿ƒåº¦CA(mol/L), æµé‡F(L/min)]
            self.states = np.array([
                [350.0, 2.0, 100.0],  # CSTR1
                [340.0, 1.5, 100.0],  # CSTR2
                [330.0, 1.0, 100.0]   # CSTR3
            ])
            self.time = 0
            return self._get_observations()
    
        def _get_observations(self):
            """å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¦³æ¸¬ï¼ˆå±€æ‰€æƒ…å ±+éš£æ¥æƒ…å ±ï¼‰"""
            obs = []
            for i in range(self.n_agents):
                local = self.states[i].copy()
                # å‰æ®µã®æƒ…å ±ï¼ˆå”èª¿ã®ãŸã‚ï¼‰
                prev = self.states[i-1] if i > 0 else np.zeros(3)
                # å¾Œæ®µã®æƒ…å ±
                next_ = self.states[i+1] if i < self.n_agents-1 else np.zeros(3)
                obs.append(np.concatenate([local, prev, next_]))
            return obs
    
        def step(self, actions):
            """
            actions: [n_agents, 2] = [[Q1, Tin1], [Q2, Tin2], [Q3, Tin3]]
            Q: å†·å´æµé‡ [L/min] (0-50)
            Tin: å…¥å£æ¸©åº¦ [K] (300-400)
            """
            rewards = []
    
            for i in range(self.n_agents):
                T, CA, F = self.states[i]
                Q = actions[i][0] * 50  # æ­£è¦åŒ–è§£é™¤
                Tin = actions[i][1] * 100 + 300
    
                # åå¿œé€Ÿåº¦å®šæ•°ï¼ˆArrheniuså¼ï¼‰
                Ea = 50000  # [J/mol]
                R = 8.314   # [J/(molÂ·K)]
                k = 1e10 * np.exp(-Ea / (R * T))
    
                # CSTRãƒ¢ãƒ‡ãƒ«
                V = 1000  # åå¿œå™¨ä½“ç© [L]
                rho = 1000  # å¯†åº¦ [g/L]
                Cp = 4.18   # æ¯”ç†± [J/(gÂ·K)]
                dHr = -50000  # åå¿œç†± [J/mol]
    
                # å…¥å£æ¿ƒåº¦ï¼ˆå‰æ®µã‹ã‚‰ã®æµå…¥ï¼‰
                CA_in = self.states[i-1, 1] if i > 0 else 2.5
    
                # ç‰©è³ªåæ”¯
                dCA = (F / V) * (CA_in - CA) - k * CA
                CA_new = CA + dCA * self.dt
    
                # ã‚¨ãƒãƒ«ã‚®ãƒ¼åæ”¯
                Q_reaction = -dHr * k * CA * V
                Q_cooling = Q * rho * Cp * (T - Tin)
                dT = (Q_reaction - Q_cooling) / (V * rho * Cp)
                T_new = T + dT * self.dt
    
                self.states[i] = [T_new, max(0, CA_new), F]
    
                # å”èª¿å ±é…¬ï¼šå…¨ä½“ã®ç”Ÿç”£æ€§ã¨å®‰å®šæ€§
                production = k * CA  # åå¿œé€Ÿåº¦
                temp_penalty = -abs(T_new - 350) ** 2  # ç›®æ¨™æ¸©åº¦ã‹ã‚‰ã®åå·®
                flow_continuity = -abs(F - 100) ** 2 if i > 0 else 0
    
                reward = production * 100 + temp_penalty * 0.1 + flow_continuity * 0.01
                rewards.append(reward)
    
            # å…±é€šå ±é…¬ï¼ˆå”èª¿ã‚¿ã‚¹ã‚¯ï¼‰
            total_production = sum([self.states[i, 1] for i in range(self.n_agents)])
            common_reward = total_production * 10
            rewards = [r + common_reward for r in rewards]
    
            self.time += self.dt
            done = self.time >= 10  # 10åˆ†é–“ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
    
            return self._get_observations(), np.array(rewards), [done]*self.n_agents
    
    # QMIXå­¦ç¿’ï¼ˆç°¡ç•¥ç‰ˆï¼‰
    class SimpleQMIX:
        def __init__(self, n_agents, obs_dim, action_dim):
            self.n_agents = n_agents
            self.q_nets = [nn.Sequential(
                nn.Linear(obs_dim, 64), nn.ReLU(),
                nn.Linear(64, action_dim)
            ) for _ in range(n_agents)]
            self.mixer = nn.Sequential(
                nn.Linear(n_agents, 32), nn.ReLU(),
                nn.Linear(32, 1)
            )
            params = list(self.mixer.parameters())
            for net in self.q_nets:
                params += list(net.parameters())
            self.optimizer = optim.Adam(params, lr=1e-3)
    
        def select_actions(self, observations):
            actions = []
            for i, obs in enumerate(observations):
                with torch.no_grad():
                    q_vals = self.q_nets[i](torch.FloatTensor(obs))
                    # é€£ç¶šè¡Œå‹•ã‚’é›¢æ•£åŒ–ï¼ˆç°¡ç•¥åŒ–ï¼‰
                    action = torch.rand(2)  # [Qæ­£è¦åŒ–, Tinæ­£è¦åŒ–]
                    actions.append(action.numpy())
            return np.array(actions)
    
    # å­¦ç¿’å®Ÿè¡Œ
    env = CooperativeCSTREnv()
    agent = SimpleQMIX(n_agents=3, obs_dim=9, action_dim=10)
    
    for episode in range(100):
        obs = env.reset()
        episode_reward = 0
    
        for step in range(100):
            actions = agent.select_actions(obs)
            next_obs, rewards, dones = env.step(actions)
            episode_reward += sum(rewards)
            obs = next_obs
    
            if dones[0]:
                break
    
        if episode % 10 == 0:
            print(f"Episode {episode}, Total Reward: {episode_reward:.2f}")
            print(f"  Final Temps: {[f'{s[0]:.1f}K' for s in env.states]}")
            print(f"  Final Concs: {[f'{s[1]:.3f}mol/L' for s in env.states]}")

6 ç«¶äº‰ã‚¿ã‚¹ã‚¯ï¼šé™ã‚‰ã‚ŒãŸãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã®é…åˆ† 

è¤‡æ•°ã®ãƒ—ãƒ©ãƒ³ãƒˆãŒé™ã‚‰ã‚ŒãŸè’¸æ°—ã‚„å†·å´æ°´ã¨ã„ã£ãŸãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’å–ã‚Šåˆã†ç«¶äº‰ã‚·ãƒŠãƒªã‚ªã§ã™ã€‚ å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯è‡ªèº«ã®ç”Ÿç”£ã‚’æœ€å¤§åŒ–ã—ã¤ã¤ã€è³‡æºåˆ¶ç´„ã®ä¸‹ã§èª¿æ•´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ 
    
    
    # ç«¶äº‰ã‚¿ã‚¹ã‚¯ï¼šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é…åˆ†å•é¡Œ
    import numpy as np
    import torch
    import torch.nn as nn
    
    class CompetitiveUtilityEnv:
        """é™ã‚‰ã‚ŒãŸãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’è¤‡æ•°ãƒ—ãƒ©ãƒ³ãƒˆã§ç«¶äº‰"""
        def __init__(self, n_plants=3):
            self.n_plants = n_plants
            self.total_steam = 500.0  # ç·è’¸æ°—ä¾›çµ¦é‡ [kg/h]
            self.total_cooling = 1000.0  # ç·å†·å´æ°´ [L/min]
            self.reset()
    
        def reset(self):
            # å„ãƒ—ãƒ©ãƒ³ãƒˆ: [ç”Ÿç”£é€Ÿåº¦, æ¸©åº¦, è’¸æ°—ä½¿ç”¨é‡, å†·å´æ°´ä½¿ç”¨é‡]
            self.states = np.array([
                [50.0, 350.0, 150.0, 300.0] for _ in range(self.n_plants)
            ])
            return self._get_observations()
    
        def _get_observations(self):
            """å„ãƒ—ãƒ©ãƒ³ãƒˆã®è¦³æ¸¬"""
            obs = []
            for i in range(self.n_plants):
                # è‡ªèº«ã®çŠ¶æ…‹ + è³‡æºã®æ®‹é‡æƒ…å ±
                steam_used = sum(self.states[:, 2])
                cooling_used = sum(self.states[:, 3])
                steam_avail = max(0, self.total_steam - steam_used)
                cooling_avail = max(0, self.total_cooling - cooling_used)
    
                obs_i = np.concatenate([
                    self.states[i],
                    [steam_avail, cooling_avail]
                ])
                obs.append(obs_i)
            return obs
    
        def step(self, actions):
            """
            actions: [n_plants, 2] = [[steam_request, cooling_request], ...]
            å„å€¤ã¯0-1ã§æ­£è¦åŒ–
            """
            # è¦æ±‚é‡ã‚’å®Ÿæ•°å€¤ã«å¤‰æ›
            steam_requests = actions[:, 0] * 200  # 0-200 kg/h
            cooling_requests = actions[:, 1] * 400  # 0-400 L/min
    
            # è³‡æºé…åˆ†ï¼ˆæ¯”ä¾‹é…åˆ†ï¼‰
            total_steam_req = sum(steam_requests)
            total_cooling_req = sum(cooling_requests)
    
            if total_steam_req > self.total_steam:
                steam_allocated = steam_requests * (self.total_steam / total_steam_req)
            else:
                steam_allocated = steam_requests
    
            if total_cooling_req > self.total_cooling:
                cooling_allocated = cooling_requests * (self.total_cooling / total_cooling_req)
            else:
                cooling_allocated = cooling_requests
    
            rewards = []
            for i in range(self.n_plants):
                # ç”Ÿç”£é€Ÿåº¦ã¯è³‡æºã«ä¾å­˜
                steam_factor = steam_allocated[i] / 200
                cooling_factor = cooling_allocated[i] / 400
                production = 100 * steam_factor * cooling_factor
    
                # æ¸©åº¦ç®¡ç†ï¼ˆå†·å´ä¸è¶³ã§ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰
                temp_change = (steam_allocated[i] * 0.5 - cooling_allocated[i] * 0.3)
                temp_new = self.states[i, 1] + temp_change
                temp_penalty = -abs(temp_new - 350) if temp_new > 380 else 0
    
                self.states[i] = [production, temp_new,
                                steam_allocated[i], cooling_allocated[i]]
    
                # å ±é…¬ï¼šç”Ÿç”£æ€§ - ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³ãƒšãƒŠãƒ«ãƒ†ã‚£
                shortage_penalty = 0
                if steam_allocated[i] < steam_requests[i]:
                    shortage_penalty += (steam_requests[i] - steam_allocated[i]) * 0.5
                if cooling_allocated[i] < cooling_requests[i]:
                    shortage_penalty += (cooling_requests[i] - cooling_allocated[i]) * 0.3
    
                reward = production - shortage_penalty + temp_penalty
                rewards.append(reward)
    
            done = False
            return self._get_observations(), np.array(rewards), [done]*self.n_plants
    
    # Nash Q-Learningï¼ˆç«¶äº‰ã‚¿ã‚¹ã‚¯ç”¨ï¼‰
    class NashQLearningAgent:
        """Nashå‡è¡¡ã‚’å­¦ç¿’ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
        def __init__(self, obs_dim, action_dim, agent_id):
            self.agent_id = agent_id
            self.q_net = nn.Sequential(
                nn.Linear(obs_dim, 64), nn.ReLU(),
                nn.Linear(64, 64), nn.ReLU(),
                nn.Linear(64, action_dim)
            )
            self.optimizer = optim.Adam(self.q_net.parameters(), lr=1e-3)
            self.epsilon = 0.3
    
        def select_action(self, obs):
            """Îµ-greedy with Nash equilibrium consideration"""
            if np.random.rand() < self.epsilon:
                return np.random.rand(2)
            else:
                with torch.no_grad():
                    # é€£ç¶šè¡Œå‹•ã®è¿‘ä¼¼ï¼ˆç°¡ç•¥åŒ–ï¼‰
                    return torch.sigmoid(torch.randn(2)).numpy()
    
        def update_policy(self, obs, action, reward, next_obs):
            """Qå­¦ç¿’æ›´æ–°ï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
            # å®Ÿè£…ã¯çœç•¥ï¼ˆç«¶äº‰ç’°å¢ƒã§ã®å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰
            pass
    
    # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    env = CompetitiveUtilityEnv(n_plants=3)
    agents = [NashQLearningAgent(obs_dim=6, action_dim=2, agent_id=i)
              for i in range(3)]
    
    for episode in range(50):
        obs = env.reset()
        episode_rewards = [0] * 3
    
        for step in range(50):
            actions = np.array([agents[i].select_action(obs[i]) for i in range(3)])
            next_obs, rewards, dones = env.step(actions)
    
            for i in range(3):
                episode_rewards[i] += rewards[i]
    
            obs = next_obs
    
        if episode % 10 == 0:
            print(f"\nEpisode {episode}")
            print(f"  Individual Rewards: {[f'{r:.1f}' for r in episode_rewards]}")
            print(f"  Productions: {[f'{s[0]:.1f}' for s in env.states]}")
            print(f"  Steam Usage: {[f'{s[2]:.1f}' for s in env.states]} / {env.total_steam}")
            print(f"  Cooling Usage: {[f'{s[3]:.1f}' for s in env.states]} / {env.total_cooling}")

7 æ··åˆã‚¿ã‚¹ã‚¯ï¼šå”èª¿ã¨ç«¶äº‰ãŒå…±å­˜ã™ã‚‹ç”Ÿç”£ã‚·ã‚¹ãƒ†ãƒ  

ç¾å®Ÿã®ãƒ—ãƒ©ãƒ³ãƒˆã§ã¯ã€å”èª¿ã¨ç«¶äº‰ã®ä¸¡é¢ãŒå­˜åœ¨ã—ã¾ã™ã€‚ å„ãƒ—ãƒ©ãƒ³ãƒˆã¯è‡ªèº«ã®ç”Ÿç”£ç›®æ¨™ã‚’é”æˆã—ã¤ã¤ï¼ˆç«¶äº‰ï¼‰ã€å…¨ä½“ã®å®‰å®šæ€§ã‚„åŠ¹ç‡ã‚‚è€ƒæ…®ã™ã‚‹ï¼ˆå”èª¿ï¼‰å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ 
    
    
    # æ··åˆã‚¿ã‚¹ã‚¯ï¼šå”èª¿ã¨ç«¶äº‰ãŒå…±å­˜ã™ã‚‹ç”Ÿç”£ã‚·ã‚¹ãƒ†ãƒ 
    import numpy as np
    import torch
    import torch.nn as nn
    import torch.optim as optim
    
    class MixedCoopCompEnv:
        """å”èª¿ã¨ç«¶äº‰ãŒæ··åœ¨ã™ã‚‹è¤‡é›‘ãªç’°å¢ƒ"""
        def __init__(self, n_plants=3):
            self.n_plants = n_plants
            self.total_energy = 1000.0  # ç·ã‚¨ãƒãƒ«ã‚®ãƒ¼ [kW]
            self.production_targets = [100, 120, 80]  # å„ãƒ—ãƒ©ãƒ³ãƒˆã®ç›®æ¨™ç”Ÿç”£é‡
            self.reset()
    
        def reset(self):
            # å„ãƒ—ãƒ©ãƒ³ãƒˆ: [ç”Ÿç”£é‡, æ¸©åº¦, ã‚¨ãƒãƒ«ã‚®ãƒ¼ä½¿ç”¨, å“è³ª]
            self.states = np.array([
                [50.0, 350.0, 300.0, 0.9] for _ in range(self.n_plants)
            ])
            self.time = 0
            return self._get_observations()
    
        def _get_observations(self):
            obs = []
            for i in range(self.n_plants):
                # è‡ªèº«ã®çŠ¶æ…‹ + ç›®æ¨™ + ä»–ãƒ—ãƒ©ãƒ³ãƒˆã®ç”Ÿç”£é‡ï¼ˆå”èª¿ã®ãŸã‚ï¼‰
                others_production = [self.states[j, 0] for j in range(self.n_plants) if j != i]
                total_energy_used = sum(self.states[:, 2])
    
                obs_i = np.concatenate([
                    self.states[i],
                    [self.production_targets[i]],
                    others_production,
                    [total_energy_used, self.total_energy]
                ])
                obs.append(obs_i)
            return obs
    
        def step(self, actions):
            """
            actions: [n_plants, 3] = [[energy_req, temp_setpoint, quality_target], ...]
            """
            # ã‚¨ãƒãƒ«ã‚®ãƒ¼é…åˆ†ï¼ˆç«¶äº‰è¦ç´ ï¼‰
            energy_requests = actions[:, 0] * 500
            total_energy_req = sum(energy_requests)
    
            if total_energy_req > self.total_energy:
                # ä¸è¶³æ™‚ã¯å„ªå…ˆåº¦ãƒ™ãƒ¼ã‚¹ã§é…åˆ†ï¼ˆç›®æ¨™é”æˆåº¦ãŒä½ã„ãƒ—ãƒ©ãƒ³ãƒˆã‚’å„ªå…ˆï¼‰
                priorities = [
                    max(0, self.production_targets[i] - self.states[i, 0])
                    for i in range(self.n_plants)
                ]
                total_priority = sum(priorities) + 1e-6
                energy_allocated = [
                    self.total_energy * (priorities[i] / total_priority)
                    for i in range(self.n_plants)
                ]
            else:
                energy_allocated = energy_requests
    
            rewards = []
            for i in range(self.n_plants):
                temp_setpoint = actions[i, 1] * 100 + 300  # 300-400K
                quality_target = actions[i, 2]  # 0-1
    
                # ç”Ÿç”£ãƒ¢ãƒ‡ãƒ«
                energy_factor = energy_allocated[i] / 500
                temp_factor = 1.0 - abs(temp_setpoint - 350) / 100
                production = self.production_targets[i] * energy_factor * temp_factor
    
                # å“è³ªãƒ¢ãƒ‡ãƒ«
                quality = 0.5 + 0.5 * quality_target * temp_factor
    
                # æ¸©åº¦æ›´æ–°
                temp = self.states[i, 1] + (temp_setpoint - self.states[i, 1]) * 0.3
    
                self.states[i] = [production, temp, energy_allocated[i], quality]
    
                # å ±é…¬è¨­è¨ˆï¼ˆæ··åˆï¼‰
                # 1. å€‹åˆ¥ç›®æ¨™é”æˆï¼ˆç«¶äº‰è¦ç´ ï¼‰
                target_achievement = -abs(production - self.production_targets[i])
    
                # 2. å“è³ªãƒšãƒŠãƒ«ãƒ†ã‚£
                quality_reward = quality * 10
    
                # 3. ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡ï¼ˆå”èª¿è¦ç´ ï¼‰
                energy_efficiency = production / (energy_allocated[i] + 1)
    
                reward = target_achievement + quality_reward + energy_efficiency * 5
                rewards.append(reward)
    
            # å”èª¿ãƒœãƒ¼ãƒŠã‚¹ï¼šå…¨ä½“ã®å®‰å®šæ€§
            total_production = sum(self.states[:, 0])
            stability = -np.std(self.states[:, 1])  # æ¸©åº¦ã®æ¨™æº–åå·®
            cooperation_bonus = (total_production / sum(self.production_targets)) * 50 + stability
    
            # æœ€çµ‚å ±é…¬ = å€‹åˆ¥å ±é…¬ + å”èª¿ãƒœãƒ¼ãƒŠã‚¹
            rewards = [r + cooperation_bonus * 0.3 for r in rewards]
    
            self.time += 1
            done = self.time >= 100
    
            return self._get_observations(), np.array(rewards), [done]*self.n_plants
    
    # COMA (Counterfactual Multi-Agent Policy Gradient)
    class COMAAgent:
        """æ··åˆã‚¿ã‚¹ã‚¯ç”¨ã®COMAã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
        def __init__(self, n_agents, obs_dim, action_dim, state_dim):
            self.n_agents = n_agents
    
            # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®Actor
            self.actors = [nn.Sequential(
                nn.Linear(obs_dim, 64), nn.ReLU(),
                nn.Linear(64, action_dim), nn.Tanh()
            ) for _ in range(n_agents)]
    
            # ä¸­å¤®Criticï¼ˆåäº‹å®Ÿãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
            self.critic = nn.Sequential(
                nn.Linear(state_dim + n_agents * action_dim, 128), nn.ReLU(),
                nn.Linear(128, n_agents)  # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®Qå€¤
            )
    
            self.actor_opts = [optim.Adam(a.parameters(), lr=3e-4) for a in self.actors]
            self.critic_opt = optim.Adam(self.critic.parameters(), lr=1e-3)
    
        def select_actions(self, observations):
            actions = []
            for i, obs in enumerate(observations):
                with torch.no_grad():
                    action = self.actors[i](torch.FloatTensor(obs)).numpy()
                actions.append(action)
            return np.array(actions)
    
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    env = MixedCoopCompEnv(n_plants=3)
    agent = COMAAgent(n_agents=3, obs_dim=9, action_dim=3, state_dim=12)
    
    for episode in range(100):
        obs = env.reset()
        episode_rewards = [0] * 3
    
        for step in range(100):
            actions = agent.select_actions(obs)
            next_obs, rewards, dones = env.step(actions)
    
            for i in range(3):
                episode_rewards[i] += rewards[i]
    
            obs = next_obs
    
            if dones[0]:
                break
    
        if episode % 20 == 0:
            print(f"\nEpisode {episode}")
            print(f"  Individual Rewards: {[f'{r:.1f}' for r in episode_rewards]}")
            print(f"  Productions: {[f'{s[0]:.1f}' for s in env.states]}")
            print(f"  Targets: {env.production_targets}")
            print(f"  Qualities: {[f'{s[3]:.2f}' for s in env.states]}")
            print(f"  Total Energy Used: {sum(env.states[:, 2]):.1f} / {env.total_energy}")

## Chapter 4 ã¾ã¨ã‚

### å­¦ã‚“ã ã“ã¨

  * **CTDE** ï¼šå­¦ç¿’æ™‚ã¯ä¸­å¤®é›†æ¨©ã€å®Ÿè¡Œæ™‚ã¯åˆ†æ•£ã®åŠ¹ç‡çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
  * **Independent Q-Learning** ï¼šæœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã ãŒéå®šå¸¸æ€§ã®å•é¡Œã‚ã‚Š
  * **QMIX** ï¼šä¾¡å€¤åˆ†è§£ã¨å˜èª¿æ€§åˆ¶ç´„ã«ã‚ˆã‚‹ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆå‰²å½“
  * **é€šä¿¡æ©Ÿæ§‹** ï¼šæ³¨æ„æ©Ÿæ§‹ãƒ™ãƒ¼ã‚¹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã§å”èª¿å¼·åŒ–
  * **å”èª¿ã‚¿ã‚¹ã‚¯** ï¼šå…±é€šå ±é…¬ã§å…¨ä½“æœ€é©ã‚’ç›®æŒ‡ã™
  * **ç«¶äº‰ã‚¿ã‚¹ã‚¯** ï¼šé™ã‚‰ã‚ŒãŸè³‡æºã®é…åˆ†å•é¡Œ
  * **æ··åˆã‚¿ã‚¹ã‚¯** ï¼šç¾å®Ÿã®ãƒ—ãƒ©ãƒ³ãƒˆé‹è»¢ã«è¿‘ã„è¤‡é›‘ãªã‚·ãƒŠãƒªã‚ª

### ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ¯”è¼ƒ

æ‰‹æ³• | é©ç”¨ã‚¿ã‚¹ã‚¯ | å­¦ç¿’åŠ¹ç‡ | å®Ÿè£…é›£æ˜“åº¦  
---|---|---|---  
IQL | å˜ç´”ãªå”èª¿ | ä½ | ä½  
QMIX | å”èª¿ã‚¿ã‚¹ã‚¯ | é«˜ | ä¸­  
CTDE | æ··åˆã‚¿ã‚¹ã‚¯ | é«˜ | ä¸­  
CommNet | è¤‡é›‘ãªå”èª¿ | é«˜ | é«˜  
  
### ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã¸ã®ç¤ºå”†

  * è¤‡æ•°ã®åå¿œå™¨ã‚„è’¸ç•™å¡”ã®å”èª¿åˆ¶å¾¡ã«ã¯QMIXã‚„CTDEãŒæœ‰åŠ¹
  * ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é…åˆ†ã®ã‚ˆã†ãªç«¶äº‰ã‚¿ã‚¹ã‚¯ã«ã¯Nash Q-Learningã‚’æ¤œè¨
  * é€šä¿¡é…å»¶ãŒã‚ã‚‹å ´åˆã¯ã€è¦³æ¸¬ã«ãƒãƒƒãƒ•ã‚¡æƒ…å ±ã‚’å«ã‚ã‚‹
  * å®Ÿãƒ—ãƒ©ãƒ³ãƒˆã§ã¯æ··åˆã‚¿ã‚¹ã‚¯ãŒä¸€èˆ¬çš„ã§ã€å ±é…¬è¨­è¨ˆãŒé‡è¦

### æ¬¡ã®Chapter

**[Chapter 5: å®Ÿãƒ—ãƒ©ãƒ³ãƒˆã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã¨å®‰å…¨æ€§](<chapter-5.html>)**

å®Ÿéš›ã®ãƒ—ãƒ©ãƒ³ãƒˆã«å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹éš›ã®å®‰å…¨æ€§ç¢ºä¿ã€sim-to-realè»¢ç§»ã€ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–ã€äººé–“ã«ã‚ˆã‚‹ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰æ©Ÿæ§‹ãªã©ã‚’å­¦ã³ã¾ã™ã€‚

### å…è²¬äº‹é …

  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹Code examplesã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚
  * å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚
