---
title: AIå¯ºå­å±‹
chapter_title: AIå¯ºå­å±‹
subtitle: AIã¨ãƒãƒ†ãƒªã‚¢ãƒ«ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
---

ğŸŒ JP | [ğŸ‡¬ğŸ‡§ EN](<../../../en/PI/ai-agent-process/chapter-5.html>) | Last sync: 2025-11-16

[AIå¯ºå­å±‹ãƒˆãƒƒãƒ—](<../../index.html>)â€º[ãƒ—ãƒ­ã‚»ã‚¹ãƒ»ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹](<../../PI/index.html>)â€º[Ai Agent Process](<../../PI/ai-agent-process/index.html>)â€ºChapter 5

AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹è‡ªå¾‹ãƒ—ãƒ­ã‚»ã‚¹é‹è»¢ ã‚·ãƒªãƒ¼ã‚º

# Chapter 5: å®Ÿãƒ—ãƒ©ãƒ³ãƒˆã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã¨å®‰å…¨æ€§

â† Chapter 4: ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå”èª¿åˆ¶å¾¡ï¼ˆæº–å‚™ä¸­ï¼‰ [ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡](<index.html>)

## Chapter 5ã®æ¦‚è¦

å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒã§å­¦ç¿’ã•ã›ã‚‹ã ã‘ã§ã¯ä¸ååˆ†ã§ã™ã€‚ å®Ÿéš›ã®ãƒ—ãƒ©ãƒ³ãƒˆã«ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹éš›ã«ã¯ã€**å®‰å…¨æ€§ã®ç¢ºä¿** ã€**ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ç¾å®Ÿã®ã‚®ãƒ£ãƒƒãƒ—ã®å…‹æœ** ã€ **ä¸ç¢ºå®Ÿæ€§ã¸ã®å¯¾å‡¦** ãªã©ã€å¤šãã®èª²é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ 

æœ¬ç« ã§ã¯ã€å®Ÿãƒ—ãƒ©ãƒ³ãƒˆã¸ã®é©ç”¨ã‚’è¦‹æ®ãˆãŸ7ã¤ã®é‡è¦æŠ€è¡“ã‚’å®Ÿè£…ä¾‹ã¨ã¨ã‚‚ã«è§£èª¬ã—ã¾ã™ã€‚ åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆã¯é«˜æ¸©ãƒ»é«˜åœ§ãƒ»å±é™ºç‰©ã‚’æ‰±ã†ãŸã‚ã€AIã«ã‚ˆã‚‹è‡ªå¾‹åˆ¶å¾¡ã«ã¯ç‰¹ã«æ…é‡ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¿…è¦ã§ã™ã€‚ 

#### å®‰å…¨æ€§ã«é–¢ã™ã‚‹é‡è¦ãªæ³¨æ„

æœ¬ç« ã§æ‰±ã†æŠ€è¡“ã¯ã€å®Ÿãƒ—ãƒ©ãƒ³ãƒˆã¸ã®é©ç”¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚ å®Ÿéš›ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã«ã¯ã€ãƒ—ãƒ­ã‚»ã‚¹å®‰å…¨ã®å°‚é–€çŸ¥è­˜ã€è¦åˆ¶éµå®ˆã€ååˆ†ãªæ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆãŒå¿…è¦ã§ã™ã€‚ æœ¬ã‚³ãƒ¼ãƒ‰ã¯æ•™è‚²ç›®çš„ã§ã‚ã‚Šã€å®Ÿãƒ—ãƒ©ãƒ³ãƒˆã§ã®ä½¿ç”¨ã«ã¯é©åˆ‡ãªå®‰å…¨è©•ä¾¡ãŒä¸å¯æ¬ ã§ã™ã€‚ 

### æœ¬ç« ã§å­¦ã¶ã“ã¨

  * **Sim-to-Realè»¢ç§»** ï¼šãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒ€ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹ãƒ­ãƒã‚¹ãƒˆæ€§å‘ä¸Š
  * **å®‰å…¨ãªæ¢ç´¢** ï¼šè¡Œå‹•åˆ¶ç´„ã«ã‚ˆã‚‹å±é™ºé ˜åŸŸã®å›é¿
  * **ä¿å®ˆçš„Qå­¦ç¿’ï¼ˆCQLï¼‰** ï¼šéå¤§è©•ä¾¡ã‚’é˜²ãã‚ªãƒ•ãƒ©ã‚¤ãƒ³å­¦ç¿’
  * **äººé–“ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰** ï¼šç·Šæ€¥æ™‚ã®äººé–“ä»‹å…¥æ©Ÿæ§‹
  * **ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–** ï¼šãƒ™ã‚¤ã‚ºNNã‚„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã«ã‚ˆã‚‹ä¿¡é ¼åŒºé–“æ¨å®š
  * **æ€§èƒ½ç›£è¦–ã¨ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º** ï¼šç¶™ç¶šçš„ãªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
  * **çµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯** ï¼šå…¨è¦ç´ ã‚’çµ„ã¿åˆã‚ã›ãŸå®Ÿè£…

## å®Ÿãƒ—ãƒ©ãƒ³ãƒˆãƒ‡ãƒ—ãƒ­ã‚¤ã®èª²é¡Œ

### Sim-to-Real Gapï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ç¾å®Ÿã®ã‚®ãƒ£ãƒƒãƒ—ï¼‰
    
    
    ```mermaid
    graph LR
                        A[ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒ] -->|ç†æƒ³çš„ãªãƒ¢ãƒ‡ãƒ«| B[å®Œç’§ãªåˆ¶å¾¡]
                        C[å®Ÿãƒ—ãƒ©ãƒ³ãƒˆ] -->|ãƒ¢ãƒ‡ãƒ«èª¤å·®å¤–ä¹±ã‚»ãƒ³ã‚µãƒ¼ãƒã‚¤ã‚º| D[æ€§èƒ½åŠ£åŒ–]
                        B -.->|Sim-to-Real Gap| D
                        E[ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒ€ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³] --> F[ãƒ­ãƒã‚¹ãƒˆãªæ–¹ç­–]
                        F --> C
    ```

### å®‰å…¨æ€§ã®éšå±¤

ãƒ¬ã‚¤ãƒ¤ãƒ¼ | æ©Ÿèƒ½ | å®Ÿè£…  
---|---|---  
1\. è¡Œå‹•åˆ¶ç´„ | å±é™ºãªè¡Œå‹•ã®ç¦æ­¢ | ãƒãƒ¼ãƒ‰ãƒªãƒŸãƒƒãƒˆã€ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿  
2\. ä¸ç¢ºå®Ÿæ€§è€ƒæ…® | ä¿¡é ¼åŒºé–“ã®è©•ä¾¡ | ãƒ™ã‚¤ã‚ºNNã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«  
3\. æ€§èƒ½ç›£è¦– | ç•°å¸¸æ¤œçŸ¥ | ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã€KPIãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°  
4\. äººé–“ä»‹å…¥ | ç·Šæ€¥åœæ­¢ | ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰æ©Ÿæ§‹  
  
1 ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒ€ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹Sim-to-Realè»¢ç§» 

ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã—ã¦å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€ å®Ÿç’°å¢ƒã®ä¸ç¢ºå®Ÿæ€§ã«å¯¾ã—ã¦ãƒ­ãƒã‚¹ãƒˆãªæ–¹ç­–ã‚’ç²å¾—ã—ã¾ã™ã€‚ 

**ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒ€ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ï¼š**

\\[ \theta \sim p(\Theta), \quad \pi^* = \arg\max_\pi \mathbb{E}_{\theta \sim p(\Theta)} [J(\pi; \theta)] \\] 

ã“ã“ã§ã€\\(\theta\\)ã¯ç’°å¢ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€\\(p(\Theta)\\)ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†å¸ƒ
    
    
    # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒ€ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè£…
    import numpy as np
    import torch
    import torch.nn as nn
    import torch.optim as optim
    
    class RandomizedCSTREnv:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã—ãŸCSTRç’°å¢ƒ"""
        def __init__(self, randomize=True):
            self.randomize = randomize
            self.reset()
    
        def _sample_parameters(self):
            """ç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
            if self.randomize:
                # æ´»æ€§åŒ–ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆÂ±20%ã®å¤‰å‹•ï¼‰
                self.Ea = np.random.uniform(40000, 60000)
    
                # åå¿œç†±ï¼ˆÂ±15%ã®å¤‰å‹•ï¼‰
                self.dHr = np.random.uniform(-57500, -42500)
    
                # ç†±ä¼é”ä¿‚æ•°ï¼ˆÂ±25%ã®å¤‰å‹•ï¼‰
                self.U = np.random.uniform(300, 500)
    
                # ä½“ç©ï¼ˆè£½é€ ã°ã‚‰ã¤ãÂ±5%ï¼‰
                self.V = np.random.uniform(950, 1050)
    
                # ã‚»ãƒ³ã‚µãƒ¼ãƒã‚¤ã‚ºæ¨™æº–åå·®
                self.temp_noise = np.random.uniform(0.1, 1.0)
                self.conc_noise = np.random.uniform(0.01, 0.05)
    
                # åˆ¶å¾¡é…å»¶ï¼ˆé€šä¿¡é…å»¶+ãƒãƒ«ãƒ–å¿œç­”ï¼‰
                self.control_delay = np.random.randint(1, 4)
            else:
                # å…¬ç§°å€¤
                self.Ea = 50000
                self.dHr = -50000
                self.U = 400
                self.V = 1000
                self.temp_noise = 0.5
                self.conc_noise = 0.02
                self.control_delay = 2
    
        def reset(self):
            self._sample_parameters()
            self.state = np.array([350.0, 2.0])  # [æ¸©åº¦, æ¿ƒåº¦]
            self.action_buffer = [0.5] * self.control_delay
            return self._get_observation()
    
        def _get_observation(self):
            """ãƒã‚¤ã‚ºã‚’å«ã‚€è¦³æ¸¬"""
            T, CA = self.state
            T_obs = T + np.random.normal(0, self.temp_noise)
            CA_obs = CA + np.random.normal(0, self.conc_noise)
            return np.array([T_obs, CA_obs])
    
        def step(self, action):
            """åˆ¶å¾¡é…å»¶ã‚’è€ƒæ…®ã—ãŸã‚¹ãƒ†ãƒƒãƒ—"""
            # é…å»¶ã®ã‚ã‚‹è¡Œå‹•ã‚’é©ç”¨
            self.action_buffer.append(action)
            actual_action = self.action_buffer.pop(0)
    
            T, CA = self.state
    
            # åå¿œé€Ÿåº¦ï¼ˆãƒ©ãƒ³ãƒ€ãƒ åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
            R = 8.314
            k = 1e10 * np.exp(-self.Ea / (R * T))
    
            # CSTR dynamics
            dt = 0.1
            F = 100  # æµé‡ [L/min]
            CA_in = 2.5
            Tin = 350
            rho = 1000
            Cp = 4.18
    
            # å†·å´é‡ï¼ˆè¡Œå‹•ï¼‰
            Q_cool = actual_action * 10000  # 0-10000 W
    
            # ç‰©è³ªåæ”¯
            dCA = (F / self.V) * (CA_in - CA) - k * CA
            CA_new = CA + dCA * dt
    
            # ã‚¨ãƒãƒ«ã‚®ãƒ¼åæ”¯
            Q_rxn = -self.dHr * k * CA * self.V
            Q_jacket = self.U * 10 * (T - Tin) + Q_cool
            dT = (Q_rxn - Q_jacket) / (self.V * rho * Cp)
            T_new = T + dT * dt
    
            self.state = np.array([T_new, max(0, CA_new)])
    
            # å ±é…¬
            temp_penalty = -abs(T_new - 350) ** 2 * 0.01
            production = k * CA * 10
            reward = temp_penalty + production
    
            done = T_new > 400 or T_new < 300  # å®‰å…¨ç¯„å›²å¤–
            return self._get_observation(), reward, done
    
    # ãƒ­ãƒã‚¹ãƒˆãªSACå­¦ç¿’
    class RobustSACAgent:
        """ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒ€ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½¿ã†SACã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
        def __init__(self, obs_dim, action_dim):
            self.actor = nn.Sequential(
                nn.Linear(obs_dim, 128), nn.ReLU(),
                nn.Linear(128, 128), nn.ReLU(),
                nn.Linear(128, action_dim), nn.Tanh()
            )
            self.optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)
    
        def select_action(self, obs):
            with torch.no_grad():
                return self.actor(torch.FloatTensor(obs)).numpy()
    
    # å­¦ç¿’ï¼šãƒ©ãƒ³ãƒ€ãƒ åŒ–ç’°å¢ƒã§è¨“ç·´
    train_env = RandomizedCSTREnv(randomize=True)
    agent = RobustSACAgent(obs_dim=2, action_dim=1)
    
    print("ãƒ©ãƒ³ãƒ€ãƒ åŒ–ç’°å¢ƒã§ã®å­¦ç¿’...")
    for episode in range(500):
        obs = train_env.reset()
        episode_reward = 0
    
        for step in range(100):
            action = agent.select_action(obs)
            next_obs, reward, done = train_env.step(action[0])
            episode_reward += reward
            obs = next_obs
    
            if done:
                break
    
        if episode % 100 == 0:
            print(f"Episode {episode}, Reward: {episode_reward:.2f}")
            print(f"  Env params: Ea={train_env.Ea:.0f}, V={train_env.V:.0f}, "
                  f"delay={train_env.control_delay}")
    
    # è©•ä¾¡ï¼šå…¬ç§°ç’°å¢ƒï¼ˆå®Ÿãƒ—ãƒ©ãƒ³ãƒˆæƒ³å®šï¼‰ã§ãƒ†ã‚¹ãƒˆ
    test_env = RandomizedCSTREnv(randomize=False)
    print("\nå…¬ç§°ç’°å¢ƒã§ã®ãƒ†ã‚¹ãƒˆ...")
    
    obs = test_env.reset()
    test_reward = 0
    temps = []
    
    for step in range(100):
        action = agent.select_action(obs)
        obs, reward, done = test_env.step(action[0])
        test_reward += reward
        temps.append(obs[0])
    
        if done:
            break
    
    print(f"Test Reward: {test_reward:.2f}")
    print(f"Temp Mean: {np.mean(temps):.2f}K, Std: {np.std(temps):.2f}K")

2 å®‰å…¨ãªæ¢ç´¢ï¼šè¡Œå‹•åˆ¶ç´„ã«ã‚ˆã‚‹ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿ 

å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå±é™ºãªè¡Œå‹•ã‚’å–ã‚‰ãªã„ã‚ˆã†ã€ç‰©ç†çš„ãƒ»å®‰å…¨çš„åˆ¶ç´„ã‚’èª²ã—ã¾ã™ã€‚ Control Barrier Functions (CBF)ã‚„ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å®Ÿè£…ã—ã¾ã™ã€‚ 
    
    
    ```mermaid
    graph LR
                            A[RLæ–¹ç­–Ï€] -->|å±é™ºãªè¡Œå‹•?| B[ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿]
                            B -->|å®‰å…¨ãªè¡Œå‹•| C[å®Ÿè¡Œ]
                            B -->|åˆ¶ç´„é•å| D[å®‰å…¨ãªä»£æ›¿è¡Œå‹•]
                            D --> C
    ```
    
    
    # å®‰å…¨ãªæ¢ç´¢ï¼šã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿å®Ÿè£…
    import numpy as np
    import torch
    import torch.nn as nn
    
    class SafetyConstraints:
        """CSTRã®å®‰å…¨åˆ¶ç´„"""
        def __init__(self):
            # æ¸©åº¦åˆ¶ç´„
            self.T_min = 310.0  # [K]
            self.T_max = 390.0  # [K]
            self.T_target = 350.0
    
            # æ¿ƒåº¦åˆ¶ç´„
            self.CA_min = 0.1  # [mol/L]
            self.CA_max = 3.0
    
            # åˆ¶å¾¡å…¥åŠ›åˆ¶ç´„
            self.u_min = -100.0  # æœ€å¤§å†·å´ [kW]
            self.u_max = 50.0    # æœ€å¤§åŠ ç†± [kW]
    
            # å¤‰åŒ–ç‡åˆ¶ç´„
            self.du_max = 20.0  # [kW/step]
    
        def is_safe_state(self, state):
            """çŠ¶æ…‹ãŒå®‰å…¨ã‹ãƒã‚§ãƒƒã‚¯"""
            T, CA = state
            return (self.T_min <= T <= self.T_max and
                    self.CA_min <= CA <= self.CA_max)
    
        def is_safe_action(self, state, action, prev_action):
            """è¡Œå‹•ãŒå®‰å…¨ã‹ãƒã‚§ãƒƒã‚¯"""
            # åˆ¶å¾¡å…¥åŠ›ç¯„å›²
            if not (self.u_min <= action <= self.u_max):
                return False
    
            # å¤‰åŒ–ç‡åˆ¶ç´„
            if abs(action - prev_action) > self.du_max:
                return False
    
            return True
    
        def project_to_safe(self, action, prev_action):
            """è¡Œå‹•ã‚’å®‰å…¨é ˜åŸŸã«å°„å½±"""
            # ç¯„å›²åˆ¶ç´„
            action = np.clip(action, self.u_min, self.u_max)
    
            # å¤‰åŒ–ç‡åˆ¶ç´„
            delta = action - prev_action
            if abs(delta) > self.du_max:
                action = prev_action + np.sign(delta) * self.du_max
    
            return action
    
    class ControlBarrierFunction:
        """Control Barrier Function (CBF)ã«ã‚ˆã‚‹å®‰å…¨ä¿è¨¼"""
        def __init__(self, safety_constraints):
            self.constraints = safety_constraints
            self.alpha = 0.5  # ã‚¯ãƒ©ã‚¹Ké–¢æ•°ã®ã‚²ã‚¤ãƒ³
    
        def barrier_function(self, state):
            """ãƒãƒªã‚¢é–¢æ•° h(x) >= 0 ãŒå®‰å…¨é ˜åŸŸ"""
            T, CA = state
    
            # æ¸©åº¦ãƒãƒªã‚¢ï¼ˆè·é›¢é–¢æ•°ï¼‰
            h_T_min = T - self.constraints.T_min
            h_T_max = self.constraints.T_max - T
    
            # æ¿ƒåº¦ãƒãƒªã‚¢
            h_CA_min = CA - self.constraints.CA_min
            h_CA_max = self.constraints.CA_max - CA
    
            # æœ€å°å€¤ï¼ˆæœ€ã‚‚å³ã—ã„åˆ¶ç´„ï¼‰
            return min(h_T_min, h_T_max, h_CA_min, h_CA_max)
    
        def safe_action(self, state, desired_action, env_model):
            """CBFåˆ¶ç´„ã‚’æº€ãŸã™å®‰å…¨ãªè¡Œå‹•ã‚’è¨ˆç®—"""
            h = self.barrier_function(state)
    
            # å®‰å…¨ãªé ˜åŸŸãªã‚‰ä½•ã‚‚ã—ãªã„
            if h > 10.0:
                return desired_action
    
            # å¢ƒç•Œè¿‘ãã§ã¯åˆ¶ç´„ã‚’èª²ã™
            # ç°¡æ˜“å®Ÿè£…ï¼šäºˆæ¸¬ã•ã‚Œã‚‹æ¬¡çŠ¶æ…‹ã§ãƒãƒªã‚¢æ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
            next_state_pred = env_model.predict(state, desired_action)
            h_next = self.barrier_function(next_state_pred)
    
            # CBFæ¡ä»¶: h_next >= -alpha * h
            if h_next >= -self.alpha * h:
                return desired_action
            else:
                # å®‰å…¨å´ã«ä¿®æ­£ï¼ˆä¿å®ˆçš„ãªè¡Œå‹•ï¼‰
                T, CA = state
                if T > self.constraints.T_target:
                    # å†·å´å¼·åŒ–
                    return max(desired_action, 0)
                else:
                    # åŠ ç†±æŠ‘åˆ¶
                    return min(desired_action, 0)
    
    class SimpleCSTRModel:
        """CSTRã®ç°¡æ˜“äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«"""
        def predict(self, state, action, dt=0.1):
            T, CA = state
            k = 1e10 * np.exp(-50000 / (8.314 * T))
    
            # ç°¡æ˜“dynamics
            dCA = -k * CA * dt
            dT = (action * 1000 - 400 * (T - 350)) / 4180 * dt
    
            return np.array([T + dT, CA + dCA])
    
    # ä½¿ç”¨ä¾‹
    safety = SafetyConstraints()
    cbf = ControlBarrierFunction(safety)
    model = SimpleCSTRModel()
    
    # RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•ã«ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
    class SafeRLAgent:
        def __init__(self, base_agent, safety_filter):
            self.base_agent = base_agent
            self.safety_filter = safety_filter
            self.prev_action = 0.0
    
        def select_safe_action(self, state):
            # ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•
            desired_action = self.base_agent.select_action(state)[0] * 100
    
            # ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
            safe_action = self.safety_filter.project_to_safe(
                desired_action, self.prev_action)
    
            # CBFåˆ¶ç´„
            safe_action = cbf.safe_action(state, safe_action, model)
    
            self.prev_action = safe_action
            return safe_action
    
    # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    from example1 import RobustSACAgent, RandomizedCSTREnv
    
    base_agent = RobustSACAgent(obs_dim=2, action_dim=1)
    safe_agent = SafeRLAgent(base_agent, safety)
    env = RandomizedCSTREnv(randomize=False)
    
    print("å®‰å…¨åˆ¶ç´„ä»˜ãå®Ÿè¡Œ...")
    state = env.reset()
    unsafe_count = 0
    
    for step in range(200):
        action = safe_agent.select_safe_action(state)
        next_state, reward, done = env.step(action / 100)
    
        if not safety.is_safe_state(state):
            unsafe_count += 1
            print(f"Step {step}: UNSAFE STATE! T={state[0]:.1f}K, CA={state[1]:.3f}")
    
        state = next_state
    
        if done:
            break
    
    print(f"\nUnsafe states encountered: {unsafe_count} / {step+1}")
    print(f"Safety rate: {(1 - unsafe_count/(step+1))*100:.1f}%")

3 Conservative Q-Learningï¼ˆCQLï¼‰ï¼šä¿å®ˆçš„ãªã‚ªãƒ•ãƒ©ã‚¤ãƒ³å­¦ç¿’ 

å®Ÿãƒ—ãƒ©ãƒ³ãƒˆã§ã¯æ¢ç´¢ãŒå±é™ºãªãŸã‚ã€éå»ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§å­¦ç¿’ã—ã¾ã™ã€‚ CQLã¯åˆ†å¸ƒå¤–è¡Œå‹•ã®Qå€¤ã‚’éå°è©•ä¾¡ã—ã€å®‰å…¨ãªæ–¹ç­–ã‚’å­¦ç¿’ã—ã¾ã™ã€‚ 

**CQLç›®çš„é–¢æ•°ï¼š**

\\[ \min_Q \alpha \cdot \mathbb{E}_{s \sim \mathcal{D}} \left[ \log \sum_a \exp(Q(s,a)) - \mathbb{E}_{a \sim \mu(a|s)} [Q(s,a)] \right] + \mathcal{L}_{TD}(Q) \\] 

ç¬¬1é …ï¼šåˆ†å¸ƒå¤–è¡Œå‹•ã®Qå€¤ã‚’ä¸‹ã’ã‚‹ã€ç¬¬2é …ï¼šãƒ‡ãƒ¼ã‚¿å†…è¡Œå‹•ã®Qå€¤ã‚’ä¿ã¤
    
    
    # Conservative Q-Learning (CQL) å®Ÿè£…
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import numpy as np
    from collections import deque
    import random
    
    class CQLQNetwork(nn.Module):
        """CQLç”¨ã®Qé–¢æ•°"""
        def __init__(self, state_dim, action_dim):
            super().__init__()
            self.net = nn.Sequential(
                nn.Linear(state_dim + action_dim, 256), nn.ReLU(),
                nn.Linear(256, 256), nn.ReLU(),
                nn.Linear(256, 1)
            )
    
        def forward(self, state, action):
            x = torch.cat([state, action], dim=-1)
            return self.net(x)
    
    class CQLAgent:
        """Conservative Q-Learning ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
        def __init__(self, state_dim, action_dim, alpha=1.0):
            self.state_dim = state_dim
            self.action_dim = action_dim
            self.alpha = alpha  # CQLæ­£å‰‡åŒ–ä¿‚æ•°
    
            self.q_net = CQLQNetwork(state_dim, action_dim)
            self.target_q = CQLQNetwork(state_dim, action_dim)
            self.target_q.load_state_dict(self.q_net.state_dict())
    
            self.policy = nn.Sequential(
                nn.Linear(state_dim, 256), nn.ReLU(),
                nn.Linear(256, action_dim), nn.Tanh()
            )
    
            self.q_optimizer = optim.Adam(self.q_net.parameters(), lr=3e-4)
            self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=3e-4)
    
        def select_action(self, state, deterministic=False):
            with torch.no_grad():
                state_t = torch.FloatTensor(state).unsqueeze(0)
                action = self.policy(state_t)
                if not deterministic:
                    action += torch.randn_like(action) * 0.1
                return action.squeeze().numpy()
    
        def train_step(self, batch):
            """CQLæ›´æ–°"""
            states, actions, rewards, next_states, dones = batch
    
            states_t = torch.FloatTensor(states)
            actions_t = torch.FloatTensor(actions)
            rewards_t = torch.FloatTensor(rewards).unsqueeze(1)
            next_states_t = torch.FloatTensor(next_states)
            dones_t = torch.FloatTensor(dones).unsqueeze(1)
    
            # Qé–¢æ•°ã®æ›´æ–°
            # 1. Bellmanèª¤å·®
            with torch.no_grad():
                next_actions = self.policy(next_states_t)
                target_q = rewards_t + 0.99 * self.target_q(
                    next_states_t, next_actions) * (1 - dones_t)
    
            current_q = self.q_net(states_t, actions_t)
            bellman_loss = nn.MSELoss()(current_q, target_q)
    
            # 2. CQLæ­£å‰‡åŒ–é …
            # ãƒ©ãƒ³ãƒ€ãƒ è¡Œå‹•ã®Qå€¤ã‚’è¨ˆç®—
            num_random = 10
            random_actions = torch.FloatTensor(
                np.random.uniform(-1, 1, (states_t.shape[0], num_random, self.action_dim)))
    
            random_q = []
            for i in range(num_random):
                q = self.q_net(states_t, random_actions[:, i, :])
                random_q.append(q)
            random_q = torch.cat(random_q, dim=1)
    
            # ãƒãƒªã‚·ãƒ¼ã«ã‚ˆã‚‹è¡Œå‹•ã®Qå€¤
            policy_actions = self.policy(states_t)
            policy_q = self.q_net(states_t, policy_actions)
    
            # CQLé …ï¼šãƒ©ãƒ³ãƒ€ãƒ è¡Œå‹•ã®Qå€¤ã‚’å¤§ããã€ãƒ‡ãƒ¼ã‚¿å†…è¡Œå‹•ã‚’å°ã•ãè©•ä¾¡
            cql_loss = (torch.logsumexp(random_q, dim=1).mean() -
                        policy_q.mean())
    
            # åˆè¨ˆæå¤±
            q_loss = bellman_loss + self.alpha * cql_loss
    
            self.q_optimizer.zero_grad()
            q_loss.backward()
            self.q_optimizer.step()
    
            # 3. ãƒãƒªã‚·ãƒ¼æ›´æ–°ï¼ˆQå€¤æœ€å¤§åŒ–ï¼‰
            policy_loss = -self.q_net(states_t, self.policy(states_t)).mean()
    
            self.policy_optimizer.zero_grad()
            policy_loss.backward()
            self.policy_optimizer.step()
    
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ›´æ–°
            for target_param, param in zip(self.target_q.parameters(),
                                            self.q_net.parameters()):
                target_param.data.copy_(0.995 * target_param.data + 0.005 * param.data)
    
            return q_loss.item(), cql_loss.item(), policy_loss.item()
    
    # ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆï¼ˆéå»ã®é‹è»¢ãƒ‡ãƒ¼ã‚¿ï¼‰
    def generate_offline_data(n_trajectories=100):
        from example1 import RandomizedCSTREnv
    
        env = RandomizedCSTREnv(randomize=True)
        dataset = []
    
        for _ in range(n_trajectories):
            state = env.reset()
            for _ in range(50):
                # ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒªã‚·ãƒ¼ + ãƒã‚¤ã‚ºï¼ˆç¾å®Ÿçš„ãªãƒ‡ãƒ¼ã‚¿ï¼‰
                action = np.random.uniform(-1, 1, 1)
                next_state, reward, done = env.step(action[0])
                dataset.append((state, action, reward, next_state, done))
                state = next_state
                if done:
                    break
    
        return dataset
    
    # CQLå­¦ç¿’
    print("ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ...")
    offline_data = generate_offline_data(n_trajectories=200)
    print(f"Dataset size: {len(offline_data)}")
    
    agent = CQLAgent(state_dim=2, action_dim=1, alpha=1.0)
    
    print("\nCQLå­¦ç¿’ä¸­...")
    batch_size = 256
    for epoch in range(100):
        # ãƒŸãƒ‹ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        batch = random.sample(offline_data, min(batch_size, len(offline_data)))
        states, actions, rewards, next_states, dones = zip(*batch)
    
        batch_tuple = (
            np.array(states),
            np.array(actions),
            np.array(rewards),
            np.array(next_states),
            np.array(dones)
        )
    
        q_loss, cql_loss, p_loss = agent.train_step(batch_tuple)
    
        if epoch % 20 == 0:
            print(f"Epoch {epoch}: Q_loss={q_loss:.3f}, CQL_loss={cql_loss:.3f}, "
                  f"Policy_loss={p_loss:.3f}")
    
    # ãƒ†ã‚¹ãƒˆ
    print("\nCQLæ–¹ç­–ã®ãƒ†ã‚¹ãƒˆ...")
    from example1 import RandomizedCSTREnv
    test_env = RandomizedCSTREnv(randomize=False)
    
    state = test_env.reset()
    total_reward = 0
    
    for step in range(100):
        action = agent.select_action(state, deterministic=True)
        next_state, reward, done = test_env.step(action[0])
        total_reward += reward
        state = next_state
        if done:
            break
    
    print(f"Test Reward: {total_reward:.2f}")

4 Human-in-the-Loopï¼šäººé–“ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰æ©Ÿæ§‹ 

AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒäºˆæœŸã—ãªã„è¡Œå‹•ã‚’ã¨ã‚‹å ´åˆã«å‚™ãˆã€ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ãŒä»‹å…¥ã§ãã‚‹æ©Ÿæ§‹ãŒå¿…è¦ã§ã™ã€‚ ä»‹å…¥åˆ¤æ–­ã€ã‚¹ãƒ ãƒ¼ã‚ºãªç§»è¡Œã€å±¥æ­´è¨˜éŒ²ã‚’å®Ÿè£…ã—ã¾ã™ã€‚ 
    
    
    ```mermaid
    graph TD
                            A[AIåˆ¶å¾¡] -->|ç•°å¸¸æ¤œçŸ¥| B{äººé–“åˆ¤æ–­}
                            B -->|OK| A
                            B -->|ä»‹å…¥| C[æ‰‹å‹•åˆ¶å¾¡]
                            C -->|å®‰å®šåŒ–| D{å¾©å¸°æ¡ä»¶}
                            D -->|æº€ãŸã™| A
                            D -->|æº€ãŸã•ãªã„| C
    ```
    
    
    # Human-in-the-Loop ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…
    import numpy as np
    import time
    from datetime import datetime
    from enum import Enum
    
    class ControlMode(Enum):
        """åˆ¶å¾¡ãƒ¢ãƒ¼ãƒ‰"""
        AI_CONTROL = "AI"
        HUMAN_OVERRIDE = "Human"
        TRANSITION = "Transition"
    
    class HumanOverrideSystem:
        """äººé–“ä»‹å…¥ã‚·ã‚¹ãƒ†ãƒ """
        def __init__(self):
            self.mode = ControlMode.AI_CONTROL
            self.intervention_history = []
            self.confidence_threshold = 0.7
    
        def check_intervention_needed(self, state, ai_action, confidence):
            """ä»‹å…¥ãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯"""
            T, CA = state
    
            # ãƒˆãƒªã‚¬ãƒ¼æ¡ä»¶
            triggers = {
                'high_temp': T > 380,
                'low_temp': T < 320,
                'low_confidence': confidence < self.confidence_threshold,
                'extreme_action': abs(ai_action) > 0.9,
                'unstable_state': CA < 0.2 or CA > 2.8
            }
    
            if any(triggers.values()):
                reason = [k for k, v in triggers.items() if v]
                return True, reason
            return False, []
    
        def request_human_action(self, state, ai_suggestion):
            """ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ã«è¡Œå‹•ã‚’å•ã„åˆã‚ã›ï¼ˆå®Ÿéš›ã¯GUI/CLIï¼‰"""
            T, CA = state
            print(f"\n{'='*60}")
            print(f"äººé–“ä»‹å…¥è¦æ±‚ï¼")
            print(f"ç¾åœ¨ã®çŠ¶æ…‹: T={T:.2f}K, CA={CA:.3f}mol/L")
            print(f"AIææ¡ˆè¡Œå‹•: {ai_suggestion:.3f}")
            print(f"{'='*60}")
    
            # ç°¡ç•¥åŒ–ï¼šãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            # å®Ÿéš›ã¯ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ã®å…¥åŠ›ã‚’å¾…ã¤
            if T > 380:
                human_action = -0.8  # å¼·å†·å´
                override = True
            elif T < 320:
                human_action = 0.6   # åŠ ç†±
                override = True
            else:
                human_action = ai_suggestion
                override = False
    
            return human_action, override
    
        def smooth_transition(self, from_action, to_action, alpha=0.3):
            """ã‚¹ãƒ ãƒ¼ã‚ºãªåˆ¶å¾¡ç§»è¡Œ"""
            return alpha * to_action + (1 - alpha) * from_action
    
        def log_intervention(self, timestamp, state, ai_action, human_action, reason):
            """ä»‹å…¥å±¥æ­´è¨˜éŒ²"""
            log_entry = {
                'timestamp': timestamp,
                'state': state.copy(),
                'ai_action': ai_action,
                'human_action': human_action,
                'reason': reason
            }
            self.intervention_history.append(log_entry)
    
        def generate_report(self):
            """ä»‹å…¥ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
            if not self.intervention_history:
                return "No interventions recorded."
    
            report = "\n" + "="*60 + "\n"
            report += "Human Intervention Report\n"
            report += "="*60 + "\n"
            report += f"Total interventions: {len(self.intervention_history)}\n\n"
    
            for i, entry in enumerate(self.intervention_history):
                report += f"Intervention {i+1}:\n"
                report += f"  Time: {entry['timestamp']}\n"
                report += f"  State: T={entry['state'][0]:.2f}K, CA={entry['state'][1]:.3f}\n"
                report += f"  AI action: {entry['ai_action']:.3f}\n"
                report += f"  Human action: {entry['human_action']:.3f}\n"
                report += f"  Reason: {', '.join(entry['reason'])}\n\n"
    
            return report
    
    class HITLController:
        """Human-in-the-Loopåˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ """
        def __init__(self, ai_agent, override_system):
            self.ai_agent = ai_agent
            self.override_system = override_system
            self.prev_action = 0.0
    
        def select_action(self, state, confidence=1.0):
            """äººé–“ä»‹å…¥ã‚’è€ƒæ…®ã—ãŸè¡Œå‹•é¸æŠ"""
            # AIææ¡ˆ
            ai_action = self.ai_agent.select_action(state)[0]
    
            # ä»‹å…¥åˆ¤å®š
            need_intervention, reasons = self.override_system.check_intervention_needed(
                state, ai_action, confidence)
    
            if need_intervention:
                # äººé–“ã«å•ã„åˆã‚ã›
                human_action, overridden = self.override_system.request_human_action(
                    state, ai_action)
    
                if overridden:
                    # ä»‹å…¥è¨˜éŒ²
                    self.override_system.log_intervention(
                        datetime.now(), state, ai_action, human_action, reasons)
                    self.override_system.mode = ControlMode.HUMAN_OVERRIDE
                    final_action = human_action
                else:
                    final_action = ai_action
            else:
                final_action = ai_action
                self.override_system.mode = ControlMode.AI_CONTROL
    
            # ã‚¹ãƒ ãƒ¼ã‚ºé·ç§»
            final_action = self.override_system.smooth_transition(
                self.prev_action, final_action)
    
            self.prev_action = final_action
            return final_action
    
    # ä½¿ç”¨ä¾‹
    from example1 import RobustSACAgent, RandomizedCSTREnv
    
    print("Human-in-the-Loop ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•...")
    
    ai_agent = RobustSACAgent(obs_dim=2, action_dim=1)
    override_system = HumanOverrideSystem()
    controller = HITLController(ai_agent, override_system)
    
    env = RandomizedCSTREnv(randomize=True)
    state = env.reset()
    
    # éé…·ãªæ¡ä»¶ã§ãƒ†ã‚¹ãƒˆ
    for step in range(50):
        # ä¿¡é ¼åº¦ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„çŠ¶æ³ï¼‰
        confidence = np.random.uniform(0.5, 1.0)
    
        action = controller.select_action(state, confidence)
        next_state, reward, done = env.step(action)
    
        print(f"Step {step}: Mode={override_system.mode.value}, "
              f"T={state[0]:.1f}K, Action={action:.3f}")
    
        state = next_state
    
        if done:
            print("ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ï¼ˆç•°å¸¸çŠ¶æ…‹ï¼‰")
            break
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print(override_system.generate_report())

5 ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–ï¼šãƒ™ã‚¤ã‚ºNNã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³• 

AIã®äºˆæ¸¬ãŒã©ã‚Œã ã‘ç¢ºã‹ã‚‰ã—ã„ã‹ã‚’å®šé‡åŒ–ã™ã‚‹ã“ã¨ã§ã€ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„çŠ¶æ³ã§ã¯ä¿å®ˆçš„ãªè¡Œå‹•ã‚’å–ã‚Šã¾ã™ã€‚ ãƒ™ã‚¤ã‚ºãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã§ä¿¡é ¼åŒºé–“ã‚’æ¨å®šã—ã¾ã™ã€‚ 
    
    
    # ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–ï¼šã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã¨ãƒ™ã‚¤ã‚ºè¿‘ä¼¼
    import torch
    import torch.nn as nn
    import numpy as np
    
    class EnsembleQNetwork:
        """Qé–¢æ•°ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«"""
        def __init__(self, state_dim, action_dim, n_models=5):
            self.n_models = n_models
            self.models = [
                nn.Sequential(
                    nn.Linear(state_dim + action_dim, 128), nn.ReLU(),
                    nn.Linear(128, 128), nn.ReLU(),
                    nn.Linear(128, 1)
                ) for _ in range(n_models)
            ]
            self.optimizers = [
                torch.optim.Adam(m.parameters(), lr=1e-3) for m in self.models
            ]
    
        def predict_with_uncertainty(self, state, action):
            """äºˆæ¸¬å¹³å‡ã¨ä¸ç¢ºå®Ÿæ€§ï¼ˆæ¨™æº–åå·®ï¼‰ã‚’è¿”ã™"""
            state_t = torch.FloatTensor(state).unsqueeze(0)
            action_t = torch.FloatTensor(action).unsqueeze(0)
            x = torch.cat([state_t, action_t], dim=-1)
    
            predictions = []
            for model in self.models:
                with torch.no_grad():
                    pred = model(x).item()
                predictions.append(pred)
    
            mean = np.mean(predictions)
            std = np.std(predictions)
    
            return mean, std
    
        def train_step(self, batch):
            """å…¨ãƒ¢ãƒ‡ãƒ«ã‚’ç•°ãªã‚‹ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒ«ã§å­¦ç¿’"""
            states, actions, targets = batch
    
            losses = []
            for i, (model, opt) in enumerate(zip(self.models, self.optimizers)):
                # ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                indices = np.random.choice(len(states), len(states), replace=True)
                s = torch.FloatTensor(states[indices])
                a = torch.FloatTensor(actions[indices])
                t = torch.FloatTensor(targets[indices])
    
                x = torch.cat([s, a], dim=-1)
                pred = model(x).squeeze()
    
                loss = nn.MSELoss()(pred, t)
                opt.zero_grad()
                loss.backward()
                opt.step()
    
                losses.append(loss.item())
    
            return np.mean(losses)
    
    class MCDropoutQNetwork(nn.Module):
        """Monte Carlo Dropoutï¼ˆãƒ™ã‚¤ã‚ºè¿‘ä¼¼ï¼‰"""
        def __init__(self, state_dim, action_dim, dropout_rate=0.1):
            super().__init__()
            self.net = nn.Sequential(
                nn.Linear(state_dim + action_dim, 128),
                nn.ReLU(),
                nn.Dropout(dropout_rate),
                nn.Linear(128, 128),
                nn.ReLU(),
                nn.Dropout(dropout_rate),
                nn.Linear(128, 1)
            )
    
        def forward(self, state, action):
            x = torch.cat([state, action], dim=-1)
            return self.net(x)
    
        def predict_with_uncertainty(self, state, action, n_samples=20):
            """MC Dropoutã§ä¸ç¢ºå®Ÿæ€§æ¨å®š"""
            self.train()  # Dropoutã‚’æœ‰åŠ¹åŒ–
    
            state_t = torch.FloatTensor(state).unsqueeze(0)
            action_t = torch.FloatTensor(action).unsqueeze(0)
    
            predictions = []
            for _ in range(n_samples):
                pred = self.forward(state_t, action_t).item()
                predictions.append(pred)
    
            mean = np.mean(predictions)
            std = np.std(predictions)
    
            return mean, std
    
    class UncertaintyAwareAgent:
        """ä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®ã—ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
        def __init__(self, state_dim, action_dim, method='ensemble'):
            self.method = method
            if method == 'ensemble':
                self.q_network = EnsembleQNetwork(state_dim, action_dim, n_models=5)
            else:
                self.q_network = MCDropoutQNetwork(state_dim, action_dim)
    
            self.policy = nn.Sequential(
                nn.Linear(state_dim, 128), nn.ReLU(),
                nn.Linear(128, action_dim), nn.Tanh()
            )
            self.uncertainty_threshold = 0.5  # ä¸ç¢ºå®Ÿæ€§ã®é–¾å€¤
    
        def select_action(self, state):
            """ä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®ã—ãŸè¡Œå‹•é¸æŠ"""
            with torch.no_grad():
                nominal_action = self.policy(torch.FloatTensor(state)).numpy()
    
            # è¤‡æ•°å€™è£œã®ä¸ç¢ºå®Ÿæ€§è©•ä¾¡
            action_candidates = [
                nominal_action,
                nominal_action * 0.5,  # ä¿å®ˆçš„
                np.zeros_like(nominal_action)  # ç¾çŠ¶ç¶­æŒ
            ]
    
            best_action = nominal_action
            min_uncertainty = float('inf')
    
            for action in action_candidates:
                if self.method == 'ensemble':
                    q_mean, q_std = self.q_network.predict_with_uncertainty(state, action)
                else:
                    q_mean, q_std = self.q_network.predict_with_uncertainty(state, action)
    
                # ä¸ç¢ºå®Ÿæ€§ãŒä½ãã€Qå€¤ãŒé«˜ã„è¡Œå‹•ã‚’é¸æŠ
                if q_std < self.uncertainty_threshold and q_std < min_uncertainty:
                    best_action = action
                    min_uncertainty = q_std
    
            return best_action, min_uncertainty
    
    # ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    print("ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–ãƒ‡ãƒ¢\n")
    
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•
    ensemble_agent = UncertaintyAwareAgent(state_dim=2, action_dim=1, method='ensemble')
    
    test_states = [
        np.array([350.0, 1.5]),  # æ­£å¸¸
        np.array([385.0, 0.8]),  # é«˜æ¸©
        np.array([310.0, 2.5])   # ä½æ¸©
    ]
    
    print("Ensemble Method:")
    for i, state in enumerate(test_states):
        action, uncertainty = ensemble_agent.select_action(state)
        print(f"State {i+1}: T={state[0]}K, CA={state[1]}")
        print(f"  Action: {action[0]:.3f}, Uncertainty: {uncertainty:.3f}")
    
        if uncertainty > ensemble_agent.uncertainty_threshold:
            print(f"  WARNING: High uncertainty! Consider human oversight.")
        print()
    
    # MC Dropoutæ‰‹æ³•
    mc_agent = UncertaintyAwareAgent(state_dim=2, action_dim=1, method='mcdropout')
    
    print("\nMC Dropout Method:")
    for i, state in enumerate(test_states):
        action, uncertainty = mc_agent.select_action(state)
        print(f"State {i+1}: T={state[0]}K, CA={state[1]}")
        print(f"  Action: {action[0]:.3f}, Uncertainty: {uncertainty:.3f}")
    
        if uncertainty > mc_agent.uncertainty_threshold:
            print(f"  WARNING: High uncertainty!")
        print()

6 æ€§èƒ½ç›£è¦–ã¨ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º 

ãƒ‡ãƒ—ãƒ­ã‚¤å¾Œã‚‚ç¶™ç¶šçš„ã«æ€§èƒ½ã‚’ç›£è¦–ã—ã€ãƒ—ãƒ©ãƒ³ãƒˆã®çµŒå¹´åŠ£åŒ–ã‚„ãƒ¢ãƒ‡ãƒ«ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡ºã—ã¾ã™ã€‚ KPIãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã€çµ±è¨ˆçš„æ¤œå®šã€ç•°å¸¸æ¤œçŸ¥ã‚’å®Ÿè£…ã—ã¾ã™ã€‚ 
    
    
    # æ€§èƒ½ç›£è¦–ã¨ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 
    import numpy as np
    from collections import deque
    from scipy import stats
    
    class PerformanceMonitor:
        """æ€§èƒ½ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ """
        def __init__(self, window_size=100):
            self.window_size = window_size
    
            # KPIå±¥æ­´
            self.rewards = deque(maxlen=window_size)
            self.temperatures = deque(maxlen=window_size)
            self.concentrations = deque(maxlen=window_size)
            self.actions = deque(maxlen=window_size)
    
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµ±è¨ˆ
            self.baseline_reward_mean = None
            self.baseline_reward_std = None
    
            # ç•°å¸¸ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
            self.anomaly_count = 0
            self.total_steps = 0
    
        def update(self, state, action, reward):
            """KPIæ›´æ–°"""
            T, CA = state
            self.rewards.append(reward)
            self.temperatures.append(T)
            self.concentrations.append(CA)
            self.actions.append(action)
            self.total_steps += 1
    
        def set_baseline(self):
            """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½ã‚’è¨­å®š"""
            if len(self.rewards) >= self.window_size:
                self.baseline_reward_mean = np.mean(self.rewards)
                self.baseline_reward_std = np.std(self.rewards)
                print(f"Baseline set: Î¼={self.baseline_reward_mean:.2f}, "
                      f"Ïƒ={self.baseline_reward_std:.2f}")
    
        def detect_drift(self, alpha=0.05):
            """çµ±è¨ˆçš„ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºï¼ˆtæ¤œå®šï¼‰"""
            if self.baseline_reward_mean is None or len(self.rewards) < 50:
                return False, None
    
            current_mean = np.mean(list(self.rewards)[-50:])
    
            # tæ¤œå®š
            t_stat, p_value = stats.ttest_1samp(
                list(self.rewards)[-50:],
                self.baseline_reward_mean
            )
    
            drift_detected = p_value < alpha and current_mean < self.baseline_reward_mean
    
            return drift_detected, {
                't_stat': t_stat,
                'p_value': p_value,
                'current_mean': current_mean,
                'baseline_mean': self.baseline_reward_mean
            }
    
        def detect_anomaly(self, state, action):
            """ç•°å¸¸æ¤œå‡ºï¼ˆ3Ïƒãƒ«ãƒ¼ãƒ«ï¼‰"""
            if len(self.rewards) < 30:
                return False
    
            T, CA = state
    
            # æ¸©åº¦ç•°å¸¸
            temp_mean = np.mean(self.temperatures)
            temp_std = np.std(self.temperatures)
            temp_anomaly = abs(T - temp_mean) > 3 * temp_std
    
            # æ¿ƒåº¦ç•°å¸¸
            conc_mean = np.mean(self.concentrations)
            conc_std = np.std(self.concentrations)
            conc_anomaly = abs(CA - conc_mean) > 3 * conc_std
    
            # è¡Œå‹•ç•°å¸¸
            action_mean = np.mean(self.actions)
            action_std = np.std(self.actions)
            action_anomaly = abs(action - action_mean) > 3 * action_std
    
            is_anomaly = temp_anomaly or conc_anomaly or action_anomaly
    
            if is_anomaly:
                self.anomaly_count += 1
    
            return is_anomaly
    
        def generate_report(self):
            """ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
            if len(self.rewards) == 0:
                return "No data collected."
    
            report = "\n" + "="*60 + "\n"
            report += "Performance Monitoring Report\n"
            report += "="*60 + "\n\n"
    
            report += f"Total steps: {self.total_steps}\n"
            report += f"Anomalies detected: {self.anomaly_count} "
            report += f"({self.anomaly_count/self.total_steps*100:.2f}%)\n\n"
    
            report += "KPI Statistics (last {} steps):\n".format(len(self.rewards))
            report += f"  Reward: Î¼={np.mean(self.rewards):.2f}, "
            report += f"Ïƒ={np.std(self.rewards):.2f}\n"
            report += f"  Temperature: Î¼={np.mean(self.temperatures):.2f}K, "
            report += f"Ïƒ={np.std(self.temperatures):.2f}K\n"
            report += f"  Concentration: Î¼={np.mean(self.concentrations):.3f}, "
            report += f"Ïƒ={np.std(self.concentrations):.3f}\n"
            report += f"  Action: Î¼={np.mean(self.actions):.3f}, "
            report += f"Ïƒ={np.std(self.actions):.3f}\n\n"
    
            # ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º
            drift, drift_info = self.detect_drift()
            if drift:
                report += "WARNING: Performance drift detected!\n"
                report += f"  Current mean reward: {drift_info['current_mean']:.2f}\n"
                report += f"  Baseline mean reward: {drift_info['baseline_mean']:.2f}\n"
                report += f"  p-value: {drift_info['p_value']:.4f}\n"
            else:
                report += "No significant drift detected.\n"
    
            report += "="*60 + "\n"
            return report
    
    class DriftDetector:
        """ã‚ˆã‚Šé«˜åº¦ãªãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºï¼ˆADWINï¼‰"""
        def __init__(self, delta=0.002):
            self.delta = delta
            self.window = deque()
            self.drift_detected = False
    
        def add_element(self, value):
            """ãƒ‡ãƒ¼ã‚¿ç‚¹ã‚’è¿½åŠ ã—ã¦ãƒ‰ãƒªãƒ•ãƒˆãƒã‚§ãƒƒã‚¯"""
            self.window.append(value)
    
            # ç°¡æ˜“ç‰ˆADWIN: ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’2åˆ†å‰²ã—ã¦å¹³å‡ã‚’æ¯”è¼ƒ
            if len(self.window) > 50:
                mid = len(self.window) // 2
                window1 = list(self.window)[:mid]
                window2 = list(self.window)[mid:]
    
                # Welchã®tæ¤œå®šï¼ˆç­‰åˆ†æ•£ã‚’ä»®å®šã—ãªã„ï¼‰
                t_stat, p_value = stats.ttest_ind(window1, window2, equal_var=False)
    
                if p_value < self.delta:
                    self.drift_detected = True
                    self.window.clear()  # ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºå¾Œãƒªã‚»ãƒƒãƒˆ
                    return True
    
            return False
    
    # ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    from example1 import RandomizedCSTREnv, RobustSACAgent
    
    print("æ€§èƒ½ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•...\n")
    
    env = RandomizedCSTREnv(randomize=False)
    agent = RobustSACAgent(obs_dim=2, action_dim=1)
    monitor = PerformanceMonitor(window_size=100)
    drift_detector = DriftDetector()
    
    # ãƒ•ã‚§ãƒ¼ã‚º1: æ­£å¸¸é‹è»¢ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¨­å®šï¼‰
    print("Phase 1: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¨­å®šä¸­...")
    state = env.reset()
    
    for step in range(100):
        action = agent.select_action(state)
        next_state, reward, done = env.step(action[0])
    
        monitor.update(state, action[0], reward)
    
        state = next_state if not done else env.reset()
    
    monitor.set_baseline()
    
    # ãƒ•ã‚§ãƒ¼ã‚º2: åŠ£åŒ–ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    print("\nPhase 2: ãƒ—ãƒ©ãƒ³ãƒˆåŠ£åŒ–ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ...")
    
    for step in range(200):
        action = agent.select_action(state)
    
        # æ™‚é–“çµŒéã§æ€§èƒ½åŠ£åŒ–ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        degradation = step / 200 * 0.3
        next_state, reward, done = env.step(action[0])
        reward -= degradation * 10  # æ€§èƒ½ä½ä¸‹
    
        monitor.update(state, action[0], reward)
    
        # ç•°å¸¸æ¤œå‡º
        if monitor.detect_anomaly(state, action[0]):
            print(f"Step {step+100}: Anomaly detected!")
    
        # ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º
        if drift_detector.add_element(reward):
            print(f"Step {step+100}: DRIFT DETECTED by ADWIN!")
    
        state = next_state if not done else env.reset()
    
        # å®šæœŸçš„ãªãƒ‰ãƒªãƒ•ãƒˆãƒã‚§ãƒƒã‚¯
        if step % 50 == 0:
            drift, info = monitor.detect_drift()
            if drift:
                print(f"\nStep {step+100}: Statistical drift detected!")
                print(f"  Current: {info['current_mean']:.2f}, "
                      f"Baseline: {info['baseline_mean']:.2f}")
    
    # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
    print(monitor.generate_report())

7 çµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ 

ã“ã‚Œã¾ã§ã®å…¨è¦ç´ ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒ€ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã€å®‰å…¨åˆ¶ç´„ã€äººé–“ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã€ ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–ã€æ€§èƒ½ç›£è¦–ï¼‰ã‚’çµ±åˆã—ãŸå®Ÿè·µçš„ãªãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚ 
    
    
    ```mermaid
    graph TD
                            A[ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿] --> B[å‰å‡¦ç†]
                            B --> C[ä¸ç¢ºå®Ÿæ€§æ¨å®š]
                            C --> D{ä¸ç¢ºå®Ÿæ€§é«˜?}
                            D -->|Yes| E[ä¿å®ˆçš„è¡Œå‹•]
                            D -->|No| F[AIæ–¹ç­–]
                            F --> G[å®‰å…¨ãƒ•ã‚£ãƒ«ã‚¿]
                            E --> G
                            G --> H{å®‰å…¨?}
                            H -->|No| I[äººé–“ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰]
                            H -->|Yes| J[ã‚¢ã‚¯ãƒãƒ¥ã‚¨ãƒ¼ã‚¿]
                            I --> J
                            J --> K[æ€§èƒ½ç›£è¦–]
                            K --> L{ãƒ‰ãƒªãƒ•ãƒˆ?}
                            L -->|Yes| M[ã‚¢ãƒ©ãƒ¼ãƒˆ+å†å­¦ç¿’]
                            L -->|No| A
    ```
    
    
    # çµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
    import numpy as np
    import torch
    from datetime import datetime
    
    class IntegratedDeploymentSystem:
        """å…¨æ©Ÿèƒ½çµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ """
        def __init__(self, agent, env):
            # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
            self.agent = agent
            self.env = env
    
            # ä¾‹2: å®‰å…¨åˆ¶ç´„
            from example2 import SafetyConstraints, ControlBarrierFunction, SimpleCSTRModel
            self.safety = SafetyConstraints()
            self.cbf = ControlBarrierFunction(self.safety)
            self.model = SimpleCSTRModel()
    
            # ä¾‹4: äººé–“ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
            from example4 import HumanOverrideSystem, ControlMode
            self.override_system = HumanOverrideSystem()
    
            # ä¾‹5: ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–
            from example5 import EnsembleQNetwork
            self.uncertainty_estimator = EnsembleQNetwork(
                state_dim=2, action_dim=1, n_models=5)
    
            # ä¾‹6: æ€§èƒ½ç›£è¦–
            from example6 import PerformanceMonitor, DriftDetector
            self.monitor = PerformanceMonitor(window_size=100)
            self.drift_detector = DriftDetector()
    
            # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
            self.prev_action = 0.0
            self.running = True
            self.emergency_stop = False
    
        def preprocess_observation(self, raw_obs):
            """ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†"""
            # å¤–ã‚Œå€¤é™¤å»ï¼ˆç°¡æ˜“ï¼‰
            T, CA = raw_obs
            T = np.clip(T, 250, 450)
            CA = np.clip(CA, 0, 5)
            return np.array([T, CA])
    
        def estimate_uncertainty(self, state, action):
            """ä¸ç¢ºå®Ÿæ€§æ¨å®š"""
            q_mean, q_std = self.uncertainty_estimator.predict_with_uncertainty(
                state, action)
            return q_std
    
        def apply_safety_filter(self, state, action):
            """å®‰å…¨ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨"""
            # åˆ¶ç´„å°„å½±
            safe_action = self.safety.project_to_safe(action, self.prev_action)
    
            # CBFåˆ¶ç´„
            safe_action = self.cbf.safe_action(state, safe_action, self.model)
    
            return safe_action
    
        def check_human_override(self, state, action, uncertainty):
            """äººé–“ä»‹å…¥ãƒã‚§ãƒƒã‚¯"""
            need_intervention, reasons = self.override_system.check_intervention_needed(
                state, action, confidence=1.0 - uncertainty)
    
            if need_intervention:
                human_action, overridden = self.override_system.request_human_action(
                    state, action)
    
                if overridden:
                    self.override_system.log_intervention(
                        datetime.now(), state, action, human_action, reasons)
                    return human_action, True
    
            return action, False
    
        def monitor_performance(self, state, action, reward):
            """æ€§èƒ½ç›£è¦–ã¨ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º"""
            self.monitor.update(state, action, reward)
    
            # ç•°å¸¸æ¤œå‡º
            if self.monitor.detect_anomaly(state, action):
                print(f"  [MONITOR] Anomaly detected at step {self.monitor.total_steps}")
    
            # ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º
            if self.drift_detector.add_element(reward):
                print(f"  [MONITOR] Performance drift detected!")
                return True  # å†å­¦ç¿’ãƒˆãƒªã‚¬ãƒ¼
    
            # å®šæœŸçš„ãªçµ±è¨ˆçš„ãƒ‰ãƒªãƒ•ãƒˆãƒã‚§ãƒƒã‚¯
            if self.monitor.total_steps % 100 == 0:
                drift, info = self.monitor.detect_drift()
                if drift:
                    print(f"  [MONITOR] Statistical drift: "
                          f"current={info['current_mean']:.2f}, "
                          f"baseline={info['baseline_mean']:.2f}")
                    return True
    
            return False
    
        def control_loop(self, n_steps=500):
            """ãƒ¡ã‚¤ãƒ³åˆ¶å¾¡ãƒ«ãƒ¼ãƒ—"""
            print("çµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ èµ·å‹•\n")
            print("="*60)
    
            state = self.env.reset()
            state = self.preprocess_observation(state)
    
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¨­å®š
            for step in range(100):
                action = self.agent.select_action(state)[0]
                next_state, reward, done = self.env.step(action)
                next_state = self.preprocess_observation(next_state)
    
                self.monitor.update(state, action, reward)
                state = next_state if not done else self.env.reset()
    
            self.monitor.set_baseline()
            print("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¨­å®šå®Œäº†\n")
    
            # ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
            for step in range(n_steps):
                if self.emergency_stop:
                    print("ç·Šæ€¥åœæ­¢ï¼")
                    break
    
                # 1. AIæ–¹ç­–
                raw_action = self.agent.select_action(state)[0]
    
                # 2. ä¸ç¢ºå®Ÿæ€§æ¨å®š
                uncertainty = self.estimate_uncertainty(state, np.array([raw_action]))
    
                # é«˜ä¸ç¢ºå®Ÿæ€§æ™‚ã¯ä¿å®ˆçš„ã«
                if uncertainty > 0.5:
                    raw_action *= 0.5
                    print(f"  [UNCERTAINTY] High uncertainty ({uncertainty:.3f}), "
                          f"conservative action")
    
                # 3. å®‰å…¨ãƒ•ã‚£ãƒ«ã‚¿
                safe_action = self.apply_safety_filter(state, raw_action)
    
                # 4. äººé–“ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
                final_action, overridden = self.check_human_override(
                    state, safe_action, uncertainty)
    
                # 5. å®Ÿè¡Œ
                next_state, reward, done = self.env.step(final_action)
                next_state = self.preprocess_observation(next_state)
    
                # 6. æ€§èƒ½ç›£è¦–
                need_retraining = self.monitor_performance(state, final_action, reward)
    
                if need_retraining:
                    print(f"  [SYSTEM] å†å­¦ç¿’ãŒæ¨å¥¨ã•ã‚Œã¾ã™")
    
                # å®šæœŸãƒ¬ãƒãƒ¼ãƒˆ
                if step % 100 == 0:
                    print(f"\nStep {step}:")
                    print(f"  State: T={state[0]:.2f}K, CA={state[1]:.3f}")
                    print(f"  Action: {final_action:.3f}, Uncertainty: {uncertainty:.3f}")
                    print(f"  Mode: {self.override_system.mode.value}")
                    print(f"  Anomalies: {self.monitor.anomaly_count}")
    
                self.prev_action = final_action
                state = next_state if not done else self.env.reset()
    
            # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
            print("\n" + "="*60)
            print("é‹è»¢çµ‚äº†")
            print("="*60)
            print(self.monitor.generate_report())
            print(self.override_system.generate_report())
    
    # å®Ÿè¡Œ
    from example1 import RobustSACAgent, RandomizedCSTREnv
    
    print("çµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n")
    
    env = RandomizedCSTREnv(randomize=True)
    agent = RobustSACAgent(obs_dim=2, action_dim=1)
    
    system = IntegratedDeploymentSystem(agent, env)
    system.control_loop(n_steps=300)

## Chapter 5 ã¾ã¨ã‚

### å­¦ã‚“ã ã“ã¨

  * **Sim-to-Realè»¢ç§»** ï¼šãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒ€ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã§ãƒ­ãƒã‚¹ãƒˆæ€§ã‚’ç²å¾—
  * **å®‰å…¨ãªæ¢ç´¢** ï¼šã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿ã¨CBFã§å±é™ºå›é¿
  * **ä¿å®ˆçš„Qå­¦ç¿’** ï¼šã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®‰å…¨ã«å­¦ç¿’
  * **äººé–“ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰** ï¼šç·Šæ€¥æ™‚ã®ä»‹å…¥æ©Ÿæ§‹
  * **ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–** ï¼šã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚„MCDropoutã§ä¿¡é ¼åº¦è©•ä¾¡
  * **æ€§èƒ½ç›£è¦–** ï¼šãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã¨ç•°å¸¸æ¤œçŸ¥
  * **çµ±åˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯** ï¼šå…¨è¦ç´ ã‚’çµ„ã¿åˆã‚ã›ãŸå®Ÿç”¨ã‚·ã‚¹ãƒ†ãƒ 

### ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæˆç†Ÿåº¦ãƒ¢ãƒ‡ãƒ«

ãƒ¬ãƒ™ãƒ« | èª¬æ˜ | å¿…è¦æŠ€è¡“  
---|---|---  
L1: å®Ÿé¨“å®¤ | ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ã¿ | åŸºæœ¬RL  
L2: ãƒ†ã‚¹ãƒˆ | ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆãƒ—ãƒ©ãƒ³ãƒˆ | Sim-to-realã€å®‰å…¨åˆ¶ç´„  
L3: ç›£è¦–ä»˜ã | å®Ÿãƒ—ãƒ©ãƒ³ãƒˆï¼ˆäººé–“ç›£è¦–ï¼‰ | ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã€æ€§èƒ½ç›£è¦–  
L4: è‡ªå¾‹ | å®Œå…¨è‡ªå¾‹é‹è»¢ | å…¨æ©Ÿèƒ½çµ±åˆã€ç¶™ç¶šå­¦ç¿’  
  
### å®Ÿãƒ—ãƒ©ãƒ³ãƒˆé©ç”¨ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

  * âœ“ ååˆ†ãªã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¤œè¨¼ï¼ˆ1000+ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰
  * âœ“ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ©ãƒ³ãƒ€ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã®å¦¥å½“æ€§ç¢ºèª
  * âœ“ å®‰å…¨åˆ¶ç´„ã®ç¶²ç¾…çš„å®šç¾©
  * âœ“ ç·Šæ€¥åœæ­¢ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®å®Ÿè£…ã¨ãƒ†ã‚¹ãƒˆ
  * âœ“ ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼è¨“ç·´ã¨ãƒãƒ‹ãƒ¥ã‚¢ãƒ«æ•´å‚™
  * âœ“ æ€§èƒ½ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ§‹ç¯‰
  * âœ“ å®šæœŸçš„ãªæ€§èƒ½è©•ä¾¡ã¨å†å­¦ç¿’è¨ˆç”»
  * âœ“ è¦åˆ¶å½“å±€ã¸ã®å ±å‘Šä½“åˆ¶

#### æœ€çµ‚æ³¨æ„äº‹é …

å¼·åŒ–å­¦ç¿’ã®å®Ÿãƒ—ãƒ©ãƒ³ãƒˆé©ç”¨ã¯ã€ã¾ã ç™ºå±•é€”ä¸Šã®æŠ€è¡“ã§ã™ã€‚ ç‰¹ã«åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆã®ã‚ˆã†ãªå®‰å…¨ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªç’°å¢ƒã§ã¯ã€æ®µéšçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒä¸å¯æ¬ ã§ã™ï¼š 

  1. ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®å¾¹åº•æ¤œè¨¼
  2. ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆãƒ—ãƒ©ãƒ³ãƒˆã§ã®å®Ÿè¨¼
  3. äººé–“ç›£è¦–ä¸‹ã§ã®é™å®šé‹ç”¨
  4. æ®µéšçš„ãªè‡ªå¾‹ãƒ¬ãƒ™ãƒ«å‘ä¸Š

å¸¸ã«äººé–“ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã¨å”åƒã—ã€AIã‚’é“å…·ã¨ã—ã¦æ´»ç”¨ã™ã‚‹å§¿å‹¢ãŒé‡è¦ã§ã™ã€‚ 

### ã‚·ãƒªãƒ¼ã‚ºå®Œäº†

**ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼**

ã€ŒAIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹è‡ªå¾‹ãƒ—ãƒ­ã‚»ã‚¹é‹è»¢ã€ã‚·ãƒªãƒ¼ã‚ºã®å…¨5ç« ã‚’ä¿®äº†ã—ã¾ã—ãŸã€‚ å¼·åŒ–å­¦ç¿’ã®åŸºç¤ã‹ã‚‰å®Ÿãƒ—ãƒ©ãƒ³ãƒˆãƒ‡ãƒ—ãƒ­ã‚¤ã¾ã§ã€å¹…åºƒã„çŸ¥è­˜ã‚’ç¿’å¾—ã•ã‚Œã¾ã—ãŸã€‚ 

[ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹](<index.html>)

ã•ã‚‰ãªã‚‹å­¦ç¿’ã«ã¯ã€ä»¥ä¸‹ã®ã‚·ãƒªãƒ¼ã‚ºã‚‚ã”æ´»ç”¨ãã ã•ã„ï¼š 

  * ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ãƒ»åˆ¶å¾¡å…¥é–€ï¼ˆæº–å‚™ä¸­ï¼‰
  * ãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ–å…¥é–€ï¼ˆæº–å‚™ä¸­ï¼‰

### å…è²¬äº‹é …

  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹Code examplesã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚
  * å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚
