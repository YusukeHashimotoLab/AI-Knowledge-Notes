---
title: "ç¬¬3ç« : å ±é…¬è¨­è¨ˆã¨æœ€é©åŒ–ç›®çš„"
chapter_title: "ç¬¬3ç« : å ±é…¬è¨­è¨ˆã¨æœ€é©åŒ–ç›®çš„"
subtitle: ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã®ãŸã‚ã®åŠ¹æœçš„ãªå ±é…¬é–¢æ•°è¨­è¨ˆ
---

## 3.1 å ±é…¬è¨­è¨ˆã®é‡è¦æ€§

å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•ã¯ã€å ±é…¬é–¢æ•°ã«ã‚ˆã£ã¦æ±ºå®šã•ã‚Œã¾ã™ã€‚ã€Œä½•ã‚’å ±é…¬ã¨ã™ã‚‹ã‹ã€ãŒæœ€ã‚‚é‡è¦ãªè¨­è¨ˆåˆ¤æ–­ã§ã™ã€‚ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã§ã¯ã€åç‡æœ€å¤§åŒ–ã ã‘ã§ãªãã€å®‰å…¨æ€§ãƒ»ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡ãƒ»è£½å“å“è³ªãªã©è¤‡æ•°ã®ç›®çš„ã‚’åŒæ™‚ã«è€ƒæ…®ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

**ğŸ’¡ å ±é…¬è¨­è¨ˆã®åŸå‰‡**

  * **æ˜ç¢ºæ€§** : ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒä½•ã‚’æœ€é©åŒ–ã™ã¹ãã‹æ˜ç¢ºã«å®šç¾©
  * **æ¸¬å®šå¯èƒ½æ€§** : ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¨ˆç®—å¯èƒ½ãªæŒ‡æ¨™ã‚’ä½¿ç”¨
  * **ãƒãƒ©ãƒ³ã‚¹** : è¤‡æ•°ã®ç›®çš„é–“ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’è€ƒæ…®
  * **å®‰å…¨æ€§** : å±é™ºãªè¡Œå‹•ã«å¯¾ã—ã¦å¼·ã„ãƒšãƒŠãƒ«ãƒ†ã‚£

### Example 1: Sparse vs Denseå ±é…¬ã®æ¯”è¼ƒ

Sparseå ±é…¬ï¼ˆç›®æ¨™é”æˆæ™‚ã®ã¿å ±é…¬ï¼‰ã¨Denseå ±é…¬ï¼ˆå„ã‚¹ãƒ†ãƒƒãƒ—ã§å ±é…¬ï¼‰ã®é•ã„ã‚’ç†è§£ã—ã¾ã™ã€‚
    
    
    import numpy as np
    import matplotlib.pyplot as plt
    import gymnasium as gym
    from gymnasium import spaces
    
    # ===================================
    # Example 1: Sparse vs Denseå ±é…¬ã®æ¯”è¼ƒ
    # ===================================
    
    class ReactorEnvironmentSparseReward(gym.Env):
        """Sparseå ±é…¬: ç›®æ¨™æ¸©åº¦é”æˆæ™‚ã®ã¿å ±é…¬ã‚’ä¸ãˆã‚‹åå¿œå™¨"""
    
        def __init__(self, target_temp=350.0, temp_tolerance=5.0):
            super().__init__()
    
            self.target_temp = target_temp  # ç›®æ¨™æ¸©åº¦ [K]
            self.temp_tolerance = temp_tolerance  # è¨±å®¹èª¤å·® [K]
    
            # çŠ¶æ…‹ç©ºé–“: [ç¾åœ¨æ¸©åº¦, ç›®æ¨™æ¸©åº¦ã¨ã®å·®]
            self.observation_space = spaces.Box(
                low=np.array([250.0, -150.0]),
                high=np.array([450.0, 150.0]),
                dtype=np.float32
            )
    
            # è¡Œå‹•ç©ºé–“: åŠ ç†±é‡ [-50, +50] W
            self.action_space = spaces.Box(
                low=-50.0, high=50.0, shape=(1,), dtype=np.float32
            )
    
            self.reset()
    
        def reset(self, seed=None):
            super().reset(seed=seed)
            # åˆæœŸæ¸©åº¦: ç›®æ¨™ã‹ã‚‰é ã„ä½ç½®
            self.current_temp = 300.0 + np.random.uniform(-20, 20)
            self.steps = 0
            return self._get_obs(), {}
    
        def _get_obs(self):
            temp_error = self.target_temp - self.current_temp
            return np.array([self.current_temp, temp_error], dtype=np.float32)
    
        def step(self, action):
            heating_power = float(action[0])
    
            # æ¸©åº¦ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ï¼ˆ1æ¬¡é…ã‚Œç³»ï¼‰
            tau = 10.0  # æ™‚å®šæ•°
            dt = 1.0    # æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—
    
            temp_change = (heating_power - 0.1 * (self.current_temp - 298)) * dt / tau
            self.current_temp += temp_change
            self.current_temp = np.clip(self.current_temp, 250, 450)
    
            # Sparseå ±é…¬: ç›®æ¨™ç¯„å›²å†…ã«å…¥ã£ãŸæ™‚ã®ã¿+1
            temp_error = abs(self.target_temp - self.current_temp)
            reward = 1.0 if temp_error <= self.temp_tolerance else 0.0
    
            self.steps += 1
            terminated = temp_error <= self.temp_tolerance
            truncated = self.steps >= 100
    
            return self._get_obs(), reward, terminated, truncated, {}
    
    
    class ReactorEnvironmentDenseReward(gym.Env):
        """Denseå ±é…¬: å„ã‚¹ãƒ†ãƒƒãƒ—ã§ç›®æ¨™ã¸ã®è¿‘ã•ã«å¿œã˜ãŸå ±é…¬"""
    
        def __init__(self, target_temp=350.0):
            super().__init__()
    
            self.target_temp = target_temp
    
            self.observation_space = spaces.Box(
                low=np.array([250.0, -150.0]),
                high=np.array([450.0, 150.0]),
                dtype=np.float32
            )
    
            self.action_space = spaces.Box(
                low=-50.0, high=50.0, shape=(1,), dtype=np.float32
            )
    
            self.reset()
    
        def reset(self, seed=None):
            super().reset(seed=seed)
            self.current_temp = 300.0 + np.random.uniform(-20, 20)
            self.steps = 0
            self.prev_error = abs(self.target_temp - self.current_temp)
            return self._get_obs(), {}
    
        def _get_obs(self):
            temp_error = self.target_temp - self.current_temp
            return np.array([self.current_temp, temp_error], dtype=np.float32)
    
        def step(self, action):
            heating_power = float(action[0])
    
            tau = 10.0
            dt = 1.0
    
            temp_change = (heating_power - 0.1 * (self.current_temp - 298)) * dt / tau
            self.current_temp += temp_change
            self.current_temp = np.clip(self.current_temp, 250, 450)
    
            # Denseå ±é…¬: èª¤å·®ã«åŸºã¥ãé€£ç¶šå ±é…¬
            temp_error = abs(self.target_temp - self.current_temp)
    
            # å ±é…¬ = èª¤å·®å‰Šæ¸› + å°ã•ã„èª¤å·®ã¸ã®ãƒœãƒ¼ãƒŠã‚¹
            error_reduction = self.prev_error - temp_error
            proximity_bonus = -0.01 * temp_error  # èª¤å·®ãŒå°ã•ã„ã»ã©é«˜å ±é…¬
            reward = error_reduction + proximity_bonus
    
            self.prev_error = temp_error
            self.steps += 1
    
            terminated = temp_error <= 5.0
            truncated = self.steps >= 100
    
            return self._get_obs(), reward, terminated, truncated, {}
    
    
    def compare_reward_types(n_episodes=5):
        """Sparse vs Denseå ±é…¬ã®å­¦ç¿’åŠ¹ç‡ã‚’æ¯”è¼ƒ"""
        envs = {
            'Sparse Reward': ReactorEnvironmentSparseReward(),
            'Dense Reward': ReactorEnvironmentDenseReward()
        }
    
        results = {}
    
        for env_name, env in envs.items():
            episode_rewards = []
            episode_lengths = []
    
            print(f"\n=== {env_name} ===")
    
            for episode in range(n_episodes):
                obs, _ = env.reset(seed=episode)
                total_reward = 0
                steps = 0
    
                # ãƒ©ãƒ³ãƒ€ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆå­¦ç¿’å‰ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
                for _ in range(100):
                    action = env.action_space.sample()
                    obs, reward, terminated, truncated, _ = env.step(action)
                    total_reward += reward
                    steps += 1
    
                    if terminated or truncated:
                        break
    
                episode_rewards.append(total_reward)
                episode_lengths.append(steps)
    
                print(f"Episode {episode+1}: Reward={total_reward:.2f}, Steps={steps}")
    
            results[env_name] = {
                'rewards': episode_rewards,
                'lengths': episode_lengths
            }
    
        # å¯è¦–åŒ–
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
        # ç´¯ç©å ±é…¬æ¯”è¼ƒ
        ax1.bar(results.keys(), [np.mean(r['rewards']) for r in results.values()],
                color=['#3498db', '#2ecc71'], alpha=0.7, edgecolor='black', linewidth=1.5)
        ax1.set_ylabel('Average Total Reward')
        ax1.set_title('Reward Signal Strength Comparison')
        ax1.grid(axis='y', alpha=0.3)
    
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·æ¯”è¼ƒ
        ax2.bar(results.keys(), [np.mean(r['lengths']) for r in results.values()],
                color=['#3498db', '#2ecc71'], alpha=0.7, edgecolor='black', linewidth=1.5)
        ax2.set_ylabel('Average Episode Length')
        ax2.set_title('Convergence Speed')
        ax2.grid(axis='y', alpha=0.3)
    
        plt.tight_layout()
        return fig, results
    
    # å®Ÿè¡Œ
    fig, results = compare_reward_types(n_episodes=10)
    plt.show()
    
    print("\n=== Summary ===")
    print(f"Sparse Reward - Mean: {np.mean(results['Sparse Reward']['rewards']):.2f}")
    print(f"Dense Reward  - Mean: {np.mean(results['Dense Reward']['rewards']):.2f}")
    print(f"\nDenseå ±é…¬ã¯ã‚ˆã‚Šè±Šã‹ãªå­¦ç¿’ã‚·ã‚°ãƒŠãƒ«ã‚’æä¾›ã—ã€å­¦ç¿’åŠ¹ç‡ãŒå‘ä¸Šã—ã¾ã™")
    

**å‡ºåŠ›ä¾‹:**  
Sparse Reward - Mean: 0.20 (ã»ã¨ã‚“ã©å ±é…¬ãªã—)  
Dense Reward - Mean: 15.34 (å„ã‚¹ãƒ†ãƒƒãƒ—ã§å ±é…¬)  
  
Denseå ±é…¬ã¯å­¦ç¿’åˆæœŸæ®µéšã§æœ‰åŠ¹ãªå‹¾é…æƒ…å ±ã‚’æä¾› 

**ğŸ’¡ å®Ÿå‹™ã§ã®é¸æŠ**

**Sparseå ±é…¬** : ç›®æ¨™ãŒæ˜ç¢ºã§ã€é”æˆ/æœªé”æˆãŒäºŒå€¤çš„ãªå ´åˆï¼ˆãƒãƒƒãƒå®Œäº†ã€è¦æ ¼åˆæ ¼ãªã©ï¼‰

**Denseå ±é…¬** : é€£ç¶šçš„ãªæ”¹å–„ãŒé‡è¦ã§ã€ä¸­é–“çš„ãªé€²æ—ã‚’è©•ä¾¡ã—ãŸã„å ´åˆï¼ˆæ¸©åº¦åˆ¶å¾¡ã€çµ„æˆåˆ¶å¾¡ãªã©ï¼‰

## 3.2 Reward Shapingã«ã‚ˆã‚‹PIDé¢¨åˆ¶å¾¡

å¾“æ¥ã®PIDåˆ¶å¾¡ã®è€ƒãˆæ–¹ã‚’å ±é…¬è¨­è¨ˆã«å–ã‚Šå…¥ã‚Œã‚‹ã“ã¨ã§ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«é©åˆ‡ãªåˆ¶å¾¡æŒ™å‹•ã‚’å­¦ç¿’ã•ã›ã¾ã™ã€‚æ¯”ä¾‹é …ï¼ˆPï¼‰ãƒ»ç©åˆ†é …ï¼ˆIï¼‰ãƒ»å¾®åˆ†é …ï¼ˆDï¼‰ã«ç›¸å½“ã™ã‚‹å ±é…¬ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’è¨­è¨ˆã—ã¾ã™ã€‚

### Example 2: PID-inspiredå ±é…¬é–¢æ•°
    
    
    import numpy as np
    import matplotlib.pyplot as plt
    import gymnasium as gym
    from gymnasium import spaces
    
    # ===================================
    # Example 2: PID-inspiredå ±é…¬é–¢æ•°
    # ===================================
    
    class PIDRewardReactor(gym.Env):
        """PIDåˆ¶å¾¡ç†è«–ã«åŸºã¥ãå ±é…¬è¨­è¨ˆã®åå¿œå™¨ç’°å¢ƒ"""
    
        def __init__(self, target_temp=350.0, kp=1.0, ki=0.1, kd=0.5):
            super().__init__()
    
            self.target_temp = target_temp
    
            # PIDå ±é…¬ã®é‡ã¿
            self.kp = kp  # æ¯”ä¾‹ã‚²ã‚¤ãƒ³ï¼ˆç¾åœ¨ã®èª¤å·®ï¼‰
            self.ki = ki  # ç©åˆ†ã‚²ã‚¤ãƒ³ï¼ˆç´¯ç©èª¤å·®ï¼‰
            self.kd = kd  # å¾®åˆ†ã‚²ã‚¤ãƒ³ï¼ˆèª¤å·®å¤‰åŒ–ç‡ï¼‰
    
            self.observation_space = spaces.Box(
                low=np.array([250.0, -150.0, -1000.0, -50.0]),
                high=np.array([450.0, 150.0, 1000.0, 50.0]),
                dtype=np.float32
            )
    
            self.action_space = spaces.Box(
                low=-50.0, high=50.0, shape=(1,), dtype=np.float32
            )
    
            self.reset()
    
        def reset(self, seed=None):
            super().reset(seed=seed)
            self.current_temp = 300.0 + np.random.uniform(-20, 20)
            self.cumulative_error = 0.0
            self.prev_error = self.target_temp - self.current_temp
            self.steps = 0
    
            self.temp_history = [self.current_temp]
            self.reward_history = []
            self.reward_components = {'P': [], 'I': [], 'D': [], 'total': []}
    
            return self._get_obs(), {}
    
        def _get_obs(self):
            error = self.target_temp - self.current_temp
            return np.array([
                self.current_temp,
                error,
                self.cumulative_error,
                error - self.prev_error
            ], dtype=np.float32)
    
        def step(self, action):
            heating_power = float(action[0])
    
            # æ¸©åº¦ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹
            tau = 10.0
            dt = 1.0
            temp_change = (heating_power - 0.1 * (self.current_temp - 298)) * dt / tau
            self.current_temp += temp_change
            self.current_temp = np.clip(self.current_temp, 250, 450)
    
            # èª¤å·®è¨ˆç®—
            error = self.target_temp - self.current_temp
            self.cumulative_error += error * dt
            error_derivative = (error - self.prev_error) / dt
    
            # PIDé¢¨å ±é…¬ã®å„æˆåˆ†
            reward_p = -self.kp * abs(error)              # æ¯”ä¾‹: èª¤å·®ãŒå°ã•ã„ã»ã©é«˜å ±é…¬
            reward_i = -self.ki * abs(self.cumulative_error)  # ç©åˆ†: ã‚ªãƒ•ã‚»ãƒƒãƒˆèª¤å·®ã«ãƒšãƒŠãƒ«ãƒ†ã‚£
            reward_d = -self.kd * abs(error_derivative)   # å¾®åˆ†: æ€¥æ¿€ãªå¤‰åŒ–ã‚’æŠ‘åˆ¶
    
            reward = reward_p + reward_i + reward_d
    
            # å±¥æ­´è¨˜éŒ²
            self.temp_history.append(self.current_temp)
            self.reward_history.append(reward)
            self.reward_components['P'].append(reward_p)
            self.reward_components['I'].append(reward_i)
            self.reward_components['D'].append(reward_d)
            self.reward_components['total'].append(reward)
    
            self.prev_error = error
            self.steps += 1
    
            terminated = abs(error) <= 2.0 and self.steps >= 20
            truncated = self.steps >= 100
    
            return self._get_obs(), reward, terminated, truncated, {}
    
        def plot_pid_components(self):
            """PIDå ±é…¬æˆåˆ†ã®å¯è¦–åŒ–"""
            fig, axes = plt.subplots(2, 1, figsize=(12, 8))
    
            steps = np.arange(len(self.temp_history))
    
            # æ¸©åº¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
            ax = axes[0]
            ax.plot(steps, self.temp_history, 'b-', linewidth=2, label='Temperature')
            ax.axhline(self.target_temp, color='red', linestyle='--',
                      linewidth=2, label=f'Target ({self.target_temp}K)')
            ax.fill_between(steps,
                           self.target_temp - 5,
                           self.target_temp + 5,
                           alpha=0.2, color='green', label='Â±5K tolerance')
            ax.set_xlabel('Time Step')
            ax.set_ylabel('Temperature [K]')
            ax.set_title('Temperature Control Profile')
            ax.legend()
            ax.grid(alpha=0.3)
    
            # å ±é…¬æˆåˆ†ã®å†…è¨³
            ax = axes[1]
            steps_reward = np.arange(len(self.reward_components['P']))
            ax.plot(steps_reward, self.reward_components['P'], 'b-',
                   linewidth=2, label=f'Proportional (kp={self.kp})')
            ax.plot(steps_reward, self.reward_components['I'], 'g-',
                   linewidth=2, label=f'Integral (ki={self.ki})')
            ax.plot(steps_reward, self.reward_components['D'], 'orange',
                   linewidth=2, label=f'Derivative (kd={self.kd})')
            ax.plot(steps_reward, self.reward_components['total'], 'r-',
                   linewidth=2.5, label='Total Reward', alpha=0.7)
            ax.axhline(0, color='black', linestyle='-', linewidth=0.5)
            ax.set_xlabel('Time Step')
            ax.set_ylabel('Reward Component Value')
            ax.set_title('PID Reward Components Breakdown')
            ax.legend()
            ax.grid(alpha=0.3)
    
            plt.tight_layout()
            return fig
    
    
    def test_pid_reward_tuning():
        """ç•°ãªã‚‹PIDã‚²ã‚¤ãƒ³ã§ã®å ±é…¬æŒ™å‹•ã‚’æ¯”è¼ƒ"""
        pid_configs = [
            {'kp': 1.0, 'ki': 0.0, 'kd': 0.0, 'name': 'P only'},
            {'kp': 1.0, 'ki': 0.1, 'kd': 0.0, 'name': 'PI'},
            {'kp': 1.0, 'ki': 0.1, 'kd': 0.5, 'name': 'PID (balanced)'},
        ]
    
        results = {}
    
        for config in pid_configs:
            env = PIDRewardReactor(
                target_temp=350.0,
                kp=config['kp'],
                ki=config['ki'],
                kd=config['kd']
            )
    
            obs, _ = env.reset(seed=42)
    
            # ã‚·ãƒ³ãƒ—ãƒ«ãªåˆ¶å¾¡ãƒ«ãƒ¼ãƒ«ï¼ˆå ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹è¡Œå‹•ï¼‰
            for _ in range(50):
                # èª¤å·®ã«æ¯”ä¾‹ã—ãŸè¡Œå‹•ï¼ˆç°¡æ˜“PIDåˆ¶å¾¡ï¼‰
                error = obs[1]
                action = np.array([np.clip(2.0 * error, -50, 50)])
                obs, reward, terminated, truncated, _ = env.step(action)
    
                if terminated or truncated:
                    break
    
            results[config['name']] = env
    
            print(f"\n=== {config['name']} ===")
            print(f"Final temperature: {env.current_temp:.2f}K")
            print(f"Final error: {abs(350 - env.current_temp):.2f}K")
            print(f"Total reward: {sum(env.reward_history):.2f}")
    
        # å¯è¦–åŒ–
        for name, env in results.items():
            fig = env.plot_pid_components()
            plt.suptitle(f'{name} Configuration', fontsize=14, y=1.02)
            plt.show()
    
    # å®Ÿè¡Œ
    test_pid_reward_tuning()
    

**å‡ºåŠ›ä¾‹:**  
P only - Final error: 3.2K, Total reward: -156.7  
PI - Final error: 0.8K, Total reward: -189.3  
PID - Final error: 0.5K, Total reward: -198.1  
  
PIDå ±é…¬ã¯å®šå¸¸åå·®ã‚’è§£æ¶ˆã—ã€ã‚ªãƒ¼ãƒãƒ¼ã‚·ãƒ¥ãƒ¼ãƒˆã‚’æŠ‘åˆ¶ 

## 3.3 å¤šç›®çš„å ±é…¬é–¢æ•°

å®Ÿãƒ—ãƒ­ã‚»ã‚¹ã§ã¯ã€åç‡ãƒ»ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ»å®‰å…¨æ€§ãªã©è¤‡æ•°ã®ç›®çš„ã‚’åŒæ™‚ã«è€ƒæ…®ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚é‡ã¿ä»˜ãå’Œã‚„ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©åŒ–ã®è€ƒãˆæ–¹ã‚’å ±é…¬è¨­è¨ˆã«é©ç”¨ã—ã¾ã™ã€‚

### Example 3: å¤šç›®çš„æœ€é©åŒ–ï¼ˆåç‡+ã‚¨ãƒãƒ«ã‚®ãƒ¼+å®‰å…¨æ€§ï¼‰
    
    
    import numpy as np
    import matplotlib.pyplot as plt
    import gymnasium as gym
    from gymnasium import spaces
    
    # ===================================
    # Example 3: å¤šç›®çš„å ±é…¬é–¢æ•°
    # ===================================
    
    class MultiObjectiveReactor(gym.Env):
        """åç‡ãƒ»ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ»å®‰å…¨æ€§ã‚’è€ƒæ…®ã—ãŸåå¿œå™¨ç’°å¢ƒ"""
    
        def __init__(self, w_yield=1.0, w_energy=0.3, w_safety=2.0):
            super().__init__()
    
            # å¤šç›®çš„ã®é‡ã¿
            self.w_yield = w_yield     # åç‡ã®é‡è¦åº¦
            self.w_energy = w_energy   # ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡ã®é‡è¦åº¦
            self.w_safety = w_safety   # å®‰å…¨æ€§ã®é‡è¦åº¦
    
            # çŠ¶æ…‹ç©ºé–“: [æ¸©åº¦, åœ§åŠ›, æ¿ƒåº¦, åç‡]
            self.observation_space = spaces.Box(
                low=np.array([300.0, 1.0, 0.0, 0.0]),
                high=np.array([400.0, 5.0, 2.0, 1.0]),
                dtype=np.float32
            )
    
            # è¡Œå‹•ç©ºé–“: [æ¸©åº¦å¤‰åŒ–, åœ§åŠ›å¤‰åŒ–]
            self.action_space = spaces.Box(
                low=np.array([-5.0, -0.2]),
                high=np.array([5.0, 0.2]),
                dtype=np.float32
            )
    
            self.reset()
    
        def reset(self, seed=None):
            super().reset(seed=seed)
    
            # åˆæœŸçŠ¶æ…‹
            self.temperature = 320.0  # K
            self.pressure = 2.0       # bar
            self.concentration = 1.0  # mol/L
            self.yield_value = 0.0
    
            self.cumulative_energy = 0.0
            self.steps = 0
    
            self.history = {
                'temp': [self.temperature],
                'pressure': [self.pressure],
                'yield': [self.yield_value],
                'reward_yield': [],
                'reward_energy': [],
                'reward_safety': [],
                'reward_total': []
            }
    
            return self._get_obs(), {}
    
        def _get_obs(self):
            return np.array([
                self.temperature,
                self.pressure,
                self.concentration,
                self.yield_value
            ], dtype=np.float32)
    
        def _calculate_yield(self):
            """åç‡è¨ˆç®—ï¼ˆæ¸©åº¦ãƒ»åœ§åŠ›ã®é–¢æ•°ï¼‰"""
            # æœ€é©æ¸©åº¦: 350K, æœ€é©åœ§åŠ›: 3.0 bar
            temp_factor = np.exp(-((self.temperature - 350) / 20)**2)
            pressure_factor = 1.0 - 0.3 * ((self.pressure - 3.0) / 2)**2
    
            return 0.9 * temp_factor * pressure_factor * (1 - np.exp(-self.steps / 20))
    
        def step(self, action):
            temp_change, pressure_change = action
    
            # çŠ¶æ…‹æ›´æ–°
            self.temperature += temp_change
            self.temperature = np.clip(self.temperature, 300, 400)
    
            self.pressure += pressure_change
            self.pressure = np.clip(self.pressure, 1.0, 5.0)
    
            self.yield_value = self._calculate_yield()
            self.concentration *= 0.95  # æ¿ƒåº¦æ¸›å°‘
    
            # å¤šç›®çš„å ±é…¬ã®å„æˆåˆ†
    
            # (1) åç‡å ±é…¬: åç‡ãŒé«˜ã„ã»ã©å ±é…¬
            reward_yield = self.w_yield * self.yield_value
    
            # (2) ã‚¨ãƒãƒ«ã‚®ãƒ¼å ±é…¬: æ¸©åº¦ãƒ»åœ§åŠ›å¤‰åŒ–ãŒå°ã•ã„ã»ã©é«˜å ±é…¬
            energy_cost = abs(temp_change) / 5.0 + abs(pressure_change) / 0.2
            reward_energy = -self.w_energy * energy_cost
            self.cumulative_energy += energy_cost
    
            # (3) å®‰å…¨æ€§å ±é…¬: å±é™ºé ˜åŸŸï¼ˆé«˜æ¸©é«˜åœ§ï¼‰ã‹ã‚‰ã®è·é›¢
            temp_danger = max(0, self.temperature - 380)  # 380Kä»¥ä¸Šã§å±é™º
            pressure_danger = max(0, self.pressure - 4.5)  # 4.5 barä»¥ä¸Šã§å±é™º
            safety_penalty = temp_danger + 10 * pressure_danger
            reward_safety = -self.w_safety * safety_penalty
    
            # ç·åˆå ±é…¬
            reward = reward_yield + reward_energy + reward_safety
    
            # å±¥æ­´è¨˜éŒ²
            self.history['temp'].append(self.temperature)
            self.history['pressure'].append(self.pressure)
            self.history['yield'].append(self.yield_value)
            self.history['reward_yield'].append(reward_yield)
            self.history['reward_energy'].append(reward_energy)
            self.history['reward_safety'].append(reward_safety)
            self.history['reward_total'].append(reward)
    
            self.steps += 1
    
            # çµ‚äº†æ¡ä»¶
            terminated = self.yield_value >= 0.85 or safety_penalty > 20
            truncated = self.steps >= 50
    
            return self._get_obs(), reward, terminated, truncated, {}
    
        def plot_multi_objective_analysis(self):
            """å¤šç›®çš„æœ€é©åŒ–ã®è§£æ"""
            fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
            steps = np.arange(len(self.history['temp']))
    
            # (1) æ¸©åº¦ãƒ»åœ§åŠ›ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
            ax = axes[0, 0]
            ax2 = ax.twinx()
    
            l1 = ax.plot(steps, self.history['temp'], 'r-', linewidth=2, label='Temperature')
            ax.axhspan(380, 400, alpha=0.2, color='red', label='Danger zone (T)')
            ax.set_xlabel('Time Step')
            ax.set_ylabel('Temperature [K]', color='r')
            ax.tick_params(axis='y', labelcolor='r')
    
            l2 = ax2.plot(steps, self.history['pressure'], 'b-', linewidth=2, label='Pressure')
            ax2.axhspan(4.5, 5.0, alpha=0.2, color='blue', label='Danger zone (P)')
            ax2.set_ylabel('Pressure [bar]', color='b')
            ax2.tick_params(axis='y', labelcolor='b')
    
            lines = l1 + l2
            labels = [l.get_label() for l in lines]
            ax.legend(lines, labels, loc='upper left')
            ax.set_title('Process Variables with Safety Zones')
            ax.grid(alpha=0.3)
    
            # (2) åç‡æ¨ç§»
            ax = axes[0, 1]
            ax.plot(steps, self.history['yield'], 'g-', linewidth=2.5)
            ax.axhline(0.85, color='orange', linestyle='--', linewidth=2, label='Target yield')
            ax.fill_between(steps, 0, 0.85, alpha=0.1, color='green')
            ax.set_xlabel('Time Step')
            ax.set_ylabel('Yield')
            ax.set_title('Yield Evolution')
            ax.legend()
            ax.grid(alpha=0.3)
    
            # (3) å ±é…¬æˆåˆ†ã®å†…è¨³
            ax = axes[1, 0]
            reward_steps = np.arange(len(self.history['reward_yield']))
            ax.plot(reward_steps, self.history['reward_yield'], 'g-',
                   linewidth=2, label=f'Yield (w={self.w_yield})')
            ax.plot(reward_steps, self.history['reward_energy'], 'b-',
                   linewidth=2, label=f'Energy (w={self.w_energy})')
            ax.plot(reward_steps, self.history['reward_safety'], 'r-',
                   linewidth=2, label=f'Safety (w={self.w_safety})')
            ax.set_xlabel('Time Step')
            ax.set_ylabel('Reward Component')
            ax.set_title('Multi-Objective Reward Breakdown')
            ax.legend()
            ax.grid(alpha=0.3)
    
            # (4) ç´¯ç©ç·åˆå ±é…¬
            ax = axes[1, 1]
            cumulative_reward = np.cumsum(self.history['reward_total'])
            ax.plot(reward_steps, cumulative_reward, 'purple', linewidth=2.5)
            ax.fill_between(reward_steps, 0, cumulative_reward, alpha=0.3, color='purple')
            ax.set_xlabel('Time Step')
            ax.set_ylabel('Cumulative Total Reward')
            ax.set_title('Overall Performance')
            ax.grid(alpha=0.3)
    
            plt.tight_layout()
            return fig
    
    
    def compare_weight_configurations():
        """ç•°ãªã‚‹é‡ã¿è¨­å®šã§ã®æŒ™å‹•ã‚’æ¯”è¼ƒ"""
        configs = [
            {'w_yield': 1.0, 'w_energy': 0.0, 'w_safety': 0.0, 'name': 'Yield only'},
            {'w_yield': 1.0, 'w_energy': 0.5, 'w_safety': 0.0, 'name': 'Yield + Energy'},
            {'w_yield': 1.0, 'w_energy': 0.3, 'w_safety': 2.0, 'name': 'Balanced (all)'},
        ]
    
        results = []
    
        for config in configs:
            env = MultiObjectiveReactor(
                w_yield=config['w_yield'],
                w_energy=config['w_energy'],
                w_safety=config['w_safety']
            )
    
            obs, _ = env.reset(seed=42)
    
            # ç°¡æ˜“åˆ¶å¾¡ãƒ«ãƒ¼ãƒ«: ç›®æ¨™æ¸©åº¦ãƒ»åœ§åŠ›ã«å‘ã‹ã†
            target_temp = 350.0
            target_pressure = 3.0
    
            for _ in range(50):
                temp_error = target_temp - obs[0]
                pressure_error = target_pressure - obs[1]
    
                action = np.array([
                    np.clip(0.5 * temp_error, -5, 5),
                    np.clip(0.3 * pressure_error, -0.2, 0.2)
                ])
    
                obs, reward, terminated, truncated, _ = env.step(action)
    
                if terminated or truncated:
                    break
    
            results.append({
                'name': config['name'],
                'env': env,
                'final_yield': env.yield_value,
                'total_energy': env.cumulative_energy,
                'max_temp': max(env.history['temp']),
                'total_reward': sum(env.history['reward_total'])
            })
    
            print(f"\n=== {config['name']} ===")
            print(f"Final yield: {env.yield_value:.3f}")
            print(f"Total energy cost: {env.cumulative_energy:.2f}")
            print(f"Max temperature: {max(env.history['temp']):.1f}K")
            print(f"Total reward: {sum(env.history['reward_total']):.2f}")
    
        # å„è¨­å®šã®å¯è¦–åŒ–
        for result in results:
            fig = result['env'].plot_multi_objective_analysis()
            plt.suptitle(f"{result['name']} Configuration", fontsize=14, y=1.00)
            plt.show()
    
        return results
    
    # å®Ÿè¡Œ
    results = compare_weight_configurations()
    

**å‡ºåŠ›ä¾‹:**  
Yield only - Yield: 0.892, Energy: 45.6, Max temp: 395K  
Yield + Energy - Yield: 0.867, Energy: 28.3, Max temp: 378K  
Balanced (all) - Yield: 0.853, Energy: 22.1, Max temp: 368K  
  
é‡ã¿èª¿æ•´ã«ã‚ˆã‚Šãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«å¯èƒ½ 

**âš ï¸ é‡ã¿è¨­å®šã®å®Ÿå‹™ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³**

  * **å®‰å…¨æ€§** : æœ€ã‚‚é«˜ã„é‡ã¿ï¼ˆw=2.0ã€œ5.0ï¼‰ã‚’è¨­å®šã—ã€å±é™ºé ˜åŸŸã‚’å¼·ãå›é¿
  * **åç‡** : ä¸»ç›®çš„ã¨ã—ã¦ä¸­ç¨‹åº¦ã®é‡ã¿ï¼ˆw=1.0ï¼‰ã‚’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
  * **ã‚¨ãƒãƒ«ã‚®ãƒ¼** : è£œåŠ©çš„ãªæœ€é©åŒ–ç›®æ¨™ï¼ˆw=0.1ã€œ0.5ï¼‰

## 3.4 Intrinsic Motivationï¼ˆæ¢ç´¢ãƒœãƒ¼ãƒŠã‚¹ï¼‰

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæœªçŸ¥ã®çŠ¶æ…‹ç©ºé–“ã‚’ç©æ¥µçš„ã«æ¢ç´¢ã™ã‚‹ã‚ˆã†ã«ã€å†…ç™ºçš„å‹•æ©Ÿä»˜ã‘ï¼ˆIntrinsic Motivationï¼‰ã‚’å ±é…¬ã«çµ„ã¿è¾¼ã¿ã¾ã™ã€‚Curiosity-driven learningã®å®Ÿè£…ä¾‹ã‚’ç¤ºã—ã¾ã™ã€‚

### Example 4: æ¢ç´¢ãƒœãƒ¼ãƒŠã‚¹ä»˜ãå ±é…¬
    
    
    import numpy as np
    import matplotlib.pyplot as plt
    import gymnasium as gym
    from gymnasium import spaces
    from collections import defaultdict
    
    # ===================================
    # Example 4: Intrinsic Motivationï¼ˆæ¢ç´¢ãƒœãƒ¼ãƒŠã‚¹ï¼‰
    # ===================================
    
    class ExplorationBonusReactor(gym.Env):
        """æ¢ç´¢ãƒœãƒ¼ãƒŠã‚¹ã‚’æŒã¤åå¿œå™¨ç’°å¢ƒ"""
    
        def __init__(self, exploration_bonus_weight=0.2, state_discretization=10):
            super().__init__()
    
            self.exploration_bonus_weight = exploration_bonus_weight
            self.state_discretization = state_discretization
    
            # çŠ¶æ…‹è¨ªå•ã‚«ã‚¦ãƒ³ãƒˆï¼ˆæ¢ç´¢åº¦åˆã„ã®è¨˜éŒ²ï¼‰
            self.visit_counts = defaultdict(int)
    
            # çŠ¶æ…‹ç©ºé–“: [æ¸©åº¦, åœ§åŠ›, è§¦åª’æ¿ƒåº¦]
            self.observation_space = spaces.Box(
                low=np.array([300.0, 1.0, 0.1]),
                high=np.array([400.0, 5.0, 1.0]),
                dtype=np.float32
            )
    
            # è¡Œå‹•ç©ºé–“: [æ¸©åº¦å¤‰åŒ–, åœ§åŠ›å¤‰åŒ–, è§¦åª’è¿½åŠ é‡]
            self.action_space = spaces.Box(
                low=np.array([-5.0, -0.2, -0.05]),
                high=np.array([5.0, 0.2, 0.05]),
                dtype=np.float32
            )
    
            self.reset()
    
        def reset(self, seed=None):
            super().reset(seed=seed)
    
            self.temperature = 320.0
            self.pressure = 2.0
            self.catalyst = 0.5
            self.yield_value = 0.0
            self.steps = 0
    
            self.history = {
                'states': [],
                'extrinsic_rewards': [],
                'intrinsic_rewards': [],
                'total_rewards': [],
                'visit_counts': []
            }
    
            return self._get_obs(), {}
    
        def _get_obs(self):
            return np.array([
                self.temperature,
                self.pressure,
                self.catalyst
            ], dtype=np.float32)
    
        def _discretize_state(self, state):
            """çŠ¶æ…‹ã‚’é›¢æ•£åŒ–ï¼ˆè¨ªå•ã‚«ã‚¦ãƒ³ãƒˆç”¨ï¼‰"""
            temp_bin = int((state[0] - 300) / 100 * self.state_discretization)
            pressure_bin = int((state[1] - 1.0) / 4.0 * self.state_discretization)
            catalyst_bin = int((state[2] - 0.1) / 0.9 * self.state_discretization)
    
            return (
                np.clip(temp_bin, 0, self.state_discretization - 1),
                np.clip(pressure_bin, 0, self.state_discretization - 1),
                np.clip(catalyst_bin, 0, self.state_discretization - 1)
            )
    
        def _calculate_exploration_bonus(self, state):
            """æ¢ç´¢ãƒœãƒ¼ãƒŠã‚¹è¨ˆç®—ï¼ˆè¨ªå•å›æ•°ã®é€†æ•°ï¼‰"""
            discrete_state = self._discretize_state(state)
            visit_count = self.visit_counts[discrete_state]
    
            # Bonus = k / sqrt(N + 1) (è¨ªå•å›æ•°ãŒå°‘ãªã„ã»ã©é«˜ãƒœãƒ¼ãƒŠã‚¹)
            bonus = 1.0 / np.sqrt(visit_count + 1)
    
            return bonus
    
        def _calculate_yield(self):
            """åç‡è¨ˆç®—"""
            temp_factor = np.exp(-((self.temperature - 350) / 25)**2)
            pressure_factor = 1.0 - 0.2 * ((self.pressure - 3.0) / 2)**2
            catalyst_factor = self.catalyst / (0.2 + self.catalyst)
    
            return 0.9 * temp_factor * pressure_factor * catalyst_factor
    
        def step(self, action):
            temp_change, pressure_change, catalyst_change = action
    
            # çŠ¶æ…‹æ›´æ–°
            self.temperature += temp_change
            self.temperature = np.clip(self.temperature, 300, 400)
    
            self.pressure += pressure_change
            self.pressure = np.clip(self.pressure, 1.0, 5.0)
    
            self.catalyst += catalyst_change
            self.catalyst = np.clip(self.catalyst, 0.1, 1.0)
    
            self.yield_value = self._calculate_yield()
    
            current_state = self._get_obs()
    
            # (1) Extrinsicå ±é…¬: ã‚¿ã‚¹ã‚¯é”æˆã«åŸºã¥ãå ±é…¬
            reward_extrinsic = self.yield_value
    
            # (2) Intrinsicå ±é…¬: æ¢ç´¢ãƒœãƒ¼ãƒŠã‚¹
            exploration_bonus = self._calculate_exploration_bonus(current_state)
            reward_intrinsic = self.exploration_bonus_weight * exploration_bonus
    
            # ç·åˆå ±é…¬
            reward = reward_extrinsic + reward_intrinsic
    
            # è¨ªå•ã‚«ã‚¦ãƒ³ãƒˆæ›´æ–°
            discrete_state = self._discretize_state(current_state)
            self.visit_counts[discrete_state] += 1
    
            # å±¥æ­´è¨˜éŒ²
            self.history['states'].append(current_state.copy())
            self.history['extrinsic_rewards'].append(reward_extrinsic)
            self.history['intrinsic_rewards'].append(reward_intrinsic)
            self.history['total_rewards'].append(reward)
            self.history['visit_counts'].append(len(self.visit_counts))
    
            self.steps += 1
    
            terminated = self.yield_value >= 0.88
            truncated = self.steps >= 100
    
            return current_state, reward, terminated, truncated, {}
    
        def plot_exploration_analysis(self):
            """æ¢ç´¢æŒ™å‹•ã®åˆ†æ"""
            fig = plt.figure(figsize=(14, 10))
    
            # (1) çŠ¶æ…‹ç©ºé–“ã®æ¢ç´¢è»Œè·¡ï¼ˆ3Dï¼‰
            ax = fig.add_subplot(221, projection='3d')
            states = np.array(self.history['states'])
    
            scatter = ax.scatter(states[:, 0], states[:, 1], states[:, 2],
                               c=range(len(states)), cmap='viridis',
                               s=50, alpha=0.6, edgecolor='black', linewidth=0.5)
            ax.set_xlabel('Temperature [K]')
            ax.set_ylabel('Pressure [bar]')
            ax.set_zlabel('Catalyst [mol/L]')
            ax.set_title('State Space Exploration Trajectory')
            plt.colorbar(scatter, ax=ax, label='Time Step', shrink=0.6)
    
            # (2) Extrinsic vs Intrinsicå ±é…¬
            ax = fig.add_subplot(222)
            steps = np.arange(len(self.history['extrinsic_rewards']))
            ax.plot(steps, self.history['extrinsic_rewards'], 'b-',
                   linewidth=2, label='Extrinsic (task reward)', alpha=0.7)
            ax.plot(steps, self.history['intrinsic_rewards'], 'orange',
                   linewidth=2, label='Intrinsic (exploration bonus)', alpha=0.7)
            ax.plot(steps, self.history['total_rewards'], 'g-',
                   linewidth=2.5, label='Total reward')
            ax.set_xlabel('Time Step')
            ax.set_ylabel('Reward')
            ax.set_title('Reward Decomposition')
            ax.legend()
            ax.grid(alpha=0.3)
    
            # (3) æ¢ç´¢é€²æ—ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯çŠ¶æ…‹æ•°ï¼‰
            ax = fig.add_subplot(223)
            ax.plot(steps, self.history['visit_counts'], 'purple', linewidth=2.5)
            ax.fill_between(steps, 0, self.history['visit_counts'], alpha=0.3, color='purple')
            ax.set_xlabel('Time Step')
            ax.set_ylabel('Number of Unique States Visited')
            ax.set_title('Exploration Progress')
            ax.grid(alpha=0.3)
    
            # (4) è¨ªå•ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆæ¸©åº¦ vs åœ§åŠ›ã€è§¦åª’=0.5ä»˜è¿‘ï¼‰
            ax = fig.add_subplot(224)
    
            # ã‚°ãƒªãƒƒãƒ‰ã‚’ä½œæˆã—ã¦è¨ªå•å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            temp_bins = np.linspace(300, 400, 20)
            pressure_bins = np.linspace(1, 5, 20)
            heatmap = np.zeros((len(pressure_bins)-1, len(temp_bins)-1))
    
            for state in self.history['states']:
                if 0.4 <= state[2] <= 0.6:  # è§¦åª’æ¿ƒåº¦0.5ä»˜è¿‘
                    temp_idx = np.digitize(state[0], temp_bins) - 1
                    pressure_idx = np.digitize(state[1], pressure_bins) - 1
                    if 0 <= temp_idx < len(temp_bins)-1 and 0 <= pressure_idx < len(pressure_bins)-1:
                        heatmap[pressure_idx, temp_idx] += 1
    
            im = ax.imshow(heatmap, extent=[300, 400, 1, 5], aspect='auto',
                          origin='lower', cmap='YlOrRd', interpolation='bilinear')
            ax.set_xlabel('Temperature [K]')
            ax.set_ylabel('Pressure [bar]')
            ax.set_title('State Visitation Heatmap (Catalyst â‰ˆ 0.5)')
            plt.colorbar(im, ax=ax, label='Visit Count')
    
            plt.tight_layout()
            return fig
    
    
    def compare_with_without_exploration_bonus():
        """æ¢ç´¢ãƒœãƒ¼ãƒŠã‚¹ã‚ã‚Šã¨ãªã—ã‚’æ¯”è¼ƒ"""
        configs = [
            {'bonus_weight': 0.0, 'name': 'No exploration bonus'},
            {'bonus_weight': 0.3, 'name': 'With exploration bonus'}
        ]
    
        results = []
    
        for config in configs:
            env = ExplorationBonusReactor(
                exploration_bonus_weight=config['bonus_weight'],
                state_discretization=10
            )
    
            obs, _ = env.reset(seed=42)
    
            # ãƒ©ãƒ³ãƒ€ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆæ¢ç´¢åŠ¹æœã‚’è¦‹ã‚‹ãŸã‚ï¼‰
            for _ in range(100):
                action = env.action_space.sample()
                obs, reward, terminated, truncated, _ = env.step(action)
    
                if terminated or truncated:
                    break
    
            unique_states = len(env.visit_counts)
            final_yield = env.yield_value
    
            results.append({
                'name': config['name'],
                'env': env,
                'unique_states': unique_states,
                'final_yield': final_yield
            })
    
            print(f"\n=== {config['name']} ===")
            print(f"Unique states explored: {unique_states}")
            print(f"Final yield: {final_yield:.3f}")
            print(f"Exploration efficiency: {unique_states / env.steps:.3f} states/step")
    
        # å¯è¦–åŒ–
        for result in results:
            fig = result['env'].plot_exploration_analysis()
            plt.suptitle(f"{result['name']}", fontsize=14, y=1.00)
            plt.show()
    
        return results
    
    # å®Ÿè¡Œ
    results = compare_with_without_exploration_bonus()
    
    print("\n=== Exploration Bonus Impact ===")
    print(f"Without bonus: {results[0]['unique_states']} states explored")
    print(f"With bonus: {results[1]['unique_states']} states explored")
    print(f"Improvement: {(results[1]['unique_states'] - results[0]['unique_states']) / results[0]['unique_states'] * 100:.1f}%")
    

**å‡ºåŠ›ä¾‹:**  
Without bonus: 78 states explored, Final yield: 0.743  
With bonus: 145 states explored, Final yield: 0.812  
Improvement: 85.9% more states explored  
  
æ¢ç´¢ãƒœãƒ¼ãƒŠã‚¹ã«ã‚ˆã‚Šåºƒç¯„ãªçŠ¶æ…‹ç©ºé–“ã‚’æ¢ç´¢ã—ã€ã‚ˆã‚Šè‰¯ã„è§£ã‚’ç™ºè¦‹ 

## 3.5 Curriculum Learningï¼ˆæ®µéšçš„é›£æ˜“åº¦èª¿æ•´ï¼‰

è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã‚’å­¦ç¿’ã™ã‚‹éš›ã€ç°¡å˜ãªã‚¿ã‚¹ã‚¯ã‹ã‚‰å¾ã€…ã«é›£æ˜“åº¦ã‚’ä¸Šã’ã‚‹Curriculum LearningãŒæœ‰åŠ¹ã§ã™ã€‚å ±é…¬é–¢æ•°ã®ç›®æ¨™å€¤ã‚’æ®µéšçš„ã«å³ã—ãã™ã‚‹å®Ÿè£…ã‚’ç¤ºã—ã¾ã™ã€‚

### Example 5: æ®µéšçš„ãªç›®æ¨™è¨­å®š
    
    
    import numpy as np
    import matplotlib.pyplot as plt
    import gymnasium as gym
    from gymnasium import spaces
    
    # ===================================
    # Example 5: Curriculum Learning
    # ===================================
    
    class CurriculumReactor(gym.Env):
        """é›£æ˜“åº¦ãŒæ®µéšçš„ã«ä¸ŠãŒã‚‹åå¿œå™¨ç’°å¢ƒ"""
    
        def __init__(self, curriculum_level=1):
            super().__init__()
    
            self.curriculum_level = curriculum_level
            self._update_curriculum_parameters()
    
            self.observation_space = spaces.Box(
                low=np.array([300.0, -50.0, 0.0, 0]),
                high=np.array([400.0, 50.0, 1.0, 5]),
                dtype=np.float32
            )
    
            self.action_space = spaces.Box(
                low=-10.0, high=10.0, shape=(1,), dtype=np.float32
            )
    
            self.reset()
    
        def _update_curriculum_parameters(self):
            """ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´"""
            curricula = {
                1: {  # åˆç´š: ç°¡å˜ãªç›®æ¨™ã€å¤§ããªè¨±å®¹èª¤å·®
                    'target_yield': 0.60,
                    'tolerance': 0.10,
                    'disturbance_std': 0.0,
                    'time_constant': 5.0,  # é€Ÿã„å¿œç­”
                    'description': 'Easy: Low target, no disturbance'
                },
                2: {  # ä¸­ç´š: ä¸­ç¨‹åº¦ã®ç›®æ¨™ã€é€šå¸¸ã®è¨±å®¹èª¤å·®
                    'target_yield': 0.75,
                    'tolerance': 0.05,
                    'disturbance_std': 1.0,
                    'time_constant': 10.0,
                    'description': 'Medium: Higher target, small disturbance'
                },
                3: {  # ä¸Šç´š: é«˜ã„ç›®æ¨™ã€å³ã—ã„è¨±å®¹èª¤å·®ã€å¤–ä¹±ã‚ã‚Š
                    'target_yield': 0.85,
                    'tolerance': 0.03,
                    'disturbance_std': 2.0,
                    'time_constant': 15.0,  # é…ã„å¿œç­”
                    'description': 'Hard: High target, large disturbance, slow response'
                },
                4: {  # å°‚é–€å®¶ç´š: æœ€é«˜ç›®æ¨™ã€æ¥µã‚ã¦å³ã—ã„æ¡ä»¶
                    'target_yield': 0.90,
                    'tolerance': 0.02,
                    'disturbance_std': 3.0,
                    'time_constant': 20.0,
                    'description': 'Expert: Maximum target, heavy disturbance'
                },
            }
    
            params = curricula.get(self.curriculum_level, curricula[1])
    
            self.target_yield = params['target_yield']
            self.tolerance = params['tolerance']
            self.disturbance_std = params['disturbance_std']
            self.time_constant = params['time_constant']
            self.description = params['description']
    
        def set_curriculum_level(self, level):
            """ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ãƒ¬ãƒ™ãƒ«ã‚’å¤‰æ›´"""
            self.curriculum_level = np.clip(level, 1, 4)
            self._update_curriculum_parameters()
            print(f"\nğŸ“š Curriculum Level {self.curriculum_level}: {self.description}")
    
        def reset(self, seed=None):
            super().reset(seed=seed)
    
            self.temperature = 320.0 + np.random.uniform(-10, 10)
            self.current_yield = 0.0
            self.steps = 0
    
            self.history = {
                'temp': [self.temperature],
                'yield': [self.current_yield],
                'rewards': [],
                'disturbances': []
            }
    
            return self._get_obs(), {}
    
        def _get_obs(self):
            yield_error = self.target_yield - self.current_yield
            return np.array([
                self.temperature,
                yield_error,
                self.current_yield,
                self.curriculum_level
            ], dtype=np.float32)
    
        def _calculate_yield(self):
            """åç‡è¨ˆç®—ï¼ˆæ¸©åº¦ã®é–¢æ•°ï¼‰"""
            optimal_temp = 350.0
            temp_factor = np.exp(-((self.temperature - optimal_temp) / 25)**2)
            return 0.95 * temp_factor
    
        def step(self, action):
            heating_power = float(action[0])
    
            # å¤–ä¹±è¿½åŠ ï¼ˆã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ãƒ¬ãƒ™ãƒ«ãŒé«˜ã„ã»ã©å¤§ãã„ï¼‰
            disturbance = np.random.normal(0, self.disturbance_std)
    
            # æ¸©åº¦ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ï¼ˆæ™‚å®šæ•°ã§å¿œç­”é€Ÿåº¦ãŒå¤‰ã‚ã‚‹ï¼‰
            dt = 1.0
            temp_change = (heating_power + disturbance - 0.1 * (self.temperature - 298)) * dt / self.time_constant
            self.temperature += temp_change
            self.temperature = np.clip(self.temperature, 300, 400)
    
            self.current_yield = self._calculate_yield()
    
            # å ±é…¬è¨­è¨ˆ: ç›®æ¨™é”æˆåº¦ã«å¿œã˜ãŸå ±é…¬
            yield_error = abs(self.target_yield - self.current_yield)
    
            # æ®µéšçš„å ±é…¬: è¨±å®¹èª¤å·®å†…ã§å¤§ããªå ±é…¬
            if yield_error <= self.tolerance:
                reward = 10.0 + (self.tolerance - yield_error) * 50  # ãƒœãƒ¼ãƒŠã‚¹
            else:
                reward = -yield_error * 10  # ãƒšãƒŠãƒ«ãƒ†ã‚£
    
            # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚³ã‚¹ãƒˆ
            reward -= 0.01 * abs(heating_power)
    
            # å±¥æ­´è¨˜éŒ²
            self.history['temp'].append(self.temperature)
            self.history['yield'].append(self.current_yield)
            self.history['rewards'].append(reward)
            self.history['disturbances'].append(disturbance)
    
            self.steps += 1
    
            # æˆåŠŸæ¡ä»¶: ç›®æ¨™è¨±å®¹èª¤å·®å†…ã‚’10ã‚¹ãƒ†ãƒƒãƒ—ç¶­æŒ
            recent_yields = self.history['yield'][-10:]
            success = all(abs(self.target_yield - y) <= self.tolerance for y in recent_yields) if len(recent_yields) == 10 else False
    
            terminated = success
            truncated = self.steps >= 100
    
            return self._get_obs(), reward, terminated, truncated, {'success': success}
    
        def plot_curriculum_performance(self):
            """ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ å­¦ç¿’ã®é€²æ—ã‚’å¯è¦–åŒ–"""
            fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
            steps = np.arange(len(self.history['temp']))
    
            # (1) æ¸©åº¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
            ax = axes[0, 0]
            ax.plot(steps, self.history['temp'], 'r-', linewidth=2)
            ax.axhline(350, color='green', linestyle='--', linewidth=2, label='Optimal temp (350K)')
            ax.set_xlabel('Time Step')
            ax.set_ylabel('Temperature [K]')
            ax.set_title(f'Temperature Control (Level {self.curriculum_level})')
            ax.legend()
            ax.grid(alpha=0.3)
    
            # (2) åç‡ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
            ax = axes[0, 1]
            ax.plot(steps, self.history['yield'], 'b-', linewidth=2, label='Actual yield')
            ax.axhline(self.target_yield, color='green', linestyle='--',
                      linewidth=2, label=f'Target ({self.target_yield:.2f})')
            ax.axhspan(self.target_yield - self.tolerance,
                      self.target_yield + self.tolerance,
                      alpha=0.2, color='green', label=f'Tolerance (Â±{self.tolerance:.2f})')
            ax.set_xlabel('Time Step')
            ax.set_ylabel('Yield')
            ax.set_title(f'Yield Tracking (Level {self.curriculum_level})')
            ax.legend()
            ax.grid(alpha=0.3)
    
            # (3) å ±é…¬æ¨ç§»
            ax = axes[1, 0]
            reward_steps = np.arange(len(self.history['rewards']))
            ax.plot(reward_steps, self.history['rewards'], 'purple', linewidth=2, alpha=0.7)
            ax.axhline(0, color='black', linestyle='-', linewidth=0.5)
            ax.set_xlabel('Time Step')
            ax.set_ylabel('Reward')
            ax.set_title('Reward Signal')
            ax.grid(alpha=0.3)
    
            # (4) å¤–ä¹±ã®å½±éŸ¿
            ax = axes[1, 1]
            ax.plot(reward_steps, self.history['disturbances'], 'orange', linewidth=1.5, alpha=0.7)
            ax.fill_between(reward_steps, 0, self.history['disturbances'], alpha=0.3, color='orange')
            ax.axhline(0, color='black', linestyle='-', linewidth=0.5)
            ax.set_xlabel('Time Step')
            ax.set_ylabel('Disturbance [W]')
            ax.set_title(f'External Disturbance (Ïƒ={self.disturbance_std:.1f})')
            ax.grid(alpha=0.3)
    
            plt.tight_layout()
            return fig
    
    
    def progressive_curriculum_training():
        """æ®µéšçš„ãªã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ å­¦ç¿’ã®ãƒ‡ãƒ¢"""
        results = {}
    
        for level in range(1, 5):
            print(f"\n{'='*60}")
            print(f"Training at Curriculum Level {level}")
            print(f"{'='*60}")
    
            env = CurriculumReactor(curriculum_level=level)
    
            # ç°¡æ˜“åˆ¶å¾¡ãƒ«ãƒ¼ãƒ«ï¼ˆPIDé¢¨ï¼‰
            obs, _ = env.reset(seed=42)
    
            success_count = 0
            episodes = 3
    
            for ep in range(episodes):
                obs, _ = env.reset(seed=42 + ep)
    
                for _ in range(100):
                    # ç›®æ¨™åç‡ã«å‘ã‘ãŸæ¸©åº¦èª¿æ•´
                    yield_error = obs[1]  # target - current
                    optimal_temp = 350.0
                    temp_error = optimal_temp - obs[0]
    
                    # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: æ¸©åº¦èª¤å·®ã¨åç‡èª¤å·®ã«åŸºã¥ã
                    action = np.array([np.clip(2.0 * temp_error + 5.0 * yield_error, -10, 10)])
    
                    obs, reward, terminated, truncated, info = env.step(action)
    
                    if terminated:
                        if info.get('success', False):
                            success_count += 1
                        break
    
                    if truncated:
                        break
    
                print(f"Episode {ep+1}: Final yield={env.current_yield:.3f}, "
                      f"Target={env.target_yield:.3f}, "
                      f"Success={'âœ“' if info.get('success', False) else 'âœ—'}")
    
            results[level] = {
                'env': env,
                'success_rate': success_count / episodes,
                'final_yield': env.current_yield
            }
    
            # å¯è¦–åŒ–
            fig = env.plot_curriculum_performance()
            plt.suptitle(f"Curriculum Level {level}: {env.description}", fontsize=13, y=1.00)
            plt.show()
    
        # ã‚µãƒãƒªãƒ¼
        print(f"\n{'='*60}")
        print("Curriculum Learning Summary")
        print(f"{'='*60}")
        for level, result in results.items():
            print(f"Level {level}: Success rate = {result['success_rate']*100:.0f}%, "
                  f"Final yield = {result['final_yield']:.3f}")
    
        return results
    
    # å®Ÿè¡Œ
    results = progressive_curriculum_training()
    

**å‡ºåŠ›ä¾‹:**  
Level 1: Success rate = 100%, Final yield = 0.612  
Level 2: Success rate = 67%, Final yield = 0.762  
Level 3: Success rate = 33%, Final yield = 0.838  
Level 4: Success rate = 0%, Final yield = 0.876  
  
æ®µéšçš„ã«é›£æ˜“åº¦ã‚’ä¸Šã’ã‚‹ã“ã¨ã§ã€è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã¸ã®å­¦ç¿’ã‚’ä¿ƒé€² 

## 3.6 Inverse Reinforcement Learningï¼ˆé€†å¼·åŒ–å­¦ç¿’ï¼‰

å°‚é–€ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ã®æ“ä½œå±¥æ­´ã‹ã‚‰å ±é…¬é–¢æ•°ã‚’æ¨å®šã™ã‚‹Inverse RLï¼ˆIRLï¼‰ã®åŸºç¤æ¦‚å¿µã‚’ç†è§£ã—ã¾ã™ã€‚

### Example 6: å°‚é–€å®¶è»Œè·¡ã‹ã‚‰ã®å ±é…¬æ¨å®š
    
    
    import numpy as np
    import matplotlib.pyplot as plt
    from scipy.optimize import minimize
    
    # ===================================
    # Example 6: Inverse Reinforcement Learning
    # ===================================
    
    class ExpertDemonstrationCollector:
        """å°‚é–€å®¶ï¼ˆç†æƒ³çš„ãªPIDåˆ¶å¾¡ï¼‰ã®æ“ä½œè»Œè·¡ã‚’ç”Ÿæˆ"""
    
        def __init__(self, target_temp=350.0):
            self.target_temp = target_temp
    
        def expert_policy(self, current_temp, prev_error, integral_error):
            """PIDåˆ¶å¾¡ã«ã‚ˆã‚‹å°‚é–€å®¶è¡Œå‹•"""
            # PIDã‚²ã‚¤ãƒ³
            kp = 2.0
            ki = 0.1
            kd = 0.5
    
            error = self.target_temp - current_temp
            derivative = error - prev_error
    
            action = kp * error + ki * integral_error + kd * derivative
            return np.clip(action, -10, 10)
    
        def collect_demonstrations(self, n_episodes=5, episode_length=50):
            """å°‚é–€å®¶ã®æ“ä½œè»Œè·¡ã‚’åé›†"""
            demonstrations = []
    
            for ep in range(n_episodes):
                trajectory = {
                    'states': [],
                    'actions': [],
                    'next_states': []
                }
    
                # åˆæœŸçŠ¶æ…‹
                temp = 320.0 + np.random.uniform(-10, 10)
                prev_error = self.target_temp - temp
                integral_error = 0.0
    
                for step in range(episode_length):
                    # ç¾åœ¨çŠ¶æ…‹
                    state = np.array([temp, self.target_temp - temp, integral_error])
    
                    # å°‚é–€å®¶è¡Œå‹•
                    action = self.expert_policy(temp, prev_error, integral_error)
    
                    # çŠ¶æ…‹é·ç§»ï¼ˆç°¡æ˜“åå¿œå™¨ãƒ¢ãƒ‡ãƒ«ï¼‰
                    tau = 10.0
                    dt = 1.0
                    temp_change = (action - 0.1 * (temp - 298)) * dt / tau
                    temp += temp_change
                    temp = np.clip(temp, 300, 400)
    
                    error = self.target_temp - temp
                    integral_error += error * dt
                    prev_error = error
    
                    # æ¬¡çŠ¶æ…‹
                    next_state = np.array([temp, error, integral_error])
    
                    trajectory['states'].append(state)
                    trajectory['actions'].append(action)
                    trajectory['next_states'].append(next_state)
    
                demonstrations.append(trajectory)
    
            return demonstrations
    
    
    class InverseRLRewardEstimator:
        """é€†å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹å ±é…¬é–¢æ•°æ¨å®š"""
    
        def __init__(self, demonstrations):
            self.demonstrations = demonstrations
    
            # å ±é…¬é–¢æ•°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼‰
            # R(s) = w1 * f1(s) + w2 * f2(s) + w3 * f3(s)
            self.feature_names = [
                'Temperature error',
                'Absolute integral error',
                'Action smoothness'
            ]
    
        def extract_features(self, state, action, next_state):
            """çŠ¶æ…‹-è¡Œå‹•ãƒšã‚¢ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
            # ç‰¹å¾´é‡:
            # f1: æ¸©åº¦èª¤å·®ã®çµ¶å¯¾å€¤ï¼ˆå°ã•ã„ã»ã©è‰¯ã„ï¼‰
            # f2: ç©åˆ†èª¤å·®ã®çµ¶å¯¾å€¤ï¼ˆå®šå¸¸åå·®ã®æŒ‡æ¨™ï¼‰
            # f3: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å¤§ãã•ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚³ã‚¹ãƒˆï¼‰
    
            f1 = -abs(state[1])  # æ¸©åº¦èª¤å·®
            f2 = -abs(state[2])  # ç©åˆ†èª¤å·®
            f3 = -abs(action)    # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å¤§ãã•
    
            return np.array([f1, f2, f3])
    
        def compute_reward(self, features, weights):
            """é‡ã¿ä»˜ãç‰¹å¾´é‡ã®å’Œã¨ã—ã¦å ±é…¬ã‚’è¨ˆç®—"""
            return np.dot(features, weights)
    
        def estimate_reward_weights(self):
            """å°‚é–€å®¶ã®è»Œè·¡ã‹ã‚‰å ±é…¬é–¢æ•°ã®é‡ã¿ã‚’æ¨å®š"""
    
            # å…¨è»Œè·¡ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º
            all_features = []
    
            for demo in self.demonstrations:
                for i in range(len(demo['states'])):
                    features = self.extract_features(
                        demo['states'][i],
                        demo['actions'][i],
                        demo['next_states'][i]
                    )
                    all_features.append(features)
    
            all_features = np.array(all_features)
    
            # ç›®çš„: å°‚é–€å®¶ã®è»Œè·¡ãŒæœ€å¤§å ±é…¬ã‚’å¾—ã‚‹ã‚ˆã†ãªé‡ã¿ã‚’æ¨å®š
            # ç°¡æ˜“ç‰ˆ: ç‰¹å¾´é‡ã®åˆ†æ•£ã‚’æœ€å°åŒ–ã™ã‚‹é‡ã¿ï¼ˆå°‚é–€å®¶ã¯ä¸€è²«ã—ãŸè¡Œå‹•ï¼‰
    
            def objective(weights):
                """é‡ã¿ä»˜ãç‰¹å¾´é‡ã®åˆ†æ•£ã‚’è¨ˆç®—"""
                rewards = all_features @ weights
                return np.var(rewards)  # åˆ†æ•£ã‚’æœ€å°åŒ–ï¼ˆä¸€è²«æ€§ï¼‰
    
            # æœ€é©åŒ–
            w_init = np.array([1.0, 0.5, 0.2])
            bounds = [(0, 10) for _ in range(3)]
    
            result = minimize(objective, w_init, bounds=bounds, method='L-BFGS-B')
    
            estimated_weights = result.x
    
            # æ­£è¦åŒ–
            estimated_weights = estimated_weights / np.linalg.norm(estimated_weights)
    
            return estimated_weights
    
        def visualize_reward_function(self, weights):
            """æ¨å®šã•ã‚ŒãŸå ±é…¬é–¢æ•°ã‚’å¯è¦–åŒ–"""
            fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
            # å„ç‰¹å¾´é‡ã«å¯¾ã™ã‚‹å ±é…¬ã®æ„Ÿåº¦
            for i, (ax, feature_name) in enumerate(zip(axes, self.feature_names)):
                # ç‰¹å¾´é‡ã®ç¯„å›²ã§ã‚¹ã‚­ãƒ£ãƒ³
                if i == 0:  # æ¸©åº¦èª¤å·®
                    feature_range = np.linspace(-50, 0, 100)
                elif i == 1:  # ç©åˆ†èª¤å·®
                    feature_range = np.linspace(-100, 0, 100)
                else:  # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
                    feature_range = np.linspace(-10, 0, 100)
    
                rewards = weights[i] * feature_range
    
                ax.plot(feature_range, rewards, linewidth=2.5, color=['b', 'g', 'orange'][i])
                ax.fill_between(feature_range, 0, rewards, alpha=0.3, color=['b', 'g', 'orange'][i])
                ax.set_xlabel(feature_name)
                ax.set_ylabel(f'Reward contribution (w={weights[i]:.3f})')
                ax.set_title(f'{feature_name}')
                ax.grid(alpha=0.3)
                ax.axhline(0, color='black', linestyle='-', linewidth=0.5)
    
            plt.tight_layout()
            return fig
    
    
    def demonstrate_inverse_rl():
        """Inverse RLã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    
        print("=== Step 1: å°‚é–€å®¶ã®è»Œè·¡ã‚’åé›† ===")
        collector = ExpertDemonstrationCollector(target_temp=350.0)
        demonstrations = collector.collect_demonstrations(n_episodes=10, episode_length=50)
    
        print(f"Collected {len(demonstrations)} expert demonstrations")
        print(f"Each demonstration has {len(demonstrations[0]['states'])} steps")
    
        # å°‚é–€å®¶è»Œè·¡ã®å¯è¦–åŒ–
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
        for i, demo in enumerate(demonstrations[:3]):  # æœ€åˆã®3ã¤ã®ã¿è¡¨ç¤º
            temps = [s[0] for s in demo['states']]
            actions = demo['actions']
    
            ax1.plot(temps, alpha=0.7, linewidth=2, label=f'Episode {i+1}')
            ax2.plot(actions, alpha=0.7, linewidth=2, label=f'Episode {i+1}')
    
        ax1.axhline(350, color='red', linestyle='--', linewidth=2, label='Target')
        ax1.set_xlabel('Time Step')
        ax1.set_ylabel('Temperature [K]')
        ax1.set_title('Expert Temperature Trajectories')
        ax1.legend()
        ax1.grid(alpha=0.3)
    
        ax2.axhline(0, color='black', linestyle='-', linewidth=0.5)
        ax2.set_xlabel('Time Step')
        ax2.set_ylabel('Action (Heating Power)')
        ax2.set_title('Expert Actions')
        ax2.legend()
        ax2.grid(alpha=0.3)
    
        plt.tight_layout()
        plt.show()
    
        print("\n=== Step 2: å ±é…¬é–¢æ•°ã‚’æ¨å®š ===")
        estimator = InverseRLRewardEstimator(demonstrations)
        estimated_weights = estimator.estimate_reward_weights()
    
        print("\næ¨å®šã•ã‚ŒãŸå ±é…¬é–¢æ•°ã®é‡ã¿:")
        for name, weight in zip(estimator.feature_names, estimated_weights):
            print(f"  {name:25s}: {weight:.4f}")
    
        # å ±é…¬é–¢æ•°ã®å¯è¦–åŒ–
        fig = estimator.visualize_reward_function(estimated_weights)
        plt.suptitle('Estimated Reward Function from Expert Demonstrations', fontsize=13, y=1.02)
        plt.show()
    
        print("\n=== Step 3: æ¨å®šã•ã‚ŒãŸå ±é…¬é–¢æ•°ã®è§£é‡ˆ ===")
        print("é‡ã¿ã®å¤§ãã•ã¯ã€å„ç›®çš„ã®é‡è¦åº¦ã‚’ç¤ºã—ã¾ã™:")
        print(f"  - æ¸©åº¦èª¤å·®ã®æœ€å°åŒ–ãŒ{'æœ€ã‚‚' if np.argmax(estimated_weights) == 0 else ''}é‡è¦")
        print(f"  - å®šå¸¸åå·®ã®è§£æ¶ˆãŒ{'æœ€ã‚‚' if np.argmax(estimated_weights) == 1 else ''}é‡è¦")
        print(f"  - ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡ãŒ{'æœ€ã‚‚' if np.argmax(estimated_weights) == 2 else ''}é‡è¦")
    
        return demonstrations, estimated_weights
    
    # å®Ÿè¡Œ
    demonstrations, weights = demonstrate_inverse_rl()
    

**å‡ºåŠ›ä¾‹:**  
æ¨å®šã•ã‚ŒãŸå ±é…¬é–¢æ•°ã®é‡ã¿:  
Temperature error: 0.8123  
Absolute integral error: 0.4567  
Action smoothness: 0.2341  
  
å°‚é–€å®¶ã®æ„å›³ï¼ˆç›®çš„é–¢æ•°ï¼‰ã‚’è»Œè·¡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é€†ç®— 

**ğŸ’¡ IRLã®å®Ÿå‹™å¿œç”¨**

ç†Ÿç·´ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ã®æ“ä½œãƒ­ã‚°ã‹ã‚‰æš—é»™çš„ãªé‹è»¢æ–¹é‡ã‚’æŠ½å‡ºã—ã€ãã‚Œã‚’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å­¦ç¿’ã«æ´»ç”¨ã§ãã¾ã™ã€‚ç‰¹ã«ã€æ˜ç¤ºçš„ãªå ±é…¬è¨­è¨ˆãŒå›°é›£ãªå ´åˆã«æœ‰åŠ¹ã§ã™ã€‚

## 3.7 å ±é…¬é–¢æ•°ã®è©•ä¾¡ã¨æ¯”è¼ƒ

è¨­è¨ˆã—ãŸè¤‡æ•°ã®å ±é…¬é–¢æ•°ã‚’å®šé‡çš„ã«è©•ä¾¡ã—ã€æœ€é©ãªè¨­è¨ˆã‚’é¸æŠã™ã‚‹æ–¹æ³•ã‚’å­¦ã³ã¾ã™ã€‚

### Example 7: å ±é…¬é–¢æ•°ã®æ€§èƒ½è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
    
    
    import numpy as np
    import matplotlib.pyplot as plt
    import gymnasium as gym
    from gymnasium import spaces
    
    # ===================================
    # Example 7: å ±é…¬é–¢æ•°ã®è©•ä¾¡ã¨æ¯”è¼ƒ
    # ===================================
    
    class ReactorBenchmark(gym.Env):
        """å ±é…¬é–¢æ•°è©•ä¾¡ç”¨ã®æ¨™æº–åå¿œå™¨ç’°å¢ƒ"""
    
        def __init__(self, reward_function_type='basic'):
            super().__init__()
    
            self.reward_function_type = reward_function_type
    
            self.observation_space = spaces.Box(
                low=np.array([300.0, -50.0, 0.0]),
                high=np.array([400.0, 50.0, 1.0]),
                dtype=np.float32
            )
    
            self.action_space = spaces.Box(
                low=-10.0, high=10.0, shape=(1,), dtype=np.float32
            )
    
            self.target_temp = 350.0
            self.reset()
    
        def reset(self, seed=None):
            super().reset(seed=seed)
    
            self.temperature = 320.0 + np.random.uniform(-10, 10)
            self.prev_error = self.target_temp - self.temperature
            self.integral_error = 0.0
            self.steps = 0
    
            self.metrics = {
                'tracking_errors': [],
                'energy_consumption': [],
                'safety_violations': 0,
                'rewards': []
            }
    
            return self._get_obs(), {}
    
        def _get_obs(self):
            error = self.target_temp - self.temperature
            return np.array([self.temperature, error, self.integral_error], dtype=np.float32)
    
        def _reward_basic(self, error, action):
            """åŸºæœ¬çš„ãªå ±é…¬: èª¤å·®ã®ã¿"""
            return -abs(error)
    
        def _reward_pid_inspired(self, error, action):
            """PIDé¢¨å ±é…¬"""
            r_p = -1.0 * abs(error)
            r_i = -0.1 * abs(self.integral_error)
            r_d = -0.5 * abs(error - self.prev_error)
            return r_p + r_i + r_d
    
        def _reward_multi_objective(self, error, action):
            """å¤šç›®çš„å ±é…¬"""
            r_tracking = -abs(error)
            r_energy = -0.05 * abs(action)
            r_safety = -2.0 * max(0, self.temperature - 380)
            return r_tracking + r_energy + r_safety
    
        def _reward_shaped(self, error, action):
            """Shapedå ±é…¬ï¼ˆå¯†ãªå ±é…¬ï¼‰"""
            # é€²æ—ãƒœãƒ¼ãƒŠã‚¹
            progress = abs(self.prev_error) - abs(error)
            r_progress = 2.0 * progress
    
            # ç›®æ¨™ä»˜è¿‘ã§ã®ãƒœãƒ¼ãƒŠã‚¹
            proximity_bonus = 5.0 * np.exp(-abs(error) / 5.0)
    
            # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚¹ãƒ ãƒ¼ã‚ºãƒã‚¹ãƒšãƒŠãƒ«ãƒ†ã‚£
            r_smoothness = -0.02 * abs(action)
    
            return r_progress + proximity_bonus + r_smoothness
    
        def step(self, action):
            heating_power = float(action[0])
    
            # æ¸©åº¦ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹
            tau = 10.0
            dt = 1.0
            disturbance = np.random.normal(0, 1.0)
    
            temp_change = (heating_power + disturbance - 0.1 * (self.temperature - 298)) * dt / tau
            self.temperature += temp_change
            self.temperature = np.clip(self.temperature, 300, 400)
    
            error = self.target_temp - self.temperature
            self.integral_error += error * dt
    
            # å ±é…¬é–¢æ•°ã®é¸æŠ
            reward_functions = {
                'basic': self._reward_basic,
                'pid_inspired': self._reward_pid_inspired,
                'multi_objective': self._reward_multi_objective,
                'shaped': self._reward_shaped
            }
    
            reward_func = reward_functions.get(self.reward_function_type, self._reward_basic)
            reward = reward_func(error, heating_power)
    
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
            self.metrics['tracking_errors'].append(abs(error))
            self.metrics['energy_consumption'].append(abs(heating_power))
            if self.temperature > 380:
                self.metrics['safety_violations'] += 1
            self.metrics['rewards'].append(reward)
    
            self.prev_error = error
            self.steps += 1
    
            terminated = abs(error) <= 2.0 and self.steps >= 20
            truncated = self.steps >= 100
    
            return self._get_obs(), reward, terminated, truncated, {}
    
    
    class RewardFunctionEvaluator:
        """å ±é…¬é–¢æ•°ã®å®šé‡çš„è©•ä¾¡"""
    
        def __init__(self):
            self.reward_types = ['basic', 'pid_inspired', 'multi_objective', 'shaped']
            self.results = {}
    
        def evaluate_reward_function(self, reward_type, n_episodes=10):
            """æŒ‡å®šã•ã‚ŒãŸå ±é…¬é–¢æ•°ã§è©•ä¾¡å®Ÿé¨“ã‚’å®Ÿè¡Œ"""
            env = ReactorBenchmark(reward_function_type=reward_type)
    
            episode_metrics = []
    
            for ep in range(n_episodes):
                obs, _ = env.reset(seed=ep)
    
                # ã‚·ãƒ³ãƒ—ãƒ«ãªPIDåˆ¶å¾¡ãƒ«ãƒ¼ãƒ«
                for _ in range(100):
                    error = obs[1]
                    integral = obs[2]
    
                    action = np.array([np.clip(2.0 * error + 0.1 * integral, -10, 10)])
                    obs, reward, terminated, truncated, _ = env.step(action)
    
                    if terminated or truncated:
                        break
    
                episode_metrics.append(env.metrics)
    
            # çµ±è¨ˆé›†è¨ˆ
            aggregated = {
                'mae': np.mean([np.mean(m['tracking_errors']) for m in episode_metrics]),
                'rmse': np.sqrt(np.mean([np.mean(np.array(m['tracking_errors'])**2) for m in episode_metrics])),
                'total_energy': np.mean([np.sum(m['energy_consumption']) for m in episode_metrics]),
                'safety_violations': np.mean([m['safety_violations'] for m in episode_metrics]),
                'total_reward': np.mean([np.sum(m['rewards']) for m in episode_metrics]),
                'convergence_time': np.mean([np.argmax(np.array(m['tracking_errors']) < 5.0) for m in episode_metrics])
            }
    
            return aggregated
    
        def compare_all_reward_functions(self, n_episodes=10):
            """å…¨ã¦ã®å ±é…¬é–¢æ•°ã‚’æ¯”è¼ƒè©•ä¾¡"""
            print("="*70)
            print("Reward Function Comparison Benchmark")
            print("="*70)
    
            for reward_type in self.reward_types:
                print(f"\nEvaluating: {reward_type}")
                results = self.evaluate_reward_function(reward_type, n_episodes=n_episodes)
                self.results[reward_type] = results
    
                print(f"  MAE (tracking error): {results['mae']:.3f}")
                print(f"  RMSE: {results['rmse']:.3f}")
                print(f"  Total energy: {results['total_energy']:.1f}")
                print(f"  Safety violations: {results['safety_violations']:.1f}")
                print(f"  Total reward: {results['total_reward']:.1f}")
                print(f"  Convergence time: {results['convergence_time']:.1f} steps")
    
            return self.results
    
        def visualize_comparison(self):
            """æ¯”è¼ƒçµæœã®å¯è¦–åŒ–"""
            metrics = ['mae', 'rmse', 'total_energy', 'safety_violations', 'convergence_time']
            metric_labels = [
                'MAE [K]',
                'RMSE [K]',
                'Total Energy',
                'Safety Violations',
                'Convergence Time [steps]'
            ]
    
            fig, axes = plt.subplots(2, 3, figsize=(15, 8))
            axes = axes.flatten()
    
            for i, (metric, label) in enumerate(zip(metrics, metric_labels)):
                ax = axes[i]
    
                values = [self.results[rt][metric] for rt in self.reward_types]
                colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']
    
                bars = ax.bar(self.reward_types, values, color=colors, alpha=0.7,
                             edgecolor='black', linewidth=1.5)
    
                # æœ€è‰¯å€¤ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
                if metric in ['mae', 'rmse', 'total_energy', 'safety_violations', 'convergence_time']:
                    best_idx = np.argmin(values)
                else:
                    best_idx = np.argmax(values)
    
                bars[best_idx].set_edgecolor('gold')
                bars[best_idx].set_linewidth(3)
    
                ax.set_ylabel(label)
                ax.set_title(label)
                ax.tick_params(axis='x', rotation=15)
                ax.grid(axis='y', alpha=0.3)
    
            # ç·åˆã‚¹ã‚³ã‚¢ï¼ˆæ­£è¦åŒ–ã—ã¦åˆè¨ˆï¼‰
            ax = axes[5]
    
            # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’0-1ã«æ­£è¦åŒ–ï¼ˆå°ã•ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ï¼‰
            normalized_scores = {}
            for rt in self.reward_types:
                score = 0
                for metric in ['mae', 'rmse', 'total_energy', 'safety_violations', 'convergence_time']:
                    values = [self.results[r][metric] for r in self.reward_types]
                    min_val, max_val = min(values), max(values)
                    normalized = (max_val - self.results[rt][metric]) / (max_val - min_val + 1e-10)
                    score += normalized
                normalized_scores[rt] = score / 5  # å¹³å‡
    
            bars = ax.bar(normalized_scores.keys(), normalized_scores.values(),
                         color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)
    
            best_idx = np.argmax(list(normalized_scores.values()))
            bars[best_idx].set_edgecolor('gold')
            bars[best_idx].set_linewidth(3)
    
            ax.set_ylabel('Overall Score (normalized)')
            ax.set_title('Overall Performance Score')
            ax.tick_params(axis='x', rotation=15)
            ax.grid(axis='y', alpha=0.3)
            ax.set_ylim([0, 1])
    
            plt.tight_layout()
            return fig
    
        def print_recommendation(self):
            """æ¨å¥¨äº‹é …ã®å‡ºåŠ›"""
            print("\n" + "="*70)
            print("Recommendation")
            print("="*70)
    
            # ç·åˆè©•ä¾¡
            scores = {}
            for rt in self.reward_types:
                score = 0
                for metric in ['mae', 'rmse', 'total_energy', 'safety_violations', 'convergence_time']:
                    values = [self.results[r][metric] for r in self.reward_types]
                    min_val, max_val = min(values), max(values)
                    normalized = (max_val - self.results[rt][metric]) / (max_val - min_val + 1e-10)
                    score += normalized
                scores[rt] = score / 5
    
            best_reward = max(scores, key=scores.get)
    
            print(f"\nâœ… Best overall: {best_reward} (score: {scores[best_reward]:.3f})")
    
            print("\nğŸ“Š Use case recommendations:")
            print("  - basic: ã‚·ãƒ³ãƒ—ãƒ«ãªæ¸©åº¦åˆ¶å¾¡ï¼ˆå­¦ç¿’åˆæœŸæ®µéšï¼‰")
            print("  - pid_inspired: å®šå¸¸åå·®ã®è§£æ¶ˆãŒé‡è¦ãªå ´åˆ")
            print("  - multi_objective: å®‰å…¨æ€§ãƒ»ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡ã‚’åŒæ™‚è€ƒæ…®")
            print("  - shaped: å­¦ç¿’é€Ÿåº¦ã‚’é‡è¦–ã™ã‚‹å ´åˆï¼ˆå¯†ãªå ±é…¬ï¼‰")
    
    
    # å®Ÿè¡Œ
    evaluator = RewardFunctionEvaluator()
    results = evaluator.compare_all_reward_functions(n_episodes=20)
    
    fig = evaluator.visualize_comparison()
    plt.show()
    
    evaluator.print_recommendation()
    

**å‡ºåŠ›ä¾‹:**  
basic: MAE=4.567, Total reward=-245.3  
pid_inspired: MAE=2.134, Total reward=-189.7  
multi_objective: MAE=2.890, Total reward=-156.2  
shaped: MAE=2.045, Total reward=+123.4  
  
Best overall: shaped (å­¦ç¿’é€Ÿåº¦ã¨ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹) 

**âœ… å ±é…¬é–¢æ•°é¸æŠã®ãƒã‚¤ãƒ³ãƒˆ**

  * **å­¦ç¿’åˆæœŸ** : Denseå ±é…¬ï¼ˆshapedï¼‰ã§ç´ æ—©ãå­¦ç¿’
  * **å®Ÿé‹ç”¨** : Multi-objectiveå ±é…¬ã§å®Ÿç”¨æ€§ã¨ãƒãƒ©ãƒ³ã‚¹
  * **å®‰å…¨æ€§é‡è¦–** : å®‰å…¨åˆ¶ç´„ã¸ã®é«˜ãƒšãƒŠãƒ«ãƒ†ã‚£è¨­å®š

## å­¦ç¿’ç›®æ¨™ã®ç¢ºèª

ã“ã®ç« ã‚’å®Œäº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ã‚’èª¬æ˜ãƒ»å®Ÿè£…ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

### åŸºæœ¬ç†è§£

  * âœ… Sparseå ±é…¬ã¨Denseå ±é…¬ã®é•ã„ã¨ä½¿ã„åˆ†ã‘ã‚’èª¬æ˜ã§ãã‚‹
  * âœ… PIDåˆ¶å¾¡ç†è«–ã‚’å ±é…¬è¨­è¨ˆã«å¿œç”¨ã™ã‚‹æ–¹æ³•ã‚’ç†è§£ã—ã¦ã„ã‚‹
  * âœ… å¤šç›®çš„æœ€é©åŒ–ã®é‡ã¿èª¿æ•´ãŒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæŒ™å‹•ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’èª¬æ˜ã§ãã‚‹

### å®Ÿè·µã‚¹ã‚­ãƒ«

  * âœ… ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã‚¿ã‚¹ã‚¯ã«é©ã—ãŸå ±é…¬é–¢æ•°ã‚’è¨­è¨ˆã§ãã‚‹
  * âœ… Intrinsic motivationã‚’å®Ÿè£…ã—ã¦æ¢ç´¢åŠ¹ç‡ã‚’å‘ä¸Šã§ãã‚‹
  * âœ… Curriculum learningã§æ®µéšçš„ãªå­¦ç¿’ç’°å¢ƒã‚’æ§‹ç¯‰ã§ãã‚‹
  * âœ… è¤‡æ•°ã®å ±é…¬é–¢æ•°ã‚’å®šé‡çš„ã«è©•ä¾¡ãƒ»æ¯”è¼ƒã§ãã‚‹

### å¿œç”¨åŠ›

  * âœ… å°‚é–€å®¶è»Œè·¡ã‹ã‚‰Inverse RLã§å ±é…¬ã‚’æ¨å®šã§ãã‚‹
  * âœ… å®Ÿãƒ—ãƒ­ã‚»ã‚¹ã®åˆ¶ç´„æ¡ä»¶ã‚’å ±é…¬ã«çµ„ã¿è¾¼ã‚ã‚‹
  * âœ… å ±é…¬è¨­è¨ˆã®è‰¯ã—æ‚ªã—ã‚’å®Ÿé¨“çš„ã«æ¤œè¨¼ã§ãã‚‹

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ç¬¬3ç« ã§ã¯ã€åŠ¹æœçš„ãªå ±é…¬è¨­è¨ˆã®æ–¹æ³•ã‚’å­¦ã³ã¾ã—ãŸã€‚æ¬¡ç« ã§ã¯ã€è¤‡æ•°ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå”èª¿ãƒ»ç«¶äº‰ã—ãªãŒã‚‰å‹•ä½œã™ã‚‹ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã«ã¤ã„ã¦å­¦ã³ã¾ã™ã€‚

**ğŸ“š æ¬¡ç« ã®å†…å®¹ï¼ˆç¬¬4ç« äºˆå‘Šï¼‰**

  * ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¼·åŒ–å­¦ç¿’ã®åŸºç¤
  * ä¸­å¤®é›†æ¨©å‹å­¦ç¿’ã¨åˆ†æ•£å®Ÿè¡Œï¼ˆCTDEï¼‰
  * ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“é€šä¿¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«
  * å”èª¿ãƒ»ç«¶äº‰ã‚¿ã‚¹ã‚¯ã®å®Ÿè£…

### å…è²¬äº‹é …

  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹Code examplesã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚
  * å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚
