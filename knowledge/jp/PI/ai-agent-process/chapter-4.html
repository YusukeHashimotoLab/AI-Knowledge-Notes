<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 4: ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå”èª¿åˆ¶å¾¡ - AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹è‡ªå¾‹ãƒ—ãƒ­ã‚»ã‚¹é‹è»¢</title>
    <meta name="description" content="è¤‡æ•°ã®AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå”èª¿ã—ã¦ãƒ—ãƒ­ã‚»ã‚¹ã‚’åˆ¶å¾¡ã™ã‚‹æ‰‹æ³•ã‚’å­¦ã³ã¾ã™ã€‚CTDEã€QMIXã€é€šä¿¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã€å”èª¿ãƒ»ç«¶äº‰ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè£…ä¾‹ã¨ã¨ã‚‚ã«è§£èª¬ã€‚">

    <!-- CSS -->
    <!-- CSS is inline -->

    <!-- Prism.js for syntax highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">

    <!-- MathJax for mathematical expressions -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>

        <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.8; color: #333; background: #f5f5f5;
        }
        header {
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white; padding: 2rem 1rem; text-align: center;
        }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; }
        .subtitle { opacity: 0.9; font-size: 1.1rem; }
        .container { max-width: 1200px; margin: 2rem auto; padding: 0 1rem; }
        .back-link {
            display: inline-block; margin-bottom: 2rem; padding: 0.5rem 1rem;
            background: white; color: #11998e; text-decoration: none;
            border-radius: 6px; font-weight: 600;
        }
        .content-box {
            background: white; padding: 2rem; border-radius: 12px;
            margin-bottom: 2rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        h2 {
            color: #11998e; margin: 2rem 0 1rem 0;
            padding-bottom: 0.5rem; border-bottom: 3px solid #11998e;
        }
        h3 { color: #2c3e50; margin: 1.5rem 0 1rem 0; }
        h4 { color: #2c3e50; margin: 1rem 0 0.5rem 0; }
        p { margin-bottom: 1rem; }
        ul, ol { margin-left: 2rem; margin-bottom: 1rem; }
        li { margin-bottom: 0.5rem; }
        pre {
            background: #1e1e1e; color: #d4d4d4; padding: 1.5rem;
            border-radius: 8px; overflow-x: auto; margin: 1rem 0;
            border-left: 4px solid #11998e;
        }
        code {
            font-family: 'Courier New', monospace; font-size: 0.9rem;
        }
        .key-point {
            background: #e8f5e9; padding: 1rem; border-radius: 6px;
            border-left: 4px solid #4caf50; margin: 1rem 0;
        }
        .tech-note {
            background: #e3f2fd; padding: 1rem; border-radius: 6px;
            border-left: 4px solid #2196f3; margin: 1rem 0;
        }
        .formula {
            background: #f0f7ff; padding: 1rem; border-radius: 6px;
            margin: 1rem 0; overflow-x: auto;
        }
        table {
            width: 100%; border-collapse: collapse; margin: 1rem 0;
        }
        th, td {
            border: 1px solid #ddd; padding: 0.75rem; text-align: left;
        }
        th {
            background: #11998e; color: white; font-weight: 600;
        }
        tr:nth-child(even) { background: #f9f9f9; }
        .nav-buttons {
            display: flex; justify-content: space-between; margin-top: 3rem;
        }
        .nav-buttons a {
            padding: 0.75rem 1.5rem;
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white; text-decoration: none; border-radius: 6px;
            font-weight: 600;
        }
        footer {
            background: #2c3e50; color: white; text-align: center;
            padding: 2rem 1rem; margin-top: 4rem;
        }
        @media (max-width: 768px) {
            h1 { font-size: 1.6rem; }
            .container { padding: 0 0.5rem; }
            pre { padding: 1rem; }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/PI/ai-agent-process/chapter-4.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../PI/index.html">ãƒ—ãƒ­ã‚»ã‚¹ãƒ»ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹</a><span class="breadcrumb-separator">â€º</span><a href="../../PI/ai-agent-process/index.html">Ai Agent Process</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 4</span>
        </div>
    </nav>

        <header>
        <h1>AIå¯ºå­å±‹</h1>
        <p class="subtitle">AIã¨ãƒãƒ†ãƒªã‚¢ãƒ«ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„</p>
    </header>

    <main class="article-content">
        <article>
            <div class="series-header">
                <div class="series-title">AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹è‡ªå¾‹ãƒ—ãƒ­ã‚»ã‚¹é‹è»¢ ã‚·ãƒªãƒ¼ã‚º</div>
                <h1 class="chapter-title">Chapter 4: ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå”èª¿åˆ¶å¾¡</h1>
                <div class="chapter-nav">
                    <a href="chapter-3.html">â† Chapter 3: ãƒ¢ãƒ‡ãƒ«ãƒ•ãƒªãƒ¼åˆ¶å¾¡</a>
                    <a href="chapter-5.html">Chapter 5: å®Ÿãƒ—ãƒ©ãƒ³ãƒˆã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã¨å®‰å…¨æ€§ â†’</a>
                    <a href="index.html">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a>
                </div>
            </div>

            <section id="overview">
                <h2>Chapter 4ã®æ¦‚è¦</h2>
                <p>
                    è¤‡é›‘ãªãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ©ãƒ³ãƒˆã§ã¯ã€è¤‡æ•°ã®åå¿œå™¨ã‚„è’¸ç•™å¡”ãŒç›¸äº’ã«å½±éŸ¿ã—åˆã„ãªãŒã‚‰é‹è»¢ã•ã‚Œã¾ã™ã€‚
                    ã“ã®ã‚ˆã†ãªåˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€å˜ä¸€ã®AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã¯ãªãã€è¤‡æ•°ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå”èª¿ã—ã¦åˆ¶å¾¡ã‚’è¡Œã†ã“ã¨ã§ã€
                    ã‚ˆã‚ŠåŠ¹ç‡çš„ã§æŸ”è»Ÿãªé‹è»¢ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚
                </p>
                <p>
                    æœ¬ç« ã§ã¯ã€ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¼·åŒ–å­¦ç¿’ï¼ˆMARL: Multi-Agent Reinforcement Learningï¼‰ã®åŸºç¤ã‹ã‚‰ã€
                    å®Ÿéš›ã®ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã¸ã®å¿œç”¨ã¾ã§ã‚’7ã¤ã®å®Ÿè£…ä¾‹ã¨ã¨ã‚‚ã«è§£èª¬ã—ã¾ã™ã€‚
                </p>

                <div class="key-points">
                    <h3>æœ¬ç« ã§å­¦ã¶ã“ã¨</h3>
                    <ul>
                        <li><strong>CTDEï¼ˆCentralized Training with Decentralized Executionï¼‰</strong>ï¼šå­¦ç¿’æ™‚ã¯ä¸­å¤®é›†æ¨©ã€å®Ÿè¡Œæ™‚ã¯åˆ†æ•£</li>
                        <li><strong>Independent Q-Learning</strong>ï¼šå„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç‹¬ç«‹ã—ã¦å­¦ç¿’</li>
                        <li><strong>QMIX</strong>ï¼šä¾¡å€¤é–¢æ•°ã®åˆ†è§£ã¨æ··åˆã«ã‚ˆã‚‹ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆå‰²å½“</li>
                        <li><strong>ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“é€šä¿¡</strong>ï¼šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã«ã‚ˆã‚‹å”èª¿</li>
                        <li><strong>å”èª¿ã‚¿ã‚¹ã‚¯</strong>ï¼šè¤‡æ•°åå¿œå™¨ã®åŒæœŸåˆ¶å¾¡</li>
                        <li><strong>ç«¶äº‰ã‚¿ã‚¹ã‚¯</strong>ï¼šé™ã‚‰ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ã®é…åˆ†</li>
                        <li><strong>æ··åˆã‚¿ã‚¹ã‚¯</strong>ï¼šå”èª¿ã¨ç«¶äº‰ã®ä¸¡é¢ã‚’æŒã¤ç¾å®Ÿçš„ãªã‚·ãƒŠãƒªã‚ª</li>
                    </ul>
                </div>
            </section>

            <section id="marl-basics">
                <h2>ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¼·åŒ–å­¦ç¿’ã®åŸºç¤</h2>

                <h3>å®šå¼åŒ–</h3>
                <p>
                    ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç³»ã¯ã€éƒ¨åˆ†è¦³æ¸¬ãƒãƒ«ã‚³ãƒ•ã‚²ãƒ¼ãƒ ï¼ˆPartially Observable Stochastic Gameï¼‰ã¨ã—ã¦å®šå¼åŒ–ã•ã‚Œã¾ã™ï¼š
                </p>

                <div class="formula-box">
                    <p><strong>ãƒãƒ«ã‚³ãƒ•ã‚²ãƒ¼ãƒ ï¼š</strong></p>
                    \[
                    \mathcal{G} = \langle \mathcal{N}, \mathcal{S}, \{\mathcal{A}^i\}_{i \in \mathcal{N}}, \mathcal{T}, \{R^i\}_{i \in \mathcal{N}}, \{\mathcal{O}^i\}_{i \in \mathcal{N}}, \gamma \rangle
                    \]
                    <ul style="margin-top: 1rem;">
                        <li>\(\mathcal{N} = \{1, 2, \ldots, n\}\)ï¼šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé›†åˆ</li>
                        <li>\(\mathcal{S}\)ï¼šã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ç©ºé–“</li>
                        <li>\(\mathcal{A}^i\)ï¼šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ\(i\)ã®è¡Œå‹•ç©ºé–“</li>
                        <li>\(\mathcal{T}: \mathcal{S} \times \mathcal{A}^1 \times \cdots \times \mathcal{A}^n \to \Delta(\mathcal{S})\)ï¼šçŠ¶æ…‹é·ç§»é–¢æ•°</li>
                        <li>\(R^i: \mathcal{S} \times \mathcal{A}^1 \times \cdots \times \mathcal{A}^n \to \mathbb{R}\)ï¼šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ\(i\)ã®å ±é…¬é–¢æ•°</li>
                        <li>\(\mathcal{O}^i\)ï¼šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ\(i\)ã®è¦³æ¸¬ç©ºé–“</li>
                    </ul>
                </div>

                <h3>å”èª¿ãƒ»ç«¶äº‰ãƒ»æ··åˆã®åˆ†é¡</h3>
                <div class="mermaid">
                graph TD
                    A[ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¿ã‚¹ã‚¯] --> B[å®Œå…¨å”èª¿]
                    A --> C[å®Œå…¨ç«¶äº‰]
                    A --> D[æ··åˆ]
                    B --> E[å…±é€šå ±é…¬<br/>RÂ¹=RÂ²=...=Râ¿]
                    C --> F[ã‚¼ãƒ­ã‚µãƒ <br/>Î£áµ¢ Râ± = 0]
                    D --> G[ä¸€èˆ¬ã‚²ãƒ¼ãƒ <br/>å”èª¿+ç«¶äº‰]
                </div>

                <p>
                    ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã§ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªçŠ¶æ³ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ï¼š
                </p>
                <ul>
                    <li><strong>å”èª¿ã‚¿ã‚¹ã‚¯</strong>ï¼šè¤‡æ•°ã®åå¿œå™¨ã‚’å”èª¿ã•ã›ã¦å…¨ä½“ã®ç”Ÿç”£æ€§ã‚’æœ€å¤§åŒ–</li>
                    <li><strong>ç«¶äº‰ã‚¿ã‚¹ã‚¯</strong>ï¼šé™ã‚‰ã‚ŒãŸãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆè’¸æ°—ã€å†·å´æ°´ï¼‰ã‚’å„ãƒ—ãƒ©ãƒ³ãƒˆãŒå–ã‚Šåˆã†</li>
                    <li><strong>æ··åˆã‚¿ã‚¹ã‚¯</strong>ï¼šå„ãƒ—ãƒ©ãƒ³ãƒˆãŒè‡ªèº«ã®ç”Ÿç”£ç›®æ¨™ã‚’é”æˆã—ã¤ã¤ã€å…¨ä½“ã®å®‰å®šæ€§ã‚‚ç¶­æŒ</li>
                </ul>
            </section>

            <section id="example1">
                <div class="example-box">
                    <div class="example-title">
                        <span class="example-number">1</span>
                        CTDEï¼ˆCentralized Training with Decentralized Executionï¼‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
                    </div>
                    <p>
                        CTDEã¯ã€å­¦ç¿’æ™‚ã«ã¯å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æƒ…å ±ã‚’ä½¿ã£ã¦ä¸­å¤®é›†æ¨©çš„ã«å­¦ç¿’ã—ã€
                        å®Ÿè¡Œæ™‚ã«ã¯å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè‡ªèº«ã®è¦³æ¸¬ã®ã¿ã§åˆ†æ•£çš„ã«è¡Œå‹•ã™ã‚‹æ çµ„ã¿ã§ã™ã€‚
                        ã“ã‚Œã«ã‚ˆã‚Šã€å­¦ç¿’ã®åŠ¹ç‡æ€§ã¨å®Ÿè¡Œæ™‚ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’ä¸¡ç«‹ã—ã¾ã™ã€‚
                    </p>

                    <div class="mermaid">
                    graph LR
                        subgraph Training[å­¦ç¿’æ™‚ï¼ˆä¸­å¤®é›†æ¨©ï¼‰]
                            S[ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹] --> C[ä¸­å¤®Critic]
                            O1[è¦³æ¸¬1] --> A1[Actor1]
                            O2[è¦³æ¸¬2] --> A2[Actor2]
                            O3[è¦³æ¸¬3] --> A3[Actor3]
                            C --> A1
                            C --> A2
                            C --> A3
                        end
                        subgraph Execution[å®Ÿè¡Œæ™‚ï¼ˆåˆ†æ•£ï¼‰]
                            O1'[è¦³æ¸¬1] --> A1'[Actor1]
                            O2' [è¦³æ¸¬2] --> A2'[Actor2]
                            O3' [è¦³æ¸¬3] --> A3'[Actor3]
                        end
                    </div>

                    <pre><code class="language-python"># CTDEåŸºæœ¬ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å®Ÿè£…
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

class Actor(nn.Module):
    """å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®Actorï¼ˆåˆ†æ•£å®Ÿè¡Œå¯èƒ½ï¼‰"""
    def __init__(self, obs_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, 64), nn.ReLU(),
            nn.Linear(64, 64), nn.ReLU(),
            nn.Linear(64, action_dim), nn.Tanh()
        )

    def forward(self, obs):
        return self.net(obs)

class CentralizedCritic(nn.Module):
    """ä¸­å¤®Criticï¼ˆå­¦ç¿’æ™‚ã®ã¿ä½¿ç”¨ï¼‰"""
    def __init__(self, state_dim, n_agents, action_dim):
        super().__init__()
        total_action_dim = n_agents * action_dim
        self.net = nn.Sequential(
            nn.Linear(state_dim + total_action_dim, 128), nn.ReLU(),
            nn.Linear(128, 128), nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, state, actions):
        # actions: [n_agents, action_dim] -> flatten
        x = torch.cat([state, actions.flatten()], dim=-1)
        return self.net(x)

class CTDEAgent:
    """CTDEå­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self, n_agents, obs_dim, action_dim, state_dim):
        self.n_agents = n_agents
        self.actors = [Actor(obs_dim, action_dim) for _ in range(n_agents)]
        self.critic = CentralizedCritic(state_dim, n_agents, action_dim)

        self.actor_opts = [optim.Adam(a.parameters(), lr=3e-4) for a in self.actors]
        self.critic_opt = optim.Adam(self.critic.parameters(), lr=1e-3)

    def select_actions(self, observations):
        """å®Ÿè¡Œæ™‚ï¼šå„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç‹¬ç«‹ã«è¡Œå‹•é¸æŠ"""
        actions = []
        for i, obs in enumerate(observations):
            with torch.no_grad():
                obs_t = torch.FloatTensor(obs)
                action = self.actors[i](obs_t).numpy()
            actions.append(action)
        return np.array(actions)

    def train_step(self, batch):
        """å­¦ç¿’æ™‚ï¼šä¸­å¤®Criticã‚’ä½¿ã£ãŸæ›´æ–°"""
        states, obs, actions, rewards, next_states, next_obs, dones = batch

        # Criticæ›´æ–°ï¼ˆTDèª¤å·®ï¼‰
        with torch.no_grad():
            next_actions = torch.stack([
                self.actors[i](torch.FloatTensor(next_obs[i]))
                for i in range(self.n_agents)
            ])
            target_q = rewards + 0.99 * self.critic(
                torch.FloatTensor(next_states), next_actions
            ) * (1 - dones)

        current_q = self.critic(torch.FloatTensor(states), torch.FloatTensor(actions))
        critic_loss = nn.MSELoss()(current_q, target_q)

        self.critic_opt.zero_grad()
        critic_loss.backward()
        self.critic_opt.step()

        # Actoræ›´æ–°ï¼ˆãƒãƒªã‚·ãƒ¼å‹¾é…ï¼‰
        for i in range(self.n_agents):
            new_actions = [
                self.actors[j](torch.FloatTensor(obs[j])) if j == i
                else torch.FloatTensor(actions[j])
                for j in range(self.n_agents)
            ]
            new_actions = torch.stack(new_actions)

            actor_loss = -self.critic(torch.FloatTensor(states), new_actions).mean()

            self.actor_opts[i].zero_grad()
            actor_loss.backward()
            self.actor_opts[i].step()

# ä½¿ç”¨ä¾‹ï¼š3ã¤ã®CSTRã®å”èª¿åˆ¶å¾¡
n_agents = 3
agent = CTDEAgent(n_agents=n_agents, obs_dim=4, action_dim=2, state_dim=12)

# å®Ÿè¡Œæ™‚ã¯åˆ†æ•£ï¼ˆå„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯è‡ªèº«ã®è¦³æ¸¬ã®ã¿ä½¿ç”¨ï¼‰
observations = [np.random.randn(4) for _ in range(n_agents)]
actions = agent.select_actions(observations)  # åˆ†æ•£å®Ÿè¡Œ
print(f"åˆ†æ•£å®Ÿè¡Œã§ã®è¡Œå‹•: {actions.shape}")  # (3, 2)</code></pre>
                </div>
            </section>

            <section id="example2">
                <div class="example-box">
                    <div class="example-title">
                        <span class="example-number">2</span>
                        Independent Q-Learningï¼ˆIQLï¼‰ã«ã‚ˆã‚‹ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’
                    </div>
                    <p>
                        æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’æ‰‹æ³•ã¯ã€å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç‹¬ç«‹ã—ã¦Qå­¦ç¿’ã‚’è¡Œã†ã‚‚ã®ã§ã™ã€‚
                        ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ç’°å¢ƒã®ä¸€éƒ¨ã¨ã¿ãªã•ã‚Œã€éå®šå¸¸æ€§ã®å•é¡ŒãŒã‚ã‚Šã¾ã™ãŒã€å®Ÿè£…ãŒå®¹æ˜“ã§å®Ÿç”¨çš„ãªã‚±ãƒ¼ã‚¹ã‚‚å¤šã„ã§ã™ã€‚
                    </p>

                    <pre><code class="language-python"># Independent Q-Learningå®Ÿè£…
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random

class QNetwork(nn.Module):
    """å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç‹¬ç«‹ã®Qé–¢æ•°"""
    def __init__(self, obs_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, 64), nn.ReLU(),
            nn.Linear(64, 64), nn.ReLU(),
            nn.Linear(64, action_dim)
        )

    def forward(self, obs):
        return self.net(obs)

class IQLAgent:
    """Independent Q-Learningã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self, obs_dim, action_dim, agent_id):
        self.agent_id = agent_id
        self.action_dim = action_dim
        self.q_net = QNetwork(obs_dim, action_dim)
        self.target_net = QNetwork(obs_dim, action_dim)
        self.target_net.load_state_dict(self.q_net.state_dict())
        self.optimizer = optim.Adam(self.q_net.parameters(), lr=1e-3)
        self.memory = deque(maxlen=10000)
        self.epsilon = 1.0

    def select_action(self, obs):
        """Îµ-greedyè¡Œå‹•é¸æŠ"""
        if random.random() < self.epsilon:
            return np.random.randint(self.action_dim)
        else:
            with torch.no_grad():
                q_values = self.q_net(torch.FloatTensor(obs))
                return q_values.argmax().item()

    def store_transition(self, obs, action, reward, next_obs, done):
        self.memory.append((obs, action, reward, next_obs, done))

    def train_step(self, batch_size=32):
        if len(self.memory) < batch_size:
            return 0.0

        batch = random.sample(self.memory, batch_size)
        obs, actions, rewards, next_obs, dones = zip(*batch)

        obs_t = torch.FloatTensor(obs)
        actions_t = torch.LongTensor(actions)
        rewards_t = torch.FloatTensor(rewards)
        next_obs_t = torch.FloatTensor(next_obs)
        dones_t = torch.FloatTensor(dones)

        # ç¾åœ¨ã®Qå€¤
        q_values = self.q_net(obs_t).gather(1, actions_t.unsqueeze(1)).squeeze()

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆQå€¤
        with torch.no_grad():
            next_q_values = self.target_net(next_obs_t).max(1)[0]
            target_q = rewards_t + 0.99 * next_q_values * (1 - dones_t)

        # æ›´æ–°
        loss = nn.MSELoss()(q_values, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # Îµæ¸›è¡°
        self.epsilon = max(0.01, self.epsilon * 0.995)

        return loss.item()

    def update_target(self):
        self.target_net.load_state_dict(self.q_net.state_dict())

# ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
class MultiCSTREnv:
    """3ã¤ã®CSTRãŒç›´åˆ—æ¥ç¶šã•ã‚ŒãŸç’°å¢ƒ"""
    def __init__(self):
        self.n_agents = 3
        self.reset()

    def reset(self):
        # å„åå¿œå™¨ã®çŠ¶æ…‹ [æ¸©åº¦, æ¿ƒåº¦]
        self.states = np.array([[350.0, 0.5], [340.0, 0.3], [330.0, 0.1]])
        return self.states

    def step(self, actions):
        # actions: [0=å†·å´, 1=åŠ ç†±, 2=ç¶­æŒ] for each agent
        rewards = []
        for i in range(self.n_agents):
            T, C = self.states[i]

            # è¡Œå‹•ã«ã‚ˆã‚‹æ¸©åº¦å¤‰åŒ–
            if actions[i] == 0:  # å†·å´
                T -= 5
            elif actions[i] == 1:  # åŠ ç†±
                T += 5

            # åå¿œé€²è¡Œï¼ˆç°¡æ˜“ãƒ¢ãƒ‡ãƒ«ï¼‰
            k = 0.1 * np.exp((T - 350) / 10)
            C = C * (1 - k * 0.1)

            # æ¬¡ã®åå¿œå™¨ã¸ã®æµå…¥
            if i < self.n_agents - 1:
                self.states[i+1, 1] += C * 0.3

            self.states[i] = [T, C]

            # å ±é…¬ï¼šç›®æ¨™æ¸©åº¦ã¨ã®å·®ã¨ç”Ÿç”£æ€§
            temp_penalty = -abs(T - 350)
            production = k * C
            rewards.append(temp_penalty * 0.1 + production * 10)

        done = False
        return self.states.copy(), np.array(rewards), [done]*self.n_agents

# å­¦ç¿’ãƒ«ãƒ¼ãƒ—
env = MultiCSTREnv()
agents = [IQLAgent(obs_dim=2, action_dim=3, agent_id=i) for i in range(3)]

for episode in range(500):
    obs = env.reset()
    episode_rewards = [0] * 3

    for step in range(100):
        actions = [agents[i].select_action(obs[i]) for i in range(3)]
        next_obs, rewards, dones = env.step(actions)

        # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç‹¬ç«‹ã«å­¦ç¿’
        for i in range(3):
            agents[i].store_transition(obs[i], actions[i], rewards[i],
                                       next_obs[i], dones[i])
            agents[i].train_step()
            episode_rewards[i] += rewards[i]

        obs = next_obs

    # å®šæœŸçš„ã«ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ›´æ–°
    if episode % 10 == 0:
        for agent in agents:
            agent.update_target()

    if episode % 50 == 0:
        print(f"Episode {episode}, Total Rewards: {sum(episode_rewards):.2f}")</code></pre>
                </div>
            </section>

            <section id="example3">
                <div class="example-box">
                    <div class="example-title">
                        <span class="example-number">3</span>
                        QMIXï¼šä¾¡å€¤åˆ†è§£ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆå‰²å½“
                    </div>
                    <p>
                        QMIXã¯ã€å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å€‹åˆ¥Qå€¤ã‚’æ··åˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§çµ±åˆã—ã€
                        å˜èª¿æ€§åˆ¶ç´„ï¼ˆIndividual-Global-Max: IGMï¼‰ã‚’ä¿ã¡ãªãŒã‚‰ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆå‰²å½“ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
                    </p>

                    <div class="formula-box">
                        <p><strong>QMIXä¾¡å€¤åˆ†è§£ï¼š</strong></p>
                        \[
                        Q_{tot}(\boldsymbol{\tau}, \mathbf{u}) = f_{mix}(Q_1(\tau^1, u^1), \ldots, Q_n(\tau^n, u^n); s)
                        \]
                        <p>å˜èª¿æ€§åˆ¶ç´„ï¼š</p>
                        \[
                        \frac{\partial Q_{tot}}{\partial Q_i} \geq 0, \quad \forall i
                        \]
                    </div>

                    <pre><code class="language-python"># QMIXå®Ÿè£…
import torch
import torch.nn as nn
import torch.optim as optim

class AgentQNetwork(nn.Module):
    """å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®Qé–¢æ•°"""
    def __init__(self, obs_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, 64), nn.ReLU(),
            nn.Linear(64, action_dim)
        )

    def forward(self, obs):
        return self.net(obs)

class QMixerNetwork(nn.Module):
    """å˜èª¿æ€§ã‚’ä¿è¨¼ã™ã‚‹æ··åˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""
    def __init__(self, n_agents, state_dim):
        super().__init__()
        self.n_agents = n_agents

        # ãƒã‚¤ãƒ‘ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆçŠ¶æ…‹ã‹ã‚‰é‡ã¿ã‚’ç”Ÿæˆï¼‰
        self.hyper_w1 = nn.Linear(state_dim, n_agents * 32)
        self.hyper_b1 = nn.Linear(state_dim, 32)
        self.hyper_w2 = nn.Linear(state_dim, 32)
        self.hyper_b2 = nn.Sequential(
            nn.Linear(state_dim, 32), nn.ReLU(),
            nn.Linear(32, 1)
        )

    def forward(self, agent_qs, state):
        """
        agent_qs: [batch, n_agents]
        state: [batch, state_dim]
        """
        batch_size = agent_qs.size(0)
        agent_qs = agent_qs.view(batch_size, 1, self.n_agents)

        # ç¬¬1å±¤ã®é‡ã¿ï¼ˆçµ¶å¯¾å€¤ã§å˜èª¿æ€§ä¿è¨¼ï¼‰
        w1 = torch.abs(self.hyper_w1(state))
        w1 = w1.view(batch_size, self.n_agents, 32)
        b1 = self.hyper_b1(state).view(batch_size, 1, 32)

        # ç¬¬1å±¤ã®å‡ºåŠ›
        hidden = torch.bmm(agent_qs, w1) + b1
        hidden = torch.relu(hidden)

        # ç¬¬2å±¤ã®é‡ã¿ï¼ˆçµ¶å¯¾å€¤ã§å˜èª¿æ€§ä¿è¨¼ï¼‰
        w2 = torch.abs(self.hyper_w2(state))
        w2 = w2.view(batch_size, 32, 1)
        b2 = self.hyper_b2(state).view(batch_size, 1, 1)

        # æœ€çµ‚å‡ºåŠ›
        q_tot = torch.bmm(hidden, w2) + b2
        return q_tot.view(batch_size)

class QMIX:
    """QMIXå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ """
    def __init__(self, n_agents, obs_dim, action_dim, state_dim):
        self.n_agents = n_agents
        self.agent_networks = [AgentQNetwork(obs_dim, action_dim)
                               for _ in range(n_agents)]
        self.mixer = QMixerNetwork(n_agents, state_dim)
        self.target_networks = [AgentQNetwork(obs_dim, action_dim)
                                for _ in range(n_agents)]
        self.target_mixer = QMixerNetwork(n_agents, state_dim)

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆæœŸåŒ–
        for i in range(n_agents):
            self.target_networks[i].load_state_dict(
                self.agent_networks[i].state_dict())
        self.target_mixer.load_state_dict(self.mixer.state_dict())

        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        params = list(self.mixer.parameters())
        for net in self.agent_networks:
            params += list(net.parameters())
        self.optimizer = optim.Adam(params, lr=5e-4)

    def select_actions(self, observations, epsilon=0.05):
        """å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•é¸æŠ"""
        actions = []
        for i, obs in enumerate(observations):
            if torch.rand(1).item() < epsilon:
                actions.append(torch.randint(0, 5, (1,)).item())
            else:
                with torch.no_grad():
                    q_vals = self.agent_networks[i](torch.FloatTensor(obs))
                    actions.append(q_vals.argmax().item())
        return actions

    def train_step(self, batch):
        """QMIXæ›´æ–°"""
        states, obs_list, actions, rewards, next_states, next_obs_list, dones = batch

        # ç¾åœ¨ã®Qå€¤ï¼ˆå„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼‰
        agent_qs = []
        for i in range(self.n_agents):
            q_vals = self.agent_networks[i](torch.FloatTensor(obs_list[i]))
            q = q_vals.gather(1, torch.LongTensor(actions[:, i]).unsqueeze(1))
            agent_qs.append(q)
        agent_qs = torch.cat(agent_qs, dim=1)

        # æ··åˆã—ã¦Q_tot
        q_tot = self.mixer(agent_qs, torch.FloatTensor(states))

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆQå€¤
        with torch.no_grad():
            target_agent_qs = []
            for i in range(self.n_agents):
                target_q = self.target_networks[i](
                    torch.FloatTensor(next_obs_list[i])).max(1)[0]
                target_agent_qs.append(target_q.unsqueeze(1))
            target_agent_qs = torch.cat(target_agent_qs, dim=1)
            target_q_tot = self.target_mixer(target_agent_qs,
                                             torch.FloatTensor(next_states))
            target = rewards + 0.99 * target_q_tot * (1 - dones)

        # æå¤±
        loss = nn.MSELoss()(q_tot, target)
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.mixer.parameters(), 10)
        self.optimizer.step()

        return loss.item()

# ä½¿ç”¨ä¾‹
qmix = QMIX(n_agents=3, obs_dim=4, action_dim=5, state_dim=12)
observations = [torch.randn(4) for _ in range(3)]
actions = qmix.select_actions(observations)
print(f"QMIX actions: {actions}")</code></pre>
                </div>
            </section>

            <section id="example4">
                <div class="example-box">
                    <div class="example-title">
                        <span class="example-number">4</span>
                        ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“é€šä¿¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«ï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ï¼‰
                    </div>
                    <p>
                        ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã§æƒ…å ±ã‚’äº¤æ›ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚ŠåŠ¹æœçš„ãªå”èª¿ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚
                        CommNetã‚„TarMACãªã©ã®æ‰‹æ³•ã§ã¯ã€æ³¨æ„æ©Ÿæ§‹ã‚’ç”¨ã„ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã‚’å®Ÿè£…ã—ã¾ã™ã€‚
                    </p>

                    <div class="mermaid">
                    graph LR
                        A1[Agent 1] -->|msgâ‚| C[Communication<br/>Channel]
                        A2[Agent 2] -->|msgâ‚‚| C
                        A3[Agent 3] -->|msgâ‚ƒ| C
                        C -->|aggregated| A1
                        C -->|aggregated| A2
                        C -->|aggregated| A3
                    </div>

                    <pre><code class="language-python"># ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°æ©Ÿæ§‹ã®å®Ÿè£…
import torch
import torch.nn as nn

class AttentionCommModule(nn.Module):
    """æ³¨æ„æ©Ÿæ§‹ãƒ™ãƒ¼ã‚¹ã®é€šä¿¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«"""
    def __init__(self, hidden_dim, n_agents):
        super().__init__()
        self.n_agents = n_agents
        self.hidden_dim = hidden_dim

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
        self.msg_encoder = nn.Linear(hidden_dim, hidden_dim)

        # æ³¨æ„æ©Ÿæ§‹
        self.query = nn.Linear(hidden_dim, hidden_dim)
        self.key = nn.Linear(hidden_dim, hidden_dim)
        self.value = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, hidden_states):
        """
        hidden_states: [n_agents, hidden_dim]
        returns: [n_agents, hidden_dim] (é€šä¿¡å¾Œã®éš ã‚ŒçŠ¶æ…‹)
        """
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
        messages = self.msg_encoder(hidden_states)  # [n_agents, hidden_dim]

        # æ³¨æ„ã‚¹ã‚³ã‚¢è¨ˆç®—
        Q = self.query(hidden_states)  # [n_agents, hidden_dim]
        K = self.key(messages)  # [n_agents, hidden_dim]
        V = self.value(messages)  # [n_agents, hidden_dim]

        # ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‰ãƒ‰ãƒƒãƒˆç©æ³¨æ„
        attn_scores = torch.matmul(Q, K.T) / (self.hidden_dim ** 0.5)
        attn_weights = torch.softmax(attn_scores, dim=-1)

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é›†ç´„
        aggregated = torch.matmul(attn_weights, V)

        return hidden_states + aggregated  # æ®‹å·®æ¥ç¶š

class CommunicativeAgent(nn.Module):
    """é€šä¿¡å¯èƒ½ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self, obs_dim, action_dim, hidden_dim, n_agents):
        super().__init__()
        self.obs_encoder = nn.Sequential(
            nn.Linear(obs_dim, hidden_dim), nn.ReLU()
        )
        self.comm_module = AttentionCommModule(hidden_dim, n_agents)
        self.policy = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )

    def forward(self, observations, agent_idx):
        """
        observations: [n_agents, obs_dim]
        agent_idx: ã“ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        """
        # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¦³æ¸¬ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        hidden_states = self.obs_encoder(observations)

        # é€šä¿¡ãƒ•ã‚§ãƒ¼ã‚º
        comm_hidden = self.comm_module(hidden_states)

        # è‡ªåˆ†ã®éš ã‚ŒçŠ¶æ…‹ã§è¡Œå‹•é¸æŠ
        my_hidden = comm_hidden[agent_idx]
        action_logits = self.policy(my_hidden)

        return action_logits, comm_hidden

# è¤‡æ•°ãƒ©ã‚¦ãƒ³ãƒ‰ã®é€šä¿¡
class MultiRoundCommAgent(nn.Module):
    """è¤‡æ•°ãƒ©ã‚¦ãƒ³ãƒ‰ã®é€šä¿¡ã‚’è¡Œã†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self, obs_dim, action_dim, hidden_dim, n_agents, n_comm_rounds=2):
        super().__init__()
        self.n_comm_rounds = n_comm_rounds

        self.obs_encoder = nn.Linear(obs_dim, hidden_dim)
        self.comm_modules = nn.ModuleList([
            AttentionCommModule(hidden_dim, n_agents)
            for _ in range(n_comm_rounds)
        ])
        self.policy = nn.Linear(hidden_dim, action_dim)

    def forward(self, observations):
        """
        observations: [n_agents, obs_dim]
        returns: [n_agents, action_dim]
        """
        hidden = torch.relu(self.obs_encoder(observations))

        # è¤‡æ•°ãƒ©ã‚¦ãƒ³ãƒ‰ã®é€šä¿¡
        for comm_module in self.comm_modules:
            hidden = comm_module(hidden)

        # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè¡Œå‹•é¸æŠ
        actions = self.policy(hidden)
        return actions

# ä½¿ç”¨ä¾‹ï¼š3åå¿œå™¨ã®å”èª¿åˆ¶å¾¡
n_agents = 3
obs_dim = 4  # [æ¸©åº¦, æ¿ƒåº¦, æµé‡, åœ§åŠ›]
action_dim = 5  # é›¢æ•£è¡Œå‹•
hidden_dim = 64

agent = MultiRoundCommAgent(obs_dim, action_dim, hidden_dim, n_agents, n_comm_rounds=2)

# ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
observations = torch.randn(n_agents, obs_dim)
actions = agent(observations)
print(f"é€šä¿¡å¾Œã®è¡Œå‹•: {actions.shape}")  # [3, 5]

# å€‹åˆ¥ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã®ä½¿ç”¨
single_agent = CommunicativeAgent(obs_dim, action_dim, hidden_dim, n_agents)
action_logits, comm_hidden = single_agent(observations, agent_idx=0)
print(f"Agent 0 ã®è¡Œå‹•: {torch.softmax(action_logits, dim=-1)}")
print(f"é€šä¿¡å¾Œã®éš ã‚ŒçŠ¶æ…‹: {comm_hidden.shape}")  # [3, 64]</code></pre>
                </div>
            </section>

            <section id="example5">
                <div class="example-box">
                    <div class="example-title">
                        <span class="example-number">5</span>
                        å”èª¿ã‚¿ã‚¹ã‚¯ï¼š3ã¤ã®CSTRã®åŒæœŸåˆ¶å¾¡
                    </div>
                    <p>
                        3ã¤ã®é€£ç¶šæ”ªæ‹Œæ§½åå¿œå™¨ï¼ˆCSTRï¼‰ã‚’ç›´åˆ—ã«æ¥ç¶šã—ã€å…¨ä½“ã®ç”Ÿç”£æ€§ã‚’æœ€å¤§åŒ–ã—ãªãŒã‚‰
                        å„åå¿œå™¨ã®æ¸©åº¦ã‚’é©åˆ‡ã«åˆ¶å¾¡ã™ã‚‹å”èª¿ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè£…ã—ã¾ã™ã€‚
                    </p>

                    <pre><code class="language-python"># å”èª¿ã‚¿ã‚¹ã‚¯ï¼š3ã¤ã®CSTRã®åŒæœŸåˆ¶å¾¡
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

class CooperativeCSTREnv:
    """3ã¤ã®CSTRãŒç›´åˆ—æ¥ç¶šã•ã‚ŒãŸå”èª¿ç’°å¢ƒ"""
    def __init__(self):
        self.n_agents = 3
        self.dt = 0.1  # æ™‚é–“åˆ»ã¿ [min]
        self.reset()

    def reset(self):
        # å„CSTR: [æ¸©åº¦T(K), æ¿ƒåº¦CA(mol/L), æµé‡F(L/min)]
        self.states = np.array([
            [350.0, 2.0, 100.0],  # CSTR1
            [340.0, 1.5, 100.0],  # CSTR2
            [330.0, 1.0, 100.0]   # CSTR3
        ])
        self.time = 0
        return self._get_observations()

    def _get_observations(self):
        """å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¦³æ¸¬ï¼ˆå±€æ‰€æƒ…å ±+éš£æ¥æƒ…å ±ï¼‰"""
        obs = []
        for i in range(self.n_agents):
            local = self.states[i].copy()
            # å‰æ®µã®æƒ…å ±ï¼ˆå”èª¿ã®ãŸã‚ï¼‰
            prev = self.states[i-1] if i > 0 else np.zeros(3)
            # å¾Œæ®µã®æƒ…å ±
            next_ = self.states[i+1] if i < self.n_agents-1 else np.zeros(3)
            obs.append(np.concatenate([local, prev, next_]))
        return obs

    def step(self, actions):
        """
        actions: [n_agents, 2] = [[Q1, Tin1], [Q2, Tin2], [Q3, Tin3]]
        Q: å†·å´æµé‡ [L/min] (0-50)
        Tin: å…¥å£æ¸©åº¦ [K] (300-400)
        """
        rewards = []

        for i in range(self.n_agents):
            T, CA, F = self.states[i]
            Q = actions[i][0] * 50  # æ­£è¦åŒ–è§£é™¤
            Tin = actions[i][1] * 100 + 300

            # åå¿œé€Ÿåº¦å®šæ•°ï¼ˆArrheniuså¼ï¼‰
            Ea = 50000  # [J/mol]
            R = 8.314   # [J/(molÂ·K)]
            k = 1e10 * np.exp(-Ea / (R * T))

            # CSTRãƒ¢ãƒ‡ãƒ«
            V = 1000  # åå¿œå™¨ä½“ç© [L]
            rho = 1000  # å¯†åº¦ [g/L]
            Cp = 4.18   # æ¯”ç†± [J/(gÂ·K)]
            dHr = -50000  # åå¿œç†± [J/mol]

            # å…¥å£æ¿ƒåº¦ï¼ˆå‰æ®µã‹ã‚‰ã®æµå…¥ï¼‰
            CA_in = self.states[i-1, 1] if i > 0 else 2.5

            # ç‰©è³ªåæ”¯
            dCA = (F / V) * (CA_in - CA) - k * CA
            CA_new = CA + dCA * self.dt

            # ã‚¨ãƒãƒ«ã‚®ãƒ¼åæ”¯
            Q_reaction = -dHr * k * CA * V
            Q_cooling = Q * rho * Cp * (T - Tin)
            dT = (Q_reaction - Q_cooling) / (V * rho * Cp)
            T_new = T + dT * self.dt

            self.states[i] = [T_new, max(0, CA_new), F]

            # å”èª¿å ±é…¬ï¼šå…¨ä½“ã®ç”Ÿç”£æ€§ã¨å®‰å®šæ€§
            production = k * CA  # åå¿œé€Ÿåº¦
            temp_penalty = -abs(T_new - 350) ** 2  # ç›®æ¨™æ¸©åº¦ã‹ã‚‰ã®åå·®
            flow_continuity = -abs(F - 100) ** 2 if i > 0 else 0

            reward = production * 100 + temp_penalty * 0.1 + flow_continuity * 0.01
            rewards.append(reward)

        # å…±é€šå ±é…¬ï¼ˆå”èª¿ã‚¿ã‚¹ã‚¯ï¼‰
        total_production = sum([self.states[i, 1] for i in range(self.n_agents)])
        common_reward = total_production * 10
        rewards = [r + common_reward for r in rewards]

        self.time += self.dt
        done = self.time >= 10  # 10åˆ†é–“ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰

        return self._get_observations(), np.array(rewards), [done]*self.n_agents

# QMIXå­¦ç¿’ï¼ˆç°¡ç•¥ç‰ˆï¼‰
class SimpleQMIX:
    def __init__(self, n_agents, obs_dim, action_dim):
        self.n_agents = n_agents
        self.q_nets = [nn.Sequential(
            nn.Linear(obs_dim, 64), nn.ReLU(),
            nn.Linear(64, action_dim)
        ) for _ in range(n_agents)]
        self.mixer = nn.Sequential(
            nn.Linear(n_agents, 32), nn.ReLU(),
            nn.Linear(32, 1)
        )
        params = list(self.mixer.parameters())
        for net in self.q_nets:
            params += list(net.parameters())
        self.optimizer = optim.Adam(params, lr=1e-3)

    def select_actions(self, observations):
        actions = []
        for i, obs in enumerate(observations):
            with torch.no_grad():
                q_vals = self.q_nets[i](torch.FloatTensor(obs))
                # é€£ç¶šè¡Œå‹•ã‚’é›¢æ•£åŒ–ï¼ˆç°¡ç•¥åŒ–ï¼‰
                action = torch.rand(2)  # [Qæ­£è¦åŒ–, Tinæ­£è¦åŒ–]
                actions.append(action.numpy())
        return np.array(actions)

# å­¦ç¿’å®Ÿè¡Œ
env = CooperativeCSTREnv()
agent = SimpleQMIX(n_agents=3, obs_dim=9, action_dim=10)

for episode in range(100):
    obs = env.reset()
    episode_reward = 0

    for step in range(100):
        actions = agent.select_actions(obs)
        next_obs, rewards, dones = env.step(actions)
        episode_reward += sum(rewards)
        obs = next_obs

        if dones[0]:
            break

    if episode % 10 == 0:
        print(f"Episode {episode}, Total Reward: {episode_reward:.2f}")
        print(f"  Final Temps: {[f'{s[0]:.1f}K' for s in env.states]}")
        print(f"  Final Concs: {[f'{s[1]:.3f}mol/L' for s in env.states]}")</code></pre>
                </div>
            </section>

            <section id="example6">
                <div class="example-box">
                    <div class="example-title">
                        <span class="example-number">6</span>
                        ç«¶äº‰ã‚¿ã‚¹ã‚¯ï¼šé™ã‚‰ã‚ŒãŸãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã®é…åˆ†
                    </div>
                    <p>
                        è¤‡æ•°ã®ãƒ—ãƒ©ãƒ³ãƒˆãŒé™ã‚‰ã‚ŒãŸè’¸æ°—ã‚„å†·å´æ°´ã¨ã„ã£ãŸãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’å–ã‚Šåˆã†ç«¶äº‰ã‚·ãƒŠãƒªã‚ªã§ã™ã€‚
                        å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯è‡ªèº«ã®ç”Ÿç”£ã‚’æœ€å¤§åŒ–ã—ã¤ã¤ã€è³‡æºåˆ¶ç´„ã®ä¸‹ã§èª¿æ•´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
                    </p>

                    <pre><code class="language-python"># ç«¶äº‰ã‚¿ã‚¹ã‚¯ï¼šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é…åˆ†å•é¡Œ
import numpy as np
import torch
import torch.nn as nn

class CompetitiveUtilityEnv:
    """é™ã‚‰ã‚ŒãŸãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’è¤‡æ•°ãƒ—ãƒ©ãƒ³ãƒˆã§ç«¶äº‰"""
    def __init__(self, n_plants=3):
        self.n_plants = n_plants
        self.total_steam = 500.0  # ç·è’¸æ°—ä¾›çµ¦é‡ [kg/h]
        self.total_cooling = 1000.0  # ç·å†·å´æ°´ [L/min]
        self.reset()

    def reset(self):
        # å„ãƒ—ãƒ©ãƒ³ãƒˆ: [ç”Ÿç”£é€Ÿåº¦, æ¸©åº¦, è’¸æ°—ä½¿ç”¨é‡, å†·å´æ°´ä½¿ç”¨é‡]
        self.states = np.array([
            [50.0, 350.0, 150.0, 300.0] for _ in range(self.n_plants)
        ])
        return self._get_observations()

    def _get_observations(self):
        """å„ãƒ—ãƒ©ãƒ³ãƒˆã®è¦³æ¸¬"""
        obs = []
        for i in range(self.n_plants):
            # è‡ªèº«ã®çŠ¶æ…‹ + è³‡æºã®æ®‹é‡æƒ…å ±
            steam_used = sum(self.states[:, 2])
            cooling_used = sum(self.states[:, 3])
            steam_avail = max(0, self.total_steam - steam_used)
            cooling_avail = max(0, self.total_cooling - cooling_used)

            obs_i = np.concatenate([
                self.states[i],
                [steam_avail, cooling_avail]
            ])
            obs.append(obs_i)
        return obs

    def step(self, actions):
        """
        actions: [n_plants, 2] = [[steam_request, cooling_request], ...]
        å„å€¤ã¯0-1ã§æ­£è¦åŒ–
        """
        # è¦æ±‚é‡ã‚’å®Ÿæ•°å€¤ã«å¤‰æ›
        steam_requests = actions[:, 0] * 200  # 0-200 kg/h
        cooling_requests = actions[:, 1] * 400  # 0-400 L/min

        # è³‡æºé…åˆ†ï¼ˆæ¯”ä¾‹é…åˆ†ï¼‰
        total_steam_req = sum(steam_requests)
        total_cooling_req = sum(cooling_requests)

        if total_steam_req > self.total_steam:
            steam_allocated = steam_requests * (self.total_steam / total_steam_req)
        else:
            steam_allocated = steam_requests

        if total_cooling_req > self.total_cooling:
            cooling_allocated = cooling_requests * (self.total_cooling / total_cooling_req)
        else:
            cooling_allocated = cooling_requests

        rewards = []
        for i in range(self.n_plants):
            # ç”Ÿç”£é€Ÿåº¦ã¯è³‡æºã«ä¾å­˜
            steam_factor = steam_allocated[i] / 200
            cooling_factor = cooling_allocated[i] / 400
            production = 100 * steam_factor * cooling_factor

            # æ¸©åº¦ç®¡ç†ï¼ˆå†·å´ä¸è¶³ã§ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰
            temp_change = (steam_allocated[i] * 0.5 - cooling_allocated[i] * 0.3)
            temp_new = self.states[i, 1] + temp_change
            temp_penalty = -abs(temp_new - 350) if temp_new > 380 else 0

            self.states[i] = [production, temp_new,
                            steam_allocated[i], cooling_allocated[i]]

            # å ±é…¬ï¼šç”Ÿç”£æ€§ - ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³ãƒšãƒŠãƒ«ãƒ†ã‚£
            shortage_penalty = 0
            if steam_allocated[i] < steam_requests[i]:
                shortage_penalty += (steam_requests[i] - steam_allocated[i]) * 0.5
            if cooling_allocated[i] < cooling_requests[i]:
                shortage_penalty += (cooling_requests[i] - cooling_allocated[i]) * 0.3

            reward = production - shortage_penalty + temp_penalty
            rewards.append(reward)

        done = False
        return self._get_observations(), np.array(rewards), [done]*self.n_plants

# Nash Q-Learningï¼ˆç«¶äº‰ã‚¿ã‚¹ã‚¯ç”¨ï¼‰
class NashQLearningAgent:
    """Nashå‡è¡¡ã‚’å­¦ç¿’ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self, obs_dim, action_dim, agent_id):
        self.agent_id = agent_id
        self.q_net = nn.Sequential(
            nn.Linear(obs_dim, 64), nn.ReLU(),
            nn.Linear(64, 64), nn.ReLU(),
            nn.Linear(64, action_dim)
        )
        self.optimizer = optim.Adam(self.q_net.parameters(), lr=1e-3)
        self.epsilon = 0.3

    def select_action(self, obs):
        """Îµ-greedy with Nash equilibrium consideration"""
        if np.random.rand() < self.epsilon:
            return np.random.rand(2)
        else:
            with torch.no_grad():
                # é€£ç¶šè¡Œå‹•ã®è¿‘ä¼¼ï¼ˆç°¡ç•¥åŒ–ï¼‰
                return torch.sigmoid(torch.randn(2)).numpy()

    def update_policy(self, obs, action, reward, next_obs):
        """Qå­¦ç¿’æ›´æ–°ï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
        # å®Ÿè£…ã¯çœç•¥ï¼ˆç«¶äº‰ç’°å¢ƒã§ã®å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰
        pass

# ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
env = CompetitiveUtilityEnv(n_plants=3)
agents = [NashQLearningAgent(obs_dim=6, action_dim=2, agent_id=i)
          for i in range(3)]

for episode in range(50):
    obs = env.reset()
    episode_rewards = [0] * 3

    for step in range(50):
        actions = np.array([agents[i].select_action(obs[i]) for i in range(3)])
        next_obs, rewards, dones = env.step(actions)

        for i in range(3):
            episode_rewards[i] += rewards[i]

        obs = next_obs

    if episode % 10 == 0:
        print(f"\nEpisode {episode}")
        print(f"  Individual Rewards: {[f'{r:.1f}' for r in episode_rewards]}")
        print(f"  Productions: {[f'{s[0]:.1f}' for s in env.states]}")
        print(f"  Steam Usage: {[f'{s[2]:.1f}' for s in env.states]} / {env.total_steam}")
        print(f"  Cooling Usage: {[f'{s[3]:.1f}' for s in env.states]} / {env.total_cooling}")</code></pre>
                </div>
            </section>

            <section id="example7">
                <div class="example-box">
                    <div class="example-title">
                        <span class="example-number">7</span>
                        æ··åˆã‚¿ã‚¹ã‚¯ï¼šå”èª¿ã¨ç«¶äº‰ãŒå…±å­˜ã™ã‚‹ç”Ÿç”£ã‚·ã‚¹ãƒ†ãƒ 
                    </div>
                    <p>
                        ç¾å®Ÿã®ãƒ—ãƒ©ãƒ³ãƒˆã§ã¯ã€å”èª¿ã¨ç«¶äº‰ã®ä¸¡é¢ãŒå­˜åœ¨ã—ã¾ã™ã€‚
                        å„ãƒ—ãƒ©ãƒ³ãƒˆã¯è‡ªèº«ã®ç”Ÿç”£ç›®æ¨™ã‚’é”æˆã—ã¤ã¤ï¼ˆç«¶äº‰ï¼‰ã€å…¨ä½“ã®å®‰å®šæ€§ã‚„åŠ¹ç‡ã‚‚è€ƒæ…®ã™ã‚‹ï¼ˆå”èª¿ï¼‰å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
                    </p>

                    <pre><code class="language-python"># æ··åˆã‚¿ã‚¹ã‚¯ï¼šå”èª¿ã¨ç«¶äº‰ãŒå…±å­˜ã™ã‚‹ç”Ÿç”£ã‚·ã‚¹ãƒ†ãƒ 
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

class MixedCoopCompEnv:
    """å”èª¿ã¨ç«¶äº‰ãŒæ··åœ¨ã™ã‚‹è¤‡é›‘ãªç’°å¢ƒ"""
    def __init__(self, n_plants=3):
        self.n_plants = n_plants
        self.total_energy = 1000.0  # ç·ã‚¨ãƒãƒ«ã‚®ãƒ¼ [kW]
        self.production_targets = [100, 120, 80]  # å„ãƒ—ãƒ©ãƒ³ãƒˆã®ç›®æ¨™ç”Ÿç”£é‡
        self.reset()

    def reset(self):
        # å„ãƒ—ãƒ©ãƒ³ãƒˆ: [ç”Ÿç”£é‡, æ¸©åº¦, ã‚¨ãƒãƒ«ã‚®ãƒ¼ä½¿ç”¨, å“è³ª]
        self.states = np.array([
            [50.0, 350.0, 300.0, 0.9] for _ in range(self.n_plants)
        ])
        self.time = 0
        return self._get_observations()

    def _get_observations(self):
        obs = []
        for i in range(self.n_plants):
            # è‡ªèº«ã®çŠ¶æ…‹ + ç›®æ¨™ + ä»–ãƒ—ãƒ©ãƒ³ãƒˆã®ç”Ÿç”£é‡ï¼ˆå”èª¿ã®ãŸã‚ï¼‰
            others_production = [self.states[j, 0] for j in range(self.n_plants) if j != i]
            total_energy_used = sum(self.states[:, 2])

            obs_i = np.concatenate([
                self.states[i],
                [self.production_targets[i]],
                others_production,
                [total_energy_used, self.total_energy]
            ])
            obs.append(obs_i)
        return obs

    def step(self, actions):
        """
        actions: [n_plants, 3] = [[energy_req, temp_setpoint, quality_target], ...]
        """
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼é…åˆ†ï¼ˆç«¶äº‰è¦ç´ ï¼‰
        energy_requests = actions[:, 0] * 500
        total_energy_req = sum(energy_requests)

        if total_energy_req > self.total_energy:
            # ä¸è¶³æ™‚ã¯å„ªå…ˆåº¦ãƒ™ãƒ¼ã‚¹ã§é…åˆ†ï¼ˆç›®æ¨™é”æˆåº¦ãŒä½ã„ãƒ—ãƒ©ãƒ³ãƒˆã‚’å„ªå…ˆï¼‰
            priorities = [
                max(0, self.production_targets[i] - self.states[i, 0])
                for i in range(self.n_plants)
            ]
            total_priority = sum(priorities) + 1e-6
            energy_allocated = [
                self.total_energy * (priorities[i] / total_priority)
                for i in range(self.n_plants)
            ]
        else:
            energy_allocated = energy_requests

        rewards = []
        for i in range(self.n_plants):
            temp_setpoint = actions[i, 1] * 100 + 300  # 300-400K
            quality_target = actions[i, 2]  # 0-1

            # ç”Ÿç”£ãƒ¢ãƒ‡ãƒ«
            energy_factor = energy_allocated[i] / 500
            temp_factor = 1.0 - abs(temp_setpoint - 350) / 100
            production = self.production_targets[i] * energy_factor * temp_factor

            # å“è³ªãƒ¢ãƒ‡ãƒ«
            quality = 0.5 + 0.5 * quality_target * temp_factor

            # æ¸©åº¦æ›´æ–°
            temp = self.states[i, 1] + (temp_setpoint - self.states[i, 1]) * 0.3

            self.states[i] = [production, temp, energy_allocated[i], quality]

            # å ±é…¬è¨­è¨ˆï¼ˆæ··åˆï¼‰
            # 1. å€‹åˆ¥ç›®æ¨™é”æˆï¼ˆç«¶äº‰è¦ç´ ï¼‰
            target_achievement = -abs(production - self.production_targets[i])

            # 2. å“è³ªãƒšãƒŠãƒ«ãƒ†ã‚£
            quality_reward = quality * 10

            # 3. ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡ï¼ˆå”èª¿è¦ç´ ï¼‰
            energy_efficiency = production / (energy_allocated[i] + 1)

            reward = target_achievement + quality_reward + energy_efficiency * 5
            rewards.append(reward)

        # å”èª¿ãƒœãƒ¼ãƒŠã‚¹ï¼šå…¨ä½“ã®å®‰å®šæ€§
        total_production = sum(self.states[:, 0])
        stability = -np.std(self.states[:, 1])  # æ¸©åº¦ã®æ¨™æº–åå·®
        cooperation_bonus = (total_production / sum(self.production_targets)) * 50 + stability

        # æœ€çµ‚å ±é…¬ = å€‹åˆ¥å ±é…¬ + å”èª¿ãƒœãƒ¼ãƒŠã‚¹
        rewards = [r + cooperation_bonus * 0.3 for r in rewards]

        self.time += 1
        done = self.time >= 100

        return self._get_observations(), np.array(rewards), [done]*self.n_plants

# COMA (Counterfactual Multi-Agent Policy Gradient)
class COMAAgent:
    """æ··åˆã‚¿ã‚¹ã‚¯ç”¨ã®COMAã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self, n_agents, obs_dim, action_dim, state_dim):
        self.n_agents = n_agents

        # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®Actor
        self.actors = [nn.Sequential(
            nn.Linear(obs_dim, 64), nn.ReLU(),
            nn.Linear(64, action_dim), nn.Tanh()
        ) for _ in range(n_agents)]

        # ä¸­å¤®Criticï¼ˆåäº‹å®Ÿãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
        self.critic = nn.Sequential(
            nn.Linear(state_dim + n_agents * action_dim, 128), nn.ReLU(),
            nn.Linear(128, n_agents)  # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®Qå€¤
        )

        self.actor_opts = [optim.Adam(a.parameters(), lr=3e-4) for a in self.actors]
        self.critic_opt = optim.Adam(self.critic.parameters(), lr=1e-3)

    def select_actions(self, observations):
        actions = []
        for i, obs in enumerate(observations):
            with torch.no_grad():
                action = self.actors[i](torch.FloatTensor(obs)).numpy()
            actions.append(action)
        return np.array(actions)

# å­¦ç¿’ãƒ«ãƒ¼ãƒ—
env = MixedCoopCompEnv(n_plants=3)
agent = COMAAgent(n_agents=3, obs_dim=9, action_dim=3, state_dim=12)

for episode in range(100):
    obs = env.reset()
    episode_rewards = [0] * 3

    for step in range(100):
        actions = agent.select_actions(obs)
        next_obs, rewards, dones = env.step(actions)

        for i in range(3):
            episode_rewards[i] += rewards[i]

        obs = next_obs

        if dones[0]:
            break

    if episode % 20 == 0:
        print(f"\nEpisode {episode}")
        print(f"  Individual Rewards: {[f'{r:.1f}' for r in episode_rewards]}")
        print(f"  Productions: {[f'{s[0]:.1f}' for s in env.states]}")
        print(f"  Targets: {env.production_targets}")
        print(f"  Qualities: {[f'{s[3]:.2f}' for s in env.states]}")
        print(f"  Total Energy Used: {sum(env.states[:, 2]):.1f} / {env.total_energy}")</code></pre>
                </div>
            </section>

            <section id="summary">
                <h2>Chapter 4 ã¾ã¨ã‚</h2>

                <div class="key-points">
                    <h3>å­¦ã‚“ã ã“ã¨</h3>
                    <ul>
                        <li><strong>CTDE</strong>ï¼šå­¦ç¿’æ™‚ã¯ä¸­å¤®é›†æ¨©ã€å®Ÿè¡Œæ™‚ã¯åˆ†æ•£ã®åŠ¹ç‡çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯</li>
                        <li><strong>Independent Q-Learning</strong>ï¼šæœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã ãŒéå®šå¸¸æ€§ã®å•é¡Œã‚ã‚Š</li>
                        <li><strong>QMIX</strong>ï¼šä¾¡å€¤åˆ†è§£ã¨å˜èª¿æ€§åˆ¶ç´„ã«ã‚ˆã‚‹ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆå‰²å½“</li>
                        <li><strong>é€šä¿¡æ©Ÿæ§‹</strong>ï¼šæ³¨æ„æ©Ÿæ§‹ãƒ™ãƒ¼ã‚¹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã§å”èª¿å¼·åŒ–</li>
                        <li><strong>å”èª¿ã‚¿ã‚¹ã‚¯</strong>ï¼šå…±é€šå ±é…¬ã§å…¨ä½“æœ€é©ã‚’ç›®æŒ‡ã™</li>
                        <li><strong>ç«¶äº‰ã‚¿ã‚¹ã‚¯</strong>ï¼šé™ã‚‰ã‚ŒãŸè³‡æºã®é…åˆ†å•é¡Œ</li>
                        <li><strong>æ··åˆã‚¿ã‚¹ã‚¯</strong>ï¼šç¾å®Ÿã®ãƒ—ãƒ©ãƒ³ãƒˆé‹è»¢ã«è¿‘ã„è¤‡é›‘ãªã‚·ãƒŠãƒªã‚ª</li>
                    </ul>
                </div>

                <h3>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ¯”è¼ƒ</h3>
                <table style="width: 100%; border-collapse: collapse; margin: 1.5rem 0;">
                    <thead style="background: var(--pi-primary); color: white;">
                        <tr>
                            <th style="padding: 0.75rem; border: 1px solid #ddd;">æ‰‹æ³•</th>
                            <th style="padding: 0.75rem; border: 1px solid #ddd;">é©ç”¨ã‚¿ã‚¹ã‚¯</th>
                            <th style="padding: 0.75rem; border: 1px solid #ddd;">å­¦ç¿’åŠ¹ç‡</th>
                            <th style="padding: 0.75rem; border: 1px solid #ddd;">å®Ÿè£…é›£æ˜“åº¦</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">IQL</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">å˜ç´”ãªå”èª¿</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">ä½</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">ä½</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">QMIX</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">å”èª¿ã‚¿ã‚¹ã‚¯</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">é«˜</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">ä¸­</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">CTDE</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">æ··åˆã‚¿ã‚¹ã‚¯</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">é«˜</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">ä¸­</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">CommNet</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">è¤‡é›‘ãªå”èª¿</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">é«˜</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">é«˜</td>
                        </tr>
                    </tbody>
                </table>

                <h3>ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã¸ã®ç¤ºå”†</h3>
                <ul>
                    <li>è¤‡æ•°ã®åå¿œå™¨ã‚„è’¸ç•™å¡”ã®å”èª¿åˆ¶å¾¡ã«ã¯QMIXã‚„CTDEãŒæœ‰åŠ¹</li>
                    <li>ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é…åˆ†ã®ã‚ˆã†ãªç«¶äº‰ã‚¿ã‚¹ã‚¯ã«ã¯Nash Q-Learningã‚’æ¤œè¨</li>
                    <li>é€šä¿¡é…å»¶ãŒã‚ã‚‹å ´åˆã¯ã€è¦³æ¸¬ã«ãƒãƒƒãƒ•ã‚¡æƒ…å ±ã‚’å«ã‚ã‚‹</li>
                    <li>å®Ÿãƒ—ãƒ©ãƒ³ãƒˆã§ã¯æ··åˆã‚¿ã‚¹ã‚¯ãŒä¸€èˆ¬çš„ã§ã€å ±é…¬è¨­è¨ˆãŒé‡è¦</li>
                </ul>
            </section>

            <div style="margin-top: 3rem; padding: 2rem; background: linear-gradient(135deg, rgba(17, 153, 142, 0.1) 0%, rgba(56, 239, 125, 0.1) 100%); border-radius: 8px;">
                <h3 style="color: var(--pi-primary); margin-top: 0;">æ¬¡ã®Chapter</h3>
                <p><strong><a href="chapter-5.html" style="color: var(--pi-primary);">Chapter 5: å®Ÿãƒ—ãƒ©ãƒ³ãƒˆã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã¨å®‰å…¨æ€§</a></strong></p>
                <p>å®Ÿéš›ã®ãƒ—ãƒ©ãƒ³ãƒˆã«å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹éš›ã®å®‰å…¨æ€§ç¢ºä¿ã€sim-to-realè»¢ç§»ã€ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–ã€äººé–“ã«ã‚ˆã‚‹ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰æ©Ÿæ§‹ãªã©ã‚’å­¦ã³ã¾ã™ã€‚</p>
            </div>
        </article>
    <section class="disclaimer">
<h3>å…è²¬äº‹é …</h3>
<ul>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹Code examplesã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
<li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
</ul>
</section>

</main>

    <footer>
        <div class="footer-content">
            <p>&copy; 2025 Hashimoto Laboratory, Tohoku University. All rights reserved.</p>
        </div>
    </footer>

    <!-- Prism.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>

    <!-- Mermaid -->
    <script>
        mermaid.initialize({
            startOnLoad: true,
            theme: 'default',
            themeVariables: {
                primaryColor: '#11998e',
                primaryTextColor: '#fff',
                primaryBorderColor: '#0e7c74',
                lineColor: '#11998e',
                secondaryColor: '#38ef7d',
                tertiaryColor: '#f0f0f0'
            }
        });
    </script>
</body>
</html>
