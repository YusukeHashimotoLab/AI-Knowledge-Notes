<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 5: å®Ÿãƒ—ãƒ©ãƒ³ãƒˆã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã¨å®‰å…¨æ€§ - AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹è‡ªå¾‹ãƒ—ãƒ­ã‚»ã‚¹é‹è»¢</title>
    <meta name="description" content="å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿãƒ—ãƒ©ãƒ³ãƒˆã«ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹éš›ã®å®‰å…¨æ€§ç¢ºä¿æ‰‹æ³•ã‚’å­¦ã³ã¾ã™ã€‚Sim-to-realè»¢ç§»ã€å®‰å…¨ãªæ¢ç´¢ã€ä¿å®ˆçš„Qå­¦ç¿’ã€äººé–“ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã€ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–ã‚’å®Ÿè£…ä¾‹ã¨ã¨ã‚‚ã«è§£èª¬ã€‚">

    <!-- CSS -->
    <link rel="stylesheet" href="../../../../../assets/css/variables.css">
    <link rel="stylesheet" href="../../../../../assets/css/reset.css">
    <link rel="stylesheet" href="../../../../../assets/css/base.css">
    <link rel="stylesheet" href="../../../../../assets/css/components.css">
    <link rel="stylesheet" href="../../../../../assets/css/layout.css">
    <link rel="stylesheet" href="../../../../../assets/css/responsive.css">
    <link rel="stylesheet" href="../../../../../assets/css/article.css">

    <!-- Prism.js for syntax highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">

    <!-- MathJax for mathematical expressions -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>

        <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.8; color: #333; background: #f5f5f5;
        }
        header {
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white; padding: 2rem 1rem; text-align: center;
        }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; }
        .subtitle { opacity: 0.9; font-size: 1.1rem; }
        .container { max-width: 1200px; margin: 2rem auto; padding: 0 1rem; }
        .back-link {
            display: inline-block; margin-bottom: 2rem; padding: 0.5rem 1rem;
            background: white; color: #11998e; text-decoration: none;
            border-radius: 6px; font-weight: 600;
        }
        .content-box {
            background: white; padding: 2rem; border-radius: 12px;
            margin-bottom: 2rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        h2 {
            color: #11998e; margin: 2rem 0 1rem 0;
            padding-bottom: 0.5rem; border-bottom: 3px solid #11998e;
        }
        h3 { color: #2c3e50; margin: 1.5rem 0 1rem 0; }
        h4 { color: #2c3e50; margin: 1rem 0 0.5rem 0; }
        p { margin-bottom: 1rem; }
        ul, ol { margin-left: 2rem; margin-bottom: 1rem; }
        li { margin-bottom: 0.5rem; }
        pre {
            background: #1e1e1e; color: #d4d4d4; padding: 1.5rem;
            border-radius: 8px; overflow-x: auto; margin: 1rem 0;
            border-left: 4px solid #11998e;
        }
        code {
            font-family: 'Courier New', monospace; font-size: 0.9rem;
        }
        .key-point {
            background: #e8f5e9; padding: 1rem; border-radius: 6px;
            border-left: 4px solid #4caf50; margin: 1rem 0;
        }
        .tech-note {
            background: #e3f2fd; padding: 1rem; border-radius: 6px;
            border-left: 4px solid #2196f3; margin: 1rem 0;
        }
        .formula {
            background: #f0f7ff; padding: 1rem; border-radius: 6px;
            margin: 1rem 0; overflow-x: auto;
        }
        table {
            width: 100%; border-collapse: collapse; margin: 1rem 0;
        }
        th, td {
            border: 1px solid #ddd; padding: 0.75rem; text-align: left;
        }
        th {
            background: #11998e; color: white; font-weight: 600;
        }
        tr:nth-child(even) { background: #f9f9f9; }
        .nav-buttons {
            display: flex; justify-content: space-between; margin-top: 3rem;
        }
        .nav-buttons a {
            padding: 0.75rem 1.5rem;
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white; text-decoration: none; border-radius: 6px;
            font-weight: 600;
        }
        footer {
            background: #2c3e50; color: white; text-align: center;
            padding: 2rem 1rem; margin-top: 4rem;
        }
        @media (max-width: 768px) {
            h1 { font-size: 1.6rem; }
            .container { padding: 0 0.5rem; }
            pre { padding: 1rem; }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/PI/ai-agent-process/chapter-5.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../PI/index.html">ãƒ—ãƒ­ã‚»ã‚¹ãƒ»ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹</a><span class="breadcrumb-separator">â€º</span><a href="../../PI/ai-agent-process/index.html">Ai Agent Process</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 5</span>
        </div>
    </nav>

        <header>
        <div class="container">
            <div class="site-title">
                <a href="../../../../../jp/index.html">Hashimoto Laboratory</a>
            </div>
            <nav class="main-nav" role="navigation" aria-label="Main navigation">
                <ul>
                    <li><a href="../../../../../jp/research.html">ç ”ç©¶å†…å®¹</a></li>
                    <li><a href="../../../../../jp/publications.html">ç ”ç©¶æ¥­ç¸¾</a></li>
                    <li><a href="../../../../../jp/members.html">ãƒ¡ãƒ³ãƒãƒ¼</a></li>
                    <li><a href="../../../../../jp/equipment.html">è¨­å‚™</a></li>
                    <li><a href="../../../../../jp/talks.html">è¬›æ¼”</a></li>
                    <li><a href="../../../../../jp/contact.html">ãŠå•ã„åˆã‚ã›</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main class="article-content">
        <article>
            <div class="series-header">
                <div class="series-title">AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹è‡ªå¾‹ãƒ—ãƒ­ã‚»ã‚¹é‹è»¢ ã‚·ãƒªãƒ¼ã‚º</div>
                <h1 class="chapter-title">Chapter 5: å®Ÿãƒ—ãƒ©ãƒ³ãƒˆã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã¨å®‰å…¨æ€§</h1>
                <div class="chapter-nav">
                    <a href="chapter-4.html">â† Chapter 4: ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå”èª¿åˆ¶å¾¡</a>
                    <a href="index.html">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a>
                </div>
            </div>

            <section id="overview">
                <h2>Chapter 5ã®æ¦‚è¦</h2>
                <p>
                    å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒã§å­¦ç¿’ã•ã›ã‚‹ã ã‘ã§ã¯ä¸ååˆ†ã§ã™ã€‚
                    å®Ÿéš›ã®ãƒ—ãƒ©ãƒ³ãƒˆã«ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹éš›ã«ã¯ã€<strong>å®‰å…¨æ€§ã®ç¢ºä¿</strong>ã€<strong>ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ç¾å®Ÿã®ã‚®ãƒ£ãƒƒãƒ—ã®å…‹æœ</strong>ã€
                    <strong>ä¸ç¢ºå®Ÿæ€§ã¸ã®å¯¾å‡¦</strong>ãªã©ã€å¤šãã®èª²é¡ŒãŒã‚ã‚Šã¾ã™ã€‚
                </p>
                <p>
                    æœ¬ç« ã§ã¯ã€å®Ÿãƒ—ãƒ©ãƒ³ãƒˆã¸ã®é©ç”¨ã‚’è¦‹æ®ãˆãŸ7ã¤ã®é‡è¦æŠ€è¡“ã‚’å®Ÿè£…ä¾‹ã¨ã¨ã‚‚ã«è§£èª¬ã—ã¾ã™ã€‚
                    åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆã¯é«˜æ¸©ãƒ»é«˜åœ§ãƒ»å±é™ºç‰©ã‚’æ‰±ã†ãŸã‚ã€AIã«ã‚ˆã‚‹è‡ªå¾‹åˆ¶å¾¡ã«ã¯ç‰¹ã«æ…é‡ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¿…è¦ã§ã™ã€‚
                </p>

                <div class="warning-box">
                    <h4>å®‰å…¨æ€§ã«é–¢ã™ã‚‹é‡è¦ãªæ³¨æ„</h4>
                    <p>
                        æœ¬ç« ã§æ‰±ã†æŠ€è¡“ã¯ã€å®Ÿãƒ—ãƒ©ãƒ³ãƒˆã¸ã®é©ç”¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚
                        å®Ÿéš›ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã«ã¯ã€ãƒ—ãƒ­ã‚»ã‚¹å®‰å…¨ã®å°‚é–€çŸ¥è­˜ã€è¦åˆ¶éµå®ˆã€ååˆ†ãªæ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆãŒå¿…è¦ã§ã™ã€‚
                        æœ¬ã‚³ãƒ¼ãƒ‰ã¯æ•™è‚²ç›®çš„ã§ã‚ã‚Šã€å®Ÿãƒ—ãƒ©ãƒ³ãƒˆã§ã®ä½¿ç”¨ã«ã¯é©åˆ‡ãªå®‰å…¨è©•ä¾¡ãŒä¸å¯æ¬ ã§ã™ã€‚
                    </p>
                </div>

                <div class="key-points">
                    <h3>æœ¬ç« ã§å­¦ã¶ã“ã¨</h3>
                    <ul>
                        <li><strong>Sim-to-Realè»¢ç§»</strong>ï¼šãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒ€ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹ãƒ­ãƒã‚¹ãƒˆæ€§å‘ä¸Š</li>
                        <li><strong>å®‰å…¨ãªæ¢ç´¢</strong>ï¼šè¡Œå‹•åˆ¶ç´„ã«ã‚ˆã‚‹å±é™ºé ˜åŸŸã®å›é¿</li>
                        <li><strong>ä¿å®ˆçš„Qå­¦ç¿’ï¼ˆCQLï¼‰</strong>ï¼šéå¤§è©•ä¾¡ã‚’é˜²ãã‚ªãƒ•ãƒ©ã‚¤ãƒ³å­¦ç¿’</li>
                        <li><strong>äººé–“ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰</strong>ï¼šç·Šæ€¥æ™‚ã®äººé–“ä»‹å…¥æ©Ÿæ§‹</li>
                        <li><strong>ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–</strong>ï¼šãƒ™ã‚¤ã‚ºNNã‚„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã«ã‚ˆã‚‹ä¿¡é ¼åŒºé–“æ¨å®š</li>
                        <li><strong>æ€§èƒ½ç›£è¦–ã¨ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º</strong>ï¼šç¶™ç¶šçš„ãªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°</li>
                        <li><strong>çµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯</strong>ï¼šå…¨è¦ç´ ã‚’çµ„ã¿åˆã‚ã›ãŸå®Ÿè£…</li>
                    </ul>
                </div>
            </section>

            <section id="deployment-challenges">
                <h2>å®Ÿãƒ—ãƒ©ãƒ³ãƒˆãƒ‡ãƒ—ãƒ­ã‚¤ã®èª²é¡Œ</h2>

                <h3>Sim-to-Real Gapï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ç¾å®Ÿã®ã‚®ãƒ£ãƒƒãƒ—ï¼‰</h3>
                <div class="mermaid">
                graph LR
                    A[ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒ] -->|ç†æƒ³çš„ãªãƒ¢ãƒ‡ãƒ«| B[å®Œç’§ãªåˆ¶å¾¡]
                    C[å®Ÿãƒ—ãƒ©ãƒ³ãƒˆ] -->|ãƒ¢ãƒ‡ãƒ«èª¤å·®<br/>å¤–ä¹±<br/>ã‚»ãƒ³ã‚µãƒ¼ãƒã‚¤ã‚º| D[æ€§èƒ½åŠ£åŒ–]
                    B -.->|Sim-to-Real Gap| D
                    E[ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒ€ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³] --> F[ãƒ­ãƒã‚¹ãƒˆãªæ–¹ç­–]
                    F --> C
                </div>

                <h3>å®‰å…¨æ€§ã®éšå±¤</h3>
                <table style="width: 100%; border-collapse: collapse; margin: 1.5rem 0;">
                    <thead style="background: var(--pi-primary); color: white;">
                        <tr>
                            <th style="padding: 0.75rem; border: 1px solid #ddd;">ãƒ¬ã‚¤ãƒ¤ãƒ¼</th>
                            <th style="padding: 0.75rem; border: 1px solid #ddd;">æ©Ÿèƒ½</th>
                            <th style="padding: 0.75rem; border: 1px solid #ddd;">å®Ÿè£…</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">1. è¡Œå‹•åˆ¶ç´„</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">å±é™ºãªè¡Œå‹•ã®ç¦æ­¢</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">ãƒãƒ¼ãƒ‰ãƒªãƒŸãƒƒãƒˆã€ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">2. ä¸ç¢ºå®Ÿæ€§è€ƒæ…®</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">ä¿¡é ¼åŒºé–“ã®è©•ä¾¡</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">ãƒ™ã‚¤ã‚ºNNã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">3. æ€§èƒ½ç›£è¦–</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">ç•°å¸¸æ¤œçŸ¥</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã€KPIãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">4. äººé–“ä»‹å…¥</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">ç·Šæ€¥åœæ­¢</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰æ©Ÿæ§‹</td>
                        </tr>
                    </tbody>
                </table>
            </section>

            <section id="example1">
                <div class="example-box">
                    <div class="example-title">
                        <span class="example-number">1</span>
                        ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒ€ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹Sim-to-Realè»¢ç§»
                    </div>
                    <p>
                        ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã—ã¦å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€
                        å®Ÿç’°å¢ƒã®ä¸ç¢ºå®Ÿæ€§ã«å¯¾ã—ã¦ãƒ­ãƒã‚¹ãƒˆãªæ–¹ç­–ã‚’ç²å¾—ã—ã¾ã™ã€‚
                    </p>

                    <div class="formula-box">
                        <p><strong>ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒ€ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ï¼š</strong></p>
                        \[
                        \theta \sim p(\Theta), \quad \pi^* = \arg\max_\pi \mathbb{E}_{\theta \sim p(\Theta)} [J(\pi; \theta)]
                        \]
                        <p>ã“ã“ã§ã€\(\theta\)ã¯ç’°å¢ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€\(p(\Theta)\)ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†å¸ƒ</p>
                    </div>

                    <pre><code class="language-python"># ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒ€ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè£…
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

class RandomizedCSTREnv:
    """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã—ãŸCSTRç’°å¢ƒ"""
    def __init__(self, randomize=True):
        self.randomize = randomize
        self.reset()

    def _sample_parameters(self):
        """ç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        if self.randomize:
            # æ´»æ€§åŒ–ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆÂ±20%ã®å¤‰å‹•ï¼‰
            self.Ea = np.random.uniform(40000, 60000)

            # åå¿œç†±ï¼ˆÂ±15%ã®å¤‰å‹•ï¼‰
            self.dHr = np.random.uniform(-57500, -42500)

            # ç†±ä¼é”ä¿‚æ•°ï¼ˆÂ±25%ã®å¤‰å‹•ï¼‰
            self.U = np.random.uniform(300, 500)

            # ä½“ç©ï¼ˆè£½é€ ã°ã‚‰ã¤ãÂ±5%ï¼‰
            self.V = np.random.uniform(950, 1050)

            # ã‚»ãƒ³ã‚µãƒ¼ãƒã‚¤ã‚ºæ¨™æº–åå·®
            self.temp_noise = np.random.uniform(0.1, 1.0)
            self.conc_noise = np.random.uniform(0.01, 0.05)

            # åˆ¶å¾¡é…å»¶ï¼ˆé€šä¿¡é…å»¶+ãƒãƒ«ãƒ–å¿œç­”ï¼‰
            self.control_delay = np.random.randint(1, 4)
        else:
            # å…¬ç§°å€¤
            self.Ea = 50000
            self.dHr = -50000
            self.U = 400
            self.V = 1000
            self.temp_noise = 0.5
            self.conc_noise = 0.02
            self.control_delay = 2

    def reset(self):
        self._sample_parameters()
        self.state = np.array([350.0, 2.0])  # [æ¸©åº¦, æ¿ƒåº¦]
        self.action_buffer = [0.5] * self.control_delay
        return self._get_observation()

    def _get_observation(self):
        """ãƒã‚¤ã‚ºã‚’å«ã‚€è¦³æ¸¬"""
        T, CA = self.state
        T_obs = T + np.random.normal(0, self.temp_noise)
        CA_obs = CA + np.random.normal(0, self.conc_noise)
        return np.array([T_obs, CA_obs])

    def step(self, action):
        """åˆ¶å¾¡é…å»¶ã‚’è€ƒæ…®ã—ãŸã‚¹ãƒ†ãƒƒãƒ—"""
        # é…å»¶ã®ã‚ã‚‹è¡Œå‹•ã‚’é©ç”¨
        self.action_buffer.append(action)
        actual_action = self.action_buffer.pop(0)

        T, CA = self.state

        # åå¿œé€Ÿåº¦ï¼ˆãƒ©ãƒ³ãƒ€ãƒ åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
        R = 8.314
        k = 1e10 * np.exp(-self.Ea / (R * T))

        # CSTR dynamics
        dt = 0.1
        F = 100  # æµé‡ [L/min]
        CA_in = 2.5
        Tin = 350
        rho = 1000
        Cp = 4.18

        # å†·å´é‡ï¼ˆè¡Œå‹•ï¼‰
        Q_cool = actual_action * 10000  # 0-10000 W

        # ç‰©è³ªåæ”¯
        dCA = (F / self.V) * (CA_in - CA) - k * CA
        CA_new = CA + dCA * dt

        # ã‚¨ãƒãƒ«ã‚®ãƒ¼åæ”¯
        Q_rxn = -self.dHr * k * CA * self.V
        Q_jacket = self.U * 10 * (T - Tin) + Q_cool
        dT = (Q_rxn - Q_jacket) / (self.V * rho * Cp)
        T_new = T + dT * dt

        self.state = np.array([T_new, max(0, CA_new)])

        # å ±é…¬
        temp_penalty = -abs(T_new - 350) ** 2 * 0.01
        production = k * CA * 10
        reward = temp_penalty + production

        done = T_new > 400 or T_new < 300  # å®‰å…¨ç¯„å›²å¤–
        return self._get_observation(), reward, done

# ãƒ­ãƒã‚¹ãƒˆãªSACå­¦ç¿’
class RobustSACAgent:
    """ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒ€ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½¿ã†SACã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self, obs_dim, action_dim):
        self.actor = nn.Sequential(
            nn.Linear(obs_dim, 128), nn.ReLU(),
            nn.Linear(128, 128), nn.ReLU(),
            nn.Linear(128, action_dim), nn.Tanh()
        )
        self.optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)

    def select_action(self, obs):
        with torch.no_grad():
            return self.actor(torch.FloatTensor(obs)).numpy()

# å­¦ç¿’ï¼šãƒ©ãƒ³ãƒ€ãƒ åŒ–ç’°å¢ƒã§è¨“ç·´
train_env = RandomizedCSTREnv(randomize=True)
agent = RobustSACAgent(obs_dim=2, action_dim=1)

print("ãƒ©ãƒ³ãƒ€ãƒ åŒ–ç’°å¢ƒã§ã®å­¦ç¿’...")
for episode in range(500):
    obs = train_env.reset()
    episode_reward = 0

    for step in range(100):
        action = agent.select_action(obs)
        next_obs, reward, done = train_env.step(action[0])
        episode_reward += reward
        obs = next_obs

        if done:
            break

    if episode % 100 == 0:
        print(f"Episode {episode}, Reward: {episode_reward:.2f}")
        print(f"  Env params: Ea={train_env.Ea:.0f}, V={train_env.V:.0f}, "
              f"delay={train_env.control_delay}")

# è©•ä¾¡ï¼šå…¬ç§°ç’°å¢ƒï¼ˆå®Ÿãƒ—ãƒ©ãƒ³ãƒˆæƒ³å®šï¼‰ã§ãƒ†ã‚¹ãƒˆ
test_env = RandomizedCSTREnv(randomize=False)
print("\nå…¬ç§°ç’°å¢ƒã§ã®ãƒ†ã‚¹ãƒˆ...")

obs = test_env.reset()
test_reward = 0
temps = []

for step in range(100):
    action = agent.select_action(obs)
    obs, reward, done = test_env.step(action[0])
    test_reward += reward
    temps.append(obs[0])

    if done:
        break

print(f"Test Reward: {test_reward:.2f}")
print(f"Temp Mean: {np.mean(temps):.2f}K, Std: {np.std(temps):.2f}K")</code></pre>
                </div>
            </section>

            <section id="example2">
                <div class="example-box">
                    <div class="example-title">
                        <span class="example-number">2</span>
                        å®‰å…¨ãªæ¢ç´¢ï¼šè¡Œå‹•åˆ¶ç´„ã«ã‚ˆã‚‹ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿
                    </div>
                    <p>
                        å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå±é™ºãªè¡Œå‹•ã‚’å–ã‚‰ãªã„ã‚ˆã†ã€ç‰©ç†çš„ãƒ»å®‰å…¨çš„åˆ¶ç´„ã‚’èª²ã—ã¾ã™ã€‚
                        Control Barrier Functions (CBF)ã‚„ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å®Ÿè£…ã—ã¾ã™ã€‚
                    </p>

                    <div class="mermaid">
                    graph LR
                        A[RLæ–¹ç­–Ï€] -->|å±é™ºãªè¡Œå‹•?| B[ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿]
                        B -->|å®‰å…¨ãªè¡Œå‹•| C[å®Ÿè¡Œ]
                        B -->|åˆ¶ç´„é•å| D[å®‰å…¨ãªä»£æ›¿è¡Œå‹•]
                        D --> C
                    </div>

                    <pre><code class="language-python"># å®‰å…¨ãªæ¢ç´¢ï¼šã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿å®Ÿè£…
import numpy as np
import torch
import torch.nn as nn

class SafetyConstraints:
    """CSTRã®å®‰å…¨åˆ¶ç´„"""
    def __init__(self):
        # æ¸©åº¦åˆ¶ç´„
        self.T_min = 310.0  # [K]
        self.T_max = 390.0  # [K]
        self.T_target = 350.0

        # æ¿ƒåº¦åˆ¶ç´„
        self.CA_min = 0.1  # [mol/L]
        self.CA_max = 3.0

        # åˆ¶å¾¡å…¥åŠ›åˆ¶ç´„
        self.u_min = -100.0  # æœ€å¤§å†·å´ [kW]
        self.u_max = 50.0    # æœ€å¤§åŠ ç†± [kW]

        # å¤‰åŒ–ç‡åˆ¶ç´„
        self.du_max = 20.0  # [kW/step]

    def is_safe_state(self, state):
        """çŠ¶æ…‹ãŒå®‰å…¨ã‹ãƒã‚§ãƒƒã‚¯"""
        T, CA = state
        return (self.T_min <= T <= self.T_max and
                self.CA_min <= CA <= self.CA_max)

    def is_safe_action(self, state, action, prev_action):
        """è¡Œå‹•ãŒå®‰å…¨ã‹ãƒã‚§ãƒƒã‚¯"""
        # åˆ¶å¾¡å…¥åŠ›ç¯„å›²
        if not (self.u_min <= action <= self.u_max):
            return False

        # å¤‰åŒ–ç‡åˆ¶ç´„
        if abs(action - prev_action) > self.du_max:
            return False

        return True

    def project_to_safe(self, action, prev_action):
        """è¡Œå‹•ã‚’å®‰å…¨é ˜åŸŸã«å°„å½±"""
        # ç¯„å›²åˆ¶ç´„
        action = np.clip(action, self.u_min, self.u_max)

        # å¤‰åŒ–ç‡åˆ¶ç´„
        delta = action - prev_action
        if abs(delta) > self.du_max:
            action = prev_action + np.sign(delta) * self.du_max

        return action

class ControlBarrierFunction:
    """Control Barrier Function (CBF)ã«ã‚ˆã‚‹å®‰å…¨ä¿è¨¼"""
    def __init__(self, safety_constraints):
        self.constraints = safety_constraints
        self.alpha = 0.5  # ã‚¯ãƒ©ã‚¹Ké–¢æ•°ã®ã‚²ã‚¤ãƒ³

    def barrier_function(self, state):
        """ãƒãƒªã‚¢é–¢æ•° h(x) >= 0 ãŒå®‰å…¨é ˜åŸŸ"""
        T, CA = state

        # æ¸©åº¦ãƒãƒªã‚¢ï¼ˆè·é›¢é–¢æ•°ï¼‰
        h_T_min = T - self.constraints.T_min
        h_T_max = self.constraints.T_max - T

        # æ¿ƒåº¦ãƒãƒªã‚¢
        h_CA_min = CA - self.constraints.CA_min
        h_CA_max = self.constraints.CA_max - CA

        # æœ€å°å€¤ï¼ˆæœ€ã‚‚å³ã—ã„åˆ¶ç´„ï¼‰
        return min(h_T_min, h_T_max, h_CA_min, h_CA_max)

    def safe_action(self, state, desired_action, env_model):
        """CBFåˆ¶ç´„ã‚’æº€ãŸã™å®‰å…¨ãªè¡Œå‹•ã‚’è¨ˆç®—"""
        h = self.barrier_function(state)

        # å®‰å…¨ãªé ˜åŸŸãªã‚‰ä½•ã‚‚ã—ãªã„
        if h > 10.0:
            return desired_action

        # å¢ƒç•Œè¿‘ãã§ã¯åˆ¶ç´„ã‚’èª²ã™
        # ç°¡æ˜“å®Ÿè£…ï¼šäºˆæ¸¬ã•ã‚Œã‚‹æ¬¡çŠ¶æ…‹ã§ãƒãƒªã‚¢æ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
        next_state_pred = env_model.predict(state, desired_action)
        h_next = self.barrier_function(next_state_pred)

        # CBFæ¡ä»¶: h_next >= -alpha * h
        if h_next >= -self.alpha * h:
            return desired_action
        else:
            # å®‰å…¨å´ã«ä¿®æ­£ï¼ˆä¿å®ˆçš„ãªè¡Œå‹•ï¼‰
            T, CA = state
            if T > self.constraints.T_target:
                # å†·å´å¼·åŒ–
                return max(desired_action, 0)
            else:
                # åŠ ç†±æŠ‘åˆ¶
                return min(desired_action, 0)

class SimpleCSTRModel:
    """CSTRã®ç°¡æ˜“äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«"""
    def predict(self, state, action, dt=0.1):
        T, CA = state
        k = 1e10 * np.exp(-50000 / (8.314 * T))

        # ç°¡æ˜“dynamics
        dCA = -k * CA * dt
        dT = (action * 1000 - 400 * (T - 350)) / 4180 * dt

        return np.array([T + dT, CA + dCA])

# ä½¿ç”¨ä¾‹
safety = SafetyConstraints()
cbf = ControlBarrierFunction(safety)
model = SimpleCSTRModel()

# RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•ã«ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
class SafeRLAgent:
    def __init__(self, base_agent, safety_filter):
        self.base_agent = base_agent
        self.safety_filter = safety_filter
        self.prev_action = 0.0

    def select_safe_action(self, state):
        # ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•
        desired_action = self.base_agent.select_action(state)[0] * 100

        # ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
        safe_action = self.safety_filter.project_to_safe(
            desired_action, self.prev_action)

        # CBFåˆ¶ç´„
        safe_action = cbf.safe_action(state, safe_action, model)

        self.prev_action = safe_action
        return safe_action

# ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
from example1 import RobustSACAgent, RandomizedCSTREnv

base_agent = RobustSACAgent(obs_dim=2, action_dim=1)
safe_agent = SafeRLAgent(base_agent, safety)
env = RandomizedCSTREnv(randomize=False)

print("å®‰å…¨åˆ¶ç´„ä»˜ãå®Ÿè¡Œ...")
state = env.reset()
unsafe_count = 0

for step in range(200):
    action = safe_agent.select_safe_action(state)
    next_state, reward, done = env.step(action / 100)

    if not safety.is_safe_state(state):
        unsafe_count += 1
        print(f"Step {step}: UNSAFE STATE! T={state[0]:.1f}K, CA={state[1]:.3f}")

    state = next_state

    if done:
        break

print(f"\nUnsafe states encountered: {unsafe_count} / {step+1}")
print(f"Safety rate: {(1 - unsafe_count/(step+1))*100:.1f}%")</code></pre>
                </div>
            </section>

            <section id="example3">
                <div class="example-box">
                    <div class="example-title">
                        <span class="example-number">3</span>
                        Conservative Q-Learningï¼ˆCQLï¼‰ï¼šä¿å®ˆçš„ãªã‚ªãƒ•ãƒ©ã‚¤ãƒ³å­¦ç¿’
                    </div>
                    <p>
                        å®Ÿãƒ—ãƒ©ãƒ³ãƒˆã§ã¯æ¢ç´¢ãŒå±é™ºãªãŸã‚ã€éå»ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§å­¦ç¿’ã—ã¾ã™ã€‚
                        CQLã¯åˆ†å¸ƒå¤–è¡Œå‹•ã®Qå€¤ã‚’éå°è©•ä¾¡ã—ã€å®‰å…¨ãªæ–¹ç­–ã‚’å­¦ç¿’ã—ã¾ã™ã€‚
                    </p>

                    <div class="formula-box">
                        <p><strong>CQLç›®çš„é–¢æ•°ï¼š</strong></p>
                        \[
                        \min_Q \alpha \cdot \mathbb{E}_{s \sim \mathcal{D}} \left[ \log \sum_a \exp(Q(s,a)) - \mathbb{E}_{a \sim \mu(a|s)} [Q(s,a)] \right] + \mathcal{L}_{TD}(Q)
                        \]
                        <p>ç¬¬1é …ï¼šåˆ†å¸ƒå¤–è¡Œå‹•ã®Qå€¤ã‚’ä¸‹ã’ã‚‹ã€ç¬¬2é …ï¼šãƒ‡ãƒ¼ã‚¿å†…è¡Œå‹•ã®Qå€¤ã‚’ä¿ã¤</p>
                    </div>

                    <pre><code class="language-python"># Conservative Q-Learning (CQL) å®Ÿè£…
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import deque
import random

class CQLQNetwork(nn.Module):
    """CQLç”¨ã®Qé–¢æ•°"""
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim + action_dim, 256), nn.ReLU(),
            nn.Linear(256, 256), nn.ReLU(),
            nn.Linear(256, 1)
        )

    def forward(self, state, action):
        x = torch.cat([state, action], dim=-1)
        return self.net(x)

class CQLAgent:
    """Conservative Q-Learning ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self, state_dim, action_dim, alpha=1.0):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.alpha = alpha  # CQLæ­£å‰‡åŒ–ä¿‚æ•°

        self.q_net = CQLQNetwork(state_dim, action_dim)
        self.target_q = CQLQNetwork(state_dim, action_dim)
        self.target_q.load_state_dict(self.q_net.state_dict())

        self.policy = nn.Sequential(
            nn.Linear(state_dim, 256), nn.ReLU(),
            nn.Linear(256, action_dim), nn.Tanh()
        )

        self.q_optimizer = optim.Adam(self.q_net.parameters(), lr=3e-4)
        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=3e-4)

    def select_action(self, state, deterministic=False):
        with torch.no_grad():
            state_t = torch.FloatTensor(state).unsqueeze(0)
            action = self.policy(state_t)
            if not deterministic:
                action += torch.randn_like(action) * 0.1
            return action.squeeze().numpy()

    def train_step(self, batch):
        """CQLæ›´æ–°"""
        states, actions, rewards, next_states, dones = batch

        states_t = torch.FloatTensor(states)
        actions_t = torch.FloatTensor(actions)
        rewards_t = torch.FloatTensor(rewards).unsqueeze(1)
        next_states_t = torch.FloatTensor(next_states)
        dones_t = torch.FloatTensor(dones).unsqueeze(1)

        # Qé–¢æ•°ã®æ›´æ–°
        # 1. Bellmanèª¤å·®
        with torch.no_grad():
            next_actions = self.policy(next_states_t)
            target_q = rewards_t + 0.99 * self.target_q(
                next_states_t, next_actions) * (1 - dones_t)

        current_q = self.q_net(states_t, actions_t)
        bellman_loss = nn.MSELoss()(current_q, target_q)

        # 2. CQLæ­£å‰‡åŒ–é …
        # ãƒ©ãƒ³ãƒ€ãƒ è¡Œå‹•ã®Qå€¤ã‚’è¨ˆç®—
        num_random = 10
        random_actions = torch.FloatTensor(
            np.random.uniform(-1, 1, (states_t.shape[0], num_random, self.action_dim)))

        random_q = []
        for i in range(num_random):
            q = self.q_net(states_t, random_actions[:, i, :])
            random_q.append(q)
        random_q = torch.cat(random_q, dim=1)

        # ãƒãƒªã‚·ãƒ¼ã«ã‚ˆã‚‹è¡Œå‹•ã®Qå€¤
        policy_actions = self.policy(states_t)
        policy_q = self.q_net(states_t, policy_actions)

        # CQLé …ï¼šãƒ©ãƒ³ãƒ€ãƒ è¡Œå‹•ã®Qå€¤ã‚’å¤§ããã€ãƒ‡ãƒ¼ã‚¿å†…è¡Œå‹•ã‚’å°ã•ãè©•ä¾¡
        cql_loss = (torch.logsumexp(random_q, dim=1).mean() -
                    policy_q.mean())

        # åˆè¨ˆæå¤±
        q_loss = bellman_loss + self.alpha * cql_loss

        self.q_optimizer.zero_grad()
        q_loss.backward()
        self.q_optimizer.step()

        # 3. ãƒãƒªã‚·ãƒ¼æ›´æ–°ï¼ˆQå€¤æœ€å¤§åŒ–ï¼‰
        policy_loss = -self.q_net(states_t, self.policy(states_t)).mean()

        self.policy_optimizer.zero_grad()
        policy_loss.backward()
        self.policy_optimizer.step()

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ›´æ–°
        for target_param, param in zip(self.target_q.parameters(),
                                        self.q_net.parameters()):
            target_param.data.copy_(0.995 * target_param.data + 0.005 * param.data)

        return q_loss.item(), cql_loss.item(), policy_loss.item()

# ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆï¼ˆéå»ã®é‹è»¢ãƒ‡ãƒ¼ã‚¿ï¼‰
def generate_offline_data(n_trajectories=100):
    from example1 import RandomizedCSTREnv

    env = RandomizedCSTREnv(randomize=True)
    dataset = []

    for _ in range(n_trajectories):
        state = env.reset()
        for _ in range(50):
            # ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒªã‚·ãƒ¼ + ãƒã‚¤ã‚ºï¼ˆç¾å®Ÿçš„ãªãƒ‡ãƒ¼ã‚¿ï¼‰
            action = np.random.uniform(-1, 1, 1)
            next_state, reward, done = env.step(action[0])
            dataset.append((state, action, reward, next_state, done))
            state = next_state
            if done:
                break

    return dataset

# CQLå­¦ç¿’
print("ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ...")
offline_data = generate_offline_data(n_trajectories=200)
print(f"Dataset size: {len(offline_data)}")

agent = CQLAgent(state_dim=2, action_dim=1, alpha=1.0)

print("\nCQLå­¦ç¿’ä¸­...")
batch_size = 256
for epoch in range(100):
    # ãƒŸãƒ‹ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    batch = random.sample(offline_data, min(batch_size, len(offline_data)))
    states, actions, rewards, next_states, dones = zip(*batch)

    batch_tuple = (
        np.array(states),
        np.array(actions),
        np.array(rewards),
        np.array(next_states),
        np.array(dones)
    )

    q_loss, cql_loss, p_loss = agent.train_step(batch_tuple)

    if epoch % 20 == 0:
        print(f"Epoch {epoch}: Q_loss={q_loss:.3f}, CQL_loss={cql_loss:.3f}, "
              f"Policy_loss={p_loss:.3f}")

# ãƒ†ã‚¹ãƒˆ
print("\nCQLæ–¹ç­–ã®ãƒ†ã‚¹ãƒˆ...")
from example1 import RandomizedCSTREnv
test_env = RandomizedCSTREnv(randomize=False)

state = test_env.reset()
total_reward = 0

for step in range(100):
    action = agent.select_action(state, deterministic=True)
    next_state, reward, done = test_env.step(action[0])
    total_reward += reward
    state = next_state
    if done:
        break

print(f"Test Reward: {total_reward:.2f}")</code></pre>
                </div>
            </section>

            <section id="example4">
                <div class="example-box">
                    <div class="example-title">
                        <span class="example-number">4</span>
                        Human-in-the-Loopï¼šäººé–“ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰æ©Ÿæ§‹
                    </div>
                    <p>
                        AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒäºˆæœŸã—ãªã„è¡Œå‹•ã‚’ã¨ã‚‹å ´åˆã«å‚™ãˆã€ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ãŒä»‹å…¥ã§ãã‚‹æ©Ÿæ§‹ãŒå¿…è¦ã§ã™ã€‚
                        ä»‹å…¥åˆ¤æ–­ã€ã‚¹ãƒ ãƒ¼ã‚ºãªç§»è¡Œã€å±¥æ­´è¨˜éŒ²ã‚’å®Ÿè£…ã—ã¾ã™ã€‚
                    </p>

                    <div class="mermaid">
                    graph TD
                        A[AIåˆ¶å¾¡] -->|ç•°å¸¸æ¤œçŸ¥| B{äººé–“åˆ¤æ–­}
                        B -->|OK| A
                        B -->|ä»‹å…¥| C[æ‰‹å‹•åˆ¶å¾¡]
                        C -->|å®‰å®šåŒ–| D{å¾©å¸°æ¡ä»¶}
                        D -->|æº€ãŸã™| A
                        D -->|æº€ãŸã•ãªã„| C
                    </div>

                    <pre><code class="language-python"># Human-in-the-Loop ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…
import numpy as np
import time
from datetime import datetime
from enum import Enum

class ControlMode(Enum):
    """åˆ¶å¾¡ãƒ¢ãƒ¼ãƒ‰"""
    AI_CONTROL = "AI"
    HUMAN_OVERRIDE = "Human"
    TRANSITION = "Transition"

class HumanOverrideSystem:
    """äººé–“ä»‹å…¥ã‚·ã‚¹ãƒ†ãƒ """
    def __init__(self):
        self.mode = ControlMode.AI_CONTROL
        self.intervention_history = []
        self.confidence_threshold = 0.7

    def check_intervention_needed(self, state, ai_action, confidence):
        """ä»‹å…¥ãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯"""
        T, CA = state

        # ãƒˆãƒªã‚¬ãƒ¼æ¡ä»¶
        triggers = {
            'high_temp': T > 380,
            'low_temp': T < 320,
            'low_confidence': confidence < self.confidence_threshold,
            'extreme_action': abs(ai_action) > 0.9,
            'unstable_state': CA < 0.2 or CA > 2.8
        }

        if any(triggers.values()):
            reason = [k for k, v in triggers.items() if v]
            return True, reason
        return False, []

    def request_human_action(self, state, ai_suggestion):
        """ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ã«è¡Œå‹•ã‚’å•ã„åˆã‚ã›ï¼ˆå®Ÿéš›ã¯GUI/CLIï¼‰"""
        T, CA = state
        print(f"\n{'='*60}")
        print(f"äººé–“ä»‹å…¥è¦æ±‚ï¼")
        print(f"ç¾åœ¨ã®çŠ¶æ…‹: T={T:.2f}K, CA={CA:.3f}mol/L")
        print(f"AIææ¡ˆè¡Œå‹•: {ai_suggestion:.3f}")
        print(f"{'='*60}")

        # ç°¡ç•¥åŒ–ï¼šãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        # å®Ÿéš›ã¯ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ã®å…¥åŠ›ã‚’å¾…ã¤
        if T > 380:
            human_action = -0.8  # å¼·å†·å´
            override = True
        elif T < 320:
            human_action = 0.6   # åŠ ç†±
            override = True
        else:
            human_action = ai_suggestion
            override = False

        return human_action, override

    def smooth_transition(self, from_action, to_action, alpha=0.3):
        """ã‚¹ãƒ ãƒ¼ã‚ºãªåˆ¶å¾¡ç§»è¡Œ"""
        return alpha * to_action + (1 - alpha) * from_action

    def log_intervention(self, timestamp, state, ai_action, human_action, reason):
        """ä»‹å…¥å±¥æ­´è¨˜éŒ²"""
        log_entry = {
            'timestamp': timestamp,
            'state': state.copy(),
            'ai_action': ai_action,
            'human_action': human_action,
            'reason': reason
        }
        self.intervention_history.append(log_entry)

    def generate_report(self):
        """ä»‹å…¥ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if not self.intervention_history:
            return "No interventions recorded."

        report = "\n" + "="*60 + "\n"
        report += "Human Intervention Report\n"
        report += "="*60 + "\n"
        report += f"Total interventions: {len(self.intervention_history)}\n\n"

        for i, entry in enumerate(self.intervention_history):
            report += f"Intervention {i+1}:\n"
            report += f"  Time: {entry['timestamp']}\n"
            report += f"  State: T={entry['state'][0]:.2f}K, CA={entry['state'][1]:.3f}\n"
            report += f"  AI action: {entry['ai_action']:.3f}\n"
            report += f"  Human action: {entry['human_action']:.3f}\n"
            report += f"  Reason: {', '.join(entry['reason'])}\n\n"

        return report

class HITLController:
    """Human-in-the-Loopåˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ """
    def __init__(self, ai_agent, override_system):
        self.ai_agent = ai_agent
        self.override_system = override_system
        self.prev_action = 0.0

    def select_action(self, state, confidence=1.0):
        """äººé–“ä»‹å…¥ã‚’è€ƒæ…®ã—ãŸè¡Œå‹•é¸æŠ"""
        # AIææ¡ˆ
        ai_action = self.ai_agent.select_action(state)[0]

        # ä»‹å…¥åˆ¤å®š
        need_intervention, reasons = self.override_system.check_intervention_needed(
            state, ai_action, confidence)

        if need_intervention:
            # äººé–“ã«å•ã„åˆã‚ã›
            human_action, overridden = self.override_system.request_human_action(
                state, ai_action)

            if overridden:
                # ä»‹å…¥è¨˜éŒ²
                self.override_system.log_intervention(
                    datetime.now(), state, ai_action, human_action, reasons)
                self.override_system.mode = ControlMode.HUMAN_OVERRIDE
                final_action = human_action
            else:
                final_action = ai_action
        else:
            final_action = ai_action
            self.override_system.mode = ControlMode.AI_CONTROL

        # ã‚¹ãƒ ãƒ¼ã‚ºé·ç§»
        final_action = self.override_system.smooth_transition(
            self.prev_action, final_action)

        self.prev_action = final_action
        return final_action

# ä½¿ç”¨ä¾‹
from example1 import RobustSACAgent, RandomizedCSTREnv

print("Human-in-the-Loop ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•...")

ai_agent = RobustSACAgent(obs_dim=2, action_dim=1)
override_system = HumanOverrideSystem()
controller = HITLController(ai_agent, override_system)

env = RandomizedCSTREnv(randomize=True)
state = env.reset()

# éé…·ãªæ¡ä»¶ã§ãƒ†ã‚¹ãƒˆ
for step in range(50):
    # ä¿¡é ¼åº¦ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„çŠ¶æ³ï¼‰
    confidence = np.random.uniform(0.5, 1.0)

    action = controller.select_action(state, confidence)
    next_state, reward, done = env.step(action)

    print(f"Step {step}: Mode={override_system.mode.value}, "
          f"T={state[0]:.1f}K, Action={action:.3f}")

    state = next_state

    if done:
        print("ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ï¼ˆç•°å¸¸çŠ¶æ…‹ï¼‰")
        break

# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
print(override_system.generate_report())</code></pre>
                </div>
            </section>

            <section id="example5">
                <div class="example-box">
                    <div class="example-title">
                        <span class="example-number">5</span>
                        ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–ï¼šãƒ™ã‚¤ã‚ºNNã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•
                    </div>
                    <p>
                        AIã®äºˆæ¸¬ãŒã©ã‚Œã ã‘ç¢ºã‹ã‚‰ã—ã„ã‹ã‚’å®šé‡åŒ–ã™ã‚‹ã“ã¨ã§ã€ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„çŠ¶æ³ã§ã¯ä¿å®ˆçš„ãªè¡Œå‹•ã‚’å–ã‚Šã¾ã™ã€‚
                        ãƒ™ã‚¤ã‚ºãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã§ä¿¡é ¼åŒºé–“ã‚’æ¨å®šã—ã¾ã™ã€‚
                    </p>

                    <pre><code class="language-python"># ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–ï¼šã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã¨ãƒ™ã‚¤ã‚ºè¿‘ä¼¼
import torch
import torch.nn as nn
import numpy as np

class EnsembleQNetwork:
    """Qé–¢æ•°ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«"""
    def __init__(self, state_dim, action_dim, n_models=5):
        self.n_models = n_models
        self.models = [
            nn.Sequential(
                nn.Linear(state_dim + action_dim, 128), nn.ReLU(),
                nn.Linear(128, 128), nn.ReLU(),
                nn.Linear(128, 1)
            ) for _ in range(n_models)
        ]
        self.optimizers = [
            torch.optim.Adam(m.parameters(), lr=1e-3) for m in self.models
        ]

    def predict_with_uncertainty(self, state, action):
        """äºˆæ¸¬å¹³å‡ã¨ä¸ç¢ºå®Ÿæ€§ï¼ˆæ¨™æº–åå·®ï¼‰ã‚’è¿”ã™"""
        state_t = torch.FloatTensor(state).unsqueeze(0)
        action_t = torch.FloatTensor(action).unsqueeze(0)
        x = torch.cat([state_t, action_t], dim=-1)

        predictions = []
        for model in self.models:
            with torch.no_grad():
                pred = model(x).item()
            predictions.append(pred)

        mean = np.mean(predictions)
        std = np.std(predictions)

        return mean, std

    def train_step(self, batch):
        """å…¨ãƒ¢ãƒ‡ãƒ«ã‚’ç•°ãªã‚‹ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒ«ã§å­¦ç¿’"""
        states, actions, targets = batch

        losses = []
        for i, (model, opt) in enumerate(zip(self.models, self.optimizers)):
            # ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            indices = np.random.choice(len(states), len(states), replace=True)
            s = torch.FloatTensor(states[indices])
            a = torch.FloatTensor(actions[indices])
            t = torch.FloatTensor(targets[indices])

            x = torch.cat([s, a], dim=-1)
            pred = model(x).squeeze()

            loss = nn.MSELoss()(pred, t)
            opt.zero_grad()
            loss.backward()
            opt.step()

            losses.append(loss.item())

        return np.mean(losses)

class MCDropoutQNetwork(nn.Module):
    """Monte Carlo Dropoutï¼ˆãƒ™ã‚¤ã‚ºè¿‘ä¼¼ï¼‰"""
    def __init__(self, state_dim, action_dim, dropout_rate=0.1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim + action_dim, 128),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(128, 1)
        )

    def forward(self, state, action):
        x = torch.cat([state, action], dim=-1)
        return self.net(x)

    def predict_with_uncertainty(self, state, action, n_samples=20):
        """MC Dropoutã§ä¸ç¢ºå®Ÿæ€§æ¨å®š"""
        self.train()  # Dropoutã‚’æœ‰åŠ¹åŒ–

        state_t = torch.FloatTensor(state).unsqueeze(0)
        action_t = torch.FloatTensor(action).unsqueeze(0)

        predictions = []
        for _ in range(n_samples):
            pred = self.forward(state_t, action_t).item()
            predictions.append(pred)

        mean = np.mean(predictions)
        std = np.std(predictions)

        return mean, std

class UncertaintyAwareAgent:
    """ä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®ã—ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self, state_dim, action_dim, method='ensemble'):
        self.method = method
        if method == 'ensemble':
            self.q_network = EnsembleQNetwork(state_dim, action_dim, n_models=5)
        else:
            self.q_network = MCDropoutQNetwork(state_dim, action_dim)

        self.policy = nn.Sequential(
            nn.Linear(state_dim, 128), nn.ReLU(),
            nn.Linear(128, action_dim), nn.Tanh()
        )
        self.uncertainty_threshold = 0.5  # ä¸ç¢ºå®Ÿæ€§ã®é–¾å€¤

    def select_action(self, state):
        """ä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®ã—ãŸè¡Œå‹•é¸æŠ"""
        with torch.no_grad():
            nominal_action = self.policy(torch.FloatTensor(state)).numpy()

        # è¤‡æ•°å€™è£œã®ä¸ç¢ºå®Ÿæ€§è©•ä¾¡
        action_candidates = [
            nominal_action,
            nominal_action * 0.5,  # ä¿å®ˆçš„
            np.zeros_like(nominal_action)  # ç¾çŠ¶ç¶­æŒ
        ]

        best_action = nominal_action
        min_uncertainty = float('inf')

        for action in action_candidates:
            if self.method == 'ensemble':
                q_mean, q_std = self.q_network.predict_with_uncertainty(state, action)
            else:
                q_mean, q_std = self.q_network.predict_with_uncertainty(state, action)

            # ä¸ç¢ºå®Ÿæ€§ãŒä½ãã€Qå€¤ãŒé«˜ã„è¡Œå‹•ã‚’é¸æŠ
            if q_std < self.uncertainty_threshold and q_std < min_uncertainty:
                best_action = action
                min_uncertainty = q_std

        return best_action, min_uncertainty

# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–ãƒ‡ãƒ¢\n")

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•
ensemble_agent = UncertaintyAwareAgent(state_dim=2, action_dim=1, method='ensemble')

test_states = [
    np.array([350.0, 1.5]),  # æ­£å¸¸
    np.array([385.0, 0.8]),  # é«˜æ¸©
    np.array([310.0, 2.5])   # ä½æ¸©
]

print("Ensemble Method:")
for i, state in enumerate(test_states):
    action, uncertainty = ensemble_agent.select_action(state)
    print(f"State {i+1}: T={state[0]}K, CA={state[1]}")
    print(f"  Action: {action[0]:.3f}, Uncertainty: {uncertainty:.3f}")

    if uncertainty > ensemble_agent.uncertainty_threshold:
        print(f"  WARNING: High uncertainty! Consider human oversight.")
    print()

# MC Dropoutæ‰‹æ³•
mc_agent = UncertaintyAwareAgent(state_dim=2, action_dim=1, method='mcdropout')

print("\nMC Dropout Method:")
for i, state in enumerate(test_states):
    action, uncertainty = mc_agent.select_action(state)
    print(f"State {i+1}: T={state[0]}K, CA={state[1]}")
    print(f"  Action: {action[0]:.3f}, Uncertainty: {uncertainty:.3f}")

    if uncertainty > mc_agent.uncertainty_threshold:
        print(f"  WARNING: High uncertainty!")
    print()</code></pre>
                </div>
            </section>

            <section id="example6">
                <div class="example-box">
                    <div class="example-title">
                        <span class="example-number">6</span>
                        æ€§èƒ½ç›£è¦–ã¨ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º
                    </div>
                    <p>
                        ãƒ‡ãƒ—ãƒ­ã‚¤å¾Œã‚‚ç¶™ç¶šçš„ã«æ€§èƒ½ã‚’ç›£è¦–ã—ã€ãƒ—ãƒ©ãƒ³ãƒˆã®çµŒå¹´åŠ£åŒ–ã‚„ãƒ¢ãƒ‡ãƒ«ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡ºã—ã¾ã™ã€‚
                        KPIãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã€çµ±è¨ˆçš„æ¤œå®šã€ç•°å¸¸æ¤œçŸ¥ã‚’å®Ÿè£…ã—ã¾ã™ã€‚
                    </p>

                    <pre><code class="language-python"># æ€§èƒ½ç›£è¦–ã¨ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 
import numpy as np
from collections import deque
from scipy import stats

class PerformanceMonitor:
    """æ€§èƒ½ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ """
    def __init__(self, window_size=100):
        self.window_size = window_size

        # KPIå±¥æ­´
        self.rewards = deque(maxlen=window_size)
        self.temperatures = deque(maxlen=window_size)
        self.concentrations = deque(maxlen=window_size)
        self.actions = deque(maxlen=window_size)

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµ±è¨ˆ
        self.baseline_reward_mean = None
        self.baseline_reward_std = None

        # ç•°å¸¸ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        self.anomaly_count = 0
        self.total_steps = 0

    def update(self, state, action, reward):
        """KPIæ›´æ–°"""
        T, CA = state
        self.rewards.append(reward)
        self.temperatures.append(T)
        self.concentrations.append(CA)
        self.actions.append(action)
        self.total_steps += 1

    def set_baseline(self):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½ã‚’è¨­å®š"""
        if len(self.rewards) >= self.window_size:
            self.baseline_reward_mean = np.mean(self.rewards)
            self.baseline_reward_std = np.std(self.rewards)
            print(f"Baseline set: Î¼={self.baseline_reward_mean:.2f}, "
                  f"Ïƒ={self.baseline_reward_std:.2f}")

    def detect_drift(self, alpha=0.05):
        """çµ±è¨ˆçš„ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºï¼ˆtæ¤œå®šï¼‰"""
        if self.baseline_reward_mean is None or len(self.rewards) < 50:
            return False, None

        current_mean = np.mean(list(self.rewards)[-50:])

        # tæ¤œå®š
        t_stat, p_value = stats.ttest_1samp(
            list(self.rewards)[-50:],
            self.baseline_reward_mean
        )

        drift_detected = p_value < alpha and current_mean < self.baseline_reward_mean

        return drift_detected, {
            't_stat': t_stat,
            'p_value': p_value,
            'current_mean': current_mean,
            'baseline_mean': self.baseline_reward_mean
        }

    def detect_anomaly(self, state, action):
        """ç•°å¸¸æ¤œå‡ºï¼ˆ3Ïƒãƒ«ãƒ¼ãƒ«ï¼‰"""
        if len(self.rewards) < 30:
            return False

        T, CA = state

        # æ¸©åº¦ç•°å¸¸
        temp_mean = np.mean(self.temperatures)
        temp_std = np.std(self.temperatures)
        temp_anomaly = abs(T - temp_mean) > 3 * temp_std

        # æ¿ƒåº¦ç•°å¸¸
        conc_mean = np.mean(self.concentrations)
        conc_std = np.std(self.concentrations)
        conc_anomaly = abs(CA - conc_mean) > 3 * conc_std

        # è¡Œå‹•ç•°å¸¸
        action_mean = np.mean(self.actions)
        action_std = np.std(self.actions)
        action_anomaly = abs(action - action_mean) > 3 * action_std

        is_anomaly = temp_anomaly or conc_anomaly or action_anomaly

        if is_anomaly:
            self.anomaly_count += 1

        return is_anomaly

    def generate_report(self):
        """ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if len(self.rewards) == 0:
            return "No data collected."

        report = "\n" + "="*60 + "\n"
        report += "Performance Monitoring Report\n"
        report += "="*60 + "\n\n"

        report += f"Total steps: {self.total_steps}\n"
        report += f"Anomalies detected: {self.anomaly_count} "
        report += f"({self.anomaly_count/self.total_steps*100:.2f}%)\n\n"

        report += "KPI Statistics (last {} steps):\n".format(len(self.rewards))
        report += f"  Reward: Î¼={np.mean(self.rewards):.2f}, "
        report += f"Ïƒ={np.std(self.rewards):.2f}\n"
        report += f"  Temperature: Î¼={np.mean(self.temperatures):.2f}K, "
        report += f"Ïƒ={np.std(self.temperatures):.2f}K\n"
        report += f"  Concentration: Î¼={np.mean(self.concentrations):.3f}, "
        report += f"Ïƒ={np.std(self.concentrations):.3f}\n"
        report += f"  Action: Î¼={np.mean(self.actions):.3f}, "
        report += f"Ïƒ={np.std(self.actions):.3f}\n\n"

        # ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º
        drift, drift_info = self.detect_drift()
        if drift:
            report += "WARNING: Performance drift detected!\n"
            report += f"  Current mean reward: {drift_info['current_mean']:.2f}\n"
            report += f"  Baseline mean reward: {drift_info['baseline_mean']:.2f}\n"
            report += f"  p-value: {drift_info['p_value']:.4f}\n"
        else:
            report += "No significant drift detected.\n"

        report += "="*60 + "\n"
        return report

class DriftDetector:
    """ã‚ˆã‚Šé«˜åº¦ãªãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºï¼ˆADWINï¼‰"""
    def __init__(self, delta=0.002):
        self.delta = delta
        self.window = deque()
        self.drift_detected = False

    def add_element(self, value):
        """ãƒ‡ãƒ¼ã‚¿ç‚¹ã‚’è¿½åŠ ã—ã¦ãƒ‰ãƒªãƒ•ãƒˆãƒã‚§ãƒƒã‚¯"""
        self.window.append(value)

        # ç°¡æ˜“ç‰ˆADWIN: ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’2åˆ†å‰²ã—ã¦å¹³å‡ã‚’æ¯”è¼ƒ
        if len(self.window) > 50:
            mid = len(self.window) // 2
            window1 = list(self.window)[:mid]
            window2 = list(self.window)[mid:]

            # Welchã®tæ¤œå®šï¼ˆç­‰åˆ†æ•£ã‚’ä»®å®šã—ãªã„ï¼‰
            t_stat, p_value = stats.ttest_ind(window1, window2, equal_var=False)

            if p_value < self.delta:
                self.drift_detected = True
                self.window.clear()  # ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºå¾Œãƒªã‚»ãƒƒãƒˆ
                return True

        return False

# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
from example1 import RandomizedCSTREnv, RobustSACAgent

print("æ€§èƒ½ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•...\n")

env = RandomizedCSTREnv(randomize=False)
agent = RobustSACAgent(obs_dim=2, action_dim=1)
monitor = PerformanceMonitor(window_size=100)
drift_detector = DriftDetector()

# ãƒ•ã‚§ãƒ¼ã‚º1: æ­£å¸¸é‹è»¢ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¨­å®šï¼‰
print("Phase 1: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¨­å®šä¸­...")
state = env.reset()

for step in range(100):
    action = agent.select_action(state)
    next_state, reward, done = env.step(action[0])

    monitor.update(state, action[0], reward)

    state = next_state if not done else env.reset()

monitor.set_baseline()

# ãƒ•ã‚§ãƒ¼ã‚º2: åŠ£åŒ–ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("\nPhase 2: ãƒ—ãƒ©ãƒ³ãƒˆåŠ£åŒ–ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ...")

for step in range(200):
    action = agent.select_action(state)

    # æ™‚é–“çµŒéã§æ€§èƒ½åŠ£åŒ–ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    degradation = step / 200 * 0.3
    next_state, reward, done = env.step(action[0])
    reward -= degradation * 10  # æ€§èƒ½ä½ä¸‹

    monitor.update(state, action[0], reward)

    # ç•°å¸¸æ¤œå‡º
    if monitor.detect_anomaly(state, action[0]):
        print(f"Step {step+100}: Anomaly detected!")

    # ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º
    if drift_detector.add_element(reward):
        print(f"Step {step+100}: DRIFT DETECTED by ADWIN!")

    state = next_state if not done else env.reset()

    # å®šæœŸçš„ãªãƒ‰ãƒªãƒ•ãƒˆãƒã‚§ãƒƒã‚¯
    if step % 50 == 0:
        drift, info = monitor.detect_drift()
        if drift:
            print(f"\nStep {step+100}: Statistical drift detected!")
            print(f"  Current: {info['current_mean']:.2f}, "
                  f"Baseline: {info['baseline_mean']:.2f}")

# æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
print(monitor.generate_report())</code></pre>
                </div>
            </section>

            <section id="example7">
                <div class="example-box">
                    <div class="example-title">
                        <span class="example-number">7</span>
                        çµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
                    </div>
                    <p>
                        ã“ã‚Œã¾ã§ã®å…¨è¦ç´ ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒ€ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã€å®‰å…¨åˆ¶ç´„ã€äººé–“ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã€
                        ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–ã€æ€§èƒ½ç›£è¦–ï¼‰ã‚’çµ±åˆã—ãŸå®Ÿè·µçš„ãªãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚
                    </p>

                    <div class="mermaid">
                    graph TD
                        A[ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿] --> B[å‰å‡¦ç†]
                        B --> C[ä¸ç¢ºå®Ÿæ€§æ¨å®š]
                        C --> D{ä¸ç¢ºå®Ÿæ€§é«˜?}
                        D -->|Yes| E[ä¿å®ˆçš„è¡Œå‹•]
                        D -->|No| F[AIæ–¹ç­–]
                        F --> G[å®‰å…¨ãƒ•ã‚£ãƒ«ã‚¿]
                        E --> G
                        G --> H{å®‰å…¨?}
                        H -->|No| I[äººé–“ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰]
                        H -->|Yes| J[ã‚¢ã‚¯ãƒãƒ¥ã‚¨ãƒ¼ã‚¿]
                        I --> J
                        J --> K[æ€§èƒ½ç›£è¦–]
                        K --> L{ãƒ‰ãƒªãƒ•ãƒˆ?}
                        L -->|Yes| M[ã‚¢ãƒ©ãƒ¼ãƒˆ+å†å­¦ç¿’]
                        L -->|No| A
                    </div>

                    <pre><code class="language-python"># çµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
import numpy as np
import torch
from datetime import datetime

class IntegratedDeploymentSystem:
    """å…¨æ©Ÿèƒ½çµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ """
    def __init__(self, agent, env):
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.agent = agent
        self.env = env

        # ä¾‹2: å®‰å…¨åˆ¶ç´„
        from example2 import SafetyConstraints, ControlBarrierFunction, SimpleCSTRModel
        self.safety = SafetyConstraints()
        self.cbf = ControlBarrierFunction(self.safety)
        self.model = SimpleCSTRModel()

        # ä¾‹4: äººé–“ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
        from example4 import HumanOverrideSystem, ControlMode
        self.override_system = HumanOverrideSystem()

        # ä¾‹5: ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–
        from example5 import EnsembleQNetwork
        self.uncertainty_estimator = EnsembleQNetwork(
            state_dim=2, action_dim=1, n_models=5)

        # ä¾‹6: æ€§èƒ½ç›£è¦–
        from example6 import PerformanceMonitor, DriftDetector
        self.monitor = PerformanceMonitor(window_size=100)
        self.drift_detector = DriftDetector()

        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
        self.prev_action = 0.0
        self.running = True
        self.emergency_stop = False

    def preprocess_observation(self, raw_obs):
        """ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†"""
        # å¤–ã‚Œå€¤é™¤å»ï¼ˆç°¡æ˜“ï¼‰
        T, CA = raw_obs
        T = np.clip(T, 250, 450)
        CA = np.clip(CA, 0, 5)
        return np.array([T, CA])

    def estimate_uncertainty(self, state, action):
        """ä¸ç¢ºå®Ÿæ€§æ¨å®š"""
        q_mean, q_std = self.uncertainty_estimator.predict_with_uncertainty(
            state, action)
        return q_std

    def apply_safety_filter(self, state, action):
        """å®‰å…¨ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨"""
        # åˆ¶ç´„å°„å½±
        safe_action = self.safety.project_to_safe(action, self.prev_action)

        # CBFåˆ¶ç´„
        safe_action = self.cbf.safe_action(state, safe_action, self.model)

        return safe_action

    def check_human_override(self, state, action, uncertainty):
        """äººé–“ä»‹å…¥ãƒã‚§ãƒƒã‚¯"""
        need_intervention, reasons = self.override_system.check_intervention_needed(
            state, action, confidence=1.0 - uncertainty)

        if need_intervention:
            human_action, overridden = self.override_system.request_human_action(
                state, action)

            if overridden:
                self.override_system.log_intervention(
                    datetime.now(), state, action, human_action, reasons)
                return human_action, True

        return action, False

    def monitor_performance(self, state, action, reward):
        """æ€§èƒ½ç›£è¦–ã¨ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º"""
        self.monitor.update(state, action, reward)

        # ç•°å¸¸æ¤œå‡º
        if self.monitor.detect_anomaly(state, action):
            print(f"  [MONITOR] Anomaly detected at step {self.monitor.total_steps}")

        # ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º
        if self.drift_detector.add_element(reward):
            print(f"  [MONITOR] Performance drift detected!")
            return True  # å†å­¦ç¿’ãƒˆãƒªã‚¬ãƒ¼

        # å®šæœŸçš„ãªçµ±è¨ˆçš„ãƒ‰ãƒªãƒ•ãƒˆãƒã‚§ãƒƒã‚¯
        if self.monitor.total_steps % 100 == 0:
            drift, info = self.monitor.detect_drift()
            if drift:
                print(f"  [MONITOR] Statistical drift: "
                      f"current={info['current_mean']:.2f}, "
                      f"baseline={info['baseline_mean']:.2f}")
                return True

        return False

    def control_loop(self, n_steps=500):
        """ãƒ¡ã‚¤ãƒ³åˆ¶å¾¡ãƒ«ãƒ¼ãƒ—"""
        print("çµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ èµ·å‹•\n")
        print("="*60)

        state = self.env.reset()
        state = self.preprocess_observation(state)

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¨­å®š
        for step in range(100):
            action = self.agent.select_action(state)[0]
            next_state, reward, done = self.env.step(action)
            next_state = self.preprocess_observation(next_state)

            self.monitor.update(state, action, reward)
            state = next_state if not done else self.env.reset()

        self.monitor.set_baseline()
        print("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¨­å®šå®Œäº†\n")

        # ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
        for step in range(n_steps):
            if self.emergency_stop:
                print("ç·Šæ€¥åœæ­¢ï¼")
                break

            # 1. AIæ–¹ç­–
            raw_action = self.agent.select_action(state)[0]

            # 2. ä¸ç¢ºå®Ÿæ€§æ¨å®š
            uncertainty = self.estimate_uncertainty(state, np.array([raw_action]))

            # é«˜ä¸ç¢ºå®Ÿæ€§æ™‚ã¯ä¿å®ˆçš„ã«
            if uncertainty > 0.5:
                raw_action *= 0.5
                print(f"  [UNCERTAINTY] High uncertainty ({uncertainty:.3f}), "
                      f"conservative action")

            # 3. å®‰å…¨ãƒ•ã‚£ãƒ«ã‚¿
            safe_action = self.apply_safety_filter(state, raw_action)

            # 4. äººé–“ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
            final_action, overridden = self.check_human_override(
                state, safe_action, uncertainty)

            # 5. å®Ÿè¡Œ
            next_state, reward, done = self.env.step(final_action)
            next_state = self.preprocess_observation(next_state)

            # 6. æ€§èƒ½ç›£è¦–
            need_retraining = self.monitor_performance(state, final_action, reward)

            if need_retraining:
                print(f"  [SYSTEM] å†å­¦ç¿’ãŒæ¨å¥¨ã•ã‚Œã¾ã™")

            # å®šæœŸãƒ¬ãƒãƒ¼ãƒˆ
            if step % 100 == 0:
                print(f"\nStep {step}:")
                print(f"  State: T={state[0]:.2f}K, CA={state[1]:.3f}")
                print(f"  Action: {final_action:.3f}, Uncertainty: {uncertainty:.3f}")
                print(f"  Mode: {self.override_system.mode.value}")
                print(f"  Anomalies: {self.monitor.anomaly_count}")

            self.prev_action = final_action
            state = next_state if not done else self.env.reset()

        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        print("\n" + "="*60)
        print("é‹è»¢çµ‚äº†")
        print("="*60)
        print(self.monitor.generate_report())
        print(self.override_system.generate_report())

# å®Ÿè¡Œ
from example1 import RobustSACAgent, RandomizedCSTREnv

print("çµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n")

env = RandomizedCSTREnv(randomize=True)
agent = RobustSACAgent(obs_dim=2, action_dim=1)

system = IntegratedDeploymentSystem(agent, env)
system.control_loop(n_steps=300)</code></pre>
                </div>
            </section>

            <section id="summary">
                <h2>Chapter 5 ã¾ã¨ã‚</h2>

                <div class="key-points">
                    <h3>å­¦ã‚“ã ã“ã¨</h3>
                    <ul>
                        <li><strong>Sim-to-Realè»¢ç§»</strong>ï¼šãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒ€ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã§ãƒ­ãƒã‚¹ãƒˆæ€§ã‚’ç²å¾—</li>
                        <li><strong>å®‰å…¨ãªæ¢ç´¢</strong>ï¼šã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿ã¨CBFã§å±é™ºå›é¿</li>
                        <li><strong>ä¿å®ˆçš„Qå­¦ç¿’</strong>ï¼šã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®‰å…¨ã«å­¦ç¿’</li>
                        <li><strong>äººé–“ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰</strong>ï¼šç·Šæ€¥æ™‚ã®ä»‹å…¥æ©Ÿæ§‹</li>
                        <li><strong>ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–</strong>ï¼šã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚„MCDropoutã§ä¿¡é ¼åº¦è©•ä¾¡</li>
                        <li><strong>æ€§èƒ½ç›£è¦–</strong>ï¼šãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã¨ç•°å¸¸æ¤œçŸ¥</li>
                        <li><strong>çµ±åˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯</strong>ï¼šå…¨è¦ç´ ã‚’çµ„ã¿åˆã‚ã›ãŸå®Ÿç”¨ã‚·ã‚¹ãƒ†ãƒ </li>
                    </ul>
                </div>

                <h3>ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæˆç†Ÿåº¦ãƒ¢ãƒ‡ãƒ«</h3>
                <table style="width: 100%; border-collapse: collapse; margin: 1.5rem 0;">
                    <thead style="background: var(--pi-primary); color: white;">
                        <tr>
                            <th style="padding: 0.75rem; border: 1px solid #ddd;">ãƒ¬ãƒ™ãƒ«</th>
                            <th style="padding: 0.75rem; border: 1px solid #ddd;">èª¬æ˜</th>
                            <th style="padding: 0.75rem; border: 1px solid #ddd;">å¿…è¦æŠ€è¡“</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">L1: å®Ÿé¨“å®¤</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ã¿</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">åŸºæœ¬RL</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">L2: ãƒ†ã‚¹ãƒˆ</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆãƒ—ãƒ©ãƒ³ãƒˆ</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">Sim-to-realã€å®‰å…¨åˆ¶ç´„</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">L3: ç›£è¦–ä»˜ã</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">å®Ÿãƒ—ãƒ©ãƒ³ãƒˆï¼ˆäººé–“ç›£è¦–ï¼‰</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã€æ€§èƒ½ç›£è¦–</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">L4: è‡ªå¾‹</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">å®Œå…¨è‡ªå¾‹é‹è»¢</td>
                            <td style="padding: 0.75rem; border: 1px solid #ddd;">å…¨æ©Ÿèƒ½çµ±åˆã€ç¶™ç¶šå­¦ç¿’</td>
                        </tr>
                    </tbody>
                </table>

                <h3>å®Ÿãƒ—ãƒ©ãƒ³ãƒˆé©ç”¨ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ</h3>
                <ul>
                    <li>âœ“ ååˆ†ãªã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¤œè¨¼ï¼ˆ1000+ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰</li>
                    <li>âœ“ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ©ãƒ³ãƒ€ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã®å¦¥å½“æ€§ç¢ºèª</li>
                    <li>âœ“ å®‰å…¨åˆ¶ç´„ã®ç¶²ç¾…çš„å®šç¾©</li>
                    <li>âœ“ ç·Šæ€¥åœæ­¢ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®å®Ÿè£…ã¨ãƒ†ã‚¹ãƒˆ</li>
                    <li>âœ“ ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼è¨“ç·´ã¨ãƒãƒ‹ãƒ¥ã‚¢ãƒ«æ•´å‚™</li>
                    <li>âœ“ æ€§èƒ½ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ§‹ç¯‰</li>
                    <li>âœ“ å®šæœŸçš„ãªæ€§èƒ½è©•ä¾¡ã¨å†å­¦ç¿’è¨ˆç”»</li>
                    <li>âœ“ è¦åˆ¶å½“å±€ã¸ã®å ±å‘Šä½“åˆ¶</li>
                </ul>

                <div class="warning-box">
                    <h4>æœ€çµ‚æ³¨æ„äº‹é …</h4>
                    <p>
                        å¼·åŒ–å­¦ç¿’ã®å®Ÿãƒ—ãƒ©ãƒ³ãƒˆé©ç”¨ã¯ã€ã¾ã ç™ºå±•é€”ä¸Šã®æŠ€è¡“ã§ã™ã€‚
                        ç‰¹ã«åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆã®ã‚ˆã†ãªå®‰å…¨ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªç’°å¢ƒã§ã¯ã€æ®µéšçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒä¸å¯æ¬ ã§ã™ï¼š
                    </p>
                    <ol>
                        <li>ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®å¾¹åº•æ¤œè¨¼</li>
                        <li>ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆãƒ—ãƒ©ãƒ³ãƒˆã§ã®å®Ÿè¨¼</li>
                        <li>äººé–“ç›£è¦–ä¸‹ã§ã®é™å®šé‹ç”¨</li>
                        <li>æ®µéšçš„ãªè‡ªå¾‹ãƒ¬ãƒ™ãƒ«å‘ä¸Š</li>
                    </ol>
                    <p>
                        å¸¸ã«äººé–“ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã¨å”åƒã—ã€AIã‚’é“å…·ã¨ã—ã¦æ´»ç”¨ã™ã‚‹å§¿å‹¢ãŒé‡è¦ã§ã™ã€‚
                    </p>
                </div>
            </section>

            <div style="margin-top: 3rem; padding: 2rem; background: linear-gradient(135deg, rgba(17, 153, 142, 0.1) 0%, rgba(56, 239, 125, 0.1) 100%); border-radius: 8px;">
                <h3 style="color: var(--pi-primary); margin-top: 0;">ã‚·ãƒªãƒ¼ã‚ºå®Œäº†</h3>
                <p><strong>ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼</strong></p>
                <p>
                    ã€ŒAIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹è‡ªå¾‹ãƒ—ãƒ­ã‚»ã‚¹é‹è»¢ã€ã‚·ãƒªãƒ¼ã‚ºã®å…¨5ç« ã‚’ä¿®äº†ã—ã¾ã—ãŸã€‚
                    å¼·åŒ–å­¦ç¿’ã®åŸºç¤ã‹ã‚‰å®Ÿãƒ—ãƒ©ãƒ³ãƒˆãƒ‡ãƒ—ãƒ­ã‚¤ã¾ã§ã€å¹…åºƒã„çŸ¥è­˜ã‚’ç¿’å¾—ã•ã‚Œã¾ã—ãŸã€‚
                </p>
                <p>
                    <a href="index.html" style="color: var(--pi-primary);">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
                </p>
                <p>
                    ã•ã‚‰ãªã‚‹å­¦ç¿’ã«ã¯ã€ä»¥ä¸‹ã®ã‚·ãƒªãƒ¼ã‚ºã‚‚ã”æ´»ç”¨ãã ã•ã„ï¼š
                </p>
                <ul>
                    <li><a href="../process-monitoring/index.html" style="color: var(--pi-primary);">ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ãƒ»åˆ¶å¾¡å…¥é–€</a></li>
                    <li><a href="../process-optimization/index.html" style="color: var(--pi-primary);">ãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ–å…¥é–€</a></li>
                </ul>
            </div>
        </article>
    <section class="disclaimer">
<h3>å…è²¬äº‹é …</h3>
<ul>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹Code examplesã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
<li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
</ul>
</section>

</main>

    <footer>
        <div class="footer-content">
            <p>&copy; 2025 Hashimoto Laboratory, Tohoku University. All rights reserved.</p>
        </div>
    </footer>

    <!-- Prism.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>

    <!-- Mermaid -->
    <script>
        mermaid.initialize({
            startOnLoad: true,
            theme: 'default',
            themeVariables: {
                primaryColor: '#11998e',
                primaryTextColor: '#fff',
                primaryBorderColor: '#0e7c74',
                lineColor: '#11998e',
                secondaryColor: '#38ef7d',
                tertiaryColor: '#f0f0f0'
            }
        });
    </script>
</body>
</html>
