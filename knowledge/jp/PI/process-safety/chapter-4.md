---
title: ç¬¬4ç« ï¼šå®‰å…¨æ€§ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨äº‹æ•…äºˆæ¸¬
chapter_title: ç¬¬4ç« ï¼šå®‰å…¨æ€§ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨äº‹æ•…äºˆæ¸¬
subtitle: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã¨æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹äºˆæ¸¬çš„å®‰å…¨ç®¡ç†
---

ğŸŒ JP | [ğŸ‡¬ğŸ‡§ EN](<../../../en/PI/process-safety/chapter-4.html>) | Last sync: 2025-11-16

[AIå¯ºå­å±‹ãƒˆãƒƒãƒ—](<../../index.html>)â€º[ãƒ—ãƒ­ã‚»ã‚¹ãƒ»ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹](<../../PI/index.html>)â€º[Process Safety](<../../PI/process-safety/index.html>)â€ºChapter 4

## ã“ã®ç« ã§å­¦ã¶ã“ã¨

  * **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®‰å…¨æ€§ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°** ã®å®Ÿè£…æ–¹æ³•
  * **ç•°å¸¸çŠ¶æ³æ¤œçŸ¥** ï¼ˆå¤šå¤‰é‡çµ±è¨ˆçš„ãƒ—ãƒ­ã‚»ã‚¹ç®¡ç†ï¼‰
  * **å…ˆè¡ŒæŒ‡æ¨™ï¼ˆLeading Indicatorsï¼‰** ã«ã‚ˆã‚‹äºˆå…†ç®¡ç†
  * **æ©Ÿæ¢°å­¦ç¿’** ã‚’ç”¨ã„ãŸäº‹æ•…äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
  * **å®‰å…¨æ€§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™** ï¼ˆTRIRã€LTIRï¼‰ã®è¨ˆç®—
  * ãƒ‹ã‚¢ãƒŸã‚¹è§£æã¨ãƒ’ãƒ¤ãƒªãƒãƒƒãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 

## 4.1 ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®‰å…¨æ€§ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

ãƒ—ãƒ­ã‚»ã‚¹å®‰å…¨æ€§ã®ç¢ºä¿ã«ã¯ã€ãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°ã®é€£ç¶šç›£è¦–ã¨ç•°å¸¸ã®æ—©æœŸæ¤œçŸ¥ãŒä¸å¯æ¬ ã§ã™ã€‚ æ¸©åº¦ã€åœ§åŠ›ã€æµé‡ãªã©ã®é‡è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ç›£è¦–ã—ã€ å®‰å…¨é™ç•Œã¸ã®æ¥è¿‘ã‚’æ¤œçŸ¥ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚ 

### 4.1.1 å®‰å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
    
    
    # Example 1: Real-time Safety Parameter Monitoring
    import numpy as np
    import pandas as pd
    from dataclasses import dataclass
    from typing import Dict, List, Optional
    from datetime import datetime, timedelta
    from enum import Enum
    
    class SafetyLevel(Enum):
        """å®‰å…¨ãƒ¬ãƒ™ãƒ«"""
        NORMAL = "Normal"
        CAUTION = "Caution"
        WARNING = "Warning"
        CRITICAL = "Critical"
    
    @dataclass
    class SafetyLimit:
        """å®‰å…¨é™ç•Œå€¤"""
        low_critical: float      # ä¸‹é™Critical
        low_warning: float       # ä¸‹é™Warning
        low_caution: float       # ä¸‹é™Caution
        high_caution: float      # ä¸Šé™Caution
        high_warning: float      # ä¸Šé™Warning
        high_critical: float     # ä¸Šé™Critical
    
    class SafetyMonitor:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®‰å…¨æ€§ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
    
        ãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°ã‚’é€£ç¶šç›£è¦–ã—ã€å®‰å…¨é™ç•Œã¸ã®æ¥è¿‘ã‚’æ¤œçŸ¥
        """
    
        def __init__(self):
            self.limits: Dict[str, SafetyLimit] = {}
            self.current_values: Dict[str, float] = {}
            self.alert_history: List[Dict] = []
    
        def set_safety_limits(self, parameter: str, limits: SafetyLimit):
            """å®‰å…¨é™ç•Œã‚’è¨­å®š"""
            self.limits[parameter] = limits
    
        def update_value(self, parameter: str, value: float,
                        timestamp: datetime = None) -> Dict:
            """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤ã‚’æ›´æ–°ã—ã€å®‰å…¨æ€§ã‚’è©•ä¾¡
    
            Args:
                parameter: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å
                value: æ¸¬å®šå€¤
                timestamp: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    
            Returns:
                è©•ä¾¡çµæœï¼ˆãƒ¬ãƒ™ãƒ«ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰
            """
            if timestamp is None:
                timestamp = datetime.now()
    
            self.current_values[parameter] = value
    
            # å®‰å…¨ãƒ¬ãƒ™ãƒ«åˆ¤å®š
            safety_assessment = self._assess_safety(parameter, value)
    
            # ã‚¢ãƒ©ãƒ¼ãƒˆå±¥æ­´ã«è¨˜éŒ²
            if safety_assessment['level'] != SafetyLevel.NORMAL:
                self.alert_history.append({
                    'timestamp': timestamp,
                    'parameter': parameter,
                    'value': value,
                    'level': safety_assessment['level'],
                    'message': safety_assessment['message']
                })
    
            return safety_assessment
    
        def _assess_safety(self, parameter: str, value: float) -> Dict:
            """å®‰å…¨æ€§ã‚’è©•ä¾¡"""
            if parameter not in self.limits:
                return {
                    'level': SafetyLevel.NORMAL,
                    'message': 'No limits defined',
                    'distance_to_limit': None
                }
    
            limits = self.limits[parameter]
    
            # Criticalç¯„å›²
            if value <= limits.low_critical or value >= limits.high_critical:
                level = SafetyLevel.CRITICAL
                message = "Critical limit exceeded! Immediate action required."
                distance = min(
                    abs(value - limits.low_critical),
                    abs(value - limits.high_critical)
                )
    
            # Warningç¯„å›²
            elif value <= limits.low_warning or value >= limits.high_warning:
                level = SafetyLevel.WARNING
                message = "Warning: Approaching critical limit"
                distance = min(
                    abs(value - limits.low_critical),
                    abs(value - limits.high_critical)
                )
    
            # Cautionç¯„å›²
            elif value <= limits.low_caution or value >= limits.high_caution:
                level = SafetyLevel.CAUTION
                message = "Caution: Monitor closely"
                distance = min(
                    abs(value - limits.low_critical),
                    abs(value - limits.high_critical)
                )
    
            # Normalç¯„å›²
            else:
                level = SafetyLevel.NORMAL
                message = "Normal operation"
                distance = min(
                    abs(value - limits.low_critical),
                    abs(value - limits.high_critical)
                )
    
            return {
                'level': level,
                'message': message,
                'distance_to_limit': distance,
                'value': value
            }
    
        def get_current_status(self) -> pd.DataFrame:
            """ç¾åœ¨ã®å®‰å…¨æ€§ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—"""
            status_data = []
    
            for param, value in self.current_values.items():
                assessment = self._assess_safety(param, value)
                status_data.append({
                    'Parameter': param,
                    'Current Value': value,
                    'Safety Level': assessment['level'].value,
                    'Distance to Limit': assessment['distance_to_limit'],
                    'Message': assessment['message']
                })
    
            df = pd.DataFrame(status_data)
            return df.sort_values('Distance to Limit')
    
        def get_alert_summary(self, hours: int = 24) -> pd.DataFrame:
            """æŒ‡å®šæœŸé–“ã®ã‚¢ãƒ©ãƒ¼ãƒˆçµ±è¨ˆ"""
            if not self.alert_history:
                return pd.DataFrame()
    
            df = pd.DataFrame(self.alert_history)
            df['timestamp'] = pd.to_datetime(df['timestamp'])
    
            # æŒ‡å®šæœŸé–“ã§ãƒ•ã‚£ãƒ«ã‚¿
            cutoff_time = datetime.now() - timedelta(hours=hours)
            df_recent = df[df['timestamp'] >= cutoff_time]
    
            # ãƒ¬ãƒ™ãƒ«åˆ¥é›†è¨ˆ
            summary = df_recent.groupby(['parameter', 'level']).size().unstack(fill_value=0)
    
            return summary
    
    # å®Ÿä½¿ç”¨ä¾‹ï¼šåŒ–å­¦åå¿œå™¨ã®ç›£è¦–
    monitor = SafetyMonitor()
    
    # åå¿œå™¨æ¸©åº¦ã®å®‰å…¨é™ç•Œè¨­å®š
    monitor.set_safety_limits(
        'Reactor Temperature',
        SafetyLimit(
            low_critical=50,    # 50â„ƒæœªæº€ã§Critical
            low_warning=60,
            low_caution=70,
            high_caution=180,
            high_warning=190,
            high_critical=200   # 200â„ƒè¶…ã§Critical
        )
    )
    
    # åå¿œå™¨åœ§åŠ›ã®å®‰å…¨é™ç•Œè¨­å®š
    monitor.set_safety_limits(
        'Reactor Pressure',
        SafetyLimit(
            low_critical=0.5,   # 0.5 MPaæœªæº€ã§Critical
            low_warning=1.0,
            low_caution=1.5,
            high_caution=8.0,
            high_warning=9.0,
            high_critical=10.0  # 10 MPaè¶…ã§Critical
        )
    )
    
    # å†·å´æ°´æµé‡ã®å®‰å…¨é™ç•Œè¨­å®š
    monitor.set_safety_limits(
        'Cooling Water Flow',
        SafetyLimit(
            low_critical=50,    # 50 L/minæœªæº€ã§Critical
            low_warning=70,
            low_caution=90,
            high_caution=500,
            high_warning=550,
            high_critical=600
        )
    )
    
    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¸¬å®šå€¤ã®æ›´æ–°ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
    np.random.seed(42)
    
    # æ­£å¸¸é‹è»¢
    print("=== æ­£å¸¸é‹è»¢æ™‚ ===")
    monitor.update_value('Reactor Temperature', 150)
    monitor.update_value('Reactor Pressure', 5.0)
    monitor.update_value('Cooling Water Flow', 200)
    
    status = monitor.get_current_status()
    print(status.to_string(index=False))
    
    # ç•°å¸¸çŠ¶æ…‹ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    print("\n=== ç•°å¸¸çŠ¶æ…‹ï¼ˆæ¸©åº¦ä¸Šæ˜‡ï¼‰ ===")
    result = monitor.update_value('Reactor Temperature', 195)
    print(f"Temperature 195â„ƒ: {result['level'].value} - {result['message']}")
    
    result = monitor.update_value('Reactor Temperature', 205)
    print(f"Temperature 205â„ƒ: {result['level'].value} - {result['message']}")
    
    # åœ§åŠ›ç•°å¸¸
    print("\n=== ç•°å¸¸çŠ¶æ…‹ï¼ˆåœ§åŠ›ä¸Šæ˜‡ï¼‰ ===")
    result = monitor.update_value('Reactor Pressure', 9.5)
    print(f"Pressure 9.5 MPa: {result['level'].value} - {result['message']}")
    
    # å†·å´æ°´æµé‡ç•°å¸¸
    print("\n=== ç•°å¸¸çŠ¶æ…‹ï¼ˆå†·å´æ°´æµé‡ä½ä¸‹ï¼‰ ===")
    result = monitor.update_value('Cooling Water Flow', 65)
    print(f"Flow 65 L/min: {result['level'].value} - {result['message']}")
    
    # ã‚¢ãƒ©ãƒ¼ãƒˆã‚µãƒãƒªãƒ¼
    print("\n=== ã‚¢ãƒ©ãƒ¼ãƒˆã‚µãƒãƒªãƒ¼ï¼ˆ24æ™‚é–“ï¼‰ ===")
    alert_summary = monitor.get_alert_summary(hours=24)
    if not alert_summary.empty:
        print(alert_summary)
    
    # å‡ºåŠ›ä¾‹:
    # === æ­£å¸¸é‹è»¢æ™‚ ===
    # Parameter             Current Value Safety Level  Distance to Limit                    Message
    # Cooling Water Flow            200.0       Normal               250.0         Normal operation
    # Reactor Temperature           150.0       Normal                50.0         Normal operation
    # Reactor Pressure                5.0       Normal                 5.0         Normal operation
    #
    # === ç•°å¸¸çŠ¶æ…‹ï¼ˆæ¸©åº¦ä¸Šæ˜‡ï¼‰ ===
    # Temperature 195â„ƒ: Warning - Warning: Approaching critical limit
    # Temperature 205â„ƒ: Critical - Critical limit exceeded! Immediate action required.
    

### 4.1.2 å¤šå¤‰é‡ç•°å¸¸æ¤œçŸ¥ï¼ˆMSPCï¼‰

å¤šå¤‰é‡çµ±è¨ˆçš„ãƒ—ãƒ­ã‚»ã‚¹ç®¡ç†ï¼ˆMSPC: Multivariate Statistical Process Controlï¼‰ã¯ã€ è¤‡æ•°ã®ãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°ã‚’åŒæ™‚ã«ç›£è¦–ã—ã€å¤‰æ•°é–“ã®ç›¸é–¢é–¢ä¿‚ã‚’è€ƒæ…®ã—ãŸç•°å¸¸æ¤œçŸ¥ã‚’è¡Œã„ã¾ã™ã€‚ ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ãƒ™ãƒ¼ã‚¹ã®TÂ²çµ±è¨ˆé‡ã¨Qçµ±è¨ˆé‡ã‚’ç”¨ã„ã¦ç•°å¸¸ã‚’æ¤œå‡ºã—ã¾ã™ã€‚ 
    
    
    # Example 2: Multivariate Anomaly Detection (MSPC)
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import StandardScaler
    import scipy.stats as stats
    
    class MSPCMonitor:
        """å¤šå¤‰é‡çµ±è¨ˆçš„ãƒ—ãƒ­ã‚»ã‚¹ç®¡ç†ï¼ˆMSPCï¼‰
    
        PCAãƒ™ãƒ¼ã‚¹ã®TÂ²çµ±è¨ˆé‡ã¨SPEï¼ˆQçµ±è¨ˆé‡ï¼‰ã§ç•°å¸¸æ¤œçŸ¥
        """
    
        def __init__(self, n_components: int = None, alpha: float = 0.01):
            """
            Args:
                n_components: ä¸»æˆåˆ†æ•°ï¼ˆNoneã§è‡ªå‹•æ±ºå®šï¼‰
                alpha: æœ‰æ„æ°´æº–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1%ï¼‰
            """
            self.n_components = n_components
            self.alpha = alpha
            self.scaler = StandardScaler()
            self.pca = None
            self.t2_limit = None
            self.spe_limit = None
            self.is_trained = False
    
        def train(self, normal_data: np.ndarray):
            """æ­£å¸¸é‹è»¢ãƒ‡ãƒ¼ã‚¿ã§çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰
    
            Args:
                normal_data: æ­£å¸¸é‹è»¢æ™‚ã®ãƒ‡ãƒ¼ã‚¿ (n_samples, n_features)
            """
            # ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
            X_scaled = self.scaler.fit_transform(normal_data)
    
            # PCAãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
            if self.n_components is None:
                # ç´¯ç©å¯„ä¸ç‡90%ä»¥ä¸Šã¨ãªã‚‹æˆåˆ†æ•°ã‚’è‡ªå‹•é¸æŠ
                pca_temp = PCA()
                pca_temp.fit(X_scaled)
                cumsum = np.cumsum(pca_temp.explained_variance_ratio_)
                self.n_components = np.argmax(cumsum >= 0.90) + 1
    
            self.pca = PCA(n_components=self.n_components)
            T = self.pca.fit_transform(X_scaled)
    
            # TÂ²çµ±è¨ˆé‡ã®ç®¡ç†é™ç•Œï¼ˆFåˆ†å¸ƒï¼‰
            n, p = normal_data.shape
            a = self.n_components
            f_value = stats.f.ppf(1 - self.alpha, a, n - a)
            self.t2_limit = (a * (n - 1) * (n + 1)) / (n * (n - a)) * f_value
    
            # SPEï¼ˆQçµ±è¨ˆé‡ï¼‰ã®ç®¡ç†é™ç•Œ
            residuals = X_scaled - self.pca.inverse_transform(T)
            spe_values = np.sum(residuals ** 2, axis=1)
    
            # SPEåˆ†å¸ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š
            theta1 = np.mean(spe_values)
            theta2 = np.var(spe_values)
            theta3 = np.mean((spe_values - theta1) ** 3)
    
            h0 = 1 - (2 * theta1 * theta3) / (3 * theta2 ** 2)
            ca = stats.norm.ppf(1 - self.alpha)
    
            self.spe_limit = theta1 * (
                1 + (ca * np.sqrt(2 * theta2) * h0) / theta1 +
                (theta2 * h0 * (h0 - 1)) / theta1 ** 2
            ) ** (1 / h0)
    
            self.is_trained = True
    
            return {
                'n_components': self.n_components,
                'variance_explained': self.pca.explained_variance_ratio_.sum(),
                't2_limit': self.t2_limit,
                'spe_limit': self.spe_limit
            }
    
        def detect(self, new_data: np.ndarray) -> Dict:
            """æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸æ¤œçŸ¥
    
            Args:
                new_data: ç›£è¦–å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ (n_samples, n_features)
    
            Returns:
                ç•°å¸¸æ¤œçŸ¥çµæœ
            """
            if not self.is_trained:
                raise ValueError("Model not trained. Call train() first.")
    
            # ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
            X_scaled = self.scaler.transform(new_data)
    
            # TÂ²çµ±è¨ˆé‡è¨ˆç®—
            T = self.pca.transform(X_scaled)
            t2_values = np.sum(
                (T ** 2) / self.pca.explained_variance_, axis=1
            )
    
            # SPEè¨ˆç®—
            residuals = X_scaled - self.pca.inverse_transform(T)
            spe_values = np.sum(residuals ** 2, axis=1)
    
            # ç•°å¸¸åˆ¤å®š
            t2_anomaly = t2_values > self.t2_limit
            spe_anomaly = spe_values > self.spe_limit
            any_anomaly = t2_anomaly | spe_anomaly
    
            return {
                't2': t2_values,
                't2_limit': self.t2_limit,
                't2_anomaly': t2_anomaly,
                'spe': spe_values,
                'spe_limit': self.spe_limit,
                'spe_anomaly': spe_anomaly,
                'anomaly': any_anomaly,
                'anomaly_rate': any_anomaly.mean()
            }
    
    # å®Ÿä½¿ç”¨ä¾‹ï¼šåå¿œå™¨ã®å¤šå¤‰é‡ç›£è¦–
    np.random.seed(42)
    
    # æ­£å¸¸é‹è»¢ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆç›¸é–¢ã®ã‚ã‚‹5å¤‰æ•°ï¼‰
    n_samples = 500
    mean_normal = [150, 5.0, 200, 80, 0.85]  # æ¸©åº¦ã€åœ§åŠ›ã€æµé‡ã€è»¢åŒ–ç‡ã€ç´”åº¦
    cov_normal = np.array([
        [25, 0.5, 5, 2, 0.01],
        [0.5, 0.04, 0.1, 0.02, 0.001],
        [5, 0.1, 100, 10, 0.1],
        [2, 0.02, 10, 4, 0.05],
        [0.01, 0.001, 0.1, 0.05, 0.0025]
    ])
    
    normal_data = np.random.multivariate_normal(mean_normal, cov_normal, n_samples)
    
    # MSPCãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    mspc = MSPCMonitor(alpha=0.01)
    train_result = mspc.train(normal_data)
    
    print("=== MSPC ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ ===")
    print(f"ä¸»æˆåˆ†æ•°: {train_result['n_components']}")
    print(f"ç´¯ç©å¯„ä¸ç‡: {train_result['variance_explained']:.2%}")
    print(f"TÂ² ç®¡ç†é™ç•Œ: {train_result['t2_limit']:.2f}")
    print(f"SPE ç®¡ç†é™ç•Œ: {train_result['spe_limit']:.2f}")
    
    # æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã§ã®æ¤œè¨¼
    print("\n=== æ­£å¸¸ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ ===")
    normal_test = np.random.multivariate_normal(mean_normal, cov_normal, 100)
    result_normal = mspc.detect(normal_test)
    print(f"ç•°å¸¸ç‡: {result_normal['anomaly_rate']*100:.1f}% "
          f"(æœŸå¾…å€¤: {mspc.alpha*100:.1f}%)")
    
    # ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã§ã®æ¤œè¨¼ï¼ˆæ¸©åº¦ç•°å¸¸ï¼‰
    print("\n=== ç•°å¸¸ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ï¼ˆæ¸©åº¦+20â„ƒä¸Šæ˜‡ï¼‰ ===")
    mean_abnormal = [170, 5.5, 200, 75, 0.82]  # æ¸©åº¦ä¸Šæ˜‡ã€åœ§åŠ›ä¸Šæ˜‡ã€è»¢åŒ–ç‡ä½ä¸‹
    abnormal_data = np.random.multivariate_normal(mean_abnormal, cov_normal, 50)
    result_abnormal = mspc.detect(abnormal_data)
    
    print(f"ç•°å¸¸ç‡: {result_abnormal['anomaly_rate']*100:.1f}%")
    print(f"TÂ²ç•°å¸¸æ¤œçŸ¥: {result_abnormal['t2_anomaly'].sum()}ä»¶")
    print(f"SPEç•°å¸¸æ¤œçŸ¥: {result_abnormal['spe_anomaly'].sum()}ä»¶")
    
    # è©³ç´°åˆ†æ
    anomaly_indices = np.where(result_abnormal['anomaly'])[0]
    if len(anomaly_indices) > 0:
        print(f"\nç•°å¸¸ã‚µãƒ³ãƒ—ãƒ«ä¾‹ï¼ˆæœ€åˆã®5ä»¶ï¼‰:")
        for i in anomaly_indices[:5]:
            print(f"  Sample {i}: TÂ²={result_abnormal['t2'][i]:.2f} "
                  f"(limit={mspc.t2_limit:.2f}), "
                  f"SPE={result_abnormal['spe'][i]:.2f} "
                  f"(limit={mspc.spe_limit:.2f})")
    
    # å‡ºåŠ›ä¾‹:
    # === MSPC ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ ===
    # ä¸»æˆåˆ†æ•°: 3
    # ç´¯ç©å¯„ä¸ç‡: 91.23%
    # TÂ² ç®¡ç†é™ç•Œ: 11.34
    # SPE ç®¡ç†é™ç•Œ: 8.76
    #
    # === æ­£å¸¸ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ ===
    # ç•°å¸¸ç‡: 1.0% (æœŸå¾…å€¤: 1.0%)
    #
    # === ç•°å¸¸ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ï¼ˆæ¸©åº¦+20â„ƒä¸Šæ˜‡ï¼‰ ===
    # ç•°å¸¸ç‡: 94.0%
    # TÂ²ç•°å¸¸æ¤œçŸ¥: 47ä»¶
    # SPEç•°å¸¸æ¤œçŸ¥: 12ä»¶
    

## 4.2 å…ˆè¡ŒæŒ‡æ¨™ï¼ˆLeading Indicatorsï¼‰ç®¡ç†

å…ˆè¡ŒæŒ‡æ¨™ã¯ã€äº‹æ•…ã‚„é‡å¤§ç½å®³ã®ã€Œäºˆå…†ã€ã‚’æ¤œçŸ¥ã™ã‚‹ãŸã‚ã®æŒ‡æ¨™ã§ã™ã€‚ é…è¡ŒæŒ‡æ¨™ï¼ˆäº‹æ•…ç‡ãªã©çµæœæŒ‡æ¨™ï¼‰ã¨ã¯ç•°ãªã‚Šã€äº‹æ•…ç™ºç”Ÿå‰ã®å®‰å…¨ç®¡ç†æ´»å‹•ã‚„ ãƒªã‚¹ã‚¯çŠ¶æ…‹ã‚’æ¸¬å®šã—ã€äºˆé˜²çš„ãªå¯¾ç­–ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚ 

### 4.2.1 å…ˆè¡ŒæŒ‡æ¨™ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
    
    
    # Example 3: Leading Indicators Tracking System
    from typing import List, Tuple
    from collections import deque
    
    class LeadingIndicator:
        """å…ˆè¡ŒæŒ‡æ¨™ã‚¯ãƒ©ã‚¹"""
    
        def __init__(self, name: str, target: float, unit: str,
                     direction: str = 'lower'):
            """
            Args:
                name: æŒ‡æ¨™å
                target: ç›®æ¨™å€¤
                unit: å˜ä½
                direction: 'lower'ï¼ˆä½ã„æ–¹ãŒè‰¯ã„ï¼‰or 'higher'ï¼ˆé«˜ã„æ–¹ãŒè‰¯ã„ï¼‰
            """
            self.name = name
            self.target = target
            self.unit = unit
            self.direction = direction
            self.values = deque(maxlen=12)  # ç›´è¿‘12ãƒ¶æœˆåˆ†
            self.timestamps = deque(maxlen=12)
    
        def add_value(self, value: float, timestamp: datetime):
            """æŒ‡æ¨™å€¤ã‚’è¿½åŠ """
            self.values.append(value)
            self.timestamps.append(timestamp)
    
        def get_current_value(self) -> Optional[float]:
            """ç¾åœ¨å€¤ã‚’å–å¾—"""
            return self.values[-1] if self.values else None
    
        def get_trend(self) -> str:
            """ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ¤å®šï¼ˆImproving/Stable/Deterioratingï¼‰"""
            if len(self.values) < 3:
                return "Insufficient Data"
    
            # ç›´è¿‘3ãƒ¶æœˆã®å¹³å‡å¤‰åŒ–ç‡
            recent_3 = list(self.values)[-3:]
            trend_value = (recent_3[-1] - recent_3[0]) / abs(recent_3[0]) if recent_3[0] != 0 else 0
    
            # æ–¹å‘ã‚’è€ƒæ…®
            if self.direction == 'lower':
                # å€¤ãŒä¸‹ãŒã‚‹æ–¹ãŒæ”¹å–„
                if trend_value < -0.05:
                    return "Improving"
                elif trend_value > 0.05:
                    return "Deteriorating"
                else:
                    return "Stable"
            else:
                # å€¤ãŒä¸ŠãŒã‚‹æ–¹ãŒæ”¹å–„
                if trend_value > 0.05:
                    return "Improving"
                elif trend_value < -0.05:
                    return "Deteriorating"
                else:
                    return "Stable"
    
        def is_target_met(self) -> bool:
            """ç›®æ¨™é”æˆåˆ¤å®š"""
            current = self.get_current_value()
            if current is None:
                return False
    
            if self.direction == 'lower':
                return current <= self.target
            else:
                return current >= self.target
    
    class LeadingIndicatorTracker:
        """å…ˆè¡ŒæŒ‡æ¨™ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ """
    
        def __init__(self):
            self.indicators: Dict[str, LeadingIndicator] = {}
    
        def add_indicator(self, indicator: LeadingIndicator):
            """æŒ‡æ¨™ã‚’è¿½åŠ """
            self.indicators[indicator.name] = indicator
    
        def update_indicator(self, name: str, value: float,
                            timestamp: datetime = None):
            """æŒ‡æ¨™å€¤ã‚’æ›´æ–°"""
            if name not in self.indicators:
                raise ValueError(f"Indicator '{name}' not found")
    
            if timestamp is None:
                timestamp = datetime.now()
    
            self.indicators[name].add_value(value, timestamp)
    
        def get_dashboard(self) -> pd.DataFrame:
            """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
            data = []
            for name, indicator in self.indicators.items():
                current = indicator.get_current_value()
                target_met = indicator.is_target_met()
                trend = indicator.get_trend()
    
                data.append({
                    'Indicator': name,
                    'Current Value': current,
                    'Target': indicator.target,
                    'Unit': indicator.unit,
                    'Target Met': 'âœ“' if target_met else 'âœ—',
                    'Trend': trend
                })
    
            return pd.DataFrame(data)
    
        def get_risk_score(self) -> float:
            """ç·åˆãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆ0-100ã€ä½ã„æ–¹ãŒè‰¯ã„ï¼‰"""
            if not self.indicators:
                return 0
    
            scores = []
            for indicator in self.indicators.values():
                current = indicator.get_current_value()
                if current is None:
                    continue
    
                # ç›®æ¨™ã‹ã‚‰ã®ä¹–é›¢åº¦ã‚’è¨ˆç®—
                deviation = abs(current - indicator.target) / abs(indicator.target)
    
                # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è€ƒæ…®ï¼ˆæ‚ªåŒ–ä¸­ã¯é‡ã¿ä»˜ã‘ï¼‰
                trend = indicator.get_trend()
                weight = 1.5 if trend == "Deteriorating" else 1.0
    
                scores.append(min(deviation * 100 * weight, 100))
    
            return np.mean(scores) if scores else 0
    
    # å®Ÿä½¿ç”¨ä¾‹
    tracker = LeadingIndicatorTracker()
    
    # å…ˆè¡ŒæŒ‡æ¨™ã®å®šç¾©
    indicators_config = [
        ('Near Miss Reports', 10, 'reports/month', 'higher'),   # æœˆ10ä»¶ä»¥ä¸ŠãŒç›®æ¨™
        ('Safety Training Hours', 8, 'hours/person/month', 'higher'),
        ('Overdue Inspections', 5, 'count', 'lower'),          # 5ä»¶ä»¥ä¸‹ãŒç›®æ¨™
        ('Safety Walk Completions', 20, 'walks/month', 'higher'),
        ('Permit Violations', 2, 'violations/month', 'lower'),
        ('Maintenance Backlog', 15, 'work orders', 'lower')
    ]
    
    for name, target, unit, direction in indicators_config:
        indicator = LeadingIndicator(name, target, unit, direction)
        tracker.add_indicator(indicator)
    
    # 12ãƒ¶æœˆåˆ†ã®ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
    np.random.seed(42)
    base_date = datetime(2024, 1, 1)
    
    # æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    monthly_data = {
        'Near Miss Reports': [8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],
        'Safety Training Hours': [7, 7.5, 8, 8.2, 8.5, 8.7, 9, 9.2, 9.5, 9.8, 10, 10.2],
        'Overdue Inspections': [12, 10, 9, 8, 7, 6, 5, 4, 4, 3, 3, 2],
        'Safety Walk Completions': [15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27],
        'Permit Violations': [5, 4, 4, 3, 3, 2, 2, 1, 1, 1, 0, 0],
        'Maintenance Backlog': [25, 23, 21, 20, 18, 17, 16, 15, 14, 13, 12, 11]
    }
    
    for month in range(12):
        timestamp = base_date + timedelta(days=30*month)
        for indicator_name, values in monthly_data.items():
            tracker.update_indicator(indicator_name, values[month], timestamp)
    
    # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¡¨ç¤º
    print("=== å…ˆè¡ŒæŒ‡æ¨™ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆæœ€æ–°æœˆï¼‰ ===")
    dashboard = tracker.get_dashboard()
    print(dashboard.to_string(index=False))
    
    # ç·åˆãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢
    risk_score = tracker.get_risk_score()
    print(f"\nã€ç·åˆãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã€‘: {risk_score:.1f}/100")
    if risk_score < 20:
        print("è©•ä¾¡: å„ªè‰¯ï¼ˆå®‰å…¨ç®¡ç†ãŒåŠ¹æœçš„ã«æ©Ÿèƒ½ï¼‰")
    elif risk_score < 40:
        print("è©•ä¾¡: è‰¯å¥½ï¼ˆç¶™ç¶šçš„ãªæ”¹å–„ãŒè¦‹ã‚‰ã‚Œã‚‹ï¼‰")
    elif risk_score < 60:
        print("è©•ä¾¡: æ³¨æ„ï¼ˆæ”¹å–„ã®ä½™åœ°ã‚ã‚Šï¼‰")
    else:
        print("è©•ä¾¡: è¦æ”¹å–„ï¼ˆæ—©æ€¥ãªå¯¾ç­–ãŒå¿…è¦ï¼‰")
    
    # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
    print("\n=== ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ ===")
    improving = [name for name, ind in tracker.indicators.items()
                 if ind.get_trend() == "Improving"]
    deteriorating = [name for name, ind in tracker.indicators.items()
                    if ind.get_trend() == "Deteriorating"]
    
    print(f"æ”¹å–„ä¸­ã®æŒ‡æ¨™ ({len(improving)}ä»¶):")
    for name in improving:
        print(f"  âœ“ {name}")
    
    if deteriorating:
        print(f"\næ‚ªåŒ–ä¸­ã®æŒ‡æ¨™ ({len(deteriorating)}ä»¶):")
        for name in deteriorating:
            print(f"  âš ï¸  {name}")
    
    # å‡ºåŠ›ä¾‹:
    # === å…ˆè¡ŒæŒ‡æ¨™ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆæœ€æ–°æœˆï¼‰ ===
    # Indicator                   Current Value  Target                Unit  Target Met     Trend
    # Near Miss Reports                    20.0    10.0   reports/month           âœ“  Improving
    # Safety Training Hours                10.2     8.0  hours/person/month       âœ“  Improving
    # Overdue Inspections                   2.0     5.0           count           âœ“  Improving
    # Safety Walk Completions              27.0    20.0     walks/month           âœ“  Improving
    # Permit Violations                     0.0     2.0  violations/month         âœ“  Improving
    # Maintenance Backlog                  11.0    15.0     work orders           âœ“  Improving
    #
    # ã€ç·åˆãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã€‘: 8.3/100
    # è©•ä¾¡: å„ªè‰¯ï¼ˆå®‰å…¨ç®¡ç†ãŒåŠ¹æœçš„ã«æ©Ÿèƒ½ï¼‰
    

## 4.3 æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹äº‹æ•…äºˆæ¸¬

éå»ã®äº‹æ•…ãƒ‡ãƒ¼ã‚¿ã€ãƒ‹ã‚¢ãƒŸã‚¹ãƒ‡ãƒ¼ã‚¿ã€é‹è»¢ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã€ äº‹æ•…ç™ºç”Ÿç¢ºç‡ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚Random Forestã‚„ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãªã©ã® ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ãŒé«˜ã„äºˆæ¸¬ç²¾åº¦ã‚’ç¤ºã—ã¾ã™ã€‚ 

### 4.3.1 äº‹æ•…äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
    
    
    # Example 4: Incident Prediction with Machine Learning
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
    import warnings
    warnings.filterwarnings('ignore')
    
    class IncidentPredictor:
        """æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹äº‹æ•…äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
    
        é‹è»¢ãƒ‡ãƒ¼ã‚¿ã¨å®‰å…¨æŒ‡æ¨™ã‹ã‚‰äº‹æ•…ãƒªã‚¹ã‚¯ã‚’äºˆæ¸¬
        """
    
        def __init__(self, model_type: str = 'random_forest'):
            """
            Args:
                model_type: 'random_forest' or 'gradient_boosting'
            """
            if model_type == 'random_forest':
                self.model = RandomForestClassifier(
                    n_estimators=100,
                    max_depth=10,
                    min_samples_split=10,
                    random_state=42
                )
            elif model_type == 'gradient_boosting':
                self.model = GradientBoostingClassifier(
                    n_estimators=100,
                    learning_rate=0.1,
                    max_depth=5,
                    random_state=42
                )
            else:
                raise ValueError("model_type must be 'random_forest' or 'gradient_boosting'")
    
            self.feature_names = None
            self.is_trained = False
    
        def train(self, X: np.ndarray, y: np.ndarray,
                  feature_names: List[str] = None) -> Dict:
            """ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
    
            Args:
                X: ç‰¹å¾´é‡ (n_samples, n_features)
                y: ãƒ©ãƒ™ãƒ«ï¼ˆ0: æ­£å¸¸, 1: äº‹æ•…ç™ºç”Ÿï¼‰
                feature_names: ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
    
            Returns:
                è¨“ç·´çµæœã®çµ±è¨ˆ
            """
            self.feature_names = feature_names
    
            # è¨“ç·´/ãƒ†ã‚¹ãƒˆåˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
    
            # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            self.model.fit(X_train, y_train)
    
            # æ€§èƒ½è©•ä¾¡
            y_pred = self.model.predict(X_test)
            y_prob = self.model.predict_proba(X_test)[:, 1]
    
            # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            cv_scores = cross_val_score(self.model, X, y, cv=5, scoring='roc_auc')
    
            self.is_trained = True
    
            return {
                'train_score': self.model.score(X_train, y_train),
                'test_score': self.model.score(X_test, y_test),
                'roc_auc': roc_auc_score(y_test, y_prob),
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'confusion_matrix': confusion_matrix(y_test, y_pred),
                'classification_report': classification_report(y_test, y_pred)
            }
    
        def predict_risk(self, X: np.ndarray) -> np.ndarray:
            """äº‹æ•…ãƒªã‚¹ã‚¯ç¢ºç‡ã‚’äºˆæ¸¬
    
            Args:
                X: ç‰¹å¾´é‡
    
            Returns:
                äº‹æ•…ç™ºç”Ÿç¢ºç‡ï¼ˆ0-1ï¼‰
            """
            if not self.is_trained:
                raise ValueError("Model not trained")
    
            return self.model.predict_proba(X)[:, 1]
    
        def get_feature_importance(self) -> pd.DataFrame:
            """ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—"""
            if not self.is_trained:
                raise ValueError("Model not trained")
    
            importances = self.model.feature_importances_
            indices = np.argsort(importances)[::-1]
    
            df = pd.DataFrame({
                'Feature': [self.feature_names[i] if self.feature_names else f'Feature_{i}'
                           for i in indices],
                'Importance': importances[indices]
            })
    
            return df
    
    # å®Ÿä½¿ç”¨ä¾‹ï¼šäº‹æ•…äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    np.random.seed(42)
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆ1000ã‚µãƒ³ãƒ—ãƒ«ï¼‰
    n_samples = 1000
    n_incidents = 100  # 10%ãŒäº‹æ•…ç™ºç”Ÿ
    
    # ç‰¹å¾´é‡ã®ç”Ÿæˆ
    features = {
        'Temperature Deviation': np.random.randn(n_samples) * 10,  # æ¸©åº¦åå·®
        'Pressure Deviation': np.random.randn(n_samples) * 0.5,   # åœ§åŠ›åå·®
        'Near Miss Count': np.random.poisson(2, n_samples),        # ãƒ‹ã‚¢ãƒŸã‚¹ä»¶æ•°
        'Overdue Maintenance': np.random.poisson(3, n_samples),    # æœªå®Ÿæ–½ä¿å…¨
        'Operator Experience': np.random.uniform(1, 20, n_samples), # çµŒé¨“å¹´æ•°
        'Safety Training Hours': np.random.uniform(5, 15, n_samples), # è¨“ç·´æ™‚é–“
        'Equipment Age': np.random.uniform(0, 30, n_samples),      # è¨­å‚™å¹´æ•°
        'Alarm Rate': np.random.poisson(10, n_samples)             # ã‚¢ãƒ©ãƒ¼ãƒ ç‡
    }
    
    # äº‹æ•…ç™ºç”Ÿãƒ©ãƒ™ãƒ«ç”Ÿæˆï¼ˆç‰¹å¾´é‡ã«ä¾å­˜ï¼‰
    incident_score = (
        np.abs(features['Temperature Deviation']) * 0.1 +
        np.abs(features['Pressure Deviation']) * 0.5 +
        features['Near Miss Count'] * 0.3 +
        features['Overdue Maintenance'] * 0.2 +
        (20 - features['Operator Experience']) * 0.05 +
        (15 - features['Safety Training Hours']) * 0.1 +
        features['Equipment Age'] * 0.05 +
        features['Alarm Rate'] * 0.1 +
        np.random.randn(n_samples) * 2  # ãƒã‚¤ã‚º
    )
    
    # ä¸Šä½10%ã‚’äº‹æ•…ã¨ãƒ©ãƒ™ãƒ«ä»˜ã‘
    threshold = np.percentile(incident_score, 90)
    y = (incident_score > threshold).astype(int)
    
    # ç‰¹å¾´é‡è¡Œåˆ—ä½œæˆ
    X = np.column_stack([features[k] for k in features.keys()])
    feature_names = list(features.keys())
    
    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    predictor = IncidentPredictor(model_type='random_forest')
    train_result = predictor.train(X, y, feature_names)
    
    print("=== äº‹æ•…äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ ===")
    print(f"è¨“ç·´ç²¾åº¦: {train_result['train_score']:.3f}")
    print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {train_result['test_score']:.3f}")
    print(f"ROC-AUC: {train_result['roc_auc']:.3f}")
    print(f"CVå¹³å‡: {train_result['cv_mean']:.3f} (Â±{train_result['cv_std']:.3f})")
    
    print("\n=== æ··åŒè¡Œåˆ— ===")
    cm = train_result['confusion_matrix']
    print(f"True Negative: {cm[0,0]}, False Positive: {cm[0,1]}")
    print(f"False Negative: {cm[1,0]}, True Positive: {cm[1,1]}")
    
    print("\n=== åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ ===")
    print(train_result['classification_report'])
    
    # ç‰¹å¾´é‡é‡è¦åº¦
    print("=== ç‰¹å¾´é‡é‡è¦åº¦ ===")
    importance_df = predictor.get_feature_importance()
    print(importance_df.to_string(index=False))
    
    # æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬
    print("\n=== æ–°è¦ãƒ‡ãƒ¼ã‚¿äºˆæ¸¬ä¾‹ ===")
    new_samples = np.array([
        [15, 1.0, 5, 8, 3, 6, 25, 20],   # é«˜ãƒªã‚¹ã‚¯ã‚±ãƒ¼ã‚¹
        [2, 0.1, 0, 1, 15, 12, 5, 5],    # ä½ãƒªã‚¹ã‚¯ã‚±ãƒ¼ã‚¹
        [8, 0.5, 3, 4, 10, 10, 15, 12]   # ä¸­ãƒªã‚¹ã‚¯ã‚±ãƒ¼ã‚¹
    ])
    
    risk_probs = predictor.predict_risk(new_samples)
    for i, prob in enumerate(risk_probs):
        risk_level = "é«˜" if prob > 0.5 else "ä¸­" if prob > 0.2 else "ä½"
        print(f"ã‚µãƒ³ãƒ—ãƒ«{i+1}: äº‹æ•…ãƒªã‚¹ã‚¯={prob*100:.1f}% ({risk_level}ãƒªã‚¹ã‚¯)")
    
    # å‡ºåŠ›ä¾‹:
    # === äº‹æ•…äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ ===
    # è¨“ç·´ç²¾åº¦: 0.975
    # ãƒ†ã‚¹ãƒˆç²¾åº¦: 0.885
    # ROC-AUC: 0.932
    # CVå¹³å‡: 0.918 (Â±0.023)
    

## 4.4 å®‰å…¨æ€§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™

å®‰å…¨æ€§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ã€TRIRï¼ˆç·åˆç½å®³åº¦æ•°ç‡ï¼‰ã€LTIRï¼ˆä¼‘æ¥­ç½å®³åº¦æ•°ç‡ï¼‰ãªã©ã® æ¨™æº–åŒ–ã•ã‚ŒãŸæŒ‡æ¨™ã§æ¸¬å®šã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã¯ç”£æ¥­é–“ãƒ»ä¼æ¥­é–“ã®æ¯”è¼ƒã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚ 

### 4.4.1 å®‰å…¨æ€§KPIè¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ 
    
    
    # Example 5: Safety Performance Metrics Calculation
    from dataclasses import dataclass
    from typing import List
    
    @dataclass
    class Incident:
        """äº‹æ•…è¨˜éŒ²"""
        date: datetime
        type: str  # 'fatality', 'lost_time', 'restricted_work', 'medical_treatment', 'first_aid'
        days_away: int = 0  # ä¼‘æ¥­æ—¥æ•°
        description: str = ""
    
    class SafetyPerformanceCalculator:
        """å®‰å…¨æ€§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™è¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ 
    
        OSHAåŸºæº–ã«åŸºã¥ãå„ç¨®å®‰å…¨æ€§æŒ‡æ¨™ã‚’è¨ˆç®—
        """
    
        def __init__(self):
            self.incidents: List[Incident] = []
            self.total_hours_worked = 0
    
        def add_incident(self, incident: Incident):
            """äº‹æ•…ã‚’è¿½åŠ """
            self.incidents.append(incident)
    
        def set_hours_worked(self, hours: float):
            """ç·åŠ´åƒæ™‚é–“ã‚’è¨­å®š"""
            self.total_hours_worked = hours
    
        def calculate_trir(self) -> float:
            """TRIRï¼ˆç·åˆç½å®³åº¦æ•°ç‡ï¼‰ã‚’è¨ˆç®—
    
            TRIR = (è¨˜éŒ²å¯¾è±¡ç½å®³ä»¶æ•° Ã— 200,000) / ç·åŠ´åƒæ™‚é–“
    
            è¨˜éŒ²å¯¾è±¡: æ­»äº¡ã€ä¼‘æ¥­ã€åˆ¶é™ä½œæ¥­ã€åŒ»ç™‚å‡¦ç½®
            200,000 = 100äººãŒå¹´é–“åƒãæ™‚é–“ï¼ˆ100äººÃ—2,000æ™‚é–“ï¼‰
            """
            recordable = [i for i in self.incidents
                         if i.type in ['fatality', 'lost_time',
                                      'restricted_work', 'medical_treatment']]
    
            if self.total_hours_worked == 0:
                return 0
    
            return (len(recordable) * 200000) / self.total_hours_worked
    
        def calculate_ltir(self) -> float:
            """LTIRï¼ˆä¼‘æ¥­ç½å®³åº¦æ•°ç‡ï¼‰ã‚’è¨ˆç®—
    
            LTIR = (ä¼‘æ¥­ç½å®³ä»¶æ•° Ã— 200,000) / ç·åŠ´åƒæ™‚é–“
            """
            lost_time_incidents = [i for i in self.incidents
                                  if i.type in ['fatality', 'lost_time']]
    
            if self.total_hours_worked == 0:
                return 0
    
            return (len(lost_time_incidents) * 200000) / self.total_hours_worked
    
        def calculate_severity_rate(self) -> float:
            """é‡å¤§åº¦ç‡ã‚’è¨ˆç®—
    
            Severity Rate = (ç·ä¼‘æ¥­æ—¥æ•° Ã— 200,000) / ç·åŠ´åƒæ™‚é–“
            """
            total_days_lost = sum(i.days_away for i in self.incidents)
    
            if self.total_hours_worked == 0:
                return 0
    
            return (total_days_lost * 200000) / self.total_hours_worked
    
        def calculate_fatality_rate(self) -> float:
            """æ­»äº¡ç½å®³ç‡ã‚’è¨ˆç®—
    
            Fatality Rate = (æ­»äº¡ä»¶æ•° Ã— 200,000) / ç·åŠ´åƒæ™‚é–“
            """
            fatalities = [i for i in self.incidents if i.type == 'fatality']
    
            if self.total_hours_worked == 0:
                return 0
    
            return (len(fatalities) * 200000) / self.total_hours_worked
    
        def get_incident_breakdown(self) -> pd.DataFrame:
            """äº‹æ•…ã‚¿ã‚¤ãƒ—åˆ¥ã®å†…è¨³"""
            if not self.incidents:
                return pd.DataFrame()
    
            type_counts = {}
            for incident in self.incidents:
                type_counts[incident.type] = type_counts.get(incident.type, 0) + 1
    
            df = pd.DataFrame({
                'Incident Type': list(type_counts.keys()),
                'Count': list(type_counts.values())
            })
    
            # å‰²åˆã‚’è¨ˆç®—
            df['Percentage'] = (df['Count'] / df['Count'].sum() * 100).round(1)
    
            return df.sort_values('Count', ascending=False)
    
        def get_comprehensive_report(self) -> Dict:
            """ç·åˆå®‰å…¨æ€§ãƒ¬ãƒãƒ¼ãƒˆ"""
            return {
                'Total Incidents': len(self.incidents),
                'Total Hours Worked': self.total_hours_worked,
                'TRIR': self.calculate_trir(),
                'LTIR': self.calculate_ltir(),
                'Severity Rate': self.calculate_severity_rate(),
                'Fatality Rate': self.calculate_fatality_rate(),
                'Incident Breakdown': self.get_incident_breakdown()
            }
    
        def benchmark_against_industry(self, industry_trir: float,
                                       industry_ltir: float) -> Dict:
            """æ¥­ç•Œå¹³å‡ã¨ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    
            Args:
                industry_trir: æ¥­ç•Œå¹³å‡TRIR
                industry_ltir: æ¥­ç•Œå¹³å‡LTIR
    
            Returns:
                ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
            """
            company_trir = self.calculate_trir()
            company_ltir = self.calculate_ltir()
    
            return {
                'Company TRIR': company_trir,
                'Industry TRIR': industry_trir,
                'TRIR Performance': 'Above Average' if company_trir < industry_trir else 'Below Average',
                'TRIR Difference': company_trir - industry_trir,
                'Company LTIR': company_ltir,
                'Industry LTIR': industry_ltir,
                'LTIR Performance': 'Above Average' if company_ltir < industry_ltir else 'Below Average',
                'LTIR Difference': company_ltir - industry_ltir
            }
    
    # å®Ÿä½¿ç”¨ä¾‹
    spc = SafetyPerformanceCalculator()
    
    # å¹´é–“ç·åŠ´åƒæ™‚é–“ï¼ˆå¾“æ¥­å“¡500äººã€å¹´é–“2000æ™‚é–“/äººï¼‰
    spc.set_hours_worked(500 * 2000)  # 1,000,000æ™‚é–“
    
    # äº‹æ•…ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ ï¼ˆ1å¹´é–“ï¼‰
    incidents_data = [
        Incident(datetime(2024, 1, 15), 'first_aid', 0, "è»½ã„åˆ‡ã‚Šå‚·"),
        Incident(datetime(2024, 2, 3), 'medical_treatment', 0, "æ‰“æ’²ã€åŒ»ç™‚å‡¦ç½®"),
        Incident(datetime(2024, 3, 12), 'lost_time', 5, "è»¢å€’ã€è¶³é¦–æ»æŒ«"),
        Incident(datetime(2024, 4, 8), 'first_aid', 0, "ç›®ã«ã‚´ãƒŸ"),
        Incident(datetime(2024, 5, 20), 'restricted_work', 3, "è…°ç—›ã€è»½ä½œæ¥­ã®ã¿"),
        Incident(datetime(2024, 6, 15), 'lost_time', 10, "æ©Ÿæ¢°æŒŸã¾ã‚Œã€æŒ‡éª¨æŠ˜"),
        Incident(datetime(2024, 7, 22), 'medical_treatment', 0, "åŒ–å­¦è–¬å“é£›æ•£"),
        Incident(datetime(2024, 8, 9), 'first_aid', 0, "è»½ã„ç«å‚·"),
        Incident(datetime(2024, 9, 14), 'lost_time', 15, "é«˜æ‰€è»¢è½ã€è„šéƒ¨éª¨æŠ˜"),
        Incident(datetime(2024, 10, 5), 'medical_treatment', 0, "ã‚¬ã‚¹å¸å…¥"),
        Incident(datetime(2024, 11, 18), 'first_aid', 0, "æ“¦ã‚Šå‚·"),
        Incident(datetime(2024, 12, 2), 'restricted_work', 2, "æ‰‹é¦–ç—›")
    ]
    
    for incident in incidents_data:
        spc.add_incident(incident)
    
    # ç·åˆãƒ¬ãƒãƒ¼ãƒˆ
    report = spc.get_comprehensive_report()
    
    print("=== å¹´é–“å®‰å…¨æ€§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ ===\n")
    print(f"ç·äº‹æ•…ä»¶æ•°: {report['Total Incidents']}ä»¶")
    print(f"ç·åŠ´åƒæ™‚é–“: {report['Total Hours Worked']:,}æ™‚é–“")
    print(f"\nã€ä¸»è¦æŒ‡æ¨™ã€‘")
    print(f"TRIRï¼ˆç·åˆç½å®³åº¦æ•°ç‡ï¼‰: {report['TRIR']:.2f}")
    print(f"LTIRï¼ˆä¼‘æ¥­ç½å®³åº¦æ•°ç‡ï¼‰: {report['LTIR']:.2f}")
    print(f"é‡å¤§åº¦ç‡: {report['Severity Rate']:.2f}")
    print(f"æ­»äº¡ç½å®³ç‡: {report['Fatality Rate']:.2f}")
    
    print("\n=== äº‹æ•…ã‚¿ã‚¤ãƒ—åˆ¥å†…è¨³ ===")
    print(report['Incident Breakdown'].to_string(index=False))
    
    # æ¥­ç•Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆåŒ–å­¦ç”£æ¥­ã®å…¸å‹å€¤ï¼‰
    print("\n=== æ¥­ç•Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ===")
    industry_trir = 1.2  # åŒ–å­¦ç”£æ¥­å¹³å‡TRIR
    industry_ltir = 0.5  # åŒ–å­¦ç”£æ¥­å¹³å‡LTIR
    
    benchmark = spc.benchmark_against_industry(industry_trir, industry_ltir)
    print(f"å½“ç¤¾TRIR: {benchmark['Company TRIR']:.2f}")
    print(f"æ¥­ç•Œå¹³å‡TRIR: {benchmark['Industry TRIR']:.2f}")
    print(f"è©•ä¾¡: {benchmark['TRIR Performance']}")
    print(f"å·®åˆ†: {benchmark['TRIR Difference']:+.2f}")
    
    print(f"\nå½“ç¤¾LTIR: {benchmark['Company LTIR']:.2f}")
    print(f"æ¥­ç•Œå¹³å‡LTIR: {benchmark['Industry LTIR']:.2f}")
    print(f"è©•ä¾¡: {benchmark['LTIR Performance']}")
    print(f"å·®åˆ†: {benchmark['LTIR Difference']:+.2f}")
    
    # å‡ºåŠ›ä¾‹:
    # === å¹´é–“å®‰å…¨æ€§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ ===
    #
    # ç·äº‹æ•…ä»¶æ•°: 12ä»¶
    # ç·åŠ´åƒæ™‚é–“: 1,000,000æ™‚é–“
    #
    # ã€ä¸»è¦æŒ‡æ¨™ã€‘
    # TRIRï¼ˆç·åˆç½å®³åº¦æ•°ç‡ï¼‰: 1.40
    # LTIRï¼ˆä¼‘æ¥­ç½å®³åº¦æ•°ç‡ï¼‰: 0.60
    # é‡å¤§åº¦ç‡: 6.00
    # æ­»äº¡ç½å®³ç‡: 0.00
    

### 4.4.2 ãƒ‹ã‚¢ãƒŸã‚¹åˆ†æã‚·ã‚¹ãƒ†ãƒ 
    
    
    # Example 6: Near-Miss Analysis System
    class NearMissAnalyzer:
        """ãƒ‹ã‚¢ãƒŸã‚¹ï¼ˆãƒ’ãƒ¤ãƒªãƒãƒƒãƒˆï¼‰åˆ†æã‚·ã‚¹ãƒ†ãƒ 
    
        ãƒã‚¤ãƒ³ãƒªãƒƒãƒ’ã®æ³•å‰‡ã«åŸºã¥ãäºˆé˜²çš„å®‰å…¨ç®¡ç†
        """
    
        def __init__(self):
            self.near_misses: List[Dict] = []
    
        def add_near_miss(self, category: str, severity: int,
                         description: str, timestamp: datetime = None):
            """ãƒ‹ã‚¢ãƒŸã‚¹ã‚’è¿½åŠ 
    
            Args:
                category: ã‚«ãƒ†ã‚´ãƒªï¼ˆ'slip_trip', 'chemical', 'equipment', etc.ï¼‰
                severity: é‡å¤§åº¦ï¼ˆ1-5ã€5ãŒæœ€ã‚‚æ·±åˆ»ï¼‰
                description: èª¬æ˜
                timestamp: ç™ºç”Ÿæ™‚åˆ»
            """
            if timestamp is None:
                timestamp = datetime.now()
    
            self.near_misses.append({
                'timestamp': timestamp,
                'category': category,
                'severity': severity,
                'description': description
            })
    
        def get_heinrich_ratio_analysis(self, actual_incidents: int) -> Dict:
            """ãƒã‚¤ãƒ³ãƒªãƒƒãƒ’ã®æ³•å‰‡ã«åŸºã¥ãåˆ†æ
    
            ãƒã‚¤ãƒ³ãƒªãƒƒãƒ’ã®æ³•å‰‡: é‡å¤§äº‹æ•…1ä»¶ã®èƒŒå¾Œã«
            - è»½å‚·äº‹æ•…29ä»¶
            - ãƒ‹ã‚¢ãƒŸã‚¹300ä»¶
            ãŒå­˜åœ¨ã™ã‚‹ã¨ã„ã†çµŒé¨“å‰‡
            """
            near_miss_count = len(self.near_misses)
    
            # æœŸå¾…ã•ã‚Œã‚‹é‡å¤§äº‹æ•…ä»¶æ•°
            expected_major = near_miss_count / 300
    
            return {
                'Near Miss Count': near_miss_count,
                'Actual Incidents': actual_incidents,
                'Expected Major Incidents (Heinrich)': expected_major,
                'Heinrich Ratio': f"1:{actual_incidents}:{near_miss_count}",
                'Prevention Effectiveness': (
                    (expected_major - actual_incidents) / expected_major * 100
                    if expected_major > 0 else 0
                )
            }
    
        def get_category_analysis(self) -> pd.DataFrame:
            """ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ"""
            if not self.near_misses:
                return pd.DataFrame()
    
            df = pd.DataFrame(self.near_misses)
    
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ
            category_stats = df.groupby('category').agg({
                'severity': ['count', 'mean', 'max'],
                'description': 'count'
            }).round(2)
    
            category_stats.columns = ['Count', 'Avg Severity', 'Max Severity', 'Total']
            category_stats = category_stats[['Count', 'Avg Severity', 'Max Severity']]
    
            return category_stats.sort_values('Count', ascending=False)
    
        def get_high_severity_near_misses(self, threshold: int = 4) -> pd.DataFrame:
            """é«˜é‡å¤§åº¦ãƒ‹ã‚¢ãƒŸã‚¹ã‚’æŠ½å‡º
    
            Args:
                threshold: é‡å¤§åº¦ã—ãã„å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ4ä»¥ä¸Šï¼‰
            """
            if not self.near_misses:
                return pd.DataFrame()
    
            df = pd.DataFrame(self.near_misses)
            high_severity = df[df['severity'] >= threshold]
    
            return high_severity[['timestamp', 'category', 'severity', 'description']]
    
        def calculate_near_miss_rate(self, hours_worked: float) -> float:
            """ãƒ‹ã‚¢ãƒŸã‚¹ç‡ã‚’è¨ˆç®—
    
            Near Miss Rate = (ãƒ‹ã‚¢ãƒŸã‚¹ä»¶æ•° Ã— 200,000) / ç·åŠ´åƒæ™‚é–“
            """
            if hours_worked == 0:
                return 0
    
            return (len(self.near_misses) * 200000) / hours_worked
    
    # å®Ÿä½¿ç”¨ä¾‹
    nma = NearMissAnalyzer()
    
    # ãƒ‹ã‚¢ãƒŸã‚¹ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ ï¼ˆ3ãƒ¶æœˆåˆ†ï¼‰
    near_miss_data = [
        ('slip_trip', 2, "åºŠã«æ°´ãŸã¾ã‚Šã€è»¢å€’ã—ãã†ã«ãªã£ãŸ"),
        ('chemical', 4, "ã‚¿ãƒ³ã‚¯å¼ãŒå°‘ã—é–‹ã„ã¦ãŠã‚Šã€è–¬å“ãŒæ»´ä¸‹"),
        ('equipment', 3, "å®‰å…¨ã‚«ãƒãƒ¼ãŒå¤–ã‚ŒãŸã¾ã¾é‹è»¢é–‹å§‹ã—ãã†ã«ãªã£ãŸ"),
        ('slip_trip', 1, "æ®µå·®ã«ã¤ã¾ãšã„ãŸ"),
        ('electrical', 5, "é…ç·šãŒæå‚·ã€æ„Ÿé›»ã®å±é™º"),
        ('chemical', 3, "ä¿è­·å…·ãªã—ã§è–¬å“ã‚’æ‰±ã„ãã†ã«ãªã£ãŸ"),
        ('equipment', 4, "ãƒãƒ³ãƒ—ç•°éŸ³ã€æ•…éšœã®å…†å€™"),
        ('slip_trip', 2, "é€šè·¯ã«éšœå®³ç‰©ã€ã¤ã¾ãšã„ãŸ"),
        ('chemical', 2, "è–¬å“å®¹å™¨ã®ãƒ©ãƒ™ãƒ«ä¸æ˜ç­"),
        ('equipment', 3, "åœ§åŠ›è¨ˆã®é‡ãŒç•°å¸¸å€¤ã‚’ç¤ºã—ãŸ"),
        ('electrical', 4, "ã‚³ãƒ³ã‚»ãƒ³ãƒˆãŒéç†±ã—ã¦ã„ãŸ"),
        ('slip_trip', 1, "åºŠãŒæ»‘ã‚Šã‚„ã™ã‹ã£ãŸ"),
        ('chemical', 5, "é…ç®¡æ¥ç¶šéƒ¨ã‹ã‚‰ã‚¬ã‚¹æ¼ã‚Œã®è‡­ã„"),
        ('equipment', 2, "å·¥å…·ãŒæ‰€å®šä½ç½®ã«ãªã‹ã£ãŸ"),
    ]
    
    base_time = datetime(2024, 10, 1)
    for i, (cat, sev, desc) in enumerate(near_miss_data):
        timestamp = base_time + timedelta(days=i*3)
        nma.add_near_miss(cat, sev, desc, timestamp)
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
    print("=== ãƒ‹ã‚¢ãƒŸã‚¹ ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ ===")
    category_analysis = nma.get_category_analysis()
    print(category_analysis)
    
    # ãƒã‚¤ãƒ³ãƒªãƒƒãƒ’ã®æ³•å‰‡åˆ†æ
    print("\n=== ãƒã‚¤ãƒ³ãƒªãƒƒãƒ’ã®æ³•å‰‡åˆ†æ ===")
    actual_incidents = 3  # åŒæœŸé–“ã®å®Ÿäº‹æ•…ä»¶æ•°
    heinrich = nma.get_heinrich_ratio_analysis(actual_incidents)
    print(f"ãƒ‹ã‚¢ãƒŸã‚¹ä»¶æ•°: {heinrich['Near Miss Count']}")
    print(f"å®Ÿäº‹æ•…ä»¶æ•°: {heinrich['Actual Incidents']}")
    print(f"æœŸå¾…é‡å¤§äº‹æ•…ä»¶æ•°ï¼ˆæ³•å‰‡ï¼‰: {heinrich['Expected Major Incidents (Heinrich)']:.2f}")
    print(f"æ¯”ç‡: {heinrich['Heinrich Ratio']}")
    print(f"äºˆé˜²åŠ¹æœ: {heinrich['Prevention Effectiveness']:.1f}%")
    
    # é«˜é‡å¤§åº¦ãƒ‹ã‚¢ãƒŸã‚¹
    print("\n=== é«˜é‡å¤§åº¦ãƒ‹ã‚¢ãƒŸã‚¹ï¼ˆé‡å¤§åº¦â‰¥4ï¼‰ ===")
    high_severity = nma.get_high_severity_near_misses(threshold=4)
    for _, row in high_severity.iterrows():
        print(f"[{row['timestamp'].strftime('%Y-%m-%d')}] "
              f"{row['category']} (é‡å¤§åº¦{row['severity']}): {row['description']}")
    
    # ãƒ‹ã‚¢ãƒŸã‚¹ç‡è¨ˆç®—
    hours_worked = 500 * 2000  # 500äººÃ—å¹´é–“2000æ™‚é–“
    nm_rate = nma.calculate_near_miss_rate(hours_worked)
    print(f"\nãƒ‹ã‚¢ãƒŸã‚¹ç‡: {nm_rate:.2f} (per 200,000 hours)")
    
    # å‡ºåŠ›ä¾‹:
    # === ãƒ‹ã‚¢ãƒŸã‚¹ ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ ===
    #             Count  Avg Severity  Max Severity
    # category
    # slip_trip       4          1.50             2
    # chemical        4          3.50             5
    # equipment       4          3.00             4
    # electrical      2          4.50             5
    

## 4.5 çµ±åˆå®‰å…¨ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 

ã“ã‚Œã¾ã§ã®ã™ã¹ã¦ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’çµ±åˆã—ã€åŒ…æ‹¬çš„ãªå®‰å…¨ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã€äºˆæ¸¬ã€KPIç®¡ç†ã‚’ä¸€å…ƒåŒ–ã—ã¾ã™ã€‚ 
    
    
    # Example 7: Integrated Safety Management System
    class IntegratedSafetyManagementSystem:
        """çµ±åˆå®‰å…¨ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
    
        ç›£è¦–ã€äºˆæ¸¬ã€KPIç®¡ç†ã‚’ä¸€å…ƒåŒ–ã—ãŸç·åˆå®‰å…¨ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
        """
    
        def __init__(self):
            self.safety_monitor = SafetyMonitor()
            self.mspc_monitor = None  # è¨“ç·´å¾Œã«è¨­å®š
            self.leading_indicator_tracker = LeadingIndicatorTracker()
            self.incident_predictor = None  # è¨“ç·´å¾Œã«è¨­å®š
            self.performance_calculator = SafetyPerformanceCalculator()
            self.near_miss_analyzer = NearMissAnalyzer()
    
        def initialize_monitoring(self, safety_limits: Dict[str, SafetyLimit]):
            """ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–"""
            for param, limits in safety_limits.items():
                self.safety_monitor.set_safety_limits(param, limits)
    
        def initialize_leading_indicators(self, indicators: List[LeadingIndicator]):
            """å…ˆè¡ŒæŒ‡æ¨™ã‚’åˆæœŸåŒ–"""
            for indicator in indicators:
                self.leading_indicator_tracker.add_indicator(indicator)
    
        def train_anomaly_detector(self, normal_data: np.ndarray):
            """ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´"""
            self.mspc_monitor = MSPCMonitor()
            return self.mspc_monitor.train(normal_data)
    
        def train_incident_predictor(self, X: np.ndarray, y: np.ndarray,
                                    feature_names: List[str]):
            """äº‹æ•…äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´"""
            self.incident_predictor = IncidentPredictor()
            return self.incident_predictor.train(X, y, feature_names)
    
        def get_real_time_status(self) -> Dict:
            """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—"""
            return {
                'Process Safety': self.safety_monitor.get_current_status(),
                'Leading Indicators': self.leading_indicator_tracker.get_dashboard(),
                'Risk Score': self.leading_indicator_tracker.get_risk_score()
            }
    
        def generate_safety_dashboard(self) -> str:
            """ç·åˆå®‰å…¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""
            lines = []
            lines.append("=" * 70)
            lines.append("çµ±åˆå®‰å…¨ç®¡ç†ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰".center(70))
            lines.append("=" * 70)
            lines.append("")
    
            # ãƒ—ãƒ­ã‚»ã‚¹å®‰å…¨çŠ¶æ…‹
            lines.append("ã€ãƒ—ãƒ­ã‚»ã‚¹å®‰å…¨çŠ¶æ…‹ã€‘")
            process_status = self.safety_monitor.get_current_status()
            for _, row in process_status.iterrows():
                status_icon = "âœ“" if row['Safety Level'] == 'Normal' else "âš ï¸"
                lines.append(f"  {status_icon} {row['Parameter']}: "
                            f"{row['Current Value']:.1f} ({row['Safety Level']})")
            lines.append("")
    
            # å…ˆè¡ŒæŒ‡æ¨™
            lines.append("ã€å…ˆè¡ŒæŒ‡æ¨™ã€‘")
            indicators = self.leading_indicator_tracker.get_dashboard()
            for _, row in indicators.iterrows():
                lines.append(f"  {row['Target Met']} {row['Indicator']}: "
                            f"{row['Current Value']:.1f} ({row['Trend']})")
    
            risk_score = self.leading_indicator_tracker.get_risk_score()
            lines.append(f"\n  ç·åˆãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢: {risk_score:.1f}/100")
            lines.append("")
    
            # å®‰å…¨æ€§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
            if self.performance_calculator.total_hours_worked > 0:
                lines.append("ã€å®‰å…¨æ€§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€‘")
                lines.append(f"  TRIR: {self.performance_calculator.calculate_trir():.2f}")
                lines.append(f"  LTIR: {self.performance_calculator.calculate_ltir():.2f}")
                lines.append("")
    
            # ãƒ‹ã‚¢ãƒŸã‚¹çµ±è¨ˆ
            if len(self.near_miss_analyzer.near_misses) > 0:
                lines.append("ã€ãƒ‹ã‚¢ãƒŸã‚¹ã€‘")
                lines.append(f"  ç·ä»¶æ•°: {len(self.near_miss_analyzer.near_misses)}")
                high_severity = self.near_miss_analyzer.get_high_severity_near_misses()
                lines.append(f"  é«˜é‡å¤§åº¦ï¼ˆâ‰¥4ï¼‰: {len(high_severity)}ä»¶")
                lines.append("")
    
            return "\n".join(lines)
    
        def predict_incident_risk(self, current_conditions: np.ndarray) -> Dict:
            """ç¾åœ¨ã®æ¡ä»¶ã‹ã‚‰äº‹æ•…ãƒªã‚¹ã‚¯ã‚’äºˆæ¸¬"""
            if self.incident_predictor is None:
                return {'error': 'Incident predictor not trained'}
    
            risk_prob = self.incident_predictor.predict_risk(current_conditions)[0]
    
            risk_level = (
                "Critical" if risk_prob > 0.7 else
                "High" if risk_prob > 0.5 else
                "Medium" if risk_prob > 0.3 else
                "Low"
            )
    
            return {
                'risk_probability': risk_prob,
                'risk_level': risk_level,
                'recommendation': self._get_recommendation(risk_level)
            }
    
        def _get_recommendation(self, risk_level: str) -> str:
            """ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸæ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
            recommendations = {
                'Critical': "å³åº§ã«é‹è»¢åœæ­¢ã‚’æ¤œè¨ã€‚ç·Šæ€¥å¯¾å¿œãƒãƒ¼ãƒ æ‹›é›†ã€‚",
                'High': "é‹è»¢æ¡ä»¶ã‚’è¦‹ç›´ã—ã€ç›£è¦–ã‚’å¼·åŒ–ã€‚ç®¡ç†è€…ã«å ±å‘Šã€‚",
                'Medium': "ç¶™ç¶šç›£è¦–ã€‚ç•°å¸¸å…†å€™ã«æ³¨æ„ã€‚",
                'Low': "é€šå¸¸é‹è»¢ç¶™ç¶šã€‚å®šæœŸç›£è¦–ã‚’ç¶­æŒã€‚"
            }
            return recommendations.get(risk_level, "")
    
    # å®Ÿä½¿ç”¨ä¾‹ï¼ˆå®Œå…¨ãªçµ±åˆã‚·ã‚¹ãƒ†ãƒ ï¼‰
    isms = IntegratedSafetyManagementSystem()
    
    # 1. ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    safety_limits_config = {
        'Reactor Temperature': SafetyLimit(50, 60, 70, 180, 190, 200),
        'Reactor Pressure': SafetyLimit(0.5, 1.0, 1.5, 8.0, 9.0, 10.0)
    }
    isms.initialize_monitoring(safety_limits_config)
    
    # 2. å…ˆè¡ŒæŒ‡æ¨™åˆæœŸåŒ–
    indicators = [
        LeadingIndicator('Near Miss Reports', 10, 'reports/month', 'higher'),
        LeadingIndicator('Safety Training Hours', 8, 'hours/person/month', 'higher')
    ]
    isms.initialize_leading_indicators(indicators)
    
    # 3. ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿æ›´æ–°
    isms.safety_monitor.update_value('Reactor Temperature', 150)
    isms.safety_monitor.update_value('Reactor Pressure', 5.0)
    
    # 4. å…ˆè¡ŒæŒ‡æ¨™æ›´æ–°
    isms.leading_indicator_tracker.update_indicator('Near Miss Reports', 15)
    isms.leading_indicator_tracker.update_indicator('Safety Training Hours', 9.5)
    
    # 5. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”Ÿæˆ
    print(isms.generate_safety_dashboard())
    
    # 6. äº‹æ•…ãƒªã‚¹ã‚¯äºˆæ¸¬ï¼ˆè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹å ´åˆï¼‰
    # current_conditions = np.array([[10, 0.8, 3, 5, 8, 10, 15, 12]])
    # risk_prediction = isms.predict_incident_risk(current_conditions)
    # print(f"\nã€äº‹æ•…ãƒªã‚¹ã‚¯äºˆæ¸¬ã€‘")
    # print(f"ãƒªã‚¹ã‚¯ç¢ºç‡: {risk_prediction['risk_probability']*100:.1f}%")
    # print(f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {risk_prediction['risk_level']}")
    # print(f"æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {risk_prediction['recommendation']}")
    
    # å‡ºåŠ›ä¾‹:
    # ======================================================================
    #                         çµ±åˆå®‰å…¨ç®¡ç†ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
    # ======================================================================
    #
    # ã€ãƒ—ãƒ­ã‚»ã‚¹å®‰å…¨çŠ¶æ…‹ã€‘
    #   âœ“ Reactor Temperature: 150.0 (Normal)
    #   âœ“ Reactor Pressure: 5.0 (Normal)
    #
    # ã€å…ˆè¡ŒæŒ‡æ¨™ã€‘
    #   âœ“ Near Miss Reports: 15.0 (Improving)
    #   âœ“ Safety Training Hours: 9.5 (Improving)
    #
    #   ç·åˆãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢: 12.5/100
    

## å­¦ç¿’ç›®æ¨™ã®ç¢ºèª

ã“ã®ç« ã‚’å®Œäº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

  * âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®‰å…¨æ€§ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã§ãã‚‹
  * âœ… å¤šå¤‰é‡çµ±è¨ˆçš„ãƒ—ãƒ­ã‚»ã‚¹ç®¡ç†ï¼ˆMSPCï¼‰ã§ç•°å¸¸ã‚’æ¤œçŸ¥ã§ãã‚‹
  * âœ… å…ˆè¡ŒæŒ‡æ¨™ã‚’è¿½è·¡ã—ã€äºˆé˜²çš„å®‰å…¨ç®¡ç†ãŒã§ãã‚‹
  * âœ… æ©Ÿæ¢°å­¦ç¿’ã§äº‹æ•…ãƒªã‚¹ã‚¯ã‚’äºˆæ¸¬ã§ãã‚‹
  * âœ… TRIRã€LTIRç­‰ã®å®‰å…¨æ€§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚’è¨ˆç®—ã§ãã‚‹
  * âœ… ãƒ‹ã‚¢ãƒŸã‚¹ã‚’åˆ†æã—ã€ãƒã‚¤ãƒ³ãƒªãƒƒãƒ’ã®æ³•å‰‡ã‚’æ´»ç”¨ã§ãã‚‹
  * âœ… çµ±åˆå®‰å…¨ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã€ç·åˆçš„ã«è©•ä¾¡ã§ãã‚‹

[ãƒ—ãƒ­ã‚»ã‚¹å®‰å…¨æ€§è©•ä¾¡å…¥é–€ ç›®æ¬¡ã«æˆ»ã‚‹](<index.html>)

[PIå®Ÿè·µæŠ€è¡“ãƒˆãƒƒãƒ—](<../../index.html>) | [Homeã«æˆ»ã‚‹](<../../index.html>)

### å…è²¬äº‹é …

  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹Code examplesã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚
  * å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚
