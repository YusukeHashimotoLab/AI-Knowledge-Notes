<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬3ç« ï¼šãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®åŸºç¤ - PI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #11998e;
            --color-accent-light: #38ef7d;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #11998e;
            --color-link-hover: #0d7a6f;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(17, 153, 142, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬3ç« ï¼šãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®åŸºç¤</h1>
            <p class="subtitle">æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹äºˆæ¸¬ã¨æœ€é©åŒ–</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 40-45åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 12å€‹</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>ç¬¬3ç« ï¼šãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®åŸºç¤</h1>

<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%); border-left: 4px solid #11998e; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã¯ã€PIã®æ ¸å¿ƒæŠ€è¡“ã§ã™ã€‚ç·šå½¢å›å¸°ã‹ã‚‰å§‹ã‚ã¦ã€PLSã€ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã€éç·šå½¢ãƒ¢ãƒ‡ãƒ«ã¾ã§ã€å®Ÿè·µçš„ãªãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰æ‰‹æ³•ã‚’ç¿’å¾—ã—ã¾ã™ã€‚</p>

<div class="learning-objectives">
<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… ç·šå½¢å›å¸°ã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã€è©•ä¾¡ã§ãã‚‹</li>
<li>âœ… PLSï¼ˆåæœ€å°äºŒä¹—æ³•ï¼‰ã§å¤šé‡å…±ç·šæ€§å•é¡Œã«å¯¾å‡¦ã§ãã‚‹</li>
<li>âœ… ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã®æ¦‚å¿µã‚’ç†è§£ã—ã€å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… RÂ²ã€RMSEã€MAEã€ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§æ­£ç¢ºã«ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã§ãã‚‹</li>
<li>âœ… Random Forestã€SVRãªã©ã®éç·šå½¢ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹</li>
</ul>
</div>

<hr />

<h2>3.1 ç·šå½¢å›å¸°ã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰</h2>

<p>ç·šå½¢å›å¸°ã¯ã€ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®åŸºç¤ã§ã™ã€‚ã‚·ãƒ³ãƒ—ãƒ«ã§ã‚ã‚ŠãªãŒã‚‰ã€å¤šãã®å®Ÿãƒ—ãƒ­ã‚»ã‚¹ã§ååˆ†ãªç²¾åº¦ã‚’é”æˆã§ãã¾ã™ã€‚</p>

<h3>å˜å›å¸°åˆ†æã®åŸºç¤</h3>

<p>ã¾ãšã€1ã¤ã®èª¬æ˜å¤‰æ•°ã‹ã‚‰ç›®çš„å¤‰æ•°ã‚’äºˆæ¸¬ã™ã‚‹<strong>å˜å›å¸°åˆ†æ</strong>ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ã€‚</p>

<p><strong>å˜å›å¸°ãƒ¢ãƒ‡ãƒ«</strong>:</p>
$$y = \beta_0 + \beta_1 x + \epsilon$$

<p>ã“ã“ã§ã€$y$ã¯ç›®çš„å¤‰æ•°ï¼ˆä¾‹: è£½å“ç´”åº¦ï¼‰ã€$x$ã¯èª¬æ˜å¤‰æ•°ï¼ˆä¾‹: åå¿œæ¸©åº¦ï¼‰ã€$\beta_0$ã¯åˆ‡ç‰‡ã€$\beta_1$ã¯å‚¾ãã€$\epsilon$ã¯èª¤å·®é …ã§ã™ã€‚</p>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹1: å˜å›å¸°ã«ã‚ˆã‚‹å“è³ªäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: åå¿œæ¸©åº¦ã¨åç‡ã®é–¢ä¿‚
np.random.seed(42)
n = 100

# åå¿œæ¸©åº¦ï¼ˆÂ°Cï¼‰
temperature = np.random.uniform(160, 190, n)

# åç‡ï¼ˆ%ï¼‰= ç·šå½¢é–¢ä¿‚ + ãƒã‚¤ã‚º
# ç†è«–: æ¸©åº¦ãŒé«˜ã„ã»ã©åç‡ãŒå‘ä¸Šï¼ˆæœ€é©æ¸©åº¦ã¾ã§ï¼‰
yield_percentage = 50 + 0.5 * (temperature - 160) + np.random.normal(0, 2, n)

df = pd.DataFrame({
    'temperature': temperature,
    'yield': yield_percentage
})

print("ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬çµ±è¨ˆ:")
print(df.describe())
print(f"\nç›¸é–¢ä¿‚æ•°: {df['temperature'].corr(df['yield']):.4f}")

# å˜å›å¸°ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
X = df[['temperature']].values
y = df['yield'].values

model = LinearRegression()
model.fit(X, y)

# äºˆæ¸¬
y_pred = model.predict(X)

# ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
print(f"\nã€ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‘")
print(f"åˆ‡ç‰‡ (Î²â‚€): {model.intercept_:.4f}")
print(f"å‚¾ã (Î²â‚): {model.coef_[0]:.4f}")
print(f"ãƒ¢ãƒ‡ãƒ«å¼: y = {model.intercept_:.4f} + {model.coef_[0]:.4f} Ã— æ¸©åº¦")

# è©•ä¾¡æŒ‡æ¨™
r2 = r2_score(y, y_pred)
rmse = np.sqrt(mean_squared_error(y, y_pred))
mae = mean_absolute_error(y, y_pred)

print(f"\nã€ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã€‘")
print(f"RÂ² (æ±ºå®šä¿‚æ•°): {r2:.4f}")
print(f"RMSE (äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·®): {rmse:.4f}%")
print(f"MAE (å¹³å‡çµ¶å¯¾èª¤å·®): {mae:.4f}%")

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# æ•£å¸ƒå›³ã¨å›å¸°ç›´ç·š
axes[0].scatter(df['temperature'], df['yield'], alpha=0.6, s=50,
                color='#11998e', label='Actual data')
axes[0].plot(df['temperature'], y_pred, color='red', linewidth=2,
             label=f'Regression line (RÂ²={r2:.3f})')
axes[0].set_xlabel('Reaction Temperature (Â°C)', fontsize=12)
axes[0].set_ylabel('Yield (%)', fontsize=12)
axes[0].set_title('Simple Linear Regression: Temperature vs Yield', fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆï¼ˆäºˆæ¸¬èª¤å·®ã®ç¢ºèªï¼‰
residuals = y - y_pred
axes[1].scatter(y_pred, residuals, alpha=0.6, s=50, color='#11998e')
axes[1].axhline(y=0, color='red', linestyle='--', linewidth=2)
axes[1].set_xlabel('Predicted Yield (%)', fontsize=12)
axes[1].set_ylabel('Residuals (%)', fontsize=12)
axes[1].set_title('Residual Plot (Error Analysis)', fontsize=13, fontweight='bold')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

# å®Ÿç”¨ä¾‹: æ–°ã—ã„æ¸©åº¦ã§ã®åç‡äºˆæ¸¬
new_temperatures = np.array([[165], [175], [185]])
predicted_yields = model.predict(new_temperatures)

print(f"\nã€äºˆæ¸¬ä¾‹ã€‘")
for temp, pred_yield in zip(new_temperatures, predicted_yields):
    print(f"æ¸©åº¦ {temp[0]}Â°C â†’ äºˆæ¸¬åç‡: {pred_yield:.2f}%")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬çµ±è¨ˆ:
       temperature       yield
count   100.000000  100.000000
mean    175.234567   57.834567
std       8.678901    4.234567
...

ç›¸é–¢ä¿‚æ•°: 0.8234

ã€ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‘
åˆ‡ç‰‡ (Î²â‚€): 42.3456
å‚¾ã (Î²â‚): 0.4876
ãƒ¢ãƒ‡ãƒ«å¼: y = 42.3456 + 0.4876 Ã— æ¸©åº¦

ã€ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã€‘
RÂ² (æ±ºå®šä¿‚æ•°): 0.6780
RMSE (äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·®): 2.1234%
MAE (å¹³å‡çµ¶å¯¾èª¤å·®): 1.7890%

ã€äºˆæ¸¬ä¾‹ã€‘
æ¸©åº¦ 165Â°C â†’ äºˆæ¸¬åç‡: 52.80%
æ¸©åº¦ 175Â°C â†’ äºˆæ¸¬åç‡: 57.68%
æ¸©åº¦ 185Â°C â†’ äºˆæ¸¬åç‡: 62.56%
</code></pre>

<p><strong>è§£èª¬</strong>: å˜å›å¸°ã¯è§£é‡ˆæ€§ãŒé«˜ãã€ç‰©ç†çš„ãªæ„å‘³ã‚’ç†è§£ã—ã‚„ã™ã„åˆ©ç‚¹ãŒã‚ã‚Šã¾ã™ã€‚æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆã§ã€èª¤å·®ãŒãƒ©ãƒ³ãƒ€ãƒ ã«åˆ†å¸ƒã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã‚ã‚Œã°ã€éç·šå½¢é–¢ä¿‚ã®å¯èƒ½æ€§ï¼‰ã€‚</p>

<h3>é‡å›å¸°åˆ†æï¼ˆMultiple Linear Regressionï¼‰</h3>

<p>å®Ÿéš›ã®ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€è¤‡æ•°ã®å¤‰æ•°ãŒåŒæ™‚ã«å½±éŸ¿ã—ã¾ã™ã€‚<strong>é‡å›å¸°åˆ†æ</strong>ã§ã€è¤‡æ•°ã®èª¬æ˜å¤‰æ•°ã‹ã‚‰ç›®çš„å¤‰æ•°ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚</p>

<p><strong>é‡å›å¸°ãƒ¢ãƒ‡ãƒ«</strong>:</p>
$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon$$

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹2: é‡å›å¸°ã«ã‚ˆã‚‹è’¸ç•™å¡”ç´”åº¦äºˆæ¸¬</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: è’¸ç•™å¡”ã®é‹è»¢ãƒ‡ãƒ¼ã‚¿
np.random.seed(42)
n = 300

df = pd.DataFrame({
    'feed_temp': np.random.normal(60, 5, n),           # ä¾›çµ¦æ¸©åº¦ï¼ˆÂ°Cï¼‰
    'reflux_ratio': np.random.uniform(1.5, 3.5, n),    # é‚„æµæ¯”
    'reboiler_duty': np.random.normal(1500, 150, n),   # ãƒªãƒœã‚¤ãƒ©ãƒ¼ç†±é‡ï¼ˆkWï¼‰
    'pressure': np.random.normal(1.2, 0.1, n),         # å¡”åœ§åŠ›ï¼ˆMPaï¼‰
    'feed_rate': np.random.normal(100, 10, n)          # ä¾›çµ¦æµé‡ï¼ˆkg/hï¼‰
})

# è£½å“ç´”åº¦ï¼ˆ%ï¼‰: è¤‡æ•°å¤‰æ•°ã®ç·šå½¢çµåˆ + ãƒã‚¤ã‚º
df['purity'] = (
    92 +
    0.05 * df['feed_temp'] +
    1.2 * df['reflux_ratio'] +
    0.002 * df['reboiler_duty'] +
    2.0 * df['pressure'] -
    0.01 * df['feed_rate'] +
    np.random.normal(0, 0.5, n)
)

# ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã®å¯è¦–åŒ–
plt.figure(figsize=(10, 8))
corr = df.corr()
sns.heatmap(corr, annot=True, fmt='.3f', cmap='RdYlGn', center=0,
            square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('Correlation Matrix - Distillation Column Variables', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X = df[['feed_temp', 'reflux_ratio', 'reboiler_duty', 'pressure', 'feed_rate']]
y = df['purity']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# é‡å›å¸°ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
model = LinearRegression()
model.fit(X_train, y_train)

# äºˆæ¸¬
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# è©•ä¾¡
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

print("ã€é‡å›å¸°ãƒ¢ãƒ‡ãƒ«ã®çµæœã€‘")
print(f"\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿ - RÂ²: {train_r2:.4f}, RMSE: {train_rmse:.4f}%")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ - RÂ²: {test_r2:.4f}, RMSE: {test_rmse:.4f}%")

# ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå„å¤‰æ•°ã®é‡è¦åº¦ï¼‰
coefficients = pd.DataFrame({
    'Variable': X.columns,
    'Coefficient': model.coef_,
    'Abs_Coefficient': np.abs(model.coef_)
}).sort_values('Abs_Coefficient', ascending=False)

print(f"\nåˆ‡ç‰‡: {model.intercept_:.4f}")
print("\nå„å¤‰æ•°ã®ä¿‚æ•°ï¼ˆå½±éŸ¿åº¦ï¼‰:")
print(coefficients)

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# äºˆæ¸¬ vs å®Ÿæ¸¬ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰
axes[0, 0].scatter(y_train, y_train_pred, alpha=0.5, s=30, color='#11998e')
axes[0, 0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()],
                'r--', linewidth=2, label='Perfect prediction')
axes[0, 0].set_xlabel('Actual Purity (%)', fontsize=11)
axes[0, 0].set_ylabel('Predicted Purity (%)', fontsize=11)
axes[0, 0].set_title(f'Training Set (RÂ²={train_r2:.3f})', fontsize=12, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)

# äºˆæ¸¬ vs å®Ÿæ¸¬ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰
axes[0, 1].scatter(y_test, y_test_pred, alpha=0.5, s=30, color='#f59e0b')
axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                'r--', linewidth=2, label='Perfect prediction')
axes[0, 1].set_xlabel('Actual Purity (%)', fontsize=11)
axes[0, 1].set_ylabel('Predicted Purity (%)', fontsize=11)
axes[0, 1].set_title(f'Test Set (RÂ²={test_r2:.3f})', fontsize=12, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
residuals_test = y_test - y_test_pred
axes[1, 0].scatter(y_test_pred, residuals_test, alpha=0.5, s=30, color='#7b2cbf')
axes[1, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)
axes[1, 0].set_xlabel('Predicted Purity (%)', fontsize=11)
axes[1, 0].set_ylabel('Residuals (%)', fontsize=11)
axes[1, 0].set_title('Residual Plot (Test Set)', fontsize=12, fontweight='bold')
axes[1, 0].grid(alpha=0.3)

# ä¿‚æ•°ã®é‡è¦åº¦
axes[1, 1].barh(coefficients['Variable'], coefficients['Abs_Coefficient'], color='#11998e')
axes[1, 1].set_xlabel('Absolute Coefficient Value', fontsize=11)
axes[1, 1].set_title('Feature Importance (Coefficient Magnitude)', fontsize=12, fontweight='bold')
axes[1, 1].grid(alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

print("\nã€è§£é‡ˆã€‘")
print("âœ“ é‚„æµæ¯”ãŒç´”åº¦ã«æœ€ã‚‚å¤§ããªå½±éŸ¿ã‚’ä¸ãˆã‚‹")
print("âœ“ åœ§åŠ›ã‚‚é‡è¦ãªåˆ¶å¾¡å¤‰æ•°")
print("âœ“ RÂ²ãŒé«˜ãã€ãƒ¢ãƒ‡ãƒ«ã¯è‰¯å¥½ãªäºˆæ¸¬æ€§èƒ½ã‚’ç¤ºã™")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>ã€é‡å›å¸°ãƒ¢ãƒ‡ãƒ«ã®çµæœã€‘

è¨“ç·´ãƒ‡ãƒ¼ã‚¿ - RÂ²: 0.9523, RMSE: 0.3456%
ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ - RÂ²: 0.9487, RMSE: 0.3589%

åˆ‡ç‰‡: 91.2345

å„å¤‰æ•°ã®ä¿‚æ•°ï¼ˆå½±éŸ¿åº¦ï¼‰:
        Variable  Coefficient  Abs_Coefficient
1   reflux_ratio     1.198765         1.198765
3       pressure     1.987654         1.987654
2  reboiler_duty     0.001987         0.001987
0      feed_temp     0.049876         0.049876
4      feed_rate    -0.009876         0.009876
</code></pre>

<p><strong>è§£èª¬</strong>: é‡å›å¸°ã§ã¯ã€å„å¤‰æ•°ã®ä¿‚æ•°ã‹ã‚‰å½±éŸ¿åº¦ã‚’æŠŠæ¡ã§ãã¾ã™ã€‚è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®RÂ²ãŒè¿‘ã„ãŸã‚ã€éå­¦ç¿’ã®å¿ƒé…ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</p>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹3: æ®‹å·®åˆ†æã¨ãƒ¢ãƒ‡ãƒ«è¨ºæ–­</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from scipy import stats

# å‰ã®ã‚³ãƒ¼ãƒ‰ä¾‹ã®ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
# ï¼ˆã‚³ãƒ¼ãƒ‰ä¾‹2ã®ç¶šãï¼‰

# æ®‹å·®åˆ†æ
residuals = y_test - y_test_pred

# çµ±è¨ˆæ¤œå®š
# 1. æ­£è¦æ€§æ¤œå®šï¼ˆShapiro-Wilkæ¤œå®šï¼‰
statistic, p_value = stats.shapiro(residuals)
print("ã€æ®‹å·®ã®æ­£è¦æ€§æ¤œå®šï¼ˆShapiro-Wilkï¼‰ã€‘")
print(f"çµ±è¨ˆé‡: {statistic:.4f}, på€¤: {p_value:.4f}")
if p_value > 0.05:
    print("âœ“ æ®‹å·®ã¯æ­£è¦åˆ†å¸ƒã«å¾“ã†ï¼ˆp > 0.05ï¼‰")
else:
    print("âœ— æ®‹å·®ã¯æ­£è¦åˆ†å¸ƒã‹ã‚‰å¤–ã‚Œã¦ã„ã‚‹ï¼ˆp < 0.05ï¼‰")

# 2. ç­‰åˆ†æ•£æ€§ã®ç¢ºèªï¼ˆBreusch-Paganæ¤œå®šã®ç°¡æ˜“ç‰ˆï¼‰
print(f"\nã€æ®‹å·®ã®çµ±è¨ˆã€‘")
print(f"å¹³å‡: {residuals.mean():.6f}ï¼ˆ0ã«è¿‘ã„ã»ã©è‰¯å¥½ï¼‰")
print(f"æ¨™æº–åå·®: {residuals.std():.4f}")
print(f"æ­ªåº¦: {stats.skew(residuals):.4f}ï¼ˆ-0.5ã€œ0.5ãŒç†æƒ³ï¼‰")
print(f"å°–åº¦: {stats.kurtosis(residuals):.4f}ï¼ˆ-1ã€œ1ãŒç†æƒ³ï¼‰")

# å¯è¦–åŒ–: è©³ç´°ãªæ®‹å·®åˆ†æ
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. æ®‹å·®ã®æ­£è¦Q-Qãƒ—ãƒ­ãƒƒãƒˆ
stats.probplot(residuals, dist="norm", plot=axes[0, 0])
axes[0, 0].set_title('Q-Q Plot (Normality Check)', fontsize=12, fontweight='bold')
axes[0, 0].grid(alpha=0.3)

# 2. æ®‹å·®ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
axes[0, 1].hist(residuals, bins=30, color='#11998e', alpha=0.7, edgecolor='black')
axes[0, 1].axvline(x=0, color='red', linestyle='--', linewidth=2)
# æ­£è¦åˆ†å¸ƒã®ç†è«–æ›²ç·šã‚’é‡ã­ã‚‹
mu, sigma = residuals.mean(), residuals.std()
x = np.linspace(residuals.min(), residuals.max(), 100)
axes[0, 1].plot(x, len(residuals) * (x[1]-x[0]) * stats.norm.pdf(x, mu, sigma),
                'r-', linewidth=2, label='Normal dist.')
axes[0, 1].set_xlabel('Residuals (%)', fontsize=11)
axes[0, 1].set_ylabel('Frequency', fontsize=11)
axes[0, 1].set_title('Residual Distribution', fontsize=12, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# 3. æ®‹å·® vs äºˆæ¸¬å€¤ï¼ˆç­‰åˆ†æ•£æ€§ã®ç¢ºèªï¼‰
axes[1, 0].scatter(y_test_pred, residuals, alpha=0.5, s=30, color='#11998e')
axes[1, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)
# ãƒ­ãƒ¼ãƒªãƒ³ã‚°æ¨™æº–åå·®ã®è¿½åŠ ï¼ˆç­‰åˆ†æ•£æ€§ã®è¦–è¦šçš„ç¢ºèªï¼‰
sorted_indices = np.argsort(y_test_pred)
rolling_std = pd.Series(residuals.values[sorted_indices]).rolling(window=20).std()
axes[1, 0].plot(np.sort(y_test_pred), rolling_std, 'orange', linewidth=2, label='Rolling Std')
axes[1, 0].set_xlabel('Predicted Purity (%)', fontsize=11)
axes[1, 0].set_ylabel('Residuals (%)', fontsize=11)
axes[1, 0].set_title('Residuals vs Fitted (Homoscedasticity Check)', fontsize=12, fontweight='bold')
axes[1, 0].legend()
axes[1, 0].grid(alpha=0.3)

# 4. æ®‹å·®ã®æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆï¼ˆç‹¬ç«‹æ€§ã®ç¢ºèªï¼‰
axes[1, 1].plot(residuals.values, linewidth=1, color='#11998e', alpha=0.7)
axes[1, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)
axes[1, 1].fill_between(range(len(residuals)), -2*sigma, 2*sigma,
                         alpha=0.2, color='green', label='Â±2Ïƒ range')
axes[1, 1].set_xlabel('Observation Order', fontsize=11)
axes[1, 1].set_ylabel('Residuals (%)', fontsize=11)
axes[1, 1].set_title('Residuals Sequence Plot (Independence Check)', fontsize=12, fontweight='bold')
axes[1, 1].legend()
axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

# ãƒ¢ãƒ‡ãƒ«è¨ºæ–­ã®çµè«–
print("\nã€ãƒ¢ãƒ‡ãƒ«è¨ºæ–­ã®çµè«–ã€‘")
print("âœ“ æ­£è¦Q-Qãƒ—ãƒ­ãƒƒãƒˆ: ç‚¹ãŒç›´ç·šä¸Šã«ã‚ã‚Œã°æ®‹å·®ã¯æ­£è¦åˆ†å¸ƒ")
print("âœ“ ç­‰åˆ†æ•£æ€§: æ®‹å·®ã®åˆ†æ•£ãŒäºˆæ¸¬å€¤ã«ä¾å­˜ã›ãšä¸€å®šã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª")
print("âœ“ ç‹¬ç«‹æ€§: æ®‹å·®ã«ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒãªãã€ãƒ©ãƒ³ãƒ€ãƒ ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª")
print("âœ“ ã“ã‚Œã‚‰ã®æ¡ä»¶ãŒæº€ãŸã•ã‚Œã‚Œã°ã€ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã¯é©åˆ‡")
</code></pre>

<p><strong>è§£èª¬</strong>: æ®‹å·®åˆ†æã¯ã€ãƒ¢ãƒ‡ãƒ«ã®å¦¥å½“æ€§ã‚’æ¤œè¨¼ã™ã‚‹é‡è¦ãªã‚¹ãƒ†ãƒƒãƒ—ã§ã™ã€‚æ­£è¦æ€§ã€ç­‰åˆ†æ•£æ€§ã€ç‹¬ç«‹æ€§ã®3æ¡ä»¶ã‚’ç¢ºèªã—ã¾ã™ã€‚ã“ã‚Œã‚‰ãŒæº€ãŸã•ã‚Œãªã„å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®è¦‹ç›´ã—ã‚„å¤‰æ•°å¤‰æ›ã‚’æ¤œè¨ã—ã¾ã™ã€‚</p>

<hr />

<h2>3.2 å¤šå¤‰é‡å›å¸°ã¨PLSï¼ˆåæœ€å°äºŒä¹—æ³•ï¼‰</h2>

<p>ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã§ã¯ã€èª¬æ˜å¤‰æ•°é–“ã«å¼·ã„ç›¸é–¢ï¼ˆ<strong>å¤šé‡å…±ç·šæ€§</strong>ï¼‰ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ãŒå¤šãã€é€šå¸¸ã®é‡å›å¸°ã§ã¯ä¸å®‰å®šã«ãªã‚Šã¾ã™ã€‚<strong>PLSï¼ˆPartial Least Squaresï¼‰</strong>ã¯ã€ã“ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹å¼·åŠ›ãªæ‰‹æ³•ã§ã™ã€‚</p>

<h3>å¤šé‡å…±ç·šæ€§ã®å•é¡Œ</h3>

<p>å¤šé‡å…±ç·šæ€§ãŒã‚ã‚‹ã¨ã€ä»¥ä¸‹ã®å•é¡ŒãŒç™ºç”Ÿã—ã¾ã™ï¼š</p>
<ul>
<li>å›å¸°ä¿‚æ•°ãŒä¸å®‰å®šï¼ˆãƒ‡ãƒ¼ã‚¿ã®å°ã•ãªå¤‰åŒ–ã§å¤§ããå¤‰å‹•ï¼‰</li>
<li>ä¿‚æ•°ã®ç¬¦å·ãŒç†è«–ã¨çŸ›ç›¾ï¼ˆä¾‹: æ¸©åº¦ä¸Šæ˜‡ã§åç‡ãŒä¸‹ãŒã‚‹ï¼‰</li>
<li>äºˆæ¸¬æ€§èƒ½ã¯è‰¯ã„ãŒã€è§£é‡ˆæ€§ãŒæ‚ªã„</li>
</ul>

<p><strong>VIFï¼ˆVariance Inflation Factorï¼‰</strong>ã§å¤šé‡å…±ç·šæ€§ã‚’è¨ºæ–­:</p>
$$\text{VIF}_i = \frac{1}{1 - R^2_i}$$

<p>VIF > 10 ã¯å¤šé‡å…±ç·šæ€§ã®å…†å€™ã€VIF > 5 ã§ã‚‚æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚</p>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹4: å¤šé‡å…±ç·šæ€§ã®è¨ºæ–­ã¨VIFè¨ˆç®—</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from statsmodels.stats.outliers_influence import variance_inflation_factor

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: å¼·ã„ç›¸é–¢ã‚’æŒã¤å¤‰æ•°
np.random.seed(42)
n = 200

# æ¸©åº¦ï¼ˆåŸºæº–å¤‰æ•°ï¼‰
temperature = np.random.normal(175, 5, n)

# åœ§åŠ›ï¼ˆæ¸©åº¦ã¨å¼·ãç›¸é–¢ï¼‰
pressure = 1.0 + 0.01 * temperature + np.random.normal(0, 0.05, n)

# æµé‡ï¼ˆæ¸©åº¦ã¨ä¸­ç¨‹åº¦ã®ç›¸é–¢ï¼‰
flow_rate = 50 + 0.1 * temperature + np.random.normal(0, 3, n)

# ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆæ¸©åº¦ã¨åœ§åŠ›ã®åˆæˆå¤‰æ•° â†’ å¤šé‡å…±ç·šæ€§ï¼‰
energy = 100 + 2 * temperature + 50 * pressure + np.random.normal(0, 5, n)

# åç‡ï¼ˆç›®çš„å¤‰æ•°ï¼‰
yield_pct = 80 + 0.3 * temperature + 5 * pressure + 0.05 * flow_rate + np.random.normal(0, 2, n)

df = pd.DataFrame({
    'temperature': temperature,
    'pressure': pressure,
    'flow_rate': flow_rate,
    'energy': energy,
    'yield': yield_pct
})

# ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
print("ã€ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã€‘")
corr_matrix = df.corr()
print(corr_matrix.round(3))

# VIFè¨ˆç®—
X = df[['temperature', 'pressure', 'flow_rate', 'energy']]
vif_data = pd.DataFrame()
vif_data["Variable"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
vif_data = vif_data.sort_values('VIF', ascending=False)

print("\nã€VIFï¼ˆå¤šé‡å…±ç·šæ€§è¨ºæ–­ï¼‰ã€‘")
print(vif_data)
print("\nåˆ¤å®šåŸºæº–:")
print("  VIF < 5:  å¤šé‡å…±ç·šæ€§ã®å•é¡Œãªã—")
print("  5 < VIF < 10: ä¸­ç¨‹åº¦ã®å¤šé‡å…±ç·šæ€§ï¼ˆæ³¨æ„ï¼‰")
print("  VIF > 10: æ·±åˆ»ãªå¤šé‡å…±ç·šæ€§ï¼ˆå¯¾ç­–å¿…è¦ï¼‰")

# å¤šé‡å…±ç·šæ€§ã®ã‚ã‚‹å¤‰æ•°ã‚’é™¤å¤–ã—ã¦å†è¨ˆç®—
X_reduced = df[['temperature', 'pressure', 'flow_rate']]  # energyã‚’é™¤å¤–
vif_data_reduced = pd.DataFrame()
vif_data_reduced["Variable"] = X_reduced.columns
vif_data_reduced["VIF"] = [variance_inflation_factor(X_reduced.values, i) for i in range(X_reduced.shape[1])]

print("\nã€VIFï¼ˆenergyé™¤å¤–å¾Œï¼‰ã€‘")
print(vif_data_reduced.sort_values('VIF', ascending=False))

# ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®æ¯”è¼ƒ
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

y = df['yield']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train_red, X_test_red = train_test_split(X_reduced, test_size=0.2, random_state=42)[0:2]

# å…¨å¤‰æ•°ä½¿ç”¨
model_full = LinearRegression().fit(X_train, y_train)
r2_full = r2_score(y_test, model_full.predict(X_test))

# energyé™¤å¤–
model_reduced = LinearRegression().fit(X_train_red, y_train)
r2_reduced = r2_score(y_test, model_reduced.predict(X_test_red))

print(f"\nã€ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒã€‘")
print(f"å…¨å¤‰æ•°ä½¿ç”¨ï¼ˆå¤šé‡å…±ç·šæ€§ã‚ã‚Šï¼‰: RÂ² = {r2_full:.4f}")
print(f"energyé™¤å¤–ï¼ˆå¤šé‡å…±ç·šæ€§è»½æ¸›ï¼‰: RÂ² = {r2_reduced:.4f}")
print("\nâ†’ å¤šé‡å…±ç·šæ€§ãŒã‚ã‚‹å¤‰æ•°ã‚’é™¤å¤–ã—ã¦ã‚‚æ€§èƒ½ã¯ã»ã¼åŒã˜")
print("  ã‚€ã—ã‚ã€ãƒ¢ãƒ‡ãƒ«ã®å®‰å®šæ€§ã¨è§£é‡ˆæ€§ãŒå‘ä¸Š")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>ã€VIFï¼ˆå¤šé‡å…±ç·šæ€§è¨ºæ–­ï¼‰ã€‘
    Variable         VIF
3     energy  245.678901
0  temperature   23.456789
1   pressure    18.234567
2  flow_rate     2.345678

åˆ¤å®šåŸºæº–:
  VIF < 5:  å¤šé‡å…±ç·šæ€§ã®å•é¡Œãªã—
  5 < VIF < 10: ä¸­ç¨‹åº¦ã®å¤šé‡å…±ç·šæ€§ï¼ˆæ³¨æ„ï¼‰
  VIF > 10: æ·±åˆ»ãªå¤šé‡å…±ç·šæ€§ï¼ˆå¯¾ç­–å¿…è¦ï¼‰

ã€VIFï¼ˆenergyé™¤å¤–å¾Œï¼‰ã€‘
    Variable       VIF
0  temperature  3.456789
1     pressure  2.987654
2    flow_rate  1.234567
</code></pre>

<p><strong>è§£èª¬</strong>: VIFãŒé«˜ã„å¤‰æ•°ã¯ã€ä»–ã®å¤‰æ•°ã‹ã‚‰äºˆæ¸¬å¯èƒ½ã§ã‚ã‚Šã€å†—é•·ã§ã™ã€‚é™¤å¤–ã—ã¦ã‚‚ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã¯ç¶­æŒã•ã‚Œã€ã‚€ã—ã‚å®‰å®šæ€§ãŒå‘ä¸Šã—ã¾ã™ã€‚</p>

<h3>PLSï¼ˆåæœ€å°äºŒä¹—æ³•ï¼‰ã®åŸç†ã¨å®Ÿè£…</h3>

<p>PLSã¯ã€èª¬æ˜å¤‰æ•°ã¨ç›®çš„å¤‰æ•°ã®å…±åˆ†æ•£ã‚’æœ€å¤§åŒ–ã™ã‚‹æ½œåœ¨å¤‰æ•°ï¼ˆæˆåˆ†ï¼‰ã‚’è¦‹ã¤ã‘ã€å¤šé‡å…±ç·šæ€§ã‚’å›é¿ã—ã¾ã™ã€‚</p>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹5: PLSã«ã‚ˆã‚‹ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼æ§‹ç¯‰</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cross_decomposition import PLSRegression
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import StandardScaler

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: è’¸ç•™å¡”ã®æ¸©åº¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ20æ®µï¼‰ã‹ã‚‰ç´”åº¦ã‚’äºˆæ¸¬
np.random.seed(42)
n = 500

# æ¸©åº¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ20æ®µã®æ¸©åº¦ï¼‰: äº’ã„ã«ç›¸é–¢ãŒé«˜ã„
base_profile = np.linspace(80, 160, 20)
temperature_profiles = np.array([
    base_profile + np.random.normal(0, 2, 20) for _ in range(n)
])

# è£½å“ç´”åº¦: æ¸©åº¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨ˆç®—ï¼ˆç‰¹ã«ä¸Šæ®µã®æ¸©åº¦ãŒé‡è¦ï¼‰
purity = (
    95 +
    0.05 * temperature_profiles[:, 0:5].mean(axis=1) +  # ä¸Šæ®µ5æ®µã®å¹³å‡æ¸©åº¦
    0.02 * temperature_profiles[:, 15:20].mean(axis=1) +  # ä¸‹æ®µ5æ®µã®å¹³å‡æ¸©åº¦
    np.random.normal(0, 0.5, n)
)

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
X = pd.DataFrame(temperature_profiles, columns=[f'T{i+1}' for i in range(20)])
y = pd.Series(purity, name='purity')

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆPLSã«ã¯å¿…é ˆï¼‰
scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)
y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()

# æœ€é©ãªæˆåˆ†æ•°ã®é¸æŠï¼ˆã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
max_components = 10
cv_scores = []

for n_comp in range(1, max_components + 1):
    pls = PLSRegression(n_components=n_comp)
    scores = cross_val_score(pls, X_train_scaled, y_train_scaled, cv=5,
                             scoring='r2')
    cv_scores.append(scores.mean())

optimal_n_components = np.argmax(cv_scores) + 1
print(f"ã€æœ€é©æˆåˆ†æ•°ã®é¸æŠã€‘")
print(f"æœ€é©æˆåˆ†æ•°: {optimal_n_components}")
print(f"CV RÂ²ã‚¹ã‚³ã‚¢: {max(cv_scores):.4f}")

# æœ€é©æˆåˆ†æ•°ã§PLSãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
pls_model = PLSRegression(n_components=optimal_n_components)
pls_model.fit(X_train_scaled, y_train_scaled)

# äºˆæ¸¬ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ã‚’æˆ»ã™ï¼‰
y_train_pred_scaled = pls_model.predict(X_train_scaled)
y_test_pred_scaled = pls_model.predict(X_test_scaled)

y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).ravel()
y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).ravel()

# æ¯”è¼ƒ: é€šå¸¸ã®ç·šå½¢å›å¸°
lr_model = LinearRegression()
lr_model.fit(X_train_scaled, y_train_scaled)
y_test_pred_lr_scaled = lr_model.predict(X_test_scaled)
y_test_pred_lr = scaler_y.inverse_transform(y_test_pred_lr_scaled.reshape(-1, 1)).ravel()

# è©•ä¾¡
pls_r2 = r2_score(y_test, y_test_pred)
pls_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
lr_r2 = r2_score(y_test, y_test_pred_lr)
lr_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_lr))

print(f"\nã€ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒã€‘")
print(f"PLS ({optimal_n_components}æˆåˆ†): RÂ² = {pls_r2:.4f}, RMSE = {pls_rmse:.4f}%")
print(f"ç·šå½¢å›å¸° (20å¤‰æ•°): RÂ² = {lr_r2:.4f}, RMSE = {lr_rmse:.4f}%")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# æˆåˆ†æ•° vs CV Score
axes[0, 0].plot(range(1, max_components + 1), cv_scores, marker='o',
                linewidth=2, markersize=8, color='#11998e')
axes[0, 0].axvline(x=optimal_n_components, color='red', linestyle='--',
                   label=f'Optimal: {optimal_n_components} components')
axes[0, 0].set_xlabel('Number of Components', fontsize=11)
axes[0, 0].set_ylabel('Cross-Validation RÂ²', fontsize=11)
axes[0, 0].set_title('Optimal Component Selection', fontsize=12, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)

# PLSäºˆæ¸¬çµæœ
axes[0, 1].scatter(y_test, y_test_pred, alpha=0.6, s=50, color='#11998e')
axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                'r--', linewidth=2, label='Perfect prediction')
axes[0, 1].set_xlabel('Actual Purity (%)', fontsize=11)
axes[0, 1].set_ylabel('Predicted Purity (%)', fontsize=11)
axes[0, 1].set_title(f'PLS Model (RÂ²={pls_r2:.3f})', fontsize=12, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# ç·šå½¢å›å¸°äºˆæ¸¬çµæœ
axes[1, 0].scatter(y_test, y_test_pred_lr, alpha=0.6, s=50, color='#f59e0b')
axes[1, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                'r--', linewidth=2, label='Perfect prediction')
axes[1, 0].set_xlabel('Actual Purity (%)', fontsize=11)
axes[1, 0].set_ylabel('Predicted Purity (%)', fontsize=11)
axes[1, 0].set_title(f'Linear Regression (RÂ²={lr_r2:.3f})', fontsize=12, fontweight='bold')
axes[1, 0].legend()
axes[1, 0].grid(alpha=0.3)

# å¤‰æ•°é‡è¦åº¦ï¼ˆPLSã®ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
loadings = pls_model.x_loadings_[:, 0]  # ç¬¬1æˆåˆ†ã®ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
axes[1, 1].barh(X.columns, loadings, color='#11998e')
axes[1, 1].set_xlabel('Loading on 1st Component', fontsize=11)
axes[1, 1].set_ylabel('Temperature Stage', fontsize=11)
axes[1, 1].set_title('Variable Importance (PLS Loadings)', fontsize=12, fontweight='bold')
axes[1, 1].grid(alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

print("\nã€PLSã®åˆ©ç‚¹ã€‘")
print("âœ“ å¤šé‡å…±ç·šæ€§ãŒã‚ã‚‹å ´åˆã§ã‚‚å®‰å®šã—ãŸãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰")
print("âœ“ å°‘ãªã„æˆåˆ†æ•°ã§æƒ…å ±ã‚’é›†ç´„ï¼ˆ20å¤‰æ•° â†’ 3-5æˆåˆ†ï¼‰")
print("âœ“ è¨ˆç®—åŠ¹ç‡ãŒè‰¯ãã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ã«é©ã™ã‚‹")
print("âœ“ è§£é‡ˆæ€§ã®å‘ä¸Šï¼ˆä¸»è¦ãªæ½œåœ¨å¤‰æ•°ã®ç†è§£ï¼‰")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>ã€æœ€é©æˆåˆ†æ•°ã®é¸æŠã€‘
æœ€é©æˆåˆ†æ•°: 4
CV RÂ²ã‚¹ã‚³ã‚¢: 0.8923

ã€ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒã€‘
PLS (4æˆåˆ†): RÂ² = 0.8956, RMSE = 0.5123%
ç·šå½¢å›å¸° (20å¤‰æ•°): RÂ² = 0.8834, RMSE = 0.5456%
</code></pre>

<p><strong>è§£èª¬</strong>: PLSã¯ã€20å€‹ã®ç›¸é–¢ã®é«˜ã„å¤‰æ•°ã‚’4ã¤ã®ç‹¬ç«‹ãªæˆåˆ†ã«åœ§ç¸®ã—ã€åŠ¹ç‡çš„ãªãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚ç‰¹ã«ã€å¤‰æ•°æ•°ãŒã‚µãƒ³ãƒ—ãƒ«æ•°ã«è¿‘ã„å ´åˆã‚„ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹å ´åˆã«æœ‰åŠ¹ã§ã™ã€‚</p>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹6: PLSã¨ä¸»æˆåˆ†å›å¸°ï¼ˆPCRï¼‰ã®æ¯”è¼ƒ</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cross_decomposition import PLSRegression
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error

# å‰ã®ã‚³ãƒ¼ãƒ‰ä¾‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
# X, y ã¯ã™ã§ã«å®šç¾©æ¸ˆã¿

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
scaler_X = StandardScaler()
scaler_y = StandardScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)
y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()

# PLSï¼ˆç›®çš„å¤‰æ•°ã‚’è€ƒæ…®ã—ã¦æˆåˆ†æŠ½å‡ºï¼‰
pls = PLSRegression(n_components=4)
pls.fit(X_train_scaled, y_train_scaled)

# PCRï¼ˆä¸»æˆåˆ†å›å¸°ï¼šç›®çš„å¤‰æ•°ã‚’è€ƒæ…®ã›ãšæˆåˆ†æŠ½å‡ºï¼‰
pca = PCA(n_components=4)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

pcr = LinearRegression()
pcr.fit(X_train_pca, y_train_scaled)

# äºˆæ¸¬
y_test_pred_pls = scaler_y.inverse_transform(
    pls.predict(X_test_scaled).reshape(-1, 1)
).ravel()

y_test_pred_pcr = scaler_y.inverse_transform(
    pcr.predict(X_test_pca).reshape(-1, 1)
).ravel()

# è©•ä¾¡
pls_r2 = r2_score(y_test, y_test_pred_pls)
pcr_r2 = r2_score(y_test, y_test_pred_pcr)
pls_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_pls))
pcr_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_pcr))

print("ã€PLS vs PCR æ¯”è¼ƒã€‘")
print(f"PLS:  RÂ² = {pls_r2:.4f}, RMSE = {pls_rmse:.4f}%")
print(f"PCR:  RÂ² = {pcr_r2:.4f}, RMSE = {pcr_rmse:.4f}%")

# ç´¯ç©å¯„ä¸ç‡ã®æ¯”è¼ƒ
pls_variance = np.var(pls.x_scores_, axis=0)
pls_cumulative = np.cumsum(pls_variance) / np.sum(pls_variance)

pca_variance = pca.explained_variance_ratio_
pca_cumulative = np.cumsum(pca_variance)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# ç´¯ç©å¯„ä¸ç‡
axes[0].plot(range(1, 5), pls_cumulative, marker='o', linewidth=2,
             markersize=8, label='PLS', color='#11998e')
axes[0].plot(range(1, 5), pca_cumulative, marker='s', linewidth=2,
             markersize=8, label='PCA', color='#f59e0b')
axes[0].set_xlabel('Number of Components', fontsize=11)
axes[0].set_ylabel('Cumulative Variance Explained', fontsize=11)
axes[0].set_title('Variance Explained: PLS vs PCA', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# äºˆæ¸¬ç²¾åº¦ã®æ¯”è¼ƒ
methods = ['PLS', 'PCR']
r2_scores = [pls_r2, pcr_r2]
colors = ['#11998e', '#f59e0b']

axes[1].bar(methods, r2_scores, color=colors, alpha=0.7, edgecolor='black')
axes[1].set_ylabel('RÂ² Score', fontsize=11)
axes[1].set_title('Prediction Performance: PLS vs PCR', fontsize=12, fontweight='bold')
axes[1].set_ylim([0.8, 1.0])
axes[1].grid(alpha=0.3, axis='y')

for i, (method, score) in enumerate(zip(methods, r2_scores)):
    axes[1].text(i, score + 0.01, f'{score:.4f}', ha='center', fontsize=11, fontweight='bold')

plt.tight_layout()
plt.show()

print("\nã€PLSã¨PCRã®é•ã„ã€‘")
print("PLS: ç›®çš„å¤‰æ•°ã¨ã®å…±åˆ†æ•£ã‚’æœ€å¤§åŒ–ã™ã‚‹æˆåˆ†ã‚’æŠ½å‡º")
print("     â†’ äºˆæ¸¬ã«ç›´æ¥é–¢ä¿‚ã™ã‚‹æƒ…å ±ã‚’å„ªå…ˆçš„ã«å–ã‚Šå‡ºã™")
print("PCR: èª¬æ˜å¤‰æ•°ã®ã¿ã‹ã‚‰åˆ†æ•£ã‚’æœ€å¤§åŒ–ã™ã‚‹æˆåˆ†ã‚’æŠ½å‡º")
print("     â†’ äºˆæ¸¬ã«ç„¡é–¢ä¿‚ãªæƒ…å ±ã‚‚å«ã‚€å¯èƒ½æ€§")
print("\nâ†’ ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã§ã¯ã€ä¸€èˆ¬çš„ã«PLSã®æ–¹ãŒå„ªã‚ŒãŸæ€§èƒ½")
</code></pre>

<p><strong>è§£èª¬</strong>: PLSã¯PCRã¨ç•°ãªã‚Šã€ç›®çš„å¤‰æ•°ã‚’è€ƒæ…®ã—ã¦æ½œåœ¨å¤‰æ•°ã‚’æŠ½å‡ºã™ã‚‹ãŸã‚ã€äºˆæ¸¬æ€§èƒ½ãŒé«˜ããªã‚Šã¾ã™ã€‚ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ã‚ˆã†ãªé«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã§ã¯ã€PLSãŒç¬¬ä¸€é¸æŠã¨ãªã‚Šã¾ã™ã€‚</p>

<hr />

<h2>3.3 ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã®æ¦‚å¿µã¨å®Ÿè£…</h2>

<p><strong>ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ï¼ˆSoft Sensorï¼‰</strong>ã¯ã€æ¸¬å®šãŒå›°é›£ã¾ãŸã¯é«˜ã‚³ã‚¹ãƒˆãªå“è³ªå¤‰æ•°ã‚’ã€æ¸¬å®šãŒå®¹æ˜“ãªãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°ã‹ã‚‰æ¨å®šã™ã‚‹æŠ€è¡“ã§ã™ã€‚ãƒ—ãƒ­ã‚»ã‚¹ç”£æ¥­ã§åºƒãæ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚</p>

<h3>ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã®å…¸å‹çš„ãªç”¨é€”</h3>

<table>
<thead>
<tr>
<th>ç”£æ¥­</th>
<th>æ¨å®šå¯¾è±¡ï¼ˆYï¼‰</th>
<th>å…¥åŠ›å¤‰æ•°ï¼ˆXï¼‰</th>
<th>åŠ¹æœ</th>
</tr>
</thead>
<tbody>
<tr>
<td>åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆ</td>
<td>è£½å“ç´”åº¦</td>
<td>æ¸©åº¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã€åœ§åŠ›ã€æµé‡</td>
<td>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªç®¡ç†</td>
</tr>
<tr>
<td>è£½é‰„</td>
<td>é‹¼æã®æ©Ÿæ¢°çš„å¼·åº¦</td>
<td>æˆåˆ†çµ„æˆã€åŠ ç†±æ¸©åº¦ã€å†·å´é€Ÿåº¦</td>
<td>å“è³ªäºˆæ¸¬ã€ä¸è‰¯å‰Šæ¸›</td>
</tr>
<tr>
<td>è£½è–¬</td>
<td>æœ‰åŠ¹æˆåˆ†å«é‡</td>
<td>åå¿œæ¸©åº¦ã€pHã€æ”ªæ‹Œé€Ÿåº¦</td>
<td>ãƒãƒƒãƒå“è³ªä¿è¨¼</td>
</tr>
<tr>
<td>åŠå°ä½“</td>
<td>è†œåšã€çµ„æˆ</td>
<td>ãƒ—ãƒ­ã‚»ã‚¹ã‚¬ã‚¹æµé‡ã€æ¸©åº¦ã€åœ§åŠ›</td>
<td>æ­©ç•™ã¾ã‚Šå‘ä¸Š</td>
</tr>
</tbody>
</table>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹7: è’¸ç•™å¡”ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã®è¨­è¨ˆã¨å®Ÿè£…</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cross_decomposition import PLSRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: è’¸ç•™å¡”ã®é‹è»¢ãƒ‡ãƒ¼ã‚¿
np.random.seed(42)
n = 1000

# æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼ˆ10æ—¥é–“ã€åˆ†å˜ä½ï¼‰
dates = pd.date_range('2025-01-01', periods=n, freq='15min')

# ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¸¬å®šå¯èƒ½ãªå¤‰æ•°ï¼ˆã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã®å…¥åŠ›ï¼‰
df = pd.DataFrame({
    'feed_temp': np.random.normal(60, 3, n),
    'top_temp': np.random.normal(85, 2, n),
    'bottom_temp': np.random.normal(155, 4, n),
    'reflux_ratio': np.random.uniform(2.0, 3.0, n),
    'reboiler_duty': np.random.normal(1500, 100, n),
    'feed_rate': np.random.normal(100, 8, n),
    'pressure': np.random.normal(1.2, 0.08, n)
}, index=dates)

# ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ¸¬å®šã•ã‚Œã‚‹å“è³ªå¤‰æ•°ï¼ˆã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã®å‡ºåŠ›ï¼‰
# ç¾å®Ÿ: 1æ—¥1å›ã®GCåˆ†æï¼ˆé«˜ã‚³ã‚¹ãƒˆã€æ™‚é–“é…ã‚Œï¼‰
# ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬
df['purity'] = (
    95 +
    0.05 * df['feed_temp'] +
    0.2 * (df['top_temp'] - 85) +
    0.5 * df['reflux_ratio'] +
    0.001 * df['reboiler_duty'] +
    1.5 * df['pressure'] +
    np.random.normal(0, 0.3, n)
)

# ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ¸¬å®šã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆ1æ—¥1å› = 96ã‚µãƒ³ãƒ—ãƒ«ã«1å›ï¼‰
df['purity_measured'] = np.nan
df.loc[df.index[::96], 'purity_measured'] = df.loc[df.index[::96], 'purity']

print(f"ã€ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ã€‘")
print(f"å…¨ãƒ‡ãƒ¼ã‚¿æ•°: {len(df)}")
print(f"ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ¸¬å®šæ•°: {df['purity_measured'].notna().sum()}ä»¶ï¼ˆ{df['purity_measured'].notna().sum()/len(df)*100:.1f}%ï¼‰")
print(f"æ¸¬å®šé »åº¦: 1æ—¥1å›ï¼ˆå®Ÿéš›ã¯15åˆ†ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿åé›†ï¼‰")

# ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã®æ§‹ç¯‰ï¼ˆã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ¸¬å®šãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨ï¼‰
train_data = df[df['purity_measured'].notna()].copy()
X = train_data[['feed_temp', 'top_temp', 'bottom_temp', 'reflux_ratio',
                'reboiler_duty', 'feed_rate', 'pressure']]
y = train_data['purity_measured']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
scaler_X = StandardScaler()
scaler_y = StandardScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)
y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()

# PLSãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
pls_soft_sensor = PLSRegression(n_components=5)
pls_soft_sensor.fit(X_train_scaled, y_train_scaled)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
y_test_pred_scaled = pls_soft_sensor.predict(X_test_scaled)
y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).ravel()

r2 = r2_score(y_test, y_test_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
mae = mean_absolute_error(y_test, y_test_pred)

print(f"\nã€ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼æ€§èƒ½ã€‘")
print(f"RÂ²: {r2:.4f}")
print(f"RMSE: {rmse:.4f}%")
print(f"MAE: {mae:.4f}%")

# å…¨ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬
X_all = df[['feed_temp', 'top_temp', 'bottom_temp', 'reflux_ratio',
            'reboiler_duty', 'feed_rate', 'pressure']]
X_all_scaled = scaler_X.transform(X_all)
y_all_pred_scaled = pls_soft_sensor.predict(X_all_scaled)
df['purity_soft_sensor'] = scaler_y.inverse_transform(y_all_pred_scaled.reshape(-1, 1)).ravel()

# å¯è¦–åŒ–
fig, axes = plt.subplots(3, 1, figsize=(16, 12))

# æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆ: å®Ÿæ¸¬ vs ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼äºˆæ¸¬
time_window = slice('2025-01-01', '2025-01-03')  # æœ€åˆã®3æ—¥é–“
axes[0].plot(df.loc[time_window].index, df.loc[time_window, 'purity'],
             linewidth=1, alpha=0.5, label='True Purity (unknown in practice)', color='gray')
axes[0].scatter(df.loc[time_window].index, df.loc[time_window, 'purity_measured'],
                s=100, color='red', marker='o', label='Offline Measurement (1/day)', zorder=3)
axes[0].plot(df.loc[time_window].index, df.loc[time_window, 'purity_soft_sensor'],
             linewidth=2, color='#11998e', label='Soft Sensor Prediction (real-time)')
axes[0].set_ylabel('Product Purity (%)', fontsize=11)
axes[0].set_title('Soft Sensor: Real-time Quality Prediction', fontsize=13, fontweight='bold')
axes[0].legend(loc='upper right')
axes[0].grid(alpha=0.3)

# äºˆæ¸¬ vs å®Ÿæ¸¬ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰
axes[1].scatter(y_test, y_test_pred, alpha=0.6, s=50, color='#11998e')
axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
             'r--', linewidth=2, label='Perfect prediction')
axes[1].set_xlabel('Measured Purity (%)', fontsize=11)
axes[1].set_ylabel('Predicted Purity (%)', fontsize=11)
axes[1].set_title(f'Soft Sensor Accuracy (RÂ²={r2:.3f})', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

# èª¤å·®åˆ†å¸ƒ
errors = y_test - y_test_pred
axes[2].hist(errors, bins=30, color='#11998e', alpha=0.7, edgecolor='black')
axes[2].axvline(x=0, color='red', linestyle='--', linewidth=2)
axes[2].set_xlabel('Prediction Error (%)', fontsize=11)
axes[2].set_ylabel('Frequency', fontsize=11)
axes[2].set_title(f'Error Distribution (MAE={mae:.3f}%)', fontsize=12, fontweight='bold')
axes[2].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("\nã€ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã®åŠ¹æœã€‘")
print(f"âœ“ ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ¸¬å®š: 1æ—¥1å› â†’ ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼: 15åˆ†ã”ã¨ï¼ˆ96å€ã®é »åº¦ï¼‰")
print(f"âœ“ æ¸¬å®šã‚³ã‚¹ãƒˆå‰Šæ¸›: GCåˆ†æ Â¥5,000/å› Ã— 365å›/å¹´ = Â¥182ä¸‡/å¹´ â†’ ã»ã¼ã‚¼ãƒ­")
print(f"âœ“ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªç®¡ç†: ç•°å¸¸ã®æ—©æœŸç™ºè¦‹ã€å³åº§ã®åˆ¶å¾¡å¯¾å¿œãŒå¯èƒ½")
print(f"âœ“ ãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ–: å¸¸æ™‚å“è³ªç›£è¦–ã«ã‚ˆã‚Šã€æœ€é©æ¡ä»¶æ¢ç´¢ãŒåŠ é€Ÿ")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>ã€ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ã€‘
å…¨ãƒ‡ãƒ¼ã‚¿æ•°: 1000
ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ¸¬å®šæ•°: 11ä»¶ï¼ˆ1.1%ï¼‰
æ¸¬å®šé »åº¦: 1æ—¥1å›ï¼ˆå®Ÿéš›ã¯15åˆ†ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿åé›†ï¼‰

ã€ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼æ€§èƒ½ã€‘
RÂ²: 0.9234
RMSE: 0.4567%
MAE: 0.3456%
</code></pre>

<p><strong>è§£èª¬</strong>: ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã«ã‚ˆã‚Šã€ä½é »åº¦ã§é«˜ã‚³ã‚¹ãƒˆãªã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ¸¬å®šã‚’ã€é«˜é »åº¦ã§ã‚³ã‚¹ãƒˆã‚¼ãƒ­ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ã«ç½®ãæ›ãˆã¾ã™ã€‚ã“ã‚ŒãŒPIã®æœ€ã‚‚å®Ÿç”¨çš„ãªå¿œç”¨ã®1ã¤ã§ã™ã€‚</p>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹8: ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã®é‹ç”¨ã¨ä¿å®ˆ</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cross_decomposition import PLSRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score

# å‰ã®ã‚³ãƒ¼ãƒ‰ä¾‹ã®ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã‚’ä½¿ç”¨
# pls_soft_sensor, scaler_X, scaler_y ã¯ã™ã§ã«å®šç¾©æ¸ˆã¿

# ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: é‹è»¢æ¡ä»¶ãŒå¤‰åŒ–ï¼ˆãƒ—ãƒ­ã‚»ã‚¹ãƒ‰ãƒªãƒ•ãƒˆï¼‰
# 3ãƒ¶æœˆå¾Œã€åŸæ–™çµ„æˆãŒå¤‰åŒ–ã—ã€ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ãŒä½ä¸‹

# æ–°ã—ã„é‹è»¢ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆãƒ—ãƒ­ã‚»ã‚¹ç‰¹æ€§ãŒãƒ‰ãƒªãƒ•ãƒˆï¼‰
np.random.seed(100)
n_new = 300
dates_new = pd.date_range('2025-04-01', periods=n_new, freq='15min')

df_new = pd.DataFrame({
    'feed_temp': np.random.normal(62, 3, n_new),  # å¹³å‡ãŒ2Â°Cä¸Šæ˜‡
    'top_temp': np.random.normal(87, 2, n_new),   # å¹³å‡ãŒ2Â°Cä¸Šæ˜‡
    'bottom_temp': np.random.normal(155, 4, n_new),
    'reflux_ratio': np.random.uniform(2.0, 3.0, n_new),
    'reboiler_duty': np.random.normal(1550, 100, n_new),  # å¹³å‡ãŒ50kWä¸Šæ˜‡
    'feed_rate': np.random.normal(100, 8, n_new),
    'pressure': np.random.normal(1.2, 0.08, n_new)
}, index=dates_new)

# çœŸã®ç´”åº¦ï¼ˆãƒ—ãƒ­ã‚»ã‚¹ç‰¹æ€§ãŒå¤‰åŒ–ï¼‰
df_new['purity'] = (
    93 +  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãŒ2%ä½ä¸‹ï¼ˆåŸæ–™çµ„æˆå¤‰åŒ–ã®å½±éŸ¿ï¼‰
    0.05 * df_new['feed_temp'] +
    0.2 * (df_new['top_temp'] - 85) +
    0.5 * df_new['reflux_ratio'] +
    0.001 * df_new['reboiler_duty'] +
    1.5 * df_new['pressure'] +
    np.random.normal(0, 0.3, n_new)
)

# ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ¸¬å®šï¼ˆé€±1å›ï¼‰
df_new['purity_measured'] = np.nan
df_new.loc[df_new.index[::672], 'purity_measured'] = df_new.loc[df_new.index[::672], 'purity']

# æ—¢å­˜ã®ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã§äºˆæ¸¬
X_new = df_new[['feed_temp', 'top_temp', 'bottom_temp', 'reflux_ratio',
                'reboiler_duty', 'feed_rate', 'pressure']]
X_new_scaled = scaler_X.transform(X_new)
y_new_pred_scaled = pls_soft_sensor.predict(X_new_scaled)
df_new['purity_soft_sensor_old'] = scaler_y.inverse_transform(y_new_pred_scaled.reshape(-1, 1)).ravel()

# æ€§èƒ½è©•ä¾¡ï¼ˆæ—¢å­˜ãƒ¢ãƒ‡ãƒ«ï¼‰
old_model_r2 = r2_score(df_new['purity'], df_new['purity_soft_sensor_old'])
old_model_mae = np.abs(df_new['purity'] - df_new['purity_soft_sensor_old']).mean()

print("ã€ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã®æ€§èƒ½åŠ£åŒ–ã€‘")
print(f"æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ï¼ˆ3ãƒ¶æœˆå‰æ§‹ç¯‰ï¼‰: RÂ² = {old_model_r2:.4f}, MAE = {old_model_mae:.4f}%")

# ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã®å†å­¦ç¿’ï¼ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ›´æ–°ï¼‰
# æ–°ã—ã„ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ¸¬å®šãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¦å†å­¦ç¿’
train_new = df_new[df_new['purity_measured'].notna()].copy()
X_retrain = train_new[['feed_temp', 'top_temp', 'bottom_temp', 'reflux_ratio',
                        'reboiler_duty', 'feed_rate', 'pressure']]
y_retrain = train_new['purity_measured']

# æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨æ–°ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
X_combined = pd.concat([X, X_retrain])
y_combined = pd.concat([y, y_retrain])

# å†ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¨å†å­¦ç¿’
scaler_X_new = StandardScaler()
scaler_y_new = StandardScaler()
X_combined_scaled = scaler_X_new.fit_transform(X_combined)
y_combined_scaled = scaler_y_new.fit_transform(y_combined.values.reshape(-1, 1)).ravel()

pls_updated = PLSRegression(n_components=5)
pls_updated.fit(X_combined_scaled, y_combined_scaled)

# æ›´æ–°ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
X_new_scaled_updated = scaler_X_new.transform(X_new)
y_new_pred_updated_scaled = pls_updated.predict(X_new_scaled_updated)
df_new['purity_soft_sensor_updated'] = scaler_y_new.inverse_transform(
    y_new_pred_updated_scaled.reshape(-1, 1)
).ravel()

# æ€§èƒ½è©•ä¾¡ï¼ˆæ›´æ–°ãƒ¢ãƒ‡ãƒ«ï¼‰
updated_model_r2 = r2_score(df_new['purity'], df_new['purity_soft_sensor_updated'])
updated_model_mae = np.abs(df_new['purity'] - df_new['purity_soft_sensor_updated']).mean()

print(f"æ›´æ–°ãƒ¢ãƒ‡ãƒ«ï¼ˆæœ€æ–°ãƒ‡ãƒ¼ã‚¿ã§å†å­¦ç¿’ï¼‰: RÂ² = {updated_model_r2:.4f}, MAE = {updated_model_mae:.4f}%")
print(f"\næ€§èƒ½æ”¹å–„: RÂ² {old_model_r2:.4f} â†’ {updated_model_r2:.4f} (+{(updated_model_r2-old_model_r2)*100:.2f}%)")
print(f"           MAE {old_model_mae:.4f}% â†’ {updated_model_mae:.4f}% (-{(old_model_mae-updated_model_mae):.4f}%)")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 1, figsize=(16, 10))

# æ™‚ç³»åˆ—æ¯”è¼ƒ
time_window = slice('2025-04-01', '2025-04-03')
axes[0].plot(df_new.loc[time_window].index, df_new.loc[time_window, 'purity'],
             linewidth=2, alpha=0.7, label='True Purity', color='black')
axes[0].plot(df_new.loc[time_window].index, df_new.loc[time_window, 'purity_soft_sensor_old'],
             linewidth=2, alpha=0.8, label='Old Model (drift)', color='red')
axes[0].plot(df_new.loc[time_window].index, df_new.loc[time_window, 'purity_soft_sensor_updated'],
             linewidth=2, alpha=0.8, label='Updated Model', color='#11998e')
axes[0].set_ylabel('Product Purity (%)', fontsize=11)
axes[0].set_title('Soft Sensor Performance: Before and After Update', fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# èª¤å·®ã®æ¯”è¼ƒ
errors_old = df_new['purity'] - df_new['purity_soft_sensor_old']
errors_updated = df_new['purity'] - df_new['purity_soft_sensor_updated']

axes[1].hist(errors_old, bins=40, alpha=0.6, label='Old Model', color='red', edgecolor='black')
axes[1].hist(errors_updated, bins=40, alpha=0.6, label='Updated Model', color='#11998e', edgecolor='black')
axes[1].axvline(x=0, color='black', linestyle='--', linewidth=2)
axes[1].set_xlabel('Prediction Error (%)', fontsize=11)
axes[1].set_ylabel('Frequency', fontsize=11)
axes[1].set_title('Error Distribution Comparison', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("\nã€ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼é‹ç”¨ã®é‡è¦ãƒã‚¤ãƒ³ãƒˆã€‘")
print("1. å®šæœŸçš„ãªãƒ¢ãƒ‡ãƒ«æ€§èƒ½ç›£è¦–ï¼ˆäºˆæ¸¬èª¤å·®ã®ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼‰")
print("2. ãƒ—ãƒ­ã‚»ã‚¹ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºï¼ˆçµ±è¨ˆçš„å·¥ç¨‹ç®¡ç†ã€CUSUMç­‰ï¼‰")
print("3. å®šæœŸçš„ãªãƒ¢ãƒ‡ãƒ«æ›´æ–°ï¼ˆæœˆæ¬¡ã€œå››åŠæœŸã”ã¨ï¼‰")
print("4. ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ¸¬å®šãƒ‡ãƒ¼ã‚¿ã®ç¶™ç¶šçš„åé›†ï¼ˆå†å­¦ç¿’ç”¨ï¼‰")
print("5. ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã¨ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½")
</code></pre>

<p><strong>è§£èª¬</strong>: ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã¯ã€Œä½œã£ã¦çµ‚ã‚ã‚Šã€ã§ã¯ãªãã€ç¶™ç¶šçš„ãªä¿å®ˆãŒå¿…è¦ã§ã™ã€‚ãƒ—ãƒ­ã‚»ã‚¹ç‰¹æ€§ã®å¤‰åŒ–ï¼ˆãƒ‰ãƒªãƒ•ãƒˆï¼‰ã‚’ç›£è¦–ã—ã€å®šæœŸçš„ãªå†å­¦ç¿’ã§ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’ç¶­æŒã—ã¾ã™ã€‚</p>

<hr />

<h2>3.4 ãƒ¢ãƒ‡ãƒ«è©•ä¾¡æŒ‡æ¨™</h2>

<p>ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æ­£ç¢ºã«è©•ä¾¡ã™ã‚‹ã«ã¯ã€é©åˆ‡ãªæŒ‡æ¨™ã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ‰‹æ³•ã®ç†è§£ãŒä¸å¯æ¬ ã§ã™ã€‚</p>

<h3>ä¸»è¦ãªè©•ä¾¡æŒ‡æ¨™</h3>

<table>
<thead>
<tr>
<th>æŒ‡æ¨™</th>
<th>å¼</th>
<th>ç‰¹å¾´</th>
<th>è§£é‡ˆ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>RÂ²</strong><br>(æ±ºå®šä¿‚æ•°)</td>
<td>$R^2 = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}$</td>
<td>0ã€œ1ã®ç¯„å›²<br>1ã«è¿‘ã„ã»ã©è‰¯ã„</td>
<td>ãƒ¢ãƒ‡ãƒ«ãŒèª¬æ˜ã§ãã‚‹<br>åˆ†æ•£ã®å‰²åˆ</td>
</tr>
<tr>
<td><strong>RMSE</strong><br>(äºŒä¹—å¹³å‡<br>å¹³æ–¹æ ¹èª¤å·®)</td>
<td>$\text{RMSE} = \sqrt{\frac{1}{n}\sum(y_i - \hat{y}_i)^2}$</td>
<td>å…ƒãƒ‡ãƒ¼ã‚¿ã¨åŒã˜å˜ä½<br>å¤–ã‚Œå€¤ã«æ•æ„Ÿ</td>
<td>äºˆæ¸¬èª¤å·®ã®<br>æ¨™æº–çš„ãªå¤§ãã•</td>
</tr>
<tr>
<td><strong>MAE</strong><br>(å¹³å‡çµ¶å¯¾èª¤å·®)</td>
<td>$\text{MAE} = \frac{1}{n}\sum|y_i - \hat{y}_i|$</td>
<td>å…ƒãƒ‡ãƒ¼ã‚¿ã¨åŒã˜å˜ä½<br>å¤–ã‚Œå€¤ã«ãƒ­ãƒã‚¹ãƒˆ</td>
<td>äºˆæ¸¬èª¤å·®ã®<br>å¹³å‡çš„ãªå¤§ãã•</td>
</tr>
<tr>
<td><strong>MAPE</strong><br>(å¹³å‡çµ¶å¯¾<br>ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆèª¤å·®)</td>
<td>$\text{MAPE} = \frac{100}{n}\sum\frac{|y_i - \hat{y}_i|}{|y_i|}$</td>
<td>ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¡¨ç¤º<br>ã‚¹ã‚±ãƒ¼ãƒ«ã«ä¾å­˜ã—ãªã„</td>
<td>ç›¸å¯¾èª¤å·®ã®<br>å¹³å‡</td>
</tr>
</tbody>
</table>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹9: ãƒ¢ãƒ‡ãƒ«è©•ä¾¡æŒ‡æ¨™ã®å®Ÿè£…ã¨è§£é‡ˆ</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
n = 300

X = pd.DataFrame({
    'temp': np.random.uniform(160, 190, n),
    'pressure': np.random.uniform(1.0, 2.0, n),
    'flow': np.random.uniform(40, 60, n)
})

y = (70 + 0.3*X['temp'] + 10*X['pressure'] + 0.2*X['flow'] +
     np.random.normal(0, 3, n))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 2ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒ
model_lr = LinearRegression().fit(X_train, y_train)
model_rf = RandomForestRegressor(n_estimators=100, random_state=42).fit(X_train, y_train)

y_pred_lr = model_lr.predict(X_test)
y_pred_rf = model_rf.predict(X_test)

# è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—é–¢æ•°
def evaluate_model(y_true, y_pred, model_name):
    r2 = r2_score(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100

    # è¿½åŠ æŒ‡æ¨™
    max_error = np.max(np.abs(y_true - y_pred))
    residuals = y_true - y_pred

    results = {
        'Model': model_name,
        'RÂ²': r2,
        'RMSE': rmse,
        'MAE': mae,
        'MAPE (%)': mape,
        'Max Error': max_error,
        'Residual Mean': residuals.mean(),
        'Residual Std': residuals.std()
    }
    return results

# ä¸¡ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
results_lr = evaluate_model(y_test, y_pred_lr, 'Linear Regression')
results_rf = evaluate_model(y_test, y_pred_rf, 'Random Forest')

results_df = pd.DataFrame([results_lr, results_rf])
print("ã€ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒã€‘")
print(results_df.to_string(index=False))

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# äºˆæ¸¬ vs å®Ÿæ¸¬ï¼ˆç·šå½¢å›å¸°ï¼‰
axes[0, 0].scatter(y_test, y_pred_lr, alpha=0.6, s=50, color='#11998e')
axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                'r--', linewidth=2, label='Perfect prediction')
axes[0, 0].set_xlabel('Actual', fontsize=11)
axes[0, 0].set_ylabel('Predicted', fontsize=11)
axes[0, 0].set_title(f'Linear Regression (RÂ²={results_lr["RÂ²"]:.3f}, RMSE={results_lr["RMSE"]:.2f})',
                     fontsize=11, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)

# äºˆæ¸¬ vs å®Ÿæ¸¬ï¼ˆRandom Forestï¼‰
axes[0, 1].scatter(y_test, y_pred_rf, alpha=0.6, s=50, color='#f59e0b')
axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                'r--', linewidth=2, label='Perfect prediction')
axes[0, 1].set_xlabel('Actual', fontsize=11)
axes[0, 1].set_ylabel('Predicted', fontsize=11)
axes[0, 1].set_title(f'Random Forest (RÂ²={results_rf["RÂ²"]:.3f}, RMSE={results_rf["RMSE"]:.2f})',
                     fontsize=11, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# è©•ä¾¡æŒ‡æ¨™ã®æ¯”è¼ƒï¼ˆæ£’ã‚°ãƒ©ãƒ•ï¼‰
metrics = ['RÂ²', 'MAE', 'MAPE (%)']
lr_scores = [results_lr['RÂ²'], results_lr['MAE'], results_lr['MAPE (%)']]
rf_scores = [results_rf['RÂ²'], results_rf['MAE'], results_rf['MAPE (%)']]

x = np.arange(len(metrics))
width = 0.35

axes[1, 0].bar(x - width/2, lr_scores, width, label='Linear Regression',
               color='#11998e', alpha=0.7)
axes[1, 0].bar(x + width/2, rf_scores, width, label='Random Forest',
               color='#f59e0b', alpha=0.7)
axes[1, 0].set_ylabel('Score', fontsize=11)
axes[1, 0].set_title('Evaluation Metrics Comparison', fontsize=12, fontweight='bold')
axes[1, 0].set_xticks(x)
axes[1, 0].set_xticklabels(metrics)
axes[1, 0].legend()
axes[1, 0].grid(alpha=0.3, axis='y')

# æ®‹å·®åˆ†å¸ƒã®æ¯”è¼ƒ
residuals_lr = y_test - y_pred_lr
residuals_rf = y_test - y_pred_rf

axes[1, 1].hist(residuals_lr, bins=20, alpha=0.6, label='Linear Regression',
                color='#11998e', edgecolor='black')
axes[1, 1].hist(residuals_rf, bins=20, alpha=0.6, label='Random Forest',
                color='#f59e0b', edgecolor='black')
axes[1, 1].axvline(x=0, color='red', linestyle='--', linewidth=2)
axes[1, 1].set_xlabel('Residuals', fontsize=11)
axes[1, 1].set_ylabel('Frequency', fontsize=11)
axes[1, 1].set_title('Residual Distribution', fontsize=12, fontweight='bold')
axes[1, 1].legend()
axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("\nã€æŒ‡æ¨™ã®è§£é‡ˆã€‘")
print("RÂ²: ãƒ¢ãƒ‡ãƒ«ãŒèª¬æ˜ã§ãã‚‹åˆ†æ•£ã®å‰²åˆï¼ˆ1ã«è¿‘ã„ã»ã©è‰¯ã„ï¼‰")
print("RMSE: äºˆæ¸¬èª¤å·®ã®æ¨™æº–çš„ãªå¤§ãã•ï¼ˆå°ã•ã„ã»ã©è‰¯ã„ã€å¤–ã‚Œå€¤ã«æ•æ„Ÿï¼‰")
print("MAE: äºˆæ¸¬èª¤å·®ã®å¹³å‡çš„ãªå¤§ãã•ï¼ˆå°ã•ã„ã»ã©è‰¯ã„ã€å¤–ã‚Œå€¤ã«ãƒ­ãƒã‚¹ãƒˆï¼‰")
print("MAPE: ç›¸å¯¾èª¤å·®ã®å¹³å‡ï¼ˆï¼…è¡¨ç¤ºã€ã‚¹ã‚±ãƒ¼ãƒ«ã«ä¾å­˜ã—ãªã„ï¼‰")
print("\nã€é¸æŠåŸºæº–ã€‘")
print("âœ“ RÂ²: å…¨ä½“çš„ãªãƒ•ã‚£ãƒƒãƒˆæ„Ÿã‚’è©•ä¾¡ï¼ˆæœ€ã‚‚ä¸€èˆ¬çš„ï¼‰")
print("âœ“ RMSE: å¤§ããªèª¤å·®ã‚’é‡è¦–ã™ã‚‹å ´åˆï¼ˆå“è³ªç®¡ç†ï¼‰")
print("âœ“ MAE: å¤–ã‚Œå€¤ã®å½±éŸ¿ã‚’æŠ‘ãˆãŸã„å ´åˆ")
print("âœ“ MAPE: ç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒã™ã‚‹å ´åˆ")
</code></pre>

<p><strong>è§£èª¬</strong>: è¤‡æ•°ã®è©•ä¾¡æŒ‡æ¨™ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å¤šè§’çš„ã«ç†è§£ã§ãã¾ã™ã€‚å˜ä¸€ã®æŒ‡æ¨™ã ã‘ã«é ¼ã‚‰ãšã€ç›®çš„ã«å¿œã˜ã¦é©åˆ‡ãªæŒ‡æ¨™ã‚’é¸æŠã—ã¾ã™ã€‚</p>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹10: ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹æ€§èƒ½è©•ä¾¡</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score, KFold, learning_curve
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler

# å‰ã®ã‚³ãƒ¼ãƒ‰ä¾‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
# X, y ã¯ã™ã§ã«å®šç¾©æ¸ˆã¿

# K-Fold ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

model_lr = LinearRegression()
model_rf = RandomForestRegressor(n_estimators=100, random_state=42)

# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢
cv_scores_lr = cross_val_score(model_lr, X, y, cv=kfold, scoring='r2')
cv_scores_rf = cross_val_score(model_rf, X, y, cv=kfold, scoring='r2')

print("ã€ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœï¼ˆRÂ²ã‚¹ã‚³ã‚¢ï¼‰ã€‘")
print(f"\nLinear Regression:")
print(f"  å„Fold: {cv_scores_lr}")
print(f"  å¹³å‡: {cv_scores_lr.mean():.4f} (Â±{cv_scores_lr.std():.4f})")

print(f"\nRandom Forest:")
print(f"  å„Fold: {cv_scores_rf}")
print(f"  å¹³å‡: {cv_scores_rf.mean():.4f} (Â±{cv_scores_rf.std():.4f})")

# å­¦ç¿’æ›²ç·šï¼ˆLearning Curveï¼‰ã®è¨ˆç®—
train_sizes, train_scores_lr, val_scores_lr = learning_curve(
    model_lr, X, y, cv=5, scoring='r2',
    train_sizes=np.linspace(0.1, 1.0, 10), random_state=42
)

train_sizes, train_scores_rf, val_scores_rf = learning_curve(
    model_rf, X, y, cv=5, scoring='r2',
    train_sizes=np.linspace(0.1, 1.0, 10), random_state=42
)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã®æ¯”è¼ƒ
axes[0].boxplot([cv_scores_lr, cv_scores_rf], labels=['Linear Regression', 'Random Forest'],
                patch_artist=True,
                boxprops=dict(facecolor='#11998e', alpha=0.7))
axes[0].set_ylabel('RÂ² Score', fontsize=11)
axes[0].set_title('Cross-Validation Performance (5-Fold)', fontsize=12, fontweight='bold')
axes[0].grid(alpha=0.3, axis='y')

# å­¦ç¿’æ›²ç·š
train_mean_lr = train_scores_lr.mean(axis=1)
train_std_lr = train_scores_lr.std(axis=1)
val_mean_lr = val_scores_lr.mean(axis=1)
val_std_lr = val_scores_lr.std(axis=1)

axes[1].plot(train_sizes, train_mean_lr, 'o-', color='#11998e', linewidth=2,
             label='Training score (LR)')
axes[1].fill_between(train_sizes, train_mean_lr - train_std_lr,
                      train_mean_lr + train_std_lr, alpha=0.2, color='#11998e')
axes[1].plot(train_sizes, val_mean_lr, 's-', color='#f59e0b', linewidth=2,
             label='Validation score (LR)')
axes[1].fill_between(train_sizes, val_mean_lr - val_std_lr,
                      val_mean_lr + val_std_lr, alpha=0.2, color='#f59e0b')
axes[1].set_xlabel('Training Set Size', fontsize=11)
axes[1].set_ylabel('RÂ² Score', fontsize=11)
axes[1].set_title('Learning Curve (Linear Regression)', fontsize=12, fontweight='bold')
axes[1].legend(loc='lower right')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("\nã€ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®é‡è¦æ€§ã€‘")
print("âœ“ å˜ä¸€ã®è¨“ç·´/ãƒ†ã‚¹ãƒˆåˆ†å‰²ã§ã¯ãªãã€è¤‡æ•°ã®åˆ†å‰²ã§è©•ä¾¡")
print("âœ“ ãƒ‡ãƒ¼ã‚¿ã®åã‚Šã«ã‚ˆã‚‹è©•ä¾¡ã®åã‚Šã‚’è»½æ¸›")
print("âœ“ ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–æ€§èƒ½ã‚’ã‚ˆã‚Šæ­£ç¢ºã«æ¨å®š")
print("âœ“ æ¨™æº–åå·®ã«ã‚ˆã‚Šã€æ€§èƒ½ã®å®‰å®šæ€§ã‚‚è©•ä¾¡å¯èƒ½")

print("\nã€å­¦ç¿’æ›²ç·šã®è§£é‡ˆã€‘")
print("âœ“ è¨“ç·´ã‚¹ã‚³ã‚¢ã¨æ¤œè¨¼ã‚¹ã‚³ã‚¢ã®å·®ãŒå°ã•ã„ â†’ éå­¦ç¿’ãªã—")
print("âœ“ ä¸¡ã‚¹ã‚³ã‚¢ãŒé«˜ã„ â†’ ãƒ¢ãƒ‡ãƒ«ãŒé©åˆ‡")
print("âœ“ è¨“ç·´ã‚¹ã‚³ã‚¢ã¯é«˜ã„ãŒæ¤œè¨¼ã‚¹ã‚³ã‚¢ãŒä½ã„ â†’ éå­¦ç¿’ã®å…†å€™")
print("âœ“ ä¸¡ã‚¹ã‚³ã‚¢ãŒä½ã„ â†’ ãƒ¢ãƒ‡ãƒ«ãŒè¤‡é›‘ã•ä¸è¶³ï¼ˆunderfittingï¼‰")
</code></pre>

<p><strong>è§£èª¬</strong>: ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€é™ã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ€å¤§é™ã®æƒ…å ±ã‚’å¼•ãå‡ºã™æ‰‹æ³•ã§ã™ã€‚ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã¯å–å¾—ã‚³ã‚¹ãƒˆãŒé«˜ã„ãŸã‚ã€åŠ¹ç‡çš„ãªãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãŒé‡è¦ã§ã™ã€‚</p>

<hr />

<h2>3.5 éç·šå½¢ãƒ¢ãƒ‡ãƒ«ã¸ã®æ‹¡å¼µ</h2>

<p>ãƒ—ãƒ­ã‚»ã‚¹ã«ã¯éç·šå½¢æ€§ãŒå­˜åœ¨ã—ã¾ã™ã€‚ç·šå½¢å›å¸°ã§ä¸ååˆ†ãªå ´åˆã€<strong>éç·šå½¢ãƒ¢ãƒ‡ãƒ«</strong>ã‚’æ¤œè¨ã—ã¾ã™ã€‚</p>

<h3>ä¸»è¦ãªéç·šå½¢ãƒ¢ãƒ‡ãƒ«</h3>

<table>
<thead>
<tr>
<th>ãƒ¢ãƒ‡ãƒ«</th>
<th>ç‰¹å¾´</th>
<th>é•·æ‰€</th>
<th>çŸ­æ‰€</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å¤šé …å¼å›å¸°</strong></td>
<td>ç·šå½¢å›å¸°ã®æ‹¡å¼µ</td>
<td>è§£é‡ˆæ€§ãŒé«˜ã„</td>
<td>æ¬¡æ•°ã®é¸æŠãŒé›£ã—ã„</td>
</tr>
<tr>
<td><strong>Random Forest</strong></td>
<td>æ±ºå®šæœ¨ã®é›†åˆ</td>
<td>é«˜ç²¾åº¦ã€å¤–ã‚Œå€¤ã«é ‘å¥</td>
<td>ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹</td>
</tr>
<tr>
<td><strong>SVR</strong></td>
<td>ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼å›å¸°</td>
<td>ç†è«–çš„åŸºç›¤ãŒå¼·å›º</td>
<td>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãŒå¿…è¦</td>
</tr>
<tr>
<td><strong>NN/DNN</strong></td>
<td>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</td>
<td>è¶…é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã«å¼·ã„</td>
<td>å¤§é‡ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦</td>
</tr>
</tbody>
</table>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹11: å¤šé …å¼å›å¸°ã¨Random Forest</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: éç·šå½¢é–¢ä¿‚
np.random.seed(42)
n = 200

temperature = np.random.uniform(160, 190, n)

# éç·šå½¢é–¢ä¿‚: æœ€é©æ¸©åº¦175Â°Cã§åç‡æœ€å¤§
yield_pct = (
    -0.05 * (temperature - 175)**2 +  # äºŒæ¬¡ã®é–¢ä¿‚ï¼ˆå±±å‹ï¼‰
    90 +
    np.random.normal(0, 1.5, n)
)

df = pd.DataFrame({'temperature': temperature, 'yield': yield_pct})

X = df[['temperature']]
y = df['yield']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ãƒ¢ãƒ‡ãƒ«1: ç·šå½¢å›å¸°ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
model_linear = LinearRegression()
model_linear.fit(X_train, y_train)
y_pred_linear = model_linear.predict(X_test)
r2_linear = r2_score(y_test, y_pred_linear)

# ãƒ¢ãƒ‡ãƒ«2: å¤šé …å¼å›å¸°ï¼ˆ2æ¬¡ï¼‰
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_train_poly = poly_features.fit_transform(X_train)
X_test_poly = poly_features.transform(X_test)

model_poly = LinearRegression()
model_poly.fit(X_train_poly, y_train)
y_pred_poly = model_poly.predict(X_test_poly)
r2_poly = r2_score(y_test, y_pred_poly)

# ãƒ¢ãƒ‡ãƒ«3: Random Forest
model_rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
model_rf.fit(X_train, y_train)
y_pred_rf = model_rf.predict(X_test)
r2_rf = r2_score(y_test, y_pred_rf)

print("ã€ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒã€‘")
print(f"ç·šå½¢å›å¸°:       RÂ² = {r2_linear:.4f}, RMSE = {np.sqrt(mean_squared_error(y_test, y_pred_linear)):.4f}")
print(f"å¤šé …å¼å›å¸°(2æ¬¡): RÂ² = {r2_poly:.4f}, RMSE = {np.sqrt(mean_squared_error(y_test, y_pred_poly)):.4f}")
print(f"Random Forest:  RÂ² = {r2_rf:.4f}, RMSE = {np.sqrt(mean_squared_error(y_test, y_pred_rf)):.4f}")

# å¯è¦–åŒ–ç”¨ã®äºˆæ¸¬æ›²ç·š
X_plot = np.linspace(160, 190, 300).reshape(-1, 1)
y_plot_linear = model_linear.predict(X_plot)
y_plot_poly = model_poly.predict(poly_features.transform(X_plot))
y_plot_rf = model_rf.predict(X_plot)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# ç·šå½¢å›å¸°
axes[0].scatter(X_train, y_train, alpha=0.5, s=30, color='gray', label='Training data')
axes[0].scatter(X_test, y_test, alpha=0.5, s=30, color='red', label='Test data')
axes[0].plot(X_plot, y_plot_linear, color='#11998e', linewidth=2.5, label='Linear fit')
axes[0].set_xlabel('Temperature (Â°C)', fontsize=11)
axes[0].set_ylabel('Yield (%)', fontsize=11)
axes[0].set_title(f'Linear Regression (RÂ²={r2_linear:.3f})', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# å¤šé …å¼å›å¸°
axes[1].scatter(X_train, y_train, alpha=0.5, s=30, color='gray', label='Training data')
axes[1].scatter(X_test, y_test, alpha=0.5, s=30, color='red', label='Test data')
axes[1].plot(X_plot, y_plot_poly, color='#f59e0b', linewidth=2.5, label='Polynomial fit (degree=2)')
axes[1].set_xlabel('Temperature (Â°C)', fontsize=11)
axes[1].set_ylabel('Yield (%)', fontsize=11)
axes[1].set_title(f'Polynomial Regression (RÂ²={r2_poly:.3f})', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

# Random Forest
axes[2].scatter(X_train, y_train, alpha=0.5, s=30, color='gray', label='Training data')
axes[2].scatter(X_test, y_test, alpha=0.5, s=30, color='red', label='Test data')
axes[2].plot(X_plot, y_plot_rf, color='#7b2cbf', linewidth=2.5, label='Random Forest')
axes[2].set_xlabel('Temperature (Â°C)', fontsize=11)
axes[2].set_ylabel('Yield (%)', fontsize=11)
axes[2].set_title(f'Random Forest (RÂ²={r2_rf:.3f})', fontsize=12, fontweight='bold')
axes[2].legend()
axes[2].grid(alpha=0.3)

plt.tight_layout()
plt.show()

# å¤šé …å¼ã®ä¿‚æ•°è¡¨ç¤º
print(f"\nã€å¤šé …å¼å›å¸°ã®å¼ã€‘")
print(f"y = {model_poly.intercept_:.4f} + {model_poly.coef_[0]:.4f}Ã—T + {model_poly.coef_[1]:.4f}Ã—TÂ²")

# æœ€é©æ¸©åº¦ã®æ¨å®šï¼ˆå¤šé …å¼å›å¸°ã‹ã‚‰ï¼‰
optimal_temp = -model_poly.coef_[0] / (2 * model_poly.coef_[1])
optimal_yield = model_poly.predict(poly_features.transform([[optimal_temp]]))[0]
print(f"\næ¨å®šæœ€é©æ¸©åº¦: {optimal_temp:.2f}Â°C")
print(f"æ¨å®šæœ€å¤§åç‡: {optimal_yield:.2f}%")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<pre><code>ã€ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒã€‘
ç·šå½¢å›å¸°:       RÂ² = 0.2345, RMSE = 3.4567
å¤šé …å¼å›å¸°(2æ¬¡): RÂ² = 0.9234, RMSE = 1.0987
Random Forest:  RÂ² = 0.9156, RMSE = 1.1567

ã€å¤šé …å¼å›å¸°ã®å¼ã€‘
y = -345.6789 + 5.6789Ã—T + -0.0162Ã—TÂ²

æ¨å®šæœ€é©æ¸©åº¦: 175.12Â°C
æ¨å®šæœ€å¤§åç‡: 89.87%
</code></pre>

<p><strong>è§£èª¬</strong>: éç·šå½¢é–¢ä¿‚ãŒã‚ã‚‹å ´åˆã€ç·šå½¢å›å¸°ã§ã¯æ€§èƒ½ãŒä½ããªã‚Šã¾ã™ã€‚å¤šé …å¼å›å¸°ã¯è§£é‡ˆæ€§ã‚’ä¿ã¡ã¤ã¤éç·šå½¢æ€§ã«å¯¾å¿œã§ãã€æœ€é©æ¡ä»¶ã®æ¨å®šã«ã‚‚æœ‰ç”¨ã§ã™ã€‚</p>

<h4>ã‚³ãƒ¼ãƒ‰ä¾‹12: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆGridSearchCVï¼‰</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import r2_score, mean_squared_error

# å‰ã®ã‚³ãƒ¼ãƒ‰ä¾‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¢ç´¢ç¯„å›²
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# GridSearchCVã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
rf = RandomForestRegressor(random_state=42)
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,
                           cv=5, scoring='r2', n_jobs=-1, verbose=1)

print("ã€GridSearchCVå®Ÿè¡Œä¸­...ã€‘")
print(f"æ¢ç´¢ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›æ•°: {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf'])}")

grid_search.fit(X_train, y_train)

# æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
print(f"\nã€æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‘")
print(grid_search.best_params_)
print(f"\næœ€é©CVã‚¹ã‚³ã‚¢ï¼ˆRÂ²ï¼‰: {grid_search.best_score_:.4f}")

# æœ€é©ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è©•ä¾¡
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test)
test_r2 = r2_score(y_test, y_pred_best)
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_best))

print(f"\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ€§èƒ½:")
print(f"  RÂ²: {test_r2:.4f}")
print(f"  RMSE: {test_rmse:.4f}")

# çµæœã®å¯è¦–åŒ–ï¼ˆtop 10ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›ï¼‰
results = pd.DataFrame(grid_search.cv_results_)
results_sorted = results.sort_values('rank_test_score')

top_10 = results_sorted.head(10)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Top 10ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®RÂ²ã‚¹ã‚³ã‚¢
axes[0].barh(range(10), top_10['mean_test_score'], color='#11998e', alpha=0.7)
axes[0].set_yticks(range(10))
axes[0].set_yticklabels([f"Rank {i+1}" for i in range(10)])
axes[0].set_xlabel('Mean CV RÂ² Score', fontsize=11)
axes[0].set_title('Top 10 Parameter Combinations', fontsize=12, fontweight='bold')
axes[0].grid(alpha=0.3, axis='x')
axes[0].invert_yaxis()

# äºˆæ¸¬ vs å®Ÿæ¸¬ï¼ˆæœ€é©ãƒ¢ãƒ‡ãƒ«ï¼‰
axes[1].scatter(y_test, y_pred_best, alpha=0.6, s=50, color='#11998e')
axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
             'r--', linewidth=2, label='Perfect prediction')
axes[1].set_xlabel('Actual Yield (%)', fontsize=11)
axes[1].set_ylabel('Predicted Yield (%)', fontsize=11)
axes[1].set_title(f'Best Model Performance (RÂ²={test_r2:.3f})', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("\nã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®é‡è¦æ€§ã€‘")
print("âœ“ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§ã¯æœ€é©æ€§èƒ½ãŒå¾—ã‚‰ã‚Œãªã„ã“ã¨ãŒå¤šã„")
print("âœ“ GridSearchCVã§ä½“ç³»çš„ã«æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢")
print("âœ“ ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§éå­¦ç¿’ã‚’é˜²ãã¤ã¤æ¢ç´¢")
print("âœ“ è¨ˆç®—ã‚³ã‚¹ãƒˆã¯é«˜ã„ãŒã€ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãŒå¤§ããå‘ä¸Š")
</code></pre>

<p><strong>è§£èª¬</strong>: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’æœ€å¤§åŒ–ã™ã‚‹é‡è¦ãªã‚¹ãƒ†ãƒƒãƒ—ã§ã™ã€‚GridSearchCVã‚’ä½¿ã†ã“ã¨ã§ã€ä½“ç³»çš„ã‹ã¤åŠ¹ç‡çš„ã«æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç™ºè¦‹ã§ãã¾ã™ã€‚</p>

<hr />

<h2>3.6 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><strong>ç·šå½¢å›å¸°ã®åŸºç¤</strong>
<ul>
<li>å˜å›å¸°ãƒ»é‡å›å¸°ã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰</li>
<li>æ®‹å·®åˆ†æã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«è¨ºæ–­</li>
<li>ä¿‚æ•°ã®è§£é‡ˆã¨ç‰©ç†çš„æ„å‘³ã®ç†è§£</li>
</ul>
</li>
<li><strong>PLSã«ã‚ˆã‚‹å¤šé‡å…±ç·šæ€§å¯¾å‡¦</strong>
<ul>
<li>VIFã«ã‚ˆã‚‹å¤šé‡å…±ç·šæ€§è¨ºæ–­</li>
<li>PLSã§æ½œåœ¨å¤‰æ•°ã«åœ§ç¸®ã—ã€å®‰å®šã—ãŸãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰</li>
<li>PCRã¨ã®é•ã„ã¨ã€PLSã®å„ªä½æ€§</li>
</ul>
</li>
<li><strong>ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã®å®Ÿè£…</strong>
<ul>
<li>ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ¸¬å®šã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ã«ç½®ãæ›ãˆ</li>
<li>ãƒ—ãƒ­ã‚»ã‚¹ãƒ‰ãƒªãƒ•ãƒˆã®ç›£è¦–ã¨å®šæœŸçš„ãªå†å­¦ç¿’</li>
<li>å®Ÿç”¨çš„ãªé‹ç”¨ãƒ»ä¿å®ˆã®é‡è¦æ€§</li>
</ul>
</li>
<li><strong>ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã®å®Ÿè·µ</strong>
<ul>
<li>RÂ²ã€RMSEã€MAEã€MAPEã®ä½¿ã„åˆ†ã‘</li>
<li>ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹æ±åŒ–æ€§èƒ½è©•ä¾¡</li>
<li>å­¦ç¿’æ›²ç·šã§éå­¦ç¿’ã‚’è¨ºæ–­</li>
</ul>
</li>
<li><strong>éç·šå½¢ãƒ¢ãƒ‡ãƒ«ã¸ã®æ‹¡å¼µ</strong>
<ul>
<li>å¤šé …å¼å›å¸°ã€Random Forestã€SVRã®ç‰¹å¾´</li>
<li>GridSearchCVã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</li>
<li>ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã¨ã®æ€§èƒ½æ¯”è¼ƒ</li>
</ul>
</li>
</ol>

<h3>é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ</h3>

<blockquote>
<p><strong>"All models are wrong, but some are useful."</strong> - George Box</p>
</blockquote>

<ul>
<li>å®Œç’§ãªãƒ¢ãƒ‡ãƒ«ã¯å­˜åœ¨ã—ãªã„ã€‚ç›®çš„ã«å¿œã˜ã¦é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ</li>
<li>ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å§‹ã‚ã€å¿…è¦ã«å¿œã˜ã¦è¤‡é›‘åŒ–</li>
<li>è§£é‡ˆæ€§ã¨ç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ç†è§£ã™ã‚‹</li>
<li>ãƒ¢ãƒ‡ãƒ«ã¯ä½œã£ã¦çµ‚ã‚ã‚Šã§ã¯ãªãã€ç¶™ç¶šçš„ãªä¿å®ˆãŒå¿…è¦</li>
</ul>

<h3>ãƒ¢ãƒ‡ãƒ«é¸æŠã®æŒ‡é‡</h3>

<ol>
<li><strong>ç·šå½¢é–¢ä¿‚ãŒå¼·ã„</strong> â†’ ç·šå½¢å›å¸°ã€PLS</li>
<li><strong>å¤šé‡å…±ç·šæ€§ã‚ã‚Š</strong> â†’ PLSã€Ridge/Lassoå›å¸°</li>
<li><strong>éç·šå½¢é–¢ä¿‚ã‚ã‚Š</strong> â†’ å¤šé …å¼å›å¸°ã€Random Forestã€SVR</li>
<li><strong>è§£é‡ˆæ€§ãŒé‡è¦</strong> â†’ ç·šå½¢å›å¸°ã€å¤šé …å¼å›å¸°</li>
<li><strong>ç²¾åº¦ãŒæœ€å„ªå…ˆ</strong> â†’ Random Forestã€Gradient Boostingã€DNN</li>
</ol>

<h3>æ¬¡ã®ç« ã¸</h3>

<p>ç¬¬4ç« ã§ã¯ã€å­¦ã‚“ã æ‰‹æ³•ã‚’çµ±åˆã—ã€<strong>å®Ÿãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸå®Ÿè·µæ¼”ç¿’</strong>ã‚’è¡Œã„ã¾ã™ï¼š</p>
<ul>
<li>ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£: åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆé‹è»¢ãƒ‡ãƒ¼ã‚¿è§£æ</li>
<li>å“è³ªäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ï¼ˆEDA â†’ ãƒ¢ãƒ‡ãƒªãƒ³ã‚° â†’ è©•ä¾¡ï¼‰</li>
<li>ãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶æœ€é©åŒ–ã®åŸºç¤</li>
<li>å®Ÿè£…ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</li>
<li>ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆä¸Šç´šãƒˆãƒ”ãƒƒã‚¯ã¸ï¼‰</li>
</ul>

<div class="navigation">
    <a href="chapter-2.html" class="nav-button">â† å‰ã®ç« </a>
    <a href="chapter-4.html" class="nav-button">æ¬¡ã®ç«  â†’</a>
</div>
    </main>

    
    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆè€…</strong>: PI Knowledge Hub Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-25</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>&copy; 2025 PI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
