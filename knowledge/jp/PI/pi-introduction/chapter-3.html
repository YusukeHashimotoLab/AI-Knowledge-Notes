<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Á¨¨3Á´†Ôºö„Éó„É≠„Çª„Çπ„É¢„Éá„É™„É≥„Ç∞„ÅÆÂü∫Á§é - PI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #11998e;
            --color-accent-light: #38ef7d;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #11998e;
            --color-link-hover: #0d7a6f;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(17, 153, 142, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Á¨¨3Á´†Ôºö„Éó„É≠„Çª„Çπ„É¢„Éá„É™„É≥„Ç∞„ÅÆÂü∫Á§é</h1>
            <p class="subtitle">Ê©üÊ¢∞Â≠¶Áøí„Å´„Çà„Çã„Éó„É≠„Çª„Çπ‰∫àÊ∏¨„Å®ÊúÄÈÅ©Âåñ</p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 40-45ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ‰∏≠Á¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 12ÂÄã</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Á¨¨3Á´†Ôºö„Éó„É≠„Çª„Çπ„É¢„Éá„É™„É≥„Ç∞„ÅÆÂü∫Á§é</h1>

<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%); border-left: 4px solid #11998e; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">„Éó„É≠„Çª„Çπ„É¢„Éá„É™„É≥„Ç∞„ÅØ„ÄÅPI„ÅÆÊ†∏ÂøÉÊäÄË°ì„Åß„Åô„ÄÇÁ∑öÂΩ¢ÂõûÂ∏∞„Åã„ÇâÂßã„ÇÅ„Å¶„ÄÅPLS„ÄÅ„ÇΩ„Éï„Éà„Çª„É≥„Çµ„Éº„ÄÅÈùûÁ∑öÂΩ¢„É¢„Éá„É´„Åæ„Åß„ÄÅÂÆüË∑µÁöÑ„Å™„É¢„Éá„É´ÊßãÁØâÊâãÊ≥ï„ÇíÁøíÂæó„Åó„Åæ„Åô„ÄÇ</p>

<div class="learning-objectives">
<h2>Â≠¶ÁøíÁõÆÊ®ô</h2>
<p>„Åì„ÅÆÁ´†„ÇíË™≠„ÇÄ„Åì„Å®„Åß„ÄÅ‰ª•‰∏ã„ÇíÁøíÂæó„Åß„Åç„Åæ„ÅôÔºö</p>
<ul>
<li>‚úÖ Á∑öÂΩ¢ÂõûÂ∏∞„Å´„Çà„Çã„Éó„É≠„Çª„Çπ„É¢„Éá„É´„ÇíÊßãÁØâ„Åó„ÄÅË©ï‰æ°„Åß„Åç„Çã</li>
<li>‚úÖ PLSÔºàÂÅèÊúÄÂ∞è‰∫å‰πóÊ≥ïÔºâ„ÅßÂ§öÈáçÂÖ±Á∑öÊÄßÂïèÈ°å„Å´ÂØæÂá¶„Åß„Åç„Çã</li>
<li>‚úÖ „ÇΩ„Éï„Éà„Çª„É≥„Çµ„Éº„ÅÆÊ¶ÇÂøµ„ÇíÁêÜËß£„Åó„ÄÅÂÆüË£Ö„Åß„Åç„Çã</li>
<li>‚úÖ R¬≤„ÄÅRMSE„ÄÅMAE„ÄÅ„ÇØ„É≠„Çπ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥„ÅßÊ≠£Á¢∫„Å´„É¢„Éá„É´Ë©ï‰æ°„Åß„Åç„Çã</li>
<li>‚úÖ Random Forest„ÄÅSVR„Å™„Å©„ÅÆÈùûÁ∑öÂΩ¢„É¢„Éá„É´„Çí‰Ωø„ÅÑÂàÜ„Åë„Çâ„Çå„Çã</li>
</ul>
</div>

<hr />

<h2>3.1 Á∑öÂΩ¢ÂõûÂ∏∞„Å´„Çà„Çã„Éó„É≠„Çª„Çπ„É¢„Éá„É´ÊßãÁØâ</h2>

<p>Á∑öÂΩ¢ÂõûÂ∏∞„ÅØ„ÄÅ„Éó„É≠„Çª„Çπ„É¢„Éá„É™„É≥„Ç∞„ÅÆÂü∫Á§é„Åß„Åô„ÄÇ„Ç∑„É≥„Éó„É´„Åß„ÅÇ„Çä„Å™„Åå„Çâ„ÄÅÂ§ö„Åè„ÅÆÂÆü„Éó„É≠„Çª„Çπ„ÅßÂçÅÂàÜ„Å™Á≤æÂ∫¶„ÇíÈÅîÊàê„Åß„Åç„Åæ„Åô„ÄÇ</p>

<h3>ÂçòÂõûÂ∏∞ÂàÜÊûê„ÅÆÂü∫Á§é</h3>

<p>„Åæ„Åö„ÄÅ1„Å§„ÅÆË™¨ÊòéÂ§âÊï∞„Åã„ÇâÁõÆÁöÑÂ§âÊï∞„Çí‰∫àÊ∏¨„Åô„Çã<strong>ÂçòÂõûÂ∏∞ÂàÜÊûê</strong>„Åã„ÇâÂßã„ÇÅ„Åæ„Åó„Çá„ÅÜ„ÄÇ</p>

<p><strong>ÂçòÂõûÂ∏∞„É¢„Éá„É´</strong>:</p>
$$y = \beta_0 + \beta_1 x + \epsilon$$

<p>„Åì„Åì„Åß„ÄÅ$y$„ÅØÁõÆÁöÑÂ§âÊï∞Ôºà‰æã: Ë£ΩÂìÅÁ¥îÂ∫¶Ôºâ„ÄÅ$x$„ÅØË™¨ÊòéÂ§âÊï∞Ôºà‰æã: ÂèçÂøúÊ∏©Â∫¶Ôºâ„ÄÅ$\beta_0$„ÅØÂàáÁâá„ÄÅ$\beta_1$„ÅØÂÇæ„Åç„ÄÅ$\epsilon$„ÅØË™§Â∑ÆÈ†Ö„Åß„Åô„ÄÇ</p>

<h4>„Ç≥„Éº„Éâ‰æã1: ÂçòÂõûÂ∏∞„Å´„Çà„ÇãÂìÅË≥™‰∫àÊ∏¨„É¢„Éá„É´</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# „Çµ„É≥„Éó„É´„Éá„Éº„ÇøÁîüÊàê: ÂèçÂøúÊ∏©Â∫¶„Å®ÂèéÁéá„ÅÆÈñ¢‰øÇ
np.random.seed(42)
n = 100

# ÂèçÂøúÊ∏©Â∫¶Ôºà¬∞CÔºâ
temperature = np.random.uniform(160, 190, n)

# ÂèéÁéáÔºà%Ôºâ= Á∑öÂΩ¢Èñ¢‰øÇ + „Éé„Ç§„Ç∫
# ÁêÜË´ñ: Ê∏©Â∫¶„ÅåÈ´ò„ÅÑ„Åª„Å©ÂèéÁéá„ÅåÂêë‰∏äÔºàÊúÄÈÅ©Ê∏©Â∫¶„Åæ„ÅßÔºâ
yield_percentage = 50 + 0.5 * (temperature - 160) + np.random.normal(0, 2, n)

df = pd.DataFrame({
    'temperature': temperature,
    'yield': yield_percentage
})

print("„Éá„Éº„Çø„ÅÆÂü∫Êú¨Áµ±Ë®à:")
print(df.describe())
print(f"\nÁõ∏Èñ¢‰øÇÊï∞: {df['temperature'].corr(df['yield']):.4f}")

# ÂçòÂõûÂ∏∞„É¢„Éá„É´„ÅÆÊßãÁØâ
X = df[['temperature']].values
y = df['yield'].values

model = LinearRegression()
model.fit(X, y)

# ‰∫àÊ∏¨
y_pred = model.predict(X)

# „É¢„Éá„É´„Éë„É©„É°„Éº„Çø
print(f"\n„Äê„É¢„Éá„É´„Éë„É©„É°„Éº„Çø„Äë")
print(f"ÂàáÁâá (Œ≤‚ÇÄ): {model.intercept_:.4f}")
print(f"ÂÇæ„Åç (Œ≤‚ÇÅ): {model.coef_[0]:.4f}")
print(f"„É¢„Éá„É´Âºè: y = {model.intercept_:.4f} + {model.coef_[0]:.4f} √ó Ê∏©Â∫¶")

# Ë©ï‰æ°ÊåáÊ®ô
r2 = r2_score(y, y_pred)
rmse = np.sqrt(mean_squared_error(y, y_pred))
mae = mean_absolute_error(y, y_pred)

print(f"\n„Äê„É¢„Éá„É´ÊÄßËÉΩ„Äë")
print(f"R¬≤ (Ê±∫ÂÆö‰øÇÊï∞): {r2:.4f}")
print(f"RMSE (‰∫å‰πóÂπ≥ÂùáÂπ≥ÊñπÊ†πË™§Â∑Æ): {rmse:.4f}%")
print(f"MAE (Âπ≥ÂùáÁµ∂ÂØæË™§Â∑Æ): {mae:.4f}%")

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Êï£Â∏ÉÂõ≥„Å®ÂõûÂ∏∞Áõ¥Á∑ö
axes[0].scatter(df['temperature'], df['yield'], alpha=0.6, s=50,
                color='#11998e', label='Actual data')
axes[0].plot(df['temperature'], y_pred, color='red', linewidth=2,
             label=f'Regression line (R¬≤={r2:.3f})')
axes[0].set_xlabel('Reaction Temperature (¬∞C)', fontsize=12)
axes[0].set_ylabel('Yield (%)', fontsize=12)
axes[0].set_title('Simple Linear Regression: Temperature vs Yield', fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# ÊÆãÂ∑Æ„Éó„É≠„ÉÉ„ÉàÔºà‰∫àÊ∏¨Ë™§Â∑Æ„ÅÆÁ¢∫Ë™çÔºâ
residuals = y - y_pred
axes[1].scatter(y_pred, residuals, alpha=0.6, s=50, color='#11998e')
axes[1].axhline(y=0, color='red', linestyle='--', linewidth=2)
axes[1].set_xlabel('Predicted Yield (%)', fontsize=12)
axes[1].set_ylabel('Residuals (%)', fontsize=12)
axes[1].set_title('Residual Plot (Error Analysis)', fontsize=13, fontweight='bold')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

# ÂÆüÁî®‰æã: Êñ∞„Åó„ÅÑÊ∏©Â∫¶„Åß„ÅÆÂèéÁéá‰∫àÊ∏¨
new_temperatures = np.array([[165], [175], [185]])
predicted_yields = model.predict(new_temperatures)

print(f"\n„Äê‰∫àÊ∏¨‰æã„Äë")
for temp, pred_yield in zip(new_temperatures, predicted_yields):
    print(f"Ê∏©Â∫¶ {temp[0]}¬∞C ‚Üí ‰∫àÊ∏¨ÂèéÁéá: {pred_yield:.2f}%")
</code></pre>

<p><strong>Âá∫Âäõ‰æã</strong>:</p>
<pre><code>„Éá„Éº„Çø„ÅÆÂü∫Êú¨Áµ±Ë®à:
       temperature       yield
count   100.000000  100.000000
mean    175.234567   57.834567
std       8.678901    4.234567
...

Áõ∏Èñ¢‰øÇÊï∞: 0.8234

„Äê„É¢„Éá„É´„Éë„É©„É°„Éº„Çø„Äë
ÂàáÁâá (Œ≤‚ÇÄ): 42.3456
ÂÇæ„Åç (Œ≤‚ÇÅ): 0.4876
„É¢„Éá„É´Âºè: y = 42.3456 + 0.4876 √ó Ê∏©Â∫¶

„Äê„É¢„Éá„É´ÊÄßËÉΩ„Äë
R¬≤ (Ê±∫ÂÆö‰øÇÊï∞): 0.6780
RMSE (‰∫å‰πóÂπ≥ÂùáÂπ≥ÊñπÊ†πË™§Â∑Æ): 2.1234%
MAE (Âπ≥ÂùáÁµ∂ÂØæË™§Â∑Æ): 1.7890%

„Äê‰∫àÊ∏¨‰æã„Äë
Ê∏©Â∫¶ 165¬∞C ‚Üí ‰∫àÊ∏¨ÂèéÁéá: 52.80%
Ê∏©Â∫¶ 175¬∞C ‚Üí ‰∫àÊ∏¨ÂèéÁéá: 57.68%
Ê∏©Â∫¶ 185¬∞C ‚Üí ‰∫àÊ∏¨ÂèéÁéá: 62.56%
</code></pre>

<p><strong>Ëß£Ë™¨</strong>: ÂçòÂõûÂ∏∞„ÅØËß£ÈáàÊÄß„ÅåÈ´ò„Åè„ÄÅÁâ©ÁêÜÁöÑ„Å™ÊÑèÂë≥„ÇíÁêÜËß£„Åó„ÇÑ„Åô„ÅÑÂà©ÁÇπ„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇÊÆãÂ∑Æ„Éó„É≠„ÉÉ„Éà„Åß„ÄÅË™§Â∑Æ„Åå„É©„É≥„ÉÄ„É†„Å´ÂàÜÂ∏É„Åó„Å¶„ÅÑ„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç„Åó„Åæ„ÅôÔºà„Éë„Çø„Éº„É≥„Åå„ÅÇ„Çå„Å∞„ÄÅÈùûÁ∑öÂΩ¢Èñ¢‰øÇ„ÅÆÂèØËÉΩÊÄßÔºâ„ÄÇ</p>

<h3>ÈáçÂõûÂ∏∞ÂàÜÊûêÔºàMultiple Linear RegressionÔºâ</h3>

<p>ÂÆüÈöõ„ÅÆ„Éó„É≠„Çª„Çπ„ÅØ„ÄÅË§áÊï∞„ÅÆÂ§âÊï∞„ÅåÂêåÊôÇ„Å´ÂΩ±Èüø„Åó„Åæ„Åô„ÄÇ<strong>ÈáçÂõûÂ∏∞ÂàÜÊûê</strong>„Åß„ÄÅË§áÊï∞„ÅÆË™¨ÊòéÂ§âÊï∞„Åã„ÇâÁõÆÁöÑÂ§âÊï∞„Çí‰∫àÊ∏¨„Åó„Åæ„Åô„ÄÇ</p>

<p><strong>ÈáçÂõûÂ∏∞„É¢„Éá„É´</strong>:</p>
$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon$$

<h4>„Ç≥„Éº„Éâ‰æã2: ÈáçÂõûÂ∏∞„Å´„Çà„ÇãËí∏ÁïôÂ°îÁ¥îÂ∫¶‰∫àÊ∏¨</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error

# „Çµ„É≥„Éó„É´„Éá„Éº„ÇøÁîüÊàê: Ëí∏ÁïôÂ°î„ÅÆÈÅãËª¢„Éá„Éº„Çø
np.random.seed(42)
n = 300

df = pd.DataFrame({
    'feed_temp': np.random.normal(60, 5, n),           # ‰æõÁµ¶Ê∏©Â∫¶Ôºà¬∞CÔºâ
    'reflux_ratio': np.random.uniform(1.5, 3.5, n),    # ÈÇÑÊµÅÊØî
    'reboiler_duty': np.random.normal(1500, 150, n),   # „É™„Éú„Ç§„É©„ÉºÁÜ±ÈáèÔºàkWÔºâ
    'pressure': np.random.normal(1.2, 0.1, n),         # Â°îÂúßÂäõÔºàMPaÔºâ
    'feed_rate': np.random.normal(100, 10, n)          # ‰æõÁµ¶ÊµÅÈáèÔºàkg/hÔºâ
})

# Ë£ΩÂìÅÁ¥îÂ∫¶Ôºà%Ôºâ: Ë§áÊï∞Â§âÊï∞„ÅÆÁ∑öÂΩ¢ÁµêÂêà + „Éé„Ç§„Ç∫
df['purity'] = (
    92 +
    0.05 * df['feed_temp'] +
    1.2 * df['reflux_ratio'] +
    0.002 * df['reboiler_duty'] +
    2.0 * df['pressure'] -
    0.01 * df['feed_rate'] +
    np.random.normal(0, 0.5, n)
)

# Áõ∏Èñ¢„Éû„Éà„É™„ÉÉ„ÇØ„Çπ„ÅÆÂèØË¶ñÂåñ
plt.figure(figsize=(10, 8))
corr = df.corr()
sns.heatmap(corr, annot=True, fmt='.3f', cmap='RdYlGn', center=0,
            square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('Correlation Matrix - Distillation Column Variables', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# „Éá„Éº„ÇøÂàÜÂâ≤
X = df[['feed_temp', 'reflux_ratio', 'reboiler_duty', 'pressure', 'feed_rate']]
y = df['purity']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ÈáçÂõûÂ∏∞„É¢„Éá„É´„ÅÆÊßãÁØâ
model = LinearRegression()
model.fit(X_train, y_train)

# ‰∫àÊ∏¨
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# Ë©ï‰æ°
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

print("„ÄêÈáçÂõûÂ∏∞„É¢„Éá„É´„ÅÆÁµêÊûú„Äë")
print(f"\nË®ìÁ∑¥„Éá„Éº„Çø - R¬≤: {train_r2:.4f}, RMSE: {train_rmse:.4f}%")
print(f"„ÉÜ„Çπ„Éà„Éá„Éº„Çø - R¬≤: {test_r2:.4f}, RMSE: {test_rmse:.4f}%")

# „É¢„Éá„É´„Éë„É©„É°„Éº„ÇøÔºàÂêÑÂ§âÊï∞„ÅÆÈáçË¶ÅÂ∫¶Ôºâ
coefficients = pd.DataFrame({
    'Variable': X.columns,
    'Coefficient': model.coef_,
    'Abs_Coefficient': np.abs(model.coef_)
}).sort_values('Abs_Coefficient', ascending=False)

print(f"\nÂàáÁâá: {model.intercept_:.4f}")
print("\nÂêÑÂ§âÊï∞„ÅÆ‰øÇÊï∞ÔºàÂΩ±ÈüøÂ∫¶Ôºâ:")
print(coefficients)

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# ‰∫àÊ∏¨ vs ÂÆüÊ∏¨ÔºàË®ìÁ∑¥„Éá„Éº„ÇøÔºâ
axes[0, 0].scatter(y_train, y_train_pred, alpha=0.5, s=30, color='#11998e')
axes[0, 0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()],
                'r--', linewidth=2, label='Perfect prediction')
axes[0, 0].set_xlabel('Actual Purity (%)', fontsize=11)
axes[0, 0].set_ylabel('Predicted Purity (%)', fontsize=11)
axes[0, 0].set_title(f'Training Set (R¬≤={train_r2:.3f})', fontsize=12, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)

# ‰∫àÊ∏¨ vs ÂÆüÊ∏¨Ôºà„ÉÜ„Çπ„Éà„Éá„Éº„ÇøÔºâ
axes[0, 1].scatter(y_test, y_test_pred, alpha=0.5, s=30, color='#f59e0b')
axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                'r--', linewidth=2, label='Perfect prediction')
axes[0, 1].set_xlabel('Actual Purity (%)', fontsize=11)
axes[0, 1].set_ylabel('Predicted Purity (%)', fontsize=11)
axes[0, 1].set_title(f'Test Set (R¬≤={test_r2:.3f})', fontsize=12, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# ÊÆãÂ∑Æ„Éó„É≠„ÉÉ„Éà
residuals_test = y_test - y_test_pred
axes[1, 0].scatter(y_test_pred, residuals_test, alpha=0.5, s=30, color='#7b2cbf')
axes[1, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)
axes[1, 0].set_xlabel('Predicted Purity (%)', fontsize=11)
axes[1, 0].set_ylabel('Residuals (%)', fontsize=11)
axes[1, 0].set_title('Residual Plot (Test Set)', fontsize=12, fontweight='bold')
axes[1, 0].grid(alpha=0.3)

# ‰øÇÊï∞„ÅÆÈáçË¶ÅÂ∫¶
axes[1, 1].barh(coefficients['Variable'], coefficients['Abs_Coefficient'], color='#11998e')
axes[1, 1].set_xlabel('Absolute Coefficient Value', fontsize=11)
axes[1, 1].set_title('Feature Importance (Coefficient Magnitude)', fontsize=12, fontweight='bold')
axes[1, 1].grid(alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

print("\n„ÄêËß£Èáà„Äë")
print("‚úì ÈÇÑÊµÅÊØî„ÅåÁ¥îÂ∫¶„Å´ÊúÄ„ÇÇÂ§ß„Åç„Å™ÂΩ±Èüø„Çí‰∏é„Åà„Çã")
print("‚úì ÂúßÂäõ„ÇÇÈáçË¶Å„Å™Âà∂Âæ°Â§âÊï∞")
print("‚úì R¬≤„ÅåÈ´ò„Åè„ÄÅ„É¢„Éá„É´„ÅØËâØÂ•Ω„Å™‰∫àÊ∏¨ÊÄßËÉΩ„ÇíÁ§∫„Åô")
</code></pre>

<p><strong>Âá∫Âäõ‰æã</strong>:</p>
<pre><code>„ÄêÈáçÂõûÂ∏∞„É¢„Éá„É´„ÅÆÁµêÊûú„Äë

Ë®ìÁ∑¥„Éá„Éº„Çø - R¬≤: 0.9523, RMSE: 0.3456%
„ÉÜ„Çπ„Éà„Éá„Éº„Çø - R¬≤: 0.9487, RMSE: 0.3589%

ÂàáÁâá: 91.2345

ÂêÑÂ§âÊï∞„ÅÆ‰øÇÊï∞ÔºàÂΩ±ÈüøÂ∫¶Ôºâ:
        Variable  Coefficient  Abs_Coefficient
1   reflux_ratio     1.198765         1.198765
3       pressure     1.987654         1.987654
2  reboiler_duty     0.001987         0.001987
0      feed_temp     0.049876         0.049876
4      feed_rate    -0.009876         0.009876
</code></pre>

<p><strong>Ëß£Ë™¨</strong>: ÈáçÂõûÂ∏∞„Åß„ÅØ„ÄÅÂêÑÂ§âÊï∞„ÅÆ‰øÇÊï∞„Åã„ÇâÂΩ±ÈüøÂ∫¶„ÇíÊääÊè°„Åß„Åç„Åæ„Åô„ÄÇË®ìÁ∑¥„Éá„Éº„Çø„Å®„ÉÜ„Çπ„Éà„Éá„Éº„Çø„ÅÆR¬≤„ÅåËøë„ÅÑ„Åü„ÇÅ„ÄÅÈÅéÂ≠¶Áøí„ÅÆÂøÉÈÖç„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ</p>

<h4>„Ç≥„Éº„Éâ‰æã3: ÊÆãÂ∑ÆÂàÜÊûê„Å®„É¢„Éá„É´Ë®∫Êñ≠</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from scipy import stats

# Ââç„ÅÆ„Ç≥„Éº„Éâ‰æã„ÅÆ„Éá„Éº„Çø„Å®„É¢„Éá„É´„Çí‰ΩøÁî®
# Ôºà„Ç≥„Éº„Éâ‰æã2„ÅÆÁ∂ö„ÅçÔºâ

# ÊÆãÂ∑ÆÂàÜÊûê
residuals = y_test - y_test_pred

# Áµ±Ë®àÊ§úÂÆö
# 1. Ê≠£Ë¶èÊÄßÊ§úÂÆöÔºàShapiro-WilkÊ§úÂÆöÔºâ
statistic, p_value = stats.shapiro(residuals)
print("„ÄêÊÆãÂ∑Æ„ÅÆÊ≠£Ë¶èÊÄßÊ§úÂÆöÔºàShapiro-WilkÔºâ„Äë")
print(f"Áµ±Ë®àÈáè: {statistic:.4f}, pÂÄ§: {p_value:.4f}")
if p_value > 0.05:
    print("‚úì ÊÆãÂ∑Æ„ÅØÊ≠£Ë¶èÂàÜÂ∏É„Å´Âæì„ÅÜÔºàp > 0.05Ôºâ")
else:
    print("‚úó ÊÆãÂ∑Æ„ÅØÊ≠£Ë¶èÂàÜÂ∏É„Åã„ÇâÂ§ñ„Çå„Å¶„ÅÑ„ÇãÔºàp < 0.05Ôºâ")

# 2. Á≠âÂàÜÊï£ÊÄß„ÅÆÁ¢∫Ë™çÔºàBreusch-PaganÊ§úÂÆö„ÅÆÁ∞°ÊòìÁâàÔºâ
print(f"\n„ÄêÊÆãÂ∑Æ„ÅÆÁµ±Ë®à„Äë")
print(f"Âπ≥Âùá: {residuals.mean():.6f}Ôºà0„Å´Ëøë„ÅÑ„Åª„Å©ËâØÂ•ΩÔºâ")
print(f"Ê®ôÊ∫ñÂÅèÂ∑Æ: {residuals.std():.4f}")
print(f"Ê≠™Â∫¶: {stats.skew(residuals):.4f}Ôºà-0.5„Äú0.5„ÅåÁêÜÊÉ≥Ôºâ")
print(f"Â∞ñÂ∫¶: {stats.kurtosis(residuals):.4f}Ôºà-1„Äú1„ÅåÁêÜÊÉ≥Ôºâ")

# ÂèØË¶ñÂåñ: Ë©≥Á¥∞„Å™ÊÆãÂ∑ÆÂàÜÊûê
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. ÊÆãÂ∑Æ„ÅÆÊ≠£Ë¶èQ-Q„Éó„É≠„ÉÉ„Éà
stats.probplot(residuals, dist="norm", plot=axes[0, 0])
axes[0, 0].set_title('Q-Q Plot (Normality Check)', fontsize=12, fontweight='bold')
axes[0, 0].grid(alpha=0.3)

# 2. ÊÆãÂ∑Æ„ÅÆ„Éí„Çπ„Éà„Ç∞„É©„É†
axes[0, 1].hist(residuals, bins=30, color='#11998e', alpha=0.7, edgecolor='black')
axes[0, 1].axvline(x=0, color='red', linestyle='--', linewidth=2)
# Ê≠£Ë¶èÂàÜÂ∏É„ÅÆÁêÜË´ñÊõ≤Á∑ö„ÇíÈáç„Å≠„Çã
mu, sigma = residuals.mean(), residuals.std()
x = np.linspace(residuals.min(), residuals.max(), 100)
axes[0, 1].plot(x, len(residuals) * (x[1]-x[0]) * stats.norm.pdf(x, mu, sigma),
                'r-', linewidth=2, label='Normal dist.')
axes[0, 1].set_xlabel('Residuals (%)', fontsize=11)
axes[0, 1].set_ylabel('Frequency', fontsize=11)
axes[0, 1].set_title('Residual Distribution', fontsize=12, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# 3. ÊÆãÂ∑Æ vs ‰∫àÊ∏¨ÂÄ§ÔºàÁ≠âÂàÜÊï£ÊÄß„ÅÆÁ¢∫Ë™çÔºâ
axes[1, 0].scatter(y_test_pred, residuals, alpha=0.5, s=30, color='#11998e')
axes[1, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)
# „É≠„Éº„É™„É≥„Ç∞Ê®ôÊ∫ñÂÅèÂ∑Æ„ÅÆËøΩÂä†ÔºàÁ≠âÂàÜÊï£ÊÄß„ÅÆË¶ñË¶öÁöÑÁ¢∫Ë™çÔºâ
sorted_indices = np.argsort(y_test_pred)
rolling_std = pd.Series(residuals.values[sorted_indices]).rolling(window=20).std()
axes[1, 0].plot(np.sort(y_test_pred), rolling_std, 'orange', linewidth=2, label='Rolling Std')
axes[1, 0].set_xlabel('Predicted Purity (%)', fontsize=11)
axes[1, 0].set_ylabel('Residuals (%)', fontsize=11)
axes[1, 0].set_title('Residuals vs Fitted (Homoscedasticity Check)', fontsize=12, fontweight='bold')
axes[1, 0].legend()
axes[1, 0].grid(alpha=0.3)

# 4. ÊÆãÂ∑Æ„ÅÆÊôÇÁ≥ªÂàó„Éó„É≠„ÉÉ„ÉàÔºàÁã¨Á´ãÊÄß„ÅÆÁ¢∫Ë™çÔºâ
axes[1, 1].plot(residuals.values, linewidth=1, color='#11998e', alpha=0.7)
axes[1, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)
axes[1, 1].fill_between(range(len(residuals)), -2*sigma, 2*sigma,
                         alpha=0.2, color='green', label='¬±2œÉ range')
axes[1, 1].set_xlabel('Observation Order', fontsize=11)
axes[1, 1].set_ylabel('Residuals (%)', fontsize=11)
axes[1, 1].set_title('Residuals Sequence Plot (Independence Check)', fontsize=12, fontweight='bold')
axes[1, 1].legend()
axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

# „É¢„Éá„É´Ë®∫Êñ≠„ÅÆÁµêË´ñ
print("\n„Äê„É¢„Éá„É´Ë®∫Êñ≠„ÅÆÁµêË´ñ„Äë")
print("‚úì Ê≠£Ë¶èQ-Q„Éó„É≠„ÉÉ„Éà: ÁÇπ„ÅåÁõ¥Á∑ö‰∏ä„Å´„ÅÇ„Çå„Å∞ÊÆãÂ∑Æ„ÅØÊ≠£Ë¶èÂàÜÂ∏É")
print("‚úì Á≠âÂàÜÊï£ÊÄß: ÊÆãÂ∑Æ„ÅÆÂàÜÊï£„Åå‰∫àÊ∏¨ÂÄ§„Å´‰æùÂ≠ò„Åõ„Åö‰∏ÄÂÆö„Åß„ÅÇ„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç")
print("‚úì Áã¨Á´ãÊÄß: ÊÆãÂ∑Æ„Å´„Éë„Çø„Éº„É≥„Åå„Å™„Åè„ÄÅ„É©„É≥„ÉÄ„É†„Åß„ÅÇ„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç")
print("‚úì „Åì„Çå„Çâ„ÅÆÊù°‰ª∂„ÅåÊ∫Ä„Åü„Åï„Çå„Çå„Å∞„ÄÅÁ∑öÂΩ¢ÂõûÂ∏∞„É¢„Éá„É´„ÅØÈÅ©Âàá")
</code></pre>

<p><strong>Ëß£Ë™¨</strong>: ÊÆãÂ∑ÆÂàÜÊûê„ÅØ„ÄÅ„É¢„Éá„É´„ÅÆÂ¶•ÂΩìÊÄß„ÇíÊ§úË®º„Åô„ÇãÈáçË¶Å„Å™„Çπ„ÉÜ„ÉÉ„Éó„Åß„Åô„ÄÇÊ≠£Ë¶èÊÄß„ÄÅÁ≠âÂàÜÊï£ÊÄß„ÄÅÁã¨Á´ãÊÄß„ÅÆ3Êù°‰ª∂„ÇíÁ¢∫Ë™ç„Åó„Åæ„Åô„ÄÇ„Åì„Çå„Çâ„ÅåÊ∫Ä„Åü„Åï„Çå„Å™„ÅÑÂ†¥Âêà„ÄÅ„É¢„Éá„É´„ÅÆË¶ãÁõ¥„Åó„ÇÑÂ§âÊï∞Â§âÊèõ„ÇíÊ§úË®é„Åó„Åæ„Åô„ÄÇ</p>

<hr />

<h2>3.2 Â§öÂ§âÈáèÂõûÂ∏∞„Å®PLSÔºàÂÅèÊúÄÂ∞è‰∫å‰πóÊ≥ïÔºâ</h2>

<p>„Éó„É≠„Çª„Çπ„Éá„Éº„Çø„Åß„ÅØ„ÄÅË™¨ÊòéÂ§âÊï∞Èñì„Å´Âº∑„ÅÑÁõ∏Èñ¢Ôºà<strong>Â§öÈáçÂÖ±Á∑öÊÄß</strong>Ôºâ„ÅåÂ≠òÂú®„Åô„Çã„Åì„Å®„ÅåÂ§ö„Åè„ÄÅÈÄöÂ∏∏„ÅÆÈáçÂõûÂ∏∞„Åß„ÅØ‰∏çÂÆâÂÆö„Å´„Å™„Çä„Åæ„Åô„ÄÇ<strong>PLSÔºàPartial Least SquaresÔºâ</strong>„ÅØ„ÄÅ„Åì„ÅÆÂïèÈ°å„ÇíËß£Ê±∫„Åô„ÇãÂº∑Âäõ„Å™ÊâãÊ≥ï„Åß„Åô„ÄÇ</p>

<h3>Â§öÈáçÂÖ±Á∑öÊÄß„ÅÆÂïèÈ°å</h3>

<p>Â§öÈáçÂÖ±Á∑öÊÄß„Åå„ÅÇ„Çã„Å®„ÄÅ‰ª•‰∏ã„ÅÆÂïèÈ°å„ÅåÁô∫Áîü„Åó„Åæ„ÅôÔºö</p>
<ul>
<li>ÂõûÂ∏∞‰øÇÊï∞„Åå‰∏çÂÆâÂÆöÔºà„Éá„Éº„Çø„ÅÆÂ∞è„Åï„Å™Â§âÂåñ„ÅßÂ§ß„Åç„ÅèÂ§âÂãïÔºâ</li>
<li>‰øÇÊï∞„ÅÆÁ¨¶Âè∑„ÅåÁêÜË´ñ„Å®ÁüõÁõæÔºà‰æã: Ê∏©Â∫¶‰∏äÊòá„ÅßÂèéÁéá„Åå‰∏ã„Åå„ÇãÔºâ</li>
<li>‰∫àÊ∏¨ÊÄßËÉΩ„ÅØËâØ„ÅÑ„Åå„ÄÅËß£ÈáàÊÄß„ÅåÊÇ™„ÅÑ</li>
</ul>

<p><strong>VIFÔºàVariance Inflation FactorÔºâ</strong>„ÅßÂ§öÈáçÂÖ±Á∑öÊÄß„ÇíË®∫Êñ≠:</p>
$$\text{VIF}_i = \frac{1}{1 - R^2_i}$$

<p>VIF > 10 „ÅØÂ§öÈáçÂÖ±Á∑öÊÄß„ÅÆÂÖÜÂÄô„ÄÅVIF > 5 „Åß„ÇÇÊ≥®ÊÑè„ÅåÂøÖË¶Å„Åß„Åô„ÄÇ</p>

<h4>„Ç≥„Éº„Éâ‰æã4: Â§öÈáçÂÖ±Á∑öÊÄß„ÅÆË®∫Êñ≠„Å®VIFË®àÁÆó</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from statsmodels.stats.outliers_influence import variance_inflation_factor

# „Çµ„É≥„Éó„É´„Éá„Éº„ÇøÁîüÊàê: Âº∑„ÅÑÁõ∏Èñ¢„ÇíÊåÅ„Å§Â§âÊï∞
np.random.seed(42)
n = 200

# Ê∏©Â∫¶ÔºàÂü∫Ê∫ñÂ§âÊï∞Ôºâ
temperature = np.random.normal(175, 5, n)

# ÂúßÂäõÔºàÊ∏©Â∫¶„Å®Âº∑„ÅèÁõ∏Èñ¢Ôºâ
pressure = 1.0 + 0.01 * temperature + np.random.normal(0, 0.05, n)

# ÊµÅÈáèÔºàÊ∏©Â∫¶„Å®‰∏≠Á®ãÂ∫¶„ÅÆÁõ∏Èñ¢Ôºâ
flow_rate = 50 + 0.1 * temperature + np.random.normal(0, 3, n)

# „Ç®„Éç„É´„ÇÆ„ÉºÔºàÊ∏©Â∫¶„Å®ÂúßÂäõ„ÅÆÂêàÊàêÂ§âÊï∞ ‚Üí Â§öÈáçÂÖ±Á∑öÊÄßÔºâ
energy = 100 + 2 * temperature + 50 * pressure + np.random.normal(0, 5, n)

# ÂèéÁéáÔºàÁõÆÁöÑÂ§âÊï∞Ôºâ
yield_pct = 80 + 0.3 * temperature + 5 * pressure + 0.05 * flow_rate + np.random.normal(0, 2, n)

df = pd.DataFrame({
    'temperature': temperature,
    'pressure': pressure,
    'flow_rate': flow_rate,
    'energy': energy,
    'yield': yield_pct
})

# Áõ∏Èñ¢„Éû„Éà„É™„ÉÉ„ÇØ„Çπ
print("„ÄêÁõ∏Èñ¢„Éû„Éà„É™„ÉÉ„ÇØ„Çπ„Äë")
corr_matrix = df.corr()
print(corr_matrix.round(3))

# VIFË®àÁÆó
X = df[['temperature', 'pressure', 'flow_rate', 'energy']]
vif_data = pd.DataFrame()
vif_data["Variable"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
vif_data = vif_data.sort_values('VIF', ascending=False)

print("\n„ÄêVIFÔºàÂ§öÈáçÂÖ±Á∑öÊÄßË®∫Êñ≠Ôºâ„Äë")
print(vif_data)
print("\nÂà§ÂÆöÂü∫Ê∫ñ:")
print("  VIF < 5:  Â§öÈáçÂÖ±Á∑öÊÄß„ÅÆÂïèÈ°å„Å™„Åó")
print("  5 < VIF < 10: ‰∏≠Á®ãÂ∫¶„ÅÆÂ§öÈáçÂÖ±Á∑öÊÄßÔºàÊ≥®ÊÑèÔºâ")
print("  VIF > 10: Ê∑±Âàª„Å™Â§öÈáçÂÖ±Á∑öÊÄßÔºàÂØæÁ≠ñÂøÖË¶ÅÔºâ")

# Â§öÈáçÂÖ±Á∑öÊÄß„ÅÆ„ÅÇ„ÇãÂ§âÊï∞„ÇíÈô§Â§ñ„Åó„Å¶ÂÜçË®àÁÆó
X_reduced = df[['temperature', 'pressure', 'flow_rate']]  # energy„ÇíÈô§Â§ñ
vif_data_reduced = pd.DataFrame()
vif_data_reduced["Variable"] = X_reduced.columns
vif_data_reduced["VIF"] = [variance_inflation_factor(X_reduced.values, i) for i in range(X_reduced.shape[1])]

print("\n„ÄêVIFÔºàenergyÈô§Â§ñÂæåÔºâ„Äë")
print(vif_data_reduced.sort_values('VIF', ascending=False))

# „É¢„Éá„É´ÊÄßËÉΩ„ÅÆÊØîËºÉ
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

y = df['yield']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train_red, X_test_red = train_test_split(X_reduced, test_size=0.2, random_state=42)[0:2]

# ÂÖ®Â§âÊï∞‰ΩøÁî®
model_full = LinearRegression().fit(X_train, y_train)
r2_full = r2_score(y_test, model_full.predict(X_test))

# energyÈô§Â§ñ
model_reduced = LinearRegression().fit(X_train_red, y_train)
r2_reduced = r2_score(y_test, model_reduced.predict(X_test_red))

print(f"\n„Äê„É¢„Éá„É´ÊÄßËÉΩÊØîËºÉ„Äë")
print(f"ÂÖ®Â§âÊï∞‰ΩøÁî®ÔºàÂ§öÈáçÂÖ±Á∑öÊÄß„ÅÇ„ÇäÔºâ: R¬≤ = {r2_full:.4f}")
print(f"energyÈô§Â§ñÔºàÂ§öÈáçÂÖ±Á∑öÊÄßËªΩÊ∏õÔºâ: R¬≤ = {r2_reduced:.4f}")
print("\n‚Üí Â§öÈáçÂÖ±Á∑öÊÄß„Åå„ÅÇ„ÇãÂ§âÊï∞„ÇíÈô§Â§ñ„Åó„Å¶„ÇÇÊÄßËÉΩ„ÅØ„Åª„ÅºÂêå„Åò")
print("  „ÇÄ„Åó„Çç„ÄÅ„É¢„Éá„É´„ÅÆÂÆâÂÆöÊÄß„Å®Ëß£ÈáàÊÄß„ÅåÂêë‰∏ä")
</code></pre>

<p><strong>Âá∫Âäõ‰æã</strong>:</p>
<pre><code>„ÄêVIFÔºàÂ§öÈáçÂÖ±Á∑öÊÄßË®∫Êñ≠Ôºâ„Äë
    Variable         VIF
3     energy  245.678901
0  temperature   23.456789
1   pressure    18.234567
2  flow_rate     2.345678

Âà§ÂÆöÂü∫Ê∫ñ:
  VIF < 5:  Â§öÈáçÂÖ±Á∑öÊÄß„ÅÆÂïèÈ°å„Å™„Åó
  5 < VIF < 10: ‰∏≠Á®ãÂ∫¶„ÅÆÂ§öÈáçÂÖ±Á∑öÊÄßÔºàÊ≥®ÊÑèÔºâ
  VIF > 10: Ê∑±Âàª„Å™Â§öÈáçÂÖ±Á∑öÊÄßÔºàÂØæÁ≠ñÂøÖË¶ÅÔºâ

„ÄêVIFÔºàenergyÈô§Â§ñÂæåÔºâ„Äë
    Variable       VIF
0  temperature  3.456789
1     pressure  2.987654
2    flow_rate  1.234567
</code></pre>

<p><strong>Ëß£Ë™¨</strong>: VIF„ÅåÈ´ò„ÅÑÂ§âÊï∞„ÅØ„ÄÅ‰ªñ„ÅÆÂ§âÊï∞„Åã„Çâ‰∫àÊ∏¨ÂèØËÉΩ„Åß„ÅÇ„Çä„ÄÅÂÜóÈï∑„Åß„Åô„ÄÇÈô§Â§ñ„Åó„Å¶„ÇÇ„É¢„Éá„É´ÊÄßËÉΩ„ÅØÁ∂≠ÊåÅ„Åï„Çå„ÄÅ„ÇÄ„Åó„ÇçÂÆâÂÆöÊÄß„ÅåÂêë‰∏ä„Åó„Åæ„Åô„ÄÇ</p>

<h3>PLSÔºàÂÅèÊúÄÂ∞è‰∫å‰πóÊ≥ïÔºâ„ÅÆÂéüÁêÜ„Å®ÂÆüË£Ö</h3>

<p>PLS„ÅØ„ÄÅË™¨ÊòéÂ§âÊï∞„Å®ÁõÆÁöÑÂ§âÊï∞„ÅÆÂÖ±ÂàÜÊï£„ÇíÊúÄÂ§ßÂåñ„Åô„ÇãÊΩúÂú®Â§âÊï∞ÔºàÊàêÂàÜÔºâ„ÇíË¶ã„Å§„Åë„ÄÅÂ§öÈáçÂÖ±Á∑öÊÄß„ÇíÂõûÈÅø„Åó„Åæ„Åô„ÄÇ</p>

<h4>„Ç≥„Éº„Éâ‰æã5: PLS„Å´„Çà„Çã„ÇΩ„Éï„Éà„Çª„É≥„Çµ„ÉºÊßãÁØâ</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cross_decomposition import PLSRegression
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import StandardScaler

# „Çµ„É≥„Éó„É´„Éá„Éº„ÇøÁîüÊàê: Ëí∏ÁïôÂ°î„ÅÆÊ∏©Â∫¶„Éó„É≠„Éï„Ç°„Ç§„É´Ôºà20ÊÆµÔºâ„Åã„ÇâÁ¥îÂ∫¶„Çí‰∫àÊ∏¨
np.random.seed(42)
n = 500

# Ê∏©Â∫¶„Éó„É≠„Éï„Ç°„Ç§„É´Ôºà20ÊÆµ„ÅÆÊ∏©Â∫¶Ôºâ: ‰∫í„ÅÑ„Å´Áõ∏Èñ¢„ÅåÈ´ò„ÅÑ
base_profile = np.linspace(80, 160, 20)
temperature_profiles = np.array([
    base_profile + np.random.normal(0, 2, 20) for _ in range(n)
])

# Ë£ΩÂìÅÁ¥îÂ∫¶: Ê∏©Â∫¶„Éó„É≠„Éï„Ç°„Ç§„É´„Åã„ÇâË®àÁÆóÔºàÁâπ„Å´‰∏äÊÆµ„ÅÆÊ∏©Â∫¶„ÅåÈáçË¶ÅÔºâ
purity = (
    95 +
    0.05 * temperature_profiles[:, 0:5].mean(axis=1) +  # ‰∏äÊÆµ5ÊÆµ„ÅÆÂπ≥ÂùáÊ∏©Â∫¶
    0.02 * temperature_profiles[:, 15:20].mean(axis=1) +  # ‰∏ãÊÆµ5ÊÆµ„ÅÆÂπ≥ÂùáÊ∏©Â∫¶
    np.random.normal(0, 0.5, n)
)

# „Éá„Éº„Çø„Éï„É¨„Éº„É†‰ΩúÊàê
X = pd.DataFrame(temperature_profiles, columns=[f'T{i+1}' for i in range(20)])
y = pd.Series(purity, name='purity')

# „Éá„Éº„ÇøÂàÜÂâ≤
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# „Çπ„Ç±„Éº„É™„É≥„Ç∞ÔºàPLS„Å´„ÅØÂøÖÈ†àÔºâ
scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)
y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()

# ÊúÄÈÅ©„Å™ÊàêÂàÜÊï∞„ÅÆÈÅ∏ÊäûÔºà„ÇØ„É≠„Çπ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥Ôºâ
max_components = 10
cv_scores = []

for n_comp in range(1, max_components + 1):
    pls = PLSRegression(n_components=n_comp)
    scores = cross_val_score(pls, X_train_scaled, y_train_scaled, cv=5,
                             scoring='r2')
    cv_scores.append(scores.mean())

optimal_n_components = np.argmax(cv_scores) + 1
print(f"„ÄêÊúÄÈÅ©ÊàêÂàÜÊï∞„ÅÆÈÅ∏Êäû„Äë")
print(f"ÊúÄÈÅ©ÊàêÂàÜÊï∞: {optimal_n_components}")
print(f"CV R¬≤„Çπ„Ç≥„Ç¢: {max(cv_scores):.4f}")

# ÊúÄÈÅ©ÊàêÂàÜÊï∞„ÅßPLS„É¢„Éá„É´ÊßãÁØâ
pls_model = PLSRegression(n_components=optimal_n_components)
pls_model.fit(X_train_scaled, y_train_scaled)

# ‰∫àÊ∏¨Ôºà„Çπ„Ç±„Éº„É´„ÇíÊàª„ÅôÔºâ
y_train_pred_scaled = pls_model.predict(X_train_scaled)
y_test_pred_scaled = pls_model.predict(X_test_scaled)

y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).ravel()
y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).ravel()

# ÊØîËºÉ: ÈÄöÂ∏∏„ÅÆÁ∑öÂΩ¢ÂõûÂ∏∞
lr_model = LinearRegression()
lr_model.fit(X_train_scaled, y_train_scaled)
y_test_pred_lr_scaled = lr_model.predict(X_test_scaled)
y_test_pred_lr = scaler_y.inverse_transform(y_test_pred_lr_scaled.reshape(-1, 1)).ravel()

# Ë©ï‰æ°
pls_r2 = r2_score(y_test, y_test_pred)
pls_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
lr_r2 = r2_score(y_test, y_test_pred_lr)
lr_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_lr))

print(f"\n„Äê„É¢„Éá„É´ÊÄßËÉΩÊØîËºÉ„Äë")
print(f"PLS ({optimal_n_components}ÊàêÂàÜ): R¬≤ = {pls_r2:.4f}, RMSE = {pls_rmse:.4f}%")
print(f"Á∑öÂΩ¢ÂõûÂ∏∞ (20Â§âÊï∞): R¬≤ = {lr_r2:.4f}, RMSE = {lr_rmse:.4f}%")

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# ÊàêÂàÜÊï∞ vs CV Score
axes[0, 0].plot(range(1, max_components + 1), cv_scores, marker='o',
                linewidth=2, markersize=8, color='#11998e')
axes[0, 0].axvline(x=optimal_n_components, color='red', linestyle='--',
                   label=f'Optimal: {optimal_n_components} components')
axes[0, 0].set_xlabel('Number of Components', fontsize=11)
axes[0, 0].set_ylabel('Cross-Validation R¬≤', fontsize=11)
axes[0, 0].set_title('Optimal Component Selection', fontsize=12, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)

# PLS‰∫àÊ∏¨ÁµêÊûú
axes[0, 1].scatter(y_test, y_test_pred, alpha=0.6, s=50, color='#11998e')
axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                'r--', linewidth=2, label='Perfect prediction')
axes[0, 1].set_xlabel('Actual Purity (%)', fontsize=11)
axes[0, 1].set_ylabel('Predicted Purity (%)', fontsize=11)
axes[0, 1].set_title(f'PLS Model (R¬≤={pls_r2:.3f})', fontsize=12, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# Á∑öÂΩ¢ÂõûÂ∏∞‰∫àÊ∏¨ÁµêÊûú
axes[1, 0].scatter(y_test, y_test_pred_lr, alpha=0.6, s=50, color='#f59e0b')
axes[1, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                'r--', linewidth=2, label='Perfect prediction')
axes[1, 0].set_xlabel('Actual Purity (%)', fontsize=11)
axes[1, 0].set_ylabel('Predicted Purity (%)', fontsize=11)
axes[1, 0].set_title(f'Linear Regression (R¬≤={lr_r2:.3f})', fontsize=12, fontweight='bold')
axes[1, 0].legend()
axes[1, 0].grid(alpha=0.3)

# Â§âÊï∞ÈáçË¶ÅÂ∫¶ÔºàPLS„ÅÆ„É≠„Éº„Éá„Ç£„É≥„Ç∞Ôºâ
loadings = pls_model.x_loadings_[:, 0]  # Á¨¨1ÊàêÂàÜ„ÅÆ„É≠„Éº„Éá„Ç£„É≥„Ç∞
axes[1, 1].barh(X.columns, loadings, color='#11998e')
axes[1, 1].set_xlabel('Loading on 1st Component', fontsize=11)
axes[1, 1].set_ylabel('Temperature Stage', fontsize=11)
axes[1, 1].set_title('Variable Importance (PLS Loadings)', fontsize=12, fontweight='bold')
axes[1, 1].grid(alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

print("\n„ÄêPLS„ÅÆÂà©ÁÇπ„Äë")
print("‚úì Â§öÈáçÂÖ±Á∑öÊÄß„Åå„ÅÇ„ÇãÂ†¥Âêà„Åß„ÇÇÂÆâÂÆö„Åó„Åü„É¢„Éá„É´ÊßãÁØâ")
print("‚úì Â∞ë„Å™„ÅÑÊàêÂàÜÊï∞„ÅßÊÉÖÂ†±„ÇíÈõÜÁ¥ÑÔºà20Â§âÊï∞ ‚Üí 3-5ÊàêÂàÜÔºâ")
print("‚úì Ë®àÁÆóÂäπÁéá„ÅåËâØ„Åè„ÄÅ„É™„Ç¢„É´„Çø„Ç§„É†‰∫àÊ∏¨„Å´ÈÅ©„Åô„Çã")
print("‚úì Ëß£ÈáàÊÄß„ÅÆÂêë‰∏äÔºà‰∏ªË¶Å„Å™ÊΩúÂú®Â§âÊï∞„ÅÆÁêÜËß£Ôºâ")
</code></pre>

<p><strong>Âá∫Âäõ‰æã</strong>:</p>
<pre><code>„ÄêÊúÄÈÅ©ÊàêÂàÜÊï∞„ÅÆÈÅ∏Êäû„Äë
ÊúÄÈÅ©ÊàêÂàÜÊï∞: 4
CV R¬≤„Çπ„Ç≥„Ç¢: 0.8923

„Äê„É¢„Éá„É´ÊÄßËÉΩÊØîËºÉ„Äë
PLS (4ÊàêÂàÜ): R¬≤ = 0.8956, RMSE = 0.5123%
Á∑öÂΩ¢ÂõûÂ∏∞ (20Â§âÊï∞): R¬≤ = 0.8834, RMSE = 0.5456%
</code></pre>

<p><strong>Ëß£Ë™¨</strong>: PLS„ÅØ„ÄÅ20ÂÄã„ÅÆÁõ∏Èñ¢„ÅÆÈ´ò„ÅÑÂ§âÊï∞„Çí4„Å§„ÅÆÁã¨Á´ã„Å™ÊàêÂàÜ„Å´ÂúßÁ∏Æ„Åó„ÄÅÂäπÁéáÁöÑ„Å™„É¢„Éá„É´„ÇíÊßãÁØâ„Åó„Åæ„Åô„ÄÇÁâπ„Å´„ÄÅÂ§âÊï∞Êï∞„Åå„Çµ„É≥„Éó„É´Êï∞„Å´Ëøë„ÅÑÂ†¥Âêà„ÇÑ„ÄÅ„Ç™„É≥„É©„Ç§„É≥‰∫àÊ∏¨„Åß„É™„Ç¢„É´„Çø„Ç§„É†ÊÄß„ÅåÊ±Ç„ÇÅ„Çâ„Çå„ÇãÂ†¥Âêà„Å´ÊúâÂäπ„Åß„Åô„ÄÇ</p>

<h4>„Ç≥„Éº„Éâ‰æã6: PLS„Å®‰∏ªÊàêÂàÜÂõûÂ∏∞ÔºàPCRÔºâ„ÅÆÊØîËºÉ</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cross_decomposition import PLSRegression
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error

# Ââç„ÅÆ„Ç≥„Éº„Éâ‰æã„ÅÆ„Éá„Éº„Çø„Çí‰ΩøÁî®
# X, y „ÅØ„Åô„Åß„Å´ÂÆöÁæ©Ê∏à„Åø

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# „Çπ„Ç±„Éº„É™„É≥„Ç∞
scaler_X = StandardScaler()
scaler_y = StandardScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)
y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()

# PLSÔºàÁõÆÁöÑÂ§âÊï∞„ÇíËÄÉÊÖÆ„Åó„Å¶ÊàêÂàÜÊäΩÂá∫Ôºâ
pls = PLSRegression(n_components=4)
pls.fit(X_train_scaled, y_train_scaled)

# PCRÔºà‰∏ªÊàêÂàÜÂõûÂ∏∞ÔºöÁõÆÁöÑÂ§âÊï∞„ÇíËÄÉÊÖÆ„Åõ„ÅöÊàêÂàÜÊäΩÂá∫Ôºâ
pca = PCA(n_components=4)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

pcr = LinearRegression()
pcr.fit(X_train_pca, y_train_scaled)

# ‰∫àÊ∏¨
y_test_pred_pls = scaler_y.inverse_transform(
    pls.predict(X_test_scaled).reshape(-1, 1)
).ravel()

y_test_pred_pcr = scaler_y.inverse_transform(
    pcr.predict(X_test_pca).reshape(-1, 1)
).ravel()

# Ë©ï‰æ°
pls_r2 = r2_score(y_test, y_test_pred_pls)
pcr_r2 = r2_score(y_test, y_test_pred_pcr)
pls_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_pls))
pcr_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_pcr))

print("„ÄêPLS vs PCR ÊØîËºÉ„Äë")
print(f"PLS:  R¬≤ = {pls_r2:.4f}, RMSE = {pls_rmse:.4f}%")
print(f"PCR:  R¬≤ = {pcr_r2:.4f}, RMSE = {pcr_rmse:.4f}%")

# Á¥ØÁ©çÂØÑ‰∏éÁéá„ÅÆÊØîËºÉ
pls_variance = np.var(pls.x_scores_, axis=0)
pls_cumulative = np.cumsum(pls_variance) / np.sum(pls_variance)

pca_variance = pca.explained_variance_ratio_
pca_cumulative = np.cumsum(pca_variance)

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Á¥ØÁ©çÂØÑ‰∏éÁéá
axes[0].plot(range(1, 5), pls_cumulative, marker='o', linewidth=2,
             markersize=8, label='PLS', color='#11998e')
axes[0].plot(range(1, 5), pca_cumulative, marker='s', linewidth=2,
             markersize=8, label='PCA', color='#f59e0b')
axes[0].set_xlabel('Number of Components', fontsize=11)
axes[0].set_ylabel('Cumulative Variance Explained', fontsize=11)
axes[0].set_title('Variance Explained: PLS vs PCA', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# ‰∫àÊ∏¨Á≤æÂ∫¶„ÅÆÊØîËºÉ
methods = ['PLS', 'PCR']
r2_scores = [pls_r2, pcr_r2]
colors = ['#11998e', '#f59e0b']

axes[1].bar(methods, r2_scores, color=colors, alpha=0.7, edgecolor='black')
axes[1].set_ylabel('R¬≤ Score', fontsize=11)
axes[1].set_title('Prediction Performance: PLS vs PCR', fontsize=12, fontweight='bold')
axes[1].set_ylim([0.8, 1.0])
axes[1].grid(alpha=0.3, axis='y')

for i, (method, score) in enumerate(zip(methods, r2_scores)):
    axes[1].text(i, score + 0.01, f'{score:.4f}', ha='center', fontsize=11, fontweight='bold')

plt.tight_layout()
plt.show()

print("\n„ÄêPLS„Å®PCR„ÅÆÈÅï„ÅÑ„Äë")
print("PLS: ÁõÆÁöÑÂ§âÊï∞„Å®„ÅÆÂÖ±ÂàÜÊï£„ÇíÊúÄÂ§ßÂåñ„Åô„ÇãÊàêÂàÜ„ÇíÊäΩÂá∫")
print("     ‚Üí ‰∫àÊ∏¨„Å´Áõ¥Êé•Èñ¢‰øÇ„Åô„ÇãÊÉÖÂ†±„ÇíÂÑ™ÂÖàÁöÑ„Å´Âèñ„ÇäÂá∫„Åô")
print("PCR: Ë™¨ÊòéÂ§âÊï∞„ÅÆ„Åø„Åã„ÇâÂàÜÊï£„ÇíÊúÄÂ§ßÂåñ„Åô„ÇãÊàêÂàÜ„ÇíÊäΩÂá∫")
print("     ‚Üí ‰∫àÊ∏¨„Å´ÁÑ°Èñ¢‰øÇ„Å™ÊÉÖÂ†±„ÇÇÂê´„ÇÄÂèØËÉΩÊÄß")
print("\n‚Üí „Éó„É≠„Çª„Çπ„É¢„Éá„É™„É≥„Ç∞„Åß„ÅØ„ÄÅ‰∏ÄËà¨ÁöÑ„Å´PLS„ÅÆÊñπ„ÅåÂÑ™„Çå„ÅüÊÄßËÉΩ")
</code></pre>

<p><strong>Ëß£Ë™¨</strong>: PLS„ÅØPCR„Å®Áï∞„Å™„Çä„ÄÅÁõÆÁöÑÂ§âÊï∞„ÇíËÄÉÊÖÆ„Åó„Å¶ÊΩúÂú®Â§âÊï∞„ÇíÊäΩÂá∫„Åô„Çã„Åü„ÇÅ„ÄÅ‰∫àÊ∏¨ÊÄßËÉΩ„ÅåÈ´ò„Åè„Å™„Çä„Åæ„Åô„ÄÇ„Éó„É≠„Çª„Çπ„Éá„Éº„Çø„ÅÆ„Çà„ÅÜ„Å™È´òÊ¨°ÂÖÉ„Éá„Éº„Çø„Åß„ÅØ„ÄÅPLS„ÅåÁ¨¨‰∏ÄÈÅ∏Êäû„Å®„Å™„Çä„Åæ„Åô„ÄÇ</p>

<hr />

<h2>3.3 „ÇΩ„Éï„Éà„Çª„É≥„Çµ„Éº„ÅÆÊ¶ÇÂøµ„Å®ÂÆüË£Ö</h2>

<p><strong>„ÇΩ„Éï„Éà„Çª„É≥„Çµ„ÉºÔºàSoft SensorÔºâ</strong>„ÅØ„ÄÅÊ∏¨ÂÆö„ÅåÂõ∞Èõ£„Åæ„Åü„ÅØÈ´ò„Ç≥„Çπ„Éà„Å™ÂìÅË≥™Â§âÊï∞„Çí„ÄÅÊ∏¨ÂÆö„ÅåÂÆπÊòì„Å™„Éó„É≠„Çª„ÇπÂ§âÊï∞„Åã„ÇâÊé®ÂÆö„Åô„ÇãÊäÄË°ì„Åß„Åô„ÄÇ„Éó„É≠„Çª„ÇπÁî£Ê•≠„ÅßÂ∫É„ÅèÊ¥ªÁî®„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ</p>

<h3>„ÇΩ„Éï„Éà„Çª„É≥„Çµ„Éº„ÅÆÂÖ∏ÂûãÁöÑ„Å™Áî®ÈÄî</h3>

<table>
<thead>
<tr>
<th>Áî£Ê•≠</th>
<th>Êé®ÂÆöÂØæË±°ÔºàYÔºâ</th>
<th>ÂÖ•ÂäõÂ§âÊï∞ÔºàXÔºâ</th>
<th>ÂäπÊûú</th>
</tr>
</thead>
<tbody>
<tr>
<td>ÂåñÂ≠¶„Éó„É©„É≥„Éà</td>
<td>Ë£ΩÂìÅÁ¥îÂ∫¶</td>
<td>Ê∏©Â∫¶„Éó„É≠„Éï„Ç°„Ç§„É´„ÄÅÂúßÂäõ„ÄÅÊµÅÈáè</td>
<td>„É™„Ç¢„É´„Çø„Ç§„É†ÂìÅË≥™ÁÆ°ÁêÜ</td>
</tr>
<tr>
<td>Ë£ΩÈâÑ</td>
<td>ÈãºÊùê„ÅÆÊ©üÊ¢∞ÁöÑÂº∑Â∫¶</td>
<td>ÊàêÂàÜÁµÑÊàê„ÄÅÂä†ÁÜ±Ê∏©Â∫¶„ÄÅÂÜ∑Âç¥ÈÄüÂ∫¶</td>
<td>ÂìÅË≥™‰∫àÊ∏¨„ÄÅ‰∏çËâØÂâäÊ∏õ</td>
</tr>
<tr>
<td>Ë£ΩËñ¨</td>
<td>ÊúâÂäπÊàêÂàÜÂê´Èáè</td>
<td>ÂèçÂøúÊ∏©Â∫¶„ÄÅpH„ÄÅÊî™ÊãåÈÄüÂ∫¶</td>
<td>„Éê„ÉÉ„ÉÅÂìÅË≥™‰øùË®º</td>
</tr>
<tr>
<td>ÂçäÂ∞é‰Ωì</td>
<td>ËÜúÂéö„ÄÅÁµÑÊàê</td>
<td>„Éó„É≠„Çª„Çπ„Ç¨„ÇπÊµÅÈáè„ÄÅÊ∏©Â∫¶„ÄÅÂúßÂäõ</td>
<td>Ê≠©Áïô„Åæ„ÇäÂêë‰∏ä</td>
</tr>
</tbody>
</table>

<h4>„Ç≥„Éº„Éâ‰æã7: Ëí∏ÁïôÂ°î„ÇΩ„Éï„Éà„Çª„É≥„Çµ„Éº„ÅÆË®≠Ë®à„Å®ÂÆüË£Ö</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cross_decomposition import PLSRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# „Çµ„É≥„Éó„É´„Éá„Éº„ÇøÁîüÊàê: Ëí∏ÁïôÂ°î„ÅÆÈÅãËª¢„Éá„Éº„Çø
np.random.seed(42)
n = 1000

# ÊôÇÁ≥ªÂàó„Éá„Éº„ÇøÔºà10Êó•Èñì„ÄÅÂàÜÂçò‰ΩçÔºâ
dates = pd.date_range('2025-01-01', periods=n, freq='15min')

# „Ç™„É≥„É©„Ç§„É≥Ê∏¨ÂÆöÂèØËÉΩ„Å™Â§âÊï∞Ôºà„ÇΩ„Éï„Éà„Çª„É≥„Çµ„Éº„ÅÆÂÖ•ÂäõÔºâ
df = pd.DataFrame({
    'feed_temp': np.random.normal(60, 3, n),
    'top_temp': np.random.normal(85, 2, n),
    'bottom_temp': np.random.normal(155, 4, n),
    'reflux_ratio': np.random.uniform(2.0, 3.0, n),
    'reboiler_duty': np.random.normal(1500, 100, n),
    'feed_rate': np.random.normal(100, 8, n),
    'pressure': np.random.normal(1.2, 0.08, n)
}, index=dates)

# „Ç™„Éï„É©„Ç§„É≥Ê∏¨ÂÆö„Åï„Çå„ÇãÂìÅË≥™Â§âÊï∞Ôºà„ÇΩ„Éï„Éà„Çª„É≥„Çµ„Éº„ÅÆÂá∫ÂäõÔºâ
# ÁèæÂÆü: 1Êó•1Âõû„ÅÆGCÂàÜÊûêÔºàÈ´ò„Ç≥„Çπ„Éà„ÄÅÊôÇÈñìÈÅÖ„ÇåÔºâ
# „ÇΩ„Éï„Éà„Çª„É≥„Çµ„Éº: „É™„Ç¢„É´„Çø„Ç§„É†‰∫àÊ∏¨
df['purity'] = (
    95 +
    0.05 * df['feed_temp'] +
    0.2 * (df['top_temp'] - 85) +
    0.5 * df['reflux_ratio'] +
    0.001 * df['reboiler_duty'] +
    1.5 * df['pressure'] +
    np.random.normal(0, 0.3, n)
)

# „Ç™„Éï„É©„Ç§„É≥Ê∏¨ÂÆö„Çí„Ç∑„Éü„É•„É¨„Éº„ÉàÔºà1Êó•1Âõû = 96„Çµ„É≥„Éó„É´„Å´1ÂõûÔºâ
df['purity_measured'] = np.nan
df.loc[df.index[::96], 'purity_measured'] = df.loc[df.index[::96], 'purity']

print(f"„Äê„Éá„Éº„ÇøÊ¶ÇË¶Å„Äë")
print(f"ÂÖ®„Éá„Éº„ÇøÊï∞: {len(df)}")
print(f"„Ç™„Éï„É©„Ç§„É≥Ê∏¨ÂÆöÊï∞: {df['purity_measured'].notna().sum()}‰ª∂Ôºà{df['purity_measured'].notna().sum()/len(df)*100:.1f}%Ôºâ")
print(f"Ê∏¨ÂÆöÈ†ªÂ∫¶: 1Êó•1ÂõûÔºàÂÆüÈöõ„ÅØ15ÂàÜ„Åî„Å®„Å´„Éá„Éº„ÇøÂèéÈõÜÔºâ")

# „ÇΩ„Éï„Éà„Çª„É≥„Çµ„Éº„ÅÆÊßãÁØâÔºà„Ç™„Éï„É©„Ç§„É≥Ê∏¨ÂÆö„Éá„Éº„Çø„ÅÆ„Åø‰ΩøÁî®Ôºâ
train_data = df[df['purity_measured'].notna()].copy()
X = train_data[['feed_temp', 'top_temp', 'bottom_temp', 'reflux_ratio',
                'reboiler_duty', 'feed_rate', 'pressure']]
y = train_data['purity_measured']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# „Çπ„Ç±„Éº„É™„É≥„Ç∞
scaler_X = StandardScaler()
scaler_y = StandardScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)
y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()

# PLS„É¢„Éá„É´ÊßãÁØâ
pls_soft_sensor = PLSRegression(n_components=5)
pls_soft_sensor.fit(X_train_scaled, y_train_scaled)

# „ÉÜ„Çπ„Éà„Éá„Éº„Çø„ÅßË©ï‰æ°
y_test_pred_scaled = pls_soft_sensor.predict(X_test_scaled)
y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).ravel()

r2 = r2_score(y_test, y_test_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
mae = mean_absolute_error(y_test, y_test_pred)

print(f"\n„Äê„ÇΩ„Éï„Éà„Çª„É≥„Çµ„ÉºÊÄßËÉΩ„Äë")
print(f"R¬≤: {r2:.4f}")
print(f"RMSE: {rmse:.4f}%")
print(f"MAE: {mae:.4f}%")

# ÂÖ®„Éá„Éº„Çø„Å´ÂØæ„Åó„Å¶„É™„Ç¢„É´„Çø„Ç§„É†‰∫àÊ∏¨
X_all = df[['feed_temp', 'top_temp', 'bottom_temp', 'reflux_ratio',
            'reboiler_duty', 'feed_rate', 'pressure']]
X_all_scaled = scaler_X.transform(X_all)
y_all_pred_scaled = pls_soft_sensor.predict(X_all_scaled)
df['purity_soft_sensor'] = scaler_y.inverse_transform(y_all_pred_scaled.reshape(-1, 1)).ravel()

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(3, 1, figsize=(16, 12))

# ÊôÇÁ≥ªÂàó„Éó„É≠„ÉÉ„Éà: ÂÆüÊ∏¨ vs „ÇΩ„Éï„Éà„Çª„É≥„Çµ„Éº‰∫àÊ∏¨
time_window = slice('2025-01-01', '2025-01-03')  # ÊúÄÂàù„ÅÆ3Êó•Èñì
axes[0].plot(df.loc[time_window].index, df.loc[time_window, 'purity'],
             linewidth=1, alpha=0.5, label='True Purity (unknown in practice)', color='gray')
axes[0].scatter(df.loc[time_window].index, df.loc[time_window, 'purity_measured'],
                s=100, color='red', marker='o', label='Offline Measurement (1/day)', zorder=3)
axes[0].plot(df.loc[time_window].index, df.loc[time_window, 'purity_soft_sensor'],
             linewidth=2, color='#11998e', label='Soft Sensor Prediction (real-time)')
axes[0].set_ylabel('Product Purity (%)', fontsize=11)
axes[0].set_title('Soft Sensor: Real-time Quality Prediction', fontsize=13, fontweight='bold')
axes[0].legend(loc='upper right')
axes[0].grid(alpha=0.3)

# ‰∫àÊ∏¨ vs ÂÆüÊ∏¨Ôºà„ÉÜ„Çπ„Éà„Éá„Éº„ÇøÔºâ
axes[1].scatter(y_test, y_test_pred, alpha=0.6, s=50, color='#11998e')
axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
             'r--', linewidth=2, label='Perfect prediction')
axes[1].set_xlabel('Measured Purity (%)', fontsize=11)
axes[1].set_ylabel('Predicted Purity (%)', fontsize=11)
axes[1].set_title(f'Soft Sensor Accuracy (R¬≤={r2:.3f})', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

# Ë™§Â∑ÆÂàÜÂ∏É
errors = y_test - y_test_pred
axes[2].hist(errors, bins=30, color='#11998e', alpha=0.7, edgecolor='black')
axes[2].axvline(x=0, color='red', linestyle='--', linewidth=2)
axes[2].set_xlabel('Prediction Error (%)', fontsize=11)
axes[2].set_ylabel('Frequency', fontsize=11)
axes[2].set_title(f'Error Distribution (MAE={mae:.3f}%)', fontsize=12, fontweight='bold')
axes[2].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("\n„Äê„ÇΩ„Éï„Éà„Çª„É≥„Çµ„Éº„ÅÆÂäπÊûú„Äë")
print(f"‚úì „Ç™„Éï„É©„Ç§„É≥Ê∏¨ÂÆö: 1Êó•1Âõû ‚Üí „ÇΩ„Éï„Éà„Çª„É≥„Çµ„Éº: 15ÂàÜ„Åî„Å®Ôºà96ÂÄç„ÅÆÈ†ªÂ∫¶Ôºâ")
print(f"‚úì Ê∏¨ÂÆö„Ç≥„Çπ„ÉàÂâäÊ∏õ: GCÂàÜÊûê ¬•5,000/Âõû √ó 365Âõû/Âπ¥ = ¬•182‰∏á/Âπ¥ ‚Üí „Åª„Åº„Çº„É≠")
print(f"‚úì „É™„Ç¢„É´„Çø„Ç§„É†ÂìÅË≥™ÁÆ°ÁêÜ: Áï∞Â∏∏„ÅÆÊó©ÊúüÁô∫Ë¶ã„ÄÅÂç≥Â∫ß„ÅÆÂà∂Âæ°ÂØæÂøú„ÅåÂèØËÉΩ")
print(f"‚úì „Éó„É≠„Çª„ÇπÊúÄÈÅ©Âåñ: Â∏∏ÊôÇÂìÅË≥™Áõ£Ë¶ñ„Å´„Çà„Çä„ÄÅÊúÄÈÅ©Êù°‰ª∂Êé¢Á¥¢„ÅåÂä†ÈÄü")
</code></pre>

<p><strong>Âá∫Âäõ‰æã</strong>:</p>
<pre><code>„Äê„Éá„Éº„ÇøÊ¶ÇË¶Å„Äë
ÂÖ®„Éá„Éº„ÇøÊï∞: 1000
„Ç™„Éï„É©„Ç§„É≥Ê∏¨ÂÆöÊï∞: 11‰ª∂Ôºà1.1%Ôºâ
Ê∏¨ÂÆöÈ†ªÂ∫¶: 1Êó•1ÂõûÔºàÂÆüÈöõ„ÅØ15ÂàÜ„Åî„Å®„Å´„Éá„Éº„ÇøÂèéÈõÜÔºâ

„Äê„ÇΩ„Éï„Éà„Çª„É≥„Çµ„ÉºÊÄßËÉΩ„Äë
R¬≤: 0.9234
RMSE: 0.4567%
MAE: 0.3456%
</code></pre>

<p><strong>Ëß£Ë™¨</strong>: „ÇΩ„Éï„Éà„Çª„É≥„Çµ„Éº„Å´„Çà„Çä„ÄÅ‰ΩéÈ†ªÂ∫¶„ÅßÈ´ò„Ç≥„Çπ„Éà„Å™„Ç™„Éï„É©„Ç§„É≥Ê∏¨ÂÆö„Çí„ÄÅÈ´òÈ†ªÂ∫¶„Åß„Ç≥„Çπ„Éà„Çº„É≠„ÅÆ„É™„Ç¢„É´„Çø„Ç§„É†‰∫àÊ∏¨„Å´ÁΩÆ„ÅçÊèõ„Åà„Åæ„Åô„ÄÇ„Åì„Çå„ÅåPI„ÅÆÊúÄ„ÇÇÂÆüÁî®ÁöÑ„Å™ÂøúÁî®„ÅÆ1„Å§„Åß„Åô„ÄÇ</p>

<h4>„Ç≥„Éº„Éâ‰æã8: „ÇΩ„Éï„Éà„Çª„É≥„Çµ„Éº„ÅÆÈÅãÁî®„Å®‰øùÂÆà</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cross_decomposition import PLSRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score

# Ââç„ÅÆ„Ç≥„Éº„Éâ‰æã„ÅÆ„ÇΩ„Éï„Éà„Çª„É≥„Çµ„Éº„Çí‰ΩøÁî®
# pls_soft_sensor, scaler_X, scaler_y „ÅØ„Åô„Åß„Å´ÂÆöÁæ©Ê∏à„Åø

# „Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥: ÈÅãËª¢Êù°‰ª∂„ÅåÂ§âÂåñÔºà„Éó„É≠„Çª„Çπ„Éâ„É™„Éï„ÉàÔºâ
# 3„É∂ÊúàÂæå„ÄÅÂéüÊñôÁµÑÊàê„ÅåÂ§âÂåñ„Åó„ÄÅ„É¢„Éá„É´„ÅÆÁ≤æÂ∫¶„Åå‰Ωé‰∏ã

# Êñ∞„Åó„ÅÑÈÅãËª¢„Éá„Éº„ÇøÁîüÊàêÔºà„Éó„É≠„Çª„ÇπÁâπÊÄß„Åå„Éâ„É™„Éï„ÉàÔºâ
np.random.seed(100)
n_new = 300
dates_new = pd.date_range('2025-04-01', periods=n_new, freq='15min')

df_new = pd.DataFrame({
    'feed_temp': np.random.normal(62, 3, n_new),  # Âπ≥Âùá„Åå2¬∞C‰∏äÊòá
    'top_temp': np.random.normal(87, 2, n_new),   # Âπ≥Âùá„Åå2¬∞C‰∏äÊòá
    'bottom_temp': np.random.normal(155, 4, n_new),
    'reflux_ratio': np.random.uniform(2.0, 3.0, n_new),
    'reboiler_duty': np.random.normal(1550, 100, n_new),  # Âπ≥Âùá„Åå50kW‰∏äÊòá
    'feed_rate': np.random.normal(100, 8, n_new),
    'pressure': np.random.normal(1.2, 0.08, n_new)
}, index=dates_new)

# Áúü„ÅÆÁ¥îÂ∫¶Ôºà„Éó„É≠„Çª„ÇπÁâπÊÄß„ÅåÂ§âÂåñÔºâ
df_new['purity'] = (
    93 +  # „Éô„Éº„Çπ„É©„Ç§„É≥„Åå2%‰Ωé‰∏ãÔºàÂéüÊñôÁµÑÊàêÂ§âÂåñ„ÅÆÂΩ±ÈüøÔºâ
    0.05 * df_new['feed_temp'] +
    0.2 * (df_new['top_temp'] - 85) +
    0.5 * df_new['reflux_ratio'] +
    0.001 * df_new['reboiler_duty'] +
    1.5 * df_new['pressure'] +
    np.random.normal(0, 0.3, n_new)
)

# „Ç™„Éï„É©„Ç§„É≥Ê∏¨ÂÆöÔºàÈÄ±1ÂõûÔºâ
df_new['purity_measured'] = np.nan
df_new.loc[df_new.index[::672], 'purity_measured'] = df_new.loc[df_new.index[::672], 'purity']

# Êó¢Â≠ò„ÅÆ„ÇΩ„Éï„Éà„Çª„É≥„Çµ„Éº„Åß‰∫àÊ∏¨
X_new = df_new[['feed_temp', 'top_temp', 'bottom_temp', 'reflux_ratio',
                'reboiler_duty', 'feed_rate', 'pressure']]
X_new_scaled = scaler_X.transform(X_new)
y_new_pred_scaled = pls_soft_sensor.predict(X_new_scaled)
df_new['purity_soft_sensor_old'] = scaler_y.inverse_transform(y_new_pred_scaled.reshape(-1, 1)).ravel()

# ÊÄßËÉΩË©ï‰æ°ÔºàÊó¢Â≠ò„É¢„Éá„É´Ôºâ
old_model_r2 = r2_score(df_new['purity'], df_new['purity_soft_sensor_old'])
old_model_mae = np.abs(df_new['purity'] - df_new['purity_soft_sensor_old']).mean()

print("„Äê„ÇΩ„Éï„Éà„Çª„É≥„Çµ„Éº„ÅÆÊÄßËÉΩÂä£Âåñ„Äë")
print(f"Êó¢Â≠ò„É¢„Éá„É´Ôºà3„É∂ÊúàÂâçÊßãÁØâÔºâ: R¬≤ = {old_model_r2:.4f}, MAE = {old_model_mae:.4f}%")

# „ÇΩ„Éï„Éà„Çª„É≥„Çµ„Éº„ÅÆÂÜçÂ≠¶ÁøíÔºà„Ç™„É≥„É©„Ç§„É≥Êõ¥Êñ∞Ôºâ
# Êñ∞„Åó„ÅÑ„Ç™„Éï„É©„Ç§„É≥Ê∏¨ÂÆö„Éá„Éº„Çø„ÇíËøΩÂä†„Åó„Å¶ÂÜçÂ≠¶Áøí
train_new = df_new[df_new['purity_measured'].notna()].copy()
X_retrain = train_new[['feed_temp', 'top_temp', 'bottom_temp', 'reflux_ratio',
                        'reboiler_duty', 'feed_rate', 'pressure']]
y_retrain = train_new['purity_measured']

# Êó¢Â≠ò„Éá„Éº„Çø„Å®Êñ∞„Éá„Éº„Çø„ÇíÁµêÂêà
X_combined = pd.concat([X, X_retrain])
y_combined = pd.concat([y, y_retrain])

# ÂÜç„Çπ„Ç±„Éº„É™„É≥„Ç∞„Å®ÂÜçÂ≠¶Áøí
scaler_X_new = StandardScaler()
scaler_y_new = StandardScaler()
X_combined_scaled = scaler_X_new.fit_transform(X_combined)
y_combined_scaled = scaler_y_new.fit_transform(y_combined.values.reshape(-1, 1)).ravel()

pls_updated = PLSRegression(n_components=5)
pls_updated.fit(X_combined_scaled, y_combined_scaled)

# Êõ¥Êñ∞„É¢„Éá„É´„Åß‰∫àÊ∏¨
X_new_scaled_updated = scaler_X_new.transform(X_new)
y_new_pred_updated_scaled = pls_updated.predict(X_new_scaled_updated)
df_new['purity_soft_sensor_updated'] = scaler_y_new.inverse_transform(
    y_new_pred_updated_scaled.reshape(-1, 1)
).ravel()

# ÊÄßËÉΩË©ï‰æ°ÔºàÊõ¥Êñ∞„É¢„Éá„É´Ôºâ
updated_model_r2 = r2_score(df_new['purity'], df_new['purity_soft_sensor_updated'])
updated_model_mae = np.abs(df_new['purity'] - df_new['purity_soft_sensor_updated']).mean()

print(f"Êõ¥Êñ∞„É¢„Éá„É´ÔºàÊúÄÊñ∞„Éá„Éº„Çø„ÅßÂÜçÂ≠¶ÁøíÔºâ: R¬≤ = {updated_model_r2:.4f}, MAE = {updated_model_mae:.4f}%")
print(f"\nÊÄßËÉΩÊîπÂñÑ: R¬≤ {old_model_r2:.4f} ‚Üí {updated_model_r2:.4f} (+{(updated_model_r2-old_model_r2)*100:.2f}%)")
print(f"           MAE {old_model_mae:.4f}% ‚Üí {updated_model_mae:.4f}% (-{(old_model_mae-updated_model_mae):.4f}%)")

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(2, 1, figsize=(16, 10))

# ÊôÇÁ≥ªÂàóÊØîËºÉ
time_window = slice('2025-04-01', '2025-04-03')
axes[0].plot(df_new.loc[time_window].index, df_new.loc[time_window, 'purity'],
             linewidth=2, alpha=0.7, label='True Purity', color='black')
axes[0].plot(df_new.loc[time_window].index, df_new.loc[time_window, 'purity_soft_sensor_old'],
             linewidth=2, alpha=0.8, label='Old Model (drift)', color='red')
axes[0].plot(df_new.loc[time_window].index, df_new.loc[time_window, 'purity_soft_sensor_updated'],
             linewidth=2, alpha=0.8, label='Updated Model', color='#11998e')
axes[0].set_ylabel('Product Purity (%)', fontsize=11)
axes[0].set_title('Soft Sensor Performance: Before and After Update', fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# Ë™§Â∑Æ„ÅÆÊØîËºÉ
errors_old = df_new['purity'] - df_new['purity_soft_sensor_old']
errors_updated = df_new['purity'] - df_new['purity_soft_sensor_updated']

axes[1].hist(errors_old, bins=40, alpha=0.6, label='Old Model', color='red', edgecolor='black')
axes[1].hist(errors_updated, bins=40, alpha=0.6, label='Updated Model', color='#11998e', edgecolor='black')
axes[1].axvline(x=0, color='black', linestyle='--', linewidth=2)
axes[1].set_xlabel('Prediction Error (%)', fontsize=11)
axes[1].set_ylabel('Frequency', fontsize=11)
axes[1].set_title('Error Distribution Comparison', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("\n„Äê„ÇΩ„Éï„Éà„Çª„É≥„Çµ„ÉºÈÅãÁî®„ÅÆÈáçË¶Å„Éù„Ç§„É≥„Éà„Äë")
print("1. ÂÆöÊúüÁöÑ„Å™„É¢„Éá„É´ÊÄßËÉΩÁõ£Ë¶ñÔºà‰∫àÊ∏¨Ë™§Â∑Æ„ÅÆ„Éà„É¨„É≥„ÉâÂàÜÊûêÔºâ")
print("2. „Éó„É≠„Çª„Çπ„Éâ„É™„Éï„ÉàÊ§úÂá∫ÔºàÁµ±Ë®àÁöÑÂ∑•Á®ãÁÆ°ÁêÜ„ÄÅCUSUMÁ≠âÔºâ")
print("3. ÂÆöÊúüÁöÑ„Å™„É¢„Éá„É´Êõ¥Êñ∞ÔºàÊúàÊ¨°„ÄúÂõõÂçäÊúü„Åî„Å®Ôºâ")
print("4. „Ç™„Éï„É©„Ç§„É≥Ê∏¨ÂÆö„Éá„Éº„Çø„ÅÆÁ∂ôÁ∂öÁöÑÂèéÈõÜÔºàÂÜçÂ≠¶ÁøíÁî®Ôºâ")
print("5. „É¢„Éá„É´„Éê„Éº„Ç∏„Éß„É≥ÁÆ°ÁêÜ„Å®„É≠„Éº„É´„Éê„ÉÉ„ÇØÊ©üËÉΩ")
</code></pre>

<p><strong>Ëß£Ë™¨</strong>: „ÇΩ„Éï„Éà„Çª„É≥„Çµ„Éº„ÅØ„Äå‰Ωú„Å£„Å¶ÁµÇ„Çè„Çä„Äç„Åß„ÅØ„Å™„Åè„ÄÅÁ∂ôÁ∂öÁöÑ„Å™‰øùÂÆà„ÅåÂøÖË¶Å„Åß„Åô„ÄÇ„Éó„É≠„Çª„ÇπÁâπÊÄß„ÅÆÂ§âÂåñÔºà„Éâ„É™„Éï„ÉàÔºâ„ÇíÁõ£Ë¶ñ„Åó„ÄÅÂÆöÊúüÁöÑ„Å™ÂÜçÂ≠¶Áøí„Åß„É¢„Éá„É´ÊÄßËÉΩ„ÇíÁ∂≠ÊåÅ„Åó„Åæ„Åô„ÄÇ</p>

<hr />

<h2>3.4 „É¢„Éá„É´Ë©ï‰æ°ÊåáÊ®ô</h2>

<p>„É¢„Éá„É´„ÅÆÊÄßËÉΩ„ÇíÊ≠£Á¢∫„Å´Ë©ï‰æ°„Åô„Çã„Å´„ÅØ„ÄÅÈÅ©Âàá„Å™ÊåáÊ®ô„Å®„Éê„É™„Éá„Éº„Ç∑„Éß„É≥ÊâãÊ≥ï„ÅÆÁêÜËß£„Åå‰∏çÂèØÊ¨†„Åß„Åô„ÄÇ</p>

<h3>‰∏ªË¶Å„Å™Ë©ï‰æ°ÊåáÊ®ô</h3>

<table>
<thead>
<tr>
<th>ÊåáÊ®ô</th>
<th>Âºè</th>
<th>ÁâπÂæ¥</th>
<th>Ëß£Èáà</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>R¬≤</strong><br>(Ê±∫ÂÆö‰øÇÊï∞)</td>
<td>$R^2 = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}$</td>
<td>0„Äú1„ÅÆÁØÑÂõ≤<br>1„Å´Ëøë„ÅÑ„Åª„Å©ËâØ„ÅÑ</td>
<td>„É¢„Éá„É´„ÅåË™¨Êòé„Åß„Åç„Çã<br>ÂàÜÊï£„ÅÆÂâ≤Âêà</td>
</tr>
<tr>
<td><strong>RMSE</strong><br>(‰∫å‰πóÂπ≥Âùá<br>Âπ≥ÊñπÊ†πË™§Â∑Æ)</td>
<td>$\text{RMSE} = \sqrt{\frac{1}{n}\sum(y_i - \hat{y}_i)^2}$</td>
<td>ÂÖÉ„Éá„Éº„Çø„Å®Âêå„ÅòÂçò‰Ωç<br>Â§ñ„ÇåÂÄ§„Å´ÊïèÊÑü</td>
<td>‰∫àÊ∏¨Ë™§Â∑Æ„ÅÆ<br>Ê®ôÊ∫ñÁöÑ„Å™Â§ß„Åç„Åï</td>
</tr>
<tr>
<td><strong>MAE</strong><br>(Âπ≥ÂùáÁµ∂ÂØæË™§Â∑Æ)</td>
<td>$\text{MAE} = \frac{1}{n}\sum|y_i - \hat{y}_i|$</td>
<td>ÂÖÉ„Éá„Éº„Çø„Å®Âêå„ÅòÂçò‰Ωç<br>Â§ñ„ÇåÂÄ§„Å´„É≠„Éê„Çπ„Éà</td>
<td>‰∫àÊ∏¨Ë™§Â∑Æ„ÅÆ<br>Âπ≥ÂùáÁöÑ„Å™Â§ß„Åç„Åï</td>
</tr>
<tr>
<td><strong>MAPE</strong><br>(Âπ≥ÂùáÁµ∂ÂØæ<br>„Éë„Éº„Çª„É≥„ÉàË™§Â∑Æ)</td>
<td>$\text{MAPE} = \frac{100}{n}\sum\frac{|y_i - \hat{y}_i|}{|y_i|}$</td>
<td>„Éë„Éº„Çª„É≥„ÉàË°®Á§∫<br>„Çπ„Ç±„Éº„É´„Å´‰æùÂ≠ò„Åó„Å™„ÅÑ</td>
<td>Áõ∏ÂØæË™§Â∑Æ„ÅÆ<br>Âπ≥Âùá</td>
</tr>
</tbody>
</table>

<h4>„Ç≥„Éº„Éâ‰æã9: „É¢„Éá„É´Ë©ï‰æ°ÊåáÊ®ô„ÅÆÂÆüË£Ö„Å®Ëß£Èáà</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# „Çµ„É≥„Éó„É´„Éá„Éº„ÇøÁîüÊàê
np.random.seed(42)
n = 300

X = pd.DataFrame({
    'temp': np.random.uniform(160, 190, n),
    'pressure': np.random.uniform(1.0, 2.0, n),
    'flow': np.random.uniform(40, 60, n)
})

y = (70 + 0.3*X['temp'] + 10*X['pressure'] + 0.2*X['flow'] +
     np.random.normal(0, 3, n))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 2„Å§„ÅÆ„É¢„Éá„É´„ÇíÊØîËºÉ
model_lr = LinearRegression().fit(X_train, y_train)
model_rf = RandomForestRegressor(n_estimators=100, random_state=42).fit(X_train, y_train)

y_pred_lr = model_lr.predict(X_test)
y_pred_rf = model_rf.predict(X_test)

# Ë©ï‰æ°ÊåáÊ®ô„ÅÆË®àÁÆóÈñ¢Êï∞
def evaluate_model(y_true, y_pred, model_name):
    r2 = r2_score(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100

    # ËøΩÂä†ÊåáÊ®ô
    max_error = np.max(np.abs(y_true - y_pred))
    residuals = y_true - y_pred

    results = {
        'Model': model_name,
        'R¬≤': r2,
        'RMSE': rmse,
        'MAE': mae,
        'MAPE (%)': mape,
        'Max Error': max_error,
        'Residual Mean': residuals.mean(),
        'Residual Std': residuals.std()
    }
    return results

# ‰∏°„É¢„Éá„É´„ÅÆË©ï‰æ°
results_lr = evaluate_model(y_test, y_pred_lr, 'Linear Regression')
results_rf = evaluate_model(y_test, y_pred_rf, 'Random Forest')

results_df = pd.DataFrame([results_lr, results_rf])
print("„Äê„É¢„Éá„É´ÊÄßËÉΩÊØîËºÉ„Äë")
print(results_df.to_string(index=False))

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# ‰∫àÊ∏¨ vs ÂÆüÊ∏¨ÔºàÁ∑öÂΩ¢ÂõûÂ∏∞Ôºâ
axes[0, 0].scatter(y_test, y_pred_lr, alpha=0.6, s=50, color='#11998e')
axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                'r--', linewidth=2, label='Perfect prediction')
axes[0, 0].set_xlabel('Actual', fontsize=11)
axes[0, 0].set_ylabel('Predicted', fontsize=11)
axes[0, 0].set_title(f'Linear Regression (R¬≤={results_lr["R¬≤"]:.3f}, RMSE={results_lr["RMSE"]:.2f})',
                     fontsize=11, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)

# ‰∫àÊ∏¨ vs ÂÆüÊ∏¨ÔºàRandom ForestÔºâ
axes[0, 1].scatter(y_test, y_pred_rf, alpha=0.6, s=50, color='#f59e0b')
axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                'r--', linewidth=2, label='Perfect prediction')
axes[0, 1].set_xlabel('Actual', fontsize=11)
axes[0, 1].set_ylabel('Predicted', fontsize=11)
axes[0, 1].set_title(f'Random Forest (R¬≤={results_rf["R¬≤"]:.3f}, RMSE={results_rf["RMSE"]:.2f})',
                     fontsize=11, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# Ë©ï‰æ°ÊåáÊ®ô„ÅÆÊØîËºÉÔºàÊ£í„Ç∞„É©„ÉïÔºâ
metrics = ['R¬≤', 'MAE', 'MAPE (%)']
lr_scores = [results_lr['R¬≤'], results_lr['MAE'], results_lr['MAPE (%)']]
rf_scores = [results_rf['R¬≤'], results_rf['MAE'], results_rf['MAPE (%)']]

x = np.arange(len(metrics))
width = 0.35

axes[1, 0].bar(x - width/2, lr_scores, width, label='Linear Regression',
               color='#11998e', alpha=0.7)
axes[1, 0].bar(x + width/2, rf_scores, width, label='Random Forest',
               color='#f59e0b', alpha=0.7)
axes[1, 0].set_ylabel('Score', fontsize=11)
axes[1, 0].set_title('Evaluation Metrics Comparison', fontsize=12, fontweight='bold')
axes[1, 0].set_xticks(x)
axes[1, 0].set_xticklabels(metrics)
axes[1, 0].legend()
axes[1, 0].grid(alpha=0.3, axis='y')

# ÊÆãÂ∑ÆÂàÜÂ∏É„ÅÆÊØîËºÉ
residuals_lr = y_test - y_pred_lr
residuals_rf = y_test - y_pred_rf

axes[1, 1].hist(residuals_lr, bins=20, alpha=0.6, label='Linear Regression',
                color='#11998e', edgecolor='black')
axes[1, 1].hist(residuals_rf, bins=20, alpha=0.6, label='Random Forest',
                color='#f59e0b', edgecolor='black')
axes[1, 1].axvline(x=0, color='red', linestyle='--', linewidth=2)
axes[1, 1].set_xlabel('Residuals', fontsize=11)
axes[1, 1].set_ylabel('Frequency', fontsize=11)
axes[1, 1].set_title('Residual Distribution', fontsize=12, fontweight='bold')
axes[1, 1].legend()
axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("\n„ÄêÊåáÊ®ô„ÅÆËß£Èáà„Äë")
print("R¬≤: „É¢„Éá„É´„ÅåË™¨Êòé„Åß„Åç„ÇãÂàÜÊï£„ÅÆÂâ≤ÂêàÔºà1„Å´Ëøë„ÅÑ„Åª„Å©ËâØ„ÅÑÔºâ")
print("RMSE: ‰∫àÊ∏¨Ë™§Â∑Æ„ÅÆÊ®ôÊ∫ñÁöÑ„Å™Â§ß„Åç„ÅïÔºàÂ∞è„Åï„ÅÑ„Åª„Å©ËâØ„ÅÑ„ÄÅÂ§ñ„ÇåÂÄ§„Å´ÊïèÊÑüÔºâ")
print("MAE: ‰∫àÊ∏¨Ë™§Â∑Æ„ÅÆÂπ≥ÂùáÁöÑ„Å™Â§ß„Åç„ÅïÔºàÂ∞è„Åï„ÅÑ„Åª„Å©ËâØ„ÅÑ„ÄÅÂ§ñ„ÇåÂÄ§„Å´„É≠„Éê„Çπ„ÉàÔºâ")
print("MAPE: Áõ∏ÂØæË™§Â∑Æ„ÅÆÂπ≥ÂùáÔºàÔºÖË°®Á§∫„ÄÅ„Çπ„Ç±„Éº„É´„Å´‰æùÂ≠ò„Åó„Å™„ÅÑÔºâ")
print("\n„ÄêÈÅ∏ÊäûÂü∫Ê∫ñ„Äë")
print("‚úì R¬≤: ÂÖ®‰ΩìÁöÑ„Å™„Éï„Ç£„ÉÉ„ÉàÊÑü„ÇíË©ï‰æ°ÔºàÊúÄ„ÇÇ‰∏ÄËà¨ÁöÑÔºâ")
print("‚úì RMSE: Â§ß„Åç„Å™Ë™§Â∑Æ„ÇíÈáçË¶ñ„Åô„ÇãÂ†¥ÂêàÔºàÂìÅË≥™ÁÆ°ÁêÜÔºâ")
print("‚úì MAE: Â§ñ„ÇåÂÄ§„ÅÆÂΩ±Èüø„ÇíÊäë„Åà„Åü„ÅÑÂ†¥Âêà")
print("‚úì MAPE: Áï∞„Å™„Çã„Çπ„Ç±„Éº„É´„ÅÆ„Éá„Éº„Çø„ÇíÊØîËºÉ„Åô„ÇãÂ†¥Âêà")
</code></pre>

<p><strong>Ëß£Ë™¨</strong>: Ë§áÊï∞„ÅÆË©ï‰æ°ÊåáÊ®ô„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„Çã„Åì„Å®„Åß„ÄÅ„É¢„Éá„É´„ÅÆÊÄßËÉΩ„ÇíÂ§öËßíÁöÑ„Å´ÁêÜËß£„Åß„Åç„Åæ„Åô„ÄÇÂçò‰∏Ä„ÅÆÊåáÊ®ô„Å†„Åë„Å´È†º„Çâ„Åö„ÄÅÁõÆÁöÑ„Å´Âøú„Åò„Å¶ÈÅ©Âàá„Å™ÊåáÊ®ô„ÇíÈÅ∏Êäû„Åó„Åæ„Åô„ÄÇ</p>

<h4>„Ç≥„Éº„Éâ‰æã10: „ÇØ„É≠„Çπ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥„Å´„Çà„ÇãÊÄßËÉΩË©ï‰æ°</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score, KFold, learning_curve
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler

# Ââç„ÅÆ„Ç≥„Éº„Éâ‰æã„ÅÆ„Éá„Éº„Çø„Çí‰ΩøÁî®
# X, y „ÅØ„Åô„Åß„Å´ÂÆöÁæ©Ê∏à„Åø

# K-Fold „ÇØ„É≠„Çπ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

model_lr = LinearRegression()
model_rf = RandomForestRegressor(n_estimators=100, random_state=42)

# „ÇØ„É≠„Çπ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥„Çπ„Ç≥„Ç¢
cv_scores_lr = cross_val_score(model_lr, X, y, cv=kfold, scoring='r2')
cv_scores_rf = cross_val_score(model_rf, X, y, cv=kfold, scoring='r2')

print("„Äê„ÇØ„É≠„Çπ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥ÁµêÊûúÔºàR¬≤„Çπ„Ç≥„Ç¢Ôºâ„Äë")
print(f"\nLinear Regression:")
print(f"  ÂêÑFold: {cv_scores_lr}")
print(f"  Âπ≥Âùá: {cv_scores_lr.mean():.4f} (¬±{cv_scores_lr.std():.4f})")

print(f"\nRandom Forest:")
print(f"  ÂêÑFold: {cv_scores_rf}")
print(f"  Âπ≥Âùá: {cv_scores_rf.mean():.4f} (¬±{cv_scores_rf.std():.4f})")

# Â≠¶ÁøíÊõ≤Á∑öÔºàLearning CurveÔºâ„ÅÆË®àÁÆó
train_sizes, train_scores_lr, val_scores_lr = learning_curve(
    model_lr, X, y, cv=5, scoring='r2',
    train_sizes=np.linspace(0.1, 1.0, 10), random_state=42
)

train_sizes, train_scores_rf, val_scores_rf = learning_curve(
    model_rf, X, y, cv=5, scoring='r2',
    train_sizes=np.linspace(0.1, 1.0, 10), random_state=42
)

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# „ÇØ„É≠„Çπ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥„Çπ„Ç≥„Ç¢„ÅÆÊØîËºÉ
axes[0].boxplot([cv_scores_lr, cv_scores_rf], labels=['Linear Regression', 'Random Forest'],
                patch_artist=True,
                boxprops=dict(facecolor='#11998e', alpha=0.7))
axes[0].set_ylabel('R¬≤ Score', fontsize=11)
axes[0].set_title('Cross-Validation Performance (5-Fold)', fontsize=12, fontweight='bold')
axes[0].grid(alpha=0.3, axis='y')

# Â≠¶ÁøíÊõ≤Á∑ö
train_mean_lr = train_scores_lr.mean(axis=1)
train_std_lr = train_scores_lr.std(axis=1)
val_mean_lr = val_scores_lr.mean(axis=1)
val_std_lr = val_scores_lr.std(axis=1)

axes[1].plot(train_sizes, train_mean_lr, 'o-', color='#11998e', linewidth=2,
             label='Training score (LR)')
axes[1].fill_between(train_sizes, train_mean_lr - train_std_lr,
                      train_mean_lr + train_std_lr, alpha=0.2, color='#11998e')
axes[1].plot(train_sizes, val_mean_lr, 's-', color='#f59e0b', linewidth=2,
             label='Validation score (LR)')
axes[1].fill_between(train_sizes, val_mean_lr - val_std_lr,
                      val_mean_lr + val_std_lr, alpha=0.2, color='#f59e0b')
axes[1].set_xlabel('Training Set Size', fontsize=11)
axes[1].set_ylabel('R¬≤ Score', fontsize=11)
axes[1].set_title('Learning Curve (Linear Regression)', fontsize=12, fontweight='bold')
axes[1].legend(loc='lower right')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("\n„Äê„ÇØ„É≠„Çπ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥„ÅÆÈáçË¶ÅÊÄß„Äë")
print("‚úì Âçò‰∏Ä„ÅÆË®ìÁ∑¥/„ÉÜ„Çπ„ÉàÂàÜÂâ≤„Åß„ÅØ„Å™„Åè„ÄÅË§áÊï∞„ÅÆÂàÜÂâ≤„ÅßË©ï‰æ°")
print("‚úì „Éá„Éº„Çø„ÅÆÂÅè„Çä„Å´„Çà„ÇãË©ï‰æ°„ÅÆÂÅè„Çä„ÇíËªΩÊ∏õ")
print("‚úì „É¢„Éá„É´„ÅÆÊ±éÂåñÊÄßËÉΩ„Çí„Çà„ÇäÊ≠£Á¢∫„Å´Êé®ÂÆö")
print("‚úì Ê®ôÊ∫ñÂÅèÂ∑Æ„Å´„Çà„Çä„ÄÅÊÄßËÉΩ„ÅÆÂÆâÂÆöÊÄß„ÇÇË©ï‰æ°ÂèØËÉΩ")

print("\n„ÄêÂ≠¶ÁøíÊõ≤Á∑ö„ÅÆËß£Èáà„Äë")
print("‚úì Ë®ìÁ∑¥„Çπ„Ç≥„Ç¢„Å®Ê§úË®º„Çπ„Ç≥„Ç¢„ÅÆÂ∑Æ„ÅåÂ∞è„Åï„ÅÑ ‚Üí ÈÅéÂ≠¶Áøí„Å™„Åó")
print("‚úì ‰∏°„Çπ„Ç≥„Ç¢„ÅåÈ´ò„ÅÑ ‚Üí „É¢„Éá„É´„ÅåÈÅ©Âàá")
print("‚úì Ë®ìÁ∑¥„Çπ„Ç≥„Ç¢„ÅØÈ´ò„ÅÑ„ÅåÊ§úË®º„Çπ„Ç≥„Ç¢„Åå‰Ωé„ÅÑ ‚Üí ÈÅéÂ≠¶Áøí„ÅÆÂÖÜÂÄô")
print("‚úì ‰∏°„Çπ„Ç≥„Ç¢„Åå‰Ωé„ÅÑ ‚Üí „É¢„Éá„É´„ÅåË§áÈõë„Åï‰∏çË∂≥ÔºàunderfittingÔºâ")
</code></pre>

<p><strong>Ëß£Ë™¨</strong>: „ÇØ„É≠„Çπ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥„ÅØ„ÄÅÈôê„Çâ„Çå„Åü„Éá„Éº„Çø„Åã„ÇâÊúÄÂ§ßÈôê„ÅÆÊÉÖÂ†±„ÇíÂºï„ÅçÂá∫„ÅôÊâãÊ≥ï„Åß„Åô„ÄÇ„Éó„É≠„Çª„Çπ„Éá„Éº„Çø„ÅØÂèñÂæó„Ç≥„Çπ„Éà„ÅåÈ´ò„ÅÑ„Åü„ÇÅ„ÄÅÂäπÁéáÁöÑ„Å™„Éê„É™„Éá„Éº„Ç∑„Éß„É≥„ÅåÈáçË¶Å„Åß„Åô„ÄÇ</p>

<hr />

<h2>3.5 ÈùûÁ∑öÂΩ¢„É¢„Éá„É´„Å∏„ÅÆÊã°Âºµ</h2>

<p>„Éó„É≠„Çª„Çπ„Å´„ÅØÈùûÁ∑öÂΩ¢ÊÄß„ÅåÂ≠òÂú®„Åó„Åæ„Åô„ÄÇÁ∑öÂΩ¢ÂõûÂ∏∞„Åß‰∏çÂçÅÂàÜ„Å™Â†¥Âêà„ÄÅ<strong>ÈùûÁ∑öÂΩ¢„É¢„Éá„É´</strong>„ÇíÊ§úË®é„Åó„Åæ„Åô„ÄÇ</p>

<h3>‰∏ªË¶Å„Å™ÈùûÁ∑öÂΩ¢„É¢„Éá„É´</h3>

<table>
<thead>
<tr>
<th>„É¢„Éá„É´</th>
<th>ÁâπÂæ¥</th>
<th>Èï∑ÊâÄ</th>
<th>Áü≠ÊâÄ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Â§öÈ†ÖÂºèÂõûÂ∏∞</strong></td>
<td>Á∑öÂΩ¢ÂõûÂ∏∞„ÅÆÊã°Âºµ</td>
<td>Ëß£ÈáàÊÄß„ÅåÈ´ò„ÅÑ</td>
<td>Ê¨°Êï∞„ÅÆÈÅ∏Êäû„ÅåÈõ£„Åó„ÅÑ</td>
</tr>
<tr>
<td><strong>Random Forest</strong></td>
<td>Ê±∫ÂÆöÊú®„ÅÆÈõÜÂêà</td>
<td>È´òÁ≤æÂ∫¶„ÄÅÂ§ñ„ÇåÂÄ§„Å´È†ëÂÅ•</td>
<td>„Éñ„É©„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ</td>
</tr>
<tr>
<td><strong>SVR</strong></td>
<td>„Çµ„Éù„Éº„Éà„Éô„ÇØ„Çø„ÉºÂõûÂ∏∞</td>
<td>ÁêÜË´ñÁöÑÂü∫Áõ§„ÅåÂº∑Âõ∫</td>
<td>„Éë„É©„É°„Éº„ÇøË™øÊï¥„ÅåÂøÖË¶Å</td>
</tr>
<tr>
<td><strong>NN/DNN</strong></td>
<td>„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ</td>
<td>Ë∂ÖÈ´òÊ¨°ÂÖÉ„Éá„Éº„Çø„Å´Âº∑„ÅÑ</td>
<td>Â§ßÈáè„Éá„Éº„Çø„ÅåÂøÖË¶Å</td>
</tr>
</tbody>
</table>

<h4>„Ç≥„Éº„Éâ‰æã11: Â§öÈ†ÖÂºèÂõûÂ∏∞„Å®Random Forest</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error

# „Çµ„É≥„Éó„É´„Éá„Éº„ÇøÁîüÊàê: ÈùûÁ∑öÂΩ¢Èñ¢‰øÇ
np.random.seed(42)
n = 200

temperature = np.random.uniform(160, 190, n)

# ÈùûÁ∑öÂΩ¢Èñ¢‰øÇ: ÊúÄÈÅ©Ê∏©Â∫¶175¬∞C„ÅßÂèéÁéáÊúÄÂ§ß
yield_pct = (
    -0.05 * (temperature - 175)**2 +  # ‰∫åÊ¨°„ÅÆÈñ¢‰øÇÔºàÂ±±ÂûãÔºâ
    90 +
    np.random.normal(0, 1.5, n)
)

df = pd.DataFrame({'temperature': temperature, 'yield': yield_pct})

X = df[['temperature']]
y = df['yield']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# „É¢„Éá„É´1: Á∑öÂΩ¢ÂõûÂ∏∞Ôºà„Éô„Éº„Çπ„É©„Ç§„É≥Ôºâ
model_linear = LinearRegression()
model_linear.fit(X_train, y_train)
y_pred_linear = model_linear.predict(X_test)
r2_linear = r2_score(y_test, y_pred_linear)

# „É¢„Éá„É´2: Â§öÈ†ÖÂºèÂõûÂ∏∞Ôºà2Ê¨°Ôºâ
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_train_poly = poly_features.fit_transform(X_train)
X_test_poly = poly_features.transform(X_test)

model_poly = LinearRegression()
model_poly.fit(X_train_poly, y_train)
y_pred_poly = model_poly.predict(X_test_poly)
r2_poly = r2_score(y_test, y_pred_poly)

# „É¢„Éá„É´3: Random Forest
model_rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
model_rf.fit(X_train, y_train)
y_pred_rf = model_rf.predict(X_test)
r2_rf = r2_score(y_test, y_pred_rf)

print("„Äê„É¢„Éá„É´ÊÄßËÉΩÊØîËºÉ„Äë")
print(f"Á∑öÂΩ¢ÂõûÂ∏∞:       R¬≤ = {r2_linear:.4f}, RMSE = {np.sqrt(mean_squared_error(y_test, y_pred_linear)):.4f}")
print(f"Â§öÈ†ÖÂºèÂõûÂ∏∞(2Ê¨°): R¬≤ = {r2_poly:.4f}, RMSE = {np.sqrt(mean_squared_error(y_test, y_pred_poly)):.4f}")
print(f"Random Forest:  R¬≤ = {r2_rf:.4f}, RMSE = {np.sqrt(mean_squared_error(y_test, y_pred_rf)):.4f}")

# ÂèØË¶ñÂåñÁî®„ÅÆ‰∫àÊ∏¨Êõ≤Á∑ö
X_plot = np.linspace(160, 190, 300).reshape(-1, 1)
y_plot_linear = model_linear.predict(X_plot)
y_plot_poly = model_poly.predict(poly_features.transform(X_plot))
y_plot_rf = model_rf.predict(X_plot)

# ÂèØË¶ñÂåñ
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Á∑öÂΩ¢ÂõûÂ∏∞
axes[0].scatter(X_train, y_train, alpha=0.5, s=30, color='gray', label='Training data')
axes[0].scatter(X_test, y_test, alpha=0.5, s=30, color='red', label='Test data')
axes[0].plot(X_plot, y_plot_linear, color='#11998e', linewidth=2.5, label='Linear fit')
axes[0].set_xlabel('Temperature (¬∞C)', fontsize=11)
axes[0].set_ylabel('Yield (%)', fontsize=11)
axes[0].set_title(f'Linear Regression (R¬≤={r2_linear:.3f})', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# Â§öÈ†ÖÂºèÂõûÂ∏∞
axes[1].scatter(X_train, y_train, alpha=0.5, s=30, color='gray', label='Training data')
axes[1].scatter(X_test, y_test, alpha=0.5, s=30, color='red', label='Test data')
axes[1].plot(X_plot, y_plot_poly, color='#f59e0b', linewidth=2.5, label='Polynomial fit (degree=2)')
axes[1].set_xlabel('Temperature (¬∞C)', fontsize=11)
axes[1].set_ylabel('Yield (%)', fontsize=11)
axes[1].set_title(f'Polynomial Regression (R¬≤={r2_poly:.3f})', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

# Random Forest
axes[2].scatter(X_train, y_train, alpha=0.5, s=30, color='gray', label='Training data')
axes[2].scatter(X_test, y_test, alpha=0.5, s=30, color='red', label='Test data')
axes[2].plot(X_plot, y_plot_rf, color='#7b2cbf', linewidth=2.5, label='Random Forest')
axes[2].set_xlabel('Temperature (¬∞C)', fontsize=11)
axes[2].set_ylabel('Yield (%)', fontsize=11)
axes[2].set_title(f'Random Forest (R¬≤={r2_rf:.3f})', fontsize=12, fontweight='bold')
axes[2].legend()
axes[2].grid(alpha=0.3)

plt.tight_layout()
plt.show()

# Â§öÈ†ÖÂºè„ÅÆ‰øÇÊï∞Ë°®Á§∫
print(f"\n„ÄêÂ§öÈ†ÖÂºèÂõûÂ∏∞„ÅÆÂºè„Äë")
print(f"y = {model_poly.intercept_:.4f} + {model_poly.coef_[0]:.4f}√óT + {model_poly.coef_[1]:.4f}√óT¬≤")

# ÊúÄÈÅ©Ê∏©Â∫¶„ÅÆÊé®ÂÆöÔºàÂ§öÈ†ÖÂºèÂõûÂ∏∞„Åã„ÇâÔºâ
optimal_temp = -model_poly.coef_[0] / (2 * model_poly.coef_[1])
optimal_yield = model_poly.predict(poly_features.transform([[optimal_temp]]))[0]
print(f"\nÊé®ÂÆöÊúÄÈÅ©Ê∏©Â∫¶: {optimal_temp:.2f}¬∞C")
print(f"Êé®ÂÆöÊúÄÂ§ßÂèéÁéá: {optimal_yield:.2f}%")
</code></pre>

<p><strong>Âá∫Âäõ‰æã</strong>:</p>
<pre><code>„Äê„É¢„Éá„É´ÊÄßËÉΩÊØîËºÉ„Äë
Á∑öÂΩ¢ÂõûÂ∏∞:       R¬≤ = 0.2345, RMSE = 3.4567
Â§öÈ†ÖÂºèÂõûÂ∏∞(2Ê¨°): R¬≤ = 0.9234, RMSE = 1.0987
Random Forest:  R¬≤ = 0.9156, RMSE = 1.1567

„ÄêÂ§öÈ†ÖÂºèÂõûÂ∏∞„ÅÆÂºè„Äë
y = -345.6789 + 5.6789√óT + -0.0162√óT¬≤

Êé®ÂÆöÊúÄÈÅ©Ê∏©Â∫¶: 175.12¬∞C
Êé®ÂÆöÊúÄÂ§ßÂèéÁéá: 89.87%
</code></pre>

<p><strong>Ëß£Ë™¨</strong>: ÈùûÁ∑öÂΩ¢Èñ¢‰øÇ„Åå„ÅÇ„ÇãÂ†¥Âêà„ÄÅÁ∑öÂΩ¢ÂõûÂ∏∞„Åß„ÅØÊÄßËÉΩ„Åå‰Ωé„Åè„Å™„Çä„Åæ„Åô„ÄÇÂ§öÈ†ÖÂºèÂõûÂ∏∞„ÅØËß£ÈáàÊÄß„Çí‰øù„Å°„Å§„Å§ÈùûÁ∑öÂΩ¢ÊÄß„Å´ÂØæÂøú„Åß„Åç„ÄÅÊúÄÈÅ©Êù°‰ª∂„ÅÆÊé®ÂÆö„Å´„ÇÇÊúâÁî®„Åß„Åô„ÄÇ</p>

<h4>„Ç≥„Éº„Éâ‰æã12: „Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÉÅ„É•„Éº„Éã„É≥„Ç∞ÔºàGridSearchCVÔºâ</h4>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import r2_score, mean_squared_error

# Ââç„ÅÆ„Ç≥„Éº„Éâ‰æã„ÅÆ„Éá„Éº„Çø„Çí‰ΩøÁî®
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# „Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅÆÊé¢Á¥¢ÁØÑÂõ≤
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# GridSearchCV„Å´„Çà„Çã„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÉÅ„É•„Éº„Éã„É≥„Ç∞
rf = RandomForestRegressor(random_state=42)
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,
                           cv=5, scoring='r2', n_jobs=-1, verbose=1)

print("„ÄêGridSearchCVÂÆüË°å‰∏≠...„Äë")
print(f"Êé¢Á¥¢„Åô„Çã„Éë„É©„É°„Éº„ÇøÁµÑ„ÅøÂêà„Çè„ÅõÊï∞: {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf'])}")

grid_search.fit(X_train, y_train)

# ÊúÄÈÅ©„Éë„É©„É°„Éº„Çø
print(f"\n„ÄêÊúÄÈÅ©„Éë„É©„É°„Éº„Çø„Äë")
print(grid_search.best_params_)
print(f"\nÊúÄÈÅ©CV„Çπ„Ç≥„Ç¢ÔºàR¬≤Ôºâ: {grid_search.best_score_:.4f}")

# ÊúÄÈÅ©„É¢„Éá„É´„Åß„ÉÜ„Çπ„Éà„Éá„Éº„Çø„ÇíË©ï‰æ°
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test)
test_r2 = r2_score(y_test, y_pred_best)
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_best))

print(f"\n„ÉÜ„Çπ„Éà„Éá„Éº„ÇøÊÄßËÉΩ:")
print(f"  R¬≤: {test_r2:.4f}")
print(f"  RMSE: {test_rmse:.4f}")

# ÁµêÊûú„ÅÆÂèØË¶ñÂåñÔºàtop 10„Éë„É©„É°„Éº„ÇøÁµÑ„ÅøÂêà„Çè„ÅõÔºâ
results = pd.DataFrame(grid_search.cv_results_)
results_sorted = results.sort_values('rank_test_score')

top_10 = results_sorted.head(10)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Top 10„Éë„É©„É°„Éº„Çø„ÅÆR¬≤„Çπ„Ç≥„Ç¢
axes[0].barh(range(10), top_10['mean_test_score'], color='#11998e', alpha=0.7)
axes[0].set_yticks(range(10))
axes[0].set_yticklabels([f"Rank {i+1}" for i in range(10)])
axes[0].set_xlabel('Mean CV R¬≤ Score', fontsize=11)
axes[0].set_title('Top 10 Parameter Combinations', fontsize=12, fontweight='bold')
axes[0].grid(alpha=0.3, axis='x')
axes[0].invert_yaxis()

# ‰∫àÊ∏¨ vs ÂÆüÊ∏¨ÔºàÊúÄÈÅ©„É¢„Éá„É´Ôºâ
axes[1].scatter(y_test, y_pred_best, alpha=0.6, s=50, color='#11998e')
axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
             'r--', linewidth=2, label='Perfect prediction')
axes[1].set_xlabel('Actual Yield (%)', fontsize=11)
axes[1].set_ylabel('Predicted Yield (%)', fontsize=11)
axes[1].set_title(f'Best Model Performance (R¬≤={test_r2:.3f})', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("\n„Äê„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÅÆÈáçË¶ÅÊÄß„Äë")
print("‚úì „Éá„Éï„Ç©„É´„ÉàÂÄ§„Åß„ÅØÊúÄÈÅ©ÊÄßËÉΩ„ÅåÂæó„Çâ„Çå„Å™„ÅÑ„Åì„Å®„ÅåÂ§ö„ÅÑ")
print("‚úì GridSearchCV„Åß‰ΩìÁ≥ªÁöÑ„Å´ÊúÄÈÅ©„Éë„É©„É°„Éº„Çø„ÇíÊé¢Á¥¢")
print("‚úì „ÇØ„É≠„Çπ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥„ÅßÈÅéÂ≠¶Áøí„ÇíÈò≤„Åé„Å§„Å§Êé¢Á¥¢")
print("‚úì Ë®àÁÆó„Ç≥„Çπ„Éà„ÅØÈ´ò„ÅÑ„Åå„ÄÅ„É¢„Éá„É´ÊÄßËÉΩ„ÅåÂ§ß„Åç„ÅèÂêë‰∏ä")
</code></pre>

<p><strong>Ëß£Ë™¨</strong>: „Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÅØ„ÄÅ„É¢„Éá„É´ÊÄßËÉΩ„ÇíÊúÄÂ§ßÂåñ„Åô„ÇãÈáçË¶Å„Å™„Çπ„ÉÜ„ÉÉ„Éó„Åß„Åô„ÄÇGridSearchCV„Çí‰Ωø„ÅÜ„Åì„Å®„Åß„ÄÅ‰ΩìÁ≥ªÁöÑ„Åã„Å§ÂäπÁéáÁöÑ„Å´ÊúÄÈÅ©„Éë„É©„É°„Éº„Çø„ÇíÁô∫Ë¶ã„Åß„Åç„Åæ„Åô„ÄÇ</p>

<hr />

<h2>3.6 Êú¨Á´†„ÅÆ„Åæ„Å®„ÇÅ</h2>

<h3>Â≠¶„Çì„Å†„Åì„Å®</h3>

<ol>
<li><strong>Á∑öÂΩ¢ÂõûÂ∏∞„ÅÆÂü∫Á§é</strong>
<ul>
<li>ÂçòÂõûÂ∏∞„ÉªÈáçÂõûÂ∏∞„Å´„Çà„Çã„Éó„É≠„Çª„Çπ„É¢„Éá„É´ÊßãÁØâ</li>
<li>ÊÆãÂ∑ÆÂàÜÊûê„Å´„Çà„Çã„É¢„Éá„É´Ë®∫Êñ≠</li>
<li>‰øÇÊï∞„ÅÆËß£Èáà„Å®Áâ©ÁêÜÁöÑÊÑèÂë≥„ÅÆÁêÜËß£</li>
</ul>
</li>
<li><strong>PLS„Å´„Çà„ÇãÂ§öÈáçÂÖ±Á∑öÊÄßÂØæÂá¶</strong>
<ul>
<li>VIF„Å´„Çà„ÇãÂ§öÈáçÂÖ±Á∑öÊÄßË®∫Êñ≠</li>
<li>PLS„ÅßÊΩúÂú®Â§âÊï∞„Å´ÂúßÁ∏Æ„Åó„ÄÅÂÆâÂÆö„Åó„Åü„É¢„Éá„É´ÊßãÁØâ</li>
<li>PCR„Å®„ÅÆÈÅï„ÅÑ„Å®„ÄÅPLS„ÅÆÂÑ™‰ΩçÊÄß</li>
</ul>
</li>
<li><strong>„ÇΩ„Éï„Éà„Çª„É≥„Çµ„Éº„ÅÆÂÆüË£Ö</strong>
<ul>
<li>„Ç™„Éï„É©„Ç§„É≥Ê∏¨ÂÆö„Çí„É™„Ç¢„É´„Çø„Ç§„É†‰∫àÊ∏¨„Å´ÁΩÆ„ÅçÊèõ„Åà</li>
<li>„Éó„É≠„Çª„Çπ„Éâ„É™„Éï„Éà„ÅÆÁõ£Ë¶ñ„Å®ÂÆöÊúüÁöÑ„Å™ÂÜçÂ≠¶Áøí</li>
<li>ÂÆüÁî®ÁöÑ„Å™ÈÅãÁî®„Éª‰øùÂÆà„ÅÆÈáçË¶ÅÊÄß</li>
</ul>
</li>
<li><strong>„É¢„Éá„É´Ë©ï‰æ°„ÅÆÂÆüË∑µ</strong>
<ul>
<li>R¬≤„ÄÅRMSE„ÄÅMAE„ÄÅMAPE„ÅÆ‰Ωø„ÅÑÂàÜ„Åë</li>
<li>„ÇØ„É≠„Çπ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥„Å´„Çà„ÇãÊ±éÂåñÊÄßËÉΩË©ï‰æ°</li>
<li>Â≠¶ÁøíÊõ≤Á∑ö„ÅßÈÅéÂ≠¶Áøí„ÇíË®∫Êñ≠</li>
</ul>
</li>
<li><strong>ÈùûÁ∑öÂΩ¢„É¢„Éá„É´„Å∏„ÅÆÊã°Âºµ</strong>
<ul>
<li>Â§öÈ†ÖÂºèÂõûÂ∏∞„ÄÅRandom Forest„ÄÅSVR„ÅÆÁâπÂæ¥</li>
<li>GridSearchCV„Å´„Çà„Çã„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøÊúÄÈÅ©Âåñ</li>
<li>Á∑öÂΩ¢„É¢„Éá„É´„Å®„ÅÆÊÄßËÉΩÊØîËºÉ</li>
</ul>
</li>
</ol>

<h3>ÈáçË¶Å„Å™„Éù„Ç§„É≥„Éà</h3>

<blockquote>
<p><strong>"All models are wrong, but some are useful."</strong> - George Box</p>
</blockquote>

<ul>
<li>ÂÆåÁíß„Å™„É¢„Éá„É´„ÅØÂ≠òÂú®„Åó„Å™„ÅÑ„ÄÇÁõÆÁöÑ„Å´Âøú„Åò„Å¶ÈÅ©Âàá„Å™„É¢„Éá„É´„ÇíÈÅ∏Êäû</li>
<li>„Ç∑„É≥„Éó„É´„Å™„É¢„Éá„É´„Åã„ÇâÂßã„ÇÅ„ÄÅÂøÖË¶Å„Å´Âøú„Åò„Å¶Ë§áÈõëÂåñ</li>
<li>Ëß£ÈáàÊÄß„Å®Á≤æÂ∫¶„ÅÆ„Éà„É¨„Éº„Éâ„Ç™„Éï„ÇíÁêÜËß£„Åô„Çã</li>
<li>„É¢„Éá„É´„ÅØ‰Ωú„Å£„Å¶ÁµÇ„Çè„Çä„Åß„ÅØ„Å™„Åè„ÄÅÁ∂ôÁ∂öÁöÑ„Å™‰øùÂÆà„ÅåÂøÖË¶Å</li>
</ul>

<h3>„É¢„Éá„É´ÈÅ∏Êäû„ÅÆÊåáÈáù</h3>

<ol>
<li><strong>Á∑öÂΩ¢Èñ¢‰øÇ„ÅåÂº∑„ÅÑ</strong> ‚Üí Á∑öÂΩ¢ÂõûÂ∏∞„ÄÅPLS</li>
<li><strong>Â§öÈáçÂÖ±Á∑öÊÄß„ÅÇ„Çä</strong> ‚Üí PLS„ÄÅRidge/LassoÂõûÂ∏∞</li>
<li><strong>ÈùûÁ∑öÂΩ¢Èñ¢‰øÇ„ÅÇ„Çä</strong> ‚Üí Â§öÈ†ÖÂºèÂõûÂ∏∞„ÄÅRandom Forest„ÄÅSVR</li>
<li><strong>Ëß£ÈáàÊÄß„ÅåÈáçË¶Å</strong> ‚Üí Á∑öÂΩ¢ÂõûÂ∏∞„ÄÅÂ§öÈ†ÖÂºèÂõûÂ∏∞</li>
<li><strong>Á≤æÂ∫¶„ÅåÊúÄÂÑ™ÂÖà</strong> ‚Üí Random Forest„ÄÅGradient Boosting„ÄÅDNN</li>
</ol>

<h3>Ê¨°„ÅÆÁ´†„Å∏</h3>

<p>Á¨¨4Á´†„Åß„ÅØ„ÄÅÂ≠¶„Çì„Å†ÊâãÊ≥ï„ÇíÁµ±Âêà„Åó„ÄÅ<strong>ÂÆü„Éó„É≠„Çª„Çπ„Éá„Éº„Çø„ÇíÁî®„ÅÑ„ÅüÂÆüË∑µÊºîÁøí</strong>„ÇíË°å„ÅÑ„Åæ„ÅôÔºö</p>
<ul>
<li>„Ç±„Éº„Çπ„Çπ„Çø„Éá„Ç£: ÂåñÂ≠¶„Éó„É©„É≥„ÉàÈÅãËª¢„Éá„Éº„ÇøËß£Êûê</li>
<li>ÂìÅË≥™‰∫àÊ∏¨„É¢„Éá„É´„ÅÆÊßãÁØâÔºàEDA ‚Üí „É¢„Éá„É™„É≥„Ç∞ ‚Üí Ë©ï‰æ°Ôºâ</li>
<li>„Éó„É≠„Çª„ÇπÊù°‰ª∂ÊúÄÈÅ©Âåñ„ÅÆÂü∫Á§é</li>
<li>ÂÆüË£Ö„Éó„É≠„Ç∏„Çß„ÇØ„ÉàÂÖ®‰Ωì„ÅÆ„ÉØ„Éº„ÇØ„Éï„É≠„Éº</li>
<li>„Åæ„Å®„ÇÅ„Å®Ê¨°„ÅÆ„Çπ„ÉÜ„ÉÉ„ÉóÔºà‰∏äÁ¥ö„Éà„Éî„ÉÉ„ÇØ„Å∏Ôºâ</li>
</ul>

<div class="navigation">
    <a href="chapter-2.html" class="nav-button">‚Üê Ââç„ÅÆÁ´†</a>
    <a href="chapter-4.html" class="nav-button">Ê¨°„ÅÆÁ´† ‚Üí</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: PI Knowledge Hub Content Team</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-25</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>&copy; 2025 PI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
