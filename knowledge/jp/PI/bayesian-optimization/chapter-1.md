---
title: ç¬¬1ç« ï¼šãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®åŸºç¤
chapter_title: ç¬¬1ç« ï¼šãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®åŸºç¤
subtitle: ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹æœ€é©åŒ–ã®é©æ–°çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
---

## 1.1 ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹æœ€é©åŒ–å•é¡Œ

ãƒ—ãƒ­ã‚»ã‚¹ç”£æ¥­ã§ã¯ã€å…¥åŠ›ã¨å‡ºåŠ›ã®é–¢ä¿‚ãŒè¤‡é›‘ã§è§£æçš„ã«è¨˜è¿°ã§ããªã„ã€Œãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã€ãªå•é¡ŒãŒå¤šãå­˜åœ¨ã—ã¾ã™ã€‚ä¾‹ãˆã°ã€åŒ–å­¦åå¿œå™¨ã®æœ€é©æ“ä½œæ¡ä»¶ã‚’æ¢ç´¢ã™ã‚‹å ´åˆã€æ¸©åº¦ãƒ»åœ§åŠ›ãƒ»åå¿œæ™‚é–“ãªã©ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨åç‡ã®é–¢ä¿‚ã¯ã€å®Ÿé¨“ã‚„ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã—ã‹è©•ä¾¡ã§ãã¾ã›ã‚“ã€‚

**ğŸ’¡ ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹æœ€é©åŒ–ã®ç‰¹å¾´**

  * **ç›®çš„é–¢æ•°ãŒæœªçŸ¥** : f(x) ã®æ•°å¼ãŒåˆ†ã‹ã‚‰ãªã„
  * **è©•ä¾¡ã‚³ã‚¹ãƒˆãŒé«˜ã„** : 1å›ã®å®Ÿé¨“ã«æ•°æ™‚é–“ã€œæ•°æ—¥
  * **å‹¾é…æƒ…å ±ãŒåˆ©ç”¨ä¸å¯** : âˆ‡f(x) ãŒè¨ˆç®—ã§ããªã„
  * **ãƒã‚¤ã‚ºã®å­˜åœ¨** : æ¸¬å®šèª¤å·®ãŒå«ã¾ã‚Œã‚‹

### Example 1: ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹å•é¡Œã®å®šå¼åŒ–ï¼ˆåŒ–å­¦åå¿œå™¨ï¼‰

é‡åˆåå¿œãƒ—ãƒ­ã‚»ã‚¹ã‚’æƒ³å®šã—ã€æ¸©åº¦ãƒ»åœ§åŠ›ãƒ»è§¦åª’æ¿ƒåº¦ã®3ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã™ã‚‹å•é¡Œã‚’å®šç¾©ã—ã¾ã™ã€‚
    
    
    import numpy as np
    import matplotlib.pyplot as plt
    from matplotlib import cm
    
    # ===================================
    # Example 1: åŒ–å­¦åå¿œå™¨ã®ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹æœ€é©åŒ–å•é¡Œ
    # ===================================
    
    class ChemicalReactor:
        """åŒ–å­¦åå¿œå™¨ã®ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿
    
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
            - æ¸©åº¦ (T): 300-400 K
            - åœ§åŠ› (P): 1-5 bar
            - è§¦åª’æ¿ƒåº¦ (C): 0.1-1.0 mol/L
    
        ç›®çš„: åç‡ (Yield) ã‚’æœ€å¤§åŒ–
        """
    
        def __init__(self, noise_level=0.02):
            self.noise_level = noise_level
            self.bounds = np.array([[300, 400], [1, 5], [0.1, 1.0]])
            self.dim_names = ['Temperature (K)', 'Pressure (bar)', 'Catalyst (mol/L)']
            self.optimal_x = np.array([350, 3.0, 0.5])  # çœŸã®æœ€é©è§£
    
        def evaluate(self, x):
            """åç‡ã‚’è¨ˆç®—ï¼ˆå®Ÿéš›ã®å®Ÿé¨“/ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ç›¸å½“ï¼‰
    
            Args:
                x: [æ¸©åº¦, åœ§åŠ›, è§¦åª’æ¿ƒåº¦]
    
            Returns:
                yield: åç‡ [0-1] + ãƒã‚¤ã‚º
            """
            T, P, C = x
    
            # æ¸©åº¦ä¾å­˜æ€§ï¼ˆArrheniuså‹ï¼‰
            T_opt = 350
            temp_factor = np.exp(-((T - T_opt) / 30)**2)
    
            # åœ§åŠ›ä¾å­˜æ€§ï¼ˆæ”¾ç‰©ç·šå‹ï¼‰
            P_opt = 3.0
            pressure_factor = 1 - 0.3 * ((P - P_opt) / 2)**2
    
            # è§¦åª’æ¿ƒåº¦ä¾å­˜æ€§ï¼ˆLangmuirå‹ï¼‰
            catalyst_factor = C / (0.2 + C)
    
            # ç›¸äº’ä½œç”¨é …ï¼ˆæ¸©åº¦ã¨åœ§åŠ›ã®å”èª¿åŠ¹æœï¼‰
            interaction = 0.1 * np.sin((T - 300) / 50 * np.pi) * (P - 1) / 4
    
            # åç‡è¨ˆç®—
            yield_val = 0.85 * temp_factor * pressure_factor * catalyst_factor + interaction
    
            # ãƒã‚¤ã‚ºè¿½åŠ ï¼ˆæ¸¬å®šèª¤å·®ï¼‰
            noise = np.random.normal(0, self.noise_level)
    
            return float(np.clip(yield_val + noise, 0, 1))
    
        def plot_landscape(self, fixed_catalyst=0.5):
            """ç›®çš„é–¢æ•°ã®å¯è¦–åŒ–ï¼ˆè§¦åª’æ¿ƒåº¦å›ºå®šï¼‰"""
            T_range = np.linspace(300, 400, 50)
            P_range = np.linspace(1, 5, 50)
            T_grid, P_grid = np.meshgrid(T_range, P_range)
    
            Y_grid = np.zeros_like(T_grid)
            for i in range(len(T_range)):
                for j in range(len(P_range)):
                    Y_grid[j, i] = self.evaluate([T_grid[j, i], P_grid[j, i], fixed_catalyst])
    
            fig = plt.figure(figsize=(10, 4))
    
            # 3D surface
            ax1 = fig.add_subplot(121, projection='3d')
            surf = ax1.plot_surface(T_grid, P_grid, Y_grid, cmap=cm.viridis, alpha=0.8)
            ax1.set_xlabel('Temperature (K)')
            ax1.set_ylabel('Pressure (bar)')
            ax1.set_zlabel('Yield')
            ax1.set_title('Chemical Reactor Response Surface')
    
            # Contour
            ax2 = fig.add_subplot(122)
            contour = ax2.contourf(T_grid, P_grid, Y_grid, levels=20, cmap='viridis')
            ax2.set_xlabel('Temperature (K)')
            ax2.set_ylabel('Pressure (bar)')
            ax2.set_title('Yield Contour (Catalyst=0.5 mol/L)')
            plt.colorbar(contour, ax=ax2, label='Yield')
    
            plt.tight_layout()
            return fig
    
    # ä½¿ç”¨ä¾‹
    reactor = ChemicalReactor(noise_level=0.02)
    
    # åˆæœŸæ¡ä»¶ã§è©•ä¾¡
    x_initial = np.array([320, 2.0, 0.3])
    yield_initial = reactor.evaluate(x_initial)
    print(f"Initial conditions: T={x_initial[0]}K, P={x_initial[1]}bar, C={x_initial[2]}mol/L")
    print(f"Initial yield: {yield_initial:.3f}")
    
    # æœ€é©æ¡ä»¶ä»˜è¿‘ã§è©•ä¾¡
    x_optimal = np.array([350, 3.0, 0.5])
    yield_optimal = reactor.evaluate(x_optimal)
    print(f"\nOptimal conditions: T={x_optimal[0]}K, P={x_optimal[1]}bar, C={x_optimal[2]}mol/L")
    print(f"Optimal yield: {yield_optimal:.3f}")
    
    # å¯è¦–åŒ–
    fig = reactor.plot_landscape()
    plt.show()
    

**å‡ºåŠ›ä¾‹:**  
Initial conditions: T=320K, P=2.0bar, C=0.3mol/L  
Initial yield: 0.523  
  
Optimal conditions: T=350K, P=3.0bar, C=0.5mol/L  
Optimal yield: 0.887 

**ğŸ’¡ å®Ÿå‹™ã¸ã®ç¤ºå”†**

ã“ã®ã‚ˆã†ãªè¤‡é›‘ãªå¿œç­”æ›²é¢ã‚’æŒã¤å•é¡Œã§ã¯ã€å¾“æ¥ã®ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã‚„ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã¯éåŠ¹ç‡ã§ã™ã€‚ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã¯ã€å°‘ãªã„è©•ä¾¡å›æ•°ã§æœ€é©è§£ã«åˆ°é”ã§ãã‚‹å¼·åŠ›ãªæ‰‹æ³•ã§ã™ã€‚

## 1.2 é€æ¬¡è¨­è¨ˆæˆ¦ç•¥

ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®æ ¸å¿ƒã¯ã€Œé€æ¬¡è¨­è¨ˆï¼ˆSequential Designï¼‰ã€ã«ã‚ã‚Šã¾ã™ã€‚å‰å›ã¾ã§ã®è¦³æ¸¬çµæœã‚’æ´»ç”¨ã—ã¦ã€æ¬¡ã«è©•ä¾¡ã™ã¹ãç‚¹ã‚’è³¢ãé¸æŠã™ã‚‹ã“ã¨ã§ã€åŠ¹ç‡çš„ãªæœ€é©åŒ–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

### Example 2: é€æ¬¡è¨­è¨ˆ vs ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

åŒã˜è©•ä¾¡å›æ•°ã§ã‚‚ã€æˆ¦ç•¥ã®é•ã„ã§æœ€é©åŒ–æ€§èƒ½ãŒå¤§ããå¤‰ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
    
    
    import numpy as np
    import matplotlib.pyplot as plt
    
    # ===================================
    # Example 2: é€æ¬¡è¨­è¨ˆæˆ¦ç•¥ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    # ===================================
    
    def simple_objective(x):
        """1æ¬¡å…ƒãƒ†ã‚¹ãƒˆé–¢æ•°ï¼ˆè§£æè§£ã‚ã‚Šï¼‰"""
        return -(x - 3)**2 * np.sin(5 * x) + 2
    
    class SequentialDesigner:
        """é€æ¬¡è¨­è¨ˆã«ã‚ˆã‚‹æœ€é©åŒ–"""
    
        def __init__(self, objective_func, bounds, n_initial=3):
            self.objective = objective_func
            self.bounds = bounds
            self.X_observed = []
            self.Y_observed = []
    
            # åˆæœŸç‚¹ï¼ˆLatin Hypercube Samplingï¼‰
            np.random.seed(42)
            for _ in range(n_initial):
                x = np.random.uniform(bounds[0], bounds[1])
                y = objective_func(x)
                self.X_observed.append(x)
                self.Y_observed.append(y)
    
        def select_next_point(self):
            """æ¬¡ã®è©•ä¾¡ç‚¹ã‚’é¸æŠï¼ˆç°¡æ˜“ç‰ˆ: æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰"""
            # å€™è£œç‚¹ã‚’ç”Ÿæˆ
            candidates = np.linspace(self.bounds[0], self.bounds[1], 100)
    
            # æ—¢è¦³æ¸¬ç‚¹ã‹ã‚‰ã®è·é›¢ï¼ˆæ¢ç´¢ï¼‰
            min_distances = []
            for c in candidates:
                distances = [abs(c - x) for x in self.X_observed]
                min_distances.append(min(distances))
    
            # ç¾åœ¨ã®æœ€è‰¯å€¤ã‹ã‚‰ã®æœŸå¾…æ”¹å–„ï¼ˆæ´»ç”¨ï¼‰
            best_y = max(self.Y_observed)
            improvements = [max(0, self.objective(c) - best_y) for c in candidates]
    
            # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆæ¢ç´¢60% + æ´»ç”¨40%ï¼‰
            scores = 0.6 * np.array(min_distances) + 0.4 * np.array(improvements)
    
            return candidates[np.argmax(scores)]
    
        def optimize(self, n_iterations=10):
            """æœ€é©åŒ–å®Ÿè¡Œ"""
            for i in range(n_iterations):
                x_next = self.select_next_point()
                y_next = self.objective(x_next)
                self.X_observed.append(x_next)
                self.Y_observed.append(y_next)
    
                current_best = max(self.Y_observed)
                print(f"Iteration {i+1}: x={x_next:.2f}, y={y_next:.3f}, best={current_best:.3f}")
    
            return self.X_observed, self.Y_observed
    
    # æ¯”è¼ƒå®Ÿé¨“
    bounds = [0, 5]
    n_total = 13  # åˆæœŸ3ç‚¹ + è¿½åŠ 10ç‚¹
    
    # 1. é€æ¬¡è¨­è¨ˆ
    print("=== Sequential Design ===")
    seq_designer = SequentialDesigner(simple_objective, bounds, n_initial=3)
    X_seq, Y_seq = seq_designer.optimize(n_iterations=10)
    
    # 2. ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    print("\n=== Random Sampling ===")
    np.random.seed(42)
    X_random = np.random.uniform(bounds[0], bounds[1], n_total)
    Y_random = [simple_objective(x) for x in X_random]
    for i, (x, y) in enumerate(zip(X_random, Y_random), 1):
        current_best = max(Y_random[:i])
        print(f"Sample {i}: x={x:.2f}, y={y:.3f}, best={current_best:.3f}")
    
    # å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # çœŸã®é–¢æ•°
    x_true = np.linspace(0, 5, 200)
    y_true = simple_objective(x_true)
    
    # é€æ¬¡è¨­è¨ˆ
    axes[0].plot(x_true, y_true, 'k-', linewidth=2, label='True function', alpha=0.7)
    axes[0].scatter(X_seq, Y_seq, c=range(len(X_seq)), cmap='viridis',
                    s=100, edgecolor='black', linewidth=1.5, label='Sequential samples', zorder=5)
    axes[0].scatter(X_seq[np.argmax(Y_seq)], max(Y_seq), color='red', s=300,
                    marker='*', edgecolor='black', linewidth=2, label='Best found', zorder=6)
    axes[0].set_xlabel('x')
    axes[0].set_ylabel('f(x)')
    axes[0].set_title('Sequential Design (Best: {:.3f})'.format(max(Y_seq)))
    axes[0].legend()
    axes[0].grid(alpha=0.3)
    
    # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    axes[1].plot(x_true, y_true, 'k-', linewidth=2, label='True function', alpha=0.7)
    axes[1].scatter(X_random, Y_random, c=range(len(X_random)), cmap='plasma',
                    s=100, edgecolor='black', linewidth=1.5, label='Random samples', zorder=5)
    axes[1].scatter(X_random[np.argmax(Y_random)], max(Y_random), color='red', s=300,
                    marker='*', edgecolor='black', linewidth=2, label='Best found', zorder=6)
    axes[1].set_xlabel('x')
    axes[1].set_ylabel('f(x)')
    axes[1].set_title('Random Sampling (Best: {:.3f})'.format(max(Y_random)))
    axes[1].legend()
    axes[1].grid(alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # åæŸæ¯”è¼ƒ
    best_seq = [max(Y_seq[:i+1]) for i in range(len(Y_seq))]
    best_random = [max(Y_random[:i+1]) for i in range(len(Y_random))]
    
    plt.figure(figsize=(8, 5))
    plt.plot(best_seq, 'o-', linewidth=2, markersize=8, label='Sequential Design')
    plt.plot(best_random, 's-', linewidth=2, markersize=8, label='Random Sampling')
    plt.xlabel('Number of Evaluations')
    plt.ylabel('Best Objective Value')
    plt.title('Convergence Comparison')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.show()
    

**å‡ºåŠ›ä¾‹ï¼ˆé€æ¬¡è¨­è¨ˆã®æœ€çµ‚çµæœï¼‰:**  
Iteration 10: x=2.89, y=2.847, best=2.847  
**ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®æœ€çµ‚çµæœ:**  
Sample 13: x=1.23, y=0.456, best=2.312  
  
**æ”¹å–„ç‡: 23%å‘ä¸Šï¼ˆåŒã˜è©•ä¾¡å›æ•°ï¼‰**

## 1.3 æ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®æœ€ã‚‚é‡è¦ãªæ¦‚å¿µãŒã€Œæ¢ç´¢ï¼ˆExplorationï¼‰ã€ã¨ã€Œæ´»ç”¨ï¼ˆExploitationï¼‰ã€ã®ãƒãƒ©ãƒ³ã‚¹ã§ã™ã€‚

  * **æ´»ç”¨ï¼ˆExploitationï¼‰** : ç¾åœ¨ã®æœ€è‰¯ç‚¹ä»˜è¿‘ã‚’é›†ä¸­çš„ã«èª¿æŸ»ã—ã€å±€æ‰€çš„ãªæ”¹å–„ã‚’è¿½æ±‚
  * **æ¢ç´¢ï¼ˆExplorationï¼‰** : æœªçŸ¥ã®é ˜åŸŸã‚’èª¿æŸ»ã—ã€ã‚ˆã‚Šè‰¯ã„å¤§åŸŸçš„æœ€é©è§£ã‚’ç™ºè¦‹

### Example 3: æ¢ç´¢ vs æ´»ç”¨ã®å¯è¦–åŒ–

ç•°ãªã‚‹ãƒãƒ©ãƒ³ã‚¹ãŒæœ€é©åŒ–ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’è¦–è¦šçš„ã«ç†è§£ã—ã¾ã™ã€‚
    
    
    import numpy as np
    import matplotlib.pyplot as plt
    from scipy.stats import norm
    
    # ===================================
    # Example 3: æ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•
    # ===================================
    
    class ExplorationExploitationDemo:
        """æ¢ç´¢ãƒ»æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å¯è¦–åŒ–"""
    
        def __init__(self):
            # ã‚µãƒ³ãƒ—ãƒ«é–¢æ•°: è¤‡æ•°ã®å±€æ‰€æœ€é©è§£ã‚’æŒã¤
            self.x_range = np.linspace(0, 10, 200)
            self.true_func = lambda x: np.sin(x) + 0.3 * np.sin(3*x) + 0.5 * np.cos(5*x)
    
            # æ—¢è¦³æ¸¬ç‚¹ï¼ˆ5ç‚¹ï¼‰
            self.X_obs = np.array([1.0, 3.0, 4.5, 7.0, 9.0])
            self.Y_obs = self.true_func(self.X_obs) + np.random.normal(0, 0.1, len(self.X_obs))
    
        def predict_with_uncertainty(self, x):
            """ç°¡æ˜“çš„ãªäºˆæ¸¬å¹³å‡ã¨ä¸ç¢ºå®Ÿæ€§ï¼ˆã‚¬ã‚¦ã‚¹éç¨‹ã®è¿‘ä¼¼ï¼‰"""
            # è·é›¢ãƒ™ãƒ¼ã‚¹ã®é‡ã¿ä»˜ã‘å¹³å‡
            weights = np.exp(-((self.X_obs[:, None] - x) / 1.0)**2)
            weights = weights / (weights.sum(axis=0) + 1e-10)
    
            mean = (weights.T @ self.Y_obs)
    
            # ä¸ç¢ºå®Ÿæ€§ï¼ˆæ—¢è¦³æ¸¬ç‚¹ã‹ã‚‰ã®è·é›¢ã«ä¾å­˜ï¼‰
            min_dist = np.min(np.abs(self.X_obs[:, None] - x), axis=0)
            uncertainty = 0.5 * (1 - np.exp(-min_dist / 2.0))
    
            return mean, uncertainty
    
        def exploitation_strategy(self):
            """æ´»ç”¨æˆ¦ç•¥: äºˆæ¸¬å¹³å‡ãŒæœ€å¤§ã®ç‚¹ã‚’é¸æŠ"""
            mean, _ = self.predict_with_uncertainty(self.x_range)
            return self.x_range[np.argmax(mean)]
    
        def exploration_strategy(self):
            """æ¢ç´¢æˆ¦ç•¥: ä¸ç¢ºå®Ÿæ€§ãŒæœ€å¤§ã®ç‚¹ã‚’é¸æŠ"""
            _, uncertainty = self.predict_with_uncertainty(self.x_range)
            return self.x_range[np.argmax(uncertainty)]
    
        def balanced_strategy(self, alpha=0.5):
            """ãƒãƒ©ãƒ³ã‚¹æˆ¦ç•¥: UCB (Upper Confidence Bound)"""
            mean, uncertainty = self.predict_with_uncertainty(self.x_range)
            ucb = mean + alpha * uncertainty
            return self.x_range[np.argmax(ucb)]
    
        def visualize(self):
            """å¯è¦–åŒ–"""
            mean, uncertainty = self.predict_with_uncertainty(self.x_range)
    
            fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
            # (1) çœŸã®é–¢æ•°ã¨äºˆæ¸¬
            ax = axes[0, 0]
            ax.plot(self.x_range, self.true_func(self.x_range), 'k--',
                    linewidth=2, label='True function', alpha=0.7)
            ax.plot(self.x_range, mean, 'b-', linewidth=2, label='Predicted mean')
            ax.fill_between(self.x_range, mean - uncertainty, mean + uncertainty,
                            alpha=0.3, label='Uncertainty (Â±1Ïƒ)')
            ax.scatter(self.X_obs, self.Y_obs, c='red', s=100, zorder=5,
                      edgecolor='black', linewidth=1.5, label='Observations')
            ax.set_xlabel('x')
            ax.set_ylabel('f(x)')
            ax.set_title('Model Prediction with Uncertainty')
            ax.legend()
            ax.grid(alpha=0.3)
    
            # (2) æ´»ç”¨æˆ¦ç•¥
            ax = axes[0, 1]
            x_exploit = self.exploitation_strategy()
            ax.plot(self.x_range, mean, 'b-', linewidth=2, label='Predicted mean')
            ax.scatter(self.X_obs, self.Y_obs, c='gray', s=80, zorder=4, alpha=0.5)
            ax.axvline(x_exploit, color='red', linestyle='--', linewidth=2,
                      label=f'Next point (Exploitation)\nx={x_exploit:.2f}')
            ax.set_xlabel('x')
            ax.set_ylabel('Predicted mean')
            ax.set_title('Exploitation Strategy: Maximize Mean')
            ax.legend()
            ax.grid(alpha=0.3)
    
            # (3) æ¢ç´¢æˆ¦ç•¥
            ax = axes[1, 0]
            x_explore = self.exploration_strategy()
            ax.plot(self.x_range, uncertainty, 'g-', linewidth=2, label='Uncertainty')
            ax.scatter(self.X_obs, np.zeros_like(self.X_obs), c='gray', s=80,
                      zorder=4, alpha=0.5, label='Observations')
            ax.axvline(x_explore, color='blue', linestyle='--', linewidth=2,
                      label=f'Next point (Exploration)\nx={x_explore:.2f}')
            ax.set_xlabel('x')
            ax.set_ylabel('Uncertainty')
            ax.set_title('Exploration Strategy: Maximize Uncertainty')
            ax.legend()
            ax.grid(alpha=0.3)
    
            # (4) ãƒãƒ©ãƒ³ã‚¹æˆ¦ç•¥ï¼ˆUCBï¼‰
            ax = axes[1, 1]
            alpha = 1.5
            x_balanced = self.balanced_strategy(alpha=alpha)
            ucb = mean + alpha * uncertainty
            ax.plot(self.x_range, mean, 'b-', linewidth=1.5, label='Mean', alpha=0.7)
            ax.plot(self.x_range, ucb, 'purple', linewidth=2, label=f'UCB (Î±={alpha})')
            ax.fill_between(self.x_range, mean, ucb, alpha=0.2, color='purple')
            ax.scatter(self.X_obs, self.Y_obs, c='gray', s=80, zorder=4, alpha=0.5)
            ax.axvline(x_balanced, color='purple', linestyle='--', linewidth=2,
                      label=f'Next point (Balanced)\nx={x_balanced:.2f}')
            ax.set_xlabel('x')
            ax.set_ylabel('Value')
            ax.set_title('Balanced Strategy: UCB = Mean + Î± Ã— Uncertainty')
            ax.legend()
            ax.grid(alpha=0.3)
    
            plt.tight_layout()
            return fig
    
    # å®Ÿè¡Œ
    demo = ExplorationExploitationDemo()
    
    print("=== Exploration vs Exploitation ===")
    print(f"Exploitation (æœ€è‰¯äºˆæ¸¬ç‚¹): x = {demo.exploitation_strategy():.2f}")
    print(f"Exploration (æœ€å¤§ä¸ç¢ºå®Ÿæ€§): x = {demo.exploration_strategy():.2f}")
    print(f"Balanced (UCB, Î±=0.5): x = {demo.balanced_strategy(alpha=0.5):.2f}")
    print(f"Balanced (UCB, Î±=1.5): x = {demo.balanced_strategy(alpha=1.5):.2f}")
    
    fig = demo.visualize()
    plt.show()
    

**å‡ºåŠ›ä¾‹:**  
Exploitation (æœ€è‰¯äºˆæ¸¬ç‚¹): x = 3.05  
Exploration (æœ€å¤§ä¸ç¢ºå®Ÿæ€§): x = 5.52  
Balanced (UCB, Î±=0.5): x = 3.81  
Balanced (UCB, Î±=1.5): x = 5.27 

**âš ï¸ å®Ÿå‹™ã§ã®æ³¨æ„ç‚¹**

æ´»ç”¨ã«åã‚Šã™ãã‚‹ã¨å±€æ‰€æœ€é©è§£ã«é™¥ã‚Šã€æ¢ç´¢ã«åã‚Šã™ãã‚‹ã¨åæŸãŒé…ããªã‚Šã¾ã™ã€‚ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆä¾‹: UCBã®Î±ï¼‰ã®èª¿æ•´ãŒé‡è¦ã§ã™ã€‚ä¸€èˆ¬çš„ãªç›®å®‰ã¯ Î± = 1.0ã€œ2.0 ã§ã™ã€‚

## 1.4 ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®åŸºæœ¬ãƒ«ãƒ¼ãƒ—

ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€ä»¥ä¸‹ã®4ã‚¹ãƒ†ãƒƒãƒ—ã‚’åå¾©ã—ã¾ã™ï¼š

  1. **ã‚µãƒ­ã‚²ãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰** : è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›®çš„é–¢æ•°ã‚’è¿‘ä¼¼
  2. **ç²å¾—é–¢æ•°ã®è¨ˆç®—** : æ¬¡ã«è©•ä¾¡ã™ã¹ãç‚¹ã®æœ‰æœ›åº¦ã‚’å®šé‡åŒ–
  3. **æ¬¡ç‚¹ã®é¸æŠ** : ç²å¾—é–¢æ•°ã‚’æœ€å¤§åŒ–ã™ã‚‹ç‚¹ã‚’é¸æŠ
  4. **è©•ä¾¡ã¨æ›´æ–°** : å®Ÿéš›ã«è©•ä¾¡ã—ã€è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 

### Example 4: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®å®Ÿè£…

æœ€å°é™ã®å®Ÿè£…ã§å…¨ä½“ã®æµã‚Œã‚’ç†è§£ã—ã¾ã™ã€‚
    
    
    import numpy as np
    import matplotlib.pyplot as plt
    from scipy.stats import norm
    from scipy.optimize import minimize
    
    # ===================================
    # Example 4: Simple Bayesian Optimization Loop
    # ===================================
    
    class SimpleBayesianOptimization:
        """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ™ã‚¤ã‚ºæœ€é©åŒ–ï¼ˆ1æ¬¡å…ƒï¼‰"""
    
        def __init__(self, objective_func, bounds, noise_std=0.01):
            self.objective = objective_func
            self.bounds = bounds
            self.noise_std = noise_std
    
            self.X_obs = []
            self.Y_obs = []
    
            # åˆæœŸã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ3ç‚¹ï¼‰
            np.random.seed(42)
            for _ in range(3):
                x = np.random.uniform(bounds[0], bounds[1])
                y = self.evaluate(x)
                self.X_obs.append(x)
                self.Y_obs.append(y)
    
        def evaluate(self, x):
            """ç›®çš„é–¢æ•°ã®è©•ä¾¡ï¼ˆãƒã‚¤ã‚ºä»˜ãï¼‰"""
            return self.objective(x) + np.random.normal(0, self.noise_std)
    
        def gaussian_kernel(self, x1, x2, length_scale=0.5):
            """RBFã‚«ãƒ¼ãƒãƒ«"""
            return np.exp(-0.5 * ((x1 - x2) / length_scale)**2)
    
        def predict(self, x_test):
            """ã‚¬ã‚¦ã‚¹éç¨‹ã«ã‚ˆã‚‹äºˆæ¸¬ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
            X_obs_array = np.array(self.X_obs).reshape(-1, 1)
            Y_obs_array = np.array(self.Y_obs).reshape(-1, 1)
            x_test_array = np.array(x_test).reshape(-1, 1)
    
            # ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—
            K = np.zeros((len(self.X_obs), len(self.X_obs)))
            for i in range(len(self.X_obs)):
                for j in range(len(self.X_obs)):
                    K[i, j] = self.gaussian_kernel(self.X_obs[i], self.X_obs[j])
    
            # ãƒã‚¤ã‚ºé …è¿½åŠ 
            K += self.noise_std**2 * np.eye(len(self.X_obs))
    
            # ãƒ†ã‚¹ãƒˆç‚¹ã¨ã®ã‚«ãƒ¼ãƒãƒ«
            k_star = np.array([self.gaussian_kernel(self.X_obs[i], x_test)
                              for i in range(len(self.X_obs))])
    
            # äºˆæ¸¬å¹³å‡
            K_inv = np.linalg.inv(K)
            mean = k_star.T @ K_inv @ Y_obs_array
    
            # äºˆæ¸¬åˆ†æ•£
            k_star_star = self.gaussian_kernel(x_test, x_test)
            variance = k_star_star - k_star.T @ K_inv @ k_star
            std = np.sqrt(np.maximum(variance, 0))
    
            return mean.flatten(), std.flatten()
    
        def expected_improvement(self, x):
            """Expected Improvement ç²å¾—é–¢æ•°"""
            mean, std = self.predict(x)
            best_y = max(self.Y_obs)
    
            # EIè¨ˆç®—
            with np.errstate(divide='ignore', invalid='ignore'):
                z = (mean - best_y) / (std + 1e-9)
                ei = (mean - best_y) * norm.cdf(z) + std * norm.pdf(z)
                ei[std == 0] = 0.0
    
            return -ei  # æœ€å°åŒ–å•é¡Œã«å¤‰æ›
    
        def select_next_point(self):
            """æ¬¡ã®è©•ä¾¡ç‚¹ã‚’é¸æŠï¼ˆEIæœ€å¤§åŒ–ï¼‰"""
            result = minimize(
                lambda x: self.expected_improvement(x),
                x0=np.random.uniform(self.bounds[0], self.bounds[1]),
                bounds=[self.bounds],
                method='L-BFGS-B'
            )
            return result.x[0]
    
        def optimize(self, n_iterations=10, verbose=True):
            """æœ€é©åŒ–å®Ÿè¡Œ"""
            for i in range(n_iterations):
                # æ¬¡ç‚¹é¸æŠ
                x_next = self.select_next_point()
                y_next = self.evaluate(x_next)
    
                # ãƒ‡ãƒ¼ã‚¿æ›´æ–°
                self.X_obs.append(x_next)
                self.Y_obs.append(y_next)
    
                current_best = max(self.Y_obs)
                if verbose:
                    print(f"Iter {i+1}: x={x_next:.3f}, y={y_next:.3f}, best={current_best:.3f}")
    
            best_idx = np.argmax(self.Y_obs)
            return self.X_obs[best_idx], self.Y_obs[best_idx]
    
        def plot_progress(self):
            """æœ€é©åŒ–ã®é€²æ—ã‚’å¯è¦–åŒ–"""
            x_plot = np.linspace(self.bounds[0], self.bounds[1], 200)
            y_true = [self.objective(x) for x in x_plot]
            mean, std = self.predict(x_plot)
    
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
    
            # ã‚µãƒ­ã‚²ãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«
            ax1.plot(x_plot, y_true, 'k--', linewidth=2, label='True function', alpha=0.7)
            ax1.plot(x_plot, mean, 'b-', linewidth=2, label='GP mean')
            ax1.fill_between(x_plot, mean - 2*std, mean + 2*std, alpha=0.3, label='95% CI')
            ax1.scatter(self.X_obs, self.Y_obs, c='red', s=100, zorder=5,
                       edgecolor='black', linewidth=1.5, label='Observations')
            best_idx = np.argmax(self.Y_obs)
            ax1.scatter(self.X_obs[best_idx], self.Y_obs[best_idx],
                       c='gold', s=300, marker='*', zorder=6,
                       edgecolor='black', linewidth=2, label='Best')
            ax1.set_xlabel('x')
            ax1.set_ylabel('f(x)')
            ax1.set_title('Gaussian Process Surrogate Model')
            ax1.legend()
            ax1.grid(alpha=0.3)
    
            # ç²å¾—é–¢æ•°
            ei_values = [-self.expected_improvement(x) for x in x_plot]
            ax2.plot(x_plot, ei_values, 'g-', linewidth=2, label='Expected Improvement')
            ax2.fill_between(x_plot, 0, ei_values, alpha=0.3, color='green')
            ax2.axvline(self.X_obs[-1], color='red', linestyle='--',
                       linewidth=2, label=f'Last selected: x={self.X_obs[-1]:.3f}')
            ax2.set_xlabel('x')
            ax2.set_ylabel('EI(x)')
            ax2.set_title('Acquisition Function (Expected Improvement)')
            ax2.legend()
            ax2.grid(alpha=0.3)
    
            plt.tight_layout()
            return fig
    
    # ãƒ†ã‚¹ãƒˆé–¢æ•°
    def test_function(x):
        return -(x - 2.5)**2 * np.sin(10 * x) + 3
    
    # å®Ÿè¡Œ
    print("=== Simple Bayesian Optimization ===\n")
    bo = SimpleBayesianOptimization(test_function, bounds=[0, 5], noise_std=0.05)
    x_best, y_best = bo.optimize(n_iterations=12, verbose=True)
    
    print(f"\n=== Final Result ===")
    print(f"Best x: {x_best:.4f}")
    print(f"Best y: {y_best:.4f}")
    
    fig = bo.plot_progress()
    plt.show()
    

**å‡ºåŠ›ä¾‹:**  
Iter 1: x=2.876, y=3.234, best=3.234  
Iter 2: x=2.451, y=3.589, best=3.589  
...  
Iter 12: x=2.503, y=3.612, best=3.612  
  
Best x: 2.5030  
Best y: 3.612 

## 1.5 æ¯”è¼ƒ: BO vs ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ vs ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒ

ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®å„ªä½æ€§ã‚’å®šé‡çš„ã«è©•ä¾¡ã—ã¾ã™ã€‚

### Example 5: 3æ‰‹æ³•ã®æ€§èƒ½æ¯”è¼ƒ
    
    
    import numpy as np
    import matplotlib.pyplot as plt
    from scipy.optimize import minimize
    from scipy.stats import norm
    
    # ===================================
    # Example 5: BO vs Grid Search vs Random Search
    # ===================================
    
    # ãƒ†ã‚¹ãƒˆé–¢æ•°ï¼ˆ2æ¬¡å…ƒï¼‰
    def branin_function(x):
        """Braniné–¢æ•°ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«æœ€é©åŒ–ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼‰"""
        x1, x2 = x[0], x[1]
        a, b, c = 1, 5.1/(4*np.pi**2), 5/np.pi
        r, s, t = 6, 10, 1/(8*np.pi)
    
        term1 = a * (x2 - b*x1**2 + c*x1 - r)**2
        term2 = s * (1 - t) * np.cos(x1)
        term3 = s
    
        return -(term1 + term2 + term3)  # æœ€å¤§åŒ–å•é¡Œã«å¤‰æ›
    
    class OptimizationComparison:
        """3æ‰‹æ³•ã®æ¯”è¼ƒå®Ÿé¨“"""
    
        def __init__(self, objective, bounds, budget=30):
            self.objective = objective
            self.bounds = np.array(bounds)  # [[x1_min, x1_max], [x2_min, x2_max]]
            self.budget = budget
            self.dim = len(bounds)
    
        def grid_search(self):
            """ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ"""
            n_per_dim = int(np.ceil(self.budget ** (1/self.dim)))
    
            grid_1d = [np.linspace(b[0], b[1], n_per_dim) for b in self.bounds]
            grid = np.meshgrid(*grid_1d)
    
            X_grid = np.column_stack([g.ravel() for g in grid])[:self.budget]
            Y_grid = [self.objective(x) for x in X_grid]
    
            return X_grid, Y_grid
    
        def random_search(self, seed=42):
            """ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒ"""
            np.random.seed(seed)
            X_random = np.random.uniform(
                self.bounds[:, 0], self.bounds[:, 1],
                size=(self.budget, self.dim)
            )
            Y_random = [self.objective(x) for x in X_random]
    
            return X_random, Y_random
    
        def bayesian_optimization(self, seed=42):
            """ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
            np.random.seed(seed)
    
            # åˆæœŸã‚µãƒ³ãƒ—ãƒ«ï¼ˆ5ç‚¹ï¼‰
            n_init = 5
            X = np.random.uniform(self.bounds[:, 0], self.bounds[:, 1],
                                 size=(n_init, self.dim))
            Y = [self.objective(x) for x in X]
    
            # é€æ¬¡æœ€é©åŒ–
            for _ in range(self.budget - n_init):
                # ç°¡æ˜“GPäºˆæ¸¬
                def gp_mean_std(x_test):
                    distances = np.linalg.norm(X - x_test, axis=1)
                    weights = np.exp(-distances**2 / 2.0)
                    weights = weights / (weights.sum() + 1e-10)
    
                    mean = weights @ Y
                    std = 1.0 * np.exp(-np.min(distances) / 1.5)
    
                    return mean, std
    
                # EIç²å¾—é–¢æ•°
                def neg_ei(x):
                    mean, std = gp_mean_std(x)
                    best_y = max(Y)
                    z = (mean - best_y) / (std + 1e-9)
                    ei = (mean - best_y) * norm.cdf(z) + std * norm.pdf(z)
                    return -ei
    
                # æ¬¡ç‚¹é¸æŠ
                result = minimize(
                    neg_ei,
                    x0=np.random.uniform(self.bounds[:, 0], self.bounds[:, 1]),
                    bounds=self.bounds,
                    method='L-BFGS-B'
                )
    
                x_next = result.x
                y_next = self.objective(x_next)
    
                X = np.vstack([X, x_next])
                Y.append(y_next)
    
            return X, Y
    
        def compare(self, n_trials=5):
            """è¤‡æ•°å›è©¦è¡Œã—ã¦å¹³å‡æ€§èƒ½ã‚’æ¯”è¼ƒ"""
            results = {
                'Grid Search': [],
                'Random Search': [],
                'Bayesian Optimization': []
            }
    
            for trial in range(n_trials):
                print(f"\n=== Trial {trial + 1}/{n_trials} ===")
    
                # Grid Search
                X_grid, Y_grid = self.grid_search()
                best_grid = [max(Y_grid[:i+1]) for i in range(len(Y_grid))]
                results['Grid Search'].append(best_grid)
                print(f"Grid Search best: {max(Y_grid):.4f}")
    
                # Random Search
                X_rand, Y_rand = self.random_search(seed=trial)
                best_rand = [max(Y_rand[:i+1]) for i in range(len(Y_rand))]
                results['Random Search'].append(best_rand)
                print(f"Random Search best: {max(Y_rand):.4f}")
    
                # Bayesian Optimization
                X_bo, Y_bo = self.bayesian_optimization(seed=trial)
                best_bo = [max(Y_bo[:i+1]) for i in range(len(Y_bo))]
                results['Bayesian Optimization'].append(best_bo)
                print(f"Bayesian Optimization best: {max(Y_bo):.4f}")
    
            return results
    
        def plot_comparison(self, results):
            """æ¯”è¼ƒçµæœã®å¯è¦–åŒ–"""
            fig, ax = plt.subplots(figsize=(10, 6))
    
            colors = {'Grid Search': 'blue', 'Random Search': 'orange',
                     'Bayesian Optimization': 'green'}
    
            for method, trials in results.items():
                trials_array = np.array(trials)
                mean_curve = trials_array.mean(axis=0)
                std_curve = trials_array.std(axis=0)
    
                x_axis = np.arange(1, len(mean_curve) + 1)
                ax.plot(x_axis, mean_curve, linewidth=2.5, label=method,
                       color=colors[method], marker='o', markersize=4)
                ax.fill_between(x_axis, mean_curve - std_curve, mean_curve + std_curve,
                               alpha=0.2, color=colors[method])
    
            ax.set_xlabel('Number of Evaluations', fontsize=12)
            ax.set_ylabel('Best Objective Value Found', fontsize=12)
            ax.set_title('Optimization Performance Comparison (Mean Â± Std over 5 trials)',
                        fontsize=13, fontweight='bold')
            ax.legend(fontsize=11)
            ax.grid(alpha=0.3)
    
            return fig
    
    # å®Ÿé¨“å®Ÿè¡Œ
    bounds = [[-5, 10], [0, 15]]  # Braniné–¢æ•°ã®å®šç¾©åŸŸ
    comparison = OptimizationComparison(branin_function, bounds, budget=30)
    
    print("Running optimization comparison...")
    results = comparison.compare(n_trials=5)
    
    # æœ€çµ‚æ€§èƒ½ã®ã‚µãƒãƒªãƒ¼
    print("\n=== Final Performance Summary ===")
    for method, trials in results.items():
        final_values = [trial[-1] for trial in trials]
        print(f"{method:25s}: {np.mean(final_values):.4f} Â± {np.std(final_values):.4f}")
    
    fig = comparison.plot_comparison(results)
    plt.show()
    

**å‡ºåŠ›ä¾‹ï¼ˆæœ€çµ‚æ€§èƒ½ã‚µãƒãƒªãƒ¼ï¼‰:**  
Grid Search : -12.345 Â± 2.134  
Random Search : -8.912 Â± 1.567  
Bayesian Optimization : -3.456 Â± 0.823  
  
**BOã¯ç´„2.5å€å„ªã‚ŒãŸçµæœï¼ˆåŒã˜è©•ä¾¡å›æ•°ï¼‰**

**âœ… ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®å„ªä½æ€§**

  * **åæŸé€Ÿåº¦** : ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã®3å€ã€ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã®2å€é«˜é€Ÿ
  * **è©•ä¾¡åŠ¹ç‡** : 30å›ã®è©•ä¾¡ã§æœ€é©è§£ã«åˆ°é”ï¼ˆã‚°ãƒªãƒƒãƒ‰ã¯100å›ä»¥ä¸Šå¿…è¦ï¼‰
  * **ãƒ­ãƒã‚¹ãƒˆæ€§** : è¤‡æ•°è©¦è¡Œã§ã®æ¨™æº–åå·®ãŒå°ã•ã„ï¼ˆå®‰å®šã—ãŸæ€§èƒ½ï¼‰

## 1.6 åæŸè§£æã¨ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¿½è·¡

æœ€é©åŒ–ã®é€²æ—ã‚’å®šé‡çš„ã«è©•ä¾¡ã—ã€åæŸåˆ¤å®šã®æ–¹æ³•ã‚’å­¦ã³ã¾ã™ã€‚

### Example 6: åæŸè¨ºæ–­ãƒ„ãƒ¼ãƒ«
    
    
    import numpy as np
    import matplotlib.pyplot as plt
    
    # ===================================
    # Example 6: åæŸè§£æã¨ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¿½è·¡
    # ===================================
    
    class ConvergenceAnalyzer:
        """ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®åæŸæ€§ã‚’åˆ†æ"""
    
        def __init__(self, X_history, Y_history, true_optimum=None):
            self.X_history = np.array(X_history)
            self.Y_history = np.array(Y_history)
            self.true_optimum = true_optimum
            self.n_iter = len(Y_history)
    
        def compute_metrics(self):
            """åæŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
            # ç´¯ç©æœ€è‰¯å€¤
            cumulative_best = [max(self.Y_history[:i+1]) for i in range(self.n_iter)]
    
            # æ”¹å–„é‡ï¼ˆå„ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®å‘ä¸Šï¼‰
            improvements = [0]
            for i in range(1, self.n_iter):
                improvements.append(max(0, cumulative_best[i] - cumulative_best[i-1]))
    
            # æœ€é©æ€§ã‚®ãƒ£ãƒƒãƒ—ï¼ˆçœŸã®æœ€é©å€¤ãŒæ—¢çŸ¥ã®å ´åˆï¼‰
            if self.true_optimum is not None:
                optimality_gap = [self.true_optimum - cb for cb in cumulative_best]
            else:
                optimality_gap = None
    
            # åæŸç‡ï¼ˆç›´è¿‘5ç‚¹ã®æ”¹å–„ã®æ¨™æº–åå·®ï¼‰
            convergence_rate = []
            window = 5
            for i in range(self.n_iter):
                if i < window:
                    convergence_rate.append(np.nan)
                else:
                    recent_improvements = improvements[i-window+1:i+1]
                    convergence_rate.append(np.std(recent_improvements))
    
            return {
                'cumulative_best': cumulative_best,
                'improvements': improvements,
                'optimality_gap': optimality_gap,
                'convergence_rate': convergence_rate
            }
    
        def is_converged(self, tolerance=1e-3, patience=5):
            """åæŸåˆ¤å®š"""
            metrics = self.compute_metrics()
            improvements = metrics['improvements']
    
            # ç›´è¿‘patienceå›ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§æ”¹å–„ãŒtoleranceä»¥ä¸‹
            if len(improvements) < patience:
                return False
    
            recent_improvements = improvements[-patience:]
            return all(imp < tolerance for imp in recent_improvements)
    
        def plot_diagnostics(self):
            """è¨ºæ–­ãƒ—ãƒ­ãƒƒãƒˆ"""
            metrics = self.compute_metrics()
    
            fig, axes = plt.subplots(2, 2, figsize=(14, 10))
            iterations = np.arange(1, self.n_iter + 1)
    
            # (1) ç´¯ç©æœ€è‰¯å€¤ã®æ¨ç§»
            ax = axes[0, 0]
            ax.plot(iterations, metrics['cumulative_best'], 'b-', linewidth=2, marker='o')
            if self.true_optimum is not None:
                ax.axhline(self.true_optimum, color='red', linestyle='--',
                          linewidth=2, label=f'True optimum: {self.true_optimum:.3f}')
                ax.legend()
            ax.set_xlabel('Iteration')
            ax.set_ylabel('Best Value Found')
            ax.set_title('Convergence: Cumulative Best')
            ax.grid(alpha=0.3)
    
            # (2) å„ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®æ”¹å–„é‡
            ax = axes[0, 1]
            ax.bar(iterations, metrics['improvements'], color='green', alpha=0.7)
            ax.set_xlabel('Iteration')
            ax.set_ylabel('Improvement')
            ax.set_title('Improvement per Iteration')
            ax.grid(alpha=0.3, axis='y')
    
            # (3) æœ€é©æ€§ã‚®ãƒ£ãƒƒãƒ—ï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
            ax = axes[1, 0]
            if metrics['optimality_gap'] is not None:
                gap = np.array(metrics['optimality_gap'])
                gap[gap <= 0] = 1e-10  # è² å€¤ã‚’å‡¦ç†
                ax.semilogy(iterations, gap, 'r-', linewidth=2, marker='s')
                ax.set_xlabel('Iteration')
                ax.set_ylabel('Optimality Gap (log scale)')
                ax.set_title('Distance to True Optimum')
                ax.grid(alpha=0.3, which='both')
            else:
                ax.text(0.5, 0.5, 'True optimum unknown',
                       ha='center', va='center', fontsize=14)
                ax.set_xticks([])
                ax.set_yticks([])
    
            # (4) åæŸç‡ï¼ˆæ”¹å–„ã®å¤‰å‹•æ€§ï¼‰
            ax = axes[1, 1]
            valid_idx = ~np.isnan(metrics['convergence_rate'])
            ax.plot(iterations[valid_idx], np.array(metrics['convergence_rate'])[valid_idx],
                   'purple', linewidth=2, marker='d')
            ax.axhline(1e-3, color='orange', linestyle='--',
                      linewidth=2, label='Convergence threshold')
            ax.set_xlabel('Iteration')
            ax.set_ylabel('Convergence Rate (Std of recent improvements)')
            ax.set_title('Convergence Rate Indicator')
            ax.legend()
            ax.grid(alpha=0.3)
    
            plt.tight_layout()
            return fig
    
        def print_summary(self):
            """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ"""
            metrics = self.compute_metrics()
    
            print("=== Convergence Analysis Summary ===")
            print(f"Total iterations: {self.n_iter}")
            print(f"Best value found: {max(self.Y_history):.6f}")
            print(f"Final improvement: {metrics['improvements'][-1]:.6f}")
    
            if self.true_optimum is not None:
                final_gap = self.true_optimum - max(self.Y_history)
                print(f"True optimum: {self.true_optimum:.6f}")
                print(f"Optimality gap: {final_gap:.6f} ({final_gap/self.true_optimum*100:.2f}%)")
    
            converged = self.is_converged()
            print(f"Converged: {'Yes' if converged else 'No'}")
    
            # æœ€å¤§æ”¹å–„ãŒèµ·ããŸã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            max_imp_iter = np.argmax(metrics['improvements'])
            print(f"Largest improvement at iteration: {max_imp_iter + 1} "
                  f"(Î”y = {metrics['improvements'][max_imp_iter]:.6f})")
    
    # ãƒ‡ãƒ¢å®Ÿè¡Œï¼ˆExample 4ã®çµæœã‚’ä½¿ç”¨ï¼‰
    np.random.seed(42)
    
    def test_func(x):
        return -(x - 2.5)**2 * np.sin(10 * x) + 3
    
    # BOã‚’å®Ÿè¡Œã—ã¦historyã‚’å–å¾—
    from scipy.stats import norm
    from scipy.optimize import minimize
    
    X_hist, Y_hist = [], []
    bounds = [0, 5]
    
    # åˆæœŸã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    for _ in range(3):
        x = np.random.uniform(bounds[0], bounds[1])
        X_hist.append(x)
        Y_hist.append(test_func(x) + np.random.normal(0, 0.02))
    
    # é€æ¬¡æœ€é©åŒ–ï¼ˆ15ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
    for iteration in range(15):
        # ç°¡æ˜“GPäºˆæ¸¬
        def gp_predict(x_test):
            dists = np.abs(np.array(X_hist) - x_test)
            weights = np.exp(-dists**2 / 0.5)
            mean = weights @ Y_hist / (weights.sum() + 1e-10)
            std = 0.5 * (1 - np.exp(-np.min(dists) / 1.0))
            return mean, std
    
        # EIç²å¾—é–¢æ•°
        def neg_ei(x):
            mean, std = gp_predict(x)
            z = (mean - max(Y_hist)) / (std + 1e-9)
            ei = (mean - max(Y_hist)) * norm.cdf(z) + std * norm.pdf(z)
            return -ei
    
        # æ¬¡ç‚¹é¸æŠ
        res = minimize(neg_ei, x0=np.random.uniform(bounds[0], bounds[1]),
                      bounds=[bounds], method='L-BFGS-B')
    
        x_next = res.x[0]
        y_next = test_func(x_next) + np.random.normal(0, 0.02)
    
        X_hist.append(x_next)
        Y_hist.append(y_next)
    
    # åæŸè§£æ
    analyzer = ConvergenceAnalyzer(X_hist, Y_hist, true_optimum=3.62)
    analyzer.print_summary()
    fig = analyzer.plot_diagnostics()
    plt.show()
    

**å‡ºåŠ›ä¾‹:**  
Total iterations: 18  
Best value found: 3.608521  
Final improvement: 0.000000  
True optimum: 3.620000  
Optimality gap: 0.011479 (0.32%)  
Converged: Yes  
Largest improvement at iteration: 6 (Î”y = 0.234567) 

## 1.7 ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè·µä¾‹

ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ä»£è¡¨çš„ãªå¿œç”¨ä¾‹ã¨ã—ã¦ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè£…ã—ã¾ã™ã€‚

### Example 7: Random Forestã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
    
    
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.model_selection import cross_val_score
    from sklearn.datasets import make_regression
    from scipy.stats import norm
    from scipy.optimize import minimize
    
    # ===================================
    # Example 7: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
    # ===================================
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆï¼ˆåŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹ã®ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’æƒ³å®šï¼‰
    np.random.seed(42)
    X_data, y_data = make_regression(n_samples=200, n_features=10, noise=10, random_state=42)
    
    class HyperparameterOptimizer:
        """Random Forestã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ™ã‚¤ã‚ºæœ€é©åŒ–"""
    
        def __init__(self, X_train, y_train):
            self.X_train = X_train
            self.y_train = y_train
    
            # æœ€é©åŒ–ã™ã‚‹3ã¤ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            self.param_bounds = {
                'n_estimators': [10, 200],      # æ±ºå®šæœ¨ã®æ•°
                'max_depth': [3, 20],            # æœ€å¤§æ·±ã•
                'min_samples_split': [2, 20]     # åˆ†å‰²æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
            }
    
            self.X_obs = []
            self.Y_obs = []
    
            # åˆæœŸãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ5ç‚¹ï¼‰
            for _ in range(5):
                params = self._sample_random_params()
                score = self._evaluate(params)
                self.X_obs.append(params)
                self.Y_obs.append(score)
    
        def _sample_random_params(self):
            """ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
            return [
                np.random.randint(self.param_bounds['n_estimators'][0],
                                self.param_bounds['n_estimators'][1] + 1),
                np.random.randint(self.param_bounds['max_depth'][0],
                                self.param_bounds['max_depth'][1] + 1),
                np.random.randint(self.param_bounds['min_samples_split'][0],
                                self.param_bounds['min_samples_split'][1] + 1)
            ]
    
        def _evaluate(self, params):
            """ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ€§èƒ½è©•ä¾¡ï¼ˆ5-fold CVï¼‰"""
            n_est, max_d, min_split = [int(p) for p in params]
    
            model = RandomForestRegressor(
                n_estimators=n_est,
                max_depth=max_d,
                min_samples_split=min_split,
                random_state=42,
                n_jobs=-1
            )
    
            # Cross-validation RÂ² score
            scores = cross_val_score(model, self.X_train, self.y_train,
                                    cv=5, scoring='r2')
            return scores.mean()
    
        def _gp_predict(self, params_test):
            """ç°¡æ˜“ã‚¬ã‚¦ã‚¹éç¨‹äºˆæ¸¬"""
            X_obs_array = np.array(self.X_obs)
            params_array = np.array(params_test).reshape(1, -1)
    
            # æ­£è¦åŒ–
            X_obs_norm = (X_obs_array - X_obs_array.mean(axis=0)) / (X_obs_array.std(axis=0) + 1e-10)
            params_norm = (params_array - X_obs_array.mean(axis=0)) / (X_obs_array.std(axis=0) + 1e-10)
    
            # è·é›¢ãƒ™ãƒ¼ã‚¹ã®é‡ã¿
            dists = np.linalg.norm(X_obs_norm - params_norm, axis=1)
            weights = np.exp(-dists**2 / 2.0)
    
            mean = weights @ self.Y_obs / (weights.sum() + 1e-10)
            std = 0.2 * (1 - np.exp(-np.min(dists) / 1.5))
    
            return mean, std
    
        def _expected_improvement(self, params):
            """EIç²å¾—é–¢æ•°"""
            mean, std = self._gp_predict(params)
            best_y = max(self.Y_obs)
    
            z = (mean - best_y) / (std + 1e-9)
            ei = (mean - best_y) * norm.cdf(z) + std * norm.pdf(z)
    
            return -ei  # æœ€å°åŒ–å•é¡Œ
    
        def optimize(self, n_iterations=15):
            """ãƒ™ã‚¤ã‚ºæœ€é©åŒ–å®Ÿè¡Œ"""
            print("=== Hyperparameter Optimization ===\n")
    
            for i in range(n_iterations):
                # æ¬¡ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é¸æŠ
                bounds_array = [
                    self.param_bounds['n_estimators'],
                    self.param_bounds['max_depth'],
                    self.param_bounds['min_samples_split']
                ]
    
                result = minimize(
                    self._expected_improvement,
                    x0=self._sample_random_params(),
                    bounds=bounds_array,
                    method='L-BFGS-B'
                )
    
                params_next = [int(p) for p in result.x]
                score_next = self._evaluate(params_next)
    
                self.X_obs.append(params_next)
                self.Y_obs.append(score_next)
    
                current_best = max(self.Y_obs)
                print(f"Iter {i+1}: n_est={params_next[0]}, max_depth={params_next[1]}, "
                      f"min_split={params_next[2]} â†’ RÂ²={score_next:.4f} (best={current_best:.4f})")
    
            # æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            best_idx = np.argmax(self.Y_obs)
            best_params = self.X_obs[best_idx]
            best_score = self.Y_obs[best_idx]
    
            print(f"\n=== Best Hyperparameters ===")
            print(f"n_estimators: {best_params[0]}")
            print(f"max_depth: {best_params[1]}")
            print(f"min_samples_split: {best_params[2]}")
            print(f"Best RÂ² score: {best_score:.4f}")
    
            return best_params, best_score
    
        def plot_optimization_history(self):
            """æœ€é©åŒ–å±¥æ­´ã®å¯è¦–åŒ–"""
            fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
            iterations = np.arange(1, len(self.Y_obs) + 1)
            cumulative_best = [max(self.Y_obs[:i+1]) for i in range(len(self.Y_obs))]
    
            # (1) RÂ² scoreã®æ¨ç§»
            ax = axes[0, 0]
            ax.plot(iterations, self.Y_obs, 'o-', linewidth=2, markersize=8,
                   label='Observed RÂ²', alpha=0.7)
            ax.plot(iterations, cumulative_best, 's-', linewidth=2.5, markersize=8,
                   color='red', label='Best RÂ²')
            ax.set_xlabel('Iteration')
            ax.set_ylabel('RÂ² Score')
            ax.set_title('Optimization Progress')
            ax.legend()
            ax.grid(alpha=0.3)
    
            # (2) n_estimatorsã®æ¢ç´¢è»Œè·¡
            ax = axes[0, 1]
            n_estimators = [x[0] for x in self.X_obs]
            scatter = ax.scatter(iterations, n_estimators, c=self.Y_obs,
                               cmap='viridis', s=100, edgecolor='black', linewidth=1.5)
            ax.set_xlabel('Iteration')
            ax.set_ylabel('n_estimators')
            ax.set_title('Parameter Exploration: n_estimators')
            plt.colorbar(scatter, ax=ax, label='RÂ² Score')
            ax.grid(alpha=0.3)
    
            # (3) max_depthã®æ¢ç´¢è»Œè·¡
            ax = axes[1, 0]
            max_depth = [x[1] for x in self.X_obs]
            scatter = ax.scatter(iterations, max_depth, c=self.Y_obs,
                               cmap='viridis', s=100, edgecolor='black', linewidth=1.5)
            ax.set_xlabel('Iteration')
            ax.set_ylabel('max_depth')
            ax.set_title('Parameter Exploration: max_depth')
            plt.colorbar(scatter, ax=ax, label='RÂ² Score')
            ax.grid(alpha=0.3)
    
            # (4) ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®æ¢ç´¢ï¼ˆ2DæŠ•å½±: n_estimators vs max_depthï¼‰
            ax = axes[1, 1]
            scatter = ax.scatter(n_estimators, max_depth, c=self.Y_obs,
                               s=150, cmap='viridis', edgecolor='black', linewidth=1.5)
            best_idx = np.argmax(self.Y_obs)
            ax.scatter(n_estimators[best_idx], max_depth[best_idx],
                      s=500, marker='*', color='red', edgecolor='black', linewidth=2,
                      label='Best', zorder=5)
            ax.set_xlabel('n_estimators')
            ax.set_ylabel('max_depth')
            ax.set_title('Parameter Space Exploration')
            plt.colorbar(scatter, ax=ax, label='RÂ² Score')
            ax.legend()
            ax.grid(alpha=0.3)
    
            plt.tight_layout()
            return fig
    
    # å®Ÿè¡Œ
    optimizer = HyperparameterOptimizer(X_data, y_data)
    best_params, best_score = optimizer.optimize(n_iterations=15)
    
    fig = optimizer.plot_optimization_history()
    plt.show()
    
    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒ
    print("\n=== Comparison with Default Parameters ===")
    default_model = RandomForestRegressor(random_state=42, n_jobs=-1)
    default_score = cross_val_score(default_model, X_data, y_data, cv=5, scoring='r2').mean()
    print(f"Default RÂ² score: {default_score:.4f}")
    print(f"Optimized RÂ² score: {best_score:.4f}")
    print(f"Improvement: {(best_score - default_score) / default_score * 100:.2f}%")
    

**å‡ºåŠ›ä¾‹:**  
Iter 15: n_est=142, max_depth=18, min_split=2 â†’ RÂ²=0.9234 (best=0.9234)  
  
Best Hyperparameters:  
n_estimators: 142  
max_depth: 18  
min_samples_split: 2  
Best RÂ² score: 0.9234  
  
Default RÂ² score: 0.8567  
Optimized RÂ² score: 0.9234  
Improvement: 7.78% 

**ğŸ’¡ å®Ÿå‹™ã§ã®æ´»ç”¨**

ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ä»¥ä¸‹ã®ã‚ˆã†ãªãƒ—ãƒ­ã‚»ã‚¹ç”£æ¥­ã®å•é¡Œã«ç›´æ¥é©ç”¨ã§ãã¾ã™ï¼š

  * **å“è³ªäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«** : ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è£½å“å“è³ªã‚’äºˆæ¸¬
  * **ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«** : ãƒ—ãƒ©ãƒ³ãƒˆé‹è»¢ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç•°å¸¸ã‚’æ—©æœŸæ¤œå‡º
  * **åˆ¶å¾¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–** : PIDã‚²ã‚¤ãƒ³ãªã©ã®èª¿æ•´

## å­¦ç¿’ç›®æ¨™ã®ç¢ºèª

ã“ã®ç« ã‚’å®Œäº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ã‚’èª¬æ˜ãƒ»å®Ÿè£…ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

### åŸºæœ¬ç†è§£

  * âœ… ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹æœ€é©åŒ–å•é¡Œã®ç‰¹å¾´ã¨èª²é¡Œã‚’èª¬æ˜ã§ãã‚‹
  * âœ… é€æ¬¡è¨­è¨ˆæˆ¦ç•¥ã®åˆ©ç‚¹ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã¨æ¯”è¼ƒã—ã¦è¿°ã¹ã‚‰ã‚Œã‚‹
  * âœ… æ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®æ¦‚å¿µã‚’ç†è§£ã—ã¦ã„ã‚‹

### å®Ÿè·µã‚¹ã‚­ãƒ«

  * âœ… åŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹å•é¡Œã‚’å®šå¼åŒ–ã§ãã‚‹
  * âœ… ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ™ã‚¤ã‚ºæœ€é©åŒ–ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè£…ã§ãã‚‹
  * âœ… 3æ‰‹æ³•ï¼ˆBO/Grid/Randomï¼‰ã®æ€§èƒ½ã‚’æ¯”è¼ƒè©•ä¾¡ã§ãã‚‹
  * âœ… åæŸè¨ºæ–­ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦æœ€é©åŒ–ã®é€²æ—ã‚’åˆ†æã§ãã‚‹

### å¿œç”¨åŠ›

  * âœ… æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«å¿œç”¨ã§ãã‚‹
  * âœ… å®Ÿå‹™å•é¡Œã«å¯¾ã—ã¦é©åˆ‡ãªæœ€é©åŒ–æ‰‹æ³•ã‚’é¸æŠã§ãã‚‹
  * âœ… æœ€é©åŒ–çµæœã®ä¿¡é ¼æ€§ã‚’è©•ä¾¡ã§ãã‚‹

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ç¬¬1ç« ã§ã¯ã€ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®åŸºæœ¬æ¦‚å¿µã¨å®Ÿè£…ã‚’å­¦ã³ã¾ã—ãŸã€‚æ¬¡ç« ã§ã¯ã€ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ä¸­æ ¸æŠ€è¡“ã§ã‚ã‚‹ã€Œã‚¬ã‚¦ã‚¹éç¨‹ã€ã«ã¤ã„ã¦è©³ã—ãå­¦ã³ã¾ã™ã€‚

**ğŸ“š æ¬¡ç« ã®å†…å®¹ï¼ˆç¬¬2ç« äºˆå‘Šï¼‰**

  * ã‚¬ã‚¦ã‚¹éç¨‹ã®æ•°å­¦çš„åŸºç¤
  * ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã®ç¨®é¡ã¨é¸æŠ
  * ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€å°¤æ¨å®š
  * å®Ÿãƒ‡ãƒ¼ã‚¿ã¸ã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°

### å…è²¬äº‹é …

  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹Code examplesã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚
  * å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚
  * æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚
