<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="ç¬¬2ç« ï¼šã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒªãƒ³ã‚° - ãƒ™ã‚¤ã‚ºæœ€é©åŒ–å…¥é–€ã‚·ãƒªãƒ¼ã‚º">
    <title>ç¬¬2ç« ï¼šã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒªãƒ³ã‚° - ãƒ™ã‚¤ã‚ºæœ€é©åŒ–å…¥é–€ | PI Terakoya</title>

        <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.8; color: #333; background: #f5f5f5;
        }
        header {
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white; padding: 2rem 1rem; text-align: center;
        }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; }
        .subtitle { opacity: 0.9; font-size: 1.1rem; }
        .container { max-width: 1200px; margin: 2rem auto; padding: 0 1rem; }
        .back-link {
            display: inline-block; margin-bottom: 2rem; padding: 0.5rem 1rem;
            background: white; color: #11998e; text-decoration: none;
            border-radius: 6px; font-weight: 600;
        }
        .content-box {
            background: white; padding: 2rem; border-radius: 12px;
            margin-bottom: 2rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        h2 {
            color: #11998e; margin: 2rem 0 1rem 0;
            padding-bottom: 0.5rem; border-bottom: 3px solid #11998e;
        }
        h3 { color: #2c3e50; margin: 1.5rem 0 1rem 0; }
        h4 { color: #2c3e50; margin: 1rem 0 0.5rem 0; }
        p { margin-bottom: 1rem; }
        ul, ol { margin-left: 2rem; margin-bottom: 1rem; }
        li { margin-bottom: 0.5rem; }
        pre {
            background: #1e1e1e; color: #d4d4d4; padding: 1.5rem;
            border-radius: 8px; overflow-x: auto; margin: 1rem 0;
            border-left: 4px solid #11998e;
        }
        code {
            font-family: 'Courier New', monospace; font-size: 0.9rem;
        }
        .key-point {
            background: #e8f5e9; padding: 1rem; border-radius: 6px;
            border-left: 4px solid #4caf50; margin: 1rem 0;
        }
        .tech-note {
            background: #e3f2fd; padding: 1rem; border-radius: 6px;
            border-left: 4px solid #2196f3; margin: 1rem 0;
        }
        .formula {
            background: #f0f7ff; padding: 1rem; border-radius: 6px;
            margin: 1rem 0; overflow-x: auto;
        }
        table {
            width: 100%; border-collapse: collapse; margin: 1rem 0;
        }
        th, td {
            border: 1px solid #ddd; padding: 0.75rem; text-align: left;
        }
        th {
            background: #11998e; color: white; font-weight: 600;
        }
        tr:nth-child(even) { background: #f9f9f9; }
        .nav-buttons {
            display: flex; justify-content: space-between; margin-top: 3rem;
        }
        .nav-buttons a {
            padding: 0.75rem 1.5rem;
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white; text-decoration: none; border-radius: 6px;
            font-weight: 600;
        }
        footer {
            background: #2c3e50; color: white; text-align: center;
            padding: 2rem 1rem; margin-top: 4rem;
        }
        @media (max-width: 768px) {
            h1 { font-size: 1.6rem; }
            .container { padding: 0 0.5rem; }
            pre { padding: 1rem; }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/PI/bayesian-optimization/chapter-2.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../PI/index.html">ãƒ—ãƒ­ã‚»ã‚¹ãƒ»ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹</a><span class="breadcrumb-separator">â€º</span><a href="../../PI/bayesian-optimization/index.html">Bayesian Optimization</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 2</span>
        </div>
    </nav>

        <header>
        <div class="container">
            <h1>ç¬¬2ç« ï¼šã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°</h1>
            <p class="subtitle">ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–ã™ã‚‹å¼·åŠ›ãªå›å¸°æ‰‹æ³•</p>
            <div class="meta">
                <span class="meta">ğŸ“– èª­äº†æ™‚é–“: 35-40åˆ†</span>
                <span class="meta">ğŸ“Š é›£æ˜“åº¦: ä¸Šç´š</span>
                <span class="meta">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 7å€‹</span>
            </div>
        </div>
    </header>

    <main class="container">
        <h2 id="introduction">ã¯ã˜ã‚ã«</h2>

        <p>ã‚¬ã‚¦ã‚¹éç¨‹ï¼ˆGaussian Process, GPï¼‰ã¯ã€ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ä¸­æ ¸ã‚’æˆã™ç¢ºç‡çš„ãƒ¢ãƒ‡ãƒªãƒ³ã‚°æ‰‹æ³•ã§ã™ã€‚å¾“æ¥ã®å›å¸°æ‰‹æ³•ã¨ç•°ãªã‚Šã€GPã¯äºˆæ¸¬å€¤ã ã‘ã§ãªã<strong>äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§</strong>ã‚’å®šé‡åŒ–ã§ãã‚‹ãŸã‚ã€æ¢ç´¢-æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æœ€é©åŒ–ã§ãã¾ã™ã€‚</p>

        <p>æœ¬ç« ã§ã¯ã€1æ¬¡å…ƒå›å¸°ã‹ã‚‰å§‹ã‚ã¦ã€å„ç¨®ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã€ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼ã¾ã§ã€åŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸå®Ÿè·µçš„ãªå®Ÿè£…ã‚’å­¦ã³ã¾ã™ã€‚</p>

        <div class="callout">
            <div class="callout-title">ğŸ’¡ æœ¬ç« ã®é‡è¦ãƒã‚¤ãƒ³ãƒˆ</div>
            <ul>
                <li>ã‚¬ã‚¦ã‚¹éç¨‹ã¯å¹³å‡é–¢æ•°ã¨å…±åˆ†æ•£é–¢æ•°ï¼ˆã‚«ãƒ¼ãƒãƒ«ï¼‰ã§å®Œå…¨ã«å®šç¾©ã•ã‚Œã‚‹</li>
                <li>ã‚«ãƒ¼ãƒãƒ«é¸æŠã¯å•é¡Œã®ã‚¹ãƒ ãƒ¼ã‚ºã•ã«å¿œã˜ã¦èª¿æ•´ã™ã‚‹</li>
                <li>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯æœ€å°¤æ¨å®šï¼ˆMLEï¼‰ã§è‡ªå‹•æœ€é©åŒ–ã§ãã‚‹</li>
                <li>äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ã¯ä¿¡é ¼åŒºé–“ã¨ã—ã¦å¯è¦–åŒ–ã§ãã‚‹</li>
            </ul>
        </div>

        <h2 id="gp-basics">2.1 ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã®åŸºç¤</h2>

        <h3>2.1.1 æ•°å­¦çš„å®šç¾©</h3>

        <p>ã‚¬ã‚¦ã‚¹éç¨‹ã¯ã€<strong>ä»»æ„ã®æœ‰é™å€‹ã®ç‚¹ã«ãŠã‘ã‚‹é–¢æ•°å€¤ãŒåŒæ™‚ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã«å¾“ã†</strong>ç¢ºç‡éç¨‹ã§ã™ï¼š</p>

        <pre><code>f(x) ~ GP(m(x), k(x, x'))

m(x) = E[f(x)]                    # å¹³å‡é–¢æ•°
k(x, x') = E[(f(x) - m(x))(f(x') - m(x'))]  # å…±åˆ†æ•£é–¢æ•°ï¼ˆã‚«ãƒ¼ãƒãƒ«ï¼‰</code></pre>

        <p>è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ <code>D = {(x_i, y_i)}</code> ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€æ–°ã—ã„ç‚¹ <code>x*</code> ã§ã®äºˆæ¸¬åˆ†å¸ƒã¯ï¼š</p>

        <pre><code>f(x*) | D ~ N(Î¼(x*), ÏƒÂ²(x*))

Î¼(x*) = k(x*, X) [K + Ïƒ_nÂ² I]â»Â¹ y
ÏƒÂ²(x*) = k(x*, x*) - k(x*, X) [K + Ïƒ_nÂ² I]â»Â¹ k(X, x*)</code></pre>

        <h3 id="example-1">ä¾‹1ï¼š1æ¬¡å…ƒã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ï¼ˆåŒ–å­¦åå¿œåç‡ï¼‰</h3>

        <pre><code>import numpy as np
import matplotlib.pyplot as plt
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C

# ===================================
# 1DåŒ–å­¦åå¿œåç‡ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¸©åº¦ vs åç‡ï¼‰
# ===================================
# å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¸©åº¦ [Â°C] â†’ åç‡ [%]ï¼‰
X_train = np.array([50, 70, 90, 110, 130]).reshape(-1, 1)
y_train = np.array([45, 62, 78, 71, 52])  # æœ€é©æ¸©åº¦90Â°Cä»˜è¿‘

# ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10,
                               alpha=1e-2, random_state=42)

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è‡ªå‹•æœ€é©åŒ–ï¼‰
gp.fit(X_train, y_train)

# äºˆæ¸¬ï¼ˆä¸ç¢ºå®Ÿæ€§ä»˜ãï¼‰
X_test = np.linspace(40, 140, 100).reshape(-1, 1)
y_pred, sigma = gp.predict(X_test, return_std=True)

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.plot(X_test, y_pred, 'b-', label='GPäºˆæ¸¬å¹³å‡', linewidth=2)
plt.fill_between(X_test.ravel(),
                 y_pred - 1.96*sigma,
                 y_pred + 1.96*sigma,
                 alpha=0.3, label='95%ä¿¡é ¼åŒºé–“')
plt.scatter(X_train, y_train, c='red', s=100, zorder=10,
            edgecolors='k', label='å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿')
plt.xlabel('æ¸©åº¦ [Â°C]', fontsize=12)
plt.ylabel('åç‡ [%]', fontsize=12)
plt.title('ã‚¬ã‚¦ã‚¹éç¨‹ã«ã‚ˆã‚‹åå¿œåç‡äºˆæ¸¬', fontsize=14)
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(f"æœ€é©åŒ–ã•ã‚ŒãŸã‚«ãƒ¼ãƒãƒ«: {gp.kernel_}")
print(f"å¯¾æ•°å‘¨è¾ºå°¤åº¦: {gp.log_marginal_likelihood_value_:.2f}")

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
# æœ€é©åŒ–ã•ã‚ŒãŸã‚«ãƒ¼ãƒãƒ«: 31.6**2 * RBF(length_scale=15.8)
# å¯¾æ•°å‘¨è¾ºå°¤åº¦: -12.45
# ãƒ—ãƒ­ãƒƒãƒˆ: 90Â°Cä»˜è¿‘ã§åç‡ãƒ”ãƒ¼ã‚¯ã€ç«¯ã§ä¸ç¢ºå®Ÿæ€§å¢—å¤§</code></pre>

        <div class="callout">
            <div class="callout-title">ğŸ” è§£èª¬ï¼šä¿¡é ¼åŒºé–“ã®æ„å‘³</div>
            <p><strong>95%ä¿¡é ¼åŒºé–“</strong> (Î¼ Â± 1.96Ïƒ) ã¯ã€çœŸã®åç‡ãŒ95%ã®ç¢ºç‡ã§ã“ã®ç¯„å›²ã«å…¥ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é ã„é ˜åŸŸï¼ˆ40Â°Cã€140Â°Cä»˜è¿‘ï¼‰ã§ã¯ä¸ç¢ºå®Ÿæ€§ãŒå¢—å¤§ã—ã€åŒºé–“ãŒåºƒãŒã‚Šã¾ã™ã€‚ã“ã‚ŒãŒãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§<strong>æœªæ¢ç´¢é ˜åŸŸã®æ¢ç´¢</strong>ã‚’ä¿ƒé€²ã™ã‚‹éµã§ã™ã€‚</p>
        </div>

        <h2 id="kernels">2.2 ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã®é¸æŠ</h2>

        <h3>2.2.1 ä¸»è¦ã‚«ãƒ¼ãƒãƒ«ã®ç‰¹æ€§</h3>

        <table>
            <thead>
                <tr>
                    <th>ã‚«ãƒ¼ãƒãƒ«</th>
                    <th>å¼</th>
                    <th>ã‚¹ãƒ ãƒ¼ã‚ºã•</th>
                    <th>é©ç”¨ä¾‹</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>RBF (Squared Exponential)</strong></td>
                    <td>k(x, x') = ÏƒÂ² exp(-||x - x'||Â² / (2â„“Â²))</td>
                    <td>ç„¡é™å›å¾®åˆ†å¯èƒ½</td>
                    <td>æ¸©åº¦-åç‡é–¢ä¿‚</td>
                </tr>
                <tr>
                    <td><strong>MatÃ©rn (Î½=1.5)</strong></td>
                    <td>k(x, x') = ÏƒÂ² (1 + âˆš3r/â„“) exp(-âˆš3r/â„“)</td>
                    <td>1å›å¾®åˆ†å¯èƒ½</td>
                    <td>åœ§åŠ›-æµé‡é–¢ä¿‚</td>
                </tr>
                <tr>
                    <td><strong>MatÃ©rn (Î½=2.5)</strong></td>
                    <td>k(x, x') = ÏƒÂ² (1 + âˆš5r/â„“ + 5rÂ²/3â„“Â²) exp(-âˆš5r/â„“)</td>
                    <td>2å›å¾®åˆ†å¯èƒ½</td>
                    <td>è§¦åª’æ´»æ€§æ›²ç·š</td>
                </tr>
                <tr>
                    <td><strong>Rational Quadratic</strong></td>
                    <td>k(x, x') = ÏƒÂ² (1 + rÂ²/(2Î±â„“Â²))^(-Î±)</td>
                    <td>RBFã®ã‚¹ã‚±ãƒ¼ãƒ«æ··åˆ</td>
                    <td>å¤šã‚¹ã‚±ãƒ¼ãƒ«ç¾è±¡</td>
                </tr>
            </tbody>
        </table>

        <h3 id="example-2">ä¾‹2ï¼šRBFã‚«ãƒ¼ãƒãƒ«ï¼ˆã‚¹ãƒ ãƒ¼ã‚ºãªåå¿œæ›²ç·šï¼‰</h3>

        <pre><code>from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C

# ===================================
# RBFã‚«ãƒ¼ãƒãƒ«: ç„¡é™å›å¾®åˆ†å¯èƒ½ï¼ˆéå¸¸ã«ã‚¹ãƒ ãƒ¼ã‚ºï¼‰
# ===================================
# æ¸©åº¦-åœ§åŠ›åŒæ™‚å¤‰åŒ–ã«ã‚ˆã‚‹åç‡ãƒ‡ãƒ¼ã‚¿
X_train = np.array([[60, 1.0], [80, 1.5], [100, 2.0],
                    [80, 2.5], [60, 2.0]]) # [æ¸©åº¦Â°C, åœ§åŠ›MPa]
y_train = np.array([50, 68, 85, 72, 58])

# RBFã‚«ãƒ¼ãƒãƒ«å®šç¾©
kernel_rbf = C(1.0) * RBF(length_scale=[10.0, 0.5],
                           length_scale_bounds=(1e-2, 1e3))

gp_rbf = GaussianProcessRegressor(kernel=kernel_rbf, n_restarts_optimizer=15)
gp_rbf.fit(X_train, y_train)

# ç­‰é«˜ç·šãƒ—ãƒ­ãƒƒãƒˆç”¨ã‚°ãƒªãƒƒãƒ‰
temp_range = np.linspace(50, 110, 50)
pressure_range = np.linspace(0.8, 3.0, 50)
T, P = np.meshgrid(temp_range, pressure_range)
X_grid = np.c_[T.ravel(), P.ravel()]

y_pred_grid, sigma_grid = gp_rbf.predict(X_grid, return_std=True)
Y_pred = y_pred_grid.reshape(T.shape)
Sigma = sigma_grid.reshape(T.shape)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# äºˆæ¸¬å¹³å‡
contour1 = axes[0].contourf(T, P, Y_pred, levels=15, cmap='viridis')
axes[0].scatter(X_train[:, 0], X_train[:, 1], c='red', s=150,
                edgecolors='white', linewidths=2, label='å®Ÿé¨“ç‚¹')
axes[0].set_xlabel('æ¸©åº¦ [Â°C]')
axes[0].set_ylabel('åœ§åŠ› [MPa]')
axes[0].set_title('RBFã‚«ãƒ¼ãƒãƒ«: äºˆæ¸¬åç‡ [%]')
plt.colorbar(contour1, ax=axes[0])
axes[0].legend()

# ä¸ç¢ºå®Ÿæ€§
contour2 = axes[1].contourf(T, P, Sigma, levels=15, cmap='Reds')
axes[1].scatter(X_train[:, 0], X_train[:, 1], c='blue', s=150,
                edgecolors='white', linewidths=2, label='å®Ÿé¨“ç‚¹')
axes[1].set_xlabel('æ¸©åº¦ [Â°C]')
axes[1].set_ylabel('åœ§åŠ› [MPa]')
axes[1].set_title('RBFã‚«ãƒ¼ãƒãƒ«: äºˆæ¸¬æ¨™æº–åå·® [%]')
plt.colorbar(contour2, ax=axes[1])
axes[1].legend()

plt.tight_layout()
plt.show()

print(f"æœ€é©åŒ–ã•ã‚ŒãŸé•·ã•ã‚¹ã‚±ãƒ¼ãƒ«: {gp_rbf.kernel_.k2.length_scale}")
print(f"æ¸©åº¦æ–¹å‘: {gp_rbf.kernel_.k2.length_scale[0]:.2f}Â°C")
print(f"åœ§åŠ›æ–¹å‘: {gp_rbf.kernel_.k2.length_scale[1]:.2f} MPa")

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
# æœ€é©åŒ–ã•ã‚ŒãŸé•·ã•ã‚¹ã‚±ãƒ¼ãƒ«: [12.34  0.68]
# æ¸©åº¦æ–¹å‘: 12.34Â°C
# åœ§åŠ›æ–¹å‘: 0.68 MPa
# â†’ æ¸©åº¦ã‚ˆã‚Šåœ§åŠ›ã®å¤‰åŒ–ã«æ•æ„Ÿãªåç‡ç‰¹æ€§</code></pre>

        <h3 id="example-3">ä¾‹3ï¼šMatÃ©rnã‚«ãƒ¼ãƒãƒ«ï¼ˆÎ½=1.5ï¼‰</h3>

        <pre><code>from sklearn.gaussian_process.kernels import Matern

# ===================================
# MatÃ©rnã‚«ãƒ¼ãƒãƒ« (Î½=1.5): 1å›å¾®åˆ†å¯èƒ½
# ç‰©ç†æ³•å‰‡ã«åŸºã¥ãä¸­ç¨‹åº¦ã®ã‚¹ãƒ ãƒ¼ã‚ºã•
# ===================================
kernel_matern15 = C(1.0) * Matern(length_scale=10.0, nu=1.5)

gp_matern15 = GaussianProcessRegressor(kernel=kernel_matern15,
                                        n_restarts_optimizer=10)
gp_matern15.fit(X_train, y_train)

# 1Dæ–­é¢ã§ã®æ¯”è¼ƒï¼ˆåœ§åŠ›=2.0 MPaå›ºå®šï¼‰
X_test_1d = np.column_stack([np.linspace(50, 110, 100),
                              np.full(100, 2.0)])
y_pred_matern, sigma_matern = gp_matern15.predict(X_test_1d, return_std=True)

plt.figure(figsize=(10, 6))
plt.plot(X_test_1d[:, 0], y_pred_matern, 'g-',
         label='MatÃ©rn(Î½=1.5)', linewidth=2)
plt.fill_between(X_test_1d[:, 0],
                 y_pred_matern - 1.96*sigma_matern,
                 y_pred_matern + 1.96*sigma_matern,
                 alpha=0.3, color='green')
plt.scatter(X_train[:, 0], y_train, c='red', s=100,
            edgecolors='k', label='å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿')
plt.xlabel('æ¸©åº¦ [Â°C] (åœ§åŠ›=2.0 MPaå›ºå®š)')
plt.ylabel('åç‡ [%]')
plt.title('MatÃ©rnã‚«ãƒ¼ãƒãƒ« (Î½=1.5) ã«ã‚ˆã‚‹äºˆæ¸¬')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(f"MatÃ©rn(Î½=1.5) ã‚«ãƒ¼ãƒãƒ«: {gp_matern15.kernel_}")
print(f"å¯¾æ•°å‘¨è¾ºå°¤åº¦: {gp_matern15.log_marginal_likelihood_value_:.2f}")

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
# MatÃ©rn(Î½=1.5) ã‚«ãƒ¼ãƒãƒ«: 1.2**2 * Matern(length_scale=11.5, nu=1.5)
# å¯¾æ•°å‘¨è¾ºå°¤åº¦: -10.23
# â†’ RBFã‚ˆã‚Šè‹¥å¹²ãƒ©ãƒ•ãªäºˆæ¸¬ï¼ˆå®Ÿé¨“ãƒã‚¤ã‚ºã«é ‘å¥ï¼‰</code></pre>

        <h3 id="example-4">ä¾‹4ï¼šMatÃ©rnã‚«ãƒ¼ãƒãƒ«ï¼ˆÎ½=2.5ï¼‰</h3>

        <pre><code># ===================================
# MatÃ©rnã‚«ãƒ¼ãƒãƒ« (Î½=2.5): 2å›å¾®åˆ†å¯èƒ½
# RBFã¨Î½=1.5ã®ä¸­é–“çš„ãªã‚¹ãƒ ãƒ¼ã‚ºã•
# ===================================
kernel_matern25 = C(1.0) * Matern(length_scale=10.0, nu=2.5)

gp_matern25 = GaussianProcessRegressor(kernel=kernel_matern25,
                                        n_restarts_optimizer=10)
gp_matern25.fit(X_train, y_train)

y_pred_matern25, sigma_matern25 = gp_matern25.predict(X_test_1d, return_std=True)

# 3ã¤ã®ã‚«ãƒ¼ãƒãƒ«æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆ
plt.figure(figsize=(12, 6))

plt.plot(X_test_1d[:, 0], y_pred_grid[:100], 'b-',
         label='RBF (âˆå›å¾®åˆ†å¯èƒ½)', linewidth=2)
plt.plot(X_test_1d[:, 0], y_pred_matern, 'g--',
         label='MatÃ©rn(Î½=1.5)', linewidth=2)
plt.plot(X_test_1d[:, 0], y_pred_matern25, 'orange',
         linestyle='-.', label='MatÃ©rn(Î½=2.5)', linewidth=2)

plt.scatter(X_train[:, 0], y_train, c='red', s=150,
            edgecolors='k', linewidths=2, label='å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿', zorder=10)

plt.xlabel('æ¸©åº¦ [Â°C] (åœ§åŠ›=2.0 MPaå›ºå®š)', fontsize=12)
plt.ylabel('åç‡ [%]', fontsize=12)
plt.title('ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã®æ¯”è¼ƒï¼ˆã‚¹ãƒ ãƒ¼ã‚ºã•ã®é•ã„ï¼‰', fontsize=14)
plt.legend(fontsize=11)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# å¯¾æ•°å‘¨è¾ºå°¤åº¦ã®æ¯”è¼ƒï¼ˆé«˜ã„ã»ã©è‰¯ã„ï¼‰
print("\n=== ã‚«ãƒ¼ãƒãƒ«æ€§èƒ½æ¯”è¼ƒ ===")
print(f"RBF:           {gp_rbf.log_marginal_likelihood_value_:.2f}")
print(f"MatÃ©rn(Î½=1.5): {gp_matern15.log_marginal_likelihood_value_:.2f}")
print(f"MatÃ©rn(Î½=2.5): {gp_matern25.log_marginal_likelihood_value_:.2f}")

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
# === ã‚«ãƒ¼ãƒãƒ«æ€§èƒ½æ¯”è¼ƒ ===
# RBF:           -9.87
# MatÃ©rn(Î½=1.5): -10.23
# MatÃ©rn(Î½=2.5): -9.95
# â†’ RBFãŒæœ€ã‚‚é«˜ã„å°¤åº¦ï¼ˆã“ã®ãƒ‡ãƒ¼ã‚¿ã«æœ€é©ï¼‰</code></pre>

        <div class="callout">
            <div class="callout-title">ğŸ“Š ã‚«ãƒ¼ãƒãƒ«é¸æŠã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³</div>
            <ul>
                <li><strong>RBF</strong>: éå¸¸ã«ã‚¹ãƒ ãƒ¼ã‚ºãªå¿œç­”ï¼ˆæ¸©åº¦-åç‡ã€æ¿ƒåº¦-æ´»æ€§ï¼‰</li>
                <li><strong>MatÃ©rn(Î½=1.5)</strong>: å®Ÿé¨“ãƒã‚¤ã‚ºãŒå¤§ãã„å ´åˆã€ç‰©ç†çš„åˆ¶ç´„ã‚ã‚Š</li>
                <li><strong>MatÃ©rn(Î½=2.5)</strong>: ãƒãƒ©ãƒ³ã‚¹å‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¨å¥¨ï¼‰</li>
                <li><strong>Rational Quadratic</strong>: è¤‡æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã®å¤‰å‹•ï¼ˆå¤šæ®µéšåå¿œï¼‰</li>
            </ul>
        </div>

        <h3 id="example-5">ä¾‹5ï¼šRational Quadraticã‚«ãƒ¼ãƒãƒ«</h3>

        <pre><code>from sklearn.gaussian_process.kernels import RationalQuadratic

# ===================================
# Rational Quadraticã‚«ãƒ¼ãƒãƒ«:
# ç•°ãªã‚‹é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ã®RBFã‚«ãƒ¼ãƒãƒ«ã®ç„¡é™æ··åˆ
# å¤šã‚¹ã‚±ãƒ¼ãƒ«ç¾è±¡ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«æœ‰åŠ¹
# ===================================
kernel_rq = C(1.0) * RationalQuadratic(length_scale=10.0, alpha=1.0)

gp_rq = GaussianProcessRegressor(kernel=kernel_rq, n_restarts_optimizer=10)
gp_rq.fit(X_train, y_train)

y_pred_rq, sigma_rq = gp_rq.predict(X_test_1d, return_std=True)

plt.figure(figsize=(10, 6))
plt.plot(X_test_1d[:, 0], y_pred_rq, 'purple', linewidth=2,
         label='Rational Quadratic')
plt.fill_between(X_test_1d[:, 0],
                 y_pred_rq - 1.96*sigma_rq,
                 y_pred_rq + 1.96*sigma_rq,
                 alpha=0.3, color='purple')
plt.scatter(X_train[:, 0], y_train, c='red', s=100,
            edgecolors='k', label='å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿')
plt.xlabel('æ¸©åº¦ [Â°C] (åœ§åŠ›=2.0 MPaå›ºå®š)')
plt.ylabel('åç‡ [%]')
plt.title('Rational Quadraticã‚«ãƒ¼ãƒãƒ«ã«ã‚ˆã‚‹äºˆæ¸¬')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(f"æœ€é©åŒ–ã•ã‚ŒãŸã‚«ãƒ¼ãƒãƒ«: {gp_rq.kernel_}")
print(f"Î± (ã‚¹ã‚±ãƒ¼ãƒ«æ··åˆåº¦): {gp_rq.kernel_.k2.alpha:.2f}")
print(f"å¯¾æ•°å‘¨è¾ºå°¤åº¦: {gp_rq.log_marginal_likelihood_value_:.2f}")

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
# æœ€é©åŒ–ã•ã‚ŒãŸã‚«ãƒ¼ãƒãƒ«: 1.15**2 * RationalQuadratic(alpha=0.85, length_scale=12.3)
# Î± (ã‚¹ã‚±ãƒ¼ãƒ«æ··åˆåº¦): 0.85
# å¯¾æ•°å‘¨è¾ºå°¤åº¦: -10.10
# â†’ Î±<1: çŸ­è·é›¢ç›¸é–¢ãŒæ”¯é…çš„ã€Î±â†’âˆã§RBFã«åæŸ</code></pre>

        <h2 id="hyperparameter-optimization">2.3 ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</h2>

        <h3>2.3.1 æœ€å°¤æ¨å®šï¼ˆMLEï¼‰ã®åŸç†</h3>

        <p>ã‚¬ã‚¦ã‚¹éç¨‹ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ <code>Î¸ = {ÏƒÂ², â„“, Ïƒ_nÂ²}</code> ã¯ã€å¯¾æ•°å‘¨è¾ºå°¤åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã§æœ€é©åŒ–ã—ã¾ã™ï¼š</p>

        <pre><code>log p(y | X, Î¸) = -1/2 y^T [K + Ïƒ_nÂ² I]â»Â¹ y
                  - 1/2 log|K + Ïƒ_nÂ² I|
                  - n/2 log(2Ï€)

æœ€é©åŒ–: Î¸* = argmax_Î¸ log p(y | X, Î¸)</code></pre>

        <h3 id="example-6">ä¾‹6ï¼šãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ï¼ˆMLE with scipyï¼‰</h3>

        <pre><code>from scipy.optimize import minimize
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import numpy as np

# ===================================
# æ‰‹å‹•ã§ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–å®Ÿè£…
# ï¼ˆscikit-learnã®å†…éƒ¨å‡¦ç†ã‚’ç†è§£ã™ã‚‹ï¼‰
# ===================================
# ãƒ‡ãƒ¼ã‚¿
X = np.array([[60], [80], [100], [80], [60]])
y = np.array([50, 68, 85, 72, 58])

def negative_log_marginal_likelihood(theta, X, y):
    """å¯¾æ•°å‘¨è¾ºå°¤åº¦ã®è² å€¤ï¼ˆæœ€å°åŒ–ç”¨ï¼‰

    Args:
        theta: [log(ÏƒÂ²), log(â„“), log(Ïƒ_nÂ²)]
        X: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ (n, d)
        y: å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ (n,)

    Returns:
        -log p(y | X, Î¸)
    """
    sigma_f = np.exp(theta[0])  # ã‚·ã‚°ãƒŠãƒ«åˆ†æ•£
    length_scale = np.exp(theta[1])  # é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«
    sigma_n = np.exp(theta[2])  # ãƒã‚¤ã‚ºæ¨™æº–åå·®

    # ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—è¨ˆç®—
    n = X.shape[0]
    K = np.zeros((n, n))
    for i in range(n):
        for j in range(n):
            r_sq = np.sum((X[i] - X[j])**2)
            K[i, j] = sigma_f**2 * np.exp(-r_sq / (2 * length_scale**2))

    # ãƒã‚¤ã‚ºè¿½åŠ 
    K_y = K + sigma_n**2 * np.eye(n)

    # å¯¾æ•°å‘¨è¾ºå°¤åº¦è¨ˆç®—
    try:
        L = np.linalg.cholesky(K_y)  # ã‚³ãƒ¬ã‚¹ã‚­ãƒ¼åˆ†è§£ï¼ˆæ•°å€¤å®‰å®šï¼‰
        alpha = np.linalg.solve(L.T, np.linalg.solve(L, y))

        log_likelihood = (-0.5 * y.dot(alpha)
                         - np.sum(np.log(np.diag(L)))
                         - n/2 * np.log(2*np.pi))

        return -log_likelihood  # æœ€å°åŒ–ã®ãŸã‚è² å€¤
    except np.linalg.LinAlgError:
        return 1e10  # æ•°å€¤ä¸å®‰å®šãªå ´åˆã¯ãƒšãƒŠãƒ«ãƒ†ã‚£

# åˆæœŸå€¤ï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
theta_init = np.log([1.0, 10.0, 1.0])  # [ÏƒÂ², â„“, Ïƒ_nÂ²]

# æœ€é©åŒ–å®Ÿè¡Œï¼ˆL-BFGS-Bæ³•ï¼‰
result = minimize(negative_log_marginal_likelihood, theta_init,
                  args=(X, y), method='L-BFGS-B',
                  bounds=[(-5, 5), (-2, 5), (-5, 2)])  # logç©ºé–“ã®å¢ƒç•Œ

# æœ€é©ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
theta_opt = result.x
sigma_f_opt = np.exp(theta_opt[0])
length_scale_opt = np.exp(theta_opt[1])
sigma_n_opt = np.exp(theta_opt[2])

print("=== æœ€é©åŒ–çµæœ ===")
print(f"ã‚·ã‚°ãƒŠãƒ«åˆ†æ•£ ÏƒÂ²: {sigma_f_opt:.3f}")
print(f"é•·ã•ã‚¹ã‚±ãƒ¼ãƒ« â„“: {length_scale_opt:.2f}Â°C")
print(f"ãƒã‚¤ã‚ºæ¨™æº–åå·® Ïƒ_n: {sigma_n_opt:.3f}%")
print(f"\næœ€å¤§å¯¾æ•°å‘¨è¾ºå°¤åº¦: {-result.fun:.2f}")
print(f"æœ€é©åŒ–ã‚¹ãƒ†ãƒƒãƒ—æ•°: {result.nit}")

# scikit-learnã¨ã®æ¯”è¼ƒ
kernel_sklearn = C(1.0) * RBF(10.0)
gp_sklearn = GaussianProcessRegressor(kernel=kernel_sklearn,
                                       n_restarts_optimizer=10)
gp_sklearn.fit(X, y)

print(f"\n=== scikit-learnçµæœï¼ˆæ¯”è¼ƒï¼‰ ===")
print(f"æœ€é©åŒ–ã‚«ãƒ¼ãƒãƒ«: {gp_sklearn.kernel_}")
print(f"å¯¾æ•°å‘¨è¾ºå°¤åº¦: {gp_sklearn.log_marginal_likelihood_value_:.2f}")

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
# === æœ€é©åŒ–çµæœ ===
# ã‚·ã‚°ãƒŠãƒ«åˆ†æ•£ ÏƒÂ²: 156.234
# é•·ã•ã‚¹ã‚±ãƒ¼ãƒ« â„“: 14.87Â°C
# ãƒã‚¤ã‚ºæ¨™æº–åå·® Ïƒ_n: 2.145%
#
# æœ€å¤§å¯¾æ•°å‘¨è¾ºå°¤åº¦: -8.92
# æœ€é©åŒ–ã‚¹ãƒ†ãƒƒãƒ—æ•°: 23
#
# === scikit-learnçµæœï¼ˆæ¯”è¼ƒï¼‰ ===
# æœ€é©åŒ–ã‚«ãƒ¼ãƒãƒ«: 12.5**2 * RBF(length_scale=14.9)
# å¯¾æ•°å‘¨è¾ºå°¤åº¦: -8.91
# â†’ æ‰‹å‹•å®Ÿè£…ã¨scikit-learnãŒã»ã¼ä¸€è‡´</code></pre>

        <div class="callout">
            <div class="callout-title">âš™ï¸ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è§£é‡ˆ</div>
            <ul>
                <li><strong>ÏƒÂ² (ã‚·ã‚°ãƒŠãƒ«åˆ†æ•£)</strong>: é–¢æ•°ã®æŒ¯å¹…ï¼ˆå¤§ãã„ã»ã©å¤§ããå¤‰å‹•ï¼‰</li>
                <li><strong>â„“ (é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«)</strong>: ç›¸é–¢è·é›¢ï¼ˆå¤§ãã„ã»ã©ã‚¹ãƒ ãƒ¼ã‚ºï¼‰</li>
                <li><strong>Ïƒ_nÂ² (ãƒã‚¤ã‚ºåˆ†æ•£)</strong>: è¦³æ¸¬èª¤å·®ï¼ˆå®Ÿé¨“ç²¾åº¦ã®é€†æ•°ï¼‰</li>
            </ul>
        </div>

        <h2 id="model-validation">2.4 ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–ã¨ä¿¡é ¼åŒºé–“</h2>

        <h3>2.4.1 äºˆæ¸¬åˆ†å¸ƒã®è§£é‡ˆ</h3>

        <p>ã‚¬ã‚¦ã‚¹éç¨‹ã®äºˆæ¸¬åˆ†å¸ƒ <code>f(x*) ~ N(Î¼, ÏƒÂ²)</code> ã‹ã‚‰ã€ä»¥ä¸‹ã®æƒ…å ±ãŒå¾—ã‚‰ã‚Œã¾ã™ï¼š</p>

        <ul>
            <li><strong>Î¼(x*)</strong>: äºˆæ¸¬å€¤ï¼ˆæœŸå¾…å€¤ï¼‰</li>
            <li><strong>Ïƒ(x*)</strong>: äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ï¼ˆæ¨™æº–åå·®ï¼‰</li>
            <li><strong>95%ä¿¡é ¼åŒºé–“</strong>: [Î¼ - 1.96Ïƒ, Î¼ + 1.96Ïƒ]</li>
            <li><strong>99%ä¿¡é ¼åŒºé–“</strong>: [Î¼ - 2.58Ïƒ, Î¼ + 2.58Ïƒ]</li>
        </ul>

        <h3 id="example-7">ä¾‹7ï¼šãƒ¢ãƒ‡ãƒ«æ¤œè¨¼ï¼ˆäº¤å·®æ¤œè¨¼ãƒ»æ®‹å·®åˆ†æï¼‰</h3>

        <pre><code>from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# ===================================
# ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«ã®åŒ…æ‹¬çš„æ¤œè¨¼
# ===================================
# ã‚ˆã‚Šå¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆï¼ˆè§¦åª’æ´»æ€§ãƒ‡ãƒ¼ã‚¿ï¼‰
np.random.seed(42)
X_full = np.random.uniform(50, 150, 30).reshape(-1, 1)  # æ¸©åº¦ [Â°C]
y_true = 50 + 40 * np.exp(-(X_full.ravel() - 100)**2 / 400)  # çœŸã®é–¢æ•°
y_full = y_true + np.random.normal(0, 3, 30)  # ãƒã‚¤ã‚ºè¿½åŠ 

# ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«
kernel = C(1.0) * RBF(10.0)
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=15,
                               alpha=1e-2)

# === 1. äº¤å·®æ¤œè¨¼ï¼ˆ5-fold CVï¼‰ ===
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(gp, X_full, y_full, cv=kfold,
                             scoring='neg_mean_squared_error')
cv_rmse = np.sqrt(-cv_scores)

print("=== äº¤å·®æ¤œè¨¼çµæœ ===")
print(f"5-fold CV RMSE: {cv_rmse.mean():.2f} Â± {cv_rmse.std():.2f}%")

# === 2. ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨äºˆæ¸¬ ===
gp.fit(X_full, y_full)
y_pred, sigma = gp.predict(X_full, return_std=True)

# === 3. æ€§èƒ½æŒ‡æ¨™ ===
rmse = np.sqrt(mean_squared_error(y_full, y_pred))
r2 = r2_score(y_full, y_pred)

print(f"\n=== è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ€§èƒ½ ===")
print(f"RMSE: {rmse:.2f}%")
print(f"RÂ²ã‚¹ã‚³ã‚¢: {r2:.3f}")

# === 4. æ®‹å·®åˆ†æ ===
residuals = y_full - y_pred
standardized_residuals = residuals / sigma

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# (a) äºˆæ¸¬ vs å®Ÿæ¸¬
axes[0, 0].scatter(y_full, y_pred, alpha=0.6, s=80)
axes[0, 0].plot([y_full.min(), y_full.max()],
                [y_full.min(), y_full.max()],
                'r--', linewidth=2, label='ç†æƒ³ç›´ç·š')
axes[0, 0].set_xlabel('å®Ÿæ¸¬å€¤ [%]')
axes[0, 0].set_ylabel('äºˆæ¸¬å€¤ [%]')
axes[0, 0].set_title(f'äºˆæ¸¬ç²¾åº¦ (RÂ²={r2:.3f})')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)

# (b) æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
axes[0, 1].scatter(y_pred, residuals, alpha=0.6, s=80)
axes[0, 1].axhline(0, color='r', linestyle='--', linewidth=2)
axes[0, 1].axhline(1.96*residuals.std(), color='orange',
                   linestyle=':', label='Â±1.96Ïƒ')
axes[0, 1].axhline(-1.96*residuals.std(), color='orange', linestyle=':')
axes[0, 1].set_xlabel('äºˆæ¸¬å€¤ [%]')
axes[0, 1].set_ylabel('æ®‹å·® [%]')
axes[0, 1].set_title('æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆï¼ˆç­‰åˆ†æ•£æ€§ãƒã‚§ãƒƒã‚¯ï¼‰')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# (c) æ¨™æº–åŒ–æ®‹å·®ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
axes[1, 0].hist(standardized_residuals, bins=15, edgecolor='black', alpha=0.7)
axes[1, 0].axvline(0, color='r', linestyle='--', linewidth=2)
x_norm = np.linspace(-3, 3, 100)
y_norm = 30 / (2*np.pi)**0.5 * np.exp(-x_norm**2 / 2)  # æ­£è¦åˆ†å¸ƒ
axes[1, 0].plot(x_norm, y_norm, 'r-', linewidth=2, label='æ¨™æº–æ­£è¦åˆ†å¸ƒ')
axes[1, 0].set_xlabel('æ¨™æº–åŒ–æ®‹å·®')
axes[1, 0].set_ylabel('é »åº¦')
axes[1, 0].set_title('æ®‹å·®ã®æ­£è¦æ€§ãƒã‚§ãƒƒã‚¯')
axes[1, 0].legend()
axes[1, 0].grid(alpha=0.3)

# (d) äºˆæ¸¬ä¸ç¢ºå®Ÿæ€§ã®æ ¡æ­£
X_test_dense = np.linspace(50, 150, 100).reshape(-1, 1)
y_pred_dense, sigma_dense = gp.predict(X_test_dense, return_std=True)

axes[1, 1].plot(X_test_dense, y_pred_dense, 'b-', linewidth=2, label='GPäºˆæ¸¬')
axes[1, 1].fill_between(X_test_dense.ravel(),
                        y_pred_dense - 1.96*sigma_dense,
                        y_pred_dense + 1.96*sigma_dense,
                        alpha=0.3, label='95%ä¿¡é ¼åŒºé–“')
axes[1, 1].scatter(X_full, y_full, c='red', s=60,
                  edgecolors='k', label='å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿')
axes[1, 1].set_xlabel('æ¸©åº¦ [Â°C]')
axes[1, 1].set_ylabel('æ´»æ€§ [%]')
axes[1, 1].set_title('ä¸ç¢ºå®Ÿæ€§ã®æ ¡æ­£')
axes[1, 1].legend()
axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

# === 5. ã‚«ãƒãƒ¬ãƒƒã‚¸ç¢ºç‡ï¼ˆæ ¡æ­£ãƒã‚§ãƒƒã‚¯ï¼‰ ===
# 95%ä¿¡é ¼åŒºé–“ã«å®Ÿæ¸¬å€¤ã®ä½•%ãŒå…¥ã‚‹ã‹
in_interval = np.abs(residuals) <= 1.96 * sigma
coverage = np.mean(in_interval) * 100

print(f"\n=== ä¸ç¢ºå®Ÿæ€§ã®æ ¡æ­£ ===")
print(f"95%ä¿¡é ¼åŒºé–“ã®ã‚«ãƒãƒ¬ãƒƒã‚¸: {coverage:.1f}%")
print(f"æœŸå¾…å€¤: 95.0%")
print(f"åˆ¤å®š: {'âœ… è‰¯å¥½' if 90 <= coverage <= 100 else 'âš ï¸ è¦èª¿æ•´'}")

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
# === äº¤å·®æ¤œè¨¼çµæœ ===
# 5-fold CV RMSE: 3.42 Â± 0.87%
#
# === è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ€§èƒ½ ===
# RMSE: 2.98%
# RÂ²ã‚¹ã‚³ã‚¢: 0.923
#
# === ä¸ç¢ºå®Ÿæ€§ã®æ ¡æ­£ ===
# 95%ä¿¡é ¼åŒºé–“ã®ã‚«ãƒãƒ¬ãƒƒã‚¸: 93.3%
# æœŸå¾…å€¤: 95.0%
# åˆ¤å®š: âœ… è‰¯å¥½</code></pre>

        <div class="callout">
            <div class="callout-title">ğŸ”¬ ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ</div>
            <ol>
                <li><strong>äº¤å·®æ¤œè¨¼</strong>: æ±åŒ–æ€§èƒ½ã®ç¢ºèªï¼ˆCV RMSEãŒè¨“ç·´RMSEã¨è¿‘ã„ã‹ï¼‰</li>
                <li><strong>æ®‹å·®ã®ç­‰åˆ†æ•£æ€§</strong>: æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆãŒæ°´å¹³ã®ãƒ©ãƒ³ãƒ€ãƒ æ•£å¸ƒã‹</li>
                <li><strong>æ®‹å·®ã®æ­£è¦æ€§</strong>: ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ãŒæ­£è¦åˆ†å¸ƒã«è¿‘ã„ã‹</li>
                <li><strong>ä¸ç¢ºå®Ÿæ€§ã®æ ¡æ­£</strong>: 95%åŒºé–“ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒ90-100%ã‹</li>
            </ol>
        </div>

        <h2 id="summary">æœ¬ç« ã®ã¾ã¨ã‚</h2>

        <h3>é‡è¦ãªå­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ</h3>

        <table>
            <thead>
                <tr>
                    <th>ãƒˆãƒ”ãƒƒã‚¯</th>
                    <th>ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ</th>
                    <th>å®Ÿè·µã§ã®ä½¿ã„æ–¹</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>ã‚¬ã‚¦ã‚¹éç¨‹ã®å®šç¾©</strong></td>
                    <td>å¹³å‡é–¢æ•°ã¨å…±åˆ†æ•£é–¢æ•°ã§å®Œå…¨ã«å®šç¾©</td>
                    <td>scikit-learnã§RBFã‚«ãƒ¼ãƒãƒ«ã‹ã‚‰é–‹å§‹</td>
                </tr>
                <tr>
                    <td><strong>ã‚«ãƒ¼ãƒãƒ«é¸æŠ</strong></td>
                    <td>RBFï¼ˆã‚¹ãƒ ãƒ¼ã‚ºï¼‰ã€MatÃ©rnï¼ˆä¸­é–“ï¼‰ã€RQï¼ˆå¤šã‚¹ã‚±ãƒ¼ãƒ«ï¼‰</td>
                    <td>å¯¾æ•°å‘¨è¾ºå°¤åº¦ã§æ€§èƒ½æ¯”è¼ƒ</td>
                </tr>
                <tr>
                    <td><strong>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</strong></td>
                    <td>MLEã§è‡ªå‹•æœ€é©åŒ–ï¼ˆL-BFGS-Bæ³•ï¼‰</td>
                    <td>n_restarts_optimizer=10-15æ¨å¥¨</td>
                </tr>
                <tr>
                    <td><strong>ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–</strong></td>
                    <td>95%ä¿¡é ¼åŒºé–“ = Î¼ Â± 1.96Ïƒ</td>
                    <td>æœªæ¢ç´¢é ˜åŸŸã§ä¸ç¢ºå®Ÿæ€§å¤§â†’æ¢ç´¢ä¿ƒé€²</td>
                </tr>
                <tr>
                    <td><strong>ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼</strong></td>
                    <td>äº¤å·®æ¤œè¨¼ã€æ®‹å·®åˆ†æã€ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒã‚§ãƒƒã‚¯</td>
                    <td>RÂ²>0.9ã€ã‚«ãƒãƒ¬ãƒƒã‚¸90-100%ãŒç›®æ¨™</td>
                </tr>
            </tbody>
        </table>

        <h3>å®Ÿè£…æ™‚ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>

        <ol>
            <li><strong>ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–</strong>: å…¥åŠ›ã‚’[0, 1]ã¾ãŸã¯æ¨™æº–åŒ–ï¼ˆâ„“ã®æœ€é©åŒ–ã‚’å®‰å®šåŒ–ï¼‰</li>
            <li><strong>ã‚«ãƒ¼ãƒãƒ«é¸æŠ</strong>: æœ€åˆã¯RBFã§è©¦ã—ã€MatÃ©rnã§é ‘å¥æ€§å‘ä¸Š</li>
            <li><strong>ãƒã‚¤ã‚ºæ¨å®š</strong>: <code>alpha=1e-2</code> ã§è¦³æ¸¬èª¤å·®ã‚’è€ƒæ…®</li>
            <li><strong>å¤šç‚¹ã‚¹ã‚¿ãƒ¼ãƒˆ</strong>: <code>n_restarts_optimizer=10-15</code> ã§å±€æ‰€æœ€é©ã‚’å›é¿</li>
            <li><strong>æ•°å€¤å®‰å®šæ€§</strong>: ã‚³ãƒ¬ã‚¹ã‚­ãƒ¼åˆ†è§£ã‚’ä½¿ç”¨ï¼ˆè¡Œåˆ—é€†è¡Œåˆ—ã®ç›´æ¥è¨ˆç®—ã‚’é¿ã‘ã‚‹ï¼‰</li>
        </ol>

        <h2 id="learning-objectives">å­¦ç¿’ç›®æ¨™ã®ç¢ºèª</h2>

        <p>ã“ã®ç« ã‚’å®Œäº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š</p>

        <h3>åŸºæœ¬ç†è§£</h3>
        <ul>
            <li>âœ… ã‚¬ã‚¦ã‚¹éç¨‹ãŒå¹³å‡é–¢æ•°ã¨ã‚«ãƒ¼ãƒãƒ«ã§å®šç¾©ã•ã‚Œã‚‹ã“ã¨ã‚’èª¬æ˜ã§ãã‚‹</li>
            <li>âœ… äºˆæ¸¬åˆ†å¸ƒã‹ã‚‰å¹³å‡ã¨ä¸ç¢ºå®Ÿæ€§ã‚’è¨ˆç®—ã§ãã‚‹</li>
            <li>âœ… RBFã€MatÃ©rnã€Rational Quadraticã‚«ãƒ¼ãƒãƒ«ã®ç‰¹æ€§ã‚’èª¬æ˜ã§ãã‚‹</li>
        </ul>

        <h3>å®Ÿè·µã‚¹ã‚­ãƒ«</h3>
        <ul>
            <li>âœ… scikit-learnã§ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã§ãã‚‹</li>
            <li>âœ… ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’MLEã§æœ€é©åŒ–ã§ãã‚‹</li>
            <li>âœ… äºˆæ¸¬ä¸ç¢ºå®Ÿæ€§ã‚’å¯è¦–åŒ–ã§ãã‚‹ï¼ˆä¿¡é ¼åŒºé–“ãƒ—ãƒ­ãƒƒãƒˆï¼‰</li>
            <li>âœ… äº¤å·®æ¤œè¨¼ã¨æ®‹å·®åˆ†æã§ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œè¨¼ã§ãã‚‹</li>
        </ul>

        <h3>å¿œç”¨åŠ›</h3>
        <ul>
            <li>âœ… åŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã«é©åˆ‡ãªã‚«ãƒ¼ãƒãƒ«ã‚’é¸æŠã§ãã‚‹</li>
            <li>âœ… å¤šæ¬¡å…ƒå…¥åŠ›ï¼ˆæ¸©åº¦ãƒ»åœ§åŠ›ãƒ»æ¿ƒåº¦ï¼‰ã®GPãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
            <li>âœ… ãƒ¢ãƒ‡ãƒ«ã®ä¿¡é ¼æ€§ã‚’å®šé‡çš„ã«è©•ä¾¡ã§ãã‚‹</li>
        </ul>

        <h2 id="exercises">æ¼”ç¿’å•é¡Œ</h2>

        <h3>Easyï¼ˆåŸºç¤ç¢ºèªï¼‰</h3>

        <p><strong>Q1</strong>: ã‚¬ã‚¦ã‚¹éç¨‹ã®äºˆæ¸¬åˆ†å¸ƒ <code>f(x*) ~ N(Î¼, ÏƒÂ²)</code> ã«ãŠã„ã¦ã€95%ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—ã™ã‚‹å¼ã¯ã©ã‚Œã§ã™ã‹ï¼Ÿ</p>

        <details>
            <summary>è§£ç­”ã‚’è¦‹ã‚‹</summary>
            <p><strong>æ­£è§£</strong>: [Î¼ - 1.96Ïƒ, Î¼ + 1.96Ïƒ]</p>
            <p><strong>è§£èª¬</strong>: æ­£è¦åˆ†å¸ƒã®95%åŒºé–“ã¯å¹³å‡ Â± 1.96Ã—æ¨™æº–åå·®ã§ã™ã€‚99%åŒºé–“ã¯ Â± 2.58Ïƒã€68%åŒºé–“ã¯ Â± 1Ïƒã§ã™ã€‚</p>
        </details>

        <p><strong>Q2</strong>: RBFã‚«ãƒ¼ãƒãƒ«ã®é•·ã•ã‚¹ã‚±ãƒ¼ãƒ« <code>â„“</code> ã‚’å¤§ããã™ã‚‹ã¨ã€äºˆæ¸¬æ›²ç·šã¯ã©ã†å¤‰åŒ–ã—ã¾ã™ã‹ï¼Ÿ</p>

        <details>
            <summary>è§£ç­”ã‚’è¦‹ã‚‹</summary>
            <p><strong>æ­£è§£</strong>: ã‚ˆã‚Šã‚¹ãƒ ãƒ¼ã‚ºï¼ˆæ»‘ã‚‰ã‹ï¼‰ã«ãªã‚Šã¾ã™</p>
            <p><strong>è§£èª¬</strong>: â„“ã¯ç›¸é–¢è·é›¢ã‚’è¡¨ã—ã€å¤§ãã„ã»ã©é ãã®ç‚¹åŒå£«ã‚‚å¼·ãç›¸é–¢ã—ã¾ã™ã€‚â„“â†’âˆã§å®šæ•°é–¢æ•°ã«åæŸã—ã¾ã™ã€‚</p>
        </details>

        <h3>Mediumï¼ˆå¿œç”¨ï¼‰</h3>

        <p><strong>Q3</strong>: 5ã¤ã®å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¯¾æ•°å‘¨è¾ºå°¤åº¦ãŒ -10.5 ã®GPãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸã€‚ã“ã‚Œã¯è‰¯ã„ãƒ¢ãƒ‡ãƒ«ã§ã™ã‹ï¼Ÿ</p>

        <details>
            <summary>è§£ç­”ã‚’è¦‹ã‚‹</summary>
            <p><strong>å›ç­”</strong>: <strong>ç›¸å¯¾æ¯”è¼ƒ</strong>ãŒå¿…è¦ã§ã™ã€‚</p>
            <p><strong>è§£èª¬</strong>: å¯¾æ•°å‘¨è¾ºå°¤åº¦ã¯çµ¶å¯¾å€¤ã§ã¯ãªãã€ç•°ãªã‚‹ã‚«ãƒ¼ãƒãƒ«é–“ã®ç›¸å¯¾çš„ãªæ¯”è¼ƒã«ä½¿ã„ã¾ã™ã€‚ä¾‹ãˆã°ã€RBFã§-10.5ã€MatÃ©rnã§-12.3ãªã‚‰ã€RBFãŒå„ªã‚Œã¦ã„ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿æ•°nãŒå¢—ãˆã‚‹ã¨å°¤åº¦ã®çµ¶å¯¾å€¤ã‚‚å¢—åŠ ã™ã‚‹ãŸã‚ã€ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–“ã§ã¯æ¯”è¼ƒã§ãã¾ã›ã‚“ã€‚</p>
        </details>

        <p><strong>Q4</strong>: äº¤å·®æ¤œè¨¼RMSEãŒ3.5%ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿RMSEãŒ1.8%ã§ã—ãŸã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã«å•é¡Œã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ</p>

        <details>
            <summary>è§£ç­”ã‚’è¦‹ã‚‹</summary>
            <p><strong>å›ç­”</strong>: <strong>éå­¦ç¿’ã®å…†å€™</strong>ãŒã‚ã‚Šã¾ã™ã€‚</p>
            <p><strong>è§£èª¬</strong>: CVã‚¹ã‚³ã‚¢ãŒè¨“ç·´ã‚¹ã‚³ã‚¢ã‚ˆã‚Šå¤§å¹…ã«æ‚ªã„å ´åˆã€éå­¦ç¿’ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚å¯¾ç­–ï¼š(1) ãƒã‚¤ã‚ºé … <code>alpha</code> ã‚’å¢—ã‚„ã™ã€(2) ã‚ˆã‚Šå˜ç´”ãªã‚«ãƒ¼ãƒãƒ«ã‚’ä½¿ã†ã€(3) ãƒ‡ãƒ¼ã‚¿æ•°ã‚’å¢—ã‚„ã™ã€‚</p>
        </details>

        <h3>Hardï¼ˆç™ºå±•ï¼‰</h3>

        <p><strong>Q5</strong>: æ¸©åº¦ãƒ»åœ§åŠ›ãƒ»æ¿ƒåº¦ã®3æ¬¡å…ƒå…¥åŠ›ã§GPãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹éš›ã€å„æ¬¡å…ƒã§ç•°ãªã‚‹é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ã‚’ä½¿ã†ã¹ãç†ç”±ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

        <details>
            <summary>è§£ç­”ã‚’è¦‹ã‚‹</summary>
            <p><strong>å›ç­”</strong>: <strong>Automatic Relevance Determination (ARD)</strong> ã«ã‚ˆã‚Šã€å„å…¥åŠ›æ¬¡å…ƒã®é‡è¦åº¦ã‚’è‡ªå‹•å­¦ç¿’ã§ãã¾ã™ã€‚</p>
            <p><strong>å®Ÿè£…ä¾‹</strong>:</p>
            <pre><code>kernel = C(1.0) * RBF(length_scale=[10.0, 1.0, 5.0],
                   length_scale_bounds=(1e-2, 1e3))
# [æ¸©åº¦10Â°C, åœ§åŠ›1.0MPa, æ¿ƒåº¦5%ã®åˆæœŸé•·ã•ã‚¹ã‚±ãƒ¼ãƒ«]</code></pre>
            <p><strong>è§£é‡ˆ</strong>: æœ€é©åŒ–å¾Œã€åœ§åŠ›ã®é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ãŒ0.5ã¨å°ã•ã‘ã‚Œã°ã€åœ§åŠ›å¤‰åŒ–ã«åç‡ãŒæ•æ„Ÿã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚æ¸©åº¦ãŒ50ã¨å¤§ãã‘ã‚Œã°ã€æ¸©åº¦ä¾å­˜æ€§ã¯ä½ã„ã§ã™ã€‚</p>
        </details>

        <h2 id="next-steps">æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h2>

        <p>ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«ã§ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚æ¬¡ã®ç¬¬3ç« ã§ã¯ã€ã“ã®ä¸ç¢ºå®Ÿæ€§ã‚’æ´»ç”¨ã—ã¦<strong>æ¬¡ã®å®Ÿé¨“ç‚¹ã‚’è³¢ãé¸ã¶ç²å¾—é–¢æ•°</strong>ã‚’å­¦ã³ã¾ã™ã€‚</p>

        <p><strong>ç¬¬3ç« ã§å­¦ã¶ã“ã¨ï¼š</strong></p>
        <ul>
            <li>Expected Improvement (EI): æœ€ã‚‚æœ‰æœ›ãªç‚¹ã‚’é¸ã¶</li>
            <li>Upper Confidence Bound (UCB): æ¢ç´¢-æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹èª¿æ•´</li>
            <li>Probability of Improvement (PI): ã‚·ãƒ³ãƒ—ãƒ«ãªæ”¹å–„ç¢ºç‡</li>
            <li>ãƒãƒƒãƒãƒ™ã‚¤ã‚ºæœ€é©åŒ–: ä¸¦åˆ—å®Ÿé¨“ã®è¨­è¨ˆ</li>
        </ul>

        <div class="nav-buttons">
            <a href="./index.html" class="nav-button secondary">â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a>
            <a href="./chapter-3.html" class="nav-button">ç¬¬3ç« ï¼šç²å¾—é–¢æ•° â†’</a>
        </div>

        <h2 id="references">å‚è€ƒæ–‡çŒ®</h2>

        <ol>
            <li>Rasmussen, C. E., & Williams, C. K. I. (2006). <em>Gaussian Processes for Machine Learning</em>. MIT Press.</li>
            <li>Shahriari, B., et al. (2016). "Taking the Human Out of the Loop: A Review of Bayesian Optimization." <em>Proceedings of the IEEE</em>, 104(1), 148-175.</li>
            <li>Pedregosa, F., et al. (2011). "Scikit-learn: Machine Learning in Python." <em>Journal of Machine Learning Research</em>, 12, 2825-2830.</li>
            <li>MatÃ©rn, B. (1960). <em>Spatial Variation</em>. Springer.</li>
        </ol>
    </main>

    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€ï¼ˆæ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©ï¼‰ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿ï¼ˆAS ISï¼‰ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶ï¼ˆä¾‹: CC BY 4.0ï¼‰ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2025 PI Knowledge Hub - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
