---
title: ç¬¬2ç« ï¼šã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°
chapter_title: ç¬¬2ç« ï¼šã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°
subtitle: ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–ã™ã‚‹å¼·åŠ›ãªå›å¸°æ‰‹æ³•
---

## ã¯ã˜ã‚ã«

ã‚¬ã‚¦ã‚¹éç¨‹ï¼ˆGaussian Process, GPï¼‰ã¯ã€ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ä¸­æ ¸ã‚’æˆã™ç¢ºç‡çš„ãƒ¢ãƒ‡ãƒªãƒ³ã‚°æ‰‹æ³•ã§ã™ã€‚å¾“æ¥ã®å›å¸°æ‰‹æ³•ã¨ç•°ãªã‚Šã€GPã¯äºˆæ¸¬å€¤ã ã‘ã§ãªã**äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§** ã‚’å®šé‡åŒ–ã§ãã‚‹ãŸã‚ã€æ¢ç´¢-æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æœ€é©åŒ–ã§ãã¾ã™ã€‚

æœ¬ç« ã§ã¯ã€1æ¬¡å…ƒå›å¸°ã‹ã‚‰å§‹ã‚ã¦ã€å„ç¨®ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã€ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼ã¾ã§ã€åŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸå®Ÿè·µçš„ãªå®Ÿè£…ã‚’å­¦ã³ã¾ã™ã€‚

ğŸ’¡ æœ¬ç« ã®é‡è¦ãƒã‚¤ãƒ³ãƒˆ

  * ã‚¬ã‚¦ã‚¹éç¨‹ã¯å¹³å‡é–¢æ•°ã¨å…±åˆ†æ•£é–¢æ•°ï¼ˆã‚«ãƒ¼ãƒãƒ«ï¼‰ã§å®Œå…¨ã«å®šç¾©ã•ã‚Œã‚‹
  * ã‚«ãƒ¼ãƒãƒ«é¸æŠã¯å•é¡Œã®ã‚¹ãƒ ãƒ¼ã‚ºã•ã«å¿œã˜ã¦èª¿æ•´ã™ã‚‹
  * ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯æœ€å°¤æ¨å®šï¼ˆMLEï¼‰ã§è‡ªå‹•æœ€é©åŒ–ã§ãã‚‹
  * äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ã¯ä¿¡é ¼åŒºé–“ã¨ã—ã¦å¯è¦–åŒ–ã§ãã‚‹

## 2.1 ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã®åŸºç¤

### 2.1.1 æ•°å­¦çš„å®šç¾©

ã‚¬ã‚¦ã‚¹éç¨‹ã¯ã€**ä»»æ„ã®æœ‰é™å€‹ã®ç‚¹ã«ãŠã‘ã‚‹é–¢æ•°å€¤ãŒåŒæ™‚ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã«å¾“ã†** ç¢ºç‡éç¨‹ã§ã™ï¼š
    
    
    f(x) ~ GP(m(x), k(x, x'))
    
    m(x) = E[f(x)]                    # å¹³å‡é–¢æ•°
    k(x, x') = E[(f(x) - m(x))(f(x') - m(x'))]  # å…±åˆ†æ•£é–¢æ•°ï¼ˆã‚«ãƒ¼ãƒãƒ«ï¼‰

è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ `D = {(x_i, y_i)}` ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€æ–°ã—ã„ç‚¹ `x*` ã§ã®äºˆæ¸¬åˆ†å¸ƒã¯ï¼š
    
    
    f(x*) | D ~ N(Î¼(x*), ÏƒÂ²(x*))
    
    Î¼(x*) = k(x*, X) [K + Ïƒ_nÂ² I]â»Â¹ y
    ÏƒÂ²(x*) = k(x*, x*) - k(x*, X) [K + Ïƒ_nÂ² I]â»Â¹ k(X, x*)

### ä¾‹1ï¼š1æ¬¡å…ƒã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ï¼ˆåŒ–å­¦åå¿œåç‡ï¼‰
    
    
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.gaussian_process import GaussianProcessRegressor
    from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
    
    # ===================================
    # 1DåŒ–å­¦åå¿œåç‡ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¸©åº¦ vs åç‡ï¼‰
    # ===================================
    # å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¸©åº¦ [Â°C] â†’ åç‡ [%]ï¼‰
    X_train = np.array([50, 70, 90, 110, 130]).reshape(-1, 1)
    y_train = np.array([45, 62, 78, 71, 52])  # æœ€é©æ¸©åº¦90Â°Cä»˜è¿‘
    
    # ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
    kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10,
                                   alpha=1e-2, random_state=42)
    
    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è‡ªå‹•æœ€é©åŒ–ï¼‰
    gp.fit(X_train, y_train)
    
    # äºˆæ¸¬ï¼ˆä¸ç¢ºå®Ÿæ€§ä»˜ãï¼‰
    X_test = np.linspace(40, 140, 100).reshape(-1, 1)
    y_pred, sigma = gp.predict(X_test, return_std=True)
    
    # å¯è¦–åŒ–
    plt.figure(figsize=(10, 6))
    plt.plot(X_test, y_pred, 'b-', label='GPäºˆæ¸¬å¹³å‡', linewidth=2)
    plt.fill_between(X_test.ravel(),
                     y_pred - 1.96*sigma,
                     y_pred + 1.96*sigma,
                     alpha=0.3, label='95%ä¿¡é ¼åŒºé–“')
    plt.scatter(X_train, y_train, c='red', s=100, zorder=10,
                edgecolors='k', label='å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿')
    plt.xlabel('æ¸©åº¦ [Â°C]', fontsize=12)
    plt.ylabel('åç‡ [%]', fontsize=12)
    plt.title('ã‚¬ã‚¦ã‚¹éç¨‹ã«ã‚ˆã‚‹åå¿œåç‡äºˆæ¸¬', fontsize=14)
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    print(f"æœ€é©åŒ–ã•ã‚ŒãŸã‚«ãƒ¼ãƒãƒ«: {gp.kernel_}")
    print(f"å¯¾æ•°å‘¨è¾ºå°¤åº¦: {gp.log_marginal_likelihood_value_:.2f}")
    
    # æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
    # æœ€é©åŒ–ã•ã‚ŒãŸã‚«ãƒ¼ãƒãƒ«: 31.6**2 * RBF(length_scale=15.8)
    # å¯¾æ•°å‘¨è¾ºå°¤åº¦: -12.45
    # ãƒ—ãƒ­ãƒƒãƒˆ: 90Â°Cä»˜è¿‘ã§åç‡ãƒ”ãƒ¼ã‚¯ã€ç«¯ã§ä¸ç¢ºå®Ÿæ€§å¢—å¤§

ğŸ” è§£èª¬ï¼šä¿¡é ¼åŒºé–“ã®æ„å‘³

**95%ä¿¡é ¼åŒºé–“** (Î¼ Â± 1.96Ïƒ) ã¯ã€çœŸã®åç‡ãŒ95%ã®ç¢ºç‡ã§ã“ã®ç¯„å›²ã«å…¥ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é ã„é ˜åŸŸï¼ˆ40Â°Cã€140Â°Cä»˜è¿‘ï¼‰ã§ã¯ä¸ç¢ºå®Ÿæ€§ãŒå¢—å¤§ã—ã€åŒºé–“ãŒåºƒãŒã‚Šã¾ã™ã€‚ã“ã‚ŒãŒãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§**æœªæ¢ç´¢é ˜åŸŸã®æ¢ç´¢** ã‚’ä¿ƒé€²ã™ã‚‹éµã§ã™ã€‚

## 2.2 ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã®é¸æŠ

### 2.2.1 ä¸»è¦ã‚«ãƒ¼ãƒãƒ«ã®ç‰¹æ€§

ã‚«ãƒ¼ãƒãƒ« | å¼ | ã‚¹ãƒ ãƒ¼ã‚ºã• | é©ç”¨ä¾‹  
---|---|---|---  
**RBF (Squared Exponential)** | k(x, x') = ÏƒÂ² exp(-||x - x'||Â² / (2â„“Â²)) | ç„¡é™å›å¾®åˆ†å¯èƒ½ | æ¸©åº¦-åç‡é–¢ä¿‚  
**MatÃ©rn (Î½=1.5)** | k(x, x') = ÏƒÂ² (1 + âˆš3r/â„“) exp(-âˆš3r/â„“) | 1å›å¾®åˆ†å¯èƒ½ | åœ§åŠ›-æµé‡é–¢ä¿‚  
**MatÃ©rn (Î½=2.5)** | k(x, x') = ÏƒÂ² (1 + âˆš5r/â„“ + 5rÂ²/3â„“Â²) exp(-âˆš5r/â„“) | 2å›å¾®åˆ†å¯èƒ½ | è§¦åª’æ´»æ€§æ›²ç·š  
**Rational Quadratic** | k(x, x') = ÏƒÂ² (1 + rÂ²/(2Î±â„“Â²))^(-Î±) | RBFã®ã‚¹ã‚±ãƒ¼ãƒ«æ··åˆ | å¤šã‚¹ã‚±ãƒ¼ãƒ«ç¾è±¡  
  
### ä¾‹2ï¼šRBFã‚«ãƒ¼ãƒãƒ«ï¼ˆã‚¹ãƒ ãƒ¼ã‚ºãªåå¿œæ›²ç·šï¼‰
    
    
    from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
    
    # ===================================
    # RBFã‚«ãƒ¼ãƒãƒ«: ç„¡é™å›å¾®åˆ†å¯èƒ½ï¼ˆéå¸¸ã«ã‚¹ãƒ ãƒ¼ã‚ºï¼‰
    # ===================================
    # æ¸©åº¦-åœ§åŠ›åŒæ™‚å¤‰åŒ–ã«ã‚ˆã‚‹åç‡ãƒ‡ãƒ¼ã‚¿
    X_train = np.array([[60, 1.0], [80, 1.5], [100, 2.0],
                        [80, 2.5], [60, 2.0]]) # [æ¸©åº¦Â°C, åœ§åŠ›MPa]
    y_train = np.array([50, 68, 85, 72, 58])
    
    # RBFã‚«ãƒ¼ãƒãƒ«å®šç¾©
    kernel_rbf = C(1.0) * RBF(length_scale=[10.0, 0.5],
                               length_scale_bounds=(1e-2, 1e3))
    
    gp_rbf = GaussianProcessRegressor(kernel=kernel_rbf, n_restarts_optimizer=15)
    gp_rbf.fit(X_train, y_train)
    
    # ç­‰é«˜ç·šãƒ—ãƒ­ãƒƒãƒˆç”¨ã‚°ãƒªãƒƒãƒ‰
    temp_range = np.linspace(50, 110, 50)
    pressure_range = np.linspace(0.8, 3.0, 50)
    T, P = np.meshgrid(temp_range, pressure_range)
    X_grid = np.c_[T.ravel(), P.ravel()]
    
    y_pred_grid, sigma_grid = gp_rbf.predict(X_grid, return_std=True)
    Y_pred = y_pred_grid.reshape(T.shape)
    Sigma = sigma_grid.reshape(T.shape)
    
    # å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # äºˆæ¸¬å¹³å‡
    contour1 = axes[0].contourf(T, P, Y_pred, levels=15, cmap='viridis')
    axes[0].scatter(X_train[:, 0], X_train[:, 1], c='red', s=150,
                    edgecolors='white', linewidths=2, label='å®Ÿé¨“ç‚¹')
    axes[0].set_xlabel('æ¸©åº¦ [Â°C]')
    axes[0].set_ylabel('åœ§åŠ› [MPa]')
    axes[0].set_title('RBFã‚«ãƒ¼ãƒãƒ«: äºˆæ¸¬åç‡ [%]')
    plt.colorbar(contour1, ax=axes[0])
    axes[0].legend()
    
    # ä¸ç¢ºå®Ÿæ€§
    contour2 = axes[1].contourf(T, P, Sigma, levels=15, cmap='Reds')
    axes[1].scatter(X_train[:, 0], X_train[:, 1], c='blue', s=150,
                    edgecolors='white', linewidths=2, label='å®Ÿé¨“ç‚¹')
    axes[1].set_xlabel('æ¸©åº¦ [Â°C]')
    axes[1].set_ylabel('åœ§åŠ› [MPa]')
    axes[1].set_title('RBFã‚«ãƒ¼ãƒãƒ«: äºˆæ¸¬æ¨™æº–åå·® [%]')
    plt.colorbar(contour2, ax=axes[1])
    axes[1].legend()
    
    plt.tight_layout()
    plt.show()
    
    print(f"æœ€é©åŒ–ã•ã‚ŒãŸé•·ã•ã‚¹ã‚±ãƒ¼ãƒ«: {gp_rbf.kernel_.k2.length_scale}")
    print(f"æ¸©åº¦æ–¹å‘: {gp_rbf.kernel_.k2.length_scale[0]:.2f}Â°C")
    print(f"åœ§åŠ›æ–¹å‘: {gp_rbf.kernel_.k2.length_scale[1]:.2f} MPa")
    
    # æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
    # æœ€é©åŒ–ã•ã‚ŒãŸé•·ã•ã‚¹ã‚±ãƒ¼ãƒ«: [12.34  0.68]
    # æ¸©åº¦æ–¹å‘: 12.34Â°C
    # åœ§åŠ›æ–¹å‘: 0.68 MPa
    # â†’ æ¸©åº¦ã‚ˆã‚Šåœ§åŠ›ã®å¤‰åŒ–ã«æ•æ„Ÿãªåç‡ç‰¹æ€§

### ä¾‹3ï¼šMatÃ©rnã‚«ãƒ¼ãƒãƒ«ï¼ˆÎ½=1.5ï¼‰
    
    
    from sklearn.gaussian_process.kernels import Matern
    
    # ===================================
    # MatÃ©rnã‚«ãƒ¼ãƒãƒ« (Î½=1.5): 1å›å¾®åˆ†å¯èƒ½
    # ç‰©ç†æ³•å‰‡ã«åŸºã¥ãä¸­ç¨‹åº¦ã®ã‚¹ãƒ ãƒ¼ã‚ºã•
    # ===================================
    kernel_matern15 = C(1.0) * Matern(length_scale=10.0, nu=1.5)
    
    gp_matern15 = GaussianProcessRegressor(kernel=kernel_matern15,
                                            n_restarts_optimizer=10)
    gp_matern15.fit(X_train, y_train)
    
    # 1Dæ–­é¢ã§ã®æ¯”è¼ƒï¼ˆåœ§åŠ›=2.0 MPaå›ºå®šï¼‰
    X_test_1d = np.column_stack([np.linspace(50, 110, 100),
                                  np.full(100, 2.0)])
    y_pred_matern, sigma_matern = gp_matern15.predict(X_test_1d, return_std=True)
    
    plt.figure(figsize=(10, 6))
    plt.plot(X_test_1d[:, 0], y_pred_matern, 'g-',
             label='MatÃ©rn(Î½=1.5)', linewidth=2)
    plt.fill_between(X_test_1d[:, 0],
                     y_pred_matern - 1.96*sigma_matern,
                     y_pred_matern + 1.96*sigma_matern,
                     alpha=0.3, color='green')
    plt.scatter(X_train[:, 0], y_train, c='red', s=100,
                edgecolors='k', label='å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿')
    plt.xlabel('æ¸©åº¦ [Â°C] (åœ§åŠ›=2.0 MPaå›ºå®š)')
    plt.ylabel('åç‡ [%]')
    plt.title('MatÃ©rnã‚«ãƒ¼ãƒãƒ« (Î½=1.5) ã«ã‚ˆã‚‹äºˆæ¸¬')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    print(f"MatÃ©rn(Î½=1.5) ã‚«ãƒ¼ãƒãƒ«: {gp_matern15.kernel_}")
    print(f"å¯¾æ•°å‘¨è¾ºå°¤åº¦: {gp_matern15.log_marginal_likelihood_value_:.2f}")
    
    # æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
    # MatÃ©rn(Î½=1.5) ã‚«ãƒ¼ãƒãƒ«: 1.2**2 * Matern(length_scale=11.5, nu=1.5)
    # å¯¾æ•°å‘¨è¾ºå°¤åº¦: -10.23
    # â†’ RBFã‚ˆã‚Šè‹¥å¹²ãƒ©ãƒ•ãªäºˆæ¸¬ï¼ˆå®Ÿé¨“ãƒã‚¤ã‚ºã«é ‘å¥ï¼‰

### ä¾‹4ï¼šMatÃ©rnã‚«ãƒ¼ãƒãƒ«ï¼ˆÎ½=2.5ï¼‰
    
    
    # ===================================
    # MatÃ©rnã‚«ãƒ¼ãƒãƒ« (Î½=2.5): 2å›å¾®åˆ†å¯èƒ½
    # RBFã¨Î½=1.5ã®ä¸­é–“çš„ãªã‚¹ãƒ ãƒ¼ã‚ºã•
    # ===================================
    kernel_matern25 = C(1.0) * Matern(length_scale=10.0, nu=2.5)
    
    gp_matern25 = GaussianProcessRegressor(kernel=kernel_matern25,
                                            n_restarts_optimizer=10)
    gp_matern25.fit(X_train, y_train)
    
    y_pred_matern25, sigma_matern25 = gp_matern25.predict(X_test_1d, return_std=True)
    
    # 3ã¤ã®ã‚«ãƒ¼ãƒãƒ«æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆ
    plt.figure(figsize=(12, 6))
    
    plt.plot(X_test_1d[:, 0], y_pred_grid[:100], 'b-',
             label='RBF (âˆå›å¾®åˆ†å¯èƒ½)', linewidth=2)
    plt.plot(X_test_1d[:, 0], y_pred_matern, 'g--',
             label='MatÃ©rn(Î½=1.5)', linewidth=2)
    plt.plot(X_test_1d[:, 0], y_pred_matern25, 'orange',
             linestyle='-.', label='MatÃ©rn(Î½=2.5)', linewidth=2)
    
    plt.scatter(X_train[:, 0], y_train, c='red', s=150,
                edgecolors='k', linewidths=2, label='å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿', zorder=10)
    
    plt.xlabel('æ¸©åº¦ [Â°C] (åœ§åŠ›=2.0 MPaå›ºå®š)', fontsize=12)
    plt.ylabel('åç‡ [%]', fontsize=12)
    plt.title('ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã®æ¯”è¼ƒï¼ˆã‚¹ãƒ ãƒ¼ã‚ºã•ã®é•ã„ï¼‰', fontsize=14)
    plt.legend(fontsize=11)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    # å¯¾æ•°å‘¨è¾ºå°¤åº¦ã®æ¯”è¼ƒï¼ˆé«˜ã„ã»ã©è‰¯ã„ï¼‰
    print("\n=== ã‚«ãƒ¼ãƒãƒ«æ€§èƒ½æ¯”è¼ƒ ===")
    print(f"RBF:           {gp_rbf.log_marginal_likelihood_value_:.2f}")
    print(f"MatÃ©rn(Î½=1.5): {gp_matern15.log_marginal_likelihood_value_:.2f}")
    print(f"MatÃ©rn(Î½=2.5): {gp_matern25.log_marginal_likelihood_value_:.2f}")
    
    # æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
    # === ã‚«ãƒ¼ãƒãƒ«æ€§èƒ½æ¯”è¼ƒ ===
    # RBF:           -9.87
    # MatÃ©rn(Î½=1.5): -10.23
    # MatÃ©rn(Î½=2.5): -9.95
    # â†’ RBFãŒæœ€ã‚‚é«˜ã„å°¤åº¦ï¼ˆã“ã®ãƒ‡ãƒ¼ã‚¿ã«æœ€é©ï¼‰

ğŸ“Š ã‚«ãƒ¼ãƒãƒ«é¸æŠã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

  * **RBF** : éå¸¸ã«ã‚¹ãƒ ãƒ¼ã‚ºãªå¿œç­”ï¼ˆæ¸©åº¦-åç‡ã€æ¿ƒåº¦-æ´»æ€§ï¼‰
  * **MatÃ©rn(Î½=1.5)** : å®Ÿé¨“ãƒã‚¤ã‚ºãŒå¤§ãã„å ´åˆã€ç‰©ç†çš„åˆ¶ç´„ã‚ã‚Š
  * **MatÃ©rn(Î½=2.5)** : ãƒãƒ©ãƒ³ã‚¹å‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¨å¥¨ï¼‰
  * **Rational Quadratic** : è¤‡æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã®å¤‰å‹•ï¼ˆå¤šæ®µéšåå¿œï¼‰

### ä¾‹5ï¼šRational Quadraticã‚«ãƒ¼ãƒãƒ«
    
    
    from sklearn.gaussian_process.kernels import RationalQuadratic
    
    # ===================================
    # Rational Quadraticã‚«ãƒ¼ãƒãƒ«:
    # ç•°ãªã‚‹é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ã®RBFã‚«ãƒ¼ãƒãƒ«ã®ç„¡é™æ··åˆ
    # å¤šã‚¹ã‚±ãƒ¼ãƒ«ç¾è±¡ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«æœ‰åŠ¹
    # ===================================
    kernel_rq = C(1.0) * RationalQuadratic(length_scale=10.0, alpha=1.0)
    
    gp_rq = GaussianProcessRegressor(kernel=kernel_rq, n_restarts_optimizer=10)
    gp_rq.fit(X_train, y_train)
    
    y_pred_rq, sigma_rq = gp_rq.predict(X_test_1d, return_std=True)
    
    plt.figure(figsize=(10, 6))
    plt.plot(X_test_1d[:, 0], y_pred_rq, 'purple', linewidth=2,
             label='Rational Quadratic')
    plt.fill_between(X_test_1d[:, 0],
                     y_pred_rq - 1.96*sigma_rq,
                     y_pred_rq + 1.96*sigma_rq,
                     alpha=0.3, color='purple')
    plt.scatter(X_train[:, 0], y_train, c='red', s=100,
                edgecolors='k', label='å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿')
    plt.xlabel('æ¸©åº¦ [Â°C] (åœ§åŠ›=2.0 MPaå›ºå®š)')
    plt.ylabel('åç‡ [%]')
    plt.title('Rational Quadraticã‚«ãƒ¼ãƒãƒ«ã«ã‚ˆã‚‹äºˆæ¸¬')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    print(f"æœ€é©åŒ–ã•ã‚ŒãŸã‚«ãƒ¼ãƒãƒ«: {gp_rq.kernel_}")
    print(f"Î± (ã‚¹ã‚±ãƒ¼ãƒ«æ··åˆåº¦): {gp_rq.kernel_.k2.alpha:.2f}")
    print(f"å¯¾æ•°å‘¨è¾ºå°¤åº¦: {gp_rq.log_marginal_likelihood_value_:.2f}")
    
    # æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
    # æœ€é©åŒ–ã•ã‚ŒãŸã‚«ãƒ¼ãƒãƒ«: 1.15**2 * RationalQuadratic(alpha=0.85, length_scale=12.3)
    # Î± (ã‚¹ã‚±ãƒ¼ãƒ«æ··åˆåº¦): 0.85
    # å¯¾æ•°å‘¨è¾ºå°¤åº¦: -10.10
    # â†’ Î±<1: çŸ­è·é›¢ç›¸é–¢ãŒæ”¯é…çš„ã€Î±â†’âˆã§RBFã«åæŸ

## 2.3 ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–

### 2.3.1 æœ€å°¤æ¨å®šï¼ˆMLEï¼‰ã®åŸç†

ã‚¬ã‚¦ã‚¹éç¨‹ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ `Î¸ = {ÏƒÂ², â„“, Ïƒ_nÂ²}` ã¯ã€å¯¾æ•°å‘¨è¾ºå°¤åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã§æœ€é©åŒ–ã—ã¾ã™ï¼š
    
    
    log p(y | X, Î¸) = -1/2 y^T [K + Ïƒ_nÂ² I]â»Â¹ y
                      - 1/2 log|K + Ïƒ_nÂ² I|
                      - n/2 log(2Ï€)
    
    æœ€é©åŒ–: Î¸* = argmax_Î¸ log p(y | X, Î¸)

### ä¾‹6ï¼šãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ï¼ˆMLE with scipyï¼‰
    
    
    from scipy.optimize import minimize
    from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
    import numpy as np
    
    # ===================================
    # æ‰‹å‹•ã§ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–å®Ÿè£…
    # ï¼ˆscikit-learnã®å†…éƒ¨å‡¦ç†ã‚’ç†è§£ã™ã‚‹ï¼‰
    # ===================================
    # ãƒ‡ãƒ¼ã‚¿
    X = np.array([[60], [80], [100], [80], [60]])
    y = np.array([50, 68, 85, 72, 58])
    
    def negative_log_marginal_likelihood(theta, X, y):
        """å¯¾æ•°å‘¨è¾ºå°¤åº¦ã®è² å€¤ï¼ˆæœ€å°åŒ–ç”¨ï¼‰
    
        Args:
            theta: [log(ÏƒÂ²), log(â„“), log(Ïƒ_nÂ²)]
            X: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ (n, d)
            y: å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ (n,)
    
        Returns:
            -log p(y | X, Î¸)
        """
        sigma_f = np.exp(theta[0])  # ã‚·ã‚°ãƒŠãƒ«åˆ†æ•£
        length_scale = np.exp(theta[1])  # é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«
        sigma_n = np.exp(theta[2])  # ãƒã‚¤ã‚ºæ¨™æº–åå·®
    
        # ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—è¨ˆç®—
        n = X.shape[0]
        K = np.zeros((n, n))
        for i in range(n):
            for j in range(n):
                r_sq = np.sum((X[i] - X[j])**2)
                K[i, j] = sigma_f**2 * np.exp(-r_sq / (2 * length_scale**2))
    
        # ãƒã‚¤ã‚ºè¿½åŠ 
        K_y = K + sigma_n**2 * np.eye(n)
    
        # å¯¾æ•°å‘¨è¾ºå°¤åº¦è¨ˆç®—
        try:
            L = np.linalg.cholesky(K_y)  # ã‚³ãƒ¬ã‚¹ã‚­ãƒ¼åˆ†è§£ï¼ˆæ•°å€¤å®‰å®šï¼‰
            alpha = np.linalg.solve(L.T, np.linalg.solve(L, y))
    
            log_likelihood = (-0.5 * y.dot(alpha)
                             - np.sum(np.log(np.diag(L)))
                             - n/2 * np.log(2*np.pi))
    
            return -log_likelihood  # æœ€å°åŒ–ã®ãŸã‚è² å€¤
        except np.linalg.LinAlgError:
            return 1e10  # æ•°å€¤ä¸å®‰å®šãªå ´åˆã¯ãƒšãƒŠãƒ«ãƒ†ã‚£
    
    # åˆæœŸå€¤ï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
    theta_init = np.log([1.0, 10.0, 1.0])  # [ÏƒÂ², â„“, Ïƒ_nÂ²]
    
    # æœ€é©åŒ–å®Ÿè¡Œï¼ˆL-BFGS-Bæ³•ï¼‰
    result = minimize(negative_log_marginal_likelihood, theta_init,
                      args=(X, y), method='L-BFGS-B',
                      bounds=[(-5, 5), (-2, 5), (-5, 2)])  # logç©ºé–“ã®å¢ƒç•Œ
    
    # æœ€é©ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    theta_opt = result.x
    sigma_f_opt = np.exp(theta_opt[0])
    length_scale_opt = np.exp(theta_opt[1])
    sigma_n_opt = np.exp(theta_opt[2])
    
    print("=== æœ€é©åŒ–çµæœ ===")
    print(f"ã‚·ã‚°ãƒŠãƒ«åˆ†æ•£ ÏƒÂ²: {sigma_f_opt:.3f}")
    print(f"é•·ã•ã‚¹ã‚±ãƒ¼ãƒ« â„“: {length_scale_opt:.2f}Â°C")
    print(f"ãƒã‚¤ã‚ºæ¨™æº–åå·® Ïƒ_n: {sigma_n_opt:.3f}%")
    print(f"\næœ€å¤§å¯¾æ•°å‘¨è¾ºå°¤åº¦: {-result.fun:.2f}")
    print(f"æœ€é©åŒ–ã‚¹ãƒ†ãƒƒãƒ—æ•°: {result.nit}")
    
    # scikit-learnã¨ã®æ¯”è¼ƒ
    kernel_sklearn = C(1.0) * RBF(10.0)
    gp_sklearn = GaussianProcessRegressor(kernel=kernel_sklearn,
                                           n_restarts_optimizer=10)
    gp_sklearn.fit(X, y)
    
    print(f"\n=== scikit-learnçµæœï¼ˆæ¯”è¼ƒï¼‰ ===")
    print(f"æœ€é©åŒ–ã‚«ãƒ¼ãƒãƒ«: {gp_sklearn.kernel_}")
    print(f"å¯¾æ•°å‘¨è¾ºå°¤åº¦: {gp_sklearn.log_marginal_likelihood_value_:.2f}")
    
    # æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
    # === æœ€é©åŒ–çµæœ ===
    # ã‚·ã‚°ãƒŠãƒ«åˆ†æ•£ ÏƒÂ²: 156.234
    # é•·ã•ã‚¹ã‚±ãƒ¼ãƒ« â„“: 14.87Â°C
    # ãƒã‚¤ã‚ºæ¨™æº–åå·® Ïƒ_n: 2.145%
    #
    # æœ€å¤§å¯¾æ•°å‘¨è¾ºå°¤åº¦: -8.92
    # æœ€é©åŒ–ã‚¹ãƒ†ãƒƒãƒ—æ•°: 23
    #
    # === scikit-learnçµæœï¼ˆæ¯”è¼ƒï¼‰ ===
    # æœ€é©åŒ–ã‚«ãƒ¼ãƒãƒ«: 12.5**2 * RBF(length_scale=14.9)
    # å¯¾æ•°å‘¨è¾ºå°¤åº¦: -8.91
    # â†’ æ‰‹å‹•å®Ÿè£…ã¨scikit-learnãŒã»ã¼ä¸€è‡´

âš™ï¸ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è§£é‡ˆ

  * **ÏƒÂ² (ã‚·ã‚°ãƒŠãƒ«åˆ†æ•£)** : é–¢æ•°ã®æŒ¯å¹…ï¼ˆå¤§ãã„ã»ã©å¤§ããå¤‰å‹•ï¼‰
  * **â„“ (é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«)** : ç›¸é–¢è·é›¢ï¼ˆå¤§ãã„ã»ã©ã‚¹ãƒ ãƒ¼ã‚ºï¼‰
  * **Ïƒ_nÂ² (ãƒã‚¤ã‚ºåˆ†æ•£)** : è¦³æ¸¬èª¤å·®ï¼ˆå®Ÿé¨“ç²¾åº¦ã®é€†æ•°ï¼‰

## 2.4 ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–ã¨ä¿¡é ¼åŒºé–“

### 2.4.1 äºˆæ¸¬åˆ†å¸ƒã®è§£é‡ˆ

ã‚¬ã‚¦ã‚¹éç¨‹ã®äºˆæ¸¬åˆ†å¸ƒ `f(x*) ~ N(Î¼, ÏƒÂ²)` ã‹ã‚‰ã€ä»¥ä¸‹ã®æƒ…å ±ãŒå¾—ã‚‰ã‚Œã¾ã™ï¼š

  * **Î¼(x*)** : äºˆæ¸¬å€¤ï¼ˆæœŸå¾…å€¤ï¼‰
  * **Ïƒ(x*)** : äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ï¼ˆæ¨™æº–åå·®ï¼‰
  * **95%ä¿¡é ¼åŒºé–“** : [Î¼ - 1.96Ïƒ, Î¼ + 1.96Ïƒ]
  * **99%ä¿¡é ¼åŒºé–“** : [Î¼ - 2.58Ïƒ, Î¼ + 2.58Ïƒ]

### ä¾‹7ï¼šãƒ¢ãƒ‡ãƒ«æ¤œè¨¼ï¼ˆäº¤å·®æ¤œè¨¼ãƒ»æ®‹å·®åˆ†æï¼‰
    
    
    from sklearn.model_selection import cross_val_score, KFold
    from sklearn.metrics import mean_squared_error, r2_score
    import numpy as np
    
    # ===================================
    # ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«ã®åŒ…æ‹¬çš„æ¤œè¨¼
    # ===================================
    # ã‚ˆã‚Šå¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆï¼ˆè§¦åª’æ´»æ€§ãƒ‡ãƒ¼ã‚¿ï¼‰
    np.random.seed(42)
    X_full = np.random.uniform(50, 150, 30).reshape(-1, 1)  # æ¸©åº¦ [Â°C]
    y_true = 50 + 40 * np.exp(-(X_full.ravel() - 100)**2 / 400)  # çœŸã®é–¢æ•°
    y_full = y_true + np.random.normal(0, 3, 30)  # ãƒã‚¤ã‚ºè¿½åŠ 
    
    # ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«
    kernel = C(1.0) * RBF(10.0)
    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=15,
                                   alpha=1e-2)
    
    # === 1. äº¤å·®æ¤œè¨¼ï¼ˆ5-fold CVï¼‰ ===
    kfold = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = cross_val_score(gp, X_full, y_full, cv=kfold,
                                 scoring='neg_mean_squared_error')
    cv_rmse = np.sqrt(-cv_scores)
    
    print("=== äº¤å·®æ¤œè¨¼çµæœ ===")
    print(f"5-fold CV RMSE: {cv_rmse.mean():.2f} Â± {cv_rmse.std():.2f}%")
    
    # === 2. ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨äºˆæ¸¬ ===
    gp.fit(X_full, y_full)
    y_pred, sigma = gp.predict(X_full, return_std=True)
    
    # === 3. æ€§èƒ½æŒ‡æ¨™ ===
    rmse = np.sqrt(mean_squared_error(y_full, y_pred))
    r2 = r2_score(y_full, y_pred)
    
    print(f"\n=== è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ€§èƒ½ ===")
    print(f"RMSE: {rmse:.2f}%")
    print(f"RÂ²ã‚¹ã‚³ã‚¢: {r2:.3f}")
    
    # === 4. æ®‹å·®åˆ†æ ===
    residuals = y_full - y_pred
    standardized_residuals = residuals / sigma
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # (a) äºˆæ¸¬ vs å®Ÿæ¸¬
    axes[0, 0].scatter(y_full, y_pred, alpha=0.6, s=80)
    axes[0, 0].plot([y_full.min(), y_full.max()],
                    [y_full.min(), y_full.max()],
                    'r--', linewidth=2, label='ç†æƒ³ç›´ç·š')
    axes[0, 0].set_xlabel('å®Ÿæ¸¬å€¤ [%]')
    axes[0, 0].set_ylabel('äºˆæ¸¬å€¤ [%]')
    axes[0, 0].set_title(f'äºˆæ¸¬ç²¾åº¦ (RÂ²={r2:.3f})')
    axes[0, 0].legend()
    axes[0, 0].grid(alpha=0.3)
    
    # (b) æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
    axes[0, 1].scatter(y_pred, residuals, alpha=0.6, s=80)
    axes[0, 1].axhline(0, color='r', linestyle='--', linewidth=2)
    axes[0, 1].axhline(1.96*residuals.std(), color='orange',
                       linestyle=':', label='Â±1.96Ïƒ')
    axes[0, 1].axhline(-1.96*residuals.std(), color='orange', linestyle=':')
    axes[0, 1].set_xlabel('äºˆæ¸¬å€¤ [%]')
    axes[0, 1].set_ylabel('æ®‹å·® [%]')
    axes[0, 1].set_title('æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆï¼ˆç­‰åˆ†æ•£æ€§ãƒã‚§ãƒƒã‚¯ï¼‰')
    axes[0, 1].legend()
    axes[0, 1].grid(alpha=0.3)
    
    # (c) æ¨™æº–åŒ–æ®‹å·®ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    axes[1, 0].hist(standardized_residuals, bins=15, edgecolor='black', alpha=0.7)
    axes[1, 0].axvline(0, color='r', linestyle='--', linewidth=2)
    x_norm = np.linspace(-3, 3, 100)
    y_norm = 30 / (2*np.pi)**0.5 * np.exp(-x_norm**2 / 2)  # æ­£è¦åˆ†å¸ƒ
    axes[1, 0].plot(x_norm, y_norm, 'r-', linewidth=2, label='æ¨™æº–æ­£è¦åˆ†å¸ƒ')
    axes[1, 0].set_xlabel('æ¨™æº–åŒ–æ®‹å·®')
    axes[1, 0].set_ylabel('é »åº¦')
    axes[1, 0].set_title('æ®‹å·®ã®æ­£è¦æ€§ãƒã‚§ãƒƒã‚¯')
    axes[1, 0].legend()
    axes[1, 0].grid(alpha=0.3)
    
    # (d) äºˆæ¸¬ä¸ç¢ºå®Ÿæ€§ã®æ ¡æ­£
    X_test_dense = np.linspace(50, 150, 100).reshape(-1, 1)
    y_pred_dense, sigma_dense = gp.predict(X_test_dense, return_std=True)
    
    axes[1, 1].plot(X_test_dense, y_pred_dense, 'b-', linewidth=2, label='GPäºˆæ¸¬')
    axes[1, 1].fill_between(X_test_dense.ravel(),
                            y_pred_dense - 1.96*sigma_dense,
                            y_pred_dense + 1.96*sigma_dense,
                            alpha=0.3, label='95%ä¿¡é ¼åŒºé–“')
    axes[1, 1].scatter(X_full, y_full, c='red', s=60,
                      edgecolors='k', label='å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿')
    axes[1, 1].set_xlabel('æ¸©åº¦ [Â°C]')
    axes[1, 1].set_ylabel('æ´»æ€§ [%]')
    axes[1, 1].set_title('ä¸ç¢ºå®Ÿæ€§ã®æ ¡æ­£')
    axes[1, 1].legend()
    axes[1, 1].grid(alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # === 5. ã‚«ãƒãƒ¬ãƒƒã‚¸ç¢ºç‡ï¼ˆæ ¡æ­£ãƒã‚§ãƒƒã‚¯ï¼‰ ===
    # 95%ä¿¡é ¼åŒºé–“ã«å®Ÿæ¸¬å€¤ã®ä½•%ãŒå…¥ã‚‹ã‹
    in_interval = np.abs(residuals) <= 1.96 * sigma
    coverage = np.mean(in_interval) * 100
    
    print(f"\n=== ä¸ç¢ºå®Ÿæ€§ã®æ ¡æ­£ ===")
    print(f"95%ä¿¡é ¼åŒºé–“ã®ã‚«ãƒãƒ¬ãƒƒã‚¸: {coverage:.1f}%")
    print(f"æœŸå¾…å€¤: 95.0%")
    print(f"åˆ¤å®š: {'âœ… è‰¯å¥½' if 90 <= coverage <= 100 else 'âš ï¸ è¦èª¿æ•´'}")
    
    # æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
    # === äº¤å·®æ¤œè¨¼çµæœ ===
    # 5-fold CV RMSE: 3.42 Â± 0.87%
    #
    # === è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ€§èƒ½ ===
    # RMSE: 2.98%
    # RÂ²ã‚¹ã‚³ã‚¢: 0.923
    #
    # === ä¸ç¢ºå®Ÿæ€§ã®æ ¡æ­£ ===
    # 95%ä¿¡é ¼åŒºé–“ã®ã‚«ãƒãƒ¬ãƒƒã‚¸: 93.3%
    # æœŸå¾…å€¤: 95.0%
    # åˆ¤å®š: âœ… è‰¯å¥½

ğŸ”¬ ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

  1. **äº¤å·®æ¤œè¨¼** : æ±åŒ–æ€§èƒ½ã®ç¢ºèªï¼ˆCV RMSEãŒè¨“ç·´RMSEã¨è¿‘ã„ã‹ï¼‰
  2. **æ®‹å·®ã®ç­‰åˆ†æ•£æ€§** : æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆãŒæ°´å¹³ã®ãƒ©ãƒ³ãƒ€ãƒ æ•£å¸ƒã‹
  3. **æ®‹å·®ã®æ­£è¦æ€§** : ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ãŒæ­£è¦åˆ†å¸ƒã«è¿‘ã„ã‹
  4. **ä¸ç¢ºå®Ÿæ€§ã®æ ¡æ­£** : 95%åŒºé–“ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒ90-100%ã‹

## æœ¬ç« ã®ã¾ã¨ã‚

### é‡è¦ãªå­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ

ãƒˆãƒ”ãƒƒã‚¯ | ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ | å®Ÿè·µã§ã®ä½¿ã„æ–¹  
---|---|---  
**ã‚¬ã‚¦ã‚¹éç¨‹ã®å®šç¾©** | å¹³å‡é–¢æ•°ã¨å…±åˆ†æ•£é–¢æ•°ã§å®Œå…¨ã«å®šç¾© | scikit-learnã§RBFã‚«ãƒ¼ãƒãƒ«ã‹ã‚‰é–‹å§‹  
**ã‚«ãƒ¼ãƒãƒ«é¸æŠ** | RBFï¼ˆã‚¹ãƒ ãƒ¼ã‚ºï¼‰ã€MatÃ©rnï¼ˆä¸­é–“ï¼‰ã€RQï¼ˆå¤šã‚¹ã‚±ãƒ¼ãƒ«ï¼‰ | å¯¾æ•°å‘¨è¾ºå°¤åº¦ã§æ€§èƒ½æ¯”è¼ƒ  
**ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–** | MLEã§è‡ªå‹•æœ€é©åŒ–ï¼ˆL-BFGS-Bæ³•ï¼‰ | n_restarts_optimizer=10-15æ¨å¥¨  
**ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–** | 95%ä¿¡é ¼åŒºé–“ = Î¼ Â± 1.96Ïƒ | æœªæ¢ç´¢é ˜åŸŸã§ä¸ç¢ºå®Ÿæ€§å¤§â†’æ¢ç´¢ä¿ƒé€²  
**ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼** | äº¤å·®æ¤œè¨¼ã€æ®‹å·®åˆ†æã€ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒã‚§ãƒƒã‚¯ | RÂ²>0.9ã€ã‚«ãƒãƒ¬ãƒƒã‚¸90-100%ãŒç›®æ¨™  
  
### å®Ÿè£…æ™‚ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

  1. **ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–** : å…¥åŠ›ã‚’[0, 1]ã¾ãŸã¯æ¨™æº–åŒ–ï¼ˆâ„“ã®æœ€é©åŒ–ã‚’å®‰å®šåŒ–ï¼‰
  2. **ã‚«ãƒ¼ãƒãƒ«é¸æŠ** : æœ€åˆã¯RBFã§è©¦ã—ã€MatÃ©rnã§é ‘å¥æ€§å‘ä¸Š
  3. **ãƒã‚¤ã‚ºæ¨å®š** : `alpha=1e-2` ã§è¦³æ¸¬èª¤å·®ã‚’è€ƒæ…®
  4. **å¤šç‚¹ã‚¹ã‚¿ãƒ¼ãƒˆ** : `n_restarts_optimizer=10-15` ã§å±€æ‰€æœ€é©ã‚’å›é¿
  5. **æ•°å€¤å®‰å®šæ€§** : ã‚³ãƒ¬ã‚¹ã‚­ãƒ¼åˆ†è§£ã‚’ä½¿ç”¨ï¼ˆè¡Œåˆ—é€†è¡Œåˆ—ã®ç›´æ¥è¨ˆç®—ã‚’é¿ã‘ã‚‹ï¼‰

## å­¦ç¿’ç›®æ¨™ã®ç¢ºèª

ã“ã®ç« ã‚’å®Œäº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

### åŸºæœ¬ç†è§£

  * âœ… ã‚¬ã‚¦ã‚¹éç¨‹ãŒå¹³å‡é–¢æ•°ã¨ã‚«ãƒ¼ãƒãƒ«ã§å®šç¾©ã•ã‚Œã‚‹ã“ã¨ã‚’èª¬æ˜ã§ãã‚‹
  * âœ… äºˆæ¸¬åˆ†å¸ƒã‹ã‚‰å¹³å‡ã¨ä¸ç¢ºå®Ÿæ€§ã‚’è¨ˆç®—ã§ãã‚‹
  * âœ… RBFã€MatÃ©rnã€Rational Quadraticã‚«ãƒ¼ãƒãƒ«ã®ç‰¹æ€§ã‚’èª¬æ˜ã§ãã‚‹

### å®Ÿè·µã‚¹ã‚­ãƒ«

  * âœ… scikit-learnã§ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã§ãã‚‹
  * âœ… ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’MLEã§æœ€é©åŒ–ã§ãã‚‹
  * âœ… äºˆæ¸¬ä¸ç¢ºå®Ÿæ€§ã‚’å¯è¦–åŒ–ã§ãã‚‹ï¼ˆä¿¡é ¼åŒºé–“ãƒ—ãƒ­ãƒƒãƒˆï¼‰
  * âœ… äº¤å·®æ¤œè¨¼ã¨æ®‹å·®åˆ†æã§ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œè¨¼ã§ãã‚‹

### å¿œç”¨åŠ›

  * âœ… åŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã«é©åˆ‡ãªã‚«ãƒ¼ãƒãƒ«ã‚’é¸æŠã§ãã‚‹
  * âœ… å¤šæ¬¡å…ƒå…¥åŠ›ï¼ˆæ¸©åº¦ãƒ»åœ§åŠ›ãƒ»æ¿ƒåº¦ï¼‰ã®GPãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã§ãã‚‹
  * âœ… ãƒ¢ãƒ‡ãƒ«ã®ä¿¡é ¼æ€§ã‚’å®šé‡çš„ã«è©•ä¾¡ã§ãã‚‹

## æ¼”ç¿’å•é¡Œ

### Easyï¼ˆåŸºç¤ç¢ºèªï¼‰

**Q1** : ã‚¬ã‚¦ã‚¹éç¨‹ã®äºˆæ¸¬åˆ†å¸ƒ `f(x*) ~ N(Î¼, ÏƒÂ²)` ã«ãŠã„ã¦ã€95%ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—ã™ã‚‹å¼ã¯ã©ã‚Œã§ã™ã‹ï¼Ÿ

è§£ç­”ã‚’è¦‹ã‚‹

**æ­£è§£** : [Î¼ - 1.96Ïƒ, Î¼ + 1.96Ïƒ]

**è§£èª¬** : æ­£è¦åˆ†å¸ƒã®95%åŒºé–“ã¯å¹³å‡ Â± 1.96Ã—æ¨™æº–åå·®ã§ã™ã€‚99%åŒºé–“ã¯ Â± 2.58Ïƒã€68%åŒºé–“ã¯ Â± 1Ïƒã§ã™ã€‚

**Q2** : RBFã‚«ãƒ¼ãƒãƒ«ã®é•·ã•ã‚¹ã‚±ãƒ¼ãƒ« `â„“` ã‚’å¤§ããã™ã‚‹ã¨ã€äºˆæ¸¬æ›²ç·šã¯ã©ã†å¤‰åŒ–ã—ã¾ã™ã‹ï¼Ÿ

è§£ç­”ã‚’è¦‹ã‚‹

**æ­£è§£** : ã‚ˆã‚Šã‚¹ãƒ ãƒ¼ã‚ºï¼ˆæ»‘ã‚‰ã‹ï¼‰ã«ãªã‚Šã¾ã™

**è§£èª¬** : â„“ã¯ç›¸é–¢è·é›¢ã‚’è¡¨ã—ã€å¤§ãã„ã»ã©é ãã®ç‚¹åŒå£«ã‚‚å¼·ãç›¸é–¢ã—ã¾ã™ã€‚â„“â†’âˆã§å®šæ•°é–¢æ•°ã«åæŸã—ã¾ã™ã€‚

### Mediumï¼ˆå¿œç”¨ï¼‰

**Q3** : 5ã¤ã®å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¯¾æ•°å‘¨è¾ºå°¤åº¦ãŒ -10.5 ã®GPãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸã€‚ã“ã‚Œã¯è‰¯ã„ãƒ¢ãƒ‡ãƒ«ã§ã™ã‹ï¼Ÿ

è§£ç­”ã‚’è¦‹ã‚‹

**å›ç­”** : **ç›¸å¯¾æ¯”è¼ƒ** ãŒå¿…è¦ã§ã™ã€‚

**è§£èª¬** : å¯¾æ•°å‘¨è¾ºå°¤åº¦ã¯çµ¶å¯¾å€¤ã§ã¯ãªãã€ç•°ãªã‚‹ã‚«ãƒ¼ãƒãƒ«é–“ã®ç›¸å¯¾çš„ãªæ¯”è¼ƒã«ä½¿ã„ã¾ã™ã€‚ä¾‹ãˆã°ã€RBFã§-10.5ã€MatÃ©rnã§-12.3ãªã‚‰ã€RBFãŒå„ªã‚Œã¦ã„ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿æ•°nãŒå¢—ãˆã‚‹ã¨å°¤åº¦ã®çµ¶å¯¾å€¤ã‚‚å¢—åŠ ã™ã‚‹ãŸã‚ã€ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–“ã§ã¯æ¯”è¼ƒã§ãã¾ã›ã‚“ã€‚

**Q4** : äº¤å·®æ¤œè¨¼RMSEãŒ3.5%ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿RMSEãŒ1.8%ã§ã—ãŸã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã«å•é¡Œã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ

è§£ç­”ã‚’è¦‹ã‚‹

**å›ç­”** : **éå­¦ç¿’ã®å…†å€™** ãŒã‚ã‚Šã¾ã™ã€‚

**è§£èª¬** : CVã‚¹ã‚³ã‚¢ãŒè¨“ç·´ã‚¹ã‚³ã‚¢ã‚ˆã‚Šå¤§å¹…ã«æ‚ªã„å ´åˆã€éå­¦ç¿’ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚å¯¾ç­–ï¼š(1) ãƒã‚¤ã‚ºé … `alpha` ã‚’å¢—ã‚„ã™ã€(2) ã‚ˆã‚Šå˜ç´”ãªã‚«ãƒ¼ãƒãƒ«ã‚’ä½¿ã†ã€(3) ãƒ‡ãƒ¼ã‚¿æ•°ã‚’å¢—ã‚„ã™ã€‚

### Hardï¼ˆç™ºå±•ï¼‰

**Q5** : æ¸©åº¦ãƒ»åœ§åŠ›ãƒ»æ¿ƒåº¦ã®3æ¬¡å…ƒå…¥åŠ›ã§GPãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹éš›ã€å„æ¬¡å…ƒã§ç•°ãªã‚‹é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ã‚’ä½¿ã†ã¹ãç†ç”±ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

è§£ç­”ã‚’è¦‹ã‚‹

**å›ç­”** : **Automatic Relevance Determination (ARD)** ã«ã‚ˆã‚Šã€å„å…¥åŠ›æ¬¡å…ƒã®é‡è¦åº¦ã‚’è‡ªå‹•å­¦ç¿’ã§ãã¾ã™ã€‚

**å®Ÿè£…ä¾‹** :
    
    
    kernel = C(1.0) * RBF(length_scale=[10.0, 1.0, 5.0],
                       length_scale_bounds=(1e-2, 1e3))
    # [æ¸©åº¦10Â°C, åœ§åŠ›1.0MPa, æ¿ƒåº¦5%ã®åˆæœŸé•·ã•ã‚¹ã‚±ãƒ¼ãƒ«]

**è§£é‡ˆ** : æœ€é©åŒ–å¾Œã€åœ§åŠ›ã®é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ãŒ0.5ã¨å°ã•ã‘ã‚Œã°ã€åœ§åŠ›å¤‰åŒ–ã«åç‡ãŒæ•æ„Ÿã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚æ¸©åº¦ãŒ50ã¨å¤§ãã‘ã‚Œã°ã€æ¸©åº¦ä¾å­˜æ€§ã¯ä½ã„ã§ã™ã€‚

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«ã§ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚æ¬¡ã®ç¬¬3ç« ã§ã¯ã€ã“ã®ä¸ç¢ºå®Ÿæ€§ã‚’æ´»ç”¨ã—ã¦**æ¬¡ã®å®Ÿé¨“ç‚¹ã‚’è³¢ãé¸ã¶ç²å¾—é–¢æ•°** ã‚’å­¦ã³ã¾ã™ã€‚

**ç¬¬3ç« ã§å­¦ã¶ã“ã¨ï¼š**

  * Expected Improvement (EI): æœ€ã‚‚æœ‰æœ›ãªç‚¹ã‚’é¸ã¶
  * Upper Confidence Bound (UCB): æ¢ç´¢-æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹èª¿æ•´
  * Probability of Improvement (PI): ã‚·ãƒ³ãƒ—ãƒ«ãªæ”¹å–„ç¢ºç‡
  * ãƒãƒƒãƒãƒ™ã‚¤ã‚ºæœ€é©åŒ–: ä¸¦åˆ—å®Ÿé¨“ã®è¨­è¨ˆ

[â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡](<./index.html>) [ç¬¬3ç« ï¼šç²å¾—é–¢æ•° â†’](<./chapter-3.html>)

## å‚è€ƒæ–‡çŒ®

  1. Rasmussen, C. E., & Williams, C. K. I. (2006). _Gaussian Processes for Machine Learning_. MIT Press.
  2. Shahriari, B., et al. (2016). "Taking the Human Out of the Loop: A Review of Bayesian Optimization." _Proceedings of the IEEE_ , 104(1), 148-175.
  3. Pedregosa, F., et al. (2011). "Scikit-learn: Machine Learning in Python." _Journal of Machine Learning Research_ , 12, 2825-2830.
  4. MatÃ©rn, B. (1960). _Spatial Variation_. Springer.
