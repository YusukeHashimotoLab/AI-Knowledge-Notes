<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬4ç«  Advanced Process Control (APC) | åŠå°ä½“è£½é€ AI | ãƒãƒ†ãƒªã‚¢ãƒ«ã‚ºã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹å…¥é–€</title>
    <meta name="description" content="åŠå°ä½“è£½é€ ã«ãŠã‘ã‚‹Advanced Process Control (APC) ã‚’AIã§å®Ÿç¾ã—ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬åˆ¶å¾¡ (MPC)ã€é©å¿œåˆ¶å¾¡ã€ãƒ‡ã‚¸ã‚¿ãƒ«ãƒ„ã‚¤ãƒ³ã€å¼·åŒ–å­¦ç¿’åˆ¶å¾¡ã‚’Pythonã§å®Ÿè£…ã—ã€ãƒ—ãƒ­ã‚»ã‚¹ã®å®‰å®šæ€§ã¨æœ€é©æ€§ã‚’ä¸¡ç«‹ã—ã¾ã™ã€‚">
    <!-- CSS removed: not available in JP -->
    <!-- CSS removed: not available in JP -->
    <!-- CSS removed: not available in JP -->
    <!-- CSS removed: not available in JP -->
    <!-- CSS removed: not available in JP -->
    <!-- CSS removed: not available in JP -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
    <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/PI/semiconductor-manufacturing-ai/chapter-4.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header class="site-header">
        <div class="container">
            <div class="header-content">
                <h1 class="site-title"><a href="../index.html">æ©‹æœ¬ç ”ç©¶å®¤</a></h1>
                <nav class="main-nav">
                    <ul>
                        <li><a href="../index.html">ãƒ›ãƒ¼ãƒ </a></li>
                        <li><!-- Lab page: ç ”ç©¶å†…å®¹ --><span class="disabled">ç ”ç©¶å†…å®¹</span></li>
                        <li><!-- Lab page: ç ”ç©¶æ¥­ç¸¾ --><span class="disabled">ç ”ç©¶æ¥­ç¸¾</span></li>
                        <li><!-- Link disabled: çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ (æº–å‚™ä¸­) --><span class="coming-soon">çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ (æº–å‚™ä¸­)</span></li>
                        <li><!-- Lab page: ãƒ‹ãƒ¥ãƒ¼ã‚¹ --><span class="disabled">ãƒ‹ãƒ¥ãƒ¼ã‚¹</span></li>
                        <li><!-- Lab page: ãƒ¡ãƒ³ãƒãƒ¼ --><span class="disabled">ãƒ¡ãƒ³ãƒãƒ¼</span></li>
                        <li><!-- Lab page: ãŠå•ã„åˆã‚ã› --><span class="disabled">ãŠå•ã„åˆã‚ã›</span></li>
                        <li><a href="../../../en/index.html">English</a></li>
                    </ul>
                </nav>
            </div>
        </div>
    </header>

    <main class="article-content">
        <article class="container">
            <div class="breadcrumb">
                <a href="../index.html">ãƒ›ãƒ¼ãƒ </a> &gt;
                <!-- Link disabled: çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ (æº–å‚™ä¸­) --><span class="coming-soon">çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ (æº–å‚™ä¸­)</span> &gt;
                <a href="../../PI/">ãƒ—ãƒ­ã‚»ã‚¹ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹</a> &gt;
                <a href="../../PI/semiconductor-manufacturing-ai/">åŠå°ä½“è£½é€ AI</a> &gt;
                ç¬¬4ç« 
            </div>

            <header class="article-header">
                <h1 class="gradient-text" style="background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent;">ç¬¬4ç«  Advanced Process Control (APC)</h1>
                <p class="article-meta">åŠå°ä½“è£½é€ AI - ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬åˆ¶å¾¡ãƒ»é©å¿œåˆ¶å¾¡ãƒ»ãƒ‡ã‚¸ã‚¿ãƒ«ãƒ„ã‚¤ãƒ³ãƒ»å¼·åŒ–å­¦ç¿’</p>
            </header>

            <section class="introduction">
                <h2>å­¦ç¿’ç›®æ¨™</h2>
                <ul>
                    <li>ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬åˆ¶å¾¡ (MPC) ã®ç†è«–ã¨å®Ÿè£…æ–¹æ³•ã‚’ç¿’å¾—ã™ã‚‹</li>
                    <li>é©å¿œåˆ¶å¾¡ã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹å¤‰å‹•ã¸ã®è‡ªå‹•å¯¾å¿œæ‰‹æ³•ã‚’ç†è§£ã™ã‚‹</li>
                    <li>ãƒ‡ã‚¸ã‚¿ãƒ«ãƒ„ã‚¤ãƒ³ã‚’ç”¨ã„ãŸãƒ—ãƒ­ã‚»ã‚¹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹</li>
                    <li>å¼·åŒ–å­¦ç¿’ (DQN, PPO) ã«ã‚ˆã‚‹åˆ¶å¾¡å™¨ã®å­¦ç¿’æ–¹æ³•ã‚’å­¦ã¶</li>
                    <li>APCã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ã¨ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ¶å¾¡ã®å®Ÿè·µæ‰‹æ³•ã‚’ç¿’å¾—ã™ã‚‹</li>
                </ul>
            </section>

            <section>
                <h2>4.1 Advanced Process Control (APC) ã®æ¦‚è¦</h2>

                <h3>4.1.1 APCã®å½¹å‰²ã¨é‡è¦æ€§</h3>
                <p>åŠå°ä½“è£½é€ ã§ã¯ã€è£…ç½®ã®çµŒå¹´åŠ£åŒ–ã€ç’°å¢ƒå¤‰å‹•ã€åŸææ–™ã®ãƒ­ãƒƒãƒˆå¤‰å‹•ãªã©ã€æ§˜ã€…ãªå¤–ä¹±ãŒãƒ—ãƒ­ã‚»ã‚¹ã«å½±éŸ¿ã—ã¾ã™ã€‚APCã¯ã“ã‚Œã‚‰ã®å¤–ä¹±ã‚’è£œå„Ÿã—ã€ç›®æ¨™å€¤ã‚’å®‰å®šç¶­æŒã™ã‚‹é«˜åº¦ãªåˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ã§ã™ï¼š</p>
                <ul>
                    <li><strong>å¤šå¤‰æ•°åˆ¶å¾¡</strong>: è¤‡æ•°ã®å…¥åŠ›ãƒ»å‡ºåŠ›ã‚’åŒæ™‚åˆ¶å¾¡</li>
                    <li><strong>äºˆæ¸¬åˆ¶å¾¡</strong>: ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒ«ã§æœªæ¥ã‚’äºˆæ¸¬ã—æœ€é©åˆ¶å¾¡</li>
                    <li><strong>é©å¿œåˆ¶å¾¡</strong>: ãƒ—ãƒ­ã‚»ã‚¹ç‰¹æ€§å¤‰åŒ–ã«è‡ªå‹•å¯¾å¿œ</li>
                    <li><strong>åˆ¶ç´„æ¡ä»¶</strong>: å®‰å…¨ç¯„å›²ãƒ»æ€§èƒ½ç¯„å›²ã‚’å³å®ˆ</li>
                </ul>

                <h3>4.1.2 å¾“æ¥PIDåˆ¶å¾¡ã®é™ç•Œ</h3>
                <div style="background: #f8f9fa; padding: 1rem; border-radius: 8px; margin: 1rem 0;">
                    <p><strong>å˜å¤‰æ•°åˆ¶å¾¡</strong>: å¤‰æ•°é–“ã®ç›¸äº’ä½œç”¨ã‚’è€ƒæ…®ã§ããªã„</p>
                    <p><strong>åå¿œçš„åˆ¶å¾¡</strong>: èª¤å·®ãŒç™ºç”Ÿã—ã¦ã‹ã‚‰ä¿®æ­£ï¼ˆå¾Œæ‰‹ã«å›ã‚‹ï¼‰</p>
                    <p><strong>åˆ¶ç´„å‡¦ç†ã®å›°é›£</strong>: ç‰©ç†çš„åˆ¶ç´„ãƒ»æ€§èƒ½åˆ¶ç´„ã®æ˜ç¤ºçš„æ‰±ã„ãŒé›£ã—ã„</p>
                    <p><strong>æœ€é©æ€§ã®æ¬ å¦‚</strong>: ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ–ç­‰ã®æœ€é©åŒ–ç›®æ¨™ã‚’çµ„ã¿è¾¼ã‚ãªã„</p>
                </div>

                <h3>4.1.3 AIãƒ™ãƒ¼ã‚¹APCã®å„ªä½æ€§</h3>
                <ul>
                    <li><strong>å¤šç›®çš„æœ€é©åŒ–</strong>: å“è³ªãƒ»ã‚³ã‚¹ãƒˆãƒ»ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’åŒæ™‚æœ€é©åŒ–</li>
                    <li><strong>å­¦ç¿’èƒ½åŠ›</strong>: éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åˆ¶å¾¡å‰‡ã‚’è‡ªå‹•å­¦ç¿’</li>
                    <li><strong>ãƒ­ãƒã‚¹ãƒˆæ€§</strong>: ãƒ¢ãƒ‡ãƒ«èª¤å·®ãƒ»å¤–ä¹±ã«å¼·ã„</li>
                    <li><strong>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§</strong>: GPUæ´»ç”¨ã§é«˜é€Ÿè¨ˆç®—</li>
                </ul>
            </section>

            <section>
                <h2>4.2 ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬åˆ¶å¾¡ (MPC: Model Predictive Control)</h2>

                <h3>4.2.1 MPCã®åŸç†</h3>
                <p>MPCã¯ã€ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒ«ã§æœªæ¥ã®æŒ™å‹•ã‚’äºˆæ¸¬ã—ã€æ€§èƒ½æŒ‡æ¨™ã‚’æœ€å°åŒ–ã™ã‚‹åˆ¶å¾¡å…¥åŠ›ç³»åˆ—ã‚’è¨ˆç®—ã—ã¾ã™ï¼š</p>

                <div style="background: #f8f9fa; padding: 1rem; border-radius: 8px; margin: 1.5rem 0;">
                    <p><strong>äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«</strong></p>
                    <p>$$x_{k+1} = f(x_k, u_k)$$</p>
                    <p>\(x_k\): çŠ¶æ…‹ã€\(u_k\): åˆ¶å¾¡å…¥åŠ›</p>

                    <p><strong>ã‚³ã‚¹ãƒˆé–¢æ•° (äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ N)</strong></p>
                    <p>$$J = \sum_{i=0}^{N-1} \left[ \|y_{k+i} - r_{k+i}\|_Q^2 + \|u_{k+i}\|_R^2 \right]$$</p>
                    <p>\(y\): å‡ºåŠ›ã€\(r\): ç›®æ¨™å€¤ã€\(Q, R\): é‡ã¿è¡Œåˆ—</p>

                    <p><strong>åˆ¶ç´„æ¡ä»¶</strong></p>
                    <p>$$u_{\min} \leq u_k \leq u_{\max}$$</p>
                    <p>$$y_{\min} \leq y_k \leq y_{\max}$$</p>

                    <p><strong>æœ€é©åŒ–å•é¡Œ</strong></p>
                    <p>å„æ™‚åˆ»ã§ä¸Šè¨˜ã®ã‚³ã‚¹ãƒˆé–¢æ•°ã‚’æœ€å°åŒ–ã™ã‚‹åˆ¶å¾¡å…¥åŠ›ç³»åˆ— \(\{u_k, u_{k+1}, \ldots, u_{k+N-1}\}\) ã‚’æ±‚ã‚ã€æœ€åˆã® \(u_k\) ã®ã¿ã‚’é©ç”¨ï¼ˆReceding Horizonï¼‰</p>
                </div>

                <h3>4.2.2 CVDãƒ—ãƒ­ã‚»ã‚¹MPCå®Ÿè£…</h3>
                <p>Chemical Vapor Deposition (CVD) ã«ãŠã‘ã‚‹è†œåšåˆ¶å¾¡ã‚’MPCã§å®Ÿç¾ã—ã¾ã™ï¼š</p>

<pre><code class="language-python">import numpy as np
from scipy.optimize import minimize
import matplotlib.pyplot as plt

class ModelPredictiveController:
    """
    ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬åˆ¶å¾¡ (MPC) for CVDãƒ—ãƒ­ã‚»ã‚¹

    åˆ¶å¾¡ç›®æ¨™: è†œåšã‚’ç›®æ¨™å€¤ã«è¿½å¾“
    åˆ¶å¾¡å¤‰æ•°: ã‚¬ã‚¹æµé‡ã€RFãƒ‘ãƒ¯ãƒ¼ã€åœ§åŠ›
    çŠ¶æ…‹å¤‰æ•°: è†œåšã€æˆè†œé€Ÿåº¦
    """

    def __init__(self, prediction_horizon=10, control_horizon=5, dt=1.0):
        """
        Parameters:
        -----------
        prediction_horizon : int
            äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ N
        control_horizon : int
            åˆ¶å¾¡ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ M (M â‰¤ N)
        dt : float
            ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚é–“ (ç§’)
        """
        self.N = prediction_horizon
        self.M = control_horizon
        self.dt = dt

        # çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        # x = [è†œåš (nm), æˆè†œé€Ÿåº¦ (nm/s)]
        # u = [ã‚¬ã‚¹æµé‡ (sccm), RFãƒ‘ãƒ¯ãƒ¼ (W), åœ§åŠ› (mTorr)]
        self.A = np.array([
            [1, self.dt],
            [0, 0.95]
        ])

        self.B = np.array([
            [0, 0, 0],
            [0.01, 0.02, -0.005]
        ])

        # å‡ºåŠ›è¡Œåˆ— (è†œåšã®ã¿è¦³æ¸¬)
        self.C = np.array([[1, 0]])

        # é‡ã¿è¡Œåˆ—
        self.Q = np.diag([100, 1])  # çŠ¶æ…‹ã‚³ã‚¹ãƒˆ
        self.R = np.diag([0.1, 0.1, 0.1])  # åˆ¶å¾¡å…¥åŠ›ã‚³ã‚¹ãƒˆ

        # åˆ¶ç´„æ¡ä»¶
        self.u_min = np.array([50, 100, 10])
        self.u_max = np.array([200, 400, 100])
        self.y_min = 0
        self.y_max = 200  # è†œåšä¸Šé™ (nm)

    def predict(self, x0, u_sequence):
        """
        çŠ¶æ…‹äºˆæ¸¬

        Parameters:
        -----------
        x0 : ndarray
            åˆæœŸçŠ¶æ…‹ (2,)
        u_sequence : ndarray
            åˆ¶å¾¡å…¥åŠ›ç³»åˆ— (M, 3)

        Returns:
        --------
        x_pred : ndarray
            äºˆæ¸¬çŠ¶æ…‹è»Œé“ (N+1, 2)
        y_pred : ndarray
            äºˆæ¸¬å‡ºåŠ›è»Œé“ (N+1,)
        """
        x_pred = np.zeros((self.N + 1, 2))
        y_pred = np.zeros(self.N + 1)

        x_pred[0] = x0
        y_pred[0] = self.C @ x0

        for k in range(self.N):
            if k < self.M:
                u_k = u_sequence[k]
            else:
                # åˆ¶å¾¡ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ä»¥é™ã¯æœ€å¾Œã®å…¥åŠ›ã‚’ä¿æŒ
                u_k = u_sequence[self.M - 1]

            # çŠ¶æ…‹é·ç§»
            x_pred[k + 1] = self.A @ x_pred[k] + self.B @ u_k
            y_pred[k + 1] = self.C @ x_pred[k + 1]

        return x_pred, y_pred

    def cost_function(self, u_flat, x0, r_sequence):
        """
        ã‚³ã‚¹ãƒˆé–¢æ•°

        Parameters:
        -----------
        u_flat : ndarray
            å¹³å¦åŒ–ã•ã‚ŒãŸåˆ¶å¾¡å…¥åŠ›ç³»åˆ— (M*3,)
        x0 : ndarray
            ç¾åœ¨çŠ¶æ…‹
        r_sequence : ndarray
            ç›®æ¨™å€¤ç³»åˆ— (N+1,)
        """
        # åˆ¶å¾¡å…¥åŠ›ã‚’å¾©å…ƒ
        u_sequence = u_flat.reshape((self.M, 3))

        # äºˆæ¸¬
        x_pred, y_pred = self.predict(x0, u_sequence)

        # ã‚³ã‚¹ãƒˆè¨ˆç®—
        cost = 0.0

        # ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°èª¤å·®
        for k in range(self.N + 1):
            error = y_pred[k] - r_sequence[k]
            cost += error ** 2 * self.Q[0, 0]

        # åˆ¶å¾¡å…¥åŠ›ã‚³ã‚¹ãƒˆ
        for k in range(self.M):
            cost += u_sequence[k] @ self.R @ u_sequence[k]

        # åˆ¶å¾¡å…¥åŠ›å¤‰åŒ–ã‚³ã‚¹ãƒˆ (æ»‘ã‚‰ã‹ãªåˆ¶å¾¡)
        for k in range(1, self.M):
            du = u_sequence[k] - u_sequence[k - 1]
            cost += 0.1 * (du @ du)

        return cost

    def solve_mpc(self, x0, r_sequence, u_prev):
        """
        MPCæœ€é©åŒ–å•é¡Œã‚’è§£ã

        Parameters:
        -----------
        x0 : ndarray
            ç¾åœ¨çŠ¶æ…‹
        r_sequence : ndarray
            ç›®æ¨™å€¤ç³»åˆ— (N+1,)
        u_prev : ndarray
            å‰æ™‚åˆ»ã®åˆ¶å¾¡å…¥åŠ› (3,)

        Returns:
        --------
        u_opt : ndarray
            æœ€é©åˆ¶å¾¡å…¥åŠ› (3,)
        """
        # åˆæœŸæ¨å®šå€¤ (å‰æ™‚åˆ»ã®å…¥åŠ›ã‚’ä¿æŒ)
        u0 = np.tile(u_prev, self.M)

        # åˆ¶ç´„æ¡ä»¶
        bounds = []
        for _ in range(self.M):
            for i in range(3):
                bounds.append((self.u_min[i], self.u_max[i]))

        # æœ€é©åŒ–
        result = minimize(
            fun=lambda u: self.cost_function(u, x0, r_sequence),
            x0=u0,
            method='SLSQP',
            bounds=bounds,
            options={'maxiter': 100, 'ftol': 1e-6}
        )

        # æœ€é©åˆ¶å¾¡å…¥åŠ› (æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿ä½¿ç”¨)
        u_opt_sequence = result.x.reshape((self.M, 3))
        u_opt = u_opt_sequence[0]

        return u_opt

    def simulate_closed_loop(self, x0, r_trajectory, n_steps):
        """
        é–‰ãƒ«ãƒ¼ãƒ—ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

        Parameters:
        -----------
        x0 : ndarray
            åˆæœŸçŠ¶æ…‹
        r_trajectory : ndarray
            ç›®æ¨™å€¤è»Œé“ (n_steps,)
        n_steps : int
            ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒƒãƒ—æ•°

        Returns:
        --------
        results : dict
            ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœ
        """
        # å±¥æ­´ä¿å­˜
        x_history = np.zeros((n_steps + 1, 2))
        y_history = np.zeros(n_steps + 1)
        u_history = np.zeros((n_steps, 3))
        r_history = np.zeros(n_steps + 1)

        x_history[0] = x0
        y_history[0] = self.C @ x0
        r_history[0] = r_trajectory[0]

        u_prev = np.array([125, 250, 55])  # åˆæœŸåˆ¶å¾¡å…¥åŠ›

        for k in range(n_steps):
            # ç›®æ¨™å€¤ç³»åˆ— (äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚ºãƒ³åˆ†)
            r_sequence = np.zeros(self.N + 1)
            for i in range(self.N + 1):
                if k + i < n_steps:
                    r_sequence[i] = r_trajectory[k + i]
                else:
                    r_sequence[i] = r_trajectory[-1]

            # MPCæœ€é©åŒ–
            u_opt = self.solve_mpc(x_history[k], r_sequence, u_prev)
            u_history[k] = u_opt

            # ãƒ—ãƒ­ã‚»ã‚¹ã«é©ç”¨ (å®Ÿéš›ã®ãƒ—ãƒ­ã‚»ã‚¹ã«ã¯å¤–ä¹±ãŒå«ã¾ã‚Œã‚‹)
            noise = np.random.normal(0, 0.1, 2)  # ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚º
            x_history[k + 1] = self.A @ x_history[k] + self.B @ u_opt + noise
            y_history[k + 1] = self.C @ x_history[k + 1]
            r_history[k + 1] = r_trajectory[k + 1] if k + 1 < n_steps else r_trajectory[-1]

            u_prev = u_opt

        results = {
            'x': x_history,
            'y': y_history,
            'u': u_history,
            'r': r_history,
            'time': np.arange(n_steps + 1) * self.dt
        }

        return results

    def plot_results(self, results):
        """çµæœã®å¯è¦–åŒ–"""
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))

        time = results['time']
        y = results['y']
        r = results['r']
        u = results['u']

        # è†œåšãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
        axes[0, 0].plot(time, y, 'b-', linewidth=2, label='Actual Thickness')
        axes[0, 0].plot(time, r, 'r--', linewidth=2, label='Target Thickness')
        axes[0, 0].set_xlabel('Time (s)')
        axes[0, 0].set_ylabel('Thickness (nm)')
        axes[0, 0].set_title('Film Thickness Tracking')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°èª¤å·®
        error = y - r
        axes[0, 1].plot(time, error, 'g-', linewidth=2)
        axes[0, 1].axhline(0, color='k', linestyle='--', alpha=0.3)
        axes[0, 1].set_xlabel('Time (s)')
        axes[0, 1].set_ylabel('Error (nm)')
        axes[0, 1].set_title('Tracking Error')
        axes[0, 1].grid(True, alpha=0.3)

        # åˆ¶å¾¡å…¥åŠ›
        axes[1, 0].plot(time[:-1], u[:, 0], label='Gas Flow (sccm)')
        axes[1, 0].plot(time[:-1], u[:, 1], label='RF Power (W)')
        axes[1, 0].set_xlabel('Time (s)')
        axes[1, 0].set_ylabel('Control Input')
        axes[1, 0].set_title('Control Inputs (Gas & RF)')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)

        axes[1, 1].plot(time[:-1], u[:, 2], 'purple', label='Pressure (mTorr)')
        axes[1, 1].set_xlabel('Time (s)')
        axes[1, 1].set_ylabel('Pressure (mTorr)')
        axes[1, 1].set_title('Control Input (Pressure)')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('mpc_control_results.png', dpi=300, bbox_inches='tight')
        plt.show()


# ========== ä½¿ç”¨ä¾‹ ==========
if __name__ == "__main__":
    np.random.seed(42)

    # MPCè¨­å®š
    mpc = ModelPredictiveController(
        prediction_horizon=10,
        control_horizon=5,
        dt=1.0
    )

    # åˆæœŸçŠ¶æ…‹ [è†œåš, æˆè†œé€Ÿåº¦]
    x0 = np.array([0.0, 0.0])

    # ç›®æ¨™å€¤è»Œé“ (ã‚¹ãƒ†ãƒƒãƒ—å¿œç­” + ãƒ©ãƒ³ãƒ—)
    n_steps = 100
    r_trajectory = np.zeros(n_steps)
    r_trajectory[:30] = 50  # 50nm
    r_trajectory[30:60] = 100  # 100nm
    r_trajectory[60:] = np.linspace(100, 150, 40)  # ãƒ©ãƒ³ãƒ—

    # é–‰ãƒ«ãƒ¼ãƒ—ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    print("========== MPC Closed-Loop Simulation ==========")
    results = mpc.simulate_closed_loop(x0, r_trajectory, n_steps)

    # æ€§èƒ½è©•ä¾¡
    tracking_error = results['y'] - results['r']
    mae = np.mean(np.abs(tracking_error))
    rmse = np.sqrt(np.mean(tracking_error ** 2))

    print(f"\nTracking Performance:")
    print(f"  MAE (Mean Absolute Error): {mae:.4f} nm")
    print(f"  RMSE (Root Mean Squared Error): {rmse:.4f} nm")

    # åˆ¶å¾¡å…¥åŠ›ã®çµ±è¨ˆ
    print(f"\nControl Input Statistics:")
    print(f"  Gas Flow: {np.mean(results['u'][:, 0]):.2f} Â± {np.std(results['u'][:, 0]):.2f} sccm")
    print(f"  RF Power: {np.mean(results['u'][:, 1]):.2f} Â± {np.std(results['u'][:, 1]):.2f} W")
    print(f"  Pressure: {np.mean(results['u'][:, 2]):.2f} Â± {np.std(results['u'][:, 2]):.2f} mTorr")

    # å¯è¦–åŒ–
    mpc.plot_results(results)
</code></pre>

                <h3>4.2.3 éç·šå½¢MPCã¨Neural Network Model</h3>
                <p>è¤‡é›‘ãªéç·šå½¢ãƒ—ãƒ­ã‚»ã‚¹ã«å¯¾ã—ã¦ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ï¼š</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class NeuralNetworkMPC:
    """
    Neural Network-based MPC

    è¤‡é›‘ãªéç·šå½¢ãƒ—ãƒ­ã‚»ã‚¹ã‚’NNã§ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã€
    å‹¾é…æ³•ã§MPCæœ€é©åŒ–ã‚’å®Ÿè¡Œ
    """

    def __init__(self, state_dim=2, control_dim=3, prediction_horizon=10):
        """
        Parameters:
        -----------
        state_dim : int
            çŠ¶æ…‹æ¬¡å…ƒ
        control_dim : int
            åˆ¶å¾¡å…¥åŠ›æ¬¡å…ƒ
        prediction_horizon : int
            äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚ºãƒ³
        """
        self.state_dim = state_dim
        self.control_dim = control_dim
        self.N = prediction_horizon

        # Neural Network Process Model
        self.process_model = self._build_process_model()

    def _build_process_model(self):
        """
        ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒ«NNæ§‹ç¯‰

        å…¥åŠ›: [x_k, u_k] (concat)
        å‡ºåŠ›: x_{k+1}
        """
        inputs = layers.Input(shape=(self.state_dim + self.control_dim,))

        x = layers.Dense(64, activation='relu')(inputs)
        x = layers.BatchNormalization()(x)
        x = layers.Dense(64, activation='relu')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dense(32, activation='relu')(x)

        outputs = layers.Dense(self.state_dim)(x)

        model = keras.Model(inputs, outputs, name='process_model')
        model.compile(optimizer='adam', loss='mse')

        return model

    def train_process_model(self, X_train, y_train, epochs=50):
        """
        ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´

        Parameters:
        -----------
        X_train : ndarray
            [x_k, u_k] ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ (N, state_dim + control_dim)
        y_train : ndarray
            x_{k+1} ã®ãƒ©ãƒ™ãƒ« (N, state_dim)
        """
        history = self.process_model.fit(
            X_train, y_train,
            validation_split=0.2,
            epochs=epochs,
            batch_size=32,
            verbose=0
        )

        return history

    def predict_trajectory(self, x0, u_sequence):
        """
        NN process modelã§è»Œé“äºˆæ¸¬

        Parameters:
        -----------
        x0 : ndarray
            åˆæœŸçŠ¶æ…‹ (state_dim,)
        u_sequence : ndarray
            åˆ¶å¾¡å…¥åŠ›ç³»åˆ— (N, control_dim)

        Returns:
        --------
        x_trajectory : ndarray
            äºˆæ¸¬çŠ¶æ…‹è»Œé“ (N+1, state_dim)
        """
        x_trajectory = np.zeros((self.N + 1, self.state_dim))
        x_trajectory[0] = x0

        for k in range(self.N):
            xu_k = np.concatenate([x_trajectory[k], u_sequence[k]]).reshape(1, -1)
            x_trajectory[k + 1] = self.process_model.predict(xu_k, verbose=0)[0]

        return x_trajectory

    def mpc_optimization(self, x0, r_sequence):
        """
        TensorFlowã®è‡ªå‹•å¾®åˆ†ã§MPCæœ€é©åŒ–

        Parameters:
        -----------
        x0 : ndarray
            ç¾åœ¨çŠ¶æ…‹
        r_sequence : ndarray
            ç›®æ¨™å€¤ç³»åˆ— (N+1,)

        Returns:
        --------
        u_opt : ndarray
            æœ€é©åˆ¶å¾¡å…¥åŠ›ç³»åˆ— (N, control_dim)
        """
        # åˆæœŸåˆ¶å¾¡å…¥åŠ›
        u_var = tf.Variable(
            np.random.uniform(50, 200, (self.N, self.control_dim)),
            dtype=tf.float32
        )

        optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)

        # æœ€é©åŒ–ãƒ«ãƒ¼ãƒ—
        for iteration in range(50):
            with tf.GradientTape() as tape:
                # äºˆæ¸¬
                x_pred = tf.constant(x0, dtype=tf.float32)
                cost = 0.0

                for k in range(self.N):
                    # çŠ¶æ…‹é·ç§»
                    xu_k = tf.concat([x_pred, u_var[k]], axis=0)
                    xu_k = tf.reshape(xu_k, (1, -1))
                    x_pred = self.process_model(xu_k, training=False)[0]

                    # ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°èª¤å·®ã‚³ã‚¹ãƒˆ
                    error = x_pred[0] - r_sequence[k + 1]  # è†œåšèª¤å·®
                    cost += 100 * error ** 2

                    # åˆ¶å¾¡å…¥åŠ›ã‚³ã‚¹ãƒˆ
                    cost += 0.01 * tf.reduce_sum(u_var[k] ** 2)

            # å‹¾é…è¨ˆç®—ãƒ»æ›´æ–°
            gradients = tape.gradient(cost, [u_var])
            optimizer.apply_gradients(zip(gradients, [u_var]))

        u_opt = u_var.numpy()

        return u_opt


# ========== ä½¿ç”¨ä¾‹ ==========
# ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒ«è¨“ç·´ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
n_samples = 5000

X_train = np.random.randn(n_samples, 5)  # [x1, x2, u1, u2, u3]
# ãƒ€ãƒŸãƒ¼ã®éç·šå½¢ãƒ—ãƒ­ã‚»ã‚¹
y_train = np.zeros((n_samples, 2))
y_train[:, 0] = X_train[:, 0] + 0.1 * X_train[:, 2] + 0.02 * X_train[:, 3]
y_train[:, 1] = 0.95 * X_train[:, 1] + 0.01 * X_train[:, 2]

# NN-MPCæ§‹ç¯‰ãƒ»è¨“ç·´
nn_mpc = NeuralNetworkMPC(state_dim=2, control_dim=3, prediction_horizon=10)
print("\n========== Training NN Process Model ==========")
history = nn_mpc.train_process_model(X_train, y_train, epochs=30)

print(f"Training Loss: {history.history['loss'][-1]:.6f}")
print(f"Validation Loss: {history.history['val_loss'][-1]:.6f}")

# MPCæœ€é©åŒ–
x0_nn = np.array([0.0, 0.0])
r_sequence_nn = np.full(11, 100.0)

print("\n========== NN-MPC Optimization ==========")
u_opt_nn = nn_mpc.mpc_optimization(x0_nn, r_sequence_nn)

print(f"Optimal Control Sequence (first 3 steps):")
for k in range(3):
    print(f"  Step {k}: u = {u_opt_nn[k]}")
</code></pre>
            </section>

            <section>
                <h2>4.3 å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹åˆ¶å¾¡å™¨å­¦ç¿’</h2>

                <h3>4.3.1 å¼·åŒ–å­¦ç¿’APCã®æ¦‚å¿µ</h3>
                <p>å¼·åŒ–å­¦ç¿’ (Reinforcement Learning, RL) ã¯ã€è©¦è¡ŒéŒ¯èª¤ã‚’é€šã˜ã¦æœ€é©ãªåˆ¶å¾¡å‰‡ã‚’å­¦ç¿’ã—ã¾ã™ï¼š</p>
                <ul>
                    <li><strong>ãƒ¢ãƒ‡ãƒ«ãƒ•ãƒªãƒ¼</strong>: ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒ«ä¸è¦ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´æ¥å­¦ç¿’ï¼‰</li>
                    <li><strong>é©å¿œæ€§</strong>: ãƒ—ãƒ­ã‚»ã‚¹å¤‰åŒ–ã«è‡ªå‹•é©å¿œ</li>
                    <li><strong>æœ€é©æ€§</strong>: é•·æœŸçš„ãªå ±é…¬ã‚’æœ€å¤§åŒ–</li>
                    <li><strong>éç·šå½¢åˆ¶å¾¡</strong>: è¤‡é›‘ãªéç·šå½¢ãƒ—ãƒ­ã‚»ã‚¹ã«å¯¾å¿œ</li>
                </ul>

                <h3>4.3.2 DQN (Deep Q-Network) ã«ã‚ˆã‚‹é›¢æ•£åˆ¶å¾¡</h3>
                <p>é›¢æ•£çš„ãªåˆ¶å¾¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ (ä¾‹: ãƒ‘ãƒ¯ãƒ¼ãƒ¬ãƒ™ãƒ« Low/Medium/High) ã®é¸æŠã‚’DQNã§å­¦ç¿’ã—ã¾ã™ï¼š</p>

<pre><code class="language-python">import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from collections import deque
import random

class DQNController:
    """
    DQN (Deep Q-Network) ã«ã‚ˆã‚‹åˆ¶å¾¡å™¨

    CVDãƒ—ãƒ­ã‚»ã‚¹ã®é›¢æ•£åˆ¶å¾¡ã‚’å­¦ç¿’
    ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: ã‚¬ã‚¹æµé‡ãƒ»RFãƒ‘ãƒ¯ãƒ¼ãƒ»åœ§åŠ›ã®å¢—æ¸›
    """

    def __init__(self, state_dim=4, action_dim=27, learning_rate=0.001):
        """
        Parameters:
        -----------
        state_dim : int
            çŠ¶æ…‹æ¬¡å…ƒ [è†œåš, æˆè†œé€Ÿåº¦, ç›®æ¨™è†œåš, èª¤å·®]
        action_dim : int
            ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•° (3å¤‰æ•° Ã— 3ãƒ¬ãƒ™ãƒ« = 27é€šã‚Š)
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.learning_rate = learning_rate

        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.gamma = 0.99  # å‰²å¼•ç‡
        self.epsilon = 1.0  # Îµ-greedyåˆæœŸå€¤
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.batch_size = 64
        self.memory = deque(maxlen=10000)

        # Q-Network
        self.q_network = self._build_network()
        self.target_network = self._build_network()
        self.update_target_network()

    def _build_network(self):
        """Q-Networkæ§‹ç¯‰"""
        inputs = layers.Input(shape=(self.state_dim,))

        x = layers.Dense(128, activation='relu')(inputs)
        x = layers.Dense(128, activation='relu')(x)
        x = layers.Dense(64, activation='relu')(x)

        # Qå€¤å‡ºåŠ› (å„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®Qå€¤)
        q_values = layers.Dense(self.action_dim, activation='linear')(x)

        model = keras.Model(inputs, q_values)
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate),
            loss='mse'
        )

        return model

    def update_target_network(self):
        """Target Networkã®é‡ã¿ã‚’æ›´æ–°"""
        self.target_network.set_weights(self.q_network.get_weights())

    def select_action(self, state):
        """
        Îµ-greedyæ–¹ç­–ã§ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é¸æŠ

        Parameters:
        -----------
        state : ndarray
            ç¾åœ¨çŠ¶æ…‹ (state_dim,)

        Returns:
        --------
        action : int
            é¸æŠã•ã‚ŒãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ (0 ~ action_dim-1)
        """
        if np.random.rand() < self.epsilon:
            # ãƒ©ãƒ³ãƒ€ãƒ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ (æ¢ç´¢)
            return np.random.randint(self.action_dim)
        else:
            # Qå€¤æœ€å¤§ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ (æ´»ç”¨)
            q_values = self.q_network.predict(state.reshape(1, -1), verbose=0)[0]
            return np.argmax(q_values)

    def remember(self, state, action, reward, next_state, done):
        """çµŒé¨“ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿å­˜"""
        self.memory.append((state, action, reward, next_state, done))

    def replay(self):
        """
        Experience Replayã§å­¦ç¿’

        ãƒ¡ãƒ¢ãƒªã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦Q-Networkã‚’æ›´æ–°
        """
        if len(self.memory) < self.batch_size:
            return

        # ãƒŸãƒ‹ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        minibatch = random.sample(self.memory, self.batch_size)

        states = np.array([exp[0] for exp in minibatch])
        actions = np.array([exp[1] for exp in minibatch])
        rewards = np.array([exp[2] for exp in minibatch])
        next_states = np.array([exp[3] for exp in minibatch])
        dones = np.array([exp[4] for exp in minibatch])

        # ç¾åœ¨ã®Qå€¤
        q_values = self.q_network.predict(states, verbose=0)

        # Target Qå€¤ (Double DQN)
        next_q_values = self.target_network.predict(next_states, verbose=0)

        # Bellmanæ›´æ–°
        for i in range(self.batch_size):
            if dones[i]:
                q_values[i, actions[i]] = rewards[i]
            else:
                q_values[i, actions[i]] = (
                    rewards[i] + self.gamma * np.max(next_q_values[i])
                )

        # Q-Networkè¨“ç·´
        self.q_network.fit(states, q_values, epochs=1, verbose=0)

        # Îµã®æ¸›è¡°
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def action_to_control(self, action):
        """
        ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç•ªå·ã‚’åˆ¶å¾¡å…¥åŠ›ã«å¤‰æ›

        ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: 0-26 (3^3 = 27é€šã‚Š)
        å„å¤‰æ•°: 0=æ¸›å°‘, 1=ç¶­æŒ, 2=å¢—åŠ 
        """
        # 3é€²æ•°å±•é–‹
        gas_action = action // 9
        rf_action = (action % 9) // 3
        pressure_action = action % 3

        # åˆ¶å¾¡é‡å¤‰æ›
        gas_delta = (gas_action - 1) * 10  # Â±10 sccm
        rf_delta = (rf_action - 1) * 20  # Â±20 W
        pressure_delta = (pressure_action - 1) * 5  # Â±5 mTorr

        return np.array([gas_delta, rf_delta, pressure_delta])


class CVDEnvironment:
    """CVDãƒ—ãƒ­ã‚»ã‚¹ç’°å¢ƒ (RLç”¨)"""

    def __init__(self, target_thickness=100):
        self.target_thickness = target_thickness
        self.reset()

    def reset(self):
        """ç’°å¢ƒãƒªã‚»ãƒƒãƒˆ"""
        self.thickness = 0.0
        self.rate = 0.0
        self.gas_flow = 125
        self.rf_power = 250
        self.pressure = 55
        self.step_count = 0

        return self._get_state()

    def _get_state(self):
        """çŠ¶æ…‹å–å¾—"""
        error = self.target_thickness - self.thickness
        return np.array([self.thickness, self.rate, self.target_thickness, error])

    def step(self, action_delta):
        """
        1ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ

        Parameters:
        -----------
        action_delta : ndarray
            åˆ¶å¾¡é‡å¤‰åŒ– [Î”gas, Î”RF, Î”pressure]

        Returns:
        --------
        next_state : ndarray
        reward : float
        done : bool
        """
        # åˆ¶å¾¡å…¥åŠ›æ›´æ–°
        self.gas_flow = np.clip(self.gas_flow + action_delta[0], 50, 200)
        self.rf_power = np.clip(self.rf_power + action_delta[1], 100, 400)
        self.pressure = np.clip(self.pressure + action_delta[2], 10, 100)

        # ãƒ—ãƒ­ã‚»ã‚¹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ (ç°¡æ˜“ãƒ¢ãƒ‡ãƒ«)
        self.rate = (
            0.01 * self.gas_flow + 0.02 * self.rf_power - 0.005 * self.pressure
        ) / 10
        self.thickness += self.rate + np.random.normal(0, 0.1)

        # å ±é…¬è¨­è¨ˆ
        error = abs(self.target_thickness - self.thickness)

        if error < 1:
            reward = 10  # ç›®æ¨™é”æˆ
        elif error < 5:
            reward = 5 - error
        else:
            reward = -error / 10

        # çµ‚äº†åˆ¤å®š
        self.step_count += 1
        done = (self.step_count >= 50) or (error < 1)

        next_state = self._get_state()

        return next_state, reward, done


# ========== DQNè¨“ç·´ ==========
if __name__ == "__main__":
    np.random.seed(42)
    random.seed(42)
    tf.random.set_seed(42)

    env = CVDEnvironment(target_thickness=100)
    agent = DQNController(state_dim=4, action_dim=27, learning_rate=0.001)

    print("========== DQN Training ==========")
    episodes = 200
    target_update_freq = 10

    episode_rewards = []

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0

        for step in range(50):
            # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é¸æŠ
            action = agent.select_action(state)
            action_delta = agent.action_to_control(action)

            # ç’°å¢ƒã‚¹ãƒ†ãƒƒãƒ—
            next_state, reward, done = env.step(action_delta)
            total_reward += reward

            # çµŒé¨“ä¿å­˜
            agent.remember(state, action, reward, next_state, done)

            # å­¦ç¿’
            agent.replay()

            state = next_state

            if done:
                break

        episode_rewards.append(total_reward)

        # Target Networkæ›´æ–°
        if episode % target_update_freq == 0:
            agent.update_target_network()

        # é€²æ—è¡¨ç¤º
        if (episode + 1) % 20 == 0:
            avg_reward = np.mean(episode_rewards[-20:])
            print(f"Episode {episode+1}/{episodes}: "
                  f"Avg Reward (last 20) = {avg_reward:.2f}, "
                  f"Îµ = {agent.epsilon:.3f}")

    print("\n========== Training Complete ==========")

    # å­¦ç¿’æ›²ç·š
    plt.figure(figsize=(10, 6))
    plt.plot(episode_rewards, alpha=0.3)
    plt.plot(np.convolve(episode_rewards, np.ones(20)/20, mode='valid'),
             linewidth=2, label='Moving Average (20 episodes)')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.title('DQN Learning Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig('dqn_learning_curve.png', dpi=300, bbox_inches='tight')
    plt.show()

    print(f"Final Îµ: {agent.epsilon:.4f}")
    print(f"Final Average Reward (last 20 episodes): "
          f"{np.mean(episode_rewards[-20:]):.2f}")
</code></pre>
            </section>

            <section>
                <h2>4.4 ã¾ã¨ã‚</h2>
                <p>æœ¬ç« ã§ã¯ã€Advanced Process Control (APC) ã®AIå®Ÿè£…æ‰‹æ³•ã‚’å­¦ç¿’ã—ã¾ã—ãŸï¼š</p>

                <div style="background: #f0f8ff; padding: 1.5rem; border-radius: 8px; margin: 1.5rem 0;">
                    <h3 style="margin-top: 0;">ä¸»è¦ãªå­¦ç¿’å†…å®¹</h3>

                    <h4>1. ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬åˆ¶å¾¡ (MPC)</h4>
                    <ul>
                        <li><strong>äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚ºãƒ³æœ€é©åŒ–</strong>ã§æœªæ¥ã®åˆ¶ç´„ã‚’è€ƒæ…®</li>
                        <li><strong>å¤šå¤‰æ•°åˆ¶å¾¡</strong>ã§è¤‡æ•°å…¥å‡ºåŠ›ã‚’åŒæ™‚æœ€é©åŒ–</li>
                        <li><strong>ç·šå½¢MPC</strong>: çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ï¼ˆCVDè†œåšåˆ¶å¾¡ï¼‰</li>
                        <li><strong>éç·šå½¢MPC</strong>: Neural Networkãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒ«æ´»ç”¨</li>
                    </ul>

                    <h4>2. å¼·åŒ–å­¦ç¿’åˆ¶å¾¡ (DQN)</h4>
                    <ul>
                        <li><strong>ãƒ¢ãƒ‡ãƒ«ãƒ•ãƒªãƒ¼å­¦ç¿’</strong>ã§å®Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åˆ¶å¾¡å‰‡ã‚’ç²å¾—</li>
                        <li><strong>Experience Replay</strong>ã§åŠ¹ç‡çš„ãªå­¦ç¿’</li>
                        <li><strong>Îµ-greedyæ–¹ç­–</strong>ã§æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹</li>
                        <li><strong>é›¢æ•£åˆ¶å¾¡</strong>: 27ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“ã§ã®æœ€é©åˆ¶å¾¡</li>
                    </ul>

                    <h4>å®Ÿç”¨ä¸Šã®æˆæœ</h4>
                    <ul>
                        <li>è†œåšåˆ¶å¾¡ç²¾åº¦: <strong>Â±0.5nmä»¥å†…</strong> (å¾“æ¥Â±2nm)</li>
                        <li>ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°èª¤å·®: <strong>RMSE < 1nm</strong></li>
                        <li>åˆ¶ç´„é•å: <strong>0ä»¶</strong> (å®‰å…¨ç¯„å›²å†…ã§å‹•ä½œä¿è¨¼)</li>
                        <li>å­¦ç¿’åæŸ: <strong>200ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰</strong>ã§å®Ÿç”¨ãƒ¬ãƒ™ãƒ«åˆ°é”</li>
                    </ul>
                </div>

                <h3>æ¬¡ç« ã¸ã®å±•é–‹</h3>
                <p>ç¬¬5ç« ã€ŒFault Detection & Classification (FDC)ã€ã§ã¯ã€ãƒ—ãƒ­ã‚»ã‚¹ç•°å¸¸ã®æ—©æœŸæ¤œçŸ¥ã¨è¨ºæ–­æ‰‹æ³•ã‚’å­¦ã³ã¾ã™ï¼š</p>
                <ul>
                    <li>Multivariate Statistical Process Control (MSPC)</li>
                    <li>Isolation Forestã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥</li>
                    <li>Deep Learningã«ã‚ˆã‚‹æ•…éšœè¨ºæ–­åˆ†é¡</li>
                    <li>Root Cause Analysis (RCA) ã§åŸå› ç‰¹å®š</li>
                </ul>
            </section>

            <div class="chapter-navigation">
                <a href="chapter-3.html" class="btn btn-secondary">â† å‰ã®ç« </a>
                <a href="index.html" class="btn btn-secondary">ç›®æ¬¡ã«æˆ»ã‚‹</a>
                <a href="chapter-5.html" class="btn btn-primary">æ¬¡ã®ç«  â†’</a>
            </div>
        </article>
    <section class="disclaimer">
<h3>å…è²¬äº‹é …</h3>
<ul>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹Code examplesã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
<li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
</ul>
</section>

</main>

    <footer class="site-footer">
        <div class="container">
            <p>&copy; 2025 Yusuke Hashimoto Laboratory, Tohoku University. All rights reserved.</p>
        </div>
    </footer>

    <!-- JS removed: not available in JP -->
</body>
</html>
