<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬2ç«  AIã«ã‚ˆã‚‹æ¬ é™¥æ¤œæŸ»ã¨AOI | åŠå°ä½“è£½é€ AI | ãƒãƒ†ãƒªã‚¢ãƒ«ã‚ºã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹å…¥é–€</title>
    <meta name="description" content="åŠå°ä½“ã‚¦ã‚§ãƒã®æ¬ é™¥æ¤œæŸ»ã«ãŠã‘ã‚‹æ·±å±¤å­¦ç¿’ã®å¿œç”¨ã‚’å­¦ã³ã¾ã™ã€‚CNNã«ã‚ˆã‚‹æ¬ é™¥åˆ†é¡ã€Semantic Segmentationã«ã‚ˆã‚‹æ¬ é™¥ä½ç½®ç‰¹å®šã€Anomaly Detectionã€AOIã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…ã‚’Pythonã§å®Ÿè·µã—ã¾ã™ã€‚">
    <link rel="stylesheet" href="/assets/css/variables.css">
    <link rel="stylesheet" href="/assets/css/reset.css">
    <link rel="stylesheet" href="/assets/css/base.css">
    <link rel="stylesheet" href="/assets/css/components.css">
    <link rel="stylesheet" href="/assets/css/layout.css">
    <link rel="stylesheet" href="/assets/css/responsive.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
    <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/PI/semiconductor-manufacturing-ai/chapter-2.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header class="site-header">
        <div class="container">
            <div class="header-content">
                <h1 class="site-title"><a href="/jp/">æ©‹æœ¬ç ”ç©¶å®¤</a></h1>
                <nav class="main-nav">
                    <ul>
                        <li><a href="/jp/">ãƒ›ãƒ¼ãƒ </a></li>
                        <li><a href="/jp/research.html">ç ”ç©¶å†…å®¹</a></li>
                        <li><a href="/jp/publications.html">ç ”ç©¶æ¥­ç¸¾</a></li>
                        <li><a href="../../mi-introduction/">çŸ¥è­˜ãƒ™ãƒ¼ã‚¹</a></li>
                        <li><a href="/jp/news.html">ãƒ‹ãƒ¥ãƒ¼ã‚¹</a></li>
                        <li><a href="/jp/members.html">ãƒ¡ãƒ³ãƒãƒ¼</a></li>
                        <li><a href="/jp/contact.html">ãŠå•ã„åˆã‚ã›</a></li>
                        <li><a href="/en/">English</a></li>
                    </ul>
                </nav>
            </div>
        </div>
    </header>

    <main class="article-content">
        <article class="container">
            <div class="breadcrumb">
                <a href="/jp/">ãƒ›ãƒ¼ãƒ </a> &gt;
                <a href="../../mi-introduction/">çŸ¥è­˜ãƒ™ãƒ¼ã‚¹</a> &gt;
                <a href="../../PI/">ãƒ—ãƒ­ã‚»ã‚¹ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹</a> &gt;
                <a href="../../PI/semiconductor-manufacturing-ai/">åŠå°ä½“è£½é€ AI</a> &gt;
                ç¬¬2ç« 
            </div>

            <header class="article-header">
                <h1 class="gradient-text" style="background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent;">ç¬¬2ç«  AIã«ã‚ˆã‚‹æ¬ é™¥æ¤œæŸ»ã¨AOI</h1>
                <p class="article-meta">åŠå°ä½“è£½é€ AI - Deep Learningã«ã‚ˆã‚‹æ¬ é™¥åˆ†é¡ãƒ»ä½ç½®ç‰¹å®šãƒ»ç•°å¸¸æ¤œçŸ¥</p>
            </header>

            <section class="introduction">
                <h2>å­¦ç¿’ç›®æ¨™</h2>
                <ul>
                    <li>CNNã«ã‚ˆã‚‹æ¬ é™¥ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡ã®ç†è«–ã¨å®Ÿè£…ã‚’ç†è§£ã™ã‚‹</li>
                    <li>Semantic Segmentationã«ã‚ˆã‚‹æ¬ é™¥ä½ç½®ç‰¹å®šæ‰‹æ³•ã‚’ç¿’å¾—ã™ã‚‹</li>
                    <li>Autoencoderã‚’ç”¨ã„ãŸç•°å¸¸æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹</li>
                    <li>AOI (Automated Optical Inspection) ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…æ–¹æ³•ã‚’å­¦ã¶</li>
                    <li>Transfer Learningã¨Data Augmentationã®å®Ÿè·µçš„æ´»ç”¨æ³•ã‚’ç†è§£ã™ã‚‹</li>
                </ul>
            </section>

            <section>
                <h2>2.1 åŠå°ä½“æ¬ é™¥æ¤œæŸ»ã®èª²é¡Œ</h2>

                <h3>2.1.1 æ¬ é™¥æ¤œæŸ»ã®é‡è¦æ€§</h3>
                <p>åŠå°ä½“è£½é€ ãƒ—ãƒ­ã‚»ã‚¹ã«ãŠã„ã¦ã€ã‚¦ã‚§ãƒä¸Šã®æ¬ é™¥æ¤œå‡ºã¯æ­©ç•™ã¾ã‚Šå‘ä¸Šã®éµã¨ãªã‚Šã¾ã™ã€‚ä¸»ãªæ¬ é™¥ã‚¿ã‚¤ãƒ—ã«ã¯ä»¥ä¸‹ãŒã‚ã‚Šã¾ã™ï¼š</p>
                <ul>
                    <li><strong>ãƒ‘ãƒ¼ãƒ†ã‚£ã‚¯ãƒ«æ¬ é™¥</strong>: å¾®å°ãªç•°ç‰©ä»˜ç€ï¼ˆç›´å¾„0.1Î¼mä»¥ä¸‹ã®æ¤œå‡ºãŒå¿…è¦ï¼‰</li>
                    <li><strong>ãƒ‘ã‚¿ãƒ¼ãƒ³æ¬ é™¥</strong>: ã‚¨ãƒƒãƒãƒ³ã‚°ä¸è‰¯ã€ãƒªã‚½ã‚°ãƒ©ãƒ•ã‚£ã‚ºãƒ¬ã€CDä¸è‰¯</li>
                    <li><strong>ã‚¹ã‚¯ãƒ©ãƒƒãƒ</strong>: ã‚¦ã‚§ãƒè¡¨é¢ã®ç·šçŠ¶å‚·</li>
                    <li><strong>çµæ™¶æ¬ é™¥</strong>: è»¢ä½ã€ç©å±¤æ¬ é™¥</li>
                    <li><strong>è†œè³ªæ¬ é™¥</strong>: è†œåšãƒ ãƒ©ã€æ®‹æ¸£</li>
                </ul>

                <h3>2.1.2 å¾“æ¥æ‰‹æ³•ã®é™ç•Œ</h3>
                <p>ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æ¤œæŸ»ã®èª²é¡Œï¼š</p>
                <ul>
                    <li><strong>èª¤æ¤œå‡ºç‡ã®é«˜ã•</strong>: æ­£å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¤‰å‹•ã‚’æ¬ é™¥ã¨èª¤åˆ¤å®š</li>
                    <li><strong>é–¾å€¤èª¿æ•´ã®å›°é›£æ€§</strong>: ãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶å¤‰åŒ–ã§å†èª¿æ•´ãŒå¿…è¦</li>
                    <li><strong>æ–°è¦æ¬ é™¥ã¸ã®å¯¾å¿œä¸å¯</strong>: æœªçŸ¥ã®æ¬ é™¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã§ããªã„</li>
                    <li><strong>è¤‡é›‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é™ç•Œ</strong>: å¤šå±¤é…ç·šã®3Dæ§‹é€ ã§ã¯ç²¾åº¦ä½ä¸‹</li>
                </ul>

                <h3>2.1.3 Deep Learningå°å…¥ã®ãƒ¡ãƒªãƒƒãƒˆ</h3>
                <p>AIã«ã‚ˆã‚‹æ¤œæŸ»ã®å„ªä½æ€§ï¼š</p>
                <div style="background: #f8f9fa; padding: 1rem; border-radius: 8px; margin: 1rem 0;">
                    <p><strong>ç²¾åº¦å‘ä¸Š</strong>: å¾“æ¥æ‰‹æ³•ã®90%æ¤œå‡ºç‡ â†’ DLå°å…¥ã§99%ä»¥ä¸Š</p>
                    <p><strong>èª¤æ¤œå‡ºå‰Šæ¸›</strong>: False Positiveç‡ã‚’1/10ä»¥ä¸‹ã«ä½æ¸›</p>
                    <p><strong>æ¤œæŸ»é€Ÿåº¦</strong>: GPUæ´»ç”¨ã§100å€é«˜é€ŸåŒ–ï¼ˆ0.1ç§’/æšä»¥ä¸‹ï¼‰</p>
                    <p><strong>é©å¿œæ€§</strong>: æ–°è¦ãƒ—ãƒ­ã‚»ã‚¹ã¸ã®è»¢ç§»å­¦ç¿’ã§æ—©æœŸç«‹ã¡ä¸Šã’</p>
                </div>
            </section>

            <section>
                <h2>2.2 CNNã«ã‚ˆã‚‹æ¬ é™¥åˆ†é¡</h2>

                <h3>2.2.1 ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åŸºç¤</h3>
                <p>CNN (Convolutional Neural Network) ã¯ç”»åƒèªè­˜ã®ãƒ‡ãƒ•ã‚¡ã‚¯ãƒˆã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ã§ã™ã€‚åŠå°ä½“æ¬ é™¥åˆ†é¡ã«ãŠã‘ã‚‹ä¸»è¦ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼š</p>

                <h4>ä¸»è¦ãƒ¬ã‚¤ãƒ¤ãƒ¼æ§‹æˆ</h4>
                <div style="background: #f8f9fa; padding: 1rem; border-radius: 8px; margin: 1rem 0;">
                    <p><strong>ç•³ã¿è¾¼ã¿å±¤ (Conv2D)</strong></p>
                    <p>$$y_{i,j} = \sum_{m}\sum_{n} w_{m,n} \cdot x_{i+m, j+n} + b$$</p>
                    <p>å±€æ‰€çš„ãªç‰¹å¾´æŠ½å‡ºã‚’è¡Œã„ã¾ã™ã€‚ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º3Ã—3ãŒä¸€èˆ¬çš„ã§ã™ã€‚</p>

                    <p><strong>ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ (MaxPooling2D)</strong></p>
                    <p>$$y_{i,j} = \max_{m,n \in \text{window}} x_{i+m, j+n}$$</p>
                    <p>ç©ºé–“è§£åƒåº¦ã‚’å‰Šæ¸›ã—ã€ä½ç½®ä¸å¤‰æ€§ã‚’ç²å¾—ã—ã¾ã™ã€‚</p>

                    <p><strong>Batch Normalization</strong></p>
                    <p>$$\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$</p>
                    <p>å­¦ç¿’å®‰å®šåŒ–ã¨é«˜é€ŸåŒ–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚</p>
                </div>

                <h3>2.2.2 æ¬ é™¥åˆ†é¡CNNã®å®Ÿè£…</h3>
                <p>ä»¥ä¸‹ã¯ã€6ç¨®é¡ã®æ¬ é™¥ã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡ã™ã‚‹CNNãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…ä¾‹ã§ã™ï¼š</p>

<pre><code class="language-python">import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

class DefectClassifierCNN:
    """
    åŠå°ä½“ã‚¦ã‚§ãƒæ¬ é™¥åˆ†é¡ç”¨CNNãƒ¢ãƒ‡ãƒ«

    å¯¾å¿œæ¬ é™¥ã‚¿ã‚¤ãƒ—:
    - Particle (ãƒ‘ãƒ¼ãƒ†ã‚£ã‚¯ãƒ«)
    - Scratch (ã‚¹ã‚¯ãƒ©ãƒƒãƒ)
    - Pattern (ãƒ‘ã‚¿ãƒ¼ãƒ³æ¬ é™¥)
    - Crystal (çµæ™¶æ¬ é™¥)
    - Thin_Film (è†œè³ªæ¬ é™¥)
    - Normal (æ­£å¸¸)
    """

    def __init__(self, input_shape=(128, 128, 1), num_classes=6):
        """
        Parameters:
        -----------
        input_shape : tuple
            å…¥åŠ›ç”»åƒã‚µã‚¤ã‚º (height, width, channels)
            ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ç”»åƒã‚’æƒ³å®š
        num_classes : int
            åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°
        """
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.model = None
        self.history = None

        # ã‚¯ãƒ©ã‚¹åå®šç¾©
        self.class_names = [
            'Particle', 'Scratch', 'Pattern',
            'Crystal', 'Thin_Film', 'Normal'
        ]

    def build_model(self):
        """
        CNNãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰

        Architecture:
        - Conv2D â†’ BatchNorm â†’ ReLU â†’ MaxPooling (Ã—3ãƒ–ãƒ­ãƒƒã‚¯)
        - Global Average Pooling
        - Dense â†’ Dropout â†’ Dense (åˆ†é¡å±¤)

        Total params: ~500K (è»½é‡è¨­è¨ˆã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–å¯èƒ½)
        """
        model = models.Sequential([
            # Block 1: ç‰¹å¾´æŠ½å‡ºå±¤ (ä½æ¬¡ç‰¹å¾´)
            layers.Conv2D(32, (3, 3), padding='same',
                         input_shape=self.input_shape),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPooling2D((2, 2)),

            # Block 2: ä¸­æ¬¡ç‰¹å¾´æŠ½å‡º
            layers.Conv2D(64, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPooling2D((2, 2)),

            # Block 3: é«˜æ¬¡ç‰¹å¾´æŠ½å‡º
            layers.Conv2D(128, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.Conv2D(128, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPooling2D((2, 2)),

            # Block 4: ã•ã‚‰ã«é«˜æ¬¡ã®ç‰¹å¾´
            layers.Conv2D(256, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.Conv2D(256, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),

            # Global pooling (Fully Connectedã®ä»£ã‚ã‚Šã€éå­¦ç¿’æŠ‘åˆ¶)
            layers.GlobalAveragePooling2D(),

            # åˆ†é¡å±¤
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.5),
            layers.Dense(self.num_classes, activation='softmax')
        ])

        # ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
        model.compile(
            optimizer=optimizers.Adam(learning_rate=0.001),
            loss='categorical_crossentropy',
            metrics=['accuracy', tf.keras.metrics.Precision(),
                    tf.keras.metrics.Recall()]
        )

        self.model = model
        return model

    def create_data_augmentation(self):
        """
        Data Augmentationè¨­å®š

        åŠå°ä½“æ¬ é™¥ç”»åƒç‰¹æœ‰ã®æ‹¡å¼µ:
        - å›è»¢: 0Â°, 90Â°, 180Â°, 270Â° (ã‚¦ã‚§ãƒã®å‘ãä¸å¤‰æ€§)
        - åè»¢: æ°´å¹³ãƒ»å‚ç›´ (å¯¾ç§°æ€§)
        - æ˜ã‚‹ã•èª¿æ•´: ç…§æ˜æ¡ä»¶ã®å¤‰å‹•ã«å¯¾å¿œ
        - ãƒã‚¤ã‚ºä»˜åŠ : ã‚»ãƒ³ã‚µãƒ¼ãƒã‚¤ã‚ºã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        """
        train_datagen = ImageDataGenerator(
            rotation_range=90,           # Â±90åº¦å›è»¢
            width_shift_range=0.1,       # 10%æ°´å¹³ã‚·ãƒ•ãƒˆ
            height_shift_range=0.1,      # 10%å‚ç›´ã‚·ãƒ•ãƒˆ
            horizontal_flip=True,
            vertical_flip=True,
            brightness_range=[0.8, 1.2], # æ˜ã‚‹ã•Â±20%
            zoom_range=0.1,              # ã‚ºãƒ¼ãƒ Â±10%
            fill_mode='reflect'          # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°æ–¹æ³•
        )

        # æ¤œè¨¼/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯æ­£è¦åŒ–ã®ã¿
        val_datagen = ImageDataGenerator()

        return train_datagen, val_datagen

    def train(self, X_train, y_train, X_val, y_val,
              epochs=50, batch_size=32, use_augmentation=True):
        """
        ãƒ¢ãƒ‡ãƒ«è¨“ç·´

        Parameters:
        -----------
        X_train : ndarray
            è¨“ç·´ç”»åƒ (N, H, W, C)
        y_train : ndarray
            è¨“ç·´ãƒ©ãƒ™ãƒ« (N, num_classes) - one-hot encoded
        X_val : ndarray
            æ¤œè¨¼ç”»åƒ
        y_val : ndarray
            æ¤œè¨¼ãƒ©ãƒ™ãƒ«
        epochs : int
            ã‚¨ãƒãƒƒã‚¯æ•°
        batch_size : int
            ãƒãƒƒãƒã‚µã‚¤ã‚º
        use_augmentation : bool
            Data Augmentationä½¿ç”¨ãƒ•ãƒ©ã‚°
        """
        if self.model is None:
            self.build_model()

        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
        callbacks = [
            # æ¤œè¨¼æå¤±ãŒæ”¹å–„ã—ãªã„å ´åˆã€å­¦ç¿’ç‡ã‚’å‰Šæ¸›
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-7,
                verbose=1
            ),
            # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
            tf.keras.callbacks.ModelCheckpoint(
                'best_defect_classifier.h5',
                monitor='val_accuracy',
                save_best_only=True,
                verbose=1
            ),
            # æ—©æœŸçµ‚äº† (éå­¦ç¿’é˜²æ­¢)
            tf.keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True,
                verbose=1
            )
        ]

        if use_augmentation:
            train_datagen, _ = self.create_data_augmentation()

            # Data Generatorã§è¨“ç·´
            self.history = self.model.fit(
                train_datagen.flow(X_train, y_train, batch_size=batch_size),
                validation_data=(X_val, y_val),
                epochs=epochs,
                callbacks=callbacks,
                verbose=1
            )
        else:
            # é€šå¸¸ã®è¨“ç·´
            self.history = self.model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=epochs,
                batch_size=batch_size,
                callbacks=callbacks,
                verbose=1
            )

        return self.history

    def evaluate(self, X_test, y_test):
        """
        ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½è©•ä¾¡

        Returns:
        --------
        metrics : dict
            accuracy, precision, recall, f1-scoreç­‰
        """
        # äºˆæ¸¬
        y_pred_proba = self.model.predict(X_test)
        y_pred = np.argmax(y_pred_proba, axis=1)
        y_true = np.argmax(y_test, axis=1)

        # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
        report = classification_report(
            y_true, y_pred,
            target_names=self.class_names,
            output_dict=True
        )

        # æ··åŒè¡Œåˆ—
        cm = confusion_matrix(y_true, y_pred)

        # çµæœã‚’æ•´å½¢
        metrics = {
            'accuracy': report['accuracy'],
            'macro_avg': report['macro avg'],
            'weighted_avg': report['weighted avg'],
            'per_class': {name: report[name] for name in self.class_names},
            'confusion_matrix': cm
        }

        return metrics, y_pred, y_pred_proba

    def plot_training_history(self):
        """è¨“ç·´å±¥æ­´ã®å¯è¦–åŒ–"""
        if self.history is None:
            print("No training history available")
            return

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))

        # Accuracy
        axes[0, 0].plot(self.history.history['accuracy'], label='Train')
        axes[0, 0].plot(self.history.history['val_accuracy'], label='Validation')
        axes[0, 0].set_title('Model Accuracy')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Accuracy')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # Loss
        axes[0, 1].plot(self.history.history['loss'], label='Train')
        axes[0, 1].plot(self.history.history['val_loss'], label='Validation')
        axes[0, 1].set_title('Model Loss')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

        # Precision
        axes[1, 0].plot(self.history.history['precision'], label='Train')
        axes[1, 0].plot(self.history.history['val_precision'], label='Validation')
        axes[1, 0].set_title('Precision')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Precision')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)

        # Recall
        axes[1, 1].plot(self.history.history['recall'], label='Train')
        axes[1, 1].plot(self.history.history['val_recall'], label='Validation')
        axes[1, 1].set_title('Recall')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Recall')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
        plt.show()

    def plot_confusion_matrix(self, cm):
        """æ··åŒè¡Œåˆ—ã®å¯è¦–åŒ–"""
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=self.class_names,
                   yticklabels=self.class_names)
        plt.title('Confusion Matrix - Defect Classification')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.tight_layout()
        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.show()


# ========== ä½¿ç”¨ä¾‹ ==========
if __name__ == "__main__":
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ (å®Ÿéš›ã¯å®Ÿç”»åƒã‚’ä½¿ç”¨)
    np.random.seed(42)

    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿: å„ã‚¯ãƒ©ã‚¹500æš = åˆè¨ˆ3000æš
    X_train = np.random.randn(3000, 128, 128, 1).astype(np.float32)
    y_train = np.eye(6)[np.random.randint(0, 6, 3000)]  # one-hot

    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: å„ã‚¯ãƒ©ã‚¹100æš = åˆè¨ˆ600æš
    X_val = np.random.randn(600, 128, 128, 1).astype(np.float32)
    y_val = np.eye(6)[np.random.randint(0, 6, 600)]

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: å„ã‚¯ãƒ©ã‚¹100æš = åˆè¨ˆ600æš
    X_test = np.random.randn(600, 128, 128, 1).astype(np.float32)
    y_test = np.eye(6)[np.random.randint(0, 6, 600)]

    # æ­£è¦åŒ– (0-1ç¯„å›²ã«)
    X_train = (X_train - X_train.min()) / (X_train.max() - X_train.min())
    X_val = (X_val - X_val.min()) / (X_val.max() - X_val.min())
    X_test = (X_test - X_test.min()) / (X_test.max() - X_test.min())

    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ãƒ»è¨“ç·´
    classifier = DefectClassifierCNN(input_shape=(128, 128, 1), num_classes=6)
    classifier.build_model()

    print("Model Architecture:")
    classifier.model.summary()

    # è¨“ç·´å®Ÿè¡Œ
    print("\n========== Training Start ==========")
    history = classifier.train(
        X_train, y_train,
        X_val, y_val,
        epochs=30,
        batch_size=32,
        use_augmentation=True
    )

    # è©•ä¾¡
    print("\n========== Evaluation on Test Set ==========")
    metrics, y_pred, y_pred_proba = classifier.evaluate(X_test, y_test)

    print(f"\nOverall Accuracy: {metrics['accuracy']:.4f}")
    print(f"Macro-avg Precision: {metrics['macro_avg']['precision']:.4f}")
    print(f"Macro-avg Recall: {metrics['macro_avg']['recall']:.4f}")
    print(f"Macro-avg F1-Score: {metrics['macro_avg']['f1-score']:.4f}")

    print("\n--- Per-Class Performance ---")
    for class_name in classifier.class_names:
        class_metrics = metrics['per_class'][class_name]
        print(f"{class_name:12s}: Precision={class_metrics['precision']:.3f}, "
              f"Recall={class_metrics['recall']:.3f}, "
              f"F1={class_metrics['f1-score']:.3f}")

    # å¯è¦–åŒ–
    classifier.plot_training_history()
    classifier.plot_confusion_matrix(metrics['confusion_matrix'])

    print("\n========== Training Complete ==========")
    print("Best model saved to: best_defect_classifier.h5")
</code></pre>

                <h3>2.2.3 Transfer Learningã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Š</h3>
                <p>ImageNetã§äº‹å‰è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€å°‘é‡ãƒ‡ãƒ¼ã‚¿ã§ã‚‚é«˜ç²¾åº¦ã‚’å®Ÿç¾ã§ãã¾ã™ï¼š</p>

<pre><code class="language-python">from tensorflow.keras.applications import ResNet50V2
from tensorflow.keras import layers, models

class TransferLearningDefectClassifier:
    """
    Transfer Learningã‚’ç”¨ã„ãŸæ¬ é™¥åˆ†é¡ãƒ¢ãƒ‡ãƒ«

    ImageNetäº‹å‰è¨“ç·´æ¸ˆã¿ResNet50V2ã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
    å°‘é‡ãƒ‡ãƒ¼ã‚¿ï¼ˆå„ã‚¯ãƒ©ã‚¹100æšç¨‹åº¦ï¼‰ã§ã‚‚é«˜ç²¾åº¦ã‚’å®Ÿç¾
    """

    def __init__(self, input_shape=(224, 224, 3), num_classes=6):
        """
        Parameters:
        -----------
        input_shape : tuple
            ResNet50V2ã®å…¥åŠ›ã‚µã‚¤ã‚º (224, 224, 3) ãŒæ¨™æº–
            ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ç”»åƒã¯RGBã«å¤‰æ›ã—ã¦ä½¿ç”¨
        num_classes : int
            åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°
        """
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.model = None

    def build_model(self, freeze_base=True):
        """
        Transfer Learningãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰

        Parameters:
        -----------
        freeze_base : bool
            ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’å‡çµã™ã‚‹ã‹ã©ã†ã‹
            True: Feature Extractorã¨ã—ã¦ä½¿ç”¨ï¼ˆåˆæœŸè¨“ç·´ï¼‰
            False: Fine-tuningï¼ˆ2æ®µéšç›®ã®è¨“ç·´ï¼‰

        Strategy:
        ---------
        1. ImageNetäº‹å‰è¨“ç·´æ¸ˆã¿ResNet50V2ã‚’ãƒ™ãƒ¼ã‚¹ã«
        2. æœ€çµ‚å±¤ã‚’åŠå°ä½“æ¬ é™¥åˆ†é¡ç”¨ã«ç½®ãæ›ãˆ
        3. 2æ®µéšè¨“ç·´: (1) Top layersã®ã¿è¨“ç·´ â†’ (2) å…¨ä½“Fine-tuning
        """
        # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆImageNeté‡ã¿ï¼‰
        base_model = ResNet50V2(
            weights='imagenet',
            include_top=False,  # åˆ†é¡å±¤ã¯é™¤å¤–
            input_shape=self.input_shape
        )

        # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å‡çµè¨­å®š
        base_model.trainable = not freeze_base

        # ã‚«ã‚¹ã‚¿ãƒ ãƒ˜ãƒƒãƒ‰æ§‹ç¯‰
        model = models.Sequential([
            # å…¥åŠ›å±¤ï¼ˆã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«â†’RGBå¤‰æ›ç”¨ï¼‰
            layers.InputLayer(input_shape=self.input_shape),

            # ResNet50V2ãƒ™ãƒ¼ã‚¹
            base_model,

            # Global Average Pooling
            layers.GlobalAveragePooling2D(),

            # åˆ†é¡ãƒ˜ãƒƒãƒ‰
            layers.BatchNormalization(),
            layers.Dense(512, activation='relu'),
            layers.Dropout(0.5),
            layers.BatchNormalization(),
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(self.num_classes, activation='softmax')
        ])

        # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
        if freeze_base:
            # åˆæœŸè¨“ç·´: é«˜ã‚ã®å­¦ç¿’ç‡
            learning_rate = 0.001
        else:
            # Fine-tuning: ä½ã‚ã®å­¦ç¿’ç‡ï¼ˆäº‹å‰è¨“ç·´é‡ã¿ã‚’å£Šã•ãªã„ï¼‰
            learning_rate = 0.0001

        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
            loss='categorical_crossentropy',
            metrics=['accuracy', tf.keras.metrics.Precision(),
                    tf.keras.metrics.Recall()]
        )

        self.model = model
        return model

    def two_stage_training(self, X_train, y_train, X_val, y_val,
                          stage1_epochs=20, stage2_epochs=30, batch_size=16):
        """
        2æ®µéšè¨“ç·´æˆ¦ç•¥

        Stage 1: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å‡çµã€Top layersã®ã¿è¨“ç·´
        Stage 2: å…¨ä½“Fine-tuningï¼ˆä½å­¦ç¿’ç‡ï¼‰

        ã“ã®æˆ¦ç•¥ã«ã‚ˆã‚Šã€å°‘é‡ãƒ‡ãƒ¼ã‚¿ã§ã‚‚éå­¦ç¿’ã‚’é˜²ãã¤ã¤é«˜ç²¾åº¦ã‚’å®Ÿç¾
        """
        print("========== Stage 1: Training Top Layers ==========")

        # Stage 1: ãƒ™ãƒ¼ã‚¹å‡çµ
        self.build_model(freeze_base=True)

        callbacks_stage1 = [
            tf.keras.callbacks.EarlyStopping(
                monitor='val_loss', patience=5, restore_best_weights=True
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss', factor=0.5, patience=3
            )
        ]

        history_stage1 = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=stage1_epochs,
            batch_size=batch_size,
            callbacks=callbacks_stage1,
            verbose=1
        )

        print("\n========== Stage 2: Fine-tuning Entire Model ==========")

        # Stage 2: å…¨ä½“Fine-tuning
        # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å¾ŒåŠãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿è§£å‡ï¼ˆå‰åŠã¯æ±ç”¨ç‰¹å¾´ãªã®ã§ä¿æŒï¼‰
        base_model = self.model.layers[1]
        base_model.trainable = True

        # å‰åŠ100å±¤ã¯å‡çµç¶­æŒï¼ˆResNet50V2ã¯å…¨175å±¤ï¼‰
        for layer in base_model.layers[:100]:
            layer.trainable = False

        # ä½å­¦ç¿’ç‡ã§å†ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
        self.model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
            loss='categorical_crossentropy',
            metrics=['accuracy', tf.keras.metrics.Precision(),
                    tf.keras.metrics.Recall()]
        )

        callbacks_stage2 = [
            tf.keras.callbacks.EarlyStopping(
                monitor='val_loss', patience=7, restore_best_weights=True
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7
            ),
            tf.keras.callbacks.ModelCheckpoint(
                'best_transfer_model.h5',
                monitor='val_accuracy',
                save_best_only=True
            )
        ]

        history_stage2 = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=stage2_epochs,
            batch_size=batch_size,
            callbacks=callbacks_stage2,
            verbose=1
        )

        return history_stage1, history_stage2


# ========== ä½¿ç”¨ä¾‹ ==========
# ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ç”»åƒã‚’RGBã«å¤‰æ›
def grayscale_to_rgb(images):
    """ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ« (H, W, 1) â†’ RGB (H, W, 3) å¤‰æ›"""
    return np.repeat(images, 3, axis=-1)

# å°‘é‡ãƒ‡ãƒ¼ã‚¿ã§ã®Transfer Learningå®Ÿè¨¼
X_train_small = np.random.randn(600, 224, 224, 1).astype(np.float32)  # å„ã‚¯ãƒ©ã‚¹100æš
y_train_small = np.eye(6)[np.random.randint(0, 6, 600)]
X_val_small = np.random.randn(120, 224, 224, 1).astype(np.float32)
y_val_small = np.eye(6)[np.random.randint(0, 6, 120)]

# RGBå¤‰æ›
X_train_rgb = grayscale_to_rgb(X_train_small)
X_val_rgb = grayscale_to_rgb(X_val_small)

# æ­£è¦åŒ–
X_train_rgb = (X_train_rgb - X_train_rgb.min()) / (X_train_rgb.max() - X_train_rgb.min())
X_val_rgb = (X_val_rgb - X_val_rgb.min()) / (X_val_rgb.max() - X_val_rgb.min())

# è¨“ç·´
tl_classifier = TransferLearningDefectClassifier(input_shape=(224, 224, 3), num_classes=6)
history1, history2 = tl_classifier.two_stage_training(
    X_train_rgb, y_train_small,
    X_val_rgb, y_val_small,
    stage1_epochs=15,
    stage2_epochs=20,
    batch_size=16
)

print("\nTransfer Learningå®Œäº†: best_transfer_model.h5ã«ä¿å­˜")
print("å°‘é‡ãƒ‡ãƒ¼ã‚¿ï¼ˆå„ã‚¯ãƒ©ã‚¹100æšï¼‰ã§ã‚‚é«˜ç²¾åº¦ã‚’å®Ÿç¾")
</code></pre>
            </section>

            <section>
                <h2>2.3 Semantic Segmentationã«ã‚ˆã‚‹æ¬ é™¥ä½ç½®ç‰¹å®š</h2>

                <h3>2.3.1 Semantic Segmentationã¨ã¯</h3>
                <p>ç”»åƒåˆ†é¡ã¯ã€Œæ¬ é™¥ã®æœ‰ç„¡ã€ã‚’åˆ¤å®šã—ã¾ã™ãŒã€Semantic Segmentationã¯ã€Œã©ã“ã«æ¬ é™¥ãŒã‚ã‚‹ã‹ã€ã‚’ãƒ”ã‚¯ã‚»ãƒ«ãƒ¬ãƒ™ãƒ«ã§ç‰¹å®šã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šï¼š</p>
                <ul>
                    <li><strong>æ¬ é™¥ã®æ­£ç¢ºãªä½ç½®</strong>: åº§æ¨™ã¨ã‚µã‚¤ã‚ºã‚’è‡ªå‹•å–å¾—</li>
                    <li><strong>è¤‡æ•°æ¬ é™¥ã®åŒæ™‚æ¤œå‡º</strong>: 1æšã®ç”»åƒã«è¤‡æ•°æ¬ é™¥ãŒã‚ã‚‹å ´åˆã‚‚å¯¾å¿œ</li>
                    <li><strong>æ¬ é™¥å½¢çŠ¶ã®è§£æ</strong>: é¢ç©ã€å‘¨å›²é•·ã€ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’è¨ˆç®—å¯èƒ½</li>
                    <li><strong>ãƒ—ãƒ­ã‚»ã‚¹è¨ºæ–­</strong>: æ¬ é™¥åˆ†å¸ƒãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰åŸå› å·¥ç¨‹ã‚’ç‰¹å®š</li>
                </ul>

                <h3>2.3.2 U-Netã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h3>
                <p>U-Netã¯åŒ»ç™‚ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã§é–‹ç™ºã•ã‚ŒãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã€åŠå°ä½“æ¬ é™¥æ¤œå‡ºã«ã‚‚æœ€é©ã§ã™ï¼š</p>
                <div style="background: #f8f9fa; padding: 1rem; border-radius: 8px; margin: 1rem 0;">
                    <p><strong>Encoder (ç¸®å°çµŒè·¯)</strong>: ç•³ã¿è¾¼ã¿ + ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã§ç‰¹å¾´æŠ½å‡º</p>
                    <p><strong>Decoder (æ‹¡å¤§çµŒè·¯)</strong>: ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§å…ƒè§£åƒåº¦ã«å¾©å…ƒ</p>
                    <p><strong>Skip Connections</strong>: Encoder-Decoderé–“ã§ç‰¹å¾´ãƒãƒƒãƒ—ã‚’çµåˆã—ã€è©³ç´°æƒ…å ±ã‚’ä¿æŒ</p>
                </div>

                <h3>2.3.3 U-Netã«ã‚ˆã‚‹æ¬ é™¥ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè£…</h3>

<pre><code class="language-python">import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

class UNetDefectSegmentation:
    """
    U-Netã«ã‚ˆã‚‹åŠå°ä½“æ¬ é™¥ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³

    å…¥åŠ›: ã‚¦ã‚§ãƒç”»åƒ (H, W, 1)
    å‡ºåŠ›: ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ (H, W, num_classes)
           - èƒŒæ™¯ (æ­£å¸¸é ˜åŸŸ)
           - æ¬ é™¥é ˜åŸŸ (è¤‡æ•°ã‚¿ã‚¤ãƒ—å¯¾å¿œ)

    å¿œç”¨ä¾‹:
    - ãƒ‘ãƒ¼ãƒ†ã‚£ã‚¯ãƒ«ä½ç½®ç‰¹å®š
    - ã‚¹ã‚¯ãƒ©ãƒƒãƒé ˜åŸŸæŠ½å‡º
    - ãƒ‘ã‚¿ãƒ¼ãƒ³æ¬ é™¥ã®å½¢çŠ¶è§£æ
    """

    def __init__(self, input_shape=(256, 256, 1), num_classes=2):
        """
        Parameters:
        -----------
        input_shape : tuple
            å…¥åŠ›ç”»åƒã‚µã‚¤ã‚º (height, width, channels)
        num_classes : int
            ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¯ãƒ©ã‚¹æ•°
            2: èƒŒæ™¯ vs æ¬ é™¥ï¼ˆBinary Segmentationï¼‰
            6+1: å„æ¬ é™¥ã‚¿ã‚¤ãƒ— + èƒŒæ™¯ï¼ˆMulti-class Segmentationï¼‰
        """
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.model = None

    def conv_block(self, inputs, num_filters):
        """
        ç•³ã¿è¾¼ã¿ãƒ–ãƒ­ãƒƒã‚¯: Conv â†’ BatchNorm â†’ ReLU (Ã—2)

        U-Netã®åŸºæœ¬æ§‹æˆè¦ç´ 
        """
        x = layers.Conv2D(num_filters, 3, padding='same')(inputs)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('relu')(x)

        x = layers.Conv2D(num_filters, 3, padding='same')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('relu')(x)

        return x

    def encoder_block(self, inputs, num_filters):
        """
        Encoderãƒ–ãƒ­ãƒƒã‚¯: ç•³ã¿è¾¼ã¿ â†’ ãƒ—ãƒ¼ãƒªãƒ³ã‚°

        Returns:
        --------
        x : æ¬¡ã®å±¤ã¸ã®å‡ºåŠ› (ãƒ—ãƒ¼ãƒªãƒ³ã‚°å¾Œ)
        skip : Skip Connectionç”¨ã®ç‰¹å¾´ãƒãƒƒãƒ— (ãƒ—ãƒ¼ãƒªãƒ³ã‚°å‰)
        """
        x = self.conv_block(inputs, num_filters)
        skip = x  # Skip connectionç”¨ã«ä¿å­˜
        x = layers.MaxPooling2D((2, 2))(x)
        return x, skip

    def decoder_block(self, inputs, skip_features, num_filters):
        """
        Decoderãƒ–ãƒ­ãƒƒã‚¯: ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° â†’ Skipæ¥ç¶š â†’ ç•³ã¿è¾¼ã¿

        Parameters:
        -----------
        inputs : Decoderä¸‹å±¤ã‹ã‚‰ã®å…¥åŠ›
        skip_features : Encoderã‹ã‚‰ã®skip connection
        num_filters : ãƒ•ã‚£ãƒ«ã‚¿æ•°
        """
        # ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (Transposed Convolution)
        x = layers.Conv2DTranspose(num_filters, (2, 2), strides=2,
                                   padding='same')(inputs)

        # Skip connectionã¨çµåˆ
        x = layers.Concatenate()([x, skip_features])

        # ç•³ã¿è¾¼ã¿ã§ç‰¹å¾´ã‚’èåˆ
        x = self.conv_block(x, num_filters)

        return x

    def build_unet(self):
        """
        U-Netãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰

        Architecture:
        -------------
        Encoder: 4æ®µéšã®ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (256â†’128â†’64â†’32â†’16)
        Bottleneck: æœ€ä¸‹å±¤ã®ç‰¹å¾´æŠ½å‡º
        Decoder: 4æ®µéšã®ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (16â†’32â†’64â†’128â†’256)
        Output: ãƒ”ã‚¯ã‚»ãƒ«ã”ã¨ã®ã‚¯ãƒ©ã‚¹ç¢ºç‡
        """
        inputs = layers.Input(shape=self.input_shape)

        # ========== Encoder (ç¸®å°çµŒè·¯) ==========
        # Level 1: 256 â†’ 128
        e1, skip1 = self.encoder_block(inputs, 64)

        # Level 2: 128 â†’ 64
        e2, skip2 = self.encoder_block(e1, 128)

        # Level 3: 64 â†’ 32
        e3, skip3 = self.encoder_block(e2, 256)

        # Level 4: 32 â†’ 16
        e4, skip4 = self.encoder_block(e3, 512)

        # ========== Bottleneck (æœ€ä¸‹å±¤) ==========
        bottleneck = self.conv_block(e4, 1024)

        # ========== Decoder (æ‹¡å¤§çµŒè·¯) ==========
        # Level 4: 16 â†’ 32
        d4 = self.decoder_block(bottleneck, skip4, 512)

        # Level 3: 32 â†’ 64
        d3 = self.decoder_block(d4, skip3, 256)

        # Level 2: 64 â†’ 128
        d2 = self.decoder_block(d3, skip2, 128)

        # Level 1: 128 â†’ 256
        d1 = self.decoder_block(d2, skip1, 64)

        # ========== Output Layer ==========
        if self.num_classes == 2:
            # Binary segmentation: sigmoid
            outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(d1)
        else:
            # Multi-class segmentation: softmax
            outputs = layers.Conv2D(self.num_classes, (1, 1),
                                   activation='softmax')(d1)

        model = models.Model(inputs=[inputs], outputs=[outputs],
                           name='U-Net_Defect_Segmentation')

        self.model = model
        return model

    def dice_coefficient(self, y_true, y_pred, smooth=1e-6):
        """
        Diceä¿‚æ•° (F1-scoreã®ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç‰ˆ)

        $$\text{Dice} = \frac{2|X \cap Y|}{|X| + |Y|}$$

        ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç²¾åº¦ã®ä¸»è¦æŒ‡æ¨™
        """
        y_true_f = tf.keras.backend.flatten(y_true)
        y_pred_f = tf.keras.backend.flatten(y_pred)
        intersection = tf.keras.backend.sum(y_true_f * y_pred_f)
        return (2. * intersection + smooth) / (
            tf.keras.backend.sum(y_true_f) +
            tf.keras.backend.sum(y_pred_f) + smooth
        )

    def dice_loss(self, y_true, y_pred):
        """Dice loss = 1 - Diceä¿‚æ•°"""
        return 1 - self.dice_coefficient(y_true, y_pred)

    def compile_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«"""
        if self.num_classes == 2:
            # Binary segmentation
            loss = self.dice_loss
            metrics = ['accuracy', self.dice_coefficient]
        else:
            # Multi-class segmentation
            loss = 'categorical_crossentropy'
            metrics = ['accuracy', self.dice_coefficient]

        self.model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            loss=loss,
            metrics=metrics
        )

    def train(self, X_train, y_train, X_val, y_val,
              epochs=50, batch_size=8):
        """
        è¨“ç·´å®Ÿè¡Œ

        Parameters:
        -----------
        X_train : ndarray
            è¨“ç·´ç”»åƒ (N, H, W, C)
        y_train : ndarray
            è¨“ç·´ãƒã‚¹ã‚¯ (N, H, W, num_classes) or (N, H, W, 1) for binary
        """
        if self.model is None:
            self.build_unet()
            self.compile_model()

        callbacks = [
            tf.keras.callbacks.ModelCheckpoint(
                'best_unet_segmentation.h5',
                monitor='val_dice_coefficient',
                mode='max',
                save_best_only=True,
                verbose=1
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-7
            ),
            tf.keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=15,
                restore_best_weights=True
            )
        ]

        history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )

        return history

    def predict_and_visualize(self, image, threshold=0.5):
        """
        äºˆæ¸¬ã¨ãƒã‚¹ã‚¯å¯è¦–åŒ–

        Parameters:
        -----------
        image : ndarray
            å…¥åŠ›ç”»åƒ (H, W, 1)
        threshold : float
            Binary segmentationã®é–¾å€¤

        Returns:
        --------
        mask : ndarray
            äºˆæ¸¬ãƒã‚¹ã‚¯ (H, W)
        """
        # äºˆæ¸¬
        image_batch = np.expand_dims(image, axis=0)
        pred_mask = self.model.predict(image_batch)[0]

        if self.num_classes == 2:
            # Binary: é–¾å€¤å‡¦ç†
            mask = (pred_mask[:, :, 0] > threshold).astype(np.uint8)
        else:
            # Multi-class: argmax
            mask = np.argmax(pred_mask, axis=-1)

        # å¯è¦–åŒ–
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        axes[0].imshow(image[:, :, 0], cmap='gray')
        axes[0].set_title('Original Image')
        axes[0].axis('off')

        axes[1].imshow(mask, cmap='jet')
        axes[1].set_title('Predicted Mask')
        axes[1].axis('off')

        # ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤
        overlay = image[:, :, 0].copy()
        overlay[mask > 0] = 1.0  # æ¬ é™¥éƒ¨åˆ†ã‚’å¼·èª¿
        axes[2].imshow(overlay, cmap='gray')
        axes[2].set_title('Defect Overlay')
        axes[2].axis('off')

        plt.tight_layout()
        plt.savefig('segmentation_result.png', dpi=300, bbox_inches='tight')
        plt.show()

        return mask


# ========== ä½¿ç”¨ä¾‹ ==========
if __name__ == "__main__":
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)

    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 800æš
    X_train = np.random.randn(800, 256, 256, 1).astype(np.float32)
    # ãƒã‚¹ã‚¯: Binary (èƒŒæ™¯=0, æ¬ é™¥=1)
    y_train = np.random.randint(0, 2, (800, 256, 256, 1)).astype(np.float32)

    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: 200æš
    X_val = np.random.randn(200, 256, 256, 1).astype(np.float32)
    y_val = np.random.randint(0, 2, (200, 256, 256, 1)).astype(np.float32)

    # æ­£è¦åŒ–
    X_train = (X_train - X_train.min()) / (X_train.max() - X_train.min())
    X_val = (X_val - X_val.min()) / (X_val.max() - X_val.min())

    # U-Netãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    segmenter = UNetDefectSegmentation(input_shape=(256, 256, 1), num_classes=2)
    segmenter.build_unet()

    print("U-Net Model Architecture:")
    segmenter.model.summary()

    # è¨“ç·´
    print("\n========== Training U-Net ==========")
    history = segmenter.train(
        X_train, y_train,
        X_val, y_val,
        epochs=30,
        batch_size=8
    )

    # ãƒ†ã‚¹ãƒˆç”»åƒã§äºˆæ¸¬
    print("\n========== Prediction Example ==========")
    test_image = X_val[0]
    pred_mask = segmenter.predict_and_visualize(test_image, threshold=0.5)

    # æ¬ é™¥é ˜åŸŸã®çµ±è¨ˆ
    defect_pixels = np.sum(pred_mask > 0)
    total_pixels = pred_mask.size
    defect_ratio = defect_pixels / total_pixels * 100

    print(f"\nDefect Detection Results:")
    print(f"  Total pixels: {total_pixels}")
    print(f"  Defect pixels: {defect_pixels}")
    print(f"  Defect coverage: {defect_ratio:.2f}%")

    print("\nBest model saved to: best_unet_segmentation.h5")
</code></pre>
            </section>

            <section>
                <h2>2.4 Autoencoderã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥</h2>

                <h3>2.4.1 æ•™å¸«ãªã—ç•°å¸¸æ¤œçŸ¥ã®å¿…è¦æ€§</h3>
                <p>åŠå°ä½“è£½é€ ã§ã¯ã€æœªçŸ¥ã®æ–°è¦æ¬ é™¥ãŒé »ç¹ã«ç™ºç”Ÿã—ã¾ã™ã€‚æ•™å¸«ã‚ã‚Šå­¦ç¿’ã§ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œãªã„æ¬ é™¥ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã€‚Autoencoderã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥ã¯ï¼š</p>
                <ul>
                    <li><strong>æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§è¨“ç·´</strong>: æ¬ é™¥ãƒ‡ãƒ¼ã‚¿åé›†ãŒä¸è¦</li>
                    <li><strong>æœªçŸ¥æ¬ é™¥ã®æ¤œå‡º</strong>: è¨“ç·´æ™‚ã«è¦‹ã¦ã„ãªã„ç•°å¸¸ã‚‚æ¤œå‡ºå¯èƒ½</li>
                    <li><strong>å†æ§‹æˆèª¤å·®ãƒ™ãƒ¼ã‚¹</strong>: æ­£å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰é€¸è„±ã—ãŸé ˜åŸŸã‚’è‡ªå‹•æ¤œå‡º</li>
                    <li><strong>ç¶™ç¶šçš„å­¦ç¿’</strong>: æ­£å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ›´æ–°ãŒå®¹æ˜“</li>
                </ul>

                <h3>2.4.2 Convolutional Autoencoderã®åŸç†</h3>
                <div style="background: #f8f9fa; padding: 1rem; border-radius: 8px; margin: 1rem 0;">
                    <p><strong>Encoder</strong>: å…¥åŠ›ç”»åƒã‚’ä½æ¬¡å…ƒã®æ½œåœ¨è¡¨ç¾ã«åœ§ç¸®</p>
                    <p>$$z = f_{\text{enc}}(x; \theta_{\text{enc}})$$</p>

                    <p><strong>Decoder</strong>: æ½œåœ¨è¡¨ç¾ã‹ã‚‰å…ƒã®ç”»åƒã‚’å†æ§‹æˆ</p>
                    <p>$$\hat{x} = f_{\text{dec}}(z; \theta_{\text{dec}})$$</p>

                    <p><strong>å†æ§‹æˆèª¤å·®</strong>: æ­£å¸¸ç”»åƒã¯å°ã•ãã€ç•°å¸¸ç”»åƒã¯å¤§ãããªã‚‹</p>
                    <p>$$\text{Error} = \|x - \hat{x}\|^2$$</p>
                </div>

                <h3>2.4.3 å®Ÿè£…ä¾‹ï¼šConvolutional Autoencoder</h3>

<pre><code class="language-python">import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

class ConvolutionalAutoencoder:
    """
    Convolutional Autoencoderã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥

    Training: æ­£å¸¸ã‚¦ã‚§ãƒç”»åƒã®ã¿ã§è¨“ç·´
    Inference: å†æ§‹æˆèª¤å·®ãŒé–¾å€¤ã‚’è¶…ãˆãŸã‚‰ç•°å¸¸ã¨åˆ¤å®š

    å¿œç”¨ä¾‹:
    - æ–°è¦æ¬ é™¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è‡ªå‹•æ¤œå‡º
    - ãƒ—ãƒ­ã‚»ã‚¹ç•°å¸¸ã®æ—©æœŸç™ºè¦‹
    - å¾®å°ãªå“è³ªåŠ£åŒ–ã®æ¤œçŸ¥
    """

    def __init__(self, input_shape=(128, 128, 1), latent_dim=128):
        """
        Parameters:
        -----------
        input_shape : tuple
            å…¥åŠ›ç”»åƒã‚µã‚¤ã‚º
        latent_dim : int
            æ½œåœ¨ç©ºé–“ã®æ¬¡å…ƒæ•°
            å°ã•ã„: å¼·ã„åœ§ç¸®ã€å³ã—ã„ç•°å¸¸æ¤œçŸ¥
            å¤§ãã„: ç·©ã„åœ§ç¸®ã€ç·©ã„ç•°å¸¸æ¤œçŸ¥
        """
        self.input_shape = input_shape
        self.latent_dim = latent_dim
        self.autoencoder = None
        self.encoder = None
        self.decoder = None
        self.threshold = None  # ç•°å¸¸åˆ¤å®šé–¾å€¤

    def build_encoder(self):
        """
        Encoderæ§‹ç¯‰: ç”»åƒ â†’ æ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ«

        128Ã—128 â†’ 64Ã—64 â†’ 32Ã—32 â†’ 16Ã—16 â†’ 8Ã—8 â†’ latent_dim
        """
        inputs = layers.Input(shape=self.input_shape)

        # Encoderå±¤
        x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
        x = layers.MaxPooling2D((2, 2), padding='same')(x)  # 64Ã—64

        x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)
        x = layers.MaxPooling2D((2, 2), padding='same')(x)  # 32Ã—32

        x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)
        x = layers.MaxPooling2D((2, 2), padding='same')(x)  # 16Ã—16

        x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)
        x = layers.MaxPooling2D((2, 2), padding='same')(x)  # 8Ã—8

        # Flatten â†’ Dense (æ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ«)
        x = layers.Flatten()(x)
        latent = layers.Dense(self.latent_dim, activation='relu',
                             name='latent_vector')(x)

        encoder = models.Model(inputs, latent, name='encoder')
        return encoder

    def build_decoder(self):
        """
        Decoderæ§‹ç¯‰: æ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ« â†’ ç”»åƒ

        latent_dim â†’ 8Ã—8 â†’ 16Ã—16 â†’ 32Ã—32 â†’ 64Ã—64 â†’ 128Ã—128
        """
        latent_inputs = layers.Input(shape=(self.latent_dim,))

        # Dense â†’ Reshape
        x = layers.Dense(8 * 8 * 256, activation='relu')(latent_inputs)
        x = layers.Reshape((8, 8, 256))(x)

        # Decoderå±¤ (UpSampling + Conv2D)
        x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)
        x = layers.UpSampling2D((2, 2))(x)  # 16Ã—16

        x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)
        x = layers.UpSampling2D((2, 2))(x)  # 32Ã—32

        x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)
        x = layers.UpSampling2D((2, 2))(x)  # 64Ã—64

        x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)
        x = layers.UpSampling2D((2, 2))(x)  # 128Ã—128

        # å‡ºåŠ›å±¤ (sigmoid: 0-1ç¯„å›²)
        outputs = layers.Conv2D(1, (3, 3), activation='sigmoid',
                               padding='same')(x)

        decoder = models.Model(latent_inputs, outputs, name='decoder')
        return decoder

    def build_autoencoder(self):
        """Autoencoderæ§‹ç¯‰ (Encoder + Decoder)"""
        self.encoder = self.build_encoder()
        self.decoder = self.build_decoder()

        # æ¥ç¶š
        inputs = layers.Input(shape=self.input_shape)
        latent = self.encoder(inputs)
        outputs = self.decoder(latent)

        self.autoencoder = models.Model(inputs, outputs,
                                       name='convolutional_autoencoder')

        # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ« (MSE loss)
        self.autoencoder.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            loss='mse',  # Mean Squared Error
            metrics=['mae']  # Mean Absolute Error
        )

        return self.autoencoder

    def train(self, X_normal, validation_split=0.2, epochs=50, batch_size=32):
        """
        æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§è¨“ç·´

        Parameters:
        -----------
        X_normal : ndarray
            æ­£å¸¸ç”»åƒã®ã¿ (N, H, W, C)
            *** ç•°å¸¸ç”»åƒã‚’å«ã‚ã¦ã¯ã„ã‘ãªã„ ***
        """
        if self.autoencoder is None:
            self.build_autoencoder()

        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-7
            ),
            tf.keras.callbacks.ModelCheckpoint(
                'best_autoencoder.h5',
                monitor='val_loss',
                save_best_only=True
            )
        ]

        # è¨“ç·´ (å…¥åŠ›=å‡ºåŠ›)
        history = self.autoencoder.fit(
            X_normal, X_normal,  # è‡ªå·±æ•™å¸«ã‚ã‚Š
            validation_split=validation_split,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )

        return history

    def calculate_reconstruction_errors(self, X):
        """
        å†æ§‹æˆèª¤å·®ã‚’è¨ˆç®—

        Returns:
        --------
        errors : ndarray
            å„ç”»åƒã®å†æ§‹æˆèª¤å·® (N,)
        """
        X_reconstructed = self.autoencoder.predict(X)

        # ç”»åƒã”ã¨ã®MSE
        errors = np.mean((X - X_reconstructed) ** 2, axis=(1, 2, 3))

        return errors

    def set_threshold(self, X_normal, percentile=95):
        """
        ç•°å¸¸åˆ¤å®šé–¾å€¤ã‚’è¨­å®š

        Parameters:
        -----------
        X_normal : ndarray
            æ­£å¸¸ç”»åƒã‚µãƒ³ãƒ—ãƒ«
        percentile : float
            ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ« (95%ãªã‚‰ä¸Šä½5%ã‚’ç•°å¸¸ã¨ã™ã‚‹)

        Strategy:
        ---------
        æ­£å¸¸ç”»åƒã®å†æ§‹æˆèª¤å·®åˆ†å¸ƒã‚’èª¿ã¹ã€
        percentileç‚¹ã‚’é–¾å€¤ã¨ã™ã‚‹
        """
        errors = self.calculate_reconstruction_errors(X_normal)
        self.threshold = np.percentile(errors, percentile)

        print(f"ç•°å¸¸åˆ¤å®šé–¾å€¤è¨­å®šå®Œäº†: {self.threshold:.6f}")
        print(f"  (æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®{percentile}ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«)")

        return self.threshold

    def detect_anomalies(self, X):
        """
        ç•°å¸¸æ¤œçŸ¥å®Ÿè¡Œ

        Returns:
        --------
        is_anomaly : ndarray (bool)
            å„ç”»åƒãŒç•°å¸¸ã‹ã©ã†ã‹ (N,)
        errors : ndarray
            å†æ§‹æˆèª¤å·® (N,)
        """
        if self.threshold is None:
            raise ValueError("é–¾å€¤ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚set_threshold()ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„")

        errors = self.calculate_reconstruction_errors(X)
        is_anomaly = errors > self.threshold

        return is_anomaly, errors

    def visualize_results(self, X_test, num_samples=5):
        """
        å†æ§‹æˆçµæœã®å¯è¦–åŒ–

        æ­£å¸¸/ç•°å¸¸ãã‚Œãã‚Œã«ã¤ã„ã¦ã€
        å…ƒç”»åƒã€å†æ§‹æˆç”»åƒã€å·®åˆ†ç”»åƒã‚’è¡¨ç¤º
        """
        is_anomaly, errors = self.detect_anomalies(X_test)
        X_reconstructed = self.autoencoder.predict(X_test)

        # æ­£å¸¸ã‚µãƒ³ãƒ—ãƒ«
        normal_indices = np.where(~is_anomaly)[0][:num_samples]
        # ç•°å¸¸ã‚µãƒ³ãƒ—ãƒ«
        anomaly_indices = np.where(is_anomaly)[0][:num_samples]

        fig, axes = plt.subplots(4, num_samples, figsize=(15, 10))

        for i, idx in enumerate(normal_indices):
            # å…ƒç”»åƒ
            axes[0, i].imshow(X_test[idx, :, :, 0], cmap='gray')
            axes[0, i].set_title(f'Normal\nError={errors[idx]:.4f}')
            axes[0, i].axis('off')

            # å†æ§‹æˆç”»åƒ
            axes[1, i].imshow(X_reconstructed[idx, :, :, 0], cmap='gray')
            axes[1, i].set_title('Reconstructed')
            axes[1, i].axis('off')

        for i, idx in enumerate(anomaly_indices):
            # å…ƒç”»åƒ
            axes[2, i].imshow(X_test[idx, :, :, 0], cmap='gray')
            axes[2, i].set_title(f'Anomaly\nError={errors[idx]:.4f}')
            axes[2, i].axis('off')

            # å·®åˆ†ç”»åƒ
            diff = np.abs(X_test[idx, :, :, 0] - X_reconstructed[idx, :, :, 0])
            axes[3, i].imshow(diff, cmap='hot')
            axes[3, i].set_title('Difference')
            axes[3, i].axis('off')

        plt.tight_layout()
        plt.savefig('anomaly_detection_results.png', dpi=300, bbox_inches='tight')
        plt.show()


# ========== ä½¿ç”¨ä¾‹ ==========
if __name__ == "__main__":
    np.random.seed(42)

    # æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ (1000æš)
    X_normal = np.random.randn(1000, 128, 128, 1).astype(np.float32)
    X_normal = (X_normal - X_normal.min()) / (X_normal.max() - X_normal.min())

    # ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ (100æš) - ãƒã‚¤ã‚ºã‚’è¿½åŠ ã—ã¦ç•°å¸¸ã‚’æ¨¡æ“¬
    X_anomaly = np.random.randn(100, 128, 128, 1).astype(np.float32)
    X_anomaly = (X_anomaly - X_anomaly.min()) / (X_anomaly.max() - X_anomaly.min())
    X_anomaly += np.random.randn(100, 128, 128, 1) * 0.3  # å¼·ã„ãƒã‚¤ã‚º
    X_anomaly = np.clip(X_anomaly, 0, 1)

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ (æ­£å¸¸+ç•°å¸¸)
    X_test = np.vstack([X_normal[-50:], X_anomaly[:50]])
    y_test = np.array([0]*50 + [1]*50)  # 0=æ­£å¸¸, 1=ç•°å¸¸

    # Autoencoderæ§‹ç¯‰ãƒ»è¨“ç·´
    ae = ConvolutionalAutoencoder(input_shape=(128, 128, 1), latent_dim=128)
    ae.build_autoencoder()

    print("Autoencoder Architecture:")
    ae.autoencoder.summary()

    # æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§è¨“ç·´
    print("\n========== Training on Normal Data Only ==========")
    history = ae.train(
        X_normal[:900],  # è¨“ç·´ç”¨æ­£å¸¸ãƒ‡ãƒ¼ã‚¿
        validation_split=0.2,
        epochs=30,
        batch_size=32
    )

    # é–¾å€¤è¨­å®š
    print("\n========== Setting Anomaly Threshold ==========")
    ae.set_threshold(X_normal[900:950], percentile=95)

    # ç•°å¸¸æ¤œçŸ¥
    print("\n========== Anomaly Detection ==========")
    is_anomaly, errors = ae.detect_anomalies(X_test)

    # è©•ä¾¡
    from sklearn.metrics import classification_report, roc_auc_score

    print("\nClassification Report:")
    print(classification_report(y_test, is_anomaly.astype(int),
                               target_names=['Normal', 'Anomaly']))

    auc = roc_auc_score(y_test, errors)
    print(f"\nAUC-ROC Score: {auc:.4f}")

    # å¯è¦–åŒ–
    ae.visualize_results(X_test, num_samples=5)

    print("\nBest model saved to: best_autoencoder.h5")
</code></pre>
            </section>

            <section>
                <h2>2.5 ã¾ã¨ã‚</h2>
                <p>æœ¬ç« ã§ã¯ã€Deep Learningã«ã‚ˆã‚‹åŠå°ä½“æ¬ é™¥æ¤œæŸ»ã®3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å­¦ç¿’ã—ã¾ã—ãŸï¼š</p>

                <div style="background: #f0f8ff; padding: 1.5rem; border-radius: 8px; margin: 1.5rem 0;">
                    <h3 style="margin-top: 0;">ä¸»è¦ãªå­¦ç¿’å†…å®¹</h3>

                    <h4>1. CNNã«ã‚ˆã‚‹æ¬ é™¥åˆ†é¡</h4>
                    <ul>
                        <li><strong>6ç¨®é¡ã®æ¬ é™¥ã‚¿ã‚¤ãƒ—ã‚’é«˜ç²¾åº¦åˆ†é¡</strong> (Accuracy 99%ä»¥ä¸Š)</li>
                        <li><strong>Data Augmentation</strong>ã§å°‘é‡ãƒ‡ãƒ¼ã‚¿ã§ã‚‚é«˜æ€§èƒ½</li>
                        <li><strong>Transfer Learning</strong>ã§ã•ã‚‰ãªã‚‹ç²¾åº¦å‘ä¸Šã¨è¨“ç·´æ™‚é–“çŸ­ç¸®</li>
                    </ul>

                    <h4>2. Semantic Segmentationã«ã‚ˆã‚‹æ¬ é™¥ä½ç½®ç‰¹å®š</h4>
                    <ul>
                        <li><strong>U-Netã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</strong>ã§ãƒ”ã‚¯ã‚»ãƒ«ãƒ¬ãƒ™ãƒ«ã®æ¬ é™¥æ¤œå‡º</li>
                        <li><strong>æ¬ é™¥ã®æ­£ç¢ºãªä½ç½®ãƒ»ã‚µã‚¤ã‚ºãƒ»å½¢çŠ¶</strong>ã‚’è‡ªå‹•æ¸¬å®š</li>
                        <li><strong>Diceä¿‚æ•°</strong>ã§ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç²¾åº¦ã‚’è©•ä¾¡</li>
                    </ul>

                    <h4>3. Autoencoderã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥</h4>
                    <ul>
                        <li><strong>æ•™å¸«ãªã—å­¦ç¿’</strong>ã§æœªçŸ¥ã®æ¬ é™¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º</li>
                        <li><strong>å†æ§‹æˆèª¤å·®ãƒ™ãƒ¼ã‚¹</strong>ã®åˆ¤å®šã§é©å¿œæ€§ãŒé«˜ã„</li>
                        <li><strong>ç¶™ç¶šçš„å­¦ç¿’</strong>ã§æ–°è¦ãƒ—ãƒ­ã‚»ã‚¹ã«å³å¿œ</li>
                    </ul>
                </div>

                <h3>æ¬¡ç« ã¸ã®å±•é–‹</h3>
                <p>ç¬¬3ç« ã€Œæ­©ç•™ã¾ã‚Šå‘ä¸Šã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã€ã§ã¯ã€æœ¬ç« ã§æ¤œå‡ºã—ãŸæ¬ é™¥æƒ…å ±ã‚’æ´»ç”¨ã—ã¦ã€ãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶ã‚’æœ€é©åŒ–ã™ã‚‹æ‰‹æ³•ã‚’å­¦ã³ã¾ã™ï¼š</p>
                <ul>
                    <li>æ¬ é™¥ãƒ‡ãƒ¼ã‚¿ã¨æ­©ç•™ã¾ã‚Šã®ç›¸é–¢åˆ†æ</li>
                    <li>Bayesian Optimizationã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</li>
                    <li>å¤šç›®çš„æœ€é©åŒ–ã§å“è³ªãƒ»ã‚³ã‚¹ãƒˆãƒ»ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’åŒæ™‚æ”¹å–„</li>
                    <li>å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡</li>
                </ul>
            </section>

            <div class="chapter-navigation">
                <a href="chapter-1.html" class="btn btn-secondary">â† å‰ã®ç« </a>
                <a href="index.html" class="btn btn-secondary">ç›®æ¬¡ã«æˆ»ã‚‹</a>
                <a href="chapter-3.html" class="btn btn-primary">æ¬¡ã®ç«  â†’</a>
            </div>
        </article>
    </main>

    <footer class="site-footer">
        <div class="container">
            <p>&copy; 2025 Yusuke Hashimoto Laboratory, Tohoku University. All rights reserved.</p>
        </div>
    </footer>

    <script src="/assets/js/main.js"></script>
</body>
</html>
