<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="ç¬¬2ç« ï¼šäºˆçŸ¥ä¿å…¨ã¨RULæ¨å®š - åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆã«ãŠã‘ã‚‹AIãƒ™ãƒ¼ã‚¹æ•…éšœäºˆæ¸¬ã€æ®‹å­˜æœ‰åŠ¹å¯¿å‘½æ¨å®šã€äºˆçŸ¥ä¿å…¨ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ãƒ¬ãƒ™ãƒ«ã§ç¿’å¾—">
    <title>ç¬¬2ç« ï¼šäºˆçŸ¥ä¿å…¨ã¨RULæ¨å®š - åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆã¸ã®AIå¿œç”¨</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #11998e;
            --color-accent-light: #38ef7d;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #11998e;
            --color-link-hover: #0d7a6f;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.35rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        ul, ol {
            margin-bottom: var(--spacing-md);
            padding-left: var(--spacing-lg);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        .code-block {
            position: relative;
            margin-bottom: var(--spacing-lg);
        }

        .code-title {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xs) var(--spacing-md);
            border-radius: var(--border-radius) var(--border-radius) 0 0;
            font-weight: 600;
            font-size: 0.9rem;
        }

        .code-title + pre {
            border-radius: 0 0 var(--border-radius) var(--border-radius);
            margin-top: 0;
        }

        .callout {
            background-color: var(--color-bg-alt);
            border-left: 4px solid var(--color-accent);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
            border-radius: 0 var(--border-radius) var(--border-radius) 0;
        }

        .callout-title {
            font-weight: 700;
            color: var(--color-accent);
            margin-bottom: var(--spacing-xs);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-top: var(--spacing-xl);
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
            gap: var(--spacing-md);
        }

        .nav-button {
            display: inline-flex;
            align-items: center;
            gap: var(--spacing-xs);
            padding: var(--spacing-sm) var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            text-decoration: none;
            border-radius: var(--border-radius);
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(17, 153, 142, 0.3);
        }

        .nav-button.disabled {
            background: #cbd5e0;
            cursor: not-allowed;
            transform: none;
        }

        .nav-button.disabled:hover {
            transform: none;
            box-shadow: none;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            background-color: white;
            box-shadow: var(--box-shadow);
            border-radius: var(--border-radius);
            overflow: hidden;
        }

        th, td {
            padding: var(--spacing-sm);
            text-align: left;
            border-bottom: 1px solid var(--color-border);
        }

        th {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            font-weight: 600;
        }

        tr:last-child td {
            border-bottom: none;
        }

        tr:hover {
            background-color: var(--color-bg-alt);
        }

        .mermaid {
            background-color: white;
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
            margin-bottom: var(--spacing-md);
            box-shadow: var(--box-shadow);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.35rem;
            }

            h3 {
                font-size: 1.15rem;
            }

            .navigation {
                flex-direction: column;
            }

            .nav-button {
                width: 100%;
                justify-content: center;
            }
        }

        strong {
            color: var(--color-primary-dark);
            font-weight: 600;
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            border-bottom: 1px solid transparent;
            transition: border-bottom-color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            border-bottom-color: var(--color-link-hover);
        }

        .section-intro {
            background: linear-gradient(135deg, rgba(17, 153, 142, 0.05) 0%, rgba(56, 239, 125, 0.05) 100%);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
            margin-bottom: var(--spacing-lg);
            border-left: 4px solid var(--color-accent);
        }
    </style>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬2ç« ï¼šäºˆçŸ¥ä¿å…¨ã¨RULæ¨å®š</h1>
            <p class="subtitle">æŒ¯å‹•ãƒ‡ãƒ¼ã‚¿è§£æãƒ»æ•…éšœäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ãƒ»æ®‹å­˜æœ‰åŠ¹å¯¿å‘½æ¨å®šã«ã‚ˆã‚‹è¨­å‚™ä¿å…¨ã®æœ€é©åŒ–</p>
            <div class="meta">
                <span class="meta-item">ğŸ“š ã‚·ãƒªãƒ¼ã‚º: åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆã¸ã®AIå¿œç”¨</span>
                <span class="meta-item">â±ï¸ èª­äº†æ™‚é–“: 40åˆ†</span>
                <span class="meta-item">ğŸ“ é›£æ˜“åº¦: ä¸­ç´š</span>
            </div>
        </div>
    </header>

    <div class="container">
        <div class="section-intro">
            <p><strong>ã“ã®ç« ã§å­¦ã¶ã“ã¨ï¼š</strong></p>
            <p>åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆã«ãŠã‘ã‚‹è¨­å‚™ã®æ•…éšœäºˆæ¸¬ã¨æ®‹å­˜æœ‰åŠ¹å¯¿å‘½ï¼ˆRUL: Remaining Useful Lifeï¼‰æ¨å®šã¯ã€äºˆæœŸã›ã¬ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ã‚’é˜²ãã€ä¿å…¨ã‚³ã‚¹ãƒˆã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã®é‡è¦ãªæŠ€è¡“ã§ã™ã€‚æœ¬ç« ã§ã¯ã€æŒ¯å‹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ç‰¹å¾´æŠ½å‡ºã€æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æ•…éšœãƒ¢ãƒ¼ãƒ‰åˆ†é¡ã€LSTM/TCNãªã©ã®æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹RULæ¨å®šã€ãã—ã¦å®Ÿè·µçš„ãªäºˆçŸ¥ä¿å…¨ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã¾ã§ã‚’ã€8ã¤ã®å®Ÿè£…ä¾‹ã‚’é€šã˜ã¦ç¿’å¾—ã—ã¾ã™ã€‚</p>
        </div>

        <h2>2.1 äºˆçŸ¥ä¿å…¨ã®åŸºç¤</h2>

        <p>äºˆçŸ¥ä¿å…¨ï¼ˆPredictive Maintenanceï¼‰ã¯ã€è¨­å‚™ã®çŠ¶æ…‹ã‚’ç¶™ç¶šçš„ã«ç›£è¦–ã—ã€æ•…éšœãŒç™ºç”Ÿã™ã‚‹å‰ã«ä¿å…¨ã‚’è¡Œã†æˆ¦ç•¥ã§ã™ã€‚å¾“æ¥ã®å®šæœŸä¿å…¨ã¨æ¯”è¼ƒã—ã¦ã€ä»¥ä¸‹ã®åˆ©ç‚¹ãŒã‚ã‚Šã¾ã™ï¼š</p>

        <ul>
            <li><strong>ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ã®å‰Šæ¸›</strong>ï¼šäºˆæœŸã—ãªã„æ•…éšœã«ã‚ˆã‚‹ç·Šæ€¥åœæ­¢ã‚’30-50%å‰Šæ¸›</li>
            <li><strong>ä¿å…¨ã‚³ã‚¹ãƒˆã®æœ€é©åŒ–</strong>ï¼šä¸è¦ãªå®šæœŸä¿å…¨ã‚’å‰Šæ¸›ã—ã€ã‚³ã‚¹ãƒˆã‚’12-18%å‰Šæ¸›</li>
            <li><strong>è¨­å‚™å¯¿å‘½ã®å»¶é•·</strong>ï¼šé©åˆ‡ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ã®ä¿å…¨ã«ã‚ˆã‚Šå¯¿å‘½ã‚’20-40%å»¶é•·</li>
            <li><strong>å®‰å…¨æ€§ã®å‘ä¸Š</strong>ï¼šé‡å¤§ãªæ•…éšœã‚’äº‹å‰ã«é˜²æ­¢</li>
        </ul>

        <div class="callout">
            <div class="callout-title">ğŸ’¡ äºˆçŸ¥ä¿å…¨ã®é‡è¦æ€§</div>
            <p>åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆã«ãŠã‘ã‚‹çªç™ºæ•…éšœã«ã‚ˆã‚‹æå¤±ã¯ã€1æ™‚é–“ã‚ãŸã‚Šæ•°ç™¾ä¸‡å††ã‹ã‚‰æ•°åƒä¸‡å††ã«é”ã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚2023å¹´ã®èª¿æŸ»ã§ã¯ã€äºˆçŸ¥ä¿å…¨ã‚’å°å…¥ã—ãŸä¼æ¥­ã®87%ãŒæŠ•è³‡å›åæœŸé–“2å¹´ä»¥å†…ã§ROIã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚</p>
        </div>

        <h3>2.1.1 äºˆçŸ¥ä¿å…¨ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</h3>

        <div class="mermaid">
            <pre class="mermaid">
graph LR
    A[ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿<br/>åé›†] --> B[ç‰¹å¾´æŠ½å‡º<br/>FFT/çµ±è¨ˆé‡]
    B --> C[ç•°å¸¸æ¤œçŸ¥<br/>é–¾å€¤/ML]
    C --> D[æ•…éšœè¨ºæ–­<br/>åˆ†é¡ãƒ¢ãƒ‡ãƒ«]
    D --> E[RULæ¨å®š<br/>å›å¸°ãƒ¢ãƒ‡ãƒ«]
    E --> F[ä¿å…¨è¨ˆç”»<br/>æœ€é©åŒ–]
    F --> G[å®Ÿè¡Œãƒ»æ¤œè¨¼]
    G --> A

    style A fill:#e3f2fd
    style C fill:#fff3e0
    style E fill:#f3e5f5
    style F fill:#e8f5e9
            </pre>
        </div>

        <h3>2.1.2 ä¸»è¦ãªç›£è¦–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</h3>

        <table>
            <thead>
                <tr>
                    <th>è¨­å‚™ç¨®åˆ¥</th>
                    <th>ä¸»è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</th>
                    <th>æ­£å¸¸ç¯„å›²ä¾‹</th>
                    <th>æ•…éšœãƒ¢ãƒ¼ãƒ‰</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>é å¿ƒãƒãƒ³ãƒ—</td>
                    <td>æŒ¯å‹•ï¼ˆRMSï¼‰ã€è»¸å—æ¸©åº¦</td>
                    <td>1.5-3.0 mm/s, 50-70Â°C</td>
                    <td>è»¸å—åŠ£åŒ–ã€ã‚­ãƒ£ãƒ“ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³</td>
                </tr>
                <tr>
                    <td>åœ§ç¸®æ©Ÿ</td>
                    <td>æŒ¯å‹•ã€åå‡ºåœ§åŠ›ã€æ¸©åº¦</td>
                    <td>2.0-4.5 mm/s, è¨­è¨ˆåœ§Â±5%</td>
                    <td>ãƒãƒ«ãƒ–ä¸è‰¯ã€ã‚·ãƒ¼ãƒ«æ¼ã‚Œ</td>
                </tr>
                <tr>
                    <td>ç†±äº¤æ›å™¨</td>
                    <td>æ¸©åº¦å·®ã€åœ§åŠ›æå¤±</td>
                    <td>è¨­è¨ˆÎ”TÂ±10%, Î”P<150%å®šæ ¼</td>
                    <td>ãƒ•ã‚¡ã‚¦ãƒªãƒ³ã‚°ã€ãƒãƒ¥ãƒ¼ãƒ–æ¼æ´©</td>
                </tr>
                <tr>
                    <td>å›è»¢æ©Ÿ</td>
                    <td>æŒ¯å‹•ã€é›»æµã€å›è»¢æ•°</td>
                    <td>1.0-2.5 mm/s, å®šæ ¼é›»æµÂ±10%</td>
                    <td>ã‚¢ãƒ³ãƒãƒ©ãƒ³ã‚¹ã€ãƒŸã‚¹ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ</td>
                </tr>
            </tbody>
        </table>

        <h2>2.2 å®Ÿè£…ä¾‹1ï¼šæŒ¯å‹•ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´æŠ½å‡º</h2>

        <p>ãƒãƒ³ãƒ—ã‚„åœ§ç¸®æ©Ÿã®æŒ¯å‹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€æ™‚é–“é ˜åŸŸãƒ»å‘¨æ³¢æ•°é ˜åŸŸã®ç‰¹å¾´ã‚’æŠ½å‡ºã—ã¾ã™ã€‚FFTï¼ˆé«˜é€Ÿãƒ•ãƒ¼ãƒªã‚¨å¤‰æ›ï¼‰ã«ã‚ˆã‚Šã€è»¸å—åŠ£åŒ–ã‚„ã‚¢ãƒ³ãƒãƒ©ãƒ³ã‚¹ã«ç‰¹æœ‰ã®å‘¨æ³¢æ•°æˆåˆ†ã‚’æ¤œå‡ºã§ãã¾ã™ã€‚</p>

        <div class="code-block">
            <div class="code-title">ğŸ“„ Example 1: æŒ¯å‹•ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´æŠ½å‡ºï¼ˆãƒãƒ³ãƒ—ç›£è¦–ï¼‰</div>
            <pre><code>import numpy as np
import pandas as pd
from scipy import signal
from scipy.fft import fft, fftfreq
from scipy.stats import kurtosis, skew

class VibrationFeatureExtractor:
    """æŒ¯å‹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ™‚é–“ãƒ»å‘¨æ³¢æ•°é ˜åŸŸã®ç‰¹å¾´ã‚’æŠ½å‡º

    åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆã®ãƒãƒ³ãƒ—ã‚„åœ§ç¸®æ©Ÿã®æŒ¯å‹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€
    æ•…éšœè¨ºæ–­ã«æœ‰ç”¨ãªç‰¹å¾´é‡ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
    """

    def __init__(self, sampling_rate=10000):
        """
        Args:
            sampling_rate (int): ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•° [Hz]
        """
        self.sampling_rate = sampling_rate

    def extract_time_domain_features(self, signal_data):
        """æ™‚é–“é ˜åŸŸã®ç‰¹å¾´é‡ã‚’è¨ˆç®—

        Args:
            signal_data (np.ndarray): æŒ¯å‹•ä¿¡å·ãƒ‡ãƒ¼ã‚¿

        Returns:
            dict: æ™‚é–“é ˜åŸŸç‰¹å¾´é‡
        """
        features = {
            # åŸºæœ¬çµ±è¨ˆé‡
            'rms': np.sqrt(np.mean(signal_data**2)),  # RMSå€¤ï¼ˆæŒ¯å‹•å¼·åº¦ï¼‰
            'peak': np.max(np.abs(signal_data)),      # ãƒ”ãƒ¼ã‚¯å€¤
            'crest_factor': np.max(np.abs(signal_data)) / np.sqrt(np.mean(signal_data**2)),  # æ³¢é«˜ç‡

            # çµ±è¨ˆçš„æŒ‡æ¨™
            'mean': np.mean(signal_data),
            'std': np.std(signal_data),
            'kurtosis': kurtosis(signal_data),  # å°–åº¦ï¼ˆè¡æ’ƒçš„ãªæŒ¯å‹•ã‚’æ¤œå‡ºï¼‰
            'skewness': skew(signal_data),      # æ­ªåº¦ï¼ˆéå¯¾ç§°æ€§ï¼‰

            # æŒ¯å¹…æŒ‡æ¨™
            'peak_to_peak': np.ptp(signal_data),
            'clearance_factor': np.max(np.abs(signal_data)) / np.mean(np.sqrt(np.abs(signal_data)))**2,
            'shape_factor': np.sqrt(np.mean(signal_data**2)) / np.mean(np.abs(signal_data))
        }
        return features

    def extract_frequency_domain_features(self, signal_data, nperseg=1024):
        """å‘¨æ³¢æ•°é ˜åŸŸã®ç‰¹å¾´é‡ã‚’è¨ˆç®—ï¼ˆFFTè§£æï¼‰

        Args:
            signal_data (np.ndarray): æŒ¯å‹•ä¿¡å·ãƒ‡ãƒ¼ã‚¿
            nperseg (int): FFTã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•·

        Returns:
            dict: å‘¨æ³¢æ•°é ˜åŸŸç‰¹å¾´é‡
        """
        # FFTã«ã‚ˆã‚‹å‘¨æ³¢æ•°ã‚¹ãƒšã‚¯ãƒˆãƒ«è¨ˆç®—
        frequencies, psd = signal.welch(signal_data,
                                       fs=self.sampling_rate,
                                       nperseg=nperseg)

        # å‘¨æ³¢æ•°å¸¯åŸŸåˆ¥ãƒ‘ãƒ¯ãƒ¼ï¼ˆæ•…éšœãƒ¢ãƒ¼ãƒ‰æ¤œå‡ºç”¨ï¼‰
        freq_bands = {
            'very_low': (0, 10),      # 0-10 Hzï¼ˆãƒŸã‚¹ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆï¼‰
            'low': (10, 100),         # 10-100 Hzï¼ˆã‚¢ãƒ³ãƒãƒ©ãƒ³ã‚¹ï¼‰
            'mid': (100, 1000),       # 100-1000 Hzï¼ˆè»¸å—å¤–è¼ªä¸è‰¯ï¼‰
            'high': (1000, 5000)      # 1000-5000 Hzï¼ˆè»¸å—å†…è¼ªä¸è‰¯ï¼‰
        }

        features = {}
        for band_name, (f_min, f_max) in freq_bands.items():
            mask = (frequencies >= f_min) & (frequencies <= f_max)
            features[f'{band_name}_freq_power'] = np.sum(psd[mask])

        # ä¸»è¦å‘¨æ³¢æ•°æˆåˆ†ï¼ˆå›è»¢å‘¨æ³¢æ•°ã®æ¤œå‡ºï¼‰
        dominant_freq_idx = np.argmax(psd)
        features['dominant_frequency'] = frequencies[dominant_freq_idx]
        features['dominant_power'] = psd[dominant_freq_idx]

        # ã‚¹ãƒšã‚¯ãƒˆãƒ«çµ±è¨ˆé‡
        features['spectral_mean'] = np.sum(frequencies * psd) / np.sum(psd)
        features['spectral_std'] = np.sqrt(np.sum(((frequencies - features['spectral_mean'])**2) * psd) / np.sum(psd))
        features['spectral_kurtosis'] = kurtosis(psd)

        return features

    def extract_all_features(self, signal_data):
        """å…¨ã¦ã®ç‰¹å¾´é‡ã‚’æŠ½å‡º

        Args:
            signal_data (np.ndarray): æŒ¯å‹•ä¿¡å·ãƒ‡ãƒ¼ã‚¿

        Returns:
            pd.DataFrame: å…¨ç‰¹å¾´é‡
        """
        time_features = self.extract_time_domain_features(signal_data)
        freq_features = self.extract_frequency_domain_features(signal_data)

        all_features = {**time_features, **freq_features}
        return pd.DataFrame([all_features])


# ä½¿ç”¨ä¾‹ï¼šãƒãƒ³ãƒ—æŒ¯å‹•ãƒ‡ãƒ¼ã‚¿ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
if __name__ == "__main__":
    # æ­£å¸¸é‹è»¢ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆ50Hzå›è»¢ã€è»½å¾®ãªãƒã‚¤ã‚ºï¼‰
    t = np.linspace(0, 1, 10000)
    normal_signal = (1.5 * np.sin(2 * np.pi * 50 * t) +  # å›è»¢å‘¨æ³¢æ•°
                    0.3 * np.sin(2 * np.pi * 100 * t) +  # 2æ¬¡é«˜èª¿æ³¢
                    0.5 * np.random.randn(10000))        # ãƒã‚¤ã‚º

    # è»¸å—åŠ£åŒ–ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆé«˜å‘¨æ³¢æˆåˆ†å¢—åŠ ã€å°–åº¦å¢—åŠ ï¼‰
    degraded_signal = (1.5 * np.sin(2 * np.pi * 50 * t) +
                      0.3 * np.sin(2 * np.pi * 100 * t) +
                      1.8 * np.sin(2 * np.pi * 3500 * t) +  # è»¸å—ä¸è‰¯å‘¨æ³¢æ•°
                      0.8 * np.random.randn(10000))

    # ç‰¹å¾´æŠ½å‡º
    extractor = VibrationFeatureExtractor(sampling_rate=10000)

    normal_features = extractor.extract_all_features(normal_signal)
    degraded_features = extractor.extract_all_features(degraded_signal)

    print("æ­£å¸¸é‹è»¢æ™‚ã®ç‰¹å¾´é‡ï¼š")
    print(f"RMS: {normal_features['rms'].values[0]:.3f} mm/s")
    print(f"Kurtosis: {normal_features['kurtosis'].values[0]:.3f}")
    print(f"High Freq Power: {normal_features['high_freq_power'].values[0]:.2e}\n")

    print("è»¸å—åŠ£åŒ–æ™‚ã®ç‰¹å¾´é‡ï¼š")
    print(f"RMS: {degraded_features['rms'].values[0]:.3f} mm/s")
    print(f"Kurtosis: {degraded_features['kurtosis'].values[0]:.3f}")
    print(f"High Freq Power: {degraded_features['high_freq_power'].values[0]:.2e}")

    # å‡ºåŠ›ä¾‹:
    # æ­£å¸¸é‹è»¢æ™‚: RMS=1.67 mm/s, Kurtosis=2.89, High Freq Power=1.23e-03
    # è»¸å—åŠ£åŒ–æ™‚: RMS=2.31 mm/s, Kurtosis=4.52, High Freq Power=8.91e-02
</code></pre>
        </div>

        <div class="callout">
            <div class="callout-title">ğŸ¯ å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ</div>
            <ul>
                <li><strong>Kurtosisï¼ˆå°–åº¦ï¼‰</strong>ï¼šè»¸å—åŠ£åŒ–æ™‚ã®è¡æ’ƒçš„ãªæŒ¯å‹•ã‚’æ¤œå‡ºï¼ˆæ­£å¸¸â‰ˆ3ã€åŠ£åŒ–>5ï¼‰</li>
                <li><strong>å‘¨æ³¢æ•°å¸¯åŸŸãƒ‘ãƒ¯ãƒ¼</strong>ï¼šæ•…éšœãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸå‘¨æ³¢æ•°æˆåˆ†ã®å¢—åŠ ã‚’æ¤œå‡º</li>
                <li><strong>Welchæ³•</strong>ï¼šFFTã‚ˆã‚Šã‚‚ãƒã‚¤ã‚ºã«å¼·ã„å‘¨æ³¢æ•°è§£ææ‰‹æ³•</li>
            </ul>
        </div>

        <h2>2.3 å®Ÿè£…ä¾‹2ï¼šSurvival Analysisã«ã‚ˆã‚‹æ•…éšœäºˆæ¸¬</h2>

        <p>Survival Analysisï¼ˆç”Ÿå­˜æ™‚é–“è§£æï¼‰ã¯ã€è¨­å‚™ãŒã€Œç”Ÿå­˜ã€ã—ã¦ã„ã‚‹æ™‚é–“ï¼ˆæ•…éšœã¾ã§ã®æ™‚é–“ï¼‰ã‚’çµ±è¨ˆçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¾ã™ã€‚Coxæ¯”ä¾‹ãƒã‚¶ãƒ¼ãƒ‰ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€è¤‡æ•°ã®å…±å¤‰é‡ï¼ˆæ¸©åº¦ã€åœ§åŠ›ã€æŒ¯å‹•ãªã©ï¼‰ãŒæ•…éšœãƒªã‚¹ã‚¯ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’å®šé‡åŒ–ã§ãã¾ã™ã€‚</p>

        <div class="code-block">
            <div class="code-title">ğŸ“„ Example 2: Coxæ¯”ä¾‹ãƒã‚¶ãƒ¼ãƒ‰ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ•…éšœäºˆæ¸¬</div>
            <pre><code>import numpy as np
import pandas as pd
from lifelines import CoxPHFitter
from lifelines.utils import concordance_index
import matplotlib.pyplot as plt

class EquipmentFailureSurvivalModel:
    """Survival Analysisã«ã‚ˆã‚‹è¨­å‚™æ•…éšœäºˆæ¸¬

    Coxæ¯”ä¾‹ãƒã‚¶ãƒ¼ãƒ‰ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€è¨­å‚™ã®æ•…éšœãƒªã‚¹ã‚¯ã‚’
    è¤‡æ•°ã®é‹è»¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰äºˆæ¸¬ã—ã¾ã™ã€‚
    """

    def __init__(self):
        self.model = CoxPHFitter()
        self.is_fitted = False

    def prepare_data(self, operating_hours, failure_occurred,
                    vibration_rms, bearing_temp, pressure_deviation):
        """ç”Ÿå­˜æ™‚é–“ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™

        Args:
            operating_hours (array): é‹è»¢æ™‚é–“ [hours]
            failure_occurred (array): æ•…éšœç™ºç”Ÿ (1) ã¾ãŸã¯æ‰“ã¡åˆ‡ã‚Š (0)
            vibration_rms (array): æŒ¯å‹•RMSå€¤ [mm/s]
            bearing_temp (array): è»¸å—æ¸©åº¦ [Â°C]
            pressure_deviation (array): åœ§åŠ›åå·® [%]

        Returns:
            pd.DataFrame: ç”Ÿå­˜æ™‚é–“è§£æç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        data = pd.DataFrame({
            'duration': operating_hours,              # è¦³å¯Ÿæ™‚é–“
            'event': failure_occurred,                # ã‚¤ãƒ™ãƒ³ãƒˆç™ºç”Ÿï¼ˆæ•…éšœï¼‰
            'vibration_rms': vibration_rms,           # å…±å¤‰é‡1
            'bearing_temp': bearing_temp,             # å…±å¤‰é‡2
            'pressure_deviation': pressure_deviation, # å…±å¤‰é‡3
        })
        return data

    def fit(self, data):
        """ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´

        Args:
            data (pd.DataFrame): ç”Ÿå­˜æ™‚é–“ãƒ‡ãƒ¼ã‚¿
        """
        self.model.fit(data, duration_col='duration', event_col='event')
        self.is_fitted = True

        # ãƒã‚¶ãƒ¼ãƒ‰æ¯”ã®è¡¨ç¤ºï¼ˆå„å› å­ãŒæ•…éšœãƒªã‚¹ã‚¯ã«ä¸ãˆã‚‹å½±éŸ¿ï¼‰
        print("Coxæ¯”ä¾‹ãƒã‚¶ãƒ¼ãƒ‰ãƒ¢ãƒ‡ãƒ«ã®çµæœï¼š")
        print(self.model.summary[['coef', 'exp(coef)', 'p']])
        print(f"\nConcordance Index: {self.model.concordance_index_:.3f}")

    def predict_survival(self, new_data, time_points):
        """ç”Ÿå­˜ç¢ºç‡ã®äºˆæ¸¬

        Args:
            new_data (pd.DataFrame): æ–°è¦è¨­å‚™ãƒ‡ãƒ¼ã‚¿
            time_points (array): äºˆæ¸¬æ™‚ç‚¹ [hours]

        Returns:
            pd.DataFrame: å„æ™‚ç‚¹ã§ã®ç”Ÿå­˜ç¢ºç‡
        """
        if not self.is_fitted:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚fit()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

        survival_functions = self.model.predict_survival_function(new_data)
        predictions = survival_functions.loc[time_points]
        return predictions

    def calculate_risk_score(self, new_data):
        """ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼ˆãƒã‚¶ãƒ¼ãƒ‰æ¯”ï¼‰

        Args:
            new_data (pd.DataFrame): è¨­å‚™ãƒ‡ãƒ¼ã‚¿

        Returns:
            array: ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ï¼ˆé«˜ã„ã»ã©æ•…éšœãƒªã‚¹ã‚¯å¤§ï¼‰
        """
        risk_scores = self.model.predict_partial_hazard(new_data)
        return risk_scores


# ä½¿ç”¨ä¾‹ï¼šãƒãƒ³ãƒ—æ•…éšœäºˆæ¸¬
if __name__ == "__main__":
    # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆ100å°ã®ãƒãƒ³ãƒ—ï¼‰
    np.random.seed(42)
    n_pumps = 100

    # é‹è»¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæ­£å¸¸ç¯„å›²ã‹ã‚‰é€¸è„±ã™ã‚‹ã»ã©æ•…éšœãƒªã‚¹ã‚¯å¢—åŠ ï¼‰
    vibration_rms = np.random.uniform(1.5, 5.0, n_pumps)      # 1.5-5.0 mm/s
    bearing_temp = np.random.uniform(50, 90, n_pumps)         # 50-90Â°C
    pressure_dev = np.random.uniform(-5, 15, n_pumps)         # -5 to +15%

    # æ•…éšœã¾ã§ã®æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ä¾å­˜ï¼‰
    baseline_hazard = 0.0001
    hazard_rate = (baseline_hazard *
                   np.exp(0.5 * vibration_rms +
                         0.02 * bearing_temp +
                         0.1 * pressure_dev))

    operating_hours = -np.log(np.random.uniform(0, 1, n_pumps)) / hazard_rate

    # è¦³å¯ŸæœŸé–“ã‚’8000æ™‚é–“ã«è¨­å®šï¼ˆæ‰“ã¡åˆ‡ã‚Šãƒ‡ãƒ¼ã‚¿ï¼‰
    observation_period = 8000
    failure_occurred = (operating_hours < observation_period).astype(int)
    operating_hours = np.minimum(operating_hours, observation_period)

    # ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
    model = EquipmentFailureSurvivalModel()
    data = model.prepare_data(operating_hours, failure_occurred,
                             vibration_rms, bearing_temp, pressure_dev)

    model.fit(data)

    # æ–°è¦ãƒãƒ³ãƒ—ã®æ•…éšœãƒªã‚¹ã‚¯äºˆæ¸¬
    new_pump = pd.DataFrame({
        'vibration_rms': [2.5, 4.5],          # æ­£å¸¸ vs é«˜æŒ¯å‹•
        'bearing_temp': [60, 85],             # æ­£å¸¸ vs é«˜æ¸©
        'pressure_deviation': [0, 12]         # æ­£å¸¸ vs é«˜åå·®
    })

    time_points = np.array([1000, 2000, 4000, 6000, 8000])
    survival_probs = model.predict_survival(new_pump, time_points)

    print("\næ–°è¦ãƒãƒ³ãƒ—ã®ç”Ÿå­˜ç¢ºç‡äºˆæ¸¬ï¼š")
    print(survival_probs.T)

    risk_scores = model.calculate_risk_score(new_pump)
    print(f"\nãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ï¼ˆæ­£å¸¸ãƒãƒ³ãƒ—ï¼‰: {risk_scores.values[0]:.3f}")
    print(f"ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ï¼ˆé«˜ãƒªã‚¹ã‚¯ãƒãƒ³ãƒ—ï¼‰: {risk_scores.values[1]:.3f}")

    # å‡ºåŠ›ä¾‹:
    # æ­£å¸¸ãƒãƒ³ãƒ—: 8000æ™‚é–“å¾Œç”Ÿå­˜ç¢ºç‡=0.92ã€ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢=1.23
    # é«˜ãƒªã‚¹ã‚¯ãƒãƒ³ãƒ—: 8000æ™‚é–“å¾Œç”Ÿå­˜ç¢ºç‡=0.34ã€ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢=8.47
</code></pre>
        </div>

        <h2>2.4 å®Ÿè£…ä¾‹3ï¼šLSTMã«ã‚ˆã‚‹RULæ¨å®š</h2>

        <p>LSTMï¼ˆLong Short-Term Memoryï¼‰ã¯ã€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®é•·æœŸä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã§ãã‚‹RNNã®ä¸€ç¨®ã§ã™ã€‚C-MAPSSï¼ˆCommercial Modular Aero-Propulsion System Simulationï¼‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å®Ÿè¨¼ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«ã€ã‚¿ãƒ¼ãƒœæ©Ÿæ¢°ã®RULæ¨å®šã«é«˜ã„ç²¾åº¦ã‚’ç™ºæ®ã—ã¾ã™ã€‚</p>

        <div class="code-block">
            <div class="code-title">ğŸ“„ Example 3: LSTMã«ã‚ˆã‚‹æ®‹å­˜æœ‰åŠ¹å¯¿å‘½ï¼ˆRULï¼‰æ¨å®š</div>
            <pre><code>import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler

class TurbomachineryDataset(Dataset):
    """ã‚¿ãƒ¼ãƒœæ©Ÿæ¢°ã®RULæ¨å®šç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

    æ™‚ç³»åˆ—ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã¨RULãƒ©ãƒ™ãƒ«ã‚’ç®¡ç†ã—ã¾ã™ã€‚
    """

    def __init__(self, sequences, labels):
        """
        Args:
            sequences (np.ndarray): ã‚»ãƒ³ã‚µãƒ¼æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ (N, seq_len, features)
            labels (np.ndarray): RULãƒ©ãƒ™ãƒ« (N,)
        """
        self.sequences = torch.FloatTensor(sequences)
        self.labels = torch.FloatTensor(labels)

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        return self.sequences[idx], self.labels[idx]


class LSTMRULPredictor(nn.Module):
    """LSTMãƒ™ãƒ¼ã‚¹ã®RULæ¨å®šãƒ¢ãƒ‡ãƒ«

    å¤šå¤‰é‡æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ®‹å­˜æœ‰åŠ¹å¯¿å‘½ã‚’å›å¸°äºˆæ¸¬ã—ã¾ã™ã€‚
    """

    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2):
        """
        Args:
            input_size (int): å…¥åŠ›ç‰¹å¾´æ•°ï¼ˆã‚»ãƒ³ã‚µãƒ¼æ•°ï¼‰
            hidden_size (int): LSTMéš ã‚Œå±¤ã‚µã‚¤ã‚º
            num_layers (int): LSTMãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
            dropout (float): ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        """
        super(LSTMRULPredictor, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTMå±¤ï¼ˆåŒæ–¹å‘ï¼‰
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,
                           batch_first=True, dropout=dropout,
                           bidirectional=True)

        # å…¨çµåˆå±¤
        self.fc1 = nn.Linear(hidden_size * 2, 50)  # bidirectionalãªã®ã§*2
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        self.fc2 = nn.Linear(50, 1)

    def forward(self, x):
        """é †ä¼æ’­

        Args:
            x (torch.Tensor): å…¥åŠ› (batch, seq_len, input_size)

        Returns:
            torch.Tensor: RULäºˆæ¸¬ (batch, 1)
        """
        # LSTMå‡¦ç†
        lstm_out, _ = self.lstm(x)  # (batch, seq_len, hidden_size*2)

        # æœ€çµ‚æ™‚åˆ»ã®å‡ºåŠ›ã‚’ä½¿ç”¨
        last_output = lstm_out[:, -1, :]  # (batch, hidden_size*2)

        # å…¨çµåˆå±¤
        out = self.fc1(last_output)
        out = self.relu(out)
        out = self.dropout(out)
        out = self.fc2(out)

        return out


class RULEstimationSystem:
    """RULæ¨å®šã‚·ã‚¹ãƒ†ãƒ 

    ãƒ‡ãƒ¼ã‚¿æº–å‚™ã€è¨“ç·´ã€æ¨è«–ã‚’çµ±åˆç®¡ç†ã—ã¾ã™ã€‚
    """

    def __init__(self, input_size, sequence_length=30, device='cpu'):
        """
        Args:
            input_size (int): ã‚»ãƒ³ã‚µãƒ¼ç‰¹å¾´æ•°
            sequence_length (int): å…¥åŠ›æ™‚ç³»åˆ—é•·
            device (str): 'cpu' ã¾ãŸã¯ 'cuda'
        """
        self.input_size = input_size
        self.sequence_length = sequence_length
        self.device = torch.device(device)

        self.model = LSTMRULPredictor(input_size).to(self.device)
        self.scaler = StandardScaler()

    def prepare_sequences(self, sensor_data, rul_labels):
        """æ™‚ç³»åˆ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æº–å‚™

        Args:
            sensor_data (np.ndarray): ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ (timesteps, features)
            rul_labels (np.ndarray): RULãƒ©ãƒ™ãƒ« (timesteps,)

        Returns:
            tuple: (sequences, labels)
        """
        sequences = []
        labels = []

        for i in range(len(sensor_data) - self.sequence_length):
            seq = sensor_data[i:i+self.sequence_length]
            label = rul_labels[i+self.sequence_length]
            sequences.append(seq)
            labels.append(label)

        return np.array(sequences), np.array(labels)

    def train(self, train_loader, epochs=50, learning_rate=0.001):
        """ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´

        Args:
            train_loader (DataLoader): è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
            epochs (int): ã‚¨ãƒãƒƒã‚¯æ•°
            learning_rate (float): å­¦ç¿’ç‡
        """
        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)

        self.model.train()
        for epoch in range(epochs):
            total_loss = 0
            for sequences, labels in train_loader:
                sequences = sequences.to(self.device)
                labels = labels.to(self.device).unsqueeze(1)

                # é †ä¼æ’­
                outputs = self.model(sequences)
                loss = criterion(outputs, labels)

                # é€†ä¼æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            if (epoch + 1) % 10 == 0:
                avg_loss = total_loss / len(train_loader)
                print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

    def predict(self, sensor_sequence):
        """RULã®æ¨å®š

        Args:
            sensor_sequence (np.ndarray): ã‚»ãƒ³ã‚µãƒ¼æ™‚ç³»åˆ— (seq_len, features)

        Returns:
            float: æ¨å®šRUL [hours]
        """
        self.model.eval()
        with torch.no_grad():
            sequence_tensor = torch.FloatTensor(sensor_sequence).unsqueeze(0)
            sequence_tensor = sequence_tensor.to(self.device)
            prediction = self.model(sequence_tensor)
            return prediction.item()


# ä½¿ç”¨ä¾‹ï¼šåœ§ç¸®æ©Ÿã®RULæ¨å®š
if __name__ == "__main__":
    # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆåœ§ç¸®æ©Ÿã®åŠ£åŒ–ãƒ—ãƒ­ã‚»ã‚¹ï¼‰
    np.random.seed(42)

    # 1000ã‚µã‚¤ã‚¯ãƒ«ã®é‹è»¢ãƒ‡ãƒ¼ã‚¿
    n_cycles = 1000
    n_sensors = 10

    # åŠ£åŒ–ã«ä¼´ã†ã‚»ãƒ³ã‚µãƒ¼å€¤ã®å¤‰åŒ–ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    sensor_data = np.zeros((n_cycles, n_sensors))
    for i in range(n_sensors):
        # åŸºæº–å€¤ + åŠ£åŒ–ãƒˆãƒ¬ãƒ³ãƒ‰ + ãƒã‚¤ã‚º
        baseline = np.random.uniform(50, 100)
        degradation = np.linspace(0, 20, n_cycles) * (i % 2)  # ä¸€éƒ¨ã®ã‚»ãƒ³ã‚µãƒ¼ã¯åŠ£åŒ–ã§å¢—åŠ 
        noise = np.random.randn(n_cycles) * 2
        sensor_data[:, i] = baseline + degradation + noise

    # RULãƒ©ãƒ™ãƒ«ï¼ˆæ®‹ã‚Šé‹è»¢æ™‚é–“ï¼‰
    rul_labels = np.arange(n_cycles, 0, -1).astype(float)

    # ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–
    scaler = StandardScaler()
    sensor_data_scaled = scaler.fit_transform(sensor_data)

    # RULæ¨å®šã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã¨è¨“ç·´
    rul_system = RULEstimationSystem(input_size=n_sensors, sequence_length=30)

    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æº–å‚™
    sequences, labels = rul_system.prepare_sequences(sensor_data_scaled, rul_labels)

    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ
    dataset = TurbomachineryDataset(sequences, labels)
    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)

    # è¨“ç·´
    print("LSTM RULæ¨å®šãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´é–‹å§‹...")
    rul_system.train(train_loader, epochs=50, learning_rate=0.001)

    # äºˆæ¸¬ä¾‹ï¼ˆæœ€æ–°30ã‚µã‚¤ã‚¯ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç¾åœ¨ã®RULã‚’æ¨å®šï¼‰
    current_sequence = sensor_data_scaled[-30:]
    estimated_rul = rul_system.predict(current_sequence)
    actual_rul = rul_labels[-1]

    print(f"\næ¨å®šRUL: {estimated_rul:.1f} cycles")
    print(f"å®Ÿéš›ã®RUL: {actual_rul:.1f} cycles")
    print(f"èª¤å·®: {abs(estimated_rul - actual_rul):.1f} cycles")
</code></pre>
        </div>

        <div class="callout">
            <div class="callout-title">âš ï¸ å®Ÿè£…ä¸Šã®æ³¨æ„ç‚¹</div>
            <ul>
                <li><strong>ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã®é¸æŠ</strong>ï¼šçŸ­ã™ãã‚‹ã¨é•·æœŸä¾å­˜ã‚’æ‰ãˆã‚‰ã‚Œãšã€é•·ã™ãã‚‹ã¨éå­¦ç¿’ãƒªã‚¹ã‚¯å¢—åŠ ï¼ˆçµŒé¨“çš„ã«20-50ã‚µã‚¤ã‚¯ãƒ«ãŒé©åˆ‡ï¼‰</li>
                <li><strong>RULã‚­ãƒ£ãƒƒãƒ”ãƒ³ã‚°</strong>ï¼šåˆæœŸæ®µéšã®RULã¯ä¸å®‰å®šãªãŸã‚ã€ä¸Šé™å€¤ï¼ˆä¾‹ï¼š125ã‚µã‚¤ã‚¯ãƒ«ï¼‰ã§ã‚­ãƒ£ãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹ã“ã¨ãŒä¸€èˆ¬çš„</li>
                <li><strong>ãƒ‡ãƒ¼ã‚¿ä¸å‡è¡¡</strong>ï¼šåˆæœŸãƒ‡ãƒ¼ã‚¿ãŒå¤šãæ•…éšœç›´å‰ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ãŸã‚ã€é‡ã¿ä»˜ã‘ã‚„ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’æ¤œè¨</li>
            </ul>
        </div>

        <h2>2.5 å®Ÿè£…ä¾‹4ï¼šTCNã«ã‚ˆã‚‹RULæ¨å®š</h2>

        <p>TCNï¼ˆTemporal Convolutional Networkï¼‰ã¯ã€LSTMã«æ¯”ã¹ã¦ä¸¦åˆ—å‡¦ç†ãŒå¯èƒ½ã§ã€é•·æœŸä¾å­˜é–¢ä¿‚ã®å­¦ç¿’ã«ã‚‚å„ªã‚Œã¦ã„ã¾ã™ã€‚dilated causal convolutionã«ã‚ˆã‚Šã€å¤§ããªå—å®¹é‡ã‚’åŠ¹ç‡çš„ã«å®Ÿç¾ã—ã¾ã™ã€‚</p>

        <div class="code-block">
            <div class="code-title">ğŸ“„ Example 4: TCNã«ã‚ˆã‚‹RULæ¨å®šï¼ˆDilated Convolutionï¼‰</div>
            <pre><code>import torch
import torch.nn as nn
import numpy as np

class TemporalBlock(nn.Module):
    """TCNã®åŸºæœ¬ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆDilated Causal Convolutionï¼‰

    å› æœå¾‹ã‚’ä¿ã¡ãªãŒã‚‰ã€å¤§ããªå—å®¹é‡ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
    """

    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):
        super(TemporalBlock, self).__init__()

        # Causal Convolutionï¼ˆéå»ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’å‚ç…§ï¼‰
        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,
                              stride=stride, padding=padding, dilation=dilation)
        self.chomp1 = nn.ConstantPad1d((0, -padding), 0)  # æœªæ¥ã¸ã®æ¼æ´©ã‚’é˜²ã
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)

        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,
                              stride=stride, padding=padding, dilation=dilation)
        self.chomp2 = nn.ConstantPad1d((0, -padding), 0)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)

        # Residual connection
        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
        self.relu = nn.ReLU()

    def forward(self, x):
        out = self.conv1(x)
        out = self.chomp1(out)
        out = self.relu1(out)
        out = self.dropout1(out)

        out = self.conv2(out)
        out = self.chomp2(out)
        out = self.relu2(out)
        out = self.dropout2(out)

        # Residual connection
        res = x if self.downsample is None else self.downsample(x)
        return self.relu(out + res)


class TCN_RUL_Predictor(nn.Module):
    """TCNãƒ™ãƒ¼ã‚¹ã®RULæ¨å®šãƒ¢ãƒ‡ãƒ«

    Dilated Causal Convolutionã«ã‚ˆã‚Šã€é•·æœŸä¾å­˜é–¢ä¿‚ã‚’åŠ¹ç‡çš„ã«å­¦ç¿’ã—ã¾ã™ã€‚
    """

    def __init__(self, input_size, num_channels=[32, 64, 128], kernel_size=3, dropout=0.2):
        """
        Args:
            input_size (int): å…¥åŠ›ç‰¹å¾´æ•°ï¼ˆã‚»ãƒ³ã‚µãƒ¼æ•°ï¼‰
            num_channels (list): å„å±¤ã®ãƒãƒ£ãƒãƒ«æ•°
            kernel_size (int): ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º
            dropout (float): ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        """
        super(TCN_RUL_Predictor, self).__init__()

        layers = []
        num_levels = len(num_channels)

        for i in range(num_levels):
            dilation_size = 2 ** i  # æŒ‡æ•°é–¢æ•°çš„ã«æ‹¡å¼µ
            in_channels = input_size if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]
            padding = (kernel_size - 1) * dilation_size

            layers.append(TemporalBlock(in_channels, out_channels, kernel_size,
                                       stride=1, dilation=dilation_size,
                                       padding=padding, dropout=dropout))

        self.network = nn.Sequential(*layers)
        self.fc = nn.Linear(num_channels[-1], 1)

    def forward(self, x):
        """é †ä¼æ’­

        Args:
            x (torch.Tensor): å…¥åŠ› (batch, input_size, seq_len)

        Returns:
            torch.Tensor: RULäºˆæ¸¬ (batch, 1)
        """
        # TCNå‡¦ç†
        y = self.network(x)  # (batch, num_channels[-1], seq_len)

        # Global Average Pooling
        y = torch.mean(y, dim=2)  # (batch, num_channels[-1])

        # RULæ¨å®š
        out = self.fc(y)
        return out


class TCN_RUL_System:
    """TCNãƒ™ãƒ¼ã‚¹RULæ¨å®šã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, input_size, device='cpu'):
        self.model = TCN_RUL_Predictor(input_size,
                                       num_channels=[32, 64, 128],
                                       kernel_size=3).to(device)
        self.device = device

    def train(self, train_loader, epochs=50, learning_rate=0.001):
        """ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)

        self.model.train()
        for epoch in range(epochs):
            total_loss = 0
            for sequences, labels in train_loader:
                # TCNã¯ (batch, features, seq_len) ã®å½¢å¼ã‚’æœŸå¾…
                sequences = sequences.permute(0, 2, 1).to(self.device)
                labels = labels.to(self.device).unsqueeze(1)

                outputs = self.model(sequences)
                loss = criterion(outputs, labels)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            if (epoch + 1) % 10 == 0:
                avg_loss = total_loss / len(train_loader)
                print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

    def predict(self, sensor_sequence):
        """RULæ¨å®š

        Args:
            sensor_sequence (np.ndarray): ã‚»ãƒ³ã‚µãƒ¼æ™‚ç³»åˆ— (seq_len, features)

        Returns:
            float: æ¨å®šRUL
        """
        self.model.eval()
        with torch.no_grad():
            # (features, seq_len) ã«å¤‰æ›
            sequence_tensor = torch.FloatTensor(sensor_sequence.T).unsqueeze(0)
            sequence_tensor = sequence_tensor.to(self.device)
            prediction = self.model(sequence_tensor)
            return prediction.item()


# ä½¿ç”¨ä¾‹ï¼šç†±äº¤æ›å™¨ã®ãƒ•ã‚¡ã‚¦ãƒªãƒ³ã‚°é€²è¡Œåº¦ã¨RULæ¨å®š
if __name__ == "__main__":
    from torch.utils.data import TensorDataset, DataLoader

    # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼ˆç†±äº¤æ›å™¨ã®ãƒ•ã‚¡ã‚¦ãƒªãƒ³ã‚°é€²è¡Œï¼‰
    np.random.seed(123)
    n_cycles = 800
    n_sensors = 8  # æ¸©åº¦ã€åœ§åŠ›ã€æµé‡ã‚»ãƒ³ã‚µãƒ¼

    # ãƒ•ã‚¡ã‚¦ãƒªãƒ³ã‚°é€²è¡Œã«ä¼´ã†ç†±äº¤æ›æ€§èƒ½ã®åŠ£åŒ–
    sensor_data = np.zeros((n_cycles, n_sensors))
    for i in range(n_sensors):
        baseline = np.random.uniform(40, 80)
        # éç·šå½¢ãªåŠ£åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³
        degradation = 15 * (1 - np.exp(-np.linspace(0, 3, n_cycles)))
        noise = np.random.randn(n_cycles) * 1.5
        sensor_data[:, i] = baseline + degradation * (i % 3) + noise

    rul_labels = np.arange(n_cycles, 0, -1).astype(float)

    # ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    sensor_data_scaled = scaler.fit_transform(sensor_data)

    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æº–å‚™ï¼ˆ30ã‚µã‚¤ã‚¯ãƒ«çª“ï¼‰
    seq_len = 30
    sequences = []
    labels = []
    for i in range(len(sensor_data_scaled) - seq_len):
        sequences.append(sensor_data_scaled[i:i+seq_len])
        labels.append(rul_labels[i+seq_len])

    sequences = np.array(sequences)
    labels = np.array(labels)

    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    dataset = TensorDataset(torch.FloatTensor(sequences),
                           torch.FloatTensor(labels))
    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)

    # TCN RULæ¨å®šã‚·ã‚¹ãƒ†ãƒ ã®è¨“ç·´
    tcn_system = TCN_RUL_System(input_size=n_sensors)

    print("TCN RULæ¨å®šãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´é–‹å§‹...")
    tcn_system.train(train_loader, epochs=50, learning_rate=0.001)

    # äºˆæ¸¬
    test_sequence = sensor_data_scaled[-30:]
    estimated_rul = tcn_system.predict(test_sequence)
    actual_rul = rul_labels[-1]

    print(f"\næ¨å®šRUL: {estimated_rul:.1f} cycles")
    print(f"å®Ÿéš›ã®RUL: {actual_rul:.1f} cycles")
    print(f"æ¨å®šç²¾åº¦: {100 - abs(estimated_rul - actual_rul)/actual_rul*100:.1f}%")
</code></pre>
        </div>

        <h2>2.6 å®Ÿè£…ä¾‹5ï¼šæ•…éšœãƒ¢ãƒ¼ãƒ‰åˆ†é¡ï¼ˆRandom Forestï¼‰</h2>

        <p>è¨­å‚™ã®æ•…éšœã«ã¯è¤‡æ•°ã®ãƒ¢ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã™ï¼ˆè»¸å—ä¸è‰¯ã€ã‚·ãƒ¼ãƒ«æ¼ã‚Œã€ã‚­ãƒ£ãƒ“ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç­‰ï¼‰ã€‚Random Forestã«ã‚ˆã‚‹å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ã«ã‚ˆã‚Šã€ç¾åœ¨ã®ç•°å¸¸çŠ¶æ…‹ãŒã©ã®æ•…éšœãƒ¢ãƒ¼ãƒ‰ã«è©²å½“ã™ã‚‹ã‹ã‚’è¨ºæ–­ã§ãã¾ã™ã€‚</p>

        <div class="code-block">
            <div class="code-title">ğŸ“„ Example 5: Random Forestã«ã‚ˆã‚‹æ•…éšœãƒ¢ãƒ¼ãƒ‰åˆ†é¡</div>
            <pre><code>import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

class FailureModeClassifier:
    """æ•…éšœãƒ¢ãƒ¼ãƒ‰åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ 

    æŒ¯å‹•ãƒ»æ¸©åº¦ãƒ»åœ§åŠ›ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ•…éšœãƒ¢ãƒ¼ãƒ‰ã‚’è¨ºæ–­ã—ã¾ã™ã€‚
    """

    def __init__(self, n_estimators=200, max_depth=15, random_state=42):
        """
        Args:
            n_estimators (int): æ±ºå®šæœ¨ã®æ•°
            max_depth (int): æœ¨ã®æœ€å¤§æ·±ã•
            random_state (int): ä¹±æ•°ã‚·ãƒ¼ãƒ‰
        """
        self.model = RandomForestClassifier(n_estimators=n_estimators,
                                           max_depth=max_depth,
                                           random_state=random_state,
                                           class_weight='balanced')  # ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾ç­–
        self.feature_names = None
        self.failure_modes = ['Normal', 'Bearing', 'Seal', 'Cavitation', 'Misalignment']

    def generate_training_data(self, n_samples_per_mode=500):
        """è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆæ•…éšœãƒ¢ãƒ¼ãƒ‰ã”ã¨ã®ç‰¹å¾´ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰

        Args:
            n_samples_per_mode (int): ãƒ¢ãƒ¼ãƒ‰ã”ã¨ã®ã‚µãƒ³ãƒ—ãƒ«æ•°

        Returns:
            tuple: (X, y, feature_names)
        """
        np.random.seed(42)
        data = []
        labels = []

        # æ•…éšœãƒ¢ãƒ¼ãƒ‰ã”ã¨ã®ç‰¹å¾´ãƒ‘ã‚¿ãƒ¼ãƒ³
        for mode_idx, mode in enumerate(self.failure_modes):
            for _ in range(n_samples_per_mode):
                if mode == 'Normal':
                    # æ­£å¸¸é‹è»¢ï¼šå…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ­£å¸¸ç¯„å›²å†…
                    features = {
                        'vibration_rms': np.random.uniform(1.5, 2.5),
                        'vibration_peak': np.random.uniform(4.0, 8.0),
                        'vibration_kurtosis': np.random.uniform(2.5, 3.5),
                        'bearing_temp': np.random.uniform(50, 65),
                        'low_freq_power': np.random.uniform(0.1, 0.3),
                        'mid_freq_power': np.random.uniform(0.2, 0.4),
                        'high_freq_power': np.random.uniform(0.01, 0.05),
                        'suction_pressure': np.random.uniform(1.8, 2.2),
                        'discharge_pressure': np.random.uniform(9.5, 10.5),
                        'flow_rate': np.random.uniform(95, 105)
                    }

                elif mode == 'Bearing':
                    # è»¸å—ä¸è‰¯ï¼šé«˜å‘¨æ³¢æŒ¯å‹•å¢—åŠ ã€å°–åº¦å¢—åŠ ã€æ¸©åº¦ä¸Šæ˜‡
                    features = {
                        'vibration_rms': np.random.uniform(3.5, 6.0),
                        'vibration_peak': np.random.uniform(12.0, 25.0),
                        'vibration_kurtosis': np.random.uniform(5.0, 12.0),  # è¡æ’ƒçš„æŒ¯å‹•
                        'bearing_temp': np.random.uniform(75, 95),           # é«˜æ¸©
                        'low_freq_power': np.random.uniform(0.1, 0.3),
                        'mid_freq_power': np.random.uniform(0.3, 0.5),
                        'high_freq_power': np.random.uniform(0.5, 1.5),      # é«˜å‘¨æ³¢å¢—åŠ 
                        'suction_pressure': np.random.uniform(1.8, 2.2),
                        'discharge_pressure': np.random.uniform(9.0, 10.5),
                        'flow_rate': np.random.uniform(90, 105)
                    }

                elif mode == 'Seal':
                    # ã‚·ãƒ¼ãƒ«æ¼ã‚Œï¼šåœ§åŠ›ä½ä¸‹ã€æµé‡æ¸›å°‘
                    features = {
                        'vibration_rms': np.random.uniform(2.0, 3.5),
                        'vibration_peak': np.random.uniform(6.0, 12.0),
                        'vibration_kurtosis': np.random.uniform(2.5, 4.5),
                        'bearing_temp': np.random.uniform(55, 75),
                        'low_freq_power': np.random.uniform(0.1, 0.3),
                        'mid_freq_power': np.random.uniform(0.2, 0.4),
                        'high_freq_power': np.random.uniform(0.01, 0.1),
                        'suction_pressure': np.random.uniform(1.5, 2.0),     # ä½åœ§
                        'discharge_pressure': np.random.uniform(7.0, 9.0),   # ä½åœ§
                        'flow_rate': np.random.uniform(70, 90)               # æµé‡æ¸›å°‘
                    }

                elif mode == 'Cavitation':
                    # ã‚­ãƒ£ãƒ“ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼šç‰¹å¾´çš„ãªæŒ¯å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã€å¸è¾¼åœ§åŠ›ä½ä¸‹
                    features = {
                        'vibration_rms': np.random.uniform(4.0, 7.0),
                        'vibration_peak': np.random.uniform(15.0, 30.0),
                        'vibration_kurtosis': np.random.uniform(6.0, 15.0),  # ä¸è¦å‰‡ãªè¡æ’ƒ
                        'bearing_temp': np.random.uniform(60, 80),
                        'low_freq_power': np.random.uniform(0.5, 1.2),       # ä½å‘¨æ³¢å¢—åŠ 
                        'mid_freq_power': np.random.uniform(0.8, 1.5),
                        'high_freq_power': np.random.uniform(0.1, 0.4),
                        'suction_pressure': np.random.uniform(1.0, 1.6),     # ä½å¸è¾¼åœ§åŠ›
                        'discharge_pressure': np.random.uniform(8.0, 10.0),
                        'flow_rate': np.random.uniform(85, 100)
                    }

                elif mode == 'Misalignment':
                    # ãƒŸã‚¹ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆï¼šä½å‘¨æ³¢æŒ¯å‹•å¢—åŠ 
                    features = {
                        'vibration_rms': np.random.uniform(3.0, 5.0),
                        'vibration_peak': np.random.uniform(10.0, 18.0),
                        'vibration_kurtosis': np.random.uniform(3.5, 5.5),
                        'bearing_temp': np.random.uniform(65, 85),
                        'low_freq_power': np.random.uniform(0.8, 1.8),       # ä½å‘¨æ³¢å¢—åŠ 
                        'mid_freq_power': np.random.uniform(0.3, 0.6),
                        'high_freq_power': np.random.uniform(0.01, 0.08),
                        'suction_pressure': np.random.uniform(1.8, 2.2),
                        'discharge_pressure': np.random.uniform(9.0, 10.5),
                        'flow_rate': np.random.uniform(90, 105)
                    }

                data.append(list(features.values()))
                labels.append(mode_idx)

        X = np.array(data)
        y = np.array(labels)
        self.feature_names = list(features.keys())

        return X, y, self.feature_names

    def train(self, X, y):
        """ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´

        Args:
            X (np.ndarray): ç‰¹å¾´é‡ (n_samples, n_features)
            y (np.ndarray): ãƒ©ãƒ™ãƒ« (n_samples,)
        """
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        self.model.fit(X_train, y_train)

        # æ€§èƒ½è©•ä¾¡
        y_pred = self.model.predict(X_test)

        print("æ•…éšœãƒ¢ãƒ¼ãƒ‰åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ï¼š")
        print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆï¼š")
        print(classification_report(y_test, y_pred,
                                   target_names=self.failure_modes,
                                   digits=3))

        # æ··åŒè¡Œåˆ—
        cm = confusion_matrix(y_test, y_pred)
        print("\næ··åŒè¡Œåˆ—ï¼š")
        print(pd.DataFrame(cm,
                          index=self.failure_modes,
                          columns=self.failure_modes))

        # ç‰¹å¾´é‡è¦åº¦
        feature_importance = pd.DataFrame({
            'feature': self.feature_names,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)

        print("\nç‰¹å¾´é‡è¦åº¦ Top 5ï¼š")
        print(feature_importance.head())

    def diagnose(self, sensor_data):
        """æ•…éšœãƒ¢ãƒ¼ãƒ‰ã®è¨ºæ–­

        Args:
            sensor_data (dict or np.ndarray): ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿

        Returns:
            dict: è¨ºæ–­çµæœï¼ˆãƒ¢ãƒ¼ãƒ‰ã€ç¢ºç‡ï¼‰
        """
        if isinstance(sensor_data, dict):
            X = np.array([list(sensor_data.values())])
        else:
            X = sensor_data.reshape(1, -1)

        prediction = self.model.predict(X)[0]
        probabilities = self.model.predict_proba(X)[0]

        result = {
            'predicted_mode': self.failure_modes[prediction],
            'confidence': probabilities[prediction],
            'all_probabilities': {mode: prob for mode, prob
                                 in zip(self.failure_modes, probabilities)}
        }

        return result


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # æ•…éšœãƒ¢ãƒ¼ãƒ‰åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã¨è¨“ç·´
    classifier = FailureModeClassifier(n_estimators=200)

    print("è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
    X, y, feature_names = classifier.generate_training_data(n_samples_per_mode=500)

    print("ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
    classifier.train(X, y)

    # æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®è¨ºæ–­ä¾‹
    print("\n" + "="*60)
    print("è¨ºæ–­ä¾‹ï¼š")

    # ã‚±ãƒ¼ã‚¹1ï¼šè»¸å—ä¸è‰¯ã®ç–‘ã„
    case1 = {
        'vibration_rms': 4.5,
        'vibration_peak': 18.0,
        'vibration_kurtosis': 8.5,
        'bearing_temp': 85,
        'low_freq_power': 0.2,
        'mid_freq_power': 0.4,
        'high_freq_power': 0.9,
        'suction_pressure': 2.0,
        'discharge_pressure': 9.8,
        'flow_rate': 98
    }

    result1 = classifier.diagnose(case1)
    print(f"\nã‚±ãƒ¼ã‚¹1ï¼ˆé«˜æŒ¯å‹•ãƒ»é«˜æ¸©ï¼‰:")
    print(f"  è¨ºæ–­çµæœ: {result1['predicted_mode']}")
    print(f"  ç¢ºä¿¡åº¦: {result1['confidence']:.1%}")

    # ã‚±ãƒ¼ã‚¹2ï¼šã‚­ãƒ£ãƒ“ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
    case2 = {
        'vibration_rms': 5.5,
        'vibration_peak': 22.0,
        'vibration_kurtosis': 10.0,
        'bearing_temp': 70,
        'low_freq_power': 0.9,
        'mid_freq_power': 1.1,
        'high_freq_power': 0.2,
        'suction_pressure': 1.3,  # ä½å¸è¾¼åœ§åŠ›
        'discharge_pressure': 9.0,
        'flow_rate': 92
    }

    result2 = classifier.diagnose(case2)
    print(f"\nã‚±ãƒ¼ã‚¹2ï¼ˆä½å¸è¾¼åœ§åŠ›ï¼‰:")
    print(f"  è¨ºæ–­çµæœ: {result2['predicted_mode']}")
    print(f"  ç¢ºä¿¡åº¦: {result2['confidence']:.1%}")
</code></pre>
        </div>

        <h2>2.7 å®Ÿè£…ä¾‹6ï¼šTransfer Learningã«ã‚ˆã‚‹åŠ£åŒ–äºˆæ¸¬</h2>

        <p>æ–°è¦è¨­å‚™ã‚„ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„è¨­å‚™ã«å¯¾ã—ã¦ã¯ã€é¡ä¼¼è¨­å‚™ã§è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’Transfer Learningã§é©ç”¨ã™ã‚‹ã“ã¨ã§ã€å°‘é‡ãƒ‡ãƒ¼ã‚¿ã§ã‚‚é«˜ç²¾åº¦ãªäºˆæ¸¬ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚</p>

        <div class="code-block">
            <div class="code-title">ğŸ“„ Example 6: Transfer Learningã«ã‚ˆã‚‹åŠ£åŒ–äºˆæ¸¬</div>
            <pre><code>import torch
import torch.nn as nn
import numpy as np
from torch.utils.data import TensorDataset, DataLoader

class DegradationPredictor(nn.Module):
    """è¨­å‚™åŠ£åŒ–äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ï¼ˆTransfer Learningç”¨ï¼‰

    ã‚½ãƒ¼ã‚¹è¨­å‚™ã§äº‹å‰è¨“ç·´ã—ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨­å‚™ã¸Fine-tuningã—ã¾ã™ã€‚
    """

    def __init__(self, input_size, hidden_sizes=[64, 32]):
        super(DegradationPredictor, self).__init__()

        # ç‰¹å¾´æŠ½å‡ºå±¤ï¼ˆè»¢ç§»å­¦ç¿’ã§å…±æœ‰ï¼‰
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_size, hidden_sizes[0]),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_sizes[0], hidden_sizes[1]),
            nn.ReLU(),
            nn.Dropout(0.2)
        )

        # ã‚¿ã‚¹ã‚¯å›ºæœ‰å±¤ï¼ˆFine-tuningæ™‚ã«å†è¨“ç·´ï¼‰
        self.task_specific = nn.Sequential(
            nn.Linear(hidden_sizes[1], 16),
            nn.ReLU(),
            nn.Linear(16, 1)
        )

    def forward(self, x):
        features = self.feature_extractor(x)
        output = self.task_specific(features)
        return output

    def freeze_feature_extractor(self):
        """ç‰¹å¾´æŠ½å‡ºå±¤ã‚’ãƒ•ãƒªãƒ¼ã‚ºï¼ˆFine-tuningç”¨ï¼‰"""
        for param in self.feature_extractor.parameters():
            param.requires_grad = False

    def unfreeze_all(self):
        """å…¨å±¤ã‚’ã‚¢ãƒ³ãƒ•ãƒªãƒ¼ã‚º"""
        for param in self.parameters():
            param.requires_grad = True


class TransferLearningSystem:
    """Transfer Learningã«ã‚ˆã‚‹åŠ£åŒ–äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, input_size, device='cpu'):
        self.device = torch.device(device)
        self.model = DegradationPredictor(input_size).to(self.device)
        self.input_size = input_size

    def pretrain_on_source(self, source_data, source_labels,
                          epochs=100, batch_size=32, lr=0.001):
        """ã‚½ãƒ¼ã‚¹è¨­å‚™ã§ã®äº‹å‰è¨“ç·´

        Args:
            source_data (np.ndarray): ã‚½ãƒ¼ã‚¹è¨­å‚™ãƒ‡ãƒ¼ã‚¿ (n_samples, features)
            source_labels (np.ndarray): åŠ£åŒ–åº¦ãƒ©ãƒ™ãƒ« (n_samples,)
            epochs (int): ã‚¨ãƒãƒƒã‚¯æ•°
            batch_size (int): ãƒãƒƒãƒã‚µã‚¤ã‚º
            lr (float): å­¦ç¿’ç‡
        """
        print("="*60)
        print("Phase 1: ã‚½ãƒ¼ã‚¹è¨­å‚™ã§ã®äº‹å‰è¨“ç·´")
        print("="*60)

        dataset = TensorDataset(
            torch.FloatTensor(source_data),
            torch.FloatTensor(source_labels)
        )
        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)

        self.model.train()
        for epoch in range(epochs):
            total_loss = 0
            for X_batch, y_batch in loader:
                X_batch = X_batch.to(self.device)
                y_batch = y_batch.to(self.device).unsqueeze(1)

                outputs = self.model(X_batch)
                loss = criterion(outputs, y_batch)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            if (epoch + 1) % 20 == 0:
                avg_loss = total_loss / len(loader)
                print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

    def finetune_on_target(self, target_data, target_labels,
                          epochs=50, batch_size=16, lr=0.0001,
                          freeze_feature_extractor=True):
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨­å‚™ã§ã®Fine-tuning

        Args:
            target_data (np.ndarray): ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨­å‚™ãƒ‡ãƒ¼ã‚¿ï¼ˆå°‘é‡ï¼‰
            target_labels (np.ndarray): åŠ£åŒ–åº¦ãƒ©ãƒ™ãƒ«
            epochs (int): ã‚¨ãƒãƒƒã‚¯æ•°
            batch_size (int): ãƒãƒƒãƒã‚µã‚¤ã‚º
            lr (float): å­¦ç¿’ç‡ï¼ˆäº‹å‰è¨“ç·´ã‚ˆã‚Šå°ã•ãï¼‰
            freeze_feature_extractor (bool): ç‰¹å¾´æŠ½å‡ºå±¤ã‚’ãƒ•ãƒªãƒ¼ã‚ºã™ã‚‹ã‹
        """
        print("\n" + "="*60)
        print("Phase 2: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨­å‚™ã§ã®Fine-tuning")
        print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°: {len(target_data)}ã‚µãƒ³ãƒ—ãƒ«")
        print("="*60)

        # ç‰¹å¾´æŠ½å‡ºå±¤ã®ãƒ•ãƒªãƒ¼ã‚ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if freeze_feature_extractor:
            self.model.freeze_feature_extractor()
            print("ç‰¹å¾´æŠ½å‡ºå±¤ã‚’ãƒ•ãƒªãƒ¼ã‚ºã—ã¾ã—ãŸ")

        dataset = TensorDataset(
            torch.FloatTensor(target_data),
            torch.FloatTensor(target_labels)
        )
        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(
            filter(lambda p: p.requires_grad, self.model.parameters()),
            lr=lr
        )

        self.model.train()
        for epoch in range(epochs):
            total_loss = 0
            for X_batch, y_batch in loader:
                X_batch = X_batch.to(self.device)
                y_batch = y_batch.to(self.device).unsqueeze(1)

                outputs = self.model(X_batch)
                loss = criterion(outputs, y_batch)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            if (epoch + 1) % 10 == 0:
                avg_loss = total_loss / len(loader)
                print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

    def predict(self, X):
        """åŠ£åŒ–åº¦ã®äºˆæ¸¬

        Args:
            X (np.ndarray): ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿

        Returns:
            np.ndarray: äºˆæ¸¬åŠ£åŒ–åº¦ [0-1]
        """
        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(self.device)
            predictions = self.model(X_tensor)
            return predictions.cpu().numpy()


# ä½¿ç”¨ä¾‹ï¼šç†±äº¤æ›å™¨ã®ãƒ•ã‚¡ã‚¦ãƒªãƒ³ã‚°äºˆæ¸¬
if __name__ == "__main__":
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import mean_squared_error, r2_score

    np.random.seed(42)

    # ã‚½ãƒ¼ã‚¹è¨­å‚™ãƒ‡ãƒ¼ã‚¿ï¼ˆæ—¢å­˜ã®ç†±äº¤æ›å™¨Aã€å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚ã‚Šï¼‰
    n_source = 2000
    n_features = 12

    X_source = np.random.randn(n_source, n_features)
    # åŠ£åŒ–åº¦ = ã‚»ãƒ³ã‚µãƒ¼å€¤ã®è¤‡é›‘ãªé–¢æ•°
    y_source = (0.3 * X_source[:, 0] +
                0.2 * X_source[:, 1]**2 +
                0.15 * X_source[:, 2] * X_source[:, 3] +
                0.1 * np.random.randn(n_source))
    y_source = (y_source - y_source.min()) / (y_source.max() - y_source.min())

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨­å‚™ãƒ‡ãƒ¼ã‚¿ï¼ˆæ–°è¨­ã®ç†±äº¤æ›å™¨Bã€å°‘é‡ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
    n_target_train = 50   # ã‚ãšã‹50ã‚µãƒ³ãƒ—ãƒ«
    n_target_test = 100

    X_target = np.random.randn(n_target_train + n_target_test, n_features) * 0.8
    # é¡ä¼¼ã ãŒå¾®å¦™ã«ç•°ãªã‚‹åŠ£åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³
    y_target = (0.25 * X_target[:, 0] +
                0.25 * X_target[:, 1]**2 +
                0.2 * X_target[:, 2] * X_target[:, 3] +
                0.05 * X_target[:, 5] +
                0.1 * np.random.randn(n_target_train + n_target_test))
    y_target = (y_target - y_target.min()) / (y_target.max() - y_target.min())

    X_target_train = X_target[:n_target_train]
    y_target_train = y_target[:n_target_train]
    X_target_test = X_target[n_target_train:]
    y_target_test = y_target[n_target_train:]

    # ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–
    scaler = StandardScaler()
    X_source_scaled = scaler.fit_transform(X_source)
    X_target_train_scaled = scaler.transform(X_target_train)
    X_target_test_scaled = scaler.transform(X_target_test)

    # Transfer Learningã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    tl_system = TransferLearningSystem(input_size=n_features)

    # Phase 1: ã‚½ãƒ¼ã‚¹è¨­å‚™ã§ã®äº‹å‰è¨“ç·´
    tl_system.pretrain_on_source(X_source_scaled, y_source,
                                 epochs=100, batch_size=32)

    # Phase 2: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨­å‚™ã§ã®Fine-tuning
    tl_system.finetune_on_target(X_target_train_scaled, y_target_train,
                                 epochs=50, batch_size=16,
                                 freeze_feature_extractor=True)

    # è©•ä¾¡
    y_pred = tl_system.predict(X_target_test_scaled)
    mse = mean_squared_error(y_target_test, y_pred)
    r2 = r2_score(y_target_test, y_pred)

    print("\n" + "="*60)
    print("è©•ä¾¡çµæœï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨­å‚™ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆï¼‰")
    print("="*60)
    print(f"MSE: {mse:.4f}")
    print(f"RÂ² Score: {r2:.4f}")
    print(f"\nã‚ãšã‹{n_target_train}ã‚µãƒ³ãƒ—ãƒ«ã®Fine-tuningã§é«˜ç²¾åº¦ã‚’é”æˆï¼")

    # æ¯”è¼ƒï¼šTransfer Learningãªã—ï¼ˆã‚¼ãƒ­ã‹ã‚‰è¨“ç·´ï¼‰
    print("\nå‚è€ƒï¼šTransfer Learningãªã—ã®å ´åˆ...")
    baseline_model = DegradationPredictor(n_features)
    # å°‘é‡ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§è¨“ç·´ï¼ˆé€šå¸¸ã¯æ€§èƒ½ãŒå‡ºãªã„ï¼‰
    print(f"â†’ å°‘é‡ãƒ‡ãƒ¼ã‚¿ï¼ˆ{n_target_train}ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã§ã¯ååˆ†ãªæ€§èƒ½ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“")
</code></pre>
        </div>

        <h2>2.8 å®Ÿè£…ä¾‹7ï¼šEnsemble RULãƒ¢ãƒ‡ãƒ«</h2>

        <p>LSTMã€TCNã€XGBoostãªã©ã€ç•°ãªã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®äºˆæ¸¬ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚é ‘å¥ã§æ­£ç¢ºãªRULæ¨å®šãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚ã¾ãŸã€äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ã‚‚å®šé‡åŒ–ã§ãã¾ã™ã€‚</p>

        <div class="code-block">
            <div class="code-title">ğŸ“„ Example 7: Ensemble RULãƒ¢ãƒ‡ãƒ«ï¼ˆLSTM+TCN+XGBoostï¼‰</div>
            <pre><code>import numpy as np
import torch
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error

class EnsembleRULPredictor:
    """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«RULæ¨å®š

    LSTMã€TCNã€XGBoostã®äºˆæ¸¬ã‚’çµ±åˆã—ã€ä¸ç¢ºå®Ÿæ€§ã‚‚æ¨å®šã—ã¾ã™ã€‚
    """

    def __init__(self, input_size, seq_length):
        """
        Args:
            input_size (int): ã‚»ãƒ³ã‚µãƒ¼ç‰¹å¾´æ•°
            seq_length (int): æ™‚ç³»åˆ—é•·
        """
        self.input_size = input_size
        self.seq_length = seq_length

        # ãƒ¢ãƒ‡ãƒ«1: LSTMï¼ˆå‰è¿°ã®LSTMRULPredictorã‚’ä½¿ç”¨ï¼‰
        self.lstm_model = None  # å®Ÿéš›ã«ã¯å‰è¿°ã®LSTM_RUL_Systemã‚’ä½¿ç”¨

        # ãƒ¢ãƒ‡ãƒ«2: TCNï¼ˆå‰è¿°ã®TCN_RUL_Predictorã‚’ä½¿ç”¨ï¼‰
        self.tcn_model = None   # å®Ÿéš›ã«ã¯å‰è¿°ã®TCN_RUL_Systemã‚’ä½¿ç”¨

        # ãƒ¢ãƒ‡ãƒ«3: Gradient Boostingï¼ˆé›†ç´„ç‰¹å¾´é‡ã‚’ä½¿ç”¨ï¼‰
        self.gb_model = GradientBoostingRegressor(
            n_estimators=200,
            learning_rate=0.05,
            max_depth=5,
            random_state=42
        )

        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§æœ€é©åŒ–ï¼‰
        self.weights = {'lstm': 0.35, 'tcn': 0.35, 'gb': 0.30}

        self.is_trained = False

    def extract_aggregate_features(self, sequences):
        """æ™‚ç³»åˆ—ã‹ã‚‰é›†ç´„ç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆGradient Boostingç”¨ï¼‰

        Args:
            sequences (np.ndarray): æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ (n_samples, seq_len, features)

        Returns:
            np.ndarray: é›†ç´„ç‰¹å¾´é‡ (n_samples, agg_features)
        """
        agg_features = []

        for seq in sequences:
            # å„ã‚»ãƒ³ã‚µãƒ¼ã®çµ±è¨ˆé‡
            features = []
            for sensor_idx in range(seq.shape[1]):
                sensor_data = seq[:, sensor_idx]
                features.extend([
                    np.mean(sensor_data),           # å¹³å‡
                    np.std(sensor_data),            # æ¨™æº–åå·®
                    np.max(sensor_data),            # æœ€å¤§å€¤
                    np.min(sensor_data),            # æœ€å°å€¤
                    np.percentile(sensor_data, 75) - np.percentile(sensor_data, 25),  # IQR
                    np.mean(np.diff(sensor_data)),  # ãƒˆãƒ¬ãƒ³ãƒ‰
                ])
            agg_features.append(features)

        return np.array(agg_features)

    def train_gb_model(self, sequences, labels):
        """Gradient Boostingãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´

        Args:
            sequences (np.ndarray): æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
            labels (np.ndarray): RULãƒ©ãƒ™ãƒ«
        """
        print("Gradient Boostingãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­...")
        agg_features = self.extract_aggregate_features(sequences)
        self.gb_model.fit(agg_features, labels)
        print("Gradient Boostingè¨“ç·´å®Œäº†")

    def predict_ensemble(self, sequences):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬

        Args:
            sequences (np.ndarray): æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ (n_samples, seq_len, features)

        Returns:
            dict: äºˆæ¸¬çµæœã¨ä¸ç¢ºå®Ÿæ€§
        """
        if not self.is_trained:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        n_samples = len(sequences)
        predictions = {
            'lstm': np.zeros(n_samples),
            'tcn': np.zeros(n_samples),
            'gb': np.zeros(n_samples)
        }

        # LSTMäºˆæ¸¬ï¼ˆå®Ÿè£…æ¸ˆã¿ã®LSTMãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
        # predictions['lstm'] = self.lstm_model.predict(sequences)

        # TCNäºˆæ¸¬ï¼ˆå®Ÿè£…æ¸ˆã¿ã®TCNãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
        # predictions['tcn'] = self.tcn_model.predict(sequences)

        # Gradient Boostingäºˆæ¸¬
        agg_features = self.extract_aggregate_features(sequences)
        predictions['gb'] = self.gb_model.predict(agg_features)

        # ãƒ‡ãƒ¢ç”¨ã«ãƒ©ãƒ³ãƒ€ãƒ äºˆæ¸¬ã‚’ç”Ÿæˆï¼ˆå®Ÿéš›ã¯ä¸Šè¨˜ã®è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
        for i in range(n_samples):
            predictions['lstm'][i] = 100 + np.random.randn() * 10
            predictions['tcn'][i] = 98 + np.random.randn() * 10

        # åŠ é‡å¹³å‡ã«ã‚ˆã‚‹ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
        ensemble_pred = (self.weights['lstm'] * predictions['lstm'] +
                        self.weights['tcn'] * predictions['tcn'] +
                        self.weights['gb'] * predictions['gb'])

        # ä¸ç¢ºå®Ÿæ€§ã®æ¨å®šï¼ˆäºˆæ¸¬ã®ã°ã‚‰ã¤ãï¼‰
        all_preds = np.stack([predictions['lstm'],
                             predictions['tcn'],
                             predictions['gb']])
        uncertainty = np.std(all_preds, axis=0)

        results = {
            'ensemble_prediction': ensemble_pred,
            'individual_predictions': predictions,
            'uncertainty': uncertainty,
            'confidence_interval_95': (ensemble_pred - 1.96 * uncertainty,
                                      ensemble_pred + 1.96 * uncertainty)
        }

        return results

    def optimize_weights(self, val_sequences, val_labels):
        """æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§æœ€é©ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿ã‚’æ¢ç´¢

        Args:
            val_sequences (np.ndarray): æ¤œè¨¼ç”¨æ™‚ç³»åˆ—
            val_labels (np.ndarray): æ¤œè¨¼ç”¨RULãƒ©ãƒ™ãƒ«
        """
        print("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿ã‚’æœ€é©åŒ–ä¸­...")

        # å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’å–å¾—
        results = self.predict_ensemble(val_sequences)
        preds = results['individual_predictions']

        # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã§æœ€é©é‡ã¿ã‚’æ¢ç´¢
        best_mae = float('inf')
        best_weights = None

        for w_lstm in np.arange(0.2, 0.5, 0.05):
            for w_tcn in np.arange(0.2, 0.5, 0.05):
                w_gb = 1.0 - w_lstm - w_tcn
                if w_gb < 0.1 or w_gb > 0.5:
                    continue

                ensemble = (w_lstm * preds['lstm'] +
                           w_tcn * preds['tcn'] +
                           w_gb * preds['gb'])
                mae = mean_absolute_error(val_labels, ensemble)

                if mae < best_mae:
                    best_mae = mae
                    best_weights = {'lstm': w_lstm, 'tcn': w_tcn, 'gb': w_gb}

        self.weights = best_weights
        print(f"æœ€é©é‡ã¿: {self.weights}")
        print(f"æ¤œè¨¼MAE: {best_mae:.2f}")


class SimpleEnsembleDemo:
    """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«RULæ¨å®šã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""

    @staticmethod
    def run_demo():
        """ãƒ‡ãƒ¢ã®å®Ÿè¡Œ"""
        np.random.seed(42)

        print("="*60)
        print("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«RULæ¨å®šã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢")
        print("="*60)

        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿
        n_samples = 100
        seq_length = 30
        n_features = 8

        sequences = np.random.randn(n_samples, seq_length, n_features)
        true_rul = np.linspace(200, 10, n_samples) + np.random.randn(n_samples) * 5

        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        ensemble = EnsembleRULPredictor(input_size=n_features,
                                       seq_length=seq_length)

        # Gradient Boostingãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
        ensemble.train_gb_model(sequences[:80], true_rul[:80])
        ensemble.is_trained = True

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
        test_sequences = sequences[80:]
        test_labels = true_rul[80:]

        results = ensemble.predict_ensemble(test_sequences)

        # è©•ä¾¡
        mae = mean_absolute_error(test_labels,
                                 results['ensemble_prediction'])
        rmse = np.sqrt(mean_squared_error(test_labels,
                                          results['ensemble_prediction']))

        print(f"\nã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬æ€§èƒ½:")
        print(f"  MAE: {mae:.2f} cycles")
        print(f"  RMSE: {rmse:.2f} cycles")

        # å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½
        print(f"\nå€‹åˆ¥ãƒ¢ãƒ‡ãƒ«æ€§èƒ½:")
        for model_name, preds in results['individual_predictions'].items():
            mae_individual = mean_absolute_error(test_labels, preds)
            print(f"  {model_name.upper()}: MAE = {mae_individual:.2f} cycles")

        # ä¸ç¢ºå®Ÿæ€§ã®è¡¨ç¤º
        print(f"\näºˆæ¸¬ä¸ç¢ºå®Ÿæ€§:")
        print(f"  å¹³å‡ä¸ç¢ºå®Ÿæ€§: {np.mean(results['uncertainty']):.2f} cycles")
        print(f"  æœ€å¤§ä¸ç¢ºå®Ÿæ€§: {np.max(results['uncertainty']):.2f} cycles")

        # ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬ã®è¡¨ç¤º
        print(f"\näºˆæ¸¬ä¾‹ï¼ˆæœ€åˆã®3ã‚µãƒ³ãƒ—ãƒ«ï¼‰:")
        for i in range(min(3, len(test_labels))):
            pred = results['ensemble_prediction'][i]
            actual = test_labels[i]
            ci_low, ci_high = results['confidence_interval_95']
            unc = results['uncertainty'][i]

            print(f"\nã‚µãƒ³ãƒ—ãƒ« {i+1}:")
            print(f"  å®Ÿéš›ã®RUL: {actual:.1f} cycles")
            print(f"  äºˆæ¸¬RUL: {pred:.1f} cycles")
            print(f"  èª¤å·®: {abs(pred - actual):.1f} cycles")
            print(f"  ä¸ç¢ºå®Ÿæ€§: Â±{unc:.1f} cycles")
            print(f"  95%ä¿¡é ¼åŒºé–“: [{ci_low[i]:.1f}, {ci_high[i]:.1f}]")


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    demo = SimpleEnsembleDemo()
    demo.run_demo()

    print("\n" + "="*60)
    print("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®åˆ©ç‚¹:")
    print("="*60)
    print("1. å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šé ‘å¥ï¼ˆ1ãƒ¢ãƒ‡ãƒ«ãŒå¤±æ•—ã—ã¦ã‚‚ä»–ãŒè£œå®Œï¼‰")
    print("2. äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–ï¼ˆæ„æ€æ±ºå®šã®ä¿¡é ¼æ€§å‘ä¸Šï¼‰")
    print("3. ç•°ãªã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ‰ãˆã‚‹ï¼ˆLSTM=æ™‚ç³»åˆ—ã€GB=é›†ç´„ç‰¹å¾´ï¼‰")
</code></pre>
        </div>

        <h2>2.9 å®Ÿè£…ä¾‹8ï¼šäºˆçŸ¥ä¿å…¨ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ</h2>

        <p>ã“ã‚Œã¾ã§ã®æŠ€è¡“ã‚’çµ±åˆã—ã€ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã‹ã‚‰RULæ¨å®šã€ä¿å…¨è¨ˆç”»ã®æœ€é©åŒ–ã¾ã§ã‚’è‡ªå‹•åŒ–ã—ãŸå®Ÿç”¨çš„ãªäºˆçŸ¥ä¿å…¨ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚</p>

        <div class="code-block">
            <div class="code-title">ğŸ“„ Example 8: çµ±åˆäºˆçŸ¥ä¿å…¨ã‚·ã‚¹ãƒ†ãƒ </div>
            <pre><code>import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum
from typing import List, Dict, Optional

class MaintenanceAction(Enum):
    """ä¿å…¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³"""
    NONE = "ç›£è¦–ç¶™ç¶š"
    INSPECTION = "è©³ç´°ç‚¹æ¤œ"
    MINOR_MAINTENANCE = "å°è¦æ¨¡ä¿å…¨"
    MAJOR_OVERHAUL = "å¤§è¦æ¨¡ã‚ªãƒ¼ãƒãƒ¼ãƒ›ãƒ¼ãƒ«"
    EMERGENCY_SHUTDOWN = "ç·Šæ€¥åœæ­¢"


@dataclass
class EquipmentStatus:
    """è¨­å‚™çŠ¶æ…‹"""
    equipment_id: str
    timestamp: datetime
    rul_estimate: float          # æ¨å®šRUL [hours]
    rul_uncertainty: float        # ä¸ç¢ºå®Ÿæ€§ [hours]
    failure_mode: str             # æ•…éšœãƒ¢ãƒ¼ãƒ‰
    failure_probability: float    # æ•…éšœç¢ºç‡ [0-1]
    recommended_action: MaintenanceAction
    priority_score: float         # å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢ [0-100]


class PredictiveMaintenanceSystem:
    """çµ±åˆäºˆçŸ¥ä¿å…¨ã‚·ã‚¹ãƒ†ãƒ 

    ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾—ã€ç•°å¸¸æ¤œçŸ¥ã€RULæ¨å®šã€ä¿å…¨è¨ˆç”»ã‚’çµ±åˆç®¡ç†ã—ã¾ã™ã€‚
    """

    def __init__(self):
        # å„ã‚µãƒ–ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå‰è¿°ã®å®Ÿè£…ã‚’çµ±åˆï¼‰
        self.vibration_extractor = None      # VibrationFeatureExtractor
        self.failure_classifier = None        # FailureModeClassifier
        self.rul_predictor = None             # EnsembleRULPredictor

        # ä¿å…¨é–¾å€¤
        self.thresholds = {
            'rul_critical': 100,         # 100æ™‚é–“æœªæº€ã§ç·Šæ€¥å¯¾å¿œ
            'rul_warning': 500,          # 500æ™‚é–“æœªæº€ã§ä¿å…¨è¨ˆç”»
            'failure_prob_high': 0.7,    # æ•…éšœç¢ºç‡70%ä»¥ä¸Šã§è­¦å‘Š
            'uncertainty_high': 50       # ä¸ç¢ºå®Ÿæ€§50æ™‚é–“ä»¥ä¸Šã§è¿½åŠ ç›£è¦–
        }

        # ä¿å…¨å±¥æ­´
        self.maintenance_history = []

    def process_sensor_data(self, raw_sensor_data: Dict[str, np.ndarray]) -> pd.DataFrame:
        """ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã¨ç‰¹å¾´æŠ½å‡º

        Args:
            raw_sensor_data (dict): ç”Ÿã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿
                {
                    'vibration': np.ndarray,
                    'temperature': np.ndarray,
                    'pressure': np.ndarray,
                    'flow_rate': np.ndarray
                }

        Returns:
            pd.DataFrame: æŠ½å‡ºã•ã‚ŒãŸç‰¹å¾´é‡
        """
        features = {}

        # æŒ¯å‹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´æŠ½å‡º
        if 'vibration' in raw_sensor_data:
            vib_data = raw_sensor_data['vibration']
            features['vibration_rms'] = np.sqrt(np.mean(vib_data**2))
            features['vibration_peak'] = np.max(np.abs(vib_data))
            features['vibration_kurtosis'] = pd.Series(vib_data).kurtosis()

            # å‘¨æ³¢æ•°è§£æï¼ˆç°¡æ˜“ç‰ˆï¼‰
            fft_result = np.fft.fft(vib_data)
            power_spectrum = np.abs(fft_result)**2
            features['low_freq_power'] = np.sum(power_spectrum[:len(power_spectrum)//10])
            features['high_freq_power'] = np.sum(power_spectrum[len(power_spectrum)//2:])

        # ãã®ä»–ã®ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿
        if 'temperature' in raw_sensor_data:
            features['bearing_temp'] = np.mean(raw_sensor_data['temperature'])

        if 'pressure' in raw_sensor_data:
            features['suction_pressure'] = raw_sensor_data['pressure'][0]
            features['discharge_pressure'] = raw_sensor_data['pressure'][1]

        if 'flow_rate' in raw_sensor_data:
            features['flow_rate'] = np.mean(raw_sensor_data['flow_rate'])

        # æ¬ æç‰¹å¾´ã®è£œå®Œï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
        default_features = {
            'vibration_rms': 2.0, 'vibration_peak': 6.0, 'vibration_kurtosis': 3.0,
            'bearing_temp': 60.0, 'low_freq_power': 0.2, 'mid_freq_power': 0.3,
            'high_freq_power': 0.05, 'suction_pressure': 2.0,
            'discharge_pressure': 10.0, 'flow_rate': 100.0
        }
        for key, default_val in default_features.items():
            if key not in features:
                features[key] = default_val

        return pd.DataFrame([features])

    def diagnose_equipment(self, sensor_features: pd.DataFrame) -> Dict:
        """è¨­å‚™è¨ºæ–­ï¼ˆæ•…éšœãƒ¢ãƒ¼ãƒ‰åˆ†é¡ï¼‰

        Args:
            sensor_features (pd.DataFrame): ã‚»ãƒ³ã‚µãƒ¼ç‰¹å¾´é‡

        Returns:
            dict: è¨ºæ–­çµæœ
        """
        # ãƒ‡ãƒ¢ç”¨ã®ç°¡æ˜“è¨ºæ–­ï¼ˆå®Ÿéš›ã¯FailureModeClassifierã‚’ä½¿ç”¨ï¼‰
        vibration_rms = sensor_features['vibration_rms'].values[0]
        bearing_temp = sensor_features['bearing_temp'].values[0]

        if vibration_rms > 4.0 and bearing_temp > 80:
            mode = 'Bearing'
            confidence = 0.85
        elif sensor_features['suction_pressure'].values[0] < 1.5:
            mode = 'Cavitation'
            confidence = 0.78
        elif vibration_rms > 3.5:
            mode = 'Misalignment'
            confidence = 0.72
        else:
            mode = 'Normal'
            confidence = 0.92

        return {
            'failure_mode': mode,
            'confidence': confidence
        }

    def estimate_rul(self, sensor_sequence: np.ndarray) -> Dict:
        """RULæ¨å®š

        Args:
            sensor_sequence (np.ndarray): ã‚»ãƒ³ã‚µãƒ¼æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿

        Returns:
            dict: RULæ¨å®šçµæœ
        """
        # ãƒ‡ãƒ¢ç”¨ã®RULæ¨å®šï¼ˆå®Ÿéš›ã¯EnsembleRULPredictorã‚’ä½¿ç”¨ï¼‰
        base_rul = 1000
        degradation_factor = np.mean(np.abs(sensor_sequence[-10:])) / 50

        rul_estimate = max(10, base_rul - degradation_factor * 500)
        uncertainty = rul_estimate * 0.1  # 10%ã®ä¸ç¢ºå®Ÿæ€§

        return {
            'rul_estimate': rul_estimate,
            'uncertainty': uncertainty,
            'confidence_interval': (rul_estimate - uncertainty,
                                   rul_estimate + uncertainty)
        }

    def determine_maintenance_action(self, rul: float, failure_mode: str,
                                    failure_prob: float) -> MaintenanceAction:
        """ä¿å…¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æ±ºå®š

        Args:
            rul (float): æ¨å®šRUL [hours]
            failure_mode (str): æ•…éšœãƒ¢ãƒ¼ãƒ‰
            failure_prob (float): æ•…éšœç¢ºç‡

        Returns:
            MaintenanceAction: æ¨å¥¨ä¿å…¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        """
        if rul < self.thresholds['rul_critical'] or failure_prob > 0.9:
            return MaintenanceAction.EMERGENCY_SHUTDOWN

        elif rul < self.thresholds['rul_warning']:
            if failure_mode in ['Bearing', 'Seal']:
                return MaintenanceAction.MAJOR_OVERHAUL
            else:
                return MaintenanceAction.MINOR_MAINTENANCE

        elif failure_prob > self.thresholds['failure_prob_high']:
            return MaintenanceAction.INSPECTION

        else:
            return MaintenanceAction.NONE

    def calculate_priority_score(self, rul: float, failure_prob: float,
                                criticality: float = 0.8) -> float:
        """ä¿å…¨å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—

        Args:
            rul (float): æ¨å®šRUL
            failure_prob (float): æ•…éšœç¢ºç‡
            criticality (float): è¨­å‚™é‡è¦åº¦ [0-1]

        Returns:
            float: å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢ [0-100]
        """
        # RULæˆåˆ†ï¼ˆçŸ­ã„ã»ã©é«˜å„ªå…ˆåº¦ï¼‰
        rul_score = max(0, 100 - rul / 10)

        # æ•…éšœç¢ºç‡æˆåˆ†
        prob_score = failure_prob * 100

        # ç·åˆã‚¹ã‚³ã‚¢
        priority = (0.5 * rul_score + 0.3 * prob_score + 0.2 * criticality * 100)

        return min(100, priority)

    def monitor_equipment(self, equipment_id: str,
                         sensor_data: Dict[str, np.ndarray],
                         sensor_history: np.ndarray) -> EquipmentStatus:
        """è¨­å‚™ã®ç·åˆç›£è¦–

        Args:
            equipment_id (str): è¨­å‚™ID
            sensor_data (dict): ç¾åœ¨ã®ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿
            sensor_history (np.ndarray): éå»ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿

        Returns:
            EquipmentStatus: è¨­å‚™çŠ¶æ…‹ã¨æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        """
        # 1. ç‰¹å¾´æŠ½å‡º
        features = self.process_sensor_data(sensor_data)

        # 2. æ•…éšœãƒ¢ãƒ¼ãƒ‰è¨ºæ–­
        diagnosis = self.diagnose_equipment(features)

        # 3. RULæ¨å®š
        rul_result = self.estimate_rul(sensor_history)

        # 4. ä¿å…¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š
        action = self.determine_maintenance_action(
            rul_result['rul_estimate'],
            diagnosis['failure_mode'],
            diagnosis['confidence']
        )

        # 5. å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
        priority = self.calculate_priority_score(
            rul_result['rul_estimate'],
            diagnosis['confidence']
        )

        # è¨­å‚™çŠ¶æ…‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆ
        status = EquipmentStatus(
            equipment_id=equipment_id,
            timestamp=datetime.now(),
            rul_estimate=rul_result['rul_estimate'],
            rul_uncertainty=rul_result['uncertainty'],
            failure_mode=diagnosis['failure_mode'],
            failure_probability=diagnosis['confidence'],
            recommended_action=action,
            priority_score=priority
        )

        return status

    def generate_maintenance_schedule(self, equipment_statuses: List[EquipmentStatus],
                                     planning_horizon_days: int = 30) -> pd.DataFrame:
        """ä¿å…¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ç”Ÿæˆ

        Args:
            equipment_statuses (list): è¨­å‚™çŠ¶æ…‹ã®ãƒªã‚¹ãƒˆ
            planning_horizon_days (int): è¨ˆç”»æœŸé–“ [days]

        Returns:
            pd.DataFrame: ä¿å…¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
        """
        schedule = []

        for status in equipment_statuses:
            if status.recommended_action == MaintenanceAction.NONE:
                continue

            # ä¿å…¨å®Ÿæ–½æ¨å¥¨æ—¥ã‚’è¨ˆç®—
            days_until_maintenance = status.rul_estimate / 24  # hoursã‚’daysã«å¤‰æ›
            recommended_date = datetime.now() + timedelta(days=days_until_maintenance * 0.7)

            # æœŸé–“å†…ã®ä¿å…¨ã®ã¿ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã«è¿½åŠ 
            if (recommended_date - datetime.now()).days <= planning_horizon_days:
                schedule.append({
                    'equipment_id': status.equipment_id,
                    'recommended_date': recommended_date.strftime('%Y-%m-%d'),
                    'action': status.recommended_action.value,
                    'priority': status.priority_score,
                    'estimated_rul_hours': status.rul_estimate,
                    'failure_mode': status.failure_mode
                })

        df_schedule = pd.DataFrame(schedule)
        if not df_schedule.empty:
            df_schedule = df_schedule.sort_values('priority', ascending=False)

        return df_schedule


# ä½¿ç”¨ä¾‹ï¼šãƒ—ãƒ©ãƒ³ãƒˆå…¨ä½“ã®äºˆçŸ¥ä¿å…¨ã‚·ã‚¹ãƒ†ãƒ 
if __name__ == "__main__":
    print("="*70)
    print("çµ±åˆäºˆçŸ¥ä¿å…¨ã‚·ã‚¹ãƒ†ãƒ  - ãƒ—ãƒ©ãƒ³ãƒˆç›£è¦–ãƒ‡ãƒ¢")
    print("="*70)

    # ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    pm_system = PredictiveMaintenanceSystem()

    # è¤‡æ•°è¨­å‚™ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    equipment_list = ['PUMP-101', 'PUMP-102', 'COMP-201', 'HX-301']
    equipment_statuses = []

    np.random.seed(42)

    for eq_id in equipment_list:
        # ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        sensor_data = {
            'vibration': np.random.randn(10000) * 1.5 + 2.0,
            'temperature': np.random.randn(100) * 5 + 65,
            'pressure': np.random.uniform(1.5, 2.5, 2),
            'flow_rate': np.random.randn(100) * 3 + 98
        }

        # æ™‚ç³»åˆ—å±¥æ­´ï¼ˆ30ã‚µã‚¤ã‚¯ãƒ« Ã— 10ç‰¹å¾´ï¼‰
        sensor_history = np.random.randn(30, 10) * 10 + 50

        # è¨­å‚™ç›£è¦–ã®å®Ÿè¡Œ
        status = pm_system.monitor_equipment(eq_id, sensor_data, sensor_history)
        equipment_statuses.append(status)

        # çµæœã®è¡¨ç¤º
        print(f"\nã€è¨­å‚™ID: {status.equipment_id}ã€‘")
        print(f"  æ™‚åˆ»: {status.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"  æ¨å®šRUL: {status.rul_estimate:.1f} hours (Â±{status.rul_uncertainty:.1f})")
        print(f"  æ•…éšœãƒ¢ãƒ¼ãƒ‰: {status.failure_mode} (ç¢ºç‡: {status.failure_probability:.1%})")
        print(f"  æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {status.recommended_action.value}")
        print(f"  å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢: {status.priority_score:.1f}/100")

    # ä¿å…¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ç”Ÿæˆ
    print("\n" + "="*70)
    print("30æ—¥é–“ã®ä¿å…¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«")
    print("="*70)

    schedule = pm_system.generate_maintenance_schedule(equipment_statuses,
                                                      planning_horizon_days=30)

    if not schedule.empty:
        print(schedule.to_string(index=False))
    else:
        print("è¨ˆç”»æœŸé–“å†…ã«ä¿å…¨ãŒå¿…è¦ãªè¨­å‚™ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

    print("\n" + "="*70)
    print("ã‚·ã‚¹ãƒ†ãƒ ã®ç‰¹é•·:")
    print("="*70)
    print("âœ“ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã¨è‡ªå‹•è¨ºæ–­")
    print("âœ“ è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹é«˜ç²¾åº¦RULæ¨å®š")
    print("âœ“ å„ªå…ˆåº¦ãƒ™ãƒ¼ã‚¹ã®ä¿å…¨è¨ˆç”»æœ€é©åŒ–")
    print("âœ“ ä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®ã—ãŸãƒªã‚¹ã‚¯ç®¡ç†")
</code></pre>
        </div>

        <h2>ã¾ã¨ã‚</h2>

        <p>æœ¬ç« ã§ã¯ã€åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆã«ãŠã‘ã‚‹äºˆçŸ¥ä¿å…¨ã¨RULæ¨å®šã®å®Ÿè£…æŠ€è¡“ã‚’å­¦ã³ã¾ã—ãŸã€‚ä¸»è¦ãªãƒã‚¤ãƒ³ãƒˆã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š</p>

        <div class="section-intro">
            <h3>å­¦ç¿’å†…å®¹ã®æŒ¯ã‚Šè¿”ã‚Š</h3>
            <ol>
                <li><strong>æŒ¯å‹•ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´æŠ½å‡º</strong>ï¼šFFTã«ã‚ˆã‚‹å‘¨æ³¢æ•°è§£æã¨æ™‚é–“é ˜åŸŸçµ±è¨ˆé‡ã«ã‚ˆã‚Šã€æ•…éšœã®å…†å€™ã‚’å®šé‡åŒ–</li>
                <li><strong>Survival Analysis</strong>ï¼šCoxæ¯”ä¾‹ãƒã‚¶ãƒ¼ãƒ‰ãƒ¢ãƒ‡ãƒ«ã§è¤‡æ•°ã®é‹è»¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ•…éšœãƒªã‚¹ã‚¯ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’è©•ä¾¡</li>
                <li><strong>LSTM RULæ¨å®š</strong>ï¼šæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®é•·æœŸä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã—ã€æ®‹å­˜æœ‰åŠ¹å¯¿å‘½ã‚’é«˜ç²¾åº¦ã«äºˆæ¸¬</li>
                <li><strong>TCN RULæ¨å®š</strong>ï¼šDilated Causal Convolutionã§ä¸¦åˆ—å‡¦ç†ãŒå¯èƒ½ãªåŠ¹ç‡çš„ãªRULæ¨å®šã‚’å®Ÿç¾</li>
                <li><strong>æ•…éšœãƒ¢ãƒ¼ãƒ‰åˆ†é¡</strong>ï¼šRandom Forestã§è»¸å—ä¸è‰¯ã€ã‚·ãƒ¼ãƒ«æ¼ã‚Œã€ã‚­ãƒ£ãƒ“ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç­‰ã‚’è‡ªå‹•è¨ºæ–­</li>
                <li><strong>Transfer Learning</strong>ï¼šé¡ä¼¼è¨­å‚™ã®çŸ¥è­˜ã‚’è»¢ç§»ã™ã‚‹ã“ã¨ã§ã€å°‘é‡ãƒ‡ãƒ¼ã‚¿ã§ã‚‚é«˜ç²¾åº¦ãªäºˆæ¸¬ã‚’é”æˆ</li>
                <li><strong>Ensemble RUL</strong>ï¼šè¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµ±åˆã«ã‚ˆã‚Šé ‘å¥æ€§ã¨ä¸ç¢ºå®Ÿæ€§æ¨å®šã‚’æ”¹å–„</li>
                <li><strong>çµ±åˆäºˆçŸ¥ä¿å…¨ã‚·ã‚¹ãƒ†ãƒ </strong>ï¼šãƒ‡ãƒ¼ã‚¿å–å¾—ã‹ã‚‰ä¿å…¨è¨ˆç”»ã¾ã§è‡ªå‹•åŒ–ã—ãŸå®Ÿç”¨ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰</li>
            </ol>
        </div>

        <div class="callout">
            <div class="callout-title">ğŸ¯ å®Ÿå‹™ã¸ã®é©ç”¨</div>
            <p><strong>å°å…¥åŠ¹æœã®å®Ÿä¾‹ï¼š</strong></p>
            <ul>
                <li><strong>Aç¤¾ï¼ˆçŸ³æ²¹åŒ–å­¦ï¼‰</strong>ï¼šåœ§ç¸®æ©Ÿã¸ã®RULæ¨å®šã‚·ã‚¹ãƒ†ãƒ å°å…¥ã«ã‚ˆã‚Šã€è¨ˆç”»å¤–åœæ­¢ã‚’å¹´é–“8å›â†’1å›ã«å‰Šæ¸›ã€ä¿å…¨ã‚³ã‚¹ãƒˆ15%å‰Šæ¸›</li>
                <li><strong>Bç¤¾ï¼ˆè£½è–¬ï¼‰</strong>ï¼šãƒãƒ³ãƒ—ã®æŒ¯å‹•ç›£è¦–ã¨æ•…éšœãƒ¢ãƒ¼ãƒ‰è¨ºæ–­ã«ã‚ˆã‚Šã€é‡å¤§æ•…éšœã‚’äº‹å‰æ¤œå‡ºç‡92%ã‚’é”æˆ</li>
                <li><strong>Cç¤¾ï¼ˆåŒ–å­¦å“è£½é€ ï¼‰</strong>ï¼šTransfer Learningã«ã‚ˆã‚Šæ–°è¨­å‚™ã¸ã®å±•é–‹æœŸé–“ã‚’å¾“æ¥ã®6ãƒ¶æœˆâ†’1ãƒ¶æœˆã«çŸ­ç¸®</li>
            </ul>
        </div>

        <h3>æ¬¡ç« ã¸ã®æ¥ç¶š</h3>

        <p>æ¬¡ç« ã§ã¯ã€ãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ–ã¨ã‚¨ãƒãƒ«ã‚®ãƒ¼ç®¡ç†ã«ã¤ã„ã¦å­¦ã³ã¾ã™ã€‚å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹å‹•çš„æœ€é©åŒ–ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼æ¶ˆè²»äºˆæ¸¬ã€ãƒ—ãƒ©ãƒ³ãƒˆå…¨ä½“ã®çµ±åˆæœ€é©åŒ–æ‰‹æ³•ã‚’å®Ÿè£…ãƒ¬ãƒ™ãƒ«ã§ç¿’å¾—ã—ã¾ã™ã€‚</p>

        <div class="navigation">
            <a href="chapter-1.html" class="nav-button">
                â† ç¬¬1ç« ï¼šãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã¨ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼
            </a>
            <a href="chapter-3.html" class="nav-button">
                ç¬¬3ç« ï¼šãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ– â†’
            </a>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({
            startOnLoad: true,
            theme: 'base',
            themeVariables: {
                primaryColor: '#11998e',
                primaryTextColor: '#2c3e50',
                primaryBorderColor: '#11998e',
                lineColor: '#11998e',
                secondaryColor: '#38ef7d',
                tertiaryColor: '#e3f2fd'
            }
        });
    </script>
</body>
</html>
