<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="ç¬¬1ç« ï¼šãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã¨ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ - åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆã«ãŠã‘ã‚‹AIãƒ™ãƒ¼ã‚¹ç•°å¸¸æ¤œçŸ¥ã€å“è³ªäºˆæ¸¬ã€ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼è¨­è¨ˆã‚’å®Ÿè£…ãƒ¬ãƒ™ãƒ«ã§ç¿’å¾—">
    <title>ç¬¬1ç« ï¼šãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã¨ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ - åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆã¸ã®AIå¿œç”¨</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #11998e;
            --color-accent-light: #38ef7d;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #11998e;
            --color-link-hover: #0d7a6f;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(17, 153, 142, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        .code-example {
            background: #f8f9fa;
            border-left: 4px solid #11998e;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 8px;
        }

        .code-example h4 {
            margin-top: 0;
            color: #11998e;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/AI-Knowledge-Notes/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/AI-Knowledge-Notes/knowledge/jp/PI/index.html">ãƒ—ãƒ­ã‚»ã‚¹ãƒ»ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹</a><span class="breadcrumb-separator">â€º</span><a href="/AI-Knowledge-Notes/knowledge/jp/PI/chemical-plant-ai/index.html">Chemical Plant Ai</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 1</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>ç¬¬1ç« ï¼šãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã¨ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼</h1>
            <p class="subtitle">AIãƒ™ãƒ¼ã‚¹ç•°å¸¸æ¤œçŸ¥ã¨å“è³ªäºˆæ¸¬ã®å®Ÿè£…</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 30-35åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: å®Ÿè·µãƒ»å¿œç”¨</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
            </div>
        </div>
    </header>

    <main class="container">

<div class="learning-objectives">
<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… çµ±è¨ˆçš„ç•°å¸¸æ¤œçŸ¥ï¼ˆPCAã€Qçµ±è¨ˆé‡ã€TÂ²çµ±è¨ˆé‡ï¼‰ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… æ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹ç•°å¸¸æ¤œçŸ¥ï¼ˆIsolation Forestã€Autoencoderã€LSTMï¼‰ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
<li>âœ… å“è³ªäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ï¼ˆRandom Forestï¼‰ã§è£½å“å“è³ªã‚’äºˆæ¸¬ã§ãã‚‹</li>
<li>âœ… ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ï¼ˆGPRã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆï¼‰ã§æ¸¬å®šå›°é›£ãªå¤‰æ•°ã‚’æ¨å®šã§ãã‚‹</li>
<li>âœ… çµ±åˆãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’è¨­è¨ˆãƒ»å®Ÿè£…ã§ãã‚‹</li>
</ul>
</div>

<hr />

<h2>1.1 åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆç›£è¦–ã®èª²é¡Œã¨AIæŠ€è¡“</h2>

<h3>åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆç‰¹æœ‰ã®ç›£è¦–èª²é¡Œ</h3>

<p>åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆã®ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã¯ã€è£½å“å“è³ªã€å®‰å…¨æ€§ã€çµŒæ¸ˆæ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã®æœ€é‡è¦èª²é¡Œã§ã™ã€‚å¾“æ¥ã®é–¾å€¤ãƒ™ãƒ¼ã‚¹ç›£è¦–ã§ã¯æ¤œå‡ºå›°é›£ãªç•°å¸¸ãŒå¤šæ•°å­˜åœ¨ã—ã¾ã™ï¼š</p>

<ul>
<li><strong>å¤šå¤‰é‡ç›¸é–¢ç•°å¸¸</strong>: å€‹åˆ¥å¤‰æ•°ã¯æ­£å¸¸ç¯„å›²å†…ã§ã‚‚ã€å¤‰æ•°é–“ã®ç›¸é–¢ãŒç•°å¸¸</li>
<li><strong>ç·©ã‚„ã‹ãªåŠ£åŒ–</strong>: è§¦åª’æ´»æ€§ä½ä¸‹ã€ç†±äº¤æ›å™¨æ±šã‚Œãªã©ã€æ•°é€±é–“ï½æ•°ãƒ¶æœˆå˜ä½ã®å¤‰åŒ–</li>
<li><strong>æ¸¬å®šå›°é›£å¤‰æ•°</strong>: è£½å“å“è³ªï¼ˆç´”åº¦ã€ç²˜åº¦ï¼‰ã€åå¿œç‡ãªã©ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¸¬å®šãŒå›°é›£</li>
<li><strong>éç·šå½¢æŒ™å‹•</strong>: åå¿œå™¨ã®éç·šå½¢å‹•ç‰¹æ€§ã€è’¸ç•™å¡”ã®è¤‡é›‘ãªç›¸äº’ä½œç”¨</li>
</ul>

<h3>AIæŠ€è¡“ã«ã‚ˆã‚‹è§£æ±ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</h3>

<div class="mermaid">
graph TD
    A[ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–èª²é¡Œ] --> B[çµ±è¨ˆçš„æ‰‹æ³•]
    A --> C[æ©Ÿæ¢°å­¦ç¿’]
    A --> D[æ·±å±¤å­¦ç¿’]

    B --> B1[PCAç•°å¸¸æ¤œçŸ¥]
    B --> B2[çµ±è¨ˆçš„ãƒ—ãƒ­ã‚»ã‚¹ç®¡ç†]

    C --> C1[Isolation Forest]
    C --> C2[Random Forestå“è³ªäºˆæ¸¬]
    C --> C3[GPRã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼]

    D --> D1[Autoencoderç•°å¸¸æ¤œçŸ¥]
    D --> D2[LSTMæ™‚ç³»åˆ—äºˆæ¸¬]
    D --> D3[NN-ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼]

    style A fill:#11998e,stroke:#0d7a6f,color:#fff
    style B fill:#38ef7d,stroke:#2bc766,color:#333
    style C fill:#38ef7d,stroke:#2bc766,color:#333
    style D fill:#38ef7d,stroke:#2bc766,color:#333
</div>

<hr />

<h2>1.2 çµ±è¨ˆçš„ç•°å¸¸æ¤œçŸ¥ã®å®Ÿè£…</h2>

<div class="code-example">
<h4>ã‚³ãƒ¼ãƒ‰ä¾‹1: PCAæ³•ã«ã‚ˆã‚‹å¤šå¤‰é‡çµ±è¨ˆçš„ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–</h4>

<p><strong>ç›®çš„</strong>: ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ã‚’ç”¨ã„ã¦Qçµ±è¨ˆé‡ï¼ˆSPEï¼‰ã¨TÂ²çµ±è¨ˆé‡ã§ãƒ—ãƒ­ã‚»ã‚¹ç•°å¸¸ã‚’æ¤œå‡ºã™ã‚‹ã€‚</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from scipy import stats

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.sans-serif'] = ['Hiragino Sans', 'Arial']
plt.rcParams['axes.unicode_minus'] = False

# åŒ–å­¦åå¿œå™¨ã®æ­£å¸¸é‹è»¢ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰
np.random.seed(42)
n_samples_normal = 500

# ãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°: æ¸©åº¦ã€åœ§åŠ›ã€æµé‡ã€æ¿ƒåº¦ï¼ˆç›¸é–¢ã‚ã‚Šï¼‰
temperature = np.random.normal(350, 10, n_samples_normal)  # K
pressure = 2.0 + 0.01 * (temperature - 350) + np.random.normal(0, 0.1, n_samples_normal)  # bar
flow_rate = 100 + 0.5 * (temperature - 350) + np.random.normal(0, 5, n_samples_normal)  # L/h
concentration = 0.8 - 0.001 * (temperature - 350) + np.random.normal(0, 0.05, n_samples_normal)  # mol/L

# æ­£å¸¸é‹è»¢ãƒ‡ãƒ¼ã‚¿
X_normal = np.column_stack([temperature, pressure, flow_rate, concentration])

# ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
mean = X_normal.mean(axis=0)
std = X_normal.std(axis=0)
X_scaled = (X_normal - mean) / std

# PCAãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ï¼ˆä¸»æˆåˆ†æ•°=2ï¼‰
pca = PCA(n_components=2)
pca.fit(X_scaled)

print("=== PCAãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ ===")
print(f"ç´¯ç©å¯„ä¸ç‡: {pca.explained_variance_ratio_.cumsum()}")
print(f"ä¸»æˆåˆ†1ã®å¯„ä¸ç‡: {pca.explained_variance_ratio_[0]:.3f}")
print(f"ä¸»æˆåˆ†2ã®å¯„ä¸ç‡: {pca.explained_variance_ratio_[1]:.3f}")

# Qçµ±è¨ˆé‡ï¼ˆSPE: Squared Prediction Errorï¼‰ã®è¨ˆç®—
def compute_Q_statistic(X, pca_model):
    """Qçµ±è¨ˆé‡ï¼ˆæ®‹å·®ç©ºé–“ã®ãƒãƒ«ãƒ ï¼‰ã‚’è¨ˆç®—"""
    X_reconstructed = pca_model.inverse_transform(pca_model.transform(X))
    residuals = X - X_reconstructed
    Q = np.sum(residuals**2, axis=1)
    return Q

# TÂ²çµ±è¨ˆé‡ï¼ˆHotelling's T-squaredï¼‰ã®è¨ˆç®—
def compute_T2_statistic(X, pca_model):
    """TÂ²çµ±è¨ˆé‡ï¼ˆä¸»æˆåˆ†ç©ºé–“ã®è·é›¢ï¼‰ã‚’è¨ˆç®—"""
    scores = pca_model.transform(X)
    eigenvalues = pca_model.explained_variance_
    T2 = np.sum((scores**2) / eigenvalues, axis=1)
    return T2

# æ­£å¸¸é‹è»¢ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆé‡
Q_normal = compute_Q_statistic(X_scaled, pca)
T2_normal = compute_T2_statistic(X_scaled, pca)

# ç®¡ç†é™ç•Œã®è¨ˆç®—ï¼ˆ99%ä¿¡é ¼åŒºé–“ï¼‰
Q_limit = np.percentile(Q_normal, 99)
T2_limit = np.percentile(T2_normal, 99)

print(f"\nç®¡ç†é™ç•Œ:")
print(f"Qçµ±è¨ˆé‡é™ç•Œ: {Q_limit:.3f}")
print(f"TÂ²çµ±è¨ˆé‡é™ç•Œ: {T2_limit:.3f}")

# ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰
n_samples_test = 100

# ã‚±ãƒ¼ã‚¹1: æ¸©åº¦ç•°å¸¸ï¼ˆåå¿œæš´èµ°ï¼‰
temp_anomaly = np.random.normal(380, 10, 20)  # é«˜æ¸©ç•°å¸¸
press_anomaly = 2.0 + 0.01 * (temp_anomaly - 350) + np.random.normal(0, 0.1, 20)
flow_anomaly = 100 + 0.5 * (temp_anomaly - 350) + np.random.normal(0, 5, 20)
conc_anomaly = 0.8 - 0.001 * (temp_anomaly - 350) + np.random.normal(0, 0.05, 20)

# ã‚±ãƒ¼ã‚¹2: ç›¸é–¢ç•°å¸¸ï¼ˆã‚»ãƒ³ã‚µãƒ¼æ•…éšœï¼‰
temp_corr_anomaly = np.random.normal(350, 10, 20)
press_corr_anomaly = np.random.normal(2.0, 0.5, 20)  # åœ§åŠ›ã®ç›¸é–¢ãŒå´©ã‚Œã‚‹
flow_corr_anomaly = 100 + 0.5 * (temp_corr_anomaly - 350) + np.random.normal(0, 5, 20)
conc_corr_anomaly = 0.8 - 0.001 * (temp_corr_anomaly - 350) + np.random.normal(0, 0.05, 20)

# æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¯”è¼ƒç”¨ï¼‰
temp_test = np.random.normal(350, 10, 60)
press_test = 2.0 + 0.01 * (temp_test - 350) + np.random.normal(0, 0.1, 60)
flow_test = 100 + 0.5 * (temp_test - 350) + np.random.normal(0, 5, 60)
conc_test = 0.8 - 0.001 * (temp_test - 350) + np.random.normal(0, 0.05, 60)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿çµåˆ
X_test = np.vstack([
    np.column_stack([temp_test, press_test, flow_test, conc_test]),
    np.column_stack([temp_anomaly, press_anomaly, flow_anomaly, conc_anomaly]),
    np.column_stack([temp_corr_anomaly, press_corr_anomaly, flow_corr_anomaly, conc_corr_anomaly])
])

# ãƒ©ãƒ™ãƒ«ï¼ˆ0: æ­£å¸¸, 1: æ¸©åº¦ç•°å¸¸, 2: ç›¸é–¢ç•°å¸¸ï¼‰
labels = np.array([0]*60 + [1]*20 + [2]*20)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ–
X_test_scaled = (X_test - mean) / std

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆé‡è¨ˆç®—
Q_test = compute_Q_statistic(X_test_scaled, pca)
T2_test = compute_T2_statistic(X_test_scaled, pca)

# ç•°å¸¸æ¤œå‡º
anomaly_Q = Q_test > Q_limit
anomaly_T2 = T2_test > T2_limit
anomaly_combined = anomaly_Q | anomaly_T2

print(f"\nç•°å¸¸æ¤œå‡ºçµæœ:")
print(f"Qçµ±è¨ˆé‡ã«ã‚ˆã‚‹æ¤œå‡ºæ•°: {anomaly_Q.sum()}/{len(Q_test)}")
print(f"TÂ²çµ±è¨ˆé‡ã«ã‚ˆã‚‹æ¤œå‡ºæ•°: {anomaly_T2.sum()}/{len(T2_test)}")
print(f"çµ±åˆæ¤œå‡ºæ•°: {anomaly_combined.sum()}/{len(Q_test)}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Qçµ±è¨ˆé‡ã®æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆ
axes[0, 0].plot(Q_test, 'o-', markersize=4, linewidth=0.8, color='#11998e', label='Qçµ±è¨ˆé‡')
axes[0, 0].axhline(y=Q_limit, color='red', linestyle='--', linewidth=2, label=f'ç®¡ç†é™ç•Œ (99%)')
axes[0, 0].fill_between(range(len(Q_test)), 0, Q_limit, alpha=0.1, color='green')
axes[0, 0].scatter(np.where(labels==1)[0], Q_test[labels==1], color='orange', s=80,
                   marker='x', linewidths=3, label='æ¸©åº¦ç•°å¸¸', zorder=5)
axes[0, 0].scatter(np.where(labels==2)[0], Q_test[labels==2], color='purple', s=80,
                   marker='^', linewidths=3, label='ç›¸é–¢ç•°å¸¸', zorder=5)
axes[0, 0].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ç•ªå·', fontsize=11)
axes[0, 0].set_ylabel('Qçµ±è¨ˆé‡', fontsize=11)
axes[0, 0].set_title('Qçµ±è¨ˆé‡ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥ï¼ˆæ®‹å·®ç©ºé–“ï¼‰', fontsize=13, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)

# TÂ²çµ±è¨ˆé‡ã®æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆ
axes[0, 1].plot(T2_test, 'o-', markersize=4, linewidth=0.8, color='#38ef7d', label='TÂ²çµ±è¨ˆé‡')
axes[0, 1].axhline(y=T2_limit, color='red', linestyle='--', linewidth=2, label=f'ç®¡ç†é™ç•Œ (99%)')
axes[0, 1].fill_between(range(len(T2_test)), 0, T2_limit, alpha=0.1, color='green')
axes[0, 1].scatter(np.where(labels==1)[0], T2_test[labels==1], color='orange', s=80,
                   marker='x', linewidths=3, label='æ¸©åº¦ç•°å¸¸', zorder=5)
axes[0, 1].scatter(np.where(labels==2)[0], T2_test[labels==2], color='purple', s=80,
                   marker='^', linewidths=3, label='ç›¸é–¢ç•°å¸¸', zorder=5)
axes[0, 1].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ç•ªå·', fontsize=11)
axes[0, 1].set_ylabel('TÂ²çµ±è¨ˆé‡', fontsize=11)
axes[0, 1].set_title('TÂ²çµ±è¨ˆé‡ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥ï¼ˆä¸»æˆåˆ†ç©ºé–“ï¼‰', fontsize=13, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# Q-TÂ²ãƒ—ãƒ­ãƒƒãƒˆ
axes[1, 0].scatter(Q_test[labels==0], T2_test[labels==0], c='blue', s=30, alpha=0.6, label='æ­£å¸¸')
axes[1, 0].scatter(Q_test[labels==1], T2_test[labels==1], c='orange', s=80, marker='x',
                   linewidths=3, label='æ¸©åº¦ç•°å¸¸')
axes[1, 0].scatter(Q_test[labels==2], T2_test[labels==2], c='purple', s=80, marker='^',
                   linewidths=3, label='ç›¸é–¢ç•°å¸¸')
axes[1, 0].axvline(x=Q_limit, color='red', linestyle='--', alpha=0.5, label='Qé™ç•Œ')
axes[1, 0].axhline(y=T2_limit, color='red', linestyle='--', alpha=0.5, label='TÂ²é™ç•Œ')
axes[1, 0].set_xlabel('Qçµ±è¨ˆé‡', fontsize=11)
axes[1, 0].set_ylabel('TÂ²çµ±è¨ˆé‡', fontsize=11)
axes[1, 0].set_title('Q-TÂ²ãƒ—ãƒ­ãƒƒãƒˆï¼ˆç•°å¸¸è¨ºæ–­ï¼‰', fontsize=13, fontweight='bold')
axes[1, 0].legend()
axes[1, 0].grid(alpha=0.3)

# ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢ãƒ—ãƒ­ãƒƒãƒˆ
scores = pca.transform(X_test_scaled)
axes[1, 1].scatter(scores[labels==0, 0], scores[labels==0, 1], c='blue', s=30, alpha=0.6, label='æ­£å¸¸')
axes[1, 1].scatter(scores[labels==1, 0], scores[labels==1, 1], c='orange', s=80, marker='x',
                   linewidths=3, label='æ¸©åº¦ç•°å¸¸')
axes[1, 1].scatter(scores[labels==2, 0], scores[labels==2, 1], c='purple', s=80, marker='^',
                   linewidths=3, label='ç›¸é–¢ç•°å¸¸')
axes[1, 1].set_xlabel(f'ç¬¬1ä¸»æˆåˆ† ({pca.explained_variance_ratio_[0]:.1%})', fontsize=11)
axes[1, 1].set_ylabel(f'ç¬¬2ä¸»æˆåˆ† ({pca.explained_variance_ratio_[1]:.1%})', fontsize=11)
axes[1, 1].set_title('ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢ãƒ—ãƒ­ãƒƒãƒˆ', fontsize=13, fontweight='bold')
axes[1, 1].legend()
axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>è§£èª¬</strong>: PCAãƒ™ãƒ¼ã‚¹ç›£è¦–ã¯åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆã§æœ€ã‚‚åºƒãä½¿ç”¨ã•ã‚Œã‚‹çµ±è¨ˆçš„æ‰‹æ³•ã§ã™ã€‚Qçµ±è¨ˆé‡ã¯æ®‹å·®ç©ºé–“ã®ç•°å¸¸ï¼ˆã‚»ãƒ³ã‚µãƒ¼æ•…éšœã€ç›¸é–¢å´©ã‚Œï¼‰ã‚’æ¤œå‡ºã—ã€TÂ²çµ±è¨ˆé‡ã¯ä¸»æˆåˆ†ç©ºé–“ã®ç•°å¸¸ï¼ˆãƒ—ãƒ­ã‚»ã‚¹å¤‰å‹•ï¼‰ã‚’æ¤œå‡ºã—ã¾ã™ã€‚ä¸¡è€…ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ç•°ãªã‚‹ç¨®é¡ã®ç•°å¸¸ã‚’è¨ºæ–­ã§ãã¾ã™ã€‚</p>
</div>

<div class="code-example">
<h4>ã‚³ãƒ¼ãƒ‰ä¾‹2: Isolation Forestã«ã‚ˆã‚‹å¤šå¤‰é‡ç•°å¸¸æ¤œçŸ¥</h4>

<p><strong>ç›®çš„</strong>: Isolation Forestã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ãƒ—ãƒ­ã‚»ã‚¹ç•°å¸¸ã‚’æ¤œå‡ºã—ã€ç•°å¸¸ã‚¹ã‚³ã‚¢ã‚’å¯è¦–åŒ–ã™ã‚‹ã€‚</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.metrics import classification_report, confusion_matrix

np.random.seed(42)

# è’¸ç•™å¡”ã®é‹è»¢ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
n_normal = 800
n_anomaly = 50

# æ­£å¸¸é‹è»¢ãƒ‡ãƒ¼ã‚¿
å¡”é ‚æ¸©åº¦_æ­£å¸¸ = np.random.normal(85, 2, n_normal)  # Â°C
å¡”åº•æ¸©åº¦_æ­£å¸¸ = np.random.normal(155, 3, n_normal)  # Â°C
é‚„æµæ¯”_æ­£å¸¸ = np.random.normal(3.5, 0.3, n_normal)
è£½å“ç´”åº¦_æ­£å¸¸ = 0.98 + 0.01 * (å¡”é ‚æ¸©åº¦_æ­£å¸¸ - 85) / 10 + np.random.normal(0, 0.005, n_normal)

# ç•°å¸¸é‹è»¢ãƒ‡ãƒ¼ã‚¿ï¼ˆè¤‡æ•°ã®ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
# ãƒ‘ã‚¿ãƒ¼ãƒ³1: å¡”é ‚æ¸©åº¦ç•°å¸¸ï¼ˆå†·å´å™¨æ•…éšœï¼‰
å¡”é ‚æ¸©åº¦_ç•°å¸¸1 = np.random.normal(95, 3, 20)
å¡”åº•æ¸©åº¦_ç•°å¸¸1 = np.random.normal(155, 3, 20)
é‚„æµæ¯”_ç•°å¸¸1 = np.random.normal(3.5, 0.3, 20)
è£½å“ç´”åº¦_ç•°å¸¸1 = 0.98 + 0.01 * (å¡”é ‚æ¸©åº¦_ç•°å¸¸1 - 85) / 10 + np.random.normal(0, 0.01, 20)

# ãƒ‘ã‚¿ãƒ¼ãƒ³2: é‚„æµæ¯”ç•°å¸¸ï¼ˆãƒãƒ³ãƒ—æ•…éšœï¼‰
å¡”é ‚æ¸©åº¦_ç•°å¸¸2 = np.random.normal(85, 2, 15)
å¡”åº•æ¸©åº¦_ç•°å¸¸2 = np.random.normal(155, 3, 15)
é‚„æµæ¯”_ç•°å¸¸2 = np.random.normal(2.0, 0.5, 15)
è£½å“ç´”åº¦_ç•°å¸¸2 = 0.85 + np.random.normal(0, 0.02, 15)

# ãƒ‘ã‚¿ãƒ¼ãƒ³3: è¤‡åˆç•°å¸¸ï¼ˆåŸæ–™çµ„æˆå¤‰å‹•ï¼‰
å¡”é ‚æ¸©åº¦_ç•°å¸¸3 = np.random.normal(90, 4, 15)
å¡”åº•æ¸©åº¦_ç•°å¸¸3 = np.random.normal(165, 5, 15)
é‚„æµæ¯”_ç•°å¸¸3 = np.random.normal(4.5, 0.5, 15)
è£½å“ç´”åº¦_ç•°å¸¸3 = 0.92 + np.random.normal(0, 0.015, 15)

# ãƒ‡ãƒ¼ã‚¿çµ±åˆ
X = np.vstack([
    np.column_stack([å¡”é ‚æ¸©åº¦_æ­£å¸¸, å¡”åº•æ¸©åº¦_æ­£å¸¸, é‚„æµæ¯”_æ­£å¸¸, è£½å“ç´”åº¦_æ­£å¸¸]),
    np.column_stack([å¡”é ‚æ¸©åº¦_ç•°å¸¸1, å¡”åº•æ¸©åº¦_ç•°å¸¸1, é‚„æµæ¯”_ç•°å¸¸1, è£½å“ç´”åº¦_ç•°å¸¸1]),
    np.column_stack([å¡”é ‚æ¸©åº¦_ç•°å¸¸2, å¡”åº•æ¸©åº¦_ç•°å¸¸2, é‚„æµæ¯”_ç•°å¸¸2, è£½å“ç´”åº¦_ç•°å¸¸2]),
    np.column_stack([å¡”é ‚æ¸©åº¦_ç•°å¸¸3, å¡”åº•æ¸©åº¦_ç•°å¸¸3, é‚„æµæ¯”_ç•°å¸¸3, è£½å“ç´”åº¦_ç•°å¸¸3])
])

# ãƒ©ãƒ™ãƒ«ï¼ˆ1: æ­£å¸¸, -1: ç•°å¸¸ï¼‰
y_true = np.array([1]*n_normal + [-1]*n_anomaly)

# DataFrameã«å¤‰æ›
df = pd.DataFrame(X, columns=['å¡”é ‚æ¸©åº¦', 'å¡”åº•æ¸©åº¦', 'é‚„æµæ¯”', 'è£½å“ç´”åº¦'])
df['ãƒ©ãƒ™ãƒ«'] = y_true

# Isolation Forestãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
iso_forest = IsolationForest(
    contamination=0.05,  # ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆï¼ˆ5%ï¼‰
    n_estimators=100,
    max_samples='auto',
    random_state=42,
    n_jobs=-1
)

# å…¨ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ï¼ˆå®Ÿå‹™ã§ã¯æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§è¨“ç·´ï¼‰
iso_forest.fit(X)

# ç•°å¸¸äºˆæ¸¬
y_pred = iso_forest.predict(X)
anomaly_scores = iso_forest.decision_function(X)  # ç•°å¸¸ã‚¹ã‚³ã‚¢ï¼ˆè² ã®å€¤ã»ã©ç•°å¸¸ï¼‰

# æ€§èƒ½è©•ä¾¡
print("=== Isolation Forest ç•°å¸¸æ¤œçŸ¥æ€§èƒ½ ===")
print("\næ··åŒè¡Œåˆ—:")
cm = confusion_matrix(y_true, y_pred, labels=[1, -1])
print(cm)
print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_true, y_pred, target_names=['æ­£å¸¸', 'ç•°å¸¸']))

# ç•°å¸¸ã‚¹ã‚³ã‚¢ã®çµ±è¨ˆ
print(f"\nç•°å¸¸ã‚¹ã‚³ã‚¢çµ±è¨ˆ:")
print(f"æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡ã‚¹ã‚³ã‚¢: {anomaly_scores[y_true==1].mean():.4f}")
print(f"ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡ã‚¹ã‚³ã‚¢: {anomaly_scores[y_true==-1].mean():.4f}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# ç•°å¸¸ã‚¹ã‚³ã‚¢ã®æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆ
axes[0, 0].plot(anomaly_scores, 'o-', markersize=3, linewidth=0.6, color='#11998e')
axes[0, 0].scatter(np.where(y_true==-1)[0], anomaly_scores[y_true==-1],
                   color='red', s=50, marker='x', linewidths=2, label='çœŸã®ç•°å¸¸', zorder=5)
axes[0, 0].axhline(y=0, color='orange', linestyle='--', linewidth=2, label='åˆ¤å®šå¢ƒç•Œ')
axes[0, 0].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ç•ªå·', fontsize=11)
axes[0, 0].set_ylabel('ç•°å¸¸ã‚¹ã‚³ã‚¢', fontsize=11)
axes[0, 0].set_title('Isolation Forest ç•°å¸¸ã‚¹ã‚³ã‚¢', fontsize=13, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)

# ç•°å¸¸ã‚¹ã‚³ã‚¢ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
axes[0, 1].hist(anomaly_scores[y_true==1], bins=30, alpha=0.6, color='blue', label='æ­£å¸¸', edgecolor='black')
axes[0, 1].hist(anomaly_scores[y_true==-1], bins=15, alpha=0.8, color='red', label='ç•°å¸¸', edgecolor='black')
axes[0, 1].axvline(x=0, color='orange', linestyle='--', linewidth=2, label='åˆ¤å®šå¢ƒç•Œ')
axes[0, 1].set_xlabel('ç•°å¸¸ã‚¹ã‚³ã‚¢', fontsize=11)
axes[0, 1].set_ylabel('é »åº¦', fontsize=11)
axes[0, 1].set_title('ç•°å¸¸ã‚¹ã‚³ã‚¢åˆ†å¸ƒ', fontsize=13, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# å¡”é ‚æ¸©åº¦ vs è£½å“ç´”åº¦ï¼ˆç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³å¯è¦–åŒ–ï¼‰
colors = ['blue' if label == 1 else 'red' for label in y_pred]
axes[1, 0].scatter(df['å¡”é ‚æ¸©åº¦'], df['è£½å“ç´”åº¦'], c=colors, s=30, alpha=0.6)
axes[1, 0].set_xlabel('å¡”é ‚æ¸©åº¦ (Â°C)', fontsize=11)
axes[1, 0].set_ylabel('è£½å“ç´”åº¦', fontsize=11)
axes[1, 0].set_title('å¡”é ‚æ¸©åº¦ vs è£½å“ç´”åº¦ï¼ˆç•°å¸¸æ¤œå‡ºçµæœï¼‰', fontsize=13, fontweight='bold')
axes[1, 0].grid(alpha=0.3)

# é‚„æµæ¯” vs è£½å“ç´”åº¦
axes[1, 1].scatter(df['é‚„æµæ¯”'], df['è£½å“ç´”åº¦'], c=colors, s=30, alpha=0.6)
axes[1, 1].set_xlabel('é‚„æµæ¯”', fontsize=11)
axes[1, 1].set_ylabel('è£½å“ç´”åº¦', fontsize=11)
axes[1, 1].set_title('é‚„æµæ¯” vs è£½å“ç´”åº¦ï¼ˆç•°å¸¸æ¤œå‡ºçµæœï¼‰', fontsize=13, fontweight='bold')
axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>è§£èª¬</strong>: Isolation Forestã¯ã€ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ãŒæ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šã‚‚ã€Œåˆ†é›¢ã—ã‚„ã™ã„ã€ã¨ã„ã†æ€§è³ªã‚’åˆ©ç”¨ã—ãŸæ•™å¸«ãªã—å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚çµ±è¨ˆçš„ä»®å®šãŒä¸è¦ã§ã€éç·šå½¢ãªç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚æ¤œå‡ºã§ãã€åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆã®å¤šæ§˜ãªç•°å¸¸ã«å¯¾å¿œã§ãã¾ã™ã€‚è¨ˆç®—ã‚³ã‚¹ãƒˆãŒä½ãã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã«é©ã—ã¦ã„ã¾ã™ã€‚</p>
</div>

<div class="code-example">
<h4>ã‚³ãƒ¼ãƒ‰ä¾‹3: Autoencoderã«ã‚ˆã‚‹éç·šå½¢ç•°å¸¸æ¤œçŸ¥</h4>

<p><strong>ç›®çš„</strong>: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®Autoencoderã§å†æ§‹æˆèª¤å·®ã«åŸºã¥ãç•°å¸¸æ¤œçŸ¥ã‚’å®Ÿè£…ã™ã‚‹ã€‚</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, roc_curve

np.random.seed(42)
torch.manual_seed(42)

# åŒ–å­¦åå¿œå™¨ã®æ­£å¸¸é‹è»¢ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
n_normal_train = 1000
n_normal_test = 200
n_anomaly_test = 100

# æ­£å¸¸é‹è»¢ãƒ‡ãƒ¼ã‚¿ï¼ˆè¨“ç·´ç”¨ï¼‰
def generate_normal_data(n_samples):
    """æ­£å¸¸é‹è»¢ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆéç·šå½¢ç›¸é–¢ã‚’å«ã‚€ï¼‰"""
    temperature = np.random.normal(400, 15, n_samples)  # K
    pressure = 5.0 + 0.02 * (temperature - 400) + 0.0001 * (temperature - 400)**2 + np.random.normal(0, 0.2, n_samples)  # bar
    flow_rate = 200 + 1.5 * np.log(temperature/300) + np.random.normal(0, 10, n_samples)  # L/h
    conversion = 0.85 * (1 - np.exp(-0.01 * (temperature - 350))) + np.random.normal(0, 0.03, n_samples)
    return np.column_stack([temperature, pressure, flow_rate, conversion])

X_train = generate_normal_data(n_normal_train)
X_test_normal = generate_normal_data(n_normal_test)

# ç•°å¸¸é‹è»¢ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
def generate_anomaly_data(n_samples):
    """ç•°å¸¸é‹è»¢ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
    anomalies = []

    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: æ¸©åº¦æš´èµ°
    temp_high = np.random.normal(450, 20, n_samples//3)
    press_high = 5.0 + 0.02 * (temp_high - 400) + np.random.normal(0, 0.3, n_samples//3)
    flow_high = 200 + 1.5 * np.log(temp_high/300) + np.random.normal(0, 15, n_samples//3)
    conv_high = 0.95 + np.random.normal(0, 0.02, n_samples//3)
    anomalies.append(np.column_stack([temp_high, press_high, flow_high, conv_high]))

    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: åœ§åŠ›ç•°å¸¸
    temp_norm = np.random.normal(400, 15, n_samples//3)
    press_low = np.random.normal(3.0, 0.5, n_samples//3)  # åœ§åŠ›ä½ä¸‹
    flow_norm = 200 + 1.5 * np.log(temp_norm/300) + np.random.normal(0, 10, n_samples//3)
    conv_low = 0.60 + np.random.normal(0, 0.05, n_samples//3)  # è»¢åŒ–ç‡ä½ä¸‹
    anomalies.append(np.column_stack([temp_norm, press_low, flow_norm, conv_low]))

    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: æµé‡ç•°å¸¸
    temp_norm2 = np.random.normal(400, 15, n_samples - 2*(n_samples//3))
    press_norm2 = 5.0 + 0.02 * (temp_norm2 - 400) + np.random.normal(0, 0.2, n_samples - 2*(n_samples//3))
    flow_low = np.random.normal(100, 20, n_samples - 2*(n_samples//3))  # æµé‡ä½ä¸‹
    conv_norm2 = 0.85 * (1 - np.exp(-0.01 * (temp_norm2 - 350))) + np.random.normal(0, 0.03, n_samples - 2*(n_samples//3))
    anomalies.append(np.column_stack([temp_norm2, press_norm2, flow_low, conv_norm2]))

    return np.vstack(anomalies)

X_test_anomaly = generate_anomaly_data(n_anomaly_test)

# ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_normal_scaled = scaler.transform(X_test_normal)
X_test_anomaly_scaled = scaler.transform(X_test_anomaly)

# Autoencoderãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
class Autoencoder(nn.Module):
    def __init__(self, input_dim=4, encoding_dim=2):
        super(Autoencoder, self).__init__()
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 8),
            nn.ReLU(),
            nn.Linear(8, encoding_dim),
            nn.ReLU()
        )
        # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼
        self.decoder = nn.Sequential(
            nn.Linear(encoding_dim, 8),
            nn.ReLU(),
            nn.Linear(8, input_dim)
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
model = Autoencoder(input_dim=4, encoding_dim=2)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’Tensorã«å¤‰æ›
X_train_tensor = torch.FloatTensor(X_train_scaled)

# ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
print("=== Autoencoderè¨“ç·´é–‹å§‹ ===")
n_epochs = 100
batch_size = 32
losses = []

model.train()
for epoch in range(n_epochs):
    epoch_loss = 0
    for i in range(0, len(X_train_tensor), batch_size):
        batch = X_train_tensor[i:i+batch_size]

        # é †ä¼æ’­
        outputs = model(batch)
        loss = criterion(outputs, batch)

        # é€†ä¼æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    avg_loss = epoch_loss / (len(X_train_tensor) / batch_size)
    losses.append(avg_loss)

    if (epoch + 1) % 20 == 0:
        print(f"Epoch [{epoch+1}/{n_epochs}], Loss: {avg_loss:.6f}")

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§å†æ§‹æˆèª¤å·®ã‚’è¨ˆç®—
model.eval()
with torch.no_grad():
    # æ­£å¸¸ãƒ‡ãƒ¼ã‚¿
    X_test_normal_tensor = torch.FloatTensor(X_test_normal_scaled)
    reconstructed_normal = model(X_test_normal_tensor)
    reconstruction_error_normal = torch.mean((X_test_normal_tensor - reconstructed_normal)**2, dim=1).numpy()

    # ç•°å¸¸ãƒ‡ãƒ¼ã‚¿
    X_test_anomaly_tensor = torch.FloatTensor(X_test_anomaly_scaled)
    reconstructed_anomaly = model(X_test_anomaly_tensor)
    reconstruction_error_anomaly = torch.mean((X_test_anomaly_tensor - reconstructed_anomaly)**2, dim=1).numpy()

# é–¾å€¤ã®è¨­å®šï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®99ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼‰
with torch.no_grad():
    reconstructed_train = model(X_train_tensor)
    reconstruction_error_train = torch.mean((X_train_tensor - reconstructed_train)**2, dim=1).numpy()
threshold = np.percentile(reconstruction_error_train, 99)

print(f"\nå†æ§‹æˆèª¤å·®é–¾å€¤ï¼ˆ99%): {threshold:.6f}")
print(f"æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡å†æ§‹æˆèª¤å·®: {reconstruction_error_normal.mean():.6f}")
print(f"ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡å†æ§‹æˆèª¤å·®: {reconstruction_error_anomaly.mean():.6f}")

# ROC-AUCè©•ä¾¡
y_true = np.array([0]*len(reconstruction_error_normal) + [1]*len(reconstruction_error_anomaly))
y_scores = np.concatenate([reconstruction_error_normal, reconstruction_error_anomaly])
auc_score = roc_auc_score(y_true, y_scores)
print(f"\nROC-AUC ã‚¹ã‚³ã‚¢: {auc_score:.4f}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# è¨“ç·´æå¤±
axes[0, 0].plot(losses, color='#11998e', linewidth=2)
axes[0, 0].set_xlabel('Epoch', fontsize=11)
axes[0, 0].set_ylabel('MSE Loss', fontsize=11)
axes[0, 0].set_title('è¨“ç·´æå¤±ã®æ¨ç§»', fontsize=13, fontweight='bold')
axes[0, 0].grid(alpha=0.3)

# å†æ§‹æˆèª¤å·®ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
axes[0, 1].hist(reconstruction_error_normal, bins=30, alpha=0.6, color='blue',
                label='æ­£å¸¸', edgecolor='black')
axes[0, 1].hist(reconstruction_error_anomaly, bins=30, alpha=0.8, color='red',
                label='ç•°å¸¸', edgecolor='black')
axes[0, 1].axvline(x=threshold, color='orange', linestyle='--', linewidth=2, label='é–¾å€¤ (99%)')
axes[0, 1].set_xlabel('å†æ§‹æˆèª¤å·®', fontsize=11)
axes[0, 1].set_ylabel('é »åº¦', fontsize=11)
axes[0, 1].set_title('å†æ§‹æˆèª¤å·®åˆ†å¸ƒ', fontsize=13, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# å†æ§‹æˆèª¤å·®ã®æ™‚ç³»åˆ—ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰
all_errors = np.concatenate([reconstruction_error_normal, reconstruction_error_anomaly])
colors = ['blue']*len(reconstruction_error_normal) + ['red']*len(reconstruction_error_anomaly)
axes[1, 0].scatter(range(len(all_errors)), all_errors, c=colors, s=30, alpha=0.6)
axes[1, 0].axhline(y=threshold, color='orange', linestyle='--', linewidth=2, label='é–¾å€¤')
axes[1, 0].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ç•ªå·', fontsize=11)
axes[1, 0].set_ylabel('å†æ§‹æˆèª¤å·®', fontsize=11)
axes[1, 0].set_title('ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å†æ§‹æˆèª¤å·®', fontsize=13, fontweight='bold')
axes[1, 0].legend()
axes[1, 0].grid(alpha=0.3)

# ROCæ›²ç·š
fpr, tpr, thresholds_roc = roc_curve(y_true, y_scores)
axes[1, 1].plot(fpr, tpr, color='#11998e', linewidth=2, label=f'AUC = {auc_score:.4f}')
axes[1, 1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')
axes[1, 1].set_xlabel('False Positive Rate', fontsize=11)
axes[1, 1].set_ylabel('True Positive Rate', fontsize=11)
axes[1, 1].set_title('ROCæ›²ç·š', fontsize=13, fontweight='bold')
axes[1, 1].legend()
axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>è§£èª¬</strong>: Autoencoderã¯ã€æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´ã‚’ä½æ¬¡å…ƒè¡¨ç¾ï¼ˆæ½œåœ¨å¤‰æ•°ï¼‰ã«åœ§ç¸®ã—ã€å†æ§‹æˆã™ã‚‹æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã¯æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã¨ã¯ç•°ãªã‚‹ç‰¹å¾´ã‚’æŒã¤ãŸã‚ã€å†æ§‹æˆèª¤å·®ãŒå¤§ãããªã‚Šã¾ã™ã€‚éç·šå½¢ãªå¤‰æ•°é–“é–¢ä¿‚ã‚’å­¦ç¿’ã§ãã€PCAã§ã¯æ‰ãˆã‚‰ã‚Œãªã„è¤‡é›‘ãªç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã§ãã¾ã™ã€‚</p>
</div>

<div class="code-example">
<h4>ã‚³ãƒ¼ãƒ‰ä¾‹4: LSTMã«ã‚ˆã‚‹æ™‚ç³»åˆ—ç•°å¸¸æ¤œçŸ¥</h4>

<p><strong>ç›®çš„</strong>: LSTMï¼ˆLong Short-Term Memoryï¼‰ã§æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã€äºˆæ¸¬èª¤å·®ã«åŸºã¥ãç•°å¸¸æ¤œçŸ¥ã‚’å®Ÿè£…ã™ã‚‹ã€‚</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler

np.random.seed(42)
torch.manual_seed(42)

# æ™‚ç³»åˆ—ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆãƒãƒƒãƒåå¿œå™¨ã®æ¸©åº¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
def generate_batch_reactor_data(n_batches, batch_length=100, anomaly=False):
    """ãƒãƒƒãƒåå¿œå™¨ã®æ¸©åº¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ"""
    data = []
    for _ in range(n_batches):
        t = np.linspace(0, 10, batch_length)  # æ™‚é–“ï¼ˆæ™‚é–“ï¼‰

        if not anomaly:
            # æ­£å¸¸ãƒãƒƒãƒ: å…¸å‹çš„ãªç™ºç†±åå¿œãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
            temp = 320 + 50 * (1 - np.exp(-0.5 * t)) * np.exp(-0.1 * t) + np.random.normal(0, 2, batch_length)
        else:
            # ç•°å¸¸ãƒãƒƒãƒ: ç•°å¸¸ãªæ¸©åº¦ä¸Šæ˜‡ãƒ‘ã‚¿ãƒ¼ãƒ³
            if np.random.rand() < 0.5:
                # ãƒ‘ã‚¿ãƒ¼ãƒ³1: éåº¦ãªç™ºç†±
                temp = 320 + 80 * (1 - np.exp(-0.7 * t)) * np.exp(-0.05 * t) + np.random.normal(0, 3, batch_length)
            else:
                # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ä¸ååˆ†ãªåå¿œ
                temp = 320 + 20 * (1 - np.exp(-0.3 * t)) * np.exp(-0.15 * t) + np.random.normal(0, 2, batch_length)

        data.append(temp)
    return np.array(data)

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆæ­£å¸¸ãƒãƒƒãƒã®ã¿ï¼‰
n_train_batches = 200
n_test_normal = 50
n_test_anomaly = 30
batch_length = 100

X_train = generate_batch_reactor_data(n_train_batches, batch_length, anomaly=False)
X_test_normal = generate_batch_reactor_data(n_test_normal, batch_length, anomaly=False)
X_test_anomaly = generate_batch_reactor_data(n_test_anomaly, batch_length, anomaly=True)

print(f"=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ===")
print(f"è¨“ç·´ãƒãƒƒãƒæ•°: {n_train_batches}")
print(f"ãƒ†ã‚¹ãƒˆæ­£å¸¸ãƒãƒƒãƒæ•°: {n_test_normal}")
print(f"ãƒ†ã‚¹ãƒˆç•°å¸¸ãƒãƒƒãƒæ•°: {n_test_anomaly}")
print(f"ãƒãƒƒãƒé•·: {batch_length}")

# ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
scaler = StandardScaler()
X_train_flat = X_train.reshape(-1, 1)
scaler.fit(X_train_flat)
X_train_scaled = scaler.transform(X_train.reshape(-1, 1)).reshape(X_train.shape)
X_test_normal_scaled = scaler.transform(X_test_normal.reshape(-1, 1)).reshape(X_test_normal.shape)
X_test_anomaly_scaled = scaler.transform(X_test_anomaly.reshape(-1, 1)).reshape(X_test_anomaly.shape)

# æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’LSTMç”¨ã«æ•´å½¢ï¼ˆãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, ç‰¹å¾´æ•°ï¼‰
X_train_tensor = torch.FloatTensor(X_train_scaled).unsqueeze(-1)  # (200, 100, 1)
X_test_normal_tensor = torch.FloatTensor(X_test_normal_scaled).unsqueeze(-1)
X_test_anomaly_tensor = torch.FloatTensor(X_test_anomaly_scaled).unsqueeze(-1)

# LSTMãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
class LSTMPredictor(nn.Module):
    def __init__(self, input_dim=1, hidden_dim=32, num_layers=2):
        super(LSTMPredictor, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, input_dim)

    def forward(self, x):
        # LSTMå±¤
        lstm_out, _ = self.lstm(x)
        # å…¨çµåˆå±¤ã§å„æ™‚åˆ»ã®äºˆæ¸¬å€¤ã‚’å‡ºåŠ›
        predictions = self.fc(lstm_out)
        return predictions

# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
model = LSTMPredictor(input_dim=1, hidden_dim=32, num_layers=2)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆ1ã‚¹ãƒ†ãƒƒãƒ—å…ˆäºˆæ¸¬ï¼‰
print("\n=== LSTMè¨“ç·´é–‹å§‹ ===")
n_epochs = 50
batch_size = 16
losses = []

model.train()
for epoch in range(n_epochs):
    epoch_loss = 0
    for i in range(0, len(X_train_tensor), batch_size):
        batch = X_train_tensor[i:i+batch_size]

        # å…¥åŠ›: t=0~98, ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: t=1~99ï¼ˆ1ã‚¹ãƒ†ãƒƒãƒ—å…ˆäºˆæ¸¬ï¼‰
        inputs = batch[:, :-1, :]
        targets = batch[:, 1:, :]

        # é †ä¼æ’­
        outputs = model(inputs)
        loss = criterion(outputs, targets)

        # é€†ä¼æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    avg_loss = epoch_loss / (len(X_train_tensor) / batch_size)
    losses.append(avg_loss)

    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch+1}/{n_epochs}], Loss: {avg_loss:.6f}")

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬èª¤å·®ã‚’è¨ˆç®—
model.eval()
with torch.no_grad():
    # æ­£å¸¸ãƒãƒƒãƒ
    inputs_normal = X_test_normal_tensor[:, :-1, :]
    targets_normal = X_test_normal_tensor[:, 1:, :]
    predictions_normal = model(inputs_normal)
    prediction_error_normal = torch.mean((predictions_normal - targets_normal)**2, dim=(1, 2)).numpy()

    # ç•°å¸¸ãƒãƒƒãƒ
    inputs_anomaly = X_test_anomaly_tensor[:, :-1, :]
    targets_anomaly = X_test_anomaly_tensor[:, 1:, :]
    predictions_anomaly = model(inputs_anomaly)
    prediction_error_anomaly = torch.mean((predictions_anomaly - targets_anomaly)**2, dim=(1, 2)).numpy()

# é–¾å€¤è¨­å®šï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼‰
with torch.no_grad():
    inputs_train = X_train_tensor[:, :-1, :]
    targets_train = X_train_tensor[:, 1:, :]
    predictions_train = model(inputs_train)
    prediction_error_train = torch.mean((predictions_train - targets_train)**2, dim=(1, 2)).numpy()
threshold = np.percentile(prediction_error_train, 95)

print(f"\näºˆæ¸¬èª¤å·®é–¾å€¤ï¼ˆ95%): {threshold:.6f}")
print(f"æ­£å¸¸ãƒãƒƒãƒã®å¹³å‡äºˆæ¸¬èª¤å·®: {prediction_error_normal.mean():.6f}")
print(f"ç•°å¸¸ãƒãƒƒãƒã®å¹³å‡äºˆæ¸¬èª¤å·®: {prediction_error_anomaly.mean():.6f}")

# ç•°å¸¸æ¤œå‡ºæ€§èƒ½
y_true = np.array([0]*len(prediction_error_normal) + [1]*len(prediction_error_anomaly))
y_pred = np.array([0 if e < threshold else 1 for e in np.concatenate([prediction_error_normal, prediction_error_anomaly])])
accuracy = np.mean(y_true == y_pred)
print(f"\næ¤œå‡ºç²¾åº¦: {accuracy:.4f}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# è¨“ç·´æå¤±
axes[0, 0].plot(losses, color='#11998e', linewidth=2)
axes[0, 0].set_xlabel('Epoch', fontsize=11)
axes[0, 0].set_ylabel('MSE Loss', fontsize=11)
axes[0, 0].set_title('LSTMè¨“ç·´æå¤±', fontsize=13, fontweight='bold')
axes[0, 0].grid(alpha=0.3)

# äºˆæ¸¬èª¤å·®ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
axes[0, 1].hist(prediction_error_normal, bins=20, alpha=0.6, color='blue',
                label='æ­£å¸¸', edgecolor='black')
axes[0, 1].hist(prediction_error_anomaly, bins=20, alpha=0.8, color='red',
                label='ç•°å¸¸', edgecolor='black')
axes[0, 1].axvline(x=threshold, color='orange', linestyle='--', linewidth=2, label='é–¾å€¤ (95%)')
axes[0, 1].set_xlabel('äºˆæ¸¬èª¤å·®ï¼ˆMSEï¼‰', fontsize=11)
axes[0, 1].set_ylabel('é »åº¦', fontsize=11)
axes[0, 1].set_title('äºˆæ¸¬èª¤å·®åˆ†å¸ƒ', fontsize=13, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# æ­£å¸¸ãƒãƒƒãƒã®äºˆæ¸¬ä¾‹
sample_idx = 0
axes[1, 0].plot(X_test_normal[sample_idx], 'b-', linewidth=2, label='å®Ÿæ¸¬å€¤')
pred_sample = predictions_normal[sample_idx].squeeze().numpy()
pred_sample_rescaled = scaler.inverse_transform(pred_sample.reshape(-1, 1)).flatten()
actual_rescaled = scaler.inverse_transform(X_test_normal[sample_idx, 1:].reshape(-1, 1)).flatten()
axes[1, 0].plot(range(1, 100), pred_sample_rescaled, 'g--', linewidth=2, label='LSTMäºˆæ¸¬')
axes[1, 0].set_xlabel('æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—', fontsize=11)
axes[1, 0].set_ylabel('æ¸©åº¦ (K)', fontsize=11)
axes[1, 0].set_title('æ­£å¸¸ãƒãƒƒãƒã®äºˆæ¸¬ä¾‹', fontsize=13, fontweight='bold')
axes[1, 0].legend()
axes[1, 0].grid(alpha=0.3)

# ç•°å¸¸ãƒãƒƒãƒã®äºˆæ¸¬ä¾‹
sample_idx_anomaly = 0
axes[1, 1].plot(X_test_anomaly[sample_idx_anomaly], 'b-', linewidth=2, label='å®Ÿæ¸¬å€¤ï¼ˆç•°å¸¸ï¼‰')
pred_sample_anom = predictions_anomaly[sample_idx_anomaly].squeeze().numpy()
pred_sample_anom_rescaled = scaler.inverse_transform(pred_sample_anom.reshape(-1, 1)).flatten()
axes[1, 1].plot(range(1, 100), pred_sample_anom_rescaled, 'g--', linewidth=2, label='LSTMäºˆæ¸¬')
axes[1, 1].set_xlabel('æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—', fontsize=11)
axes[1, 1].set_ylabel('æ¸©åº¦ (K)', fontsize=11)
axes[1, 1].set_title('ç•°å¸¸ãƒãƒƒãƒã®äºˆæ¸¬ä¾‹ï¼ˆäºˆæ¸¬èª¤å·®å¤§ï¼‰', fontsize=13, fontweight='bold')
axes[1, 1].legend()
axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>è§£èª¬</strong>: LSTMã¯æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®é•·æœŸä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã§ãã‚‹å†å¸°å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ãƒãƒƒãƒãƒ—ãƒ­ã‚»ã‚¹ã®æ¸©åº¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãªã©ã€æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒé‡è¦ãªç›£è¦–å¯¾è±¡ã«æœ‰åŠ¹ã§ã™ã€‚æ­£å¸¸ãªæ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã€ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã¯äºˆæ¸¬èª¤å·®ãŒå¢—å¤§ã™ã‚‹ãŸã‚ã€ç•°å¸¸æ¤œçŸ¥ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚</p>
</div>

<hr />

<h2>1.3 å“è³ªäºˆæ¸¬ã¨ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼</h2>

<h3>ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã¨ã¯</h3>

<p><strong>ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ï¼ˆSoft Sensorï¼‰</strong>ã¯ã€æ¸¬å®šå›°é›£ã¾ãŸã¯æ¸¬å®šã‚³ã‚¹ãƒˆãŒé«˜ã„å¤‰æ•°ï¼ˆè£½å“å“è³ªã€åå¿œç‡ã€ä¸ç´”ç‰©æ¿ƒåº¦ãªã©ï¼‰ã‚’ã€æ¸¬å®šå®¹æ˜“ãªãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°ï¼ˆæ¸©åº¦ã€åœ§åŠ›ã€æµé‡ãªã©ï¼‰ã‹ã‚‰æ¨å®šã™ã‚‹æŠ€è¡“ã§ã™ã€‚</p>

<p><strong>åˆ©ç‚¹</strong>:</p>
<ul>
<li>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªç›£è¦–ï¼ˆåˆ†æè¨ˆã¯æ•°åˆ†ï½æ•°æ™‚é–“ã®é…ã‚Œï¼‰</li>
<li>ã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼ˆé«˜ä¾¡ãªåˆ†æè¨ˆã®ä»£æ›¿ï¼‰</li>
<li>ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã®é«˜åº¦åŒ–ï¼ˆå“è³ªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ¶å¾¡ï¼‰</li>
<li>ä¿å…¨æ€§å‘ä¸Šï¼ˆåˆ†æè¨ˆã®æ•…éšœæ™‚ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰</li>
</ul>

<div class="code-example">
<h4>ã‚³ãƒ¼ãƒ‰ä¾‹5: Random Forestã«ã‚ˆã‚‹è£½å“å“è³ªäºˆæ¸¬</h4>

<p><strong>ç›®çš„</strong>: Random Forestã§è’¸ç•™å¡”ã®è£½å“ç´”åº¦ã‚’äºˆæ¸¬ã™ã‚‹å“è³ªäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

np.random.seed(42)

# è’¸ç•™å¡”ã®é‹è»¢ãƒ‡ãƒ¼ã‚¿ã¨è£½å“ç´”åº¦ã®é–¢ä¿‚ã‚’ç”Ÿæˆ
n_samples = 1500

# ãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°ï¼ˆå…¥åŠ›ï¼‰
å¡”é ‚æ¸©åº¦ = np.random.normal(85, 3, n_samples)  # Â°C
å¡”åº•æ¸©åº¦ = np.random.normal(155, 5, n_samples)  # Â°C
é‚„æµæ¯” = np.random.normal(3.5, 0.5, n_samples)
åŸæ–™æµé‡ = np.random.normal(50, 8, n_samples)  # mÂ³/h
å¡”å†…åœ§åŠ› = np.random.normal(1.2, 0.15, n_samples)  # bar

# è£½å“ç´”åº¦ï¼ˆç›®çš„å¤‰æ•°ï¼‰- éç·šå½¢ãªé–¢ä¿‚
è£½å“ç´”åº¦ = (
    0.95
    - 0.002 * (å¡”é ‚æ¸©åº¦ - 85)  # å¡”é ‚æ¸©åº¦ãŒé«˜ã„ã¨ç´”åº¦ä½ä¸‹
    + 0.0005 * (å¡”åº•æ¸©åº¦ - 155)  # å¡”åº•æ¸©åº¦ãŒé«˜ã„ã¨ç´”åº¦å‘ä¸Š
    + 0.02 * (é‚„æµæ¯” - 3.5)  # é‚„æµæ¯”ãŒé«˜ã„ã¨ç´”åº¦å‘ä¸Š
    - 0.0003 * (åŸæ–™æµé‡ - 50)  # æµé‡ãŒå¤šã„ã¨ç´”åº¦ä½ä¸‹
    + 0.01 * (å¡”å†…åœ§åŠ› - 1.2)  # åœ§åŠ›ãŒé«˜ã„ã¨ç´”åº¦å‘ä¸Š
    - 0.0001 * (å¡”é ‚æ¸©åº¦ - 85)**2  # éç·šå½¢åŠ¹æœ
    + 0.001 * (é‚„æµæ¯” - 3.5) * (å¡”åº•æ¸©åº¦ - 155) / 10  # äº¤äº’ä½œç”¨
    + np.random.normal(0, 0.005, n_samples)  # æ¸¬å®šãƒã‚¤ã‚º
)

# DataFrameã«æ ¼ç´
df = pd.DataFrame({
    'å¡”é ‚æ¸©åº¦': å¡”é ‚æ¸©åº¦,
    'å¡”åº•æ¸©åº¦': å¡”åº•æ¸©åº¦,
    'é‚„æµæ¯”': é‚„æµæ¯”,
    'åŸæ–™æµé‡': åŸæ–™æµé‡,
    'å¡”å†…åœ§åŠ›': å¡”å†…åœ§åŠ›,
    'è£½å“ç´”åº¦': è£½å“ç´”åº¦
})

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X = df.drop('è£½å“ç´”åº¦', axis=1)
y = df['è£½å“ç´”åº¦']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(f"=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ===")
print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°: {len(X_train)}")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°: {len(X_test)}")
print(f"ç‰¹å¾´å¤‰æ•°æ•°: {X.shape[1]}")

# Random Forestãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)

rf_model.fit(X_train, y_train)

# äºˆæ¸¬
y_pred_train = rf_model.predict(X_train)
y_pred_test = rf_model.predict(X_test)

# æ€§èƒ½è©•ä¾¡
mae_train = mean_absolute_error(y_train, y_pred_train)
rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))
r2_train = r2_score(y_train, y_pred_train)

mae_test = mean_absolute_error(y_test, y_pred_test)
rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))
r2_test = r2_score(y_test, y_pred_test)

print(f"\n=== ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ ===")
print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿:")
print(f"  MAE: {mae_train:.5f}")
print(f"  RMSE: {rmse_train:.5f}")
print(f"  RÂ²: {r2_train:.4f}")
print(f"\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿:")
print(f"  MAE: {mae_test:.5f}")
print(f"  RMSE: {rmse_test:.5f}")
print(f"  RÂ²: {r2_test:.4f}")

# ç‰¹å¾´é‡è¦åº¦
feature_importance = pd.DataFrame({
    'ç‰¹å¾´é‡': X.columns,
    'é‡è¦åº¦': rf_model.feature_importances_
}).sort_values('é‡è¦åº¦', ascending=False)

print(f"\n=== ç‰¹å¾´é‡è¦åº¦ ===")
print(feature_importance)

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# äºˆæ¸¬ vs å®Ÿæ¸¬ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰
axes[0, 0].scatter(y_test, y_pred_test, alpha=0.5, s=30, color='#11998e')
axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                'r--', linewidth=2, label='ç†æƒ³ç›´ç·š')
axes[0, 0].set_xlabel('å®Ÿæ¸¬ç´”åº¦', fontsize=11)
axes[0, 0].set_ylabel('äºˆæ¸¬ç´”åº¦', fontsize=11)
axes[0, 0].set_title(f'äºˆæ¸¬ vs å®Ÿæ¸¬ï¼ˆRÂ²={r2_test:.4f}ï¼‰', fontsize=13, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)

# æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
residuals = y_test - y_pred_test
axes[0, 1].scatter(y_pred_test, residuals, alpha=0.5, s=30, color='#38ef7d')
axes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)
axes[0, 1].fill_between([y_pred_test.min(), y_pred_test.max()], -2*rmse_test, 2*rmse_test,
                         alpha=0.2, color='orange', label='Â±2Ïƒç¯„å›²')
axes[0, 1].set_xlabel('äºˆæ¸¬ç´”åº¦', fontsize=11)
axes[0, 1].set_ylabel('æ®‹å·®', fontsize=11)
axes[0, 1].set_title('æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ', fontsize=13, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# ç‰¹å¾´é‡è¦åº¦
axes[1, 0].barh(feature_importance['ç‰¹å¾´é‡'], feature_importance['é‡è¦åº¦'], color='#11998e')
axes[1, 0].set_xlabel('é‡è¦åº¦', fontsize=11)
axes[1, 0].set_title('ç‰¹å¾´é‡è¦åº¦ï¼ˆRandom Forestï¼‰', fontsize=13, fontweight='bold')
axes[1, 0].grid(alpha=0.3)

# æ™‚ç³»åˆ—äºˆæ¸¬ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæœ€åˆã®100ã‚µãƒ³ãƒ—ãƒ«ï¼‰
test_indices = range(100)
axes[1, 1].plot(test_indices, y_test.iloc[:100].values, 'b-', linewidth=2,
                label='å®Ÿæ¸¬å€¤', marker='o', markersize=3)
axes[1, 1].plot(test_indices, y_pred_test[:100], 'r--', linewidth=2,
                label='äºˆæ¸¬å€¤', marker='s', markersize=3)
axes[1, 1].fill_between(test_indices, y_pred_test[:100] - 2*rmse_test,
                         y_pred_test[:100] + 2*rmse_test, alpha=0.2, color='red', label='Â±2Ïƒç¯„å›²')
axes[1, 1].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ç•ªå·', fontsize=11)
axes[1, 1].set_ylabel('è£½å“ç´”åº¦', fontsize=11)
axes[1, 1].set_title('å“è³ªäºˆæ¸¬ã®æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆ', fontsize=13, fontweight='bold')
axes[1, 1].legend()
axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>è§£èª¬</strong>: Random Forestã¯ã€éç·šå½¢é–¢ä¿‚ã‚„å¤‰æ•°é–“ã®äº¤äº’ä½œç”¨ã‚’è‡ªå‹•çš„ã«å­¦ç¿’ã§ãã€å¤–ã‚Œå€¤ã«é ‘å¥ãªç‰¹æ€§ã‚’æŒã¡ã¾ã™ã€‚ç‰¹å¾´é‡è¦åº¦ã«ã‚ˆã‚Šã€å“è³ªã«å½±éŸ¿ã™ã‚‹ä¸»è¦ãªãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°ã‚’ç‰¹å®šã§ãã¾ã™ã€‚åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆã§ã¯ã€åˆ†æè¨ˆã®æ¸¬å®šé…ã‚Œï¼ˆæ•°åˆ†ï½æ•°æ™‚é–“ï¼‰ã‚’è£œå®Œã—ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªç›£è¦–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚</p>
</div>

<div class="code-example">
<h4>ã‚³ãƒ¼ãƒ‰ä¾‹6: ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ï¼ˆGPRï¼‰ã«ã‚ˆã‚‹ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼è¨­è¨ˆ</h4>

<p><strong>ç›®çš„</strong>: Gaussian Process Regressionã§ä¸ç¢ºå®Ÿæ€§ã‚’å«ã‚€å“è³ªäºˆæ¸¬ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel as C
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, r2_score

np.random.seed(42)

# åŒ–å­¦åå¿œå™¨ã®è»¢åŒ–ç‡äºˆæ¸¬ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼
n_samples = 500

# ãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°
æ¸©åº¦ = np.random.normal(380, 20, n_samples)  # K
åœ§åŠ› = np.random.normal(5.0, 0.5, n_samples)  # bar
è§¦åª’æ¿ƒåº¦ = np.random.normal(0.1, 0.02, n_samples)  # mol/L

# è»¢åŒ–ç‡ï¼ˆã‚¢ãƒ¬ãƒ‹ã‚¦ã‚¹å‹ã®éç·šå½¢é–¢ä¿‚ï¼‰
æ´»æ€§åŒ–ã‚¨ãƒãƒ«ã‚®ãƒ¼ = 80000  # J/mol
R = 8.314  # J/(molÂ·K)
åå¿œé€Ÿåº¦å®šæ•° = np.exp(-æ´»æ€§åŒ–ã‚¨ãƒãƒ«ã‚®ãƒ¼ / (R * æ¸©åº¦))
è»¢åŒ–ç‡ = (
    1 - np.exp(-åå¿œé€Ÿåº¦å®šæ•° * åœ§åŠ› * è§¦åª’æ¿ƒåº¦ * 100)
    + np.random.normal(0, 0.02, n_samples)
)
è»¢åŒ–ç‡ = np.clip(è»¢åŒ–ç‡, 0, 1)  # 0-1ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—

# DataFrameã«æ ¼ç´
df = pd.DataFrame({
    'æ¸©åº¦': æ¸©åº¦,
    'åœ§åŠ›': åœ§åŠ›,
    'è§¦åª’æ¿ƒåº¦': è§¦åª’æ¿ƒåº¦,
    'è»¢åŒ–ç‡': è»¢åŒ–ç‡
})

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X = df[['æ¸©åº¦', 'åœ§åŠ›', 'è§¦åª’æ¿ƒåº¦']].values
y = df['è»¢åŒ–ç‡'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
scaler_X = StandardScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)

print(f"=== ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼æ§‹ç¯‰ ===")
print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°: {len(X_train)}")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°: {len(X_test)}")

# ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã‚«ãƒ¼ãƒãƒ«ã®å®šç¾©
# RBFã‚«ãƒ¼ãƒãƒ« + ãƒ›ãƒ¯ã‚¤ãƒˆãƒã‚¤ã‚ºï¼ˆæ¸¬å®šãƒã‚¤ã‚ºã‚’è€ƒæ…®ï¼‰
kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=[1.0, 1.0, 1.0],
                                    length_scale_bounds=(1e-2, 1e2)) + WhiteKernel(noise_level=0.01)

# GPRãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
gpr = GaussianProcessRegressor(
    kernel=kernel,
    n_restarts_optimizer=10,
    alpha=1e-10,
    random_state=42
)

gpr.fit(X_train_scaled, y_train)

print(f"\næœ€é©åŒ–ã•ã‚ŒãŸã‚«ãƒ¼ãƒãƒ«:")
print(gpr.kernel_)

# äºˆæ¸¬ï¼ˆå¹³å‡ã¨æ¨™æº–åå·®ï¼‰
y_pred_test, y_std_test = gpr.predict(X_test_scaled, return_std=True)
y_pred_train, y_std_train = gpr.predict(X_train_scaled, return_std=True)

# æ€§èƒ½è©•ä¾¡
mae_test = mean_absolute_error(y_test, y_pred_test)
r2_test = r2_score(y_test, y_pred_test)

print(f"\n=== ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼æ€§èƒ½ ===")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ MAE: {mae_test:.5f}")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_test:.4f}")
print(f"å¹³å‡äºˆæ¸¬ä¸ç¢ºå®Ÿæ€§ï¼ˆÏƒï¼‰: {y_std_test.mean():.5f}")

# äºˆæ¸¬åŒºé–“å†…ã®ã‚«ãƒãƒ¼ç‡ï¼ˆ95%ä¿¡é ¼åŒºé–“ï¼‰
lower_bound = y_pred_test - 1.96 * y_std_test
upper_bound = y_pred_test + 1.96 * y_std_test
coverage = np.mean((y_test >= lower_bound) & (y_test <= upper_bound))
print(f"95%äºˆæ¸¬åŒºé–“ã‚«ãƒãƒ¼ç‡: {coverage:.2%}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# äºˆæ¸¬ vs å®Ÿæ¸¬ï¼ˆä¸ç¢ºå®Ÿæ€§ä»˜ãï¼‰
axes[0, 0].scatter(y_test, y_pred_test, alpha=0.5, s=30, c=y_std_test, cmap='viridis')
cbar = plt.colorbar(axes[0, 0].collections[0], ax=axes[0, 0])
cbar.set_label('äºˆæ¸¬æ¨™æº–åå·®', fontsize=10)
axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                'r--', linewidth=2, label='ç†æƒ³ç›´ç·š')
axes[0, 0].set_xlabel('å®Ÿæ¸¬è»¢åŒ–ç‡', fontsize=11)
axes[0, 0].set_ylabel('äºˆæ¸¬è»¢åŒ–ç‡', fontsize=11)
axes[0, 0].set_title(f'GPRã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ï¼ˆRÂ²={r2_test:.4f}ï¼‰', fontsize=13, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)

# äºˆæ¸¬ä¸ç¢ºå®Ÿæ€§ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
axes[0, 1].hist(y_std_test, bins=30, color='#11998e', alpha=0.7, edgecolor='black')
axes[0, 1].set_xlabel('äºˆæ¸¬æ¨™æº–åå·®ï¼ˆÏƒï¼‰', fontsize=11)
axes[0, 1].set_ylabel('é »åº¦', fontsize=11)
axes[0, 1].set_title('äºˆæ¸¬ä¸ç¢ºå®Ÿæ€§ã®åˆ†å¸ƒ', fontsize=13, fontweight='bold')
axes[0, 1].axvline(x=y_std_test.mean(), color='red', linestyle='--',
                   linewidth=2, label=f'å¹³å‡Ïƒ: {y_std_test.mean():.5f}')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# æ™‚ç³»åˆ—äºˆæ¸¬ãƒ—ãƒ­ãƒƒãƒˆï¼ˆ95%ä¿¡é ¼åŒºé–“ä»˜ãï¼‰
sorted_indices = np.argsort(y_test)[:60]
axes[1, 0].plot(range(60), y_test[sorted_indices], 'bo-', linewidth=2,
                markersize=4, label='å®Ÿæ¸¬å€¤')
axes[1, 0].plot(range(60), y_pred_test[sorted_indices], 'r--', linewidth=2,
                marker='s', markersize=4, label='GPRäºˆæ¸¬')
axes[1, 0].fill_between(range(60),
                         y_pred_test[sorted_indices] - 1.96*y_std_test[sorted_indices],
                         y_pred_test[sorted_indices] + 1.96*y_std_test[sorted_indices],
                         alpha=0.3, color='red', label='95%ä¿¡é ¼åŒºé–“')
axes[1, 0].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ç•ªå·', fontsize=11)
axes[1, 0].set_ylabel('è»¢åŒ–ç‡', fontsize=11)
axes[1, 0].set_title('ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼äºˆæ¸¬ï¼ˆä¸ç¢ºå®Ÿæ€§ä»˜ãï¼‰', fontsize=13, fontweight='bold')
axes[1, 0].legend()
axes[1, 0].grid(alpha=0.3)

# èª¤å·® vs ä¸ç¢ºå®Ÿæ€§ã®é–¢ä¿‚
abs_error = np.abs(y_test - y_pred_test)
axes[1, 1].scatter(y_std_test, abs_error, alpha=0.5, s=30, color='#38ef7d')
axes[1, 1].plot([y_std_test.min(), y_std_test.max()],
                [y_std_test.min(), y_std_test.max()],
                'r--', linewidth=2, label='å®Œå…¨ä¸€è‡´')
axes[1, 1].set_xlabel('äºˆæ¸¬æ¨™æº–åå·®ï¼ˆÏƒï¼‰', fontsize=11)
axes[1, 1].set_ylabel('çµ¶å¯¾èª¤å·®', fontsize=11)
axes[1, 1].set_title('äºˆæ¸¬èª¤å·® vs ä¸ç¢ºå®Ÿæ€§', fontsize=13, fontweight='bold')
axes[1, 1].legend()
axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>è§£èª¬</strong>: ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ï¼ˆGPRï¼‰ã¯ã€äºˆæ¸¬å€¤ã ã‘ã§ãªãäºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ï¼ˆæ¨™æº–åå·®ï¼‰ã‚‚å‡ºåŠ›ã—ã¾ã™ã€‚ã“ã®ä¸ç¢ºå®Ÿæ€§æƒ…å ±ã¯ã€ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã‚„æ„æ€æ±ºå®šã«æ¥µã‚ã¦é‡è¦ã§ã™ã€‚ä¸ç¢ºå®Ÿæ€§ãŒå¤§ãã„äºˆæ¸¬å€¤ã¯ä¿¡é ¼æ€§ãŒä½ã„ã¨åˆ¤æ–­ã—ã€è¿½åŠ æ¸¬å®šã‚„ä¿å®ˆçš„ãªåˆ¶å¾¡ã‚’å®Ÿæ–½ã§ãã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„é ˜åŸŸã§ã¯è‡ªå‹•çš„ã«ä¸ç¢ºå®Ÿæ€§ãŒå¤§ãããªã‚Šã¾ã™ã€‚</p>
</div>

<div class="code-example">
<h4>ã‚³ãƒ¼ãƒ‰ä¾‹7: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ï¼ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³é©å¿œï¼‰</h4>

<p><strong>ç›®çš„</strong>: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã‚’æ§‹ç¯‰ã—ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³é©å¿œï¼ˆAdaptive Learningï¼‰ã‚’å®Ÿè£…ã™ã‚‹ã€‚</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, r2_score

np.random.seed(42)
torch.manual_seed(42)

# ãƒ—ãƒ­ã‚»ã‚¹ç‰¹æ€§ã®ãƒ‰ãƒªãƒ•ãƒˆï¼ˆçµŒæ™‚å¤‰åŒ–ï¼‰ã‚’è€ƒæ…®ã—ãŸãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
def generate_process_data_with_drift(n_samples, drift_factor=0):
    """
    è§¦åª’æ´»æ€§ã®çµŒæ™‚åŠ£åŒ–ã‚’è€ƒæ…®ã—ãŸãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    drift_factor: 0ï¼ˆåˆæœŸï¼‰ï½1ï¼ˆå¤§å¹…åŠ£åŒ–ï¼‰
    """
    æ¸©åº¦ = np.random.normal(370, 15, n_samples)
    åœ§åŠ› = np.random.normal(4.5, 0.4, n_samples)
    æµé‡ = np.random.normal(180, 20, n_samples)

    # è§¦åª’æ´»æ€§ã®åŠ£åŒ–ï¼ˆãƒ‰ãƒªãƒ•ãƒˆï¼‰
    æ´»æ€§ä¿‚æ•° = 1.0 - 0.3 * drift_factor  # æœ€å¤§30%ã®æ´»æ€§ä½ä¸‹

    # è£½å“åç‡ï¼ˆæ´»æ€§åŠ£åŒ–ã«ã‚ˆã‚Šä½ä¸‹ï¼‰
    åç‡ = (
        æ´»æ€§ä¿‚æ•° * (0.75 + 0.001 * (æ¸©åº¦ - 370) + 0.02 * (åœ§åŠ› - 4.5) - 0.0002 * (æµé‡ - 180))
        + np.random.normal(0, 0.02, n_samples)
    )
    åç‡ = np.clip(åç‡, 0, 1)

    return np.column_stack([æ¸©åº¦, åœ§åŠ›, æµé‡]), åç‡

# åˆæœŸè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆæ–°å“è§¦åª’ï¼‰
X_train, y_train = generate_process_data_with_drift(1000, drift_factor=0)

# æ¨™æº–åŒ–
scaler_X = StandardScaler()
scaler_y = StandardScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()

# ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã®å®šç¾©
class SoftSensor(nn.Module):
    def __init__(self, input_dim=3, hidden_dim=16):
        super(SoftSensor, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, x):
        return self.network(x)

# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
model = SoftSensor(input_dim=3, hidden_dim=16)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# åˆæœŸè¨“ç·´
print("=== ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼åˆæœŸè¨“ç·´ ===")
X_train_tensor = torch.FloatTensor(X_train_scaled)
y_train_tensor = torch.FloatTensor(y_train_scaled).unsqueeze(1)

n_epochs = 100
for epoch in range(n_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 20 == 0:
        print(f"Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.6f}")

# ã‚ªãƒ³ãƒ©ã‚¤ãƒ³é‹è»¢ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆè§¦åª’åŠ£åŒ–ã‚’å«ã‚€ï¼‰
n_online_samples = 500
drift_schedule = np.linspace(0, 0.8, n_online_samples)  # å¾ã€…ã«åŠ£åŒ–

# æ€§èƒ½è¿½è·¡ç”¨ãƒªã‚¹ãƒˆ
mae_history = []
predictions_history = []
actuals_history = []
adaptation_points = []  # é©å¿œæ›´æ–°ã‚’è¡Œã£ãŸãƒã‚¤ãƒ³ãƒˆ

print("\n=== ã‚ªãƒ³ãƒ©ã‚¤ãƒ³é‹è»¢ã¨ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼é©å¿œ ===")
adaptation_threshold = 0.05  # MAEãŒã“ã®å€¤ã‚’è¶…ãˆãŸã‚‰é©å¿œå­¦ç¿’
adaptation_window = 50  # é©å¿œå­¦ç¿’ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦

for t in range(n_online_samples):
    # ç¾åœ¨ã®è§¦åª’åŠ£åŒ–çŠ¶æ…‹ã§ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆ1ã‚µãƒ³ãƒ—ãƒ«ï¼‰
    X_new, y_new = generate_process_data_with_drift(1, drift_factor=drift_schedule[t])
    X_new_scaled = scaler_X.transform(X_new)

    # äºˆæ¸¬
    model.eval()
    with torch.no_grad():
        X_tensor = torch.FloatTensor(X_new_scaled)
        y_pred_scaled = model(X_tensor).numpy()[0, 0]
        y_pred = scaler_y.inverse_transform([[y_pred_scaled]])[0, 0]

    predictions_history.append(y_pred)
    actuals_history.append(y_new[0])

    # ç§»å‹•çª“ã§ã®æ€§èƒ½è©•ä¾¡
    if t >= 20:
        recent_mae = mean_absolute_error(
            actuals_history[t-20:t+1],
            predictions_history[t-20:t+1]
        )
        mae_history.append(recent_mae)

        # æ€§èƒ½åŠ£åŒ–æ¤œå‡ºã¨ã‚ªãƒ³ãƒ©ã‚¤ãƒ³é©å¿œ
        if recent_mae > adaptation_threshold and t >= adaptation_window:
            # æœ€è¿‘ã®ãƒ‡ãƒ¼ã‚¿ã§é©å¿œå­¦ç¿’
            X_adapt = np.array([scaler_X.transform([[actuals_history[i],
                                                      4.5 + np.random.normal(0, 0.1),
                                                      180 + np.random.normal(0, 5)]])
                                for i in range(t-adaptation_window, t)]).squeeze()
            y_adapt = np.array([actuals_history[i] for i in range(t-adaptation_window, t)])

            # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³é©å¿œï¼ˆå°‘æ•°ã‚¨ãƒãƒƒã‚¯ï¼‰
            X_adapt_tensor = torch.FloatTensor(X_adapt)
            y_adapt_scaled = scaler_y.transform(y_adapt.reshape(-1, 1)).flatten()
            y_adapt_tensor = torch.FloatTensor(y_adapt_scaled).unsqueeze(1)

            model.train()
            for _ in range(10):  # å°‘æ•°ã‚¨ãƒãƒƒã‚¯ã§å¾®èª¿æ•´
                optimizer.zero_grad()
                outputs = model(X_adapt_tensor)
                loss = criterion(outputs, y_adapt_tensor)
                loss.backward()
                optimizer.step()

            adaptation_points.append(t)
            print(f"æ™‚åˆ» {t}: ã‚ªãƒ³ãƒ©ã‚¤ãƒ³é©å¿œå®Ÿè¡Œï¼ˆMAE={recent_mae:.4f}ï¼‰")

    if (t + 1) % 100 == 0:
        print(f"æ™‚åˆ» {t+1}: å‡¦ç†å®Œäº†")

print(f"\né©å¿œå­¦ç¿’å®Ÿè¡Œå›æ•°: {len(adaptation_points)}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# äºˆæ¸¬ vs å®Ÿæ¸¬ã®æ™‚ç³»åˆ—
axes[0, 0].plot(actuals_history, 'b-', linewidth=1.5, alpha=0.7, label='å®Ÿæ¸¬å€¤')
axes[0, 0].plot(predictions_history, 'r--', linewidth=1.5, alpha=0.7, label='äºˆæ¸¬å€¤')
for ap in adaptation_points:
    axes[0, 0].axvline(x=ap, color='green', linestyle=':', alpha=0.5)
axes[0, 0].set_xlabel('æ™‚åˆ»', fontsize=11)
axes[0, 0].set_ylabel('è£½å“åç‡', fontsize=11)
axes[0, 0].set_title('ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼äºˆæ¸¬ï¼ˆç·‘ç·šï¼šé©å¿œå­¦ç¿’å®Ÿè¡Œï¼‰', fontsize=13, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)

# MAEã®æ™‚ç³»åˆ—ï¼ˆæ€§èƒ½åŠ£åŒ–ã®è¿½è·¡ï¼‰
axes[0, 1].plot(range(20, len(actuals_history)), mae_history, color='#11998e', linewidth=2)
axes[0, 1].axhline(y=adaptation_threshold, color='red', linestyle='--',
                   linewidth=2, label='é©å¿œé–¾å€¤')
for ap in adaptation_points:
    axes[0, 1].axvline(x=ap, color='green', linestyle=':', alpha=0.5)
axes[0, 1].set_xlabel('æ™‚åˆ»', fontsize=11)
axes[0, 1].set_ylabel('ç§»å‹•å¹³å‡MAEï¼ˆ20ã‚µãƒ³ãƒ—ãƒ«ï¼‰', fontsize=11)
axes[0, 1].set_title('ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼æ€§èƒ½ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°', fontsize=13, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# è§¦åª’åŠ£åŒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
axes[1, 0].plot(drift_schedule * 100, color='#38ef7d', linewidth=2)
axes[1, 0].set_xlabel('æ™‚åˆ»', fontsize=11)
axes[1, 0].set_ylabel('è§¦åª’æ´»æ€§ä½ä¸‹ç‡ (%)', fontsize=11)
axes[1, 0].set_title('è§¦åª’çµŒæ™‚åŠ£åŒ–ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰', fontsize=13, fontweight='bold')
axes[1, 0].grid(alpha=0.3)

# äºˆæ¸¬èª¤å·®ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼ˆé©å¿œå‰å¾Œï¼‰
errors_before = np.array(actuals_history[:adaptation_points[0]]) - np.array(predictions_history[:adaptation_points[0]]) if len(adaptation_points) > 0 else []
errors_after = np.array(actuals_history[adaptation_points[-1]:]) - np.array(predictions_history[adaptation_points[-1]:]) if len(adaptation_points) > 0 else []

if len(errors_before) > 0 and len(errors_after) > 0:
    axes[1, 1].hist(errors_before, bins=20, alpha=0.6, color='blue', label='é©å¿œå‰', edgecolor='black')
    axes[1, 1].hist(errors_after, bins=20, alpha=0.6, color='green', label='é©å¿œå¾Œ', edgecolor='black')
    axes[1, 1].axvline(x=0, color='red', linestyle='--', linewidth=2)
    axes[1, 1].set_xlabel('äºˆæ¸¬èª¤å·®', fontsize=11)
    axes[1, 1].set_ylabel('é »åº¦', fontsize=11)
    axes[1, 1].set_title('äºˆæ¸¬èª¤å·®åˆ†å¸ƒï¼ˆé©å¿œåŠ¹æœï¼‰', fontsize=13, fontweight='bold')
    axes[1, 1].legend()
    axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>è§£èª¬</strong>: åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆã§ã¯è§¦åª’åŠ£åŒ–ã€åŸæ–™çµ„æˆå¤‰å‹•ã€å­£ç¯€å¤‰å‹•ãªã©ã«ã‚ˆã‚Šãƒ—ãƒ­ã‚»ã‚¹ç‰¹æ€§ãŒãƒ‰ãƒªãƒ•ãƒˆã—ã¾ã™ã€‚å›ºå®šãƒ¢ãƒ‡ãƒ«ã®ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã¯æ™‚é–“ã¨ã¨ã‚‚ã«æ€§èƒ½ãŒåŠ£åŒ–ã—ã¾ã™ã€‚ã‚ªãƒ³ãƒ©ã‚¤ãƒ³é©å¿œå­¦ç¿’ï¼ˆAdaptive Learningï¼‰ã«ã‚ˆã‚Šã€æœ€æ–°ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã§å®šæœŸçš„ã«ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°ã—ã€é•·æœŸé–“ã®é«˜ç²¾åº¦é‹è»¢ã‚’ç¶­æŒã§ãã¾ã™ã€‚</p>
</div>

<div class="code-example">
<h4>ã‚³ãƒ¼ãƒ‰ä¾‹8: çµ±åˆãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ï¼ˆç•°å¸¸æ¤œçŸ¥+ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ï¼‰</h4>

<p><strong>ç›®çš„</strong>: ç•°å¸¸æ¤œçŸ¥ã¨ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã‚’çµ±åˆã—ãŸç·åˆãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã™ã‚‹ã€‚</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest, RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, r2_score

np.random.seed(42)

# çµ±åˆãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒ©ã‚¹
class IntegratedProcessMonitoring:
    """
    ç•°å¸¸æ¤œçŸ¥ã¨ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã‚’çµ±åˆã—ãŸãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
    """
    def __init__(self):
        self.anomaly_detector = IsolationForest(contamination=0.05, random_state=42)
        self.soft_sensor = RandomForestRegressor(n_estimators=100, random_state=42)
        self.scaler = StandardScaler()
        self.is_trained = False

    def train(self, X_process, y_quality):
        """
        ã‚·ã‚¹ãƒ†ãƒ ã®è¨“ç·´
        X_process: ãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°ï¼ˆæ¸©åº¦ã€åœ§åŠ›ã€æµé‡ãªã©ï¼‰
        y_quality: å“è³ªå¤‰æ•°ï¼ˆã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰
        """
        # ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
        X_scaled = self.scaler.fit_transform(X_process)

        # ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
        self.anomaly_detector.fit(X_scaled)

        # ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã®è¨“ç·´
        self.soft_sensor.fit(X_scaled, y_quality)

        self.is_trained = True
        print("çµ±åˆç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®è¨“ç·´å®Œäº†")

    def monitor(self, X_new):
        """
        æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã®ç›£è¦–
        Returns: (ç•°å¸¸åˆ¤å®š, å“è³ªäºˆæ¸¬, ç•°å¸¸ã‚¹ã‚³ã‚¢)
        """
        if not self.is_trained:
            raise ValueError("ã‚·ã‚¹ãƒ†ãƒ ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        X_scaled = self.scaler.transform(X_new.reshape(1, -1))

        # ç•°å¸¸æ¤œçŸ¥
        anomaly_pred = self.anomaly_detector.predict(X_scaled)[0]
        anomaly_score = self.anomaly_detector.decision_function(X_scaled)[0]

        # å“è³ªäºˆæ¸¬
        quality_pred = self.soft_sensor.predict(X_scaled)[0]

        is_anomaly = (anomaly_pred == -1)

        return is_anomaly, quality_pred, anomaly_score

    def get_alert_message(self, is_anomaly, quality_pred, quality_threshold=0.95):
        """ã‚¢ãƒ©ãƒ¼ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ç”Ÿæˆ"""
        alerts = []

        if is_anomaly:
            alerts.append("âš ï¸ ãƒ—ãƒ­ã‚»ã‚¹ç•°å¸¸æ¤œå‡º")

        if quality_pred < quality_threshold:
            alerts.append(f"âš ï¸ å“è³ªä½ä¸‹äºˆæ¸¬ï¼ˆäºˆæ¸¬ç´”åº¦: {quality_pred:.3f} < é–¾å€¤: {quality_threshold}ï¼‰")

        if len(alerts) == 0:
            return "âœ“ æ­£å¸¸é‹è»¢"
        else:
            return " | ".join(alerts)

# åŒ–å­¦ãƒ—ãƒ©ãƒ³ãƒˆã®é€£ç¶šé‹è»¢ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
n_normal_train = 800
n_online = 200

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆæ­£å¸¸é‹è»¢ï¼‰
å¡”é ‚æ¸©åº¦_è¨“ç·´ = np.random.normal(85, 2, n_normal_train)
å¡”åº•æ¸©åº¦_è¨“ç·´ = np.random.normal(155, 3, n_normal_train)
é‚„æµæ¯”_è¨“ç·´ = np.random.normal(3.5, 0.3, n_normal_train)
è£½å“ç´”åº¦_è¨“ç·´ = 0.98 - 0.002 * (å¡”é ‚æ¸©åº¦_è¨“ç·´ - 85) + 0.015 * (é‚„æµæ¯”_è¨“ç·´ - 3.5) + np.random.normal(0, 0.005, n_normal_train)

X_train = np.column_stack([å¡”é ‚æ¸©åº¦_è¨“ç·´, å¡”åº•æ¸©åº¦_è¨“ç·´, é‚„æµæ¯”_è¨“ç·´])
y_train = è£½å“ç´”åº¦_è¨“ç·´

# çµ±åˆç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®è¨“ç·´
monitor_system = IntegratedProcessMonitoring()
monitor_system.train(X_train, y_train)

# ã‚ªãƒ³ãƒ©ã‚¤ãƒ³é‹è»¢ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("\n=== ã‚ªãƒ³ãƒ©ã‚¤ãƒ³é‹è»¢ç›£è¦–é–‹å§‹ ===\n")

# æ­£å¸¸é‹è»¢ãƒ‡ãƒ¼ã‚¿ï¼ˆ150ã‚µãƒ³ãƒ—ãƒ«ï¼‰
å¡”é ‚æ¸©åº¦_æ­£å¸¸ = np.random.normal(85, 2, 150)
å¡”åº•æ¸©åº¦_æ­£å¸¸ = np.random.normal(155, 3, 150)
é‚„æµæ¯”_æ­£å¸¸ = np.random.normal(3.5, 0.3, 150)
è£½å“ç´”åº¦_æ­£å¸¸ = 0.98 - 0.002 * (å¡”é ‚æ¸©åº¦_æ­£å¸¸ - 85) + 0.015 * (é‚„æµæ¯”_æ­£å¸¸ - 3.5) + np.random.normal(0, 0.005, 150)

# ç•°å¸¸é‹è»¢ãƒ‡ãƒ¼ã‚¿ï¼ˆ50ã‚µãƒ³ãƒ—ãƒ«ï¼‰
# ã‚±ãƒ¼ã‚¹1: å†·å´å™¨æ•…éšœï¼ˆå¡”é ‚æ¸©åº¦ä¸Šæ˜‡ï¼‰
å¡”é ‚æ¸©åº¦_ç•°å¸¸1 = np.random.normal(95, 3, 20)
å¡”åº•æ¸©åº¦_ç•°å¸¸1 = np.random.normal(155, 3, 20)
é‚„æµæ¯”_ç•°å¸¸1 = np.random.normal(3.5, 0.3, 20)
è£½å“ç´”åº¦_ç•°å¸¸1 = 0.93 - 0.002 * (å¡”é ‚æ¸©åº¦_ç•°å¸¸1 - 85) + 0.015 * (é‚„æµæ¯”_ç•°å¸¸1 - 3.5) + np.random.normal(0, 0.01, 20)

# ã‚±ãƒ¼ã‚¹2: é‚„æµãƒãƒ³ãƒ—æ•…éšœï¼ˆé‚„æµæ¯”ä½ä¸‹ï¼‰
å¡”é ‚æ¸©åº¦_ç•°å¸¸2 = np.random.normal(85, 2, 15)
å¡”åº•æ¸©åº¦_ç•°å¸¸2 = np.random.normal(155, 3, 15)
é‚„æµæ¯”_ç•°å¸¸2 = np.random.normal(2.0, 0.4, 15)
è£½å“ç´”åº¦_ç•°å¸¸2 = 0.88 + 0.015 * (é‚„æµæ¯”_ç•°å¸¸2 - 3.5) + np.random.normal(0, 0.01, 15)

# ã‚±ãƒ¼ã‚¹3: åŸæ–™çµ„æˆç•°å¸¸
å¡”é ‚æ¸©åº¦_ç•°å¸¸3 = np.random.normal(90, 4, 15)
å¡”åº•æ¸©åº¦_ç•°å¸¸3 = np.random.normal(165, 6, 15)
é‚„æµæ¯”_ç•°å¸¸3 = np.random.normal(4.0, 0.5, 15)
è£½å“ç´”åº¦_ç•°å¸¸3 = 0.90 + np.random.normal(0, 0.015, 15)

# ãƒ‡ãƒ¼ã‚¿çµ±åˆ
X_online = np.vstack([
    np.column_stack([å¡”é ‚æ¸©åº¦_æ­£å¸¸, å¡”åº•æ¸©åº¦_æ­£å¸¸, é‚„æµæ¯”_æ­£å¸¸]),
    np.column_stack([å¡”é ‚æ¸©åº¦_ç•°å¸¸1, å¡”åº•æ¸©åº¦_ç•°å¸¸1, é‚„æµæ¯”_ç•°å¸¸1]),
    np.column_stack([å¡”é ‚æ¸©åº¦_ç•°å¸¸2, å¡”åº•æ¸©åº¦_ç•°å¸¸2, é‚„æµæ¯”_ç•°å¸¸2]),
    np.column_stack([å¡”é ‚æ¸©åº¦_ç•°å¸¸3, å¡”åº•æ¸©åº¦_ç•°å¸¸3, é‚„æµæ¯”_ç•°å¸¸3])
])

y_online = np.concatenate([è£½å“ç´”åº¦_æ­£å¸¸, è£½å“ç´”åº¦_ç•°å¸¸1, è£½å“ç´”åº¦_ç•°å¸¸2, è£½å“ç´”åº¦_ç•°å¸¸3])
labels = np.array([0]*150 + [1]*20 + [2]*15 + [3]*15)  # 0: æ­£å¸¸, 1-3: ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³

# ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç›£è¦–å®Ÿè¡Œ
anomaly_flags = []
quality_predictions = []
anomaly_scores = []
alert_messages = []

for i in range(len(X_online)):
    is_anomaly, quality_pred, anomaly_score = monitor_system.monitor(X_online[i])
    alert_msg = monitor_system.get_alert_message(is_anomaly, quality_pred, quality_threshold=0.95)

    anomaly_flags.append(is_anomaly)
    quality_predictions.append(quality_pred)
    anomaly_scores.append(anomaly_score)
    alert_messages.append(alert_msg)

    # ç•°å¸¸æ¤œå‡ºæ™‚ã®ãƒ­ã‚°å‡ºåŠ›
    if is_anomaly or quality_pred < 0.95:
        print(f"æ™‚åˆ» {i}: {alert_msg}")
        print(f"  ãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°: å¡”é ‚æ¸©åº¦={X_online[i, 0]:.1f}Â°C, å¡”åº•æ¸©åº¦={X_online[i, 1]:.1f}Â°C, é‚„æµæ¯”={X_online[i, 2]:.2f}")
        print(f"  å“è³ªäºˆæ¸¬: {quality_pred:.4f}, ç•°å¸¸ã‚¹ã‚³ã‚¢: {anomaly_score:.4f}\n")

# æ€§èƒ½è©•ä¾¡
true_anomalies = (labels > 0)
detected_anomalies = np.array(anomaly_flags)
detection_rate = np.sum(true_anomalies & detected_anomalies) / np.sum(true_anomalies)
false_alarm_rate = np.sum((~true_anomalies) & detected_anomalies) / np.sum(~true_anomalies)

print(f"\n=== çµ±åˆç›£è¦–ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½ ===")
print(f"ç•°å¸¸æ¤œå‡ºç‡: {detection_rate:.2%}")
print(f"èª¤å ±ç‡: {false_alarm_rate:.2%}")

quality_mae = mean_absolute_error(y_online, quality_predictions)
quality_r2 = r2_score(y_online, quality_predictions)
print(f"\nã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼æ€§èƒ½:")
print(f"  MAE: {quality_mae:.5f}")
print(f"  RÂ²: {quality_r2:.4f}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(3, 1, figsize=(14, 12))

# ãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°ã®æ™‚ç³»åˆ—ï¼ˆç•°å¸¸æ¤œå‡ºçµæœï¼‰
colors = ['blue' if not flag else 'red' for flag in anomaly_flags]
axes[0].scatter(range(len(X_online)), X_online[:, 0], c=colors, s=30, alpha=0.6)
axes[0].set_ylabel('å¡”é ‚æ¸©åº¦ (Â°C)', fontsize=11)
axes[0].set_title('ãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°ç›£è¦–ï¼ˆèµ¤: ç•°å¸¸æ¤œå‡ºï¼‰', fontsize=13, fontweight='bold')
axes[0].grid(alpha=0.3)

# å“è³ªäºˆæ¸¬ã®æ™‚ç³»åˆ—
axes[1].plot(y_online, 'b-', linewidth=1.5, alpha=0.6, label='å®Ÿæ¸¬å€¤')
axes[1].plot(quality_predictions, 'g--', linewidth=1.5, alpha=0.8, label='ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼äºˆæ¸¬')
axes[1].axhline(y=0.95, color='red', linestyle='--', linewidth=2, label='å“è³ªä¸‹é™')
axes[1].scatter(np.where(anomaly_flags)[0], np.array(quality_predictions)[anomaly_flags],
                color='red', s=80, marker='x', linewidths=3, label='ç•°å¸¸æ¤œå‡ºæ™‚', zorder=5)
axes[1].set_ylabel('è£½å“ç´”åº¦', fontsize=11)
axes[1].set_title('ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã«ã‚ˆã‚‹å“è³ªç›£è¦–', fontsize=13, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

# ç•°å¸¸ã‚¹ã‚³ã‚¢ã®æ™‚ç³»åˆ—
axes[2].plot(anomaly_scores, 'o-', markersize=3, linewidth=0.8, color='#11998e')
axes[2].axhline(y=0, color='red', linestyle='--', linewidth=2, label='ç•°å¸¸åˆ¤å®šå¢ƒç•Œ')
axes[2].fill_between(range(len(anomaly_scores)),
                      [min(anomaly_scores)]*len(anomaly_scores), 0,
                      alpha=0.1, color='red', label='ç•°å¸¸é ˜åŸŸ')
axes[2].set_xlabel('æ™‚åˆ»', fontsize=11)
axes[2].set_ylabel('ç•°å¸¸ã‚¹ã‚³ã‚¢', fontsize=11)
axes[2].set_title('Isolation Forest ç•°å¸¸ã‚¹ã‚³ã‚¢', fontsize=13, fontweight='bold')
axes[2].legend()
axes[2].grid(alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>è§£èª¬</strong>: çµ±åˆãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã¯ã€ç•°å¸¸æ¤œçŸ¥ã¨ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ãƒ—ãƒ­ã‚»ã‚¹ã®ç•°å¸¸çŠ¶æ…‹ã¨å“è³ªä½ä¸‹ã®ä¸¡æ–¹ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã«ç›£è¦–ã—ã¾ã™ã€‚ç•°å¸¸æ¤œçŸ¥ã¯å¤šå¤‰é‡ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ç•°å¸¸ã‚’æ‰ãˆã€ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã¯å“è³ªã¸ã®å½±éŸ¿ã‚’å®šé‡çš„ã«äºˆæ¸¬ã—ã¾ã™ã€‚ä¸¡è€…ã‚’çµ±åˆã™ã‚‹ã“ã¨ã§ã€åŒ…æ‹¬çš„ãªãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã¨æ—©æœŸè­¦å ±ãŒå®Ÿç¾ã§ãã¾ã™ã€‚</p>
</div>

<hr />

<h2>1.4 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><strong>çµ±è¨ˆçš„ç•°å¸¸æ¤œçŸ¥</strong>
    <ul>
        <li>PCAæ³•ã«ã‚ˆã‚‹Qçµ±è¨ˆé‡ãƒ»TÂ²çµ±è¨ˆé‡ã§ã®å¤šå¤‰é‡ç›£è¦–</li>
        <li>ä¸»æˆåˆ†ç©ºé–“ã¨æ®‹å·®ç©ºé–“ã§ã®ç•°å¸¸è¨ºæ–­</li>
    </ul>
</li>
<li><strong>æ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹ç•°å¸¸æ¤œçŸ¥</strong>
    <ul>
        <li>Isolation Forest: æ•™å¸«ãªã—å­¦ç¿’ã«ã‚ˆã‚‹æŸ”è»Ÿãªç•°å¸¸æ¤œå‡º</li>
        <li>Autoencoder: æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹éç·šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’</li>
        <li>LSTM: æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®äºˆæ¸¬èª¤å·®ã«åŸºã¥ãç•°å¸¸æ¤œçŸ¥</li>
    </ul>
</li>
<li><strong>å“è³ªäºˆæ¸¬ã¨ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼</strong>
    <ul>
        <li>Random Forest: éç·šå½¢å“è³ªäºˆæ¸¬ã¨ç‰¹å¾´é‡è¦åº¦åˆ†æ</li>
        <li>ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°: ä¸ç¢ºå®Ÿæ€§ã‚’å«ã‚€äºˆæ¸¬</li>
        <li>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯: ã‚ªãƒ³ãƒ©ã‚¤ãƒ³é©å¿œå­¦ç¿’</li>
    </ul>
</li>
<li><strong>çµ±åˆç›£è¦–ã‚·ã‚¹ãƒ†ãƒ </strong>
    <ul>
        <li>ç•°å¸¸æ¤œçŸ¥ã¨ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã®çµ„ã¿åˆã‚ã›</li>
        <li>åŒ…æ‹¬çš„ãªãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒ ç”Ÿæˆ</li>
    </ul>
</li>
</ol>

<h3>å®Ÿå‹™é©ç”¨ã®ãƒã‚¤ãƒ³ãƒˆ</h3>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>é©ç”¨å ´é¢</th>
<th>ãƒ¡ãƒªãƒƒãƒˆ</th>
<th>æ³¨æ„ç‚¹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>PCAç•°å¸¸æ¤œçŸ¥</strong></td>
<td>é€£ç¶šãƒ—ãƒ­ã‚»ã‚¹ã€å¤šå¤‰é‡ç›¸é–¢ç›£è¦–</td>
<td>è§£é‡ˆæ€§ãŒé«˜ã„ã€è¨ˆç®—ã‚³ã‚¹ãƒˆä½</td>
<td>ç·šå½¢ä»®å®šã€æ­£è¦åˆ†å¸ƒä»®å®š</td>
</tr>
<tr>
<td><strong>Isolation Forest</strong></td>
<td>éç·šå½¢ç•°å¸¸ã€è¤‡é›‘ãƒ‘ã‚¿ãƒ¼ãƒ³</td>
<td>ä»®å®šä¸è¦ã€é«˜é€Ÿã€é ‘å¥</td>
<td>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãŒå¿…è¦</td>
</tr>
<tr>
<td><strong>Autoencoder</strong></td>
<td>é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã€éç·šå½¢é–¢ä¿‚</td>
<td>æŸ”è»Ÿæ€§ãŒé«˜ã„ã€ç‰¹å¾´æŠ½å‡º</td>
<td>è¨“ç·´æ™‚é–“ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</td>
</tr>
<tr>
<td><strong>LSTM</strong></td>
<td>ãƒãƒƒãƒãƒ—ãƒ­ã‚»ã‚¹ã€æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³</td>
<td>æ™‚é–“ä¾å­˜æ€§å­¦ç¿’</td>
<td>ãƒ‡ãƒ¼ã‚¿é‡ã€è¨ˆç®—ã‚³ã‚¹ãƒˆ</td>
</tr>
<tr>
<td><strong>Random Forest</strong></td>
<td>å“è³ªäºˆæ¸¬ã€ç‰¹å¾´é¸æŠ</td>
<td>å¤–ã‚Œå€¤ã«é ‘å¥ã€è§£é‡ˆå¯èƒ½</td>
<td>å¤–æŒ¿æ€§èƒ½ã«åˆ¶é™</td>
</tr>
<tr>
<td><strong>GPR</strong></td>
<td>ãƒ‡ãƒ¼ã‚¿å°‘ã€ä¸ç¢ºå®Ÿæ€§é‡è¦</td>
<td>äºˆæ¸¬ä¿¡é ¼åŒºé–“ã€ãƒ™ã‚¤ã‚ºçš„</td>
<td>è¨ˆç®—ã‚³ã‚¹ãƒˆï¼ˆå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰</td>
</tr>
<tr>
<td><strong>NNé©å¿œå­¦ç¿’</strong></td>
<td>ãƒ—ãƒ­ã‚»ã‚¹ãƒ‰ãƒªãƒ•ãƒˆå¯¾ç­–</td>
<td>é•·æœŸç²¾åº¦ç¶­æŒ</td>
<td>é©å¿œã‚¿ã‚¤ãƒŸãƒ³ã‚°è¨­è¨ˆ</td>
</tr>
</tbody>
</table>

<h3>æ¬¡ã®ç« ã¸</h3>

<p>ç¬¬2ç« ã§ã¯ã€<strong>äºˆçŸ¥ä¿å…¨ã¨RULæ¨å®š</strong>ã‚’å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>æŒ¯å‹•ãƒ‡ãƒ¼ã‚¿è§£æã¨ã‚¹ãƒšã‚¯ãƒˆãƒ«ç‰¹å¾´æŠ½å‡º</li>
<li>LSTM/TCNã«ã‚ˆã‚‹åŠ£åŒ–äºˆæ¸¬</li>
<li>æ®‹å­˜è€ç”¨å¯¿å‘½ï¼ˆRUL: Remaining Useful Lifeï¼‰æ¨å®š</li>
<li>æ•…éšœãƒ¢ãƒ¼ãƒ‰åˆ†é¡ã¨è¨ºæ–­</li>
<li>ä¿å…¨è¨ˆç”»æœ€é©åŒ–</li>
</ul>

<div class="navigation">
    <a href="index.html" class="nav-button">â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter-2.html" class="nav-button">ç¬¬2ç« ã¸é€²ã‚€ â†’</a>
</div>

    </main>

    <section class="disclaimer">
        <h3>å…è²¬äº‹é …</h3>
        <ul>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾›ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€(æ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼ãªã©)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã‚ˆã³ä»˜éšã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã¯ã€Œç¾çŠ¶æœ‰å§¿(AS IS)ã€ã§æä¾›ã•ã‚Œã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€å•†å“æ€§ã€ç‰¹å®šç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã—ã¾ã›ã‚“ã€‚</li>
            <li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ã®å†…å®¹ãƒ»å¯ç”¨æ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ©ç”¨ãƒ»å®Ÿè¡Œãƒ»è§£é‡ˆã«ã‚ˆã‚Šç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ãŒç”Ÿã˜ãŸå ´åˆã§ã‚‚ã€é©ç”¨æ³•ã§è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§é™ã®ç¯„å›²ã§ã€ä½œæˆè€…ãŠã‚ˆã³æ±åŒ—å¤§å­¦ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã¯ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</li>
            <li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯æ˜è¨˜ã•ã‚ŒãŸæ¡ä»¶(ä¾‹: CC BY 4.0)ã«å¾“ã„ã¾ã™ã€‚å½“è©²ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯é€šå¸¸ã€ç„¡ä¿è¨¼æ¡é …ã‚’å«ã¿ã¾ã™ã€‚</li>
        </ul>
    </section>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: PI Knowledge Hub - Dr. Yusuke Hashimoto, Tohoku University</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-26</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>&copy; 2025 PI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
