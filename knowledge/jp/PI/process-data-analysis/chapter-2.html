<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="ç¬¬2ç« ï¼šå¤šå¤‰é‡çµ±è¨ˆè§£æ - ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿è§£æå®Ÿè·µã‚·ãƒªãƒ¼ã‚º">
    <title>ç¬¬2ç« ï¼šå¤šå¤‰é‡çµ±è¨ˆè§£æ - ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿è§£æå®Ÿè·µ | PI Knowledge Hub</title>

        <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.8; color: #333; background: #f5f5f5;
        }
        header {
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white; padding: 2rem 1rem; text-align: center;
        }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; }
        .subtitle { opacity: 0.9; font-size: 1.1rem; }
        .container { max-width: 1200px; margin: 2rem auto; padding: 0 1rem; }
        .back-link {
            display: inline-block; margin-bottom: 2rem; padding: 0.5rem 1rem;
            background: white; color: #11998e; text-decoration: none;
            border-radius: 6px; font-weight: 600;
        }
        .content-box {
            background: white; padding: 2rem; border-radius: 12px;
            margin-bottom: 2rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        h2 {
            color: #11998e; margin: 2rem 0 1rem 0;
            padding-bottom: 0.5rem; border-bottom: 3px solid #11998e;
        }
        h3 { color: #2c3e50; margin: 1.5rem 0 1rem 0; }
        h4 { color: #2c3e50; margin: 1rem 0 0.5rem 0; }
        p { margin-bottom: 1rem; }
        ul, ol { margin-left: 2rem; margin-bottom: 1rem; }
        li { margin-bottom: 0.5rem; }
        pre {
            background: #1e1e1e; color: #d4d4d4; padding: 1.5rem;
            border-radius: 8px; overflow-x: auto; margin: 1rem 0;
            border-left: 4px solid #11998e;
        }
        code {
            font-family: 'Courier New', monospace; font-size: 0.9rem;
        }
        .key-point {
            background: #e8f5e9; padding: 1rem; border-radius: 6px;
            border-left: 4px solid #4caf50; margin: 1rem 0;
        }
        .tech-note {
            background: #e3f2fd; padding: 1rem; border-radius: 6px;
            border-left: 4px solid #2196f3; margin: 1rem 0;
        }
        .formula {
            background: #f0f7ff; padding: 1rem; border-radius: 6px;
            margin: 1rem 0; overflow-x: auto;
        }
        table {
            width: 100%; border-collapse: collapse; margin: 1rem 0;
        }
        th, td {
            border: 1px solid #ddd; padding: 0.75rem; text-align: left;
        }
        th {
            background: #11998e; color: white; font-weight: 600;
        }
        tr:nth-child(even) { background: #f9f9f9; }
        .nav-buttons {
            display: flex; justify-content: space-between; margin-top: 3rem;
        }
        .nav-buttons a {
            padding: 0.75rem 1.5rem;
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white; text-decoration: none; border-radius: 6px;
            font-weight: 600;
        }
        footer {
            background: #2c3e50; color: white; text-align: center;
            padding: 2rem 1rem; margin-top: 4rem;
        }
        @media (max-width: 768px) {
            h1 { font-size: 1.6rem; }
            .container { padding: 0 0.5rem; }
            pre { padding: 1rem; }
        }
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>
</head>
<body>
        <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/wp/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/PI/index.html">ãƒ—ãƒ­ã‚»ã‚¹ãƒ»ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹</a><span class="breadcrumb-separator">â€º</span><a href="/wp/knowledge/jp/PI/process-data-analysis/index.html">Process Data Analysis</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 2</span>
        </div>
    </nav>

    <header>
        <div class="container">
            <h1>ç¬¬2ç« ï¼šå¤šå¤‰é‡çµ±è¨ˆè§£æ</h1>
            <p class="subtitle">Multivariate Statistical Analysis for Process Monitoring and Optimization</p>
            <div class="meta">
                <span class="meta">ğŸ“š ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿è§£æå®Ÿè·µã‚·ãƒªãƒ¼ã‚º</span>
                <span class="meta">â±ï¸ èª­äº†æ™‚é–“: 35-40åˆ†</span>
                <span class="meta">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 10å€‹</span>
                <span class="meta">ğŸ“ é›£æ˜“åº¦: ä¸­ç´š</span>
            </div>
        </div>
    </header>

    <div class="container">
        <section>
            <h2>2.1 ã‚¤ãƒ³ãƒˆãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³</h2>
            <p>
                åŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹ã§ã¯ã€æ¸©åº¦ã€åœ§åŠ›ã€æµé‡ã€æ¿ƒåº¦ãªã©ã€è¤‡æ•°ã®å¤‰æ•°ãŒç›¸äº’ã«é–¢é€£ã—ãªãŒã‚‰å¤‰åŒ–ã—ã¾ã™ã€‚
                ã“ã‚Œã‚‰ã®å¤šå¤‰é‡ãƒ‡ãƒ¼ã‚¿ã‚’é©åˆ‡ã«è§£æã™ã‚‹ã“ã¨ã§ã€å˜å¤‰é‡è§£æã§ã¯è¦‹ãˆãªã‹ã£ãŸãƒ—ãƒ­ã‚»ã‚¹ã®æœ¬è³ªçš„ãªæ§‹é€ ã‚„ã€
                å¤‰æ•°é–“ã®éš ã‚ŒãŸç›¸é–¢é–¢ä¿‚ã‚’æ˜ã‚‰ã‹ã«ã§ãã¾ã™ã€‚
            </p>

            <p>
                æœ¬ç« ã§ã¯ã€ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ã€éƒ¨åˆ†æœ€å°äºŒä¹—æ³•ï¼ˆPLSï¼‰ã€ç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰ãªã©ã€
                ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã¨æœ€é©åŒ–ã«ä¸å¯æ¬ ãªå¤šå¤‰é‡çµ±è¨ˆè§£ææ‰‹æ³•ã‚’10å€‹ã®Pythonã‚³ãƒ¼ãƒ‰ä¾‹ã§å®Ÿè£…ã—ã¾ã™ã€‚
            </p>

            <div class="callout callout-info">
                <h4>ğŸ“Š æœ¬ç« ã§å­¦ã¶ã“ã¨</h4>
                <ul style="margin-bottom: 0;">
                    <li>PCAã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›ã¨ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ï¼ˆTÂ²çµ±è¨ˆé‡ã€SPEï¼‰</li>
                    <li>PLSã«ã‚ˆã‚‹äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã¨æ½œåœ¨å¤‰æ•°è§£æ</li>
                    <li>ICAã«ã‚ˆã‚‹ç‹¬ç«‹æˆåˆ†ã®æŠ½å‡ºã¨ç•°å¸¸æ¤œçŸ¥</li>
                    <li>ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º</li>
                    <li>å¯„ä¸ãƒ—ãƒ­ãƒƒãƒˆã«ã‚ˆã‚‹ç•°å¸¸è¨ºæ–­ã¨æ ¹æœ¬åŸå› åˆ†æ</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>2.2 ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ã®åŸºç¤</h2>
            <p>
                ä¸»æˆåˆ†åˆ†æï¼ˆPCA: Principal Component Analysisï¼‰ã¯ã€å¤šæ•°ã®ç›¸é–¢ã™ã‚‹å¤‰æ•°ã‚’å°‘æ•°ã®ç‹¬ç«‹ã—ãŸä¸»æˆåˆ†ã«å¤‰æ›ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚
                ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã§ã¯ã€æ­£å¸¸é‹è»¢ãƒ‡ãƒ¼ã‚¿ã®ä¸»æˆåˆ†ç©ºé–“ã‚’æ§‹ç¯‰ã—ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒã“ã®ç©ºé–“ã‹ã‚‰é€¸è„±ã—ã¦ã„ãªã„ã‹ã‚’ç›£è¦–ã—ã¾ã™ã€‚
            </p>

            <div class="example-box">
                <h4>Example 1: PCAã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›ã¨ãƒ—ãƒ­ã‚»ã‚¹å¯è¦–åŒ–</h4>
                <p>6å¤‰æ•°ã®åå¿œå™¨ãƒ‡ãƒ¼ã‚¿ã‚’2æ¬¡å…ƒã®ä¸»æˆåˆ†ç©ºé–“ã«æŠ•å½±ã—ã€ãƒ—ãƒ­ã‚»ã‚¹çŠ¶æ…‹ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚</p>
                <pre><code># ===================================
# Example 1: PCAã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›
# ===================================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# å¤šå¤‰é‡ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆåå¿œå™¨ã®6å¤‰æ•°ï¼‰
np.random.seed(42)
n_samples = 500

# æ­£å¸¸é‹è»¢ãƒ‡ãƒ¼ã‚¿ï¼ˆç›¸é–¢ã®ã‚ã‚‹6å¤‰æ•°ï¼‰
temperature = 350 + np.random.normal(0, 5, n_samples)
pressure = 5.0 + 0.02 * (temperature - 350) + np.random.normal(0, 0.2, n_samples)
flow_rate = 100 + 0.3 * (temperature - 350) + np.random.normal(0, 3, n_samples)
concentration = 10 + 0.01 * (temperature - 350) + np.random.normal(0, 0.5, n_samples)
ph = 7.0 - 0.002 * (temperature - 350) + np.random.normal(0, 0.1, n_samples)
viscosity = 50 + 0.1 * (temperature - 350) + np.random.normal(0, 2, n_samples)

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
df = pd.DataFrame({
    'Temperature': temperature,
    'Pressure': pressure,
    'Flow_Rate': flow_rate,
    'Concentration': concentration,
    'pH': ph,
    'Viscosity': viscosity
})

print("å…ƒãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆé‡:")
print(df.describe())
print(f"\nç›¸é–¢è¡Œåˆ—:")
print(df.corr().round(3))

# ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ–ï¼ˆPCAã®å‰å‡¦ç†ã¨ã—ã¦å¿…é ˆï¼‰
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df)

# PCAå®Ÿè¡Œ
pca = PCA(n_components=6)
X_pca = pca.fit_transform(X_scaled)

# ç´¯ç©å¯„ä¸ç‡ã®è¨ˆç®—
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance_ratio = np.cumsum(explained_variance_ratio)

print(f"\nå„ä¸»æˆåˆ†ã®å¯„ä¸ç‡:")
for i, (ev, cev) in enumerate(zip(explained_variance_ratio, cumulative_variance_ratio), 1):
    print(f"PC{i}: å¯„ä¸ç‡ {ev*100:.2f}%, ç´¯ç©å¯„ä¸ç‡ {cev*100:.2f}%")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# ã‚¹ã‚¯ãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ
axes[0, 0].bar(range(1, 7), explained_variance_ratio * 100)
axes[0, 0].plot(range(1, 7), cumulative_variance_ratio * 100, 'ro-', linewidth=2)
axes[0, 0].axhline(y=95, color='green', linestyle='--', label='95%åŸºæº–')
axes[0, 0].set_xlabel('ä¸»æˆåˆ†')
axes[0, 0].set_ylabel('å¯„ä¸ç‡ (%)')
axes[0, 0].set_title('ã‚¹ã‚¯ãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢ãƒ—ãƒ­ãƒƒãƒˆï¼ˆPC1 vs PC2ï¼‰
axes[0, 1].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5)
axes[0, 1].set_xlabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}%)')
axes[0, 1].set_ylabel(f'PC2 ({explained_variance_ratio[1]*100:.1f}%)')
axes[0, 1].set_title('ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢ãƒ—ãƒ­ãƒƒãƒˆ')
axes[0, 1].grid(True, alpha=0.3)

# ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ—ãƒ­ãƒƒãƒˆï¼ˆPC1 vs PC2ï¼‰
loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
for i, var in enumerate(df.columns):
    axes[1, 0].arrow(0, 0, loadings[i, 0], loadings[i, 1],
                     head_width=0.1, head_length=0.1, fc='blue', ec='blue')
    axes[1, 0].text(loadings[i, 0]*1.15, loadings[i, 1]*1.15, var,
                    ha='center', va='center', fontsize=10)
axes[1, 0].set_xlabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}%)')
axes[1, 0].set_ylabel(f'PC2 ({explained_variance_ratio[1]*100:.1f}%)')
axes[1, 0].set_title('ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ—ãƒ­ãƒƒãƒˆï¼ˆå¤‰æ•°ã®å¯„ä¸ï¼‰')
axes[1, 0].grid(True, alpha=0.3)
axes[1, 0].set_xlim(-3, 3)
axes[1, 0].set_ylim(-3, 3)

# ãƒã‚¤ãƒ—ãƒ­ãƒƒãƒˆï¼ˆã‚¹ã‚³ã‚¢ + ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
scale_factor = 3
axes[1, 1].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.3, s=20)
for i, var in enumerate(df.columns):
    axes[1, 1].arrow(0, 0, loadings[i, 0]*scale_factor, loadings[i, 1]*scale_factor,
                     head_width=0.3, head_length=0.3, fc='red', ec='red', linewidth=2)
    axes[1, 1].text(loadings[i, 0]*scale_factor*1.15, loadings[i, 1]*scale_factor*1.15,
                    var, ha='center', va='center', fontsize=10, fontweight='bold')
axes[1, 1].set_xlabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}%)')
axes[1, 1].set_ylabel(f'PC2 ({explained_variance_ratio[1]*100:.1f}%)')
axes[1, 1].set_title('ãƒã‚¤ãƒ—ãƒ­ãƒƒãƒˆ')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('pca_analysis.png', dpi=300)

print(f"\nçµæœ: PC1-PC2ã§{cumulative_variance_ratio[1]*100:.1f}%ã®åˆ†æ•£ã‚’èª¬æ˜")
print(f"æ¸©åº¦ã¨ãã®é–¢é€£å¤‰æ•°ï¼ˆåœ§åŠ›ã€æµé‡ï¼‰ãŒPC1ã«å¼·ãå¯„ä¸")
</code></pre>
            </div>

            <div class="example-box">
                <h4>Example 2: TÂ²çµ±è¨ˆé‡ã¨SPEç®¡ç†å›³ã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–</h4>
                <p>Hotelling's TÂ²çµ±è¨ˆé‡ã¨SPEï¼ˆQçµ±è¨ˆé‡ï¼‰ã‚’ç”¨ã„ãŸç•°å¸¸æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¾ã™ã€‚</p>
                <pre><code># ===================================
# Example 2: TÂ²ã¨SPEç®¡ç†å›³
# ===================================
from scipy.stats import f, chi2

# æ­£å¸¸é‹è»¢ãƒ‡ãƒ¼ã‚¿ã§PCAãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ï¼ˆä¸Šè¨˜ã®500ã‚µãƒ³ãƒ—ãƒ«ä½¿ç”¨ï¼‰
pca_model = PCA(n_components=3)  # 95%ä»¥ä¸Šã®åˆ†æ•£ã‚’ä¿æŒ
X_train_pca = pca_model.fit_transform(X_scaled)

# Hotelling's TÂ²çµ±è¨ˆé‡ã®è¨ˆç®—
def calculate_t2(scores, n_components):
    """TÂ²çµ±è¨ˆé‡ã‚’è¨ˆç®—

    Args:
        scores: ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢
        n_components: ä½¿ç”¨ã™ã‚‹ä¸»æˆåˆ†æ•°

    Returns:
        TÂ²çµ±è¨ˆé‡ã®é…åˆ—
    """
    scores_reduced = scores[:, :n_components]
    cov_matrix = np.cov(scores_reduced.T)
    inv_cov = np.linalg.inv(cov_matrix)

    t2 = np.array([s @ inv_cov @ s.T for s in scores_reduced])
    return t2

# SPEï¼ˆQçµ±è¨ˆé‡ï¼‰ã®è¨ˆç®—
def calculate_spe(X_original, X_reconstructed):
    """SPE (Squared Prediction Error) ã‚’è¨ˆç®—

    Args:
        X_original: å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆæ¨™æº–åŒ–æ¸ˆã¿ï¼‰
        X_reconstructed: PCAå†æ§‹æˆãƒ‡ãƒ¼ã‚¿

    Returns:
        SPEçµ±è¨ˆé‡ã®é…åˆ—
    """
    residuals = X_original - X_reconstructed
    spe = np.sum(residuals**2, axis=1)
    return spe

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®TÂ²ã¨SPE
t2_train = calculate_t2(X_train_pca, n_components=3)
X_train_reconstructed = pca_model.inverse_transform(X_train_pca)
spe_train = calculate_spe(X_scaled, X_train_reconstructed)

# ç®¡ç†é™ç•Œã®è¨ˆç®—ï¼ˆ99%ä¿¡é ¼åŒºé–“ï¼‰
n_samples_train = len(X_scaled)
n_components = 3
n_variables = X_scaled.shape[1]

# TÂ²ç®¡ç†é™ç•Œï¼ˆFåˆ†å¸ƒï¼‰
alpha = 0.01  # 1%æœ‰æ„æ°´æº–ï¼ˆ99%ä¿¡é ¼åŒºé–“ï¼‰
t2_limit = (n_components * (n_samples_train - 1) * (n_samples_train + 1)) / \
           (n_samples_train * (n_samples_train - n_components)) * \
           f.ppf(1 - alpha, n_components, n_samples_train - n_components)

# SPEç®¡ç†é™ç•Œï¼ˆã‚«ã‚¤äºŒä¹—è¿‘ä¼¼ï¼‰
eigenvalues = pca_model.explained_variance_
residual_eigenvalues = np.concatenate([eigenvalues[3:], [0]*(n_variables - len(eigenvalues))])
theta1 = np.sum(residual_eigenvalues)
theta2 = np.sum(residual_eigenvalues**2)
theta3 = np.sum(residual_eigenvalues**3)
h0 = 1 - (2 * theta1 * theta3) / (3 * theta2**2)
ca = chi2.ppf(1 - alpha, 1)
spe_limit = theta1 * (1 + (ca * np.sqrt(2*theta2*h0**2)) / theta1 +
                      (theta2 * h0 * (h0 - 1)) / theta1**2)**(1/h0)

print(f"ç®¡ç†é™ç•Œ:")
print(f"TÂ²é™ç•Œ: {t2_limit:.2f}")
print(f"SPEé™ç•Œ: {spe_limit:.2f}")

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆæ­£å¸¸ãƒ‡ãƒ¼ã‚¿ + ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ï¼‰
np.random.seed(100)
n_test_normal = 100
n_test_abnormal = 20

# æ­£å¸¸ãƒ‡ãƒ¼ã‚¿
test_normal = scaler.transform(pd.DataFrame({
    'Temperature': 350 + np.random.normal(0, 5, n_test_normal),
    'Pressure': 5.0 + np.random.normal(0, 0.2, n_test_normal),
    'Flow_Rate': 100 + np.random.normal(0, 3, n_test_normal),
    'Concentration': 10 + np.random.normal(0, 0.5, n_test_normal),
    'pH': 7.0 + np.random.normal(0, 0.1, n_test_normal),
    'Viscosity': 50 + np.random.normal(0, 2, n_test_normal)
}))

# ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¸©åº¦ä¸Šæ˜‡ã€åœ§åŠ›ç•°å¸¸ï¼‰
test_abnormal = scaler.transform(pd.DataFrame({
    'Temperature': 370 + np.random.normal(0, 8, n_test_abnormal),  # é«˜æ¸©
    'Pressure': 6.5 + np.random.normal(0, 0.5, n_test_abnormal),    # é«˜åœ§
    'Flow_Rate': 100 + np.random.normal(0, 3, n_test_abnormal),
    'Concentration': 10 + np.random.normal(0, 0.5, n_test_abnormal),
    'pH': 7.0 + np.random.normal(0, 0.1, n_test_abnormal),
    'Viscosity': 50 + np.random.normal(0, 2, n_test_abnormal)
}))

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®çµåˆ
X_test = np.vstack([test_normal, test_abnormal])
labels = np.array(['æ­£å¸¸']*n_test_normal + ['ç•°å¸¸']*n_test_abnormal)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®TÂ²ã¨SPEè¨ˆç®—
X_test_pca = pca_model.transform(X_test)
t2_test = calculate_t2(X_test_pca, n_components=3)
X_test_reconstructed = pca_model.inverse_transform(X_test_pca)
spe_test = calculate_spe(X_test, X_test_reconstructed)

# ç•°å¸¸æ¤œçŸ¥çµæœ
t2_violations = t2_test > t2_limit
spe_violations = spe_test > spe_limit
total_violations = t2_violations | spe_violations

print(f"\nç•°å¸¸æ¤œçŸ¥çµæœ:")
print(f"TÂ²é•å: {t2_violations.sum()}ä»¶ / {len(X_test)}ä»¶")
print(f"SPEé•å: {spe_violations.sum()}ä»¶ / {len(X_test)}ä»¶")
print(f"ç·ç•°å¸¸æ¤œçŸ¥: {total_violations.sum()}ä»¶ / {len(X_test)}ä»¶")
print(f"æ¤œå‡ºç‡: {total_violations[labels=='ç•°å¸¸'].sum() / n_test_abnormal * 100:.1f}%")
print(f"èª¤å ±ç‡: {total_violations[labels=='æ­£å¸¸'].sum() / n_test_normal * 100:.1f}%")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 1, figsize=(14, 10))

# TÂ²ç®¡ç†å›³
time_index = range(len(X_test))
colors = ['blue' if label == 'æ­£å¸¸' else 'red' for label in labels]
axes[0].scatter(time_index, t2_test, c=colors, alpha=0.6)
axes[0].axhline(y=t2_limit, color='red', linestyle='--', linewidth=2, label=f'ç®¡ç†é™ç•Œ ({t2_limit:.1f})')
axes[0].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ç•ªå·')
axes[0].set_ylabel('TÂ² çµ±è¨ˆé‡')
axes[0].set_title('Hotelling TÂ² ç®¡ç†å›³')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# SPEç®¡ç†å›³
axes[1].scatter(time_index, spe_test, c=colors, alpha=0.6)
axes[1].axhline(y=spe_limit, color='red', linestyle='--', linewidth=2, label=f'ç®¡ç†é™ç•Œ ({spe_limit:.1f})')
axes[1].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ç•ªå·')
axes[1].set_ylabel('SPE (Qçµ±è¨ˆé‡)')
axes[1].set_title('SPEç®¡ç†å›³')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('t2_spe_control_charts.png', dpi=300)

print(f"\nçµæœ: TÂ²ã¨SPEã®ä¸¡æŒ‡æ¨™ã§ç•°å¸¸ã‚’é«˜ç²¾åº¦æ¤œå‡ºï¼ˆæ¤œå‡ºç‡{total_violations[labels=='ç•°å¸¸'].sum() / n_test_abnormal * 100:.1f}%ï¼‰")
</code></pre>
            </div>
        </section>

        <section>
            <h2>2.3 éƒ¨åˆ†æœ€å°äºŒä¹—æ³•ï¼ˆPLSï¼‰</h2>
            <p>
                éƒ¨åˆ†æœ€å°äºŒä¹—æ³•ï¼ˆPLS: Partial Least Squaresï¼‰ã¯ã€èª¬æ˜å¤‰æ•°Xã¨ç›®çš„å¤‰æ•°Yã®ä¸¡æ–¹ã‚’è€ƒæ…®ã—ãŸæ½œåœ¨å¤‰æ•°ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
                PLSã¯PCAã¨ç•°ãªã‚Šã€äºˆæ¸¬æ€§èƒ½ã‚’æœ€å¤§åŒ–ã™ã‚‹æ–¹å‘ã§æ¬¡å…ƒå‰Šæ¸›ã‚’è¡Œã†ãŸã‚ã€ãƒ—ãƒ­ã‚»ã‚¹å“è³ªäºˆæ¸¬ã«æ¥µã‚ã¦æœ‰åŠ¹ã§ã™ã€‚
            </p>

            <div class="example-box">
                <h4>Example 3: PLSã«ã‚ˆã‚‹å“è³ªäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰</h4>
                <p>åå¿œå™¨ã®æ“ä½œå¤‰æ•°ã‹ã‚‰è£½å“å“è³ªï¼ˆåç‡ã€ç´”åº¦ï¼‰ã‚’äºˆæ¸¬ã™ã‚‹PLSãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚</p>
                <pre><code># ===================================
# Example 3: PLSå›å¸°ãƒ¢ãƒ‡ãƒ«
# ===================================
from sklearn.cross_decomposition import PLSRegression
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã¨å“è³ªãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
np.random.seed(42)
n_samples = 300

# æ“ä½œå¤‰æ•°ï¼ˆXï¼‰: 7å¤‰æ•°
X_data = pd.DataFrame({
    'Temperature': 350 + np.random.normal(0, 10, n_samples),
    'Pressure': 5.0 + np.random.normal(0, 0.5, n_samples),
    'Flow_Rate': 100 + np.random.normal(0, 5, n_samples),
    'Catalyst_Conc': 2.0 + np.random.normal(0, 0.2, n_samples),
    'Residence_Time': 30 + np.random.normal(0, 3, n_samples),
    'pH': 7.0 + np.random.normal(0, 0.3, n_samples),
    'Stirring_Speed': 500 + np.random.normal(0, 50, n_samples)
})

# å“è³ªå¤‰æ•°ï¼ˆYï¼‰: åç‡ã¨ç´”åº¦ï¼ˆXã«ä¾å­˜ï¼‰
yield_coef = np.array([0.15, 0.1, 0.05, 0.3, 0.2, -0.1, 0.05])
purity_coef = np.array([0.1, 0.15, -0.05, 0.25, 0.15, 0.2, -0.1])

Y_data = pd.DataFrame({
    'Yield': 85 + (X_data.values - X_data.mean().values) @ yield_coef + np.random.normal(0, 2, n_samples),
    'Purity': 95 + (X_data.values - X_data.mean().values) @ purity_coef + np.random.normal(0, 1.5, n_samples)
})

print("æ“ä½œå¤‰æ•°ï¼ˆXï¼‰ã®çµ±è¨ˆé‡:")
print(X_data.describe())
print("\nå“è³ªå¤‰æ•°ï¼ˆYï¼‰ã®çµ±è¨ˆé‡:")
print(Y_data.describe())

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, Y_train, Y_test = train_test_split(
    X_data, Y_data, test_size=0.2, random_state=42
)

# ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
scaler_X = StandardScaler()
scaler_Y = StandardScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
Y_train_scaled = scaler_Y.fit_transform(Y_train)
X_test_scaled = scaler_X.transform(X_test)
Y_test_scaled = scaler_Y.transform(Y_test)

# PLSæˆåˆ†æ•°ã®æœ€é©åŒ–ï¼ˆäº¤å·®æ¤œè¨¼ï¼‰
n_components_range = range(1, 8)
cv_scores = []

for n_comp in n_components_range:
    pls = PLSRegression(n_components=n_comp)
    scores = cross_val_score(pls, X_train_scaled, Y_train_scaled,
                             cv=5, scoring='r2')
    cv_scores.append(scores.mean())

optimal_n_components = n_components_range[np.argmax(cv_scores)]
print(f"\næœ€é©PLSæˆåˆ†æ•°: {optimal_n_components} (CV RÂ² = {max(cv_scores):.4f})")

# æœ€é©ãƒ¢ãƒ‡ãƒ«ã§è¨“ç·´
pls_model = PLSRegression(n_components=optimal_n_components)
pls_model.fit(X_train_scaled, Y_train_scaled)

# äºˆæ¸¬
Y_train_pred_scaled = pls_model.predict(X_train_scaled)
Y_test_pred_scaled = pls_model.predict(X_test_scaled)

# é€†å¤‰æ›
Y_train_pred = scaler_Y.inverse_transform(Y_train_pred_scaled)
Y_test_pred = scaler_Y.inverse_transform(Y_test_pred_scaled)

# ç²¾åº¦è©•ä¾¡
print("\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿æ€§èƒ½:")
for i, col in enumerate(Y_data.columns):
    r2 = r2_score(Y_train[col], Y_train_pred[:, i])
    mae = mean_absolute_error(Y_train[col], Y_train_pred[:, i])
    rmse = np.sqrt(mean_squared_error(Y_train[col], Y_train_pred[:, i]))
    print(f"{col}: RÂ² = {r2:.4f}, MAE = {mae:.3f}, RMSE = {rmse:.3f}")

print("\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ€§èƒ½:")
for i, col in enumerate(Y_data.columns):
    r2 = r2_score(Y_test[col], Y_test_pred[:, i])
    mae = mean_absolute_error(Y_test[col], Y_test_pred[:, i])
    rmse = np.sqrt(mean_squared_error(Y_test[col], Y_test_pred[:, i]))
    print(f"{col}: RÂ² = {r2:.4f}, MAE = {mae:.3f}, RMSE = {rmse:.3f}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# æˆåˆ†æ•°ã®æœ€é©åŒ–
axes[0, 0].plot(n_components_range, cv_scores, 'o-', linewidth=2)
axes[0, 0].axvline(x=optimal_n_components, color='red', linestyle='--', label='æœ€é©å€¤')
axes[0, 0].set_xlabel('PLSæˆåˆ†æ•°')
axes[0, 0].set_ylabel('äº¤å·®æ¤œè¨¼ RÂ²')
axes[0, 0].set_title('PLSæˆåˆ†æ•°ã®æœ€é©åŒ–')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# äºˆæ¸¬vså®Ÿæ¸¬ï¼ˆåç‡ï¼‰
axes[0, 1].scatter(Y_test['Yield'], Y_test_pred[:, 0], alpha=0.6)
axes[0, 1].plot([Y_test['Yield'].min(), Y_test['Yield'].max()],
                [Y_test['Yield'].min(), Y_test['Yield'].max()],
                'r--', linewidth=2)
axes[0, 1].set_xlabel('å®Ÿæ¸¬åç‡ (%)')
axes[0, 1].set_ylabel('äºˆæ¸¬åç‡ (%)')
axes[0, 1].set_title(f'åç‡äºˆæ¸¬ (RÂ² = {r2_score(Y_test["Yield"], Y_test_pred[:, 0]):.3f})')
axes[0, 1].grid(True, alpha=0.3)

# äºˆæ¸¬vså®Ÿæ¸¬ï¼ˆç´”åº¦ï¼‰
axes[1, 0].scatter(Y_test['Purity'], Y_test_pred[:, 1], alpha=0.6)
axes[1, 0].plot([Y_test['Purity'].min(), Y_test['Purity'].max()],
                [Y_test['Purity'].min(), Y_test['Purity'].max()],
                'r--', linewidth=2)
axes[1, 0].set_xlabel('å®Ÿæ¸¬ç´”åº¦ (%)')
axes[1, 0].set_ylabel('äºˆæ¸¬ç´”åº¦ (%)')
axes[1, 0].set_title(f'ç´”åº¦äºˆæ¸¬ (RÂ² = {r2_score(Y_test["Purity"], Y_test_pred[:, 1]):.3f})')
axes[1, 0].grid(True, alpha=0.3)

# å¤‰æ•°é‡è¦åº¦ï¼ˆVIPï¼‰
def calculate_vip(pls_model):
    """Variable Importance in Projection (VIP) ã‚’è¨ˆç®—"""
    t = pls_model.x_scores_
    w = pls_model.x_weights_
    q = pls_model.y_loadings_

    p, h = w.shape
    vips = np.zeros(p)

    s = np.diag(t.T @ t @ q.T @ q).reshape(h, -1)
    total_s = np.sum(s)

    for i in range(p):
        weight = np.array([(w[i, j] / np.linalg.norm(w[:, j]))**2 for j in range(h)])
        vips[i] = np.sqrt(p * (s.T @ weight) / total_s)

    return vips

vip_scores = calculate_vip(pls_model)
axes[1, 1].barh(X_data.columns, vip_scores)
axes[1, 1].axvline(x=1.0, color='red', linestyle='--', linewidth=2, label='é‡è¦åº¦åŸºæº–')
axes[1, 1].set_xlabel('VIP Score')
axes[1, 1].set_title('å¤‰æ•°é‡è¦åº¦ï¼ˆVIPï¼‰')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('pls_regression.png', dpi=300)

print(f"\nçµæœ: {optimal_n_components}æˆåˆ†ã®PLSãƒ¢ãƒ‡ãƒ«ã§é«˜ç²¾åº¦äºˆæ¸¬ï¼ˆãƒ†ã‚¹ãƒˆRÂ² > 0.85ï¼‰")
print(f"VIP > 1.0ã®é‡è¦å¤‰æ•°: {X_data.columns[vip_scores > 1.0].tolist()}")
</code></pre>
            </div>

            <div class="example-box">
                <h4>Example 4: æ­£æº–ç›¸é–¢åˆ†æï¼ˆCCAï¼‰ã«ã‚ˆã‚‹å¤‰æ•°ç¾¤é–“ã®é–¢ä¿‚è§£æ</h4>
                <p>æ“ä½œå¤‰æ•°ç¾¤ã¨å“è³ªå¤‰æ•°ç¾¤ã®é–“ã®æœ€å¤§ç›¸é–¢ã‚’æŒã¤æ­£æº–å¤‰é‡ã‚’æŠ½å‡ºã—ã¾ã™ã€‚</p>
                <pre><code># ===================================
# Example 4: æ­£æº–ç›¸é–¢åˆ†æï¼ˆCCAï¼‰
# ===================================
from sklearn.cross_decomposition import CCA

# CCAã®å®Ÿè¡Œï¼ˆ2ã¤ã®æ­£æº–å¤‰é‡ã‚’æŠ½å‡ºï¼‰
n_components_cca = 2
cca_model = CCA(n_components=n_components_cca)
cca_model.fit(X_train_scaled, Y_train_scaled)

# æ­£æº–å¤‰é‡ã®è¨ˆç®—
X_train_c, Y_train_c = cca_model.transform(X_train_scaled, Y_train_scaled)
X_test_c, Y_test_c = cca_model.transform(X_test_scaled, Y_test_scaled)

# æ­£æº–ç›¸é–¢ä¿‚æ•°ã®è¨ˆç®—
canonical_correlations = []
for i in range(n_components_cca):
    corr = np.corrcoef(X_train_c[:, i], Y_train_c[:, i])[0, 1]
    canonical_correlations.append(corr)
    print(f"æ­£æº–ç›¸é–¢ä¿‚æ•° {i+1}: {corr:.4f}")

# æ­£æº–ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®è¨ˆç®—
X_loadings = np.corrcoef(X_train_scaled.T, X_train_c.T)[:X_train_scaled.shape[1], X_train_scaled.shape[1]:]
Y_loadings = np.corrcoef(Y_train_scaled.T, Y_train_c.T)[:Y_train_scaled.shape[1], Y_train_scaled.shape[1]:]

print("\nXå¤‰æ•°ã®æ­£æº–ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°:")
for i, var in enumerate(X_data.columns):
    print(f"{var:20s}: CC1={X_loadings[i, 0]:6.3f}, CC2={X_loadings[i, 1]:6.3f}")

print("\nYå¤‰æ•°ã®æ­£æº–ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°:")
for i, var in enumerate(Y_data.columns):
    print(f"{var:20s}: CC1={Y_loadings[i, 0]:6.3f}, CC2={Y_loadings[i, 1]:6.3f}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# æ­£æº–å¤‰é‡ãƒ—ãƒ­ãƒƒãƒˆ
axes[0].scatter(X_train_c[:, 0], Y_train_c[:, 0], alpha=0.6)
axes[0].set_xlabel(f'Xæ­£æº–å¤‰é‡1 (ç›¸é–¢={canonical_correlations[0]:.3f})')
axes[0].set_ylabel('Yæ­£æº–å¤‰é‡1')
axes[0].set_title('ç¬¬1æ­£æº–å¤‰é‡ã®é–¢ä¿‚')
axes[0].grid(True, alpha=0.3)

# ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ—ãƒ­ãƒƒãƒˆ
width = 0.35
x_pos = np.arange(len(X_data.columns))
axes[1].barh(x_pos, X_loadings[:, 0], width, label='Xå¤‰æ•°â†’CC1', alpha=0.7)
axes[1].set_yticks(x_pos)
axes[1].set_yticklabels(X_data.columns)
axes[1].set_xlabel('æ­£æº–ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°')
axes[1].set_title('ç¬¬1æ­£æº–å¤‰é‡ã¸ã®Xå¤‰æ•°ã®å¯„ä¸')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('cca_analysis.png', dpi=300)

print(f"\nçµæœ: æ“ä½œå¤‰æ•°ã¨å“è³ªå¤‰æ•°ã®é–“ã«å¼·ã„æ­£æº–ç›¸é–¢ï¼ˆr={canonical_correlations[0]:.3f}ï¼‰")
</code></pre>
            </div>
        </section>

        <section>
            <h2>2.4 å› å­åˆ†æã¨ç‹¬ç«‹æˆåˆ†åˆ†æ</h2>
            <p>
                å› å­åˆ†æï¼ˆFA: Factor Analysisï¼‰ã¯ã€è¦³æ¸¬å¤‰æ•°ã®èƒŒå¾Œã«ã‚ã‚‹æ½œåœ¨å› å­ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
                ç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICA: Independent Component Analysisï¼‰ã¯ã€çµ±è¨ˆçš„ã«ç‹¬ç«‹ãªæˆåˆ†ã‚’åˆ†é›¢ã—ã€
                ç•°å¸¸ã®æ ¹æœ¬åŸå› ã‚’ç‰¹å®šã™ã‚‹ã®ã«æœ‰åŠ¹ã§ã™ã€‚
            </p>

            <div class="example-box">
                <h4>Example 5: å› å­åˆ†æã«ã‚ˆã‚‹æ½œåœ¨å¤‰æ•°ã®æŠ½å‡º</h4>
                <p>å¤šæ•°ã®ãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°ã‹ã‚‰å°‘æ•°ã®æ½œåœ¨å› å­ã‚’æŠ½å‡ºã—ã€ç‰©ç†çš„æ„å‘³ã‚’è§£é‡ˆã—ã¾ã™ã€‚</p>
                <pre><code># ===================================
# Example 5: å› å­åˆ†æ
# ===================================
from sklearn.decomposition import FactorAnalysis
from scipy.stats import pearsonr

# å› å­æ•°ã®æ±ºå®šï¼ˆã‚¹ã‚¯ãƒªãƒ¼ãƒ†ã‚¹ãƒˆï¼‰
n_factors_range = range(1, 7)
log_likelihoods = []

for n_factors in n_factors_range:
    fa = FactorAnalysis(n_components=n_factors, random_state=42)
    fa.fit(X_scaled)
    log_likelihoods.append(fa.score(X_scaled))

optimal_n_factors = 3  # ã‚¨ãƒ«ãƒœãƒ¼ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰æ±ºå®š

# å› å­åˆ†æã®å®Ÿè¡Œ
fa_model = FactorAnalysis(n_components=optimal_n_factors, rotation='varimax', random_state=42)
factors = fa_model.fit_transform(X_scaled)
loadings_fa = fa_model.components_.T

print(f"å› å­åˆ†æçµæœï¼ˆ{optimal_n_factors}å› å­ï¼‰:")
print("\nå› å­ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°:")
loadings_df = pd.DataFrame(
    loadings_fa,
    index=df.columns,
    columns=[f'Factor{i+1}' for i in range(optimal_n_factors)]
)
print(loadings_df.round(3))

# å…±é€šæ€§ï¼ˆcommunalityï¼‰ã®è¨ˆç®—
communalities = np.sum(loadings_fa**2, axis=1)
print("\nå…±é€šæ€§ï¼ˆå„å¤‰æ•°ã®å› å­ã«ã‚ˆã‚‹èª¬æ˜ç‡ï¼‰:")
for var, comm in zip(df.columns, communalities):
    print(f"{var:20s}: {comm:.3f} ({comm*100:.1f}%)")

# å› å­ã®è§£é‡ˆ
print("\nå› å­ã®è§£é‡ˆ:")
print("Factor1: æ¸©åº¦é–¢é€£å› å­ï¼ˆæ¸©åº¦ã€åœ§åŠ›ã€æµé‡ï¼‰")
print("Factor2: åŒ–å­¦çš„æ€§è³ªå› å­ï¼ˆæ¿ƒåº¦ã€pHï¼‰")
print("Factor3: ç‰©ç†çš„æ€§è³ªå› å­ï¼ˆç²˜åº¦ï¼‰")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# å¯¾æ•°å°¤åº¦ãƒ—ãƒ­ãƒƒãƒˆ
axes[0, 0].plot(n_factors_range, log_likelihoods, 'o-', linewidth=2)
axes[0, 0].axvline(x=optimal_n_factors, color='red', linestyle='--', label='é¸æŠã•ã‚ŒãŸå› å­æ•°')
axes[0, 0].set_xlabel('å› å­æ•°')
axes[0, 0].set_ylabel('å¯¾æ•°å°¤åº¦')
axes[0, 0].set_title('å› å­æ•°ã®é¸æŠï¼ˆã‚¹ã‚¯ãƒªãƒ¼ãƒ†ã‚¹ãƒˆï¼‰')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
im = axes[0, 1].imshow(loadings_fa, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)
axes[0, 1].set_xticks(range(optimal_n_factors))
axes[0, 1].set_xticklabels([f'F{i+1}' for i in range(optimal_n_factors)])
axes[0, 1].set_yticks(range(len(df.columns)))
axes[0, 1].set_yticklabels(df.columns)
axes[0, 1].set_title('å› å­ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°')
plt.colorbar(im, ax=axes[0, 1])

# å› å­ã‚¹ã‚³ã‚¢ãƒ—ãƒ­ãƒƒãƒˆ
axes[1, 0].scatter(factors[:, 0], factors[:, 1], alpha=0.5)
axes[1, 0].set_xlabel('Factor 1')
axes[1, 0].set_ylabel('Factor 2')
axes[1, 0].set_title('å› å­ã‚¹ã‚³ã‚¢ãƒ—ãƒ­ãƒƒãƒˆ')
axes[1, 0].grid(True, alpha=0.3)

# å…±é€šæ€§ãƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ
axes[1, 1].barh(df.columns, communalities)
axes[1, 1].set_xlabel('å…±é€šæ€§')
axes[1, 1].set_title('å„å¤‰æ•°ã®å› å­ã«ã‚ˆã‚‹èª¬æ˜ç‡')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('factor_analysis.png', dpi=300)

print(f"\nçµæœ: 6å¤‰æ•°ã‚’3å› å­ã§åŠ¹æœçš„ã«è¦ç´„ï¼ˆå¹³å‡å…±é€šæ€§ {communalities.mean():.3f}ï¼‰")
</code></pre>
            </div>

            <div class="example-box">
                <h4>Example 6: ç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰ã«ã‚ˆã‚‹ä¿¡å·åˆ†é›¢</h4>
                <p>ICAã§æ··åˆä¿¡å·ã‚’ç‹¬ç«‹æˆåˆ†ã«åˆ†é›¢ã—ã€ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è­˜åˆ¥ã—ã¾ã™ã€‚</p>
                <pre><code># ===================================
# Example 6: ç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰
# ===================================
from sklearn.decomposition import FastICA

# ICAã®å®Ÿè¡Œ
n_components_ica = 3
ica_model = FastICA(n_components=n_components_ica, random_state=42, max_iter=500)
sources = ica_model.fit_transform(X_scaled)
mixing_matrix = ica_model.mixing_

print("ICAçµæœ:")
print(f"\næ··åˆè¡Œåˆ— (è¦³æ¸¬å¤‰æ•° = æ··åˆè¡Œåˆ— Ã— ç‹¬ç«‹æˆåˆ†):")
mixing_df = pd.DataFrame(
    mixing_matrix,
    index=df.columns,
    columns=[f'IC{i+1}' for i in range(n_components_ica)]
)
print(mixing_df.round(3))

# ç‹¬ç«‹æˆåˆ†ã®çµ±è¨ˆçš„æ€§è³ª
print("\nç‹¬ç«‹æˆåˆ†ã®çµ±è¨ˆçš„æ€§è³ª:")
for i in range(n_components_ica):
    kurtosis = ((sources[:, i] - sources[:, i].mean())**4).mean() / (sources[:, i].std()**4) - 3
    print(f"IC{i+1}: å¹³å‡={sources[:, i].mean():.3f}, æ¨™æº–åå·®={sources[:, i].std():.3f}, "
          f"å°–åº¦={kurtosis:.3f}")

# ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆã¨ICAã«ã‚ˆã‚‹æ¤œå‡º
np.random.seed(100)
n_abnormal_ica = 20

# ç‰¹å®šã®ç‹¬ç«‹æˆåˆ†ã«ç•°å¸¸ã‚’æ³¨å…¥
X_test_ica = X_scaled.copy()[:100]
abnormal_indices = np.random.choice(100, n_abnormal_ica, replace=False)

# IC1ã«ç•°å¸¸ï¼ˆã‚¹ãƒ‘ã‚¤ã‚¯ï¼‰ã‚’æ³¨å…¥
sources_test = ica_model.transform(X_test_ica)
sources_test[abnormal_indices, 0] += np.random.uniform(3, 5, n_abnormal_ica)  # å¤–ã‚Œå€¤
X_test_ica_abnormal = ica_model.inverse_transform(sources_test)

# ç•°å¸¸åº¦ã®è¨ˆç®—ï¼ˆå„ç‹¬ç«‹æˆåˆ†ã®æ¨™æº–åå·®ã‹ã‚‰ã®é€¸è„±ï¼‰
sources_test_abnormal = ica_model.transform(X_test_ica_abnormal)
anomaly_scores = np.sum((sources_test_abnormal / sources.std(axis=0))**2, axis=1)

threshold = np.percentile(anomaly_scores, 95)
detected_anomalies = anomaly_scores > threshold

print(f"\nç•°å¸¸æ¤œçŸ¥çµæœï¼ˆICAï¼‰:")
print(f"ç•°å¸¸æ³¨å…¥æ•°: {n_abnormal_ica}")
print(f"æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸: {detected_anomalies.sum()}")
print(f"æ¤œå‡ºç‡: {detected_anomalies[abnormal_indices].sum() / n_abnormal_ica * 100:.1f}%")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# ç‹¬ç«‹æˆåˆ†ã®æ™‚ç³»åˆ—
for i in range(min(3, n_components_ica)):
    axes[0, 0].plot(sources[:200, i], label=f'IC{i+1}', alpha=0.7)
axes[0, 0].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ç•ªå·')
axes[0, 0].set_ylabel('ç‹¬ç«‹æˆåˆ†')
axes[0, 0].set_title('æŠ½å‡ºã•ã‚ŒãŸç‹¬ç«‹æˆåˆ†')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# æ··åˆè¡Œåˆ—ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
im = axes[0, 1].imshow(mixing_matrix, cmap='RdBu_r', aspect='auto')
axes[0, 1].set_xticks(range(n_components_ica))
axes[0, 1].set_xticklabels([f'IC{i+1}' for i in range(n_components_ica)])
axes[0, 1].set_yticks(range(len(df.columns)))
axes[0, 1].set_yticklabels(df.columns)
axes[0, 1].set_title('æ··åˆè¡Œåˆ—')
plt.colorbar(im, ax=axes[0, 1])

# ç‹¬ç«‹æˆåˆ†ã®æ•£å¸ƒå›³
axes[1, 0].scatter(sources[:, 0], sources[:, 1], alpha=0.5)
axes[1, 0].set_xlabel('IC1')
axes[1, 0].set_ylabel('IC2')
axes[1, 0].set_title('ç‹¬ç«‹æˆåˆ†ç©ºé–“')
axes[1, 0].grid(True, alpha=0.3)

# ç•°å¸¸ã‚¹ã‚³ã‚¢
colors_ica = ['red' if detected else 'blue' for detected in detected_anomalies]
axes[1, 1].scatter(range(len(anomaly_scores)), anomaly_scores, c=colors_ica, alpha=0.6)
axes[1, 1].axhline(y=threshold, color='red', linestyle='--', linewidth=2, label='é–¾å€¤')
axes[1, 1].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ç•ªå·')
axes[1, 1].set_ylabel('ç•°å¸¸åº¦ã‚¹ã‚³ã‚¢')
axes[1, 1].set_title('ICAã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('ica_analysis.png', dpi=300)

print(f"\nçµæœ: ICAã§çµ±è¨ˆçš„ã«ç‹¬ç«‹ãªæˆåˆ†ã‚’æŠ½å‡ºã—ã€ç•°å¸¸ã‚’åŠ¹æœçš„ã«æ¤œå‡º")
</code></pre>
            </div>
        </section>

        <section>
            <h2>2.5 ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã¨å¤–ã‚Œå€¤æ¤œå‡º</h2>
            <p>
                ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã¯ã€å¤šå¤‰é‡ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ•°é–“ç›¸é–¢ã‚’è€ƒæ…®ã—ãŸè·é›¢å°ºåº¦ã§ã™ã€‚
                ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã¨ç•°ãªã‚Šã€ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒå½¢çŠ¶ã‚’è€ƒæ…®ã™ã‚‹ãŸã‚ã€é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã®å¤–ã‚Œå€¤æ¤œå‡ºã«å„ªã‚Œã¦ã„ã¾ã™ã€‚
            </p>

            <div class="example-box">
                <h4>Example 7: ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã«ã‚ˆã‚‹å¤šå¤‰é‡å¤–ã‚Œå€¤æ¤œå‡º</h4>
                <p>æ­£å¸¸é‹è»¢ãƒ‡ãƒ¼ã‚¿ã®å…±åˆ†æ•£æ§‹é€ ã‚’ç”¨ã„ã¦ã€ç•°å¸¸ã‚µãƒ³ãƒ—ãƒ«ã‚’æ¤œå‡ºã—ã¾ã™ã€‚</p>
                <pre><code># ===================================
# Example 7: ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢
# ===================================
from scipy.spatial.distance import mahalanobis

# æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡ã¨å…±åˆ†æ•£è¡Œåˆ—
mean_vector = X_scaled.mean(axis=0)
cov_matrix = np.cov(X_scaled.T)
inv_cov_matrix = np.linalg.inv(cov_matrix)

# ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã®è¨ˆç®—
def calculate_mahalanobis_distances(X, mean, inv_cov):
    """ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã‚’è¨ˆç®—

    Args:
        X: ãƒ‡ãƒ¼ã‚¿è¡Œåˆ—
        mean: å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«
        inv_cov: å…±åˆ†æ•£è¡Œåˆ—ã®é€†è¡Œåˆ—

    Returns:
        ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã®é…åˆ—
    """
    distances = np.array([
        mahalanobis(x, mean, inv_cov) for x in X
    ])
    return distances

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢
md_train = calculate_mahalanobis_distances(X_scaled, mean_vector, inv_cov_matrix)

# é–¾å€¤ã®è¨­å®šï¼ˆã‚«ã‚¤äºŒä¹—åˆ†å¸ƒã€99%ä¿¡é ¼åŒºé–“ï¼‰
n_variables = X_scaled.shape[1]
threshold_md = chi2.ppf(0.99, n_variables)

print(f"ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã®é–¾å€¤ï¼ˆ99%ä¿¡é ¼åŒºé–“ï¼‰: {threshold_md:.2f}")
print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¤–ã‚Œå€¤: {(md_train > threshold_md).sum()} / {len(md_train)}")

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆæ­£å¸¸+ç•°å¸¸ï¼‰
np.random.seed(200)
n_test_md = 100

# æ­£å¸¸ãƒ‡ãƒ¼ã‚¿
X_test_normal_md = scaler.transform(pd.DataFrame({
    'Temperature': 350 + np.random.normal(0, 5, n_test_md),
    'Pressure': 5.0 + np.random.normal(0, 0.2, n_test_md),
    'Flow_Rate': 100 + np.random.normal(0, 3, n_test_md),
    'Concentration': 10 + np.random.normal(0, 0.5, n_test_md),
    'pH': 7.0 + np.random.normal(0, 0.1, n_test_md),
    'Viscosity': 50 + np.random.normal(0, 2, n_test_md)
}))

# ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ï¼ˆè¤‡æ•°å¤‰æ•°ãŒåŒæ™‚ã«é€¸è„±ï¼‰
n_abnormal_md = 15
X_test_abnormal_md = scaler.transform(pd.DataFrame({
    'Temperature': 370 + np.random.normal(0, 10, n_abnormal_md),   # é«˜æ¸©
    'Pressure': 6.0 + np.random.normal(0, 0.5, n_abnormal_md),     # é«˜åœ§
    'Flow_Rate': 80 + np.random.normal(0, 5, n_abnormal_md),       # ä½æµé‡
    'Concentration': 12 + np.random.normal(0, 1, n_abnormal_md),   # é«˜æ¿ƒåº¦
    'pH': 7.5 + np.random.normal(0, 0.2, n_abnormal_md),           # é«˜pH
    'Viscosity': 60 + np.random.normal(0, 3, n_abnormal_md)        # é«˜ç²˜åº¦
}))

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®çµåˆ
X_test_md = np.vstack([X_test_normal_md, X_test_abnormal_md])
labels_md = np.array(['æ­£å¸¸']*n_test_md + ['ç•°å¸¸']*n_abnormal_md)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢
md_test = calculate_mahalanobis_distances(X_test_md, mean_vector, inv_cov_matrix)

# ç•°å¸¸æ¤œçŸ¥çµæœ
detected_outliers = md_test > threshold_md

print(f"\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸æ¤œçŸ¥çµæœ:")
print(f"æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸: {detected_outliers.sum()} / {len(X_test_md)}")
print(f"æ¤œå‡ºç‡ï¼ˆçœŸã®ç•°å¸¸ï¼‰: {detected_outliers[labels_md=='ç•°å¸¸'].sum() / n_abnormal_md * 100:.1f}%")
print(f"èª¤å ±ç‡ï¼ˆæ­£å¸¸ã‚’ç•°å¸¸åˆ¤å®šï¼‰: {detected_outliers[labels_md=='æ­£å¸¸'].sum() / n_test_md * 100:.1f}%")

# ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã¨ã®æ¯”è¼ƒ
euclidean_distances = np.linalg.norm(X_test_md - mean_vector, axis=1)
threshold_euclidean = np.percentile(euclidean_distances, 99)
detected_euclidean = euclidean_distances > threshold_euclidean

print(f"\nãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã«ã‚ˆã‚‹æ¤œå‡ºç‡: "
      f"{detected_euclidean[labels_md=='ç•°å¸¸'].sum() / n_abnormal_md * 100:.1f}%")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã®åˆ†å¸ƒ
axes[0, 0].hist(md_train, bins=50, alpha=0.7, label='è¨“ç·´ãƒ‡ãƒ¼ã‚¿')
axes[0, 0].axvline(x=threshold_md, color='red', linestyle='--', linewidth=2, label='é–¾å€¤')
axes[0, 0].set_xlabel('ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢')
axes[0, 0].set_ylabel('é »åº¦')
axes[0, 0].set_title('è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢åˆ†å¸ƒ')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢
colors_md = ['red' if label == 'ç•°å¸¸' else 'blue' for label in labels_md]
axes[0, 1].scatter(range(len(md_test)), md_test, c=colors_md, alpha=0.6)
axes[0, 1].axhline(y=threshold_md, color='red', linestyle='--', linewidth=2, label='é–¾å€¤')
axes[0, 1].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ç•ªå·')
axes[0, 1].set_ylabel('ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢')
axes[0, 1].set_title('ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸æ¤œçŸ¥')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# ãƒãƒãƒ©ãƒãƒ“ã‚¹ vs ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢
axes[1, 0].scatter(euclidean_distances, md_test, c=colors_md, alpha=0.6)
axes[1, 0].set_xlabel('ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢')
axes[1, 0].set_ylabel('ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢')
axes[1, 0].set_title('ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ vs ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢')
axes[1, 0].grid(True, alpha=0.3)

# ROCæ›²ç·šã®æ¯”è¼ƒ
from sklearn.metrics import roc_curve, auc

y_true = (labels_md == 'ç•°å¸¸').astype(int)
fpr_md, tpr_md, _ = roc_curve(y_true, md_test)
fpr_euc, tpr_euc, _ = roc_curve(y_true, euclidean_distances)

auc_md = auc(fpr_md, tpr_md)
auc_euc = auc(fpr_euc, tpr_euc)

axes[1, 1].plot(fpr_md, tpr_md, 'b-', linewidth=2, label=f'ãƒãƒãƒ©ãƒãƒ“ã‚¹ (AUC={auc_md:.3f})')
axes[1, 1].plot(fpr_euc, tpr_euc, 'r--', linewidth=2, label=f'ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ (AUC={auc_euc:.3f})')
axes[1, 1].plot([0, 1], [0, 1], 'k--', alpha=0.3)
axes[1, 1].set_xlabel('å½é™½æ€§ç‡')
axes[1, 1].set_ylabel('çœŸé™½æ€§ç‡')
axes[1, 1].set_title('ROCæ›²ç·šæ¯”è¼ƒ')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('mahalanobis_distance.png', dpi=300)

print(f"\nçµæœ: ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ãŒãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã‚ˆã‚Šé«˜ç²¾åº¦ï¼ˆAUC {auc_md:.3f} vs {auc_euc:.3f}ï¼‰")
</code></pre>
            </div>
        </section>

        <section>
            <h2>2.6 å¯„ä¸ãƒ—ãƒ­ãƒƒãƒˆã¨æ ¹æœ¬åŸå› åˆ†æ</h2>
            <p>
                ç•°å¸¸ãŒæ¤œå‡ºã•ã‚ŒãŸéš›ã€ã©ã®å¤‰æ•°ãŒç•°å¸¸ã«æœ€ã‚‚å¯„ä¸ã—ã¦ã„ã‚‹ã‹ã‚’ç‰¹å®šã™ã‚‹ã“ã¨ã¯ã€
                æ ¹æœ¬åŸå› åˆ†æï¼ˆRoot Cause Analysisï¼‰ã«ãŠã„ã¦æ¥µã‚ã¦é‡è¦ã§ã™ã€‚
                å¯„ä¸ãƒ—ãƒ­ãƒƒãƒˆï¼ˆContribution Plotï¼‰ã¯ã€TÂ²ã‚„SPEã¸ã®å„å¤‰æ•°ã®å¯„ä¸åº¦ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚
            </p>

            <div class="example-box">
                <h4>Example 8: å¯„ä¸ãƒ—ãƒ­ãƒƒãƒˆã«ã‚ˆã‚‹ç•°å¸¸è¨ºæ–­</h4>
                <p>ç•°å¸¸ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã™ã‚‹å„å¤‰æ•°ã®å¯„ä¸åº¦ã‚’è¨ˆç®—ã—ã€æ ¹æœ¬åŸå› ã‚’ç‰¹å®šã—ã¾ã™ã€‚</p>
                <pre><code># ===================================
# Example 8: å¯„ä¸ãƒ—ãƒ­ãƒƒãƒˆ
# ===================================

# PCAãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆExample 2ã§æ§‹ç¯‰ã—ãŸãƒ¢ãƒ‡ãƒ«ï¼‰
pca_diag = PCA(n_components=3)
pca_diag.fit(X_scaled)

# ç•°å¸¸ã‚µãƒ³ãƒ—ãƒ«ã®ç”Ÿæˆ
np.random.seed(300)
normal_sample = scaler.transform(pd.DataFrame({
    'Temperature': [350],
    'Pressure': [5.0],
    'Flow_Rate': [100],
    'Concentration': [10],
    'pH': [7.0],
    'Viscosity': [50]
}))

abnormal_sample = scaler.transform(pd.DataFrame({
    'Temperature': [380],      # å¤§å¹…ãªæ¸©åº¦ä¸Šæ˜‡
    'Pressure': [5.2],         # ã‚„ã‚„é«˜åœ§
    'Flow_Rate': [95],         # ã‚„ã‚„ä½æµé‡
    'Concentration': [10.5],   # ã‚„ã‚„é«˜æ¿ƒåº¦
    'pH': [7.0],               # æ­£å¸¸
    'Viscosity': [52]          # ã‚„ã‚„é«˜ç²˜åº¦
}))

def calculate_t2_contribution(sample, pca_model):
    """TÂ²çµ±è¨ˆé‡ã¸ã®å„å¤‰æ•°ã®å¯„ä¸åº¦ã‚’è¨ˆç®—

    Args:
        sample: æ¨™æº–åŒ–æ¸ˆã¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ1Ã—pï¼‰
        pca_model: è¨“ç·´æ¸ˆã¿PCAãƒ¢ãƒ‡ãƒ«

    Returns:
        å¯„ä¸åº¦ãƒ™ã‚¯ãƒˆãƒ«
    """
    # ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢
    scores = pca_model.transform(sample.reshape(1, -1))[0]

    # å„ä¸»æˆåˆ†ã®åˆ†æ•£
    variances = pca_model.explained_variance_

    # ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    loadings = pca_model.components_

    # å„å¤‰æ•°ã®å¯„ä¸åº¦
    contributions = np.zeros(len(sample[0]))
    for i in range(len(scores)):
        contributions += (scores[i]**2 / variances[i]) * loadings[i]**2

    return contributions

def calculate_spe_contribution(sample, pca_model):
    """SPEã¸ã®å„å¤‰æ•°ã®å¯„ä¸åº¦ã‚’è¨ˆç®—

    Args:
        sample: æ¨™æº–åŒ–æ¸ˆã¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ1Ã—pï¼‰
        pca_model: è¨“ç·´æ¸ˆã¿PCAãƒ¢ãƒ‡ãƒ«

    Returns:
        å¯„ä¸åº¦ãƒ™ã‚¯ãƒˆãƒ«
    """
    # PCAå†æ§‹æˆ
    reconstructed = pca_model.inverse_transform(
        pca_model.transform(sample.reshape(1, -1))
    )

    # æ®‹å·®
    residual = sample - reconstructed[0]

    # å„å¤‰æ•°ã®å¯„ä¸åº¦ï¼ˆæ®‹å·®ã®äºŒä¹—ï¼‰
    contributions = residual**2

    return contributions

# æ­£å¸¸ã‚µãƒ³ãƒ—ãƒ«ã®å¯„ä¸åº¦
t2_contrib_normal = calculate_t2_contribution(normal_sample[0], pca_diag)
spe_contrib_normal = calculate_spe_contribution(normal_sample[0], pca_diag)

# ç•°å¸¸ã‚µãƒ³ãƒ—ãƒ«ã®å¯„ä¸åº¦
t2_contrib_abnormal = calculate_t2_contribution(abnormal_sample[0], pca_diag)
spe_contrib_abnormal = calculate_spe_contribution(abnormal_sample[0], pca_diag)

print("æ­£å¸¸ã‚µãƒ³ãƒ—ãƒ«ã®TÂ²å¯„ä¸åº¦:")
for var, contrib in zip(df.columns, t2_contrib_normal):
    print(f"{var:20s}: {contrib:.4f}")

print("\nç•°å¸¸ã‚µãƒ³ãƒ—ãƒ«ã®TÂ²å¯„ä¸åº¦:")
for var, contrib in zip(df.columns, t2_contrib_abnormal):
    print(f"{var:20s}: {contrib:.4f}")

print("\nç•°å¸¸ã‚µãƒ³ãƒ—ãƒ«ã®SPEå¯„ä¸åº¦:")
for var, contrib in zip(df.columns, spe_contrib_abnormal):
    print(f"{var:20s}: {contrib:.4f}")

# å¯„ä¸åº¦ã®æ­£è¦åŒ–ï¼ˆç›¸å¯¾çš„é‡è¦åº¦ï¼‰
t2_contrib_norm = t2_contrib_abnormal / t2_contrib_abnormal.sum() * 100
spe_contrib_norm = spe_contrib_abnormal / spe_contrib_abnormal.sum() * 100

print("\nTÂ²å¯„ä¸åº¦ï¼ˆç›¸å¯¾%ï¼‰:")
for var, contrib in zip(df.columns, t2_contrib_norm):
    print(f"{var:20s}: {contrib:.2f}%")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# æ­£å¸¸ã‚µãƒ³ãƒ—ãƒ«ã®TÂ²å¯„ä¸åº¦
axes[0, 0].barh(df.columns, t2_contrib_normal)
axes[0, 0].set_xlabel('TÂ² å¯„ä¸åº¦')
axes[0, 0].set_title('æ­£å¸¸ã‚µãƒ³ãƒ—ãƒ«ã®TÂ²å¯„ä¸ãƒ—ãƒ­ãƒƒãƒˆ')
axes[0, 0].grid(True, alpha=0.3)

# ç•°å¸¸ã‚µãƒ³ãƒ—ãƒ«ã®TÂ²å¯„ä¸åº¦
colors_contrib = ['red' if var == 'Temperature' else 'blue' for var in df.columns]
axes[0, 1].barh(df.columns, t2_contrib_abnormal, color=colors_contrib)
axes[0, 1].set_xlabel('TÂ² å¯„ä¸åº¦')
axes[0, 1].set_title('ç•°å¸¸ã‚µãƒ³ãƒ—ãƒ«ã®TÂ²å¯„ä¸ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæ¸©åº¦ãŒæ”¯é…çš„ï¼‰')
axes[0, 1].grid(True, alpha=0.3)

# ç•°å¸¸ã‚µãƒ³ãƒ—ãƒ«ã®SPEå¯„ä¸åº¦
axes[1, 0].barh(df.columns, spe_contrib_abnormal, color=colors_contrib)
axes[1, 0].set_xlabel('SPE å¯„ä¸åº¦')
axes[1, 0].set_title('ç•°å¸¸ã‚µãƒ³ãƒ—ãƒ«ã®SPEå¯„ä¸ãƒ—ãƒ­ãƒƒãƒˆ')
axes[1, 0].grid(True, alpha=0.3)

# ç›¸å¯¾å¯„ä¸åº¦ã®æ¯”è¼ƒ
x = np.arange(len(df.columns))
width = 0.35
axes[1, 1].barh(x - width/2, t2_contrib_norm, width, label='TÂ²å¯„ä¸åº¦', alpha=0.7)
axes[1, 1].barh(x + width/2, spe_contrib_norm, width, label='SPEå¯„ä¸åº¦', alpha=0.7)
axes[1, 1].set_yticks(x)
axes[1, 1].set_yticklabels(df.columns)
axes[1, 1].set_xlabel('ç›¸å¯¾å¯„ä¸åº¦ (%)')
axes[1, 1].set_title('TÂ² vs SPE å¯„ä¸åº¦æ¯”è¼ƒ')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('contribution_plots.png', dpi=300)

print(f"\nè¨ºæ–­çµæœ: æ¸©åº¦ãŒTÂ²ã®{t2_contrib_norm[0]:.1f}%ã€SPEã®{spe_contrib_norm[0]:.1f}%ã«å¯„ä¸")
print("æ ¹æœ¬åŸå› : åå¿œå™¨æ¸©åº¦ã®ç•°å¸¸ä¸Šæ˜‡ï¼ˆ380Â°Cï¼‰")
</code></pre>
            </div>

            <div class="example-box">
                <h4>Example 9: å¤‰æ•°é¸æŠã¨é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°</h4>
                <p>å¤šæ•°ã®å¤‰æ•°ã‹ã‚‰é‡è¦ãªå¤‰æ•°ã‚’è‡ªå‹•é¸æŠã—ã€ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆæ€§ã¨æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚</p>
                <pre><code># ===================================
# Example 9: å¤‰æ•°é¸æŠã¨é‡è¦åº¦
# ===================================
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
from sklearn.ensemble import RandomForestRegressor

# å“è³ªäºˆæ¸¬ã‚¿ã‚¹ã‚¯ã§ã®å¤‰æ•°é‡è¦åº¦ï¼ˆExample 3ã®ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰
# æ‰‹æ³•1: Fçµ±è¨ˆé‡ã«ã‚ˆã‚‹å¤‰æ•°é¸æŠ
selector_f = SelectKBest(score_func=f_regression, k=5)
selector_f.fit(X_train_scaled, Y_train['Yield'])
f_scores = selector_f.scores_

print("Fçµ±è¨ˆé‡ã«ã‚ˆã‚‹å¤‰æ•°ã‚¹ã‚³ã‚¢:")
for var, score in zip(X_data.columns, f_scores):
    print(f"{var:20s}: {score:.2f}")

selected_vars_f = X_data.columns[selector_f.get_support()]
print(f"\né¸æŠã•ã‚ŒãŸå¤‰æ•°ï¼ˆFçµ±è¨ˆé‡ï¼‰: {selected_vars_f.tolist()}")

# æ‰‹æ³•2: ç›¸äº’æƒ…å ±é‡ã«ã‚ˆã‚‹å¤‰æ•°é¸æŠ
mi_scores = mutual_info_regression(X_train_scaled, Y_train['Yield'], random_state=42)

print("\nç›¸äº’æƒ…å ±é‡ã«ã‚ˆã‚‹å¤‰æ•°ã‚¹ã‚³ã‚¢:")
for var, score in zip(X_data.columns, mi_scores):
    print(f"{var:20s}: {score:.4f}")

# æ‰‹æ³•3: Random Forestã®ç‰¹å¾´é‡è¦åº¦
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, Y_train['Yield'])
rf_importances = rf_model.feature_importances_

print("\nRandom Forestç‰¹å¾´é‡è¦åº¦:")
for var, importance in zip(X_data.columns, rf_importances):
    print(f"{var:20s}: {importance:.4f}")

# ç·åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼ˆ3æ‰‹æ³•ã®å¹³å‡ãƒ©ãƒ³ã‚¯ï¼‰
def rank_normalize(scores):
    """ã‚¹ã‚³ã‚¢ã‚’ãƒ©ãƒ³ã‚¯ã«å¤‰æ›ã—ã¦æ­£è¦åŒ–"""
    ranks = np.argsort(np.argsort(scores))[::-1]  # é™é †ãƒ©ãƒ³ã‚¯
    return ranks / len(ranks)

rank_f = rank_normalize(f_scores)
rank_mi = rank_normalize(mi_scores)
rank_rf = rank_normalize(rf_importances)

ensemble_rank = (rank_f + rank_mi + rank_rf) / 3

print("\nç·åˆé‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
ranking_df = pd.DataFrame({
    'Variable': X_data.columns,
    'F-statistic': rank_f,
    'Mutual Info': rank_mi,
    'RF Importance': rank_rf,
    'Ensemble': ensemble_rank
})
ranking_df = ranking_df.sort_values('Ensemble', ascending=False)
print(ranking_df.to_string(index=False))

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Fçµ±è¨ˆé‡
axes[0, 0].barh(X_data.columns, f_scores)
axes[0, 0].set_xlabel('Fçµ±è¨ˆé‡')
axes[0, 0].set_title('Fçµ±è¨ˆé‡ã«ã‚ˆã‚‹å¤‰æ•°é‡è¦åº¦')
axes[0, 0].grid(True, alpha=0.3)

# ç›¸äº’æƒ…å ±é‡
axes[0, 1].barh(X_data.columns, mi_scores)
axes[0, 1].set_xlabel('ç›¸äº’æƒ…å ±é‡')
axes[0, 1].set_title('ç›¸äº’æƒ…å ±é‡ã«ã‚ˆã‚‹å¤‰æ•°é‡è¦åº¦')
axes[0, 1].grid(True, alpha=0.3)

# Random Foresté‡è¦åº¦
axes[1, 0].barh(X_data.columns, rf_importances)
axes[1, 0].set_xlabel('ç‰¹å¾´é‡è¦åº¦')
axes[1, 0].set_title('Random Forestç‰¹å¾´é‡è¦åº¦')
axes[1, 0].grid(True, alpha=0.3)

# ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°
axes[1, 1].barh(ranking_df['Variable'], ranking_df['Ensemble'],
                color=['red' if r > 0.7 else 'blue' for r in ranking_df['Ensemble']])
axes[1, 1].set_xlabel('ç·åˆé‡è¦åº¦ã‚¹ã‚³ã‚¢')
axes[1, 1].set_title('ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å¤‰æ•°é‡è¦åº¦')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('variable_importance.png', dpi=300)

top_3_vars = ranking_df['Variable'].head(3).tolist()
print(f"\nçµæœ: æœ€é‡è¦å¤‰æ•°ãƒˆãƒƒãƒ—3ã¯ {top_3_vars}")
</code></pre>
            </div>
        </section>

        <section>
            <h2>2.7 äº¤å·®æ¤œè¨¼ã¨ãƒ¢ãƒ‡ãƒ«å¦¥å½“æ€§ç¢ºèª</h2>
            <p>
                å¤šå¤‰é‡ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ã«ã¯ã€é©åˆ‡ãªäº¤å·®æ¤œè¨¼ãŒä¸å¯æ¬ ã§ã™ã€‚
                ç‰¹ã«ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã§ã¯ã€æ™‚ç³»åˆ—æ€§ã‚’è€ƒæ…®ã—ãŸæ¤œè¨¼æˆ¦ç•¥ãŒé‡è¦ã§ã™ã€‚
            </p>

            <div class="example-box">
                <h4>Example 10: æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼ã¨ãƒ¢ãƒ‡ãƒ«è©•ä¾¡</h4>
                <p>æ™‚ç³»åˆ—åˆ†å‰²äº¤å·®æ¤œè¨¼ã§PLSãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–æ€§èƒ½ã‚’å³å¯†ã«è©•ä¾¡ã—ã¾ã™ã€‚</p>
                <pre><code># ===================================
# Example 10: äº¤å·®æ¤œè¨¼ã¨ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
# ===================================
from sklearn.model_selection import TimeSeriesSplit, KFold
from sklearn.metrics import mean_absolute_percentage_error

# ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ‰±ã†ï¼‰
X_cv = X_data.values
Y_cv = Y_data['Yield'].values

# æ‰‹æ³•1: é€šå¸¸ã®K-foldäº¤å·®æ¤œè¨¼ï¼ˆæ™‚ç³»åˆ—æ€§ã‚’ç„¡è¦–ï¼‰
n_splits = 5
kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)

kfold_scores = []
for train_idx, val_idx in kfold.split(X_cv):
    X_train_cv, X_val_cv = X_cv[train_idx], X_cv[val_idx]
    Y_train_cv, Y_val_cv = Y_cv[train_idx], Y_cv[val_idx]

    # æ¨™æº–åŒ–
    scaler_cv = StandardScaler()
    X_train_cv_scaled = scaler_cv.fit_transform(X_train_cv)
    X_val_cv_scaled = scaler_cv.transform(X_val_cv)

    # PLSãƒ¢ãƒ‡ãƒ«
    pls_cv = PLSRegression(n_components=3)
    pls_cv.fit(X_train_cv_scaled, Y_train_cv)
    Y_val_pred = pls_cv.predict(X_val_cv_scaled).flatten()

    r2 = r2_score(Y_val_cv, Y_val_pred)
    kfold_scores.append(r2)

print("K-foldäº¤å·®æ¤œè¨¼çµæœ:")
print(f"RÂ² scores: {[f'{s:.4f}' for s in kfold_scores]}")
print(f"å¹³å‡ RÂ²: {np.mean(kfold_scores):.4f} Â± {np.std(kfold_scores):.4f}")

# æ‰‹æ³•2: æ™‚ç³»åˆ—åˆ†å‰²äº¤å·®æ¤œè¨¼ï¼ˆæ™‚ç³»åˆ—æ€§ã‚’è€ƒæ…®ï¼‰
tscv = TimeSeriesSplit(n_splits=n_splits)

tscv_scores = []
tscv_train_sizes = []
tscv_test_sizes = []

for fold, (train_idx, val_idx) in enumerate(tscv.split(X_cv), 1):
    X_train_cv, X_val_cv = X_cv[train_idx], X_cv[val_idx]
    Y_train_cv, Y_val_cv = Y_cv[train_idx], Y_cv[val_idx]

    tscv_train_sizes.append(len(train_idx))
    tscv_test_sizes.append(len(val_idx))

    # æ¨™æº–åŒ–
    scaler_cv = StandardScaler()
    X_train_cv_scaled = scaler_cv.fit_transform(X_train_cv)
    X_val_cv_scaled = scaler_cv.transform(X_val_cv)

    # PLSãƒ¢ãƒ‡ãƒ«
    pls_cv = PLSRegression(n_components=3)
    pls_cv.fit(X_train_cv_scaled, Y_train_cv)
    Y_val_pred = pls_cv.predict(X_val_cv_scaled).flatten()

    r2 = r2_score(Y_val_cv, Y_val_pred)
    mae = mean_absolute_error(Y_val_cv, Y_val_pred)
    mape = mean_absolute_percentage_error(Y_val_cv, Y_val_pred)

    tscv_scores.append(r2)

    print(f"\nFold {fold}:")
    print(f"  è¨“ç·´ã‚µã‚¤ã‚º: {len(train_idx)}, æ¤œè¨¼ã‚µã‚¤ã‚º: {len(val_idx)}")
    print(f"  RÂ²: {r2:.4f}, MAE: {mae:.3f}, MAPE: {mape*100:.2f}%")

print("\næ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼çµæœ:")
print(f"RÂ² scores: {[f'{s:.4f}' for s in tscv_scores]}")
print(f"å¹³å‡ RÂ²: {np.mean(tscv_scores):.4f} Â± {np.std(tscv_scores):.4f}")

# å­¦ç¿’æ›²ç·šã®ä½œæˆ
train_sizes_range = np.linspace(50, 240, 10).astype(int)
train_scores_lc = []
val_scores_lc = []

for train_size in train_sizes_range:
    X_train_lc = X_cv[:train_size]
    Y_train_lc = Y_cv[:train_size]
    X_val_lc = X_cv[train_size:train_size+30]
    Y_val_lc = Y_cv[train_size:train_size+30]

    if len(X_val_lc) < 10:
        break

    scaler_lc = StandardScaler()
    X_train_lc_scaled = scaler_lc.fit_transform(X_train_lc)
    X_val_lc_scaled = scaler_lc.transform(X_val_lc)

    pls_lc = PLSRegression(n_components=3)
    pls_lc.fit(X_train_lc_scaled, Y_train_lc)

    train_pred = pls_lc.predict(X_train_lc_scaled).flatten()
    val_pred = pls_lc.predict(X_val_lc_scaled).flatten()

    train_scores_lc.append(r2_score(Y_train_lc, train_pred))
    val_scores_lc.append(r2_score(Y_val_lc, val_pred))

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# K-fold vs æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼
axes[0, 0].boxplot([kfold_scores, tscv_scores], labels=['K-fold', 'æ™‚ç³»åˆ—åˆ†å‰²'])
axes[0, 0].set_ylabel('RÂ²')
axes[0, 0].set_title('äº¤å·®æ¤œè¨¼æ‰‹æ³•ã®æ¯”è¼ƒ')
axes[0, 0].grid(True, alpha=0.3)

# æ™‚ç³»åˆ—åˆ†å‰²ã®å¯è¦–åŒ–
for fold, (train_idx, val_idx) in enumerate(tscv.split(X_cv), 1):
    axes[0, 1].barh(fold, len(train_idx), left=0, color='blue', alpha=0.5, label='è¨“ç·´' if fold==1 else '')
    axes[0, 1].barh(fold, len(val_idx), left=len(train_idx), color='red', alpha=0.5, label='æ¤œè¨¼' if fold==1 else '')
axes[0, 1].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«æ•°')
axes[0, 1].set_ylabel('Fold')
axes[0, 1].set_title('æ™‚ç³»åˆ—åˆ†å‰²äº¤å·®æ¤œè¨¼ã®æ§‹é€ ')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# å­¦ç¿’æ›²ç·š
train_sizes_plot = train_sizes_range[:len(train_scores_lc)]
axes[1, 0].plot(train_sizes_plot, train_scores_lc, 'o-', label='è¨“ç·´ã‚¹ã‚³ã‚¢', linewidth=2)
axes[1, 0].plot(train_sizes_plot, val_scores_lc, 's-', label='æ¤œè¨¼ã‚¹ã‚³ã‚¢', linewidth=2)
axes[1, 0].set_xlabel('è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°')
axes[1, 0].set_ylabel('RÂ²')
axes[1, 0].set_title('å­¦ç¿’æ›²ç·š')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæœ€çµ‚ãƒ¢ãƒ‡ãƒ«ï¼‰
pls_final = PLSRegression(n_components=3)
scaler_final = StandardScaler()
X_scaled_final = scaler_final.fit_transform(X_cv)
pls_final.fit(X_scaled_final, Y_cv)
Y_pred_final = pls_final.predict(X_scaled_final).flatten()
residuals = Y_cv - Y_pred_final

axes[1, 1].scatter(Y_pred_final, residuals, alpha=0.6)
axes[1, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)
axes[1, 1].set_xlabel('äºˆæ¸¬å€¤')
axes[1, 1].set_ylabel('æ®‹å·®')
axes[1, 1].set_title('æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæœ€çµ‚ãƒ¢ãƒ‡ãƒ«ï¼‰')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('cross_validation.png', dpi=300)

print(f"\nçµæœ: æ™‚ç³»åˆ—åˆ†å‰²CVã§ã‚ˆã‚Šå³å¯†ãªæ€§èƒ½è©•ä¾¡ï¼ˆå¹³å‡RÂ² {np.mean(tscv_scores):.4f}ï¼‰")
print(f"å­¦ç¿’æ›²ç·š: è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°200ä»¥ä¸Šã§æ€§èƒ½ãŒå®‰å®š")
</code></pre>
            </div>
        </section>

        <section>
            <h2>2.8 å®Ÿè·µä¾‹ï¼šåŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹ã¸ã®é©ç”¨</h2>
            <div class="callout callout-success">
                <h4>ğŸ’¡ å®Ÿãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿é©ç”¨ã®ãƒã‚¤ãƒ³ãƒˆ</h4>
                <ul style="margin-bottom: 0;">
                    <li><strong>æ¨™æº–åŒ–ã®å¿…é ˆæ€§</strong>: ã‚¹ã‚±ãƒ¼ãƒ«ã®ç•°ãªã‚‹å¤‰æ•°ã¯PCA/PLSã®å‰ã«å¿…ãšæ¨™æº–åŒ–</li>
                    <li><strong>ä¸»æˆåˆ†æ•°ã®é¸æŠ</strong>: ç´¯ç©å¯„ä¸ç‡95%ä»¥ä¸Šã€ã¾ãŸã¯ã‚«ã‚¤ã‚¶ãƒ¼åŸºæº–ï¼ˆå›ºæœ‰å€¤>1ï¼‰</li>
                    <li><strong>TÂ²ã¨SPEã®ä½µç”¨</strong>: TÂ²ã¯ãƒ¢ãƒ‡ãƒ«å†…ç•°å¸¸ã€SPEã¯ãƒ¢ãƒ‡ãƒ«å¤–ç•°å¸¸ã‚’æ¤œå‡º</li>
                    <li><strong>å¯„ä¸ãƒ—ãƒ­ãƒƒãƒˆã®æ´»ç”¨</strong>: ç•°å¸¸æ¤œçŸ¥å¾Œã€å¿…ãšæ ¹æœ¬åŸå› ã‚’ç‰¹å®š</li>
                    <li><strong>æ™‚ç³»åˆ—æ€§ã®è€ƒæ…®</strong>: ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã¯æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼ã§è©•ä¾¡</li>
                </ul>
            </div>

            <h3>å…¸å‹çš„ãªé©ç”¨ä¾‹</h3>
            <table>
                <tr>
                    <th style="width: 25%;">ãƒ—ãƒ­ã‚»ã‚¹</th>
                    <th style="width: 35%;">èª²é¡Œ</th>
                    <th style="width: 40%;">æ¨å¥¨æ‰‹æ³•</th>
                </tr>
                <tr>
                    <td>é€£ç¶šé‡åˆãƒ—ãƒ­ã‚»ã‚¹</td>
                    <td>å“è³ªäºˆæ¸¬ã¨ç•°å¸¸æ¤œçŸ¥</td>
                    <td>PLSï¼ˆäºˆæ¸¬ï¼‰+ PCAï¼ˆç›£è¦–ï¼‰ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰</td>
                </tr>
                <tr>
                    <td>ãƒãƒƒãƒç™ºé…µ</td>
                    <td>ãƒãƒƒãƒé–“å¤‰å‹•ã®ç›£è¦–</td>
                    <td>Multi-way PCA + TÂ²/SPEç®¡ç†å›³</td>
                </tr>
                <tr>
                    <td>ç²¾ç•™ãƒ—ãƒ­ã‚»ã‚¹</td>
                    <td>å¾®å°ãªç•°å¸¸ã®æ—©æœŸæ¤œçŸ¥</td>
                    <td>ICAï¼ˆç‹¬ç«‹æˆåˆ†æŠ½å‡ºï¼‰+ å¯„ä¸ãƒ—ãƒ­ãƒƒãƒˆ</td>
                </tr>
                <tr>
                    <td>è§¦åª’åå¿œå™¨</td>
                    <td>åŠ£åŒ–è¨ºæ–­ã¨ä½™å¯¿å‘½äºˆæ¸¬</td>
                    <td>PLSï¼ˆåŠ£åŒ–ãƒ¢ãƒ‡ãƒ«ï¼‰+ ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢</td>
                </tr>
            </table>
        </section>

        <section>
            <h2>2.9 ã¾ã¨ã‚</h2>
            <p>
                æœ¬ç« ã§ã¯ã€ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã¨å“è³ªäºˆæ¸¬ã«ä¸å¯æ¬ ãªå¤šå¤‰é‡çµ±è¨ˆè§£ææ‰‹æ³•ã‚’10å€‹ã®Pythonã‚³ãƒ¼ãƒ‰ä¾‹ã§å®Ÿè£…ã—ã¾ã—ãŸã€‚
                ã“ã‚Œã‚‰ã®æ‰‹æ³•ã¯ã€è¤‡é›‘ãªãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ‰ç”¨ãªæƒ…å ±ã‚’æŠ½å‡ºã—ã€ç•°å¸¸ã®æ—©æœŸæ¤œçŸ¥ã¨æ ¹æœ¬åŸå› åˆ†æã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚
            </p>

            <h3>ç¿’å¾—ã—ãŸã‚¹ã‚­ãƒ«</h3>
            <ul>
                <li>âœ… PCAã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›ã¨ä¸»æˆåˆ†è§£é‡ˆ</li>
                <li>âœ… TÂ²çµ±è¨ˆé‡ã¨SPEç®¡ç†å›³ã«ã‚ˆã‚‹å¤šå¤‰é‡ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–</li>
                <li>âœ… PLSã«ã‚ˆã‚‹äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã¨å¤‰æ•°é‡è¦åº¦è©•ä¾¡ï¼ˆVIPï¼‰</li>
                <li>âœ… CCAã«ã‚ˆã‚‹å¤‰æ•°ç¾¤é–“ã®æ­£æº–ç›¸é–¢åˆ†æ</li>
                <li>âœ… å› å­åˆ†æã«ã‚ˆã‚‹æ½œåœ¨å› å­ã®æŠ½å‡ºã¨è§£é‡ˆ</li>
                <li>âœ… ICAã«ã‚ˆã‚‹ç‹¬ç«‹æˆåˆ†ã®åˆ†é›¢ã¨ç•°å¸¸æ¤œçŸ¥</li>
                <li>âœ… ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã«ã‚ˆã‚‹é«˜ç²¾åº¦å¤–ã‚Œå€¤æ¤œå‡º</li>
                <li>âœ… å¯„ä¸ãƒ—ãƒ­ãƒƒãƒˆã«ã‚ˆã‚‹ç•°å¸¸è¨ºæ–­ã¨æ ¹æœ¬åŸå› ç‰¹å®š</li>
                <li>âœ… å¤‰æ•°é¸æŠã¨é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®å®Ÿè£…</li>
                <li>âœ… æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹å³å¯†ãªãƒ¢ãƒ‡ãƒ«è©•ä¾¡</li>
            </ul>

            <div class="callout callout-info">
                <h4>ğŸ“š æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h4>
                <p>
                    ç¬¬3ç« ã§ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹äºˆæ¸¬ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚’å­¦ç¿’ã—ã¾ã™ã€‚
                    å›å¸°ãƒ¢ãƒ‡ãƒ«ã€åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ãªã©ã‚’ç”¨ã„ã¦ã€ãƒ—ãƒ­ã‚»ã‚¹å“è³ªäºˆæ¸¬ã¨ç•°å¸¸åˆ†é¡ã‚’é«˜ç²¾åº¦åŒ–ã—ã¾ã™ã€‚
                </p>
            </div>
        </section>

        <section>
            <h2>2.10 æ¼”ç¿’å•é¡Œ</h2>

            <h4>æ¼”ç¿’1ï¼ˆåŸºç¤ï¼‰: PCAã®è§£é‡ˆ</h4>
            <p>
                Example 1ã§å¾—ã‚‰ã‚ŒãŸç¬¬1ä¸»æˆåˆ†ï¼ˆPC1ï¼‰ã®ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‹ã‚‰ã€PC1ãŒè¡¨ç¾ã—ã¦ã„ã‚‹ç‰©ç†çš„æ„å‘³ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
                ã©ã®å¤‰æ•°ç¾¤ãŒPC1ã«å¼·ãå¯„ä¸ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ
            </p>

            <h4>æ¼”ç¿’2ï¼ˆä¸­ç´šï¼‰: TÂ²ã¨SPEã®ä½¿ã„åˆ†ã‘</h4>
            <p>
                ä»¥ä¸‹ã®ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾ã—ã¦ã€TÂ²çµ±è¨ˆé‡ã¨SPEçµ±è¨ˆé‡ã®ã©ã¡ã‚‰ãŒã‚ˆã‚Šæ•æ„Ÿã«åå¿œã™ã‚‹ã‹ã€ç†ç”±ã¨ã¨ã‚‚ã«èª¬æ˜ã—ã¦ãã ã•ã„ï¼š
            </p>
            <ul>
                <li>ãƒ‘ã‚¿ãƒ¼ãƒ³A: æ¸©åº¦ã¨åœ§åŠ›ãŒåŒæ™‚ã«ä¸Šæ˜‡ï¼ˆç›¸é–¢ã‚’ä¿ã£ãŸã¾ã¾ï¼‰</li>
                <li>ãƒ‘ã‚¿ãƒ¼ãƒ³B: æ¸©åº¦ã®ã¿ãŒä¸Šæ˜‡ï¼ˆä»–å¤‰æ•°ã¯æ­£å¸¸ï¼‰</li>
                <li>ãƒ‘ã‚¿ãƒ¼ãƒ³C: å…¨å¤‰æ•°ãŒå¾“æ¥ã¨ç•°ãªã‚‹ç›¸é–¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¤ºã™</li>
            </ul>

            <h4>æ¼”ç¿’3ï¼ˆä¸Šç´šï¼‰: PLSãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–</h4>
            <p>
                Example 3ã®PLSãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š
            </p>
            <ol>
                <li>VIPã‚¹ã‚³ã‚¢ã«åŸºã¥ã„ã¦å¤‰æ•°ã‚’çµã‚Šè¾¼ã¿ã€ç°¡ç•¥åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹</li>
                <li>å…ƒã®ãƒ¢ãƒ‡ãƒ«ã¨ç°¡ç•¥åŒ–ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬æ€§èƒ½ã‚’æ¯”è¼ƒã™ã‚‹</li>
                <li>ã©ã¡ã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿé‹ç”¨ã«æ¨å¥¨ã™ã‚‹ã‹ã€ç†ç”±ã¨ã¨ã‚‚ã«èª¬æ˜ã™ã‚‹</li>
            </ol>

            <div class="callout callout-tip">
                <h4>ğŸ’¡ ãƒ’ãƒ³ãƒˆ</h4>
                <p>
                    æ¼”ç¿’3ã§ã¯ã€VIP > 1.0ã®å¤‰æ•°ã®ã¿ã‚’ä½¿ç”¨ã—ãŸç°¡ç•¥åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¾ã—ã‚‡ã†ã€‚
                    æ€§èƒ½è©•ä¾¡ã«ã¯ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®RÂ²ã€MAEã€MAPEã«åŠ ãˆã¦ã€ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆæ€§ã‚„ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ€§ã‚‚è€ƒæ…®ã—ã¦ãã ã•ã„ã€‚
                    å®Ÿãƒ—ãƒ­ã‚»ã‚¹ã§ã¯ã€ç²¾åº¦ã ã‘ã§ãªãã€é‹ç”¨ã®ã—ã‚„ã™ã•ã‚‚é‡è¦ãªåˆ¤æ–­åŸºæº–ã§ã™ã€‚
                </p>
            </div>
        </section>

        <div class="nav-buttons">
            <a href="./chapter-1.html" class="btn">â† ç¬¬1ç« ã¸æˆ»ã‚‹</a>
            <a href="#" class="btn" style="background: #ccc; cursor: not-allowed;">ç¬¬3ç« ã¸é€²ã‚€ â†’</a>
        </div>

        <footer>
            <p>&copy; 2025 Yusuke Hashimoto, Tohoku University. All rights reserved.</p>
            <p style="margin-top: 0.5rem;">
                <a href="./index.html">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a> |
                <a href="../index.html">PIãƒŠãƒ¬ãƒƒã‚¸ãƒãƒ–TOP</a>
            </p>
        </footer>
    </div>

    <!-- Mermaid.js for diagrams -->
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({
            startOnLoad: true,
            theme: 'default'
        });
    </script>
</body>
</html>
