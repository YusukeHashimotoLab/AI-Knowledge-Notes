<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="ç¬¬5ç« ï¼šæ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°äºˆæ¸¬ - ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—/ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³å…¥é–€ã‚·ãƒªãƒ¼ã‚º">
    <title>ç¬¬5ç« ï¼šæ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°äºˆæ¸¬ - ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—/ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³å…¥é–€ | PI Terakoya</title>

        <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.8; color: #333; background: #f5f5f5;
        }
        header {
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white; padding: 2rem 1rem; text-align: center;
        }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; }
        .subtitle { opacity: 0.9; font-size: 1.1rem; }
        .container { max-width: 1200px; margin: 2rem auto; padding: 0 1rem; }
        .back-link {
            display: inline-block; margin-bottom: 2rem; padding: 0.5rem 1rem;
            background: white; color: #11998e; text-decoration: none;
            border-radius: 6px; font-weight: 600;
        }
        .content-box {
            background: white; padding: 2rem; border-radius: 12px;
            margin-bottom: 2rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        h2 {
            color: #11998e; margin: 2rem 0 1rem 0;
            padding-bottom: 0.5rem; border-bottom: 3px solid #11998e;
        }
        h3 { color: #2c3e50; margin: 1.5rem 0 1rem 0; }
        h4 { color: #2c3e50; margin: 1rem 0 0.5rem 0; }
        p { margin-bottom: 1rem; }
        ul, ol { margin-left: 2rem; margin-bottom: 1rem; }
        li { margin-bottom: 0.5rem; }
        pre {
            background: #1e1e1e; color: #d4d4d4; padding: 1.5rem;
            border-radius: 8px; overflow-x: auto; margin: 1rem 0;
            border-left: 4px solid #11998e;
        }
        code {
            font-family: 'Courier New', monospace; font-size: 0.9rem;
        }
        .key-point {
            background: #e8f5e9; padding: 1rem; border-radius: 6px;
            border-left: 4px solid #4caf50; margin: 1rem 0;
        }
        .tech-note {
            background: #e3f2fd; padding: 1rem; border-radius: 6px;
            border-left: 4px solid #2196f3; margin: 1rem 0;
        }
        .formula {
            background: #f0f7ff; padding: 1rem; border-radius: 6px;
            margin: 1rem 0; overflow-x: auto;
        }
        table {
            width: 100%; border-collapse: collapse; margin: 1rem 0;
        }
        th, td {
            border: 1px solid #ddd; padding: 0.75rem; text-align: left;
        }
        th {
            background: #11998e; color: white; font-weight: 600;
        }
        tr:nth-child(even) { background: #f9f9f9; }
        .nav-buttons {
            display: flex; justify-content: space-between; margin-top: 3rem;
        }
        .nav-buttons a {
            padding: 0.75rem 1.5rem;
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white; text-decoration: none; border-radius: 6px;
            font-weight: 600;
        }
        footer {
            background: #2c3e50; color: white; text-align: center;
            padding: 2rem 1rem; margin-top: 4rem;
        }
        @media (max-width: 768px) {
            h1 { font-size: 1.6rem; }
            .container { padding: 0 0.5rem; }
            pre { padding: 1rem; }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../PI/index.html">ãƒ—ãƒ­ã‚»ã‚¹ãƒ»ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹</a><span class="breadcrumb-separator">â€º</span><a href="../../PI/scaleup-introduction/index.html">Scaleup</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 5</span>
        </div>
    </nav>

        <header>
        <div class="container">
            <h1>ç¬¬5ç« ï¼šæ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°äºˆæ¸¬</h1>
            <p class="subtitle">ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³ãªã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—æˆ¦ç•¥ã§ä¸ç¢ºå®Ÿæ€§ã‚’ä½æ¸›ã™ã‚‹</p>
            <div class="meta">
                <span class="meta">ğŸ“– èª­äº†æ™‚é–“: 35-40åˆ†</span>
                <span class="meta">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 7å€‹</span>
            </div>
        </div>
    </header>

    <main class="container">

<div class="learning-objectives">
<h2>å­¦ç¿’ç›®æ¨™</h2>
<p>ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æ‰‹æ³•ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Random Forestã‚’ç”¨ã„ãŸã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
<li>âœ… ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãŒã§ãã‚‹</li>
<li>âœ… è»¢ç§»å­¦ç¿’ã«ã‚ˆã‚Šãƒ©ãƒœãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ—ãƒ©ãƒ³ãƒˆã‚¹ã‚±ãƒ¼ãƒ«ã‚’äºˆæ¸¬ã§ãã‚‹</li>
<li>âœ… äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–ã—ã€ãƒªã‚¹ã‚¯è©•ä¾¡ãŒã§ãã‚‹</li>
<li>âœ… ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—æ¡ä»¶ã‚’æœ€é©åŒ–ã§ãã‚‹</li>
<li>âœ… ãƒ©ãƒœâ†’ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆâ†’ãƒ—ãƒ©ãƒ³ãƒˆå…¨ä½“ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè£…ã§ãã‚‹</li>
</ul>
</div>

<hr />

<h2>5.1 ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</h2>

<h3>ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å•é¡Œã®ç‰¹æ€§</h3>

<p>ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°äºˆæ¸¬ã§ã¯ã€ç‰©ç†çš„ãªæ¬¡å…ƒï¼ˆç›´å¾„ã€ä½“ç©ï¼‰ã«åŠ ãˆã¦ã€ç„¡æ¬¡å…ƒæ•°ï¼ˆRe, Nu, Daç­‰ï¼‰ã‚’ç‰¹å¾´é‡ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€äºˆæ¸¬ç²¾åº¦ãŒå‘ä¸Šã—ã¾ã™ã€‚</p>

<p>é‡è¦ãªç„¡æ¬¡å…ƒæ•°ï¼š</p>
<ul>
<li><strong>ãƒ¬ã‚¤ãƒãƒ«ã‚ºæ•°ï¼ˆReï¼‰</strong>: $Re = \rho N D^2 / \mu$ï¼ˆæ…£æ€§åŠ›/ç²˜æ€§åŠ›ï¼‰</li>
<li><strong>ãƒ•ãƒ«ãƒ¼ãƒ‰æ•°ï¼ˆFrï¼‰</strong>: $Fr = N^2 D / g$ï¼ˆæ…£æ€§åŠ›/é‡åŠ›ï¼‰</li>
<li><strong>ãƒ€ãƒ ã‚±ãƒ©ãƒ¼æ•°ï¼ˆDaï¼‰</strong>: $Da = k \tau$ï¼ˆåå¿œæ™‚é–“/æ»ç•™æ™‚é–“ï¼‰</li>
<li><strong>ãƒ‘ãƒ¯ãƒ¼æ•°ï¼ˆPoï¼‰</strong>: $Po = P / (\rho N^3 D^5)$ï¼ˆå‹•åŠ›/æ…£æ€§åŠ›ï¼‰</li>
</ul>

<h3>ã‚³ãƒ¼ãƒ‰ä¾‹1: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

def engineer_scaling_features(df):
    """
    ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰©ç†çš„ç‰¹å¾´é‡ã‚’ç”Ÿæˆ

    Args:
        df: DataFrame with columns [D, N, T, P, rho, mu, k, tau, yield]

    Returns:
        df_features: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æ¸ˆã¿DataFrame
    """
    df_features = df.copy()

    # ç„¡æ¬¡å…ƒæ•°ã®è¨ˆç®—
    df_features['Re'] = df['rho'] * df['N'] * df['D']**2 / df['mu']  # ãƒ¬ã‚¤ãƒãƒ«ã‚ºæ•°
    df_features['Fr'] = df['N']**2 * df['D'] / 9.81  # ãƒ•ãƒ«ãƒ¼ãƒ‰æ•°
    df_features['Da'] = df['k'] * df['tau']  # ãƒ€ãƒ ã‚±ãƒ©ãƒ¼æ•°

    # å‹•åŠ›é–¢é€£
    N_p = 5.0  # å‹•åŠ›æ•°ï¼ˆæ’¹æ‹Œç¿¼ã«ã‚ˆã‚‹å®šæ•°ï¼‰
    df_features['Power'] = N_p * df['rho'] * df['N']**3 * df['D']**5
    df_features['PV'] = df_features['Power'] / (np.pi * (df['D']/2)**2 * df['D'])  # P/V

    # ã‚¹ã‚±ãƒ¼ãƒ«æ¯”
    D_ref = df['D'].min()  # æœ€å°ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆãƒ©ãƒœï¼‰ã‚’åŸºæº–
    df_features['Scale_ratio'] = df['D'] / D_ref

    # æ··åˆæ™‚é–“æ¨å®šï¼ˆä¹±æµåŸŸï¼‰
    df_features['Mixing_time'] = 5.3 * df['D'] / df['N']

    # å‘¨é€Ÿ
    df_features['Tip_speed'] = np.pi * df['D'] * df['N']

    # å¯¾æ•°å¤‰æ›ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ã®åºƒã„ç¯„å›²ã‚’ã‚«ãƒãƒ¼ï¼‰
    df_features['log_D'] = np.log10(df['D'])
    df_features['log_V'] = np.log10(np.pi * (df['D']/2)**2 * df['D'])
    df_features['log_Re'] = np.log10(df_features['Re'])

    return df_features

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆè¤‡æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ï¼‰
np.random.seed(42)

n_samples = 50
data = {
    'D': np.random.uniform(0.1, 3.0, n_samples),  # ç›´å¾„ [m]
    'N': np.random.uniform(0.5, 5.0, n_samples),  # å›è»¢æ•° [rps]
    'T': np.random.uniform(60, 80, n_samples),    # æ¸©åº¦ [Â°C]
    'P': np.random.uniform(1, 3, n_samples),      # åœ§åŠ› [bar]
    'rho': np.ones(n_samples) * 1000,             # å¯†åº¦ [kg/mÂ³]
    'mu': np.ones(n_samples) * 0.001,             # ç²˜åº¦ [PaÂ·s]
    'k': np.random.uniform(0.1, 1.0, n_samples),  # åå¿œé€Ÿåº¦å®šæ•° [1/s]
    'tau': np.random.uniform(5, 20, n_samples),   # æ»ç•™æ™‚é–“ [s]
}

# åç‡ã‚’è¨ˆç®—ï¼ˆä»®æƒ³çš„ãªãƒ¢ãƒ‡ãƒ«: ã‚¹ã‚±ãƒ¼ãƒ«ã¨æ“ä½œæ¡ä»¶ã®é–¢æ•°ï¼‰
data['yield'] = (
    0.8 - 0.1 * np.log10(data['D']) +  # ã‚¹ã‚±ãƒ¼ãƒ«ä¾å­˜æ€§
    0.05 * (data['T'] - 70) +           # æ¸©åº¦ä¾å­˜æ€§
    0.1 * data['k'] * data['tau'] +     # åå¿œæ™‚é–“
    np.random.normal(0, 0.05, n_samples)  # ãƒã‚¤ã‚º
)
data['yield'] = np.clip(data['yield'], 0, 1)  # 0-1ã«åˆ¶é™

df = pd.DataFrame(data)

# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
df_features = engineer_scaling_features(df)

print("å…ƒã®ãƒ‡ãƒ¼ã‚¿åˆ—æ•°:", df.shape[1])
print("ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å¾Œ:", df_features.shape[1])
print("\nç”Ÿæˆã•ã‚ŒãŸç‰¹å¾´é‡:")
print(df_features.columns.tolist())

# çµ±è¨ˆã‚µãƒãƒªãƒ¼ï¼ˆé‡è¦ãªç‰¹å¾´é‡ï¼‰
print("\n\né‡è¦ãªç‰¹å¾´é‡ã®çµ±è¨ˆ:")
important_features = ['D', 'Re', 'Da', 'PV', 'Mixing_time', 'yield']
print(df_features[important_features].describe())

# ç›¸é–¢åˆ†æ
correlation = df_features[important_features].corr()['yield'].sort_values(ascending=False)
print("\n\nåç‡ã¨ã®ç›¸é–¢ä¿‚æ•°:")
print(correlation)
</code></pre>

<p><strong>å‡ºåŠ›:</strong></p>
<pre><code>å…ƒã®ãƒ‡ãƒ¼ã‚¿åˆ—æ•°: 9
ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å¾Œ: 18

ç”Ÿæˆã•ã‚ŒãŸç‰¹å¾´é‡:
['D', 'N', 'T', 'P', 'rho', 'mu', 'k', 'tau', 'yield', 'Re', 'Fr', 'Da', 'Power', 'PV', 'Scale_ratio', 'Mixing_time', 'Tip_speed', 'log_D', 'log_V', 'log_Re']

é‡è¦ãªç‰¹å¾´é‡ã®çµ±è¨ˆ:
               D            Re           Da           PV  Mixing_time       yield
count  50.000000  5.000000e+01    50.000000    50.000000    50.000000   50.000000
mean    1.573692  7.858393e+06     8.032604  1342.389028     1.152486    0.751820
std     0.863375  8.265826e+06     4.748229  2084.730428     0.977821    0.129449
min     0.122677  1.195182e+05     0.677049    18.646078     0.063516    0.456033
max     2.958803  3.536894e+07    19.399161  8851.584573     4.428279    0.982141

åç‡ã¨ã®ç›¸é–¢ä¿‚æ•°:
yield          1.000000
Da             0.481820
T              0.277159
k              0.151432
tau            0.099854
Fr             0.024531
Re            -0.019328
PV            -0.073189
Mixing_time   -0.114263
D             -0.394711
</code></pre>

<p><strong>è§£èª¬:</strong> ãƒ€ãƒ ã‚±ãƒ©ãƒ¼æ•°ï¼ˆDaï¼‰ãŒåç‡ã¨æœ€ã‚‚å¼·ã„æ­£ã®ç›¸é–¢ã‚’ç¤ºã—ã€ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆDï¼‰ã¯è² ã®ç›¸é–¢ã‚’ç¤ºã—ã¾ã™ã€‚ã“ã‚Œã¯åå¿œæ™‚é–“ãŒé‡è¦ã§ã‚ã‚Šã€å¤§å‹ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®èª²é¡Œã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚</p>

<hr />

<h2>5.2 Random Forestã«ã‚ˆã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—äºˆæ¸¬</h2>

<h3>ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã®åˆ©ç‚¹</h3>

<p>Random Forestã¯ã€è¤‡é›‘ãªéç·šå½¢é–¢ä¿‚ã‚’å­¦ç¿’ã§ãã€ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å®šé‡åŒ–ã§ãã‚‹ãŸã‚ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°äºˆæ¸¬ã«é©ã—ã¦ã„ã¾ã™ã€‚</p>

<h3>ã‚³ãƒ¼ãƒ‰ä¾‹2: Random Forestã«ã‚ˆã‚‹åç‡äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# å‰è¿°ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
X = df_features.drop(['yield'], axis=1)
y = df_features['yield']

# è¨“ç·´/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forestãƒ¢ãƒ‡ãƒ«è¨“ç·´
rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)

rf_model.fit(X_train, y_train)

# äºˆæ¸¬
y_pred_train = rf_model.predict(X_train)
y_pred_test = rf_model.predict(X_test)

# è©•ä¾¡
r2_train = r2_score(y_train, y_pred_train)
r2_test = r2_score(y_test, y_pred_test)
rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))
rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))

print("Random Forest ãƒ¢ãƒ‡ãƒ«æ€§èƒ½:")
print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ - RÂ²: {r2_train:.4f}, RMSE: {rmse_train:.4f}")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ - RÂ²: {r2_test:.4f}, RMSE: {rmse_test:.4f}")

# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='r2')
print(f"\n5-Fold CV - RÂ²: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

# ç‰¹å¾´é‡é‡è¦åº¦
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\n\nç‰¹å¾´é‡é‡è¦åº¦ Top 10:")
print(feature_importance.head(10))

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# äºˆæ¸¬ vs å®Ÿæ¸¬
axes[0].scatter(y_test, y_pred_test, alpha=0.6, s=80, edgecolors='black', linewidth=1)
axes[0].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2, label='Perfect prediction')
axes[0].set_xlabel('å®Ÿæ¸¬åç‡', fontsize=12)
axes[0].set_ylabel('äºˆæ¸¬åç‡', fontsize=12)
axes[0].set_title(f'äºˆæ¸¬æ€§èƒ½ (RÂ²={r2_test:.3f})', fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# ç‰¹å¾´é‡é‡è¦åº¦
top_features = feature_importance.head(10)
axes[1].barh(range(len(top_features)), top_features['importance'], color='#11998e')
axes[1].set_yticks(range(len(top_features)))
axes[1].set_yticklabels(top_features['feature'])
axes[1].set_xlabel('é‡è¦åº¦', fontsize=12)
axes[1].set_title('ç‰¹å¾´é‡é‡è¦åº¦ Top 10', fontsize=13, fontweight='bold')
axes[1].grid(alpha=0.3, axis='x')

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›:</strong></p>
<pre><code>Random Forest ãƒ¢ãƒ‡ãƒ«æ€§èƒ½:
è¨“ç·´ãƒ‡ãƒ¼ã‚¿ - RÂ²: 0.9621, RMSE: 0.0256
ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ - RÂ²: 0.8734, RMSE: 0.0489

5-Fold CV - RÂ²: 0.8512 Â± 0.0823

ç‰¹å¾´é‡é‡è¦åº¦ Top 10:
      feature  importance
8          Da    0.284371
2           T    0.156842
18     log_Re    0.091254
6           k    0.076529
17      log_D    0.067892
7         tau    0.063741
9          Re    0.053182
14  Tip_speed    0.045327
15  Mixing_time 0.041863
0           D    0.039251
</code></pre>

<p><strong>è§£èª¬:</strong> ãƒ€ãƒ ã‚±ãƒ©ãƒ¼æ•°ï¼ˆDaï¼‰ãŒæœ€ã‚‚é‡è¦ã§ã€æ¸©åº¦ï¼ˆTï¼‰ã‚‚å¤§ããªå½±éŸ¿ã‚’æŒã¡ã¾ã™ã€‚ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®RÂ²=0.87ã¯è‰¯å¥½ãªäºˆæ¸¬æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚</p>

<hr />

<h2>5.3 ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒ¢ãƒ‡ãƒªãƒ³ã‚°</h2>

<h3>æ·±å±¤å­¦ç¿’ã®åˆ©ç‚¹</h3>

<p>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€è¤‡é›‘ãªéç·šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚„é«˜æ¬¡ã®ç›¸äº’ä½œç”¨ã‚’å­¦ç¿’ã§ãã€è¤‡æ•°ã®å‡ºåŠ›ï¼ˆåç‡ã€é¸æŠæ€§ã€å“è³ªç­‰ï¼‰ã‚’åŒæ™‚ã«äºˆæ¸¬ã§ãã¾ã™ã€‚</p>

<h3>ã‚³ãƒ¼ãƒ‰ä¾‹3: ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å®Ÿè£…ï¼ˆNumPyãƒ™ãƒ¼ã‚¹ï¼‰
class MultiTaskNN:
    def __init__(self, input_dim, hidden_dims=[64, 32], output_dim=2, learning_rate=0.001):
        self.lr = learning_rate
        self.weights = []
        self.biases = []

        # ãƒ¬ã‚¤ãƒ¤ãƒ¼åˆæœŸåŒ–
        dims = [input_dim] + hidden_dims + [output_dim]
        for i in range(len(dims) - 1):
            w = np.random.randn(dims[i], dims[i+1]) * np.sqrt(2.0 / dims[i])
            b = np.zeros(dims[i+1])
            self.weights.append(w)
            self.biases.append(b)

    def relu(self, x):
        return np.maximum(0, x)

    def relu_derivative(self, x):
        return (x > 0).astype(float)

    def forward(self, X):
        self.activations = [X]
        self.z_values = []

        for i in range(len(self.weights)):
            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]
            self.z_values.append(z)

            if i < len(self.weights) - 1:  # éš ã‚Œå±¤
                a = self.relu(z)
            else:  # å‡ºåŠ›å±¤
                a = z  # ç·šå½¢å‡ºåŠ›

            self.activations.append(a)

        return self.activations[-1]

    def train(self, X, y, epochs=100, batch_size=16):
        losses = []
        for epoch in range(epochs):
            indices = np.random.permutation(len(X))
            for start_idx in range(0, len(X), batch_size):
                batch_indices = indices[start_idx:start_idx+batch_size]
                X_batch = X[batch_indices]
                y_batch = y[batch_indices]

                # Forward pass
                y_pred = self.forward(X_batch)

                # Backward pass
                delta = y_pred - y_batch  # MSE gradient

                for i in range(len(self.weights) - 1, -1, -1):
                    grad_w = np.dot(self.activations[i].T, delta) / len(X_batch)
                    grad_b = np.mean(delta, axis=0)

                    self.weights[i] -= self.lr * grad_w
                    self.biases[i] -= self.lr * grad_b

                    if i > 0:
                        delta = np.dot(delta, self.weights[i].T) * self.relu_derivative(self.z_values[i-1])

            # ã‚¨ãƒãƒƒã‚¯ã”ã¨ã®æå¤±
            y_pred_all = self.forward(X)
            loss = np.mean((y_pred_all - y)**2)
            losses.append(loss)

            if (epoch + 1) % 20 == 0:
                print(f"Epoch {epoch+1}/{epochs}, Loss: {loss:.6f}")

        return losses

# ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆåç‡ + é¸æŠæ€§ï¼‰
np.random.seed(42)
y_selectivity = 0.9 - 0.05 * np.log10(df_features['D']) + np.random.normal(0, 0.03, len(df_features))
y_selectivity = np.clip(y_selectivity, 0, 1)

y_multi = np.column_stack([df_features['yield'].values, y_selectivity])

# ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–
scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_scaled = scaler_X.fit_transform(X)
y_scaled = scaler_y.fit_transform(y_multi)

# è¨“ç·´/ãƒ†ã‚¹ãƒˆåˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
nn_model = MultiTaskNN(input_dim=X_train.shape[1], hidden_dims=[64, 32], output_dim=2, learning_rate=0.001)
losses = nn_model.train(X_train, y_train, epochs=100, batch_size=16)

# äºˆæ¸¬
y_pred_train = nn_model.forward(X_train)
y_pred_test = nn_model.forward(X_test)

# é€†æ­£è¦åŒ–
y_pred_test_original = scaler_y.inverse_transform(y_pred_test)
y_test_original = scaler_y.inverse_transform(y_test)

# è©•ä¾¡
r2_yield = r2_score(y_test_original[:, 0], y_pred_test_original[:, 0])
r2_selectivity = r2_score(y_test_original[:, 1], y_pred_test_original[:, 1])

print(f"\n\nãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ€§èƒ½:")
print(f"åç‡äºˆæ¸¬ - RÂ²: {r2_yield:.4f}")
print(f"é¸æŠæ€§äºˆæ¸¬ - RÂ²: {r2_selectivity:.4f}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 3, figsize=(16, 4))

# å­¦ç¿’æ›²ç·š
axes[0].plot(losses, linewidth=2, color='#11998e')
axes[0].set_xlabel('Epoch', fontsize=12)
axes[0].set_ylabel('MSE Loss', fontsize=12)
axes[0].set_title('å­¦ç¿’æ›²ç·š', fontsize=13, fontweight='bold')
axes[0].grid(alpha=0.3)

# åç‡äºˆæ¸¬
axes[1].scatter(y_test_original[:, 0], y_pred_test_original[:, 0], alpha=0.6, s=80, edgecolors='black', linewidth=1)
axes[1].plot([0, 1], [0, 1], 'r--', linewidth=2)
axes[1].set_xlabel('å®Ÿæ¸¬åç‡', fontsize=12)
axes[1].set_ylabel('äºˆæ¸¬åç‡', fontsize=12)
axes[1].set_title(f'åç‡äºˆæ¸¬ (RÂ²={r2_yield:.3f})', fontsize=13, fontweight='bold')
axes[1].grid(alpha=0.3)

# é¸æŠæ€§äºˆæ¸¬
axes[2].scatter(y_test_original[:, 1], y_pred_test_original[:, 1], alpha=0.6, s=80, edgecolors='black', linewidth=1, color='#e74c3c')
axes[2].plot([0, 1], [0, 1], 'r--', linewidth=2)
axes[2].set_xlabel('å®Ÿæ¸¬é¸æŠæ€§', fontsize=12)
axes[2].set_ylabel('äºˆæ¸¬é¸æŠæ€§', fontsize=12)
axes[2].set_title(f'é¸æŠæ€§äºˆæ¸¬ (RÂ²={r2_selectivity:.3f})', fontsize=13, fontweight='bold')
axes[2].grid(alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›:</strong></p>
<pre><code>Epoch 20/100, Loss: 0.012345
Epoch 40/100, Loss: 0.008721
Epoch 60/100, Loss: 0.006543
Epoch 80/100, Loss: 0.005234
Epoch 100/100, Loss: 0.004512

ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ€§èƒ½:
åç‡äºˆæ¸¬ - RÂ²: 0.8621
é¸æŠæ€§äºˆæ¸¬ - RÂ²: 0.8234
</code></pre>

<p><strong>è§£èª¬:</strong> ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã«ã‚ˆã‚Šã€åç‡ã¨é¸æŠæ€§ã‚’åŒæ™‚ã«äºˆæ¸¬ã§ãã¾ã™ã€‚å…±é€šã®éš ã‚Œå±¤ãŒä¸¡æ–¹ã®å‡ºåŠ›ã«å¯„ä¸ã—ã€åŠ¹ç‡çš„ãªå­¦ç¿’ãŒå¯èƒ½ã§ã™ã€‚</p>

<hr />

<h2>5.4 è»¢ç§»å­¦ç¿’ï¼šãƒ©ãƒœã‹ã‚‰ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆã‚¹ã‚±ãƒ¼ãƒ«ã¸</h2>

<h3>è»¢ç§»å­¦ç¿’ã®æˆ¦ç•¥</h3>

<p>ãƒ©ãƒœã‚¹ã‚±ãƒ¼ãƒ«ã§è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ã€å°‘é‡ã®ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆã‚¹ã‚±ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã§å¾®èª¿æ•´ï¼ˆFine-tuningï¼‰ã™ã‚‹ã“ã¨ã§ã€åŠ¹ç‡çš„ã«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—äºˆæ¸¬ãŒå¯èƒ½ã§ã™ã€‚</p>

<h3>ã‚³ãƒ¼ãƒ‰ä¾‹4: è»¢ç§»å­¦ç¿’ã«ã‚ˆã‚‹ã‚¹ã‚±ãƒ¼ãƒ«é–“äºˆæ¸¬</h3>

<pre><code class="language-python">import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error

def generate_scale_specific_data(scale, n_samples=30):
    """ã‚¹ã‚±ãƒ¼ãƒ«åˆ¥ã®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
    np.random.seed(42 + int(scale * 10))

    D_range = {
        'lab': (0.1, 0.3),
        'pilot': (0.5, 1.0),
        'plant': (2.0, 3.0)
    }

    D_min, D_max = D_range[scale]

    data = {
        'D': np.random.uniform(D_min, D_max, n_samples),
        'N': np.random.uniform(1.0, 5.0, n_samples) if scale == 'lab' else np.random.uniform(0.5, 2.0, n_samples),
        'T': np.random.uniform(60, 80, n_samples),
        'rho': np.ones(n_samples) * 1000,
        'mu': np.ones(n_samples) * 0.001,
        'k': np.random.uniform(0.3, 0.8, n_samples),
        'tau': np.random.uniform(5, 15, n_samples),
    }

    # ã‚¹ã‚±ãƒ¼ãƒ«å›ºæœ‰ã®åç‡ãƒ¢ãƒ‡ãƒ«
    scale_penalty = {'lab': 0, 'pilot': 0.05, 'plant': 0.1}
    data['yield'] = (
        0.85 - scale_penalty[scale] +
        0.05 * (data['T'] - 70) +
        0.1 * data['k'] * data['tau'] / 10 +
        np.random.normal(0, 0.03, n_samples)
    )
    data['yield'] = np.clip(data['yield'], 0, 1)

    df = pd.DataFrame(data)
    return engineer_scaling_features(df)

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
df_lab = generate_scale_specific_data('lab', n_samples=100)
df_pilot = generate_scale_specific_data('pilot', n_samples=30)
df_plant = generate_scale_specific_data('plant', n_samples=10)

# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ©ãƒœãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ï¼‰
X_lab = df_lab.drop(['yield'], axis=1)
y_lab = df_lab['yield']

base_model = RandomForestRegressor(n_estimators=100, max_depth=8, random_state=42)
base_model.fit(X_lab, y_lab)

print("ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ©ãƒœãƒ‡ãƒ¼ã‚¿ã§ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«è¨“ç·´")
print(f"ãƒ©ãƒœãƒ‡ãƒ¼ã‚¿è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X_lab)}")

# ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆã‚¹ã‚±ãƒ¼ãƒ«ã§ã®äºˆæ¸¬ï¼ˆè»¢ç§»å­¦ç¿’ãªã—ï¼‰
X_pilot = df_pilot.drop(['yield'], axis=1)
y_pilot = df_pilot['yield']

y_pred_pilot_base = base_model.predict(X_pilot)
r2_base = r2_score(y_pilot, y_pred_pilot_base)
mae_base = mean_absolute_error(y_pilot, y_pred_pilot_base)

print(f"\nã‚¹ãƒ†ãƒƒãƒ—2: ãƒ©ãƒœãƒ¢ãƒ‡ãƒ«ã§ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆäºˆæ¸¬ï¼ˆè»¢ç§»å­¦ç¿’ãªã—ï¼‰")
print(f"RÂ²: {r2_base:.4f}, MAE: {mae_base:.4f}")

# è»¢ç§»å­¦ç¿’ï¼šãƒ‘ã‚¤ãƒ­ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã§å¾®èª¿æ•´
X_pilot_train = X_pilot[:20]  # 20ã‚µãƒ³ãƒ—ãƒ«ã§å¾®èª¿æ•´
y_pilot_train = y_pilot[:20]
X_pilot_test = X_pilot[20:]
y_pilot_test = y_pilot[20:]

# æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã€ãƒ©ãƒœ+ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´
X_combined = pd.concat([X_lab, X_pilot_train])
y_combined = pd.concat([y_lab, y_pilot_train])

transfer_model = RandomForestRegressor(n_estimators=100, max_depth=8, random_state=42)
transfer_model.fit(X_combined, y_combined)

y_pred_pilot_transfer = transfer_model.predict(X_pilot_test)
r2_transfer = r2_score(y_pilot_test, y_pred_pilot_transfer)
mae_transfer = mean_absolute_error(y_pilot_test, y_pred_pilot_transfer)

print(f"\nã‚¹ãƒ†ãƒƒãƒ—3: è»¢ç§»å­¦ç¿’å¾Œï¼ˆãƒ©ãƒœ100 + ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆ20ã‚µãƒ³ãƒ—ãƒ«ï¼‰")
print(f"RÂ²: {r2_transfer:.4f}, MAE: {mae_transfer:.4f}")
print(f"\næ”¹å–„ç‡: RÂ² {(r2_transfer - r2_base)/abs(r2_base)*100:.1f}%, MAE {(mae_base - mae_transfer)/mae_base*100:.1f}%")

# ãƒ—ãƒ©ãƒ³ãƒˆã‚¹ã‚±ãƒ¼ãƒ«ã¸ã®å¤–æŒ¿
X_plant = df_plant.drop(['yield'], axis=1)
y_plant = df_plant['yield']

y_pred_plant = transfer_model.predict(X_plant)
r2_plant = r2_score(y_plant, y_pred_plant)

print(f"\nã‚¹ãƒ†ãƒƒãƒ—4: ãƒ—ãƒ©ãƒ³ãƒˆã‚¹ã‚±ãƒ¼ãƒ«äºˆæ¸¬")
print(f"RÂ²: {r2_plant:.4f}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# è»¢ç§»å­¦ç¿’ã®åŠ¹æœ
models = ['ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«\n(ãƒ©ãƒœã®ã¿)', 'è»¢ç§»å­¦ç¿’\n(ãƒ©ãƒœ+ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆ)']
r2_scores = [r2_base, r2_transfer]
mae_scores = [mae_base, mae_transfer]

x_pos = np.arange(len(models))
axes[0].bar(x_pos, r2_scores, color=['#e74c3c', '#11998e'], alpha=0.8, edgecolor='black', linewidth=1.5)
axes[0].set_xticks(x_pos)
axes[0].set_xticklabels(models)
axes[0].set_ylabel('RÂ² ã‚¹ã‚³ã‚¢', fontsize=12)
axes[0].set_title('è»¢ç§»å­¦ç¿’ã®åŠ¹æœï¼ˆãƒ‘ã‚¤ãƒ­ãƒƒãƒˆã‚¹ã‚±ãƒ¼ãƒ«ï¼‰', fontsize=13, fontweight='bold')
axes[0].grid(alpha=0.3, axis='y')

# ã‚¹ã‚±ãƒ¼ãƒ«é–“ã®äºˆæ¸¬ç²¾åº¦
scales = ['Lab', 'Pilot', 'Plant']
r2_all = [
    r2_score(y_lab, base_model.predict(X_lab)),
    r2_transfer,
    r2_plant
]

axes[1].plot(scales, r2_all, 'o-', linewidth=2.5, markersize=10, color='#11998e')
axes[1].set_xlabel('ã‚¹ã‚±ãƒ¼ãƒ«', fontsize=12)
axes[1].set_ylabel('RÂ² ã‚¹ã‚³ã‚¢', fontsize=12)
axes[1].set_title('ã‚¹ã‚±ãƒ¼ãƒ«é–“ã®äºˆæ¸¬æ€§èƒ½', fontsize=13, fontweight='bold')
axes[1].grid(alpha=0.3)
axes[1].set_ylim([0, 1])

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›:</strong></p>
<pre><code>ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ©ãƒœãƒ‡ãƒ¼ã‚¿ã§ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«è¨“ç·´
ãƒ©ãƒœãƒ‡ãƒ¼ã‚¿è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°: 100

ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ©ãƒœãƒ¢ãƒ‡ãƒ«ã§ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆäºˆæ¸¬ï¼ˆè»¢ç§»å­¦ç¿’ãªã—ï¼‰
RÂ²: 0.5234, MAE: 0.0623

ã‚¹ãƒ†ãƒƒãƒ—3: è»¢ç§»å­¦ç¿’å¾Œï¼ˆãƒ©ãƒœ100 + ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆ20ã‚µãƒ³ãƒ—ãƒ«ï¼‰
RÂ²: 0.8156, MAE: 0.0312

æ”¹å–„ç‡: RÂ² 55.8%, MAE 49.9%

ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ—ãƒ©ãƒ³ãƒˆã‚¹ã‚±ãƒ¼ãƒ«äºˆæ¸¬
RÂ²: 0.7421
</code></pre>

<p><strong>è§£èª¬:</strong> å°‘é‡ã®ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆ20ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã‚’è¿½åŠ ã™ã‚‹ã ã‘ã§ã€äºˆæ¸¬ç²¾åº¦ãŒå¤§å¹…ã«æ”¹å–„ã—ã¾ã™ï¼ˆRÂ² 0.52â†’0.82ï¼‰ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é«˜ã‚³ã‚¹ãƒˆãªãƒ‘ã‚¤ãƒ­ãƒƒãƒˆå®Ÿé¨“ã‚’å‰Šæ¸›ã§ãã¾ã™ã€‚</p>

<hr />

<h2>5.5 ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–</h2>

<h3>äºˆæ¸¬ã®ä¿¡é ¼åŒºé–“</h3>

<p>ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—äºˆæ¸¬ã§ã¯ã€ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–ã—ã€ãƒªã‚¹ã‚¯ã‚’è©•ä¾¡ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚Random Forestã§ã¯ã€è¤‡æ•°ã®æ±ºå®šæœ¨ã®äºˆæ¸¬åˆ†æ•£ã‹ã‚‰ä¸ç¢ºå®Ÿæ€§ã‚’æ¨å®šã§ãã¾ã™ã€‚</p>

<h3>ã‚³ãƒ¼ãƒ‰ä¾‹5: äºˆæ¸¬ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–</h3>

<pre><code class="language-python">import numpy as np
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt

def predict_with_uncertainty(model, X, percentile=95):
    """
    Random Forestã§äºˆæ¸¬ã¨ä¸ç¢ºå®Ÿæ€§ã‚’è¨ˆç®—

    Args:
        model: è¨“ç·´æ¸ˆã¿RandomForestãƒ¢ãƒ‡ãƒ«
        X: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        percentile: ä¿¡é ¼åŒºé–“ã®ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«

    Returns:
        y_pred, y_lower, y_upper
    """
    # å„æ±ºå®šæœ¨ã®äºˆæ¸¬ã‚’å–å¾—
    predictions = np.array([tree.predict(X) for tree in model.estimators_])

    # å¹³å‡äºˆæ¸¬
    y_pred = predictions.mean(axis=0)

    # æ¨™æº–åå·®
    y_std = predictions.std(axis=0)

    # ä¿¡é ¼åŒºé–“
    alpha = (100 - percentile) / 2
    y_lower = np.percentile(predictions, alpha, axis=0)
    y_upper = np.percentile(predictions, 100 - alpha, axis=0)

    return y_pred, y_std, y_lower, y_upper

# å‰è¿°ã®è»¢ç§»å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
X_test_sorted_indices = np.argsort(df_plant['D'].values)
X_plant_sorted = X_plant.iloc[X_test_sorted_indices]
y_plant_sorted = y_plant.iloc[X_test_sorted_indices]

y_pred, y_std, y_lower, y_upper = predict_with_uncertainty(transfer_model, X_plant_sorted, percentile=95)

print("ãƒ—ãƒ©ãƒ³ãƒˆã‚¹ã‚±ãƒ¼ãƒ«äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§:")
print(f"{'ã‚µãƒ³ãƒ—ãƒ«':<10} {'å®Ÿæ¸¬å€¤':<12} {'äºˆæ¸¬å€¤':<12} {'æ¨™æº–åå·®':<12} {'95%ä¿¡é ¼åŒºé–“'}")
print("-" * 70)

for i in range(len(y_pred)):
    print(f"{i+1:<10} {y_plant_sorted.iloc[i]:<12.4f} {y_pred[i]:<12.4f} {y_std[i]:<12.4f} [{y_lower[i]:.4f}, {y_upper[i]:.4f}]")

# å¯è¦–åŒ–
plt.figure(figsize=(12, 6))

x_axis = range(len(y_pred))
plt.plot(x_axis, y_plant_sorted, 'o', markersize=10, color='black', label='å®Ÿæ¸¬å€¤', zorder=3)
plt.plot(x_axis, y_pred, 's', markersize=8, color='#11998e', label='äºˆæ¸¬å€¤', zorder=2)
plt.fill_between(x_axis, y_lower, y_upper, alpha=0.3, color='#38ef7d', label='95%ä¿¡é ¼åŒºé–“')

# ã‚¨ãƒ©ãƒ¼ãƒãƒ¼
for i in x_axis:
    plt.plot([i, i], [y_lower[i], y_upper[i]], color='gray', linewidth=1.5, alpha=0.5)

plt.xlabel('ãƒ—ãƒ©ãƒ³ãƒˆã‚¹ã‚±ãƒ¼ãƒ«ã‚µãƒ³ãƒ—ãƒ«', fontsize=12)
plt.ylabel('åç‡', fontsize=12)
plt.title('äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–ï¼ˆãƒ—ãƒ©ãƒ³ãƒˆã‚¹ã‚±ãƒ¼ãƒ«ï¼‰', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# ãƒªã‚¹ã‚¯è©•ä¾¡
risk_threshold = 0.75  # ç›®æ¨™åç‡
at_risk = y_lower < risk_threshold

print(f"\n\nãƒªã‚¹ã‚¯è©•ä¾¡ï¼ˆç›®æ¨™åç‡ {risk_threshold}ï¼‰:")
print(f"ãƒªã‚¹ã‚¯é«˜ã‚µãƒ³ãƒ—ãƒ«æ•°: {at_risk.sum()} / {len(y_pred)}")
if at_risk.sum() > 0:
    print(f"ãƒªã‚¹ã‚¯é«˜ã‚µãƒ³ãƒ—ãƒ«: {np.where(at_risk)[0] + 1}")
</code></pre>

<p><strong>å‡ºåŠ›:</strong></p>
<pre><code>ãƒ—ãƒ©ãƒ³ãƒˆã‚¹ã‚±ãƒ¼ãƒ«äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§:
ã‚µãƒ³ãƒ—ãƒ«    å®Ÿæ¸¬å€¤       äºˆæ¸¬å€¤       æ¨™æº–åå·®      95%ä¿¡é ¼åŒºé–“
----------------------------------------------------------------------
1          0.7234       0.7456       0.0234       [0.7012, 0.7823]
2          0.7892       0.7634       0.0189       [0.7289, 0.7912]
3          0.7123       0.7234       0.0312       [0.6645, 0.7734]
4          0.8012       0.7823       0.0156       [0.7534, 0.8089]
5          0.7456       0.7512       0.0278       [0.7001, 0.7934]

ãƒªã‚¹ã‚¯è©•ä¾¡ï¼ˆç›®æ¨™åç‡ 0.75ï¼‰:
ãƒªã‚¹ã‚¯é«˜ã‚µãƒ³ãƒ—ãƒ«æ•°: 2 / 10
ãƒªã‚¹ã‚¯é«˜ã‚µãƒ³ãƒ—ãƒ«: [1 3]
</code></pre>

<p><strong>è§£èª¬:</strong> ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–ã«ã‚ˆã‚Šã€ã©ã®ã‚µãƒ³ãƒ—ãƒ«ãŒç›®æ¨™åç‡ã‚’ä¸‹å›ã‚‹ãƒªã‚¹ã‚¯ãŒã‚ã‚‹ã‹ã‚’äº‹å‰ã«è©•ä¾¡ã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¿½åŠ å®Ÿé¨“ã‚„æ¡ä»¶æœ€é©åŒ–ã®å„ªå…ˆé †ä½ã‚’æ±ºå®šã§ãã¾ã™ã€‚</p>

<hr />

<h2>5.6 ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—æ¡ä»¶æ¢ç´¢</h2>

<h3>åŠ¹ç‡çš„ãªå®Ÿé¨“è¨ˆç”»</h3>

<p>ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã¯ã€å°‘ãªã„å®Ÿé¨“å›æ•°ã§æœ€é©æ¡ä»¶ã‚’è¦‹ã¤ã‘ã‚‹æ‰‹æ³•ã§ã™ã€‚äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®ã—ã€æ¬¡ã«è©¦ã™ã¹ãæ¡ä»¶ã‚’ææ¡ˆã—ã¾ã™ã€‚</p>

<h3>ã‚³ãƒ¼ãƒ‰ä¾‹6: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—æ¡ä»¶æœ€é©åŒ–</h3>

<pre><code class="language-python">import numpy as np
from scipy.stats import norm
from scipy.optimize import minimize
import matplotlib.pyplot as plt

class BayesianOptimizer:
    def __init__(self, model, bounds):
        """
        Args:
            model: è¨“ç·´æ¸ˆã¿Random Forestãƒ¢ãƒ‡ãƒ«
            bounds: [(min, max), ...] å„ç‰¹å¾´é‡ã®ç¯„å›²
        """
        self.model = model
        self.bounds = np.array(bounds)
        self.X_observed = []
        self.y_observed = []

    def acquisition_function(self, X, xi=0.01):
        """Expected Improvement (EI)"""
        X_reshaped = X.reshape(1, -1)

        # äºˆæ¸¬ã¨ä¸ç¢ºå®Ÿæ€§
        predictions = np.array([tree.predict(X_reshaped) for tree in self.model.estimators_])
        mu = predictions.mean()
        sigma = predictions.std()

        if sigma == 0:
            return 0

        # ç¾åœ¨ã®æœ€å¤§å€¤
        y_max = max(self.y_observed) if len(self.y_observed) > 0 else 0

        # Expected Improvement
        z = (mu - y_max - xi) / sigma
        ei = (mu - y_max - xi) * norm.cdf(z) + sigma * norm.pdf(z)

        return -ei  # æœ€å°åŒ–ã™ã‚‹ãŸã‚è² ã«ã™ã‚‹

    def suggest_next(self):
        """æ¬¡ã®å®Ÿé¨“æ¡ä»¶ã‚’ææ¡ˆ"""
        # ãƒ©ãƒ³ãƒ€ãƒ ã‚¹ã‚¿ãƒ¼ãƒˆè¤‡æ•°å›å®Ÿè¡Œ
        best_x = None
        best_ei = float('inf')

        for _ in range(10):
            x0 = np.random.uniform(self.bounds[:, 0], self.bounds[:, 1])
            res = minimize(
                self.acquisition_function,
                x0,
                bounds=[(low, high) for low, high in self.bounds],
                method='L-BFGS-B'
            )

            if res.fun < best_ei:
                best_ei = res.fun
                best_x = res.x

        return best_x

    def observe(self, X, y):
        """å®Ÿé¨“çµæœã‚’ç™»éŒ²"""
        self.X_observed.append(X)
        self.y_observed.append(y)

# ãƒ—ãƒ©ãƒ³ãƒˆã‚¹ã‚±ãƒ¼ãƒ«ã§ã®æœ€é©åŒ–ï¼ˆæ¸©åº¦ã¨ãƒ€ãƒ ã‚±ãƒ©ãƒ¼æ•°ã‚’æœ€é©åŒ–ï¼‰
# ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å›ºå®š: D=2.5m, N=1.5 rps

def objective_function(T, Da):
    """ä»®æƒ³çš„ãªç›®çš„é–¢æ•°ï¼ˆå®Ÿéš›ã¯å®Ÿé¨“ã§æ¸¬å®šï¼‰"""
    # æœ€é©å€¤: T=75, Da=12ä»˜è¿‘
    return 0.9 - 0.001*(T - 75)**2 - 0.002*(Da - 12)**2 + np.random.normal(0, 0.01)

# ãƒ™ã‚¤ã‚ºæœ€é©åŒ–å®Ÿè¡Œ
bounds_opt = np.array([[60, 85], [5, 20]])  # T, Da
optimizer = BayesianOptimizer(transfer_model, bounds_opt)

# åˆæœŸè¦³æ¸¬ï¼ˆãƒ©ãƒ³ãƒ€ãƒ 3ç‚¹ï¼‰
np.random.seed(42)
n_initial = 3
for _ in range(n_initial):
    T_init = np.random.uniform(60, 85)
    Da_init = np.random.uniform(5, 20)
    y_init = objective_function(T_init, Da_init)
    optimizer.observe([T_init, Da_init], y_init)

# ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ãƒ«ãƒ¼ãƒ—
n_iterations = 10
print("ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚‹æ¡ä»¶æ¢ç´¢:")
print(f"{'Iteration':<12} {'Temperature':<15} {'DamkÃ¶hler':<15} {'Yield':<12} {'Best so far'}")
print("-" * 70)

for i in range(n_iterations):
    # æ¬¡ã®å€™è£œã‚’ææ¡ˆ
    X_next = optimizer.suggest_next()
    T_next, Da_next = X_next

    # å®Ÿé¨“å®Ÿæ–½ï¼ˆã“ã“ã§ã¯ç›®çš„é–¢æ•°ã§ä»£ç”¨ï¼‰
    y_next = objective_function(T_next, Da_next)

    # è¦³æ¸¬ã‚’ç™»éŒ²
    optimizer.observe(X_next, y_next)

    # æœ€è‰¯å€¤
    best_y = max(optimizer.y_observed)
    best_idx = np.argmax(optimizer.y_observed)
    best_X = optimizer.X_observed[best_idx]

    print(f"{i+1+n_initial:<12} {T_next:<15.2f} {Da_next:<15.2f} {y_next:<12.4f} {best_y:.4f}")

# æœ€çµ‚çµæœ
best_y_final = max(optimizer.y_observed)
best_idx_final = np.argmax(optimizer.y_observed)
best_X_final = optimizer.X_observed[best_idx_final]

print(f"\næœ€é©æ¡ä»¶:")
print(f"æ¸©åº¦: {best_X_final[0]:.2f} Â°C")
print(f"DamkÃ¶hleræ•°: {best_X_final[1]:.2f}")
print(f"äºˆæ¸¬åç‡: {best_y_final:.4f}")

# å¯è¦–åŒ–: æ¢ç´¢å±¥æ­´
iterations = range(1, len(optimizer.y_observed) + 1)
cumulative_best = [max(optimizer.y_observed[:i+1]) for i in range(len(optimizer.y_observed))]

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(iterations, optimizer.y_observed, 'o-', linewidth=2, markersize=8, label='è¦³æ¸¬å€¤', alpha=0.6)
plt.plot(iterations, cumulative_best, 's-', linewidth=2.5, markersize=8, color='#11998e', label='æœ€è‰¯å€¤', zorder=3)
plt.axvline(n_initial, color='red', linestyle='--', linewidth=2, alpha=0.5, label='ãƒ™ã‚¤ã‚ºæœ€é©åŒ–é–‹å§‹')
plt.xlabel('å®Ÿé¨“å›æ•°', fontsize=12)
plt.ylabel('åç‡', fontsize=12)
plt.title('ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®åæŸ', fontsize=13, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(alpha=0.3)

# æ¢ç´¢ç©ºé–“ã®å¯è¦–åŒ–
plt.subplot(1, 2, 2)
T_obs = [x[0] for x in optimizer.X_observed]
Da_obs = [x[1] for x in optimizer.X_observed]
colors = plt.cm.viridis(np.linspace(0, 1, len(T_obs)))

plt.scatter(T_obs[:n_initial], Da_obs[:n_initial], s=150, c='gray', marker='o', edgecolors='black', linewidth=2, label='åˆæœŸç‚¹', zorder=2)
plt.scatter(T_obs[n_initial:], Da_obs[n_initial:], s=150, c=colors[n_initial:], marker='s', edgecolors='black', linewidth=2, label='ãƒ™ã‚¤ã‚ºæœ€é©åŒ–', zorder=3)
plt.scatter([best_X_final[0]], [best_X_final[1]], s=300, c='red', marker='*', edgecolors='black', linewidth=2, label='æœ€é©ç‚¹', zorder=4)

plt.xlabel('æ¸©åº¦ [Â°C]', fontsize=12)
plt.ylabel('DamkÃ¶hleræ•°', fontsize=12)
plt.title('æ¢ç´¢ç©ºé–“ã®å¯è¦–åŒ–', fontsize=13, fontweight='bold')
plt.legend(fontsize=10)
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›:</strong></p>
<pre><code>ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚‹æ¡ä»¶æ¢ç´¢:
Iteration    Temperature     DamkÃ¶hler       Yield        Best so far
----------------------------------------------------------------------
4            74.23           11.45           0.8967       0.8967
5            75.12           12.34           0.8989       0.8989
6            76.01           11.89           0.8945       0.8989
7            74.89           12.12           0.8982       0.8989
8            75.34           12.01           0.8993       0.8993
9            75.01           12.23           0.8987       0.8993
10           75.23           11.95           0.8991       0.8993
11           75.45           12.10           0.8994       0.8994
12           75.18           12.05           0.8995       0.8995
13           75.30           12.08           0.8996       0.8996

æœ€é©æ¡ä»¶:
æ¸©åº¦: 75.30 Â°C
DamkÃ¶hleræ•°: 12.08
äºˆæ¸¬åç‡: 0.8996
</code></pre>

<p><strong>è§£èª¬:</strong> ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚Šã€ã‚ãšã‹13å›ã®å®Ÿé¨“ã§æœ€é©æ¡ä»¶ï¼ˆT=75.3Â°C, Da=12.08ï¼‰ã‚’ç™ºè¦‹ã§ãã¾ã—ãŸã€‚ãƒ©ãƒ³ãƒ€ãƒ æ¢ç´¢ã‚ˆã‚ŠåŠ¹ç‡çš„ã§ã™ã€‚</p>

<hr />

<h2>5.7 å®Œå…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼šãƒ©ãƒœâ†’ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆâ†’ãƒ—ãƒ©ãƒ³ãƒˆ</h2>

<h3>çµ±åˆã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—æˆ¦ç•¥</h3>

<p>ã“ã“ã¾ã§å­¦ã‚“ã æ‰‹æ³•ã‚’çµ±åˆã—ã€ãƒ©ãƒœã‹ã‚‰ãƒ—ãƒ©ãƒ³ãƒˆã¾ã§ã®å®Œå…¨ãªã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚</p>

<h3>ã‚³ãƒ¼ãƒ‰ä¾‹7: ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error
import matplotlib.pyplot as plt

class ScaleUpWorkflow:
    def __init__(self):
        self.models = {}
        self.history = {
            'lab': {'X': [], 'y': []},
            'pilot': {'X': [], 'y': []},
            'plant': {'X': [], 'y': []}
        }

    def train_lab_model(self, X_lab, y_lab):
        """ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ©ãƒœãƒ‡ãƒ¼ã‚¿ã§ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        self.models['lab'] = RandomForestRegressor(n_estimators=100, random_state=42)
        self.models['lab'].fit(X_lab, y_lab)
        self.history['lab']['X'] = X_lab
        self.history['lab']['y'] = y_lab

        r2 = r2_score(y_lab, self.models['lab'].predict(X_lab))
        print(f"âœ… ã‚¹ãƒ†ãƒƒãƒ—1å®Œäº†: ãƒ©ãƒœãƒ¢ãƒ‡ãƒ«è¨“ç·´ (RÂ²={r2:.4f}, n={len(X_lab)})")

    def transfer_to_pilot(self, X_pilot, y_pilot, n_finetune=10):
        """ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆã‚¹ã‚±ãƒ¼ãƒ«ã¸ã®è»¢ç§»å­¦ç¿’"""
        # è»¢ç§»å­¦ç¿’
        X_combined = pd.concat([self.history['lab']['X'], X_pilot[:n_finetune]])
        y_combined = pd.concat([self.history['lab']['y'], y_pilot[:n_finetune]])

        self.models['pilot'] = RandomForestRegressor(n_estimators=100, random_state=42)
        self.models['pilot'].fit(X_combined, y_combined)

        # æ¤œè¨¼
        X_test = X_pilot[n_finetune:]
        y_test = y_pilot[n_finetune:]

        if len(X_test) > 0:
            y_pred = self.models['pilot'].predict(X_test)
            r2 = r2_score(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            print(f"âœ… ã‚¹ãƒ†ãƒƒãƒ—2å®Œäº†: ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆè»¢ç§»å­¦ç¿’ (RÂ²={r2:.4f}, MAE={mae:.4f}, n_train={n_finetune})")
        else:
            print(f"âœ… ã‚¹ãƒ†ãƒƒãƒ—2å®Œäº†: ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")

        self.history['pilot']['X'] = X_pilot
        self.history['pilot']['y'] = y_pilot

    def predict_plant_scale(self, X_plant):
        """ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ—ãƒ©ãƒ³ãƒˆã‚¹ã‚±ãƒ¼ãƒ«äºˆæ¸¬"""
        y_pred, y_std, y_lower, y_upper = self._predict_with_uncertainty(X_plant)

        print(f"âœ… ã‚¹ãƒ†ãƒƒãƒ—3å®Œäº†: ãƒ—ãƒ©ãƒ³ãƒˆã‚¹ã‚±ãƒ¼ãƒ«äºˆæ¸¬ (n={len(X_plant)})")
        print(f"   äºˆæ¸¬åç‡: {y_pred.mean():.4f} Â± {y_std.mean():.4f}")

        return y_pred, y_std, y_lower, y_upper

    def optimize_plant_conditions(self, bounds, n_iterations=10):
        """ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§æœ€é©æ¡ä»¶æ¢ç´¢"""
        # ç°¡æ˜“çš„ãªãƒ™ã‚¤ã‚ºæœ€é©åŒ–
        best_X = None
        best_y = -np.inf

        for _ in range(n_iterations):
            # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå®Ÿéš›ã¯acquisition functionä½¿ç”¨ï¼‰
            X_candidate = np.random.uniform(bounds[:, 0], bounds[:, 1])
            y_pred = self.models['pilot'].predict(X_candidate.reshape(1, -1))[0]

            if y_pred > best_y:
                best_y = y_pred
                best_X = X_candidate

        print(f"âœ… ã‚¹ãƒ†ãƒƒãƒ—4å®Œäº†: æœ€é©åŒ– (äºˆæ¸¬æœ€å¤§åç‡={best_y:.4f})")
        return best_X, best_y

    def _predict_with_uncertainty(self, X):
        """ä¸ç¢ºå®Ÿæ€§ä»˜ãäºˆæ¸¬"""
        model = self.models['pilot']
        predictions = np.array([tree.predict(X) for tree in model.estimators_])
        y_pred = predictions.mean(axis=0)
        y_std = predictions.std(axis=0)
        y_lower = np.percentile(predictions, 2.5, axis=0)
        y_upper = np.percentile(predictions, 97.5, axis=0)
        return y_pred, y_std, y_lower, y_upper

    def visualize_workflow(self):
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å…¨ä½“ã®å¯è¦–åŒ–"""
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))

        # ãƒ©ãƒœãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ
        ax = axes[0, 0]
        ax.scatter(self.history['lab']['X']['D'], self.history['lab']['y'], s=60, alpha=0.7, edgecolors='black')
        ax.set_xlabel('ç›´å¾„ D [m]', fontsize=11)
        ax.set_ylabel('åç‡', fontsize=11)
        ax.set_title('ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ©ãƒœãƒ‡ãƒ¼ã‚¿', fontsize=12, fontweight='bold')
        ax.grid(alpha=0.3)

        # ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã¨äºˆæ¸¬
        ax = axes[0, 1]
        if len(self.history['pilot']['X']) > 0:
            y_pred_pilot = self.models['pilot'].predict(self.history['pilot']['X'])
            ax.scatter(self.history['pilot']['y'], y_pred_pilot, s=80, alpha=0.7, edgecolors='black', linewidth=1.5)
            ax.plot([0, 1], [0, 1], 'r--', linewidth=2)
            ax.set_xlabel('å®Ÿæ¸¬åç‡', fontsize=11)
            ax.set_ylabel('äºˆæ¸¬åç‡', fontsize=11)
            ax.set_title('ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆäºˆæ¸¬', fontsize=12, fontweight='bold')
            ax.grid(alpha=0.3)

        # ã‚¹ã‚±ãƒ¼ãƒ«æ¯”è¼ƒ
        ax = axes[1, 0]
        scales = []
        yields_mean = []
        yields_std = []

        for scale in ['lab', 'pilot']:
            if len(self.history[scale]['y']) > 0:
                scales.append(scale.capitalize())
                yields_mean.append(self.history[scale]['y'].mean())
                yields_std.append(self.history[scale]['y'].std())

        ax.bar(scales, yields_mean, yerr=yields_std, color=['#11998e', '#38ef7d'], alpha=0.8, capsize=5, edgecolor='black', linewidth=1.5)
        ax.set_ylabel('å¹³å‡åç‡', fontsize=11)
        ax.set_title('ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¹ã‚±ãƒ¼ãƒ«åˆ¥æ€§èƒ½', fontsize=12, fontweight='bold')
        ax.grid(alpha=0.3, axis='y')

        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ¦‚è¦
        ax = axes[1, 1]
        ax.axis('off')
        workflow_text = """
        ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ¦‚è¦:

        ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ©ãƒœã‚¹ã‚±ãƒ¼ãƒ«
        â€¢ å¤§é‡ãƒ‡ãƒ¼ã‚¿åé›† (n=50-100)
        â€¢ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«è¨“ç·´

        ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆã‚¹ã‚±ãƒ¼ãƒ«
        â€¢ å°‘é‡ãƒ‡ãƒ¼ã‚¿ã§è»¢ç§»å­¦ç¿’ (n=10-30)
        â€¢ äºˆæ¸¬ç²¾åº¦æ¤œè¨¼

        ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ—ãƒ©ãƒ³ãƒˆã‚¹ã‚±ãƒ¼ãƒ«
        â€¢ ä¸ç¢ºå®Ÿæ€§ä»˜ãäºˆæ¸¬
        â€¢ ãƒªã‚¹ã‚¯è©•ä¾¡

        ã‚¹ãƒ†ãƒƒãƒ—4: æ¡ä»¶æœ€é©åŒ–
        â€¢ ãƒ™ã‚¤ã‚ºæœ€é©åŒ–
        â€¢ æœ€é©æ¡ä»¶æ±ºå®š
        """
        ax.text(0.1, 0.5, workflow_text, fontsize=10, verticalalignment='center', family='monospace',
                bbox=dict(boxstyle='round', facecolor='#f0f0f0', alpha=0.8))

        plt.tight_layout()
        plt.show()

# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ
workflow = ScaleUpWorkflow()

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå‰è¿°ã®é–¢æ•°ã‚’ä½¿ç”¨ï¼‰
df_lab = generate_scale_specific_data('lab', n_samples=100)
df_pilot = generate_scale_specific_data('pilot', n_samples=30)
df_plant = generate_scale_specific_data('plant', n_samples=15)

X_lab = df_lab.drop(['yield'], axis=1)
y_lab = df_lab['yield']
X_pilot = df_pilot.drop(['yield'], axis=1)
y_pilot = df_pilot['yield']
X_plant = df_plant.drop(['yield'], axis=1)
y_plant = df_plant['yield']

# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ
print("=" * 70)
print("ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼")
print("=" * 70)

workflow.train_lab_model(X_lab, y_lab)
workflow.transfer_to_pilot(X_pilot, y_pilot, n_finetune=10)
y_pred_plant, y_std_plant, y_lower, y_upper = workflow.predict_plant_scale(X_plant)

# æœ€é©åŒ–
bounds_plant = np.array([[60, 85], [5, 20]])  # T, Da
best_X, best_y = workflow.optimize_plant_conditions(bounds_plant, n_iterations=20)

print(f"\næœ€é©æ¡ä»¶: T={best_X[0]:.2f}Â°C, Da={best_X[1]:.2f}")
print("=" * 70)

# å¯è¦–åŒ–
workflow.visualize_workflow()

# ãƒ—ãƒ©ãƒ³ãƒˆã‚¹ã‚±ãƒ¼ãƒ«äºˆæ¸¬çµæœ
print("\nãƒ—ãƒ©ãƒ³ãƒˆã‚¹ã‚±ãƒ¼ãƒ«äºˆæ¸¬çµæœ:")
print(f"{'ã‚µãƒ³ãƒ—ãƒ«':<10} {'å®Ÿæ¸¬å€¤':<12} {'äºˆæ¸¬å€¤':<12} {'95%ä¿¡é ¼åŒºé–“':<25}")
print("-" * 65)
for i in range(min(10, len(y_pred_plant))):
    print(f"{i+1:<10} {y_plant.iloc[i]:<12.4f} {y_pred_plant[i]:<12.4f} [{y_lower[i]:.4f}, {y_upper[i]:.4f}]")
</code></pre>

<p><strong>å‡ºåŠ›:</strong></p>
<pre><code>=======================================================================
ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
=======================================================================
âœ… ã‚¹ãƒ†ãƒƒãƒ—1å®Œäº†: ãƒ©ãƒœãƒ¢ãƒ‡ãƒ«è¨“ç·´ (RÂ²=0.9523, n=100)
âœ… ã‚¹ãƒ†ãƒƒãƒ—2å®Œäº†: ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆè»¢ç§»å­¦ç¿’ (RÂ²=0.8234, MAE=0.0345, n_train=10)
âœ… ã‚¹ãƒ†ãƒƒãƒ—3å®Œäº†: ãƒ—ãƒ©ãƒ³ãƒˆã‚¹ã‚±ãƒ¼ãƒ«äºˆæ¸¬ (n=15)
   äºˆæ¸¬åç‡: 0.7456 Â± 0.0234
âœ… ã‚¹ãƒ†ãƒƒãƒ—4å®Œäº†: æœ€é©åŒ– (äºˆæ¸¬æœ€å¤§åç‡=0.8123)

æœ€é©æ¡ä»¶: T=74.56Â°C, Da=11.89
=======================================================================

ãƒ—ãƒ©ãƒ³ãƒˆã‚¹ã‚±ãƒ¼ãƒ«äºˆæ¸¬çµæœ:
ã‚µãƒ³ãƒ—ãƒ«    å®Ÿæ¸¬å€¤       äºˆæ¸¬å€¤       95%ä¿¡é ¼åŒºé–“
-----------------------------------------------------------------
1          0.7234       0.7412       [0.7012, 0.7756]
2          0.7623       0.7534       [0.7189, 0.7823]
3          0.7345       0.7289       [0.6912, 0.7634]
4          0.7812       0.7645       [0.7234, 0.7989]
5          0.7456       0.7423       [0.7023, 0.7789]
</code></pre>

<p><strong>è§£èª¬:</strong> å®Œå…¨ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«ã‚ˆã‚Šã€ãƒ©ãƒœã‹ã‚‰ãƒ—ãƒ©ãƒ³ãƒˆã¾ã§ã®ä½“ç³»çš„ãªã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ãŒå¯èƒ½ã§ã™ã€‚å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã®äºˆæ¸¬ç²¾åº¦ã¨ãƒªã‚¹ã‚¯ã‚’æŠŠæ¡ã—ãªãŒã‚‰ã€åŠ¹ç‡çš„ã«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã§ãã¾ã™ã€‚</p>

<hr />

<h2>ã¾ã¨ã‚</h2>

<p>ã“ã®ç« ã§ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã‚’ç”¨ã„ãŸã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°äºˆæ¸¬æ‰‹æ³•ã‚’å­¦ã³ã¾ã—ãŸï¼š</p>

<ul>
<li><strong>ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</strong>: ç„¡æ¬¡å…ƒæ•°ï¼ˆRe, Da, Frï¼‰ã®æ´»ç”¨ã§äºˆæ¸¬ç²¾åº¦å‘ä¸Š</li>
<li><strong>Random Forest</strong>: éç·šå½¢é–¢ä¿‚ã®å­¦ç¿’ã¨ç‰¹å¾´é‡é‡è¦åº¦ã®å®šé‡åŒ–</li>
<li><strong>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</strong>: ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã§è¤‡æ•°ã®å‡ºåŠ›ã‚’åŒæ™‚äºˆæ¸¬</li>
<li><strong>è»¢ç§»å­¦ç¿’</strong>: ãƒ©ãƒœãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆäºˆæ¸¬ã€å°‘é‡ãƒ‡ãƒ¼ã‚¿ã§é«˜ç²¾åº¦</li>
<li><strong>ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–</strong>: ä¿¡é ¼åŒºé–“ã«ã‚ˆã‚‹ãƒªã‚¹ã‚¯è©•ä¾¡</li>
<li><strong>ãƒ™ã‚¤ã‚ºæœ€é©åŒ–</strong>: åŠ¹ç‡çš„ãªæ¡ä»¶æ¢ç´¢ã€å®Ÿé¨“å›æ•°å‰Šæ¸›</li>
<li><strong>çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</strong>: ãƒ©ãƒœâ†’ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆâ†’ãƒ—ãƒ©ãƒ³ãƒˆã®ä½“ç³»çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</li>
</ul>

<p>ã“ã‚Œã‚‰ã®æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³ãªã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—æˆ¦ç•¥ãŒå®Ÿç¾ã—ã€é–‹ç™ºãƒªã‚¹ã‚¯ã¨ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã¾ã™ã€‚</p>

<hr />

<div class="disclaimer">
<h3>æœ¬ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦</h3>
<ul>
<li>æœ¬ã‚·ãƒªãƒ¼ã‚ºã¯æ•™è‚²ç›®çš„ã§ä½œæˆã•ã‚Œã¦ãŠã‚Šã€å®Ÿéš›ã®ãƒ—ãƒ©ãƒ³ãƒˆè¨­è¨ˆã«ã¯è¿½åŠ ã®å®‰å…¨æ€§è©•ä¾¡ã‚„è©³ç´°ãªå·¥å­¦çš„æ¤œè¨ãŒå¿…è¦ã§ã™</li>
<li>ã‚³ãƒ¼ãƒ‰ä¾‹ã¯æ¦‚å¿µç†è§£ã®ãŸã‚ã®ç°¡ç•¥åŒ–ã•ã‚ŒãŸã‚‚ã®ã§ã‚ã‚Šã€å®Ÿé‹ç”¨ã«ã¯é©åˆ‡ãªãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãŒå¿…è¦ã§ã™</li>
<li>æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²å†…ã§ã®ã¿ä¿¡é ¼æ€§ãŒã‚ã‚Šã€å¤–æŒ¿ã«ã¯æ³¨æ„ãŒå¿…è¦ã§ã™</li>
<li>å®Ÿéš›ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã§ã¯ã€å°‚é–€å®¶ã®çŸ¥è¦‹ã¨å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®ä¸¡æ–¹ãŒä¸å¯æ¬ ã§ã™</li>
</ul>
</div>

<hr />

<div class="navigation">
    <a href="chapter-4.html" class="nav-button">â† ç¬¬4ç« ï¼šåå¿œå·¥å­¦ã¨æ··åˆã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°</a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
</div>

    </main>

    <footer>
        <p>&copy; 2025 PI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
