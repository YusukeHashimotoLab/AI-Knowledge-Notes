<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="ç¬¬2ç« ï¼šTransformerãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿è§£æ - Self-Attentionã¨æ™‚ç³»åˆ—äºˆæ¸¬">
    <title>ç¬¬2ç« ï¼šTransformerãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿è§£æ - æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚° | PI Terakoya</title>

        <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.8; color: #333; background: #f5f5f5;
        }
        header {
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white; padding: 2rem 1rem; text-align: center;
        }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; }
        .subtitle { opacity: 0.9; font-size: 1.1rem; }
        .container { max-width: 1200px; margin: 2rem auto; padding: 0 1rem; }
        .back-link {
            display: inline-block; margin-bottom: 2rem; padding: 0.5rem 1rem;
            background: white; color: #11998e; text-decoration: none;
            border-radius: 6px; font-weight: 600;
        }
        .content-box {
            background: white; padding: 2rem; border-radius: 12px;
            margin-bottom: 2rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        h2 {
            color: #11998e; margin: 2rem 0 1rem 0;
            padding-bottom: 0.5rem; border-bottom: 3px solid #11998e;
        }
        h3 { color: #2c3e50; margin: 1.5rem 0 1rem 0; }
        h4 { color: #2c3e50; margin: 1rem 0 0.5rem 0; }
        p { margin-bottom: 1rem; }
        ul, ol { margin-left: 2rem; margin-bottom: 1rem; }
        li { margin-bottom: 0.5rem; }
        pre {
            background: #1e1e1e; color: #d4d4d4; padding: 1.5rem;
            border-radius: 8px; overflow-x: auto; margin: 1rem 0;
            border-left: 4px solid #11998e;
        }
        code {
            font-family: 'Courier New', monospace; font-size: 0.9rem;
        }
        .key-point {
            background: #e8f5e9; padding: 1rem; border-radius: 6px;
            border-left: 4px solid #4caf50; margin: 1rem 0;
        }
        .tech-note {
            background: #e3f2fd; padding: 1rem; border-radius: 6px;
            border-left: 4px solid #2196f3; margin: 1rem 0;
        }
        .formula {
            background: #f0f7ff; padding: 1rem; border-radius: 6px;
            margin: 1rem 0; overflow-x: auto;
        }
        table {
            width: 100%; border-collapse: collapse; margin: 1rem 0;
        }
        th, td {
            border: 1px solid #ddd; padding: 0.75rem; text-align: left;
        }
        th {
            background: #11998e; color: white; font-weight: 600;
        }
        tr:nth-child(even) { background: #f9f9f9; }
        .nav-buttons {
            display: flex; justify-content: space-between; margin-top: 3rem;
        }
        .nav-buttons a {
            padding: 0.75rem 1.5rem;
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white; text-decoration: none; border-radius: 6px;
            font-weight: 600;
        }
        footer {
            background: #2c3e50; color: white; text-align: center;
            padding: 2rem 1rem; margin-top: 4rem;
        }
        @media (max-width: 768px) {
            h1 { font-size: 1.6rem; }
            .container { padding: 0 0.5rem; }
            pre { padding: 1rem; }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <style>
        /* Locale Switcher Styles */
        .locale-switcher {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .current-locale {
            font-weight: 600;
            color: #7b2cbf;
            display: flex;
            align-items: center;
            gap: 0.25rem;
        }

        .locale-separator {
            color: #adb5bd;
            font-weight: 300;
        }

        .locale-link {
            color: #f093fb;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        .locale-link:hover {
            background: rgba(240, 147, 251, 0.1);
            color: #d07be8;
            transform: translateY(-1px);
        }

        .locale-meta {
            color: #868e96;
            font-size: 0.85rem;
            font-style: italic;
            margin-left: auto;
        }

        @media (max-width: 768px) {
            .locale-switcher {
                font-size: 0.85rem;
                padding: 0.4rem 0.8rem;
            }
            .locale-meta {
                display: none;
            }
        }
    </style>
</head>
<body>
            <div class="locale-switcher">
<span class="current-locale">ğŸŒ JP</span>
<span class="locale-separator">|</span>
<a href="../../../en/PI/deep-learning-modeling/chapter-2.html" class="locale-link">ğŸ‡¬ğŸ‡§ EN</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../PI/index.html">ãƒ—ãƒ­ã‚»ã‚¹ãƒ»ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹</a><span class="breadcrumb-separator">â€º</span><a href="../../PI/deep-learning-modeling/index.html">Deep Learning Modeling</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 2</span>
        </div>
    </nav>

        <header>
        <div class="container">
            <h1>ç¬¬2ç« ï¼šTransformerãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿è§£æ</h1>
            <p class="subtitle">Self-Attentionã«ã‚ˆã‚‹ä¸¦åˆ—æ™‚ç³»åˆ—å‡¦ç†ã¨é•·æœŸä¾å­˜æ€§ã®å­¦ç¿’</p>
            <div class="meta">
                <span class="meta">ğŸ“– èª­äº†æ™‚é–“: 30-35åˆ†</span>
                <span class="meta">ğŸ’¡ é›£æ˜“åº¦: ä¸Šç´š</span>
                <span class="meta">ğŸ”¬ å®Ÿä¾‹: å¤šå¤‰é‡ãƒ—ãƒ­ã‚»ã‚¹äºˆæ¸¬</span>
            </div>
        </div>
    </header>

    <main class="container">
        <section>
            <h2>2.1 Self-Attentionæ©Ÿæ§‹ã®åŸºç¤</h2>

            <p>Transformerã®æ ¸ã¨ãªã‚‹Self-Attentionæ©Ÿæ§‹ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®å…¨ã¦ã®è¦ç´ é–“ã®é–¢ä¿‚ã‚’ä¸¦åˆ—ã«è¨ˆç®—ã§ãã¾ã™ã€‚RNN/LSTMã¨ç•°ãªã‚Šã€é€æ¬¡å‡¦ç†ãŒä¸è¦ã§ã€é•·è·é›¢ä¾å­˜æ€§ã‚’ç›´æ¥æ‰ãˆã‚‰ã‚Œã¾ã™ã€‚</p>

            <div class="info-box">
                <p><strong>ğŸ’¡ Self-Attentionã®åˆ©ç‚¹</strong></p>
                <ul>
                    <li><strong>ä¸¦åˆ—å‡¦ç†</strong>: GPUã§é«˜é€Ÿã«è¨ˆç®—å¯èƒ½</li>
                    <li><strong>é•·è·é›¢ä¾å­˜æ€§</strong>: è·é›¢ã«é–¢ã‚ã‚‰ãšå…¨è¦ç´ ã«ã‚¢ã‚¯ã‚»ã‚¹</li>
                    <li><strong>è§£é‡ˆæ€§</strong>: Attentioné‡ã¿ã§è¦ç´ é–“ã®é–¢ä¿‚ã‚’å¯è¦–åŒ–</li>
                </ul>
            </div>

            <p>Self-Attentionã®è¨ˆç®—å¼ï¼š</p>

            <p>$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$</p>

            <p>ã“ã“ã§ã€\(Q\)ï¼ˆQueryï¼‰ã€\(K\)ï¼ˆKeyï¼‰ã€\(V\)ï¼ˆValueï¼‰ã¯å…¥åŠ›ã®ç·šå½¢å¤‰æ›ã§ã™ã€‚</p>

            <h3>ä¾‹1: Scaled Dot-Product Attentionå®Ÿè£…</h3>

            <pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math

class ScaledDotProductAttention(nn.Module):
    """Scaled Dot-Product Attention"""

    def __init__(self, temperature):
        """
        Args:
            temperature: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ï¼ˆé€šå¸¸ã¯ sqrt(d_k)ï¼‰
        """
        super(ScaledDotProductAttention, self).__init__()
        self.temperature = temperature

    def forward(self, q, k, v, mask=None):
        """
        Args:
            q: Query [batch, n_head, seq_len, d_k]
            k: Key [batch, n_head, seq_len, d_k]
            v: Value [batch, n_head, seq_len, d_v]
            mask: ãƒã‚¹ã‚¯ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        Returns:
            output: [batch, n_head, seq_len, d_v]
            attn: Attentioné‡ã¿ [batch, n_head, seq_len, seq_len]
        """
        # Qãƒ»K^T ã®è¨ˆç®—
        attn = torch.matmul(q, k.transpose(-2, -1)) / self.temperature

        # ãƒã‚¹ã‚¯é©ç”¨ï¼ˆæœªæ¥ã®æƒ…å ±ã‚’éš ã™ç­‰ï¼‰
        if mask is not None:
            attn = attn.masked_fill(mask == 0, -1e9)

        # Softmaxã§æ­£è¦åŒ–
        attn = F.softmax(attn, dim=-1)

        # é‡ã¿ä»˜ãå’Œ
        output = torch.matmul(attn, v)

        return output, attn

# ä½¿ç”¨ä¾‹ï¼ˆãƒ—ãƒ­ã‚»ã‚¹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼‰
batch_size = 2
seq_len = 50  # 50æ™‚åˆ»åˆ†ã®ãƒ‡ãƒ¼ã‚¿
d_model = 64  # ç‰¹å¾´æ¬¡å…ƒ

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆåå¿œå™¨ã®æ¸©åº¦ãƒ»åœ§åŠ›ãƒ»æµé‡ãªã©ï¼‰
x = torch.randn(batch_size, seq_len, d_model)

# Q, K, V ã‚’ä½œæˆï¼ˆã“ã®ä¾‹ã§ã¯å…¨ã¦åŒã˜ï¼‰
q = k = v = x.unsqueeze(1)  # [batch, 1, seq_len, d_model] (1 head)

# Attentionè¨ˆç®—
attention = ScaledDotProductAttention(temperature=math.sqrt(d_model))
output, attn_weights = attention(q, k, v)

print(f"Output shape: {output.shape}")  # [2, 1, 50, 64]
print(f"Attention weights shape: {attn_weights.shape}")  # [2, 1, 50, 50]
print(f"Attention weights sum (should be 1.0): {attn_weights[0, 0, 0, :].sum():.4f}")

# å‡ºåŠ›ä¾‹:
# Output shape: torch.Size([2, 1, 50, 64])
# Attention weights shape: torch.Size([2, 1, 50, 50])
# Attention weights sum (should be 1.0): 1.0000
</code></pre>
        </section>

        <section>
            <h2>2.2 Positional Encodingï¼ˆä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰</h2>

            <p>Transformerã¯ä¸¦åˆ—å‡¦ç†ã®ãŸã‚ã€å…¥åŠ›ã®é †åºæƒ…å ±ã‚’æŒã¡ã¾ã›ã‚“ã€‚æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã¯æ™‚åˆ»ã®é †åºãŒé‡è¦ãªã®ã§ã€Positional Encodingã§ä½ç½®æƒ…å ±ã‚’ä»˜åŠ ã—ã¾ã™ã€‚</p>

            <p>æ­£å¼¦æ³¢ãƒ™ãƒ¼ã‚¹ã®Positional Encodingï¼š</p>

            <p>$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$</p>
            <p>$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$</p>

            <h3>ä¾‹2: Positional Encodingå®Ÿè£…</h3>

            <pre><code class="language-python">class PositionalEncoding(nn.Module):
    """æ­£å¼¦æ³¢ãƒ™ãƒ¼ã‚¹ã®ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""

    def __init__(self, d_model, max_len=5000):
        """
        Args:
            d_model: ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒæ•°
            max_len: æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
        """
        super(PositionalEncoding, self).__init__()

        # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’äº‹å‰è¨ˆç®—
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()

        # åˆ†æ¯ã®è¨ˆç®—
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                            -(math.log(10000.0) / d_model))

        # sin/cosã®é©ç”¨
        pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

        pe = pe.unsqueeze(0)  # [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        Args:
            x: å…¥åŠ› [batch, seq_len, d_model]
        Returns:
            x + pe: ä½ç½®æƒ…å ±ã‚’ä»˜åŠ  [batch, seq_len, d_model]
        """
        seq_len = x.size(1)
        x = x + self.pe[:, :seq_len, :]
        return x

# ä½¿ç”¨ä¾‹
d_model = 64
seq_len = 100
batch_size = 4

# ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¸©åº¦ã®æ™‚ç³»åˆ—ãªã©ï¼‰
process_data = torch.randn(batch_size, seq_len, d_model)

# ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é©ç”¨
pos_encoder = PositionalEncoding(d_model=d_model, max_len=5000)
encoded_data = pos_encoder(process_data)

print(f"Input shape: {process_data.shape}")
print(f"Output shape: {encoded_data.shape}")

# ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å¯è¦–åŒ–
import matplotlib.pyplot as plt

pe_matrix = pos_encoder.pe[0, :100, :].numpy()  # æœ€åˆã®100æ™‚åˆ»
plt.figure(figsize=(10, 4))
plt.imshow(pe_matrix.T, cmap='RdBu', aspect='auto')
plt.colorbar()
plt.xlabel('Position (Time Step)')
plt.ylabel('Dimension')
plt.title('Positional Encoding Pattern')
plt.tight_layout()
# plt.savefig('positional_encoding.png')

print("ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯è¦–åŒ–å®Œäº†")

# å‡ºåŠ›ä¾‹:
# Input shape: torch.Size([4, 100, 64])
# Output shape: torch.Size([4, 100, 64])
# ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯è¦–åŒ–å®Œäº†
</code></pre>

            <div class="tip-box">
                <p><strong>ğŸ’¡ å­¦ç¿’å¯èƒ½ãªä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°</strong></p>
                <p>å›ºå®šã®æ­£å¼¦æ³¢ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ä»£ã‚ã‚Šã«ã€å­¦ç¿’å¯èƒ½ãªembeddingã‚‚ä½¿ç”¨ã§ãã¾ã™ï¼š</p>
                <pre><code>self.pos_embedding = nn.Parameter(torch.randn(1, max_len, d_model))</code></pre>
                <p>ãƒ‡ãƒ¼ã‚¿é‡ãŒååˆ†ã«ã‚ã‚‹å ´åˆã€ã“ã¡ã‚‰ã®æ–¹ãŒæ€§èƒ½ãŒè‰¯ã„ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</p>
            </div>
        </section>

        <section>
            <h2>2.3 Multi-Head Attention</h2>

            <p>Multi-Head Attentionã¯ã€è¤‡æ•°ã®ç•°ãªã‚‹Attentionæ©Ÿæ§‹ã‚’ä¸¦åˆ—å®Ÿè¡Œã—ã€å¤šæ§˜ãªç‰¹å¾´ã‚’æ‰ãˆã¾ã™ã€‚å„ãƒ˜ãƒƒãƒ‰ãŒç•°ãªã‚‹æ™‚é–“ä¾å­˜æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã§ãã¾ã™ã€‚</p>

            <h3>ä¾‹3: Multi-Head Attentionå®Ÿè£…</h3>

            <pre><code class="language-python">class MultiHeadAttention(nn.Module):
    """Multi-Head Attention"""

    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):
        """
        Args:
            n_head: ãƒ˜ãƒƒãƒ‰æ•°
            d_model: ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ
            d_k: Keyã®æ¬¡å…ƒ
            d_v: Valueã®æ¬¡å…ƒ
        """
        super(MultiHeadAttention, self).__init__()

        self.n_head = n_head
        self.d_k = d_k
        self.d_v = d_v

        # Q, K, V ã®ç·šå½¢å¤‰æ›
        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)
        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)
        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)

        # å‡ºåŠ›ã®ç·šå½¢å¤‰æ›
        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)

        self.attention = ScaledDotProductAttention(temperature=math.sqrt(d_k))
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model)

    def forward(self, q, k, v, mask=None):
        """
        Args:
            q, k, v: [batch, seq_len, d_model]
        Returns:
            output: [batch, seq_len, d_model]
            attn: [batch, n_head, seq_len, seq_len]
        """
        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head
        batch_size, len_q, _ = q.size()
        len_k, len_v = k.size(1), v.size(1)

        residual = q

        # Q, K, V ã«ç·šå½¢å¤‰æ›ã‚’é©ç”¨ã—ã€ãƒ˜ãƒƒãƒ‰ã”ã¨ã«åˆ†å‰²
        q = self.w_qs(q).view(batch_size, len_q, n_head, d_k).transpose(1, 2)
        k = self.w_ks(k).view(batch_size, len_k, n_head, d_k).transpose(1, 2)
        v = self.w_vs(v).view(batch_size, len_v, n_head, d_v).transpose(1, 2)

        # Attentionè¨ˆç®—
        q, attn = self.attention(q, k, v, mask=mask)

        # ãƒ˜ãƒƒãƒ‰ã‚’çµåˆ
        q = q.transpose(1, 2).contiguous().view(batch_size, len_q, -1)

        # å‡ºåŠ›å±¤
        q = self.dropout(self.fc(q))

        # Residual connection + Layer Normalization
        q = self.layer_norm(q + residual)

        return q, attn

# ä½¿ç”¨ä¾‹ï¼ˆãƒ—ãƒ­ã‚»ã‚¹å¤šå¤‰é‡æ™‚ç³»åˆ—ï¼‰
batch_size = 4
seq_len = 50
d_model = 128
n_head = 8

# åå¿œå™¨ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¸©åº¦ã€åœ§åŠ›ã€æµé‡ã€æ¿ƒåº¦ãªã©ï¼‰
process_seq = torch.randn(batch_size, seq_len, d_model)

# Multi-Head Attentioné©ç”¨
mha = MultiHeadAttention(n_head=n_head, d_model=d_model,
                         d_k=d_model//n_head, d_v=d_model//n_head)
output, attention = mha(process_seq, process_seq, process_seq)

print(f"Output shape: {output.shape}")  # [4, 50, 128]
print(f"Attention shape: {attention.shape}")  # [4, 8, 50, 50]

# å„ãƒ˜ãƒƒãƒ‰ãŒç•°ãªã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
print("\nAttention weights variance per head:")
for h in range(n_head):
    var = attention[0, h].var().item()
    print(f"  Head {h+1}: variance = {var:.4f}")

# å‡ºåŠ›ä¾‹:
# Output shape: torch.Size([4, 50, 128])
# Attention shape: torch.Size([4, 8, 50, 50])
#
# Attention weights variance per head:
#   Head 1: variance = 0.0023
#   Head 2: variance = 0.0019
#   Head 3: variance = 0.0025
#   ...ï¼ˆå„ãƒ˜ãƒƒãƒ‰ã§ç•°ãªã‚‹åˆ†æ•£ï¼‰
</code></pre>
        </section>

        <section>
            <h2>2.4 Transformer Encoder</h2>

            <p>Transformer Encoderã¯ã€Multi-Head Attentionã¨Feed-Forward Networkã‚’çµ„ã¿åˆã‚ã›ãŸå±¤ã‚’ç©ã¿é‡ã­ã¾ã™ã€‚ãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°ã®äºˆæ¸¬ã«ä½¿ç”¨ã—ã¾ã™ã€‚</p>

            <h3>ä¾‹4: Transformer Encoderã«ã‚ˆã‚‹äºˆæ¸¬</h3>

            <pre><code class="language-python">class TransformerEncoderLayer(nn.Module):
    """Transformer Encoder Layer"""

    def __init__(self, d_model, n_head, d_ff, dropout=0.1):
        """
        Args:
            d_model: ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ
            n_head: ãƒ˜ãƒƒãƒ‰æ•°
            d_ff: Feed-Forwardå±¤ã®ä¸­é–“æ¬¡å…ƒ
        """
        super(TransformerEncoderLayer, self).__init__()

        # Multi-Head Attention
        self.self_attn = MultiHeadAttention(
            n_head=n_head, d_model=d_model,
            d_k=d_model//n_head, d_v=d_model//n_head
        )

        # Feed-Forward Network
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )

        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model)

    def forward(self, x, mask=None):
        """
        Args:
            x: [batch, seq_len, d_model]
        """
        # Multi-Head Attention
        attn_output, _ = self.self_attn(x, x, x, mask)

        # Feed-Forward Network + Residual
        residual = attn_output
        ffn_output = self.ffn(attn_output)
        output = self.layer_norm(ffn_output + residual)

        return output

class ProcessTransformer(nn.Module):
    """ãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°äºˆæ¸¬ç”¨Transformer"""

    def __init__(self, n_features, d_model=128, n_head=8, n_layers=4,
                 d_ff=512, dropout=0.1):
        """
        Args:
            n_features: å…¥åŠ›å¤‰æ•°æ•°ï¼ˆæ¸©åº¦ã€åœ§åŠ›ãªã©ï¼‰
            d_model: ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ
            n_head: Attentionãƒ˜ãƒƒãƒ‰æ•°
            n_layers: Encoderãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
            d_ff: Feed-Forwardä¸­é–“æ¬¡å…ƒ
        """
        super(ProcessTransformer, self).__init__()

        # å…¥åŠ›åŸ‹ã‚è¾¼ã¿
        self.input_embedding = nn.Linear(n_features, d_model)

        # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        self.pos_encoder = PositionalEncoding(d_model)

        # Transformer Encoder layers
        self.encoder_layers = nn.ModuleList([
            TransformerEncoderLayer(d_model, n_head, d_ff, dropout)
            for _ in range(n_layers)
        ])

        # å‡ºåŠ›å±¤
        self.fc_out = nn.Linear(d_model, n_features)

    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, n_features]
        Returns:
            output: [batch, n_features] æ¬¡æ™‚åˆ»ã®äºˆæ¸¬
        """
        # åŸ‹ã‚è¾¼ã¿ + ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        x = self.input_embedding(x)
        x = self.pos_encoder(x)

        # Encoder layers
        for layer in self.encoder_layers:
            x = layer(x)

        # æœ€å¾Œã®æ™‚åˆ»ã®å‡ºåŠ›ã‚’ä½¿ç”¨
        output = self.fc_out(x[:, -1, :])

        return output

# åˆæˆãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿
def generate_process_data(n_samples=1000, n_features=3):
    """åå¿œå™¨ã®æ¸©åº¦ãƒ»åœ§åŠ›ãƒ»æµé‡ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
    time = np.linspace(0, 50, n_samples)
    data = np.zeros((n_samples, n_features))

    # æ¸©åº¦ï¼ˆ300-500Kï¼‰
    data[:, 0] = 400 + 50*np.sin(0.1*time) + 10*np.random.randn(n_samples)

    # åœ§åŠ›ï¼ˆ1-10 barï¼‰
    data[:, 1] = 5 + 2*np.cos(0.15*time) + 0.5*np.random.randn(n_samples)

    # æµé‡ï¼ˆ50-150 L/minï¼‰
    data[:, 2] = 100 + 30*np.sin(0.08*time) + 5*np.random.randn(n_samples)

    return data

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
data = generate_process_data(n_samples=1000, n_features=3)

# ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ä½œæˆ
window_size = 50
X, y = [], []
for i in range(len(data) - window_size):
    X.append(data[i:i+window_size])
    y.append(data[i+window_size])

X = torch.FloatTensor(np.array(X))
y = torch.FloatTensor(np.array(y))

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = ProcessTransformer(n_features=3, d_model=128, n_head=8, n_layers=4)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)

for epoch in range(30):
    model.train()
    optimizer.zero_grad()

    pred = model(X)
    loss = criterion(pred, y)

    loss.backward()
    optimizer.step()

    if (epoch+1) % 5 == 0:
        print(f'Epoch {epoch+1}, Loss: {loss.item():.6f}')

# å‡ºåŠ›ä¾‹:
# Epoch 5, Loss: 0.234567
# Epoch 10, Loss: 0.123456
# Epoch 15, Loss: 0.078901
# Epoch 20, Loss: 0.056789
# Epoch 25, Loss: 0.045678
# Epoch 30, Loss: 0.039876
</code></pre>
        </section>

        <section>
            <h2>2.5 Temporal Fusion Transformer</h2>

            <p>Temporal Fusion Transformerï¼ˆTFTï¼‰ã¯ã€æ™‚ç³»åˆ—äºˆæ¸¬ã«ç‰¹åŒ–ã—ãŸTransformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚é™çš„å¤‰æ•°ï¼ˆãƒ—ãƒ­ã‚»ã‚¹è¨­è¨ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã€æ—¢çŸ¥å¤‰æ•°ï¼ˆåˆ¶å¾¡å…¥åŠ›ï¼‰ã€æœªçŸ¥å¤‰æ•°ï¼ˆå¤–ä¹±ï¼‰ã‚’çµ±åˆçš„ã«æ‰±ãˆã¾ã™ã€‚</p>

            <div class="info-box">
                <p><strong>ğŸ’¡ TFTã®ç‰¹å¾´</strong></p>
                <ul>
                    <li><strong>Variable Selection</strong>: é‡è¦ãªå¤‰æ•°ã‚’è‡ªå‹•é¸æŠ</li>
                    <li><strong>Gatingæ©Ÿæ§‹</strong>: æƒ…å ±ã®æµã‚Œã‚’é©å¿œçš„ã«åˆ¶å¾¡</li>
                    <li><strong>Interpretable Multi-Horizon</strong>: è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—å…ˆã‚’è§£é‡ˆå¯èƒ½ã«äºˆæ¸¬</li>
                </ul>
            </div>

            <h3>ä¾‹5: ç°¡æ˜“TFTå®Ÿè£…</h3>

            <pre><code class="language-python">class VariableSelectionNetwork(nn.Module):
    """å¤‰æ•°é¸æŠãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""

    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.1):
        super(VariableSelectionNetwork, self).__init__()

        self.hidden_dim = hidden_dim

        # Gatingç”¨ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.grn = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, output_dim)
        )

        # é‡è¦åº¦ã‚¹ã‚³ã‚¢
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        """
        Args:
            x: [batch, n_vars, dim]
        Returns:
            weighted: é‡ã¿ä»˜ã‘ã•ã‚ŒãŸå¤‰æ•° [batch, dim]
            weights: å¤‰æ•°ã®é‡è¦åº¦ [batch, n_vars]
        """
        # å„å¤‰æ•°ã®é‡è¦åº¦ã‚’è¨ˆç®—
        weights = self.softmax(self.grn(x))  # [batch, n_vars, 1]

        # é‡ã¿ä»˜ãå’Œ
        weighted = torch.sum(x * weights, dim=1)

        return weighted, weights.squeeze(-1)

class SimplifiedTFT(nn.Module):
    """ç°¡æ˜“ç‰ˆTemporal Fusion Transformer"""

    def __init__(self, n_features, static_dim, d_model=128, n_head=4,
                 n_layers=2, pred_len=10):
        """
        Args:
            n_features: æ™‚ç³»åˆ—å¤‰æ•°æ•°
            static_dim: é™çš„å¤‰æ•°æ•°ï¼ˆåå¿œå™¨ã‚¿ã‚¤ãƒ—ãªã©ï¼‰
            pred_len: äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚ºãƒ³
        """
        super(SimplifiedTFT, self).__init__()
        self.pred_len = pred_len

        # Variable Selection
        self.var_selection = VariableSelectionNetwork(
            input_dim=n_features, hidden_dim=64, output_dim=d_model
        )

        # Static enrichment
        self.static_enrichment = nn.Linear(static_dim, d_model)

        # Transformer Encoder
        self.pos_encoder = PositionalEncoding(d_model)
        self.encoder_layers = nn.ModuleList([
            TransformerEncoderLayer(d_model, n_head, d_ff=512)
            for _ in range(n_layers)
        ])

        # Decoder
        self.decoder = nn.Linear(d_model, n_features * pred_len)

    def forward(self, x, static_vars):
        """
        Args:
            x: [batch, seq_len, n_features] æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
            static_vars: [batch, static_dim] é™çš„å¤‰æ•°
        Returns:
            pred: [batch, pred_len, n_features]
        """
        batch_size = x.size(0)

        # Variable Selection
        x_selected, var_weights = self.var_selection(x.transpose(1, 2))

        # Static enrichment
        static_encoded = self.static_enrichment(static_vars)
        x_enriched = x_selected + static_encoded.unsqueeze(1)

        # Positional encoding
        x_enriched = self.pos_encoder(x_enriched)

        # Transformer Encoder
        for layer in self.encoder_layers:
            x_enriched = layer(x_enriched)

        # Decode to predictions
        pred = self.decoder(x_enriched[:, -1, :])
        pred = pred.view(batch_size, self.pred_len, -1)

        return pred, var_weights

# ä½¿ç”¨ä¾‹
n_features = 4  # æ¸©åº¦ã€åœ§åŠ›ã€æµé‡ã€æ¿ƒåº¦
static_dim = 2  # åå¿œå™¨ã‚¿ã‚¤ãƒ—ã€è§¦åª’ç¨®é¡
seq_len = 50
pred_len = 10
batch_size = 16

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
x = torch.randn(batch_size, seq_len, n_features)
static_vars = torch.randn(batch_size, static_dim)

# ãƒ¢ãƒ‡ãƒ«
model = SimplifiedTFT(n_features=n_features, static_dim=static_dim,
                     d_model=128, pred_len=pred_len)

# äºˆæ¸¬
pred, var_weights = model(x, static_vars)

print(f"Prediction shape: {pred.shape}")  # [16, 10, 4]
print(f"Variable importance weights shape: {var_weights.shape}")  # [16, 4]

# å¤‰æ•°ã®é‡è¦åº¦
print("\nVariable importance (sample 1):")
for i in range(n_features):
    print(f"  Variable {i+1}: {var_weights[0, i].item():.4f}")

# å‡ºåŠ›ä¾‹:
# Prediction shape: torch.Size([16, 10, 4])
# Variable importance weights shape: torch.Size([16, 4])
#
# Variable importance (sample 1):
#   Variable 1: 0.3456
#   Variable 2: 0.2123
#   Variable 3: 0.2789
#   Variable 4: 0.1632
</code></pre>
        </section>

        <section>
            <h2>2.6 å¤šå¤‰é‡æ™‚ç³»åˆ—äºˆæ¸¬</h2>

            <p>ãƒ—ãƒ­ã‚»ã‚¹ç”£æ¥­ã§ã¯ã€æ¸©åº¦ãƒ»åœ§åŠ›ãƒ»æµé‡ãƒ»æ¿ƒåº¦ãªã©è¤‡æ•°ã®å¤‰æ•°ãŒç›¸äº’ä½œç”¨ã—ã¾ã™ã€‚Transformerã¯ã“ã‚Œã‚‰ã®è¤‡é›‘ãªç›¸äº’ä¾å­˜æ€§ã‚’å­¦ç¿’ã§ãã¾ã™ã€‚</p>

            <h3>ä¾‹6: å¤šå¤‰é‡ãƒ—ãƒ­ã‚»ã‚¹äºˆæ¸¬</h3>

            <pre><code class="language-python">class MultivariateProcessTransformer(nn.Module):
    """å¤šå¤‰é‡ãƒ—ãƒ­ã‚»ã‚¹äºˆæ¸¬ç”¨Transformer"""

    def __init__(self, n_features, d_model=256, n_head=8, n_layers=6,
                 pred_horizon=20, dropout=0.1):
        """
        Args:
            n_features: å¤‰æ•°æ•°
            pred_horizon: äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—æ•°
        """
        super(MultivariateProcessTransformer, self).__init__()
        self.n_features = n_features
        self.pred_horizon = pred_horizon

        # å„å¤‰æ•°ã«å¯¾ã™ã‚‹åŸ‹ã‚è¾¼ã¿
        self.feature_embedding = nn.Linear(1, d_model // n_features)

        # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        self.pos_encoder = PositionalEncoding(d_model)

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=n_head, dim_feedforward=1024,
            dropout=dropout, batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer, num_layers=n_layers
        )

        # äºˆæ¸¬ãƒ˜ãƒƒãƒ‰
        self.prediction_head = nn.Sequential(
            nn.Linear(d_model, 512),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(512, n_features * pred_horizon)
        )

    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, n_features]
        Returns:
            pred: [batch, pred_horizon, n_features]
        """
        batch_size, seq_len, _ = x.shape

        # å„ç‰¹å¾´ã‚’åŸ‹ã‚è¾¼ã¿ã€çµåˆ
        embedded_features = []
        for i in range(self.n_features):
            feat = x[:, :, i:i+1]  # [batch, seq_len, 1]
            emb = self.feature_embedding(feat)  # [batch, seq_len, d_model//n_features]
            embedded_features.append(emb)

        x_embedded = torch.cat(embedded_features, dim=-1)  # [batch, seq_len, d_model]

        # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        x_encoded = self.pos_encoder(x_embedded)

        # Transformer Encoder
        transformer_out = self.transformer_encoder(x_encoded)

        # æœ€çµ‚æ™‚åˆ»ã§äºˆæ¸¬
        final_hidden = transformer_out[:, -1, :]  # [batch, d_model]

        # äºˆæ¸¬
        pred = self.prediction_head(final_hidden)  # [batch, n_features * pred_horizon]
        pred = pred.view(batch_size, self.pred_horizon, self.n_features)

        return pred

# å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆåå¿œå™¨ã®è¤‡é›‘ãªãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ï¼‰
def generate_coupled_process_data(n_samples=2000):
    """ç›¸äº’ä½œç”¨ã™ã‚‹å¤šå¤‰é‡ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿"""
    time = np.linspace(0, 100, n_samples)

    # æ¸©åº¦ï¼ˆTï¼‰
    T = 400 + 50*np.sin(0.05*time) + 10*np.random.randn(n_samples)

    # åœ§åŠ›ï¼ˆPï¼‰: æ¸©åº¦ã«ä¾å­˜
    P = 5 + 0.01*T + 2*np.cos(0.07*time) + 0.5*np.random.randn(n_samples)

    # æµé‡ï¼ˆFï¼‰
    F = 100 + 20*np.sin(0.06*time) + 5*np.random.randn(n_samples)

    # æ¿ƒåº¦ï¼ˆCï¼‰: æ¸©åº¦ã¨æµé‡ã«ä¾å­˜
    C = 0.8 - 0.0005*T + 0.001*F + 0.05*np.random.randn(n_samples)

    return np.stack([T, P, F, C], axis=1)

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
data = generate_coupled_process_data(n_samples=2000)

# æ­£è¦åŒ–
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
data_normalized = scaler.fit_transform(data)

# ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ä½œæˆ
window_size = 100
pred_horizon = 20

X, y = [], []
for i in range(len(data_normalized) - window_size - pred_horizon):
    X.append(data_normalized[i:i+window_size])
    y.append(data_normalized[i+window_size:i+window_size+pred_horizon])

X = torch.FloatTensor(np.array(X))
y = torch.FloatTensor(np.array(y))

# è¨“ç·´/ãƒ†ã‚¹ãƒˆåˆ†å‰²
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = MultivariateProcessTransformer(
    n_features=4, d_model=256, n_head=8, n_layers=6, pred_horizon=20
)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)

for epoch in range(20):
    model.train()
    optimizer.zero_grad()

    pred = model(X_train)
    loss = criterion(pred, y_train)

    loss.backward()
    optimizer.step()

    if (epoch+1) % 5 == 0:
        model.eval()
        with torch.no_grad():
            test_pred = model(X_test)
            test_loss = criterion(test_pred, y_test)
        print(f'Epoch {epoch+1}, Train: {loss.item():.6f}, Test: {test_loss.item():.6f}')

# å‡ºåŠ›ä¾‹:
# Epoch 5, Train: 0.145678, Test: 0.156789
# Epoch 10, Train: 0.089012, Test: 0.098765
# Epoch 15, Train: 0.067890, Test: 0.076543
# Epoch 20, Train: 0.056789, Test: 0.065432
</code></pre>
        </section>

        <section>
            <h2>2.7 è»¢ç§»å­¦ç¿’ï¼ˆTransfer Learningï¼‰</h2>

            <p>äº‹å‰å­¦ç¿’æ¸ˆã¿Transformerãƒ¢ãƒ‡ãƒ«ã‚’ã€å°‘é‡ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã‚’æ”¹å–„ã§ãã¾ã™ã€‚</p>

            <h3>ä¾‹7: äº‹å‰å­¦ç¿’ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</h3>

            <pre><code class="language-python">class PretrainedProcessTransformer(nn.Module):
    """äº‹å‰å­¦ç¿’ç”¨Transformerï¼ˆãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰"""

    def __init__(self, n_features, d_model=128, n_head=8, n_layers=4):
        super(PretrainedProcessTransformer, self).__init__()

        self.embedding = nn.Linear(n_features, d_model)
        self.pos_encoder = PositionalEncoding(d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=n_head, dim_feedforward=512,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)

        # ãƒã‚¹ã‚¯ã•ã‚ŒãŸå€¤ã‚’å†æ§‹æˆ
        self.reconstruction_head = nn.Linear(d_model, n_features)

    def forward(self, x, mask=None):
        """
        Args:
            x: [batch, seq_len, n_features]
            mask: ãƒã‚¹ã‚¯ä½ç½® [batch, seq_len]
        Returns:
            reconstructed: [batch, seq_len, n_features]
        """
        x = self.embedding(x)
        x = self.pos_encoder(x)
        x = self.transformer(x)
        reconstructed = self.reconstruction_head(x)

        return reconstructed

# ã‚¹ãƒ†ãƒƒãƒ—1: å¤§é‡ã®æœªãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’
def pretrain_model(model, data, epochs=50, mask_ratio=0.15):
    """ãƒã‚¹ã‚¯äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã§äº‹å‰å­¦ç¿’"""
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
    criterion = nn.MSELoss()

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()

        # ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒã‚¹ã‚¯
        masked_data = data.clone()
        mask = torch.rand(data.shape[:2]) < mask_ratio  # [batch, seq_len]

        for b in range(data.size(0)):
            for t in range(data.size(1)):
                if mask[b, t]:
                    masked_data[b, t] = 0  # ã‚¼ãƒ­ã§ãƒã‚¹ã‚¯

        # å†æ§‹æˆ
        pred = model(masked_data)

        # ãƒã‚¹ã‚¯ä½ç½®ã®ã¿ã§lossè¨ˆç®—
        loss = criterion(pred[mask], data[mask])

        loss.backward()
        optimizer.step()

        if (epoch+1) % 10 == 0:
            print(f'Pretrain Epoch {epoch+1}, Loss: {loss.item():.6f}')

    return model

# ã‚¹ãƒ†ãƒƒãƒ—2: ä¸‹æµã‚¿ã‚¹ã‚¯ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
class FineTunedPredictor(nn.Module):
    """äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"""

    def __init__(self, pretrained_model, n_features, pred_len=10):
        super(FineTunedPredictor, self).__init__()

        # äº‹å‰å­¦ç¿’æ¸ˆã¿ã®é‡ã¿ã‚’ä½¿ç”¨
        self.backbone = pretrained_model

        # ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®äºˆæ¸¬ãƒ˜ãƒƒãƒ‰
        self.pred_head = nn.Linear(128, n_features * pred_len)
        self.pred_len = pred_len
        self.n_features = n_features

    def forward(self, x):
        """äºˆæ¸¬ã‚¿ã‚¹ã‚¯"""
        # Backboneï¼ˆå‡çµ or ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰
        features = self.backbone.transformer(
            self.backbone.pos_encoder(self.backbone.embedding(x))
        )

        # æœ€çµ‚æ™‚åˆ»ã§äºˆæ¸¬
        pred = self.pred_head(features[:, -1, :])
        pred = pred.view(-1, self.pred_len, self.n_features)

        return pred

# å®Ÿé¨“
# å¤§é‡ã®æœªãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿
unlabeled_data = torch.randn(500, 100, 4)  # 500ã‚·ãƒ¼ã‚±ãƒ³ã‚¹

# äº‹å‰å­¦ç¿’
pretrain_model_instance = PretrainedProcessTransformer(
    n_features=4, d_model=128, n_head=8
)
pretrain_model_instance = pretrain_model(pretrain_model_instance, unlabeled_data, epochs=30)

# å°‘é‡ã®ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
small_labeled_data = torch.randn(50, 100, 4)  # ã‚ãšã‹50ã‚µãƒ³ãƒ—ãƒ«
small_labels = torch.randn(50, 10, 4)

finetuned_model = FineTunedPredictor(pretrain_model_instance, n_features=4, pred_len=10)
optimizer = torch.optim.Adam(finetuned_model.parameters(), lr=0.0001)
criterion = nn.MSELoss()

for epoch in range(20):
    optimizer.zero_grad()
    pred = finetuned_model(small_labeled_data)
    loss = criterion(pred, small_labels)
    loss.backward()
    optimizer.step()

    if (epoch+1) % 5 == 0:
        print(f'Finetune Epoch {epoch+1}, Loss: {loss.item():.6f}')

# å‡ºåŠ›ä¾‹:
# Pretrain Epoch 10, Loss: 0.234567
# Pretrain Epoch 20, Loss: 0.123456
# Pretrain Epoch 30, Loss: 0.089012
# Finetune Epoch 5, Loss: 0.045678
# Finetune Epoch 10, Loss: 0.023456
# Finetune Epoch 15, Loss: 0.015678
# Finetune Epoch 20, Loss: 0.012345
</code></pre>

            <div class="success-box">
                <p><strong>âœ… è»¢ç§»å­¦ç¿’ã®åˆ©ç‚¹</strong></p>
                <ul>
                    <li><strong>ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡</strong>: å°‘é‡ã®ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã§é«˜ç²¾åº¦</li>
                    <li><strong>æ±åŒ–æ€§èƒ½</strong>: éå­¦ç¿’ã‚’æŠ‘åˆ¶</li>
                    <li><strong>å†·é–“èµ·å‹•å•é¡Œã®è§£æ±º</strong>: æ–°ãƒ—ãƒ­ã‚»ã‚¹ã§ã‚‚å³åº§ã«äºˆæ¸¬å¯èƒ½</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>2.8 Attentioné‡ã¿ã®å¯è¦–åŒ–ã¨è§£é‡ˆ</h2>

            <p>Transformerã®å¼·åŠ›ãªåˆ©ç‚¹ã¯ã€Attentioné‡ã¿ã‚’å¯è¦–åŒ–ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬æ ¹æ‹ ã‚’ç†è§£ã§ãã‚‹ã“ã¨ã§ã™ã€‚ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã«ãŠã„ã¦ã€ã©ã®æ™‚åˆ»ãƒ»å¤‰æ•°ãŒé‡è¦ã‹ã‚’æŠŠæ¡ã§ãã¾ã™ã€‚</p>

            <h3>ä¾‹8: Attentionå¯è¦–åŒ–</h3>

            <pre><code class="language-python">import matplotlib.pyplot as plt
import seaborn as sns

class InterpretableTransformer(nn.Module):
    """Attentioné‡ã¿ã‚’è¿”ã™Transformer"""

    def __init__(self, n_features, d_model=128, n_head=4, n_layers=3):
        super(InterpretableTransformer, self).__init__()

        self.embedding = nn.Linear(n_features, d_model)
        self.pos_encoder = PositionalEncoding(d_model)

        # ã‚«ã‚¹ã‚¿ãƒ Encoderï¼ˆAttentioné‡ã¿ã‚’ä¿å­˜ï¼‰
        self.encoder_layers = nn.ModuleList([
            MultiHeadAttention(n_head, d_model, d_model//n_head, d_model//n_head)
            for _ in range(n_layers)
        ])

        self.fc_out = nn.Linear(d_model, n_features)
        self.attention_weights = []  # Attentioné‡ã¿ã‚’ä¿å­˜

    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, n_features]
        Returns:
            output: [batch, n_features]
            attention_maps: List of [batch, n_head, seq_len, seq_len]
        """
        x = self.embedding(x)
        x = self.pos_encoder(x)

        self.attention_weights = []

        # å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®Attentionã‚’å–å¾—
        for layer in self.encoder_layers:
            x, attn = layer(x, x, x)
            self.attention_weights.append(attn.detach())

        output = self.fc_out(x[:, -1, :])

        return output, self.attention_weights

def visualize_attention(attention_weights, layer_idx=0, head_idx=0,
                       variable_names=None, save_path=None):
    """Attentioné‡ã¿ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–

    Args:
        attention_weights: List of attention maps
        layer_idx: å¯è¦–åŒ–ã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼
        head_idx: å¯è¦–åŒ–ã™ã‚‹ãƒ˜ãƒƒãƒ‰
        variable_names: å¤‰æ•°åã®ãƒªã‚¹ãƒˆ
    """
    attn = attention_weights[layer_idx][0, head_idx].numpy()  # [seq_len, seq_len]

    plt.figure(figsize=(10, 8))
    sns.heatmap(attn, cmap='RdYlBu_r', center=0,
                xticklabels=10, yticklabels=10,
                cbar_kws={'label': 'Attention Weight'})

    plt.xlabel('Key Position (Time Step)')
    plt.ylabel('Query Position (Time Step)')
    plt.title(f'Attention Map - Layer {layer_idx+1}, Head {head_idx+1}')

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')

    plt.tight_layout()
    # plt.show()

    print(f"Attention map visualized for Layer {layer_idx+1}, Head {head_idx+1}")

# ä½¿ç”¨ä¾‹
model = InterpretableTransformer(n_features=4, d_model=128, n_head=4, n_layers=3)

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
sample_data = torch.randn(1, 50, 4)  # 50æ™‚åˆ»åˆ†ã®ãƒ‡ãƒ¼ã‚¿

# äºˆæ¸¬ã¨Attentionå–å¾—
model.eval()
with torch.no_grad():
    pred, attn_weights = model(sample_data)

print(f"Prediction: {pred}")
print(f"Number of attention layers: {len(attn_weights)}")

# å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®Attentionå¯è¦–åŒ–
for layer_idx in range(len(attn_weights)):
    for head_idx in range(4):
        visualize_attention(attn_weights, layer_idx=layer_idx, head_idx=head_idx)

# ç‰¹å®šæ™‚åˆ»ã®Attentioné‡ã¿åˆ†æ
def analyze_critical_timesteps(attention_weights, query_timestep=-1):
    """ç‰¹å®šæ™‚åˆ»ãŒã©ã®éå»ã«æ³¨ç›®ã—ã¦ã„ã‚‹ã‹åˆ†æ"""
    layer_idx = -1  # æœ€çµ‚ãƒ¬ã‚¤ãƒ¤ãƒ¼
    attn = attention_weights[layer_idx][0]  # [n_head, seq_len, seq_len]

    # æœ€çµ‚æ™‚åˆ»ã®Attentionï¼ˆå…¨ãƒ˜ãƒƒãƒ‰å¹³å‡ï¼‰
    final_attn = attn[:, query_timestep, :].mean(dim=0)  # [seq_len]

    # Top-5ã®é‡è¦æ™‚åˆ»
    top_k = 5
    top_values, top_indices = torch.topk(final_attn, k=top_k)

    print(f"\nCritical timesteps for prediction (Top {top_k}):")
    for i, (idx, val) in enumerate(zip(top_indices, top_values)):
        print(f"  {i+1}. Time step {idx.item()}: weight = {val.item():.4f}")

analyze_critical_timesteps(attn_weights)

# å‡ºåŠ›ä¾‹:
# Prediction: tensor([[ 0.1234, -0.5678,  0.9012, -0.3456]])
# Number of attention layers: 3
# Attention map visualized for Layer 1, Head 1
# Attention map visualized for Layer 1, Head 2
# ...ï¼ˆå…¨12ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
#
# Critical timesteps for prediction (Top 5):
#   1. Time step 49: weight = 0.0876
#   2. Time step 48: weight = 0.0654
#   3. Time step 45: weight = 0.0543
#   4. Time step 40: weight = 0.0432
#   5. Time step 35: weight = 0.0321
# â†’ ç›´è¿‘5-10ã‚¹ãƒ†ãƒƒãƒ—ã¨ã€15ã‚¹ãƒ†ãƒƒãƒ—å‰ãŒé‡è¦
</code></pre>

            <div class="tip-box">
                <p><strong>ğŸ’¡ Attentionè§£é‡ˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</strong></p>
                <ul>
                    <li><strong>è¤‡æ•°ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ç¢ºèª</strong>: æµ…ã„å±¤ã¯å±€æ‰€çš„ã€æ·±ã„å±¤ã¯å¤§åŸŸçš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ‰ãˆã‚‹</li>
                    <li><strong>è¤‡æ•°ãƒ˜ãƒƒãƒ‰ã‚’æ¯”è¼ƒ</strong>: å„ãƒ˜ãƒƒãƒ‰ãŒç•°ãªã‚‹ä¾å­˜æ€§ã‚’å­¦ç¿’</li>
                    <li><strong>ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã¨ç…§åˆ</strong>: åŒ–å­¦çš„ã«å¦¥å½“ãªä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã‹æ¤œè¨¼</li>
                    <li><strong>ç•°å¸¸æ¤œçŸ¥ã«æ´»ç”¨</strong>: é€šå¸¸ã¨ç•°ãªã‚‹Attentionãƒ‘ã‚¿ãƒ¼ãƒ³ã§ç•°å¸¸ã‚’ç™ºè¦‹</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>å­¦ç¿’ç›®æ¨™ã®ç¢ºèª</h2>

            <p>ã“ã®ç« ã‚’å®Œäº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ã‚’å®Ÿè£…ãƒ»èª¬æ˜ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š</p>

            <h3>åŸºæœ¬ç†è§£</h3>
            <ul>
                <li>Self-Attentionæ©Ÿæ§‹ã®æ•°å¼ã¨è¨ˆç®—éç¨‹ã‚’èª¬æ˜ã§ãã‚‹</li>
                <li>Positional Encodingã®å¿…è¦æ€§ã¨å®Ÿè£…æ–¹æ³•ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
                <li>Multi-Head AttentionãŒè¤‡æ•°ã®ä¾å­˜æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ‰ãˆã‚‹ä»•çµ„ã¿ã‚’èª¬æ˜ã§ãã‚‹</li>
                <li>Transformerã¨LSTMã®æ ¹æœ¬çš„ãªé•ã„ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
            </ul>

            <h3>å®Ÿè·µã‚¹ã‚­ãƒ«</h3>
            <ul>
                <li>PyTorchã§Transformer Encoderã‚’å®Ÿè£…ã§ãã‚‹</li>
                <li>å¤šå¤‰é‡æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦Transformerãƒ¢ãƒ‡ãƒ«ã‚’é©ç”¨ã§ãã‚‹</li>
                <li>Temporal Fusion Transformerã§å¤‰æ•°é¸æŠã¨è§£é‡ˆå¯èƒ½ãªäºˆæ¸¬ãŒã§ãã‚‹</li>
                <li>äº‹å‰å­¦ç¿’ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã‚’æ”¹å–„ã§ãã‚‹</li>
                <li>Attentioné‡ã¿ã‚’å¯è¦–åŒ–ã—ã€äºˆæ¸¬æ ¹æ‹ ã‚’è§£é‡ˆã§ãã‚‹</li>
            </ul>

            <h3>å¿œç”¨åŠ›</h3>
            <ul>
                <li>ãƒ—ãƒ­ã‚»ã‚¹ã®ç‰¹æ€§ã«å¿œã˜ã¦LSTM vs Transformerã‚’ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹</li>
                <li>Attentionè§£æã‹ã‚‰åŒ–å­¦çš„çŸ¥è¦‹ï¼ˆé‡è¦ãªæ™‚é–“é…ã‚Œã€å¤‰æ•°é–“ç›¸äº’ä½œç”¨ï¼‰ã‚’æŠ½å‡ºã§ãã‚‹</li>
                <li>å°‘é‡ãƒ‡ãƒ¼ã‚¿ã®ãƒ—ãƒ­ã‚»ã‚¹ã«è»¢ç§»å­¦ç¿’ã‚’é©ç”¨ã§ãã‚‹</li>
                <li>å¤‰æ•°é¸æŠæ©Ÿæ§‹ã§é‡è¦ãªãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°ã‚’åŒå®šã§ãã‚‹</li>
            </ul>
        </section>

        <section>
            <h2>LSTM vs Transformeræ¯”è¼ƒ</h2>

            <table>
                <thead>
                    <tr>
                        <th>ç‰¹æ€§</th>
                        <th>LSTM</th>
                        <th>Transformer</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>å‡¦ç†æ–¹å¼</strong></td>
                        <td>é€æ¬¡å‡¦ç†ï¼ˆé…ã„ï¼‰</td>
                        <td>ä¸¦åˆ—å‡¦ç†ï¼ˆé«˜é€Ÿï¼‰</td>
                    </tr>
                    <tr>
                        <td><strong>é•·è·é›¢ä¾å­˜</strong></td>
                        <td>ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹ã§æ”¹å–„ï¼ˆé™ç•Œã‚ã‚Šï¼‰</td>
                        <td>ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆå„ªã‚Œã‚‹ï¼‰</td>
                    </tr>
                    <tr>
                        <td><strong>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°</strong></td>
                        <td>å°‘ãªã„</td>
                        <td>å¤šã„</td>
                    </tr>
                    <tr>
                        <td><strong>ãƒ‡ãƒ¼ã‚¿é‡è¦ä»¶</strong></td>
                        <td>å°‘é‡ã§ã‚‚å­¦ç¿’å¯èƒ½</td>
                        <td>å¤§é‡ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦</td>
                    </tr>
                    <tr>
                        <td><strong>è§£é‡ˆæ€§</strong></td>
                        <td>Attentionè¿½åŠ ã§å¯èƒ½</td>
                        <td>æ¨™æº–ã§é«˜ã„è§£é‡ˆæ€§</td>
                    </tr>
                    <tr>
                        <td><strong>è¨“ç·´æ™‚é–“</strong></td>
                        <td>çŸ­ã„</td>
                        <td>é•·ã„</td>
                    </tr>
                    <tr>
                        <td><strong>æ¨è«–é€Ÿåº¦</strong></td>
                        <td>é…ã„ï¼ˆé€æ¬¡ï¼‰</td>
                        <td>é€Ÿã„ï¼ˆä¸¦åˆ—ï¼‰</td>
                    </tr>
                    <tr>
                        <td><strong>é©ç”¨å ´é¢</strong></td>
                        <td>å°ã€œä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†</td>
                        <td>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã€é«˜ç²¾åº¦è¦æ±‚</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section>
            <h2>å‚è€ƒæ–‡çŒ®</h2>
            <ol>
                <li>Vaswani, A., et al. (2017). "Attention is All You Need." NeurIPS 2017.</li>
                <li>Lim, B., et al. (2021). "Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting." International Journal of Forecasting, 37(4), 1748-1764.</li>
                <li>Devlin, J., et al. (2019). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding." NAACL 2019.</li>
                <li>Zhou, H., et al. (2021). "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting." AAAI 2021.</li>
                <li>Wu, N., et al. (2020). "Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case." arXiv:2001.08317.</li>
            </ol>
        </section>

        <div class="navigation">
            <a href="chapter-1.html" class="nav-button">â† ç¬¬1ç« ï¼šRNN/LSTM</a>
            <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a>
        </div>
    </main>
</body>
</html>
