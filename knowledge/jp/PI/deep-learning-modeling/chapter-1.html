<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="ç¬¬1ç« ï¼šRNN/LSTMã«ã‚ˆã‚‹æ™‚ç³»åˆ—äºˆæ¸¬ - ãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°ã®äºˆæ¸¬ã¨ãƒ¢ãƒ‡ãƒªãƒ³ã‚°">
    <title>ç¬¬1ç« ï¼šRNN/LSTMã«ã‚ˆã‚‹æ™‚ç³»åˆ—äºˆæ¸¬ - æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚° | PI Terakoya</title>

        <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.8; color: #333; background: #f5f5f5;
        }
        header {
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white; padding: 2rem 1rem; text-align: center;
        }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; }
        .subtitle { opacity: 0.9; font-size: 1.1rem; }
        .container { max-width: 1200px; margin: 2rem auto; padding: 0 1rem; }
        .back-link {
            display: inline-block; margin-bottom: 2rem; padding: 0.5rem 1rem;
            background: white; color: #11998e; text-decoration: none;
            border-radius: 6px; font-weight: 600;
        }
        .content-box {
            background: white; padding: 2rem; border-radius: 12px;
            margin-bottom: 2rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        h2 {
            color: #11998e; margin: 2rem 0 1rem 0;
            padding-bottom: 0.5rem; border-bottom: 3px solid #11998e;
        }
        h3 { color: #2c3e50; margin: 1.5rem 0 1rem 0; }
        h4 { color: #2c3e50; margin: 1rem 0 0.5rem 0; }
        p { margin-bottom: 1rem; }
        ul, ol { margin-left: 2rem; margin-bottom: 1rem; }
        li { margin-bottom: 0.5rem; }
        pre {
            background: #1e1e1e; color: #d4d4d4; padding: 1.5rem;
            border-radius: 8px; overflow-x: auto; margin: 1rem 0;
            border-left: 4px solid #11998e;
        }
        code {
            font-family: 'Courier New', monospace; font-size: 0.9rem;
        }
        .key-point {
            background: #e8f5e9; padding: 1rem; border-radius: 6px;
            border-left: 4px solid #4caf50; margin: 1rem 0;
        }
        .tech-note {
            background: #e3f2fd; padding: 1rem; border-radius: 6px;
            border-left: 4px solid #2196f3; margin: 1rem 0;
        }
        .formula {
            background: #f0f7ff; padding: 1rem; border-radius: 6px;
            margin: 1rem 0; overflow-x: auto;
        }
        table {
            width: 100%; border-collapse: collapse; margin: 1rem 0;
        }
        th, td {
            border: 1px solid #ddd; padding: 0.75rem; text-align: left;
        }
        th {
            background: #11998e; color: white; font-weight: 600;
        }
        tr:nth-child(even) { background: #f9f9f9; }
        .nav-buttons {
            display: flex; justify-content: space-between; margin-top: 3rem;
        }
        .nav-buttons a {
            padding: 0.75rem 1.5rem;
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white; text-decoration: none; border-radius: 6px;
            font-weight: 600;
        }
        footer {
            background: #2c3e50; color: white; text-align: center;
            padding: 2rem 1rem; margin-top: 4rem;
        }
        @media (max-width: 768px) {
            h1 { font-size: 1.6rem; }
            .container { padding: 0 0.5rem; }
            pre { padding: 1rem; }
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <div class="container">
            <h1>ç¬¬1ç« ï¼šRNN/LSTMã«ã‚ˆã‚‹æ™‚ç³»åˆ—äºˆæ¸¬</h1>
            <p class="subtitle">ãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°ã®é€æ¬¡äºˆæ¸¬ã¨æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒªãƒ³ã‚°</p>
            <div class="meta">
                <span class="meta">ğŸ“– èª­äº†æ™‚é–“: 30-35åˆ†</span>
                <span class="meta">ğŸ’¡ é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta">ğŸ”¬ å®Ÿä¾‹: åå¿œå™¨æ¸©åº¦ãƒ»åœ§åŠ›äºˆæ¸¬</span>
            </div>
        </div>
    </header>

    <main class="container">
        <section>
            <h2>1.1 RNNåŸºç¤ã¨ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³</h2>

            <p>ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆRNNï¼‰ã¯ã€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®é€æ¬¡çš„ãªä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã§ãã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚åŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹ã«ãŠã‘ã‚‹æ¸©åº¦ãƒ»åœ§åŠ›ãƒ»æµé‡ãªã©ã®æ™‚ç³»åˆ—å¤‰æ•°ã¯ã€éå»ã®çŠ¶æ…‹ã«ä¾å­˜ã—ã¦å¤‰åŒ–ã™ã‚‹ãŸã‚ã€RNNãŒæœ‰åŠ¹ã§ã™ã€‚</p>

            <div class="info-box">
                <p><strong>ğŸ’¡ RNNã®åŸºæœ¬åŸç†</strong></p>
                <ul>
                    <li><strong>éš ã‚ŒçŠ¶æ…‹</strong>: éå»ã®æƒ…å ±ã‚’ä¿æŒã™ã‚‹å†…éƒ¨ãƒ¡ãƒ¢ãƒª</li>
                    <li><strong>æ™‚é–“å±•é–‹</strong>: åŒã˜é‡ã¿ã‚’å„æ™‚åˆ»ã§å…±æœ‰</li>
                    <li><strong>é€æ¬¡å‡¦ç†</strong>: å…¥åŠ›ã‚’æ™‚ç³»åˆ—ã«æ²¿ã£ã¦å‡¦ç†</li>
                </ul>
            </div>

            <p>RNNã®æ›´æ–°å¼ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š</p>

            <p>$$h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$</p>
            <p>$$y_t = W_{hy} h_t + b_y$$</p>

            <h3>ä¾‹1: Vanilla RNNå®Ÿè£…ï¼ˆåå¿œå™¨æ¸©åº¦äºˆæ¸¬ï¼‰</h3>

            <pre><code class="language-python">import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

# ç°¡æ˜“RNNã‚»ãƒ«ã®å®Ÿè£…
class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        """ã‚·ãƒ³ãƒ—ãƒ«ãªRNN

        Args:
            input_size: å…¥åŠ›æ¬¡å…ƒï¼ˆä¾‹ï¼šæ¸©åº¦1å¤‰æ•°ãªã‚‰1ï¼‰
            hidden_size: éš ã‚Œå±¤æ¬¡å…ƒ
            output_size: å‡ºåŠ›æ¬¡å…ƒï¼ˆäºˆæ¸¬å¤‰æ•°æ•°ï¼‰
        """
        super(SimpleRNN, self).__init__()
        self.hidden_size = hidden_size

        # é‡ã¿è¡Œåˆ—
        self.W_xh = nn.Linear(input_size, hidden_size)  # å…¥åŠ›â†’éš ã‚Œ
        self.W_hh = nn.Linear(hidden_size, hidden_size)  # éš ã‚Œâ†’éš ã‚Œ
        self.W_hy = nn.Linear(hidden_size, output_size)  # éš ã‚Œâ†’å‡ºåŠ›

    def forward(self, x, h_prev):
        """1ã‚¹ãƒ†ãƒƒãƒ—ã®æ›´æ–°

        Args:
            x: ç¾åœ¨ã®å…¥åŠ› [batch, input_size]
            h_prev: å‰æ™‚åˆ»ã®éš ã‚ŒçŠ¶æ…‹ [batch, hidden_size]
        """
        # éš ã‚ŒçŠ¶æ…‹ã®æ›´æ–°
        h = torch.tanh(self.W_xh(x) + self.W_hh(h_prev))
        # å‡ºåŠ›ã®è¨ˆç®—
        y = self.W_hy(h)
        return y, h

# åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆåå¿œå™¨æ¸©åº¦ã®æ™‚ç³»åˆ—ï¼‰
np.random.seed(42)
time = np.linspace(0, 50, 500)
# åŸºæº–æ¸©åº¦350K + å‘¨æœŸå¤‰å‹• + ãƒã‚¤ã‚º
temperature = 350 + 20*np.sin(0.2*time) + 5*np.random.randn(len(time))

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
data = torch.FloatTensor(temperature).unsqueeze(1)  # [500, 1]

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = SimpleRNN(input_size=1, hidden_size=32, output_size=1)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# è¨“ç·´ãƒ«ãƒ¼ãƒ—
seq_length = 20  # éå»20ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰æ¬¡ã‚’äºˆæ¸¬
for epoch in range(100):
    total_loss = 0
    h = torch.zeros(1, 32)  # åˆæœŸéš ã‚ŒçŠ¶æ…‹

    for i in range(seq_length, len(data)):
        # å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
        x_seq = data[i-seq_length:i]
        target = data[i]

        # é€æ¬¡äºˆæ¸¬
        h = torch.zeros(1, 32)  # ãƒªã‚»ãƒƒãƒˆ
        for t in range(seq_length):
            _, h = model(x_seq[t:t+1], h)

        # æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ã§äºˆæ¸¬
        pred, h = model(x_seq[-1:], h)

        loss = criterion(pred, target)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        h = h.detach()  # å‹¾é…ã‚’åˆ‡æ–­

    if (epoch+1) % 20 == 0:
        print(f'Epoch {epoch+1}, Loss: {total_loss/(len(data)-seq_length):.4f}')

# å‡ºåŠ›ä¾‹:
# Epoch 20, Loss: 15.3421
# Epoch 40, Loss: 8.7654
# Epoch 60, Loss: 5.2341
# Epoch 80, Loss: 3.8765
# Epoch 100, Loss: 2.9123
</code></pre>

            <div class="warning-box">
                <p><strong>âš ï¸ Vanilla RNNã®é™ç•Œ</strong></p>
                <p>å‹¾é…æ¶ˆå¤±å•é¡Œã«ã‚ˆã‚Šã€é•·æœŸä¾å­˜æ€§ï¼ˆ100ã‚¹ãƒ†ãƒƒãƒ—ä»¥ä¸Šï¼‰ã‚’å­¦ç¿’ã§ãã¾ã›ã‚“ã€‚ãƒ—ãƒ­ã‚»ã‚¹ç”£æ¥­ã§ã¯æ•°æ™‚é–“ã€œæ•°æ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†ãŸã‚ã€LSTM/GRUãŒå¿…è¦ã§ã™ã€‚</p>
            </div>
        </section>

        <section>
            <h2>1.2 LSTM Architecture</h2>

            <p>Long Short-Term Memoryï¼ˆLSTMï¼‰ã¯ã€ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹ã«ã‚ˆã‚Šé•·æœŸä¾å­˜æ€§ã‚’å­¦ç¿’ã§ãã‚‹æ”¹è‰¯å‹RNNã§ã™ã€‚å¿˜å´ã‚²ãƒ¼ãƒˆãƒ»å…¥åŠ›ã‚²ãƒ¼ãƒˆãƒ»å‡ºåŠ›ã‚²ãƒ¼ãƒˆã®3ã¤ã®ã‚²ãƒ¼ãƒˆã§æƒ…å ±ã®æµã‚Œã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚</p>

            <p>LSTMã®æ›´æ–°å¼ï¼š</p>

            <p>$$f_t = \sigma(W_f [h_{t-1}, x_t] + b_f) \quad \text{(å¿˜å´ã‚²ãƒ¼ãƒˆ)}$$</p>
            <p>$$i_t = \sigma(W_i [h_{t-1}, x_t] + b_i) \quad \text{(å…¥åŠ›ã‚²ãƒ¼ãƒˆ)}$$</p>
            <p>$$\tilde{C}_t = \tanh(W_C [h_{t-1}, x_t] + b_C) \quad \text{(å€™è£œã‚»ãƒ«)}$$</p>
            <p>$$C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \quad \text{(ã‚»ãƒ«çŠ¶æ…‹æ›´æ–°)}$$</p>
            <p>$$o_t = \sigma(W_o [h_{t-1}, x_t] + b_o) \quad \text{(å‡ºåŠ›ã‚²ãƒ¼ãƒˆ)}$$</p>
            <p>$$h_t = o_t \odot \tanh(C_t) \quad \text{(éš ã‚ŒçŠ¶æ…‹)}$$</p>

            <h3>ä¾‹2: LSTMå®Ÿè£…ï¼ˆåå¿œå™¨åœ§åŠ›äºˆæ¸¬ï¼‰</h3>

            <pre><code class="language-python">import torch.nn as nn

# LSTM ãƒ¢ãƒ‡ãƒ«
class ProcessLSTM(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=1):
        """ãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°äºˆæ¸¬ç”¨LSTM

        Args:
            input_size: å…¥åŠ›å¤‰æ•°æ•°
            hidden_size: LSTMéš ã‚Œå±¤ã‚µã‚¤ã‚º
            num_layers: LSTMãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
            output_size: å‡ºåŠ›å¤‰æ•°æ•°
        """
        super(ProcessLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        """é †ä¼æ’­

        Args:
            x: [batch, seq_len, input_size]
        Returns:
            out: [batch, output_size]
        """
        # LSTMã¯å†…éƒ¨ã§éš ã‚ŒçŠ¶æ…‹ã‚’ç®¡ç†
        lstm_out, _ = self.lstm(x)  # [batch, seq_len, hidden_size]

        # æœ€å¾Œã®æ™‚åˆ»ã®å‡ºåŠ›ã‚’ä½¿ç”¨
        out = self.fc(lstm_out[:, -1, :])  # [batch, output_size]
        return out

# åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆåå¿œå™¨åœ§åŠ›: 1-10 barï¼‰
time = np.linspace(0, 100, 1000)
pressure = 5 + 2*np.sin(0.1*time) + 0.5*np.cos(0.3*time) + 0.3*np.random.randn(len(time))

# ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒ‡ãƒ¼ã‚¿ä½œæˆ
def create_windows(data, window_size=50):
    """æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«åˆ†å‰²"""
    X, y = [], []
    for i in range(len(data) - window_size):
        X.append(data[i:i+window_size])
        y.append(data[i+window_size])
    return np.array(X), np.array(y)

X, y = create_windows(pressure, window_size=50)
X = torch.FloatTensor(X).unsqueeze(2)  # [samples, 50, 1]
y = torch.FloatTensor(y).unsqueeze(1)  # [samples, 1]

# è¨“ç·´/ãƒ†ã‚¹ãƒˆåˆ†å‰²
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = ProcessLSTM(input_size=1, hidden_size=64, num_layers=2)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(50):
    model.train()
    optimizer.zero_grad()

    # äºˆæ¸¬
    pred = model(X_train)
    loss = criterion(pred, y_train)

    # é€†ä¼æ’­
    loss.backward()
    optimizer.step()

    if (epoch+1) % 10 == 0:
        model.eval()
        with torch.no_grad():
            test_pred = model(X_test)
            test_loss = criterion(test_pred, y_test)
        print(f'Epoch {epoch+1}, Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}')

# å‡ºåŠ›ä¾‹:
# Epoch 10, Train Loss: 0.0523, Test Loss: 0.0587
# Epoch 20, Train Loss: 0.0234, Test Loss: 0.0298
# Epoch 30, Train Loss: 0.0156, Test Loss: 0.0201
# Epoch 40, Train Loss: 0.0112, Test Loss: 0.0167
# Epoch 50, Train Loss: 0.0089, Test Loss: 0.0145
</code></pre>
        </section>

        <section>
            <h2>1.3 GRU (Gated Recurrent Unit)</h2>

            <p>GRUã¯LSTMã‚’ç°¡ç•¥åŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã€2ã¤ã®ã‚²ãƒ¼ãƒˆï¼ˆãƒªã‚»ãƒƒãƒˆã‚²ãƒ¼ãƒˆãƒ»æ›´æ–°ã‚²ãƒ¼ãƒˆï¼‰ã®ã¿ã§æ§‹æˆã•ã‚Œã¾ã™ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå°‘ãªãã€è¨“ç·´ãŒé«˜é€Ÿã§ã™ã€‚</p>

            <h3>ä¾‹3: GRUå®Ÿè£…ã¨LSTMã¨ã®æ¯”è¼ƒ</h3>

            <pre><code class="language-python">class ProcessGRU(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=1):
        """GRUãƒ™ãƒ¼ã‚¹ã®ãƒ—ãƒ­ã‚»ã‚¹äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«"""
        super(ProcessGRU, self).__init__()
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        gru_out, _ = self.gru(x)
        out = self.fc(gru_out[:, -1, :])
        return out

# æ€§èƒ½æ¯”è¼ƒ
def compare_models(X_train, y_train, X_test, y_test):
    """LSTM vs GRUã®æ€§èƒ½æ¯”è¼ƒ"""
    results = {}

    for name, ModelClass in [('LSTM', ProcessLSTM), ('GRU', ProcessGRU)]:
        model = ModelClass(input_size=1, hidden_size=64, num_layers=2)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.MSELoss()

        # è¨“ç·´
        for epoch in range(30):
            model.train()
            optimizer.zero_grad()
            pred = model(X_train)
            loss = criterion(pred, y_train)
            loss.backward()
            optimizer.step()

        # è©•ä¾¡
        model.eval()
        with torch.no_grad():
            test_pred = model(X_test)
            test_loss = criterion(test_pred, y_test).item()

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
        n_params = sum(p.numel() for p in model.parameters())

        results[name] = {'test_loss': test_loss, 'n_params': n_params}

    return results

results = compare_models(X_train, y_train, X_test, y_test)
for name, metrics in results.items():
    print(f"{name}: Test Loss={metrics['test_loss']:.4f}, Parameters={metrics['n_params']:,}")

# å‡ºåŠ›ä¾‹:
# LSTM: Test Loss=0.0145, Parameters=50,241
# GRU: Test Loss=0.0152, Parameters=37,825
# â†’ GRUã¯ç´„25%å°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§åŒç­‰ã®æ€§èƒ½
</code></pre>

            <div class="tip-box">
                <p><strong>ğŸ’¡ ãƒ¢ãƒ‡ãƒ«é¸æŠã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³</strong></p>
                <ul>
                    <li><strong>LSTM</strong>: é•·æœŸä¾å­˜æ€§ãŒé‡è¦ï¼ˆ100+ ã‚¹ãƒ†ãƒƒãƒ—ï¼‰ã€ç²¾åº¦å„ªå…ˆ</li>
                    <li><strong>GRU</strong>: ãƒ‡ãƒ¼ã‚¿é‡ãŒå°‘ãªã„ã€è¨“ç·´é€Ÿåº¦é‡è¦–</li>
                    <li><strong>Vanilla RNN</strong>: çŸ­æœŸä¾å­˜ï¼ˆ<20ã‚¹ãƒ†ãƒƒãƒ—ï¼‰ã€è¨ˆç®—ã‚³ã‚¹ãƒˆæœ€å°åŒ–</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>1.4 æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†</h2>

            <p>ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã¯é€šå¸¸ã€ã‚¹ã‚±ãƒ¼ãƒ«ãŒç•°ãªã‚‹è¤‡æ•°ã®å¤‰æ•°ï¼ˆæ¸©åº¦: 300-500Kã€åœ§åŠ›: 1-10 barï¼‰ã‚’å«ã¿ã¾ã™ã€‚é©åˆ‡ãªæ­£è¦åŒ–ã¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åˆ†å‰²ãŒé‡è¦ã§ã™ã€‚</p>

            <h3>ä¾‹4: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h3>

            <pre><code class="language-python">import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler

class ProcessDataPreprocessor:
    """ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, window_size=50, normalization='standard'):
        """
        Args:
            window_size: å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
            normalization: 'standard' (æ¨™æº–åŒ–) or 'minmax' (0-1æ­£è¦åŒ–)
        """
        self.window_size = window_size
        self.normalization = normalization

        if normalization == 'standard':
            self.scaler = StandardScaler()
        else:
            self.scaler = MinMaxScaler()

    def fit_transform(self, data):
        """ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–ã¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åˆ†å‰²

        Args:
            data: numpy array [time_steps, features]
        Returns:
            X: [samples, window_size, features]
            y: [samples, features]
        """
        # æ­£è¦åŒ–
        data_normalized = self.scaler.fit_transform(data)

        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åˆ†å‰²
        X, y = [], []
        for i in range(len(data_normalized) - self.window_size):
            X.append(data_normalized[i:i+self.window_size])
            y.append(data_normalized[i+self.window_size])

        return np.array(X), np.array(y)

    def inverse_transform(self, data):
        """æ­£è¦åŒ–ã®é€†å¤‰æ›"""
        return self.scaler.inverse_transform(data)

# å®Ÿãƒ‡ãƒ¼ã‚¿ä¾‹ï¼ˆåå¿œå™¨ã®æ¸©åº¦ãƒ»åœ§åŠ›ãƒ»æµé‡ï¼‰
np.random.seed(42)
n_samples = 1000
data = pd.DataFrame({
    'temperature': 350 + 50*np.sin(np.linspace(0, 10, n_samples)) + 10*np.random.randn(n_samples),
    'pressure': 5 + 2*np.cos(np.linspace(0, 10, n_samples)) + 0.5*np.random.randn(n_samples),
    'flow_rate': 100 + 20*np.sin(np.linspace(0, 15, n_samples)) + 5*np.random.randn(n_samples)
})

# å‰å‡¦ç†
preprocessor = ProcessDataPreprocessor(window_size=50, normalization='standard')
X, y = preprocessor.fit_transform(data.values)

print(f"Input shape: {X.shape}")   # [950, 50, 3]
print(f"Target shape: {y.shape}")  # [950, 3]
print(f"Data range before: T=[{data['temperature'].min():.1f}, {data['temperature'].max():.1f}]K")
print(f"Data range after: X=[{X.min():.2f}, {X.max():.2f}] (æ¨™æº–åŒ–)")

# å‡ºåŠ›ä¾‹:
# Input shape: (950, 50, 3)
# Target shape: (950, 3)
# Data range before: T=[285.4, 414.6]K
# Data range after: X=[-3.12, 3.24] (æ¨™æº–åŒ–)
</code></pre>
        </section>

        <section>
            <h2>1.5 Single-Stepäºˆæ¸¬</h2>

            <p>1ã‚¹ãƒ†ãƒƒãƒ—å…ˆã®äºˆæ¸¬ã¯æœ€ã‚‚åŸºæœ¬çš„ãªã‚¿ã‚¹ã‚¯ã§ã™ã€‚åå¿œå™¨æ¸©åº¦ã‚„åœ§åŠ›ã®æ¬¡ã®æ™‚åˆ»ã®å€¤ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚</p>

            <h3>ä¾‹5: å˜å¤‰é‡1ã‚¹ãƒ†ãƒƒãƒ—äºˆæ¸¬ï¼ˆæ¸©åº¦ï¼‰</h3>

            <pre><code class="language-python">class SingleStepPredictor(nn.Module):
    """1ã‚¹ãƒ†ãƒƒãƒ—äºˆæ¸¬ç”¨LSTMãƒ¢ãƒ‡ãƒ«"""

    def __init__(self, n_features=1, hidden_size=128, num_layers=3):
        super(SingleStepPredictor, self).__init__()
        self.lstm = nn.LSTM(n_features, hidden_size, num_layers,
                           batch_first=True, dropout=0.2)
        self.fc = nn.Linear(hidden_size, n_features)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        pred = self.fc(lstm_out[:, -1, :])
        return pred

# è¨“ç·´é–¢æ•°
def train_single_step(model, X_train, y_train, epochs=50, lr=0.001):
    """1ã‚¹ãƒ†ãƒƒãƒ—äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    losses = []
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()

        pred = model(X_train)
        loss = criterion(pred, y_train)

        loss.backward()
        optimizer.step()

        losses.append(loss.item())

        if (epoch+1) % 10 == 0:
            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}')

    return losses

# åˆæˆãƒ‡ãƒ¼ã‚¿ï¼ˆåå¿œå™¨æ¸©åº¦: 300-500Kï¼‰
time = np.linspace(0, 50, 500)
temp_data = 400 + 50*np.sin(0.2*time) + 10*np.random.randn(len(time))

# å‰å‡¦ç†
preprocessor = ProcessDataPreprocessor(window_size=30)
X, y = preprocessor.fit_transform(temp_data.reshape(-1, 1))

# Tensorå¤‰æ›
X = torch.FloatTensor(X)
y = torch.FloatTensor(y)

# è¨“ç·´
model = SingleStepPredictor(n_features=1, hidden_size=128, num_layers=3)
losses = train_single_step(model, X, y, epochs=50)

# å‡ºåŠ›ä¾‹:
# Epoch 10/50, Loss: 0.045231
# Epoch 20/50, Loss: 0.018765
# Epoch 30/50, Loss: 0.009876
# Epoch 40/50, Loss: 0.006543
# Epoch 50/50, Loss: 0.005234
</code></pre>
        </section>

        <section>
            <h2>1.6 Multi-Stepäºˆæ¸¬ (Sequence-to-Sequence)</h2>

            <p>è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—å…ˆã‚’äºˆæ¸¬ã™ã‚‹ã«ã¯ã€Sequence-to-Sequenceï¼ˆSeq2Seqï¼‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚Encoderã§éå»ã‚’ç¬¦å·åŒ–ã—ã€Decoderã§æœªæ¥ã‚’ç”Ÿæˆã—ã¾ã™ã€‚</p>

            <h3>ä¾‹6: å¤šæ®µéšäºˆæ¸¬ï¼ˆ10ã‚¹ãƒ†ãƒƒãƒ—å…ˆã¾ã§ï¼‰</h3>

            <pre><code class="language-python">class Seq2SeqLSTM(nn.Module):
    """Sequence-to-Sequenceãƒ¢ãƒ‡ãƒ«"""

    def __init__(self, input_size=1, hidden_size=128, num_layers=2, output_steps=10):
        super(Seq2SeqLSTM, self).__init__()
        self.output_steps = output_steps

        # Encoder
        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)

        # Decoder
        self.decoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, input_size)

    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, input_size] å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
        Returns:
            outputs: [batch, output_steps, input_size] äºˆæ¸¬ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
        """
        batch_size = x.size(0)

        # Encoderã§éå»ã‚’ç¬¦å·åŒ–
        _, (h, c) = self.encoder(x)

        # Decoderã§æœªæ¥ã‚’ç”Ÿæˆ
        decoder_input = x[:, -1:, :]  # æœ€å¾Œã®å€¤ã‹ã‚‰é–‹å§‹
        outputs = []

        for _ in range(self.output_steps):
            decoder_output, (h, c) = self.decoder(decoder_input, (h, c))
            pred = self.fc(decoder_output)
            outputs.append(pred)
            decoder_input = pred  # æ¬¡ã®å…¥åŠ›ã«ä½¿ç”¨

        outputs = torch.cat(outputs, dim=1)  # [batch, output_steps, input_size]
        return outputs

# ãƒãƒ«ãƒã‚¹ãƒ†ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ä½œæˆ
def create_multistep_data(data, input_len=50, output_len=10):
    """ãƒãƒ«ãƒã‚¹ãƒ†ãƒƒãƒ—äºˆæ¸¬ç”¨ãƒ‡ãƒ¼ã‚¿"""
    X, y = [], []
    for i in range(len(data) - input_len - output_len):
        X.append(data[i:i+input_len])
        y.append(data[i+input_len:i+input_len+output_len])
    return np.array(X), np.array(y)

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
pressure_data = 5 + 2*np.sin(0.1*np.linspace(0, 100, 1000)) + 0.3*np.random.randn(1000)
X_multi, y_multi = create_multistep_data(pressure_data, input_len=50, output_len=10)

X_multi = torch.FloatTensor(X_multi).unsqueeze(2)  # [samples, 50, 1]
y_multi = torch.FloatTensor(y_multi).unsqueeze(2)  # [samples, 10, 1]

# è¨“ç·´
model = Seq2SeqLSTM(input_size=1, hidden_size=128, output_steps=10)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(30):
    model.train()
    optimizer.zero_grad()

    pred = model(X_multi)
    loss = criterion(pred, y_multi)

    loss.backward()
    optimizer.step()

    if (epoch+1) % 5 == 0:
        print(f'Epoch {epoch+1}, Loss: {loss.item():.6f}')

# å‡ºåŠ›ä¾‹:
# Epoch 5, Loss: 0.123456
# Epoch 10, Loss: 0.056789
# Epoch 15, Loss: 0.034567
# Epoch 20, Loss: 0.023456
# Epoch 25, Loss: 0.018765
# Epoch 30, Loss: 0.015432
</code></pre>
        </section>

        <section>
            <h2>1.7 Bidirectional LSTM</h2>

            <p>Bidirectional LSTMã¯ã€éå»ã¨æœªæ¥ã®ä¸¡æ–¹å‘ã‹ã‚‰æƒ…å ±ã‚’çµ±åˆã—ã¾ã™ã€‚ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸æ¤œçŸ¥ã‚„ã€å®Œå…¨ãªæ™‚ç³»åˆ—ãŒå¾—ã‚‰ã‚Œã¦ã„ã‚‹å ´åˆã®è§£æã«æœ‰åŠ¹ã§ã™ã€‚</p>

            <h3>ä¾‹7: åŒæ–¹å‘LSTMã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥</h3>

            <pre><code class="language-python">class BidirectionalProcessLSTM(nn.Module):
    """åŒæ–¹å‘LSTMã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥"""

    def __init__(self, input_size=1, hidden_size=64, num_layers=2):
        super(BidirectionalProcessLSTM, self).__init__()

        # bidirectional=True ã§åŒæ–¹å‘LSTM
        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers,
                             batch_first=True, bidirectional=True)

        # åŒæ–¹å‘ãªã®ã§ hidden_size * 2
        self.fc = nn.Linear(hidden_size * 2, input_size)

    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, input_size]
        Returns:
            reconstructed: [batch, seq_len, input_size]
        """
        bilstm_out, _ = self.bilstm(x)  # [batch, seq_len, hidden_size*2]
        reconstructed = self.fc(bilstm_out)  # [batch, seq_len, input_size]
        return reconstructed

# æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´
normal_data = 400 + 30*np.sin(0.1*np.linspace(0, 100, 500)) + 5*np.random.randn(500)

# ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆ200-250ã‚¹ãƒ†ãƒƒãƒ—ã«ç•°å¸¸å€¤ï¼‰
test_data = normal_data.copy()
test_data[200:250] += 50  # ç•°å¸¸: æ¸©åº¦ãŒæ€¥ä¸Šæ˜‡

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X_normal, _ = create_windows(normal_data, window_size=50)
X_normal = torch.FloatTensor(X_normal).unsqueeze(2)

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆå†æ§‹æˆã‚¿ã‚¹ã‚¯ï¼‰
model = BidirectionalProcessLSTM(input_size=1, hidden_size=64)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(50):
    model.train()
    optimizer.zero_grad()

    # å…¥åŠ›ã‚’å†æ§‹æˆ
    reconstructed = model(X_normal)
    loss = criterion(reconstructed, X_normal)

    loss.backward()
    optimizer.step()

# ç•°å¸¸æ¤œçŸ¥
model.eval()
with torch.no_grad():
    X_test, _ = create_windows(test_data, window_size=50)
    X_test = torch.FloatTensor(X_test).unsqueeze(2)

    reconstructed_test = model(X_test)
    reconstruction_error = torch.mean((X_test - reconstructed_test)**2, dim=(1, 2))

    # é–¾å€¤ã‚’è¶…ãˆã‚‹ç®‡æ‰€ã‚’ç•°å¸¸ã¨ã—ã¦æ¤œå‡º
    threshold = torch.quantile(reconstruction_error, 0.95)
    anomalies = reconstruction_error > threshold

    print(f"æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸: {anomalies.sum().item()} / {len(anomalies)} ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦")
    print(f"ç•°å¸¸ã‚¹ã‚³ã‚¢ç¯„å›²: [{reconstruction_error.min():.4f}, {reconstruction_error.max():.4f}]")

# å‡ºåŠ›ä¾‹:
# æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸: 47 / 450 ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
# ç•°å¸¸ã‚¹ã‚³ã‚¢ç¯„å›²: [0.0023, 0.3456]
# â†’ ç•°å¸¸æ³¨å…¥åŒºé–“ï¼ˆ200-250ï¼‰ã§å†æ§‹æˆèª¤å·®ãŒå¢—å¤§
</code></pre>
        </section>

        <section>
            <h2>1.8 Attentionæ©Ÿæ§‹ã«ã‚ˆã‚‹è§£é‡ˆæ€§å‘ä¸Š</h2>

            <p>Attentionæ©Ÿæ§‹ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒã©ã®æ™‚åˆ»ã®æƒ…å ±ã‚’é‡è¦–ã—ã¦ã„ã‚‹ã‹ã‚’å¯è¦–åŒ–ã§ãã¾ã™ã€‚ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã«ãŠã„ã¦ã€ã©ã®éå»ã®ã‚¤ãƒ™ãƒ³ãƒˆãŒç¾åœ¨ã®çŠ¶æ…‹ã«å½±éŸ¿ã—ã¦ã„ã‚‹ã‹ã‚’ç†è§£ã§ãã¾ã™ã€‚</p>

            <h3>ä¾‹8: Attentionä»˜ãLSTM</h3>

            <pre><code class="language-python">class AttentionLSTM(nn.Module):
    """Attentionæ©Ÿæ§‹ä»˜ãLSTM"""

    def __init__(self, input_size=1, hidden_size=128, num_layers=2):
        super(AttentionLSTM, self).__init__()

        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)

        # Attentioné‡ã¿è¨ˆç®—
        self.attention = nn.Linear(hidden_size, 1)

        # æœ€çµ‚äºˆæ¸¬
        self.fc = nn.Linear(hidden_size, input_size)

    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, input_size]
        Returns:
            output: [batch, input_size]
            attention_weights: [batch, seq_len] (è§£é‡ˆç”¨)
        """
        # LSTMå‡¦ç†
        lstm_out, _ = self.lstm(x)  # [batch, seq_len, hidden_size]

        # Attentioné‡ã¿ã®è¨ˆç®—
        attention_scores = self.attention(lstm_out)  # [batch, seq_len, 1]
        attention_weights = torch.softmax(attention_scores, dim=1)  # [batch, seq_len, 1]

        # é‡ã¿ä»˜ãå’Œï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰
        context = torch.sum(attention_weights * lstm_out, dim=1)  # [batch, hidden_size]

        # äºˆæ¸¬
        output = self.fc(context)  # [batch, input_size]

        return output, attention_weights.squeeze(2)

# è¨“ç·´ã¨è§£é‡ˆ
model = AttentionLSTM(input_size=1, hidden_size=128)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆåå¿œå™¨æ¸©åº¦ã€åˆæœŸã«æ€¥æ¿€ãªå¤‰åŒ–ï¼‰
time = np.linspace(0, 50, 500)
temp = 350 + 50*np.exp(-0.1*time) * np.sin(0.5*time) + 5*np.random.randn(500)
X, y = create_windows(temp, window_size=50)
X = torch.FloatTensor(X).unsqueeze(2)
y = torch.FloatTensor(y).unsqueeze(1)

# è¨“ç·´
for epoch in range(30):
    model.train()
    optimizer.zero_grad()

    pred, _ = model(X)
    loss = criterion(pred, y)

    loss.backward()
    optimizer.step()

    if (epoch+1) % 10 == 0:
        print(f'Epoch {epoch+1}, Loss: {loss.item():.6f}')

# Attentioné‡ã¿ã®å¯è¦–åŒ–
model.eval()
with torch.no_grad():
    sample_idx = 100
    sample_input = X[sample_idx:sample_idx+1]
    pred, attention = model(sample_input)

    print(f"\näºˆæ¸¬å€¤: {pred.item():.2f}K")
    print(f"å®Ÿéš›ã®å€¤: {y[sample_idx].item():.2f}K")
    print(f"\nAttentioné‡ã¿ãŒé«˜ã„æ™‚åˆ»ï¼ˆTop 5ï¼‰:")

    top_indices = torch.topk(attention[0], k=5).indices
    for i, idx in enumerate(top_indices):
        print(f"  {i+1}. æ™‚åˆ» t-{50-idx.item()}: é‡ã¿={attention[0, idx].item():.4f}")

# å‡ºåŠ›ä¾‹:
# Epoch 10, Loss: 0.034567
# Epoch 20, Loss: 0.012345
# Epoch 30, Loss: 0.006789
#
# äºˆæ¸¬å€¤: 382.45K
# å®Ÿéš›ã®å€¤: 381.87K
#
# Attentioné‡ã¿ãŒé«˜ã„æ™‚åˆ»ï¼ˆTop 5ï¼‰:
#   1. æ™‚åˆ» t-1: é‡ã¿=0.1234
#   2. æ™‚åˆ» t-2: é‡ã¿=0.0987
#   3. æ™‚åˆ» t-5: é‡ã¿=0.0765
#   4. æ™‚åˆ» t-10: é‡ã¿=0.0543
#   5. æ™‚åˆ» t-3: é‡ã¿=0.0432
# â†’ ç›´è¿‘1-5ã‚¹ãƒ†ãƒƒãƒ—ãŒäºˆæ¸¬ã«é‡è¦
</code></pre>

            <div class="success-box">
                <p><strong>âœ… Attentionã®åˆ©ç‚¹</strong></p>
                <ul>
                    <li><strong>è§£é‡ˆæ€§</strong>: ã©ã®éå»ãƒ‡ãƒ¼ã‚¿ãŒé‡è¦ã‹å¯è¦–åŒ–</li>
                    <li><strong>é•·æœŸä¾å­˜æ€§</strong>: é ã„éå»ã«ã‚‚ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½</li>
                    <li><strong>ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã®æ¤œè¨¼</strong>: åŒ–å­¦çš„ã«å¦¥å½“ãªä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã‹ç¢ºèª</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>å­¦ç¿’ç›®æ¨™ã®ç¢ºèª</h2>

            <p>ã“ã®ç« ã‚’å®Œäº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ã‚’å®Ÿè£…ãƒ»èª¬æ˜ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š</p>

            <h3>åŸºæœ¬ç†è§£</h3>
            <ul>
                <li>RNN/LSTM/GRUã®æ§‹é€ ã¨æ›´æ–°å¼ã‚’èª¬æ˜ã§ãã‚‹</li>
                <li>å‹¾é…æ¶ˆå¤±å•é¡Œã¨LSTMã®ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹ã®å½¹å‰²ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
                <li>Sequence-to-Sequenceã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®åŸç†ã‚’èª¬æ˜ã§ãã‚‹</li>
            </ul>

            <h3>å®Ÿè·µã‚¹ã‚­ãƒ«</h3>
            <ul>
                <li>PyTorchã§LSTM/GRUã‚’å®Ÿè£…ã—ã€ãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°ã‚’äºˆæ¸¬ã§ãã‚‹</li>
                <li>æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®é©åˆ‡ãªå‰å‡¦ç†ï¼ˆæ­£è¦åŒ–ãƒ»ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åˆ†å‰²ï¼‰ãŒã§ãã‚‹</li>
                <li>1ã‚¹ãƒ†ãƒƒãƒ—äºˆæ¸¬ã¨ãƒãƒ«ãƒã‚¹ãƒ†ãƒƒãƒ—äºˆæ¸¬ã‚’å®Ÿè£…ã§ãã‚‹</li>
                <li>Bidirectional LSTMã§ç•°å¸¸æ¤œçŸ¥ãŒã§ãã‚‹</li>
                <li>Attentionæ©Ÿæ§‹ã§äºˆæ¸¬ã®è§£é‡ˆæ€§ã‚’å‘ä¸Šã§ãã‚‹</li>
            </ul>

            <h3>å¿œç”¨åŠ›</h3>
            <ul>
                <li>ãƒ—ãƒ­ã‚»ã‚¹ã®ç‰¹æ€§ã«å¿œã˜ã¦RNN/LSTM/GRUã‚’ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹</li>
                <li>Attentioné‡ã¿ã‹ã‚‰åŒ–å­¦çš„ã«æ„å‘³ã®ã‚ã‚‹çŸ¥è¦‹ã‚’æŠ½å‡ºã§ãã‚‹</li>
                <li>ç•°å¸¸æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã€é–¾å€¤ã‚’é©åˆ‡ã«è¨­å®šã§ãã‚‹</li>
            </ul>
        </section>

        <section>
            <h2>å‚è€ƒæ–‡çŒ®</h2>
            <ol>
                <li>Hochreiter, S., & Schmidhuber, J. (1997). "Long Short-Term Memory." Neural Computation, 9(8), 1735-1780.</li>
                <li>Cho, K., et al. (2014). "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation." EMNLP 2014.</li>
                <li>Bahdanau, D., et al. (2015). "Neural Machine Translation by Jointly Learning to Align and Translate." ICLR 2015.</li>
                <li>Sutskever, I., et al. (2014). "Sequence to Sequence Learning with Neural Networks." NIPS 2014.</li>
            </ol>
        </section>

        <div class="navigation">
            <a href="index.html" class="nav-button">â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</a>
            <a href="chapter-2.html" class="nav-button">ç¬¬2ç« ï¼šTransformerãƒ¢ãƒ‡ãƒ« â†’</a>
        </div>
    </main>
</body>
</html>
