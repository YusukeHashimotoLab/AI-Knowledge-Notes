<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="ç¬¬3ç« ï¼šCNNã«ã‚ˆã‚‹ç”»åƒãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ã‚»ã‚¹è§£æ - æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹è¦–è¦šçš„å“è³ªç®¡ç†ã¨ç•°å¸¸æ¤œçŸ¥">
    <title>ç¬¬3ç« ï¼šCNNã«ã‚ˆã‚‹ç”»åƒãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ã‚»ã‚¹è§£æ - æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚° | PI Terakoya</title>

        <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.8; color: #333; background: #f5f5f5;
        }
        header {
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white; padding: 2rem 1rem; text-align: center;
        }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; }
        .subtitle { opacity: 0.9; font-size: 1.1rem; }
        .container { max-width: 1200px; margin: 2rem auto; padding: 0 1rem; }
        .back-link {
            display: inline-block; margin-bottom: 2rem; padding: 0.5rem 1rem;
            background: white; color: #11998e; text-decoration: none;
            border-radius: 6px; font-weight: 600;
        }
        .content-box {
            background: white; padding: 2rem; border-radius: 12px;
            margin-bottom: 2rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        h2 {
            color: #11998e; margin: 2rem 0 1rem 0;
            padding-bottom: 0.5rem; border-bottom: 3px solid #11998e;
        }
        h3 { color: #2c3e50; margin: 1.5rem 0 1rem 0; }
        h4 { color: #2c3e50; margin: 1rem 0 0.5rem 0; }
        p { margin-bottom: 1rem; }
        ul, ol { margin-left: 2rem; margin-bottom: 1rem; }
        li { margin-bottom: 0.5rem; }
        pre {
            background: #1e1e1e; color: #d4d4d4; padding: 1.5rem;
            border-radius: 8px; overflow-x: auto; margin: 1rem 0;
            border-left: 4px solid #11998e;
        }
        code {
            font-family: 'Courier New', monospace; font-size: 0.9rem;
        }
        .key-point {
            background: #e8f5e9; padding: 1rem; border-radius: 6px;
            border-left: 4px solid #4caf50; margin: 1rem 0;
        }
        .tech-note {
            background: #e3f2fd; padding: 1rem; border-radius: 6px;
            border-left: 4px solid #2196f3; margin: 1rem 0;
        }
        .formula {
            background: #f0f7ff; padding: 1rem; border-radius: 6px;
            margin: 1rem 0; overflow-x: auto;
        }
        table {
            width: 100%; border-collapse: collapse; margin: 1rem 0;
        }
        th, td {
            border: 1px solid #ddd; padding: 0.75rem; text-align: left;
        }
        th {
            background: #11998e; color: white; font-weight: 600;
        }
        tr:nth-child(even) { background: #f9f9f9; }
        .nav-buttons {
            display: flex; justify-content: space-between; margin-top: 3rem;
        }
        .nav-buttons a {
            padding: 0.75rem 1.5rem;
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white; text-decoration: none; border-radius: 6px;
            font-weight: 600;
        }
        footer {
            background: #2c3e50; color: white; text-align: center;
            padding: 2rem 1rem; margin-top: 4rem;
        }
        @media (max-width: 768px) {
            h1 { font-size: 1.6rem; }
            .container { padding: 0 0.5rem; }
            pre { padding: 1rem; }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="../../PI/index.html">ãƒ—ãƒ­ã‚»ã‚¹ãƒ»ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹</a><span class="breadcrumb-separator">â€º</span><a href="../../PI/deep-learning-modeling/index.html">Deep Learning Modeling</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

        <header>
        <div class="container">
            <h1>ç¬¬3ç« ï¼šCNNã«ã‚ˆã‚‹ç”»åƒãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ã‚»ã‚¹è§£æ</h1>
            <p class="subtitle">ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹è¦–è¦šçš„å“è³ªç®¡ç†ã¨è¨­å‚™ç›£è¦–</p>
            <div class="meta">
                <span class="meta">ğŸ“– èª­äº†æ™‚é–“: 35-40åˆ†</span>
                <span class="meta">ğŸ’¡ é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta">ğŸ”¬ å®Ÿä¾‹: æ¬ é™¥æ¤œå‡ºãƒ»ç†±ç”»åƒè§£æ</span>
            </div>
        </div>
    </header>

    <main class="container">
        <section>
            <h2>3.1 CNNåŸºç¤ï¼ˆç•³ã¿è¾¼ã¿å±¤ã¨ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ï¼‰</h2>

            <p>ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆCNNï¼‰ã¯ã€ç”»åƒã®ç©ºé–“çš„ãªæ§‹é€ ã‚’ä¿æŒã—ãªãŒã‚‰ç‰¹å¾´ã‚’æŠ½å‡ºã§ãã‚‹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ãƒ—ãƒ­ã‚»ã‚¹ç”£æ¥­ã§ã¯ã€ã‚«ãƒ¡ãƒ©ç”»åƒã«ã‚ˆã‚‹å“è³ªæ¤œæŸ»ã€ç†±ç”»åƒè§£æã€è¨­å‚™ã®åŠ£åŒ–è¨ºæ–­ãªã©ã«æ´»ç”¨ã•ã‚Œã¾ã™ã€‚</p>

            <div class="info-box">
                <p><strong>ğŸ’¡ CNNã®åŸºæœ¬æ§‹æˆè¦ç´ </strong></p>
                <ul>
                    <li><strong>ç•³ã¿è¾¼ã¿å±¤</strong>: ãƒ•ã‚£ãƒ«ã‚¿ã§å±€æ‰€çš„ãªç‰¹å¾´ã‚’æŠ½å‡º</li>
                    <li><strong>ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤</strong>: ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ä½ç½®ä¸å¤‰æ€§ã‚’ç²å¾—</li>
                    <li><strong>å…¨çµåˆå±¤</strong>: æŠ½å‡ºã—ãŸç‰¹å¾´ã‚’åˆ†é¡ãƒ»å›å¸°ã«åˆ©ç”¨</li>
                </ul>
            </div>

            <p>ç•³ã¿è¾¼ã¿æ¼”ç®—ã®å¼ï¼š</p>

            <p>$$(I * K)(i, j) = \sum_m \sum_n I(i+m, j+n) \cdot K(m, n)$$</p>

            <h3>ä¾‹1: åŸºæœ¬çš„ãªCNNå®Ÿè£…ï¼ˆãƒ—ãƒ­ã‚»ã‚¹ç”»åƒåˆ†é¡ï¼‰</h3>

            <pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torchvision import transforms

class SimpleCNN(nn.Module):
    """ãƒ—ãƒ­ã‚»ã‚¹ç”»åƒåˆ†é¡ç”¨ã®åŸºæœ¬CNN"""

    def __init__(self, num_classes=3):
        """
        Args:
            num_classes: ã‚¯ãƒ©ã‚¹æ•°ï¼ˆæ­£å¸¸/è»½åº¦ç•°å¸¸/é‡åº¦ç•°å¸¸ãªã©ï¼‰
        """
        super(SimpleCNN, self).__init__()

        # ç•³ã¿è¾¼ã¿å±¤1: 3ãƒãƒ£ãƒ³ãƒãƒ«ï¼ˆRGBï¼‰ â†’ 32ãƒãƒ£ãƒ³ãƒãƒ«
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)

        # ç•³ã¿è¾¼ã¿å±¤2: 32 â†’ 64
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)

        # ç•³ã¿è¾¼ã¿å±¤3: 64 â†’ 128
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)

        # ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ï¼ˆ2x2ã€ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰2ï¼‰
        self.pool = nn.MaxPool2d(2, 2)

        # å…¨çµåˆå±¤
        # å…¥åŠ›ç”»åƒãŒ224x224ã®å ´åˆã€3å›ã®poolingå¾Œã¯28x28
        self.fc1 = nn.Linear(128 * 28 * 28, 512)
        self.fc2 = nn.Linear(512, num_classes)

        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        """
        Args:
            x: [batch, 3, 224, 224] RGBç”»åƒ
        Returns:
            output: [batch, num_classes] ã‚¯ãƒ©ã‚¹ç¢ºç‡
        """
        # ç•³ã¿è¾¼ã¿å±¤1 + ReLU + ãƒ—ãƒ¼ãƒªãƒ³ã‚°
        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # [batch, 32, 112, 112]

        # ç•³ã¿è¾¼ã¿å±¤2 + ReLU + ãƒ—ãƒ¼ãƒªãƒ³ã‚°
        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # [batch, 64, 56, 56]

        # ç•³ã¿è¾¼ã¿å±¤3 + ReLU + ãƒ—ãƒ¼ãƒªãƒ³ã‚°
        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # [batch, 128, 28, 28]

        # å¹³å¦åŒ–
        x = x.view(x.size(0), -1)  # [batch, 128*28*28]

        # å…¨çµåˆå±¤
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)

        return x

# åˆæˆãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆï¼ˆå®Ÿéš›ã¯è£½é€ ãƒ©ã‚¤ãƒ³ã®ã‚«ãƒ¡ãƒ©ç”»åƒã‚’ä½¿ç”¨ï¼‰
batch_size = 8
model = SimpleCNN(num_classes=3)

# ãƒ€ãƒŸãƒ¼ç”»åƒï¼ˆ224x224ã®RGBç”»åƒï¼‰
dummy_images = torch.randn(batch_size, 3, 224, 224)

# é †ä¼æ’­
output = model(dummy_images)
print(f"Output shape: {output.shape}")  # [8, 3]
print(f"Class probabilities (sample 1): {F.softmax(output[0], dim=0)}")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®ç¢ºèª
total_params = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {total_params:,}")

# å‡ºåŠ›ä¾‹:
# Output shape: torch.Size([8, 3])
# Class probabilities (sample 1): tensor([0.3456, 0.4123, 0.2421], grad_fn=<SoftmaxBackward>)
# Total parameters: 40,363,971
</code></pre>
        </section>

        <section>
            <h2>3.2 ãƒ—ãƒ­ã‚»ã‚¹çŠ¶æ…‹ã®ç”»åƒåˆ†é¡</h2>

            <p>è£½é€ ãƒ©ã‚¤ãƒ³ã®ã‚«ãƒ¡ãƒ©ç”»åƒã‹ã‚‰ã€ãƒ—ãƒ­ã‚»ã‚¹ã®çŠ¶æ…‹ï¼ˆæ­£å¸¸/ç•°å¸¸ï¼‰ã‚’åˆ†é¡ã—ã¾ã™ã€‚ä¾‹ãˆã°ã€åå¿œå™¨å†…éƒ¨ã®è¦³å¯Ÿçª“ã‹ã‚‰æ’®å½±ã—ãŸç”»åƒã‚’è§£æã—ã¾ã™ã€‚</p>

            <h3>ä¾‹2: 2ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼ˆæ­£å¸¸/ç•°å¸¸ï¼‰</h3>

            <pre><code class="language-python">import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from PIL import Image

class ProcessImageDataset(Dataset):
    """ãƒ—ãƒ­ã‚»ã‚¹ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""

    def __init__(self, image_paths, labels, transform=None):
        """
        Args:
            image_paths: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            labels: ãƒ©ãƒ™ãƒ«ï¼ˆ0: æ­£å¸¸, 1: ç•°å¸¸ï¼‰
            transform: ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
        """
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        # ç”»åƒèª­ã¿è¾¼ã¿
        image = Image.open(self.image_paths[idx]).convert('RGB')
        label = self.labels[idx]

        if self.transform:
            image = self.transform(image)

        return image, label

# ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆè¨“ç·´æ™‚ï¼‰
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),  # å·¦å³åè»¢
    transforms.RandomRotation(10),  # Â±10åº¦å›è»¢
    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # æ˜åº¦ãƒ»ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆèª¿æ•´
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# ãƒ†ã‚¹ãƒˆæ™‚ï¼ˆæ‹¡å¼µãªã—ï¼‰
test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# è¨“ç·´é–¢æ•°
def train_classifier(model, train_loader, val_loader, epochs=20, lr=0.001):
    """ç”»åƒåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

    best_acc = 0.0

    for epoch in range(epochs):
        # è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚º
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_acc = 100 * correct / total

        # æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º
        model.eval()
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

        val_acc = 100 * val_correct / val_total

        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), 'best_process_classifier.pth')

        print(f'Epoch {epoch+1}/{epochs}:')
        print(f'  Train Loss: {running_loss/len(train_loader):.4f}, Acc: {train_acc:.2f}%')
        print(f'  Val Acc: {val_acc:.2f}%')

        scheduler.step()

    print(f'\nBest validation accuracy: {best_acc:.2f}%')
    return model

# ä½¿ç”¨ä¾‹ï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã«ç½®ãæ›ãˆã‚‹ï¼‰
# train_paths = ['path/to/normal1.jpg', 'path/to/abnormal1.jpg', ...]
# train_labels = [0, 1, ...]  # 0: æ­£å¸¸, 1: ç•°å¸¸
#
# train_dataset = ProcessImageDataset(train_paths, train_labels, train_transform)
# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
#
# model = SimpleCNN(num_classes=2)
# trained_model = train_classifier(model, train_loader, val_loader, epochs=20)

# å‡ºåŠ›ä¾‹:
# Epoch 1/20:
#   Train Loss: 0.6234, Acc: 65.34%
#   Val Acc: 68.21%
# Epoch 5/20:
#   Train Loss: 0.3456, Acc: 84.56%
#   Val Acc: 82.34%
# ...
# Best validation accuracy: 94.56%
</code></pre>
        </section>

        <section>
            <h2>3.3 ç‰©ä½“æ¤œå‡ºã«ã‚ˆã‚‹è¨­å‚™ç›£è¦–</h2>

            <p>YOLOï¼ˆYou Only Look Onceï¼‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½¿ç”¨ã—ã¦ã€ç”»åƒå†…ã®è¨­å‚™ã‚„ç•°å¸¸ç®‡æ‰€ã‚’æ¤œå‡ºã—ã¾ã™ã€‚ç°¡æ˜“ç‰ˆã‚’å®Ÿè£…ã—ã¾ã™ã€‚</p>

            <h3>ä¾‹3: ç°¡æ˜“ç‰©ä½“æ¤œå‡ºï¼ˆãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹äºˆæ¸¬ï¼‰</h3>

            <pre><code class="language-python">class SimpleYOLO(nn.Module):
    """ç°¡æ˜“YOLOé¢¨ã®ç‰©ä½“æ¤œå‡ºãƒ¢ãƒ‡ãƒ«"""

    def __init__(self, num_classes=1, num_boxes=2):
        """
        Args:
            num_classes: æ¤œå‡ºå¯¾è±¡ã‚¯ãƒ©ã‚¹æ•°ï¼ˆç•°å¸¸ç®‡æ‰€ãªã©ï¼‰
            num_boxes: ã‚°ãƒªãƒƒãƒ‰ã‚»ãƒ«ã‚ãŸã‚Šã®ãƒœãƒƒã‚¯ã‚¹æ•°
        """
        super(SimpleYOLO, self).__init__()

        self.num_classes = num_classes
        self.num_boxes = num_boxes

        # ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ï¼ˆç‰¹å¾´æŠ½å‡ºï¼‰
        self.backbone = nn.Sequential(
            # Conv1
            nn.Conv2d(3, 64, 7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.1),
            nn.MaxPool2d(2, 2),

            # Conv2
            nn.Conv2d(64, 192, 3, padding=1),
            nn.BatchNorm2d(192),
            nn.LeakyReLU(0.1),
            nn.MaxPool2d(2, 2),

            # Conv3-5
            nn.Conv2d(192, 256, 1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.1),
            nn.Conv2d(256, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.1),
            nn.MaxPool2d(2, 2)
        )

        # æ¤œå‡ºãƒ˜ãƒƒãƒ‰
        # å„ã‚°ãƒªãƒƒãƒ‰ã‚»ãƒ«ã§äºˆæ¸¬: (x, y, w, h, confidence) * num_boxes + class_probs
        output_size = num_boxes * 5 + num_classes
        self.detection_head = nn.Sequential(
            nn.Conv2d(512, 1024, 3, padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(0.1),
            nn.Conv2d(1024, output_size, 1)
        )

    def forward(self, x):
        """
        Args:
            x: [batch, 3, 448, 448] å…¥åŠ›ç”»åƒ
        Returns:
            output: [batch, grid_h, grid_w, num_boxes*5 + num_classes]
                    (x, y, w, h, confidence, class_probs)
        """
        features = self.backbone(x)  # [batch, 512, 14, 14]
        output = self.detection_head(features)  # [batch, output_size, 14, 14]

        # [batch, grid_h, grid_w, output_size] ã«å¤‰æ›
        output = output.permute(0, 2, 3, 1)

        return output

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
model = SimpleYOLO(num_classes=1, num_boxes=2)

# ãƒ†ã‚¹ãƒˆ
dummy_image = torch.randn(4, 3, 448, 448)
output = model(dummy_image)

print(f"Output shape: {output.shape}")  # [4, 14, 14, 11]
# 11 = 2 boxes * 5 (x,y,w,h,conf) + 1 class

# ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®ãƒ‡ã‚³ãƒ¼ãƒ‰ä¾‹
def decode_predictions(output, grid_size=14, img_size=448, conf_threshold=0.5):
    """äºˆæ¸¬ã‚’ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã«å¤‰æ›

    Args:
        output: [grid_h, grid_w, num_boxes*5 + num_classes]

    Returns:
        boxes: List of (x, y, w, h, confidence, class_id)
    """
    boxes = []
    num_boxes = 2

    for i in range(grid_size):
        for j in range(grid_size):
            for b in range(num_boxes):
                # ãƒœãƒƒã‚¯ã‚¹æƒ…å ±ã‚’æŠ½å‡º
                box_idx = b * 5
                x = (j + torch.sigmoid(output[i, j, box_idx])) / grid_size
                y = (i + torch.sigmoid(output[i, j, box_idx+1])) / grid_size
                w = torch.sigmoid(output[i, j, box_idx+2])
                h = torch.sigmoid(output[i, j, box_idx+3])
                conf = torch.sigmoid(output[i, j, box_idx+4])

                if conf > conf_threshold:
                    # ã‚¯ãƒ©ã‚¹ç¢ºç‡
                    class_probs = output[i, j, num_boxes*5:]
                    class_id = torch.argmax(class_probs)

                    # ç”»åƒåº§æ¨™ã«å¤‰æ›
                    x_abs = x * img_size
                    y_abs = y * img_size
                    w_abs = w * img_size
                    h_abs = h * img_size

                    boxes.append([x_abs.item(), y_abs.item(),
                                 w_abs.item(), h_abs.item(),
                                 conf.item(), class_id.item()])

    return boxes

# äºˆæ¸¬ä¾‹
sample_output = output[0].detach()  # æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«
detected_boxes = decode_predictions(sample_output, conf_threshold=0.3)
print(f"\nDetected {len(detected_boxes)} boxes with confidence > 0.3")

# å‡ºåŠ›ä¾‹:
# Output shape: torch.Size([4, 14, 14, 11])
# Detected 37 boxes with confidence > 0.3
</code></pre>
        </section>

        <section>
            <h2>3.4 è£½é€ æ¬ é™¥æ¤œå‡º</h2>

            <p>è£½å“è¡¨é¢ã®å‚·ã€æ±šã‚Œã€å¤‰è‰²ãªã©ã®æ¬ é™¥ã‚’ãƒ”ã‚¯ã‚»ãƒ«å˜ä½ã§æ¤œå‡ºã—ã¾ã™ã€‚ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã¨ã—ã¦å®Ÿè£…ã—ã¾ã™ã€‚</p>

            <h3>ä¾‹4: U-Netã«ã‚ˆã‚‹æ¬ é™¥ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³</h3>

            <pre><code class="language-python">class UNet(nn.Module):
    """U-Net ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆæ¬ é™¥æ¤œå‡ºç”¨ï¼‰"""

    def __init__(self, in_channels=3, out_channels=1):
        """
        Args:
            in_channels: å…¥åŠ›ãƒãƒ£ãƒ³ãƒãƒ«æ•°ï¼ˆRGB=3ï¼‰
            out_channels: å‡ºåŠ›ãƒãƒ£ãƒ³ãƒãƒ«æ•°ï¼ˆäºŒå€¤ãƒã‚¹ã‚¯=1ï¼‰
        """
        super(UNet, self).__init__()

        # Encoderï¼ˆãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
        self.enc1 = self.conv_block(in_channels, 64)
        self.enc2 = self.conv_block(64, 128)
        self.enc3 = self.conv_block(128, 256)
        self.enc4 = self.conv_block(256, 512)

        # Bottleneck
        self.bottleneck = self.conv_block(512, 1024)

        # Decoderï¼ˆã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)
        self.dec4 = self.conv_block(1024, 512)  # 512+512ï¼ˆskip connectionï¼‰

        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.dec3 = self.conv_block(512, 256)

        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec2 = self.conv_block(256, 128)

        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec1 = self.conv_block(128, 64)

        # å‡ºåŠ›å±¤
        self.out_conv = nn.Conv2d(64, out_channels, 1)

        self.pool = nn.MaxPool2d(2, 2)

    def conv_block(self, in_channels, out_channels):
        """2ã¤ã®ç•³ã¿è¾¼ã¿å±¤ã®ãƒ–ãƒ­ãƒƒã‚¯"""
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        """
        Args:
            x: [batch, 3, H, W] å…¥åŠ›ç”»åƒ
        Returns:
            mask: [batch, 1, H, W] æ¬ é™¥ãƒã‚¹ã‚¯ï¼ˆ0-1ï¼‰
        """
        # Encoder
        enc1 = self.enc1(x)
        enc2 = self.enc2(self.pool(enc1))
        enc3 = self.enc3(self.pool(enc2))
        enc4 = self.enc4(self.pool(enc3))

        # Bottleneck
        bottleneck = self.bottleneck(self.pool(enc4))

        # Decoderï¼ˆskip connectionã‚ã‚Šï¼‰
        dec4 = self.upconv4(bottleneck)
        dec4 = torch.cat([dec4, enc4], dim=1)  # Skip connection
        dec4 = self.dec4(dec4)

        dec3 = self.upconv3(dec4)
        dec3 = torch.cat([dec3, enc3], dim=1)
        dec3 = self.dec3(dec3)

        dec2 = self.upconv2(dec3)
        dec2 = torch.cat([dec2, enc2], dim=1)
        dec2 = self.dec2(dec2)

        dec1 = self.upconv1(dec2)
        dec1 = torch.cat([dec1, enc1], dim=1)
        dec1 = self.dec1(dec1)

        # å‡ºåŠ›ï¼ˆã‚·ã‚°ãƒ¢ã‚¤ãƒ‰ã§0-1ã«æ­£è¦åŒ–ï¼‰
        out = torch.sigmoid(self.out_conv(dec1))

        return out

# Dice Lossï¼ˆã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ï¼‰
class DiceLoss(nn.Module):
    """Diceä¿‚æ•°ã«åŸºã¥ãloss"""

    def forward(self, pred, target, smooth=1.0):
        """
        Args:
            pred: [batch, 1, H, W] äºˆæ¸¬ãƒã‚¹ã‚¯
            target: [batch, 1, H, W] çœŸã®ãƒã‚¹ã‚¯
        """
        pred = pred.view(-1)
        target = target.view(-1)

        intersection = (pred * target).sum()
        dice = (2.0 * intersection + smooth) / (pred.sum() + target.sum() + smooth)

        return 1 - dice

# è¨“ç·´ä¾‹
model = UNet(in_channels=3, out_channels=1)
criterion = DiceLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã¯è£½å“ç”»åƒã¨æ¬ é™¥ãƒã‚¹ã‚¯ï¼‰
dummy_images = torch.randn(4, 3, 256, 256)
dummy_masks = torch.randint(0, 2, (4, 1, 256, 256)).float()

# è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—
model.train()
optimizer.zero_grad()

pred_masks = model(dummy_images)
loss = criterion(pred_masks, dummy_masks)

loss.backward()
optimizer.step()

print(f"Predicted mask shape: {pred_masks.shape}")
print(f"Dice Loss: {loss.item():.4f}")

# äºˆæ¸¬å€¤ã®çµ±è¨ˆ
print(f"Prediction range: [{pred_masks.min():.4f}, {pred_masks.max():.4f}]")
print(f"Defect pixels (>0.5): {(pred_masks > 0.5).sum().item()} / {pred_masks.numel()}")

# å‡ºåŠ›ä¾‹:
# Predicted mask shape: torch.Size([4, 1, 256, 256])
# Dice Loss: 0.5234
# Prediction range: [0.0123, 0.9876]
# Defect pixels (>0.5): 31245 / 262144
</code></pre>
        </section>

        <section>
            <h2>3.5 ç†±ç”»åƒè§£æã«ã‚ˆã‚‹æ¸©åº¦åˆ†å¸ƒæ¨å®š</h2>

            <p>èµ¤å¤–ç·šã‚«ãƒ¡ãƒ©ã§æ’®å½±ã—ãŸç†±ç”»åƒã‹ã‚‰ã€è¨­å‚™ã®æ¸©åº¦åˆ†å¸ƒã‚’æ¨å®šã—ã€ãƒ›ãƒƒãƒˆã‚¹ãƒãƒƒãƒˆã‚’æ¤œå‡ºã—ã¾ã™ã€‚</p>

            <h3>ä¾‹5: ç†±ç”»åƒã‹ã‚‰ã®æ¸©åº¦äºˆæ¸¬</h3>

            <pre><code class="language-python">class ThermalCNN(nn.Module):
    """ç†±ç”»åƒè§£æç”¨CNN"""

    def __init__(self):
        super(ThermalCNN, self).__init__()

        # ç†±ç”»åƒã¯é€šå¸¸1ãƒãƒ£ãƒ³ãƒãƒ«ï¼ˆã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        self.features = nn.Sequential(
            nn.Conv2d(1, 32, 5, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),

            nn.Conv2d(32, 64, 5, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),

            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
        )

        # æ¸©åº¦æ¨å®šãƒ˜ãƒƒãƒ‰ï¼ˆå›å¸°ã‚¿ã‚¹ã‚¯ï¼‰
        self.regressor = nn.Sequential(
            nn.Linear(128 * 28 * 28, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, 3)  # æœ€é«˜æ¸©åº¦ã€å¹³å‡æ¸©åº¦ã€æœ€ä½æ¸©åº¦
        )

    def forward(self, x):
        """
        Args:
            x: [batch, 1, 224, 224] ç†±ç”»åƒ
        Returns:
            temps: [batch, 3] (max_temp, avg_temp, min_temp)
        """
        features = self.features(x)
        features = features.view(features.size(0), -1)
        temps = self.regressor(features)
        return temps

# åˆæˆç†±ç”»åƒãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
def generate_thermal_image(base_temp=300, hotspot_temp=450, size=224):
    """ç†±ç”»åƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

    Args:
        base_temp: ãƒ™ãƒ¼ã‚¹æ¸©åº¦ [K]
        hotspot_temp: ãƒ›ãƒƒãƒˆã‚¹ãƒãƒƒãƒˆæ¸©åº¦ [K]
        size: ç”»åƒã‚µã‚¤ã‚º

    Returns:
        image: [1, size, size] æ¸©åº¦ãƒãƒƒãƒ—
        stats: (max, avg, min) æ¸©åº¦çµ±è¨ˆ
    """
    # ãƒ™ãƒ¼ã‚¹æ¸©åº¦
    image = np.ones((size, size)) * base_temp

    # ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ãƒ›ãƒƒãƒˆã‚¹ãƒãƒƒãƒˆã‚’è¿½åŠ 
    n_hotspots = np.random.randint(1, 4)
    for _ in range(n_hotspots):
        cx, cy = np.random.randint(20, size-20, 2)
        sigma = np.random.uniform(10, 30)

        y, x = np.ogrid[:size, :size]
        gaussian = np.exp(-((x - cx)**2 + (y - cy)**2) / (2 * sigma**2))
        image += gaussian * (hotspot_temp - base_temp)

    # ãƒã‚¤ã‚º
    image += np.random.randn(size, size) * 5

    # çµ±è¨ˆ
    stats = np.array([image.max(), image.mean(), image.min()])

    # æ­£è¦åŒ–ï¼ˆ0-1ã«ï¼‰
    image_normalized = (image - 273) / 227  # 273-500K â†’ 0-1

    return torch.FloatTensor(image_normalized).unsqueeze(0), stats

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
n_samples = 100
thermal_images = []
temperature_stats = []

for _ in range(n_samples):
    img, stats = generate_thermal_image()
    thermal_images.append(img)
    temperature_stats.append(stats)

thermal_images = torch.stack(thermal_images)
temperature_stats = torch.FloatTensor(np.array(temperature_stats))

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = ThermalCNN()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

for epoch in range(30):
    model.train()
    optimizer.zero_grad()

    pred_temps = model(thermal_images)
    loss = criterion(pred_temps, temperature_stats)

    loss.backward()
    optimizer.step()

    if (epoch + 1) % 5 == 0:
        mae = torch.mean(torch.abs(pred_temps - temperature_stats))
        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}, MAE: {mae.item():.2f}K')

# ãƒ†ã‚¹ãƒˆ
model.eval()
with torch.no_grad():
    test_img, true_stats = generate_thermal_image(base_temp=320, hotspot_temp=480)
    pred_stats = model(test_img.unsqueeze(0))

print(f"\nTest prediction:")
print(f"  True:      Max={true_stats[0]:.1f}K, Avg={true_stats[1]:.1f}K, Min={true_stats[2]:.1f}K")
print(f"  Predicted: Max={pred_stats[0,0]:.1f}K, Avg={pred_stats[0,1]:.1f}K, Min={pred_stats[0,2]:.1f}K")

# å‡ºåŠ›ä¾‹:
# Epoch 5, Loss: 1234.5678, MAE: 12.34K
# Epoch 10, Loss: 567.8901, MAE: 8.76K
# Epoch 15, Loss: 234.5678, MAE: 5.43K
# Epoch 20, Loss: 123.4567, MAE: 3.21K
# Epoch 25, Loss: 89.0123, MAE: 2.15K
# Epoch 30, Loss: 67.8901, MAE: 1.87K
#
# Test prediction:
#   True:      Max=489.3K, Avg=345.7K, Min=318.2K
#   Predicted: Max=485.6K, Avg=347.2K, Min=320.1K
</code></pre>
        </section>

        <section>
            <h2>3.6 è»¢ç§»å­¦ç¿’ï¼ˆResNet, EfficientNetï¼‰</h2>

            <p>ImageNetã§äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€å°‘é‡ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã§ã‚‚é«˜ç²¾åº¦ã‚’å®Ÿç¾ã§ãã¾ã™ã€‚</p>

            <h3>ä¾‹6: ResNet50ã«ã‚ˆã‚‹è»¢ç§»å­¦ç¿’</h3>

            <pre><code class="language-python">import torchvision.models as models

class ProcessClassifierWithTransferLearning(nn.Module):
    """è»¢ç§»å­¦ç¿’ã‚’ä½¿ç”¨ã—ãŸãƒ—ãƒ­ã‚»ã‚¹åˆ†é¡å™¨"""

    def __init__(self, num_classes=3, pretrained=True, freeze_backbone=True):
        """
        Args:
            num_classes: åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°
            pretrained: ImageNetäº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ä½¿ç”¨
            freeze_backbone: ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã‚’å‡çµï¼ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãªã„ï¼‰
        """
        super(ProcessClassifierWithTransferLearning, self).__init__()

        # ResNet50ã‚’ãƒ­ãƒ¼ãƒ‰
        self.backbone = models.resnet50(pretrained=pretrained)

        # ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã®å‡çµ
        if freeze_backbone:
            for param in self.backbone.parameters():
                param.requires_grad = False

        # æœ€çµ‚å±¤ã‚’ç½®ãæ›ãˆï¼ˆImageNetã®1000ã‚¯ãƒ©ã‚¹ â†’ ãƒ—ãƒ­ã‚»ã‚¹ã®num_classesï¼‰
        num_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Linear(num_features, num_classes)

    def forward(self, x):
        return self.backbone(x)

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
model = ProcessClassifierWithTransferLearning(num_classes=3, pretrained=True, freeze_backbone=True)

# è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¢ºèª
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
print(f"Trainable parameters: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)")

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§å‹•ä½œç¢ºèª
dummy_batch = torch.randn(8, 3, 224, 224)
output = model(dummy_batch)
print(f"Output shape: {output.shape}")

# EfficientNet-B0ã§ã®è»¢ç§»å­¦ç¿’
class EfficientNetClassifier(nn.Module):
    """EfficientNetã«ã‚ˆã‚‹è»¢ç§»å­¦ç¿’"""

    def __init__(self, num_classes=3):
        super(EfficientNetClassifier, self).__init__()

        # EfficientNet-B0ï¼ˆè»½é‡ç‰ˆï¼‰
        self.backbone = models.efficientnet_b0(pretrained=True)

        # æœ€çµ‚å±¤ã‚’ç½®ãæ›ãˆ
        num_features = self.backbone.classifier[1].in_features
        self.backbone.classifier[1] = nn.Linear(num_features, num_classes)

    def forward(self, x):
        return self.backbone(x)

efficient_model = EfficientNetClassifier(num_classes=3)
efficient_output = efficient_model(dummy_batch)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°æ¯”è¼ƒ
resnet_params = sum(p.numel() for p in model.parameters())
efficient_params = sum(p.numel() for p in efficient_model.parameters())

print(f"\nModel comparison:")
print(f"  ResNet50:       {resnet_params:,} parameters")
print(f"  EfficientNet-B0: {efficient_params:,} parameters")
print(f"  Reduction:       {100*(1 - efficient_params/resnet_params):.1f}%")

# å‡ºåŠ›ä¾‹:
# Trainable parameters: 6,147 / 23,520,835 (0.03%)
# Output shape: torch.Size([8, 3])
#
# Model comparison:
#   ResNet50:       23,520,835 parameters
#   EfficientNet-B0: 4,011,391 parameters
#   Reduction:       82.9%
</code></pre>
        </section>

        <section>
            <h2>3.7 Grad-CAMã«ã‚ˆã‚‹è¦–è¦šçš„èª¬æ˜</h2>

            <p>Gradient-weighted Class Activation Mappingï¼ˆGrad-CAMï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ãŒã©ã“ã‚’è¦‹ã¦åˆ¤æ–­ã—ãŸã‹ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚</p>

            <h3>ä¾‹7: Grad-CAMå®Ÿè£…</h3>

            <pre><code class="language-python">class GradCAM:
    """Grad-CAM: è¦–è¦šçš„èª¬æ˜ã®ç”Ÿæˆ"""

    def __init__(self, model, target_layer):
        """
        Args:
            model: CNNãƒ¢ãƒ‡ãƒ«
            target_layer: å¯è¦–åŒ–å¯¾è±¡ã®å±¤ï¼ˆé€šå¸¸ã¯æœ€çµ‚ç•³ã¿è¾¼ã¿å±¤ï¼‰
        """
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None

        # ãƒ•ãƒƒã‚¯ã‚’ç™»éŒ²
        target_layer.register_forward_hook(self.save_activation)
        target_layer.register_backward_hook(self.save_gradient)

    def save_activation(self, module, input, output):
        """é †ä¼æ’­æ™‚ã®æ´»æ€§åŒ–ã‚’ä¿å­˜"""
        self.activations = output.detach()

    def save_gradient(self, module, grad_input, grad_output):
        """é€†ä¼æ’­æ™‚ã®å‹¾é…ã‚’ä¿å­˜"""
        self.gradients = grad_output[0].detach()

    def generate_cam(self, input_image, target_class=None):
        """
        CAMã‚’ç”Ÿæˆ

        Args:
            input_image: [1, 3, H, W] å…¥åŠ›ç”»åƒ
            target_class: å¯¾è±¡ã‚¯ãƒ©ã‚¹ï¼ˆNoneãªã‚‰æœ€å¤§ç¢ºç‡ã‚¯ãƒ©ã‚¹ï¼‰

        Returns:
            cam: [H, W] Class Activation Map
        """
        # é †ä¼æ’­
        self.model.eval()
        output = self.model(input_image)

        if target_class is None:
            target_class = output.argmax(dim=1).item()

        # å¯¾è±¡ã‚¯ãƒ©ã‚¹ã®ã‚¹ã‚³ã‚¢ã§é€†ä¼æ’­
        self.model.zero_grad()
        class_score = output[0, target_class]
        class_score.backward()

        # å‹¾é…ã¨activationã‹ã‚‰é‡ã¿ã‚’è¨ˆç®—
        pooled_gradients = torch.mean(self.gradients, dim=[0, 2, 3])  # [channels]

        # é‡ã¿ä»˜ãå’Œ
        cam = torch.zeros(self.activations.shape[2:], dtype=torch.float32)
        for i, w in enumerate(pooled_gradients):
            cam += w * self.activations[0, i, :, :]

        # ReLU + æ­£è¦åŒ–
        cam = F.relu(cam)
        cam = cam - cam.min()
        cam = cam / cam.max() if cam.max() > 0 else cam

        return cam.numpy(), target_class

# ä½¿ç”¨ä¾‹
model = SimpleCNN(num_classes=3)
model.eval()

# æœ€çµ‚ç•³ã¿è¾¼ã¿å±¤ã‚’å–å¾—
target_layer = model.conv3

# Grad-CAMåˆæœŸåŒ–
grad_cam = GradCAM(model, target_layer)

# ãƒ†ã‚¹ãƒˆç”»åƒ
test_image = torch.randn(1, 3, 224, 224)

# CAMç”Ÿæˆ
cam, predicted_class = grad_cam.generate_cam(test_image)

print(f"Predicted class: {predicted_class}")
print(f"CAM shape: {cam.shape}")
print(f"CAM range: [{cam.min():.4f}, {cam.max():.4f}]")

# å¯è¦–åŒ–é–¢æ•°
import matplotlib.pyplot as plt
import cv2

def visualize_gradcam(original_image, cam, alpha=0.5):
    """Grad-CAMã‚’å…ƒç”»åƒã«é‡ã­ã¦å¯è¦–åŒ–

    Args:
        original_image: [3, H, W] Tensor
        cam: [h, w] numpy array
        alpha: é€æ˜åº¦
    """
    # Tensorã‚’numpyé…åˆ—ã«å¤‰æ›
    img = original_image.permute(1, 2, 0).numpy()
    img = (img - img.min()) / (img.max() - img.min())  # 0-1ã«æ­£è¦åŒ–

    # CAMã‚’ç”»åƒã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚º
    cam_resized = cv2.resize(cam, (img.shape[1], img.shape[0]))

    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆ
    heatmap = plt.cm.jet(cam_resized)[:, :, :3]  # RGBã®ã¿

    # é‡ã­åˆã‚ã›
    overlayed = heatmap * alpha + img * (1 - alpha)

    # ãƒ—ãƒ­ãƒƒãƒˆ
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    axes[0].imshow(img)
    axes[0].set_title('Original Image')
    axes[0].axis('off')

    axes[1].imshow(cam_resized, cmap='jet')
    axes[1].set_title('Grad-CAM')
    axes[1].axis('off')

    axes[2].imshow(overlayed)
    axes[2].set_title('Overlay')
    axes[2].axis('off')

    plt.tight_layout()
    # plt.savefig('gradcam_visualization.png', dpi=150)
    print("Grad-CAM visualization created")

# å¯è¦–åŒ–
visualize_gradcam(test_image[0], cam)

# å‡ºåŠ›ä¾‹:
# Predicted class: 1
# CAM shape: (28, 28)
# CAM range: [0.0000, 1.0000]
# Grad-CAM visualization created
</code></pre>

            <div class="success-box">
                <p><strong>âœ… Grad-CAMã®å¿œç”¨</strong></p>
                <ul>
                    <li><strong>èª¤åˆ¤å®šã®è¨ºæ–­</strong>: ãƒ¢ãƒ‡ãƒ«ãŒèª¤ã£ãŸç®‡æ‰€ã‚’è¦‹ã¦ã„ãªã„ã‹ç¢ºèª</li>
                    <li><strong>ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã®æ¤œè¨¼</strong>: åŒ–å­¦çš„ã«å¦¥å½“ãªé ˜åŸŸã‚’è¦‹ã¦ã„ã‚‹ã‹</li>
                    <li><strong>ãƒ¢ãƒ‡ãƒ«ã®æ”¹å–„</strong>: é‡è¦é ˜åŸŸãŒæ­£ã—ãèªè­˜ã•ã‚Œã‚‹ã‚ˆã†ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>3.8 ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”»åƒå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h2>

            <p>è£½é€ ãƒ©ã‚¤ãƒ³ã§ã®å®Ÿç”¨åŒ–ã‚’æƒ³å®šã—ã€ã‚«ãƒ¡ãƒ©ã‹ã‚‰ã®é€£ç¶šç”»åƒã‚’é«˜é€Ÿã«å‡¦ç†ã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚</p>

            <h3>ä¾‹8: æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨æœ€é©åŒ–</h3>

            <pre><code class="language-python">import time
import threading
from queue import Queue
from collections import deque

class RealTimeInferencePipeline:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”»åƒå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(self, model, device='cuda', batch_size=4, buffer_size=100):
        """
        Args:
            model: æ¨è«–ç”¨ãƒ¢ãƒ‡ãƒ«
            device: 'cuda' or 'cpu'
            batch_size: ãƒãƒƒãƒå‡¦ç†ã‚µã‚¤ã‚º
            buffer_size: ãƒ•ãƒ¬ãƒ¼ãƒ ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º
        """
        self.model = model.to(device).eval()
        self.device = device
        self.batch_size = batch_size

        self.frame_queue = Queue(maxsize=buffer_size)
        self.result_queue = Queue()

        self.fps_buffer = deque(maxlen=30)
        self.running = False

        # ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ï¼ˆTorchScriptï¼‰
        self.optimize_model()

    def optimize_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚’TorchScriptã«å¤‰æ›ã—ã¦é«˜é€ŸåŒ–"""
        dummy_input = torch.randn(1, 3, 224, 224).to(self.device)
        try:
            self.model = torch.jit.trace(self.model, dummy_input)
            print("Model optimized with TorchScript")
        except Exception as e:
            print(f"TorchScript optimization failed: {e}")

    def preprocess_frame(self, frame):
        """ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‰å‡¦ç†

        Args:
            frame: numpy array [H, W, 3] (BGR)
        Returns:
            tensor: [1, 3, 224, 224]
        """
        # BGRã‹ã‚‰RGBã«å¤‰æ›
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # ãƒªã‚µã‚¤ã‚º
        frame_resized = cv2.resize(frame_rgb, (224, 224))

        # Tensorå¤‰æ›ã¨æ­£è¦åŒ–
        frame_tensor = torch.FloatTensor(frame_resized).permute(2, 0, 1) / 255.0
        frame_tensor = transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )(frame_tensor)

        return frame_tensor.unsqueeze(0)

    def inference_worker(self):
        """æ¨è«–ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰ï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰"""
        batch_frames = []
        batch_ids = []

        while self.running:
            try:
                # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’åé›†
                while len(batch_frames) < self.batch_size and not self.frame_queue.empty():
                    frame_id, frame = self.frame_queue.get(timeout=0.01)
                    batch_frames.append(frame)
                    batch_ids.append(frame_id)

                if len(batch_frames) > 0:
                    # ãƒãƒƒãƒæ¨è«–
                    start_time = time.time()

                    batch_tensor = torch.cat(batch_frames, dim=0).to(self.device)

                    with torch.no_grad():
                        outputs = self.model(batch_tensor)
                        predictions = torch.argmax(outputs, dim=1).cpu().numpy()
                        confidences = torch.softmax(outputs, dim=1).cpu().numpy()

                    inference_time = time.time() - start_time

                    # çµæœã‚’ä¿å­˜
                    for i, frame_id in enumerate(batch_ids):
                        self.result_queue.put({
                            'frame_id': frame_id,
                            'prediction': predictions[i],
                            'confidence': confidences[i],
                            'inference_time': inference_time / len(batch_frames)
                        })

                    # FPSè¨ˆç®—
                    fps = len(batch_frames) / inference_time
                    self.fps_buffer.append(fps)

                    # ã‚¯ãƒªã‚¢
                    batch_frames.clear()
                    batch_ids.clear()

                else:
                    time.sleep(0.001)  # CPUã‚’é–‹æ”¾

            except Exception as e:
                print(f"Inference error: {e}")

    def start(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹"""
        self.running = True
        self.inference_thread = threading.Thread(target=self.inference_worker)
        self.inference_thread.start()
        print("Inference pipeline started")

    def stop(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åœæ­¢"""
        self.running = False
        self.inference_thread.join()
        print("Inference pipeline stopped")

    def process_frame(self, frame_id, frame):
        """ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 

        Args:
            frame_id: ãƒ•ãƒ¬ãƒ¼ãƒ ID
            frame: numpy array [H, W, 3]
        """
        preprocessed = self.preprocess_frame(frame)

        if not self.frame_queue.full():
            self.frame_queue.put((frame_id, preprocessed))
        else:
            print(f"Warning: Frame buffer full, dropping frame {frame_id}")

    def get_result(self):
        """å‡¦ç†çµæœã‚’å–å¾—"""
        if not self.result_queue.empty():
            return self.result_queue.get()
        return None

    def get_fps(self):
        """ç¾åœ¨ã®FPSã‚’å–å¾—"""
        if len(self.fps_buffer) > 0:
            return sum(self.fps_buffer) / len(self.fps_buffer)
        return 0.0

# ä½¿ç”¨ä¾‹
model = SimpleCNN(num_classes=3)
device = 'cuda' if torch.cuda.is_available() else 'cpu'

pipeline = RealTimeInferencePipeline(model, device=device, batch_size=8)
pipeline.start()

# ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼šé€£ç¶šãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†
n_frames = 100
results = []

start_time = time.time()

for frame_id in range(n_frames):
    # ãƒ€ãƒŸãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆï¼ˆå®Ÿéš›ã¯ã‚«ãƒ¡ãƒ©ã‹ã‚‰ã®ç”»åƒï¼‰
    dummy_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)

    # å‡¦ç†
    pipeline.process_frame(frame_id, dummy_frame)

    # çµæœã‚’å–å¾—
    result = pipeline.get_result()
    if result:
        results.append(result)

    time.sleep(0.01)  # 100 FPSã‚’æ¨¡æ“¬

# æ®‹ã‚Šã®çµæœã‚’å–å¾—
time.sleep(1)
while True:
    result = pipeline.get_result()
    if result is None:
        break
    results.append(result)

pipeline.stop()

# çµ±è¨ˆ
total_time = time.time() - start_time
avg_fps = pipeline.get_fps()

print(f"\nProcessing statistics:")
print(f"  Total frames: {n_frames}")
print(f"  Processed frames: {len(results)}")
print(f"  Total time: {total_time:.2f}s")
print(f"  Average FPS: {avg_fps:.2f}")
print(f"  Average inference time: {np.mean([r['inference_time'] for r in results])*1000:.2f}ms")

# å‡ºåŠ›ä¾‹:
# Model optimized with TorchScript
# Inference pipeline started
# Inference pipeline stopped
#
# Processing statistics:
#   Total frames: 100
#   Processed frames: 100
#   Total time: 1.23s
#   Average FPS: 87.34
#   Average inference time: 11.47ms
</code></pre>

            <div class="tip-box">
                <p><strong>ğŸ’¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ã®æœ€é©åŒ–</strong></p>
                <ul>
                    <li><strong>TorchScriptå¤‰æ›</strong>: æ¨è«–é€Ÿåº¦ã‚’20-40%å‘ä¸Š</li>
                    <li><strong>é‡å­åŒ–</strong>: INT8é‡å­åŒ–ã§ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¨æ¨è«–æ™‚é–“ã‚’1/4ã«å‰Šæ¸›</li>
                    <li><strong>ONNXå¤‰æ›</strong>: TensorRTãªã©ã®ã‚¨ãƒ³ã‚¸ãƒ³ã§æ›´ãªã‚‹é«˜é€ŸåŒ–</li>
                    <li><strong>GPUä¸¦åˆ—åŒ–</strong>: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’èª¿æ•´ã—ã¦GPUä½¿ç”¨ç‡æœ€å¤§åŒ–</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>å­¦ç¿’ç›®æ¨™ã®ç¢ºèª</h2>

            <p>ã“ã®ç« ã‚’å®Œäº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ã‚’å®Ÿè£…ãƒ»èª¬æ˜ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š</p>

            <h3>åŸºæœ¬ç†è§£</h3>
            <ul>
                <li>ç•³ã¿è¾¼ã¿å±¤ã¨ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã®å½¹å‰²ã‚’èª¬æ˜ã§ãã‚‹</li>
                <li>CNNãŒç”»åƒã®ç©ºé–“çš„æ§‹é€ ã‚’ä¿æŒã™ã‚‹ä»•çµ„ã¿ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
                <li>U-Netã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€æ§‹é€ ã¨skip connectionã®æ„ç¾©ã‚’èª¬æ˜ã§ãã‚‹</li>
                <li>è»¢ç§»å­¦ç¿’ãŒãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã‚’æ”¹å–„ã™ã‚‹ç†ç”±ã‚’ç†è§£ã—ã¦ã„ã‚‹</li>
            </ul>

            <h3>å®Ÿè·µã‚¹ã‚­ãƒ«</h3>
            <ul>
                <li>PyTorchã§CNNã‚’å®Ÿè£…ã—ã€ãƒ—ãƒ­ã‚»ã‚¹ç”»åƒã‚’åˆ†é¡ã§ãã‚‹</li>
                <li>U-Netã§è£½å“æ¬ é™¥ã‚’ãƒ”ã‚¯ã‚»ãƒ«å˜ä½ã§æ¤œå‡ºã§ãã‚‹</li>
                <li>YOLOã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§è¨­å‚™ã®ç‰©ä½“æ¤œå‡ºãŒã§ãã‚‹</li>
                <li>ç†±ç”»åƒã‹ã‚‰æ¸©åº¦åˆ†å¸ƒã‚’æ¨å®šã§ãã‚‹</li>
                <li>ResNet/EfficientNetã§è»¢ç§»å­¦ç¿’ã‚’å®Ÿè£…ã§ãã‚‹</li>
                <li>Grad-CAMã§ãƒ¢ãƒ‡ãƒ«ã®åˆ¤æ–­æ ¹æ‹ ã‚’å¯è¦–åŒ–ã§ãã‚‹</li>
                <li>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
            </ul>

            <h3>å¿œç”¨åŠ›</h3>
            <ul>
                <li>ãƒ—ãƒ­ã‚»ã‚¹ã®ç‰¹æ€§ã«å¿œã˜ã¦é©åˆ‡ãªCNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’é¸æŠã§ãã‚‹</li>
                <li>Grad-CAMè§£æã‹ã‚‰ç•°å¸¸æ¤œçŸ¥ã®ç²¾åº¦å‘ä¸Šç­–ã‚’å°å‡ºã§ãã‚‹</li>
                <li>è£½é€ ãƒ©ã‚¤ãƒ³ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¦ä»¶ã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«ã‚’æœ€é©åŒ–ã§ãã‚‹</li>
                <li>å°‘é‡ãƒ‡ãƒ¼ã‚¿ã§è»¢ç§»å­¦ç¿’ã‚’æ´»ç”¨ã—ã€å®Ÿç”¨ãƒ¬ãƒ™ãƒ«ã®ç²¾åº¦ã‚’é”æˆã§ãã‚‹</li>
            </ul>
        </section>

        <section>
            <h2>å‚è€ƒæ–‡çŒ®</h2>
            <ol>
                <li>LeCun, Y., et al. (1998). "Gradient-Based Learning Applied to Document Recognition." Proceedings of the IEEE, 86(11), 2278-2324.</li>
                <li>Ronneberger, O., et al. (2015). "U-Net: Convolutional Networks for Biomedical Image Segmentation." MICCAI 2015.</li>
                <li>Redmon, J., et al. (2016). "You Only Look Once: Unified, Real-Time Object Detection." CVPR 2016.</li>
                <li>He, K., et al. (2016). "Deep Residual Learning for Image Recognition." CVPR 2016.</li>
                <li>Tan, M., & Le, Q. V. (2019). "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks." ICML 2019.</li>
                <li>Selvaraju, R. R., et al. (2017). "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization." ICCV 2017.</li>
            </ol>
        </section>

        <div class="navigation">
            <a href="chapter-2.html" class="nav-button">â† ç¬¬2ç« ï¼šTransformer</a>
            <a href="chapter-4.html" class="nav-button">ç¬¬4ç« ï¼šã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ â†’</a>
        </div>
    </main>
</body>
</html>
