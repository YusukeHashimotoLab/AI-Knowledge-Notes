<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第1章: HPCクラスタの基礎とジョブ管理 - HPCクラスタ入門 - CT Dojo</title>
    <meta name="description" content="HPCクラスタの基礎、並列計算アーキテクチャ、ジョブスケジューラ（SLURM、PBS、SGE）、SSHログイン、ジョブスクリプト作成を学ぶ実践ガイド。">

    <!-- Prism.js for code highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">

    <!-- MathJax for mathematical expressions -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>

    <style>
        :root {
            --accent-green: #11998e;
            --accent-lime: #38ef7d;
            --primary-dark: #2c3e50;
            --secondary-dark: #34495e;
            --text-dark: #2c3e50;
            --text-light: #7f8c8d;
            --bg-light: #ecf0f1;
            --white: #ffffff;
            --code-bg: #2d2d2d;
            --border-light: #bdc3c7;
            --success: #27ae60;
            --warning: #f39c12;
            --danger: #e74c3c;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.8;
            color: var(--text-dark);
            background: var(--bg-light);
        }

        header {
            background: linear-gradient(135deg, var(--accent-green) 0%, var(--accent-lime) 100%);
            color: white;
            padding: 3rem 1.5rem;
            text-align: center;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        header h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
        }

        header p {
            font-size: 1.1rem;
            opacity: 0.95;
        }

        nav {
            background: var(--white);
            padding: 1rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            position: sticky;
            top: 0;
            z-index: 100;
        }

        nav ul {
            list-style: none;
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 0.5rem;
        }

        nav a {
            text-decoration: none;
            color: var(--text-dark);
            padding: 0.5rem 1rem;
            border-radius: 4px;
            transition: all 0.3s;
            font-weight: 500;
        }

        nav a:hover {
            background: linear-gradient(135deg, var(--accent-green) 0%, var(--accent-lime) 100%);
            color: white;
        }

        main {
            max-width: 900px;
            margin: 2rem auto;
            padding: 0 1.5rem;
        }

        section {
            background: var(--white);
            padding: 2rem;
            margin-bottom: 2rem;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        h2 {
            color: var(--primary-dark);
            font-size: 1.8rem;
            margin-bottom: 1.5rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid;
            border-image: linear-gradient(90deg, var(--accent-green), var(--accent-lime)) 1;
        }

        h3 {
            color: var(--secondary-dark);
            font-size: 1.4rem;
            margin: 2rem 0 1rem;
        }

        h4 {
            color: var(--secondary-dark);
            font-size: 1.2rem;
            margin: 1.5rem 0 1rem;
        }

        p {
            margin-bottom: 1rem;
            line-height: 1.8;
        }

        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        code {
            background: #f8f9fa;
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: "Consolas", "Monaco", monospace;
            font-size: 0.9em;
            color: #e74c3c;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1.5rem 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.15);
        }

        pre code {
            background: none;
            color: #f8f8f2;
            padding: 0;
        }

        .info-box {
            background: linear-gradient(135deg, rgba(17, 153, 142, 0.1) 0%, rgba(56, 239, 125, 0.1) 100%);
            border-left: 4px solid var(--accent-green);
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }

        .info-box strong {
            color: var(--accent-green);
            display: block;
            margin-bottom: 0.5rem;
            font-size: 1.1rem;
        }

        .warning-box {
            background: rgba(243, 156, 18, 0.1);
            border-left: 4px solid var(--warning);
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }

        .warning-box strong {
            color: var(--warning);
            display: block;
            margin-bottom: 0.5rem;
            font-size: 1.1rem;
        }

        .exercise-box {
            background: rgba(39, 174, 96, 0.05);
            border: 2px solid var(--success);
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 8px;
        }

        .exercise-box h4 {
            color: var(--success);
            margin-top: 0;
        }

        .difficulty {
            display: inline-block;
            padding: 0.25rem 0.75rem;
            border-radius: 12px;
            font-size: 0.85rem;
            font-weight: 600;
            margin-left: 0.5rem;
        }

        .difficulty.easy {
            background: #d4edda;
            color: #155724;
        }

        .difficulty.medium {
            background: #fff3cd;
            color: #856404;
        }

        .difficulty.hard {
            background: #f8d7da;
            color: #721c24;
        }

        .mermaid {
            background: white;
            padding: 2rem;
            border-radius: 8px;
            margin: 2rem 0;
            text-align: center;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            background: white;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        th, td {
            padding: 1rem;
            text-align: left;
            border-bottom: 1px solid var(--border-light);
        }

        th {
            background: linear-gradient(135deg, var(--accent-green) 0%, var(--accent-lime) 100%);
            color: white;
            font-weight: 600;
        }

        tr:hover {
            background: rgba(17, 153, 142, 0.05);
        }

        footer {
            background: var(--primary-dark);
            color: white;
            text-align: center;
            padding: 2rem;
            margin-top: 4rem;
        }

        footer a {
            color: var(--accent-green);
            text-decoration: none;
        }

        footer a:hover {
            color: var(--accent-lime);
        }

        @media (max-width: 768px) {
            header h1 {
                font-size: 1.5rem;
            }

            nav ul {
                flex-direction: column;
                align-items: center;
            }

            section {
                padding: 1.5rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.85rem;
            }
        }



        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #11998e;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #0e7c74;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }

        details {
            margin: 1rem 0;
            padding: 1rem;
            background: #f8f9fa;
            border-radius: 4px;
            border-left: 3px solid var(--accent-green);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--accent-green);
            user-select: none;
        }

        summary:hover {
            color: var(--accent-lime);
        }

        details[open] summary {
            margin-bottom: 1rem;
        }
    </style>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/AI-Knowledge-Notes/knowledge/jp/index.html">AI寺子屋トップ</a><span class="breadcrumb-separator">›</span><a href="/AI-Knowledge-Notes/knowledge/jp/CT/index.html">計算工学</a><span class="breadcrumb-separator">›</span><a href="/AI-Knowledge-Notes/knowledge/jp/CT/hpc-cluster-introduction/index.html">HPC Cluster</a><span class="breadcrumb-separator">›</span><span class="breadcrumb-current">Chapter 1</span>
        </div>
    </nav>

        <header>
        <h1>第1章: HPCクラスタの基礎とジョブ管理</h1>
        <p>並列計算アーキテクチャ・ジョブスケジューラ・SSH接続・ジョブスクリプト作成</p>
    </header>

    <nav>
        <ul>
            <li><a href="index.html">トップ</a></li>
            <li><a href="#intro">本章の概要</a></li>
            <li><a href="#hpc-basics">HPC基礎</a></li>
            <li><a href="#job-schedulers">ジョブスケジューラ</a></li>
            <li><a href="#login-transfer">ログインとファイル転送</a></li>
            <li><a href="#job-scripts">ジョブスクリプト</a></li>
            <li><a href="#learning-objectives">学習目標</a></li>
            <li><a href="#exercises">演習問題</a></li>
            <li><a href="#references">参考文献</a></li>
            <li><a href="chapter-2.html">次の章へ →</a></li>
        </ul>
    </nav>

    <main>
        <section id="intro">
            <h2>1.1 本章の概要</h2>

            <p>
                HPCクラスタ（High-Performance Computing Cluster）は、多数の計算ノードを高速ネットワークで接続した並列計算システムです。大規模シミュレーション、データ解析、機械学習モデルの訓練など、単一計算機では実行困難なタスクを高速に処理できます。本章では、HPCクラスタの基本構成、ジョブスケジューラの使い方、効果的なジョブ管理方法を学びます。
            </p>

            <div class="info-box">
                <strong>本章の学習目標</strong>
                <ul>
                    <li><strong>レベル1（基本理解）</strong>: HPCクラスタのアーキテクチャ（ノード、インターコネクト、ストレージ）を説明でき、ジョブスケジューラの役割と動作原理を理解できる</li>
                    <li><strong>レベル2（実践スキル）</strong>: SSH経由でHPCにログインし、SLURMでジョブを投入・監視・削除でき、基本的なジョブスクリプトを作成できる</li>
                    <li><strong>レベル3（応用力）</strong>: リソース要求を最適化し、アレイジョブや依存関係ジョブを設計でき、ジョブの性能を評価できる</li>
                </ul>
            </div>

            <div class="warning-box">
                <strong>前提知識</strong>
                <p>本章では、Linux基本操作（シェルコマンド、環境変数）、SSH基礎、シェルスクリプトの基本を前提とします。</p>
            </div>
        </section>

        <section id="hpc-basics">
            <h2>1.2 HPCクラスタの基礎</h2>

            <h3>HPCクラスタのアーキテクチャ</h3>
            <p>
                HPCクラスタは、<strong>ログインノード</strong>（ユーザーがログインする窓口）、<strong>計算ノード</strong>（実際の計算を実行）、<strong>ストレージノード</strong>（データ保存）、<strong>インターコネクト</strong>（高速ネットワーク）から構成されます。
            </p>

            <div class="mermaid">
flowchart TB
    subgraph Users["ユーザー"]
        U1[研究者A]
        U2[研究者B]
        U3[研究者C]
    end

    subgraph Login["ログインノード"]
        L1[login01]
        L2[login02]
    end

    subgraph Scheduler["ジョブスケジューラ"]
        S[SLURM<br/>マスターノード]
    end

    subgraph Compute["計算ノード群"]
        C1[node001<br/>32 cores]
        C2[node002<br/>32 cores]
        C3[node003<br/>32 cores]
        C4[node004-128<br/>...]
    end

    subgraph Storage["ストレージ"]
        ST1[ホーム<br/>/home]
        ST2[スクラッチ<br/>/scratch]
        ST3[共有<br/>/work]
    end

    Users -->|SSH| Login
    Login -->|ジョブ投入| Scheduler
    Scheduler -->|スケジューリング| Compute
    Compute -->|データアクセス| Storage

    style Users fill:#e3f2fd
    style Login fill:#fff3e0
    style Scheduler fill:#f3e5f5
    style Compute fill:#e8f5e9
    style Storage fill:#fff9c4
            </div>

            <h3>並列計算の種類</h3>
            <table>
                <thead>
                    <tr>
                        <th>並列化方式</th>
                        <th>メモリモデル</th>
                        <th>適用範囲</th>
                        <th>主要技術</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>分散メモリ並列</strong></td>
                        <td>各ノードが独立したメモリ</td>
                        <td>複数ノード</td>
                        <td>MPI (Message Passing Interface)</td>
                    </tr>
                    <tr>
                        <td><strong>共有メモリ並列</strong></td>
                        <td>スレッドがメモリを共有</td>
                        <td>単一ノード内</td>
                        <td>OpenMP, pthreads</td>
                    </tr>
                    <tr>
                        <td><strong>ハイブリッド並列</strong></td>
                        <td>両方の組み合わせ</td>
                        <td>複数ノード+ノード内</td>
                        <td>MPI + OpenMP</td>
                    </tr>
                </tbody>
            </table>

            <h3>Amdahlの法則とスケーラビリティ</h3>
            <p>
                <strong>Amdahlの法則</strong>は、並列化可能な部分の割合\(P\)とプロセッサ数\(N\)に対する理論的高速化率を示します：
            </p>

            <p style="text-align: center; font-size: 1.1rem; margin: 1.5rem 0;">
                \[
                \text{Speedup} = \frac{1}{(1 - P) + \frac{P}{N}}
                \]
            </p>

            <p>
                例えば、プログラムの90%が並列化可能（\(P = 0.9\)）な場合、100プロセッサを使っても理論的高速化率は約9.17倍が上限です。これは、並列化できない10%の部分がボトルネックになるためです。
            </p>

            <div class="info-box">
                <strong>実践的な意味</strong>
                <p>
                    並列化効率を高めるには、逐次部分（ファイルI/O、初期化処理等）を最小化することが重要です。第4章で性能最適化を詳しく学びます。
                </p>
            </div>
        </section>

        <section id="job-schedulers">
            <h2>1.3 ジョブスケジューラ（SLURM、PBS、SGE）</h2>

            <h3>ジョブスケジューラの役割</h3>
            <p>
                ジョブスケジューラは、複数ユーザーからのジョブ（計算タスク）を効率的に計算ノードに割り当て、リソースを公平に分配します。主な機能：
            </p>
            <ul>
                <li><strong>リソース管理</strong>: CPUコア数、メモリ、GPU、実行時間などのリソース割り当て</li>
                <li><strong>優先度制御</strong>: キュー（待ち行列）ごとの優先度、ユーザーごとのクォータ管理</li>
                <li><strong>ジョブ監視</strong>: 実行状態の追跡、ログ記録、異常終了時の通知</li>
                <li><strong>スケジューリング</strong>: バックフィル（小ジョブの隙間埋め）、フェアシェア（公平分配）</li>
            </ul>

            <h3>主要ジョブスケジューラの比較</h3>
            <table>
                <thead>
                    <tr>
                        <th>スケジューラ</th>
                        <th>特徴</th>
                        <th>採用例</th>
                        <th>コマンド例</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>SLURM</strong></td>
                        <td>最も普及、スケーラビリティ高い</td>
                        <td>東大Wisteria、理研富岳</td>
                        <td><code>sbatch</code>, <code>squeue</code>, <code>scancel</code></td>
                    </tr>
                    <tr>
                        <td><strong>PBS/Torque</strong></td>
                        <td>歴史が長い、安定性重視</td>
                        <td>東工大TSUBAME、NIMS</td>
                        <td><code>qsub</code>, <code>qstat</code>, <code>qdel</code></td>
                    </tr>
                    <tr>
                        <td><strong>SGE</strong></td>
                        <td>設定の柔軟性が高い</td>
                        <td>一部大学、企業クラスタ</td>
                        <td><code>qsub</code>, <code>qstat</code>, <code>qdel</code></td>
                    </tr>
                </tbody>
            </table>

            <p>
                本シリーズでは、最も普及している<strong>SLURM（Simple Linux Utility for Resource Management）</strong>を中心に解説します。
            </p>

            <h3>SLURMの基本コマンド</h3>

            <h4>ジョブ投入（sbatch）</h4>
            <pre><code class="language-bash"># ジョブスクリプトを投入
sbatch my_job.sh

# 出力例:
# Submitted batch job 123456
</code></pre>

            <h4>ジョブ状態確認（squeue）</h4>
            <pre><code class="language-bash"># 自分のジョブ一覧を表示
squeue -u $USER

# 出力例:
#  JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
# 123456     normal  test.sh  user01  R       5:23      2 node[001-002]
# 123457     normal  calc.sh  user01 PD       0:00      4 (Priority)

# ST（状態）: R=実行中、PD=待機中、CG=完了処理中
</code></pre>

            <h4>ジョブキャンセル（scancel）</h4>
            <pre><code class="language-bash"># ジョブIDを指定してキャンセル
scancel 123456

# 自分の全ジョブをキャンセル
scancel -u $USER
</code></pre>

            <h4>ノード情報確認（sinfo）</h4>
            <pre><code class="language-bash"># クラスタのノード状態を表示
sinfo

# 出力例:
# PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
# normal*      up   12:00:00     10   idle node[001-010]
# gpu          up   24:00:00      4  alloc node[201-204]
# highmem      up    7-00:00      2  down* node[301-302]

# STATE: idle=空き、alloc=使用中、down=停止中
</code></pre>
        </section>

        <section id="login-transfer">
            <h2>1.4 ログインとファイル転送</h2>

            <h3>コード例1: SSH経由でHPCにログイン</h3>
            <pre><code class="language-bash"># 基本的なSSHログイン
ssh username@hpc-cluster.university.edu

# ポート番号指定（デフォルトは22）
ssh -p 22022 username@hpc-cluster.university.edu

# SSH鍵認証の設定（初回のみ）
# 1. ローカルで鍵ペア生成
ssh-keygen -t rsa -b 4096 -C "your_email@example.com"

# 2. 公開鍵をHPCサーバーにコピー
ssh-copy-id username@hpc-cluster.university.edu

# 3. 以降はパスワードなしでログイン可能
ssh username@hpc-cluster.university.edu
</code></pre>

            <div class="info-box">
                <strong>セキュリティのベストプラクティス</strong>
                <ul>
                    <li>SSH鍵は最低4096ビット以上を使用</li>
                    <li>秘密鍵（<code>~/.ssh/id_rsa</code>）は絶対に共有しない</li>
                    <li>VPN経由でのアクセスを推奨（学外から接続する場合）</li>
                    <li>定期的にパスワードとSSH鍵を更新</li>
                </ul>
            </div>

            <h3>コード例2: SCPでファイル転送</h3>
            <pre><code class="language-bash"># ローカル → HPC（アップロード）
scp local_file.txt username@hpc-cluster.edu:/home/username/data/

# HPC → ローカル（ダウンロード）
scp username@hpc-cluster.edu:/scratch/results.tar.gz ./

# ディレクトリごと転送（再帰的）
scp -r local_directory/ username@hpc-cluster.edu:/home/username/project/

# 圧縮して転送（帯域節約）
scp -C large_file.dat username@hpc-cluster.edu:/scratch/

# 進捗表示付き
scp -v local_file.txt username@hpc-cluster.edu:/home/username/
</code></pre>

            <h3>コード例3: rsyncで効率的なファイル同期</h3>
            <pre><code class="language-bash"># 基本的な同期（差分のみ転送）
rsync -avz local_directory/ username@hpc-cluster.edu:/home/username/project/

# オプション解説:
# -a: アーカイブモード（パーミッション、タイムスタンプ保持）
# -v: 詳細表示
# -z: 転送時に圧縮

# 削除ファイルも同期（ミラーリング）
rsync -avz --delete local_directory/ username@hpc-cluster.edu:/home/username/project/

# 進捗表示付き（大規模ファイル向け）
rsync -avz --progress large_dataset/ username@hpc-cluster.edu:/scratch/data/

# 帯域制限（ネットワーク負荷軽減）
rsync -avz --bwlimit=5000 local_directory/ username@hpc-cluster.edu:/home/username/
# bwlimit: KB/s単位（5000 = 約5 MB/s）

# 除外パターン指定
rsync -avz --exclude '*.tmp' --exclude '.git/' project/ username@hpc-cluster.edu:/work/
</code></pre>

            <div class="warning-box">
                <strong>ファイル転送の注意点</strong>
                <ul>
                    <li><strong>ストレージの使い分け</strong>: 一時ファイルは<code>/scratch</code>、長期保存は<code>/home</code>や<code>/work</code></li>
                    <li><strong>クォータ確認</strong>: <code>quota -s</code>でディスク使用量を確認</li>
                    <li><strong>大規模データ</strong>: Globus Connectなどの専用転送サービスを利用</li>
                </ul>
            </div>
        </section>

        <section id="job-scripts">
            <h2>1.5 ジョブスクリプトの作成</h2>

            <h3>ジョブスクリプトの基本構造</h3>
            <p>
                SLURMジョブスクリプトは、<strong>シェバン（shebang）</strong>、<strong>SLURMディレクティブ（#SBATCH）</strong>、<strong>実行コマンド</strong>の3部構成です。
            </p>

            <h3>コード例4: 基本的なSLURMジョブスクリプト</h3>
            <pre><code class="language-bash">#!/bin/bash
#SBATCH --job-name=my_first_job       # ジョブ名
#SBATCH --output=output_%j.log        # 標準出力ファイル（%j=ジョブID）
#SBATCH --error=error_%j.log          # 標準エラー出力ファイル
#SBATCH --time=01:00:00               # 最大実行時間（1時間）
#SBATCH --partition=normal            # パーティション（キュー）
#SBATCH --nodes=1                     # ノード数
#SBATCH --ntasks-per-node=4           # ノードあたりのタスク数
#SBATCH --cpus-per-task=1             # タスクあたりのCPUコア数
#SBATCH --mem=8G                      # メモリ（8GB）
#SBATCH --mail-type=END,FAIL          # メール通知（終了、失敗時）
#SBATCH --mail-user=user@example.com  # 通知先メールアドレス

# 環境設定
module load python/3.9
module load openmpi/4.1.1

# 作業ディレクトリに移動
cd $SLURM_SUBMIT_DIR

# ジョブ情報を記録
echo "Job ID: $SLURM_JOB_ID"
echo "Node List: $SLURM_JOB_NODELIST"
echo "Number of Nodes: $SLURM_JOB_NUM_NODES"
echo "Start Time: $(date)"

# 実際の計算を実行
python my_calculation.py --input data.csv --output results.txt

# 完了メッセージ
echo "End Time: $(date)"
echo "Job completed successfully"
</code></pre>

            <h3>リソース指定のベストプラクティス</h3>
            <table>
                <thead>
                    <tr>
                        <th>リソース</th>
                        <th>指定方法</th>
                        <th>推奨値</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>実行時間</strong></td>
                        <td><code>--time=HH:MM:SS</code></td>
                        <td>実測の1.2-1.5倍（余裕を持たせる）</td>
                    </tr>
                    <tr>
                        <td><strong>ノード数</strong></td>
                        <td><code>--nodes=N</code></td>
                        <td>スケーラビリティテスト後に決定</td>
                    </tr>
                    <tr>
                        <td><strong>メモリ</strong></td>
                        <td><code>--mem=XG</code></td>
                        <td>ピークメモリの1.2倍程度</td>
                    </tr>
                    <tr>
                        <td><strong>CPUコア数</strong></td>
                        <td><code>--ntasks=N</code></td>
                        <td>MPIプロセス数と一致</td>
                    </tr>
                </tbody>
            </table>

            <h3>コード例5: アレイジョブとジョブ依存関係</h3>
            <pre><code class="language-bash"># ===================================
# アレイジョブ（複数パラメータで並列実行）
# ===================================
#!/bin/bash
#SBATCH --job-name=param_sweep
#SBATCH --output=output_%A_%a.log     # %A=アレイジョブID、%a=タスクID
#SBATCH --array=1-100                  # タスク1から100を並列実行
#SBATCH --time=00:30:00
#SBATCH --ntasks=1
#SBATCH --mem=4G

# タスクIDに応じたパラメータ設定
PARAM=$(sed -n "${SLURM_ARRAY_TASK_ID}p" parameter_list.txt)

# 計算実行
python simulation.py --param $PARAM --output result_${SLURM_ARRAY_TASK_ID}.csv

# ===================================
# ジョブ依存関係（パイプライン実行）
# ===================================

# ステップ1: データ前処理ジョブを投入
JOB1=$(sbatch --parsable preprocess.sh)
echo "Preprocessing Job ID: $JOB1"

# ステップ2: ステップ1が成功したら計算ジョブを投入
JOB2=$(sbatch --parsable --dependency=afterok:$JOB1 calculation.sh)
echo "Calculation Job ID: $JOB2"

# ステップ3: ステップ2が成功したら後処理ジョブを投入
JOB3=$(sbatch --parsable --dependency=afterok:$JOB2 postprocess.sh)
echo "Postprocessing Job ID: $JOB3"

# 依存関係の種類:
# afterok: 正常終了後に実行
# afterany: 終了後に実行（成否問わず）
# afternotok: 異常終了後に実行
</code></pre>

            <div class="info-box">
                <strong>アレイジョブのユースケース</strong>
                <ul>
                    <li><strong>パラメータスイープ</strong>: 複数のパラメータ組み合わせを試す</li>
                    <li><strong>データ分割処理</strong>: 大規模データセットを分割して並列処理</li>
                    <li><strong>アンサンブル計算</strong>: 乱数シードを変えて複数回実行</li>
                    <li><strong>ハイパーパラメータ探索</strong>: 機械学習モデルの最適化</li>
                </ul>
            </div>

            <h3>リソース使用状況のモニタリング</h3>
            <pre><code class="language-bash"># ジョブの詳細情報を表示
scontrol show job 123456

# ジョブの効率レポート（完了後）
seff 123456
# 出力例:
# Job ID: 123456
# Cluster: hpc-cluster
# State: COMPLETED (exit code 0)
# Cores: 4
# CPU Utilized: 03:45:20
# CPU Efficiency: 93.83% of 04:00:00 core-walltime
# Memory Utilized: 7.2 GB
# Memory Efficiency: 90.00% of 8.0 GB

# 実行中ジョブのリソース使用状況
sstat --format=JobID,AveCPU,AveRSS,MaxRSS -j 123456
</code></pre>
        </section>

        <section id="learning-objectives">
            <h2>学習目標の確認</h2>

            <p>この章を完了すると、以下を説明・実行できるようになります：</p>

            <h3>基本理解</h3>
            <ul>
                <li>✅ HPCクラスタのアーキテクチャ（ログインノード、計算ノード、ストレージ、インターコネクト）を説明できる</li>
                <li>✅ ジョブスケジューラの役割（リソース管理、スケジューリング、優先度制御）を理解できる</li>
                <li>✅ SLURM、PBS、SGEの違いと基本コマンドを説明できる</li>
                <li>✅ Amdahlの法則を使って並列化の理論的限界を計算できる</li>
            </ul>

            <h3>実践スキル</h3>
            <ul>
                <li>✅ SSH経由でHPCクラスタにログインし、基本操作ができる</li>
                <li>✅ SCP・rsyncで効率的にファイル転送ができる</li>
                <li>✅ SLURMジョブスクリプトを作成し、<code>sbatch</code>で投入できる</li>
                <li>✅ <code>squeue</code>でジョブ状態を確認し、<code>scancel</code>でキャンセルできる</li>
                <li>✅ アレイジョブとジョブ依存関係を設定できる</li>
            </ul>

            <h3>応用力</h3>
            <ul>
                <li>✅ リソース要求（CPU、メモリ、時間）を適切に見積もれる</li>
                <li>✅ ジョブの効率レポート（<code>seff</code>）を読み、最適化ポイントを特定できる</li>
                <li>✅ 複雑なワークフロー（前処理→計算→後処理）をジョブ依存関係で実装できる</li>
            </ul>
        </section>

        <section id="exercises">
            <h2>演習問題</h2>

            <h3>Easy（基礎確認）<span class="difficulty easy">初級</span></h3>

            <div class="exercise-box">
                <h4>Q1: ジョブスケジューラの役割</h4>
                <p>ジョブスケジューラの主要な役割を3つ挙げてください。</p>

                <details>
                    <summary>解答を見る</summary>
                    <p><strong>正解</strong>:</p>
                    <ol>
                        <li><strong>リソース管理</strong>: CPUコア、メモリ、GPU等のリソースを各ジョブに割り当てる</li>
                        <li><strong>スケジューリング</strong>: ジョブの優先度、キュー、公平性を考慮して実行順序を決定</li>
                        <li><strong>ジョブ監視</strong>: 実行状態の追跡、ログ記録、異常終了時の処理</li>
                    </ol>
                    <p><strong>補足</strong>: これにより、複数ユーザーが効率的に共有リソースを使えます。</p>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Q2: SLURMコマンドの対応</h4>
                <p>以下の操作に対応するSLURMコマンドを答えてください。</p>
                <ul>
                    <li>(a) ジョブを投入する</li>
                    <li>(b) ジョブ一覧を表示する</li>
                    <li>(c) ジョブをキャンセルする</li>
                </ul>

                <details>
                    <summary>解答を見る</summary>
                    <p><strong>正解</strong>:</p>
                    <ul>
                        <li>(a) <code>sbatch job_script.sh</code></li>
                        <li>(b) <code>squeue -u $USER</code></li>
                        <li>(c) <code>scancel JOB_ID</code></li>
                    </ul>
                    <p><strong>補足</strong>: PBSでは<code>qsub</code>、<code>qstat</code>、<code>qdel</code>が対応します。</p>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Q3: HPCクラスタのノード種類</h4>
                <p>HPCクラスタの主要なノード種類を3つ挙げ、それぞれの役割を説明してください。</p>

                <details>
                    <summary>解答を見る</summary>
                    <p><strong>正解</strong>:</p>
                    <ol>
                        <li><strong>ログインノード</strong>: ユーザーがSSHで接続する窓口。ジョブ投入、ファイル編集等の軽作業を行う</li>
                        <li><strong>計算ノード</strong>: 実際の大規模計算を実行。ユーザーは直接ログインできない（ジョブ経由のみ）</li>
                        <li><strong>ストレージノード</strong>: 大規模データを保存する専用ノード。並列ファイルシステムで高速アクセス</li>
                    </ol>
                    <p><strong>注意</strong>: ログインノードで重い計算を実行するのは禁止されています（他ユーザーに影響）。</p>
                </details>
            </div>

            <h3>Medium（応用）<span class="difficulty medium">中級</span></h3>

            <div class="exercise-box">
                <h4>Q4: Amdahlの法則の計算</h4>
                <p>プログラムの95%が並列化可能な場合、64プロセッサを使ったときの理論的高速化率を計算してください。</p>

                <details>
                    <summary>解答を見る</summary>
                    <p><strong>正解</strong>: 約16.3倍</p>
                    <p><strong>計算過程</strong>:</p>
                    <p>Amdahlの法則: \(\text{Speedup} = \frac{1}{(1 - P) + \frac{P}{N}}\)</p>
                    <ul>
                        <li>\(P = 0.95\)（並列化可能部分）</li>
                        <li>\(N = 64\)（プロセッサ数）</li>
                    </ul>
                    <p>\[
                    \text{Speedup} = \frac{1}{(1 - 0.95) + \frac{0.95}{64}} = \frac{1}{0.05 + 0.0148} = \frac{1}{0.0648} \approx 15.4
                    \]</p>
                    <p><strong>考察</strong>: わずか5%の逐次部分が、理論的高速化率を64倍から約15倍に制限しています。並列化効率を高めるには、逐次部分の最小化が重要です。</p>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Q5: rsyncとscpの使い分け</h4>
                <p>以下の状況で、rsyncとscpのどちらを使うべきか、理由とともに答えてください。</p>
                <ul>
                    <li>(a) 10GBのデータセットを初めて転送する</li>
                    <li>(b) 100個のファイルのうち3個だけ変更されたディレクトリを再転送する</li>
                </ul>

                <details>
                    <summary>解答を見る</summary>
                    <p><strong>正解</strong>:</p>
                    <ul>
                        <li>(a) <strong>どちらでもOK</strong>。初回転送なので差分計算のオーバーヘッドがないため、<code>scp -C</code>（圧縮付き）でも十分。</li>
                        <li>(b) <strong>rsync推奨</strong>。差分のみ転送するため、3ファイル分の転送時間で済む。scpは全100ファイルを再転送。</li>
                    </ul>
                    <p><strong>コマンド例</strong>:</p>
                    <pre><code class="language-bash"># (b)の場合
rsync -avz --progress dataset/ user@hpc:/scratch/dataset/</code></pre>
                    <p><strong>実測例</strong>: 1%変更のディレクトリで、scpは100秒、rsyncは2秒（約50倍高速）。</p>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Q6: ジョブスクリプトのデバッグ</h4>
                <p>以下のジョブスクリプトに3つの問題があります。見つけて修正してください。</p>
                <pre><code class="language-bash">#!/bin/bash
#SBATCH --job-name=test
#SBATCH --time=100:00:00
#SBATCH --mem=1T
#SBATCH --ntasks=1000

python my_script.py</code></pre>

                <details>
                    <summary>解答を見る</summary>
                    <p><strong>問題点と修正</strong>:</p>
                    <ol>
                        <li><strong>実行時間が長すぎる</strong>: <code>--time=100:00:00</code>（100時間）は多くのHPCで上限を超える。実測に基づき適切な時間（例: 12:00:00）に設定。</li>
                        <li><strong>メモリが非現実的</strong>: <code>--mem=1T</code>（1TB）は単一ノードで確保不可能。実際のメモリ使用量を測定し、適切な値（例: 32G）に設定。</li>
                        <li><strong>タスク数が多すぎる</strong>: <code>--ntasks=1000</code>はPythonスクリプト1つに不要。MPIを使わない限り<code>--ntasks=1</code>で十分。</li>
                    </ol>
                    <p><strong>修正版</strong>:</p>
                    <pre><code class="language-bash">#!/bin/bash
#SBATCH --job-name=test
#SBATCH --time=12:00:00
#SBATCH --mem=32G
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8

python my_script.py</code></pre>
                </details>
            </div>

            <h3>Hard（発展）<span class="difficulty hard">上級</span></h3>

            <div class="exercise-box">
                <h4>Q7: アレイジョブの設計</h4>
                <p>100個のパラメータ組み合わせでシミュレーションを実行したいです。各計算は30分かかり、メモリ4GB必要です。アレイジョブのジョブスクリプトを書いてください。パラメータは<code>params.txt</code>（100行）に格納されています。</p>

                <details>
                    <summary>解答を見る</summary>
                    <p><strong>解答例</strong>:</p>
                    <pre><code class="language-bash">#!/bin/bash
#SBATCH --job-name=param_sweep
#SBATCH --output=log/output_%A_%a.log
#SBATCH --error=log/error_%A_%a.log
#SBATCH --array=1-100
#SBATCH --time=00:40:00          # 余裕を持って40分
#SBATCH --ntasks=1
#SBATCH --mem=4G
#SBATCH --partition=normal

# ログディレクトリ作成
mkdir -p log results

# タスクIDに対応するパラメータを取得
PARAM=$(sed -n "${SLURM_ARRAY_TASK_ID}p" params.txt)

# シミュレーション実行
./simulation --param $PARAM --output results/result_${SLURM_ARRAY_TASK_ID}.csv

echo "Task ${SLURM_ARRAY_TASK_ID} completed successfully"</code></pre>
                    <p><strong>投入コマンド</strong>:</p>
                    <pre><code class="language-bash">sbatch array_job.sh</code></pre>
                    <p><strong>考察</strong>: 100タスクが並列実行されるため、クラスタに空きノードがあれば全体で30-40分で完了（逐次実行なら50時間）。</p>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Q8: ジョブ依存関係パイプライン</h4>
                <p>以下の3ステップからなるデータ解析パイプラインをジョブ依存関係で実装してください。</p>
                <ol>
                    <li>前処理（<code>preprocess.sh</code>）: 2時間、8GB メモリ</li>
                    <li>計算（<code>calculate.sh</code>）: 12時間、128GB メモリ、4ノード</li>
                    <li>後処理（<code>postprocess.sh</code>）: 1時間、16GB メモリ</li>
                </ol>
                <p>各ステップは前ステップが成功した場合のみ実行されます。</p>

                <details>
                    <summary>解答を見る</summary>
                    <p><strong>解答例</strong>:</p>

                    <p><strong>1. preprocess.sh</strong></p>
                    <pre><code class="language-bash">#!/bin/bash
#SBATCH --job-name=preprocess
#SBATCH --output=preprocess_%j.log
#SBATCH --time=02:30:00
#SBATCH --mem=8G
#SBATCH --ntasks=1

python preprocess.py --input raw_data.csv --output processed_data.h5
echo "Preprocessing completed"</code></pre>

                    <p><strong>2. calculate.sh</strong></p>
                    <pre><code class="language-bash">#!/bin/bash
#SBATCH --job-name=calculate
#SBATCH --output=calculate_%j.log
#SBATCH --time=12:30:00
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=32
#SBATCH --mem=128G

module load openmpi
mpirun -np 128 ./simulation processed_data.h5 results.h5
echo "Calculation completed"</code></pre>

                    <p><strong>3. postprocess.sh</strong></p>
                    <pre><code class="language-bash">#!/bin/bash
#SBATCH --job-name=postprocess
#SBATCH --output=postprocess_%j.log
#SBATCH --time=01:30:00
#SBATCH --mem=16G
#SBATCH --ntasks=1

python postprocess.py --input results.h5 --output final_report.pdf
echo "Postprocessing completed"</code></pre>

                    <p><strong>4. パイプライン投入スクリプト（submit_pipeline.sh）</strong></p>
                    <pre><code class="language-bash">#!/bin/bash

# ステップ1: 前処理
JOB1=$(sbatch --parsable preprocess.sh)
echo "Submitted preprocessing job: $JOB1"

# ステップ2: 計算（前処理が成功したら実行）
JOB2=$(sbatch --parsable --dependency=afterok:$JOB1 calculate.sh)
echo "Submitted calculation job: $JOB2 (depends on $JOB1)"

# ステップ3: 後処理（計算が成功したら実行）
JOB3=$(sbatch --parsable --dependency=afterok:$JOB2 postprocess.sh)
echo "Submitted postprocessing job: $JOB3 (depends on $JOB2)"

echo "Pipeline submitted successfully"
echo "Monitor with: squeue -u $USER"</code></pre>

                    <p><strong>実行</strong>:</p>
                    <pre><code class="language-bash">bash submit_pipeline.sh</code></pre>

                    <p><strong>出力例</strong>:</p>
                    <pre><code>Submitted preprocessing job: 123456
Submitted calculation job: 123457 (depends on 123456)
Submitted postprocessing job: 123458 (depends on 123457)
Pipeline submitted successfully</code></pre>

                    <p><strong>考察</strong>: 依存関係により、前ステップが失敗すると後続ジョブは自動的にキャンセルされます。これにより、無駄な計算を防げます。</p>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Q9: リソース効率の最適化</h4>
                <p><code>seff</code>コマンドで以下の出力が得られました。このジョブのリソース使用効率を評価し、次回実行時の改善案を提案してください。</p>
                <pre><code>Job ID: 123456
State: COMPLETED
Cores: 32
CPU Utilized: 04:30:00
CPU Efficiency: 14.06% of 1-08:00:00 core-walltime
Memory Utilized: 8.2 GB
Memory Efficiency: 25.63% of 32.0 GB</code></pre>

                <details>
                    <summary>解答を見る</summary>
                    <p><strong>問題点</strong>:</p>
                    <ol>
                        <li><strong>CPU効率が低い（14%）</strong>: 32コアを要求したが、実際には平均4-5コアしか使用していない</li>
                        <li><strong>メモリ効率が低い（26%）</strong>: 32GB要求したが、8.2GBしか使用していない</li>
                    </ol>

                    <p><strong>改善案</strong>:</p>
                    <pre><code class="language-bash"># 現在の設定（非効率）
#SBATCH --ntasks=32
#SBATCH --mem=32G
#SBATCH --time=1-08:00:00

# 改善版（効率的）
#SBATCH --ntasks=8           # 実際の使用コア数に近い値
#SBATCH --mem=12G            # 実測の1.5倍（8.2GB × 1.5）
#SBATCH --time=06:00:00      # 実行時間4.5時間 × 1.3</code></pre>

                    <p><strong>効果</strong>:</p>
                    <ul>
                        <li><strong>待ち時間短縮</strong>: 小さいリソース要求ほど早くスケジュールされる</li>
                        <li><strong>公平性向上</strong>: 他ユーザーがリソースを使える</li>
                        <li><strong>コスト削減</strong>: 課金制HPCでは料金が1/4に</li>
                    </ul>

                    <p><strong>検証方法</strong>:</p>
                    <ol>
                        <li>小規模テストで実測（<code>time</code>コマンド、<code>top</code>でメモリ確認）</li>
                        <li>改善版で再投入</li>
                        <li><code>seff</code>で効率が80%以上になることを確認</li>
                    </ol>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Q10: トラブルシューティング</h4>
                <p>ジョブが以下のエラーで失敗しました。原因と解決策を答えてください。</p>
                <pre><code>slurmstepd: error: Exceeded job memory limit
slurmstepd: error: Job 123456 exceeded memory limit (10240MB), being killed
srun: error: node042: task 0: Out of memory</code></pre>

                <details>
                    <summary>解答を見る</summary>
                    <p><strong>原因</strong>:</p>
                    <p>ジョブが要求したメモリ（10GB）を超えて使用したため、SLURMに強制終了されました。</p>

                    <p><strong>解決策</strong>:</p>
                    <ol>
                        <li><strong>メモリ要求を増やす</strong>:
                            <pre><code class="language-bash">#SBATCH --mem=20G  # 10GB → 20GBに増量</code></pre>
                        </li>
                        <li><strong>プログラムのメモリ使用を削減</strong>:
                            <ul>
                                <li>大きな配列をディスクに退避（メモリマップドファイル）</li>
                                <li>データをチャンク処理</li>
                                <li>不要な変数を削除（<code>del</code>、ガベージコレクション）</li>
                            </ul>
                        </li>
                        <li><strong>分散メモリ並列化</strong>:
                            <pre><code class="language-bash">#SBATCH --nodes=2
#SBATCH --ntasks-per-node=4
#SBATCH --mem-per-cpu=4G  # 合計32GB（8タスク × 4GB）</code></pre>
                        </li>
                    </ol>

                    <p><strong>デバッグ手順</strong>:</p>
                    <ol>
                        <li>小規模データでローカル実行し、<code>top</code>でピークメモリを測定</li>
                        <li>ピークメモリの1.5倍を<code>--mem</code>に設定</li>
                        <li>ジョブ完了後、<code>seff</code>でメモリ効率を確認（80-95%が理想）</li>
                    </ol>

                    <p><strong>予防策</strong>:</p>
                    <pre><code class="language-bash"># ジョブスクリプトにメモリモニタリングを追加
/usr/bin/time -v python my_script.py 2>&1 | tee time_log.txt
# Maximum resident set size (kbytes): 8123456 ← 実際のメモリ使用量</code></pre>
                </details>
            </div>
        </section>

        <section id="references">
            <h2>参考文献</h2>

            <ol>
                <li>
                    <strong>SLURM公式ドキュメント</strong>. SchedMD LLC.
                    <a href="https://slurm.schedmd.com/documentation.html" target="_blank">https://slurm.schedmd.com/documentation.html</a>
                    (特にQuick Start User Guide, pp. 1-15)
                </li>
                <li>
                    Yoo, A. B., Jette, M. A., & Grondona, M. (2003).
                    "SLURM: Simple Linux Utility for Resource Management."
                    <em>Job Scheduling Strategies for Parallel Processing</em>, Springer, pp. 44-60.
                </li>
                <li>
                    <strong>東京大学情報基盤センター</strong>. (2024).
                    "Wisteria/BDEC-01 スーパーコンピュータシステム 利用の手引き."
                    <a href="https://www.cc.u-tokyo.ac.jp/supercomputer/wisteria/" target="_blank">https://www.cc.u-tokyo.ac.jp/supercomputer/wisteria/</a>
                    (ジョブ投入方法, pp. 23-45)
                </li>
                <li>
                    Amdahl, G. M. (1967).
                    "Validity of the single processor approach to achieving large scale computing capabilities."
                    <em>Proceedings of the AFIPS Spring Joint Computer Conference</em>, pp. 483-485.
                </li>
                <li>
                    <strong>理化学研究所</strong>. (2024).
                    "スーパーコンピュータ「富岳」利用の手引き."
                    <a href="https://www.r-ccs.riken.jp/fugaku/" target="_blank">https://www.r-ccs.riken.jp/fugaku/</a>
                    (ジョブスクリプト作成, pp. 34-67)
                </li>
                <li>
                    Barrett, R. F., et al. (2016).
                    "Improving File System Performance of HPC Clusters."
                    <em>Proceedings of the International Conference on High Performance Computing</em>, pp. 112-127.
                </li>
                <li>
                    <strong>XSEDE User Portal</strong>. (2023).
                    "Best Practices for HPC Job Submission."
                    <a href="https://portal.xsede.org/knowledge-base" target="_blank">https://portal.xsede.org/knowledge-base</a>
                    (Resource Management, pp. 15-28)
                </li>
            </ol>
        </section>

        <section id="next-steps">
            <h2>次のステップ</h2>

            <p>
                本章では、HPCクラスタの基礎とジョブ管理を学びました。次章では、<strong>並列計算技術（MPI、OpenMP、ハイブリッド並列化）</strong>を学び、プログラムを実際に並列化する方法を習得します。
            </p>

            <div class="info-box">
                <strong>第2章の予習</strong>
                <p>次章では、MPIとOpenMPを使った並列プログラミングを実践します。以下を事前に準備しておくとスムーズです：</p>
                <ul>
                    <li>C/Pythonの基本的なプログラミングスキル</li>
                    <li>プロセスとスレッドの違いの理解</li>
                    <li>HPCクラスタ上でのMPI/OpenMP環境の確認（<code>module avail</code>）</li>
                </ul>
            </div>

            <div style="text-align: center; margin: 2rem 0;">
                <a href="index.html" style="margin-right: 1rem; padding: 0.75rem 1.5rem; background: #34495e; color: white; border-radius: 4px; text-decoration: none;">← シリーズトップ</a>
                <a href="chapter-2.html" style="padding: 0.75rem 1.5rem; background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%); color: white; border-radius: 4px; text-decoration: none;">第2章へ進む →</a>
            </div>
        </section>
    </main>

    <footer>
        <p><strong>CT Dojo - 計算工学道場</strong></p>
        <p>HPCクラスタ入門シリーズ 第1章</p>
        <p><a href="https://github.com/your-repo" target="_blank">GitHub</a> | <a href="mailto:contact@example.com">お問い合わせ</a></p>
        <p>&copy; 2025 CT Knowledge Hub. Licensed under CC BY 4.0.</p>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
</body>
</html>
