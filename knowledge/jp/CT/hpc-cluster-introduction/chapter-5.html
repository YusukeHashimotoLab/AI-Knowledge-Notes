<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬5ç« : Pythonå®Ÿè·µï¼šHPCãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è‡ªå‹•åŒ– - HPCã‚¯ãƒ©ã‚¹ã‚¿å…¥é–€ - CT Dojo</title>
    <meta name="description" content="Pythonã«ã‚ˆã‚‹HPCãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è‡ªå‹•åŒ–ã€ã‚¸ãƒ§ãƒ–ç®¡ç†ã€ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ã€ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨æœ€é©åŒ–ã‚’å­¦ã¶å®Ÿè·µã‚¬ã‚¤ãƒ‰ã€‚">

    <!-- Prism.js for code highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">

    <!-- MathJax for mathematical expressions -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>

    <style>
        :root {
            --accent-green: #11998e;
            --accent-lime: #38ef7d;
            --primary-dark: #2c3e50;
            --secondary-dark: #34495e;
            --text-dark: #2c3e50;
            --text-light: #7f8c8d;
            --bg-light: #ecf0f1;
            --white: #ffffff;
            --code-bg: #2d2d2d;
            --border-light: #bdc3c7;
            --success: #27ae60;
            --warning: #f39c12;
            --danger: #e74c3c;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.8;
            color: var(--text-dark);
            background: var(--bg-light);
        }

        header {
            background: linear-gradient(135deg, var(--accent-green) 0%, var(--accent-lime) 100%);
            color: white;
            padding: 3rem 1.5rem;
            text-align: center;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        header h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
        }

        header p {
            font-size: 1.1rem;
            opacity: 0.95;
        }

        nav {
            background: var(--white);
            padding: 1rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            position: sticky;
            top: 0;
            z-index: 100;
        }

        nav ul {
            list-style: none;
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 0.5rem;
        }

        nav a {
            text-decoration: none;
            color: var(--text-dark);
            padding: 0.5rem 1rem;
            border-radius: 4px;
            transition: all 0.3s;
            font-weight: 500;
        }

        nav a:hover {
            background: linear-gradient(135deg, var(--accent-green) 0%, var(--accent-lime) 100%);
            color: white;
        }

        main {
            max-width: 900px;
            margin: 2rem auto;
            padding: 0 1.5rem 3rem;
        }

        section {
            background: var(--white);
            margin-bottom: 2rem;
            padding: 2rem;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        h2 {
            color: var(--primary-dark);
            font-size: 1.8rem;
            margin-bottom: 1.5rem;
            padding-bottom: 0.75rem;
            border-bottom: 3px solid;
            border-image: linear-gradient(135deg, var(--accent-green) 0%, var(--accent-lime) 100%) 1;
        }

        h3 {
            color: var(--secondary-dark);
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 1rem;
        }

        h4 {
            color: var(--text-dark);
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.75rem;
        }

        p, li {
            margin-bottom: 0.75rem;
            line-height: 1.8;
        }

        ul, ol {
            margin-left: 1.5rem;
            margin-bottom: 1rem;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 6px;
            overflow-x: auto;
            margin: 1.5rem 0;
            border-left: 4px solid var(--accent-green);
        }

        code {
            font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
            font-size: 0.9rem;
        }

        .note {
            background: #e8f5e9;
            border-left: 4px solid var(--success);
            padding: 1rem 1.5rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }

        .warning {
            background: #fff3e0;
            border-left: 4px solid var(--warning);
            padding: 1rem 1.5rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }

        .danger {
            background: #ffebee;
            border-left: 4px solid var(--danger);
            padding: 1rem 1.5rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.95rem;
        }

        th, td {
            padding: 0.75rem;
            text-align: left;
            border-bottom: 1px solid var(--border-light);
        }

        th {
            background: linear-gradient(135deg, var(--accent-green) 0%, var(--accent-lime) 100%);
            color: white;
            font-weight: 600;
        }

        tr:hover {
            background: #f5f5f5;
        }

        .mermaid {
            background: white;
            padding: 1.5rem;
            border-radius: 6px;
            margin: 1.5rem 0;
            border: 1px solid var(--border-light);
        }

        footer {
            background: var(--primary-dark);
            color: white;
            text-align: center;
            padding: 2rem 1.5rem;
            margin-top: 3rem;
        }

        footer a {
            color: var(--accent-lime);
            text-decoration: none;
        }

        footer a:hover {
            text-decoration: underline;
        }

        .learning-objectives {
            background: linear-gradient(135deg, rgba(17, 153, 142, 0.05) 0%, rgba(56, 239, 125, 0.05) 100%);
            border: 2px solid var(--accent-green);
            border-radius: 8px;
            padding: 1.5rem;
            margin: 2rem 0;
        }

        .learning-objectives h3 {
            color: var(--accent-green);
            margin-top: 0;
        }

        .learning-level {
            margin: 1rem 0;
            padding-left: 1rem;
        }

        .learning-level strong {
            color: var(--accent-green);
        }

        @media (max-width: 768px) {
            header h1 {
                font-size: 1.5rem;
            }

            nav ul {
                flex-direction: column;
            }

            main {
                padding: 0 1rem 2rem;
            }

            section {
                padding: 1.5rem;
            }

            h2 {
                font-size: 1.5rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: 0.5rem;
            }
        }
    </style>
</head>
<body>
    <header>
        <h1>ç¬¬5ç« : Pythonå®Ÿè·µï¼šHPCãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è‡ªå‹•åŒ–</h1>
        <p>Python Practice: HPC Workflow Automation</p>
    </header>

    <nav>
        <ul>
            <li><a href="index.html">ãƒ›ãƒ¼ãƒ </a></li>
            <li><a href="chapter-1.html">ç¬¬1ç« : HPCåŸºç¤</a></li>
            <li><a href="chapter-2.html">ç¬¬2ç« : ã‚¸ãƒ§ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©</a></li>
            <li><a href="chapter-3.html">ç¬¬3ç« : ä¸¦åˆ—è¨ˆç®—ã¨MPI</a></li>
            <li><a href="chapter-4.html">ç¬¬4ç« : ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç®¡ç†</a></li>
            <li><a href="chapter-5.html">ç¬¬5ç« : Pythonè‡ªå‹•åŒ–</a></li>
        </ul>
    </nav>

    <main>
        <section id="intro">
            <h2>5.1 æœ¬ç« ã®æ¦‚è¦</h2>
            <p>æœ¬ç« ã§ã¯ã€Pythonã«ã‚ˆã‚‹HPCãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®è‡ªå‹•åŒ–æ‰‹æ³•ã‚’å®Ÿè·µçš„ã«å­¦ã³ã¾ã™ã€‚ã‚¸ãƒ§ãƒ–ç®¡ç†ã€ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ã€ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã€æœ€é©åŒ–ã¾ã§ã‚’å«ã‚€ç·åˆçš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ§‹ç¯‰ã‚¹ã‚­ãƒ«ã‚’èº«ã«ã¤ã‘ã¾ã™ã€‚</p>

            <div class="learning-objectives">
                <h3>å­¦ç¿’ç›®æ¨™</h3>
                <div class="learning-level">
                    <strong>ãƒ¬ãƒ™ãƒ«1: åŸºæœ¬ç†è§£</strong>
                    <ul>
                        <li>HPCãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ç†è§£ï¼ˆmap-reduceã€parameter sweepã€DAGï¼‰</li>
                        <li>Pythonã‚¸ãƒ§ãƒ–ç®¡ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ç‰¹å¾´æŠŠæ¡ï¼ˆDaskã€Rayã€Parslï¼‰</li>
                        <li>ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åŸºæœ¬æ§‹é€ ç†è§£ï¼ˆETLã€checkpointingï¼‰</li>
                    </ul>
                </div>
                <div class="learning-level">
                    <strong>ãƒ¬ãƒ™ãƒ«2: å®Ÿè·µã‚¹ã‚­ãƒ«</strong>
                    <ul>
                        <li>Slurmã‚¸ãƒ§ãƒ–è‡ªå‹•æŠ•å…¥ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…</li>
                        <li>Dask/Parslã«ã‚ˆã‚‹ä¸¦åˆ—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ§‹ç¯‰</li>
                        <li>ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®checkpointingæ©Ÿèƒ½å®Ÿè£…</li>
                        <li>ã‚¸ãƒ§ãƒ–ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ä½œæˆ</li>
                    </ul>
                </div>
                <div class="learning-level">
                    <strong>ãƒ¬ãƒ™ãƒ«3: å¿œç”¨åŠ›</strong>
                    <ul>
                        <li>è¤‡é›‘ãªå¤šæ®µéšãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®è¨­è¨ˆã¨å®Ÿè£…</li>
                        <li>é©å¿œçš„ãƒªã‚½ãƒ¼ã‚¹å‰²ã‚Šå½“ã¦ã®å®Ÿè£…</li>
                        <li>ãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ãƒˆãªåˆ†æ•£è¨ˆç®—ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®æ§‹ç¯‰</li>
                        <li>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®è‡ªå‹•åŒ–</li>
                    </ul>
                </div>
            </div>
        </section>

        <section id="workflow-design">
            <h2>5.2 HPCãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è¨­è¨ˆ</h2>
            <h3>5.2.1 ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³</h3>
            <p>HPCãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ä»£è¡¨çš„ãªè¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š</p>

            <table>
                <thead>
                    <tr>
                        <th>ãƒ‘ã‚¿ãƒ¼ãƒ³</th>
                        <th>ç‰¹å¾´</th>
                        <th>é©ç”¨ä¾‹</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Map-Reduce</td>
                        <td>ä¸¦åˆ—å‡¦ç†â†’çµæœé›†ç´„</td>
                        <td>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿è§£æã€çµ±è¨ˆå‡¦ç†</td>
                    </tr>
                    <tr>
                        <td>Parameter Sweep</td>
                        <td>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“æ¢ç´¢</td>
                        <td>æœ€é©åŒ–ã€æ„Ÿåº¦è§£æ</td>
                    </tr>
                    <tr>
                        <td>DAG (Directed Acyclic Graph)</td>
                        <td>ä¾å­˜é–¢ä¿‚ã‚’æŒã¤ã‚¿ã‚¹ã‚¯é€£é–</td>
                        <td>å¤šæ®µéšã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€ML pipeline</td>
                    </tr>
                    <tr>
                        <td>Master-Worker</td>
                        <td>å‹•çš„ã‚¿ã‚¹ã‚¯å‰²ã‚Šå½“ã¦</td>
                        <td>ä¸å‡è¡¡è² è·ã€å‹•çš„ç”Ÿæˆã‚¿ã‚¹ã‚¯</td>
                    </tr>
                </tbody>
            </table>

            <h3>5.2.2 ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ã‚¹</h3>
            <p>HPCç’°å¢ƒã§ã¯é•·æ™‚é–“å®Ÿè¡Œã‚¸ãƒ§ãƒ–ãŒä¸€èˆ¬çš„ãªãŸã‚ã€robust ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æˆ¦ç•¥ãŒä¸å¯æ¬ ã§ã™ï¼š</p>

            <ul>
                <li><strong>Checkpointing</strong>: å®šæœŸçš„ãªä¸­é–“çŠ¶æ…‹ä¿å­˜ã«ã‚ˆã‚Šã€éšœå®³æ™‚ã«æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹</li>
                <li><strong>Retry Logic</strong>: ä¸€æ™‚çš„éšœå®³ã«å¯¾ã™ã‚‹è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ï¼ˆexponential backoffæ¨å¥¨ï¼‰</li>
                <li><strong>Graceful Degradation</strong>: ä¸€éƒ¨ã‚¿ã‚¹ã‚¯å¤±æ•—æ™‚ã‚‚å…¨ä½“ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¯ç¶™ç¶š</li>
                <li><strong>Dead Letter Queue</strong>: ç¹°ã‚Šè¿”ã—å¤±æ•—ã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚’éš”é›¢ã—ã¦å¾Œå‡¦ç†</li>
            </ul>

            <div class="mermaid">
                graph TB
                    A[ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–‹å§‹] --> B[ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ]
                    B --> C{æˆåŠŸ?}
                    C -->|Yes| D[æ¬¡ã‚¿ã‚¹ã‚¯]
                    C -->|No| E{ãƒªãƒˆãƒ©ã‚¤å›æ•°<br/>ä¸Šé™ä»¥å†…?}
                    E -->|Yes| F[Wait with<br/>Exponential Backoff]
                    F --> B
                    E -->|No| G[Dead Letter Queue]
                    G --> H[ç®¡ç†è€…é€šçŸ¥]
                    D --> I[çµæœä¿å­˜]
                    I --> J[ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Œäº†]
            </div>

            <h3>5.2.3 ãƒªã‚½ãƒ¼ã‚¹è¦‹ç©ã‚‚ã‚Šã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°</h3>
            <p>é©åˆ‡ãªãƒªã‚½ãƒ¼ã‚¹è¦‹ç©ã‚‚ã‚Šã¯ã€ã‚­ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚°æ™‚é–“çŸ­ç¸®ã¨ç„¡é§„ãªãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»å›é¿ã®ä¸¡ç«‹ã«é‡è¦ã§ã™ï¼š</p>

            <p><strong>ã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Šå…¬å¼ï¼š</strong></p>
            <p>\[
            \text{Total Cost} = N_{\text{nodes}} \times T_{\text{walltime}} \times C_{\text{cost\_per\_node\_hour}}
            \]</p>

            <p><strong>ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæœ€é©åŒ–ï¼š</strong></p>
            <p>\[
            \text{Throughput} = \frac{N_{\text{tasks}}}{\max(T_{\text{queue}}, T_{\text{exec}}/N_{\text{parallel}})}
            \]</p>

            <div class="note">
                <strong>ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ï¼š</strong> ã¾ãšå°è¦æ¨¡ãƒ†ã‚¹ãƒˆãƒ©ãƒ³ï¼ˆ1-2ãƒãƒ¼ãƒ‰ï¼‰ã§å®Ÿéš›ã®å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆæ¸¬ã—ã€ãã®çµæœã‚’åŸºã«ãƒªã‚½ãƒ¼ã‚¹è¦æ±‚ã‚’æ±ºå®šã—ã¾ã™ã€‚éå‰°ãªãƒªã‚½ãƒ¼ã‚¹è¦æ±‚ã¯ã‚­ãƒ¥ãƒ¼å¾…ã¡æ™‚é–“ã‚’å¢—åŠ ã•ã›ã¾ã™ã€‚
            </div>
        </section>

        <section id="job-management">
            <h2>5.3 Pythonã‚¸ãƒ§ãƒ–ç®¡ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒª</h2>

            <h3>5.3.1 ä¸»è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæ¯”è¼ƒ</h3>
            <table>
                <thead>
                    <tr>
                        <th>ãƒ©ã‚¤ãƒ–ãƒ©ãƒª</th>
                        <th>ç‰¹å¾´</th>
                        <th>é©ç”¨ã‚·ãƒ¼ãƒ³</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Dask Distributed</td>
                        <td>NumPy/Pandasäº’æ›ã®ä¸¦åˆ—å‡¦ç†</td>
                        <td>ãƒ‡ãƒ¼ã‚¿è§£æã€é…åˆ—æ¼”ç®—</td>
                    </tr>
                    <tr>
                        <td>Ray</td>
                        <td>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«MLã€ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·</td>
                        <td>å¼·åŒ–å­¦ç¿’ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–</td>
                    </tr>
                    <tr>
                        <td>Parsl</td>
                        <td>HPCç‰¹åŒ–ã€è¤‡é›‘DAGã‚µãƒãƒ¼ãƒˆ</td>
                        <td>ç§‘å­¦è¨ˆç®—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã€parameter sweep</td>
                    </tr>
                    <tr>
                        <td>subprocess/fabric</td>
                        <td>ã‚·ãƒ³ãƒ—ãƒ«ãªãƒªãƒ¢ãƒ¼ãƒˆå®Ÿè¡Œ</td>
                        <td>å°è¦æ¨¡è‡ªå‹•åŒ–ã€æ—¢å­˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆçµ±åˆ</td>
                    </tr>
                </tbody>
            </table>

            <h3>5.3.2 Slurm Job Managerå®Ÿè£…</h3>
            <p>åŒ…æ‹¬çš„ãªSlurmã‚¸ãƒ§ãƒ–ç®¡ç†ã‚¯ãƒ©ã‚¹ã®å®Ÿè£…ä¾‹ã§ã™ï¼š</p>

            <pre><code class="language-python">#!/usr/bin/env python3
"""
Slurmã‚¸ãƒ§ãƒ–ç®¡ç†ã‚¯ãƒ©ã‚¹
æ©Ÿèƒ½: ã‚¸ãƒ§ãƒ–æŠ•å…¥ã€ç›£è¦–ã€ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã€ãƒªã‚½ãƒ¼ã‚¹æ¨å®š
"""

import subprocess
import re
import time
import json
from pathlib import Path
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict

@dataclass
class JobConfig:
    """ã‚¸ãƒ§ãƒ–è¨­å®šãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    job_name: str
    nodes: int
    ntasks_per_node: int
    cpus_per_task: int
    time: str  # HH:MM:SSå½¢å¼
    partition: str
    mem_per_cpu: str  # e.g., "4G"
    output: str
    error: str
    executable: str
    args: List[str]
    modules: List[str] = None
    env_vars: Dict[str, str] = None

class SlurmJobManager:
    """Slurmã‚¸ãƒ§ãƒ–ç®¡ç†ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""

    def __init__(self, workspace: Path):
        """
        Parameters
        ----------
        workspace : Path
            ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ­ã‚°ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¿å­˜å…ˆï¼‰
        """
        self.workspace = Path(workspace)
        self.workspace.mkdir(parents=True, exist_ok=True)
        self.scripts_dir = self.workspace / "scripts"
        self.logs_dir = self.workspace / "logs"
        self.scripts_dir.mkdir(exist_ok=True)
        self.logs_dir.mkdir(exist_ok=True)

    def generate_script(self, config: JobConfig) -> Path:
        """
        Slurmã‚¸ãƒ§ãƒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ

        Parameters
        ----------
        config : JobConfig
            ã‚¸ãƒ§ãƒ–è¨­å®š

        Returns
        -------
        Path
            ç”Ÿæˆã•ã‚ŒãŸã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        script_path = self.scripts_dir / f"{config.job_name}.sh"

        # SBATCHãƒ˜ãƒƒãƒ€ãƒ¼ç”Ÿæˆ
        lines = [
            "#!/bin/bash",
            f"#SBATCH --job-name={config.job_name}",
            f"#SBATCH --nodes={config.nodes}",
            f"#SBATCH --ntasks-per-node={config.ntasks_per_node}",
            f"#SBATCH --cpus-per-task={config.cpus_per_task}",
            f"#SBATCH --time={config.time}",
            f"#SBATCH --partition={config.partition}",
            f"#SBATCH --mem-per-cpu={config.mem_per_cpu}",
            f"#SBATCH --output={config.output}",
            f"#SBATCH --error={config.error}",
            "",
            "# ã‚¨ãƒ©ãƒ¼æ™‚å³åº§çµ‚äº†",
            "set -e",
            "",
        ]

        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ­ãƒ¼ãƒ‰
        if config.modules:
            lines.append("# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ­ãƒ¼ãƒ‰")
            for module in config.modules:
                lines.append(f"module load {module}")
            lines.append("")

        # ç’°å¢ƒå¤‰æ•°è¨­å®š
        if config.env_vars:
            lines.append("# ç’°å¢ƒå¤‰æ•°è¨­å®š")
            for key, value in config.env_vars.items():
                lines.append(f"export {key}={value}")
            lines.append("")

        # å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰
        args_str = " ".join(config.args)
        lines.extend([
            "# ã‚¸ãƒ§ãƒ–å®Ÿè¡Œ",
            f"{config.executable} {args_str}",
            "",
            "# çµ‚äº†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¨˜éŒ²",
            "echo \"Job completed with status: $?\"",
        ])

        with open(script_path, "w") as f:
            f.write("\n".join(lines))

        script_path.chmod(0o755)  # å®Ÿè¡Œæ¨©é™ä»˜ä¸
        return script_path

    def submit(self, config: JobConfig) -> str:
        """
        ã‚¸ãƒ§ãƒ–æŠ•å…¥

        Parameters
        ----------
        config : JobConfig
            ã‚¸ãƒ§ãƒ–è¨­å®š

        Returns
        -------
        str
            ã‚¸ãƒ§ãƒ–IDï¼ˆä¾‹: "12345"ï¼‰
        """
        script_path = self.generate_script(config)

        # sbatchå®Ÿè¡Œ
        result = subprocess.run(
            ["sbatch", str(script_path)],
            capture_output=True,
            text=True,
            check=True
        )

        # ã‚¸ãƒ§ãƒ–IDæŠ½å‡ºï¼ˆ"Submitted batch job 12345"å½¢å¼ï¼‰
        match = re.search(r"Submitted batch job (\d+)", result.stdout)
        if not match:
            raise RuntimeError(f"Failed to parse job ID from: {result.stdout}")

        job_id = match.group(1)

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        metadata = {
            "job_id": job_id,
            "config": asdict(config),
            "script_path": str(script_path),
            "submit_time": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        metadata_path = self.logs_dir / f"{job_id}_metadata.json"
        with open(metadata_path, "w") as f:
            json.dump(metadata, f, indent=2)

        return job_id

    def get_status(self, job_id: str) -> Dict:
        """
        ã‚¸ãƒ§ãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å–å¾—

        Parameters
        ----------
        job_id : str
            ã‚¸ãƒ§ãƒ–ID

        Returns
        -------
        Dict
            ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æƒ…å ±ï¼ˆstate, reason, elapsed_timeç­‰ï¼‰
        """
        result = subprocess.run(
            ["scontrol", "show", "job", job_id],
            capture_output=True,
            text=True
        )

        if result.returncode != 0:
            return {"state": "NOT_FOUND"}

        # scontrolå‡ºåŠ›ã‚’ãƒ‘ãƒ¼ã‚¹
        status = {}
        for line in result.stdout.split("\n"):
            if "JobState=" in line:
                match = re.search(r"JobState=(\S+)", line)
                status["state"] = match.group(1) if match else "UNKNOWN"
            if "Reason=" in line:
                match = re.search(r"Reason=(\S+)", line)
                status["reason"] = match.group(1) if match else "None"
            if "RunTime=" in line:
                match = re.search(r"RunTime=(\S+)", line)
                status["elapsed_time"] = match.group(1) if match else "00:00:00"

        return status

    def cancel(self, job_id: str) -> bool:
        """
        ã‚¸ãƒ§ãƒ–ã‚­ãƒ£ãƒ³ã‚»ãƒ«

        Parameters
        ----------
        job_id : str
            ã‚¸ãƒ§ãƒ–ID

        Returns
        -------
        bool
            æˆåŠŸæ™‚True
        """
        result = subprocess.run(
            ["scancel", job_id],
            capture_output=True,
            text=True
        )
        return result.returncode == 0

    def wait_for_completion(self, job_id: str, check_interval: int = 30) -> str:
        """
        ã‚¸ãƒ§ãƒ–å®Œäº†ã¾ã§å¾…æ©Ÿ

        Parameters
        ----------
        job_id : str
            ã‚¸ãƒ§ãƒ–ID
        check_interval : int
            ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèªé–“éš”ï¼ˆç§’ï¼‰

        Returns
        -------
        str
            æœ€çµ‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼ˆCOMPLETEDã€FAILEDç­‰ï¼‰
        """
        while True:
            status = self.get_status(job_id)
            state = status.get("state", "UNKNOWN")

            # çµ‚äº†çŠ¶æ…‹ã®åˆ¤å®š
            if state in ["COMPLETED", "FAILED", "CANCELLED", "TIMEOUT", "NOT_FOUND"]:
                return state

            # å®Ÿè¡Œä¸­ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
            print(f"Job {job_id}: {state} (elapsed: {status.get('elapsed_time', 'N/A')})")
            time.sleep(check_interval)

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    manager = SlurmJobManager(workspace=Path("./workspace"))

    # ã‚¸ãƒ§ãƒ–è¨­å®š
    config = JobConfig(
        job_name="my_simulation",
        nodes=2,
        ntasks_per_node=8,
        cpus_per_task=4,
        time="01:00:00",
        partition="standard",
        mem_per_cpu="4G",
        output="logs/sim_%j.out",
        error="logs/sim_%j.err",
        executable="mpirun",
        args=["-np", "16", "./my_mpi_app"],
        modules=["openmpi/4.1.1", "python/3.9"],
        env_vars={"OMP_NUM_THREADS": "4"}
    )

    # ã‚¸ãƒ§ãƒ–æŠ•å…¥
    job_id = manager.submit(config)
    print(f"Submitted job: {job_id}")

    # å®Œäº†å¾…æ©Ÿ
    final_status = manager.wait_for_completion(job_id)
    print(f"Job {job_id} finished with status: {final_status}")
</code></pre>

            <h3>5.3.3 Dask Distributedã«ã‚ˆã‚‹ä¸¦åˆ—è¨ˆç®—</h3>
            <p>Daskã¯Pandasã‚„NumPyã¨äº’æ›æ€§ã®é«˜ã„APIã‚’æä¾›ã—ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿è§£æã‚’ä¸¦åˆ—åŒ–ã§ãã¾ã™ï¼š</p>

            <pre><code class="language-python">#!/usr/bin/env python3
"""
Dask Distributed ã«ã‚ˆã‚‹HPCã‚¯ãƒ©ã‚¹ã‚¿ä¸¦åˆ—è¨ˆç®—
"""

from dask.distributed import Client, LocalCluster
import dask.array as da
import numpy as np

# HPCã‚¯ãƒ©ã‚¹ã‚¿ä¸Šã§Daskã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’èµ·å‹•ã™ã‚‹å ´åˆ
# $ dask-scheduler --port 8786
# $ dask-worker scheduler-node:8786 --nprocs 4 --nthreads 2

def setup_dask_client(scheduler_address: str = None):
    """
    Daskã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®š

    Parameters
    ----------
    scheduler_address : str, optional
        ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆä¾‹: "scheduler-node:8786"ï¼‰
        Noneã®å ´åˆã¯ãƒ­ãƒ¼ã‚«ãƒ«ã‚¯ãƒ©ã‚¹ã‚¿èµ·å‹•
    """
    if scheduler_address:
        # æ—¢å­˜ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«æ¥ç¶š
        client = Client(scheduler_address)
    else:
        # ãƒ­ãƒ¼ã‚«ãƒ«ã‚¯ãƒ©ã‚¹ã‚¿èµ·å‹•ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        cluster = LocalCluster(n_workers=4, threads_per_worker=2)
        client = Client(cluster)

    print(f"Dashboard: {client.dashboard_link}")
    return client

def parallel_array_computation():
    """å¤§è¦æ¨¡é…åˆ—æ¼”ç®—ã®ä¸¦åˆ—åŒ–ä¾‹"""
    client = setup_dask_client()

    # 100GB ã®å¤§è¦æ¨¡é…åˆ—ç”Ÿæˆï¼ˆå®Ÿéš›ã«ã¯ãƒ¡ãƒ¢ãƒªã«ä¹—ã‚‰ãªã„ã‚µã‚¤ã‚ºï¼‰
    # Daskã¯é…å»¶è©•ä¾¡ã§å®Ÿéš›ã«ã¯è¨ˆç®—ã—ãªã„
    x = da.random.random((100000, 100000), chunks=(10000, 10000))
    y = da.random.random((100000, 100000), chunks=(10000, 10000))

    # è¡Œåˆ—æ¼”ç®—ï¼ˆå®Ÿéš›ã«ã¯ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•æ§‹ç¯‰ã®ã¿ï¼‰
    z = (x + y).mean(axis=0)

    # compute()ã§å®Ÿéš›ã®ä¸¦åˆ—è¨ˆç®—å®Ÿè¡Œ
    result = z.compute()

    print(f"Result shape: {result.shape}")
    print(f"Mean: {result.mean():.6f}")

    client.close()
    return result

def parallel_monte_carlo_pi(n_samples: int = 1_000_000_000):
    """
    Monte Carloã«ã‚ˆã‚‹PIæ¨å®šï¼ˆä¸¦åˆ—ç‰ˆï¼‰

    Parameters
    ----------
    n_samples : int
        ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆ10å„„ç‚¹ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    """
    client = setup_dask_client()

    # ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆï¼ˆãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã§ä¸¦åˆ—åŒ–ï¼‰
    x = da.random.uniform(-1, 1, size=n_samples, chunks=10_000_000)
    y = da.random.uniform(-1, 1, size=n_samples, chunks=10_000_000)

    # å††å†…åˆ¤å®š
    inside = (x**2 + y**2) <= 1

    # PIæ¨å®šï¼ˆä¸¦åˆ—é›†ç´„ï¼‰
    pi_estimate = 4 * inside.sum() / n_samples
    pi_value = pi_estimate.compute()

    print(f"PI estimate ({n_samples:,} samples): {pi_value:.10f}")
    print(f"Error: {abs(pi_value - np.pi):.10f}")

    client.close()
    return pi_value

if __name__ == "__main__":
    # ä¸¦åˆ—é…åˆ—æ¼”ç®—
    parallel_array_computation()

    # ä¸¦åˆ—Monte Carlo
    parallel_monte_carlo_pi(n_samples=1_000_000_000)
</code></pre>
        </section>

        <section id="automated-submission">
            <h2>5.4 ã‚¸ãƒ§ãƒ–è‡ªå‹•æŠ•å…¥ã‚·ã‚¹ãƒ†ãƒ </h2>

            <h3>5.4.1 Parameter Sweep Framework</h3>
            <p>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“æ¢ç´¢ã‚’è‡ªå‹•åŒ–ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®å®Ÿè£…ä¾‹ã§ã™ï¼š</p>

            <pre><code class="language-python">#!/usr/bin/env python3
"""
Parameter Sweepè‡ªå‹•åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
å¤šæ¬¡å…ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“æ¢ç´¢ã¨ã‚¸ãƒ§ãƒ–ä¾å­˜é–¢ä¿‚ç®¡ç†
"""

import itertools
import json
from pathlib import Path
from typing import List, Dict, Any
from dataclasses import dataclass
import subprocess

@dataclass
class Parameter:
    """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å®šç¾©"""
    name: str
    values: List[Any]

class ParameterSweepManager:
    """Parameter Sweepãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""

    def __init__(self, workspace: Path, job_template: str):
        """
        Parameters
        ----------
        workspace : Path
            ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        job_template : str
            Slurmã‚¸ãƒ§ãƒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ‘ã‚¹
        """
        self.workspace = Path(workspace)
        self.workspace.mkdir(parents=True, exist_ok=True)
        self.job_template = Path(job_template).read_text()
        self.job_ids = []

    def generate_parameter_combinations(self, parameters: List[Parameter]) -> List[Dict]:
        """
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›ç”Ÿæˆ

        Parameters
        ----------
        parameters : List[Parameter]
            ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆ

        Returns
        -------
        List[Dict]
            å…¨çµ„ã¿åˆã‚ã›è¾æ›¸ã®ãƒªã‚¹ãƒˆ
        """
        # å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å€¤ãƒªã‚¹ãƒˆã‚’å–å¾—
        names = [p.name for p in parameters]
        value_lists = [p.values for p in parameters]

        # ç›´ç©ï¼ˆCartesian productï¼‰ã§å…¨çµ„ã¿åˆã‚ã›ç”Ÿæˆ
        combinations = []
        for values in itertools.product(*value_lists):
            combo = dict(zip(names, values))
            combinations.append(combo)

        return combinations

    def create_job_script(self, params: Dict, job_id: int) -> Path:
        """
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åŸ‹ã‚è¾¼ã‚“ã ã‚¸ãƒ§ãƒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ

        Parameters
        ----------
        params : Dict
            ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¾æ›¸
        job_id : int
            ã‚¸ãƒ§ãƒ–è­˜åˆ¥ç•ªå·

        Returns
        -------
        Path
            ç”Ÿæˆã•ã‚ŒãŸã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‘ã‚¹
        """
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ•°ç½®æ›
        script_content = self.job_template
        script_content = script_content.replace("{JOB_ID}", str(job_id))

        for key, value in params.items():
            script_content = script_content.replace(f"{{{key}}}", str(value))

        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¿å­˜
        script_path = self.workspace / f"job_{job_id:04d}.sh"
        script_path.write_text(script_content)
        script_path.chmod(0o755)

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        metadata = {
            "job_id": job_id,
            "parameters": params,
            "script_path": str(script_path)
        }
        metadata_path = self.workspace / f"job_{job_id:04d}_params.json"
        with open(metadata_path, "w") as f:
            json.dump(metadata, f, indent=2)

        return script_path

    def submit_sweep(self, parameters: List[Parameter],
                     max_concurrent: int = None) -> List[str]:
        """
        Parameter Sweepã‚¸ãƒ§ãƒ–ä¸€æ‹¬æŠ•å…¥

        Parameters
        ----------
        parameters : List[Parameter]
            ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆ
        max_concurrent : int, optional
            åŒæ™‚å®Ÿè¡Œæ•°ä¸Šé™ï¼ˆSlurmä¾å­˜é–¢ä¿‚ã§åˆ¶å¾¡ï¼‰

        Returns
        -------
        List[str]
            æŠ•å…¥ã•ã‚ŒãŸSlurm Job IDãƒªã‚¹ãƒˆ
        """
        combinations = self.generate_parameter_combinations(parameters)
        print(f"Total combinations: {len(combinations)}")

        slurm_job_ids = []
        for i, params in enumerate(combinations):
            script_path = self.create_job_script(params, i)

            # sbatchå®Ÿè¡Œ
            cmd = ["sbatch"]

            # åŒæ™‚å®Ÿè¡Œæ•°åˆ¶é™ï¼ˆä¾å­˜é–¢ä¿‚ã§å®Ÿç¾ï¼‰
            if max_concurrent and i >= max_concurrent:
                # å‰ã®ã‚¸ãƒ§ãƒ–ãŒå®Œäº†ã—ã¦ã‹ã‚‰èµ·å‹•
                dependency_idx = i - max_concurrent
                dependency_job_id = slurm_job_ids[dependency_idx]
                cmd.extend(["--dependency", f"afterok:{dependency_job_id}"])

            cmd.append(str(script_path))

            result = subprocess.run(cmd, capture_output=True, text=True, check=True)

            # Job IDæŠ½å‡º
            import re
            match = re.search(r"Submitted batch job (\d+)", result.stdout)
            if match:
                job_id = match.group(1)
                slurm_job_ids.append(job_id)
                print(f"Submitted job {i:04d} (Slurm ID: {job_id}) with params: {params}")

        self.job_ids = slurm_job_ids
        return slurm_job_ids

    def collect_results(self, output_pattern: str) -> List[Dict]:
        """
        å…¨ã‚¸ãƒ§ãƒ–çµæœåé›†

        Parameters
        ----------
        output_pattern : str
            å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¾‹: "result_{JOB_ID:04d}.json"ï¼‰

        Returns
        -------
        List[Dict]
            çµæœè¾æ›¸ã®ãƒªã‚¹ãƒˆ
        """
        results = []
        for i in range(len(self.job_ids)):
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            metadata_path = self.workspace / f"job_{i:04d}_params.json"
            with open(metadata_path) as f:
                metadata = json.load(f)

            # çµæœãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            output_file = self.workspace / output_pattern.replace("{JOB_ID:04d}", f"{i:04d}")
            if output_file.exists():
                with open(output_file) as f:
                    result_data = json.load(f)

                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨çµæœã‚’çµ±åˆ
                results.append({
                    "parameters": metadata["parameters"],
                    "result": result_data
                })

        return results

# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä¾‹ï¼ˆtemplate.shï¼‰
# #!/bin/bash
# #SBATCH --job-name=sweep_{JOB_ID}
# #SBATCH --output=logs/job_{JOB_ID}.out
# #SBATCH --time=00:30:00
#
# python simulation.py --alpha {alpha} --beta {beta} --gamma {gamma}

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    manager = ParameterSweepManager(
        workspace=Path("./sweep_workspace"),
        job_template="template.sh"
    )

    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å®šç¾©ï¼ˆ3æ¬¡å…ƒç©ºé–“: 2Ã—3Ã—4 = 24 combinationsï¼‰
    parameters = [
        Parameter("alpha", [0.1, 0.5]),
        Parameter("beta", [1.0, 2.0, 3.0]),
        Parameter("gamma", [0.01, 0.05, 0.1, 0.5])
    ]

    # ä¸€æ‹¬æŠ•å…¥ï¼ˆæœ€å¤§5ã‚¸ãƒ§ãƒ–åŒæ™‚å®Ÿè¡Œï¼‰
    job_ids = manager.submit_sweep(parameters, max_concurrent=5)

    print(f"\nSubmitted {len(job_ids)} jobs")
    print("Waiting for completion...")

    # å…¨ã‚¸ãƒ§ãƒ–å®Œäº†å¾Œã«çµæœåé›†
    # results = manager.collect_results("result_{JOB_ID:04d}.json")
</code></pre>
        </section>

        <section id="data-pipeline">
            <h2>5.5 ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰</h2>

            <h3>5.5.1 Checkpointingæ©Ÿèƒ½ä»˜ãETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h3>
            <p>éšœå®³ã‹ã‚‰ã®è‡ªå‹•å›å¾©æ©Ÿèƒ½ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè£…ã§ã™ï¼š</p>

            <pre><code class="language-python">#!/usr/bin/env python3
"""
Checkpointingæ©Ÿèƒ½ä»˜ããƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
ETL (Extract, Transform, Load) + ãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ã‚¹
"""

import pickle
import hashlib
import json
from pathlib import Path
from typing import Any, Callable, List, Dict
from dataclasses import dataclass
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

@dataclass
class PipelineStage:
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¹ãƒ†ãƒ¼ã‚¸å®šç¾©"""
    name: str
    function: Callable
    dependencies: List[str] = None

class CheckpointedPipeline:
    """Checkpointingæ©Ÿèƒ½ä»˜ããƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(self, checkpoint_dir: Path):
        """
        Parameters
        ----------
        checkpoint_dir : Path
            ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.stages = []
        self.results = {}

    def add_stage(self, name: str, function: Callable, dependencies: List[str] = None):
        """
        ã‚¹ãƒ†ãƒ¼ã‚¸è¿½åŠ 

        Parameters
        ----------
        name : str
            ã‚¹ãƒ†ãƒ¼ã‚¸å
        function : Callable
            å®Ÿè¡Œé–¢æ•°ï¼ˆå¼•æ•°: ä¾å­˜ã‚¹ãƒ†ãƒ¼ã‚¸ã®çµæœè¾æ›¸ï¼‰
        dependencies : List[str], optional
            ä¾å­˜ã‚¹ãƒ†ãƒ¼ã‚¸åãƒªã‚¹ãƒˆ
        """
        stage = PipelineStage(name, function, dependencies or [])
        self.stages.append(stage)

    def _get_checkpoint_path(self, stage_name: str, input_hash: str) -> Path:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ç”Ÿæˆ"""
        return self.checkpoint_dir / f"{stage_name}_{input_hash}.pkl"

    def _compute_input_hash(self, dependencies: Dict[str, Any]) -> str:
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥è¨ˆç®—"""
        # ä¾å­˜ãƒ‡ãƒ¼ã‚¿ã‚’JSONåŒ–ã—ã¦ãƒãƒƒã‚·ãƒ¥åŒ–
        data_str = json.dumps(dependencies, sort_keys=True, default=str)
        return hashlib.md5(data_str.encode()).hexdigest()[:16]

    def _load_checkpoint(self, stage_name: str, input_hash: str) -> Any:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿"""
        checkpoint_path = self._get_checkpoint_path(stage_name, input_hash)
        if checkpoint_path.exists():
            with open(checkpoint_path, "rb") as f:
                return pickle.load(f)
        return None

    def _save_checkpoint(self, stage_name: str, input_hash: str, result: Any):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        checkpoint_path = self._get_checkpoint_path(stage_name, input_hash)
        with open(checkpoint_path, "wb") as f:
            pickle.dump(result, f)

    def execute(self, force_recompute: bool = False) -> Dict[str, Any]:
        """
        ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ

        Parameters
        ----------
        force_recompute : bool
            Trueã®å ´åˆã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç„¡è¦–ã—ã¦å†è¨ˆç®—

        Returns
        -------
        Dict[str, Any]
            å…¨ã‚¹ãƒ†ãƒ¼ã‚¸ã®çµæœè¾æ›¸
        """
        for stage in self.stages:
            # ä¾å­˜ã‚¹ãƒ†ãƒ¼ã‚¸ã®çµæœå–å¾—
            deps = {dep: self.results[dep] for dep in stage.dependencies}

            # å…¥åŠ›ãƒãƒƒã‚·ãƒ¥è¨ˆç®—
            input_hash = self._compute_input_hash(deps)

            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¢ºèª
            if not force_recompute:
                cached = self._load_checkpoint(stage.name, input_hash)
                if cached is not None:
                    logging.info(f"Stage '{stage.name}': Loaded from checkpoint")
                    self.results[stage.name] = cached
                    continue

            # ã‚¹ãƒ†ãƒ¼ã‚¸å®Ÿè¡Œ
            logging.info(f"Stage '{stage.name}': Executing...")
            try:
                result = stage.function(deps)
                self.results[stage.name] = result

                # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
                self._save_checkpoint(stage.name, input_hash, result)
                logging.info(f"Stage '{stage.name}': Completed and checkpointed")
            except Exception as e:
                logging.error(f"Stage '{stage.name}': Failed with error: {e}")
                raise

        return self.results

# ETLä¾‹ï¼šå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
def example_etl_pipeline():
    """ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè£…ä¾‹"""
    pipeline = CheckpointedPipeline(checkpoint_dir=Path("./checkpoints"))

    # Stage 1: Extract - ãƒ‡ãƒ¼ã‚¿åé›†
    def extract(deps):
        import numpy as np
        import time
        logging.info("Extracting data from source...")
        time.sleep(2)  # é‡ã„å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        data = np.random.randn(1000000, 10)  # 1Mè¡Œã®ãƒ‡ãƒ¼ã‚¿
        return {"raw_data": data}

    pipeline.add_stage("extract", extract)

    # Stage 2: Clean - ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    def clean(deps):
        import numpy as np
        import time
        logging.info("Cleaning data...")
        time.sleep(2)
        data = deps["extract"]["raw_data"]
        # æ¬ æå€¤é™¤å»ã€å¤–ã‚Œå€¤å‡¦ç†
        cleaned = data[~np.isnan(data).any(axis=1)]
        return {"cleaned_data": cleaned}

    pipeline.add_stage("clean", clean, dependencies=["extract"])

    # Stage 3: Transform - ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    def transform(deps):
        import numpy as np
        import time
        logging.info("Transforming features...")
        time.sleep(2)
        data = deps["clean"]["cleaned_data"]
        # æ¨™æº–åŒ–
        mean = data.mean(axis=0)
        std = data.std(axis=0)
        normalized = (data - mean) / std
        return {"features": normalized, "mean": mean, "std": std}

    pipeline.add_stage("transform", transform, dependencies=["clean"])

    # Stage 4: Load - çµæœä¿å­˜
    def load(deps):
        import numpy as np
        logging.info("Saving processed data...")
        features = deps["transform"]["features"]
        np.save("processed_features.npy", features)
        return {"output_path": "processed_features.npy"}

    pipeline.add_stage("load", load, dependencies=["transform"])

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    results = pipeline.execute()
    logging.info(f"Pipeline completed. Output: {results['load']}")

if __name__ == "__main__":
    example_etl_pipeline()
</code></pre>
        </section>

        <section id="monitoring">
            <h2>5.6 ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨æœ€é©åŒ–</h2>

            <h3>5.6.1 ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¸ãƒ§ãƒ–ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰</h3>
            <p>Streamlitã‚’ä½¿ç”¨ã—ãŸã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¸ãƒ§ãƒ–ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®å®Ÿè£…ä¾‹ã§ã™ï¼š</p>

            <pre><code class="language-python">#!/usr/bin/env python3
"""
Streamlitãƒ™ãƒ¼ã‚¹ã®ã‚¸ãƒ§ãƒ–ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¸ãƒ§ãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã€ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡è¡¨ç¤º

èµ·å‹•æ–¹æ³•:
$ streamlit run dashboard.py
"""

import streamlit as st
import subprocess
import re
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import time

def get_queue_status():
    """Slurmã‚­ãƒ¥ãƒ¼çŠ¶æ…‹å–å¾—"""
    result = subprocess.run(
        ["squeue", "-u", "$USER", "-o", "%i,%j,%t,%M,%D,%C"],
        capture_output=True,
        text=True
    )

    if result.returncode != 0:
        return pd.DataFrame()

    # CSVå½¢å¼ã§ãƒ‘ãƒ¼ã‚¹
    lines = result.stdout.strip().split("\n")
    if len(lines) < 2:
        return pd.DataFrame()

    data = []
    for line in lines[1:]:  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚¹ã‚­ãƒƒãƒ—
        parts = line.split(",")
        if len(parts) >= 6:
            data.append({
                "JobID": parts[0],
                "Name": parts[1],
                "State": parts[2],
                "Time": parts[3],
                "Nodes": int(parts[4]),
                "CPUs": int(parts[5])
            })

    return pd.DataFrame(data)

def get_cluster_utilization():
    """ã‚¯ãƒ©ã‚¹ã‚¿å…¨ä½“ã®ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡å–å¾—"""
    result = subprocess.run(
        ["sinfo", "-o", "%P,%a,%l,%D,%T,%C"],
        capture_output=True,
        text=True
    )

    if result.returncode != 0:
        return pd.DataFrame()

    lines = result.stdout.strip().split("\n")
    data = []
    for line in lines[1:]:
        parts = line.split(",")
        if len(parts) >= 6:
            # CPUs: allocated/idle/other/total
            cpu_info = parts[5]
            match = re.match(r"(\d+)/(\d+)/(\d+)/(\d+)", cpu_info)
            if match:
                alloc, idle, other, total = map(int, match.groups())
                data.append({
                    "Partition": parts[0],
                    "Availability": parts[1],
                    "Time Limit": parts[2],
                    "Nodes": int(parts[3]),
                    "State": parts[4],
                    "CPUs Allocated": alloc,
                    "CPUs Idle": idle,
                    "CPUs Total": total,
                    "Utilization": alloc / total * 100 if total > 0 else 0
                })

    return pd.DataFrame(data)

def main():
    st.set_page_config(page_title="HPC Job Monitor", layout="wide")

    st.title("ğŸ–¥ï¸ HPC Cluster Job Monitor")
    st.markdown("Real-time Slurm job status and cluster utilization dashboard")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    st.sidebar.header("Settings")
    refresh_interval = st.sidebar.slider("Refresh Interval (seconds)", 5, 60, 10)
    auto_refresh = st.sidebar.checkbox("Auto Refresh", value=True)

    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    col1, col2 = st.columns(2)

    with col1:
        st.header("Your Jobs")
        jobs_df = get_queue_status()

        if not jobs_df.empty:
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹é›†è¨ˆ
            status_counts = jobs_df["State"].value_counts()
            st.metric("Total Jobs", len(jobs_df))

            # çŠ¶æ…‹åˆ¥ã‚«ã‚¦ãƒ³ãƒˆè¡¨ç¤º
            st.dataframe(jobs_df, use_container_width=True)

            # çŠ¶æ…‹åˆ¥ãƒ‘ã‚¤ãƒãƒ£ãƒ¼ãƒˆ
            fig_pie = px.pie(
                values=status_counts.values,
                names=status_counts.index,
                title="Job States Distribution"
            )
            st.plotly_chart(fig_pie, use_container_width=True)
        else:
            st.info("No running jobs")

    with col2:
        st.header("Cluster Utilization")
        cluster_df = get_cluster_utilization()

        if not cluster_df.empty:
            # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ¥ä½¿ç”¨ç‡
            fig_bar = px.bar(
                cluster_df,
                x="Partition",
                y="Utilization",
                title="CPU Utilization by Partition",
                color="Utilization",
                color_continuous_scale="RdYlGn_r"
            )
            st.plotly_chart(fig_bar, use_container_width=True)

            # è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«
            st.dataframe(cluster_df, use_container_width=True)
        else:
            st.warning("No cluster information available")

    # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ï¼ˆå®Ÿé‹ç”¨ã§ã¯DBã«ä¿å­˜ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºï¼‰
    st.header("Historical Resource Usage")
    st.info("Feature under development - will show time-series CPU/memory usage")

    # è‡ªå‹•æ›´æ–°
    if auto_refresh:
        time.sleep(refresh_interval)
        st.rerun()

if __name__ == "__main__":
    main()
</code></pre>

            <h3>5.6.2 é©å¿œçš„ãƒªã‚½ãƒ¼ã‚¹å‰²ã‚Šå½“ã¦</h3>
            <p>ã‚¸ãƒ§ãƒ–ã®éå»å®Ÿç¸¾ã«åŸºã¥ã„ã¦æœ€é©ãªãƒªã‚½ãƒ¼ã‚¹ã‚’è‡ªå‹•æ¨å®šã™ã‚‹ä»•çµ„ã¿ã§ã™ï¼š</p>

            <pre><code class="language-python">#!/usr/bin/env python3
"""
é©å¿œçš„ãƒªã‚½ãƒ¼ã‚¹å‰²ã‚Šå½“ã¦ã‚·ã‚¹ãƒ†ãƒ 
éå»ã®ã‚¸ãƒ§ãƒ–å®Ÿç¸¾ã‹ã‚‰ãƒªã‚½ãƒ¼ã‚¹è¦æ±‚ã‚’æœ€é©åŒ–
"""

import json
import sqlite3
from pathlib import Path
from typing import Dict, Optional
from dataclasses import dataclass
import subprocess
import re

@dataclass
class ResourceRecommendation:
    """ãƒªã‚½ãƒ¼ã‚¹æ¨å¥¨å€¤"""
    nodes: int
    cpus_per_node: int
    memory_gb: int
    walltime_hours: float
    confidence: float  # 0-1ã®ä¿¡é ¼åº¦

class AdaptiveResourceAllocator:
    """é©å¿œçš„ãƒªã‚½ãƒ¼ã‚¹å‰²ã‚Šå½“ã¦"""

    def __init__(self, db_path: Path):
        """
        Parameters
        ----------
        db_path : Path
            ã‚¸ãƒ§ãƒ–å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        """
        self.db_path = db_path
        self._init_database()

    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS job_history (
                job_id TEXT PRIMARY KEY,
                job_name TEXT,
                nodes INTEGER,
                cpus_per_node INTEGER,
                memory_gb INTEGER,
                requested_time_hours REAL,
                actual_time_hours REAL,
                max_memory_gb REAL,
                exit_code INTEGER,
                timestamp TEXT
            )
        """)
        conn.commit()
        conn.close()

    def record_job(self, job_id: str):
        """
        å®Œäº†ã‚¸ãƒ§ãƒ–ã®å®Ÿç¸¾ã‚’è¨˜éŒ²

        Parameters
        ----------
        job_id : str
            Slurm Job ID
        """
        # sacctã§ã‚¸ãƒ§ãƒ–æƒ…å ±å–å¾—
        result = subprocess.run(
            ["sacct", "-j", job_id, "--format=JobName,NNodes,NCPUS,ReqMem,Timelimit,Elapsed,MaxRSS,ExitCode", "-P", "-n"],
            capture_output=True,
            text=True
        )

        if result.returncode != 0:
            return

        # ãƒ‘ãƒ¼ã‚¹
        line = result.stdout.strip().split("\n")[0]
        parts = line.split("|")

        if len(parts) < 8:
            return

        job_name = parts[0]
        nodes = int(parts[1]) if parts[1] else 1
        cpus = int(parts[2]) if parts[2] else 1

        # ãƒ¡ãƒ¢ãƒªï¼ˆGcã€Mcãªã©ã®å˜ä½ã‚’å‡¦ç†ï¼‰
        req_mem = parts[3]
        memory_gb = self._parse_memory(req_mem)

        # æ™‚é–“ï¼ˆHH:MM:SSå½¢å¼ã‚’hoursã«å¤‰æ›ï¼‰
        requested_time = self._parse_time(parts[4])
        actual_time = self._parse_time(parts[5])

        # æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        max_rss = self._parse_memory(parts[6])

        # çµ‚äº†ã‚³ãƒ¼ãƒ‰
        exit_code = int(parts[7].split(":")[0])

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("""
            INSERT OR REPLACE INTO job_history VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now'))
        """, (job_id, job_name, nodes, cpus, memory_gb, requested_time, actual_time, max_rss, exit_code))
        conn.commit()
        conn.close()

    def _parse_memory(self, mem_str: str) -> float:
        """ãƒ¡ãƒ¢ãƒªæ–‡å­—åˆ—ã‚’GBã«å¤‰æ›"""
        if not mem_str:
            return 0.0
        match = re.match(r"([\d.]+)([KMGT]?)", mem_str)
        if not match:
            return 0.0
        value, unit = match.groups()
        value = float(value)
        unit_map = {"K": 1e-6, "M": 1e-3, "G": 1.0, "T": 1e3, "": 1e-3}
        return value * unit_map.get(unit, 1.0)

    def _parse_time(self, time_str: str) -> float:
        """HH:MM:SSå½¢å¼ã‚’æ™‚é–“ã«å¤‰æ›"""
        if not time_str or time_str == "UNLIMITED":
            return 0.0
        parts = list(map(int, time_str.split(":")))
        if len(parts) == 3:
            return parts[0] + parts[1]/60 + parts[2]/3600
        return 0.0

    def recommend(self, job_name: str, safety_factor: float = 1.2) -> Optional[ResourceRecommendation]:
        """
        ãƒªã‚½ãƒ¼ã‚¹æ¨å¥¨å€¤è¨ˆç®—

        Parameters
        ----------
        job_name : str
            ã‚¸ãƒ§ãƒ–åï¼ˆéå»ã®åŒåã‚¸ãƒ§ãƒ–ã‹ã‚‰æ¨å®šï¼‰
        safety_factor : float
            å®‰å…¨ç‡ï¼ˆéå»å®Ÿç¸¾ã®ä½•å€ã‚’è¦æ±‚ã™ã‚‹ã‹ï¼‰

        Returns
        -------
        ResourceRecommendation or None
            æ¨å¥¨å€¤ï¼ˆå±¥æ­´ãŒãªã„å ´åˆã¯Noneï¼‰
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # åŒåã‚¸ãƒ§ãƒ–ã®æˆåŠŸå®Ÿç¸¾ã‚’å–å¾—
        cursor.execute("""
            SELECT AVG(nodes), AVG(cpus_per_node), AVG(max_memory_gb), AVG(actual_time_hours), COUNT(*)
            FROM job_history
            WHERE job_name = ? AND exit_code = 0
        """, (job_name,))

        result = cursor.fetchone()
        conn.close()

        if not result or result[4] == 0:  # å±¥æ­´ãªã—
            return None

        avg_nodes, avg_cpus, avg_mem, avg_time, count = result

        # å®‰å…¨ç‡é©ç”¨
        rec = ResourceRecommendation(
            nodes=int(avg_nodes * safety_factor),
            cpus_per_node=int(avg_cpus * safety_factor),
            memory_gb=int(avg_mem * safety_factor),
            walltime_hours=avg_time * safety_factor,
            confidence=min(count / 10.0, 1.0)  # 10å›ä»¥ä¸Šã§ä¿¡é ¼åº¦1.0
        )

        return rec

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    allocator = AdaptiveResourceAllocator(db_path=Path("job_history.db"))

    # ã‚¸ãƒ§ãƒ–å®Œäº†å¾Œã«å®Ÿç¸¾è¨˜éŒ²
    allocator.record_job("12345")

    # æ¬¡å›åŒç¨®ã‚¸ãƒ§ãƒ–ã®ãƒªã‚½ãƒ¼ã‚¹æ¨å¥¨å–å¾—
    rec = allocator.recommend("my_simulation")
    if rec:
        print(f"Recommended resources (confidence: {rec.confidence:.2f}):")
        print(f"  Nodes: {rec.nodes}")
        print(f"  CPUs/node: {rec.cpus_per_node}")
        print(f"  Memory: {rec.memory_gb} GB")
        print(f"  Walltime: {rec.walltime_hours:.2f} hours")
    else:
        print("No historical data available for this job")
</code></pre>
        </section>

        <section id="exercises">
            <h2>5.7 æ¼”ç¿’å•é¡Œ</h2>

            <h3>æ¼”ç¿’1: ç°¡æ˜“ã‚¸ãƒ§ãƒ–ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆå™¨ï¼ˆEasyï¼‰</h3>
            <p>ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‹ã‚‰Slurmã‚¸ãƒ§ãƒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹CLIãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚</p>
            <details>
                <summary>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</summary>
                <pre><code class="language-python">#!/usr/bin/env python3
"""
ç°¡æ˜“Slurmã‚¸ãƒ§ãƒ–ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆå™¨
"""

def generate_slurm_script():
    """å¯¾è©±å¼ã‚¸ãƒ§ãƒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ"""
    print("=== Slurm Job Script Generator ===")

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
    job_name = input("Job name: ")
    nodes = int(input("Number of nodes: "))
    ntasks = int(input("Tasks per node: "))
    time = input("Walltime (HH:MM:SS): ")
    partition = input("Partition: ")
    command = input("Command to execute: ")

    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ
    script = f"""#!/bin/bash
#SBATCH --job-name={job_name}
#SBATCH --nodes={nodes}
#SBATCH --ntasks-per-node={ntasks}
#SBATCH --time={time}
#SBATCH --partition={partition}
#SBATCH --output={job_name}_%j.out
#SBATCH --error={job_name}_%j.err

echo "Starting job on $(hostname) at $(date)"

{command}

echo "Job finished at $(date)"
"""

    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    filename = f"{job_name}.sh"
    with open(filename, "w") as f:
        f.write(script)

    print(f"\nScript saved to: {filename}")
    return filename

if __name__ == "__main__":
    generate_slurm_script()
</code></pre>
            </details>

            <h3>æ¼”ç¿’2: ã‚¸ãƒ§ãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒã‚§ãƒƒã‚«ãƒ¼ï¼ˆEasyï¼‰</h3>
            <p>è¤‡æ•°ã®ã‚¸ãƒ§ãƒ–IDã‚’å—ã‘å–ã‚Šã€å„ã‚¸ãƒ§ãƒ–ã®çŠ¶æ…‹ã‚’è¡¨å½¢å¼ã§è¡¨ç¤ºã™ã‚‹ãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚</p>
            <details>
                <summary>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</summary>
                <pre><code class="language-python">#!/usr/bin/env python3
"""
è¤‡æ•°ã‚¸ãƒ§ãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ä¸€æ‹¬ç¢ºèªãƒ„ãƒ¼ãƒ«
"""

import subprocess
import re
from typing import List, Dict

def get_job_status(job_id: str) -> Dict:
    """å˜ä¸€ã‚¸ãƒ§ãƒ–ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å–å¾—"""
    result = subprocess.run(
        ["scontrol", "show", "job", job_id],
        capture_output=True,
        text=True
    )

    if result.returncode != 0:
        return {"job_id": job_id, "state": "NOT_FOUND"}

    # ãƒ‘ãƒ¼ã‚¹
    state = re.search(r"JobState=(\S+)", result.stdout)
    reason = re.search(r"Reason=(\S+)", result.stdout)
    elapsed = re.search(r"RunTime=(\S+)", result.stdout)

    return {
        "job_id": job_id,
        "state": state.group(1) if state else "UNKNOWN",
        "reason": reason.group(1) if reason else "None",
        "elapsed": elapsed.group(1) if elapsed else "00:00:00"
    }

def check_multiple_jobs(job_ids: List[str]):
    """è¤‡æ•°ã‚¸ãƒ§ãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª"""
    print(f"{'Job ID':<12} {'State':<12} {'Elapsed':<12} {'Reason':<20}")
    print("=" * 60)

    for job_id in job_ids:
        status = get_job_status(job_id)
        print(f"{status['job_id']:<12} {status['state']:<12} {status['elapsed']:<12} {status['reason']:<20}")

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print("Usage: python check_jobs.py <job_id1> <job_id2> ...")
        sys.exit(1)

    job_ids = sys.argv[1:]
    check_multiple_jobs(job_ids)
</code></pre>
            </details>

            <h3>æ¼”ç¿’3: Parameter Sweepä¾å­˜é–¢ä¿‚ç®¡ç†ï¼ˆMediumï¼‰</h3>
            <p>å‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒæˆåŠŸã—ãŸå ´åˆã®ã¿æ¬¡ã‚’å®Ÿè¡Œã™ã‚‹Parameter Sweepã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>
            <details>
                <summary>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</summary>
                <pre><code class="language-python">#!/usr/bin/env python3
"""
ä¾å­˜é–¢ä¿‚ç®¡ç†ä»˜ãParameter Sweep
å‰ã‚¿ã‚¹ã‚¯æˆåŠŸæ™‚ã®ã¿æ¬¡ã‚’å®Ÿè¡Œ
"""

import subprocess
import re
from typing import List, Dict

def submit_with_dependency(script: str, dependency_job_id: str = None) -> str:
    """
    ä¾å­˜é–¢ä¿‚ä»˜ãã‚¸ãƒ§ãƒ–æŠ•å…¥

    Parameters
    ----------
    script : str
        ã‚¸ãƒ§ãƒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‘ã‚¹
    dependency_job_id : str, optional
        ä¾å­˜ã™ã‚‹ã‚¸ãƒ§ãƒ–IDï¼ˆã“ã®ã‚¸ãƒ§ãƒ–æˆåŠŸå¾Œã«å®Ÿè¡Œï¼‰

    Returns
    -------
    str
        æŠ•å…¥ã•ã‚ŒãŸã‚¸ãƒ§ãƒ–ID
    """
    cmd = ["sbatch"]

    if dependency_job_id:
        # afterok: ä¾å­˜ã‚¸ãƒ§ãƒ–ãŒæˆåŠŸï¼ˆexit code 0ï¼‰ã—ãŸå ´åˆã®ã¿å®Ÿè¡Œ
        cmd.extend(["--dependency", f"afterok:{dependency_job_id}"])

    cmd.append(script)

    result = subprocess.run(cmd, capture_output=True, text=True, check=True)
    match = re.search(r"Submitted batch job (\d+)", result.stdout)
    if not match:
        raise RuntimeError(f"Failed to parse job ID: {result.stdout}")

    return match.group(1)

def sequential_parameter_sweep(parameters: List[Dict], template: str):
    """
    é€æ¬¡çš„Parameter Sweepï¼ˆå‰ã‚¿ã‚¹ã‚¯æˆåŠŸãŒæ¬¡ã®å®Ÿè¡Œæ¡ä»¶ï¼‰

    Parameters
    ----------
    parameters : List[Dict]
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¾æ›¸ãƒªã‚¹ãƒˆ
    template : str
        ã‚¸ãƒ§ãƒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    """
    prev_job_id = None

    for i, params in enumerate(parameters):
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ•°ç½®æ›
        script_content = template
        for key, value in params.items():
            script_content = script_content.replace(f"{{{key}}}", str(value))

        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¿å­˜
        script_path = f"job_{i:04d}.sh"
        with open(script_path, "w") as f:
            f.write(script_content)

        # ä¾å­˜é–¢ä¿‚ä»˜ãã§æŠ•å…¥
        job_id = submit_with_dependency(script_path, prev_job_id)
        print(f"Submitted job {i:04d} (ID: {job_id}) with dependency: {prev_job_id or 'None'}")

        prev_job_id = job_id

# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä¾‹
template = """#!/bin/bash
#SBATCH --job-name=sweep_{idx}
#SBATCH --output=logs/job_{idx}.out
#SBATCH --time=00:10:00

python simulation.py --param1 {param1} --param2 {param2}

# çµ‚äº†ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ï¼ˆå¤±æ•—æ™‚ã¯ä¾å­˜ãƒã‚§ãƒ¼ãƒ³ã‚’åœæ­¢ï¼‰
if [ $? -ne 0 ]; then
    echo "Simulation failed, stopping chain"
    exit 1
fi
"""

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    parameters = [
        {"idx": 0, "param1": 0.1, "param2": 1.0},
        {"idx": 1, "param1": 0.2, "param2": 1.5},
        {"idx": 2, "param1": 0.3, "param2": 2.0},
    ]

    sequential_parameter_sweep(parameters, template)
</code></pre>
            </details>

            <h3>æ¼”ç¿’4: ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè£…ï¼ˆMediumï¼‰</h3>
            <p>ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã€å¤‰æ›ã€ãƒ­ãƒ¼ãƒ‰ã®3ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’æŒã¤ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚å„ã‚¹ãƒ†ãƒ¼ã‚¸ã§ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã„ã€ã‚¨ãƒ©ãƒ¼æ™‚ã¯é©åˆ‡ã«ãƒ­ã‚°ã‚’è¨˜éŒ²ã—ã¦ãã ã•ã„ã€‚</p>
            <details>
                <summary>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</summary>
                <pre><code class="language-python">#!/usr/bin/env python3
"""
ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ä»˜ãETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
"""

import logging
import pandas as pd
from pathlib import Path

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class ETLPipeline:
    """ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(self, source_file: Path, output_file: Path):
        self.source_file = Path(source_file)
        self.output_file = Path(output_file)

    def extract(self) -> pd.DataFrame:
        """Extract: ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        logging.info(f"Extracting data from {self.source_file}")

        try:
            df = pd.read_csv(self.source_file)
            logging.info(f"Extracted {len(df)} rows, {len(df.columns)} columns")
            return df
        except Exception as e:
            logging.error(f"Extraction failed: {e}")
            raise

    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """Transform: ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
        logging.info("Transforming data...")

        # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
        missing = df.isnull().sum()
        if missing.any():
            logging.warning(f"Missing values detected:\n{missing[missing > 0]}")

        # æ¬ æå€¤é™¤å»
        df_clean = df.dropna()
        logging.info(f"Removed {len(df) - len(df_clean)} rows with missing values")

        # æ•°å€¤ã‚«ãƒ©ãƒ æ¨™æº–åŒ–
        numeric_cols = df_clean.select_dtypes(include=['float64', 'int64']).columns
        for col in numeric_cols:
            df_clean[col] = (df_clean[col] - df_clean[col].mean()) / df_clean[col].std()

        logging.info(f"Standardized {len(numeric_cols)} numeric columns")

        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        assert len(df_clean) > 0, "No data remaining after transformation"
        assert not df_clean.isnull().any().any(), "Unexpected missing values after cleaning"

        return df_clean

    def load(self, df: pd.DataFrame):
        """Load: ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        logging.info(f"Loading data to {self.output_file}")

        try:
            df.to_csv(self.output_file, index=False)
            logging.info(f"Successfully saved {len(df)} rows")
        except Exception as e:
            logging.error(f"Load failed: {e}")
            raise

    def run(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        try:
            data = self.extract()
            transformed = self.transform(data)
            self.load(transformed)
            logging.info("ETL pipeline completed successfully")
        except Exception as e:
            logging.error(f"Pipeline failed: {e}")
            raise

if __name__ == "__main__":
    pipeline = ETLPipeline(
        source_file="raw_data.csv",
        output_file="processed_data.csv"
    )
    pipeline.run()
</code></pre>
            </details>

            <h3>æ¼”ç¿’5: ãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ãƒˆåˆ†æ•£è¨ˆç®—ï¼ˆHardï¼‰</h3>
            <p>ã‚¿ã‚¹ã‚¯ãŒå¤±æ•—ã—ãŸå ´åˆã«è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ï¼ˆæœ€å¤§3å›ï¼‰ã—ã€ãã‚Œã§ã‚‚å¤±æ•—ã™ã‚‹å ´åˆã¯Dead Letter Queueã«é€ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>
            <details>
                <summary>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</summary>
                <pre><code class="language-python">#!/usr/bin/env python3
"""
ãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ãƒˆåˆ†æ•£è¨ˆç®—ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
ãƒªãƒˆãƒ©ã‚¤ã€Dead Letter Queueã€ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°æ©Ÿèƒ½
"""

import time
import logging
from typing import Callable, Any, List
from dataclasses import dataclass
from pathlib import Path
import json

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

@dataclass
class Task:
    """ã‚¿ã‚¹ã‚¯å®šç¾©"""
    task_id: str
    function: Callable
    args: tuple
    kwargs: dict
    max_retries: int = 3
    retry_delay: float = 1.0  # ç§’

@dataclass
class TaskResult:
    """ã‚¿ã‚¹ã‚¯å®Ÿè¡Œçµæœ"""
    task_id: str
    success: bool
    result: Any = None
    error: str = None
    attempts: int = 0

class FaultTolerantExecutor:
    """ãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ãƒˆã‚¿ã‚¹ã‚¯å®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self, dlq_path: Path):
        """
        Parameters
        ----------
        dlq_path : Path
            Dead Letter Queueãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        self.dlq_path = Path(dlq_path)
        self.dlq_path.parent.mkdir(parents=True, exist_ok=True)
        self.results = []

    def execute_task(self, task: Task) -> TaskResult:
        """
        ã‚¿ã‚¹ã‚¯å®Ÿè¡Œï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰

        Parameters
        ----------
        task : Task
            å®Ÿè¡Œã‚¿ã‚¹ã‚¯

        Returns
        -------
        TaskResult
            å®Ÿè¡Œçµæœ
        """
        attempts = 0
        last_error = None

        while attempts < task.max_retries:
            attempts += 1
            try:
                logging.info(f"Task {task.task_id}: Attempt {attempts}/{task.max_retries}")
                result = task.function(*task.args, **task.kwargs)
                logging.info(f"Task {task.task_id}: Success")
                return TaskResult(task.task_id, True, result, attempts=attempts)

            except Exception as e:
                last_error = str(e)
                logging.warning(f"Task {task.task_id}: Failed with {e}")

                if attempts < task.max_retries:
                    # Exponential backoff
                    delay = task.retry_delay * (2 ** (attempts - 1))
                    logging.info(f"Task {task.task_id}: Retrying in {delay:.1f}s...")
                    time.sleep(delay)

        # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°åˆ°é”
        logging.error(f"Task {task.task_id}: Failed after {attempts} attempts")
        return TaskResult(task.task_id, False, error=last_error, attempts=attempts)

    def send_to_dlq(self, task: Task, result: TaskResult):
        """
        Dead Letter Queueã«å¤±æ•—ã‚¿ã‚¹ã‚¯è¨˜éŒ²

        Parameters
        ----------
        task : Task
            å¤±æ•—ã‚¿ã‚¹ã‚¯
        result : TaskResult
            å®Ÿè¡Œçµæœ
        """
        dlq_entry = {
            "task_id": task.task_id,
            "attempts": result.attempts,
            "error": result.error,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }

        # è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã§JSON Lineså½¢å¼ã§ä¿å­˜
        with open(self.dlq_path, "a") as f:
            f.write(json.dumps(dlq_entry) + "\n")

        logging.error(f"Task {task.task_id} sent to DLQ: {self.dlq_path}")

    def run(self, tasks: List[Task]) -> List[TaskResult]:
        """
        ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆå®Ÿè¡Œ

        Parameters
        ----------
        tasks : List[Task]
            ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ

        Returns
        -------
        List[TaskResult]
            å®Ÿè¡Œçµæœãƒªã‚¹ãƒˆ
        """
        results = []

        for task in tasks:
            result = self.execute_task(task)

            if not result.success:
                self.send_to_dlq(task, result)

            results.append(result)

        # ã‚µãƒãƒªãƒ¼
        success_count = sum(1 for r in results if r.success)
        logging.info(f"Execution complete: {success_count}/{len(results)} tasks succeeded")

        return results

# ãƒ†ã‚¹ãƒˆç”¨ã‚¿ã‚¹ã‚¯é–¢æ•°
def flaky_function(value: int, fail_rate: float = 0.5):
    """ç¢ºç‡çš„ã«å¤±æ•—ã™ã‚‹é–¢æ•°ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
    import random
    if random.random() < fail_rate:
        raise RuntimeError(f"Simulated failure for value {value}")
    return value * 2

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    executor = FaultTolerantExecutor(dlq_path=Path("./logs/dlq.jsonl"))

    # ã‚¿ã‚¹ã‚¯å®šç¾©
    tasks = [
        Task(f"task_{i}", flaky_function, (i,), {"fail_rate": 0.3}, max_retries=3)
        for i in range(10)
    ]

    # å®Ÿè¡Œ
    results = executor.run(tasks)

    # çµæœé›†è¨ˆ
    for result in results:
        if result.success:
            print(f"{result.task_id}: SUCCESS (result={result.result}, attempts={result.attempts})")
        else:
            print(f"{result.task_id}: FAILED (error={result.error}, attempts={result.attempts})")
</code></pre>
            </details>

            <h3>æ¼”ç¿’6: é©å¿œçš„ã‚¸ãƒ§ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼ˆHardï¼‰</h3>
            <p>ã‚­ãƒ¥ãƒ¼ã®æ··é›‘çŠ¶æ³ã«å¿œã˜ã¦ã‚¸ãƒ§ãƒ–ã‚µã‚¤ã‚ºã‚’å‹•çš„ã«èª¿æ•´ã™ã‚‹é©å¿œçš„ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚ã‚­ãƒ¥ãƒ¼ãŒç©ºã„ã¦ã„ã‚‹æ™‚ã¯å¤§ããªã‚¸ãƒ§ãƒ–ã‚’ã€æ··é›‘æ™‚ã¯å°ã•ãªã‚¸ãƒ§ãƒ–ã«åˆ†å‰²ã—ã¦æŠ•å…¥ã—ã¾ã™ã€‚</p>
            <details>
                <summary>è§£ç­”ä¾‹ã‚’è¡¨ç¤º</summary>
                <pre><code class="language-python">#!/usr/bin/env python3
"""
é©å¿œçš„ã‚¸ãƒ§ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
ã‚­ãƒ¥ãƒ¼æ··é›‘åº¦ã«å¿œã˜ã¦ã‚¸ãƒ§ãƒ–ã‚µã‚¤ã‚ºã‚’å‹•çš„èª¿æ•´
"""

import subprocess
import re
from typing import Dict
from dataclasses import dataclass

@dataclass
class ClusterState:
    """ã‚¯ãƒ©ã‚¹ã‚¿çŠ¶æ…‹"""
    total_nodes: int
    idle_nodes: int
    pending_jobs: int
    running_jobs: int
    utilization: float

@dataclass
class JobSpec:
    """ã‚¸ãƒ§ãƒ–ä»•æ§˜"""
    total_tasks: int
    script_template: str
    min_nodes: int = 1
    max_nodes: int = 16

class AdaptiveScheduler:
    """é©å¿œçš„ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©"""

    def get_cluster_state(self) -> ClusterState:
        """ã‚¯ãƒ©ã‚¹ã‚¿çŠ¶æ…‹å–å¾—"""
        # sinfo ã§ãƒãƒ¼ãƒ‰æƒ…å ±å–å¾—
        result = subprocess.run(
            ["sinfo", "-h", "-o", "%D,%T"],
            capture_output=True,
            text=True
        )

        total_nodes = 0
        idle_nodes = 0
        for line in result.stdout.strip().split("\n"):
            parts = line.split(",")
            if len(parts) == 2:
                nodes = int(parts[0])
                state = parts[1]
                total_nodes += nodes
                if "idle" in state.lower():
                    idle_nodes += nodes

        # squeue ã§å®Ÿè¡Œä¸­ãƒ»å¾…æ©Ÿä¸­ã‚¸ãƒ§ãƒ–æ•°å–å¾—
        result = subprocess.run(
            ["squeue", "-h", "-o", "%T"],
            capture_output=True,
            text=True
        )

        running = result.stdout.count("RUNNING")
        pending = result.stdout.count("PENDING")

        utilization = 1 - (idle_nodes / total_nodes) if total_nodes > 0 else 1.0

        return ClusterState(total_nodes, idle_nodes, pending, running, utilization)

    def decide_job_size(self, job: JobSpec, state: ClusterState) -> int:
        """
        ã‚¯ãƒ©ã‚¹ã‚¿çŠ¶æ…‹ã«åŸºã¥ã„ã¦ã‚¸ãƒ§ãƒ–ã‚µã‚¤ã‚ºæ±ºå®š

        Parameters
        ----------
        job : JobSpec
            ã‚¸ãƒ§ãƒ–ä»•æ§˜
        state : ClusterState
            ã‚¯ãƒ©ã‚¹ã‚¿çŠ¶æ…‹

        Returns
        -------
        int
            å‰²ã‚Šå½“ã¦ãƒãƒ¼ãƒ‰æ•°
        """
        # åˆ©ç”¨ç‡ãƒ™ãƒ¼ã‚¹ã®æˆ¦ç•¥
        if state.utilization < 0.3:
            # ç©ºã„ã¦ã„ã‚‹ï¼šæœ€å¤§ã‚µã‚¤ã‚º
            nodes = min(job.max_nodes, state.idle_nodes)
        elif state.utilization < 0.7:
            # ä¸­ç¨‹åº¦ï¼šåŠåˆ†
            nodes = min(job.max_nodes // 2, state.idle_nodes)
        else:
            # æ··é›‘ï¼šæœ€å°ã‚µã‚¤ã‚º
            nodes = job.min_nodes

        # ãƒšãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ãŒå¤šã„å ´åˆã¯ã•ã‚‰ã«ç¸®å°
        if state.pending_jobs > 10:
            nodes = max(job.min_nodes, nodes // 2)

        return max(job.min_nodes, min(nodes, job.max_nodes))

    def submit_adaptive_job(self, job: JobSpec):
        """
        é©å¿œçš„ã‚¸ãƒ§ãƒ–æŠ•å…¥

        Parameters
        ----------
        job : JobSpec
            ã‚¸ãƒ§ãƒ–ä»•æ§˜
        """
        state = self.get_cluster_state()
        nodes = self.decide_job_size(job, state)

        print(f"Cluster state: {state.utilization*100:.1f}% utilized, {state.idle_nodes} idle nodes")
        print(f"Decision: Allocating {nodes} nodes")

        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ
        script = job.script_template.replace("{NODES}", str(nodes))
        script_path = "adaptive_job.sh"
        with open(script_path, "w") as f:
            f.write(script)

        # æŠ•å…¥
        result = subprocess.run(
            ["sbatch", script_path],
            capture_output=True,
            text=True,
            check=True
        )

        match = re.search(r"Submitted batch job (\d+)", result.stdout)
        if match:
            print(f"Submitted job: {match.group(1)}")

# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä¾‹
template = """#!/bin/bash
#SBATCH --job-name=adaptive_job
#SBATCH --nodes={NODES}
#SBATCH --ntasks-per-node=32
#SBATCH --time=01:00:00
#SBATCH --output=adaptive_%j.out

echo "Running on {NODES} nodes"
srun ./my_mpi_app
"""

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    scheduler = AdaptiveScheduler()
    job = JobSpec(
        total_tasks=1000,
        script_template=template,
        min_nodes=1,
        max_nodes=16
    )

    scheduler.submit_adaptive_job(job)
</code></pre>
            </details>
        </section>

        <section id="references">
            <h2>5.8 å‚è€ƒæ–‡çŒ®</h2>
            <ol>
                <li>Rocklin, M. (2015). Dask: Parallel Computation with Blocked Algorithms and Task Scheduling. <em>Proceedings of the 14th Python in Science Conference (SciPy 2015)</em>, pp. 126-132.</li>
                <li>Moritz, P., Nishihara, R., Wang, S., et al. (2018). Ray: A Distributed Framework for Emerging AI Applications. <em>Proceedings of OSDI 2018</em>, pp. 561-577.</li>
                <li>Babuji, Y., Woodard, A., Li, Z., et al. (2019). Parsl: Pervasive Parallel Programming in Python. <em>Proceedings of HPDC 2019</em>, pp. 25-36.</li>
                <li>McKinney, W. (2022). <em>Python for Data Analysis: Data Wrangling with pandas, NumPy, and Jupyter</em>, 3rd Edition. O'Reilly Media, pp. 200-280, 350-420.</li>
                <li>Beazley, D. (2009). <em>Python Essential Reference</em>, 4th Edition. Addison-Wesley Professional, pp. 180-245.</li>
                <li>Deelman, E., Vahi, K., Juve, G., et al. (2015). Pegasus: A Workflow Management System for Science Automation. <em>Future Generation Computer Systems</em>, 46, pp. 17-35.</li>
                <li>Amstutz, P., Crusoe, M. R., TijaniÄ‡, N., et al. (2016). Common Workflow Language, v1.0. <em>figshare</em>. https://doi.org/10.6084/m9.figshare.3115156.v2</li>
            </ol>
        </section>
    </main>

    <footer>
        <p>&copy; 2025 CT Dojo. All rights reserved.</p>
        <p><a href="../index.html">CTãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ›ãƒ¼ãƒ ã¸æˆ»ã‚‹</a> | <a href="index.html">HPCã‚¯ãƒ©ã‚¹ã‚¿å…¥é–€ãƒˆãƒƒãƒ—ã¸</a></p>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/line-numbers/prism-line-numbers.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</body>
</html>