<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第3章: 並列計算とMPI（Parallel Computing and MPI） - HPCクラスタ入門 - CT Dojo</title>
    <meta name="description" content="並列計算の基礎、MPIプログラミング、集団通信、領域分割、mpi4pyによるPython並列化、性能解析とスケーラビリティを学ぶ実践ガイド。">

    <!-- Prism.js for code highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">

    <!-- MathJax for mathematical expressions -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>

    <style>
        :root {
            --accent-green: #11998e;
            --accent-lime: #38ef7d;
            --primary-dark: #2c3e50;
            --secondary-dark: #34495e;
            --text-dark: #2c3e50;
            --text-light: #7f8c8d;
            --bg-light: #ecf0f1;
            --white: #ffffff;
            --code-bg: #2d2d2d;
            --border-light: #bdc3c7;
            --success: #27ae60;
            --warning: #f39c12;
            --danger: #e74c3c;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.8;
            color: var(--text-dark);
            background: var(--bg-light);
        }

        header {
            background: linear-gradient(135deg, var(--accent-green) 0%, var(--accent-lime) 100%);
            color: white;
            padding: 3rem 1.5rem;
            text-align: center;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        header h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
        }

        header p {
            font-size: 1.1rem;
            opacity: 0.95;
        }

        nav {
            background: var(--white);
            padding: 1rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            position: sticky;
            top: 0;
            z-index: 100;
        }

        nav ul {
            list-style: none;
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 0.5rem;
        }

        nav a {
            text-decoration: none;
            color: var(--text-dark);
            padding: 0.5rem 1rem;
            border-radius: 4px;
            transition: all 0.3s;
            font-weight: 500;
        }

        nav a:hover {
            background: linear-gradient(135deg, var(--accent-green) 0%, var(--accent-lime) 100%);
            color: white;
        }

        main {
            max-width: 900px;
            margin: 2rem auto;
            padding: 0 1.5rem;
        }

        section {
            background: var(--white);
            padding: 2rem;
            margin-bottom: 2rem;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        h2 {
            color: var(--primary-dark);
            font-size: 1.8rem;
            margin-bottom: 1.5rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid;
            border-image: linear-gradient(90deg, var(--accent-green), var(--accent-lime)) 1;
        }

        h3 {
            color: var(--secondary-dark);
            font-size: 1.4rem;
            margin: 2rem 0 1rem;
        }

        h4 {
            color: var(--secondary-dark);
            font-size: 1.2rem;
            margin: 1.5rem 0 1rem;
        }

        p {
            margin-bottom: 1rem;
            line-height: 1.8;
        }

        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        code {
            background: #f8f9fa;
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: "Consolas", "Monaco", monospace;
            font-size: 0.9em;
            color: #e74c3c;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1.5rem 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.15);
        }

        pre code {
            background: none;
            color: #f8f8f2;
            padding: 0;
        }

        .info-box {
            background: linear-gradient(135deg, rgba(17, 153, 142, 0.1) 0%, rgba(56, 239, 125, 0.1) 100%);
            border-left: 4px solid var(--accent-green);
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }

        .info-box strong {
            color: var(--accent-green);
            display: block;
            margin-bottom: 0.5rem;
            font-size: 1.1rem;
        }

        .warning-box {
            background: rgba(243, 156, 18, 0.1);
            border-left: 4px solid var(--warning);
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }

        .warning-box strong {
            color: var(--warning);
            display: block;
            margin-bottom: 0.5rem;
            font-size: 1.1rem;
        }

        .exercise-box {
            background: rgba(39, 174, 96, 0.05);
            border: 2px solid var(--success);
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 8px;
        }

        .exercise-box h4 {
            color: var(--success);
            margin-top: 0;
        }

        .difficulty {
            display: inline-block;
            padding: 0.25rem 0.75rem;
            border-radius: 12px;
            font-size: 0.85rem;
            font-weight: 600;
            margin-left: 0.5rem;
        }

        .difficulty.easy {
            background: #d4edda;
            color: #155724;
        }

        .difficulty.medium {
            background: #fff3cd;
            color: #856404;
        }

        .difficulty.hard {
            background: #f8d7da;
            color: #721c24;
        }

        .mermaid {
            background: white;
            padding: 2rem;
            border-radius: 8px;
            margin: 2rem 0;
            text-align: center;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            background: white;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        th, td {
            padding: 1rem;
            text-align: left;
            border-bottom: 1px solid var(--border-light);
        }

        th {
            background: linear-gradient(135deg, var(--accent-green) 0%, var(--accent-lime) 100%);
            color: white;
            font-weight: 600;
        }

        tr:hover {
            background: rgba(17, 153, 142, 0.05);
        }

        footer {
            background: var(--primary-dark);
            color: white;
            text-align: center;
            padding: 2rem;
            margin-top: 4rem;
        }

        footer a {
            color: var(--accent-green);
            text-decoration: none;
        }

        footer a:hover {
            color: var(--accent-lime);
        }

        @media (max-width: 768px) {
            header h1 {
                font-size: 1.5rem;
            }

            nav ul {
                flex-direction: column;
                align-items: center;
            }

            section {
                padding: 1.5rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.85rem;
            }
        }

        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #11998e;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #0e7c74;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }

        details {
            margin: 1rem 0;
            padding: 1rem;
            background: #f8f9fa;
            border-radius: 4px;
            border-left: 3px solid var(--accent-green);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--accent-green);
            user-select: none;
        }

        summary:hover {
            color: var(--accent-lime);
        }

        details[open] summary {
            margin-bottom: 1rem;
        }
    </style>
</head>
<body>
    <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/AI-Knowledge-Notes/knowledge/jp/index.html">AI寺子屋トップ</a><span class="breadcrumb-separator">›</span><a href="/AI-Knowledge-Notes/knowledge/jp/CT/index.html">計算工学</a><span class="breadcrumb-separator">›</span><a href="/AI-Knowledge-Notes/knowledge/jp/CT/hpc-cluster-introduction/index.html">HPC Cluster</a><span class="breadcrumb-separator">›</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

    <header>
        <h1>第3章: 並列計算とMPI（Parallel Computing and MPI）</h1>
        <p>並列計算の基礎・MPIプログラミング・集団通信・領域分割・mpi4py・性能解析</p>
    </header>

    <nav>
        <ul>
            <li><a href="index.html">トップ</a></li>
            <li><a href="#intro">本章の概要</a></li>
            <li><a href="#parallel-fundamentals">並列計算基礎</a></li>
            <li><a href="#mpi-basics">MPI基礎</a></li>
            <li><a href="#collective">集団通信</a></li>
            <li><a href="#domain-decomposition">領域分割</a></li>
            <li><a href="#mpi4py">mpi4py</a></li>
            <li><a href="#performance">性能解析</a></li>
            <li><a href="#exercises">演習問題</a></li>
            <li><a href="#references">参考文献</a></li>
            <li><a href="chapter-2.html">← 前の章</a></li>
            <li><a href="chapter-4.html">次の章へ →</a></li>
        </ul>
    </nav>

    <main>
        <section id="intro">
            <h2>3.1 本章の概要</h2>

            <p>
                並列計算は、大規模な科学技術計算を高速化するための基本技術です。本章では、<strong>MPI（Message Passing Interface）</strong>を使った分散メモリ並列計算の理論と実践を学びます。MPI_SendやMPI_Recvによる基本的な通信から、MPI_Bcast、MPI_Reduceなどの集団通信、さらに領域分割による実践的な並列化戦略まで、包括的に習得します。
            </p>

            <div class="info-box">
                <strong>本章の学習目標</strong>
                <ul>
                    <li><strong>レベル1（基本理解）</strong>: Amdahlの法則とGustafsonの法則を説明でき、Strong scalingとWeak scalingの違いを理解できる</li>
                    <li><strong>レベル2（実践スキル）</strong>: MPIの基本通信（Send/Recv）と集団通信（Bcast/Reduce）を使ったプログラムを作成でき、mpi4pyでPythonコードを並列化できる</li>
                    <li><strong>レベル3（応用力）</strong>: 領域分割戦略を設計し、並列プログラムの性能を測定・評価し、スケーラビリティのボトルネックを特定・解決できる</li>
                </ul>
            </div>

            <div class="warning-box">
                <strong>前提知識</strong>
                <p>本章では、C言語またはPythonの基本、第1章・第2章で学んだHPCクラスタとジョブスケジューラの知識を前提とします。</p>
            </div>
        </section>

        <section id="parallel-fundamentals">
            <h2>3.2 並列計算の基礎概念</h2>

            <h3>Amdahlの法則とGustafsonの法則</h3>
            <p>
                <strong>Amdahlの法則</strong>は、プログラムの並列化可能な部分の割合\(P\)とプロセッサ数\(N\)に対する理論的な高速化率を示します：
            </p>

            <p style="text-align: center; font-size: 1.1rem; margin: 1.5rem 0;">
                \[
                S(N) = \frac{1}{(1 - P) + \frac{P}{N}}
                \]
            </p>

            <p>
                例えば、プログラムの95%が並列化可能（\(P = 0.95\)）な場合、100プロセッサを使っても理論的高速化率は約17倍が上限です。これは、並列化できない5%の部分がボトルネックになるためです。
            </p>

            <p>
                一方、<strong>Gustafsonの法則</strong>は、問題サイズを大きくすることで並列化効率を維持できることを示します：
            </p>

            <p style="text-align: center; font-size: 1.1rem; margin: 1.5rem 0;">
                \[
                S(N) = N - \alpha(N - 1) = N + (1 - N)\alpha
                \]
            </p>

            <p>
                ここで、\(\alpha\)は逐次部分の割合です。この法則は、「プロセッサ数を増やすと同時に問題サイズも大きくする」現実的な使い方において、高い並列化効率を維持できることを示しています。
            </p>

            <h3>Strong Scaling vs Weak Scaling</h3>
            <table>
                <thead>
                    <tr>
                        <th>スケーリング種別</th>
                        <th>定義</th>
                        <th>測定方法</th>
                        <th>理想的な結果</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Strong Scaling</strong></td>
                        <td>問題サイズ固定でプロセッサ数を増やす</td>
                        <td>\(T_1 / T_N\)（1プロセッサ時間 / N プロセッサ時間）</td>
                        <td>高速化率 = \(N\)（完全線形）</td>
                    </tr>
                    <tr>
                        <td><strong>Weak Scaling</strong></td>
                        <td>プロセッサあたりの問題サイズ固定で全体を増やす</td>
                        <td>実行時間が一定に保たれるか</td>
                        <td>実行時間 = 一定</td>
                    </tr>
                </tbody>
            </table>

            <h3>並列計算の分類</h3>
            <table>
                <thead>
                    <tr>
                        <th>並列化タイプ</th>
                        <th>特徴</th>
                        <th>通信オーバーヘッド</th>
                        <th>適用例</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Embarrassingly Parallel</strong></td>
                        <td>プロセス間通信がほぼ不要</td>
                        <td>最小</td>
                        <td>パラメータスイープ、モンテカルロシミュレーション</td>
                    </tr>
                    <tr>
                        <td><strong>Communication-bound</strong></td>
                        <td>頻繁な通信が必要</td>
                        <td>大</td>
                        <td>流体力学シミュレーション、分子動力学</td>
                    </tr>
                    <tr>
                        <td><strong>Memory-bound</strong></td>
                        <td>メモリアクセスが律速</td>
                        <td>中</td>
                        <td>行列計算、FFT</td>
                    </tr>
                </tbody>
            </table>

            <div class="info-box">
                <strong>実践的な意味</strong>
                <p>
                    アプリケーションの並列化戦略は、これらの分類により大きく異なります。Embarrassingly Parallelな問題では通信を最小化し、Communication-boundな問題では領域分割と通信パターンの最適化が重要です。
                </p>
            </div>
        </section>

        <section id="mpi-basics">
            <h2>3.3 MPIの基礎</h2>

            <h3>MPIアーキテクチャとコミュニケータ</h3>
            <p>
                MPIは、複数のプロセス（通常は異なるノード上）が<strong>メッセージ通信</strong>によってデータを交換する分散メモリ並列化モデルです。各プロセスは独立したメモリ空間を持ち、明示的な通信命令でデータをやり取りします。
            </p>

            <div class="mermaid">
flowchart LR
    subgraph Node1["ノード1"]
        P0[Rank 0<br/>独立メモリ]
        P1[Rank 1<br/>独立メモリ]
    end

    subgraph Node2["ノード2"]
        P2[Rank 2<br/>独立メモリ]
        P3[Rank 3<br/>独立メモリ]
    end

    P0 <-->|MPI_Send/Recv| P1
    P0 <-->|メッセージ通信| P2
    P1 <-->|インターコネクト| P3
    P2 <-->|MPI通信| P3

    style Node1 fill:#e3f2fd
    style Node2 fill:#fff3e0
            </div>

            <p>
                <strong>コミュニケータ（Communicator）</strong>は、通信に参加するプロセスのグループを定義します。デフォルトのコミュニケータは<code>MPI_COMM_WORLD</code>で、全プロセスが含まれます。
            </p>

            <h3>コード例1: MPI Hello World（C版とPython版）</h3>
            <p><strong>C言語版:</strong></p>
            <pre><code class="language-c">/* hello_mpi.c - MPI基本プログラム */
#include &lt;mpi.h&gt;
#include &lt;stdio.h&gt;

int main(int argc, char** argv) {
    int rank, size;
    char processor_name[MPI_MAX_PROCESSOR_NAME];
    int name_len;

    /* MPIの初期化 */
    MPI_Init(&argc, &argv);

    /* ランク（プロセスID）を取得 */
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    /* 全プロセス数を取得 */
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    /* プロセッサ名を取得 */
    MPI_Get_processor_name(processor_name, &name_len);

    /* 各プロセスが情報を出力 */
    printf("Hello from rank %d of %d on %s\n",
           rank, size, processor_name);

    /* MPIの終了処理 */
    MPI_Finalize();
    return 0;
}

/* コンパイルと実行:
 * mpicc -o hello_mpi hello_mpi.c
 * mpirun -np 4 ./hello_mpi
 *
 * 出力例:
 * Hello from rank 0 of 4 on node001
 * Hello from rank 1 of 4 on node001
 * Hello from rank 2 of 4 on node002
 * Hello from rank 3 of 4 on node002
 */</code></pre>

            <p><strong>Python版（mpi4py）:</strong></p>
            <pre><code class="language-python"># hello_mpi.py - mpi4pyによるMPI基本プログラム
from mpi4py import MPI

# コミュニケータを取得
comm = MPI.COMM_WORLD

# ランク（プロセスID）を取得
rank = comm.Get_rank()

# 全プロセス数を取得
size = comm.Get_size()

# プロセッサ名を取得
processor_name = MPI.Get_processor_name()

# 各プロセスが情報を出力
print(f"Hello from rank {rank} of {size} on {processor_name}")

# 実行:
# mpiexec -n 4 python hello_mpi.py
#
# 出力例:
# Hello from rank 0 of 4 on node001
# Hello from rank 1 of 4 on node001
# Hello from rank 2 of 4 on node002
# Hello from rank 3 of 4 on node002</code></pre>

            <h3>Point-to-Point通信（MPI_Send、MPI_Recv）</h3>
            <p>
                Point-to-Point通信は、2つのプロセス間で直接データをやり取りする最も基本的な通信パターンです。
            </p>

            <h3>コード例2: Ping-Pongパターン（ブロッキング通信）</h3>
            <pre><code class="language-c">/* ping_pong.c - ブロッキング通信の例 */
#include &lt;mpi.h&gt;
#include &lt;stdio.h&gt;

int main(int argc, char** argv) {
    int rank, size;
    int ping_pong_count = 0;
    int partner_rank;
    MPI_Status status;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    /* 2プロセスでのみ実行 */
    if (size != 2) {
        if (rank == 0) {
            fprintf(stderr, "This program requires exactly 2 processes\n");
        }
        MPI_Finalize();
        return 1;
    }

    /* パートナーランクを決定 */
    partner_rank = (rank + 1) % 2;

    /* Ping-Pongを10回実行 */
    while (ping_pong_count < 10) {
        if (rank == ping_pong_count % 2) {
            /* 送信側: カウントを増やして送信 */
            ping_pong_count++;
            MPI_Send(&ping_pong_count, 1, MPI_INT, partner_rank, 0,
                     MPI_COMM_WORLD);
            printf("Rank %d sent ping_pong_count %d to rank %d\n",
                   rank, ping_pong_count, partner_rank);
        } else {
            /* 受信側: データを受け取る */
            MPI_Recv(&ping_pong_count, 1, MPI_INT, partner_rank, 0,
                     MPI_COMM_WORLD, &status);
            printf("Rank %d received ping_pong_count %d from rank %d\n",
                   rank, ping_pong_count, partner_rank);
        }
    }

    MPI_Finalize();
    return 0;
}

/* コンパイルと実行:
 * mpicc -o ping_pong ping_pong.c
 * mpirun -np 2 ./ping_pong
 */</code></pre>

            <h3>ブロッキング通信 vs ノンブロッキング通信</h3>
            <table>
                <thead>
                    <tr>
                        <th>通信タイプ</th>
                        <th>関数</th>
                        <th>動作</th>
                        <th>利点</th>
                        <th>欠点</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>ブロッキング</strong></td>
                        <td><code>MPI_Send</code><br/><code>MPI_Recv</code></td>
                        <td>通信完了まで待機</td>
                        <td>実装が単純、デバッグしやすい</td>
                        <td>通信中は計算できない</td>
                    </tr>
                    <tr>
                        <td><strong>ノンブロッキング</strong></td>
                        <td><code>MPI_Isend</code><br/><code>MPI_Irecv</code><br/><code>MPI_Wait</code></td>
                        <td>通信要求を発行後すぐに戻る</td>
                        <td>通信と計算をオーバーラップ可能</td>
                        <td>実装が複雑、Wait管理が必要</td>
                    </tr>
                </tbody>
            </table>

            <div class="warning-box">
                <strong>デッドロックに注意</strong>
                <p>
                    ブロッキング通信で送信と受信の順序を誤ると、全プロセスが相手の通信を待ち続ける<strong>デッドロック</strong>が発生します。例えば、全プロセスが先に<code>MPI_Recv</code>を呼ぶと、誰も送信しないため永久に待機します。
                </p>
            </div>
        </section>

        <section id="collective">
            <h2>3.4 集団通信（Collective Communication）</h2>

            <h3>集団通信の種類と用途</h3>
            <p>
                集団通信は、コミュニケータ内の全プロセスが参加する通信操作です。手動でループを書くよりも効率的に実装されており、通信パターンに応じて最適化されています。
            </p>

            <div class="mermaid">
flowchart TB
    subgraph Bcast["MPI_Bcast（ブロードキャスト）"]
        B0[Rank 0<br/>データ: X] --> B1[Rank 1<br/>データ: X]
        B0 --> B2[Rank 2<br/>データ: X]
        B0 --> B3[Rank 3<br/>データ: X]
    end

    subgraph Scatter["MPI_Scatter（分散）"]
        S0[Rank 0<br/>データ: ABCD] --> S1[Rank 1<br/>データ: B]
        S0 --> S2[Rank 2<br/>データ: C]
        S0 --> S3[Rank 3<br/>データ: D]
        S0 -.->|A| S0
    end

    subgraph Gather["MPI_Gather（収集）"]
        G1[Rank 1<br/>データ: B] --> G0[Rank 0<br/>データ: ABCD]
        G2[Rank 2<br/>データ: C] --> G0
        G3[Rank 3<br/>データ: D] --> G0
        G0A[Rank 0<br/>データ: A] -.->|A| G0
    end

    subgraph Reduce["MPI_Reduce（削減）"]
        R0[Rank 0<br/>値: 10] --> R[Rank 0<br/>合計: 100]
        R1[Rank 1<br/>値: 20] --> R
        R2[Rank 2<br/>値: 30] --> R
        R3[Rank 3<br/>値: 40] --> R
    end

    style Bcast fill:#e3f2fd
    style Scatter fill:#fff3e0
    style Gather fill:#e8f5e9
    style Reduce fill:#f3e5f5
            </div>

            <h3>コード例3: 集団通信の基本操作</h3>
            <pre><code class="language-c">/* collective_ops.c - 集団通信の例 */
#include &lt;mpi.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

int main(int argc, char** argv) {
    int rank, size;
    int data;
    int *send_array = NULL;
    int recv_value;
    int sum;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    /* ==========================
     * 1. MPI_Bcast（ブロードキャスト）
     * Rank 0から全プロセスにデータを配信
     * ========================== */
    if (rank == 0) {
        data = 42;  /* Rank 0が初期値を設定 */
        printf("Rank 0 broadcasting data: %d\n", data);
    }

    /* 全プロセスがBcastを呼ぶ（集団通信） */
    MPI_Bcast(&data, 1, MPI_INT, 0, MPI_COMM_WORLD);
    printf("Rank %d received broadcast data: %d\n", rank, data);

    /* ==========================
     * 2. MPI_Scatter（分散）
     * Rank 0が配列を分割して各プロセスに配信
     * ========================== */
    if (rank == 0) {
        send_array = (int*)malloc(size * sizeof(int));
        for (int i = 0; i < size; i++) {
            send_array[i] = i * 10;  /* 0, 10, 20, 30, ... */
        }
        printf("Rank 0 scattering array: ");
        for (int i = 0; i < size; i++) {
            printf("%d ", send_array[i]);
        }
        printf("\n");
    }

    MPI_Scatter(send_array, 1, MPI_INT, &recv_value, 1, MPI_INT,
                0, MPI_COMM_WORLD);
    printf("Rank %d received scattered value: %d\n", rank, recv_value);

    /* ==========================
     * 3. MPI_Reduce（削減：合計）
     * 各プロセスの値を合計してRank 0に集約
     * ========================== */
    int my_value = rank + 1;  /* Rank 0: 1, Rank 1: 2, ... */
    MPI_Reduce(&my_value, &sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Sum of all ranks: %d (expected: %d)\n",
               sum, size * (size + 1) / 2);
    }

    /* ==========================
     * 4. MPI_Allreduce（全削減）
     * 全プロセスが削減結果を受け取る
     * ========================== */
    int global_sum;
    MPI_Allreduce(&my_value, &global_sum, 1, MPI_INT, MPI_SUM,
                  MPI_COMM_WORLD);
    printf("Rank %d: Global sum from Allreduce: %d\n", rank, global_sum);

    if (rank == 0 && send_array != NULL) {
        free(send_array);
    }

    MPI_Finalize();
    return 0;
}

/* コンパイルと実行:
 * mpicc -o collective_ops collective_ops.c
 * mpirun -np 4 ./collective_ops
 */</code></pre>

            <h3>主要な集団通信操作</h3>
            <table>
                <thead>
                    <tr>
                        <th>MPI関数</th>
                        <th>動作</th>
                        <th>用途例</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><code>MPI_Bcast</code></td>
                        <td>1つのプロセスから全プロセスへデータを配信</td>
                        <td>設定パラメータの共有、初期値の配信</td>
                    </tr>
                    <tr>
                        <td><code>MPI_Scatter</code></td>
                        <td>配列を分割して各プロセスに配信</td>
                        <td>データの並列処理への分割</td>
                    </tr>
                    <tr>
                        <td><code>MPI_Gather</code></td>
                        <td>各プロセスのデータを1つのプロセスに収集</td>
                        <td>並列計算結果の集約</td>
                    </tr>
                    <tr>
                        <td><code>MPI_Reduce</code></td>
                        <td>削減操作（合計、最大値等）を適用して1プロセスに集約</td>
                        <td>全体の合計、最大値の計算</td>
                    </tr>
                    <tr>
                        <td><code>MPI_Allreduce</code></td>
                        <td>削減操作の結果を全プロセスに配信</td>
                        <td>全プロセスが結果を必要とする場合</td>
                    </tr>
                    <tr>
                        <td><code>MPI_Allgather</code></td>
                        <td>全プロセスのデータを全プロセスに収集</td>
                        <td>全プロセスが全データを必要とする場合</td>
                    </tr>
                </tbody>
            </table>

            <h3>性能上の考慮事項</h3>
            <div class="info-box">
                <strong>集団通信の最適化</strong>
                <ul>
                    <li><strong>ツリーアルゴリズム</strong>: MPI_Bcastは単純なループではなく、バイナリツリーで実装され、\(O(\log N)\)の通信ステップで完了</li>
                    <li><strong>パイプライン化</strong>: 大きなデータは分割され、パイプライン方式で転送されることがある</li>
                    <li><strong>ベンダー最適化</strong>: InfiniBandなどの高速インターコネクトに対して、MPIライブラリが最適化されている</li>
                    <li><strong>同期コスト</strong>: 集団通信は全プロセスの同期を伴うため、負荷不均衡があると性能が低下</li>
                </ul>
            </div>
        </section>

        <section id="domain-decomposition">
            <h2>3.5 領域分割と並列化戦略</h2>

            <h3>領域分割の基本概念</h3>
            <p>
                領域分割（Domain Decomposition）は、計算対象の空間やデータを複数の部分領域に分割し、各プロセスに割り当てる並列化手法です。シミュレーション、画像処理、行列計算など、多くのアプリケーションで使われます。
            </p>

            <h3>1D、2D、3D分割の選択</h3>
            <table>
                <thead>
                    <tr>
                        <th>分割次元</th>
                        <th>通信パターン</th>
                        <th>通信量</th>
                        <th>適用例</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>1D分割</strong></td>
                        <td>上下（または左右）の隣接プロセスのみ</td>
                        <td>少</td>
                        <td>1次元偏微分方程式、時系列データ処理</td>
                    </tr>
                    <tr>
                        <td><strong>2D分割</strong></td>
                        <td>上下左右の4方向</td>
                        <td>中</td>
                        <td>2次元シミュレーション、画像処理</td>
                    </tr>
                    <tr>
                        <td><strong>3D分割</strong></td>
                        <td>6方向（上下左右前後）</td>
                        <td>多</td>
                        <td>3次元流体力学、分子動力学</td>
                    </tr>
                </tbody>
            </table>

            <h3>ゴーストセル（Ghost Cells）とHalo Exchange</h3>
            <p>
                領域分割では、境界付近の計算に隣接領域のデータが必要になります。このため、各プロセスは自分の領域の周囲に<strong>ゴーストセル</strong>（または<strong>ハロー領域</strong>）を持ち、隣接プロセスとデータを交換します。
            </p>

            <h3>コード例4: 1D領域分割による熱伝導方程式の並列化</h3>
            <pre><code class="language-python"># heat_equation_1d_mpi.py - 1D熱伝導方程式のMPI並列化
from mpi4py import MPI
import numpy as np
import matplotlib.pyplot as plt

def heat_equation_1d_parallel(n_global, n_steps, alpha=0.01):
    """
    1D熱伝導方程式を領域分割で並列計算

    Parameters:
    -----------
    n_global : int
        全体のグリッドポイント数
    n_steps : int
        時間ステップ数
    alpha : float
        熱拡散係数
    """
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()

    # 各プロセスの担当領域サイズを計算
    n_local = n_global // size

    # ゴーストセル（境界の隣接データ）を含む配列を確保
    # [左ゴースト, 実データ, 右ゴースト]
    u = np.zeros(n_local + 2)
    u_new = np.zeros(n_local + 2)

    # 初期条件: 中央に熱源を配置
    global_mid = n_global // 2
    local_start = rank * n_local
    local_end = (rank + 1) * n_local

    if local_start <= global_mid < local_end:
        local_index = global_mid - local_start + 1  # +1はゴーストセル分
        u[local_index] = 100.0

    # 時間発展
    for step in range(n_steps):
        # ゴーストセル交換（Halo Exchange）
        # 左隣と通信
        if rank > 0:
            comm.Send(u[1:2], dest=rank-1, tag=0)  # 左端の実データを送信
            comm.Recv(u[0:1], source=rank-1, tag=1)  # 左ゴーストを受信

        # 右隣と通信
        if rank < size - 1:
            comm.Send(u[-2:-1], dest=rank+1, tag=1)  # 右端の実データを送信
            comm.Recv(u[-1:], source=rank+1, tag=0)  # 右ゴーストを受信

        # 熱伝導方程式を差分法で解く（実データ部分のみ）
        for i in range(1, n_local + 1):
            u_new[i] = u[i] + alpha * (u[i-1] - 2*u[i] + u[i+1])

        # 境界条件（両端は0度固定）
        if rank == 0:
            u_new[1] = 0.0
        if rank == size - 1:
            u_new[n_local] = 0.0

        # 配列を更新
        u, u_new = u_new, u

    # 結果を収集（Rank 0に集約）
    local_result = u[1:n_local+1]  # ゴーストセルを除く実データ
    global_result = None

    if rank == 0:
        global_result = np.zeros(n_global)

    comm.Gather(local_result, global_result, root=0)

    # Rank 0が結果を可視化
    if rank == 0:
        plt.plot(global_result)
        plt.xlabel('Position')
        plt.ylabel('Temperature')
        plt.title(f'1D Heat Equation (MPI, {size} processes, {n_steps} steps)')
        plt.savefig('heat_equation_mpi.png')
        print(f"Simulation completed. Result saved to heat_equation_mpi.png")

    return global_result

if __name__ == "__main__":
    # 実行: mpiexec -n 4 python heat_equation_1d_mpi.py
    result = heat_equation_1d_parallel(n_global=1000, n_steps=500)
</code></pre>

            <h3>負荷分散（Load Balancing）</h3>
            <p>
                領域分割において、各プロセスの計算量が均等であることが重要です。不均等な場合、一部のプロセスが早く終わっても、遅いプロセスを待つ必要があります。
            </p>

            <div class="info-box">
                <strong>負荷分散の戦略</strong>
                <ul>
                    <li><strong>静的分割</strong>: 事前に領域を均等に分割（実装が簡単、計算量が均一な場合に有効）</li>
                    <li><strong>動的分割</strong>: 実行時に計算量を測定し、動的に再分割（適応メッシュ細分化など）</li>
                    <li><strong>空間充填曲線</strong>: ヒルベルト曲線などで3D空間を1Dにマッピングし、連続した領域を割り当て</li>
                </ul>
            </div>
        </section>

        <section id="mpi4py">
            <h2>3.6 mpi4pyによるPython並列化</h2>

            <h3>mpi4pyの基本</h3>
            <p>
                mpi4pyは、PythonでMPIを使うための高レベルインターフェースを提供します。NumPy配列の効率的な通信、Pythonオブジェクトのpickle通信など、Python特有の機能をサポートしています。
            </p>

            <h3>コード例5: mpi4pyによる並列行列ベクトル積</h3>
            <pre><code class="language-python"># matvec_mpi.py - mpi4pyによる並列行列ベクトル積
from mpi4py import MPI
import numpy as np

def parallel_matvec(A, x):
    """
    並列行列ベクトル積: y = A * x
    行列Aを行方向に分割して並列計算

    Parameters:
    -----------
    A : numpy array (only on rank 0)
        行列 (m × n)
    x : numpy array (only on rank 0)
        ベクトル (n,)

    Returns:
    --------
    y : numpy array (only on rank 0)
        結果ベクトル (m,)
    """
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()

    # Rank 0が行列の次元を全プロセスにブロードキャスト
    if rank == 0:
        m, n = A.shape
    else:
        m, n = None, None

    m, n = comm.bcast((m, n), root=0)

    # ベクトルxを全プロセスにブロードキャスト
    if rank == 0:
        x_global = x.copy()
    else:
        x_global = np.empty(n, dtype=np.float64)

    comm.Bcast(x_global, root=0)

    # 行列Aを行方向に分割
    rows_per_proc = m // size

    # 各プロセスの担当行数を決定
    if rank < m % size:
        local_rows = rows_per_proc + 1
        start_row = rank * (rows_per_proc + 1)
    else:
        local_rows = rows_per_proc
        start_row = (m % size) * (rows_per_proc + 1) + \
                    (rank - m % size) * rows_per_proc

    # Rank 0からローカル行列を分散
    if rank == 0:
        A_local = A[start_row:start_row + local_rows, :]
        # 他のプロセスにも送信
        for r in range(1, size):
            if r < m % size:
                r_rows = rows_per_proc + 1
                r_start = r * (rows_per_proc + 1)
            else:
                r_rows = rows_per_proc
                r_start = (m % size) * (rows_per_proc + 1) + \
                          (r - m % size) * rows_per_proc
            comm.Send(A[r_start:r_start + r_rows, :], dest=r, tag=0)
    else:
        A_local = np.empty((local_rows, n), dtype=np.float64)
        comm.Recv(A_local, source=0, tag=0)

    # ローカル行列とベクトルの積を計算
    y_local = A_local @ x_global

    # 結果をRank 0に収集
    if rank == 0:
        y = np.empty(m, dtype=np.float64)
    else:
        y = None

    # Gatherv（可変長Gather）を使う
    recvcounts = comm.gather(local_rows, root=0)

    if rank == 0:
        displacements = [0]
        for i in range(size - 1):
            displacements.append(displacements[-1] + recvcounts[i])
    else:
        displacements = None

    comm.Gatherv(y_local, [y, recvcounts, displacements, MPI.DOUBLE], root=0)

    return y

if __name__ == "__main__":
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()

    # Rank 0が行列とベクトルを生成
    if rank == 0:
        m, n = 1000, 500
        A = np.random.rand(m, n)
        x = np.random.rand(n)

        # 逐次計算（正解確認用）
        y_serial = A @ x

        print(f"Matrix size: {m} × {n}")
        print(f"Running parallel matrix-vector product...")
    else:
        A, x = None, None

    # 並列計算
    y_parallel = parallel_matvec(A, x)

    # Rank 0が結果を検証
    if rank == 0:
        error = np.linalg.norm(y_serial - y_parallel)
        print(f"Error (||y_serial - y_parallel||): {error:.2e}")
        if error < 1e-10:
            print("✓ Verification passed!")
        else:
            print("✗ Verification failed!")

# 実行: mpiexec -n 4 python matvec_mpi.py
</code></pre>

            <h3>NumPy配列の効率的な通信</h3>
            <div class="info-box">
                <strong>mpi4pyの通信方式</strong>
                <ul>
                    <li><strong>大文字メソッド（Send/Recv/Bcast等）</strong>: NumPy配列をバイナリとして直接転送（高速、pickle不要）</li>
                    <li><strong>小文字メソッド（send/recv/bcast等）</strong>: Pythonオブジェクトをpickle化して転送（低速だが汎用的）</li>
                    <li><strong>推奨</strong>: NumPy配列には大文字メソッドを使うことで、性能が10-100倍向上</li>
                </ul>
            </div>

            <h3>コード例6: 並列I/Oパターン（MPI-IO）</h3>
            <pre><code class="language-python"># parallel_io.py - mpi4pyによる並列ファイルI/O
from mpi4py import MPI
import numpy as np

def parallel_write_array(filename, local_array):
    """
    各プロセスのNumPy配列を並列に1つのファイルに書き込む

    Parameters:
    -----------
    filename : str
        出力ファイル名
    local_array : numpy array
        各プロセスが書き込むデータ
    """
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()

    # 各プロセスのデータサイズを収集
    local_size = local_array.size
    sizes = comm.gather(local_size, root=0)

    # 各プロセスの書き込み開始位置を計算
    if rank == 0:
        offsets = [0]
        for i in range(size - 1):
            offsets.append(offsets[-1] + sizes[i])
    else:
        offsets = None

    offsets = comm.bcast(offsets, root=0)
    offset = offsets[rank]

    # MPI-IOでファイルを開く（全プロセスが同時アクセス）
    fh = MPI.File.Open(comm, filename,
                       MPI.MODE_CREATE | MPI.MODE_WRONLY)

    # 各プロセスが自分の担当範囲にデータを書き込む
    fh.Write_at(offset * local_array.itemsize, local_array)

    fh.Close()

    if rank == 0:
        print(f"Parallel write completed: {filename}")

def parallel_read_array(filename, dtype=np.float64):
    """
    並列ファイルからNumPy配列を読み込む

    Parameters:
    -----------
    filename : str
        入力ファイル名
    dtype : numpy dtype
        データ型

    Returns:
    --------
    local_array : numpy array
        各プロセスが読み込んだデータ
    """
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()

    # ファイルサイズを取得
    fh = MPI.File.Open(comm, filename, MPI.MODE_RDONLY)
    file_size = fh.Get_size()
    total_elements = file_size // np.dtype(dtype).itemsize

    # 各プロセスの担当要素数を計算
    local_elements = total_elements // size
    if rank < total_elements % size:
        local_elements += 1
        offset = rank * local_elements
    else:
        offset = (total_elements % size) * (total_elements // size + 1) + \
                 (rank - total_elements % size) * (total_elements // size)

    # データを読み込む
    local_array = np.empty(local_elements, dtype=dtype)
    fh.Read_at(offset * local_array.itemsize, local_array)

    fh.Close()

    if rank == 0:
        print(f"Parallel read completed: {filename}")

    return local_array

if __name__ == "__main__":
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()

    # 各プロセスがローカルデータを生成
    local_data = np.arange(rank * 100, (rank + 1) * 100, dtype=np.float64)

    # 並列書き込み
    parallel_write_array("parallel_data.bin", local_data)

    # 並列読み込み
    read_data = parallel_read_array("parallel_data.bin")

    # 検証
    if np.allclose(local_data, read_data):
        print(f"Rank {rank}: I/O verification passed!")
    else:
        print(f"Rank {rank}: I/O verification failed!")

# 実行: mpiexec -n 4 python parallel_io.py
</code></pre>
        </section>

        <section id="performance">
            <h2>3.7 性能解析とスケーラビリティ</h2>

            <h3>性能指標の定義</h3>
            <p>
                並列プログラムの性能を評価するには、以下の指標を測定します：
            </p>

            <p style="font-size: 1.1rem; margin: 1.5rem 0;">
                \[
                \begin{align}
                \text{Speedup}(N) &= \frac{T_1}{T_N} \\
                \text{Efficiency}(N) &= \frac{\text{Speedup}(N)}{N} = \frac{T_1}{N \cdot T_N} \\
                \text{Parallel Overhead} &= N \cdot T_N - T_1
                \end{align}
                \]
            </p>

            <ul>
                <li><strong>Speedup</strong>: 1プロセッサ実行時間に対する高速化率（理想は\(N\)倍）</li>
                <li><strong>Efficiency</strong>: プロセッサ利用効率（理想は100%）</li>
                <li><strong>Parallel Overhead</strong>: 並列化により増加した総計算量（通信、同期等）</li>
            </ul>

            <h3>コード例7: スケーラビリティ測定ツール</h3>
            <pre><code class="language-python"># scaling_test.py - Strong/Weak Scalingの測定
from mpi4py import MPI
import numpy as np
import time
import json

def benchmark_matrix_multiply(n, n_iterations=10):
    """
    行列積のベンチマーク

    Parameters:
    -----------
    n : int
        行列サイズ (n × n)
    n_iterations : int
        測定反復回数

    Returns:
    --------
    elapsed_time : float
        平均実行時間（秒）
    """
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()

    # 各プロセスが担当する行数
    rows_per_proc = n // size

    # ランダム行列を生成（Rank 0のみ）
    if rank == 0:
        A = np.random.rand(n, n)
        B = np.random.rand(n, n)
    else:
        A, B = None, None

    # 行列Bを全プロセスにブロードキャスト
    if rank == 0:
        B_global = B.copy()
    else:
        B_global = np.empty((n, n), dtype=np.float64)
    comm.Bcast(B_global, root=0)

    # 行列Aを分散
    A_local = np.empty((rows_per_proc, n), dtype=np.float64)
    if rank == 0:
        for r in range(size):
            if r == 0:
                A_local[:] = A[0:rows_per_proc, :]
            else:
                comm.Send(A[r*rows_per_proc:(r+1)*rows_per_proc, :],
                         dest=r, tag=0)
    else:
        comm.Recv(A_local, source=0, tag=0)

    # ウォームアップ
    C_local = A_local @ B_global

    # ベンチマーク測定
    comm.Barrier()  # 全プロセスを同期
    start_time = time.time()

    for _ in range(n_iterations):
        C_local = A_local @ B_global

    comm.Barrier()
    elapsed_time = time.time() - start_time

    # 平均時間を計算
    avg_time = elapsed_time / n_iterations

    return avg_time

def run_strong_scaling_test(n_base=1000, max_procs=16):
    """
    Strong Scaling テスト（問題サイズ固定）
    """
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()

    # ベンチマーク実行
    elapsed = benchmark_matrix_multiply(n_base)

    # 結果を収集
    all_times = comm.gather(elapsed, root=0)

    if rank == 0:
        # 結果を保存
        result = {
            'test_type': 'strong_scaling',
            'matrix_size': n_base,
            'num_processes': size,
            'time_per_process': all_times,
            'max_time': max(all_times),
            'min_time': min(all_times),
            'avg_time': sum(all_times) / len(all_times)
        }

        # JSONファイルに出力
        filename = f'strong_scaling_np{size}_n{n_base}.json'
        with open(filename, 'w') as f:
            json.dump(result, f, indent=2)

        print(f"Strong scaling test completed:")
        print(f"  Processes: {size}")
        print(f"  Matrix size: {n_base} × {n_base}")
        print(f"  Time: {result['avg_time']:.4f} s")
        print(f"  Result saved to {filename}")

def run_weak_scaling_test(n_per_proc=500, max_procs=16):
    """
    Weak Scaling テスト（プロセスあたりの問題サイズ固定）
    """
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()

    # プロセス数に応じて全体の問題サイズを増やす
    n_total = n_per_proc * size

    # ベンチマーク実行
    elapsed = benchmark_matrix_multiply(n_total)

    # 結果を収集
    all_times = comm.gather(elapsed, root=0)

    if rank == 0:
        result = {
            'test_type': 'weak_scaling',
            'matrix_size_per_proc': n_per_proc,
            'total_matrix_size': n_total,
            'num_processes': size,
            'time_per_process': all_times,
            'max_time': max(all_times),
            'min_time': min(all_times),
            'avg_time': sum(all_times) / len(all_times)
        }

        filename = f'weak_scaling_np{size}_n{n_per_proc}.json'
        with open(filename, 'w') as f:
            json.dump(result, f, indent=2)

        print(f"Weak scaling test completed:")
        print(f"  Processes: {size}")
        print(f"  Matrix size per process: {n_per_proc} × {n_per_proc}")
        print(f"  Total matrix size: {n_total} × {n_total}")
        print(f"  Time: {result['avg_time']:.4f} s")
        print(f"  Result saved to {filename}")

if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: mpiexec -n <N> python scaling_test.py [strong|weak]")
        sys.exit(1)

    test_type = sys.argv[1]

    if test_type == "strong":
        run_strong_scaling_test(n_base=2000)
    elif test_type == "weak":
        run_weak_scaling_test(n_per_proc=1000)
    else:
        print(f"Unknown test type: {test_type}")
        print("Use 'strong' or 'weak'")

# 実行例:
# Strong Scaling: mpiexec -n 4 python scaling_test.py strong
# Weak Scaling: mpiexec -n 8 python scaling_test.py weak
</code></pre>

            <h3>プロファイリングツール</h3>
            <table>
                <thead>
                    <tr>
                        <th>ツール</th>
                        <th>機能</th>
                        <th>使用方法</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>mpiP</strong></td>
                        <td>MPI関数呼び出しのプロファイリング</td>
                        <td><code>mpicc -g my_prog.c -lmpiP -lm</code></td>
                    </tr>
                    <tr>
                        <td><strong>Scalasca</strong></td>
                        <td>並列プログラムの性能ボトルネック分析</td>
                        <td><code>scalasca -analyze mpirun -np 8 ./a.out</code></td>
                    </tr>
                    <tr>
                        <td><strong>TAU（Tuning and Analysis Utilities）</strong></td>
                        <td>詳細な性能プロファイリングとトレース</td>
                        <td><code>tau_cc.sh my_prog.c -o my_prog</code></td>
                    </tr>
                    <tr>
                        <td><strong>Intel VTune</strong></td>
                        <td>CPU/メモリ性能の詳細分析</td>
                        <td><code>vtune -collect hotspots -- mpirun ...</code></td>
                    </tr>
                </tbody>
            </table>

            <h3>コード例8: 簡易プロファイリングスクリプト</h3>
            <pre><code class="language-python"># simple_profiler.py - MPI通信時間の測定
from mpi4py import MPI
import numpy as np
import time

class MPIProfiler:
    """簡易MPIプロファイラー"""

    def __init__(self):
        self.comm = MPI.COMM_WORLD
        self.rank = self.comm.Get_rank()
        self.timings = {}

    def time_operation(self, name, func, *args, **kwargs):
        """
        操作の実行時間を測定

        Parameters:
        -----------
        name : str
            操作名
        func : callable
            測定する関数
        *args, **kwargs
            funcに渡す引数

        Returns:
        --------
        result
            funcの戻り値
        """
        start = time.time()
        result = func(*args, **kwargs)
        elapsed = time.time() - start

        if name not in self.timings:
            self.timings[name] = []
        self.timings[name].append(elapsed)

        return result

    def report(self):
        """
        プロファイリング結果を出力
        """
        # 各プロセスのタイミングを収集
        all_timings = self.comm.gather(self.timings, root=0)

        if self.rank == 0:
            print("\n" + "="*60)
            print("MPI Profiling Report")
            print("="*60)

            # 操作ごとに集計
            operation_names = set()
            for timing in all_timings:
                operation_names.update(timing.keys())

            for op_name in sorted(operation_names):
                print(f"\nOperation: {op_name}")
                print(f"{'Rank':<6} {'Calls':<8} {'Total (s)':<12} {'Avg (s)':<12} {'Min (s)':<12} {'Max (s)':<12}")
                print("-" * 60)

                for rank, timing in enumerate(all_timings):
                    if op_name in timing:
                        times = timing[op_name]
                        n_calls = len(times)
                        total_time = sum(times)
                        avg_time = total_time / n_calls
                        min_time = min(times)
                        max_time = max(times)

                        print(f"{rank:<6} {n_calls:<8} {total_time:<12.6f} {avg_time:<12.6f} {min_time:<12.6f} {max_time:<12.6f}")

def demo_profiling():
    """プロファイラーの使用例"""
    profiler = MPIProfiler()
    comm = profiler.comm
    rank = profiler.rank
    size = comm.Get_size()

    # ブロードキャストのプロファイリング
    if rank == 0:
        data = np.random.rand(1000000)
    else:
        data = np.empty(1000000, dtype=np.float64)

    profiler.time_operation("MPI_Bcast", comm.Bcast, data, root=0)

    # Point-to-point通信のプロファイリング
    if rank == 0:
        send_data = np.arange(100000, dtype=np.float64)
        profiler.time_operation("MPI_Send", comm.Send, send_data, dest=1, tag=0)
    elif rank == 1:
        recv_data = np.empty(100000, dtype=np.float64)
        profiler.time_operation("MPI_Recv", comm.Recv, recv_data, source=0, tag=0)

    # Reduceのプロファイリング
    local_sum = np.sum(np.random.rand(1000000))
    global_sum = profiler.time_operation("MPI_Reduce",
                                         lambda: comm.reduce(local_sum, op=MPI.SUM, root=0))

    # レポート出力
    profiler.report()

if __name__ == "__main__":
    demo_profiling()

# 実行: mpiexec -n 4 python simple_profiler.py
</code></pre>

            <h3>ボトルネック特定の手順</h3>
            <div class="info-box">
                <strong>性能最適化のワークフロー</strong>
                <ol>
                    <li><strong>ベースライン測定</strong>: 最適化前の性能を記録（Strong/Weak Scaling）</li>
                    <li><strong>プロファイリング</strong>: mpiP、Scalascaで通信時間、計算時間を分析</li>
                    <li><strong>ボトルネック特定</strong>: 最も時間を消費している操作を特定</li>
                    <li><strong>最適化実施</strong>:
                        <ul>
                            <li>通信ボトルネック → 通信回数削減、ノンブロッキング通信</li>
                            <li>計算ボトルネック → アルゴリズム改善、コンパイラ最適化</li>
                            <li>負荷不均衡 → 領域分割の見直し、動的負荷分散</li>
                        </ul>
                    </li>
                    <li><strong>効果検証</strong>: 再度性能測定し、改善を定量評価</li>
                    <li><strong>反復</strong>: 次のボトルネックを特定し、ステップ2に戻る</li>
                </ol>
            </div>
        </section>

        <section id="exercises">
            <h2>演習問題</h2>

            <h3>Easy（基礎確認）<span class="difficulty easy">初級</span></h3>

            <div class="exercise-box">
                <h4>Q1: Amdahlの法則の計算</h4>
                <p>プログラムの90%が並列化可能な場合、16プロセッサを使ったときの理論的高速化率を計算してください。</p>

                <details>
                    <summary>解答を見る</summary>
                    <p><strong>正解</strong>: 約8.89倍</p>
                    <p><strong>計算過程</strong>:</p>
                    <p>Amdahlの法則: \(S(N) = \frac{1}{(1 - P) + \frac{P}{N}}\)</p>
                    <ul>
                        <li>\(P = 0.90\)（並列化可能部分）</li>
                        <li>\(N = 16\)（プロセッサ数）</li>
                    </ul>
                    <p>\[
                    S(16) = \frac{1}{(1 - 0.90) + \frac{0.90}{16}} = \frac{1}{0.10 + 0.05625} = \frac{1}{0.15625} \approx 8.89
                    \]</p>
                    <p><strong>考察</strong>: わずか10%の逐次部分が、理論的高速化率を16倍から約9倍に制限しています。</p>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Q2: MPI基本関数の対応</h4>
                <p>以下の操作に対応するMPI関数を答えてください。</p>
                <ul>
                    <li>(a) 自分のランク（プロセスID）を取得する</li>
                    <li>(b) 全プロセス数を取得する</li>
                    <li>(c) 1つのプロセスから全プロセスにデータを配信する</li>
                    <li>(d) 各プロセスの値を合計して1つのプロセスに集約する</li>
                </ul>

                <details>
                    <summary>解答を見る</summary>
                    <p><strong>正解</strong>:</p>
                    <ul>
                        <li>(a) <code>MPI_Comm_rank(MPI_COMM_WORLD, &rank)</code>（C）または<code>comm.Get_rank()</code>（Python）</li>
                        <li>(b) <code>MPI_Comm_size(MPI_COMM_WORLD, &size)</code>（C）または<code>comm.Get_size()</code>（Python）</li>
                        <li>(c) <code>MPI_Bcast(&data, count, MPI_INT, root, MPI_COMM_WORLD)</code></li>
                        <li>(d) <code>MPI_Reduce(&send_data, &recv_data, count, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD)</code></li>
                    </ul>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Q3: Strong Scaling vs Weak Scaling</h4>
                <p>Strong ScalingとWeak Scalingの違いを説明し、それぞれどのような場合に測定すべきか答えてください。</p>

                <details>
                    <summary>解答を見る</summary>
                    <p><strong>正解</strong>:</p>
                    <ul>
                        <li><strong>Strong Scaling</strong>: 問題サイズを固定してプロセッサ数を増やす。「同じ問題をより速く解けるか」を評価。固定サイズの問題を高速化したい場合に測定。</li>
                        <li><strong>Weak Scaling</strong>: プロセッサあたりの問題サイズを固定して全体を大きくする。「大きな問題でも効率を維持できるか」を評価。問題サイズを拡大したい場合に測定。</li>
                    </ul>
                    <p><strong>例</strong>:</p>
                    <ul>
                        <li>Strong Scaling: 100万粒子のシミュレーションを1プロセッサで10時間 → 16プロセッサで何時間?</li>
                        <li>Weak Scaling: 1プロセッサで1万粒子/プロセッサ → 16プロセッサで16万粒子（1万粒子/プロセッサを維持）</li>
                    </ul>
                </details>
            </div>

            <h3>Medium（応用）<span class="difficulty medium">中級</span></h3>

            <div class="exercise-box">
                <h4>Q4: 並列合計の実装</h4>
                <p>mpi4pyを使って、各プロセスが10個の乱数を生成し、全プロセスの合計を計算するプログラムを書いてください。結果は全プロセスが受け取ります。</p>

                <details>
                    <summary>解答を見る</summary>
                    <p><strong>解答例</strong>:</p>
                    <pre><code class="language-python"># parallel_sum.py
from mpi4py import MPI
import numpy as np

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

# 各プロセスが10個の乱数を生成
np.random.seed(rank)  # プロセスごとに異なる乱数
local_data = np.random.rand(10)

# ローカル合計を計算
local_sum = np.sum(local_data)

print(f"Rank {rank}: local_sum = {local_sum:.4f}")

# MPI_Allreduceで全プロセスの合計を全プロセスに配信
global_sum = comm.allreduce(local_sum, op=MPI.SUM)

print(f"Rank {rank}: global_sum = {global_sum:.4f}")

# 検証（Rank 0のみ）
if rank == 0:
    print(f"\nTotal expected: {size} processes × ~5.0 = ~{size * 5.0}")
    print(f"Actual global_sum: {global_sum:.4f}")

# 実行: mpiexec -n 4 python parallel_sum.py
</code></pre>
                    <p><strong>期待される出力</strong>:</p>
                    <pre><code>Rank 0: local_sum = 5.2341
Rank 1: local_sum = 4.8763
Rank 2: local_sum = 5.1234
Rank 3: local_sum = 4.9876
Rank 0: global_sum = 20.2214
Rank 1: global_sum = 20.2214
Rank 2: global_sum = 20.2214
Rank 3: global_sum = 20.2214

Total expected: 4 processes × ~5.0 = ~20.0
Actual global_sum: 20.2214
</code></pre>
                    <p><strong>考察</strong>: <code>allreduce</code>を使うことで、<code>reduce</code> + <code>bcast</code>の2ステップを1回で実行できます。</p>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Q5: リング通信パターン</h4>
                <p>各プロセスが自分のランク番号を右隣のプロセスに送信し、左隣から受信するリング通信パターンを実装してください（最後のプロセスは最初のプロセスに送信）。</p>

                <details>
                    <summary>解答を見る</summary>
                    <p><strong>解答例</strong>:</p>
                    <pre><code class="language-python"># ring_communication.py
from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

# 送信先と受信元を決定（リング構造）
right_neighbor = (rank + 1) % size
left_neighbor = (rank - 1 + size) % size

# 送信データ（自分のランク）
send_data = rank

# ノンブロッキング送受信でデッドロック回避
send_req = comm.isend(send_data, dest=right_neighbor, tag=0)
recv_data = comm.recv(source=left_neighbor, tag=0)

# 送信完了を待機
send_req.wait()

print(f"Rank {rank}: sent {send_data} to rank {right_neighbor}, "
      f"received {recv_data} from rank {left_neighbor}")

# 実行: mpiexec -n 6 python ring_communication.py
</code></pre>
                    <p><strong>期待される出力</strong>:</p>
                    <pre><code>Rank 0: sent 0 to rank 1, received 5 from rank 5
Rank 1: sent 1 to rank 2, received 0 from rank 0
Rank 2: sent 2 to rank 3, received 1 from rank 1
Rank 3: sent 3 to rank 4, received 2 from rank 2
Rank 4: sent 4 to rank 5, received 3 from rank 3
Rank 5: sent 5 to rank 0, received 4 from rank 4
</code></pre>
                    <p><strong>重要ポイント</strong>: ノンブロッキング送信（<code>isend</code>）を使うことで、全プロセスが同時に送受信してもデッドロックが発生しません。</p>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Q6: 1D熱伝導方程式の並列化</h4>
                <p>1D熱伝導方程式を差分法で解く並列プログラムを作成してください。領域を分割し、各プロセスが境界のゴーストセルを隣接プロセスと交換します。</p>

                <details>
                    <summary>解答を見る</summary>
                    <p><strong>解答例</strong>:</p>
                    <pre><code class="language-python"># heat_1d_parallel.py
from mpi4py import MPI
import numpy as np

def heat_1d_parallel(nx_global=100, nt=1000, alpha=0.01):
    """
    1D熱伝導方程式の並列計算

    Parameters:
    -----------
    nx_global : int
        全体のグリッドポイント数
    nt : int
        時間ステップ数
    alpha : float
        熱拡散係数
    """
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()

    # 各プロセスの担当領域サイズ
    nx_local = nx_global // size

    # ゴーストセルを含む配列（左右に1セルずつ）
    u = np.zeros(nx_local + 2)
    u_new = np.zeros(nx_local + 2)

    # 初期条件: 中央に熱源
    global_mid = nx_global // 2
    local_start = rank * nx_local
    local_end = (rank + 1) * nx_local

    if local_start <= global_mid < local_end:
        local_idx = global_mid - local_start + 1  # +1はゴーストセル分
        u[local_idx] = 100.0

    # 時間発展
    for t in range(nt):
        # ゴーストセル交換
        if rank > 0:
            comm.Sendrecv(u[1], dest=rank-1, sendtag=0,
                         recvbuf=u[0:1], source=rank-1, recvtag=1)

        if rank < size - 1:
            comm.Sendrecv(u[nx_local], dest=rank+1, sendtag=1,
                         recvbuf=u[nx_local+1:nx_local+2], source=rank+1, recvtag=0)

        # 差分法で更新（実データ部分のみ）
        for i in range(1, nx_local + 1):
            u_new[i] = u[i] + alpha * (u[i-1] - 2*u[i] + u[i+1])

        # 境界条件（両端は0固定）
        if rank == 0:
            u_new[1] = 0.0
        if rank == size - 1:
            u_new[nx_local] = 0.0

        # 配列を交換
        u, u_new = u_new, u

    # 結果を収集
    local_result = u[1:nx_local+1]
    global_result = None
    if rank == 0:
        global_result = np.empty(nx_global)

    comm.Gather(local_result, global_result, root=0)

    if rank == 0:
        print(f"Simulation completed: {nt} steps")
        print(f"Final temperature at center: {global_result[nx_global//2]:.4f}")

    return global_result

if __name__ == "__main__":
    result = heat_1d_parallel(nx_global=1000, nt=5000, alpha=0.01)

# 実行: mpiexec -n 4 python heat_1d_parallel.py
</code></pre>
                    <p><strong>解説</strong>:</p>
                    <ul>
                        <li><code>Sendrecv</code>を使うことで、送信と受信を同時に実行し、デッドロックを回避</li>
                        <li>ゴーストセルにより、境界の計算に必要な隣接データを保持</li>
                        <li>各プロセスは自分の領域のみを更新し、通信は境界データのみ</li>
                    </ul>
                </details>
            </div>

            <h3>Hard（発展）<span class="difficulty hard">上級</span></h3>

            <div class="exercise-box">
                <h4>Q7: 2D領域分割とゴーストセル交換</h4>
                <p>2次元のラプラス方程式を解く並列プログラムを作成してください。領域を2D分割し、上下左右の4方向でゴーストセルを交換します。</p>

                <details>
                    <summary>解答を見る</summary>
                    <p><strong>解答例</strong>:</p>
                    <pre><code class="language-python"># laplace_2d_parallel.py - 2D領域分割によるラプラス方程式の並列計算
from mpi4py import MPI
import numpy as np
import math

def laplace_2d_parallel(nx_global=100, ny_global=100, max_iter=10000, tol=1e-6):
    """
    2Dラプラス方程式をJacobi法で並列計算

    Parameters:
    -----------
    nx_global, ny_global : int
        全体のグリッドサイズ
    max_iter : int
        最大反復回数
    tol : float
        収束判定の閾値
    """
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()

    # 2D分割のためのプロセス配置を決定
    px = int(math.sqrt(size))
    while size % px != 0:
        px -= 1
    py = size // px

    # 自分のプロセス座標
    proc_x = rank % px
    proc_y = rank // px

    # 各プロセスの担当領域サイズ
    nx_local = nx_global // px
    ny_local = ny_global // py

    # ゴーストセルを含む配列（上下左右に1セルずつ）
    u = np.zeros((ny_local + 2, nx_local + 2))
    u_new = np.zeros((ny_local + 2, nx_local + 2))

    # 境界条件: 上辺を100度に設定
    if proc_y == 0:
        u[1, 1:nx_local+1] = 100.0
        u_new[1, 1:nx_local+1] = 100.0

    # 隣接プロセスのランクを計算
    north = rank - px if proc_y > 0 else MPI.PROC_NULL
    south = rank + px if proc_y < py - 1 else MPI.PROC_NULL
    west = rank - 1 if proc_x > 0 else MPI.PROC_NULL
    east = rank + 1 if proc_x < px - 1 else MPI.PROC_NULL

    # 反復計算
    for iteration in range(max_iter):
        # ゴーストセル交換（4方向）
        # 北（上）
        comm.Sendrecv(u[1, 1:nx_local+1], dest=north, sendtag=0,
                     recvbuf=u[0, 1:nx_local+1], source=north, recvtag=1)

        # 南（下）
        comm.Sendrecv(u[ny_local, 1:nx_local+1], dest=south, sendtag=1,
                     recvbuf=u[ny_local+1, 1:nx_local+1], source=south, recvtag=0)

        # 西（左）
        comm.Sendrecv(u[1:ny_local+1, 1], dest=west, sendtag=2,
                     recvbuf=u[1:ny_local+1, 0], source=west, recvtag=3)

        # 東（右）
        comm.Sendrecv(u[1:ny_local+1, nx_local], dest=east, sendtag=3,
                     recvbuf=u[1:ny_local+1, nx_local+1], source=east, recvtag=2)

        # Jacobi法で更新
        for j in range(1, ny_local + 1):
            for i in range(1, nx_local + 1):
                u_new[j, i] = 0.25 * (u[j-1, i] + u[j+1, i] +
                                      u[j, i-1] + u[j, i+1])

        # 収束判定（局所誤差を計算）
        local_error = np.max(np.abs(u_new[1:ny_local+1, 1:nx_local+1] -
                                     u[1:ny_local+1, 1:nx_local+1]))

        # 全プロセスの最大誤差を取得
        global_error = comm.allreduce(local_error, op=MPI.MAX)

        # 配列を交換
        u, u_new = u_new, u

        # 収束判定
        if global_error < tol:
            if rank == 0:
                print(f"Converged at iteration {iteration}, error = {global_error:.2e}")
            break

    # 結果を収集（簡易版: Rank 0が全データを受け取る）
    local_result = u[1:ny_local+1, 1:nx_local+1]

    if rank == 0:
        print(f"2D Laplace equation solved:")
        print(f"  Grid: {nx_global} × {ny_global}")
        print(f"  Processes: {size} ({px} × {py})")
        print(f"  Iterations: {iteration}")

    return local_result

if __name__ == "__main__":
    result = laplace_2d_parallel(nx_global=200, ny_global=200, max_iter=10000)

# 実行: mpiexec -n 4 python laplace_2d_parallel.py
</code></pre>
                    <p><strong>解説</strong>:</p>
                    <ul>
                        <li>2D分割により、各プロセスは矩形領域を担当</li>
                        <li>4方向（上下左右）でゴーストセル交換が必要</li>
                        <li><code>MPI.PROC_NULL</code>を使って境界プロセスの処理を簡潔化</li>
                        <li><code>allreduce</code>で全プロセスの収束判定を共有</li>
                    </ul>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Q8: ハイブリッドMPI+OpenMP実装</h4>
                <p>MPI（プロセス間並列）とOpenMP（スレッド並列）を組み合わせたハイブリッド並列プログラムを作成してください。行列ベクトル積を例に、ノード間はMPI、ノード内はOpenMPで並列化します。</p>

                <details>
                    <summary>解答を見る</summary>
                    <p><strong>解答例（C言語）</strong>:</p>
                    <pre><code class="language-c">/* hybrid_mpi_openmp.c - MPI+OpenMPハイブリッド並列 */
#include &lt;mpi.h&gt;
#include &lt;omp.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

int main(int argc, char** argv) {
    int rank, size, provided;
    int n = 10000;  /* ベクトルサイズ */
    int rows_per_proc;
    double *A_local, *x, *y_local;
    double start_time, end_time;

    /* MPIをスレッドサポート付きで初期化 */
    MPI_Init_thread(&argc, &argv, MPI_THREAD_FUNNELED, &provided);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    rows_per_proc = n / size;

    /* メモリ確保 */
    A_local = (double*)malloc(rows_per_proc * n * sizeof(double));
    x = (double*)malloc(n * sizeof(double));
    y_local = (double*)malloc(rows_per_proc * sizeof(double));

    /* データ初期化（簡略化のためランダム） */
    #pragma omp parallel for
    for (int i = 0; i < rows_per_proc * n; i++) {
        A_local[i] = (double)rand() / RAND_MAX;
    }

    #pragma omp parallel for
    for (int i = 0; i < n; i++) {
        x[i] = (double)rand() / RAND_MAX;
    }

    /* ベクトルxを全プロセスにブロードキャスト */
    MPI_Bcast(x, n, MPI_DOUBLE, 0, MPI_COMM_WORLD);

    /* 計測開始 */
    MPI_Barrier(MPI_COMM_WORLD);
    start_time = MPI_Wtime();

    /* 行列ベクトル積（OpenMPで並列化） */
    #pragma omp parallel for
    for (int i = 0; i < rows_per_proc; i++) {
        y_local[i] = 0.0;
        for (int j = 0; j < n; j++) {
            y_local[i] += A_local[i * n + j] * x[j];
        }
    }

    /* 計測終了 */
    MPI_Barrier(MPI_COMM_WORLD);
    end_time = MPI_Wtime();

    if (rank == 0) {
        int num_threads;
        #pragma omp parallel
        {
            #pragma omp master
            num_threads = omp_get_num_threads();
        }

        printf("Hybrid MPI+OpenMP Matrix-Vector Multiply:\n");
        printf("  MPI processes: %d\n", size);
        printf("  OpenMP threads per process: %d\n", num_threads);
        printf("  Total parallelism: %d\n", size * num_threads);
        printf("  Matrix size: %d × %d\n", n, n);
        printf("  Execution time: %.6f seconds\n", end_time - start_time);
    }

    /* メモリ解放 */
    free(A_local);
    free(x);
    free(y_local);

    MPI_Finalize();
    return 0;
}

/* コンパイルと実行:
 * mpicc -fopenmp -o hybrid hybrid_mpi_openmp.c
 * export OMP_NUM_THREADS=4
 * mpirun -np 4 ./hybrid
 *
 * → 4プロセス × 4スレッド = 16並列で実行
 */
</code></pre>
                    <p><strong>Pythonによる簡易版</strong>:</p>
                    <pre><code class="language-python"># hybrid_mpi_numba.py - MPI + Numba並列化
from mpi4py import MPI
import numpy as np
from numba import njit, prange

@njit(parallel=True)
def matvec_numba(A, x):
    """Numbaによるマルチスレッド行列ベクトル積"""
    m, n = A.shape
    y = np.zeros(m)
    for i in prange(m):  # prange = parallel range
        for j in range(n):
            y[i] += A[i, j] * x[j]
    return y

def hybrid_matvec():
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()

    n = 5000
    rows_per_proc = n // size

    # 各プロセスがローカル行列を生成
    A_local = np.random.rand(rows_per_proc, n)

    # ベクトルxを全プロセスで共有
    if rank == 0:
        x = np.random.rand(n)
    else:
        x = np.empty(n)
    comm.Bcast(x, root=0)

    # 計測開始
    comm.Barrier()
    start_time = MPI.Wtime()

    # Numbaでマルチスレッド計算
    y_local = matvec_numba(A_local, x)

    # 計測終了
    comm.Barrier()
    elapsed = MPI.Wtime() - start_time

    if rank == 0:
        print(f"Hybrid MPI + Numba:")
        print(f"  MPI processes: {size}")
        print(f"  Numba threads: auto")
        print(f"  Matrix size: {n} × {n}")
        print(f"  Time: {elapsed:.6f} s")

if __name__ == "__main__":
    hybrid_matvec()

# 実行: NUMBA_NUM_THREADS=4 mpiexec -n 4 python hybrid_mpi_numba.py
</code></pre>
                    <p><strong>ハイブリッド並列の利点</strong>:</p>
                    <ul>
                        <li><strong>通信削減</strong>: ノード内はスレッドで共有メモリアクセス（MPIより高速）</li>
                        <li><strong>スケーラビリティ向上</strong>: メニーコアノード（64コア以上）で効率的</li>
                        <li><strong>メモリ効率</strong>: スレッドはメモリを共有するため、プロセスより省メモリ</li>
                    </ul>
                </details>
            </div>
        </section>

        <section id="references">
            <h2>参考文献</h2>

            <ol>
                <li>
                    Gropp, W., Lusk, E., Skjellum, A. (2014).
                    <em>Using MPI: Portable Parallel Programming with the Message-Passing Interface (3rd Edition)</em>.
                    MIT Press, pp. 15-120, 180-245.
                </li>
                <li>
                    Pacheco, P. (1997).
                    <em>Parallel Programming with MPI</em>.
                    Morgan Kaufmann, pp. 30-95, 150-210.
                </li>
                <li>
                    Sterling, T., Anderson, M., Brodowicz, M. (2018).
                    <em>High Performance Computing: Modern Systems and Practices</em>.
                    Morgan Kaufmann, pp. 280-340.
                </li>
                <li>
                    Dalcin, L., Paz, R., Kler, P., Cosimo, A. (2021).
                    <em>mpi4py: MPI for Python Documentation</em>.
                    <a href="https://mpi4py.readthedocs.io/" target="_blank">https://mpi4py.readthedocs.io/</a>, pp. 10-85.
                </li>
                <li>
                    Eijkhout, V. (2020).
                    <em>Introduction to High Performance Scientific Computing</em>.
                    Lulu.com, pp. 200-265.
                </li>
                <li>
                    Thakur, R., Rabenseifner, R., Gropp, W. (2005).
                    "Optimization of Collective Communication Operations in MPICH."
                    <em>International Journal of High Performance Computing Applications</em>, 19(1), pp. 49-66.
                </li>
                <li>
                    Hoefler, T., Lumsdaine, A. (2008).
                    "Message Progression in Parallel Computing - To Thread or not to Thread?"
                    <em>IEEE International Conference on Cluster Computing</em>, pp. 213-222.
                </li>
            </ol>
        </section>

        <section id="next-steps">
            <h2>次のステップ</h2>

            <p>
                本章では、MPIによる分散メモリ並列計算の理論と実践を学びました。次章では、<strong>共有メモリ並列計算（OpenMP）とハイブリッド並列化</strong>を学び、ノード内並列化との組み合わせ方を習得します。
            </p>

            <div class="info-box">
                <strong>第4章の予習</strong>
                <p>次章では、OpenMPによるスレッド並列化、ハイブリッドMPI+OpenMP、GPUコンピューティングを学びます。以下を事前に準備しておくとスムーズです：</p>
                <ul>
                    <li>マルチスレッドプログラミングの基本理解</li>
                    <li>HPCクラスタ上でのOpenMP環境の確認（<code>module avail openmp</code>）</li>
                    <li>本章で学んだMPI基礎の復習</li>
                </ul>
            </div>

            <div style="text-align: center; margin: 2rem 0;">
                <a href="chapter-2.html" style="margin-right: 1rem; padding: 0.75rem 1.5rem; background: #34495e; color: white; border-radius: 4px; text-decoration: none;">← 第2章に戻る</a>
                <a href="index.html" style="margin-right: 1rem; padding: 0.75rem 1.5rem; background: #34495e; color: white; border-radius: 4px; text-decoration: none;">シリーズトップ</a>
                <a href="chapter-4.html" style="padding: 0.75rem 1.5rem; background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%); color: white; border-radius: 4px; text-decoration: none;">第4章へ進む →</a>
            </div>
        </section>
    </main>

    <footer>
        <p><strong>CT Dojo - 計算工学道場</strong></p>
        <p>HPCクラスタ入門シリーズ 第3章</p>
        <p><a href="https://github.com/your-repo" target="_blank">GitHub</a> | <a href="mailto:contact@example.com">お問い合わせ</a></p>
        <p>&copy; 2025 CT Knowledge Hub. Licensed under CC BY 4.0.</p>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-c.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
</body>
</html>
