<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬2ç« : CUDAåŸºç¤ã¨ãƒ¡ãƒ¢ãƒªéšå±¤ - GPUä¸¦åˆ—è¨ˆç®—ã®ç†è«–ã¨å®Ÿè£… - GPUä¸¦åˆ—è¨ˆç®—å…¥é–€ - CT Dojo</title>
    <meta name="description" content="CUDAãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã€ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ»ãƒ–ãƒ­ãƒƒã‚¯ãƒ»ã‚°ãƒªãƒƒãƒ‰æ§‹é€ ã€GPUãƒ¡ãƒ¢ãƒªéšå±¤ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ»å…±æœ‰ãƒ»ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ»å®šæ•°ãƒ»ãƒ†ã‚¯ã‚¹ãƒãƒ£ãƒ¡ãƒ¢ãƒªï¼‰ã€ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–æŠ€è¡“ã‚’å­¦ã¶ã€‚">

    <!-- Prism.js for code highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">

    <!-- MathJax for mathematical expressions -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>

    <style>
        :root {
            --accent-green: #11998e;
            --accent-lime: #38ef7d;
            --primary-dark: #2c3e50;
            --secondary-dark: #34495e;
            --text-dark: #2c3e50;
            --text-light: #7f8c8d;
            --bg-light: #ecf0f1;
            --white: #ffffff;
            --code-bg: #2d2d2d;
            --border-light: #bdc3c7;
            --success: #27ae60;
            --warning: #f39c12;
            --danger: #e74c3c;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.8;
            color: var(--text-dark);
            background: var(--bg-light);
        }

        header {
            background: linear-gradient(135deg, var(--accent-green) 0%, var(--accent-lime) 100%);
            color: white;
            padding: 3rem 1.5rem;
            text-align: center;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        header h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
        }

        header p {
            font-size: 1.1rem;
            opacity: 0.95;
        }

        nav {
            background: var(--white);
            padding: 1rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            position: sticky;
            top: 0;
            z-index: 100;
        }

        nav ul {
            list-style: none;
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 0.5rem;
        }

        nav a {
            text-decoration: none;
            color: var(--text-dark);
            padding: 0.5rem 1rem;
            border-radius: 4px;
            transition: all 0.3s;
            font-weight: 500;
        }

        nav a:hover {
            background: linear-gradient(135deg, var(--accent-green) 0%, var(--accent-lime) 100%);
            color: white;
        }

        main {
            max-width: 900px;
            margin: 2rem auto;
            padding: 0 1.5rem;
        }

        section {
            background: var(--white);
            padding: 2rem;
            margin-bottom: 2rem;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        h2 {
            color: var(--primary-dark);
            font-size: 1.8rem;
            margin-bottom: 1.5rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid;
            border-image: linear-gradient(90deg, var(--accent-green), var(--accent-lime)) 1;
        }

        h3 {
            color: var(--secondary-dark);
            font-size: 1.4rem;
            margin: 2rem 0 1rem;
        }

        h4 {
            color: var(--secondary-dark);
            font-size: 1.2rem;
            margin: 1.5rem 0 1rem;
        }

        p {
            margin-bottom: 1rem;
            line-height: 1.8;
        }

        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        code {
            background: #f8f9fa;
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: "Consolas", "Monaco", monospace;
            font-size: 0.9em;
            color: #e74c3c;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1.5rem 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.15);
        }

        pre code {
            background: none;
            color: #f8f8f2;
            padding: 0;
        }

        .info-box {
            background: linear-gradient(135deg, rgba(17, 153, 142, 0.1) 0%, rgba(56, 239, 125, 0.1) 100%);
            border-left: 4px solid var(--accent-green);
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }

        .info-box strong {
            color: var(--accent-green);
            display: block;
            margin-bottom: 0.5rem;
            font-size: 1.1rem;
        }

        .warning-box {
            background: rgba(243, 156, 18, 0.1);
            border-left: 4px solid var(--warning);
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }

        .warning-box strong {
            color: var(--warning);
            display: block;
            margin-bottom: 0.5rem;
            font-size: 1.1rem;
        }

        .exercise-box {
            background: rgba(39, 174, 96, 0.05);
            border: 2px solid var(--success);
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 8px;
        }

        .exercise-box h4 {
            color: var(--success);
            margin-top: 0;
        }

        .difficulty {
            display: inline-block;
            padding: 0.25rem 0.75rem;
            border-radius: 12px;
            font-size: 0.85rem;
            font-weight: 600;
            margin-left: 0.5rem;
        }

        .difficulty.easy {
            background: #d4edda;
            color: #155724;
        }

        .difficulty.medium {
            background: #fff3cd;
            color: #856404;
        }

        .difficulty.hard {
            background: #f8d7da;
            color: #721c24;
        }

        .mermaid {
            background: white;
            padding: 2rem;
            border-radius: 8px;
            margin: 2rem 0;
            text-align: center;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            background: white;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        th, td {
            padding: 1rem;
            text-align: left;
            border-bottom: 1px solid var(--border-light);
        }

        th {
            background: linear-gradient(135deg, var(--accent-green) 0%, var(--accent-lime) 100%);
            color: white;
            font-weight: 600;
        }

        tr:hover {
            background: rgba(17, 153, 142, 0.05);
        }

        footer {
            background: var(--primary-dark);
            color: white;
            text-align: center;
            padding: 2rem;
            margin-top: 4rem;
        }

        footer a {
            color: var(--accent-green);
            text-decoration: none;
        }

        footer a:hover {
            color: var(--accent-lime);
        }

        @media (max-width: 768px) {
            header h1 {
                font-size: 1.5rem;
            }

            nav ul {
                flex-direction: column;
                align-items: center;
            }

            section {
                padding: 1.5rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.85rem;
            }
        }



        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #11998e;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #0e7c74;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/AI-Knowledge-Notes/knowledge/jp/index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a><span class="breadcrumb-separator">â€º</span><a href="/AI-Knowledge-Notes/knowledge/jp/CT/index.html">è¨ˆç®—å·¥å­¦</a><span class="breadcrumb-separator">â€º</span><a href="/AI-Knowledge-Notes/knowledge/jp/CT/gpu-computing-introduction/index.html">GPU Computing</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 2</span>
        </div>
    </nav>

        <header>
        <h1>ç¬¬2ç« : CUDAåŸºç¤ã¨ãƒ¡ãƒ¢ãƒªéšå±¤ - GPUä¸¦åˆ—è¨ˆç®—ã®ç†è«–ã¨å®Ÿè£…</h1>
        <p>CUDAãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚¹ãƒ¬ãƒƒãƒ‰éšå±¤ãƒ»GPUãƒ¡ãƒ¢ãƒªéšå±¤ãƒ»ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹æœ€é©åŒ–</p>
    </header>

    <nav>
        <ul>
            <li><a href="index.html">ãƒˆãƒƒãƒ—</a></li>
            <li><a href="#intro">æœ¬ç« ã®æ¦‚è¦</a></li>
            <li><a href="#cuda-programming-model">CUDAãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«</a></li>
            <li><a href="#memory-hierarchy">ãƒ¡ãƒ¢ãƒªéšå±¤</a></li>
            <li><a href="#memory-optimization">ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–</a></li>
            <li><a href="#learning-objectives">å­¦ç¿’ç›®æ¨™</a></li>
            <li><a href="#exercises">æ¼”ç¿’å•é¡Œ</a></li>
            <li><a href="#references">å‚è€ƒæ–‡çŒ®</a></li>
            <li><a href="chapter-1.html">â† å‰ã®ç« </a></li>
            <li><a href="chapter-3.html">æ¬¡ã®ç«  â†’</a></li>
        </ul>
    </nav>

    <main>
        <section id="intro">
            <h2>2.1 æœ¬ç« ã®æ¦‚è¦</h2>

            <p>
                CUDAï¼ˆCompute Unified Device Architectureï¼‰ã¯ã€NVIDIA GPUã®ä¸¦åˆ—è¨ˆç®—èƒ½åŠ›ã‚’ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‹ã‚‰ç›´æ¥åˆ¶å¾¡ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã™ã€‚æœ¬ç« ã§ã¯ã€CUDAã®ã‚¹ãƒ¬ãƒƒãƒ‰éšå±¤æ§‹é€ ã€å®Ÿè¡Œãƒ¢ãƒ‡ãƒ«ã€ãƒ¡ãƒ¢ãƒªéšå±¤ã‚’å­¦ã³ã€Pythonãƒ™ãƒ¼ã‚¹ã®CUDAãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ï¼ˆCuPyã€Numbaï¼‰ã‚’é€šã˜ã¦ã€å®Ÿè·µçš„ãªä¸¦åˆ—è¨ˆç®—æŠ€è¡“ã‚’ç¿’å¾—ã—ã¾ã™ã€‚
            </p>

            <div class="info-box">
                <strong>æœ¬ç« ã®å­¦ç¿’ç›®æ¨™</strong>
                <ul>
                    <li><strong>ãƒ¬ãƒ™ãƒ«1ï¼ˆåŸºæœ¬ç†è§£ï¼‰</strong>: CUDAã®ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ»ãƒ–ãƒ­ãƒƒã‚¯ãƒ»ã‚°ãƒªãƒƒãƒ‰éšå±¤ã€ãƒ¡ãƒ¢ãƒªã‚¿ã‚¤ãƒ—ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ã€å…±æœ‰ã€ãƒ¬ã‚¸ã‚¹ã‚¿ã€å®šæ•°ã€ãƒ†ã‚¯ã‚¹ãƒãƒ£ï¼‰ã€SIMTå®Ÿè¡Œãƒ¢ãƒ‡ãƒ«ã‚’èª¬æ˜ã§ãã‚‹</li>
                    <li><strong>ãƒ¬ãƒ™ãƒ«2ï¼ˆå®Ÿè·µã‚¹ã‚­ãƒ«ï¼‰</strong>: CuPy/Numbaã§CUDAã‚«ãƒ¼ãƒãƒ«ã‚’è¨˜è¿°ã—ã€å…±æœ‰ãƒ¡ãƒ¢ãƒªã‚’æ´»ç”¨ã§ãã€ã‚³ã‚¢ãƒ¬ã‚¹ãƒ‰ãƒ»ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã‚’å®Ÿè£…ã§ãã€ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨ˆç®—ãŒã§ãã‚‹</li>
                    <li><strong>ãƒ¬ãƒ™ãƒ«3ï¼ˆå¿œç”¨åŠ›ï¼‰</strong>: ä¸¦åˆ—ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è¨­è¨ˆã—æœ€é©åŒ–ã§ãã€ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æã—ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®šã§ãã€ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã§æ€§èƒ½ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒã§ãã‚‹</li>
                </ul>
            </div>

            <div class="warning-box">
                <strong>å‰æçŸ¥è­˜</strong>
                <p>æœ¬ç« ã§ã¯ã€ç¬¬1ç« ã®GPUã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£çŸ¥è­˜ã€PythonåŸºç¤ï¼ˆNumPyï¼‰ã€Cè¨€èªåŸºæœ¬æ§‹æ–‡ï¼ˆã‚«ãƒ¼ãƒãƒ«ã‚³ãƒ¼ãƒ‰ç†è§£ã®ãŸã‚ï¼‰ã‚’å‰æã¨ã—ã¾ã™ã€‚</p>
            </div>
        </section>

        <section id="cuda-programming-model">
            <h2>2.2 CUDAãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«</h2>

            <h3>ãƒ›ã‚¹ãƒˆã¨ãƒ‡ãƒã‚¤ã‚¹ã®æ¦‚å¿µ</h3>
            <p>
                CUDAãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã§ã¯ã€<strong>ãƒ›ã‚¹ãƒˆï¼ˆHostï¼‰</strong>ã¨<strong>ãƒ‡ãƒã‚¤ã‚¹ï¼ˆDeviceï¼‰</strong>ã¨ã„ã†2ã¤ã®å®Ÿè¡Œç©ºé–“ã‚’åŒºåˆ¥ã—ã¾ã™ã€‚ãƒ›ã‚¹ãƒˆã¯CPUã¨ãã®ãƒ¡ãƒ¢ãƒªã‚’æŒ‡ã—ã€ãƒ‡ãƒã‚¤ã‚¹ã¯GPUã¨ãã®ãƒ¡ãƒ¢ãƒªã‚’æŒ‡ã—ã¾ã™ã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¯ãƒ›ã‚¹ãƒˆã§èµ·å‹•ã—ã€ä¸¦åˆ—è¨ˆç®—ãŒå¿…è¦ãªéƒ¨åˆ†ã‚’<strong>ã‚«ãƒ¼ãƒãƒ«ï¼ˆKernelï¼‰</strong>ã¨ã—ã¦ãƒ‡ãƒã‚¤ã‚¹ã§å®Ÿè¡Œã—ã¾ã™ã€‚
            </p>

            <h3>ã‚¹ãƒ¬ãƒƒãƒ‰éšå±¤æ§‹é€ </h3>
            <p>
                CUDAã®ã‚¹ãƒ¬ãƒƒãƒ‰éšå±¤ã¯ã€<strong>ã‚¹ãƒ¬ãƒƒãƒ‰ï¼ˆThreadï¼‰â†’ ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆBlockï¼‰â†’ ã‚°ãƒªãƒƒãƒ‰ï¼ˆGridï¼‰</strong>ã¨ã„ã†3å±¤æ§‹é€ ã§ã™ã€‚å„ã‚¹ãƒ¬ãƒƒãƒ‰ã¯ç‹¬ç«‹ã—ãŸå®Ÿè¡Œå˜ä½ã§ã€è¤‡æ•°ã®ã‚¹ãƒ¬ãƒƒãƒ‰ãŒ<strong>ãƒ–ãƒ­ãƒƒã‚¯</strong>ã«ã¾ã¨ã¾ã‚Šã€è¤‡æ•°ã®ãƒ–ãƒ­ãƒƒã‚¯ãŒ<strong>ã‚°ãƒªãƒƒãƒ‰</strong>ã‚’å½¢æˆã—ã¾ã™ã€‚ã“ã®éšå±¤æ§‹é€ ã«ã‚ˆã‚Šã€æ•°ä¸‡ã€œæ•°ç™¾ä¸‡ã‚¹ãƒ¬ãƒƒãƒ‰ã®ä¸¦åˆ—å®Ÿè¡Œã‚’åŠ¹ç‡çš„ã«ç®¡ç†ã§ãã¾ã™ã€‚
            </p>

            <h3>SIMTï¼ˆSingle Instruction, Multiple Threadsï¼‰å®Ÿè¡Œ</h3>
            <p>
                CUDAã¯<strong>SIMTï¼ˆSingle Instruction, Multiple Threadsï¼‰</strong>ãƒ¢ãƒ‡ãƒ«ã§å‹•ä½œã—ã¾ã™ã€‚32å€‹ã®ã‚¹ãƒ¬ãƒƒãƒ‰ãŒ<strong>ãƒ¯ãƒ¼ãƒ—ï¼ˆWarpï¼‰</strong>ã‚’å½¢æˆã—ã€ãƒ¯ãƒ¼ãƒ—å†…ã®å…¨ã‚¹ãƒ¬ãƒƒãƒ‰ã¯åŒã˜å‘½ä»¤ã‚’åŒæ™‚å®Ÿè¡Œã—ã¾ã™ã€‚ã“ã®ç‰¹æ€§ã«ã‚ˆã‚Šã€åˆ†å²ï¼ˆifæ–‡ï¼‰ã§ãƒ¯ãƒ¼ãƒ—å†…ã®ã‚¹ãƒ¬ãƒƒãƒ‰ãŒç•°ãªã‚‹ãƒ‘ã‚¹ã‚’å–ã‚‹ã¨<strong>warp divergenceï¼ˆãƒ¯ãƒ¼ãƒ—åˆ†å²ï¼‰</strong>ãŒç™ºç”Ÿã—ã€æ€§èƒ½ãŒä½ä¸‹ã—ã¾ã™ã€‚
            </p>

            <div class="mermaid">
flowchart TB
    subgraph Grid ["Gridï¼ˆã‚°ãƒªãƒƒãƒ‰ï¼‰"]
        subgraph B1 ["Block (0,0)"]
            T1[Thread<br/>0,0,0]
            T2[Thread<br/>1,0,0]
            T3[Thread<br/>...,0,0]
            T4[Thread<br/>N,0,0]
        end
        subgraph B2 ["Block (1,0)"]
            T5[Thread<br/>0,1,0]
            T6[Thread<br/>1,1,0]
            T7[Thread<br/>...,1,0]
            T8[Thread<br/>N,1,0]
        end
        subgraph B3 ["Block (0,1)"]
            T9[Thread<br/>0,0,1]
            T10[Thread<br/>1,0,1]
            T11[Thread<br/>...,0,1]
            T12[Thread<br/>N,0,1]
        end
        subgraph B4 ["... Block (M,N)"]
            T13[Threads...]
        end
    end

    style Grid fill:#e8f4f8
    style B1 fill:#d1e7dd
    style B2 fill:#d1e7dd
    style B3 fill:#d1e7dd
    style B4 fill:#d1e7dd
            </div>

            <h3>ãƒ–ãƒ­ãƒƒã‚¯ãƒ»ã‚°ãƒªãƒƒãƒ‰ã®æ¬¡å…ƒè¨­å®š</h3>
            <table>
                <thead>
                    <tr>
                        <th>éšå±¤</th>
                        <th>æ¬¡å…ƒè¨­å®š</th>
                        <th>æœ€å¤§ã‚µã‚¤ã‚º</th>
                        <th>ç”¨é€”ä¾‹</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Gridï¼ˆã‚°ãƒªãƒƒãƒ‰ï¼‰</td>
                        <td>1D, 2D, 3D</td>
                        <td>2^31-1 (1D), 65535 (2D/3Då„æ¬¡å…ƒ)</td>
                        <td>ç”»åƒå…¨ä½“ã€å¤§è¦æ¨¡è¡Œåˆ—</td>
                    </tr>
                    <tr>
                        <td>Blockï¼ˆãƒ–ãƒ­ãƒƒã‚¯ï¼‰</td>
                        <td>1D, 2D, 3D</td>
                        <td>1024ã‚¹ãƒ¬ãƒƒãƒ‰/ãƒ–ãƒ­ãƒƒã‚¯</td>
                        <td>ç”»åƒã‚¿ã‚¤ãƒ«ã€è¡Œåˆ—ãƒ–ãƒ­ãƒƒã‚¯</td>
                    </tr>
                    <tr>
                        <td>Warpï¼ˆãƒ¯ãƒ¼ãƒ—ï¼‰</td>
                        <td>å›ºå®šï¼ˆ32ã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰</td>
                        <td>32ã‚¹ãƒ¬ãƒƒãƒ‰</td>
                        <td>SIMTå®Ÿè¡Œã®æœ€å°å˜ä½</td>
                    </tr>
                    <tr>
                        <td>Threadï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰</td>
                        <td>ãƒ–ãƒ­ãƒƒã‚¯å†…ã®ä½ç½®</td>
                        <td>ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã«ä¾å­˜</td>
                        <td>1ãƒ”ã‚¯ã‚»ãƒ«ã€1è¡Œåˆ—è¦ç´ </td>
                    </tr>
                </tbody>
            </table>

            <h3>çµ„ã¿è¾¼ã¿å¤‰æ•°ã§ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨ˆç®—</h3>
            <p>
                CUDAã‚«ãƒ¼ãƒãƒ«å†…ã§ã¯ã€ä»¥ä¸‹ã®çµ„ã¿è¾¼ã¿å¤‰æ•°ã‚’ä½¿ã£ã¦ã‚¹ãƒ¬ãƒƒãƒ‰ã®ä¸€æ„ãªIDã‚’è¨ˆç®—ã—ã¾ã™:
            </p>
            <ul>
                <li><code>threadIdx.x</code>, <code>threadIdx.y</code>, <code>threadIdx.z</code> - ãƒ–ãƒ­ãƒƒã‚¯å†…ã®ã‚¹ãƒ¬ãƒƒãƒ‰ä½ç½®</li>
                <li><code>blockIdx.x</code>, <code>blockIdx.y</code>, <code>blockIdx.z</code> - ã‚°ãƒªãƒƒãƒ‰å†…ã®ãƒ–ãƒ­ãƒƒã‚¯ä½ç½®</li>
                <li><code>blockDim.x</code>, <code>blockDim.y</code>, <code>blockDim.z</code> - ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚µã‚¤ã‚º</li>
                <li><code>gridDim.x</code>, <code>gridDim.y</code>, <code>gridDim.z</code> - ã‚°ãƒªãƒƒãƒ‰ã®ã‚µã‚¤ã‚º</li>
            </ul>
            <p>
                1Dã‚°ãƒªãƒƒãƒ‰ã§ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨ˆç®—ã®å…¸å‹ãƒ‘ã‚¿ãƒ¼ãƒ³:
            </p>
            <pre><code class="language-c">int idx = blockIdx.x * blockDim.x + threadIdx.x;</code></pre>

            <h3>ã‚³ãƒ¼ãƒ‰ä¾‹1: åŸºæœ¬çš„ãªãƒ™ã‚¯ãƒˆãƒ«åŠ ç®—ã‚«ãƒ¼ãƒãƒ«ï¼ˆCuPy RawKernelï¼‰</h3>
            <pre><code class="language-python">import cupy as cp
import numpy as np

# CUDAã‚«ãƒ¼ãƒãƒ«ã‚³ãƒ¼ãƒ‰ã®å®šç¾©ï¼ˆCè¨€èªï¼‰
vector_add_kernel = cp.RawKernel(r'''
extern "C" __global__
void vector_add(const float* a, const float* b, float* c, int n) {
    // ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è¨ˆç®—
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    // å¢ƒç•Œãƒã‚§ãƒƒã‚¯ï¼ˆé…åˆ—ã‚µã‚¤ã‚ºã‚’è¶…ãˆãªã„ã‚ˆã†ã«ï¼‰
    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}
''', 'vector_add')

# ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
n = 1000000  # 100ä¸‡è¦ç´ 
a = cp.random.rand(n, dtype=cp.float32)
b = cp.random.rand(n, dtype=cp.float32)
c = cp.zeros(n, dtype=cp.float32)

# ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã®è¨­å®š
threads_per_block = 256  # 1ãƒ–ãƒ­ãƒƒã‚¯ã‚ãŸã‚Š256ã‚¹ãƒ¬ãƒƒãƒ‰
blocks = (n + threads_per_block - 1) // threads_per_block  # å¿…è¦ãªãƒ–ãƒ­ãƒƒã‚¯æ•°

print(f"é…åˆ—ã‚µã‚¤ã‚º: {n:,} è¦ç´ ")
print(f"ãƒ–ãƒ­ãƒƒã‚¯æ•°: {blocks:,} ãƒ–ãƒ­ãƒƒã‚¯")
print(f"ãƒ–ãƒ­ãƒƒã‚¯ã‚ãŸã‚Šã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {threads_per_block} ã‚¹ãƒ¬ãƒƒãƒ‰")
print(f"åˆè¨ˆã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {blocks * threads_per_block:,} ã‚¹ãƒ¬ãƒƒãƒ‰")

# ã‚«ãƒ¼ãƒãƒ«ã®èµ·å‹•
vector_add_kernel(
    (blocks,),              # ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚ºï¼ˆãƒ–ãƒ­ãƒƒã‚¯æ•°ï¼‰
    (threads_per_block,),   # ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰æ•°ï¼‰
    (a, b, c, n)            # ã‚«ãƒ¼ãƒãƒ«å¼•æ•°
)

# GPUè¨ˆç®—ã®å®Œäº†ã‚’å¾…ã¤
cp.cuda.Stream.null.synchronize()

# çµæœã®æ¤œè¨¼ï¼ˆCPUã§è¨ˆç®—ã—ãŸçµæœã¨æ¯”è¼ƒï¼‰
expected = a + b  # CuPyã®ãƒ“ãƒ«ãƒˆã‚¤ãƒ³æ¼”ç®—
error = cp.abs(c - expected).max()
print(f"\næœ€å¤§èª¤å·®: {error:.2e}")
print(f"çµæœã®ä¸€è‡´: {'âœ“' if error < 1e-6 else 'âœ—'}")

# å‡ºåŠ›ä¾‹:
# é…åˆ—ã‚µã‚¤ã‚º: 1,000,000 è¦ç´ 
# ãƒ–ãƒ­ãƒƒã‚¯æ•°: 3,907 ãƒ–ãƒ­ãƒƒã‚¯
# ãƒ–ãƒ­ãƒƒã‚¯ã‚ãŸã‚Šã‚¹ãƒ¬ãƒƒãƒ‰æ•°: 256 ã‚¹ãƒ¬ãƒƒãƒ‰
# åˆè¨ˆã‚¹ãƒ¬ãƒƒãƒ‰æ•°: 1,000,192 ã‚¹ãƒ¬ãƒƒãƒ‰
#
# æœ€å¤§èª¤å·®: 1.19e-07
# çµæœã®ä¸€è‡´: âœ“
</code></pre>

            <div class="info-box">
                <strong>ğŸ’¡ ã‚³ãƒ¼ãƒ‰è§£èª¬</strong>
                <p>
                    ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã€CUDAã®åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚<strong>ã‚«ãƒ¼ãƒãƒ«</strong>ã¯Cè¨€èªã§è¨˜è¿°ã•ã‚Œã€<code>__global__</code>ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ‡ãƒã‚¤ã‚¹ï¼ˆGPUï¼‰ä¸Šã§å®Ÿè¡Œã•ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚å„ã‚¹ãƒ¬ãƒƒãƒ‰ã¯<code>idx = blockIdx.x * blockDim.x + threadIdx.x</code>ã§è‡ªèº«ã®æ‹…å½“ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—ã—ã€1è¦ç´ ã®åŠ ç®—ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚100ä¸‡è¦ç´ ã‚’ç´„3,900ãƒ–ãƒ­ãƒƒã‚¯Ã—256ã‚¹ãƒ¬ãƒƒãƒ‰ã§ä¸¦åˆ—å‡¦ç†ã—ã¾ã™ã€‚
                </p>
            </div>
        </section>

        <section id="memory-hierarchy">
            <h2>2.3 GPUãƒ¡ãƒ¢ãƒªéšå±¤</h2>

            <h3>ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªï¼ˆGlobal Memoryï¼‰</h3>
            <p>
                <strong>ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒª</strong>ã¯ã€GPUã®ä¸»è¨˜æ†¶è£…ç½®ã§ã€æœ€å¤§å®¹é‡ï¼ˆ16-80GBï¼‰ã‚’æŒã¡ã¾ã™ãŒã€ã‚¢ã‚¯ã‚»ã‚¹ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒæœ€ã‚‚é…ã„ï¼ˆ200-800ã‚µã‚¤ã‚¯ãƒ«ï¼‰ãƒ¡ãƒ¢ãƒªã§ã™ã€‚å…¨ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ»å…¨ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã§ã€ãƒ‡ãƒã‚¤ã‚¹-ãƒ›ã‚¹ãƒˆé–“ã®ãƒ‡ãƒ¼ã‚¿è»¢é€ã«ã‚‚ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚é«˜ã„ãƒãƒ³ãƒ‰å¹…ï¼ˆ500-2000 GB/sï¼‰ã‚’æŒã¡ã¾ã™ãŒã€åŠ¹ç‡çš„ãªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚³ã‚¢ãƒ¬ã‚¹ãƒ‰ãƒ»ã‚¢ã‚¯ã‚»ã‚¹ï¼‰ãŒæ€§èƒ½å‘ä¸Šã®éµã§ã™ã€‚
            </p>

            <h3>å…±æœ‰ãƒ¡ãƒ¢ãƒªï¼ˆShared Memoryï¼‰</h3>
            <p>
                <strong>å…±æœ‰ãƒ¡ãƒ¢ãƒª</strong>ã¯ã€ãƒ–ãƒ­ãƒƒã‚¯å†…ã®å…¨ã‚¹ãƒ¬ãƒƒãƒ‰ã§å…±æœ‰ã•ã‚Œã‚‹é«˜é€Ÿãƒ¡ãƒ¢ãƒªï¼ˆãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: ~20ã‚µã‚¤ã‚¯ãƒ«ï¼‰ã§ã™ã€‚å„SMã«48-164KBã®å…±æœ‰ãƒ¡ãƒ¢ãƒªãŒã‚ã‚Šã€<strong>ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ–ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥</strong>ã¨ã—ã¦æ©Ÿèƒ½ã—ã¾ã™ã€‚ã‚¿ã‚¤ãƒªãƒ³ã‚°æŠ€è¡“ã§é »ç¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å…±æœ‰ãƒ¡ãƒ¢ãƒªã«é…ç½®ã™ã‚‹ã“ã¨ã§ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã‚’å‰Šæ¸›ã—ã€åŠ‡çš„ãªæ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾ã—ã¾ã™ã€‚ãŸã ã—ã€<strong>ãƒãƒ³ã‚¯ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆ</strong>ã‚’é¿ã‘ã‚‹è¨­è¨ˆãŒå¿…è¦ã§ã™ã€‚
            </p>

            <h3>ãƒ¬ã‚¸ã‚¹ã‚¿ï¼ˆRegistersï¼‰</h3>
            <p>
                <strong>ãƒ¬ã‚¸ã‚¹ã‚¿</strong>ã¯ã€å„ã‚¹ãƒ¬ãƒƒãƒ‰å°‚ç”¨ã®æœ€é€Ÿãƒ¡ãƒ¢ãƒªï¼ˆãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: 1ã‚µã‚¤ã‚¯ãƒ«ï¼‰ã§ã™ã€‚SMã”ã¨ã«64Kå€‹ã®ãƒ¬ã‚¸ã‚¹ã‚¿ãŒã‚ã‚Šã€ã‚¹ãƒ¬ãƒƒãƒ‰é–“ã§åˆ†å‰²ã•ã‚Œã¾ã™ã€‚ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã¯ãƒ¬ã‚¸ã‚¹ã‚¿ã«æ ¼ç´ã•ã‚Œã€ãƒ¬ã‚¸ã‚¹ã‚¿æ•°ãŒå¤šã™ãã‚‹ã¨<strong>ãƒ¬ã‚¸ã‚¹ã‚¿ã‚¹ãƒ”ãƒ«ï¼ˆregister spillingï¼‰</strong>ãŒç™ºç”Ÿã—ã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¡ãƒ¢ãƒªï¼ˆå®Ÿéš›ã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªã®ä¸€éƒ¨ã§é…ã„ï¼‰ã«ã‚ãµã‚Œã¾ã™ã€‚
            </p>

            <h3>å®šæ•°ãƒ¡ãƒ¢ãƒªï¼ˆConstant Memoryï¼‰</h3>
            <p>
                <strong>å®šæ•°ãƒ¡ãƒ¢ãƒª</strong>ã¯ã€èª­ã¿å–ã‚Šå°‚ç”¨ã®64KBãƒ¡ãƒ¢ãƒªã§ã€å…¨ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰é«˜é€Ÿã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚ã‚«ãƒ¼ãƒãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚„å¤‰æ›´ã•ã‚Œãªã„ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ä¿‚æ•°ã€ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ†ãƒ¼ãƒ–ãƒ«ç­‰ï¼‰ã‚’æ ¼ç´ã—ã¾ã™ã€‚ãƒ¯ãƒ¼ãƒ—å†…ã®å…¨ã‚¹ãƒ¬ãƒƒãƒ‰ãŒåŒã˜ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’èª­ã‚€å ´åˆã€ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã§1å›ã®ã‚¢ã‚¯ã‚»ã‚¹ã§æ¸ˆã¿ã¾ã™ã€‚
            </p>

            <h3>ãƒ†ã‚¯ã‚¹ãƒãƒ£ãƒ¡ãƒ¢ãƒªã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥</h3>
            <p>
                <strong>ãƒ†ã‚¯ã‚¹ãƒãƒ£ãƒ¡ãƒ¢ãƒª</strong>ã¯ã€èª­ã¿å–ã‚Šå°‚ç”¨ã§ç©ºé–“çš„å±€æ‰€æ€§ã‚’æ´»ã‹ã—ãŸ2Dã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æŒã¤ãƒ¡ãƒ¢ãƒªã§ã™ã€‚ç”»åƒå‡¦ç†ã§éš£æ¥ãƒ”ã‚¯ã‚»ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹å ´åˆã«åŠ¹æœçš„ã§ã™ã€‚<strong>L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆSMå†…ã€16-128KBï¼‰</strong>ã¨<strong>L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆå…¨SMå…±æœ‰ã€æ•°MBï¼‰</strong>ã‚‚ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã‚’é«˜é€ŸåŒ–ã—ã¾ã™ã€‚
            </p>

            <div class="mermaid">
flowchart TB
    subgraph Thread ["Threadï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰"]
        R[ãƒ¬ã‚¸ã‚¹ã‚¿<br/>ï½65Kå€‹/SM<br/>1ã‚µã‚¤ã‚¯ãƒ«]
        L[ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¡ãƒ¢ãƒª<br/>ã‚¹ãƒ”ãƒ«æ™‚ã®ã¿<br/>200-800ã‚µã‚¤ã‚¯ãƒ«]
    end

    subgraph Block ["Blockï¼ˆãƒ–ãƒ­ãƒƒã‚¯ï¼‰"]
        SM[å…±æœ‰ãƒ¡ãƒ¢ãƒª<br/>48-164KB/SM<br/>~20ã‚µã‚¤ã‚¯ãƒ«<br/>ãƒ–ãƒ­ãƒƒã‚¯å†…å…±æœ‰]
    end

    subgraph Device ["Deviceï¼ˆãƒ‡ãƒã‚¤ã‚¹ï¼‰"]
        GM[ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒª<br/>16-80GB<br/>200-800ã‚µã‚¤ã‚¯ãƒ«<br/>å…¨ã‚¹ãƒ¬ãƒƒãƒ‰å…±æœ‰]
        CM[å®šæ•°ãƒ¡ãƒ¢ãƒª<br/>64KB<br/>ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Š<br/>èª­ã¿å–ã‚Šå°‚ç”¨]
        TM[ãƒ†ã‚¯ã‚¹ãƒãƒ£ãƒ¡ãƒ¢ãƒª<br/>ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Š<br/>èª­ã¿å–ã‚Šå°‚ç”¨]
    end

    subgraph Cache ["ã‚­ãƒ£ãƒƒã‚·ãƒ¥"]
        L1[L1 ã‚­ãƒ£ãƒƒã‚·ãƒ¥<br/>16-128KB/SM]
        L2[L2 ã‚­ãƒ£ãƒƒã‚·ãƒ¥<br/>æ•°MB<br/>å…¨SMå…±æœ‰]
    end

    R --> |ã‚¹ãƒ”ãƒ«| L
    Thread --> SM
    Block --> GM
    GM --> L2
    L2 --> L1
    CM --> L1
    TM --> L1

    style Thread fill:#ffe4e4
    style Block fill:#d1e7dd
    style Device fill:#fff3cd
    style Cache fill:#e8f4f8
            </div>

            <h3>ãƒ¡ãƒ¢ãƒªéšå±¤ã®æ€§èƒ½æ¯”è¼ƒ</h3>
            <table>
                <thead>
                    <tr>
                        <th>ãƒ¡ãƒ¢ãƒªç¨®é¡</th>
                        <th>ã‚µã‚¤ã‚º</th>
                        <th>ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆã‚µã‚¤ã‚¯ãƒ«ï¼‰</th>
                        <th>ã‚¹ã‚³ãƒ¼ãƒ—</th>
                        <th>æ›¸ãè¾¼ã¿</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>ãƒ¬ã‚¸ã‚¹ã‚¿</strong></td>
                        <td>ï½65Kå€‹/SM</td>
                        <td>1</td>
                        <td>ã‚¹ãƒ¬ãƒƒãƒ‰</td>
                        <td>å¯</td>
                    </tr>
                    <tr>
                        <td><strong>å…±æœ‰ãƒ¡ãƒ¢ãƒª</strong></td>
                        <td>48-164KB/SM</td>
                        <td>~20</td>
                        <td>ãƒ–ãƒ­ãƒƒã‚¯</td>
                        <td>å¯</td>
                    </tr>
                    <tr>
                        <td><strong>L1 ã‚­ãƒ£ãƒƒã‚·ãƒ¥</strong></td>
                        <td>16-128KB/SM</td>
                        <td>~20</td>
                        <td>SM</td>
                        <td>è‡ªå‹•ç®¡ç†</td>
                    </tr>
                    <tr>
                        <td><strong>å®šæ•°ãƒ¡ãƒ¢ãƒª</strong></td>
                        <td>64KB</td>
                        <td>~20ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ™‚ï¼‰</td>
                        <td>å…¨ã‚¹ãƒ¬ãƒƒãƒ‰</td>
                        <td>ä¸å¯ï¼ˆèª­ã¿å–ã‚Šå°‚ç”¨ï¼‰</td>
                    </tr>
                    <tr>
                        <td><strong>ãƒ†ã‚¯ã‚¹ãƒãƒ£ãƒ¡ãƒ¢ãƒª</strong></td>
                        <td>ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªã®ã‚µãƒ–ã‚»ãƒƒãƒˆ</td>
                        <td>~20ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ™‚ï¼‰</td>
                        <td>å…¨ã‚¹ãƒ¬ãƒƒãƒ‰</td>
                        <td>ä¸å¯ï¼ˆèª­ã¿å–ã‚Šå°‚ç”¨ï¼‰</td>
                    </tr>
                    <tr>
                        <td><strong>L2 ã‚­ãƒ£ãƒƒã‚·ãƒ¥</strong></td>
                        <td>æ•°MB</td>
                        <td>~200</td>
                        <td>å…¨SM</td>
                        <td>è‡ªå‹•ç®¡ç†</td>
                    </tr>
                    <tr>
                        <td><strong>ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒª</strong></td>
                        <td>16-80GB</td>
                        <td>200-800</td>
                        <td>å…¨ã‚¹ãƒ¬ãƒƒãƒ‰</td>
                        <td>å¯</td>
                    </tr>
                    <tr>
                        <td><strong>ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¡ãƒ¢ãƒª</strong></td>
                        <td>ãƒ¬ã‚¸ã‚¹ã‚¿ã‚¹ãƒ”ãƒ«åˆ†</td>
                        <td>200-800</td>
                        <td>ã‚¹ãƒ¬ãƒƒãƒ‰</td>
                        <td>å¯ï¼ˆè‡ªå‹•ï¼‰</td>
                    </tr>
                </tbody>
            </table>

            <h3>ã‚³ãƒ¼ãƒ‰ä¾‹2: å…±æœ‰ãƒ¡ãƒ¢ãƒªã‚’ä½¿ã£ãŸè¡Œåˆ—ä¹—ç®—ã‚¿ã‚¤ãƒªãƒ³ã‚°</h3>
            <pre><code class="language-python">import cupy as cp
import numpy as np

# ã‚¿ã‚¤ãƒªãƒ³ã‚°ä»˜ãè¡Œåˆ—ä¹—ç®—ã‚«ãƒ¼ãƒãƒ«ï¼ˆå…±æœ‰ãƒ¡ãƒ¢ãƒªä½¿ç”¨ï¼‰
matmul_shared_kernel = cp.RawKernel(r'''
#define TILE_SIZE 16

extern "C" __global__
void matmul_shared(const float* A, const float* B, float* C, int N) {
    // å…±æœ‰ãƒ¡ãƒ¢ãƒªã®å®£è¨€ï¼ˆãƒ–ãƒ­ãƒƒã‚¯å†…å…¨ã‚¹ãƒ¬ãƒƒãƒ‰ã§å…±æœ‰ï¼‰
    __shared__ float tile_A[TILE_SIZE][TILE_SIZE];
    __shared__ float tile_B[TILE_SIZE][TILE_SIZE];

    // ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è¨ˆç®—
    int row = blockIdx.y * TILE_SIZE + threadIdx.y;
    int col = blockIdx.x * TILE_SIZE + threadIdx.x;

    float sum = 0.0f;

    // ã‚¿ã‚¤ãƒ«å˜ä½ã§ãƒ«ãƒ¼ãƒ—
    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; t++) {
        // ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªã‹ã‚‰å…±æœ‰ãƒ¡ãƒ¢ãƒªã¸ã‚¿ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        if (row < N && t * TILE_SIZE + threadIdx.x < N) {
            tile_A[threadIdx.y][threadIdx.x] = A[row * N + t * TILE_SIZE + threadIdx.x];
        } else {
            tile_A[threadIdx.y][threadIdx.x] = 0.0f;
        }

        if (col < N && t * TILE_SIZE + threadIdx.y < N) {
            tile_B[threadIdx.y][threadIdx.x] = B[(t * TILE_SIZE + threadIdx.y) * N + col];
        } else {
            tile_B[threadIdx.y][threadIdx.x] = 0.0f;
        }

        // ãƒ–ãƒ­ãƒƒã‚¯å†…å…¨ã‚¹ãƒ¬ãƒƒãƒ‰ãŒå…±æœ‰ãƒ¡ãƒ¢ãƒªã¸ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†ã‚’å¾…ã¤
        __syncthreads();

        // å…±æœ‰ãƒ¡ãƒ¢ãƒªä¸Šã®ã‚¿ã‚¤ãƒ«ã§ç©å’Œæ¼”ç®—
        for (int k = 0; k < TILE_SIZE; k++) {
            sum += tile_A[threadIdx.y][k] * tile_B[k][threadIdx.x];
        }

        // æ¬¡ã®ã‚¿ã‚¤ãƒ«ãƒ­ãƒ¼ãƒ‰å‰ã«å…¨ã‚¹ãƒ¬ãƒƒãƒ‰ã®è¨ˆç®—å®Œäº†ã‚’å¾…ã¤
        __syncthreads();
    }

    // çµæœã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªã«æ›¸ãè¾¼ã¿
    if (row < N && col < N) {
        C[row * N + col] = sum;
    }
}
''', 'matmul_shared')

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
N = 512  # è¡Œåˆ—ã‚µã‚¤ã‚º (512x512)
A = cp.random.rand(N, N, dtype=cp.float32)
B = cp.random.rand(N, N, dtype=cp.float32)
C = cp.zeros((N, N), dtype=cp.float32)

# ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ–ãƒ­ãƒƒã‚¯è¨­å®š
TILE_SIZE = 16
grid_dim = ((N + TILE_SIZE - 1) // TILE_SIZE, (N + TILE_SIZE - 1) // TILE_SIZE)
block_dim = (TILE_SIZE, TILE_SIZE)

print(f"è¡Œåˆ—ã‚µã‚¤ã‚º: {N}x{N}")
print(f"ã‚¿ã‚¤ãƒ«ã‚µã‚¤ã‚º: {TILE_SIZE}x{TILE_SIZE}")
print(f"ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º: {grid_dim}")
print(f"ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º: {block_dim}")

# ã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œ
matmul_shared_kernel(grid_dim, block_dim, (A, B, C, N))
cp.cuda.Stream.null.synchronize()

# çµæœæ¤œè¨¼ï¼ˆCuPyçµ„ã¿è¾¼ã¿é–¢æ•°ã¨æ¯”è¼ƒï¼‰
C_expected = cp.dot(A, B)
error = cp.abs(C - C_expected).max()

print(f"\næœ€å¤§èª¤å·®: {error:.2e}")
print(f"çµæœã®ä¸€è‡´: {'âœ“' if error < 1e-3 else 'âœ—'}")

# æ€§èƒ½æ¸¬å®š
import time
iterations = 100

# å…±æœ‰ãƒ¡ãƒ¢ãƒªç‰ˆ
start = time.time()
for _ in range(iterations):
    matmul_shared_kernel(grid_dim, block_dim, (A, B, C, N))
cp.cuda.Stream.null.synchronize()
shared_time = (time.time() - start) / iterations

print(f"\nå…±æœ‰ãƒ¡ãƒ¢ãƒªç‰ˆã®å®Ÿè¡Œæ™‚é–“: {shared_time*1000:.4f} ms")
print(f"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {2*N**3 / shared_time / 1e9:.2f} GFLOPS")

# å‡ºåŠ›ä¾‹:
# è¡Œåˆ—ã‚µã‚¤ã‚º: 512x512
# ã‚¿ã‚¤ãƒ«ã‚µã‚¤ã‚º: 16x16
# ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º: (32, 32)
# ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º: (16, 16)
#
# æœ€å¤§èª¤å·®: 3.81e-05
# çµæœã®ä¸€è‡´: âœ“
#
# å…±æœ‰ãƒ¡ãƒ¢ãƒªç‰ˆã®å®Ÿè¡Œæ™‚é–“: 0.3214 ms
# ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: 835.47 GFLOPS
</code></pre>

            <div class="info-box">
                <strong>ğŸ’¡ å…±æœ‰ãƒ¡ãƒ¢ãƒªã®åŠ¹æœ</strong>
                <p>
                    ã“ã®ã‚«ãƒ¼ãƒãƒ«ã¯ã€<strong>ã‚¿ã‚¤ãƒªãƒ³ã‚°æŠ€è¡“</strong>ã§å…±æœ‰ãƒ¡ãƒ¢ãƒªã‚’æ´»ç”¨ã—ã¾ã™ã€‚16x16ã‚¿ã‚¤ãƒ«ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªã‹ã‚‰å…±æœ‰ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ–ãƒ­ãƒƒã‚¯å†…ã®å…¨ã‚¹ãƒ¬ãƒƒãƒ‰ãŒé«˜é€Ÿãªå…±æœ‰ãƒ¡ãƒ¢ãƒªã‹ã‚‰ç¹°ã‚Šè¿”ã—ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿ã¾ã™ã€‚<code>__syncthreads()</code>ã§ãƒ–ãƒ­ãƒƒã‚¯å†…ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’åŒæœŸã—ã€ãƒ‡ãƒ¼ã‚¿ç«¶åˆã‚’é˜²ãã¾ã™ã€‚ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£…ã¨æ¯”è¼ƒã—ã¦ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãŒ1/16ã«å‰Šæ¸›ã•ã‚Œã€åŠ‡çš„ãªé«˜é€ŸåŒ–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
                </p>
            </div>
        </section>

        <section id="memory-optimization">
            <h2>2.4 ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹æœ€é©åŒ–</h2>

            <h3>ã‚³ã‚¢ãƒ¬ã‚¹ãƒ‰ãƒ»ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆCoalesced Accessï¼‰</h3>
            <p>
                <strong>ã‚³ã‚¢ãƒ¬ã‚¹ãƒ‰ãƒ»ã‚¢ã‚¯ã‚»ã‚¹</strong>ã¯ã€ãƒ¯ãƒ¼ãƒ—å†…ã®32ã‚¹ãƒ¬ãƒƒãƒ‰ãŒé€£ç¶šã—ãŸãƒ¡ãƒ¢ãƒªã‚¢ãƒ‰ãƒ¬ã‚¹ã«åŒæ™‚ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚GPUã¯128ãƒã‚¤ãƒˆã®ãƒ¡ãƒ¢ãƒªãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å˜ä½ã§ãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€ã™ã‚‹ãŸã‚ã€ã‚¹ãƒ¬ãƒƒãƒ‰0ãŒã‚¢ãƒ‰ãƒ¬ã‚¹0ã€ã‚¹ãƒ¬ãƒƒãƒ‰1ãŒã‚¢ãƒ‰ãƒ¬ã‚¹4ï¼ˆfloat32ï¼‰ã€...ã€ã‚¹ãƒ¬ãƒƒãƒ‰31ãŒã‚¢ãƒ‰ãƒ¬ã‚¹124ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã¨ã€1å›ã®ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã§å®Œäº†ã—ã¾ã™ã€‚
            </p>
            <p>
                é€†ã«ã€ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ãƒ»ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆå„ã‚¹ãƒ¬ãƒƒãƒ‰ãŒé›¢ã‚ŒãŸã‚¢ãƒ‰ãƒ¬ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ï¼‰ã§ã¯ã€è¤‡æ•°ã®ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ãŒå¿…è¦ã¨ãªã‚Šã€ãƒãƒ³ãƒ‰å¹…ãŒå¤§å¹…ã«ä½ä¸‹ã—ã¾ã™ã€‚ä¾‹ãˆã°ã€ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰32ã®ã‚¢ã‚¯ã‚»ã‚¹ã§ã¯ã€ãƒãƒ³ãƒ‰å¹…ãŒç†è«–å€¤ã®1/32ã«ä½ä¸‹ã™ã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ã€‚
            </p>

            <h3>å…±æœ‰ãƒ¡ãƒ¢ãƒªã®ãƒãƒ³ã‚¯ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆ</h3>
            <p>
                å…±æœ‰ãƒ¡ãƒ¢ãƒªã¯32å€‹ã®<strong>ãƒãƒ³ã‚¯</strong>ã«åˆ†å‰²ã•ã‚Œã¦ãŠã‚Šã€ç•°ãªã‚‹ãƒãƒ³ã‚¯ã¸ã®åŒæ™‚ã‚¢ã‚¯ã‚»ã‚¹ã¯ä¸¦åˆ—å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚ã—ã‹ã—ã€è¤‡æ•°ã®ã‚¹ãƒ¬ãƒƒãƒ‰ãŒåŒã˜ãƒãƒ³ã‚¯ã®ç•°ãªã‚‹ã‚¢ãƒ‰ãƒ¬ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã¨<strong>ãƒãƒ³ã‚¯ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆ</strong>ãŒç™ºç”Ÿã—ã€ã‚¢ã‚¯ã‚»ã‚¹ãŒç›´åˆ—åŒ–ã•ã‚Œã¾ã™ã€‚
            </p>
            <p>
                ãƒãƒ³ã‚¯ã¯4ãƒã‚¤ãƒˆå˜ä½ã§å‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¾ã™ã€‚ä¾‹ãˆã°ã€<code>shared[threadIdx.x]</code>ã¯ç«¶åˆãªã—ã§ã™ãŒã€<code>shared[threadIdx.x * 2]</code>ã§ã¯å¶æ•°ã‚¹ãƒ¬ãƒƒãƒ‰ãŒãƒãƒ³ã‚¯0, 2, 4...ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€16-way ãƒãƒ³ã‚¯ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆãŒç™ºç”Ÿã—ã¾ã™ã€‚ã“ã‚Œã‚’é¿ã‘ã‚‹ã«ã¯ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¿½åŠ ã—ã¦é…åˆ—æ§‹é€ ã‚’èª¿æ•´ã—ã¾ã™ã€‚
            </p>

            <h3>ã‚ªã‚­ãƒ¥ãƒ‘ãƒ³ã‚·ãƒ¼ï¼ˆOccupancyï¼‰ã®æœ€é©åŒ–</h3>
            <p>
                <strong>ã‚ªã‚­ãƒ¥ãƒ‘ãƒ³ã‚·ãƒ¼</strong>ã¯ã€SMä¸Šã§å®Ÿéš›ã«ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ¯ãƒ¼ãƒ—æ•°ã¨ç†è«–æœ€å¤§ãƒ¯ãƒ¼ãƒ—æ•°ã®æ¯”ç‡ã§ã™ã€‚é«˜ã„ã‚ªã‚­ãƒ¥ãƒ‘ãƒ³ã‚·ãƒ¼ï¼ˆ75-100%ï¼‰ã¯ã€ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·éš è”½ã«æœ‰åŠ¹ã§ã™ã€‚ãŸã ã—ã€å…±æœ‰ãƒ¡ãƒ¢ãƒªã‚„ãƒ¬ã‚¸ã‚¹ã‚¿ä½¿ç”¨é‡ãŒå¤šã„ã¨ã€1SMä¸Šã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ–ãƒ­ãƒƒã‚¯æ•°ãŒåˆ¶é™ã•ã‚Œã€ã‚ªã‚­ãƒ¥ãƒ‘ãƒ³ã‚·ãƒ¼ãŒä½ä¸‹ã—ã¾ã™ã€‚
            </p>
            <p>
                æœ€é©ãªãƒãƒ©ãƒ³ã‚¹ã¯å•é¡Œä¾å­˜ã§ã™ã€‚å…±æœ‰ãƒ¡ãƒ¢ãƒªã‚’å¤šç”¨ã™ã‚‹æœ€é©åŒ–ï¼ˆä½ã‚ªã‚­ãƒ¥ãƒ‘ãƒ³ã‚·ãƒ¼ï¼‰ãŒã€ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹å‰Šæ¸›ã§é«˜ã‚ªã‚­ãƒ¥ãƒ‘ãƒ³ã‚·ãƒ¼ã®å˜ç´”å®Ÿè£…ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ã€‚<code>nvprof</code>ã‚„<code>Nsight Compute</code>ã§ã‚ªã‚­ãƒ¥ãƒ‘ãƒ³ã‚·ãƒ¼ã¨æ€§èƒ½ã®é–¢ä¿‚ã‚’åˆ†æã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚
            </p>

            <h3>ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†é¡</h3>

            <table>
                <thead>
                    <tr>
                        <th>ãƒ¡ãƒ¢ãƒªç¨®åˆ¥</th>
                        <th>ã‚µã‚¤ã‚º</th>
                        <th>ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·</th>
                        <th>ãƒãƒ³ãƒ‰å¹…</th>
                        <th>ç”¨é€”</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>ãƒ¬ã‚¸ã‚¹ã‚¿</strong></td>
                        <td>~256 KB/SM</td>
                        <td>0-1ã‚¯ãƒ­ãƒƒã‚¯</td>
                        <td>æ•°TB/s</td>
                        <td>ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°</td>
                    </tr>
                    <tr>
                        <td><strong>å…±æœ‰ãƒ¡ãƒ¢ãƒª</strong></td>
                        <td>48-164 KB/SM</td>
                        <td>æ•°ã‚¯ãƒ­ãƒƒã‚¯</td>
                        <td>~10 TB/s</td>
                        <td>ã‚¹ãƒ¬ãƒƒãƒ‰é–“ãƒ‡ãƒ¼ã‚¿å…±æœ‰</td>
                    </tr>
                    <tr>
                        <td><strong>L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥</strong></td>
                        <td>128 KB/SM</td>
                        <td>~30ã‚¯ãƒ­ãƒƒã‚¯</td>
                        <td>æ•°TB/s</td>
                        <td>è‡ªå‹•ã‚­ãƒ£ãƒƒã‚·ãƒ¥</td>
                    </tr>
                    <tr>
                        <td><strong>L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥</strong></td>
                        <td>6-40 MB</td>
                        <td>~200ã‚¯ãƒ­ãƒƒã‚¯</td>
                        <td>æ•°TB/s</td>
                        <td>SMé–“ãƒ‡ãƒ¼ã‚¿å…±æœ‰</td>
                    </tr>
                    <tr>
                        <td><strong>ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒª</strong></td>
                        <td>16-80 GB</td>
                        <td>æ•°ç™¾ã‚¯ãƒ­ãƒƒã‚¯</td>
                        <td>0.9-3 TB/s</td>
                        <td>å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿</td>
                    </tr>
                    <tr>
                        <td><strong>ã‚³ãƒ³ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒ¢ãƒª</strong></td>
                        <td>64 KB</td>
                        <td>æ•°ã‚¯ãƒ­ãƒƒã‚¯</td>
                        <td>é«˜é€Ÿï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨ï¼‰</td>
                        <td>èª­ã¿å–ã‚Šå°‚ç”¨å®šæ•°</td>
                    </tr>
                </tbody>
            </table>

            <h3>ã‚³ãƒ¼ãƒ‰ä¾‹3: Memory bandwidth measurement</h3>
            <pre><code class="language-python">import cupy as cp
import time
import numpy as np

def measure_memory_bandwidth():
    """GPUãƒ¡ãƒ¢ãƒªãƒãƒ³ãƒ‰å¹…ã‚’æ¸¬å®š"""
    # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºï¼ˆ1 GBï¼‰
    size = 1024 * 1024 * 1024 // 8  # 1 GB / 8 bytes per float64

    print("=== GPU ãƒ¡ãƒ¢ãƒªãƒãƒ³ãƒ‰å¹…æ¸¬å®š ===\n")
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {size * 8 / 1e9:.2f} GB\n")

    # Host to Device (H2D)
    print("--- Host to Device (H2D) ---")
    data_host = np.random.rand(size).astype(np.float64)

    # Warm-up
    _ = cp.asarray(data_host)

    start = time.time()
    data_device = cp.asarray(data_host)
    cp.cuda.Stream.null.synchronize()
    h2d_time = time.time() - start

    h2d_bandwidth = (size * 8 / 1e9) / h2d_time
    print(f"è»¢é€æ™‚é–“: {h2d_time:.4f}s")
    print(f"ãƒãƒ³ãƒ‰å¹…: {h2d_bandwidth:.2f} GB/s\n")

    # Device to Host (D2H)
    print("--- Device to Host (D2H) ---")
    start = time.time()
    data_back = cp.asnumpy(data_device)
    d2h_time = time.time() - start

    d2h_bandwidth = (size * 8 / 1e9) / d2h_time
    print(f"è»¢é€æ™‚é–“: {d2h_time:.4f}s")
    print(f"ãƒãƒ³ãƒ‰å¹…: {d2h_bandwidth:.2f} GB/s\n")

    # Device to Device (D2D)
    print("--- Device to Device (D2D) - Copy ---")
    start = time.time()
    data_copy = cp.copy(data_device)
    cp.cuda.Stream.null.synchronize()
    d2d_time = time.time() - start

    d2d_bandwidth = (size * 8 / 1e9) / d2d_time
    print(f"è»¢é€æ™‚é–“: {d2d_time:.4f}s")
    print(f"ãƒãƒ³ãƒ‰å¹…: {d2d_bandwidth:.2f} GB/s\n")

    # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªèª­ã¿è¾¼ã¿ï¼ˆè¨ˆç®—ã‚ã‚Šï¼‰
    print("--- Global Memory Read with Computation ---")
    data_a = cp.random.rand(size, dtype=cp.float64)
    data_b = cp.random.rand(size, dtype=cp.float64)

    # Warm-up
    _ = data_a + data_b
    cp.cuda.Stream.null.synchronize()

    start = time.time()
    result = data_a + data_b
    cp.cuda.Stream.null.synchronize()
    compute_time = time.time() - start

    # èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿é‡: data_a + data_b = 2å€ã€æ›¸ãè¾¼ã¿: result = 1å€
    total_bytes = (size * 8 * 3) / 1e9  # 3x read/write
    compute_bandwidth = total_bytes / compute_time
    print(f"è¨ˆç®—æ™‚é–“: {compute_time:.4f}s")
    print(f"å®ŸåŠ¹ãƒãƒ³ãƒ‰å¹…: {compute_bandwidth:.2f} GB/s\n")

    # ç†è«–å€¤ã¨ã®æ¯”è¼ƒ
    print("--- ç†è«–å€¤ã¨ã®æ¯”è¼ƒ ---")
    # RTX 3090ã®ç†è«–ãƒãƒ³ãƒ‰å¹…: 936 GB/s
    theoretical_bandwidth = 936  # GB/sï¼ˆç’°å¢ƒã«å¿œã˜ã¦å¤‰æ›´ï¼‰
    efficiency = (d2d_bandwidth / theoretical_bandwidth) * 100
    print(f"ç†è«–ãƒ¡ãƒ¢ãƒªãƒãƒ³ãƒ‰å¹…: {theoretical_bandwidth} GB/s")
    print(f"å®Ÿæ¸¬å€¤ï¼ˆD2Dï¼‰: {d2d_bandwidth:.2f} GB/s")
    print(f"é”æˆç‡: {efficiency:.1f}%")

if __name__ == "__main__":
    measure_memory_bandwidth()

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ä¾‹ï¼ˆRTX 3090ã®å ´åˆï¼‰:
# === GPU ãƒ¡ãƒ¢ãƒªãƒãƒ³ãƒ‰å¹…æ¸¬å®š ===
#
# ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: 1.00 GB
#
# --- Host to Device (H2D) ---
# è»¢é€æ™‚é–“: 0.0823s
# ãƒãƒ³ãƒ‰å¹…: 12.15 GB/s
#
# --- Device to Host (D2H) ---
# è»¢é€æ™‚é–“: 0.0891s
# ãƒãƒ³ãƒ‰å¹…: 11.22 GB/s
#
# --- Device to Device (D2D) - Copy ---
# è»¢é€æ™‚é–“: 0.0012s
# ãƒãƒ³ãƒ‰å¹…: 833.33 GB/s
#
# --- Global Memory Read with Computation ---
# è¨ˆç®—æ™‚é–“: 0.0035s
# å®ŸåŠ¹ãƒãƒ³ãƒ‰å¹…: 857.14 GB/s
#
# --- ç†è«–å€¤ã¨ã®æ¯”è¼ƒ ---
# ç†è«–ãƒ¡ãƒ¢ãƒªãƒãƒ³ãƒ‰å¹…: 936 GB/s
# å®Ÿæ¸¬å€¤ï¼ˆD2Dï¼‰: 833.33 GB/s
# é”æˆç‡: 89.0%
</code></pre>

            <div class="warning-box">
                <strong>âš ï¸ ãƒ¡ãƒ¢ãƒªãƒãƒ³ãƒ‰å¹…ã®å¾‹é€Ÿ</strong>
                <p>
                    å¤šãã®ç§‘å­¦æŠ€è¡“è¨ˆç®—ã¯<strong>ãƒ¡ãƒ¢ãƒªãƒãƒ³ãƒ‰å¹…å¾‹é€Ÿ</strong>ï¼ˆmemory-boundï¼‰ã§ã™ã€‚è¨ˆç®—é‡ãŒå°‘ãªãã€ãƒ‡ãƒ¼ã‚¿è»¢é€ãŒå¤šã„ã‚¿ã‚¹ã‚¯ã§ã¯ã€ã„ãã‚‰CUDAã‚³ã‚¢ãŒé«˜é€Ÿã§ã‚‚ã€ãƒ¡ãƒ¢ãƒªã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ä¾›çµ¦ãŒè¿½ã„ã¤ã‹ãšã€æ€§èƒ½ãŒé ­æ‰“ã¡ã«ãªã‚Šã¾ã™ã€‚å…±æœ‰ãƒ¡ãƒ¢ãƒªã®æ´»ç”¨ã‚„ãƒ‡ãƒ¼ã‚¿å†åˆ©ç”¨ãŒé‡è¦ã§ã™ã€‚
                </p>
            </div>
        </section>

        <section id="compute-capability">
            <h2>1.5 è¨ˆç®—èƒ½åŠ›ï¼ˆCompute Capabilityï¼‰ã®ç¢ºèª</h2>

            <h3>Compute Capabilityã¨ã¯</h3>
            <p>
                <strong>Compute Capability</strong>ã¯ã€NVIDIA GPUã®<strong>æ©Ÿèƒ½ã‚»ãƒƒãƒˆã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ä¸–ä»£</strong>ã‚’ç¤ºã™ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã§ã™ã€‚ä¾‹ãˆã°ã€Compute Capability 8.6ï¼ˆAmpereä¸–ä»£ã®RTX 30ã‚·ãƒªãƒ¼ã‚ºï¼‰ã¯ã€ãƒ†ãƒ³ã‚½ãƒ«ã‚³ã‚¢ã€æ··åˆç²¾åº¦è¨ˆç®—ï¼ˆFP16, TF32ï¼‰ã€éåŒæœŸã‚³ãƒ”ãƒ¼ç­‰ã®æ©Ÿèƒ½ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚
            </p>

            <h3>ã‚µãƒãƒ¼ãƒˆã•ã‚Œã‚‹æ©Ÿèƒ½ã®ç¢ºèª</h3>
            <table>
                <thead>
                    <tr>
                        <th>Compute Capability</th>
                        <th>ä¸–ä»£</th>
                        <th>ä¸»è¦æ©Ÿèƒ½</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>7.0</td>
                        <td>Volta</td>
                        <td>ãƒ†ãƒ³ã‚½ãƒ«ã‚³ã‚¢ï¼ˆFP16ï¼‰ã€ç‹¬ç«‹ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°</td>
                    </tr>
                    <tr>
                        <td>7.5</td>
                        <td>Turing</td>
                        <td>INT8ãƒ†ãƒ³ã‚½ãƒ«ã‚³ã‚¢ã€RT Coreï¼ˆãƒ¬ã‚¤ãƒˆãƒ¬ï¼‰</td>
                    </tr>
                    <tr>
                        <td>8.0</td>
                        <td>Ampere (A100)</td>
                        <td>TF32ãƒ†ãƒ³ã‚½ãƒ«ã‚³ã‚¢ã€ãƒãƒ«ãƒã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹GPUï¼ˆMIGï¼‰</td>
                    </tr>
                    <tr>
                        <td>8.6</td>
                        <td>Ampere (RTX 30xx)</td>
                        <td>FP16/INT8ãƒ†ãƒ³ã‚½ãƒ«ã‚³ã‚¢ã€DLSS</td>
                    </tr>
                    <tr>
                        <td>9.0</td>
                        <td>Hopper</td>
                        <td>ç¬¬4ä¸–ä»£ãƒ†ãƒ³ã‚½ãƒ«ã‚³ã‚¢ã€Transformer Engineã€FP8</td>
                    </tr>
                </tbody>
            </table>

            <h3>æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®æ–¹æ³•</h3>
            <p>
                GPUæ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹éš›ã¯ã€ä»¥ä¸‹ã®3ã¤ã®æŒ‡æ¨™ã‚’ç¢ºèªã—ã¾ã™ï¼š
            </p>
            <ul>
                <li><strong>æ¼”ç®—æ€§èƒ½ï¼ˆFLOPSï¼‰</strong>: 1ç§’é–“ã«å®Ÿè¡Œã§ãã‚‹æµ®å‹•å°æ•°ç‚¹æ¼”ç®—ã®å›æ•°</li>
                <li><strong>ãƒ¡ãƒ¢ãƒªãƒãƒ³ãƒ‰å¹…</strong>: 1ç§’é–“ã«è»¢é€ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿é‡ï¼ˆGB/sï¼‰</li>
                <li><strong>å®ŸåŠ¹æ€§èƒ½</strong>: å®Ÿéš›ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®æ€§èƒ½ï¼ˆã‚¢ãƒ—ãƒªä¾å­˜ï¼‰</li>
            </ul>

            <h3>nvidia-smi ã‚³ãƒãƒ³ãƒ‰ã®ä½¿ã„æ–¹</h3>
            <p>
                <code>nvidia-smi</code>ï¼ˆNVIDIA System Management Interfaceï¼‰ã¯ã€GPUçŠ¶æ…‹ã‚’ç¢ºèªã™ã‚‹æ¨™æº–ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®GPUåˆ©ç”¨ç‡ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã€æ¸©åº¦ã€ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±ã‚’ç¢ºèªã§ãã¾ã™ã€‚
            </p>

            <pre><code class="language-bash"># åŸºæœ¬çš„ãªä½¿ã„æ–¹
nvidia-smi

# 1ç§’ã”ã¨ã«ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥
watch -n 1 nvidia-smi

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è©³ç´°è¡¨ç¤º
nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv

# ãƒ—ãƒ­ã‚»ã‚¹ã”ã¨ã®GPUä½¿ç”¨çŠ¶æ³
nvidia-smi pmon -i 0 -s u -c 10

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ä¾‹:
# +-----------------------------------------------------------------------------+
# | NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |
# |-------------------------------+----------------------+----------------------+
# | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
# | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
# |                               |                      |               MIG M. |
# |===============================+======================+======================|
# |   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
# | 30%   45C    P2    75W / 350W |  12345MiB / 24576MiB |     35%      Default |
# |                               |                      |                  N/A |
# +-------------------------------+----------------------+----------------------+
</code></pre>

            <h3>ã‚³ãƒ¼ãƒ‰ä¾‹4: Complete system information (PyTorch)</h3>
            <pre><code class="language-python">import torch
import subprocess

def print_complete_system_info():
    """PyTorchã¨CUDAã®è©³ç´°ãªã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’è¡¨ç¤º"""
    print("=" * 70)
    print("=== PyTorch GPU å®Œå…¨æƒ…å ± ===")
    print("=" * 70)

    # CUDA availability
    print("\n--- CUDA åŸºæœ¬æƒ…å ± ---")
    print(f"CUDA Available: {torch.cuda.is_available()}")

    if not torch.cuda.is_available():
        print("CUDAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚GPUãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    print(f"CUDA Version: {torch.version.cuda}")
    print(f"cuDNN Version: {torch.backends.cudnn.version()}")
    print(f"cuDNN Enabled: {torch.backends.cudnn.enabled}")
    print(f"Number of GPUs: {torch.cuda.device_count()}")
    print(f"Current Device ID: {torch.cuda.current_device()}")

    # å„GPUã®è©³ç´°æƒ…å ±
    for i in range(torch.cuda.device_count()):
        print(f"\n{'=' * 70}")
        print(f"--- GPU {i} è©³ç´°æƒ…å ± ---")
        print(f"{'=' * 70}")

        props = torch.cuda.get_device_properties(i)

        print(f"Name: {props.name}")
        print(f"Compute Capability: {props.major}.{props.minor}")
        print(f"Total Memory: {props.total_memory / 1e9:.2f} GB")
        print(f"Multi-Processor Count: {props.multi_processor_count}")

        # ãƒ¡ãƒ¢ãƒªæƒ…å ±
        mem_allocated = torch.cuda.memory_allocated(i) / 1e9
        mem_reserved = torch.cuda.memory_reserved(i) / 1e9
        max_mem_allocated = torch.cuda.max_memory_allocated(i) / 1e9

        print(f"\n--- ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ ---")
        print(f"Allocated: {mem_allocated:.2f} GB")
        print(f"Reserved: {mem_reserved:.2f} GB")
        print(f"Max Allocated: {max_mem_allocated:.2f} GB")
        print(f"Free: {(props.total_memory / 1e9) - mem_reserved:.2f} GB")

        # æ€§èƒ½æŒ‡æ¨™
        print(f"\n--- æ€§èƒ½æŒ‡æ¨™ ---")

        # FP32 ç†è«–æ€§èƒ½ (TFLOPS)
        clock_rate_ghz = props.clock_rate / 1e6
        # æ¨å®šCUDAã‚³ã‚¢æ•°ï¼ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ä¾å­˜ï¼‰
        if props.major == 8 and props.minor == 6:  # Ampere RTX
            cores_per_sm = 128
        elif props.major == 8 and props.minor == 0:  # Ampere A100
            cores_per_sm = 64
        elif props.major == 7 and props.minor == 0:  # Volta
            cores_per_sm = 64
        else:
            cores_per_sm = 64

        total_cores = props.multi_processor_count * cores_per_sm
        fp32_tflops = (total_cores * clock_rate_ghz * 2) / 1000  # FMA: 2 ops/cycle

        print(f"Clock Rate: {clock_rate_ghz:.2f} GHz")
        print(f"Estimated CUDA Cores: {total_cores}")
        print(f"Theoretical FP32 Performance: {fp32_tflops:.2f} TFLOPS")

        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        print(f"\n--- å®Ÿæ¸¬ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ---")
        device = torch.device(f'cuda:{i}')

        # è¡Œåˆ—ç©ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        size = 8192
        a = torch.randn(size, size, device=device, dtype=torch.float32)
        b = torch.randn(size, size, device=device, dtype=torch.float32)

        # Warm-up
        _ = torch.mm(a, b)
        torch.cuda.synchronize(device)

        # æ¸¬å®š
        start_event = torch.cuda.Event(enable_timing=True)
        end_event = torch.cuda.Event(enable_timing=True)

        start_event.record()
        c = torch.mm(a, b)
        end_event.record()

        torch.cuda.synchronize(device)
        elapsed_ms = start_event.elapsed_time(end_event)

        # FLOPSè¨ˆç®—ï¼ˆè¡Œåˆ—ç©: 2 * N^3 operationsï¼‰
        flops = 2 * (size ** 3)
        measured_tflops = flops / (elapsed_ms * 1e-3) / 1e12
        efficiency = (measured_tflops / fp32_tflops) * 100

        print(f"Matrix Multiply ({size}x{size}): {elapsed_ms:.2f} ms")
        print(f"Measured Performance: {measured_tflops:.2f} TFLOPS")
        print(f"Efficiency: {efficiency:.1f}% of theoretical peak")

    # nvidia-smi å‡ºåŠ›
    print(f"\n{'=' * 70}")
    print("--- nvidia-smi å‡ºåŠ› ---")
    print(f"{'=' * 70}")
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        print(result.stdout)
    except FileNotFoundError:
        print("nvidia-smi ã‚³ãƒãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

if __name__ == "__main__":
    print_complete_system_info()

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ä¾‹ï¼ˆRTX 3090ã®å ´åˆï¼‰:
# ======================================================================
# === PyTorch GPU å®Œå…¨æƒ…å ± ===
# ======================================================================
#
# --- CUDA åŸºæœ¬æƒ…å ± ---
# CUDA Available: True
# CUDA Version: 12.1
# cuDNN Version: 8902
# cuDNN Enabled: True
# Number of GPUs: 1
# Current Device ID: 0
#
# ======================================================================
# --- GPU 0 è©³ç´°æƒ…å ± ---
# ======================================================================
# Name: NVIDIA GeForce RTX 3090
# Compute Capability: 8.6
# Total Memory: 24.00 GB
# Multi-Processor Count: 82
#
# --- ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ ---
# Allocated: 0.52 GB
# Reserved: 0.54 GB
# Max Allocated: 0.52 GB
# Free: 23.46 GB
#
# --- æ€§èƒ½æŒ‡æ¨™ ---
# Clock Rate: 1.70 GHz
# Estimated CUDA Cores: 10496
# Theoretical FP32 Performance: 35.69 TFLOPS
#
# --- å®Ÿæ¸¬ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ---
# Matrix Multiply (8192x8192): 35.21 ms
# Measured Performance: 31.15 TFLOPS
# Efficiency: 87.3% of theoretical peak
</code></pre>

            <div class="info-box">
                <strong>ğŸ’¡ æ€§èƒ½åŠ¹ç‡ã®è§£é‡ˆ</strong>
                <p>
                    å®Ÿæ¸¬æ€§èƒ½ãŒç†è«–ãƒ”ãƒ¼ã‚¯ã®<strong>80-90%</strong>ã§ã‚ã‚Œã°ã€éå¸¸ã«è‰¯å¥½ã§ã™ã€‚70%æœªæº€ã®å ´åˆã¯ã€ãƒ¡ãƒ¢ãƒªãƒãƒ³ãƒ‰å¹…å¾‹é€Ÿã€ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ä¸è¶³ã€ã‚«ãƒ¼ãƒãƒ«éåŠ¹ç‡ç­‰ã®å•é¡ŒãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ï¼ˆç¬¬6ç« ã§è©³è¿°ï¼‰ã§åŸå› ã‚’ç‰¹å®šã—ã¾ã™ã€‚
                </p>
            </div>
        </section>

        <section id="materials-science">
            <h2>1.6 GPUæ´»ç”¨ã®å®Ÿä¾‹ï¼šææ–™ç§‘å­¦ã§ã®å¿œç”¨</h2>

            <h3>å¯†åº¦æ±é–¢æ•°ç†è«–ï¼ˆDFTï¼‰è¨ˆç®—ã®ä¸¦åˆ—åŒ–</h3>
            <p>
                DFTè¨ˆç®—ã§ã¯ã€<strong>Hamiltonianè¡Œåˆ—ã®å¯¾è§’åŒ–</strong>ã€<strong>ãƒ•ãƒ¼ãƒªã‚¨å¤‰æ›</strong>ã€<strong>é›»å­å¯†åº¦ã®è¨ˆç®—</strong>ãŒä¸»è¦ãªè¨ˆç®—ã‚¿ã‚¹ã‚¯ã§ã™ã€‚ã“ã‚Œã‚‰ã¯è¡Œåˆ—æ¼”ç®—ãŒä¸­å¿ƒã§ã€GPUã®å¾—æ„åˆ†é‡ã§ã™ã€‚VASP-GPUç‰ˆã§ã¯ã€kç‚¹ä¸¦åˆ—ã¨ãƒãƒ³ãƒ‰ä¸¦åˆ—ã‚’çµ„ã¿åˆã‚ã›ã€10-50å€ã®é«˜é€ŸåŒ–ãŒå ±å‘Šã•ã‚Œã¦ã„ã¾ã™ã€‚
            </p>

            <h3>åˆ†å­å‹•åŠ›å­¦ï¼ˆMDï¼‰ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</h3>
            <p>
                MDè¨ˆç®—ã§ã¯ã€<strong>åŠ›å ´è¨ˆç®—</strong>ã€<strong>åŸå­åº§æ¨™æ›´æ–°</strong>ã€<strong>è¿‘æ¥ãƒªã‚¹ãƒˆä½œæˆ</strong>ãŒå¾‹é€Ÿã§ã™ã€‚LAMMPS-GPUã‚„GROMACS-GPUã¯ã€ã“ã‚Œã‚‰ã‚’CUDAã‚«ãƒ¼ãƒãƒ«ã§å®Ÿè£…ã—ã€æ•°ä¸‡ã€œæ•°ç™¾ä¸‡åŸå­ç³»ã§20-100å€ã®é«˜é€ŸåŒ–ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚
            </p>

            <h3>æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´</h3>
            <p>
                æ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆPyTorch, TensorFlowï¼‰ã¯ã€ç•³ã¿è¾¼ã¿ã€è¡Œåˆ—ç©ã€æ´»æ€§åŒ–é–¢æ•°ã‚’GPUã§è‡ªå‹•ä¸¦åˆ—åŒ–ã—ã¾ã™ã€‚ãƒ†ãƒ³ã‚½ãƒ«ã‚³ã‚¢ã‚’ä½¿ã£ãŸæ··åˆç²¾åº¦è¨“ç·´ï¼ˆFP16/TF32ï¼‰ã«ã‚ˆã‚Šã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’åŠæ¸›ã•ã›ã¤ã¤ã€è¨“ç·´é€Ÿåº¦ã‚’2-3å€å‘ä¸Šã§ãã¾ã™ã€‚
            </p>

            <h3>å¤§è¦æ¨¡è¡Œåˆ—æ¼”ç®—</h3>
            <p>
                ææ–™ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€å¤§è¦æ¨¡ç–è¡Œåˆ—ã®æ±‚è§£ï¼ˆå…±å½¹å‹¾é…æ³•ã€GMRESæ³•ï¼‰ãŒé »å‡ºã—ã¾ã™ã€‚cuSPARSEãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ã€ç–è¡Œåˆ—æ¼”ç®—ã‚’GPUã§é«˜é€ŸåŒ–ã—ã€CPUç‰ˆã®10-50å€ã®æ€§èƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
            </p>

            <h3>ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£: ãƒãƒ³ãƒ‰æ§‹é€ è¨ˆç®—ã®é«˜é€ŸåŒ–</h3>
            <p>
                Siã®ãƒãƒ³ãƒ‰æ§‹é€ è¨ˆç®—ã‚’ä¾‹ã«ã€GPUé«˜é€ŸåŒ–ã®åŠ¹æœã‚’ç¤ºã—ã¾ã™ã€‚128 kç‚¹ã€512ãƒãƒ³ãƒ‰ã®ã‚±ãƒ¼ã‚¹ã§ã€CPUç‰ˆï¼ˆ16ã‚³ã‚¢ï¼‰ã§ã¯30åˆ†ã‹ã‹ã‚‹è¨ˆç®—ãŒã€GPUç‰ˆï¼ˆRTX 3090ï¼‰ã§ã¯1.5åˆ†ã§å®Œäº†ã—ã¾ã—ãŸï¼ˆ20å€é«˜é€ŸåŒ–ï¼‰ã€‚
            </p>

            <h3>ã‚³ãƒ¼ãƒ‰ä¾‹5: Matrix multiplication performance comparison</h3>
            <pre><code class="language-python">import numpy as np
import cupy as cp
import time
import pandas as pd

def benchmark_matrix_multiply():
    """è¡Œåˆ—ç©ã®æ€§èƒ½æ¯”è¼ƒï¼ˆCPU vs GPUï¼‰"""
    sizes = [1000, 2000, 5000, 10000]
    results = []

    print("=== è¡Œåˆ—ç© æ€§èƒ½æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ===\n")
    print("å„ã‚µã‚¤ã‚ºã§5å›æ¸¬å®šã—ã€ä¸­å¤®å€¤ã‚’æ¡ç”¨\n")

    for size in sizes:
        print(f"--- Matrix Size: {size}x{size} ---")

        # CPU (NumPy)
        a_cpu = np.random.rand(size, size).astype(np.float32)
        b_cpu = np.random.rand(size, size).astype(np.float32)

        cpu_times = []
        for _ in range(5):
            start = time.time()
            c_cpu = np.dot(a_cpu, b_cpu)
            cpu_time = time.time() - start
            cpu_times.append(cpu_time)

        cpu_time_median = np.median(cpu_times)
        print(f"CPU Time (NumPy): {cpu_time_median:.4f}s")

        # GPU (CuPy)
        a_gpu = cp.random.rand(size, size, dtype=cp.float32)
        b_gpu = cp.random.rand(size, size, dtype=cp.float32)

        # Warm-up
        _ = cp.dot(a_gpu, b_gpu)
        cp.cuda.Stream.null.synchronize()

        gpu_times = []
        for _ in range(5):
            start = time.time()
            c_gpu = cp.dot(a_gpu, b_gpu)
            cp.cuda.Stream.null.synchronize()
            gpu_time = time.time() - start
            gpu_times.append(gpu_time)

        gpu_time_median = np.median(gpu_times)
        print(f"GPU Time (CuPy):  {gpu_time_median:.4f}s")

        speedup = cpu_time_median / gpu_time_median
        print(f"Speedup: {speedup:.2f}x\n")

        # FLOPSè¨ˆç®—
        flops = 2 * (size ** 3)  # è¡Œåˆ—ç©: 2N^3 operations
        cpu_gflops = flops / cpu_time_median / 1e9
        gpu_gflops = flops / gpu_time_median / 1e9

        results.append({
            'Size': f'{size}x{size}',
            'CPU Time (s)': f'{cpu_time_median:.4f}',
            'GPU Time (s)': f'{gpu_time_median:.4f}',
            'Speedup': f'{speedup:.2f}x',
            'CPU GFLOPS': f'{cpu_gflops:.2f}',
            'GPU GFLOPS': f'{gpu_gflops:.2f}'
        })

    # çµæœã‚’ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
    print("=" * 80)
    print("=== ç·åˆçµæœ ===")
    print("=" * 80)
    df = pd.DataFrame(results)
    print(df.to_string(index=False))

    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¨å®š
    print("\n" + "=" * 80)
    print("=== ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¨å®š ===")
    print("=" * 80)
    for size in sizes:
        matrix_size_mb = (size * size * 4) / (1024 * 1024)  # float32 = 4 bytes
        total_mb = matrix_size_mb * 3  # A, B, C
        print(f"{size}x{size}: 1è¡Œåˆ— = {matrix_size_mb:.2f} MB, åˆè¨ˆ = {total_mb:.2f} MB")

if __name__ == "__main__":
    benchmark_matrix_multiply()

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ä¾‹:
# === è¡Œåˆ—ç© æ€§èƒ½æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ===
#
# å„ã‚µã‚¤ã‚ºã§5å›æ¸¬å®šã—ã€ä¸­å¤®å€¤ã‚’æ¡ç”¨
#
# --- Matrix Size: 1000x1000 ---
# CPU Time (NumPy): 0.0521s
# GPU Time (CuPy):  0.0012s
# Speedup: 43.42x
#
# --- Matrix Size: 2000x2000 ---
# CPU Time (NumPy): 0.3842s
# GPU Time (CuPy):  0.0043s
# Speedup: 89.35x
#
# --- Matrix Size: 5000x5000 ---
# CPU Time (NumPy): 5.9213s
# GPU Time (CuPy):  0.0421s
# Speedup: 140.65x
#
# --- Matrix Size: 10000x10000 ---
# CPU Time (NumPy): 48.2156s
# GPU Time (CuPy):  0.2834s
# Speedup: 170.16x
#
# ================================================================================
# === ç·åˆçµæœ ===
# ================================================================================
#          Size  CPU Time (s)  GPU Time (s)   Speedup  CPU GFLOPS  GPU GFLOPS
#   1000x1000        0.0521        0.0012   43.42x       38.37     1665.56
#   2000x2000        0.3842        0.0043   89.35x       41.68     3724.19
#   5000x5000        5.9213        0.0421  140.65x       42.19     5932.07
# 10000x10000       48.2156        0.2834  170.16x       41.42     7047.99
#
# ================================================================================
# === ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¨å®š ===
# ================================================================================
# 1000x1000: 1è¡Œåˆ— = 3.81 MB, åˆè¨ˆ = 11.44 MB
# 2000x2000: 1è¡Œåˆ— = 15.26 MB, åˆè¨ˆ = 45.78 MB
# 5000x5000: 1è¡Œåˆ— = 95.37 MB, åˆè¨ˆ = 286.10 MB
# 10000x10000: 1è¡Œåˆ— = 381.47 MB, åˆè¨ˆ = 1144.41 MB
</code></pre>

            <div class="info-box">
                <strong>ğŸ’¡ è¡Œåˆ—ã‚µã‚¤ã‚ºã¨é«˜é€ŸåŒ–ç‡ã®é–¢ä¿‚</strong>
                <p>
                    è¡Œåˆ—ã‚µã‚¤ã‚ºãŒå¤§ãã„ã»ã©ã€é«˜é€ŸåŒ–ç‡ãŒå‘ä¸Šã—ã¾ã™ã€‚å°ã•ã„è¡Œåˆ—ï¼ˆ1000x1000ï¼‰ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿è»¢é€ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã§é«˜é€ŸåŒ–ãŒé™å®šçš„ï¼ˆ50å€ç¨‹åº¦ï¼‰ã§ã™ãŒã€å¤§ãã„è¡Œåˆ—ï¼ˆ10000x10000ï¼‰ã§ã¯170å€ä»¥ä¸Šã®é«˜é€ŸåŒ–ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚<strong>è¨ˆç®—é‡ > ãƒ‡ãƒ¼ã‚¿è»¢é€é‡</strong>ã®ãƒãƒ©ãƒ³ã‚¹ãŒé‡è¦ã§ã™ã€‚
                </p>
            </div>
        </section>

        <section id="learning-objectives">
            <h2>1.7 å­¦ç¿’ç›®æ¨™ã®ç¢ºèª</h2>

            <p>æœ¬ç« ã‚’å®Œäº†ã—ãŸæ™‚ç‚¹ã§ã€ä»¥ä¸‹ã®å­¦ç¿’ç›®æ¨™ã‚’é”æˆã§ãã¦ã„ã‚‹ã‹ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚</p>

            <h3>ãƒ¬ãƒ™ãƒ«1: åŸºæœ¬ç†è§£</h3>
            <ul>
                <li>âœ… GPUã¨CPUã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹ï¼ˆä¸¦åˆ—å‡¦ç† vs é€æ¬¡å‡¦ç†ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæŒ‡å‘ vs ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æŒ‡å‘ï¼‰</li>
                <li>âœ… CUDAã‚³ã‚¢ã¨ãƒ†ãƒ³ã‚½ãƒ«ã‚³ã‚¢ã®å½¹å‰²ã‚’ç†è§£ã—ã¦ã„ã‚‹ï¼ˆCUDAã‚³ã‚¢: æ±ç”¨æ¼”ç®—ã€ãƒ†ãƒ³ã‚½ãƒ«ã‚³ã‚¢: è¡Œåˆ—ç©æ¼”ç®—å°‚ç”¨ï¼‰</li>
                <li>âœ… ãƒ¡ãƒ¢ãƒªéšå±¤ã®ç¨®é¡ã¨ç‰¹æ€§ã‚’èª¬æ˜ã§ãã‚‹ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ã€å…±æœ‰ã€ãƒ¬ã‚¸ã‚¹ã‚¿ã€L1/L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€ã‚³ãƒ³ã‚¹ã‚¿ãƒ³ãƒˆã€ãƒ†ã‚¯ã‚¹ãƒãƒ£ï¼‰</li>
            </ul>

            <h3>ãƒ¬ãƒ™ãƒ«2: å®Ÿè·µã‚¹ã‚­ãƒ«</h3>
            <ul>
                <li>âœ… Pythonã§ GPUæƒ…å ±ã‚’å–å¾—ã§ãã‚‹ï¼ˆPyCUDAã€PyTorchã‚’ä½¿ã£ãŸãƒ‡ãƒã‚¤ã‚¹æƒ…å ±å–å¾—ï¼‰</li>
                <li>âœ… ç°¡å˜ãªGPUè¨ˆç®—ã®æ€§èƒ½æ¸¬å®šãŒã§ãã‚‹ï¼ˆCuPyã§è¡Œåˆ—ç©ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€CPU vs GPUæ¯”è¼ƒï¼‰</li>
                <li>âœ… nvidia-smiã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã£ã¦GPUçŠ¶æ…‹ã‚’ç¢ºèªã§ãã‚‹ï¼ˆåˆ©ç”¨ç‡ã€ãƒ¡ãƒ¢ãƒªã€æ¸©åº¦ã€ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±ï¼‰</li>
            </ul>

            <h3>ãƒ¬ãƒ™ãƒ«3: å¿œç”¨åŠ›</h3>
            <ul>
                <li>âœ… ææ–™ç§‘å­¦è¨ˆç®—ã®GPUé©ç”¨å¯èƒ½æ€§ã‚’åˆ¤æ–­ã§ãã‚‹ï¼ˆä¸¦åˆ—åŒ–å¯èƒ½æ€§ã€è¨ˆç®—é‡ã€ãƒ¡ãƒ¢ãƒªè¦ä»¶ã®è©•ä¾¡ï¼‰</li>
                <li>âœ… CPU vs GPUæ€§èƒ½æ¯”è¼ƒã‚’é©åˆ‡ã«å®Ÿæ–½ã§ãã‚‹ï¼ˆWarm-upã€åŒæœŸã€è¤‡æ•°å›æ¸¬å®šã€ä¸­å¤®å€¤æ¡ç”¨ï¼‰</li>
                <li>âœ… ãƒ¡ãƒ¢ãƒªãƒãƒ³ãƒ‰å¹…ãŒæ€§èƒ½ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’è©•ä¾¡ã§ãã‚‹ï¼ˆmemory-bound vs compute-bound ã®åˆ¤æ–­ï¼‰</li>
            </ul>
        </section>

        <section id="exercises">
            <h2>2.5 æ¼”ç¿’å•é¡Œ</h2>

            <div class="exercise-box">
                <h4>Easy 1: ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨ˆç®— <span class="difficulty easy">åŸºç¤</span></h4>
                <p><strong>å•é¡Œ</strong>: ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º256ã€ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º100ã®ã¨ãã€ãƒ–ãƒ­ãƒƒã‚¯5ã®ã‚¹ãƒ¬ãƒƒãƒ‰128ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—ã—ãªã•ã„ã€‚</p>
                <details>
                    <summary>è§£ç­”ã‚’è¦‹ã‚‹</summary>
                    <p><strong>è§£ç­”</strong>:</p>
                    <pre><code class="language-c">// CUDAã‚«ãƒ¼ãƒãƒ«å†…ã§ã®è¨ˆç®—
int idx = blockIdx.x * blockDim.x + threadIdx.x;

// å…·ä½“çš„ãªå€¤
// blockIdx.x = 5 (ãƒ–ãƒ­ãƒƒã‚¯5)
// blockDim.x = 256 (ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º)
// threadIdx.x = 128 (ã‚¹ãƒ¬ãƒƒãƒ‰128)

idx = 5 * 256 + 128 = 1280 + 128 = 1408
</code></pre>
                    <p><strong>ç­”ãˆ</strong>: ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯<strong>1408</strong></p>
                    <p><strong>è£œè¶³</strong>: ã“ã®è¨ˆç®—å¼ã¯1Dã‚°ãƒªãƒƒãƒ‰ã®æ¨™æº–ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚2Dã‚„3Dã‚°ãƒªãƒƒãƒ‰ã§ã¯ã€ã‚ˆã‚Šè¤‡é›‘ãªè¨ˆç®—ãŒå¿…è¦ã§ã™ï¼ˆä¾‹: <code>row = blockIdx.y * blockDim.y + threadIdx.y</code>ï¼‰ã€‚</p>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Easy 2: nvidia-smiå®Ÿè¡Œ <span class="difficulty easy">åŸºç¤</span></h4>
                <p><strong>å•é¡Œ</strong>: ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ <code>nvidia-smi</code> ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã€GPUåˆ©ç”¨ç‡ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç¢ºèªã—ãªã•ã„ã€‚</p>
                <details>
                    <summary>è§£ç­”ã‚’è¦‹ã‚‹</summary>
                    <p><strong>å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰</strong>:</p>
                    <pre><code class="language-bash">nvidia-smi</code></pre>
                    <p><strong>ç¢ºèªé …ç›®</strong>:</p>
                    <ul>
                        <li><strong>GPU-Util</strong>: GPUåˆ©ç”¨ç‡ï¼ˆ0-100%ï¼‰</li>
                        <li><strong>Memory-Usage</strong>: ä½¿ç”¨ä¸­ãƒ¡ãƒ¢ãƒª / ç·ãƒ¡ãƒ¢ãƒªï¼ˆä¾‹: 2048MiB / 24576MiBï¼‰</li>
                        <li><strong>Temperature</strong>: GPUæ¸©åº¦ï¼ˆä¾‹: 45Cï¼‰</li>
                        <li><strong>Power</strong>: æ¶ˆè²»é›»åŠ› / æœ€å¤§é›»åŠ›ï¼ˆä¾‹: 75W / 350Wï¼‰</li>
                    </ul>
                    <p><strong>å¿œç”¨</strong>: <code>watch -n 1 nvidia-smi</code> ã§1ç§’ã”ã¨ã«ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã—ã¦ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã§ãã¾ã™ã€‚</p>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Easy 3: CuPyè¡Œåˆ—ç© <span class="difficulty easy">åŸºç¤</span></h4>
                <p><strong>å•é¡Œ</strong>: CuPyã‚’ä½¿ã£ã¦2ã¤ã®1000x1000è¡Œåˆ—ã®ç©ã‚’è¨ˆç®—ã—ã€å®Ÿè¡Œæ™‚é–“ã‚’æ¸¬å®šã—ãªã•ã„ã€‚</p>
                <details>
                    <summary>è§£ç­”ã‚’è¦‹ã‚‹</summary>
                    <pre><code class="language-python">import cupy as cp
import time

# è¡Œåˆ—ã®ç”Ÿæˆ
a = cp.random.rand(1000, 1000, dtype=cp.float32)
b = cp.random.rand(1000, 1000, dtype=cp.float32)

# Warm-up
_ = cp.dot(a, b)
cp.cuda.Stream.null.synchronize()

# æ¸¬å®š
start = time.time()
c = cp.dot(a, b)
cp.cuda.Stream.null.synchronize()
elapsed = time.time() - start

print(f"å®Ÿè¡Œæ™‚é–“: {elapsed:.6f}s")
print(f"çµæœã®å½¢çŠ¶: {c.shape}")

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ä¾‹:
# å®Ÿè¡Œæ™‚é–“: 0.001234s
# çµæœã®å½¢çŠ¶: (1000, 1000)
</code></pre>
                    <p><strong>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</strong>: <code>cp.cuda.Stream.null.synchronize()</code> ã§GPUè¨ˆç®—ã®å®Œäº†ã‚’å¾…ã¤ã“ã¨ãŒå¿…é ˆã§ã™ã€‚ã“ã‚ŒãŒãªã„ã¨ã€CPUå´ã®å‡¦ç†ãŒå…ˆã«é€²ã‚“ã§ã—ã¾ã„ã€æ­£ç¢ºãªæ™‚é–“æ¸¬å®šãŒã§ãã¾ã›ã‚“ã€‚</p>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Medium 1: ãƒ¡ãƒ¢ãƒªéšå±¤ã®é•ã„ <span class="difficulty medium">å¿œç”¨</span></h4>
                <p><strong>å•é¡Œ</strong>: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªã¨å…±æœ‰ãƒ¡ãƒ¢ãƒªã®é•ã„ã‚’èª¬æ˜ã—ã€ãã‚Œãã‚Œã®ç”¨é€”ã‚’è¿°ã¹ãªã•ã„ã€‚</p>
                <details>
                    <summary>è§£ç­”ã‚’è¦‹ã‚‹</summary>
                    <p><strong>è§£ç­”</strong>:</p>
                    <table>
                        <thead>
                            <tr>
                                <th>ç‰¹æ€§</th>
                                <th>ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒª</th>
                                <th>å…±æœ‰ãƒ¡ãƒ¢ãƒª</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>ã‚µã‚¤ã‚º</td>
                                <td>16-80 GBï¼ˆå¤§å®¹é‡ï¼‰</td>
                                <td>48-164 KB/SMï¼ˆå°å®¹é‡ï¼‰</td>
                            </tr>
                            <tr>
                                <td>ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·</td>
                                <td>æ•°ç™¾ã‚¯ãƒ­ãƒƒã‚¯ï¼ˆé…ã„ï¼‰</td>
                                <td>æ•°ã‚¯ãƒ­ãƒƒã‚¯ï¼ˆé€Ÿã„ï¼‰</td>
                            </tr>
                            <tr>
                                <td>ã‚¹ã‚³ãƒ¼ãƒ—</td>
                                <td>å…¨ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½</td>
                                <td>åŒã˜ãƒ–ãƒ­ãƒƒã‚¯å†…ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã®ã¿</td>
                            </tr>
                            <tr>
                                <td>ç”¨é€”</td>
                                <td>å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã€å¤§è¦æ¨¡é…åˆ—</td>
                                <td>ã‚¹ãƒ¬ãƒƒãƒ‰é–“ãƒ‡ãƒ¼ã‚¿å…±æœ‰ã€ä¸€æ™‚ãƒãƒƒãƒ•ã‚¡</td>
                            </tr>
                        </tbody>
                    </table>
                    <p><strong>å…·ä½“ä¾‹</strong>:</p>
                    <ul>
                        <li><strong>ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒª</strong>: è¡Œåˆ—ãƒ‡ãƒ¼ã‚¿ã€å…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ«ã€è¨ˆç®—çµæœã®æ ¼ç´</li>
                        <li><strong>å…±æœ‰ãƒ¡ãƒ¢ãƒª</strong>: ã‚¿ã‚¤ãƒ«è¡Œåˆ—ç©ã§ã®éƒ¨åˆ†è¡Œåˆ—ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³æ¼”ç®—ã®ä¸­é–“çµæœ</li>
                    </ul>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Medium 2: Compute Capabilityèª¿æŸ» <span class="difficulty medium">å¿œç”¨</span></h4>
                <p><strong>å•é¡Œ</strong>: Compute Capability 7.5 ã¨ 8.0 ã®é•ã„ã‚’èª¿ã¹ã€ä¸»è¦ãªæ©Ÿèƒ½å·®ã‚’3ã¤æŒ™ã’ãªã•ã„ã€‚</p>
                <details>
                    <summary>è§£ç­”ã‚’è¦‹ã‚‹</summary>
                    <p><strong>è§£ç­”</strong>:</p>
                    <table>
                        <thead>
                            <tr>
                                <th>æ©Ÿèƒ½</th>
                                <th>7.5ï¼ˆTuringï¼‰</th>
                                <th>8.0ï¼ˆAmpere A100ï¼‰</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>ãƒ†ãƒ³ã‚½ãƒ«ã‚³ã‚¢</td>
                                <td>ç¬¬2ä¸–ä»£ï¼ˆFP16, INT8ï¼‰</td>
                                <td>ç¬¬3ä¸–ä»£ï¼ˆFP16, TF32, BF16, INT8ï¼‰</td>
                            </tr>
                            <tr>
                                <td>TF32ã‚µãƒãƒ¼ãƒˆ</td>
                                <td>âŒ ãªã—</td>
                                <td>âœ… ã‚ã‚Šï¼ˆ10bitä»®æ•°ã€8bitæŒ‡æ•°ï¼‰</td>
                            </tr>
                            <tr>
                                <td>MIGï¼ˆMulti-Instance GPUï¼‰</td>
                                <td>âŒ ãªã—</td>
                                <td>âœ… ã‚ã‚Šï¼ˆ1GPU â†’ æœ€å¤§7ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼‰</td>
                            </tr>
                            <tr>
                                <td>FP64æ€§èƒ½ï¼ˆå¯¾FP32æ¯”ï¼‰</td>
                                <td>1/32</td>
                                <td>1/2ï¼ˆA100ã®ã¿ï¼‰</td>
                            </tr>
                        </tbody>
                    </table>
                    <p><strong>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</strong>:</p>
                    <ul>
                        <li><strong>TF32</strong>ã¯ã€FP32ã‚³ãƒ¼ãƒ‰ã‚’å¤‰æ›´ã›ãšã«è‡ªå‹•ã§ä½¿ãˆã‚‹æ··åˆç²¾åº¦æ¼”ç®—ã§ã€æ·±å±¤å­¦ç¿’ã§8-10å€ã®é«˜é€ŸåŒ–</li>
                        <li><strong>MIG</strong>ã¯ã€1ã¤ã®GPUã‚’è¤‡æ•°ã®ç‹¬ç«‹ã—ãŸGPUã¨ã—ã¦åˆ†å‰²ã—ã€è¤‡æ•°ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§å…±æœ‰å¯èƒ½</li>
                        <li><strong>FP64æ€§èƒ½</strong>ã®å‘ä¸Šã¯ã€ç§‘å­¦æŠ€è¡“è¨ˆç®—ï¼ˆDFTç­‰ï¼‰ã§é‡è¦</li>
                    </ul>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Medium 3: å›ºæœ‰å€¤è¨ˆç®—ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ <span class="difficulty medium">å¿œç”¨</span></h4>
                <p><strong>å•é¡Œ</strong>: CuPyã¨NumPyã§10000x10000è¡Œåˆ—ã®å›ºæœ‰å€¤ã‚’è¨ˆç®—ã—ã€é€Ÿåº¦ã‚’æ¯”è¼ƒã—ãªã•ã„ã€‚</p>
                <details>
                    <summary>è§£ç­”ã‚’è¦‹ã‚‹</summary>
                    <pre><code class="language-python">import numpy as np
import cupy as cp
import time

size = 10000

print("=== å›ºæœ‰å€¤è¨ˆç®—ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ===\n")

# å¯¾ç§°è¡Œåˆ—ã‚’ç”Ÿæˆï¼ˆå®Ÿå›ºæœ‰å€¤ã‚’ä¿è¨¼ï¼‰
print("å¯¾ç§°è¡Œåˆ—ã‚’ç”Ÿæˆä¸­...")
A_cpu = np.random.rand(size, size).astype(np.float64)
A_cpu = (A_cpu + A_cpu.T) / 2  # å¯¾ç§°åŒ–

print(f"CPUï¼ˆNumPyï¼‰ã§ã®å›ºæœ‰å€¤è¨ˆç®—é–‹å§‹...")
start = time.time()
eigenvalues_cpu = np.linalg.eigvalsh(A_cpu)
cpu_time = time.time() - start
print(f"CPU Time: {cpu_time:.2f}s\n")

# GPU
A_gpu = cp.asarray(A_cpu)

print(f"GPUï¼ˆCuPyï¼‰ã§ã®å›ºæœ‰å€¤è¨ˆç®—é–‹å§‹...")
start = time.time()
eigenvalues_gpu = cp.linalg.eigvalsh(A_gpu)
cp.cuda.Stream.null.synchronize()
gpu_time = time.time() - start
print(f"GPU Time: {gpu_time:.2f}s\n")

speedup = cpu_time / gpu_time
print(f"Speedup: {speedup:.2f}x")

# çµæœã®æ¤œè¨¼ï¼ˆæœ€å¤§ãƒ»æœ€å°å›ºæœ‰å€¤ï¼‰
print(f"\næœ€å¤§å›ºæœ‰å€¤ (CPU): {eigenvalues_cpu[-1]:.6f}")
print(f"æœ€å¤§å›ºæœ‰å€¤ (GPU): {float(eigenvalues_gpu[-1]):.6f}")
print(f"æœ€å°å›ºæœ‰å€¤ (CPU): {eigenvalues_cpu[0]:.6f}")
print(f"æœ€å°å›ºæœ‰å€¤ (GPU): {float(eigenvalues_gpu[0]):.6f}")

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ä¾‹:
# === å›ºæœ‰å€¤è¨ˆç®—ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ===
#
# å¯¾ç§°è¡Œåˆ—ã‚’ç”Ÿæˆä¸­...
# CPUï¼ˆNumPyï¼‰ã§ã®å›ºæœ‰å€¤è¨ˆç®—é–‹å§‹...
# CPU Time: 127.45s
#
# GPUï¼ˆCuPyï¼‰ã§ã®å›ºæœ‰å€¤è¨ˆç®—é–‹å§‹...
# GPU Time: 8.32s
#
# Speedup: 15.32x
#
# æœ€å¤§å›ºæœ‰å€¤ (CPU): 5003.234567
# æœ€å¤§å›ºæœ‰å€¤ (GPU): 5003.234512
# æœ€å°å›ºæœ‰å€¤ (CPU): -2.451234
# æœ€å°å›ºæœ‰å€¤ (GPU): -2.451238
</code></pre>
                    <p><strong>è€ƒå¯Ÿ</strong>: å›ºæœ‰å€¤è¨ˆç®—ã¯è¡Œåˆ—ç©ã»ã©ä¸¦åˆ—åŒ–åŠ¹ç‡ãŒé«˜ããªã„ãŸã‚ã€é«˜é€ŸåŒ–ç‡ã¯10-20å€ç¨‹åº¦ã§ã™ã€‚</p>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Medium 4: ãƒ¡ãƒ¢ãƒªãƒãƒ³ãƒ‰å¹…æ¸¬å®š <span class="difficulty medium">å¿œç”¨</span></h4>
                <p><strong>å•é¡Œ</strong>: ãƒ¡ãƒ¢ãƒªãƒãƒ³ãƒ‰å¹…ã‚’æ¸¬å®šã—ã€ç†è«–å€¤ï¼ˆã‚«ã‚¿ãƒ­ã‚°ã‚¹ãƒšãƒƒã‚¯ï¼‰ã¨æ¯”è¼ƒã—ãªã•ã„ã€‚</p>
                <details>
                    <summary>è§£ç­”ã‚’è¦‹ã‚‹</summary>
                    <p><strong>è§£ç­”</strong>: ã‚³ãƒ¼ãƒ‰ä¾‹3ã‚’å‚ç…§ã€‚æ¸¬å®šã—ãŸãƒãƒ³ãƒ‰å¹…ã¨ã€GPUã®ã‚«ã‚¿ãƒ­ã‚°ã‚¹ãƒšãƒƒã‚¯ï¼ˆnvidia-smiã¾ãŸã¯ä»•æ§˜æ›¸ã§ç¢ºèªï¼‰ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚</p>
                    <pre><code class="language-python"># ã‚³ãƒ¼ãƒ‰ä¾‹3ã‚’å®Ÿè¡Œã—ã€ä»¥ä¸‹ã‚’ç¢ºèª
# 1. Device to Device (D2D) ãƒãƒ³ãƒ‰å¹…ã‚’æ¸¬å®š
# 2. GPUã®ç†è«–ãƒãƒ³ãƒ‰å¹…ã‚’ç¢ºèªï¼ˆä¾‹: RTX 3090 = 936 GB/sï¼‰
# 3. é”æˆç‡ã‚’è¨ˆç®—: (å®Ÿæ¸¬å€¤ / ç†è«–å€¤) * 100

# çµæœã®ä¾‹:
# å®Ÿæ¸¬å€¤ï¼ˆD2Dï¼‰: 833.33 GB/s
# ç†è«–å€¤: 936 GB/s
# é”æˆç‡: 89.0%
</code></pre>
                    <p><strong>è€ƒå¯Ÿ</strong>:</p>
                    <ul>
                        <li><strong>80-90%</strong>: éå¸¸ã«è‰¯å¥½ï¼ˆã‚·ã‚¹ãƒ†ãƒ ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’è€ƒæ…®ã™ã‚‹ã¨ä¸Šé™ï¼‰</li>
                        <li><strong>60-80%</strong>: è‰¯å¥½ï¼ˆä¸€èˆ¬çš„ãªé”æˆç‡ï¼‰</li>
                        <li><strong>&lt;60%</strong>: è¦æ”¹å–„ï¼ˆéé€£ç¶šã‚¢ã‚¯ã‚»ã‚¹ã€å°ã•ã„ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºç­‰ãŒåŸå› ï¼‰</li>
                    </ul>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Hard 1: ææ–™ç§‘å­¦è¨ˆç®—ã®GPUé©ç”¨åˆ¤æ–­ <span class="difficulty hard">ç™ºå±•</span></h4>
                <p><strong>å•é¡Œ</strong>: ææ–™ç§‘å­¦ã®è¨ˆç®—ã‚¿ã‚¹ã‚¯ï¼ˆDFTã€MDã€æ©Ÿæ¢°å­¦ç¿’ï¼‰ãã‚Œãã‚Œã«ã¤ã„ã¦ã€GPUåŒ–ã®åŠ¹æœãŒé«˜ã„ç†ç”±ã‚’è€ƒå¯Ÿã—ãªã•ã„ã€‚</p>
                <details>
                    <summary>è§£ç­”ã‚’è¦‹ã‚‹</summary>
                    <p><strong>è§£ç­”</strong>:</p>

                    <h5>1. å¯†åº¦æ±é–¢æ•°ç†è«–ï¼ˆDFTï¼‰</h5>
                    <p><strong>GPUé©åˆåº¦: â˜…â˜…â˜…â˜…â˜†ï¼ˆé«˜ï¼‰</strong></p>
                    <ul>
                        <li><strong>ä¸¦åˆ—åŒ–å¯èƒ½ã‚¿ã‚¹ã‚¯</strong>: FFTï¼ˆé«˜é€Ÿãƒ•ãƒ¼ãƒªã‚¨å¤‰æ›ï¼‰ã€è¡Œåˆ—å¯¾è§’åŒ–ã€é›»å­å¯†åº¦è¨ˆç®—</li>
                        <li><strong>é«˜é€ŸåŒ–ç‡</strong>: 10-50å€ï¼ˆkç‚¹ä¸¦åˆ—ã€ãƒãƒ³ãƒ‰ä¸¦åˆ—ã®çµ„ã¿åˆã‚ã›ï¼‰</li>
                        <li><strong>ç†ç”±</strong>: å¤§è¦æ¨¡è¡Œåˆ—æ¼”ç®—ãŒä¸­å¿ƒã§ã€cuBLAS/cuSOLVERã§æœ€é©åŒ–å¯èƒ½</li>
                        <li><strong>åˆ¶ç´„</strong>: ãƒ¡ãƒ¢ãƒªå®¹é‡ï¼ˆå¤§è¦æ¨¡ç³»ã§ã¯48GBä»¥ä¸Šæ¨å¥¨ï¼‰</li>
                    </ul>

                    <h5>2. åˆ†å­å‹•åŠ›å­¦ï¼ˆMDï¼‰</h5>
                    <p><strong>GPUé©åˆåº¦: â˜…â˜…â˜…â˜…â˜…ï¼ˆéå¸¸ã«é«˜ï¼‰</strong></p>
                    <ul>
                        <li><strong>ä¸¦åˆ—åŒ–å¯èƒ½ã‚¿ã‚¹ã‚¯</strong>: åŠ›å ´è¨ˆç®—ï¼ˆåŸå­ãƒšã‚¢ã”ã¨ç‹¬ç«‹ï¼‰ã€åº§æ¨™æ›´æ–°ã€è¿‘æ¥ãƒªã‚¹ãƒˆä½œæˆ</li>
                        <li><strong>é«˜é€ŸåŒ–ç‡</strong>: 20-100å€ï¼ˆæ•°ä¸‡ã€œæ•°ç™¾ä¸‡åŸå­ç³»ï¼‰</li>
                        <li><strong>ç†ç”±</strong>: åŸå­æ•°Ã—åŸå­æ•°ã®ç‹¬ç«‹è¨ˆç®—ã§ã€å®Œå…¨ä¸¦åˆ—åŒ–å¯èƒ½</li>
                        <li><strong>æœ€é©æ¡ä»¶</strong>: åŸå­æ•°ãŒå¤šã„ï¼ˆ>10,000åŸå­ï¼‰ã€é•·æ™‚é–“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</li>
                    </ul>

                    <h5>3. æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«è¨“ç·´</h5>
                    <p><strong>GPUé©åˆåº¦: â˜…â˜…â˜…â˜…â˜…ï¼ˆéå¸¸ã«é«˜ï¼‰</strong></p>
                    <ul>
                        <li><strong>ä¸¦åˆ—åŒ–å¯èƒ½ã‚¿ã‚¹ã‚¯</strong>: è¡Œåˆ—ç©ï¼ˆå…¨çµåˆå±¤ï¼‰ã€ç•³ã¿è¾¼ã¿ã€å‹¾é…è¨ˆç®—</li>
                        <li><strong>é«˜é€ŸåŒ–ç‡</strong>: 10-100å€ï¼ˆæ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰</li>
                        <li><strong>ç†ç”±</strong>: ãƒ†ãƒ³ã‚½ãƒ«ã‚³ã‚¢ã«ã‚ˆã‚‹è¡Œåˆ—ç©é«˜é€ŸåŒ–ã€ãƒãƒƒãƒä¸¦åˆ—å‡¦ç†</li>
                        <li><strong>æœ€é©æ¡ä»¶</strong>: ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå¤§ãã„ã€ãƒ¢ãƒ‡ãƒ«ãŒæ·±ã„</li>
                    </ul>

                    <p><strong>ç·åˆåˆ¤æ–­åŸºæº–</strong>:</p>
                    <table>
                        <thead>
                            <tr>
                                <th>æ¡ä»¶</th>
                                <th>GPUé©åˆåº¦</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>è¨ˆç®—é‡ > ãƒ‡ãƒ¼ã‚¿è»¢é€é‡</td>
                                <td>âœ… é«˜</td>
                            </tr>
                            <tr>
                                <td>ã‚¿ã‚¹ã‚¯ãŒç‹¬ç«‹ï¼ˆãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—åŒ–å¯èƒ½ï¼‰</td>
                                <td>âœ… é«˜</td>
                            </tr>
                            <tr>
                                <td>è¡Œåˆ—æ¼”ç®—ãŒä¸­å¿ƒ</td>
                                <td>âœ… é«˜</td>
                            </tr>
                            <tr>
                                <td>è¤‡é›‘ãªåˆ†å²å‡¦ç†ãŒå¤šã„</td>
                                <td>âŒ ä½</td>
                            </tr>
                            <tr>
                                <td>ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå°ã•ã„ï¼ˆ<1MBï¼‰</td>
                                <td>âŒ ä½</td>
                            </tr>
                        </tbody>
                    </table>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Hard 2: ãƒãƒ«ãƒGPUæˆ¦ç•¥ <span class="difficulty hard">ç™ºå±•</span></h4>
                <p><strong>å•é¡Œ</strong>: ãƒãƒ«ãƒGPUã‚·ã‚¹ãƒ†ãƒ ã§è¤‡æ•°ã®GPUã«è¨ˆç®—ã‚’åˆ†æ•£ã•ã›ã‚‹æˆ¦ç•¥ã‚’3ã¤ææ¡ˆã—ãªã•ã„ã€‚</p>
                <details>
                    <summary>è§£ç­”ã‚’è¦‹ã‚‹</summary>
                    <p><strong>è§£ç­”</strong>:</p>

                    <h5>æˆ¦ç•¥1: ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ï¼ˆData Parallelismï¼‰</h5>
                    <ul>
                        <li><strong>æ–¹æ³•</strong>: åŒã˜ãƒ¢ãƒ‡ãƒ«ã‚’å„GPUã«ã‚³ãƒ”ãƒ¼ã—ã€ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã¦ä¸¦åˆ—å‡¦ç†</li>
                        <li><strong>é©ç”¨ä¾‹</strong>: æ©Ÿæ¢°å­¦ç¿’ã®è¨“ç·´ï¼ˆå„GPUãŒç•°ãªã‚‹ãƒãƒƒãƒã‚’å‡¦ç†ï¼‰</li>
                        <li><strong>åˆ©ç‚¹</strong>: å®Ÿè£…ãŒç°¡å˜ã€ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆN GPU â†’ Nå€é€Ÿï¼‰</li>
                        <li><strong>æ¬ ç‚¹</strong>: ãƒ¢ãƒ‡ãƒ«ãŒå„GPUã®ãƒ¡ãƒ¢ãƒªã«åã¾ã‚‹å¿…è¦ãŒã‚ã‚‹</li>
                        <li><strong>å®Ÿè£…</strong>: PyTorchã® <code>DistributedDataParallel</code></li>
                    </ul>

                    <h5>æˆ¦ç•¥2: ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—ï¼ˆModel Parallelismï¼‰</h5>
                    <ul>
                        <li><strong>æ–¹æ³•</strong>: ãƒ¢ãƒ‡ãƒ«ã‚’è¤‡æ•°GPUã«åˆ†å‰²ï¼ˆå±¤ã”ã¨ã€ã¾ãŸã¯ãƒ†ãƒ³ã‚½ãƒ«åˆ†å‰²ï¼‰</li>
                        <li><strong>é©ç”¨ä¾‹</strong>: å·¨å¤§ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆGPT-3ç­‰ï¼‰</li>
                        <li><strong>åˆ©ç‚¹</strong>: 1GPUã«åã¾ã‚‰ãªã„å·¨å¤§ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œå¯èƒ½</li>
                        <li><strong>æ¬ ç‚¹</strong>: GPUé–“é€šä¿¡ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã‚Šã‚„ã™ã„</li>
                        <li><strong>å®Ÿè£…</strong>: PyTorchã® <code>torch.nn.parallel</code> ã¾ãŸã¯ Megatron-LM</li>
                    </ul>

                    <h5>æˆ¦ç•¥3: ç©ºé–“åˆ†å‰²ä¸¦åˆ—ï¼ˆSpatial Domain Decompositionï¼‰</h5>
                    <ul>
                        <li><strong>æ–¹æ³•</strong>: ç‰©ç†ç©ºé–“ã‚’é ˜åŸŸåˆ†å‰²ã—ã€å„GPUãŒæ‹…å½“é ˜åŸŸã‚’è¨ˆç®—</li>
                        <li><strong>é©ç”¨ä¾‹</strong>: åˆ†å­å‹•åŠ›å­¦ï¼ˆåŸå­åº§æ¨™ã‚’ç©ºé–“åˆ†å‰²ï¼‰ã€æµä½“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</li>
                        <li><strong>åˆ©ç‚¹</strong>: å¤§è¦æ¨¡ç³»ã‚’åŠ¹ç‡çš„ã«å‡¦ç†ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’åˆ†æ•£</li>
                        <li><strong>æ¬ ç‚¹</strong>: å¢ƒç•Œæ¡ä»¶ã®é€šä¿¡ãŒå¿…è¦ã€è² è·ãƒãƒ©ãƒ³ã‚¹ã®èª¿æ•´ãŒé‡è¦</li>
                        <li><strong>å®Ÿè£…</strong>: LAMMPS-GPUã® <code>domain decomposition</code></li>
                    </ul>

                    <p><strong>é¸æŠåŸºæº–</strong>:</p>
                    <table>
                        <thead>
                            <tr>
                                <th>ã‚¿ã‚¹ã‚¯ç¨®åˆ¥</th>
                                <th>æ¨å¥¨æˆ¦ç•¥</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>æ©Ÿæ¢°å­¦ç¿’è¨“ç·´ï¼ˆä¸­è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼‰</td>
                                <td>ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—</td>
                            </tr>
                            <tr>
                                <td>å·¨å¤§ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</td>
                                <td>ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ— + ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—</td>
                            </tr>
                            <tr>
                                <td>åˆ†å­å‹•åŠ›å­¦</td>
                                <td>ç©ºé–“åˆ†å‰²ä¸¦åˆ—</td>
                            </tr>
                            <tr>
                                <td>DFTè¨ˆç®—</td>
                                <td>kç‚¹ä¸¦åˆ—ï¼ˆãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ã®ä¸€ç¨®ï¼‰</td>
                            </tr>
                        </tbody>
                    </table>
                </details>
            </div>

            <div class="exercise-box">
                <h4>Hard 3: GPUãƒ¡ãƒ¢ãƒªä¸è¶³å¯¾å‡¦æ³• <span class="difficulty hard">ç™ºå±•</span></h4>
                <p><strong>å•é¡Œ</strong>: GPUãƒ¡ãƒ¢ãƒªãŒä¸è¶³ã™ã‚‹å ´åˆã®å¯¾å‡¦æ³•ã‚’5ã¤æŒ™ã’ã€ãã‚Œãã‚Œã®åˆ©ç‚¹ãƒ»æ¬ ç‚¹ã‚’è¿°ã¹ãªã•ã„ã€‚</p>
                <details>
                    <summary>è§£ç­”ã‚’è¦‹ã‚‹</summary>
                    <p><strong>è§£ç­”</strong>:</p>

                    <h5>å¯¾å‡¦æ³•1: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™</h5>
                    <ul>
                        <li><strong>æ–¹æ³•</strong>: è¨“ç·´/æ¨è«–æ™‚ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹</li>
                        <li><strong>åˆ©ç‚¹</strong>: æœ€ã‚‚ç°¡å˜ã€å³åº§ã«åŠ¹æœ</li>
                        <li><strong>æ¬ ç‚¹</strong>: è¨“ç·´æ™‚é–“ãŒå¢—åŠ ã€ç²¾åº¦ãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§</li>
                    </ul>

                    <h5>å¯¾å‡¦æ³•2: æ··åˆç²¾åº¦è¨ˆç®—ï¼ˆFP16/TF32ï¼‰</h5>
                    <ul>
                        <li><strong>æ–¹æ³•</strong>: FP32ã®ä»£ã‚ã‚Šã«FP16ã‚’ä½¿ç”¨ï¼ˆPyTorchã® <code>torch.cuda.amp</code>ï¼‰</li>
                        <li><strong>åˆ©ç‚¹</strong>: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåŠæ¸›ã€è¨ˆç®—ã‚‚1.5-3å€é«˜é€ŸåŒ–</li>
                        <li><strong>æ¬ ç‚¹</strong>: æ•°å€¤ç²¾åº¦ãŒä½ä¸‹ã€ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ã§åæŸã—ãªã„å ´åˆãŒã‚ã‚‹</li>
                    </ul>

                    <h5>å¯¾å‡¦æ³•3: Gradient Checkpointing</h5>
                    <ul>
                        <li><strong>æ–¹æ³•</strong>: é †ä¼æ’­æ™‚ã«ä¸­é–“å±¤ã®å‡ºåŠ›ã‚’ä¿å­˜ã›ãšã€é€†ä¼æ’­æ™‚ã«å†è¨ˆç®—</li>
                        <li><strong>åˆ©ç‚¹</strong>: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å¤§å¹…å‰Šæ¸›ï¼ˆO(âˆšn) â†’ O(n) ã®å‰Šæ¸›ï¼‰</li>
                        <li><strong>æ¬ ç‚¹</strong>: å†è¨ˆç®—ã«ã‚ˆã‚Šè¨“ç·´æ™‚é–“ãŒ30-50%å¢—åŠ </li>
                        <li><strong>å®Ÿè£…</strong>: PyTorchã® <code>torch.utils.checkpoint</code></li>
                    </ul>

                    <h5>å¯¾å‡¦æ³•4: ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—åŒ–ï¼ˆè¤‡æ•°GPUï¼‰</h5>
                    <ul>
                        <li><strong>æ–¹æ³•</strong>: ãƒ¢ãƒ‡ãƒ«ã‚’è¤‡æ•°GPUã«åˆ†å‰²</li>
                        <li><strong>åˆ©ç‚¹</strong>: å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œå¯èƒ½</li>
                        <li><strong>æ¬ ç‚¹</strong>: GPUé–“é€šä¿¡ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã€å®Ÿè£…ãŒè¤‡é›‘</li>
                    </ul>

                    <h5>å¯¾å‡¦æ³•5: CPU-GPUãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼ˆOffloadingï¼‰</h5>
                    <ul>
                        <li><strong>æ–¹æ³•</strong>: ä½¿ç”¨é »åº¦ã®ä½ã„å±¤ã‚’CPUãƒ¡ãƒ¢ãƒªã«é€€é¿ã—ã€å¿…è¦æ™‚ã«GPUã¸è»¢é€</li>
                        <li><strong>åˆ©ç‚¹</strong>: ç†è«–ä¸Šç„¡é™ã®ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«å¯¾å¿œå¯èƒ½</li>
                        <li><strong>æ¬ ç‚¹</strong>: ãƒ‡ãƒ¼ã‚¿è»¢é€ãŒéå¸¸ã«é…ã„ï¼ˆPCIeå¸¯åŸŸå¹…: 16-32 GB/sï¼‰</li>
                        <li><strong>å®Ÿè£…</strong>: DeepSpeedã® <code>ZeRO-Offload</code></li>
                    </ul>

                    <p><strong>æ¨å¥¨å„ªå…ˆé †ä½</strong>:</p>
                    <ol>
                        <li>æ··åˆç²¾åº¦è¨ˆç®—ï¼ˆæœ€ã‚‚åŠ¹æœçš„ï¼‰</li>
                        <li>Gradient Checkpointingï¼ˆæ·±ã„ãƒ¢ãƒ‡ãƒ«ã«æœ‰åŠ¹ï¼‰</li>
                        <li>ãƒãƒƒãƒã‚µã‚¤ã‚ºå‰Šæ¸›ï¼ˆæœ€çµ‚æ‰‹æ®µï¼‰</li>
                        <li>ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—åŒ–ï¼ˆãƒãƒ«ãƒGPUç’°å¢ƒãŒã‚ã‚Œã°ï¼‰</li>
                        <li>CPU Offloadingï¼ˆæœ€çµ‚æ‰‹æ®µã€é€Ÿåº¦ã¯å¤§å¹…ä½ä¸‹ï¼‰</li>
                    </ol>

                    <p><strong>å®Ÿè·µä¾‹ï¼ˆPyTorchï¼‰</strong>:</p>
                    <pre><code class="language-python">import torch

# æ··åˆç²¾åº¦è¨ˆç®—ã®æœ‰åŠ¹åŒ–
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for data, target in train_loader:
    optimizer.zero_grad()

    # FP16ã§é †ä¼æ’­
    with autocast():
        output = model(data)
        loss = criterion(output, target)

    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ã¦é€†ä¼æ’­
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()

# Gradient Checkpointing
from torch.utils.checkpoint import checkpoint

def custom_forward(module, input):
    return checkpoint(module, input)

# ä½¿ç”¨ä¾‹
output = custom_forward(model.layer1, x)
</code></pre>
                </details>
            </div>
        </section>

        <section id="references">
            <h2>2.6 å‚è€ƒæ–‡çŒ®</h2>

            <ol>
                <li>Kirk, D.B., Hwu, W.W. (2016). <em>Programming Massively Parallel Processors: A Hands-on Approach</em> (3rd ed.). Morgan Kaufmann, pp. 45-78 (CUDA execution model), pp. 123-156 (memory hierarchy), pp. 201-234 (memory optimization).</li>
                <li>NVIDIA Corporation. (2023). <em>CUDA C Programming Guide v12.0</em>. NVIDIA Developer Documentation, pp. 15-45 (programming model), pp. 67-92 (memory hierarchy), pp. 120-145 (performance guidelines). <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/" target="_blank">https://docs.nvidia.com/cuda/cuda-c-programming-guide/</a></li>
                <li>Sanders, J., Kandrot, E. (2010). <em>CUDA by Example: An Introduction to General-Purpose GPU Programming</em>. Addison-Wesley Professional, pp. 89-124 (thread hierarchy), pp. 145-178 (shared memory optimization).</li>
                <li>Cheng, J., Grossman, M., McKercher, T. (2014). <em>Professional CUDA C Programming</em>. Wrox Press, pp. 156-198 (memory coalescing), pp. 245-289 (occupancy optimization), pp. 312-345 (performance profiling).</li>
                <li>Harris, M. (2013). "Optimizing Parallel Reduction in CUDA", <em>NVIDIA Technical Blog</em>. <a href="https://developer.nvidia.com/blog/faster-parallel-reductions-kepler/" target="_blank">https://developer.nvidia.com/blog/faster-parallel-reductions-kepler/</a></li>
                <li>CuPy Development Team. (2023). <em>CuPy User Guide v12.0</em>. <a href="https://docs.cupy.dev/en/stable/" target="_blank">https://docs.cupy.dev/en/stable/</a>, pp. 23-56 (RawKernel programming), pp. 78-102 (memory management).</li>
                <li>Numba Development Team. (2023). <em>Numba CUDA Documentation v0.58</em>. <a href="https://numba.readthedocs.io/en/stable/cuda/" target="_blank">https://numba.readthedocs.io/en/stable/cuda/</a>, pp. 12-45 (CUDA kernel basics), pp. 67-89 (memory management).</li>
            </ol>
        </section>

        <section id="navigation">
            <h2>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</h2>
            <p>æœ¬ç« ã§ã€CUDAãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¨ãƒ¡ãƒ¢ãƒªéšå±¤ã®åŸºç¤ã‚’ç†è§£ã—ã¾ã—ãŸã€‚æ¬¡ç« ã§ã¯ã€CUDAç’°å¢ƒæ§‹ç¯‰ã€ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã€ãƒ‡ãƒãƒƒã‚°æŠ€æ³•ã‚’å­¦ã³ã€å®Ÿè·µçš„ãªGPUãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚¹ã‚­ãƒ«ã‚’èº«ã«ã¤ã‘ã¾ã™ã€‚</p>
            <div style="display: flex; justify-content: space-between; margin-top: 2rem; padding-top: 1rem; border-top: 2px solid var(--border-light);">
                <a href="chapter-1.html" style="padding: 1rem 2rem; background: linear-gradient(135deg, var(--accent-green) 0%, var(--accent-lime) 100%); color: white; border-radius: 8px; text-decoration: none; font-weight: 600;">â† ç¬¬1ç« : GPUã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®åŸºç¤</a>
                <a href="chapter-3.html" style="padding: 1rem 2rem; background: linear-gradient(135deg, var(--accent-green) 0%, var(--accent-lime) 100%); color: white; border-radius: 8px; text-decoration: none; font-weight: 600;">ç¬¬3ç« : CUDAç’°å¢ƒæ§‹ç¯‰ã¨ãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆ â†’</a>
            </div>
        </section>
    </main>

    <footer>
        <p>&copy; 2025 CT Dojo. All rights reserved.</p>
        <p><a href="https://github.com/yourusername/gpu-computing-tutorial">GitHub Repository</a> | <a href="mailto:yusuke.hashimoto.b8@tohoku.ac.jp">Contact</a></p>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
</body>
</html>
