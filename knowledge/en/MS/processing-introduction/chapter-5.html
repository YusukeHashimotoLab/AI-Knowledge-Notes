<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>ç¬¬5ç« ï¼šPythonpracticeï¼šprocess data analysisworkflow - MS Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<!-- Prism.js for syntax highlighting -->
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body><div class="locale-switcher">
<span class="current-locale">ğŸŒ EN</span>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>ç¬¬5ç« ï¼šPythonpracticeï¼šprocess data analysisworkflow</h1>
<p class="subtitle">SPC, DOE, Machine Learning, Automated Reporting</p>
<div class="meta">
<span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 35-45åˆ†</span>
<span class="meta-item">ğŸ“Š hardeasyåº¦: mediumç´šã€œä¸Šç´š</span>
<span class="meta-item">ğŸ’» Code Example: 7å€‹</span>
</div>
</div>
</header>
<div class="breadcrumb">
<a href="../index.html">AIå¯ºå­å±‹ãƒˆãƒƒãƒ—</a> &gt;
        <a href="../index.html">MS Dojo</a> &gt;
        <a href="index.html">ãƒ—ãƒ­ã‚»ã‚¹æŠ€è¡“å…¥é–€</a> &gt;
        ç¬¬5ç« 
    </div>
<main class="container">
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #fce7f3 0%, #fbcfe8 100%); border-left: 4px solid #f093fb; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">
            process data analysisã€material manufacturing withquality controlandoptimization withcoreã€‚In this chapterã€Statistical Process Controlï¼ˆSPCï¼‰ã€Design of Experimentsï¼ˆDOEï¼‰ã€machine learningwithpredictive modelconstructionã€anomaly detectionã€automated report generationintegratedPythonworkflowpractice andã€immediately applicableskillsacquireã€‚
        </p>
<div class="learning-objectives">
<h2>learnç›®æ¨™</h2>
<p>ã“ withç« èª­ã‚€ã“and forã€ä»¥ä¸‹acquireï¼š</p>
<ul>
<li>âœ… diverseprocess dataformatsï¼ˆCSV, JSON, Excel, equipment-specificformatï¼‰ withloadingandpreprocessing</li>
<li>âœ… SPCï¼ˆStatistical Process Controlï¼‰chartsï¼ˆX-bar, R-chart, Cp/Cpkï¼‰generateãƒ»interpret</li>
<li>âœ… Design of Experimentsï¼ˆDOE: Design of Experimentsï¼‰designã€Response Surface Methodologyï¼ˆRSMï¼‰ foroptimization</li>
<li>âœ… machine learningï¼ˆregressionãƒ»classificationï¼‰ forprocess outcomespredictã€feature importanceevaluate</li>
<li>âœ… anomaly detectionï¼ˆIsolation Forest, One-Class SVMï¼‰ fordefective productsearly detection</li>
<li>âœ… automated report generationï¼ˆmatplotlib, seaborn, Jinja2ï¼‰ fordaily/weeklyreportingstreamline</li>
<li>âœ… fully integratedworkflowï¼ˆdata â†’ analysis â†’ optimization â†’ reportï¼‰construction</li>
</ul>
</div>
<h2>5.1 process data withloadingandpreprocessing</h2>
<h3>5.1.1 diversedataformatsonå¯¾å¿œ</h3>
<p>actualprocess dataã€equipment logsï¼ˆCSV, TXTï¼‰ã€dataãƒ™ãƒ¼ã‚¹ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆJSON, Excelï¼‰ã€proprietaryformatï¼ˆbinaryï¼‰etc.variousã€‚</p>
<p><strong>majordataformats</strong>ï¼š</p>
<ul>
<li><strong>CSV/TSV</strong>ï¼šmost commonã€‚pandas.read_csv() forloading</li>
<li><strong>Excel (.xlsx, .xls)</strong>ï¼špandas.read_excel() forloading</li>
<li><strong>JSON</strong>ï¼špandas.read_json()orjson.load() forloading</li>
<li><strong>HDF5</strong>ï¼šå¤§è¦æ¨¡dataã€‚pandas.read_hdf() forloading</li>
<li><strong>SQL Database</strong>ï¼špandas.read_sql() forquery directly</li>
</ul>
<h4>Code Example5-1: å¤šformatsdataãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆBatch Processingï¼‰</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

import pandas as pd
import numpy as np
import json
import glob
from pathlib import Path

class ProcessDataLoader:
    """
    process data withcombineãƒ­ãƒ¼ãƒ€ãƒ¼

    è¤‡æ•°formatsãƒ»multiple files withBatch Processingsupports
    """

    def __init__(self, data_dir='./process_data'):
        """
        Parameters
        ----------
        data_dir : str or Path
            dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªpath
        """
        self.data_dir = Path(data_dir)
        self.supported_formats = ['.csv', '.xlsx', '.json', '.txt']

    def load_single_file(self, filepath):
        """
        single file withloading

        Parameters
        ----------
        filepath : str or Path
            file path

        Returns
        -------
        df : pd.DataFrame
            loadeddata
        """
        filepath = Path(filepath)
        ext = filepath.suffix.lower()

        try:
            if ext == '.csv' or ext == '.txt':
                # CSV/TXT withloadingï¼ˆåŒºåˆ‡ã‚Šæ–‡å­—è‡ªå‹•detectedï¼‰
                df = pd.read_csv(filepath, sep=None, engine='python')
            elif ext == '.xlsx' or ext == '.xls':
                # Excel withloadingï¼ˆæœ€åˆ withã‚·ãƒ¼ãƒˆ withã¿ï¼‰
                df = pd.read_excel(filepath)
            elif ext == '.json':
                # JSON withloading
                df = pd.read_json(filepath)
            else:
                raise ValueError(f"Unsupported file format: {ext}")

            # ãƒ¡ã‚¿dataè¿½åŠ 
            df['source_file'] = filepath.name
            df['load_timestamp'] = pd.Timestamp.now()

            print(f"Loaded: {filepath.name} ({len(df)} rows, {len(df.columns)} columns)")
            return df

        except Exception as e:
            print(f"Error loading {filepath}: {e}")
            return None

    def load_batch(self, pattern='*', file_extension='.csv'):
        """
        ãƒãƒƒãƒloadingï¼ˆmultiple filescombineï¼‰

        Parameters
        ----------
        pattern : str
            file name patternï¼ˆwildcards allowedï¼‰
        file_extension : str
            file extensionfilter

        Returns
        -------
        df_combined : pd.DataFrame
            combineddataãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        search_pattern = str(self.data_dir / f"{pattern}{file_extension}")
        files = glob.glob(search_pattern)

        if not files:
            print(f"No files found matching: {search_pattern}")
            return None

        print(f"Found {len(files)} files matching pattern '{pattern}{file_extension}'")

        dfs = []
        for filepath in sorted(files):
            df = self.load_single_file(filepath)
            if df is not None:
                dfs.append(df)

        if not dfs:
            print("No data loaded successfully")
            return None

        # combine
        df_combined = pd.concat(dfs, ignore_index=True)
        print(f"\nCombined data: {len(df_combined)} rows, {len(df_combined.columns)} columns")

        return df_combined

    def preprocess(self, df, dropna_thresh=0.5, drop_duplicates=True):
        """
        basicpreprocessing

        Parameters
        ----------
        df : pd.DataFrame
            inputdataãƒ•ãƒ¬ãƒ¼ãƒ 
        dropna_thresh : float
            missing valuesabove this ratiodrop columnsï¼ˆ0-1ï¼‰
        drop_duplicates : bool
            duplicate rowswhether to drop

        Returns
        -------
        df_clean : pd.DataFrame
            cleaneddata
        """
        df_clean = df.copy()

        # original size
        n_rows_orig, n_cols_orig = df_clean.shape

        # 1. missing valueså¤šã„columns withdelete
        thresh = int(len(df_clean) * dropna_thresh)
        df_clean = df_clean.dropna(thresh=thresh, axis=1)

        # 2. completely emptydrop rows
        df_clean = df_clean.dropna(how='all', axis=0)

        # 3. duplicate rows withdelete
        if drop_duplicates:
            df_clean = df_clean.drop_duplicates()

        # 4. dataå‹ withauto-infer
        df_clean = df_clean.infer_objects()

        # 5. numeric columns withdetect outliersï¼ˆsimple versionï¼šÂ±5Ïƒï¼‰
        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            mean = df_clean[col].mean()
            std = df_clean[col].std()
            lower = mean - 5 * std
            upper = mean + 5 * std
            outliers = (df_clean[col] &lt; lower) | (df_clean[col] &gt; upper)
            if outliers.sum() &gt; 0:
                print(f"  {col}: {outliers.sum()} outliers detected (outside Â±5Ïƒ)")
                # outliersreplace with NaNï¼ˆoptionalï¼‰
                # df_clean.loc[outliers, col] = np.nan

        n_rows_clean, n_cols_clean = df_clean.shape

        print(f"\nPreprocessing summary:")
        print(f"  Rows: {n_rows_orig} â†’ {n_rows_clean} ({n_rows_orig - n_rows_clean} removed)")
        print(f"  Columns: {n_cols_orig} â†’ {n_cols_clean} ({n_cols_orig - n_cols_clean} removed)")

        return df_clean

# usage example
if __name__ == "__main__":
    # sampledata withgenerateï¼ˆnormallyload from filesï¼‰
    import os
    os.makedirs('./process_data', exist_ok=True)

    # sampleCSVcreate files
    for i in range(3):
        df_sample = pd.DataFrame({
            'timestamp': pd.date_range('2025-01-01', periods=100, freq='h'),
            'temperature': np.random.normal(400, 10, 100),
            'pressure': np.random.normal(0.5, 0.05, 100),
            'power': np.random.normal(300, 20, 100),
            'thickness': np.random.normal(100, 5, 100)
        })
        df_sample.to_csv(f'./process_data/run_{i+1}.csv', index=False)

    # use loader
    loader = ProcessDataLoader(data_dir='./process_data')

    # ãƒãƒƒãƒloading
    df = loader.load_batch(pattern='run_*', file_extension='.csv')

    if df is not None:
        # preprocessing
        df_clean = loader.preprocess(df, dropna_thresh=0.5, drop_duplicates=True)

        # statistical summary
        print("\nData summary:")
        print(df_clean.describe())
</code></pre>
<h2>5.2 Statistical Process Controlï¼ˆSPC: Statistical Process Controlï¼‰</h2>
<h3>5.2.1 SPC withfundamentals</h3>
<p>SPCã€process variationstatisticallymonitorã€ç•°å¸¸early detectionã™ã‚‹methodã€‚</p>
<p><strong>majorcontrol chartsï¼ˆControl Chartsï¼‰</strong>ï¼š</p>
<ul>
<li><strong>X-bar charts</strong>ï¼šsampleå¹³å‡ withtrendsmonitor</li>
<li><strong>R charts</strong>ï¼šsampleç¯„å›²ï¼ˆRangeï¼‰ withtrendsmonitor</li>
<li><strong>S charts</strong>ï¼šsampleæ¨™æº–åå·® withtrendsmonitor</li>
<li><strong>I-MR charts</strong>ï¼šindividual valuesandmoving rangeï¼ˆIndividual &amp; Moving Rangeï¼‰</li>
</ul>
<p><strong>control limitsï¼ˆControl Limitsï¼‰</strong>ï¼š</p>

        $$
        \text{UCL} = \bar{X} + 3\sigma, \quad \text{LCL} = \bar{X} - 3\sigma
        $$

        <ul>
<li>UCL: Upper Control Limitï¼ˆuppercontrol limitsï¼‰</li>
<li>LCL: Lower Control Limitï¼ˆlowercontrol limitsï¼‰</li>
<li>$\bar{X}$: process mean</li>
<li>$\sigma$: process standard deviation</li>
</ul>
<h3>5.2.2 process capability indicesï¼ˆCp/Cpkï¼‰</h3>
<p>ãƒ—ãƒ­ã‚»ã‚¹specificationsæº€ãŸã™èƒ½åŠ›evaluateã™ã‚‹æŒ‡æ¨™ã€‚</p>
<p><strong>Cpï¼ˆProcess Capabilityï¼‰</strong>ï¼š</p>

        $$
        C_p = \frac{\text{USL} - \text{LSL}}{6\sigma}
        $$

        <ul>
<li>USL: Upper Specification Limitï¼ˆupper specification limitï¼‰</li>
<li>LSL: Lower Specification Limitï¼ˆlower specification limitï¼‰</li>
</ul>
<p><strong>Cpkï¼ˆProcess Capability Indexï¼‰</strong>ï¼š</p>

        $$
        C_{pk} = \min\left(\frac{\text{USL} - \mu}{3\sigma}, \frac{\mu - \text{LSL}}{3\sigma}\right)
        $$

        <ul>
<li>$\mu$: process mean</li>
</ul>
<p><strong>evaluateåŸºæº–</strong>ï¼š</p>
<ul>
<li>Cpk â‰¥ 1.33: excellentï¼ˆdefect rate &lt;64 ppmï¼‰</li>
<li>Cpk â‰¥ 1.00: adequateï¼ˆdefect rate &lt;2700 ppmï¼‰</li>
<li>Cpk &lt; 1.00: ä¸adequateï¼ˆprocess improvement neededï¼‰</li>
</ul>
<h4>Code Example5-2: SPCchartsgenerateï¼ˆX-bar, R-chart, Cp/Cpkï¼‰</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0
# - scipy&gt;=1.11.0

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats

class SPCAnalyzer:
    """
    Statistical Process Controlï¼ˆSPCï¼‰analysisclass
    """

    def __init__(self, data, sample_size=5):
        """
        Parameters
        ----------
        data : array-like
            process dataï¼ˆæ™‚ç³»columnsï¼‰
        sample_size : int
            sampleã‚µã‚¤ã‚ºï¼ˆsubgroup sizeï¼‰
        """
        self.data = np.array(data)
        self.sample_size = sample_size
        self.n_samples = len(data) // sample_size

        # split into subgroups
        self.samples = self.data[:self.n_samples * sample_size].reshape(-1, sample_size)

    def calculate_xbar_r(self):
        """
        X-bar chartsandRcharts withstatisticscalculate

        Returns
        -------
        stats_dict : dict
            statisticsï¼ˆxbar, R, UCL, LCLï¼‰
        """
        # sampleå¹³å‡andsampleç¯„å›²
        xbar = np.mean(self.samples, axis=1)
        R = np.ptp(self.samples, axis=1)  # Range (max - min)

        # overall meanandaverage range
        xbar_mean = np.mean(xbar)
        R_mean = np.mean(R)

        # control chartsconstantsï¼ˆn=5 withcaseï¼‰
        # A2, D3, D4from statistical tablesï¼ˆJIS Z 9020-2ï¼‰
        control_constants = {
            2: {'A2': 1.880, 'D3': 0, 'D4': 3.267},
            3: {'A2': 1.023, 'D3': 0, 'D4': 2.574},
            4: {'A2': 0.729, 'D3': 0, 'D4': 2.282},
            5: {'A2': 0.577, 'D3': 0, 'D4': 2.114},
            6: {'A2': 0.483, 'D3': 0, 'D4': 2.004},
            7: {'A2': 0.419, 'D3': 0.076, 'D4': 1.924},
            8: {'A2': 0.373, 'D3': 0.136, 'D4': 1.864},
            9: {'A2': 0.337, 'D3': 0.184, 'D4': 1.816},
            10: {'A2': 0.308, 'D3': 0.223, 'D4': 1.777}
        }

        if self.sample_size not in control_constants:
            raise ValueError(f"Sample size {self.sample_size} not supported (use 2-10)")

        consts = control_constants[self.sample_size]

        # X-barcharts withcontrol limits
        xbar_UCL = xbar_mean + consts['A2'] * R_mean
        xbar_LCL = xbar_mean - consts['A2'] * R_mean

        # Rcharts withcontrol limits
        R_UCL = consts['D4'] * R_mean
        R_LCL = consts['D3'] * R_mean

        return {
            'xbar': xbar,
            'xbar_mean': xbar_mean,
            'xbar_UCL': xbar_UCL,
            'xbar_LCL': xbar_LCL,
            'R': R,
            'R_mean': R_mean,
            'R_UCL': R_UCL,
            'R_LCL': R_LCL
        }

    def calculate_cp_cpk(self, USL, LSL):
        """
        process capability indicesï¼ˆCp, Cpkï¼‰calculate

        Parameters
        ----------
        USL : float
            upper specification limit
        LSL : float
            lower specification limit

        Returns
        -------
        cp_cpk : dict
            {'Cp': float, 'Cpk': float, 'ppm': float}
        """
        mu = np.mean(self.data)
        sigma = np.std(self.data, ddof=1)  # sample standard deviation

        # Cp
        Cp = (USL - LSL) / (6 * sigma)

        # Cpk
        Cpk_upper = (USL - mu) / (3 * sigma)
        Cpk_lower = (mu - LSL) / (3 * sigma)
        Cpk = min(Cpk_upper, Cpk_lower)

        # defect rate withestimateï¼ˆppm: parts per millionï¼‰
        # assume normal distribution
        z_USL = (USL - mu) / sigma
        z_LSL = (LSL - mu) / sigma

        ppm_upper = (1 - stats.norm.cdf(z_USL)) * 1e6
        ppm_lower = stats.norm.cdf(z_LSL) * 1e6
        ppm_total = ppm_upper + ppm_lower

        return {
            'Cp': Cp,
            'Cpk': Cpk,
            'ppm': ppm_total,
            'sigma': sigma,
            'mu': mu
        }

    def plot_control_charts(self, USL=None, LSL=None):
        """
        control chartsplot

        Parameters
        ----------
        USL, LSL : float, optional
            specification limitsï¼ˆCp/Cpkcalculateforï¼‰
        """
        stats_dict = self.calculate_xbar_r()

        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 12))

        sample_indices = np.arange(1, len(stats_dict['xbar']) + 1)

        # X-barcharts
        ax1.plot(sample_indices, stats_dict['xbar'], 'bo-', linewidth=2, markersize=6,
                label='Sample Mean')
        ax1.axhline(stats_dict['xbar_mean'], color='green', linestyle='-', linewidth=2,
                   label=f"Center Line: {stats_dict['xbar_mean']:.2f}")
        ax1.axhline(stats_dict['xbar_UCL'], color='red', linestyle='--', linewidth=2,
                   label=f"UCL: {stats_dict['xbar_UCL']:.2f}")
        ax1.axhline(stats_dict['xbar_LCL'], color='red', linestyle='--', linewidth=2,
                   label=f"LCL: {stats_dict['xbar_LCL']:.2f}")

        # control limitså¤– withç‚¹highlight
        out_of_control = (stats_dict['xbar'] &gt; stats_dict['xbar_UCL']) | \
                         (stats_dict['xbar'] &lt; stats_dict['xbar_LCL'])
        if out_of_control.any():
            ax1.scatter(sample_indices[out_of_control], stats_dict['xbar'][out_of_control],
                       color='red', s=150, marker='x', linewidths=3, zorder=5,
                       label='Out of Control')

        ax1.set_xlabel('Sample Number', fontsize=12)
        ax1.set_ylabel('Sample Mean', fontsize=12)
        ax1.set_title('X-bar Control Chart', fontsize=14, fontweight='bold')
        ax1.legend(fontsize=10)
        ax1.grid(alpha=0.3)

        # Rcharts
        ax2.plot(sample_indices, stats_dict['R'], 'go-', linewidth=2, markersize=6,
                label='Sample Range')
        ax2.axhline(stats_dict['R_mean'], color='blue', linestyle='-', linewidth=2,
                   label=f"Center Line: {stats_dict['R_mean']:.2f}")
        ax2.axhline(stats_dict['R_UCL'], color='red', linestyle='--', linewidth=2,
                   label=f"UCL: {stats_dict['R_UCL']:.2f}")
        ax2.axhline(stats_dict['R_LCL'], color='red', linestyle='--', linewidth=2,
                   label=f"LCL: {stats_dict['R_LCL']:.2f}")

        ax2.set_xlabel('Sample Number', fontsize=12)
        ax2.set_ylabel('Sample Range', fontsize=12)
        ax2.set_title('R Control Chart', fontsize=14, fontweight='bold')
        ax2.legend(fontsize=10)
        ax2.grid(alpha=0.3)

        # histogramandprocess capability
        ax3.hist(self.data, bins=30, alpha=0.7, color='skyblue', edgecolor='black',
                density=True, label='Data Distribution')

        # normal distribution fit
        mu = np.mean(self.data)
        sigma = np.std(self.data, ddof=1)
        x_range = np.linspace(self.data.min(), self.data.max(), 200)
        ax3.plot(x_range, stats.norm.pdf(x_range, mu, sigma), 'r-', linewidth=2,
                label=f'Normal Fit (Î¼={mu:.2f}, Ïƒ={sigma:.2f})')

        # specification limits
        if USL is not None and LSL is not None:
            ax3.axvline(USL, color='red', linestyle='--', linewidth=2, label=f'USL: {USL}')
            ax3.axvline(LSL, color='red', linestyle='--', linewidth=2, label=f'LSL: {LSL}')

            # Cp/Cpkcalculate
            cp_cpk = self.calculate_cp_cpk(USL, LSL)
            textstr = f"Cp = {cp_cpk['Cp']:.2f}\nCpk = {cp_cpk['Cpk']:.2f}\nDefect Rate â‰ˆ {cp_cpk['ppm']:.1f} ppm"
            ax3.text(0.02, 0.98, textstr, transform=ax3.transAxes, fontsize=11,
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

        ax3.set_xlabel('Value', fontsize=12)
        ax3.set_ylabel('Density', fontsize=12)
        ax3.set_title('Process Distribution &amp; Capability', fontsize=14, fontweight='bold')
        ax3.legend(fontsize=10)
        ax3.grid(alpha=0.3)

        plt.tight_layout()
        plt.show()

# å®Ÿrowsexample
if __name__ == "__main__":
    # sampledatagenerateï¼ˆprocess datasimulationï¼‰
    np.random.seed(42)

    # normalãƒ—ãƒ­ã‚»ã‚¹
    data_normal = np.random.normal(100, 2, 100)

    # insert anomalyï¼ˆ90-110ç•ªç›®shift atï¼‰
    data_shift = np.random.normal(105, 2, 20)
    data = np.concatenate([data_normal[:90], data_shift, data_normal[90:]])

    # SPCanalysis
    spc = SPCAnalyzer(data, sample_size=5)

    # control chartsplotï¼ˆspecifications: 95-105ï¼‰
    spc.plot_control_charts(USL=105, LSL=95)

    print("\nSPC Analysis Summary:")
    print(f"Total samples: {len(data)}")
    print(f"Subgroups: {spc.n_samples}")
    cp_cpk = spc.calculate_cp_cpk(USL=105, LSL=95)
    print(f"Cp = {cp_cpk['Cp']:.3f}")
    print(f"Cpk = {cp_cpk['Cpk']:.3f}")
    print(f"Estimated defect rate: {cp_cpk['ppm']:.1f} ppm")

    if cp_cpk['Cpk'] &gt;= 1.33:
        print("Process capability: Excellent")
    elif cp_cpk['Cpk'] &gt;= 1.00:
        print("Process capability: Adequate")
    else:
        print("Process capability: Poor - Improvement needed")
</code></pre>
<h2>5.3 Design of Experimentsï¼ˆDOE: Design of Experimentsï¼‰</h2>
<h3>5.3.1 DOE withfundamentals</h3>
<p>DOEã€minimal experiments formultiple parameters witheffectsefficientlyinvestigatemethodã€‚</p>
<p><strong>majorexperimentsè¨ˆç”»</strong>ï¼š</p>
<ul>
<li><strong>full factorial design</strong>ï¼šall combinationsexperimentsï¼ˆ2<sup>k</sup>experimentsï¼‰</li>
<li><strong>fractional factorial design</strong>ï¼šmain effects onlyevaluateï¼ˆ2<sup>k-p</sup>experimentsï¼‰</li>
<li><strong>orthogonal array</strong>ï¼šTaguchi methodï¼ˆL8, L16, L27etc.ï¼‰</li>
<li><strong>central composite designï¼ˆCCDï¼‰</strong>ï¼šResponse Surface Methodologyï¼ˆRSMï¼‰for</li>
</ul>
<h3>5.3.2 Response Surface Methodologyï¼ˆRSM: Response Surface Methodologyï¼‰</h3>
<p>RSMã€response variableï¼ˆobjective functionï¼‰andexplanatory variablesï¼ˆparametersï¼‰ withrelationship2order polynomialmodel usingã€‚</p>

        $$
        y = \beta_0 + \sum_{i=1}^{k} \beta_i x_i + \sum_{i=1}^{k} \beta_{ii} x_i^2 + \sum_{i<j} $$="" +="" <ul="" \beta_{ij}="" \epsilon="" x_i="" x_j="">
<li>$y$: response variableï¼ˆfilm thicknessã€stressã€quality scoreetc.ï¼‰</li>
<li>$x_i$: explanatory variablesï¼ˆtemperatureã€pressureã€poweretc.ï¼‰</li>
<li>$\beta$: regressioncoefficients</li>
<h4>Code Example5-3: Design of Experimentsï¼ˆ2factorfull factorial design+RSMï¼‰</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from itertools import product

def full_factorial_design(factors, levels):
    """
    full factorial design withexperimentsè¨ˆç”»generate

    Parameters
    ----------
    factors : dict
        {'factor_name': [level1, level2, ...]}
    levels : int
        å„factor withnumber of levelsï¼ˆ2levelã€3leveletc.ï¼‰

    Returns
    -------
    design : pd.DataFrame
        experimentsè¨ˆç”»è¡¨
    """
    factor_names = list(factors.keys())
    factor_values = [factors[name] for name in factor_names]

    # all combinationsgenerate
    combinations = list(product(*factor_values))

    design = pd.DataFrame(combinations, columns=factor_names)

    return design

def response_surface_model(X, y, degree=2):
    """
    response surfacemodelï¼ˆå¤šé …å¼regressionï¼‰

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        explanatory variables
    y : array-like, shape (n_samples,)
        response variable
    degree : int
        polynomial degreeï¼ˆusually2ï¼‰

    Returns
    -------
    model : sklearn model
        fittedmodel
    poly : PolynomialFeatures
        polynomial transformer
    """
    # polynomial featuresgenerate
    poly = PolynomialFeatures(degree=degree, include_bias=True)
    X_poly = poly.fit_transform(X)

    # ç·šå½¢regression
    model = LinearRegression()
    model.fit(X_poly, y)

    print(f"RÂ² score: {model.score(X_poly, y):.3f}")

    return model, poly

# experimentsè¨ˆç”» withè¨­å®š
factors = {
    'Temperature': [300, 350, 400, 450, 500],  # [Â°C]
    'Pressure': [0.2, 0.35, 0.5, 0.65, 0.8]     # [Pa]
}

design = full_factorial_design(factors, levels=5)
print("Experimental Design (Full Factorial):")
print(design.head(10))
print(f"Total experiments: {len(design)}")

# response variable withsimulationï¼ˆnormallyexperiments foræ¸¬å®šï¼‰
# truemodel: y = 100 + 0.2*T + 50*P - 0.0002*T^2 - 50*P^2 + 0.05*T*P
def true_response(T, P):
    """trueå¿œç­”é–¢æ•°ï¼ˆæœªçŸ¥and andã¦æ‰±ã†ï¼‰"""
    y = 100 + 0.2*T + 50*P - 0.0002*T**2 - 50*P**2 + 0.05*T*P
    # add noise
    y += np.random.normal(0, 2, len(T))
    return y

design['Response'] = true_response(design['Temperature'], design['Pressure'])

# dataæº–å‚™
X = design[['Temperature', 'Pressure']].values
y = design['Response'].values

# response surfacemodelfit
model, poly = response_surface_model(X, y, degree=2)

# prediction gridgenerate
T_range = np.linspace(300, 500, 50)
P_range = np.linspace(0.2, 0.8, 50)
T_grid, P_grid = np.meshgrid(T_range, P_range)

X_grid = np.c_[T_grid.ravel(), P_grid.ravel()]
X_grid_poly = poly.transform(X_grid)
y_pred_grid = model.predict(X_grid_poly).reshape(T_grid.shape)

# visualization
fig = plt.figure(figsize=(16, 6))

# left plot: 3Dresponse surface
ax1 = fig.add_subplot(1, 3, 1, projection='3d')
surf = ax1.plot_surface(T_grid, P_grid, y_pred_grid, cmap='viridis',
                        alpha=0.8, edgecolor='none')
ax1.scatter(X[:, 0], X[:, 1], y, color='red', s=50, marker='o',
           edgecolors='black', linewidths=1.5, label='Experimental Data')
ax1.set_xlabel('Temperature [Â°C]', fontsize=11)
ax1.set_ylabel('Pressure [Pa]', fontsize=11)
ax1.set_zlabel('Response', fontsize=11)
ax1.set_title('Response Surface (3D)', fontsize=13, fontweight='bold')
fig.colorbar(surf, ax=ax1, shrink=0.5, aspect=10)

# center plot: contour plot
ax2 = fig.add_subplot(1, 3, 2)
contour = ax2.contourf(T_grid, P_grid, y_pred_grid, levels=20, cmap='viridis', alpha=0.8)
contour_lines = ax2.contour(T_grid, P_grid, y_pred_grid, levels=10,
                             colors='white', linewidths=1, alpha=0.5)
ax2.clabel(contour_lines, inline=True, fontsize=8)
ax2.scatter(X[:, 0], X[:, 1], color='red', s=50, marker='o',
           edgecolors='black', linewidths=1.5, label='Exp. Points')

# optimal point
optimal_idx = np.argmax(y_pred_grid)
T_opt = T_grid.ravel()[optimal_idx]
P_opt = P_grid.ravel()[optimal_idx]
y_opt = y_pred_grid.ravel()[optimal_idx]

ax2.scatter(T_opt, P_opt, color='yellow', s=300, marker='*',
           edgecolors='black', linewidths=2, label=f'Optimum: {y_opt:.1f}', zorder=5)

ax2.set_xlabel('Temperature [Â°C]', fontsize=12)
ax2.set_ylabel('Pressure [Pa]', fontsize=12)
ax2.set_title('Response Surface (Contour)', fontsize=13, fontweight='bold')
ax2.legend(fontsize=10)
fig.colorbar(contour, ax=ax2, label='Response')

# right plot: main effects plot
ax3 = fig.add_subplot(1, 3, 3)

# temperature main effectï¼ˆpressure fixed at medianï¼‰
P_center = np.median(factors['Pressure'])
T_effect = np.linspace(300, 500, 50)
X_effect_T = np.c_[T_effect, np.full(50, P_center)]
X_effect_T_poly = poly.transform(X_effect_T)
y_effect_T = model.predict(X_effect_T_poly)

ax3.plot(T_effect, y_effect_T, 'b-', linewidth=2, label=f'Temperature (P={P_center} Pa)')

# pressure main effectï¼ˆtemperature fixed at medianï¼‰
T_center = np.median(factors['Temperature'])
P_effect = np.linspace(0.2, 0.8, 50)
X_effect_P = np.c_[np.full(50, T_center), P_effect]
X_effect_P_poly = poly.transform(X_effect_P)
y_effect_P = model.predict(X_effect_P_poly)

# right axis
ax3_twin = ax3.twinx()
ax3_twin.plot(P_effect*500, y_effect_P, 'r-', linewidth=2,
             label=f'Pressure (T={T_center}Â°C)')

ax3.set_xlabel('Temperature [Â°C]', fontsize=12, color='blue')
ax3_twin.set_xlabel('Pressure [Pa] (scaled Ã—500)', fontsize=12, color='red')
ax3.set_ylabel('Response (Temperature effect)', fontsize=12, color='blue')
ax3_twin.set_ylabel('Response (Pressure effect)', fontsize=12, color='red')
ax3.set_title('Main Effects Plot', fontsize=13, fontweight='bold')
ax3.tick_params(axis='x', labelcolor='blue')
ax3_twin.tick_params(axis='x', labelcolor='red')
ax3.grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\nOptimal Conditions:")
print(f"  Temperature: {T_opt:.1f} Â°C")
print(f"  Pressure: {P_opt:.3f} Pa")
print(f"  Predicted Response: {y_opt:.2f}")

# regressioncoefficients withdisplay
coef_names = poly.get_feature_names_out(['T', 'P'])
print(f"\nRegression Coefficients:")
for name, coef in zip(coef_names, [model.intercept_] + list(model.coef_[1:])):
    print(f"  {name}: {coef:.4f}")
</code></pre>
<h2>5.4 machine learningwithãƒ—ãƒ­ã‚»ã‚¹äºˆæ¸¬</h2>
<h3>5.4.1 regressionmodelwithå“è³ªäºˆæ¸¬</h3>
<p>machine learningforã„ã¦ã€ãƒ—ãƒ­ã‚»ã‚¹parametersfromproduct qualityäºˆæ¸¬ã€‚</p>
<p><strong>majoralgorithms</strong>ï¼š</p>
<ul>
<li><strong>ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆregression</strong>ï¼šhigh accuracyã€interpretæ€§ï¼ˆfeature importanceï¼‰</li>
<li><strong>gradient boostingï¼ˆXGBoost, LightGBMï¼‰</strong>ï¼šæœ€high accuracy</li>
<li><strong>neural networks</strong>ï¼šnonlinear patternslearn</li>
<li><strong>ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼regressionï¼ˆSVRï¼‰</strong>ï¼šå°datasetfor</li>
</ul>
<h4>Code Example5-4: ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆwithprocess qualityäºˆæ¸¬</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0
# - seaborn&gt;=0.12.0

"""
Example: Code Example5-4: ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆwithprocess qualityäºˆæ¸¬

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 30-60 seconds
Dependencies: None
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import seaborn as sns

# sampledatasetgenerateï¼ˆnormallyexperimentsdataä½¿forï¼‰
np.random.seed(42)
n_samples = 200

# ãƒ—ãƒ­ã‚»ã‚¹parameters
data = pd.DataFrame({
    'Temperature': np.random.uniform(300, 500, n_samples),
    'Pressure': np.random.uniform(0.2, 0.8, n_samples),
    'Power': np.random.uniform(100, 400, n_samples),
    'Flow_Rate': np.random.uniform(50, 150, n_samples),
    'Time': np.random.uniform(30, 120, n_samples)
})

# target variableï¼ˆfilm thicknessï¼‰ withsimulation
# truemodel: è¤‡é›‘ãªéç·šå½¢relationship
data['Thickness'] = (
    0.5 * data['Temperature'] +
    100 * data['Pressure'] +
    0.3 * data['Power'] +
    0.2 * data['Flow_Rate'] +
    1.0 * data['Time'] +
    0.001 * data['Temperature'] * data['Pressure'] -
    0.0005 * data['Temperature']**2 +
    np.random.normal(0, 10, n_samples)  # noise
)

print("Dataset shape:", data.shape)
print("\nFeature summary:")
print(data.describe())

# featuresandtarget variablesplit
X = data.drop('Thickness', axis=1)
y = data['Thickness']

# è¨“ç·´dataandãƒ†ã‚¹ãƒˆdatasplit into
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆmodeltrain
rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)

rf_model.fit(X_train, y_train)

# äºˆæ¸¬
y_train_pred = rf_model.predict(X_train)
y_test_pred = rf_model.predict(X_test)

# evaluateæŒ‡æ¨™
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
test_mae = mean_absolute_error(y_test, y_test_pred)

print("\n" + "="*60)
print("Model Performance:")
print("="*60)
print(f"Train RÂ²: {train_r2:.4f}")
print(f"Test RÂ²: {test_r2:.4f}")
print(f"Train RMSE: {train_rmse:.2f}")
print(f"Test RMSE: {test_rmse:.2f}")
print(f"Test MAE: {test_mae:.2f}")

# cross-validation
cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='r2')
print(f"\n5-Fold CV RÂ² score: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
print("="*60)

# visualization
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# top-left: äºˆæ¸¬vsactualï¼ˆè¨“ç·´dataï¼‰
axes[0, 0].scatter(y_train, y_train_pred, alpha=0.6, s=30, edgecolors='black', linewidth=0.5)
axes[0, 0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()],
               'r--', linewidth=2, label='Perfect Prediction')
axes[0, 0].set_xlabel('Actual Thickness', fontsize=12)
axes[0, 0].set_ylabel('Predicted Thickness', fontsize=12)
axes[0, 0].set_title(f'Training Set\nRÂ² = {train_r2:.3f}, RMSE = {train_rmse:.2f}',
                    fontsize=13, fontweight='bold')
axes[0, 0].legend(fontsize=10)
axes[0, 0].grid(alpha=0.3)

# top-right: äºˆæ¸¬vsactualï¼ˆãƒ†ã‚¹ãƒˆdataï¼‰
axes[0, 1].scatter(y_test, y_test_pred, alpha=0.6, s=30, color='green',
                  edgecolors='black', linewidth=0.5)
axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
               'r--', linewidth=2, label='Perfect Prediction')
axes[0, 1].set_xlabel('Actual Thickness', fontsize=12)
axes[0, 1].set_ylabel('Predicted Thickness', fontsize=12)
axes[0, 1].set_title(f'Test Set\nRÂ² = {test_r2:.3f}, RMSE = {test_rmse:.2f}',
                    fontsize=13, fontweight='bold')
axes[0, 1].legend(fontsize=10)
axes[0, 1].grid(alpha=0.3)

# bottom-left: feature importance
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

bars = axes[1, 0].barh(feature_importance['Feature'], feature_importance['Importance'],
                       color='skyblue', edgecolor='black')
axes[1, 0].set_xlabel('Importance', fontsize=12)
axes[1, 0].set_title('Feature Importance', fontsize=13, fontweight='bold')
axes[1, 0].grid(alpha=0.3, axis='x')

# å€¤to barsdisplay
for bar, importance in zip(bars, feature_importance['Importance']):
    axes[1, 0].text(importance + 0.01, bar.get_y() + bar.get_height()/2,
                   f'{importance:.3f}', va='center', fontsize=10)

# bottom-right: residualsplot
residuals = y_test - y_test_pred
axes[1, 1].scatter(y_test_pred, residuals, alpha=0.6, s=30, color='orange',
                  edgecolors='black', linewidth=0.5)
axes[1, 1].axhline(0, color='red', linestyle='--', linewidth=2)
axes[1, 1].set_xlabel('Predicted Thickness', fontsize=12)
axes[1, 1].set_ylabel('Residuals (Actual - Predicted)', fontsize=12)
axes[1, 1].set_title('Residual Plot (Test Set)', fontsize=13, fontweight='bold')
axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("\nFeature Importance Ranking:")
for idx, row in feature_importance.iterrows():
    print(f"  {row['Feature']}: {row['Importance']:.4f}")

print("\nInterpretation:")
print("  - Time has the highest importance (longer deposition â†’ thicker film)")
print("  - Temperature and Pressure also significant")
print("  - Model can predict thickness with ~Â±10 nm accuracy")
</code></pre>
<h3>5.4.2 classificationmodelwithdefective productsdetected</h3>
<p>ãƒ—ãƒ­ã‚»ã‚¹parametersfromdefective productsin advanceäºˆæ¸¬ã€‚</p>
<h4>Code Example5-5: ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯regressionwithdefective productsäºˆæ¸¬</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0
# - seaborn&gt;=0.12.0

"""
Example: Code Example5-5: ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯regressionwithdefective productsäºˆæ¸¬

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 30-60 seconds
Dependencies: None
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import seaborn as sns

# sampledatasetgenerate
np.random.seed(42)
n_samples = 300

# good productsï¼ˆ70%ï¼‰anddefective productsï¼ˆ30%ï¼‰
data = pd.DataFrame({
    'Temperature': np.random.normal(400, 30, n_samples),
    'Pressure': np.random.normal(0.5, 0.1, n_samples),
    'Power': np.random.normal(250, 50, n_samples)
})

# defective productslabelsgenerateï¼ˆspecificationså¤–conditions fordefect rate increasesï¼‰
# good productsconditions: 380&lt;t&lt;420, (data['temperature']="" 0.4&lt;p&lt;0.6,="" 200&lt;power&lt;300="" good_condition="("&gt; 380) &amp; (data['Temperature'] &lt; 420) &amp;
    (data['Pressure'] &gt; 0.4) &amp; (data['Pressure'] &lt; 0.6) &amp;
    (data['Power'] &gt; 200) &amp; (data['Power'] &lt; 300)
)

# conditionså¤– forã‚‚probabilisticallygood productsã«ãªã‚‹ã“andã‚ã‚‹
data['Defect'] = 0
data.loc[~good_condition, 'Defect'] = np.random.choice([0, 1], size=(~good_condition).sum(),
                                                        p=[0.3, 0.7])

print(f"Dataset: {len(data)} samples")
print(f"Defect rate: {data['Defect'].mean()*100:.1f}%")

# featuresandtarget variable
X = data[['Temperature', 'Pressure', 'Power']]
y = data['Defect']

# train-teståˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                    stratify=y, random_state=42)

# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯regressionmodel
lr_model = LogisticRegression(max_iter=1000, random_state=42)
lr_model.fit(X_train, y_train)

# äºˆæ¸¬
y_test_pred = lr_model.predict(X_test)
y_test_prob = lr_model.predict_proba(X_test)[:, 1]

# evaluate
print("\n" + "="*60)
print("Classification Report:")
print("="*60)
print(classification_report(y_test, y_test_pred, target_names=['Good', 'Defect']))

auc_score = roc_auc_score(y_test, y_test_prob)
print(f"ROC-AUC Score: {auc_score:.3f}")
print("="*60)

# visualization
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# top-left: æ··åŒrowscolumns
cm = confusion_matrix(y_test, y_test_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[0, 0],
           xticklabels=['Good', 'Defect'], yticklabels=['Good', 'Defect'])
axes[0, 0].set_xlabel('Predicted', fontsize=12)
axes[0, 0].set_ylabel('Actual', fontsize=12)
axes[0, 0].set_title('Confusion Matrix', fontsize=13, fontweight='bold')

# top-right: ROCcurve
fpr, tpr, thresholds = roc_curve(y_test, y_test_prob)
axes[0, 1].plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {auc_score:.3f})')
axes[0, 1].plot([0, 1], [0, 1], 'r--', linewidth=2, label='Random Classifier')
axes[0, 1].set_xlabel('False Positive Rate', fontsize=12)
axes[0, 1].set_ylabel('True Positive Rate', fontsize=12)
axes[0, 1].set_title('ROC Curve', fontsize=13, fontweight='bold')
axes[0, 1].legend(fontsize=11)
axes[0, 1].grid(alpha=0.3)

# bottom-left: featurescoefficients
coef_df = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': lr_model.coef_[0]
}).sort_values('Coefficient', key=abs, ascending=False)

bars = axes[1, 0].barh(coef_df['Feature'], coef_df['Coefficient'],
                       color=['red' if c &gt; 0 else 'blue' for c in coef_df['Coefficient']],
                       edgecolor='black')
axes[1, 0].axvline(0, color='black', linewidth=1)
axes[1, 0].set_xlabel('Coefficient (Defect Risk)', fontsize=12)
axes[1, 0].set_title('Feature Coefficients\n(Positive = Increases Defect Risk)',
                    fontsize=13, fontweight='bold')
axes[1, 0].grid(alpha=0.3, axis='x')

# bottom-right: probability distribution
axes[1, 1].hist(y_test_prob[y_test == 0], bins=20, alpha=0.6, label='Good',
               color='green', edgecolor='black')
axes[1, 1].hist(y_test_prob[y_test == 1], bins=20, alpha=0.6, label='Defect',
               color='red', edgecolor='black')
axes[1, 1].axvline(0.5, color='black', linestyle='--', linewidth=2,
                  label='Decision Threshold')
axes[1, 1].set_xlabel('Predicted Probability (Defect)', fontsize=12)
axes[1, 1].set_ylabel('Count', fontsize=12)
axes[1, 1].set_title('Predicted Probability Distribution', fontsize=13, fontweight='bold')
axes[1, 1].legend(fontsize=11)
axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("\nModel Interpretation:")
for idx, row in coef_df.iterrows():
    direction = "increases" if row['Coefficient'] &gt; 0 else "decreases"
    print(f"  {row['Feature']}: {row['Coefficient']:+.4f} â†’ {direction} defect risk")
&lt;/t&lt;420,&gt;</code></pre>
<h2>5.5 anomaly detectionï¼ˆAnomaly Detectionï¼‰</h2>
<h3>5.5.1 Isolation Forestwithanomaly detection</h3>
<p>Isolation Forestã€æ•™å¸«ãª andlearn forç•°å¸¸datadetectedã€‚normaldata patternsfromdeviate fromdataã€Œç•°å¸¸ã€and andã¦identifyã€‚</p>
<h4>Code Example5-6: Isolation Forestwithç•°å¸¸ãƒ—ãƒ­ã‚»ã‚¹detected</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0
# - seaborn&gt;=0.12.0

"""
Example: Code Example5-6: Isolation Forestwithç•°å¸¸ãƒ—ãƒ­ã‚»ã‚¹detected

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# sampledatagenerate
np.random.seed(42)

# normaldataï¼ˆ200sampleï¼‰
normal_data = pd.DataFrame({
    'Temperature': np.random.normal(400, 10, 200),
    'Pressure': np.random.normal(0.5, 0.05, 200),
    'Power': np.random.normal(250, 20, 200),
    'Thickness': np.random.normal(100, 5, 200)
})

# ç•°å¸¸dataï¼ˆ20sampleï¼‰
anomaly_data = pd.DataFrame({
    'Temperature': np.random.uniform(350, 450, 20),
    'Pressure': np.random.uniform(0.3, 0.7, 20),
    'Power': np.random.uniform(150, 350, 20),
    'Thickness': np.random.uniform(70, 130, 20)
})

# datacombine
data = pd.concat([normal_data, anomaly_data], ignore_index=True)
true_labels = np.array([0]*len(normal_data) + [1]*len(anomaly_data))  # 0: normal, 1: anomaly

print(f"Total samples: {len(data)}")
print(f"Anomaly rate: {(true_labels == 1).mean()*100:.1f}%")

# features withstandardize
scaler = StandardScaler()
X_scaled = scaler.fit_transform(data)

# Isolation Forestmodel
iso_forest = IsolationForest(
    contamination=0.1,  # expectedç•°å¸¸rate
    random_state=42,
    n_estimators=100
)

# äºˆæ¸¬ï¼ˆ-1: ç•°å¸¸, 1: normalï¼‰
predictions = iso_forest.fit_predict(X_scaled)
anomaly_scores = iso_forest.score_samples(X_scaled)

# äºˆæ¸¬labels0/1convert to
pred_labels = (predictions == -1).astype(int)

# evaluateï¼ˆtruelabelswhen availableï¼‰
from sklearn.metrics import classification_report, confusion_matrix

print("\n" + "="*60)
print("Anomaly Detection Results:")
print("="*60)
print(classification_report(true_labels, pred_labels,
                          target_names=['Normal', 'Anomaly']))
print("="*60)

# PCA for2dimensionsã«compressï¼ˆvisualizationforï¼‰
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_scaled)

# visualization
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# top-left: PCAspace for withanomaly detectionçµæœ
scatter1 = axes[0, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=pred_labels,
                             cmap='RdYlGn_r', s=80, alpha=0.7,
                             edgecolors='black', linewidth=1)
axes[0, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)',
                     fontsize=11)
axes[0, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)',
                     fontsize=11)
axes[0, 0].set_title('Anomaly Detection (Predicted)', fontsize=13, fontweight='bold')
cbar1 = plt.colorbar(scatter1, ax=axes[0, 0])
cbar1.set_label('0: Normal, 1: Anomaly', fontsize=10)
axes[0, 0].grid(alpha=0.3)

# top-right: truelabelsï¼ˆæ¯”è¼ƒforï¼‰
scatter2 = axes[0, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=true_labels,
                             cmap='RdYlGn_r', s=80, alpha=0.7,
                             edgecolors='black', linewidth=1)
axes[0, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)',
                     fontsize=11)
axes[0, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)',
                     fontsize=11)
axes[0, 1].set_title('Ground Truth Labels', fontsize=13, fontweight='bold')
cbar2 = plt.colorbar(scatter2, ax=axes[0, 1])
cbar2.set_label('0: Normal, 1: Anomaly', fontsize=10)
axes[0, 1].grid(alpha=0.3)

# bottom-left: ç•°å¸¸scoreåˆ†å¸ƒ
axes[1, 0].hist(anomaly_scores[true_labels == 0], bins=30, alpha=0.6,
               label='Normal', color='green', edgecolor='black')
axes[1, 0].hist(anomaly_scores[true_labels == 1], bins=30, alpha=0.6,
               label='Anomaly', color='red', edgecolor='black')
axes[1, 0].set_xlabel('Anomaly Score (lower = more anomalous)', fontsize=12)
axes[1, 0].set_ylabel('Count', fontsize=12)
axes[1, 0].set_title('Anomaly Score Distribution', fontsize=13, fontweight='bold')
axes[1, 0].legend(fontsize=11)
axes[1, 0].grid(alpha=0.3)

# bottom-right: æ··åŒrowscolumns
cm = confusion_matrix(true_labels, pred_labels)
import seaborn as sns
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[1, 1],
           xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])
axes[1, 1].set_xlabel('Predicted', fontsize=12)
axes[1, 1].set_ylabel('Actual', fontsize=12)
axes[1, 1].set_title('Confusion Matrix', fontsize=13, fontweight='bold')

plt.tight_layout()
plt.show()

# mostç•°å¸¸ãªsampleãƒªã‚¹ãƒˆ
top_anomalies = np.argsort(anomaly_scores)[:5]
print("\nTop 5 Most Anomalous Samples:")
print(data.iloc[top_anomalies])
print("\nAnomaly Scores:")
print(anomaly_scores[top_anomalies])
</code></pre>
<h2>5.6 automated report generation</h2>
<h3>5.6.1 daily/weeklyãƒ—ãƒ­ã‚»ã‚¹reportautomatedåŒ–</h3>
<p>analysisçµæœautomaticallyPDFreportor HTML dashboardsã€‚</p>
<h4>Code Example5-7: fully integratedworkflowï¼ˆdata â†’ analysis â†’ reportï¼‰</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0
# - scipy&gt;=1.11.0
# - seaborn&gt;=0.12.0

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from matplotlib.backends.backend_pdf import PdfPages
import warnings
warnings.filterwarnings('ignore')

class ProcessReportGenerator:
    """
    ãƒ—ãƒ­ã‚»ã‚¹analysis withautomated report generationclass
    """

    def __init__(self, data, report_title="Process Analysis Report"):
        """
        Parameters
        ----------
        data : pd.DataFrame
            process data
        report_title : str
            reporttitle
        """
        self.data = data
        self.report_title = report_title
        self.timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    def generate_summary_statistics(self):
        """statistical summary withgenerate"""
        summary = self.data.describe().T
        summary['missing'] = self.data.isnull().sum()
        summary['missing_pct'] = (summary['missing'] / len(self.data) * 100).round(2)

        return summary

    def plot_time_series(self, ax, column):
        """æ™‚ç³»columnsplot"""
        if 'timestamp' in self.data.columns:
            x = self.data['timestamp']
        else:
            x = np.arange(len(self.data))

        ax.plot(x, self.data[column], 'b-', linewidth=1.5, alpha=0.7)

        # control limitsï¼ˆÂ±3Ïƒï¼‰
        mean = self.data[column].mean()
        std = self.data[column].std()
        ucl = mean + 3*std
        lcl = mean - 3*std

        ax.axhline(mean, color='green', linestyle='-', linewidth=2, label='Mean')
        ax.axhline(ucl, color='red', linestyle='--', linewidth=2, label='UCL (Â±3Ïƒ)')
        ax.axhline(lcl, color='red', linestyle='--', linewidth=2, label='LCL')

        # control limitså¤– withç‚¹highlight
        out_of_control = (self.data[column] &gt; ucl) | (self.data[column] &lt; lcl)
        if out_of_control.any():
            ax.scatter(np.where(out_of_control)[0], self.data.loc[out_of_control, column],
                      color='red', s=100, marker='x', linewidths=3, zorder=5,
                      label='Out of Control')

        ax.set_xlabel('Sample Index', fontsize=10)
        ax.set_ylabel(column, fontsize=10)
        ax.set_title(f'Time Series: {column}', fontsize=12, fontweight='bold')
        ax.legend(fontsize=9)
        ax.grid(alpha=0.3)

    def plot_distribution(self, ax, column, bins=30):
        """åˆ†å¸ƒplot"""
        ax.hist(self.data[column], bins=bins, alpha=0.7, color='skyblue',
               edgecolor='black', density=True)

        # normal distribution fit
        from scipy import stats
        mu = self.data[column].mean()
        sigma = self.data[column].std()
        x_range = np.linspace(self.data[column].min(), self.data[column].max(), 100)
        ax.plot(x_range, stats.norm.pdf(x_range, mu, sigma), 'r-', linewidth=2,
               label=f'Normal Fit\nÎ¼={mu:.2f}, Ïƒ={sigma:.2f}')

        ax.set_xlabel(column, fontsize=10)
        ax.set_ylabel('Density', fontsize=10)
        ax.set_title(f'Distribution: {column}', fontsize=12, fontweight='bold')
        ax.legend(fontsize=9)
        ax.grid(alpha=0.3)

    def plot_correlation_matrix(self, ax):
        """ç›¸é–¢rowscolumnsãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—"""
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        corr = self.data[numeric_cols].corr()

        import seaborn as sns
        sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0,
                   square=True, linewidths=1, cbar_kws={"shrink": 0.8}, ax=ax)
        ax.set_title('Correlation Matrix', fontsize=12, fontweight='bold')

    def generate_pdf_report(self, filename='process_report.pdf'):
        """
        PDFreport withgenerate

        Parameters
        ----------
        filename : str
            å‡ºåŠ›PDFãƒ•ã‚¡ã‚¤ãƒ«å
        """
        with PdfPages(filename) as pdf:
            # ãƒšãƒ¼ã‚¸1: titleandstatistical summary
            fig = plt.figure(figsize=(11, 8.5))
            fig.suptitle(self.report_title, fontsize=18, fontweight='bold', y=0.98)

            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
            fig.text(0.5, 0.94, f'Generated: {self.timestamp}', ha='center',
                    fontsize=10, style='italic')

            # statistical summaryãƒ†ãƒ¼ãƒ–ãƒ«
            ax_table = fig.add_subplot(111)
            ax_table.axis('off')

            summary = self.generate_summary_statistics()
            summary_display = summary[['mean', 'std', 'min', 'max', 'missing_pct']]
            summary_display.columns = ['Mean', 'Std', 'Min', 'Max', 'Missing%']

            table = ax_table.table(cellText=summary_display.round(2).values,
                                  rowLabels=summary_display.index,
                                  colLabels=summary_display.columns,
                                  cellLoc='center', rowLoc='center',
                                  loc='center', bbox=[0.1, 0.3, 0.8, 0.6])
            table.auto_set_font_size(False)
            table.set_fontsize(9)
            table.scale(1, 2)

            # ãƒ˜ãƒƒãƒ€ãƒ¼rows withè£…é£¾
            for i in range(len(summary_display.columns)):
                table[(0, i)].set_facecolor('#4CAF50')
                table[(0, i)].set_text_props(weight='bold', color='white')

            pdf.savefig(fig, bbox_inches='tight')
            plt.close()

            # ãƒšãƒ¼ã‚¸2-N: å„å¤‰æ•° withæ™‚ç³»columnsandåˆ†å¸ƒ
            numeric_cols = self.data.select_dtypes(include=[np.number]).columns

            for col in numeric_cols:
                fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(11, 8.5))

                self.plot_time_series(ax1, col)
                self.plot_distribution(ax2, col)

                plt.tight_layout()
                pdf.savefig(fig, bbox_inches='tight')
                plt.close()

            # æœ€çµ‚ãƒšãƒ¼ã‚¸: ç›¸é–¢rowscolumns
            fig, ax = plt.subplots(figsize=(11, 8.5))
            self.plot_correlation_matrix(ax)

            plt.tight_layout()
            pdf.savefig(fig, bbox_inches='tight')
            plt.close()

        print(f"PDF report generated: {filename}")

# usage example
if __name__ == "__main__":
    # sampledatagenerate
    np.random.seed(42)
    n_samples = 100

    data = pd.DataFrame({
        'timestamp': pd.date_range('2025-01-01', periods=n_samples, freq='h'),
        'Temperature': np.random.normal(400, 10, n_samples),
        'Pressure': np.random.normal(0.5, 0.05, n_samples),
        'Power': np.random.normal(250, 20, n_samples),
        'Thickness': np.random.normal(100, 5, n_samples),
        'Uniformity': np.random.normal(95, 2, n_samples)
    })

    # someç•°å¸¸insert
    data.loc[50:55, 'Temperature'] = np.random.normal(430, 5, 6)
    data.loc[50:55, 'Thickness'] = np.random.normal(110, 3, 6)

    # reportgenerate
    report_gen = ProcessReportGenerator(data, report_title="Weekly Process Analysis Report")

    # statistical summarydisplay
    print("Statistical Summary:")
    print(report_gen.generate_summary_statistics())

    # PDFreportgenerate
    report_gen.generate_pdf_report('process_weekly_report.pdf')

    print("\nReport generation complete!")
    print("  - PDF: process_weekly_report.pdf")
    print("  - Contains: Time series, distributions, correlation matrix")
</code></pre>
<div class="mermaid">
flowchart TD
    A[Raw Process Data<br/>CSV/Excel/JSON] --&gt; B[Data Loading<br/>ProcessDataLoader]
    B --&gt; C[Preprocessing<br/>Clean &amp; Standardize]
    C --&gt; D[SPC Analysis<br/>Control Charts]
    C --&gt; E[DOE/RSM<br/>Optimization]
    C --&gt; F[ML Prediction<br/>Quality Forecast]
    C --&gt; G[Anomaly Detection<br/>Isolation Forest]
    D --&gt; H[Report Generation<br/>PDF/HTML]
    E --&gt; H
    F --&gt; H
    G --&gt; H
    H --&gt; I[Automated Report<br/>Daily/Weekly]

    style A fill:#99ccff,stroke:#0066cc
    style H fill:#f5576c,stroke:#f093fb,stroke-width:2px,color:#fff
    style I fill:#f093fb,stroke:#f5576c,stroke-width:2px,color:#fff
        </div>
<h2>5.7 exerciseproblem</h2>
<h3>exercise5-1: Cp/Cpkcalculateï¼ˆeasyï¼‰</h3>
<p><strong>problem</strong>ï¼šfilm thicknessdataå¹³å‡100 nmã€æ¨™æº–åå·®3 nmã€specifications95-105 nm withandãã€CpandCpkcalculateã€‚ãƒ—ãƒ­ã‚»ã‚¹é©åˆ‡ï¼Ÿ</p>
<details>
<summary><strong>answerdisplay</strong></summary>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - scipy&gt;=1.11.0

"""
Example: problemï¼šfilm thicknessdataå¹³å‡100 nmã€æ¨™æº–åå·®3 nmã€specifications95

Purpose: Demonstrate core concepts and implementation patterns
Target: Beginner to Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

import numpy as np

mu = 100  # [nm]
sigma = 3  # [nm]
USL = 105  # [nm]
LSL = 95   # [nm]

# Cp
Cp = (USL - LSL) / (6 * sigma)

# Cpk
Cpk_upper = (USL - mu) / (3 * sigma)
Cpk_lower = (mu - LSL) / (3 * sigma)
Cpk = min(Cpk_upper, Cpk_lower)

print(f"Cp = {Cp:.3f}")
print(f"Cpk = {Cpk:.3f}")

if Cpk &gt;= 1.33:
    print("process capability: excellent")
elif Cpk &gt;= 1.00:
    print("process capability: adequate")
else:
    print("process capability: ä¸adequateï¼ˆimprovement neededï¼‰")

# defect rateestimate
from scipy import stats
ppm = (stats.norm.cdf((LSL - mu) / sigma) +
       (1 - stats.norm.cdf((USL - mu) / sigma))) * 1e6
print(f"estimatedefect rate: {ppm:.1f} ppm")
</code></pre>
</details>
<h3>exercise5-2: 2factorexperimentsè¨ˆç”»ï¼ˆmediumï¼‰</h3>
<p><strong>problem</strong>ï¼štemperatureï¼ˆ300, 400, 500Â°Cï¼‰andpressureï¼ˆ0.3, 0.5, 0.7 Paï¼‰ with2factor3levelexperimentsdesignã€all combinations withexperimentsè¨ˆç”»è¡¨ä½œæˆã€‚</p>
<details>
<summary><strong>answerdisplay</strong></summary>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: problemï¼štemperatureï¼ˆ300, 400, 500Â°Cï¼‰andpressureï¼ˆ0.3, 0.5, 0.

Purpose: Demonstrate data manipulation and preprocessing
Target: Beginner to Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

import pandas as pd
from itertools import product

# factorandlevel
factors = {
    'Temperature': [300, 400, 500],
    'Pressure': [0.3, 0.5, 0.7]
}

# all combinationsgenerate
combinations = list(product(factors['Temperature'], factors['Pressure']))
design = pd.DataFrame(combinations, columns=['Temperature', 'Pressure'])

print("Experimental Design (Full Factorial):")
print(design)
print(f"\nTotal experiments: {len(design)}")
</code></pre>
</details>
<h3>exercise5-3: ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆfeature importanceï¼ˆmediumï¼‰</h3>
<p><strong>problem</strong>ï¼š5ã¤ withãƒ—ãƒ­ã‚»ã‚¹parametersï¼ˆtemperatureã€pressureã€powerã€æµé‡ã€æ™‚é–“ï¼‰fromfilm thicknessäºˆæ¸¬ã™ã‚‹model forã€mostimportantfactoridentifyã€‚</p>
<details>
<summary><strong>answerdisplay</strong></summary>
<p><strong>Code Example5-4 withãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆmodel</strong>å®Ÿrows andã€feature importancecheckï¼š</p>
<pre><code class="language-python"># feature_importancefromimportancecheck
print(feature_importance)

# å…¸å‹çš„ãªçµæœ:
#   Time: 0.35 (æœ€é‡è¦: longer deposition = thicker film)
#   Temperature: 0.25 (æˆé•·é€Ÿåº¦ã«effects)
#   Power: 0.20 (ã‚¹ãƒ‘ãƒƒã‚¿årateã«effects)
#   Pressure: 0.15 (ã‚¬ã‚¹æ•£ä¹±ã«effects)
#   Flow_Rate: 0.05 (é–“æ¥çš„effects)
</code></pre>
</details>
<h3>exercise5-4: anomaly detection withé–¾å€¤è¨­å®šï¼ˆmediumï¼‰</h3>
<p><strong>problem</strong>ï¼šIsolation Forest with`contamination`parameters0.05, 0.1, 0.2 forå¤‰ãˆãŸcaseã€detectedã•ã‚Œã‚‹ç•°å¸¸æ•°ã©ã†å¤‰åŒ–ã™ã‚‹ï¼Ÿ</p>
<details>
<summary><strong>answerdisplay</strong></summary>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: problemï¼šIsolation Forest with`contamination`parameters0.05, 

Purpose: Demonstrate machine learning model training and evaluation
Target: Advanced
Execution time: ~5 seconds
Dependencies: None
"""

from sklearn.ensemble import IsolationForest
import numpy as np

# sampledata
data = np.random.normal(0, 1, (100, 4))

contaminations = [0.05, 0.1, 0.2]

for cont in contaminations:
    iso_forest = IsolationForest(contamination=cont, random_state=42)
    predictions = iso_forest.fit_predict(data)
    n_anomalies = (predictions == -1).sum()

    print(f"Contamination = {cont}: {n_anomalies} anomalies detected ({n_anomalies/len(data)*100:.1f}%)")

# å‡ºåŠ›example:
# Contamination = 0.05: 5 anomalies (5.0%)
# Contamination = 0.1: 10 anomalies (10.0%)
# Contamination = 0.2: 20 anomalies (20.0%)

print("\ninterpret: contaminationexpectedç•°å¸¸rateã€‚highå¤šãdetectedï¼ˆå½é™½æ€§ãƒªã‚¹ã‚¯å¢—ï¼‰")
</code></pre>
</details>
<h3>exercise5-5: Response Surface Methodology withoptimizationï¼ˆhardï¼‰</h3>
<p><strong>problem</strong>ï¼štemperatureandpressure with2factor forfilm thicknessmaximize andãŸã„ã€‚response surfacemodelfromoptimumconditionsï¼ˆtemperatureã€pressureï¼‰numericallyfindã€‚</p>
<details>
<summary><strong>answerdisplay</strong></summary>
<pre><code class="language-python">from scipy.optimize import minimize

# Code Example5-3 withmodelandpolyä½¿for
def objective(x):
    """æœ€å°åŒ–ç›®æ¨™é–¢æ•°ï¼ˆmaximize withãŸã‚è² å·ï¼‰"""
    X_input = np.array(x).reshape(1, -1)
    X_poly = poly.transform(X_input)
    y_pred = model.predict(X_poly)
    return -y_pred[0]  # maximize withãŸã‚è² å·

# initial guessandbounds
x0 = [400, 0.5]  # [Temperature, Pressure]
bounds = [(300, 500), (0.2, 0.8)]

# optimization
result = minimize(objective, x0, bounds=bounds, method='L-BFGS-B')

T_opt = result.x[0]
P_opt = result.x[1]
y_opt = -result.fun

print(f"Optimal conditions:")
print(f"  Temperature: {T_opt:.1f} Â°C")
print(f"  Pressure: {P_opt:.3f} Pa")
print(f"  Predicted maximum response: {y_opt:.2f}")
</code></pre>
</details>
<h3>exercise5-6: datapreprocessing witheffectsï¼ˆhardï¼‰</h3>
<p><strong>problem</strong>ï¼šmissing valueswithdataset forã€(a)æ¬ ærowsdeleteã€(b)å¹³å‡å€¤imputationã€(c)KNNimputation with3methodæ¯”è¼ƒ andã€machine learningmodel withç²¾åº¦oneffectsevaluateã€‚</p>
<details>
<summary><strong>answerdisplay</strong></summary>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: problemï¼šmissing valueswithdataset forã€(a)æ¬ ærowsdeleteã€(b)å¹³å‡å€¤

Purpose: Demonstrate machine learning model training and evaluation
Target: Advanced
Execution time: 30-60 seconds
Dependencies: None
"""

import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# sampledataï¼ˆmissing valuesã‚ã‚Šï¼‰
np.random.seed(42)
n = 200
data = pd.DataFrame({
    'X1': np.random.normal(0, 1, n),
    'X2': np.random.normal(0, 1, n),
    'X3': np.random.normal(0, 1, n),
    'y': np.random.normal(0, 1, n)
})

# missing valuesãƒ©ãƒ³ãƒ€ãƒ ã«å°å…¥ï¼ˆ10%ï¼‰
for col in ['X1', 'X2', 'X3']:
    missing_idx = np.random.choice(n, size=int(n*0.1), replace=False)
    data.loc[missing_idx, col] = np.nan

print(f"Missing values: {data.isnull().sum().sum()}")

methods = {
    'Dropna': data.dropna(),
    'Mean Imputation': pd.DataFrame(
        SimpleImputer(strategy='mean').fit_transform(data),
        columns=data.columns
    ),
    'KNN Imputation': pd.DataFrame(
        KNNImputer(n_neighbors=5).fit_transform(data),
        columns=data.columns
    )
}

for method_name, df in methods.items():
    X = df.drop('y', axis=1)
    y = df['y']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = RandomForestRegressor(n_estimators=50, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    r2 = r2_score(y_test, y_pred)
    print(f"{method_name}: RÂ² = {r2:.3f}, n_samples = {len(df)}")

print("\nconclusion: dataé‡andmodelç²¾åº¦ withãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•è€ƒæ…® andã¦é¸æŠ")
</code></pre>
</details>
<h3>exercise5-7: SPCcontrol limits withadjustingï¼ˆhardï¼‰</h3>
<p><strong>problem</strong>ï¼š3Ïƒcontrol limitsï¼ˆ99.73%ï¼‰ instead of2Ïƒï¼ˆ95.45%ï¼‰when usingã€å½è­¦å ±rateandè¦‹é€ƒ andrateã©ã†å¤‰åŒ–ã™ã‚‹ï¼Ÿã©ã¡ã‚‰é©åˆ‡discuss whichã€‚</p>
<details>
<summary><strong>answerdisplay</strong></summary>
<p><strong>theoreticalæ¯”è¼ƒ</strong>ï¼š</p>
<ul>
<li><strong>3Ïƒcontrol limits</strong>ï¼š
                    <ul>
<li>å½è­¦å ±rateï¼ˆÎ±ï¼‰: 0.27%ï¼ˆnormalãª withã«ç•°å¸¸andåˆ¤å®šï¼‰</li>
<li>è¦‹é€ƒ andrateï¼ˆÎ²ï¼‰: highï¼ˆç•°å¸¸ãª withã«detectedæ¼ã‚Œï¼‰</li>
<li>é©for: stableãƒ—ãƒ­ã‚»ã‚¹ã€adjustingã‚³ã‚¹ãƒˆhighcase</li>
</ul>
</li>
<li><strong>2Ïƒcontrol limits</strong>ï¼š
                    <ul>
<li>å½è­¦å ±rateï¼ˆÎ±ï¼‰: 4.55%ï¼ˆå½è­¦å ±å¢—ãˆã‚‹ï¼‰</li>
<li>è¦‹é€ƒ andrateï¼ˆÎ²ï¼‰: lowï¼ˆearlydetectedï¼‰</li>
<li>é©for: ä¸stableãƒ—ãƒ­ã‚»ã‚¹ã€earlyè­¦å‘Šimportantcase</li>
</ul>
</li>
</ul>
<p><strong>practiceçš„é¸æŠ</strong>ï¼š</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - scipy&gt;=1.11.0

"""
Example: practiceçš„é¸æŠï¼š

Purpose: Demonstrate core concepts and implementation patterns
Target: Beginner to Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

# å½è­¦å ±rate withcalculate
from scipy import stats

alpha_3sigma = 2 * (1 - stats.norm.cdf(3))
alpha_2sigma = 2 * (1 - stats.norm.cdf(2))

print(f"3Ïƒcontrol limits: å½è­¦å ±rate = {alpha_3sigma*100:.2f}%")
print(f"2Ïƒcontrol limits: å½è­¦å ±rate = {alpha_2sigma*100:.2f}%")

print("\næ¨å¥¨:")
print("  - 3Ïƒ: matureãƒ—ãƒ­ã‚»ã‚¹ã€adjustingã‚³ã‚¹ãƒˆhighï¼ˆåŠå°ä½“è£½é€ etc.ï¼‰")
print("  - 2Ïƒ: newãƒ—ãƒ­ã‚»ã‚¹ã€å“è³ªcriticalã€earlyä»‹å…¥neededï¼ˆåŒ»è–¬å“etc.ï¼‰")
</code></pre>
</details>
<h3>exercise5-8: combineworkflowdesignï¼ˆhardï¼‰</h3>
<p><strong>problem</strong>ï¼šactualå·¥å ´ forã€dataacquisition â†’ SPCmonitoring â†’ anomaly detection â†’ è‡ªå‹•ã‚¢ãƒ©ãƒ¼ãƒˆ â†’ weeklyreport withå®Œå…¨è‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ designã€‚neededãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆanddataãƒ•ãƒ­ãƒ¼describeã€‚</p>
<details>
<summary><strong>answerdisplay</strong></summary>
<p><strong>ã‚·ã‚¹ãƒ†ãƒ architecture</strong>ï¼š</p>
<ol>
<li><strong>dataacquisitionlayer</strong>ï¼š
                    <ul>
<li>è£…ç½®from withreal-timeãƒ­ã‚°åé›†ï¼ˆOPC UA, MQTTï¼‰</li>
<li>dataãƒ™ãƒ¼ã‚¹storageï¼ˆTimescaleDB, InfluxDBï¼‰</li>
</ul>
</li>
<li><strong>å‡¦ç†layer</strong>ï¼š
                    <ul>
<li>å®šæœŸå®Ÿrowsï¼ˆcron, Airflowï¼‰</li>
<li>SPCanalysisï¼ˆPython + pandasï¼‰</li>
<li>anomaly detectionï¼ˆIsolation Forestï¼‰</li>
</ul>
</li>
<li><strong>ã‚¢ãƒ©ãƒ¼ãƒˆlayer</strong>ï¼š
                    <ul>
<li>control limitså¤–detected â†’ ãƒ¡ãƒ¼ãƒ«/Slacké€šçŸ¥</li>
<li>ç•°å¸¸scoreé–¾å€¤exceeded â†’ emergencyã‚¢ãƒ©ãƒ¼ãƒˆ</li>
</ul>
</li>
<li><strong>reportlayer</strong>ï¼š
                    <ul>
<li>weeklyreportè‡ªå‹•generateï¼ˆPDFï¼‰</li>
<li>Webãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆDash, Streamlitï¼‰</li>
</ul>
</li>
</ol>
<p><strong>å®Ÿè£…exampleï¼ˆpseudo-codeï¼‰</strong>ï¼š</p>
<pre><code class="language-python"># daily_monitoring.py (cron: daily1å›å®Ÿrows)
def daily_monitoring():
    # 1. dataacquisition
    data = load_data_from_database(start_date=yesterday, end_date=today)

    # 2. SPCanalysis
    spc = SPCAnalyzer(data)
    out_of_control = spc.detect_out_of_control()

    # 3. anomaly detection
    anomalies = detect_anomalies(data)

    # 4. ã‚¢ãƒ©ãƒ¼ãƒˆ
    if out_of_control or anomalies:
        send_alert(subject="Process Anomaly Detected",
                  body=f"Out of control: {out_of_control}\nAnomalies: {anomalies}")

    # 5. ãƒ­ã‚°save
    save_monitoring_log(data, spc_results, anomalies)

# weekly_report.py (cron: everyæœˆæ›œå®Ÿrows)
def weekly_report():
    data = load_data_from_database(start_date=last_week, end_date=today)
    report_gen = ProcessReportGenerator(data)
    report_gen.generate_pdf_report('weekly_report.pdf')
    send_email(to='manager@example.com', attachment='weekly_report.pdf')
</code></pre>
</details>
<h2>5.8 learn withcheck</h2>
<h3>åŸºæœ¬ç†è§£åº¦check</h3>
<ol>
<li>CSV, Excel, JSONetc.diverseformatfromdataèª­ã¿è¾¼ã‚ã¾ã™ï¼Ÿ</li>
<li>X-barchartsandRcharts withæ„å‘³andä½¿ã„åˆ†ã‘ç†è§£ andã¦ã„ã¾ã™ï¼Ÿ</li>
<li>Cp/Cpk withé•ã„andã€process capabilityevaluate withåŸºæº–èª¬æ˜ï¼Ÿ</li>
<li>full factorial designandResponse Surface Methodology withé•ã„ç†è§£ andã¦ã„ã¾ã™ï¼Ÿ</li>
<li>ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ withfeature importance withinterpretï¼Ÿ</li>
<li>Isolation Forestwithanomaly detection withåŸç†ç†è§£ andã¦ã„ã¾ã™ï¼Ÿ</li>
</ol>
<h3>practiceskillscheck</h3>
<ol>
<li>actualprocess datafromSPCchartsgenerate andã€control limitså¤– withç‚¹identifyï¼Ÿ</li>
<li>2factorä»¥ä¸Š withexperimentsè¨ˆç”»designã€response surfacemodel foroptimizationï¼Ÿ</li>
<li>machine learningmodelï¼ˆregressionãƒ»classificationï¼‰ forprocess qualityäºˆæ¸¬ï¼Ÿ</li>
<li>anomaly detectionalgorithms fordefective productsearly detectionï¼Ÿ</li>
<li>analysisçµæœautomaticallyPDFreportã«å‡ºåŠ›ï¼Ÿ</li>
</ol>
<h3>å¿œforåŠ›check</h3>
<ol>
<li>fully integratedworkflowï¼ˆdata â†’ analysis â†’ optimization â†’ reportï¼‰designãƒ»å®Ÿè£…ï¼Ÿ</li>
<li>actualå·¥å ´data forã€å“è³ªproblem withæ ¹æœ¬åŸå› datafromidentifyï¼Ÿ</li>
<li>newãƒ—ãƒ­ã‚»ã‚¹ withç«‹ã¡ä¸Šã’æ™‚ã«ã€DOEandMLçµ„ã¿åˆã‚ã›ãŸoptimizationæˆ¦ç•¥ææ¡ˆï¼Ÿ</li>
</ol>
<h2>5.9 references</h2>
<ol>
<li>Montgomery, D.C. (2012). <em>Statistical Quality Control</em> (7th ed.). Wiley. pp. 156-234 (Control charts), pp. 289-345 (Process capability).</li>
<li>Box, G.E.P., Hunter, J.S., Hunter, W.G. (2005). <em>Statistics for Experimenters: Design, Innovation, and Discovery</em> (2nd ed.). Wiley. pp. 123-189 (Factorial designs), pp. 289-345 (Response surface methods).</li>
<li>James, G., Witten, D., Hastie, T., Tibshirani, R. (2021). <em>An Introduction to Statistical Learning with Applications in Python</em>. Springer. pp. 303-335 (Random forests), pp. 445-489 (Unsupervised learning).</li>
<li>Pedregosa, F., et al. (2011). "Scikit-learn: Machine Learning in Python." <em>Journal of Machine Learning Research</em>, 12:2825-2830. - scikit-learn documentation.</li>
<li>McKinney, W. (2017). <em>Python for Data Analysis</em> (2nd ed.). O'Reilly. pp. 89-156 (pandas basics), pp. 234-289 (Data cleaning and preparation).</li>
<li>Liu, F.T., Ting, K.M., Zhou, Z.H. (2008). "Isolation Forest." <em>Proceedings of the 8th IEEE International Conference on Data Mining</em>, pp. 413-422. DOI: 10.1109/ICDM.2008.17</li>
<li>Hunter, J.D. (2007). "Matplotlib: A 2D Graphics Environment." <em>Computing in Science &amp; Engineering</em>, 9(3):90-95. DOI: 10.1109/MCSE.2007.55</li>
<li>Waskom, M. (2021). "seaborn: statistical data visualization." <em>Journal of Open Source Software</em>, 6(60):3021. DOI: 10.21105/joss.03021</li>
</ol>
<h2>5.10 æ¬¡ withsteps</h2>
<p>ã“ withç«  forå­¦ã‚“ã process data analysis withpracticeskillsã€ææ–™ç§‘å­¦ç ”ç©¶andç”£æ¥­å¿œfor withä¸¡æ–¹ forimmediatelyæ´»forã€‚æ¬¡ withstepsand andã¦ï¼š</p>
<ul>
<li><strong>å®Ÿdataoné©for</strong>ï¼šè‡ªèº« withç ”ç©¶å®¤ã‚„å·¥å ´ withprocess data forã€å­¦ã‚“ã methodpractice</li>
<li><strong>advancedmachine learning</strong>ï¼šæ·±layerlearnï¼ˆLSTM, Transformerï¼‰withæ™‚ç³»columnsäºˆæ¸¬</li>
<li><strong>real-timemonitoring</strong>ï¼šã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°dataå‡¦ç†ï¼ˆApache Kafka, Flinkï¼‰ withå°å…¥</li>
<li><strong>combineã‚·ã‚¹ãƒ†ãƒ development</strong>ï¼šWebãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆDash, Streamlitï¼‰ for withvisualization</li>
<li><strong>è‡ªå‹•åŒ– withæ·±åŒ–</strong>ï¼šCI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³constructionï¼ˆGitHub Actionsï¼‰ forå®Œå…¨è‡ªå‹•é‹for</li>
</ul>
<p>ãƒ—ãƒ­ã‚»ã‚¹æŠ€è¡“å…¥é–€ã‚·ãƒªãƒ¼ã‚ºå®Œèµ°ã•ã‚ŒãŸã‚ãªãŸã€è–„è†œæˆé•·ãƒ»ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ãƒ»dataanalysis withå…¨é ˜åŸŸcombineçš„ã«ç†è§£ andã€practiceåŠ›èº«ã«ã¤ã‘ã¾ andãŸã€‚ã“ withçŸ¥è­˜åŸºç›¤ã«ã€ã•ã‚‰ãªã‚‹å°‚é–€æ€§æ·±ã‚ã€ææ–™ç§‘å­¦ withæœ€å‰ç·š foræ´»èºã•ã‚Œã‚‹ã“andæœŸå¾…ï¼</p>
<div class="navigation">
<a class="nav-button" href="chapter-4.html">â† ç¬¬4ç« ï¼šè–„è†œæˆé•·ãƒ—ãƒ­ã‚»ã‚¹</a>
<a class="nav-button" href="index.html">ç›®æ¬¡ã«æˆ»ã‚‹</a>
</div>
</j}></main>
<section class="disclaimer">
<h3>disclaimer</h3>
<ul>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ•™è‚²ãƒ»ç ”ç©¶ãƒ»æƒ…å ±æä¾› withã¿ç›®çš„and andã¦ãŠã‚Šã€å°‚é–€çš„ãªåŠ©è¨€ï¼ˆæ³•å¾‹ãƒ»ä¼šè¨ˆãƒ»æŠ€è¡“çš„ä¿è¨¼etc.ï¼‰æä¾›ã™ã‚‹ã‚‚ with forã‚ã‚Šã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŠã³ä»˜éšã™ã‚‹Code Exampleã€Œç¾çŠ¶æœ‰å§¿ï¼ˆAS ISï¼‰ã€ foræä¾›ã•ã‚Œã€æ˜ç¤ºoré»™ç¤ºå•ã‚ãšã€å•†å“æ€§ã€identifyç›®çš„é©åˆæ€§ã€æ¨©åˆ©éä¾µå®³ã€positiveç¢ºæ€§ãƒ»å®Œå…¨æ€§ã€å‹•ä½œãƒ»å®‰å…¨æ€§ç­‰ã„ãªã‚‹ä¿è¨¼ã‚‚ andã¾ã›ã‚“ã€‚</li>
<li>å¤–éƒ¨ãƒªãƒ³ã‚¯ã€ç¬¬ä¸‰è€…æä¾›ã™ã‚‹dataãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ withå†…å®¹ãƒ»å¯foræ€§ãƒ»å®‰å…¨æ€§ã«ã¤ã„ã¦ã€authorãŠã³æ±åŒ—å¤§å­¦ä¸€åˆ‡ withè²¬ä»»è² ã„ã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ withåˆ©forãƒ»å®Ÿrowsãƒ»interpretã«moreç›´æ¥çš„ãƒ»é–“æ¥çš„ãƒ»ä»˜éšçš„ãƒ»ç‰¹åˆ¥ãƒ»çµæœçš„ãƒ»æ‡²ç½°çš„æå®³ç”Ÿã˜ãŸcase forã‚‚ã€é©foræ³• forè¨±å®¹ã•ã‚Œã‚‹maximumé™ withç¯„å›² forã€authorãŠã³æ±åŒ—å¤§å­¦è²¬ä»»è² ã„ã¾ã›ã‚“ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ withå†…å®¹ã€äºˆå‘Šãªãå¤‰æ›´ãƒ»æ›´æ–°ãƒ»æä¾›åœæ­¢ã•ã‚Œã‚‹ã“andã‚ã‚Šã¾ã™ã€‚</li>
<li>æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ withè‘—ä½œæ¨©ãƒ»licenseæ˜è¨˜ã•ã‚ŒãŸconditionsï¼ˆexample: CC BY 4.0ï¼‰ã«å¾“ã„ã¾ã™ã€‚å½“è©²licenseusuallyã€ç„¡ä¿è¨¼æ¡é …å«ã¿ã¾ã™ã€‚</li>
</ul>
</section>
<footer>
<p><strong>author</strong>: MS Knowledge Hub Content Team</p>
<p><strong>version</strong>: 1.0 | <strong>created</strong>: 2025-10-28</p>
<p><strong>license</strong>: Creative Commons BY 4.0</p>
<p>Â© 2025 MS Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
