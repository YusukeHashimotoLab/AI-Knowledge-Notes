<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 3: Eigenvalues, Eigenvectors, and Diagonalization | Linear Algebra and Tensor Analysis</title>
<meta content="Learn eigenvalue problems, diagonalization, symmetric matrix properties, PCA (principal component analysis), and vibration mode analysis." name="description"/>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet"/>
</head>
<body><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/FM/linear-algebra-tensor/chapter-3.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<h1>Chapter 3: Eigenvalues, Eigenvectors, and Diagonalization</h1>
<p class="subtitle">Eigenvalues, Eigenvectors, and Diagonalization</p>
</header>
<div class="container">
<div class="breadcrumb">
<a href="../index.html">Fundamentals of Mathematics Dojo</a> &gt;
            <a href="index.html">Linear Algebra and Tensor Analysis</a> &gt;
            Chapter 3
        </div>
<div class="content">
<h2>3.1 Definition of Eigenvalues and Eigenvectors</h2>
<div class="definition">
<strong>Definition: Eigenvalues and Eigenvectors</strong><br/>
                For square matrix A, non-zero vector v satisfying Av = Œªv is called an eigenvector,
                and scalar Œª is called an eigenvalue.<br/>
                Geometric meaning: eigenvectors are vectors whose direction doesn't change under matrix transformation
            </div>
<h3>Code Example 1: Calculating Eigenvalues and Eigenvectors</h3>
<div class="code-title">Python Implementation: Calculating Eigenvalues and Eigenvectors</div>
<div class="code-example"><pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Code Example 1: Calculating Eigenvalues and Eigenvectors

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt

# 2√ó2 matrix
A = np.array([[4, 1],
              [2, 3]])

# Calculate eigenvalues and eigenvectors
eigenvalues, eigenvectors = np.linalg.eig(A)

print("Eigenvalue and Eigenvector Calculation:")
print(f"A =\n{A}\n")
print(f"Eigenvalues: {eigenvalues}")
print(f"Eigenvectors:\n{eigenvectors}\n")

# Verification: Av = Œªv
for i in range(len(eigenvalues)):
    lam = eigenvalues[i]
    v = eigenvectors[:, i]
    Av = A @ v
    lam_v = lam * v
    print(f"Œª{i+1} = {lam:.4f}")
    print(f"  Av = {Av}")
    print(f"  Œªv = {lam_v}")
    print(f"  Av = Œªv? {np.allclose(Av, lam_v)}\n")</code></pre></div>
<h3>Code Example 2: Geometric Meaning of Eigenvectors</h3>
<div class="code-title">Python Implementation: Visualizing Eigenvectors</div>
<div class="code-example"><pre><code class="language-python"># Visualize eigenvectors
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# Random vector and eigenvectors
random_vec = np.array([1, 1])
eigen_vec1 = eigenvectors[:, 0]
eigen_vec2 = eigenvectors[:, 1]

# Vectors before transformation
origin = [0, 0]
ax1.quiver(*origin, *random_vec, angles='xy', scale_units='xy', scale=1,
           color='blue', width=0.01, label='General Vector')
ax1.quiver(*origin, *eigen_vec1, angles='xy', scale_units='xy', scale=1,
           color='red', width=0.01, label=f'Eigenvector 1 (Œª={eigenvalues[0]:.2f})')
ax1.quiver(*origin, *eigen_vec2, angles='xy', scale_units='xy', scale=1,
           color='green', width=0.01, label=f'Eigenvector 2 (Œª={eigenvalues[1]:.2f})')
ax1.set_xlim(-1, 5)
ax1.set_ylim(-1, 5)
ax1.set_xlabel('x')
ax1.set_ylabel('y')
ax1.set_title('Before Transformation')
ax1.legend()
ax1.grid(True, alpha=0.3)
ax1.axhline(y=0, color='k', linewidth=0.5)
ax1.axvline(x=0, color='k', linewidth=0.5)

# Vectors after transformation
random_transformed = A @ random_vec
eigen_transformed1 = A @ eigen_vec1
eigen_transformed2 = A @ eigen_vec2

ax2.quiver(*origin, *random_transformed, angles='xy', scale_units='xy', scale=1,
           color='blue', width=0.01, label='General Vector (direction changed)', alpha=0.7)
ax2.quiver(*origin, *eigen_transformed1, angles='xy', scale_units='xy', scale=1,
           color='red', width=0.01, label='Eigenvector 1 (direction unchanged)')
ax2.quiver(*origin, *eigen_transformed2, angles='xy', scale_units='xy', scale=1,
           color='green', width=0.01, label='Eigenvector 2 (direction unchanged)')

# Show original vectors faintly
ax2.quiver(*origin, *random_vec, angles='xy', scale_units='xy', scale=1,
           color='blue', width=0.005, alpha=0.2)
ax2.quiver(*origin, *eigen_vec1, angles='xy', scale_units='xy', scale=1,
           color='red', width=0.005, alpha=0.2)
ax2.quiver(*origin, *eigen_vec2, angles='xy', scale_units='xy', scale=1,
           color='green', width=0.005, alpha=0.2)

ax2.set_xlim(-1, 5)
ax2.set_ylim(-1, 5)
ax2.set_xlabel('x')
ax2.set_ylabel('y')
ax2.set_title('After Transformation by Matrix A')
ax2.legend(fontsize=8)
ax2.grid(True, alpha=0.3)
ax2.axhline(y=0, color='k', linewidth=0.5)
ax2.axvline(x=0, color='k', linewidth=0.5)

plt.tight_layout()
plt.show()</code></pre></div>
<h2>3.2 Characteristic Equation and Eigenvalue Calculation</h2>
<div class="theorem">
<strong>Theorem: Characteristic Equation</strong><br/>
                Eigenvalue Œª is found as the solution of the characteristic equation det(A - ŒªI) = 0.<br/>
                An n√ón matrix has n eigenvalues (including multiplicities).
            </div>
<h3>Code Example 3: Deriving Characteristic Equation with SymPy</h3>
<div class="code-title">Python Implementation: Symbolic Calculation of Characteristic Equation</div>
<div class="code-example"><pre><code class="language-python">import sympy as sp

# Symbolic variable
lam = sp.Symbol('lambda')

# Define matrix A symbolically
A_sym = sp.Matrix([[4, 1],
                   [2, 3]])

# I - identity matrix
I = sp.eye(2)

# Characteristic matrix A - ŒªI
char_matrix = A_sym - lam * I

print("Deriving Characteristic Equation:")
print(f"A - ŒªI =")
sp.pprint(char_matrix)

# Characteristic equation det(A - ŒªI) = 0
char_poly = char_matrix.det()
print(f"\nCharacteristic Polynomial: {char_poly} = 0")

# Find eigenvalues
eigenvals_sym = sp.solve(char_poly, lam)
print(f"Eigenvalues: {eigenvals_sym}")

# Compare with NumPy results
print(f"\nNumPy Eigenvalues: {eigenvalues}")</code></pre></div>
<h2>3.3 Diagonalization</h2>
<div class="definition">
<strong>Definition: Diagonalization</strong><br/>
                Matrix A is diagonalizable ‚áî there exists matrix P (with eigenvectors as columns) and
                diagonal matrix D (with eigenvalues on diagonal) such that P^(-1)AP = D.
            </div>
<h3>Code Example 4: Implementation of Diagonalization</h3>
<div class="code-title">Python Implementation: Diagonalization and Applications</div>
<div class="code-example"><pre><code class="language-python"># Diagonal matrix D with eigenvalues on diagonal
D = np.diag(eigenvalues)

# Matrix P with eigenvectors as columns
P = eigenvectors

# P^(-1)
P_inv = np.linalg.inv(P)

# Verification: P^(-1) A P = D
result = P_inv @ A @ P

print("Diagonalization Verification:")
print(f"P (eigenvector matrix) =\n{P}\n")
print(f"D (eigenvalue diagonal matrix) =\n{D}\n")
print(f"P^(-1) A P =\n{result}\n")
print(f"Matches D? {np.allclose(result, D)}")

# Application of diagonalization: fast computation of A^n
# A^n = P D^n P^(-1)
n = 10
A_n_fast = P @ np.linalg.matrix_power(D, n) @ P_inv
A_n_direct = np.linalg.matrix_power(A, n)

print(f"\nComputation of A^{n}:")
print(f"Using diagonalization: {A_n_fast[0,0]:.4f}")
print(f"Direct calculation:   {A_n_direct[0,0]:.4f}")
print(f"Match? {np.allclose(A_n_fast, A_n_direct)}")</code></pre></div>
<h2>3.4 Properties of Symmetric Matrices</h2>
<div class="theorem">
<strong>Theorem: Spectral Theorem for Symmetric Matrices</strong><br/>
                Real symmetric matrices (A = A^T):<br/>
<ul style="margin-top: 0.5rem;">
<li>All eigenvalues are real</li>
<li>Eigenvectors corresponding to different eigenvalues are orthogonal</li>
<li>Always diagonalizable, orthogonally diagonalizable</li>
</ul>
</div>
<h3>Code Example 5: Eigenvalue Decomposition of Symmetric Matrices</h3>
<div class="code-title">Python Implementation: Eigenvalue Decomposition of Symmetric Matrices</div>
<div class="code-example"><pre><code class="language-python"># Symmetric matrix
A_sym = np.array([[3, 1],
                  [1, 3]])

# Eigenvalues and eigenvectors
eigenvals_sym, eigenvecs_sym = np.linalg.eigh(A_sym)  # For symmetric matrices

print("Eigenvalue Decomposition of Symmetric Matrix:")
print(f"A (symmetric) =\n{A_sym}\n")
print(f"Eigenvalues: {eigenvals_sym}")
print(f"Eigenvectors:\n{eigenvecs_sym}\n")

# Confirm orthogonality of eigenvectors
v1 = eigenvecs_sym[:, 0]
v2 = eigenvecs_sym[:, 1]
inner_product = np.dot(v1, v2)
print(f"Inner product of eigenvectors: {inner_product:.10f}")
print(f"Orthogonal? {np.abs(inner_product) &lt; 1e-10}")

# Confirm orthogonality: Q^T Q = I
Q = eigenvecs_sym
QTQ = Q.T @ Q
print(f"\nQ^T Q =\n{QTQ}")
print(f"Identity matrix? {np.allclose(QTQ, np.eye(2))}")</code></pre></div>
<h2>3.5 PCA (Principal Component Analysis)</h2>
<div class="example">
<strong>Application Example: Principal Component Analysis (PCA)</strong><br/>
                By eigenvalue decomposition of the data covariance matrix, the eigenvector direction
                corresponding to the largest eigenvalue becomes the direction of maximum variance (first principal component).
            </div>
<h3>Code Example 6: PCA Implementation</h3>
<div class="code-title">Python Implementation: Principal Component Analysis (PCA)</div>
<div class="code-example"><pre><code class="language-python">from sklearn.datasets import make_blobs

# Generate sample data
np.random.seed(42)
X, _ = make_blobs(n_samples=100, n_features=2, centers=1, cluster_std=2.0)

# Center the data
X_centered = X - np.mean(X, axis=0)

# Covariance matrix
cov_matrix = np.cov(X_centered.T)

# Eigenvalue decomposition
eigenvals_pca, eigenvecs_pca = np.linalg.eigh(cov_matrix)

# Sort in descending order of eigenvalues
idx = eigenvals_pca.argsort()[::-1]
eigenvals_pca = eigenvals_pca[idx]
eigenvecs_pca = eigenvecs_pca[:, idx]

print("PCA (Principal Component Analysis):")
print(f"Covariance Matrix:\n{cov_matrix}\n")
print(f"Eigenvalues: {eigenvals_pca}")
print(f"Contribution Ratio: {eigenvals_pca / eigenvals_pca.sum()}")

# Visualization
plt.figure(figsize=(10, 8))
plt.scatter(X_centered[:, 0], X_centered[:, 1], alpha=0.5)

# Draw principal component directions
origin = [0, 0]
scale = np.sqrt(eigenvals_pca)
for i in range(2):
    vec = eigenvecs_pca[:, i] * scale[i] * 3
    plt.quiver(*origin, *vec, angles='xy', scale_units='xy', scale=1,
              color=['red', 'blue'][i], width=0.01,
              label=f'PC{i+1} (Œª={eigenvals_pca[i]:.2f})')

plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Principal Component Analysis (PCA)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.axis('equal')
plt.show()</code></pre></div>
<h3>Code Example 7: Dimensionality Reduction with Data Compression</h3>
<div class="code-title">Python Implementation: Dimensionality Reduction and Reconstruction</div>
<div class="code-example"><pre><code class="language-python"># Project onto first principal component only (2D ‚Üí 1D)
PC1 = eigenvecs_pca[:, 0].reshape(-1, 1)

# Projection
X_pca = X_centered @ PC1

# Reconstruct in original space (approximate reconstruction)
X_reconstructed = X_pca @ PC1.T

# Reconstruction error
reconstruction_error = np.mean(np.linalg.norm(X_centered - X_reconstructed, axis=1)**2)

print(f"\nDimensionality Reduction:")
print(f"Original dimensions: {X_centered.shape[1]}D")
print(f"After reduction: 1D")
print(f"Reconstruction error: {reconstruction_error:.4f}")
print(f"First principal component contribution: {eigenvals_pca[0]/eigenvals_pca.sum()*100:.1f}%")

# Visualization
plt.figure(figsize=(10, 8))
plt.scatter(X_centered[:, 0], X_centered[:, 1], alpha=0.3, label='Original Data')
plt.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], alpha=0.5, label='Reconstructed Data', s=10)
plt.plot([0, PC1[0,0]*scale[0]*3], [0, PC1[1,0]*scale[0]*3], 'r-', linewidth=3, label='PC1')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Projection onto First Principal Component and Reconstruction')
plt.legend()
plt.grid(True, alpha=0.3)
plt.axis('equal')
plt.show()</code></pre></div>
<h2>3.6 Application to Materials Science: Vibration Mode Analysis</h2>
<h3>Code Example 8: Natural Frequencies of Coupled Oscillation Systems</h3>
<div class="code-title">Python Implementation: Vibration Mode Analysis</div>
<div class="code-example"><pre><code class="language-python"># 2-DOF coupled oscillation system
# m1 = m2 = 1 kg, k1 = k2 = k3 = 1 N/m
# Equation of motion: M x'' + K x = 0
# Eigenvalue problem: det(K - œâ¬≤ M) = 0

M = np.array([[1, 0],
              [0, 1]])  # Mass matrix

K = np.array([[2, -1],
              [-1, 2]])  # Stiffness matrix

# Solve generalized eigenvalue problem
eigenvals_vibration, eigenvecs_vibration = np.linalg.eigh(K, M)

# Natural angular frequency œâ = sqrt(Œª)
omega = np.sqrt(eigenvals_vibration)

print("Vibration Mode Analysis:")
print(f"Mass Matrix M:\n{M}\n")
print(f"Stiffness Matrix K:\n{K}\n")
print(f"Eigenvalues Œª: {eigenvals_vibration}")
print(f"Natural Angular Frequencies œâ: {omega} rad/s")
print(f"Natural Frequencies f: {omega/(2*np.pi)} Hz\n")

print("Vibration Modes (Eigenvectors):")
for i in range(2):
    print(f"Mode {i+1} (f={omega[i]/(2*np.pi):.3f} Hz):")
    print(f"  Mass 1: {eigenvecs_vibration[0,i]:.4f}")
    print(f"  Mass 2: {eigenvecs_vibration[1,i]:.4f}")

    if eigenvecs_vibration[0,i] * eigenvecs_vibration[1,i] &gt; 0:
        print(f"  ‚Üí In-phase vibration\n")
    else:
        print(f"  ‚Üí Out-of-phase vibration\n")</code></pre></div>
<h2>Summary</h2>
<ul>
<li>Eigenvectors are vectors whose direction doesn't change under linear transformation, eigenvalues are the scaling factors</li>
<li>Diagonalization makes matrix power calculations efficient, useful for dynamic system analysis</li>
<li>Symmetric matrices have real eigenvalues and can be diagonalized with orthogonal eigenvectors</li>
<li>PCA extracts principal variation directions from data via eigenvalue decomposition of covariance matrix</li>
<li>Materials science has various applications including vibration mode analysis and crystal symmetry</li>
</ul>
<div class="nav-buttons">
<a class="nav-button" href="chapter-2.html">‚Üê Chapter 2: Determinants</a>
<a class="nav-button" href="chapter-4.html">Chapter 4: Singular Value Decomposition ‚Üí</a>
</div>
</div>
</div>
<footer>
<p>¬© 2025 AI Terakoya - Fundamentals of Mathematics Dojo</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</body>
</html>
