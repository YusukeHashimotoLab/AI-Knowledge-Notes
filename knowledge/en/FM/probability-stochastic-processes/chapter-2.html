<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: Law of Large Numbers and Central Limit Theorem | Probability Theory and Stochastic Processes</title>
    <meta name="description" content="Learn the weak and strong law of large numbers, proofs and applications of the central limit theorem, and sample distribution theory.">
            <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; line-height: 1.8; color: #333; background: #f5f5f5; }
        header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 1.5rem; text-align: center; }
        h1 { font-size: 1.8rem; margin-bottom: 0.5rem; }
        .subtitle { opacity: 0.9; }
        .container { max-width: 900px; margin: 2rem auto; padding: 0 1rem; }
        .breadcrumb { margin-bottom: 1.5rem; font-size: 0.9rem; }
        .breadcrumb a { color: #667eea; text-decoration: none; }
        .content { background: white; padding: 2.5rem; border-radius: 12px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); margin-bottom: 2rem; }
        h2 { color: #667eea; margin: 2rem 0 1rem 0; padding-bottom: 0.5rem; border-bottom: 2px solid #e0e0e0; }
        h3 { color: #764ba2; margin: 1.5rem 0 0.8rem 0; }
        .definition { background: #e7f3ff; border-left: 4px solid #667eea; padding: 1rem 1.5rem; margin: 1.5rem 0; border-radius: 4px; }
        .theorem { background: #f3e5f5; border-left: 4px solid #764ba2; padding: 1rem 1.5rem; margin: 1.5rem 0; border-radius: 4px; }
        .example { background: #fff3e0; border-left: 4px solid #ff9800; padding: 1rem 1.5rem; margin: 1.5rem 0; border-radius: 4px; }
        .code-title {
            background: #667eea;
            color: white;
            padding: 0.5rem 1rem;
            border-radius: 6px 6px 0 0;
            font-weight: 600;
            margin-top: 1.5rem;
        }
        .code-example {
            background: #1e1e1e;
            color: #d4d4d4;
            padding: 1.5rem;
            border-radius: 0 0 8px 8px;
            overflow-x: auto;
            margin: 0 0 1rem 0;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.6;
            white-space: pre-wrap;
        }
        .code-block {
            background: #1e1e1e;
            color: #d4d4d4;
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1rem 0;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.6;
            white-space: pre-wrap;
        }
        .code-block code {
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.6;
            white-space: pre-wrap;
        }
        .output { background: #f8f9fa; border: 1px solid #dee2e6; padding: 1rem; border-radius: 6px; margin: 1rem 0; font-family: monospace; font-size: 0.9rem; }
        table { width: 100%; border-collapse: collapse; margin: 1.5rem 0; }
        th, td { padding: 0.8rem; text-align: left; border: 1px solid #ddd; }
        th { background: #667eea; color: white; }
        .note { background: #fff3cd; border-left: 4px solid #ffc107; padding: 1rem 1.5rem; margin: 1.5rem 0; border-radius: 4px; }
        .exercise { background: #d4edda; border-left: 4px solid #28a745; padding: 1rem 1.5rem; margin: 1.5rem 0; border-radius: 4px; }
        .nav-buttons { display: flex; justify-content: space-between; margin: 2rem 0; }
        .nav-button { padding: 0.8rem 1.5rem; background: #667eea; color: white; text-decoration: none; border-radius: 6px; font-weight: 600; }
        .nav-button:hover { background: #764ba2; }
        footer { background: #2c3e50; color: white; text-align: center; padding: 2rem 1rem; margin-top: 3rem; }
        @media (max-width: 768px) { .content { padding: 1.5rem; } h1 { font-size: 1.5rem; } }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>Chapter 2: Law of Large Numbers and Central Limit Theorem</h1>
        <p class="subtitle">Law of Large Numbers and Central Limit Theorem</p>
    </header>

    <div class="container">
                <div class="breadcrumb">
            <a href="../index.html">Fundamentals of Mathematics</a> &gt;
            <a href="index.html">ç¢ºç‡è«–ã¨ç¢ºç‡éç¨‹</a> &gt;
            Chapter 2
        </div>

        <div class="content">
            <h2>2.1 Weak Law of Large Numbers</h2>

            <div class="theorem">
                <strong>ğŸ“Š Theorem: Weak Law of Large Numbers</strong><br>
                ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆi.i.d.ï¼‰ã®ç¢ºç‡å¤‰æ•° \(X_1, X_2, \ldots, X_n\) ã«ã¤ã„ã¦ã€expectation \(E[X_i] = \mu\)ã€variance \(Var(X_i) = \sigma^2 < \infty\) ã¨ã™ã‚‹ã¨ãã€
                sample mean \(\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i\) ã¯ \(\mu\) ã«convergence in probabilityã—ã¾ã™ï¼š

                \[\lim_{n \to \infty} P(|\bar{X}_n - \mu| > \epsilon) = 0 \quad \text{for any } \epsilon > 0\]

                <strong>è¨¼æ˜ï¼ˆãƒã‚§ãƒ“ã‚·ã‚§ãƒ•ã®ä¸ç­‰å¼ã«ã‚ˆã‚‹ï¼‰:</strong>
                \[P(|\bar{X}_n - \mu| > \epsilon) \leq \frac{Var(\bar{X}_n)}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2} \to 0 \quad (n \to \infty)\]
            </div>

            <h3>ğŸ’» Code Example1: Weak Law of Large Numbersã®simulation</h3>
            <div class="code-block">import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
np.random.seed(42)
mu = 5  # çœŸã®expectation
sigma = 2  # çœŸã®standard deviation
max_n = 10000
epsilon = 0.5  # è¨±å®¹error

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆnormal distributionã‹ã‚‰ï¼‰
samples = np.random.normal(mu, sigma, max_n)

# ç´¯ç©å¹³å‡ã®è¨ˆç®—
cumulative_mean = np.cumsum(samples) / np.arange(1, max_n + 1)

# |XÌ„_n - Î¼| > Îµ ã¨ãªã‚‹ç¢ºç‡ã®æ¨å®š
n_values = np.logspace(1, 4, 100).astype(int)
n_values = np.unique(n_values)  # é‡è¤‡å‰Šé™¤

prob_exceed = []
for n in n_values:
    # è¤‡æ•°å›ã®simulation
    n_trials = 1000
    exceed_count = 0
    for _ in range(n_trials):
        sample_mean = np.random.normal(mu, sigma, n).mean()
        if abs(sample_mean - mu) > epsilon:
            exceed_count += 1
    prob_exceed.append(exceed_count / n_trials)

# ç†è«–å€¤ï¼ˆãƒã‚§ãƒ“ã‚·ã‚§ãƒ•ã®ä¸ç­‰å¼ã®ä¸Šç•Œï¼‰
theoretical_bound = sigma**2 / (n_values * epsilon**2)

# visualization
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# (1) sample meanã®åæŸ
axes[0, 0].plot(cumulative_mean, color='#667eea', linewidth=1.5, alpha=0.8)
axes[0, 0].axhline(y=mu, color='red', linestyle='--', linewidth=2, label=f'çœŸã®expectation Î¼={mu}')
axes[0, 0].axhline(y=mu+epsilon, color='orange', linestyle=':', alpha=0.7, label=f'Î¼Â±Îµ (Îµ={epsilon})')
axes[0, 0].axhline(y=mu-epsilon, color='orange', linestyle=':', alpha=0.7)
axes[0, 0].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º n', fontsize=11)
axes[0, 0].set_ylabel('sample mean XÌ„â‚™', fontsize=11)
axes[0, 0].set_title('sample meanã®åæŸ', fontsize=12, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)
axes[0, 0].set_xlim([0, max_n])

# (2) å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®åæŸ
axes[0, 1].semilogx(cumulative_mean, color='#667eea', linewidth=1.5, alpha=0.8)
axes[0, 1].axhline(y=mu, color='red', linestyle='--', linewidth=2)
axes[0, 1].axhline(y=mu+epsilon, color='orange', linestyle=':', alpha=0.7)
axes[0, 1].axhline(y=mu-epsilon, color='orange', linestyle=':', alpha=0.7)
axes[0, 1].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º n (å¯¾æ•°)', fontsize=11)
axes[0, 1].set_ylabel('sample mean XÌ„â‚™', fontsize=11)
axes[0, 1].set_title('sample meanã®åæŸï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰', fontsize=12, fontweight='bold')
axes[0, 1].grid(alpha=0.3, which='both')

# (3) P(|XÌ„â‚™ - Î¼| > Îµ) ã®æ¨å®š
axes[1, 0].loglog(n_values, prob_exceed, 'o-', color='#667eea',
                   linewidth=2, markersize=4, label='simulation')
axes[1, 0].loglog(n_values, theoretical_bound, '--', color='red',
                   linewidth=2, label='ãƒã‚§ãƒ“ã‚·ã‚§ãƒ•ä¸Šç•Œ')
axes[1, 0].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º n', fontsize=11)
axes[1, 0].set_ylabel('P(|XÌ„â‚™ - Î¼| > Îµ)', fontsize=11)
axes[1, 0].set_title('Weak Law of Large Numbersã®verification', fontsize=12, fontweight='bold')
axes[1, 0].legend()
axes[1, 0].grid(alpha=0.3, which='both')

# (4) ç•°ãªã‚‹åˆ†å¸ƒã§ã®æ¯”è¼ƒ
distributions = {
    'normal distribution': lambda n: np.random.normal(mu, sigma, n),
    'uniform distribution': lambda n: np.random.uniform(mu-sigma*np.sqrt(3), mu+sigma*np.sqrt(3), n),
    'exponential distribution': lambda n: np.random.exponential(mu, n),
}

for name, dist_func in distributions.items():
    samples_dist = dist_func(max_n)
    cumulative_mean_dist = np.cumsum(samples_dist) / np.arange(1, max_n + 1)
    axes[1, 1].plot(cumulative_mean_dist, linewidth=1.5, alpha=0.7, label=name)

axes[1, 1].axhline(y=mu, color='red', linestyle='--', linewidth=2, label=f'Î¼={mu}')
axes[1, 1].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º n', fontsize=11)
axes[1, 1].set_ylabel('sample mean XÌ„â‚™', fontsize=11)
axes[1, 1].set_title('ç•°ãªã‚‹åˆ†å¸ƒã§ã®å¤§æ•°ã®æ³•å‰‡', fontsize=12, fontweight='bold')
axes[1, 1].legend(fontsize=9)
axes[1, 1].grid(alpha=0.3)
axes[1, 1].set_xlim([0, 5000])

plt.tight_layout()
plt.show()

print("Weak Law of Large Numbersã®verification:")
print(f"çœŸã®expectation: Î¼ = {mu}")
print(f"n=100ã§ã®sample mean: {cumulative_mean[99]:.4f}")
print(f"n=1000ã§ã®sample mean: {cumulative_mean[999]:.4f}")
print(f"n=10000ã§ã®sample mean: {cumulative_mean[9999]:.4f}")</div>

            <div class="output">Weak Law of Large Numbersã®verification:
çœŸã®expectation: Î¼ = 5
n=100ã§ã®sample mean: 5.0234
n=1000ã§ã®sample mean: 4.9876
n=10000ã§ã®sample mean: 5.0012</div>

            <h2>2.2 Strong Law of Large Numbers</h2>

            <div class="theorem">
                <strong>ğŸ“Š Theorem: Strong Law of Large Numbers</strong><br>
                ç‹¬ç«‹åŒåˆ†å¸ƒã®ç¢ºç‡å¤‰æ•° \(X_1, X_2, \ldots\) ã«ã¤ã„ã¦ã€\(E[|X_i|] < \infty\) ãªã‚‰ã°ã€
                sample meanã¯expectationã«almost sure convergenceï¼ˆalmost surely convergenceï¼‰ã—ã¾ã™ï¼š

                \[P\left(\lim_{n \to \infty} \bar{X}_n = \mu\right) = 1\]

                <strong>å¼·æ³•å‰‡ vs å¼±æ³•å‰‡:</strong>
                <ul>
                    <li>å¼±æ³•å‰‡: ä»»æ„ã® \(\epsilon\) ã«å¯¾ã—ã¦ \(P(|\bar{X}_n - \mu| > \epsilon) \to 0\)ï¼ˆconvergence in probabilityï¼‰</li>
                    <li>å¼·æ³•å‰‡: \(P(\bar{X}_n \to \mu) = 1\)ï¼ˆalmost sure convergenceï¼‰</li>
                </ul>
                almost sure convergenceã¯convergence in probabilityã‚ˆã‚Šã‚‚å¼·ã„æ¡ä»¶.
            </div>

            <h3>ğŸ’» Code Example2: Strong Law of Large Numbersã®demonstration</h3>
            <div class="code-block"># è¤‡æ•°ã®ç‹¬ç«‹ãªçµŒè·¯ã§ã®simulation
np.random.seed(123)
n_paths = 50  # çµŒè·¯æ•°
max_n = 5000
mu = 10
sigma = 3

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# (1) è¤‡æ•°çµŒè·¯ã§ã®sample meanã®æ¨ç§»
for i in range(n_paths):
    samples = np.random.normal(mu, sigma, max_n)
    cumulative_mean = np.cumsum(samples) / np.arange(1, max_n + 1)
    axes[0, 0].plot(cumulative_mean, alpha=0.3, linewidth=0.8, color='#667eea')

axes[0, 0].axhline(y=mu, color='red', linestyle='--', linewidth=2.5, label=f'Î¼={mu}')
axes[0, 0].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º n', fontsize=11)
axes[0, 0].set_ylabel('sample mean XÌ„â‚™', fontsize=11)
axes[0, 0].set_title(f'{n_paths}å€‹ã®ç‹¬ç«‹çµŒè·¯ï¼ˆStrong Law of Large Numbersï¼‰', fontsize=12, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)
axes[0, 0].set_ylim([mu-2, mu+2])

# (2) æœ€å¤§åå·®ã®æ¸›è¡°
n_checkpoints = np.array([10, 50, 100, 500, 1000, 2000, 5000])
max_deviations = []

for n in n_checkpoints:
    deviations = []
    for _ in range(1000):
        samples = np.random.normal(mu, sigma, n)
        sample_mean = samples.mean()
        deviations.append(abs(sample_mean - mu))
    max_deviations.append(np.max(deviations))

axes[0, 1].loglog(n_checkpoints, max_deviations, 'o-', color='#764ba2',
                   linewidth=2, markersize=8, label='æœ€å¤§åå·®')
axes[0, 1].loglog(n_checkpoints, sigma / np.sqrt(n_checkpoints), '--',
                   color='red', linewidth=2, label='Ïƒ/âˆšnï¼ˆç†è«–ï¼‰')
axes[0, 1].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º n', fontsize=11)
axes[0, 1].set_ylabel('max |XÌ„â‚™ - Î¼|', fontsize=11)
axes[0, 1].set_title('æœ€å¤§åå·®ã®æ¸›è¡°', fontsize=12, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3, which='both')

# (3) åæŸã®å¸¯åŸŸï¼ˆconfidence bandsï¼‰
n_trials = 500
all_paths = []
for _ in range(n_trials):
    samples = np.random.normal(mu, sigma, max_n)
    cumulative_mean = np.cumsum(samples) / np.arange(1, max_n + 1)
    all_paths.append(cumulative_mean)

all_paths = np.array(all_paths)
percentile_5 = np.percentile(all_paths, 5, axis=0)
percentile_95 = np.percentile(all_paths, 95, axis=0)
median = np.percentile(all_paths, 50, axis=0)

x_axis = np.arange(1, max_n + 1)
axes[1, 0].fill_between(x_axis, percentile_5, percentile_95,
                         alpha=0.3, color='#667eea', label='90%ä¿¡é ¼å¸¯')
axes[1, 0].plot(median, color='#667eea', linewidth=2, label='ä¸­å¤®å€¤')
axes[1, 0].axhline(y=mu, color='red', linestyle='--', linewidth=2, label=f'Î¼={mu}')
axes[1, 0].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º n', fontsize=11)
axes[1, 0].set_ylabel('sample mean XÌ„â‚™', fontsize=11)
axes[1, 0].set_title('åæŸã®ä¿¡é ¼å¸¯ï¼ˆ90%ï¼‰', fontsize=12, fontweight='bold')
axes[1, 0].legend()
axes[1, 0].grid(alpha=0.3)
axes[1, 0].set_ylim([mu-1, mu+1])

# (4) varianceã®æ¸›è¡°
variance_n = np.var(all_paths, axis=0)
theoretical_var = sigma**2 / x_axis

axes[1, 1].loglog(x_axis[::10], variance_n[::10], 'o', color='#667eea',
                   markersize=3, alpha=0.6, label='å®Ÿæ¸¬variance')
axes[1, 1].loglog(x_axis, theoretical_var, '--', color='red',
                   linewidth=2, label='ÏƒÂ²/nï¼ˆç†è«–ï¼‰')
axes[1, 1].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º n', fontsize=11)
axes[1, 1].set_ylabel('Var(XÌ„â‚™)', fontsize=11)
axes[1, 1].set_title('sample meanã®varianceã®æ¸›è¡°', fontsize=12, fontweight='bold')
axes[1, 1].legend()
axes[1, 1].grid(alpha=0.3, which='both')

plt.tight_layout()
plt.show()

print("Strong Law of Large Numbersã®demonstration:")
print(f"500å›ã®ç‹¬ç«‹è©¦è¡Œã«ãŠã‘ã‚‹ã€n=5000ã§ã®sample meanã®ç¯„å›²:")
print(f"  æœ€å°å€¤: {all_paths[:, -1].min():.4f}")
print(f"  æœ€å¤§å€¤: {all_paths[:, -1].max():.4f}")
print(f"  ä¸­å¤®å€¤: {np.median(all_paths[:, -1]):.4f}")
print(f"  standard deviation: {np.std(all_paths[:, -1]):.4f}")</div>

            <h2>2.3 Central Limit Theorem</h2>

            <div class="theorem">
                <strong>ğŸ“Š Theorem: Central Limit Theorem (CLT)</strong><br>
                ç‹¬ç«‹åŒåˆ†å¸ƒã®ç¢ºç‡å¤‰æ•° \(X_1, X_2, \ldots, X_n\) ã«ã¤ã„ã¦ã€expectation \(E[X_i] = \mu\)ã€variance \(Var(X_i) = \sigma^2 < \infty\) ã¨ã™ã‚‹ã¨ãã€
                standardizationã—ãŸsample meanã¯standard normal distributionã«convergence in distributionã—ã¾ã™ï¼š

                \[Z_n = \frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}} = \frac{\sum_{i=1}^n X_i - n\mu}{\sigma\sqrt{n}} \xrightarrow{d} N(0, 1)\]

                ã™ãªã‚ã¡ï¼š
                \[\lim_{n \to \infty} P(Z_n \leq z) = \Phi(z) = \int_{-\infty}^z \frac{1}{\sqrt{2\pi}} e^{-t^2/2} dt\]

                <strong>é‡è¦æ€§:</strong> å…ƒã®åˆ†å¸ƒã«é–¢ã‚ã‚‰ãšã€sample meanã¯normal distributionã«è¿‘ã¥ã.
            </div>

            <h3>ğŸ’» Code Example3: Central Limit Theoremã®visualization</h3>
            <div class="code-block"># æ§˜ã€…ãªåˆ†å¸ƒã‹ã‚‰sample meanã®åˆ†å¸ƒã‚’èª¿ã¹ã‚‹
np.random.seed(42)

# å…ƒã®åˆ†å¸ƒ
distributions = {
    'uniform distribution U(0,1)': {
        'generator': lambda n: np.random.uniform(0, 1, n),
        'mu': 0.5,
        'sigma': 1/np.sqrt(12)
    },
    'Bernoulli (p=0.3)': {
        'generator': lambda n: np.random.binomial(1, 0.3, n),
        'mu': 0.3,
        'sigma': np.sqrt(0.3 * 0.7)
    },
    'exponential distribution Exp(1)': {
        'generator': lambda n: np.random.exponential(1, n),
        'mu': 1,
        'sigma': 1
    },
    'beta distribution Beta(0.5,0.5)': {
        'generator': lambda n: np.random.beta(0.5, 0.5, n),
        'mu': 0.5,
        'sigma': 1/(2*np.sqrt(2))
    }
}

sample_sizes = [1, 5, 30, 100]
n_trials = 10000

fig, axes = plt.subplots(len(distributions), len(sample_sizes),
                          figsize=(16, 12))

for i, (dist_name, dist_info) in enumerate(distributions.items()):
    generator = dist_info['generator']
    mu = dist_info['mu']
    sigma = dist_info['sigma']

    for j, n in enumerate(sample_sizes):
        # sample meanã‚’è¨ˆç®—
        sample_means = []
        for _ in range(n_trials):
            samples = generator(n)
            sample_means.append(samples.mean())

        sample_means = np.array(sample_means)

        # standardization
        z_scores = (sample_means - mu) / (sigma / np.sqrt(n))

        # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
        axes[i, j].hist(z_scores, bins=50, density=True, alpha=0.6,
                         color='#667eea', edgecolor='black', linewidth=0.5)

        # standard normal distributionã‚’é‡ã­ã‚‹
        x = np.linspace(-4, 4, 1000)
        axes[i, j].plot(x, stats.norm.pdf(x), 'r-', linewidth=2.5,
                         label='N(0,1)')

        axes[i, j].set_xlim([-4, 4])
        axes[i, j].set_ylim([0, 0.5])

        if j == 0:
            axes[i, j].set_ylabel(dist_name, fontsize=10, fontweight='bold')
        if i == 0:
            axes[i, j].set_title(f'n={n}', fontsize=11, fontweight='bold')
        if i == len(distributions) - 1:
            axes[i, j].set_xlabel('Z = (XÌ„â‚™ - Î¼)/(Ïƒ/âˆšn)', fontsize=9)

        axes[i, j].grid(alpha=0.3)
        axes[i, j].legend(fontsize=8)

plt.suptitle('Central Limit Theoremã®visualizationï¼šæ§˜ã€…ãªåˆ†å¸ƒã§ã®verification',
             fontsize=14, fontweight='bold', y=0.995)
plt.tight_layout()
plt.show()

print("Central Limit Theoremã®verification:")
print("="*60)
for dist_name, dist_info in distributions.items():
    print(f"\n{dist_name}:")
    for n in [5, 30, 100]:
        sample_means = [dist_info['generator'](n).mean() for _ in range(n_trials)]
        z_scores = (np.array(sample_means) - dist_info['mu']) / (dist_info['sigma'] / np.sqrt(n))

        print(f"  n={n:3d}: å¹³å‡={np.mean(z_scores):6.3f}, variance={np.var(z_scores):6.3f}")</div>

            <h2>2.4 Convergence from Non-Normal to Normal Distribution</h2>

            <h3>ğŸ’» Code Example4: Convergence from Non-Normal to Normal Distribution</h3>
            <div class="code-block"># æ¥µç«¯ã«éæ­£è¦ãªåˆ†å¸ƒã§ã®Central Limit Theorem
np.random.seed(42)

# æ¥µç«¯ã«æ­ªã‚“ã åˆ†å¸ƒï¼ˆlognormal distributionï¼‰
mu_log = 0
sigma_log = 1

def lognormal_generator(n):
    return np.random.lognormal(mu_log, sigma_log, n)

# lognormal distributionã®ç†è«–çµ±è¨ˆé‡
true_mean = np.exp(mu_log + sigma_log**2 / 2)
true_var = (np.exp(sigma_log**2) - 1) * np.exp(2*mu_log + sigma_log**2)
true_sigma = np.sqrt(true_var)

sample_sizes = [1, 2, 5, 10, 30, 100]
n_trials = 5000

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

for idx, n in enumerate(sample_sizes):
    # sample meanã‚’è¨ˆç®—
    sample_means = []
    for _ in range(n_trials):
        samples = lognormal_generator(n)
        sample_means.append(samples.mean())

    sample_means = np.array(sample_means)

    # standardization
    z_scores = (sample_means - true_mean) / (true_sigma / np.sqrt(n))

    # Q-Qãƒ—ãƒ­ãƒƒãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿
    theoretical_quantiles = stats.norm.ppf(np.linspace(0.01, 0.99, 100))
    sample_quantiles = np.percentile(z_scores, np.linspace(1, 99, 100))

    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  + Q-Qãƒ—ãƒ­ãƒƒãƒˆ
    ax_main = axes[idx]

    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    ax_main.hist(z_scores, bins=40, density=True, alpha=0.5,
                  color='#667eea', edgecolor='black', linewidth=0.5)

    # ç†è«–çš„ãªnormal distribution
    x = np.linspace(-4, 4, 1000)
    ax_main.plot(x, stats.norm.pdf(x), 'r-', linewidth=2.5, label='N(0,1)')

    ax_main.set_xlim([-4, 4])
    ax_main.set_title(f'n={n}', fontsize=12, fontweight='bold')
    ax_main.set_xlabel('standardizationã•ã‚ŒãŸsample mean', fontsize=10)
    ax_main.set_ylabel('å¯†åº¦', fontsize=10)
    ax_main.legend(fontsize=9)
    ax_main.grid(alpha=0.3)

plt.suptitle('lognormal distributionï¼ˆæ¥µç«¯ã«æ­ªã‚“ã åˆ†å¸ƒï¼‰ã§ã®Central Limit Theorem',
             fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# Kolmogorov-Smirnovæ¤œå®šã§æ­£è¦æ€§ã‚’æ¤œå®š
print("lognormal distributionã§ã®Central Limit Theoremã®verification:")
print("="*60)
print(f"å…ƒã®åˆ†å¸ƒã®æ­ªåº¦: {stats.lognorm.stats(s=sigma_log, moments='s'):.4f}")
print(f"å…ƒã®åˆ†å¸ƒã®å°–åº¦: {stats.lognorm.stats(s=sigma_log, moments='k'):.4f}")
print("\nKolmogorov-Smirnovæ¤œå®šï¼ˆæ­£è¦æ€§ã®æ¤œå®šï¼‰:")
for n in sample_sizes:
    sample_means = [lognormal_generator(n).mean() for _ in range(n_trials)]
    z_scores = (np.array(sample_means) - true_mean) / (true_sigma / np.sqrt(n))

    ks_stat, p_value = stats.kstest(z_scores, 'norm')
    print(f"  n={n:3d}: KSçµ±è¨ˆé‡={ks_stat:.4f}, på€¤={p_value:.4f} "
          f"{'ï¼ˆnormal distributionã«å¾“ã†ï¼‰' if p_value > 0.05 else 'ï¼ˆæœ‰æ„ã«ç•°ãªã‚‹ï¼‰'}")</div>

            <h2>2.5 Distribution of Sample Mean and Sample Variance</h2>

            <div class="theorem">
                <strong>ğŸ“Š Theorem: Distribution of Sample Statistics</strong><br>
                \(X_1, \ldots, X_n \sim N(\mu, \sigma^2)\) ï¼ˆi.i.d.ï¼‰ã®ã¨ãï¼š

                <strong>sample mean:</strong>
                \[\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i \sim N\left(\mu, \frac{\sigma^2}{n}\right)\]

                <strong>sample variance:</strong>
                \[S^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})^2, \quad \frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}\]

                <strong>tçµ±è¨ˆé‡:</strong>
                \[T = \frac{\bar{X} - \mu}{S / \sqrt{n}} \sim t_{n-1}\]
            </div>

            <h3>ğŸ’» Code Example5: sample meanã®åˆ†å¸ƒ</h3>
            <div class="code-block"># sample meanã®åˆ†å¸ƒ
np.random.seed(42)
mu = 50
sigma = 10
sample_sizes = [5, 10, 30, 100]
n_trials = 10000

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

for idx, n in enumerate(sample_sizes):
    # sample meanã‚’è¨ˆç®—
    sample_means = []
    for _ in range(n_trials):
        samples = np.random.normal(mu, sigma, n)
        sample_means.append(samples.mean())

    sample_means = np.array(sample_means)

    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    axes[idx].hist(sample_means, bins=50, density=True, alpha=0.6,
                    color='#667eea', edgecolor='black', linewidth=0.5,
                    label='simulation')

    # ç†è«–åˆ†å¸ƒ N(Î¼, ÏƒÂ²/n)
    x = np.linspace(mu - 4*sigma/np.sqrt(n), mu + 4*sigma/np.sqrt(n), 1000)
    theoretical_pdf = stats.norm.pdf(x, mu, sigma/np.sqrt(n))
    axes[idx].plot(x, theoretical_pdf, 'r-', linewidth=2.5,
                    label=f'N({mu}, {sigma**2/n:.2f})')

    axes[idx].axvline(mu, color='green', linestyle='--', linewidth=2,
                       label=f'Î¼={mu}')
    axes[idx].set_title(f'n={n}', fontsize=12, fontweight='bold')
    axes[idx].set_xlabel('sample mean XÌ„', fontsize=11)
    axes[idx].set_ylabel('å¯†åº¦', fontsize=11)
    axes[idx].legend(fontsize=9)
    axes[idx].grid(alpha=0.3)

    # çµ±è¨ˆé‡
    print(f"n={n:3d}: ç†è«– ÏƒÂ²/n={sigma**2/n:6.2f}, "
          f"å®Ÿæ¸¬ Var(XÌ„)={np.var(sample_means):6.2f}")

plt.suptitle('sample meanã®åˆ†å¸ƒ', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()</div>

            <h3>ğŸ’» Code Example6: sample varianceã®åˆ†å¸ƒ</h3>
            <div class="code-block"># sample varianceã®åˆ†å¸ƒ
np.random.seed(42)
mu = 0
sigma = 5
n = 10  # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
n_trials = 10000

# sample varianceã‚’è¨ˆç®—
sample_variances = []
for _ in range(n_trials):
    samples = np.random.normal(mu, sigma, n)
    sample_var = np.var(samples, ddof=1)  # ä¸åvarianceï¼ˆn-1ã§å‰²ã‚‹ï¼‰
    sample_variances.append(sample_var)

sample_variances = np.array(sample_variances)

# (n-1)SÂ²/ÏƒÂ² ã®åˆ†å¸ƒï¼ˆã‚«ã‚¤äºŒä¹—åˆ†å¸ƒï¼‰
chi_squared_stat = (n - 1) * sample_variances / sigma**2

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# (1) sample varianceã®åˆ†å¸ƒ
axes[0].hist(sample_variances, bins=50, density=True, alpha=0.6,
              color='#667eea', edgecolor='black', linewidth=0.5,
              label='simulation')

# ç†è«–åˆ†å¸ƒï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ã•ã‚ŒãŸã‚«ã‚¤äºŒä¹—åˆ†å¸ƒï¼‰
x_var = np.linspace(0, 80, 1000)
theoretical_var_pdf = stats.chi2.pdf((n-1)*x_var/sigma**2, n-1) * (n-1)/sigma**2
axes[0].plot(x_var, theoretical_var_pdf, 'r-', linewidth=2.5,
              label='ç†è«–åˆ†å¸ƒ')

axes[0].axvline(sigma**2, color='green', linestyle='--', linewidth=2,
                 label=f'ÏƒÂ²={sigma**2}')
axes[0].set_xlabel('sample variance SÂ²', fontsize=11)
axes[0].set_ylabel('å¯†åº¦', fontsize=11)
axes[0].set_title('sample varianceã®åˆ†å¸ƒ', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# (2) ã‚«ã‚¤äºŒä¹—åˆ†å¸ƒã¨ã®æ¯”è¼ƒ
axes[1].hist(chi_squared_stat, bins=50, density=True, alpha=0.6,
              color='#764ba2', edgecolor='black', linewidth=0.5,
              label='simulation')

x_chi = np.linspace(0, 30, 1000)
theoretical_chi_pdf = stats.chi2.pdf(x_chi, n-1)
axes[1].plot(x_chi, theoretical_chi_pdf, 'r-', linewidth=2.5,
              label=f'Ï‡Â²({n-1})')

axes[1].set_xlabel('(n-1)SÂ²/ÏƒÂ²', fontsize=11)
axes[1].set_ylabel('å¯†åº¦', fontsize=11)
axes[1].set_title(f'ã‚«ã‚¤äºŒä¹—åˆ†å¸ƒ Ï‡Â²({n-1}) ã¨ã®æ¯”è¼ƒ', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("sample varianceã®verification:")
print(f"ç†è«–: E[SÂ²] = ÏƒÂ² = {sigma**2}")
print(f"å®Ÿæ¸¬: E[SÂ²] = {np.mean(sample_variances):.4f}")
print(f"\nç†è«–: E[(n-1)SÂ²/ÏƒÂ²] = n-1 = {n-1}")
print(f"å®Ÿæ¸¬: E[(n-1)SÂ²/ÏƒÂ²] = {np.mean(chi_squared_stat):.4f}")</div>

            <h2>2.6 Applications to Materials Science Data</h2>

            <h3>ğŸ’» Code Example7: Applications to Materials Science Data</h3>
            <div class="code-block"># å®Ÿãƒ‡ãƒ¼ã‚¿simulationï¼šææ–™å¼·åº¦ã®measurement
np.random.seed(42)

# ä»®æƒ³ãƒ‡ãƒ¼ã‚¿ï¼šã‚»ãƒ©ãƒŸãƒƒã‚¯ã‚¹ã®æ›²ã’å¼·åº¦ï¼ˆMPaï¼‰
# çœŸã®åˆ†å¸ƒã¯Weibull distributionï¼ˆææ–™å¼·åº¦ã®å…¸å‹çš„ãªåˆ†å¸ƒï¼‰
shape_param = 10  # å½¢çŠ¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
scale_param = 500  # ç‰¹æ€§å¼·åº¦ (MPa)

true_dist = stats.weibull_min(c=shape_param, scale=scale_param)
true_mean = true_dist.mean()
true_std = true_dist.std()

print("ææ–™å¼·åº¦ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆè§£æ")
print("="*60)
print(f"çœŸã®åˆ†å¸ƒ: Weibull distribution (shape={shape_param}, scale={scale_param})")
print(f"çœŸã®å¹³å‡: {true_mean:.2f} MPa")
print(f"çœŸã®standard deviation: {true_std:.2f} MPa")

# å®Ÿé¨“: ç•°ãªã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã§ã®measurement
sample_sizes = [3, 5, 10, 30, 50, 100]
n_experiments = 1000  # å®Ÿé¨“å›æ•°

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

for idx, n in enumerate(sample_sizes):
    # å„ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã§ã®sample meanã‚’è¨ˆç®—
    sample_means = []
    for _ in range(n_experiments):
        samples = true_dist.rvs(n)
        sample_means.append(samples.mean())

    sample_means = np.array(sample_means)

    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    axes[idx].hist(sample_means, bins=40, density=True, alpha=0.6,
                    color='#667eea', edgecolor='black', linewidth=0.5,
                    label='measurementçµæœ')

    # Central Limit Theoremã«ã‚ˆã‚‹è¿‘ä¼¼ï¼ˆnormal distributionï¼‰
    x = np.linspace(sample_means.min(), sample_means.max(), 1000)
    clt_pdf = stats.norm.pdf(x, true_mean, true_std/np.sqrt(n))
    axes[idx].plot(x, clt_pdf, 'r-', linewidth=2.5,
                    label='CLTã«ã‚ˆã‚‹è¿‘ä¼¼')

    axes[idx].axvline(true_mean, color='green', linestyle='--',
                       linewidth=2, label=f'çœŸã®å¹³å‡')
    axes[idx].set_title(f'è©¦æ–™æ•° n={n}', fontsize=12, fontweight='bold')
    axes[idx].set_xlabel('å¹³å‡å¼·åº¦ (MPa)', fontsize=10)
    axes[idx].set_ylabel('å¯†åº¦', fontsize=10)
    axes[idx].legend(fontsize=8)
    axes[idx].grid(alpha=0.3)

    # çµ±è¨ˆçš„æ¨è«–
    sem = true_std / np.sqrt(n)  # æ¨™æº–error
    ci_95 = stats.norm.interval(0.95, loc=true_mean, scale=sem)

    print(f"\nn={n:3d}:")
    print(f"  æ¨™æº–error (SEM): {sem:.2f} MPa")
    print(f"  95%confidence interval: [{ci_95[0]:.2f}, {ci_95[1]:.2f}] MPa")
    print(f"  å®Ÿæ¸¬å¹³å‡ã®standard deviation: {np.std(sample_means):.2f} MPa")

plt.suptitle('ææ–™å¼·åº¦measurementã«ãŠã‘ã‚‹Central Limit Theoremã®å¿œç”¨',
             fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# å®Ÿç”¨çš„ãªæ¨è«–
print("\n\nã€å®Ÿç”¨çš„ãªæ¨è«–ã€‘")
print("="*60)
n_test = 10  # å®Ÿéš›ã®è©¦é¨“ç‰‡æ•°
test_samples = true_dist.rvs(n_test)
test_mean = test_samples.mean()
test_std = test_samples.std(ddof=1)

print(f"è©¦é¨“ç‰‡æ•°: {n_test}")
print(f"measurementã•ã‚ŒãŸå¹³å‡å¼·åº¦: {test_mean:.2f} MPa")
print(f"measurementã•ã‚ŒãŸstandard deviation: {test_std:.2f} MPa")

# tåˆ†å¸ƒã«ã‚ˆã‚‹confidence intervalï¼ˆçœŸã®varianceãŒæœªçŸ¥ï¼‰
t_critical = stats.t.ppf(0.975, n_test - 1)
ci_lower = test_mean - t_critical * test_std / np.sqrt(n_test)
ci_upper = test_mean + t_critical * test_std / np.sqrt(n_test)

print(f"\n95%confidence intervalï¼ˆtåˆ†å¸ƒï¼‰: [{ci_lower:.2f}, {ci_upper:.2f}] MPa")
print(f"çœŸã®å¹³å‡ {true_mean:.2f} MPa ã¯confidence intervalã«{'å«ã¾ã‚Œã‚‹' if ci_lower <= true_mean <= ci_upper else 'å«ã¾ã‚Œãªã„'}")</div>

            <div class="note">
                <strong>ğŸ’¡ Note:</strong> materials scienceã§ is é™ã‚‰ã‚ŒãŸè©¦é¨“ç‰‡æ•°ã‹ã‚‰æ¯é›†å›£ã®ç‰¹æ€§ã‚’æ¨å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Š.Central Limit Theoremã«ã‚ˆã‚Šã€å…ƒã®åˆ†å¸ƒï¼ˆWeibull distributionãªã©ï¼‰ãŒnormal distributionã§ãªãã¦ã‚‚ã€sample meanã¯normal distributionã«å¾“ã†ãŸã‚ã€tæ¤œå®šã‚„confidence intervalã®æ§‹ç¯‰ãŒå¯èƒ½ã«ãªã‚Š.
            </div>

            <h2>Exercises</h2>

            <div class="exercise">
                <strong>ğŸ“ Exercise1: å¤§æ•°ã®æ³•å‰‡ã®verification</strong><br>
                ã‚µã‚¤ã‚³ãƒ­ã‚’æŒ¯ã‚‹å®Ÿé¨“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã—ã€ä»¥ä¸‹ã‚’ç¢ºèªã›ã‚ˆï¼š
                <ol>
                    <li>å‡ºç›®ã®sample meanãŒç†è«–å€¤3.5ã«åæŸã™ã‚‹ã“ã¨ã‚’visualizationã›ã‚ˆ</li>
                    <li>ãƒã‚§ãƒ“ã‚·ã‚§ãƒ•ã®ä¸ç­‰å¼ã‚’ç”¨ã„ã¦ã€|XÌ„â‚™ - 3.5| > 0.2 ã¨ãªã‚‹ç¢ºç‡ã®ä¸Šç•Œã‚’è¨ˆç®—ã›ã‚ˆ</li>
                    <li>ç•°ãªã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆn=10, 100, 1000ï¼‰ã§ã®åæŸé€Ÿåº¦ã‚’æ¯”è¼ƒã›ã‚ˆ</li>
                </ol>
            </div>

            <div class="exercise">
                <strong>ğŸ“ Exercise2: Central Limit Theoremã®å¿œç”¨</strong><br>
                uniform distribution U(0, 10) ã‹ã‚‰ç‹¬ç«‹ã« n å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å–ã‚Šã€sample meanã‚’è¨ˆç®—ã™ã‚‹ï¼š
                <ol>
                    <li>n=5, 10, 30, 100 ã®å„å ´åˆã§ã€sample meanã®åˆ†å¸ƒã‚’ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã§visualizationã›ã‚ˆ</li>
                    <li>standardizationã—ãŸsample meanãŒstandard normal distributionã«å¾“ã†ã“ã¨ã‚’ Q-Q ãƒ—ãƒ­ãƒƒãƒˆã§ç¢ºèªã›ã‚ˆ</li>
                    <li>Kolmogorov-Smirnovæ¤œå®šã‚’ç”¨ã„ã¦ã€æ­£è¦æ€§ã‚’çµ±è¨ˆçš„ã«verificationã›ã‚ˆ</li>
                </ol>
            </div>

            <div class="exercise">
                <strong>ğŸ“ Exercise3: confidence intervalã®æ§‹ç¯‰</strong><br>
                normal distribution N(100, 15Â²) ã‹ã‚‰ n=25 ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’è¨ˆç®—ã›ã‚ˆï¼š
                <ol>
                    <li>sample meanã®95%confidence intervalã‚’æ§‹ç¯‰ã›ã‚ˆï¼ˆæ¯varianceæ—¢çŸ¥ã®å ´åˆï¼‰</li>
                    <li>sample meanã¨sample varianceã‚’è¨ˆç®—ã—ã€tåˆ†å¸ƒã‚’ç”¨ã„ãŸ95%confidence intervalã‚’æ§‹ç¯‰ã›ã‚ˆï¼ˆæ¯varianceæœªçŸ¥ã®å ´åˆï¼‰</li>
                    <li>1000å›ã®ç‹¬ç«‹ãªsimulationã‚’è¡Œã„ã€confidence intervalãŒçœŸã®å¹³å‡ã‚’å«ã‚€å‰²åˆã‚’ç¢ºèªã›ã‚ˆ</li>
                </ol>
            </div>

            <div class="nav-buttons">
                <a href="chapter-1.html" class="nav-button">â† Chapter 1</a>
                <a href="chapter-3.html" class="nav-button">To Chapter 3 â†’</a>
            </div>
        </div>
    </div>

    <footer>
        <p>&copy; 2025 AI Terakoya - Fundamentals of Mathematics & Physics Dojo</p>
    </footer>
</body>
</html>
