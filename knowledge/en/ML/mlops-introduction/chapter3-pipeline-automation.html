<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 3: Pipeline Automation - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/mlops-introduction/index.html">Mlops</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 3: Pipeline Automation</h1>
            <p class="subtitle">MLãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®è‡ªå‹•åŒ–ã¨ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– Reading Time: 30-35 minutes</span>
                <span class="meta-item">ğŸ“Š Difficulty: Intermediate</span>
                <span class="meta-item">ğŸ’» Code Examples: 12</span>
                <span class="meta-item">ğŸ“ Exercises: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Learning Objectives</h2>
<p>ã“ã® ChapterReadã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è¨­è¨ˆåŸå‰‡ã¨DAGæ§‹é€ ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Apache Airflowã§ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
<li>âœ… Kubeflow Pipelinesã§ã‚³ãƒ³ãƒ†ãƒŠåŒ–ã•ã‚ŒãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆã§ãã‚‹</li>
<li>âœ… Prefectã§å‹•çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… å†ªç­‰æ€§ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã€ãƒ†ã‚¹ã‚¿ãƒ“ãƒªãƒ†ã‚£ã‚’å®Ÿç¾ã§ãã‚‹</li>
<li>âœ… æœ¬ç•ªç’°å¢ƒã§é‹ç”¨å¯èƒ½ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’è¨­è¨ˆã§ãã‚‹</li>
</ul>

<hr>

<h2>3.1 MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è¨­è¨ˆ</h2>

<h3>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ã¯</h3>

<p><strong>MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆMachine Learning Pipelineï¼‰</strong>ã¯ã€ãƒ‡ãƒ¼ã‚¿åé›†ã‹ã‚‰äºˆæ¸¬ã¾ã§ã®ä¸€é€£ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’è‡ªå‹•åŒ–ã™ã‚‹ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ã™ã€‚</p>

<blockquote>
<p>ã€Œæ‰‹ä½œæ¥­ã§ã®å†ç¾ã¯ä¸å¯èƒ½ã€‚è‡ªå‹•åŒ–ã•ã‚ŒãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã“ããŒã€ä¿¡é ¼ã§ãã‚‹MLã‚·ã‚¹ãƒ†ãƒ ã®åŸºç›¤ã§ã™ã€‚ã€</p>
</blockquote>

<h3>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹æˆè¦ç´ </h3>

<table>
<thead>
<tr>
<th>è¦ç´ </th>
<th>èª¬æ˜</th>
<th>ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ‡ãƒ¼ã‚¿å–å¾—</strong></td>
<td>å¤–éƒ¨ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’åé›†</td>
<td>APIå‘¼ã³å‡ºã—ã€DBæŠ½å‡º</td>
</tr>
<tr>
<td><strong>å‰å‡¦ç†</strong></td>
<td>ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã€å¤‰æ›</td>
<td>æ¬ æå€¤å‡¦ç†ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°</td>
</tr>
<tr>
<td><strong>ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</strong></td>
<td>ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ç”¨ã®ç‰¹å¾´é‡ä½œæˆ</td>
<td>ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ›ã€é›†ç´„</td>
</tr>
<tr>
<td><strong>ãƒ¢ãƒ‡ãƒ«è¨“ç·´</strong></td>
<td>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å­¦ç¿’</td>
<td>fitã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´</td>
</tr>
<tr>
<td><strong>è©•ä¾¡</strong></td>
<td>ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®æ¸¬å®š</td>
<td>ç²¾åº¦ã€å†ç¾ç‡ã€F1ã‚¹ã‚³ã‚¢</td>
</tr>
<tr>
<td><strong>ãƒ‡ãƒ—ãƒ­ã‚¤</strong></td>
<td>æœ¬ç•ªç’°å¢ƒã¸ã®é…ç½®</td>
<td>ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã€APIåŒ–</td>
</tr>
</tbody>
</table>

<h3>DAGï¼ˆDirected Acyclic Graphï¼‰</h3>

<p><strong>DAG</strong>ã¯ã€ã‚¿ã‚¹ã‚¯é–“ã®ä¾å­˜é–¢ä¿‚ã‚’è¡¨ã™æœ‰å‘éå·¡å›ã‚°ãƒ©ãƒ•ã§ã™ã€‚MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ¨™æº–çš„ãªè¡¨ç¾æ–¹æ³•ã¨ã—ã¦åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚</p>

<div class="mermaid">
graph TD
    A[ãƒ‡ãƒ¼ã‚¿å–å¾—] --> B[ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼]
    B --> C[å‰å‡¦ç†]
    C --> D[ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°]
    D --> E[è¨“ç·´ãƒ‡ãƒ¼ã‚¿ minuteså‰²]
    E --> F[ãƒ¢ãƒ‡ãƒ«è¨“ç·´]
    E --> G[ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´]
    F --> H[ãƒ¢ãƒ‡ãƒ«è©•ä¾¡]
    G --> H
    H --> I{æ€§èƒ½OK?}
    I -->|Yes| J[ãƒ¢ãƒ‡ãƒ«ç™»éŒ²]
    I -->|No| K[ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡]
    J --> L[ãƒ‡ãƒ—ãƒ­ã‚¤]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
    style F fill:#ffe0b2
    style G fill:#ffe0b2
    style H fill:#c8e6c9
    style I fill:#ffccbc
    style J fill:#c5cae9
    style K fill:#ffcdd2
    style L fill:#b2dfdb
</div>

<h3>ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ vs ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</h3>

<table>
<thead>
<tr>
<th>è¦³ç‚¹</th>
<th>ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</th>
<th>ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>åˆ¶å¾¡</strong></td>
<td>ä¸­å¤®é›†æ¨©çš„ï¼ˆã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãŒç®¡ç†ï¼‰</td>
<td> distributedçš„ï¼ˆå„ã‚¿ã‚¹ã‚¯ãŒç‹¬ç«‹ï¼‰</td>
</tr>
<tr>
<td><strong>ä¾‹</strong></td>
<td>Airflowã€Prefectã€Dagster</td>
<td>Step Functionsã€Argo Workflows</td>
</tr>
<tr>
<td><strong>é©ç”¨å ´é¢</strong></td>
<td>è¤‡é›‘ãªä¾å­˜é–¢ä¿‚ã€å‹•çš„ã‚¿ã‚¹ã‚¯</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ•ãƒ­ãƒ¼ã€ã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•</td>
</tr>
<tr>
<td><strong>å¯è¦–åŒ–</strong></td>
<td>UIå®Œå‚™ã€ãƒ­ã‚°è¿½è·¡</td>
<td>åŸºæœ¬çš„ãªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º</td>
</tr>
</tbody>
</table>

<h3>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­è¨ˆã®åŸå‰‡</h3>

<pre><code class="language-python">"""
MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­è¨ˆã®5ã¤ã®åŸå‰‡
"""

# 1. å†ªç­‰æ€§ï¼ˆIdempotencyï¼‰
# åŒã˜å…¥åŠ›ã‹ã‚‰åŒã˜å‡ºåŠ›ãŒå¾—ã‚‰ã‚Œã‚‹
def preprocess_data(input_path, output_path):
    """åŒã˜input_pathã‹ã‚‰å¸¸ã«åŒã˜output_pathã‚’ç”Ÿæˆ"""
    # æ—¢å­˜ã®outputã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰å†ç”Ÿæˆ
    if os.path.exists(output_path):
        os.remove(output_path)
    # å‡¦ç†å®Ÿè¡Œ...

# 2. å†å®Ÿè¡Œå¯èƒ½æ€§ï¼ˆRerunabilityï¼‰
# å¤±æ•—ã—ãŸã‚¿ã‚¹ã‚¯ã‚’å®‰å…¨ã«å†å®Ÿè¡Œã§ãã‚‹
def train_model(data_path, model_path, force=False):
    """force=Trueã§æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šæ›¸ã"""
    if os.path.exists(model_path) and not force:
        print(f"ãƒ¢ãƒ‡ãƒ«æ—¢å­˜: {model_path}")
        return
    # è¨“ç·´å®Ÿè¡Œ...

# 3. ç–çµåˆï¼ˆLoose Couplingï¼‰
# ã‚¿ã‚¹ã‚¯é–“ã®ä¾å­˜ã‚’æœ€å°åŒ–
def extract_features(raw_data):
    """rawãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆå‰ã®ã‚¿ã‚¹ã‚¯ã«ä¾å­˜ã—ãªã„ï¼‰"""
    return features

# 4. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ï¼ˆParameterizationï¼‰
# ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã‚’é¿ã‘ã€è¨­å®šã‚’å¤–éƒ¨åŒ–
def run_pipeline(config_path):
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    with open(config_path) as f:
        config = yaml.safe_load(f)
    # config['model_type'], config['batch_size']ãªã©ã‚’ä½¿ç”¨

# 5. å¯è¦³æ¸¬æ€§ï¼ˆObservabilityï¼‰
# ãƒ­ã‚°ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’è¨˜éŒ²
import logging

def process_batch(batch_id):
    logging.info(f"Batch {batch_id} å‡¦ç†é–‹å§‹")
    try:
        # å‡¦ç†...
        logging.info(f"Batch {batch_id} æˆåŠŸ")
    except Exception as e:
        logging.error(f"Batch {batch_id} å¤±æ•—: {e}")
        raise
</code></pre>

<hr>

<h2>3.2 Apache Airflow</h2>

<h3>Airflowã¨ã¯</h3>

<p><strong>Apache Airflow</strong>ã¯ã€Pythonã§ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®šç¾©ã—ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œã§ãã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã™ã€‚</p>

<h4>Airflowã®ç‰¹å¾´</h4>

<ul>
<li><strong>DAGãƒ™ãƒ¼ã‚¹</strong>: ã‚¿ã‚¹ã‚¯ã‚’DAGã§è¡¨ç¾</li>
<li><strong>è±Šå¯Œãªã‚ªãƒšãƒ¬ãƒ¼ã‚¿</strong>: Pythonã€Bashã€SQLã€ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ãªã©</li>
<li><strong>ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©</strong>: Cronå¼ã§ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°</li>
<li><strong>Web UI</strong>: DAGã®å¯è¦–åŒ–ã€ãƒ­ã‚°ç¢ºèª</li>
<li><strong>æ‹¡å¼µæ€§</strong>: ã‚«ã‚¹ã‚¿ãƒ ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ã€ãƒ•ãƒƒã‚¯ã€ã‚»ãƒ³ã‚µãƒ¼</li>
</ul>

<h3>Airflow ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h3>

<div class="mermaid">
graph TB
    A[Web Server] --> B[Scheduler]
    B --> C[Executor]
    C --> D1[Worker 1]
    C --> D2[Worker 2]
    C --> D3[Worker N]
    B --> E[Metadata DB]
    A --> E
    D1 --> F[Task Logs]
    D2 --> F
    D3 --> F

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D1 fill:#e8f5e9
    style D2 fill:#e8f5e9
    style D3 fill:#e8f5e9
    style E fill:#ffe0b2
    style F fill:#ffccbc
</div>

<table>
<thead>
<tr>
<th>ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</th>
<th>å½¹å‰²</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Scheduler</strong></td>
<td>DAGã®ç›£è¦–ã€ã‚¿ã‚¹ã‚¯ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«</td>
</tr>
<tr>
<td><strong>Executor</strong></td>
<td>ã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œç®¡ç†ï¼ˆLocalã€Celeryã€Kubernetesï¼‰</td>
</tr>
<tr>
<td><strong>Worker</strong></td>
<td>å®Ÿéš›ã®ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ</td>
</tr>
<tr>
<td><strong>Web Server</strong></td>
<td>UIæä¾›ã€DAGå¯è¦–åŒ–</td>
</tr>
<tr>
<td><strong>Metadata DB</strong></td>
<td>DAGã€ã‚¿ã‚¹ã‚¯ã€å®Ÿè¡Œå±¥æ­´ã®ä¿å­˜</td>
</tr>
</tbody>
</table>

<h3>åŸºæœ¬çš„ãªDAGå®šç¾©</h3>

<pre><code class="language-python">from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¼•æ•°
default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,  # éå»ã®å®Ÿè¡Œã«ä¾å­˜ã—ãªã„
    'email': ['alerts@example.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,  # å¤±æ•—æ™‚ã«2å›å†è©¦è¡Œ
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=1),
}

# DAGå®šç¾©
dag = DAG(
    'ml_pipeline_basic',
    default_args=default_args,
    description='åŸºæœ¬çš„ãªMLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³',
    schedule_interval='0 2 * * *',  # æ¯æ—¥2:00 AMå®Ÿè¡Œ
    start_date=datetime(2025, 1, 1),
    catchup=False,  # éå»ã®æœªå®Ÿè¡Œ minutesã‚’å®Ÿè¡Œã—ãªã„
    tags=['ml', 'training'],
)

# ã‚¿ã‚¹ã‚¯å®šç¾©
def extract_data(**context):
    """ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
    print("ãƒ‡ãƒ¼ã‚¿ã‚’DBã‹ã‚‰æŠ½å‡ºä¸­...")
    # å®Ÿéš›ã®æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯
    data = {'records': 1000, 'timestamp': datetime.now().isoformat()}
    # XComã§æ¬¡ã®ã‚¿ã‚¹ã‚¯ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã™
    context['ti'].xcom_push(key='extracted_data', value=data)
    return data

def transform_data(**context):
    """ãƒ‡ãƒ¼ã‚¿å¤‰æ›"""
    # å‰ã®ã‚¿ã‚¹ã‚¯ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    ti = context['ti']
    data = ti.xcom_pull(key='extracted_data', task_ids='extract')
    print(f"ãƒ‡ãƒ¼ã‚¿å¤‰æ›ä¸­: {data['records']}ä»¶")
    # å¤‰æ›å‡¦ç†...
    transformed = {'records': data['records'], 'features': 50}
    return transformed

def train_model(**context):
    """ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
    ti = context['ti']
    data = ti.xcom_pull(task_ids='transform')
    print(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­: {data['features']}ç‰¹å¾´é‡")
    # è¨“ç·´å‡¦ç†...
    model_metrics = {'accuracy': 0.92, 'f1': 0.89}
    return model_metrics

# ã‚¿ã‚¹ã‚¯ã®ä½œæˆ
extract_task = PythonOperator(
    task_id='extract',
    python_callable=extract_data,
    dag=dag,
)

transform_task = PythonOperator(
    task_id='transform',
    python_callable=transform_data,
    dag=dag,
)

train_task = PythonOperator(
    task_id='train',
    python_callable=train_model,
    dag=dag,
)

validate_task = BashOperator(
    task_id='validate',
    bash_command='echo "ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼å®Œäº†"',
    dag=dag,
)

# ã‚¿ã‚¹ã‚¯ä¾å­˜é–¢ä¿‚ã®å®šç¾©
extract_task >> transform_task >> train_task >> validate_task
</code></pre>

<h3>å®Œå…¨ãªMLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¾‹</h3>

<pre><code class="language-python">from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.utils.trigger_rule import TriggerRule
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score

default_args = {
    'owner': 'data-science',
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'complete_ml_pipeline',
    default_args=default_args,
    description='å®Œå…¨ãªMLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆè¨“ç·´ã‹ã‚‰è©•ä¾¡ã¾ã§ï¼‰',
    schedule_interval='@daily',
    start_date=datetime(2025, 1, 1),
    catchup=False,
)

# ãƒ‡ãƒ¼ã‚¿åé›†
def collect_data(**context):
    """ãƒ‡ãƒ¼ã‚¿åé›†ã‚¿ã‚¹ã‚¯"""
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå®Ÿéš›ã¯DBã‚„APIã‹ã‚‰å–å¾—ï¼‰
    import numpy as np
    np.random.seed(42)

    n_samples = 1000
    data = pd.DataFrame({
        'feature1': np.random.randn(n_samples),
        'feature2': np.random.randn(n_samples),
        'feature3': np.random.randn(n_samples),
        'target': np.random.randint(0, 2, n_samples)
    })

    # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    data.to_csv('/tmp/raw_data.csv', index=False)
    print(f"ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†: {len(data)}ä»¶")

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’XComã§å…±æœ‰
    context['ti'].xcom_push(key='data_size', value=len(data))
    return '/tmp/raw_data.csv'

# ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
def validate_data(**context):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼"""
    data = pd.read_csv('/tmp/raw_data.csv')

    # æ¤œè¨¼ãƒã‚§ãƒƒã‚¯
    checks = {
        'no_nulls': data.isnull().sum().sum() == 0,
        'sufficient_size': len(data) >= 500,
        'target_balance': data['target'].value_counts().min() / len(data) >= 0.3
    }

    print(f"ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼çµæœ: {checks}")

    if not all(checks.values()):
        raise ValueError(f"ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯å¤±æ•—: {checks}")

    return True

# å‰å‡¦ç†
def preprocess_data(**context):
    """å‰å‡¦ç†ã‚¿ã‚¹ã‚¯"""
    data = pd.read_csv('/tmp/raw_data.csv')

    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã® minutesé›¢
    X = data.drop('target', axis=1)
    y = data['target']

    # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆ minuteså‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # ä¿å­˜
    X_train.to_csv('/tmp/X_train.csv', index=False)
    X_test.to_csv('/tmp/X_test.csv', index=False)
    y_train.to_csv('/tmp/y_train.csv', index=False)
    y_test.to_csv('/tmp/y_test.csv', index=False)

    print(f"å‰å‡¦ç†å®Œäº†: è¨“ç·´={len(X_train)}, ãƒ†ã‚¹ãƒˆ={len(X_test)}")
    return True

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
def train_model(**context):
    """ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚¿ã‚¹ã‚¯"""
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    X_train = pd.read_csv('/tmp/X_train.csv')
    y_train = pd.read_csv('/tmp/y_train.csv').values.ravel()

    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    joblib.dump(model, '/tmp/model.pkl')
    print("ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")
    return '/tmp/model.pkl'

# ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
def evaluate_model(**context):
    """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚¿ã‚¹ã‚¯"""
    # ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    X_test = pd.read_csv('/tmp/X_test.csv')
    y_test = pd.read_csv('/tmp/y_test.csv').values.ravel()
    model = joblib.load('/tmp/model.pkl')

    # äºˆæ¸¬ã¨è©•ä¾¡
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    metrics = {
        'accuracy': float(accuracy),
        'f1_score': float(f1)
    }

    print(f"è©•ä¾¡å®Œäº†: {metrics}")

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’XComã§å…±æœ‰
    context['ti'].xcom_push(key='metrics', value=metrics)
    return metrics

# ãƒ¢ãƒ‡ãƒ«å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆ branchingï¼‰
def check_model_quality(**context):
    """ãƒ¢ãƒ‡ãƒ«å“è³ªã«åŸºã¥ã„ã¦æ¬¡ã®ã‚¿ã‚¹ã‚¯ã‚’æ±ºå®š"""
    ti = context['ti']
    metrics = ti.xcom_pull(key='metrics', task_ids='evaluate')

    # ç²¾åº¦é–¾å€¤
    threshold = 0.8

    if metrics['accuracy'] >= threshold:
        print(f"ãƒ¢ãƒ‡ãƒ«æ‰¿èª: accuracy={metrics['accuracy']:.3f}")
        return 'register_model'
    else:
        print(f"ãƒ¢ãƒ‡ãƒ«å´ä¸‹: accuracy={metrics['accuracy']:.3f} < {threshold}")
        return 'send_alert'

# ãƒ¢ãƒ‡ãƒ«ç™»éŒ²
def register_model(**context):
    """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²"""
    ti = context['ti']
    metrics = ti.xcom_pull(key='metrics', task_ids='evaluate')

    # å®Ÿéš›ã¯MLflowãªã©ã®ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²
    print(f"ãƒ¢ãƒ‡ãƒ«ç™»éŒ²: accuracy={metrics['accuracy']:.3f}")

    # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
    import shutil
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    model_version = f'/tmp/model_{timestamp}.pkl'
    shutil.copy('/tmp/model.pkl', model_version)

    print(f"ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {model_version}")
    return model_version

# ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡
def send_alert(**context):
    """å“è³ªä¸è¶³ã®å ´åˆã«ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡"""
    ti = context['ti']
    metrics = ti.xcom_pull(key='metrics', task_ids='evaluate')

    # å®Ÿéš›ã¯Slackã‚„Emailã§é€šçŸ¥
    print(f"âš ï¸ ã‚¢ãƒ©ãƒ¼ãƒˆ: ãƒ¢ãƒ‡ãƒ«å“è³ªä¸è¶³ - {metrics}")
    return True

# ã‚¿ã‚¹ã‚¯å®šç¾©
start = DummyOperator(task_id='start', dag=dag)

collect = PythonOperator(
    task_id='collect_data',
    python_callable=collect_data,
    dag=dag,
)

validate = PythonOperator(
    task_id='validate_data',
    python_callable=validate_data,
    dag=dag,
)

preprocess = PythonOperator(
    task_id='preprocess',
    python_callable=preprocess_data,
    dag=dag,
)

train = PythonOperator(
    task_id='train',
    python_callable=train_model,
    dag=dag,
)

evaluate = PythonOperator(
    task_id='evaluate',
    python_callable=evaluate_model,
    dag=dag,
)

quality_check = BranchPythonOperator(
    task_id='quality_check',
    python_callable=check_model_quality,
    dag=dag,
)

register = PythonOperator(
    task_id='register_model',
    python_callable=register_model,
    dag=dag,
)

alert = PythonOperator(
    task_id='send_alert',
    python_callable=send_alert,
    dag=dag,
)

end = DummyOperator(
    task_id='end',
    trigger_rule=TriggerRule.ONE_SUCCESS,  # ã©ã¡ã‚‰ã‹ãŒæˆåŠŸã™ã‚Œã°å®Ÿè¡Œ
    dag=dag,
)

# DAGæ§‹é€ 
start >> collect >> validate >> preprocess >> train >> evaluate >> quality_check
quality_check >> [register, alert]
register >> end
alert >> end
</code></pre>

<h3>Airflowã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>

<pre><code class="language-python">"""
Airflow ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
"""

# 1. ã‚¿ã‚¹ã‚¯ã®å†ªç­‰æ€§ã‚’ä¿è¨¼
def idempotent_task(output_path):
    """åŒã˜å…¥åŠ›ã‹ã‚‰å¸¸ã«åŒã˜å‡ºåŠ›ã‚’ç”Ÿæˆ"""
    # æ—¢å­˜ã®å‡ºåŠ›ã‚’å‰Šé™¤
    if os.path.exists(output_path):
        os.remove(output_path)
    # å‡¦ç†å®Ÿè¡Œ
    process_data(output_path)

# 2. XComã¯å°ã•ãªãƒ‡ãƒ¼ã‚¿ã®ã¿
def small_xcom(**context):
    """å¤§ããªãƒ‡ãƒ¼ã‚¿ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã§å—ã‘æ¸¡ã—"""
    # âŒ æ‚ªã„ä¾‹: å¤§ããªDataFrameã‚’XComã§æ¸¡ã™
    # context['ti'].xcom_push(key='data', value=large_df)

    # âœ… è‰¯ã„ä¾‹: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ¸¡ã™
    large_df.to_parquet('/tmp/data.parquet')
    context['ti'].xcom_push(key='data_path', value='/tmp/data.parquet')

# 3. ã‚¿ã‚¹ã‚¯ã®ç²’åº¦ã‚’é©åˆ‡ã«
# âŒ æ‚ªã„ä¾‹: 1ã¤ã®ã‚¿ã‚¹ã‚¯ã§å…¨å‡¦ç†
def monolithic_task():
    collect_data()
    preprocess_data()
    train_model()
    evaluate_model()

# âœ… è‰¯ã„ä¾‹: å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’ minutesé›¢
collect_task >> preprocess_task >> train_task >> evaluate_task

# 4. å‹•çš„ã‚¿ã‚¹ã‚¯ç”Ÿæˆ
from airflow.operators.python import PythonOperator

def create_dynamic_tasks(dag):
    """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’ä¸¦åˆ—è¨“ç·´"""
    models = ['rf', 'xgboost', 'lightgbm']

    for model_name in models:
        PythonOperator(
            task_id=f'train_{model_name}',
            python_callable=train_specific_model,
            op_kwargs={'model_type': model_name},
            dag=dag,
        )

# 5. ã‚»ãƒ³ã‚µãƒ¼ã§ã®å¾…æ©Ÿ
from airflow.sensors.filesystem import FileSensor

wait_for_data = FileSensor(
    task_id='wait_for_data',
    filepath='/data/input.csv',
    poke_interval=60,  # 60ç§’ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
    timeout=3600,  # 1 hoursã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
    dag=dag,
)
</code></pre>

<hr>

<h2>3.3 Kubeflow Pipelines</h2>

<h3>Kubeflow Pipelinesã¨ã¯</h3>

<p><strong>Kubeflow Pipelines</strong>ã¯ã€Kubernetesä¸Šã§MLãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ§‹ç¯‰ã€ãƒ‡ãƒ—ãƒ­ã‚¤ã€ç®¡ç†ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã™ã€‚</p>

<h4>ä¸»ãªç‰¹å¾´</h4>

<ul>
<li><strong>ã‚³ãƒ³ãƒ†ãƒŠãƒã‚¤ãƒ†ã‚£ãƒ–</strong>: å„ã‚¿ã‚¹ã‚¯ãŒDockerã‚³ãƒ³ãƒ†ãƒŠã§å®Ÿè¡Œ</li>
<li><strong>å†åˆ©ç”¨å¯èƒ½ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</strong>: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®éƒ¨å“åŒ–</li>
<li><strong>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£</strong>: Kubernetesã®è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ´»ç”¨</li>
<li><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°</strong>: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†</li>
<li><strong>å®Ÿé¨“è¿½è·¡</strong>: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã®æ¯”è¼ƒã¨ minutesæ</li>
</ul>

<h3>Kubeflowã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h3>

<div class="mermaid">
graph TB
    A[Pipeline DSL] --> B[Compiler]
    B --> C[Pipeline YAML]
    C --> D[Kubeflow API Server]
    D --> E[Argo Workflows]
    E --> F1[Pod: ãƒ‡ãƒ¼ã‚¿åé›†]
    E --> F2[Pod: å‰å‡¦ç†]
    E --> F3[Pod: è¨“ç·´]
    E --> F4[Pod: è©•ä¾¡]
    D --> G[Metadata Store]
    D --> H[Artifact Store]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#ffe0b2
    style F1 fill:#ffccbc
    style F2 fill:#ffccbc
    style F3 fill:#ffccbc
    style F4 fill:#ffccbc
    style G fill:#c5cae9
    style H fill:#b2dfdb
</div>

<h3>åŸºæœ¬çš„ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h3>

<pre><code class="language-python">import kfp
from kfp import dsl
from kfp.dsl import component, Input, Output, Dataset, Model

# ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå®šç¾©ï¼ˆè»½é‡ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼‰
@component(
    base_image='python:3.9',
    packages_to_install=['pandas==2.0.0', 'scikit-learn==1.3.0']
)
def load_data(output_dataset: Output[Dataset]):
    """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"""
    import pandas as pd

    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    data = pd.DataFrame({
        'feature1': [1, 2, 3, 4, 5],
        'feature2': [2, 4, 6, 8, 10],
        'target': [0, 0, 1, 1, 1]
    })

    # å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ä¿å­˜
    data.to_csv(output_dataset.path, index=False)
    print(f"ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {output_dataset.path}")

@component(
    base_image='python:3.9',
    packages_to_install=['pandas==2.0.0', 'scikit-learn==1.3.0']
)
def train_model(
    input_dataset: Input[Dataset],
    output_model: Output[Model],
    n_estimators: int = 100
):
    """ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"""
    import pandas as pd
    from sklearn.ensemble import RandomForestClassifier
    import joblib

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data = pd.read_csv(input_dataset.path)
    X = data[['feature1', 'feature2']]
    y = data['target']

    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
    model.fit(X, y)

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    joblib.dump(model, output_model.path)
    print(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {output_model.path}")

@component(
    base_image='python:3.9',
    packages_to_install=['pandas==2.0.0', 'scikit-learn==1.3.0']
)
def evaluate_model(
    input_dataset: Input[Dataset],
    input_model: Input[Model]
) -> float:
    """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"""
    import pandas as pd
    import joblib
    from sklearn.metrics import accuracy_score

    # ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    data = pd.read_csv(input_dataset.path)
    X = data[['feature1', 'feature2']]
    y = data['target']
    model = joblib.load(input_model.path)

    # è©•ä¾¡
    y_pred = model.predict(X)
    accuracy = accuracy_score(y, y_pred)

    print(f"ç²¾åº¦: {accuracy:.3f}")
    return accuracy

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®šç¾©
@dsl.pipeline(
    name='ML Training Pipeline',
    description='åŸºæœ¬çš„ãªMLè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³'
)
def ml_pipeline(n_estimators: int = 100):
    """MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    # ã‚¿ã‚¹ã‚¯å®šç¾©
    load_task = load_data()

    train_task = train_model(
        input_dataset=load_task.outputs['output_dataset'],
        n_estimators=n_estimators
    )

    evaluate_task = evaluate_model(
        input_dataset=load_task.outputs['output_dataset'],
        input_model=train_task.outputs['output_model']
    )

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
if __name__ == '__main__':
    kfp.compiler.Compiler().compile(
        pipeline_func=ml_pipeline,
        package_path='ml_pipeline.yaml'
    )
    print("ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å®Œäº†: ml_pipeline.yaml")
</code></pre>

<h3>ã‚³ãƒ³ãƒ†ãƒŠåŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</h3>

<pre><code class="language-python">"""
Dockerã‚³ãƒ³ãƒ†ãƒŠãƒ™ãƒ¼ã‚¹ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå®šç¾©
"""

from kfp import dsl
from kfp.dsl import ContainerOp

# Dockerfile
"""
FROM python:3.9-slim

RUN pip install pandas scikit-learn

COPY train.py /app/train.py
WORKDIR /app

ENTRYPOINT ["python", "train.py"]
"""

# train.py
"""
import argparse
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
import joblib

def main(args):
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data = pd.read_csv(args.input_data)
    X = data.drop('target', axis=1)
    y = data['target']

    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    model = RandomForestClassifier(n_estimators=args.n_estimators)
    model.fit(X, y)

    # ä¿å­˜
    joblib.dump(model, args.output_model)
    print(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {args.output_model}")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--input-data', required=True)
    parser.add_argument('--output-model', required=True)
    parser.add_argument('--n-estimators', type=int, default=100)
    args = parser.parse_args()
    main(args)
"""

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã®ã‚³ãƒ³ãƒ†ãƒŠä½¿ç”¨
@dsl.pipeline(
    name='Containerized ML Pipeline',
    description='ã‚³ãƒ³ãƒ†ãƒŠåŒ–ã•ã‚ŒãŸMLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³'
)
def containerized_pipeline(n_estimators: int = 100):
    """ã‚³ãƒ³ãƒ†ãƒŠãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    # ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚³ãƒ³ãƒ†ãƒŠ
    prepare_op = dsl.ContainerOp(
        name='prepare-data',
        image='gcr.io/my-project/data-prep:v1',
        arguments=['--output', '/data/prepared.csv'],
        file_outputs={'data': '/data/prepared.csv'}
    )

    # è¨“ç·´ã‚³ãƒ³ãƒ†ãƒŠ
    train_op = dsl.ContainerOp(
        name='train-model',
        image='gcr.io/my-project/train:v1',
        arguments=[
            '--input-data', prepare_op.outputs['data'],
            '--output-model', '/models/model.pkl',
            '--n-estimators', n_estimators
        ],
        file_outputs={'model': '/models/model.pkl'}
    )

    # è©•ä¾¡ã‚³ãƒ³ãƒ†ãƒŠ
    evaluate_op = dsl.ContainerOp(
        name='evaluate-model',
        image='gcr.io/my-project/evaluate:v1',
        arguments=[
            '--input-data', prepare_op.outputs['data'],
            '--input-model', train_op.outputs['model']
        ]
    )

    # GPUä½¿ç”¨ã®æŒ‡å®š
    train_op.set_gpu_limit(1)
    train_op.add_node_selector_constraint('cloud.google.com/gke-accelerator', 'nvidia-tesla-t4')

# ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã¨å®Ÿè¡Œ
if __name__ == '__main__':
    kfp.compiler.Compiler().compile(
        pipeline_func=containerized_pipeline,
        package_path='containerized_pipeline.yaml'
    )
</code></pre>

<h3>Kubeflow ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œä¾‹</h3>

<pre><code class="language-python">import kfp
from kfp import dsl
from kfp.dsl import component, Input, Output, Dataset, Model, Metrics

@component(base_image='python:3.9', packages_to_install=['pandas', 'scikit-learn'])
def preprocess_data(
    input_dataset: Input[Dataset],
    output_train: Output[Dataset],
    output_test: Output[Dataset],
    test_size: float = 0.2
):
    """ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¨ãƒˆãƒ¬ã‚¤ãƒ³ãƒ»ãƒ†ã‚¹ãƒˆ minuteså‰²"""
    import pandas as pd
    from sklearn.model_selection import train_test_split

    data = pd.read_csv(input_dataset.path)

    X = data.drop('target', axis=1)
    y = data['target']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42
    )

    # ä¿å­˜
    train_df = X_train.copy()
    train_df['target'] = y_train
    test_df = X_test.copy()
    test_df['target'] = y_test

    train_df.to_csv(output_train.path, index=False)
    test_df.to_csv(output_test.path, index=False)

    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_df)}ä»¶")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_df)}ä»¶")

@component(base_image='python:3.9', packages_to_install=['pandas', 'scikit-learn'])
def hyperparameter_tuning(
    input_train: Input[Dataset],
    output_best_params: Output[Metrics]
) -> dict:
    """ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"""
    import pandas as pd
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import GridSearchCV
    import json

    data = pd.read_csv(input_train.path)
    X = data.drop('target', axis=1)
    y = data['target']

    # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [5, 10, 15]
    }

    model = RandomForestClassifier(random_state=42)
    grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1)
    grid_search.fit(X, y)

    best_params = grid_search.best_params_

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
    with open(output_best_params.path, 'w') as f:
        json.dump(best_params, f)

    print(f"æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {best_params}")
    print(f"æœ€é«˜ã‚¹ã‚³ã‚¢: {grid_search.best_score_:.3f}")

    return best_params

@component(base_image='python:3.9', packages_to_install=['pandas', 'scikit-learn'])
def train_final_model(
    input_train: Input[Dataset],
    best_params: dict,
    output_model: Output[Model]
):
    """æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
    import pandas as pd
    from sklearn.ensemble import RandomForestClassifier
    import joblib

    data = pd.read_csv(input_train.path)
    X = data.drop('target', axis=1)
    y = data['target']

    # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    model = RandomForestClassifier(**best_params, random_state=42)
    model.fit(X, y)

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    joblib.dump(model, output_model.path)
    print(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†: {best_params}")

@component(base_image='python:3.9', packages_to_install=['pandas', 'scikit-learn'])
def evaluate_final_model(
    input_test: Input[Dataset],
    input_model: Input[Model],
    output_metrics: Output[Metrics]
):
    """æœ€çµ‚è©•ä¾¡"""
    import pandas as pd
    import joblib
    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
    import json

    data = pd.read_csv(input_test.path)
    X = data.drop('target', axis=1)
    y = data['target']

    model = joblib.load(input_model.path)
    y_pred = model.predict(X)

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
    metrics = {
        'accuracy': float(accuracy_score(y, y_pred)),
        'f1_score': float(f1_score(y, y_pred)),
        'precision': float(precision_score(y, y_pred)),
        'recall': float(recall_score(y, y_pred))
    }

    # ä¿å­˜
    with open(output_metrics.path, 'w') as f:
        json.dump(metrics, f)

    print(f"è©•ä¾¡å®Œäº†: {metrics}")

@dsl.pipeline(
    name='Complete ML Pipeline with Tuning',
    description='ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä»˜ãå®Œå…¨MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³'
)
def complete_ml_pipeline(test_size: float = 0.2):
    """å®Œå…¨ãªMLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
    load_task = load_data()

    # å‰å‡¦ç†
    preprocess_task = preprocess_data(
        input_dataset=load_task.outputs['output_dataset'],
        test_size=test_size
    )

    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
    tuning_task = hyperparameter_tuning(
        input_train=preprocess_task.outputs['output_train']
    )

    # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    train_task = train_final_model(
        input_train=preprocess_task.outputs['output_train'],
        best_params=tuning_task.output
    )

    # è©•ä¾¡
    evaluate_task = evaluate_final_model(
        input_test=preprocess_task.outputs['output_test'],
        input_model=train_task.outputs['output_model']
    )

# ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
if __name__ == '__main__':
    kfp.compiler.Compiler().compile(
        pipeline_func=complete_ml_pipeline,
        package_path='complete_ml_pipeline.yaml'
    )
    print("ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å®Œäº†")
</code></pre>

<hr>

<h2>3.4 Prefect</h2>

<h3>Prefectã¨ã¯</h3>

<p><strong>Prefect</strong>ã¯ã€Pythonãƒã‚¤ãƒ†ã‚£ãƒ–ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚å‹•çš„ã‚¿ã‚¹ã‚¯ç”Ÿæˆã¨æŸ”è»Ÿãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒç‰¹å¾´ã§ã™ã€‚</p>

<h4>Prefectã®ç‰¹å¾´</h4>

<ul>
<li><strong>Pythonic</strong>: é€šå¸¸ã®Pythoné–¢æ•°ã‚’ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã§ã‚¿ã‚¹ã‚¯åŒ–</li>
<li><strong>å‹•çš„ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</strong>: å®Ÿè¡Œæ™‚ã«ã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆå¯èƒ½</li>
<li><strong>ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ</strong>: Development Environmentã§ç°¡å˜ã«ãƒ†ã‚¹ãƒˆ</li>
<li><strong>ã‚¯ãƒ©ã‚¦ãƒ‰UI</strong>: Prefect Cloudã§å¯è¦–åŒ–ã¨ç®¡ç†</li>
<li><strong>æŸ”è»Ÿãªã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°</strong>: Cronã€Intervalã€Eventé§†å‹•</li>
</ul>

<h3>åŸºæœ¬çš„ãªFlow</h3>

<pre><code class="language-python">from prefect import flow, task
from datetime import timedelta

@task(retries=3, retry_delay_seconds=60)
def extract_data():
    """ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¿ã‚¹ã‚¯"""
    print("ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºä¸­...")
    # æŠ½å‡ºå‡¦ç†
    data = {'records': 1000}
    return data

@task
def transform_data(data):
    """ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚¿ã‚¹ã‚¯"""
    print(f"ãƒ‡ãƒ¼ã‚¿å¤‰æ›ä¸­: {data['records']}ä»¶")
    # å¤‰æ›å‡¦ç†
    transformed = {'records': data['records'], 'features': 50}
    return transformed

@task(timeout_seconds=3600)
def load_data(data):
    """ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ ã‚¿ã‚¹ã‚¯"""
    print(f"ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ä¸­: {data['records']}ä»¶")
    # ãƒ­ãƒ¼ãƒ‰å‡¦ç†
    return True

@flow(name="ETL Pipeline", log_prints=True)
def etl_pipeline():
    """ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    # ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
    raw_data = extract_data()
    transformed_data = transform_data(raw_data)
    load_data(transformed_data)

    print("ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†")

if __name__ == "__main__":
    etl_pipeline()
</code></pre>

<h3>å‹•çš„ã‚¿ã‚¹ã‚¯ç”Ÿæˆ</h3>

<pre><code class="language-python">from prefect import flow, task
from typing import List

@task
def train_model(model_type: str, data_path: str):
    """åˆ¥ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
    print(f"{model_type}ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­...")
    # è¨“ç·´å‡¦ç†
    metrics = {'model': model_type, 'accuracy': 0.85}
    return metrics

@task
def select_best_model(results: List[dict]):
    """æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ"""
    best = max(results, key=lambda x: x['accuracy'])
    print(f"æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best['model']} (accuracy={best['accuracy']:.3f})")
    return best

@flow(name="Multi-Model Training")
def multi_model_training(data_path: str):
    """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’ä¸¦åˆ—è¨“ç·´"""
    # è¨“ç·´ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ
    model_types = ['random_forest', 'xgboost', 'lightgbm', 'catboost']

    # å‹•çš„ã«ã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆ
    results = []
    for model_type in model_types:
        result = train_model(model_type, data_path)
        results.append(result)

    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    best_model = select_best_model(results)

    return best_model

if __name__ == "__main__":
    best = multi_model_training(data_path="/data/train.csv")
    print(f"é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«: {best}")
</code></pre>

<h3>Prefect 2.0 å®Œå…¨ãªä¾‹</h3>

<pre><code class="language-python">from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.deployments import Deployment
from prefect.server.schemas.schedules import CronSchedule
from datetime import timedelta
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib

@task(
    name="ãƒ‡ãƒ¼ã‚¿åé›†",
    retries=3,
    retry_delay_seconds=60,
    cache_key_fn=None,  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–
    timeout_seconds=300
)
def collect_data():
    """ãƒ‡ãƒ¼ã‚¿åé›†ã‚¿ã‚¹ã‚¯"""
    logger = get_run_logger()
    logger.info("ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")

    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    import numpy as np
    np.random.seed(42)

    data = pd.DataFrame({
        'feature1': np.random.randn(1000),
        'feature2': np.random.randn(1000),
        'feature3': np.random.randn(1000),
        'target': np.random.randint(0, 2, 1000)
    })

    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    output_path = '/tmp/raw_data.csv'
    data.to_csv(output_path, index=False)

    logger.info(f"ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†: {len(data)}ä»¶")
    return output_path

@task(name="ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼")
def validate_data(data_path: str):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼"""
    logger = get_run_logger()
    logger.info("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼é–‹å§‹")

    data = pd.read_csv(data_path)

    # æ¤œè¨¼
    checks = {
        'no_nulls': data.isnull().sum().sum() == 0,
        'sufficient_size': len(data) >= 500,
        'feature_count': data.shape[1] >= 4
    }

    logger.info(f"æ¤œè¨¼çµæœ: {checks}")

    if not all(checks.values()):
        raise ValueError(f"ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å¤±æ•—: {checks}")

    return True

@task(name="å‰å‡¦ç†")
def preprocess_data(data_path: str):
    """ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"""
    logger = get_run_logger()
    logger.info("å‰å‡¦ç†é–‹å§‹")

    data = pd.read_csv(data_path)

    #  minuteså‰²
    X = data.drop('target', axis=1)
    y = data['target']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # ä¿å­˜
    paths = {
        'X_train': '/tmp/X_train.csv',
        'X_test': '/tmp/X_test.csv',
        'y_train': '/tmp/y_train.csv',
        'y_test': '/tmp/y_test.csv'
    }

    X_train.to_csv(paths['X_train'], index=False)
    X_test.to_csv(paths['X_test'], index=False)
    y_train.to_csv(paths['y_train'], index=False)
    y_test.to_csv(paths['y_test'], index=False)

    logger.info(f"å‰å‡¦ç†å®Œäº†: è¨“ç·´={len(X_train)}, ãƒ†ã‚¹ãƒˆ={len(X_test)}")
    return paths

@task(name="ãƒ¢ãƒ‡ãƒ«è¨“ç·´", timeout_seconds=1800)
def train_model(data_paths: dict, n_estimators: int = 100):
    """ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
    logger = get_run_logger()
    logger.info(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹: n_estimators={n_estimators}")

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    X_train = pd.read_csv(data_paths['X_train'])
    y_train = pd.read_csv(data_paths['y_train']).values.ravel()

    # è¨“ç·´
    model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
    model.fit(X_train, y_train)

    # ä¿å­˜
    model_path = '/tmp/model.pkl'
    joblib.dump(model, model_path)

    logger.info("ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")
    return model_path

@task(name="ãƒ¢ãƒ‡ãƒ«è©•ä¾¡")
def evaluate_model(model_path: str, data_paths: dict):
    """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
    logger = get_run_logger()
    logger.info("ãƒ¢ãƒ‡ãƒ«è©•ä¾¡é–‹å§‹")

    # èª­ã¿è¾¼ã¿
    X_test = pd.read_csv(data_paths['X_test'])
    y_test = pd.read_csv(data_paths['y_test']).values.ravel()
    model = joblib.load(model_path)

    # è©•ä¾¡
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    metrics = {
        'accuracy': float(accuracy),
        'test_samples': len(y_test)
    }

    logger.info(f"è©•ä¾¡å®Œäº†: {metrics}")
    return metrics

@task(name="ãƒ¢ãƒ‡ãƒ«ç™»éŒ²")
def register_model(model_path: str, metrics: dict):
    """ãƒ¢ãƒ‡ãƒ«ç™»éŒ²"""
    logger = get_run_logger()

    # å“è³ªãƒã‚§ãƒƒã‚¯
    if metrics['accuracy'] < 0.7:
        logger.warning(f"ãƒ¢ãƒ‡ãƒ«å“è³ªä¸è¶³: accuracy={metrics['accuracy']:.3f}")
        return False

    # ç™»éŒ²ï¼ˆå®Ÿéš›ã¯MLflowãªã©ï¼‰
    import shutil
    from datetime import datetime

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    registry_path = f'/tmp/models/model_{timestamp}.pkl'

    import os
    os.makedirs('/tmp/models', exist_ok=True)
    shutil.copy(model_path, registry_path)

    logger.info(f"ãƒ¢ãƒ‡ãƒ«ç™»éŒ²å®Œäº†: {registry_path}")
    return registry_path

@flow(
    name="ML Training Pipeline",
    description="å®Œå…¨ãªMLè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³",
    task_runner=ConcurrentTaskRunner(),  # ä¸¦åˆ—å®Ÿè¡Œ
    log_prints=True
)
def ml_training_pipeline(n_estimators: int = 100):
    """ãƒ¡ã‚¤ãƒ³ã®MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    logger = get_run_logger()
    logger.info("ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")

    # ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
    data_path = collect_data()
    validate_data(data_path)
    data_paths = preprocess_data(data_path)
    model_path = train_model(data_paths, n_estimators)
    metrics = evaluate_model(model_path, data_paths)
    registry_path = register_model(model_path, metrics)

    logger.info(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†: {registry_path}")
    return {
        'model_path': registry_path,
        'metrics': metrics
    }

# ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå®šç¾©
if __name__ == "__main__":
    # ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ
    result = ml_training_pipeline(n_estimators=100)
    print(f"çµæœ: {result}")

    # ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆä½œæˆï¼ˆPrefect Cloudã¸ï¼‰
    """
    deployment = Deployment.build_from_flow(
        flow=ml_training_pipeline,
        name="daily-ml-training",
        schedule=CronSchedule(cron="0 2 * * *"),  # æ¯æ—¥2:00 AM
        work_queue_name="ml-training",
        parameters={"n_estimators": 100}
    )
    deployment.apply()
    """
</code></pre>

<h3>Prefect Cloudçµ±åˆ</h3>

<pre><code class="language-python">"""
Prefect Cloudçµ±åˆã¨ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
"""

from prefect import flow, task
from prefect.deployments import Deployment
from prefect.server.schemas.schedules import IntervalSchedule
from datetime import timedelta

@flow
def production_ml_pipeline():
    """æœ¬ç•ªMLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†...
    pass

# ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆè¨­å®š
deployment = Deployment.build_from_flow(
    flow=production_ml_pipeline,
    name="production-deployment",
    schedule=IntervalSchedule(interval=timedelta(hours=6)),  # 6 hoursã”ã¨
    work_queue_name="production",
    tags=["ml", "production"],
    parameters={},
    description="æœ¬ç•ªç’°å¢ƒã®MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"
)

# ãƒ‡ãƒ—ãƒ­ã‚¤
# deployment.apply()

# CLIã§ã®ãƒ‡ãƒ—ãƒ­ã‚¤
"""
# Prefect Cloudã«ãƒ­ã‚°ã‚¤ãƒ³
prefect cloud login

# ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆä½œæˆ
prefect deployment build ml_pipeline.py:production_ml_pipeline -n production -q production

# ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆé©ç”¨
prefect deployment apply production_ml_pipeline-deployment.yaml

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆèµ·å‹•
prefect agent start -q production
"""
</code></pre>

<hr>

<h2>3.5 ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­è¨ˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h2>

<h3>å†ªç­‰æ€§ã®ç¢ºä¿</h3>

<p><strong>å†ªç­‰æ€§ï¼ˆIdempotencyï¼‰</strong>ã¨ã¯ã€åŒã˜å…¥åŠ›ã§ä½•åº¦å®Ÿè¡Œã—ã¦ã‚‚åŒã˜çµæœãŒå¾—ã‚‰ã‚Œã‚‹æ€§è³ªã§ã™ã€‚</p>

<pre><code class="language-python">"""
å†ªç­‰æ€§ã‚’ç¢ºä¿ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
"""

import os
import shutil
from pathlib import Path

# âŒ éå†ªç­‰çš„ãªå‡¦ç†
def non_idempotent_process(output_path):
    """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«è¿½è¨˜ï¼ˆå®Ÿè¡Œã®ãŸã³ã«çµæœãŒå¤‰ã‚ã‚‹ï¼‰"""
    with open(output_path, 'a') as f:  # append mode
        f.write("new data\n")

# âœ… å†ªç­‰çš„ãªå‡¦ç†
def idempotent_process(output_path):
    """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä¸Šæ›¸ãï¼ˆå¸¸ã«åŒã˜çµæœï¼‰"""
    if os.path.exists(output_path):
        os.remove(output_path)  # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤

    with open(output_path, 'w') as f:  # write mode
        f.write("new data\n")

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å†ªç­‰çš„ãªä½œæˆ
def create_output_dir(dir_path):
    """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å†ªç­‰çš„ã«ä½œæˆ"""
    if os.path.exists(dir_path):
        shutil.rmtree(dir_path)  # æ—¢å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤
    os.makedirs(dir_path)

# ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å«ã‚€å†ªç­‰çš„ãªå‡¦ç†
def process_with_version(input_path, output_dir, version):
    """ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã§å†ªç­‰æ€§ã‚’ç¢ºä¿"""
    output_path = os.path.join(output_dir, f'output_v{version}.csv')

    # åŒã˜ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯å¸¸ã«åŒã˜çµæœ
    if os.path.exists(output_path):
        os.remove(output_path)

    # å‡¦ç†å®Ÿè¡Œ
    process_data(input_path, output_path)

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å†ªç­‰çš„ãªæ›´æ–°
def upsert_data(data, table_name):
    """UPSERTï¼ˆå­˜åœ¨ã™ã‚Œã°æ›´æ–°ã€ãªã‘ã‚Œã°æŒ¿å…¥ï¼‰"""
    # SQLä¾‹
    query = f"""
    INSERT INTO {table_name} (id, value)
    VALUES (%(id)s, %(value)s)
    ON CONFLICT (id) DO UPDATE
    SET value = EXCLUDED.value
    """
    # å®Ÿè¡Œ...
</code></pre>

<h3>ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°</h3>

<pre><code class="language-python">"""
å …ç‰¢ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
"""

from typing import Optional
import logging
import time

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ãƒªãƒˆãƒ©ã‚¤ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
def retry_on_failure(max_retries=3, delay=5, backoff=2):
    """å¤±æ•—æ™‚ã«ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            retries = 0
            current_delay = delay

            while retries < max_retries:
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    retries += 1
                    if retries >= max_retries:
                        logger.error(f"{func.__name__} å¤±æ•—ï¼ˆæœ€å¤§ãƒªãƒˆãƒ©ã‚¤åˆ°é”ï¼‰: {e}")
                        raise

                    logger.warning(
                        f"{func.__name__} å¤±æ•—ï¼ˆ{retries}/{max_retries}ï¼‰: {e}. "
                        f"{current_delay}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤..."
                    )
                    time.sleep(current_delay)
                    current_delay *= backoff  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•

        return wrapper
    return decorator

# ä½¿ç”¨ä¾‹
@retry_on_failure(max_retries=3, delay=5, backoff=2)
def fetch_data_from_api(url):
    """APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆãƒªãƒˆãƒ©ã‚¤ã‚ã‚Šï¼‰"""
    import requests
    response = requests.get(url, timeout=30)
    response.raise_for_status()
    return response.json()

# ã‚¿ã‚¹ã‚¯Levelã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
def safe_task_execution(task_func, *args, **kwargs):
    """ã‚¿ã‚¹ã‚¯ã‚’å®‰å…¨ã«å®Ÿè¡Œ"""
    try:
        logger.info(f"ã‚¿ã‚¹ã‚¯é–‹å§‹: {task_func.__name__}")
        result = task_func(*args, **kwargs)
        logger.info(f"ã‚¿ã‚¹ã‚¯æˆåŠŸ: {task_func.__name__}")
        return result, None

    except Exception as e:
        logger.error(f"ã‚¿ã‚¹ã‚¯å¤±æ•—: {task_func.__name__} - {e}", exc_info=True)
        return None, str(e)

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³Levelã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
def run_pipeline_with_recovery(tasks):
    """ãƒªã‚«ãƒãƒªãƒ¼æ©Ÿèƒ½ä»˜ããƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
    results = {}
    failed_tasks = []

    for task_name, task_func in tasks.items():
        result, error = safe_task_execution(task_func)

        if error:
            failed_tasks.append({
                'task': task_name,
                'error': error
            })
            # é‡è¦ã‚¿ã‚¹ã‚¯ã¯å¤±æ•—æ™‚ã«ä¸­æ–­
            if is_critical_task(task_name):
                logger.error(f"é‡è¦ã‚¿ã‚¹ã‚¯å¤±æ•—: {task_name}. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸­æ–­")
                break
        else:
            results[task_name] = result

    # å¤±æ•—ã‚µãƒãƒªãƒ¼
    if failed_tasks:
        logger.warning(f"å¤±æ•—ã‚¿ã‚¹ã‚¯æ•°: {len(failed_tasks)}")
        for failure in failed_tasks:
            logger.warning(f"  - {failure['task']}: {failure['error']}")

    return results, failed_tasks

def is_critical_task(task_name):
    """é‡è¦ã‚¿ã‚¹ã‚¯ã®åˆ¤å®š"""
    critical_tasks = ['data_validation', 'model_training']
    return task_name in critical_tasks
</code></pre>

<h3>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–</h3>

<pre><code class="language-python">"""
è¨­å®šã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–
"""

import yaml
import json
from dataclasses import dataclass
from typing import Dict, Any

# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã§ã®è¨­å®šç®¡ç†
@dataclass
class PipelineConfig:
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­å®š"""
    data_source: str
    output_dir: str
    model_type: str
    n_estimators: int = 100
    test_size: float = 0.2
    random_state: int = 42

# YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
"""
# config.yaml
pipeline:
  data_source: "s3://bucket/data.csv"
  output_dir: "/tmp/output"
  model_type: "random_forest"
  n_estimators: 100
  test_size: 0.2
  random_state: 42

hyperparameters:
  max_depth: 10
  min_samples_split: 5
"""

def load_config(config_path: str) -> Dict[str, Any]:
    """YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config

def run_pipeline_with_config(config_path: str):
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
    config = load_config(config_path)

    # è¨­å®šå–å¾—
    pipeline_config = PipelineConfig(**config['pipeline'])
    hyperparams = config['hyperparameters']

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: {pipeline_config.data_source}")
    print(f"ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {pipeline_config.model_type}")
    print(f"ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {hyperparams}")

    # å‡¦ç†...

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã®è¨­å®šèª­ã¿è¾¼ã¿
import os

def get_config_from_env():
    """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—"""
    config = {
        'data_source': os.getenv('DATA_SOURCE', 'default.csv'),
        'model_type': os.getenv('MODEL_TYPE', 'random_forest'),
        'n_estimators': int(os.getenv('N_ESTIMATORS', '100')),
        'output_dir': os.getenv('OUTPUT_DIR', '/tmp/output')
    }
    return config

# ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°
import argparse

def parse_args():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®ãƒ‘ãƒ¼ã‚¹"""
    parser = argparse.ArgumentParser(description='ML Pipeline')

    parser.add_argument('--config', type=str, required=True,
                       help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    parser.add_argument('--data-source', type=str,
                       help='ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ãï¼‰')
    parser.add_argument('--n-estimators', type=int, default=100,
                       help='æ±ºå®šæœ¨ã®æ•°')

    return parser.parse_args()

if __name__ == '__main__':
    args = parse_args()

    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    config = load_config(args.config)

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§ä¸Šæ›¸ã
    if args.data_source:
        config['pipeline']['data_source'] = args.data_source
    if args.n_estimators:
        config['pipeline']['n_estimators'] = args.n_estimators

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    run_pipeline_with_config(args.config)
</code></pre>

<h3>ãƒ†ã‚¹ã‚¿ãƒ“ãƒªãƒ†ã‚£</h3>

<pre><code class="language-python">"""
ãƒ†ã‚¹ãƒˆå¯èƒ½ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­è¨ˆ
"""

import unittest
from unittest.mock import Mock, patch
import pandas as pd

# ãƒ†ã‚¹ãƒˆå¯èƒ½ãªé–¢æ•°è¨­è¨ˆ
def load_data(data_source):
    """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆãƒ†ã‚¹ãƒˆå®¹æ˜“ï¼‰"""
    # å®Ÿè£…...
    pass

def preprocess(data):
    """å‰å‡¦ç†ï¼ˆpure functionï¼‰"""
    # å‰¯ä½œç”¨ãªã—ã€å…¥åŠ›ã‹ã‚‰å‡ºåŠ›ã‚’ç”Ÿæˆ
    processed = data.copy()
    # å‡¦ç†...
    return processed

def train_model(X, y, model_class, **hyperparams):
    """ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆä¾å­˜æ€§æ³¨å…¥ï¼‰"""
    model = model_class(**hyperparams)
    model.fit(X, y)
    return model

# ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
class TestPreprocessing(unittest.TestCase):
    """å‰å‡¦ç†ã®ãƒ†ã‚¹ãƒˆ"""

    def test_preprocess_removes_nulls(self):
        """æ¬ æå€¤ãŒå‰Šé™¤ã•ã‚Œã‚‹ã‹ãƒ†ã‚¹ãƒˆ"""
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        data = pd.DataFrame({
            'feature1': [1, 2, None, 4],
            'feature2': [5, None, 7, 8]
        })

        # å®Ÿè¡Œ
        result = preprocess(data)

        # æ¤œè¨¼
        self.assertEqual(result.isnull().sum().sum(), 0)

    def test_preprocess_scales_features(self):
        """ç‰¹å¾´é‡ãŒã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ã‹ãƒ†ã‚¹ãƒˆ"""
        data = pd.DataFrame({
            'feature1': [1, 2, 3, 4],
            'feature2': [10, 20, 30, 40]
        })

        result = preprocess(data)

        # å¹³å‡ãŒ0ã«è¿‘ã„
        self.assertAlmostEqual(result['feature1'].mean(), 0, places=1)

# ãƒ¢ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ãŸçµ±åˆãƒ†ã‚¹ãƒˆ
class TestMLPipeline(unittest.TestCase):
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®ãƒ†ã‚¹ãƒˆ"""

    @patch('my_pipeline.load_data')
    @patch('my_pipeline.save_model')
    def test_pipeline_end_to_end(self, mock_save, mock_load):
        """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ"""
        # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿
        mock_data = pd.DataFrame({
            'feature1': [1, 2, 3],
            'feature2': [4, 5, 6],
            'target': [0, 1, 0]
        })
        mock_load.return_value = mock_data

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        # run_pipeline(config)

        # æ¤œè¨¼
        mock_save.assert_called_once()

# ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
def validate_pipeline_output(output_path):
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡ºåŠ›ã®æ¤œè¨¼"""
    import os

    checks = {
        'file_exists': os.path.exists(output_path),
        'file_size': os.path.getsize(output_path) > 0 if os.path.exists(output_path) else False
    }

    assert all(checks.values()), f"å‡ºåŠ›æ¤œè¨¼å¤±æ•—: {checks}"
    return True

if __name__ == '__main__':
    unittest.main()
</code></pre>

<hr>

<h2>3.6 æœ¬ Chapterã®Summary</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­è¨ˆã®åŸå‰‡</strong></p>
<ul>
<li>DAGæ§‹é€ ã§ã‚¿ã‚¹ã‚¯ä¾å­˜ã‚’è¡¨ç¾</li>
<li>å†ªç­‰æ€§ã€å†å®Ÿè¡Œå¯èƒ½æ€§ã€ç–çµåˆ</li>
<li>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã¨å¯è¦³æ¸¬æ€§</li>
</ul></li>

<li><p><strong>Apache Airflow</strong></p>
<ul>
<li>Pythonãƒ™ãƒ¼ã‚¹ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</li>
<li>ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã¨Executorã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</li>
<li>è±Šå¯Œãªã‚ªãƒšãƒ¬ãƒ¼ã‚¿ã¨UI</li>
</ul></li>

<li><p><strong>Kubeflow Pipelines</strong></p>
<ul>
<li>Kubernetesãƒã‚¤ãƒ†ã‚£ãƒ–ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</li>
<li>ã‚³ãƒ³ãƒ†ãƒŠåŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</li>
<li>å†åˆ©ç”¨å¯èƒ½ãªéƒ¨å“è¨­è¨ˆ</li>
</ul></li>

<li><p><strong>Prefect</strong></p>
<ul>
<li>Pythonicãªå‹•çš„ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</li>
<li>æŸ”è»Ÿãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°</li>
<li>ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºã¨Cloudçµ±åˆ</li>
</ul></li>

<li><p><strong>ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</strong></p>
<ul>
<li>å†ªç­‰æ€§ã®ç¢ºä¿ã¨å®‰å…¨ãªå†å®Ÿè¡Œ</li>
<li>å …ç‰¢ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒªãƒˆãƒ©ã‚¤</li>
<li>è¨­å®šã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–</li>
<li>ãƒ†ã‚¹ãƒˆå¯èƒ½ãªè¨­è¨ˆ</li>
</ul></li>
</ol>

<h3>ãƒ„ãƒ¼ãƒ«æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>ãƒ„ãƒ¼ãƒ«</th>
<th>å¼·ã¿</th>
<th>é©ç”¨å ´é¢</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Airflow</strong></td>
<td>æˆç†Ÿã—ãŸã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã€è±Šå¯Œãªã‚ªãƒšãƒ¬ãƒ¼ã‚¿</td>
<td>è¤‡é›‘ãªãƒãƒƒãƒå‡¦ç†ã€ãƒ‡ãƒ¼ã‚¿ETL</td>
</tr>
<tr>
<td><strong>Kubeflow</strong></td>
<td>Kubernetesãƒã‚¤ãƒ†ã‚£ãƒ–ã€MLã«ç‰¹åŒ–</td>
<td>å¤§è¦æ¨¡MLã€GPUæ´»ç”¨ã€ãƒãƒ«ãƒã‚¯ãƒ©ã‚¦ãƒ‰</td>
</tr>
<tr>
<td><strong>Prefect</strong></td>
<td>Pythonicã€å‹•çš„ã‚¿ã‚¹ã‚¯ã€ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™º</td>
<td>æŸ”è»Ÿãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã€ã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•</td>
</tr>
</tbody>
</table>

<h3>Next Chapterã¸</h3>

<p>Chapter 4 Chapterã§ã¯ã€<strong>ãƒ¢ãƒ‡ãƒ«ç®¡ç†ã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°</strong>ã‚’å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>MLflowã§ã®ãƒ¢ãƒ‡ãƒ«è¿½è·¡</li>
<li>ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒª</li>
<li>å®Ÿé¨“ç®¡ç†ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²</li>
<li>ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†</li>
<li>æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ</li>
</ul>

<hr>

<h2>Exercises</h2>

<h3>å•é¡Œ1ï¼ˆDifficultyï¼šeasyï¼‰</h3>
<p>DAGï¼ˆDirected Acyclic Graphï¼‰ã¨ã¯ä½•ã‹èª¬æ˜ã—ã€MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§DAGãŒé‡è¦ãªç†ç”±ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>DAGï¼ˆæœ‰å‘éå·¡ç’°ã‚°ãƒ©ãƒ•ï¼‰</strong>ã¯ã€ãƒãƒ¼ãƒ‰ï¼ˆã‚¿ã‚¹ã‚¯ï¼‰ã¨ã‚¨ãƒƒã‚¸ï¼ˆä¾å­˜é–¢ä¿‚ï¼‰ã‹ã‚‰æ§‹æˆã•ã‚Œã€ä»¥ä¸‹ã®ç‰¹æ€§ã‚’æŒã¡ã¾ã™ï¼š</p>

<ol>
<li><strong>æœ‰å‘ï¼ˆDirectedï¼‰</strong>: ã‚¨ãƒƒã‚¸ã«æ–¹å‘ãŒã‚ã‚‹ï¼ˆã‚¿ã‚¹ã‚¯Aã‹ã‚‰ã‚¿ã‚¹ã‚¯Bã¸ï¼‰</li>
<li><strong>éå·¡ç’°ï¼ˆAcyclicï¼‰</strong>: ãƒ«ãƒ¼ãƒ—ãŒãªã„ï¼ˆåŒã˜ã‚¿ã‚¹ã‚¯ã«æˆ»ã‚‰ãªã„ï¼‰</li>
</ol>

<p><strong>MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§DAGãŒé‡è¦ãªç†ç”±</strong>ï¼š</p>

<ul>
<li><strong>ä¾å­˜é–¢ä¿‚ã®æ˜ç¢ºåŒ–</strong>: ã‚¿ã‚¹ã‚¯é–“ã®å®Ÿè¡Œé †åºãŒè¦–è¦šçš„ã«ç†è§£ã§ãã‚‹</li>
<li><strong>ä¸¦åˆ—å®Ÿè¡Œã®æœ€é©åŒ–</strong>: ä¾å­˜ã®ãªã„ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å®Ÿè¡Œå¯èƒ½</li>
<li><strong>å†ç¾æ€§</strong>: åŒã˜DAGã‹ã‚‰å¸¸ã«åŒã˜å®Ÿè¡Œé †åºãŒä¿è¨¼ã•ã‚Œã‚‹</li>
<li><strong>ãƒ‡ãƒãƒƒã‚°ã®å®¹æ˜“ã•</strong>: å¤±æ•—ç®‡æ‰€ã®ç‰¹å®šã¨å†å®Ÿè¡ŒãŒç°¡å˜</li>
<li><strong>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£</strong>: è¤‡é›‘ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ç®¡ç†å¯èƒ½</li>
</ul>

<p>ä¾‹ï¼š</p>
<pre><code>ãƒ‡ãƒ¼ã‚¿åé›† â†’ å‰å‡¦ç† â†’ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° â†’ ãƒ¢ãƒ‡ãƒ«è¨“ç·´ â†’ è©•ä¾¡
</code></pre>

<p>ã“ã®æ§‹é€ ã«ã‚ˆã‚Šã€å„ã‚¹ãƒ†ãƒƒãƒ—ãŒç‹¬ç«‹ã—ã¦å®Ÿè¡Œå¯èƒ½ã§ã€å¤±æ•—æ™‚ã¯è©²å½“ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿å†å®Ÿè¡Œã§ãã¾ã™ã€‚</p>

</details>

<h3>å•é¡Œ2ï¼ˆDifficultyï¼šmediumï¼‰</h3>
<p>ä»¥ä¸‹ã®Airflow DAGã®ã‚³ãƒ¼ãƒ‰ã«èª¤ã‚ŠãŒã‚ã‚Šã¾ã™ã€‚å•é¡Œç‚¹ã‚’æŒ‡æ‘˜ã—ã€ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def task_a():
    print("Task A")

def task_b():
    print("Task B")

dag = DAG('example', start_date=datetime(2025, 1, 1))

task1 = PythonOperator(task_id='task_a', python_callable=task_a)
task2 = PythonOperator(task_id='task_b', python_callable=task_b)

task1 >> task2
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>å•é¡Œç‚¹</strong>ï¼š</p>

<ol>
<li><code>PythonOperator</code>ã«<code>dag</code>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„</li>
<li><code>schedule_interval</code>ãŒå®šç¾©ã•ã‚Œã¦ã„ãªã„</li>
<li><code>default_args</code>ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ï¼ˆRecommendedï¼‰</li>
</ol>

<p><strong>ä¿®æ­£ç‰ˆ</strong>ï¼š</p>

<pre><code class="language-python">from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¼•æ•°ã‚’å®šç¾©
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def task_a():
    print("Task A")

def task_b():
    print("Task B")

# DAGå®šç¾©ï¼ˆschedule_intervalã‚’è¿½åŠ ï¼‰
dag = DAG(
    'example',
    default_args=default_args,
    start_date=datetime(2025, 1, 1),
    schedule_interval='@daily',  # æ¯æ—¥å®Ÿè¡Œ
    catchup=False
)

# dagå¼•æ•°ã‚’è¿½åŠ 
task1 = PythonOperator(
    task_id='task_a',
    python_callable=task_a,
    dag=dag  # âœ… dagå¼•æ•°ã‚’è¿½åŠ 
)

task2 = PythonOperator(
    task_id='task_b',
    python_callable=task_b,
    dag=dag  # âœ… dagå¼•æ•°ã‚’è¿½åŠ 
)

# ä¾å­˜é–¢ä¿‚
task1 >> task2
</code></pre>

<p><strong>æ”¹å–„ç‚¹</strong>ï¼š</p>
<ul>
<li><code>default_args</code>ã§ãƒªãƒˆãƒ©ã‚¤è¨­å®šã‚’è¿½åŠ </li>
<li><code>schedule_interval</code>ã§å®Ÿè¡Œé »åº¦ã‚’æ˜ç¤º</li>
<li><code>catchup=False</code>ã§éå»ã®æœªå®Ÿè¡Œ minutesã‚’ã‚¹ã‚­ãƒƒãƒ—</li>
</ul>

</details>

<h3>å•é¡Œ3ï¼ˆDifficultyï¼šmediumï¼‰</h3>
<p>å†ªç­‰æ€§ï¼ˆIdempotencyï¼‰ã¨ã¯ä½•ã‹èª¬æ˜ã—ã€ä»¥ä¸‹ã®é–¢æ•°ã‚’å†ªç­‰çš„ã«ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">def process_data(input_file, output_file):
    data = pd.read_csv(input_file)
    # å‡¦ç†...
    processed_data = data * 2

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜
    with open(output_file, 'a') as f:
        processed_data.to_csv(f, index=False)
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>å†ªç­‰æ€§ã¨ã¯</strong>ï¼š</p>

<p>åŒã˜å…¥åŠ›ã§ä½•åº¦å®Ÿè¡Œã—ã¦ã‚‚ã€å¸¸ã«åŒã˜çµæœãŒå¾—ã‚‰ã‚Œã‚‹æ€§è³ªã§ã™ã€‚MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ãŠã„ã¦é‡è¦ãªç†ç”±ï¼š</p>

<ul>
<li><strong>å†å®Ÿè¡Œã®å®‰å…¨æ€§</strong>: å¤±æ•—æ™‚ã«å®‰å¿ƒã—ã¦å†å®Ÿè¡Œã§ãã‚‹</li>
<li><strong>äºˆæ¸¬å¯èƒ½æ€§</strong>: çµæœãŒå¸¸ã«ä¸€è²«ã—ã¦ã„ã‚‹</li>
<li><strong>ãƒ‡ãƒãƒƒã‚°ã®å®¹æ˜“ã•</strong>: å•é¡Œã®å†ç¾ãŒç¢ºå®Ÿ</li>
</ul>

<p><strong>å…ƒã®ã‚³ãƒ¼ãƒ‰ã®å•é¡Œ</strong>ï¼š</p>

<p>è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ï¼ˆ<code>'a'</code>ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŸã‚ã€å®Ÿè¡Œã™ã‚‹ãŸã³ã«ãƒ‡ãƒ¼ã‚¿ãŒè¿½åŠ ã•ã‚Œã€ç•°ãªã‚‹çµæœã«ãªã‚Šã¾ã™ã€‚</p>

<p><strong>ä¿®æ­£ç‰ˆï¼ˆå†ªç­‰çš„ï¼‰</strong>ï¼š</p>

<pre><code class="language-python">import os
import pandas as pd

def process_data(input_file, output_file):
    """å†ªç­‰çš„ãªãƒ‡ãƒ¼ã‚¿å‡¦ç†"""
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data = pd.read_csv(input_file)

    # å‡¦ç†
    processed_data = data * 2

    # âœ… æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰æ›¸ãè¾¼ã¿
    if os.path.exists(output_file):
        os.remove(output_file)

    # æ–°è¦æ›¸ãè¾¼ã¿ï¼ˆä¸Šæ›¸ããƒ¢ãƒ¼ãƒ‰ï¼‰
    processed_data.to_csv(output_file, index=False)

    print(f"å‡¦ç†å®Œäº†: {output_file}")

# åˆ¥ã®æ–¹æ³•: ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚¢ãƒˆãƒŸãƒƒã‚¯ãªç§»å‹•
import shutil
import tempfile

def process_data_atomic(input_file, output_file):
    """ã‚¢ãƒˆãƒŸãƒƒã‚¯ãªæ›¸ãè¾¼ã¿ã§å†ªç­‰æ€§ã‚’ç¢ºä¿"""
    data = pd.read_csv(input_file)
    processed_data = data * 2

    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv') as tmp_file:
        tmp_path = tmp_file.name
        processed_data.to_csv(tmp_path, index=False)

    # ã‚¢ãƒˆãƒŸãƒƒã‚¯ã«ç§»å‹•ï¼ˆæ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ãï¼‰
    shutil.move(tmp_path, output_file)
    print(f"å‡¦ç†å®Œäº†: {output_file}")
</code></pre>

<p><strong>æ¤œè¨¼</strong>ï¼š</p>

<pre><code class="language-python"># åŒã˜å…¥åŠ›ã§2å›å®Ÿè¡Œ
process_data('input.csv', 'output.csv')  # 1å›ç›®
process_data('input.csv', 'output.csv')  # 2å›ç›®

# output.csvã¯å¸¸ã«åŒã˜å†…å®¹ï¼ˆå†ªç­‰æ€§ã‚ã‚Šï¼‰
</code></pre>

</details>

<h3>å•é¡Œ4ï¼ˆDifficultyï¼šhardï¼‰</h3>
<p>Prefectã‚’ä½¿ç”¨ã—ã¦ã€è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸¦åˆ—è¨“ç·´ã—ã€æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚ãƒ¢ãƒ‡ãƒ«ã¯['random_forest', 'xgboost', 'lightgbm']ã®3ç¨®é¡ã¨ã—ã¾ã™ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from typing import List, Dict
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import joblib

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
@task
def generate_data():
    """ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
    import numpy as np
    np.random.seed(42)

    data = pd.DataFrame({
        'feature1': np.random.randn(1000),
        'feature2': np.random.randn(1000),
        'feature3': np.random.randn(1000),
        'target': np.random.randint(0, 2, 1000)
    })

    return data

@task
def split_data(data: pd.DataFrame, test_size: float = 0.2):
    """ãƒ‡ãƒ¼ã‚¿ minuteså‰²"""
    logger = get_run_logger()

    X = data.drop('target', axis=1)
    y = data['target']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42
    )

    logger.info(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train)}ä»¶, ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)}ä»¶")

    return {
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train,
        'y_test': y_test
    }

@task
def train_model(model_type: str, data: Dict):
    """åˆ¥ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
    logger = get_run_logger()
    logger.info(f"{model_type}ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹")

    X_train = data['X_train']
    y_train = data['y_train']

    # ãƒ¢ãƒ‡ãƒ«é¸æŠ
    if model_type == 'random_forest':
        from sklearn.ensemble import RandomForestClassifier
        model = RandomForestClassifier(n_estimators=100, random_state=42)
    elif model_type == 'xgboost':
        # xgboostãŒãªã„å ´åˆã¯RFã§ä»£æ›¿
        try:
            import xgboost as xgb
            model = xgb.XGBClassifier(n_estimators=100, random_state=42)
        except ImportError:
            logger.warning("XGBoostæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€‚RandomForestã§ä»£æ›¿")
            from sklearn.ensemble import RandomForestClassifier
            model = RandomForestClassifier(n_estimators=100, random_state=42)
    elif model_type == 'lightgbm':
        # lightgbmãŒãªã„å ´åˆã¯RFã§ä»£æ›¿
        try:
            import lightgbm as lgb
            model = lgb.LGBMClassifier(n_estimators=100, random_state=42)
        except ImportError:
            logger.warning("LightGBMæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€‚RandomForestã§ä»£æ›¿")
            from sklearn.ensemble import RandomForestClassifier
            model = RandomForestClassifier(n_estimators=100, random_state=42)
    else:
        raise ValueError(f"æœªå¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")

    # è¨“ç·´
    model.fit(X_train, y_train)

    logger.info(f"{model_type}ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")

    return {
        'model_type': model_type,
        'model': model
    }

@task
def evaluate_model(model_info: Dict, data: Dict):
    """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
    logger = get_run_logger()

    model_type = model_info['model_type']
    model = model_info['model']

    X_test = data['X_test']
    y_test = data['y_test']

    # äºˆæ¸¬ã¨è©•ä¾¡
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    result = {
        'model_type': model_type,
        'accuracy': float(accuracy),
        'model': model
    }

    logger.info(f"{model_type} - ç²¾åº¦: {accuracy:.4f}")

    return result

@task
def select_best_model(results: List[Dict]):
    """æœ€è‰¯ãƒ¢ãƒ‡ãƒ«é¸æŠ"""
    logger = get_run_logger()

    # ç²¾åº¦ã§æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    best_result = max(results, key=lambda x: x['accuracy'])

    logger.info(f"æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_result['model_type']} (ç²¾åº¦: {best_result['accuracy']:.4f})")

    # å…¨ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ
    logger.info("\n=== ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ ===")
    for result in sorted(results, key=lambda x: x['accuracy'], reverse=True):
        logger.info(f"{result['model_type']}: {result['accuracy']:.4f}")

    return best_result

@task
def save_best_model(best_result: Dict, output_path: str = '/tmp/best_model.pkl'):
    """æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
    logger = get_run_logger()

    model_type = best_result['model_type']
    model = best_result['model']

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    joblib.dump(model, output_path)

    logger.info(f"æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {output_path} ({model_type})")

    return {
        'model_type': model_type,
        'accuracy': best_result['accuracy'],
        'path': output_path
    }

@flow(
    name="Multi-Model Training Pipeline",
    description="è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³",
    task_runner=ConcurrentTaskRunner()  # ä¸¦åˆ—å®Ÿè¡Œ
)
def multi_model_training_pipeline(
    model_types: List[str] = None,
    test_size: float = 0.2
):
    """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    logger = get_run_logger()

    if model_types is None:
        model_types = ['random_forest', 'xgboost', 'lightgbm']

    logger.info(f"è¨“ç·´ãƒ¢ãƒ‡ãƒ«: {model_types}")

    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    data = generate_data()
    split_result = split_data(data, test_size)

    # ä¸¦åˆ—è¨“ç·´
    trained_models = []
    for model_type in model_types:
        trained_model = train_model(model_type, split_result)
        trained_models.append(trained_model)

    # ä¸¦åˆ—è©•ä¾¡
    results = []
    for trained_model in trained_models:
        result = evaluate_model(trained_model, split_result)
        results.append(result)

    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«é¸æŠ
    best_result = select_best_model(results)

    # ä¿å­˜
    saved_info = save_best_model(best_result)

    logger.info(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†: {saved_info}")

    return saved_info

# å®Ÿè¡Œ
if __name__ == "__main__":
    result = multi_model_training_pipeline(
        model_types=['random_forest', 'xgboost', 'lightgbm'],
        test_size=0.2
    )

    print(f"\næœ€çµ‚çµæœ: {result}")
</code></pre>

<p><strong>å®Ÿè¡Œçµæœä¾‹</strong>ï¼š</p>

<pre><code>=== ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ ===
lightgbm: 0.9250
random_forest: 0.9200
xgboost: 0.9150

æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: lightgbm (ç²¾åº¦: 0.9250)
æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ä¿å­˜: /tmp/best_model.pkl (lightgbm)
</code></pre>

</details>

<h3>å•é¡Œ5ï¼ˆDifficultyï¼šhardï¼‰</h3>
<p>ä»¥ä¸‹ã®è¦ä»¶ã‚’æº€ãŸã™ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š
1. æœ€å¤§3å›ã¾ã§ãƒªãƒˆãƒ©ã‚¤
2. æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼ˆ1ç§’ã€2ç§’ã€4ç§’ï¼‰
3. ç‰¹å®šã®ä¾‹å¤–ã®ã¿ãƒªãƒˆãƒ©ã‚¤ï¼ˆValueErrorã€ConnectionErrorãªã©ï¼‰
4. ãƒªãƒˆãƒ©ã‚¤å±¥æ­´ã®ãƒ­ã‚®ãƒ³ã‚°</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import time
import logging
from functools import wraps
from typing import Callable, Tuple, Type

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def retry_with_backoff(
    max_retries: int = 3,
    initial_delay: float = 1.0,
    backoff_factor: float = 2.0,
    retryable_exceptions: Tuple[Type[Exception], ...] = (ValueError, ConnectionError)
):
    """
    ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ããƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ï¼ˆæŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‰

    Args:
        max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
        initial_delay: åˆå›ãƒªãƒˆãƒ©ã‚¤ã¾ã§ã®å¾…æ©Ÿ hoursï¼ˆç§’ï¼‰
        backoff_factor: ãƒãƒƒã‚¯ã‚ªãƒ•ã®å€ç‡
        retryable_exceptions: ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡ã®ä¾‹å¤–ã‚¿ãƒ—ãƒ«
    """
    def decorator(func: Callable):
        @wraps(func)
        def wrapper(*args, **kwargs):
            retries = 0
            delay = initial_delay

            while True:
                try:
                    # é–¢æ•°å®Ÿè¡Œ
                    result = func(*args, **kwargs)

                    # æˆåŠŸæ™‚ã®ãƒ­ã‚°
                    if retries > 0:
                        logger.info(
                            f"{func.__name__} æˆåŠŸï¼ˆ{retries}å›ãƒªãƒˆãƒ©ã‚¤å¾Œï¼‰"
                        )

                    return result

                except retryable_exceptions as e:
                    retries += 1

                    # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤åˆ°é”
                    if retries > max_retries:
                        logger.error(
                            f"{func.__name__} å¤±æ•—: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ï¼ˆ{max_retries}ï¼‰åˆ°é”"
                        )
                        logger.error(f"æœ€çµ‚ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")
                        raise

                    # ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚°
                    logger.warning(
                        f"{func.__name__} å¤±æ•—ï¼ˆ{retries}/{max_retries}ï¼‰: "
                        f"{type(e).__name__}: {e}"
                    )
                    logger.info(f"{delay:.1f}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤...")

                    # å¾…æ©Ÿ
                    time.sleep(delay)

                    # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                    delay *= backoff_factor

                except Exception as e:
                    # ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡å¤–ã®ä¾‹å¤–ã¯å³åº§ã«å†é€å‡º
                    logger.error(
                        f"{func.__name__} å¤±æ•—ï¼ˆãƒªãƒˆãƒ©ã‚¤å¯¾è±¡å¤–ï¼‰: "
                        f"{type(e).__name__}: {e}"
                    )
                    raise

        return wrapper
    return decorator

# ä½¿ç”¨ä¾‹1: APIå‘¼ã³å‡ºã—
@retry_with_backoff(
    max_retries=3,
    initial_delay=1.0,
    backoff_factor=2.0,
    retryable_exceptions=(ConnectionError, TimeoutError)
)
def fetch_data_from_api(url: str):
    """APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆãƒªãƒˆãƒ©ã‚¤ã‚ã‚Šï¼‰"""
    import requests

    logger.info(f"APIå‘¼ã³å‡ºã—: {url}")

    # å®Ÿéš›ã®APIå‘¼ã³å‡ºã—
    response = requests.get(url, timeout=10)
    response.raise_for_status()

    return response.json()

# ä½¿ç”¨ä¾‹2: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
@retry_with_backoff(
    max_retries=5,
    initial_delay=2.0,
    backoff_factor=2.0,
    retryable_exceptions=(ConnectionError,)
)
def connect_to_database(host: str, port: int):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šï¼ˆãƒªãƒˆãƒ©ã‚¤ã‚ã‚Šï¼‰"""
    logger.info(f"DBæ¥ç¶šè©¦è¡Œ: {host}:{port}")

    # å®Ÿéš›ã®DBæ¥ç¶šå‡¦ç†
    # connection = psycopg2.connect(host=host, port=port, ...)

    return "connection_object"

# ä½¿ç”¨ä¾‹3: ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
@retry_with_backoff(
    max_retries=3,
    initial_delay=1.0,
    retryable_exceptions=(ValueError,)
)
def validate_and_process_data(data):
    """ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã¨å‡¦ç†ï¼ˆãƒªãƒˆãƒ©ã‚¤ã‚ã‚Šï¼‰"""
    logger.info("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼é–‹å§‹")

    # æ¤œè¨¼
    if data is None:
        raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒNone")

    if len(data) < 100:
        raise ValueError(f"ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {len(data)}ä»¶")

    # å‡¦ç†
    processed = data * 2

    return processed

# ãƒ†ã‚¹ãƒˆé–¢æ•°
def test_retry_mechanism():
    """ãƒªãƒˆãƒ©ã‚¤ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®ãƒ†ã‚¹ãƒˆ"""

    # ãƒ†ã‚¹ãƒˆ1: æœ€çµ‚çš„ã«æˆåŠŸã™ã‚‹ã‚±ãƒ¼ã‚¹
    attempt_count = 0

    @retry_with_backoff(max_retries=3, initial_delay=0.5)
    def flaky_function():
        nonlocal attempt_count
        attempt_count += 1

        if attempt_count < 3:
            raise ValueError(f"å¤±æ•— ({attempt_count}å›ç›®)")

        return "æˆåŠŸ"

    print("\n=== ãƒ†ã‚¹ãƒˆ1: ãƒªãƒˆãƒ©ã‚¤å¾Œã«æˆåŠŸ ===")
    result = flaky_function()
    print(f"çµæœ: {result}")
    print(f"è©¦è¡Œå›æ•°: {attempt_count}")

    # ãƒ†ã‚¹ãƒˆ2: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤ã§å¤±æ•—
    @retry_with_backoff(max_retries=2, initial_delay=0.5)
    def always_fails():
        raise ValueError("å¸¸ã«å¤±æ•—")

    print("\n=== ãƒ†ã‚¹ãƒˆ2: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤ã§å¤±æ•— ===")
    try:
        always_fails()
    except ValueError as e:
        print(f"äºˆæœŸé€šã‚Šå¤±æ•—: {e}")

    # ãƒ†ã‚¹ãƒˆ3: ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡å¤–ã®ä¾‹å¤–
    @retry_with_backoff(
        max_retries=3,
        retryable_exceptions=(ValueError,)
    )
    def non_retryable_error():
        raise RuntimeError("ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡å¤–ã®ã‚¨ãƒ©ãƒ¼")

    print("\n=== ãƒ†ã‚¹ãƒˆ3: ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡å¤–ã®ä¾‹å¤– ===")
    try:
        non_retryable_error()
    except RuntimeError as e:
        print(f"å³åº§ã«å¤±æ•—: {e}")

if __name__ == "__main__":
    test_retry_mechanism()
</code></pre>

<p><strong>å®Ÿè¡Œä¾‹</strong>ï¼š</p>

<pre><code>=== ãƒ†ã‚¹ãƒˆ1: ãƒªãƒˆãƒ©ã‚¤å¾Œã«æˆåŠŸ ===
WARNING - flaky_function å¤±æ•—ï¼ˆ1/3ï¼‰: ValueError: å¤±æ•— (1å›ç›®)
INFO - 0.5ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤...
WARNING - flaky_function å¤±æ•—ï¼ˆ2/3ï¼‰: ValueError: å¤±æ•— (2å›ç›®)
INFO - 1.0ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤...
INFO - flaky_function æˆåŠŸï¼ˆ2å›ãƒªãƒˆãƒ©ã‚¤å¾Œï¼‰
çµæœ: æˆåŠŸ
è©¦è¡Œå›æ•°: 3

=== ãƒ†ã‚¹ãƒˆ2: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤ã§å¤±æ•— ===
WARNING - always_fails å¤±æ•—ï¼ˆ1/2ï¼‰: ValueError: å¸¸ã«å¤±æ•—
INFO - 0.5ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤...
WARNING - always_fails å¤±æ•—ï¼ˆ2/2ï¼‰: ValueError: å¸¸ã«å¤±æ•—
INFO - 1.0ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤...
ERROR - always_fails å¤±æ•—: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ï¼ˆ2ï¼‰åˆ°é”
ERROR - æœ€çµ‚ã‚¨ãƒ©ãƒ¼: ValueError: å¸¸ã«å¤±æ•—
äºˆæœŸé€šã‚Šå¤±æ•—: å¸¸ã«å¤±æ•—

=== ãƒ†ã‚¹ãƒˆ3: ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡å¤–ã®ä¾‹å¤– ===
ERROR - non_retryable_error å¤±æ•—ï¼ˆãƒªãƒˆãƒ©ã‚¤å¯¾è±¡å¤–ï¼‰: RuntimeError: ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡å¤–ã®ã‚¨ãƒ©ãƒ¼
å³åº§ã«å¤±æ•—: ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡å¤–ã®ã‚¨ãƒ©ãƒ¼
</code></pre>

</details>

<hr>

<h2>References</h2>

<ol>
<li>Apache Airflow Documentation. (2025). <em>Airflow Concepts</em>. Retrieved from https://airflow.apache.org/docs/</li>
<li>Kubeflow Pipelines Documentation. (2025). <em>Building Pipelines</em>. Retrieved from https://www.kubeflow.org/docs/components/pipelines/</li>
<li>Prefect Documentation. (2025). <em>Core Concepts</em>. Retrieved from https://docs.prefect.io/</li>
<li>Kleppmann, M. (2017). <em>Designing Data-Intensive Applications</em>. O'Reilly Media.</li>
<li>Lakshmanan, V., Robinson, S., & Munn, M. (2020). <em>Machine Learning Design Patterns</em>. O'Reilly Media.</li>
</ol>

<div class="navigation">
    <a href="chapter2-ci-cd.html" class="nav-button">â† Previous Chapter: CI/CDã®å®Ÿè£…</a>
    <a href="chapter4-model-management.html" class="nav-button">Next Chapter: ãƒ¢ãƒ‡ãƒ«ç®¡ç†ã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚° â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, either express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The creators and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
            <li>To the maximum extent permitted by applicable law, the creators and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are subject to the specified terms (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆ learner</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
