<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 1: MLOps Fundamentals - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/mlops-introduction/index.html">MLOps</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>

<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="header-content">
<h1>Chapter 1: MLOps Fundamentals</h1>
<p class="subtitle">Foundation Technologies Supporting Machine Learning System Operations</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 25-30 minutes</span>
<span class="meta-item">üìä Difficulty: Beginner</span>
<span class="meta-item">üíª Code Examples: 10</span>
<span class="meta-item">üìù Practice Problems: 5</span>
</div>
</div>
</header>
<main class="container">
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master the following:</p>
<ul>
<li>‚úÖ Understand the definition and necessity of MLOps</li>
<li>‚úÖ Grasp the entire machine learning lifecycle</li>
<li>‚úÖ Understand the main components of MLOps</li>
<li>‚úÖ Learn about the MLOps tool ecosystem</li>
<li>‚úÖ Understand the MLOps maturity model and evaluate current status</li>
<li>‚úÖ Build the foundation of practical MLOps pipelines</li>
</ul>
<hr/>
<h2>1.1 What is MLOps</h2>
<h3>Challenges in Machine Learning Systems</h3>
<p>Many machine learning projects end at the PoC (Proof of Concept) stage and fail to deploy to production environments. The main reasons are as follows:</p>
<table>
<thead>
<tr>
<th>Challenge</th>
<th>Description</th>
<th>Impact</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Gap Between Model and Code</strong></td>
<td>Jupyter Notebook code doesn't work in production</td>
<td>Deployment delays, increased manual work</td>
</tr>
<tr>
<td><strong>Lack of Reproducibility</strong></td>
<td>Cannot reproduce the same results (data, code, environment mismatch)</td>
<td>Difficult debugging, quality degradation</td>
</tr>
<tr>
<td><strong>Model Degradation</strong></td>
<td>Model performance deteriorates over time</td>
<td>Prediction accuracy decline, business loss</td>
</tr>
<tr>
<td><strong>Scalability</strong></td>
<td>Cannot handle large volumes of requests</td>
<td>System downtime, response delays</td>
</tr>
<tr>
<td><strong>Lack of Governance</strong></td>
<td>Unclear who deployed which model when</td>
<td>Compliance violations, audit impossibility</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Statistics</strong>: According to Gartner research, approximately 85% of machine learning projects do not reach production environments.</p>
</blockquote>
<h3>Definition and Purpose of MLOps</h3>
<p><strong>MLOps (Machine Learning Operations)</strong> is a set of practices and tools to automate and standardize the development, deployment, and operation of machine learning models.</p>
<p><strong>Purpose of MLOps</strong>:</p>
<ul>
<li><strong>Rapid Deployment</strong>: Deploy models to production quickly and reliably</li>
<li><strong>Reproducibility</strong>: Guarantee complete reproduction of experiments and models</li>
<li><strong>Automation</strong>: Reduce manual work and minimize errors</li>
<li><strong>Monitoring</strong>: Continuous monitoring and improvement of model performance</li>
<li><strong>Scalability</strong>: Support numerous models and data</li>
<li><strong>Governance</strong>: Compliance and audit support</li>
</ul>
<h3>Relationship with DevOps/DataOps</h3>
<p>MLOps applies DevOps and DataOps principles to machine learning:</p>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Focus</th>
<th>Main Practices</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>DevOps</strong></td>
<td>Software development and operations</td>
<td>CI/CD, infrastructure automation, monitoring</td>
</tr>
<tr>
<td><strong>DataOps</strong></td>
<td>Data pipelines and quality</td>
<td>Data versioning, quality checks, metadata management</td>
</tr>
<tr>
<td><strong>MLOps</strong></td>
<td>Machine learning model lifecycle</td>
<td>Experiment management, model versioning, automatic retraining</td>
</tr>
</tbody>
</table>
<div class="mermaid">
graph LR
    A[DevOps] --&gt; D[MLOps]
    B[DataOps] --&gt; D
    C[Machine Learning] --&gt; D

    D --&gt; E[Automated ML<br/>Lifecycle]

    style A fill:#e3f2fd
    style B fill:#f3e5f5
    style C fill:#fff3e0
    style D fill:#c8e6c9
    style E fill:#ffccbc
</div>
<h3>Real Examples of Problems MLOps Solves</h3>
<pre><code class="language-python">"""
Problem: Models developed in Jupyter Notebook don't work in production

Causes:
- Different library versions between development and production
- Data preprocessing steps are not documented
- Model dependencies are unclear
"""

# ‚ùå Problematic approach (no reproducibility)
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

# Data loading (which version? when?)
df = pd.read_csv('data.csv')

# Preprocessing (unclear steps)
df = df.dropna()
X = df.drop('target', axis=1)
y = df['target']

# Model training (no hyperparameter recording)
model = RandomForestClassifier()
model.fit(X, y)

# Save (no metadata)
import pickle
pickle.dump(model, open('model.pkl', 'wb'))
</code></pre>
<pre><code class="language-python">"""
‚úÖ MLOps approach (with reproducibility)

Features:
- Version control (code, data, model)
- Explicit environment (requirements.txt, Docker)
- Metadata recording (experiment results, parameters)
- Pipelined (reproducible processing flow)
"""

import mlflow
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import json
from datetime import datetime

# Start MLflow experiment
mlflow.set_experiment("customer_churn_prediction")

with mlflow.start_run():
    # 1. Record data version
    data_version = "v1.2.3"
    mlflow.log_param("data_version", data_version)

    # 2. Load data (version-controlled data)
    df = pd.read_csv(f'data/{data_version}/data.csv')

    # 3. Preprocessing (explicit steps)
    df = df.dropna()
    X = df.drop('target', axis=1)
    y = df['target']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # 4. Record hyperparameters
    params = {
        'n_estimators': 100,
        'max_depth': 10,
        'random_state': 42
    }
    mlflow.log_params(params)

    # 5. Model training
    model = RandomForestClassifier(**params)
    model.fit(X_train, y_train)

    # 6. Evaluation and recording
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    mlflow.log_metric("accuracy", accuracy)

    # 7. Save model (with metadata)
    mlflow.sklearn.log_model(
        model,
        "model",
        registered_model_name="churn_predictor"
    )

    # 8. Additional metadata
    mlflow.set_tag("model_type", "RandomForest")
    mlflow.set_tag("created_by", "data_science_team")
    mlflow.set_tag("timestamp", datetime.now().isoformat())

    print(f"‚úì Model training complete - Accuracy: {accuracy:.3f}")
    print(f"‚úì Experiment ID: {mlflow.active_run().info.run_id}")
</code></pre>
<hr/>
<h2>1.2 ML Lifecycle</h2>
<h3>Overview of Machine Learning Projects</h3>
<p>Machine learning projects are an iterative process consisting of the following phases:</p>
<div class="mermaid">
graph TB
    A[Business Understanding] --&gt; B[Data Collection &amp; Preparation]
    B --&gt; C[Model Development &amp; Training]
    C --&gt; D[Model Evaluation]
    D --&gt; E{Performance OK?}
    E --&gt;|No| C
    E --&gt;|Yes| F[Deployment]
    F --&gt; G[Monitoring]
    G --&gt; H{Retraining Needed?}
    H --&gt;|Yes| B
    H --&gt;|No| G

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#fce4ec
    style F fill:#c8e6c9
    style G fill:#ffccbc
</div>
<h3>1. Data Collection and Preparation</h3>
<p>Building data pipelines and quality assurance:</p>
<pre><code class="language-python">"""
Data Collection &amp; Preparation Phase

Purpose:
- Build high-quality datasets
- Data validation and versioning
- Reproducible preprocessing pipeline
"""

import pandas as pd
import great_expectations as ge
from sklearn.model_selection import train_test_split
import hashlib
import json

class DataPipeline:
    """Data pipeline class"""

    def __init__(self, data_path, version):
        self.data_path = data_path
        self.version = version
        self.metadata = {}

    def load_data(self):
        """Load data"""
        df = pd.read_csv(self.data_path)

        # Calculate data hash (for integrity check)
        data_hash = hashlib.md5(
            pd.util.hash_pandas_object(df).values
        ).hexdigest()

        self.metadata['data_hash'] = data_hash
        self.metadata['n_samples'] = len(df)
        self.metadata['n_features'] = len(df.columns)

        print(f"‚úì Data loaded: {len(df)} rows, {len(df.columns)} columns")
        print(f"‚úì Data hash: {data_hash[:8]}...")

        return df

    def validate_data(self, df):
        """Data quality validation"""
        # Data validation using Great Expectations
        df_ge = ge.from_pandas(df)

        # Define expectations
        expectations = []

        # 1. Missing value check
        for col in df.columns:
            missing_pct = df[col].isnull().mean()
            expectations.append({
                'column': col,
                'check': 'missing_values',
                'value': f"{missing_pct:.2%}"
            })
            if missing_pct &gt; 0.3:
                print(f"‚ö†Ô∏è  Warning: {col} has more than 30% missing values")

        # 2. Data type check
        expectations.append({
            'check': 'data_types',
            'dtypes': df.dtypes.to_dict()
        })

        # 3. Duplicate check
        n_duplicates = df.duplicated().sum()
        expectations.append({
            'check': 'duplicates',
            'value': n_duplicates
        })

        self.metadata['validation'] = expectations

        print(f"‚úì Data validation complete")
        print(f"  - Duplicate rows: {n_duplicates}")

        return df

    def preprocess_data(self, df):
        """Data preprocessing"""
        # Handle missing values
        df_clean = df.copy()

        # Numeric columns: median imputation
        numeric_cols = df_clean.select_dtypes(include=['float64', 'int64']).columns
        for col in numeric_cols:
            if df_clean[col].isnull().any():
                median_val = df_clean[col].median()
                df_clean[col].fillna(median_val, inplace=True)

        # Categorical columns: mode imputation
        cat_cols = df_clean.select_dtypes(include=['object']).columns
        for col in cat_cols:
            if df_clean[col].isnull().any():
                mode_val = df_clean[col].mode()[0]
                df_clean[col].fillna(mode_val, inplace=True)

        print(f"‚úì Preprocessing complete")

        return df_clean

    def save_metadata(self, filepath):
        """Save metadata"""
        with open(filepath, 'w') as f:
            json.dump(self.metadata, f, indent=2, default=str)
        print(f"‚úì Metadata saved: {filepath}")

# Usage example
pipeline = DataPipeline('customer_data.csv', 'v1.0.0')
df = pipeline.load_data()
df = pipeline.validate_data(df)
df_clean = pipeline.preprocess_data(df)
pipeline.save_metadata('data_metadata.json')
</code></pre>
<h3>2. Model Development and Training</h3>
<p>Experiment management and ensuring reproducibility:</p>
<pre><code class="language-python">"""
Model Development &amp; Training Phase

Purpose:
- Systematic experiment management
- Hyperparameter optimization
- Model versioning
"""

import mlflow
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
import numpy as np

class ExperimentManager:
    """Experiment management class"""

    def __init__(self, experiment_name):
        mlflow.set_experiment(experiment_name)
        self.experiment_name = experiment_name

    def train_and_log(self, model, X_train, y_train, model_name, params):
        """Train and log model"""
        with mlflow.start_run(run_name=model_name):
            # Log parameters
            mlflow.log_params(params)

            # Cross-validation
            cv_scores = cross_val_score(
                model, X_train, y_train, cv=5, scoring='accuracy'
            )

            # Training
            model.fit(X_train, y_train)

            # Log metrics
            mlflow.log_metric("cv_mean_accuracy", cv_scores.mean())
            mlflow.log_metric("cv_std_accuracy", cv_scores.std())

            # Save model
            mlflow.sklearn.log_model(model, "model")

            print(f"‚úì {model_name}")
            print(f"  - CV accuracy: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}")

            return cv_scores.mean()

    def compare_models(self, X_train, y_train):
        """Compare multiple models"""
        models = {
            'LogisticRegression': {
                'model': LogisticRegression(max_iter=1000),
                'params': {'C': 1.0, 'max_iter': 1000}
            },
            'RandomForest': {
                'model': RandomForestClassifier(n_estimators=100, random_state=42),
                'params': {'n_estimators': 100, 'max_depth': 10}
            },
            'GradientBoosting': {
                'model': GradientBoostingClassifier(n_estimators=100, random_state=42),
                'params': {'n_estimators': 100, 'learning_rate': 0.1}
            }
        }

        results = {}
        for name, config in models.items():
            score = self.train_and_log(
                config['model'],
                X_train,
                y_train,
                name,
                config['params']
            )
            results[name] = score

        # Select best model
        best_model = max(results, key=results.get)
        print(f"\nüèÜ Best model: {best_model} (accuracy: {results[best_model]:.3f})")

        return results

# Usage example
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# Sample data
X, y = make_classification(
    n_samples=1000, n_features=20, n_informative=15,
    n_redundant=5, random_state=42
)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Run experiment
exp_manager = ExperimentManager("model_comparison")
results = exp_manager.compare_models(X_train, y_train)
</code></pre>
<h3>3. Deployment and Operations</h3>
<p>Deploying models to production environments:</p>
<pre><code class="language-python">"""
Deployment Phase

Purpose:
- Model API-ification
- Version control
- A/B testing support
"""

from flask import Flask, request, jsonify
import mlflow.pyfunc
import numpy as np
import logging

class ModelServer:
    """Model server class"""

    def __init__(self, model_uri, model_version):
        """
        Args:
            model_uri: MLflow model URI
            model_version: Model version
        """
        self.model = mlflow.pyfunc.load_model(model_uri)
        self.model_version = model_version
        self.prediction_count = 0

        # Logging setup
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def predict(self, features):
        """Execute prediction"""
        try:
            # Input validation
            if not isinstance(features, (list, np.ndarray)):
                raise ValueError("Input must be an array")

            # Prediction
            prediction = self.model.predict(np.array(features).reshape(1, -1))

            # Update count
            self.prediction_count += 1

            # Log recording
            self.logger.info(f"Prediction executed #{self.prediction_count}")

            return {
                'prediction': int(prediction[0]),
                'model_version': self.model_version,
                'prediction_id': self.prediction_count
            }

        except Exception as e:
            self.logger.error(f"Prediction error: {str(e)}")
            return {'error': str(e)}

# Flask API
app = Flask(__name__)

# Load model (from MLflow Model Registry in production)
model_server = ModelServer(
    model_uri="models:/churn_predictor/production",
    model_version="1.0.0"
)

@app.route('/predict', methods=['POST'])
def predict():
    """Prediction endpoint"""
    data = request.get_json()
    features = data.get('features')

    if features is None:
        return jsonify({'error': 'features are required'}), 400

    result = model_server.predict(features)

    if 'error' in result:
        return jsonify(result), 500

    return jsonify(result), 200

@app.route('/health', methods=['GET'])
def health():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'model_version': model_server.model_version,
        'total_predictions': model_server.prediction_count
    }), 200

# Sample client
def sample_client():
    """API client usage example"""
    import requests

    # Prediction request
    response = requests.post(
        'http://localhost:5000/predict',
        json={'features': [0.5, 1.2, -0.3, 2.1, 0.8]}
    )

    if response.status_code == 200:
        result = response.json()
        print(f"Prediction result: {result['prediction']}")
        print(f"Model version: {result['model_version']}")
    else:
        print(f"Error: {response.json()}")

# if __name__ == '__main__':
#     app.run(host='0.0.0.0', port=5000)
</code></pre>
<h3>4. Monitoring and Improvement</h3>
<p>Model performance monitoring in production:</p>
<pre><code class="language-python">"""
Monitoring Phase

Purpose:
- Continuous monitoring of model performance
- Data drift detection
- Alerts and automatic retraining triggers
"""

import numpy as np
import pandas as pd
from scipy import stats
from datetime import datetime, timedelta

class ModelMonitor:
    """Model monitoring class"""

    def __init__(self, baseline_data, threshold=0.05):
        """
        Args:
            baseline_data: Baseline data (training data)
            threshold: Drift detection threshold
        """
        self.baseline_data = baseline_data
        self.threshold = threshold
        self.drift_history = []

    def detect_data_drift(self, new_data, feature_name):
        """Data drift detection (Kolmogorov-Smirnov test)"""
        baseline_feature = self.baseline_data[feature_name]
        new_feature = new_data[feature_name]

        # KS test
        statistic, p_value = stats.ks_2samp(baseline_feature, new_feature)

        is_drift = p_value &lt; self.threshold

        drift_info = {
            'timestamp': datetime.now(),
            'feature': feature_name,
            'statistic': statistic,
            'p_value': p_value,
            'drift_detected': is_drift
        }

        self.drift_history.append(drift_info)

        if is_drift:
            print(f"‚ö†Ô∏è  Data drift detected: {feature_name}")
            print(f"   KS statistic: {statistic:.4f}, p-value: {p_value:.4f}")

        return is_drift

    def monitor_predictions(self, predictions, actuals=None):
        """Prediction monitoring"""
        monitoring_report = {
            'timestamp': datetime.now(),
            'n_predictions': len(predictions),
            'prediction_distribution': {
                'mean': np.mean(predictions),
                'std': np.std(predictions),
                'min': np.min(predictions),
                'max': np.max(predictions)
            }
        }

        # Calculate accuracy if actuals are available
        if actuals is not None:
            accuracy = np.mean(predictions == actuals)
            monitoring_report['accuracy'] = accuracy

            if accuracy &lt; 0.7:  # Example threshold
                print(f"‚ö†Ô∏è  Accuracy degradation detected: {accuracy:.3f}")
                print("   Retraining recommended")

        return monitoring_report

    def generate_report(self):
        """Generate monitoring report"""
        if not self.drift_history:
            return "No monitoring data available"

        df_drift = pd.DataFrame(self.drift_history)

        report = f"""
=== Model Monitoring Report ===
Period: {df_drift['timestamp'].min()} ~ {df_drift['timestamp'].max()}
Total checks: {len(df_drift)}
Drift detections: {df_drift['drift_detected'].sum()}

Features with drift detected:
{df_drift[df_drift['drift_detected']][['feature', 'p_value']].to_string()}
        """

        return report

# Usage example
from sklearn.datasets import make_classification

# Baseline data (training data)
X_baseline, _ = make_classification(
    n_samples=1000, n_features=5, random_state=42
)
df_baseline = pd.DataFrame(
    X_baseline,
    columns=[f'feature_{i}' for i in range(5)]
)

# New data (production input data)
# Add shift to simulate drift
X_new, _ = make_classification(
    n_samples=500, n_features=5, random_state=43
)
X_new[:, 0] += 1.5  # Add shift to feature_0
df_new = pd.DataFrame(
    X_new,
    columns=[f'feature_{i}' for i in range(5)]
)

# Monitoring
monitor = ModelMonitor(df_baseline, threshold=0.05)

print("=== Data Drift Detection ===")
for col in df_baseline.columns:
    monitor.detect_data_drift(df_new, col)

print("\n" + monitor.generate_report())
</code></pre>
<hr/>
<h2>1.3 Main Components of MLOps</h2>
<h3>1. Data Management</h3>
<p>Data versioning, quality management, and lineage tracking:</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Purpose</th>
<th>Example Tools</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Data Versioning</strong></td>
<td>Dataset change history management</td>
<td>DVC, LakeFS, Delta Lake</td>
</tr>
<tr>
<td><strong>Data Quality</strong></td>
<td>Data validation and anomaly detection</td>
<td>Great Expectations, Deequ</td>
</tr>
<tr>
<td><strong>Data Lineage</strong></td>
<td>Data origin and transformation history</td>
<td>Apache Atlas, Marquez</td>
</tr>
<tr>
<td><strong>Feature Store</strong></td>
<td>Feature reuse and consistency</td>
<td>Feast, Tecton</td>
</tr>
</tbody>
</table>
<pre><code class="language-python">"""
Data versioning implementation example (DVC-style)
"""

import os
import hashlib
import json
from pathlib import Path

class SimpleDataVersioning:
    """Simple data versioning system"""

    def __init__(self, storage_dir='.data_versions'):
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(exist_ok=True)
        self.manifest_file = self.storage_dir / 'manifest.json'
        self.manifest = self._load_manifest()

    def _load_manifest(self):
        """Load manifest file"""
        if self.manifest_file.exists():
            with open(self.manifest_file, 'r') as f:
                return json.load(f)
        return {}

    def _save_manifest(self):
        """Save manifest file"""
        with open(self.manifest_file, 'w') as f:
            json.dump(self.manifest, f, indent=2)

    def _compute_hash(self, filepath):
        """Compute file hash"""
        hasher = hashlib.md5()
        with open(filepath, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b''):
                hasher.update(chunk)
        return hasher.hexdigest()

    def add(self, filepath, version_tag):
        """Add data file to version control"""
        filepath = Path(filepath)

        if not filepath.exists():
            raise FileNotFoundError(f"{filepath} not found")

        # Compute hash
        file_hash = self._compute_hash(filepath)

        # Copy to storage
        storage_path = self.storage_dir / f"{version_tag}_{file_hash[:8]}"
        import shutil
        shutil.copy(filepath, storage_path)

        # Update manifest
        self.manifest[version_tag] = {
            'original_path': str(filepath),
            'storage_path': str(storage_path),
            'hash': file_hash,
            'size': filepath.stat().st_size,
            'timestamp': str(pd.Timestamp.now())
        }

        self._save_manifest()

        print(f"‚úì Added {filepath.name} as version {version_tag}")
        print(f"  Hash: {file_hash[:8]}...")

    def checkout(self, version_tag, output_path=None):
        """Retrieve specific version of data"""
        if version_tag not in self.manifest:
            raise ValueError(f"Version {version_tag} not found")

        version_info = self.manifest[version_tag]
        storage_path = Path(version_info['storage_path'])

        if output_path is None:
            output_path = version_info['original_path']

        import shutil
        shutil.copy(storage_path, output_path)

        print(f"‚úì Checked out version {version_tag}")
        print(f"  Output: {output_path}")

    def list_versions(self):
        """List all versions"""
        if not self.manifest:
            print("No versioned data available")
            return

        print("=== Data Version List ===")
        for tag, info in self.manifest.items():
            print(f"\nVersion: {tag}")
            print(f"  Path: {info['original_path']}")
            print(f"  Size: {info['size']:,} bytes")
            print(f"  Hash: {info['hash'][:8]}...")
            print(f"  Created: {info['timestamp']}")

# Usage example (demo)
# dvc = SimpleDataVersioning()
# dvc.add('data.csv', 'v1.0.0')
# dvc.add('data.csv', 'v1.1.0')  # After data update
# dvc.list_versions()
# dvc.checkout('v1.0.0', 'data_old.csv')
</code></pre>
<h3>2. Model Management</h3>
<p>Managing the entire model lifecycle:</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Purpose</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Experiment Tracking</strong></td>
<td>Recording and comparing experiment results</td>
<td>Logging parameters, metrics, and artifacts</td>
</tr>
<tr>
<td><strong>Model Registry</strong></td>
<td>Centralized model management</td>
<td>Version control, stage management, approval flow</td>
</tr>
<tr>
<td><strong>Model Packaging</strong></td>
<td>Converting to deployable format</td>
<td>Dependency resolution, containerization</td>
</tr>
</tbody>
</table>
<h3>3. Infrastructure Management</h3>
<p>Scalable and reproducible infrastructure:</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Purpose</th>
<th>Example Tools</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Containerization</strong></td>
<td>Ensuring environment consistency</td>
<td>Docker, Kubernetes</td>
</tr>
<tr>
<td><strong>Orchestration</strong></td>
<td>Workflow automation</td>
<td>Airflow, Kubeflow, Argo</td>
</tr>
<tr>
<td><strong>Resource Management</strong></td>
<td>Efficient use of computational resources</td>
<td>Kubernetes, Ray</td>
</tr>
</tbody>
</table>
<h3>4. Governance</h3>
<p>Compliance and audit support:</p>
<table>
<thead>
<tr>
<th>Element</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Model Explainability</strong></td>
<td>Explaining prediction rationale</td>
</tr>
<tr>
<td><strong>Bias Detection</strong></td>
<td>Fairness verification</td>
</tr>
<tr>
<td><strong>Audit Logs</strong></td>
<td>Recording all change history</td>
</tr>
<tr>
<td><strong>Access Control</strong></td>
<td>Permission management and approval flow</td>
</tr>
</tbody>
</table>
<hr/>
<h2>1.4 MLOps Tool Ecosystem</h2>
<h3>Experiment Management Tools</h3>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Features</th>
<th>Main Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MLflow</strong></td>
<td>Open source, multi-functional</td>
<td>Experiment management, model registry, deployment</td>
</tr>
<tr>
<td><strong>Weights &amp; Biases</strong></td>
<td>Real-time visualization, collaboration</td>
<td>Experiment comparison, hyperparameter optimization</td>
</tr>
<tr>
<td><strong>Neptune.ai</strong></td>
<td>Specialized in metadata management</td>
<td>Long-term experiment management, team collaboration</td>
</tr>
</tbody>
</table>
<h3>Pipeline Orchestration</h3>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Features</th>
<th>Main Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Kubeflow</strong></td>
<td>ML on Kubernetes</td>
<td>End-to-end ML pipelines</td>
</tr>
<tr>
<td><strong>Apache Airflow</strong></td>
<td>General-purpose workflow</td>
<td>Data pipelines, scheduling</td>
</tr>
<tr>
<td><strong>Prefect</strong></td>
<td>Python-native, modern API</td>
<td>Data flow, error handling</td>
</tr>
</tbody>
</table>
<h3>Model Deployment</h3>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Features</th>
<th>Main Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>BentoML</strong></td>
<td>Specialized in model serving</td>
<td>REST API, batch inference</td>
</tr>
<tr>
<td><strong>Seldon Core</strong></td>
<td>Deployment on Kubernetes</td>
<td>Microservices, A/B testing</td>
</tr>
<tr>
<td><strong>TensorFlow Serving</strong></td>
<td>TensorFlow-specific</td>
<td>Fast inference, GPU support</td>
</tr>
</tbody>
</table>
<h3>Monitoring Tools</h3>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Features</th>
<th>Main Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Evidently</strong></td>
<td>Data drift detection</td>
<td>Model performance monitoring, report generation</td>
</tr>
<tr>
<td><strong>Prometheus + Grafana</strong></td>
<td>General-purpose metrics monitoring</td>
<td>System monitoring, alerts</td>
</tr>
<tr>
<td><strong>Arize AI</strong></td>
<td>ML-specialized observability</td>
<td>Model monitoring, root cause analysis</td>
</tr>
</tbody>
</table>
<h3>Integrated Platforms</h3>
<table>
<thead>
<tr>
<th>Platform</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>AWS SageMaker</strong></td>
<td>AWS-native, fully managed</td>
</tr>
<tr>
<td><strong>Azure ML</strong></td>
<td>Azure ecosystem integration</td>
</tr>
<tr>
<td><strong>Google Vertex AI</strong></td>
<td>GCP service integration, AutoML</td>
</tr>
<tr>
<td><strong>Databricks</strong></td>
<td>Data + ML integration, Spark foundation</td>
</tr>
</tbody>
</table>
<hr/>
<h2>1.5 MLOps Maturity Model</h2>
<p>Framework for evaluating organizational MLOps maturity (proposed by Google):</p>
<h3>Level 0: Manual Process</h3>
<p><strong>Characteristics</strong>:</p>
<ul>
<li>All steps are manual</li>
<li>Jupyter Notebook-based development</li>
<li>Manual model deployment</li>
<li>No reproducibility</li>
</ul>
<p><strong>Challenges</strong>:</p>
<ul>
<li>Does not scale</li>
<li>Frequent errors</li>
<li>Time-consuming deployment</li>
<li>No monitoring</li>
</ul>
<h3>Level 1: ML Pipeline Automation</h3>
<p><strong>Characteristics</strong>:</p>
<ul>
<li>Automated training pipeline</li>
<li>Continuous Training (CT)</li>
<li>Experiment tracking</li>
<li>Use of model registry</li>
</ul>
<p><strong>Achievements</strong>:</p>
<ul>
<li>Automatic retraining with new data</li>
<li>Model versioning</li>
<li>Basic monitoring</li>
</ul>
<pre><code class="language-python">"""
Level 1 implementation example: Automated training pipeline
"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
import mlflow
import schedule
import time

class AutoTrainingPipeline:
    """Automated training pipeline"""

    def __init__(self, experiment_name):
        mlflow.set_experiment(experiment_name)
        self.experiment_name = experiment_name

    def create_pipeline(self):
        """Build ML pipeline"""
        return Pipeline([
            ('scaler', StandardScaler()),
            ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
        ])

    def train(self, X_train, y_train, X_val, y_val):
        """Execute training"""
        with mlflow.start_run():
            # Create pipeline
            pipeline = self.create_pipeline()

            # Training
            pipeline.fit(X_train, y_train)

            # Evaluation
            train_score = pipeline.score(X_train, y_train)
            val_score = pipeline.score(X_val, y_val)

            # Log to MLflow
            mlflow.log_metric("train_accuracy", train_score)
            mlflow.log_metric("val_accuracy", val_score)
            mlflow.sklearn.log_model(pipeline, "model")

            # Register to model registry
            if val_score &gt; 0.8:  # Only if above threshold
                mlflow.register_model(
                    f"runs:/{mlflow.active_run().info.run_id}/model",
                    "production_model"
                )
                print(f"‚úì Registered new model (validation accuracy: {val_score:.3f})")
            else:
                print(f"‚ö†Ô∏è  Accuracy below threshold ({val_score:.3f} &lt; 0.8)")

            return pipeline

    def scheduled_training(self, data_loader, schedule_time="00:00"):
        """Scheduled training"""
        def job():
            print(f"=== Automatic training started: {pd.Timestamp.now()} ===")
            X_train, X_val, y_train, y_val = data_loader()
            self.train(X_train, X_val, y_train, y_val)

        # Execute daily at specified time
        schedule.every().day.at(schedule_time).do(job)

        print(f"‚úì Automatic training scheduled: Daily at {schedule_time}")

        # Schedule execution (run as background service in actual environment)
        # while True:
        #     schedule.run_pending()
        #     time.sleep(60)

# Usage example (demo)
# def load_latest_data():
#     # Latest data loading logic
#     return X_train, X_val, y_train, y_val
#
# pipeline = AutoTrainingPipeline("auto_training")
# pipeline.scheduled_training(load_latest_data, "02:00")
</code></pre>
<h3>Level 2: CI/CD Pipeline Automation</h3>
<p><strong>Characteristics</strong>:</p>
<ul>
<li>Complete automation (from code changes to deployment)</li>
<li>CI/CD integration</li>
<li>Automated testing (data, model, infrastructure)</li>
<li>Comprehensive monitoring</li>
</ul>
<p><strong>Achievements</strong>:</p>
<ul>
<li>Automatic deployment of code changes</li>
<li>A/B testing</li>
<li>Canary deployment</li>
<li>Automatic rollback</li>
</ul>
<div class="mermaid">
graph TB
    subgraph "Level 0: Manual"
        A1[Notebook Development] --&gt; A2[Manual Training]
        A2 --&gt; A3[Manual Deployment]
    end

    subgraph "Level 1: ML Pipeline"
        B1[Code Development] --&gt; B2[Automated Training Pipeline]
        B2 --&gt; B3[Model Registry]
        B3 --&gt; B4[Manual Deployment Approval]
        B4 --&gt; B5[Deployment]
    end

    subgraph "Level 2: CI/CD"
        C1[Code Change] --&gt; C2[CI: Automated Testing]
        C2 --&gt; C3[Automated Training]
        C3 --&gt; C4[Automated Validation]
        C4 --&gt; C5[CD: Automated Deployment]
        C5 --&gt; C6[Monitoring]
        C6 --&gt; C7{Performance OK?}
        C7 --&gt;|No| C8[Automatic Rollback]
        C7 --&gt;|Yes| C6
    end

    style A1 fill:#ffebee
    style B2 fill:#fff3e0
    style C5 fill:#e8f5e9
</div>
<h3>Comparison of Maturity Levels</h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Level 0</th>
<th>Level 1</th>
<th>Level 2</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Deployment Frequency</strong></td>
<td>Monthly to yearly</td>
<td>Weekly to monthly</td>
<td>Daily to weekly</td>
</tr>
<tr>
<td><strong>Reproducibility</strong></td>
<td>Low</td>
<td>Medium</td>
<td>High</td>
</tr>
<tr>
<td><strong>Automation</strong></td>
<td>None</td>
<td>Training only</td>
<td>End-to-end</td>
</tr>
<tr>
<td><strong>Monitoring</strong></td>
<td>None/manual</td>
<td>Basic</td>
<td>Comprehensive</td>
</tr>
<tr>
<td><strong>Testing</strong></td>
<td>None</td>
<td>Model only</td>
<td>All components</td>
</tr>
<tr>
<td><strong>Application Scale</strong></td>
<td>1-2 models</td>
<td>Several models</td>
<td>Many models</td>
</tr>
</tbody>
</table>
<hr/>
<h2>1.6 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li><p><strong>Necessity of MLOps</strong></p>
<ul>
<li>85% of machine learning projects don't reach production</li>
<li>MLOps bridges the gap from development to operations</li>
<li>Integrated approach of DevOps, DataOps, and ML</li>
</ul></li>
<li><p><strong>ML Lifecycle</strong></p>
<ul>
<li>Four phases: data collection/preparation, model development, deployment, monitoring</li>
<li>Iterative and continuous process</li>
<li>Automation and quality assurance important in each phase</li>
</ul></li>
<li><p><strong>Main Components</strong></p>
<ul>
<li>Data management: versioning, quality, lineage</li>
<li>Model management: experiment tracking, registry</li>
<li>Infrastructure management: containerization, orchestration</li>
<li>Governance: explainability, audit, access control</li>
</ul></li>
<li><p><strong>Tool Ecosystem</strong></p>
<ul>
<li>Experiment management: MLflow, Weights &amp; Biases</li>
<li>Pipelines: Kubeflow, Airflow</li>
<li>Deployment: BentoML, Seldon</li>
<li>Monitoring: Evidently, Prometheus</li>
</ul></li>
<li><p><strong>Maturity Model</strong></p>
<ul>
<li>Level 0: Fully manual (does not scale)</li>
<li>Level 1: Training pipeline automation</li>
<li>Level 2: Complete CI/CD automation (enterprise-ready)</li>
</ul></li>
</ol>
<h3>MLOps Implementation Best Practices</h3>
<table>
<thead>
<tr>
<th>Principle</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Start Small</strong></td>
<td>Evolve gradually from Level 0 ‚Üí Level 1 ‚Üí Level 2</td>
</tr>
<tr>
<td><strong>Automation First</strong></td>
<td>Minimize manual work and reduce errors</td>
</tr>
<tr>
<td><strong>Monitoring Required</strong></td>
<td>Continuously monitor performance in production</td>
</tr>
<tr>
<td><strong>Ensure Reproducibility</strong></td>
<td>Make all experiments and models reproducible</td>
</tr>
<tr>
<td><strong>Team Collaboration</strong></td>
<td>Cooperation between data scientists and engineers</td>
</tr>
</tbody>
</table>
<h3>To the Next Chapter</h3>
<p>In Chapter 2, we will learn about <strong>Experiment Management and Model Tracking</strong> in detail:</p>
<ul>
<li>Experiment management with MLflow</li>
<li>Hyperparameter optimization</li>
<li>Utilizing model registry</li>
<li>Visualization and comparison of experiment results</li>
<li>Sharing experiments in teams</li>
</ul>
<hr/>
<h2>Practice Problems</h2>
<h3>Problem 1 (Difficulty: easy)</h3>
<p>List and explain three differences between MLOps and DevOps. Focus on machine learning-specific challenges.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<ol>
<li><p><strong>Handling Data</strong></p>
<ul>
<li><strong>DevOps</strong>: Focuses on code version control</li>
<li><strong>MLOps</strong>: Requires version control of code, data, and models</li>
<li>In machine learning, the same code with different data produces different results, making data versioning essential</li>
</ul></li>
<li><p><strong>Testing Complexity</strong></p>
<ul>
<li><strong>DevOps</strong>: Deterministic testing (same input ‚Üí same output)</li>
<li><strong>MLOps</strong>: Probabilistic testing (model performance, data drift, bias, etc.)</li>
<li>Model testing must evaluate not only accuracy but also fairness and interpretability</li>
</ul></li>
<li><p><strong>Continuous Monitoring</strong></p>
<ul>
<li><strong>DevOps</strong>: Monitor system uptime and error rates</li>
<li><strong>MLOps</strong>: Monitor model performance degradation, data drift, and prediction distribution changes</li>
<li>Model degradation over time is unavoidable, requiring automatic retraining mechanisms</li>
</ul></li>
</ol>
</details>
<h3>Problem 2 (Difficulty: medium)</h3>
<p>Improve the following code to implement experiment management following MLOps best practices. Use MLflow to record parameters, metrics, and models.</p>
<pre><code class="language-python">import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load data
df = pd.read_csv('data.csv')
X = df.drop('target', axis=1)
y = df['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Train model
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)

# Evaluate
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
</code></pre>
<details>
<summary>Sample Answer</summary>
<pre><code class="language-python">import pandas as pd
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import hashlib
import json

# MLflow experiment setup
mlflow.set_experiment("customer_classification")

# Data version calculation
def compute_data_version(df):
    """Calculate data hash to use as version"""
    data_str = pd.util.hash_pandas_object(df).values.tobytes()
    return hashlib.md5(data_str).hexdigest()[:8]

# Load data
df = pd.read_csv('data.csv')
data_version = compute_data_version(df)

X = df.drop('target', axis=1)
y = df['target']

# Start experiment
with mlflow.start_run(run_name="rf_baseline"):

    # Define hyperparameters
    params = {
        'n_estimators': 100,
        'max_depth': 10,
        'min_samples_split': 5,
        'random_state': 42
    }

    # Data split (ensure reproducibility with fixed seed)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # Log parameters
    mlflow.log_params(params)
    mlflow.log_param("data_version", data_version)
    mlflow.log_param("test_size", 0.2)
    mlflow.log_param("n_train_samples", len(X_train))
    mlflow.log_param("n_test_samples", len(X_test))

    # Train model
    model = RandomForestClassifier(**params)
    model.fit(X_train, y_train)

    # Cross-validation
    cv_scores = cross_val_score(model, X_train, y_train, cv=5)
    mlflow.log_metric("cv_mean_accuracy", cv_scores.mean())
    mlflow.log_metric("cv_std_accuracy", cv_scores.std())

    # Test set evaluation
    y_pred = model.predict(X_test)

    # Log multiple metrics
    metrics = {
        'test_accuracy': accuracy_score(y_test, y_pred),
        'test_precision': precision_score(y_test, y_pred, average='weighted'),
        'test_recall': recall_score(y_test, y_pred, average='weighted'),
        'test_f1': f1_score(y_test, y_pred, average='weighted')
    }

    mlflow.log_metrics(metrics)

    # Log feature importance
    feature_importance = dict(zip(X.columns, model.feature_importances_))
    mlflow.log_dict(feature_importance, "feature_importance.json")

    # Save model
    mlflow.sklearn.log_model(
        model,
        "model",
        registered_model_name="customer_classifier"
    )

    # Set tags
    mlflow.set_tag("model_type", "RandomForest")
    mlflow.set_tag("framework", "scikit-learn")
    mlflow.set_tag("environment", "development")

    # Display results
    print("=== Experiment Results ===")
    print(f"Run ID: {mlflow.active_run().info.run_id}")
    print(f"Data Version: {data_version}")
    print(f"\nMetrics:")
    for metric, value in metrics.items():
        print(f"  {metric}: {value:.4f}")
    print(f"\nCV Accuracy: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")

    # Confirm model registration
    print(f"\n‚úì Model logged to MLflow")
    print(f"‚úì Experiment name: customer_classification")
</code></pre>
<p><strong>Improvements</strong>:</p>
<ol>
<li>Manage experiments with MLflow</li>
<li>Record data version</li>
<li>Log all hyperparameters</li>
<li>Record multiple evaluation metrics</li>
<li>Perform cross-validation</li>
<li>Save feature importance</li>
<li>Register to model registry</li>
<li>Ensure reproducibility (random_state, stratify)</li>
</ol>
</details>
<h3>Problem 3 (Difficulty: medium)</h3>
<p>Implement a data drift detection system. Use the Kolmogorov-Smirnov test to determine if new data is statistically different from baseline data and output alerts.</p>
<details>
<summary>Sample Answer</summary>
<pre><code class="language-python">import numpy as np
import pandas as pd
from scipy import stats
from datetime import datetime
import json

class DataDriftMonitor:
    """Data drift monitoring system"""

    def __init__(self, baseline_data, threshold=0.05, alert_features=None):
        """
        Args:
            baseline_data: Baseline data (DataFrame)
            threshold: p-value threshold (default: 0.05)
            alert_features: List of features to monitor (all features if None)
        """
        self.baseline_data = baseline_data
        self.threshold = threshold
        self.alert_features = alert_features or baseline_data.columns.tolist()
        self.drift_history = []
        self.baseline_stats = self._compute_baseline_stats()

    def _compute_baseline_stats(self):
        """Compute baseline statistics"""
        stats_dict = {}
        for col in self.baseline_data.columns:
            if pd.api.types.is_numeric_dtype(self.baseline_data[col]):
                stats_dict[col] = {
                    'mean': self.baseline_data[col].mean(),
                    'std': self.baseline_data[col].std(),
                    'min': self.baseline_data[col].min(),
                    'max': self.baseline_data[col].max(),
                    'median': self.baseline_data[col].median()
                }
        return stats_dict

    def detect_drift(self, new_data, feature):
        """Detect drift for single feature"""
        if feature not in self.baseline_data.columns:
            raise ValueError(f"Feature {feature} not found")

        # Process only numeric types
        if not pd.api.types.is_numeric_dtype(self.baseline_data[feature]):
            return None

        # KS test
        baseline_values = self.baseline_data[feature].dropna()
        new_values = new_data[feature].dropna()

        statistic, p_value = stats.ks_2samp(baseline_values, new_values)

        is_drift = p_value &lt; self.threshold

        # Calculate statistical changes
        baseline_mean = baseline_values.mean()
        new_mean = new_values.mean()
        mean_shift = (new_mean - baseline_mean) / baseline_mean * 100

        drift_info = {
            'timestamp': datetime.now().isoformat(),
            'feature': feature,
            'ks_statistic': float(statistic),
            'p_value': float(p_value),
            'drift_detected': bool(is_drift),
            'baseline_mean': float(baseline_mean),
            'new_mean': float(new_mean),
            'mean_shift_pct': float(mean_shift),
            'n_baseline': len(baseline_values),
            'n_new': len(new_values)
        }

        return drift_info

    def monitor_all_features(self, new_data):
        """Monitor drift for all features"""
        results = []
        alerts = []

        print(f"=== Data Drift Monitoring Execution ===")
        print(f"Time: {datetime.now()}")
        print(f"Features monitored: {len(self.alert_features)}")
        print(f"New data samples: {len(new_data)}\n")

        for feature in self.alert_features:
            if feature not in new_data.columns:
                continue

            drift_info = self.detect_drift(new_data, feature)

            if drift_info is None:
                continue

            results.append(drift_info)
            self.drift_history.append(drift_info)

            # Alert on drift detection
            if drift_info['drift_detected']:
                alert_msg = (
                    f"‚ö†Ô∏è  Drift detected: {feature}\n"
                    f"   KS statistic: {drift_info['ks_statistic']:.4f}\n"
                    f"   p-value: {drift_info['p_value']:.4f}\n"
                    f"   Mean shift: {drift_info['mean_shift_pct']:.2f}%"
                )
                alerts.append(alert_msg)
                print(alert_msg + "\n")

        # Summary
        n_drift = sum(r['drift_detected'] for r in results)
        print(f"=== Monitoring Results ===")
        print(f"Drift detected: {n_drift}/{len(results)} features")

        if n_drift &gt; len(results) * 0.3:  # Alert if &gt;30%
            print("‚ö†Ô∏è  Warning: Drift detected in many features")
            print("   Model retraining recommended")

        return results, alerts

    def generate_report(self):
        """Generate drift report"""
        if not self.drift_history:
            return "No drift history available"

        df_history = pd.DataFrame(self.drift_history)

        report = f"""
=== Data Drift Monitoring Report ===

Monitoring period: {df_history['timestamp'].min()} ~ {df_history['timestamp'].max()}
Total monitoring instances: {len(df_history)}
Unique features: {df_history['feature'].nunique()}

Drift detection summary:
{df_history.groupby('feature')['drift_detected'].agg(['sum', 'count']).to_string()}

Top 5 drift detection rates:
{df_history[df_history['drift_detected']].groupby('feature').size().sort_values(ascending=False).head().to_string()}

Top 5 average shift rates (absolute):
{df_history.groupby('feature')['mean_shift_pct'].apply(lambda x: abs(x).mean()).sort_values(ascending=False).head().to_string()}
        """

        return report

    def save_report(self, filepath):
        """Save report as JSON"""
        report_data = {
            'baseline_stats': self.baseline_stats,
            'drift_history': self.drift_history,
            'summary': {
                'total_checks': len(self.drift_history),
                'total_drifts': sum(d['drift_detected'] for d in self.drift_history)
            }
        }

        with open(filepath, 'w') as f:
            json.dump(report_data, f, indent=2)

        print(f"‚úì Report saved: {filepath}")

# Usage example
from sklearn.datasets import make_classification

# Baseline data (training data)
X_baseline, _ = make_classification(
    n_samples=1000, n_features=10, random_state=42
)
df_baseline = pd.DataFrame(
    X_baseline,
    columns=[f'feature_{i}' for i in range(10)]
)

# New data (with drift)
X_new, _ = make_classification(
    n_samples=500, n_features=10, random_state=43
)
# Add shift to some features
X_new[:, 0] += 2.0  # Large shift to feature_0
X_new[:, 3] += 0.5  # Small shift to feature_3

df_new = pd.DataFrame(
    X_new,
    columns=[f'feature_{i}' for i in range(10)]
)

# Execute drift monitoring
monitor = DataDriftMonitor(df_baseline, threshold=0.05)
results, alerts = monitor.monitor_all_features(df_new)

# Generate report
print("\n" + monitor.generate_report())

# Save report
monitor.save_report('drift_report.json')
</code></pre>
<p><strong>Example output</strong>:</p>
<pre><code>=== Data Drift Monitoring Execution ===
Time: 2025-10-21 10:30:45.123456
Features monitored: 10
New data samples: 500

‚ö†Ô∏è  Drift detected: feature_0
   KS statistic: 0.8920
   p-value: 0.0000
   Mean shift: 412.34%

‚ö†Ô∏è  Drift detected: feature_3
   KS statistic: 0.2145
   p-value: 0.0023
   Mean shift: 87.56%

=== Monitoring Results ===
Drift detected: 2/10 features
</code></pre>
</details>
<h3>Problem 4 (Difficulty: hard)</h3>
<p>Implement an automated training pipeline for MLOps maturity Level 1. Include the following features:
</p><ul>
<li>Automatic data acquisition</li>
<li>Preprocessing pipeline</li>
<li>Model training</li>
<li>Performance evaluation and threshold judgment</li>
<li>Recording to MLflow</li>
<li>Register to model registry only when conditions are met</li>
</ul>
<details>
<summary>Sample Answer</summary>
<p>Due to length constraints, please refer to the detailed implementation in the Japanese version. The key implementation points include:</p>
<ul>
<li>AutoMLPipeline class with experiment management</li>
<li>Automated data loading with validation</li>
<li>Preprocessing and model pipeline creation</li>
<li>Cross-validation and threshold-based model registration</li>
<li>Comprehensive MLflow logging</li>
<li>Scheduled training support</li>
</ul>
</details>
<h3>Problem 5 (Difficulty: hard)</h3>
<p>Create a checklist to evaluate MLOps maturity level. Make it possible to determine whether an organization is at Level 0, 1, or 2.</p>
<details>
<summary>Sample Answer</summary>
<p>The MLOpsMaturityAssessment class provides:</p>
<ul>
<li>Criteria for each maturity level (Data Management, Model Development, Deployment, Monitoring)</li>
<li>Interactive assessment with yes/no questions</li>
<li>Scoring system (0-100%) for each level</li>
<li>Level determination based on ‚â•70% threshold</li>
<li>Recommendations for next steps based on current level</li>
<li>Visual progress bars in assessment report</li>
</ul>
</details>
<hr/>
<h2>References</h2>
<ol>
<li>G√©ron, A. (2022). <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em> (3rd ed.). O'Reilly Media.</li>
<li>Kreuzberger, D., K√ºhl, N., &amp; Hirschl, S. (2023). Machine Learning Operations (MLOps): Overview, Definition, and Architecture. <em>IEEE Access</em>, 11, 31866-31879.</li>
<li>Google Cloud. (2023). <em>MLOps: Continuous delivery and automation pipelines in machine learning</em>. https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning</li>
<li>Huyen, C. (2022). <em>Designing Machine Learning Systems</em>. O'Reilly Media.</li>
<li>Treveil, M., et al. (2020). <em>Introducing MLOps</em>. O'Reilly Media.</li>
</ol>
<div class="navigation">
<a class="nav-button" href="index.html">‚Üê Series Contents</a>
<a class="nav-button" href="chapter2-experiment-tracking.html">Next Chapter: Experiment Management and Model Tracking ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty provisions.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-21</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>