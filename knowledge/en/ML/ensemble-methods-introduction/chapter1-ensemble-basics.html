<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="Chapter 1: Fundamentals of Ensemble Learning - AI Terakoya" name="description"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 1: Fundamentals of Ensemble Learning - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/ensemble-methods-introduction/index.html">Ensemble Methods</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/ML/ensemble-methods-introduction/chapter1-ensemble-basics.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 1: Fundamentals of Ensemble Learning</h1>
<p class="subtitle">Improving Prediction Accuracy through Model Combination - Principles of Bagging, Boosting, and Stacking</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 20-25 minutes</span>
<span class="meta-item">üìä Difficulty: Beginner to Intermediate</span>
<span class="meta-item">üíª Code Examples: 8</span>
<span class="meta-item">üìù Practice Problems: 5</span>
</div>
</div>
</header>
<main class="container">

<p class="chapter-description">This chapter covers the fundamentals of Fundamentals of Ensemble Learning, which what is ensemble learning?. You will learn principles of ensemble learning, concept of bias-variance decomposition, and bagging (Random Forest.</p>
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Understand the principles of ensemble learning</li>
<li>‚úÖ Explain the concept of bias-variance decomposition</li>
<li>‚úÖ Implement bagging (Random Forest, Extra Trees)</li>
<li>‚úÖ Understand the mechanisms of boosting (AdaBoost, Gradient Boosting)</li>
<li>‚úÖ Master stacking and meta-learners</li>
<li>‚úÖ Determine when to use each method</li>
</ul>
<hr/>
<h2>1.1 What is Ensemble Learning?</h2>
<h3>Definition</h3>
<p><strong>Ensemble Learning</strong> is a machine learning approach that combines multiple weak learners to construct a more powerful prediction model.</p>
<blockquote>
<p>"Combining multiple models achieves higher performance than a single model"</p>
</blockquote>
<h3>Why Combine Multiple Models?</h3>
<div class="mermaid">
graph LR
    A[Single Model Limitations] --&gt; B[Prone to overfitting]
    A --&gt; C[High bias]
    A --&gt; D[Sensitive to noise]

    E[Ensemble] --&gt; F[Reduce variance]
    E --&gt; G[Reduce bias]
    E --&gt; H[Improve stability]

    style A fill:#ffebee
    style E fill:#e8f5e9
</div>
<h3>Effectiveness of Ensembles</h3>
<p><strong>Example</strong>: When three models each predict independently with 70% accuracy</p>
<p>Accuracy through majority voting:</p>
<p>$$
P(\text{correct}) = P(\text{2 or more correct}) = \binom{3}{2}(0.7)^2(0.3) + \binom{3}{3}(0.7)^3 = 0.784
$$</p>
<p>Achieves higher accuracy (78.4%) than a single model (70%)!</p>
<h3>Bias-Variance Decomposition</h3>
<p>Prediction error can be decomposed as follows:</p>
<p>$$
\text{Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}
$$</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Meaning</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Bias</strong></td>
<td>Error due to model simplification</td>
<td>Complex models, boosting</td>
</tr>
<tr>
<td><strong>Variance</strong></td>
<td>Sensitivity to training data variation</td>
<td>Bagging, averaging</td>
</tr>
<tr>
<td><strong>Irreducible Error</strong></td>
<td>Noise inherent in data</td>
<td>Cannot be reduced</td>
</tr>
</tbody>
</table>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: $$
\text{Error} = \text{Bias}^2 + \text{Variance} + \text{Ir

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 30-60 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor

# Generate data
X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Predict with decision trees of different depths
depths = [1, 3, 10]
plt.figure(figsize=(15, 4))

for i, depth in enumerate(depths, 1):
    model = DecisionTreeRegressor(max_depth=depth, random_state=42)
    model.fit(X_train, y_train)

    X_plot = np.linspace(X.min(), X.max(), 300).reshape(-1, 1)
    y_pred = model.predict(X_plot)

    plt.subplot(1, 3, i)
    plt.scatter(X_train, y_train, alpha=0.5, label='Training data')
    plt.plot(X_plot, y_pred, 'r-', linewidth=2, label=f'Depth={depth}')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title(f'Depth={depth}: {"High Bias" if depth==1 else "High Variance" if depth==10 else "Balanced"}')
    plt.legend()
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<hr/>
<h2>1.2 Bagging</h2>
<h3>Overview</h3>
<p><strong>Bagging (Bootstrap Aggregating)</strong> is a method that trains multiple models using bootstrap sampling of training data, then averages predictions (regression) or uses majority voting (classification).</p>
<h3>Algorithm</h3>
<ol>
<li>Generate $B$ <strong>bootstrap samples</strong> from training data</li>
<li>Train models independently on each sample</li>
<li>Aggregate predictions:
<ul>
<li>Regression: $\hat{y} = \frac{1}{B}\sum_{b=1}^{B} \hat{f}_b(x)$</li>
<li>Classification: Majority voting</li>
</ul></li>
</ol>
<div class="mermaid">
graph TD
    A[Training Data] --&gt; B1[Bootstrap 1]
    A --&gt; B2[Bootstrap 2]
    A --&gt; B3[Bootstrap 3]

    B1 --&gt; M1[Model 1]
    B2 --&gt; M2[Model 2]
    B3 --&gt; M3[Model 3]

    M1 --&gt; AGG[Aggregation]
    M2 --&gt; AGG
    M3 --&gt; AGG

    AGG --&gt; PRED[Final Prediction]

    style A fill:#e3f2fd
    style AGG fill:#fff3e0
    style PRED fill:#e8f5e9
</div>
<h3>Random Forest</h3>
<p><strong>Random Forest</strong> is a method that combines bagging with random feature selection.</p>
<p><strong>Features</strong>:</p>
<ul>
<li>Selects optimal split from a randomly chosen subset of features at each split</li>
<li>Reduces correlation between models and improves diversity</li>
</ul>
<pre><code class="language-python">from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Generate data
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,
                          n_redundant=5, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Single decision tree
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)
dt_acc = accuracy_score(y_test, dt.predict(X_test))

# Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
rf_acc = accuracy_score(y_test, rf.predict(X_test))

print("=== Effect of Bagging ===")
print(f"Decision Tree (single): {dt_acc:.4f}")
print(f"Random Forest: {rf_acc:.4f}")
print(f"Improvement: {(rf_acc - dt_acc):.4f}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Effect of Bagging ===
Decision Tree (single): 0.8600
Random Forest: 0.9250
Improvement: 0.0650
</code></pre>
<h3>Extra Trees</h3>
<p><strong>Extra Trees (Extremely Randomized Trees)</strong> is an even more randomized version of Random Forest.</p>
<p><strong>Differences</strong>:</p>
<ul>
<li>Split thresholds are also chosen randomly</li>
<li>Does not use bootstrap sampling (uses all data)</li>
</ul>
<pre><code class="language-python">from sklearn.ensemble import ExtraTreesClassifier

# Extra Trees
et = ExtraTreesClassifier(n_estimators=100, random_state=42)
et.fit(X_train, y_train)
et_acc = accuracy_score(y_test, et.predict(X_test))

print("=== Extra Trees vs Random Forest ===")
print(f"Random Forest: {rf_acc:.4f}")
print(f"Extra Trees: {et_acc:.4f}")
</code></pre>
<hr/>
<h2>1.3 Boosting</h2>
<h3>Overview</h3>
<p><strong>Boosting</strong> is a method that sequentially trains weak learners, with each subsequent model correcting the errors of previous models.</p>
<div class="mermaid">
graph LR
    A[Data] --&gt; M1[Model 1]
    M1 --&gt; W1[Update Weights]
    W1 --&gt; M2[Model 2]
    M2 --&gt; W2[Update Weights]
    W2 --&gt; M3[Model 3]
    M3 --&gt; F[Weighted Sum]

    style A fill:#e3f2fd
    style F fill:#e8f5e9
</div>
<h3>AdaBoost</h3>
<p><strong>AdaBoost (Adaptive Boosting)</strong> sequentially trains models while increasing the weights of misclassified samples.</p>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Initialize weights for all samples: $w_i = \frac{1}{m}$</li>
<li>For each iteration $t = 1, ..., T$:
<ul>
<li>Train weak learner $h_t$ on weighted data</li>
<li>Error rate: $\epsilon_t = \sum_{i: h_t(x_i) \neq y_i} w_i$</li>
<li>Model weight: $\alpha_t = \frac{1}{2}\ln\frac{1-\epsilon_t}{\epsilon_t}$</li>
<li>Update sample weights</li>
</ul></li>
<li>Final prediction: $H(x) = \text{sign}\left(\sum_{t=1}^{T} \alpha_t h_t(x)\right)$</li>
</ol>
<pre><code class="language-python">from sklearn.ensemble import AdaBoostClassifier

# AdaBoost
ada = AdaBoostClassifier(n_estimators=100, random_state=42)
ada.fit(X_train, y_train)
ada_acc = accuracy_score(y_test, ada.predict(X_test))

print("=== AdaBoost ===")
print(f"Accuracy: {ada_acc:.4f}")

# Accuracy progression with iterations
from sklearn.metrics import accuracy_score

n_trees = [1, 5, 10, 25, 50, 100]
train_scores = []
test_scores = []

for n in n_trees:
    ada_temp = AdaBoostClassifier(n_estimators=n, random_state=42)
    ada_temp.fit(X_train, y_train)
    train_scores.append(ada_temp.score(X_train, y_train))
    test_scores.append(ada_temp.score(X_test, y_test))

plt.figure(figsize=(10, 6))
plt.plot(n_trees, train_scores, 'o-', label='Training data', linewidth=2)
plt.plot(n_trees, test_scores, 's-', label='Test data', linewidth=2)
plt.xlabel('Number of weak learners', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.title('AdaBoost: Relationship between Number of Learners and Accuracy', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>
<h3>Gradient Boosting Fundamentals</h3>
<p><strong>Gradient Boosting</strong> is a method that adds models in the direction of the gradient of the loss function.</p>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Initial prediction: $F_0(x) = \arg\min_{\gamma} \sum_{i=1}^{m} L(y_i, \gamma)$</li>
<li>For each iteration $t = 1, ..., T$:
<ul>
<li>Compute residuals (negative gradient): $r_i = -\frac{\partial L(y_i, F_{t-1}(x_i))}{\partial F_{t-1}(x_i)}$</li>
<li>Train weak learner $h_t$ on residuals</li>
<li>Update model: $F_t(x) = F_{t-1}(x) + \nu \cdot h_t(x)$</li>
</ul></li>
</ol>
<pre><code class="language-python">from sklearn.ensemble import GradientBoostingClassifier

# Gradient Boosting
gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,
                                max_depth=3, random_state=42)
gb.fit(X_train, y_train)
gb_acc = accuracy_score(y_test, gb.predict(X_test))

print("=== Gradient Boosting ===")
print(f"Accuracy: {gb_acc:.4f}")
</code></pre>
<hr/>
<h2>1.4 Stacking</h2>
<h3>Overview</h3>
<p><strong>Stacking</strong> is a method where a meta-learner makes final predictions using the predictions of multiple different models (base models) as input.</p>
<div class="mermaid">
graph TD
    A[Training Data] --&gt; M1[Model 1: Logistic Regression]
    A --&gt; M2[Model 2: Random Forest]
    A --&gt; M3[Model 3: SVM]

    M1 --&gt; P1[Prediction 1]
    M2 --&gt; P2[Prediction 2]
    M3 --&gt; P3[Prediction 3]

    P1 --&gt; META[Meta-learner]
    P2 --&gt; META
    P3 --&gt; META

    META --&gt; FINAL[Final Prediction]

    style A fill:#e3f2fd
    style META fill:#fff3e0
    style FINAL fill:#e8f5e9
</div>
<h3>Meta-learner</h3>
<p>The meta-learner learns using the predictions of base models as features.</p>
<p><strong>Common meta-learners</strong>:</p>
<ul>
<li>Logistic Regression</li>
<li>Ridge Regression</li>
<li>Neural Networks</li>
</ul>
<h3>Cross-Validation Strategy</h3>
<p>To prevent overfitting, <strong>K-Fold cross-validation</strong> is used to generate base model predictions.</p>
<pre><code class="language-python">from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

# Base models
estimators = [
    ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),
    ('et', ExtraTreesClassifier(n_estimators=50, random_state=42)),
    ('ada', AdaBoostClassifier(n_estimators=50, random_state=42))
]

# Stacking
stack = StackingClassifier(
    estimators=estimators,
    final_estimator=LogisticRegression(),
    cv=5
)
stack.fit(X_train, y_train)
stack_acc = accuracy_score(y_test, stack.predict(X_test))

print("=== Stacking ===")
print(f"Random Forest: {rf_acc:.4f}")
print(f"Extra Trees: {et_acc:.4f}")
print(f"AdaBoost: {ada_acc:.4f}")
print(f"Stacking: {stack_acc:.4f}")
</code></pre>
<hr/>
<h2>1.5 Comparison and Selection</h2>
<h3>Performance Comparison</h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>Variance Reduction</th>
<th>Bias Reduction</th>
<th>Parallelization</th>
<th>Training Speed</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Bagging</strong></td>
<td>‚úì</td>
<td>-</td>
<td>Possible</td>
<td>Fast</td>
</tr>
<tr>
<td><strong>Random Forest</strong></td>
<td>‚úì‚úì</td>
<td>-</td>
<td>Possible</td>
<td>Fast</td>
</tr>
<tr>
<td><strong>AdaBoost</strong></td>
<td>-</td>
<td>‚úì</td>
<td>Not possible</td>
<td>Moderate</td>
</tr>
<tr>
<td><strong>Gradient Boosting</strong></td>
<td>-</td>
<td>‚úì‚úì</td>
<td>Not possible</td>
<td>Slow</td>
</tr>
<tr>
<td><strong>Stacking</strong></td>
<td>‚úì</td>
<td>‚úì</td>
<td>Possible</td>
<td>Slow</td>
</tr>
</tbody>
</table>
<h3>Application Scenarios</h3>
<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommended Method</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>High Variance Models</strong></td>
<td>Bagging, Random Forest</td>
<td>Effectively reduces variance</td>
</tr>
<tr>
<td><strong>High Bias Models</strong></td>
<td>Boosting</td>
<td>Learns complex patterns</td>
</tr>
<tr>
<td><strong>Large-scale Data</strong></td>
<td>Random Forest</td>
<td>Parallelizable and fast</td>
</tr>
<tr>
<td><strong>Imbalanced Data</strong></td>
<td>AdaBoost</td>
<td>Focuses on misclassified samples</td>
</tr>
<tr>
<td><strong>Pursuing Best Performance</strong></td>
<td>Stacking, GB</td>
<td>Integrates strengths of multiple methods</td>
</tr>
</tbody>
</table>
<pre><code class="language-python"># Comparison of all methods
results = {
    'Decision Tree': dt_acc,
    'Random Forest': rf_acc,
    'Extra Trees': et_acc,
    'AdaBoost': ada_acc,
    'Gradient Boosting': gb_acc,
    'Stacking': stack_acc
}

plt.figure(figsize=(10, 6))
methods = list(results.keys())
accuracies = list(results.values())

bars = plt.bar(methods, accuracies, color=['#e74c3c', '#3498db', '#2ecc71',
                                           '#f39c12', '#9b59b6', '#1abc9c'])
plt.ylabel('Accuracy', fontsize=12)
plt.title('Performance Comparison of Ensemble Methods', fontsize=14)
plt.xticks(rotation=15, ha='right')
plt.ylim([0.8, 1.0])
plt.grid(axis='y', alpha=0.3)

# Display values on top of bars
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
            f'{height:.4f}', ha='center', va='bottom', fontsize=10)

plt.tight_layout()
plt.show()

print("\n=== Final Results ===")
for method, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):
    print(f"{method:20s}: {acc:.4f}")
</code></pre>
<hr/>
<h2>1.6 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li><p><strong>Principles of Ensemble Learning</strong></p>
<ul>
<li>Improved performance through model combination</li>
<li>Bias-variance decomposition</li>
</ul></li>
<li><p><strong>Bagging</strong></p>
<ul>
<li>Bootstrap sampling and averaging</li>
<li>Random Forest: Improved diversity through random feature selection</li>
<li>Extra Trees: Further randomization</li>
</ul></li>
<li><p><strong>Boosting</strong></p>
<ul>
<li>AdaBoost: Focuses on misclassified samples</li>
<li>Gradient Boosting: Optimization in gradient direction</li>
</ul></li>
<li><p><strong>Stacking</strong></p>
<ul>
<li>Integration through meta-learner</li>
<li>Overfitting prevention through cross-validation</li>
</ul></li>
<li><p><strong>Selection Criteria</strong></p>
<ul>
<li>Bagging: Variance reduction, parallelization</li>
<li>Boosting: Bias reduction, high accuracy</li>
<li>Stacking: Pursuing best performance</li>
</ul></li>
</ol>
<h3>To the Next Chapter</h3>
<p>In Chapter 2, we will learn about <strong>advanced gradient boosting</strong>:</p>
<ul>
<li>XGBoost</li>
<li>LightGBM</li>
<li>CatBoost</li>
<li>Hyperparameter tuning</li>
</ul>
<hr/>
<h2>Practice Problems</h2>
<h3>Problem 1 (Difficulty: easy)</h3>
<p>Explain the difference between bias and variance, and name the ensemble methods that reduce each.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Bias</strong>:</p>
<ul>
<li>Error due to model simplification</li>
<li>High error even on training data</li>
<li>Example: Predicting non-linear data with a linear model</li>
</ul>
<p><strong>Variance</strong>:</p>
<ul>
<li>Sensitivity to training data variation</li>
<li>Good on training data but poor on test data</li>
<li>Example: Overfitting with deep decision trees</li>
</ul>
<p><strong>Reduction Methods</strong>:</p>
<ul>
<li><strong>Variance Reduction</strong>: Bagging, Random Forest (reduces variance through averaging)</li>
<li><strong>Bias Reduction</strong>: Boosting (learns complex patterns sequentially)</li>
</ul>
</details>
<h3>Problem 2 (Difficulty: medium)</h3>
<p>Name two differences between Random Forest and Extra Trees, and explain the characteristics of each.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Difference 1: Sampling</strong></p>
<ul>
<li><strong>Random Forest</strong>: Bootstrap sampling (sampling with replacement)</li>
<li><strong>Extra Trees</strong>: Uses all data (no sampling)</li>
</ul>
<p><strong>Difference 2: Splitting Method</strong></p>
<ul>
<li><strong>Random Forest</strong>: Selects optimal split from random feature subset</li>
<li><strong>Extra Trees</strong>: Randomly selects both features and thresholds</li>
</ul>
<p><strong>Characteristics</strong>:</p>
<ul>
<li><strong>Random Forest</strong>: Variance reduction, takes some time to train</li>
<li><strong>Extra Trees</strong>: Faster, improved diversity through further randomization</li>
</ul>
</details>
<h3>Problem 3 (Difficulty: medium)</h3>
<p>Explain from an algorithmic perspective why the weights of misclassified samples increase in AdaBoost.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Reason</strong>:</p>
<ul>
<li>AdaBoost is designed so that each iteration <strong>focuses on samples the previous model struggled with</strong></li>
<li>By increasing the weights of misclassified samples, the next model tries to classify them correctly</li>
</ul>
<p><strong>Algorithm</strong>:</p>
<pre><code>Weight update for misclassified sample i:
w_i ‚Üê w_i * exp(Œ±_t)

Weight update for correctly classified sample j:
w_j ‚Üê w_j * exp(-Œ±_t)

where Œ±_t = 0.5 * ln((1 - Œµ_t) / Œµ_t) &gt; 0
</code></pre>
<p><strong>Effect</strong>:</p>
<ul>
<li>Weak learners sequentially learn difficult samples</li>
<li>Eventually forms complex decision boundaries</li>
</ul>
</details>
<h3>Problem 4 (Difficulty: hard)</h3>
<p>Explain from an overfitting perspective why K-Fold cross-validation is used in stacking.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Problem</strong>: If base models are trained on the entire training data and predictions are generated on the same data:</p>
<ul>
<li>Meta-learner overfits to training data</li>
<li>Base model predictions are optimized for "previously seen data"</li>
</ul>
<p><strong>K-Fold Cross-Validation Solution</strong>:</p>
<pre><code>1. Split data into K folds
2. For each fold k:
   - Train base models on all folds except fold k
   - Generate predictions for fold k (predictions on unseen data)
3. Combine predictions from all folds to train meta-learner
</code></pre>
<p><strong>Effect</strong>:</p>
<ul>
<li>Input to meta-learner is "predictions on unseen data"</li>
<li>Improved generalization performance</li>
<li>Prevents overfitting</li>
</ul>
<p><strong>Implementation Example</strong>:</p>
<pre><code class="language-python">StackingClassifier(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5  # 5-Fold cross-validation
)
</code></pre>
</details>
<h3>Problem 5 (Difficulty: hard)</h3>
<p>Implement and compare Random Forest and Gradient Boosting using the iris dataset. Report training time and accuracy.</p>
<details>
<summary>Sample Answer</summary>
<pre><code class="language-python">import time
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report

# Load data
iris = load_iris()
X, y = iris.data, iris.target

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Random Forest
print("=== Random Forest ===")
start = time.time()
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
rf_time = time.time() - start

rf_acc = rf.score(X_test, y_test)
cv_rf = cross_val_score(rf, X, y, cv=5).mean()

print(f"Training time: {rf_time:.4f} seconds")
print(f"Test accuracy: {rf_acc:.4f}")
print(f"Cross-validation accuracy: {cv_rf:.4f}")

# Gradient Boosting
print("\n=== Gradient Boosting ===")
start = time.time()
gb = GradientBoostingClassifier(n_estimators=100, random_state=42)
gb.fit(X_train, y_train)
gb_time = time.time() - start

gb_acc = gb.score(X_test, y_test)
cv_gb = cross_val_score(gb, X, y, cv=5).mean()

print(f"Training time: {gb_time:.4f} seconds")
print(f"Test accuracy: {gb_acc:.4f}")
print(f"Cross-validation accuracy: {cv_gb:.4f}")

# Comparison
print("\n=== Comparison ===")
print(f"Accuracy: RF={rf_acc:.4f} vs GB={gb_acc:.4f}")
print(f"Training time: RF={rf_time:.4f}s vs GB={gb_time:.4f}s")
print(f"Speed ratio: GB/RF = {gb_time/rf_time:.2f}x")
</code></pre>
<p><strong>Example Output</strong>:</p>
<pre><code>=== Random Forest ===
Training time: 0.0523 seconds
Test accuracy: 1.0000
Cross-validation accuracy: 0.9533

=== Gradient Boosting ===
Training time: 0.1245 seconds
Test accuracy: 1.0000
Cross-validation accuracy: 0.9467

=== Comparison ===
Accuracy: RF=1.0000 vs GB=1.0000
Training time: RF=0.0523s vs GB=0.1245s
Speed ratio: GB/RF = 2.38x
</code></pre>
<p><strong>Analysis</strong>:</p>
<ul>
<li>Accuracy is nearly equivalent</li>
<li>Random Forest trains faster (parallelizable)</li>
<li>Both methods achieve high accuracy on this small, simple dataset</li>
</ul>
</details>
<hr/>
<h2>References</h2>
<ol>
<li>Breiman, L. (1996). <em>Bagging predictors</em>. Machine Learning, 24(2), 123-140.</li>
<li>Breiman, L. (2001). <em>Random forests</em>. Machine Learning, 45(1), 5-32.</li>
<li>Freund, Y., &amp; Schapire, R. E. (1997). <em>A decision-theoretic generalization of on-line learning and an application to boosting</em>. Journal of Computer and System Sciences, 55(1), 119-139.</li>
<li>Friedman, J. H. (2001). <em>Greedy function approximation: A gradient boosting machine</em>. Annals of Statistics, 1189-1232.</li>
</ol>
<div class="navigation">
<a class="nav-button" href="index.html">‚Üê Back to Table of Contents</a>
<a class="nav-button" href="chapter2-gradient-boosting.html">Next Chapter: Advanced Gradient Boosting ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to warranties of merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content may be modified, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the stated conditions (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-21</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
