<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 3: LightGBM and CatBoost - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/ensemble-methods-introduction/index.html">Ensemble Methods</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 3</span>
</div>
</nav>
<header>
<div class="header-content">
<h1>Chapter 3: LightGBM and CatBoost</h1>
<p class="subtitle">Next-Generation Gradient Boosting - Acceleration and Categorical Variable Handling</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 25-30 minutes</span>
<span class="meta-item">üìä Difficulty: Intermediate</span>
<span class="meta-item">üíª Code Examples: 9</span>
<span class="meta-item">üìù Exercises: 5</span>
</div>
</div>
</header>
<main class="container">
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master the following:</p>
<ul>
<li>‚úÖ Understanding LightGBM's acceleration techniques (GOSS, EFB, Histogram-based)</li>
<li>‚úÖ Implementing LightGBM and performing parameter tuning</li>
<li>‚úÖ Understanding CatBoost's Ordered Boosting and Categorical Features processing</li>
<li>‚úÖ Implementing CatBoost and selecting encoding strategies</li>
<li>‚úÖ Comparing characteristics of XGBoost, LightGBM, and CatBoost to make informed choices</li>
</ul>
<hr/>
<h2>3.1 LightGBM - Acceleration Mechanisms</h2>
<h3>What is LightGBM?</h3>
<p><strong>LightGBM (Light Gradient Boosting Machine)</strong> is a fast and efficient gradient boosting framework developed by Microsoft.</p>
<blockquote>
<p>As its "Light" name suggests, it is lighter and faster than XGBoost, making it suitable for large-scale datasets.</p>
</blockquote>
<h3>Key Technical Innovations</h3>
<h4>1. Histogram-based Algorithm</h4>
<p>Significantly reduces computational complexity by discretizing (binning) continuous values.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Complexity</th>
<th>Memory</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Pre-sorted</strong> (XGBoost)</td>
<td>$O(n \log n)$</td>
<td>High</td>
<td>High</td>
</tr>
<tr>
<td><strong>Histogram-based</strong> (LightGBM)</td>
<td>$O(n \times k)$</td>
<td>Low</td>
<td>Nearly equivalent</td>
</tr>
</tbody>
</table>
<p>$k$: Number of bins (typically 255), $n$: Number of data points</p>
<div class="mermaid">
graph LR
    A[Continuous Value Data] --&gt; B[Histogramming]
    B --&gt; C[Discretize into 255 bins]
    C --&gt; D[Fast Split Search]

    style A fill:#ffebee
    style B fill:#e3f2fd
    style C fill:#f3e5f5
    style D fill:#c8e6c9
</div>
<h4>2. GOSS (Gradient-based One-Side Sampling)</h4>
<p><strong>GOSS</strong> accelerates training by emphasizing data with large gradients and sampling data with small gradients.</p>
<p>Algorithm:</p>
<ol>
<li>Sort data by absolute gradient value</li>
<li>Keep all top $a\%$ (large gradients)</li>
<li>Randomly sample $b\%$ from remaining $(1-a)\%$</li>
<li>Adjust weight of sampled data by $(1-a)/b$</li>
</ol>
<h4>3. EFB (Exclusive Feature Bundling)</h4>
<p><strong>EFB</strong> reduces dimensionality by bundling mutually exclusive features (never non-zero simultaneously).</p>
<p>Example: One-Hot Encoded features</p>
<pre><code>color_red:   [1, 0, 0, 1, 0]
color_blue:  [0, 1, 0, 0, 1]
color_green: [0, 0, 1, 0, 0]
‚Üí Can be merged into a single feature
</code></pre>
<h3>Leaf-wise vs Level-wise Growth Strategy</h3>
<table>
<thead>
<tr>
<th>Strategy</th>
<th>Description</th>
<th>Used By</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Level-wise</strong></td>
<td>Depth-first splitting of all nodes</td>
<td>XGBoost</td>
<td>Balanced trees</td>
<td>Splits even low-gain nodes</td>
</tr>
<tr>
<td><strong>Leaf-wise</strong></td>
<td>Split leaf with maximum gain</td>
<td>LightGBM</td>
<td>Efficient, high accuracy</td>
<td>Prone to overfitting</td>
</tr>
</tbody>
</table>
<div class="mermaid">
graph TD
    A[Level-wise: XGBoost] --&gt; B1[Level 1: Split all]
    B1 --&gt; C1[Level 2: Split all]

    D[Leaf-wise: LightGBM] --&gt; E1[Split only max gain node]
    E1 --&gt; F1[Split next max gain node]

    style A fill:#e3f2fd
    style D fill:#f3e5f5
</div>
<hr/>
<h2>3.2 LightGBM Implementation</h2>
<h3>Basic Usage</h3>
<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score
import lightgbm as lgb

# Generate data
X, y = make_classification(
    n_samples=10000,
    n_features=20,
    n_informative=15,
    n_redundant=5,
    random_state=42
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Build LightGBM model
model = lgb.LGBMClassifier(
    objective='binary',
    n_estimators=100,
    learning_rate=0.1,
    max_depth=7,
    num_leaves=31,
    random_state=42
)

# Train
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_proba)

print("=== LightGBM Basic Implementation ===")
print(f"Accuracy: {accuracy:.4f}")
print(f"AUC: {auc:.4f}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== LightGBM Basic Implementation ===
Accuracy: 0.9350
AUC: 0.9712
</code></pre>
<h3>Important Parameters</h3>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Description</th>
<th>Recommended Values</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>num_leaves</code></td>
<td>Maximum number of leaves in tree</td>
<td>31-255 (default: 31)</td>
</tr>
<tr>
<td><code>max_depth</code></td>
<td>Maximum tree depth (overfitting control)</td>
<td>3-10 (default: -1=unlimited)</td>
</tr>
<tr>
<td><code>learning_rate</code></td>
<td>Learning rate</td>
<td>0.01-0.1</td>
</tr>
<tr>
<td><code>n_estimators</code></td>
<td>Number of trees</td>
<td>100-1000</td>
</tr>
<tr>
<td><code>min_child_samples</code></td>
<td>Minimum samples per leaf</td>
<td>20-100</td>
</tr>
<tr>
<td><code>subsample</code></td>
<td>Data sampling ratio</td>
<td>0.7-1.0</td>
</tr>
<tr>
<td><code>colsample_bytree</code></td>
<td>Feature sampling ratio</td>
<td>0.7-1.0</td>
</tr>
<tr>
<td><code>reg_alpha</code></td>
<td>L1 regularization</td>
<td>0-1</td>
</tr>
<tr>
<td><code>reg_lambda</code></td>
<td>L2 regularization</td>
<td>0-1</td>
</tr>
</tbody>
</table>
<h3>Early Stopping and Validation</h3>
<pre><code class="language-python"># Further split training data
X_tr, X_val, y_tr, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42
)

# Train with early stopping
model_early = lgb.LGBMClassifier(
    objective='binary',
    n_estimators=1000,
    learning_rate=0.05,
    max_depth=7,
    num_leaves=31,
    random_state=42
)

model_early.fit(
    X_tr, y_tr,
    eval_set=[(X_val, y_val)],
    eval_metric='auc',
    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=True)]
)

print(f"\n=== Early Stopping ===")
print(f"Optimal iteration count: {model_early.best_iteration_}")
print(f"Validation AUC: {model_early.best_score_['valid_0']['auc']:.4f}")

# Evaluate on test data
y_pred_early = model_early.predict(X_test)
accuracy_early = accuracy_score(y_test, y_pred_early)
print(f"Test Accuracy: {accuracy_early:.4f}")
</code></pre>
<h3>Feature Importance Visualization</h3>
<pre><code class="language-python">import matplotlib.pyplot as plt

# Get feature importance
feature_importance = model.feature_importances_
feature_names = [f'feature_{i}' for i in range(X.shape[1])]

# Convert to DataFrame
importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

print("\n=== Feature Importance Top 10 ===")
print(importance_df.head(10))

# Visualize
plt.figure(figsize=(10, 8))
lgb.plot_importance(model, max_num_features=15, importance_type='gain')
plt.title('LightGBM Feature Importance (Gain)', fontsize=14)
plt.tight_layout()
plt.show()
</code></pre>
<h3>GPU Support</h3>
<pre><code class="language-python"># GPU usage (CUDA environment required)
model_gpu = lgb.LGBMClassifier(
    objective='binary',
    n_estimators=100,
    learning_rate=0.1,
    device='gpu',  # Use GPU
    gpu_platform_id=0,
    gpu_device_id=0,
    random_state=42
)

# Train (accelerated on GPU)
# model_gpu.fit(X_train, y_train)

print("\n=== GPU Support ===")
print("LightGBM supports GPU (CUDA), enabling 10-30x speedup on large datasets")
print("Enable with device='gpu' parameter")
</code></pre>
<blockquote>
<p><strong>Note</strong>: To use the GPU version, LightGBM must be built with GPU support.</p>
</blockquote>
<hr/>
<h2>3.3 CatBoost - Ordered Boosting and Categorical Variable Handling</h2>
<h3>What is CatBoost?</h3>
<p><strong>CatBoost (Categorical Boosting)</strong> is a gradient boosting framework developed by Yandex, featuring automatic handling of categorical variables.</p>
<h3>Key Technical Innovations</h3>
<h4>1. Ordered Boosting</h4>
<p><strong>Ordered Boosting</strong> is a technique to prevent prediction shift.</p>
<p><strong>Problem</strong>: Traditional boosting calculates gradients and trains on the same data, making overfitting likely.</p>
<p><strong>Solution</strong>:</p>
<ol>
<li>Randomly permute the data</li>
<li>For each sample $i$, use only samples $1, ..., i-1$ for prediction</li>
<li>Build multiple models with different orderings</li>
</ol>
<div class="mermaid">
graph LR
    A[Traditional Boosting] --&gt; B[Train on all data]
    B --&gt; C[Predict on same data]
    C --&gt; D[Prediction shift occurs]

    E[Ordered Boosting] --&gt; F[Train only on past data]
    F --&gt; G[Predict on future data]
    G --&gt; H[Prevent prediction shift]

    style D fill:#ffebee
    style H fill:#c8e6c9
</div>
<h4>2. Automatic Processing of Categorical Features</h4>
<p>CatBoost automatically encodes categorical variables.</p>
<p><strong>Target Statistics</strong> calculation:</p>

$$
\text{TS}(x_i) = \frac{\sum_{j=1}^{i-1} \mathbb{1}_{x_j = x_i} \cdot y_j + a \cdot P}{\sum_{j=1}^{i-1} \mathbb{1}_{x_j = x_i} + a}
$$

<ul>
<li>$x_i$: Category value</li>
<li>$y_j$: Target value</li>
<li>$a$: Smoothing parameter</li>
<li>$P$: Prior probability</li>
</ul>
<p>This approach offers the following advantages:</p>
<ul>
<li>No need for One-Hot Encoding</li>
<li>Handles high cardinality (many categories)</li>
<li>Prevents target leakage</li>
</ul>
<h3>Symmetric Trees (Oblivious Trees)</h3>
<p>CatBoost uses <strong>symmetric trees</strong> (Oblivious Decision Trees).</p>
<table>
<thead>
<tr>
<th>Characteristic</th>
<th>Regular Decision Tree</th>
<th>Symmetric Tree (CatBoost)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Split Condition</td>
<td>Different at each node</td>
<td>Same condition at same level</td>
</tr>
<tr>
<td>Structure</td>
<td>Asymmetric</td>
<td>Perfectly symmetric</td>
</tr>
<tr>
<td>Overfitting</td>
<td>Prone</td>
<td>Resistant</td>
</tr>
<tr>
<td>Prediction Speed</td>
<td>Normal</td>
<td>Very fast</td>
</tr>
</tbody>
</table>
<hr/>
<h2>3.4 CatBoost Implementation</h2>
<h3>Basic Usage</h3>
<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score
from catboost import CatBoostClassifier

# Generate data
X, y = make_classification(
    n_samples=10000,
    n_features=20,
    n_informative=15,
    n_redundant=5,
    random_state=42
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Build CatBoost model
model = CatBoostClassifier(
    iterations=100,
    learning_rate=0.1,
    depth=6,
    loss_function='Logloss',
    random_seed=42,
    verbose=0
)

# Train
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_proba)

print("=== CatBoost Basic Implementation ===")
print(f"Accuracy: {accuracy:.4f}")
print(f"AUC: {auc:.4f}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== CatBoost Basic Implementation ===
Accuracy: 0.9365
AUC: 0.9721
</code></pre>
<h3>Handling Categorical Variables</h3>
<pre><code class="language-python"># Generate dataset with categorical variables
np.random.seed(42)
n = 5000

df = pd.DataFrame({
    'num_feature1': np.random.randn(n),
    'num_feature2': np.random.uniform(0, 100, n),
    'cat_feature1': np.random.choice(['A', 'B', 'C', 'D'], n),
    'cat_feature2': np.random.choice(['Low', 'Medium', 'High'], n),
    'cat_feature3': np.random.choice([f'Cat_{i}' for i in range(50)], n)  # High cardinality
})

# Target variable (depends on categories)
df['target'] = (
    (df['cat_feature1'].isin(['A', 'B'])) &amp;
    (df['num_feature1'] &gt; 0) &amp;
    (df['num_feature2'] &gt; 50)
).astype(int)

# Separate features and target
X = df.drop('target', axis=1)
y = df['target']

# Specify categorical variable columns
cat_features = ['cat_feature1', 'cat_feature2', 'cat_feature3']

# Split train/test data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("=== Data with Categorical Variables ===")
print(f"Data shape: {X.shape}")
print(f"Categorical variables: {cat_features}")
print(f"\nUnique counts for each category:")
for col in cat_features:
    print(f"  {col}: {X[col].nunique()}")

# Train with CatBoost (automatic categorical processing)
model_cat = CatBoostClassifier(
    iterations=100,
    learning_rate=0.1,
    depth=6,
    cat_features=cat_features,  # Specify categorical variables
    random_seed=42,
    verbose=0
)

model_cat.fit(X_train, y_train)

# Evaluate
y_pred_cat = model_cat.predict(X_test)
y_proba_cat = model_cat.predict_proba(X_test)[:, 1]

accuracy_cat = accuracy_score(y_test, y_pred_cat)
auc_cat = roc_auc_score(y_test, y_proba_cat)

print(f"\n=== Categorical Variable Processing Results ===")
print(f"Accuracy: {accuracy_cat:.4f}")
print(f"AUC: {auc_cat:.4f}")
print("‚úì Handles high cardinality without One-Hot Encoding")
</code></pre>
<h3>Encoding Strategies</h3>
<p>CatBoost supports multiple encoding modes:</p>
<table>
<thead>
<tr>
<th>Mode</th>
<th>Description</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Ordered</code></td>
<td>Ordered Target Statistics</td>
<td>Prevent overfitting (default)</td>
</tr>
<tr>
<td><code>GreedyLogSum</code></td>
<td>Greedy log sum</td>
<td>Large-scale data</td>
</tr>
<tr>
<td><code>OneHot</code></td>
<td>One-Hot Encoding</td>
<td>Low cardinality (‚â§10)</td>
</tr>
</tbody>
</table>
<pre><code class="language-python"># Compare encoding strategies
from catboost import Pool

# Create CatBoost Pool (efficient data structure)
train_pool = Pool(
    X_train,
    y_train,
    cat_features=cat_features
)
test_pool = Pool(
    X_test,
    y_test,
    cat_features=cat_features
)

# Different encoding strategies
strategies = {
    'Ordered': 'Ordered',
    'GreedyLogSum': 'GreedyLogSum',
    'OneHot': {'one_hot_max_size': 10}  # One-Hot for cardinality‚â§10
}

print("\n=== Encoding Strategy Comparison ===")
for name, strategy in strategies.items():
    model_strategy = CatBoostClassifier(
        iterations=100,
        learning_rate=0.1,
        depth=6,
        cat_features=cat_features,
        random_seed=42,
        verbose=0
    )

    if name == 'OneHot':
        model_strategy.set_params(**strategy)

    model_strategy.fit(train_pool)
    y_pred = model_strategy.predict(test_pool)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"{name:15s}: Accuracy = {accuracy:.4f}")
</code></pre>
<h3>Early Stopping and Validation</h3>
<pre><code class="language-python"># Further split training data
X_tr, X_val, y_tr, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42
)

# Train with early stopping
model_early = CatBoostClassifier(
    iterations=1000,
    learning_rate=0.05,
    depth=6,
    cat_features=cat_features,
    random_seed=42,
    early_stopping_rounds=50,
    verbose=100
)

model_early.fit(
    X_tr, y_tr,
    eval_set=(X_val, y_val),
    use_best_model=True
)

print(f"\n=== Early Stopping ===")
print(f"Optimal iteration count: {model_early.get_best_iteration()}")
print(f"Best score: {model_early.get_best_score()}")

# Evaluate on test data
y_pred_early = model_early.predict(X_test)
accuracy_early = accuracy_score(y_test, y_pred_early)
print(f"Test Accuracy: {accuracy_early:.4f}")
</code></pre>
<hr/>
<h2>3.5 Comparison of XGBoost, LightGBM, and CatBoost</h2>
<h3>Algorithm Characteristics Comparison</h3>
<table>
<thead>
<tr>
<th>Characteristic</th>
<th>XGBoost</th>
<th>LightGBM</th>
<th>CatBoost</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Developer</strong></td>
<td>Tianqi Chen (DMLC)</td>
<td>Microsoft</td>
<td>Yandex</td>
</tr>
<tr>
<td><strong>Splitting Algorithm</strong></td>
<td>Pre-sorted</td>
<td>Histogram-based</td>
<td>Histogram-based</td>
</tr>
<tr>
<td><strong>Tree Growth Strategy</strong></td>
<td>Level-wise</td>
<td>Leaf-wise</td>
<td>Level-wise (symmetric trees)</td>
</tr>
<tr>
<td><strong>Speed</strong></td>
<td>Normal</td>
<td>Fast</td>
<td>Somewhat slow</td>
</tr>
<tr>
<td><strong>Memory Efficiency</strong></td>
<td>Normal</td>
<td>High efficiency</td>
<td>Normal</td>
</tr>
<tr>
<td><strong>Categorical Processing</strong></td>
<td>Manual encoding required</td>
<td>Manual encoding required</td>
<td>Automatic processing</td>
</tr>
<tr>
<td><strong>Overfitting Resistance</strong></td>
<td>High</td>
<td>Medium (caution with Leaf-wise)</td>
<td>Very high</td>
</tr>
<tr>
<td><strong>GPU Support</strong></td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td><strong>Hyperparameter Tuning</strong></td>
<td>Somewhat complex</td>
<td>Somewhat complex</td>
<td>Simple</td>
</tr>
</tbody>
</table>
<h3>Performance Comparison Experiment</h3>
<pre><code class="language-python">import time
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, roc_auc_score

# Generate large-scale data
X_large, y_large = make_classification(
    n_samples=50000,
    n_features=50,
    n_informative=30,
    n_redundant=10,
    random_state=42
)

X_train_lg, X_test_lg, y_train_lg, y_test_lg = train_test_split(
    X_large, y_large, test_size=0.2, random_state=42
)

# Common parameters
common_params = {
    'n_estimators': 100,
    'max_depth': 6,
    'learning_rate': 0.1,
    'random_state': 42
}

# Define models
models = {
    'XGBoost': XGBClassifier(**common_params, verbosity=0),
    'LightGBM': LGBMClassifier(**common_params, verbose=-1),
    'CatBoost': CatBoostClassifier(
        iterations=100,
        depth=6,
        learning_rate=0.1,
        random_seed=42,
        verbose=0
    )
}

print("=== Performance Comparison (50,000 samples, 50 features) ===\n")
results = []

for name, model in models.items():
    # Measure training time
    start_time = time.time()
    model.fit(X_train_lg, y_train_lg)
    train_time = time.time() - start_time

    # Measure prediction time
    start_time = time.time()
    y_pred = model.predict(X_test_lg)
    y_proba = model.predict_proba(X_test_lg)[:, 1]
    pred_time = time.time() - start_time

    # Evaluate
    accuracy = accuracy_score(y_test_lg, y_pred)
    auc = roc_auc_score(y_test_lg, y_proba)

    results.append({
        'Model': name,
        'Train Time (s)': train_time,
        'Predict Time (s)': pred_time,
        'Accuracy': accuracy,
        'AUC': auc
    })

    print(f"{name}:")
    print(f"  Training time: {train_time:.3f} seconds")
    print(f"  Prediction time: {pred_time:.3f} seconds")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  AUC: {auc:.4f}\n")

# Display results as DataFrame
results_df = pd.DataFrame(results)
print("=== Results Summary ===")
print(results_df.to_string(index=False))
</code></pre>
<h3>Memory Usage Comparison</h3>
<pre><code class="language-python">import sys

print("\n=== Memory Usage Estimation ===")
for name, model in models.items():
    # Model memory size (approximate)
    model_size = sys.getsizeof(model) / (1024 * 1024)  # MB
    print(f"{name:10s}: Approx. {model_size:.2f} MB")

print("\nCharacteristics:")
print("‚Ä¢ LightGBM: Minimum memory through histogramming")
print("‚Ä¢ XGBoost: Medium memory with pre-sorted method")
print("‚Ä¢ CatBoost: Compact with symmetric trees")
</code></pre>
<h3>Usage Guidelines</h3>
<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommended</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Large-scale data (&gt;1M rows)</strong></td>
<td>LightGBM</td>
<td>Fastest, low memory</td>
</tr>
<tr>
<td><strong>Many categorical variables</strong></td>
<td>CatBoost</td>
<td>Automatic processing, high accuracy</td>
</tr>
<tr>
<td><strong>High cardinality</strong></td>
<td>CatBoost</td>
<td>Target Statistics</td>
</tr>
<tr>
<td><strong>Overfitting concerns</strong></td>
<td>CatBoost</td>
<td>Ordered Boosting</td>
</tr>
<tr>
<td><strong>Balanced performance</strong></td>
<td>XGBoost</td>
<td>Stable, proven track record</td>
</tr>
<tr>
<td><strong>Speed priority</strong></td>
<td>LightGBM</td>
<td>Leaf-wise + Histogram</td>
</tr>
<tr>
<td><strong>Accuracy priority</strong></td>
<td>CatBoost</td>
<td>Overfitting resistance</td>
</tr>
<tr>
<td><strong>Limited tuning time</strong></td>
<td>CatBoost</td>
<td>Good default performance</td>
</tr>
<tr>
<td><strong>GPU acceleration</strong></td>
<td>All supported</td>
<td>Choose based on environment</td>
</tr>
</tbody>
</table>
<h3>Practical Selection Flowchart</h3>
<div class="mermaid">
graph TD
    A[Gradient boosting needed] --&gt; B{Many categorical variables?}
    B --&gt;|Yes| C[CatBoost]
    B --&gt;|No| D{Data size?}
    D --&gt;|Large-scale &gt;1M rows| E[LightGBM]
    D --&gt;|Small-medium scale| F{What to prioritize?}
    F --&gt;|Speed| E
    F --&gt;|Accuracy| C
    F --&gt;|Balance| G[XGBoost]

    style C fill:#c8e6c9
    style E fill:#fff9c4
    style G fill:#e1bee7
</div>
<hr/>
<h2>3.6 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li><p><strong>LightGBM Technical Innovations</strong></p>
<ul>
<li>Histogram-based Algorithm: Reduced computational complexity</li>
<li>GOSS: Gradient-based sampling</li>
<li>EFB: Bundling exclusive features</li>
<li>Leaf-wise growth: Efficient tree construction</li>
</ul></li>
<li><p><strong>LightGBM Implementation</strong></p>
<ul>
<li>Fast and efficient training</li>
<li>Further acceleration through GPU support</li>
<li>Flexible tuning with abundant parameters</li>
</ul></li>
<li><p><strong>CatBoost Technical Innovations</strong></p>
<ul>
<li>Ordered Boosting: Prevent prediction shift</li>
<li>Automatic categorical variable processing</li>
<li>Symmetric trees: Overfitting resistance and fast prediction</li>
</ul></li>
<li><p><strong>CatBoost Implementation</strong></p>
<ul>
<li>Direct handling of categorical variables</li>
<li>High cardinality support</li>
<li>High performance with default parameters</li>
</ul></li>
<li><p><strong>Comparison of Three Tools</strong></p>
<ul>
<li>XGBoost: Balance and proven track record</li>
<li>LightGBM: Speed and memory efficiency</li>
<li>CatBoost: Categorical processing and accuracy</li>
</ul></li>
</ol>
<h3>Selection Points</h3>
<table>
<thead>
<tr>
<th>Priority Item</th>
<th>First Choice</th>
<th>Second Choice</th>
</tr>
</thead>
<tbody>
<tr>
<td>Training Speed</td>
<td>LightGBM</td>
<td>XGBoost</td>
</tr>
<tr>
<td>Prediction Accuracy</td>
<td>CatBoost</td>
<td>XGBoost</td>
</tr>
<tr>
<td>Memory Efficiency</td>
<td>LightGBM</td>
<td>CatBoost</td>
</tr>
<tr>
<td>Categorical Processing</td>
<td>CatBoost</td>
<td>-</td>
</tr>
<tr>
<td>Ease of Tuning</td>
<td>CatBoost</td>
<td>XGBoost</td>
</tr>
<tr>
<td>Stability</td>
<td>XGBoost</td>
<td>CatBoost</td>
</tr>
</tbody>
</table>
<h3>Next Steps</h3>
<ul>
<li>Automatic hyperparameter tuning (Optuna, Hyperopt)</li>
<li>Combining ensemble methods (stacking, blending)</li>
<li>Integration with feature engineering</li>
<li>Improving model interpretability (SHAP, LIME)</li>
</ul>
<hr/>
<h2>Exercises</h2>
<h3>Problem 1 (Difficulty: easy)</h3>
<p>Explain each of the three main acceleration techniques in LightGBM (Histogram-based, GOSS, EFB).</p>
<details>
<summary>Solution</summary>
<p><strong>Answer</strong>:</p>
<ol>
<li><p><strong>Histogram-based Algorithm</strong></p>
<ul>
<li>Description: Discretizes continuous values into a fixed number of bins (typically 255)</li>
<li>Effect: Reduces complexity from $O(n \log n)$ to $O(n \times k)$</li>
<li>Advantages: Improved memory efficiency, faster split search</li>
</ul></li>
<li><p><strong>GOSS (Gradient-based One-Side Sampling)</strong></p>
<ul>
<li>Description: Prioritizes data with large gradients</li>
<li>Procedure: Keep all top $a\%$ gradients + sample $b\%$ from remainder</li>
<li>Advantages: Acceleration through data reduction, maintains accuracy</li>
</ul></li>
<li><p><strong>EFB (Exclusive Feature Bundling)</strong></p>
<ul>
<li>Description: Bundles exclusive features (never non-zero simultaneously)</li>
<li>Example: Merge One-Hot Encoded variables into one</li>
<li>Advantages: Acceleration through feature reduction</li>
</ul></li>
</ol>
</details>
<h3>Problem 2 (Difficulty: medium)</h3>
<p>Explain the differences between Level-wise (XGBoost) and Leaf-wise (LightGBM) tree growth strategies, and describe their respective advantages and disadvantages.</p>
<details>
<summary>Solution</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Level-wise</strong>:</p>
<ul>
<li>Strategy: Depth-first, split all nodes at same level</li>
<li>Advantages: Balanced trees, less prone to overfitting</li>
<li>Disadvantages: Inefficient as it splits even low-gain nodes</li>
<li>Used By: XGBoost, CatBoost</li>
</ul>
<p><strong>Leaf-wise</strong>:</p>
<ul>
<li>Strategy: Split only the leaf with maximum gain</li>
<li>Advantages: Efficient, high accuracy, fast</li>
<li>Disadvantages: Prone to overfitting (depth limit important)</li>
<li>Used By: LightGBM</li>
</ul>
<p><strong>Comparison Table</strong>:</p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Level-wise</th>
<th>Leaf-wise</th>
</tr>
</thead>
<tbody>
<tr>
<td>Efficiency</td>
<td>Normal</td>
<td>High</td>
</tr>
<tr>
<td>Accuracy</td>
<td>Stable</td>
<td>High but watch overfitting</td>
</tr>
<tr>
<td>Tree Shape</td>
<td>Symmetric</td>
<td>Asymmetric</td>
</tr>
<tr>
<td>Overfitting Resistance</td>
<td>High</td>
<td>Medium (depth limit needed)</td>
</tr>
</tbody>
</table>
</details>
<h3>Problem 3 (Difficulty: medium)</h3>
<p>Explain why CatBoost's Ordered Boosting can prevent prediction shift.</p>
<details>
<summary>Solution</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Prediction Shift Problem</strong>:</p>
<p>Traditional boosting has the following issues:</p>
<ol>
<li>Calculate gradients on all data</li>
<li>Train next weak learner on same data</li>
<li>Overfit to training data (seeing same data for prediction and training)</li>
<li>Performance degradation on test data</li>
</ol>
<p><strong>Ordered Boosting Solution</strong>:</p>
<ol>
<li><strong>Data Ordering</strong>: Randomly permute the data</li>
<li><strong>Use Only Past Data</strong>: For sample $i$, use only samples $1, ..., i-1$</li>
<li><strong>Validate on Future Data</strong>: Don't predict on data used for training</li>
<li><strong>Multiple Models</strong>: Build multiple models with different orderings and average</li>
</ol>
<p><strong>Effects</strong>:</p>
<ul>
<li>Same conditions for training and testing (predict only from past data)</li>
<li>Prevention of prediction shift</li>
<li>Improved generalization performance</li>
<li>Overfitting suppression</li>
</ul>
<p><strong>Formula</strong>:</p>
<p>Prediction $\hat{y}_i$ for sample $i$ is:</p>
$$
\hat{y}_i = M(\{(x_j, y_j)\}_{j=1}^{i-1})
$$
<p>That is, use model $M$ trained only on data before $i$.</p>
</details>
<h3>Problem 4 (Difficulty: hard)</h3>
<p>Train LightGBM and CatBoost on the following data and compare their performance. Pay attention to differences in categorical variable processing methods.</p>
<pre><code class="language-python">import numpy as np
import pandas as pd

np.random.seed(42)
n = 10000

df = pd.DataFrame({
    'num1': np.random.randn(n),
    'num2': np.random.uniform(0, 100, n),
    'cat1': np.random.choice(['A', 'B', 'C', 'D', 'E'], n),
    'cat2': np.random.choice([f'Cat_{i}' for i in range(100)], n),  # High cardinality
    'target': np.random.choice([0, 1], n)
})
</code></pre>
<details>
<summary>Solution</summary>
<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, roc_auc_score
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

# Generate data
np.random.seed(42)
n = 10000

df = pd.DataFrame({
    'num1': np.random.randn(n),
    'num2': np.random.uniform(0, 100, n),
    'cat1': np.random.choice(['A', 'B', 'C', 'D', 'E'], n),
    'cat2': np.random.choice([f'Cat_{i}' for i in range(100)], n),
})

# Generate target (depends on categories)
df['target'] = (
    (df['cat1'].isin(['A', 'B'])) &amp;
    (df['num1'] &gt; 0)
).astype(int)

X = df.drop('target', axis=1)
y = df['target']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("=== Data Information ===")
print(f"Sample count: {n}")
print(f"Categorical variables:")
print(f"  cat1: {X['cat1'].nunique()} unique values")
print(f"  cat2: {X['cat2'].nunique()} unique values (high cardinality)")

# ===== LightGBM: Requires Label Encoding =====
print("\n=== LightGBM (Using Label Encoding) ===")

X_train_lgb = X_train.copy()
X_test_lgb = X_test.copy()

# Label Encoding
le_cat1 = LabelEncoder()
le_cat2 = LabelEncoder()

X_train_lgb['cat1'] = le_cat1.fit_transform(X_train_lgb['cat1'])
X_test_lgb['cat1'] = le_cat1.transform(X_test_lgb['cat1'])

X_train_lgb['cat2'] = le_cat2.fit_transform(X_train_lgb['cat2'])
# Handle unknown categories in test data
X_test_lgb['cat2'] = X_test_lgb['cat2'].map(
    {v: k for k, v in enumerate(le_cat2.classes_)}
).fillna(-1).astype(int)

model_lgb = LGBMClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=6,
    random_state=42,
    verbose=-1
)

model_lgb.fit(X_train_lgb, y_train)
y_pred_lgb = model_lgb.predict(X_test_lgb)
y_proba_lgb = model_lgb.predict_proba(X_test_lgb)[:, 1]

acc_lgb = accuracy_score(y_test, y_pred_lgb)
auc_lgb = roc_auc_score(y_test, y_proba_lgb)

print(f"Accuracy: {acc_lgb:.4f}")
print(f"AUC: {auc_lgb:.4f}")
print("Processing: Numerization via Label Encoding (no ordinal information)")

# ===== CatBoost: Can handle categorical variables directly =====
print("\n=== CatBoost (Automatic Categorical Processing) ===")

cat_features = ['cat1', 'cat2']

model_cat = CatBoostClassifier(
    iterations=100,
    learning_rate=0.1,
    depth=6,
    cat_features=cat_features,
    random_seed=42,
    verbose=0
)

model_cat.fit(X_train, y_train)
y_pred_cat = model_cat.predict(X_test)
y_proba_cat = model_cat.predict_proba(X_test)[:, 1]

acc_cat = accuracy_score(y_test, y_pred_cat)
auc_cat = roc_auc_score(y_test, y_proba_cat)

print(f"Accuracy: {acc_cat:.4f}")
print(f"AUC: {auc_cat:.4f}")
print("Processing: Automatic encoding via Target Statistics")

# ===== Comparison =====
print("\n=== Comparison Results ===")
comparison = pd.DataFrame({
    'Model': ['LightGBM', 'CatBoost'],
    'Accuracy': [acc_lgb, acc_cat],
    'AUC': [auc_lgb, auc_cat],
    'Categorical Handling': ['Manual (Label Encoding)', 'Automatic (Target Statistics)']
})
print(comparison.to_string(index=False))

print("\n=== Discussion ===")
print("‚Ä¢ LightGBM: Label Encoding with no ordinal information (suboptimal)")
print("‚Ä¢ CatBoost: Meaningful encoding via Target Statistics")
print("‚Ä¢ CatBoost advantageous for high cardinality")
print("‚Ä¢ One-Hot Encoding impractical due to dimension explosion (100 categories)")
</code></pre>
</details>
<h3>Problem 5 (Difficulty: hard)</h3>
<p>For each of the following situations, choose the most optimal among XGBoost, LightGBM, and CatBoost, and explain your reasoning:</p>
<ol>
<li>Dataset with 100 million rows and 100 features</li>
<li>5 high-cardinality variables with 100 categories each</li>
<li>Small dataset (10,000 rows) where you want to maximize accuracy</li>
</ol>
<details>
<summary>Solution</summary>
<p><strong>Answer</strong>:</p>
<p><strong>1. Dataset with 100 million rows and 100 features</strong></p>
<ul>
<li><strong>Recommended</strong>: LightGBM</li>
<li><strong>Reason</strong>:
<ul>
<li>Fastest with Histogram-based Algorithm</li>
<li>Further acceleration through GOSS data sampling</li>
<li>Best memory efficiency (essential for large-scale data)</li>
<li>Can be further accelerated with GPU support</li>
</ul></li>
<li><strong>Alternative</strong>: XGBoost (GPU mode) is an option but slower than LightGBM</li>
</ul>
<p><strong>2. 5 high-cardinality variables with 100 categories each</strong></p>
<ul>
<li><strong>Recommended</strong>: CatBoost</li>
<li><strong>Reason</strong>:
<ul>
<li>Automatically processes categorical variables (Target Statistics)</li>
<li>No need for One-Hot Encoding (avoids 100 categories√ó5 = 500 dimension explosion)</li>
<li>Prevents target leakage with Ordered Boosting</li>
<li>Design optimized for high cardinality</li>
</ul></li>
<li><strong>Issues with Other Options</strong>:
<ul>
<li>XGBoost: Label Encoding has meaningless ordinal information, One-Hot causes dimension explosion</li>
<li>LightGBM: Same issues</li>
</ul></li>
</ul>
<p><strong>3. Small dataset (10,000 rows) to maximize accuracy</strong></p>
<ul>
<li><strong>Recommended</strong>: CatBoost</li>
<li><strong>Reason</strong>:
<ul>
<li>Ordered Boosting prevents overfitting and provides high generalization performance</li>
<li>Proven accuracy on small datasets</li>
<li>Excellent default parameters (reduces tuning time)</li>
<li>Stable learning with symmetric trees</li>
<li>Speed is not an issue (small data)</li>
</ul></li>
<li><strong>Alternative</strong>: XGBoost (good balance and stability)</li>
<li><strong>LightGBM Issue</strong>: Leaf-wise strategy prone to overfitting on small datasets</li>
</ul>
<p><strong>Summary Table</strong>:</p>
<table>
<thead>
<tr>
<th>Situation</th>
<th>First Choice</th>
<th>Second Choice</th>
<th>Key Factor</th>
</tr>
</thead>
<tbody>
<tr>
<td>Large-scale data</td>
<td>LightGBM</td>
<td>XGBoost (GPU)</td>
<td>Speed, memory</td>
</tr>
<tr>
<td>High cardinality</td>
<td>CatBoost</td>
<td>-</td>
<td>Automatic categorical processing</td>
</tr>
<tr>
<td>Small-scale, high accuracy</td>
<td>CatBoost</td>
<td>XGBoost</td>
<td>Overfitting resistance</td>
</tr>
</tbody>
</table>
</details>
<hr/>
<h2>References</h2>
<ol>
<li>Ke, G., et al. (2017). "LightGBM: A Highly Efficient Gradient Boosting Decision Tree." <em>Advances in Neural Information Processing Systems</em> 30.</li>
<li>Prokhorenkova, L., et al. (2018). "CatBoost: unbiased boosting with categorical features." <em>Advances in Neural Information Processing Systems</em> 31.</li>
<li>Chen, T., &amp; Guestrin, C. (2016). "XGBoost: A Scalable Tree Boosting System." <em>Proceedings of the 22nd ACM SIGKDD</em>.</li>
<li>Microsoft LightGBM Documentation: <a href="https://lightgbm.readthedocs.io/">https://lightgbm.readthedocs.io/</a></li>
<li>Yandex CatBoost Documentation: <a href="https://catboost.ai/docs/">https://catboost.ai/docs/</a></li>
</ol>
<div class="navigation">
<a class="nav-button" href="index.html">‚Üê Series Index</a>
<a class="nav-button" href="chapter4-advanced-techniques.html">Next Chapter: Advanced Ensemble Techniques ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided for educational, research, and informational purposes only, and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the specified terms (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-21</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
