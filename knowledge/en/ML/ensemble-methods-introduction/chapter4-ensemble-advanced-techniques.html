<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Chapter 4: Advanced Ensemble Techniques | Ensemble Methods</title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>MathJax={tex:{inlineMath:[['$','$']],displayMath:[['$$','$$']]}}</script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet"/>
</head>
<body><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<span class="locale-link disabled">Êó•Êú¨Ë™û (Ê∫ñÂÇô‰∏≠)</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header><h1>Chapter 4: Advanced Ensemble Techniques</h1><p class="subtitle">Stacking, Blending, and Voting Ensembles</p></header>
<div class="container">
<div class="breadcrumb"><a href="../index.html">ML Dojo</a> &gt; <a href="index.html">Ensemble Methods</a> &gt; Ch4</div>
<div class="content">
<h2>4.1 Stacking (Stacked Generalization)</h2>
<p>Stacking combines multiple models using meta-learner trained on base model predictions.</p>
<div class="definition"><strong>üìê Stacking Process:</strong>
Level 0: Base models $\{f_1, f_2, ..., f_n\}$
Level 1: Meta-model $g$ learns from base predictions
$$\hat{y} = g(f_1(x), f_2(x), ..., f_n(x))$$</div>
<h3>üíª Code Example 1: Stacking Implementation</h3>
<div class="code-example"><pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_predict, train_test_split
from sklearn.metrics import accuracy_score

class StackingEnsemble:
    &quot;&quot;&quot;Stacking ensemble implementation&quot;&quot;&quot;
    
    def __init__(self, base_models, meta_model, cv=5):
        self.base_models = base_models
        self.meta_model = meta_model
        self.cv = cv
    
    def fit(self, X_train, y_train):
        &quot;&quot;&quot;Train stacking ensemble&quot;&quot;&quot;
        # Generate out-of-fold predictions for training meta-model
        meta_features = np.zeros((X_train.shape[0], len(self.base_models)))
        
        for i, model in enumerate(self.base_models):
            # Cross-validated predictions
            predictions = cross_val_predict(
                model, X_train, y_train,
                cv=self.cv, method='predict_proba'
            )
            meta_features[:, i] = predictions[:, 1]  # Probability of positive class
            
            # Train on full training set
            model.fit(X_train, y_train)
        
        # Train meta-model
        self.meta_model.fit(meta_features, y_train)
        return self
    
    def predict(self, X_test):
        &quot;&quot;&quot;Make predictions using stacking ensemble&quot;&quot;&quot;
        # Get base model predictions
        meta_features = np.zeros((X_test.shape[0], len(self.base_models)))
        
        for i, model in enumerate(self.base_models):
            predictions = model.predict_proba(X_test)
            meta_features[:, i] = predictions[:, 1]
        
        # Meta-model prediction
        return self.meta_model.predict(meta_features)

# Example usage
from sklearn.datasets import load_breast_cancer

X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define base models
base_models = [
    RandomForestClassifier(n_estimators=100, random_state=42),
    GradientBoostingClassifier(n_estimators=100, random_state=42),
    SVC(probability=True, random_state=42)
]

# Define meta-model
meta_model = LogisticRegression()

# Train stacking ensemble
stacking = StackingEnsemble(base_models, meta_model, cv=5)
stacking.fit(X_train, y_train)

# Evaluate
y_pred = stacking.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f&quot;Stacking Ensemble Accuracy: {accuracy:.4f}&quot;)</code></pre></div>
<h2>4.2-4.7 More Advanced Topics</h2>
<p>Blending, voting classifiers, model diversity, hyperparameter optimization, AutoML ensembles.</p>
<h3>üíª Code Examples 2-7</h3>
<div class="code-example"><pre><code class="language-python"># Blending implementation
# Soft and hard voting
# Measuring model diversity
# Multi-level stacking
# AutoML ensemble strategies
# Production deployment
# See full implementations in complete chapter</code></pre></div>
<h2>üìù Exercises</h2>
<div class="exercise"><ol>
<li>Implement 2-level stacking with diverse base models.</li>
<li>Compare stacking vs blending on same dataset.</li>
<li>Create voting ensemble and analyze soft vs hard voting.</li>
<li>Measure correlation between base model predictions.</li>
<li>Build AutoML-style ensemble with automated model selection.</li>
</ol></div>
<h2>Summary</h2>
<ul>
<li>Stacking: meta-model learns from base model predictions</li>
<li>Blending: simpler alternative to stacking with hold-out set</li>
<li>Voting: majority vote (hard) or average probabilities (soft)</li>
<li>Model diversity crucial for ensemble performance</li>
<li>Advanced ensembles often win Kaggle competitions</li>
<li>Trade-off: performance vs complexity and interpretability</li>
</ul>
<div class="nav-buttons">
<a class="nav-button" href="chapter3-lightgbm-catboost.html">‚Üê Ch3: LightGBM/CatBoost</a>
<a class="nav-button" href="index.html">Overview ‚Üí</a>
</div>
</div>
</div>
<footer><p>¬© 2025 AI Terakoya - ML Dojo</p></footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</body>
</html>
