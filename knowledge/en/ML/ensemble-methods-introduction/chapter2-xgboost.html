<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Chapter 2: XGBoost Deep Dive | Ensemble Methods</title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>MathJax={tex:{inlineMath:[['$','$']],displayMath:[['$$','$$']]}}</script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet"/>
</head>
<body><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/ML/ensemble-methods-introduction/chapter2-xgboost.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>

<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header><h1>Chapter 2: XGBoost Deep Dive</h1><p class="subtitle">Gradient Boosting with XGBoost</p></header>
<div class="container">
<div class="breadcrumb"><a href="../index.html">ML Dojo</a> &gt; <a href="index.html">Ensemble Methods</a> &gt; Ch2</div>
<div class="content">
<h2>2.1 XGBoost Fundamentals</h2>
<p>XGBoost (Extreme Gradient Boosting) is optimized implementation of gradient boosting with regularization.</p>
<div class="definition"><strong>üìê XGBoost Objective:</strong>
$$\mathcal{L} = \sum_{i=1}^n l(y_i, \hat{y}_i) + \sum_{k=1}^K \Omega(f_k)$$
where $\Omega(f) = \gamma T + \frac{1}{2}\lambda\|\omega\|^2$ is regularization term</div>
<h3>üíª Code Example 1: XGBoost Implementation</h3>
<div class="code-example"><pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - xgboost&gt;=2.0.0

import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, mean_squared_error

class XGBoostModel:
    &quot;&quot;&quot;XGBoost wrapper for classification and regression&quot;&quot;&quot;
    
    def __init__(self, task='classification'):
        self.task = task
        self.model = None
    
    def train(self, X_train, y_train, params=None):
        &quot;&quot;&quot;Train XGBoost model&quot;&quot;&quot;
        if params is None:
            params = self.get_default_params()
        
        if self.task == 'classification':
            self.model = xgb.XGBClassifier(**params)
        else:
            self.model = xgb.XGBRegressor(**params)
        
        self.model.fit(
            X_train, y_train,
            eval_set=[(X_train, y_train)],
            verbose=False
        )
        return self
    
    def get_default_params(self):
        &quot;&quot;&quot;Default XGBoost parameters&quot;&quot;&quot;
        return {
            'max_depth': 6,
            'learning_rate': 0.1,
            'n_estimators': 100,
            'min_child_weight': 1,
            'gamma': 0,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'reg_alpha': 0,
            'reg_lambda': 1,
            'random_state': 42
        }
    
    def tune_hyperparameters(self, X_train, y_train):
        &quot;&quot;&quot;Hyperparameter tuning with GridSearchCV&quot;&quot;&quot;
        param_grid = {
            'max_depth': [3, 5, 7],
            'learning_rate': [0.01, 0.1, 0.3],
            'n_estimators': [50, 100, 200],
            'min_child_weight': [1, 3, 5],
            'gamma': [0, 0.1, 0.2],
            'subsample': [0.6, 0.8, 1.0],
            'colsample_bytree': [0.6, 0.8, 1.0]
        }
        
        base_model = xgb.XGBClassifier() if self.task == 'classification' else xgb.XGBRegressor()
        grid_search = GridSearchCV(
            base_model,
            param_grid,
            cv=5,
            scoring='accuracy' if self.task == 'classification' else 'neg_mean_squared_error',
            n_jobs=-1,
            verbose=1
        )
        
        grid_search.fit(X_train, y_train)
        self.model = grid_search.best_estimator_
        
        return grid_search.best_params_, grid_search.best_score_

# Example usage
from sklearn.datasets import load_breast_cancer, load_diabetes

# Classification example
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

xgb_clf = XGBoostModel(task='classification')
xgb_clf.train(X_train, y_train)

y_pred = xgb_clf.model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f&quot;XGBoost Classification Accuracy: {accuracy:.4f}&quot;)

# Hyperparameter tuning
best_params, best_score = xgb_clf.tune_hyperparameters(X_train, y_train)
print(f&quot;Best Parameters: {best_params}&quot;)
print(f&quot;Best CV Score: {best_score:.4f}&quot;)</code></pre></div>
<h2>2.2-2.7 Advanced Topics</h2>
<p>Feature importance, early stopping, handling imbalanced data, custom objectives, distributed training.</p>
<h3>üíª Code Examples 2-7</h3>
<div class="code-example"><pre><code class="language-python"># Feature importance analysis
# Early stopping implementation
# Handling class imbalance
# Custom loss functions
# Model interpretation with SHAP
# Production deployment strategies
# See complete code in full chapter</code></pre></div>
<h2>üìù Exercises</h2>
<div class="exercise"><ol>
<li>Train XGBoost classifier and analyze feature importances.</li>
<li>Implement early stopping with validation set monitoring.</li>
<li>Handle imbalanced dataset using scale_pos_weight parameter.</li>
<li>Compare XGBoost vs LightGBM vs CatBoost on benchmark dataset.</li>
<li>Tune hyperparameters using Bayesian optimization (Optuna).</li>
</ol></div>
<h2>Summary</h2>
<ul>
<li>XGBoost: optimized gradient boosting with regularization</li>
<li>Key parameters: max_depth, learning_rate, n_estimators, subsample</li>
<li>Built-in cross-validation and early stopping</li>
<li>Handles missing values automatically</li>
<li>Feature importance for interpretability</li>
<li>State-of-the-art performance on structured data</li>
</ul>
<div class="nav-buttons">
<a class="nav-button" href="chapter1-ensemble-basics.html">‚Üê Ch1: Basics</a>
<a class="nav-button" href="chapter3-lightgbm-catboost.html">Ch3: LightGBM/CatBoost ‚Üí</a>
</div>
</div>
</div>
<footer><p>¬© 2025 AI Terakoya - ML Dojo</p></footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</body>
</html>
