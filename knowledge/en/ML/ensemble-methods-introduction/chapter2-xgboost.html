<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: XGBoost - AI Terakoya</title>

        <link rel="stylesheet" href="../../assets/css/knowledge-base.css">

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/ensemble-methods-introduction/index.html">Ensemble Methods</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 2</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 2: XGBoost</h1>
            <p class="subtitle">Optimized Implementation of Gradient Boosting - Fast and Accurate Predictive Models</p>
            <div class="meta">
                <span class="meta-item">üìñ Reading Time: 25-30 minutes</span>
                <span class="meta-item">üìä Difficulty: Intermediate</span>
                <span class="meta-item">üíª Code Examples: 9</span>
                <span class="meta-item">üìù Practice Problems: 5</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master the following:</p>
<ul>
<li>‚úÖ Understand the principles of XGBoost and regularization techniques</li>
<li>‚úÖ Master the roles and adjustment methods of hyperparameters</li>
<li>‚úÖ Execute efficient training with the xgboost library</li>
<li>‚úÖ Interpret feature importance and improve models</li>
<li>‚úÖ Practice GPU acceleration and parameter tuning strategies</li>
</ul>

<hr>

<h2>2.1 Principles of XGBoost</h2>

<h3>What is XGBoost</h3>
<p><strong>XGBoost (eXtreme Gradient Boosting)</strong> is a fast and efficient implementation of gradient boosting decision trees.</p>

<blockquote>
<p>"XGBoost has been used as a winning model in many Kaggle competitions."</p>
</blockquote>

<h3>Key Features</h3>

<table>
<thead>
<tr>
<th>Feature</th>
<th>Description</th>
<th>Advantage</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Regularization</strong></td>
<td>Overfitting prevention through L1/L2 regularization</td>
<td>Improved generalization performance</td>
</tr>
<tr>
<td><strong>Tree Pruning</strong></td>
<td>Pruning after max_depth</td>
<td>Efficient learning</td>
</tr>
<tr>
<td><strong>Parallel Processing</strong></td>
<td>Parallelization by columns (features)</td>
<td>Fast training</td>
</tr>
<tr>
<td><strong>Missing Value Handling</strong></td>
<td>Automatically learns optimal direction</td>
<td>No preprocessing required</td>
</tr>
</tbody>
</table>

<h3>XGBoost Objective Function</h3>

<p>XGBoost minimizes the following objective function:</p>

<p>$$
\mathcal{L} = \sum_{i=1}^{n} l(y_i, \hat{y}_i) + \sum_{k=1}^{K} \Omega(f_k)
$$</p>

<ul>
<li>$l$: Loss function (squared error, log loss, etc.)</li>
<li>$\Omega(f_k)$: Regularization term (tree complexity penalty)</li>
</ul>

<p>Regularization term:</p>

<p>$$
\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T} w_j^2
$$</p>

<ul>
<li>$T$: Number of leaves</li>
<li>$w_j$: Leaf weights</li>
<li>$\gamma$: Leaf count penalty</li>
<li>$\lambda$: L2 regularization coefficient</li>
</ul>

<h3>Basic Implementation</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Generate sample data
X, y = make_classification(
    n_samples=1000,
    n_features=20,
    n_informative=15,
    n_redundant=5,
    random_state=42
)

# Train-test data split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Build XGBoost model
model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=3,
    learning_rate=0.1,
    random_state=42
)

# Train
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
print("=== XGBoost Basic Performance ===")
print(f"Accuracy: {accuracy:.3f}")
print(f"\nDetailed Report:")
print(classification_report(y_test, y_pred))
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== XGBoost Basic Performance ===
Accuracy: 0.935

Detailed Report:
              precision    recall  f1-score   support

           0       0.94      0.93      0.93       102
           1       0.93      0.94      0.93        98

    accuracy                           0.93       200
   macro avg       0.93      0.93      0.93       200
weighted avg       0.93      0.93      0.93       200
</code></pre>

<hr>

<h2>2.2 Hyperparameters</h2>

<h3>Main Hyperparameters</h3>

<table>
<thead>
<tr>
<th>Parameter</th>
<th>Description</th>
<th>Recommended Range</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>learning_rate (eta)</strong></td>
<td>Learning rate, shrinks contribution of each tree</td>
<td>0.01 - 0.3</td>
<td>0.3</td>
</tr>
<tr>
<td><strong>max_depth</strong></td>
<td>Maximum depth of trees</td>
<td>3 - 10</td>
<td>6</td>
</tr>
<tr>
<td><strong>n_estimators</strong></td>
<td>Number of trees</td>
<td>100 - 1000</td>
<td>100</td>
</tr>
<tr>
<td><strong>subsample</strong></td>
<td>Fraction of rows used per tree</td>
<td>0.5 - 1.0</td>
<td>1.0</td>
</tr>
<tr>
<td><strong>colsample_bytree</strong></td>
<td>Fraction of columns used per tree</td>
<td>0.5 - 1.0</td>
<td>1.0</td>
</tr>
<tr>
<td><strong>gamma</strong></td>
<td>Minimum loss reduction for split</td>
<td>0 - 5</td>
<td>0</td>
</tr>
<tr>
<td><strong>reg_alpha</strong></td>
<td>L1 regularization (absolute value of weights)</td>
<td>0 - 1</td>
<td>0</td>
</tr>
<tr>
<td><strong>reg_lambda</strong></td>
<td>L2 regularization (squared weights)</td>
<td>0 - 1</td>
<td>1</td>
</tr>
</tbody>
</table>

<h3>Visualizing Parameter Effects</h3>

<pre><code class="language-python">import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score

# Data preparation
X, y = make_classification(
    n_samples=1000,
    n_features=20,
    n_informative=15,
    random_state=42
)

# Effect of learning_rate
learning_rates = [0.01, 0.05, 0.1, 0.2, 0.3]
lr_scores = []

for lr in learning_rates:
    model = xgb.XGBClassifier(
        learning_rate=lr,
        n_estimators=100,
        max_depth=3,
        random_state=42
    )
    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
    lr_scores.append(scores.mean())

# Effect of max_depth
max_depths = [2, 3, 4, 5, 6, 8, 10]
depth_scores = []

for depth in max_depths:
    model = xgb.XGBClassifier(
        max_depth=depth,
        n_estimators=100,
        learning_rate=0.1,
        random_state=42
    )
    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
    depth_scores.append(scores.mean())

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].plot(learning_rates, lr_scores, marker='o', linewidth=2)
axes[0].set_xlabel('Learning Rate', fontsize=12)
axes[0].set_ylabel('Cross-Validation Accuracy', fontsize=12)
axes[0].set_title('Learning Rate vs Accuracy', fontsize=14)
axes[0].grid(True, alpha=0.3)

axes[1].plot(max_depths, depth_scores, marker='s', linewidth=2, color='orange')
axes[1].set_xlabel('Max Depth', fontsize=12)
axes[1].set_ylabel('Cross-Validation Accuracy', fontsize=12)
axes[1].set_title('Max Depth vs Accuracy', fontsize=14)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Parameter Tuning Results ===")
print(f"Optimal learning_rate: {learning_rates[np.argmax(lr_scores)]}")
print(f"Optimal max_depth: {max_depths[np.argmax(depth_scores)]}")
</code></pre>

<h3>Effect of subsample and colsample</h3>

<pre><code class="language-python"># Combination of subsample and colsample_bytree
subsample_values = [0.5, 0.7, 0.9, 1.0]
colsample_values = [0.5, 0.7, 0.9, 1.0]

results = []

for sub in subsample_values:
    for col in colsample_values:
        model = xgb.XGBClassifier(
            subsample=sub,
            colsample_bytree=col,
            n_estimators=100,
            max_depth=3,
            learning_rate=0.1,
            random_state=42
        )
        scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
        results.append({
            'subsample': sub,
            'colsample': col,
            'accuracy': scores.mean()
        })

# Visualizing results
df_results = pd.DataFrame(results)
pivot_table = df_results.pivot(
    index='subsample',
    columns='colsample',
    values='accuracy'
)

import seaborn as sns
plt.figure(figsize=(10, 8))
sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='YlGnBu',
            cbar_kws={'label': 'Accuracy'})
plt.title('Subsample vs Colsample_bytree', fontsize=14)
plt.xlabel('Colsample_bytree', fontsize=12)
plt.ylabel('Subsample', fontsize=12)
plt.tight_layout()
plt.show()

print("\n=== Optimal Combination ===")
best_idx = df_results['accuracy'].idxmax()
print(f"subsample: {df_results.loc[best_idx, 'subsample']}")
print(f"colsample_bytree: {df_results.loc[best_idx, 'colsample']}")
print(f"Accuracy: {df_results.loc[best_idx, 'accuracy']:.3f}")
</code></pre>

<hr>

<h2>2.3 Implementation and Training</h2>

<h3>Efficient Training with DMatrix Format</h3>

<pre><code class="language-python">import xgboost as xgb
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

# Load data
data = load_breast_cancer()
X, y = data.data, data.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Convert to DMatrix format (speedup)
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Parameter settings
params = {
    'objective': 'binary:logistic',
    'max_depth': 3,
    'learning_rate': 0.1,
    'eval_metric': 'logloss'
}

# Training (with evaluation set)
evals = [(dtrain, 'train'), (dtest, 'test')]
num_rounds = 100

print("=== XGBoost Training (DMatrix Format) ===")
model = xgb.train(
    params,
    dtrain,
    num_boost_round=num_rounds,
    evals=evals,
    early_stopping_rounds=10,
    verbose_eval=20
)

print(f"\nOptimal number of iterations: {model.best_iteration}")
print(f"Best score: {model.best_score:.4f}")
</code></pre>

<h3>Overfitting Prevention with Early Stopping</h3>

<pre><code class="language-python">from sklearn.model_selection import train_test_split

# Split including validation set
X_train_full, X_test, y_train_full, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_full, y_train_full, test_size=0.2, random_state=42
)

# XGBoost model (sklearn API)
model = xgb.XGBClassifier(
    n_estimators=1000,
    max_depth=3,
    learning_rate=0.1,
    random_state=42
)

# Training with Early Stopping
model.fit(
    X_train, y_train,
    eval_set=[(X_train, y_train), (X_val, y_val)],
    early_stopping_rounds=20,
    verbose=50
)

# Evaluation on test set
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print("\n=== Early Stopping Results ===")
print(f"Number of trees used: {model.best_iteration}")
print(f"Validation accuracy: {accuracy:.3f}")

# Visualize learning curve
results = model.evals_result()

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(results['validation_0']['logloss'], label='Train')
plt.plot(results['validation_1']['logloss'], label='Validation')
plt.axvline(x=model.best_iteration, color='r', linestyle='--',
            label=f'Best iteration: {model.best_iteration}')
plt.xlabel('Number of Trees')
plt.ylabel('Log Loss')
plt.title('Learning Curve with Early Stopping')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(results['validation_0']['logloss'][-100:], label='Train (last 100)')
plt.plot(results['validation_1']['logloss'][-100:], label='Validation (last 100)')
plt.xlabel('Number of Trees')
plt.ylabel('Log Loss')
plt.title('Learning Curve (Detail)')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3>Performance Evaluation with Cross-Validation</h3>

<pre><code class="language-python">import xgboost as xgb

# Data in DMatrix format
dtrain = xgb.DMatrix(X, label=y)

# Parameter settings
params = {
    'objective': 'binary:logistic',
    'max_depth': 3,
    'learning_rate': 0.1,
    'eval_metric': 'logloss'
}

# Cross-Validation
cv_results = xgb.cv(
    params,
    dtrain,
    num_boost_round=200,
    nfold=5,
    metrics='logloss',
    early_stopping_rounds=20,
    seed=42,
    verbose_eval=50
)

print("\n=== Cross-Validation Results ===")
print(f"Optimal number of iterations: {len(cv_results)}")
print(f"Train log loss: {cv_results['train-logloss-mean'].iloc[-1]:.4f}")
print(f"Validation log loss: {cv_results['test-logloss-mean'].iloc[-1]:.4f}")
print(f"Standard deviation: {cv_results['test-logloss-std'].iloc[-1]:.4f}")

# Visualize CV results
plt.figure(figsize=(10, 6))
plt.plot(cv_results['train-logloss-mean'], label='Train')
plt.plot(cv_results['test-logloss-mean'], label='Test')
plt.fill_between(
    range(len(cv_results)),
    cv_results['test-logloss-mean'] - cv_results['test-logloss-std'],
    cv_results['test-logloss-mean'] + cv_results['test-logloss-std'],
    alpha=0.2
)
plt.xlabel('Number of Trees')
plt.ylabel('Log Loss')
plt.title('Cross-Validation Learning Curve')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<hr>

<h2>2.4 Feature Importance</h2>

<h3>Types of Importance</h3>

<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>gain</strong></td>
<td>Average improvement in loss by this feature</td>
<td>Contribution to prediction accuracy</td>
</tr>
<tr>
<td><strong>weight</strong></td>
<td>Number of times feature is used for split</td>
<td>Usage frequency</td>
</tr>
<tr>
<td><strong>cover</strong></td>
<td>Number of samples covered in splits</td>
<td>Range of influence</td>
</tr>
</tbody>
</table>

<h3>Calculating and Visualizing Feature Importance</h3>

<pre><code class="language-python">import xgboost as xgb
from sklearn.datasets import load_breast_cancer
import matplotlib.pyplot as plt

# Load data
data = load_breast_cancer()
X, y = data.data, data.target
feature_names = data.feature_names

# Train model
model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=3,
    learning_rate=0.1,
    random_state=42
)
model.fit(X, y)

# Get three importance types
importance_gain = model.get_booster().get_score(importance_type='gain')
importance_weight = model.get_booster().get_score(importance_type='weight')
importance_cover = model.get_booster().get_score(importance_type='cover')

# Visualization
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# Gain
xgb.plot_importance(model, importance_type='gain', max_num_features=10,
                    ax=axes[0], title='Feature Importance (Gain)')
axes[0].set_xlabel('Gain')

# Weight
xgb.plot_importance(model, importance_type='weight', max_num_features=10,
                    ax=axes[1], title='Feature Importance (Weight)')
axes[1].set_xlabel('Weight')

# Cover
xgb.plot_importance(model, importance_type='cover', max_num_features=10,
                    ax=axes[2], title='Feature Importance (Cover)')
axes[2].set_xlabel('Cover')

plt.tight_layout()
plt.show()

# Check numerically
print("=== Top 10 Features (Gain Criteria) ===")
importance_dict = model.get_booster().get_score(importance_type='gain')
sorted_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)
for i, (feature, score) in enumerate(sorted_features[:10], 1):
    feature_idx = int(feature.replace('f', ''))
    print(f"{i}. {feature_names[feature_idx]}: {score:.2f}")
</code></pre>

<h3>Detailed Analysis with SHAP Values</h3>

<pre><code class="language-python">import shap
import xgboost as xgb
import matplotlib.pyplot as plt

# Train model
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = xgb.XGBClassifier(n_estimators=100, max_depth=3, random_state=42)
model.fit(X_train, y_train)

# Calculate SHAP values
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# SHAP Summary Plot
plt.figure(figsize=(12, 8))
shap.summary_plot(shap_values, X_test, feature_names=feature_names,
                  show=False)
plt.title('SHAP Summary Plot', fontsize=14)
plt.tight_layout()
plt.show()

# SHAP Feature Importance
plt.figure(figsize=(10, 6))
shap.summary_plot(shap_values, X_test, feature_names=feature_names,
                  plot_type='bar', show=False)
plt.title('SHAP Feature Importance', fontsize=14)
plt.tight_layout()
plt.show()

print("=== Importance Analysis with SHAP Values ===")
print("Mean absolute SHAP value for each feature (Top 5):")
shap_importance = np.abs(shap_values).mean(axis=0)
top_indices = np.argsort(shap_importance)[-5:][::-1]
for idx in top_indices:
    print(f"{feature_names[idx]}: {shap_importance[idx]:.4f}")
</code></pre>

<hr>

<h2>2.5 Practical Optimization</h2>

<h3>GPU Acceleration</h3>

<pre><code class="language-python">import xgboost as xgb
import time

# Data preparation (large-scale data)
X, y = make_classification(
    n_samples=100000,
    n_features=50,
    n_informative=40,
    random_state=42
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# CPU training
start_time = time.time()
model_cpu = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    tree_method='hist',  # CPU speedup
    random_state=42
)
model_cpu.fit(X_train, y_train)
cpu_time = time.time() - start_time

# GPU training (if GPU is available)
try:
    start_time = time.time()
    model_gpu = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        tree_method='gpu_hist',  # GPU acceleration
        random_state=42
    )
    model_gpu.fit(X_train, y_train)
    gpu_time = time.time() - start_time

    print("=== GPU vs CPU Performance Comparison ===")
    print(f"CPU training time: {cpu_time:.2f} seconds")
    print(f"GPU training time: {gpu_time:.2f} seconds")
    print(f"Speed improvement: {cpu_time/gpu_time:.2f}x")
except Exception as e:
    print("=== CPU Training Results ===")
    print(f"Training time: {cpu_time:.2f} seconds")
    print(f"GPU unavailable: {str(e)}")

# Performance evaluation
y_pred = model_cpu.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"\nTest accuracy: {accuracy:.3f}")
</code></pre>

<h3>Hyperparameter Optimization with Grid Search</h3>

<pre><code class="language-python">from sklearn.model_selection import GridSearchCV
import xgboost as xgb

# Data preparation
X, y = make_classification(
    n_samples=2000,
    n_features=20,
    n_informative=15,
    random_state=42
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Parameter grid
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'n_estimators': [50, 100, 200],
    'subsample': [0.7, 0.9, 1.0],
    'colsample_bytree': [0.7, 0.9, 1.0]
}

# Grid search
model = xgb.XGBClassifier(random_state=42)

grid_search = GridSearchCV(
    model,
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=2
)

print("=== Running Grid Search ===")
grid_search.fit(X_train, y_train)

# Optimal parameters
print("\n=== Optimal Parameters ===")
print(grid_search.best_params_)
print(f"\nBest CV accuracy: {grid_search.best_score_:.3f}")

# Evaluation on test set
y_pred = grid_search.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Test accuracy: {test_accuracy:.3f}")

# Top 5 combinations
cv_results = pd.DataFrame(grid_search.cv_results_)
top_5 = cv_results.nlargest(5, 'mean_test_score')[
    ['params', 'mean_test_score', 'std_test_score']
]
print("\n=== Top 5 Parameter Combinations ===")
for i, row in top_5.iterrows():
    print(f"\n{i+1}. Score: {row['mean_test_score']:.4f} (¬±{row['std_test_score']:.4f})")
    print(f"   Parameters: {row['params']}")
</code></pre>

<h3>Efficient Search with Bayesian Optimization</h3>

<pre><code class="language-python">from sklearn.model_selection import cross_val_score
from scipy.stats import uniform, randint

# Random search (alternative to Bayesian optimization)
from sklearn.model_selection import RandomizedSearchCV

# Parameter distributions
param_distributions = {
    'max_depth': randint(3, 10),
    'learning_rate': uniform(0.01, 0.29),
    'n_estimators': randint(50, 300),
    'subsample': uniform(0.6, 0.4),
    'colsample_bytree': uniform(0.6, 0.4),
    'gamma': uniform(0, 5),
    'reg_alpha': uniform(0, 1),
    'reg_lambda': uniform(0, 1)
}

# Random search
model = xgb.XGBClassifier(random_state=42)

random_search = RandomizedSearchCV(
    model,
    param_distributions,
    n_iter=50,  # Number of trials
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42,
    verbose=1
)

print("=== Running Random Search ===")
random_search.fit(X_train, y_train)

# Optimal parameters
print("\n=== Optimal Parameters (Random Search) ===")
print(random_search.best_params_)
print(f"\nBest CV accuracy: {random_search.best_score_:.3f}")

# Test accuracy
y_pred = random_search.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Test accuracy: {test_accuracy:.3f}")

# Visualize search process
cv_results = pd.DataFrame(random_search.cv_results_)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(range(len(cv_results)), cv_results['mean_test_score'],
            alpha=0.6, edgecolors='black')
plt.axhline(y=random_search.best_score_, color='r', linestyle='--',
            label=f'Best: {random_search.best_score_:.3f}')
plt.xlabel('Iteration')
plt.ylabel('CV Score')
plt.title('Random Search Progress')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
sorted_scores = sorted(cv_results['mean_test_score'])
plt.plot(sorted_scores, linewidth=2)
plt.xlabel('Rank')
plt.ylabel('CV Score')
plt.title('Sorted CV Scores')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<hr>

<h2>2.6 Chapter Summary</h2>

<h3>What We Learned</h3>

<ol>
<li><p><strong>Principles of XGBoost</strong></p>
<ul>
<li>Optimized implementation of gradient boosting</li>
<li>Overfitting prevention through regularization</li>
<li>Efficient tree pruning algorithm</li>
</ul></li>

<li><p><strong>Hyperparameters</strong></p>
<ul>
<li>learning_rate: Learning stability and speed</li>
<li>max_depth: Controlling model complexity</li>
<li>subsample/colsample: Introducing randomness</li>
<li>gamma, reg_alpha, reg_lambda: Regularization</li>
</ul></li>

<li><p><strong>Implementation and Training</strong></p>
<ul>
<li>Efficient processing with DMatrix format</li>
<li>Automatic optimization with Early Stopping</li>
<li>Robust evaluation with Cross-Validation</li>
</ul></li>

<li><p><strong>Feature Importance</strong></p>
<ul>
<li>Three types: gain, weight, cover</li>
<li>Detailed interpretation with SHAP values</li>
<li>Utilizing for model improvement</li>
</ul></li>

<li><p><strong>Practical Optimization</strong></p>
<ul>
<li>Speedup with GPU acceleration</li>
<li>Grid search and random search</li>
<li>Efficient parameter search strategies</li>
</ul></li>
</ol>

<h3>Best Practices for Using XGBoost</h3>

<table>
<thead>
<tr>
<th>Principle</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Small learning_rate</strong></td>
<td>Stable learning with 0.01-0.1, increase n_estimators</td>
</tr>
<tr>
<td><strong>Use Early Stopping</strong></td>
<td>Prevent overfitting and reduce training time</td>
</tr>
<tr>
<td><strong>Introduce subsample</strong></td>
<td>Improve randomness and generalization with 0.7-0.9</td>
</tr>
<tr>
<td><strong>Check feature importance</strong></td>
<td>Simplify model by removing unnecessary features</td>
</tr>
<tr>
<td><strong>CV evaluation</strong></td>
<td>Robust performance evaluation and parameter selection</td>
</tr>
</tbody>
</table>

<h3>To the Next Chapter</h3>

<p>In Chapter 3, we will learn about <strong>LightGBM</strong>:</p>
<ul>
<li>Leaf-wise algorithm</li>
<li>Direct handling of categorical variables</li>
<li>Achieving ultra-fast training</li>
<li>Comparison with XGBoost</li>
</ul>

<hr>

<h2>Practice Problems</h2>

<h3>Problem 1 (Difficulty: Easy)</h3>
<p>Explain the meaning of each term in XGBoost's regularization term $\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T} w_j^2$.</p>

<details>
<summary>Solution</summary>

<p><strong>Answer</strong>:</p>

<ul>
<li><p><strong>$\gamma T$</strong>: Penalty for the number of leaves</p>
<ul>
<li>$\gamma$: Leaf count penalty coefficient</li>
<li>$T$: Number of leaves in the tree</li>
<li>Effect: Penalty increases with more leaves, preventing trees from becoming too complex</li>
</ul></li>

<li><p><strong>$\frac{1}{2}\lambda \sum_{j=1}^{T} w_j^2$</strong>: L2 regularization for leaf weights</p>
<ul>
<li>$\lambda$: L2 regularization coefficient</li>
<li>$w_j$: Weight of each leaf (predicted value)</li>
<li>Effect: Prevents weights from becoming too large, promoting smooth predictions</li>
</ul></li>
</ul>

<p>Through these regularizations, XGBoost prevents overfitting and improves generalization performance.</p>

</details>

<h3>Problem 2 (Difficulty: Medium)</h3>
<p>Train an XGBoost model on the following data and find the optimal number of trees using Early Stopping.</p>

<pre><code class="language-python">from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split

data = load_diabetes()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
</code></pre>

<details>
<summary>Solution</summary>

<pre><code class="language-python">import xgboost as xgb
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Load data
data = load_diabetes()
X, y = data.data, data.target

# Data split (train, validation, test)
X_train_full, X_test, y_train_full, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_full, y_train_full, test_size=0.2, random_state=42
)

# XGBoost model (regression)
model = xgb.XGBRegressor(
    n_estimators=1000,
    max_depth=3,
    learning_rate=0.05,
    random_state=42
)

# Training with Early Stopping
model.fit(
    X_train, y_train,
    eval_set=[(X_train, y_train), (X_val, y_val)],
    early_stopping_rounds=50,
    verbose=100
)

# Results
print("\n=== Early Stopping Results ===")
print(f"Optimal number of trees: {model.best_iteration}")
print(f"Training RMSE: {model.best_score:.2f}")

# Evaluation on test set
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"\nTest Set Performance:")
print(f"MSE: {mse:.2f}")
print(f"RMSE: {np.sqrt(mse):.2f}")
print(f"R¬≤: {r2:.3f}")

# Visualize learning curve
import matplotlib.pyplot as plt

results = model.evals_result()
plt.figure(figsize=(10, 6))
plt.plot(results['validation_0']['rmse'], label='Train')
plt.plot(results['validation_1']['rmse'], label='Validation')
plt.axvline(x=model.best_iteration, color='r', linestyle='--',
            label=f'Best: {model.best_iteration}')
plt.xlabel('Number of Trees')
plt.ylabel('RMSE')
plt.title('Early Stopping - Learning Curve')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

</details>

<h3>Problem 3 (Difficulty: Medium)</h3>
<p>Explain the relationship between learning_rate and n_estimators, and describe a strategy for choosing the optimal combination.</p>

<details>
<summary>Solution</summary>

<p><strong>Answer</strong>:</p>

<p><strong>Relationship between learning_rate and n_estimators</strong>:</p>

<ul>
<li><strong>learning_rate (learning rate)</strong>: Coefficient that shrinks the contribution of each tree
<ul>
<li>Small values (0.01-0.05): Stable learning, less overfitting</li>
<li>Large values (0.2-0.3): Fast learning, overfitting risk</li>
</ul></li>

<li><strong>n_estimators (number of trees)</strong>: Total number of trees to ensemble
<ul>
<li>Many (500-1000): Learn complex patterns, high computational cost</li>
<li>Few (50-100): Fast, simple model</li>
</ul></li>
</ul>

<p><strong>Tradeoff</strong>:</p>
<ul>
<li>Reducing learning_rate requires increasing n_estimators to reach the same performance</li>
<li>Rule of thumb: <code>learning_rate √ó n_estimators ‚âà constant</code></li>
</ul>

<p><strong>Optimization Strategy</strong>:</p>

<ol>
<li><p><strong>Initial exploration</strong> (speed-focused)</p>
<ul>
<li>learning_rate = 0.1, n_estimators = 100</li>
<li>Check baseline performance</li>
</ul></li>

<li><p><strong>Use Early Stopping</strong></p>
<ul>
<li>learning_rate = 0.05, n_estimators = 1000 (large)</li>
<li>Automatically determine optimal number of trees with Early Stopping</li>
</ul></li>

<li><p><strong>Fine tuning</strong> (accuracy-focused)</p>
<ul>
<li>learning_rate = 0.01, n_estimators = 500-2000</li>
<li>Pursue highest accuracy in final model</li>
</ul></li>

<li><p><strong>Production environment</strong> (balanced)</p>
<ul>
<li>learning_rate = 0.03-0.05</li>
<li>Use n_estimators determined by Early Stopping</li>
</ul></li>
</ol>

<p><strong>Implementation Example</strong>:</p>
<pre><code class="language-python"># Strategy 1: Speed-focused
model_fast = xgb.XGBClassifier(learning_rate=0.1, n_estimators=100)

# Strategy 2: Early Stopping (recommended)
model_auto = xgb.XGBClassifier(learning_rate=0.05, n_estimators=1000)
model_auto.fit(X_train, y_train, eval_set=[(X_val, y_val)],
               early_stopping_rounds=50)

# Strategy 3: Accuracy-focused
model_accurate = xgb.XGBClassifier(learning_rate=0.01, n_estimators=1500)
</code></pre>

</details>

<h3>Problem 4 (Difficulty: Hard)</h3>
<p>Execute a grid search on the following data to find optimal hyperparameters. Also propose which parameters to search and their ranges.</p>

<pre><code class="language-python">from sklearn.datasets import make_classification

X, y = make_classification(n_samples=5000, n_features=30, n_informative=20, random_state=42)
</code></pre>

<details>
<summary>Solution</summary>

<pre><code class="language-python">import xgboost as xgb
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd

# Generate data
X, y = make_classification(
    n_samples=5000,
    n_features=30,
    n_informative=20,
    random_state=42
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Parameter grid (staged search)
# Phase 1: Coarse search of main parameters
param_grid_phase1 = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'n_estimators': [100, 200],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

model = xgb.XGBClassifier(random_state=42)

print("=== Phase 1: Coarse Search ===")
grid_search_phase1 = GridSearchCV(
    model,
    param_grid_phase1,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search_phase1.fit(X_train, y_train)
print(f"\nPhase 1 optimal parameters: {grid_search_phase1.best_params_}")
print(f"Phase 1 best CV accuracy: {grid_search_phase1.best_score_:.4f}")

# Phase 2: Fine tuning of regularization parameters
best_params = grid_search_phase1.best_params_

param_grid_phase2 = {
    'max_depth': [best_params['max_depth']],
    'learning_rate': [best_params['learning_rate']],
    'n_estimators': [best_params['n_estimators']],
    'subsample': [best_params['subsample']],
    'colsample_bytree': [best_params['colsample_bytree']],
    'gamma': [0, 0.1, 0.5, 1],
    'reg_alpha': [0, 0.1, 0.5],
    'reg_lambda': [1, 1.5, 2]
}

print("\n=== Phase 2: Regularization Tuning ===")
grid_search_phase2 = GridSearchCV(
    model,
    param_grid_phase2,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search_phase2.fit(X_train, y_train)
print(f"\nPhase 2 optimal parameters: {grid_search_phase2.best_params_}")
print(f"Phase 2 best CV accuracy: {grid_search_phase2.best_score_:.4f}")

# Final evaluation on test set
final_model = grid_search_phase2.best_estimator_
y_pred = final_model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)

print("\n=== Final Model Performance ===")
print(f"Test accuracy: {test_accuracy:.4f}")
print(f"\nClassification report:")
print(classification_report(y_test, y_pred))

# Analysis of search results
cv_results = pd.DataFrame(grid_search_phase2.cv_results_)
top_10 = cv_results.nlargest(10, 'mean_test_score')[
    ['params', 'mean_test_score', 'std_test_score']
]

print("\n=== Top 10 Parameter Combinations ===")
for i, row in top_10.iterrows():
    print(f"\n{i+1}. CV accuracy: {row['mean_test_score']:.4f} (¬±{row['std_test_score']:.4f})")
    print(f"   gamma: {row['params']['gamma']}")
    print(f"   reg_alpha: {row['params']['reg_alpha']}")
    print(f"   reg_lambda: {row['params']['reg_lambda']}")

# Visualization: Impact of parameters
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for ax, param in zip(axes, ['gamma', 'reg_alpha', 'reg_lambda']):
    param_values = cv_results['param_' + param].values
    scores = cv_results['mean_test_score'].values

    ax.scatter(param_values, scores, alpha=0.6, edgecolors='black')
    ax.set_xlabel(param, fontsize=12)
    ax.set_ylabel('CV Accuracy', fontsize=12)
    ax.set_title(f'Impact of {param}', fontsize=14)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>Recommended Parameter Ranges</strong>:</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Initial Search</th>
<th>Fine Tuning</th>
</tr>
</thead>
<tbody>
<tr>
<td>max_depth</td>
<td>3, 5, 7</td>
<td>¬±1 range</td>
</tr>
<tr>
<td>learning_rate</td>
<td>0.01, 0.1, 0.3</td>
<td>Around optimal value</td>
</tr>
<tr>
<td>n_estimators</td>
<td>100, 200</td>
<td>Early Stopping recommended</td>
</tr>
<tr>
<td>subsample</td>
<td>0.8, 1.0</td>
<td>0.7-1.0</td>
</tr>
<tr>
<td>colsample_bytree</td>
<td>0.8, 1.0</td>
<td>0.7-1.0</td>
</tr>
<tr>
<td>gamma</td>
<td>-</td>
<td>0, 0.1, 0.5, 1</td>
</tr>
<tr>
<td>reg_alpha</td>
<td>-</td>
<td>0, 0.1, 0.5</td>
</tr>
<tr>
<td>reg_lambda</td>
<td>-</td>
<td>1, 1.5, 2</td>
</tr>
</tbody>
</table>

</details>

<h3>Problem 5 (Difficulty: Hard)</h3>
<p>Explain the differences between gain, weight, and cover in XGBoost feature importance, and describe when each should be used.</p>

<details>
<summary>Solution</summary>

<p><strong>Answer</strong>:</p>

<p><strong>Three Importance Metrics</strong>:</p>

<ol>
<li><p><strong>Gain</strong></p>
<ul>
<li>Definition: Average improvement in loss function by this feature</li>
<li>Calculation: Sum of loss reduction at splits divided by number of times the feature is used</li>
<li>Meaning: Direct contribution to prediction accuracy</li>
<li>Formula: $\text{Gain}_f = \sum_{t \in \text{splits}(f)} \Delta L_t / |\text{splits}(f)|$</li>
</ul></li>

<li><p><strong>Weight</strong></p>
<ul>
<li>Definition: Number of times this feature is used for splits</li>
<li>Calculation: Total count of times the feature is used for splits across all trees</li>
<li>Meaning: Usage frequency in model construction</li>
<li>Formula: $\text{Weight}_f = |\text{splits}(f)|$</li>
</ul></li>

<li><p><strong>Cover</strong></p>
<ul>
<li>Definition: Total number of samples covered in splits</li>
<li>Calculation: Sum of number of samples affected when splitting on this feature</li>
<li>Meaning: Range of data the feature influences</li>
<li>Formula: $\text{Cover}_f = \sum_{t \in \text{splits}(f)} n_t$ (where $n_t$ is the number of samples in the split)</li>
</ul></li>
</ol>

<p><strong>Usage Guidelines</strong>:</p>

<table>
<thead>
<tr>
<th>Scenario</th>
<th>Recommended Metric</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>Feature selection</td>
<td>Gain</td>
<td>Direct understanding of contribution to accuracy</td>
</tr>
<tr>
<td>Model interpretation</td>
<td>Gain</td>
<td>Easy to explain business value</td>
</tr>
<tr>
<td>Computational efficiency</td>
<td>Weight</td>
<td>Prioritize computation for frequently used features</td>
</tr>
<tr>
<td>Data influence range</td>
<td>Cover</td>
<td>How many samples are affected</td>
</tr>
<tr>
<td>Imbalanced data</td>
<td>Cover</td>
<td>Evaluate impact on minority class</td>
</tr>
<tr>
<td>General purpose</td>
<td>Gain</td>
<td>Most interpretable and practical</td>
</tr>
</tbody>
</table>

<p><strong>Implementation Example</strong>:</p>
<pre><code class="language-python">import xgboost as xgb
from sklearn.datasets import load_breast_cancer
import matplotlib.pyplot as plt

# Load data
data = load_breast_cancer()
X, y = data.data, data.target

# Train model
model = xgb.XGBClassifier(n_estimators=100, max_depth=3, random_state=42)
model.fit(X, y)

# Get three importance types
importance_gain = model.get_booster().get_score(importance_type='gain')
importance_weight = model.get_booster().get_score(importance_type='weight')
importance_cover = model.get_booster().get_score(importance_type='cover')

# Compare Top 5
print("=== Comparison of Top 5 Features ===\n")

for imp_type, imp_dict in [('Gain', importance_gain),
                            ('Weight', importance_weight),
                            ('Cover', importance_cover)]:
    print(f"{imp_type} criteria:")
    sorted_features = sorted(imp_dict.items(), key=lambda x: x[1], reverse=True)
    for i, (feature, score) in enumerate(sorted_features[:5], 1):
        feature_idx = int(feature.replace('f', ''))
        print(f"  {i}. {data.feature_names[feature_idx]}: {score:.2f}")
    print()

# Visualization: Correlation of three importance types
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Gain vs Weight
common_features = set(importance_gain.keys()) & set(importance_weight.keys())
gain_vals = [importance_gain[f] for f in common_features]
weight_vals = [importance_weight[f] for f in common_features]

axes[0].scatter(weight_vals, gain_vals, alpha=0.6, edgecolors='black')
axes[0].set_xlabel('Weight')
axes[0].set_ylabel('Gain')
axes[0].set_title('Gain vs Weight')
axes[0].grid(True, alpha=0.3)

# Gain vs Cover
cover_vals = [importance_cover[f] for f in common_features]

axes[1].scatter(cover_vals, gain_vals, alpha=0.6, edgecolors='black', color='orange')
axes[1].set_xlabel('Cover')
axes[1].set_ylabel('Gain')
axes[1].set_title('Gain vs Cover')
axes[1].grid(True, alpha=0.3)

# Weight vs Cover
axes[2].scatter(weight_vals, cover_vals, alpha=0.6, edgecolors='black', color='green')
axes[2].set_xlabel('Weight')
axes[2].set_ylabel('Cover')
axes[2].set_title('Weight vs Cover')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>Conclusion</strong>:</p>
<ul>
<li>Generally use <strong>Gain</strong> (clear contribution to prediction accuracy)</li>
<li><strong>Weight</strong> and <strong>Cover</strong> are also useful for computational efficiency and data structure analysis</li>
<li>It is important to make comprehensive judgments by combining multiple metrics</li>
</ul>

</details>

<hr>

<h2>References</h2>

<ol>
<li>Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>.</li>
<li>Chen, T., He, T., Benesty, M., et al. (2023). <em>XGBoost Documentation</em>. https://xgboost.readthedocs.io/</li>
<li>Hastie, T., Tibshirani, R., & Friedman, J. (2009). <em>The Elements of Statistical Learning</em> (2nd ed.). Springer.</li>
<li>Lundberg, S. M., & Lee, S. I. (2017). A Unified Approach to Interpreting Model Predictions. <em>Advances in Neural Information Processing Systems</em>.</li>
</ol>

<div class="navigation">
    <a href="chapter1-gradient-boosting.html" class="nav-button">‚Üê Previous Chapter: Gradient Boosting Basics</a>
    <a href="chapter3-lightgbm.html" class="nav-button">Next Chapter: LightGBM ‚Üí</a>
</div>

    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is intended solely for educational, research, and informational purposes and does not provide professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without any warranty, express or implied, including but not limited to warranties of merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, or operational safety.</li>
            <li>The creator and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
            <li>The creator and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content, to the maximum extent permitted by applicable law.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content shall follow the specified conditions (e.g., CC BY 4.0). Such licenses typically include a disclaimer of warranty.</li>
        </ul>
    </section>

<footer>
        <p><strong>Author</strong>: AI Terakoya Content Team</p>
        <p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-21</p>
        <p><strong>License</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
