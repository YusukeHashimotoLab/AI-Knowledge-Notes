<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 2: Bayesian Optimization and Optuna - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/hyperparameter-tuning-introduction/index.html">Hyperparameter Tuning</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 2</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 2: Bayesian Optimization and Optuna</h1>
<p class="subtitle">Efficient Hyperparameter Tuning - Smart Search Strategies</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 25-30 minutes</span>
<span class="meta-item">üìä Difficulty: Intermediate</span>
<span class="meta-item">üíª Code Examples: 8</span>
<span class="meta-item">üéØ Importance: High</span>
</div>
</div>
</header>
<main class="container">

<p class="chapter-description">This chapter covers Bayesian Optimization and Optuna. You will learn fundamental principles of Bayesian Optimization, how TPE (Tree-structured Parzen Estimator) works, and Achieve efficient search with Pruning.</p>
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master the following:</p>
<ul>
<li>‚úÖ Understand the fundamental principles of Bayesian Optimization</li>
<li>‚úÖ Learn how TPE (Tree-structured Parzen Estimator) works</li>
<li>‚úÖ Master the basic concepts and API of Optuna</li>
<li>‚úÖ Achieve efficient search with Pruning</li>
<li>‚úÖ Optimize hyperparameters for deep learning models</li>
<li>‚úÖ Analyze optimization processes with visualization tools</li>
</ul>
<hr/>
<h2>2.1 Foundations of Bayesian Optimization</h2>
<h3>What is Bayesian Optimization</h3>
<p><strong>Bayesian Optimization</strong> is a method for efficiently optimizing objective functions with high evaluation costs. Compared to grid search and random search, it has the following characteristics:</p>
<ul>
<li>Utilizes past trial results to determine the next search point</li>
<li>Automatically balances exploration and exploitation</li>
<li>Finds good solutions with fewer trials</li>
</ul>
<h3>The Exploration-Exploitation Trade-off</h3>
<p>The core of Bayesian Optimization is the balance between <strong>Exploration</strong> and <strong>Exploitation</strong>.</p>
<table>
<thead>
<tr>
<th>Strategy</th>
<th>Description</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Exploration</strong></td>
<td>Investigate unknown regions</td>
<td>Discovery of global optimum</td>
<td>May increase unnecessary trials</td>
</tr>
<tr>
<td><strong>Exploitation</strong></td>
<td>Intensively investigate around good performance</td>
<td>Quickly converge to good solutions</td>
<td>May get trapped in local optima</td>
</tr>
</tbody>
</table>
<div class="mermaid">
graph LR
    A[Initial Random Sampling] --&gt; B[Build Surrogate Model]
    B --&gt; C[Select Next Point with Acquisition Function]
    C --&gt; D[Evaluate Objective Function]
    D --&gt; E{Stop Condition?}
    E --&gt;|No| B
    E --&gt;|Yes| F[Return Best Point]

    style A fill:#ffebee
    style B fill:#e3f2fd
    style C fill:#f3e5f5
    style D fill:#fff3e0
    style E fill:#fce4ec
    style F fill:#c8e6c9
</div>
<h3>Surrogate Model (Gaussian Process)</h3>
<p><strong>Surrogate Model</strong> is a proxy model of the objective function. The most common one is the <strong>Gaussian Process (GP)</strong>.</p>
<p>Gaussian processes provide both prediction values and uncertainty at each point:</p>
<p>$$
f(x) \sim \mathcal{N}(\mu(x), \sigma^2(x))
$$</p>
<ul>
<li>$\mu(x)$: Prediction mean (expected value)</li>
<li>$\sigma^2(x)$: Prediction variance (uncertainty)</li>
</ul>
<blockquote>
<p><strong>Important</strong>: The farther from observed points, the greater the uncertainty, promoting exploration.</p>
</blockquote>
<h3>Acquisition Function</h3>
<p><strong>Acquisition Function</strong> is an index that determines the next point to evaluate. Major acquisition functions:</p>
<h4>1. Expected Improvement (EI)</h4>
<p>Expected value of improvement from the current best value:</p>
<p>$$
\text{EI}(x) = \mathbb{E}[\max(f(x) - f(x^+), 0)]
$$</p>
<ul>
<li>$f(x^+)$: Current best value</li>
<li>Prioritizes points with expected improvement</li>
</ul>
<h4>2. Upper Confidence Bound (UCB)</h4>
<p>Balance between mean and uncertainty:</p>
<p>$$
\text{UCB}(x) = \mu(x) + \kappa \sigma(x)
$$</p>
<ul>
<li>$\kappa$: Controls exploration strength (typically 1.96)</li>
<li>Selects points with high mean or high uncertainty</li>
</ul>
<h4>3. Probability of Improvement (PI)</h4>
<p>Probability of improvement:</p>
<p>$$
\text{PI}(x) = P(f(x) &gt; f(x^+))
$$</p>
<ul>
<li>Selects points with high probability of improvement</li>
<li>Relatively conservative</li>
</ul>
<h3>Bayesian Optimization Implementation Example</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Bayesian Optimization Implementation Example

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 10-30 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel
from scipy.stats import norm

# Objective function (example: complex 1D function)
def objective_function(x):
    return -(x ** 2) * np.sin(5 * x)

# Acquisition function: Expected Improvement (EI)
def expected_improvement(X, X_sample, Y_sample, gpr, xi=0.01):
    mu, sigma = gpr.predict(X, return_std=True)
    mu_sample = gpr.predict(X_sample)

    sigma = sigma.reshape(-1, 1)

    # Current best value
    mu_sample_opt = np.max(mu_sample)

    with np.errstate(divide='warn'):
        imp = mu - mu_sample_opt - xi
        Z = imp / sigma
        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)
        ei[sigma == 0.0] = 0.0

    return ei

# Execute Bayesian optimization
np.random.seed(42)

# Search space
X_true = np.linspace(-3, 3, 1000).reshape(-1, 1)
y_true = objective_function(X_true)

# Initial sampling
n_initial = 3
X_sample = np.random.uniform(-3, 3, n_initial).reshape(-1, 1)
Y_sample = objective_function(X_sample)

# Define Gaussian process
kernel = ConstantKernel(1.0) * RBF(length_scale=1.0)
gpr = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, n_restarts_optimizer=10)

# Iterative optimization
n_iterations = 7
plt.figure(figsize=(16, 12))

for i in range(n_iterations):
    # Fit Gaussian process
    gpr.fit(X_sample, Y_sample)

    # Prediction
    mu, sigma = gpr.predict(X_true, return_std=True)

    # Calculate acquisition function
    ei = expected_improvement(X_true, X_sample, Y_sample, gpr)

    # Select next point (maximize EI)
    X_next = X_true[np.argmax(ei)]
    Y_next = objective_function(X_next)

    # Plot
    plt.subplot(3, 3, i + 1)

    # True function
    plt.plot(X_true, y_true, 'r--', label='True Function', alpha=0.7)

    # Gaussian process prediction
    plt.plot(X_true, mu, 'b-', label='GP Mean')
    plt.fill_between(X_true.ravel(),
                     mu.ravel() - 1.96 * sigma,
                     mu.ravel() + 1.96 * sigma,
                     alpha=0.2, label='95% Confidence Interval')

    # Observed points
    plt.scatter(X_sample, Y_sample, c='green', s=100,
                zorder=10, label='Observed Points', edgecolors='black')

    # Next point
    plt.axvline(x=X_next, color='purple', linestyle='--',
                linewidth=2, label='Next Search Point')

    plt.xlabel('x')
    plt.ylabel('f(x)')
    plt.title(f'Iteration {i+1}/{n_iterations}', fontsize=12)
    plt.legend(loc='best', fontsize=8)
    plt.grid(True, alpha=0.3)

    # Add sample
    X_sample = np.vstack((X_sample, X_next))
    Y_sample = np.vstack((Y_sample, Y_next))

plt.tight_layout()
plt.show()

# Final results
best_idx = np.argmax(Y_sample)
print(f"\n=== Bayesian Optimization Results ===")
print(f"Best x: {X_sample[best_idx][0]:.4f}")
print(f"Best f(x): {Y_sample[best_idx][0]:.4f}")
print(f"Total evaluations: {len(X_sample)}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Bayesian Optimization Results ===
Best x: 1.7854
Best f(x): 2.8561
Total evaluations: 10
</code></pre>
<blockquote>
<p><strong>Observation</strong>: Efficiently converges to the maximum value with few trials.</p>
</blockquote>
<hr/>
<h2>2.2 TPE (Tree-structured Parzen Estimator)</h2>
<h3>How TPE Works</h3>
<p><strong>TPE (Tree-structured Parzen Estimator)</strong> is an efficient implementation of Bayesian Optimization. It's the default optimization algorithm in Optuna.</p>
<p>Core ideas of TPE:</p>
<ol>
<li>Divide observed data into good and bad results</li>
<li>Model each distribution</li>
<li>Select points sampled often from good distribution and rarely from bad distribution</li>
</ol>
<h3>Differences from Gaussian Processes</h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Gaussian Process (GP)</th>
<th>TPE</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Modeling Target</strong></td>
<td>$P(y|x)$ - Predict output</td>
<td>$P(x|y)$ - Conditional distribution of input</td>
</tr>
<tr>
<td><strong>Computational Cost</strong></td>
<td>$O(n^3)$ - High for sample size</td>
<td>$O(n)$ - Linear</td>
</tr>
<tr>
<td><strong>High-dimensional Performance</strong></td>
<td>Degrades with high dimensions</td>
<td>Stable even in high dimensions</td>
</tr>
<tr>
<td><strong>Categorical Variables</strong></td>
<td>Difficult to handle</td>
<td>Naturally handled</td>
</tr>
<tr>
<td><strong>Parallelization</strong></td>
<td>Difficult</td>
<td>Easy</td>
</tr>
</tbody>
</table>
<h3>TPE Formulation</h3>
<p>TPE defines two distributions as follows:</p>
<p>$$
P(x|y) = \begin{cases}
\ell(x) &amp; \text{if } y &lt; y^* \\
g(x) &amp; \text{if } y \geq y^*
\end{cases}
$$</p>
<ul>
<li>$\ell(x)$: Distribution of good results</li>
<li>$g(x)$: Distribution of bad results</li>
<li>$y^*$: Threshold (typically top 20-25%)</li>
</ul>
<p>The acquisition function maximizes the following ratio:</p>
<p>$$
\text{EI}(x) \propto \frac{\ell(x)}{g(x)}
$$</p>
<blockquote>
<p><strong>Intuition</strong>: Select points with high probability in good distribution and low probability in bad distribution.</p>
</blockquote>
<h3>Implementation Efficiency</h3>
<p>Main advantages of TPE:</p>
<ol>
<li><strong>Scalability</strong>: Fast even in large search spaces</li>
<li><strong>Flexibility</strong>: Uniformly handles continuous, discrete, and categorical variables</li>
<li><strong>Parallelization</strong>: Can execute multiple trials simultaneously</li>
<li><strong>Conditional Spaces</strong>: Handles dependencies between hyperparameters</li>
</ol>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Main advantages of TPE:

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

# TPE behavior image (Optuna internal operation)
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import gaussian_kde

# Sample data (hyperparameters and performance)
np.random.seed(42)
n_trials = 50

# Random hyperparameter values
x_trials = np.random.uniform(0, 10, n_trials)

# Performance (true function + noise)
y_trials = -(x_trials - 6) ** 2 + 30 + np.random.normal(0, 2, n_trials)

# Set threshold (top 25%)
threshold_idx = int(n_trials * 0.75)
sorted_indices = np.argsort(y_trials)
threshold_value = y_trials[sorted_indices[threshold_idx]]

# Divide into good and bad trials
good_x = x_trials[y_trials &gt;= threshold_value]
bad_x = x_trials[y_trials &lt; threshold_value]

# Kernel density estimation
x_range = np.linspace(0, 10, 1000)

if len(good_x) &gt; 1:
    kde_good = gaussian_kde(good_x)
    density_good = kde_good(x_range)
else:
    density_good = np.zeros_like(x_range)

if len(bad_x) &gt; 1:
    kde_bad = gaussian_kde(bad_x)
    density_bad = kde_bad(x_range)
else:
    density_bad = np.zeros_like(x_range)

# EI approximation (‚Ñì(x) / g(x))
ei_approx = np.zeros_like(x_range)
mask = density_bad &gt; 1e-6
ei_approx[mask] = density_good[mask] / density_bad[mask]

# Plot
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. Distribution of trials
axes[0, 0].scatter(x_trials, y_trials, c='blue', alpha=0.6,
                   s=50, edgecolors='black')
axes[0, 0].axhline(y=threshold_value, color='red',
                   linestyle='--', linewidth=2, label=f'Threshold (Top 25%)')
axes[0, 0].scatter(good_x, y_trials[y_trials &gt;= threshold_value],
                   c='green', s=100, label='Good Trials',
                   edgecolors='black', zorder=5)
axes[0, 0].set_xlabel('Hyperparameter x')
axes[0, 0].set_ylabel('Performance y')
axes[0, 0].set_title('Distribution of Trials', fontsize=12)
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# 2. Distribution of good trials ‚Ñì(x)
axes[0, 1].fill_between(x_range, density_good, alpha=0.5,
                        color='green', label='‚Ñì(x): Good Trial Distribution')
axes[0, 1].scatter(good_x, np.zeros_like(good_x),
                   c='green', s=50, marker='|', linewidths=2)
axes[0, 1].set_xlabel('Hyperparameter x')
axes[0, 1].set_ylabel('Density')
axes[0, 1].set_title('Good Trial Distribution ‚Ñì(x)', fontsize=12)
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# 3. Distribution of bad trials g(x)
axes[1, 0].fill_between(x_range, density_bad, alpha=0.5,
                        color='red', label='g(x): Bad Trial Distribution')
axes[1, 0].scatter(bad_x, np.zeros_like(bad_x),
                   c='red', s=50, marker='|', linewidths=2)
axes[1, 0].set_xlabel('Hyperparameter x')
axes[1, 0].set_ylabel('Density')
axes[1, 0].set_title('Bad Trial Distribution g(x)', fontsize=12)
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# 4. Acquisition function EI ‚àù ‚Ñì(x) / g(x)
axes[1, 1].plot(x_range, ei_approx, 'purple', linewidth=2,
                label='EI ‚àù ‚Ñì(x) / g(x)')
next_x = x_range[np.argmax(ei_approx)]
axes[1, 1].axvline(x=next_x, color='purple', linestyle='--',
                   linewidth=2, label=f'Next Search Point: {next_x:.2f}')
axes[1, 1].set_xlabel('Hyperparameter x')
axes[1, 1].set_ylabel('Acquisition Function Value')
axes[1, 1].set_title('Acquisition Function (TPE)', fontsize=12)
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\n=== TPE Operation ===")
print(f"Total trials: {n_trials}")
print(f"Good trials: {len(good_x)}")
print(f"Bad trials: {len(bad_x)}")
print(f"Threshold: {threshold_value:.2f}")
print(f"Next search point: {next_x:.2f}")
</code></pre>
<hr/>
<h2>2.3 Optuna Basics</h2>
<h3>What is Optuna</h3>
<p><strong>Optuna</strong> is a hyperparameter optimization framework developed by Preferred Networks.</p>
<p>Features:</p>
<ul>
<li>Define-by-Run API: Dynamic search space definition</li>
<li>Efficient algorithms: TPE by default</li>
<li>Pruning: Efficiency through early termination</li>
<li>Parallelization: Supports distributed optimization</li>
<li>Visualization: Rich plotting capabilities</li>
</ul>
<h3>Installation</h3>
<pre><code class="language-bash"># Basic installation
pip install optuna

# With visualization
pip install optuna[visualization]

# PyTorch integration
pip install optuna[pytorch]
</code></pre>
<h3>Basic Concepts</h3>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Study</strong></td>
<td>The entire optimization task. Manages multiple Trials</td>
</tr>
<tr>
<td><strong>Trial</strong></td>
<td>A single trial. A combination of hyperparameters</td>
</tr>
<tr>
<td><strong>Objective</strong></td>
<td>The objective function to minimize or maximize</td>
</tr>
<tr>
<td><strong>Sampler</strong></td>
<td>Hyperparameter sampling strategy (such as TPE)</td>
</tr>
<tr>
<td><strong>Pruner</strong></td>
<td>Early termination of unpromising trials based on intermediate progress</td>
</tr>
</tbody>
</table>
<div class="mermaid">
graph TD
    A[Create Study] --&gt; B[Define Objective Function]
    B --&gt; C[Start Trial]
    C --&gt; D[Get Parameters with suggest_*]
    D --&gt; E[Train Model]
    E --&gt; F[Return Evaluation Metric]
    F --&gt; G{Optimization Finished?}
    G --&gt;|No| C
    G --&gt;|Yes| H[Get Best Parameters]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
    style F fill:#ffebee
    style G fill:#f3e5f5
    style H fill:#c8e6c9
</div>
<h3>Basic Optimization Example</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - optuna&gt;=3.2.0

"""
Example: Basic Optimization Example

Purpose: Demonstrate optimization techniques
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

import optuna
from sklearn.datasets import load_iris
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

# Prepare data
iris = load_iris()
X, y = iris.data, iris.target

# Define Objective function
def objective(trial):
    # Suggest hyperparameters
    n_estimators = trial.suggest_int('n_estimators', 10, 100)
    max_depth = trial.suggest_int('max_depth', 2, 32, log=True)
    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)
    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)

    # Train and evaluate model
    clf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        random_state=42
    )

    # Cross-validation
    score = cross_val_score(clf, X, y, cv=3, scoring='accuracy').mean()

    return score

# Create Study and optimize
study = optuna.create_study(
    direction='maximize',  # Maximize accuracy
    sampler=optuna.samplers.TPESampler(seed=42)
)

study.optimize(objective, n_trials=50)

# Display results
print("\n=== Optuna Optimization Results ===")
print(f"Best accuracy: {study.best_value:.4f}")
print(f"Best parameters:")
for key, value in study.best_params.items():
    print(f"  {key}: {value}")

print(f"\nTotal trials: {len(study.trials)}")
print(f"Completed trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Optuna Optimization Results ===
Best accuracy: 0.9733
Best parameters:
  n_estimators: 87
  max_depth: 8
  min_samples_split: 2
  min_samples_leaf: 1

Total trials: 50
Completed trials: 50
</code></pre>
<hr/>
<h2>2.4 Optuna Practical Techniques</h2>
<h3>Defining Search Space</h3>
<p>Optuna provides various <code>suggest_*</code> methods:</p>
<h4>suggest Method List</h4>
<table>
<thead>
<tr>
<th>Method</th>
<th>Purpose</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>suggest_int</code></td>
<td>Integer values</td>
<td><code>trial.suggest_int('n_layers', 1, 5)</code></td>
</tr>
<tr>
<td><code>suggest_float</code></td>
<td>Floating point</td>
<td><code>trial.suggest_float('lr', 1e-5, 1e-1, log=True)</code></td>
</tr>
<tr>
<td><code>suggest_categorical</code></td>
<td>Categorical</td>
<td><code>trial.suggest_categorical('optimizer', ['adam', 'sgd'])</code></td>
</tr>
<tr>
<td><code>suggest_uniform</code></td>
<td>Uniform distribution (deprecated, use float)</td>
<td><code>trial.suggest_float('dropout', 0.0, 0.5)</code></td>
</tr>
<tr>
<td><code>suggest_loguniform</code></td>
<td>Log-uniform distribution (deprecated, use float+log)</td>
<td><code>trial.suggest_float('lr', 1e-5, 1e-1, log=True)</code></td>
</tr>
</tbody>
</table>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - optuna&gt;=3.2.0

"""
Example: suggest Method List

Purpose: Demonstrate optimization techniques
Target: Beginner to Intermediate
Execution time: 10-30 seconds
Dependencies: None
"""

import optuna

def objective_comprehensive(trial):
    # Integer (linear scale)
    batch_size = trial.suggest_int('batch_size', 16, 128, step=16)

    # Integer (log scale) - effective for large ranges
    hidden_size = trial.suggest_int('hidden_size', 32, 512, log=True)

    # Float (linear scale)
    dropout_rate = trial.suggest_float('dropout', 0.0, 0.5)

    # Float (log scale) - effective for learning rates
    learning_rate = trial.suggest_float('lr', 1e-5, 1e-1, log=True)

    # Categorical variables
    optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'sgd', 'rmsprop'])
    activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'sigmoid'])

    # Conditional parameters
    if optimizer_name == 'sgd':
        momentum = trial.suggest_float('momentum', 0.0, 0.99)
    else:
        momentum = None

    print(f"\n--- Trial {trial.number} ---")
    print(f"batch_size: {batch_size}")
    print(f"hidden_size: {hidden_size}")
    print(f"dropout: {dropout_rate:.4f}")
    print(f"lr: {learning_rate:.6f}")
    print(f"optimizer: {optimizer_name}")
    print(f"activation: {activation}")
    if momentum is not None:
        print(f"momentum: {momentum:.4f}")

    # Dummy evaluation value
    score = 0.85 + 0.1 * (learning_rate / 1e-1)

    return score

# Execution example
study = optuna.create_study(direction='maximize')
study.optimize(objective_comprehensive, n_trials=5, show_progress_bar=True)
</code></pre>
<h3>Utilizing Pruning</h3>
<p><strong>Pruning</strong> is a feature that terminates unpromising trials early during training. It's particularly effective for deep learning.</p>
<h4>Major Pruners</h4>
<table>
<thead>
<tr>
<th>Pruner</th>
<th>Description</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MedianPruner</strong></td>
<td>Prunes trials below median</td>
<td>General purpose</td>
</tr>
<tr>
<td><strong>PercentilePruner</strong></td>
<td>Prunes below specified percentile</td>
<td>More conservative/aggressive pruning</td>
</tr>
<tr>
<td><strong>SuccessiveHalvingPruner</strong></td>
<td>Allocates resources progressively</td>
<td>Many trials</td>
</tr>
<tr>
<td><strong>HyperbandPruner</strong></td>
<td>Improved version of Successive Halving</td>
<td>Large-scale optimization</td>
</tr>
</tbody>
</table>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - optuna&gt;=3.2.0

"""
Example: Major Pruners

Purpose: Demonstrate neural network implementation
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

import optuna
from optuna.pruners import MedianPruner
import numpy as np
import time

def objective_with_pruning(trial):
    # Suggest hyperparameters
    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)
    n_layers = trial.suggest_int('n_layers', 1, 5)

    # Simulation per epoch
    n_epochs = 20

    for epoch in range(n_epochs):
        # Dummy performance (gradual improvement)
        # Bad hyperparameters improve slowly
        score = 0.5 + 0.5 * (epoch / n_epochs) * lr * n_layers / 5
        score += np.random.normal(0, 0.05)  # Noise

        # Report intermediate progress
        trial.report(score, epoch)

        # Pruning decision
        if trial.should_prune():
            print(f"  Trial {trial.number} pruned at epoch {epoch}")
            raise optuna.TrialPruned()

        time.sleep(0.05)  # Training simulation

    return score

# Use MedianPruner
study = optuna.create_study(
    direction='maximize',
    pruner=MedianPruner(
        n_startup_trials=5,  # Don't prune first 5 trials
        n_warmup_steps=5,    # Don't prune first 5 steps
        interval_steps=1     # Check every step
    )
)

print("=== Optimization with Pruning ===")
study.optimize(objective_with_pruning, n_trials=20, show_progress_bar=False)

# Analyze results
n_complete = len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])
n_pruned = len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])

print(f"\nCompleted trials: {n_complete}")
print(f"Pruned trials: {n_pruned}")
print(f"Reduction rate: {n_pruned / len(study.trials) * 100:.1f}%")
print(f"\nBest accuracy: {study.best_value:.4f}")
print(f"Best parameters: {study.best_params}")
</code></pre>
<p><strong>Example Output</strong>:</p>
<pre><code>=== Optimization with Pruning ===
  Trial 5 pruned at epoch 7
  Trial 7 pruned at epoch 6
  Trial 9 pruned at epoch 8
  ...

Completed trials: 12
Pruned trials: 8
Reduction rate: 40.0%

Best accuracy: 0.9234
Best parameters: {'lr': 0.08234, 'n_layers': 5}
</code></pre>
<blockquote>
<p><strong>Effect</strong>: Pruning reduced unnecessary computation by 40%.</p>
</blockquote>
<h3>Parallel Optimization</h3>
<p>Optuna allows easy parallel optimization:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - joblib&gt;=1.3.0
# - optuna&gt;=3.2.0

"""
Example: Optuna allows easy parallel optimization:

Purpose: Demonstrate optimization techniques
Target: Advanced
Execution time: 10-30 seconds
Dependencies: None
"""

import optuna
from joblib import Parallel, delayed

def objective(trial):
    x = trial.suggest_float('x', -10, 10)
    return (x - 2) ** 2

# Method 1: n_jobs parameter
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100, n_jobs=4)  # 4 parallel

# Method 2: Shared storage (RDB)
storage = 'sqlite:///optuna_study.db'
study = optuna.create_study(
    study_name='parallel_optimization',
    storage=storage,
    load_if_exists=True
)

# Can execute simultaneously from multiple processes
study.optimize(objective, n_trials=50)
</code></pre>
<hr/>
<h2>2.5 Practice: Tuning Deep Learning Models</h2>
<h3>PyTorch Model Integration with Optuna</h3>
<p>Here's a complete example of utilizing Optuna with an actual deep learning model.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - optuna&gt;=3.2.0
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: Here's a complete example of utilizing Optuna with an actual

Purpose: Demonstrate machine learning model training and evaluation
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

import optuna
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Prepare data
X, y = make_classification(
    n_samples=5000, n_features=20, n_informative=15,
    n_redundant=5, n_classes=2, random_state=42
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Convert to PyTorch tensors
X_train_t = torch.FloatTensor(X_train).to(device)
y_train_t = torch.LongTensor(y_train).to(device)
X_test_t = torch.FloatTensor(X_test).to(device)
y_test_t = torch.LongTensor(y_test).to(device)

# Model definition function
def create_model(trial, input_size, output_size):
    # Suggest hyperparameters
    n_layers = trial.suggest_int('n_layers', 1, 4)
    hidden_sizes = []

    for i in range(n_layers):
        hidden_size = trial.suggest_int(f'hidden_size_l{i}', 32, 256, log=True)
        hidden_sizes.append(hidden_size)

    dropout_rate = trial.suggest_float('dropout', 0.0, 0.5)
    activation_name = trial.suggest_categorical('activation', ['relu', 'tanh', 'elu'])

    # Select activation function
    if activation_name == 'relu':
        activation = nn.ReLU()
    elif activation_name == 'tanh':
        activation = nn.Tanh()
    else:
        activation = nn.ELU()

    # Build network
    layers = []
    in_features = input_size

    for hidden_size in hidden_sizes:
        layers.append(nn.Linear(in_features, hidden_size))
        layers.append(activation)
        layers.append(nn.Dropout(dropout_rate))
        in_features = hidden_size

    layers.append(nn.Linear(in_features, output_size))

    model = nn.Sequential(*layers)
    return model

# Objective function
def objective(trial):
    # Create model
    model = create_model(trial, input_size=20, output_size=2).to(device)

    # Optimizer hyperparameters
    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])
    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)

    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=lr)
    elif optimizer_name == 'SGD':
        momentum = trial.suggest_float('momentum', 0.0, 0.99)
        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)
    else:
        optimizer = optim.RMSprop(model.parameters(), lr=lr)

    # Batch size
    batch_size = trial.suggest_int('batch_size', 16, 256, step=16)

    # DataLoader
    train_dataset = TensorDataset(X_train_t, y_train_t)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    n_epochs = 20

    for epoch in range(n_epochs):
        model.train()
        train_loss = 0.0

        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        # Validation (test set)
        model.eval()
        with torch.no_grad():
            outputs = model(X_test_t)
            _, predicted = torch.max(outputs.data, 1)
            accuracy = (predicted == y_test_t).sum().item() / len(y_test_t)

        # Report intermediate progress (for Pruning)
        trial.report(accuracy, epoch)

        # Pruning decision
        if trial.should_prune():
            raise optuna.TrialPruned()

    return accuracy

# Create Study and optimize
study = optuna.create_study(
    direction='maximize',
    sampler=optuna.samplers.TPESampler(seed=42),
    pruner=optuna.pruners.MedianPruner(
        n_startup_trials=10,
        n_warmup_steps=5
    )
)

print("\n=== Deep Learning Model Optimization Started ===")
study.optimize(objective, n_trials=50, timeout=600)

# Display results
print("\n=== Optimization Complete ===")
print(f"Best accuracy: {study.best_value:.4f}")
print(f"\nBest hyperparameters:")
for key, value in study.best_params.items():
    print(f"  {key}: {value}")

# Statistics
n_complete = len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])
n_pruned = len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])
print(f"\nCompleted trials: {n_complete}")
print(f"Pruned trials: {n_pruned}")
</code></pre>
<h3>Visualization</h3>
<p>Optuna provides powerful visualization capabilities:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - optuna&gt;=3.2.0

"""
Example: Optuna provides powerful visualization capabilities:

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 2-5 seconds
Dependencies: None
"""

import optuna
from optuna.visualization import (
    plot_optimization_history,
    plot_param_importances,
    plot_slice,
    plot_parallel_coordinate,
    plot_contour
)
import matplotlib.pyplot as plt

# 1. Optimization history
fig = plot_optimization_history(study)
fig.show()

# 2. Parameter importance
fig = plot_param_importances(study)
fig.show()

# 3. Slice plot (effect of each parameter)
fig = plot_slice(study)
fig.show()

# 4. Parallel coordinate plot
fig = plot_parallel_coordinate(study)
fig.show()

# 5. Contour plot (2D relationship)
fig = plot_contour(study, params=['lr', 'n_layers'])
fig.show()

# Custom Plot with Matplotlib
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. Accuracy per trial
trial_numbers = [t.number for t in study.trials]
values = [t.value for t in study.trials if t.value is not None]
axes[0, 0].plot(trial_numbers[:len(values)], values, 'o-', alpha=0.6)
axes[0, 0].axhline(y=study.best_value, color='r',
                   linestyle='--', label=f'Best: {study.best_value:.4f}')
axes[0, 0].set_xlabel('Trial Number')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].set_title('Accuracy Progress per Trial')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# 2. Learning rate vs accuracy
lrs = [t.params['lr'] for t in study.trials if t.value is not None]
values = [t.value for t in study.trials if t.value is not None]
axes[0, 1].scatter(lrs, values, alpha=0.6, s=50, edgecolors='black')
axes[0, 1].set_xscale('log')
axes[0, 1].set_xlabel('Learning Rate')
axes[0, 1].set_ylabel('Accuracy')
axes[0, 1].set_title('Learning Rate vs Accuracy')
axes[0, 1].grid(True, alpha=0.3)

# 3. Number of layers vs accuracy
n_layers_list = [t.params['n_layers'] for t in study.trials if t.value is not None]
values = [t.value for t in study.trials if t.value is not None]
axes[1, 0].scatter(n_layers_list, values, alpha=0.6, s=50, edgecolors='black')
axes[1, 0].set_xlabel('Number of Layers')
axes[1, 0].set_ylabel('Accuracy')
axes[1, 0].set_title('Number of Layers vs Accuracy')
axes[1, 0].grid(True, alpha=0.3)

# 4. Batch size vs accuracy
batch_sizes = [t.params['batch_size'] for t in study.trials if t.value is not None]
values = [t.value for t in study.trials if t.value is not None]
axes[1, 1].scatter(batch_sizes, values, alpha=0.6, s=50, edgecolors='black')
axes[1, 1].set_xlabel('Batch Size')
axes[1, 1].set_ylabel('Accuracy')
axes[1, 1].set_title('Batch Size vs Accuracy')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>Optuna Dashboard</h3>
<p>Visualize results with an interactive web dashboard:</p>
<pre><code class="language-bash"># Installation
pip install optuna-dashboard

# Start dashboard
optuna-dashboard sqlite:///optuna_study.db
</code></pre>
<p>Access <code>http://127.0.0.1:8080</code> in your browser to monitor optimization progress in real-time.</p>
<hr/>
<h2>2.6 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li><p><strong>Bayesian Optimization Principles</strong></p>
<ul>
<li>Approximate objective function with surrogate model (Gaussian Process)</li>
<li>Determine next search point with acquisition function (EI, UCB, PI)</li>
<li>Efficient optimization through balance of exploration and exploitation</li>
</ul></li>
<li><p><strong>TPE Algorithm</strong></p>
<ul>
<li>Efficient method modeling P(x|y)</li>
<li>Strong with high dimensions and categorical variables</li>
<li>Easy parallelization</li>
</ul></li>
<li><p><strong>Optuna Basics</strong></p>
<ul>
<li>Concepts of Study, Trial, Objective</li>
<li>Flexible search space definition with Define-by-Run API</li>
<li>Rich suggest_* methods</li>
</ul></li>
<li><p><strong>Practical Techniques</strong></p>
<ul>
<li>Reduce computation time with Pruning</li>
<li>Speed up with parallel optimization</li>
<li>Handling conditional hyperparameters</li>
</ul></li>
<li><p><strong>Deep Learning Applications</strong></p>
<ul>
<li>PyTorch model integration</li>
<li>Optimization of learning rate, architecture, optimizer</li>
<li>Gaining insights through visualization</li>
</ul></li>
</ol>
<h3>Bayesian Optimization vs Random Search</h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Random Search</th>
<th>Bayesian Optimization (Optuna)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Number of Trials</strong></td>
<td>Many required</td>
<td>Converges with few</td>
</tr>
<tr>
<td><strong>Use of Past Information</strong></td>
<td>None</td>
<td>Yes (surrogate model)</td>
</tr>
<tr>
<td><strong>Computational Cost</strong></td>
<td>Low</td>
<td>Somewhat high (TPE is lightweight)</td>
</tr>
<tr>
<td><strong>High-dimensional Performance</strong></td>
<td>Good</td>
<td>TPE is good, GP degrades</td>
</tr>
<tr>
<td><strong>Parallelization</strong></td>
<td>Easy</td>
<td>Easy (Optuna)</td>
</tr>
<tr>
<td><strong>Implementation Complexity</strong></td>
<td>Simple</td>
<td>Easy with Optuna</td>
</tr>
</tbody>
</table>
<h3>Recommended Use Cases</h3>
<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommended Method</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>High training cost</td>
<td>Optuna + Pruning</td>
<td>Efficiency through early termination</td>
</tr>
<tr>
<td>Low dimensions (&lt; 10)</td>
<td>Grid Search or Optuna</td>
<td>Both effective</td>
</tr>
<tr>
<td>High dimensions (&gt; 20)</td>
<td>Optuna (TPE)</td>
<td>Strong against curse of dimensionality</td>
</tr>
<tr>
<td>Many categorical variables</td>
<td>Optuna</td>
<td>Naturally handled</td>
</tr>
<tr>
<td>Initial exploration</td>
<td>Random Search</td>
<td>Simple and fast</td>
</tr>
<tr>
<td>Final tuning</td>
<td>Optuna</td>
<td>Precise optimization</td>
</tr>
</tbody>
</table>
<h3>Next Chapter</h3>
<p>In Chapter 3, we'll learn about <strong>Automated Machine Learning (AutoML)</strong>:</p>
<ul>
<li>Auto-sklearn: Automatic model selection and ensembling</li>
<li>H2O AutoML: For large-scale data</li>
<li>PyCaret: Low-code ML</li>
<li>TPOT: Genetic programming</li>
<li>AutoML limitations and use cases</li>
</ul>
<hr/>
<h2>References</h2>
<ol>
<li>Akiba, T., Sano, S., Yanase, T., Ohta, T., &amp; Koyama, M. (2019). Optuna: A Next-generation Hyperparameter Optimization Framework. <em>Proceedings of the 25th ACM SIGKDD</em>.</li>
<li>Bergstra, J., Bardenet, R., Bengio, Y., &amp; K√©gl, B. (2011). Algorithms for Hyper-Parameter Optimization. <em>NIPS</em>.</li>
<li>Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., &amp; de Freitas, N. (2016). Taking the Human Out of the Loop: A Review of Bayesian Optimization. <em>Proceedings of the IEEE</em>, 104(1), 148-175.</li>
<li>Snoek, J., Larochelle, H., &amp; Adams, R. P. (2012). Practical Bayesian Optimization of Machine Learning Algorithms. <em>NIPS</em>.</li>
<li>Falkner, S., Klein, A., &amp; Hutter, F. (2018). BOHB: Robust and Efficient Hyperparameter Optimization at Scale. <em>ICML</em>.</li>
</ol>
<div class="navigation">
<a class="nav-button" href="chapter1-grid-random-search.html">‚Üê Previous Chapter: Grid and Random Search</a>
<a class="nav-button" href="chapter3-automl.html">Next Chapter: AutoML ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The authors and Tohoku University assume no responsibility for the content, availability, or safety of external links, data, tools, or libraries provided by third parties.</li>
<li>To the maximum extent permitted by applicable law, the authors and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material is subject to change, update, or discontinuation without notice.</li>
<li>The copyright and license of this content shall be governed by the specified terms (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
</ul>
</section>
<footer>
<p><strong>Created by</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-21</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
