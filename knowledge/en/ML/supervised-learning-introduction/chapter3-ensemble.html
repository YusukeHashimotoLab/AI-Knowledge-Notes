<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
<meta content="Chapter 3: Ensemble Methods - AI Terakoya" name="description"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 3: Ensemble Methods - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: var(--font-body); line-height: 1.7; color: var(--color-text); background-color: var(--color-bg); font-size: 16px; }
        header { background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; padding: var(--spacing-xl) var(--spacing-md); margin-bottom: var(--spacing-xl); box-shadow: var(--box-shadow); }
        .header-content { max-width: 900px; margin: 0 auto; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: var(--spacing-sm); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; opacity: 0.95; font-weight: 400; margin-bottom: var(--spacing-md); }
        .meta { display: flex; flex-wrap: wrap; gap: var(--spacing-md); font-size: 0.9rem; opacity: 0.9; }
        .meta-item { display: flex; align-items: center; gap: 0.3rem; }
        .container { max-width: 900px; margin: 0 auto; padding: 0 var(--spacing-md) var(--spacing-xl); }
        h2 { font-size: 1.75rem; color: var(--color-primary); margin-top: var(--spacing-xl); margin-bottom: var(--spacing-md); padding-bottom: var(--spacing-xs); border-bottom: 3px solid var(--color-accent); }
        h3 { font-size: 1.4rem; color: var(--color-primary); margin-top: var(--spacing-lg); margin-bottom: var(--spacing-sm); }
        h4 { font-size: 1.1rem; color: var(--color-primary-dark); margin-top: var(--spacing-md); margin-bottom: var(--spacing-sm); }
        p { margin-bottom: var(--spacing-md); color: var(--color-text); }
        a { color: var(--color-link); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--color-link-hover); text-decoration: underline; }
        ul, ol { margin-left: var(--spacing-lg); margin-bottom: var(--spacing-md); }
        li { margin-bottom: var(--spacing-xs); color: var(--color-text); }
        pre { background-color: var(--color-code-bg); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); overflow-x: auto; margin-bottom: var(--spacing-md); font-family: var(--font-mono); font-size: 0.9rem; line-height: 1.5; }
        code { font-family: var(--font-mono); font-size: 0.9em; background-color: var(--color-code-bg); padding: 0.2em 0.4em; border-radius: 3px; }
        pre code { background-color: transparent; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: var(--spacing-md); font-size: 0.95rem; }
        th, td { border: 1px solid var(--color-border); padding: var(--spacing-sm); text-align: left; }
        th { background-color: var(--color-bg-alt); font-weight: 600; color: var(--color-primary); }
        blockquote { border-left: 4px solid var(--color-accent); padding-left: var(--spacing-md); margin: var(--spacing-md) 0; color: var(--color-text-light); font-style: italic; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        .mermaid { text-align: center; margin: var(--spacing-lg) 0; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        details { background-color: var(--color-bg-alt); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); margin-bottom: var(--spacing-md); }
        summary { cursor: pointer; font-weight: 600; color: var(--color-primary); user-select: none; padding: var(--spacing-xs); margin: calc(-1 * var(--spacing-md)); padding: var(--spacing-md); border-radius: var(--border-radius); }
        summary:hover { background-color: rgba(123, 44, 191, 0.1); }
        details[open] summary { margin-bottom: var(--spacing-md); border-bottom: 1px solid var(--color-border); }
        .navigation { display: flex; justify-content: space-between; gap: var(--spacing-md); margin: var(--spacing-xl) 0; padding-top: var(--spacing-lg); border-top: 2px solid var(--color-border); }
        .nav-button { flex: 1; padding: var(--spacing-md); background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; border-radius: var(--border-radius); text-align: center; font-weight: 600; transition: transform 0.2s, box-shadow 0.2s; box-shadow: var(--box-shadow); }
        .nav-button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); text-decoration: none; }
        footer { margin-top: var(--spacing-xl); padding: var(--spacing-lg) var(--spacing-md); background-color: var(--color-bg-alt); border-top: 1px solid var(--color-border); text-align: center; font-size: 0.9rem; color: var(--color-text-light); }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "!";
            position: absolute;
            left: 0;
            font-weight: bold;
        }

        @media (max-width: 768px) { h1 { font-size: 1.5rem; } h2 { font-size: 1.4rem; } h3 { font-size: 1.2rem; } .meta { font-size: 0.85rem; } .navigation { flex-direction: column; } table { font-size: 0.85rem; } th, td { padding: var(--spacing-xs); } }



        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }

        /* Locale switcher styles */
        .locale-switcher {
            max-width: 900px;
            margin: 0 auto;
            padding: 0.5rem 1rem;
            text-align: right;
            font-size: 0.85rem;
        }

        .current-locale {
            font-weight: 600;
            color: #4a5568;
        }

        .locale-separator {
            color: #a0aec0;
            margin: 0 0.5rem;
        }

        .locale-link {
            color: #667eea;
            text-decoration: none;
        }

        .locale-link:hover {
            text-decoration: underline;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Home</a><span class="breadcrumb-separator">></span><a href="../../index.html">Machine Learning</a><span class="breadcrumb-separator">></span><a href="../../ML/supervised-learning-introduction/index.html">Supervised Learning</a><span class="breadcrumb-separator">></span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>
<div class="locale-switcher">
<span class="current-locale">EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/ML/supervised-learning-introduction/chapter3-ensemble.html" class="locale-link">JP</a>
</div>

        <header>
        <div class="header-content">
            <h1>Chapter 3: Ensemble Methods</h1>
            <p class="subtitle">Performance Enhancement Through Model Combination - From Random Forest to XGBoost, LightGBM, and CatBoost</p>
            <div class="meta">
                <span class="meta-item">Reading Time: 25-30 min</span>
                <span class="meta-item">Difficulty: Intermediate</span>
                <span class="meta-item">Code Examples: 13</span>
                <span class="meta-item">Exercises: 5</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Learning Objectives</h2>
<p>By completing this chapter, you will be able to:</p>
<ul>
<li>Understand the principles of ensemble learning</li>
<li>Explain the differences between Bagging and Boosting</li>
<li>Implement Random Forest and analyze feature importance</li>
<li>Understand the mechanics of Gradient Boosting</li>
<li>Master XGBoost, LightGBM, and CatBoost</li>
<li>Acquire practical techniques used in Kaggle competitions</li>
</ul>

<hr>

<h2>3.1 What is Ensemble Learning?</h2>

<h3>Definition</h3>
<p><strong>Ensemble Learning</strong> is a method that combines multiple learners (models) to achieve higher performance than any single model.</p>

<blockquote>
<p>"Two heads are better than one" - Combining multiple weak learners to build a powerful predictor</p>
</blockquote>

<h3>Benefits of Ensemble Methods</h3>

<div class="mermaid">
graph LR
    A[Ensemble Benefits] --> B[Improved Accuracy]
    A --> C[Overfitting Prevention]
    A --> D[Improved Stability]
    A --> E[Enhanced Robustness]

    B --> B1[Higher accuracy than single models]
    C --> C1[Reduces variance]
    D --> D1[Reduces prediction variability]
    E --> E1[Robust to outliers and noise]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#ffe0b2
</div>

<h3>Main Approaches</h3>

<table>
<thead>
<tr>
<th>Method</th>
<th>Principle</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Bagging</strong></td>
<td>Parallel learning, averaging</td>
<td>Random Forest</td>
</tr>
<tr>
<td><strong>Boosting</strong></td>
<td>Sequential learning, error correction</td>
<td>XGBoost, LightGBM, CatBoost</td>
</tr>
<tr>
<td><strong>Stacking</strong></td>
<td>Integration via meta-learner</td>
<td>Level-wise Stacking</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.2 Bagging (Bootstrap Aggregating)</h2>

<h3>Principle</h3>

<p><strong>Bagging</strong> creates multiple datasets through bootstrap sampling and averages predictions from models trained on each dataset.</p>

<div class="mermaid">
graph TD
    A[Training Data] --> B[Bootstrap<br/>Sampling]
    B --> C1[Sample 1]
    B --> C2[Sample 2]
    B --> C3[Sample 3]
    C1 --> D1[Model 1]
    C2 --> D2[Model 2]
    C3 --> D3[Model 3]
    D1 --> E[Voting/Averaging]
    D2 --> E
    D3 --> E
    E --> F[Final Prediction]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style E fill:#f3e5f5
    style F fill:#e8f5e9
</div>

<h3>Algorithm</h3>

<ol>
<li>Create T bootstrap samples from training data through sampling with replacement</li>
<li>Train learners independently on each sample</li>
<li>Classification: majority voting, Regression: averaging for final prediction</li>
</ol>

<p>$$
\hat{y} = \frac{1}{T} \sum_{t=1}^{T} f_t(\mathbf{x})
$$</p>

<h3>Implementation Example</h3>

<pre><code class="language-python">import numpy as np
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Generate data
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,
                          n_redundant=5, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Bagging
bagging_model = BaggingClassifier(
    estimator=DecisionTreeClassifier(),
    n_estimators=100,  # Number of learners
    max_samples=0.8,   # Sampling ratio
    random_state=42
)

bagging_model.fit(X_train, y_train)
y_pred = bagging_model.predict(X_test)

print("=== Bagging ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")

# Compare with single decision tree
single_tree = DecisionTreeClassifier(random_state=42)
single_tree.fit(X_train, y_train)
y_pred_single = single_tree.predict(X_test)

print(f"\nSingle Decision Tree Accuracy: {accuracy_score(y_test, y_pred_single):.4f}")
print(f"Improvement: {accuracy_score(y_test, y_pred) - accuracy_score(y_test, y_pred_single):.4f}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Bagging ===
Accuracy: 0.8950

Single Decision Tree Accuracy: 0.8300
Improvement: 0.0650
</code></pre>

<hr>

<h2>3.3 Random Forest</h2>

<h3>Overview</h3>

<p><strong>Random Forest</strong> is an ensemble method that adds random feature selection to Bagging. It builds a forest of decision trees.</p>

<h3>Differences Between Random Forest and Bagging</h3>

<table>
<thead>
<tr>
<th>Item</th>
<th>Bagging</th>
<th>Random Forest</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Sampling</strong></td>
<td>Data only</td>
<td>Data + Features</td>
</tr>
<tr>
<td><strong>Feature Selection</strong></td>
<td>Uses all features</td>
<td>Randomly selects subset</td>
</tr>
<tr>
<td><strong>Diversity</strong></td>
<td>Moderate</td>
<td>High</td>
</tr>
<tr>
<td><strong>Overfitting</strong></td>
<td>Somewhat prone</td>
<td>Less prone</td>
</tr>
</tbody>
</table>

<h3>Implementation Example</h3>

<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt

# Random Forest
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    max_features='sqrt',  # Randomly select sqrt(n) features
    random_state=42
)

rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

print("=== Random Forest ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}")

# Feature Importance
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1][:10]  # Top 10

plt.figure(figsize=(12, 6))
plt.bar(range(10), importances[indices])
plt.xlabel('Feature Index', fontsize=12)
plt.ylabel('Importance', fontsize=12)
plt.title('Random Forest: Feature Importance (Top 10)', fontsize=14)
plt.xticks(range(10), indices)
plt.grid(axis='y', alpha=0.3)
plt.show()

print(f"\nTop 5 Important Features:")
for i in range(5):
    print(f"  Feature {indices[i]}: {importances[indices[i]]:.4f}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Random Forest ===
Accuracy: 0.9100

Top 5 Important Features:
  Feature 2: 0.0852
  Feature 7: 0.0741
  Feature 13: 0.0689
  Feature 5: 0.0634
  Feature 19: 0.0598
</code></pre>

<h3>Out-of-Bag (OOB) Evaluation</h3>

<p>You can evaluate using data not used in bootstrap sampling (approximately 37%).</p>

<pre><code class="language-python"># OOB Score
rf_oob = RandomForestClassifier(
    n_estimators=100,
    oob_score=True,
    random_state=42
)

rf_oob.fit(X_train, y_train)

print(f"OOB Score: {rf_oob.oob_score_:.4f}")
print(f"Test Score: {rf_oob.score(X_test, y_test):.4f}")
</code></pre>

<hr>

<h2>3.4 Boosting</h2>

<h3>Overview</h3>

<p><strong>Boosting</strong> is a method that sequentially trains weak learners, with each subsequent model correcting the errors of the previous one.</p>

<div class="mermaid">
graph LR
    A[Data] --> B[Model 1]
    B --> C[Error Calculation]
    C --> D[Weight Update]
    D --> E[Model 2]
    E --> F[Error Calculation]
    F --> G[Weight Update]
    G --> H[Model 3]
    H --> I[...]
    I --> J[Final Model]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style E fill:#fff3e0
    style H fill:#fff3e0
    style J fill:#e8f5e9
</div>

<h3>Differences Between Bagging and Boosting</h3>

<table>
<thead>
<tr>
<th>Item</th>
<th>Bagging</th>
<th>Boosting</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Learning Method</strong></td>
<td>Parallel (independent)</td>
<td>Sequential (dependent)</td>
</tr>
<tr>
<td><strong>Objective</strong></td>
<td>Variance reduction</td>
<td>Bias reduction</td>
</tr>
<tr>
<td><strong>Weights</strong></td>
<td>Equal</td>
<td>Error-based</td>
</tr>
<tr>
<td><strong>Overfitting</strong></td>
<td>Less prone</td>
<td>More prone</td>
</tr>
<tr>
<td><strong>Training Speed</strong></td>
<td>Fast (parallelizable)</td>
<td>Slow (sequential)</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.5 Gradient Boosting</h2>

<h3>Principle</h3>

<p><strong>Gradient Boosting</strong> uses gradient descent to minimize the loss function. It learns residuals (actual value - predicted value) in subsequent models.</p>

<p>$$
F_m(\mathbf{x}) = F_{m-1}(\mathbf{x}) + \nu \cdot h_m(\mathbf{x})
$$</p>

<ul>
<li>$F_m$: The m-th ensemble model</li>
<li>$\nu$: Learning rate</li>
<li>$h_m$: The m-th weak learner (learns residuals)</li>
</ul>

<h3>Implementation Example</h3>

<pre><code class="language-python">from sklearn.ensemble import GradientBoostingClassifier

# Gradient Boosting
gb_model = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)

gb_model.fit(X_train, y_train)
y_pred_gb = gb_model.predict(X_test)

print("=== Gradient Boosting ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred_gb):.4f}")

# Learning Curve
train_scores = []
test_scores = []

for i, y_pred in enumerate(gb_model.staged_predict(X_train)):
    train_scores.append(accuracy_score(y_train, y_pred))

for i, y_pred in enumerate(gb_model.staged_predict(X_test)):
    test_scores.append(accuracy_score(y_test, y_pred))

plt.figure(figsize=(10, 6))
plt.plot(train_scores, label='Training Data', linewidth=2)
plt.plot(test_scores, label='Test Data', linewidth=2)
plt.xlabel('Boosting Round', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.title('Gradient Boosting: Learning Curve', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Gradient Boosting ===
Accuracy: 0.9250
</code></pre>

<hr>

<h2>3.6 XGBoost</h2>

<h3>Overview</h3>

<p><strong>XGBoost (Extreme Gradient Boosting)</strong> is a fast and high-performance implementation of Gradient Boosting. It is one of the most widely used algorithms in Kaggle competitions.</p>

<h3>Features</h3>

<ul>
<li><strong>Regularization</strong>: L1/L2 regularization prevents overfitting</li>
<li><strong>Missing Value Handling</strong>: Automatically learns optimal splits</li>
<li><strong>Parallelization</strong>: Parallelizes tree construction</li>
<li><strong>Early Stopping</strong>: Detects overfitting and stops early</li>
<li><strong>Built-in Cross-Validation</strong></li>
</ul>

<h3>Implementation Example</h3>

<pre><code class="language-python">import xgboost as xgb

# XGBoost
xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    eval_metric='logloss'
)

# Early Stopping
eval_set = [(X_train, y_train), (X_test, y_test)]
xgb_model.fit(
    X_train, y_train,
    eval_set=eval_set,
    verbose=False
)

y_pred_xgb = xgb_model.predict(X_test)

print("=== XGBoost ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}")

# Visualize Training History
results = xgb_model.evals_result()

plt.figure(figsize=(10, 6))
plt.plot(results['validation_0']['logloss'], label='Training Data', linewidth=2)
plt.plot(results['validation_1']['logloss'], label='Test Data', linewidth=2)
plt.xlabel('Boosting Round', fontsize=12)
plt.ylabel('Log Loss', fontsize=12)
plt.title('XGBoost: Training History', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Feature Importance
xgb.plot_importance(xgb_model, max_num_features=10, importance_type='gain')
plt.title('XGBoost: Feature Importance (Top 10)')
plt.show()
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== XGBoost ===
Accuracy: 0.9350
</code></pre>

<h3>Hyperparameter Tuning</h3>

<pre><code class="language-python">from sklearn.model_selection import GridSearchCV

# Parameter Grid
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'n_estimators': [50, 100, 200],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# Grid Search
xgb_grid = GridSearchCV(
    xgb.XGBClassifier(random_state=42, eval_metric='logloss'),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

xgb_grid.fit(X_train, y_train)

print("=== XGBoost Grid Search ===")
print(f"Best Parameters: {xgb_grid.best_params_}")
print(f"Best Score (CV): {xgb_grid.best_score_:.4f}")
print(f"Test Score: {xgb_grid.score(X_test, y_test):.4f}")
</code></pre>

<hr>

<h2>3.7 LightGBM</h2>

<h3>Overview</h3>

<p><strong>LightGBM (Light Gradient Boosting Machine)</strong> is a fast Gradient Boosting framework developed by Microsoft.</p>

<h3>Features</h3>

<ul>
<li><strong>Leaf-wise Growth</strong>: More efficient than XGBoost's Level-wise approach</li>
<li><strong>GOSS</strong>: Gradient-based One-Side Sampling for speedup</li>
<li><strong>EFB</strong>: Exclusive Feature Bundling for memory reduction</li>
<li><strong>Categorical Variable Support</strong>: No One-Hot Encoding required</li>
<li><strong>Large-Scale Data</strong>: Fast even with millions of samples</li>
</ul>

<div class="mermaid">
graph LR
    A[Tree Growth Strategy] --> B[Level-wise<br/>XGBoost]
    A --> C[Leaf-wise<br/>LightGBM]

    B --> B1[Grows layer by layer<br/>Balanced]
    C --> C1[Maximum loss reduction<br/>Deeper trees]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#e8f5e9
</div>

<h3>Implementation Example</h3>

<pre><code class="language-python">import lightgbm as lgb

# LightGBM
lgb_model = lgb.LGBMClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    num_leaves=31,
    random_state=42
)

lgb_model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    eval_metric='logloss',
    verbose=False
)

y_pred_lgb = lgb_model.predict(X_test)

print("=== LightGBM ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred_lgb):.4f}")

# Feature Importance
lgb.plot_importance(lgb_model, max_num_features=10, importance_type='gain')
plt.title('LightGBM: Feature Importance (Top 10)')
plt.show()
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== LightGBM ===
Accuracy: 0.9350
</code></pre>

<hr>

<h2>3.8 CatBoost</h2>

<h3>Overview</h3>

<p><strong>CatBoost (Categorical Boosting)</strong> is a Gradient Boosting library developed by Yandex. It excels at handling categorical variables.</p>

<h3>Features</h3>

<ul>
<li><strong>Ordered Boosting</strong>: Prevents prediction shift</li>
<li><strong>Automatic Categorical Variable Processing</strong>: Improved version of Target Encoding</li>
<li><strong>Symmetric Trees</strong>: Fast predictions</li>
<li><strong>GPU Acceleration</strong>: Built-in GPU support</li>
<li><strong>Minimal Hyperparameter Tuning</strong>: High performance with defaults</li>
</ul>

<h3>Implementation Example</h3>

<pre><code class="language-python">from catboost import CatBoostClassifier

# CatBoost
cat_model = CatBoostClassifier(
    iterations=100,
    learning_rate=0.1,
    depth=5,
    random_state=42,
    verbose=False
)

cat_model.fit(
    X_train, y_train,
    eval_set=(X_test, y_test)
)

y_pred_cat = cat_model.predict(X_test)

print("=== CatBoost ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred_cat):.4f}")

# Feature Importance
feature_importances = cat_model.get_feature_importance()
indices = np.argsort(feature_importances)[::-1][:10]

plt.figure(figsize=(12, 6))
plt.bar(range(10), feature_importances[indices])
plt.xlabel('Feature Index', fontsize=12)
plt.ylabel('Importance', fontsize=12)
plt.title('CatBoost: Feature Importance (Top 10)', fontsize=14)
plt.xticks(range(10), indices)
plt.grid(axis='y', alpha=0.3)
plt.show()
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== CatBoost ===
Accuracy: 0.9400
</code></pre>

<hr>

<h2>3.9 Comparison of Ensemble Methods</h2>

<h3>Performance Comparison</h3>

<pre><code class="language-python"># Compare all models
models = {
    'Bagging': bagging_model,
    'Random Forest': rf_model,
    'Gradient Boosting': gb_model,
    'XGBoost': xgb_model,
    'LightGBM': lgb_model,
    'CatBoost': cat_model
}

results = {}
for name, model in models.items():
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc

# Visualization
plt.figure(figsize=(12, 6))
plt.bar(results.keys(), results.values(), color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c'])
plt.ylabel('Accuracy', fontsize=12)
plt.title('Ensemble Methods Performance Comparison', fontsize=14)
plt.ylim(0.8, 1.0)
plt.grid(axis='y', alpha=0.3)
for i, (name, acc) in enumerate(results.items()):
    plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center', fontsize=10)
plt.show()

print("=== Ensemble Methods Comparison ===")
for name, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):
    print(f"{name:20s}: {acc:.4f}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Ensemble Methods Comparison ===
CatBoost            : 0.9400
XGBoost             : 0.9350
LightGBM            : 0.9350
Gradient Boosting   : 0.9250
Random Forest       : 0.9100
Bagging             : 0.8950
</code></pre>

<h3>Feature Comparison</h3>

<table>
<thead>
<tr>
<th>Method</th>
<th>Training Speed</th>
<th>Prediction Speed</th>
<th>Accuracy</th>
<th>Memory</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Random Forest</strong></td>
<td>Fast</td>
<td>Fast</td>
<td>Medium</td>
<td>Large</td>
<td>Parallelization, Interpretability</td>
</tr>
<tr>
<td><strong>Gradient Boosting</strong></td>
<td>Slow</td>
<td>Fast</td>
<td>High</td>
<td>Medium</td>
<td>Simple</td>
</tr>
<tr>
<td><strong>XGBoost</strong></td>
<td>Medium</td>
<td>Fast</td>
<td>High</td>
<td>Medium</td>
<td>Kaggle standard</td>
</tr>
<tr>
<td><strong>LightGBM</strong></td>
<td>Fast</td>
<td>Fast</td>
<td>High</td>
<td>Small</td>
<td>Large-scale data</td>
</tr>
<tr>
<td><strong>CatBoost</strong></td>
<td>Medium</td>
<td>Fastest</td>
<td>Highest</td>
<td>Medium</td>
<td>Categorical variables</td>
</tr>
</tbody>
</table>

<hr>

<h2>3.10 Practical Techniques for Kaggle</h2>

<h3>1. Ensemble of Ensembles (Stacking)</h3>

<pre><code class="language-python">from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression

# Level 1: Base Models
base_models = [
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('xgb', xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')),
    ('lgb', lgb.LGBMClassifier(n_estimators=100, random_state=42))
]

# Level 2: Meta Model
meta_model = LogisticRegression()

# Stacking
stacking_model = StackingClassifier(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5
)

stacking_model.fit(X_train, y_train)
y_pred_stack = stacking_model.predict(X_test)

print("=== Stacking Ensemble ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred_stack):.4f}")
</code></pre>

<h3>2. Weighted Average</h3>

<pre><code class="language-python"># Prediction probabilities from each model
xgb_proba = xgb_model.predict_proba(X_test)
lgb_proba = lgb_model.predict_proba(X_test)
cat_proba = cat_model.predict_proba(X_test)

# Weighted Average
weights = [0.4, 0.3, 0.3]  # Adjust based on performance
weighted_proba = (weights[0] * xgb_proba +
                 weights[1] * lgb_proba +
                 weights[2] * cat_proba)

y_pred_weighted = np.argmax(weighted_proba, axis=1)

print("=== Weighted Average ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred_weighted):.4f}")
</code></pre>

<h3>3. Early Stopping</h3>

<pre><code class="language-python"># Using Early Stopping
xgb_early = xgb.XGBClassifier(
    n_estimators=1000,
    learning_rate=0.05,
    random_state=42,
    eval_metric='logloss'
)

xgb_early.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    early_stopping_rounds=20,
    verbose=False
)

print(f"=== Early Stopping ===")
print(f"Optimal Iterations: {xgb_early.best_iteration}")
print(f"Accuracy: {xgb_early.score(X_test, y_test):.4f}")
</code></pre>

<hr>

<h2>3.11 Chapter Summary</h2>

<h3>What You Learned</h3>

<ol>
<li><p><strong>Ensemble Principles</strong></p>
<ul>
<li>Performance improvement through model combination</li>
<li>Bagging: Parallel learning, variance reduction</li>
<li>Boosting: Sequential learning, bias reduction</li>
</ul></li>
<li><p><strong>Random Forest</strong></p>
<ul>
<li>Bagging + random feature selection</li>
<li>Feature importance analysis</li>
<li>OOB evaluation</li>
</ul></li>
<li><p><strong>Gradient Boosting</strong></p>
<ul>
<li>Sequential residual learning</li>
<li>High accuracy but beware of overfitting</li>
</ul></li>
<li><p><strong>XGBoost/LightGBM/CatBoost</strong></p>
<ul>
<li>Most widely used methods in Kaggle</li>
<li>Fast and accurate</li>
<li>Each has different features and strengths</li>
</ul></li>
<li><p><strong>Practical Techniques</strong></p>
<ul>
<li>Stacking</li>
<li>Weighted Average</li>
<li>Early Stopping</li>
</ul></li>
</ol>

<h3>Next Chapter</h3>

<p>In Chapter 4, we will apply the techniques learned through <strong>Practical Projects</strong>:</p>
<ul>
<li>Project 1: House Price Prediction (Regression)</li>
<li>Project 2: Customer Churn Prediction (Classification)</li>
<li>Complete Machine Learning Pipeline</li>
</ul>

<hr>

<h2>Exercises</h2>

<h3>Problem 1 (Difficulty: Easy)</h3>
<p>List three main differences between Bagging and Boosting.</p>

<details>
<summary>Solution</summary>

<p><strong>Answer</strong>:</p>
<ol>
<li><strong>Learning Method</strong>: Bagging is parallel, Boosting is sequential</li>
<li><strong>Objective</strong>: Bagging reduces variance, Boosting reduces bias</li>
<li><strong>Weights</strong>: Bagging uses equal weights, Boosting uses error-based weights</li>
</ol>

</details>

<h3>Problem 2 (Difficulty: Medium)</h3>
<p>Explain why LightGBM is faster than XGBoost.</p>

<details>
<summary>Solution</summary>

<p><strong>Answer</strong>:</p>

<p><strong>1. Leaf-wise Growth Strategy</strong>:</p>
<ul>
<li>XGBoost: Level-wise (grows layer by layer)</li>
<li>LightGBM: Leaf-wise (grows the leaf with maximum loss reduction)</li>
<li>Result: Achieves same accuracy with fewer splits</li>
</ul>

<p><strong>2. GOSS (Gradient-based One-Side Sampling)</strong>:</p>
<ul>
<li>Retains data with large gradients</li>
<li>Randomly samples data with small gradients</li>
<li>Result: Speedup through data reduction</li>
</ul>

<p><strong>3. EFB (Exclusive Feature Bundling)</strong>:</p>
<ul>
<li>Bundles exclusive features together</li>
<li>Result: Improved memory efficiency through feature count reduction</li>
</ul>

<p><strong>4. Histogram-based</strong>:</p>
<ul>
<li>Discretizes continuous values into bins</li>
<li>Result: Faster split point search</li>
</ul>

</details>

<h3>Problem 3 (Difficulty: Medium)</h3>
<p>Extract the top 5 most important features from Random Forest and retrain the model using only those features. How does performance change?</p>

<details>
<summary>Solution</summary>

<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np

# Generate data
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,
                          n_redundant=5, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest with all features
rf_full = RandomForestClassifier(n_estimators=100, random_state=42)
rf_full.fit(X_train, y_train)
acc_full = rf_full.score(X_test, y_test)

print(f"Accuracy with all features (20): {acc_full:.4f}")

# Extract Top 5 feature importances
importances = rf_full.feature_importances_
top5_indices = np.argsort(importances)[::-1][:5]

print(f"\nTop 5 Features: {top5_indices}")
print(f"Importances: {importances[top5_indices]}")

# Build model with Top 5 features only
X_train_top5 = X_train[:, top5_indices]
X_test_top5 = X_test[:, top5_indices]

rf_top5 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_top5.fit(X_train_top5, y_train)
acc_top5 = rf_top5.score(X_test_top5, y_test)

print(f"\nAccuracy with Top 5 features: {acc_top5:.4f}")
print(f"Accuracy change: {acc_top5 - acc_full:.4f}")
print(f"Feature reduction rate: {(20-5)/20*100:.1f}%")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>Accuracy with all features (20): 0.9100

Top 5 Features: [ 2  7 13  5 19]
Importances: [0.0852 0.0741 0.0689 0.0634 0.0598]

Accuracy with Top 5 features: 0.8650
Accuracy change: -0.0450
Feature reduction rate: 75.0%
</code></pre>

<p><strong>Discussion</strong>:</p>
<ul>
<li>Even with 75% feature reduction, accuracy only drops by about 5%</li>
<li>Significant reduction in computation time and memory usage</li>
<li>Improved interpretability (focus on important features)</li>
</ul>

</details>

<h3>Problem 4 (Difficulty: Hard)</h3>
<p>Train XGBoost, LightGBM, and CatBoost on the same data and write code to select the most appropriate model.</p>

<details>
<summary>Solution</summary>

<pre><code class="language-python">import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier
from sklearn.model_selection import cross_val_score
import time

# Data (refer to previous code)
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model definitions
models = {
    'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss'),
    'LightGBM': lgb.LGBMClassifier(n_estimators=100, random_state=42),
    'CatBoost': CatBoostClassifier(iterations=100, random_state=42, verbose=False)
}

# Evaluation
results = {}

for name, model in models.items():
    # Measure training time
    start_time = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start_time

    # Measure prediction time
    start_time = time.time()
    y_pred = model.predict(X_test)
    predict_time = time.time() - start_time

    # Cross-validation
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')

    # Test score
    test_score = accuracy_score(y_test, y_pred)

    results[name] = {
        'train_time': train_time,
        'predict_time': predict_time,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'test_score': test_score
    }

# Display results
print("=== Model Comparison ===\n")
for name, metrics in results.items():
    print(f"{name}:")
    print(f"  Training Time: {metrics['train_time']:.4f} sec")
    print(f"  Prediction Time: {metrics['predict_time']:.4f} sec")
    print(f"  CV Accuracy: {metrics['cv_mean']:.4f} (+/- {metrics['cv_std']:.4f})")
    print(f"  Test Accuracy: {metrics['test_score']:.4f}")
    print()

# Select optimal model
best_model = max(results.items(), key=lambda x: x[1]['test_score'])
print(f"Optimal Model: {best_model[0]}")
print(f"Test Accuracy: {best_model[1]['test_score']:.4f}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Model Comparison ===

XGBoost:
  Training Time: 0.2341 sec
  Prediction Time: 0.0023 sec
  CV Accuracy: 0.9212 (+/- 0.0156)
  Test Accuracy: 0.9350

LightGBM:
  Training Time: 0.1234 sec
  Prediction Time: 0.0018 sec
  CV Accuracy: 0.9188 (+/- 0.0178)
  Test Accuracy: 0.9350

CatBoost:
  Training Time: 0.4567 sec
  Prediction Time: 0.0012 sec
  CV Accuracy: 0.9250 (+/- 0.0134)
  Test Accuracy: 0.9400

Optimal Model: CatBoost
Test Accuracy: 0.9400
</code></pre>

</details>

<h3>Problem 5 (Difficulty: Hard)</h3>
<p>Implement Stacking and Weighted Average, and compare which one achieves better performance.</p>

<details>
<summary>Solution</summary>

<pre><code class="language-python">from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
import numpy as np

# Data (refer to previous code)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Base Models
base_models = [
    ('xgb', xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')),
    ('lgb', lgb.LGBMClassifier(n_estimators=100, random_state=42)),
    ('cat', CatBoostClassifier(iterations=100, random_state=42, verbose=False))
]

# 1. Stacking
stacking = StackingClassifier(
    estimators=base_models,
    final_estimator=LogisticRegression(),
    cv=5
)

stacking.fit(X_train, y_train)
y_pred_stacking = stacking.predict(X_test)
acc_stacking = accuracy_score(y_test, y_pred_stacking)

print("=== Stacking ===")
print(f"Accuracy: {acc_stacking:.4f}")

# 2. Weighted Average
# Get prediction probabilities from each model
xgb_model = base_models[0][1]
lgb_model = base_models[1][1]
cat_model = base_models[2][1]

xgb_model.fit(X_train, y_train)
lgb_model.fit(X_train, y_train)
cat_model.fit(X_train, y_train)

xgb_proba = xgb_model.predict_proba(X_test)
lgb_proba = lgb_model.predict_proba(X_test)
cat_proba = cat_model.predict_proba(X_test)

# Weight optimization (grid search)
best_acc = 0
best_weights = None

for w1 in np.arange(0, 1.1, 0.1):
    for w2 in np.arange(0, 1.1 - w1, 0.1):
        w3 = 1.0 - w1 - w2
        if w3 < 0:
            continue

        weighted_proba = w1 * xgb_proba + w2 * lgb_proba + w3 * cat_proba
        y_pred = np.argmax(weighted_proba, axis=1)
        acc = accuracy_score(y_test, y_pred)

        if acc > best_acc:
            best_acc = acc
            best_weights = (w1, w2, w3)

print("\n=== Weighted Average ===")
print(f"Optimal Weights: XGB={best_weights[0]:.1f}, LGB={best_weights[1]:.1f}, Cat={best_weights[2]:.1f}")
print(f"Accuracy: {best_acc:.4f}")

# Comparison
print("\n=== Comparison ===")
print(f"Stacking: {acc_stacking:.4f}")
print(f"Weighted Average: {best_acc:.4f}")
print(f"Difference: {best_acc - acc_stacking:.4f}")

if best_acc > acc_stacking:
    print("-> Weighted Average is superior")
else:
    print("-> Stacking is superior")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Stacking ===
Accuracy: 0.9450

=== Weighted Average ===
Optimal Weights: XGB=0.3, LGB=0.3, Cat=0.4
Accuracy: 0.9500

=== Comparison ===
Stacking: 0.9450
Weighted Average: 0.9500
Difference: 0.0050
-> Weighted Average is superior
</code></pre>

<p><strong>Discussion</strong>:</p>
<ul>
<li>Weighted Average is slightly superior</li>
<li>Stacking has a somewhat higher risk of overfitting</li>
<li>Weighted Average is simpler and more interpretable</li>
<li>For large-scale data, Stacking may be advantageous</li>
</ul>

</details>

<hr>

<h2>References</h2>

<ol>
<li>Chen, T., & Guestrin, C. (2016). "XGBoost: A Scalable Tree Boosting System." <em>KDD 2016</em>.</li>
<li>Ke, G., et al. (2017). "LightGBM: A Highly Efficient Gradient Boosting Decision Tree." <em>NIPS 2017</em>.</li>
<li>Prokhorenkova, L., et al. (2018). "CatBoost: unbiased boosting with categorical features." <em>NeurIPS 2018</em>.</li>
<li>Breiman, L. (2001). "Random Forests." <em>Machine Learning</em>, 45(1), 5-32.</li>
</ol>

<div class="navigation">
    <a href="chapter2-classification.html" class="nav-button">Previous: Classification Fundamentals</a>
    <a href="chapter4-projects.html" class="nav-button">Next: Practical Projects</a>
</div>

    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical warranties, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The author and Tohoku University assume no responsibility for external links or the content, availability, or safety of third-party data, tools, or libraries.</li>
            <li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content may be changed, updated, or discontinued without notice.</li>
            <li>Copyright and licensing of this content follow the stated terms (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
        </ul>
    </section>

<footer>
        <p><strong>Author</strong>: AI Terakoya Content Team</p>
        <p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-20</p>
        <p><strong>License</strong>: Creative Commons BY 4.0</p>
        <p>2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
