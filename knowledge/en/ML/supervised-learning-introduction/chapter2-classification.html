<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
<meta content="Chapter 2: Classification Fundamentals - AI Terakoya" name="description"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: Classification Fundamentals - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "⚠️";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }



        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }

        /* Locale switcher styles */
        .locale-switcher {
            max-width: 900px;
            margin: 0 auto;
            padding: 0.5rem 1rem;
            text-align: right;
            font-size: 0.85rem;
        }

        .current-locale {
            font-weight: 600;
            color: #4a5568;
        }

        .locale-separator {
            color: #a0aec0;
            margin: 0 0.5rem;
        }

        .locale-link {
            color: #667eea;
            text-decoration: none;
        }

        .locale-link:hover {
            text-decoration: underline;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Home</a><span class="breadcrumb-separator">›</span><a href="../../index.html">Machine Learning</a><span class="breadcrumb-separator">›</span><a href="../../ML/supervised-learning-introduction/index.html">Supervised Learning</a><span class="breadcrumb-separator">›</span><span class="breadcrumb-current">Chapter 2</span>
        </div>
    </nav>
    <div class="locale-switcher">
    <span class="current-locale">EN</span>
    <span class="locale-separator">|</span>
    <a href="../../../jp/ML/supervised-learning-introduction/chapter2-classification.html" class="locale-link">JP</a>
    </div>

        <header>
        <div class="header-content">
            <h1>Chapter 2: Classification Fundamentals</h1>
            <p class="subtitle">Theory and Implementation of Category Prediction - From Logistic Regression to Decision Trees and SVM</p>
            <div class="meta">
                <span class="meta-item">Reading Time: 25-30 min</span>
                <span class="meta-item">Difficulty: Beginner to Intermediate</span>
                <span class="meta-item">Code Examples: 12</span>
                <span class="meta-item">Exercises: 5</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Learning Objectives</h2>
<p>By the end of this chapter, you will be able to:</p>
<ul>
<li>Understand the definition and applications of classification problems</li>
<li>Understand the theory and implement logistic regression</li>
<li>Explain the sigmoid function and probability interpretation</li>
<li>Understand the mechanism and implement decision trees</li>
<li>Apply k-NN and SVM</li>
<li>Evaluate models using confusion matrix, precision, recall, and F1 score</li>
<li>Understand and utilize ROC curves and AUC</li>
</ul>

<hr>

<h2>2.1 What is Classification?</h2>

<h3>Definition</h3>
<p><strong>Classification</strong> is a supervised learning task that predicts <strong>discrete values (categories)</strong> from input variables.</p>

<blockquote>
<p>"Learn a function $f: X \rightarrow y$ that predicts discrete class labels $y \in \{1, 2, ..., K\}$ from features $X$"</p>
</blockquote>

<h3>Types of Classification</h3>

<table>
<thead>
<tr>
<th>Type</th>
<th>Number of Classes</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Binary Classification</strong></td>
<td>2 classes</td>
<td>Spam detection, disease diagnosis, customer churn prediction</td>
</tr>
<tr>
<td><strong>Multi-class Classification</strong></td>
<td>3+ classes</td>
<td>Handwritten digit recognition, image classification, sentiment analysis</td>
</tr>
<tr>
<td><strong>Multi-label Classification</strong></td>
<td>Multiple labels</td>
<td>Tagging, gene function prediction</td>
</tr>
</tbody>
</table>

<h3>Real-World Applications</h3>

<div class="mermaid">
graph LR
    A[Classification Applications] --> B[Healthcare: Disease Diagnosis]
    A --> C[Finance: Credit Scoring]
    A --> D[Marketing: Customer Segmentation]
    A --> E[Security: Fraud Detection]
    A --> F[Vision: Object Recognition]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#fff3e0
    style D fill:#fff3e0
    style E fill:#fff3e0
    style F fill:#fff3e0
</div>

<hr>

<h2>2.2 Logistic Regression</h2>

<h3>Overview</h3>

<p><strong>Logistic Regression</strong> is a linear model used for binary classification. It applies the sigmoid function to linear regression to output probabilities.</p>

<h3>Sigmoid Function</h3>

<p>$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$</p>

<p>Properties:</p>
<ul>
<li>Output range: $[0, 1]$ (interpretable as probability)</li>
<li>$z = 0$ yields $\sigma(z) = 0.5$</li>
<li>Smooth S-shaped curve</li>
</ul>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Visualization
z = np.linspace(-10, 10, 100)
y = sigmoid(z)

plt.figure(figsize=(10, 6))
plt.plot(z, y, linewidth=2, label='σ(z)')
plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Threshold 0.5')
plt.axvline(x=0, color='g', linestyle='--', alpha=0.5)
plt.xlabel('z = w^T x', fontsize=12)
plt.ylabel('σ(z)', fontsize=12)
plt.title('Sigmoid Function', fontsize=14)
plt.grid(True, alpha=0.3)
plt.legend()
plt.show()
</code></pre>

<h3>Model Definition</h3>

<p>$$
P(y=1 | \mathbf{x}) = \sigma(\mathbf{w}^T \mathbf{x}) = \frac{1}{1 + e^{-\mathbf{w}^T \mathbf{x}}}
$$</p>

<p>Prediction:</p>

<p>$$
\hat{y} = \begin{cases}
1 & \text{if } P(y=1 | \mathbf{x}) \geq 0.5 \\
0 & \text{otherwise}
\end{cases}
$$</p>

<h3>Loss Function: Cross-Entropy</h3>

<p>$$
J(\mathbf{w}) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right]
$$</p>

<h3>Implementation Example</h3>

<pre><code class="language-python">from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Generate data
X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0,
                          n_informative=2, random_state=42, n_clusters_per_class=1)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Logistic regression
model = LogisticRegression()
model.fit(X_train, y_train)

# Prediction
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)

print("=== Logistic Regression ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"\nWeights: {model.coef_[0]}")
print(f"Intercept: {model.intercept_[0]:.4f}")

# Decision boundary visualization
def plot_decision_boundary(model, X, y):
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                        np.arange(y_min, y_max, h))

    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(10, 6))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')
    plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', marker='o',
                edgecolors='k', s=80, label='Class 0')
    plt.scatter(X[y==1, 0], X[y==1, 1], c='red', marker='s',
                edgecolors='k', s=80, label='Class 1')
    plt.xlabel('Feature 1', fontsize=12)
    plt.ylabel('Feature 2', fontsize=12)
    plt.title('Decision Boundary of Logistic Regression', fontsize=14)
    plt.legend()
    plt.show()

plot_decision_boundary(model, X_test, y_test)
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Logistic Regression ===
Accuracy: 0.9550

Weights: [2.14532851 1.87653214]
Intercept: -0.2341
</code></pre>

<hr>

<h2>2.3 Decision Trees</h2>

<h3>Overview</h3>

<p><strong>Decision Trees</strong> perform classification using a hierarchical structure of if-then-else rules. They recursively split data based on features.</p>

<div class="mermaid">
graph TD
    A[Feature 1 <= 0.5] -->|Yes| B[Feature 2 <= 1.2]
    A -->|No| C[Class 1]
    B -->|Yes| D[Class 0]
    B -->|No| E[Class 1]

    style A fill:#fff3e0
    style B fill:#fff3e0
    style C fill:#e8f5e9
    style D fill:#e3f2fd
    style E fill:#e8f5e9
</div>

<h3>Splitting Criteria</h3>

<p><strong>1. Gini Impurity</strong>:</p>

<p>$$
\text{Gini}(S) = 1 - \sum_{i=1}^{K} p_i^2
$$</p>

<ul>
<li>$p_i$: Proportion of class $i$</li>
<li>Lower values indicate higher purity (biased toward one class)</li>
</ul>

<p><strong>2. Entropy</strong>:</p>

<p>$$
\text{Entropy}(S) = -\sum_{i=1}^{K} p_i \log_2(p_i)
$$</p>

<h3>Implementation Example</h3>

<pre><code class="language-python">from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree

# Decision tree model
dt_model = DecisionTreeClassifier(max_depth=3, random_state=42)
dt_model.fit(X_train, y_train)

# Prediction
y_pred_dt = dt_model.predict(X_test)

print("=== Decision Tree ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}")

# Decision tree visualization
plt.figure(figsize=(16, 10))
plot_tree(dt_model, filled=True, feature_names=['Feature 1', 'Feature 2'],
          class_names=['Class 0', 'Class 1'], fontsize=10)
plt.title('Decision Tree Structure', fontsize=16)
plt.show()

# Decision boundary
plot_decision_boundary(dt_model, X_test, y_test)
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Decision Tree ===
Accuracy: 0.9450
</code></pre>

<h3>Feature Importance</h3>

<pre><code class="language-python"># Feature importance
importances = dt_model.feature_importances_

plt.figure(figsize=(8, 6))
plt.bar(['Feature 1', 'Feature 2'], importances, color=['#3498db', '#e74c3c'])
plt.ylabel('Importance', fontsize=12)
plt.title('Feature Importance', fontsize=14)
plt.grid(axis='y', alpha=0.3)
plt.show()

print(f"\nFeature 1 importance: {importances[0]:.4f}")
print(f"Feature 2 importance: {importances[1]:.4f}")
</code></pre>

<hr>

<h2>2.4 k-Nearest Neighbors (k-NN)</h2>

<h3>Overview</h3>

<p><strong>k-NN</strong> classifies by majority voting among the $k$ nearest training samples.</p>

<h3>Algorithm</h3>

<ol>
<li>Calculate distances from test data $\mathbf{x}$ to all training data</li>
<li>Select the $k$ nearest data points</li>
<li>Predict the class with the most votes</li>
</ol>

<h3>Distance Types</h3>

<table>
<thead>
<tr>
<th>Distance</th>
<th>Formula</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Euclidean Distance</strong></td>
<td>$\sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}$</td>
</tr>
<tr>
<td><strong>Manhattan Distance</strong></td>
<td>$\sum_{i=1}^{n} |x_i - y_i|$</td>
</tr>
<tr>
<td><strong>Minkowski Distance</strong></td>
<td>$\left(\sum_{i=1}^{n} |x_i - y_i|^p\right)^{1/p}$</td>
</tr>
</tbody>
</table>

<h3>Implementation Example</h3>

<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier

# k-NN model
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)

# Prediction
y_pred_knn = knn_model.predict(X_test)

print("=== k-NN (k=5) ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}")

# Decision boundary
plot_decision_boundary(knn_model, X_test, y_test)
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== k-NN (k=5) ===
Accuracy: 0.9400
</code></pre>

<h3>Choosing k</h3>

<pre><code class="language-python"># Accuracy comparison for different k values
k_range = range(1, 31)
train_scores = []
test_scores = []

for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)

    train_scores.append(knn.score(X_train, y_train))
    test_scores.append(knn.score(X_test, y_test))

plt.figure(figsize=(10, 6))
plt.plot(k_range, train_scores, 'o-', label='Training Data', linewidth=2)
plt.plot(k_range, test_scores, 's-', label='Test Data', linewidth=2)
plt.xlabel('k (Number of Neighbors)', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.title('k-NN: Relationship between k and Accuracy', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

best_k = k_range[np.argmax(test_scores)]
print(f"\nOptimal k: {best_k}")
print(f"Best Accuracy: {max(test_scores):.4f}")
</code></pre>

<hr>

<h2>2.5 Support Vector Machine (SVM)</h2>

<h3>Overview</h3>

<p><strong>SVM</strong> finds the decision boundary that maximizes the margin.</p>

<div class="mermaid">
graph LR
    A[SVM] --> B[Linear SVM]
    A --> C[Non-linear SVM Kernel Methods]

    B --> B1[Linearly Separable Data]
    C --> C1[RBF Kernel]
    C --> C2[Polynomial Kernel]
    C --> C3[Sigmoid Kernel]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
</div>

<h3>Margin Maximization</h3>

<p>$$
\text{maximize} \quad \frac{2}{||\mathbf{w}||} \quad \text{subject to} \quad y^{(i)}(\mathbf{w}^T \mathbf{x}^{(i)} + b) \geq 1
$$</p>

<h3>Kernel Trick</h3>

<p><strong>RBF (Gaussian) Kernel</strong>:</p>

<p>$$
K(\mathbf{x}, \mathbf{x}') = \exp\left(-\frac{||\mathbf{x} - \mathbf{x}'||^2}{2\sigma^2}\right)
$$</p>

<h3>Implementation Example</h3>

<pre><code class="language-python">from sklearn.svm import SVC

# Linear SVM
svm_linear = SVC(kernel='linear')
svm_linear.fit(X_train, y_train)

# RBF SVM
svm_rbf = SVC(kernel='rbf', gamma='auto')
svm_rbf.fit(X_train, y_train)

print("=== SVM (Linear Kernel) ===")
print(f"Accuracy: {svm_linear.score(X_test, y_test):.4f}")

print("\n=== SVM (RBF Kernel) ===")
print(f"Accuracy: {svm_rbf.score(X_test, y_test):.4f}")

# Decision boundary comparison
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

for ax, model, title in zip(axes, [svm_linear, svm_rbf],
                            ['Linear SVM', 'RBF SVM']):
    h = 0.02
    x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1
    y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                        np.arange(y_min, y_max, h))

    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')
    ax.scatter(X_test[y_test==0, 0], X_test[y_test==0, 1],
              c='blue', marker='o', edgecolors='k', s=80, label='Class 0')
    ax.scatter(X_test[y_test==1, 0], X_test[y_test==1, 1],
              c='red', marker='s', edgecolors='k', s=80, label='Class 1')
    ax.set_xlabel('Feature 1')
    ax.set_ylabel('Feature 2')
    ax.set_title(title, fontsize=14)
    ax.legend()

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== SVM (Linear Kernel) ===
Accuracy: 0.9550

=== SVM (RBF Kernel) ===
Accuracy: 0.9650
</code></pre>

<hr>

<h2>2.6 Evaluation of Classification Models</h2>

<h3>Confusion Matrix</h3>

<table>
<thead>
<tr>
<th></th>
<th>Predicted: Positive</th>
<th>Predicted: Negative</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Actual: Positive</strong></td>
<td>TP (True Positive)</td>
<td>FN (False Negative)</td>
</tr>
<tr>
<td><strong>Actual: Negative</strong></td>
<td>FP (False Positive)</td>
<td>TN (True Negative)</td>
</tr>
</tbody>
</table>

<h3>Evaluation Metrics</h3>

<table>
<thead>
<tr>
<th>Metric</th>
<th>Formula</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Accuracy</strong></td>
<td>$\frac{TP + TN}{TP + TN + FP + FN}$</td>
<td>Overall correct rate</td>
</tr>
<tr>
<td><strong>Precision</strong></td>
<td>$\frac{TP}{TP + FP}$</td>
<td>Accuracy of positive predictions</td>
</tr>
<tr>
<td><strong>Recall</strong></td>
<td>$\frac{TP}{TP + FN}$</td>
<td>Rate of capturing actual positives</td>
</tr>
<tr>
<td><strong>F1 Score</strong></td>
<td>$2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$</td>
<td>Harmonic mean of Precision and Recall</td>
</tr>
</tbody>
</table>

<h3>Implementation Example</h3>

<pre><code class="language-python">from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Class 0', 'Class 1'],
            yticklabels=['Class 0', 'Class 1'])
plt.xlabel('Predicted', fontsize=12)
plt.ylabel('Actual', fontsize=12)
plt.title('Confusion Matrix', fontsize=14)
plt.show()

# Detailed evaluation report
print("\n=== Classification Report ===")
print(classification_report(y_test, y_pred,
                          target_names=['Class 0', 'Class 1']))
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Classification Report ===
              precision    recall  f1-score   support

     Class 0       0.96      0.95      0.95        99
     Class 1       0.95      0.96      0.96       101

    accuracy                           0.96       200
   macro avg       0.96      0.96      0.96       200
weighted avg       0.96      0.96      0.96       200
</code></pre>

<h3>ROC Curve and AUC</h3>

<p><strong>ROC (Receiver Operating Characteristic) curve</strong> shows the relationship between TPR (True Positive Rate) and FPR (False Positive Rate) as the threshold varies.</p>

<p>$$
\text{TPR} = \frac{TP}{TP + FN}, \quad \text{FPR} = \frac{FP}{FP + TN}
$$</p>

<pre><code class="language-python">from sklearn.metrics import roc_curve, roc_auc_score

# ROC curve calculation
fpr, tpr, thresholds = roc_curve(y_test, y_proba[:, 1])
auc = roc_auc_score(y_test, y_proba[:, 1])

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc:.4f})')
plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random (AUC = 0.5)')
plt.xlabel('False Positive Rate (FPR)', fontsize=12)
plt.ylabel('True Positive Rate (TPR)', fontsize=12)
plt.title('ROC Curve', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)
plt.show()

print(f"AUC: {auc:.4f}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>AUC: 0.9876
</code></pre>

<blockquote>
<p><strong>AUC (Area Under the Curve)</strong>: The area under the ROC curve. Values closer to 1 indicate a better model.</p>
</blockquote>

<hr>

<h2>2.7 Chapter Summary</h2>

<h3>What We Learned</h3>

<ol>
<li><p><strong>Definition of Classification Problems</strong></p>
<ul>
<li>Tasks predicting discrete values (categories)</li>
<li>Binary classification, multi-class classification, multi-label classification</li>
</ul></li>
<li><p><strong>Logistic Regression</strong></p>
<ul>
<li>Probability output using sigmoid function</li>
<li>Cross-entropy loss</li>
</ul></li>
<li><p><strong>Decision Trees</strong></p>
<ul>
<li>Hierarchical structure of if-then-else rules</li>
<li>Gini impurity, entropy</li>
<li>Feature importance</li>
</ul></li>
<li><p><strong>k-NN</strong></p>
<ul>
<li>Majority voting among nearest neighbors</li>
<li>Importance of choosing k</li>
</ul></li>
<li><p><strong>SVM</strong></p>
<ul>
<li>Margin maximization</li>
<li>Kernel trick</li>
</ul></li>
<li><p><strong>Evaluation Metrics</strong></p>
<ul>
<li>Confusion matrix, accuracy, precision, recall, F1 score</li>
<li>ROC curve and AUC</li>
</ul></li>
</ol>

<h3>Next Chapter</h3>

<p>In Chapter 3, we will learn about <strong>Ensemble Methods</strong>:</p>
<ul>
<li>Principles of Bagging</li>
<li>Random Forest</li>
<li>Boosting (Gradient Boosting, XGBoost, LightGBM, CatBoost)</li>
</ul>

<hr>

<h2>Exercises</h2>

<h3>Problem 1 (Difficulty: Easy)</h3>
<p>Explain situations where high accuracy may be inappropriate.</p>

<details>
<summary>Solution</summary>

<p><strong>Answer</strong>:</p>
<p>In cases of <strong>Imbalanced Data</strong>, accuracy is inappropriate.</p>

<p><strong>Example</strong>:</p>
<ul>
<li>Cancer diagnosis data: 1% positive, 99% negative</li>
<li>Predicting all as "negative" gives 99% accuracy but is meaningless</li>
<li>Missing positive cases leads to serious consequences</li>
</ul>

<p><strong>Appropriate metrics</strong>:</p>
<ul>
<li>Recall: Avoid missing positives</li>
<li>F1 Score: Balance between Precision and Recall</li>
<li>AUC: Threshold-independent evaluation</li>
</ul>

</details>

<h3>Problem 2 (Difficulty: Medium)</h3>
<p>Calculate accuracy, precision, recall, and F1 score from the following confusion matrix.</p>

<table>
<thead>
<tr>
<th></th>
<th>Predicted: Positive</th>
<th>Predicted: Negative</th>
</tr>
</thead>
<tbody>
<tr>
<td>Actual: Positive</td>
<td>80</td>
<td>20</td>
</tr>
<tr>
<td>Actual: Negative</td>
<td>10</td>
<td>90</td>
</tr>
</tbody>
</table>

<details>
<summary>Solution</summary>

<p><strong>Answer</strong>:</p>

<pre><code>TP = 80, FN = 20, FP = 10, TN = 90

Accuracy = (TP + TN) / (TP + TN + FP + FN)
         = (80 + 90) / (80 + 90 + 10 + 20)
         = 170 / 200 = 0.85 = 85%

Precision = TP / (TP + FP)
          = 80 / (80 + 10)
          = 80 / 90 = 0.8889 = 88.89%

Recall = TP / (TP + FN)
       = 80 / (80 + 20)
       = 80 / 100 = 0.80 = 80%

F1 Score = 2 * (Precision * Recall) / (Precision + Recall)
         = 2 * (0.8889 * 0.80) / (0.8889 + 0.80)
         = 2 * 0.7111 / 1.6889
         = 0.8421 = 84.21%
</code></pre>

</details>

<h3>Problem 3 (Difficulty: Medium)</h3>
<p>Explain the problems when k is too small or too large when choosing the optimal k for k-NN.</p>

<details>
<summary>Solution</summary>

<p><strong>When k is too small (e.g., k=1)</strong>:</p>
<ul>
<li><strong>Overfitting</strong>: Sensitive to noise</li>
<li>High accuracy on training data but low on test data</li>
<li>Complex, jagged decision boundary</li>
</ul>

<p><strong>When k is too large (e.g., k=total data count)</strong>:</p>
<ul>
<li><strong>Excessive simplification</strong>: Everything classified as the majority class</li>
<li>Decision boundary is too simple</li>
<li>Low accuracy on both training and test data</li>
</ul>

<p><strong>Optimal k</strong>:</p>
<ul>
<li>Selected through cross-validation</li>
<li>Usually around $\sqrt{N}$ ($N$ is the number of data points)</li>
<li>Choose odd numbers (to avoid ties in binary classification)</li>
</ul>

</details>

<h3>Problem 4 (Difficulty: Hard)</h3>
<p>Explain the significance of using the kernel trick in SVM from a computational complexity perspective.</p>

<details>
<summary>Solution</summary>

<p><strong>Significance of the Kernel Trick</strong>:</p>

<p><strong>Problem</strong>: To classify non-linearly separable data, transformation to a higher-dimensional space is necessary.</p>

<p><strong>Direct approach</strong>:</p>
<ul>
<li>Explicitly transform features to higher dimensions: $\phi(\mathbf{x})$</li>
<li>Computational complexity: $O(d^2)$ or $O(d^3)$ ($d$ is the dimension)</li>
<li>Computation becomes infeasible for high dimensions</li>
</ul>

<p><strong>Kernel trick</strong>:</p>
<ul>
<li>Directly compute the inner product $\langle \phi(\mathbf{x}), \phi(\mathbf{x}') \rangle$ using the kernel function $K(\mathbf{x}, \mathbf{x}')$</li>
<li>No explicit high-dimensional transformation</li>
<li>Computational complexity: $O(d)$ (stays in original dimension)</li>
</ul>

<p><strong>Example (RBF kernel)</strong>:</p>
<ul>
<li>Computes transformation to infinite dimensions in $O(d)$</li>
<li>$K(\mathbf{x}, \mathbf{x}') = \exp(-\gamma ||\mathbf{x} - \mathbf{x}'||^2)$</li>
</ul>

<p><strong>Conclusion</strong>: The kernel trick enables efficient execution of high-dimensional computations in low-dimensional space.</p>

</details>

<h3>Problem 5 (Difficulty: Hard)</h3>
<p>Implement logistic regression and perform binary classification on the iris dataset (setosa vs versicolor).</p>

<details>
<summary>Solution</summary>

<pre><code class="language-python">import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
iris = load_iris()
X = iris.data[iris.target != 2]  # setosa (0) and versicolor (1) only
y = iris.target[iris.target != 2]

# Use only the first 2 features (for visualization)
X = X[:, :2]

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Standardization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Logistic regression
model = LogisticRegression()
model.fit(X_train_scaled, y_train)

# Prediction
y_pred = model.predict(X_test_scaled)
y_proba = model.predict_proba(X_test_scaled)

# Evaluation
print("=== Classification Report ===")
print(classification_report(y_test, y_pred,
                          target_names=['setosa', 'versicolor']))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['setosa', 'versicolor'],
            yticklabels=['setosa', 'versicolor'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Decision boundary visualization
h = 0.02
x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1
y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                    np.arange(y_min, y_max, h))

Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.figure(figsize=(10, 6))
plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')
plt.scatter(X_train_scaled[y_train==0, 0], X_train_scaled[y_train==0, 1],
           c='blue', marker='o', edgecolors='k', s=80, label='setosa')
plt.scatter(X_train_scaled[y_train==1, 0], X_train_scaled[y_train==1, 1],
           c='red', marker='s', edgecolors='k', s=80, label='versicolor')
plt.xlabel('Sepal length (standardized)')
plt.ylabel('Sepal width (standardized)')
plt.title('Decision Boundary of Logistic Regression')
plt.legend()
plt.show()

print(f"\nAccuracy: {model.score(X_test_scaled, y_test):.4f}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Classification Report ===
              precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        10
  versicolor       1.00      1.00      1.00        10

    accuracy                           1.00        20
   macro avg       1.00      1.00      1.00        20
weighted avg       1.00      1.00      1.00        20

Accuracy: 1.0000
</code></pre>

</details>

<hr>

<h2>References</h2>

<ol>
<li>Hastie, T., Tibshirani, R., & Friedman, J. (2009). <em>The Elements of Statistical Learning</em>. Springer.</li>
<li>Murphy, K. P. (2012). <em>Machine Learning: A Probabilistic Perspective</em>. MIT Press.</li>
<li>James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). <em>An Introduction to Statistical Learning</em>. Springer.</li>
</ol>

<div class="navigation">
    <a href="chapter1-regression.html" class="nav-button">Previous Chapter: Regression Fundamentals</a>
    <a href="chapter3-ensemble.html" class="nav-button">Next Chapter: Ensemble Methods</a>
</div>

    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without warranties of any kind, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The creators and Tohoku University assume no responsibility for the content, availability, or security of external links or third-party data, tools, or libraries.</li>
            <li>To the maximum extent permitted by applicable law, the creators and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content may be changed, updated, or discontinued without notice.</li>
            <li>Copyright and license of this content follow the stated conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
        </ul>
    </section>

<footer>
        <p><strong>Author</strong>: AI Terakoya Content Team</p>
        <p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-20</p>
        <p><strong>License</strong>: Creative Commons BY 4.0</p>
        <p>© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
