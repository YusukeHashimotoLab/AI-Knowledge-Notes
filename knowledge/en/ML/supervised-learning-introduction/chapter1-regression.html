<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
<meta content="Chapter 1: Regression Fundamentals - AI Terakoya" name="description"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 1: Regression Fundamentals - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "⚠️";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }



        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }

        /* Locale switcher styles */
        .locale-switcher {
            max-width: 900px;
            margin: 0 auto;
            padding: 0.5rem 1rem;
            text-align: right;
            font-size: 0.85rem;
        }

        .current-locale {
            font-weight: 600;
            color: var(--color-primary);
        }

        .locale-separator {
            color: #a0aec0;
            margin: 0 0.5rem;
        }

        .locale-link {
            color: #667eea;
            text-decoration: none;
        }

        .locale-link:hover {
            text-decoration: underline;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">></span><a href="../../index.html">Machine Learning</a><span class="breadcrumb-separator">></span><a href="../../ML/supervised-learning-introduction/index.html">Supervised Learning</a><span class="breadcrumb-separator">></span><span class="breadcrumb-current">Chapter 1</span>
        </div>
    </nav>

    <div class="locale-switcher">
    <span class="current-locale">EN</span>
    <span class="locale-separator">|</span>
    <a href="../../../jp/ML/supervised-learning-introduction/chapter1-regression.html" class="locale-link">JP</a>
    </div>

        <header>
        <div class="header-content">
            <h1>Chapter 1: Regression Fundamentals</h1>
            <p class="subtitle">Theory and Implementation of Continuous Value Prediction - From Linear Regression to Regularization</p>
            <div class="meta">
                <span class="meta-item">Reading Time: 20-25 min</span>
                <span class="meta-item">Difficulty: Beginner</span>
                <span class="meta-item">Code Examples: 12</span>
                <span class="meta-item">Exercises: 5</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Learning Objectives</h2>
<p>By reading this chapter, you will be able to:</p>
<ul>
<li>Understand the definition and applications of regression problems</li>
<li>Explain the mathematical background of linear regression</li>
<li>Implement ordinary least squares and gradient descent</li>
<li>Model nonlinear relationships with polynomial regression</li>
<li>Apply regularization (Ridge, Lasso, Elastic Net)</li>
<li>Evaluate regression models using R-squared, RMSE, and MAE</li>
</ul>

<hr>

<h2>1.1 What is a Regression Problem?</h2>

<h3>Definition</h3>
<p><strong>Regression</strong> is a supervised learning task that predicts <strong>continuous values</strong> from input variables.</p>

<blockquote>
<p>"Learning a function $f: X \rightarrow y$ that predicts the target variable $y$ from features $X$"</p>
</blockquote>

<h3>Regression vs Classification</h3>

<table>
<thead>
<tr>
<th>Task</th>
<th>Output</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Regression</strong></td>
<td>Continuous values (numerical)</td>
<td>House price prediction, temperature forecasting, sales prediction</td>
</tr>
<tr>
<td><strong>Classification</strong></td>
<td>Discrete values (categories)</td>
<td>Image classification, spam detection, disease diagnosis</td>
</tr>
</tbody>
</table>

<h3>Real-World Applications</h3>

<div class="mermaid">
graph LR
    A[Regression Applications] --> B[Finance: Stock Price Prediction]
    A --> C[Real Estate: House Price Prediction]
    A --> D[Manufacturing: Demand Forecasting]
    A --> E[Healthcare: Patient Length of Stay Prediction]
    A --> F[Marketing: Sales Forecasting]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#fff3e0
    style D fill:#fff3e0
    style E fill:#fff3e0
    style F fill:#fff3e0
</div>

<hr>

<h2>1.2 Theory of Linear Regression</h2>

<h3>Simple Linear Regression</h3>

<p><strong>Simple Linear Regression</strong> makes predictions from a single feature.</p>

<p>$$
y = w_0 + w_1 x + \epsilon
$$</p>

<ul>
<li>$y$: Target variable (value to predict)</li>
<li>$x$: Explanatory variable (feature)</li>
<li>$w_0$: Intercept (bias)</li>
<li>$w_1$: Slope (weight)</li>
<li>$\epsilon$: Error term</li>
</ul>

<h3>Multiple Linear Regression</h3>

<p><strong>Multiple Linear Regression</strong> uses multiple features.</p>

<p>$$
y = w_0 + w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + \epsilon
$$</p>

<p>Matrix notation:</p>

<p>$$
\mathbf{y} = \mathbf{X}\mathbf{w} + \epsilon
$$</p>

<ul>
<li>$\mathbf{y}$: Target variable vector (shape: $m \times 1$)</li>
<li>$\mathbf{X}$: Feature matrix (shape: $m \times (n+1)$)</li>
<li>$\mathbf{w}$: Weight vector (shape: $(n+1) \times 1$)</li>
<li>$m$: Number of samples, $n$: Number of features</li>
</ul>

<h3>Loss Function</h3>

<p>We minimize the <strong>Mean Squared Error (MSE)</strong>:</p>

<p>$$
J(\mathbf{w}) = \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2 = \frac{1}{m} ||\mathbf{y} - \mathbf{X}\mathbf{w}||^2
$$</p>

<ul>
<li>$y^{(i)}$: Actual value</li>
<li>$\hat{y}^{(i)} = \mathbf{w}^T \mathbf{x}^{(i)}$: Predicted value</li>
</ul>

<hr>

<h2>1.3 Ordinary Least Squares</h2>

<h3>Analytical Solution</h3>

<p>The weights $\mathbf{w}$ that minimize MSE can be found analytically:</p>

<p>$$
\mathbf{w}^* = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
$$</p>

<p>This is called the <strong>Normal Equation</strong>.</p>

<h3>Implementation Example: Simple Regression</h3>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Data generation
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Add bias term
X_b = np.c_[np.ones((100, 1)), X]  # shape: (100, 2)

# Calculate weights using normal equation
w_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y

print("Learned weights:")
print(f"w0 (intercept): {w_best[0][0]:.4f}")
print(f"w1 (slope): {w_best[1][0]:.4f}")

# Prediction
X_new = np.array([[0], [2]])
X_new_b = np.c_[np.ones((2, 1)), X_new]
y_predict = X_new_b @ w_best

# Visualization
plt.figure(figsize=(10, 6))
plt.scatter(X, y, alpha=0.6, label='Data')
plt.plot(X_new, y_predict, 'r-', linewidth=2, label='Prediction line')
plt.xlabel('X', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('Linear Regression - Least Squares Method', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>Learned weights:
w0 (intercept): 4.2153
w1 (slope): 2.7702
</code></pre>

<h3>Implementation with scikit-learn</h3>

<pre><code class="language-python">from sklearn.linear_model import LinearRegression

# Build model
model = LinearRegression()
model.fit(X, y)

print("\nscikit-learn:")
print(f"Intercept: {model.intercept_[0]:.4f}")
print(f"Slope: {model.coef_[0][0]:.4f}")

# Prediction
y_pred = model.predict(X_new)
print(f"\nPredicted values: {y_pred.flatten()}")
</code></pre>

<hr>

<h2>1.4 Gradient Descent</h2>

<h3>Principle</h3>

<p>Calculate the gradient of the loss function and update weights in the opposite direction of the gradient.</p>

<div class="mermaid">
graph LR
    A[Initial weights w] --> B[Calculate gradient nabla J]
    B --> C[Update weights w := w - alpha nabla J]
    C --> D{Converged?}
    D -->|No| B
    D -->|Yes| E[Optimal weights w*]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#ffe0b2
    style E fill:#e8f5e9
</div>

<h3>Update Rule</h3>

<p>$$
\mathbf{w} := \mathbf{w} - \alpha \nabla_{\mathbf{w}} J(\mathbf{w})
$$</p>

<p>Gradient:</p>

<p>$$
\nabla_{\mathbf{w}} J(\mathbf{w}) = \frac{2}{m} \mathbf{X}^T (\mathbf{X}\mathbf{w} - \mathbf{y})
$$</p>

<ul>
<li>$\alpha$: Learning rate</li>
</ul>

<h3>Implementation Example</h3>

<pre><code class="language-python">def gradient_descent(X, y, alpha=0.01, n_iterations=1000):
    """
    Train linear regression using gradient descent

    Args:
        X: Feature matrix (including bias term)
        y: Target variable
        alpha: Learning rate
        n_iterations: Number of iterations

    Returns:
        w: Learned weights
        history: Loss function history
    """
    m = len(y)
    w = np.random.randn(X.shape[1], 1)  # Initialize weights
    history = []

    for i in range(n_iterations):
        # Prediction
        y_pred = X @ w

        # Calculate loss
        loss = (1 / m) * np.sum((y_pred - y) ** 2)
        history.append(loss)

        # Calculate gradient
        gradients = (2 / m) * X.T @ (y_pred - y)

        # Update weights
        w = w - alpha * gradients

        if i % 100 == 0:
            print(f"Iteration {i}: Loss = {loss:.4f}")

    return w, history

# Execute
w_gd, loss_history = gradient_descent(X_b, y, alpha=0.1, n_iterations=1000)

print("\nWeights learned by gradient descent:")
print(f"w0 (intercept): {w_gd[0][0]:.4f}")
print(f"w1 (slope): {w_gd[1][0]:.4f}")

# Visualize loss function progression
plt.figure(figsize=(10, 6))
plt.plot(loss_history, linewidth=2)
plt.xlabel('Iteration', fontsize=12)
plt.ylabel('MSE Loss', fontsize=12)
plt.title('Gradient Descent Convergence', fontsize=14)
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>Iteration 0: Loss = 6.8421
Iteration 100: Loss = 0.8752
Iteration 200: Loss = 0.8284
Iteration 300: Loss = 0.8243
Iteration 400: Loss = 0.8236
Iteration 500: Loss = 0.8235
Iteration 600: Loss = 0.8235
Iteration 700: Loss = 0.8235
Iteration 800: Loss = 0.8235
Iteration 900: Loss = 0.8235

Weights learned by gradient descent:
w0 (intercept): 4.2152
w1 (slope): 2.7703
</code></pre>

<h3>Importance of Learning Rate</h3>

<pre><code class="language-python"># Comparison with different learning rates
learning_rates = [0.001, 0.01, 0.1, 0.5]

plt.figure(figsize=(12, 8))
for i, alpha in enumerate(learning_rates):
    w, history = gradient_descent(X_b, y, alpha=alpha, n_iterations=100)
    plt.subplot(2, 2, i+1)
    plt.plot(history, linewidth=2)
    plt.title(f'Learning Rate alpha = {alpha}', fontsize=12)
    plt.xlabel('Iteration')
    plt.ylabel('MSE Loss')
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<hr>

<h2>1.5 Polynomial Regression</h2>

<h3>Overview</h3>

<p>Models <strong>nonlinear relationships</strong> that cannot be expressed by linear regression.</p>

<p>$$
y = w_0 + w_1 x + w_2 x^2 + \cdots + w_d x^d
$$</p>

<p>By transforming features, we can use the linear regression framework.</p>

<h3>Implementation Example</h3>

<pre><code class="language-python">from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline

# Generate nonlinear data
np.random.seed(42)
X = 6 * np.random.rand(100, 1) - 3
y = 0.5 * X**2 + X + 2 + np.random.randn(100, 1)

# Polynomial regression (degree 2)
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)

model = LinearRegression()
model.fit(X_poly, y)

print("Polynomial regression coefficients:")
print(f"w1 (x): {model.coef_[0][0]:.4f}")
print(f"w2 (x^2): {model.coef_[0][1]:.4f}")
print(f"Intercept: {model.intercept_[0]:.4f}")

# Prediction and visualization
X_test = np.linspace(-3, 3, 100).reshape(-1, 1)
X_test_poly = poly_features.transform(X_test)
y_pred = model.predict(X_test_poly)

plt.figure(figsize=(10, 6))
plt.scatter(X, y, alpha=0.6, label='Data')
plt.plot(X_test, y_pred, 'r-', linewidth=2, label='Polynomial regression (degree 2)')
plt.xlabel('X', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('Polynomial Regression', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

<h3>Risk of Overfitting</h3>

<pre><code class="language-python"># Comparison with different degrees
degrees = [1, 2, 5, 10]

plt.figure(figsize=(14, 10))
for i, degree in enumerate(degrees):
    poly_features = PolynomialFeatures(degree=degree, include_bias=False)
    X_poly = poly_features.fit_transform(X)

    model = LinearRegression()
    model.fit(X_poly, y)

    X_test_poly = poly_features.transform(X_test)
    y_pred = model.predict(X_test_poly)

    plt.subplot(2, 2, i+1)
    plt.scatter(X, y, alpha=0.6, label='Data')
    plt.plot(X_test, y_pred, 'r-', linewidth=2, label=f'Degree {degree}')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title(f'Polynomial Regression (Degree {degree})', fontsize=12)
    plt.ylim(-5, 15)
    plt.legend()
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<blockquote>
<p><strong>Note</strong>: If the degree is too high, overfitting occurs. At degree 10, the model overfits the data, reducing generalization performance.</p>
</blockquote>

<hr>

<h2>1.6 Regularization</h2>

<h3>Overview</h3>

<p>To prevent overfitting, we add a <strong>penalty term</strong> to the loss function.</p>

<div class="mermaid">
graph TD
    A[Regularization Methods] --> B[Ridge L2 Regularization]
    A --> C[Lasso L1 Regularization]
    A --> D[Elastic Net L1+L2]

    B --> B1[Suppresses weight magnitude]
    C --> C1[Sets some weights to zero]
    D --> D1[Balance of both]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
</div>

<h3>Ridge Regression (L2 Regularization)</h3>

<p>$$
J(\mathbf{w}) = \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2 + \alpha \sum_{j=1}^{n} w_j^2
$$</p>

<ul>
<li>$\alpha$: Regularization parameter</li>
<li>Penalizes the sum of squared weights</li>
</ul>

<pre><code class="language-python">from sklearn.linear_model import Ridge

# Ridge regression
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_poly, y)

print("Ridge regression coefficients:")
print(f"Weights: {ridge_model.coef_[0]}")
print(f"Intercept: {ridge_model.intercept_[0]:.4f}")
</code></pre>

<h3>Lasso Regression (L1 Regularization)</h3>

<p>$$
J(\mathbf{w}) = \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2 + \alpha \sum_{j=1}^{n} |w_j|
$$</p>

<ul>
<li>Penalizes the sum of absolute values of weights</li>
<li><strong>Sparsity</strong>: Sets weights of unimportant features to zero</li>
</ul>

<pre><code class="language-python">from sklearn.linear_model import Lasso

# Lasso regression
lasso_model = Lasso(alpha=0.1)
lasso_model.fit(X_poly, y)

print("\nLasso regression coefficients:")
print(f"Weights: {lasso_model.coef_}")
print(f"Intercept: {lasso_model.intercept_:.4f}")
print(f"Number of zero weights: {np.sum(lasso_model.coef_ == 0)}")
</code></pre>

<h3>Elastic Net (L1 + L2 Regularization)</h3>

<p>$$
J(\mathbf{w}) = \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2 + \alpha \rho \sum_{j=1}^{n} |w_j| + \frac{\alpha(1-\rho)}{2} \sum_{j=1}^{n} w_j^2
$$</p>

<ul>
<li>$\rho$: Balance between L1 and L2 (0 to 1)</li>
</ul>

<pre><code class="language-python">from sklearn.linear_model import ElasticNet

# Elastic Net
elastic_model = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic_model.fit(X_poly, y)

print("\nElastic Net regression coefficients:")
print(f"Weights: {elastic_model.coef_}")
print(f"Intercept: {elastic_model.intercept_:.4f}")
</code></pre>

<h3>Comparison of Regularization Parameters</h3>

<pre><code class="language-python"># Comparison with different alphas
alphas = np.logspace(-3, 2, 100)
ridge_coefs = []
lasso_coefs = []

for alpha in alphas:
    ridge = Ridge(alpha=alpha)
    ridge.fit(X_poly, y)
    ridge_coefs.append(ridge.coef_[0])

    lasso = Lasso(alpha=alpha, max_iter=10000)
    lasso.fit(X_poly, y)
    lasso_coefs.append(lasso.coef_)

ridge_coefs = np.array(ridge_coefs)
lasso_coefs = np.array(lasso_coefs)

plt.figure(figsize=(14, 6))

# Ridge
plt.subplot(1, 2, 1)
for i in range(X_poly.shape[1]):
    plt.plot(alphas, ridge_coefs[:, i], label=f'w{i+1}')
plt.xscale('log')
plt.xlabel('Alpha (Regularization Strength)', fontsize=12)
plt.ylabel('Coefficient Magnitude', fontsize=12)
plt.title('Ridge Regression: Effect of Regularization Parameter', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

# Lasso
plt.subplot(1, 2, 2)
for i in range(X_poly.shape[1]):
    plt.plot(alphas, lasso_coefs[:, i], label=f'w{i+1}')
plt.xscale('log')
plt.xlabel('Alpha (Regularization Strength)', fontsize=12)
plt.ylabel('Coefficient Magnitude', fontsize=12)
plt.title('Lasso Regression: Effect of Regularization Parameter', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<hr>

<h2>1.7 Evaluation of Regression Models</h2>

<h3>Evaluation Metrics</h3>

<table>
<thead>
<tr>
<th>Metric</th>
<th>Formula</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Mean Absolute Error</strong><br>(MAE)</td>
<td>$\frac{1}{m}\sum|y_i - \hat{y}_i|$</td>
<td>Average of prediction errors (robust to outliers)</td>
</tr>
<tr>
<td><strong>Mean Squared Error</strong><br>(MSE)</td>
<td>$\frac{1}{m}\sum(y_i - \hat{y}_i)^2$</td>
<td>Average of squared prediction errors (sensitive to outliers)</td>
</tr>
<tr>
<td><strong>Root Mean Squared Error</strong><br>(RMSE)</td>
<td>$\sqrt{\frac{1}{m}\sum(y_i - \hat{y}_i)^2}$</td>
<td>Square root of MSE (in original units)</td>
</tr>
<tr>
<td><strong>Coefficient of Determination</strong><br>(R-squared)</td>
<td>$1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}$</td>
<td>Model's explanatory power (0 to 1, higher is better)</td>
</tr>
</tbody>
</table>

<h3>Implementation Example</h3>

<pre><code class="language-python">from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Data split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_poly, y, test_size=0.2, random_state=42
)

# Model training
model = LinearRegression()
model.fit(X_train, y_train)

# Prediction
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# Evaluation
print("=== Training Data ===")
print(f"MAE:  {mean_absolute_error(y_train, y_train_pred):.4f}")
print(f"MSE:  {mean_squared_error(y_train, y_train_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_train, y_train_pred)):.4f}")
print(f"R^2:  {r2_score(y_train, y_train_pred):.4f}")

print("\n=== Test Data ===")
print(f"MAE:  {mean_absolute_error(y_test, y_test_pred):.4f}")
print(f"MSE:  {mean_squared_error(y_test, y_test_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred)):.4f}")
print(f"R^2:  {r2_score(y_test, y_test_pred):.4f}")

# Residual plot
residuals = y_test - y_test_pred

plt.figure(figsize=(14, 6))

# Predicted vs Actual
plt.subplot(1, 2, 1)
plt.scatter(y_test, y_test_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)
plt.xlabel('Actual Values', fontsize=12)
plt.ylabel('Predicted Values', fontsize=12)
plt.title('Predicted vs Actual', fontsize=14)
plt.grid(True, alpha=0.3)

# Residual plot
plt.subplot(1, 2, 2)
plt.scatter(y_test_pred, residuals, alpha=0.6)
plt.axhline(y=0, color='r', linestyle='--', linewidth=2)
plt.xlabel('Predicted Values', fontsize=12)
plt.ylabel('Residuals', fontsize=12)
plt.title('Residual Plot', fontsize=14)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Training Data ===
MAE:  0.7234
MSE:  0.8456
RMSE: 0.9196
R^2:  0.9145

=== Test Data ===
MAE:  0.7891
MSE:  0.9234
RMSE: 0.9609
R^2:  0.9023
</code></pre>

<hr>

<h2>1.8 Chapter Summary</h2>

<h3>What We Learned</h3>

<ol>
<li><p><strong>Definition of Regression Problems</strong></p>
<ul>
<li>Task of predicting continuous values</li>
<li>Real-world applications (price prediction, demand forecasting, etc.)</li>
</ul></li>
<li><p><strong>Linear Regression</strong></p>
<ul>
<li>Analytical solution using least squares method</li>
<li>Numerical solution using gradient descent</li>
</ul></li>
<li><p><strong>Polynomial Regression</strong></p>
<ul>
<li>Modeling nonlinear relationships</li>
<li>Risk of overfitting</li>
</ul></li>
<li><p><strong>Regularization</strong></p>
<ul>
<li>Ridge (L2): Suppresses weight magnitude</li>
<li>Lasso (L1): Introduces sparsity</li>
<li>Elastic Net: Balance of both</li>
</ul></li>
<li><p><strong>Evaluation Metrics</strong></p>
<ul>
<li>MAE, MSE, RMSE, R-squared</li>
<li>Importance of residual analysis</li>
</ul></li>
</ol>

<h3>Next Chapter</h3>

<p>In Chapter 2, we will learn the <strong>fundamentals of classification problems</strong>:</p>
<ul>
<li>Logistic regression</li>
<li>Decision trees</li>
<li>k-NN, SVM</li>
<li>Evaluation metrics (accuracy, recall, F1 score)</li>
</ul>

<hr>

<h2>Exercises</h2>

<h3>Problem 1 (Difficulty: Easy)</h3>
<p>List three differences between regression and classification problems.</p>

<details>
<summary>Sample Answer</summary>

<p><strong>Answer</strong>:</p>
<ol>
<li><strong>Type of output</strong>: Regression outputs continuous values, classification outputs discrete values (categories)</li>
<li><strong>Loss function</strong>: Regression uses MSE, classification uses cross-entropy</li>
<li><strong>Evaluation metrics</strong>: Regression uses RMSE/R-squared, classification uses accuracy/F1 score</li>
</ol>

</details>

<h3>Problem 2 (Difficulty: Medium)</h3>
<p>Implement linear regression with the following data and find the weights and bias.</p>

<pre><code class="language-python">X = np.array([[1], [2], [3], [4], [5]])
y = np.array([[2], [4], [5], [4], [5]])
</code></pre>

<details>
<summary>Sample Answer</summary>

<pre><code class="language-python">import numpy as np

X = np.array([[1], [2], [3], [4], [5]])
y = np.array([[2], [4], [5], [4], [5]])

# Add bias term
X_b = np.c_[np.ones((5, 1)), X]

# Normal equation
w = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y

print(f"Intercept w0: {w[0][0]:.4f}")
print(f"Slope w1: {w[1][0]:.4f}")

# Prediction
y_pred = X_b @ w
print(f"\nPredicted values: {y_pred.flatten()}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>Intercept w0: 2.2000
Slope w1: 0.6000

Predicted values: [2.8 3.4 4.  4.6 5.2]
</code></pre>

</details>

<h3>Problem 3 (Difficulty: Medium)</h3>
<p>What problems occur in gradient descent when the learning rate is too large?</p>

<details>
<summary>Sample Answer</summary>

<p><strong>Answer</strong>:</p>
<ul>
<li><strong>Divergence</strong>: The loss function overshoots the minimum and diverges</li>
<li><strong>Oscillation</strong>: Continues to oscillate around the minimum</li>
<li><strong>Non-convergence</strong>: Cannot reach the optimal solution</li>
</ul>

<p><strong>Countermeasures</strong>:</p>
<ul>
<li>Reduce the learning rate (e.g., 0.1 to 0.01)</li>
<li>Use learning rate scheduling</li>
<li>Use adaptive optimization methods (Adam, RMSprop)</li>
</ul>

</details>

<h3>Problem 4 (Difficulty: Hard)</h3>
<p>Explain the difference between Ridge regression and Lasso regression, and describe when each should be used.</p>

<details>
<summary>Sample Answer</summary>

<p><strong>Ridge Regression (L2 Regularization)</strong>:</p>
<ul>
<li>Penalizes the sum of squared weights</li>
<li>Makes weights smaller but does not set them to zero</li>
<li><strong>Use cases</strong>: When multicollinearity exists, when all features are important</li>
</ul>

<p><strong>Lasso Regression (L1 Regularization)</strong>:</p>
<ul>
<li>Penalizes the sum of absolute values of weights</li>
<li>Sets weights of unimportant features to zero (sparsity)</li>
<li><strong>Use cases</strong>: When feature selection is needed, when interpretability is desired</li>
</ul>

<p><strong>Selection Criteria</strong>:</p>
<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommended Method</th>
</tr>
</thead>
<tbody>
<tr>
<td>Many features with unknown importance</td>
<td>Lasso</td>
</tr>
<tr>
<td>Multicollinearity present</td>
<td>Ridge</td>
</tr>
<tr>
<td>Feature selection needed</td>
<td>Lasso</td>
</tr>
<tr>
<td>Want to use all features</td>
<td>Ridge</td>
</tr>
<tr>
<td>Uncertain which to use</td>
<td>Elastic Net</td>
</tr>
</tbody>
</table>

</details>

<h3>Problem 5 (Difficulty: Hard)</h3>
<p>Complete the following code to find the optimal alpha for Ridge regression using cross-validation.</p>

<pre><code class="language-python">from sklearn.model_selection import cross_val_score
from sklearn.linear_model import Ridge

# Data generation (omitted)
alphas = np.logspace(-3, 3, 50)

# Implement here
</code></pre>

<details>
<summary>Sample Answer</summary>

<pre><code class="language-python">from sklearn.model_selection import cross_val_score
from sklearn.linear_model import Ridge
import numpy as np
import matplotlib.pyplot as plt

# Data generation
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=5, include_bias=False)
X_poly = poly.fit_transform(X)

alphas = np.logspace(-3, 3, 50)
scores = []

for alpha in alphas:
    ridge = Ridge(alpha=alpha)
    # 5-fold cross-validation
    cv_scores = cross_val_score(ridge, X_poly, y.ravel(),
                                 cv=5, scoring='neg_mean_squared_error')
    scores.append(-cv_scores.mean())  # Convert negative MSE to positive

# Find optimal alpha
best_alpha = alphas[np.argmin(scores)]
best_score = np.min(scores)

print(f"Optimal alpha: {best_alpha:.4f}")
print(f"Minimum MSE: {best_score:.4f}")

# Visualization
plt.figure(figsize=(10, 6))
plt.plot(alphas, scores, linewidth=2)
plt.axvline(best_alpha, color='r', linestyle='--',
            label=f'Optimal alpha = {best_alpha:.4f}')
plt.xscale('log')
plt.xlabel('Alpha', fontsize=12)
plt.ylabel('MSE (Cross-Validation)', fontsize=12)
plt.title('Ridge Regression: Finding Optimal Alpha', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>Optimal alpha: 2.1544
Minimum MSE: 1.0234
</code></pre>

</details>

<hr>

<h2>References</h2>

<ol>
<li>Bishop, C. M. (2006). <em>Pattern Recognition and Machine Learning</em>. Springer.</li>
<li>Hastie, T., Tibshirani, R., & Friedman, J. (2009). <em>The Elements of Statistical Learning</em>. Springer.</li>
<li>Geron, A. (2019). <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em>. O'Reilly Media.</li>
</ol>

<div class="navigation">
    <a href="index.html" class="nav-button">Series Contents</a>
    <a href="chapter2-classification.html" class="nav-button">Next Chapter: Classification Fundamentals</a>
</div>

    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is intended solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without any warranty, express or implied, including but not limited to warranties of merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The authors and Tohoku University assume no responsibility for the content, availability, or security of external links, third-party data, tools, or libraries.</li>
            <li>To the maximum extent permitted by applicable law, the authors and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>Copyright and licensing of this content follow the stated terms (e.g., CC BY 4.0). Such licenses typically include disclaimer clauses.</li>
        </ul>
    </section>

<footer>
        <p><strong>Author</strong>: AI Terakoya Content Team</p>
        <p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-20</p>
        <p><strong>License</strong>: Creative Commons BY 4.0</p>
        <p>© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
