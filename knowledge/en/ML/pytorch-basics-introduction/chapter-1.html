<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chapter 1: PyTorch Fundamentals - PyTorch Basics Introduction - AI Terakoya</title>
  <meta name="description" content="Learn PyTorch fundamentals. Understand comparisons with TensorFlow, installation methods, Tensor basics, and automatic differentiation mechanisms through practical code examples.">

  <style>
    :root {
      --color-primary: #2c3e50;
      --color-primary-dark: #1a252f;
      --color-accent: #7b2cbf;
      --color-accent-light: #9d4edd;
      --color-text: #2d3748;
      --color-text-light: #4a5568;
      --color-bg: #ffffff;
      --color-bg-alt: #f7fafc;
      --color-border: #e2e8f0;
      --color-code-bg: #f8f9fa;
      --color-link: #3182ce;
      --color-link-hover: #2c5aa0;

      --spacing-xs: 0.5rem;
      --spacing-sm: 1rem;
      --spacing-md: 1.5rem;
      --spacing-lg: 2rem;
      --spacing-xl: 3rem;

      --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
      --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

      --border-radius: 8px;
      --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: var(--font-body);
      line-height: 1.7;
      color: var(--color-text);
      background-color: var(--color-bg);
      font-size: 16px;
    }

    header {
      background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
      color: white;
      padding: var(--spacing-xl) var(--spacing-md);
      margin-bottom: var(--spacing-xl);
      box-shadow: var(--box-shadow);
    }

    .header-content {
      max-width: 900px;
      margin: 0 auto;
    }

    h1 {
      font-size: 2rem;
      font-weight: 700;
      margin-bottom: var(--spacing-sm);
      line-height: 1.2;
    }

    .subtitle {
      font-size: 1.1rem;
      opacity: 0.95;
      font-weight: 400;
      margin-bottom: var(--spacing-md);
    }

    .meta {
      display: flex;
      flex-wrap: wrap;
      gap: var(--spacing-md);
      font-size: 0.9rem;
      opacity: 0.9;
    }

    .meta-item {
      display: flex;
      align-items: center;
      gap: 0.3rem;
    }

    .container {
      max-width: 900px;
      margin: 0 auto;
      padding: 0 var(--spacing-md) var(--spacing-xl);
    }

    h2 {
      font-size: 1.75rem;
      color: var(--color-primary);
      margin-top: var(--spacing-xl);
      margin-bottom: var(--spacing-md);
      padding-bottom: var(--spacing-xs);
      border-bottom: 3px solid var(--color-accent);
    }

    h3 {
      font-size: 1.4rem;
      color: var(--color-primary);
      margin-top: var(--spacing-lg);
      margin-bottom: var(--spacing-sm);
    }

    h4 {
      font-size: 1.1rem;
      color: var(--color-primary-dark);
      margin-top: var(--spacing-md);
      margin-bottom: var(--spacing-sm);
    }

    p {
      margin-bottom: var(--spacing-md);
      color: var(--color-text);
    }

    a {
      color: var(--color-link);
      text-decoration: none;
      transition: color 0.2s;
    }

    a:hover {
      color: var(--color-link-hover);
      text-decoration: underline;
    }

    ul, ol {
      margin-left: var(--spacing-lg);
      margin-bottom: var(--spacing-md);
    }

    li {
      margin-bottom: var(--spacing-xs);
      color: var(--color-text);
    }

    pre {
      background-color: var(--color-code-bg);
      border: 1px solid var(--color-border);
      border-radius: var(--border-radius);
      padding: var(--spacing-md);
      overflow-x: auto;
      margin-bottom: var(--spacing-md);
      font-family: var(--font-mono);
      font-size: 0.9rem;
      line-height: 1.5;
    }

    code {
      font-family: var(--font-mono);
      font-size: 0.9em;
      background-color: var(--color-code-bg);
      padding: 0.2em 0.4em;
      border-radius: 3px;
    }

    pre code {
      background-color: transparent;
      padding: 0;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin-bottom: var(--spacing-md);
      font-size: 0.95rem;
    }

    th, td {
      border: 1px solid var(--color-border);
      padding: var(--spacing-sm);
      text-align: left;
    }

    th {
      background-color: var(--color-bg-alt);
      font-weight: 600;
      color: var(--color-primary);
    }

    blockquote {
      border-left: 4px solid var(--color-accent);
      padding-left: var(--spacing-md);
      margin: var(--spacing-md) 0;
      color: var(--color-text-light);
      font-style: italic;
      background-color: var(--color-bg-alt);
      padding: var(--spacing-md);
      border-radius: var(--border-radius);
    }

    .mermaid {
      text-align: center;
      margin: var(--spacing-lg) 0;
      background-color: var(--color-bg-alt);
      padding: var(--spacing-md);
      border-radius: var(--border-radius);
    }

    details {
      background-color: var(--color-bg-alt);
      border: 1px solid var(--color-border);
      border-radius: var(--border-radius);
      padding: var(--spacing-md);
      margin-bottom: var(--spacing-md);
    }

    summary {
      cursor: pointer;
      font-weight: 600;
      color: var(--color-primary);
      user-select: none;
      padding: var(--spacing-xs);
      margin: calc(-1 * var(--spacing-md));
      padding: var(--spacing-md);
      border-radius: var(--border-radius);
    }

    summary:hover {
      background-color: rgba(123, 44, 191, 0.1);
    }

    details[open] summary {
      margin-bottom: var(--spacing-md);
      border-bottom: 1px solid var(--color-border);
    }

    .chapter-description {
      margin: 1.5rem 0;
      padding: 1rem;
      background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
      border-left: 4px solid #7b2cbf;
      border-radius: 8px;
      font-size: 1.05rem;
      line-height: 1.8;
      color: #2d3748;
    }

    .learning-objectives {
      background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
      padding: var(--spacing-lg);
      border-radius: var(--border-radius);
      border-left: 4px solid var(--color-accent);
      margin-bottom: var(--spacing-xl);
    }

    .learning-objectives h2 {
      margin-top: 0;
      border-bottom: none;
    }

    .navigation {
      display: flex;
      justify-content: space-between;
      gap: var(--spacing-md);
      margin: var(--spacing-xl) 0;
      padding-top: var(--spacing-lg);
      border-top: 2px solid var(--color-border);
    }

    .nav-button {
      flex: 1;
      padding: var(--spacing-md);
      background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
      color: white;
      border-radius: var(--border-radius);
      text-align: center;
      font-weight: 600;
      transition: transform 0.2s, box-shadow 0.2s;
      box-shadow: var(--box-shadow);
    }

    .nav-button:hover {
      transform: translateY(-2px);
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
      text-decoration: none;
    }

    .nav-button.disabled {
      opacity: 0.5;
      cursor: not-allowed;
      background: #6c757d;
    }

    .nav-button.disabled:hover {
      transform: none;
      box-shadow: var(--box-shadow);
    }

    footer {
      margin-top: var(--spacing-xl);
      padding: var(--spacing-lg) var(--spacing-md);
      background-color: var(--color-bg-alt);
      border-top: 1px solid var(--color-border);
      text-align: center;
      font-size: 0.9rem;
      color: var(--color-text-light);
    }
    .disclaimer {
      max-width: 900px;
      margin: 2rem auto;
      padding: 1.5rem;
      background: #f8f9fa;
      border-left: 4px solid #6c757d;
      border-radius: 4px;
    }

    .disclaimer h3 {
      color: #495057;
      margin-bottom: 1rem;
      font-size: 1.1rem;
    }

    .disclaimer ul {
      list-style: none;
      padding-left: 0;
    }

    .disclaimer li {
      padding: 0.5rem 0;
      padding-left: 1.5rem;
      position: relative;
      font-size: 0.9rem;
      color: #6c757d;
      line-height: 1.6;
    }

    .disclaimer li::before {
      content: "‚ö†Ô∏è";
      position: absolute;
      left: 0;
    }

    /* Breadcrumb styles */
    .breadcrumb {
      background: #f7fafc;
      padding: 0.75rem 1rem;
      border-bottom: 1px solid #e2e8f0;
      font-size: 0.9rem;
    }

    .breadcrumb-content {
      max-width: 900px;
      margin: 0 auto;
      display: flex;
      align-items: center;
      flex-wrap: wrap;
      gap: 0.5rem;
    }

    .breadcrumb a {
      color: #667eea;
      text-decoration: none;
      transition: color 0.2s;
    }

    .breadcrumb a:hover {
      color: #764ba2;
      text-decoration: underline;
    }

    .breadcrumb-separator {
      color: #a0aec0;
      margin: 0 0.25rem;
    }

    .breadcrumb-current {
      color: #4a5568;
      font-weight: 500;
    }

    .info-box {
      background: #e3f2fd;
      border-left: 4px solid #2196f3;
      padding: var(--spacing-md);
      margin: var(--spacing-md) 0;
      border-radius: var(--border-radius);
    }

    .warning-box {
      background: #fff3e0;
      border-left: 4px solid #ff9800;
      padding: var(--spacing-md);
      margin: var(--spacing-md) 0;
      border-radius: var(--border-radius);
    }

    .success-box {
      background: #e8f5e9;
      border-left: 4px solid #4caf50;
      padding: var(--spacing-md);
      margin: var(--spacing-md) 0;
      border-radius: var(--border-radius);
    }

    /* Responsive */
    @media (max-width: 768px) {
      .container {
        padding: 0 1rem 2rem;
      }

      h1 {
        font-size: 1.6rem;
      }

      h2 {
        font-size: 1.4rem;
      }

      .navigation {
        flex-direction: column;
      }
    }
  </style>

  <!-- Mermaid for diagrams -->
  <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
  <!-- Prism.js for syntax highlighting -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
  <!-- MathJax for equations -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <script>
    // Mermaid.js initialization
    document.addEventListener('DOMContentLoaded', function() {
      if (typeof mermaid !== 'undefined') {
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
      }
    });
  </script>
</head>
<body>
  <nav class="breadcrumb">
    <div class="breadcrumb-content">
      <a href="../../index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="./index.html">PyTorch Basics Introduction</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
    </div>
  </nav>

  <header>
    <div class="header-content">
      <h1>Chapter 1: PyTorch Fundamentals</h1>
      <p class="subtitle">PyTorch as a Deep Learning Framework</p>
      <div class="meta">
        <span class="meta-item">üìñ Reading time: 25-30 min</span>
        <span class="meta-item">üìä Difficulty: Beginner</span>
        <span class="meta-item">üíª Code examples: 10</span>
        <span class="meta-item">üìù Exercises: 5</span>
      </div>
    </div>
  </header>

  <main class="container">
    <div class="chapter-description">
      <p>In this chapter, you'll understand what PyTorch is and why it's widely used in deep learning. You'll learn about comparisons with TensorFlow, installation methods, Tensor fundamentals, and automatic differentiation mechanisms. Through practical code examples, you'll become familiar with the PyTorch programming style.</p>
    </div>

    <div class="learning-objectives">
      <h2>Learning Objectives</h2>
      <ul>
        <li>‚úÖ Explain PyTorch's features and differences from TensorFlow</li>
        <li>‚úÖ Install PyTorch and verify it works</li>
        <li>‚úÖ Create and manipulate basic Tensors</li>
        <li>‚úÖ Understand the concept of dynamic computation graphs</li>
        <li>‚úÖ Practice basic usage of automatic differentiation (autograd)</li>
      </ul>
    </div>

    <h2 id="what-is-pytorch">1. What is PyTorch</h2>

    <p><strong>PyTorch</strong> is an open-source deep learning framework developed by Facebook's (now Meta) AI Research Lab (FAIR). Since its release in 2016, it has been widely used by researchers and practitioners.</p>

    <h3>Main Features of PyTorch</h3>

    <ul>
      <li><strong>Dynamic Computation Graph</strong>: Computation graphs are built at runtime, enabling flexible Python-like coding</li>
      <li><strong>Pythonic API</strong>: Intuitive API design similar to NumPy</li>
      <li><strong>Powerful automatic differentiation</strong>: Automatic gradient computation via autograd functionality</li>
      <li><strong>GPU support</strong>: High-speed parallel computation using CUDA</li>
      <li><strong>Rich ecosystem</strong>: torchvision (images), torchtext (text), torchaudio (audio), etc.</li>
    </ul>

    <h3>PyTorch vs TensorFlow</h3>

    <table>
      <thead>
        <tr>
          <th>Feature</th>
          <th>PyTorch</th>
          <th>TensorFlow</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Computation Graph</td>
          <td>Dynamic (Define-by-Run)</td>
          <td>Static (Define-and-Run) *Dynamic available in TF2.0+</td>
        </tr>
        <tr>
          <td>Learning Curve</td>
          <td>Intuitive, Python-like</td>
          <td>Somewhat complex (especially TF1.x)</td>
        </tr>
        <tr>
          <td>Debugging</td>
          <td>Possible with standard Python debugger</td>
          <td>Somewhat difficult (TF1.x), improved in TF2.0</td>
        </tr>
        <tr>
          <td>Research Adoption</td>
          <td>Very high (mainstream for paper implementations)</td>
          <td>Moderate</td>
        </tr>
        <tr>
          <td>Production</td>
          <td>Deployable with TorchServe</td>
          <td>Strong with TensorFlow Serving</td>
        </tr>
        <tr>
          <td>Mobile Support</td>
          <td>PyTorch Mobile</td>
          <td>TensorFlow Lite (mature)</td>
        </tr>
      </tbody>
    </table>

    <div class="info-box">
      <p><strong>üí° Which Should You Choose?</strong></p>
      <p>PyTorch is popular for research and prototyping. On the other hand, TensorFlow is a strong choice for large-scale production environments or when existing TensorFlow infrastructure is available. Learning both allows you to use the optimal tool for each situation.</p>
    </div>

    <h2 id="installation">2. Installation and Environment Setup</h2>

    <h3>Method 1: Installation via pip (Recommended)</h3>

    <pre><code class="language-bash"># CPU version
pip install torch torchvision torchaudio

# GPU version (for CUDA 11.8)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# GPU version (for CUDA 12.1)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
</code></pre>

    <h3>Method 2: Installation via conda</h3>

    <pre><code class="language-bash"># CPU version
conda install pytorch torchvision torchaudio -c pytorch

# GPU version (for CUDA 11.8)
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
</code></pre>

    <h3>Method 3: Google Colab (No environment setup required)</h3>

    <p>Using Google Colab, you can use PyTorch with just a browser. GPU is also available for free.</p>

    <pre><code class="language-python">import torch
print(torch.__version__)
print("CUDA available:", torch.cuda.is_available())
</code></pre>

    <h3>Installation Verification</h3>

    <p>Let's verify that PyTorch is correctly installed with the following code:</p>

    <pre><code class="language-python">import torch

# PyTorch version
print(f"PyTorch version: {torch.__version__}")

# Check if CUDA (GPU) is available
print(f"CUDA available: {torch.cuda.is_available()}")

# CUDA version
if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")
    print(f"GPU device: {torch.cuda.get_device_name(0)}")

# Simple Tensor creation
x = torch.tensor([1, 2, 3])
print(f"Sample tensor: {x}")
</code></pre>

    <p><strong>Example output:</strong></p>

    <pre><code>PyTorch version: 2.1.0
CUDA available: True
CUDA version: 11.8
GPU device: Tesla T4
Sample tensor: tensor([1, 2, 3])
</code></pre>

    <h2 id="tensor-basics">3. Tensor Fundamentals</h2>

    <p><strong>Tensor</strong> is the central data structure in PyTorch. Similar to NumPy's ndarray, but supports GPU computation and automatic differentiation.</p>

    <h3>Tensor Creation</h3>

    <pre><code class="language-python">import torch

# Create Tensor from list
x = torch.tensor([1, 2, 3, 4, 5])
print(x)
# Output: tensor([1, 2, 3, 4, 5])

# 2D Tensor (matrix)
matrix = torch.tensor([[1, 2], [3, 4], [5, 6]])
print(matrix)
# Output: tensor([[1, 2],
#               [3, 4],
#               [5, 6]])

# Specify data type
float_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)
print(float_tensor)
# Output: tensor([1., 2., 3.])

# Special Tensors
zeros = torch.zeros(3, 4)  # 3x4 matrix of zeros
ones = torch.ones(2, 3)    # 2x3 matrix of ones
random = torch.rand(2, 2)  # 2x2 random matrix (uniform distribution 0-1)
randn = torch.randn(2, 2)  # 2x2 random matrix (standard normal distribution)

print("Zeros:\n", zeros)
print("Ones:\n", ones)
print("Random:\n", random)
print("Randn:\n", randn)
</code></pre>

    <h3>Tensor Attributes</h3>

    <pre><code class="language-python">x = torch.randn(3, 4)

print(f"Shape: {x.shape}")           # Shape: torch.Size([3, 4])
print(f"Size: {x.size()}")           # Shape (method version)
print(f"Data type: {x.dtype}")       # Data type: torch.float32
print(f"Device: {x.device}")         # Device: cpu
print(f"Requires grad: {x.requires_grad}")  # Gradient computation: False
print(f"Number of dimensions: {x.ndim}")    # Number of dimensions: 2
print(f"Number of elements: {x.numel()}")   # Number of elements: 12
</code></pre>

    <h3>Interconversion with NumPy</h3>

    <pre><code class="language-python">import numpy as np
import torch

# NumPy array ‚Üí PyTorch Tensor
numpy_array = np.array([1, 2, 3, 4, 5])
tensor_from_numpy = torch.from_numpy(numpy_array)
print(f"From NumPy: {tensor_from_numpy}")

# PyTorch Tensor ‚Üí NumPy array
tensor = torch.tensor([10, 20, 30])
numpy_from_tensor = tensor.numpy()
print(f"To NumPy: {numpy_from_tensor}")

# Important: Memory is shared, so modifying one changes the other
numpy_array[0] = 100
print(f"Changed tensor: {tensor_from_numpy}")  # First element becomes 100
</code></pre>

    <div class="warning-box">
      <p><strong>‚ö†Ô∏è Note</strong></p>
      <p><code>from_numpy()</code> and <code>numpy()</code> share memory. If you need a copy of the data, use <code>tensor.clone()</code> or <code>numpy_array.copy()</code>.</p>
    </div>

    <h2 id="basic-operations">4. Basic Tensor Operations</h2>

    <h3>Arithmetic Operations</h3>

    <pre><code class="language-python">import torch

x = torch.tensor([1, 2, 3])
y = torch.tensor([4, 5, 6])

# Element-wise operations
print(x + y)      # tensor([5, 7, 9])
print(x - y)      # tensor([-3, -3, -3])
print(x * y)      # tensor([4, 10, 18])
print(x / y)      # tensor([0.25, 0.4, 0.5])

# Operator vs method versions
print(torch.add(x, y))      # Same as x + y
print(torch.mul(x, y))      # Same as x * y

# In-place operations (modify original Tensor)
x.add_(y)  # Same as x = x + y (with underscore)
print(x)   # tensor([5, 7, 9])
</code></pre>

    <h3>Matrix Operations</h3>

    <pre><code class="language-python">import torch

# Matrix multiplication
A = torch.tensor([[1, 2], [3, 4]])
B = torch.tensor([[5, 6], [7, 8]])

# @ operator or torch.matmul()
C = A @ B
print(C)
# Output: tensor([[19, 22],
#               [43, 50]])

# Transpose
print(A.T)
# Output: tensor([[1, 3],
#               [2, 4]])

# Vector dot product
v1 = torch.tensor([1, 2, 3])
v2 = torch.tensor([4, 5, 6])
dot_product = torch.dot(v1, v2)
print(f"Dot product: {dot_product}")  # 1*4 + 2*5 + 3*6 = 32
</code></pre>

    <h3>Shape Operations</h3>

    <pre><code class="language-python">import torch

x = torch.randn(2, 3, 4)  # 2x3x4 Tensor
print(f"Original shape: {x.shape}")

# reshape: Convert to new shape (copies if not contiguous)
y = x.reshape(2, 12)
print(f"Reshaped: {y.shape}")  # torch.Size([2, 12])

# view: Convert to new shape (must be contiguous)
z = x.view(-1)  # Flatten to 1D (-1 means auto-calculate)
print(f"Flattened: {z.shape}")  # torch.Size([24])

# unsqueeze: Add dimension
a = torch.tensor([1, 2, 3])
b = a.unsqueeze(0)  # Add at 0th dimension
print(f"Original: {a.shape}, After unsqueeze(0): {b.shape}")
# Original: torch.Size([3]), After unsqueeze(0): torch.Size([1, 3])

# squeeze: Remove dimension (remove dimensions with size 1)
c = torch.randn(1, 3, 1, 4)
d = c.squeeze()
print(f"Before squeeze: {c.shape}, After squeeze: {d.shape}")
# Before squeeze: torch.Size([1, 3, 1, 4]), After squeeze: torch.Size([3, 4])
</code></pre>

    <h2 id="autograd">5. Automatic Differentiation (Autograd) Fundamentals</h2>

    <p>PyTorch's <strong>autograd</strong> feature provides automatic differentiation, which is key to deep learning. This allows automatic calculation of gradients for complex computations.</p>

    <h3>requires_grad and backward()</h3>

    <pre><code class="language-python">import torch

# Enable gradient tracking with requires_grad=True
x = torch.tensor([2.0], requires_grad=True)
print(f"x: {x}")
print(f"x.requires_grad: {x.requires_grad}")

# Perform computation
y = x ** 2 + 3 * x + 1
print(f"y: {y}")

# Gradient computation (dy/dx)
y.backward()  # Calculate gradient
print(f"x.grad: {x.grad}")  # dy/dx = 2x + 3 = 2*2 + 3 = 7
</code></pre>

    <p><strong>Mathematical background:</strong></p>
    <p>$$y = x^2 + 3x + 1$$</p>
    <p>$$\frac{dy}{dx} = 2x + 3$$</p>
    <p>$$x = 2 \text{ when }, \frac{dy}{dx} = 2(2) + 3 = 7$$</p>

    <h3>Complex Computation Graphs</h3>

    <pre><code class="language-python">import torch

# Build computation graph with multiple variables
a = torch.tensor([3.0], requires_grad=True)
b = torch.tensor([4.0], requires_grad=True)

# Complex computation
c = a * b           # c = 3 * 4 = 12
d = c ** 2          # d = 12^2 = 144
e = torch.sin(d)    # e = sin(144)
f = e + c           # f = sin(144) + 12

print(f"f: {f}")

# Gradient computation
f.backward()

print(f"df/da: {a.grad}")
print(f"df/db: {b.grad}")
</code></pre>

    <div class="mermaid">
graph LR
  A[a=3] --> C[c=a*b]
  B[b=4] --> C
  C --> D[d=c^2]
  D --> E[e=sin d]
  E --> F[f=e+c]
  C --> F
    </div>

    <h3>Gradient Accumulation and Initialization</h3>

    <pre><code class="language-python">import torch

x = torch.tensor([1.0], requires_grad=True)

# First computation
y = x ** 2
y.backward()
print(f"1st gradient: {x.grad}")  # 2.0

# Second computation (gradients accumulate!)
y = x ** 3
y.backward()
print(f"2nd gradient (accumulated): {x.grad}")  # 2.0 + 3.0 = 5.0

# Reset gradient to zero
x.grad.zero_()
y = x ** 3
y.backward()
print(f"3rd gradient (after reset): {x.grad}")  # 3.0
</code></pre>

    <div class="warning-box">
      <p><strong>‚ö†Ô∏è Important</strong></p>
      <p>PyTorch <strong>accumulates</strong> gradients by default. In training loops, you need to call <code>optimizer.zero_grad()</code> or <code>tensor.grad.zero_()</code> to reset gradients at each iteration.</p>
    </div>

    <h2 id="gpu">6. Using GPUs</h2>

    <p>In PyTorch, you can easily move Tensors to GPUs for high-speed computation.</p>

    <pre><code class="language-python">import torch

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Move Tensor to GPU
x = torch.randn(3, 3)
x = x.to(device)
print(f"x device: {x.device}")

# Computation on GPU
y = torch.randn(3, 3).to(device)
z = x + y
print(f"z device: {z.device}")

# Move back to CPU
z_cpu = z.to("cpu")
print(f"z_cpu device: {z_cpu.device}")

# Or use .cpu() method
z_cpu = z.cpu()
</code></pre>

    <div class="info-box">
      <p><strong>üí° Best Practice</strong></p>
      <p>It's important to place models and data on the same device. Operations between different devices will result in errors.</p>
    </div>

    <h2 id="first-program">7. Your First Complete PyTorch Program</h2>

    <p>Let's integrate what we've learned so far and write a simple program to learn a linear function.</p>

    <pre><code class="language-python">import torch

# Data generation: y = 3x + 2 with noise
torch.manual_seed(42)
X = torch.randn(100, 1)
y_true = 3 * X + 2 + torch.randn(100, 1) * 0.5

# Parameter initialization (to be learned)
w = torch.randn(1, requires_grad=True)
b = torch.randn(1, requires_grad=True)

# Learning rate
learning_rate = 0.01

# Training loop
for epoch in range(100):
    # Prediction
    y_pred = w * X + b

    # Loss computation (mean squared error)
    loss = ((y_pred - y_true) ** 2).mean()

    # Gradient computation
    loss.backward()

    # Parameter update (gradient descent)
    with torch.no_grad():
        w -= learning_rate * w.grad
        b -= learning_rate * b.grad

    # Reset gradients
    w.grad.zero_()
    b.grad.zero_()

    # Display results every 10 epochs
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1}: Loss = {loss.item():.4f}, w = {w.item():.4f}, b = {b.item():.4f}")

print(f"\nTraining complete!")
print(f"Final parameters: w = {w.item():.4f}, b = {b.item():.4f}")
print(f"True values: w = 3.0, b = 2.0")
</code></pre>

    <p><strong>Example output:</strong></p>

    <pre><code>Epoch 10: Loss = 0.3245, w = 2.7234, b = 1.8456
Epoch 20: Loss = 0.2456, w = 2.8567, b = 1.9123
Epoch 30: Loss = 0.2123, w = 2.9123, b = 1.9567
...
Epoch 100: Loss = 0.1234, w = 2.9876, b = 1.9934

Training complete!
Final parameters: w = 2.9876, b = 1.9934
True values: w = 3.0, b = 2.0
</code></pre>

    <h2 id="exercises">Exercises</h2>

    <details>
      <summary><strong>Exercise 1: Tensor Creation and Manipulation</strong></summary>
      <p>Implement the following operations:</p>
      <ol>
        <li>Create a 5x5 random Tensor (standard normal distribution)</li>
        <li>Calculate the mean, standard deviation, maximum, and minimum of that Tensor</li>
        <li>Flatten the Tensor to 1D</li>
      </ol>
      <pre><code class="language-python"># Write your code here
</code></pre>
    </details>

    <details>
      <summary><strong>Exercise 2: NumPy Conversion</strong></summary>
      <p>Convert NumPy array <code>np.array([[1, 2], [3, 4], [5, 6]])</code> to a PyTorch Tensor, multiply each element by 2, and convert back to a NumPy array.</p>
    </details>

    <details>
      <summary><strong>Exercise 3: Automatic Differentiation</strong></summary>
      <p>For function $f(x, y) = x^2 + y^2 + 2xy$, calculate partial derivatives $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ at point $(x=1, y=2)$ using autograd.</p>
      <p>Hint: Mathematically $\frac{\partial f}{\partial x} = 2x + 2y$, $\frac{\partial f}{\partial y} = 2y + 2x$</p>
    </details>

    <details>
      <summary><strong>Exercise 4: GPU Transfer</strong></summary>
      <p>Create a 3x3 random Tensor, transfer it to GPU (if available), calculate the square of that Tensor, and move it back to CPU.</p>
    </details>

    <details>
      <summary><strong>Exercise 5: Simple Optimization Problem</strong></summary>
      <p>Find the value of x that minimizes function $f(x) = (x - 5)^2$ using gradient descent. Initial value is $x=0$, learning rate is 0.1, execute for 100 steps.</p>
      <p>Expected answer: $x \approx 5$</p>
    </details>

    <h2 id="summary">Summary</h2>

    <p>In this chapter, you learned PyTorch fundamentals:</p>

    <ul>
      <li>‚úÖ PyTorch is a flexible deep learning framework adopting dynamic computation graphs</li>
      <li>‚úÖ Compared to TensorFlow, it enables intuitive Python-like coding</li>
      <li>‚úÖ Tensor creation, manipulation, and interconversion with NumPy</li>
      <li>‚úÖ Gradient computation using automatic differentiation (autograd)</li>
      <li>‚úÖ High-speed computation using GPUs</li>
    </ul>

    <div class="success-box">
      <p><strong>üéâ Next Steps</strong></p>
      <p>In the next chapter, you'll learn more advanced Tensor operations (indexing, slicing, broadcasting) and acquire skills needed for real data processing.</p>
    </div>

    <div class="navigation">
      <a href="./index.html" class="nav-button">‚Üê Series Overview</a>
      <span class="nav-button disabled">Chapter 2: Tensor Operations (Coming Soon) ‚Üí</span>
    </div>

    <hr style="margin: 2rem 0;">

    <p><strong>Reference Resources</strong></p>
    <ul>
      <li><a href="https://pytorch.org/docs/stable/index.html" target="_blank">PyTorch Official Documentation</a></li>
      <li><a href="https://pytorch.org/tutorials/" target="_blank">PyTorch Official Tutorials</a></li>
      <li><a href="https://github.com/pytorch/pytorch" target="_blank">PyTorch GitHub</a></li>
    </ul>

  </main>

  <section class="disclaimer">
    <h3>Disclaimer</h3>
    <ul>
      <li>This content is intended solely for educational, research, and informational purposes and does not provide professional advice (legal, accounting, technical guarantees, etc.).</li>
      <li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, either express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
      <li>The authors and Tohoku University assume no responsibility for the content, availability, or security of external links, third-party data, tools, libraries, etc.</li>
      <li>To the maximum extent permitted by applicable law, the authors and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
      <li>The content of this material may be changed, updated, or discontinued without notice.</li>
      <li>The copyright and license of this content are subject to the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
    </ul>
  </section>

  <footer>
    <div class="container">
      <p>&copy; 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
      <p>Licensed under CC BY 4.0</p>
    </div>
  </footer>
</body>
</html>
