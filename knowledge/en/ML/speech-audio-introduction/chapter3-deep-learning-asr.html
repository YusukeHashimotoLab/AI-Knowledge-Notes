<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 3: Speech Recognition with Deep Learning - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/speech-audio-introduction/index.html">Speech Audio</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 3</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/ML/speech-audio-introduction/chapter3-deep-learning-asr.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>

<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="header-content">
<h1>Chapter 3: Speech Recognition with Deep Learning</h1>
<p class="subtitle">State-of-the-art Speech Recognition with CTC, Attention, RNN-T, and Whisper</p>
<div class="meta">
<span class="meta-item">üìö Introduction to Speech and Audio Processing</span>
<span class="meta-item">‚è±Ô∏è 60 minutes</span>
<span class="meta-item">üè∑Ô∏è ML-D03</span>
</div>
</div>
</header>
<div class="container">
<h2>What You Will Learn in This Chapter</h2>
<p>This chapter introduces modern Automatic Speech Recognition (ASR) techniques using deep learning. We cover fundamental approaches such as CTC and Attention mechanisms, as well as state-of-the-art models like Whisper, with practical code examples.</p>
<ul>
<li>Training and inference with CTC (Connectionist Temporal Classification)</li>
<li>Encoder-Decoder models with Attention mechanism</li>
<li>Streaming speech recognition with RNN-Transducer</li>
<li>Utilizing OpenAI Whisper and Japanese speech recognition</li>
<li>Building practical ASR applications</li>
</ul>
<h2>1. CTC (Connectionist Temporal Classification)</h2>
<h3>1.1 What is CTC</h3>
<p>CTC is a method that can learn sequence-to-sequence transformations where input and output lengths differ, without requiring explicit alignment information. In speech recognition, it automatically learns the correspondence between acoustic feature frames (input) and text (output).</p>
<h4>Key Concepts of CTC</h4>
<ul>
<li><strong>Blank symbol</strong>: A special symbol representing positions with no output (typically "-")</li>
<li><strong>Collapsing repeats</strong>: Merging consecutive identical symbols into one</li>
<li><strong>Many-to-one mapping</strong>: Multiple frames can correspond to the same character</li>
<li><strong>Conditional independence</strong>: Outputs at each time step are generated independently</li>
</ul>
<h3>1.2 CTC Loss Function</h3>
<p>CTC loss is computed by marginalizing the probabilities of all possible alignment paths. It can be efficiently calculated using the Forward-Backward algorithm.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn
import torch.nn.functional as F

class CTCASRModel(nn.Module):
    &quot;&quot;&quot;Speech recognition model using CTC loss&quot;&quot;&quot;

    def __init__(self, input_dim=80, hidden_dim=256, num_classes=29, num_layers=3):
        &quot;&quot;&quot;
        Args:
            input_dim: inputfeatures dimensions(MFCC or Mel Spectrogram)
            hidden_dim: Dimension of LSTM hidden layer
            num_classes: Number of output classes (alphabet + blank)
            num_layers: Number of LSTM layers
        &quot;&quot;&quot;
        super(CTCASRModel, self).__init__()

        self.lstm = nn.LSTM(
            input_dim,
            hidden_dim,
            num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=0.2
        )

        # Output dimension is doubled due to bidirectional LSTM
        self.classifier = nn.Linear(hidden_dim * 2, num_classes)

    def forward(self, x, lengths):
        &quot;&quot;&quot;
        Args:
            x: (batch, time, features)
            lengths: Actual length of each sample
        Returns:
            log_probs: (time, batch, num_classes)
            output_lengths: Output length of each sample
        &quot;&quot;&quot;
        # Efficiently process variable-length inputs with PackedSequence
        packed = nn.utils.rnn.pack_padded_sequence(
            x, lengths.cpu(), batch_first=True, enforce_sorted=False
        )

        packed_output, _ = self.lstm(packed)
        output, output_lengths = nn.utils.rnn.pad_packed_sequence(
            packed_output, batch_first=True
        )

        # Apply log softmax for CTC
        logits = self.classifier(output)
        log_probs = F.log_softmax(logits, dim=-1)

        # CTC expects (T, N, C) format
        log_probs = log_probs.transpose(0, 1)

        return log_probs, output_lengths


# Training example
def train_ctc_model():
    &quot;&quot;&quot;CTC ASR model training example&quot;&quot;&quot;

    # Initialize model and CTC loss
    model = CTCASRModel(input_dim=80, hidden_dim=256, num_classes=29)
    ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    # Dummy data (in practice, obtained from data loader)
    batch_size = 4
    max_time = 100
    features = torch.randn(batch_size, max_time, 80)
    feature_lengths = torch.tensor([100, 95, 90, 85])

    # Target text (numerically encoded)
    targets = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 0], [8, 9, 10, 0], [11, 12, 0, 0]])
    target_lengths = torch.tensor([4, 3, 3, 2])

    model.train()

    # Forward pass
    log_probs, output_lengths = model(features, feature_lengths)

    # Calculate CTC loss
    loss = ctc_loss(log_probs, targets, output_lengths, target_lengths)

    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print(f&quot;CTC Loss: {loss.item():.4f}&quot;)

    return model

# Execution example
if __name__ == &quot;__main__&quot;:
    model = train_ctc_model()
    print(&quot;‚úì CTC model training completed&quot;)
</code></pre>
<h3>1.3 CTC Decoding</h3>
<p>To obtain text from a trained CTC model, decoding is required. The main methods are Greedy Decoding and Beam Search.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

import numpy as np
from collections import defaultdict

class CTCDecoder:
    &quot;&quot;&quot;CTC Decoder (Greedy &amp; Beam Search)&quot;&quot;&quot;

    def __init__(self, labels, blank_idx=0):
        &quot;&quot;&quot;
        Args:
            labels: List of character labels
            blank_idx: Index of blank symbol
        &quot;&quot;&quot;
        self.labels = labels
        self.blank_idx = blank_idx

    def greedy_decode(self, log_probs):
        &quot;&quot;&quot;
        Greedy Decoding: At each time step, the most probability HighSelect label

        Args:
            log_probs: (time, num_classes) log probabilities
        Returns:
            decoded_text: Decoded text
        &quot;&quot;&quot;
        # At each time step, the most probability HighGet index
        best_path = torch.argmax(log_probs, dim=-1)

        # Remove consecutive duplicates and blanks
        decoded = []
        prev_idx = self.blank_idx

        for idx in best_path:
            idx = idx.item()
            if idx != self.blank_idx and idx != prev_idx:
                decoded.append(self.labels[idx])
            prev_idx = idx

        return ''.join(decoded)

    def beam_search_decode(self, log_probs, beam_width=10):
        &quot;&quot;&quot;
        Beam Search Decoding: More accurate decoding

        Args:
            log_probs: (time, num_classes) log probabilities
            beam_width: Beam width
        Returns:
            decoded_text: Decoded text
        &quot;&quot;&quot;
        T, C = log_probs.shape
        log_probs = log_probs.cpu().numpy()

        # Beams: {sequence: probability}
        beams = {('', self.blank_idx): 0.0}  # (text, last_char): log_prob

        for t in range(T):
            new_beams = defaultdict(lambda: float('-inf'))

            for (text, last_char), log_prob in beams.items():
                for c in range(C):
                    new_log_prob = log_prob + log_probs[t, c]

                    if c == self.blank_idx:
                        # Do not modify text for blank
                        new_beams[(text, c)] = np.logaddexp(
                            new_beams[(text, c)], new_log_prob
                        )
                    else:
                        if c == last_char:
                            # Do not repeat if same as previous character
                            new_beams[(text, c)] = np.logaddexp(
                                new_beams[(text, c)], new_log_prob
                            )
                        else:
                            # Add new character
                            new_text = text + self.labels[c]
                            new_beams[(new_text, c)] = np.logaddexp(
                                new_beams[(new_text, c)], new_log_prob
                            )

            # Keep top beam_width beams
            beams = dict(sorted(new_beams.items(), key=lambda x: x[1], reverse=True)[:beam_width])

        # mostprobability HighReturn beam
        best_beam = max(beams.items(), key=lambda x: x[1])
        return best_beam[0][0]


# Decodingexample
def decode_example():
    &quot;&quot;&quot;CTC decoding execution example&quot;&quot;&quot;

    # Alphabet (0 is blank)
    labels = ['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',
              'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ', &quot;'&quot;]

    decoder = CTCDecoder(labels, blank_idx=0)

    # Dummy log probabilities (in practice, model output)
    T = 50
    C = len(labels)
    log_probs = torch.randn(T, C)
    log_probs = F.log_softmax(log_probs, dim=-1)

    # Greedy Decoding
    greedy_text = decoder.greedy_decode(log_probs)
    print(f&quot;Greedy Decode: {greedy_text}&quot;)

    # Beam Search Decoding
    beam_text = decoder.beam_search_decode(log_probs, beam_width=10)
    print(f&quot;Beam Search Decode: {beam_text}&quot;)

decode_example()
</code></pre>
<h2>2. Attention-based Models</h2>
<h3>2.1 Listen, Attend and Spell (LAS)</h3>
<p>LAS is a speech recognition model that combines an Encoder-Decoder architecture with an Attention mechanism. The Encoder converts acoustic features into high-level representations, and the Decoder generates text while focusing on necessary information through Attention.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn
import torch.nn.functional as F

class ListenAttendSpell(nn.Module):
    &quot;&quot;&quot;Listen, Attend and Spell model&quot;&quot;&quot;

    def __init__(self, input_dim=80, encoder_hidden=256, decoder_hidden=512,
                 vocab_size=29, num_layers=3):
        super(ListenAttendSpell, self).__init__()

        self.encoder = Listener(input_dim, encoder_hidden, num_layers)
        self.decoder = Speller(encoder_hidden * 2, decoder_hidden, vocab_size)

    def forward(self, inputs, input_lengths, targets=None, teacher_forcing_ratio=0.9):
        &quot;&quot;&quot;
        Args:
            inputs: (batch, time, features)
            input_lengths: Length of each sample
            targets: (batch, target_len) Decoder target
            teacher_forcing_ratio: Teacher forcing ratio
        &quot;&quot;&quot;
        # Encoder
        encoder_outputs, encoder_lengths = self.encoder(inputs, input_lengths)

        # Decoder
        if targets is not None:
            outputs = self.decoder(encoder_outputs, encoder_lengths, targets,
                                  teacher_forcing_ratio)
        else:
            outputs = self.decoder.inference(encoder_outputs, encoder_lengths)

        return outputs


class Listener(nn.Module):
    &quot;&quot;&quot;Encoder: acousticfeatures to high-level representations&quot;&quot;&quot;

    def __init__(self, input_dim, hidden_dim, num_layers):
        super(Listener, self).__init__()

        self.lstm = nn.LSTM(
            input_dim,
            hidden_dim,
            num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=0.2
        )

        # Pyramidal LSTM: Compress time dimension to improve computational efficiency
        self.pyramid_lstm = nn.LSTM(
            hidden_dim * 4,  # 2 frames combined + bidirectional
            hidden_dim,
            num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=0.2
        )

    def forward(self, x, lengths):
        &quot;&quot;&quot;
        Args:
            x: (batch, time, features)
            lengths: Length of each sample
        &quot;&quot;&quot;
        # First layer LSTM
        packed = nn.utils.rnn.pack_padded_sequence(
            x, lengths.cpu(), batch_first=True, enforce_sorted=False
        )
        output, _ = self.lstm(packed)
        output, lengths = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)

        # Pyramidal: Combine 2 frames into 1 to halve time
        batch, time, features = output.size()
        if time % 2 == 1:
            output = output[:, :-1, :]
            time -= 1

        output = output.reshape(batch, time // 2, features * 2)
        lengths = lengths // 2

        # Second layer LSTM
        packed = nn.utils.rnn.pack_padded_sequence(
            output, lengths.cpu(), batch_first=True, enforce_sorted=False
        )
        output, _ = self.pyramid_lstm(packed)
        output, lengths = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)

        return output, lengths


class Speller(nn.Module):
    &quot;&quot;&quot;Decoder: Generate text using Attention&quot;&quot;&quot;

    def __init__(self, encoder_dim, hidden_dim, vocab_size):
        super(Speller, self).__init__()

        self.vocab_size = vocab_size
        self.embedding = nn.Embedding(vocab_size, hidden_dim)
        self.lstm = nn.LSTMCell(hidden_dim + encoder_dim, hidden_dim)
        self.attention = BahdanauAttention(encoder_dim, hidden_dim)
        self.classifier = nn.Linear(hidden_dim + encoder_dim, vocab_size)

    def forward(self, encoder_outputs, encoder_lengths, targets, teacher_forcing_ratio=0.9):
        &quot;&quot;&quot;
        Training with Teacher Forcing
        &quot;&quot;&quot;
        batch_size = encoder_outputs.size(0)
        max_len = targets.size(1)

        # Initial state
        hidden = torch.zeros(batch_size, self.lstm.hidden_size, device=encoder_outputs.device)
        cell = torch.zeros(batch_size, self.lstm.hidden_size, device=encoder_outputs.device)

        # Start token
        input_token = torch.zeros(batch_size, dtype=torch.long, device=encoder_outputs.device)

        outputs = []

        for t in range(max_len):
            # Embedding
            embedded = self.embedding(input_token)

            # Attention
            context, _ = self.attention(hidden, encoder_outputs, encoder_lengths)

            # LSTM
            lstm_input = torch.cat([embedded, context], dim=1)
            hidden, cell = self.lstm(lstm_input, (hidden, cell))

            # Output
            output = self.classifier(torch.cat([hidden, context], dim=1))
            outputs.append(output)

            # Teacher Forcing
            use_teacher_forcing = torch.rand(1).item() &lt; teacher_forcing_ratio
            if use_teacher_forcing:
                input_token = targets[:, t]
            else:
                input_token = output.argmax(dim=1)

        return torch.stack(outputs, dim=1)

    def inference(self, encoder_outputs, encoder_lengths, max_len=100):
        &quot;&quot;&quot;Decoding during inference&quot;&quot;&quot;
        batch_size = encoder_outputs.size(0)

        hidden = torch.zeros(batch_size, self.lstm.hidden_size, device=encoder_outputs.device)
        cell = torch.zeros(batch_size, self.lstm.hidden_size, device=encoder_outputs.device)

        input_token = torch.zeros(batch_size, dtype=torch.long, device=encoder_outputs.device)

        outputs = []

        for t in range(max_len):
            embedded = self.embedding(input_token)
            context, _ = self.attention(hidden, encoder_outputs, encoder_lengths)

            lstm_input = torch.cat([embedded, context], dim=1)
            hidden, cell = self.lstm(lstm_input, (hidden, cell))

            output = self.classifier(torch.cat([hidden, context], dim=1))
            outputs.append(output)

            input_token = output.argmax(dim=1)

            # Stop at end token
            if (input_token == self.vocab_size - 1).all():
                break

        return torch.stack(outputs, dim=1)


class BahdanauAttention(nn.Module):
    &quot;&quot;&quot;Bahdanau Attention mechanism&quot;&quot;&quot;

    def __init__(self, encoder_dim, decoder_dim):
        super(BahdanauAttention, self).__init__()

        self.encoder_projection = nn.Linear(encoder_dim, decoder_dim)
        self.decoder_projection = nn.Linear(decoder_dim, decoder_dim)
        self.v = nn.Linear(decoder_dim, 1)

    def forward(self, decoder_hidden, encoder_outputs, encoder_lengths):
        &quot;&quot;&quot;
        Args:
            decoder_hidden: (batch, decoder_dim)
            encoder_outputs: (batch, time, encoder_dim)
            encoder_lengths: (batch,)
        Returns:
            context: (batch, encoder_dim)
            attention_weights: (batch, time)
        &quot;&quot;&quot;
        batch_size, time_steps, _ = encoder_outputs.size()

        # Projection
        encoder_proj = self.encoder_projection(encoder_outputs)  # (batch, time, decoder_dim)
        decoder_proj = self.decoder_projection(decoder_hidden).unsqueeze(1)  # (batch, 1, decoder_dim)

        # Energy computation
        energy = self.v(torch.tanh(encoder_proj + decoder_proj)).squeeze(-1)  # (batch, time)

        # Mask (ignore padding)
        mask = torch.arange(time_steps, device=encoder_outputs.device).expand(
            batch_size, time_steps
        ) &lt; encoder_lengths.unsqueeze(1)

        energy = energy.masked_fill(~mask, float('-inf'))

        # Attention weights
        attention_weights = F.softmax(energy, dim=1)  # (batch, time)

        # Context vector
        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)

        return context, attention_weights


# Usage example
def las_example():
    &quot;&quot;&quot;LAS model usage example&quot;&quot;&quot;

    model = ListenAttendSpell(
        input_dim=80,
        encoder_hidden=256,
        decoder_hidden=512,
        vocab_size=29
    )

    # Dummy data
    batch_size = 4
    inputs = torch.randn(batch_size, 100, 80)
    input_lengths = torch.tensor([100, 95, 90, 85])
    targets = torch.randint(0, 29, (batch_size, 20))

    # Training mode
    outputs = model(inputs, input_lengths, targets, teacher_forcing_ratio=0.9)
    print(f&quot;Training output shape: {outputs.shape}&quot;)

    # Inference mode
    model.eval()
    with torch.no_grad():
        predictions = model(inputs, input_lengths, targets=None)
        print(f&quot;Inference output shape: {predictions.shape}&quot;)

las_example()
</code></pre>
<h3>2.2 Transformer for ASR</h3>
<p>Applying the Transformer architecture to ASR enables parallel processing and more effectively captures long-range dependencies.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn
import math

class TransformerASR(nn.Module):
    &quot;&quot;&quot;Transformer based ASR model&quot;&quot;&quot;

    def __init__(self, input_dim=80, d_model=512, nhead=8, num_encoder_layers=6,
                 num_decoder_layers=6, dim_feedforward=2048, vocab_size=29, dropout=0.1):
        super(TransformerASR, self).__init__()

        # inputProjection
        self.input_projection = nn.Linear(input_dim, d_model)

        # Positional Encoding
        self.pos_encoder = PositionalEncoding(d_model, dropout)

        # Transformer
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=nhead,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True
        )

        # Output layer
        self.output_projection = nn.Linear(d_model, vocab_size)

        # Parameters
        self.d_model = d_model
        self.vocab_size = vocab_size

        # Embedding (for decoder input)
        self.tgt_embedding = nn.Embedding(vocab_size, d_model)

    def forward(self, src, tgt, src_mask=None, tgt_mask=None,
                src_key_padding_mask=None, tgt_key_padding_mask=None):
        &quot;&quot;&quot;
        Args:
            src: (batch, src_len, input_dim)
            tgt: (batch, tgt_len)
            src_mask: Encoder self-attention mask
            tgt_mask: Decoder self-attention mask (causal)
            src_key_padding_mask: (batch, src_len) Padding mask
            tgt_key_padding_mask: (batch, tgt_len) Padding mask
        &quot;&quot;&quot;
        # Prepare encoder input
        src = self.input_projection(src) * math.sqrt(self.d_model)
        src = self.pos_encoder(src)

        # Prepare decoder input
        tgt_embedded = self.tgt_embedding(tgt) * math.sqrt(self.d_model)
        tgt_embedded = self.pos_encoder(tgt_embedded)

        # Transformer
        output = self.transformer(
            src, tgt_embedded,
            src_mask=src_mask,
            tgt_mask=tgt_mask,
            src_key_padding_mask=src_key_padding_mask,
            tgt_key_padding_mask=tgt_key_padding_mask
        )

        # OutputProjection
        output = self.output_projection(output)

        return output

    def generate_square_subsequent_mask(self, sz):
        &quot;&quot;&quot;Generate causal mask (to prevent seeing future information)&quot;&quot;&quot;
        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()
        return mask


class PositionalEncoding(nn.Module):
    &quot;&quot;&quot;Positional Encoding for Transformer&quot;&quot;&quot;

    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Precompute positional encoding
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                            (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)

        self.register_buffer('pe', pe)

    def forward(self, x):
        &quot;&quot;&quot;
        Args:
            x: (batch, seq_len, d_model)
        &quot;&quot;&quot;
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)


# Training example
def train_transformer_asr():
    &quot;&quot;&quot;Transformer ASR training example&quot;&quot;&quot;

    model = TransformerASR(
        input_dim=80,
        d_model=512,
        nhead=8,
        num_encoder_layers=6,
        num_decoder_layers=6,
        vocab_size=29
    )

    criterion = nn.CrossEntropyLoss(ignore_index=0)  # 0 is padding
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98))

    # Dummy data
    batch_size = 8
    src = torch.randn(batch_size, 100, 80)
    tgt = torch.randint(1, 29, (batch_size, 30))

    # Padding mask
    src_key_padding_mask = torch.zeros(batch_size, 100).bool()
    tgt_key_padding_mask = torch.zeros(batch_size, 30).bool()

    # Causal mask
    tgt_mask = model.generate_square_subsequent_mask(30).to(tgt.device)

    # Forward
    output = model(src, tgt[:, :-1], tgt_mask=tgt_mask,
                   src_key_padding_mask=src_key_padding_mask,
                   tgt_key_padding_mask=tgt_key_padding_mask[:, :-1])

    # Loss calculation
    loss = criterion(output.reshape(-1, model.vocab_size), tgt[:, 1:].reshape(-1))

    # Backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print(f&quot;Transformer ASR Loss: {loss.item():.4f}&quot;)

    return model

train_transformer_asr()
</code></pre>
<h3>2.3 Joint CTC-Attention</h3>
<p>By combining CTC and Attention, we can leverage the strengths of each. CTC helps learn alignment, while Attention utilizes contextual information.</p>
<pre><code class="language-python">class JointCTCAttention(nn.Module):
    """Hybrid model combining CTC and Attention"""

    def __init__(self, input_dim=80, encoder_hidden=256, decoder_hidden=512,
                 vocab_size=29, num_layers=3, ctc_weight=0.3):
        super(JointCTCAttention, self).__init__()

        # Shared Encoder
        self.encoder = Listener(input_dim, encoder_hidden, num_layers)

        # CTC classifier
        self.ctc_classifier = nn.Linear(encoder_hidden * 2, vocab_size)

        # Attention-based Decoder
        self.decoder = Speller(encoder_hidden * 2, decoder_hidden, vocab_size)

        # CTC loss weight
        self.ctc_weight = ctc_weight
        self.ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)

    def forward(self, inputs, input_lengths, targets, target_lengths=None,
                teacher_forcing_ratio=0.9):
        """
        Args:
            inputs: (batch, time, features)
            input_lengths: Length of acoustic features for each sample
            targets: (batch, target_len) Text target
            target_lengths: Length of each target (for CTC)
            teacher_forcing_ratio: Teacher forcing ratio
        Returns:
            ctc_loss: CTC loss
            attention_loss: Attention loss
            combined_loss: Combined loss
        """
        # Encoder (shared)
        encoder_outputs, encoder_lengths = self.encoder(inputs, input_lengths)

        # CTC branch
        ctc_logits = self.ctc_classifier(encoder_outputs)
        ctc_log_probs = F.log_softmax(ctc_logits, dim=-1)
        ctc_log_probs = ctc_log_probs.transpose(0, 1)  # (T, N, C)

        # Attention branch
        attention_outputs = self.decoder(encoder_outputs, encoder_lengths,
                                        targets, teacher_forcing_ratio)

        return ctc_log_probs, encoder_lengths, attention_outputs

    def compute_loss(self, ctc_log_probs, encoder_lengths, attention_outputs,
                     targets, target_lengths):
        """
        Loss calculation
        """
        # CTC loss
        ctc_loss = self.ctc_loss(ctc_log_probs, targets, encoder_lengths, target_lengths)

        # Attention loss (Cross Entropy)
        attention_loss = F.cross_entropy(
            attention_outputs.reshape(-1, attention_outputs.size(-1)),
            targets.reshape(-1),
            ignore_index=0
        )

        # Combined loss
        combined_loss = self.ctc_weight * ctc_loss + (1 - self.ctc_weight) * attention_loss

        return ctc_loss, attention_loss, combined_loss

    def recognize(self, inputs, input_lengths, beam_width=10):
        """
        Inference: Decoding using both CTC and Attention
        """
        self.eval()
        with torch.no_grad():
            # Encoder
            encoder_outputs, encoder_lengths = self.encoder(inputs, input_lengths)

            # CTC branch (For prefix beam search)
            ctc_logits = self.ctc_classifier(encoder_outputs)
            ctc_probs = F.softmax(ctc_logits, dim=-1)

            # Attention branch
            attention_outputs = self.decoder.inference(encoder_outputs, encoder_lengths)
            attention_probs = F.softmax(attention_outputs, dim=-1)

            # Decode by combining CTC and Attention scores
            # (In practice, more complex beam search algorithms are used)
            combined_probs = (self.ctc_weight * ctc_probs[0] +
                            (1 - self.ctc_weight) * attention_probs)

            predictions = combined_probs.argmax(dim=-1)

            return predictions


# Training example
def train_joint_model():
    """Joint CTC-Attention model training"""

    model = JointCTCAttention(
        input_dim=80,
        encoder_hidden=256,
        decoder_hidden=512,
        vocab_size=29,
        ctc_weight=0.3
    )

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    # Dummy data
    batch_size = 4
    inputs = torch.randn(batch_size, 100, 80)
    input_lengths = torch.tensor([100, 95, 90, 85])
    targets = torch.randint(1, 29, (batch_size, 20))
    target_lengths = torch.tensor([20, 18, 17, 15])

    # Forward
    ctc_log_probs, encoder_lengths, attention_outputs = model(
        inputs, input_lengths, targets, target_lengths
    )

    # Calculate loss
    ctc_loss, attention_loss, combined_loss = model.compute_loss(
        ctc_log_probs, encoder_lengths, attention_outputs, targets, target_lengths
    )

    # Backward
    optimizer.zero_grad()
    combined_loss.backward()
    optimizer.step()

    print(f"CTC Loss: {ctc_loss.item():.4f}")
    print(f"Attention Loss: {attention_loss.item():.4f}")
    print(f"Combined Loss: {combined_loss.item():.4f}")

    return model

train_joint_model()
</code></pre>
<h2>3. RNN-Transducer (RNN-T)</h2>
<h3>3.1 What is RNN-T</h3>
<p>RNN-Transducer is a model suitable for streaming speech recognition. It consists of three components: an acoustic model (Encoder), a language model (Prediction Network), and a Joint Network. Unlike CTC, it can explicitly incorporate a language model.</p>
<h4>Characteristics of RNN-T</h4>
<ul>
<li><strong>Streaming support</strong>: Enables online processing</li>
<li><strong>Language model integration</strong>: Prediction Network provides linguistic information</li>
<li><strong>Flexible alignment</strong>: More flexible output timing than CTC</li>
<li><strong>Blank symbol</strong>: Can represent no output (waiting)</li>
</ul>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn
import torch.nn.functional as F

class RNNTransducer(nn.Module):
    &quot;&quot;&quot;RNN-Transducer model&quot;&quot;&quot;

    def __init__(self, input_dim=80, encoder_dim=256, pred_dim=256,
                 joint_dim=512, vocab_size=29, num_layers=3):
        &quot;&quot;&quot;
        Args:
            input_dim: input feature dimensions
            encoder_dim: Encoder hidden layer dimension
            pred_dim: Prediction Network hidden layer dimension
            joint_dim: Joint Network hidden layer dimension
            vocab_size: Vocabulary size (including blank)
        &quot;&quot;&quot;
        super(RNNTransducer, self).__init__()

        # Encoder (Transcription Network)
        self.encoder = nn.LSTM(
            input_dim,
            encoder_dim,
            num_layers,
            batch_first=True,
            bidirectional=False,  # Streaming for unidirectional
            dropout=0.2
        )

        # Prediction Network (language model)
        self.embedding = nn.Embedding(vocab_size, pred_dim)
        self.prediction = nn.LSTM(
            pred_dim,
            pred_dim,
            num_layers,
            batch_first=True,
            dropout=0.2
        )

        # Joint Network
        self.joint = JointNetwork(encoder_dim, pred_dim, joint_dim, vocab_size)

        self.vocab_size = vocab_size
        self.blank_idx = 0

    def forward(self, inputs, input_lengths, targets, target_lengths):
        &quot;&quot;&quot;
        Args:
            inputs: (batch, time, features) acousticfeatures
            input_lengths: Input length of each sample
            targets: (batch, target_len) Target labels
            target_lengths: Target length of each sample
        Returns:
            joint_output: (batch, time, target_len+1, vocab_size)
        &quot;&quot;&quot;
        # Encoder
        encoder_out, _ = self.encoder(inputs)  # (batch, time, encoder_dim)

        # Prediction Network
        # Start token add
        batch_size = targets.size(0)
        start_tokens = torch.zeros(batch_size, 1, dtype=torch.long, device=targets.device)
        pred_input = torch.cat([start_tokens, targets], dim=1)

        pred_embedded = self.embedding(pred_input)
        pred_out, _ = self.prediction(pred_embedded)  # (batch, target_len+1, pred_dim)

        # Joint Network
        joint_output = self.joint(encoder_out, pred_out)

        return joint_output

    def greedy_decode(self, inputs, input_lengths, max_len=100):
        &quot;&quot;&quot;
        Greedy decoding for inference
        &quot;&quot;&quot;
        self.eval()
        with torch.no_grad():
            batch_size = inputs.size(0)

            # Encoder
            encoder_out, _ = self.encoder(inputs)
            time_steps = encoder_out.size(1)

            # Initialization
            predictions = []
            pred_hidden = None

            # Start token
            pred_input = torch.zeros(batch_size, 1, dtype=torch.long, device=inputs.device)

            for t in range(time_steps):
                # Current encoder output
                enc_t = encoder_out[:, t:t+1, :]  # (batch, 1, encoder_dim)

                # Prediction Network
                pred_embedded = self.embedding(pred_input)
                pred_out, pred_hidden = self.prediction(pred_embedded, pred_hidden)

                # Joint Network
                joint_out = self.joint(enc_t, pred_out)  # (batch, 1, 1, vocab_size)

                # mostprobability HighTokens select
                prob = F.softmax(joint_out.squeeze(1).squeeze(1), dim=-1)
                pred_token = prob.argmax(dim=-1)

                # Add to output only if not blank
                if pred_token.item() != self.blank_idx:
                    predictions.append(pred_token.item())
                    pred_input = pred_token.unsqueeze(1)

                if len(predictions) &gt;= max_len:
                    break

            return predictions


class JointNetwork(nn.Module):
    &quot;&quot;&quot;Joint Network: Combine outputs of Encoder and Prediction Network&quot;&quot;&quot;

    def __init__(self, encoder_dim, pred_dim, joint_dim, vocab_size):
        super(JointNetwork, self).__init__()

        self.encoder_proj = nn.Linear(encoder_dim, joint_dim)
        self.pred_proj = nn.Linear(pred_dim, joint_dim)
        self.output_proj = nn.Linear(joint_dim, vocab_size)

    def forward(self, encoder_out, pred_out):
        &quot;&quot;&quot;
        Args:
            encoder_out: (batch, time, encoder_dim)
            pred_out: (batch, target_len, pred_dim)
        Returns:
            joint_out: (batch, time, target_len, vocab_size)
        &quot;&quot;&quot;
        # Projection
        enc_proj = self.encoder_proj(encoder_out)  # (batch, time, joint_dim)
        pred_proj = self.pred_proj(pred_out)  # (batch, target_len, joint_dim)

        # Broadcast and add
        # (batch, time, 1, joint_dim) + (batch, 1, target_len, joint_dim)
        joint = torch.tanh(
            enc_proj.unsqueeze(2) + pred_proj.unsqueeze(1)
        )  # (batch, time, target_len, joint_dim)

        # OutputProjection
        output = self.output_proj(joint)  # (batch, time, target_len, vocab_size)

        return output


# RNN-T Loss (Simplified version)
class RNNTLoss(nn.Module):
    &quot;&quot;&quot;RNN-T loss function (Forward-Backward algorithm)&quot;&quot;&quot;

    def __init__(self, blank_idx=0):
        super(RNNTLoss, self).__init__()
        self.blank_idx = blank_idx

    def forward(self, logits, targets, input_lengths, target_lengths):
        &quot;&quot;&quot;
        Args:
            logits: (batch, time, target_len+1, vocab_size)
            targets: (batch, target_len)
            input_lengths: (batch,)
            target_lengths: (batch,)
        &quot;&quot;&quot;
        # Use PyTorch torchaudio.functional.rnnt_loss
        # Here for simplicityCTC loss as approximation
        batch_size, time, _, vocab_size = logits.size()

        # Greedy path approximation
        log_probs = F.log_softmax(logits, dim=-1)

        # Calculate probabilities of blank and non-blank at each time step
        loss = 0
        for b in range(batch_size):
            for t in range(input_lengths[b]):
                for u in range(target_lengths[b] + 1):
                    if u &lt; target_lengths[b]:
                        # Non-blank: probability of correct label
                        target_label = targets[b, u]
                        loss -= log_probs[b, t, u, target_label]
                    else:
                        # blank
                        loss -= log_probs[b, t, u, self.blank_idx]

        return loss / batch_size


# Usage example
def train_rnnt():
    &quot;&quot;&quot;RNN-T model training example&quot;&quot;&quot;

    model = RNNTransducer(
        input_dim=80,
        encoder_dim=256,
        pred_dim=256,
        joint_dim=512,
        vocab_size=29
    )

    criterion = RNNTLoss(blank_idx=0)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    # Dummy data
    batch_size = 4
    inputs = torch.randn(batch_size, 100, 80)
    input_lengths = torch.tensor([100, 95, 90, 85])
    targets = torch.randint(1, 29, (batch_size, 20))
    target_lengths = torch.tensor([20, 18, 17, 15])

    # Forward
    logits = model(inputs, input_lengths, targets, target_lengths)

    # Loss
    loss = criterion(logits, targets, input_lengths, target_lengths)

    # Backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print(f&quot;RNN-T Loss: {loss.item():.4f}&quot;)

    # Inference example
    predictions = model.greedy_decode(inputs[:1], input_lengths[:1])
    print(f&quot;Predicted tokens: {predictions}&quot;)

    return model

train_rnnt()
</code></pre>
<h3>3.2 Comparison of CTC and RNN-T</h3>
<table>
<thead>
<tr>
<th>feature</th>
<th>CTC</th>
<th>RNN-Transducer</th>
</tr>
</thead>
<tbody>
<tr>
<td>language model</td>
<td>External LM required</td>
<td>Internally integrated in Prediction Network</td>
</tr>
<tr>
<td>Streaming</td>
<td>Possible (when using unidirectional LSTM)</td>
<td>Designed for streaming</td>
</tr>
<tr>
<td>Computational Cost</td>
<td>Low</td>
<td>Somewhat high (Joint Network)</td>
</tr>
<tr>
<td>accuracy</td>
<td>Moderate</td>
<td>High (language model integration effect)</td>
</tr>
<tr>
<td>Training Stability</td>
<td>Relatively stable</td>
<td>Can be somewhat unstable</td>
</tr>
</tbody>
</table>
<h2>4. Whisper</h2>
<h3>4.1 What is OpenAI Whisper</h3>
<p>Whisper is a multilingual speech recognition model developed by OpenAI. Trained on 680,000 hours of multilingual data, it supports speech recognition, translation, and language identification tasks for 99 languages.</p>
<h4>Characteristics of Whisper</h4>
<ul>
<li><strong>Multilingual support</strong>: Supports 99 languages</li>
<li><strong>Robustness</strong>: Resistant to noise and accents</li>
<li><strong>Zero-shot performance</strong>: High accuracy without fine-tuning</li>
<li><strong>Multifunctional</strong>: Handles speech recognition, translation, and language identification uniformly</li>
<li><strong>Multiple sizes</strong>: tiny, base, small, medium, large</li>
</ul>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - torch&gt;=2.0.0, &lt;2.3.0

import whisper
import torch
import numpy as np

# Basic Whisper model usage
def basic_whisper_usage():
    &quot;&quot;&quot;Basic usage of Whisper&quot;&quot;&quot;

    # Load model (choose from base, small, medium, large)
    model = whisper.load_model(&quot;base&quot;)

    print(f&quot;Model size: {model.dims}&quot;)
    print(f&quot;Number of supported languages: {len(whisper.tokenizer.LANGUAGES)}&quot;)

    # Load audio file and transcribe
    # audio = whisper.load_audio(&quot;audio.mp3&quot;)
    # audio = whisper.pad_or_trim(audio)

    # Dummy audio (in practice, loaded from file)
    audio = np.random.randn(16000 * 10).astype(np.float32)  # 10seconds of audio

    # Mel Spectrogram convert to
    mel = whisper.log_mel_spectrogram(torch.from_numpy(audio)).to(model.device)

    # Language detection
    _, probs = model.detect_language(mel)
    detected_language = max(probs, key=probs.get)
    print(f&quot;Detected language: {detected_language} (probability: {probs[detected_language]:.2f})&quot;)

    # Options configuration
    options = whisper.DecodingOptions(
        language=&quot;ja&quot;,  # Specify Japanese
        task=&quot;transcribe&quot;,  # transcribe or translate
        fp16=False  # Whether to use FP16
    )

    # Decoding
    result = whisper.decode(model, mel, options)

    print(f&quot;Transcription result: {result.text}&quot;)
    print(f&quot;Average logprobability: {result.avg_logprob:.4f}&quot;)
    print(f&quot;Compression ratio: {result.compression_ratio:.2f}&quot;)

    return model, result


# Higher-level API
def transcribe_audio_file(audio_path, model_size=&quot;base&quot;):
    &quot;&quot;&quot;
    Transcribe audio file

    Args:
        audio_path: Audio file path
        model_size: Model size (tiny, base, small, medium, large)
    &quot;&quot;&quot;
    # Load model
    model = whisper.load_model(model_size)

    # Transcription
    result = model.transcribe(
        audio_path,
        language=&quot;ja&quot;,  # Japanese
        task=&quot;transcribe&quot;,
        verbose=True,  # Show progress
        temperature=0.0,  # Temperature (diversity control)
        best_of=5,  # Number of beam search candidates
        beam_size=5,  # Beam width
        patience=1.0,  # Beam search patience
        length_penalty=1.0,  # Length penalty
        compression_ratio_threshold=2.4,  # Compression ratio threshold
        logprob_threshold=-1.0,  # Log probability threshold
        no_speech_threshold=0.6  # No-speech threshold
    )

    # Display results
    print(&quot;=&quot; * 50)
    print(&quot;Transcription result:&quot;)
    print(&quot;=&quot; * 50)
    print(result[&quot;text&quot;])
    print()

    # Results by segment
    print(&quot;Segment details:&quot;)
    for segment in result[&quot;segments&quot;]:
        start = segment[&quot;start&quot;]
        end = segment[&quot;end&quot;]
        text = segment[&quot;text&quot;]
        print(f&quot;[{start:.2f}s - {end:.2f}s] {text}&quot;)

    return result


# With timestampsTranscription
def transcribe_with_timestamps(audio_path):
    &quot;&quot;&quot;With timestamps withTranscription&quot;&quot;&quot;

    model = whisper.load_model(&quot;base&quot;)

    # word_timestamps=True withGet word-level timestamps
    result = model.transcribe(
        audio_path,
        language=&quot;ja&quot;,
        word_timestamps=True  # Word-level timestamps
    )

    # Display word-level results
    for segment in result[&quot;segments&quot;]:
        if &quot;words&quot; in segment:
            for word_info in segment[&quot;words&quot;]:
                word = word_info[&quot;word&quot;]
                start = word_info[&quot;start&quot;]
                end = word_info[&quot;end&quot;]
                probability = word_info.get(&quot;probability&quot;, 0.0)
                print(f&quot;{word:15s} [{start:6.2f}s - {end:6.2f}s] (probability: {probability:.3f})&quot;)

    return result


# Multilingual audio processing
def multilingual_transcription(audio_path):
    &quot;&quot;&quot;Multilingual audio processing&quot;&quot;&quot;

    model = whisper.load_model(&quot;medium&quot;)  # Medium or larger recommended for multilingual

    # Auto-detect language
    result = model.transcribe(
        audio_path,
        task=&quot;transcribe&quot;,
        language=None  # Auto-detect
    )

    detected_language = result[&quot;language&quot;]
    print(f&quot;Detected language: {detected_language}&quot;)
    print(f&quot;Transcription: {result['text']}&quot;)

    # Translate to English
    translation = model.transcribe(
        audio_path,
        task=&quot;translate&quot;,  # Translate to English
        language=detected_language
    )

    print(f&quot;English translation: {translation['text']}&quot;)

    return result, translation


# Execution example
if __name__ == &quot;__main__&quot;:
    print(&quot;Whisper usage examples&quot;)
    print(&quot;=&quot; * 50)

    # Basic usage
    model, result = basic_whisper_usage()
    print(&quot;‚úì basicTranscription completed&quot;)

    # Note: Uncomment the following to use actual audio files
    # result = transcribe_audio_file(&quot;sample.mp3&quot;, model_size=&quot;base&quot;)
    # result = transcribe_with_timestamps(&quot;sample.mp3&quot;)
    # result, translation = multilingual_transcription(&quot;sample.mp3&quot;)
</code></pre>
<h3>4.2 Whisper architecture</h3>
<p>Whisper adopts an Encoder-Decoder architecture. The Encoder processes acoustic features, and the Decoder generates text. Both are Transformer-based.</p>
<div class="mermaid">
graph LR
    A[audio input] --&gt; B[Mel Spectrogram]
    B --&gt; C[Encoder<br/>Transformer]
    C --&gt; D[Acoustic Representation]
    D --&gt; E[Decoder<br/>Transformer]
    E --&gt; F[Text Output]

    style A fill:#e1f5ff
    style F fill:#e1f5ff
    style C fill:#fff4e1
    style E fill:#fff4e1
        </div>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn
from typing import Optional

class Whisperarchitecture(nn.Module):
    &quot;&quot;&quot;
    Whisper architectureoverview
    (Actual Whisper is more complex, but this shows the main structure)
    &quot;&quot;&quot;

    def __init__(self,
                 n_mels=80,
                 n_audio_ctx=1500,
                 n_audio_state=512,
                 n_audio_head=8,
                 n_audio_layer=6,
                 n_vocab=51865,
                 n_text_ctx=448,
                 n_text_state=512,
                 n_text_head=8,
                 n_text_layer=6):
        &quot;&quot;&quot;
        Args:
            n_mels: Number of mel filter banks
            n_audio_ctx: Audio context length
            n_audio_state: Encoder state dimension
            n_audio_head: Number of encoder attention heads
            n_audio_layer: Number of encoder layers
            n_vocab: Vocabulary size
            n_text_ctx: Text context length
            n_text_state: Decoder state dimension
            n_text_head: Number of decoder attention heads
            n_text_layer: Number of decoder layers
        &quot;&quot;&quot;
        super().__init__()

        # Encoder: acousticfeatures process
        self.encoder = AudioEncoder(
            n_mels=n_mels,
            n_ctx=n_audio_ctx,
            n_state=n_audio_state,
            n_head=n_audio_head,
            n_layer=n_audio_layer
        )

        # Decoder: Generate text
        self.decoder = TextDecoder(
            n_vocab=n_vocab,
            n_ctx=n_text_ctx,
            n_state=n_text_state,
            n_head=n_text_head,
            n_layer=n_text_layer
        )

    def forward(self, mel, tokens):
        &quot;&quot;&quot;
        Args:
            mel: (batch, n_mels, time) Mel Spectrogram
            tokens: (batch, seq_len) Tokens
        &quot;&quot;&quot;
        # Encoder
        audio_features = self.encoder(mel)

        # Decoder
        logits = self.decoder(tokens, audio_features)

        return logits


class AudioEncoder(nn.Module):
    &quot;&quot;&quot;Whisper Audio Encoder&quot;&quot;&quot;

    def __init__(self, n_mels, n_ctx, n_state, n_head, n_layer):
        super().__init__()

        # Convolutional layers with feature extraction
        self.conv1 = nn.Conv1d(n_mels, n_state, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)

        # Positional embedding
        self.positional_embedding = nn.Parameter(torch.empty(n_ctx, n_state))

        # Transformer layers
        self.blocks = nn.ModuleList([
            TransformerBlock(n_state, n_head) for _ in range(n_layer)
        ])

        self.ln_post = nn.LayerNorm(n_state)

    def forward(self, x):
        &quot;&quot;&quot;
        Args:
            x: (batch, n_mels, time)
        Returns:
            (batch, time//2, n_state)
        &quot;&quot;&quot;
        x = F.gelu(self.conv1(x))
        x = F.gelu(self.conv2(x))
        x = x.permute(0, 2, 1)  # (batch, time, n_state)

        # Positional embedding
        x = x + self.positional_embedding[:x.size(1)]

        # Transformer blocks
        for block in self.blocks:
            x = block(x)

        x = self.ln_post(x)
        return x


class TextDecoder(nn.Module):
    &quot;&quot;&quot;Whisper Text Decoder&quot;&quot;&quot;

    def __init__(self, n_vocab, n_ctx, n_state, n_head, n_layer):
        super().__init__()

        # Token embedding
        self.token_embedding = nn.Embedding(n_vocab, n_state)

        # Positional embedding
        self.positional_embedding = nn.Parameter(torch.empty(n_ctx, n_state))

        # Transformer layers (with cross-attention)
        self.blocks = nn.ModuleList([
            DecoderBlock(n_state, n_head) for _ in range(n_layer)
        ])

        self.ln = nn.LayerNorm(n_state)

    def forward(self, tokens, audio_features):
        &quot;&quot;&quot;
        Args:
            tokens: (batch, seq_len)
            audio_features: (batch, audio_len, n_state)
        Returns:
            (batch, seq_len, n_vocab)
        &quot;&quot;&quot;
        x = self.token_embedding(tokens)
        x = x + self.positional_embedding[:x.size(1)]

        for block in self.blocks:
            x = block(x, audio_features)

        x = self.ln(x)

        # Weight tying: token embedding reuse
        logits = x @ self.token_embedding.weight.T

        return logits


class TransformerBlock(nn.Module):
    &quot;&quot;&quot;Transformer Encoder Block&quot;&quot;&quot;

    def __init__(self, n_state, n_head):
        super().__init__()

        self.attn = nn.MultiheadAttention(n_state, n_head, batch_first=True)
        self.attn_ln = nn.LayerNorm(n_state)

        self.mlp = nn.Sequential(
            nn.Linear(n_state, n_state * 4),
            nn.GELU(),
            nn.Linear(n_state * 4, n_state)
        )
        self.mlp_ln = nn.LayerNorm(n_state)

    def forward(self, x):
        # Self-attention
        attn_out, _ = self.attn(x, x, x)
        x = self.attn_ln(x + attn_out)

        # MLP
        mlp_out = self.mlp(x)
        x = self.mlp_ln(x + mlp_out)

        return x


class DecoderBlock(nn.Module):
    &quot;&quot;&quot;Transformer Decoder Block (with cross-attention)&quot;&quot;&quot;

    def __init__(self, n_state, n_head):
        super().__init__()

        # Self-attention
        self.self_attn = nn.MultiheadAttention(n_state, n_head, batch_first=True)
        self.self_attn_ln = nn.LayerNorm(n_state)

        # Cross-attention
        self.cross_attn = nn.MultiheadAttention(n_state, n_head, batch_first=True)
        self.cross_attn_ln = nn.LayerNorm(n_state)

        # MLP
        self.mlp = nn.Sequential(
            nn.Linear(n_state, n_state * 4),
            nn.GELU(),
            nn.Linear(n_state * 4, n_state)
        )
        self.mlp_ln = nn.LayerNorm(n_state)

    def forward(self, x, audio_features):
        # Self-attention (causal mask)
        attn_out, _ = self.self_attn(x, x, x, need_weights=False, is_causal=True)
        x = self.self_attn_ln(x + attn_out)

        # Cross-attention
        cross_out, _ = self.cross_attn(x, audio_features, audio_features)
        x = self.cross_attn_ln(x + cross_out)

        # MLP
        mlp_out = self.mlp(x)
        x = self.mlp_ln(x + mlp_out)

        return x


# Display architecture overview
def show_architecture_info():
    &quot;&quot;&quot;Information about Whisper model sizes&quot;&quot;&quot;

    models_info = {
        &quot;tiny&quot;: {
            &quot;parameters&quot;: &quot;39M&quot;,
            &quot;n_audio_layer&quot;: 4,
            &quot;n_text_layer&quot;: 4,
            &quot;n_state&quot;: 384,
            &quot;n_head&quot;: 6
        },
        &quot;base&quot;: {
            &quot;parameters&quot;: &quot;74M&quot;,
            &quot;n_audio_layer&quot;: 6,
            &quot;n_text_layer&quot;: 6,
            &quot;n_state&quot;: 512,
            &quot;n_head&quot;: 8
        },
        &quot;small&quot;: {
            &quot;parameters&quot;: &quot;244M&quot;,
            &quot;n_audio_layer&quot;: 12,
            &quot;n_text_layer&quot;: 12,
            &quot;n_state&quot;: 768,
            &quot;n_head&quot;: 12
        },
        &quot;medium&quot;: {
            &quot;parameters&quot;: &quot;769M&quot;,
            &quot;n_audio_layer&quot;: 24,
            &quot;n_text_layer&quot;: 24,
            &quot;n_state&quot;: 1024,
            &quot;n_head&quot;: 16
        },
        &quot;large&quot;: {
            &quot;parameters&quot;: &quot;1550M&quot;,
            &quot;n_audio_layer&quot;: 32,
            &quot;n_text_layer&quot;: 32,
            &quot;n_state&quot;: 1280,
            &quot;n_head&quot;: 20
        }
    }

    print(&quot;Whisper Model sizecomparison&quot;)
    print(&quot;=&quot; * 70)
    print(f&quot;{'Model':&lt;10} {'Parameters':&lt;12} {'Layers':&lt;10} {'State':&lt;10} {'Heads':&lt;10}&quot;)
    print(&quot;=&quot; * 70)

    for model, info in models_info.items():
        print(f&quot;{model:&lt;10} {info['parameters']:&lt;12} &quot;
              f&quot;{info['n_audio_layer']}/{info['n_text_layer']:&lt;10} &quot;
              f&quot;{info['n_state']:&lt;10} {info['n_head']:&lt;10}&quot;)

    print()
    print(&quot;Recommended usage:&quot;)
    print(&quot;- tiny/base: Real-time processing, resource-constrained environments&quot;)
    print(&quot;- small: Balanced approach, general purpose&quot;)
    print(&quot;- medium/large: High accuracy requirements, multilingual processing&quot;)

show_architecture_info()
</code></pre>
<h2>5. Practical ASR Systems</h2>
<h3>5.1 Fine-tuning Whisper</h3>
<p>By fine-tuning Whisper for specific domains or languages, even higher accuracy can be achieved.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0
# - transformers&gt;=4.30.0

from transformers import WhisperProcessor, WhisperForConditionalGeneration
from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer
from datasets import load_dataset, Audio
import torch

def finetune_whisper_japanese():
    &quot;&quot;&quot;
    Whisper fine-tuning for Japanese speech recognition
    &quot;&quot;&quot;

    # Load model and processor
    model_name = &quot;openai/whisper-small&quot;
    processor = WhisperProcessor.from_pretrained(model_name, language=&quot;Japanese&quot;, task=&quot;transcribe&quot;)
    model = WhisperForConditionalGeneration.from_pretrained(model_name)

    # Japanese-specific configuration
    model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(
        language=&quot;Japanese&quot;,
        task=&quot;transcribe&quot;
    )
    model.config.suppress_tokens = []

    # Prepare dataset (example: Common Voice Japanese)
    # dataset = load_dataset(&quot;mozilla-foundation/common_voice_11_0&quot;, &quot;ja&quot;, split=&quot;train[:100]&quot;)

    # Dummy dataset (use actual datasets like the one above in practice)
    def prepare_dataset(batch):
        &quot;&quot;&quot;Dataset preprocessing&quot;&quot;&quot;
        # Resample audio to 16kHz
        audio = batch[&quot;audio&quot;]

        # Convert to Mel Spectrogram
        batch[&quot;input_features&quot;] = processor(
            audio[&quot;array&quot;],
            sampling_rate=audio[&quot;sampling_rate&quot;],
            return_tensors=&quot;pt&quot;
        ).input_features[0]

        # Prepare labels
        batch[&quot;labels&quot;] = processor.tokenizer(batch[&quot;sentence&quot;]).input_ids

        return batch

    # Apply preprocessing to dataset
    # dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)

    # Data collator
    from dataclasses import dataclass
    from typing import Any, Dict, List, Union

    @dataclass
    class DataCollatorSpeechSeq2SeqWithPadding:
        &quot;&quot;&quot;Speech data collator&quot;&quot;&quot;

        processor: Any

        def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -&gt; Dict[str, torch.Tensor]:
            # Pad input features
            input_features = [{&quot;input_features&quot;: feature[&quot;input_features&quot;]} for feature in features]
            batch = self.processor.feature_extractor.pad(input_features, return_tensors=&quot;pt&quot;)

            # Pad labels
            label_features = [{&quot;input_ids&quot;: feature[&quot;labels&quot;]} for feature in features]
            labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=&quot;pt&quot;)

            # Replace padding tokens with -100 (ignored in loss calculation)
            labels = labels_batch[&quot;input_ids&quot;].masked_fill(
                labels_batch.attention_mask.ne(1), -100
            )

            # Remove bos token if present
            if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
                labels = labels[:, 1:]

            batch[&quot;labels&quot;] = labels

            return batch

    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

    # Evaluation metrics
    import evaluate

    metric = evaluate.load(&quot;wer&quot;)  # Word Error Rate

    def compute_metrics(pred):
        &quot;&quot;&quot;Calculate evaluation metrics&quot;&quot;&quot;
        pred_ids = pred.predictions
        label_ids = pred.label_ids

        # Replace -100 with pad_token_id
        label_ids[label_ids == -100] = processor.tokenizer.pad_token_id

        # Decode
        pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
        label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)

        # Calculate WER
        wer = metric.compute(predictions=pred_str, references=label_str)

        return {&quot;wer&quot;: wer}

    # Training configuration
    training_args = Seq2SeqTrainingArguments(
        output_dir=&quot;./whisper-japanese&quot;,
        per_device_train_batch_size=8,
        gradient_accumulation_steps=2,
        learning_rate=1e-5,
        warmup_steps=500,
        num_train_epochs=3,
        evaluation_strategy=&quot;steps&quot;,
        eval_steps=1000,
        save_steps=1000,
        logging_steps=100,
        generation_max_length=225,
        predict_with_generate=True,
        fp16=True,  # Mixed precision training
        push_to_hub=False,
        report_to=[&quot;tensorboard&quot;],
        load_best_model_at_end=True,
        metric_for_best_model=&quot;wer&quot;,
        greater_is_better=False,
    )

    # Initialize trainer
    # trainer = Seq2SeqTrainer(
    #     model=model,
    #     args=training_args,
    #     train_dataset=dataset[&quot;train&quot;],
    #     eval_dataset=dataset[&quot;test&quot;],
    #     data_collator=data_collator,
    #     compute_metrics=compute_metrics,
    #     tokenizer=processor.feature_extractor,
    # )

    # Start training
    # trainer.train()

    # Save model
    # model.save_pretrained(&quot;./whisper-japanese-finetuned&quot;)
    # processor.save_pretrained(&quot;./whisper-japanese-finetuned&quot;)

    print(&quot;‚úì Fine-tuning configuration completed&quot;)
    print(&quot;Use datasets like Common Voice for actual training&quot;)

    return model, processor

# Execution example
model, processor = finetune_whisper_japanese()
</code></pre>
<h3>5.2 Real-time Speech Recognition Application</h3>
<p>Build an application that transcribes audio input from a microphone in real-time.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

import pyaudio
import numpy as np
import whisper
import queue
import threading
from collections import deque

class RealtimeASR:
    &quot;&quot;&quot;Real-time speech recognition system&quot;&quot;&quot;

    def __init__(self, model_name=&quot;base&quot;, language=&quot;ja&quot;):
        &quot;&quot;&quot;
        Args:
            model_name: Whisper model size
            language: Recognition language
        &quot;&quot;&quot;
        # Load Whisper model
        print(f&quot;Loading Whisper model '{model_name}'...&quot;)
        self.model = whisper.load_model(model_name)
        self.language = language

        # Audio settings
        self.RATE = 16000  # Sampling rate
        self.CHUNK = 1024  # Buffer size
        self.CHANNELS = 1  # Mono
        self.FORMAT = pyaudio.paInt16

        # Audio buffer
        self.audio_queue = queue.Queue()
        self.audio_buffer = deque(maxlen=30)  # 30-second buffer

        # PyAudio initialization
        self.audio = pyaudio.PyAudio()

        # State management
        self.is_recording = False
        self.transcription_thread = None

        print(&quot;‚úì Real-time ASR initialization completed&quot;)

    def audio_callback(self, in_data, frame_count, time_info, status):
        &quot;&quot;&quot;Audio input callback&quot;&quot;&quot;
        if self.is_recording:
            # Add audio data to queue
            audio_data = np.frombuffer(in_data, dtype=np.int16)
            self.audio_queue.put(audio_data)

        return (in_data, pyaudio.paContinue)

    def start_recording(self):
        &quot;&quot;&quot;Start recording&quot;&quot;&quot;
        self.is_recording = True

        # Open audio stream
        self.stream = self.audio.open(
            format=self.FORMAT,
            channels=self.CHANNELS,
            rate=self.RATE,
            input=True,
            frames_per_buffer=self.CHUNK,
            stream_callback=self.audio_callback
        )

        self.stream.start_stream()

        # Start transcription thread
        self.transcription_thread = threading.Thread(target=self.transcribe_loop)
        self.transcription_thread.start()

        print(&quot;üé§ Recording started...&quot;)

    def stop_recording(self):
        &quot;&quot;&quot;Stop recording&quot;&quot;&quot;
        self.is_recording = False

        if hasattr(self, 'stream'):
            self.stream.stop_stream()
            self.stream.close()

        if self.transcription_thread:
            self.transcription_thread.join()

        print(&quot;‚èπÔ∏è  Recording stopped&quot;)

    def transcribe_loop(self):
        &quot;&quot;&quot;Transcription loop&quot;&quot;&quot;
        print(&quot;üìù Starting transcription...&quot;)

        while self.is_recording:
            # Collect audio data (1 second worth)
            audio_chunks = []
            for _ in range(int(self.RATE / self.CHUNK)):
                try:
                    chunk = self.audio_queue.get(timeout=0.1)
                    audio_chunks.append(chunk)
                except queue.Empty:
                    continue

            if not audio_chunks:
                continue

            # Concatenate audio data
            audio_data = np.concatenate(audio_chunks)
            self.audio_buffer.append(audio_data)

            # Get audio from buffer (5 seconds worth)
            if len(self.audio_buffer) &gt;= 5:
                # Use latest 5 seconds
                audio_segment = np.concatenate(list(self.audio_buffer)[-5:])

                # Normalize
                audio_segment = audio_segment.astype(np.float32) / 32768.0

                # Transcription
                try:
                    result = self.model.transcribe(
                        audio_segment,
                        language=self.language,
                        task=&quot;transcribe&quot;,
                        fp16=False,
                        temperature=0.0,
                        no_speech_threshold=0.6
                    )

                    text = result[&quot;text&quot;].strip()
                    if text:
                        print(f&quot;Recognition result: {text}&quot;)

                except Exception as e:
                    print(f&quot;Error: {e}&quot;)

    def __del__(self):
        &quot;&quot;&quot;Cleanup&quot;&quot;&quot;
        if hasattr(self, 'audio'):
            self.audio.terminate()


# Usage example
def realtime_asr_demo():
    &quot;&quot;&quot;Real-time ASR demo&quot;&quot;&quot;

    # Initialize ASR system
    asr = RealtimeASR(model_name=&quot;base&quot;, language=&quot;ja&quot;)

    try:
        # Start recording
        asr.start_recording()

        # Record for 10 seconds
        import time
        print(&quot;Please speak for 10 seconds...&quot;)
        time.sleep(10)

        # Stop recording
        asr.stop_recording()

    except KeyboardInterrupt:
        print(&quot;\nInterrupted&quot;)
        asr.stop_recording()

    print(&quot;‚úì Demo completed&quot;)


# Batch processing version (from file)
def batch_transcribe_with_speaker_diarization(audio_file):
    &quot;&quot;&quot;
    Speech recognition with speaker diarization
    (using libraries like pyannote.audio)
    &quot;&quot;&quot;
    import whisper

    # Whisper transcription
    model = whisper.load_model(&quot;medium&quot;)
    result = model.transcribe(
        audio_file,
        language=&quot;ja&quot;,
        word_timestamps=True
    )

    # Speaker diarization (dummy implementation)
    # Use libraries like pyannote.audio in practice
    print(&quot;=&quot; * 50)
    print(&quot;Transcription result (with speakers):&quot;)
    print(&quot;=&quot; * 50)

    current_speaker = &quot;Speaker 1&quot;
    for i, segment in enumerate(result[&quot;segments&quot;]):
        # Simple speaker switch detection (use actual speaker diarization model in practice)
        if i &gt; 0 and segment[&quot;start&quot;] - result[&quot;segments&quot;][i-1][&quot;end&quot;] &gt; 2.0:
            current_speaker = &quot;Speaker 2&quot; if current_speaker == &quot;Speaker 1&quot; else &quot;Speaker 1&quot;

        start = segment[&quot;start&quot;]
        end = segment[&quot;end&quot;]
        text = segment[&quot;text&quot;]

        print(f&quot;[{current_speaker}] [{start:.2f}s - {end:.2f}s]&quot;)
        print(f&quot;  {text}&quot;)
        print()

    return result


# Note: pyaudio installation is required for actual execution
# pip install pyaudio
#
# For macOS:
# brew install portaudio
# pip install pyaudio

print(&quot;Real-time ASR system implementation example displayed&quot;)
print(&quot;Requires 'pyaudio' installation for execution&quot;)
</code></pre>
<h3>5.3 Complete ASR Application</h3>
<p>Build a complete speech recognition application with a web interface.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

import gradio as gr
import whisper
import numpy as np
from pathlib import Path

class ASRApplication:
    &quot;&quot;&quot;Web-based speech recognition application&quot;&quot;&quot;

    def __init__(self):
        &quot;&quot;&quot;Application initialization&quot;&quot;&quot;
        self.models = {}
        self.current_model = None

        # Available models
        self.available_models = {
            &quot;tiny&quot;: &quot;Fastest (39M parameters)&quot;,
            &quot;base&quot;: &quot;Fast (74M parameters)&quot;,
            &quot;small&quot;: &quot;Balanced (244M parameters)&quot;,
            &quot;medium&quot;: &quot;High accuracy (769M parameters)&quot;,
            &quot;large&quot;: &quot;Highest accuracy (1550M parameters)&quot;
        }

        # Supported languages
        self.languages = {
            &quot;Auto-detect&quot;: None,
            &quot;Japanese&quot;: &quot;ja&quot;,
            &quot;English&quot;: &quot;en&quot;,
            &quot;Chinese&quot;: &quot;zh&quot;,
            &quot;Korean&quot;: &quot;ko&quot;,
            &quot;Spanish&quot;: &quot;es&quot;,
            &quot;French&quot;: &quot;fr&quot;,
            &quot;German&quot;: &quot;de&quot;
        }

    def load_model(self, model_name):
        &quot;&quot;&quot;Load model (with caching)&quot;&quot;&quot;
        if model_name not in self.models:
            print(f&quot;Loading model '{model_name}'...&quot;)
            self.models[model_name] = whisper.load_model(model_name)
            print(f&quot;‚úì Model '{model_name}' loading completed&quot;)

        self.current_model = self.models[model_name]
        return self.current_model

    def transcribe_audio(self, audio_file, model_name, language, task,
                        include_timestamps, beam_size, temperature):
        &quot;&quot;&quot;
        Transcribe audio file

        Args:
            audio_file: Audio file path
            model_name: Model to use
            language: Recognition language
            task: transcribe or translate
            include_timestamps: Whether to include timestamps
            beam_size: Beam search width
            temperature: Sampling temperature
        &quot;&quot;&quot;
        if audio_file is None:
            return &quot;Please upload an audio file&quot;, &quot;&quot;

        try:
            # Load model
            model = self.load_model(model_name)

            # Transcription
            result = model.transcribe(
                audio_file,
                language=self.languages.get(language),
                task=task,
                beam_size=beam_size,
                temperature=temperature,
                word_timestamps=include_timestamps
            )

            # Basic transcription result
            transcription = result[&quot;text&quot;]

            # Detailed information
            details = self._format_details(result, include_timestamps)

            return transcription, details

        except Exception as e:
            return f&quot;An error occurred: {str(e)}&quot;, &quot;&quot;

    def _format_details(self, result, include_timestamps):
        &quot;&quot;&quot;Format detailed information&quot;&quot;&quot;
        details = []

        # Detected language
        if &quot;language&quot; in result:
            details.append(f&quot;Detected language: {result['language']}&quot;)

        # Segment information
        if include_timestamps and &quot;segments&quot; in result:
            details.append(&quot;\n&quot; + &quot;=&quot; * 50)
            details.append(&quot;Segment details:&quot;)
            details.append(&quot;=&quot; * 50)

            for i, segment in enumerate(result[&quot;segments&quot;], 1):
                start = segment[&quot;start&quot;]
                end = segment[&quot;end&quot;]
                text = segment[&quot;text&quot;]

                details.append(f&quot;\n[{i}] {start:.2f}s - {end:.2f}s&quot;)
                details.append(f&quot;    {text}&quot;)

                # Word-level timestamps
                if &quot;words&quot; in segment:
                    details.append(&quot;    Words:&quot;)
                    for word_info in segment[&quot;words&quot;]:
                        word = word_info[&quot;word&quot;]
                        w_start = word_info[&quot;start&quot;]
                        w_end = word_info[&quot;end&quot;]
                        details.append(f&quot;      - {word:20s} [{w_start:.2f}s - {w_end:.2f}s]&quot;)

        return &quot;\n&quot;.join(details)

    def create_interface(self):
        &quot;&quot;&quot;Create Gradio interface&quot;&quot;&quot;

        with gr.Blocks(title=&quot;AI Speech Recognition System&quot;) as interface:
            gr.Markdown(
                &quot;&quot;&quot;
                # üéôÔ∏è AI Speech Recognition System

                High-accuracy speech recognition system using Whisper.
                Upload an audio file or record with microphone to perform transcription.
                &quot;&quot;&quot;
            )

            with gr.Row():
                with gr.Column(scale=1):
                    # Input controls
                    audio_input = gr.Audio(
                        sources=[&quot;upload&quot;, &quot;microphone&quot;],
                        type=&quot;filepath&quot;,
                        label=&quot;Audio input&quot;
                    )

                    model_selector = gr.Dropdown(
                        choices=list(self.available_models.keys()),
                        value=&quot;base&quot;,
                        label=&quot;Model selection&quot;,
                        info=&quot;Select accuracy vs speed tradeoff&quot;
                    )

                    language_selector = gr.Dropdown(
                        choices=list(self.languages.keys()),
                        value=&quot;Auto-detect&quot;,
                        label=&quot;Language&quot;
                    )

                    task_selector = gr.Radio(
                        choices=[&quot;transcribe&quot;, &quot;translate&quot;],
                        value=&quot;transcribe&quot;,
                        label=&quot;Task&quot;,
                        info=&quot;transcribe: Transcription in same language / translate: Translate to English&quot;
                    )

                    with gr.Accordion(&quot;Advanced settings&quot;, open=False):
                        include_timestamps = gr.Checkbox(
                            label=&quot;Include timestamps&quot;,
                            value=True
                        )

                        beam_size = gr.Slider(
                            minimum=1,
                            maximum=10,
                            value=5,
                            step=1,
                            label=&quot;Beam size&quot;,
                            info=&quot;Larger values improve accuracy but increase computation time&quot;
                        )

                        temperature = gr.Slider(
                            minimum=0.0,
                            maximum=1.0,
                            value=0.0,
                            step=0.1,
                            label=&quot;Temperature&quot;,
                            info=&quot;0: Deterministic, &gt;0: Adds randomness&quot;
                        )

                    transcribe_btn = gr.Button(&quot;Start transcription&quot;, variant=&quot;primary&quot;)

                with gr.Column(scale=2):
                    # Output
                    transcription_output = gr.Textbox(
                        label=&quot;Transcription result&quot;,
                        lines=5,
                        max_lines=10
                    )

                    details_output = gr.Textbox(
                        label=&quot;Detailed information&quot;,
                        lines=15,
                        max_lines=30
                    )

            # Event handler
            transcribe_btn.click(
                fn=self.transcribe_audio,
                inputs=[
                    audio_input,
                    model_selector,
                    language_selector,
                    task_selector,
                    include_timestamps,
                    beam_size,
                    temperature
                ],
                outputs=[transcription_output, details_output]
            )

            # Usage instructions
            gr.Markdown(
                &quot;&quot;&quot;
                ## How to Use

                1. **Audio input**: Upload a file or record with microphone button
                2. **Model selection**: Select model size based on your needs
                   - Real-time processing: tiny/base
                   - General use: small
                   - High accuracy needed: medium/large
                3. **Language selection**: Auto-detect or select specific language
                4. **Start transcription**: Click button to begin processing

                ## Model Information

                | Model | Parameters | Speed | Accuracy | Recommended Use |
                |--------|------------|-------|----------|-----------------|
                | tiny   | 39M        | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ | Real-time processing |
                | base   | 74M        | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ | Fast processing |
                | small  | 244M       | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ | Balanced |
                | medium | 769M       | ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | High accuracy |
                | large  | 1550M      | ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | Highest accuracy |
                &quot;&quot;&quot;
            )

        return interface

    def launch(self, share=False):
        &quot;&quot;&quot;Launch application&quot;&quot;&quot;
        interface = self.create_interface()
        interface.launch(share=share)


# Execute application
if __name__ == &quot;__main__&quot;:
    app = ASRApplication()

    print(&quot;=&quot; * 50)
    print(&quot;Starting AI Speech Recognition System...&quot;)
    print(&quot;=&quot; * 50)

    # Launch application
    # app.launch(share=False)

    # Note: Gradio installation required
    # pip install gradio

    print(&quot;‚úì Application setup completed&quot;)
    print(&quot;Requires 'gradio' installation for execution&quot;)
    print(&quot;Install with: pip install gradio&quot;)
</code></pre>
<h2>Practice Problems</h2>
<details>
<summary><strong>Problem 1: Understanding CTC Loss</strong></summary>
<p><strong>Problem</strong>: Explain why CTC can learn without alignment information and describe the role of the Blank token.</p>
<p><strong>Sample Answer</strong>:</p>
<p>CTC can learn without explicit alignment information by marginalizing the probabilities of all possible alignment paths. Specifically:</p>
<ul>
<li><strong>Many-to-one mapping</strong>: Multiple acoustic frames can correspond to the same character</li>
<li><strong>Role of Blank token</strong>:
                    <ul>
<li>Represent boundaries between characters</li>
<li>Distinguish consecutive identical characters (e.g., "hello" ‚Üí "hhe-ll-oo")</li>
<li>Represent positions with no output</li>
</ul>
</li>
<li><strong>Forward-Backward algorithm</strong>: Efficiently compute all paths</li>
</ul>
</details>
<details>
<summary><strong>Problem 2: Differences Between Attention Mechanism and CTC</strong></summary>
<p><strong>Problem</strong>: Explain the main differences between Attention-based models and CTC-based models from the perspectives of architecture and learning.</p>
<p><strong>Sample Answer</strong>:</p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>CTC</th>
<th>Attention-based</th>
</tr>
</thead>
<tbody>
<tr>
<td>architecture</td>
<td>Encoder + Linear classifier</td>
<td>Encoder + Attention + Decoder</td>
</tr>
<tr>
<td>alignment</td>
<td>Monotonic</td>
<td>Flexible (determined by Attention)</td>
</tr>
<tr>
<td>language model</td>
<td>Conditional independence (external LM required)</td>
<td>Integrated in Decoder</td>
</tr>
<tr>
<td>Computational Cost</td>
<td>Low</td>
<td>High</td>
</tr>
<tr>
<td>Long-range Dependencies</td>
<td>Weak</td>
<td>Strong</td>
</tr>
</tbody>
</table>
</details>
<details>
<summary><strong>Problem 3: RNN-T Implementation</strong></summary>
<p><strong>Problem</strong>: Explain the roles of the three main components of RNN-Transducer (Encoder, Prediction Network, Joint Network) and show a simple implementation.</p>
<p><strong>Answer</strong>: Refer to the RNN-T implementation in the main text. Role of each component:</p>
<ul>
<li><strong>Encoder (Transcription Network)</strong>: Converts acoustic features to high-level representations</li>
<li><strong>Prediction Network</strong>: Functions as a language model, generating next predictions from previous outputs</li>
<li><strong>Joint Network</strong>: Combines outputs from Encoder and Prediction Network to compute final output probabilities</li>
</ul>
</details>
<details>
<summary><strong>Problem 4: Fine-tuning Whisper</strong></summary>
<p><strong>Problem</strong>: Describe the main considerations when fine-tuning Whisper for specific domains (e.g., medical, legal).</p>
<p><strong>Sample Answer</strong>:</p>
<ol>
<li><strong>Dataset</strong>:
                    <ul>
<li>Collect domain-specific audio data</li>
<li>Accurate transcription of technical terms</li>
<li>Diverse speakers and acoustic conditions</li>
</ul>
</li>
<li><strong>Vocabulary expansion</strong>:
                    <ul>
<li>Add domain-specific terms to tokenizer</li>
<li>Handle abbreviations and specialized notation</li>
</ul>
</li>
<li><strong>Learning rate and regularization</strong>:
                    <ul>
<li>Train carefully with low learning rate (around 1e-5)</li>
<li>Dropout to prevent overfitting</li>
</ul>
</li>
<li><strong>Evaluation</strong>:
                    <ul>
<li>Evaluate WER on domain-specific test sets</li>
<li>Evaluate recognition accuracy of technical terms separately</li>
</ul>
</li>
</ol>
</details>
<details>
<summary><strong>Problem 5: Optimizing Real-time ASR</strong></summary>
<p><strong>Problem</strong>: List three methods to optimize the latency-accuracy tradeoff in real-time speech recognition systems.</p>
<p><strong>Sample Answer</strong>:</p>
<ol>
<li><strong>Model size selection</strong>:
                    <ul>
<li>Use lightweight models (tiny/base) for real-time processing</li>
<li>Use model distillation for downsizing if needed</li>
</ul>
</li>
<li><strong>Streaming processing</strong>:
                    <ul>
<li>Use streaming-capable architectures like RNN-T</li>
<li>Balance chunk size and recognition accuracy</li>
<li>Limit lookahead</li>
</ul>
</li>
<li><strong>Hardware optimization</strong>:
                    <ul>
<li>Utilize GPU/TPU</li>
<li>Accelerate inference with quantization (e.g., INT8)</li>
<li>Utilize batch processing</li>
</ul>
</li>
</ol>
</details>
<h2>Summary</h2>
<p>In this chapter, we learned the main techniques for speech recognition using deep learning:</p>
<ul>
<li><strong>CTC</strong>: Enables alignment-free learning, simple and efficient</li>
<li><strong>Attention-based Models</strong>: Flexible alignment and utilization of contextual information</li>
<li><strong>RNN-Transducer</strong>: Optimal for streaming ASR, language model integration</li>
<li><strong>Whisper</strong>: Multilingual support, high robustness, zero-shot performance</li>
<li><strong>Practice</strong>: Fine-tuning, real-time processing, complete applications</li>
</ul>
<p>By combining these techniques, you can build highly accurate speech recognition systems for various scenarios. In the next chapter, we will learn about speech synthesis (TTS) and voice conversion.</p>
<h2>References</h2>
<ul>
<li>Graves, A., et al. (2006). "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks"</li>
<li>Chan, W., et al. (2016). "Listen, Attend and Spell"</li>
<li>Vaswani, A., et al. (2017). "Attention Is All You Need"</li>
<li>Graves, A. (2012). "Sequence Transduction with Recurrent Neural Networks"</li>
<li>Radford, A., et al. (2022). "Robust Speech Recognition via Large-Scale Weak Supervision" (Whisper paper)</li>
<li>Watanabe, S., et al. (2017). "Hybrid CTC/Attention architecture for End-to-End Speech Recognition"</li>
<li>PyTorch Documentation: <a href="https://pytorch.org/docs/stable/nn.html#ctcloss">nn.CTCLoss</a></li>
<li>OpenAI Whisper: <a href="https://github.com/openai/whisper">GitHub Repository</a></li>
<li>Hugging Face Transformers: <a href="https://huggingface.co/docs/transformers/model_doc/whisper">Whisper Documentation</a></li>
</ul>
<div class="navigation">
<a class="nav-button" href="chapter2-feature-extraction.html">‚Üê Chapter 2: Acoustic feature Extraction</a>
<a class="nav-button" href="index.html">Back to Table of Contents</a>
<a class="nav-button" href="chapter4-tts-voice-conversion.html">Chapter 4: Speech Synthesis and Voice Conversion ‚Üí</a>
</div>
</div>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical warranty, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranty, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the stated conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>

<footer>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</body>
</html>
