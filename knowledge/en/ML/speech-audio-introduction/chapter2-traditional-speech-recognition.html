<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: Traditional Speech Recognition - AI Terakoya</title>

        <link rel="stylesheet" href="../../assets/css/knowledge-base.css">

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/speech-audio-introduction/index.html">Speech Audio</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 2</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 2: Traditional Speech Recognition</h1>
            <p class="subtitle">HMM-GMM Era Speech Recognition Technology - Statistical Model-Based Approaches</p>
            <div class="meta">
                <span class="meta-item">üìñ Reading Time: 35-40 minutes</span>
                <span class="meta-item">üìä Difficulty: Intermediate</span>
                <span class="meta-item">üíª Code Examples: 8</span>
                <span class="meta-item">üìù Exercises: 5</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Learning Objectives</h2>
<p>By reading this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Understand the definition and evaluation metrics of Automatic Speech Recognition (ASR) tasks</li>
<li>‚úÖ Implement Hidden Markov Model (HMM) principles and algorithms</li>
<li>‚úÖ Understand acoustic modeling using Gaussian Mixture Models (GMM)</li>
<li>‚úÖ Build and evaluate language models (N-grams)</li>
<li>‚úÖ Construct a complete HMM-GMM based ASR pipeline</li>
<li>‚úÖ Evaluate systems using Word Error Rate (WER)</li>
</ul>

<hr>

<h2>2.1 Fundamentals of Speech Recognition</h2>

<h3>Definition of ASR Task</h3>
<p><strong>Automatic Speech Recognition (ASR)</strong> is a task that converts speech signals into text.</p>

<blockquote>
<p>Goal of speech recognition: Given an observed acoustic signal $X$, find the most likely word sequence $W$</p>
</blockquote>

<p>This is formulated using Bayes' theorem as follows:</p>

<p>$$
\hat{W} = \arg\max_{W} P(W|X) = \arg\max_{W} \frac{P(X|W) P(W)}{P(X)} = \arg\max_{W} P(X|W) P(W)
$$</p>

<ul>
<li>$P(X|W)$: <strong>Acoustic Model</strong> - probability of the acoustic signal given a word sequence</li>
<li>$P(W)$: <strong>Language Model</strong> - prior probability of the word sequence</li>
</ul>

<h3>Components of ASR Systems</h3>

<div class="mermaid">
graph LR
    A[Speech Signal] --> B[Feature Extraction<br/>MFCC]
    B --> C[Acoustic Model<br/>HMM-GMM]
    C --> D[Decoding<br/>Viterbi]
    D --> E[Language Model<br/>N-gram]
    E --> F[Recognition Result<br/>Text]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#e3f2fd
    style D fill:#f3e5f5
    style E fill:#e8f5e9
    style F fill:#c8e6c9
</div>

<h3>Evaluation Metrics</h3>

<h4>Word Error Rate (WER)</h4>

<p><strong>WER</strong> is the standard evaluation metric for speech recognition:</p>

<p>$$
\text{WER} = \frac{S + D + I}{N} \times 100\%
$$</p>

<ul>
<li>$S$: Substitutions</li>
<li>$D$: Deletions</li>
<li>$I$: Insertions</li>
<li>$N$: Total number of words in the reference text</li>
</ul>

<h4>Character Error Rate (CER)</h4>

<p>For languages like Japanese and Chinese, character-level CER is also used:</p>

<p>$$
\text{CER} = \frac{S_c + D_c + I_c}{N_c} \times 100\%
$$</p>

<h3>Implementation: WER Calculation</h3>

<pre><code class="language-python">import numpy as np
from typing import List, Tuple

def levenshtein_distance(ref: List[str], hyp: List[str]) -> Tuple[int, int, int, int]:
    """
    Calculate Levenshtein distance and error statistics

    Returns:
        (distance, substitutions, deletions, insertions)
    """
    m, n = len(ref), len(hyp)

    # DP table
    dp = np.zeros((m + 1, n + 1), dtype=int)

    # Initialization
    for i in range(m + 1):
        dp[i][0] = i
    for j in range(n + 1):
        dp[0][j] = j

    # DP
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if ref[i-1] == hyp[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = min(
                    dp[i-1][j-1] + 1,  # substitution
                    dp[i-1][j] + 1,    # deletion
                    dp[i][j-1] + 1     # insertion
                )

    # Backtrack: Calculate error statistics
    i, j = m, n
    subs, dels, ins = 0, 0, 0

    while i > 0 or j > 0:
        if i > 0 and j > 0 and ref[i-1] == hyp[j-1]:
            i -= 1
            j -= 1
        elif i > 0 and j > 0 and dp[i][j] == dp[i-1][j-1] + 1:
            subs += 1
            i -= 1
            j -= 1
        elif i > 0 and dp[i][j] == dp[i-1][j] + 1:
            dels += 1
            i -= 1
        else:
            ins += 1
            j -= 1

    return dp[m][n], subs, dels, ins


def calculate_wer(reference: str, hypothesis: str) -> dict:
    """
    Calculate WER (Word Error Rate)

    Args:
        reference: Reference text
        hypothesis: Recognition result

    Returns:
        Dictionary containing error statistics
    """
    ref_words = reference.split()
    hyp_words = hypothesis.split()

    dist, subs, dels, ins = levenshtein_distance(ref_words, hyp_words)

    n_words = len(ref_words)
    wer = (dist / n_words * 100) if n_words > 0 else 0

    return {
        'WER': wer,
        'substitutions': subs,
        'deletions': dels,
        'insertions': ins,
        'total_errors': dist,
        'total_words': n_words
    }


# Test
reference = "the quick brown fox jumps over the lazy dog"
hypothesis = "the quick brown fox jumped over a lazy dog"

result = calculate_wer(reference, hypothesis)

print("=== WER Calculation Example ===")
print(f"Reference: {reference}")
print(f"Hypothesis: {hypothesis}")
print(f"\nWER: {result['WER']:.2f}%")
print(f"Substitutions: {result['substitutions']}")
print(f"Deletions: {result['deletions']}")
print(f"Insertions: {result['insertions']}")
print(f"Total Errors: {result['total_errors']}")
print(f"Total Words: {result['total_words']}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== WER Calculation Example ===
Reference: the quick brown fox jumps over the lazy dog
Hypothesis: the quick brown fox jumped over a lazy dog

WER: 22.22%
Substitutions: 2
Deletions: 0
Insertions: 0
Total Errors: 2
Total Words: 9
</code></pre>

<blockquote>
<p><strong>Important</strong>: WER can exceed 100% (when there are many insertion errors).</p>
</blockquote>

<hr>

<h2>2.2 Hidden Markov Models (HMM)</h2>

<h3>HMM Fundamentals</h3>

<p><strong>Hidden Markov Models (HMM)</strong> are probabilistic models consisting of unobservable hidden states and observable outputs.</p>

<h4>Components of HMM</h4>

<ul>
<li>$N$: Number of states</li>
<li>$M$: Number of observation symbols</li>
<li>$A = \{a_{ij}\}$: State transition probability matrix $a_{ij} = P(q_{t+1}=j | q_t=i)$</li>
<li>$B = \{b_j(k)\}$: Output probability distribution $b_j(k) = P(o_t=k | q_t=j)$</li>
<li>$\pi = \{\pi_i\}$: Initial state probability $\pi_i = P(q_1=i)$</li>
</ul>

<h4>Three Fundamental Problems of HMM</h4>

<table>
<thead>
<tr>
<th>Problem</th>
<th>Description</th>
<th>Algorithm</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Evaluation</strong></td>
<td>Calculate probability of observation sequence</td>
<td>Forward-Backward</td>
</tr>
<tr>
<td><strong>Decoding</strong></td>
<td>Estimate most likely state sequence</td>
<td>Viterbi</td>
</tr>
<tr>
<td><strong>Learning</strong></td>
<td>Estimate parameters</td>
<td>Baum-Welch (EM)</td>
</tr>
</tbody>
</table>

<h3>Forward-Backward Algorithm</h3>

<p><strong>Forward Algorithm</strong> calculates the probability of the observation sequence $P(O|\lambda)$:</p>

<p>$$
\alpha_t(i) = P(o_1, o_2, \ldots, o_t, q_t=i | \lambda)
$$</p>

<p>Recursion formula:</p>

<p>$$
\alpha_t(j) = \left[\sum_{i=1}^N \alpha_{t-1}(i) a_{ij}\right] b_j(o_t)
$$</p>

<h3>Viterbi Algorithm</h3>

<p><strong>Viterbi Algorithm</strong> finds the most likely state sequence:</p>

<p>$$
\delta_t(i) = \max_{q_1, \ldots, q_{t-1}} P(q_1, \ldots, q_{t-1}, q_t=i, o_1, \ldots, o_t | \lambda)
$$</p>

<p>Recursion formula:</p>

<p>$$
\delta_t(j) = \left[\max_i \delta_{t-1}(i) a_{ij}\right] b_j(o_t)
$$</p>

<h3>Implementation: Basic HMM Operations</h3>

<pre><code class="language-python">import numpy as np
from hmmlearn import hmm
import matplotlib.pyplot as plt

# Example: Weather model
# States: 0=Sunny, 1=Rainy
# Observations: 0=Walk, 1=Shopping, 2=Cleaning

# HMM parameter definition
n_states = 2
n_observations = 3

# Build the model
model = hmm.MultinomialHMM(n_components=n_states, random_state=42)

# State transition probabilities
model.startprob_ = np.array([0.6, 0.4])  # Initial probabilities
model.transmat_ = np.array([
    [0.7, 0.3],  # Sunny ‚Üí [Sunny, Rainy]
    [0.4, 0.6]   # Rainy ‚Üí [Sunny, Rainy]
])

# Emission probabilities
model.emissionprob_ = np.array([
    [0.6, 0.3, 0.1],  # Sunny day: [Walk, Shopping, Cleaning]
    [0.1, 0.4, 0.5]   # Rainy day: [Walk, Shopping, Cleaning]
])

print("=== HMM Parameters ===")
print("\nInitial state probabilities:")
print(f"  Sunny: {model.startprob_[0]:.2f}")
print(f"  Rainy: {model.startprob_[1]:.2f}")

print("\nState transition probabilities:")
print(model.transmat_)

print("\nEmission probabilities:")
print("       Walk   Shopping  Cleaning")
print(f"Sunny: {model.emissionprob_[0]}")
print(f"Rainy: {model.emissionprob_[1]}")

# Observation sequence
observations = np.array([[0], [1], [2], [1], [0]])  # Walk, Shopping, Cleaning, Shopping, Walk

# Forward algorithm: Probability of observation sequence
log_prob = model.score(observations)
print(f"\nLog-likelihood of observation sequence: {log_prob:.4f}")
print(f"Probability of observation sequence: {np.exp(log_prob):.6f}")

# Viterbi algorithm: Most likely state sequence
log_prob, states = model.decode(observations)
state_names = ['Sunny', 'Rainy']
print(f"\nMost likely state sequence:")
for i, (obs, state) in enumerate(zip(observations.flatten(), states)):
    obs_names = ['Walk', 'Shopping', 'Cleaning']
    print(f"  Day{i+1}: Observation={obs_names[obs]}, State={state_names[state]}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== HMM Parameters ===

Initial state probabilities:
  Sunny: 0.60
  Rainy: 0.40

State transition probabilities:
[[0.7 0.3]
 [0.4 0.6]]

Emission probabilities:
       Walk   Shopping  Cleaning
Sunny: [0.6 0.3 0.1]
Rainy: [0.1 0.4 0.5]

Log-likelihood of observation sequence: -6.3218
Probability of observation sequence: 0.001802

Most likely state sequence:
  Day1: Observation=Walk, State=Sunny
  Day2: Observation=Shopping, State=Sunny
  Day3: Observation=Cleaning, State=Rainy
  Day4: Observation=Shopping, State=Rainy
  Day5: Observation=Walk, State=Rainy
</code></pre>

<h3>Phoneme Modeling with HMM</h3>

<p>In speech recognition, each phoneme is modeled with a 3-state Left-to-Right HMM:</p>

<div class="mermaid">
graph LR
    Start((Start)) --> S1[State 1<br/>Phoneme Begin]
    S1 --> S1
    S1 --> S2[State 2<br/>Phoneme Middle]
    S2 --> S2
    S2 --> S3[State 3<br/>Phoneme End]
    S3 --> S3
    S3 --> End((End))

    style Start fill:#c8e6c9
    style S1 fill:#e3f2fd
    style S2 fill:#fff3e0
    style S3 fill:#f3e5f5
    style End fill:#ffcdd2
</div>

<pre><code class="language-python"># Left-to-Right HMM (Phoneme model)
n_states = 3

# Left-to-Right structure transition matrix
# States can only advance or self-loop
lr_model = hmm.GaussianHMM(n_components=n_states, covariance_type="diag", random_state=42)

# Transition probabilities (left-to-right only)
lr_model.transmat_ = np.array([
    [0.5, 0.5, 0.0],  # State 1: self-loop or to State 2
    [0.0, 0.5, 0.5],  # State 2: self-loop or to State 3
    [0.0, 0.0, 1.0]   # State 3: self-loop only
])

lr_model.startprob_ = np.array([1.0, 0.0, 0.0])  # Always start from State 1

print("=== Left-to-Right HMM ===")
print("Transition probability matrix:")
print(lr_model.transmat_)
print("\nStarts from State 1 and can only move left-to-right")
</code></pre>

<hr>

<h2>2.3 Gaussian Mixture Models (GMM)</h2>

<h3>GMM Fundamentals</h3>

<p><strong>Gaussian Mixture Models (GMM)</strong> represent a probability distribution as a linear combination of multiple Gaussian distributions:</p>

<p>$$
p(x) = \sum_{k=1}^K w_k \mathcal{N}(x | \mu_k, \Sigma_k)
$$</p>

<ul>
<li>$K$: Number of mixture components</li>
<li>$w_k$: Mixture weights ($\sum_k w_k = 1$)</li>
<li>$\mu_k$: Mean vector</li>
<li>$\Sigma_k$: Covariance matrix</li>
</ul>

<h3>Learning with EM Algorithm</h3>

<p>GMM parameters are estimated using the <strong>EM (Expectation-Maximization) algorithm</strong>:</p>

<h4>E-step: Calculate Responsibilities</h4>

<p>$$
\gamma_{nk} = \frac{w_k \mathcal{N}(x_n | \mu_k, \Sigma_k)}{\sum_{j=1}^K w_j \mathcal{N}(x_n | \mu_j, \Sigma_j)}
$$</p>

<h4>M-step: Update Parameters</h4>

<p>$$
\mu_k^{\text{new}} = \frac{1}{N_k} \sum_{n=1}^N \gamma_{nk} x_n
$$</p>

<p>$$
\Sigma_k^{\text{new}} = \frac{1}{N_k} \sum_{n=1}^N \gamma_{nk} (x_n - \mu_k^{\text{new}})(x_n - \mu_k^{\text{new}})^T
$$</p>

<p>$$
w_k^{\text{new}} = \frac{N_k}{N}, \quad N_k = \sum_{n=1}^N \gamma_{nk}
$$</p>

<h3>Implementation: Clustering with GMM</h3>

<pre><code class="language-python">import numpy as np
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt

# Data generated from 3 Gaussian distributions
np.random.seed(42)

# Data generation
n_samples = 300
X1 = np.random.randn(n_samples // 3, 2) * 0.5 + np.array([0, 0])
X2 = np.random.randn(n_samples // 3, 2) * 0.7 + np.array([3, 3])
X3 = np.random.randn(n_samples // 3, 2) * 0.6 + np.array([0, 3])

X = np.vstack([X1, X2, X3])

# GMM clustering
n_components = 3
gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=42)
gmm.fit(X)

# Prediction
labels = gmm.predict(X)
proba = gmm.predict_proba(X)

print("=== GMM Parameters ===")
print(f"Number of components: {n_components}")
print(f"Number of iterations to converge: {gmm.n_iter_}")
print(f"Log-likelihood: {gmm.score(X) * len(X):.2f}")

print("\nMixture weights:")
for i, weight in enumerate(gmm.weights_):
    print(f"  Component{i+1}: {weight:.3f}")

print("\nMean vectors:")
for i, mean in enumerate(gmm.means_):
    print(f"  Component{i+1}: [{mean[0]:.2f}, {mean[1]:.2f}]")

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Clustering results
axes[0].scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.6, edgecolors='black')
axes[0].scatter(gmm.means_[:, 0], gmm.means_[:, 1], c='red', s=200, marker='X',
                edgecolors='black', linewidths=2, label='Centers')
axes[0].set_xlabel('Feature 1')
axes[0].set_ylabel('Feature 2')
axes[0].set_title('GMM Clustering', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Probability density visualization
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                     np.linspace(y_min, y_max, 100))
Z = -gmm.score_samples(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

axes[1].contourf(xx, yy, Z, levels=20, cmap='viridis', alpha=0.6)
axes[1].scatter(X[:, 0], X[:, 1], c='white', s=10, alpha=0.5, edgecolors='black')
axes[1].scatter(gmm.means_[:, 0], gmm.means_[:, 1], c='red', s=200, marker='X',
                edgecolors='black', linewidths=2)
axes[1].set_xlabel('Feature 1')
axes[1].set_ylabel('Feature 2')
axes[1].set_title('GMM Probability Density', fontsize=14)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3>GMM-HMM System</h3>

<p>In traditional ASR, the output probabilities of each HMM state are modeled with GMMs:</p>

<p>$$
b_j(o_t) = \sum_{m=1}^M c_{jm} \mathcal{N}(o_t | \mu_{jm}, \Sigma_{jm})
$$</p>

<ul>
<li>$j$: HMM state</li>
<li>$m$: GMM component</li>
<li>$c_{jm}$: Weight of component $m$ in state $j$</li>
</ul>

<pre><code class="language-python">from hmmlearn import hmm

# HMM with GMM as emission distribution
n_states = 3
n_mix = 4  # Number of GMM components per state

# GaussianHMM: Each state has a Gaussian distribution
# n_mix > 1 makes each state a GMM
gmm_hmm = hmm.GMMHMM(n_components=n_states, n_mix=n_mix,
                      covariance_type='diag', n_iter=100, random_state=42)

# Training data generation (2D features)
np.random.seed(42)
n_samples = 200
train_data = np.random.randn(n_samples, 2) * 0.5

# Model training
gmm_hmm.fit(train_data)

print("\n=== GMM-HMM System ===")
print(f"Number of HMM states: {n_states}")
print(f"Number of GMM components per state: {n_mix}")
print(f"Number of iterations to converge: {gmm_hmm.monitor_.iter}")
print(f"Log-likelihood: {gmm_hmm.score(train_data) * len(train_data):.2f}")

# Decoding
test_data = np.random.randn(10, 2) * 0.5
log_prob, states = gmm_hmm.decode(test_data)

print(f"\nState sequence for test data:")
print(f"  {states}")
print(f"  Log probability: {log_prob:.4f}")
</code></pre>

<hr>

<h2>2.4 Language Models</h2>

<h3>N-gram Models</h3>

<p><strong>N-gram models</strong> predict the probability of a word sequence based on the previous $n-1$ words:</p>

<p>$$
P(w_1, w_2, \ldots, w_n) \approx \prod_{i=1}^n P(w_i | w_{i-n+1}, \ldots, w_{i-1})
$$</p>

<h4>Main N-gram Models</h4>

<table>
<thead>
<tr>
<th>Model</th>
<th>Definition</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Unigram</strong></td>
<td>$P(w_i)$</td>
<td>Independent word probability</td>
</tr>
<tr>
<td><strong>Bigram</strong></td>
<td>$P(w_i | w_{i-1})$</td>
<td>Depends on previous word</td>
</tr>
<tr>
<td><strong>Trigram</strong></td>
<td>$P(w_i | w_{i-2}, w_{i-1})$</td>
<td>Depends on previous 2 words</td>
</tr>
</tbody>
</table>

<h3>Maximum Likelihood Estimation</h3>

<p>N-gram probabilities are estimated from counts in the training corpus:</p>

<p>$$
P(w_i | w_{i-1}) = \frac{C(w_{i-1}, w_i)}{C(w_{i-1})}
$$</p>

<ul>
<li>$C(w_{i-1}, w_i)$: Count of bigram $(w_{i-1}, w_i)$</li>
<li>$C(w_{i-1})$: Count of word $w_{i-1}$</li>
</ul>

<h3>Perplexity</h3>

<p><strong>Perplexity</strong> is an evaluation metric for language models:</p>

<p>$$
\text{PPL} = P(w_1, \ldots, w_N)^{-1/N} = \sqrt[N]{\prod_{i=1}^N \frac{1}{P(w_i | w_1, \ldots, w_{i-1})}}
$$</p>

<p>Lower is better.</p>

<h3>Smoothing Techniques</h3>

<p>Techniques for assigning probabilities to unobserved N-grams:</p>

<h4>1. Add-k Smoothing (Laplace)</h4>

<p>$$
P(w_i | w_{i-1}) = \frac{C(w_{i-1}, w_i) + k}{C(w_{i-1}) + k|V|}
$$</p>

<h4>2. Kneser-Ney Smoothing</h4>

<p>Considers context diversity:</p>

<p>$$
P_{\text{KN}}(w_i | w_{i-1}) = \frac{\max(C(w_{i-1}, w_i) - \delta, 0)}{C(w_{i-1})} + \lambda(w_{i-1}) P_{\text{continuation}}(w_i)
$$</p>

<h3>Implementation: N-gram Language Model</h3>

<pre><code class="language-python">import numpy as np
from collections import defaultdict, Counter
from typing import List, Tuple

class BigramLanguageModel:
    """
    Bigram language model (with Add-k smoothing)
    """
    def __init__(self, k: float = 1.0):
        self.k = k
        self.unigram_counts = Counter()
        self.bigram_counts = defaultdict(Counter)
        self.vocab = set()

    def train(self, corpus: List[List[str]]):
        """
        Train the model from corpus

        Args:
            corpus: List of sentences (each sentence is a list of words)
        """
        for sentence in corpus:
            # Add start/end tags
            words = ['<s>'] + sentence + ['</s>']

            for word in words:
                self.vocab.add(word)
                self.unigram_counts[word] += 1

            for w1, w2 in zip(words[:-1], words[1:]):
                self.bigram_counts[w1][w2] += 1

        print(f"Vocabulary size: {len(self.vocab)}")
        print(f"Total words: {sum(self.unigram_counts.values())}")
        print(f"Unique bigrams: {sum(len(counts) for counts in self.bigram_counts.values())}")

    def probability(self, w1: str, w2: str) -> float:
        """
        Calculate bigram probability P(w2|w1) (with Add-k smoothing)
        """
        numerator = self.bigram_counts[w1][w2] + self.k
        denominator = self.unigram_counts[w1] + self.k * len(self.vocab)
        return numerator / denominator

    def sentence_probability(self, sentence: List[str]) -> float:
        """
        Calculate sentence probability
        """
        words = ['<s>'] + sentence + ['</s>']
        prob = 1.0

        for w1, w2 in zip(words[:-1], words[1:]):
            prob *= self.probability(w1, w2)

        return prob

    def perplexity(self, test_corpus: List[List[str]]) -> float:
        """
        Calculate perplexity of test corpus
        """
        log_prob = 0
        n_words = 0

        for sentence in test_corpus:
            words = ['<s>'] + sentence + ['</s>']
            n_words += len(words) - 1

            for w1, w2 in zip(words[:-1], words[1:]):
                prob = self.probability(w1, w2)
                log_prob += np.log2(prob)

        return 2 ** (-log_prob / n_words)


# Sample corpus
train_corpus = [
    ['I', 'love', 'machine', 'learning'],
    ['machine', 'learning', 'is', 'fun'],
    ['I', 'love', 'deep', 'learning'],
    ['deep', 'learning', 'is', 'powerful'],
    ['I', 'study', 'machine', 'learning'],
]

test_corpus = [
    ['I', 'love', 'learning'],
    ['machine', 'learning', 'is', 'interesting']
]

# Model training
print("=== Bigram Language Model ===\n")
lm = BigramLanguageModel(k=0.1)
lm.train(train_corpus)

# Probability calculation
print("\nBigram probability examples:")
bigrams = [('I', 'love'), ('love', 'learning'), ('machine', 'learning'), ('learning', 'is')]
for w1, w2 in bigrams:
    prob = lm.probability(w1, w2)
    print(f"  P({w2}|{w1}) = {prob:.4f}")

# Sentence probability
print("\nSentence probabilities:")
for sentence in test_corpus:
    prob = lm.sentence_probability(sentence)
    print(f"  '{' '.join(sentence)}': {prob:.6e}")

# Perplexity
ppl = lm.perplexity(test_corpus)
print(f"\nTest corpus perplexity: {ppl:.2f}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Bigram Language Model ===

Vocabulary size: 12
Total words: 35
Unique bigrams: 25

Bigram probability examples:
  P(love|I) = 0.4255
  P(learning|love) = 0.3571
  P(learning|machine) = 0.6667
  P(is|learning) = 0.5000

Sentence probabilities:
  'I love learning': 2.547618e-03
  'machine learning is interesting': 1.984127e-04

Test corpus perplexity: 8.91
</code></pre>

<h3>KenLM Library</h3>

<p>For practical N-gram language models, use <strong>KenLM</strong>:</p>

<pre><code class="language-python"># Advanced language model using KenLM
# Note: Installation required: pip install https://github.com/kpu/kenlm/archive/master.zip

import kenlm

# Load from ARPA format language model file
# model = kenlm.Model('path/to/model.arpa')

# Calculate sentence score
# score = model.score('this is a test sentence', bos=True, eos=True)
# perplexity = model.perplexity('this is a test sentence')

print("KenLM is an efficient N-gram language model implementation")
print("Optimized for training and querying on large corpora")
</code></pre>

<hr>

<h2>2.5 Traditional ASR Pipeline</h2>

<h3>Complete Pipeline Architecture</h3>

<div class="mermaid">
graph TD
    A[Speech Signal<br/>Waveform] --> B[Pre-processing<br/>Pre-emphasis]
    B --> C[Framing<br/>Framing]
    C --> D[Windowing<br/>Windowing]
    D --> E[MFCC Extraction<br/>Feature Extraction]
    E --> F[Delta Features<br/>Delta/Delta-Delta]
    F --> G[Acoustic Model<br/>GMM-HMM]
    G --> H[Viterbi Decoding<br/>+ Language Model]
    H --> I[Recognition Result<br/>Text]

    style A fill:#ffebee
    style E fill:#e3f2fd
    style G fill:#fff3e0
    style H fill:#f3e5f5
    style I fill:#c8e6c9
</div>

<h3>Implementation: Simple ASR System</h3>

<pre><code class="language-python">import numpy as np
import librosa
from hmmlearn import hmm
from sklearn.mixture import GaussianMixture
from typing import List, Tuple

class SimpleASR:
    """
    Simple speech recognition system (for demonstration)
    """
    def __init__(self, n_mfcc: int = 13, n_states: int = 3):
        self.n_mfcc = n_mfcc
        self.n_states = n_states
        self.models = {}  # HMM models for each word

    def extract_features(self, audio_path: str, sr: int = 16000) -> np.ndarray:
        """
        Extract MFCC features from audio file
        """
        # Load audio
        y, sr = librosa.load(audio_path, sr=sr)

        # MFCC
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=self.n_mfcc)

        # Delta features
        delta = librosa.feature.delta(mfcc)
        delta2 = librosa.feature.delta(mfcc, order=2)

        # Concatenate
        features = np.vstack([mfcc, delta, delta2])

        return features.T  # (time, features)

    def train_word_model(self, word: str, audio_files: List[str]):
        """
        Train HMM model for a specific word
        """
        # Extract features from all training data
        all_features = []
        lengths = []

        for audio_file in audio_files:
            features = self.extract_features(audio_file)
            all_features.append(features)
            lengths.append(len(features))

        # Concatenate
        X = np.vstack(all_features)

        # Left-to-Right HMM
        model = hmm.GaussianHMM(
            n_components=self.n_states,
            covariance_type='diag',
            n_iter=100,
            random_state=42
        )

        # Constrain transition probabilities (Left-to-Right)
        model.transmat_ = np.zeros((self.n_states, self.n_states))
        for i in range(self.n_states):
            if i < self.n_states - 1:
                model.transmat_[i, i] = 0.5
                model.transmat_[i, i+1] = 0.5
            else:
                model.transmat_[i, i] = 1.0

        model.startprob_ = np.zeros(self.n_states)
        model.startprob_[0] = 1.0

        # Training
        model.fit(X, lengths)

        self.models[word] = model

        print(f"Trained model for word '{word}'")
        print(f"  Number of training samples: {len(audio_files)}")
        print(f"  Total frames: {len(X)}")

    def recognize(self, audio_path: str) -> Tuple[str, float]:
        """
        Recognize audio file

        Returns:
            (recognized word, score)
        """
        # Feature extraction
        features = self.extract_features(audio_path)

        # Calculate score for each word model
        scores = {}
        for word, model in self.models.items():
            try:
                score = model.score(features)
                scores[word] = score
            except:
                scores[word] = -np.inf

        # Select word with highest score
        best_word = max(scores, key=scores.get)
        best_score = scores[best_word]

        return best_word, best_score


# Demo usage example (requires actual audio files)
print("=== Simple ASR System ===\n")
print("This system operates as follows:")
print("1. Extract MFCC features (+ delta) from speech")
print("2. Model each word with Left-to-Right HMM")
print("3. Select most likely word using Viterbi algorithm")
print("\nActual usage requires audio files")

# asr = SimpleASR(n_mfcc=13, n_states=3)
#
# # Training (multiple audio samples per word)
# asr.train_word_model('hello', ['hello1.wav', 'hello2.wav', 'hello3.wav'])
# asr.train_word_model('world', ['world1.wav', 'world2.wav', 'world3.wav'])
#
# # Recognition
# word, score = asr.recognize('test.wav')
# print(f"Recognition result: {word} (Score: {score:.2f})")
</code></pre>

<h3>Integration with Language Model</h3>

<p>In actual ASR, the acoustic model and language model are integrated:</p>

<p>$$
\hat{W} = \arg\max_W \left[\log P(X|W) + \lambda \log P(W)\right]
$$</p>

<ul>
<li>$\lambda$: Language model weight</li>
</ul>

<pre><code class="language-python">class ASRWithLanguageModel:
    """
    ASR with integrated language model
    """
    def __init__(self, acoustic_model, language_model, lm_weight: float = 1.0):
        self.acoustic_model = acoustic_model
        self.language_model = language_model
        self.lm_weight = lm_weight

    def recognize_with_lm(self, audio_features: np.ndarray,
                          previous_words: List[str] = None) -> str:
        """
        Recognition using language model
        """
        # Acoustic score (for each word candidate)
        acoustic_scores = {}
        for word in self.acoustic_model.models.keys():
            acoustic_scores[word] = self.acoustic_model.models[word].score(audio_features)

        # Language model score
        if previous_words:
            lm_scores = {}
            for word in acoustic_scores.keys():
                # Bigram probability
                prev_word = previous_words[-1] if previous_words else '<s>'
                lm_scores[word] = np.log(self.language_model.probability(prev_word, word))
        else:
            lm_scores = {word: 0 for word in acoustic_scores.keys()}

        # Combined score
        total_scores = {
            word: acoustic_scores[word] + self.lm_weight * lm_scores[word]
            for word in acoustic_scores.keys()
        }

        # Select best word
        best_word = max(total_scores, key=total_scores.get)

        return best_word

print("\n=== Language Model Integration ===")
print("By combining acoustic and language scores,")
print("recognition accuracy can be improved with context consideration")
</code></pre>

<hr>

<h2>2.6 Chapter Summary</h2>

<h3>What We Learned</h3>

<ol>
<li><p><strong>Fundamentals of Speech Recognition</strong></p>
<ul>
<li>ASR is a combination of acoustic and language models</li>
<li>Evaluation using WER (Word Error Rate)</li>
<li>Error calculation using Levenshtein distance</li>
</ul></li>

<li><p><strong>Hidden Markov Models</strong></p>
<ul>
<li>Modeling state transitions and output probabilities</li>
<li>Forward-Backward algorithm (Evaluation)</li>
<li>Viterbi algorithm (Decoding)</li>
<li>Baum-Welch algorithm (Learning)</li>
</ul></li>

<li><p><strong>Gaussian Mixture Models</strong></p>
<ul>
<li>Density estimation with multiple Gaussian distributions</li>
<li>Parameter estimation using EM algorithm</li>
<li>Acoustic modeling with GMM-HMM</li>
</ul></li>

<li><p><strong>Language Models</strong></p>
<ul>
<li>Probability modeling of word sequences with N-grams</li>
<li>Smoothing techniques (handling unseen events)</li>
<li>Evaluation using perplexity</li>
</ul></li>

<li><p><strong>ASR Pipeline</strong></p>
<ul>
<li>Feature extraction (MFCC + delta)</li>
<li>Acoustic model (GMM-HMM)</li>
<li>Decoding (Viterbi)</li>
<li>Language model integration</li>
</ul></li>
</ol>

<h3>Advantages and Disadvantages of Traditional ASR</h3>

<table>
<thead>
<tr>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
<td>Theoretically clear</td>
<td>Requires large amounts of labeled data</td>
</tr>
<tr>
<td>Independent components</td>
<td>Difficult to optimize entire pipeline</td>
</tr>
<tr>
<td>Interpretability at phoneme level</td>
<td>Weak modeling of long-term dependencies</td>
</tr>
<tr>
<td>Works with limited data</td>
<td>Dependent on feature engineering</td>
</tr>
</tbody>
</table>

<h3>Next Chapter</h3>

<p>In Chapter 3, we will learn about <strong>modern End-to-End speech recognition</strong>:</p>
<ul>
<li>Deep Speech (CTC)</li>
<li>Listen, Attend and Spell</li>
<li>Transformer-based ASR</li>
<li>Wav2Vec 2.0</li>
<li>Whisper</li>
</ul>

<hr>

<h2>Exercises</h2>

<h3>Problem 1 (Difficulty: easy)</h3>
<p>Calculate the WER by hand for the following reference and hypothesis sentences.</p>
<ul>
<li>Reference: "the cat sat on the mat"</li>
<li>Hypothesis: "the cat sit on mat"</li>
</ul>

<details>
<summary>Solution</summary>

<p><strong>Answer</strong>:</p>

<p>Align reference and hypothesis:</p>
<pre><code>Reference: the cat sat on the mat
Hypothesis: the cat sit on --- mat
</code></pre>

<p>Count errors:</p>
<ul>
<li>Substitutions (S): sat ‚Üí sit (1)</li>
<li>Deletions (D): the (1)</li>
<li>Insertions (I): 0</li>
</ul>

<p>Calculate WER:</p>
<p>$$
\text{WER} = \frac{S + D + I}{N} = \frac{1 + 1 + 0}{6} = \frac{2}{6} = 0.333 = 33.3\%
$$</p>

<p>Answer: <strong>33.3%</strong></p>

</details>

<h3>Problem 2 (Difficulty: medium)</h3>
<p>For a 3-state HMM with the following parameters, calculate the probability of the observation sequence [0, 1, 0] using the Forward algorithm.</p>

<pre><code class="language-python"># Initial probabilities
pi = [0.6, 0.3, 0.1]

# Transition probabilities
A = [[0.7, 0.2, 0.1],
     [0.3, 0.5, 0.2],
     [0.2, 0.3, 0.5]]

# Emission probabilities (observations 0 and 1)
B = [[0.8, 0.2],
     [0.4, 0.6],
     [0.3, 0.7]]
</code></pre>

<details>
<summary>Solution</summary>

<pre><code class="language-python">import numpy as np

# Parameters
pi = np.array([0.6, 0.3, 0.1])
A = np.array([[0.7, 0.2, 0.1],
              [0.3, 0.5, 0.2],
              [0.2, 0.3, 0.5]])
B = np.array([[0.8, 0.2],
              [0.4, 0.6],
              [0.3, 0.7]])

observations = [0, 1, 0]
T = len(observations)
N = len(pi)

# Forward variables
alpha = np.zeros((T, N))

# Initialization (t=0)
alpha[0] = pi * B[:, observations[0]]
print(f"t=0: Œ± = {alpha[0]}")

# Recursion (t=1, 2, ...)
for t in range(1, T):
    for j in range(N):
        alpha[t, j] = np.sum(alpha[t-1] * A[:, j]) * B[j, observations[t]]
    print(f"t={t}: Œ± = {alpha[t]}")

# Probability of observation sequence
prob = np.sum(alpha[T-1])

print(f"\nProbability of observation sequence {observations}:")
print(f"P(O|Œª) = {prob:.6f}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>t=0: Œ± = [0.48 0.12 0.03]
t=1: Œ± = [0.0672 0.1092 0.0588]
t=2: Œ± = [0.06048 0.02688 0.01512]

Probability of observation sequence [0, 1, 0]:
P(O|Œª) = 0.102480
</code></pre>

</details>

<h3>Problem 3 (Difficulty: medium)</h3>
<p>In a bigram language model, estimate P("learning"|"machine") using maximum likelihood estimation from the following corpus.</p>

<pre><code>I love machine learning
machine learning is fun
I study machine learning
deep learning is great
</code></pre>

<details>
<summary>Solution</summary>

<p><strong>Answer</strong>:</p>

<p>Counts:</p>
<ul>
<li>C(machine, learning) = 3</li>
<li>C(machine) = 3</li>
</ul>

<p>Maximum likelihood estimation:</p>
<p>$$
P(\text{learning} | \text{machine}) = \frac{C(\text{machine}, \text{learning})}{C(\text{machine})} = \frac{3}{3} = 1.0
$$</p>

<p>Answer: <strong>1.0 (100%)</strong></p>

<p>In this corpus, "machine" is always followed by "learning".</p>

</details>

<h3>Problem 4 (Difficulty: hard)</h3>
<p>Using a GMM with 2 components (K=2), cluster the following 1D data and find the parameters (mean, variance, weight) of each component.</p>

<pre><code class="language-python">data = np.array([1.2, 1.5, 1.8, 2.0, 2.1, 8.5, 9.0, 9.2, 9.5, 10.0])
</code></pre>

<details>
<summary>Solution</summary>

<pre><code class="language-python">import numpy as np
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt

data = np.array([1.2, 1.5, 1.8, 2.0, 2.1, 8.5, 9.0, 9.2, 9.5, 10.0])
X = data.reshape(-1, 1)

# GMM clustering
gmm = GaussianMixture(n_components=2, random_state=42)
gmm.fit(X)

# Parameters
print("=== GMM Parameters ===")
print(f"\nComponent 1:")
print(f"  Mean: {gmm.means_[0][0]:.3f}")
print(f"  Variance: {gmm.covariances_[0][0][0]:.3f}")
print(f"  Weight: {gmm.weights_[0]:.3f}")

print(f"\nComponent 2:")
print(f"  Mean: {gmm.means_[1][0]:.3f}")
print(f"  Variance: {gmm.covariances_[1][0][0]:.3f}")
print(f"  Weight: {gmm.weights_[1]:.3f}")

# Cluster labels
labels = gmm.predict(X)
print(f"\nCluster labels:")
for i, (val, label) in enumerate(zip(data, labels)):
    print(f"  Data{i+1}: {val:.1f} ‚Üí Cluster{label}")

# Visualization
plt.figure(figsize=(10, 6))
plt.scatter(data, np.zeros_like(data), c=labels, cmap='viridis',
            s=100, alpha=0.6, edgecolors='black')
plt.scatter(gmm.means_, [0, 0], c='red', s=200, marker='X',
            edgecolors='black', linewidths=2, label='Centers')
plt.xlabel('Value')
plt.title('1D Data Clustering with GMM', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

<p><strong>Example output</strong>:</p>
<pre><code>=== GMM Parameters ===

Component 1:
  Mean: 1.720
  Variance: 0.124
  Weight: 0.500

Component 2:
  Mean: 9.240
  Variance: 0.294
  Weight: 0.500

Cluster labels:
  Data1: 1.2 ‚Üí Cluster0
  Data2: 1.5 ‚Üí Cluster0
  Data3: 1.8 ‚Üí Cluster0
  Data4: 2.0 ‚Üí Cluster0
  Data5: 2.1 ‚Üí Cluster0
  Data6: 8.5 ‚Üí Cluster1
  Data7: 9.0 ‚Üí Cluster1
  Data8: 9.2 ‚Üí Cluster1
  Data9: 9.5 ‚Üí Cluster1
  Data10: 10.0 ‚Üí Cluster1
</code></pre>

</details>

<h3>Problem 5 (Difficulty: hard)</h3>
<p>In an ASR system that integrates acoustic and language models, explain the impact of varying the language model weight (LM weight) on recognition results. Also, describe how the optimal weight should be determined.</p>

<details>
<summary>Solution</summary>

<p><strong>Answer</strong>:</p>

<p><strong>Impact of LM weight</strong>:</p>

<p>Combined score:</p>
<p>$$
\text{Score}(W) = \log P(X|W) + \lambda \log P(W)
$$</p>

<table>
<thead>
<tr>
<th>LM weight $\lambda$</th>
<th>Impact</th>
<th>Recognition tendency</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Small (close to 0)</strong></td>
<td>Acoustic model priority</td>
<td>Selects acoustically similar words, grammatically unnatural</td>
</tr>
<tr>
<td><strong>Appropriate</strong></td>
<td>Balanced</td>
<td>Considers both acoustic and grammar, best recognition accuracy</td>
</tr>
<tr>
<td><strong>Large</strong></td>
<td>Language model priority</td>
<td>Grammatically correct but acoustically incorrect, biased toward frequent words</td>
</tr>
</tbody>
</table>

<p><strong>Methods for determining optimal weight</strong>:</p>

<ol>
<li><p><strong>Grid search on development set</strong></p>
<pre><code class="language-python">lambda_values = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
best_lambda = None
best_wer = float('inf')

for lam in lambda_values:
    wer = evaluate_asr(dev_set, lm_weight=lam)
    if wer < best_wer:
        best_wer = wer
        best_lambda = lam

print(f"Optimal LM weight: {best_lambda}")
</code></pre></li>

<li><p><strong>Considering domain dependency</strong></p>
<ul>
<li>Read speech: High acoustic quality ‚Üí smaller $\lambda$</li>
<li>Spontaneous speech: Noisy ‚Üí larger $\lambda$</li>
<li>Many technical terms: Low language model reliability ‚Üí smaller $\lambda$</li>
</ul></li>

<li><p><strong>Dynamic adjustment</strong></p>
<ul>
<li>Dynamically adjust based on confidence scores</li>
<li>Adjust according to acoustic quality (SNR)</li>
</ul></li>
</ol>

<p><strong>Example</strong>:</p>
<pre><code>Speech: "I scream" (ice cream)

Œª = 0.1 (acoustic priority):
  ‚Üí "I scream" (acoustically accurate)

Œª = 5.0 (language priority):
  ‚Üí "ice cream" (grammatically natural, high frequency in language model)

Œª = 1.0 (balanced):
  ‚Üí Appropriate selection depending on context
</code></pre>

<p><strong>Conclusion</strong>: The optimal LM weight depends on domain, acoustic conditions, and language model quality, and requires experimental tuning on a development set.</p>

</details>

<hr>

<h2>References</h2>

<ol>
<li>Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech recognition. <em>Proceedings of the IEEE</em>, 77(2), 257-286.</li>
<li>Bishop, C. M. (2006). <em>Pattern Recognition and Machine Learning</em>. Springer.</li>
<li>Jurafsky, D., & Martin, J. H. (2023). <em>Speech and Language Processing</em> (3rd ed.). Draft.</li>
<li>Gales, M., & Young, S. (2008). The application of hidden Markov models in speech recognition. <em>Foundations and Trends in Signal Processing</em>, 1(3), 195-304.</li>
<li>Heafield, K. (2011). KenLM: Faster and smaller language model queries. <em>Proceedings of the Sixth Workshop on Statistical Machine Translation</em>, 187-197.</li>
</ol>

<div class="navigation">
    <a href="chapter1-audio-basics.html" class="nav-button">‚Üê Previous Chapter: Audio Signal Fundamentals</a>
    <a href="chapter3-modern-asr.html" class="nav-button">Next Chapter: Modern Speech Recognition ‚Üí</a>
</div>

    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
            <li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are subject to the stated conditions (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
        </ul>
    </section>

<footer>
        <p><strong>Author</strong>: AI Terakoya Content Team</p>
        <p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-21</p>
        <p><strong>License</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
