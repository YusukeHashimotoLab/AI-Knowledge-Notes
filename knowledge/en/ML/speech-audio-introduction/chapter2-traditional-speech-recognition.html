<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2 Chapterï¼šä¼çµ±çš„éŸ³å£°èªè­˜ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/speech-audio-introduction/index.html">Speech Audio</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 2</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 2 Chapterï¼šä¼çµ±çš„éŸ³å£°èªè­˜</h1>
            <p class="subtitle">HMM-GMMæ™‚ä»£ã®éŸ³å£°èªè­˜æŠ€è¡“ - çµ±è¨ˆçš„ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– Reading Time: 35-40 minutes</span>
                <span class="meta-item">ğŸ“Š Difficulty: Intermediate</span>
                <span class="meta-item">ğŸ’» Code Examples: 8</span>
                <span class="meta-item">ğŸ“ Exercises: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Learning Objectives</h2>
<p>ã“ã® ChapterReadã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… éŸ³å£°èªè­˜ï¼ˆASRï¼‰ã‚¿ã‚¹ã‚¯ã®å®šç¾©ã¨è©•ä¾¡æŒ‡æ¨™ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Hidden Markov Modelï¼ˆHMMï¼‰ã®åŸç†ã¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Gaussian Mixture Modelï¼ˆGMMï¼‰ã«ã‚ˆã‚‹éŸ³éŸ¿ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆN-gramï¼‰ã®æ§‹ç¯‰ã¨è©•ä¾¡ãŒã§ãã‚‹</li>
<li>âœ… HMM-GMMãƒ™ãƒ¼ã‚¹ã®ASRãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
<li>âœ… WERï¼ˆWord Error Rateï¼‰ã‚’ç”¨ã„ã¦ã‚·ã‚¹ãƒ†ãƒ ã‚’è©•ä¾¡ã§ãã‚‹</li>
</ul>

<hr>

<h2>2.1 éŸ³å£°èªè­˜ã®åŸºç¤</h2>

<h3>ASRã‚¿ã‚¹ã‚¯ã®å®šç¾©</h3>
<p><strong>è‡ªå‹•éŸ³å£°èªè­˜ï¼ˆAutomatic Speech Recognition, ASRï¼‰</strong>ã¯ã€éŸ³å£°ä¿¡å·ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚</p>

<blockquote>
<p>éŸ³å£°èªè­˜ã®ç›®æ¨™ï¼šè¦³æ¸¬ã•ã‚ŒãŸéŸ³éŸ¿ä¿¡å· $X$ ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€æœ€ã‚‚ç¢ºã‹ã‚‰ã—ã„å˜èªåˆ— $W$ ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨</p>
</blockquote>

<p>ã“ã‚Œã¯ãƒ™ã‚¤ã‚ºã®å®šç†ã«ã‚ˆã‚Šä»¥ä¸‹ã®ã‚ˆã†ã«å®šå¼åŒ–ã•ã‚Œã¾ã™ï¼š</p>

<p>$$
\hat{W} = \arg\max_{W} P(W|X) = \arg\max_{W} \frac{P(X|W) P(W)}{P(X)} = \arg\max_{W} P(X|W) P(W)
$$</p>

<ul>
<li>$P(X|W)$: <strong>éŸ³éŸ¿ãƒ¢ãƒ‡ãƒ«ï¼ˆAcoustic Modelï¼‰</strong> - å˜èªåˆ—ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã®éŸ³éŸ¿ä¿¡å·ã®ç¢ºç‡</li>
<li>$P(W)$: <strong>è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLanguage Modelï¼‰</strong> - å˜èªåˆ—ã®äº‹å‰ç¢ºç‡</li>
</ul>

<h3>ASRã‚·ã‚¹ãƒ†ãƒ ã®æ§‹æˆè¦ç´ </h3>

<div class="mermaid">
graph LR
    A[éŸ³å£°ä¿¡å·] --> B[ç‰¹å¾´æŠ½å‡º<br/>MFCC]
    B --> C[éŸ³éŸ¿ãƒ¢ãƒ‡ãƒ«<br/>HMM-GMM]
    C --> D[ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°<br/>Viterbi]
    D --> E[è¨€èªãƒ¢ãƒ‡ãƒ«<br/>N-gram]
    E --> F[èªè­˜çµæœ<br/>ãƒ†ã‚­ã‚¹ãƒˆ]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#e3f2fd
    style D fill:#f3e5f5
    style E fill:#e8f5e9
    style F fill:#c8e6c9
</div>

<h3>è©•ä¾¡æŒ‡æ¨™</h3>

<h4>Word Error Rateï¼ˆWERï¼‰</h4>

<p><strong>WER</strong>ã¯éŸ³å£°èªè­˜ã®æ¨™æº–è©•ä¾¡æŒ‡æ¨™ã§ã™ï¼š</p>

<p>$$
\text{WER} = \frac{S + D + I}{N} \times 100\%
$$</p>

<ul>
<li>$S$: ç½®æ›ã‚¨ãƒ©ãƒ¼ï¼ˆSubstitutionsï¼‰</li>
<li>$D$: å‰Šé™¤ã‚¨ãƒ©ãƒ¼ï¼ˆDeletionsï¼‰</li>
<li>$I$: æŒ¿å…¥ã‚¨ãƒ©ãƒ¼ï¼ˆInsertionsï¼‰</li>
<li>$N$: å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆã®ç·å˜èªæ•°</li>
</ul>

<h4>Character Error Rateï¼ˆCERï¼‰</h4>

<p>æ—¥æœ¬èªã‚„ä¸­å›½èªãªã©ã®è¨€èªã§ã¯ã€æ–‡å­—Levelã®CERã‚‚ä½¿ç”¨ã•ã‚Œã¾ã™ï¼š</p>

<p>$$
\text{CER} = \frac{S_c + D_c + I_c}{N_c} \times 100\%
$$</p>

<h3>å®Ÿè£…ï¼šWERè¨ˆç®—</h3>

<pre><code class="language-python">import numpy as np
from typing import List, Tuple

def levenshtein_distance(ref: List[str], hyp: List[str]) -> Tuple[int, int, int, int]:
    """
    ãƒ¬ãƒ¼ãƒ™ãƒ³ã‚·ãƒ¥ã‚¿ã‚¤ãƒ³è·é›¢ã¨ã‚¨ãƒ©ãƒ¼çµ±è¨ˆã‚’è¨ˆç®—

    Returns:
        (distance, substitutions, deletions, insertions)
    """
    m, n = len(ref), len(hyp)

    # DP ãƒ†ãƒ¼ãƒ–ãƒ«
    dp = np.zeros((m + 1, n + 1), dtype=int)

    # åˆæœŸåŒ–
    for i in range(m + 1):
        dp[i][0] = i
    for j in range(n + 1):
        dp[0][j] = j

    # DP
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if ref[i-1] == hyp[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = min(
                    dp[i-1][j-1] + 1,  # ç½®æ›
                    dp[i-1][j] + 1,    # å‰Šé™¤
                    dp[i][j-1] + 1     # æŒ¿å…¥
                )

    # ãƒãƒƒã‚¯ãƒˆãƒ©ãƒƒã‚¯: ã‚¨ãƒ©ãƒ¼çµ±è¨ˆã‚’è¨ˆç®—
    i, j = m, n
    subs, dels, ins = 0, 0, 0

    while i > 0 or j > 0:
        if i > 0 and j > 0 and ref[i-1] == hyp[j-1]:
            i -= 1
            j -= 1
        elif i > 0 and j > 0 and dp[i][j] == dp[i-1][j-1] + 1:
            subs += 1
            i -= 1
            j -= 1
        elif i > 0 and dp[i][j] == dp[i-1][j] + 1:
            dels += 1
            i -= 1
        else:
            ins += 1
            j -= 1

    return dp[m][n], subs, dels, ins


def calculate_wer(reference: str, hypothesis: str) -> dict:
    """
    WERï¼ˆWord Error Rateï¼‰ã‚’è¨ˆç®—

    Args:
        reference: æ­£è§£ãƒ†ã‚­ã‚¹ãƒˆ
        hypothesis: èªè­˜çµæœ

    Returns:
        ã‚¨ãƒ©ãƒ¼çµ±è¨ˆã‚’å«ã‚€è¾æ›¸
    """
    ref_words = reference.split()
    hyp_words = hypothesis.split()

    dist, subs, dels, ins = levenshtein_distance(ref_words, hyp_words)

    n_words = len(ref_words)
    wer = (dist / n_words * 100) if n_words > 0 else 0

    return {
        'WER': wer,
        'substitutions': subs,
        'deletions': dels,
        'insertions': ins,
        'total_errors': dist,
        'total_words': n_words
    }


# ãƒ†ã‚¹ãƒˆ
reference = "the quick brown fox jumps over the lazy dog"
hypothesis = "the quick brown fox jumped over a lazy dog"

result = calculate_wer(reference, hypothesis)

print("=== WER è¨ˆç®—ä¾‹ ===")
print(f"å‚ç…§: {reference}")
print(f"ä»®èª¬: {hypothesis}")
print(f"\nWER: {result['WER']:.2f}%")
print(f"ç½®æ›: {result['substitutions']}")
print(f"å‰Šé™¤: {result['deletions']}")
print(f"æŒ¿å…¥: {result['insertions']}")
print(f"ç·ã‚¨ãƒ©ãƒ¼: {result['total_errors']}")
print(f"ç·å˜èªæ•°: {result['total_words']}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== WER è¨ˆç®—ä¾‹ ===
å‚ç…§: the quick brown fox jumps over the lazy dog
ä»®èª¬: the quick brown fox jumped over a lazy dog

WER: 22.22%
ç½®æ›: 2
å‰Šé™¤: 0
æŒ¿å…¥: 0
ç·ã‚¨ãƒ©ãƒ¼: 2
ç·å˜èªæ•°: 9
</code></pre>

<blockquote>
<p><strong>é‡è¦</strong>: WERã¯100%ã‚’è¶…ãˆã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ï¼ˆæŒ¿å…¥ã‚¨ãƒ©ãƒ¼ãŒå¤šã„å ´åˆï¼‰ã€‚</p>
</blockquote>

<hr>

<h2>2.2 Hidden Markov Modelsï¼ˆHMMï¼‰</h2>

<h3>HMMã®åŸºç¤</h3>

<p><strong>éš ã‚Œãƒãƒ«ã‚³ãƒ•ãƒ¢ãƒ‡ãƒ«ï¼ˆHMMï¼‰</strong>ã¯ã€è¦³æ¸¬ã§ããªã„éš ã‚ŒçŠ¶æ…‹ã¨è¦³æ¸¬å¯èƒ½ãªå‡ºåŠ›ã‹ã‚‰ãªã‚‹ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</p>

<h4>HMMã®æ§‹æˆè¦ç´ </h4>

<ul>
<li>$N$: çŠ¶æ…‹æ•°</li>
<li>$M$: è¦³æ¸¬ã‚·ãƒ³ãƒœãƒ«æ•°</li>
<li>$A = \{a_{ij}\}$: çŠ¶æ…‹é·ç§»ç¢ºç‡è¡Œåˆ— $a_{ij} = P(q_{t+1}=j | q_t=i)$</li>
<li>$B = \{b_j(k)\}$: å‡ºåŠ›ç¢ºç‡ minuteså¸ƒ $b_j(k) = P(o_t=k | q_t=j)$</li>
<li>$\pi = \{\pi_i\}$: åˆæœŸçŠ¶æ…‹ç¢ºç‡ $\pi_i = P(q_1=i)$</li>
</ul>

<h4>HMMã®3ã¤ã®åŸºæœ¬å•é¡Œ</h4>

<table>
<thead>
<tr>
<th>å•é¡Œ</th>
<th>èª¬æ˜</th>
<th>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>è©•ä¾¡å•é¡Œ</strong></td>
<td>è¦³æ¸¬åˆ—ã®ç¢ºç‡ã‚’è¨ˆç®—</td>
<td>Forward-Backward</td>
</tr>
<tr>
<td><strong>ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°</strong></td>
<td>æœ€ã‚‚ç¢ºã‹ã‚‰ã—ã„çŠ¶æ…‹åˆ—ã‚’æ¨å®š</td>
<td>Viterbi</td>
</tr>
<tr>
<td><strong>å­¦ç¿’å•é¡Œ</strong></td>
<td>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®š</td>
<td>Baum-Welchï¼ˆEMï¼‰</td>
</tr>
</tbody>
</table>

<h3>Forward-Backwardã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h3>

<p><strong>Forwardã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </strong>ã¯è¦³æ¸¬åˆ—ã®ç¢ºç‡ $P(O|\lambda)$ ã‚’è¨ˆç®—ã—ã¾ã™ï¼š</p>

<p>$$
\alpha_t(i) = P(o_1, o_2, \ldots, o_t, q_t=i | \lambda)
$$</p>

<p>å†å¸°å¼ï¼š</p>

<p>$$
\alpha_t(j) = \left[\sum_{i=1}^N \alpha_{t-1}(i) a_{ij}\right] b_j(o_t)
$$</p>

<h3>Viterbiã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h3>

<p><strong>Viterbiã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </strong>ã¯æœ€ã‚‚ç¢ºã‹ã‚‰ã—ã„çŠ¶æ…‹åˆ—ã‚’è¦‹ã¤ã‘ã¾ã™ï¼š</p>

<p>$$
\delta_t(i) = \max_{q_1, \ldots, q_{t-1}} P(q_1, \ldots, q_{t-1}, q_t=i, o_1, \ldots, o_t | \lambda)
$$</p>

<p>å†å¸°å¼ï¼š</p>

<p>$$
\delta_t(j) = \left[\max_i \delta_{t-1}(i) a_{ij}\right] b_j(o_t)
$$</p>

<h3>å®Ÿè£…ï¼šHMMã®åŸºæœ¬æ“ä½œ</h3>

<pre><code class="language-python">import numpy as np
from hmmlearn import hmm
import matplotlib.pyplot as plt

# ã‚µãƒ³ãƒ—ãƒ«ï¼šå¤©æ°—ãƒ¢ãƒ‡ãƒ«
# çŠ¶æ…‹: 0=æ™´ã‚Œ, 1=é›¨
# è¦³æ¸¬: 0=æ•£æ­©, 1=è²·ã„ç‰©, 2=æƒé™¤

# HMMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å®šç¾©
n_states = 2
n_observations = 3

# ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
model = hmm.MultinomialHMM(n_components=n_states, random_state=42)

# çŠ¶æ…‹é·ç§»ç¢ºç‡
model.startprob_ = np.array([0.6, 0.4])  # åˆæœŸç¢ºç‡
model.transmat_ = np.array([
    [0.7, 0.3],  # æ™´ã‚Œ â†’ [æ™´ã‚Œ, é›¨]
    [0.4, 0.6]   # é›¨ â†’ [æ™´ã‚Œ, é›¨]
])

# å‡ºåŠ›ç¢ºç‡
model.emissionprob_ = np.array([
    [0.6, 0.3, 0.1],  # æ™´ã‚Œã®æ—¥: [æ•£æ­©, è²·ã„ç‰©, æƒé™¤]
    [0.1, 0.4, 0.5]   # é›¨ã®æ—¥: [æ•£æ­©, è²·ã„ç‰©, æƒé™¤]
])

print("=== HMMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===")
print("\nåˆæœŸçŠ¶æ…‹ç¢ºç‡:")
print(f"  æ™´ã‚Œ: {model.startprob_[0]:.2f}")
print(f"  é›¨: {model.startprob_[1]:.2f}")

print("\nçŠ¶æ…‹é·ç§»ç¢ºç‡:")
print(model.transmat_)

print("\nå‡ºåŠ›ç¢ºç‡:")
print("       æ•£æ­©   è²·ã„ç‰©  æƒé™¤")
print(f"æ™´ã‚Œ: {model.emissionprob_[0]}")
print(f"é›¨:   {model.emissionprob_[1]}")

# è¦³æ¸¬åˆ—
observations = np.array([[0], [1], [2], [1], [0]])  # æ•£æ­©, è²·ã„ç‰©, æƒé™¤, è²·ã„ç‰©, æ•£æ­©

# Forward ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : è¦³æ¸¬åˆ—ã®ç¢ºç‡
log_prob = model.score(observations)
print(f"\nè¦³æ¸¬åˆ—ã®å¯¾æ•°å°¤åº¦: {log_prob:.4f}")
print(f"è¦³æ¸¬åˆ—ã®ç¢ºç‡: {np.exp(log_prob):.6f}")

# Viterbi ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : æœ€ã‚‚ç¢ºã‹ã‚‰ã—ã„çŠ¶æ…‹åˆ—
log_prob, states = model.decode(observations)
state_names = ['æ™´ã‚Œ', 'é›¨']
print(f"\næœ€ã‚‚ç¢ºã‹ã‚‰ã—ã„çŠ¶æ…‹åˆ—:")
for i, (obs, state) in enumerate(zip(observations.flatten(), states)):
    obs_names = ['æ•£æ­©', 'è²·ã„ç‰©', 'æƒé™¤']
    print(f"  æ—¥{i+1}: è¦³æ¸¬={obs_names[obs]}, çŠ¶æ…‹={state_names[state]}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== HMMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===

åˆæœŸçŠ¶æ…‹ç¢ºç‡:
  æ™´ã‚Œ: 0.60
  é›¨: 0.40

çŠ¶æ…‹é·ç§»ç¢ºç‡:
[[0.7 0.3]
 [0.4 0.6]]

å‡ºåŠ›ç¢ºç‡:
       æ•£æ­©   è²·ã„ç‰©  æƒé™¤
æ™´ã‚Œ: [0.6 0.3 0.1]
é›¨:   [0.1 0.4 0.5]

è¦³æ¸¬åˆ—ã®å¯¾æ•°å°¤åº¦: -6.3218
è¦³æ¸¬åˆ—ã®ç¢ºç‡: 0.001802

æœ€ã‚‚ç¢ºã‹ã‚‰ã—ã„çŠ¶æ…‹åˆ—:
  æ—¥1: è¦³æ¸¬=æ•£æ­©, çŠ¶æ…‹=æ™´ã‚Œ
  æ—¥2: è¦³æ¸¬=è²·ã„ç‰©, çŠ¶æ…‹=æ™´ã‚Œ
  æ—¥3: è¦³æ¸¬=æƒé™¤, çŠ¶æ…‹=é›¨
  æ—¥4: è¦³æ¸¬=è²·ã„ç‰©, çŠ¶æ…‹=é›¨
  æ—¥5: è¦³æ¸¬=æ•£æ­©, çŠ¶æ…‹=é›¨
</code></pre>

<h3>HMMã«ã‚ˆã‚‹éŸ³ç´ ãƒ¢ãƒ‡ãƒªãƒ³ã‚°</h3>

<p>éŸ³å£°èªè­˜ã§ã¯ã€å„éŸ³ç´ ã‚’3çŠ¶æ…‹ã®Left-to-Right HMMã§ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¾ã™ï¼š</p>

<div class="mermaid">
graph LR
    Start((é–‹å§‹)) --> S1[çŠ¶æ…‹1<br/>éŸ³ç´ é–‹å§‹]
    S1 --> S1
    S1 --> S2[çŠ¶æ…‹2<br/>éŸ³ç´ ä¸­é–“]
    S2 --> S2
    S2 --> S3[çŠ¶æ…‹3<br/>éŸ³ç´ çµ‚äº†]
    S3 --> S3
    S3 --> End((çµ‚äº†))

    style Start fill:#c8e6c9
    style S1 fill:#e3f2fd
    style S2 fill:#fff3e0
    style S3 fill:#f3e5f5
    style End fill:#ffcdd2
</div>

<pre><code class="language-python"># Left-to-Right HMMï¼ˆéŸ³ç´ ãƒ¢ãƒ‡ãƒ«ï¼‰
n_states = 3

# Left-to-Rightæ§‹é€ ã®é·ç§»è¡Œåˆ—
# çŠ¶æ…‹ã¯å‰é€²ã¾ãŸã¯è‡ªå·±ãƒ«ãƒ¼ãƒ—ã®ã¿
lr_model = hmm.GaussianHMM(n_components=n_states, covariance_type="diag", random_state=42)

# é·ç§»ç¢ºç‡ï¼ˆå·¦ã‹ã‚‰å³ã®ã¿ï¼‰
lr_model.transmat_ = np.array([
    [0.5, 0.5, 0.0],  # çŠ¶æ…‹1: è‡ªå·±ãƒ«ãƒ¼ãƒ—ã¾ãŸã¯çŠ¶æ…‹2ã¸
    [0.0, 0.5, 0.5],  # çŠ¶æ…‹2: è‡ªå·±ãƒ«ãƒ¼ãƒ—ã¾ãŸã¯çŠ¶æ…‹3ã¸
    [0.0, 0.0, 1.0]   # çŠ¶æ…‹3: è‡ªå·±ãƒ«ãƒ¼ãƒ—ã®ã¿
])

lr_model.startprob_ = np.array([1.0, 0.0, 0.0])  # å¸¸ã«çŠ¶æ…‹1ã‹ã‚‰é–‹å§‹

print("=== Left-to-Right HMM ===")
print("é·ç§»ç¢ºç‡è¡Œåˆ—:")
print(lr_model.transmat_)
print("\nçŠ¶æ…‹1ã‹ã‚‰å§‹ã¾ã‚Šã€å·¦ã‹ã‚‰å³ã¸ã®ã¿é€²ã‚€ã“ã¨ãŒã§ãã¾ã™")
</code></pre>

<hr>

<h2>2.3 Gaussian Mixture Modelsï¼ˆGMMï¼‰</h2>

<h3>GMMã®åŸºç¤</h3>

<p><strong>ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«ï¼ˆGMMï¼‰</strong>ã¯è¤‡æ•°ã®ã‚¬ã‚¦ã‚¹ minuteså¸ƒã®ç·šå½¢çµåˆã§ç¢ºç‡ minuteså¸ƒã‚’è¡¨ç¾ã—ã¾ã™ï¼š</p>

<p>$$
p(x) = \sum_{k=1}^K w_k \mathcal{N}(x | \mu_k, \Sigma_k)
$$</p>

<ul>
<li>$K$: æ··åˆæˆ minutesæ•°</li>
<li>$w_k$: æ··åˆé‡ã¿ï¼ˆ$\sum_k w_k = 1$ï¼‰</li>
<li>$\mu_k$: å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«</li>
<li>$\Sigma_k$: å…± distributedè¡Œåˆ—</li>
</ul>

<h3>EMã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹å­¦ç¿’</h3>

<p>GMMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯<strong>EMï¼ˆExpectation-Maximizationï¼‰ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </strong>ã§æ¨å®šã—ã¾ã™ï¼š</p>

<h4>Eã‚¹ãƒ†ãƒƒãƒ—ï¼šè²¬ä»»åº¦ã®è¨ˆç®—</h4>

<p>$$
\gamma_{nk} = \frac{w_k \mathcal{N}(x_n | \mu_k, \Sigma_k)}{\sum_{j=1}^K w_j \mathcal{N}(x_n | \mu_j, \Sigma_j)}
$$</p>

<h4>Mã‚¹ãƒ†ãƒƒãƒ—ï¼šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´æ–°</h4>

<p>$$
\mu_k^{\text{new}} = \frac{1}{N_k} \sum_{n=1}^N \gamma_{nk} x_n
$$</p>

<p>$$
\Sigma_k^{\text{new}} = \frac{1}{N_k} \sum_{n=1}^N \gamma_{nk} (x_n - \mu_k^{\text{new}})(x_n - \mu_k^{\text{new}})^T
$$</p>

<p>$$
w_k^{\text{new}} = \frac{N_k}{N}, \quad N_k = \sum_{n=1}^N \gamma_{nk}
$$</p>

<h3>å®Ÿè£…ï¼šGMMã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°</h3>

<pre><code class="language-python">import numpy as np
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt

# 3ã¤ã®ã‚¬ã‚¦ã‚¹ minuteså¸ƒã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
np.random.seed(42)

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
n_samples = 300
X1 = np.random.randn(n_samples // 3, 2) * 0.5 + np.array([0, 0])
X2 = np.random.randn(n_samples // 3, 2) * 0.7 + np.array([3, 3])
X3 = np.random.randn(n_samples // 3, 2) * 0.6 + np.array([0, 3])

X = np.vstack([X1, X2, X3])

# GMMã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
n_components = 3
gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=42)
gmm.fit(X)

# äºˆæ¸¬
labels = gmm.predict(X)
proba = gmm.predict_proba(X)

print("=== GMM ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===")
print(f"æ··åˆæˆ minutesæ•°: {n_components}")
print(f"åæŸåå¾©æ•°: {gmm.n_iter_}")
print(f"å¯¾æ•°å°¤åº¦: {gmm.score(X) * len(X):.2f}")

print("\næ··åˆé‡ã¿:")
for i, weight in enumerate(gmm.weights_):
    print(f"  æˆ minutes{i+1}: {weight:.3f}")

print("\nå¹³å‡ãƒ™ã‚¯ãƒˆãƒ«:")
for i, mean in enumerate(gmm.means_):
    print(f"  æˆ minutes{i+1}: [{mean[0]:.2f}, {mean[1]:.2f}]")

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ
axes[0].scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.6, edgecolors='black')
axes[0].scatter(gmm.means_[:, 0], gmm.means_[:, 1], c='red', s=200, marker='X',
                edgecolors='black', linewidths=2, label='ä¸­å¿ƒ')
axes[0].set_xlabel('ç‰¹å¾´é‡ 1')
axes[0].set_ylabel('ç‰¹å¾´é‡ 2')
axes[0].set_title('GMMã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# ç¢ºç‡å¯†åº¦ã®å¯è¦–åŒ–
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                     np.linspace(y_min, y_max, 100))
Z = -gmm.score_samples(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

axes[1].contourf(xx, yy, Z, levels=20, cmap='viridis', alpha=0.6)
axes[1].scatter(X[:, 0], X[:, 1], c='white', s=10, alpha=0.5, edgecolors='black')
axes[1].scatter(gmm.means_[:, 0], gmm.means_[:, 1], c='red', s=200, marker='X',
                edgecolors='black', linewidths=2)
axes[1].set_xlabel('ç‰¹å¾´é‡ 1')
axes[1].set_ylabel('ç‰¹å¾´é‡ 2')
axes[1].set_title('GMMç¢ºç‡å¯†åº¦', fontsize=14)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3>GMM-HMMã‚·ã‚¹ãƒ†ãƒ </h3>

<p>ä¼çµ±çš„ASRã§ã¯ã€HMMã®å„çŠ¶æ…‹ã®å‡ºåŠ›ç¢ºç‡ã‚’GMMã§ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¾ã™ï¼š</p>

<p>$$
b_j(o_t) = \sum_{m=1}^M c_{jm} \mathcal{N}(o_t | \mu_{jm}, \Sigma_{jm})
$$</p>

<ul>
<li>$j$: HMMçŠ¶æ…‹</li>
<li>$m$: GMMæˆ minutes</li>
<li>$c_{jm}$: çŠ¶æ…‹ $j$ ã«ãŠã‘ã‚‹æˆ minutes $m$ ã®é‡ã¿</li>
</ul>

<pre><code class="language-python">from hmmlearn import hmm

# GMMã‚’å‡ºåŠ› minuteså¸ƒã«æŒã¤HMM
n_states = 3
n_mix = 4  # å„çŠ¶æ…‹ã®GMMæˆ minutesæ•°

# GaussianHMM: å„çŠ¶æ…‹ãŒã‚¬ã‚¦ã‚¹ minuteså¸ƒã‚’æŒã¤
# n_mix > 1 ã§å„çŠ¶æ…‹ãŒGMMã«ãªã‚‹
gmm_hmm = hmm.GMMHMM(n_components=n_states, n_mix=n_mix,
                      covariance_type='diag', n_iter=100, random_state=42)

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆ2æ¬¡å…ƒç‰¹å¾´é‡ï¼‰
np.random.seed(42)
n_samples = 200
train_data = np.random.randn(n_samples, 2) * 0.5

# ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
gmm_hmm.fit(train_data)

print("\n=== GMM-HMM ã‚·ã‚¹ãƒ†ãƒ  ===")
print(f"HMMçŠ¶æ…‹æ•°: {n_states}")
print(f"å„çŠ¶æ…‹ã®GMMæˆ minutesæ•°: {n_mix}")
print(f"åæŸåå¾©æ•°: {gmm_hmm.monitor_.iter}")
print(f"å¯¾æ•°å°¤åº¦: {gmm_hmm.score(train_data) * len(train_data):.2f}")

# ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
test_data = np.random.randn(10, 2) * 0.5
log_prob, states = gmm_hmm.decode(test_data)

print(f"\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®çŠ¶æ…‹åˆ—:")
print(f"  {states}")
print(f"  å¯¾æ•°ç¢ºç‡: {log_prob:.4f}")
</code></pre>

<hr>

<h2>2.4 è¨€èªãƒ¢ãƒ‡ãƒ«</h2>

<h3>N-gramãƒ¢ãƒ‡ãƒ«</h3>

<p><strong>N-gramãƒ¢ãƒ‡ãƒ«</strong>ã¯ã€å˜èªåˆ—ã®ç¢ºç‡ã‚’éå» $n-1$ å˜èªã‹ã‚‰äºˆæ¸¬ã—ã¾ã™ï¼š</p>

<p>$$
P(w_1, w_2, \ldots, w_n) \approx \prod_{i=1}^n P(w_i | w_{i-n+1}, \ldots, w_{i-1})
$$</p>

<h4>ä¸»ãªN-gramãƒ¢ãƒ‡ãƒ«</h4>

<table>
<thead>
<tr>
<th>ãƒ¢ãƒ‡ãƒ«</th>
<th>å®šç¾©</th>
<th>ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Unigram</strong></td>
<td>$P(w_i)$</td>
<td>å˜èªã®ç‹¬ç«‹ç¢ºç‡</td>
</tr>
<tr>
<td><strong>Bigram</strong></td>
<td>$P(w_i | w_{i-1})$</td>
<td>ç›´å‰ã®å˜èªã«ä¾å­˜</td>
</tr>
<tr>
<td><strong>Trigram</strong></td>
<td>$P(w_i | w_{i-2}, w_{i-1})$</td>
<td>ç›´å‰2å˜èªã«ä¾å­˜</td>
</tr>
</tbody>
</table>

<h3>æœ€å°¤æ¨å®š</h3>

<p>N-gramã®ç¢ºç‡ã¯è¨“ç·´ã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰ã®ã‚«ã‚¦ãƒ³ãƒˆã§æ¨å®šã—ã¾ã™ï¼š</p>

<p>$$
P(w_i | w_{i-1}) = \frac{C(w_{i-1}, w_i)}{C(w_{i-1})}
$$</p>

<ul>
<li>$C(w_{i-1}, w_i)$: ãƒã‚¤ã‚°ãƒ©ãƒ  $(w_{i-1}, w_i)$ ã®å‡ºç¾å›æ•°</li>
<li>$C(w_{i-1})$: å˜èª $w_{i-1}$ ã®å‡ºç¾å›æ•°</li>
</ul>

<h3>ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ï¼ˆPerplexityï¼‰</h3>

<p><strong>ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£</strong>ã¯è¨€èªãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡æŒ‡æ¨™ã§ã™ï¼š</p>

<p>$$
\text{PPL} = P(w_1, \ldots, w_N)^{-1/N} = \sqrt[N]{\prod_{i=1}^N \frac{1}{P(w_i | w_1, \ldots, w_{i-1})}}
$$</p>

<p>ä½ã„ã»ã©è‰¯ã„ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</p>

<h3>ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°æŠ€è¡“</h3>

<p>æœªè¦³æ¸¬ã®N-gramã«ç¢ºç‡ã‚’å‰²ã‚Šå½“ã¦ã‚‹ãŸã‚ã®æŠ€è¡“ï¼š</p>

<h4>1. Add-kã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ï¼ˆLaplaceï¼‰</h4>

<p>$$
P(w_i | w_{i-1}) = \frac{C(w_{i-1}, w_i) + k}{C(w_{i-1}) + k|V|}
$$</p>

<h4>2. Kneser-Neyã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°</h4>

<p>æ–‡è„ˆã®å¤šæ§˜æ€§ã‚’è€ƒæ…®ï¼š</p>

<p>$$
P_{\text{KN}}(w_i | w_{i-1}) = \frac{\max(C(w_{i-1}, w_i) - \delta, 0)}{C(w_{i-1})} + \lambda(w_{i-1}) P_{\text{continuation}}(w_i)
$$</p>

<h3>å®Ÿè£…ï¼šN-gramè¨€èªãƒ¢ãƒ‡ãƒ«</h3>

<pre><code class="language-python">import numpy as np
from collections import defaultdict, Counter
from typing import List, Tuple

class BigramLanguageModel:
    """
    ãƒã‚¤ã‚°ãƒ©ãƒ è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆAdd-kã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ä»˜ãï¼‰
    """
    def __init__(self, k: float = 1.0):
        self.k = k
        self.unigram_counts = Counter()
        self.bigram_counts = defaultdict(Counter)
        self.vocab = set()

    def train(self, corpus: List[List[str]]):
        """
        ã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’

        Args:
            corpus: æ–‡ã®ãƒªã‚¹ãƒˆï¼ˆå„æ–‡ã¯å˜èªã®ãƒªã‚¹ãƒˆï¼‰
        """
        for sentence in corpus:
            # é–‹å§‹ãƒ»çµ‚äº†ã‚¿ã‚°ã‚’è¿½åŠ 
            words = ['<s>'] + sentence + ['</s>']

            for word in words:
                self.vocab.add(word)
                self.unigram_counts[word] += 1

            for w1, w2 in zip(words[:-1], words[1:]):
                self.bigram_counts[w1][w2] += 1

        print(f"èªå½™ã‚µã‚¤ã‚º: {len(self.vocab)}")
        print(f"ç·å˜èªæ•°: {sum(self.unigram_counts.values())}")
        print(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒã‚¤ã‚°ãƒ©ãƒ æ•°: {sum(len(counts) for counts in self.bigram_counts.values())}")

    def probability(self, w1: str, w2: str) -> float:
        """
        ãƒã‚¤ã‚°ãƒ©ãƒ ç¢ºç‡ P(w2|w1) ã‚’è¨ˆç®—ï¼ˆAdd-kã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ï¼‰
        """
        numerator = self.bigram_counts[w1][w2] + self.k
        denominator = self.unigram_counts[w1] + self.k * len(self.vocab)
        return numerator / denominator

    def sentence_probability(self, sentence: List[str]) -> float:
        """
        æ–‡ã®ç¢ºç‡ã‚’è¨ˆç®—
        """
        words = ['<s>'] + sentence + ['</s>']
        prob = 1.0

        for w1, w2 in zip(words[:-1], words[1:]):
            prob *= self.probability(w1, w2)

        return prob

    def perplexity(self, test_corpus: List[List[str]]) -> float:
        """
        ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‘ã‚¹ã®ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã‚’è¨ˆç®—
        """
        log_prob = 0
        n_words = 0

        for sentence in test_corpus:
            words = ['<s>'] + sentence + ['</s>']
            n_words += len(words) - 1

            for w1, w2 in zip(words[:-1], words[1:]):
                prob = self.probability(w1, w2)
                log_prob += np.log2(prob)

        return 2 ** (-log_prob / n_words)


# ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‘ã‚¹
train_corpus = [
    ['I', 'love', 'machine', 'learning'],
    ['machine', 'learning', 'is', 'fun'],
    ['I', 'love', 'deep', 'learning'],
    ['deep', 'learning', 'is', 'powerful'],
    ['I', 'study', 'machine', 'learning'],
]

test_corpus = [
    ['I', 'love', 'learning'],
    ['machine', 'learning', 'is', 'interesting']
]

# ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
print("=== ãƒã‚¤ã‚°ãƒ©ãƒ è¨€èªãƒ¢ãƒ‡ãƒ« ===\n")
lm = BigramLanguageModel(k=0.1)
lm.train(train_corpus)

# ç¢ºç‡ã®è¨ˆç®—
print("\nãƒã‚¤ã‚°ãƒ©ãƒ ç¢ºç‡ã®ä¾‹:")
bigrams = [('I', 'love'), ('love', 'learning'), ('machine', 'learning'), ('learning', 'is')]
for w1, w2 in bigrams:
    prob = lm.probability(w1, w2)
    print(f"  P({w2}|{w1}) = {prob:.4f}")

# æ–‡ã®ç¢ºç‡
print("\næ–‡ã®ç¢ºç‡:")
for sentence in test_corpus:
    prob = lm.sentence_probability(sentence)
    print(f"  '{' '.join(sentence)}': {prob:.6e}")

# ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£
ppl = lm.perplexity(test_corpus)
print(f"\nãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‘ã‚¹ã®ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£: {ppl:.2f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ãƒã‚¤ã‚°ãƒ©ãƒ è¨€èªãƒ¢ãƒ‡ãƒ« ===

èªå½™ã‚µã‚¤ã‚º: 12
ç·å˜èªæ•°: 35
ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒã‚¤ã‚°ãƒ©ãƒ æ•°: 25

ãƒã‚¤ã‚°ãƒ©ãƒ ç¢ºç‡ã®ä¾‹:
  P(love|I) = 0.4255
  P(learning|love) = 0.3571
  P(learning|machine) = 0.6667
  P(is|learning) = 0.5000

æ–‡ã®ç¢ºç‡:
  'I love learning': 2.547618e-03
  'machine learning is interesting': 1.984127e-04

ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‘ã‚¹ã®ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£: 8.91
</code></pre>

<h3>KenLMãƒ©ã‚¤ãƒ–ãƒ©ãƒª</h3>

<p>å®Ÿç”¨çš„ãªN-gramè¨€èªãƒ¢ãƒ‡ãƒ«ã«ã¯<strong>KenLM</strong>ã‚’ä½¿ç”¨ã—ã¾ã™ï¼š</p>

<pre><code class="language-python"># KenLM ã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªè¨€èªãƒ¢ãƒ‡ãƒ«
# æ³¨: äº‹å‰ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦: pip install https://github.com/kpu/kenlm/archive/master.zip

import kenlm

# ARPAãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
# model = kenlm.Model('path/to/model.arpa')

# æ–‡ã®ã‚¹ã‚³ã‚¢è¨ˆç®—
# score = model.score('this is a test sentence', bos=True, eos=True)
# perplexity = model.perplexity('this is a test sentence')

print("KenLMã¯åŠ¹ç‡çš„ãªN-gramè¨€èªãƒ¢ãƒ‡ãƒ«å®Ÿè£…ã§ã™")
print("å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹ã®å­¦ç¿’ã¨ã‚¯ã‚¨ãƒªã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™")
</code></pre>

<hr>

<h2>2.5 ä¼çµ±çš„ASRãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h2>

<h3>å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹æˆ</h3>

<div class="mermaid">
graph TD
    A[éŸ³å£°ä¿¡å·<br/>Waveform] --> B[å‰å‡¦ç†<br/>Pre-emphasis]
    B --> C[ãƒ•ãƒ¬ãƒ¼ãƒ åŒ–<br/>Framing]
    C --> D[çª“æ›ã‘<br/>Windowing]
    D --> E[MFCCæŠ½å‡º<br/>Feature Extraction]
    E --> F[ãƒ‡ãƒ«ã‚¿ç‰¹å¾´<br/>Delta/Delta-Delta]
    F --> G[éŸ³éŸ¿ãƒ¢ãƒ‡ãƒ«<br/>GMM-HMM]
    G --> H[Viterbiãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°<br/>+ è¨€èªãƒ¢ãƒ‡ãƒ«]
    H --> I[èªè­˜çµæœ<br/>Text]

    style A fill:#ffebee
    style E fill:#e3f2fd
    style G fill:#fff3e0
    style H fill:#f3e5f5
    style I fill:#c8e6c9
</div>

<h3>å®Ÿè£…ï¼šç°¡æ˜“ASRã‚·ã‚¹ãƒ†ãƒ </h3>

<pre><code class="language-python">import numpy as np
import librosa
from hmmlearn import hmm
from sklearn.mixture import GaussianMixture
from typing import List, Tuple

class SimpleASR:
    """
    ç°¡æ˜“çš„ãªéŸ³å£°èªè­˜ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
    """
    def __init__(self, n_mfcc: int = 13, n_states: int = 3):
        self.n_mfcc = n_mfcc
        self.n_states = n_states
        self.models = {}  # å˜èªã”ã¨ã®HMMãƒ¢ãƒ‡ãƒ«

    def extract_features(self, audio_path: str, sr: int = 16000) -> np.ndarray:
        """
        éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰MFCCç‰¹å¾´ã‚’æŠ½å‡º
        """
        # éŸ³å£°èª­ã¿è¾¼ã¿
        y, sr = librosa.load(audio_path, sr=sr)

        # MFCC
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=self.n_mfcc)

        # ãƒ‡ãƒ«ã‚¿ç‰¹å¾´
        delta = librosa.feature.delta(mfcc)
        delta2 = librosa.feature.delta(mfcc, order=2)

        # çµåˆ
        features = np.vstack([mfcc, delta, delta2])

        return features.T  # ( hours, ç‰¹å¾´é‡)

    def train_word_model(self, word: str, audio_files: List[str]):
        """
        ç‰¹å®šã®å˜èªã®HMMãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
        """
        # å…¨è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´æŠ½å‡º
        all_features = []
        lengths = []

        for audio_file in audio_files:
            features = self.extract_features(audio_file)
            all_features.append(features)
            lengths.append(len(features))

        # çµåˆ
        X = np.vstack(all_features)

        # Left-to-Right HMM
        model = hmm.GaussianHMM(
            n_components=self.n_states,
            covariance_type='diag',
            n_iter=100,
            random_state=42
        )

        # é·ç§»ç¢ºç‡ã‚’åˆ¶ç´„ï¼ˆLeft-to-Rightï¼‰
        model.transmat_ = np.zeros((self.n_states, self.n_states))
        for i in range(self.n_states):
            if i < self.n_states - 1:
                model.transmat_[i, i] = 0.5
                model.transmat_[i, i+1] = 0.5
            else:
                model.transmat_[i, i] = 1.0

        model.startprob_ = np.zeros(self.n_states)
        model.startprob_[0] = 1.0

        # å­¦ç¿’
        model.fit(X, lengths)

        self.models[word] = model

        print(f"å˜èª '{word}' ã®ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¾ã—ãŸ")
        print(f"  è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(audio_files)}")
        print(f"  ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {len(X)}")

    def recognize(self, audio_path: str) -> Tuple[str, float]:
        """
        éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èªè­˜

        Returns:
            (èªè­˜ã•ã‚ŒãŸå˜èª, ã‚¹ã‚³ã‚¢)
        """
        # ç‰¹å¾´æŠ½å‡º
        features = self.extract_features(audio_path)

        # å„å˜èªãƒ¢ãƒ‡ãƒ«ã§ã‚¹ã‚³ã‚¢è¨ˆç®—
        scores = {}
        for word, model in self.models.items():
            try:
                score = model.score(features)
                scores[word] = score
            except:
                scores[word] = -np.inf

        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®å˜èªã‚’é¸æŠ
        best_word = max(scores, key=scores.get)
        best_score = scores[best_word]

        return best_word, best_score


# ãƒ‡ãƒ¢ä½¿ç”¨ä¾‹ï¼ˆå®Ÿéš›ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ï¼‰
print("=== ç°¡æ˜“ASRã‚·ã‚¹ãƒ†ãƒ  ===\n")
print("ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯ä»¥ä¸‹ã®æ‰‹é †ã§å‹•ä½œã—ã¾ã™ï¼š")
print("1. éŸ³å£°ã‹ã‚‰MFCCç‰¹å¾´ï¼ˆ+ ãƒ‡ãƒ«ã‚¿ï¼‰ã‚’æŠ½å‡º")
print("2. å„å˜èªã‚’Left-to-Right HMMã§ãƒ¢ãƒ‡ãƒ«åŒ–")
print("3. Viterbiã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æœ€ã‚‚ç¢ºã‹ã‚‰ã—ã„å˜èªã‚’é¸æŠ")
print("\nå®Ÿéš›ã®ä½¿ç”¨ã«ã¯éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ã§ã™")

# asr = SimpleASR(n_mfcc=13, n_states=3)
#
# # è¨“ç·´ï¼ˆå˜èªã”ã¨ã«è¤‡æ•°ã®éŸ³å£°ã‚µãƒ³ãƒ—ãƒ«ï¼‰
# asr.train_word_model('hello', ['hello1.wav', 'hello2.wav', 'hello3.wav'])
# asr.train_word_model('world', ['world1.wav', 'world2.wav', 'world3.wav'])
#
# # èªè­˜
# word, score = asr.recognize('test.wav')
# print(f"èªè­˜çµæœ: {word} (ã‚¹ã‚³ã‚¢: {score:.2f})")
</code></pre>

<h3>è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ã®çµ±åˆ</h3>

<p>å®Ÿéš›ã®ASRã§ã¯ã€éŸ³éŸ¿ãƒ¢ãƒ‡ãƒ«ã¨è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã—ã¾ã™ï¼š</p>

<p>$$
\hat{W} = \arg\max_W \left[\log P(X|W) + \lambda \log P(W)\right]
$$</p>

<ul>
<li>$\lambda$: è¨€èªãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ï¼ˆLanguage Model Weightï¼‰</li>
</ul>

<pre><code class="language-python">class ASRWithLanguageModel:
    """
    è¨€èªãƒ¢ãƒ‡ãƒ«çµ±åˆASR
    """
    def __init__(self, acoustic_model, language_model, lm_weight: float = 1.0):
        self.acoustic_model = acoustic_model
        self.language_model = language_model
        self.lm_weight = lm_weight

    def recognize_with_lm(self, audio_features: np.ndarray,
                          previous_words: List[str] = None) -> str:
        """
        è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸèªè­˜
        """
        # éŸ³éŸ¿ã‚¹ã‚³ã‚¢ï¼ˆå„å˜èªå€™è£œï¼‰
        acoustic_scores = {}
        for word in self.acoustic_model.models.keys():
            acoustic_scores[word] = self.acoustic_model.models[word].score(audio_features)

        # è¨€èªãƒ¢ãƒ‡ãƒ«ã‚¹ã‚³ã‚¢
        if previous_words:
            lm_scores = {}
            for word in acoustic_scores.keys():
                # ãƒã‚¤ã‚°ãƒ©ãƒ ç¢ºç‡
                prev_word = previous_words[-1] if previous_words else '<s>'
                lm_scores[word] = np.log(self.language_model.probability(prev_word, word))
        else:
            lm_scores = {word: 0 for word in acoustic_scores.keys()}

        # ç·åˆã‚¹ã‚³ã‚¢
        total_scores = {
            word: acoustic_scores[word] + self.lm_weight * lm_scores[word]
            for word in acoustic_scores.keys()
        }

        # æœ€è‰¯ã®å˜èªã‚’é¸æŠ
        best_word = max(total_scores, key=total_scores.get)

        return best_word

print("\n=== è¨€èªãƒ¢ãƒ‡ãƒ«çµ±åˆ ===")
print("éŸ³éŸ¿ã‚¹ã‚³ã‚¢ã¨è¨€èªã‚¹ã‚³ã‚¢ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€")
print("æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸèªè­˜ç²¾åº¦ã®å‘ä¸ŠãŒå¯èƒ½ã§ã™")
</code></pre>

<hr>

<h2>2.6 æœ¬ Chapterã®Summary</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>éŸ³å£°èªè­˜ã®åŸºç¤</strong></p>
<ul>
<li>ASRã¯éŸ³éŸ¿ãƒ¢ãƒ‡ãƒ«ã¨è¨€èªãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›</li>
<li>WERï¼ˆWord Error Rateï¼‰ã«ã‚ˆã‚‹è©•ä¾¡</li>
<li>ãƒ¬ãƒ¼ãƒ™ãƒ³ã‚·ãƒ¥ã‚¿ã‚¤ãƒ³è·é›¢ã«ã‚ˆã‚‹ã‚¨ãƒ©ãƒ¼è¨ˆç®—</li>
</ul></li>

<li><p><strong>Hidden Markov Models</strong></p>
<ul>
<li>çŠ¶æ…‹é·ç§»ã¨å‡ºåŠ›ç¢ºç‡ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°</li>
<li>Forward-Backwardã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆè©•ä¾¡ï¼‰</li>
<li>Viterbiã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰</li>
<li>Baum-Welchã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆå­¦ç¿’ï¼‰</li>
</ul></li>

<li><p><strong>Gaussian Mixture Models</strong></p>
<ul>
<li>è¤‡æ•°ã®ã‚¬ã‚¦ã‚¹ minuteså¸ƒã«ã‚ˆã‚‹å¯†åº¦æ¨å®š</li>
<li>EMã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š</li>
<li>GMM-HMMã«ã‚ˆã‚‹éŸ³éŸ¿ãƒ¢ãƒ‡ãƒªãƒ³ã‚°</li>
</ul></li>

<li><p><strong>è¨€èªãƒ¢ãƒ‡ãƒ«</strong></p>
<ul>
<li>N-gramã«ã‚ˆã‚‹å˜èªåˆ—ã®ç¢ºç‡ãƒ¢ãƒ‡ãƒªãƒ³ã‚°</li>
<li>ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°æŠ€è¡“ï¼ˆæœªè¦³æ¸¬äº‹è±¡ã¸ã®å¯¾å‡¦ï¼‰</li>
<li>ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã«ã‚ˆã‚‹è©•ä¾¡</li>
</ul></li>

<li><p><strong>ASRãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</strong></p>
<ul>
<li>ç‰¹å¾´æŠ½å‡ºï¼ˆMFCC + ãƒ‡ãƒ«ã‚¿ï¼‰</li>
<li>éŸ³éŸ¿ãƒ¢ãƒ‡ãƒ«ï¼ˆGMM-HMMï¼‰</li>
<li>ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆViterbiï¼‰</li>
<li>è¨€èªãƒ¢ãƒ‡ãƒ«çµ±åˆ</li>
</ul></li>
</ol>

<h3>ä¼çµ±çš„ASRã®é•·æ‰€ã¨çŸ­æ‰€</h3>

<table>
<thead>
<tr>
<th>é•·æ‰€</th>
<th>çŸ­æ‰€</th>
</tr>
</thead>
<tbody>
<tr>
<td>ç†è«–çš„ã«æ˜ç¢º</td>
<td>å¤§é‡ã®ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ãŒå¿…è¦</td>
</tr>
<tr>
<td>å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒç‹¬ç«‹</td>
<td>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®æœ€é©åŒ–ãŒå›°é›£</td>
</tr>
<tr>
<td>éŸ³ç´ Levelã§ã®è§£é‡ˆå¯èƒ½æ€§</td>
<td>é•· hoursä¾å­˜é–¢ä¿‚ã®ãƒ¢ãƒ‡ãƒ«åŒ–ãŒå¼±ã„</td>
</tr>
<tr>
<td>å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§ã‚‚å‹•ä½œ</td>
<td>ç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã«ä¾å­˜</td>
</tr>
</tbody>
</table>

<h3>Next Chapterã¸</h3>

<p>Chapter 3 Chapterã§ã¯ã€<strong>ç¾ä»£çš„ãªEnd-to-EndéŸ³å£°èªè­˜</strong>ã‚’å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>Deep Speechï¼ˆCTCï¼‰</li>
<li>Listen, Attend and Spell</li>
<li>Transformer-based ASR</li>
<li>Wav2Vec 2.0</li>
<li>Whisper</li>
</ul>

<hr>

<h2>Exercises</h2>

<h3>å•é¡Œ1ï¼ˆDifficultyï¼šeasyï¼‰</h3>
<p>ä»¥ä¸‹ã®å‚ç…§æ–‡ã¨ä»®èª¬æ–‡ã®WERã‚’æ‰‹è¨ˆç®—ã§æ±‚ã‚ã¦ãã ã•ã„ã€‚</p>
<ul>
<li>å‚ç…§: "the cat sat on the mat"</li>
<li>ä»®èª¬: "the cat sit on mat"</li>
</ul>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p>å‚ç…§ã¨ä»®èª¬ã‚’ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆï¼š</p>
<pre><code>å‚ç…§: the cat sat on the mat
ä»®èª¬: the cat sit on --- mat
</code></pre>

<p>ã‚¨ãƒ©ãƒ¼ã®ã‚«ã‚¦ãƒ³ãƒˆï¼š</p>
<ul>
<li>ç½®æ›ï¼ˆSï¼‰: sat â†’ sit ï¼ˆ1ï¼‰</li>
<li>å‰Šé™¤ï¼ˆDï¼‰: the ï¼ˆ1ï¼‰</li>
<li>æŒ¿å…¥ï¼ˆIï¼‰: 0</li>
</ul>

<p>WERã®è¨ˆç®—ï¼š</p>
<p>$$
\text{WER} = \frac{S + D + I}{N} = \frac{1 + 1 + 0}{6} = \frac{2}{6} = 0.333 = 33.3\%
$$</p>

<p>ç­”ãˆ: <strong>33.3%</strong></p>

</details>

<h3>å•é¡Œ2ï¼ˆDifficultyï¼šmediumï¼‰</h3>
<p>3çŠ¶æ…‹ã®HMMã§ã€ä»¥ä¸‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€è¦³æ¸¬åˆ— [0, 1, 0] ã®ç¢ºç‡ã‚’Forwardã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python"># åˆæœŸç¢ºç‡
pi = [0.6, 0.3, 0.1]

# é·ç§»ç¢ºç‡
A = [[0.7, 0.2, 0.1],
     [0.3, 0.5, 0.2],
     [0.2, 0.3, 0.5]]

# å‡ºåŠ›ç¢ºç‡ (è¦³æ¸¬å€¤ 0 ã¨ 1)
B = [[0.8, 0.2],
     [0.4, 0.6],
     [0.3, 0.7]]
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import numpy as np

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
pi = np.array([0.6, 0.3, 0.1])
A = np.array([[0.7, 0.2, 0.1],
              [0.3, 0.5, 0.2],
              [0.2, 0.3, 0.5]])
B = np.array([[0.8, 0.2],
              [0.4, 0.6],
              [0.3, 0.7]])

observations = [0, 1, 0]
T = len(observations)
N = len(pi)

# Forwardå¤‰æ•°
alpha = np.zeros((T, N))

# åˆæœŸåŒ– (t=0)
alpha[0] = pi * B[:, observations[0]]
print(f"t=0: Î± = {alpha[0]}")

# å†å¸° (t=1, 2, ...)
for t in range(1, T):
    for j in range(N):
        alpha[t, j] = np.sum(alpha[t-1] * A[:, j]) * B[j, observations[t]]
    print(f"t={t}: Î± = {alpha[t]}")

# è¦³æ¸¬åˆ—ã®ç¢ºç‡
prob = np.sum(alpha[T-1])

print(f"\nè¦³æ¸¬åˆ— {observations} ã®ç¢ºç‡:")
print(f"P(O|Î») = {prob:.6f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>t=0: Î± = [0.48 0.12 0.03]
t=1: Î± = [0.0672 0.1092 0.0588]
t=2: Î± = [0.06048 0.02688 0.01512]

è¦³æ¸¬åˆ— [0, 1, 0] ã®ç¢ºç‡:
P(O|Î») = 0.102480
</code></pre>

</details>

<h3>å•é¡Œ3ï¼ˆDifficultyï¼šmediumï¼‰</h3>
<p>ãƒã‚¤ã‚°ãƒ©ãƒ è¨€èªãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰ P("learning"|"machine") ã‚’æœ€å°¤æ¨å®šã§æ±‚ã‚ã¦ãã ã•ã„ã€‚</p>

<pre><code>I love machine learning
machine learning is fun
I study machine learning
deep learning is great
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p>ã‚«ã‚¦ãƒ³ãƒˆï¼š</p>
<ul>
<li>C(machine, learning) = 3</li>
<li>C(machine) = 3</li>
</ul>

<p>æœ€å°¤æ¨å®šï¼š</p>
<p>$$
P(\text{learning} | \text{machine}) = \frac{C(\text{machine}, \text{learning})}{C(\text{machine})} = \frac{3}{3} = 1.0
$$</p>

<p>ç­”ãˆ: <strong>1.0ï¼ˆ100%ï¼‰</strong></p>

<p>ã“ã®ã‚³ãƒ¼ãƒ‘ã‚¹ã§ã¯ã€Œmachineã€ã®å¾Œã«ã¯å¿…ãšã€Œlearningã€ãŒç¶šã„ã¦ã„ã¾ã™ã€‚</p>

</details>

<h3>å•é¡Œ4ï¼ˆDifficultyï¼šhardï¼‰</h3>
<p>GMMã®2æˆ minutesï¼ˆK=2ï¼‰ã‚’ä½¿ã£ã¦ã€ä»¥ä¸‹ã®1æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã—ã€å„æˆ minutesã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå¹³å‡ã€ distributedã€é‡ã¿ï¼‰ã‚’æ±‚ã‚ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">data = np.array([1.2, 1.5, 1.8, 2.0, 2.1, 8.5, 9.0, 9.2, 9.5, 10.0])
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import numpy as np
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt

data = np.array([1.2, 1.5, 1.8, 2.0, 2.1, 8.5, 9.0, 9.2, 9.5, 10.0])
X = data.reshape(-1, 1)

# GMMã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
gmm = GaussianMixture(n_components=2, random_state=42)
gmm.fit(X)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
print("=== GMM ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===")
print(f"\næˆ minutes1:")
print(f"  å¹³å‡: {gmm.means_[0][0]:.3f}")
print(f"   distributed: {gmm.covariances_[0][0][0]:.3f}")
print(f"  é‡ã¿: {gmm.weights_[0]:.3f}")

print(f"\næˆ minutes2:")
print(f"  å¹³å‡: {gmm.means_[1][0]:.3f}")
print(f"   distributed: {gmm.covariances_[1][0][0]:.3f}")
print(f"  é‡ã¿: {gmm.weights_[1]:.3f}")

# ã‚¯ãƒ©ã‚¹ã‚¿ãƒ©ãƒ™ãƒ«
labels = gmm.predict(X)
print(f"\nã‚¯ãƒ©ã‚¹ã‚¿ãƒ©ãƒ™ãƒ«:")
for i, (val, label) in enumerate(zip(data, labels)):
    print(f"  ãƒ‡ãƒ¼ã‚¿{i+1}: {val:.1f} â†’ ã‚¯ãƒ©ã‚¹ã‚¿{label}")

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.scatter(data, np.zeros_like(data), c=labels, cmap='viridis',
            s=100, alpha=0.6, edgecolors='black')
plt.scatter(gmm.means_, [0, 0], c='red', s=200, marker='X',
            edgecolors='black', linewidths=2, label='ä¸­å¿ƒ')
plt.xlabel('å€¤')
plt.title('GMMã«ã‚ˆã‚‹1æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== GMM ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===

æˆ minutes1:
  å¹³å‡: 1.720
   distributed: 0.124
  é‡ã¿: 0.500

æˆ minutes2:
  å¹³å‡: 9.240
   distributed: 0.294
  é‡ã¿: 0.500

ã‚¯ãƒ©ã‚¹ã‚¿ãƒ©ãƒ™ãƒ«:
  ãƒ‡ãƒ¼ã‚¿1: 1.2 â†’ ã‚¯ãƒ©ã‚¹ã‚¿0
  ãƒ‡ãƒ¼ã‚¿2: 1.5 â†’ ã‚¯ãƒ©ã‚¹ã‚¿0
  ãƒ‡ãƒ¼ã‚¿3: 1.8 â†’ ã‚¯ãƒ©ã‚¹ã‚¿0
  ãƒ‡ãƒ¼ã‚¿4: 2.0 â†’ ã‚¯ãƒ©ã‚¹ã‚¿0
  ãƒ‡ãƒ¼ã‚¿5: 2.1 â†’ ã‚¯ãƒ©ã‚¹ã‚¿0
  ãƒ‡ãƒ¼ã‚¿6: 8.5 â†’ ã‚¯ãƒ©ã‚¹ã‚¿1
  ãƒ‡ãƒ¼ã‚¿7: 9.0 â†’ ã‚¯ãƒ©ã‚¹ã‚¿1
  ãƒ‡ãƒ¼ã‚¿8: 9.2 â†’ ã‚¯ãƒ©ã‚¹ã‚¿1
  ãƒ‡ãƒ¼ã‚¿9: 9.5 â†’ ã‚¯ãƒ©ã‚¹ã‚¿1
  ãƒ‡ãƒ¼ã‚¿10: 10.0 â†’ ã‚¯ãƒ©ã‚¹ã‚¿1
</code></pre>

</details>

<h3>å•é¡Œ5ï¼ˆDifficultyï¼šhardï¼‰</h3>
<p>éŸ³éŸ¿ãƒ¢ãƒ‡ãƒ«ã¨è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã—ãŸASRã‚·ã‚¹ãƒ†ãƒ ã«ãŠã„ã¦ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ï¼ˆLM weightï¼‰ã‚’å¤‰åŒ–ã•ã›ã‚‹ã¨èªè­˜çµæœã«ã©ã®ã‚ˆã†ãªå½±éŸ¿ãŒã‚ã‚‹ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€æœ€é©ãªé‡ã¿ã¯ã©ã®ã‚ˆã†ã«æ±ºå®šã™ã¹ãã‹è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>LM weightã®å½±éŸ¿</strong>ï¼š</p>

<p>ç·åˆã‚¹ã‚³ã‚¢ï¼š</p>
<p>$$
\text{Score}(W) = \log P(X|W) + \lambda \log P(W)
$$</p>

<table>
<thead>
<tr>
<th>LM weight $\lambda$</th>
<th>å½±éŸ¿</th>
<th>èªè­˜å‚¾å‘</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å°ã•ã„ï¼ˆ0ã«è¿‘ã„ï¼‰</strong></td>
<td>éŸ³éŸ¿ãƒ¢ãƒ‡ãƒ«å„ªå…ˆ</td>
<td>éŸ³éŸ¿çš„ã«ä¼¼ãŸå˜èªã‚’é¸æŠã€æ–‡æ³•çš„ã«ä¸è‡ªç„¶</td>
</tr>
<tr>
<td><strong>é©åˆ‡</strong></td>
<td>ãƒãƒ©ãƒ³ã‚¹ãŒå–ã‚Œã‚‹</td>
<td>éŸ³éŸ¿ã¨æ–‡æ³•ã®ä¸¡æ–¹ã‚’è€ƒæ…®ã€æœ€è‰¯ã®èªè­˜ç²¾åº¦</td>
</tr>
<tr>
<td><strong>å¤§ãã„</strong></td>
<td>è¨€èªãƒ¢ãƒ‡ãƒ«å„ªå…ˆ</td>
<td>æ–‡æ³•çš„ã«ã¯æ­£ã—ã„ãŒéŸ³éŸ¿çš„ã«èª¤ã‚Šã€é »å‡ºå˜èªã«åã‚‹</td>
</tr>
</tbody>
</table>

<p><strong>æœ€é©ãªé‡ã¿ã®æ±ºå®šæ–¹æ³•</strong>ï¼š</p>

<ol>
<li><p><strong>é–‹ç™ºã‚»ãƒƒãƒˆã§ã®ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ</strong></p>
<pre><code class="language-python">lambda_values = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
best_lambda = None
best_wer = float('inf')

for lam in lambda_values:
    wer = evaluate_asr(dev_set, lm_weight=lam)
    if wer < best_wer:
        best_wer = wer
        best_lambda = lam

print(f"æœ€é©ãªLM weight: {best_lambda}")
</code></pre></li>

<li><p><strong>ãƒ‰ãƒ¡ã‚¤ãƒ³ä¾å­˜æ€§ã®è€ƒæ…®</strong></p>
<ul>
<li>èª­ã¿ä¸Šã’éŸ³å£°: éŸ³éŸ¿å“è³ªãŒé«˜ã„ â†’ å°ã•ã‚ã® $\lambda$</li>
<li>è‡ªç™ºéŸ³å£°: ãƒã‚¤ã‚ºãŒå¤šã„ â†’ å¤§ãã‚ã® $\lambda$</li>
<li>å°‚é–€ç”¨èªãŒå¤šã„: è¨€èªãƒ¢ãƒ‡ãƒ«ã®ä¿¡é ¼æ€§ãŒä½ã„ â†’ å°ã•ã‚ã® $\lambda$</li>
</ul></li>

<li><p><strong>å‹•çš„èª¿æ•´</strong></p>
<ul>
<li>ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã«åŸºã¥ã„ã¦å‹•çš„ã«èª¿æ•´</li>
<li>éŸ³éŸ¿å“è³ªï¼ˆSNRï¼‰ã«å¿œã˜ã¦èª¿æ•´</li>
</ul></li>
</ol>

<p><strong>å®Ÿä¾‹</strong>ï¼š</p>
<pre><code>éŸ³å£°: "I scream"ï¼ˆã‚¢ã‚¤ã‚¹ã‚¯ãƒªãƒ¼ãƒ ï¼‰

Î» = 0.1ï¼ˆéŸ³éŸ¿å„ªå…ˆï¼‰:
  â†’ "I scream"ï¼ˆéŸ³éŸ¿çš„ã«æ­£ç¢ºï¼‰

Î» = 5.0ï¼ˆè¨€èªå„ªå…ˆï¼‰:
  â†’ "ice cream"ï¼ˆæ–‡æ³•çš„ã«è‡ªç„¶ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã§é«˜é »åº¦ï¼‰

Î» = 1.0ï¼ˆãƒãƒ©ãƒ³ã‚¹ï¼‰:
  â†’ æ–‡è„ˆæ¬¡Chapter ã§é©åˆ‡ã«é¸æŠ
</code></pre>

<p><strong>çµè«–</strong>ï¼šæœ€é©ãªLM weightã¯ãƒ‰ãƒ¡ã‚¤ãƒ³ã€éŸ³éŸ¿æ¡ä»¶ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®å“è³ªã«ä¾å­˜ã—ã€é–‹ç™ºã‚»ãƒƒãƒˆã§ã®å®Ÿé¨“çš„ãªèª¿æ•´ãŒå¿…è¦ã§ã™ã€‚</p>

</details>

<hr>

<h2>References</h2>

<ol>
<li>Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech recognition. <em>Proceedings of the IEEE</em>, 77(2), 257-286.</li>
<li>Bishop, C. M. (2006). <em>Pattern Recognition and Machine Learning</em>. Springer.</li>
<li>Jurafsky, D., & Martin, J. H. (2023). <em>Speech and Language Processing</em> (3rd ed.). Draft.</li>
<li>Gales, M., & Young, S. (2008). The application of hidden Markov models in speech recognition. <em>Foundations and Trends in Signal Processing</em>, 1(3), 195-304.</li>
<li>Heafield, K. (2011). KenLM: Faster and smaller language model queries. <em>Proceedings of the Sixth Workshop on Statistical Machine Translation</em>, 187-197.</li>
</ol>

<div class="navigation">
    <a href="chapter1-audio-basics.html" class="nav-button">â† Previous Chapter: éŸ³å£°ä¿¡å·ã®åŸºç¤</a>
    <a href="chapter3-modern-asr.html" class="nav-button">Next Chapter: ç¾ä»£çš„éŸ³å£°èªè­˜ â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, either express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The creators and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
            <li>To the maximum extent permitted by applicable law, the creators and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are subject to the specified terms (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆ learner</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
