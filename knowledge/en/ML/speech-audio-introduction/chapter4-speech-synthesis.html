<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 4: Speech Synthesis (TTS) - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/speech-audio-introduction/index.html">Speech Audio</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 4</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/ML/speech-audio-introduction/chapter4-speech-synthesis.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 4: Speech Synthesis (TTS)</h1>
<p class="subtitle">Understanding the technology of generating natural speech from text</p>
<div class="meta">
<span class="meta-item">üìñ Reading time: 30-35 minutes</span>
<span class="meta-item">üìä Difficulty: Intermediate</span>
<span class="meta-item">üíª Code examples: 7</span>
<span class="meta-item">üìù Practice problems: 5</span>
</div>
</div>
</header>
<main class="container">

<p class="chapter-description">This chapter covers Speech Synthesis (TTS). You will learn characteristics and latest TTS technologies.</p>
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Understand the basic concepts and pipeline of speech synthesis (TTS)</li>
<li>‚úÖ Understand the architecture and operating principles of Tacotron/Tacotron 2</li>
<li>‚úÖ Learn the mechanisms of non-autoregressive TTS using FastSpeech</li>
<li>‚úÖ Understand the characteristics and appropriate use cases of major neural vocoders</li>
<li>‚úÖ Grasp the latest TTS technologies and their applications</li>
<li>‚úÖ Implement speech synthesis using Python libraries</li>
</ul>
<hr/>
<h2>4.1 TTS Fundamentals</h2>
<h3>What is Text-to-Speech (TTS)?</h3>
<p><strong>Text-to-Speech (TTS)</strong> is a technology that converts text into natural speech. Recent advances in deep learning have made it possible to generate speech that closely resembles human voice.</p>
<blockquote>
<p>"TTS converts written words into spoken words through the integration of linguistic understanding and acoustic modeling."</p>
</blockquote>
<h3>TTS Pipeline</h3>
<div class="mermaid">
graph LR
    A[Text Input] --&gt; B[Text Analysis]
    B --&gt; C[Linguistic Feature Extraction]
    C --&gt; D[Acoustic Model]
    D --&gt; E[Mel-Spectrogram]
    E --&gt; F[Vocoder]
    F --&gt; G[Speech Waveform]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#e8f5e9
    style F fill:#fce4ec
    style G fill:#c8e6c9
</div>
<h3>Pipeline Stages</h3>
<table>
<thead>
<tr>
<th>Stage</th>
<th>Role</th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Text Analysis</strong></td>
<td>Normalization, tokenization</td>
<td>Normalized text</td>
</tr>
<tr>
<td><strong>Linguistic Feature Extraction</strong></td>
<td>Phoneme conversion, prosody prediction</td>
<td>Linguistic feature vectors</td>
</tr>
<tr>
<td><strong>Acoustic Model</strong></td>
<td>Mel-spectrogram generation</td>
<td>Mel-spectrogram</td>
</tr>
<tr>
<td><strong>Vocoder</strong></td>
<td>Waveform generation</td>
<td>Speech waveform</td>
</tr>
</tbody>
</table>
<h3>Role of the Vocoder</h3>
<p>A <strong>vocoder</strong> is a module that generates speech waveforms from Mel-spectrograms.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Avocoderis a module that generates speech waveforms from Mel

Purpose: Demonstrate data visualization techniques
Target: Beginner to Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
import librosa
import librosa.display

# Load sample audio
y, sr = librosa.load(librosa.example('trumpet'), sr=22050, duration=3)

# Generate Mel-spectrogram
mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80)
mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)

# Visualization
fig, axes = plt.subplots(2, 1, figsize=(14, 8))

# Waveform
axes[0].plot(np.linspace(0, len(y)/sr, len(y)), y)
axes[0].set_xlabel('Time (seconds)')
axes[0].set_ylabel('Amplitude')
axes[0].set_title('Speech Waveform', fontsize=14)
axes[0].grid(True, alpha=0.3)

# Mel-spectrogram
img = librosa.display.specshow(mel_spec_db, x_axis='time', y_axis='mel',
                               sr=sr, ax=axes[1], cmap='viridis')
axes[1].set_title('Mel-Spectrogram', fontsize=14)
fig.colorbar(img, ax=axes[1], format='%+2.0f dB')

plt.tight_layout()
plt.show()

print("=== Role of Vocoder in TTS ===")
print(f"Input: Mel-spectrogram {mel_spec.shape}")
print(f"Output: Speech waveform {y.shape}")
print(f"Sampling rate: {sr} Hz")
</code></pre>
<h3>Prosody and Naturalness</h3>
<p><strong>Prosody</strong> is a crucial element that determines the naturalness of speech:</p>
<ul>
<li><strong>Pitch</strong>: Voice height, intonation</li>
<li><strong>Duration</strong>: Length of phonemes</li>
<li><strong>Energy</strong>: Intensity of sound</li>
<li><strong>Rhythm</strong>: Tempo of speech</li>
</ul>
<h3>Evaluation Metrics</h3>
<h4>1. MOS (Mean Opinion Score)</h4>
<p><strong>MOS</strong> is the average score of subjective human evaluation.</p>
<table>
<thead>
<tr>
<th>Score</th>
<th>Quality</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>5</td>
<td>Excellent</td>
<td>Indistinguishable from natural human speech</td>
</tr>
<tr>
<td>4</td>
<td>Good</td>
<td>Slight unnaturalness</td>
</tr>
<tr>
<td>3</td>
<td>Fair</td>
<td>Clearly synthetic but understandable</td>
</tr>
<tr>
<td>2</td>
<td>Poor</td>
<td>Some parts difficult to hear</td>
</tr>
<tr>
<td>1</td>
<td>Bad</td>
<td>Difficult to understand</td>
</tr>
</tbody>
</table>
<h4>2. Naturalness</h4>
<p>Evaluates the human-likeness of speech:</p>
<ul>
<li>Naturalness of prosody</li>
<li>Smoothness of audio quality</li>
<li>Richness of emotional expression</li>
</ul>
<hr/>
<h2>4.2 Tacotron &amp; Tacotron 2</h2>
<h3>Tacotron Overview</h3>
<p><strong>Tacotron</strong> is an early end-to-end TTS model using a Seq2Seq architecture (published by Google in 2017).</p>
<h3>Seq2Seq for TTS</h3>
<div class="mermaid">
graph LR
    A[Text] --&gt; B[Encoder]
    B --&gt; C[Attention]
    C --&gt; D[Decoder]
    D --&gt; E[Mel-Spectrogram]
    E --&gt; F[Vocoder]
    F --&gt; G[Speech Waveform]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#e8f5e9
    style F fill:#fce4ec
    style G fill:#c8e6c9
</div>
<h3>Attention Mechanism</h3>
<p><strong>Attention</strong> determines which parts of the encoder output the decoder should focus on at each step.</p>
<p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$</p>
<ul>
<li>$Q$: Query (decoder hidden state)</li>
<li>$K$: Key (encoder hidden states)</li>
<li>$V$: Value (encoder hidden states)</li>
</ul>
<h3>Tacotron 2 Architecture</h3>
<p><strong>Tacotron 2</strong> improves upon Tacotron to generate higher quality speech.</p>
<h4>Key Improvements</h4>
<table>
<thead>
<tr>
<th>Component</th>
<th>Tacotron</th>
<th>Tacotron 2</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Encoder</strong></td>
<td>CBHG</td>
<td>Conv + Bi-LSTM</td>
</tr>
<tr>
<td><strong>Attention</strong></td>
<td>Basic</td>
<td>Location-sensitive</td>
</tr>
<tr>
<td><strong>Decoder</strong></td>
<td>GRU</td>
<td>LSTM + Prenet</td>
</tr>
<tr>
<td><strong>Vocoder</strong></td>
<td>Griffin-Lim</td>
<td>WaveNet</td>
</tr>
</tbody>
</table>
<h3>Mel-Spectrogram Prediction</h3>
<p>Tacotron 2 predicts 80-dimensional Mel-spectrograms.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: Tacotron 2 predicts 80-dimensional Mel-spectrograms.

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 2-5 seconds
Dependencies: None
"""

import torch
import numpy as np
import matplotlib.pyplot as plt

# Tacotron 2-style simple Mel prediction demo
# Note: Actual Tacotron 2 is much more complex

class SimpleTacotronDecoder(torch.nn.Module):
    def __init__(self, hidden_size=256, n_mels=80):
        super().__init__()
        self.prenet = torch.nn.Sequential(
            torch.nn.Linear(n_mels, 256),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.5),
            torch.nn.Linear(256, 128)
        )
        self.lstm = torch.nn.LSTM(hidden_size + 128, hidden_size,
                                  num_layers=2, batch_first=True)
        self.projection = torch.nn.Linear(hidden_size, n_mels)

    def forward(self, encoder_outputs, prev_mel):
        # Prenet processing
        prenet_out = self.prenet(prev_mel)

        # LSTM decoder
        lstm_input = torch.cat([encoder_outputs, prenet_out], dim=-1)
        lstm_out, _ = self.lstm(lstm_input)

        # Mel-spectrogram prediction
        mel_pred = self.projection(lstm_out)

        return mel_pred

# Model instantiation
model = SimpleTacotronDecoder()
print("=== Tacotron 2-style Decoder ===")
print(model)

# Test with dummy data
batch_size, seq_len = 4, 10
encoder_out = torch.randn(batch_size, seq_len, 256)
prev_mel = torch.randn(batch_size, seq_len, 80)

# Prediction
mel_output = model(encoder_out, prev_mel)
print(f"\nInput encoder output: {encoder_out.shape}")
print(f"Input Mel: {prev_mel.shape}")
print(f"Output Mel: {mel_output.shape}")
</code></pre>
<h3>Location-sensitive Attention</h3>
<p>Tacotron 2 uses <strong>Location-sensitive Attention</strong> to achieve stable and monotonic alignment.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn
import torch.nn.functional as F

class LocationSensitiveAttention(nn.Module):
    """Location-sensitive Attention for Tacotron 2"""

    def __init__(self, attention_dim=128, n_location_filters=32,
                 location_kernel_size=31):
        super().__init__()
        self.W_query = nn.Linear(256, attention_dim, bias=False)
        self.W_keys = nn.Linear(256, attention_dim, bias=False)
        self.W_location = nn.Linear(n_location_filters, attention_dim, bias=False)
        self.location_conv = nn.Conv1d(2, n_location_filters,
                                       kernel_size=location_kernel_size,
                                       padding=(location_kernel_size - 1) // 2,
                                       bias=False)
        self.v = nn.Linear(attention_dim, 1, bias=False)

    def forward(self, query, keys, attention_weights_cat):
        """
        Args:
            query: Decoder hidden state (B, 1, 256)
            keys: Encoder output (B, T, 256)
            attention_weights_cat: Previous attention weights (B, 2, T)
        """
        # Location features
        location_features = self.location_conv(attention_weights_cat)
        location_features = location_features.transpose(1, 2)

        # Attention computation
        query_proj = self.W_query(query)  # (B, 1, attention_dim)
        keys_proj = self.W_keys(keys)     # (B, T, attention_dim)
        location_proj = self.W_location(location_features)  # (B, T, attention_dim)

        # Energy computation
        energies = self.v(torch.tanh(query_proj + keys_proj + location_proj))
        energies = energies.squeeze(-1)  # (B, T)

        # Attention weights
        attention_weights = F.softmax(energies, dim=1)

        return attention_weights

# Test
attention = LocationSensitiveAttention()
query = torch.randn(4, 1, 256)
keys = torch.randn(4, 100, 256)
prev_attention = torch.randn(4, 2, 100)

weights = attention(query, keys, prev_attention)
print("=== Location-sensitive Attention ===")
print(f"Attention weights shape: {weights.shape}")
print(f"Sum of weights: {weights.sum(dim=1)}")  # Should be ~1.0
</code></pre>
<h3>Tacotron 2 Implementation Example (PyTorch)</h3>
<pre><code class="language-python"># Simplified version of Tacotron 2 encoder
class TacotronEncoder(nn.Module):
    def __init__(self, num_chars=150, embedding_dim=512, hidden_size=256):
        super().__init__()
        self.embedding = nn.Embedding(num_chars, embedding_dim)

        # Convolution layers
        self.convolutions = nn.ModuleList([
            nn.Sequential(
                nn.Conv1d(embedding_dim if i == 0 else hidden_size,
                         hidden_size, kernel_size=5, padding=2),
                nn.BatchNorm1d(hidden_size),
                nn.ReLU(),
                nn.Dropout(0.5)
            ) for i in range(3)
        ])

        # Bi-directional LSTM
        self.lstm = nn.LSTM(hidden_size, hidden_size // 2,
                           num_layers=1, batch_first=True,
                           bidirectional=True)

    def forward(self, text_inputs):
        # Embedding
        x = self.embedding(text_inputs).transpose(1, 2)  # (B, C, T)

        # Convolutions
        for conv in self.convolutions:
            x = conv(x)

        # LSTM
        x = x.transpose(1, 2)  # (B, T, C)
        outputs, _ = self.lstm(x)

        return outputs

# Test
encoder = TacotronEncoder()
text_input = torch.randint(0, 150, (4, 50))  # Batch of 4, length 50
encoder_output = encoder(text_input)

print("=== Tacotron 2 Encoder ===")
print(f"Input text: {text_input.shape}")
print(f"Encoder output: {encoder_output.shape}")
</code></pre>
<hr/>
<h2>4.3 FastSpeech</h2>
<h3>Motivation for Non-Autoregressive TTS</h3>
<p>Problems with <strong>autoregressive models</strong> like Tacotron 2:</p>
<ul>
<li>Sequential generation is slow</li>
<li>Unstable alignment</li>
<li>Word repetition or skipping</li>
</ul>
<p><strong>FastSpeech</strong> is a <strong>non-autoregressive TTS</strong> that solves these problems.</p>
<h3>FastSpeech Architecture</h3>
<div class="mermaid">
graph LR
    A[Text] --&gt; B[Encoder]
    B --&gt; C[Duration Predictor]
    C --&gt; D[Length Regulator]
    B --&gt; D
    D --&gt; E[Decoder]
    E --&gt; F[Mel-Spectrogram]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#e8f5e9
    style F fill:#c8e6c9
</div>
<h3>Duration Prediction</h3>
<p>The core of FastSpeech is explicitly predicting the duration of each phoneme.</p>
<p>$$
d_i = \text{DurationPredictor}(h_i)
$$</p>
<ul>
<li>$h_i$: Hidden state of phoneme $i$</li>
<li>$d_i$: Duration of phoneme $i$ (number of frames)</li>
</ul>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn

class DurationPredictor(nn.Module):
    """Duration predictor for FastSpeech"""

    def __init__(self, hidden_size=256, filter_size=256,
                 kernel_size=3, dropout=0.5):
        super().__init__()

        self.layers = nn.ModuleList([
            nn.Sequential(
                nn.Conv1d(hidden_size, filter_size, kernel_size,
                         padding=(kernel_size - 1) // 2),
                nn.ReLU(),
                nn.LayerNorm(filter_size),
                nn.Dropout(dropout)
            ) for _ in range(2)
        ])

        self.linear = nn.Linear(filter_size, 1)

    def forward(self, encoder_output):
        """
        Args:
            encoder_output: (B, T, hidden_size)
        Returns:
            duration: (B, T) - Duration of each phoneme
        """
        x = encoder_output.transpose(1, 2)  # (B, C, T)

        for layer in self.layers:
            x = layer(x)

        x = x.transpose(1, 2)  # (B, T, C)
        duration = self.linear(x).squeeze(-1)  # (B, T)

        return duration

# Test
duration_predictor = DurationPredictor()
encoder_out = torch.randn(4, 50, 256)  # Batch=4, Seq=50
durations = duration_predictor(encoder_out)

print("=== Duration Predictor ===")
print(f"Input: {encoder_out.shape}")
print(f"Predicted durations: {durations.shape}")
print(f"Sample durations: {durations[0, :10]}")
</code></pre>
<h3>Length Regulator</h3>
<p>The <strong>Length Regulator</strong> expands phoneme-level hidden states to frame-level based on predicted durations.</p>
<pre><code class="language-python">class LengthRegulator(nn.Module):
    """Length Regulator for FastSpeech"""

    def __init__(self):
        super().__init__()

    def forward(self, x, durations):
        """
        Args:
            x: Phoneme-level hidden states (B, T_phoneme, C)
            durations: Duration of each phoneme (B, T_phoneme)
        Returns:
            expanded: Frame-level hidden states (B, T_frame, C)
        """
        output = []
        for batch_idx in range(x.size(0)):
            expanded = []
            for phoneme_idx in range(x.size(1)):
                # Repeat each phoneme by its duration
                duration = int(durations[batch_idx, phoneme_idx].item())
                expanded.append(x[batch_idx, phoneme_idx].unsqueeze(0).expand(duration, -1))

            if expanded:
                output.append(torch.cat(expanded, dim=0))

        # Pad to same length
        max_len = max([seq.size(0) for seq in output])
        padded_output = []
        for seq in output:
            pad_len = max_len - seq.size(0)
            if pad_len &gt; 0:
                padding = torch.zeros(pad_len, seq.size(1))
                seq = torch.cat([seq, padding], dim=0)
            padded_output.append(seq.unsqueeze(0))

        return torch.cat(padded_output, dim=0)

# Test
length_regulator = LengthRegulator()
phoneme_hidden = torch.randn(2, 10, 256)  # 2 samples, 10 phonemes
durations = torch.tensor([[3, 2, 4, 1, 5, 2, 3, 2, 1, 4],
                          [2, 3, 2, 5, 1, 4, 2, 3, 2, 1]], dtype=torch.float32)

frame_hidden = length_regulator(phoneme_hidden, durations)
print("=== Length Regulator ===")
print(f"Phoneme-level: {phoneme_hidden.shape}")
print(f"Predicted durations: {durations}")
print(f"Frame-level: {frame_hidden.shape}")
</code></pre>
<h3>FastSpeech 2 Improvements</h3>
<p><strong>FastSpeech 2</strong> predicts more prosodic information:</p>
<table>
<thead>
<tr>
<th>Prediction Target</th>
<th>FastSpeech</th>
<th>FastSpeech 2</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Duration</strong></td>
<td>‚úì</td>
<td>‚úì</td>
</tr>
<tr>
<td><strong>Pitch</strong></td>
<td>-</td>
<td>‚úì</td>
</tr>
<tr>
<td><strong>Energy</strong></td>
<td>-</td>
<td>‚úì</td>
</tr>
<tr>
<td><strong>Training Target</strong></td>
<td>Teacher forcing</td>
<td>Ground truth</td>
</tr>
</tbody>
</table>
<h3>Speed vs Quality</h3>
<p>Advantages of FastSpeech:</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Tacotron 2</th>
<th>FastSpeech</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Generation Speed</strong></td>
<td>1x</td>
<td>38x</td>
<td>38x faster</td>
</tr>
<tr>
<td><strong>MOS</strong></td>
<td>4.41</td>
<td>4.27</td>
<td>-3%</td>
</tr>
<tr>
<td><strong>Robustness</strong></td>
<td>Low</td>
<td>High</td>
<td>Nearly 0% error rate</td>
</tr>
<tr>
<td><strong>Controllability</strong></td>
<td>Low</td>
<td>High</td>
<td>Speed adjustable</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Important</strong>: FastSpeech achieves significant speedup and robustness improvement with only a slight quality reduction.</p>
</blockquote>
<hr/>
<h2>4.4 Neural Vocoders</h2>
<h3>Evolution of Vocoders</h3>
<p>Neural vocoders generate high-quality speech waveforms from Mel-spectrograms.</p>
<h3>1. WaveNet</h3>
<p><strong>WaveNet</strong> is an autoregressive generative model that produces very high-quality speech (DeepMind, 2016).</p>
<h4>Dilated Causal Convolution</h4>
<p>The core of WaveNet is <strong>Dilated Causal Convolution</strong>.</p>
<p>$$
y_t = f\left(\sum_{i=0}^{k-1} w_i \cdot x_{t-d \cdot i}\right)
$$</p>
<ul>
<li>$d$: Dilation factor</li>
<li>$k$: Kernel size</li>
</ul>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn

class DilatedCausalConv1d(nn.Module):
    """Dilated Causal Convolution for WaveNet"""

    def __init__(self, in_channels, out_channels, kernel_size, dilation):
        super().__init__()
        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size,
                             padding=(kernel_size - 1) * dilation,
                             dilation=dilation)

    def forward(self, x):
        # Causal: don't use future information
        output = self.conv(x)
        # Remove right padding
        return output[:, :, :x.size(2)]

class WaveNetBlock(nn.Module):
    """WaveNet residual block"""

    def __init__(self, residual_channels, gate_channels, skip_channels,
                 kernel_size, dilation):
        super().__init__()

        self.dilated_conv = DilatedCausalConv1d(
            residual_channels, gate_channels, kernel_size, dilation
        )

        self.conv_1x1_skip = nn.Conv1d(gate_channels // 2, skip_channels, 1)
        self.conv_1x1_res = nn.Conv1d(gate_channels // 2, residual_channels, 1)

    def forward(self, x):
        # Dilated convolution
        h = self.dilated_conv(x)

        # Gated activation
        tanh_out, sigmoid_out = h.chunk(2, dim=1)
        h = torch.tanh(tanh_out) * torch.sigmoid(sigmoid_out)

        # Skip connection
        skip = self.conv_1x1_skip(h)

        # Residual connection
        residual = self.conv_1x1_res(h)
        return (x + residual) * 0.707, skip  # sqrt(0.5) for scaling

# Test
block = WaveNetBlock(residual_channels=64, gate_channels=128,
                     skip_channels=64, kernel_size=3, dilation=2)
x = torch.randn(4, 64, 1000)  # (B, C, T)
residual, skip = block(x)

print("=== WaveNet Block ===")
print(f"Input: {x.shape}")
print(f"Residual output: {residual.shape}")
print(f"Skip output: {skip.shape}")
</code></pre>
<h3>2. WaveGlow</h3>
<p><strong>WaveGlow</strong> is a flow-based generative model capable of parallel generation (NVIDIA, 2018).</p>
<h4>Features</h4>
<ul>
<li><strong>Real-time generation</strong>: Faster than WaveNet</li>
<li><strong>Parallelizable</strong>: Generate all samples at once</li>
<li><strong>Invertible transformation</strong>: Bidirectional conversion between audio and latent variables</li>
</ul>
<h3>3. HiFi-GAN</h3>
<p><strong>HiFi-GAN</strong> (High Fidelity GAN) is a fast, high-quality GAN-based vocoder (2020).</p>
<h4>Architecture</h4>
<table>
<thead>
<tr>
<th>Component</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Generator</strong></td>
<td>Upsampling with Transposed Convolution</td>
</tr>
<tr>
<td><strong>Multi-Period Discriminator</strong></td>
<td>Identifies different periodic patterns</td>
</tr>
<tr>
<td><strong>Multi-Scale Discriminator</strong></td>
<td>Identifies at different resolutions</td>
</tr>
</tbody>
</table>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn

class HiFiGANGenerator(nn.Module):
    """HiFi-GAN Generator (simplified version)"""

    def __init__(self, mel_channels=80, upsample_rates=[8, 8, 2, 2]):
        super().__init__()

        self.num_upsamples = len(upsample_rates)

        # Initial Conv
        self.conv_pre = nn.Conv1d(mel_channels, 512, 7, padding=3)

        # Upsampling layers
        self.ups = nn.ModuleList()
        for i, u in enumerate(upsample_rates):
            self.ups.append(nn.ConvTranspose1d(
                512 // (2 ** i),
                512 // (2 ** (i + 1)),
                u * 2,
                stride=u,
                padding=u // 2
            ))

        # Final Conv
        self.conv_post = nn.Conv1d(512 // (2 ** len(upsample_rates)),
                                   1, 7, padding=3)

    def forward(self, mel):
        """
        Args:
            mel: Mel-spectrogram (B, mel_channels, T)
        Returns:
            audio: Speech waveform (B, 1, T * prod(upsample_rates))
        """
        x = self.conv_pre(mel)

        for i in range(self.num_upsamples):
            x = torch.nn.functional.leaky_relu(x, 0.1)
            x = self.ups[i](x)

        x = torch.nn.functional.leaky_relu(x, 0.1)
        x = self.conv_post(x)
        x = torch.tanh(x)

        return x

# Test
generator = HiFiGANGenerator()
mel_input = torch.randn(2, 80, 100)  # (B, mel_channels, T)
audio_output = generator(mel_input)

print("=== HiFi-GAN Generator ===")
print(f"Input Mel: {mel_input.shape}")
print(f"Output audio: {audio_output.shape}")
print(f"Upsampling rate: {audio_output.size(2) / mel_input.size(2):.0f}x")
</code></pre>
<h3>Vocoder Comparison</h3>
<table>
<thead>
<tr>
<th>Vocoder</th>
<th>Generation Method</th>
<th>Speed</th>
<th>Quality (MOS)</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Griffin-Lim</strong></td>
<td>Iterative algorithm</td>
<td>Fast</td>
<td>3.0-3.5</td>
<td>Simple, low quality</td>
</tr>
<tr>
<td><strong>WaveNet</strong></td>
<td>Autoregressive</td>
<td>Very slow</td>
<td>4.5+</td>
<td>Highest quality</td>
</tr>
<tr>
<td><strong>WaveGlow</strong></td>
<td>Flow-based</td>
<td>Medium</td>
<td>4.2-4.3</td>
<td>Parallel generation</td>
</tr>
<tr>
<td><strong>HiFi-GAN</strong></td>
<td>GAN</td>
<td>Very fast</td>
<td>4.3-4.5</td>
<td>Fast &amp; high quality</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Recommendation</strong>: Currently, HiFi-GAN offers the best balance of speed and quality and is widely used.</p>
</blockquote>
<hr/>
<h2>4.5 Latest TTS Technologies</h2>
<h3>1. VITS (End-to-End TTS)</h3>
<p><strong>VITS</strong> (Variational Inference with adversarial learning for end-to-end Text-to-Speech) is an end-to-end model that integrates the acoustic model and vocoder (2021).</p>
<h4>VITS Features</h4>
<ul>
<li><strong>Integrated architecture</strong>: Acoustic model + Vocoder</li>
<li><strong>VAE + GAN</strong>: Combination of Variational Autoencoder and adversarial learning</li>
<li><strong>Fast &amp; high quality</strong>: Real-time generation capability</li>
<li><strong>Diversity</strong>: Generate diverse speech from the same text</li>
</ul>
<div class="mermaid">
graph LR
    A[Text] --&gt; B[Text Encoder]
    B --&gt; C[Posterior Encoder]
    B --&gt; D[Prior Encoder]
    C --&gt; E[Latent Variable z]
    D --&gt; E
    E --&gt; F[Decoder/Generator]
    F --&gt; G[Speech Waveform]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#f3e5f5
    style E fill:#e3f2fd
    style F fill:#e8f5e9
    style G fill:#c8e6c9
</div>
<h3>2. Voice Cloning</h3>
<p><strong>Voice Cloning</strong> is a technology that reproduces a specific speaker's voice from a small amount of audio samples.</p>
<h4>Approaches</h4>
<table>
<thead>
<tr>
<th>Method</th>
<th>Description</th>
<th>Data Required</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Speaker Adaptation</strong></td>
<td>Fine-tune existing model with small amount of data</td>
<td>Few to tens of minutes</td>
</tr>
<tr>
<td><strong>Speaker Embedding</strong></td>
<td>Learn speaker embedding vectors</td>
<td>Few seconds to minutes</td>
</tr>
<tr>
<td><strong>Zero-shot TTS</strong></td>
<td>Instantly mimic unknown speaker's voice</td>
<td>Few seconds</td>
</tr>
</tbody>
</table>
<h3>3. Multi-speaker TTS</h3>
<p><strong>Multi-speaker TTS</strong> generates multiple speakers' voices with a single model.</p>
<h4>Speaker Embedding</h4>
<p>Convert speaker ID into an embedding vector and condition the model on it.</p>
<p>$$
e_{\text{speaker}} = \text{Embedding}(\text{speaker\_id})
$$</p>
<p>$$
h = f(x, e_{\text{speaker}})
$$</p>
<h3>4. Japanese TTS Systems</h3>
<p>Features of Japanese TTS systems:</p>
<h4>Japanese-specific Challenges</h4>
<ul>
<li><strong>Accent</strong>: Reproducing pitch accent</li>
<li><strong>Intonation</strong>: Sentence-final rising and falling patterns</li>
<li><strong>Special mora</strong>: Handling of geminate consonants and long vowels</li>
<li><strong>Kanji reading</strong>: Context-dependent reading disambiguation</li>
</ul>
<h4>Major Japanese TTS Libraries</h4>
<table>
<thead>
<tr>
<th>System</th>
<th>Features</th>
<th>License</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>OpenJTalk</strong></td>
<td>HMM-based, lightweight</td>
<td>BSD</td>
</tr>
<tr>
<td><strong>VOICEVOX</strong></td>
<td>Deep learning, high quality</td>
<td>LGPL/Commercial</td>
</tr>
<tr>
<td><strong>ESPnet-TTS</strong></td>
<td>Research-oriented, latest methods</td>
<td>Apache 2.0</td>
</tr>
</tbody>
</table>
<h3>Implementation Example: gTTS (Google Text-to-Speech)</h3>
<pre><code class="language-python">from gtts import gTTS
import os
from IPython.display import Audio

# Text
text_en = "Hello, this is a demonstration of text-to-speech synthesis."
text_ja = "Hello, this is a demonstration of speech synthesis."

# English TTS
tts_en = gTTS(text=text_en, lang='en', slow=False)
tts_en.save("output_en.mp3")

# Japanese TTS
tts_ja = gTTS(text=text_ja, lang='ja', slow=False)
tts_ja.save("output_ja.mp3")

print("=== gTTS (Google Text-to-Speech) ===")
print("English audio generated: output_en.mp3")
print("Japanese audio generated: output_ja.mp3")

# Audio playback (Jupyter environment)
# display(Audio("output_en.mp3"))
# display(Audio("output_ja.mp3"))
</code></pre>
<hr/>
<h2>4.6 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li><p><strong>TTS Fundamentals</strong></p>
<ul>
<li>Text-to-Speech pipeline: Text analysis ‚Üí Acoustic model ‚Üí Vocoder</li>
<li>Importance of prosody: Pitch, duration, energy</li>
<li>Evaluation metrics: MOS, naturalness</li>
</ul></li>
<li><p><strong>Tacotron &amp; Tacotron 2</strong></p>
<ul>
<li>End-to-end TTS using Seq2Seq architecture</li>
<li>Text-speech alignment using attention mechanism</li>
<li>Improved stability with location-sensitive attention</li>
</ul></li>
<li><p><strong>FastSpeech</strong></p>
<ul>
<li>Speedup through non-autoregressive TTS</li>
<li>Explicit duration control using Duration Predictor</li>
<li>FastSpeech 2: Additional prediction of pitch and energy</li>
</ul></li>
<li><p><strong>Neural Vocoders</strong></p>
<ul>
<li>WaveNet: Highest quality but slow</li>
<li>WaveGlow: Parallel generation capable</li>
<li>HiFi-GAN: Balance of speed and quality</li>
</ul></li>
<li><p><strong>Latest Technologies</strong></p>
<ul>
<li>VITS: End-to-end integrated model</li>
<li>Voice Cloning: Voice reproduction from small amounts of data</li>
<li>Multi-speaker TTS: Multiple speakers with single model</li>
<li>Japanese TTS: Challenges of accent and intonation</li>
</ul></li>
</ol>
<h3>TTS Technology Selection Guidelines</h3>
<table>
<thead>
<tr>
<th>Purpose</th>
<th>Recommended Model</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>Highest quality</td>
<td>Tacotron 2 + WaveNet</td>
<td>MOS 4.5+</td>
</tr>
<tr>
<td>Real-time generation</td>
<td>FastSpeech 2 + HiFi-GAN</td>
<td>Fast &amp; high quality</td>
</tr>
<tr>
<td>End-to-end</td>
<td>VITS</td>
<td>Integrated architecture</td>
</tr>
<tr>
<td>Voice cloning</td>
<td>Speaker Embedding TTS</td>
<td>Works with small data</td>
</tr>
<tr>
<td>Research/experiment</td>
<td>ESPnet-TTS</td>
<td>Latest methods implemented</td>
</tr>
</tbody>
</table>
<h3>Next Chapter</h3>
<p>In Chapter 5, we'll learn about <strong>voice conversion and voice transformation</strong>:</p>
<ul>
<li>Voice Conversion fundamentals</li>
<li>Style transfer</li>
<li>Emotional expression control</li>
<li>Real-time voice conversion</li>
</ul>
<hr/>
<h2>Practice Problems</h2>
<h3>Problem 1 (Difficulty: easy)</h3>
<p>Explain the four main stages of the TTS pipeline in order, and describe the role of each stage.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<ol>
<li><p><strong>Text Analysis</strong></p>
<ul>
<li>Role: Text normalization, tokenization, expansion of numbers and abbreviations</li>
<li>Output: Normalized text</li>
</ul></li>
<li><p><strong>Linguistic Feature Extraction</strong></p>
<ul>
<li>Role: Convert text to phonemes, predict prosodic information</li>
<li>Output: Phoneme sequence and prosodic features</li>
</ul></li>
<li><p><strong>Acoustic Model</strong></p>
<ul>
<li>Role: Generate Mel-spectrogram from linguistic features</li>
<li>Output: Mel-spectrogram (acoustic features)</li>
</ul></li>
<li><p><strong>Vocoder</strong></p>
<ul>
<li>Role: Generate speech waveform from Mel-spectrogram</li>
<li>Output: Final speech waveform</li>
</ul></li>
</ol>
</details>
<h3>Problem 2 (Difficulty: medium)</h3>
<p>Compare the main differences between Tacotron 2 and FastSpeech from the perspectives of generation method, speed, and robustness.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Tacotron 2</th>
<th>FastSpeech</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Generation Method</strong></td>
<td>Autoregressive<br/>Predicts next using previous frame</td>
<td>Non-autoregressive<br/>Parallel generation of all frames</td>
</tr>
<tr>
<td><strong>Speed</strong></td>
<td>Slow (sequential generation)<br/>Baseline: 1x</td>
<td>Very fast (parallel generation)<br/>~38x faster</td>
</tr>
<tr>
<td><strong>Robustness</strong></td>
<td>Low<br/>- Word repetition<br/>- Skipping<br/>- Unstable alignment</td>
<td>High<br/>- Nearly error-free<br/>- Stable alignment<br/>- Predictable output</td>
</tr>
<tr>
<td><strong>Controllability</strong></td>
<td>Low<br/>Difficult to explicitly control speed/prosody</td>
<td>High<br/>Speed controllable via duration adjustment</td>
</tr>
<tr>
<td><strong>Quality (MOS)</strong></td>
<td>4.41 (high quality)</td>
<td>4.27 (slightly lower)</td>
</tr>
</tbody>
</table>
<p><strong>Conclusion</strong>: FastSpeech achieves significant speedup (38x) and robustness improvement with only a slight quality reduction (-3%). For practical applications, FastSpeech is advantageous.</p>
</details>
<h3>Problem 3 (Difficulty: medium)</h3>
<p>Compare WaveNet, WaveGlow, and HiFi-GAN vocoders from the perspectives of generation method, speed, and quality, and propose use cases for each.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Comparison Table</strong>:</p>
<table>
<thead>
<tr>
<th>Vocoder</th>
<th>Generation Method</th>
<th>Speed</th>
<th>Quality (MOS)</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>WaveNet</strong></td>
<td>Autoregressive<br/>Dilated Causal Conv</td>
<td>Very slow</td>
<td>4.5+ (highest)</td>
<td>- Highest quality<br/>- Not real-time</td>
</tr>
<tr>
<td><strong>WaveGlow</strong></td>
<td>Flow-based<br/>Invertible transformation</td>
<td>Medium</td>
<td>4.2-4.3</td>
<td>- Parallel generation<br/>- Stable training</td>
</tr>
<tr>
<td><strong>HiFi-GAN</strong></td>
<td>GAN<br/>Adversarial learning</td>
<td>Very fast</td>
<td>4.3-4.5</td>
<td>- Fast &amp; high quality<br/>- Training somewhat difficult</td>
</tr>
</tbody>
</table>
<p><strong>Use Case Proposals</strong>:</p>
<ol>
<li><p><strong>WaveNet</strong></p>
<ul>
<li>Use cases: Offline speech synthesis, highest quality priority</li>
<li>Examples: Studio-quality audiobook production</li>
</ul></li>
<li><p><strong>WaveGlow</strong></p>
<ul>
<li>Use cases: Research purposes, understanding flow-based models</li>
<li>Examples: Generative model research, combination with VAE</li>
</ul></li>
<li><p><strong>HiFi-GAN</strong></p>
<ul>
<li>Use cases: Real-time applications, practical systems</li>
<li>Examples: Voice assistants, live broadcast narration</li>
</ul></li>
</ol>
<p><strong>Recommendation</strong>: Currently, HiFi-GAN with its excellent balance of speed and quality is most widely used.</p>
</details>
<h3>Problem 4 (Difficulty: hard)</h3>
<p>Explain the roles of FastSpeech's Duration Predictor and Length Regulator, and describe why they are necessary for non-autoregressive TTS. Provide a simple implementation example in Python code.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Role Explanation</strong>:</p>
<ol>
<li><p><strong>Duration Predictor</strong></p>
<ul>
<li>Role: Predicts how many frames each phoneme lasts</li>
<li>Input: Encoder hidden states (phoneme-level)</li>
<li>Output: Duration of each phoneme (number of frames)</li>
</ul></li>
<li><p><strong>Length Regulator</strong></p>
<ul>
<li>Role: Expands phoneme-level representation to frame-level</li>
<li>Input: Phoneme hidden states + predicted durations</li>
<li>Output: Frame-level hidden states</li>
</ul></li>
</ol>
<p><strong>Necessity</strong>:</p>
<p>In non-autoregressive TTS, all frames are generated in parallel, so:</p>
<ul>
<li>Need to know output length in advance</li>
<li>Text (phonemes) and speech (frames) have different lengths</li>
<li>Each phoneme has different duration (e.g., "a" is 3 frames, "n" is 1 frame)</li>
</ul>
<p>By predicting duration with Duration Predictor and expanding phoneme representation to appropriate length with Length Regulator, parallel generation becomes possible.</p>
<p><strong>Implementation Example</strong>:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn

class SimpleDurationPredictor(nn.Module):
    """Duration predictor"""

    def __init__(self, hidden_size=256):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Conv1d(hidden_size, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.LayerNorm(256),
            nn.Dropout(0.5),
            nn.Conv1d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.LayerNorm(256),
            nn.Dropout(0.5)
        )
        self.output = nn.Linear(256, 1)

    def forward(self, x):
        # x: (B, T, hidden_size)
        x = x.transpose(1, 2)  # (B, hidden_size, T)
        x = self.layers(x)
        x = x.transpose(1, 2)  # (B, T, 256)
        duration = self.output(x).squeeze(-1)  # (B, T)
        return torch.relu(duration)  # Positive values only

class SimpleLengthRegulator(nn.Module):
    """Length Regulator"""

    def forward(self, x, durations):
        # x: (B, T_phoneme, hidden_size)
        # durations: (B, T_phoneme)

        output = []
        for batch_idx in range(x.size(0)):
            expanded = []
            for phoneme_idx in range(x.size(1)):
                dur = int(durations[batch_idx, phoneme_idx].item())
                if dur &gt; 0:
                    # Repeat phoneme hidden state dur times
                    phoneme_hidden = x[batch_idx, phoneme_idx].unsqueeze(0)
                    expanded.append(phoneme_hidden.expand(dur, -1))

            if expanded:
                output.append(torch.cat(expanded, dim=0))

        # Pad to same length
        max_len = max(seq.size(0) for seq in output)
        padded = []
        for seq in output:
            if seq.size(0) &lt; max_len:
                pad = torch.zeros(max_len - seq.size(0), seq.size(1))
                seq = torch.cat([seq, pad], dim=0)
            padded.append(seq.unsqueeze(0))

        return torch.cat(padded, dim=0)

# Test
print("=== Duration Predictor &amp; Length Regulator ===\n")

# Dummy data
batch_size, n_phonemes, hidden_size = 2, 5, 256
phoneme_hidden = torch.randn(batch_size, n_phonemes, hidden_size)

# Duration prediction
duration_predictor = SimpleDurationPredictor(hidden_size)
predicted_durations = duration_predictor(phoneme_hidden)

print(f"Phoneme hidden states: {phoneme_hidden.shape}")
print(f"Predicted durations: {predicted_durations.shape}")
print(f"Sample durations: {predicted_durations[0]}")

# Length Regulation
length_regulator = SimpleLengthRegulator()
frame_hidden = length_regulator(phoneme_hidden, predicted_durations)

print(f"\nFrame-level hidden states: {frame_hidden.shape}")
print(f"Expansion rate: {frame_hidden.size(1) / phoneme_hidden.size(1):.2f}x")

# Concrete example
print("\n=== Concrete Example ===")
print("Phonemes: ['k', 'o', 'n', 'n', 'i']")
print("Durations: [3, 4, 1, 1, 2] frames")
print("‚Üí Total 11 frames of speech generation")
</code></pre>
<p><strong>Output Example</strong>:</p>
<pre><code>=== Duration Predictor &amp; Length Regulator ===

Phoneme hidden states: torch.Size([2, 5, 256])
Predicted durations: torch.Size([2, 5])
Sample durations: tensor([2.3, 1.8, 3.1, 2.5, 1.2])

Frame-level hidden states: torch.Size([2, 11, 256])
Expansion rate: 2.20x

=== Concrete Example ===
Phonemes: ['k', 'o', 'n', 'n', 'i']
Durations: [3, 4, 1, 1, 2] frames
‚Üí Total 11 frames of speech generation
</code></pre>
</details>
<h3>Problem 5 (Difficulty: hard)</h3>
<p>Explain the role of Speaker Embedding in Multi-speaker TTS and describe how it is incorporated into the model. Also discuss its relationship with Voice Cloning.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Role of Speaker Embedding</strong>:</p>
<ol>
<li><p><strong>Representation of speaker characteristics</strong></p>
<ul>
<li>Represents each speaker with a low-dimensional vector (typically 64-512 dimensions)</li>
<li>Encodes speaker's voice quality, pitch, and speaking style characteristics</li>
</ul></li>
<li><p><strong>Conditioning</strong></p>
<ul>
<li>Provides speaker information to the TTS model</li>
<li>Enables generating different speakers' voices from the same text</li>
</ul></li>
</ol>
<p><strong>Methods of Incorporation into Model</strong>:</p>
<p><strong>Method 1: Embedding Lookup Table</strong></p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: Method 1: Embedding Lookup Table

Purpose: Demonstrate neural network implementation
Target: Advanced
Execution time: ~5 seconds
Dependencies: None
"""

import torch
import torch.nn as nn

class MultiSpeakerTTS(nn.Module):
    def __init__(self, n_speakers=100, speaker_embed_dim=256,
                 text_embed_dim=512):
        super().__init__()

        # Speaker Embedding
        self.speaker_embedding = nn.Embedding(n_speakers, speaker_embed_dim)

        # Text Encoder
        self.text_encoder = nn.LSTM(text_embed_dim, 512,
                                    num_layers=2, batch_first=True)

        # Speaker-conditioned Decoder
        self.decoder = nn.LSTM(512 + speaker_embed_dim, 512,
                              num_layers=2, batch_first=True)

        self.mel_projection = nn.Linear(512, 80)  # 80-dim Mel

    def forward(self, text_features, speaker_ids):
        # Get Speaker Embedding
        speaker_emb = self.speaker_embedding(speaker_ids)  # (B, speaker_dim)

        # Text Encoding
        text_encoded, _ = self.text_encoder(text_features)  # (B, T, 512)

        # Expand Speaker Embedding to all timesteps
        speaker_emb_expanded = speaker_emb.unsqueeze(1).expand(
            -1, text_encoded.size(1), -1
        )  # (B, T, speaker_dim)

        # Concatenate
        decoder_input = torch.cat([text_encoded, speaker_emb_expanded],
                                 dim=-1)  # (B, T, 512+speaker_dim)

        # Decode
        decoder_output, _ = self.decoder(decoder_input)

        # Mel prediction
        mel_output = self.mel_projection(decoder_output)

        return mel_output

# Test
model = MultiSpeakerTTS(n_speakers=100)
text_features = torch.randn(4, 50, 512)  # Batch=4, Seq=50
speaker_ids = torch.tensor([0, 5, 10, 15])  # Different speakers

mel_output = model(text_features, speaker_ids)
print("=== Multi-Speaker TTS ===")
print(f"Input text: {text_features.shape}")
print(f"Speaker IDs: {speaker_ids}")
print(f"Output Mel: {mel_output.shape}")
</code></pre>
<p><strong>Method 2: Speaker Encoder (for Voice Cloning)</strong></p>
<pre><code class="language-python">class SpeakerEncoder(nn.Module):
    """Extract speaker embedding from audio"""

    def __init__(self, mel_dim=80, embed_dim=256):
        super().__init__()
        self.lstm = nn.LSTM(mel_dim, 256, num_layers=3,
                           batch_first=True)
        self.projection = nn.Linear(256, embed_dim)

    def forward(self, mel_spectrograms):
        # mel_spectrograms: (B, T, 80)
        _, (hidden, _) = self.lstm(mel_spectrograms)
        # Use final layer hidden state
        speaker_emb = self.projection(hidden[-1])  # (B, embed_dim)
        # L2 normalization
        speaker_emb = speaker_emb / torch.norm(speaker_emb, dim=1, keepdim=True)
        return speaker_emb

# Voice Cloning workflow
speaker_encoder = SpeakerEncoder()

# Extract speaker embedding from reference audio
reference_mel = torch.randn(1, 100, 80)  # Few seconds of audio
speaker_emb = speaker_encoder(reference_mel)

print("\n=== Voice Cloning ===")
print(f"Reference audio: {reference_mel.shape}")
print(f"Extracted speaker embedding: {speaker_emb.shape}")
print("‚Üí Use this embedding to synthesize any text in this speaker's voice")
</code></pre>
<p><strong>Relationship with Voice Cloning</strong>:</p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Multi-speaker TTS</th>
<th>Voice Cloning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Speaker representation</strong></td>
<td>Embedding Lookup<br/>(Trained speakers only)</td>
<td>Speaker Encoder<br/>(Unknown speakers possible)</td>
</tr>
<tr>
<td><strong>Data required</strong></td>
<td>Large amount of data per speaker</td>
<td>Few seconds to minutes of audio</td>
</tr>
<tr>
<td><strong>Flexibility</strong></td>
<td>Low (fixed speaker set)</td>
<td>High (new speaker support)</td>
</tr>
<tr>
<td><strong>Quality</strong></td>
<td>High (optimized per speaker)</td>
<td>Medium-High (data dependent)</td>
</tr>
</tbody>
</table>
<p><strong>Integrated Approach</strong>:</p>
<p>Latest systems use a combination of both:</p>
<ol>
<li>Pre-train Multi-speaker TTS on large-scale data</li>
<li>Extract embedding for new speakers with Speaker Encoder</li>
<li>Fine-tune with small amount of additional data (Optional)</li>
</ol>
<p>This allows achieving both high quality for trained speakers and support for unknown speakers.</p>
</details>
<hr/>
<h2>References</h2>
<ol>
<li>Wang, Y., et al. (2017). "Tacotron: Towards End-to-End Speech Synthesis." <em>Interspeech 2017</em>.</li>
<li>Shen, J., et al. (2018). "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions." <em>ICASSP 2018</em>.</li>
<li>Ren, Y., et al. (2019). "FastSpeech: Fast, Robust and Controllable Text to Speech." <em>NeurIPS 2019</em>.</li>
<li>van den Oord, A., et al. (2016). "WaveNet: A Generative Model for Raw Audio." <em>arXiv:1609.03499</em>.</li>
<li>Prenger, R., Valle, R., &amp; Catanzaro, B. (2019). "WaveGlow: A Flow-based Generative Network for Speech Synthesis." <em>ICASSP 2019</em>.</li>
<li>Kong, J., Kim, J., &amp; Bae, J. (2020). "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis." <em>NeurIPS 2020</em>.</li>
<li>Kim, J., Kong, J., &amp; Son, J. (2021). "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech." <em>ICML 2021</em>.</li>
<li>Casanova, E., et al. (2022). "YourtTS: Towards Zero-Shot Multi-Speaker TTS." <em>arXiv:2112.02418</em>.</li>
</ol>
<div class="navigation">
<a class="nav-button" href="chapter3-deep-learning-asr.html">‚Üê Previous Chapter: Speech Recognition</a>
<a class="nav-button" href="index.html">Series Index</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes, and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranty, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the specified terms (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-21</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
