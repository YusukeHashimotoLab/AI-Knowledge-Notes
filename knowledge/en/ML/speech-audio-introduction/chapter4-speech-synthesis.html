<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 4 Chapterï¼šéŸ³å£°åˆæˆï¼ˆTTSï¼‰ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/speech-audio-introduction/index.html">Speech Audio</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 4</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 4 Chapterï¼šéŸ³å£°åˆæˆï¼ˆTTSï¼‰</h1>
            <p class="subtitle">ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è‡ªç„¶ãªéŸ³å£°ã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã®ç†è§£</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– Reading Time: 30-35 minutes</span>
                <span class="meta-item">ğŸ“Š Difficulty: Intermediate</span>
                <span class="meta-item">ğŸ’» Code Examples: 7</span>
                <span class="meta-item">ğŸ“ Exercises: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Learning Objectives</h2>
<p>ã“ã® ChapterReadã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… éŸ³å£°åˆæˆï¼ˆTTSï¼‰ã®åŸºæœ¬æ¦‚å¿µã¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Tacotron/Tacotron 2ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨å‹•ä½œåŸç†ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… FastSpeechã«ã‚ˆã‚‹éè‡ªå·±å›å¸°çš„TTSã®ä»•çµ„ã¿ã‚’å­¦ã¶</li>
<li>âœ… ä¸»è¦ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒœã‚³ãƒ¼ãƒ€ã®ç‰¹å¾´ã¨ä½¿ã„ minutesã‘ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… æœ€æ–°ã®TTSæŠ€è¡“ã¨ãã®å¿œç”¨ã‚’æŠŠæ¡ã™ã‚‹</li>
<li>âœ… Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ãŸéŸ³å£°åˆæˆã‚’å®Ÿè£…ã§ãã‚‹</li>
</ul>

<hr>

<h2>4.1 TTSã®åŸºç¤</h2>

<h3>Text-to-Speechï¼ˆTTSï¼‰ã¨ã¯</h3>
<p><strong>éŸ³å£°åˆæˆï¼ˆText-to-Speech, TTSï¼‰</strong>ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’è‡ªç„¶ãªéŸ³å£°ã«å¤‰æ›ã™ã‚‹æŠ€è¡“ã§ã™ã€‚è¿‘å¹´ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®ç™ºå±•ã«ã‚ˆã‚Šã€äººé–“ã«è¿‘ã„è‡ªç„¶ãªéŸ³å£°ãŒç”Ÿæˆå¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚</p>

<blockquote>
<p>ã€ŒTTSã¯ã€è¨€èªå­¦çš„ç†è§£ã¨éŸ³éŸ¿ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®çµ±åˆã«ã‚ˆã‚Šã€æ›¸ã‹ã‚ŒãŸè¨€è‘‰ã‚’è©±ã—è¨€è‘‰ã«å¤‰æ›ã—ã¾ã™ã€‚ã€</p>
</blockquote>

<h3>TTSãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h3>

<div class="mermaid">
graph LR
    A[ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›] --> B[ãƒ†ã‚­ã‚¹ãƒˆè§£æ]
    B --> C[è¨€èªç‰¹å¾´æŠ½å‡º]
    C --> D[éŸ³éŸ¿ãƒ¢ãƒ‡ãƒ«]
    D --> E[Mel-ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ]
    E --> F[Vocoder]
    F --> G[éŸ³å£°æ³¢å½¢]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#e8f5e9
    style F fill:#fce4ec
    style G fill:#c8e6c9
</div>

<h3>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å„æ®µéš</h3>

<table>
<thead>
<tr>
<th>æ®µéš</th>
<th>å½¹å‰²</th>
<th>å‡ºåŠ›</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ†ã‚­ã‚¹ãƒˆè§£æ</strong></td>
<td>æ­£è¦åŒ–ã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–</td>
<td>æ­£è¦åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ</td>
</tr>
<tr>
<td><strong>è¨€èªç‰¹å¾´æŠ½å‡º</strong></td>
<td>éŸ³ç´ å¤‰æ›ã€éŸ»å¾‹äºˆæ¸¬</td>
<td>è¨€èªç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«</td>
</tr>
<tr>
<td><strong>éŸ³éŸ¿ãƒ¢ãƒ‡ãƒ«</strong></td>
<td>Mel-ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ç”Ÿæˆ</td>
<td>Mel-ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ </td>
</tr>
<tr>
<td><strong>Vocoder</strong></td>
<td>æ³¢å½¢ç”Ÿæˆ</td>
<td>éŸ³å£°æ³¢å½¢</td>
</tr>
</tbody>
</table>

<h3>Vocoderã®å½¹å‰²</h3>

<p><strong>Vocoderï¼ˆãƒœã‚³ãƒ¼ãƒ€ãƒ¼ï¼‰</strong>ã¯ã€Mel-ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‹ã‚‰éŸ³å£°æ³¢å½¢ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã™ã€‚</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
import librosa
import librosa.display

# ã‚µãƒ³ãƒ—ãƒ«éŸ³å£°ã®èª­ã¿è¾¼ã¿
y, sr = librosa.load(librosa.example('trumpet'), sr=22050, duration=3)

# Mel-ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã®ç”Ÿæˆ
mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80)
mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 1, figsize=(14, 8))

# æ³¢å½¢
axes[0].plot(np.linspace(0, len(y)/sr, len(y)), y)
axes[0].set_xlabel(' hours (ç§’)')
axes[0].set_ylabel('æŒ¯å¹…')
axes[0].set_title('éŸ³å£°æ³¢å½¢', fontsize=14)
axes[0].grid(True, alpha=0.3)

# Mel-ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ 
img = librosa.display.specshow(mel_spec_db, x_axis='time', y_axis='mel',
                               sr=sr, ax=axes[1], cmap='viridis')
axes[1].set_title('Mel-ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ', fontsize=14)
fig.colorbar(img, ax=axes[1], format='%+2.0f dB')

plt.tight_layout()
plt.show()

print("=== TTSã«ãŠã‘ã‚‹Vocoderã®å½¹å‰² ===")
print(f"å…¥åŠ›: Mel-ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ  {mel_spec.shape}")
print(f"å‡ºåŠ›: éŸ³å£°æ³¢å½¢ {y.shape}")
print(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ: {sr} Hz")
</code></pre>

<h3>Prosodyï¼ˆéŸ»å¾‹ï¼‰ã¨è‡ªç„¶æ€§</h3>

<p><strong>éŸ»å¾‹ï¼ˆProsodyï¼‰</strong>ã¯ã€éŸ³å£°ã®è‡ªç„¶æ€§ã‚’æ±ºå®šã™ã‚‹é‡è¦ãªè¦ç´ ã§ã™ï¼š</p>

<ul>
<li><strong>ãƒ”ãƒƒãƒï¼ˆPitchï¼‰</strong>: éŸ³ã®é«˜ã•ã€æŠ‘æš</li>
<li><strong>ãƒ‡ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆDurationï¼‰</strong>: éŸ³ç´ ã®é•·ã•</li>
<li><strong>ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆEnergyï¼‰</strong>: éŸ³ã®å¼·ã•</li>
<li><strong>ãƒªã‚ºãƒ ï¼ˆRhythmï¼‰</strong>: ç™ºè©±ã®ãƒ†ãƒ³ãƒ</li>
</ul>

<h3>è©•ä¾¡æŒ‡æ¨™</h3>

<h4>1. MOSï¼ˆMean Opinion Scoreï¼‰</h4>

<p><strong>MOS</strong>ã¯ã€äººé–“ã«ã‚ˆã‚‹ä¸»è¦³è©•ä¾¡ã®å¹³å‡ã‚¹ã‚³ã‚¢ã§ã™ã€‚</p>

<table>
<thead>
<tr>
<th>ã‚¹ã‚³ã‚¢</th>
<th>å“è³ª</th>
<th>èª¬æ˜</th>
</tr>
</thead>
<tbody>
<tr>
<td>5</td>
<td>å„ªç§€</td>
<td>è‡ªç„¶ãªäººé–“ã®éŸ³å£°ã¨åŒºåˆ¥ä¸å¯</td>
</tr>
<tr>
<td>4</td>
<td>è‰¯å¥½</td>
<td>ã‚ãšã‹ãªä¸è‡ªç„¶ã•ãŒã‚ã‚‹</td>
</tr>
<tr>
<td>3</td>
<td>æ™®é€š</td>
<td>æ˜ã‚‰ã‹ã«åˆæˆéŸ³å£°ã ãŒç†è§£å¯èƒ½</td>
</tr>
<tr>
<td>2</td>
<td>åŠ£ã‚‹</td>
<td>èãå–ã‚Šã«ãã„éƒ¨ minutesãŒã‚ã‚‹</td>
</tr>
<tr>
<td>1</td>
<td>æ‚ªã„</td>
<td>ç†è§£å›°é›£</td>
</tr>
</tbody>
</table>

<h4>2. Naturalnessï¼ˆè‡ªç„¶æ€§ï¼‰</h4>

<p>éŸ³å£°ã®äººé–“ã‚‰ã—ã•ã‚’è©•ä¾¡ã—ã¾ã™ï¼š</p>
<ul>
<li>éŸ»å¾‹ã®è‡ªç„¶ã•</li>
<li>éŸ³è³ªã®æ»‘ã‚‰ã‹ã•</li>
<li>æ„Ÿæƒ…è¡¨ç¾ã®è±Šã‹ã•</li>
</ul>

<hr>

<h2>4.2 Tacotron & Tacotron 2</h2>

<h3>Tacotronã®æ¦‚è¦</h3>

<p><strong>Tacotron</strong>ã¯ã€Seq2Seqã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç”¨ã„ãŸåˆæœŸã®ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰TTSãƒ¢ãƒ‡ãƒ«ã§ã™ï¼ˆGoogleãŒ2017å¹´ã«ç™ºè¡¨ï¼‰ã€‚</p>

<h3>Seq2Seq for TTS</h3>

<div class="mermaid">
graph LR
    A[ãƒ†ã‚­ã‚¹ãƒˆ] --> B[Encoder]
    B --> C[Attention]
    C --> D[Decoder]
    D --> E[Mel-ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ]
    E --> F[Vocoder]
    F --> G[éŸ³å£°æ³¢å½¢]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#e8f5e9
    style F fill:#fce4ec
    style G fill:#c8e6c9
</div>

<h3>Attentionæ©Ÿæ§‹</h3>

<p><strong>Attention</strong>ã¯ã€ãƒ‡ã‚³ãƒ¼ãƒ€ãŒå„ã‚¹ãƒ†ãƒƒãƒ—ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®ã©ã®éƒ¨ minutesã«æ³¨ç›®ã™ã‚‹ã‹ã‚’æ±ºå®šã—ã¾ã™ã€‚</p>

<p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$</p>

<ul>
<li>$Q$: Queryï¼ˆãƒ‡ã‚³ãƒ¼ãƒ€ã®éš ã‚ŒçŠ¶æ…‹ï¼‰</li>
<li>$K$: Keyï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®éš ã‚ŒçŠ¶æ…‹ï¼‰</li>
<li>$V$: Valueï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®éš ã‚ŒçŠ¶æ…‹ï¼‰</li>
</ul>

<h3>Tacotron 2ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h3>

<p><strong>Tacotron 2</strong>ã¯ã€Tacotronã‚’æ”¹è‰¯ã—ã€ã‚ˆã‚Šé«˜å“è³ªãªéŸ³å£°ã‚’ç”Ÿæˆã—ã¾ã™ã€‚</p>

<h4>ä¸»ãªæ”¹è‰¯ç‚¹</h4>

<table>
<thead>
<tr>
<th>ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</th>
<th>Tacotron</th>
<th>Tacotron 2</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Encoder</strong></td>
<td>CBHG</td>
<td>Conv + Bi-LSTM</td>
</tr>
<tr>
<td><strong>Attention</strong></td>
<td>Basic</td>
<td>Location-sensitive</td>
</tr>
<tr>
<td><strong>Decoder</strong></td>
<td>GRU</td>
<td>LSTM + Prenet</td>
</tr>
<tr>
<td><strong>Vocoder</strong></td>
<td>Griffin-Lim</td>
<td>WaveNet</td>
</tr>
</tbody>
</table>

<h3>Mel-ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ äºˆæ¸¬</h3>

<p>Tacotron 2ã¯ã€80æ¬¡å…ƒã®Mel-ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">import torch
import numpy as np
import matplotlib.pyplot as plt

# Tacotron 2é¢¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãªMeläºˆæ¸¬ã®ãƒ‡ãƒ¢
# æ³¨: å®Ÿéš›ã®Tacotron 2ã¯éå¸¸ã«è¤‡é›‘ã§ã™

class SimpleTacotronDecoder(torch.nn.Module):
    def __init__(self, hidden_size=256, n_mels=80):
        super().__init__()
        self.prenet = torch.nn.Sequential(
            torch.nn.Linear(n_mels, 256),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.5),
            torch.nn.Linear(256, 128)
        )
        self.lstm = torch.nn.LSTM(hidden_size + 128, hidden_size,
                                  num_layers=2, batch_first=True)
        self.projection = torch.nn.Linear(hidden_size, n_mels)

    def forward(self, encoder_outputs, prev_mel):
        # Prenetå‡¦ç†
        prenet_out = self.prenet(prev_mel)

        # LSTMãƒ‡ã‚³ãƒ¼ãƒ€
        lstm_input = torch.cat([encoder_outputs, prenet_out], dim=-1)
        lstm_out, _ = self.lstm(lstm_input)

        # Mel-ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ äºˆæ¸¬
        mel_pred = self.projection(lstm_out)

        return mel_pred

# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
model = SimpleTacotronDecoder()
print("=== Tacotron 2é¢¨ãƒ‡ã‚³ãƒ¼ãƒ€ ===")
print(model)

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
batch_size, seq_len = 4, 10
encoder_out = torch.randn(batch_size, seq_len, 256)
prev_mel = torch.randn(batch_size, seq_len, 80)

# äºˆæ¸¬
mel_output = model(encoder_out, prev_mel)
print(f"\nå…¥åŠ›ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›: {encoder_out.shape}")
print(f"å…¥åŠ›Mel: {prev_mel.shape}")
print(f"å‡ºåŠ›Mel: {mel_output.shape}")
</code></pre>

<h3>Location-sensitive Attention</h3>

<p>Tacotron 2ã§ã¯ã€<strong>Location-sensitive Attention</strong>ã«ã‚ˆã‚Šã€å®‰å®šã—ãŸå˜èª¿ãªã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’å®Ÿç¾ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class LocationSensitiveAttention(nn.Module):
    """Tacotron 2ã®Location-sensitive Attention"""

    def __init__(self, attention_dim=128, n_location_filters=32,
                 location_kernel_size=31):
        super().__init__()
        self.W_query = nn.Linear(256, attention_dim, bias=False)
        self.W_keys = nn.Linear(256, attention_dim, bias=False)
        self.W_location = nn.Linear(n_location_filters, attention_dim, bias=False)
        self.location_conv = nn.Conv1d(2, n_location_filters,
                                       kernel_size=location_kernel_size,
                                       padding=(location_kernel_size - 1) // 2,
                                       bias=False)
        self.v = nn.Linear(attention_dim, 1, bias=False)

    def forward(self, query, keys, attention_weights_cat):
        """
        Args:
            query: ãƒ‡ã‚³ãƒ¼ãƒ€ã®éš ã‚ŒçŠ¶æ…‹ (B, 1, 256)
            keys: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ› (B, T, 256)
            attention_weights_cat: éå»ã®attention weights (B, 2, T)
        """
        # Location features
        location_features = self.location_conv(attention_weights_cat)
        location_features = location_features.transpose(1, 2)

        # Attentionè¨ˆç®—
        query_proj = self.W_query(query)  # (B, 1, attention_dim)
        keys_proj = self.W_keys(keys)     # (B, T, attention_dim)
        location_proj = self.W_location(location_features)  # (B, T, attention_dim)

        # ã‚¨ãƒãƒ«ã‚®ãƒ¼è¨ˆç®—
        energies = self.v(torch.tanh(query_proj + keys_proj + location_proj))
        energies = energies.squeeze(-1)  # (B, T)

        # Attention weights
        attention_weights = F.softmax(energies, dim=1)

        return attention_weights

# ãƒ†ã‚¹ãƒˆ
attention = LocationSensitiveAttention()
query = torch.randn(4, 1, 256)
keys = torch.randn(4, 100, 256)
prev_attention = torch.randn(4, 2, 100)

weights = attention(query, keys, prev_attention)
print("=== Location-sensitive Attention ===")
print(f"Attention weights shape: {weights.shape}")
print(f"Sum of weights: {weights.sum(dim=1)}")  # Should be ~1.0
</code></pre>

<h3>Tacotron 2ã®å®Ÿè£…ä¾‹ï¼ˆPyTorchï¼‰</h3>

<pre><code class="language-python"># Tacotron 2ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€éƒ¨ minutesã®ç°¡ç•¥ç‰ˆ
class TacotronEncoder(nn.Module):
    def __init__(self, num_chars=150, embedding_dim=512, hidden_size=256):
        super().__init__()
        self.embedding = nn.Embedding(num_chars, embedding_dim)

        # Convolution layers
        self.convolutions = nn.ModuleList([
            nn.Sequential(
                nn.Conv1d(embedding_dim if i == 0 else hidden_size,
                         hidden_size, kernel_size=5, padding=2),
                nn.BatchNorm1d(hidden_size),
                nn.ReLU(),
                nn.Dropout(0.5)
            ) for i in range(3)
        ])

        # Bi-directional LSTM
        self.lstm = nn.LSTM(hidden_size, hidden_size // 2,
                           num_layers=1, batch_first=True,
                           bidirectional=True)

    def forward(self, text_inputs):
        # Embedding
        x = self.embedding(text_inputs).transpose(1, 2)  # (B, C, T)

        # Convolutions
        for conv in self.convolutions:
            x = conv(x)

        # LSTM
        x = x.transpose(1, 2)  # (B, T, C)
        outputs, _ = self.lstm(x)

        return outputs

# ãƒ†ã‚¹ãƒˆ
encoder = TacotronEncoder()
text_input = torch.randint(0, 150, (4, 50))  # Batch of 4, length 50
encoder_output = encoder(text_input)

print("=== Tacotron 2 Encoder ===")
print(f"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ: {text_input.shape}")
print(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›: {encoder_output.shape}")
</code></pre>

<hr>

<h2>4.3 FastSpeech</h2>

<h3>éè‡ªå·±å›å¸°çš„TTSã®å‹•æ©Ÿ</h3>

<p>Tacotron 2ã®ã‚ˆã†ãª<strong>è‡ªå·±å›å¸°çš„ãƒ¢ãƒ‡ãƒ«</strong>ã®å•é¡Œç‚¹ï¼š</p>

<ul>
<li>ç”ŸæˆãŒé€æ¬¡çš„ã§é…ã„</li>
<li>ä¸å®‰å®šãªã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ</li>
<li>èªã®ç¹°ã‚Šè¿”ã—ã‚„ã‚¹ã‚­ãƒƒãƒ—</li>
</ul>

<p><strong>FastSpeech</strong>ã¯ã€ã“ã‚Œã‚‰ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹<strong>éè‡ªå·±å›å¸°çš„TTS</strong>ã§ã™ã€‚</p>

<h3>FastSpeechã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h3>

<div class="mermaid">
graph LR
    A[ãƒ†ã‚­ã‚¹ãƒˆ] --> B[Encoder]
    B --> C[Duration Predictor]
    C --> D[Length Regulator]
    B --> D
    D --> E[Decoder]
    E --> F[Mel-ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#e8f5e9
    style F fill:#c8e6c9
</div>

<h3>Duration Predictionï¼ˆç¶™ç¶š hoursäºˆæ¸¬ï¼‰</h3>

<p>FastSpeechã®æ ¸å¿ƒã¯ã€å„éŸ³ç´ ã®ç¶™ç¶š hoursã‚’æ˜ç¤ºçš„ã«äºˆæ¸¬ã™ã‚‹ã“ã¨ã§ã™ã€‚</p>

<p>$$
d_i = \text{DurationPredictor}(h_i)
$$</p>

<ul>
<li>$h_i$: éŸ³ç´ $i$ã®éš ã‚ŒçŠ¶æ…‹</li>
<li>$d_i$: éŸ³ç´ $i$ã®ç¶™ç¶š hoursï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼‰</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn

class DurationPredictor(nn.Module):
    """FastSpeechã®ç¶™ç¶š hoursäºˆæ¸¬å™¨"""

    def __init__(self, hidden_size=256, filter_size=256,
                 kernel_size=3, dropout=0.5):
        super().__init__()

        self.layers = nn.ModuleList([
            nn.Sequential(
                nn.Conv1d(hidden_size, filter_size, kernel_size,
                         padding=(kernel_size - 1) // 2),
                nn.ReLU(),
                nn.LayerNorm(filter_size),
                nn.Dropout(dropout)
            ) for _ in range(2)
        ])

        self.linear = nn.Linear(filter_size, 1)

    def forward(self, encoder_output):
        """
        Args:
            encoder_output: (B, T, hidden_size)
        Returns:
            duration: (B, T) - å„éŸ³ç´ ã®ç¶™ç¶š hours
        """
        x = encoder_output.transpose(1, 2)  # (B, C, T)

        for layer in self.layers:
            x = layer(x)

        x = x.transpose(1, 2)  # (B, T, C)
        duration = self.linear(x).squeeze(-1)  # (B, T)

        return duration

# ãƒ†ã‚¹ãƒˆ
duration_predictor = DurationPredictor()
encoder_out = torch.randn(4, 50, 256)  # Batch=4, Seq=50
durations = duration_predictor(encoder_out)

print("=== Duration Predictor ===")
print(f"å…¥åŠ›: {encoder_out.shape}")
print(f"äºˆæ¸¬ç¶™ç¶š hours: {durations.shape}")
print(f"ã‚µãƒ³ãƒ—ãƒ«ç¶™ç¶š hours: {durations[0, :10]}")
</code></pre>

<h3>Length Regulator</h3>

<p><strong>Length Regulator</strong>ã¯ã€äºˆæ¸¬ã•ã‚ŒãŸç¶™ç¶š hoursã«åŸºã¥ã„ã¦ã€éŸ³ç´ Levelã®éš ã‚ŒçŠ¶æ…‹ã‚’ãƒ•ãƒ¬ãƒ¼ãƒ Levelã«æ‹¡å¼µã—ã¾ã™ã€‚</p>

<pre><code class="language-python">class LengthRegulator(nn.Module):
    """FastSpeechã®Length Regulator"""

    def __init__(self):
        super().__init__()

    def forward(self, x, durations):
        """
        Args:
            x: éŸ³ç´ Levelã®éš ã‚ŒçŠ¶æ…‹ (B, T_phoneme, C)
            durations: å„éŸ³ç´ ã®ç¶™ç¶š hours (B, T_phoneme)
        Returns:
            expanded: ãƒ•ãƒ¬ãƒ¼ãƒ Levelã®éš ã‚ŒçŠ¶æ…‹ (B, T_frame, C)
        """
        output = []
        for batch_idx in range(x.size(0)):
            expanded = []
            for phoneme_idx in range(x.size(1)):
                # å„éŸ³ç´ ã‚’ç¶™ç¶š hours minutesã ã‘ç¹°ã‚Šè¿”ã™
                duration = int(durations[batch_idx, phoneme_idx].item())
                expanded.append(x[batch_idx, phoneme_idx].unsqueeze(0).expand(duration, -1))

            if expanded:
                output.append(torch.cat(expanded, dim=0))

        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦åŒã˜é•·ã•ã«ã™ã‚‹
        max_len = max([seq.size(0) for seq in output])
        padded_output = []
        for seq in output:
            pad_len = max_len - seq.size(0)
            if pad_len > 0:
                padding = torch.zeros(pad_len, seq.size(1))
                seq = torch.cat([seq, padding], dim=0)
            padded_output.append(seq.unsqueeze(0))

        return torch.cat(padded_output, dim=0)

# ãƒ†ã‚¹ãƒˆ
length_regulator = LengthRegulator()
phoneme_hidden = torch.randn(2, 10, 256)  # 2 samples, 10 phonemes
durations = torch.tensor([[3, 2, 4, 1, 5, 2, 3, 2, 1, 4],
                          [2, 3, 2, 5, 1, 4, 2, 3, 2, 1]], dtype=torch.float32)

frame_hidden = length_regulator(phoneme_hidden, durations)
print("=== Length Regulator ===")
print(f"éŸ³ç´ Level: {phoneme_hidden.shape}")
print(f"äºˆæ¸¬ç¶™ç¶š hours: {durations}")
print(f"ãƒ•ãƒ¬ãƒ¼ãƒ Level: {frame_hidden.shape}")
</code></pre>

<h3>FastSpeech 2ã®æ”¹è‰¯</h3>

<p><strong>FastSpeech 2</strong>ã¯ã€ã‚ˆã‚Šå¤šãã®éŸ»å¾‹æƒ…å ±ã‚’äºˆæ¸¬ã—ã¾ã™ï¼š</p>

<table>
<thead>
<tr>
<th>äºˆæ¸¬å¯¾è±¡</th>
<th>FastSpeech</th>
<th>FastSpeech 2</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Duration</strong></td>
<td>âœ“</td>
<td>âœ“</td>
</tr>
<tr>
<td><strong>Pitch</strong></td>
<td>-</td>
<td>âœ“</td>
</tr>
<tr>
<td><strong>Energy</strong></td>
<td>-</td>
<td>âœ“</td>
</tr>
<tr>
<td><strong>å­¦ç¿’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ</strong></td>
<td>Teacherå¼·åˆ¶</td>
<td>Ground truth</td>
</tr>
</tbody>
</table>

<h3>Speed vs Quality</h3>

<p>FastSpeechã®åˆ©ç‚¹ï¼š</p>

<table>
<thead>
<tr>
<th>æŒ‡æ¨™</th>
<th>Tacotron 2</th>
<th>FastSpeech</th>
<th>æ”¹å–„ç‡</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ç”Ÿæˆé€Ÿåº¦</strong></td>
<td>1x</td>
<td>38x</td>
<td>38å€é«˜é€Ÿ</td>
</tr>
<tr>
<td><strong>MOS</strong></td>
<td>4.41</td>
<td>4.27</td>
<td>-3%</td>
</tr>
<tr>
<td><strong>ãƒ­ãƒã‚¹ãƒˆæ€§</strong></td>
<td>ä½</td>
<td>é«˜</td>
<td>ã‚¨ãƒ©ãƒ¼ç‡ã»ã¼0%</td>
</tr>
<tr>
<td><strong>åˆ¶å¾¡æ€§</strong></td>
<td>ä½</td>
<td>é«˜</td>
<td>é€Ÿåº¦èª¿æ•´å¯èƒ½</td>
</tr>
</tbody>
</table>

<blockquote>
<p><strong>é‡è¦</strong>: FastSpeechã¯ã€ã‚ãšã‹ãªå“è³ªä½ä¸‹ã§å¤§å¹…ãªé«˜é€ŸåŒ–ã¨ãƒ­ãƒã‚¹ãƒˆæ€§å‘ä¸Šã‚’å®Ÿç¾ã—ã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>4.4 Neural Vocoders</h2>

<h3>Vocoderã®é€²åŒ–</h3>

<p>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒœã‚³ãƒ¼ãƒ€ã¯ã€Mel-ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‹ã‚‰é«˜å“è³ªãªéŸ³å£°æ³¢å½¢ã‚’ç”Ÿæˆã—ã¾ã™ã€‚</p>

<h3>1. WaveNet</h3>

<p><strong>WaveNet</strong>ã¯ã€è‡ªå·±å›å¸°çš„ãªGenerative Modelsã§ã€éå¸¸ã«é«˜å“è³ªãªéŸ³å£°ã‚’ç”Ÿæˆã—ã¾ã™ï¼ˆDeepMindã€2016å¹´ï¼‰ã€‚</p>

<h4>Dilated Causal Convolution</h4>

<p>WaveNetã®æ ¸å¿ƒã¯ã€<strong>Dilated Causal Convolution</strong>ã§ã™ã€‚</p>

<p>$$
y_t = f\left(\sum_{i=0}^{k-1} w_i \cdot x_{t-d \cdot i}\right)
$$</p>

<ul>
<li>$d$: Dilation factorï¼ˆæ‹¡å¼µä¿‚æ•°ï¼‰</li>
<li>$k$: Kernel size</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn

class DilatedCausalConv1d(nn.Module):
    """WaveNetã®Dilated Causal Convolution"""

    def __init__(self, in_channels, out_channels, kernel_size, dilation):
        super().__init__()
        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size,
                             padding=(kernel_size - 1) * dilation,
                             dilation=dilation)

    def forward(self, x):
        # Causal: æœªæ¥ã®æƒ…å ±ã‚’ä½¿ã‚ãªã„
        output = self.conv(x)
        # å³å´ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’å‰Šé™¤
        return output[:, :, :x.size(2)]

class WaveNetBlock(nn.Module):
    """WaveNetã®æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯"""

    def __init__(self, residual_channels, gate_channels, skip_channels,
                 kernel_size, dilation):
        super().__init__()

        self.dilated_conv = DilatedCausalConv1d(
            residual_channels, gate_channels, kernel_size, dilation
        )

        self.conv_1x1_skip = nn.Conv1d(gate_channels // 2, skip_channels, 1)
        self.conv_1x1_res = nn.Conv1d(gate_channels // 2, residual_channels, 1)

    def forward(self, x):
        # Dilated convolution
        h = self.dilated_conv(x)

        # Gated activation
        tanh_out, sigmoid_out = h.chunk(2, dim=1)
        h = torch.tanh(tanh_out) * torch.sigmoid(sigmoid_out)

        # Skip connection
        skip = self.conv_1x1_skip(h)

        # Residual connection
        residual = self.conv_1x1_res(h)
        return (x + residual) * 0.707, skip  # sqrt(0.5) for scaling

# ãƒ†ã‚¹ãƒˆ
block = WaveNetBlock(residual_channels=64, gate_channels=128,
                     skip_channels=64, kernel_size=3, dilation=2)
x = torch.randn(4, 64, 1000)  # (B, C, T)
residual, skip = block(x)

print("=== WaveNet Block ===")
print(f"å…¥åŠ›: {x.shape}")
print(f"æ®‹å·®å‡ºåŠ›: {residual.shape}")
print(f"ã‚¹ã‚­ãƒƒãƒ—å‡ºåŠ›: {skip.shape}")
</code></pre>

<h3>2. WaveGlow</h3>

<p><strong>WaveGlow</strong>ã¯ã€Flow-basedGenerative Modelsã§ã€ä¸¦åˆ—ç”ŸæˆãŒå¯èƒ½ã§ã™ï¼ˆNVIDIAã€2018å¹´ï¼‰ã€‚</p>

<h4>ç‰¹å¾´</h4>

<ul>
<li><strong>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆ</strong>: WaveNetã‚ˆã‚Šé«˜é€Ÿ</li>
<li><strong>ä¸¦åˆ—åŒ–å¯èƒ½</strong>: å…¨ã‚µãƒ³ãƒ—ãƒ«ã‚’ä¸€åº¦ã«ç”Ÿæˆ</li>
<li><strong>å¯é€†å¤‰æ›</strong>: éŸ³å£°ã¨latentå¤‰æ•°ã®åŒæ–¹å‘å¤‰æ›</li>
</ul>

<h3>3. HiFi-GAN</h3>

<p><strong>HiFi-GAN</strong>ï¼ˆHigh Fidelity GANï¼‰ã¯ã€GANãƒ™ãƒ¼ã‚¹ã®é«˜é€Ÿãƒ»é«˜å“è³ªãƒœã‚³ãƒ¼ãƒ€ã§ã™ï¼ˆ2020å¹´ï¼‰ã€‚</p>

<h4>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h4>

<table>
<thead>
<tr>
<th>ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</th>
<th>èª¬æ˜</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Generator</strong></td>
<td>Transposed Convolutionã«ã‚ˆã‚‹ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</td>
</tr>
<tr>
<td><strong>Multi-Period Discriminator</strong></td>
<td>ç•°ãªã‚‹å‘¨æœŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è­˜åˆ¥</td>
</tr>
<tr>
<td><strong>Multi-Scale Discriminator</strong></td>
<td>ç•°ãªã‚‹è§£åƒåº¦ã§è­˜åˆ¥</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">import torch
import torch.nn as nn

class HiFiGANGenerator(nn.Module):
    """HiFi-GANã®Generatorï¼ˆç°¡ç•¥ç‰ˆï¼‰"""

    def __init__(self, mel_channels=80, upsample_rates=[8, 8, 2, 2]):
        super().__init__()

        self.num_upsamples = len(upsample_rates)

        # åˆæœŸConv
        self.conv_pre = nn.Conv1d(mel_channels, 512, 7, padding=3)

        # Upsampling layers
        self.ups = nn.ModuleList()
        for i, u in enumerate(upsample_rates):
            self.ups.append(nn.ConvTranspose1d(
                512 // (2 ** i),
                512 // (2 ** (i + 1)),
                u * 2,
                stride=u,
                padding=u // 2
            ))

        # æœ€çµ‚Conv
        self.conv_post = nn.Conv1d(512 // (2 ** len(upsample_rates)),
                                   1, 7, padding=3)

    def forward(self, mel):
        """
        Args:
            mel: Mel-ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ  (B, mel_channels, T)
        Returns:
            audio: éŸ³å£°æ³¢å½¢ (B, 1, T * prod(upsample_rates))
        """
        x = self.conv_pre(mel)

        for i in range(self.num_upsamples):
            x = torch.nn.functional.leaky_relu(x, 0.1)
            x = self.ups[i](x)

        x = torch.nn.functional.leaky_relu(x, 0.1)
        x = self.conv_post(x)
        x = torch.tanh(x)

        return x

# ãƒ†ã‚¹ãƒˆ
generator = HiFiGANGenerator()
mel_input = torch.randn(2, 80, 100)  # (B, mel_channels, T)
audio_output = generator(mel_input)

print("=== HiFi-GAN Generator ===")
print(f"å…¥åŠ›Mel: {mel_input.shape}")
print(f"å‡ºåŠ›éŸ³å£°: {audio_output.shape}")
print(f"ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡: {audio_output.size(2) / mel_input.size(2):.0f}x")
</code></pre>

<h3>Vocoderã®æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>Vocoder</th>
<th>ç”Ÿæˆæ–¹å¼</th>
<th>é€Ÿåº¦</th>
<th>å“è³ªï¼ˆMOSï¼‰</th>
<th>ç‰¹å¾´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Griffin-Lim</strong></td>
<td>åå¾©ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </td>
<td>é«˜é€Ÿ</td>
<td>3.0-3.5</td>
<td>ã‚·ãƒ³ãƒ—ãƒ«ã€å“è³ªä½</td>
</tr>
<tr>
<td><strong>WaveNet</strong></td>
<td>è‡ªå·±å›å¸°</td>
<td>éå¸¸ã«é…ã„</td>
<td>4.5+</td>
<td>æœ€é«˜å“è³ª</td>
</tr>
<tr>
<td><strong>WaveGlow</strong></td>
<td>Flow-based</td>
<td>ä¸­é€Ÿ</td>
<td>4.2-4.3</td>
<td>ä¸¦åˆ—ç”Ÿæˆå¯èƒ½</td>
</tr>
<tr>
<td><strong>HiFi-GAN</strong></td>
<td>GAN</td>
<td>éå¸¸ã«é«˜é€Ÿ</td>
<td>4.3-4.5</td>
<td>é«˜é€Ÿãƒ»é«˜å“è³ª</td>
</tr>
</tbody>
</table>

<blockquote>
<p><strong>Recommended</strong>: ç¾åœ¨ã§ã¯ã€HiFi-GANãŒé€Ÿåº¦ã¨å“è³ªã®ãƒãƒ©ãƒ³ã‚¹ãŒæœ€è‰¯ã§åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>4.5 æœ€æ–°ã®TTSæŠ€è¡“</h2>

<h3>1. VITSï¼ˆEnd-to-End TTSï¼‰</h3>

<p><strong>VITS</strong>ï¼ˆVariational Inference with adversarial learning for end-to-end Text-to-Speechï¼‰ã¯ã€éŸ³éŸ¿ãƒ¢ãƒ‡ãƒ«ã¨ãƒœã‚³ãƒ¼ãƒ€ã‚’çµ±åˆã—ãŸã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ¢ãƒ‡ãƒ«ã§ã™ï¼ˆ2021å¹´ï¼‰ã€‚</p>

<h4>VITSã®ç‰¹å¾´</h4>

<ul>
<li><strong>çµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</strong>: éŸ³éŸ¿ãƒ¢ãƒ‡ãƒ« + Vocoder</li>
<li><strong>VAE + GAN</strong>: Variational Autoencoderã¨æ•µå¯¾çš„å­¦ç¿’ã®çµ„ã¿åˆã‚ã›</li>
<li><strong>é«˜é€Ÿãƒ»é«˜å“è³ª</strong>: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆå¯èƒ½</li>
<li><strong>å¤šæ§˜æ€§</strong>: åŒã˜ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å¤šæ§˜ãªéŸ³å£°ã‚’ç”Ÿæˆ</li>
</ul>

<div class="mermaid">
graph LR
    A[ãƒ†ã‚­ã‚¹ãƒˆ] --> B[Text Encoder]
    B --> C[Posterior Encoder]
    B --> D[Prior Encoder]
    C --> E[Latent Variable z]
    D --> E
    E --> F[Decoder/Generator]
    F --> G[éŸ³å£°æ³¢å½¢]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#f3e5f5
    style E fill:#e3f2fd
    style F fill:#e8f5e9
    style G fill:#c8e6c9
</div>

<h3>2. Voice Cloningï¼ˆéŸ³å£°ã‚¯ãƒ­ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰</h3>

<p><strong>Voice Cloning</strong>ã¯ã€å°‘é‡ã®éŸ³å£°ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰ç‰¹å®šã®è©± learnerã®å£°ã‚’å†ç¾ã™ã‚‹æŠ€è¡“ã§ã™ã€‚</p>

<h4>ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</h4>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>èª¬æ˜</th>
<th>å¿…è¦ãƒ‡ãƒ¼ã‚¿</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Speaker Adaptation</strong></td>
<td>æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’å°‘é‡ãƒ‡ãƒ¼ã‚¿ã§å¾®èª¿æ•´</td>
<td>æ•° minutesï½æ•°å minutes</td>
</tr>
<tr>
<td><strong>Speaker Embedding</strong></td>
<td>è©± learneråŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å­¦ç¿’</td>
<td>æ•°ç§’ï½æ•° minutes</td>
</tr>
<tr>
<td><strong>Zero-shot TTS</strong></td>
<td>æœªçŸ¥è©± learnerã®å£°ã‚’å³åº§ã«æ¨¡å€£</td>
<td>æ•°ç§’</td>
</tr>
</tbody>
</table>

<h3>3. Multi-speaker TTS</h3>

<p><strong>Multi-speaker TTS</strong>ã¯ã€å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã§è¤‡æ•°ã®è©± learnerã®å£°ã‚’ç”Ÿæˆã—ã¾ã™ã€‚</p>

<h4>Speaker Embedding</h4>

<p>è©± learnerIDã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã—ã€ãƒ¢ãƒ‡ãƒ«ã«æ¡ä»¶ä»˜ã‘ã—ã¾ã™ã€‚</p>

<p>$$
e_{\text{speaker}} = \text{Embedding}(\text{speaker\_id})
$$</p>

<p>$$
h = f(x, e_{\text{speaker}})
$$</p>

<h3>4. Japanese TTS Systems</h3>

<p>æ—¥æœ¬èªTTSã‚·ã‚¹ãƒ†ãƒ ã®ç‰¹å¾´ï¼š</p>

<h4>æ—¥æœ¬èªç‰¹æœ‰ã®èª²é¡Œ</h4>

<ul>
<li><strong>ã‚¢ã‚¯ã‚»ãƒ³ãƒˆ</strong>: é«˜ä½ã‚¢ã‚¯ã‚»ãƒ³ãƒˆã®å†ç¾</li>
<li><strong>ã‚¤ãƒ³ãƒˆãƒãƒ¼ã‚·ãƒ§ãƒ³</strong>: æ–‡æœ«ã®ä¸Šæ˜‡ãƒ»ä¸‹é™</li>
<li><strong>ä¿ƒéŸ³ãƒ»é•·éŸ³</strong>: ç‰¹æ®Šæ‹ã®æ‰±ã„</li>
<li><strong>æ¼¢å­—èª­ã¿</strong>: æ–‡è„ˆä¾å­˜ã®èª­ã¿ minutesã‘</li>
</ul>

<h4>ä¸»è¦ãªæ—¥æœ¬èªTTSãƒ©ã‚¤ãƒ–ãƒ©ãƒª</h4>

<table>
<thead>
<tr>
<th>ã‚·ã‚¹ãƒ†ãƒ </th>
<th>ç‰¹å¾´</th>
<th>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>OpenJTalk</strong></td>
<td>HMM-basedã€è»½é‡</td>
<td>BSD</td>
</tr>
<tr>
<td><strong>VOICEVOX</strong></td>
<td>ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã€é«˜å“è³ª</td>
<td>LGPL/Commercial</td>
</tr>
<tr>
<td><strong>ESPnet-TTS</strong></td>
<td>ç ”ç©¶ç”¨ã€æœ€æ–°æ‰‹æ³•å®Ÿè£…</td>
<td>Apache 2.0</td>
</tr>
</tbody>
</table>

<h3>å®Ÿè£…ä¾‹ï¼šgTTSï¼ˆGoogle Text-to-Speechï¼‰</h3>

<pre><code class="language-python">from gtts import gTTS
import os
from IPython.display import Audio

# ãƒ†ã‚­ã‚¹ãƒˆ
text_en = "Hello, this is a demonstration of text-to-speech synthesis."
text_ja = "ã“ã‚“ã«ã¡ã¯ã€ã“ã‚Œã¯éŸ³å£°åˆæˆã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã™ã€‚"

# è‹±èªTTS
tts_en = gTTS(text=text_en, lang='en', slow=False)
tts_en.save("output_en.mp3")

# æ—¥æœ¬èªTTS
tts_ja = gTTS(text=text_ja, lang='ja', slow=False)
tts_ja.save("output_ja.mp3")

print("=== gTTSï¼ˆGoogle Text-to-Speechï¼‰===")
print("è‹±èªéŸ³å£°ã‚’ç”Ÿæˆ: output_en.mp3")
print("æ—¥æœ¬èªéŸ³å£°ã‚’ç”Ÿæˆ: output_ja.mp3")

# éŸ³å£°ã®å†ç”Ÿï¼ˆJupyterç’°å¢ƒï¼‰
# display(Audio("output_en.mp3"))
# display(Audio("output_ja.mp3"))
</code></pre>

<hr>

<h2>4.6 æœ¬ Chapterã®Summary</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>TTSã®åŸºç¤</strong></p>
<ul>
<li>Text-to-Speechãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: ãƒ†ã‚­ã‚¹ãƒˆè§£æ â†’ éŸ³éŸ¿ãƒ¢ãƒ‡ãƒ« â†’ Vocoder</li>
<li>éŸ»å¾‹ï¼ˆProsodyï¼‰ã®é‡è¦æ€§: ãƒ”ãƒƒãƒã€ãƒ‡ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼</li>
<li>è©•ä¾¡æŒ‡æ¨™: MOSã€è‡ªç„¶æ€§</li>
</ul></li>

<li><p><strong>Tacotron & Tacotron 2</strong></p>
<ul>
<li>Seq2Seqã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚‹ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰TTS</li>
<li>Attentionæ©Ÿæ§‹ã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¨éŸ³å£°ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ</li>
<li>Location-sensitive Attentionã«ã‚ˆã‚‹å®‰å®šæ€§å‘ä¸Š</li>
</ul></li>

<li><p><strong>FastSpeech</strong></p>
<ul>
<li>éè‡ªå·±å›å¸°çš„TTSã«ã‚ˆã‚‹é«˜é€ŸåŒ–</li>
<li>Duration Predictorã«ã‚ˆã‚‹æ˜ç¤ºçš„ãªç¶™ç¶š hoursåˆ¶å¾¡</li>
<li>FastSpeech 2: ãƒ”ãƒƒãƒã¨ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®è¿½åŠ äºˆæ¸¬</li>
</ul></li>

<li><p><strong>Neural Vocoders</strong></p>
<ul>
<li>WaveNet: æœ€é«˜å“è³ªã ãŒé…ã„</li>
<li>WaveGlow: ä¸¦åˆ—ç”Ÿæˆå¯èƒ½</li>
<li>HiFi-GAN: é«˜é€Ÿãƒ»é«˜å“è³ªã®ãƒãƒ©ãƒ³ã‚¹</li>
</ul></li>

<li><p><strong>æœ€æ–°æŠ€è¡“</strong></p>
<ul>
<li>VITS: ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰çµ±åˆãƒ¢ãƒ‡ãƒ«</li>
<li>Voice Cloning: å°‘é‡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®éŸ³å£°å†ç¾</li>
<li>Multi-speaker TTS: å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã§è¤‡æ•°è©± learner</li>
<li>æ—¥æœ¬èªTTS: ã‚¢ã‚¯ã‚»ãƒ³ãƒˆãƒ»ã‚¤ãƒ³ãƒˆãƒãƒ¼ã‚·ãƒ§ãƒ³ã®èª²é¡Œ</li>
</ul></li>
</ol>

<h3>TTSæŠ€è¡“ã®é¸æŠã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³</h3>

<table>
<thead>
<tr>
<th>ç›®çš„</th>
<th>Recommendedãƒ¢ãƒ‡ãƒ«</th>
<th>ç†ç”±</th>
</tr>
</thead>
<tbody>
<tr>
<td>æœ€é«˜å“è³ª</td>
<td>Tacotron 2 + WaveNet</td>
<td>MOS 4.5+</td>
</tr>
<tr>
<td>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆ</td>
<td>FastSpeech 2 + HiFi-GAN</td>
<td>é«˜é€Ÿãƒ»é«˜å“è³ª</td>
</tr>
<tr>
<td>ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰</td>
<td>VITS</td>
<td>çµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</td>
</tr>
<tr>
<td>éŸ³å£°ã‚¯ãƒ­ãƒ¼ãƒ‹ãƒ³ã‚°</td>
<td>Speaker Embedding TTS</td>
<td>å°‘é‡ãƒ‡ãƒ¼ã‚¿ã§å¯¾å¿œ</td>
</tr>
<tr>
<td>ç ”ç©¶ãƒ»å®Ÿé¨“</td>
<td>ESPnet-TTS</td>
<td>æœ€æ–°æ‰‹æ³•å®Ÿè£…</td>
</tr>
</tbody>
</table>

<h3>Next Chapterã¸</h3>

<p>Chapter 5 Chapterã§ã¯ã€<strong>éŸ³å£°å¤‰æ›ã¨ãƒœã‚¤ã‚¹ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>ã‚’å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>Voice Conversionã®åŸºç¤</li>
<li>ã‚¹ã‚¿ã‚¤ãƒ«è»¢é€</li>
<li>æ„Ÿæƒ…è¡¨ç¾ã®åˆ¶å¾¡</li>
<li>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°å¤‰æ›</li>
</ul>

<hr>

<h2>Exercises</h2>

<h3>å•é¡Œ1ï¼ˆDifficultyï¼šeasyï¼‰</h3>
<p>TTSãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä¸»è¦ãª4ã¤ã®æ®µéšã‚’é †ç•ªã«èª¬æ˜ã—ã€å„æ®µéšã®å½¹å‰²ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<ol>
<li><p><strong>ãƒ†ã‚­ã‚¹ãƒˆè§£æ</strong></p>
<ul>
<li>å½¹å‰²: ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–ã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã€æ•°å­—ã‚„ç•¥èªã®å±•é–‹</li>
<li>å‡ºåŠ›: æ­£è¦åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ</li>
</ul></li>

<li><p><strong>è¨€èªç‰¹å¾´æŠ½å‡º</strong></p>
<ul>
<li>å½¹å‰²: ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³ç´ ã«å¤‰æ›ã€éŸ»å¾‹æƒ…å ±ã®äºˆæ¸¬</li>
<li>å‡ºåŠ›: éŸ³ç´ åˆ—ã¨éŸ»å¾‹ç‰¹å¾´</li>
</ul></li>

<li><p><strong>éŸ³éŸ¿ãƒ¢ãƒ‡ãƒ«</strong></p>
<ul>
<li>å½¹å‰²: è¨€èªç‰¹å¾´ã‹ã‚‰Mel-ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’ç”Ÿæˆ</li>
<li>å‡ºåŠ›: Mel-ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ï¼ˆéŸ³éŸ¿ç‰¹å¾´ï¼‰</li>
</ul></li>

<li><p><strong>Vocoder</strong></p>
<ul>
<li>å½¹å‰²: Mel-ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‹ã‚‰éŸ³å£°æ³¢å½¢ã‚’ç”Ÿæˆ</li>
<li>å‡ºåŠ›: æœ€çµ‚çš„ãªéŸ³å£°æ³¢å½¢</li>
</ul></li>
</ol>

</details>

<h3>å•é¡Œ2ï¼ˆDifficultyï¼šmediumï¼‰</h3>
<p>Tacotron 2ã¨FastSpeechã®ä¸»ãªé•ã„ã‚’ã€ç”Ÿæˆæ–¹å¼ã€é€Ÿåº¦ã€ãƒ­ãƒã‚¹ãƒˆæ€§ã®è¦³ç‚¹ã‹ã‚‰æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<table>
<thead>
<tr>
<th>è¦³ç‚¹</th>
<th>Tacotron 2</th>
<th>FastSpeech</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ç”Ÿæˆæ–¹å¼</strong></td>
<td>è‡ªå·±å›å¸°çš„ï¼ˆAutoregressiveï¼‰<br>å‰ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½¿ã£ã¦æ¬¡ã‚’äºˆæ¸¬</td>
<td>éè‡ªå·±å›å¸°çš„ï¼ˆNon-autoregressiveï¼‰<br>å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¸¦åˆ—ç”Ÿæˆ</td>
</tr>
<tr>
<td><strong>é€Ÿåº¦</strong></td>
<td>é…ã„ï¼ˆé€æ¬¡ç”Ÿæˆï¼‰<br>åŸºæº–: 1x</td>
<td>éå¸¸ã«é«˜é€Ÿï¼ˆä¸¦åˆ—ç”Ÿæˆï¼‰<br>ç´„38å€é«˜é€Ÿ</td>
</tr>
<tr>
<td><strong>ãƒ­ãƒã‚¹ãƒˆæ€§</strong></td>
<td>ä½ã„<br>- å˜èªã®ç¹°ã‚Šè¿”ã—<br>- ã‚¹ã‚­ãƒƒãƒ—<br>- ä¸å®‰å®šãªã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ</td>
<td>é«˜ã„<br>- ã»ã¼ã‚¨ãƒ©ãƒ¼ãªã—<br>- å®‰å®šã—ãŸã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ<br>- äºˆæ¸¬å¯èƒ½ãªå‡ºåŠ›</td>
</tr>
<tr>
<td><strong>åˆ¶å¾¡æ€§</strong></td>
<td>ä½ã„<br>é€Ÿåº¦ãƒ»éŸ»å¾‹ã®æ˜ç¤ºçš„åˆ¶å¾¡å›°é›£</td>
<td>é«˜ã„<br>Durationèª¿æ•´ã§é€Ÿåº¦åˆ¶å¾¡å¯èƒ½</td>
</tr>
<tr>
<td><strong>å“è³ªï¼ˆMOSï¼‰</strong></td>
<td>4.41ï¼ˆé«˜å“è³ªï¼‰</td>
<td>4.27ï¼ˆã‚ãšã‹ã«ä½ã„ï¼‰</td>
</tr>
</tbody>
</table>

<p><strong>çµè«–</strong>: FastSpeechã¯ã€ã‚ãšã‹ãªå“è³ªä½ä¸‹ï¼ˆ-3%ï¼‰ã§ã€å¤§å¹…ãªé«˜é€ŸåŒ–ï¼ˆ38å€ï¼‰ã¨ãƒ­ãƒã‚¹ãƒˆæ€§å‘ä¸Šã‚’å®Ÿç¾ã€‚å®Ÿç”¨ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯FastSpeechãŒæœ‰åˆ©ã€‚</p>

</details>

<h3>å•é¡Œ3ï¼ˆDifficultyï¼šmediumï¼‰</h3>
<p>WaveNetã€WaveGlowã€HiFi-GANã®3ã¤ã®Vocoderã‚’ã€ç”Ÿæˆæ–¹å¼ã€é€Ÿåº¦ã€å“è³ªã®è¦³ç‚¹ã‹ã‚‰æ¯”è¼ƒã—ã€ãã‚Œãã‚Œã®ä½¿ç”¨å ´é¢ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>æ¯”è¼ƒè¡¨</strong>ï¼š</p>

<table>
<thead>
<tr>
<th>Vocoder</th>
<th>ç”Ÿæˆæ–¹å¼</th>
<th>é€Ÿåº¦</th>
<th>å“è³ªï¼ˆMOSï¼‰</th>
<th>ç‰¹å¾´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>WaveNet</strong></td>
<td>è‡ªå·±å›å¸°çš„<br>Dilated Causal Conv</td>
<td>éå¸¸ã«é…ã„</td>
<td>4.5+ï¼ˆæœ€é«˜ï¼‰</td>
<td>- æœ€é«˜å“è³ª<br>- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ä¸å¯</td>
</tr>
<tr>
<td><strong>WaveGlow</strong></td>
<td>Flow-based<br>å¯é€†å¤‰æ›</td>
<td>ä¸­é€Ÿ</td>
<td>4.2-4.3</td>
<td>- ä¸¦åˆ—ç”Ÿæˆ<br>- å®‰å®šã—ãŸå­¦ç¿’</td>
</tr>
<tr>
<td><strong>HiFi-GAN</strong></td>
<td>GAN<br>æ•µå¯¾çš„å­¦ç¿’</td>
<td>éå¸¸ã«é«˜é€Ÿ</td>
<td>4.3-4.5</td>
<td>- é«˜é€Ÿãƒ»é«˜å“è³ª<br>- å­¦ç¿’ã‚„ã‚„é›£</td>
</tr>
</tbody>
</table>

<p><strong>ä½¿ç”¨å ´é¢ã®ææ¡ˆ</strong>ï¼š</p>

<ol>
<li><p><strong>WaveNet</strong></p>
<ul>
<li>ä½¿ç”¨å ´é¢: ã‚ªãƒ•ãƒ©ã‚¤ãƒ³éŸ³å£°åˆæˆã€é«˜å“è³ªãŒæœ€å„ªå…ˆ</li>
<li>ä¾‹: ã‚¹ã‚¿ã‚¸ã‚ªå“è³ªã®ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ–ãƒƒã‚¯åˆ¶ä½œ</li>
</ul></li>

<li><p><strong>WaveGlow</strong></p>
<ul>
<li>ä½¿ç”¨å ´é¢: ç ”ç©¶ç›®çš„ã€Flow-basedãƒ¢ãƒ‡ãƒ«ã®ç†è§£</li>
<li>ä¾‹: Generative Modelsç ”ç©¶ã€VAEã¨ã®çµ„ã¿åˆã‚ã›</li>
</ul></li>

<li><p><strong>HiFi-GAN</strong></p>
<ul>
<li>ä½¿ç”¨å ´é¢: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ãƒ—ãƒªã€å®Ÿç”¨ã‚·ã‚¹ãƒ†ãƒ </li>
<li>ä¾‹: éŸ³å£°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€ãƒ©ã‚¤ãƒ–é…ä¿¡ã®ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</li>
</ul></li>
</ol>

<p><strong>Recommended</strong>: ç¾åœ¨ã§ã¯ã€é€Ÿåº¦ã¨å“è³ªã®ãƒãƒ©ãƒ³ã‚¹ãŒå„ªã‚ŒãŸHiFi-GANãŒæœ€ã‚‚åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚</p>

</details>

<h3>å•é¡Œ4ï¼ˆDifficultyï¼šhardï¼‰</h3>
<p>FastSpeechã®Duration Predictorã¨Length Regulatorã®å½¹å‰²ã‚’èª¬æ˜ã—ã€ãªãœã“ã‚Œã‚‰ãŒéè‡ªå·±å›å¸°çš„TTSã«å¿…è¦ãªã®ã‹è¿°ã¹ã¦ãã ã•ã„ã€‚Pythonã‚³ãƒ¼ãƒ‰ã§ç°¡å˜ãªå®Ÿè£…ä¾‹ã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>å½¹å‰²ã®èª¬æ˜</strong>ï¼š</p>

<ol>
<li><p><strong>Duration Predictorï¼ˆç¶™ç¶š hoursäºˆæ¸¬å™¨ï¼‰</strong></p>
<ul>
<li>å½¹å‰²: å„éŸ³ç´ ãŒä½•ãƒ•ãƒ¬ãƒ¼ãƒ ç¶šãã‹ã‚’äºˆæ¸¬</li>
<li>å…¥åŠ›: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®éš ã‚ŒçŠ¶æ…‹ï¼ˆéŸ³ç´ Levelï¼‰</li>
<li>å‡ºåŠ›: å„éŸ³ç´ ã®ç¶™ç¶š hoursï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼‰</li>
</ul></li>

<li><p><strong>Length Regulator</strong></p>
<ul>
<li>å½¹å‰²: éŸ³ç´ Levelã®è¡¨ç¾ã‚’ãƒ•ãƒ¬ãƒ¼ãƒ Levelã«æ‹¡å¼µ</li>
<li>å…¥åŠ›: éŸ³ç´ ã®éš ã‚ŒçŠ¶æ…‹ + äºˆæ¸¬ã•ã‚ŒãŸç¶™ç¶š hours</li>
<li>å‡ºåŠ›: ãƒ•ãƒ¬ãƒ¼ãƒ Levelã®éš ã‚ŒçŠ¶æ…‹</li>
</ul></li>
</ol>

<p><strong>å¿…è¦æ€§</strong>ï¼š</p>

<p>éè‡ªå·±å›å¸°çš„TTSã§ã¯ã€å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¸¦åˆ—ç”Ÿæˆã™ã‚‹ãŸã‚ï¼š</p>
<ul>
<li>äº‹å‰ã«å‡ºåŠ›é•·ã‚’çŸ¥ã‚‹å¿…è¦ãŒã‚ã‚‹</li>
<li>ãƒ†ã‚­ã‚¹ãƒˆï¼ˆéŸ³ç´ ï¼‰ã¨éŸ³å£°ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰ã®é•·ã•ãŒç•°ãªã‚‹</li>
<li>å„éŸ³ç´ ã®ç¶™ç¶š hoursãŒç•°ãªã‚‹ï¼ˆä¾‹: ã€Œã‚ã€ã¯3ãƒ•ãƒ¬ãƒ¼ãƒ ã€ã€Œã‚“ã€ã¯1ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰</li>
</ul>

<p>Duration Predictorã§ç¶™ç¶š hoursã‚’äºˆæ¸¬ã—ã€Length Regulatorã§éŸ³ç´ è¡¨ç¾ã‚’é©åˆ‡ãªé•·ã•ã«æ‹¡å¼µã™ã‚‹ã“ã¨ã§ã€ä¸¦åˆ—ç”ŸæˆãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚</p>

<p><strong>å®Ÿè£…ä¾‹</strong>ï¼š</p>

<pre><code class="language-python">import torch
import torch.nn as nn

class SimpleDurationPredictor(nn.Module):
    """ç¶™ç¶š hoursäºˆæ¸¬å™¨"""

    def __init__(self, hidden_size=256):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Conv1d(hidden_size, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.LayerNorm(256),
            nn.Dropout(0.5),
            nn.Conv1d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.LayerNorm(256),
            nn.Dropout(0.5)
        )
        self.output = nn.Linear(256, 1)

    def forward(self, x):
        # x: (B, T, hidden_size)
        x = x.transpose(1, 2)  # (B, hidden_size, T)
        x = self.layers(x)
        x = x.transpose(1, 2)  # (B, T, 256)
        duration = self.output(x).squeeze(-1)  # (B, T)
        return torch.relu(duration)  # æ­£ã®å€¤ã®ã¿

class SimpleLengthRegulator(nn.Module):
    """Length Regulator"""

    def forward(self, x, durations):
        # x: (B, T_phoneme, hidden_size)
        # durations: (B, T_phoneme)

        output = []
        for batch_idx in range(x.size(0)):
            expanded = []
            for phoneme_idx in range(x.size(1)):
                dur = int(durations[batch_idx, phoneme_idx].item())
                if dur > 0:
                    # éŸ³ç´ ã®éš ã‚ŒçŠ¶æ…‹ã‚’durå›ç¹°ã‚Šè¿”ã™
                    phoneme_hidden = x[batch_idx, phoneme_idx].unsqueeze(0)
                    expanded.append(phoneme_hidden.expand(dur, -1))

            if expanded:
                output.append(torch.cat(expanded, dim=0))

        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦åŒã˜é•·ã•ã«
        max_len = max(seq.size(0) for seq in output)
        padded = []
        for seq in output:
            if seq.size(0) < max_len:
                pad = torch.zeros(max_len - seq.size(0), seq.size(1))
                seq = torch.cat([seq, pad], dim=0)
            padded.append(seq.unsqueeze(0))

        return torch.cat(padded, dim=0)

# ãƒ†ã‚¹ãƒˆ
print("=== Duration Predictor & Length Regulator ===\n")

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
batch_size, n_phonemes, hidden_size = 2, 5, 256
phoneme_hidden = torch.randn(batch_size, n_phonemes, hidden_size)

# Durationäºˆæ¸¬
duration_predictor = SimpleDurationPredictor(hidden_size)
predicted_durations = duration_predictor(phoneme_hidden)

print(f"éŸ³ç´ ã®éš ã‚ŒçŠ¶æ…‹: {phoneme_hidden.shape}")
print(f"äºˆæ¸¬ã•ã‚ŒãŸç¶™ç¶š hours: {predicted_durations.shape}")
print(f"ã‚µãƒ³ãƒ—ãƒ«ç¶™ç¶š hours: {predicted_durations[0]}")

# Length Regulation
length_regulator = SimpleLengthRegulator()
frame_hidden = length_regulator(phoneme_hidden, predicted_durations)

print(f"\nãƒ•ãƒ¬ãƒ¼ãƒ Levelã®éš ã‚ŒçŠ¶æ…‹: {frame_hidden.shape}")
print(f"æ‹¡å¼µç‡: {frame_hidden.size(1) / phoneme_hidden.size(1):.2f}x")

# å…·ä½“ä¾‹
print("\n=== å…·ä½“ä¾‹ ===")
print("éŸ³ç´ : ['k', 'o', 'n', 'n', 'i']")
print("ç¶™ç¶š hours: [3, 4, 1, 1, 2] ãƒ•ãƒ¬ãƒ¼ãƒ ")
print("â†’ åˆè¨ˆ 11 ãƒ•ãƒ¬ãƒ¼ãƒ ã®éŸ³å£°ç”Ÿæˆ")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== Duration Predictor & Length Regulator ===

éŸ³ç´ ã®éš ã‚ŒçŠ¶æ…‹: torch.Size([2, 5, 256])
äºˆæ¸¬ã•ã‚ŒãŸç¶™ç¶š hours: torch.Size([2, 5])
ã‚µãƒ³ãƒ—ãƒ«ç¶™ç¶š hours: tensor([2.3, 1.8, 3.1, 2.5, 1.2])

ãƒ•ãƒ¬ãƒ¼ãƒ Levelã®éš ã‚ŒçŠ¶æ…‹: torch.Size([2, 11, 256])
æ‹¡å¼µç‡: 2.20x

=== å…·ä½“ä¾‹ ===
éŸ³ç´ : ['k', 'o', 'n', 'n', 'i']
ç¶™ç¶š hours: [3, 4, 1, 1, 2] ãƒ•ãƒ¬ãƒ¼ãƒ 
â†’ åˆè¨ˆ 11 ãƒ•ãƒ¬ãƒ¼ãƒ ã®éŸ³å£°ç”Ÿæˆ
</code></pre>

</details>

<h3>å•é¡Œ5ï¼ˆDifficultyï¼šhardï¼‰</h3>
<p>Multi-speaker TTSã«ãŠã‘ã‚‹Speaker Embeddingã®å½¹å‰²ã‚’èª¬æ˜ã—ã€ã©ã®ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ã«çµ„ã¿è¾¼ã¾ã‚Œã‚‹ã‹è¿°ã¹ã¦ãã ã•ã„ã€‚ã¾ãŸã€Voice Cloningã¨ã®é–¢é€£æ€§ã‚’è«–ã˜ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>Speaker Embeddingã®å½¹å‰²</strong>ï¼š</p>

<ol>
<li><p><strong>è©± learnerç‰¹å¾´ã®è¡¨ç¾</strong></p>
<ul>
<li>å„è©± learnerã‚’ä½æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ï¼ˆé€šå¸¸64-512æ¬¡å…ƒï¼‰ã§è¡¨ç¾</li>
<li>è©± learnerã®å£°è³ªã€ãƒ”ãƒƒãƒã€è©±ã—æ–¹ã®ç‰¹å¾´ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰</li>
</ul></li>

<li><p><strong>æ¡ä»¶ä»˜ã‘</strong></p>
<ul>
<li>TTSãƒ¢ãƒ‡ãƒ«ã«è©± learneræƒ…å ±ã‚’æä¾›</li>
<li>åŒã˜ãƒ†ã‚­ã‚¹ãƒˆã§ã‚‚ç•°ãªã‚‹è©± learnerã®å£°ã‚’ç”Ÿæˆå¯èƒ½</li>
</ul></li>
</ol>

<p><strong>ãƒ¢ãƒ‡ãƒ«ã¸ã®çµ„ã¿è¾¼ã¿æ–¹æ³•</strong>ï¼š</p>

<p><strong>æ–¹æ³•1: Embedding Lookupãƒ†ãƒ¼ãƒ–ãƒ«</strong></p>
<pre><code class="language-python">import torch
import torch.nn as nn

class MultiSpeakerTTS(nn.Module):
    def __init__(self, n_speakers=100, speaker_embed_dim=256,
                 text_embed_dim=512):
        super().__init__()

        # Speaker Embedding
        self.speaker_embedding = nn.Embedding(n_speakers, speaker_embed_dim)

        # Text Encoder
        self.text_encoder = nn.LSTM(text_embed_dim, 512,
                                    num_layers=2, batch_first=True)

        # Speaker-conditioned Decoder
        self.decoder = nn.LSTM(512 + speaker_embed_dim, 512,
                              num_layers=2, batch_first=True)

        self.mel_projection = nn.Linear(512, 80)  # 80-dim Mel

    def forward(self, text_features, speaker_ids):
        # Speaker Embeddingå–å¾—
        speaker_emb = self.speaker_embedding(speaker_ids)  # (B, speaker_dim)

        # Text Encoding
        text_encoded, _ = self.text_encoder(text_features)  # (B, T, 512)

        # Speaker Embeddingã‚’å…¨ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã«æ‹¡å¼µ
        speaker_emb_expanded = speaker_emb.unsqueeze(1).expand(
            -1, text_encoded.size(1), -1
        )  # (B, T, speaker_dim)

        # Concatenate
        decoder_input = torch.cat([text_encoded, speaker_emb_expanded],
                                 dim=-1)  # (B, T, 512+speaker_dim)

        # Decode
        decoder_output, _ = self.decoder(decoder_input)

        # Mel prediction
        mel_output = self.mel_projection(decoder_output)

        return mel_output

# ãƒ†ã‚¹ãƒˆ
model = MultiSpeakerTTS(n_speakers=100)
text_features = torch.randn(4, 50, 512)  # Batch=4, Seq=50
speaker_ids = torch.tensor([0, 5, 10, 15])  # ç•°ãªã‚‹è©± learner

mel_output = model(text_features, speaker_ids)
print("=== Multi-Speaker TTS ===")
print(f"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ: {text_features.shape}")
print(f"è©± learnerID: {speaker_ids}")
print(f"å‡ºåŠ›Mel: {mel_output.shape}")
</code></pre>

<p><strong>æ–¹æ³•2: Speaker Encoderï¼ˆVoice Cloningç”¨ï¼‰</strong></p>
<pre><code class="language-python">class SpeakerEncoder(nn.Module):
    """éŸ³å£°ã‹ã‚‰è©± learneråŸ‹ã‚è¾¼ã¿ã‚’æŠ½å‡º"""

    def __init__(self, mel_dim=80, embed_dim=256):
        super().__init__()
        self.lstm = nn.LSTM(mel_dim, 256, num_layers=3,
                           batch_first=True)
        self.projection = nn.Linear(256, embed_dim)

    def forward(self, mel_spectrograms):
        # mel_spectrograms: (B, T, 80)
        _, (hidden, _) = self.lstm(mel_spectrograms)
        # æœ€çµ‚å±¤ã®éš ã‚ŒçŠ¶æ…‹ã‚’ä½¿ç”¨
        speaker_emb = self.projection(hidden[-1])  # (B, embed_dim)
        # L2æ­£è¦åŒ–
        speaker_emb = speaker_emb / torch.norm(speaker_emb, dim=1, keepdim=True)
        return speaker_emb

# Voice Cloningã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
speaker_encoder = SpeakerEncoder()

# å‚ç…§éŸ³å£°ã‹ã‚‰è©± learneråŸ‹ã‚è¾¼ã¿ã‚’æŠ½å‡º
reference_mel = torch.randn(1, 100, 80)  # æ•°ç§’ã®éŸ³å£°
speaker_emb = speaker_encoder(reference_mel)

print("\n=== Voice Cloning ===")
print(f"å‚ç…§éŸ³å£°: {reference_mel.shape}")
print(f"æŠ½å‡ºã•ã‚ŒãŸè©± learneråŸ‹ã‚è¾¼ã¿: {speaker_emb.shape}")
print("â†’ ã“ã®åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ã£ã¦ã€ä»»æ„ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã“ã®è©± learnerã®å£°ã§åˆæˆ")
</code></pre>

<p><strong>Voice Cloningã¨ã®é–¢é€£æ€§</strong>ï¼š</p>

<table>
<thead>
<tr>
<th>è¦³ç‚¹</th>
<th>Multi-speaker TTS</th>
<th>Voice Cloning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>è©± learnerè¡¨ç¾</strong></td>
<td>Embedding Lookup<br>ï¼ˆå­¦ç¿’æ¸ˆã¿è©± learnerã®ã¿ï¼‰</td>
<td>Speaker Encoder<br>ï¼ˆæœªçŸ¥è©± learnerã‚‚å¯èƒ½ï¼‰</td>
</tr>
<tr>
<td><strong>å¿…è¦ãƒ‡ãƒ¼ã‚¿</strong></td>
<td>å„è©± learnerã®å¤§é‡ãƒ‡ãƒ¼ã‚¿</td>
<td>æ•°ç§’ï½æ•° minutesã®éŸ³å£°</td>
</tr>
<tr>
<td><strong>æŸ”è»Ÿæ€§</strong></td>
<td>ä½ï¼ˆå›ºå®šè©± learnerã‚»ãƒƒãƒˆï¼‰</td>
<td>é«˜ï¼ˆæ–°è¦è©± learnerå¯¾å¿œï¼‰</td>
</tr>
<tr>
<td><strong>å“è³ª</strong></td>
<td>é«˜ï¼ˆå„è©± learnerã§æœ€é©åŒ–ï¼‰</td>
<td>ä¸­ï½é«˜ï¼ˆãƒ‡ãƒ¼ã‚¿é‡ä¾å­˜ï¼‰</td>
</tr>
</tbody>
</table>

<p><strong>çµ±åˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</strong>ï¼š</p>

<p>æœ€æ–°ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€ä¸¡æ–¹ã‚’çµ„ã¿åˆã‚ã›ã¦ä½¿ç”¨ï¼š</p>
<ol>
<li>Multi-speaker TTSã‚’å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’</li>
<li>Speaker Encoderã§æ–°è¦è©± learnerã®åŸ‹ã‚è¾¼ã¿ã‚’æŠ½å‡º</li>
<li>å°‘é‡ã®è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã§å¾®èª¿æ•´ï¼ˆOptionalï¼‰</li>
</ol>

<p>ã“ã‚Œã«ã‚ˆã‚Šã€å­¦ç¿’æ¸ˆã¿è©± learnerã®é«˜å“è³ªã¨ã€æœªçŸ¥è©± learnerã¸ã®å¯¾å¿œã‚’ä¸¡ç«‹ã§ãã¾ã™ã€‚</p>

</details>

<hr>

<h2>References</h2>

<ol>
<li>Wang, Y., et al. (2017). "Tacotron: Towards End-to-End Speech Synthesis." <em>Interspeech 2017</em>.</li>
<li>Shen, J., et al. (2018). "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions." <em>ICASSP 2018</em>.</li>
<li>Ren, Y., et al. (2019). "FastSpeech: Fast, Robust and Controllable Text to Speech." <em>NeurIPS 2019</em>.</li>
<li>van den Oord, A., et al. (2016). "WaveNet: A Generative Model for Raw Audio." <em>arXiv:1609.03499</em>.</li>
<li>Prenger, R., Valle, R., & Catanzaro, B. (2019). "WaveGlow: A Flow-based Generative Network for Speech Synthesis." <em>ICASSP 2019</em>.</li>
<li>Kong, J., Kim, J., & Bae, J. (2020). "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis." <em>NeurIPS 2020</em>.</li>
<li>Kim, J., Kong, J., & Son, J. (2021). "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech." <em>ICML 2021</em>.</li>
<li>Casanova, E., et al. (2022). "YourtTS: Towards Zero-Shot Multi-Speaker TTS." <em>arXiv:2112.02418</em>.</li>
</ol>

<div class="navigation">
    <a href="chapter3-speech-recognition.html" class="nav-button">â† Previous Chapter: éŸ³å£°èªè­˜</a>
    <a href="index.html" class="nav-button">Series Contents</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, either express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The creators and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
            <li>To the maximum extent permitted by applicable law, the creators and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are subject to the specified terms (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆ learner</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
