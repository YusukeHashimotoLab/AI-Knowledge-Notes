<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Speech Processing & Speech Recognition Introduction Series - Complete Practical Guide from Acoustic Features to Modern Speech AI">
    <title>Speech Processing & Speech Recognition Introduction Series v1.0 - AI Terakoya</title>

    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header .container {
            padding: 0 1.5rem;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }

        .meta span {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
        }

        /* Typography */
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            color: var(--primary-color);
        }

        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.6rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1.2rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--link-hover);
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1.2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Code blocks */
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border: 1px solid var(--border-color);
        }

        pre code {
            background: none;
            padding: 0;
            font-size: 0.9rem;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
            overflow-x: auto;
            display: block;
        }

        thead {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        tbody {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        th, td {
            padding: 0.8rem;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        th {
            background: var(--primary-color);
            color: white;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background: #f9f9f9;
        }

        /* Blockquotes */
        blockquote {
            border-left: 4px solid var(--secondary-color);
            padding-left: 1.5rem;
            margin: 1.5rem 0;
            font-style: italic;
            color: #666;
        }

        /* Images */
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1rem 0;
        }

        /* Mermaid diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
            background: white;
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        /* Details/Summary (for exercises) */
        details {
            margin: 1rem 0;
            padding: 1rem;
            background: #f8f9fa;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--primary-color);
            padding: 0.5rem;
        }

        summary:hover {
            color: var(--secondary-color);
        }

        /* Footer */
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "‚ö†Ô∏è";
            position: absolute;
            left: 0;
        }


        /* Navigation buttons */
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }

        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            h1 {
                font-size: 1.6rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.85rem;
            }

            table {
                font-size: 0.9rem;
            }
        }



        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        // Mermaid.js Converter - Converts markdown-style mermaid code blocks to renderable divs
        document.addEventListener('DOMContentLoaded', function() {
            // Find all code blocks with class="language-mermaid"
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                // Create a new div with mermaid class
                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                // Replace the pre element with the new div
                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            // Re-initialize mermaid after conversion
            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({ startOnLoad: true, theme: 'default' });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Speech Audio</span>
        </div>
    </nav>

        <header>
        <div class="container">
            <h1>üéôÔ∏è Speech Processing & Speech Recognition Introduction Series v1.0</h1>
            <p style="font-size: 1.1rem; margin-top: 0.5rem; opacity: 0.95;">From Acoustic Features to Modern Speech AI</p>
            <div class="meta">
                <span>üìñ Total Study Time: 5-6 hours</span>
                <span>üìä Level: Intermediate</span>
            </div>
        </div>
    </header>

    <main class="container">
        <p><strong>Master practical knowledge and skills for handling speech data, from the fundamentals of speech signal processing to deep learning-based speech recognition, speech synthesis, and speech classification</strong></p>

        <h2 id="overview">Series Overview</h2>
        <p>This series is a comprehensive 5-chapter practical educational content that teaches the theory and implementation of speech processing and speech recognition progressively from fundamentals.</p>

        <p><strong>Speech Processing and Speech Recognition</strong> are critical technologies used in various aspects of modern society, including voice assistants (Siri, Alexa, Google Assistant), automatic subtitle generation, speech translation, call center automation, and voice search. You will systematically understand the complete picture of speech AI, from digital audio fundamentals to acoustic features like MFCC and mel-spectrograms, traditional HMM-GMM models, state-of-the-art deep learning-based speech recognition (Whisper, Wav2Vec 2.0), speech synthesis (TTS, Tacotron, VITS), and applied technologies such as speaker recognition, emotion recognition, and speech enhancement. Learn the principles and implementation of cutting-edge models developed by Google, Meta, and OpenAI, and acquire practical skills using real speech data. Implementation methods using major libraries such as librosa, torchaudio, and Transformers are provided.</p>

        <p><strong>Features:</strong></p>
        <ul>
            <li>‚úÖ <strong>From Theory to Practice</strong>: Systematic learning from acoustic fundamentals to state-of-the-art deep learning models</li>
            <li>‚úÖ <strong>Implementation-Focused</strong>: Over 50 executable Python/librosa/PyTorch code examples</li>
            <li>‚úÖ <strong>Practically-Oriented</strong>: Hands-on projects using real speech data</li>
            <li>‚úÖ <strong>Latest Technology</strong>: Implementation using Whisper, Wav2Vec 2.0, VITS, and Transformers</li>
            <li>‚úÖ <strong>Practical Applications</strong>: Implementation of speech recognition, speech synthesis, speaker recognition, and emotion recognition</li>
        </ul>

        <p><strong>Total Study Time</strong>: 5-6 hours (including code execution and exercises)</p>

        <h2 id="learning">How to Study</h2>

        <h3>Recommended Study Order</h3>

        <div class="mermaid">
graph TD
    A[Chapter 1: Fundamentals of Speech Signal Processing] --> B[Chapter 2: Traditional Speech Recognition]
    B --> C[Chapter 3: Deep Learning-based Speech Recognition]
    C --> D[Chapter 4: Speech Synthesis]
    D --> E[Chapter 5: Speech Applications]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
</div>

        <p><strong>For Beginners (no knowledge of speech processing):</strong><br>
        - Chapter 1 ‚Üí Chapter 2 ‚Üí Chapter 3 ‚Üí Chapter 4 ‚Üí Chapter 5 (all chapters recommended)<br>
        - Time Required: 5-6 hours</p>

        <p><strong>For Intermediate Learners (with machine learning experience):</strong><br>
        - Chapter 1 ‚Üí Chapter 3 ‚Üí Chapter 4 ‚Üí Chapter 5<br>
        - Time Required: 4-5 hours</p>

        <p><strong>For Specific Topic Enhancement:</strong><br>
        - Speech Signal Processing & MFCC: Chapter 1 (focused study)<br>
        - HMM & GMM: Chapter 2 (focused study)<br>
        - Deep Learning Speech Recognition: Chapter 3 (focused study)<br>
        - Speech Synthesis & TTS: Chapter 4 (focused study)<br>
        - Speaker Recognition & Emotion Recognition: Chapter 5 (focused study)<br>
        - Time Required: 60-80 minutes/chapter</p>

        <h2 id="chapters">Chapter Details</h2>

        <h3><a href="./chapter1-signal-processing-basics.html">Chapter 1: Fundamentals of Speech Signal Processing</a></h3>
        <p><strong>Difficulty</strong>: Intermediate<br>
        <strong>Reading Time</strong>: 60-70 minutes<br>
        <strong>Code Examples</strong>: 12</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>Digital Audio Fundamentals</strong> - Sampling, quantization, Nyquist theorem</li>
            <li><strong>Acoustic Features</strong> - MFCC, mel-spectrogram, pitch, formants</li>
            <li><strong>Spectral Analysis</strong> - Fourier transform, STFT, spectrogram</li>
            <li><strong>Using librosa</strong> - Audio loading, feature extraction, visualization</li>
            <li><strong>Speech Preprocessing</strong> - Noise reduction, normalization, VAD (Voice Activity Detection)</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>‚úÖ Understand the fundamental principles of digital audio</li>
            <li>‚úÖ Explain acoustic features (MFCC, mel-spectrogram)</li>
            <li>‚úÖ Understand spectral analysis methods</li>
            <li>‚úÖ Process audio data using librosa</li>
            <li>‚úÖ Implement speech preprocessing techniques</li>
        </ul>

        <p><strong><a href="./chapter1-signal-processing-basics.html">Read Chapter 1 ‚Üí</a></strong></p>

        <hr>

        <h3><a href="./chapter2-traditional-asr.html">Chapter 2: Traditional Speech Recognition</a></h3>
        <p><strong>Difficulty</strong>: Intermediate<br>
        <strong>Reading Time</strong>: 60-70 minutes<br>
        <strong>Code Examples</strong>: 8</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>Speech Recognition Fundamentals</strong> - Acoustic model, language model, decoding</li>
            <li><strong>HMM (Hidden Markov Model)</strong> - State transition, observation probability, Viterbi algorithm</li>
            <li><strong>GMM (Gaussian Mixture Model)</strong> - Acoustic modeling, EM algorithm</li>
            <li><strong>Language Model</strong> - N-gram, statistical language model, smoothing</li>
            <li><strong>Evaluation Metrics</strong> - WER (Word Error Rate), CER (Character Error Rate)</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>‚úÖ Understand the basic architecture of speech recognition</li>
            <li>‚úÖ Explain HMM principles and Viterbi algorithm</li>
            <li>‚úÖ Understand acoustic modeling with GMM</li>
            <li>‚úÖ Implement N-gram language models</li>
            <li>‚úÖ Evaluate performance using WER and CER</li>
        </ul>

        <p><strong><a href="./chapter2-traditional-asr.html">Read Chapter 2 ‚Üí</a></strong></p>

        <hr>

        <h3><a href="./chapter3-deep-learning-asr.html">Chapter 3: Deep Learning-based Speech Recognition</a></h3>
        <p><strong>Difficulty</strong>: Intermediate to Advanced<br>
        <strong>Reading Time</strong>: 80-90 minutes<br>
        <strong>Code Examples</strong>: 10</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>End-to-End Speech Recognition</strong> - CTC (Connectionist Temporal Classification)</li>
            <li><strong>RNN-Transducer</strong> - Streaming speech recognition, online recognition</li>
            <li><strong>Transformer Speech Recognition</strong> - Self-Attention, Positional Encoding</li>
            <li><strong>Whisper</strong> - OpenAI's multilingual speech recognition model, zero-shot learning</li>
            <li><strong>Wav2Vec 2.0</strong> - Self-supervised learning, speech representation learning</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>‚úÖ Understand the principles of CTC loss function</li>
            <li>‚úÖ Implement streaming recognition with RNN-Transducer</li>
            <li>‚úÖ Understand Transformer applications in speech recognition</li>
            <li>‚úÖ Implement multilingual speech recognition with Whisper</li>
            <li>‚úÖ Learn speech representations with Wav2Vec 2.0</li>
        </ul>

        <p><strong><a href="./chapter3-deep-learning-asr.html">Read Chapter 3 ‚Üí</a></strong></p>

        <hr>

        <h3><a href="./chapter4-speech-synthesis.html">Chapter 4: Speech Synthesis</a></h3>
        <p><strong>Difficulty</strong>: Intermediate to Advanced<br>
        <strong>Reading Time</strong>: 70-80 minutes<br>
        <strong>Code Examples</strong>: 10</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>TTS (Text-to-Speech) Fundamentals</strong> - Phonetic conversion, prosody generation, speech synthesis</li>
            <li><strong>Tacotron 2</strong> - Seq2Seq model, Attention mechanism, mel-spectrogram generation</li>
            <li><strong>FastSpeech</strong> - Non-autoregressive model, parallel generation, fast synthesis</li>
            <li><strong>VITS</strong> - End-to-end TTS, variational inference, neural vocoder</li>
            <li><strong>Vocoders</strong> - WaveNet, WaveGlow, HiFi-GAN</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>‚úÖ Understand the basic architecture of TTS</li>
            <li>‚úÖ Generate mel-spectrograms with Tacotron 2</li>
            <li>‚úÖ Implement fast speech synthesis with FastSpeech</li>
            <li>‚úÖ Implement end-to-end TTS with VITS</li>
            <li>‚úÖ Generate speech waveforms with neural vocoders</li>
        </ul>

        <p><strong><a href="./chapter4-speech-synthesis.html">Read Chapter 4 ‚Üí</a></strong></p>

        <hr>

        <h3><a href="./chapter5-speech-applications.html">Chapter 5: Speech Applications</a></h3>
        <p><strong>Difficulty</strong>: Intermediate to Advanced<br>
        <strong>Reading Time</strong>: 70-80 minutes<br>
        <strong>Code Examples</strong>: 12</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>Speaker Recognition</strong> - Speaker identification, speaker verification, x-vector, d-vector</li>
            <li><strong>Emotion Recognition</strong> - Acoustic features, prosodic features, deep learning models</li>
            <li><strong>Speech Enhancement</strong> - Noise reduction, beamforming, masking techniques</li>
            <li><strong>Music Information Retrieval</strong> - Tempo detection, beat tracking, genre classification</li>
            <li><strong>Voice Activity Detection (VAD)</strong> - WebRTC VAD, deep learning-based VAD</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>‚úÖ Understand and implement speaker recognition methods</li>
            <li>‚úÖ Recognize emotions from speech</li>
            <li>‚úÖ Implement speech enhancement techniques</li>
            <li>‚úÖ Understand music information retrieval fundamentals</li>
            <li>‚úÖ Detect voice activity with VAD</li>
        </ul>

        <p><strong><a href="./chapter5-speech-applications.html">Read Chapter 5 ‚Üí</a></strong></p>

        <hr>

        <h2 id="outcomes">Overall Learning Outcomes</h2>

        <p>Upon completing this series, you will acquire the following skills and knowledge:</p>

        <h3>Knowledge Level (Understanding)</h3>
        <ul>
            <li>‚úÖ Explain digital audio and acoustic features like MFCC</li>
            <li>‚úÖ Understand the differences between HMM-GMM and CTC</li>
            <li>‚úÖ Explain the latest trends in deep learning speech recognition</li>
            <li>‚úÖ Understand the principles of TTS and speech synthesis</li>
            <li>‚úÖ Explain speaker recognition and emotion recognition methods</li>
        </ul>

        <h3>Practical Skills (Doing)</h3>
        <ul>
            <li>‚úÖ Process audio data using librosa</li>
            <li>‚úÖ Extract MFCC and mel-spectrograms</li>
            <li>‚úÖ Implement speech recognition with Whisper</li>
            <li>‚úÖ Implement speech synthesis with VITS</li>
            <li>‚úÖ Build speaker recognition and emotion recognition models</li>
        </ul>

        <h3>Application Ability (Applying)</h3>
        <ul>
            <li>‚úÖ Select appropriate speech recognition methods for projects</li>
            <li>‚úÖ Design speech data preprocessing pipelines</li>
            <li>‚úÖ Build custom speech recognition systems</li>
            <li>‚úÖ Develop speech synthesis applications</li>
            <li>‚úÖ Evaluate and improve speech AI systems</li>
        </ul>

        <hr>

        <h2 id="prerequisites">Prerequisites</h2>

        <p>To effectively study this series, it is desirable to have the following knowledge:</p>

        <h3>Required (Must Have)</h3>
        <ul>
            <li>‚úÖ <strong>Python Fundamentals</strong>: Variables, functions, classes, NumPy, pandas</li>
            <li>‚úÖ <strong>Machine Learning Fundamentals</strong>: Concepts of training, evaluation, loss functions</li>
            <li>‚úÖ <strong>Mathematics Fundamentals</strong>: Linear algebra, probability & statistics, calculus</li>
            <li>‚úÖ <strong>Signal Processing Basics</strong>: Fourier transform concepts (recommended)</li>
            <li>‚úÖ <strong>Deep Learning Basics</strong>: CNN, RNN, Transformer fundamentals (from Chapter 3 onwards)</li>
        </ul>

        <h3>Recommended (Nice to Have)</h3>
        <ul>
            <li>üí° <strong>PyTorch Basics</strong>: Tensor operations, model building, training loops</li>
            <li>üí° <strong>Transformers Experience</strong>: Hugging Face Transformers library</li>
            <li>üí° <strong>Acoustics Knowledge</strong>: Sound waves, frequency, decibels</li>
            <li>üí° <strong>Natural Language Processing</strong>: Tokenization, language models (for speech recognition)</li>
            <li>üí° <strong>Time Series Data Processing</strong>: RNN, LSTM, Seq2Seq</li>
        </ul>

        <p><strong>Recommended Prior Study</strong>:</p>
        <ul>
            <li>üìö <a href="../machine-learning-basics/">Machine Learning Introduction Series</a> - ML fundamental knowledge</li>
            <!-- Content in preparation <li>üìö <a href="../deep-learning-basics/">Deep Learning Introduction Series</a> - CNN, RNN, Transformer</li>
            <li>üìö <a href="../pytorch-introduction/">PyTorch Practical Introduction</a> - How to use PyTorch</li>
            <li>üìö <a href="../signal-processing-basics/">Signal Processing Introduction</a> - Fourier transform, spectral analysis</li> -->
        </ul>

        <hr>

        <h2 id="tech">Technologies and Tools Used</h2>

        <h3>Main Libraries</h3>
        <ul>
            <li><strong>librosa 0.10+</strong> - Speech signal processing, feature extraction</li>
            <li><strong>PyTorch 2.0+</strong> - Deep learning framework</li>
            <li><strong>torchaudio 2.0+</strong> - PyTorch audio processing library</li>
            <li><strong>Transformers 4.30+</strong> - Hugging Face, Whisper, Wav2Vec 2.0</li>
            <li><strong>SpeechBrain 0.5+</strong> - Speech processing toolkit</li>
            <li><strong>Kaldi</strong> - Traditional speech recognition toolkit (reference)</li>
            <li><strong>ESPnet</strong> - End-to-end speech processing toolkit</li>
        </ul>

        <h3>Development Environment</h3>
        <ul>
            <li><strong>Python 3.8+</strong> - Programming language</li>
            <li><strong>Jupyter Notebook / Google Colab</strong> - Interactive development environment</li>
            <li><strong>NumPy 1.23+</strong> - Numerical computing</li>
            <li><strong>SciPy 1.10+</strong> - Scientific computing</li>
            <li><strong>matplotlib / seaborn</strong> - Visualization</li>
        </ul>

        <h3>Datasets (Recommended)</h3>
        <ul>
            <li><strong>LibriSpeech</strong> - English speech recognition benchmark</li>
            <li><strong>Common Voice</strong> - Multilingual speech dataset</li>
            <li><strong>LJSpeech</strong> - English speech synthesis dataset</li>
            <li><strong>VCTK</strong> - Multi-speaker speech dataset</li>
            <li><strong>RAVDESS</strong> - Emotional speech dataset</li>
        </ul>

        <hr>

        <h2 id="start">Let's Get Started!</h2>
        <p>Are you ready? Begin with Chapter 1 and master speech processing and speech recognition technologies!</p>

        <p><strong><a href="./chapter1-signal-processing-basics.html">Chapter 1: Fundamentals of Speech Signal Processing ‚Üí</a></strong></p>

        <hr>

        <h2 id="next">Next Steps</h2>

        <p>After completing this series, we recommend advancing to the following topics:</p>

        <h3>Advanced Learning</h3>
        <ul>
            <li>üìö <strong>Spoken Dialogue Systems</strong>: Voice assistants, dialogue management, NLU integration</li>
            <li>üìö <strong>Multilingual Speech Processing</strong>: Cross-lingual transfer learning, low-resource language support</li>
            <li>üìö <strong>Real-time Speech Processing</strong>: Streaming processing, low-latency optimization</li>
            <li>üìö <strong>Speech Generation Models</strong>: Voice conversion, voice cloning, singing synthesis</li>
        </ul>

        <h3>Related Series</h3>
        <ul>
            <li>üéØ <a href="../nlp-introduction/">Natural Language Processing Introduction</a> - Text processing, language models</li>
            <li>üéØ <a href="../computer-vision-introduction/">Computer Vision Introduction</a> - Multimodal AI</li>
            <li>üéØ <a href="../transformer-architecture/">Complete Transformer Guide</a> - Attention mechanism</li>
        </ul>

        <h3>Practical Projects</h3>
        <ul>
            <li>üöÄ Voice Assistant - Wake word detection, speech recognition, voice response</li>
            <li>üöÄ Automatic Subtitle Generation System - Video speech recognition, timestamped subtitles</li>
            <li>üöÄ Multilingual Speech Translation App - Speech recognition ‚Üí machine translation ‚Üí speech synthesis</li>
            <li>üöÄ Emotion Recognition Call Center AI - Customer emotion analysis, quality monitoring</li>
        </ul>

        <hr>

        <p><strong>Update History</strong></p>
        <ul>
            <li><strong>2025-10-21</strong>: v1.0 Initial release</li>
        </ul>

        <hr>

        <p><strong>Your journey into speech AI begins here!</strong></p>

    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without any warranty, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, functionality, or safety.</li>
            <li>The authors and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
            <li>To the maximum extent permitted by applicable law, the authors and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are subject to the stated terms (e.g., CC BY 4.0). Such licenses typically include no-warranty provisions.</li>
        </ul>
    </section>

<footer>
        <div class="container">
            <p>&copy; 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
