<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 1: AutoML Fundamentals - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/automl-introduction/index.html">AutoML</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
</div>
</nav>
<header>
<div class="header-content">
<h1>Chapter 1: AutoML Fundamentals</h1>
<p class="subtitle">Democratization of Machine Learning - Concepts and Components of AutoML</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 25-30 minutes</span>
<span class="meta-item">üìä Difficulty: Beginner</span>
<span class="meta-item">üíª Code Examples: 7</span>
<span class="meta-item">üìù Exercises: 5</span>
</div>
</div>
</header>
<main class="container">
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master the following:</p>
<ul>
<li>‚úÖ Understand the concept and purpose of AutoML</li>
<li>‚úÖ Explain the differences from traditional ML workflows</li>
<li>‚úÖ Comprehend the components of AutoML and their roles</li>
<li>‚úÖ Understand the fundamentals of Neural Architecture Search (NAS)</li>
<li>‚úÖ Learn the concepts and applications of Meta-Learning</li>
<li>‚úÖ Master AutoML evaluation methods</li>
</ul>
<hr/>
<h2>1.1 What is AutoML</h2>
<h3>Democratization of Machine Learning</h3>
<p><strong>AutoML (Automated Machine Learning)</strong> is a technology that automates the machine learning model development process. It aims to enable even non-data scientists to build high-quality machine learning models.</p>
<blockquote>
<p>"AutoML realizes the democratization of machine learning, allowing more people to utilize AI technology"</p>
</blockquote>
<h3>Purpose of AutoML</h3>
<table>
<thead>
<tr>
<th>Purpose</th>
<th>Description</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Efficiency</strong></td>
<td>Automate manual processes</td>
<td>Reduce development time</td>
</tr>
<tr>
<td><strong>Reduce Expertise Requirements</strong></td>
<td>Deep machine learning knowledge not required</td>
<td>Lower barriers to entry</td>
</tr>
<tr>
<td><strong>Performance Improvement</strong></td>
<td>Discover optimal solutions through systematic search</td>
<td>Eliminate human bias</td>
</tr>
<tr>
<td><strong>Reproducibility</strong></td>
<td>Standardized processes</td>
<td>Improve result reliability</td>
</tr>
</tbody>
</table>
<h3>Comparison with Traditional ML Workflow</h3>
<div class="mermaid">
graph TD
    subgraph "Traditional Workflow"
    A1[Data Collection] --&gt; B1[Manual Preprocessing]
    B1 --&gt; C1[Feature Engineering]
    C1 --&gt; D1[Model Selection]
    D1 --&gt; E1[Hyperparameter Tuning]
    E1 --&gt; F1[Evaluation]
    F1 --&gt;|Trial and Error| C1
    end

    subgraph "AutoML Workflow"
    A2[Data Collection] --&gt; B2[Automatic Preprocessing]
    B2 --&gt; C2[Automatic Feature Generation]
    C2 --&gt; D2[Automatic Model Selection]
    D2 --&gt; E2[Automatic Hyperparameter Optimization]
    E2 --&gt; F2[Evaluation]
    end

    style A1 fill:#ffebee
    style A2 fill:#ffebee
    style B1 fill:#fff3e0
    style B2 fill:#e8f5e9
    style C1 fill:#f3e5f5
    style C2 fill:#e8f5e9
    style D1 fill:#e3f2fd
    style D2 fill:#e8f5e9
    style E1 fill:#fce4ec
    style E2 fill:#e8f5e9
</div>
<h3>Advantages and Disadvantages of AutoML</h3>
<h4>Advantages</h4>
<ul>
<li><strong>Time Savings</strong>: Reduce work from weeks to hours</li>
<li><strong>Accessibility</strong>: Usable by people with less expertise</li>
<li><strong>Optimization</strong>: Discover combinations humans might overlook</li>
<li><strong>Best Practices</strong>: Automatically applied</li>
</ul>
<h4>Disadvantages</h4>
<ul>
<li><strong>Computational Cost</strong>: Large-scale searches require many resources</li>
<li><strong>Black Box Nature</strong>: Reduced process transparency</li>
<li><strong>Flexibility Constraints</strong>: Customization can be difficult</li>
<li><strong>Neglect of Domain Knowledge</strong>: Cannot leverage data background knowledge</li>
</ul>
<h3>Example: Effects of AutoML</h3>
<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import time

# Data preparation
data = load_breast_cancer()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Traditional approach (fixed parameters)
start_time = time.time()
model_manual = RandomForestClassifier(n_estimators=100, random_state=42)
model_manual.fit(X_train, y_train)
y_pred_manual = model_manual.predict(X_test)
acc_manual = accuracy_score(y_test, y_pred_manual)
time_manual = time.time() - start_time

# AutoML-style simple implementation (grid search)
from sklearn.model_selection import GridSearchCV

start_time = time.time()
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}
model_auto = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=3,
    n_jobs=-1
)
model_auto.fit(X_train, y_train)
y_pred_auto = model_auto.predict(X_test)
acc_auto = accuracy_score(y_test, y_pred_auto)
time_auto = time.time() - start_time

print("=== Traditional vs AutoML-style Approach ===")
print(f"\nTraditional Approach:")
print(f"  Accuracy: {acc_manual:.4f}")
print(f"  Time: {time_manual:.2f}s")

print(f"\nAutoML-style Approach:")
print(f"  Accuracy: {acc_auto:.4f}")
print(f"  Time: {time_auto:.2f}s")
print(f"  Best Parameters: {model_auto.best_params_}")

print(f"\nImprovement:")
print(f"  Accuracy Gain: {(acc_auto - acc_manual) * 100:.2f}%")
</code></pre>
<p><strong>Example Output</strong>:</p>
<pre><code>=== Traditional vs AutoML-style Approach ===

Traditional Approach:
  Accuracy: 0.9649
  Time: 0.15s

AutoML-style Approach:
  Accuracy: 0.9737
  Time: 12.34s
  Best Parameters: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 100}

Improvement:
  Accuracy Gain: 0.88%
</code></pre>
<hr/>
<h2>1.2 Components of AutoML</h2>
<p>AutoML systems consist of multiple components to automate the entire machine learning pipeline.</p>
<h3>Data Preprocessing Automation</h3>
<p>Automates transformation from raw data to learnable formats:</p>
<ul>
<li><strong>Missing Value Handling</strong>: Automatic detection and imputation strategy selection</li>
<li><strong>Outlier Detection</strong>: Detection using statistical methods or Isolation Forest</li>
<li><strong>Scaling</strong>: Automatic selection of StandardScaler, MinMaxScaler</li>
<li><strong>Encoding</strong>: Automatic conversion of categorical variables</li>
</ul>
<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

# Sample data (with missing values)
np.random.seed(42)
data = pd.DataFrame({
    'age': [25, 30, np.nan, 45, 50, 35],
    'salary': [50000, 60000, 55000, np.nan, 80000, 65000],
    'department': ['Sales', 'IT', 'HR', 'IT', 'Sales', np.nan]
})

print("=== Original Data ===")
print(data)

# Automatic preprocessing pipeline
numeric_features = ['age', 'salary']
categorical_features = ['department']

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Execute preprocessing
data_transformed = preprocessor.fit_transform(data)

print("\n=== Data Shape After Preprocessing ===")
print(f"Shape: {data_transformed.shape}")
print(f"Missing Values: 0 (all handled)")
</code></pre>
<h3>Feature Engineering</h3>
<p>Automatically generates new features:</p>
<ul>
<li><strong>Polynomial Features</strong>: Combinations of existing features</li>
<li><strong>Aggregate Features</strong>: Statistics by group</li>
<li><strong>Time Series Features</strong>: Lags, moving averages, seasonality</li>
<li><strong>Text Features</strong>: TF-IDF, embedding representations</li>
</ul>
<pre><code class="language-python">from sklearn.preprocessing import PolynomialFeatures
from sklearn.datasets import make_regression
import matplotlib.pyplot as plt

# Sample data
X, y = make_regression(n_samples=100, n_features=2, noise=10, random_state=42)

# Polynomial feature generation
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

print("=== Feature Engineering ===")
print(f"Original feature count: {X.shape[1]}")
print(f"Feature count after generation: {X_poly.shape[1]}")
print(f"\nGenerated features:")
print(poly.get_feature_names_out(['x1', 'x2']))

# Performance comparison
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Original features
model_original = LinearRegression()
model_original.fit(X, y)
y_pred_original = model_original.predict(X)
r2_original = r2_score(y, y_pred_original)

# Polynomial features
model_poly = LinearRegression()
model_poly.fit(X_poly, y)
y_pred_poly = model_poly.predict(X_poly)
r2_poly = r2_score(y, y_pred_poly)

print(f"\n=== Performance Comparison ===")
print(f"Original Features R¬≤: {r2_original:.4f}")
print(f"Polynomial Features R¬≤: {r2_poly:.4f}")
print(f"Improvement: {(r2_poly - r2_original) * 100:.2f}%")
</code></pre>
<h3>Model Selection</h3>
<p>Automatically selects the optimal algorithm for the task and data:</p>
<ul>
<li><strong>Linear Models</strong>: Logistic Regression, Ridge, Lasso</li>
<li><strong>Tree-based</strong>: Decision Tree, Random Forest, XGBoost</li>
<li><strong>Support Vector Machines</strong>: SVC, SVR</li>
<li><strong>Neural Networks</strong>: MLP, CNN, RNN</li>
</ul>
<pre><code class="language-python">from sklearn.datasets import load_iris
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# Data preparation
iris = load_iris()
X, y = iris.data, iris.target

# Evaluate multiple models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'SVM': SVC(),
    'KNN': KNeighborsClassifier()
}

print("=== Automatic Model Selection ===")
results = {}
for name, model in models.items():
    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
    results[name] = scores.mean()
    print(f"{name:20s}: {scores.mean():.4f} (+/- {scores.std():.4f})")

# Select best model
best_model = max(results, key=results.get)
print(f"\nBest Model: {best_model} (Accuracy: {results[best_model]:.4f})")
</code></pre>
<h3>Hyperparameter Optimization</h3>
<p>Automatically tunes model parameters:</p>
<ul>
<li><strong>Grid Search</strong>: Explore all combinations</li>
<li><strong>Random Search</strong>: Random sampling</li>
<li><strong>Bayesian Optimization</strong>: Efficient search</li>
<li><strong>Evolutionary Algorithms</strong>: Genetic algorithms</li>
</ul>
<pre><code class="language-python">from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# Data preparation
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Hyperparameter search space
param_distributions = {
    'n_estimators': randint(50, 500),
    'max_depth': [None] + list(range(5, 50, 5)),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': uniform(0.1, 0.9)
}

# Random search
random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_distributions=param_distributions,
    n_iter=50,
    cv=3,
    random_state=42,
    n_jobs=-1,
    verbose=0
)

print("=== Hyperparameter Optimization ===")
random_search.fit(X_train, y_train)

print(f"Best Score (CV): {random_search.best_score_:.4f}")
print(f"Best Parameters:")
for param, value in random_search.best_params_.items():
    print(f"  {param}: {value}")

# Evaluate on test set
test_score = random_search.score(X_test, y_test)
print(f"\nTest Set Accuracy: {test_score:.4f}")
</code></pre>
<h3>AutoML Workflow Diagram</h3>
<div class="mermaid">
graph TD
    A[Raw Data] --&gt; B[Data Preprocessing Automation]
    B --&gt; C[Feature Engineering]
    C --&gt; D[Model Selection]
    D --&gt; E[Hyperparameter Optimization]
    E --&gt; F[Ensemble]
    F --&gt; G[Final Model]

    B --&gt; B1[Missing Value Handling]
    B --&gt; B2[Outlier Detection]
    B --&gt; B3[Scaling]

    C --&gt; C1[Polynomial Features]
    C --&gt; C2[Aggregate Features]
    C --&gt; C3[Feature Selection]

    D --&gt; D1[Linear Models]
    D --&gt; D2[Tree-based]
    D --&gt; D3[Neural Networks]

    E --&gt; E1[Grid Search]
    E --&gt; E2[Bayesian Optimization]
    E --&gt; E3[Evolutionary Methods]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#fce4ec
    style F fill:#e8f5e9
    style G fill:#c8e6c9
</div>
<hr/>
<h2>1.3 Neural Architecture Search (NAS)</h2>
<h3>Concept of NAS</h3>
<p><strong>Neural Architecture Search (NAS)</strong> is a technology that automatically designs neural network architectures. Algorithms automatically search for network structures that were manually designed by humans.</p>
<blockquote>
<p>NAS can be described as "a neural network that designs neural networks"</p>
</blockquote>
<h3>Search Space</h3>
<p>Design elements explored by NAS:</p>
<ul>
<li><strong>Layer Types</strong>: Convolutional layers, fully connected layers, pooling layers, etc.</li>
<li><strong>Number of Layers</strong>: Network depth</li>
<li><strong>Layer Parameters</strong>: Number of filters, kernel size, stride, etc.</li>
<li><strong>Connection Patterns</strong>: Skip connections, residual connections, etc.</li>
<li><strong>Activation Functions</strong>: ReLU, Sigmoid, Tanh, etc.</li>
</ul>
<h3>Search Strategies</h3>
<h4>1. Random Search</h4>
<p>Randomly samples and evaluates architectures. Simple but inefficient.</p>
<h4>2. Reinforcement Learning-based</h4>
<p>A controller (RNN) generates architectures and learns using their performance as rewards.</p>
<p>Reward function:</p>
<p>$$
R = \text{Accuracy} - \lambda \cdot \text{Complexity}
$$</p>
<ul>
<li>$\text{Accuracy}$: Validation accuracy</li>
<li>$\text{Complexity}$: Model complexity (number of parameters, etc.)</li>
<li>$\lambda$: Complexity penalty coefficient</li>
</ul>
<h4>3. Evolutionary Algorithms</h4>
<p>Uses genetic algorithms to evolve superior architectures.</p>
<ul>
<li><strong>Mutation</strong>: Add/remove layers, change parameters</li>
<li><strong>Crossover</strong>: Combine two architectures</li>
<li><strong>Selection</strong>: Retain high-performing architectures</li>
</ul>
<h4>4. Gradient-based Methods (DARTS)</h4>
<p>Relaxes the search space to be continuous and optimizes using gradient descent. Computationally efficient.</p>
<h3>NAS Implementation Example (Simplified Version)</h3>
<pre><code class="language-python">import numpy as np
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.neural_network import MLPClassifier

# Data preparation
digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(
    digits.data, digits.target, test_size=0.2, random_state=42
)

# Simple NAS: Explore architectures with random search
def random_architecture_search(n_trials=10):
    best_score = 0
    best_architecture = None

    print("=== Neural Architecture Search ===")
    for i in range(n_trials):
        # Randomly generate architecture
        n_layers = np.random.randint(1, 4)  # 1-3 layers
        hidden_layer_sizes = tuple(
            np.random.choice([32, 64, 128, 256]) for _ in range(n_layers)
        )
        activation = np.random.choice(['relu', 'tanh', 'logistic'])

        # Train and evaluate model
        model = MLPClassifier(
            hidden_layer_sizes=hidden_layer_sizes,
            activation=activation,
            max_iter=100,
            random_state=42
        )
        model.fit(X_train, y_train)
        score = model.score(X_test, y_test)

        print(f"Trial {i+1}: layers={hidden_layer_sizes}, "
              f"activation={activation}, score={score:.4f}")

        if score &gt; best_score:
            best_score = score
            best_architecture = {
                'hidden_layer_sizes': hidden_layer_sizes,
                'activation': activation,
                'score': score
            }

    return best_architecture

# Run NAS
best_arch = random_architecture_search(n_trials=10)

print(f"\n=== Best Architecture ===")
print(f"Layer Configuration: {best_arch['hidden_layer_sizes']}")
print(f"Activation Function: {best_arch['activation']}")
print(f"Accuracy: {best_arch['score']:.4f}")
</code></pre>
<h3>NAS Challenges</h3>
<table>
<thead>
<tr>
<th>Challenge</th>
<th>Description</th>
<th>Countermeasure</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Computational Cost</strong></td>
<td>Evaluating thousands of architectures</td>
<td>Early stopping, proxy task usage</td>
</tr>
<tr>
<td><strong>Search Space Size</strong></td>
<td>Combinatorial explosion</td>
<td>Search space constraints, hierarchical search</td>
</tr>
<tr>
<td><strong>Lack of Transferability</strong></td>
<td>Search needed for each task</td>
<td>Transfer learning, meta-learning utilization</td>
</tr>
<tr>
<td><strong>Overfitting</strong></td>
<td>Overfitting to validation data</td>
<td>Regularization, use multiple datasets</td>
</tr>
</tbody>
</table>
<hr/>
<h2>1.4 Meta-Learning</h2>
<h3>Learning to Learn</h3>
<p><strong>Meta-Learning</strong> is a method that "learns how to learn." It leverages experience from past tasks to efficiently learn new tasks.</p>
<blockquote>
<p>"Learning the learning algorithm itself" - The essence of meta-learning</p>
</blockquote>
<h3>Few-shot Learning</h3>
<p>A method for efficiently learning from a small number of samples.</p>
<p><strong>N-way K-shot learning</strong>:</p>
<ul>
<li>N: Number of classes</li>
<li>K: Number of samples per class</li>
<li>Example: 5-way 1-shot = 5 classes, 1 sample each</li>
</ul>
<pre><code class="language-python">import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

# Few-shot learning simulation
def few_shot_learning_demo(n_way=5, k_shot=3):
    # Data preparation
    digits = load_digits()
    X, y = digits.data, digits.target

    # Select task (n_way classes)
    selected_classes = np.random.choice(10, n_way, replace=False)

    # Support set (training: k_shot √ó n_way samples)
    support_X, support_y = [], []
    # Query set (testing)
    query_X, query_y = [], []

    for cls in selected_classes:
        cls_indices = np.where(y == cls)[0]
        selected = np.random.choice(cls_indices, k_shot + 10, replace=False)

        # k_shot samples to support set
        support_X.extend(X[selected[:k_shot]])
        support_y.extend([cls] * k_shot)

        # Remaining to query set
        query_X.extend(X[selected[k_shot:]])
        query_y.extend([cls] * 10)

    support_X = np.array(support_X)
    support_y = np.array(support_y)
    query_X = np.array(query_X)
    query_y = np.array(query_y)

    # Few-shot learning (using KNN)
    model = KNeighborsClassifier(n_neighbors=min(3, k_shot))
    model.fit(support_X, support_y)

    # Evaluate
    accuracy = model.score(query_X, query_y)

    print(f"=== {n_way}-way {k_shot}-shot Learning ===")
    print(f"Support Set: {len(support_X)} samples")
    print(f"Query Set: {len(query_X)} samples")
    print(f"Accuracy: {accuracy:.4f}")

    return accuracy

# Experiment with different settings
for k in [1, 3, 5]:
    few_shot_learning_demo(n_way=5, k_shot=k)
    print()
</code></pre>
<h3>Transfer Learning</h3>
<p>Transfer knowledge learned on one task to another task.</p>
<ul>
<li><strong>Pre-trained Models</strong>: Use models trained on ImageNet, etc.</li>
<li><strong>Fine-tuning</strong>: Adjust for new tasks</li>
<li><strong>Domain Adaptation</strong>: Reduce inter-domain differences</li>
</ul>
<h3>Warm-starting</h3>
<p>Use optimal parameters from past tasks as initial values to accelerate learning on new tasks.</p>
<pre><code class="language-python">from sklearn.linear_model import SGDClassifier
from sklearn.datasets import make_classification

# Task 1 and Task 2 (similar tasks)
X1, y1 = make_classification(n_samples=1000, n_features=20,
                             n_informative=15, random_state=42)
X2, y2 = make_classification(n_samples=1000, n_features=20,
                             n_informative=15, random_state=43)

print("=== Warm-starting Effect Verification ===")

# Cold start (learn Task 2 from scratch)
model_cold = SGDClassifier(max_iter=100, random_state=42)
model_cold.fit(X2[:100], y2[:100])  # Learn with small data
score_cold = model_cold.score(X2[100:], y2[100:])

# Warm start (pre-train on Task 1)
model_warm = SGDClassifier(max_iter=100, random_state=42)
model_warm.fit(X1, y1)  # Learn on Task 1
model_warm.partial_fit(X2[:100], y2[:100])  # Additional learning on Task 2
score_warm = model_warm.score(X2[100:], y2[100:])

print(f"Cold Start Accuracy: {score_cold:.4f}")
print(f"Warm Start Accuracy: {score_warm:.4f}")
print(f"Improvement: {(score_warm - score_cold) * 100:.2f}%")
</code></pre>
<hr/>
<h2>1.5 AutoML Evaluation</h2>
<h3>Performance Metrics</h3>
<p>Metrics for evaluating AutoML system performance:</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Description</th>
<th>Importance</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Prediction Accuracy</strong></td>
<td>Model prediction performance</td>
<td>Most important</td>
</tr>
<tr>
<td><strong>Search Time</strong></td>
<td>Time to find optimal model</td>
<td>Practically important</td>
</tr>
<tr>
<td><strong>Computational Cost</strong></td>
<td>Required resources (CPU, GPU, memory)</td>
<td>Scalability</td>
</tr>
<tr>
<td><strong>Robustness</strong></td>
<td>Stability across different datasets</td>
<td>Generality</td>
</tr>
</tbody>
</table>
<h3>Computational Cost</h3>
<p>Quantifying AutoML computational cost:</p>
<p>$$
\text{Total Cost} = \sum_{i=1}^{n} C_i \times T_i
$$</p>
<ul>
<li>$C_i$: Computational cost of i-th model (FLOPS, etc.)</li>
<li>$T_i$: Training time of i-th model</li>
<li>$n$: Total number of models evaluated</li>
</ul>
<h3>Reproducibility</h3>
<p>Whether the same input produces the same results:</p>
<ul>
<li><strong>Fixed Random Seeds</strong>: Reproducible experiments</li>
<li><strong>Pipeline Saving</strong>: Save trained models and preprocessing</li>
<li><strong>Version Control</strong>: Record library versions</li>
</ul>
<h3>Interpretability</h3>
<p>Understanding AutoML's decision process:</p>
<ul>
<li><strong>Feature Importance</strong>: Which features are important</li>
<li><strong>Model Selection Reasoning</strong>: Why that model was chosen</li>
<li><strong>Hyperparameter Impact</strong>: Contribution of each parameter</li>
</ul>
<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt

# Data preparation
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Model training
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Feature importance
feature_importance = model.feature_importances_
feature_names = iris.feature_names

# Permutation Importance
perm_importance = permutation_importance(
    model, X_test, y_test, n_repeats=10, random_state=42
)

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Feature importance
axes[0].barh(feature_names, feature_importance)
axes[0].set_xlabel('Importance')
axes[0].set_title('Feature Importance (Gini)')
axes[0].grid(True, alpha=0.3)

# Permutation Importance
axes[1].barh(feature_names, perm_importance.importances_mean)
axes[1].set_xlabel('Importance')
axes[1].set_title('Permutation Importance')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Interpretability Analysis ===")
for name, importance in zip(feature_names, feature_importance):
    print(f"{name:20s}: {importance:.4f}")
</code></pre>
<hr/>
<h2>1.6 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li><p><strong>AutoML Concepts</strong></p>
<ul>
<li>Realizing democratization of machine learning</li>
<li>Efficiency and reduced expertise requirements</li>
<li>Differences and advantages over traditional methods</li>
</ul></li>
<li><p><strong>AutoML Components</strong></p>
<ul>
<li>Data preprocessing automation</li>
<li>Feature engineering</li>
<li>Model selection and hyperparameter optimization</li>
</ul></li>
<li><p><strong>Neural Architecture Search</strong></p>
<ul>
<li>Automatic network structure design</li>
<li>Search strategies (RL, evolutionary, gradient-based)</li>
<li>Battle with computational cost</li>
</ul></li>
<li><p><strong>Meta-Learning</strong></p>
<ul>
<li>Learning how to learn</li>
<li>Few-shot learning, Transfer learning</li>
<li>Acceleration through warm-starting</li>
</ul></li>
<li><p><strong>AutoML Evaluation</strong></p>
<ul>
<li>Performance metrics (accuracy, time, cost)</li>
<li>Importance of reproducibility and interpretability</li>
</ul></li>
</ol>
<h3>AutoML Principles</h3>
<table>
<thead>
<tr>
<th>Principle</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Balance Automation and Transparency</strong></td>
<td>Avoid black boxes, maintain interpretability</td>
</tr>
<tr>
<td><strong>Efficiency</strong></td>
<td>Search strategies considering computational resources</td>
</tr>
<tr>
<td><strong>Generality</strong></td>
<td>Applicable to various tasks and data</td>
</tr>
<tr>
<td><strong>Leverage Domain Knowledge</strong></td>
<td>Combination of automation and expertise</td>
</tr>
<tr>
<td><strong>Continuous Improvement</strong></td>
<td>Improve learning efficiency through meta-learning</td>
</tr>
</tbody>
</table>
<h3>Next Chapter</h3>
<p>In Chapter 2, we will learn about <strong>AutoML Tools and Frameworks</strong>:</p>
<ul>
<li>Auto-sklearn</li>
<li>TPOT</li>
<li>H2O AutoML</li>
<li>Google Cloud AutoML</li>
<li>AutoKeras</li>
</ul>
<hr/>
<h2>Exercises</h2>
<h3>Question 1 (Difficulty: easy)</h3>
<p>List three main purposes of AutoML and explain each.</p>
<details>
<summary>Answer Example</summary>
<p><strong>Answer</strong>:</p>
<ol>
<li><p><strong>Efficiency</strong></p>
<ul>
<li>Description: Automate manual model development processes and significantly reduce development time</li>
<li>Effect: Can reduce work from weeks to hours</li>
</ul></li>
<li><p><strong>Reduce Expertise Requirements</strong></p>
<ul>
<li>Description: Enable building high-quality models without deep machine learning expertise</li>
<li>Effect: More people can utilize AI technology (democratization)</li>
</ul></li>
<li><p><strong>Performance Improvement</strong></p>
<ul>
<li>Description: Discover optimal combinations that humans might overlook through systematic search</li>
<li>Effect: Eliminate human bias and objectively find the best model</li>
</ul></li>
</ol>
</details>
<h3>Question 2 (Difficulty: medium)</h3>
<p>Explain four search strategies for Neural Architecture Search (NAS) and describe the advantages and disadvantages of each.</p>
<details>
<summary>Answer Example</summary>
<p><strong>Answer</strong>:</p>
<table>
<thead>
<tr>
<th>Search Strategy</th>
<th>Description</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Random Search</strong></td>
<td>Randomly sample architectures</td>
<td>Simple implementation, easy parallelization</td>
<td>Inefficient, unsuitable for large-scale search</td>
</tr>
<tr>
<td><strong>Reinforcement Learning-based</strong></td>
<td>RNN controller generates architectures</td>
<td>Efficiently explores promising regions</td>
<td>High computational cost, stability issues</td>
</tr>
<tr>
<td><strong>Evolutionary Algorithms</strong></td>
<td>Evolve superior architectures through genetic operations</td>
<td>Maintains diversity, avoids local optima</td>
<td>Slow convergence, requires large populations</td>
</tr>
<tr>
<td><strong>Gradient-based (DARTS)</strong></td>
<td>Relax search space and optimize with gradient descent</td>
<td>Computationally efficient, fast</td>
<td>Discretization errors, search space constraints</td>
</tr>
</tbody>
</table>
</details>
<h3>Question 3 (Difficulty: medium)</h3>
<p>Explain what "5-way 3-shot learning" means in few-shot learning and calculate the number of training samples in this setting.</p>
<details>
<summary>Answer Example</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Meaning of "5-way 3-shot learning"</strong>:</p>
<ul>
<li><strong>5-way</strong>: A task to classify 5 classes</li>
<li><strong>3-shot</strong>: Use only 3 samples per class for training</li>
</ul>
<p><strong>Number of training samples</strong>:</p>
<p>$$
\text{Number of samples} = \text{Number of classes} \times \text{Samples per class} = 5 \times 3 = 15
$$</p>
<p>That is, learning 5-class classification with only 15 samples.</p>
<p><strong>Concrete example</strong>:</p>
<ul>
<li>Classify 5 types of animals (dog, cat, bird, fish, horse)</li>
<li>Use only 3 images of each animal (total 15 images) for training</li>
<li>Become able to correctly classify new animal images</li>
</ul>
</details>
<h3>Question 4 (Difficulty: hard)</h3>
<p>Complete the following code to implement a simple AutoML system. Include data preprocessing, model selection, and hyperparameter optimization.</p>
<pre><code class="language-python">from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

# Data preparation
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Implement AutoML system here
</code></pre>
<details>
<summary>Answer Example</summary>
<pre><code class="language-python">from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
import numpy as np

# Data preparation
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("=== Simple AutoML System ===\n")

# Step 1: Define model candidates and hyperparameter space
models = {
    'Logistic Regression': {
        'model': LogisticRegression(max_iter=1000),
        'params': {
            'classifier__C': [0.1, 1.0, 10.0],
            'classifier__penalty': ['l2']
        }
    },
    'Random Forest': {
        'model': RandomForestClassifier(random_state=42),
        'params': {
            'classifier__n_estimators': [50, 100, 200],
            'classifier__max_depth': [None, 10, 20],
            'classifier__min_samples_split': [2, 5]
        }
    },
    'SVM': {
        'model': SVC(),
        'params': {
            'classifier__C': [0.1, 1.0, 10.0],
            'classifier__kernel': ['rbf', 'linear']
        }
    }
}

# Step 2: Preprocessing pipeline + hyperparameter optimization for each model
best_overall_score = 0
best_overall_model = None
best_overall_name = None

for name, config in models.items():
    print(f"--- {name} ---")

    # Build pipeline (preprocessing + model)
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', config['model'])
    ])

    # Hyperparameter optimization with grid search
    grid_search = GridSearchCV(
        pipeline,
        param_grid=config['params'],
        cv=5,
        scoring='accuracy',
        n_jobs=-1
    )

    grid_search.fit(X_train, y_train)

    # Results
    cv_score = grid_search.best_score_
    test_score = grid_search.score(X_test, y_test)

    print(f"  Best CV Score: {cv_score:.4f}")
    print(f"  Test Score: {test_score:.4f}")
    print(f"  Best Parameters: {grid_search.best_params_}")
    print()

    # Update best model
    if cv_score &gt; best_overall_score:
        best_overall_score = cv_score
        best_overall_model = grid_search.best_estimator_
        best_overall_name = name

# Step 3: Final results
print("=" * 50)
print(f"Best Model: {best_overall_name}")
print(f"CV Score: {best_overall_score:.4f}")
print(f"Test Score: {best_overall_model.score(X_test, y_test):.4f}")
print("=" * 50)
</code></pre>
<p><strong>Example Output</strong>:</p>
<pre><code>=== Simple AutoML System ===

--- Logistic Regression ---
  Best CV Score: 0.9780
  Test Score: 0.9825
  Best Parameters: {'classifier__C': 1.0, 'classifier__penalty': 'l2'}

--- Random Forest ---
  Best CV Score: 0.9648
  Test Score: 0.9649
  Best Parameters: {'classifier__max_depth': None, ...}

--- SVM ---
  Best CV Score: 0.9758
  Test Score: 0.9737
  Best Parameters: {'classifier__C': 1.0, 'classifier__kernel': 'linear'}

==================================================
Best Model: Logistic Regression
CV Score: 0.9780
Test Score: 0.9825
==================================================
</code></pre>
</details>
<h3>Question 5 (Difficulty: hard)</h3>
<p>Explain the tradeoff between "computational cost" and "prediction accuracy" in AutoML, and describe how to balance them practically.</p>
<details>
<summary>Answer Example</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Essence of Tradeoff</strong>:</p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>High Accuracy Pursuit</th>
<th>Low Cost Pursuit</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Search Range</strong></td>
<td>Extensive search (thousands of models)</td>
<td>Limited search (dozens of models)</td>
</tr>
<tr>
<td><strong>Time</strong></td>
<td>Days to weeks</td>
<td>Hours to days</td>
</tr>
<tr>
<td><strong>Resources</strong></td>
<td>Large-scale GPU/cluster</td>
<td>Single machine</td>
</tr>
<tr>
<td><strong>Accuracy Improvement</strong></td>
<td>+1-2% improvement</td>
<td>Baseline achievement</td>
</tr>
</tbody>
</table>
<p><strong>Strategies for Balance</strong>:</p>
<ol>
<li><p><strong>Staged Approach</strong></p>
<ul>
<li>Phase 1: Fast search to narrow down promising model candidates (hours)</li>
<li>Phase 2: Detailed optimization on candidates (days)</li>
</ul></li>
<li><p><strong>Early Stopping</strong></p>
<ul>
<li>Terminate search if validation accuracy doesn't improve</li>
<li>Set computational budget limits (time/cost)</li>
</ul></li>
<li><p><strong>Efficient Search Methods</strong></p>
<ul>
<li>Use Bayesian optimization instead of random search</li>
<li>Improve initial state with transfer learning or meta-learning</li>
</ul></li>
<li><p><strong>Task-dependent Priorities</strong></p>
<ul>
<li>Production systems: Accuracy priority (allow high cost)</li>
<li>Prototypes: Speed priority (emphasize low cost)</li>
<li>Research: Balance both</li>
</ul></li>
<li><p><strong>Multi-objective Optimization</strong></p>
<ul>
<li>Include computational cost in objective function</li>
</ul></li>
</ol>
<p>$$
\text{Objective} = \alpha \cdot \text{Accuracy} - (1-\alpha) \cdot \log(\text{Cost})
$$</p>
<ul>
<li>$\alpha$: Weight for accuracy and cost (0 to 1)</li>
</ul>
<p><strong>Practical Recommendations</strong>:</p>
<ul>
<li>First explore with low cost to understand baseline performance</li>
<li>Only perform high-cost search when business value is high</li>
<li>Quantitatively evaluate cost and effect of 1% accuracy improvement</li>
</ul>
</details>
<hr/>
<h2>References</h2>
<ol>
<li>Hutter, F., Kotthoff, L., &amp; Vanschoren, J. (Eds.). (2019). <em>Automated Machine Learning: Methods, Systems, Challenges</em>. Springer.</li>
<li>Elsken, T., Metzen, J. H., &amp; Hutter, F. (2019). Neural Architecture Search: A Survey. <em>Journal of Machine Learning Research</em>, 20(55), 1-21.</li>
<li>Hospedales, T., Antoniou, A., Micaelli, P., &amp; Storkey, A. (2021). Meta-Learning in Neural Networks: A Survey. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>.</li>
<li>Feurer, M., &amp; Hutter, F. (2019). Hyperparameter Optimization. In <em>Automated Machine Learning</em> (pp. 3-33). Springer.</li>
<li>He, X., Zhao, K., &amp; Chu, X. (2021). AutoML: A survey of the state-of-the-art. <em>Knowledge-Based Systems</em>, 212, 106622.</li>
</ol>
<div class="navigation">
<a class="nav-button" href="index.html">‚Üê Series Index</a>
<a class="nav-button" href="chapter2-hyperparameter-optimization.html">Next Chapter: Hyperparameter Optimization ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The authors and Tohoku University assume no responsibility for the content, availability, or safety of external links or third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the authors and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content follow the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<p><strong>Created by</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-21</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
