<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 1: Fundamentals of Large-Scale Data Processing - AI Terakoya</title>

        <link rel="stylesheet" href="../../assets/css/knowledge-base.css">

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/large-scale-data-processing-introduction/index.html">Large Scale Data Processing</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 1: Fundamentals of Large-Scale Data Processing</h1>
            <p class="subtitle">Understanding the Principles of Scalability and Distributed Processing</p>
            <div class="meta">
                <span class="meta-item">üìñ Reading Time: 25-30 minutes</span>
                <span class="meta-item">üìä Difficulty: Beginner to Intermediate</span>
                <span class="meta-item">üíª Code Examples: 7</span>
                <span class="meta-item">üìù Exercises: 5</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master the following:</p>
<ul>
<li>‚úÖ Understand scalability challenges in large-scale data processing</li>
<li>‚úÖ Grasp the basic concepts and architecture of distributed processing</li>
<li>‚úÖ Learn the types of parallelization strategies and how to use them</li>
<li>‚úÖ Understand challenges in distributed systems and their solutions</li>
<li>‚úÖ Understand major distributed processing tools and ecosystems</li>
<li>‚úÖ Implement parallelization in actual code</li>
</ul>

<hr>

<h2>1.1 Scalability Challenges</h2>

<h3>Data Size Growth</h3>

<p>In modern machine learning projects, data volumes are growing explosively.</p>

<blockquote>
<p>"Facing data volumes that cannot be processed on a single machine is no longer an exception but the norm."</p>
</blockquote>

<table>
<thead>
<tr>
<th>Data Scale</th>
<th>Size Range</th>
<th>Processing Method</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Small-scale</strong></td>
<td>~1GB</td>
<td>Single-machine in-memory processing</td>
</tr>
<tr>
<td><strong>Medium-scale</strong></td>
<td>1GB~100GB</td>
<td>Memory optimization, chunk processing</td>
</tr>
<tr>
<td><strong>Large-scale</strong></td>
<td>100GB~1TB</td>
<td>Distributed processing, parallelization</td>
</tr>
<tr>
<td><strong>Ultra-large-scale</strong></td>
<td>1TB+</td>
<td>Clusters, distributed file systems</td>
</tr>
</tbody>
</table>

<h3>Memory Constraints</h3>

<p>The most common problem is that data doesn't fit in memory.</p>

<pre><code class="language-python">import numpy as np
import sys

# Check memory usage
def memory_usage_mb(data):
    """Return memory usage of data in MB"""
    return sys.getsizeof(data) / (1024 ** 2)

# Large-scale data example
n_samples = 10_000_000  # 10 million samples
n_features = 100

# Normal array (loading all data into memory)
# This consumes approximately 7.5GB of memory
# data = np.random.random((n_samples, n_features))  # Potential out-of-memory error

# Check with smaller data
n_samples_small = 1_000_000  # 1 million samples
data_small = np.random.random((n_samples_small, n_features))

print("=== Memory Usage Check ===")
print(f"Data shape: {data_small.shape}")
print(f"Memory usage: {memory_usage_mb(data_small):.2f} MB")
print(f"\nEstimate: For {n_samples:,} samples")
print(f"Memory usage: {memory_usage_mb(data_small) * 10:.2f} MB ({memory_usage_mb(data_small) * 10 / 1024:.2f} GB)")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Memory Usage Check ===
Data shape: (1000000, 100)
Memory usage: 762.94 MB

Estimate: For 10,000,000 samples
Memory usage: 7629.40 MB (7.45 GB)
</code></pre>

<h3>Computation Time Issues</h3>

<p>As data size increases, computation time increases non-linearly.</p>

<pre><code class="language-python">import time
import numpy as np
import matplotlib.pyplot as plt

# Measure computation time for different sizes
sizes = [1000, 5000, 10000, 50000, 100000]
times = []

print("=== Computation Time Measurement ===")
for size in sizes:
    X = np.random.random((size, 100))

    start = time.time()
    # Simple matrix operation
    result = X @ X.T
    elapsed = time.time() - start

    times.append(elapsed)
    print(f"Size {size:6d}: {elapsed:.4f}s")

# Visualization
plt.figure(figsize=(10, 6))
plt.plot(sizes, times, marker='o', linewidth=2, markersize=8)
plt.xlabel('Data Size (Number of Samples)', fontsize=12)
plt.ylabel('Computation Time (seconds)', fontsize=12)
plt.title('Relationship Between Data Size and Computation Time', fontsize=14)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Estimate time complexity
print(f"\nTime increase ratio for 10x size increase: {times[-1] / times[0]:.1f}x")
</code></pre>

<h3>I/O Bottlenecks</h3>

<p>Disk I/O is a major bottleneck in large-scale data processing.</p>

<table>
<thead>
<tr>
<th>Storage Type</th>
<th>Read Speed</th>
<th>Relative Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td>Memory (RAM)</td>
<td>~50 GB/s</td>
<td>1,000x</td>
</tr>
<tr>
<td>SSD</td>
<td>~500 MB/s</td>
<td>10x</td>
</tr>
<tr>
<td>HDD</td>
<td>~100 MB/s</td>
<td>1x (baseline)</td>
</tr>
<tr>
<td>Network (1Gbps)</td>
<td>~125 MB/s</td>
<td>1.25x</td>
</tr>
</tbody>
</table>

<hr>

<h2>1.2 Distributed Processing Concepts</h2>

<h3>Horizontal Scaling vs Vertical Scaling</h3>

<p>There are two approaches to achieving scalability.</p>

<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Vertical Scaling</strong><br>(Scale Up)</td>
<td>Improving single machine performance<br>(CPU, memory, storage enhancement)</td>
<td>„ÉªSimple implementation<br>„ÉªNo communication overhead</td>
<td>„ÉªPhysical limitations exist<br>„ÉªCost increases non-linearly</td>
</tr>
<tr>
<td><strong>Horizontal Scaling</strong><br>(Scale Out)</td>
<td>Distributed processing across multiple machines<br>(Increasing number of nodes)</td>
<td>„ÉªTheoretically infinitely scalable<br>„ÉªImproved fault tolerance</td>
<td>„ÉªComplex implementation<br>„ÉªCommunication costs occur</td>
</tr>
</tbody>
</table>

<blockquote>
<p><strong>Practical Selection</strong>: Typically, vertical scaling is pursued to its limit before transitioning to horizontal scaling.</p>
</blockquote>

<h3>Master-Worker Architecture</h3>

<p>The most common pattern in distributed processing is the <strong>Master-Worker</strong> architecture.</p>

<div class="mermaid">
graph TD
    M[Master Node<br/>Task Distribution & Result Aggregation] --> W1[Worker 1<br/>Partial Computation]
    M --> W2[Worker 2<br/>Partial Computation]
    M --> W3[Worker 3<br/>Partial Computation]
    M --> W4[Worker 4<br/>Partial Computation]

    W1 --> R[Result Integration]
    W2 --> R
    W3 --> R
    W4 --> R

    style M fill:#9d4edd
    style W1 fill:#e3f2fd
    style W2 fill:#e3f2fd
    style W3 fill:#e3f2fd
    style W4 fill:#e3f2fd
    style R fill:#c8e6c9
</div>

<h4>Role Division</h4>

<ul>
<li><strong>Master Node</strong>:
<ul>
<li>Task splitting and assignment</li>
<li>Worker monitoring</li>
<li>Result aggregation</li>
<li>Fault detection and recovery</li>
</ul></li>
<li><strong>Worker Nodes</strong>:
<ul>
<li>Execution of assigned tasks</li>
<li>Result transmission</li>
<li>Status reporting</li>
</ul></li>
</ul>

<h3>Data Partitioning and Sharding</h3>

<p><strong>Sharding</strong> is the technique of dividing data into multiple partitions.</p>

<h4>Partitioning Strategies</h4>

<table>
<thead>
<tr>
<th>Strategy</th>
<th>Description</th>
<th>Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Horizontal Partitioning</strong></td>
<td>Split by rows</td>
<td>Time-series data, user data</td>
</tr>
<tr>
<td><strong>Vertical Partitioning</strong></td>
<td>Split by columns</td>
<td>Cases with many features</td>
</tr>
<tr>
<td><strong>Hash-based</strong></td>
<td>Split by key hash value</td>
<td>Requires uniform distribution</td>
</tr>
<tr>
<td><strong>Range-based</strong></td>
<td>Split by value ranges</td>
<td>Sorted data</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">import numpy as np
import pandas as pd

# Sample data
n_samples = 1000
data = pd.DataFrame({
    'user_id': np.random.randint(1, 100, n_samples),
    'timestamp': pd.date_range('2024-01-01', periods=n_samples, freq='1min'),
    'value': np.random.random(n_samples)
})

# 1. Horizontal partitioning (by rows)
n_partitions = 4
partition_size = len(data) // n_partitions

horizontal_shards = []
for i in range(n_partitions):
    start = i * partition_size
    end = start + partition_size if i < n_partitions - 1 else len(data)
    shard = data.iloc[start:end]
    horizontal_shards.append(shard)
    print(f"Shard {i+1}: {len(shard)} rows")

# 2. Hash-based partitioning
def hash_partition(user_id, n_partitions):
    return hash(user_id) % n_partitions

data['partition'] = data['user_id'].apply(lambda x: hash_partition(x, n_partitions))

hash_shards = []
for i in range(n_partitions):
    shard = data[data['partition'] == i]
    hash_shards.append(shard)
    print(f"Hash shard {i+1}: {len(shard)} rows")

# Check distribution
print("\n=== Check Partitioning Balance ===")
hash_sizes = [len(shard) for shard in hash_shards]
print(f"Min: {min(hash_sizes)}, Max: {max(hash_sizes)}, Mean: {np.mean(hash_sizes):.1f}")
</code></pre>

<h3>Distributed Processing Architecture Diagram</h3>

<div class="mermaid">
graph LR
    subgraph "Input Data"
        D[Large Dataset<br/>1TB]
    end

    subgraph "Distributed Storage"
        S1[Shard 1<br/>250GB]
        S2[Shard 2<br/>250GB]
        S3[Shard 3<br/>250GB]
        S4[Shard 4<br/>250GB]
    end

    subgraph "Parallel Processing"
        P1[Process 1]
        P2[Process 2]
        P3[Process 3]
        P4[Process 4]
    end

    subgraph "Result Aggregation"
        A[Aggregation & Integration]
    end

    D --> S1
    D --> S2
    D --> S3
    D --> S4

    S1 --> P1
    S2 --> P2
    S3 --> P3
    S4 --> P4

    P1 --> A
    P2 --> A
    P3 --> A
    P4 --> A

    style D fill:#ffebee
    style S1 fill:#e3f2fd
    style S2 fill:#e3f2fd
    style S3 fill:#e3f2fd
    style S4 fill:#e3f2fd
    style P1 fill:#fff3e0
    style P2 fill:#fff3e0
    style P3 fill:#fff3e0
    style P4 fill:#fff3e0
    style A fill:#c8e6c9
</div>

<hr>

<h2>1.3 Parallelization Strategies</h2>

<h3>Data Parallelism</h3>

<p><strong>Data Parallelism</strong> involves splitting data and executing the same processing on each partition in parallel.</p>

<blockquote>
<p>This is the most common and easiest-to-implement parallelization technique.</p>
</blockquote>

<pre><code class="language-python">import numpy as np
import multiprocessing as mp
import time

# Processing function
def process_chunk(data_chunk):
    """Process data chunk (example: mean calculation)"""
    return np.mean(data_chunk, axis=0)

# Single-process version
def single_process_compute(data):
    start = time.time()
    result = np.mean(data, axis=0)
    elapsed = time.time() - start
    return result, elapsed

# Multi-process version (data parallelism)
def multi_process_compute(data, n_workers=4):
    start = time.time()

    # Split data
    chunks = np.array_split(data, n_workers)

    # Parallel processing
    with mp.Pool(n_workers) as pool:
        results = pool.map(process_chunk, chunks)

    # Integrate results
    final_result = np.mean(results, axis=0)
    elapsed = time.time() - start

    return final_result, elapsed

# Test
if __name__ == '__main__':
    # Sample data
    data = np.random.random((10_000_000, 10))

    print("=== Data Parallelism Comparison ===")
    print(f"Data size: {data.shape}")

    # Single-process
    result_single, time_single = single_process_compute(data)
    print(f"\nSingle-process: {time_single:.4f}s")

    # Multi-process
    n_workers = mp.cpu_count()
    result_multi, time_multi = multi_process_compute(data, n_workers)
    print(f"Multi-process ({n_workers} workers): {time_multi:.4f}s")

    # Speedup
    speedup = time_single / time_multi
    print(f"\nSpeedup: {speedup:.2f}x")
    print(f"Efficiency: {speedup / n_workers * 100:.1f}%")
</code></pre>

<h3>Model Parallelism</h3>

<p><strong>Model Parallelism</strong> involves splitting the model itself and distributing it across multiple devices.</p>

<p>Used for large-scale neural networks:</p>

<ul>
<li>Place each layer on different GPUs</li>
<li>When model parameters don't fit in single device memory</li>
</ul>

<pre><code class="language-python">import numpy as np

# Conceptual example: splitting large models
class DistributedModel:
    """Conceptual implementation of model parallelism"""

    def __init__(self, layer_sizes):
        self.layers = []
        for i in range(len(layer_sizes) - 1):
            # Place each layer on different devices (here, arrays)
            weight = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01
            self.layers.append({
                'weight': weight,
                'device': f'GPU_{i % 4}'  # Distributed across 4 GPUs
            })

    def forward(self, x):
        """Forward propagation (each layer executed on different device)"""
        activation = x
        for i, layer in enumerate(self.layers):
            print(f"Layer {i+1} executing on {layer['device']}")
            activation = np.dot(activation, layer['weight'])
            activation = np.maximum(0, activation)  # ReLU
        return activation

# Usage example
print("=== Model Parallelism Example ===")
layer_sizes = [1000, 2000, 2000, 2000, 100]  # Large model
model = DistributedModel(layer_sizes)

# Input data
x = np.random.randn(1, 1000)
output = model.forward(x)

print(f"\nInput shape: {x.shape}")
print(f"Output shape: {output.shape}")
print(f"Total parameters: {sum(layer['weight'].size for layer in model.layers):,}")
</code></pre>

<h3>Pipeline Parallelism</h3>

<p><strong>Pipeline Parallelism</strong> involves splitting processing into multiple stages and executing each stage in parallel.</p>

<pre><code class="language-python">import multiprocessing as mp
from queue import Queue
import time

# Pipeline stages
def stage1_preprocess(input_queue, output_queue):
    """Stage 1: Preprocessing"""
    while True:
        item = input_queue.get()
        if item is None:
            output_queue.put(None)
            break
        # Preprocessing (example: normalization)
        processed = item / 255.0
        output_queue.put(processed)

def stage2_feature_extract(input_queue, output_queue):
    """Stage 2: Feature extraction"""
    while True:
        item = input_queue.get()
        if item is None:
            output_queue.put(None)
            break
        # Feature extraction (example: calculate statistics)
        features = [item.mean(), item.std(), item.max(), item.min()]
        output_queue.put(features)

def stage3_predict(input_queue, results):
    """Stage 3: Prediction"""
    while True:
        item = input_queue.get()
        if item is None:
            break
        # Prediction (simplified version)
        prediction = sum(item) > 2.0
        results.append(prediction)

# Pipeline parallelization implementation example (conceptual)
print("=== Pipeline Parallelism Concept ===")
print("Stage 1: Preprocessing ‚Üí Stage 2: Feature Extraction ‚Üí Stage 3: Prediction")
print("\nBy executing each stage in different processes in parallel,")
print("throughput improves.")
</code></pre>

<h3>Comparison of Parallelization Strategies</h3>

<table>
<thead>
<tr>
<th>Strategy</th>
<th>Application Scenarios</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Data Parallelism</strong></td>
<td>Large-scale data, same processing</td>
<td>Easy implementation, scalable</td>
<td>Communication cost, memory duplication</td>
</tr>
<tr>
<td><strong>Model Parallelism</strong></td>
<td>Large-scale models, GPU constraints</td>
<td>Avoids memory constraints</td>
<td>Complex implementation, inter-device communication</td>
</tr>
<tr>
<td><strong>Pipeline Parallelism</strong></td>
<td>Multi-stage processing, ETL</td>
<td>Improved throughput</td>
<td>Increased latency, balance adjustment</td>
</tr>
</tbody>
</table>

<hr>

<h2>1.4 Challenges in Distributed Systems</h2>

<h3>Communication Costs</h3>

<p>The largest overhead in distributed processing is inter-node communication.</p>

<blockquote>
<p><strong>Amdahl's Law</strong>: The non-parallelizable portions (such as communication) limit overall performance.</p>
</blockquote>

<p>$$
\text{Speedup} = \frac{1}{(1-P) + \frac{P}{N}}
$$</p>

<ul>
<li>$P$: Proportion of parallelizable portion</li>
<li>$N$: Number of processors</li>
</ul>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Visualize Amdahl's Law
def amdahl_speedup(P, N):
    """Calculate speedup according to Amdahl's Law"""
    return 1 / ((1 - P) + P / N)

# Cases with different parallelization rates
P_values = [0.5, 0.75, 0.9, 0.95, 0.99]
N_range = np.arange(1, 65)

plt.figure(figsize=(10, 6))
for P in P_values:
    speedups = [amdahl_speedup(P, N) for N in N_range]
    plt.plot(N_range, speedups, label=f'P = {P:.0%}', linewidth=2)

plt.xlabel('Number of Processors', fontsize=12)
plt.ylabel('Speedup', fontsize=12)
plt.title("Amdahl's Law: Parallelization Rate and Performance", fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print("=== Implications of Amdahl's Law ===")
print("With low parallelization rate, performance improvement is limited even with more processors")
print(f"Example: P=90%, 64 processors gives max {amdahl_speedup(0.9, 64):.1f}x speedup")
</code></pre>

<h3>Synchronous vs Asynchronous</h3>

<table>
<thead>
<tr>
<th>Method</th>
<th>Description</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Synchronous Processing</strong></td>
<td>Wait for all workers to complete</td>
<td>Simple implementation, consistency guaranteed</td>
<td>Depends on slowest worker</td>
</tr>
<tr>
<td><strong>Asynchronous Processing</strong></td>
<td>Proceed to next processing without waiting</td>
<td>Improved throughput</td>
<td>Complex consistency management</td>
</tr>
</tbody>
</table>

<h3>Fault Tolerance</h3>

<p>In distributed systems, node and network failures are unavoidable.</p>

<h4>Fault Handling Techniques</h4>

<ul>
<li><strong>Checkpointing</strong>: Periodically save state</li>
<li><strong>Replication</strong>: Replicate data across multiple nodes</li>
<li><strong>Retry</strong>: Re-execute failed tasks</li>
<li><strong>Redundancy</strong>: Execute same processing on multiple nodes</li>
</ul>

<h3>Debugging Difficulties</h3>

<p>Debugging distributed systems is difficult for the following reasons:</p>

<ul>
<li>Non-deterministic execution order</li>
<li>Timing-dependent bugs</li>
<li>Logs spanning multiple nodes</li>
<li>Difficult to reproduce</li>
</ul>

<hr>

<h2>1.5 Tools and Ecosystem</h2>

<h3>Apache Hadoop / Spark</h3>

<p><strong>Apache Hadoop</strong> and <strong>Apache Spark</strong> are de facto standards for large-scale data processing.</p>

<table>
<thead>
<tr>
<th>Tool</th>
<th>Features</th>
<th>Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Hadoop</strong></td>
<td>„ÉªMapReduce-based<br>„ÉªDisk-centric processing<br>„ÉªSuited for batch processing</td>
<td>Large-scale ETL, log analysis</td>
</tr>
<tr>
<td><strong>Spark</strong></td>
<td>„ÉªIn-memory processing<br>„ÉªHigh-speed (100x faster than Hadoop)<br>„ÉªMachine learning library (MLlib)</td>
<td>Iterative computation, machine learning</td>
</tr>
</tbody>
</table>

<pre><code class="language-python"># Conceptual usage example of Apache Spark (PySpark)
# Note: Requires Spark installation

"""
from pyspark.sql import SparkSession

# Create Spark session
spark = SparkSession.builder \
    .appName("LargeScaleProcessing") \
    .getOrCreate()

# Load large-scale data
df = spark.read.parquet("hdfs://path/to/large/data")

# Distributed processing
result = df.groupBy("category") \
    .agg({"value": "mean"}) \
    .orderBy("category")

# Save results
result.write.parquet("hdfs://path/to/output")

spark.stop()
"""

print("=== Apache Spark Features ===")
print("1. Lazy Evaluation: Automatically generates optimal execution plan")
print("2. In-memory processing: Cache intermediate results in memory")
print("3. Fault tolerance: Automatic recovery via RDD (Resilient Distributed Dataset)")
print("4. Unified API: Handle SQL, machine learning, graph processing uniformly")
</code></pre>

<h3>Dask</h3>

<p><strong>Dask</strong> is a Python-native parallel processing library.</p>

<pre><code class="language-python">import numpy as np

# Conceptual usage example of Dask
"""
import dask.array as da
import dask.dataframe as dd

# Dask array (NumPy-like API)
x = da.random.random((100000, 10000), chunks=(1000, 1000))
result = x.mean(axis=0).compute()  # Lazy evaluation ‚Üí execution

# Dask DataFrame (pandas-like API)
df = dd.read_csv('large_file_*.csv')
result = df.groupby('category').value.mean().compute()
"""

print("=== Dask Features ===")
print("1. NumPy/pandas compatible API: Easy migration of existing code")
print("2. Task graph: Automatically manages processing dependencies")
print("3. Scalable: Seamlessly scale from single machine ‚Üí cluster")
print("4. Integration with Python ecosystem: scikit-learn, XGBoost, etc.")

# Simple Dask-style processing example (conceptual)
print("\n=== Chunk Processing Example ===")
data = np.random.random((10000, 100))
chunk_size = 1000
results = []

for i in range(0, len(data), chunk_size):
    chunk = data[i:i+chunk_size]
    result = np.mean(chunk, axis=0)
    results.append(result)

final_result = np.mean(results, axis=0)
print(f"Number of chunks: {len(results)}")
print(f"Final result shape: {final_result.shape}")
</code></pre>

<h3>Ray</h3>

<p><strong>Ray</strong> is a unified framework for distributed applications.</p>

<pre><code class="language-python"># Conceptual usage example of Ray
"""
import ray

# Initialize Ray
ray.init()

# Remote function
@ray.remote
def process_data(data):
    return data.sum()

# Parallel execution
data_chunks = [np.random.random(1000) for _ in range(10)]
futures = [process_data.remote(chunk) for chunk in data_chunks]
results = ray.get(futures)  # Get results

ray.shutdown()
"""

print("=== Ray Features ===")
print("1. Low-level control: Flexible parallelization with task/actor model")
print("2. High performance: Distributed scheduling and shared memory")
print("3. Ecosystem: Ray Tune (hyperparameter tuning), RLlib (reinforcement learning)")
print("4. Ease of use: Easy parallelization with Python decorators")
</code></pre>

<h3>Selection Criteria</h3>

<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommended Tool</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>Large-scale batch processing (TB-scale)</td>
<td>Apache Spark</td>
<td>Mature ecosystem, fault tolerance</td>
</tr>
<tr>
<td>Python-centric development</td>
<td>Dask</td>
<td>NumPy/pandas compatibility, low learning curve</td>
</tr>
<tr>
<td>Complex distributed apps</td>
<td>Ray</td>
<td>Flexible control, high performance</td>
</tr>
<tr>
<td>Single-machine acceleration</td>
<td>multiprocessing, joblib</td>
<td>Simple, no additional installation required</td>
</tr>
<tr>
<td>Machine learning pipelines</td>
<td>Spark MLlib, Ray Tune</td>
<td>Integrated machine learning tools</td>
</tr>
</tbody>
</table>

<hr>

<h2>1.6 Chapter Summary</h2>

<h3>What We Learned</h3>

<ol>
<li><p><strong>Scalability Challenges</strong></p>
<ul>
<li>Constraints in data size, memory, computation time, and I/O</li>
<li>Single-machine limitations and the need for distributed processing</li>
</ul></li>

<li><p><strong>Distributed Processing Concepts</strong></p>
<ul>
<li>Horizontal scaling vs vertical scaling</li>
<li>Master-worker architecture</li>
<li>Data partitioning and sharding strategies</li>
</ul></li>

<li><p><strong>Parallelization Strategies</strong></p>
<ul>
<li>Data parallelism: Most common, easy to implement</li>
<li>Model parallelism: For large-scale models</li>
<li>Pipeline parallelism: Effective for multi-stage processing</li>
</ul></li>

<li><p><strong>Distributed System Challenges</strong></p>
<ul>
<li>Communication costs and Amdahl's Law</li>
<li>Synchronous vs asynchronous trade-offs</li>
<li>Fault tolerance and debugging difficulties</li>
</ul></li>

<li><p><strong>Tools and Ecosystem</strong></p>
<ul>
<li>Hadoop/Spark: Large-scale batch processing</li>
<li>Dask: Python-centric parallelization</li>
<li>Ray: Flexible distributed applications</li>
</ul></li>
</ol>

<h3>Important Principles</h3>

<table>
<thead>
<tr>
<th>Principle</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Optimization Order</strong></td>
<td>First algorithm, then implementation, finally parallelization</td>
</tr>
<tr>
<td><strong>Minimize Communication</strong></td>
<td>Reducing inter-node communication is key to performance improvement</td>
</tr>
<tr>
<td><strong>Appropriate Granularity</strong></td>
<td>Too fine-grained task splitting increases overhead</td>
</tr>
<tr>
<td><strong>Measurement-Driven</strong></td>
<td>Don't guess, measure actual performance and decide</td>
</tr>
<tr>
<td><strong>Gradual Scaling</strong></td>
<td>Validate at small scale before scaling up</td>
</tr>
</tbody>
</table>

<h3>Next Chapter</h3>

<p>In Chapter 2, we will learn <strong>MapReduce and Spark Fundamentals</strong>:</p>
<ul>
<li>MapReduce programming model</li>
<li>Apache Spark basic operations</li>
<li>RDD and DataFrame</li>
<li>Practical distributed processing pipelines</li>
</ul>

<hr>

<h2>Exercises</h2>

<h3>Problem 1 (Difficulty: easy)</h3>
<p>Explain the differences between horizontal scaling and vertical scaling, and describe their respective advantages and disadvantages.</p>

<details>
<summary>Sample Answer</summary>

<p><strong>Answer</strong>:</p>

<p><strong>Vertical Scaling (Scale Up)</strong>:</p>
<ul>
<li>Definition: Improving single machine performance (enhancing CPU, memory, storage)</li>
<li>Advantages:
<ul>
<li>Simple implementation (existing code can be used as-is)</li>
<li>No inter-node communication overhead</li>
<li>Easier to maintain data consistency</li>
</ul></li>
<li>Disadvantages:
<ul>
<li>Physical limitations exist</li>
<li>Cost increases non-linearly (high-performance hardware is expensive)</li>
<li>Single Point of Failure (SPOF) risk</li>
</ul></li>
</ul>

<p><strong>Horizontal Scaling (Scale Out)</strong>:</p>
<ul>
<li>Definition: Distributed processing by increasing number of machines (nodes)</li>
<li>Advantages:
<ul>
<li>Theoretically infinitely scalable</li>
<li>Linear cost (adding standard servers)</li>
<li>High fault tolerance (node redundancy)</li>
</ul></li>
<li>Disadvantages:
<ul>
<li>Complex implementation (requires distributed processing logic)</li>
<li>Inter-node communication costs occur</li>
<li>Difficult to manage data consistency</li>
</ul></li>
</ul>

<p><strong>Practical Selection</strong>: Typically, vertical scaling is pursued to its limit before transitioning to horizontal scaling.</p>

</details>

<h3>Problem 2 (Difficulty: medium)</h3>
<p>Using Amdahl's Law, calculate the speedup when running a program with 80% parallelization rate on 16 processors.</p>

<details>
<summary>Sample Answer</summary>

<pre><code class="language-python">def amdahl_speedup(P, N):
    """
    Calculate speedup according to Amdahl's Law

    Parameters:
    P: Proportion of parallelizable portion (0~1)
    N: Number of processors

    Returns:
    Speedup factor
    """
    return 1 / ((1 - P) + P / N)

# Problem calculation
P = 0.8  # 80%
N = 16   # 16 processors

speedup = amdahl_speedup(P, N)

print("=== Calculation by Amdahl's Law ===")
print(f"Parallelization rate: {P:.0%}")
print(f"Number of processors: {N}")
print(f"Speedup: {speedup:.2f}x")
print(f"\nExplanation:")
print(f"Theoretical maximum speedup (infinite processors): {1/(1-P):.2f}x")
print(f"Efficiency: {speedup/N*100:.1f}%")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Calculation by Amdahl's Law ===
Parallelization rate: 80%
Number of processors: 16
Speedup: 4.21x

Explanation:
Theoretical maximum speedup (infinite processors): 5.00x
Efficiency: 26.3%
</code></pre>

<p><strong>Formula</strong>:</p>
<p>$$
\text{Speedup} = \frac{1}{(1-0.8) + \frac{0.8}{16}} = \frac{1}{0.2 + 0.05} = \frac{1}{0.25} = 4
$$</p>

<p><strong>Explanation</strong>:</p>
<ul>
<li>Even with 16 processors, speedup is limited to 4.21x</li>
<li>The 20% non-parallelizable portion becomes the performance bottleneck</li>
<li>No more than 5x speedup is possible regardless of processors added</li>
</ul>

</details>

<h3>Problem 3 (Difficulty: medium)</h3>
<p>Parallelize the following code using multiprocessing for data parallelism.</p>

<pre><code class="language-python">import numpy as np

# Data to process
data = np.random.random((1_000_000, 10))

# Calculate statistics for each row
result = []
for row in data:
    stats = {
        'mean': row.mean(),
        'std': row.std(),
        'max': row.max(),
        'min': row.min()
    }
    result.append(stats)
</code></pre>

<details>
<summary>Sample Answer</summary>

<pre><code class="language-python">import numpy as np
import multiprocessing as mp
import time

# Processing function
def compute_stats(chunk):
    """Calculate statistics for data chunk"""
    result = []
    for row in chunk:
        stats = {
            'mean': row.mean(),
            'std': row.std(),
            'max': row.max(),
            'min': row.min()
        }
        result.append(stats)
    return result

# Single-process version
def single_process_version(data):
    start = time.time()
    result = []
    for row in data:
        stats = {
            'mean': row.mean(),
            'std': row.std(),
            'max': row.max(),
            'min': row.min()
        }
        result.append(stats)
    elapsed = time.time() - start
    return result, elapsed

# Multi-process version (data parallelism)
def multi_process_version(data, n_workers=4):
    start = time.time()

    # Split data into chunks
    chunks = np.array_split(data, n_workers)

    # Parallel processing
    with mp.Pool(n_workers) as pool:
        results = pool.map(compute_stats, chunks)

    # Integrate results
    final_result = []
    for chunk_result in results:
        final_result.extend(chunk_result)

    elapsed = time.time() - start
    return final_result, elapsed

if __name__ == '__main__':
    # Test data
    data = np.random.random((100_000, 10))  # Adjusted size

    print("=== Data Parallelism Implementation ===")
    print(f"Data shape: {data.shape}")

    # Single-process
    result_single, time_single = single_process_version(data)
    print(f"\nSingle-process: {time_single:.4f}s")

    # Multi-process
    n_workers = mp.cpu_count()
    result_multi, time_multi = multi_process_version(data, n_workers)
    print(f"Multi-process ({n_workers} workers): {time_multi:.4f}s")

    # Validation
    assert len(result_single) == len(result_multi), "Result lengths differ"
    print(f"\nSpeedup: {time_single / time_multi:.2f}x")
    print(f"‚úì Parallelization successful")
</code></pre>

<p><strong>Key Points</strong>:</p>
<ul>
<li>Split data evenly using <code>np.array_split()</code></li>
<li>Create worker pool with <code>multiprocessing.Pool</code></li>
<li>Process each chunk in parallel with <code>pool.map()</code></li>
<li>Integrate results with <code>extend()</code></li>
</ul>

</details>

<h3>Problem 4 (Difficulty: hard)</h3>
<p>When processing a dataset of 10 million rows, implement chunk processing that considers memory constraints. Set each chunk to 1 million rows and aggregate the results.</p>

<details>
<summary>Sample Answer</summary>

<pre><code class="language-python">import numpy as np
import time

def process_chunk(chunk):
    """Process each chunk (example: mean, std, sum)"""
    stats = {
        'mean': chunk.mean(axis=0),
        'std': chunk.std(axis=0),
        'sum': chunk.sum(axis=0),
        'count': len(chunk)
    }
    return stats

def aggregate_results(chunk_results):
    """Final aggregation of chunk results"""
    # Calculate total sum
    total_sum = sum(r['sum'] for r in chunk_results)
    total_count = sum(r['count'] for r in chunk_results)

    # Global mean
    global_mean = total_sum / total_count

    # Global standard deviation (weighted average)
    # More accurately, square root of weighted average of variances
    weighted_var = sum(
        r['count'] * (r['std']**2 + (r['mean'] - global_mean)**2)
        for r in chunk_results
    ) / total_count
    global_std = np.sqrt(weighted_var)

    return {
        'global_mean': global_mean,
        'global_std': global_std,
        'total_count': total_count
    }

def chunked_processing(n_samples=10_000_000, n_features=10, chunk_size=1_000_000):
    """Memory-efficient chunk processing"""
    print(f"=== Chunk Processing ===")
    print(f"Total samples: {n_samples:,}")
    print(f"Chunk size: {chunk_size:,}")
    print(f"Number of chunks: {n_samples // chunk_size}")

    start = time.time()
    chunk_results = []

    # Process by chunks
    for i in range(0, n_samples, chunk_size):
        # Determine chunk size
        current_chunk_size = min(chunk_size, n_samples - i)

        # Generate chunk data (in practice, read from file)
        chunk = np.random.random((current_chunk_size, n_features))

        # Process chunk
        result = process_chunk(chunk)
        chunk_results.append(result)

        print(f"Chunk {len(chunk_results)}: {current_chunk_size:,} rows processed")

    # Aggregate results
    final_result = aggregate_results(chunk_results)

    elapsed = time.time() - start

    print(f"\n=== Processing Results ===")
    print(f"Processing time: {elapsed:.2f}s")
    print(f"Global mean (first 3 dimensions): {final_result['global_mean'][:3]}")
    print(f"Global std (first 3 dimensions): {final_result['global_std'][:3]}")
    print(f"Total samples: {final_result['total_count']:,}")

    # Check memory efficiency
    import sys
    chunk_memory = sys.getsizeof(np.random.random((chunk_size, n_features))) / (1024**2)
    print(f"\nMemory usage per chunk: {chunk_memory:.2f} MB")
    print(f"(Processing with 1/{n_samples//chunk_size} of memory compared to loading all data at once)")

if __name__ == '__main__':
    # Execute (reduced size since actual size takes time)
    chunked_processing(n_samples=1_000_000, n_features=10, chunk_size=100_000)
</code></pre>

<p><strong>Sample Output</strong>:</p>
<pre><code>=== Chunk Processing ===
Total samples: 1,000,000
Chunk size: 100,000
Number of chunks: 10
Chunk 1: 100,000 rows processed
Chunk 2: 100,000 rows processed
...
Chunk 10: 100,000 rows processed

=== Processing Results ===
Processing time: 2.45s
Global mean (first 3 dimensions): [0.500 0.499 0.501]
Global std (first 3 dimensions): [0.289 0.288 0.290]
Total samples: 1,000,000

Memory usage per chunk: 7.63 MB
(Processing with 1/10 of memory compared to loading all data at once)
</code></pre>

<p><strong>Key Points</strong>:</p>
<ul>
<li>Process in chunks to limit memory usage</li>
<li>Save statistics for each chunk</li>
<li>Aggregate in statistically correct manner at the end (weighted average)</li>
<li>In practice, combine with file I/O or database queries</li>
</ul>

</details>

<h3>Problem 5 (Difficulty: hard)</h3>
<p>Regarding the three strategies of data parallelism, model parallelism, and pipeline parallelism, explain their respective application scenarios and considerations when combining them.</p>

<details>
<summary>Sample Answer</summary>

<p><strong>Answer</strong>:</p>

<h4>1. Data Parallelism</h4>

<p><strong>Application Scenarios</strong>:</p>
<ul>
<li>Large-scale datasets (TB-scale) processing</li>
<li>Each sample can be processed independently</li>
<li>Training same model with different data (mini-batch learning)</li>
</ul>

<p><strong>Examples</strong>:</p>
<ul>
<li>Training large-scale image classification datasets</li>
<li>Log data aggregation and analysis</li>
<li>Batch prediction processing</li>
</ul>

<h4>2. Model Parallelism</h4>

<p><strong>Application Scenarios</strong>:</p>
<ul>
<li>Model doesn't fit in single device memory</li>
<li>Large-scale neural networks (billions of parameters)</li>
<li>When computation graph can be partitioned</li>
</ul>

<p><strong>Examples</strong>:</p>
<ul>
<li>Large language models like GPT-3</li>
<li>High-resolution image processing networks</li>
<li>Graph neural networks</li>
</ul>

<h4>3. Pipeline Parallelism</h4>

<p><strong>Application Scenarios</strong>:</p>
<ul>
<li>Multiple processing stages exist</li>
<li>Each stage has different resource requirements</li>
<li>Streaming data processing</li>
</ul>

<p><strong>Examples</strong>:</p>
<ul>
<li>ETL pipelines (Extract‚ÜíTransform‚ÜíLoad)</li>
<li>Real-time data processing (preprocessing‚Üíinference‚Üípostprocessing)</li>
<li>Inter-layer pipelines in deep learning</li>
</ul>

<h4>Considerations for Combined Use</h4>

<p><strong>1. Data Parallelism + Model Parallelism</strong>:</p>
<pre><code class="language-python">"""
Ultra-large-scale model training

Example: GPT-3 training
- Model parallelism: Split each layer across multiple GPUs
- Data parallelism: Process different mini-batches on multiple model replicas
- Result: Can efficiently train on thousands of GPUs
"""
</code></pre>

<p><strong>Considerations</strong>:</p>
<ul>
<li>Complex communication patterns (inter-layer communication + gradient synchronization between replicas)</li>
<li>Optimized memory management (where to store activations, gradients)</li>
<li>Load balancing (both data and model)</li>
</ul>

<p><strong>2. Data Parallelism + Pipeline Parallelism</strong>:</p>
<pre><code class="language-python">"""
Large-scale ETL and machine learning pipeline

Example: Streaming prediction system
- Pipeline: Data acquisition ‚Üí Preprocessing ‚Üí Inference ‚Üí Postprocessing
- Data parallel: Multiple workers execute in parallel at each stage
- Result: High-throughput prediction system
"""
</code></pre>

<p><strong>Considerations</strong>:</p>
<ul>
<li>Inter-stage buffer management</li>
<li>Backpressure control (slow stage blocks fast stage)</li>
<li>End-to-end latency management</li>
</ul>

<p><strong>3. Combining All Three</strong>:</p>
<pre><code class="language-python">"""
Ultra-large-scale distributed training system

Example: Large-scale recommendation system
- Data parallel: Process different user segments in parallel
- Model parallel: Distribute embedding layers across multiple GPUs
- Pipeline: Stage-wise feature extraction ‚Üí Model training ‚Üí Evaluation
"""
</code></pre>

<p><strong>Considerations</strong>:</p>
<ul>
<li>Managing system complexity (debugging, monitoring becomes difficult)</li>
<li>Optimizing overall efficiency (which strategy has most impact)</li>
<li>Incremental implementation (start with simple strategy first)</li>
</ul>

<h4>Selection Guidelines</h4>

<table>
<thead>
<tr>
<th>Bottleneck</th>
<th>Recommended Strategy</th>
<th>Priority</th>
</tr>
</thead>
<tbody>
<tr>
<td>Large data size</td>
<td>Data parallelism</td>
<td>1st</td>
</tr>
<tr>
<td>Large model size</td>
<td>Model parallelism</td>
<td>1st</td>
</tr>
<tr>
<td>Many processing stages</td>
<td>Pipeline parallelism</td>
<td>2nd</td>
</tr>
<tr>
<td>Both large</td>
<td>Data+Model parallelism</td>
<td>Gradual</td>
</tr>
<tr>
<td>Real-time requirements</td>
<td>Pipeline parallelism</td>
<td>1st</td>
</tr>
</tbody>
</table>

<p><strong>Implementation Principles</strong>:</p>
<ol>
<li>First optimize with single strategy</li>
<li>Measure and identify next bottleneck</li>
<li>Introduce additional strategies as needed</li>
<li>Always measure overall system efficiency</li>
</ol>

</details>

<hr>

<h2>References</h2>

<ol>
<li>Dean, J., & Ghemawat, S. (2008). MapReduce: Simplified data processing on large clusters. <em>Communications of the ACM</em>, 51(1), 107-113.</li>
<li>Zaharia, M., et al. (2016). Apache Spark: A unified engine for big data processing. <em>Communications of the ACM</em>, 59(11), 56-65.</li>
<li>Moritz, P., et al. (2018). Ray: A distributed framework for emerging AI applications. <em>OSDI</em>, 561-577.</li>
<li>Rocklin, M. (2015). Dask: Parallel computation with blocked algorithms and task scheduling. <em>SciPy</em>, 126-132.</li>
<li>Barroso, L. A., Clidaras, J., & H√∂lzle, U. (2013). <em>The datacenter as a computer</em> (2nd ed.). Morgan & Claypool Publishers.</li>
</ol>

<div class="navigation">
    <a href="index.html" class="nav-button">‚Üê Series Index</a>
    <a href="chapter2-mapreduce-spark.html" class="nav-button">Next Chapter: MapReduce and Spark Fundamentals ‚Üí</a>
</div>

    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without any warranty, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
            <li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are subject to the specified terms (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
        </ul>
    </section>

<footer>
        <p><strong>Author</strong>: AI Terakoya Content Team</p>
        <p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-21</p>
        <p><strong>License</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
